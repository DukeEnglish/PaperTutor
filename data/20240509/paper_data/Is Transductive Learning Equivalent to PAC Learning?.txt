Is Transductive Learning Equivalent to PAC Learning?
Shaddin Dughmi * Yusuf Hakan Kalayci *
Department of Computer Science Department of Computer Science
University of Southern California University of Southern California
shaddin@usc.edu kalayci@usc.edu
Grayson York †
Department of Computer Science
University of Southern California
agyork@usc.edu
Abstract
Mostworkintheareaoflearning theory hasfocused ondesigning effective Probably Ap-
proximately Correct (PAC) learners. Recently, other models of learning such as transductive
error have seen more scrutiny. We move toward showing that these problems are equivalent
by reducing agnostic learning with a PAC guarantee to agnostic learning with a transductive
guarantee by adding a small number of samples to the dataset. We first rederive the result of
Aden-Alietal.[1]reducing PAClearning totransductive learning intherealizable setting us-
ingsimplertechniques andatmoregenerality asbackground forourmainpositiveresult. Our
agnostictransductivetoPACconversiontechniqueextendstheaforementionedargumenttothe
agnostic case, showing that anagnostic transductive learner canbe efficiently converted toan
agnostic PAC learner. Finally, we characterize the performance of the agnostic one inclusion
graphalgorithm ofAsilisetal.[4]forbinary classification, andshowthatplugging itintoour
reduction leads to an agnostic PAC learner that is essentially optimal. Our results imply that
transductive andPAClearning are essentially equivalent forsupervised learning withpseudo-
metriclosses intherealizable setting, andfor binary classification inthe agnostic setting. We
conjecture thisistruemoregenerally fortheagnostic setting.
1 Introduction
The dominant paradigm in statistical learning theory is the probably approximately correct (PAC)
modelduetoValiant[32]. Ontheotherhand,thetransductivemodeloflearning,asoriginallyem-
ployed by Haussler et al [20], does away withdistributionalassumptionsand evaluatesa learner’s
leave-one-outperformanceonanadversarially-chosensample. Theelegantsimplicityofthetrans-
ductive model has enabled application of combinatorial and graph-theoretic insights to learning,
*SupportedbyNSFGrantCCF-2009060.
†SupportedbyaViterbiSchoolofEngineeringGraduateStudentFellowship
1
4202
yaM
8
]LM.tats[
1v09150.5042:viXraas evidenced by a rich body of work which employs transductive learners as precursors to PAC
learners(e.g. [1,13,20]). Indeed,someofthisrecentliteraturesuggeststhattransductiveandPAC
learningmightbeequivalentinarichvarietyoflearningsettings,withessentiallyidenticalsample
complexities.
Thepresentpaperexaminesthestrengthofthisequivalence,notonlyinthepoorlyunderstood
agnostic setting, but also at a greater level of generality in the realizable setting. We build on the
recent work of Aden-Ali et al. [1] to show that both realizable and agnostic PAC learning can be
reduced to transductive learning so long as the loss function is a pseudometric. This reduction
comes at modest expense: a small number of additional samples determined by the error and
confidence parameters, as well as multiplicationof the error by a universal constant. This implies
a strong sense of equivalence of the two models in the realizable setting, where it is known, and
easy to show, that transductivelearning reduces to PAC learning at the cost of a constant factor in
the error. Such an essentially loss-less reduction from transductive to PAC learning is not known
intheagnosticsetting,whereitremainsplausiblethattransductivelearningisfundamentallymore
difficult than its PAC counterpart.1 We rule out such a separation for binary classification by
explicitly quantifying the performance of the optimal agnostic transductive learner in terms of
the VC dimension. This implies that transductive learning serves as a precursor to an essentially
optimalPAClearnerinagnosticbinaryclassification,viaourreduction. Weviewthisasjustifying
aconjecturethattransductiveandPAClearningareequivalentmorebroadlyintheagnosticsetting.
1.1 Background
The overarching goal of statistical learning theory is to “learn” structure from data in a manner
that generalizes to unseen future data. In supervised learning, there is a data domain X and label
set Y. The learner observes a sequence of labeled data points (x ,y ),...,(x ,y ) ∈ (X ×Y)n,
1 1 n n
calledthesampleortrainingdata,fromwhichitderivesapredictor h : X → Y whichanticipates
thelabel ofafuturetest datapoint. Thequalityoftheprediction isevaluatedusingalossfunction
ℓ : Y×Y → R,appliedtothepredictedlabelandthetrue(unseen)label. Thelossofapredictoris
compared,insomeaggregatesense,tothatofthebestpredictorinsomebenchmarkclassH ⊆ YX
ofhypotheses.
Sincetheinceptionoflearningtheory,severaldifferentdatamodelsandbenchmarkshavebeen
used to evaluate learners [9, 11, 18, 24, 32, 33]. The most prominent theoretical framework for
supervised learning is the PAC (Probably Approximately Correct) model due to Valiant [32]. In
additiontothehypothesisclassH ⊆ YX,alearningprobleminthissettingfixesaclassofplausible
datadistributionssupportedonX ×Y. Aninstanceofthelearningproblemisthendeterminedby
an(unknown)distributionD inthisclass,fromwhichbothtrainingandtestdataaredrawn. When
alldatadistributionsareplausible,thisistheagnosticsettingoflearning. When onlydistributions
that are consistent with some hypothesis are plausible, meaning that there exists h ∈ H such that
(x,y) ∼ D satisfies y = h(x) with probability 1, this is the realizable setting. For functions
ǫ,δ : N → R , a learner is said to be an (ǫ,δ)-PAC learner if, given training data consisting of
≥0
n i.i.d. draws from D, with probability at least 1 − δ(n) it outputs a predictor whose expected
loss on test data from D is at most ǫ(n) in excess of that of the best predictor in H. Inversely, the
1Indeed, the naive reduction from transductive to PAC learning loses a factor of 1/ǫ in the sample complexity,
whereǫistherequirederrorrate.
2samplecomplexitym (ǫ,δ)ofaPAClearnerquantifiestheminimumnumberofsamplesneeded
PAC
toguarantee excesslossǫwithprobability1−δ, forgivenǫ,δ ≥ 0.
Extensive research has been conducted on the sample complexity function for various super-
vised learning problems, whose existencedefines the PAC learnability of a problem. The primary
focusofPAClearnabilitytheoryistocharacterizetheprobleminstancesthatcanbelearnedinthis
frameworkacrossvariousdomains,suchasbinaryclassification[10,19],multi-classclassification
[12,13,14,25,26],regression[6,8,14,30],andbeyond[5]. Inadditiontothesecharacterizations,
a rich literature has examined upper bounds [6, 8, 9, 14, 15, 18] and lower bounds [30, 32] on the
samplecomplexityofPAClearning problems.
InthetransductivemodeloflearningthereisalsoahypothesisclassH,butnodistribution. An
instanceoftheproblemconsistsofasampleS ∈ (X ×Y)n thatisselectedbyanadversary. Ofthe
nlabeleddatapointsinthissample,n−1areselecteduniformlyatrandomasthetrainingset,and
a predictor is applied to the remaining test point. The realizable setting of transductive learning
restricts attention to samples that are consistent with some hypothesis in H. The agnostic setting
makes no such restriction. A transductivelearner achieves error ǫ = ǫ(n) if its expected loss is at
mostǫinexcessofthatincurredbythebestfixedpredictorinH. AsinthePACsetting,thesample
complexitym (ǫ) is defined inversely.
Trans
Similar to the PAC learning, numerous studies have investigated the sample complexity of
transductive learning for various domains [3, 4, 7, 19, 20, 21, 28]. Notably, Haussler et al. [20]
introduced the one-inclusion-graph (OIG) algorithm for realizable binary classification, which
achieves optimal sample complexity (also confirmed in [19, 21]). Subsequently, Rubinstein et al.
[28]extendedthisalgorithmtorealizablemulti-classclassificationproblems,andBartlettandLong
[7]extendedthealgorithmtopartial conceptlearning. Recently, Asilisetal. [4]extendedtheOIG
approach toagnosticclassification,also derivingoptimallearners.
Since it demands fine-grained sample-by-sample guarantees, the transductive setting poses
additional difficulties beyond those of the distributional setting of PAC learning. On the other
hand, transductive learning demands only expected error guarantees, which fall short of the high
probability guarantees required for PAC learning. Despite these differences, Warmuth [34] asked
whether optimal transductive learners can be used as optimal PAC learners, introducing a new
perspective into the study of PAC learning. However, Aden-Ali et al. [2] negatively resolved this
question, showing that optimal transductivelearners might not ensure high probability guarantees
outofthebox.
SubsequentworkbyAden-Alietal.[1]revealedthatessentiallyoptimalPAClearnersforvari-
ousrealizablelearningproblemscouldbederivedbyaggregatingtheoutputsofmultipleinstances
of a transductive learner, trained on different prefixes of the sample. To obtain this result, the
authors used a modification of a technique used by Wu et al. [35] to obtain a bound on perfor-
mance of an online learner. On the other hand, a well-known folklore result (also shown in [4])
demonstratesthat realizable PAC learner can beconverted into a transductivelearner with at most
aconstantincreaseintheerrorrate. Theseresultsshowthat,inrealizablesupervisedlearningwith
alargeclass oflossfunctions,PAC learningand transductivelearningare essentiallyequivalent.
The situation is more nuanced in the agnostic setting, which is the focus of the present paper.
The naive reduction from transductive to PAC learning suffers blowup of 1/ǫ in the sample com-
plexity (see e.g. [4]), and this is the case even for binary classification. No better reductions are
known, to our knowledge. Conversely, prior to our work there were no known general-purpose
reductionsfromPACtotransductivelearningintheagnosticsettingthatguaranteeonlyaconstant
3factorincrease inthesamplecomplexity.
1.2 Our Results
Thispapertackles thefollowingquestioninsettingswhereithas remained unanswered.
ArethePACandtransductivemodelsessentiallyequivalentinarichvarietyofsuper-
vised learningproblems?
We contribute to resolving this question in two ways. Our first result is a framework that
reduces PAC learning to transductive learning, building on that of Aden-Ali et al. [1]. As in [1],
wetrainseveralpredictorsusingtheprovidedtransductivelearneronprefixes ofthesamesample,
then aggregate their outputs to obtain a PAC learner. Whereas [1] focuses on several prominent
loss functions in the realizable setting, our results apply to both realizable and agnostic learning
with bounded (pseudo)-metric losses. For our results in the agnosticsetting, we need a somewhat
more involved analysis using martingales. That said, we only employ off-the-shelf martingale
inequalitiesforbothrealizableandagnosticanalyses. Generalizingtopseudometriclossesrequires
a unified aggregation step. In the realizable setting we use the median in general metric spaces,
andthemeanforslightlyimprovedconstantsinmetricspacessatisfyingJensen’sinequality. Inthe
agnosticsetting,ouraggregationstep employsa hold-outvalidationset.
Our second contribution is a characterization of the sample complexity of binary classifica-
tion in the agnostic transductive setting. This is obtained through analysis of the agnostic OIG
algorithmofAsilisetal.[4]. Webeginbyrelatingtheagnosticsamplecomplexitytotheempirical
RademachercomplexityofthehypothesisclassHforagivensampleS. Then,usingawell-known
upperboundontheRademachercomplexityoffiniteVCclasses[16,19],weconcludethattheag-
nosticOIGalgorithmhassamplecomplexityontheorderof VC(H),whereVC(H)denotestheVC
ǫ2
dimensionofthehypothesisclass. ThisresultimpliesthatPAClearnersobtainedfromtheagnostic
OIGalgorithmviaourreductionachieveessentiallyoptimalPACsamplecomplexityforthebinary
classificationproblem.
In the realizable setting, our first result (modestly) generalizes the approximate equivalence
of PAC and transductive learning to problems with pseudometric losses. In the agnostic setting,
our reduction shows that PAC learning is essentially no more difficult than transductive learning.
However, it remains open whether transductive learning is more difficult than PAC learning due
to its fine-grained sample-by-sample requirements. Our second result shows that this is not the
case for binary classification, where the agnostic PAC learner derived through our reduction is
essentially optimal. We conjecture that this phenomenon might extend to a broader spectrum of
agnosticlearningproblemswithpseudometriclosses,oreven beyond.
2 Technical Overview
2.1 Problem Definition
A supervised learning problem is determined by: a domain X, a label set Y, a loss function ℓ :
Y × Y → R , and a hypothesis class H ⊆ YX. For such a task, we provide a finite set of
≥0
labeledexamplesS = {(x ,y ),...(x ,y )} ∈ (X ×Y)n asinputtoalearningalgorithmA. This
1 1 n n
4learningalgorithmmapsthesetS toafunctionfromX toY,denotedasA(S) = h, whichwecall
thelearned modelorpredictor.
The goalin thisframework is to minimizethe“error” ofthepredictoron new,unseen samples
drawn from our input space. This error is quantified by a loss function ℓ which evaluates the
accuracy of thepredictions madeby h. Throughout thispaper, we assumethat ℓ(y,y′) is bounded
above by 1, i.e., ℓ(·,·) ∈ [0,1], and forms a pseudometric2. Thus the pair (Y,ℓ) constitutes a
pseudometric space. An illustrative example of such a pseudometric loss function is the 0-1 loss
inclassification problems,defined as
ℓ(y,y′) := [y 6= y′].
This definition employs the Iverson bracket notation [P], which equals 1 if the predicate P is true
and 0otherwise.
Intheliterature,numerouscriteriahavebeendevelopedtoassesstheeffectivenessofprediction
models. Two notable frameworks are the Probably Approximately Correct (PAC) learning model
[32], and the transductive learning model [20]. We begin by discussing the PAC learning model
introducedby Valiant.
In PAC learning, the error of a predictor h, given a probability distribution D over the sample
spaceX ×Y,is defined as theexpected loss:
L (h) = E [ℓ(h(x),y)].
D
(x,y)∼D
In addition, we consider a hypothesis class H ⊆ YX, against which we benchmark our predictor.
Predictorsh ∈ Harereferredtoashypotheses. WewillsaythatDisrealizablebyHifthereexists
ahypothesish∗ ∈ H whichattains L (h∗) = 0.
D
Thefollowingdefinitionestablishestheconcept ofPAClearning inthemostgeneral terms.
Definition1(PACLearner). LetDbeaclassofprobabilitydistributionsoverX ×Y andH ⊆ YX
a hypothesis class. A learner A is a PAC learner for H with respect to D if there exists a function
m : (0,1)2 → Z such that for any D ∈ D and ǫ,δ ∈ (0,1), with probability at least 1 − δ
≥1
over the choices of S and any internal randomness in A, an i.i.d sample S from D of size at least
m(ǫ,δ) satisfies
L (A(S)) ≤ minL (h)+ǫ.
D D
h∈H
Moreover, the pointwiseminimal sample function m of all samplefunctions for A, called the
PAC,A
samplecomplexityofthelearnerA, isdefined as
m (ǫ,δ) = argmin Pr [L (A(S)) ≥ ǫ] ≤ δ.
PAC,A D
n
S∼Dn
forallD ∈ D.
2Recallthatametricisanon-negativefunctiondonpairssatisfyingthatd(x,x)=0forallx,symmetry(d(x,y)=
d(y,x) for all x,y), the triangle inequality(d(x,z) ≤ d(x,y)+d(y,z) forall x,y,z), and positivity (d(x,y) > 0
wheneverx 6= y). Apseudo-metricisallowedtoviolatepositivity,i.e.,multiplepointsofthemetricspacecanbeat
distance0fromeachother.
5When D istheset of all probabilitydistributionsrealizableby H, we say that Ais arealizable
PAC learner. We also define the sample complexity of the realizable PAC learning problem for
the class H and the probabilitydistributionD as thepointwiseminimalsamplecomplexitythat is
attainableby anylearner, i.e.,
m (ǫ,δ) = minm (ǫ,δ).
PAC,H PAC,A
A
When D is the set of all probability distributions over X × Y, we say that A is an agnostic PAC
learner. We define a refined error notion to avoid notation clutter, called agnostic distributional
error,as
Ag
L (A(S)) := L (A(S))−minL (h).
D,H D D
h∈H
The criterion for agnostic PAC learnability is met when there exists a learner A such that for any
(ǫ,δ) ∈ (0,1)2, theagnosticdistributionalerror remainsbelow ǫwith probabilityat least 1−δ on
samples S with |S| > n for some n. Similar to the realizable setup, the sample complexity of the
agnosticPAClearningproblemfortheclassH isthepointwiseminimalsamplecomplexitythatis
needed by anylearner, i.e.,
Ag Ag
m (ǫ,δ) = minm (ǫ,δ)
PAC,H PAC,A
A
where, for a learner A, mAg denotes the point-wise minimum of all agnostic sample functions
PAC,A
for A. For binary classification, the sample complexities are known up to a constant factor. The
samplecomplexityfortherealizablecase, as establishedby[1, 17, 23], isgivenby
VC(H)+log(1/δ)
m (ǫ,δ) = Θ ,
PAC,H
ǫ
(cid:18) (cid:19)
whilefortheagnosticcase, as shownby[18], itis
VC(H)+log(1/δ)
Ag
m (ǫ,δ) = Θ .
PAC,H ǫ2
(cid:18) (cid:19)
In contrast to PAC learning, the transductivelearning modelevaluates a learner’s performance
against an adversarially selected sample S. Given samples S of size n, let S denote the set ex-
−i
cluding the i-th element, i.e., S = (x ,y ),...(x ,y ),(x ,y ),...(x ,y ). The trans-
−i 1 1 i−1 i−1 i+1 i+1 n n
ductiveerror,a.k.a. leave-one-outerror, foralearnerAon asampleS isdefined as [20]:
1
LTrans(A) := ℓ(A(S )(x ),y )
S n −i i i
i∈[n]
X
Analogously, the agnostic transductive error, for a learner A on sample S with respect to a hy-
pothesisclass H isdefined as [4]:
LTrans,Ag (A) := LTrans(A)−minLTrans(h).
S,H S S
h∈H
We call S realizable if there exists h∗ ∈ H such that h∗(x ) = y for each (x ,y ) ∈ S. In such a
S i i i i
case, the second term disappears, and the agnostic transductiveerror definition coincides with the
traditionaldefinitionofHaussleret al.[20].
6Definition2(TransductiveLearner). GivenasamplespaceX×Y andahypothesisclassH ⊆ YX,
a learner A is called a (realizable) transductive learner for X × Y and H, with an error rate
functionǫ(n), ifforanysampleset S drawn from(X ×Y)n thatisrealizablebyH,
LTrans(A) ≤ ǫ(n).
S
Additionally,Aiscalled anagnostictransductivelearnerwith anerrorratefunctionǫ (n),if for
Ag
anysampleset S (not necessarilyrealizable)drawnfrom(X ×Y)n,
LTrans,Ag
(A) ≤ ǫ (n).
S,H Ag
Analogousto PAC learning, thesamplecomplexityfora learner Ais defined as m (ǫ′) =
Trans,A
min{n : ǫ(m) < ǫ′ ∀m ≥ n}. Similarly, one can define the sample complexity of learner A as
mAg (ǫ′) = min{n : ǫ (m) ≤ ǫ′ ∀m ≥ n}. Furthermore, the transductive sample complexity
Trans,A Ag
forthehypothesisclass H, forbothrealizableand agnosticsettings,isdefined respectivelyas:
m (ǫ) = minm (ǫ),
Trans,H Trans,A
A
Ag Ag
m (ǫ) = minm (ǫ).
Trans,H Trans,A
A
Throughout the paper, we make the following two assumptions for transductive learners: (i)
a learner A is symmetric with respect to the order of the input, i.e., A(S) = A(π(S)) for all
permutationsπ,and(ii)theerrorrateofanytransductivelearner,ǫ(n)orǫ (n),isanon-increasing
Ag
functionofn. Theseassumptionsarewithoutlossofgeneralityforrandomizedlearners. Fordetails
on whythisholds,seeAppendixC.
TheprimarygoalofthisworkistomovetowardresolvingWarmuth[34]’sconceptualquestion
on constructing optimal PAC learners from transductive learners, moving beyond the scope of
realizablebinary classification. Ourmain resultsinclude:
1. A framework3 for converting agnostictransductivelearners into agnosticPAC learners with
comparable error rates, requiring only poly(1/ǫ,log(1/δ)) additional samples, along with
a simplification and generalization of the original result of Aden-Ali et al. [1] to arbitrary
pseudometriclosses.
2. A sharp characterization of the agnostic transductive error rate for binary classification in
terms of the VC dimension of the reference hypothesis class. This is obtained through ana-
lyzingtheagnosticOIGalgorithmof[4]. Combinedwiththefirstresult,thisimpliesthatthe
agnostic OIG algorithm can be converted into a PAC learner for binary classification with
essentiallyoptimalsamplecomplexity.
Throughoutthispaper,whenwesayaPAClearnerormoreformallyaPACsamplecomplexity
m (ǫ,δ)isessentiallyoptimal,wemeanthatthereexistsanabsoluteconstantcandapolynomial
PAC
p(·,·) such that m (ǫ,δ) ≤ m∗ (ǫ/c,δ/c) + p(1/ǫ,1/δ), where m∗ is the optimal sample
PAC PAC PAC
complexity.
3Ourframeworktransforms(agnostic)transductivelearnerstoPAClearnersforlearningproblemswithbounded
pseudometriclossfunctions.
72.2 Reduction from PAC learning to Transductive learning
Initially, we revisit an easy consequence of the central result of Aden-Ali et al. [1] for realizable
learning,which servesas thestartingpointforourwork.
Theorem 3. (EquivalenttoAden-Aliet al.[1, Theorem 2.1])Foranyrealizablelearningproblem
defined by sample space X × Y with a bounded pseudometric loss function ℓ(·,·) ∈ [0,1] and a
hypothesisclassH, wehave
3 2
m (ǫ,δ) ≤ m (ǫ/4)+ ·log .
PAC,H Trans,H
ǫ δ
(cid:18) (cid:19)
In the special cases of classification, partial concept classification, and regression, Theorem 3
is equivalent4 to Aden-Ali et al. [1, Theorem 2.1] up to constants as we show in Appendix C.
That said, we prove Theorem 3 from scratch in this paper for three reasons: First, we believe our
approachissimplerandusesonlyoff-the-shelfmartingaleinequalities,inparticularthemartingale
analogue of the multiplicative Chernoff bound outlined in [22]; second, we provide one proof
which captures essentially the entirety of supervised learning with (pseudo) metric losses; third,
this derivationserves as an accessible warmup for ourmain result foragnosticlearning. Note that
Aden-Aliet al. [1, Theorem2.1]isexpressedas aboundon ǫ(n) parameterized by m .
PAC,H
Our main contribution is the following theorem, which, for the first time, demonstrates the
reductionofPAC learningtotransductivelearning withintheagnosticlearningframework.
Theorem 4. For any agnostic learning problem defined by sample space X × Y with a bounded
pseudometriclossfunctionℓ(·,·) ∈ [0,1]anda hypothesisclassH, wehave
8 3
Ag Ag
m (ǫ,δ) ≤ m (ǫ/4)+ ·log .
PAC,H Trans,H ǫ2 ǫ(n)·δ
(cid:18) (cid:19)
OurstrategyforconstructingaPAClearner,applicabletobothrealizableandagnosticsettings,
utilizes a transductive learner as a subroutine in a manner similar to [1]. This entails execut-
ing the transductive learner on a set of k ∈ poly(1/ǫ,log(1/δ)) distinct (not necessarily disjoint)
samples. Specifically, let S = {(x ,y ),...(x ,y )} be n + k data points, each indepen-
1 1 n+k n+k
dently sampled from a distribution D over X × Y. We define a sequence of sample subsets
S = {(x ,y ),...(x ,y )}. Our PAC learners call the transductive learner A indepen-
i 1 1 n+i n+i Trans
dentlywithsamplesS ,S ,...S , generatingpredictors h ,...h .
0 1 k−1 0 k
Our main technical lemma, analogous to a similar Lemma in [1], states that the average (ag-
nostic) distributional error across predictors h ,...,h is small with high probability. Whereas
0 k
our proof in the realizable case follows the outline of that of [1], the agnostic case requires an
additional (third) martingale argument concerned with the difference between the best-in-class
performance and its empirical estimate. This lemma paves the way for an aggregation strategy
by carefully combining the outputs from these predictors. In the realizable case and without any
additional assumptionon the label space or the loss function, we select the generalized median of
4TheerrorboundbyAden-Alietal.[1]andoursamplecomplexityboundareequivalentuptoconstants,butthey
relyondifferentassumptionsaboutthetransductiveerrorrates. Aden-Alietal.[1,Theorem2.1]assumesǫ(n)·nis
increasing,whileourtheoremassumesǫ(n)isdecreasing.Bothassumptionsgenerallyholdforrandomizedrealizable
transductivelearners.AppendixCprovidesadetaileddiscussionoftheseconnections.
8the predictions — i.e., the prediction minimizing the sum of losses to the other predictions. This
generalizes the majority prediction used by [1] for classification to arbitrary metric losses. In the
realizablecasewithanormedvectorspaceoflabels,wecaninsteadusethemeanasourprediction
to improve the error by a constant factor. This generalizes the regression result of [1] to arbitrary
metriclosses. Intheagnosticsetup,suchaggregationstrategiesareineffective,astheycanresultin
aconstantblowupinabsoluteerror. Instead,weemployasmallvalidationsetofsizeO(1 log 1
ǫ2 ǫδ
toidentifythebestofourpredictorsh ,...,h .
0 k
(cid:0) (cid:1)
2.3 Characterizing Transductive sample complexity of agnostic binary clas-
sification
Inordertocharacterize thesamplecomplexityofalearnerinthePACsettingusingourtechnique,
one must first characterize the sample complexity of the underlying learner in the transductive
setting. In the case of realizable binary classification, i.e. Y = {+1,−1}, Haussler et al. [20]
introduced the One Inclusion Graph (OIG) algorithm, a transductive learner for hypothesis class
HwithatransductivesamplecomplexityofO VC(H) ,whereVC(H)denotestheVC-dimension
ǫ
ofH. (cid:16) (cid:17)
TheOIGalgorithm,takesasinputasampleS andconstructsagraphG = (V,E). Here,the
H |S
vertex set V = H consistsof restrictionsof H into S, i.e., all possibleclassificationsof samples
|S
S by hypothesis class H. The edge set E contains an edge for every pair of restricted hypothesis
h,h′ if they disagree at exactly one data point from S. The learner then finds an orientation of
G that minimizesthe maximumoutdegreeof a node, as this procedure also minimizesthe loss
H |S
of the learner on a worst-case sample. Haussler et al. [20] showed that if the edge density is low
everywhere simultaneously, orientations with small outdegree can be constructed for any sample
S to ensure small error rates. In particular, Haussler et al. [20] showed that when H has finite VC
dimension, then the OIG algorithm attains an optimal error rate of ǫ(n) = VC(H). Later works
n
derivedthiserrorrateusingdifferent techniques[19, 20, 21].
Combining this result with Theorem 3, we observe that a PAC learner designed using this
reduction attains essentially optimal samplecomplexity. The followingcorollary summarizes this
discussion.
Corollary 5 (Follows from [1]). PAC learners, derived from transductive learners, achieve es-
sentially optimal sample complexity in realizable binary classification for a hypothesis class with
finiteVCdimension.
However, the situation was less clear for the agnostic binary classification problem. Recently,
Asilisetal.[4]extendedtheOIGalgorithmtotheagnosticsettingbyconstructingagraphGAg
=
H |S
(VAg,EAg) where each vertex represents a possible binary classification of a given sample set S,
i.e.,VAg = YS, andedges existbetween classifiersthatdisagreeonasinglesample. Theresulting
graph GAg forms aBoolean hypercubeofdimension|S| = n.
H |S
Similar to the realizable learner, this agnostic learner orients the edges of GAg to minimize a
H |S
modified outdegree notion that accounts for the representativeness of the hypothesis class. Asilis
et al. [4] introduced the “discounted edge density” notion, which effectively determines the best
possibleerror rateachievableby an agnostictransductivelearner.
9Definition 6. Given an agnostic one inclusion graph GAg = (VAg,EAg), the discounted edge
H |S
densityof asubsetof verticesU ⊆ VAg is definedas:
|EAg(U,U)|− ku−H k
Φ (U) := u∈U |S 0 ,
discounted
|U|
P
where EAg(U,U) is the set of edges in GAg for which both endpoints lie in U, and ku−H k is
H |S |S 0
theminimumdisagreementbetween theclassifieruand a hypothesisfromH onsamplesS.
A familiarreader mightrecognizethat theleft term of theratio in thedefinitionof Φ is
discounted
actually the subgraph edge density introduced by Haussler et al. [20]. In the agnostic setting, the
negative term represents how far each particular class assignment is from the optimal hypothesis
in the class H. Therefore, we can think of this negative term as “credit” and observe that, all
else held equal, larger credit values lead to lower agnostic transductive error rates. In a sense,
creditmeasures howsuboptimalthishypothesisclass isforgeneral assignments,thereby allowing
alearner tocompetewithH.
The following lemma establishes the optimal error rate of transductive learners for agnostic
binaryclassification, basedon thediscountededgedensity:
Lemma 7. [Implied by Asilis et al. [4]] The agnosticOIG algorithmattains the optimalagnostic
errorrateforthetransductivelearningproblem,expressed as
1
ǫ (n) = · max max Φ (U)
Ag discounted
n S∈XnU⊆VAg
where VAg denotesthevertexset ofGAg .
H |S
However,thislemmadoesnotprovidean explicitcharacterization oftheagnostictransductive
error rate in terms of the VC-dimension of H. For the first time, we characterize the performance
of the agnostic OIG for agnostic transductive binary classification problem in terms of the VC
dimensionofthebenchmarkhypothesisclass. In particular, weshowthefollowingtheorem.
Theorem8. TheAgnosticOneInclusionGraphalgorithm[4]forbinaryclassificationwithbench-
mark hypothesis class H ⊆ YX attains agnostic error rate of ǫ(n) = 16 · VC(H) and sample
n
complexityof atmostO VC(H) . q
ǫ2
(cid:16) (cid:17)
This theorem together with Theorem 4 leads to a corollary that expands the optimality of the
reduction for realizable binary classification, as outlined in Corollary 5, to include the agnostic
scenario.
Corollary9. PAClearners,derivedfromtransductivelearners,achieveessentiallyoptimalsample
complexityin agnosticbinaryclassification.
The proof of Theorem 8 consists of two steps and relies on the combinatorial characterization
described in Lemma 7. First, we show that for an arbitrary sample set S and its agnostic OIG
GAg = (VAg,EAg),thediscountededgedensityincreaseswhenthesubsetofclassifiersU ⊆ VAg
H |S
is grown to a set which is symmetricwith respect to the labelingof an arbitrary data point. Recall
10that the label set is Y = {+1,−1}. We define two sets of classifiers U and U which each
i→+ i→−
contain copies of every classifier h from U with the decision for the i-th data point updated to
class +1 and class −1, respectively. The symmetrization of U with respect to the i-th data point
is defined as U′ = U ∪ U , and we show that the discounted edge density of U′ is always
i→+ i→−
larger than that of U. This observation implies that the discounted edge density is maximized for
thecompletesetofclassifiers, VAg.
In the second part, we estimate the discounted edge density of the complete set of classifiers.
By somealgebraicmanipulationsweshowthat,foranysampleS = {(x ,y )}n ofsizen
i i i=1
Φ (VAg) = O(n)·ℜ (H )
discounted S |S
whereℜ S(H |S) is theempirical RademacherofH |S, definbed as
n
b 1
ℜ (H ) := E sup h(x )·σ .
S |S i i
n σ∼{+1,−1}n
"h∈H |S i=1 #
X
b
Finally,weutilizethefollowingwell-knownupper-boundonRademachercomplexityoffiniteVC
dimensionhypothesisclasses:
Lemma 10. [Implied by [16] and [19]] For a sample S ∈ (X × Y)n of size n and finite VC
dimensionhypothesisclassH, we have
VC(H)
ℜ (H ) ≤ 31· .
S |S
n
r
Lemma 10 is a well-known application of the chaining argument introduced by Dudley [16].
b
Chaining is a technique to establish upper bounds for the expected value of the maximum (or
supremum) of a collection of random variables {X } , denoted as E[sup X ]. This bound
t t∈T t∈T t
is tighter than the naive sum of their expectations, E X , which can be loose when the
t∈T t
random variables are highly positively correlated. In such a scenario it makes sense to regroup
(cid:2)P (cid:3)
theserandomvariablesthatare nearly identical.
WhencalculatingtheRademachercomplexityℜ (H ),therandomvariables( n h(x )·σ )
S |S i=1 i i
are determined by hidden parameters, h, forming a metric space under the Hamming distance
P
metric, and their values are close when the hiddebn parameters are close within the metric space.
Dudley’s integral [16] allows us to upper-bound the supremum in terms of the size of the sphere
covering of these hidden parameters (hypotheses). On the other hand, Haussler [19] provided an
upper bound on the number of spheres that can pack into a hypothesis class H of finite VC di-
mension that immediately translates to an upper bound on sphere covering of H . Combining
|S
Dudley’schainingargument withHaussler’sboundyieldsthislemma.
For a formal proof of this lemma, see the lecture notes by Rebeschi [27]. For a self-contained
introductiontochainingand itsapplications,wereferreaders toTalagrand [31].
3 A simplified reduction in realizable case
In this section, we revisit and extend the technique, originally presented by Aden-Ali et al. [1],
fortransformingtransductivelearnerstoPAClearnersforrealizableproblems. Thissectionserves
11as a warm-up for our main result: transforming agnostic transductive learners into agnostic PAC
learners.
Ourmethodofreduction,insimpleterms,divergesfromtheapproachtakenbyAden-Alietal.
[1]intermsofproblemframing. Theirstudycentersondeterminingtheoptimalerrorboundgiven
asampleofsizenbytrainingtransductivelearnersonsubsetsofthedata. In contrast,ourstrategy
explorestherequisiteadditionaldataneededtotransformtransductivelearnerstoPAClearners,by
trainingonsupersetsoftheoriginalsample. Foramoredetailedcomparisonofthetwoapproaches,
seeAppendixC.
Algorithm 1: Reduction from PAC Learning to Transductive Learning in the realizable
setting.
Data: ni.i.d. samplesS fromD.
Result: Predictorh.
1 Let A Trans beatransductivelearner witherrorrateǫ(n).;
2 Define S i := {(x 1,by 1),...(x n,y n),(x n+1,y n+1),...(x n+i,y n+i)}foreach i ∈ [k];
3 Let h i−1 betheoutputpredictorofA Trans(S i−1)foreach i ∈ [k].;
4 Define has thepredictorwhichreturns themedianofpredictionsh 0(x),...h k−1(x).;
5
Output: Predictorh.;
b
b
WestreamlinetheprecedinganalysisbyadoptingthefollowingmultiplicativevariantofAzuma’s
inequalityrather thanformulatinga specializedmartingaleconcentrationbound.
Lemma 11 (MultiplicativeAzuma’s Inequality [22]). Let X ,...X ∈ [0,c] be real-valued ran-
1 n
dom variables for some c > 0. SupposethatE[X | X ,...X ] ≤ a for all i. Let µ = n a .
i 1 i−1 i i=1 i
Then foranyδ > 0,
P
δ2 ·µ
Pr X ≥ (1+δ)·µ ≤ exp − .
i
(2+δ)·c
" #
i (cid:18) (cid:19)
X
Beyond simplifying the process, we have expanded the scope of our reduction to cover any
supervisedlearning problemwithboundedmetricloss. Thisclass ofproblemsincludesa widear-
ray ofsupervised learning tasks,such as classification, partial concept classification, and bounded
regression. For this extension, we have adapted the aggregation phase of the reduction to output
the median of the prediction in the metric space, defining the median as the point that minimizes
theoveralldistancetoall otherpredictions.
Thefollowinglemmaisthekey assertionthatwillallowus tofinalizetheproofofTheorem3.
Lemma 12. Let A be a transductive learner with transductive error rate ǫ(n). Then, for any
Trans
δ ∈ (0,1), and a sample S ∼ Dn+k, with probability 1 − δ over the choices of S, predictors
h := A (S ) fori ∈ [k]satisfy:
i−1 Trans i−1
1
L (h ) ≤ 4·ǫ(n)
D i−1
k
i∈[k]
X
where k = 3·log(2/δ).
ǫ(n)
12Before weprovethelemma,letus showthathowitimpliesTheorem 3.
Proofof Theorem 3. Fixanytestpoint(x,y),lethbethepredictorthatoutputsthemedianamong
predictions h (x),...,h (x) which is the label y that minimizes the total loss between y and
0 k−1
predictions{h (x)} . b
i−1 i∈[k]
Thus,
b b
ℓ(y,y) ≤ minℓ(h (x),y)+ℓ(h (x),y) (Triangleinequality)
i−1 i−1
i∈[k]
1
b ≤ ℓ(h (x)b,y)+ℓ(h (x),y) (min ≤ avg)
i−1 i−1
k
i∈[k]
X
1 b
≤ 2· ℓ(h (x),y). (y ismedian)
i−1
k
i∈[k]
X
b
Finally, taking expectation over samples (x,y) ∼ D together with Lemma 12 concludes that dis-
tributionalerror ofpredictorhisat most8·ǫ(n) withprobability1−δ.
When (Y,ℓ) constitutes a normed vector space, the application of Jensen’s inequality ensures
thattheaverageofthepredicbtors’outputs,h (x),... h (x) yieldsatmost4·ǫ(n) distributional
0 k−1
error, thereby improvesgeneral metricspace result.
We now proceed to prove Lemma 12. To begin, let us define the empirical error of predictor
h forany i ∈ [k]againstthedatapoint(x ,y ) as
i−1 n+i n+i
d := ℓ(h (x ),y ).
i i−1 n+i n+i
Given the sample set S , d serves as an unbiased estimate of the distributional error associated
i−1 i
withpredictorh . Formally,
i−1
E[d | S ] = E[ℓ(h (x ),y ) | S ] = L (h )
i i−1 i−1 n+i n+i i−1 D i−1
as weknowthat(x ,y ) is independentlysampledfrom D.
n+i n+i
Next, we show that, with high probability, the total distributional error is at most a constant
factor larger than the total empirical error. The key observation is that the cumulative differences
of L (h )−d form a martingale,and thesequence can be boundedusing Azuma’s multiplica-
D i−1 i
tive inequality. This concept is detailed in the subsequent claim, whose proof is postponed to
AppendixA.1:
Claim13 (Forward MartingaleBound).
k k
δ
Pr L (h )− d > 2·k ·ǫ(n) ≤ .
D i−1 i
2
" #
i=1 i=1
X X
Followingthat,weturnourattentiontoshowthattotalempiricalerrorissmallwithhighprob-
ability. We observe that d acts as an unbiased estimator for the transductive error of the learner
i
A given the unordered set S as input. We then think of constructing our sequence of samples
Trans i
S ,...,S “backwards”byfirstconditioningonthe(unordered)setS ,theniterativelypeelingoff
0 k k
onesampleatatime—uniformlyatrandomwithoutreplacement—toobtainS ,S ,...,S .
k−1 k−2 0
13For i = k,...,1, observe that (x ,y ) is a uniformly random element from S , and that |S | ≥ n.
i i i i
Thetransductiveerror assumptionimpliesthat LTrans(A) ≤ ǫ(n). Finally,applying themultiplica-
Si
tiveversionofAzuma’sinequalityasecondtime,weobtainthefollowingbound. Theproofofthis
boundisdeferred to AppendixA.1.
Claim14 (Backward MartingaleBound).
k
δ
Pr d > 2·k ·ǫ(n) ≤ .
i
2
" #
i=1
X
Combiningtwoclaimsbyusingunionbound,weconcludetheproofofLemma12,fromwhich
Theorem 3followsaspreviouslydescribed.
4 Proof of Theorem 4: Reduction for Agnostic Learning
Thissectionisdevotedtothemainresult. Here,wewillproveTheorem4byfollowingaproofpath
similar to realizable setup. In the agnostic case, our goal is to show that the average performance
of a trained predictor is not significantly worse than that of the best hypothesis in H. To achieve
this goal, we employ forward and backward martingale bounds, akin to those used in the realiz-
able case, to show that the average distributional agnostic loss is not substantially worse than the
average observed agnostic loss with high probability. However, the agnostic case necessitates an
additionalbackwardmartingaleboundtoestablishthatsampleoptimalhypothesesdonotperform
considerablyworsethanthedistributionaloptimalhypothesison theirrespectivesamplepoints.
If the objective is to obtain a randomized predictor that achieves a small distributional loss
with high probability over the data distribution but in expectation over its internal randomness,
then we can simplyselect ahypothesisat random and return itsprediction on each new datapoint
encountered. Alternatively, if the goal is to obtain a deterministic learner that achieves small
distributionallosswithhighprobabilityoverthedatadistribution,thenweutilizeacross-validation
set containing a small number of additional data points. By testing each predictor on this cross
validationset, we can assert with high probability that the deterministicpredictor which performs
thebeston thiscross validationset willalsoattainasmallagnosticdistributionalloss.
Lemma 15. Let A be a transductive learner with agnostic error rate ǫ (n). Then for any
Trans Ag
δ ∈ (0,1), and a sample S ∼ Dn+k, with probability 1 − δ over the choices of S, predictors
h := A (S ) fori ∈ [k]satisfy:
i−1 Trans i−1
k
1 72·log(3/δ)
Pr LAg (h ) ≥ ǫ (n)+ ≤ δ.
k D,H i−1 Ag k
" r #
i=1
X
In particular, when k = 72·log(3/δ) , average distributional error is no more than 2·ǫ (n) with
ǫ2 (n) Ag
Ag
probabilityat most1−δl. m
In order to show that PAC learning reduces to transductivelearning in the agnosticsetting, we
willusethefollowingalgorithmto convertan arbitrary transductivelearnerto aPAClearner.
14Algorithm 2:Transformingtransductivelearners toPAC learners in theagnosticsetting
Data: ni.i.d. samplesS fromD.
Result: Predictorh.
1 Let A Trans beatransductivelearner withagnosticerrorrateǫ Ag(n).
2 Define S i := (x 1,yb 1),...(x n,y n),(x n+1,y n+1),...(x n+i,y n+i) foreach i ∈ [k].
3 Let h i−1 betheoutputpredictorofA Trans(S i−1)foreach i ∈ [k].
4 Let S Val beahold-outvalidationset ofsizek′ = O lo ǫg 2(k (n/δ )) .
Ag
5 Estimatetheempiricalerrorofeach predictor{h i−1(cid:16)} i∈[k] by(cid:17)usingvalidationset S Val
6 Set has thepredictorwithminimumvalidationerror among{h i−1} i∈[k].
7
Output: Predictorh.
b
b
Before provingLemma15, we argue briefly how it impliesTheorem 4. Lemma15 guarantees
that the average, and hence also the minimum, of the agnostic risks of {h ,...,h } is bounded
0 k−1
by2·ǫ(n)withprobability1−δ. Wethenuseahold-outvalidationdatasettoselectsomeh whose
2 i
agnostic risk is within ǫ(n) of this bound with probability 1− δ. Standard Hoeffding bounds and
2
theunionboundimplythatavalidationsetofsizek′ = O( 1 log(k))suffices. Therefore,intotal
ǫ2(n) δ
k + k′ = O 1 log( 1 ) samples suffice, as claimed in Theorem 4. We omit the details of
ǫ2(n) ǫ(n)·δ
theargument(cid:16)here, as itfollow(cid:17)sastandard Hoeffdingboundargument. Please seeAppendixBfor
theproofdetails.
WenowproceedtoproveLemma15. Ourprooffollowsasimilartrajectorytorealizablesetup.
Let h∗ be the optimal hypothesis in H for distributionD, i.e., h∗ ∈ argmin L (h). Next, we
D D h∈H D
definethefollowingrandomvariablesimilarto realizablesetting:
d = ℓ(h (x ),y )−ℓ(h∗ (x ),y ).
i i−1 i i D i i
Also,conditionedonsamplesS ,d becomesanunbiasedestimatorofthedistributionalagnostic
i−1 i
riskofthepredictorh . Formally,
i−1
E[d | S ] = E[ℓ(h (x ) 6= y ) | S ]−E[ℓ(h∗ (x ) 6= y ) | S ]
i i−1 i−1 i i i−1 D i i i−1
= L (h )−L (h∗)
D i−1 D D
= Lag (h ).
D,H i−1
The proof of Lemma 15 employs both forward and backward martingale bounds, mirroring
the approach used in the realizable context. In the forward martingale phase, we aim to demon-
strate that k LAg (h ) . k d , with high probability, by constructing a martingale and
i=1 D,H i−1 i=1 i
using Azuma’s martingale concentration inequality to show that martingale stays small with high
P P
probability.
The backward martingalephase, however,diverges from the realizable scenario. Unlikein the
realizable case where d serves as an unbiased estimator of the transductive error under certain
i
conditions, such a straightforward relationship does not exist here. To navigate this complexity,
we introduce d as an unbiased estimator for the agnostic transductive error of A using the
i Trans
sample sets S = (x ,y ),...,(x ,y ). It is straightforward to confirm that E[d ] ≤ E[d ].
i 1 1 n+i n+i i i
However, to esetablish a probabilisticguarantee, we construct two additionalmartingales, showing
e
15that theseries d and thedifferences d −d both act as super-martingales. UsingAzuma’s
i i i i i
inequalitytwice,we arguethat d remains smallwithhighprobability,thereby completingour
P i i P
proof. e e
P
In the agnostic case, the multiplicative version of Azuma’s inequality which we used in the
realizable case is not as powerful. Applying it in the agnostic setting could lead to a high loss for
our hypothesis class H, and any multiplicative blowup on the martingales defined below would
also amplify the error of the reference hypothesis class. This amplification would undermine the
possibility of a general PAC learnability guarantee. To that end, in the following concentration
bounds, we employ the traditional additive version of Azuma’s inequality in order to obtain the
desired guarantees ontheerrorofourpredictors.
Claim16 (Forward MartingaleBound).
k k
3
Pr LAg (h )− d > 8klog ≤ δ/3.
" D,H i−1 i s δ #
i=1 i=1 (cid:18) (cid:19)
X X
Proof. LetM = LAg (h )−d , andM = i M . Then,considerthefollowingconditional
i D,H i−1 i i j=1 j
probabilityto seethatM formsamartingale.
i P
E [M | S ] = E[M | S ]+E[M | S ]
i i−1 i i−1 i−1 i−1
(xi,yi)∼D
=
E[LAg
(h )−d | S ]+M
D,H i−1 i i−1 i−1
= M .
i−1
Since both d and LAg (h ) are bounded between [−1,1], M lies in [−2,2] and so |M | ≤ 2.
i D,H i−1 i i
ApplyingAzuma’sinequalityyieldsthedesired result.
Now,weknowthat k LAg (h ) is notmuchlarger than k d , i.e.,
i=1 D,H i−1 i=1 i
P k k P
LAg (h ) . d
D,H i−1 i
i=1 i=1
X X
with high probability. Next, we show that right hand side is small with high probability by con-
structingtwobackward martingales.
ForagivensampleS,leth∗ beanoptimalhypothesisforthissample. Wedefinethefollowing
S
unbiasedestimatorofthei-th agnostictransductiveerror LTrans,Ag (A)
Si,H
d = ℓ(h (x ),y )−ℓ(h∗ (x ) 6= y ).
i i−1 i i Si i i
We then decompose the total distributional agnostic error of our predictors as a the sum of trans-
e
ductiveerrors and thedifferencebetween distributionaland transductiveerrors, as follows.
k k k
d = d + (d −d )
i i i i
i=1 i=1 i=1
X X X
e e
In the next two claims, we show that k d is small with high probability by showing that both
i=1 i
component sums are small with high probability. We start by showing that the sum of d is small
P i
withhighprobability.
e
16Claim17 (Backward MartingaleBound I:Total TransductiveError).
k
3
Pr d > k ·ǫ+ 8klog ≤ δ/3
i
" s δ #
i=1 (cid:18) (cid:19)
X
e
Proof. We think of constructing our sequence of samples S ,...,S “backwards” by first con-
0 k
ditioning on the (unordered) set S = {(x ,y )}n+k, then iteratively peeling off one sample at
k i i i=1
a time — uniformly at random without replacement — to obtain S ,S ,...,S = S. For
k−1 k−2 0
i = k,...,1, observe that (x ,y ) is a uniformly random element from S , and that |S | ≥ n. The
i i i i
transductiveerrorassumptionimpliesthat LTrans,Ag (A) ≤ ǫand therefore
Si,H
E[d | S ] ≤ ǫ.
i i
Sinced isconditionallyindependentofd ,...,d givenS , itfollowsthat
i k i+1 i
e
E[d | d ,...,d ] ≤ ǫ.
i k i+1
e e e
Next,wedefinerandom variablesB = d −ǫ, B = k B and observethat
i ei e i e j=i i
E[B | B ] = B +E[d −ǫP| d ,...,d ] ≤ B
i i+1 i+e1 i k i+1 i+1
Thus, the sequence B is a backwards super-martingale — i.e., a super-martingale when viewed
i
e e e
backwards intime. InvokingAzuma’sinequality,weobtain
k
δ
Pr B > k ·ǫ(n) = Pr d > 2·k ·ǫ(n) ≤ .
1 i
3
" #
i=1
(cid:2) (cid:3) X
e
Next,weinvestigatethesecondsum. Bydefinition,h∗ istheoptimalhypothesisforS ,which
Si i
implies that its expected performance on that sample is at least as good as that of h∗ . In this
D
bound, we strengthen the in-expectation guarantee to a high probability guarantee, showing that
the sequence of h∗ predictors will not perform significantly worse on their respective test points
Si
compared toh∗ .
D
Claim18 (Backward MartingaleBound II: Transductivevs DistributionalError).
k
3 δ
Pr (d −d ) > 8klog < .
i i
" s δ # 3
i=1 (cid:18) (cid:19)
X
Wedecidedtoleavetheproofofthiscleaimfortheappendix,asitfollowsasimilarstructureto
thepreviousproofs.
CombiningClaims16,17, and 18usingtheunionbound,weobtain
k
3
Pr Lag (h ) > k ·ǫ+3 8klog ≤ δ
" D,H i−1 s δ #
i=1 (cid:18) (cid:19)
X
and therefore,
k−1
1 72·log(3/δ)
Pr Lag (h ) > ǫ+ ≤ δ.
k D,H i k
" r #
i=0
X
ThiscompletestheproofofLemma15, fromwhich Theorem4 followsas previouslydescribed.
175 Optimal Agnostic Transductive Learner for Binary Classifi-
cation
In this section, we aim to establish an agnostic transductive error rate of binary classification in
terms of the VC dimension of benchmark hypothesis class H. We analyze the agnostic OIG al-
gorithm of Asilis et al. [4] and show that together with our reduction, the agnostic OIG algorithm
obtainsessentiallyoptimalPACsamplecomplexity. Themainresultofthissectionisrestatedhere
fortheclarityofthediscussion.
Theorem8. TheAgnosticOneInclusionGraphalgorithm[4]forbinaryclassificationwithbench-
mark hypothesis class H ⊆ YX attains agnostic error rate of ǫ(n) = 16 · VC(H) and sample
n
complexityof atmostO VC(H) . q
ǫ2
(cid:16) (cid:17)
Before delving into the proof of this theorem, it is essential to discuss its implications. The-
orem 8 suggests that the sample complexity of agnostic transductive learning for binary classi-
fication is at most O VC(H) . Let A denote the agnostic transductive learner guaranteed
ǫ2 Trans
by this theorem. By i(cid:16)nvoking(cid:17)Theorem 4, we can infer that Algorithm 2 transforms A
Trans
into
a PAC learner with a sample complexity of O VC(H)+log(1/(ǫδ)) . Conversely, Theorem 4 and
ǫ2
the fundamental theorem of statistical learning(cid:16)theory [29] colle(cid:17)ctively imply a lower bound of
Ω VC(H)+log(1/δ) for the error rate of an agnostic transductive learner. Consequently, when
ǫ2
ǫ (cid:16)= O(poly(δ)),(cid:17)which is generally true in practice, we see that our theorem is optimal up to
aconstantfactor.
To commence our analysis, we introduce the agnostic one inclusion graph algorithm, as pro-
posed by Asilis et al. [4]. Consider an agnostic binary classification problem with a benchmark
hypothesis class H ⊆ YX, where Y = {+1,−1}, and let S ⊆ (X × Y)n be a sample of size n.
Theagnosticoneinclusiongraph(OIG)GAg = (VAg,EAg)isdefinedbythefollowingvertexand
H |S
edgesets:
• VAg = Yn, representingonenodeforeach possiblelabelingofthendatapoints.
• EAg = {(u,v) : ku − vk = 1}, where the Hamming distance ku − vk is defined as
0 0
n [u 6= v ], representingthenumberofindicesat whichu andv differ.
i=1 i i
It iswP orthnotingthatGAg isisomorphicto theBoolean hypercubeofdimension|S| = n.
H |S
The agnostic one inclusion graph algorithm, similarly to its realizable counterpart, orients the
edgesoftheagnosticOIGtoensurethateachvertexhasasmallout-degree. Thelabelingofn−1
data points (training data) corresponds to an edge of this agnostic OIG, and the algorithm selects
the vertex along the direction of this edge, outputting the label of the test data according to this
vertex. Now,werecallthe“discountededgedensity”notionofagnosticOIGsandrestatealemma
from [4] which identifies optimal error rate of agnostic OIG algorithm based on maximum local
discountededgedensity.
|E(U,U)|− ku−H k
Φ (U) := u∈U |S 0
discounted
|U|
P
18wherekv−Uk definestheHammingdistancebetweenavertexv andasubsetU astheminimum
0
Hamming distance between v and the members of U, i.e., kv − Uk = min kv − uk . The
0 u∈U 0
followinglemmacharacterizestheoptimalagnostictransductivesamplecomplexityoftheagnostic
OIG algorithmand therefore, thebinary classificationproblemas shownby Asiliset al. [4].
Lemma 7. [ImpliedbyAsiliset al.[4]] TheagnosticOIG algorithmattainstheoptimalagnostic
errorrateforthetransductivelearningproblem,expressed as
1
ǫ (n) = · max max Φ (U)
Ag discounted
n S∈XnU⊆VAg
where VAg denotesthevertexset ofGAg .
H |S
In the rest of this section, we utilize Asilis et al. [4]’s characterization to prove Theorem 8,
which states that the agnostic OIG algorithm admits an ǫ(n) = 16· VC(H) error rate. Our proof
n
consists of two parts. First, we show that the expression given in thqe lemma is maximized when
U = VAg, or equivalently,for the complete Boolean hypercube. The following lemmaformalizes
thisargument.
Lemma 19. Given a sample S and hypothesis class H, let GAg = (VAg,EAg) be their agnostic
H |S
OIG. ThefollowinginequalityholdsforallU ⊆ VAg:
Φ (U) ≤ Φ (VAg).
discounted discounted
ThesecondpartoftheproofistoboundthisratioforamaximalinstancewhereU = VAg. We
consider the hypothesis class H and the sample set S to be fixed arbitrarily, and aim to prove the
followinglemma.
Lemma 20. Given a sample S and hypothesis class H, let GAg = (VAg,EAg) be their agnostic
H |S
OIG. Then, we haveΦ (VAg) ≤ 16· n·VC(H).
discounted
Before presenting the proofs of these lepmmas, we first prove the main result of this section by
using them. For any arbitrary sample set S ⊆ (X ×Y)n of size n, let GAg = (VAg,EAg) be the
H |S
agnosticOIG forS, Lemma19and Lemma20 implythat
maxΦ (U) = Φ (VAg) ≤ 16· n·VC(H).
discounted discounted
U⊆V
p
Since S is an arbitrary sample set, invoking Lemma 7 ensures that the error rate is at most
16· VC(H), completingtheproofof Theorem 8. Theremaining part ofthis sectionis devotedto
n
thepqroofsofLemma19 and Lemma20.
5.1 Proof of Lemma 19
To avoid notational clutter, we omit the superscript “Ag” for VAg and EAg throughout the proof,
denoting them simplyas V and E, respectively. We first define the total credit (or total Hamming
distance)for anysubset U ⊆ V as kU −H k := ku−H k . We fix an arbitrary sample
|S 0 u∈U |S 0
P
19size|S| = n for somen ≥ 3. Notethat each vertexv ∈ V denotes a label assignmentfor samples
inS. Wedefine v as anothervertexsuchthat ith sampleisassignedto classlabel +1.
i→+
Formally,
v(x ) forany (x ,y ) ∈ S v(x ) forany (x ,y ) ∈ S
j j j −i j j j −i
v (x ) = v (x ) =
i→+ j i→− j
(+1 j = i. (−1 j = i.
Weextend thisdefinitionto subsetsofverticesas U := {u : u ∈ U} and U := {u :
i→+ i→+ i→− i→−
u ∈ U}.
Our objective is to demonstrate that Φ (V) ≥ Φ (U) for any U ⊆ V. Let
discounted discounted
α be the maximum possible value of the function Φ attained at any subset, i.e., α =
discounted
max Φ (U), and let U be an arbitrary non-empty set satisfying Φ (U) = α. As
U⊆V discounted discounted
Φ ispositiveforasubsetU thatcontainstwovertices: ahypothesish ∈ H anditsarbitrary
discounted |S
neighbor,α > 0 alwaysand U is non-empty. Next,wedefine
g(T) := |E(T,T)|−kT −H k −α·|T|.
|S 0
Recall that E(T,T) is the collection of edges with both endpoints in T. Our goal is to show that
g(V) ≥ 0. Noticethatg(V) ≥ 0 impliesΦ (V) ≥ α.
discounted
Fixanarbitraryindexi ∈ [n]anddatapoint(x ,y ) ∈ S. LetV+ andV− formapartitionofV
i i
such that V+ := {v ∈ V : v(x ) = +1} and V− := {v ∈ V : v(x ) = −1}. By using thesets V+
i i
and V−, we also define a partition of U as U+ = U ∩V+ and U− = U ∩V−. We further define
thefollowingsets:
N+ = U+ ∩U− N− = U− ∩U+
i→+ i→−
I+ = U+ \N+ I− = U− \N−.
Here, N+,I+,N−,I− form a partition of U. The intuition behind this partitioning is to analyze
the behavior of the function g when the i-th bit is flipped for each part of the set U. We refer the
reader toFigure1 foravisualizationofthesesets.
Byexaminingthechangeinthevalueofg underthisbit-flippingoperation,weaimtoestablish
arelationshipbetween g(U) and g(U′) whereU′ is obtainedby addingtheminimalset ofvertices
to U which yields a symmetric set U′ with respect to i-th coordinate, i.e. U′ = U ∪ U .
i→0 i→1
Ultimately,thiswillleadtothedesiredinequalityΦ (V) ≥ Φ (U). Next,weobserve
discounted discounted
that
g(U) = g(I+)+g(I−)+g(N+ ∪N−)+|E(I+,N+)|+|E(I−,N−)|.
Let us defineanothersetU′ := U ∪I+ ∪I− andevaluateg’svalueat U′ as
i→− i→+
g(U′) = g(U)+g(I+ )+g(I− )
i→− i→+
+|E(I+ ,U−)|+|E(I− ,U+)| (1)
i→− i→+
+|E(I+ ,I+)|+|E(I− ,I−)|.
i→− i→+
Now,wemakeseries ofobservations:
20I+ N+ I i−
→+
V+
V−
I+ N− I−
i→−
Figure 1: Symmetrization argument: A Boolean hypercube of dimension 4 is partitioned into
two smaller hypercubes of dimension 3 by fixing a certain coordinate i. The smaller hypercubes
V+ and V− are visualized as 8 boxes in the figure. Vertically aligned boxes correspond to bit
stringsthatdifferonlyat indexi. Theset U is indicatedbytheblueboxes.
1. Foreach vertexv ∈ V, becausekv −v k = 1wesee bythetriangleinequalitythat
i→− i→+ 0
max{kv −H k ,kv −H k } ≤ min{kv −H k ,kv −H k }+1.
i→+ |S 0 i→− |S 0 i→+ |S 0 i→− |S 0
Therefore, wehave
kv −H k +kv −H k ≤ 2·min{kv −H k ,kv −H k }+1.
i→+ |S 0 i→− |S 0 i→+ |S 0 i→− |S 0
2. From thefirst observationwededucethat
kI+ −H k ≤ kI+−H k +|I+| and kI− −H k ≤ kI−−H k +|I−|.
i→− |S 0 |S 0 i→+ |S 0 |S 0
Since|E(I+ ,I+ )| = |E(I+,I+)|and|E(I− ,I− )| = |E(I−,I−)|,subtractingthese
i→− i→− i→+ i→+
quantitiesfrom thepreviousinequalities,weseethat
g(I+ ) ≥ g(I+)−|I+| and g(I− ) ≥ g(I−)−|I−|
i→− i→+
3. Foredgesetsweobserve
|E(I+ ,U−)| ≥ |E(I+,N+)| and |E(I− ,U+)| ≥ |E(I−,N−)|
i→− i→+
and also
|E(I+ ,I+)| = |I+| and |E(I− ,I−)| = |I−|.
i→− i→+
By substitutingtheseinequalitiesto(1) weobtainthat
g(U′)−g(U) ≥ g(I+)−|I+|+g(I−)−|I−|+|E(I+,N+)|+|E(I−,N−)|+|I+|+|I−|
= g(I+)+g(I−)+|E(I+,N+)|+|E(I−,N−)|
= g(U)−g(N+ ∪N−).
21We claim that g(U) − g(N+ ∪ N−) is non-negative since assuming otherwise implies that
g(N+ ∪ N−) > 0 and so Φ (N+ ∪ N−) > α which contradicts with the optimality of U.
discounted
Thus,g(U′) ≥ g(U), andso Φ (U′) ≥ Φ (U).
discounted discounted
Note that by construction of the set U′, it is the minimal superset of U which is symmetric
with respect to the index i. Therefore, by repeating this process for each index i ∈ [n], to create a
set U′′, g will only increase. This implies that the maximal subset U′′ that satisfies g(U′′) ≥ 0 or
Φ (U′′) ≥ α is symmetricwith respect to any index. The only non-empty subset satisfying
discounted
thispropertyis V itself,thecompletebooleanhypercube. Therefore, theproofis complete.
5.2 Proof of Lemma 20
Wefirst restatethelemmahere:
Lemma 20. Given a sample S and hypothesis class H, let GAg = (VAg,EAg) be their agnostic
H |S
OIG. Then, we haveΦ (VAg) ≤ 16· n·VC(H).
discounted
Proof. To simplifynotation,we omit the supperscript “Ag” for VAg and EAg throughouttheproof,
denoting them as V and E, respectively, as the context is clear. Given an arbitrary set of samples
S ∈ (X,Y)n ofsizenwewriteΦ (V) as follows.
discounted
|E(V,V)|− ku−H k
Φ (V) = u∈V |S 0
discounted
|V|
P
|E(V,V)|− min ku−hk
= u∈V h∈H |S 0 (definitionofku−H k )
|S 0
|V|
P
n·2n−1 − min ku−hk
= u∈V h∈H |S 0 (|E| = n·2n−1,|V| = 2n)
2n
P
n 1
= − min ku−hk .
2 2n h∈H |S 0
u∈V
X
Next,weswitchnotationfromHammingdistancetoinnerproductbyusingthefollowingequality.
n n n
hu,hi := u(x )·h(x ) = [u(x ) = h(x )]− [u(x ) 6= h(x )] = n−2·ku−hk . (2)
i i i i i i 0
i=1 i=1 i=1
X X X
Then,
n 1 n−hu,hi
Φ (V) = − min (2)
discounted
2 2n h∈H |S 2
u∈V
X
n n 1 hu,hi
= − − min − (|V| = 2n)
2 2 2n h∈H |S 2
u∈V
X
1 1
= · maxhu,hi
2 2n h∈H |S
u∈V
X
1
= · E maxhu,hi .
2 u∼V (cid:20)h∈H |S
(cid:21)
22In the last step u is sampled uniformly random from V. Notice that u(x ) equals +1 or −1 with
i
probability1/2independentlyforany(x ,y ) ∈ S. Therefore,
i i
n
Φ (V) = ·ℜ (H )
discounted S |S
2
whereℜ (H ) is theempirical Rademachercomplexitbywhichis defined as:
S |S
n
b 1
ℜ (H ) := E sup σ ·h(x ) .
S |S i i
n σ∼{+1,−1}n
"h∈H |S i=1 #
X
b
Finally,weinvokeLemma10 tocompletetheproof:
n
Φ (V) = ·ℜ(H ) ≤ 16· n·VC(H).
discounted |S
2
p
b
References
[1] I.Aden-Ali,Y.Cherapanamjeri,A.Shetty,andN.Zhivotovskiy. Optimalpacboundswithout
uniform convergence. In 2023 IEEE 64th Annual Symposium on Foundations of Computer
Science (FOCS), pages 1203–1223, Los Alamitos, CA, USA, nov 2023. IEEE Computer
Society. doi: 10.1109/FOCS57990.2023.00071.
[2] I. Aden-Ali, Y. Cherapanamjeri, A. Shetty, and N. Zhivotovskiy. The one-inclusion graph
algorithmisnotalwaysoptimal. InTheThirtySixthAnnualConferenceonLearningTheory,
pages 72–88.PMLR, 2023.
[3] N. Alon, S. Hanneke, R. Holzman, and S. Moran. A theory of pac learnability of partial
conceptclasses. In2021IEEE62ndAnnualSymposiumonFoundationsofComputerScience
(FOCS), pages 658–671.IEEE, 2022.
[4] J.Asilis,S.Devic,S. Dughmi,V.Sharan, and S.-H.Teng. Regularizationand optimalmulti-
classlearning. arXivpreprintarXiv:2309.13692,2023.
[5] J.Asilis,S.Devic,S.Dughmi,V.Sharan,andS.-H.Teng. Learnabilityisacompactproperty.
arXivpreprintarXiv:2402.10360,2024.
[6] I. Attias, S. Hanneke, A. Kalavasis, A. Karbasi, and G. Velegkas. Optimal learners for real-
izableregression: Paclearning and onlinelearning. arXivpreprintarXiv:2307.03848,2023.
[7] P. L. Bartlett and P. M. Long. Prediction, learning, uniform convergence, and scale-sensitive
dimensions. Journal of Computer and System Sciences, 56(2):174–190, 1998. ISSN 0022-
0000. doi: https://doi.org/10.1006/jcss.1997.1557.
[8] P. L. Bartlett, P. M. Long, and R. C. Williamson. Fat-shattering and the learnability of real-
valued functions. Journal of Computer and System Sciences, 52(3):434–452, 1996. ISSN
0022-0000. doi: https://doi.org/10.1006/jcss.1996.0033.
23[9] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Occam’s razor. Information
Processing Letters, 24(6):377–380, 1987. ISSN 0020-0190. doi: https://doi.org/10.1016/
0020-0190(87)90114-1.
[10] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Learnability and the vapnik-
chervonenkisdimension. J.ACM,36(4):929–965,oct1989. ISSN 0004-5411. doi: 10.1145/
76359.76371. URL https://doi.org/10.1145/76359.76371.
[11] O.Bousquet,S.Hanneke,S.Moran,R.vanHandel,andA.Yehudayoff. Atheoryofuniversal
learning. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Com-
puting, STOC 2021, page 532–541, New York, NY, USA, 2021. Association for Computing
Machinery. ISBN 9781450380539. doi: 10.1145/3406325.3451087.
[12] N. Brukhim, D. Carmon, I. Dinur, S. Moran, and A. Yehudayoff. A characterization of
multiclass learnability. In 2022 IEEE 63rd Annual Symposium on Foundations of Computer
Science(FOCS), pages 943–955.IEEE, 2022.
[13] A. Daniely and S. Shalev-Shwartz. Optimal learners for multiclass problems. In M. F.
Balcan, V. Feldman, and C. Szepesva´ri, editors, Proceedings of The 27th Conference on
LearningTheory,volume35ofProceedingsofMachineLearningResearch,pages287–316,
Barcelona, Spain, 13–15Jun2014. PMLR.
[14] A. Daniely, S. Sabato, S. Ben-David, and S. Shalev-Shwartz. Multiclass learnabilityand the
ermprinciple. InS.M.KakadeandU.vonLuxburg,editors,Proceedingsofthe24thAnnual
Conference on Learning Theory, volume 19 of Proceedings of Machine Learning Research,
pages 207–232,Budapest, Hungary,09–11Jun2011.PMLR.
[15] O. David, S. Moran, and A. Yehudayoff. Supervised learning through the lens of compres-
sion. Advancesin NeuralInformationProcessingSystems,29, 2016.
[16] R. M. Dudley. The sizes of compact subsets of hilbert space and continuity of gaussian
processes. JournalofFunctionalAnalysis,1:125–165,1967.
[17] S. Hanneke. The optimal sample complexity of pac learning. Journal of Machine Learning
Research,17(38):1–15,2016.
[18] D. Haussler. Decision theoretic generalizations of the pac model for neural net and other
learning applications. Information and Computation, 100(1):78–150, 1992. ISSN 0890-
5401. doi: https://doi.org/10.1016/0890-5401(92)90010-D.
[19] D.Haussler.Spherepackingnumbersforsubsetsofthebooleann-cubewithboundedvapnik-
chervonenkis dimension. Journal of Combinatorial Theory, Series A, 69(2):217–232, 1995.
ISSN 0097-3165. doi: https://doi.org/10.1016/0097-3165(95)90052-7.
[20] D.Haussler,N.Littlestone,andM.Warmuth. Predicting{0,1}-functionsonrandomlydrawn
points. Information and Computation, 115(2):248–292, 1994. ISSN 0890-5401. doi: https:
//doi.org/10.1006/inco.1994.1097.
24[21] A. Kupavskii and N. Zhivotovskiy. When are epsilon-nets small? J. Comput. Syst. Sci., 110
(C):22–36,jun2020. ISSN 0022-0000. doi: 10.1016/j.jcss.2019.12.006.
[22] W.KuszmaulandQ.Qi. Themultiplicativeversionofazuma’sinequality,withanapplication
tocontentionanalysis. arXivpreprintarXiv:2102.05077,2021.
[23] K. G. Larsen. Bagging is an optimal pac learner. In The Thirty Sixth Annual Conference on
LearningTheory, pages 450–468.PMLR, 2023.
[24] N. Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold
algorithm. In 28th Annual Symposium on Foundations of Computer Science (sfcs 1987),
pages 68–77,1987. doi: 10.1109/SFCS.1987.37.
[25] N. Littlestone. Learning Quickly When Irrelevant Attributes Abound: A New Linear-
Threshold Algorithm. Machine Learning, 2(4):285–318, Apr. 1988. ISSN 1573-
0565. doi: 10.1023/A:1022869011914. URL https://doi.org/10.1023/A:
1022869011914.
[26] B. K. Natarajan. Onlearning setsand functions. MachineLearning, 4:67–97,1989.
[27] L. Rebeschi. Advanced foundations of learning. Lecture notes on Advanced Founda-
tions of Learning, University of Oxford, Department of Statistics, 2022. Available at
https://web.archive.org/web/20231001122838/https://www.stats.
ox.ac.uk/˜rebeschi/teaching/AFoL/22/material/lecture05.pdf.
[28] B.Rubinstein,P.Bartlett,andJ.Rubinstein. Shifting,one-inclusionmistakeboundsandtight
multiclassexpectedriskbounds. InB.Scho¨lkopf,J.Platt,andT.Hoffman,editors,Advances
inNeuralInformationProcessingSystems, volume19. MITPress, 2006.
[29] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning - From Theory to
Algorithms. CambridgeUniversityPress, 2014. ISBN 978-1-10-705713-5.
[30] H. U. Simon. General bounds on the number of examples needed for learning probabilistic
concepts. JournalofComputerandSystemSciences,52(2):239–254,1996. ISSN0022-0000.
doi: https://doi.org/10.1006/jcss.1996.0019.
[31] M.Talagrand. Majorizingmeasures: Thegenericchaining. TheAnnalsofProbability,24(3):
1049–1103,1996. ISSN 00911798.
[32] L.G.Valiant. Atheoryofthelearnable. Commun.ACM,27(11):1134–1142,nov1984. ISSN
0001-0782. doi: 10.1145/1968.1972.
[33] V.N.VapnikandA.Y.Chervonenkis. Ontheuniformconvergenceofrelativefrequenciesof
eventsto theirprobabilities. Theory of Probability& Its Applications,16(2):264–280,1971.
doi: 10.1137/1116025.
[34] M. K. Warmuth. Theoptimal pac algorithm. In InternationalConference on Computational
LearningTheory, pages 641–642.Springer, 2004.
25[35] C.Wu,M.Heidari,A.Grama,andW.Szpankowski.Expectedworstcaseregretviastochastic
sequentialcovering. arXivpreprintarXiv:2209.04417,2022.
A Omitted proofs
A.1 Omitted proofs in Section 3
Claim13 (Forward MartingaleBound).
k k
δ
Pr L (h )− d > 2·k ·ǫ(n) ≤ .
D i−1 i
2
" #
i=1 i=1
X X
Proof. LetM = L (h )−d . Then,asweobservedabove,thefollowingconditionalexpecta-
i D,H i−1 i
tionevaluatestoE[M | S ] = L (h )−E[d | S ] = 0.Then,randomvariablesM satisfies
i i−1 D i−1 i i−1 i
that M ∈ [0,1] and E[M | M ,...M ] ≤ ǫ(n) where ǫ(n) is the error rate of the transductive
i i 1 i−1
learnerA .
Trans
InvokingLemma11 givesus
k ·ǫ(n)
Pr M ≥ 2·k ·ǫ(n) < exp − .
i
  3
i∈[k] (cid:18) (cid:19)
X
 
As wehavek = 3·log(2/δ),theclaimfollows.
ǫ
Claim14 (Backward MartingaleBound).
k
δ
Pr d > 2·k ·ǫ(n) ≤ .
i
2
" #
i=1
X
Proof. Recall that we think of constructing our sequence of samples S ,...,S “backwards” by
0 k
first conditioning on the (unordered) set S , then iteratively peeling off one sample at a time —
k
uniformly at random without replacement — to obtain S ,S ,...,S = S. For i = k,...,1,
k−1 k−2 0
observe that (x ,y ) is a uniformly random element from S , and that |S | ≥ n. The transductive
i i i i
errorassumptionimpliesthatLtr(A) ≤ ǫ(n) and therefore
Si
E[d | S ] ≤ ǫ.
i i
Sinced isconditionallyindependentofd ,...,d givenS , itfollowsthat
i k i+1 i
E[d | d ,...,d ] ≤ ǫ.
i k i+1
As wehaved ∈ [0,1],invokingLemma11onceagain givesus
i
k ·ǫ(n)
Pr d ≥ 2·k ·ǫ(n) ≤ exp − .
i
  3
i∈[k] (cid:18) (cid:19)
X
 
Sincek = 3·log(2/δ), theclaimfollows.
ǫ
26A.2 Omitted proofs in Section 4
Claim18 (Backward MartingaleBound II: Transductivevs DistributionalError).
k
3 δ
Pr (d −d ) > 8klog < .
i i
" s δ # 3
i=1 (cid:18) (cid:19)
X
e
Proof. Until the end of the proof we condition on samples U. Denote ∆ = d − d and ∆ =
i i i i
k ∆ . By definitionofd and d , wehave
j=i i i i
e
P ∆
i
e= [h∗ Si(x i) 6= y i]−[h∗ D(x i) 6= y i].
Since h∗ is an optimalhypothesisin H for theempirical distributionS , the predictorh∗ is in H,
Si i D
and (x ,y ) isuniformlydistributedin S , itfollowsthat
i i i
E[∆ | S ] ≤ 0.
i i
Since∆ isconditionallyindependentof∆ ,...,∆ givenS , itfollowsthat
i i+1 k i
E[∆ | ∆ ,...,∆ ] ≤ 0,
i i+1 k
orequivalently
E[∆ | ∆ ] ≤ ∆ .
i i+1 i+1
Thesequence∆ therefore formsabackwards super-martingale.
i
InvokingAzuma’sinequalitytogetherwiththefact that∆ ∈ [−1,1], weobtain
i
3
Pr ∆ > 8klog ≤ δ/3,
1
" s δ #
(cid:18) (cid:19)
as claimed.
B Selecting a Hypothesis Using A Validation Set Gives a Low
Error Hypothesis With High Probability
Lemma 21. Let H := {h ,...h } ⊆ XY bea collectionofpredictorssatisfying
0 k−1
k−1
1
Pr · LAg (h ) > ǫ ≤ δ
k D,H i
" #
i=0
X
for some ǫ,δ ∈ [0,1], and let S := (X × Y)t be a cross-validation set of size t = log(k/δ),
val ǫ2
sampledi.i.d. fromthedistributionD. Then, for thepredictorh := argmin LAg (h ), which
hi∈H Sval,H i
minimizestheagnosticlossonthecross-validationset,we have
b
Pr LAg (h) > 3ǫ ≤ 2·δ.
D,H
h i
b27Proof. Let S := (x ,y ),...,(x ,y ). Let h∗ ∈ H be the predictor with the minimum distribu-
val 1 1 t t
tional loss among predictors in H. By assumption, with probability 1 − δ, the average distribu-
tional loss of predictors h ∈ H is at mostǫ. From now on, we condition on this event and assume
that the average agnostic distributional error is at most ǫ. Therefore, optimal h∗ guarantees that
LAg (h∗) ≤ ǫ. Forh∗, defineX := ℓ(h∗(x ),y ) fori ∈ [t].
D,H i i i
UsingHoeffding’sinequalityand thefact thatX ≤ 1 foreach i ∈ [t], weobservethat
i
t
log(1/δ) 1 log(1/δ)
Pr LAg (h∗) ≥ ǫ+ = Pr X ≥ ǫ+ ≤ δ. (3)
S,H 2·t t i 2·t
" r # " r #
i=1
X
Ontheotherhand,leth ∈ H beanarbitrarypredictorwithLAg (h) ≥ ǫ+2· log(k/δ). Using
D,H 2·t
Ho¨effding’sboundagain,weobservethat q
log(k/δ) δ
Pr LAg (h) ≤ ǫ+ ≤ .
S,H 2·t k
" r #
By the union bound, we can concludethat with probabilityat least 1−δ, there is no predictor
h ∈ H withLAg (h) ≤ ǫ+ log(k/δ) and LAg (h) ≥ ǫ+2· log(k/δ). By (3), weknowthatwith
S,H 2·t D,H 2·t
probability 1 −δ, there exisq ts a predictor whose agnostic emq pirical loss is at most ǫ+ log(k/δ).
2·t
Therefore, we conclude that h, the empirically optimal predictor, attains at most ǫ+2·q log(k/δ)
2·t
agnosticdistributionalerror. q
Finally, we release the cobndition on the average distributional error being at most ǫ and con-
cludethat
log(k/δ)
Pr LAg (h) ≥ ǫ+2· ≤ 3·δ.
D,H 2·t
" r #
Theproofiscompleteas weset t =
lobg(k/δ).
ǫ2
C Connections to the results of Aden-Ali et al. [1]
In this section, we explore the connection between our work and that of Aden-Ali et al. [1]. Both
paperspresenttechniquesforconvertingtransductivelearnerstoPAClearnersintherealizableset-
ting,buttheymakedifferentassumptionsaboutthepropertiesoftheunderlyinglearners. Aden-Ali
et al. [1]train multipletransductivelearners by takingsubsamplesofthe givensampleset and ag-
gregatingtheirinformationto obtain aPAC learner. Theirwork explicitlydefines this aggregation
stepforbinaryclassification,partialconceptclassification,andboundedlossregressionproblems.
In contrast, our approach involves training multiple learners by providing additional data to each
and aggregatingtheiroutputsto form a PAC learner. Ouraggregationmethodis moregeneric and
applicableto anylearningproblemwithboundedpseudometriclossfunctions.
Both our paper and the paper of Aden-Ali et al. [1] make assumptions on the worst case per-
formance of a learner A based on thesize of the input sample. Intuitively,this paper assumes that
28whenalearnerisgivenlargersamples,itsexpectederroronthosesamplesisnon-increasing. That
is,
ǫ (n) ≤ ǫ (n−1). (A1)
Trans Trans
Incontrast,theauthorsofAden-Alietal.[1]assumethatgivensmallersamples,theexpectederror
ofalearner doesnotincrease morethan linearly. Moreformally,
nǫ (n) ≥ (n−1)ǫ (n−1). (A2)
Trans Trans
As you may notice, these two assumptions are incomparable, or equivalently, one does not imply
theother.
In this section, we will first show that A1 is without loss of generality true for transductive
learners. We then show that assuming both (A1) and (A2) hold for transductive learners, the two
resultsare equivalentuptoconstantfactors.
C.1 The two assumptions in Section 2.1 are true without loss of generality
Lemma 22. If a learner A achieves (agnostic)transductiveerrorat most ǫ(n), then thereexists a
randomizedlearnerAwith transductiveerrorrateof atmostǫ(n) suchthatǫ(n+1) ≤ ǫ(n).
e e
Proof. For any sample S ∈ (X,Y)n+1 provided for transductivelearning task. When a transduc-
e
tive learner A is tested against data point (x ,y ), A subsamples a set S′ ⊆ S of size n − 1
i i −i
uniformly random and returns the label A′(S′)(x ). We can upper-bound transductive error of A
i
as follows.
LTrans(A) = E E [ℓ(A′(S )(x ),y )]
S −i,−j i i
i∈[n] j∈[n]\{i}
(cid:20) (cid:21)
= E [LTrans(A′)]
i∈[n]
S−i
≤ E [ǫ(n)]
i∈[n]
= ǫ(n).
e
Notice that the proof holds even for agnostic transductive learning problem when we replace
e
thetransductiveerrorfunctionwiththeagnostictransductiveerrorfunction.
Moreover, for transductive learners with VC dimension d, we know that ǫ(n) = O d and
n
ǫ Ag(n) = O nd . Therefore, AssumptionA1 holdsfortheselearners as well. (cid:0) (cid:1)
Let us sa(cid:16)yqtha(cid:17)t we have a learner A which is not symmetric and attains transductive error
guarantee LTrans(A) ≤ ǫ . Wenote that wecan construct a learner A′, where A′(S) = A(π(S))
S Trans
where π is a permutation induced by sorting according to a consistent total ordering on X. Note
that because A(S) ≤ ǫ regardless of the order of S, we see that A(π(S)) ≤ ǫ as well. In
Trans Trans
addition, we can see that A′ is symmetric over input samples, as π sorts the sample consistently.
Therefore, we can conclude that if a learner A exists attaining transductiveerror rate ǫ , then a
Trans
symmetriclearnerA′ existswhichalsoattainstransductiveerror ǫ .
Trans
29C.2 Under Both of The Above Assumptions, Our Result is Equivalent to
Aden-Ali et al. [1] Up To Constant Factors
We will show that if both assumptions (A2) and (A1) holds then Theorem 3 is equivalent up
to constant factors to the main result of Aden-Ali et al. [1]. In particular, Aden-Ali et al. [1]
demonstrated that given a learner A which attains transductive error at most ǫ(n) on samples of
size n, by training on 3n subsamples of that data, they demonstrated that under (A2), there are
4
predictorsh ,...h suchthat
1 3n/4
3n/4
4 log(2/δ)
Pr · L (h ) ≥ O ǫ(n)+ ≤ δ (4)
D i
3n n 
i=0 (cid:18) (cid:19)
X
 
On theotherhand,Lemma12statesa similarprobabilisticbound:
k
1 log(2/δ)
Pr L (h ) ≥ O ǫ(n)+ ≤ δ.
D i
k k
" #
i=1 (cid:18) (cid:19)
X
Note that two claim are equivalentto each other when theadditional data size k is set to 3n in
Lemma12.
30