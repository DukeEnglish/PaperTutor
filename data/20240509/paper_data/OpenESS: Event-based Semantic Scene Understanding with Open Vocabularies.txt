OpenESS: Event-based Semantic Scene Understanding with Open Vocabularies
LingdongKong1,2 YouquanLiu3 LaiXingNg4,5 BenoitR.Cottereau5,6 WeiTsangOoi1,5
1NationalUniversityofSingapore 2CNRS@CREATE 3HochschuleBremerhaven
4InstituteforInfocommResearch,A*STAR 5IPAL,CNRSIRL2955,Singapore
6CerCo,CNRSUMR5549,Universite¬¥ ToulouseIII
https://github.com/ldkong1205/OpenESS
InputEventStream ‚Äúdriveable‚Äù(Adjective) ‚Äúcar‚Äù(Fine-Grained) ‚Äúmanmade‚Äù(Coarse)
Back
Build
Road
Car
Pole
Veg
Wall
Zero-ShotSemanticSegmentation ‚Äúwalkable‚Äù(Adjective) ‚Äúbarrier‚Äù(Fine-Grained) ‚Äúflat‚Äù(Coarse)
Figure1.Open-vocabularyevent-basedsemanticsegmentation(OpenESS).Ourframeworkiscapableofperformingzero-shotseman-
ticsegmentationofeventdatastreamswithopenvocabularies.Givenraweventsandtextpromptsasinputs,OpenESSoutputssemantically
coherentopen-worldpredictionsacrossvariousadjective,fine-grained,andcoarsecategories. Thelastthreecolumnsshowthelanguage-
guidedattentionmapswhereregionsofahighsimilarityscoretothegiventextpromptsarehighlighted.Bestviewedincolors.
Abstract tably,weachieve53.93%and43.31%mIoUonDDD17and
DSEC-Semanticwithoutusingeithereventorframelabels.
Event-based semantic segmentation (ESS) is a funda-
mentalyetchallengingtaskforeventcamerasensing. The 1.Introduction
difficulties in interpreting and annotating event data limit
its scalability. While domain adaptation from images to Event cameras, often termed bio-inspired vision sensors,
event data can help to mitigate this issue, there exist data standdistinctivelyapartfromtraditionalframe-basedcam-
representationaldifferencesthatrequireadditionaleffortto eras and are often merited by their low latency, high dy-
resolve. Inthiswork, forthefirsttime, wesynergizeinfor- namicrange,andlowpowerconsumption[28,44,76]. The
mationfromimage,text,andevent-datadomainsandintro- realmofevent-basedvisionperception,thoughnascent,has
duce OpenESS to enable scalable ESS in an open-world, rapidlyevolvedintoafocalpointofcontemporaryresearch
annotation-efficientmanner. Weachievethisgoalbytrans- [99]. Drawing parallels with frame-based perception and
ferringthesemanticallyrichCLIPknowledgefromimage- recognition methodologies, a plethora of task-specific ap-
textpairstoeventstreams. Topursuebettercross-modality plicationsleveragingeventcamerashaveburgeoned[25].
adaptation, we propose a frame-to-event contrastive dis- Event-based semantic segmentation (ESS) emerges as
tillation and a text-to-event semantic consistency regular- one of the core event perception tasks and has gained in-
ization. Experimental results on popular ESS benchmarks creasingattention[2,6,38,79].ESSinheritsthechallenges
showed our approach outperforms existing methods. No- oftraditionalimagesegmentation[11,12,19,39,58],while
1
4202
yaM
8
]VC.sc[
1v95250.5042:viXraalso contending with the unique properties of event data 2.RelatedWork
[2], which opens up a plethora of opportunities for explo-
Event-basedVision. Themicrosecond-leveltemporalres-
ration. Although accurate and efficient dense predictions
olution, high dynamic range (typically 140 dB vs. 60 dB
fromeventcamerasaredesirableforpracticalapplications,
ofstandardcameras),andpowerconsumptionefficiencyof
thelearningandannotationofthesparse,asynchronous,and
eventcamerashaveposedaparadigmshiftfromtraditional
high-temporal-resolution event streams pose several chal-
frame-based imaging [25, 60, 77, 108]. A large variety of
lenges[47,49,61].Stemmingfromtheimagesegmentation
event-based recognition, perception, localization, and re-
community,existingESSmodelsaretrainedondenselyan-
constructiontaskshavebeenestablished,encompassingob-
notated events within a fixed and limited set of label map-
ject recognition [18, 29, 48, 68], object detection [27, 31,
ping[2,79].Suchclosed-setlearningfromexpensiveanno-
103, 109], depth estimation [17, 36, 42, 62, 65, 70], opti-
tationsinevitablyconstrainsthescalabilityofESSsystems.
calflow[7,20,33,34,53,81,105],intensity-imagerecon-
An obvious approach will be to make use of the image
struction[23,24,73,98,107],visualodometryandSLAM
domain and transfer knowledge to event data for the same
[43,56,72],stereoscopicpanoramicimaging[4,75],etc.In
vision tasks. Several recent attempts [30, 61, 79] resort to
thiswork,wefocusontherecently-emergedtaskofevent-
unsuperviseddomainadaptationtoavoidtheneedforpaired
based semantic scene understanding [2, 79]. Such a pur-
imageandeventdataannotationsfortraining. Thesemeth-
suitisanticipatedtotacklesparse,asynchronous,andhigh-
ods demonstrate the potential of leveraging frame annota-
temporal-resolution events for dense predictions, which is
tions to train a segmentation model for event data. How-
crucialforsafety-criticalin-droneorin-vehicleperceptions.
ever, transferring knowledge across frames and events is
Event-basedSemanticSegmentation.ThefocusofESSis
not straightforward and requires intermediate representa-
oncategorizingeventsintosemanticclassesforenhancing
tions such as voxel grids, frame-like reconstructions, and
scene interpretation. Alonso et al. [2] contributed the first
bio-inspiredspikes. Meanwhile,itisalsocostlytoannotate
benchmarkbasedonDDD17[5].Subsequentworksaretai-
denseframelabelsfortraining,whichlimitstheirusage.
loredtoimprovetheaccuracywhilemitigatingtheneedfor
Arecenttrendinclinestotheuseofmultimodalfounda-
extensive event annotations [30]. EvDistill [84] and DTL
tionmodels[13,50,67,69,94]totraintask-specificmod-
[83] utilized aligned frames to enhance event-based learn-
elsinanopen-vocabularyandzero-shotmanner,removing
ing.EV-Transfer[61]andESS[79]leverageddomainadap-
dependenciesonhumanannotations. Thispapercontinues
tation to transfer knowledge from existing image datasets
such a trend. We propose a novel open-vocabulary frame-
to events. Recently, HALSIE [6] and HMNet [38] inno-
workforESS,aimingattransferringpre-trainedknowledge
vated ESS in cross-domain feature synthesis and memory-
frombothimageandtextdomainstolearnbetterrepresen-
based event encoding. Another line of research pursues
tationsofeventdataforthedensesceneunderstandingtask.
to use of spiking neural networks for energy-efficient ESS
Observingthelargedomaingapinbetweenheterogeneous
[10,49,63,90]. Inthiswork,differentfrompreviouspur-
inputs, wedesigntwocross-modalityrepresentationlearn-
suits, we aim to train ESS models in an annotation-free
ing objectives that gradually align the event streams with
manner by distilling pre-trained vision-language models,
images and texts. As shown in Fig. 1, given raw events
hopingtoaddressscalabilityandannotationchallenges.
andtextpromptsastheinput,thelearnedfeaturerepresen-
Open-Vocabulary Learning. Recent advances in vision-
tationsfromourOpenESSframeworkexhibitpromisingre-
language models open up new possibilities for visual per-
sults for known and unknown class segmentation and can
ceptions[13,88,106].Suchtrendsencompassimage-based
beextendedtomoreopen-endedtextssuchas‚Äúadjectives‚Äù,
zero-shotandopen-vocabularydetection[26,52,89,96],as
‚Äúfine-grained‚Äù,and‚Äúcoarse-grained‚Äùdescriptions.
wellassemantic[35,51,55,97,100],instance[45,87],and
Tosumup,thisworkposeskeycontributionsasfollows:
panoptic[21,41,93]segmentation.Asfarasweknow,only
‚Ä¢ We introduce OpenESS, a versatile event-based seman- threeworksstudiedtheadaptationofCLIPforevent-based
ticsegmentation frameworkcapable ofgenerating open- recognition. EventCLIP [92] proposed to convert events
worlddenseeventpredictionsgivenarbitrarytextqueries. to a 2D grid map and use an adapter to align event fea-
‚Ä¢ To the best of our knowledge, this work represents the tures with CLIP‚Äôs knowledge. E-CLIP [102] uses a hier-
firstattemptatdistillinglargevision-languagemodelsto archicaltriplecontrastivealignmentthatjointlyunifiesthe
assistevent-basedsemanticsceneunderstandingtasks. event,image,andtextfeatureembedding. Ev-LaFOR[18]
‚Ä¢ We propose a frame-to-event (F2E) contrastive distilla- designed category-guided attraction and category-agnostic
tionandatext-to-event(T2E)consistencyregularization repulsionlossestobridgeeventwithCLIP.Differently,we
toencourageeffectivecross-modalityknowledgetransfer. present the first attempt at adapting CLIP for dense pre-
‚Ä¢ Ourapproachsetsupanewstateoftheartinannotation- dictions on sparse and asynchronous event streams. Our
free, annotation-efficient, and fully-supervised ESS set- work is also close to superpixel-driven contrastive learn-
tingsonDDD17-SegandDSEC-Semanticbenchmarks. ing [46, 74], where pre-processed superpixels are used to
2Calibration Input Encoding Grouping Decoding Contrastive Prompt
‚Äúroad‚Äù
‚Äúsidewalk‚Äù
‚Äúbuilding‚Äù
ùêº%&‚Äô ‚Ñ± !( #)%* ùê°$-$ ‚Ä¶
ùêü%&‚Äô ‚Ñ±$-$
!$
ùêü"#$
ùêº+* ‚Ñ±%&‚Äô ùí´%&‚Äô
!" ,"
ùê°"#$ ‚Ä¶
ùë•
ùë¶ ‚Äúdriveable‚Äù
‚Äúwalkable‚Äù
Time ùêº"#$ ‚Ñ± !" !#$ ùí´ ," !#$ ‚Äúmanmade‚Äù
Figure2.ArchitectureoverviewoftheOpenESSframework.Wedistilloff-the-shelfknowledgefromvision-languagesmodelstoevent
representations(cf.Sec.3.1).GivenacalibratedeventIevtandaframeIimg,weextracttheirfeaturesfromtheeventnetworkFevtandthe
Œ∏e
densifiedCLIP‚ÄôsimageencoderFclip,whicharethencombinedwiththetextembeddingfromCLIP‚ÄôstextencoderFtxtforopen-world
Œ∏c Œ∏t
prediction(cf.Sec.3.2). Tobetterserveforcross-modalityknowledgetransfer,weproposeaframe-to-event(F2E)contrastiveobjective
(cf.Sec.3.3)viasuperpixel-drivendistillationandatext-to-event(T2E)consistencyobjective(cf.Sec.3.4)viascene-levelregularization.
establish contrastive objectives with modalities from other textualdescriptionswithoutspecifictrainingonthosecate-
tasks, e.g., point cloudunderstanding[57], remote sensing gories. To achieve annotation-free classification on a cus-
[37], medical imaging [82], and so on. In this work, we tom dataset, one needs to combine class label mappings
proposeOpenESStoexploresuperpixel-to-eventrepresen- withhand-craftedtextpromptsastheinputtogeneratethe
tation learning. Extensive experiments verify that such an textembedding.Inthiswork,weaimtoleveragetheseman-
approachispromisingforannotation-efficientESS. tically rich CLIP feature space to assist open-vocabulary
densepredictiononsparseandasynchronouseventstreams.
3.Methodology
3.2.Open-VocabularyESS
Our study serves as an early attempt at leveraging vision-
languagefoundationmodelslikeCLIP[69]tolearnmean- Inputs. Given a set of N event data acquired by an event
ingfuleventrepresentationswithoutaccessingground-truth camera, we aim to segment each event e among the tem-
i
labels. WestartwithabriefintroductionoftheCLIPmodel porally ordered event streams Œµ , which are encoded by
i
(cf.Sec.3.1),followedbyadetailedelaborationonourpro- thepixelcoordinates(x ,y ),microsecond-leveltimestamp
i i
posedopen-vocabularyESS(cf.Sec.3.2).Toencourageef- t , and the polarity p ‚àà {‚àí1,+1} which indicates ei-
i i
fectivecross-modaleventrepresentationlearning,weintro- ther an increase or decrease of the brightness. Each event
duce a frame-to-event contrastive distillation (cf. Sec. 3.3) camera pixel generates a spike whenever it perceives a
andatext-to-eventconsistencyregularization(cf.Sec.3.4). changeinlogarithmicbrightnessthatsurpassesapredeter-
AnoverviewoftheOpenESSframeworkisshowninFig.2. mined threshold. Meanwhile, a conventional camera cap-
tures gray-scale or color frames Iimg ‚àà R3√óH√óW which
3.1.RevisitingCLIP i
arespatiallyalignedandtemporallysynchronizedwiththe
CLIP [69] learns to associate images with textual descrip- eventsorcanbealignedandsynchronizedtoeventsviasen-
tionsthroughacontrastivelearningframework.Itleverages sorcalibration,whereH andW arethespatialresolutions.
a dataset of 400 million image-text pairs, training an im- Event Representations. Due to the sparsity, high tempo-
ageencoder(basedonaResNet[39]orVisionTransformer ralresolution,andasynchronousnatureofeventstreams,it
[22]) and a text encoder (using a Transformer architecture iscommontoconvertraweventsŒµ intomoreregularrep-
i
[80]) to project images and texts into a shared embedding resentations Ievt ‚àà RC√óH√óW as the input to the neural
i
space. Such a training paradigm enables CLIP to perform network [25], where C denotes the number of embedding
zero-shot classification tasks, identifying images based on channels which is depended on the event representations
3
Dense
Group
Group
F2E T2Ethemselves. Some popular choices of such embedding in- thus becomes crucial to learn meaningful representations
cludespatiotemporalvoxelgrids[29,104,105],frame-like for both thing and stuff classes, especially their boundary
reconstructions [73], and bio-inspired spikes [49]. We in- information. However, the sparsity and asynchronous na-
vestigatethesethreemethodsandshowanexampleoftak- tureofeventstreamsinevitablyimpedesuchobjectives.
ing voxel grids as the input in Fig. 2. More analyses and Superpixel-Driven Knowledge Distillation. To pursue a
comparisons using reconstructions and spikes are in later more informative event representation learning at higher
sections. Specifically, withapredefinednumberofevents, granularity, we propose to first leverage calibrated frames
eachvoxelgridisbuiltfromnon-overlappingwindowsas: togeneratecoarse,instance-levelsuperpixelsandthendis-
till knowledge from a pre-trained image backbone to the
(cid:88)
Ievt = p Œ¥(x ‚àíx)Œ¥(y ‚àíy)max{1‚àí|t‚àó‚àít|,0}, eventsegmentationnetwork. Superpixelgroupspixelsinto
i j j j j
ej‚ààŒµi conceptuallymeaningfulatomicregions,whichcanbeused
(1) as the basis for higher-level perceptions [1, 54, 85]. The
whereŒ¥istheKroneckerdeltafunction;t‚àó =(B‚àí1)tj‚àít0 semantically coherent frame-to-event correspondences can
j ‚àÜT
isthenormalizedeventtimestampwithBasthenumberof thus be found using pre-processed or online-generated su-
temporal bins in an event stream; ‚àÜT is the time window perpixels. Such correspondences tend to bridge the sparse
andt 0denotesthetimeofthefirsteventinthewindow. events to dense frame pixels in a holistic manner without
Cross-Modality Encoding. Let Fevt : RC√óH√óW (cid:55)‚Üí involvingextratrainingorannotationefforts.
Œ∏e
RD1√óH1√óW1 beanevent-basedsegmentationnetworkwith Superpixel & Superevent Generation. We resort to the
trainable parameters Œ∏ , which takes as input an event followingtwowaysofgeneratingthesuperpixels. Thefirst
e
embedding Ievt and outputs a D -dimensional feature of wayistoleverageheuristicmethods,e.g.SLIC[1],toeffi-
i 1
downsampled spatial sizes H 1 and W 1. Meanwhile, we cientlygroupspixelsfromframeI iimg intoatotalofM slic
integrate CLIP‚Äôs image encoder Fclip : R3√óH√óW (cid:55)‚Üí segments with good boundary adherence and regularity as
RD2√óH2√óW2 into our framework anŒ∏ dc keep the parameters I isp = {I i1,I i2,...,I iMslic},whereM slic isahyperparame-
Œ∏ fixed. The output is a D -dimensional feature of sizes terthatneedstobeadjustedbasedontheinputs.Thegener-
c 2
H
2
and W 2. Our motivation is to transfer general knowl- atedsuperpixelssatisfyI i1‚à™I i2‚à™...‚à™I iMslic ={1,2,...,H√ó
edge from Fclip to Fevt, such that the event branch can W}.Forthesecondoption,weusetherecentSegmentAny-
learn useful rŒ∏ ec presentaŒ∏ te ions without using dense event an- thingModel(SAM)[50]whichtakesI iimg astheinputand
notations. Toenableopen-vocabularyESSpredictions,we outputsM samclass-agnosticmasks. Forsimplicity,weuse
leverageCLIP‚ÄôstextencoderFtxt withpre-trainedparam- M todenotethenumberofsuperpixelsusedduringknowl-
pet re or ms pŒ∏ tt. temT ph le ati en sp au nt do tf hF eŒ∏ ot tx ut tpc uŒ∏ o tt m we is llf bro em
a
tp er xe td ee mfin be ed ddt ie nx gt e ad ng de shd ois wtil mlat oi ro en, coi. me., pa{ rI ii ss op n=
s
b{ etI wi1 e, e.. n., SI Lik I} C|k [1=
]
a1 n, d... S, AM M}
extractedfromCLIP‚Äôsrichsemanticspace. [50]inlatersections.SinceI ievtandI iimghavebeenaligned
Densifications. CLIP was originally designed for image- and synchronized, we can group events from I ievt into su-
basedrecognitiontasksanddoesnotprovideper-pixelout- perevents{V isp = {V i1,...,V il}|l = 1,...,M}byusingthe
putsfordensepredictions.Severalrecentattemptsexplored knownevent-pixelcorrespondences.
theadaptationfromglobal,image-levelrecognitiontolocal, Frame-to-EventContrastiveLearning.Toencouragebet-
pixel-level prediction, via either model structure modifica- tersuperpixel-levelknowledgetransfer, weleverageapre-
tion [100] or fine-tuning [51, 71, 97]. The former directly trained image network F Œ∏i fmg : R3√óH√óW (cid:55)‚Üí RD3√óH3√óW3
reformulates the value-embedding layer in CLIP‚Äôs image as the teacher and distill information from it to the event
encoder, while the latter uses semantic labels to gradually branch F Œ∏e evt. The parameters of F Œ∏i fmg, which can come
adaptthepre-trainedweightstogeneratedensepredictions. fromeitherCLIP[69]orotherpretexttaskpre-trainedback-
Inthiswork,weimplementbothsolutionstodensifyCLIP‚Äôs bonessuchas[8,15,64],arekeptfrozenduringthedistilla-
outputsandcomparetheirperformancesinourexperiments. tion. WithF Œ∏e evt andF Œ∏i fmg,wegeneratethesupereventand
superpixelfeaturesasfollows:
Up until now, we have presented a preliminary frame-
workcapableofconductingopen-vocabularyESSbylever- 1 (cid:88)
aging knowledge from the CLIP model. However, due to f ievt = |Vsp| P œâev et(F Œ∏e evt(I ievt) l ), (2)
thelargedomaingapbetweentheeventandimagemodali- i l‚ààVsp
i
ties,ana¬®ƒ±veadaptationissub-parintacklingthechallenging 1 (cid:88)
fimg = Pimg (Fimg (Iimg) ), (3)
event-basedsemanticsceneunderstandingtask. i |Isp| œâf Œ∏f i k
i k‚ààIsp
i
3.3.F2E:Frame-to-EventContrastiveDistillation
where Pevt and Pimg are projection layers with trainable
œâe œâf
Sinceourobjectiveistoencourageeffectivecross-modality parameters œâ and œâ , respectively, for the event branch
e f
knowledge transfer for holistic event scene perception, it and frame branch. In the actual implementation, Pevt and
œâe
4Pimg consist of linear layers which map the D - and D - L (Œ∏ ,œâ )= (6)
œâf 1 3 T2E e q
d Ti hm ee fn os li lo on wa il ne gv ce on nt ta ran sd tivfr eam lee arf ne ia nt gur oe bs jeto ctit vh ee is sam ape pls ih edap te o. ‚àí(cid:88)Z log(cid:34) (cid:80)
Ti‚ààz,I
ievte(‚ü®he ivt,ht ixt‚ü©/œÑ2) (cid:35)
, (7)
theeventpredictionandtheframeprediction:
z=1
(cid:80)
jÃ∏=i,Ti‚ààz,TiÃ∏‚ààI
ievte(‚ü®he jvt,ht ixt‚ü©/œÑ2)
Ô£Æ Ô£π
(cid:88) e(‚ü®f ievt,f iimg‚ü©/œÑ1) whereœÑ 2 > 0isatemperaturecoefficientthatcontrolsthe
L F2E(Œ∏ e,œâ e,œâ f)=‚àí
i
logÔ£∞
(cid:80) jÃ∏=ie(‚ü®f ievt,f
jimg‚ü©/œÑ1)Ô£ª , p jea cc te iveof ok fn oo uw rle Od pg ee nt Era Sn Ssfe frr a. mT eh we oo rkve ir sal tl oo mpt ii nm imiza izti eon Lob =-
(4) L +Œ±L ,whereŒ±isaweightbalancingcoefficient.
F2E T2E
Role in Our Framework. Our T2E semantic consistency
where ‚ü®¬∑,¬∑‚ü© denotes the scalar product between the su-
regularizationprovidesaglobal-levelalignmenttocompen-
pereventandsuperpixelembedding;œÑ >0isatemperature
1 sate for the possible self-conflict in the superpixel-driven
coefficientthatcontrolsthepaceofknowledgetransfer.
frame-to-eventcontrastivelearning. Aswewillshowinthe
RoleinOurFramework. OurF2Econtrastivedistillation
followingsections, thetwoobjectivesworksynergistically
establishesaneffectivepipelinefortransferringsuperpixel-
inimprovingtheperformanceofopen-vocabularyESS.
levelknowledgefromdense,visualinformativeframepix-
Inference-TimeConfiguration. OurOpenESSframework
elstosparse,irregulareventstreams.Sincewearetargeting
isdesignedtopursuesegmentationaccuracyinannotation-
thesemanticsegmentationtask,thelearnedeventrepresen-
free and annotation-efficient manners, without sacrificing
tations should be able to reason in terms of instances and
event processing efficiency. As can be seen from Fig. 2,
instancepartsatandinbetweensemanticboundaries.
afterthecross-modalityknowledgetransfer,onlytheevent
3.4.T2E:Text-to-EventConsistencyRegularization branch will be kept. This guarantees that there will be no
extralatencyorpowerconsumptionaddedduringtheinfer-
Although the aforementioned frame-to-event knowledge
ence,whichisinlinewiththepracticalrequirements.
transferprovidesasimpleyeteffectivewayoftransferring
off-the-shelf knowledge from frames to events, the opti- 4.Experiments
mizationobjectivemightencounterunwantedconflicts.
Intra-ClassOptimizationConflict.Duringthemodelpre- 4.1.Settings
training,thesuperpixel-drivencontrastivelosstakesthecor-
Datasets. We conduct experiments on two popular ESS
respondingsupereventandsuperpixelpairinabatchasthe
datasets. DDD17-Seg[2]isawidelyusedESSbenchmark
positivepair, whiletreatingallremainingpairsasnegative
consisting of 40 sequences acquired by a DAVIS346B. In
samples. Sinceheuristicsuperpixelsonlyprovideacoarse
total,15950trainingand3890testingeventsofspatialsize
grouping of conceptually coherent segments (kindly refer
352 √ó 200 are used, along with synchronized gray-scale
to our Appendix for more detailed analysis), it is thus in-
frames provided by the DAVIS camera. DSEC-Semantic
evitable to encounter self-conflict during the optimization.
[79]providessemanticlabelsfor11sequencesintheDSEC
That is to say, from hindsight, there is a chance that the
[32] dataset. The training and testing splits contain 8082
superpixels belonging to the same semantic class could be
and2809eventsofspatialsize640√ó440,accompaniedby
involvedinbothpositiveandnegativesamples.
colorframes(withsensorcalibrationparametersavailable)
Text-Guided Semantic Regularization. To mitigate the
recordedat20Hz. MoredetailsareintheAppendix.
possibleself-conflictinEq.(4),weproposeatext-to-event
Benchmark Setup. In addition to the conventional fully-
semantic consistency regularization mechanism that lever-
supervisedESS,weestablishtwoopen-vocabularyESSset-
ages CLIP‚Äôs text encoder to generate semantically more
tings for annotation-free and annotation-efficient learning,
consistenttext-framepairs{Iimg,T },whereT denotesthe
i i i respectively. TheformeraimstotrainanESSmodelwith-
textembeddingextractedfromFtxt.Suchapairedrelation-
Œ∏t out using any dense event labels, while the latter assumes
shipcanbeleveragedviaCLIPwithoutadditionaltraining.
anannotationbudgetof1%,5%,10%,or20%ofeventsin
We then construct event-text pairs {Ievt,T } by propagat-
i i the training set. We treat the first few samples from each
ingthealignmentbetweeneventsandframes. Specifically,
sequenceaslabeledandtheremainingonesasunlabeled.
the paired event and text features are extracted as follows:
Implementation Details. Our framework is implemented
using PyTorch [66]. Based on the use of event represen-
hevt =Qevt(Fevt(Ievt)), htxt =Ftxt(T ), (5) tations, we form frame2voxel, frame2recon, and
i œâq Œ∏e i i Œ∏t i
frame2spikesettings,wheretheeventbranchwilladopt
where Qevt is a projection layer with trainable parameters E2VID[73],ResNet-50[39],andSpikingFCN[49],respec-
œâq
œâ ,whichissimilartothatofPevt. Nowassumethereare tively,withanAdamW[59]optimizerwithcosinelearning
q œâe
atotalofZclassesintheeventdataset,thefollowingobjec- ratescheduler.Theframebranchusesapre-trainedResNet-
tiveisappliedtoencouragetheconsistencyregularization: 50[8,9,15]andiskeptfrozen. Thenumberofsuperpixels
5Table 1. Comparative study of existing ESS approaches under
25(SAM) 37.0 36.5 36.1
the annotation-free, fully-supervised, and open-vocabulary ESS 25(SLIC) 35.1 34.5 35.2
settings, respectively, onthetest setsoftheDDD17-Seg[5]and
DSEC-Semantic[79]datasets. Allscoresareinpercentage(%). 50(SAM) 38.3 38.1 38.3
Thebestscorefromeachlearningsettingishighlightedinbold. 50(SLIC) 35.7 36.1 35.4
100(SAM) 38.3 37.1 37.9
DDD17 DSEC
Method Venue 100(SLIC) 35.9 35.7 35.6
Acc mIoU Acc mIoU
150(SAM) 34.1 33.7 34.2
Annotation-FreeESS
MaskCLIP[100] ECCV‚Äô22 81.29 31.90 58.96 21.97 150(SLIC) 32.1 32.3 32.8
FC-CLIP[97] NeurIPS‚Äô23 88.66 51.12 79.20 39.42
200(SAM) 32.9 31.1 32.6
OpenESS Ours 90.51 53.93 86.18 43.31
200(SLIC) 31.7 30.1 31.1
Fully-SupervisedESS
Ev-SegNet[2] CVPRW‚Äô19 89.76 54.81 88.61 51.76 29 33 37 41 29 33 37 41 29 33 37 41
E2VID[73] TPAMI‚Äô19 85.84 48.47 80.06 44.08 Figure3. Ablationstudyonthenumberofsuperpixels(provided
Vid2E[30] CVPR‚Äô20 90.19 56.01 - -
byeitherSAM[50]orSLIC[1])involvedincalculatingtheframe-
EVDistill[84] CVPR‚Äô21 - 58.02 - -
DTL[83] ICCV‚Äô21 - 58.80 - - to-eventcontrastiveloss. Modelsafterpre-trainingarefine-tuned
PVT-FPN[86] ICCV‚Äô21 94.28 53.89 - - with1%annotations.AllmIoUscoresareinpercentage(%).
SpikingFCN[49] NCE‚Äô22 - 34.20 - -
EV-Transfer[61] RA-L‚Äô22 51.90 15.52 63.00 24.37
ESS[79] ECCV‚Äô22 88.43 53.09 84.17 45.38
previously best-performing methods, OpenESS is 1.63%
ESS-Sup[79] ECCV‚Äô22 91.08 61.37 89.37 53.29
P2T-FPN[91] TPAMI‚Äô23 94.57 54.64 - - and 2.21% better in terms of mIoU scores on DDD17-Seg
EvSegformer[47] TIP‚Äô23 94.72 54.41 - - [2]andDSEC-Semantic[79],respectively. Itisworthmen-
HMNet-B[38] CVPR‚Äô23 - - 88.70 51.20 tioning that in addition to the performance improvements,
HMNet-L[38] CVPR‚Äô23 - - 89.80 55.00
ourapproachcangenerateopen-vocabularypredictionsthat
HALSIE[6] WACV‚Äô24 92.50 60.66 89.01 52.43
arebeyondtheclosedsetsofpredictionsofexistingmeth-
Open-VocabularyESS
ods,whichismoreinlinewiththepracticalusage.
MaskCLIP[100] ECCV‚Äô22 90.50 61.27 89.81 55.01
FC-CLIP[97] NeurIPS‚Äô23 90.68 62.01 89.97 55.67 Annotation-Efficient Learning. We establish a compre-
OpenESS Ours 91.05 63.00 90.21 57.21 hensive benchmark for ESS under limited annotation sce-
narios and show the results in Tab. 3. As can be seen, the
proposedOpenESScontributessignificantperformanceim-
involvedinthecalculationofF2Econtrastivelossissetto provementsoverrandominitializationunderlinearprobing,
100 for DSEC-Semantic [79] and 25 for DDD17-Seg [2]. few-shotfine-tuning,andfully-supervisedlearningsettings.
For evaluation, we extract the feature embedding for each Specifically,usingeithervoxelgridoreventreconstruction
text prompt offline from a frozen CLIP text encoder using representation,ourapproachachieves>30%relativegains
pre-defined templates. For linear probing, the pre-trained in mIoU on both datasets under liner probing and around
eventnetworkF Œ∏e evt iskeptfrozen, followedbyatrainable 2%higherthanpriorartinmIoUwithfullsupervisions.We
point-wise linear classification head. Due to space limits, also observe that using voxel grids to represent raw event
kindlyrefertoourAppendixforadditionaldetails. streamstendstoyieldoverallbetterESSperformance.
Qualitative Assessment. Fig. 4 provides visual compar-
4.2.ComparativeStudy
isons between OpenESS and other approaches on DSEC-
Annotation-Free ESS. In Tab. 1, we compare OpenESS Semantic[79]. WefindthatOpenESStendstopredictmore
with MaskCLIP [100] and FC-CLIP [97] in the absence consistent semantic information from sparse and irregular
of event labels. Our approach achieves zero-shot ESS eventinputs,especiallyatinstanceboundaries. Weinclude
results of 53.93% and 43.31% on DDD17-Seg [2] and morevisualexamplesandfailurecasesintheAppendix.
DSEC-Semantic [79], much higher than the two competi- Open-World Predictions. One of the core advantages of
tors and even comparable to some fully-supervised meth- OpenESSistheabilitytopredictbeyondthefixedlabelset
ods. This validates the effectiveness of conducting ESS in fromtheoriginaltrainingsets. AsshowninFig.1,ourap-
anannotation-freemannerforpracticalusage. Meanwhile, proach can take arbitrary text prompts as inputs and gen-
weobservethatafine-tunedCLIPencoder[97]couldgen- eratesemanticallycoherenteventpredictionswithoutusing
erate much better semantic predictions than the structure eventlabels.Thisiscreditedtothealignmentbetweenevent
adaptationmethod[100],asmentionedinSec.3.2. featuresandCLIP‚ÄôsknowledgeinT2E.Suchaflexibleway
ComparisonstoState-of-the-ArtMethods. Asshownin ofpredictionenablesamoreholisticeventunderstanding.
Tab.1,theproposedOpenESSsetsupseveralnewstate-of- Other Representation Learning Approaches. In Tab. 2,
the-artresultsinthetwoESSbenchmarks.Comparedtothe wecompareOpenESSwithrecentreconstruction-based[3,
6
DINO
MoCoV2
SwAVBackground Building Fence Person Pole Road Sidewalk Vegetation Car Wall Traffic-Sign
Event Reconstruction MaskCLIP ESS-Sup FC-CLIP OpenESS GT
Figure4. Qualitativecomparisonsofstate-of-the-artESSapproachesonthetestsetofDSEC-Semantic[79]. Eachcolorcorrespondsto
adistinctsemanticcategory.GTdenotesthegroundtruthsemanticmaps.Bestviewedincolorsandzoomed-inforadditionaldetails.
Table2. Comparativestudyofdifferentrepresentationlearning 55 mIoU (%) 58 mIoU (%)
methods applied on event data. OV denotes whether supporting ID 48.94 49.74 55.66 56.07
open-vocabularypredictions. AllmIoUscoresareinpercentage 50 56 ID 55.11
45.58 55.70
(%).Thebestscorefromeachdatasetishighlightedinbold. 45 43.17 42.53 54 55.02
52.02 53.72 54.05
Method Venue Backbone OV DDD17 DSEC 40 OOD 38.90 52 53.02
OOD 52.03
Random - ViT-S/16 ‚úó 48.76 40.53 34.77 39.25
35 50
Mo IC Bo oV T3 [1[1 06 1] ] I IC CC LV R‚Äô‚Äô 22 21 V Vi iT T- -S S/ /1 16 6 ‚úó ‚úó 5 43 9. .6 95 4 4 49 2. .2 51 3 30 28.90 30.42 34.11 48 49.89 Random
ECDP[95] ICCV‚Äô23 ViT-S/16 ‚úó 54.66 47.91 Random
Random - ViT-B/16 ‚úó 43.89 38.24 25 46
MB Ae EiT [4[ 03 ]] CI VCL PRR ‚Äô‚Äô 22 22 V Vi iT T- -B B/ /1 16 6 ‚úó ‚úó 5 52 2. .3 39 6 4 46 7. .5 52 6 20 23.95 DSEC-Semantic 44 45.30 DDD17-Seg
Random - ResNet-50 ‚úó 56.96 57.60 1% 5% 10% 20% 1% 5% 10% 20%
SimCLR[14] ICML‚Äô20 ResNet-50 ‚úó 57.22 59.06 Figure5. Cross-datasetrepresentationlearningresultsofcom-
ECDP[95] ICCV‚Äô23 ResNet-50 ‚úó 59.15 59.16
paring OpenESS pre-training using in-distribution (ID) and out-
Random - ResNet-50 ‚úó 55.56 52.86 of-distribution (OOD) data in-between the DDD17-Seg [5] and
OpenESS Ours ResNet-50 ‚úì 57.01 55.01
DSEC-Semantic[79]datasets. Modelsafterpre-trainingarefine-
Random - E2VID ‚úó 61.06 54.96
OpenESS Ours E2VID ‚úì 63.00 57.21 tunedwith1%,5%,10%,and20%annotations,respectively.
40, 95, 101] and contrastive learning-based [14, 16] pre- bothF2EandT2Econtributetoanovertimprovementover
training methods. As can be seen, the proposed OpenESS random initialization under linear probing and few-shot
achievescompetitiveresultsoverexistingapproaches. Itis fine-tuningsettings, whichverifiestheeffectivenessofour
worth highlighting again that our framework distinct from proposedapproach.Onceagain,wefindthatthevoxelgrids
priorartsbysupportingopen-vocabularylearning. tend to achieve better performance than other representa-
tions. The spike-based methods [49], albeit being compu-
4.3.AblationStudy
tationally more efficient, show sub-par performance com-
paredtovoxelgridsandreconstructions.
Cross-Modality Representation Learning. Tab. 4 pro-
videsacomprehensiveablationstudyontheframe-to-event Superpixel Generation. We studythe utilization of SLIC
(F2E) and text-to-event (T2E) learning objectives in Ope- [1]andSAM[50]inourframe-to-eventcontrastivedistilla-
nESS using three event representations. We observe that tionandshowtheresultsinFig.3. Usingeitherframenet-
7Table3. Comparativestudyofdifferentopen-vocabularysemanticsegmentationmethods[97,100]underthelinearprobing(LP)and
few-shot fine-tuning, and full supervision (Full) settings, respectively, on the test sets of the DDD17-Seg [5] and DSEC-Semantic [79]
datasets.AllmIoUscoresaregiveninpercentage(%).ThebestmIoUscoresfromeachlearningconfigurationarehighlightedinbold.
DSEC-Semantic DDD17-Seg
Method Configuration
LP 1% 5% 10% 20% Full LP 1% 5% 10% 20% Full
Random VoxelGrid 6.70 26.62 31.22 33.67 41.31 54.96 12.30 52.13 54.87 58.66 59.52 61.06
MaskCLIP[100] 33.08 33.89 37.03 38.83 42.40 55.01 31.91 53.91 56.27 59.32 59.97 61.27
FC-CLIP[97] VoxelGrid 43.00 39.12 43.71 44.09 47.77 55.67 54.07 56.38 58.50 60.05 60.85 62.01
OpenESS(Ours) frame2voxel 44.26 41.41 44.97 46.25 48.28 57.21 55.61 57.58 59.07 61.03 61.78 63.00
Improve‚Üë +33.56 +14.79 +13.75 +12.58 +6.97 +2.25 +43.31 +5.45 +4.20 +2.37 +2.26 +1.94
Random Reconstruction 6.22 23.95 30.42 34.11 39.25 52.86 13.89 45.30 52.03 53.02 54.05 55.56
MaskCLIP[100] 27.09 30.73 36.33 40.13 43.37 52.97 29.81 49.02 53.65 54.11 54.75 56.12
FC-CLIP[97] Reconstruction 40.08 38.99 43.34 45.35 47.18 53.05 52.17 51.01 54.09 54.99 55.05 56.34
OpenESS(Ours) frame2recon 44.08 43.17 45.58 48.94 49.74 55.01 53.61 52.02 55.11 55.66 56.07 57.01
Improve‚Üë +37.86 +19.22 +15.16 +14.83 +10.49 +2.15 +39.72 +6.72 +3.08 +2.64 +2.02 +1.45
Table4. AblationstudyofOpenESSunderlinearprobing(LP) 70 mIoU (%)
and few-shot fine-tuning settings from three learning configura- 56
tionsonthetestsetofDDD17-Seg[5].F2Edenotestheframe-to- 42
eventcontrastivelearning.T2Edenotesthetext-to-eventsemantic
28
regularization.AllmIoUscoresaregiveninpercentage(%).
14
DDD17-Seg 0
Configuration F2E T2E
LP 1% 5% 10% 20% LP 1% 5% 10% 20%
VoxelGrid Random 12.30 52.13 54.87 58.66 59.52 Figure 6. Single-modality OpenESS representation learning
study on the DSEC-Semantic [79] dataset. The results are from
‚úì 52.60 55.41 57.07 59.77 60.21
frame2voxel ‚úì 54.11 56.77 58.95 60.12 60.99
modelsofrandominitialization(‚ñ°‚ñ†),recon2voxelpre-training
‚úì ‚úì 55.61 57.58 59.07 61.03 61.78 (‚ñ°‚ñ†),andframe2voxelpre-training(‚ñ°‚ñ†),respectively,afterlin-
earprobing(LP)andannotation-efficientfine-tuning.
Reconstruction Random 13.89 45.30 52.03 53.02 54.05
‚úì 50.21 50.96 53.67 54.21 54.92
frame2recon ‚úì 52.62 51.63 54.27 55.00 55.17 Framework with Event Camera Only. Lastly, we study
‚úì ‚úì 53.61 52.02 55.11 55.66 56.07
thescenariowheretheframecamerabecomesunavailable.
Spike Random 12.04 10.01 20.02 25.81 26.03 Wereplacetheinputtotheframebranchwitheventrecon-
‚úì 15.07 14.31 21.77 26.89 27.07 structions[73]andshowtheresultsinFig.6. Sincethelim-
frame2spike ‚úì 16.11 14.67 22.61 27.97 29.01 itedvisualcuesfromthereconstructiontendtodegradethe
‚úì ‚úì 16.27 14.89 23.54 28.51 29.98
quality of representation learning, its performance is sub-
parcomparedtotheframe-basedknowledgetransfer.
works pre-trained by DINO [9], MoCoV2 [15], or SwAV 5.Conclusion
[8], the SAM-generated superpixels consistently exhibit
better performance for event representation learning. The Inthiswork,weintroducedOpenESS,anopen-vocabulary
number of superpixels involved in calculating tends to af- event-based semantic segmentation framework tailored to
fecttheeffectivenessofcontrastivelearning. Apreliminary perform open-vocabulary ESS in an annotation-efficient
search to determine this hyperparameter is required. We manner. We proposed to encourage cross-modality repre-
empirically findthat setting M to 100 for DSEC-Semantic sentationlearningbetweeneventsandframesusingframe-
[79] and 25 for DDD17-Seg [2] will likely yield the best to-event contrastive distillation and text-to-event semantic
possiblesegmentationperformanceinourframework. consistencyregularization. Throughextensiveexperiments,
Cross-DatasetKnowledgeTransfer. Sincewearetarget- wevalidatedtheeffectivenessofOpenESSintacklingdense
ing annotation-free representation learning, it is thus intu- event-basedpredictions.Wehopethisworkcouldshedlight
itivetoseethecross-datasetadaptationeffect. Asshownin onthefuturedevelopmentofmorescalableESSsystems.
Fig.5,pre-trainingonOODdatasetsalsobringsappealing Acknowledgement.ThisworkisundertheprogrammeDesCartes
improvementsovertherandominitializationbaseline. This andissupportedbytheNationalResearchFoundation,PrimeMin-
result highlights the importance of conducting representa- ister‚Äôs Office, Singapore, under its Campus for Research Excel-
tionlearningforaneffectivetransfertodownstreamtasks. lenceandTechnologicalEnterprise(CREATE)programme.
8
07.6
50.01
62.44
26.62 48.72
14.14
22.13 97.23
79.44
76.33 12.43
52.64 13.14 31.24 82.84Appendix Conduct a post-processing step on the generated pseudo
labels,includingclassmergingandimagecropping. The
dataset specification is shown in Tab. 5. In total, there
A.AdditionalImplementationDetails 9
are15950trainingand3890testsamplesintheDDD17-
A.1.Datasets . . . . . . . . . . . . . . . . . . . . 9
Seg dataset. Each pixel is labeled across six seman-
A.2.TextPrompts . . . . . . . . . . . . . . . . . 9
tic classes, including flat, background, object,
A.3.Superpixels . . . . . . . . . . . . . . . . . . 10
vegetation, human, and vehicle. For each sam-
A.4.Backbones . . . . . . . . . . . . . . . . . . 14
ple, we convert the event streams into a sequence of 20
A.5.EvaluationConfiguration . . . . . . . . . . . 14
voxel grids, each consisting of 32000 events and with a
B.AdditionalExperimentalResults 16 spatialresolutionof352√ó200. Foradditionaldetailsof
B.1.Annotation-FreeESS . . . . . . . . . . . . . 16 this dataset, kindly refer to http://sensors.ini.
B.2.Annotation-EfficientESS . . . . . . . . . . 16
uzh.ch/news_page/DDD17.html.
‚Ä¢ DSEC-Semantic[79]isasemanticsegmentationexten-
C.QualitativeAssessment 16 sionoftheDSEC(DrivingStereoEventCamera)dataset
C.1.Open-VocabularyExamples . . . . . . . . . 16 [32].DSECisanextensivedatasetdesignedforadvanced
C.2.VisualComparisons . . . . . . . . . . . . . 16 driver-assistancesystems(ADAS)andautonomousdriv-
C.3.FailureCases . . . . . . . . . . . . . . . . . 16 ingresearch,withaparticularfocusonevent-basedvision
C.4.VideoDemos . . . . . . . . . . . . . . . . . 17 andstereovision. DifferentfromDDD17[5],theDSEC
datasetcombinesdatafromevent-basedcamerasandtra-
D.BroaderImpact 17 ditional RGB cameras. The inclusion of event-based
D.1.PositiveSocietalinfluence . . . . . . . . . . 17 cameras(whichcapturechangesinlightintensity)along-
D.2.PotentialLimitation . . . . . . . . . . . . . 18 sideregularcamerasprovidesarich,complementarydata
sourceforperceptiontasks.Thedatasettypicallyfeatures
E.PublicResourcesUsed 18
high-resolutionimagesandeventdata,providingdetailed
E.1.PublicDatasetsUsed . . . . . . . . . . . . . 18 visual information from a wide range of driving condi-
E.2.PublicImplementationsUsed . . . . . . . . 18 tions, including urban, suburban, and highway environ-
ments,variousweatherconditions,anddifferenttimesof
A.AdditionalImplementationDetails
the day. This diversity is crucial for developing systems
thatcanoperatereliablyinreal-worldconditions. Based
In this section, we provide additional details to assist the
onsucharichcollection,Sunetal.[79]adoptedasimilar
implementation and reproduction of the approaches in the
pseudolabelingprocedureasDDD17-Seg[2]andgener-
proposedOpenESSframework.
ated the semantic labels for eleven sequences in DSEC,
A.1.Datasets dubbed as DSEC-Semantic. The dataset specification is
shown in Tab. 6. In total, there are 8082 training and
Inthisstudy,wefollowpriorworks[2,38,47,79]byusing
2809 test samples in the DSEC-Semantic dataset. Each
theDDD17-Seg[2]andDSEC-Semantic[79]datasetsfor
pixelislabeledacrosselevensemanticclasses,including
evaluatingandvalidatingthebaselines,priormethods,and
background, building, fence, person, pole,
theproposedOpenESSframework. Somespecificationsre-
road, sidewalk, vegetation, car, wall, and
latedtothesetwodatasetsarelistedasfollows.
traffic-sign. Foreachsample,weconverttheevent
‚Ä¢ DDD17-Seg[2]servesasthefirstbenchmarkforESS.It
streamsintoasequenceof20voxelgrids,eachconsisting
is a semantic segmentation extension of the DDD17 [5]
of 100000 events and with a spatial resolution of 640√ó
dataset, which includes hours of driving data, capturing
440. Foradditionaldetailsofthisdataset,kindlyreferto
a variety of driving conditions such as different times of
https://dsec.ifi.uzh.ch/dsec-semantic.
day,trafficscenarios,andweatherconditions.Alonsoand
Murillo[2]providethesemanticlabelsontopofDDD17 A.2.TextPrompts
to enable event-based semantic segmentation. Specifi-
To enable the conventional evaluation of our proposed
cally, they proposed to use the corresponding gray-scale
open-vocabulary approach on an event-based semantic
images along with the event streams to generate an ap-
segmentation dataset, we need to use the pre-defined
proximatedsetofsemanticlabelsfortraining,whichwas
class names as text prompts to generate the text embed-
proven effective in training models to segment directly
ding. Specifically, we follow the standard templates [69]
on event-based data. A three-step procedure is applied:
i)trainasemanticsegmentationmodelonthegray-scale when generating the embedding. The dataset-specific text
imagesintheCityscapesdataset[19];ii)Usethetrained promptsdefinedinourframeworkarelistedasfollows.
modeltolabelthegray-scaleimagesinDDD17; andiii) ‚Ä¢ DDD17-Seg. There is a total of six semantic classes in
9Table5.ThespecificationsoftheDDD17-Segdataset[2].
- Training Test
Seq dir0 dir3 dir4 dir6 dir7 dir1
#Frames 11785 20051 41071 28411 58650 71680
#Events 5550 1320 6945 1140 995 3890
Resolution 352√ó200 352√ó200
#Classes 6Classes 6Classes
Table6.ThespecificationsoftheDSEC-Semanticdataset[79].
- Training Test
Seq 00 a 01 a 02 a 04 a 05 a 06 a 07 a 08 a 13 a 14 c 15 a
#Frames 939 681 235 701 1753 1523 1463 787 379 1191 1239
#Events 933 675 229 695 1747 1517 1457 781 373 1185 1233
Resolution 640√ó440 640√ó440
#Classes 11Classes 11Classes
theDDD17-Segdataset[2],withstaticanddynamiccom- ‚Ä¢ Reducing Complexity. By aggregating pixels into su-
ponents of driving scenes. Our defined text prompts of perpixels, the complexity of image data is significantly
thisdatasetaresummarizedinTab.7. Foreachsemantic reduced[78]. Thisreductionhelpsinspeedingupsubse-
class, we generate for each text prompt the text embed- quent image processing tasks, as algorithms have fewer
ding using the CLIP text encoder and then average the elements(superpixels)toprocesscomparedtothepoten-
textembeddingofalltextpromptsasthefinalembedding tiallymillionsofpixelsinanimage.
ofthisclass. ‚Ä¢ Preserving Edges. One of the primary goals of super-
‚Ä¢ DSEC-Semantic. There is a total of eleven semantic pixelsegmentationistopreserveimportantimageedges.
classesintheDSEC-Semanticdataset[79],rangingfrom Superpixelsoftenadherecloselytotheboundariesofob-
static and dynamic components of driving scenes. Our jectsintheimage,makingthemusefulfortasksthatrely
defined text prompts of this dataset are summarized in onaccurateedgeinformation,likeobjectrecognitionand
Tab.8. Foreachsemanticclass,wegenerateforeachtext sceneunderstanding.
prompt the text embedding using the CLIP text encoder In this work, we propose to first leverage calibrated
and then average the text embedding of all text prompts frames to generate coarse, instance-level superpixels and
asthefinalembeddingofthisclass. then distill knowledge from a pre-trained image backbone
to the event segmentation network. Specifically, we resort
A.3.Superpixels tothefollowingtwowaystogeneratethesuperpixels.
In image processing and computer vision, superpixels can ‚Ä¢ SLIC. The first way is to leverage the heuristic Sim-
bedefinedasaschemethatgroupspixelsinanimageinto ple Linear Iterative Clustering (SLIC) approach [1] to
perceptuallymeaningfulatomicregions,whichareusedto efficiently group pixels from frame Iimg into a to-
i
replacetherigidstructureofthepixelgrid[1]. Superpixels tal of M segments with good boundary adherence
slic
provide a more natural representation of the image struc- and regularity. The superpixels are defined as Isp =
i
ture,oftenleadingtomoreefficientandeffectiveimagepro- {I1,I2,...,IMslic}, where M is a hyperparameter
i i i slic
cessing. Herearesomeoftheirkeyaspects: that needs to be adjusted based on the inputs. The
‚Ä¢ GroupingPixels. Superpixelsareoftenformedbyclus- generated superpixels satisfy I1 ‚à™ I2 ‚à™ ... ‚à™ IMslic =
i i i
teringpixelsbasedoncertaincriterialikecolorsimilarity, {1,2,...,H √ó W}. Several examples of the SLIC-
brightness, texture, and other low-level patterns [1], or generated superpixels are shown in the second row of
morerecently,semantics[50]. Thisresultsincontiguous Fig.7, whereeachofthecolor-codedpatchesrepresents
regionsintheimagethataremoremeaningfulthanindi- onedistinctandsemanticallycoherentsuperpixel.
vidualpixelsformanyapplications[13,57,67,94]. ‚Ä¢ SAM. For the second option, we use the recent Seg-
10Table7.ThetextpromptsdefinedontheDDD17-Segdataset[2](6classes)usedforgeneratingtheCLIPtextembedding.
DDD17(6classes)
# class textprompt
0 flat ‚Äòroad‚Äô, ‚Äòdriveable‚Äô, ‚Äòstreet‚Äô, ‚Äòlane marking‚Äô, ‚Äòbicycle lane‚Äô, ‚Äòroundabout lane‚Äô,
‚Äòparkinglane‚Äô,‚Äòterrain‚Äô,‚Äògrass‚Äô,‚Äòsoil‚Äô,‚Äòsand‚Äô,‚Äòlawn‚Äô,‚Äòmeadow‚Äô,‚Äòturf‚Äô
1 background ‚Äòsky‚Äô,‚Äòbuilding‚Äô
2 object ‚Äòpole‚Äô, ‚Äòtraffic sign pole‚Äô, ‚Äòtraffic light pole‚Äô, ‚Äòtraffic light box‚Äô, ‚Äòtraffic-sign‚Äô,
‚Äòparking-sign‚Äô,‚Äòdirection-sign‚Äô
3 vegetation ‚Äòvegetation‚Äô,‚Äòverticalvegetation‚Äô,‚Äòtree‚Äô,‚Äòtreetrunk‚Äô,‚Äòhedge‚Äô,‚Äòwoods‚Äô,‚Äòterrain‚Äô,
‚Äògrass‚Äô,‚Äòsoil‚Äô,‚Äòsand‚Äô,‚Äòlawn‚Äô,‚Äòmeadow‚Äô,‚Äòturf‚Äô
4 human ‚Äòperson‚Äô,‚Äòpedestrian‚Äô,‚Äòwalkingpeople‚Äô,‚Äòstandingpeople‚Äô,‚Äòsittingpeople‚Äô,‚Äòtod-
dler‚Äô
5 vehicle ‚Äòcar‚Äô,‚Äòjeep‚Äô,‚ÄòSUV‚Äô,‚Äòvan‚Äô,‚Äòcaravan‚Äô,‚Äòtruck‚Äô,‚Äòboxtruck‚Äô,‚Äòpickuptruck‚Äô,‚Äòtrailer‚Äô,
‚Äòbus‚Äô, ‚Äòpublic bus‚Äô, ‚Äòtrain‚Äô, ‚Äòvehicle-on-rail‚Äô, ‚Äòtram‚Äô, ‚Äòmotorbike‚Äô, ‚Äòmoped‚Äô,
‚Äòscooter‚Äô,‚Äòbicycle‚Äô
Table8.TheextpromptsdefinedontheDSEC-Semanticdataset[79](11classes)usedforgeneratingtheCLIPtextembedding.
DSEC-Semantic(11classes)
# class textprompt
0 background ‚Äòsky‚Äô
1 building ‚Äòbuilding‚Äô, ‚Äòskyscraper‚Äô, ‚Äòhouse‚Äô, ‚Äòbus stop building‚Äô, ‚Äògarage‚Äô, ‚Äòcarport‚Äô, ‚Äòscaf-
folding‚Äô
2 fence ‚Äòfence‚Äô,‚Äòfencewithhole‚Äô
3 person ‚Äòperson‚Äô,‚Äòpedestrian‚Äô,‚Äòwalkingpeople‚Äô,‚Äòstandingpeople‚Äô,‚Äòsittingpeople‚Äô,‚Äòtod-
dler‚Äô
4 pole ‚Äòpole‚Äô,‚Äòelectricpole‚Äô,‚Äòtrafficsignpole‚Äô,‚Äòtrafficlightpole‚Äô
5 road ‚Äòroad‚Äô, ‚Äòdriveable‚Äô, ‚Äòstreet‚Äô, ‚Äòlane marking‚Äô, ‚Äòbicycle lane‚Äô, ‚Äòroundabout lane‚Äô,
‚Äòparkinglane‚Äô
6 sidewalk ‚Äòsidewalk‚Äô,‚Äòdelimitingcurb‚Äô,‚Äòtrafficisland‚Äô,‚Äòwalkable‚Äô,‚Äòpedestrianzone‚Äô
7 vegetation ‚Äòvegetation‚Äô,‚Äòverticalvegetation‚Äô,‚Äòtree‚Äô,‚Äòtreetrunk‚Äô,‚Äòhedge‚Äô,‚Äòwoods‚Äô,‚Äòterrain‚Äô,
‚Äògrass‚Äô,‚Äòsoil‚Äô,‚Äòsand‚Äô,‚Äòlawn‚Äô,‚Äòmeadow‚Äô,‚Äòturf‚Äô
8 car ‚Äòcar‚Äô,‚Äòjeep‚Äô,‚ÄòSUV‚Äô,‚Äòvan‚Äô,‚Äòcaravan‚Äô,‚Äòtruck‚Äô,‚Äòboxtruck‚Äô,‚Äòpickuptruck‚Äô,‚Äòtrailer‚Äô,
‚Äòbus‚Äô, ‚Äòpublic bus‚Äô, ‚Äòtrain‚Äô, ‚Äòvehicle-on-rail‚Äô, ‚Äòtram‚Äô, ‚Äòmotorbike‚Äô, ‚Äòmoped‚Äô,
‚Äòscooter‚Äô,‚Äòbicycle‚Äô
9 wall ‚Äòwall‚Äô,‚Äòstandingwall‚Äô
10 traffic-sign ‚Äòtraffic-sign‚Äô, ‚Äòparking-sign‚Äô, ‚Äòdirection-sign‚Äô, ‚Äòtraffic-sign without pole‚Äô, ‚Äòtraffic
lightbox‚Äô
ment Anything Model (SAM) [50] which takes Iimg as Fig.7, whereeachofthecolor-codedpatchesrepresents
i
the input and outputs M class-agnostic masks. For onedistinctandsemanticallycoherentsuperpixel.
sam
simplicity, we use M to denote the number of super- We calculate the SLIC and SAM superpixel distribu-
pixels used during knowledge distillation, i.e., {I isp = tionsonthetrainingsetoftheDSEC-Semanticdataset[79]
{I i1,...,I ik}|k = 1,...,M}. Several examples of the and show the corresponding statistics in Fig. 8. As can
SAM-generatedsuperpixelsareshowninthethirdrowof beobserved, theSLIC-generatedsuperpixelsoftencontain
11Figure7. ExamplesofsuperpixelsgeneratedbySLIC[1](the2ndrow)andSAM[50](the3rdrow). TheparameterM intheSLIC
slic
algorithmissetto100.Eachcoloredpatchrepresentsonedistinctandsemanticallycoherentsuperpixel.Bestviewedincolors.
(a)HistogramofSLIC-GeneratedSuperpixels (b)HistogramofSAM-GeneratedSuperpixels
Figure8.ThestatisticaldistributionsofsuperpixelsgeneratedbySLIC[1](subfigurea)andSAM[50](subfigureb).
morelow-levelvisualcues,suchascolorsimilarity,bright- thenumberofsuperpixelsM shouldreflectthecomplex-
slic
ness, and texture. On the contrary, superpixels generated ity and detail of the image. For images with high detail
bySAMexhibitclearsemanticcoherenceandoftendepict orcomplexity(likethosewithmanyobjectsortextures), a
the boundaries of objects and backgrounds. As verified in largerM cancapturemoreofthisdetail. Conversely,for
slic
the main body of this paper, the semantically richer SAM simplerimages,fewersuperpixelsmightbesufficient. Usu-
superpixels bring higher performance gains in our Frame- ally, more superpixels mean smaller superpixels. Smaller
to-EventContrastiveLearningframework. superpixels can adhere more closely to object boundaries
andcapture finerdetails, butthey mightalsocapture more
Meanwhile, we provide more fine-grained examples of noise. Fewer superpixels result in larger, more homoge-
theSLICalgorithmusingdifferentM slic,i.e.,25,50,100, neous regions but may lead to a loss of detail, especially
150,and200. TheresultsareshowninFig.9. Specifically,
12
emarF
)CILS(lexiprepuS
)MAS(lexiprepuSFigure9.ExamplesofsuperpixelsgeneratedbySLIC[1]withdifferentnumbersofsuperpixelsM (25,50,100,150,and200).Each
slic
coloredpatchrepresentsonedistinctandsemanticallycoherentsuperpixel.Bestviewedincolors.
at the edges of objects. The choice also depends on the Thisinvolvesexperimentingwithdifferentvaluesandeval-
specific application. For instance, in object detection or uating the results based on the specific criteria of the task
segmentation tasks where boundary adherence is crucial, or application. In our event-based semantic segmentation
a higher number of superpixels might be preferable. In task,wechooseM = 100forourFrame-to-EventCon-
slic
contrast, for tasks like image compression or abstraction, trastive Learning on the DSEC-Semantic dataset [79], and
fewer superpixels might be more appropriate. Often, the M =25ontheDDD17-Segdataset[2].
slic
optimal number of superpixels is determined empirically.
Since Ievt and Iimg have been aligned and synchro-
i i
13
emarF
52=CILS
05=CILS
001=CILS
051=CILS
002=CILSnized, we can group events from Ievt into superevents E andD aretheencoderofdecoderoftheE2VID
i e2vid e2vid
{Vsp = {V1,...,Vl}|l = 1,...,M} by using the known model[73],respectively. Itisworthnotingthateventre-
i i i
event-pixelcorrespondences. constructions can lose the fine temporal resolution that
event cameras provide. They might also introduce arti-
A.4.Backbones
facts or noise, especially in scenes with fast-moving ob-
As mentioned in the main body of this paper, we estab- jects or low event rates. For additional details on the
lishthreeopen-vocabularyevent-basedsemanticsegmenta- use of event reconstructions, kindly refer to https:
tion settings based on the use of three different event rep-
//github.com/uzh-rpg/rpg_e2vid.
resentations, i.e., frame2voxel, frame2recon, and ‚Ä¢ Frame2Spike. Fortheuseofspikesastheeventembed-
frame2spike. It is worth noting that these three event ding, we follow Kim et al. [49] by converting the raw
representationstendtohavetheirownadvantages. events Œµ i into spikes I ispk ‚àà RH√óW as the input to the
We supplement additional implementation details re- event-based semantic segmentation network. The spike
gardingtheusedeventrepresentationsasfollows. representation keeps the data in its raw form ‚Äì as indi-
vidualspikesorevents. Thisrepresentationpreservesthe
‚Ä¢ Frame2Voxel.Fortheuseofvoxelgridsastheeventem-
high temporal resolution of the event data and is highly
bedding,wefollowSunetal.[79]byconvertingtheraw
efficientintermsofmemoryandcomputation,especially
events Œµ into the regular voxel grids Ivox ‚àà RC√óH√óW
i i for sparse scenes. The rate coding is used as the spike
astheinputtotheevent-basedsemanticsegmentationnet-
encoding scheme due to its reliable performance across
work.Thisrepresentationisintuitiveandalignswellwith
various tasks. Each pixel value with a random num-
conventionaleventcameradataprocessingtechniques. It
ber ranging between [s ,s ] at every time step is
is suitable for convolutional neural networks as it main- min max
recorded, where s and s are the minimum and
tainsspatialandtemporalrelationships.Specifically,with min max
maximum possible pixel intensities, respectively. If the
a predefined number of events, each voxel grid is built
random number is greater than the pixel intensity, the
fromnon-overlappingwindowsasfollows:
(cid:88) Poisson spike generator outputs a spike with amplitude
Ivox = p Œ¥(x ‚àíx)Œ¥(y ‚àíy)max{1‚àí|t‚àó‚àít|,0},
i j j j j 1. Otherwise,thePoissonspikegeneratordoesnotyield
ej‚ààŒµi
any spikes. The spikes in a certain time window are ac-
(8)
where Œ¥ is the Kronecker delta function; t‚àó = (B ‚àí cumulated to generate a frame, where such frames will
j
serve as the input to the event-based semantic segmen-
1)tj‚àít0 isthenormalizedeventtimestampwithB asthe
‚àÜT tation network. It is worth noting that processing raw
number of temporal bins in an event stream; ‚àÜT is the
spikedatarequiresspecializedalgorithms,ofteninspired
timewindowandt denotesthetimeofthefirsteventin
0 by neuromorphic computing. It might not be suitable
the window. It is worth noting that voxel grids can be
for traditional image processing techniques and can be
memory-intensive,especiallyforhigh-resolutionsensors
challengingtointerpretandvisualize. Foradditionalde-
orlong-timewindows. Theymightalsointroducequan-
tails on the use of spikes, kindly refer to https://
tizationerrorsduetothediscretizationofspaceandtime.
github.com/Intelligent-Computing-Lab-
For additional details on the use of voxel grids, kindly
Yale/SNN-Segmentation.
refertohttps://github.com/uzh-rpg/ess.
‚Ä¢ Frame2Recon. For the use of event reconstructions as Tosumup,eacheventrepresentationhasitsuniquechar-
the event embedding, we follow Sun et al. [79] and Re- acteristics and is suitable for different applications or pro-
becq et al. [73] by converting the raw events Œµ into the cessing techniques. Our proposed OpenESS framework is
i
regular frame-like event reconstructions Irec ‚àà RH√óW capable of leveraging each of the above event representa-
i
astheinputtotheevent-basedsemanticsegmentationnet- tions for efficient and accurate event-based semantic seg-
work.Thiscanbedonebyaccumulatingeventsovershort mentationinanannotation-freeandopen-vocabularyman-
timeintervalsorbyusingalgorithmstointerpolateorsim- ner.Suchaversatileandflexiblewayoflearningverifiesthe
ulateframes. Thisapproachiscompatiblewithstandard broaderapplicationpotentialofourproposedframework.
image processing techniques and algorithms developed
forframe-basedvision.Itismorefamiliartopractitioners A.5.EvaluationConfiguration
usedtoworkingwithconventionalcameras. Inthiswork,
Following the convention, we use the Intersection-over-
weadopttheE2VIDmodel[73]togeneratetheeventre-
Union(IoU)metrictomeasurethesemanticsegmentation
constructions. Thisprocesscanbedescribedasfollows:
performanceforeachsemanticclass. TheIoUscorecanbe
zrec = E (Ivox,zrec ), k =1,...,N, (9)
k e2vid k k‚àí1 calculatedviathefollowingequation:
Irec = D (zrec), (10)
i e2vid
TP
whereIvox denotesthevoxelgridsasdefinedinEq.(8); IoU= , (11)
k TP +FP +FN
14Table9. Theper-classsegmentationresultsofannotation-freeevent-basedsemanticsegmentationapproachesonthetestsetofDSEC-
Semantic[79].ScoresreportedareIoUsinpercentage(%).Foreachsemanticclass,thebestscoreineachcolumnishighlightedinbold.
Method mIoU Acc
Annotation-FreeESS
MaskCLIP[100] 21.97 26.45 52.59 0.20 0.04 4.19 65.76 2.96 48.02 40.67 0.67 0.08 58.96
FC-CLIP[97] 39.42 87.49 69.68 14.39 17.53 0.29 71.76 34.56 71.30 63.19 2.98 0.50 79.20
OpenESS(Ours) 43.31 92.53 74.22 11.96 0.00 0.41 87.32 55.09 74.23 64.25 7.98 8.47 86.18
Table10. Theper-classsegmentationresultsofannotation-efficientevent-basedsemanticsegmentationapproachesonthetestsetof
DSEC-Semantic[79]. Allapproachesadoptedtheframe2voxelrepresentation. ScoresreportedareIoUsinpercentage(%). Foreach
semanticclassundereachexperimentalsetting,thebestscoreineachcolumnishighlightedinbold.
Method Acc
LinearProbing
Random 6.70 7.85 3.37 0.00 0.00 0.00 38.60 0.00 23.83 0.01 0.00 0.00 37.94
MaskCLIP[100] 33.08 75.04 65.06 4.63 0.00 6.47 77.06 17.07 55.89 52.17 0.69 9.78 76.39
FC-CLIP[97] 43.00 92.53 72.59 12.43 0.02 0.00 88.14 52.84 71.92 64.02 10.54 7.95 86.00
OpenESS(Ours) 44.26 93.64 75.40 11.82 1.16 0.75 90.29 57.96 73.15 65.36 9.69 7.67 87.55
Fine-Tuning(1%)
Random 26.62 81.63 33.13 1.77 0.97 7.58 76.81 17.45 51.05 18.64 0.37 3.40 70.04
MaskCLIP[100] 33.89 87.56 53.24 2.34 0.60 8.92 81.71 25.76 59.37 42.56 2.52 8.24 77.79
FC-CLIP[97] 39.12 91.64 59.78 8.93 0.00 7.84 87.58 46.58 66.87 51.30 4.74 5.10 82.12
OpenESS(Ours) 41.41 93.01 74.01 3.21 10.78 14.58 84.50 34.78 69.82 55.12 4.47 11.21 84.41
Fine-Tuning(5%)
Random 31.22 77.13 50.32 12.36 1.26 0.00 86.03 41.22 21.48 50.67 2.96 0.04 71.38
MaskCLIP[100] 37.03 91.09 60.52 4.35 11.90 11.73 81.24 23.56 61.77 45.93 2.75 12.45 79.58
FC-CLIP[97] 43.71 92.91 71.21 10.84 0.00 5.60 90.11 57.54 71.30 61.04 11.41 8.81 86.38
OpenESS(Ours) 44.97 93.58 70.18 8.44 18.22 11.01 89.72 57.76 67.44 56.06 9.59 12.70 85.46
Fine-Tuning(10%)
Random 33.67 85.79 49.85 6.78 8.00 15.51 80.78 25.72 58.18 29.97 0.82 8.93 76.69
MaskCLIP[100] 38.83 92.34 69.96 3.64 5.85 12.98 82.23 23.61 66.39 53.23 3.47 13.46 82.36
FC-CLIP[97] 44.09 93.62 72.86 10.88 0.00 8.23 89.81 57.05 71.95 60.64 9.58 10.42 86.66
OpenESS(Ours) 46.25 93.92 73.34 8.13 18.61 15.41 89.03 52.56 71.76 61.71 9.99 14.26 86.72
Fine-Tuning(20%)
Random 41.31 91.08 67.90 4.68 17.90 17.41 85.11 43.24 66.62 43.95 5.03 11.55 82.99
MaskCLIP[100] 42.40 93.19 72.49 5.52 18.21 16.17 84.29 35.04 69.44 54.47 2.43 15.15 84.09
FC-CLIP[97] 47.77 91.05 70.90 7.04 21.10 14.84 91.13 64.28 71.62 61.73 13.25 18.55 86.95
OpenESS(Ours) 48.28 94.21 74.66 10.49 20.46 16.27 90.15 57.66 73.71 63.95 11.20 18.29 87.57
where TP (True Positive) denotes pixels correctly classi- fect overlap). It is a way to summarize the mIoU values
fiedasbelongingtotheclass; FP (FalsePositive)denotes for each class into a single metric that captures the overall
pixels incorrectly classified as belonging to the class; and performanceofthemodelacrossallclasses,i.e.,meanIoU
FN (FalseNegative)denotespixelsthatbelongtotheclass (mIoU).ThemIoUofagivenpredictioniscalculatedas:
butareincorrectlyclassifiedassomethingelse.
The IoU metric measures the overlap between the pre-
C
dicted segmentation and the ground truth for a specific 1 (cid:88)
mIoU= IoU , (12)
class. Itreturnsavaluebetween0(nooverlap)and1(per- C i
i=1
15
UoIm
dnuorgkcab
dnuorgkcab
gnidliub
gnidliub
ecnef
ecnef
nosrep
nosrep
elop
elop
daor
daor
klawedis
klawedis
noitategev
noitategev
rac
rac
llaw
llaw
ngis-cfifart
ngis-cfifartwhere C is the number of classes and IoU denotes the works in the established annotation-efficient event-based
i
score of class i. mIoU provides a balanced measure since semanticsegmentation.
eachclasscontributesequallytothefinalscore, regardless
ofitssizeorfrequencyinthedataset. AhighermIoUindi- C.QualitativeAssessment
catesbettersemanticsegmentationperformance.Ascoreof
In this section, we provide sufficient qualitative examples
1wouldindicateperfectsegmentationforallclasses,while
to further attest to the effectiveness and superiority of the
ascoreof0wouldimplyanabsenceofcorrectpredictions.
proposedframework.
In this work, all the compared approaches adopt the same
mIoUcalculationasintheESSbenchmarks[2,79]. Addi- C.1.Open-VocabularyExamples
tionally,wealsoreportthesemanticsegmentationaccuracy
ThekeyadvantageofourproposedOpenESSframeworkis
(Acc)forthebaselinesandtheproposedframework.
itscapabilitytoleverageopen-worldvocabulariesfromthe
CLIP text embedding space. Unlike prior event-based se-
B.AdditionalExperimentalResults
manticsegmentation,whichreliesonpre-definedandfixed
categories, our open-vocabulary segmentation aims to un-
Inthissection,weprovidetheclass-wiseIoUscoresforthe
derstandandcategorizeimageregionsintoabroader,poten-
experimentsconductedinthemainbodyofthispaper.
tiallyunlimitedrangeofcategories.Weprovidemoreopen-
B.1.Annotation-FreeESS vocabularyexamplesinFig.10. Ascanbeobserved,given
proper text prompts like ‚Äúroad‚Äù, ‚Äúsidewalk‚Äù, and ‚Äúbuild-
Theper-classzero-shotevent-basedsemanticsegmentation
ing‚Äù,ourproposedOpenESSframeworkiscapableofgen-
results are shown in Tab. 9. For almost every seman-
eratingsemanticallymeaningfulattentionmapsfordepict-
tic class, we observe that the proposed OpenESS achieves
ing the corresponding regions. Such a flexible framework
much higher IoU scores than MaskCLIP [100] and FC-
canbefurtheradaptedtoneworunseencategorieswithout
CLIP[97]. ThisvalidatestheeffectivenessofOpenESSfor
theneedforextensiveretraining,whichisparticularlybene-
conductingefficientandaccurateevent-basedsemanticseg-
ficialindynamicenvironmentswherenewobjectsorclasses
mentationwithoutusingeithertheeventorframelabels.
mightfrequentlyappear. Additionally,theopen-vocabulary
segmentationpipelineallowsuserstoworkwithamoreex-
B.2.Annotation-EfficientESS
tensive range of objects and concepts, enhancing the user
Theper-classlinearprobingevent-basedsemanticsegmen- experienceandinteractioncapabilities.
tation results are shown in the first block of Tab. 10 and
C.2.VisualComparisons
Tab.11. Specifically,comparedtotherandominitialization
baseline,aself-supervisedpre-trainednetworkalwayspro- In this section, we provide more qualitative comparisons
videsbetterfeatures. Thequalityofrepresentationlearning of our proposed OpenESS framework over prior works
oftendeterminesthelinearprobingperformance. Thenet- [79, 100] on the DSEC-Semantic dataset. Specifically, the
workpre-trainedusingourframe-to-eventcontrastivedistil- visual comparisons are shown in Fig. 11 and Fig. 12. As
lation and text-to-event consistency regularization tends to canbeobserved,OpenESSshowssuperiorevent-basedse-
achieve higher event-based semantic segmentation results mantic segmentation performance over prior works across
than MaskCLIP [100] and FC-CLIP [97]. Notably, such a wide range of event scenes under different lighting and
improvementsareholisticacrossalmostallelevensemantic weather conditions. Such consistent segmentation perfor-
classes in the dataset. These results validate the effective- manceimprovementsprovideasolidfoundationtovalidate
ness of the proposed OpenESS framework in tackling the the effectiveness and superiority of the proposed frame-
challengingevent-basedsemanticsegmentationtask. to-event contrastive distillation and text-to-event consis-
The per-class annotation-efficient event-based seman- tency regularization. For additional qualitative compar-
tic segmentation results of the frame2vodel and isons,kindlyrefertoAppendixC.4.
frame2recon settings under 1%, 5%, 10%, and 20%
C.3.FailureCases
annotation budgets are shown in Tab. 10 and Tab. 11, re-
spectively. Similar to the findings and conclusions drawn As can be observed from Fig. 10, Fig. 11, and Fig. 12,
above, we observe clear superiority of the proposed Ope- theexistingevent-basedsemanticsegmentationapproaches
nESSframeworkovertherandominitialization,MaskCLIP still have room for further improvements. Similar to the
[100], and FC-CLIP [97] approaches. Such consistent conventional semantic segmentation task, it is often hard
performanceimprovementsvalidateagaintheeffectiveness to accurately segment the boundaries between the seman-
and superiority of the proposed frame-to-event contrastive ticobjectsandbackgrounds. Inthecontextofevent-based
distillationandtext-to-eventconsistencyregularization.We semantic segmentation, such a problem tends to be partic-
hope our framework can lay a solid foundation for future ularlyovert. Unliketraditionalcamerasthatcapturedense,
16Table11. Theper-classsegmentationresultsofannotation-efficientevent-basedsemanticsegmentationapproachesonthetestsetof
DSEC-Semantic[79]. Allapproachesadoptedtheframe2reconrepresentation. ScoresreportedareIoUsinpercentage(%). Foreach
semanticclassundereachexperimentalsetting,thebestscoreineachcolumnishighlightedinbold.
Method Acc
LinearProbing
Random 6.22 7.55 5.48 0.00 0.00 0.00 39.79 0.00 15.64 0.01 0.00 0.00 36.60
MaskCLIP[100] 27.09 59.82 62.14 1.60 0.00 4.54 69.71 5.34 47.85 38.51 0.40 8.12 70.59
FC-CLIP[97] 40.08 89.22 69.08 14.62 26.90 0.00 83.14 21.79 69.56 57.78 7.86 0.92 82.70
OpenESS(Ours) 44.08 88.56 61.43 6.05 21.54 12.36 91.43 63.04 64.01 60.52 6.18 9.76 84.48
Fine-Tuning(1%)
Random 23.95 76.37 29.59 1.73 0.00 5.75 78.12 9.73 48.96 11.56 0.28 1.38 69.20
MaskCLIP[100] 30.73 79.25 47.26 0.13 1.17 5.04 78.78 19.72 56.13 43.74 1.13 5.70 74.25
FC-CLIP[97] 38.99 87.75 61.48 3.47 4.60 8.06 88.96 55.12 64.41 47.16 3.61 4.23 82.90
OpenESS(Ours) 43.17 87.85 66.15 8.82 21.52 12.41 89.36 55.35 72.45 48.76 3.40 8.81 84.56
Fine-Tuning(5%)
Random 30.42 80.25 38.43 5.50 13.45 9.08 83.45 30.88 51.75 19.53 0.16 2.19 73.65
MaskCLIP[100] 36.33 85.80 60.43 2.60 8.70 7.47 83.10 34.04 64.80 39.60 3.07 10.00 80.37
FC-CLIP[97] 43.34 88.28 64.90 6.94 20.96 9.58 91.18 62.35 68.09 52.39 4.93 7.16 84.93
OpenESS(Ours) 45.58 89.11 70.83 10.92 20.21 1.99 91.04 60.76 72.07 67.91 12.90 3.69 86.93
Fine-Tuning(10%)
Random 34.11 81.85 46.28 4.87 11.30 10.20 85.32 43.16 55.34 32.72 1.28 2.90 77.48
MaskCLIP[100] 40.13 87.31 62.54 4.93 5.09 12.86 88.30 50.60 64.74 55.21 0.32 9.51 83.52
FC-CLIP[97] 45.35 89.71 69.00 6.64 22.37 8.33 91.20 64.09 69.34 61.73 7.23 9.19 86.29
OpenESS(Ours) 48.94 90.63 71.68 12.41 29.32 9.42 92.53 66.19 73.76 69.03 10.71 12.71 87.84
Fine-Tuning(20%)
Random 39.25 87.14 61.80 6.77 3.51 13.19 88.53 56.12 61.95 44.65 1.29 6.84 82.51
MaskCLIP[100] 43.37 89.83 69.80 7.07 8.93 10.67 88.88 52.65 70.71 60.03 3.10 15.39 85.69
FC-CLIP[97] 47.18 91.20 71.39 11.53 24.92 9.60 91.58 63.88 71.52 63.44 7.55 12.36 87.07
OpenESS(Ours) 49.74 91.28 73.43 10.69 27.18 13.85 92.84 67.59 74.20 69.22 10.62 16.21 88.26
synchronous frames, event cameras generate sparse, asyn- ticsegmentationpredictionsamongourproposedOpenESS
chronousevents,whichbringsextradifficultiesforaccurate and prior works. All the provided video sequences val-
boundarysegmentation. Meanwhile,thecurrentframework idate again the unique advantage of the proposed open-
finds it hard to accurately predict the minor classes, such vocabularyevent-basedsemanticsegmentationframework.
as fence, pole, wall, and traffic-sign. We believe these are KindlyrefertoourGitHubrepository1foradditionaldetails
potential directions that future works can explore to fur- onaccessingthesevideodemos.
therimprovetheevent-basedsemanticsegmentationperfor-
manceontopofexistingframeworks. D.BroaderImpact
C.4.VideoDemos In this section, we elaborate on the positive societal in-
fluence and potential limitations of the proposed open-
In addition to the qualitative examples shown in the main vocabularyevent-basedsemanticsegmentationframework.
body and this supplementary file, we also provide several
video clips to further validate the effectiveness and supe- D.1.PositiveSocietalinfluence
riority of the proposed approach. Specifically, we provide
Event-based cameras can capture extremely fast motions
threevideodemosintheattachment,nameddemo1.mp4,
thattraditionalcamerasmightmiss,makingthemidealfor
demo2.mp4, and demo3.mp4. The first two video de-
dynamicenvironments. Inrobotics,thisleadstobetterob-
mos show open-vocabulary event-based semantic segmen-
ject detection and scene understanding, enhancing the ca-
tation examples using the class names and open-world vo-
pabilities of robots in the manufacturing, healthcare, and
cabulariesastheinputtextprompts,respectively. Thethird
videodemocontainsqualitativecomparisonsoftheseman- 1https://github.com/ldkong1205/OpenESS
17
UoIm
dnuorgkcab
gnidliub
ecnef nosrep
elop daor
klawedis
noitategev
rac
llaw
ngis-cfifartservice industries. In autonomous driving, event-based se- ‚Ä¢ ESS7 ...............GNUGeneralPublicLicensev3.0
manticsegmentationprovideshightemporalresolutionand ‚Ä¢ E2VID8.............GNUGeneralPublicLicensev3.0
low latency, which is crucial for detecting sudden changes ‚Ä¢ HMNet9.......................BSD3-ClauseLicense
intheenvironment. Thiscanleadtofasterandmoreaccu- ‚Ä¢ EV-SegNet10...............................Unknown
rate responses, potentially reducing accidents and enhanc- ‚Ä¢ SNN-Segmentation11.......................Unknown
ingroadsafety. OurproposedOpenESSisdesignedtore- ‚Ä¢ CLIP12.................................MITLicense
ducetheannotationbudgetandtrainingburdenofexisting ‚Ä¢ MaskCLIP13......................ApacheLicense2.0
event-basedsemanticsegmentationapproaches. Webelieve ‚Ä¢ FC-CLIP14 .......................ApacheLicense2.0
such an efficient way of learning helps increase the scala- ‚Ä¢ SLIC-Superpixels15 ........................ Unknown
bilityofevent-basedsemanticsegmentationsystemsandin ‚Ä¢ Segment-Anything16...............ApacheLicense2.0
turn contributes positively to impact society by enhancing
safety,efficiency,andperformanceinvariousaspects.
D.2.PotentialLimitation
Althoughourproposedframeworkiscapableofconducting
annotation-freeandopen-vocabularyevent-basedsemantic
segmentation and achieves promising performance, there
tend to exist several potential limitations. Firstly, our cur-
rentframeworkrequirestheexistenceofsynchronizedevent
andRGBcameras,whichmightnotbemaintainedbysome
older event camera systems. Secondly, we directly adopt
thestandardtextprompttemplatestogeneratethetextem-
bedding, where a more sophisticated design could further
improvetheopen-vocabularylearningabilityoftheexisting
framework. Thirdly, theremightstillbesomeself-conflict
problems in our frame-to-event contrastive distillation and
text-to-event consistency regularization. The design of a
betterrepresentationlearningparadigmontheevent-based
datacouldfurtherresolvetheseissues. Webelievetheseare
promising directions that future works can explore to fur-
therimprovethecurrentframework.
E.PublicResourcesUsed
Inthissection,weacknowledgetheuseofpublicresources,
duringthecourseofthiswork.
E.1.PublicDatasetsUsed
We acknowledge the use of the following public datasets,
duringthecourseofthiswork:
‚Ä¢ DSEC2...............................CCBY-SA4.0
‚Ä¢ DSEC-Semantic3......................CCBY-SA4.0
‚Ä¢ DDD174..............................CCBY-SA4.0
‚Ä¢ DDD17-Seg5 ..............................Unknown
‚Ä¢ E2VID-Driving6.....GNUGeneralPublicLicensev3.0 7https://github.com/uzh-rpg/ess
8https://github.com/uzh-rpg/rpg_e2vid
E.2.PublicImplementationsUsed 9https://github.com/hamarh/HMNet_pth
10https://github.com/Shathe/Ev-SegNet
Weacknowledgetheuseofthefollowingpublicimplemen- 11https://github.com/Intelligent-Computing-Lab-
tations,duringthecourseofthiswork: Yale/SNN-Segmentation
12https://github.com/openai/CLIP
2https://dsec.ifi.uzh.ch 13https://github.com/chongzhou96/MaskCLIP
3https://dsec.ifi.uzh.ch/dsec-semantic 14https://github.com/bytedance/fc-clip
4http://sensors.ini.uzh.ch/news_page/DDD17.html 15https://github.com/PSMM/SLIC-Superpixels
5https://github.com/Shathe/Ev-SegNet 16https://github.com/facebookresearch/segment-
6https://rpg.ifi.uzh.ch/E2VID.html anything
18Background Building Fence Person Pole Road Sidewalk Vegetation Car Wall Traffic-Sign
Event Reconstruction ‚Äúroad‚Äù ‚Äùsidewalk‚Äù ‚Äùbuilding‚Äù GT
Figure10.Qualitativeexamplesofthelanguage-guidedattentionmapsgeneratedbytheproposedOpenESSframework.Foreachsample,
theregionswithahighsimilarityscoretothetextpromptsarehighlighted.Bestviewedincolorsandzoomed-inforadditionaldetails.
19Background Building Fence Person Pole Road Sidewalk Vegetation Car Wall Traffic-Sign
Event Reconstruction MaskCLIP ESS-Sup OpenESS GT
Figure11.Qualitativecomparisons(1/2)amongdifferentESSapproachesonthetestsetofDSEC-Semantic[79].Bestviewedincolors.
20Background Building Fence Person Pole Road Sidewalk Vegetation Car Wall Traffic-Sign
Event Reconstruction MaskCLIP ESS-Sup OpenESS GT
Figure12.Qualitativecomparisons(2/2)amongdifferentESSapproachesonthetestsetofDSEC-Semantic[79].Bestviewedincolors.
21References atrousseparableconvolutionforsemanticimagesegmenta-
tion. InEuropeanConferenceonComputerVision, pages
[1] RadhakrishnaAchanta,AppuShaji,KevinSmith,Aurelien
801‚Äì818,2018. 1
Lucchi, PascalFua, andSabineSu¬®sstrunk. Slicsuperpix-
[13] Runnan Chen, Youquan Liu, Lingdong Kong, Nenglun
elscomparedtostate-of-the-artsuperpixelmethods. IEEE
Chen, Xinge Zhu, Yuexin Ma, Tongliang Liu, and Wen-
TransactionsonPatternAnalysisandMachineIntelligence,
pingWang. Towardslabel-freesceneunderstandingbyvi-
34(11):2274‚Äì2282,2012. 4,6,7,10,12,13
sionfoundationmodels.InAdvancesinNeuralInformation
[2] Inigo Alonso and Ana C. Murillo. Ev-segnet: Semantic
ProcessingSystems,2023. 2,10
segmentationforevent-basedcameras. InIEEE/CVFCon-
[14] TingChen,SimonKornblith,MohammadNorouzi,andGe-
ferenceonComputerVisionandPatternRecognitionWork-
offreyHinton.Asimpleframeworkforcontrastivelearning
shops,pages1‚Äì10,2019. 1,2,5,6,8,9,10,11,13,16
of visual representations. In International Conference on
[3] HangboBao,LiDong,SonghaoPiao,andFuruWei. Beit: MachineLearning,pages1597‚Äì1607,2020. 7
Bert pre-training of image transformers. In International
[15] XinleiChen,HaoqiFan,RossGirshick,andKaimingHe.
ConferenceonLearningRepresentations,2021. 6,7
Improved baselines with momentum contrastive learning.
[4] AhmedNabilBelbachir, StephanSchraml, ManfredMay- arXivpreprintarXiv:2003.04297,2020. 4,5,8
erhofer,andMichaelHofsta¬®tter. Anovelhdrdepthcamera
[16] Xinlei Chen, Saining Xie, and Kaiming He. An empiri-
forreal-time3d360panoramicvision. InIEEE/CVFCon-
calstudyoftrainingself-supervisedvisiontransformers.In
ferenceonComputerVisionandPatternRecognitionWork-
IEEE/CVF International Conference on Computer Vision,
shops,pages425‚Äì432,2014. 2
pages9620‚Äì9629,2021. 7
[5] JonathanBinas,DanielNeil,Shih-ChiiLiu,andTobiDel- [17] Hoonhee Cho, Jegyeong Cho, and Kuk-Jin Yoon. Learn-
bruck. Ddd17: End-to-end davis driving dataset. In In- ing adaptive dense event stereo from the image domain.
ternational Conference on Machine Learning Workshops, InIEEE/CVFConferenceonComputerVisionandPattern
pages1‚Äì9,2017. 2,6,7,8,9 Recognition,pages17797‚Äì17807,2023. 2
[6] ShristiDasBiswas,AdarshKosta,ChamikaLiyanagedera, [18] HoonheeCho,HyeonseongKim,YujeongChae,andKuk-
Marco Apolinario, and Kaushik Roy. Halsie: Hybrid ap- Jin Yoon. Label-free event-based object recognition via
proachtolearningsegmentationbysimultaneouslyexploit- joint learning with image reconstruction from events. In
ingimageandeventmodalities. InIEEE/CVFWinterCon- IEEE/CVF International Conference on Computer Vision,
ference on Applications of Computer Vision, 2024. 1, 2, pages19866‚Äì19877,2023. 2
6 [19] MariusCordts,MohamedOmran,SebastianRamos,Timo
[7] Vincent Brebion, Julien Moreau, and Franck Davoine. Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Real-time optical flow for vehicular perception with low- Franke, Stefan Roth, and Bernt Schiele. The cityscapes
and high-resolution event cameras. IEEE Transactions dataset for semantic urban scene understanding. In
onIntelligentTransportationSystems,23(9):15066‚Äì15078, IEEE/CVF Conference on Computer Vision and Pattern
2021. 2 Recognition,pages3213‚Äì3223,2016. 1,9
[8] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, [20] Javier Cuadrado, Ulysse Ranc¬∏on, Benoit R. Cottereau,
PiotrBojanowski,andArmandJoulin.Unsupervisedlearn- Francisco Barranco, and Timothe¬¥e Masquelier. Optical
ingofvisualfeaturesbycontrastingclusterassignments.In flowestimationfromevent-basedcamerasandspikingneu-
AdvancesinNeuralInformationProcessingSystems,pages ralnetworks.FrontiersinNeuroscience,17:1160034,2023.
9912‚Äì9924,2020. 4,5,8 2
[9] MathildeCaron,HugoTouvron,IshanMisra,Herve¬¥Je¬¥gou, [21] Zheng Ding, Jieke Wang, and Zhuowen Tu. Open-
Julien Mairal, Piotr Bojanowski, and Armand Joulin. vocabulary panoptic segmentation with maskclip. arXiv
Emergingpropertiesinself-supervisedvisiontransformers. preprintarXiv:2208.08984,2022. 2
In IEEE/CVF International Conference on Computer Vi- [22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
sion,pages9650‚Äì9660,2021. 5,8 Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
[10] Kaiwei Che, Luziwei Leng, Kaixuan Zhang, Jianguo Mostafa Dehghani, Matthias Minderer, Georg Heigold,
Zhang,QinghuMeng,JieCheng,QinghaiGuo,andJianx- SylvainGelly,JakobUszkoreit,andNeilHoulsby. Anim-
ingLiao.Differentiablehierarchicalandsurrogategradient ageisworth16x16words: Transformersforimagerecog-
search for spiking neural networks. In Advances in Neu- nition at scale. In International Conference on Learning
ral Information Processing Systems, pages 24975‚Äì24990, Representations,2021. 3
2022. 2 [23] BurakErcan,OnurEker,AykutErdem,andErkutErdem.
[11] Liang-ChiehChen,GeorgePapandreou,IasonasKokkinos, Evreal: Towardsacomprehensivebenchmarkandanalysis
KevinMurphy,andAlanL.Yuille. Deeplab:Semanticim- suite for event-based video reconstruction. In IEEE/CVF
agesegmentationwithdeepconvolutionalnets,atrouscon- Conference on Computer Vision and Pattern Recognition
volution, andfullyconnectedcrfs. IEEETransactionson Workshops,pages3942‚Äì3951,2023. 2
PatternAnalysisandMachineIntelligence,40(4):834‚Äì848, [24] Burak Ercan, Onur Eker, Canberk Saglam, Aykut Erdem,
2017. 1 and Erkut Erdem. Hypere2vid: Improving event-based
[12] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Flo- video reconstruction via hypernetworks. arXiv preprint
rian Schroff, and Hartwig Adam. Encoder-decoder with arXiv:2305.06382,2023. 2
22[25] Guillermo Gallego, Tobi Delbru¬®ck, Garrick Orchard, ence on Computer Vision and Pattern Recognition, pages
Chiara Bartolozzi, Brian Taba, Andrea Censi, Stefan 22867‚Äì22876,2023. 1,2,6,9
Leutenegger, Andrew J. Davison, Jo¬®rg Conradt, Kostas [39] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.
Daniilidis, and Davide Scaramuzza. Event-based vision: Deepresiduallearningforimagerecognition.InIEEE/CVF
Asurvey. IEEETransactionsonPatternAnalysisandMa- Conference on Computer Vision and Pattern Recognition,
chineIntelligence,44(1):154‚Äì180,2022. 1,2,3 pages770‚Äì778,2016. 1,3,5
[26] MingfeiGao,ChenXing,JuanCarlosNiebles,JunnanLi, [40] KaimingHe,XinleiChen,SainingXie,YanghaoLi,Piotr
Ran Xu, Wenhao Liu, and Caiming Xiong. Open vocab- Dolla¬¥r,andRossGirshick. Maskedautoencodersarescal-
ulary object detection with pseudo bounding-box labels. able vision learners. In IEEE/CVF Conference on Com-
In European Conference on Computer Vision Workshops, puterVisionandPatternRecognition,pages16000‚Äì16009,
pages266‚Äì282,2022. 2 2022. 7
[27] DanielGehrigandDavideScaramuzza. Pushingthelimits [41] ShutingHe,HenghuiDing,andWeiJiang. Primitivegen-
of asynchronous graph-based object detection with event erationandsemantic-relatedalignmentforuniversalzero-
cameras. arXivpreprintarXiv:2211.12324,2022. 2 shotsegmentation. InIEEE/CVFConferenceonComputer
[28] Daniel Gehrig and Davide Scaramuzza. Are high- VisionandPatternRecognition,pages11238‚Äì11247,2023.
resolution event cameras really needed? arXiv preprint 2
arXiv:2203.14672,2022. 1 [42] Javier Hidalgo-Carrio¬¥, Daniel Gehrig, and Davide Scara-
muzza. Learningmonoculardensedepthfromevents. In
[29] Daniel Gehrig, Antonio Loquercio, Konstantinos G. Der-
IEEE International Conference on 3D Vision, pages 534‚Äì
panis, and Davide Scaramuzza. End-to-end learning of
542,2020. 2
representations for asynchronous event-based data. In
IEEE/CVF International Conference on Computer Vision, [43] Javier Hidalgo-Carrio¬¥, Guillermo Gallego, and Davide
pages5633‚Äì5643,2019. 2,4 Scaramuzza. Event-aided direct sparse odometry. In
IEEE/CVF Conference on Computer Vision and Pattern
[30] DanielGehrig,MathiasGehrig,JavierHidalgo-Carrio¬¥,and
Recognition,pages5781‚Äì5790,2022. 2
Davide Scaramuzza. Video to events: Recycling video
[44] Kunping Huang, Sen Zhang, Jing Zhang, and Dacheng
datasets for event cameras. In IEEE/CVF Conference on
Tao. Event-basedsimultaneouslocalizationandmapping:
Computer Vision and Pattern Recognition, pages 3586‚Äì
Acomprehensivesurvey.arXivpreprintarXiv:2304.09793,
3595,2020. 2,6
2023. 1
[31] Mathias Gehrig and Davide Scaramuzza. Recurrent vi-
[45] DatHuynh,JasonKuen,ZheLin,JiuxiangGu,andEhsan
siontransformersforobjectdetectionwitheventcameras.
Elhamifar. Open-vocabularyinstancesegmentationviaro-
InIEEE/CVFConferenceonComputerVisionandPattern
bust cross-modal pseudo-labeling. In IEEE/CVF Confer-
Recognition,pages13884‚Äì13893,2023. 2
ence on Computer Vision and Pattern Recognition, pages
[32] Mathias Gehrig, Willem Aarents, Daniel Gehrig, and Da-
7020‚Äì7031,2022. 2
videScaramuzza. Dsec: Astereoeventcameradatasetfor
[46] OlivierJ.He¬¥naff,SkandaKoppula,Jean-BaptisteAlayrac,
drivingscenarios. IEEERoboticsandAutomationLetters,
Aaron Van den Oord, Oriol Vinyals, and Joao Carreira.
6(3):4947‚Äì4954,2021. 5,9
Efficient visual pretraining with contrastive detection. In
[33] MathiasGehrig,MarioMillha¬®usler,DanielGehrig,andDa-
IEEE/CVF International Conference on Computer Vision,
vide Scaramuzza. E-raft: Dense optical flow from event
pages10086‚Äì10096,2021. 2
cameras. InIEEEInternationalConferenceon3DVision,
[47] ZexiJia,KaichaoYou,WeihuaHe,YangTian,Yongxiang
pages197‚Äì206,2021. 2
Feng, YaoyuanWang, XuJia, YihangLou, JingyiZhang,
[34] Mathias Gehrig, Manasi Muglikar, and Davide Scara-
Guoqi Li, and Ziyang Zhang. Event-based semantic seg-
muzza. Dense continuous-time optical flow from events
mentation with posterior attentio. IEEE Transactions on
andframes. arXivpreprintarXiv:2203.13674,2022. 2
ImageProcessing,32:1829‚Äì1842,2023. 2,6,9
[35] GolnazGhiasi,XiuyeGu,YinCui,andTsung-YiLin.Scal- [48] Junho Kim, Jaehyeok Bae, Gangin Park, Dongsu Zhang,
ingopen-vocabularyimagesegmentationwithimage-level and Young Min Kim. N-imagenet: Towards robust,
labels. InEuropeanConferenceonComputerVisionWork- fine-grained object recognition with event cameras. In
shops,pages540‚Äì557,2022. 2 IEEE/CVF International Conference on Computer Vision,
[36] SumanGhoshandGuillermoGallego. Multi-event-camera pages2146‚Äì2156,2021. 2
depthestimationandoutlierrejectionbyrefocusedevents [49] YoungeunKim, JoshuaChough, andPriyadarshiniPanda.
fusion. Advanced Intelligent Systems, 4(12):2200221, Beyondclassification:Directlytrainingspikingneuralnet-
2020. 2 worksforsemanticsegmentation. NeuromorphicComput-
[37] Renxiang Guan, Zihao Li, Xianju Li, and Chang Tang. ingandEngineering,2(4):044015,2022. 2,4,5,6,7,14
Pixel-superpixelcontrastivelearningandpseudo-labelcor- [50] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi
rectionforhyperspectralimageclustering. arXivpreprint Mao,ChloeRolland,LauraGustafson,TeteXiao,Spencer
arXiv:2312.09630,2023. 3 Whitehead,AlexanderC.Berg,Wan-YenLo,PiotrDolla¬¥r,
[38] Ryuhei Hamaguchi, Yasutaka Furukawa, Masaki Onishi, and Ross Girshick. Segment anything. In IEEE/CVF In-
and Ken Sakurada. Hierarchical neural memory network ternational Conference on Computer Vision, pages 4015‚Äì
for low latency event processing. In IEEE/CVF Confer- 4026,2023. 2,4,6,7,10,11,12
23[51] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen ingneuralnetworks.IEEESignalProcessingMagazine,36
Koltun, and Rene Ranftl. Language-driven semantic seg- (6):51‚Äì63,2019. 2
mentation. InInternationalConferenceonLearningRep- [64] MaximeOquab,Timothe¬¥eDarcet,The¬¥oMoutakanni,Huy
resentations,2022. 2,4 Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
[52] LiunianHaroldLi,PengchuanZhang,HaotianZhang,Jian- Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby,
wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu MahmoudAssran,NicolasBallas,WojciechGaluba,Rus-
Yuan,LeiZhang,Jenq-NengHwang,Kai-WeiChang,and sell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra,
JianfengGao. Groundedlanguage-imagepre-training. In Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu
IEEE/CVF Conference on Computer Vision and Pattern Xu, Herve¬¥ Jegou, Julien Mairal, Patrick Labatut, Armand
Recognition,pages10965‚Äì10975,2022. 2 Joulin, and Piotr Bojanowski. Dinov2: Learning ro-
[53] YijinLi,ZhaoyangHuang,ShuoChen,XiaoyuShi,Hong- bust visual features without supervision. arXiv preprint
shengLi,HujunBao,ZhaopengCui,andGuofengZhang. arXiv:2304.07193,2023. 4
Blinkflow: Adatasettopushthelimitsofevent-basedop- [65] TianboPan, ZidongCao, andLinWang. Srfnet: Monoc-
tical flow estimation. arXiv preprint arXiv:2303.07716, ular depth estimation with fine-grained structure via spa-
2023. 2 tialreliability-orientedfusionofframesandevents. arXiv
[54] ZhengqinLiandJianshengChen.Superpixelsegmentation preprintarXiv:2309.12842,2023. 2
usinglinearspectralclustering. InIEEE/CVFConference [66] AdamPaszke, SamGross, FranciscoMassa, AdamLerer,
onComputerVisionandPatternRecognition,pages1356‚Äì JamesBradbury,GregoryChanan,TrevorKilleen,Zeming
1363,2015. 4 Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,
[55] FengLiang,BichenWu,XiaoliangDai,KunpengLi,Yinan AndreasKopf,EdwardYang,ZacharyDeVito,MartinRai-
Zhao,HangZhang,PeizhaoZhang,PeterVajda,andDiana son,AlykhanTejani,SasankChilamkurthy,BenoitSteiner,
Marculescu. Open-vocabularysemanticsegmentationwith Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An
mask-adaptedclip. InIEEE/CVFConferenceonComputer imperativestyle,high-performancedeeplearninglibrary.In
VisionandPatternRecognition,pages7061‚Äì7070,2023. 2 AdvancesinNeuralInformationProcessingSystems,2019.
[56] DaqiLiu,AlvaroParra,andTat-JunChin. Spatiotemporal 5
registrationforevent-basedvisualodometry. InIEEE/CVF [67] XidongPeng, RunnanChen, FengQiao, LingdongKong,
Conference on Computer Vision and Pattern Recognition, YouquanLiu,TaiWang,XingeZhu,andYuexinMa.Learn-
pages4937‚Äì4946,2021. 2 ingtoadaptsamforsegmentingcross-domainpointclouds.
[57] Youquan Liu, Lingdong Kong, Jun Cen, Runnan Chen, arXivpreprintarXiv:2310.08820,2023. 2,10
WenweiZhang,LiangPan,KaiChen,andZiweiLiu. Seg- [68] YansongPeng,YueyiZhang,ZhiweiXiong,XiaoyanSun,
mentanypointcloudsequencesbydistillingvisionfounda- and Feng Wu. Get: Group event transformer for event-
tionmodels.InAdvancesinNeuralInformationProcessing based vision. In IEEE/CVF International Conference on
Systems,2023. 3,10 ComputerVision,pages6038‚Äì6048,2023. 2
[58] JonathanLong,EvanShelhamer,andTrevorDarrell. Fully [69] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
convolutional networks for semantic segmentation. In Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
IEEE/CVF Conference on Computer Vision and Pattern Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Recognition,pages3431‚Äì3440,2015. 1 Krueger, and Ilya Sutskever. Learning transferable visual
[59] Ilya Loshchilov and Frank Hutter. Decoupled weight de- modelsfromnaturallanguagesupervision.InInternational
cayregularization. InInternationalConferenceonLearn- ConferenceonMachineLearning,pages8748‚Äì8763,2021.
ingRepresentations,2019. 5 2,3,4,9
[60] Ana I. Maqueda, Antonio Loquercio, Guillermo Gallego, [70] UlysseRanc¬∏on,JavierCuadrado-Anibarro,BenoitR.Cot-
NarcisoGarc¬¥ƒ±a,andDavideScaramuzza. Event-basedvi- tereau,andTimothe¬¥eMasquelier.Stereospike:Depthlearn-
sion meets deep learning on steering prediction for self- ing with a spiking neural network. IEEE Access, 10:
drivingcars.InIEEE/CVFConferenceonComputerVision 127428‚Äì127439,2022. 2
andPatternRecognition,pages5419‚Äì5427,2018. 2 [71] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong
[61] Nico Messikommer, Daniel Gehrig, Mathias Gehrig, and Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen
DavideScaramuzza. Bridgingthegapbetweeneventsand Lu. Denseclip: Language-guided dense prediction with
frames through unsupervised domain adaptation. IEEE context-aware prompting. In IEEE/CVF Conference on
Robotics and Automation Letters, 7(2):3515‚Äì3522, 2022. Computer Vision and Pattern Recognition, pages 18082‚Äì
2,6 18091,2022. 4
[62] MohammadMostafavi,Kuk-JinYoon,andJonghyunChoi. [72] HenriRebecq,TimoHorstscha¬®fer,GuillermoGallego,and
Event-intensitystereo:Estimatingdepthbythebestofboth Davide Scaramuzza. Evo: A geometric approach to
worlds. In IEEE/CVF International Conference on Com- event-based 6-dof parallel tracking and mapping in real
puterVision,pages4258‚Äì4267,2021. 2 time. IEEE Robotics and Automation Letters, 2(2):593‚Äì
[63] EmreO.Neftci,HeshamMostafa,andFriedemannZenke. 600,2016. 2
Surrogate gradient learning in spiking neural networks: [73] Henri Rebecq, Rene¬¥ Ranftl, Vladlen Koltun, and Davide
Bringingthepowerofgradient-basedoptimizationtospik- Scaramuzza. High speed and high dynamic range video
24withaneventcamera. IEEETransactionsonPatternAnal- [86] WenhaiWang,EnzeXie,XiangLi,Deng-PingFan,Kaitao
ysisandMachineIntelligence,43(6):1964‚Äì1980,2019. 2, Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.
4,5,6,8,14 Pyramidvisiontransformer:Aversatilebackbonefordense
[74] Corentin Sautier, Gilles Puy, Spyros Gidaris, Alexandre prediction without convolutions. In IEEE/CVF Interna-
Boulch,AndreiBursuc,andRenaudMarlet.Image-to-lidar tional Conference on Computer Vision, pages 568‚Äì578,
self-superviseddistillationforautonomousdrivingdata. In 2021. 6
IEEE/CVF Conference on Computer Vision and Pattern [87] Jianzong Wu, Xiangtai Li, Henghui Ding, Xia Li, Guan-
Recognition,pages9891‚Äì9901,2022. 2 gliang Cheng, Yunhai Tong, and Chen Change Loy. Be-
[75] Stephan Schraml, Ahmed Nabil Belbachir, and Horst trayedbycaptions:Jointcaptiongroundingandgeneration
Bischof. Event-driven stereo matching for real-time 3d foropenvocabularyinstancesegmentation. arXivpreprint
panoramicvision. InIEEE/CVFConferenceonComputer arXiv:2301.00805,2023. 2
VisionandPatternRecognition,pages466‚Äì474,2015. 2 [88] JianzongWu,XiangtaiLi,ShilinXu,HaoboYuan,Henghui
[76] BongkiSon,YunjaeSuh,SunghoKim,HeejaeJung,Jun- Ding, YiboYang, XiaLi, JiangningZhang, YunhaiTong,
Seok Kim, Changwoo Shin, Keunju Park, Kyoobin Lee, Xudong Jiang, Bernard Ghanem, and Dacheng Tao. To-
JinmanPark,JooyeonWoo,YohanRoh,HyunkuLee,Yib- wardsopenvocabularylearning: Asurvey. arXivpreprint
ing Wang, Ilia Ovsiannikov, and Hyunsurk Ryu. A 640√ó arXiv:2306.15880,2023. 2
480dynamicvisionsensorwitha9¬µmpixeland300meps [89] Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, and
address-eventrepresentation. InIEEEInternationalSolid- Chen Change Loy. Aligning bag of regions for open-
StateCircuitsConference,2017. 1 vocabularyobjectdetection. InIEEE/CVFConferenceon
[77] Lea Steffen, Daniel Reichard, Jakob Weinland, Jacques Computer Vision and Pattern Recognition, pages 15254‚Äì
Kaiser,ArneRoennau,andRu¬®digerDillmann. Neuromor- 15264,2023. 2
phic stereo vision: A survey of bio-inspired sensors and [90] Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, and Luping
algorithms. FrontiersinNeuroscience,13:28,2019. 2 Shi. Spatio-temporal backpropagation for training high-
[78] DavidStutz, AlexanderHermans, andBastianLeibe. Su- performancespikingneuralnetworks. FrontiersinNeuro-
perpixels: Anevaluationofthestate-of-the-art. Computer science,12:331,2018. 2
VisionandImageUnderstanding,166:1‚Äì27,2018. 10 [91] Yu-HuanWu,YunLiu,XinZhan,andMing-MingCheng.
[79] ZhaoningSun,NicoMessikommer,DanielGehrig,andDa- P2t:Pyramidpoolingtransformerforsceneunderstanding.
videScaramuzza. Ess:Learningevent-basedsemanticseg- IEEETransactionsonPatternAnalysisandMachineIntel-
mentation from still images. In European Conference on ligence,45(11):12760‚Äì12771,2023. 6
ComputerVision,pages341‚Äì357,2022. 1,2,5,6,7,8,9, [92] Ziyi Wu, Xudong Liu, and Igor Gilitschenski. Eventclip:
10,11,13,14,15,16,17,20,21 Adapting clip for event-based object recognition. arXiv
[80] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob preprintarXiv:2306.06354,2023. 2
Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, [93] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xi-
andIlliaPolosukhin.Attentionisallyouneed.InAdvances aolong Wang, and Shalini De Mello. Open-vocabulary
inNeuralInformationProcessingSystems,2017. 3 panopticsegmentationwithtext-to-imagediffusionmodels.
[81] Zhexiong Wan, Yuchao Dai, and Yuxin Mao. Learning InIEEE/CVFConferenceonComputerVisionandPattern
dense and continuous optical flow from an event camera. Recognition,pages2955‚Äì2966,2023. 2
IEEE Transactions on Image Processing, 31:7237‚Äì7251, [94] JingyiXu, WeidongYang, LingdongKong, YouquanLiu,
2022. 2 Rui Zhang, Qingyuan Zhou, and Ben Fei. Visual foun-
[82] JiachengWang,XiaomengLi,YimingHan,JingQin,Lian- dation models boost cross-modal unsupervised domain
shengWang,andZhouQichao.Separatedcontrastivelearn- adaptation for 3d semantic segmentation. arXiv preprint
ingfororgan-at-riskandgross-tumor-volumesegmentation arXiv:2403.10001,2024. 2,10
withlimitedannotation. InAAAIConferenceonArtificial [95] YanYang,LiyuanPan,andLiuLiu.Eventcameradatapre-
Intelligence,pages2459‚Äì2467,2022. 3 training. InIEEE/CVFInternationalConferenceonCom-
[83] LinWang,YujeongChae,andKuk-JinYoon. Dualtrans- puterVision,pages10699‚Äì10709,2023. 7
fer learning for event-based end-task prediction via plug- [96] Lewei Yao, Jianhua Han, Xiaodan Liang, Dan Xu, Wei
gable event to image translation. In IEEE/CVF Interna- Zhang, Zhenguo Li, and Hang Xu. Detclipv2: Scalable
tionalConferenceonComputerVision, pages2135‚Äì2145, open-vocabulary object detection pre-training via word-
2021. 2,6 regionalignment. InIEEE/CVFConferenceonComputer
[84] Lin Wang, Yujeong Chae, Sung-Hoon Yoon, Tae-Kyun VisionandPatternRecognition,pages23497‚Äì23506,2023.
Kim, and Kuk-Jin Yoon. Evdistill: Asynchronous events 2
toend-tasklearningviabidirectionalreconstruction-guided [97] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and
cross-modalknowledgedistillation. InIEEE/CVFConfer- Liang-Chieh Chen. Convolutions die hard: Open-
ence on Computer Vision and Pattern Recognition, pages vocabulary segmentation with single frozen convolutional
608‚Äì619,2021. 2,6 clip. In Advances in Neural Information Processing Sys-
[85] ShuWang,HuchuanLu,FanYang,andMing-HsuanYang. tems,2023. 2,4,6,8,15,16,17
Superpixel tracking. In IEEE/CVF International Confer- [98] Zelin Zhang, Anthony J. Yezzi, and Guillermo Gallego.
enceonComputerVision,pages1323‚Äì1330,2011. 4 Formulating event-based image reconstruction as a linear
25inverseproblemwithdeepregularizationusingopticalflow.
IEEETransactionsonPatternAnalysisandMachineIntel-
ligence,45(7):8372‚Äì8389,2023. 2
[99] Xu Zheng, Yexin Liu, Yunfan Lu, Tongyan Hua, Tianbo
Pan,WeimingZhang,DachengTao,andLinWang. Deep
learning for event-based vision: A comprehensive survey
andbenchmarks. arXivpreprintarXiv:2302.08890,2023.
1
[100] ChongZhou, ChenChangeLoy, andBoDa. Extractfree
denselabelsfromclip. InEuropeanConferenceonCom-
puterVision,pages696‚Äì712,2022. 2,4,6,8,15,16,17
[101] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Ci-
hang Xie, Alan Yuille, and Tao Kong. Image bert pre-
trainingwithonlinetokenizer. InInternationalConference
onLearningRepresentations,2021. 7
[102] Jiazhou Zhou, Xu Zheng, Yuanhuiyi Lyu, and Lin Wang.
E-clip:Towardslabel-efficientevent-basedopen-worldun-
derstanding by clip. arXiv preprint arXiv:2308.03135,
2023. 2
[103] Zhuyun Zhou, Zongwei Wu, Re¬¥mi Boutteau, Fan Yang,
Ce¬¥dric Demonceaux, and Dominique Ginhac. Rgb-event
fusionformovingobjectdetectioninautonomousdriving.
InIEEEInternationalConferenceonRoboticsandAutoma-
tion,pages7808‚Äì7815,2023. 2
[104] Alex Zihao Zhu, Liangzhe Yuan, Kenneth Chaney, and
Kostas Daniilidis. Unsupervised event-based optical flow
using motion compensation. In European Conference on
ComputerVisionWorkshops,2018. 4
[105] Alex Zihao Zhu, Liangzhe Yuan, Kenneth Chaney, and
Kostas Daniilidis. Unsupervised event-based learning of
opticalflow,depth,andegomotion. InIEEE/CVFConfer-
ence on Computer Vision and Pattern Recognition, pages
989‚Äì997,2019. 2,4
[106] Chaoyang Zhu and Long Chen. A survey on open-
vocabularydetectionandsegmentation: Past,present,and
future. arXivpreprintarXiv:2307.09220,2023. 2
[107] LinZhu,XiaoWang,YiChang,JianingLi,TiejunHuang,
andYonghongTian. Event-basedvideoreconstructionvia
potential-assisted spiking neural network. In IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages3594‚Äì3604,2022. 2
[108] Xiao-Long Zou, Tie-Jun Huang, and Si Wu. Towards a
newparadigmforbrain-inspiredcomputervision.Machine
IntelligenceResearch,19(5):412‚Äì424,2022. 2
[109] NikolaZubic¬¥,DanielGehrig,MathiasGehrig,andDavide
Scaramuzza. From chaos comes order: Ordering event
representations for object recognition and detection. In
IEEE/CVF International Conference on Computer Vision,
pages12846‚Äì128567,2023. 2
26