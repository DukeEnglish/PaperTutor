MIDGARD: Self-Consistency Using Minimum Description Length
for Structured Commonsense Reasoning
InderjeetNair and LuWang
UniversityofMichigan,AnnArbor,MI
{inair, wangluxy}@umich.edu
Abstract whichinvolvesgeneratingtask-specificreasoning
asagraph,suchasextractingargumentstructures
Westudythetaskofconductingstructuredrea-
fromargumentativetext(StabandGurevych,2017;
soning as generating a reasoning graph from
Huaetal.,2019;Mayeretal.,2020;HuaandWang,
natural language input using large language
2022; Qiao et al., 2022), generating structured
models(LLMs). Previousapproacheshaveex-
explanationsthatlayoutcommonsenseknowledge
plored various prompting schemes, yet they
suffer from error propagation due to the au- to connect an argument to a belief (Saha et al.,
toregressive nature and single-pass-based de- 2021), and inferring dependencies among events
coding,whichlackerrorcorrectioncapability. foreverydayactivities(Sakaguchietal.,2021).
Additionally, relying solely on a single sam-
There are two main challenges for struc-
ple may result in the omission of true nodes
tured reasoning tasks. (1) Style discrepancy:
and edges. To counter this, we draw inspira-
Conventional approaches for structured response
tionfromself-consistency(SC),whichinvolves
generation represent the graphs as flattened
sampling a diverse set of reasoning chains
and taking the majority vote as the final an- strings (Madaan and Yang, 2021; Madaan et al.,
swer. To tackle the substantial challenge of 2021; Sakaguchi et al., 2021; Saha et al., 2021),
applyingSCongeneratedgraphs,wepropose leadingtosubparperformanceduetooutputstyle
MIDGARD (MInimum Description length mismatch (Madaan et al., 2022). (2) Error prop-
GuidedAggregationofReasoninginDirected
agation: Any incorrect decisions made earlier in
acyclic graph) that leverages Minimum De-
theautoregressivedecodingprocesscaninfluence
scriptionLength(MDL)-basedformulationto
latergenerationsteps(Yaoetal.,2023). Recently,
identify consistent properties among the dif-
ferent graph samples generated by an LLM.
Madaan et al. (2022) propose COCOGEN to ad-
Thisformulationhelpsrejectpropertiesthatap- dress the issue of style mismatch in structured
pearinonlyafewsamples,whicharelikelyto reasoningtasks,byusingprogrammingscriptsas
beerroneous,whileenablingtheinclusionof promptsforLLMs. Itstillsuffersfromerrorpropa-
missingelementswithoutcompromisingpreci-
gation,sinceitgeneratesvariabledeclarationsand
sion. Ourmethoddemonstratessuperiorperfor-
function calls in order to describe the nodes and
mancethancomparisonsacrossvariousstruc-
edgeswithinthegraph. Anyerrorinthesedeclara-
turedreasoningtasks,includingargumentstruc-
tions/callscanaffectthesubsequentgenerations.
tureextraction,explanationgraphgeneration,
inferringdependencyrelationsamongactions Toaddresstheseissues,wetakeinspirationfrom
foreverydaytasks,andsemanticgraphgenera- theself-consistency(SC)(Wangetal.,2023b)strat-
tionfromnaturaltexts. egythatsamplesdiversereasoningpathsandthen
takesamajorityvoteasthefinalanswer. Theintu-
itionbehindSCisthatsamplingdistinctreasoning
1 Introduction
chainsleadstohigherconfidenceinthecorrectness
While large language models (LLMs) have ofaconsistentanswer. Therefore,wehypothesize
showcased impressive performance in few- that sampling diverse graphs from an LLM can
shot/zero-shotscenariosacrossdiversereasoning constructamoreaccurateaggregategraphandal-
tasks (Brown et al., 2020; Chen et al., 2021; Rae leviateerrorpropagationforstructuredreasoning
et al., 2022; Hoffmann et al., 2022; Chowdhery tasks as any errors made in one sample are less
et al., 2022), it is still challenging to apply these likelytopersistacrossallthegeneratedgraphs.
models for structured commonsense reasoning AcrucialdistinctionbetweenSCandourdesider-
4202
yaM
8
]LC.sc[
1v98150.5042:viXraObjective: run errands during a break in
the rain LLM (CoCoGen Prompting) Greedy
Action List:
0. Get into car and start.
1. Drive to errand location and complete.
2. Think of first errand needed to complete.
3. Leave house when rain stops.
4. Watch out window for rain to stop. …
5. decided to run errands during a break in the rain
6. Run errands during a break in the rain
Temperature Sampling
Figure 1: Comparison of MIDGARD with COCOGEN. In this example, our objective is to infer dependency
relationsamongitemsinthe"ActionList"toachievethespecified"Objective". COCOGENusesgreedydecoding
andexhibitserrorsintheoutput,e.g.,"decidedtorunerrandsduringabreakintherain"isnotconnectedwith
"Drivetoerrandlocationandcomplete". Incontrast,ourapproach MIDGARD (withinthe orange rectangle)
aggregatesrelevantinformationacrossdifferentsamples,resultinginmoreaccurateinference. Forthisexample,our
algorithmimprovedtheperformanceofgreedydecodingfrom66.7to85.7inedgeF -score.
1
atum is that SC focuses exclusively on common- generatedsamples,oursolutionencouragesthein-
sensereasoningtasks(Lingetal.,2017;Clarketal., clusion of graph properties that were present in
2018;Cobbeetal.,2021;Pateletal.,2021;Geva manysamples,whilerejectingpropertiesthatwere
et al., 2021) with scalar answer spaces. In con- onlypresentinafewsampleswhicharelikelyto
trast,weaimtomergemultiplegraphs,eachrep- beerroneous. Figure1showsanexampleofhow
resentingacollectionofunorderedsets(nodesand our approach reduces errors compared to relying
edges). Itisunclearhowtoapplymajorityvoteto solely on a single greedy generation. Empirical
aggregatedistinctsetsofnodesandedges. Inpar- resultsonfourdifferentstructuredreasoningtasks,
ticular,itwouldbecriticaltofilteroutinaccurate includingargumentstructureextraction,structured
nodesandedgesinoursetup. explanationconstruction,andgoal-orientedscript
To achieve this, we propose MIDGARD1, generationandsemanticgraphgeneration,oneight
based on MInimum Description length Guided benchmarksshowthatMIDGARDcanoutperform
Aggregation of Reasoning in Directed acyclic competitive baseline and model variants, demon-
graph. We employ the principle of minimum de- stratingitsstronggeneralizability.
scription length (MDL) (Rissanen, 1978) which
seekstofindthehypothesiswithshortestdescrip- 2 BackgroundandNotations
tion length of the observations. While MDL has
Instructuredreasoning,alabeleddatapointisde-
beenimplementedformodelselection(Grünwald,
notedas(T,G),whereT representstheinputand
2005),causalstructurelearning(LamandBacchus,
G isthetask-specificgraphoutputthatcapturesthe
1993,1994),dataclustering(Rissanen,2000),and
necessaryreasoningknowledge. Forexample, in
dimensionality reduction (Bruni et al., 2022), to
thetaskofargumentativestructureextraction(Stab
thebestofourknowledge,itsuseinautomatically
and Gurevych, 2017), T can be an essay, and G
merging graph samples has never been explored
representstheassociatedargumentativestructure.
before. Assumingthatgraphpropertiesconsistent
To solve this task, we employ LLM in the
acrossmultiplegeneratedsamplesaremorelikely
few-shotpromptingmodewhereN labelleddata-
tobeaccurate,wedefinethedescriptionlengthof
points {T ,G }N are fed as in-context prompt
a graph sample as the weighted sum of the trans- i i i=1
to the model to infer the output for a test input
formations required to convert a hypothesis into
the given sample. By constructing a hypothesis
T. In accordance with the COCOGEN approach,
we construct the in-context prompt as follows:
thatminimizesthedescriptionlengthacrossallthe
p = T ⊕ Gc · T ⊕ Gc · ... · T ⊕ Gc where
1 1 2 2 N N
Gc is a semantically equivalent representation of
1Ourcodeispubliclyavailableathttps://github.com/ i
launchnlp/MIDGARD. G i written in a generally purpose programminglanguage like Python and · (⊕) represents inter 3.1 DefiningDescriptionLength
(intra)-instanceseparator. P c(·,T)representsthe WedefinedescriptionlengthofG′ w.r.t. hypothe-
generativedistributionoftheLLMfortheprompt
sisgraphG asfollows:
p and T. While COCOGEN relies on a single
generation obtained from P c(·,T), our approach ∆ E(G′,G,λ) = λ·a+(1−λ)·d (1)
utilizes this to generate multiple graph samples
where a represents the number of new edges to
{G′ }T ∼ P (·,T) and then aggregates them
i i=1 c beaddedtoG,anddisthenumberofedgestobe
into a single output G. Our novelty lies in the
deletedfromG toconvertittoG’. Weintroducethe
development of a novel and generic aggregation
hyperparameterλ,whichcanbeinterpretedasthe
algorithm for the task of reasoning graph genera-
numberofbitsneededtodescribeasingleaddition,
tion. Thisalgorithmleadstosignificantlyimproved
when(1−λ)bitsareneededtodescribeasingle
performanceacrossmultipletasks.
deletion. The subscript E in Eq. 1 indicates that
onlyedgetransformationsareconsideredwhencal-
culatingthedescriptionlength. Sincethesegraphs
3 The MIDGARD Method
donothaveisolatednodesandeachnodeisassoci-
atedwithatleastoneedge,thedescriptionlength
MIDGARDisbasedontheprincipleofminimum ofG′ canbepreciselycapturedusingedgetransfor-
description length (MDL), which succinctly cap- mationsalone.
tures the regularities in the given data by finding The definition in Eq. 1 is inspired by the for-
thehypothesiswiththeshortestdescriptionlength. mulation proposed by Lam and Bacchus (1994) ,
Ingraphaggregation,MDLcanbeusedasaself- whoapplieditforrefiningcausalgraphsbasedon
consistencystrategytomergemultiplereasoning newdatabutnotforthetaskofgraphaggregation.
graphsamplesintoasingleaggregategraph. The WhileLamandBacchus(1994)assignedequalbit
coreideaistodefineadescriptionlengthforeach requirementsfordescribingasingleadditionand
graphsample,whichisproportionaltothenumber deletion,ourempiricalresultsdemonstratethesig-
oftransformationsrequiredtoconvertahypothesis nificanceofassumingdifferentbitrequirementsto
intothegivensample. Byminimizingthedescrip- achieveenhancedperformance.
tionlengthforsamples{G′}T ,MDLencourages
i i=1 3.2 ExpectedDescriptionLength
theinclusionofgraphpropertiesthatarecommon
We denote the set of nodes and edges associated
acrossdifferentsamples. Thismeansthatproper-
withG asN(G)andE(G)respectively. Similarly,
tiesthatappearfrequentlyinthegeneratedsamples
wedefineNandEasthesetsofallpossiblenodes
are more likely to be accurate and reflect the un-
andedges,suchthateachedge(node)inG′ andG
derlyingstructure. Conversely,propertiesthatare
belongstoE(N). TakingtheexpectationofEq.1
onlypresentinafewsamplestendtobewrong.
w.r.t. G′ ∼ P (,˙T)
c
Inmanystructuredreasoningtasks,thegraphs
typically do not have singleton nodes. For exam- E G′(cid:2) ∆ E(G′,G,λ)(cid:3) =λE G′[a]+(1−λ)E G′[d] (2)
ple,inargumentativestructuresofessays(Staband
(cid:34) (cid:35)
(cid:88)
Gurevych,2017),nodesareeithersupportedorat- E G′[a]=E G′ 1 {e∈E(G′)}·(1−1 {e∈E(G)}) (3)
tackedbyothernodes,ortheythemselvessupport e∈E
(cid:34) (cid:35)
orattackothernodes. Webeginbydefiningthede- (cid:88)
scriptionlengthofagraphG′ basedonthehypoth- E G′[d]=E G′ 1 {e∈/E(G′)}·1 {e∈E(G)} (4)
e∈E
esis G when the graphs do not contain singleton (5)
nodesin§3.1. Next,wederivetheexpressionfor
Aftersimplifyingtheabovesetofequationsand
theexpecteddescriptionlengthin§3.2assuming
representing 1 by the binary variable x ,
thatG′ issampledfromanLLM.Basedonthis,we {e∈E(G)} e
wearriveat:
formulate an objective that aims to minimize the
expecteddescriptionlengthofthesampledgraphs
{G′ i}T
i=1
∼ P c(·,T) in §3.3. We conclude this E G′(cid:2) ∆ E(G′,G,λ)(cid:3) =(cid:88) ((1−λ)−P G′(e))·x e+β (6)
sectionbyproposingmodificationstotheobjective e∈E
toaddressthescenariowherethegraphcanhave whereP (e)representstheprobabilityofobserv-
G′
singletonnodesin§3.4. ing e ∈ E(G′) when G′ ∼ P (·,T) and β is a
cconstantthatisindependentfromthehypothesisG.
1 1
Foreachedgee ∈ E,thedesirabilityofaddinge 1
t bo ett whe eh enyp Pothe (esi )s aG ndis (p 1ro −po λrt )i .on Aal sto tht ehe pd roif bf aer be in lic tye 2 3 4 2 4 1.0 - 0.40 1.0 - 0.40
0.25
- 0.40 G′ 5 3
of e exceeds (1−λ) by a larger extent, its desir- 2 0.25 - 0.40 3 4
a hb igil hit ey rt po rob be ain bc ill iu tyde sd ugin gt eh se tsh ty hp ao tt th he esi es di gn ecr eea wse os ua lds
2
1
4 2
1
3 4
0 0. .22 55
-
0- .40 0.40
0.25 -
0.40 0.25
-
0.400.25 - 0.4 0.0
25 -
0.40
bepresentinasignificantnumberofsamples,sug- 5 6
3 6
gestingitisaconsistentproperty. Conversely,the
presence of (1 − λ) in each coefficient prevents Output from MIDGARD after
Samples Generated By LLM applying objective for Generic
Graphs (1 - λ = 0.40, 1 - λ = 0.40)
theinclusionofedgeswithprobabilitieslowerthan 1 2
(1−λ)intheaggregatedgraph.
Figure2: PictorialrepresentationofGraphAggrega-
tion. Inthefigureabove,theprobabilitiesofnode/edge
3.3 HypothesisSelection
existenceinarandomlygeneratedsamplefromanLLM
We seek to find G that minimizes the expected areestimatedbythenormalizedfrequencyoftheiroc-
description length in Eq. 6. To estimate P G′(e), currence in the samples. The weight of an edge or
we compute the fraction of graph samples from nodeontheright-handsideisdeterminedbysubtracting
P c(·,T) that contains e as one of its edges. For- (1−λ 1)or(1−λ 2)fromthisprobability,respectively.
TheoptimizationinEq.9isequivalenttotheselection
mally,wewishtofindthefollowing:
ofthepropertiesintheaggregatedgraphsuchthatthe
(cid:32) (cid:80)T 1 (cid:33) sumofweightsismaximized. Theboldedelementsare
argm Gin(cid:88) (1−λ)− i=1 Te∈E(G i′) ·x e (7) selectedaccordingtothismaximization.
e∈E
Intheabsenceofanyadditionalconstraints,identi-
wherey isabinaryvariabledenotingthepresence
n
fyingthestructurebecomestrivial—onecansim-
ofninN(G). Notethat,anedge(n ,n )canonly
ply set x to 1 if its coefficient is negative, and 0 1 2
e existifbothn andn arepresentinthegraph. To
1 2
otherwise. However, in various tasks (Stab and
enforcethis,wehavetheconstraint: ∀n ,n ∈ N :
1 2
Gurevych,2017;Sahaetal.,2021;Sakaguchietal.,
y +y −2x ≥ 0.
2021),thegraphsneedtobedirectedacyclicgraphs n1 n2 (n1,n2)
Refer Figure 2 for pictorial representation of
(DAG). Appendix B explains how to restrict the
aggregationusingtheobjectiveinEq.9.
searchspacetoDAGswhenoptimizingEq.7.
4 ExperimentsandAnalysis
3.4 ObjectiveforGenericGraphs
Next,weproposesuitablemodificationstotheob- We evaluate on three major tasks for reasoning
jective to accommodate generic graphs that may graphgeneration: Task1-argumentstructureex-
contain singleton nodes. While Eq. 1 defines the traction on ESSAYS (Stab and Gurevych, 2017),
descriptiononlyintermsofedgetransformations, ABSTRCT(Mayeretal.,2020),andCDCP(Park
wedefinethedescriptionlengthforgenericgraphs and Cardie, 2018); Task 2-generating struc-
asfollows: tured explanations on EXPLAGRAPHS (Saha
et al., 2021); Task 3-script planning on PRO-
SCRIPT (Sakaguchi et al., 2021); and Task 4-
∆(G′,G,{λ ,λ })=∆ (G′,G,λ )+∆ (G′,G,λ ) (8)
1 2 E 1 N 2 semantic graph generation on KELM (Agarwal
where ∆ uses the same form as Eq. 1 but cal- etal.,2021), WEBNLG(Gardentetal.,2017),and
N
culatesthedescriptionlengthassociatedwiththe GENWIKI(Jinetal.,2020). Thereafter,weevalu-
additionandremovalofnodesinordertotransform atehowtheperformanceofourapproachchanges
N(G)intoN(G′). Redoingthestepsdescribedin whenusingdifferentnumbersofsamplesgenerated
§3.2and§3.3yieldsthefollowingobjective: from the LLM. Additionally, we examine the ca-
pability of our approach in handling graphs with
varied complexities. Finally, we assess the influ-
(cid:32) (cid:80)T 1 (cid:33)
argmin(cid:88) (1−λ )− i=1 e∈E(G i′) ·x enceofvaryingthenumberoffew-shotexamples
G e∈E 1 T e andexaminehowcloseourautomaticallychosen
(9)
(cid:32) (cid:80)T 1 (cid:33) hyperparameters are in comparison with the best
+(cid:88) (1−λ )− i=1 n∈N(G i′) ·y
2 T n possible ones. We also analyze the influence of
n∈N
0 4 . 0 - 5 7 . 0
04.0
-
0.1
04.0
-
52.0
04.0
-
0.1
04.0
-
0.1
04.0
-
52.0
04.0
-
5.0
04.0
-
0.1varyingthenumberoffew-shotexamples. Unless more details on these details, please refer the ap-
statedotherwise,wegenerateT = 10samplesfor pendixD.1.
approachesutilizingmultiplesamples. Tocomputetheperformanceofcomponentiden-
tification, we use BIO scheme to label the token
Base LLMs. We evaluate our approach with (a)
gpt-3.5-turbo2, a general purpose instruction- sequences. Thereafter, we compute component
identification F score (C) by tallying the num-
tunedLLMand(b) CODE-LLAMA(Roziereetal., 1
beroftruepositives(TP),falsenegatives(FN),and
2023), a code-LLM pretrained over general pur-
falsepositives(FP)intheassignedtokensequences
pose programming languages. The 16K context
asspecifiedby(Mayeretal.,2020). Toassessthe
lengthassociatedwiththeseLLMsallowsustoem-
performance of relation prediction, we compute
ployfew-shotpromptingforlongsequenceinput-
metricsdenotedbyR andR . TheR metric
outputtaskssuchasargumentstructureextraction. 100 50 100
computestheF -scorebyconsideringaprediction
Comparisons. WefirstconsideraGREEDYbase- 1
astruepositiveonlyiftheheadandtailcomponents
line that represents each graph as a semantically
(andtherelationtype)overlapexactlywiththatof
equivalent programming script and samples only
agroundtruthedge. Ontheotherhand, R con-
50
one generation from the LLM, which is decoded
siders a predicted relation as correct if there is at
greedilyasdonein COCOGEN.
leasta50%tokenoverlapbetweentheheadandtail
Our main model, MIDGARD applies the ob-
components of the prediction and a ground-truth
jectivedescribedin§3.4. Asthegraphsinallthe
relation.
consideredtasksaredirectedacyclicinnatureex-
We observe from Table 1 that MIDGARD
cept semantic graph generation, we additionally
achieves a consistent performance improvement
incorporatetheDAGconstraintsdiscussedin§3.3.
acrosscomponentidentificationandrelationpre-
Forsemanticgraphgeneration,weanalysetheper-
dictionformostofthedatasetsandLLMchoices.
formance of different variants without DAG con-
Withjust10samples,thecomponentidentification
straints. Wefurthercomparewiththreevariantsof
performanceofESSAYSiselevatedby≈ 5%and
MIDGARD:(a) MIDGARD W/O NODE TRNS:
≈ 4%,usinggpt-3.5-turboand CODE-LLAMA
Weusetheobjectivedescribedin§3.3alongwith
respectively. Ourapproachbooststheperformance
the DAG constraints. By excluding the term that
forABSTRCTforrelationpredictionbyover10%
incorporates node transformations in this formu-
when CODE-LLAMAisused.
lation, we can evaluate its impact on the overall
MIDGARDconsistentlyoutperformsotherag-
performance. Specifically, this approach is im-
gregationstrategies,withMIDGARD W/O DAG
plemented by retaining only those edges that oc-
being only slightly inferior. This indicates that
cur more than 1−λ fraction of times while en-
1 MIDGARD W/O DAG canbeusedforargument
suring there are no cycles. Thereafter, only the
mining tasks without a significant decline in per-
nodes present in the retained edges are kept. (b)
formance, even without incorporating DAG con-
MIDGARD (λ = 0.5): We apply the objective
straintsforgraphcombination. However,whenit
proposedbyLamandBacchus(1994)byassuming
comes to component identification, MIDGARD
equal description length of addition and deletion.
W/O NODE TRNS yields poor results due to the
(c)MIDGARD W/O DAGconstraints.
absence of the term describing node transforma-
4.1 Task1: ArgumentStructureExtraction tions. Table1alsojustifieswhyitisimportantto
haveunequaldescriptionlengthsforadditionand
In this task, our goal is to analyze the argumen-
deletion.
tative discourse structure of an input text. This
In our analysis of the models’ errors, we ob-
involvesdetectingandcategorizingallargumenta-
served that LLMs excel in accurately identifying
tivecomponentswithinthetextandidentifyingthe
specificcomponents. However,theytendtomiss
relationships between them. An example of this
capturing all the components present in the data.
taskisshowninAppendixD.1.
Ontheotherhand, MIDGARDeffectivelyassimi-
We assess the performance of our method on
latesrelevantcomponentsfrommultiplesamples,
the following datasets: (1) ESSAYS (Stab and
resulting in improved recall without compromis-
Gurevych, 2017), (2) ABSTRCT (Mayer et al.,
ingprecision(referAppendixE.1forquantitative
2020), (3) CDCP (Park and Cardie, 2018). For
results).
2https://openai.com/chatgpt(Version0613) Furthermore,asshowninTable2,ourapproachESSAYS ABSTRCT CDCP MacroAverage
Approach
C R100 R50 C R100 R50 C R100 R50 C R100 R50
LLM:gpt-3.5-turbo
GREEDY 67.4 21.5 32.6 84.4 38.5 55.2 53.8 11.2 16.2 68.5 23.7 34.7
MIDGARDW/ONODETRNS 65.8 23.5 35.4 83.4 41.1 58.0 48.5 12.4 18.2 65.9 25.7 37.2
MIDGARD(λ=0.5) 65.8 21.8 31.3 83.6 40.5 55.9 54.4 10.7 14.9 57.9 24.3 34.0
MIDGARDW/ODAG 72.3 23.5 35.4 84.0 41.1 57.9 54.8 12.3 17.9 70.4 25.6 37.1
MIDGARD 72.3 23.5 35.4 84.0 41.1 58.0 54.8 12.4 18.2 70.4 25.7 37.2
LLM:CODE-LLAMA
GREEDY 56.3 9.1 21.4 64.2 18.3 25.5 41.9 7.0 9.4 54.1 11.5 18.8
MIDGARDW/ONODETRNS 56.6 11.0 24.5 57 30.7 39.5 34.5 7.8 10.7 49.4 16.5 24.9
MIDGARD(λ=0.5) 49.7 3.4 5.6 62.9 24.9 31.7 36.6 3.3 4.1 49.7 10.5 13.8
MIDGARDW/ODAG 60.3 10.9 24.4 63.4 30.8 39.7 42.5 7.2 9.7 55.6 16.3 24.6
MIDGARD 60.3 11.0 24.5 63.4 30.7 39.5 42.5 7.8 10.7 55.6 16.5 24.9
Table1: Resultsfortheargumentstructureextractiontasks. Theresultswereaveragedacross5randomseeds.
Green and Blue indicatesbestandsecond-bestperformancerespectively.
Typeoferror GrE dS ySA OYS urs GA rdB ySTR OC uT rs GrdC yDC OP urs Approach StCA(↑)SeCE AXP (L ↑A )G GR -A BP SH (↑)GED(↓)
#spuriousedgesincluded 528.2 469.0 200.2 182.0 441.6 411.2 LLM:gpt-3.5-turbo
#trueedgesomitted 777.8 768.0 126.8 120.2 255.6 238.8
#reversededges 30.4 21.8 1.4 0.8 15.8 14.4 GREEDY 23.7 7.6 18.6 84.0
MIDGARDW/ODAG 12.9 2.5 10.3 91.1
MIDGARD(λ=0.5) 4.3 1.6 3.3 97.0
Table 2: Segregation of relation prediction errors MIDGARD 30.3 17.7 22.4 82.1
into distinct categories and assessing how effective LLM:CODE-LLAMA
MIDGARD(Ours)isreducingvarioustypesoferrors GREEDY 36.6 12.4 28.4 75.6
morethanGREEDY(Grdy)decoding. Performanceav- MIDGARDW/ODAG 21.2 12.6 16.1 87.3
MIDGARD(λ=0.5) 0.0 0.0 0.0 100.0
eragedover5randomseeds. MIDGARD 39.4 20.2 29.7 76.4
Table3: ResultsonEXPLAGRAPH. MIDGARD(λ=
is effective in reducing various types of errors 0.5)resultedinnoneoftheedgesbeingincludedinthe
foundintheinferrededges. Wecategorizeerrors finalgraph,astheestimatedprobabilitiesofalledgesin
associated with relation prediction and calculate thesamplesarebelow0.5.
thetotalcountofdistinctedgeerrorsintheinferred
samplesascomparedtothegroundtruthdata.
theinferredandgroundtruthedges. (4)GraphEdit
4.2 Task2: ExplanationGraphGeneration Distance(GED)computesgrapheditsrequiredto
transform the hypothesis to the ground truth. As
WeuseEXPLAGRAPHS(Sahaetal.,2021)forthis
theseevaluationmetricsmeasuretheaccuracyof
task,wherethegoalistopredictwhetheracertain
the graph as a collection of edges, we do not ex-
argumentsupportsorcountersabeliefwhilegener-
periment with MIDGARD W/O NODE TRNS as
atingacommonsenseexplanationgraphthatexplic-
there would be no performance difference from
itlyconveysthereasoningbehindthestancepredic-
MIDGARD.
tion. WerequestthereadertoreferAppendixD.2
WeobservefromTable3that MIDGARD im-
formoredetailsonpromptdesignanddataset.
provestheperformanceofsinglegenerationbased
Weemploythefollowingmetricsrecommended
techniquebyasignificantmarginforbothLLMs.
bytheauthorsofthistask(Sahaetal.,2021): (1)
Unliketheargumentstructureextractiontask,the
StructuralAccuracy(StCA)computesfractionof
performanceissignificantlyworsewhennotusing
graphs that are DAG and has 2 concepts from
theDAGconstraints.
the argument and belief. (2) Semantic Correct-
ness (SeCA) employs a learnt model to measure
4.3 Task3: ScriptPlanning
thesemanticcorrectnessoftheedgesbychecking
whethertheimpliedstancefromthegraphmatches Unliketheprevioustwotaskswhichemphasizeon
thegroundtruth. (3)G-BERTScore(G-BS)mea- constructingthecompletegraphfromscratch,we
surestheBERTScore(Zhangetal.,2019)between investigatewhetherourapproachcanbeusedforGraphEditDistance(GED).
From the Table 7, we can observe that while
our approach outperforms or achieves competi-
tive performance compared to the baseline, the
performance improvement is not significant for
gpt-3.5-turbo. Uponcloserexaminationofthe
outputsgeneratedusingtemperaturesampling,we
havenoticedalackofvariabilitywhencomparedto
Figure3: ResultsforscriptplanningonPROSCRIPT. thestructuredcommonsensereasoningtasksmen-
tioned in the main script. This limited variabil-
inferringrelationsbetweennodesthatarealready ityhinderstheopportunitytoimproveuponeach
known. Toexaminethis,weuse PROSCRIPT(Sak- sample,resultinginalesssignificantperformance
aguchi et al., 2021), which involves generating a boostthanexpected.
graph for achieving a high-level goal, with each
node representing an action and edges indicating 4.5 FurtherAnalyses
dependencyrelationsamongactions. Inoursetup,
Impactofincreasingsamplesize. Toanalyzethe
we provide the set of actions and the goal to the
impact of varying the number of samples gener-
LLM as input and prompt it to generate the se-
atedfromtheLLM,weevaluatetheperformance
quence of edges that capture the dependencies
of MIDGARD ontheargumentstructureextrac-
among the input actions. More details about this
tiontaskasitwouldallowustoexaminethetrend
taskispresentedinAppendixD.3.
on both node identification and edge prediction.
Tocomparetheperformancebetweendifferent
WeonlyshowtheanalysisfortheESSAYSdataset
approaches,weuseF1-score(F )betweenthein-
1 from the argument structure extraction task due
ferrededgesetandthegroundtruth. FromFigure3,
to limited space. Please refer to the appendix for
wecanseethatMIDGARDsignificantlyimproves
additionalplotsandsimilaranalysis.
theperformanceoverthegreedysingle-generation
basedapproach. Thefigurealsodemonstratesthe
importanceofhavingDAGconstraints.
4.4 Task4: SemanticGraphGeneration
Thegoalofthistaskistoextractthesemanticgraph
from an input natural language text as a list of
edges. Eachedgeinthegraphconsistsofasubject,
a property, and the type of property (Han et al.,
2023). An example of this task is shown in Ap- (a)ComponentIdentification (b)RelationPrediction
pendixD.4.
Figure4: PerformanceofMIDGARDincomparison
To gauge the efficacy of our model for such
withGREEDYonESSAYSwhenthenumberofsamples
a task, we consider following datasets: (1) fromtheLLMisvaried. Resultsaveragedover5differ-
KELM(Agarwaletal.,2021),(2)WEBNLG(Gar- entrandomseeds.
dent et al., 2017) and (3) GENWIKI (Jin et al.,
2020). For more details on these datasets, we re- FromFig.4aandFig.4b,weseethattheperfor-
questthereadertoreferAppendixD.4 manceincreasesonlymarginallyemphasizingthat
Weusethefollowingmetricstoassessthequan- returnsdiminishwithincreasingthenumberofsam-
titative performance as suggested by Han et al. ples. Similar trend is observed for other datasets
(2023): (1)Triple-MatchF (T-F )findsthemacro- belonging to the same task (refer Appendix E.2).
1 1
averagedF
1
betweentheedgetriplespresentinthe However,for EXPLAGRAPHS,weobservethatthe
inferenceandthegroundtruthgraphedgetriples. performancesteadilyincreaseswiththenumberof
(2) Graph Match F (G-F ) measures the perfor- samples indicating that having more and diverse
1 1
mance as the number of graphs which exactly explanation graphs is helpful towards improving
matchesthegroundtruthgraphintermsofF score. thefinalaggregatedstructureasshowninFigure5.
1
Finally,asdefinedinthe§4.2,wealsouse(3)G- EfficacyofMIDGARDfordifferentgraphcom-
BERTScore (G-BS) (Zhang et al., 2019) and (4) plexities. We compare the performance betweenApproach
KELM WEBNLG GENWIKI
T-F1(↑) G-F1(↑) G-BS(↑) GED(↓) T-F1(↑) G-F1(↑) G-BS(↑) GED(↓) T-F1(↑) G-F1(↑) G-BS(↑) GED(↓)
LLM:gpt-3.5-turbo
GREEDY 46.9 22.8 84.0 8.7 29.1 15.0 83.6 10.4 23.7 6.5 82.5 11.9
MIDGARD(λ=0.5) 47.0 22.0 83.2 8.9 27.8 13.2 82.4 10.7 24.0 7.0 82.4 11.6
MIDGARD 47.4 22.8 83.5 8.8 29.3 15.0 83.7 10.4 24.3 7.2 83.4 11.5
LLM:CODE-LLAMA
GREEDY 37.9 20.0 63.2 14.1 24.8 6.0 66.4 14.2 12.1 2.0 53.6 17.6
MIDGARD(λ=0.5) 8.8 4.0 45.0 19.8 23.0 6.0 67.3 14.6 7.1 2.0 54.9 18.2
MIDGARD 37.9 12.0 67.7 13.5 26.5 6.0 77.7 12.2 9.7 4.0 58.7 17.3
Table4: ResultsforSemanticGraphGeneration. Ineachofourmethodvariants,wedidnotapplyDAGconstraints,
astheyarenotnecessaryforthistaskunlikethepreviousexperiments.
GREEDY MIDGARD
Bin #Samples Avg.#Nodes Avg.#Edges Avg.Degree
C R C R
50 50
ESSAYS
[5,10) 3 8.0 7.0 0.88 64.7 23.8 64.7 36.2
[10,15) 28 11.2 10.2 0.91 66.2 33.5 70.2 34.8
[15,20) 33 15.6 14.6 0.94 68.5 32.5 73.0 36.4
[20,25) 14 20.6 19.6 0.95 65.7 32.0 75.5 35.5
[25,30) 2 26.0 25.0 0.96 58.3 36.1 70.0 31.2
ABSTRCT
[2,4) 5 2.8 1.4 0.47 79.4 58.2 77.6 59.6
[4,6) 41 4.6 2.7 0.58 83.4 59.8 83.5 63.1
[6,8) 36 6.5 3.6 0.56 88.6 57.9 87.6 59.9
[8,10) 14 8.4 4.0 0.47 83.8 47.0 84.5 48.3
[10,12) 3 10.3 7.0 0.68 70.1 39.6 68.5 50.5
CDCP
[2,7) 96 3.9 1.2 0.26 52.1 20.2 52.4 22.3
[7,12) 33 8.3 3.2 0.39 56.0 21.7 56.9 23.8
[12,17) 13 13.9 3.9 0.27 55.7 8.1 54.9 11.5
[17,22) 3 19.0 7.7 0.40 59.7 10.4 64.4 7.7
[22,27) 2 23.0 4.5 0.20 52.6 1.1 55.8 1.0
Table5: ComponentandRelationidentificationperformancefor GREEDY and MIDGARD fordifferentgraph
complexitieswhengpt-3.5-turboisused. Theresultsareaveragedfor5seeds.
evaluatetheinfluenceofgraphcomplexityonboth
nodeandedgeidentificationperformance.
Inthisanalysis,webinthegraphsbasedonthe
number of nodes and compute the average com-
plexitymetricssuchasnumberofnodesandedges
and degree for the graphs belonging to each bin.
Thehigherthesemetricsare,themorecomplexthe
correspondinggraphis. Foreachmethod,weem-
ploygpt-3.5-turboforgeneratingsamples. We
observethatourapproachprovidedconsistentim-
provementsacrossdifferentcomplexitiesasshown
Figure5: PerformanceofMIDGARDincomparison inTable5.
withGREEDYonEXPLAGRAPHS.
Additional analysis. The impact of varying
the number of few-shot examples on argument
structure extraction performance for GREEDY
MIDGARD andthe GREEDY approachonargu- and MIDGARD is provided in Appendix E.3.
mentstructureextractionacrossvariousgraphcom- MIDGARD consistently improves the perfor-
plexities. Wespecificallyselectargumentstructure mance across different number of few-shot ex-
extractionforthisanalysisbecauseitenablesusto amples. We compare our method and GREEDYagainst a popular decoding technique called NU- ing them to structured commonsense reasoning,
CLEUSSampling(Holtzmanetal.,2020)intheAp- which involves generating complex graph struc-
pendixE.4andfindthatitresultsinpoorerperfor- tures, presents unique challenges (Madaan et al.,
mance. Wedemonstratethatourapproachworks 2022). Additionally,taskswithinstructuredcom-
with gpt-4 for the ESSAYS dataset in Appendix monsense reasoning often require adherence to
E.5. In Appendix E.6, we assess the impact of specificconstraints(Sahaetal.,2021;Sakaguchi
varyingthehyperparameters{λ ,λ }onthefinal et al., 2021), such as directed acyclicity, which
1 2
performanceandcompareitwiththatofautomati- aredifficulttoensuresolelythroughexistingstrate-
callyestimatedhyperparameters(referAppendix gies. Ourapproachisindependentoftheprompting
C.3). Figure 15 shows that while our automatic methodologyandallowsforflexibleincorporation
hyperparameter search reaches near optimal per- oftask-specificconstraintsduringinference.
formanceforcomponentidentification, thereisa
6 Conclusion
scopeforimprovementinrelationprediction.
We proposed a novel approach for enhancing
5 RelatedWorks
theperformanceofstructuredreasoningproblems
whichinvolvegeneratingtask-specificgraphs. Tak-
Sampling based approaches using LLMs. A
ing inspiration from self-consistency, we sample
commonstrategytoaddressmanyNLPandcom-
multiplegraphsfromtheLLManddeviseamech-
monsensereasoningtasksinvolvessamplingmul-
anismtoconstructaggregatedgraph. Throughrig-
tiple solution trajectories LLMs and employing
orousexperimentation,wehavedemonstratedthe
eitherapost-hocstrategy(Fuetal.,2023;Liuetal.,
effectivenessofourapproachacrossvariousstruc-
2023;Wangetal.,2023a)oratrainedrerankerfor
turedcommonsensereasoningtasks.
sampleselection(Cobbeetal.,2021;Lietal.,2023;
Nietal.,2023). However,post-hocapproachesre- Limitations
lyingonLLMevaluationcanbepronetoposition
• Due to our approach’s reliance on generat-
bias(Wangetal.,2023a;Zhengetal.,2023)and
ingmultiplesamples,itcanbecomputation-
difficultyinjudgingresponsecorrectness(Huang
allydemandingandmayrequireasignificant
et al., 2023; Gou et al., 2023). Training-based
amountoftime,particularlywithoutbatched
samplingrequiresadditionallabeleddatafortask-
specific reranking models. The self-consistency inference. Asaresult,practitionersusingen-
terpriseLLMsmayincursubstantiallyhigher
framework is limited to problems with scalar an-
costscomparedtomethodsthatinvolvesingle
swer spaces due to its reliance on majority vot-
generation. This factor makes our approach
ing (Ling et al., 2017; Clark et al., 2018; Cobbe
less desirable in situations where there are
et al., 2021; Patel et al., 2021; Geva et al., 2021).
constraintsoncomputebudgetorlimitedma-
Moreover,existingapproacheslackintegrationof
chineryresources.
information from different samples, potentially
leading to suboptimal solutions. In contrast, our
• Fordatasetsconsistingofgraphswithasmall
MDL-basedformulationassimilatesrelevantinfor-
number of nodes and edges, applying ILP
mationfromdiversestructuredresponseswithout
does not result in significant computational
fine-tuning. By examining consistent properties
overhead. However, it is important to ac-
across samples, we construct an aggregate graph
knowledge that the time complexity of ILP
thatleveragesthestrengthsofeachsample.
solversgrowsexponentiallywiththecomplex-
LLMsforcommonsensereasoning. LLMshave ityoftheproblem. Therefore,modifications
been applied to various domains, including arith- are necessary when applying our approach
metic reasoning (He-Yueya et al., 2023), genera- tosettingswithalargenumberofedgesand
tionofmathematicalproofs(Wellecketal.,2022), nodes. Additionally, as the graph size in-
symbolicreasoning(Weietal.,2022),andlogical creases,itbecomesincreasinglychallenging
reasoning(Srivastavaetal.,2022). Whileprompt- toutilizeLLMseffectivelyingeneratingthe
ingstrategies(Weietal.,2022;Zhouetal.,2022; graph structure. The limited context length
Yao et al., 2022; Wang et al., 2023b; Yao et al., of the LLMs poses a challenge for applying
2023; Madaan et al., 2023) have been proposed themtocommonsensereasoningtasksinvolv-
toimproveperformanceacrossthesetasks,adapt- inglargergraphs. Thislimitationarisesfromthedifficultyofaccommodatingmultiplein- language models trained on code. arXiv preprint
context learning examples within the given arXiv:2107.03374.
contextlength.
AakankshaChowdhery,SharanNarang,JacobDevlin,
EthicsStatement Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
While our methodology attempts to derive struc- Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
turedrepresentationsfromtheinputdataonly,due
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
to the issue of hallucination, the LLMs are not
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
immune to generating biased, insensitive or un- Hutchinson, Reiner Pope, James Bradbury, Jacob
truthfulcontent. Hence,weurgepractitionersand Austin,MichaelIsard,GuyGur-Ari,PengchengYin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
researcherstoexercisecautionwhenapplyingour
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
framework, especially for sensitive applications
VedantMisra,KevinRobinson,LiamFedus,Denny
likepolitics,finance,andhealthcare. Zhou,DaphneIppolito,DavidLuan,HyeontaekLim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
Acknowledgements DavidDohan,ShivaniAgrawal,MarkOmernick,An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
This work is supported in part through National lai,MariePellat,AitorLewkowycz,EricaMoreira,
ScienceFoundationundergrant2302564. Weare Rewon Child, Oleksandr Polozov, Katherine Lee,
ZongweiZhou,XuezhiWang,BrennanSaeta,Mark
gratefulfortheresourcesandservicesprovidedby
Diaz,OrhanFirat,MicheleCatasta,JasonWei,Kathy
AdvancedResearchComputing(ARC),adivision
Meier-Hellstern,DouglasEck,JeffDean,SlavPetrov,
ofInformationandTechnologyServices(ITS)at andNoahFiedel.2022. Palm:Scalinglanguagemod-
theUniversityofMichigan,AnnArbor. Addition- elingwithpathways.
ally,wethankthemembersoftheLAUNCHgroup
PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,
attheUniversityofMichiganfortheirdiscussions
AshishSabharwal,CarissaSchoenick,andOyvind
andsuggestions.
Tafjord.2018. Thinkyouhavesolvedquestionan-
swering? tryarc,theai2reasoningchallenge. arXiv
preprintarXiv:1803.05457.
References
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
OshinAgarwal,HemingGe,SiamakShakeri,andRami
MarkChen,HeewooJun,LukaszKaiser,Matthias
Al-Rfou. 2021. Knowledge graph based synthetic
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
corpusgenerationforknowledge-enhancedlanguage
Nakano,etal.2021. Trainingverifierstosolvemath
model pre-training. In Proceedings of the 2021
wordproblems. arXivpreprintarXiv:2110.14168.
Conference of the North American Chapter of the
AssociationforComputationalLinguistics: Human
JinlanFu,See-KiongNg,ZhengbaoJiang,andPengfei
Language Technologies, pages 3554–3565, Online.
Liu.2023. Gptscore: Evaluateasyoudesire. arXiv
AssociationforComputationalLinguistics.
preprintarXiv:2302.04166.
Sören Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann,RichardCyganiak,andZacharyIves.2007. ClaireGardent,AnastasiaShimorina,ShashiNarayan,
Dbpedia: Anucleusforawebofopendata. InThe andLauraPerez-Beltrachini.2017. TheWebNLG
SemanticWeb,pages722–735,Berlin,Heidelberg. challenge: Generating text from RDF data. In
SpringerBerlinHeidelberg. Proceedingsofthe10thInternationalConferenceon
NaturalLanguageGeneration,pages124–133,San-
Tom Brown, Benjamin Mann, Nick Ryder, Melanie tiagodeCompostela,Spain.AssociationforCompu-
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind tationalLinguistics.
Neelakantan,PranavShyam,GirishSastry,Amanda
Askell,etal.2020. Languagemodelsarefew-shot
MorGeva,DanielKhashabi,EladSegal,TusharKhot,
learners. Advancesinneuralinformationprocessing
DanRoth,andJonathanBerant.2021. Didaristotle
systems,33:1877–1901.
usealaptop? aquestionansweringbenchmarkwith
implicit reasoning strategies. Transactions of the
VittoriaBruni,MariaLuciaCardinali,andDomenico
Association for Computational Linguistics, 9:346–
Vitulano.2022. Ashortreviewonminimumdescrip-
361.
tionlength: Anapplicationtodimensionreduction
inpca. Entropy,24(2):269.
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Shen, Yujiu Yang, Nan Duan, and Weizhu Chen.
Yuan,HenriquePondedeOliveiraPinto,JaredKa- 2023. Critic: Largelanguagemodelscanself-correct
plan, HarriEdwards, YuriBurda, NicholasJoseph, with tool-interactive critiquing. arXiv preprint
Greg Brockman, et al. 2021. Evaluating large arXiv:2305.11738.Peter Grünwald. 2005. Minimum description length Wai Lam and Fahiem Bacchus. 1994. Using new
tutorial. Advances in minimum description length: data to refine a bayesian network. In Uncertainty
Theoryandapplications,5:1–80. Proceedings1994,pages383–390.Elsevier.
JiuzhouHan,NigelCollier,WrayBuntine,andEhsan YifeiLi,ZeqiLin,ShizhuoZhang,QiangFu,BeiChen,
Shareghi.2023. Pive: Promptingwithiterativeverifi- Jian-GuangLou,andWeizhuChen.2023. Making
cationimprovinggraph-basedgenerativecapability language models better reasoners with step-aware
ofllms. arXivpreprintarXiv:2305.12392. verifier. InProceedingsofthe61stAnnualMeeting
of the Association for Computational Linguistics
Joy He-Yueya, Gabriel Poesia, Rose E Wang, and (Volume 1: Long Papers), pages 5315–5333,
NoahDGoodman.2023. Solvingmathwordprob- Toronto,Canada.AssociationforComputationalLin-
lemsbycombininglanguagemodelswithsymbolic guistics.
solvers. arXivpreprintarXiv:2304.09102.
WangLing,DaniYogatama,ChrisDyer,andPhilBlun-
som. 2017. Program induction by rationale gen-
JordanHoffmann,SebastianBorgeaud,ArthurMensch,
eration: Learning to solve and explain algebraic
Elena Buchatskaya, Trevor Cai, Eliza Rutherford,
wordproblems. InProceedingsofthe55thAnnual
DiegodeLasCasas,LisaAnneHendricks,Johannes
Meeting of the Association for Computational
Welbl, Aidan Clark, Tom Hennigan, Eric Noland,
Linguistics (Volume 1: Long Papers), pages 158–
KatieMillican,GeorgevandenDriessche,Bogdan
167, Vancouver, Canada. Association for Compu-
Damoc, Aurelia Guy, Simon Osindero, Karen Si-
tationalLinguistics.
monyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,
andLaurentSifre.2022. Trainingcompute-optimal
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,
largelanguagemodels.
Ruochen Xu, and Chenguang Zhu. 2023. Gpteval:
Nlgevaluationusinggpt-4withbetterhumanalign-
AriHoltzman,JanBuys,LiDu,MaxwellForbes,and
ment. arXivpreprintarXiv:2303.16634.
YejinChoi.2020. Thecuriouscaseofneuraltextde-
generation. InInternationalConferenceonLearning AmanMadaan,DheerajRajagopal,NiketTandon,Yim-
Representations. ingYang,andEduardHovy.2021. Couldyougive
me a hint ? generating inference graphs for de-
Xinyu Hua, Mitko Nikolov, Nikhil Badugu, and feasible reasoning. In Findings of the Association
LuWang.2019. Argumentminingforunderstanding forComputationalLinguistics: ACL-IJCNLP2021,
peerreviews. InProceedingsofthe2019Conference pages5138–5147,Online.AssociationforComputa-
of the North American Chapter of the Association tionalLinguistics.
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), AmanMadaan, NiketTandon,PrakharGupta,Skyler
pages 2131–2137, Minneapolis, Minnesota. Asso- Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
ciationforComputationalLinguistics. Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
et al. 2023. Self-refine: Iterative refinement with
Xinyu Hua and Lu Wang. 2022. Efficient argument self-feedback. arXivpreprintarXiv:2303.17651.
structure extraction with transfer learning and ac-
Aman Madaan and Yiming Yang. 2021. Neural lan-
tive learning. In Findings of the Association for
guagemodelingforcontextualizedtemporalgraph
Computational Linguistics: ACL 2022, pages 423–
generation. InProceedingsofthe2021Conference
437,Dublin,Ireland.AssociationforComputational
of the North American Chapter of the Association
Linguistics.
for Computational Linguistics: Human Language
Technologies, pages 864–881, Online. Association
Jie Huang, Xinyun Chen, Swaroop Mishra,
forComputationalLinguistics.
Huaixiu Steven Zheng, Adams Wei Yu, Xiny-
ingSong,andDennyZhou.2023. Largelanguage
AmanMadaan,ShuyanZhou,UriAlon,YimingYang,
models cannot self-correct reasoning yet. arXiv
andGrahamNeubig.2022. Languagemodelsofcode
preprintarXiv:2310.01798.
arefew-shotcommonsenselearners. InProceedings
of the 2022 Conference on Empirical Methods in
Zhijing Jin, Qipeng Guo, Xipeng Qiu, and Zheng
Natural Language Processing, pages 1384–1403,
Zhang. 2020. GenWiki: A dataset of 1.3 mil-
AbuDhabi,UnitedArabEmirates.Associationfor
lion content-sharing text and graphs for unsuper-
ComputationalLinguistics.
vised graph-to-text generation. In Proceedings of
the28thInternationalConferenceonComputational TobiasMayer,ElenaCabrio,andSerenaVillata.2020.
Linguistics,pages2398–2409,Barcelona,Spain(On- Transformer-basedargumentminingforhealthcare
line).InternationalCommitteeonComputationalLin- applications. InECAI2020,pages2108–2115.IOS
guistics. Press.
Wai Lam and Fahiem Bacchus. 1993. Using causal AnsongNi,SriniIyer,DragomirRadev,VeselinStoy-
information and local measures to learn bayesian anov, Wen-tau Yih, Sida Wang, and Xi Victoria
networks. In Uncertainty in Artificial Intelligence, Lin. 2023. Lever: Learning to verify language-to-
pages243–250.Elsevier. code generation with execution. In InternationalConference on Machine Learning, pages 26106– SwarnadeepSaha,PrateekYadav,LisaBauer,andMohit
26128.PMLR. Bansal.2021. ExplaGraphs: Anexplanationgraph
generationtaskforstructuredcommonsensereason-
Joonsuk Park and Claire Cardie. 2018. A corpus of
ing. In Proceedings of the 2021 Conference on
eRulemakingusercommentsformeasuringevalua-
EmpiricalMethodsinNaturalLanguageProcessing,
bilityofarguments. InProceedingsoftheEleventh
pages7716–7740,OnlineandPuntaCana,Domini-
International Conference on Language Resources
can Republic. Association for Computational Lin-
andEvaluation(LREC2018),Miyazaki,Japan.Eu-
guistics.
ropeanLanguageResourcesAssociation(ELRA).
Keisuke Sakaguchi, Chandra Bhagavatula, Ronan
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
LeBras,NiketTandon,PeterClark,andYejinChoi.
2021. AreNLPmodelsreallyabletosolvesimple
2021. proScript: Partiallyorderedscriptsgeneration.
math word problems? In Proceedings of the 2021
In Findings of the Association for Computational
Conference of the North American Chapter of the
Linguistics: EMNLP2021,pages2138–2149,Punta
AssociationforComputationalLinguistics: Human
Cana,DominicanRepublic.AssociationforCompu-
Language Technologies, pages 2080–2094, Online.
tationalLinguistics.
AssociationforComputationalLinguistics.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
ShuofeiQiao,YixinOu,NingyuZhang,XiangChen,
AbuAwalMdShoeb,AbubakarAbid,AdamFisch,
YunzhiYao,ShuminDeng,ChuanqiTan,FeiHuang,
Adam R Brown, Adam Santoro, Aditya Gupta,
and Huajun Chen. 2022. Reasoning with lan-
Adrià Garriga-Alonso, et al. 2022. Beyond the
guage model prompting: A survey. arXiv preprint
imitation game: Quantifying and extrapolating the
arXiv:2212.09597.
capabilities of language models. arXiv preprint
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie arXiv:2206.04615.
Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susan- Christian Stab and Iryna Gurevych. 2017. Pars-
nah Young, Eliza Rutherford, Tom Hennigan, Ja- ing argumentation structures in persuasive essays.
cobMenick,AlbinCassirer,RichardPowell,George ComputationalLinguistics,43(3):619–659.
van den Driessche, Lisa Anne Hendricks, Mari-
JiaanWang,YunlongLiang,FandongMeng,Haoxiang
beth Rauh, Po-Sen Huang, Amelia Glaese, Jo-
Shi,ZhixuLi,JinanXu,JianfengQu,andJieZhou.
hannes Welbl, Sumanth Dathathri, Saffron Huang,
2023a. Ischatgptagoodnlgevaluator?apreliminary
JonathanUesato,JohnMellor,IrinaHiggins,Anto-
study. arXivpreprintarXiv:2303.04048.
niaCreswell,NatMcAleese,AmyWu,ErichElsen,
SiddhantJayakumar,ElenaBuchatskaya,DavidBud-
XuezhiWang,JasonWei,DaleSchuurmans,QuocVLe,
den,EsmeSutherland,KarenSimonyan,MichelaPa-
EdH.Chi,SharanNarang,AakankshaChowdhery,
ganini,LaurentSifre,LenaMartens,XiangLorraine
andDennyZhou.2023b. Self-consistencyimproves
Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena
chainofthoughtreasoninginlanguagemodels. In
Gribovskaya,DomenicDonato,AngelikiLazaridou,
TheEleventhInternationalConferenceonLearning
ArthurMensch,Jean-BaptisteLespiau,MariaTsim-
Representations.
poukelli,NikolaiGrigorev,DougFritz,ThibaultSot-
tiaux,MantasPajarskas,TobyPohlen,ZhitaoGong,
JasonWei,XuezhiWang,DaleSchuurmans,Maarten
DanielToyama,CypriendeMassond’Autume,Yujia
Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,
Li,TayfunTerzi,VladimirMikulik,IgorBabuschkin,
et al. 2022. Chain-of-thought prompting elicits
Aidan Clark, Diego de Las Casas, Aurelia Guy,
reasoning in large language models. Advances in
Chris Jones, James Bradbury, Matthew Johnson,
NeuralInformationProcessingSystems, 35:24824–
Blake Hechtman, Laura Weidinger, Iason Gabriel,
24837.
WilliamIsaac,EdLockhart,SimonOsindero,Laura
Rimell,ChrisDyer,OriolVinyals,KareemAyoub,
Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh
JeffStanway,LorrayneBennett,DemisHassabis,Ko-
Hajishirzi, and Yejin Choi. 2022. Naturalprover:
rayKavukcuoglu,andGeoffreyIrving.2022. Scaling
Groundedmathematicalproofgenerationwithlan-
languagemodels: Methods,analysis&insightsfrom
guage models. Advances in Neural Information
traininggopher.
ProcessingSystems,35:4913–4927.
J.Rissanen.1978. Modelingbyshortestdatadescrip-
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
tion. Automatica,14(5):465–471.
Thomas L Griffiths, Yuan Cao, and Karthik
Jorma Rissanen. 2000. Mdl denoising. IEEE Narasimhan. 2023. Tree of thoughts: Deliberate
Transactions on Information Theory, 46(7):2537– problemsolvingwithlargelanguagemodels. arXiv
2543. preprintarXiv:2305.10601.
BaptisteRoziere,JonasGehring,FabianGloeckle,Sten Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Shafran,KarthikRNarasimhan,andYuanCao.2022.
Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023. React: Synergizingreasoningandactinginlanguage
Codellama:Openfoundationmodelsforcode. arXiv models. In The Eleventh International Conference
preprintarXiv:2308.12950. onLearningRepresentations.Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q {(n ,n )|n ∈ N,n ∈ N} where n (n ) rep-
1 2 1 2 1 2
Weinberger,andYoavArtzi.2019. Bertscore: Eval- resents the head (tail) of the edge (n ,n ). We
1 2
uating text generation with bert. In International
formulateobjective7asanIntegerLinearProgram-
ConferenceonLearningRepresentations.
ming (ILP) problem by introducing one more bi-
ChujieZheng,HaoZhou,FandongMeng,JieZhou,and naryvariableb e foreachedgee ∈ E. b e issetas1
MinlieHuang.2023. Onlargelanguagemodels’se- ifthereexistsapathfromtheheadofetoitstail.
lectionbiasinmulti-choicequestions. arXivpreprint
UnderILP,weoptimize7subjecttothefollowing
arXiv:2309.03882.
constraints:
Denny Zhou, Nathanael Schärli, Le Hou, Jason
Wei, Nathan Scales, Xuezhi Wang, Dale Schu-
∀e ∈ E : x −b ≤ 0
urmans, Claire Cui, Olivier Bousquet, Quoc V e e
(11)
Le, et al. 2022. Least-to-most prompting enables
complex reasoning in large language models. In
TheEleventhInternationalConferenceonLearning ∀n 1,n 2,n 3 ∈ N : b (n1,n3)−b (n1,n2)
Representations.
−b ≥ −1 (12)
(n2,n3)
A MinimumDescriptionLengthPrinciple
∀n ∈ N : b = 0
The principle of Minimum Description Length (n,n) (13)
(MDL) aims to find a model that can efficiently
representadatasetusingthefewestbits,whilealso
The constraint represented by 11 ensures that
minimizingmodelcomplexity. Insimplerterms,it
there is a path between two nodes if they are di-
seekstofindtheleastcomplexmodelthatcaneffec-
rectly connected by an edge. 12 enforces the re-
tivelycapturetheregularitiesinagivendatasetus-
quirementthatapathmustexistbetweentwonodes
ingtheleastamountofbits. Let’sdenotethedataset
ifthereisapathfromthefirstnodetoathirdnode,
that needs to be represented as D, and the model
andthisthirdnodeisconnectedtothesecondnode.
asH ∈ H. Werepresentthedescriptionlengthof
Lastly, 13 prevents any cycles from occurring in
D whenusingH asL(D|H),whichquantifiesthe
thegraph.
numberofbitsrequiredtodescribeDusingH. Ad-
ditionally,let’sdefineL(H)asthecomplexityof C ImplementationDetails
themodel. Formally,MDLaimstofindtheoptimal
Inthissection,webeginbyexplainingtheconstruc-
solutionfor:
tionofNandEbasedonthesamples{G′ }T ∼
i i=1
H∗ = arg minL(D|H)+L(H) (10) P c(·,T). Thereafter, we describe how the hyper-
H∈H parametersλ andλ areset.
1 2
Toexplainwhateachofthesetermscorresponds C.1 ConstructingNandE
toinourapproach,let’sconsidertheobjectivefor
TobuildN,weiteratethroughthesamples{G′}T .
graphs that do not have singleton nodes in §3.3, i i=1
ItisimportanttonotethateachnodeinG′ consists
while adhering to the constraint that the sought- i
oftwoprimaryproperties: contentandtype. For
aftergraphisaDirectedAcyclicGraph(DAG).In
example, in argument structure extraction (Stab
thisscenario,thehypothesisfamilyHencompasses
and Gurevych, 2017), a node represents an argu-
allgraphswithnodesandedgesinNandE,respec-
mentativecomponentwithcontentindicatingits
tively. Thedatasetinourcaseconsistsofsamples
value and type indicating its category, such as
derived from the LLM, which can be denoted as
premise/claim.
D = {G′}T . The formulation of L(D|H) takes
i=1 When we are iterating over the nodes in
theformofEquation1. Lastly,wedefinethecom- (cid:83)T N(G′ ), we have two choices: append it as
plexityofthemodelL(H)as0ifH isaDAG,and i=1 i
a new node or merge it with some other node
∞otherwise.
presentinN. Tokeeptrackofthehistoricalmerg-
ing of nodes with n ∈ N, we maintain two lists:
B Restrictinghypothesisselectionto
content_list and type_list. These lists store
DAGs
thecontentandtypepropertiesofthenodesthat
To describe the strategy to restrict hypothesis se- have been merged with n over time, respectively.
lection to DAGs, we can express E as the set Moreover, content_list property can also beusedtodecidewhetheranewnodehastomerged.
IftheJaccardsimilaritybetweenthesetoftokens
in the content of the new node and the set of to-
kensinanelementofcontent_listfornexceeds
a pre-defined threshold, we add the content and
typepropertiesofthenewnodetotherespective
listsassociatedwithn. Finally,thesentencefrom
the content_list with the highest Jaccard simi-
laritytotherestoftheelements,andthemodeof
thetype_listofn,arechosenasitscontentand Figure6: Relationsbetweentheargumentativecompo-
type, respectively. The number of samples con- nentsoftheexampleintroducedin§D.1
tainingnissimplythelengthofitscontent_list .
whichcanbeusedtoestimateP (n).
G′
Similarly,weinitializeE = {(n 1,n 2)|n 1,n
2
∈
OncetheLLMgeneratesthegraphasaprogram-
N}. Just like before, each edge in any sample is mingscript,weobtainthecorrespondinggraphG′,
linked to a specific type property that character- a parser is needed to process this output. During
izestheattributeassociatedwithit. Forexample,in sampling from the LLM, we assume that the tex-
argumentstructureextraction,thetypeofanedge
tualresponsecanbeparsedintothecorresponding
canbedefinedasattackorsupport,indicatingthe graphusingatask-specificrule-basedparser.
relationship between the head and the tail of the
edge. Asbefore,weassociatetype_listproperty D Additionalinformationonconsidered
to each e ∈ E, which records the observed type tasks
propertyforthatedgeacrossallthesamplesitap-
D.1 Task1: ArgumentStructureExtraction-
pearsin. Finally,thetypeofeachedgeisthemode
AdditionalInformationontask,prompt
ofitstype_listproperty.
designanddatasets
C.2 ConstructingOptimalAggregateGraph Inthissection,weshowanexampleofthistaskand
describethepromptusedforourexperiments. The
After constructing N and E, we apply an appro-
examplefordemonstratingthistaskistakenfrom
priateformulationoftheobjectivein§3togetthe
oneoftheparagraphsinadatapointbelongingto
optimal values of x (∀e ∈ E) and y (∀n ∈ N).
e n
Thereafter, we return the hypothesis G where
the ESSAYSdataset.
N(G) = {n|y = 1,n ∈ N} and E(G) =
n
First, [cloning will be beneficial for
{e|x = 1,e ∈ E}. If singleton nodes are ab-
e
many people who are in need of organ
sent, we only retain nodes that are present as a
transplants] . [Cloned organs will
headortailinE(G). Claim1 (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)
match perfectly to the blood group and
(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)
C.3 Hyperparameterselection (cid:58)ti (cid:58)s (cid:58)su (cid:58)(cid:58)e (cid:58)(cid:58)o (cid:58)f (cid:58)(cid:58)p (cid:58)a (cid:58)t (cid:58)ie (cid:58)n (cid:58)t (cid:58)s] Premise1 since [ (cid:58)t (cid:58)h (cid:58)e (cid:58)y
can be raised from cloned stem cells
Toautomaticallyselectappropriatevaluesforthe (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)
of the patients] . In addition,
hyperparameters{λ ,λ },weutilizek-foldcross
(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)Premise2
1 2 [it shortens the healing process] .
validation using the few-shot examples. In each
(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)Premise3
Usually [it is very rare to find an
fold,theheld-outsetcomprisesasingledatapoint, (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)
appropriateorgandonor] and[by
whilethetrainingsetconsistsofk−1datapoints. (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) Premise4 (cid:58)(cid:58)
using cloning in order to raise required
(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)
organsthewaitingtimecanbeshortened
C.4 GeneratinggraphsfromLLMs (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)
tremendously] .
(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) Premise5
Foreachdataset,thegraphstructureisencodedas
aprogrammingscriptfollowingtheguidelinesof Table6: Anexampletextwithannotatedargumentative
COCOGEN(referappendixD.1). Multiplesamples components.
aregeneratedfromtheLLMusingatemperature
of0.9. Toaddresstherandomnessinsamplingfew- TheexampleinTable6showsthedifferentargu-
shotexamplesandtemperaturesampling,weuse5 mentativecomponentsinatextalongwiththeircat-
differentrandomseeds. egories. Theobjectiveofargumentstructureextrac-D.2 Task2: ExplanationGraphGeneration:
Additionalinformationonpromptdesign
anddatasets
As we are focusing on the task of generating the
commonsensestructure,weassumethatthestance
isprovidedandpromptthemodeltogeneratethe
structure only as done in Madaan et al. (2022).
Weusethesamepromptschemeasemployedby
COCOGENinrepresentingagraphasaprogram-
Figure7:Programmingscriptpromptusedforargument
mingscriptbydirectlyadoptingtheirimplementa-
structureextraction
tion3.
In this implementation, 30 few-shot instances
were used to prompt the LLM and the approach
tionentailsnotonlytheidentificationofdifferent
wasevaluatedoverthedevelopmentsplitconsisting
argumentativecomponentsbutalsotheprediction
of 396 datapoints. An example explanation task
ofsupportorattackrelationsbetweenthem. The
forthistaskisshowninFigure8forthefollowing
argumentativerelationsbetweenthecomponentsis
belief,argumentandstance:
shown in Figure 6. The equivalent programming
scriptrepresentationoftheaforementionedstruc-
Belief: Factory farming should not be
tureisshowninFigure7.
banned.
Now, we provide some information on the
Argument: Factory farming feeds mil-
datasetsusedtoevaluatevariousapproaches. We
lions.
considered the following 3 datasets. (1) ES-
Stance: Support
SAYS (Stab and Gurevych, 2017) that consists of
essaysobtainedfromessaysforum.com. Eachar-
gumentativecomponentwithintheessaysislabeled
atasub-sentencelevelaseitherapremise,claim,
Factory
Millions
or major claim. The relationships between these Farming
componentsarelabeledaseitherattackorsupport.
causes
Test split consisting of 80 datapoints is used for
evaluation. Werandomlyselect7datapointsfrom Necessary Food
thetrainingsplitinthefew-shotprompt. (2) AB-
STRCT (Mayeretal.,2020)isconstructedbyan-
notatingtheargumentativestructureintheabstracts
Banned
of PubMed articles on Randomized Controlled
Trial of diseases. For our few-shot set, we ran-
domlychoose11datapointsfromthetrainingsplit.
Figure8: Explanationgraphfortheexampleshownin
Thedatasetincludesthreetestsplits: twofromho-
theAppendixD.2.
mogeneous data sources and one constructed by
collectingdatapointsfromvarioussources,includ-
ingthehomogeneousones. Wefocusonevaluating D.3 Task3: ScriptPlanning: Additional
theperformanceofourmodelonthetestsplitcu- informationontask,promptdesignand
ratedfromvarioussources,whichconsistsof100 datasets
datapoints. (3) CDCP (ParkandCardie,2018)is
Theinputin PROSCRIPT (Sakaguchietal.,2021)
obtainedfromapublicforumwhereargumentative
specifies the high-level goal to be achieved and
textsregardingproposedrulesonConsumerDebt
theintermediatestepsrequiredtoachievethegoal.
Collection Practices (CDCP) are annotated with
The task involves inferring a sequence of depen-
argumentativecomponentsandthecorresponding
dencyrelationshipsamongthesesteps,whereeach
support relations. From the training set, we ran-
directedarrowindicatesthatthestepatthearrow’s
domlyselect7datapointsasourfew-shotprompt.
head must be executed before the step at the tail.
We then assess the performance of our model on
thetestsplit,whichconsistsof150datapoints. 3github.com/reasoning-machines/CoCoGen
tsxeertinsoecd
stoaNH
txetnoc
saH
serisedetal.,2023). Anexampleofthistaskisshownin
7.
To gauge the efficacy of our model for such
a task, we consider following datasets: (1)
KELM(Agarwaletal.,2021): Thisisalargescale
syntheticdatasetwhereeachdatapointconsistsofa
sentenceinnaturallanguageandthecorresponding
semanticstructureintheformoflinearizedKnowl-
edgeGraph(KG).Mostofthegraphsinthisdataset
containsatmost6edges. (2) WEBNLG (Gardent
etal.,2017): Thedatapointsinthisdatasetwerecu-
ratedbysamplingtriplesfromtheDBpedia(Auer
et al., 2007). The sentences describing their re-
spectivegraphswerecraftedusingawiderangeof
Figure 9: Script Planning for the goal "bake a cake". lexicalization patterns. (3) GENWIKI (Jin et al.,
Thestepsshowninthefigurearealsoprovidedaspart
2020): Unlikepreviousdatasets,thisonedoesnot
oftheinput. Themodelhastopredictdirectedrelations
provide paired datapoints that map a natural lan-
betweenthestepsthatcapturesthetemporalrelations
guagesentencetoitscorrespondingsemanticgraph
amongthem.
representation. However, atechniqueformulated
by Han et al. (2023) allows for the synthesis of
Weutilizethisdatasettoevaluatetheabilityofvari-
pairwise annotated datasets, which we utilize in
ousalgorithmstoautomaticallydeterminetheorder
ourassessments.
ofoperationsneededtoachievethespecifiedgoal.
Weused15few-shotinstancesforpromptingand E AdditionalAnalysis
assessed the performance of various approaches
over 100 samples from the development dataset. E.1 Precision/RecallanalysisforArgument
structureextraction
AnexampleofthisdatapointisshowninFigure9.
In order to empirically demonstrate the effective-
D.4 Task4: SemanticGraphGeneration:
nessofouralgorithminreducingerrors,wecom-
Additionalinformationontask,prompt
pute the precision and recall in component and
designanddatasets
relation identification for argument structure ex-
traction. Thisanalysisnotonlyallowsustoassess
Input Text: While pop rock can trace its the efficacy of our approach in filtering out false
stylistic roots back to rock music, Reg- properties,butalsoincapturinggenuineproperties
gaemusicevolvedoutofdifferentmusical frommultiplesamplesthatwouldhaveotherwise
genre,knownasska. Interestingly,theTrain beenoverlookedifonlyasinglesamplewasrelied
song,Mermaid,belongstothegenreofpop upon. Instead of using the F -scores of the met-
1
rock,butisalsoconsideredtobeofthereg- rics C and R defined in §4.1, we compute the
50
gaegenreaswell precision and recall of these metrics under same
definition. Specifically, the precision and recall
SemanticStructure: ("MERMAID TRAIN
alongcomponentidentificationisdenotedbyC(P)
SONG", "GENRE", "POP ROCK"), ("MER-
andC(R).Aconsistentnotationisusedforrelation
MAID TRAIN SONG", "GENRE", "REG-
identificationaswell.
GAE"), ("POP ROCK", "STYLISTIC ORI-
FromtheTable8,MIDGARDconsistentlyim-
GIN", "ROCK MUSIC"), ("REGGAE",
provestherecallforcomponentandrelationidenti-
"STYLISTIC ORIGIN", "SKA")
ficationacrossalldatasetsasitreliesonmultiple
Table7: AnexampleforSemanticGraphGeneration. samples to formulate the final hypothesis, effec-
tively addressing the issue of omitting true prop-
The goal of this task is to extract the semantic ertiesthatwouldariseifreliedonasinglesample
graphfromaninputgraph,whichisrepresentedas alone. Moreover,utilizingtheconsistenciesamong
alistofedges. Eachedgeinthegraphconsistsofa thesamplesleadstoimprovedprecisionforrelation
subject,aproperty,andthetypeofproperty.(Han identification,therebyhelpingreducethenumberESSAYS ABSTRCT CDCP
Approach
C(P) C(R) R (P) R (R) C(P) C(R) R (P) R (R) C(P) C(R) R (P) R (R)
50 50 50 50 50 50
GREEDY 77.7 59.6 37.4 28.9 86.9 81.9 50.1 61.4 55.0 52.6 13.4 21.0
MIDGARD 74.0 70.7 40.5 31.9 86.0 82.1 53.7 63.3 55.9 53.7 14.4 25.9
Table8: ComponentandRelationIdentificationprecisionandrecallforMIDGARDandGREEDY. PandRwithin
theparenthesesrepresentprecisionandrecallrespectively.
ofspurioussamples. Whiletheprecisionforcom-
ponentidentificationisslightlyimpacted,adjusting
the value of λ allows us to achieve higher preci-
1
sionatthecostofslightlyreducedrecall.
E.2 Impactofincreasingthenumberof
samplesforotherargumentstructure
extractiontasks
(a)ComponentIdentification (b)RelationPrediction
Figure12: PerformanceofMIDGARDincomparison
withGREEDYfortheESSAYSDatasetwhenthenumber
offewshotexamples(N)isvaried. Resultsaveraged
over5differentrandomseeds.
(a)ComponentIdentification (b)RelationPrediction
Figure 10: Performance of MIDGARD in compari- {3,5,7,9})inthecontextofargumentstructureex-
sonwithGREEDYfortheABSTRCTDatasetwhenthe
tractionwhen10samplesareusedfromtheLLM.
number of samples from the LLM is varied. Results
AsshowninFigure12,MIDGARDconsistently
averagedover5differentrandomseeds.
enhances the performance of GREEDY approach
acrossdifferentnumbersoffewshotexamples. The
plotsfor ABSTRCT and CDCP areshowninFig-
ure13andFigure14respectively.
(a)ComponentIdentification (b)RelationPrediction
Figure11: PerformanceofMIDGARDincomparison
withGREEDYfortheCDCPDatasetwhenthenumber
ofsamplesfromtheLLMisvaried. Resultsaveraged
over5differentrandomseeds.
E.3 Impactofchangingthenumberof (a)ComponentIdentification (b)RelationPrediction
few-shotexamplesforargumentstructure
Figure 13: Performance of MIDGARD in compari-
extraction
sonwithGREEDYfortheABSTRCTDatasetwhenthe
We assess the effectiveness of our approach for number of few shot examples (N) is varied. Results
different numbers of few-shot instances (N ∈ averagedover5differentrandomseeds.(a)ComponentIdentification (b)RelationPrediction (a)ComponentIdentification (b)RelationPrediction
Figure14: PerformanceofMIDGARDincomparison Figure15: Assessingtheperformanceofouralgorithm
withGREEDYfortheCDCPDatasetwhenthenumber fordifferentvaluesof{λ 1,λ 2}andcomparingitwith
offewshotexamples(N)isvaried. Resultsaveraged that of automatically estimated hyperparameters. Re-
over5differentrandomseeds. sultsaveragedover5differentrandomseeds.
E.4 ComparisonwithNucleusSampling
cludes 80 test data points, the estimated cost for
WhileourevaluationsconsideredGREEDYdecod- GPT-4analysisacross5randominstancesoffew-
ing,wealsocompareagainstthe NUCLEUSdecod- shottrainingexamplescouldexceed$1000. Given
ing (Holtzman et al., 2020), a popular technique that other argument structure extraction datasets
to combat neural text degeneration, for the task comprise over 80 test points, the expected infer-
of argument structure extraction in ESSAYS. As encecostswouldsignificantlyincrease. Theresults
shownintheTable9,theapplicationofNUCLEUS forthe ESSAYSdatasetaretabulatedinTable10.
decoding degrades the performance significantly
forboththeconsideredLLMs. Approach C R 100 R 50
LLM:gpt-4
Approach C R R
100 50
GREEDY 77.1 30.7 40.9
LLM:gpt-35-turbo
MIDGARD 78.1 32.9 42.8
GREEDY 67.4 21.5 32.6
NUCLEUS 64.1 19.2 31.2 Table 10: Comparison of different approaches imple-
MIDGARD 72.3 23.5 35.4
mentedongpt-4forESSAYSDataset
LLM: CODE-LLAMA
E.6 Hyperparameters
GREEDY 56.3 9.3 21.4
NUCLEUS 48.0 6.7 16.7 In this experiment, we vary λ 1,λ
2
∈
MIDGARD 60.3 11.0 24.5 {0.0,0.1,0.2,...,1.0} and compute the per-
formanceofcomponentidentificationandrelation
Table9: Comparisonofdifferentapproachesongpt-35- prediction on ESSAYS, and compare with that
turboandCode-LLAMAmodels.
of hyperparameters automatically estimated (see
Appendix C.3 for more details). Specifically,
when varying λ , we set λ to 1 in order to
1 2
E.5 Performanceforgpt-4
include all nodes in the hypothesis and focus
Due to the prohibitive expense associated with solelyonstudyingtheinfluenceofλ onrelation
1
gpt-4, we were limited in assessing its perfor- prediction. Ananalogousstepisrepeatedtostudy
manceacrossalltasks. However,wehavesuccess- theinfluenceofλ oncomponentidentification.
2
fully evaluated its capabilities on the Argument InFigure15,weobservethattheautomatically
StructureExtractiontaskusingaselectivesubset estimatedhyperparameter(λ )forcomponentiden-
2
of 20 data points from the Essays Dataset. This tification is near optimal performance. However,
specificevaluationthoroughlyaddressesboththe thereisroomforimprovementinselectingλ . Ad-
1
identificationofcomponents(nodeevaluation)and ditionally,wefindthattheoptimalvaluesforboth
the prediction of relations (edge evaluation), of- hyperparametersareabove0.5,suggestingthatthe
fering a more comprehensive analysis compared descriptionlengthofinsertionisgreaterthanthat
to other tasks. For the Essays dataset, which in- ofdeletion,asdiscussedinSection3.2.F Intuitiveexplanationforhaving
unequaldescriptionlengthswith
additionversusdeletion
To define a single deletion, it requires ∝
log (|E(G)|)tospecifytheedgetobedeletedfrom
2
G. On the other hand, to describe the edge to be
addedoneneedstospend∝ log (|E|)bits. Clearly,
2
log (|E|) ≥ log (|E(G)|)asE ⊇ E(G).
2 2