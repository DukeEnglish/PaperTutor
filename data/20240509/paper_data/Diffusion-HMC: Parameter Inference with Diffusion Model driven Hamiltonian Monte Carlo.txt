Diffusion-HMC: Parameter Inference with Diffusion Model driven Hamiltonian Monte
Carlo
Nayantara Mudur,1,2 Carolina Cuesta-Lazaro,2,3,4 and Douglas P. Finkbeiner1,2
1Department of Physics, Harvard University, 17 Oxford Street, Cambridge, MA 02138, USA
2Harvard-Smithsonian Center for Astrophysics, 60 Garden Street, Cambridge, MA 02138, USA
3The NSF AI Institute for Artificial Intelligence and Fundamental Interactions
Massachusetts Institute of Technology, Cambridge, MA 02139, USA
4Department of Physics, Massachusetts Institute of Technology, Cambridge, MA 02139, USA
Diffusion generative models have excelled at diverse image generation and reconstruction tasks
acrossfields. Alessexploredavenueistheirapplicationtodiscriminativetasksinvolvingregression
orclassificationproblems. Thecornerstoneofmoderncosmologyistheabilitytogeneratepredictions
forobservedastrophysicalfieldsfromtheoryandconstrainphysicalmodelsfromobservationsusing
these predictions. This work uses a single diffusion generative model to address these interlinked
objectives – as a surrogate model or emulator for cold dark matter density fields conditional on
inputcosmologicalparameters,andasaparameterinferencemodelthatsolvestheinverseproblem
of constraining the cosmological parameters of an input field. The model is able to emulate fields
with summary statistics consistent with those of the simulated target distribution.
We then leverage the approximate likelihood of the diffusion generative model to derive tight
constraintsoncosmologybyusingtheHamiltonianMonteCarlomethodtosampletheposterioron
cosmological parameters for a given test image. Finally, we demonstrate that this parameter infer-
ence approach is more robust to the addition of noise than baseline parameter inference networks.
I. INTRODUCTION network is learned on the fields [8, 9].
Generative models are a class of machine learning ap-
Ongoingandupcomingmissions,suchastheDarkEn- proaches that enable one to simulate the ability to draw
ergy Spectroscopic Instrument DESI, the Vera C. Rubin samples from a complicated target probability density
Observatory’sLegacySurveyofSpaceandTimeandthe and include variational autoencoders, normalizing flows
Nancy Grace Roman Space Telescope will map the cos- and generative adversarial networks (GANs). Diffusion
mos at unprecedented resolution and volume. This has generative models [41] involve a forward diffusion (nois-
createdaproportionatedemandforsimulationsthatcan ing) process that transform samples from the target dis-
generate predictions from theory. Cosmological simula- tribution to those from the standard normal. In the
tions, however, are expensive to run, and can only be denoising diffusion probabilistic model (DDPM)[16], the
generatedforalimitedsetofinitialconditionsandpoints noising process consists of a variance schedule β t over a
in parameter space. fixednumberoftimesteps,T,thatdeterminestheincre-
The canonical summary statistic used for parameter mentalnoiseaddedtotheimage. Sincethediffusionpro-
inference is the two point correlation function, or the cess can be formulated as a stochastic differential equa-
power spectrum Pk at large, linear scales where pertur- tion (SDE) the DDPM variance schedule corresponds to
bation theory holds. At smaller scales, however, grav- thediscretizationofthisSDE.Inthegenerativedirection,
itational collapse induces non-Gaussianity in the fields. a neural network or score model is used to parameterize
Thismeanstheinformationcontentofcosmologicalfields the reverse transformation.
is not fully captured by the power spectrum at the large Diffusion models are alternatively referred to as a
scales alone. For example, [11] derived 27% stronger score-based generative model since parameterizing the
constraints of σ from the two point correlation func- reverse diffusion process is equivalent to to learning the
8
tion of galaxy clustering data by analyzing non-linear ‘score’ or ⃗ logp (x) of the data [1, 41]. Since in high
t
modes at smaller scales, (k >0.25 hMpc−1). A mul- dimensions∇ , the target distribution invariably lies on a
titude of other statistics – such as the marked power thin manifold, the incremental addition of the random
spectrum, the bispectrum, the wavelet scattering trans- noise, blurs the distribution and makes the score pro-
form, and void probability functions – have also been gressively easier to learn. Diffusion models are the un-
devised [12, 28, 32, 44] in an effort to capture higher or- derlyingmathematicalframeworkthathavegivenriseto
der correlations in non-Gaussian fields. Previous work thephotorealisticimagegenerationsuccessesofDALL.E
[14, 23, 28, 38, 45] has addressed the prohibitive cost and Stable Diffusion [34] and have been shown to mit-
of simulations by creating emulators or surrogate mod- igate mode collapse, a phenomenon, often encountered
els that learn to interpolate between predictions of a with GANs a generative model fails to generate multiple
specific summary statistic between training points using modes in a distribution. In scientific applications, they
formalisms such as Gaussian processes. More recently, have used for problems involving protein folding and lig-
simulation-basedinferenceatthefieldlevelhasalsobeen andpredictionandmedicalimagingreconstruction[7,40]
used,wherealikelihoodmodelparameterizedbyaneural . They have been applied in astrophysics to reconstruc-
4202
yaM
8
]OC.hp-ortsa[
1v55250.5042:viXra2
tion problems involving dust [15, 22], cosmological simu- preserving way.
lationsandinitialconditionsreconstruction[8,20,27,36]
(cid:112)
and strong lensing problems [18, 33]. For t [0,T 1],q(x x )= ( 1 β x ,β I)
t+1 t t t t
∈ − | N −
In this work, we apply diffusion generative models and q(x x )= (√α¯x ,(1 α¯)I)
t+1 0 t 0 t
to emulating cold dark matter density fields conditional | N −
t
on cosmological parameters and demonstrate that the (cid:89)
where α¯ = 1 β (1)
trainedmodelcanalsobeusedtoderivetightandrobust t − t′
constraints on cosmological parameters. In Section III, t′=0
weexaminetheabilityofthemodeltoappropriatelycap-
The score / noise-predictor model is a U-Net [35] sim-
turethestatisticsofthedistributionoffieldscorrespond-
ilar to that used in [16]. There are 4 downsampling lay-
ing to different parameters, and further compare the ef-
ers, with each layer consisting of 2 ResNet blocks [53],
fect of modulating a single parameter at a time on the
group-normalization [52], and attention [39, 46]. We use
statistics of the resulting fields in the true and the gen-
circular convolutions in the downsampling layers since
erated set. We then quantify the ability of the model to
theinputfieldshaveperiodicboundaryconditions. Each
capture the full range of cosmic variance for a single pa-
parameter is normalized to lie between [0, 1] with re-
rameter,asameanstoassesstheextentofmodecollapse.
spect to its range, Ω [0.1,0.5],σ [0.6,1.0]. A
m 8
In Section IV, we examine how the diffusion model’s ∈ ∈
multilayer perceptron (MLP)transforms the cosmology
approximate likelihood can be used to solve the inverse
vector into a space with the same dimension as the time
problemofconstrainingthecosmologicalparametersofa
embedding, and each ResNet block additionally has an
given input field. We then use Hamiltonian Monte Carlo
MLP conditional on cosmology. The variance schedule
[3, 10, 24] to draw samples from the estimated posterior
β isnon-linearwithsmallerstepsatsmallertandlarger
t
on the cosmological parameters given an input field, and
steps for larger values of t (see Figure 8). We train the
compare our estimates with a power spectrum baseline.
model to generate the log of the fields, and randomly
AnovelcontributionofthisworkisouruseofanHMCto
rotate and flip the image to account for these invari-
sample a posterior consisting of an approximation to the
ances. During training, for each batch of images x ,
0
diffusion model conditional likelihood to solve a down-
a batch of timesteps is sampled uniformly along with
stream inference task. Finally, we demonstrate that the
a noise pattern ϵ (0,I). The loss function mini-
Diffusion-HMC based parameter inference estimates are ∼ N
mized is ϵ ϵ (√α¯ x +√1 α¯ ϵ,t,θ)) 2 for each set
ϕ t 0 t
morerobusttotheadditionofuncorrelatednoiserelative || − − ||
of x ,t,θ,ϵ , where θ is the parameter vector. In prac-
0
totheestimatesfromadiscriminativeneuralnetworkdi- { }
tice,weminimizetheHuberloss,whichbehavesasanL1
rectly trained to estimate parameters.
lossforlargervaluesandL2forsmallervaluesoftheloss.
We used the Weights and Biases framework [4] to track
experiments. The model has 31.2 million parameters.
II. DATASETS, ARCHITECTURE AND c. Training We first downsample the images by a
TRAINING
factor of 4 and train the conditional diffusion model on
these 64x64 images for 60000 iterations. Since the UNet
a. Datasets WeworkwithcoldDarkMatterdensity isformulatedintermsofrelativedownsampling,thesame
fields at z = 0 from the IllustrisTNG [25, 30] suite from architecturecanbeappliedtoimageswithdifferentreso-
the CAMELS Multifield Dataset (CMD) [47, 49]. The lution. To train the model to emulate 256x256 fields, we
dataset contains of 1000 simulations for 1000 different initialize the checkpoint with the weights of the 64x64-
cosmologies with 15 two-dimensional fields per simula- trained model after 60000 iterations and trained for over
tion. The ‘cosmology’ is parameterized by a parameter 400000iterations. Wefoundthatinitializingthe256x256
vectorwith2cosmological(Ω andσ )and4astrophys- model with the weights of the 64x64 model and then
m 8
ical parameters. Each simulation tracked the evolution training the model led to faster convergence.
of2563 darkmatterparticlesand2563 fluidelementsand The noise prediction loss does not fully capture sam-
tookaround6000CPUhourstogenerate(see[47,49]for ple quality and convergence, and we need an alterna-
more details). The fields span 25 Mpc.h−1 on each side. tivemetrictoassessthequalityofthegeneratedsamples
We train on 70% of the parameters in the LH set, i.e. [43]. We sampled 500 fields (with 50 fields for 10 differ-
700parametersor10,500fields, andconditionthemodel entvalidationparameters)forthecheckpointsafter200k,
only on the cosmological parameters Ω and σ . Ex- 220k, 240k, 260k, 280k, 300k, 320k and 340k iterations.
m 8
ample generated dark matter fields are shown in Figure We computed the reduced chi-squared statistic (Equa-
2. tion 10) of the power spectrum of each generated field,
b. Diffusion Model Setup We follow the denois- s,withrespecttothereferencedistributioncomprisedof
ing diffusion probabilistic model (DDPM) formalism, in the 15 true fields for that parameter. We then compute
which a target image x is transformed to a sample from the mean and standard error of these values across all
0
x (0,I)overthecourseofT =1000timesteps. The parameters and sampled fields.
T
∼N
forward diffusion process follows an incremental noise Whilethediffusiongenerativemodelistrainedtogen-
schedule β , and the noise is added in a variance- erate the fields in log space, the power spectra we com-
t
{ }3
Ω =0.36σ =0.63 Ω =0.18σ =0.95 Ω =0.23σ =0.81 Ω =0.41σ =0.80 Ω =0.26σ =0.75
102 m 8 m 8 m 8 m 8 m 8
True
Generated
101
100
10−1
101 102 101 102 101 102 101 102 101 102
k(hMpc−1) k(hMpc−1) k(hMpc−1) k(hMpc−1) k(hMpc−1)
1.50
True
1.25 Generated
1.00
0.75
0.50
0.25
0.00 10 11 12 13 10 11 12 13 10 11 12 13 10 11 12 13 10 11 12 13
logδ logδ logδ logδ logδ
FIG.1. Generated fields at different cosmologies. Upper Row: Powerspectrumoftheunloggedfieldsforfivedifferent
validation parameters. The lines depict the mean power spectrum and the envelope indicates the 16th and 84th percentiles of
thedistribution. Truesimulationsareshowninblack,generatedinblue. LowerRow: Meanandstandarddeviationenvelopes
for the density histograms of the log fields.
pute here and in Figures 1- 3 correspond to the over- Varying cosmology in a Latin Hypercube (LH):
density power spectra of the ‘linear’ (10GeneratedFields) We examine the consistency of the summary statistics of
fields. The checkpoint corresponding to the 260kth iter- the distribution of true and generated fields for a given
ation had the lowest value, corresponding to 2.29 0.49. validation parameter from the LH set in Figure 1. We
±
To put this number in perspective, we can examine the have 15 true fields and 50 generated fields for each pa-
effectofcosmicvarianceonthismetricusingaleave-one- rameter. We derived the boundaries of the envelope us-
outcross-validationapproach,bycomputingthereduced ing the estimates of the 16th and 84th percentiles, while
chi-squared statistic of each sample of a true field, using the solid line demarcates the mean of the distribution of
the 14 other true fields corresponding to the same pa- power spectra in 35 log spaced k bins. The lower panel
rameter as the reference distribution. The mean of this depicts the density histograms of the log fields. The en-
valueacrossthe10parametersis1.70 0.36. Weusethe velopesareagainderivedusingthepercentiles, whilethe
±
260k checkpoint for our analysis. We plot these values, solid lines indicate the means of the histograms for the
alongwiththereducedchi-squaredstatisticofthepower true and generated fields. The distribution of the power
spectra of the log fields and the p values of the mean spectra and the density histograms of the true and the
intensity in Appendix 3a. generated fields are in good agreement with each other.
Varying cosmological parameters one at a time
(1P): In Figure 2, we generated ‘1P’ sets and exam-
III. SUMMARY STATISTICS
ined whether the effect of modulating a single param-
eter, while keeping the others constant is the same as
In this section we examine the consistency of our gen- is observed in the 1P CAMELS suite. We sample 15
erated fields relative to the true fields using three sets fields corresponding to 15 different seeds for each of the
of simulations under the IllustrisTNG CAMELS suite. parameters. The fields corresponding to the same seed
The latinhypercube, LH, suite varies the largest range across parameters have the same position and orienta-
of cosmological and astrophysical parameters but has a tion of their seeded structures as visible in Figure 2.
limited number (15) of fields for each parameter, with For each seed, we then compute the ratio of the power
all parameters varying randomly. The one-parameter, spectrum of a field at a different parameter to the power
1P, suite consists of 15 fields with the same seed, and spectrum of the fiducial parameter value, and compute
one dimensional variations of each parameter modulated the average and the percentile-based standard deviation
systematically, while the others are fixed to the fiducial envelopesacrossallseedsasdepictedintherightcolumn
value. The Cosmic Variance, CV, set consists of 405 of Figure 2. The dashed (solid) line and envelope corre-
fields for the fiducial parameter value. Sampling a batch spond to the generated (true) ratios. The ratios for the
of 50 256 256 fields from our model takes 310 seconds generated ‘1P’ set are in good agreement with the those
×
(6.2s/field). of the true ‘1P’ set, since the dashed lines are within the
)k(P4
Ωm,σ8=0.1,0.8
101
100
Ωm,σ8=0.5,0.8 Ωm=0.1
Ωm=0.18
Ωm=0.3
10−1
Ωm=0.42
Ωm=0.5
Ωm,σ8=0.3,0.6
100 101
101
100
Ωm,σ8=0.3,1.0 σ8=0.6
σ8=0.68
σ8=0.8
10−1
σ8=0.92
σ8=1.0
100 101
Wavenumberk(hMpc−1)
FIG.2. Generated ‘1P’ fields. Left column: Generatedfieldscorrespondingtotheextremevaluesofeachparameterfora
single seed, with the other value held fixed at the fiducial value (0.3 for Ω and 0.8 for σ ). Middle column: Power spectra
m 8
ofthegeneratedfieldsforthesameseed,fordifferentvaluesofeachparameter,holdingtheotherfixed. Right column: Mean
andstandarddeviationfortheratioofthepowerspectraatthemodifiedparametervaluetothepowerspectraforthefieldat
the fiducial parameter value (black) for 15 slices from the CAMELS dataset (solid) and 15 seeds for the generated fields from
the diffusion model (dashed). The effect of modulating a parameter on the generated fields’ power spectra is consistent with
that of the true fields.
solid envelopes. distribution of 450 estimates of the standard deviation
Reproducing cosmic variance (CV): The CV set for each set, where the ith estimate corresponds to the
has405(27 15)fieldsforthefiducialparametervalueof standard deviation computed using all samples exclud-
[0.3,0.8]and× varyinginitialconditions,designedtoquan- ing that of the ith field. We then compute the mean and
tifytheeffectofcosmicvariance. TheCVsetallowsusto the standard deviation for these sets in order to capture
quantify the consistency between the second moments of the mean and the standard deviation on the estimate of
thetrueandthegenerateddistributions. Inparticular,it thestandarddeviationintheupperrightpanelofFigure
allowsustotesttheabilityofthemodeltogenerateadi- 3. We use the ratios of the jackknife-estimated means
versesetofsamplesforthesamecosmologicalparameters and errors in the lower right panel.
such that it reproduces the true underlying distribution
The ability to capture the full diversity of k modes for
at fixed cosmology.
a single parameter may be in tension with the ability
We generate 405 samples from our trained diffusion to distinguish between and appropriately modulate the
model, compute their power spectra, and examine the power spectrum for different parameters. This tension is
standard deviation, and the correlation matrices of dif- enhancedforourassessmentofmodecollapseinspectral
ferentkmodesofthepowerspectruminFigures 3and 4. space, compared to canonical machine learning datasets
The correlation between the modes of the power spectra involving discrete classes, since the cosmological param-
is largely consistent between the true and the generated eters that we condition the diffusion generative model
samples, although the generated samples appear to have on lie on a continuum. Thus, for a given conditioning
a slight excess correlation around 5 hMpc−1. parameter, a generated field with too much or too little
The standard deviations of the distribution are also poweroncertainscalesrelativetothemeanofthestatis-
consistent although the standard deviations of the gen- tic is more likely to wander into the typical set of a field
erated power spectra are slightly underpredicted relative with a different cosmological parameter since modulat-
to the true power spectra at the largest length scales ing the parameters also modulates the power spectrum
(k < 5hMpc−1). To construct the standard deviation (asinFigure 2). Inthecontextofnaturalimages,oneof-
estimator, we use the jackknife-approach to generate a ten deals with categorical descriptors, where the bound-
)k(P
)k(P5
aries between whether an object qualifies as one class of the variational lower bound thus encode dependencies
or another are typically more clearly delineated, and the on the cosmological parameters. The contrast between
ability to capture the full diversity of cats is unlikely to the VLB evaluated at one parameter θ relative to an-
1
cause the model to ‘wander’ into islands of image space other parameter θ for a fixed field x can thus be used
2 0
corresponding to a dog or an airplane. Thus the slightly tofindtheregionofparameterspacethatmaximizesthe
lower standard deviation can be partly attributed to the conditional likelihood for the field.
possibility that the model chooses to compromise on the
diversityofsamplesforasingleparameter,inordertobe
able to accurately generate fields that look different for
different parameters.
Moreover, the high dimensionality of the initial condi-
A. Examining the influence of different timesteps
tionsrelativetoourtrainingsetsizeof10,000fieldsposes
a challenge. Increasing the size of the training dataset
may help mitigate this issue. Since using all the terms L that contribute to the
t
We compute the covariance between the modes of the VLB is computationally expensive, we first investigate
power spectrum. With the full covariance we can now how sensitive each of the contributing terms L is to
t
statistically quantify the consistency of the generated changes in cosmological parameters over a grid in Fig-
samples relative to that of the true samples using the ure 5a). For an input field x and a single seed, we
0
multidimensional reduced chi-squared statistic. We use evaluate each of the L (x θ⃗ ) terms over a 50 50
t 0 Eval
350 samples of the true fields to set up the reference dis- | ×
grid in [Ω ,σ ], centered on the value of the true field
m 8
tribution used to compute the covariance. We compute
θ⃗ and extending to 0.06 about the true parameter.
the inverse covariance and adjust for the Hartlap factor True ±
To disentangle the individual contribution of each term,
(Equation 8) [13]. We then compute the multidimen-
we subtract the minimum value of each L (x θ⃗ ) on
sional reduced chi-squared statistic of the entire sample t 0 Eval
|
of the true and generated fields, and compute the means the grid, and multiply it by 2 to yield 2∆lnˆ t. We
− L
of the 405 chi-squared statistics for each distribution fol- plot the contour corresponding to 2.30, or the one-sigma
lowing Equation 9. For the true fields, the mean of the contour for a chi-squared distribution with two degrees
chi-squared distribution is 33.4 while that of the gener- of freedom. All contours are minimized in the vicinity
ated is 36.6. Since our power spectra consist of 35 log- of the true parameter, and the curvature of the contours
spacedbins,thisisconsistentwiththeexpectedmeanfor decreases with increasing time. Increasing time involves
achi-squareddistributionwith35degreesoffreedom,i.e. increasing the amount of noise added to the input im-
35. age, which can explain the increased uncertainty in the
true value of the parameter. Thus dropping the terms
corresponding to the higher timesteps in Equation 2 is
IV. PARAMETER INFERENCE unlikely to result in weaker constraints on cosmology.
A trained diffusion model can be used to estimate the
variational lower bound (VLB) of the log likelihood [8,
19]. In the case of a conditional diffusion model, this
likelihood estimate is also conditional, i.e. B. HMC-based Parameter Inference
E [ logp (x θ)] E [ logp (x x ,θ)+ Tocomputetheparameterestimatesforfields,wedraw
q ϕ 0 q ϕ 0 1
− | ≤ − |
(cid:88) samples from the posterior on the parameter using the
D [q(x x ,x ) p (x x ,θ)]+
KL t | t+1 0 || ϕ t | t+1 Hamiltonian Monte Carlo (HMC) method. The Hamil-
t≥1 tonian Monte Carlo, or Hybrid Monte Carlo method
D KL[q(x T x 0) p(x T)]] (2) [3, 10, 24] draws samples from a probability distribution
| ||
L =L +L ...L +L (3) π(θ)viatheintroductionofanauxiliarymomentumvari-
VLB 0 1 T−1 T
(cid:88) ablepandsolvestheequationsofHamiltoniandynamics
Lˆ = L (4)
VLB t inordertoupdatethemomentumandposition(θ). HMC
t<TMAX enables a more efficient exploration of high dimensional
probabilitydistributions. Inourcase,usinganHMCalso
where ϕ denotes the diffusion model architecture, θ is
helpsuscircumventtheproblemofhavingtoredefinethe
the conditioning cosmology (in our case, a vector with
extents and granularity of the parameter grid depending
Ω and σ ), p are the reverse (learned) distributions,
m 8 ϕ on how confident the constraints for a given T are.
and q are the forward (analytical) distributions. During MAX
The Hamiltonian governing the dynamics in the HMC
training,thediffusionmodel’snoisepredictionlossterms
chain is:
areequivalenttotermsofthereweightedVLB[19]. Since
thepredictednoiseisconditionaloncosmology,theterms6
CV: Ω =0.30 σ =0.80 0.6 4.4 32.0
m 8
102 1.0
0.6
True
Generated
Gen
101 0.8
4.4
0.6
100 True
0.4
10−1
100 101 32.0
Wavenumberk(hMpc−1) k(h Mpc −1)
FIG. 3. Generated ‘CV’ fields. Left: Power spectra of the 405 true and generated fields, with the mean, and 16−84th
percentiles. Right: Correlation matrix of the power spectra of the true and generated fields. The lower triangular matrix
correspondstothecorrelationmatrixofthetruefieldswhiletheuppertriangularmatrixcorrespondstothecorrelationmatrix
of the generated fields.
p−1M−1p
Ratio H =U +K where U = logπ(θ) and K =
− 2
1.1
(5)
1.0 logπ(θ)=logp ϕ(x 0 θ)+logp Prior(θ)
|
(cid:88)
Lˆ +logp = L +logp (6)
VLB Prior t Prior
0.9 ≃− −
t<TMAX
We used the Hamiltorch [6] package for the HMC. We
0.8
explain more details about the HMC implementation in
Appendix 1. The prior is chosen to be a flat prior over
0.7
100 101 Ω
m
[0.1,0.5] and σ
8
[0.6,1.0]. The initial parameter
∈ ∈
k(hMpc−1) value is always the fiducial value of [0.3,0.8] and we des-
ignate the first 100 samples as burn-in samples that are
True discarded.
101 Generated We now examine the effect of truncating terms in
Equation 4 using the HMC based parameter inference.
Truncating terms allows us to perform inference faster
andcanallowustoexplorethetradeoffbetweendropping 100
termsforspeedandhigherprecisionwithmoretimesteps.
InFigure 5b),forasinglefield,weplotthemeanpredic-
tions and the 15.9 84.1 (1 sigma) percentiles computed
10−1
using 200
samples−
with the approximate logp (x θ)
ϕ 0
− |
using Equation 4 as a function of T on the x axis
MAX
100 101 for the same field. For this field and parameter, using
Wavenumberk(hMpc−1) moretermsasymptoticallyremovesthebiasonΩ m while
increasing the bias on σ . However, using the first 20
8
FIG. 4. Generated ‘CV’ fields. Upper: Ratio of the timesteps only changes the mean prediction for Ω m and
standarddeviationsofthegeneratedfieldstothatofthetrue σ 8 by 0.26% and 1.27% of the prediction using all
− −
fields. Lower: Standard deviations of the power spectra in 1000 timesteps respectively.
each k bin for the generated and true fields. We now turn our attention to the performance of
our parameter inference approach relative to a power
spectrum baseline. For subsequent HMC-based param-
eter inference in this section, we use Equation 4 with
T = 20 to approximate the conditional negative log
MAX
likelihood. Drawing 500 samples with T = 20 takes
MAX
32 minutes for a single field.
∼
To assess the constraining power of the power spec-
)k(neGσ
)k(σ
)k(eurTσ
)k(P
)1
cpM
h(k
−7
Individualcontributionofeachtimestepto-2∆ln Lˆ t 0.149
0.148
0.740
20 0.147
=
t
0.720 0.146
0
0.700
t=15 t=1 t=0 0.145 100 Num1 b01 erofTimestepsusedinVLB102
Sum
103
8 0.695
=
0.680 t 5
= 0.690
t =2
t =30 0.685
0.660 t
0.680
0.675
0.640
0.22 0.24 0.26 0.28 0.30
0t.3=
240 0.670
100 101 102 103
Ω m NumberofTimestepsusedinVLBSum
FIG. 5. Investigating the contribution of the timesteps used in the VLB sum. Left: One sigma contours of each −2∆lnL ’s
t
individual contributionfordifferenttimesteps. Thecontoursareallcenterednearthetrueparameter(blackstar)andbecome
wider as t increases. Right: Mean, and 1 sigma predictions for a single field as a function of the number of timesteps used
in the VLB sum optimized by the HMC. Reducing the number of timesteps used to compute the VLB does not significantly
affect the diffusion model predictions.
trum, we use the neural posterior estimation approach mean absolute bias for σ is 0.01 (1.26% of the true pre-
8
and train a normalizing flow to represent the posterior diction). The σ prediction uncertainties (mean z score
8
of p (θ Pk Mean) using the Lampe [37] package. Since z¯ =1.14)arebettercalibratedrelativetotheuncertain-
NF
| · | | ¯
computing the overdensity power spectrum involves di- ties for Ω (z = 2.75), but overall, we find the error-
m
| |
viding by the mean of the field, we further concatenate bars to be slightly under-predicted. We attribute the
the mean of the log field as an additional feature, since mis-calibration to the truncation of terms. It is possible
the prediction for Ω for a small box size of 25 Mpc.h−1 thatincludingmoretimesteps,averagingovermoreseeds
m
is very sensitive to the mean of the fields. For a sin- and examining alternate choices of the variance schedule
gle field, we compare the posteriors obtained by drawing could yield better calibrated uncertainties, we leave this
10000samplesfromthepowerspectrumbasedestimator exploration for future work.
and 400 samples from the diffusion model+HMC in Fig-
ure 6. Other details about our implementation are in
the Appendix 2.
1. Robustness
The diffusion model has significantly narrower con-
straintsfortheparametersrelativetothepowerspectrum
Although our constraints are much tighter than those
baseline. Note that, as shown in Figure 2, the cosmolog-
derived from the two point correlation function, robust-
icalparametersΩ andσ arestronglycorrelatedatthe
m 8 ness to noise and observational systematics is an impor-
levelofthepowerspectrumonthesmallscalesprobedby
tantconsiderationguidingtheuseofparameterinference
our simulations, since they are both modulating the am-
methodsonsurveydata. SincethetermsoftheVLB(and
plitude of the power spectrum. This result is consistent
the noise prediction loss terms) include terms where the
with[50]whoalsofoundthatthegalaxypowerspectrum
original image has been noised, we hypothesize that the
inconjunctionwithamultilayerperceptronyieldedweak
parameter estimates learned by the model are naturally
constraints on Ω and σ .
m 8 more robust to perturbations involving the addition of
In the second row, we plot the predicted cosmologi- scaled white noise to the field.
calparameters relativetothetruth for10differentfields To test this, we examine the extent to which our pa-
across different parameters in the validation set. The rameter estimates change relative to the predictions of
dots annotate the mean of the 400 samples and the er- the parameter inference network in [48]. The neural net-
ror bars correspond to the 15.9 84.1 percentiles of the work in [48] is trained to predict the mean and the stan-
−
samples. The mean of the samples is close to the true dard deviation of the parameters given an input dark
valueofθ⃗ overabroadrangeofparameters. TheΩ matter field. In the leftmost column of Figure 7, we
True m
predictions are biased by 0.006 (2% of the true predic- compare the sample predictions obtained from the dif-
−
tion) on average over the range of parameters while the fusion model (green) relative to those obtained from the
σ 8
detciderP:mΩ
detciderP:8σ8
99.7%
95.5%
68.3%
pNF(θ |Pk ·Mean)
pDM(θ |x∗)
0.82
0.80
0.240 0.245
Ω
m
0.90
0.75
0.15 0.30 0.45 0.75 0.90
Ω σ
m 8
0.5 1.0
0.4 0.9
0.3 0.8
0.2 0.7
0.1 0.6
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Ω : True σ : True
m 8
FIG.6. Top: Comparisonoftheparameterestimatesforasinglefieldwiththepowerspectrumbasedestimatorandwiththe
diffusion model likelihood. The star demarcates the true parameter corresponding to the input fields. The shaded contours
demarcatethe68.27%,95.45%and99.73%confidenceintervals. Lower Panel: Predictedparameterandgroundtruthparam-
eter for 10 different fields. The error bars correspond to the 15.9−84.1 percentiles for the marginal probability distributions
for each parameter for each input field.
parameter inference network (red) on 5 fields (x ). In V. CONCLUSION
0
thenexttwocolumns,weaddincreasinglevelsof (0,σ)
N
noisewithσ [0.01,0.02],andexaminetheestimatesfor
∈ In this work, we used a diffusion generative model to
bothapproaches. Weperformthisexperimentinanoise-
emulate dark matter density fields conditional on cos-
agnosticsetting, i.e. weassumethatwedonotknowthe
mological parameters. The model learns to reproduce
level of noise a priori and do not modify the parameter
the modulation of summary statistics, such as the power
inference approach for the diffusion model. We use the
spectra, for changes in cosmological parameters. We ad-
same T =20 and use 200 samples for the estimates.
MAX ditionally assess the extent of mode collapse through the
lens of cosmic variance by examining the diversity of the
power spectra at the fiducial cosmology.
We then directed our attention to the inverse prob-
lem of parameter inference and disentangled the contri-
detciderP
:
Ω
m
σ
8
σ
8
detciderP
:
σ
89
x x +0.01(cid:15),(cid:15) (0,1) x +0.02(cid:15),(cid:15) (0,1)
0 0 0
∼N ∼N
0.5 0.5 0.5
Diffusion-HMC
InfNet
0.4 0.4 0.4
0.3 0.3 0.3
1.0% 6.4% 23.2% 0.2 0.2 0.2
2.5% 4.3% 10.3%
0.1 0.1 0.1
0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5
Ω : True Ω : True Ω : True
m m m
1.0 1.0
1.0
0.9 0.9
0.9
0.8 0.8
0.8
0.7 0.7
1.5% 5.9% 0.7
17.0%
0.8% 1.2% 2.6%
0.6 0.6 0.6
0.6 0.7 0.8 0.9 1.0 0.6 0.7 0.8 0.9 1.0 0.6 0.7 0.8 0.9 1.0
σ : True σ : True σ : True
8 8 8
FIG. 7. Mean and 15.8−84.1 percentile predictions using the Diffusion-HMC estimates (green) and the mean and sigma
predictions obtained from the parameter inference network in [47], denoted by InfNet. The mean absolute percentage bias (of
thetrueprediction)foreachparameterisalsoindicated. TheDiffusion-HMCconstraintsaremorerobusttotheperturbations
to the input image.
bution of each term L in the expression for the Varia- p (x θ) by subsampling the terms to only use the first
t ϕ 0
|
tional Lower Bound to constraints on cosmological pa- T terms (Equation 4). This approximation allows
MAX
rameters. Our findings reveal that the strength of the us to backpropagate its gradient and plug this estimate
constraints decrease as t increases. [21] explored this for p (x θ) into an HMC and sample the posterior on
ϕ 0
|
question in the context of using the diffusion model’s p (θ x ). The Diffusion-HMC approach yields tight con-
ϕ 0
|
conditional VLB terms for classification and found that straints on cosmological parameters, competitive with a
intermediate timesteps had the highest accuracy when bespokeparameterinferencenetwork[48]trainedonlyto
only a single timestep is used. The difference in which infer cosmology given a field.
termscontainthemostinformationaboutthecondition-
In our experiments, we use only a single seed in each
ingattribute(cosmology/class)isinteresting,andcould
HMC step and a T = 20 to speed up inference. In
MAX
bepartlyexplainedbythedifferenceintheirformulation
our case, the use of an HMC allowed us to circumvent
and weighting of the VLB terms and in how changes in
the requirement of choosing a grid with the appropriate
theconditioningattributeaffectanimageinthetwoset-
resolution required to resolve the constraints, and elim-
tings. Modulating cosmology modifies global attributes
inates the dependence on the number of points in the
of the fields, such as their intensity and power spectra,
grid. However, the Diffusion-VLB estimates could also
as seen in Figure 2, while the information that distin-
be used in other settings, that do not entail the use of
guishes different breeds of pets from each other tends to
an HMC. While our approach scaled with the number
berelativelylocalizedintheboundingboxcontainingthe
of timesteps we use in the sum, in constrast, [21] scaled
animal.
with the number of classes in the classification dataset.
Ifitisindeedalwaysthecasethatthetimestepsnearest
Lastly, we demonstrate that the diffusion model likeli-
to the image manifold contain most cosmological infor-
hoodconferstheDiffusion-HMCconstraintswithgreater
mation, one could in future, swap out our discrete time
robustness to the addition of noise to the input image
architecture with continuous time diffusion models [41],
relative to the behavior of a parameter inference net-
where t is a continuous variable with t [0,1] and prior-
work. This echoes the behavior of diffusion models in
∈
itize steps that lie near the image manifold or t=0.
other discriminative tasks such as [5, 21, 31], where dif-
These insights motivated us to truncate the diffu- fusion model based classifiers have been shown to pos-
sion model conditional VLB based approximation for sesshigherrobustnesstoadversarialexamplesorpertur-
detciderP
: Ω
detciderP
:
σ
m
810
bations. This is a pertinent finding since [17] showed range of downstream tasks from image generation and
that the powerful constraints derived from neural net- restoration to inference problems.
work based parameter inference may often come at the
cost of their susceptibility to slight perturbations that
the canonical two-point correlation based analyses are
VI. CODE
impervious to.
The simulation volume of the dataset we work with
is still much smaller than the scales mapped by astro- ThecodeforthisprojectisavailableatDiffusion-HMC
physical surveys. Future work could focus on scaling up (cid:135). We acknowledge [48], Improved-Diffusion [26], The
to more survey-realistic scenarios involving larger sim- Annotated Diffusion and DDPM for use of their code
ulation volumes and directly observed tracers. [8] also snippets.
showed that diffusion generative models that work with Packages: Hamiltorch [6], Lampe [37], GNU Parallel
point clouds can allow one to emulate and perform cos- [42].
mological parameter inference with point cloud data.
Our exploration into robustness could inform applica-
tions to real data, and future investigation could focus
VII. ACKNOWLEDGEMENTS
on conferring and quantifying robustness against other
survey-related and observational noise effects. Alterna-
tive formulations of the generative process [2, 51] could This work was supported by the National Science
also be relevant to this exercise. Foundation under Cooperative Agreement PHY2019786
In this work, we demonstrated that a diffusion model (TheNSFAIInstituteforArtificialIntelligenceandFun-
canbetrainednotjusttoemulatefields,butthatit’slike- damental Interactions). We are grateful to Yueying Ni,
lihood estimate can be adapted to work with the Hamil- FranciscoVillaescusa-Navarro, CoreFranciscoPark, An-
tonianMonteCarloframeworktoderivetightconstraints drew K. Saydjari, Justina R. Yang, Yilun Du, Ana Sofia
on cosmological parameters. This makes a step toward Uzsoy,ShuchinAeronandmanyothersforinsightfulcon-
advancing the use of diffusion model based priors for a versations.
[1] Anderson,B.D.1982,StochasticProcessesandtheirAp- Information Processing Systems, 33, 6840
plications, 12, 313 [17] Horowitz, B., & Melchior, P. 2022, arXiv preprint
[2] Bansal, A., Borgnia, E., Chu, H.-M., et al. 2024, Ad- arXiv:2211.14788
vances in Neural Information Processing Systems, 36 [18] Jagvaral,Y.,Mandelbaum,R.,&Lanusse,F.2022,arXiv
[3] Betancourt, M. 2017, arXiv preprint arXiv:1701.02434 preprint arXiv:2212.05592
[4] Biewald,L.2020,ExperimentTrackingwithWeightsand [19] Kingma, D. P., & Gao, R. 2023, arXiv preprint
Biases. https://www.wandb.com/ arXiv:2303.00848
[5] Chen,H.,Dong,Y.,Shao,S.,etal.2024,arXivpreprint [20] Legin, R., Ho, M., Lemos, P., et al. 2024, Monthly No-
arXiv:2402.02316 tices of the Royal Astronomical Society: Letters, 527,
[6] Cobb, A. D., Baydin, A. G., Markham, A., & Roberts, L173
S. J. 2019, arXiv preprint arXiv:1910.06243 [21] Li, A. C., Prabhudesai, M., Duggal, S., Brown, E., &
[7] Corso, G., Sta¨rk, H., Jing, B., Barzilay, R., & Jaakkola, Pathak, D. 2023, in Proceedings of the IEEE/CVF In-
T. 2022, arXiv preprint arXiv:2210.01776 ternational Conference on Computer Vision, 2206–2217
[8] Cuesta-Lazaro, C., & Mishra-Sharma, S. 2023, arXiv [22] Mudur, N., & Finkbeiner, D. P. 2022, arXiv preprint
preprint arXiv:2311.17141 arXiv:2211.12444
[9] Dai, B., & Seljak, U. 2023, arXiv preprint [23] Mustafa, M., Bard, D., Bhimji, W., et al. 2019, Compu-
arXiv:2306.04689 tational Astrophysics and Cosmology, 6, 1
[10] Duane, S., Kennedy, A.D., Pendleton, B.J., &Roweth, [24] Neal, R. M., et al. 2011, Handbook of markov chain
D. 1987, Physics letters B, 195, 216 monte carlo, 2, 2
[11] Hahn, C., Eickenberg, M., Ho, S., et al. 2023, Pro- [25] Nelson, D., Springel, V., Pillepich, A., et al. 2019, Com-
ceedings of the National Academy of Sciences, 120, putational Astrophysics and Cosmology, 6, 1
e2218810120 [26] Nichol,A.Q.,&Dhariwal,P.2021,inInternationalCon-
[12] Hamaus,N.,Pisani,A.,Sutter,P.M.,etal.2016,Phys- ference on Machine Learning, PMLR, 8162–8171
ical Review Letters, 117, 091302 [27] Ono, V., Park, C. F., Mudur, N., et al. 2024, arXiv
[13] Hartlap,J.,Simon,P.,&Schneider,P.2007,Astronomy preprint arXiv:2403.10648
& Astrophysics, 464, 399 [28] Paillas, E., Cuesta-Lazaro, C., Percival, W. J., et al.
[14] Heitmann,K.,Higdon,D.,White,M.,etal.2009,Astro- 2023, Cosmological constraints from density-split clus-
phys. J., 705, 156, doi: 10.1088/0004-637X/705/1/156 tering in the BOSS CMASS galaxy sample. https:
[15] Heurtel-Depeiges, D., Burkhart, B., Ohana, R., & Blan- //arxiv.org/abs/2309.16541
card, B. R.-S. 2023, arXiv preprint arXiv:2310.16285 [29] Papamakarios,G.,Pavlakou,T.,&Murray,I.2017,Ad-
[16] Ho, J., Jain, A., & Abbeel, P. 2020, Advances in Neural vances in neural information processing systems, 3011
[30] Pillepich, A., Springel, V., Nelson, D., et al. 2018,
MonthlyNoticesoftheRoyalAstronomicalSociety,473,
4077
[31] Prabhudesai, M., Ke, T.-W., Li, A. C., Pathak, D., &
Fragkiadaki, K. 2023, in Thirty-seventh Conference on
Neural Information Processing Systems
[32] R´egaldo-Saint Blancard, B., Allys, E., Auclair, C., et al.
2022, arXiv e-prints, arXiv
[33] Remy, B., Lanusse, F., Jeffrey, N., et al. 2022, arXiv
preprint arXiv:2201.05561
[34] Rombach, R., Blattmann, A., Lorenz, D., Esser, P.,
& Ommer, B. 2021, High-Resolution Image Synthesis
withLatentDiffusionModels. https://arxiv.org/abs/
2112.10752
[35] Ronneberger, O., Fischer, P., & Brox, T. 2015, in In-
ternationalConferenceonMedicalimagecomputingand
computer-assisted intervention, Springer, 234–241
[36] Rouhiainen, A., Gira, M., Mu¨nchmeyer, M., Lee, K., &
Shiu, G. 2023, arXiv preprint arXiv:2311.05217
[37] Rozet,F.,Delaunoy,A.,Miller,B.,etal.2021,LAMPE:
Likelihood-freeAmortizedPosteriorEstimation,doi:10.
5281/zenodo.8405782
[38] Sharma, D., Dai, B., Villaescusa-Navarro, F., & Seljak,
U. 2024, arXiv preprint arXiv:2401.15891
[39] Shen, Z., Zhang, M., Zhao, H., Yi, S., & Li, H. 2021,
in Proceedings of the IEEE/CVF winter conference on
applications of computer vision, 3531–3539
[40] Song, Y., Shen, L., Xing, L., & Ermon, S. 2021, arXiv
preprint arXiv:2111.08005
[41] Song, Y., Sohl-Dickstein, J., Kingma, D. P., et al. 2020,
arXiv preprint arXiv:2011.13456
[42] Tange,O.2018,GNUParallel2018(OleTange),doi:10.
5281/zenodo.1146014
[43] Theis, L., Oord, A. v. d., & Bethge, M. 2015, arXiv
preprint arXiv:1511.01844
[44] Valogiannis,G.,&Dvorkin,C.2022,PhysicalReviewD,
106, 103509
[45] Valogiannis, G., Yuan, S., & Dvorkin, C. 2023, arXiv
preprint arXiv:2310.16116
[46] Vaswani, A., Shazeer, N., Parmar, N., et al. 2017, Ad-
vances in neural information processing systems, 30
[47] Villaescusa-Navarro, F., Angl´es-Alca´zar, D., Genel, S.,
et al. 2021, The Astrophysical Journal, 915, 71
[48] —. 2021, arXiv preprint arXiv:2109.09747
[49] Villaescusa-Navarro, F., Genel, S., Angles-Alcazar, D.,
et al. 2022, The Astrophysical Journal Supplement Se-
ries, 259, 61
[50] Villanueva-Domingo,P.,&Villaescusa-Navarro,F.2022,
The Astrophysical Journal, 937, 115
[51] Wildberger, J., Dax, M., Buchholz, S., et al. 2024, Ad-
vances in Neural Information Processing Systems, 36
[52] Wu, Y., & He, K. 2018, in Proceedings of the European
conference on computer vision (ECCV), 3–19
[53] Zagoruyko, S., & Komodakis, N. 2016, arXiv preprint
arXiv:1605.0714612
APPENDIX
1.0 4
0.8
2
0.6
0
0.4
2
0.2 −
0.0 4
−
0 200 400 600 800 1000 0 200 400 600 800 1000
t t
FIG. 8. The scale / standard deviation of the cumulative noise in Equation 1 added to the image over different timesteps.
1. Parameter Inference
In Equation 5, we set M−1 to be diagonal with [1, 5]. Setting the inverse mass matrix in an HMC to be close
to the covariance of the expected posterior distribution helps the chain explore the space better. In this case, we
choose a step size of 5 10−4, because of the steep gradient of the posterior distribution with respect to Ω . For the
m
×
parameter on field where the true parameter is 0.101, i.e. on the prior range, we need to further reduce the step size
to 1 10−4 in order for the parameters to be accepted. We modified the Hamiltorch package in order to generate
×
samples. For T 20 we compute the contributions to the VLB loss in batches of 10 timesteps and accumulate
MAX
≥
the gradient contribution for each batch. This enables us to compute the gradients with 1000 timesteps. The choice
of mass matrix accelerates the chain’s convergence to the correct region of parameter space for σ . We approximate
8
-logp (x θ) with the truncated variational lower bound in Equation 4. While we do not compute the expectation
ϕ 0
|
over multiple seeds within a single evaluation of Equation 5, for speed, every evaluation uses a different seed and
noise pattern. The prior is chosen to be a flat prior over Ω [0.1,0.5] and σ [0.6,1.0].
m 8
∈ ∈
L =L +L ...L +L = (7)
vlb 0 1 T−1 T
(cid:88)
=E[ logp (x θ)] E [D [q(x x ) p(x )]+ D [q(x x ,x ) p (x x ,θ)] logp (x x ,θ)]
ϕ 0 q KL T 0 T KL t t+1 0 ϕ t t+1 ϕ 0 1
− | ≤ | || | || | − |
t≥1
Notation:
• x : Normalized input field
0
• ϕ: Noise Model (Neural Network)
• θ: Conditioning cosmology i.e. a vector with Ω and σ
m 8
Computing VLB terms:
(cid:88)(x
0
µ 0)2
L = logp (x x ,θ)= ln (x µ ,β )= − +0.5ln2πβ
0 ϕ 0 1 0 0 0 0
− | − N | 2β | |
0
p
For t [1,T 1],L =D [q(x x ,x ) p (x x ,θ)]
t KL t t+1 0 ϕ t t+1
∈ − | || |
q(x x ,x )= (µ˜(x x ),β˜)
t t+1 0 t t+1 0 t
| N
(cid:112) (cid:112)
µ˜(x x )=
α¯ t−1β
t x +
α t(1 −α¯ t−1)
x and β˜ =β
1 −α¯
t−1
t t+1 0 0 t+1 t t
1 α¯ 1 α¯ 1 α¯
t t t
− − −
p (x x ,θ)= (µ˜(x ,xˆ ),β˜)
ϕ t t+1 t t+1 0,ϕ t
| N
(cid:112)
x 1 α¯ ϵ (x ,t,θ)
t+1 t ϕ t+1
xˆ = − −
0,ϕ
√α¯
t
t¯α
−1√
)t(RNS
gol
0113
0.30 0.80
0.25 0.75
0.20
0.70
0.15
0.65
0 100 200 300 0 100 200 300
Sample Sample
FIG. 9. HMC chains for 300 samples, for the same field, starting at two different initial parameters: [0.11, 0.62] (beige) and
[0.3,0.8](teal). Thechainsarewellmixedbythecutoffwedesignateasourburn-in(100samples),denotedbythedashedline.
a. HMC Convergence
Forasinglefield,weexaminetheconvergenceofparameters,whenthechainstartsfromdifferentinitialparameters
in Figure 9. The chains are indistinguishable beyond around 50 samples. We thus choose a burn-in of 100 samples.
The Rˆ for both parameters computed using the samples in [100 300] is 0.997. A Rˆ of greater than 1.1 usually
−
indicates that the chains have not converged and still retain some memory of their initialization. While the Rˆ is
theoretically expected to be around 1 or slightly greater, some numerical variation about this expected value can
result in values that are slightly less than 1. The Rˆ is a measure of the variance between chains divided by the
variance within chains.
2. Parameter Estimation Baselines
Power Spectrum NPE Baseline: We use a Masked Autoregressive Flow[29] to implement the normalizing flow
that predicts the 2 dimensional parameter vector given the 129 dimensional feature vector for a single field (128 bins
for the power spectrum+1 for the mean of the log fields). The power spectrum is the log of the overdensity power
spectrumofthe(unlogged)fields. InFigure 6,thepowerspectrumsamplecontoursaresmoothedbyconvolvingwith
a Gaussian kernel with a scale of 0.8 and the Diffusion-HMC samples are smoothed by a kernel of 0.2. The ellipses
in the inset figure are computed using the covariance of the 400 diffusion model samples and finding the ellipses
corresponding to the 68.3, 99.4 and 99.7% confidence intervals, using the eigendecomposition of the covariance.
3. Summary Statistics
a. Reduced Chi-squared Statistics
For the CV fields, where we have 450 samples of the true and generated fields for a single parameter, we compute
the reduced chi-squared statistic using an estimate of the covariance between different k bins. The number of k bins
here is 35.
N p 2
µ⃗ = Pk C =Cov[Pk ] Cˆ−1 =C−1 − − (8)
Ref Ref
⟨ ⟩ N 1
(cid:88) −
χ2(Pk )= (Pk µ⃗) (Cˆ−1(Pk µ⃗)T)T (9)
r Test Test − · Test −
k
FortheLHfieldsduringmodelselection,sincewejusthave15fieldsinthetruedataset,wecannotreliablyestimate
Ω
m
σ
814
a covariance. We use the following formula instead. The number of k bins here is 128.
χ2(s)=
1 (cid:88)(P(k)
s
−<P(k)
True
>)2
(10)
r k σ[P(k) ]2
True
| | k
b. Across Checkpoints
For the eight checkpoints we generated 500 fields, with 50 fields for each of the 10 validation parameters. We then
examinedhowdifferentthereducedchi-squaredstatisticofthepowerspectraofthelogfields,thelinearfieldsandthe
pvaluesofthemeansofthedistributionsoftrueandgeneratedfields. Avalueoflessthan0.05wouldindicatethatthe
two distributions of the means are statistically different. The p values are above 0.05 for all eight checkpoints. Since
these comparisons are limited by the number of samples in the true set for each parameter (15), we additionally plot
the reduced chi-squared statistic derived by using each true fields as the test and the other 14 fields as the reference.
While there is some oscillation across checkpoints for each of these three statistics, the variation appears random.
PkoftheLinearFields PkoftheLogFields
4.5 2.4
p=0.05
4.0 2.2 0.6
3.5 2.0
3.0 1.8 0.4
1.6
2.5
0.2
1.4
2.0
1.2
200k 220k 240k 260k 280k 300k 320k 340k 200k 220k 240k 260k 280k 300k 320k 340k 200k 220k 240k 260k 280k 300k 320k 340k
Checkpoint Checkpoint Checkpoint
FIG.10. Meanandstandarderroronthemeanforthereducedχ2 statisticofthepowerspectraofthe50linear(left)andlog
r
(center)fieldsforeachparameterrelativetothe15truefields’powerspectraforthatparameter,across10differentparameters
for each of 8 checkpoints. The dashed line demarcates the mean reduced χ2 of the true fields using the other 14 true fields as
r
the reference distribution (leave-one-out). Right: Mean and standard error on the mean of the p values of the distribution of
the means of the 50 generated log fields relative to the distribution of the means of the 15 true fields for the same parameter.
The p values are above 0.05 for all of the 8 checkpoints.
)]eurT[kP]detareneG[kP(2χ
|
r
)]eurT[kP]detareneG[kP(2χ
|
r
eulavp