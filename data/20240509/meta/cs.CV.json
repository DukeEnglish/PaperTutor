[
    {
        "title": "OpenESS: Event-based Semantic Scene Understanding with Open Vocabularies",
        "authors": "Lingdong KongYouquan LiuLai Xing NgBenoit R. CottereauWei Tsang Ooi",
        "links": "http://arxiv.org/abs/2405.05259v1",
        "entry_id": "http://arxiv.org/abs/2405.05259v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05259v1",
        "summary": "Event-based semantic segmentation (ESS) is a fundamental yet challenging task\nfor event camera sensing. The difficulties in interpreting and annotating event\ndata limit its scalability. While domain adaptation from images to event data\ncan help to mitigate this issue, there exist data representational differences\nthat require additional effort to resolve. In this work, for the first time, we\nsynergize information from image, text, and event-data domains and introduce\nOpenESS to enable scalable ESS in an open-world, annotation-efficient manner.\nWe achieve this goal by transferring the semantically rich CLIP knowledge from\nimage-text pairs to event streams. To pursue better cross-modality adaptation,\nwe propose a frame-to-event contrastive distillation and a text-to-event\nsemantic consistency regularization. Experimental results on popular ESS\nbenchmarks showed our approach outperforms existing methods. Notably, we\nachieve 53.93% and 43.31% mIoU on DDD17 and DSEC-Semantic without using either\nevent or frame labels.",
        "updated": "2024-05-08 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05259v1"
    },
    {
        "title": "Multi-Modal Data-Efficient 3D Scene Understanding for Autonomous Driving",
        "authors": "Lingdong KongXiang XuJiawei RenWenwei ZhangLiang PanKai ChenWei Tsang OoiZiwei Liu",
        "links": "http://arxiv.org/abs/2405.05258v1",
        "entry_id": "http://arxiv.org/abs/2405.05258v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05258v1",
        "summary": "Efficient data utilization is crucial for advancing 3D scene understanding in\nautonomous driving, where reliance on heavily human-annotated LiDAR point\nclouds challenges fully supervised methods. Addressing this, our study extends\ninto semi-supervised learning for LiDAR semantic segmentation, leveraging the\nintrinsic spatial priors of driving scenes and multi-sensor complements to\naugment the efficacy of unlabeled datasets. We introduce LaserMix++, an evolved\nframework that integrates laser beam manipulations from disparate LiDAR scans\nand incorporates LiDAR-camera correspondences to further assist data-efficient\nlearning. Our framework is tailored to enhance 3D scene consistency\nregularization by incorporating multi-modality, including 1) multi-modal\nLaserMix operation for fine-grained cross-sensor interactions; 2)\ncamera-to-LiDAR feature distillation that enhances LiDAR feature learning; and\n3) language-driven knowledge guidance generating auxiliary supervisions using\nopen-vocabulary models. The versatility of LaserMix++ enables applications\nacross LiDAR representations, establishing it as a universally applicable\nsolution. Our framework is rigorously validated through theoretical analysis\nand extensive experiments on popular driving perception datasets. Results\ndemonstrate that LaserMix++ markedly outperforms fully supervised alternatives,\nachieving comparable accuracy with five times fewer annotations and\nsignificantly improving the supervised-only baselines. This substantial\nadvancement underscores the potential of semi-supervised approaches in reducing\nthe reliance on extensive labeled data in LiDAR-based 3D scene understanding\nsystems.",
        "updated": "2024-05-08 17:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05258v1"
    },
    {
        "title": "THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models",
        "authors": "Prannay KaulZhizhong LiHao YangYonatan DuklerAshwin SwaminathanC. J. TaylorStefano Soatto",
        "links": "http://arxiv.org/abs/2405.05256v1",
        "entry_id": "http://arxiv.org/abs/2405.05256v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05256v1",
        "summary": "Mitigating hallucinations in large vision-language models (LVLMs) remains an\nopen problem. Recent benchmarks do not address hallucinations in open-ended\nfree-form responses, which we term \"Type I hallucinations\". Instead, they focus\non hallucinations responding to very specific question formats -- typically a\nmultiple-choice response regarding a particular object or attribute -- which we\nterm \"Type II hallucinations\". Additionally, such benchmarks often require\nexternal API calls to models which are subject to change. In practice, we\nobserve that a reduction in Type II hallucinations does not lead to a reduction\nin Type I hallucinations but rather that the two forms of hallucinations are\noften anti-correlated. To address this, we propose THRONE, a novel object-based\nautomatic framework for quantitatively evaluating Type I hallucinations in LVLM\nfree-form outputs. We use public language models (LMs) to identify\nhallucinations in LVLM responses and compute informative metrics. By evaluating\na large selection of recent LVLMs using public datasets, we show that an\nimprovement in existing metrics do not lead to a reduction in Type I\nhallucinations, and that established benchmarks for measuring Type I\nhallucinations are incomplete. Finally, we provide a simple and effective data\naugmentation method to reduce Type I and Type II hallucinations as a strong\nbaseline.",
        "updated": "2024-05-08 17:59:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05256v1"
    },
    {
        "title": "Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models",
        "authors": "Hongjie WangDifan LiuYan KangYijun LiZhe LinNiraj K. JhaYuchen Liu",
        "links": "http://arxiv.org/abs/2405.05252v1",
        "entry_id": "http://arxiv.org/abs/2405.05252v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05252v1",
        "summary": "Diffusion Models (DMs) have exhibited superior performance in generating\nhigh-quality and diverse images. However, this exceptional performance comes at\nthe cost of expensive architectural design, particularly due to the attention\nmodule heavily used in leading models. Existing works mainly adopt a retraining\nprocess to enhance DM efficiency. This is computationally expensive and not\nvery scalable. To this end, we introduce the Attention-driven Training-free\nEfficient Diffusion Model (AT-EDM) framework that leverages attention maps to\nperform run-time pruning of redundant tokens, without the need for any\nretraining. Specifically, for single-denoising-step pruning, we develop a novel\nranking algorithm, Generalized Weighted Page Rank (G-WPR), to identify\nredundant tokens, and a similarity-based recovery method to restore tokens for\nthe convolution operation. In addition, we propose a Denoising-Steps-Aware\nPruning (DSAP) approach to adjust the pruning budget across different denoising\ntimesteps for better generation quality. Extensive evaluations show that AT-EDM\nperforms favorably against prior art in terms of efficiency (e.g., 38.8% FLOPs\nsaving and up to 1.53x speed-up over Stable Diffusion XL) while maintaining\nnearly the same FID and CLIP scores as the full model. Project webpage:\nhttps://atedm.github.io.",
        "updated": "2024-05-08 17:56:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05252v1"
    },
    {
        "title": "BenthicNet: A global compilation of seafloor images for deep learning applications",
        "authors": "Scott C. LoweBenjamin MisiukIsaac XuShakhboz AbdulazizovAmit R. BaroiAlex C. BastosMerlin BestVicki FerriniAriell FriedmanDeborah HartOve Hoegh-GuldbergDaniel IerodiaconouJulia Mackin-McLaughlinKathryn MarkeyPedro S. MenandroJacquomo MonkShreya NemaniJohn O'BrienElizabeth OhLuba Y. ReshitnykKatleen RobertChris M. RoelfsemaJessica A. SameotoAlexandre C. G. SchimelJordan A. ThomsonBrittany R. WilsonMelisa C. WongCraig J. BrownThomas Trappenberg",
        "links": "http://arxiv.org/abs/2405.05241v1",
        "entry_id": "http://arxiv.org/abs/2405.05241v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05241v1",
        "summary": "Advances in underwater imaging enable the collection of extensive seafloor\nimage datasets that are necessary for monitoring important benthic ecosystems.\nThe ability to collect seafloor imagery has outpaced our capacity to analyze\nit, hindering expedient mobilization of this crucial environmental information.\nRecent machine learning approaches provide opportunities to increase the\nefficiency with which seafloor image datasets are analyzed, yet large and\nconsistent datasets necessary to support development of such approaches are\nscarce. Here we present BenthicNet: a global compilation of seafloor imagery\ndesigned to support the training and evaluation of large-scale image\nrecognition models. An initial set of over 11.4 million images was collected\nand curated to represent a diversity of seafloor environments using a\nrepresentative subset of 1.3 million images. These are accompanied by 2.6\nmillion annotations translated to the CATAMI scheme, which span 190,000 of the\nimages. A large deep learning model was trained on this compilation and\npreliminary results suggest it has utility for automating large and small-scale\nimage analysis tasks. The compilation and model are made openly available for\nuse by the scientific community at https://doi.org/10.20383/103.0614.",
        "updated": "2024-05-08 17:37:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05241v1"
    }
]