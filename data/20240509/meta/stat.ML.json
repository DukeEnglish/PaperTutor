[
    {
        "title": "Is Transductive Learning Equivalent to PAC Learning?",
        "authors": "Shaddin DughmiYusuf KalayciGrayson York",
        "links": "http://arxiv.org/abs/2405.05190v1",
        "entry_id": "http://arxiv.org/abs/2405.05190v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05190v1",
        "summary": "Most work in the area of learning theory has focused on designing effective\nProbably Approximately Correct (PAC) learners. Recently, other models of\nlearning such as transductive error have seen more scrutiny. We move toward\nshowing that these problems are equivalent by reducing agnostic learning with a\nPAC guarantee to agnostic learning with a transductive guarantee by adding a\nsmall number of samples to the dataset. We first rederive the result of\nAden-Ali et al. arXiv:2304.09167 reducing PAC learning to transductive learning\nin the realizable setting using simpler techniques and at more generality as\nbackground for our main positive result. Our agnostic transductive to PAC\nconversion technique extends the aforementioned argument to the agnostic case,\nshowing that an agnostic transductive learner can be efficiently converted to\nan agnostic PAC learner. Finally, we characterize the performance of the\nagnostic one inclusion graph algorithm of Asilis et al. arXiv:2309.13692 for\nbinary classification, and show that plugging it into our reduction leads to an\nagnostic PAC learner that is essentially optimal. Our results imply that\ntransductive and PAC learning are essentially equivalent for supervised\nlearning with pseudometric losses in the realizable setting, and for binary\nclassification in the agnostic setting. We conjecture this is true more\ngenerally for the agnostic setting.",
        "updated": "2024-05-08 16:26:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05190v1"
    },
    {
        "title": "Uncertainty quantification in metric spaces",
        "authors": "Gábor LugosiMarcos Matabuena",
        "links": "http://arxiv.org/abs/2405.05110v1",
        "entry_id": "http://arxiv.org/abs/2405.05110v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05110v1",
        "summary": "This paper introduces a novel uncertainty quantification framework for\nregression models where the response takes values in a separable metric space,\nand the predictors are in a Euclidean space. The proposed algorithms can\nefficiently handle large datasets and are agnostic to the predictive base model\nused. Furthermore, the algorithms possess asymptotic consistency guarantees\nand, in some special homoscedastic cases, we provide non-asymptotic guarantees.\nTo illustrate the effectiveness of the proposed uncertainty quantification\nframework, we use a linear regression model for metric responses (known as the\nglobal Fr\\'echet model) in various clinical applications related to precision\nand digital medicine. The different clinical outcomes analyzed are represented\nas complex statistical objects, including multivariate Euclidean data,\nLaplacian graphs, and probability distributions.",
        "updated": "2024-05-08 15:06:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05110v1"
    },
    {
        "title": "Biology-inspired joint distribution neurons based on Hierarchical Correlation Reconstruction allowing for multidirectional neural networks",
        "authors": "Jarek Duda",
        "links": "http://arxiv.org/abs/2405.05097v1",
        "entry_id": "http://arxiv.org/abs/2405.05097v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05097v1",
        "summary": "Popular artificial neural networks (ANN) optimize parameters for\nunidirectional value propagation, assuming some guessed parametrization type\nlike Multi-Layer Perceptron (MLP) or Kolmogorov-Arnold Network (KAN). In\ncontrast, for biological neurons e.g. \"it is not uncommon for axonal\npropagation of action potentials to happen in both directions\" \\cite{axon} -\nsuggesting they are optimized to continuously operate in multidirectional way.\nAdditionally, statistical dependencies a single neuron could model is not just\n(expected) value dependence, but entire joint distributions including also\nhigher moments. Such agnostic joint distribution neuron would allow for\nmultidirectional propagation (of distributions or values) e.g. $\\rho(x|y,z)$ or\n$\\rho(y,z|x)$ by substituting to $\\rho(x,y,z)$ and normalizing. There will be\ndiscussed Hierarchical Correlation Reconstruction (HCR) for such neuron model:\nassuming $\\rho(x,y,z)=\\sum_{ijk} a_{ijk} f_i(x) f_j(y) f_k(z)$ type\nparametrization of joint distribution with polynomial basis $f_i$, which allows\nfor flexible, inexpensive processing including nonlinearities, direct model\nestimation and update, trained through standard backpropagation or novel ways\nfor such structure up to tensor decomposition. Using only pairwise\n(input-output) dependencies, its expected value prediction becomes KAN-like\nwith trained activation functions as polynomials, can be extended by adding\nhigher order dependencies through included products - in conscious\ninterpretable way, allowing for multidirectional propagation of both values and\nprobability densities.",
        "updated": "2024-05-08 14:49:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05097v1"
    },
    {
        "title": "Robust deep learning from weakly dependent data",
        "authors": "William KengneModou Wade",
        "links": "http://arxiv.org/abs/2405.05081v1",
        "entry_id": "http://arxiv.org/abs/2405.05081v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05081v1",
        "summary": "Recent developments on deep learning established some theoretical properties\nof deep neural networks estimators. However, most of the existing works on this\ntopic are restricted to bounded loss functions or (sub)-Gaussian or bounded\ninput. This paper considers robust deep learning from weakly dependent\nobservations, with unbounded loss function and unbounded input/output. It is\nonly assumed that the output variable has a finite $r$ order moment, with $r\n>1$. Non asymptotic bounds for the expected excess risk of the deep neural\nnetwork estimator are established under strong mixing, and $\\psi$-weak\ndependence assumptions on the observations. We derive a relationship between\nthese bounds and $r$, and when the data have moments of any order (that is\n$r=\\infty$), the convergence rate is close to some well-known results. When the\ntarget predictor belongs to the class of H\\\"older smooth functions with\nsufficiently large smoothness index, the rate of the expected excess risk for\nexponentially strongly mixing data is close to or as same as those for obtained\nwith i.i.d. samples. Application to robust nonparametric regression and robust\nnonparametric autoregression are considered. The simulation study for models\nwith heavy-tailed errors shows that, robust estimators with absolute loss and\nHuber loss function outperform the least squares method.",
        "updated": "2024-05-08 14:25:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05081v1"
    },
    {
        "title": "Multi-fidelity Hamiltonian Monte Carlo",
        "authors": "Dhruv V. PatelJonghyun LeeMatthew W. FarthingPeter K. KitanidisEric F. Darve",
        "links": "http://arxiv.org/abs/2405.05033v1",
        "entry_id": "http://arxiv.org/abs/2405.05033v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05033v1",
        "summary": "Numerous applications in biology, statistics, science, and engineering\nrequire generating samples from high-dimensional probability distributions. In\nrecent years, the Hamiltonian Monte Carlo (HMC) method has emerged as a\nstate-of-the-art Markov chain Monte Carlo technique, exploiting the shape of\nsuch high-dimensional target distributions to efficiently generate samples.\nDespite its impressive empirical success and increasing popularity, its\nwide-scale adoption remains limited due to the high computational cost of\ngradient calculation. Moreover, applying this method is impossible when the\ngradient of the posterior cannot be computed (for example, with black-box\nsimulators). To overcome these challenges, we propose a novel two-stage\nHamiltonian Monte Carlo algorithm with a surrogate model. In this\nmulti-fidelity algorithm, the acceptance probability is computed in the first\nstage via a standard HMC proposal using an inexpensive differentiable surrogate\nmodel, and if the proposal is accepted, the posterior is evaluated in the\nsecond stage using the high-fidelity (HF) numerical solver. Splitting the\nstandard HMC algorithm into these two stages allows for approximating the\ngradient of the posterior efficiently, while producing accurate posterior\nsamples by using HF numerical solvers in the second stage. We demonstrate the\neffectiveness of this algorithm for a range of problems, including linear and\nnonlinear Bayesian inverse problems with in-silico data and experimental data.\nThe proposed algorithm is shown to seamlessly integrate with various\nlow-fidelity and HF models, priors, and datasets. Remarkably, our proposed\nmethod outperforms the traditional HMC algorithm in both computational and\nstatistical efficiency by several orders of magnitude, all while retaining or\nimproving the accuracy in computed posterior statistics.",
        "updated": "2024-05-08 13:03:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05033v1"
    }
]