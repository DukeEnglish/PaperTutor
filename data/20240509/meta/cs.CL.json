[
    {
        "title": "Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge",
        "authors": "Charles KoutchemeNicola DaineseSami SarsaArto HellasJuho LeinonenPaul Denny",
        "links": "http://arxiv.org/abs/2405.05253v1",
        "entry_id": "http://arxiv.org/abs/2405.05253v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05253v1",
        "summary": "Large language models (LLMs) have shown great potential for the automatic\ngeneration of feedback in a wide range of computing contexts. However, concerns\nhave been voiced around the privacy and ethical implications of sending student\nwork to proprietary models. This has sparked considerable interest in the use\nof open source LLMs in education, but the quality of the feedback that such\nopen models can produce remains understudied. This is a concern as providing\nflawed or misleading generated feedback could be detrimental to student\nlearning. Inspired by recent work that has utilised very powerful LLMs, such as\nGPT-4, to evaluate the outputs produced by less powerful models, we conduct an\nautomated analysis of the quality of the feedback produced by several open\nsource models using a dataset from an introductory programming course. First,\nwe investigate the viability of employing GPT-4 as an automated evaluator by\ncomparing its evaluations with those of a human expert. We observe that GPT-4\ndemonstrates a bias toward positively rating feedback while exhibiting moderate\nagreement with human raters, showcasing its potential as a feedback evaluator.\nSecond, we explore the quality of feedback generated by several leading\nopen-source LLMs by using GPT-4 to evaluate the feedback. We find that some\nmodels offer competitive performance with popular proprietary LLMs, such as\nChatGPT, indicating opportunities for their responsible use in educational\nsettings.",
        "updated": "2024-05-08 17:57:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05253v1"
    },
    {
        "title": "You Only Cache Once: Decoder-Decoder Architectures for Language Models",
        "authors": "Yutao SunLi DongYi ZhuShaohan HuangWenhui WangShuming MaQuanlu ZhangJianyong WangFuru Wei",
        "links": "http://arxiv.org/abs/2405.05254v1",
        "entry_id": "http://arxiv.org/abs/2405.05254v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05254v1",
        "summary": "We introduce a decoder-decoder architecture, YOCO, for large language models,\nwhich only caches key-value pairs once. It consists of two components, i.e., a\ncross-decoder stacked upon a self-decoder. The self-decoder efficiently encodes\nglobal key-value (KV) caches that are reused by the cross-decoder via\ncross-attention. The overall model behaves like a decoder-only Transformer,\nalthough YOCO only caches once. The design substantially reduces GPU memory\ndemands, yet retains global attention capability. Additionally, the computation\nflow enables prefilling to early exit without changing the final output,\nthereby significantly speeding up the prefill stage. Experimental results\ndemonstrate that YOCO achieves favorable performance compared to Transformer in\nvarious settings of scaling up model size and number of training tokens. We\nalso extend YOCO to 1M context length with near-perfect needle retrieval\naccuracy. The profiling results show that YOCO improves inference memory,\nprefill latency, and throughput by orders of magnitude across context lengths\nand model sizes. Code is available at https://aka.ms/YOCO.",
        "updated": "2024-05-08 17:57:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05254v1"
    },
    {
        "title": "LLMs with Personalities in Multi-issue Negotiation Games",
        "authors": "Sean NohHo-Chun Herbert Chang",
        "links": "http://arxiv.org/abs/2405.05248v1",
        "entry_id": "http://arxiv.org/abs/2405.05248v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05248v1",
        "summary": "Powered by large language models (LLMs), AI agents have become capable of\nmany human tasks. Using the most canonical definitions of the Big Five\npersonality, we measure the ability of LLMs to negotiate within a\ngame-theoretical framework, as well as methodological challenges to measuring\nnotions of fairness and risk. Simulations (n=1,500) for both single-issue and\nmulti-issue negotiation reveal increase in domain complexity with asymmetric\nissue valuations improve agreement rates but decrease surplus from aggressive\nnegotiation. Through gradient-boosted regression and Shapley explainers, we\nfind high openness, conscientiousness, and neuroticism are associated with fair\ntendencies; low agreeableness and low openness are associated with rational\ntendencies. Low conscientiousness is associated with high toxicity. These\nresults indicate that LLMs may have built-in guardrails that default to fair\nbehavior, but can be \"jail broken\" to exploit agreeable opponents. We also\noffer pragmatic insight in how negotiation bots can be designed, and a\nframework of assessing negotiation behavior based on game theory and\ncomputational social science.",
        "updated": "2024-05-08 17:51:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05248v1"
    },
    {
        "title": "CARE-SD: Classifier-based analysis for recognizing and eliminating stigmatizing and doubt marker labels in electronic health records: model development and validation",
        "authors": "Drew WalkerAnnie ThorneSudeshna DasJennifer LoveHannah LF CooperMelvin Livingston IIIAbeed Sarker",
        "links": "http://arxiv.org/abs/2405.05204v1",
        "entry_id": "http://arxiv.org/abs/2405.05204v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05204v1",
        "summary": "Objective: To detect and classify features of stigmatizing and biased\nlanguage in intensive care electronic health records (EHRs) using natural\nlanguage processing techniques. Materials and Methods: We first created a\nlexicon and regular expression lists from literature-driven stem words for\nlinguistic features of stigmatizing patient labels, doubt markers, and scare\nquotes within EHRs. The lexicon was further extended using Word2Vec and GPT\n3.5, and refined through human evaluation. These lexicons were used to search\nfor matches across 18 million sentences from the de-identified Medical\nInformation Mart for Intensive Care-III (MIMIC-III) dataset. For each\nlinguistic bias feature, 1000 sentence matches were sampled, labeled by expert\nclinical and public health annotators, and used to supervised learning\nclassifiers. Results: Lexicon development from expanded literature stem-word\nlists resulted in a doubt marker lexicon containing 58 expressions, and a\nstigmatizing labels lexicon containing 127 expressions. Classifiers for doubt\nmarkers and stigmatizing labels had the highest performance, with macro\nF1-scores of .84 and .79, positive-label recall and precision values ranging\nfrom .71 to .86, and accuracies aligning closely with human annotator agreement\n(.87). Discussion: This study demonstrated the feasibility of supervised\nclassifiers in automatically identifying stigmatizing labels and doubt markers\nin medical text, and identified trends in stigmatizing language use in an EHR\nsetting. Additional labeled data may help improve lower scare quote model\nperformance. Conclusions: Classifiers developed in this study showed high model\nperformance and can be applied to identify patterns and target interventions to\nreduce stigmatizing labels and doubt markers in healthcare systems.",
        "updated": "2024-05-08 16:40:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05204v1"
    },
    {
        "title": "MIDGARD: Self-Consistency Using Minimum Description Length for Structured Commonsense Reasoning",
        "authors": "Inderjeet NairLu Wang",
        "links": "http://arxiv.org/abs/2405.05189v1",
        "entry_id": "http://arxiv.org/abs/2405.05189v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05189v1",
        "summary": "We study the task of conducting structured reasoning as generating a\nreasoning graph from natural language input using large language models (LLMs).\nPrevious approaches have explored various prompting schemes, yet they suffer\nfrom error propagation due to the autoregressive nature and single-pass-based\ndecoding, which lack error correction capability. Additionally, relying solely\non a single sample may result in the omission of true nodes and edges. To\ncounter this, we draw inspiration from self-consistency (SC), which involves\nsampling a diverse set of reasoning chains and taking the majority vote as the\nfinal answer. To tackle the substantial challenge of applying SC on generated\ngraphs, we propose MIDGARD (MInimum Description length Guided Aggregation of\nReasoning in Directed acyclic graph) that leverages Minimum Description Length\n(MDL)-based formulation to identify consistent properties among the different\ngraph samples generated by an LLM. This formulation helps reject properties\nthat appear in only a few samples, which are likely to be erroneous, while\nenabling the inclusion of missing elements without compromising precision. Our\nmethod demonstrates superior performance than comparisons across various\nstructured reasoning tasks, including argument structure extraction,\nexplanation graph generation, inferring dependency relations among actions for\neveryday tasks, and semantic graph generation from natural texts.",
        "updated": "2024-05-08 16:25:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05189v1"
    }
]