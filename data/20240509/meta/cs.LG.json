[
    {
        "title": "Multi-Modal Data-Efficient 3D Scene Understanding for Autonomous Driving",
        "authors": "Lingdong KongXiang XuJiawei RenWenwei ZhangLiang PanKai ChenWei Tsang OoiZiwei Liu",
        "links": "http://arxiv.org/abs/2405.05258v1",
        "entry_id": "http://arxiv.org/abs/2405.05258v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05258v1",
        "summary": "Efficient data utilization is crucial for advancing 3D scene understanding in\nautonomous driving, where reliance on heavily human-annotated LiDAR point\nclouds challenges fully supervised methods. Addressing this, our study extends\ninto semi-supervised learning for LiDAR semantic segmentation, leveraging the\nintrinsic spatial priors of driving scenes and multi-sensor complements to\naugment the efficacy of unlabeled datasets. We introduce LaserMix++, an evolved\nframework that integrates laser beam manipulations from disparate LiDAR scans\nand incorporates LiDAR-camera correspondences to further assist data-efficient\nlearning. Our framework is tailored to enhance 3D scene consistency\nregularization by incorporating multi-modality, including 1) multi-modal\nLaserMix operation for fine-grained cross-sensor interactions; 2)\ncamera-to-LiDAR feature distillation that enhances LiDAR feature learning; and\n3) language-driven knowledge guidance generating auxiliary supervisions using\nopen-vocabulary models. The versatility of LaserMix++ enables applications\nacross LiDAR representations, establishing it as a universally applicable\nsolution. Our framework is rigorously validated through theoretical analysis\nand extensive experiments on popular driving perception datasets. Results\ndemonstrate that LaserMix++ markedly outperforms fully supervised alternatives,\nachieving comparable accuracy with five times fewer annotations and\nsignificantly improving the supervised-only baselines. This substantial\nadvancement underscores the potential of semi-supervised approaches in reducing\nthe reliance on extensive labeled data in LiDAR-based 3D scene understanding\nsystems.",
        "updated": "2024-05-08 17:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05258v1"
    },
    {
        "title": "THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models",
        "authors": "Prannay KaulZhizhong LiHao YangYonatan DuklerAshwin SwaminathanC. J. TaylorStefano Soatto",
        "links": "http://arxiv.org/abs/2405.05256v1",
        "entry_id": "http://arxiv.org/abs/2405.05256v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05256v1",
        "summary": "Mitigating hallucinations in large vision-language models (LVLMs) remains an\nopen problem. Recent benchmarks do not address hallucinations in open-ended\nfree-form responses, which we term \"Type I hallucinations\". Instead, they focus\non hallucinations responding to very specific question formats -- typically a\nmultiple-choice response regarding a particular object or attribute -- which we\nterm \"Type II hallucinations\". Additionally, such benchmarks often require\nexternal API calls to models which are subject to change. In practice, we\nobserve that a reduction in Type II hallucinations does not lead to a reduction\nin Type I hallucinations but rather that the two forms of hallucinations are\noften anti-correlated. To address this, we propose THRONE, a novel object-based\nautomatic framework for quantitatively evaluating Type I hallucinations in LVLM\nfree-form outputs. We use public language models (LMs) to identify\nhallucinations in LVLM responses and compute informative metrics. By evaluating\na large selection of recent LVLMs using public datasets, we show that an\nimprovement in existing metrics do not lead to a reduction in Type I\nhallucinations, and that established benchmarks for measuring Type I\nhallucinations are incomplete. Finally, we provide a simple and effective data\naugmentation method to reduce Type I and Type II hallucinations as a strong\nbaseline.",
        "updated": "2024-05-08 17:59:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05256v1"
    },
    {
        "title": "Diffusion-HMC: Parameter Inference with Diffusion Model driven Hamiltonian Monte Carlo",
        "authors": "Nayantara MudurCarolina Cuesta-LazaroDouglas P. Finkbeiner",
        "links": "http://arxiv.org/abs/2405.05255v1",
        "entry_id": "http://arxiv.org/abs/2405.05255v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05255v1",
        "summary": "Diffusion generative models have excelled at diverse image generation and\nreconstruction tasks across fields. A less explored avenue is their application\nto discriminative tasks involving regression or classification problems. The\ncornerstone of modern cosmology is the ability to generate predictions for\nobserved astrophysical fields from theory and constrain physical models from\nobservations using these predictions. This work uses a single diffusion\ngenerative model to address these interlinked objectives -- as a surrogate\nmodel or emulator for cold dark matter density fields conditional on input\ncosmological parameters, and as a parameter inference model that solves the\ninverse problem of constraining the cosmological parameters of an input field.\nThe model is able to emulate fields with summary statistics consistent with\nthose of the simulated target distribution. We then leverage the approximate\nlikelihood of the diffusion generative model to derive tight constraints on\ncosmology by using the Hamiltonian Monte Carlo method to sample the posterior\non cosmological parameters for a given test image. Finally, we demonstrate that\nthis parameter inference approach is more robust to the addition of noise than\nbaseline parameter inference networks.",
        "updated": "2024-05-08 17:59:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05255v1"
    },
    {
        "title": "Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models",
        "authors": "Hongjie WangDifan LiuYan KangYijun LiZhe LinNiraj K. JhaYuchen Liu",
        "links": "http://arxiv.org/abs/2405.05252v1",
        "entry_id": "http://arxiv.org/abs/2405.05252v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05252v1",
        "summary": "Diffusion Models (DMs) have exhibited superior performance in generating\nhigh-quality and diverse images. However, this exceptional performance comes at\nthe cost of expensive architectural design, particularly due to the attention\nmodule heavily used in leading models. Existing works mainly adopt a retraining\nprocess to enhance DM efficiency. This is computationally expensive and not\nvery scalable. To this end, we introduce the Attention-driven Training-free\nEfficient Diffusion Model (AT-EDM) framework that leverages attention maps to\nperform run-time pruning of redundant tokens, without the need for any\nretraining. Specifically, for single-denoising-step pruning, we develop a novel\nranking algorithm, Generalized Weighted Page Rank (G-WPR), to identify\nredundant tokens, and a similarity-based recovery method to restore tokens for\nthe convolution operation. In addition, we propose a Denoising-Steps-Aware\nPruning (DSAP) approach to adjust the pruning budget across different denoising\ntimesteps for better generation quality. Extensive evaluations show that AT-EDM\nperforms favorably against prior art in terms of efficiency (e.g., 38.8% FLOPs\nsaving and up to 1.53x speed-up over Stable Diffusion XL) while maintaining\nnearly the same FID and CLIP scores as the full model. Project webpage:\nhttps://atedm.github.io.",
        "updated": "2024-05-08 17:56:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05252v1"
    },
    {
        "title": "Deep learning-based variational autoencoder for classification of quantum and classical states of light",
        "authors": "Mahesh BhupatiAbhishek MallAnshuman KumarPankaj K. Jha",
        "links": "http://arxiv.org/abs/2405.05243v1",
        "entry_id": "http://arxiv.org/abs/2405.05243v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05243v1",
        "summary": "Advancements in optical quantum technologies have been enabled by the\ngeneration, manipulation, and characterization of light, with identification\nbased on its photon statistics. However, characterizing light and its sources\nthrough single photon measurements often requires efficient detectors and\nlonger measurement times to obtain high-quality photon statistics. Here we\nintroduce a deep learning-based variational autoencoder (VAE) method for\nclassifying single photon added coherent state (SPACS), single photon added\nthermal state (SPACS), mixed states between coherent/SPACS and thermal/SPATS of\nlight. Our semisupervised learning-based VAE efficiently maps the photon\nstatistics features of light to a lower dimension, enabling quasi-instantaneous\nclassification with low average photon counts. The proposed VAE method is\nrobust and maintains classification accuracy in the presence of losses inherent\nin an experiment, such as finite collection efficiency, non-unity quantum\nefficiency, finite number of detectors, etc. Additionally, leveraging the\ntransfer learning capabilities of VAE enables successful classification of data\nof any quality using a single trained model. We envision that such a deep\nlearning methodology will enable better classification of quantum light and\nlight sources even in the presence of poor detection quality.",
        "updated": "2024-05-08 17:40:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05243v1"
    }
]