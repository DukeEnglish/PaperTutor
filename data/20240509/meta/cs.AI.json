[
    {
        "title": "THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models",
        "authors": "Prannay KaulZhizhong LiHao YangYonatan DuklerAshwin SwaminathanC. J. TaylorStefano Soatto",
        "links": "http://arxiv.org/abs/2405.05256v1",
        "entry_id": "http://arxiv.org/abs/2405.05256v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05256v1",
        "summary": "Mitigating hallucinations in large vision-language models (LVLMs) remains an\nopen problem. Recent benchmarks do not address hallucinations in open-ended\nfree-form responses, which we term \"Type I hallucinations\". Instead, they focus\non hallucinations responding to very specific question formats -- typically a\nmultiple-choice response regarding a particular object or attribute -- which we\nterm \"Type II hallucinations\". Additionally, such benchmarks often require\nexternal API calls to models which are subject to change. In practice, we\nobserve that a reduction in Type II hallucinations does not lead to a reduction\nin Type I hallucinations but rather that the two forms of hallucinations are\noften anti-correlated. To address this, we propose THRONE, a novel object-based\nautomatic framework for quantitatively evaluating Type I hallucinations in LVLM\nfree-form outputs. We use public language models (LMs) to identify\nhallucinations in LVLM responses and compute informative metrics. By evaluating\na large selection of recent LVLMs using public datasets, we show that an\nimprovement in existing metrics do not lead to a reduction in Type I\nhallucinations, and that established benchmarks for measuring Type I\nhallucinations are incomplete. Finally, we provide a simple and effective data\naugmentation method to reduce Type I and Type II hallucinations as a strong\nbaseline.",
        "updated": "2024-05-08 17:59:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05256v1"
    },
    {
        "title": "Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge",
        "authors": "Charles KoutchemeNicola DaineseSami SarsaArto HellasJuho LeinonenPaul Denny",
        "links": "http://arxiv.org/abs/2405.05253v1",
        "entry_id": "http://arxiv.org/abs/2405.05253v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05253v1",
        "summary": "Large language models (LLMs) have shown great potential for the automatic\ngeneration of feedback in a wide range of computing contexts. However, concerns\nhave been voiced around the privacy and ethical implications of sending student\nwork to proprietary models. This has sparked considerable interest in the use\nof open source LLMs in education, but the quality of the feedback that such\nopen models can produce remains understudied. This is a concern as providing\nflawed or misleading generated feedback could be detrimental to student\nlearning. Inspired by recent work that has utilised very powerful LLMs, such as\nGPT-4, to evaluate the outputs produced by less powerful models, we conduct an\nautomated analysis of the quality of the feedback produced by several open\nsource models using a dataset from an introductory programming course. First,\nwe investigate the viability of employing GPT-4 as an automated evaluator by\ncomparing its evaluations with those of a human expert. We observe that GPT-4\ndemonstrates a bias toward positively rating feedback while exhibiting moderate\nagreement with human raters, showcasing its potential as a feedback evaluator.\nSecond, we explore the quality of feedback generated by several leading\nopen-source LLMs by using GPT-4 to evaluate the feedback. We find that some\nmodels offer competitive performance with popular proprietary LLMs, such as\nChatGPT, indicating opportunities for their responsible use in educational\nsettings.",
        "updated": "2024-05-08 17:57:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05253v1"
    },
    {
        "title": "Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models",
        "authors": "Hongjie WangDifan LiuYan KangYijun LiZhe LinNiraj K. JhaYuchen Liu",
        "links": "http://arxiv.org/abs/2405.05252v1",
        "entry_id": "http://arxiv.org/abs/2405.05252v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05252v1",
        "summary": "Diffusion Models (DMs) have exhibited superior performance in generating\nhigh-quality and diverse images. However, this exceptional performance comes at\nthe cost of expensive architectural design, particularly due to the attention\nmodule heavily used in leading models. Existing works mainly adopt a retraining\nprocess to enhance DM efficiency. This is computationally expensive and not\nvery scalable. To this end, we introduce the Attention-driven Training-free\nEfficient Diffusion Model (AT-EDM) framework that leverages attention maps to\nperform run-time pruning of redundant tokens, without the need for any\nretraining. Specifically, for single-denoising-step pruning, we develop a novel\nranking algorithm, Generalized Weighted Page Rank (G-WPR), to identify\nredundant tokens, and a similarity-based recovery method to restore tokens for\nthe convolution operation. In addition, we propose a Denoising-Steps-Aware\nPruning (DSAP) approach to adjust the pruning budget across different denoising\ntimesteps for better generation quality. Extensive evaluations show that AT-EDM\nperforms favorably against prior art in terms of efficiency (e.g., 38.8% FLOPs\nsaving and up to 1.53x speed-up over Stable Diffusion XL) while maintaining\nnearly the same FID and CLIP scores as the full model. Project webpage:\nhttps://atedm.github.io.",
        "updated": "2024-05-08 17:56:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05252v1"
    },
    {
        "title": "LLMs with Personalities in Multi-issue Negotiation Games",
        "authors": "Sean NohHo-Chun Herbert Chang",
        "links": "http://arxiv.org/abs/2405.05248v1",
        "entry_id": "http://arxiv.org/abs/2405.05248v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05248v1",
        "summary": "Powered by large language models (LLMs), AI agents have become capable of\nmany human tasks. Using the most canonical definitions of the Big Five\npersonality, we measure the ability of LLMs to negotiate within a\ngame-theoretical framework, as well as methodological challenges to measuring\nnotions of fairness and risk. Simulations (n=1,500) for both single-issue and\nmulti-issue negotiation reveal increase in domain complexity with asymmetric\nissue valuations improve agreement rates but decrease surplus from aggressive\nnegotiation. Through gradient-boosted regression and Shapley explainers, we\nfind high openness, conscientiousness, and neuroticism are associated with fair\ntendencies; low agreeableness and low openness are associated with rational\ntendencies. Low conscientiousness is associated with high toxicity. These\nresults indicate that LLMs may have built-in guardrails that default to fair\nbehavior, but can be \"jail broken\" to exploit agreeable opponents. We also\noffer pragmatic insight in how negotiation bots can be designed, and a\nframework of assessing negotiation behavior based on game theory and\ncomputational social science.",
        "updated": "2024-05-08 17:51:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05248v1"
    },
    {
        "title": "SVDD Challenge 2024: A Singing Voice Deepfake Detection Challenge Evaluation Plan",
        "authors": "You ZhangYongyi ZangJiatong ShiRyuichi YamamotoJionghao HanYuxun TangTomoki TodaZhiyao Duan",
        "links": "http://arxiv.org/abs/2405.05244v1",
        "entry_id": "http://arxiv.org/abs/2405.05244v1",
        "pdf_url": "http://arxiv.org/pdf/2405.05244v1",
        "summary": "The rapid advancement of AI-generated singing voices, which now closely mimic\nnatural human singing and align seamlessly with musical scores, has led to\nheightened concerns for artists and the music industry. Unlike spoken voice,\nsinging voice presents unique challenges due to its musical nature and the\npresence of strong background music, making singing voice deepfake detection\n(SVDD) a specialized field requiring focused attention. To promote SVDD\nresearch, we recently proposed the \"SVDD Challenge,\" the very first research\nchallenge focusing on SVDD for lab-controlled and in-the-wild bonafide and\ndeepfake singing voice recordings. The challenge will be held in conjunction\nwith the 2024 IEEE Spoken Language Technology Workshop (SLT 2024).",
        "updated": "2024-05-08 17:40:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.05244v1"
    }
]