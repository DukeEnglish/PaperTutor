[
    {
        "title": "Implicit Personalization in Language Models: A Systematic Study",
        "authors": "Zhijing JinNils HeilJiarui LiuShehzaad DhuliawalaYahang QiBernhard SchölkopfRada MihalceaMrinmaya Sachan",
        "links": "http://arxiv.org/abs/2405.14808v1",
        "entry_id": "http://arxiv.org/abs/2405.14808v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14808v1",
        "summary": "Implicit Personalization (IP) is a phenomenon of language models inferring a\nuser's background from the implicit cues in the input prompts and tailoring the\nresponse based on this inference. While previous work has touched upon various\ninstances of this problem, there lacks a unified framework to study this\nbehavior. This work systematically studies IP through a rigorous mathematical\nformulation, a multi-perspective moral reasoning framework, and a set of case\nstudies. Our theoretical foundation for IP relies on a structural causal model\nand introduces a novel method, indirect intervention, to estimate the causal\neffect of a mediator variable that cannot be directly intervened upon. Beyond\nthe technical approach, we also introduce a set of moral reasoning principles\nbased on three schools of moral philosophy to study when IP may or may not be\nethically appropriate. Equipped with both mathematical and ethical insights, we\npresent three diverse case studies illustrating the varied nature of the IP\nproblem and offer recommendations for future research. Our code and data are at\nhttps://github.com/jiarui-liu/IP.",
        "updated": "2024-05-23 17:18:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14808v1"
    },
    {
        "title": "RetAssist: Facilitating Vocabulary Learners with Generative Images in Story Retelling Practices",
        "authors": "Qiaoyi ChenSiyu LiuKaihui HuangXingbo WangXiaojuan MaJunkai ZhuZhenhui Peng",
        "links": "http://dx.doi.org/10.1145/3643834.3661581",
        "entry_id": "http://arxiv.org/abs/2405.14794v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14794v1",
        "summary": "Reading and repeatedly retelling a short story is a common and effective\napproach to learning the meanings and usages of target words. However, learners\noften struggle with comprehending, recalling, and retelling the story contexts\nof these target words. Inspired by the Cognitive Theory of Multimedia Learning,\nwe propose a computational workflow to generate relevant images paired with\nstories. Based on the workflow, we work with learners and teachers to\niteratively design an interactive vocabulary learning system named RetAssist.\nIt can generate sentence-level images of a story to facilitate the\nunderstanding and recall of the target words in the story retelling practices.\nOur within-subjects study (N=24) shows that compared to a baseline system\nwithout generative images, RetAssist significantly improves learners' fluency\nin expressing with target words. Participants also feel that RetAssist eases\ntheir learning workload and is more useful. We discuss insights into leveraging\ntext-to-image generative models to support learning tasks.",
        "updated": "2024-05-23 17:05:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14794v1"
    },
    {
        "title": "Low-Energy Line Codes for On-Chip Networks",
        "authors": "Beyza DabakMajor GlennJingyang LiuAlexander BuckSiyi YangRobert CalderbankNatalie Enright JergerDaniel J. Sorin",
        "links": "http://arxiv.org/abs/2405.14783v1",
        "entry_id": "http://arxiv.org/abs/2405.14783v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14783v1",
        "summary": "Energy is a primary constraint in processor design, and much of that energy\nis consumed in on-chip communication. Communication can be intra-core (e.g.,\nfrom a register file to an ALU) or inter-core (e.g., over the on-chip network).\nIn this paper, we use the on-chip network (OCN) as a case study for saving\non-chip communication energy. We have identified a new way to reduce the OCN's\nlink energy consumption by using line coding, a longstanding technique in\ninformation theory. Our line codes, called Low-Energy Line Codes (LELCs),\nreduce energy by reducing the frequency of voltage transitions of the links,\nand they achieve a range of energy/performance trade-offs.",
        "updated": "2024-05-23 16:52:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14783v1"
    },
    {
        "title": "A Transformer-Based Approach for Smart Invocation of Automatic Code Completion",
        "authors": "Aral de MoorArie van DeursenMaliheh Izadi",
        "links": "http://dx.doi.org/10.1145/3664646.3664760",
        "entry_id": "http://arxiv.org/abs/2405.14753v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14753v1",
        "summary": "Transformer-based language models are highly effective for code completion,\nwith much research dedicated to enhancing the content of these completions.\nDespite their effectiveness, these models come with high operational costs and\ncan be intrusive, especially when they suggest too often and interrupt\ndevelopers who are concentrating on their work. Current research largely\noverlooks how these models interact with developers in practice and neglects to\naddress when a developer should receive completion suggestions. To tackle this\nissue, we developed a machine learning model that can accurately predict when\nto invoke a code completion tool given the code context and available telemetry\ndata.\n  To do so, we collect a dataset of 200k developer interactions with our\ncross-IDE code completion plugin and train several invocation filtering models.\nOur results indicate that our small-scale transformer model significantly\noutperforms the baseline while maintaining low enough latency. We further\nexplore the search space for integrating additional telemetry data into a\npre-trained transformer directly and obtain promising results. To further\ndemonstrate our approach's practical potential, we deployed the model in an\nonline environment with 34 developers and provided real-world insights based on\n74k actual invocations.",
        "updated": "2024-05-23 16:19:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14753v1"
    },
    {
        "title": "HTN-Based Tutors: A New Intelligent Tutoring Framework Based on Hierarchical Task Networks",
        "authors": "Momin N. SiddiquiAdit GuptaJennifer M. ReddigChristopher J. Maclellan",
        "links": "http://dx.doi.org/10.1145/3657604",
        "entry_id": "http://arxiv.org/abs/2405.14716v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14716v1",
        "summary": "Intelligent tutors have shown success in delivering a personalized and\nadaptive learning experience. However, there exist challenges regarding the\ngranularity of knowledge in existing frameworks and the resulting instructions\nthey can provide. To address these issues, we propose HTN-based tutors, a new\nintelligent tutoring framework that represents expert models using Hierarchical\nTask Networks (HTNs). Like other tutoring frameworks, it allows flexible\nencoding of different problem-solving strategies while providing the additional\nbenefit of a hierarchical knowledge organization. We leverage the latter to\ncreate tutors that can adapt the granularity of their scaffolding. This\norganization also aligns well with the compositional nature of skills.",
        "updated": "2024-05-23 15:46:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14716v1"
    }
]