[
    {
        "title": "A Nurse is Blue and Elephant is Rugby: Cross Domain Alignment in Large Language Models Reveal Human-like Patterns",
        "authors": "Asaf YehudaiTaelin KaridiGabriel StanovskyAriel GoldsteinOmri Abend",
        "links": "http://arxiv.org/abs/2405.14863v1",
        "entry_id": "http://arxiv.org/abs/2405.14863v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14863v1",
        "summary": "Cross-domain alignment refers to the task of mapping a concept from one\ndomain to another. For example, ``If a \\textit{doctor} were a \\textit{color},\nwhat color would it be?''. This seemingly peculiar task is designed to\ninvestigate how people represent concrete and abstract concepts through their\nmappings between categories and their reasoning processes over those mappings.\nIn this paper, we adapt this task from cognitive science to evaluate the\nconceptualization and reasoning abilities of large language models (LLMs)\nthrough a behavioral study. We examine several LLMs by prompting them with a\ncross-domain mapping task and analyzing their responses at both the population\nand individual levels. Additionally, we assess the models' ability to reason\nabout their predictions by analyzing and categorizing their explanations for\nthese mappings. The results reveal several similarities between humans' and\nmodels' mappings and explanations, suggesting that models represent concepts\nsimilarly to humans. This similarity is evident not only in the model\nrepresentation but also in their behavior. Furthermore, the models mostly\nprovide valid explanations and deploy reasoning paths that are similar to those\nof humans.",
        "updated": "2024-05-23 17:59:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14863v1"
    },
    {
        "title": "Bitune: Bidirectional Instruction-Tuning",
        "authors": "Dawid J. KopiczkoTijmen BlankevoortYuki M. Asano",
        "links": "http://arxiv.org/abs/2405.14862v1",
        "entry_id": "http://arxiv.org/abs/2405.14862v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14862v1",
        "summary": "We introduce Bitune, a method that improves instruction-tuning of pretrained\ndecoder-only large language models, leading to consistent gains on downstream\ntasks. Bitune applies both causal and bidirectional attention to the prompt, to\nobtain a better representation of the query or instruction. We realize this by\nintroducing two sets of parameters, for which we apply parameter-efficient\nfinetuning techniques. These causal and bidirectional features are then\ncombined into a weighted average with trainable coefficients, which is\nsubsequently used to generate new tokens. We demonstrate significant\nimprovements in zero-shot performance on commonsense reasoning, arithmetic, and\nlanguage understanding tasks, while extensive ablation studies validate the\nrole of each component and demonstrate the method's agnosticism to different\nPEFT techniques.",
        "updated": "2024-05-23 17:59:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14862v1"
    },
    {
        "title": "A Textbook Remedy for Domain Shifts: Knowledge Priors for Medical Image Analysis",
        "authors": "Yue YangMona GandhiYufei WangYifan WuMichael S. YaoChris Callison-BurchJames C. GeeMark Yatskar",
        "links": "http://arxiv.org/abs/2405.14839v1",
        "entry_id": "http://arxiv.org/abs/2405.14839v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14839v1",
        "summary": "While deep networks have achieved broad success in analyzing natural images,\nwhen applied to medical scans, they often fail in unexcepted situations. We\ninvestigate this challenge and focus on model sensitivity to domain shifts,\nsuch as data sampled from different hospitals or data confounded by demographic\nvariables such as sex, race, etc, in the context of chest X-rays and skin\nlesion images. A key finding we show empirically is that existing visual\nbackbones lack an appropriate prior from the architecture for reliable\ngeneralization in these settings. Taking inspiration from medical training, we\npropose giving deep networks a prior grounded in explicit medical knowledge\ncommunicated in natural language. To this end, we introduce Knowledge-enhanced\nBottlenecks (KnoBo), a class of concept bottleneck models that incorporates\nknowledge priors that constrain it to reason with clinically relevant factors\nfound in medical textbooks or PubMed. KnoBo uses retrieval-augmented language\nmodels to design an appropriate concept space paired with an automatic training\nprocedure for recognizing the concept. We evaluate different resources of\nknowledge and recognition architectures on a broad range of domain shifts\nacross 20 datasets. In our comprehensive evaluation with two imaging\nmodalities, KnoBo outperforms fine-tuned models on confounded datasets by 32.4%\non average. Finally, evaluations reveal that PubMed is a promising resource for\nmaking medical models less sensitive to domain shift, outperforming other\nresources on both diversity of information and final prediction performance.",
        "updated": "2024-05-23 17:55:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14839v1"
    },
    {
        "title": "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step",
        "authors": "Yuntian DengYejin ChoiStuart Shieber",
        "links": "http://arxiv.org/abs/2405.14838v1",
        "entry_id": "http://arxiv.org/abs/2405.14838v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14838v1",
        "summary": "When leveraging language models for reasoning tasks, generating explicit\nchain-of-thought (CoT) steps often proves essential for achieving high accuracy\nin final outputs. In this paper, we investigate if models can be taught to\ninternalize these CoT steps. To this end, we propose a simple yet effective\nmethod for internalizing CoT steps: starting with a model trained for explicit\nCoT reasoning, we gradually remove the intermediate steps and finetune the\nmodel. This process allows the model to internalize the intermediate reasoning\nsteps, thus simplifying the reasoning process while maintaining high\nperformance. Our approach enables a GPT-2 Small model to solve 9-by-9\nmultiplication with up to 99% accuracy, whereas standard training cannot solve\nbeyond 4-by-4 multiplication. Furthermore, our method proves effective on\nlarger language models, such as Mistral 7B, achieving over 50% accuracy on\nGSM8K without producing any intermediate steps.",
        "updated": "2024-05-23 17:54:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14838v1"
    },
    {
        "title": "HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models",
        "authors": "Bernal Jiménez GutiérrezYiheng ShuYu GuMichihiro YasunagaYu Su",
        "links": "http://arxiv.org/abs/2405.14831v1",
        "entry_id": "http://arxiv.org/abs/2405.14831v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14831v1",
        "summary": "In order to thrive in hostile and ever-changing natural environments,\nmammalian brains evolved to store large amounts of knowledge about the world\nand continually integrate new information while avoiding catastrophic\nforgetting. Despite the impressive accomplishments, large language models\n(LLMs), even with retrieval-augmented generation (RAG), still struggle to\nefficiently and effectively integrate a large amount of new experiences after\npre-training. In this work, we introduce HippoRAG, a novel retrieval framework\ninspired by the hippocampal indexing theory of human long-term memory to enable\ndeeper and more efficient knowledge integration over new experiences. HippoRAG\nsynergistically orchestrates LLMs, knowledge graphs, and the Personalized\nPageRank algorithm to mimic the different roles of neocortex and hippocampus in\nhuman memory. We compare HippoRAG with existing RAG methods on multi-hop\nquestion answering and show that our method outperforms the state-of-the-art\nmethods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves\ncomparable or better performance than iterative retrieval like IRCoT while\nbeing 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into\nIRCoT brings further substantial gains. Finally, we show that our method can\ntackle new types of scenarios that are out of reach of existing methods. Code\nand data are available at https://github.com/OSU-NLP-Group/HippoRAG.",
        "updated": "2024-05-23 17:47:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14831v1"
    }
]