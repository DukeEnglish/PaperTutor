[
    {
        "title": "Federated Online Adaptation for Deep Stereo",
        "authors": "Matteo PoggiFabio Tosi",
        "links": "http://arxiv.org/abs/2405.14873v1",
        "entry_id": "http://arxiv.org/abs/2405.14873v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14873v1",
        "summary": "We introduce a novel approach for adapting deep stereo networks in a\ncollaborative manner. By building over principles of federated learning, we\ndevelop a distributed framework allowing for demanding the optimization process\nto a number of clients deployed in different environments. This makes it\npossible, for a deep stereo network running on resourced-constrained devices,\nto capitalize on the adaptation process carried out by other instances of the\nsame architecture, and thus improve its accuracy in challenging environments\neven when it cannot carry out adaptation on its own. Experimental results show\nhow federated adaptation performs equivalently to on-device adaptation, and\neven better when dealing with challenging environments.",
        "updated": "2024-05-23 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14873v1"
    },
    {
        "title": "An Empirical Study of Training State-of-the-Art LiDAR Segmentation Models",
        "authors": "Jiahao SunXiang XuLingdong KongYouquan LiuLi LiChenming ZhuJingwei ZhangZeqi XiaoRunnan ChenTai WangWenwei ZhangKai ChenChunmei Qing",
        "links": "http://arxiv.org/abs/2405.14870v1",
        "entry_id": "http://arxiv.org/abs/2405.14870v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14870v1",
        "summary": "In the rapidly evolving field of autonomous driving, precise segmentation of\nLiDAR data is crucial for understanding complex 3D environments. Traditional\napproaches often rely on disparate, standalone codebases, hindering unified\nadvancements and fair benchmarking across models. To address these challenges,\nwe introduce MMDetection3D-lidarseg, a comprehensive toolbox designed for the\nefficient training and evaluation of state-of-the-art LiDAR segmentation\nmodels. We support a wide range of segmentation models and integrate advanced\ndata augmentation techniques to enhance robustness and generalization.\nAdditionally, the toolbox provides support for multiple leading sparse\nconvolution backends, optimizing computational efficiency and performance. By\nfostering a unified framework, MMDetection3D-lidarseg streamlines development\nand benchmarking, setting new standards for research and application. Our\nextensive benchmark experiments on widely-used datasets demonstrate the\neffectiveness of the toolbox. The codebase and trained models have been\npublicly available, promoting further research and innovation in the field of\nLiDAR segmentation for autonomous driving.",
        "updated": "2024-05-23 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14870v1"
    },
    {
        "title": "NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections",
        "authors": "Dor VerbinPratul P. SrinivasanPeter HedmanBen MildenhallBenjamin AttalRichard SzeliskiJonathan T. Barron",
        "links": "http://arxiv.org/abs/2405.14871v1",
        "entry_id": "http://arxiv.org/abs/2405.14871v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14871v1",
        "summary": "Neural Radiance Fields (NeRFs) typically struggle to reconstruct and render\nhighly specular objects, whose appearance varies quickly with changes in\nviewpoint. Recent works have improved NeRF's ability to render detailed\nspecular appearance of distant environment illumination, but are unable to\nsynthesize consistent reflections of closer content. Moreover, these techniques\nrely on large computationally-expensive neural networks to model outgoing\nradiance, which severely limits optimization and rendering speed. We address\nthese issues with an approach based on ray tracing: instead of querying an\nexpensive neural network for the outgoing view-dependent radiance at points\nalong each camera ray, our model casts reflection rays from these points and\ntraces them through the NeRF representation to render feature vectors which are\ndecoded into color using a small inexpensive network. We demonstrate that our\nmodel outperforms prior methods for view synthesis of scenes containing shiny\nobjects, and that it is the only existing NeRF method that can synthesize\nphotorealistic specular appearance and reflections in real-world scenes, while\nrequiring comparable optimization time to current state-of-the-art view\nsynthesis models.",
        "updated": "2024-05-23 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14871v1"
    },
    {
        "title": "PuzzleAvatar: Assembling 3D Avatars from Personal Albums",
        "authors": "Yuliang XiuYufei YeZhen LiuDimitrios TzionasMichael J. Black",
        "links": "http://arxiv.org/abs/2405.14869v1",
        "entry_id": "http://arxiv.org/abs/2405.14869v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14869v1",
        "summary": "Generating personalized 3D avatars is crucial for AR/VR. However, recent\ntext-to-3D methods that generate avatars for celebrities or fictional\ncharacters, struggle with everyday people. Methods for faithful reconstruction\ntypically require full-body images in controlled settings. What if a user could\njust upload their personal \"OOTD\" (Outfit Of The Day) photo collection and get\na faithful avatar in return? The challenge is that such casual photo\ncollections contain diverse poses, challenging viewpoints, cropped views, and\nocclusion (albeit with a consistent outfit, accessories and hairstyle). We\naddress this novel \"Album2Human\" task by developing PuzzleAvatar, a novel model\nthat generates a faithful 3D avatar (in a canonical pose) from a personal OOTD\nalbum, while bypassing the challenging estimation of body and camera pose. To\nthis end, we fine-tune a foundational vision-language model (VLM) on such\nphotos, encoding the appearance, identity, garments, hairstyles, and\naccessories of a person into (separate) learned tokens and instilling these\ncues into the VLM. In effect, we exploit the learned tokens as \"puzzle pieces\"\nfrom which we assemble a faithful, personalized 3D avatar. Importantly, we can\ncustomize avatars by simply inter-changing tokens. As a benchmark for this new\ntask, we collect a new dataset, called PuzzleIOI, with 41 subjects in a total\nof nearly 1K OOTD configurations, in challenging partial photos with paired\nground-truth 3D bodies. Evaluation shows that PuzzleAvatar not only has high\nreconstruction accuracy, outperforming TeCH and MVDreamBooth, but also a unique\nscalability to album photos, and strong robustness. Our model and data will be\npublic.",
        "updated": "2024-05-23 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14869v1"
    },
    {
        "title": "Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis",
        "authors": "Basile Van HoorickRundi WuEge OzgurogluKyle SargentRuoshi LiuPavel TokmakovAchal DaveChangxi ZhengCarl Vondrick",
        "links": "http://arxiv.org/abs/2405.14868v1",
        "entry_id": "http://arxiv.org/abs/2405.14868v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14868v1",
        "summary": "Accurate reconstruction of complex dynamic scenes from just a single\nviewpoint continues to be a challenging task in computer vision. Current\ndynamic novel view synthesis methods typically require videos from many\ndifferent camera viewpoints, necessitating careful recording setups, and\nsignificantly restricting their utility in the wild as well as in terms of\nembodied AI applications. In this paper, we propose $\\textbf{GCD}$, a\ncontrollable monocular dynamic view synthesis pipeline that leverages\nlarge-scale diffusion priors to, given a video of any scene, generate a\nsynchronous video from any other chosen perspective, conditioned on a set of\nrelative camera pose parameters. Our model does not require depth as input, and\ndoes not explicitly model 3D scene geometry, instead performing end-to-end\nvideo-to-video translation in order to achieve its goal efficiently. Despite\nbeing trained on synthetic multi-view video data only, zero-shot real-world\ngeneralization experiments show promising results in multiple domains,\nincluding robotics, object permanence, and driving environments. We believe our\nframework can potentially unlock powerful applications in rich dynamic scene\nunderstanding, perception for robotics, and interactive 3D video viewing\nexperiences for virtual reality.",
        "updated": "2024-05-23 17:59:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14868v1"
    }
]