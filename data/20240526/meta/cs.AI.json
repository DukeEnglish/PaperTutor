[
    {
        "title": "PuzzleAvatar: Assembling 3D Avatars from Personal Albums",
        "authors": "Yuliang XiuYufei YeZhen LiuDimitrios TzionasMichael J. Black",
        "links": "http://arxiv.org/abs/2405.14869v1",
        "entry_id": "http://arxiv.org/abs/2405.14869v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14869v1",
        "summary": "Generating personalized 3D avatars is crucial for AR/VR. However, recent\ntext-to-3D methods that generate avatars for celebrities or fictional\ncharacters, struggle with everyday people. Methods for faithful reconstruction\ntypically require full-body images in controlled settings. What if a user could\njust upload their personal \"OOTD\" (Outfit Of The Day) photo collection and get\na faithful avatar in return? The challenge is that such casual photo\ncollections contain diverse poses, challenging viewpoints, cropped views, and\nocclusion (albeit with a consistent outfit, accessories and hairstyle). We\naddress this novel \"Album2Human\" task by developing PuzzleAvatar, a novel model\nthat generates a faithful 3D avatar (in a canonical pose) from a personal OOTD\nalbum, while bypassing the challenging estimation of body and camera pose. To\nthis end, we fine-tune a foundational vision-language model (VLM) on such\nphotos, encoding the appearance, identity, garments, hairstyles, and\naccessories of a person into (separate) learned tokens and instilling these\ncues into the VLM. In effect, we exploit the learned tokens as \"puzzle pieces\"\nfrom which we assemble a faithful, personalized 3D avatar. Importantly, we can\ncustomize avatars by simply inter-changing tokens. As a benchmark for this new\ntask, we collect a new dataset, called PuzzleIOI, with 41 subjects in a total\nof nearly 1K OOTD configurations, in challenging partial photos with paired\nground-truth 3D bodies. Evaluation shows that PuzzleAvatar not only has high\nreconstruction accuracy, outperforming TeCH and MVDreamBooth, but also a unique\nscalability to album photos, and strong robustness. Our model and data will be\npublic.",
        "updated": "2024-05-23 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14869v1"
    },
    {
        "title": "Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis",
        "authors": "Basile Van HoorickRundi WuEge OzgurogluKyle SargentRuoshi LiuPavel TokmakovAchal DaveChangxi ZhengCarl Vondrick",
        "links": "http://arxiv.org/abs/2405.14868v1",
        "entry_id": "http://arxiv.org/abs/2405.14868v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14868v1",
        "summary": "Accurate reconstruction of complex dynamic scenes from just a single\nviewpoint continues to be a challenging task in computer vision. Current\ndynamic novel view synthesis methods typically require videos from many\ndifferent camera viewpoints, necessitating careful recording setups, and\nsignificantly restricting their utility in the wild as well as in terms of\nembodied AI applications. In this paper, we propose $\\textbf{GCD}$, a\ncontrollable monocular dynamic view synthesis pipeline that leverages\nlarge-scale diffusion priors to, given a video of any scene, generate a\nsynchronous video from any other chosen perspective, conditioned on a set of\nrelative camera pose parameters. Our model does not require depth as input, and\ndoes not explicitly model 3D scene geometry, instead performing end-to-end\nvideo-to-video translation in order to achieve its goal efficiently. Despite\nbeing trained on synthetic multi-view video data only, zero-shot real-world\ngeneralization experiments show promising results in multiple domains,\nincluding robotics, object permanence, and driving environments. We believe our\nframework can potentially unlock powerful applications in rich dynamic scene\nunderstanding, perception for robotics, and interactive 3D video viewing\nexperiences for virtual reality.",
        "updated": "2024-05-23 17:59:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14868v1"
    },
    {
        "title": "A Nurse is Blue and Elephant is Rugby: Cross Domain Alignment in Large Language Models Reveal Human-like Patterns",
        "authors": "Asaf YehudaiTaelin KaridiGabriel StanovskyAriel GoldsteinOmri Abend",
        "links": "http://arxiv.org/abs/2405.14863v1",
        "entry_id": "http://arxiv.org/abs/2405.14863v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14863v1",
        "summary": "Cross-domain alignment refers to the task of mapping a concept from one\ndomain to another. For example, ``If a \\textit{doctor} were a \\textit{color},\nwhat color would it be?''. This seemingly peculiar task is designed to\ninvestigate how people represent concrete and abstract concepts through their\nmappings between categories and their reasoning processes over those mappings.\nIn this paper, we adapt this task from cognitive science to evaluate the\nconceptualization and reasoning abilities of large language models (LLMs)\nthrough a behavioral study. We examine several LLMs by prompting them with a\ncross-domain mapping task and analyzing their responses at both the population\nand individual levels. Additionally, we assess the models' ability to reason\nabout their predictions by analyzing and categorizing their explanations for\nthese mappings. The results reveal several similarities between humans' and\nmodels' mappings and explanations, suggesting that models represent concepts\nsimilarly to humans. This similarity is evident not only in the model\nrepresentation but also in their behavior. Furthermore, the models mostly\nprovide valid explanations and deploy reasoning paths that are similar to those\nof humans.",
        "updated": "2024-05-23 17:59:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14863v1"
    },
    {
        "title": "Adapting to Unknown Low-Dimensional Structures in Score-Based Diffusion Models",
        "authors": "Gen LiYuling Yan",
        "links": "http://arxiv.org/abs/2405.14861v1",
        "entry_id": "http://arxiv.org/abs/2405.14861v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14861v1",
        "summary": "This paper investigates score-based diffusion models when the underlying\ntarget distribution is concentrated on or near low-dimensional manifolds within\nthe higher-dimensional space in which they formally reside, a common\ncharacteristic of natural image distributions. Despite previous efforts to\nunderstand the data generation process of diffusion models, existing\ntheoretical support remains highly suboptimal in the presence of\nlow-dimensional structure, which we strengthen in this paper. For the popular\nDenoising Diffusion Probabilistic Model (DDPM), we find that the dependency of\nthe error incurred within each denoising step on the ambient dimension $d$ is\nin general unavoidable. We further identify a unique design of coefficients\nthat yields a converges rate at the order of $O(k^{2}/\\sqrt{T})$ (up to log\nfactors), where $k$ is the intrinsic dimension of the target distribution and\n$T$ is the number of steps. This represents the first theoretical demonstration\nthat the DDPM sampler can adapt to unknown low-dimensional structures in the\ntarget distribution, highlighting the critical importance of coefficient\ndesign. All of this is achieved by a novel set of analysis tools that\ncharacterize the algorithmic dynamics in a more deterministic manner.",
        "updated": "2024-05-23 17:59:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14861v1"
    },
    {
        "title": "Semantica: An Adaptable Image-Conditioned Diffusion Model",
        "authors": "Manoj KumarNeil HoulsbyEmiel Hoogeboom",
        "links": "http://arxiv.org/abs/2405.14857v1",
        "entry_id": "http://arxiv.org/abs/2405.14857v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14857v1",
        "summary": "We investigate the task of adapting image generative models to different\ndatasets without finetuneing. To this end, we introduce Semantica, an\nimage-conditioned diffusion model capable of generating images based on the\nsemantics of a conditioning image. Semantica is trained exclusively on\nweb-scale image pairs, that is it receives a random image from a webpage as\nconditional input and models another random image from the same webpage. Our\nexperiments highlight the expressivity of pretrained image encoders and\nnecessity of semantic-based data filtering in achieving high-quality image\ngeneration. Once trained, it can adaptively generate new images from a dataset\nby simply using images from that dataset as input. We study the transfer\nproperties of Semantica on ImageNet, LSUN Churches, LSUN Bedroom and SUN397.",
        "updated": "2024-05-23 17:58:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14857v1"
    }
]