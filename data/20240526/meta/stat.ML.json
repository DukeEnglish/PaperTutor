[
    {
        "title": "Adapting to Unknown Low-Dimensional Structures in Score-Based Diffusion Models",
        "authors": "Gen LiYuling Yan",
        "links": "http://arxiv.org/abs/2405.14861v1",
        "entry_id": "http://arxiv.org/abs/2405.14861v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14861v1",
        "summary": "This paper investigates score-based diffusion models when the underlying\ntarget distribution is concentrated on or near low-dimensional manifolds within\nthe higher-dimensional space in which they formally reside, a common\ncharacteristic of natural image distributions. Despite previous efforts to\nunderstand the data generation process of diffusion models, existing\ntheoretical support remains highly suboptimal in the presence of\nlow-dimensional structure, which we strengthen in this paper. For the popular\nDenoising Diffusion Probabilistic Model (DDPM), we find that the dependency of\nthe error incurred within each denoising step on the ambient dimension $d$ is\nin general unavoidable. We further identify a unique design of coefficients\nthat yields a converges rate at the order of $O(k^{2}/\\sqrt{T})$ (up to log\nfactors), where $k$ is the intrinsic dimension of the target distribution and\n$T$ is the number of steps. This represents the first theoretical demonstration\nthat the DDPM sampler can adapt to unknown low-dimensional structures in the\ntarget distribution, highlighting the critical importance of coefficient\ndesign. All of this is achieved by a novel set of analysis tools that\ncharacterize the algorithmic dynamics in a more deterministic manner.",
        "updated": "2024-05-23 17:59:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14861v1"
    },
    {
        "title": "Local Causal Discovery for Structural Evidence of Direct Discrimination",
        "authors": "Jacqueline MaaschKyra GanViolet ChenAgni OrfanoudakiNil-Jana AkpinarFei Wang",
        "links": "http://arxiv.org/abs/2405.14848v1",
        "entry_id": "http://arxiv.org/abs/2405.14848v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14848v1",
        "summary": "Fairness is a critical objective in policy design and algorithmic\ndecision-making. Identifying the causal pathways of unfairness requires\nknowledge of the underlying structural causal model, which may be incomplete or\nunavailable. This limits the practicality of causal fairness analysis in\ncomplex or low-knowledge domains. To mitigate this practicality gap, we\nadvocate for developing efficient causal discovery methods for fairness\napplications. To this end, we introduce local discovery for direct\ndiscrimination (LD3): a polynomial-time algorithm that recovers structural\nevidence of direct discrimination. LD3 performs a linear number of conditional\nindependence tests with respect to variable set size. Moreover, we propose a\ngraphical criterion for identifying the weighted controlled direct effect\n(CDE), a qualitative measure of direct discrimination. We prove that this\ncriterion is satisfied by the knowledge returned by LD3, increasing the\naccessibility of the weighted CDE as a causal fairness measure. Taking liver\ntransplant allocation as a case study, we highlight the potential impact of LD3\nfor modeling fairness in complex decision systems. Results on real-world data\ndemonstrate more plausible causal relations than baselines, which took 197x to\n5870x longer to execute.",
        "updated": "2024-05-23 17:56:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14848v1"
    },
    {
        "title": "Differentiable Annealed Importance Sampling Minimizes The Jensen-Shannon Divergence Between Initial and Target Distribution",
        "authors": "Johannes ZennRobert Bamler",
        "links": "http://arxiv.org/abs/2405.14840v1",
        "entry_id": "http://arxiv.org/abs/2405.14840v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14840v1",
        "summary": "Differentiable annealed importance sampling (DAIS), proposed by Geffner &\nDomke (2021) and Zhang et al. (2021), allows optimizing, among others, over the\ninitial distribution of AIS. In this paper, we show that, in the limit of many\ntransitions, DAIS minimizes the symmetrized KL divergence (Jensen-Shannon\ndivergence) between the initial and target distribution. Thus, DAIS can be seen\nas a form of variational inference (VI) in that its initial distribution is a\nparametric fit to an intractable target distribution. We empirically evaluate\nthe usefulness of the initial distribution as a variational distribution on\nsynthetic and real-world data, observing that it often provides more accurate\nuncertainty estimates than standard VI (optimizing the reverse KL divergence),\nimportance weighted VI, and Markovian score climbing (optimizing the forward KL\ndivergence).",
        "updated": "2024-05-23 17:55:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14840v1"
    },
    {
        "title": "PaGoDA: Progressive Growing of a One-Step Generator from a Low-Resolution Diffusion Teacher",
        "authors": "Dongjun KimChieh-Hsin LaiWei-Hsiang LiaoYuhta TakidaNaoki MurataToshimitsu UesakaYuki MitsufujiStefano Ermon",
        "links": "http://arxiv.org/abs/2405.14822v1",
        "entry_id": "http://arxiv.org/abs/2405.14822v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14822v1",
        "summary": "To accelerate sampling, diffusion models (DMs) are often distilled into\ngenerators that directly map noise to data in a single step. In this approach,\nthe resolution of the generator is fundamentally limited by that of the teacher\nDM. To overcome this limitation, we propose Progressive Growing of Diffusion\nAutoencoder (PaGoDA), a technique to progressively grow the resolution of the\ngenerator beyond that of the original teacher DM. Our key insight is that a\npre-trained, low-resolution DM can be used to deterministically encode\nhigh-resolution data to a structured latent space by solving the PF-ODE forward\nin time (data-to-noise), starting from an appropriately down-sampled image.\nUsing this frozen encoder in an auto-encoder framework, we train a decoder by\nprogressively growing its resolution. From the nature of progressively growing\ndecoder, PaGoDA avoids re-training teacher/student models when we upsample the\nstudent model, making the whole training pipeline much cheaper. In experiments,\nwe used our progressively growing decoder to upsample from the pre-trained\nmodel's 64x64 resolution to generate 512x512 samples, achieving 2x faster\ninference compared to single-step distilled Stable Diffusion like LCM. PaGoDA\nalso achieved state-of-the-art FIDs on ImageNet across all resolutions from\n64x64 to 512x512. Additionally, we demonstrated PaGoDA's effectiveness in\nsolving inverse problems and enabling controllable generation.",
        "updated": "2024-05-23 17:39:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14822v1"
    },
    {
        "title": "Lorentz-Equivariant Geometric Algebra Transformers for High-Energy Physics",
        "authors": "Jonas SpinnerVictor BresóPim de HaanTilman PlehnJesse ThalerJohann Brehmer",
        "links": "http://arxiv.org/abs/2405.14806v1",
        "entry_id": "http://arxiv.org/abs/2405.14806v1",
        "pdf_url": "http://arxiv.org/pdf/2405.14806v1",
        "summary": "Extracting scientific understanding from particle-physics experiments\nrequires solving diverse learning problems with high precision and good data\nefficiency. We propose the Lorentz Geometric Algebra Transformer (L-GATr), a\nnew multi-purpose architecture for high-energy physics. L-GATr represents\nhigh-energy data in a geometric algebra over four-dimensional space-time and is\nequivariant under Lorentz transformations, the symmetry group of relativistic\nkinematics. At the same time, the architecture is a Transformer, which makes it\nversatile and scalable to large systems. L-GATr is first demonstrated on\nregression and classification tasks from particle physics. We then construct\nthe first Lorentz-equivariant generative model: a continuous normalizing flow\nbased on an L-GATr network, trained with Riemannian flow matching. Across our\nexperiments, L-GATr is on par with or outperforms strong domain-specific\nbaselines.",
        "updated": "2024-05-23 17:15:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.14806v1"
    }
]