Not All Language Model Features Are Linear
JoshuaEngels∗ IsaacLiao* EricJ.Michaud
MIT MIT MIT&IAIFI
jengels@mit.edu iliao@mit.edu ericjm@mit.edu
WesGurnee MaxTegmark
MIT MIT&IAIFI
wesg@mit.edu tegmark@mit.edu
Abstract
Recent work has proposed the linear representation hypothesis: that language
modelsperformcomputationbymanipulatingone-dimensionalrepresentationsof
concepts(“features”)inactivationspace. Incontrast,weexplorewhethersome
languagemodelrepresentationsmaybeinherentlymulti-dimensional. Webegin
bydevelopingarigorousdefinitionofirreduciblemulti-dimensionalfeaturesbased
onwhethertheycanbedecomposedintoeitherindependentornon-co-occurring
lower-dimensionalfeatures. Motivatedbythesedefinitions,wedesignascalable
methodthatusessparseautoencoderstoautomaticallyfindmulti-dimensionalfea-
turesinGPT-2andMistral7B.Theseauto-discoveredfeaturesincludestrikingly
interpretableexamples,e.g. circularfeaturesrepresentingdaysoftheweekand
monthsoftheyear. Weidentifytaskswheretheseexactcirclesareusedtosolve
computational problems involving modular arithmetic in days of the week and
monthsoftheyear. Finally,weprovideevidencethatthesecircularfeaturesare
indeedthefundamentalunitofcomputationinthesetaskswithinterventionexperi-
mentsonMistral7BandLlama38B,andwefindfurthercircularrepresentations
bybreakingdownthehiddenstatesforthesetasksintointerpretablecomponents.
1 Introduction
Languagemodelstrainedfornext-tokenpredictiononlargetextcorporahavedemonstratedremark-
ablecapabilities,includingcoding,reasoning,andin-contextlearning[1,3,7,45]. However,the
specificalgorithmsmodelslearntoachievethesecapabilitiesremainlargelyamysterytoresearchers;
wedonotunderstandhowlanguagemodelswritepoetry. Mechanisticinterpretabilityisafieldthat
seekstoaddressthisgapbyreverse-engineeringtrainedmodelsfromthegroundupintovariables
(features)andtheprograms(circuits)thatprocessthesevariables[37].
One mechanistic interpretability research direction has focused on understanding toy models in
detail. Thisworkhasfoundmulti-dimensionalrepresentationsofinputssuchaslattices[30]and
circles[27,35],andhassuccessfullyreverse-engineeredthealgorithmsthatmodelsusetomanipulate
theserepresentations. Aseparatedirectionhasidentifiedone-dimensionalrepresentationsofhigh
levelconceptsandquantitiesinlargelanguagemodels[6,15,17,29]. Thesefindingshaveledto
thelinearrepresentationhypothesis: thatallrepresentationsinpretrainedlargelanguagemodelsare
one-dimensionallines,andthatwecanunderstandmodelbehaviorasnonlinearmanipulationsof
theselinearrepresentations[6,38]. Inthiswork,webridgethegapbetweenthesetworegimesby
providingevidencethatlanguagemodelsalsousemulti-dimensionalrepresentations.
∗Equalcontribution.
Preprint.Underreview.
4202
yaM
32
]GL.sc[
1v06841.5042:viXraDays of the Week Months of the Year Years of the 20th Century
Monday Friday January October
T Wu ee ds nd ea sy day S Sa ut nu dr ad yay F Me ab rr cu hary N Do ev cee mm bb ee rr 1900 1950 1999
Thursday Other April Other
May
June
July August
September
PCA axis 2 PCA axis 2 PCA axis 3
Figure1: Circularrepresentationsofdaysoftheweek,monthsoftheyear,andyearsofthe20th
century in layer 7 of GPT-2-small. These representations were discovered via clustering SAE
dictionaryelements,describedinSection4. Pointsarecoloredaccordingtothetokenwhichcreated
therepresentation. SeeFig.12forotheraxesandFig.13forsimilarplotsforMistral7B.
1.1 Contributions
1. InSection3,wegeneralizetheone-dimensionaldefinitionofalanguagemodelfeaturetomulti-
dimensional features, provide an updated superposition hypothesis to account for these new
features, and analyze the reduction in a model’s representation space implied by using multi-
dimensionalfeatures. Wealsodevelopatheoreticallygroundedandempiricallypracticaltestfor
irreduciblefeaturesandrunthistestonsomesampledistributions.
2. InSection4,wepresentamethodforfindingmulti-dimensionalfeaturesusingsparseautoencoders
andidentifymulti-dimensionalrepresentationsautomaticallyinGPT-2andMistral7B,including
circular representations for the day of the week and month of the year. To the best of our
knowledge,wearethefirsttofindanemergentcircularrepresentationinalargelanguagemodel.
3. InSection5,weproposetwotasks,modularadditionindaysoftheweekandinmonthsofthe
year,thatwehypothesizewillcausemodelstousethesecircularrepresentations. Weperform
interventionexperimentsonMistral7BandLlama38Btoshowthatmodelsdoindeedusecircular
representationsforthesetasks. Finally,wepresentnovelmethodsfordecomposingLLMhidden
statesandrevealcirclesinthecomputeddayoftheweekandmonthoftheyear.
2 RelatedWork
LinearRepresentations: EarlywordembeddingmethodssuchasGloVeandWord2vec,although
onlytrainedusingco-occurrencedata,containeddirectionsintheirvectorspacescorrespondingto
semanticconcepts,e.g. thewell-knownf(king)-f(man)+f(woman)=f(queen)[31,32,40]. Recent
researchhasfoundsimilarevidenceoflinearrepresentationsinsequencemodelstrainedonlyonnext
tokenprediction,includingOthelloboardpositions[26,36],thetruthvalueofassertions[29],and
numericquantitiessuchaslongitude,latitude,birthyear,anddeathyear[15,17]. Theseresultshave
inspiredthelinearrepresentationhypothesis[11,38]definedabove. Recenttheoreticalworkprovides
evidence for this hypothesis, assuming a latent (binary) variable-based model of language [22].
Empirically,dictionarylearninghasshownsuccessinbreakingdownamodel’sfeaturespaceintoa
sparseover-completebasisoflinearfeaturesusingsparseautoencoders[6,9]. Theseworksassume
thatthenumberoflinearfeaturesstoredinsuperpositionexceedsthemodeldimensionality[11].
Nonlinear Representations: There has beencomparatively little research onnonlinear features.
Onerecentpaper[24]provesthataone-layerswappedorder(MLPbeforeattention)transformer
canin-context-learnanonlinearmappingfunctionfollowedbyalinearregression, implyingthat
the“features”betweentheMLPandattentionblocksarenonlinear. Anotherwork[42]findsthata
transformertrainedonahiddenMarkovmodelusesafractalstructuretorepresenttheprobability
ofeachnexttoken. Theseworksanalyzetoymodelsanditisnotcleariflargelanguagemodels
will have similar nonlinear features. A separate idea [4] argues for interpreting neural networks
throughthepolytopestheysplittheinputspaceinto,andidentifiesregionsoflowpolytopedensityas
2
3
sixa
ACP
3
sixa
ACP
4
sixa
ACP“valid”regionsforapotentiallinearrepresentation. Finally,recentworkondictionarylearning[6]
hasspeculatedaboutmulti-dimensionalfeaturemanifolds;ourworkismostsimilartothisdirection
anddevelopstheideaoffeaturemanifoldstheoreticallyandempirically.
Circuits: Circuitsresearchseekstoidentifyandunderstandcircuits,subsetsofamodel(usually
representedasadirectedacyclicgraph)thatexplainspecificbehaviors[37]. Thebaseunitsthatform
a circuit can be layers, neurons [37], or sparse autoencoder features [28]. The first circuits-style
worklookedattheInceptionV1imagemodelandfoundlinefeaturesthatwerecombinedintocurve
detectionfeatures[37]. Morerecentworkhasexaminedlanguagemodels,forexampletheindirect
objectidentificationcircuitinGPT-2[47]. Giventhedifficultyofdesigningbespokeexperiments,
therehasbeenincreasedresearchinautomatedcircuitdiscoverymethods[8,28,44].
InterpretabilityforArithmeticProblems: Priorworkstudiesmodelstrainedonmodulararithmetic
problemsa+b=c(modm)andfindsthatmodelsthatgeneralizewellhavecircularrepresentations
foraandb[27]. Furtherworkshowsthatmodelsusethesecircularrepresentationstocomputecviaa
“clock”algorithm[35]andaseparate“pizza”algorithm[49]. Thesepapersarelimitedtothecaseof
asmallmodeltrainedonlyonmodulararithmetic. Anotherdirectionhasstudiedhowlargelanguage
modelsperformbasicarithmetic,includingacircuitsleveldescriptionofthegreater-thanoperationin
GPT-2[16]andadditioninGPT-J[43]. Theseworksfindthattoperformacomputation,modelscopy
pertinentinformationtothetokenbeforethecomputedresultandperformthecomputationinthe
subsequentMLPlayers. Finally,recentwork[14]investigateslanguagemodels’abilitytoincrement
numbersandfindslinearfeaturesthatfireontokensequivalentmodulo10.
3 DefinitionsandTheory
Inthissection,wefocusonLlayertransformermodelsM thattakeintokeninputt=(t ,...,t ),
1 n
have hidden states x ,...,x for layers l, and output logit vectors y ,...,y . Given a set of
1,l n,l 1 n
inputsT,weletX bethesetofallcorrespondingx . Thissectionfocusesonhypothesesthat
i,l i,l
describehowtodecomposehiddenstatesx intosumsoffunctionsoftheinput(features). While
i,l
thisisalwayspossibleifM isdeterministicviathe“trivial”evaluationofM itself,weareinterested
indecomposable,interpretablehypothesesfortheconstructionofx . Wewritematricesincapital
i,l
bold,vectorsandvectorvaluedfunctionsinlowercasebold,andsetsincapitalnon-bold.
3.1 Multi-DimensionalFeatures
Definition1(Feature). Wedefinead -dimensionalfeatureofsparsitysasafunctionf thatmapsa
f
subsetoftheinputspaceofprobability1 s>0intoad f-dimensionalpointcloudinRdf. Wesay
−
thatafeatureisactiveontheaforementionedsubset.
Asanexample,letn = 1(soinputsaresingletokens)andconsiderafeaturef thatmapsinteger
tokenstoequispacedpointsinR1. Thenf isa1-dimensionalfeaturethatisactiveonintegertokens,
andifintegertokensoccur1%ofthetimeacrosstheinputdistribution,f hassparsitys=0.99.
Forfeaturestobemeaningful,theyshouldbeirreducible. Here,wefocusonstatisticalreducibility: f
isreducibleifitiscomposedoftwostatisticallyindependentco-occurringfeatures(inwhichcasef
is“separable”)orifitiscomposedoftwonon-co-occurringfeatures(inwhichcasef isa“mixture”).
Wecomparethiswithanotherwaytodefinemulti-dimensionalirreducibilityinAppendixC.
Theprobabilitydistributionoverinputtokenstinducesad -dimensionalprobabilitydistribution
f
overfeaturevectorsf(t)—Fig.2showstwoexamples. Notethatf(t)isarandomvectorsincetisa
randomvariable;weusep(f)todenotetheprobabilitydensityfunctionoff(t).
Definition2. Afeaturef isreducibleintofeaturesaandbifthereexistsanaffinetransformation
(cid:16)a(cid:17)
f Rf +c (1)
(cid:55)→ ≡ b
forsomeorthonormald d matrixRandadditiveconstantc,suchthatthetransformedfeature
f f
×
probabilitydistributionp(a,b)satisfiesatleastoneoftheseconditions:
1. pisseparable,i.e.,factorizableasaproductofitsmarginaldistributions:
p(a,b)=p(a)p(b).
31000
1.0
500 M (cid:15)(f)=0.3621
0.5 S(f)=0.7907
0 0.0
2.5 0.0 2.5 0 π π 3π 2π
representationdim1 − normalizedv f+c 2 angleθ 2
·
(a)TestingS(a)andM (a)onareduciblefeaturea.
ϵ
S(f)=2.683
200 2
M(f)=0.1784 (cid:15)
100 1
0 0
1.5 1.0 0.5 0.0 0 π π 3π 2π
representationdim1 − norm−alizedv−f+c 2
angleθ
2
·
(b)TestingS(b)andM (b)onanirreduciblefeatureb
ϵ
Figure2: Testingirreducibilityofsyntheticfeatures. Leftineachsubfigure: Distributionsofx. For
featurea,36.21%lieswithinthenarrowdottedlines,indicatingthefeatureislikelyamixture. For
featureb,17.84%lieswithinthewidelines,indicatingthefeatureisunlikelytobeamixture. The
greencrossindicatestheangleθ thatminimizesmutualinformation. Middleineachsubfigure:
Histogramsofthedistributionofv xwithredlinesindicatinga2ϵ-wideregion. Rightineach
·
subfigure: MutualinformationbetweenaandbasafunctionoftherotationangleθofmatrixR.
Bothfeatureshavealarge( 0.5bits)minimummutualinformationandsoarelikelynotseparable.
≥
2. pisamixturep(a,b)=wp (a,b)+(1 w)p (a,b)ofdisjointprobabilitydistributions
1 2
−
forw >0,andp islower-dimensionalsuchthatp (a,b)=p (a)δ(b).
1 1 1
HereδistheDiracdeltafunction. Bytwoprobabilitydistributionsbeingdisjoint,wemeanthatthey
havedisjointsupport(thereisnosetwherebothhavepositiveprobabilitymeasure,orequivalently
thetwofeaturesaandbcannotbeactiveatthesametime). InEq.(1),aisthefirstkcomponentsof
thevectorRf +candbistheremainingd kcomponents. Whenpisseparableoramixture,we
f
−
alsosaythatf isseparableoramixture. Wetermafeatureirreducibleifitisnotreducible,i.e.,ifno
rotationandtranslationmakesitseparableoramixture.
Fig. 2a shows an example of a 2D feature that is a mixture, because it can be decomposed into
featuresaandbwherebisa1Dlinedistribution(markedinred)andaistheremainder(a2Dcloud
andaline,whichcaninturnbedecomposed). Anexampleofafeaturethatisseparableisanormal
distribution (since any multidimensional Gaussian can be rotated to have a diagonal covariance
matrix). Innaturallanguage, amixturemightbeaonehotencodingof“languageofthecurrent
token”,whileaseparabledistributionmightbethe“latitude”and“longitude”oflocationtokens.
Inpractice,becauseofnoiseandfinitesamplesize,themixtureandseparabilitydefinitionsmaynot
bepreciselysatisfied. Thus,wesoftenourdefinitionstopermitdegreesofreducibility:
Definition3(SeparabilityIndexandϵ-MixtureIndex). Considerafeaturef. Theseparabilityindex
S(f)measurestheminimalmutualinformationbetweenallpossibleaandbdefinedinEq.(1):
S(f) minI(a;b) (2)
≡
whereI denotesthemutualinformation. SmallervaluesofS(f)meanthatf ismoreseparableand
thereforemorereducible. Notethatwecansolelyminimizeoverhowmanycomponentstosplitoffas
aandoverorthonormalmatricesR,sincetheadditiveoffsetcdoesnotaffectthemutualinformation.
Theϵ-mixtureindexM (f)testshowoftenf canbeprojectednearzerowhileitisactive:
ϵ
(cid:16) (cid:112) (cid:17)
M (f)= max P v f +c <ϵ E[(v f +c)2] (3)
ϵ
v∈Rdf,c∈R | · | ·
4
2midnoitatneserper
2midnoitatneserper
tnuoc
tnuoc
)stib(ofnilautuM
)stib(ofnilautuMLargervaluesofM (f)meanthatf ismoreofamixtureandisthereforemorereducible.
ϵ
WedevelopanoptimizationproceduretosolvefortheminandmaxinDefinition3andapplyitto
syntheticfeaturedistributionsinFig.2a. Furtherdetailsonthisoptimizationprocess,moresynthetic
featuredistributions,andmoreintuitionaboutthesedefinitionscanbefoundinAppendixB.
3.2 Superposition
Wenowexaminetheimplicationsofmulti-dimensionalfeaturesforthesuperpositionhypothesis[11].
Wefirstrestatetheoriginalsuperpositionhypothesisusingourabovedefinitions:
Definition4(δ-orthogonalmatrices). TwomatricesA
1
Rd×d1 andA
2
Rd×d2 areδ-orthogonal
∈ ∈
if x x δforallunitvectorsx colspace(A )andx colspace(A ).
1 2 1 1 2 2
| · |≤ ∈ ∈
Hypothesis1(One-DimensionalSuperpositionHypothesis,paraphrasedfrom[11]). Hiddenstates
x arethesumofmany( d)sparseone-dimensionalfeaturesf andpairwiseδ-orthogonalvectors
i,l i
(cid:80)≫
v suchthatx (t)= v f (t). Wesetf (t)tozerowhentisoutsidethedomainoff .
i i,l i i i i i
Wenowpresentournewsuperpositionhypothesis,whichpositsindependencebetweenirreducible
multi-dimensionalfeaturesinsteadofunknownlevelsofindependencebetweenone-dimensional
features:
Hypothesis2(OurSuperpositionHypothesis,changesunderlined). Hiddenstatesx arethesum
i,l
ofmany( d)sparselow-dimensionalirreduciblefeaturesf andpairwiseδ-orthogonalmatrices
i
V
i
∈Rd×≫ dfi suchthatx i,l(t)=(cid:80) iV if i(t). Wesetf i(t)tozerowhentisoutsidethedomainoff i.
Anysubsetoffeaturesmustbemutuallyindependentontheirshareddomain.
The Johnson-Lindenstrauss (JL) Lemma [23] implies that we can choose eCdδ2 pairwise one-
dimensionalδ-orthogonalvectorstosatisfyHypothesis1forsomeconstantC,thusallowingusto
buildthemodel’sfeaturespacewithanumberofone-dimensionalδ-orthogonalfeaturesexponential
ind. InAppendixA,weproveasimilarresultforlow-dimensionalprojections(themainideaofthe
proofistocombineδ-orthogonalvectorsasguaranteedfromtheJLlemma):
Theorem1. Foranyd,d max,andδ,itispossibletochoose dm1 axeC(d/d2 max)δ2 pairwiseδ-orthogonal
matricesA
i
Rni×dwhere1 n
i
d maxforsomeconstantC.
∈ ≤ ≤
Thisexponentialreductioninthenumberoffeaturesthatarerepresentableδ-orthogonally(asopposed
toamultiplicativefactorreduction,whichisalsopresent)suggeststhatmodelswillemployhigher-
dimensionalfeaturesonlyforrepresentationsthatnecessitatedetailedmulti-dimensionaldescriptions.
Moreover,theserepresentationswillbehighlycompressedtofitwithinthesmallestdimensional
spacepossible,potentiallyleadingtointerestingencodings;forexample,recentwork[33]findsthat
maximum-marginsolutionsforproblemslikemodulararithmeticconsistofFourierfeatures. Note
thattheproofassumesthe“worstcase”scenariothatallofthefeaturesaredimensiond ,whilein
max
practicemanyofthefeaturesmaybe1orlowdimensional,sotheeffectonthecapacityofareal
modelthatrepresentsmulti-dimensionalfeaturesisunlikelytobethisextreme.
4 SparseAutoencodersFindMulti-DimensionalFeatures
Sparseautoencoders(SAEs)deconstructmodelhiddenstatesintosparsevectorsumsfromanover-
complete basis [6, 9]. Forhidden states X , aone-layer SAE of size m with sparsity penalty λ
i,l
minimizesthefollowingdictionarylearningloss[6,9]:
DL(X )= argmin (cid:88) (cid:104) x D ReLU(E x ) 2+λ ReLU(E x ) (cid:105) (4)
i,l ∥ i,l − · · i,l ∥2 ∥ · i,l ∥0
E∈Rm ×d,D∈Rd ×mxi,l∈Xi,l
Inpractice,theL lossonthelasttermisrelaxedtoL for0<p 1tomakethelossdifferentiable.
0 p
WecallthemcolumnsofD(vectorsinRd) dictionaryelement≤ s.
WeproposethatSAEscandiscoverirreduciblemulti-dimensionalfeaturesbylocatingpointsetswith
alowϵ-mixtureindex. Forinstance,ifX containsanirreducibletwo-dimensionalfeaturef (see
i,l
5Table1:Aggregatemodelaccuracyondays
Tue Jan Feb
oftheweekandmonthsoftheyearmodular Mon Mar
Wed Dec
arithmetictasks. Performancebrokendown
Apr
byprobleminstanceinAppendixH. Sun Nov May
Oct
Sat Thu Jun
Model Weekdays Months Fri Sep Jul
Aug
Llama38B 29/49 143/144
Mistral7B 31/49 125/144
Figure3: ToptwoPCAcomponentsontheαtoken.
GPT-2 8/49 10/144 Colorsshowα. Left: Layer30ofMistralonWeek-
days. Right: Layer5ofLlamaonMonths.
Hypothesis2),andDincludesjusttwodictionaryelementsspanningf,bothelementsmusthave
nonzeroactivationspost-ReLUtoperfectlyreconstructf (otherwisef isamixture). ThustheJaccard
similarityofthesetsoftokensthatthesetwodictionaryelementsfireonislikelytobehigh. Onthe
otherhand,ifDcontainsmorethantwodictionaryelementsthatspanf,thentheJaccardsimilarity
maybelower. However,sincetherearenowmanydictionaryelementswithahighprojectioninthe
twodimensionalfeaturespace,thecosinesimilarityofthedictionaryelementsislikelytobehigh.
Thusforatwodimensionalirreduciblefeaturef,weexpecttheretobegroupsofdictionaryelements
witheitherhighcosineorJaccardsimilaritycorrespondingtof. Weexpectthisobservationtobe
trueforhigherdimensionalirreduciblefeaturesaswell. Notethatsomeclustersmaycorrespond
toseparablefeaturesf,asthistechniqueonlyfindsfeaturesthatarenotmixtures. Thissuggestsa
naturalapproachtousingsparseautoencoderstosearchforirreduciblemulti-dimensionalfeatures:
1. ClusterdictionaryelementsbytheirpairwisecosinesimilarityorJaccardsimilarity.
2. Foreachcluster, runtheSAEsonallx X andablatealldictionaryelementsnotinthe
i,l i,l
∈
cluster. Thiswillgivethereconstructionofeachx restrictedtotheclusterfoundinstep1(ifno
i,l
clusterdictionaryelementsarenon-zeroforagivenpoint,weignorethepoint).
3. Examinetheresultingreconstructedactivationvectorsforirreduciblemulti-dimensionalfeatures,
especially ensuring that the reconstruction is not separable. This step can be done manually by
visually inspecting the PCA projections for known irreducible multi-dimensional structures (e.g.
circles,seeFig.2)orautomaticallybypassingthePCAprojectionstothetestsforDefinition3.
Thismethodsucceedsontoydatasetsofsyntheticirreduciblemulti-dimensionalfeatures;seeAp-
pendixD.2WeapplythismethodtolanguagemodelsusingGPT-2[41]SAEstrainedbyBloomfor
everylayer[5]andMistral7B[21]SAEsthatwetrainonlayers8,16,and24(trainingdetailsin
AppendixE).ClusteringdetailsareinAppendixF,includingcommentsonscalability.
Strikingly,wereconstructirreduciblemulti-dimensionalfeaturesthatareinterpretablecircles: in
GPT-2,days,months,andyearsarearrangedcircularlyinorder(seeFig.1);inMistral7B,daysand
monthsarearrangedcircularlyinorder(seeFig.13).
5 CircularRepresentationsinLargeLanguageModels
Inthissection,weseektasksinwhichmodelsusethemulti-dimensionalfeatureswediscoveredin
Section4,therebyprovidingevidencethattheserepresentationsareindeedthefundamentalunitof
computationforsomeproblems. Inspiredbypriorworkstudyingcircularrepresentationsinmodular
arithmetic[27],wedefinetwopromptsthatrepresent“natural”modulararithmetictasks:
Weekdaystask: “Let’sdosomedayoftheweekmath. TwodaysfromMondayis”
Monthstask: “Let’sdosomecalendarmath. FourmonthsfromJanuaryis”
For Weekdays, we rangeoverthe 7 days ofthe weekand durationsbetween 1 and 7 days toget
49prompts. ForMonths,werangeoverthe12monthsoftheyearanddurationsbetween1and12
monthstoget144prompts. Mistral7BandLlama38B[2]achievereasonableperformanceonthe
2Codeforreproducingexperiments:https://github.com/JoshEngels/MultiDimensionalFeatures
6Learning Circular Probe Mistral Weekdays Mistral Months
Activation 01 1 2 3 2.5 5
Space 56 432 0 6 5 4 0.0 0
2.5 5
Intervention Process
Llama Weekdays Llama Months
CP ircC uA
la
m
r
pa rtr oix
be
56 5600 441 31 322 0011
66
22 5533
44
2 5
Intervention 0 0
P is nu tePe rod
o
vio
f en
i ntn etv
o
e or nse
56 5600 441 31 322 0011 66 22 5533 44 2 0 10 20 5 0 10 20
Dataset point Layer
No-op Patch circle Average ablate
Patch layer Patch PCA
Figure4: Visualrepresentationof
theinterventionprocess. Figure5: Meanand96%errorbarsforinterventionmethods.
WeekdaystaskandexcellentperformanceontheMonthstask(measuredbycomparingthehighest
logitvalidtokenagainstthegroundtruthanswer),assummarizedinTable1. Interestingly,although
theseproblemsareequivalenttomodulararithmeticproblemsα+β ?(modm)form=7,12,
≡
bothmodelsgettrivialaccuracyonplainmodularadditionprompts,e.g. “5+3(mod7) ”. Finally,
≡
althoughGPT-2hascircularrepresentations,itgetstrivialaccuracyonWeekdaysandMonths.
Tosimplifydiscussion,letαbethedayoftheweekormonthoftheyeartoken(e.g. “Monday”or
“April”),β bethedurationtoken(e.g. “four”or“eleven”),andγ bethetargetgroundtruthtokenthe
modelshouldpredict,suchthat(abusingnotation)wehaveα+β =γ. Letthepromptsofthetask
beparameterizedbyj,suchthatthejthpromptasksaboutα ,β ,andγ .
j j j
5.1 InterveningonCircularDayandMonthRepresentations
WefirstconfirmthatLlama38BandMistral7Bhavecircularrepresentationsofαbyexamining
thePCAofhiddenstatesacrosspromptsatvariouslayersontheαtoken. Weplottwoofthesein
Fig.3andshowalllayersinFig.14intheappendix. Theseplotsshowcircularrepresentationsasthe
highestvaryingtwocomponentsinthemodel’srepresentationofαatmanylayers.
Wenowexperimentwithinterveningonthesecircularrepresentations. Webaseourexperiments
onthecommoninterpretabilitytechniqueofactivationpatching,whichreplacesactivationsfrom
a“dirty”runofthemodelwiththecorrespondingactivationsfroma“clean”run[48]. Activation
patchingempiricallytestswhetheraspecificmodelcomponent,position,and/orrepresentationhas
acausalinfluenceonthemodel’soutput. Weemployacustomsubspacepatchingmethodtoallow
testingforwhetheraspecificcircularsubspaceofahiddenstateissufficienttocausallyexplain
modeloutput. Specifically,ourpatchingtechniquereliesonthefollowingsteps(visualizedinFig.4):
1. Find a subspace with a circle to intervene on: Using a PCA reduced activation subspace
to avoid overfitting, we train a “circular probe” to identify representations which exhibit strong
circularpatterns. Moreformally,letxj bethehiddenstateatlayerltokenpositioniforprompt
i,l
j. Let W Rk×d be the matrix consisting of the top k principal component directions of
i,l
∈
xj . In our experiments, we set k = 5. We learn a linear probe P R2,k from W X to
i,l ∈ i,l · i,l
a unit circle in α. In other words, if circle(α) = [cos(2πα/7),sin(2πα/7)] for Weekdays and
circle(α)=[cos(2πα/12),sin(2πα/12)]forMonths,Pisdefinedasfollows:
(cid:88)(cid:13) (cid:13)2
P=argmin (cid:13)P′ W xj circle(α)(cid:13) (5)
P∈R2,k (cid:13) · i,l · i,l− (cid:13) 2
′ xj
i,l
2. Intervene on the subspace: Say our initial prompt had α = α and we are intervening with
j
α = α . Inthisstep,wereplacethemodel’sprojectiononthesubspaceP W ,whichwillbe
j i,l
′ ·
closetocircle(α ),withthe“clean”pointcircle(α ). Notethatwedonotusethehiddenstate
j j
′
xj′ fromthe“clean”run,onlythe“clean”labelα . Inpractice,othersubspacesofxj maybeused
i,l j ′ i,l
7
ffid
tigol
egarevAbythemodelinalternatepathwaystocomputetheanswer. Toavoidthisaffectingtheintervention,
weaverageoutallsubspacesnotintheintervenedsubspace. Lettingx betheaverageofxj across
i,l i,l
allpromptsindexedbyj andP+bethepseudoinverseofP,weinterveneviatheformula
xj∗ =x +W TP+(circle(α ) x ) (6)
i,l i,l i,l j ′ − i,l
We run our patching on all 49 Weekday problems and 144 Month problems and use as “clean”
runs the 6 or 11 other possible values for β, resulting in a total of 49 6 patching experiments
∗
for Weekdays and 144 11 patching experiments for Months. We also run baselines where we
∗
(1)replacetheentiresubspacecorrespondingtothefirst
5PCAdimensionswiththecorrespondingsubspacefrom
Task Duration = 2 Days Task Duration = 3 Days
the clean run, (2) replace the entire layer with the cor- 2 2
respondinglayerfromthecleanrun,and(3)replacethe 1 1
entirelayerwiththeaverageacrossthetask. Themetric
0 0
weuseisaveragelogitdifferenceacrossallpatchingex-
1 1
perimentsbetweentheoriginalcorrecttoken(α )andthe
j 2 2
targettoken(α ). SeeFig.5fortheseinterventionsonall
j 2 0 2 2 0 2
′
layersofMistral7BandLlama38BonWeekdaysand
Task Duration = 4 Days Task Duration = 5 Days
Months. 2 2
1 1
ThemaintakeawayfromFig.5isthatcircularsubspaces
are causally implicated in computing γ, especially for 0 0
Weekdays. Acrossallmodelsandtasks,earlylayerinter- 1 1
ventions on the circular subspace have almost the same 2 2
intervention effect as patching the entire layer, and are 2 0 2 2 0 2
sometimesevenbetterthanpatchingthetopPCAdimen- Monday Wednesday Friday Sunday
Tuesday Thursday Saturday
sionsfromthecleanproblem. Notethatpatchingexperi-
mentsinAppendixIshowαiscopiedtothefinaltokenon Figure6: Offdistributioninterventions
layers15to17,whichiswhyinterventionsdropoffthere. onMistrallayer5ontheWeekdaystask.
The color corresponds to the highest
To investigate exactly how models use the circular sub- logitγ afterperformingourcircularsub-
space,weperformoffdistributioninterventions. Wemod- spaceinterventionprocessonthatpoint.
ifyEq.(6)sothatinsteadofinterveningonthecircumfer-
encecircle(α),wesweepoveragridofpositions(r,θ)withinthecircle:
xj∗ =x +W TP+[rcos(θ),rsin(θ)]T x ) (7)
i,l i,l i,l − i,l
Weintervenewithr [0,0.1,...,2],θ [0,2π/100,...,198π/100]andrecordthehighestlogitγ
∈ ∈
aftertheforwardpass. Fig.6displaystheseresultsonMistrallayer5forβ [2,3,45]. Theyimply
∈
thatMistraltreatsthecircleasamulti-dimensionalrepresentationwithαencodedintheangle.
5.2 DecomposingHiddenStates
ToisolatetheroughcircuitforWeekdaysandMonths,weperform
3
layer-wiseactivationpatchingon40randompairsofprompts. The Tue Wed
2
results,displayedinAppendixI,showthatthecircuittocomputeγ
consistsofMLPsontopoftheαandβ tokens,acopytothetoken Mon 1 Thu
before γ, and further MLPs there (roughly similar to prior work 0
studyingarithmeticcircuits[43]). Moreover,fine-grainedpatching 4 2 1 0 2Fri 4
inAppendixIshowsthattherearejustafewresponsibleattention Sun 2 Sat
headsforthewritestothetokenbeforeγ. However,patchingalone 3
cannottellusehoworwhereγ isrepresented. Figure7: ToptwoPCAcom-
ponentsofresidualerrorsafter
Wenowintroduceanewtechniqueforempiricallyexplaininghidden
EVRwithone-hotinαandβ.
representationsinalgorithmicproblems:explanationviaregression
Mistral 7B Weekdays, layer
(EVR).GivenasetoftokensT withacorrespondingsetofhidden
25,finaltoken. Coloredbyγ.
statesX ,weexplainthevarianceinX byaddingtogetherhand-
i,l i,l
chosenfunctionsoft. ThisgivesusanexplanationofwhatthetransformationT X computes.
i,l
→
Foragivenchoiceofexplanationfunctions g (t) ,ther2valueofalinearregressionfrom g (t)
i i
{ } { }
toX givesameasureoftheexplainedvarianceinX . Butwhatfunctionsg shouldwechoose?
i,l i,l i
8layer17 layer18 layer19 layer20 layer21 layer22 layer23 layer24 layer25 layer26 layer27 layer28 layer29
original original original original original original original original original original original original original
r2:0.0% r2:0.0% r2:0.0% r2:0.0% r2:0.0% r2:0.0% r2:0.0% r2:0.0% r2:0.0% r2:0.0% r2:0.0% r2:0.0% r2:0.0%
onehotα,β onehotα,β onehotα,β onehotα,β onehotα,β onehotα,β onehotα,β onehotα,β onehotα,β onehotα,β onehotα,β onehotα,β onehotα,β
r2:94.0% r2:95.6% r2:96.3% r2:94.5% r2:91.4% r2:89.0% r2:87.3% r2:85.4% r2:81.9% r2:78.4% r2:74.8% r2:72.3% r2:70.4%
αfortmr αfortmr αfortmr circleγ circleγ circleγ circleγ circleγ circleγ circleγ circleγ circleγ
r2:98.1% r2:98.2% r2:96.8% r2:94.4% r2:93.6% r2:93.4% r2:92.2% r2:91.2% r2:88.7% r2:87.7% r2:86.4% r2:84.6%
c ri 2rc :le 9α 8.−6%β r2c :irc 9l 8e .7γ
%
r2c :irc 9l 8e .1γ
%
c ri 2rc :le 9α 6.−3%β c ri 2rc :le 9α 5.−6%β c ri 2rc :le 9α 5.−5%β c ri 2rc :le 9α 4.−4%β c ri 2rc :le 9α 3.−7%β c ri 2rc :le 9α 1.−0%β c ri 2rc :le 8α 9.−9%β
c ri 2rc :le 9α 9.−2%β c ri 2rc :le 9α 9.−0%β rα 2:fo 9r 8.t 0m %r rα 2:fo 9r 8.t 1m %r rα 2:fo 9r 7.t 6m %r
Figure8: EVRresidualRGBplotsonMistralhiddenstatesontheWeekdaysfinaltoken,layers17to
29. Fromtoptobottom,weshoweachresidualRGBplotafteraddingthefunction(s)g labelledjust
i
underneath,aswellastheresultingr2 value. Wewrite“tmr”meaning“tomorrow”forβ =1. We
alsowrite“circleforx”meaningtheinclusionoftwofunctionsg (x)= cos,sin (2πx/7).
i
{ }
Webuildalistofg iterativelyandgreedily. Ateachiteration,weperformalinearregressionwiththe
i
currentlistg ...g ,visualizeandinterprettheresidualpredictionerrors,andbuildanewfunction
1 k
g representingtheseerrorstoaddtothelist. Oncemostvarianceisexplained,wecanconclude
k+1
thatg ,...,g constitutestheentiretyofwhatisrepresentedinthehiddenstates. Thisinformation
1 k
tells us what can and cannot be extracted via a linear probe, without having to train any probes.
Furthermore,ifwetreateachg asafeature(seeDefinition1),thenthelinearregressioncoefficients
i
telluswhichdirectionsinX thesefeaturesarerepresentedin,connectingbacktoHypothesis2.
i,l
WeapplyEVRtoMonthsandWeekdays. SinceX consistsofmodularadditionproblemswith
i,l
twoinputsαandβ,wecanvisualizetheerrorsasweiterativelyconstructg ,...,g bymakinga
1 k
heatmapwithαandβ onthetwoaxes,wherethecolorshowswhatkindoferrorismade. More
specifically, we take the top 3 PCA components of the error distribution and assign them to the
colorsred,green,andblue. WecalltheresultingheatmaparesidualRGBplot. Errorsthatdepend
primarilyonα,β,orγ showupashorizontal,vertical,ordiagonalstripesontheresidualRGBplot.
In Fig.8,weperformEVRonthelayer17-29hiddenstatesofMistral7BontheWeekdaystask;
additional deconstructions are in Appendix I. We find that a circle in γ develops and grows in
explanatorypower;weplotthelayer25residualsafterexplainingwithonehotfunctionsinαandβ
(i.e. g =[α=0],g =[β =1],g =[α=1],...)inFig.7toshowthisincrediblyclearcirclein
1 2 3
γ. Thissuggeststhatthemodelsmaybegeneratingγ byusingatrigonometrybasedalgorithmlike
the“clock”[35]or“pizza”[49]algorithminlateMLPlayers.
6 Discussion
Ourworkproposesasignificantrefinementtothesimpleone-dimensionallinearrepresentationhy-
pothesis. Whilepreviousworkhasconvincinglyshowntheexistenceofone-dimensionalfeatures,we
findevidenceformulti-dimensionalrepresentationsthatarenon-separableandirreducible,requiring
us to generalize the notion of a feature to higher dimensions. Fortunately, we find that existing
featureextractionmethodologieslikesparseautoencoderscanreadilybeappliedtodiscovermulti-
dimensionalrepresentations. Althoughmulti-dimensionalrepresentationsmaybemorecomplicated,
webelievethatuncoveringthetrue(perhapsmulti-dimensional)natureofmodelrepresentationsis
necessaryfordiscoveringtheunderlyingalgorithmsthatusetheserepresentations. Ultimately,we
aimtoturncomplexcircuitsinfuturemore-capablemodelsintoformallyverifiableprograms[10,46],
which requires the ground truth “variables” of language models; we believe this work takes an
importantsteptowardsdiscoveringthesevariables. Finally,wedonotanticipateadverseimpactsof
ourwork,asitfocusesonlyondeepeningourunderstandingoflanguagemodelrepresentations.
9Limitations: It is unclear why we did not find more interpretable multi-dimensional features:
are there truly not that many, or is our clustering technique failing to find them? Our definition
for an irreducible feature (Definition 2) also had to be relaxed to hold in practice (Definition 3).
Thus, although this work provides preliminary evidence for the multi-dimensional superposition
hypothesis (Hypothesis 2), it is still unclear if this theory provides the best description for the
representations models use. We also did not find a small subset of MLP neurons implementing
the“clock”algorithm, leavingasanopenquestiontowhatextentmodelsusemulti-dimensional
representationsforalgorithmictasks. Finally, weonlyranexperimentsonmodelsuptosize8B;
however,recentwork[20]impliesrepresentationsmaybecomeuniversalwithgrowingmodelsize.
AcknowledgmentsandDisclosureofFunding
Wethank(inalphabeticalorder)DowonBaek,KaivuHariharan,VedangLad,ZimingLiu,andTony
Wangforhelpfuldiscussionsandsuggestions. ThisworkissupportedbyErikOtto,JaanTallinn,the
RothbergFamilyFundforCognitiveScience,theNSFGraduateResearchFellowship(GrantNo.
2141064),andIAIFIthroughNSFgrantPHY-2019786.
References
[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technicalreport. arXiv
preprintarXiv:2303.08774,2023.
[2] AI@Meta. Llama3modelcard,2024.
[3] Anthropic. Theclaude3modelfamily:Opus,sonnet,haiku. Technicalreport,Anthropic,2024.
[4] Sid Black, Lee Sharkey, Leo Grinsztajn, Eric Winsor, Dan Braun, Jacob Merizian, Kip Parker, Car-
losRamónGuevara, BerenMillidge, GabrielAlfour, etal. Interpretingneuralnetworksthroughthe
polytopelens. arXivpreprintarXiv:2211.12312,2022.
[5] Joseph Bloom. Open source sparse autoencoders for all residual stream layers
of gpt2 small. https://www.alignmentforum.org/posts/f9EgfLSurAiqRJySD/
open-source-sparse-autoencoders-for-all-residual-stream,2024.
[6] TrentonBricken,AdlyTempleton,JoshuaBatson,BrianChen,AdamJermyn,TomConerly,NickTurner,
Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas
Schiefer,TimMaxwell,NicholasJoseph,ZacHatfield-Dodds,AlexTamkin,KarinaNguyen,Brayden
McLean,JosiahEBurke,TristanHume,ShanCarter,TomHenighan,andChristopherOlah. Towards
monosemanticity:Decomposinglanguagemodelswithdictionarylearning. TransformerCircuitsThread,
2023. https://transformer-circuits.pub/2023/monosemantic-features/index.html.
[7] SébastienBubeck, VarunChandrasekaran, RonenEldan, JohannesGehrke, EricHorvitz, EceKamar,
PeterLee,YinTatLee,YuanzhiLi,ScottLundberg,etal. Sparksofartificialgeneralintelligence:Early
experimentswithgpt-4. arXivpreprintarXiv:2303.12712,2023.
[8] ArthurConmy,AugustineMavor-Parker,AengusLynch,StefanHeimersheim,andAdriàGarriga-Alonso.
Towardsautomatedcircuitdiscoveryformechanisticinterpretability. AdvancesinNeuralInformation
ProcessingSystems,36:16318–16352,2023.
[9] HoagyCunningham,AidanEwart,LoganRiggs,RobertHuben,andLeeSharkey. Sparseautoencoders
findhighlyinterpretablefeaturesinlanguagemodels. arXivpreprintarXiv:2309.08600,2023.
[10] DavidDalrymple,JoarSkalse,YoshuaBengio,StuartRussell,MaxTegmark,SanjitSeshia,SteveOmohun-
dro,ChristianSzegedy,BenGoldhaber,NoraAmmann,etal. Towardsguaranteedsafeai:Aframework
forensuringrobustandreliableaisystems. arXivpreprintarXiv:2405.06624,2024.
[11] NelsonElhage,TristanHume,CatherineOlsson,NicholasSchiefer,TomHenighan,ShaunaKravec,Zac
Hatfield-Dodds,RobertLasenby,DawnDrain,CarolChen,RogerGrosse,SamMcCandlish,JaredKaplan,
DarioAmodei,MartinWattenberg,andChristopherOlah. Toymodelsofsuperposition. Transformer
CircuitsThread,2022. https://transformer-circuits.pub/2022/toy_model/index.html.
[12] LeoGao,StellaBiderman,SidBlack,LaurenceGolding,TravisHoppe,CharlesFoster,JasonPhang,
HoraceHe,AnishThite,NoaNabeshima,etal. Thepile:An800gbdatasetofdiversetextforlanguage
modeling. arXivpreprintarXiv:2101.00027,2020.
10[13] SemyonAranovichGershgorin. überdieabgrenzungdereigenwerteeinermatrix. IzvestiyaRossi˘ıskoi
akademiinauk.Seriyamatematicheskaya,(6):749–754,1931.
[14] RhysGould,EuanOng,GeorgeOgden,andArthurConmy. Successorheads: Recurring,interpretable
attentionheadsinthewild. arXivpreprintarXiv:2312.09230,2023.
[15] Wes Gurnee and Max Tegmark. Language models represent space and time. arXiv preprint
arXiv:2310.02207,2023.
[16] MichaelHanna,OllieLiu,andAlexandreVariengien. Howdoesgpt-2computegreater-than?:Interpreting
mathematical abilities in a pre-trained language model. Advances in Neural Information Processing
Systems,36,2024.
[17] BenjaminHeinzerlingandKentaroInui. Monotonicrepresentationofnumericpropertiesinlanguage
models. arXivpreprintarXiv:2403.10381,2024.
[18] Nicholas J. Higham. Singular value inequalities. https://nhigham.com/2021/05/04/
singular-value-inequalities/,May2021.
[19] BillJohnson(https://mathoverflow.net/users/2554/billjohnson).Almostorthogonalvectors.MathOverflow.
URL:https://mathoverflow.net/q/24873(version:2010-05-16).
[20] MinyoungHuh,BrianCheung,TongzhouWang,andPhillipIsola. Theplatonicrepresentationhypothesis.
arXivpreprintarXiv:2405.07987,2024.
[21] AlbertQJiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,DevendraSinghChaplot,Diego
delasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,LucileSaulnier,etal. Mistral7b.
arXivpreprintarXiv:2310.06825,2023.
[22] YiboJiang,GouthamRajendran,PradeepRavikumar,BryonAragam,andVictorVeitch. Ontheoriginsof
linearrepresentationsinlargelanguagemodels. arXivpreprintarXiv:2403.03867,2024.
[23] WilliamB.JohnsonandJoramLindenstrauss. Extensionsoflipschitzmappingsintoahilbertspace. In
Conferenceinmodernanalysisandprobability(NewHaven,Conn.,1982),volume26ofContemporary
Mathematics,pages189–206.AmericanMathematicalSociety,Providence,RI,1984.
[24] JunoKimandTaijiSuzuki. Transformerslearnnonlinearfeaturesincontext: Nonconvexmean-field
dynamicsontheattentionlandscape. arXivpreprintarXiv:2402.01258,2024.
[25] Silvio Lattanzi, Thomas Lavastida, Kefu Lu, and Benjamin Moseley. A framework for parallelizing
hierarchicalclusteringmethods. InMachineLearningandKnowledgeDiscoveryinDatabases:European
Conference,ECMLPKDD2019,Würzburg,Germany,September16–20,2019,Proceedings,PartI,pages
73–89.Springer,2020.
[26] KennethLi,AspenKHopkins,DavidBau,FernandaViégas,HanspeterPfister,andMartinWattenberg.
Emergentworldrepresentations:Exploringasequencemodeltrainedonasynthetictask. arXivpreprint
arXiv:2210.13382,2022.
[27] ZimingLiu,OuailKitouni,NiklasSNolte,EricMichaud,MaxTegmark,andMikeWilliams. Towards
understandinggrokking:Aneffectivetheoryofrepresentationlearning. AdvancesinNeuralInformation
ProcessingSystems,35:34651–34663,2022.
[28] SamuelMarks,CanRager,EricJMichaud,YonatanBelinkov,DavidBau,andAaronMueller. Sparse
featurecircuits:Discoveringandeditinginterpretablecausalgraphsinlanguagemodels. arXivpreprint
arXiv:2403.19647,2024.
[29] SamuelMarksandMaxTegmark. Thegeometryoftruth: Emergentlinearstructureinlargelanguage
modelrepresentationsoftrue/falsedatasets. arXivpreprintarXiv:2310.06824,2023.
[30] EricJMichaud,IsaacLiao,VedangLad,ZimingLiu,AnishMudide,ChloeLoughridge,ZifanCarlGuo,
TaraRezaeiKheirkhah,MatejaVukelic´,andMaxTegmark. Openingtheaiblackbox:programsynthesis
viamechanisticinterpretability. arXivpreprintarXiv:2402.05110,2024.
[31] TomasMikolov,IlyaSutskever,KaiChen,GregSCorrado,andJeffDean. Distributedrepresentations
ofwordsandphrasesandtheircompositionality. Advancesinneuralinformationprocessingsystems,26,
2013.
[32] TomášMikolov,Wen-tauYih,andGeoffreyZweig. Linguisticregularitiesincontinuousspaceword
representations. InProceedingsofthe2013conferenceofthenorthamericanchapteroftheassociation
forcomputationallinguistics:Humanlanguagetechnologies,pages746–751,2013.
11[33] DepenMorwani,BenjaminLEdelman,Costin-AndreiOncescu,RosieZhao,andShamKakade. Feature
emergenceviamarginmaximization:casestudiesinalgebraictasks. arXivpreprintarXiv:2311.07568,
2023.
[34] Neel Nanda and Joseph Bloom. Transformerlens. https://github.com/TransformerLensOrg/
TransformerLens,2022.
[35] NeelNanda,LawrenceChan,TomLieberum,JessSmith,andJacobSteinhardt. Progressmeasuresfor
grokkingviamechanisticinterpretability. arXivpreprintarXiv:2301.05217,2023.
[36] NeelNanda,AndrewLee,andMartinWattenberg. Emergentlinearrepresentationsinworldmodelsof
self-supervisedsequencemodels. arXivpreprintarXiv:2309.00941,2023.
[37] ChrisOlah,NickCammarata,LudwigSchubert,GabrielGoh,MichaelPetrov,andShanCarter. Zoomin:
Anintroductiontocircuits. Distill,2020. https://distill.pub/2020/circuits/zoom-in.
[38] KihoPark,YoJoongChoe,andVictorVeitch. Thelinearrepresentationhypothesisandthegeometryof
largelanguagemodels. arXivpreprintarXiv:2311.03658,2023.
[39] BaolinPeng,ChunyuanLi,PengchengHe,MichelGalley,andJianfengGao. Instructiontuningwithgpt-4.
arXivpreprintarXiv:2304.03277,2023.
[40] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word
representation.InProceedingsofthe2014conferenceonempiricalmethodsinnaturallanguageprocessing
(EMNLP),pages1532–1543,2014.
[41] AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal. Language
modelsareunsupervisedmultitasklearners. OpenAIblog,2019.
[42] Adam Shai, Paul Riechers, Lucas Teixeira, Alexander Oldenziel, and Sarah Marzen. Transformers
representbeliefstategeometryintheirresidualstream. https://www.alignmentforum.org/posts/
gTZ2SxesbHckJ3CkF/transformers-represent-belief-state-geometry-in-their,2024.
[43] AlessandroStolfo,YonatanBelinkov,andMrinmayaSachan. Amechanisticinterpretationofarithmetic
reasoninginlanguagemodelsusingcausalmediationanalysis. InProceedingsofthe2023Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing,pages7035–7052,2023.
[44] AaquibSyed,CanRager,andArthurConmy.Attributionpatchingoutperformsautomatedcircuitdiscovery.
arXivpreprintarXiv:2310.10348,2023.
[45] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,Radu
Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable
multimodalmodels. arXivpreprintarXiv:2312.11805,2023.
[46] MaxTegmarkandSteveOmohundro. Provablysafesystems: theonlypathtocontrollableagi. arXiv
preprintarXiv:2309.01933,2023.
[47] KevinWang,AlexandreVariengien,ArthurConmy,BuckShlegeris,andJacobSteinhardt. Interpretability
inthewild: acircuitforindirectobjectidentificationingpt-2small. arXivpreprintarXiv:2211.00593,
2022.
[48] FredZhangandNeelNanda. Towardsbestpracticesofactivationpatchinginlanguagemodels:Metrics
andmethods. arXivpreprintarXiv:2309.16042,2023.
[49] ZiqianZhong,ZimingLiu,MaxTegmark,andJacobAndreas. Theclockandthepizza:Twostoriesin
mechanisticexplanationofneuralnetworks. AdvancesinNeuralInformationProcessingSystems,36,
2024.
12A Proofs
WewillfirstprovealemmathatwillhelpusproveTheorem1.
Lemma1. Picknpairwiseδ-orthogonalunitvectorsinv ,...,v Rd.Lety Rdbeaunitnorm
1 n
vectorthatisalinearcombinationofunitnormvectorsv ,...,v ∈ withcoeffici∈ entsz ...,z R.
1 n 1 n
WecanwriteA = [v ,...,v ]andz = [z ,...,z ]T,sothatwehavey = (cid:80)n z v = A∈ zT
1 n 1 n k=1 k k
with y =1. Then,
2
∥ ∥
(cid:12) (cid:12)
(cid:12)(cid:88)n (cid:12) (cid:114) n
(cid:12) z (cid:12)= z
(cid:12) (cid:12) k(cid:12) (cid:12) ∥ ∥1 ≤ 1 δn
k=1 −
Proof. WewillfirstboundtheL normofz. Ifσ istheminimumsingularvalueofA,thenwe
2 n
haveviastandardsingularvalueinequalities[18]
y y 1
σ ∥ ∥2 = z ∥ ∥2 =
n ≤ z ⇒ ∥ ∥2 ≤ σ σ
2 n n
∥ ∥
Thuswenowlowerboundσ . Thesingularvaluesarethesquarerootsoftheeigenvaluesofthe
n
matrixATA,sowenowexamineATA. SinceallelementsofAareunitvectors,thediagonalof
ATAisallones. Theoffdiagonalelementsaredotproductsofpairsofδ-orthogonalvectors,andso
arewithintherange[ δ,δ]. ThenbytheGershgorincircletheorem[13],alleigenvaluesλ ofATA
i
−
areintherange
(1 δ(n 1),1+δ(n 1))
− − −
(cid:112)
Inparticular,σ2 = λ 1 δ(n 1),andthusσ 1 δ(n 1). Pluggingintoourupper
n n ≥ − − (cid:112) n ≥ − −
boundfor z ,wehavethat z 1/ 1 δ(n 1). Finally,thelargestL forapointonan
2 2 1
∥ ∥ ∥ ∥ ≤ − −
n-hypersphereofradiusriswhenalldimensionsareequalandsuchapointhasmagnitude√nr,so
(cid:114) n (cid:114) n
z
∥ ∥1 ≤ 1 δ(n 1) ≤ 1 δn
− − −
Theorem1. Foranyd,d max,andδ,itispossibletochoose dm1 axeC(d/d2 max)δ2 pairwiseδ-orthogonal
matricesA
i
Rni×dwhere1 n
i
d maxforsomeconstantC.
∈ ≤ ≤
Proof. BytheJLlemma[19,23],foranydandδ,wecanchooseeCdδ2 δ-orthogonalunitvectors
in Rd indexed as v , for some constant C. Let A = [v ,...,v ] where each
i i dmax∗i dmax∗i+ni−1
elementinthebracketsisacolumn. ThenbyconstructionallA arematricescomposedofunique
i
δ-orthogonalvectorsandthereare 1 eCdδ2 matricesA .
dmax i
Now,considertwoofthesematricesA = [v ,...,v ]andA = [u ,...,u ],i = j;wewill
provethattheyaref(δ)-orthogonalfori somef1 unctionn fi
. Lety
j =(cid:80)ni1
z
vnj bea̸
vectorinthe
i k=1 i,k k
colspaceofA andy =
(cid:80)nj
z u beavectorinthecolspaceofA ,suchthaty andy are
i j k=1 j,k k j i j
unitvectors. Toprovef(δ)-orthogonality,wemustboundtheabsolutedotproductbetweeny and
i
y :
j
13(cid:12) (cid:12)(cid:32) (cid:88)ni (cid:33) (cid:32) (cid:88)nj (cid:33)(cid:12) (cid:12)
y y =(cid:12) z v z u (cid:12)
i j (cid:12) i,k k j,k k (cid:12)
| · | (cid:12) · (cid:12)
k=1 k=1
(cid:12) (cid:12)(cid:88)ni (cid:88)nj (cid:12) (cid:12)
=(cid:12) (z v ) (z u )(cid:12)
(cid:12)
(cid:12)
i,k1 k1
·
j,k2 k2 (cid:12)
(cid:12)
k1=1k2=1
(cid:88)ni (cid:88)nj
z z v u TriangleInequality
≤ |
i,k1 j,k2|| k1
·
k2|
k1=1k2=1
(cid:88)ni (cid:88)nj
z z δ Allv ,u areδorthogonal
≤ |
i,k1 j,k2| i j
k1=1k2=1
(cid:88)ni (cid:88)nj
=δ z z
|
i,k1 j,k2|
k1=1k2=1
(cid:12) (cid:12)(cid:88)ni (cid:12) (cid:12)(cid:12) (cid:12)(cid:88)nj (cid:12) (cid:12)
=δ(cid:12) z (cid:12)(cid:12) z (cid:12) Factoringtheproduct
(cid:12) i,k(cid:12)(cid:12) j,k(cid:12)
(cid:12) (cid:12)(cid:12) (cid:12)
k=1 k=1
(cid:114) n (cid:114) n
δ i j ByLemma1
≤ 1 δn 1 δn
i j
− −
δd
max n ,n d byassumption
≤ 1 δd i j ≤ max
max
−
Thus A and A are f(δ)-orthogonal for f(δ) = δd /(1 δd ), and so it is possible to
i j max max
−
choose 1 eCdδ2 pairwise f(δ)-orthogonal projection matrices. Remapping the variable δ with
dmax
δ
(cid:55)→
f−1(δ) = δ/(d max(1 + δ)), we find that it is possible to choose dm1 axeCdδ2/((1+δ)2d2 max)
pairwiseδ-orthogonalprojectionmatrices. Because1+δisatmost2withδ (0,1),wecanfurther
∈
simplifytheexponentandfindthatitispossibletochoose 1 eC(d/d2 max)δ2/4pairwiseδ-orthogonal
dmax
projectionmatrices. Absorbingthe4intotheconstantC finishestheproof.
B MoreonReducibility
B.1 AdditionalIntuitionforDefinitions
Here, we present some extra intuition and high level ideas for understanding our definitions and
the motivation behind them. Roughly, we intend for our definitions in the main text to identify
representationsinthemodelthatdescribeanobjectorconceptinawaythatfundamentallytakes
multiple dimensions. We operationalize this as finding a subspace of representations that 1. has
basisvectorsthat“alwaysco-occur”nomattertheorientation2. isnotmadeupofcombinationsof
independentlower-dimensionalfeatures.
1. Thefirstconditionismetbythemixturepartofourdefinition. Thefeatureinquestionshould
bepartofanirreduciblemanifold,andsoshould“fill”aplaneorhyperplane. Thereshouldn’tbe
anypartoftheplanewheretheprobabilitydistributionofthefeatureisconcentrated,becausethis
regionisthenlikelypartofalowerdimensionalfeature. Theideaofthispartofthedefinitionis
tocapturemulti-dimensionalobjects; iftheentiremulti-dimensionalspaceistrulybeingusedto
representahigh-dimensionalobject,thentherepresentationsfortheobjectshouldbe“spreadout”
entirelythroughthespace.
2. Thesecondconditionismetbytheseparabilitypartofourdefinition. Thispartofthedefinitionis
intendedtoruleoutfeaturesthatco-occurfrequentlybutarefundamentallynotdescribingthesame
objectorconcept. Forexample,latitudeandlongitudearenotamixtureinthattheyfrequentlyco-
occur,butwedonotthinkitisnecessarilycorrecttosaytheyarepartofthesamemulti-dimensional
featurebecausetheyareindependent.
14B.2 EmpiricalIrreducibleFeatureTestDetails
OurtestsforreducibilityrequirethecomputationoftwoquantitiesS(f)fortheseparabilityindex
andM (f)fortheϵ-mixtureindex. Wedescribehowwecomputeeachindexinthefollowingtwo
ϵ
subsections.
B.2.1 SeparabilityIndex
WedefinetheseparabilityindexinEquation2as
S(f)=minI(a;b)
wheretheminisoverrotationsRusedtosplitf′ = Rf +cintoaandb. Intwodimensions,the
rotationisdefinedbyasingleangle,sowecaniterateoveragridof1000anglesandestimatethe
mutualinformationbetweenaandbforeachangle. Wefirstnormalizef bysubtractingoffthemean
andthendividingbytherootmeansquarednormoff (andmultiplyingby√2sincethetoydatasets
areintwodimensions). Toestimatethemutualinformation,wefirstclipthedataf toa6by6square
centeredontheorigin. Wethenbinthepointsintoa40by40grid,toproduceadiscretedistribution
p(a,b). Aftercomputingthemarginalsp(a)andp(b)bysummingthedistributionovereachaxis,we
obtainthemutualinformationviatheformula
(cid:88) p(a,b)
I(a;b)= p(a,b)log (8)
p(a)p(b)
a,b
B.2.2 ϵ-MixtureIndex
Wedefinetheϵ-mixtureindexinEquation3as
(cid:16) (cid:112) (cid:17)
M (f)= max P v f +c <ϵ E[(v f +c)2]
ϵ
v∈Rdf,c∈R | · | ·
ThechallengewithcomputingM (f)istocomputethemaximum.Weoptedtomaximizeviagradient
ϵ
descent;andweguaranteeddifferentiabilitybysofteningtheinequality<withasigmoid,
(cid:32) (cid:32) (cid:32) (cid:33)(cid:33)(cid:33)
1 v f +c
M ϵ,T(f,v,c)=E σ
T
ϵ
−
(cid:112)E| [(v·
f
+|
c)2]
(9)
·
whereT isatemperature,whichwelinearlydecayfrom1to0throughouttraining. Weoptimize
for v and c using this loss M (f,v,c) using full batch gradient descent over 10000 steps with
ϵ,T
learningrate0.1. Withthesolution(v∗,c∗),thefinalvalueofM (f,v∗,c∗)isthenourestimate
ϵ,T=0
ofM (f).
ϵ
WealsoruntheirreducibilitytestsonadditionalsyntheticfeaturedistributionsinFig.9aandFig.9b.
C AlternativeDefinitions
Inthissection,wepresentanalternativedefinitionofareduciblefeaturethatweconsideredduringour
work. Thischieflydealswithmulti-dimensionalfeaturesfromtheangleofcomputationalreducibility
asopposedtostatisticalreducibility. Inotherwords,thisdefinitionconsiderswhetherrepresentations
offeaturesonaspecificsetoftaskscanbesplitupwithoutchangingtheaccuracyofthetask. This
capturesaninteresting(andimportant)aspectoffeaturereducibility,butbecauseitrequiresaspecific
setofprompts(asopposedtoallowingunsuperviseddiscovery)wechosenottouseitasourmain
definition.
Ouralternativedefinitionsconsiderrepresentationspacesthatarepossiblymulti-dimensional,and
definesthesespacesthroughwhethertheycancompletelyexplainafunctionhontheoutputlogits.
We consider a group theoretic approach to irreducible representations, via whether computation
involvingmultiplegroupelementscanbedecomposed.
C.1 AlternativeDefinition: InterventionsandRepresentationSpaces
AssumethatwerestricttheinputsetofpromptsT = tj tosomesubsetofpromptsandthatwe
{ }
havesomeevaluationfunctionhthatmapsfromtheoutputlogitdistributionofM toarealnumber.
15100 0.50
M(f)=0.0794
(cid:15)
50 0.25
S(f)=0.1156
0 0.00
2 0 2 0 π π 3π 2π
representationdim1 −normalizedv f+c 2 angleθ 2
·
(a)TestingS(c)andM (c)onareduciblefeaturec.
ϵ
1000 1.0
M(f)=0.259
(cid:15)
500 0.5
S(f)=0.2541
0 0.0
2.5 0.0 2.5 0 π π 3π 2π
representationdim1 − normalizedv f+c 2 angleθ 2
·
(b)TestingS(d)andM (d)onanirreduciblefeatured
ϵ
Figure9: Testingirreducibilityofsyntheticfeatures. Leftineachsubfigure: Distributionsofx.
Forfeaturec,7.94%lieswithinthenarrowdottedlines,indicatingthefeatureisunlikelytobea
mixture. Forfeatured,25.90%lieswithinthewidelines,indicatingthefeatureislikelyamixture.
Thegreencrossindicatestheangleθthatminimizesmutualinformation. Middleineachsubfigure:
Histogramsofthedistributionofv xwithredlinesindicatinga2ϵ-wideregion. Rightineach
·
subfigure: MutualinformationbetweenaandbasafunctionoftherotationangleθofmatrixR.
Bothfeatureshaveasmall(<0.5bits)minimummutualinformationandsoarelikelyseparable.
Forexample,fortheWeekdaysproblems,T isthesetof49promptsandhcouldbetheargmax
overthedaysofweeklogits. Abusingnotation,weletM alsobethefunctionfromthelayerweare
interveningon;thisisalwaysclearfromcontext. Thenwecandefinearepresentationspaceofxj as
i,l
asubspaceinwhichinterventionsalwayswork:
Definition5(RepresentationSpace). GivenapromptsetT = tj , arank-r dimensionalrepre-
sentation space of intermediate value xj is a rank r projectio{ n m} atrix P such that for all j,j′,
i,l
h(M((I P)xj +Pxj′))=h(M(xj′)).
− i,l i,l i,l
NotethatitimmediatelyfollowsthattherankddimensionalmatrixI istriviallyarankdrepresenta-
d
tionspaceforallpromptsetsT.
Definition6(Minimality). ArepresentationspaceP ofrankrisminimaliftheredoesnotexista
lowerrankrepresentationspace.
Aminimalrepresentationwithrank>1isamulti-dimensionalrepresentation.
Definition7(AlternativeReducibility). ArepresentationspaceP ofrankrisreducibleifthereare
orthonormalrepresentationspacesP andP (suchthatP +P =P,P P =0)where
1 2 1 2 1 2
h(M(P xj )+M(P xj ))=h(M(P xj +P xj ))
1 i,l 2 i,l 1 i,l 2 i,l
forallj,j′.
SupposeT,handM definethemultiplicationoftwoelementsinafinitegroupGofordern. Thenif
weinterprettheembeddingvectorsasthegrouprepresentations,ourdefinitionofreducibilityimplies
tothestandardgroup-theoreticaldefinitionofirreducibility––specifically,reducibilityintoatensor
productrepresentation.
16
2midnoitatneserper
2midnoitatneserper
tnuoc
tnuoc
)stib(ofnilautuM
)stib(ofnilautuMD ToyCaseofTrainingSAEsonCircles
ToexplorehowSAEsbehavewhenreconstructingirreduciblefeaturesofdimensiond > 1, we
f
performexperimentswiththefollowingtoysetup. Inspiredbythecircularrepresentationsofintegers
that networks learn when trained on modular addition [27, 35], we create synthetic datasets of
activationscontainingmultiplefeatureswhichareeach2dirreduciblecircles.
Firsthowever,consideractivationsforasinglecircle–pointsuniformlydistributedontheunitcircle
inR2. WetrainSAEsonthisdatawithencoderEnc(x)=ReLU(W (x b )+b )anddecoder
e d e
−
Dec(f) = W f +b . We train SAEs with m = 2 and m = 10 with the Adam optimizer and
d d
a learning rate of 10−3, sparsity penalty λ = 0.1, for 20,000 steps, and a warmup of 1000 steps.
InFig.10weshowthedictionaryelementsoftheseSAEs. Whenm=2,thenetworkmustuseboth
SAEfeaturesoneachinputpoint,andusesd toshiftthereconstructedcirclesoitiscenteredatthe
b
origin. Whenm=10,d 0andthefeaturesspreadoutacrossthecirclehavingcloseneighbors,
b
≈
withonlyasubsetbeingactiveonanyoneinput.
sparsity loss: 1.93 sparsity loss: 1.27, seed=2 sparsity loss: 1.28, seed=4
2 SAE hidden activations
db
Reconstruction 1 1
1 Dictionary element
0 0 0
1
db db
1 Reconstruction 1 Reconstruction
2 Dictionary element Dictionary element
2 1 0 1 2 1 0 1 1 0 1
Figure10: SAEstrainedtoreconstructasingle2dcirclewithm=2(left)andm=10(middleand
right)dictionaryelements. WhenthereareseveralSAEfeatures,thereisnotanaturalchoicefeature
directions,andthedictionaryelementsspreadoutacrossthecircle.
Wenowconsidersyntheticactivationswithmultiplecircularfeatures. Ourdataconsistsofpointsin
R10,wherewechoosetwoorthogonalplanesspannedby(e ,e )and(e ,e ),respectively. With
1 2 3 4
probabilityonehalfapointsissampleduniformlyontheunitcircleinthee -e plane,otherwisethe
1 2
pointwillbesampleduniformlyontheunitcircleinthee -e plane. WetrainSAEswithm=64
3 4
onthisdatawiththesamehyperparametersasthesingle-circlecase.
WenowapplytheproceduredescribedinSection4toseeifwecanautomaticallyrediscoverthese
circles. Encouragingly, we first find that the alive SAE features align almost exactly with either
the e -e or the e -e plane. When we apply spectral clustering with n_clusters = 2 to the
1 2 3 4
featureswiththepairwiseangularsimilaritiesbetweendictionaryelementsasthesimilaritymatrix
(Fig.11,left),thetwoclusterscorrespondexactlytothefeatureswhichspaneachplane. Asdescribed
inSection4,givenaclusterofdictionaryelementsS 1,...,m ,werunalargesetofactivations
⊂{ }
throughtheSAE,thenfilteroutsampleswhichdon’tactivateanyelementinS. Forsampleswhich
doactivateanelementofS,reconstructtheactivationwhilesettingallSAEfeaturesnotinS tohave
ahiddenactivationofzero. IfsomecollectionofSAEfeaturestogetherrepresentsomeirreducible
feature,wewanttoremoveallotherfeaturesfromtheactivationvector,andsoweonlyallowSAE
features in the collection to participate in reconstructing the input activation. We find that this
procedurealmostexactlyrecoverstheoriginaltwocircles,whichencouragedustoapplythismethod
fordiscoveringthefeaturesshowninFig.1andFig.13.
E TrainingMistralSAEs
OurMistral7B[21]sparseautoencoders(SAEs)aretrainedonoveronebilliontokensfromasubset
ofthePile[12]andAlpaca[39]datasets. WetrainourSAEsonlayers8,16,and24outof32total
layerstomaximizecoverageofthemodel’srepresentations. Weusea16 expansionfactor,yielding
×
atotalof65536dictionaryelementsforeachSAE.
17Cosine sim between alive features Cluster 1 reconstruction Cluster 2 reconstruction
0 1
1 1
15
0 0 0
30
1 1
1
0 15 30 1 0 1 1 0 1
alive SAE feature PCA axis 1 PCA axis 1
Figure11: AutomaticdiscoveryofsyntheticcircularfeaturesbyclusteringSAEdictionaryelements.
TotrainourSAEs,weuseanL sparsitypenaltyforp=1/2withsparsitycoefficientλ=0.012.
p
BeforeanSAEforwardpass,wenormalizeouractivationvectorstohavenorm√d = 64in
model
thecaseofMistral. Wedonotapplyapre-encoderbias. WeuseanAdamWoptimizerwithweight
decay10−3andlearningrate0.0002withalinearwarmup. Weapplydeadfeatureresampling[6]
fivetimesoverthecourseoftrainingtoconvergeonSAEswitharound1000deadfeatures.
F GPT-2andMistral7BDictionaryElementClustering
F.1 GPT-2-smallmethodsandresults
ForGPT-2-small,weperformspectralclusteringontheroughly25klayer7SAEfeaturesfrom[5],
usingpairwiseangularsimilaritiesbetweendictionaryelementsasthesimilaritymatrix. Weuse
n_clusters = 1000andmanuallylookedatroughly500oftheseclusters. Foreachcluster,we
lookedatprojectionsontoprincipalcomponents1-4ofthereconstructedactivationsfortheseclusters.
InFig.12,weshowprojectionsforthemostinterestingclustersweidentified,whichappeartobe
circularrepresentationsofdaysoftheweek,monthsoftheyear,andyearsofthe20thcentury.
F.2 Mistral7Bmethodsandresults
ForMistral7B,ourSAEshave65536dictionaryelementsandwefounditdifficulttorunspectral
clusteringonalloftheseatonce. Wethereforedevelopasimplegraphbasedclusteringalgorithm
thatwerunonMistral7BSAEs:
1. Create a graph G out of the dictionary elements by adding directed edges from each
dictionaryelementtoitskclosestdictionaryelementsbycosinesimilarity. Weusek =2.
2. Makethegraphundirectedbyturningeverydirectededgeintoanundirectededge.
3. Pruneedgeswithcosinesimilaritylessthanathresholdvalueτ. Weuseτ =0.5.
4. Returntheconnectedcomponentsasclusters.
WerunthisalgorithmontheMistral7Blayer8SAE(216 dictionaryelements)andfindroughly
2700clusterscontainingbetween2and1000elements. Wemanuallyinspectedroughly2000of
these. Fromthese,were-discovercircularrepresentationsofdaysoftheweekandmonthsofthe
year,showninFig.13. However,wedidnotfindotherobviouslyinterestingandclearlyirreducible
features.
As future work, we think it would be exciting to develop better clustering techniques for SAE
features. Ourgraphbasedclusteringtechniquecouldlikelybeimprovedbymorerecentefficient
andhigh-qualitygraphbasedclusteringtechniques,e.g. hierarchicalagglomerateclusteringwith
single-linkage[25]. Additionally, webelievewewouldseealargeimprovementbysettingedge
weightstobeacombinationofboththecosineandJaccardsimilarityofthedictionaryelements,e.g.
max(cosine,Jaccard).
18
erutaef
EAS
evila
2
sixa
ACP
2
sixa
ACPFigure12: Projectionsofdaysofweek,monthsofyear,andyearsofthe20thcenturyrepresentations
ontotopfourprincipalcomponents,showingadditionaldimensionsoftherepresentationsthanFig.1.
G FurtherExperimentDetails
G.1 AssetsInformation
We use the following open source models for our experiments: Llama 3 8B [2] (custom Llama
3 license https://llama.meta.com/llama3/license/), Mistral 7B [21] (released under the
Apache2License),andGPT-2[41](modifiedMITlicense,seehttps://github.com/openai/
gpt-2/blob/master/LICENSE).
G.2 MachineInformation
Intervention experiments were run on two V100 GPUs using less than 64 GB of CPU RAM;
all experiments can be reproduced from our open source repository in less than a day with this
configuration. WeusetheTransformerLenslibrary[34]forinterventionexperiments. ϵ-mixtureindex
measurementsontoydatasetstookaboutoneminuteeach,on8GBofCPURAM.EVRexperiments
takesecondson8GBofCPURAMandaredominatedbytimetakentohuman-interprettheRGB
plots.
19Figure13: Circularrepresentationsofdaysoftheweekandmonthsoftheyearwhichwediscover
withourunsupervisedSAEclusteringmethodinMistral7B.UnlikesimilarfeaturesinGPT-2,we
alsofindanadditional“weekend”representationinbetweenSaturdayandSundayrepresentations
(left) and additional representations of seasons among the months (right). For instance, “winter”
tokensactivatearegionofthecircleinbetweentherepresentationofJanuaryandDecember.
GPT-2 SAE clustering and plotting was run on a cluster of heterogeneous hardware. Spectral
clusteringandcomputingreconstructions+plottingwasdoneonCPUsonly. Wemadereconstruction
plotsfor500clusters,witheachtakinglessthan10minutes. Mistral7BSAEreconstructionplots
weremadeonthesamecluster. Wemaderoughly2000reconstructionplotsforMistral7B(and
manuallyinspectedeach),witheachtakinglessthan20minutestogenerate. Jobswereallocated
64GBofmemoryeach.
MistralSAEtrainingwasrunonasingleV100GPU.InitiallycachingactivationsfromMistral7Bon
onebilliontokenstookapproximately60hours. TrainingtheSAEsonthesavedactivationstook
another36hours.
G.3 ErrorBarCalculation
InFig.5wereport96%errorbarsforallinterventionmethods. Tocomputetheseerrorbars,we
loopoverallinterventionmethodsandalllayersandcomputeaconfidenceintervalforeach(method,
layer)pairacrossallprompts. Assumingnormallydistributederrors,wecomputeerrorbarswiththe
followingstandardformula:
EB =µ z SE
± ∗
whereµisthesamplemean,zisthezscore(slightlylargerthan2for96%errorbars),andSE isthe
standarderror(thestandarddeviationdividedbythesquarerootofthenumberofsamples). Weuse
standardPythonfunctionstocomputethisvalue.
The reason that the Months error bars are smaller than the Weekdays error bars is because there
are more Months prompts: there are 12 12 11 = 1584 intervention effect values, rather than
∗ ∗
7 7 6=294interventioneffectvalues.
∗ ∗
H MoreWeekdaysandMonthsPlotsandDetails
WeshowtheresultsofMistral7BandLlama38BonallindividualinstancesofWeekdaysthatat
leastoneofthemodelsgetwronginTable2andpresentasimilartableforMonthsinTable3.
WeshowprojectionsontothetoptwoPCAdirectionsforbothMistral7BandLlama38BinFig.14
onthehiddenlayersontopoftheαtoken,coloredbyα. ThesearesimilarplotstoFig.3,except
theyareonalllayers. Thecircularstructureinαisvisibleonmany—butnotall—layers. Muchof
thelinearstructurevisibleisduetoβ.
20Layer 0 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Layer 7 Layer 0 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Layer 7
Layer 8 Layer 9 Layer 10 Layer 11 Layer 12 Layer 13 Layer 14 Layer 15 Layer 8 Layer 9 Layer 10 Layer 11 Layer 12 Layer 13 Layer 14 Layer 15
Layer 16 Layer 17 Layer 18 Layer 19 Layer 20 Layer 21 Layer 22 Layer 23 Layer 16 Layer 17 Layer 18 Layer 19 Layer 20 Layer 21 Layer 22 Layer 23
Layer 24 Layer 25 Layer 26 Layer 27 Layer 28 Layer 29 Layer 30 Layer 31 Layer 24 Layer 25 Layer 26 Layer 27 Layer 28 Layer 29 Layer 30 Layer 31
Monday Tuesday Wednesday Thursday Friday Saturday Sunday Monday Tuesday Wednesday Thursday Friday Saturday Sunday
(a)Mistral7B,Weekdays (b)Llama38B,Weekdays
Layer 0 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Layer 7 Layer 0 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Layer 7
Layer 8 Layer 9 Layer 10 Layer 11 Layer 12 Layer 13 Layer 14 Layer 15 Layer 8 Layer 9 Layer 10 Layer 11 Layer 12 Layer 13 Layer 14 Layer 15
Layer 16 Layer 17 Layer 18 Layer 19 Layer 20 Layer 21 Layer 22 Layer 23 Layer 16 Layer 17 Layer 18 Layer 19 Layer 20 Layer 21 Layer 22 Layer 23
Layer 24 Layer 25 Layer 26 Layer 27 Layer 28 Layer 29 Layer 30 Layer 31 Layer 24 Layer 25 Layer 26 Layer 27 Layer 28 Layer 29 Layer 30 Layer 31
January March May July September November January March May July September November
February April June August October December February April June August October December
(c)Mistral7B,Months (d)Llama38B,Months
Figure14: ProjectionsontothetoptwoPCAdimensionsofmodelhiddenstatesontheαtokenshow
thatcircularrepresentationsofαarepresentinvariouslayers.
InFig.15andFig.16,wereportMLPandattentionheadpatchingresultsforWeekdaysandMonths.
Weexperimenton20pairsofproblemswiththesameαanddifferentβ and20pairsofproblems
withthesameβ anddifferentα,foratotalof40pairsofproblems. Foreachpairofproblems,we
patchtheMLP/attentionoutputsfromthe"clean"tothe"dirty"problemforeachlayerandtoken,and
thencompletetheforwardpass. Definingthelogitdifferenceasthelogitofthecleanγ minusthe
logitofthedirtyγ,werecordwhatpercentofthedifferencebetweentheoriginallogitdifference
ofthedirtyproblemandthelogitdifferenceofthecleanproblemisrecovereduponintervening,
andaverageacrossthese40percentagesforeachlayerandtoken. Thisgivesusascorewecallthe
AverageInterventionEffect.
For simplicity of presentation, we clip all of the (few) negative intervention averages to 0 (prior
work[48]hasalsofoundnegative-effectattentionheadsduringpatchingexperiments).
I FurtherEVRResults
In this section, we present results to support a claim that MLPs (and not attention blocks) are
responsible for computing γ. In Fig. 17, we deconstruct states on top of the final token (before
predicting γ) on Llama 3 8B Months (we show a similar plot for the states on the final token of
Mistral7BonWeekdaysinthemaintextinFig.8. Theseplotsshowthatthevalueofγ iscomputed
onthefinaltokenaroundlayers20to25. ToshowthatthiscomputationofoccursintheMLPs,we
mustshowthatnoattentionheadiscopyingγ fromapriortokenordirectlycomputingγ.
WefirstperformapatchingexperimentwiththesamesetupFig.16andFig.15onindividualattention
headsonthefinaltoken. Fromthepatchingresultsweidentifythetop10attentionheadsbyaverage
interventioneffect. Foreachattentionhead,wecomputeoneEVRrunwithexplanatoryfunctions
equaltoone-hotfunctionsofαandβ(resultingin14functionsg forWeekdaysand24forMonths)
i
andonewithexplanatoryfunctionsequaltoone-hotfunctionsofα,β,andγ. Wefindthatforall
layersbefore25,addingγ totheexplanatoryfunctionsaddsalmostnoexplanatorypower. Sincewe
21Table2: Weekdaysfinegrainedresults. Rowommitedifbothmodelsgetitcorrect.
α β Groundtruthγ Mistraltopγ Mistralcorrect? Llamatopγ Llamacorrect?
1 1 Wednesday Wednesday Yes Thursday No
3 1 Friday Friday Yes Tuesday No
4 1 Saturday Saturday Yes Thursday No
3 2 Saturday Saturday Yes Tuesday No
4 2 Sunday Sunday Yes Wednesday No
5 2 Monday Monday Yes Tuesday No
2 3 Saturday Friday No Saturday Yes
3 3 Sunday Sunday Yes Tuesday No
4 3 Monday Monday Yes Tuesday No
0 4 Friday Thursday No Friday Yes
3 4 Monday Monday Yes Tuesday No
0 5 Saturday Friday No Saturday Yes
1 5 Sunday Saturday No Wednesday No
2 5 Monday Sunday No Monday Yes
4 5 Wednesday Tuesday No Tuesday No
6 5 Friday Thursday No Thursday No
1 6 Monday Sunday No Thursday No
2 6 Tuesday Monday No Tuesday Yes
3 6 Wednesday Tuesday No Tuesday No
4 6 Thursday Thursday Yes Tuesday No
5 6 Friday Friday Yes Thursday No
6 6 Saturday Thursday No Thursday No
0 7 Monday Sunday No Tuesday No
1 7 Tuesday Sunday No Tuesday Yes
2 7 Wednesday Sunday No Wednesday Yes
3 7 Thursday Sunday No Thursday Yes
4 7 Friday Thursday No Tuesday No
5 7 Saturday Friday No Saturday Yes
6 7 Sunday Friday No Thursday No
Table3: Monthsfinegrainedresults. Rowommitedifbothmodelsgetitcorrect.
α β Groundtruthγ Mistraltopγ Mistralcorrect? Llamatopγ Llamacorrect?
0 4 May April No May Yes
6 4 November October No November Yes
0 6 July June No July Yes
0 7 August July No August Yes
1 7 September October No September Yes
3 7 November October No November Yes
5 7 January December No January Yes
6 7 February January No February Yes
7 7 March February No March Yes
9 7 May April No May Yes
4 9 February February Yes January No
2 10 January December No January Yes
8 10 July June No July Yes
1 11 January December No January Yes
2 11 February December No February Yes
3 11 March February No March Yes
7 11 July June No July Yes
8 11 August July No August Yes
9 11 September August No September Yes
0 12 January December No January Yes
22*Monday 0.3 *Monday 0.4
is is 0.3
0.2
two two 0.2
days 0.1 days
0.1
from from
0.0 0.0
0 10 20 30 0 10 20 30
Layer Layer
(a)Mistral7BMLPPatching (b)Mistral7Battentionpatching
*Monday *Monday
0.4
0.2
is is
two two
0.2 0.1
days days
from from
0.0 0.0
0 10 20 30 0 10 20 30
Layer Layer
(c)Llama38BMLPpatching (d)Llama38Battentionpatching
Figure15: AttentionandMLPpatchingresultsonWeekdays. Resultsareaveragedover20different
runswithfixedαandvaryingβ and20differentrunswithfixedβ andvaryingα.
establishedabovethatthemodelhasalreadycomputedγ atthispoint,weknowthatattentionheads
donotparticipateincomputingγ.
23*Two 0.3 *Two 0.4
months months 0.3
0.2
from from
0.2
*January 0.1 *January
0.1
is is
0.0 0.0
0 10 20 30 0 10 20 30
Layer Layer
(a)Mistral7BMLPPatching (b)Mistral7Battentionpatching
*Two 0.4 *Two
0.3
months 0.3 months
0.2
from from
0.2
*January *January 0.1
0.1
is is
0.0 0.0
0 10 20 30 0 10 20 30
Layer Layer
(c)Llama38BMLPpatching (d)Llama38Battentionpatching
Figure16: AttentionandMLPpatchingresultsonMonths. Resultsareaveragedover20different
runswithfixedαandvaryingβ and20differentrunswithfixedβ andvaryingα.
layer13 layer14 layer15 layer16 layer17 layer18 layer19 layer20 layer21 layer22 layer23 layer24 layer25 layer26 layer27 layer28 layer29
original original original original original original original original original original original original original original original original original
r2:0.0% r2:0.0% r2:0.0% r2:0.0% r2:0.0% r2:0.0% r2:0.0% r2:0.0% r2:0.0% r2:0.0% r2:0.0% r2:0.0% r2:0.0% r2:0.0% r2:0.0% r2:0.0% r2:0.0%
onehotβ onehotβ onehotβ onehotβ onehotβ onehotβ onehotβ onehotβ onehotβ circleγ circleγ circleγ circleγ circleγ γparity γparity circleγ
r2:69.7% r2:72.8% r2:69.9% r2:89.0% r2:76.6% r2:81.8% r2:54.1% r2:47.8% r2:36.8% r2:23.6% r2:24.3% r2:24.7% r2:25.5% r2:25.5% r2:12.0% r2:12.2% r2:23.7%
onehotαonehotαonehotαonehotαonehotαonehotαγparity circleγ circle2γonehotβ onehotβ onehotβ onehotβ onehotβ circleγ circleγ onehotβ
r2:95.7% r2:96.0% r2:96.8% r2:98.7% r2:99.0% r2:96.8% r2:60.2% r2:60.9% r2:47.8% r2:51.6% r2:48.3% r2:48.2% r2:48.2% r2:48.1% r2:35.4% r2:34.2% r2:43.6%
α+1=β α+1=β α+1=β α+1=βcircle2γγparitycircle2γγparity circleγ γparity γparity γparity γparity γparity onehotβ onehotβ γparity
r2:96.4% r2:96.5% r2:97.2% r2:98.9% r2:99.2% r2:98.0% r2:64.7% r2:68.0% r2:62.5% r2:59.5% r2:57.9% r2:56.9% r2:55.6% r2:55.1% r2:56.8% r2:56.5% r2:54.4%
circleγ circleγ circle2γγparitycircle2γcircle2γcircle2γcircle2γcircle2γcircle2γcircle2γcircle2γ
r2:98.8% r2:73.4% r2:73.7% r2:68.7% r2:68.6% r2:66.2% r2:64.4% r2:63.6% r2:64.0% r2:65.7% r2:65.6% r2:65.1%
onehotαonehotαonehotαonehotαonehotαonehotαonehotαonehotαonehotαonehotαonehotα
r2:92.0% r2:89.9% r2:83.8% r2:81.4% r2:80.1% r2:77.5% r2:76.2% r2:75.6% r2:76.9% r2:76.6% r2:74.7%
onehotα+βonehotα+βonehotα+βonehotα+βonehotα+βonehotα+βonehotα+βonehotα+βonehotα+βonehotα+βonehotα+β
r2:95.2% r2:94.3% r2:91.4% r2:90.6% r2:90.9% r2:90.8% r2:90.7% r2:90.6% r2:90.9% r2:91.1% r2:92.4%
Figure17: IterativedeconstructionofhiddenstaterepresentationsonthefinaltokenonLlama38B,
Months.
24Table4: Highestinterventioneffectattentionheadsfromfine-grainedattentionheadpatching,aswell
asEVRresultswithonehotα,β andonehotα,β,γ.
(a)Mistral7B,Weekdays. (b)Llama38B,Weekdays.
L H Average EVRR2 EVRR2 L H Average EVRR2 EVRR2
Inter- OneHot OneHot Inter- OneHot OneHot
vention α,β α,β,γ vention α,β α,β,γ
Effect Effect
28 18 0.22 0.39 0.73 17 0 0.18 0.98 0.99
18 30 0.17 0.95 0.96 17 1 0.08 0.98 0.98
15 13 0.17 0.94 0.95 19 10 0.08 0.95 0.96
22 15 0.11 0.77 0.82 30 17 0.07 0.85 0.90
16 21 0.09 0.92 0.93 17 3 0.07 0.93 0.95
28 16 0.08 0.42 0.69 17 27 0.06 1.00 1.00
15 14 0.06 0.98 0.99 31 22 0.05 0.37 0.78
30 24 0.05 0.43 0.79 21 9 0.04 0.73 0.78
21 26 0.04 0.53 0.63 20 28 0.04 1.00 1.00
14 2 0.04 0.93 0.95 30 16 0.04 0.73 0.85
(c)Mistral7B,Months. (d)Llama38B,Months.
L H Average EVRR2 EVRR2 L H Average EVRR2 EVRR2
Inter- OneHot OneHot Inter- OneHot OneHot
vention α,β α,β,γ vention α,β α,β,γ
Effect Effect
20 28 0.15 0.76 0.76 15 13 0.26 0.62 0.62
17 0 0.10 0.77 0.77 16 21 0.17 0.76 0.76
25 14 0.08 0.19 0.61 18 30 0.13 0.77 0.77
17 1 0.07 0.80 0.82 28 18 0.11 0.13 0.52
17 3 0.06 0.71 0.71 28 16 0.07 0.13 0.52
31 22 0.06 0.12 0.67 21 25 0.05 0.65 0.70
17 27 0.05 0.58 0.58 15 14 0.03 0.72 0.72
19 4 0.05 0.40 0.66 17 26 0.02 0.77 0.77
19 10 0.04 0.62 0.62 31 1 0.02 0.11 0.57
30 26 0.04 0.51 0.62 21 24 0.02 0.30 0.45
25