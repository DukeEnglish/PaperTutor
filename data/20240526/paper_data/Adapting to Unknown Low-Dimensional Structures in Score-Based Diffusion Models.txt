Adapting to Unknown Low-Dimensional Structures in
Score-Based Diffusion Models
Gen Li∗† Yuling Yan∗‡
May 24, 2024
Abstract
Thispaperinvestigatesscore-baseddiffusionmodelswhentheunderlyingtargetdistributionisconcen-
tratedonornearlow-dimensional manifolds withinthehigher-dimensionalspaceinwhichtheyformally
reside, a common characteristic of natural image distributions. Despite previous efforts to understand
thedatagenerationprocessofdiffusionmodels,existingtheoreticalsupportremainshighlysuboptimalin
thepresenceoflow-dimensionalstructure,whichwestrengtheninthispaper. ForthepopularDenoising
Diffusion Probabilistic Model (DDPM), we find that the dependency of the error incurred within each
denoisingstepontheambientdimensiondisingeneralunavoidable. Wefurtheridentifyauniquedesign
of coefficients that yields a converges rate at the order of O(k2/√T) (up to log factors), where k is the
intrinsic dimension of the target distribution and T is the number of steps. This represents the first
theoreticaldemonstrationthattheDDPMsamplercanadapttounknownlow-dimensional structuresin
thetargetdistribution,highlightingthecriticalimportanceofcoefficientdesign. Allofthisisachievedby
anovelset of analysis tools that characterize thealgorithmic dynamicsin a moredeterministic manner.
Keywords: diffusion model, score-based generative models, denoising diffusion probabilistic model, low-
dimensional structure, coefficient design
Contents
1 Introduction 2
1.1 Diffusion models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 Inadequacy of existing results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.3 Our contributions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2 Problem set-up 4
3 Main results 5
3.1 Convergence analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Uniqueness of coefficient design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
4 Analysis for the DDPM sampler (Proof of Theorem 1) 6
4.1 Step 1: identifying high-probability sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4.2 Step 2: connecting conditional densities p Xt−1 |Xt and p Y t⋆ −1|Yt . . . . . . . . . . . . . . . . . . 7
4 4. .3
4
S St te ep
p
3 4:
:
b bo ou un nd di in ng
g
t th he
e
K KL
L
d di iv ve er rg ge en nc ce
e
b be et tw we ee en
n
p pXt−1 |Xt a an nd
d
p pY t⋆ −1|Yt .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
8
9
Xt−1 |Xt Yt−1 |Yt
4.5 Step 5: putting everything together . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
5 Discussion 10
∗Theauthors contributed equally.
†Department ofStatistics,TheChineseUniversityofHongKong,HongKong;Email: genli@cuhk.edu.hk.
‡InstituteforData,Systems,andSociety,MassachusettsInstituteofTechnology,MA02142,USA;Email: yulingy@mit.edu.
1
4202
yaM
32
]GL.sc[
1v16841.5042:viXraA Proof of auxiliary lemmas for the DDPM sampler 10
A.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
A.2 Understanding the conditional density p ( x ). . . . . . . . . . . . . . . . . . . . . . . . 11
Xt|X0
·|
0
A.3 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
A.4 Proof of Lemma 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
A.5 Proof of Lemma 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.6 Proof of Lemma 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
A.7 Proof of Lemma 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
A.8 Proof of Lemma 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
A.9 Proof of Lemma 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
B Proof of Theorem 2 26
C Technical lemmas 27
1 Introduction
Score-based diffusion models are a class of generative models that have gained prominence in the field
of machine learning and artificial intelligence for their ability to generate high-quality new data instances
from complex distributions, such as images, audio, and text (Dhariwal and Nichol, 2021; Ho et al., 2020;
Sohl-Dickstein et al., 2015; Song and Ermon, 2019; Song et al., 2021). These models operate by gradually
transforming noise into samples from the target distribution through a denoising process guided by pre-
trainedneuralnetworksthatapproximatethescorefunctions. Inpractice,score-baseddiffusionmodelshave
demonstrated remarkable performance in generating realistic and diverse content across various domains
(Croitoru et al., 2023; Ramesh et al., 2022; Rombach et al., 2022; Saharia et al., 2022), achieving state-of-
the-art performance in generative AI.
1.1 Diffusion models
The development of score-based diffusion models is deeply rooted in the theory of stochastic processes. At
a high level, we consider a forwardprocess:
addnoise addnoise addnoise
X X X , (1.1)
0 1 T
−→ −→ ··· −→
which draws a sample from the target data distribution (i.e., X
0
pdata), then progressively diffuses it to
∼
Gaussian noise over time. The key aspect of the diffusion model is to construct a reverse process:
denoise denoise denoise
Y Y Y (1.2)
T T 1 0
−→ − −→ ··· −→
d
satisfying Y X for all t, which starts with pure Gaussian noise (i.e., Y (0,I )) and gradually
t t T d
≈ ∼ N
converts it back to a new sample Y
0
sharing a similar distribution to pdata.
The classicalresults on time-reversalof SDEs (Anderson, 1982; Haussmann and Pardoux, 1986) provide
the theoretical foundation for the above task. Consider a continuous time diffusion process:
1
dX
t
= β(t)X tdt+ β(t)dW
t
(0 t T), X
0
pdata (1.3)
−2 ≤ ≤ ∼
p
for some function β : [0,T] R+, where (W ) is a standard Brownian motion. For a wide range of
t 0 t T
→ ≤ ≤
functions β, this process converges exponentially fast to a Gaussian distribution. Let p () be the density
Xt
·
of X . One can construct a reverse-time SDE:
t
1
dY = β(t) Y +2 logp (Y ) + β(t)dZ (0 t T), Y p , (1.4)
t
−2
t
∇
XT−t t t
≤ ≤
0
∼
XT
(cid:0) (cid:1) p
e e e e d
where(Z ) isanotherstandardBrownianmotion. Define Y =Y . Itiswell-knownthatX =Y for
t 0 t T t T t t t
≤ ≤ −
all 0 t T. Here, logp is called the score function for the law of X , which is not explicitly known.
≤ ≤ ∇
Xt t
e
2The above result motivates the following paradigm: we can construct the forwardprocess (1.1) by time-
discretizingthediffusionprocess(1.3),andconstructthereverseprocess(1.2)bydiscretizingthereverse-time
SDE(1.4)andlearningthescorefunctionsfromthedata. ThisapproachleadstothepopularDDPMsampler
(Ho et al.,2020;Nichol and Dhariwal,2021). AlthoughtheideaoftheDDPMsamplerisrootedinthetheory
of SDEs, the algorithm and analysis presented in this paper do not require any prior knowledge of SDEs.
This paper examines the accuracy of the DDPM sampler by establishing the proximity between the
output distributionofthe reverseprocessandthe targetdatadistribution. Since these twodistributions are
identicalinthecontinuoustimelimitwithperfectscoreestimation,theperformanceoftheDDPMsampleris
influencedbytwosourcesoferror: discretizationerror(duetoafinite numberofsteps)andscoreestimation
error (due to imperfect estimation of the scores). This paper views the score estimation step as a black box
(often addressed by training a large neural network) and focuses on understanding how time discretization
and imperfect score estimation affect the accuracy of the DDPM sampler.
1.2 Inadequacy of existing results
ThepastfewyearshavewitnessedasignificantinterestinstudyingtheconvergenceguaranteesfortheDDPM
sampler (Benton et al., 2023; Chen et al., 2023a,c; Li et al., 2024). To facilitate discussion, we consider an
ideal setting with perfect score estimation. In this context, existing results can be interpreted as follows: to
achieveε-accuracy(i.e.,thetotalvariationdistancebetweenthetargetandtheoutputdistributionissmaller
than ε), it suffices to take a number of steps exceeding the order of poly(d)/ε2 (ignoring logarithm factors),
wheredistheproblemdimension. Amongtheseresults,thestate-of-the-artisgivenbyBenton et al.(2023),
which achieved linear dependency on the dimension d.
However,thereseemstobeasignificantgapbetweenthepracticalperformanceoftheDDPMsamplerand
theexistingtheory. Forexample,fortwowidelyusedimagedatasets,CIFAR-10(dimensiond=32 32 3)
× ×
and ImageNet (dimension d 64 64 3), it is known that 50 and 250 steps (also known as NFE,
≥ × ×
the number of function evaluations) are sufficient to generate good samples (Dhariwal and Nichol, 2021;
Nichol and Dhariwal, 2021). This is in stark contrast with the existing theoretical guarantees discussed
above,whichsuggestthatthe number ofstepsT shouldexceedtheorderofthe dimensiondto achievegood
performance.
Empirical evidence suggests that the distributions of natural images are concentrated on or near low-
dimensional manifolds within the higher-dimensionalspace in which they formally reside (Pope et al., 2021;
Simoncelli and Olshausen,2001). Inview of this, a reasonableconjecture is thatthe convergencerateof the
DDPM sampler actually depends on the intrinsic dimension rather than the ambient dimension. However,
the theoretical understanding of diffusion models when the support of the target data distribution has
a low-dimensional structure remains vastly under-explored. As some recent attempts, De Bortoli (2022)
established the first convergenceguarantee under the Wasserstein-1 metric. However,their error bound has
linear dependence on the ambient dimension d and exponential dependence on the diameter of the low-
dimensional manifold. Another recent work (Chen et al., 2023b) focused mainly on score estimation with
properly chosen neural networksthat exploit the low-dimensionalstructure, which is also different from our
main focus.
1.3 Our contributions
Inlightofthelargetheory-practicegapandtheinsufficiency ofpriorresults,this papertakesasteptowards
understandingtheperformanceoftheDDPMsamplerwhenthetargetdatadistributionhaslow-dimensional
structure. Our main contributions can be summarized as follows:
• We show that, with a particular coefficient design, the error of the DDPM sampler, evaluated by the
total variation distance between the laws of X and Y , is upper bounded by
1 1
k2 1 T
+ E s (X ) s⋆(X ) 2 ,
√T vT k t t − t t k2
u t=1
u X (cid:2) (cid:3)
t
up to some logarithmic factors, where k is the intrinsic dimension of the target data distribution (which
will be rigorously defined later), and s⋆ (resp. s ) is the true (resp. learned) score function at each
t t
3step. The first term represents the discretizationerror (which vanishes as the number of steps T goes to
infinity), while the second term should be interpreted as the score matching error. This bound is nearly
dimension-free — the ambient dimension d only appears in logarithmic terms.
• We also show that our choice of the coefficients is, in some sense, the unique schedule that does not
incurdiscretizationerrorproportionaltothe ambientdimensiondateachstep. This is insharpcontrast
with the general setting without a low-dimensional structure, where a fairly wide range of coefficient
designs can lead to convergence rates with polynomial dependence on d. Additionally, this confirms the
observation that the performance of the DDPM sampler can be improved through carefully designing
coefficients (Bao et al., 2022; Nichol and Dhariwal, 2021).
As far as we know, this paper provides the first theory demonstrating the capability of the DDPM sampler
in adapting to unknown low-dimensional structures.
2 Problem set-up
In this section, we introduce some preliminaries and key ingredients for the diffusion model and the DDPM
sampler.
Forward process. We consider the forward process (1.1) of the form
X
t
= 1 β tX
t
1+ β tW
t
(t=1,...,T), X
0
pdata, (2.1)
− − ∼
i.i.d. p p
where W ,...,W (0,I ), and the learning rates β (0,1) will be specified later. For each t 1, X
1 T d t t
has a probability
de∼ nsitN
y function (PDF) supported on
Rd∈
, and we will use q to denote the law
or≥
PDF of
t
X . Let α :=1 β and α := t α . It is straightforwardto check that
t t − t t i=1 i
X =√Qα X +√1 α W where W (0,I ). (2.2)
t t 0 t t t d
− ∼N
We will choose the learning rates β to ensure that α becomes vanishingly small, such that q (0,I ).
t T T d
≈N
Score functions. The key ingredients for constructing the reverse process with the DDPM sampler are
the score functions s⋆ :Rd Rd associated with each q , defined as
t → t
s⋆(x):= logq (x) (t=1,...,T).
t ∇ t
These score functions are not explicitly known. Here we assume access to an estimate s () for each s⋆(),
t · t ·
and we define the averaged ℓ score estimation error as
2
T
1
ε2 := E s (X) s⋆(X) 2 .
score T X ∼qt k t − t k2
Xt=1 h i
This quantity captures the effect of imperfect score estimation in our theory.
The DDPM sampler. To construct the reverse process (1.2), we use the DDPM sampler
1
Y = Y +η s (Y )+σ Z (t=T,...,1), Y (0,I ) (2.3)
t 1 t t t t t t T d
− √α t ∼N
(cid:0) (cid:1)
i.i.d.
where Z ,...,Z (0,I ). Here η ,σ > 0 are the hyperparameters that play an important role in
1 T d t t
∼ N
the performance of the DDPM sampler, especially when the target data distribution has low-dimensional
structure. As we will see, our theory suggests the following choice
(1 α )(α α )
η⋆ =1 α and σ⋆2 = − t t − t . (2.4)
t − t t 1 α
t
−
For each 1 t T, we will use p to denote the law or PDF of Y .
t t
≤ ≤
4Target data distribution. Let Rd be the support set of the targetdata distribution pdata, i.e., the
smallest closed set C Rd such thaX t p⊆ data(C) = 1. To allow for the greatest generality, we use the notion
⊆
of ε-net and covering number (see e.g., Vershynin (2018)) to characterize the intrinsic dimension of . For
X
any ε > 0, a set is said to be an ε-net of if for any x , there exists some x in such that
ε ′ ε
N ⊆ X X ∈ X N
x x ε. The covering number N ( ) is defined as the smallest possible cardinality of an ε-net of .
′ 2 ε
k − k ≤ X X
• (Low-dimensionality) Fix ε = T −cε, where c
ε
> 0 is some sufficiently large universal constant. We
define the intrinsic dimension of to be some quantity k >0 such that
X
logN ε( ) CcoverklogT
X ≤
for some constant Ccover >0.
• (Bounded support) Suppose that there exists a universal constant c >0 such that
R
sup x R where R:=TcR.
k k2 ≤
x
∈X
Namely we allow polynomial growth of the diameter of in the number of steps T.
X
Our setting allows to be concentrated on or near low-dimensional manifolds, which is less stringent than
X
assuminganexactlow-dimensionalstructure. Asasanitycheck,when residesinanr-dimensionalsubspace
ofRd,astandardvolumeargument(seee.g.,Vershynin(2018,SectionX
4.2.1))giveslogN ( ) rlog(R/ε)
ε
X ≍ ≍
rlogT, suggesting that the intrinsic dimension k is of order r in this case.
Learning rate schedule. Following Li et al. (2024), we adopt the following learning rate schedule
t
1 c logT c logT
1 1
β = , β = min β 1+ ,1 (t=1,...,T 1)
1 Tc0 t+1 T ( 1 (cid:18) T (cid:19) ) −
for some sufficiently large constants c ,c > 0. This schedule is not unique – any other schedule of β
0 1 t
satisfying the properties in Lemma 8 can lead to the same result in this paper.
3 Main results
We are now positioned to present our main theoretical guarantees for the DDPM sampler.
3.1 Convergence analysis
We first present the convergence theory for the DDPM sampler. The proof can be found in Section 4.
Theorem 1. Suppose that we take the coefficients for the DDPM sampler (2.3) to be η = η⋆ and σ =σ⋆
t t t t
(cf. (2.4)), then there exists some universal constant C >0 such that
(k+logd)2log3T
TV(q 1,p 1) C +CεscorelogT. (3.1)
≤ √T
SeveralimplicationsofTheorem1followimmediately. Thetwotermsin(3.1)correspondtodiscretization
error and score matching error, respectively. Assuming perfect score estimation (i.e., εscore = 0) for the
moment,ourerrorbound(3.1)suggestsaniterationcomplexityoforderk4/ε2 (ignoringlogarithmicfactors)
for achieving ε-accuracy, for any nontrivial target accuracy level ε < 1. In the absence of low-dimensional
structure (i.e., k d), our result also recovers the iteration complexity in Benton et al. (2023); Chen et al.
≍
(2023a,c); Li et al. (2024) of order poly(d)/ε2.1 This suggests that our choice of coefficients (2.4) allows
the DDPM sampler to adapt to any potential (unknown) low-dimensional structure in the target data
distribution, and remains a valid criterion in the most general settings. The score matching error in (3.1)
scales proportionallywith εscore, suggesting that the DDPM sampler is stable to imperfect score estimation.
1Our result exhibits a quartic dimension dependency, which is worse than the linear dependency in Bentonetal. (2023).
Thisismainlybecauseweuseacompletelydifferentanalysis. Itisnotclearwhethertheiranalysis,whichutilizestheSDEand
stochasticlocalizationtoolbox, cantackletheproblem withlow-dimensionalstructure.
53.2 Uniqueness of coefficient design
In this section, we examine the importance of the coefficient design in the adaptivity of the DDPM sampler
to intrinsic low-dimensional structure. Our goal is to show that, unless the coefficients η ,σ of the DDPM
t t
sampler (2.3) are chosen according to (2.4), discretization errors proportional to the ambient dimension d
will emerge in each denoising step.
In this paper, as well as in most previous DDPM literature, the analysis on the error TV(q ,p ) usually
1 1
starts with the following decomposition
(i) 1 (ii) 1
TV2(q ,p ) KL(p p ) KL(p p )
1 1
≤ 2
X1k Y1
≤ 2
X1,...,XTk Y1,...,YT
T
( =iii) 1 KL(p p ) +1 E KL p ( x ) p ( x ) . (3.2)
2 XTk YT 2 xt∼qt Xt−1 |Xt ·| t k Yt−1 |Yt ·| t
t=2
initializationerror X error(cid:2)incu(cid:0)rredinthe(T+1 t)-thdenoisingstep (cid:1)(cid:3)
−
Here step (i) follows from P|insker{’szinequ}ality, ste|p (ii) utilizes from th{ezdata-processing ineq}uality, while
step (iii) uses the chain rule of KL divergence. We may interpret each term in the above decomposition as
the errorincurredineachdenoisingstep. Infact,this decompositionisalsocloselyrelatedtothe variational
bound on the negative log-likelihood of the reverse process, which is the optimization target for training
DDPM (Bao et al., 2022; Ho et al., 2020; Nichol and Dhariwal, 2021).
We consider a target distribution pdata = (0,I k), where I
k
Rd ×d is a diagonal matrix with I
i,i
= 1
for 1 i k and I = 0 for k+1 i dN . This is a simple ∈ distribution over Rd that is supported on
i,i
≤ ≤ ≤ ≤
a k-dimensional subspace.2 Our second theoretical result provides a lower bound for the error incurred in
each denoising step for this target distribution. The proof can be found in Appendix B.
Theorem 2. Consider the target distribution pdata = (0,I k) and assume that k d/2. For the DDPM
N ≤
sampler (2.3) with perfect score estimation (i.e., s ()=s⋆() for all t) and arbitrary coefficients η ,σ >0,
t · t · t t
we have
d d σ⋆2 2
E KL p ( x ) p ( x ) (η η⋆)2+ t 1
xt∼qt Xt−1 |Xt ·| t k Yt−1 |Yt ·| t ≥ 4 t − t 40
(cid:18)
σ t2 −
(cid:19)
for each 2 t T. S(cid:2) ee (2(cid:0) .4) for the definitions of η⋆ and(cid:1) σ(cid:3)⋆.
≤ ≤ t t
Theorem 2 shows that, unless we choose η and σ2 to be identical (or exceedingly close) to η⋆ and σ⋆2,
t t t t
thecorrespondingdenoisingstepwillincuranundesirederrorthatislinearintheambientdimensiond. This
highlights the critical importance of coefficient design for the DDPM sampler, especially when the target
distribution exhibits a low-dimensional structure.
Finally,we wouldlike to make note thatthe aboveargumentonly demonstratesthe impactofcoefficient
designonan upper bound (3.2) ofthe errorTV(q ,p ), ratherthanthe erroritself. It mightbe possible that
1 1
a broader range of coefficients can lead to dimension-independent error bound like (3.1), while the upper
bound (3.2) remains dimension-dependent. This calls for new analysis tools (since we cannot use the loose
upper bound (3.1) in the analysis), which we leave for future works.
4 Analysis for the DDPM sampler (Proof of Theorem 1)
This sectionis devotedto establishingTheorem1. The idea is tobound the errorincurredineachdenoising
step as characterizedin the decomposition (3.2), namely for each 2 t T, we need to bound
≤ ≤
E KL p ( x ) p ( x ) .
xt∼qt Xt−1 |Xt
·|
t
k
Yt−1 |Yt
·|
t
This requires connecting the two cond(cid:2)itio(cid:0)nal distributions p and p(cid:1)(cid:3) . It would be convenient to
decoupletheerrorsfromtime
discretizationandimperfectscX ort− e1 e|X stt imationYt b− y1 |Y int
troducingauxiliaryrandom
variables
1
Y⋆ := (Y +η⋆s⋆(Y )+σ⋆Z ) (2 t T). (4.1)
t −1 √α
t
t t t t t t ≤ ≤
2Althoughthisisnotaboundeddistribution,similarresultscanbeestablishedifwetruncateN(0,Ik)attheradiusR=Tc R.
Howeverthisisnotessentialandwillmaketheresultunnecessarilycomplicated,henceisomittedforclarity.
6On a high level, for each 2 t T, our proof consists of the following steps:
≤ ≤
1. Identify a typical set Rd Rd such that (X ,X ) with high probability.
t t t 1 t
A ⊆ × − ∈A
2. Establish point-wise proximity p Xt−1 |Xt(x t −1 |x t) ≈p Y t⋆ −1|Yt(x t −1 |x t) for (x t,x t −1) ∈At.
3. Characterize the deviation of p Y t⋆ −1|Yt from p Yt−1 |Yt caused by imperfect score estimation.
4.1 Step 1: identifying high-probability sets
For simplicity of presentation, we assume without loss of generality that k logd throughout the proof.3
≥
Let x⋆ be an ε-net of , and let be a disjoint ε-cover for such that x⋆ . Let
{ i}1 ≤i ≤Nε X {Bi }1 ≤i ≤Nε X i ∈Bi
:= 1 i N :P(X ) exp( C klogT) ,
ε 0 i 1
I { ≤ ≤ ∈B ≥ − }
:= ω Rd : ω 2√d+ C klogT, and
2 1
G ∈ k k ≤
(cid:8) |(x⋆ i −x⋆ j) ⊤ω |≤p C 1klogT kx⋆ i −x⋆ jk2 for all 1 ≤i,j ≤N ε ,
where C > 0 is some sufficiently large univerpsal constants. Then and can be inte(cid:9)rpreted as high
1 i i
probability sets for the variable X and a standard Gaussian rand∪ om∈I vB ariableG in Rd. For each t = 1,...T,
0
we define a typical set for each X as follows
t
:= √α x +√1 α ω :x ,ω ,
t t 0 t 0 i i
T − ∈∪∈IB ∈G
and a typical set for (X ,X ) join(cid:8)tly as follows (cid:9)
t t 1
−
x
t
√α tx
t 1
t := (x t,x t 1):x t t, − − .
A − ∈T √1 α t ∈G
n − o
The following lemma shows that is indeed a high-probability set for (X ,X ).
t t t 1
A −
Lemma 1. Suppose that C
1
Ccover. Then for each 1 t T we have
≫ ≤ ≤
C
P((X ,X ) / ) exp 1 klogT .
t t 1 t
− ∈A ≤ − 4
(cid:16) (cid:17)
Proof. See Appendix A.3.
4.2 Step 2: connecting conditional densities p Xt−1|Xt and p Y t⋆ −1|Yt
Given the definition of Y t⋆ −1 in (4.1), we can write down the conditional density p Y t⋆ −1|Yt as follows
α d/2 √α x x η⋆s⋆(x ) 2
p Y t⋆ −1|Yt(x t −1 |x t)= (cid:18)2πσt t⋆2
(cid:19)
exp (cid:18)−k t t −1 − 2σt t⋆− 2 t t t k2 (cid:19). (4.2)
Next, we will investigate the conditional density p for the forward process. For each x , we
define the shorthand notation
Xt−1 |Xt 0
∈ X
x :=E[X X =x ]= x p (x x )dx , (4.3)
0 0
|
t t
Zx0
0 X0 |Xt 0
|
t 0
and define a function ∆ xt,xt−1b:
X
→R as follows
√α (1 α )α
∆ xt,xt−1(x 0):=
−α
t
t
α
t
(√α tx
t −1
−x t)⊤(x
0
−x 0)
− 2(α
t
−
α
t)t
(1
t
α
t)kx
0
−x
0
k2
2
− − −
(1 α )√α
− t t x
t
√bα tx
0
⊤(x
0
x 0). b (4.4)
− (α α )(1 α ) − −
t t t
− −
(cid:0) (cid:1)
The next lemma provides a characterizationfor p Xt−1 |Xt thbat shobws an explicit connection with p Y t⋆ −1|Yt.
3Ifk<logd,wemayredefine k:=logd,whichdoesnotchangethedesiredbound(3.1).
7Lemma 2. For any pair (x ,x ) Rd Rd, we have
t t 1
− ∈ ×
α d/2 √α x x η⋆s⋆(x ) 2
p
Xt−1
|Xt(x
t −1
|x t)= (cid:18)2πσt
t⋆2
(cid:19)
exp (cid:18)−k t t −1 − 2σt t⋆−
2
t t t k2
(cid:19)
exp ∆ (x ) p (x x )dx .
·
xt,xt−1 0 X0 |Xt 0
|
t 0
ZX
(cid:0) (cid:1)
Proof. See Appendix A.4.
Taking Lemma 2 and (4.2) collectively yields
p (x x )
pX Y tt ⋆ −− 11 || YX tt (x tt −− 11 || x tt ) = ZX exp (cid:0)∆ xt,xt−1(x 0) (cid:1)p X0 |Xt(x 0 |x t)dx 0,
whichallowsus to controlthe density ratiobythe magnitude of∆ . By a carefulanalysisofthe above
xt,xt−1
integral for all (x ,x ) , we show in the next lemma that the density ratio is uniformly close to 1
t t 1 t
− ∈ A
within the typical set .
t
A
Lemma 3. Suppose that T k2log3T. Then there exists some universal constant C > 0 such that, for
5
≫
any 2 t T and any (x ,x ) , we have
t t 1 t
≤ ≤ − ∈A
p Xt−1 |Xt(x t −1 |x t)
1 C
5k2log3T 1
.
(cid:12)
(cid:12)
p Y t⋆ −1|Yt(x t −1 |x t) − (cid:12) (cid:12)≤ T ≤ 2
(cid:12) (cid:12)
Proof. See Appendix A.5. (cid:12) (cid:12)
(cid:12) (cid:12)
For(x ,x )outsidethetypicalset ,thefollowinglemmagivesacoarseuniformboundforthedensity
t t 1 t
− A
ratio, which is already sufficient for our later analysis.
Lemma 4. Suppose that T 1. Then for any 2 t T and any pair (x ,x ) Rd Rd, we have
t t 1
≫ ≤ ≤ − ∈ ×
p (x x )
log Xt−1 |Xt t −1 | t Tc0+2cR( √α tx
t 1
x
t
2+ x
t
2+1).
(cid:12)
(cid:12)
p Y t⋆ −1|Yt(x t −1 |x t) (cid:12) (cid:12)≤ k − − k k k
(cid:12) (cid:12)
Proof. See Appendix(cid:12) A.6. (cid:12)
(cid:12) (cid:12)
Armed with Lemmas 3 and 4, we are ready to bound the expected KL divergence between the two
conditional distributions p Xt−1 |Xt and p Y t⋆ −1|Yt.
4.3 Step 3: bounding the KL divergence between p Xt−1|Xt and p Y t⋆ −1|Yt
We first decompose the expected KL divergence between p Xt−1 |Xt and p Y t⋆ −1|Yt into
E xt∼qt KL p Xt−1 |Xt( ·|x t) kp Y t⋆ −1|Yt( ·|x t)
h (cid:16) (cid:17)i
p (x x )
= ZAt+ ZAc t!p Xt−1 |Xt(x t −1 |x t)log pX Y tt ⋆ −− 11 || YX tt (x tt −− 11 || x tt ) !p Xt(x t)dx t −1dx t
=:∆ +∆ ,
t,1 t,2
where ∆ and ∆ are the integrals over and c. It boils down to bounding these two terms.
t,1 t,2 At At
ByadirectapplicationofLemma3togetherwiththefirst-orderTaylorexpansionoflog(x)aroundx=1,
onecaneasilyshowthat ∆ .k2log3(T)/T. Howeverthis naiveboundwillleadtoavacuousfinalbound
t,1
| |
on TV(q ,p ), which depends on the sum of ∆ over all 2 t T according to (3.2). By a more careful
1 1 t,1
≤ ≤
analysis, we achieve a better bound for ∆ , as shown in the following lemma.
t,1
8Lemma 5. Suppose that T k2log3T. Then for each 2 t T, we have
≫ ≤ ≤
k4log6T
∆ 2C2 .
| t,1 |≤ 5 T2
Proof. See Appendix A.7.
For ∆ , we can employ the course bound in Lemma 4 to show that it is exponentially small.
t,2
Lemma 6. Suppose that T 1. Then for each 2 t T, we have
≫ ≤ ≤
C
1
∆ exp klogT .
t,2
| |≤ −16
(cid:18) (cid:19)
Proof. See Appendix A.8.
By putting together Lemma 5 and Lemma 6, we achieve
k4log6T
E xt∼qt KL p Xt−1 |Xt( ·|x t) kp Y t⋆ −1|Yt( ·|x t) =∆ t,1+∆ t,2 ≤3C 52 T2 (4.5)
h (cid:16) (cid:17)i
provided that T is sufficiently large.
4.4 Step 4: bounding the KL divergence between p and p
Xt−1|Xt Yt−1|Yt
Since ourgoalisto boundthe expected KLdivergencebetween p andp ,we alsoneedto upper
bound the following difference
Xt−1 |Xt Yt−1 |Yt
E xt∼qt KL p Xt−1 |Xt( ·|x t) kp Yt−1 |Yt( ·|x t) −E xt∼qt KL p Xt−1 |Xt( ·|x t) kp Y t⋆ −1|Yt( ·|x t)
=
h (cid:0)
p (x x
)logp Y t⋆ −1|Yt((cid:1)xi t −1 |x t) dxh (cid:0)
q (x )dx
(cid:1)i
Z (cid:20)Z
Xt−1 |Xt t −1 | t p
Yt−1
|Yt(x
t −1
|x t) t −1
(cid:21)
t t t
α x µ⋆(x ) 2 α x µ (x ) 2
= p Xt−1,Xt(x
t
−1,x t)
−
t k t −1 2− σ⋆2t t k2 + t k t −1 2− σ⋆2t t k2 dx
t
−1dx
t
Z (cid:18) t t (cid:19)
η⋆2 η⋆√α
= 2σt ⋆2E
xt∼qt
kε t(x t) k2
2
+ t
σ⋆2
t p Xt−1,Xt(x
t
−1,x t)(x
t −1
−µ⋆
t
(x t))⊤ε t(x t)dx
t
−1dx t,
t t Z
(cid:2) (cid:3)
=:Kt
where we define | {z }
x +η⋆s⋆(x ) x +η⋆s (x )
ε (x ):=s⋆(x ) s (x ), µ⋆(x ):= t t t t , and µ (x ):= t t t t . (4.6)
t t t t − t t t t √α t t √α
t t
It then boils down to bounding K , which is presented in the following lemma.
t
Lemma 7. Suppose that T k2log3T. Then we have
≫
k2log3T c logT
K 4C 1 E1/2 ε (x ) 2 .
| t |≤ 5 T
r
T xt∼qt k t t k2
Proof. See Appendix A.8. (cid:2) (cid:3)
Hence we know that for 2 t T,
≤ ≤
E xt∼qt KL p Xt−1 |Xt( ·|x t) kp Yt−1 |Yt( ·|x t) −E xt∼qt KL p Xt−1 |Xt( ·|x t) kp Y t⋆ −1|Yt( ·|x t)
(1(cid:2)(cid:2) α(cid:0))(1 α ) (cid:1)(cid:3)(cid:3) 1 α
k2h log3(cid:16)
T c logT
(cid:17)i
− t − t E ε (x ) 2 +4C − t 1 E1/2 ε (x ) 2
≤ 2(α
t
−α t) xt∼qt k t t k2 5 α
t
−α
t
T
r
T xt∼qt k t t k2
4c logT (cid:2) (cid:3) k2log3T c logT (cid:2) (cid:3)
1 E ε (x ) 2 +8C 1 E1/2 ε (x ) 2 . (4.7)
≤ T xt∼qt k t t k2 5 T
r
T xt∼qt k t t k2
(cid:2) (cid:3) (cid:2) (cid:3)
Here the firstrelationfollows fromLemma 7 and(2.4); while the second relationfollows fromLemma 8 and
holds provided that T is sufficiently large.
94.5 Step 5: putting everything together
By taking (4.5) and (4.7) collectively, we have
E KL p ( x ) p ( x )
xt∼qt Xt−1 |Xt
·|
t
k
Yt−1 |Yt
·|
t
3C(cid:2) 2k4(cid:0)log6T + 4c 1logT E ε (x(cid:1)(cid:3) ) 2 +8C k2log3T c 1logT E1/2 ε (x ) 2
≤ 5 T2 T xt∼qt k t t k2 5 T
r
T xt∼qt k t t k2
k4log6T 8c logT (cid:2) (cid:3) (cid:2) (cid:3)
7C2 + 1 E ε (x ) 2 . (4.8)
≤ 5 T2 T xt∼qt k t t k2
(cid:2) (cid:3)
Here the last relation follows from an application of the AM-GM inequality
k2log3T c logT 4c logT k4log6T
8C 1 E1/2 ε (x ) 2 1 E ε (x ) 2 +4C2 .
5 T
r
T xt∼qt k t t k2 ≤ T xt∼qt k t t k2 5 T2
(cid:2) (cid:3) (cid:2) (cid:3)
Finally we conclude that
T
TV2(q ,p ) KL(p p )+ E KL p ( x ) p ( x )
1 1
≤
XTk YT xt∼qt Xt−1 |Xt
·|
t
k
Yt−1 |Yt
·|
t
t=2
X (cid:2) (cid:0) (cid:1)(cid:3)
k4log6T 8c logT T
8C2 + 1 E ε (x ) 2 ,
≤ 5 T T xt∼qt k t t k2
t=2
X (cid:2) (cid:3)
as claimed. Here the first relation follows from (3.2), while the second relation follows from the fact that
KL(p p ) T 100 provided that T is sufficiently large (see Lemma 10).
XTk YT
≤
−
5 Discussion
The present paper investigates the DDPM sampler when the target distribution is concentrated on or near
low-dimensional manifolds. We identify a particular coefficient design that enables the adaptivity of the
DDPM sampler to unknown low-dimensional structures and establish a dimension-free convergence rate at
the order of k2/√T (up to logarithmic factors). We conclude this paper by pointing out several directions
worthyoffutureinvestigation. Tobeginwith,ourtheoryyieldsaniterationcomplexitythatscalesquartically
in the intrinsic dimension k, which is likely sub-optimal. Improving this dependency calls for more refined
analysis tools. Furthermore, as we have discussed in the end of Section 3.2, it is not clear whether our
coefficient design (2.4) is unique in terms of achieving dimension-independent error TV(q ,p ). Finally, the
1 1
analysis ideas and tools developed for the DDPM sampler might be extended to study another popular
DDIM sampler.
Acknowledgements
Gen Liis supportedin partby the Chinese Universityof HongKongDirect Grantfor Research. Yuling Yan
is supported in part by a Norbert Wiener Postdoctoral Fellowship from MIT.
A Proof of auxiliary lemmas for the DDPM sampler
A.1 Preliminaries
Fix any x , there exists an index i(x ) , two points x (x ) and ω such that
t ∈Tt t
∈I
0 t ∈Bi(xt)
∈G
x =√α x (x )+√1 α ω. (A.1)
t t 0 t t
−
For any r >0, define a set
(x ;r):= 1 i N :α x⋆ x⋆ 2 r k(1 α )logT . (A.2)
I t ≤ ≤ ε t k i − i(xt)k2 ≤ · − t
n o
10For some sufficiently large constant C >0, define
3
(x ):= and (x ):= .
t t i t t i
X B Y B
i ∈I([xt;C3) i ∈/ I([xt;C3)
Namely, (x )(resp. (x )) containsthe indices ofthe ε-coveringthatareclose(resp.far)from . We
Xt t Yt t Bi(xt)
require that
1 α klogT
t
ε − min 1, , (A.3)
≪ r α t ( r d )
whichisguaranteedbyourassumptionthatε=T −cε forsomesufficiently largeconstantc
ε
>0. Under this
condition, for any x,x (x ) we have
′ t t
∈X
C k(1 α )logT C k(1 α )logT
kx −x ′ k2 ≤kx −x⋆ i(xt)k2+ kx ′ −x⋆ i(xt)k2 ≤2
s
3 −
α
tt +2ε ≤3
s
3 −
α
tt
Hence for any x,x (x ) we have
′ t t
∈X
α x x 2 9C k(1 α )logT. (A.4)
t k − ′ k2 ≤ 3 − t
In addition, for any x,x , suppose that x and x . For any ω , we have
′ i ′ j
∈X ∈B ∈B ∈G
ω (x x) = ω x⋆ x⋆ + ω (x x⋆) + ω x⋆ x
⊤ − ′ ⊤ i − j ⊤ − i ⊤ j − ′
(cid:12) (cid:12) (cid:12) (cid:12)( ≤i)(cid:12) (cid:12) C 1(cid:0) klogT k(cid:1) x(cid:12) (cid:12)⋆
i
−(cid:12) (cid:12)x⋆ jk2+ kx −(cid:12) (cid:12)x⋆ ik(cid:12) (cid:12) 2kω(cid:0) k2+ x(cid:1) ′(cid:12) (cid:12) −x⋆
j
2kω
k2
(ii)p (cid:13) (cid:13)
≤
C 1klogT kx⋆
i
−x⋆ jk2+2 2√d+ C 1klo(cid:13)gT ε (cid:13)
(iii)p (cid:16) p (cid:17)
C klogT x x +2 C klogTε+2 2√d+ C klogT ε
1 ′ 2 1 1
≤ k − k
pC klogT x x + 4√pd+4 C klogT(cid:16) ε. p (cid:17) (A.5)
1 ′ 2 1
≤ k − k
Here step (i) follows from ωp and the Cauchy-Sc(cid:0)hwarz inp equality; ste(cid:1)ps (ii) and (iii) follows from x
∈ G k −
x⋆ ε and x x⋆ ε, as well as ω √d+√C klogT, which is a property for ω .
ik2
≤ k
′
−
jk2
≤ k
k2
≤
1
∈G
A.2 Understanding the conditional density p ( x )
Xt|X0
·|
0
Conditional on X =x , for any 1 i N we have
t t ε
≤ ≤
P(X ,X =x ) P(X ,X =x )
P(X X =x )= 0 ∈Bi t t = 0 ∈Bi t t
0 ∈Bi | t t p (x ) P(X ,X =x )
Xt t 1 ≤j ≤Nε 0 ∈Bj t t
P(X ,X =x ) P(X )P(X =x X )
0 i t t P 0 i t t 0 i
∈B = ∈B | ∈B
≤ P X ,X =x P X P X =x X
0 ∈Bi(xt) t t 0 ∈Bi(xt) t t
|
0 ∈Bi(xt)
P(X =x X )
ex(cid:0)p(C klogT) (cid:1)t t |(cid:0) 0 ∈Bi (cid:1)P((cid:0)X ). (cid:1) (A.6)
≤ 1 · P X =x X · 0 ∈Bi
t t
|
0 ∈Bi(xt)
Here the last relation follows from P(X ) (cid:0) exp( C klogT) du(cid:1) e to i(x ) . We have
0 ∈Bi(xt)
≥ −
1 t
∈I
P(X =x ,X ) 1
P(X =x X )= t t 0 ∈Bi = P(X =x ,X =x˜)dx˜
t t | 0 ∈Bi P(X ) P(X ) t t 0
0 ∈Bi 0 ∈Bi Zx˜ ∈Bi
1
= P(X =x X =x˜)P(X =x˜)dx˜
P(X ) t t | 0 0
0 ∈Bi Zx˜ ∈Bi
max P(X =x X =x˜). (A.7)
t t 0
≤x˜
∈Bi
|
11For any x˜ , since X X =x (√α x˜,(1 α )I ), we have
i t 0 t t d
∈B | ∼N −
x √α x˜ 2
P(X
t
=x
t
X
0
=ex˜)=[2π(1 α t)]−d/2exp k t − t k2
| − − 2(1 α )
(cid:18) − t (cid:19)
( x √α x⋆ √α ε)2
[2π(1 α t)]−d/2exp k t − t ik2 − t . (A.8)
≤ − − 2(1 α )
(cid:18) − t (cid:19)
Taking (A.7) and (A.8) collectively to achieve
( x √α x⋆ √α ε)2
P(X
t
=x
t
X
0
i) [2π(1 α t)]−d/2exp k t − t ik2 − t . (A.9)
| ∈B ≤ − − 2(1 α )
(cid:18) − t (cid:19)
By similar argument in (A.7), (A.8) and (A.9), we can show that
( x √α x⋆ +√α ε)2
P X
t
=x
t
|X
0 ∈Bi(xt)
≥[2π(1 −α t)]−d/2exp
−
k t − 2t (1i(xt) αk t2
)
t !. (A.10)
−
(cid:0) (cid:1)
Combine (A.9) and (A.10) to achieve
P(X t =x t |X 0 ∈Bi)
exp
( kx t −√α tx⋆ ik2 −√α tε)2
+
( kx t −√α tx⋆ i(xt)k2+√α tε)2
P X t =x t |X 0 ∈Bi(xt) ≤ "− 2(1 −α t) 2(1 −α t) #
(cid:0)
exp
kx t −√α tx(cid:1)⋆ ik2 2−kx t −√α tx⋆ i(xt)k2 2−2√α tε( kx t −√α tx⋆ ik2+ kx t −√α tx⋆ i(xt)k2)
. (A.11)
≤ "− 2(1 α t) #
−
Next, we will discuss the implication of the above analysis for (x ) and (x ) respectively.
i t t i t t
B ⊆X B ⊆Y
A.2.1 Case 1: (x )
i t t
B ⊆X
For any i (x ;0,C ), we have
t 3
∈I
x √α x⋆ 2 x √α x⋆ 2 =α x⋆ x⋆ 2+2√α (x⋆ x⋆) (x √α x⋆ )
k t − t ik2−k t − t i(xt)k2 t k i(xt)− ik2 t i(xt)− i ⊤ t − t i(xt)
( =i) α
t
kx⋆ i(xt)−x⋆ ik2 2+2 α t(1 −α t)(x⋆ i(xt)−x⋆ i)⊤ω+2α t√1 −α t(x⋆ i(xt)−x⋆ i)⊤(x 0(x t) −x⋆ i(xt))
(ii) p
α x⋆ x⋆ 2 2 C α (1 α ) klogT x⋆ x⋆ 2α √1 α ε x⋆ x⋆
≥ t k i(xt)− ik2− 1 t − t k i(xt)− ik2 − t − t k i(xt)− ik2
(iii) p p
C k(1 α )logT 2(1 α )( C klogT +√α ε) C klogT
3 t t 1 t 3
≥ − − −
(iv) p p
(C /2)k(1 α )logT. (A.12)
3 t
≥ −
Here step (i) uses the decomposition(A.1); step (ii) follows from ω , Cauchy-Schwarzinequality and the
∈G
factthat x (x ) x⋆ ε;step(iii)followsfromi (x ;0,C )andholdsprovidedthatC 1;while
k
0 t
−
i(xt)k2
≤ ∈I
t 3 3
≫
step (iv) holds when C C and ε 1. In view of the decomposition (A.1), we have
3 1
≫ ≤
x √α x⋆ + x √α x⋆ √α x (x ) x⋆ +√α x (x ) x⋆ +2√1 α ω
k
t
−
t ik2
k
t
−
t i(x0)k2
≤
t
k
0 t
−
ik2 t
k
0 t
−
i(xt)k2
−
t
k
k2
(a)
6 C k(1 α )logT +2√1 α 2√d+ C klogT
3 t t 1
≤ − −
(b) p (cid:16) p (cid:17)
√1 α 8 C klogT +4√d , (A.13)
t 3
≤ −
(cid:16) p (cid:17)
where step (a) follows from (A.4), as well as the facts that x (x ),x⋆,x⋆ (x ) and ε ; while step
0 t i i(xt) ∈ Xt t ∈ G
(b) holds provided that C C . By substituting the bounds (A.12) and (A.13) into (A.11), we have
3 1
≫
P(X =x X ) (C 3/2)k(1 α t)logT 2ε α t(1 α t) 8√C 3klogT +4√d
t t 0 i − − −
| ∈B exp
P(X
t
=x
t
|X
0
∈Bi(xt)) ≤ − 2p(1 −α t) (cid:16) (cid:17)
 
12exp( (C /8)klogT) (A.14)
3
≤ −
provided that (A.3) holds and C is sufficiently large. In view of (A.6) and (A.14), we know that for any
3
i (x ;0,C ), we have
t 3
∈I
P(X X =x ) exp(C klogT (C /8)klogT)P(X )
0 i t t 1 3 0 i
∈B | ≤ − ∈B
exp( (C /16)klogT)P(X ) (A.15)
3 0 i
≤ − ∈B
as long as C C . Therefore we have
3 1
≫
C
P(X / (x ) X =x )= P(X X =x ) exp 3 klogT .
0 t t t t 0 i t t
∈X | ∈B | ≤ −16
i ∈/ I(xXt;C3/2) (cid:18) (cid:19)
A.2.2 Case 2: (x )
i t t
B ⊆Y
For any i / (x ;C ), we have
t 3
∈I
x √α x⋆ 2 x √α x⋆ 2
k t − t ik2−k t − t i(xt)k2
(i)
α x⋆ x⋆ 2 2 C α (1 α ) klogT x⋆ x⋆ 2α √1 α ε x⋆ x⋆
≥ t k i(xt)− ik2− 1 t − t k i(xt)− ik2 − t − t k i(xt)− ik2
(ii) 1 p p
α x⋆ x⋆ 2. (A.16)
≥ 2 t k i(xt)− ik2
Here step (i) follows from an intermediate step of (A.12), while the correctness of step (ii) is equivalent to
√α x⋆ x⋆ 4 C (1 α ) klogT +4 α (1 α )ε,
t k i(xt)− ik2 ≥ 1 − t t − t
p p p
which follows from i / (x ;C ), the assumptions that C C , and (A.3). In addition, we also have
t 3 3 1
∈I ≫
(i)
x √α x⋆ + x √α x⋆ √α x (x ) x⋆ +√α x (x ) x⋆ +2√1 α ω
k
t
−
t ik2
k
t
−
t i(x0)k2
≤
t
k
0 t
−
ik2 t
k
0 t
−
i(xt)k2
−
t
k
k2
(ii)
√α x⋆ x⋆ +2√α x (x ) x⋆ +2√1 α 2√d+ C klogT
≤ t k i(xt)− ik2 t k 0 t − i(xt)k2 − t 1
(iii) (cid:16) p (cid:17)
√α x⋆ x⋆ +3√1 α 2√d+ C klogT . (A.17)
≤ t k i(xt)− ik2 − t 1
(cid:16) p (cid:17)
Here step (i) follows from the intermediate step of (A.13); step (ii) utilizes the triangle inequality; whereas
step (iii) follows from x (x ) x⋆ ε and the condition (A.3). Similar to (A.14), we can substitute
k
0 t
−
i(xt)k2
≤
the bounds (A.16) and (A.17) into (A.11) to get
P(X =x X ) α
t t | 0 ∈Bi exp t x⋆ x⋆ 2 . (A.18)
P(X =x X ) ≤ −8(1 α )k i(xt)− ik2
t t | 0 ∈Bi(xt) (cid:18) − t (cid:19)
Since i / (x ;C ), we know that α x⋆ x⋆ 2 >C k(1 α )logT, hence when C C , we learnfrom
∈I t 3 t k i(xt)− ik2 3 − t 3 ≫ 1
(A.6) and (A.18) that
α
P(X X =x ) exp C klogT t x⋆ x⋆ 2 P(X )
0 ∈Bi | t t ≤ 1 − 8(1 α )k i(xt)− ik2 0 ∈Bi
(cid:18) − t (cid:19)
α
exp t x⋆ x⋆ 2 P(X ). (A.19)
≤ −16(1 α )k i(xt)− ik2 0 ∈Bi
(cid:18) − t (cid:19)
A.3 Proof of Lemma 1
It is straightforwardto check that
P((X ,X ) )( =i)P(X ,W )(ii) P X ,W ,W ,
t t 1 t t t t 0 i i t t
− ∈A ∈T ∈G ≤ ∈∪∈IB ∈G ∈G
(cid:0) (cid:1)
13where step (i) follows from the update rule (2.1), and step (ii) follows from the relation (2.2). Therefore we
have
P((X ,X ) / ) P(X / )+P(W / )+P W / . (A.20)
t t 1 t 0 i i t t
− ∈A ≤ ∈∪∈IB ∈G ∈G
By definition of the set , we have (cid:0) (cid:1)
I
1 C
P(X
0
/
i
i) N εexp( C 1klogT) exp(CcoverklogT C 1klogT) exp 1 klogT (A.21)
∈∪∈IB ≤ − ≤ − ≤ 3 − 4
(cid:18) (cid:19)
as long as C
1
Ccover. In addition, since W t,W
t
(0,I d), by the definition of we know that
≫ ∼N G
Nε Nε
P(W / ) P W >√d+ C klogT + P (x⋆ x⋆) W > C klogT x⋆ x⋆
t ∈G ≤ k t k2 1 | i − j ⊤ t | 1 k i − jk2
(cid:16) p (cid:17) Xi=1 Xj=1 (cid:16) p (cid:17)
(i) C C
≤
N ε2+1 exp
−
21 klogT ≤(exp(2CcoverklogT)+1)exp
−
21 klogT
(cid:18) (cid:19) (cid:18) (cid:19)
(ii)(cid:0)
1
(cid:1)
C
1
exp klogT (A.22)
≤ 3 − 4
(cid:18) (cid:19)
Here step (i) follows from concentrationbounds for Gaussianand chi-squarevariables (see Lemma 9); while
step (ii) holds as long as C
1
Ccover. The same bound also holds for P(W
t
/ ). Taking (A.20), (A.21)
≫ ∈ G
and (A.22) collectively yields
C
P((X ,X ) / ) exp 1 klogT
t t 1 t
− ∈A ≤ − 4
(cid:18) (cid:19)
as claimed.
A.4 Proof of Lemma 2
For any deterministic pairs (x ,x ), we have
t t 1
−
1 p (x )
p Xt−1 |Xt(x t −1 |x t)= p Xt(x t)p Xt−1,Xt(x t −1,x t)= X pt− X1 t(xt t− )1 p Xt|Xt−1(x t |x t −1). (A.23)
Recall that X X =x (√α x ,(1 α )I ), therefore we have
t t 1 t 1 t t 1 t d
| − − ∼N − −
1
p Xt|Xt−1(x
t
|x
t
−1)=[2π(1 −α t)]−d/2exp
(cid:18)−2(1 −α
t)kx
t
−√α tx
t −1
k2
2
(cid:19). (A.24)
Next, we analyze the density ratio p (x )/p (x ). It would be easier to do a change of variable
Xt−1 t −1 Xt t
p (x )=αd/2p (√α x ). (A.25)
Xt−1 t −1 t √αtXt−1 t t −1
Since √α tX
t 1
X
0
=x
0
(√α tx 0,(α
t
α t)I d), we can write
− | ∼N −
p √αtXt p− X1 t(cid:0)(√
x
tα )tx t −1
(cid:1)
=
p
Xt1
(x t)
Zx0p X0(x 0)p
√αtXt−1
|X0(√α tx
t −1
|x 0)dx
0
=
p
1
(x )
p X0(x 0)[2π(α t −α t)]−d/2exp
−k√α tx
2t (− α1
−√ αα )tx
0
k2
2 dx 0. (A.26)
Xt t Zx0 (cid:18) t − t (cid:19)
We hope to connect the above quantity with the conditional density
p (x ,x ) p (x )
p (x x )=
X0,Xt 0 t
=
X0 0
p (x x )
X0 |Xt 0 | t p Xt(x t) p Xt(x t) Xt|X0 t | 0
p (x ) 1 x √α x 2
= X0 0 exp k t − t 0 k2 . (A.27)
p Xt(x t) [2π(1 −α t)]d/2 (cid:18)− 2(1 −α t)
(cid:19)
14Towards this, we can deduce that
p √αtXt−1 √α tx t −1 ( =i) 1 −α t d/2 p X0(x 0) [2π(1 α t)]−d/2exp k√α tx t −1 −√α tx 0 k2 2 dx
0
p (x ) α α p (x ) − − 2(α α )
Xt(cid:0) t (cid:1) (cid:18) t − t(cid:19) Zx0 Xt t (cid:18) t − t (cid:19)
( =ii) (cid:18)α1 t− −α αt t(cid:19)d/2 Zx0p X0 |Xt(x 0 |x t)exp (cid:18)kx t 2− (1√ −α αtx t)0 k2 2 − k√α tx 2t (− α1 t− −√ α tα )tx 0 k2 2 (cid:19)dx 0, (A.28)
wherestep(i)followsfrom(A.26)andstep(ii)utilizes(A.27). Thetermsintheexponentcanberearranged
into
x √α x 2 √α x √α x 2 x √α x 2 √α x √α x 2 (1 α ) x √α x 2
k t − t 0 k2 k t t −1 − t 0 k2 = k t − t 0 k2−k t t −1 − t 0 k2 − t k t − t 0 k2
2(1 α ) − 2(α α ) 2(α α ) − 2(α α )(1 α )
t t t t t t t t
− − − − −
= k√α tx t −1 −x t k2 2+2 √α tx t −1 −x t ⊤ x t −√α tx 0 (1 −α t) kx t −√α tx 0 k2 2
− 2(α α ) − 2(α α )(1 α )
(cid:0) t − t (cid:1) (cid:0) (cid:1) t − t − t
= −k√α tx t −1 −x t k2 2+2 2√ (αα tx t α−1 )−x t ⊤ x t −√α tx 0
−
(1 2− (αα t) kx αt )− (1√α t αx 0 )k2 2 +∆ xt,xt−1(x 0)
(cid:0) t − t (cid:1) (cid:0) (cid:1) t − t − t
b b
where we define
x :=E[X X =x ]= x p (x x )dx ,
0 0
|
t t
Zx0
0 X0 |Xt 0
|
t 0
and
b
√α (1 α )√α
∆ xt,xt−1(x 0):=
−α
t
t
α
t
(√α tx
t −1
−x t)⊤(x
0
−x 0)
− (α
t
−
α
t)t
(1
t
α t)
x
t
−√α tx
0
⊤(x
0
−x 0)
− − −
(1 α )α (cid:0) (cid:1)
− t t x bx 2. b b
− 2(α α )(1 α )k 0 − 0 k2
t t t
− −
Substituting the above relation into (A.28) yibelds
p √αtXt−1 √α tx t −1 = 1 −α t d/2 exp k√α tx t −1 −x t k2 2
p (x ) α α − 2(α α )
Xt(cid:0) t (cid:1) (cid:18) t − t(cid:19) (cid:20) t − t (cid:21)
exp √α tx t −1 −x t ⊤ x t −√α tx 0 (1 −α t) kx t −√α tx 0 k2 2
· "− (cid:0) α t(cid:1)−α (cid:0)t (cid:1) − 2(α t −α t)(1 −α t) #
b b
p (x x )exp ∆ (x ) dx . (A.29)
·
Zx0
X0 |Xt 0
|
t xt,xt−1 0 0
(cid:0) (cid:1)
Therefore we have
p
Xt−1
|Xt(x
t −1
|x t)( =i) αd t/2p √αtXt p− X1 t(cid:0)(√
x
tα )tx t −1 (cid:1)p Xt|Xt−1(x
t
|x
t
−1) (A.30)
( =ii) αd/2 1 −α t d/2 exp √α tx t −1 −x t ⊤ x t −√α tx 0 (1 −α t) kx t −√α tx 0 k2 2
t (cid:18)α t −α t(cid:19) − (cid:0) α t(cid:1)−α (cid:0)t (cid:1) − 2(α t −α t)(1 −α t) !
2 b b
·[2π(1 −α t)]−d/2exp −(1 − 2(α 1t)
−(cid:13)
(cid:13)αx t t)− (α√ tα −tx αt t− )1
(cid:13)
(cid:13)2
!·
Zx0p
X0
|Xt(x
0
|x t)exp (cid:0)∆ xt,xt−1(x 0) (cid:1)dx
0
( =iii) (2πα σd t t⋆/ 22
)d/2
exp (cid:18)−k√α tx t −1 − 2x σt t⋆−
2
η t⋆s⋆ t (x t) k2 2
(cid:19)·
Zx0p
X0
|Xt(x
0
|x t)exp ∆ xt,xt−1(x 0) dx 0.
(cid:0) (cid:1)
Here step (i) follows from (A.23) and (A.25); step (ii) follows from (A.24) and (A.29); whereas step (iii)
follows from the definition of η⋆ and σ⋆ (cf. (2.4)) as well as the fact that
t t
1 1
s⋆(x )= p (x x ) x √α x dx = x √α x .
t t −1 −α
t Zx0
X0 |Xt 0 | t − t 0 0 −1 −α
t
t − t 0
(cid:0) (cid:1) (cid:0) (cid:1)
b
15A.5 Proof of Lemma 3
For any(x t,x
t
1) t,we know thatω
′
:=(x
t
√α tx
t
1)/√1 α
t
. We willupper bound the integral
− ∈A − − − ∈G
with two terms
p (x x )exp(∆(x ,x ,x ))dx = p (x x )exp(∆(x ,x ,x ))dx
Zx0
X0 |Xt 0
|
t t t −1 0 0
ZXt(xt)
X0 |Xt 0
|
t t t −1 0 0
=:I1
+| p (x x{)zexp(∆(x ,x ,x ))d}x ,
ZYt(xt)
X0 |Xt 0
|
t t t −1 0 0
=:I2
where we recall that | {z }
α (1 α ) (1 α )√α
t t t t
∆(x t,x
t
1,x 0)= − (x
0
x 0)⊤ω′ − (x
0
x 0)⊤ω
− pα t −α t − −(α t −α t)√1 −α t −
=:∆1(x b0) =:∆2(x0)b
|
− (α
(1 α−{ )αz (t 1)α t
α
)(x 0(} x t)| −x 0)⊤(x
0
−x{ 0z ) −2(α(1 − αα )}t () 1α t
α
)kx
0
−x
0
k2 2.
t t t t t t
− − − −
=:∆3(x0) b b =:∆4(x0) b
Inwhatfollows,wewilluse|∆(x )insteadof∆(x{z,x ,x )whenther}eis|noconfusion.{Szince(x ,x )} ,
0 t t 1 0 t t 1 t
we know that ω
′
:=(x
t
√α tx
t
1)/√1 α
t
. W− e decompose x
0
into − ∈A
− − − ∈G
x 0 = Zx0x ′0p X0 |Xt(x 0 |x t)dx ′0 b
b =x⋆ + (x x⋆ )p (x x )dx + (x x⋆ )p (x x )dx . (A.31)
i(xt)
ZXt(xt)
′0− i(xt) X0 |Xt 0 | t ′0
ZYt(xt)
′0− i(xt) X0 |Xt ′0| t ′0
=:x0 =:δ
Since (x|) is a ball in Rd cent{ezred at x⋆ , it is stra}ight|forwardto check{tzhat x (x ). W} e also have
Xt t i(xt) 0 ∈Xt t
δ x x⋆ p (x x )dx
k
k2
≤ ZYt(xt)k
′0− i(xt)k2 X0 |Xt 0
|
t ′0
(i)
x⋆ x⋆ +ε P(X X =x )
≤ k i − i(xt)k2 0 ∈Bi | t t
i ∈/ IX(xt;C3)(cid:16) (cid:17)
(ii) α
x⋆ x⋆ +ε exp t x⋆ x⋆ 2 P(X ).
≤ k i − i(xt)k2 −16(1 α )k i(xt)− ik2 0 ∈Bi
i ∈/ IX(xt;C3)(cid:16) (cid:17) (cid:18) − t (cid:19)
Here step (i) holds since for any i / (x ;C ) and x ,
∈I
t 3 ′0 ∈Bi
x x⋆ x⋆ x⋆ + x x⋆ x⋆ x⋆ +ε;
k ′0− i(xt)k2 ≤k i − i(xt)k2 k ′0− ik2 ≤k i − i(xt)k2
whilestep(ii)followsfrom(A.19). Foranyi / (x ;C ),weknowthatα x⋆ x⋆ 2 >C k(1 α )logT,
∈I t 3 t k i(xt)− ik2 3 − t
hence we can check that
α C k(1 α )logT C klogT
x⋆ x⋆ +ε exp t x⋆ x⋆ 2 3 − t +ε exp 3
k i − i(xt)k2 (cid:18)−16(1 −α t)k i(xt)− ik2 (cid:19)≤ s α t ! (cid:18)− 16 (cid:19)
(cid:0) (cid:1)
1 α C klogT
t 3
− exp
≤ α − 32
r t (cid:18) (cid:19)
as long as C is sufficiently large and the condition (A.3) holds. Therefore
3
1 α C klogT 1 α C klogT
δ − t exp 3 P(X ) − t exp 3 . (A.32)
2 0 i
k k ≤ α − 32 ∈B ≤ α − 32
i ∈/ IX(xt;C3)r t (cid:18) (cid:19) r t (cid:18) (cid:19)
16A.5.1 Step 1: deriving an upper bound for ∆(x )
0
Suppose that x forsome 1 i N (notice that here we arenotrequiringthat i ). We will bound
0 i ε
∈B ≤ ≤ ∈I
each of ∆ (x ) for i = 1,2,3,4. We first record two basic facts about the step sizes, which are immediate
i 0
|
consequences of Lemma 8:
α (1 α ) α 1 α 1 α α 8c logT 8c logT
t t t t t t 1 1
− = 1+ − − 1+
α α 1 α α α α α ≤ 1 α T T
p t − t r − tr t − tr t − t r − tr r
c logT α
1 t
3 , (A.33)
≤ T 1 α
r r − t
as long as T is sufficiently large, and
(1 α )√α 8c logT α
t t 1 t
− . (A.34)
(α α )√1 α ≤ T 1 α
t − t − t r − t
We learn from (A.5) that
max (x x ) ω , (x x ) ω C klogT x x + 4√d+4 C klogT ε. (A.35)
0 0 ⊤ 0 0 ⊤ ′ 1 0 0 2 1
− − ≤ k − k
We also ha(cid:8)ve(cid:12) (cid:12) (cid:12) (cid:12)(cid:9) p (cid:0) p (cid:1)
(cid:12) b (cid:12) (cid:12) b (cid:12) b
(i)
x x x x + δ x⋆ x⋆ + x⋆ x +ε+ δ
k 0 − 0 k2 ≤k 0 − 0 k2 k k2 ≤ k i(xt)− ik2 k i(xt)− 0 k2 k k2
(ii) C k(1 α )logT 1 α C klogT
b x⋆ x⋆ +3 3 − t +ε+ − t exp 3
≤ k i(xt)− ik2 s α t r α t (cid:18)− 32 (cid:19)
(iii) C k(1 α )logT
x⋆ x⋆ +4 3 − t . (A.36)
≤ k i(xt)− ik2 s α t
Here step (i) holds since x , hence x x⋆ ε; step (ii) follows from (A.4) and the fact that
0
∈
Bi
k
0
−
ik2
≤
x⋆ ,x (x ); while step (iii) follows from (A.3) and holds provided that C is sufficiently large. Then
i(xt) 0 ∈Xt t 3
we have
α (1 α )
t t
∆ 1(x 0) − (x
0
x 0) ⊤ω
| |≤ α α −
p t − t
(cid:12) (cid:12)
(a) 3 c 1logT (cid:12) bα t C(cid:12) klogT x x + 4√d+4 C klogT ε
1 0 0 2 1
≤ T 1 α k − k
r r − t (cid:16)p (cid:0) p (cid:1) (cid:17)
(b) c logT α 1 α
4 1 t C klogT bx⋆ x⋆ +4 C C klogT − t . (A.37a)
≤ T 1 α 1 k i(xt)− ik2 1 3 α
r r − t (cid:18) r t (cid:19)
p p
Here step (a) follows from (A.33) and (A.35); while step (b) utilizes (A.36), (A.3). Similarly we can use
(A.34) to show that
9c logT α 1 α
∆ (x ) 1 t C klogT x⋆ x⋆ +4 C C klogT − t . (A.37b)
| 2 0 |≤ T 1 α 1 k i(xt)− ik2 1 3 α
r − t (cid:18) r t (cid:19)
p p
Notice that
(i)
(x (x ) x ) (x x ) x (x ) x x x ( x (x ) x + δ ) x x
0 t − 0 ⊤ 0 − 0 ≤ k 0 t − 0 k2k 0 − 0 k2 ≤ k 0 t − 0 k2 k k2 k 0 − 0 k2
(cid:12) (cid:12)(ii) C k(1 α )logT 1 α C klogT
(cid:12) b b (cid:12) 3 3 b− t b + − t exp 3 b x x
≤ " s α t r α t (cid:18)− 32 (cid:19)#k 0 − 0 k2
(iii) C k(1 α )logT b
3 t
4 − x x ,
≤ s α t k 0 − 0 k2
b
17where step (i) utilizes the Cauchy-Schwarz inequality; step (ii) follows from (A.4), (A.32) and the fact that
x (x ),x (x ); step (iii) holds provided that C is sufficiently large. Therefore we have
0 t 0 t t 3
∈X
(1 α )α
t t
∆ 3(x 0) − (x 0(x t) x 0) ⊤(x
0
x 0)
| |≤ (α α )(1 α ) − −
t t t
− −
(cid:12) (cid:12)
(1 α t)α
t
(cid:12) C 3k(1b α t)blogT (cid:12)
− 4 − x x
≤ (α t α t)(1 α t) · s α t k 0 − 0 k2
− −
logT α b C k(1 α )logT
32c C t klogT x⋆ x⋆ +4 3 − t , (A.37c)
≤ 1 3 T r1 −α t k i(xt)− ik2 s α t 
p p
 
where the last relation follows from Lemma 8 and (A.36). Finally we have
(1 α )α
∆ (x ) − t t x x 2
| 4 0 |≤ 2(α α )(1 α )k 0 − 0 k2
t t t
− −
8c logT α C k(1 α )logT
1 t x⋆b x⋆ 2+16 3 − t . (A.37d)
≤ T 1 α k i(xt)− ik2 α
− t (cid:18) t (cid:19)
Herestep(a)followsfrom(A.36),step(b) followsfrom(A.4)andthe factthatx⋆ ,x⋆ (x )Takingthe
i(xt) i ∈Xt t
bounds in (A.37) collectively leads to
k α
∆(x ) 5 c C logT t x⋆ x⋆ +4 C klogT
| 0 |≤ 1 1 T 1 α k i(xt)− ik2 3
r (cid:18)r − t (cid:19)
p 8c logT α p
+ 1 t x⋆ x⋆ 2. (A.38)
T 1 α k i(xt)− ik2
t
−
provided that T is sufficiently large.
A.5.2 Step 2: bounding I
1
For x (x ), we know that x for some i (x ;C ), hence α x⋆ x⋆ 2 C k(1 α )logT.
0 ∈ Xt t 0 ∈ Bi ∈ I t 3 t k i − i(xt)k2 ≤ 3 − t
This combined with (A.38) gives
k2log3T 8c C klog2T k2log3T
1 3
∆(x ) 25 c C C + 26 c C C (A.39)
0 1 1 3 1 1 3
| |≤ s T T ≤ s T
p p
provided that T k2log3T. Similarly we can check that for each 1 i 4, ∆ (x ) 1. Then we know
i 0
that for x
(≫
x ), we have exp(∆(x )) 1+∆(x )+∆2(x ) as
lon≤
g
a≤
s T
| k2log|3≤
T. Hence
0 t t 0 0 0
∈X ≤ ≫
I 1+ p (x x )∆(x )dx + p (x x )∆2(x )dx
1
≤
ZXt(xt)
X0 |Xt 0
|
t 0 0
ZXt(xt)
X0 |Xt 0
|
t 0 0
=1+ p (x x )∆ (x )dx p (x x )∆ (x )dx
Z
X0 |Xt 0
|
t 1 0 0
−
ZYt(xt)
X0 |Xt 0
|
t 1 0 0
+ p (x x )[∆ (x )+∆ (x )+∆ (x )]dx + p (x x )∆2(x )dx
ZXt(xt)
X0 |Xt 0
|
t 2 0 3 0 4 0 0
ZXt(xt)
X0 |Xt 0
|
t 0 0
1+ p (x x ) ∆ (x ) dx
≤
ZYt(xt)
X0 |Xt 0
|
t
|
1 0
|
0
=:I1,1
+| p (x{zx ) ∆ (x )+}∆ (x )+∆ (x ) +∆2(x ) dx .
ZXt(xt)
X0 |Xt 0
|
t
(cid:2)|
2 0 3 0 4 0
|
0
(cid:3)
0
=:I1,2
| {z }
18Here the last step follows from the fact that p (x x )∆ (x )dx =0. The integralI can be upper
X0 |Xt 0
|
t 1 0 0 1,1
bounded similar to I , hence we defer its analysis to the next section. For the integral I , we have
2 1,2
R
I max ∆ (x )+∆ (x )+∆ (x ) +∆2(x )
1,2 2 0 3 0 4 0 0
≤x0 ∈Xt(xt) | |
(cid:8) (cid:9)
(i)
max 4∆2(x )+5 ∆ (x ) +5 ∆ (x ) +5 ∆ (x )
≤ x0 ∈Xt(xt) 1 0 | 2 0 | | 3 0 | | 4 0 |
(cid:8) (cid:9)
(ii) c logT α C k(1 α )logT 1 α
128 1 t C klogT 3 − t +16C C k2log2T − t
1 1 3
≤ T 1 α α α
− t (cid:18) t t (cid:19)
45c logT α C k(1 α )logT 1 α
1 t 3 t t
+ C klogT − +4 C C klogT −
1 1 3
T r1 −α t  s α t r α t 
p p
 
logT α C k(1 α )logT C k(1 α )logT
t 3 t 3 t
+160c C klogT − +4 −
1 3
T r1 −α t s α t s α t 
p p
40c logT α C k(1 α )logT C k(1 α )logT 
1 t 3 t 3 t
+ − +16 −
T 1 α α α
− t (cid:18) t t (cid:19)
k2log3T
2181c C C . (A.40)
1 1 3
≤ T
Herestep(i)followsfromtheCauchy-Schwarzinequalityandthefactsthat ∆ (x ) 1fori=2,3,4,while
i 0
| |≤
step (ii) follows from the bounds (A.37) and the fact that α x⋆ x⋆ 2 C k(1 α )logT.
t k i − i(xt)k2 ≤ 3 − t
A.5.3 Step 3: bounding I .
2
For x (x ), we know that x for some i / (x ;C ), hence α x⋆ x⋆ 2 > C k(1 α )logT.
0 ∈ Yt t 0 ∈ Bi ∈ I t 3 t k i − i(xt)k2 3 − t
This combined with (A.38) gives
c C logT c C logT 8c logT α
∆(x ) 5 1 1 +20 1 1 + 1 t x⋆ x⋆ 2
| 0 |≤
r
C
3 r
T
r
C
3 r
T T !1 −α tk i(xt)− ik2
c C logT α
25 1 1 t x⋆ x⋆ 2 (A.41)
≤ C T 1 α k i(xt)− ik2
r 3 r − t
as long as T is sufficiently large. Therefore we have
I = p (x x )exp(∆(x ))dx P(X X =x ) max exp(∆(x ))
2
ZYt(xt)
X0 |Xt 0
|
t 0 0
≤
i ∈/ IX(xt;C3)
0 ∈Bi
|
t t
x0 ∈Bi
0
(i) α c C logT α
exp t x⋆ x⋆ 2+25 1 1 t x⋆ x⋆ 2 P(X )
≤ i ∈/ IX(xt;C3) −16(1 −α t)k i(xt)− ik2 r C 3 r T 1 −α tk i(xt)− ik2 ! 0 ∈Bi
(ii) α
exp t x⋆ x⋆ 2 P(X )
≤ −32(1 α )k i(xt)− ik2 0 ∈Bi
i ∈/ IX(xt;C3) (cid:18) − t (cid:19)
(iii) C
3
exp klogT . (A.42)
≤ −32
(cid:18) (cid:19)
Here step (i) follows from (A.19) and (A.41); step (ii) holds as long as T is sufficiently large;while step (iii)
uses the fact that α x⋆ x⋆ 2 > C k(1 α )logT for i / (x ;C ). By similar analysis, we can show
t k i − i(xt)k2 3 − t ∈ I t 3
that
I = p (x x ) ∆ (x ) dx P(X X =x ) max ∆ (x )
1,1
ZYt(xt)
X0 |Xt 0
|
t
|
1 0
|
0
≤
i ∈/ IX(xt;C3)
0 ∈Bi
|
t t
x0 ∈Bi|
1 0
|
19(a) c C klog2T α
20 1 1 t P(X X =x ) x⋆ x⋆
≤ s T 1 α 0 ∈Bi | t t k i(xt)− ik2
r − t i ∈/ IX(xt;C3)
(b) c C klog2T α α
20 1 1 t exp t x⋆ x⋆ 2 P(X ) x⋆ x⋆
≤ s T 1 α −16(1 α )k i(xt)− ik2 0 ∈Bi k i(xt)− ik2
r − t i ∈/ IX(xt;C3) (cid:18) − t (cid:19)
(c) c C klog2T α
20 1 1 exp t x⋆ x⋆ 2 P(X )
≤ s T −32(1 α )k i(xt)− ik2 0 ∈Bi
i ∈/ IX(xt;C3) (cid:18) − t (cid:19)
(d) c C klog2T C
1 1 3
20 exp klogT . (A.43)
≤ s T −32
(cid:18) (cid:19)
Here step (a) follows from (A.37a) and the fact that α x⋆ x⋆ 2 >C k(1 α )logT for i / (x ;C );
t k i − i(xt)k2 3 − t ∈I t 3
step (b) followsfrom (A.19); step(c) holds providedthat C is sufficiently large;step(d) followsagainfrom
3
the fact that α x⋆ x⋆ 2 >C k(1 α )logT for i / (x ;C ).
t k i − i(xt)k2 3 − t ∈I t 3
A.5.4 Step 4: putting everything together
Taking (A.40), (A.42) and (A.43) collectively, we have
p (x x )exp(∆(x ,x ,x ))dx =I +I 1+I +I +I
Zx0
X0 |Xt 0
|
t t t −1 0 0 1 2
≤
1,1 1,2 2
k2log3T
1+2182c C C ,
1 1 3
≤ T
provided that T is sufficiently large. By similar argument, i.e., using the lower bounding exp(∆(x ))
0
≥
1+∆(x ) ∆2(x ) in Step 2 and repeat the same analysis, we can show that
0 0
−
k2log3T
p (x x )exp(∆(x ))dx 1 2182c C C .
Zx0
X0 |Xt 0 | t 0 0 ≥ − 1 1 3 T
This gives the desired result.
A.6 Proof of Lemma 4
Recall that
p (x x )
log pX Y tt ⋆ −− 11 || YX tt (x tt −− 11 || x tt ) =log (cid:20)Zx0p X0 |Xt(x 0 |x t)exp(∆(x t,x t −1,x 0))dx 0 (cid:21).
For any x , by the definition of ∆(x ,x ,x ) in (4.4), we have
0 t t 1 0
∈X −
√α (1 α )√α
∆(x ,x ,x ) t √α x x x x + − t t x √α x x x
t t 1 0 t t 1 t 2 0 0 2 t t 0 2 0 0 2
| − |≤ α t α tk − − k k − k (α t α t)(1 α t)k − k k − k
− − −
(1 α )α
+ − t t xb x 2 b b
2(α α )(1 α )k 0 − 0 k2
t t t
− −
(i) √α (1 α )√α (1 α )α
2R t √α x xb +2R − t t x + − t t 4R2
t t 1 t 2 t 2
≤ α t α tk − − k (α t α t)(1 α t)k k (α t α t)(1 α t)
− − − − −
(ii)
4RTc0 √α tx
t 1
x
t
2+16c 1RTc0 −1logT x
t
2+32c 1R2Tc0 −1logT.
≤ k − − k k k
Here step (i) follows from x ,x , hence max x , x R; while step (ii) follows from the facts
0 0 0 2 0 2
∈ X {k k k k } ≤
that, for 2 t T,
≤ ≤
√αb
1
b
1 2
t = 2Tc0,
α t −α t ≤ α t − t i=1α i α t 1 − t i=− 11α i ≤ 1 −α 1 ≤
Q (cid:16) Q (cid:17)
20and in view of Lemma 8,
(1 α )α (1 α )√α 8c logT 1
− t t − t t 1 8c 1Tc0 −1logT.
(α α )(1 α ) ≤ (α α )(1 α ) ≤ T 1 α ≤
t t t t t t t
− − − − −
Hence we have
p (x x )
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)log pX Yt t⋆− −1 1| |X Ytt (x tt −− 11 || x tt ) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)≤= 4(cid:12) (cid:12) (cid:12) (cid:12)l Rog T(cid:20) cZ 0 kx √0p αX t0 x| tX −t 1( −x 0 x| tx kt 2) +ex 1p 6( c∆ 1R(x Tt c, 0x −t 1− l1 o, gx T0) k) xd tx k0 2(cid:21) +(cid:12) (cid:12) (cid:12) (cid:12)≤ 32x cs 0 1u ∈ Rp X 2T|∆ c0( −x 1t, lox gt − T1,x 0) |
Tc0+2cR( √α x x + x +1)
t t 1 t 2 t 2
≤ k − − k k k
as long as T is sufficiently large.
A.7 Proof of Lemma 5
Regarding ∆ , we first utilize Lemma 3 to show that for any (x ,x ) ,
t,1 t t 1 t
− ∈A
1
p Y t⋆ −1|Yt(x t −1 |x t)
C
k2log3T
.
5
(cid:12)
(cid:12)
− p Xt−1 |Xt(x t −1 |x t)(cid:12) (cid:12)≤ T
(cid:12) (cid:12)
Since log(1 x) x x2 holds(cid:12) (cid:12)for any x [ 1/2,1/2],(cid:12) (cid:12)we know that when T k2log3T, we have
− ≥− − ∈ − ≫
p Xt−1 |Xt(x t −1 |x t)logp pX Y tt ⋆ −− 11 || YX tt (( xx tt −− 11 || xx tt )) = −p Xt−1 |Xt(x t −1 |x t)log "1 − 1 − pp XY t t⋆ − −1 1| |Y Xt t( (x xt t− −1 1| |x xt t) ) !#
2
p (x x ) 1
p Y t⋆ −1|Yt(x t −1 |x t)
+ 1
p Y t⋆ −1|Yt(x t −1 |x t)
≤ Xt−1 |Xt t −1 | t  − p Xt−1 |Xt(x t −1 |x t) − p Xt−1 |Xt(x t −1 |x t) ! 
 k4log6T 
=p Xt−1 |Xt(x t −1 |x t) −p Y t⋆ −1|Yt(x t −1 |x t)+p Xt−1 |Xt(x t −1 |x t)C 52 T2
Hence we have
k4log6T
∆ t,1 ≤
Z(xt,xt−1)
∈Ath−p Y t⋆ −1|Yt(x t −1 |x t)+p Xt−1 |Xt(x t −1 |x t) ip Xt(x t)dx t −1dx t+C 52 T2
k4log6T
=
Z(xt,xt−1) ∈Ac
thp Y t⋆ −1|Yt(x t −1 |x t) −p Xt−1 |Xt(x t −1 |x t) ip Xt(x t)dx t −1dx t+C 52 T2
k4log6T
≤ Z(xt,xt−1) ∈Ac tp Y t⋆ −1|Yt(x t −1 |x t)p Xt(x t)dx t −1dx t+C 52 T2 .
=:∆t,3
Here the penu|ltimate step follows fro{mz the fact that }
p Y t⋆ −1|Yt(x t −1 |x t)p Xt(x t)dx t −1dx t = p Xt−1 |Xt(x t −1 |x t)p Xt(x t)dx t −1dx t =1.
Z Z
It boils down to bounding ∆ . In view of (A.30), we know that
t,3
∆ t,3 = p Y t⋆ −1|Yt(x t −1 |x t)p Xt(x t)1 {x t ∈/ Tt }dx t −1dx t
Z
x √α x
+
Z
p Y t⋆ −1|Yt(x t −1 |x t)p Xt(x t)1 (cid:26)x t ∈Tt, t − √1 −t α tt −1 ∈/ G (cid:27)dx t −1dx t
X (X +η⋆s⋆(X )+σ⋆Z)
=P(X / )+P X , t − t t t t t / where Z (0,I )
t t t t d
∈T ∈T √1 α ∈G ∼N
(cid:18) − t (cid:19)
21η⋆s⋆(X )+σ⋆Z
=P(X / )+P X , t t t t /
t t t t
∈T ∈T − √1 α ∈G
(cid:18) − t (cid:19)
Here we use the fact that Y t⋆ −1|Y
t
=x
t
∼N
(x t+η t⋆s⋆ t(x t))/√α t,(σ t⋆2/α t)I
d
. Notice that
(cid:0) (cid:1)
η⋆s⋆(X )+σ⋆Z α α
t t t t = √1 α s⋆(X ) t − t Z.
− √1 α − − t t t − 1 α
− t r − t
The following claim is cricial for understanding this random variable.
Claim 1. For any x , we have
t t
∈T
1
√1 α s⋆(x ) √d+ C klogT ,
− t t t 2 ≤ 2 1
(cid:13) (cid:13) (cid:0) p (cid:1)
and for any 1 i j N , (cid:13) (cid:13)
ε
≤ ≤ ≤
1
√1 α (x⋆ x⋆) s⋆(x ) C klogT x⋆ x⋆ .
− t | i − j ⊤ t t |≤ 2 1 k i − jk2
p
Proof. See Appendix A.7.1.
Since Z (0,I ), in view of Lemma 9, with probability exceeding 1 exp( (C /64)klogT),
d 1
∼N − −
α α 1
t − t Z Z √d+ C klogT
1 α k k2 ≤k k2 ≤ 2 1
r − t
p
and for any 1 i j N ,
ε
≤ ≤ ≤
α α 1
1t − αt |(x⋆
i
−x⋆ j)⊤Z |≤|(x⋆
i
−x⋆ j)⊤Z
|≤ 2
C 1klogT kx⋆
i
−x⋆ jk2.
r − t
p
These combined with Claim 1 allow us to show that
η⋆s⋆(X )+σ⋆Z C
P X , t t t t / exp 1 klogT .
t t
∈T − √1 α ∈G ≤ −64
(cid:18) − t (cid:19) (cid:18) (cid:19)
Taking the above inequality collectively with Lemma 1 gives
C C C
1 1 1
∆ exp klogT +exp klogT 2exp klogT .
t,3
≤ − 4 −64 ≤ −64
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
Hence we have
k4log6T C k4log6T k4log6T
∆ ∆ +C2 2exp 1 klogT +C2 2C2
t,1 ≤ t,3 5 T2 ≤ −64 5 T2 ≤ 5 T2
(cid:18) (cid:19)
as long as T is sufficiently large.
A.7.1 Proof of Claim 1
Considerthe decomposition x =√α x (x )+√1 α ω as inAppendix A.1, where x (x ) for some
t t 0 t
−
t 0 t ∈Bi(xt)
i(x ) and ω . Notice that
t
∈I ∈G
1 1
s⋆(x )= x √α x = √α x (x )+√1 α ω √α (x +δ)
t t −1 α t − t 0 −1 α t 0 t − t − t 0
t t
− −
√α (cid:0) (cid:1) 1 (cid:2) √α (cid:3)
= t (x (x ) xb) ω+ t δ.
0 t 0
−1 α − − √1 α 1 α
t t t
− − −
22where x := E[X X = x ] is defined in (4.3), whereas x (x ) and δ are defined in (A.31). Therefore
0 0 t t 0 t t
| ∈ X
we can check that
b α (1 α ) 1 α α (1 α )
√1 α s⋆(x ) t − t x (x ) x + − t ω + t − t δ
− t t t 2 ≤ 1 α k 0 t − 0 k2 1 α k k2 1 α k k2
p − t r − t p − t
(cid:13) (cid:13)
(cid:13) (i) α (1 (cid:13)α ) C k(1 α )logT 1 α 1 α C klogT
t − t 3 3 − t + − t √d+ C klogT + − t exp 3
1
≤ p 1 −α t s α t r1 −α t r1 −α t (cid:18)− 32 (cid:19)
(cid:0) p (cid:1)
(ii) 8c logT C klogT
1 3 C klogT +√d+ C klogT +exp 3
3 1
≤ T − 32
r (cid:20) (cid:18) (cid:19)(cid:21)
p p
(iii) 1
√d+ C klogT .
1
≤ 2
(cid:16) p (cid:17)
Here step (i) follows from(A.4), the fact that ω , and (A.32); step (ii) follows from Lemma 8; while step
∈G
(iii) holds provided that T is sufficiently large. In addition, for any 1 i j N we have
ε
≤ ≤ ≤
(a) α (1 α ) 1 α
√1 −α t |(x⋆ i −x⋆ j) ⊤s⋆ t (x t) | ≤ 1t − α t kx 0(x t) −x 0 k2 kx⋆ i −x⋆ jk2+ 1− αt ω ⊤(x⋆ i −x⋆ j) ⊤
p − t r − t
(cid:12) (cid:12)
+ α t(1 −α t) δ x⋆ x⋆ (cid:12) (cid:12)
1 α k k2 k i − jk2
p − t
(b) α (1 α ) C k(1 α )logT 1 α
t − t 3 3 − t x⋆ x⋆ + − t C klogT x⋆ x⋆
≤ p 1 −α t s α t k i − jk2 r1 −α t 1 k i − jk2
p
α (1 α ) 1 α C klogT
+ t − t − t exp 3 x⋆ x⋆
1 α α − 32 k i − jk2
p − t r t (cid:18) (cid:19)
(c) 8c logT C klogT
1 3 C klogT + C klogT +exp 3 x⋆ x⋆
≤ T 3 1 − 32 k i − jk2
r (cid:20) (cid:18) (cid:19)(cid:21)
p p
(d) 1
C klogT x⋆ x⋆ .
≤ 2 1 k i − jk2
p
Here step (a) utilizes the Cauchy-Schwarz inequality; step (b) follows from (A.4), the fact that ω , and
∈ G
(A.32); step (c) follows from Lemma 8; while step (d) holds when T is sufficiently large.
A.8 Proof of Lemma 6
We can upper bound ∆ by
t,2
| |
(i)
∆ Tc0+2cR ( √α x x + x +1)p (x ,x )1 (x ,x ) / dx dx
|
t,2
| ≤ k
t t −1
−
t k2
k
t k2 Xt−1,Xt t −1 t
{
t t −1 ∈At
}
t −1 t
Z
=Tc0+2cRE[( √α X X + X +1)1 (X ,X ) / ]
t t 1 t 2 t 2 t t 1 t
k − − k k k { − ∈A }
( =ii) Tc0+2cRE √1 α W + X +1 1 (X ,X ) /
t t 2 t 2 t t 1 t
− k k k k { − ∈A }
(iii) Tc0+2cR√(cid:2) 1(cid:0) α E1/2 W 2 P1/2((X (cid:1) ,X ) / )+Tc0+2(cid:3)cRE1/2 X 2 P1/2((X ,X ) / )
≤ − t k t k2 t t −1 ∈At k t k2 t t −1 ∈At
+Tc0+2cRP((X t,X t(cid:2)1) / (cid:3)t). (cid:2) (cid:3) (A.44)
− ∈A
Here step (i) follows from Lemma 4; step (ii) follows from the update rule (2.1); step (iii) utilizes the
Cauchy-Schwarzinequality. In view of (2.2), we have
E X 2 =E √α X +√1 α W 2 E 2α R2+2(1 α ) W 2
k t k2 k t 0 − t t k2 ≤ t − t k t k2
(cid:2) (cid:3)=2α(cid:2)tR2+2(1 α t)d 2R2+(cid:3) 2d, (cid:2) (cid:3) (A.45)
− ≤
where we use the fact that E[ W 2]=d. Then we have
k t k2
(a)
∆ Tc0+2cR d(1 α )+ 2R2+2d P1/2((X ,X ) / )+Tc0+2cRP((X ,X ) / )
t,2 t t t 1 t t t 1 t
| | ≤ − − ∈A − ∈A
(cid:16)p p (cid:17)
23(b) C C
Tc0+2cR 2R+3√d exp 1 klogT +Tc0+2cRexp 1 klogT
≤ − 8 − 4
(cid:16) (cid:17) (cid:18) (cid:19) (cid:18) (cid:19)
(c) C
1
exp klogT
≤ −16
(cid:18) (cid:19)
Here step (a) utilizes (A.45) and the fact that E[ W 2]=d; step (b) follows from Lemma 1; while step (c)
k t k2
makes use of the assumption that k logd and holds provided that C c +c .
1 0 R
≥ ≫
A.9 Proof of Lemma 7
We first decompose K into
t
K
t
= p
Xt−1
|Xt(x
t −1
|x t)p Xt(x t)(x
t −1
−µ⋆
t
(x t))⊤ε t(x t)dx
t
−1dx
t
Z
( =i) p Xt−1 |Xt(x t −1 |x t) −p Y t⋆ −1|Yt(x t −1 |x t) p Xt(x t)(x t −1 −µ⋆ t (x t))⊤ε t(x t)dx t −1dx t
Z (cid:16) (cid:17)
( =ii) +
c
p Xt−1 |Xt(x t −1 |x t) −p Y t⋆ −1|Yt(x t −1 |x t) p Xt(x t)(x t −1 −µ⋆ t (x t))⊤ε t(x t)dx t −1dx t
(cid:18)ZAt ZAt(cid:19)(cid:16) (cid:17)
=:K +K .
t,1 t,2
Here step (i) follows from the fact that p Y t⋆ −1|Yt(x t −1 |x t)(x t −1 −µ⋆ t(x t))dx t −1 = 0 for any x t
∈
Rd, and
K andK aredefined to be the two integralsover and c in step (ii). The followingtwo claims provide
1 2 R At At
bounds for the two integrals K and K respectively.
t,1 t,2
Claim 2. Suppose that T k2log3T. Then for each 2 t T, we have
≫ ≤ ≤
k2log3T c logT
K 3C 1 E [ ε (x ) ].
|
t,1
|≤
5
T
r
T
xt∼qt
k
t t k2
Proof. See Appendix A.9.1.
Claim 3. Suppose that T 1. Then for each 2 t T, we have
≫ ≤ ≤
C
K 2exp 1 klogT E1/2 ε (x ) 2 .
| t,2 |≤ −32 xt∼qt k t t k2
(cid:18) (cid:19)
(cid:2) (cid:3)
Proof. See Appendix A.9.2.
Then we conclude that
K K + K
t t,1 t,2
| |≤| | | |
(a) k2log3T c logT C
3C 1 E [ ε (x ) ]+2exp 1 klogT E1/2 ε (x ) 2
≤ 5 T
r
T xt∼qt k t t k2 (cid:18)−32
(cid:19)
xt∼qt k t t k2
(b) k2log3T c logT (cid:2) (cid:3)
4C 1 E1/2 ε (x ) 2
≤ 5 T
r
T xt∼qt k t t k2
(cid:2) (cid:3)
as claimed. Here step (a) follows from Claim 2 and Claim 3; while step (b) utilizes Jensen’s inequality, and
holds provided that T is sufficiently large.
A.9.1 Proof of Claim 2
The term K can be upper bounded by
t,1
p (x x )
|K t,1 |= (cid:12)
(cid:12)ZAt
pX Yt t⋆− −1 1|| YX tt (x tt −− 11 || x tt ) −1 !p Y t⋆ −1|Yt(x t −1 |x t)p Xt(x t)(x t −1 −µ⋆ t (x t))⊤ε t(x t)dx t −1dx t (cid:12)
(cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
24(i) p (x x )
≤ ZAt(cid:12) (cid:12)1 − pX Y tt ⋆ −− 11 || YX tt (x tt −− 11 || x tt ) (cid:12) (cid:12)p Y t⋆ −1|Yt(x t −1 |x t)p Xt(x t) (cid:12)(x t −1 −µ⋆ t (x t))⊤ε t(x t) (cid:12)dx t −1dx t
( ≤ii) C 5k(cid:12) (cid:12) (cid:12)2lo Tg3T p Y t⋆ −1|Yt(x t −(cid:12) (cid:12) (cid:12)1 |x t)p Xt(x t) (x t −1 −µ⋆ t ((cid:12) (cid:12) x t))⊤ε t(x t) dx t −1dx t (cid:12) (cid:12)
ZAt (cid:12) (cid:12)
( =iii) C k2log3T E σ t⋆ Z ε (X ) 1 X ,X(cid:12) (cid:12)t+η ts⋆ t (X t)+σ t⋆Z (cid:12) (cid:12)
5 ⊤ t t t t
T √α √α ∈A
(cid:20) t (cid:26)(cid:18) t (cid:19) (cid:27)(cid:21)
(cid:12) (cid:12)
k2log3T (1 α(cid:12))(α α )(cid:12) (iv) k2log3T 8c logT 2
C
5
− t t − t E Z ⊤ε t(X t) C
5
1 E[ ε t(X t) 2]
≤ T s α t(1 −α t) ≤ T r T √2π k k
(cid:2)(cid:12) (cid:12)(cid:3)
k2log3T c logT (cid:12) (cid:12)
3C 1 E [ ε (x ) ].
≤
5
T
r
T
xt∼qt
k
t t k2
Herestep(i)followsfromJensen’sinequality;step(ii)utilizesLemma3;step(iii)followsfromthedefinition
of Y⋆ in (4.1) and of µ⋆ in (4.6), and Z (0,I ) is independent of X ; step (iv) follows from Lemma 8
t t t ∼ N d t
and the fact that Z ε (X ) X (0, ε (X ) 2) and hence
t⊤ t t | t ∼N k t t k2
2
E Z ε (X ) =E E Z ε (X ) X = E[ ε (X ) ].
t⊤ t t t⊤ t t
|
t
√2π k
t t k2
(cid:2)(cid:12) (cid:12)(cid:3) (cid:2) (cid:2)(cid:12) (cid:12) (cid:3)(cid:3)
(cid:12) (cid:12) (cid:12) (cid:12)
A.9.2 Proof of Claim 3
The term K can be upper bounded by
t,1
|K t,2 |≤ c p Xt−1 |Xt(x t −1 |x t)+p Y t⋆ −1|Yt(x t −1 |x t) p Xt(x t) kx t −1 −µ⋆ t (x t) k2kε t(x t) k2dx t −1dx t
ZAt(cid:16) (cid:17)
1/2
≤" c p Xt−1 |Xt(x t −1 |x t)+p Y t⋆ −1|Yt(x t −1 |x t) p Xt(x t) kx t −1 −µ⋆ t (x t) k2 2dx t −1dx t #
ZAt(cid:16) (cid:17)
=:γ1
1/2
| {z }
·" c
p Xt−1 |Xt(x t −1 |x t)+p Y t⋆ −1|Yt(x t −1 |x t) p Xt(x t) kε t(x t) k2 2dx t −1dx t
#
.
ZAt(cid:16) (cid:17)
=:γ2
The second term| γ can be easily bounded by {z }
2
γ √2E1/2 ε (x ) 2 .
2 ≤ xt∼qt k t t k2
In what follows, we will bound the first term γ . Note (cid:2)that (cid:3)
1
γ2 = p (x ,x ) x µ⋆(x ) 2dx dx
1 c Xt−1,Xt t −1 t k t −1 − t t k2 t −1 t
ZAt
=:γ1,1
| + cp Y t⋆ −1|Yt(x t −1 |x{ tz)p Xt(x t) kx t −1 −µ⋆ t (x t}) k2 2dx t −1dx t.
ZAt
=:γ1,2
We have | {z }
(i)
γ =E X µ⋆(X ) 21 (X ,X ) / E1/2 X µ⋆(X ) 4 P1/2((X ,X ) / )
1,1 k t −1 − t t k2 { t t −1 ∈At } ≤ k t −1 − t t k2 t t −1 ∈At
( ≤ii) αh −t 2E1/2 √1 −α tW t+η t⋆s⋆ t (X t) 4 2 expi −C 81 klh ogT i
h(cid:13) (cid:13) i (cid:18) (cid:19)
(cid:13) (cid:13)
25(iii) 4E1/2 √1 α W +η⋆s⋆(X ) 4 exp C 1 klogT
≤ − t t t t t 2 − 8
h(cid:13) (cid:13) i (cid:18) (cid:19)
Here step (i) follow(cid:13)s from Cauchy-Schwarz(cid:13)inequality; step (ii) follows from Lemma 1 and the definition of
µ⋆ in (4.6); while step (iii) uses the fact that α 1/2 (see Lemma 8). Recall the definition of s⋆()
t t ≥ t ·
1
s⋆(x )= x √α E[X X =x ] ,
t t −1 α t − t 0 | t t
t
−
(cid:0) (cid:1)
which leads to the following upper bound
1 √α 1 √α
s⋆(X ) X + t R= √α X +√1 α W + t R
k t t k≤ 1 α k t k2 1 α 1 α t 0 − t t 2 1 α
t t t t
− − − −
1 √α (cid:13) (cid:13)
W +2 t R. (cid:13) (cid:13) (A.46)
≤ √1 α t 2 1 α
t t
− −
(cid:13) (cid:13)
Hence we have (cid:13) (cid:13)
E √1 α W +η⋆s⋆(X ) 4 (i) 8(1 α )2E W 4 +(1 α )4E s⋆(X ) 4
− t t t t t 2 ≤ − t k t k2 − t k t t k2
h(cid:13) (cid:13) (cid:13) (cid:13) i (ii) 8 c 1logT h2 E Wi 4 + 8c 1logh T 4 E i W +R 4
≤ T k t k2 T t 2
(cid:18) (cid:19) h i (cid:18) (cid:19) h(cid:0)(cid:13) (cid:13) (cid:1) i
(iii) 1 d2+R4 . (cid:13) (cid:13)
≤ 16
(cid:0) (cid:1)
Here step (i) follows from the elementary inequality 8(x4+y4) (x+y)2; step (ii) follows from Lemma 8
≥
and (A.46); step (iii) follows from W ,W (0,I ) and the proviso that T being sufficiently large. Hence
t t d
∼N
we have
C C
γ d2+R4exp 1 klogT exp 0 klogT
1,1
≤ − 8 ≤ −16
(cid:18) (cid:19) (cid:18) (cid:19)
p
as long as C c and k logd. Regarding γ , we have
0 R 1,2
≫ ≥
γ ( =i)E X t+η ts⋆ t (X t)+σ t⋆Z t X t+η t⋆s⋆ t (X t) 2 1 (X ,X ) /
1,2 t t 1 t
"(cid:13) √α t − √α t (cid:13)2 { − ∈A }#
(cid:13) (cid:13)
σ⋆2(cid:13) (ii) (1 α )(cid:13)(α α )
= t (cid:13)E Z 21 (X ,X ) / − t(cid:13) t − t E1/2 Z 4 P1/2((X ,X ) / )
α
t
k t k2 { t t −1 ∈At } ≤ (1 α t)α
t
k t k2 t t −1 ∈At
h i − h i
(iii) 8c logT C (iv) C
1 exp 1 klogT E1/2 Z 4 exp 1 klogT .
≤ T − 8 k t k2 ≤ −16
(cid:18) (cid:19) h i (cid:18) (cid:19)
Here step (i) follows from the definition of Y⋆ in (4.1) and of µ⋆ in (4.6); step (ii) follows from the Cauchy-
t t
Schwarzinequality;step (iii)utilizes Lemma8 andLemma1; while step(iv) followsfromZ (0,I )and
t d
∼N
holds provided that T is sufficiently large and k logd. Taking the above bounds collectively yields
≥
C
K γ γ γ +γ γ 2exp 1 klogT E1/2 ε (x ) 2 .
| t,2 |≤ 1 2 ≤ 1,1 1,2 2 ≤ −32 xt∼qt k t t k2
(cid:18) (cid:19)
p (cid:2) (cid:3)
B Proof of Theorem 2
In view of the update rule (2.1), the variables X ,X ,...,X are jointly Gaussian, and we can check from
0 1 T
(2.2) that
X =√α X +√1 α W (0,α I +(1 α )I ), (B.1)
t t 0 t t t k t d
− ∼N −
hence the score functions
s⋆ t(x)= −(α tI k+(1 −α t)I d)−1x, ∀x ∈Rd. (B.2)
26We first derive the density of X conditional on X =x . Since the joint distribution of (X ,X ) is
t 1 t t t 1 t
− −
X 0 α I +(1 α )I √α (α I +(1 α )I )
t −1 , t −1 k − t −1 d t t −1 k − t −1 d ,
X t ∼N 0 √α t(α t 1I k+(1 α t 1)I d) α tI k+(1 α t)I d
(cid:20) (cid:21) (cid:18)(cid:20) (cid:21) (cid:20) − − − − (cid:21)(cid:19)
we can derive that
1 α 1 α
t 1 t 1
X t 1 X t =x t √α t I k+ − − (I d I k) x t,(1 α t) I k+ − − (I d I k) .
− | ∼N (cid:18) (cid:16) 1 −α t − (cid:17) − (cid:16) 1 −α t − (cid:17)(cid:19)
In addition, with perfect score estimation, we can use (2.3) and (B.2) to achieve
Y =
Y t+η ts⋆
t
(Y t)+σ tZ
t =
Y
t
−η t(α tI k+(1 −α t)I d)−1Y t+σ tZ
t ,
t 1
− √α t √α t
which indicates that
1 η σ2
Y Y =x (1 η )I + 1 t (I I ) , tI .
t 1 t t t k d k d
− | ∼N (cid:18)√α t (cid:18) − (cid:18) − 1 −α t(cid:19) − (cid:19) α t (cid:19)
Then we can check that for any x Rd,
t
∈
(1 α η )2 k α (1 α ) α (1 α )
KL p ( x ) p ( x ) = − t − t I x 2+ t − t log t − t 1
Xt−1 |Xt ·| t k Yt−1 |Yt ·| t 2σ t2 k k t k2 2
(cid:18)
σ t2 − σ t2 −
(cid:19)
(cid:0) (1 α η )2 (cid:1) d k (1 α )(α α ) (1 α )(α α )
+ − t − t (I I )x 2+ − − t t − t log − t t − t 1 .
2(1 α ) k d − k t k2 2 σ2(1 α ) − σ2(1 α ) −
− t (cid:18) t − t t − t (cid:19)
One can check that
z logz 1 0.1min 1,(z 1)2 , z >0.
− − ≥ − ∀
n o
We combine the above two relations as well as the assumption that k d/2 to achieve
≤
(1 α η )2 d (1 α )(α α ) 2
KL p ( x ) p ( x ) − t − t (I I )x 2+ − t t − t 1 .
Xt−1 |Xt ·| t k Yt−1 |Yt ·| t ≥ 2(1 −α t) k d − k t k2 40
(cid:18)
σ t2(1 −α t) −
(cid:19)
(cid:0) (cid:1)
By taking expectation w.r.t. x , we have
t
2
d d (1 α )(α α )
E KL p ( x ) p ( x ) (1 α η )2+ − t t − t 1 ,
xt∼qt Xt−1 |Xt ·| t k Yt−1 |Yt ·| t ≥ 4 − t − t 40
(cid:18)
σ t2(1 −α t) −
(cid:19)
(cid:2) (cid:0) (cid:1)(cid:3)
where we use the fact that
d
E (I I )x 2 =(d k)(1 α ) (1 α ).
xt∼qt k d − k t k2 − − t ≥ 2 − t
(cid:2) (cid:3)
C Technical lemmas
This section collects a few useful technical tools that are useful in the analysis.
Lemma 8. When T is sufficiently large, for 1 t T, we have
≤ ≤
c logT 1
1
α 1 .
t
≥ − T ≥ 2
In addition, for 2 t T, we have
≤ ≤
1 α 1 α 8c logT
t t 1
− − .
1 α ≤ α α ≤ T
t t t
− −
Proof. See Appendix A.2 in Li et al. (2024).
27Lemma 9. For Z (0,1) and any t 1, we know thatProposition 2.1.2
∼N ≥
P(Z t) e t2/2, t 1.
−
| |≥ ≤ ∀ ≥
In addition, for a chi-square random variable Y χ2(d), we have
∼
P(√Y √d+t) e t2/2, t 1.
−
≥ ≤ ∀ ≥
Proof. See Proposition 2.1.2 in Vershynin (2018) and Section 4.1 in Laurent and Massart (2000).
Lemma 10. Suppose that T is sufficiently large. Then we have
KL(p XTkp YT) ≤T−100.
Proof. See Lemma 3 in Li et al. (2024).
References
Anderson,B.D.(1982). Reverse-timediffusionequationmodels. StochasticProcesses andtheirApplications,
12(3):313–326.
Bao, F., Li, C., Zhu, J., and Zhang, B. (2022). Analytic-DPM: an analytic estimate of the optimal reverse
variance in diffusion probabilistic models. In International Conference on Learning Representations.
Benton,J.,DeBortoli,V.,Doucet,A.,andDeligiannidis,G.(2023). Linearconvergenceboundsfordiffusion
models via stochastic localization. arXiv preprint arXiv:2308.03686.
Chen, H., Lee, H., and Lu, J. (2023a). Improved analysis of score-based generative modeling: User-friendly
bounds underminimalsmoothnessassumptions. In International Conference on Machine Learning,pages
4735–4763.PMLR.
Chen, M., Huang, K., Zhao, T., and Wang, M. (2023b). Score approximation, estimation and distribution
recovery of diffusion models on low-dimensional data. In International Conference on Machine Learning,
pages 4672–4712.PMLR.
Chen,S.,Chewi,S.,Li,J.,Li,Y.,Salim,A.,andZhang,A.(2023c).Samplingisaseasyaslearningthescore:
theory for diffusion models with minimal data assumptions. In The Eleventh International Conference on
Learning Representations.
Croitoru, F.-A., Hondru, V., Ionescu, R. T., and Shah, M. (2023). Diffusion models in vision: A survey.
IEEE Transactions on Pattern Analysis and Machine Intelligence.
De Bortoli, V. (2022). Convergence of denoising diffusion models under the manifold hypothesis. arXiv
preprint arXiv:2208.05314.
Dhariwal, P. and Nichol, A. (2021). Diffusion models beat GANs on image synthesis. Advances in Neural
Information Processing Systems, 34:8780–8794.
Haussmann, U. G. and Pardoux, E. (1986). Time reversal of diffusions. The Annals of Probability, pages
1188–1205.
Ho, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in Neural
Information Processing Systems, 33:6840–6851.
Laurent,B.andMassart,P.(2000).Adaptiveestimationofaquadraticfunctionalbymodelselection. Annals
of statistics, pages 1302–1338.
Li, G., Wei, Y., Chen, Y., and Chi, Y. (2024). Towards non-asymptotic convergence for diffusion-based
generative models. In The Twelfth International Conference on Learning Representations.
28Nichol, A. Q. and Dhariwal, P. (2021). Improved denoising diffusion probabilistic models. In International
Conference on Machine Learning, pages 8162–8171.
Pope, P., Zhu, C., Abdelkader, A., Goldblum, M., and Goldstein, T. (2021). The intrinsic dimension of
images and its impact on learning. In 9th International Conference on Learning Representations, ICLR
2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. (2022). Hierarchical text-conditional image
generation with CLIP latents. arXiv preprint arXiv:2204.06125.
Rombach,R., Blattmann, A., Lorenz,D., Esser,P., andOmmer,B.(2022). High-resolutionimagesynthesis
with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 10684–10695.
Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R.,
Karagol Ayan, B., Salimans, T., et al. (2022). Photorealistic text-to-image diffusion models with deep
language understanding. Advances in Neural Information Processing Systems, 35:36479–36494.
Simoncelli, E. P. and Olshausen, B. A. (2001). Natural image statistics and neural representation. Annual
review of neuroscience, 24(1):1193–1216.
Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. (2015). Deep unsupervised learning
using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256–
2265.
Song, Y. and Ermon, S. (2019). Generative modeling by estimating gradients of the data distribution.
Advances in neural information processing systems, 32.
Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. (2021). Score-based
generativemodeling throughstochasticdifferential equations. International Conference on Learning Rep-
resentations.
Vershynin, R. (2018). High-dimensional probability: An introduction with applications in data science, vol-
ume 47. Cambridge university press.
29