Agent Planning with World Knowledge Model
ShuofeiQiao♠♡∗,RunnanFang♠♡∗,NingyuZhang♠♡†,YuqiZhu♠♡,XiangChen♠♡,
ShuminDeng♣,YongJiang♢,PengjunXie♢,FeiHuang♢,HuajunChen♠♡†
♠ZhejiangUniversity
♡ZhejiangUniversity-AntGroupJointLaboratoryofKnowledgeGraph
♣NationalUniversityofSingapore ♢AlibabaGroup
{shuofei,zhangningyu}@zju.edu.cn
Abstract
Recentendeavorstowardsdirectlyusinglargelanguagemodels(LLMs)asagent
models to execute interactive planning tasks have shown commendable results.
Despitetheirachievements,however,theystillstrugglewithbrainlesstrial-and-
error in global planning and generating hallucinatory actions in local planning
duetotheirpoorunderstandingofthe“real”physicalworld. Imitatinghumans’
mental world knowledge model which provides global prior knowledge before
the task and maintains local dynamic knowledge during the task, in this paper,
we introduce parametric World Knowledge Model (WKM) to facilitate agent
planning. Concretely,westeertheagentmodeltoself-synthesizeknowledgefrom
both expert and sampled trajectories. Then we develop WKM, providing prior
taskknowledgetoguidetheglobalplanninganddynamicstateknowledgetoassist
thelocalplanning. Experimentalresultsonthreecomplexreal-worldsimulated
datasetswiththreestate-of-the-artopen-sourceLLMs,Mistral-7B,Gemma-7B,
andLlama-3-8B,demonstratethatourmethodcanachievesuperiorperformance
comparedtovariousstrongbaselines. Besides,weanalyzetoillustratethatour
WKMcaneffectivelyalleviatetheblindtrial-and-errorandhallucinatoryaction
issues,providingstrongsupportfortheagent’sunderstandingoftheworld. Other
interestingfindingsinclude: 1)ourinstance-leveltaskknowledgecangeneralize
bettertounseentasks,2)weakWKMcanguidestrongagentmodelplanning,and
3)unifiedWKMtraininghaspromisingpotentialforfurtherdevelopment3.
agent model agent model
trial-and-error correct path
correct path first step task knowledge first step
state know_probs agent_probs
knowledge [ ]+[ ]
world knowledge
… model …
… … … … … …
hallucinatory
action (a) trajectories (b)
Figure1: Traditionalagentplanningvs.Agentplanningwithworldknowledgemodel.
∗ Equalcontribution.
† CorrespondingAuthor.
3Codewillbeavailableathttps://github.com/zjunlp/WKM.
Preprint.Underreview.
4202
yaM
32
]LC.sc[
1v50241.5042:viXra1 Introduction
TheremarkableadvancesinLargeLanguageModels(LLMs)havewitnessedarapiddevelopmentof
variousnaturallanguageprocessingtasks[25,16,28,47,60,33]. Recently,multipleattemptsthat
directlyexploitLLMsasagentmodelstoaddressphysicalworldplanningtaskshavedemonstrated
promising achievements [54, 57, 56, 34, 38, 64, 44]. However, as most state-of-the-art LLMs
are autoregressive models trained with next-token prediction, they lack the ability to essentially
understand the real world, leading to generating hallucinatory actions and performing brainless
trial-and-errorintheenvironmentasshowninFigure1(a).
IncontrasttoLLMs,humanspossessamentalknowledgemodelaboutthephysicalworld[1,18,
17,30]. Whenfacingaspecifictask,theywillfirstbrieflyrehearsetheentireprocessinmindusing
theirrichpriorknowledgebeforeperformingmindlessactions. Wecallthiskindofknowledgeglobal
taskknowledge(a.k.a. environment/taskcommonsense). Inaddition,duringthetaskprocedure,the
mentalworldknowledgemodelwillconstantlymaintainakindoflocalstateknowledge,representing
humans’cognitionofthecurrentworldstate. Forexample,imagineyouareinaroomandyourtask
istoputacleanegginmicrowave. ThetaskknowledgemayrefertoTheeggismost
likely in the fridge ... The workflows are: 1) locate and take the egg; 2)
cleantheeggusingsinkbasin... ThestateknowledgepossiblyreferstoMytaskisto
... Ihavefoundandtakedtheegg... NextIshould... Theabsenceofworldknowledge
can lead to blind trial-and-error in the early planning stages when environmental information is
limited. Conversely,inlaterstageswheninformationisredundant,itcaneasilyresultinaconfused
cognitionofthecurrentworldstateandgeneratehallucinatoryactions.
The process by which humans handle planning tasks reminds us to develop a parametric World
KnowledgeModel(WKM)tofacilitateagentplanning.Ashumanstypicallyacquireknowledgefrom
expertiseandpracticalexperience,webuildWKMbasedonknowledgelearnedfrombothexpert
andexploredtrajectories. Specifically,wefirststeertheagentmodeltosynthesizetaskknowledge
fromthecomparisonbetweenexpertandsampledtrajectories. Thenwepromptittosummarizestate
knowledgeforeachplanningstepfromexperttrajectoriesandcombinethepreviousandnextactions
tobuildastateknowledgebase. Lastly,weintegratethegeneratedknowledgeintoexperttrajectories
andtrainaWKM.Theagentmodelneedstoberetrainedtoadapttothetaskknowledge. Noteour
agentandknowledgemodelarebothtrainedwithLoRA[12]sharingthesamebackbone.
Duringtheplanningphase,weusetheWKMtoprovideglobalpriortaskknowledgeandmaintain
localdynamicstateknowledgefortheagentmodelasshowninFigure1(b). Thetaskknowledge
willbeconcatenatedinnaturallanguageformfollowingthespecifictasktoguidetheagentmodel’s
trial-and-error.Ateachplanningstep,topreventtheoccurrenceofhallucinatoryactions,weutilizethe
generatedstateknowledgeasthequerytoconductkNNretrievalfromthepre-builtstateknowledge
base. Wethenusetheconstraintsfromthepreviousaction,theprobabilitiesoftheretrievednext
actions,andtheprobabilitiesfromtheagentmodeltomakeaweightedpredictionforthenextaction.
Weevaluateourmethodonthreereal-worldsimulatedplanningtasks: ALFWorld[41],WebShop
[53],andScienceWorld[50]withthreestate-of-the-artopen-sourceLLMs: Mistral-7B[16],Gemma-
7B [24], and Llama-3-8B [25]. Empirical results demonstrate that our method achieves superior
performancecomparedtovariousstrongbaselinesonbothseenandunseentasks. Moreover,further
analyticalresultsshowthat1)ourWKMcaneffectivelyreduceblindtrial-and-errorandhallucinatory
actions,2)ourmodel-generatedinstance-levelknowledgecangeneralizebettertounseentasks,3)
weak-guide-strongisfeasible,4)multi-taskunifiedWKMpossessesstrongpotential,and5)explicit
stateknowledgewillhurttheperformanceofagentplanning.
2 Preliminaries
Wemainlyfocusoninteractivetaskswithpartialobservationsfromenvironments. Followingthe
task formulation in [44], the problem can be viewed as a Partially Observable Markov Decision
Process(POMDP):(U,S,A,O,T). TheinstructionspaceU definesthetaskanditscorresponding
regulations.Sisthestatespace,Oistheobservationspace,andAistheactionspace.T :S×A→S
definesthetransitionfunction,whichweassumetobegivenbytheenvironments. Itisnoticedthat
U,A,andOaresubspacesofthenaturallanguagespaceinthelanguageagentscenarios.
2Training Phase Task: put two newspapers in drawer Planning Phase
World Knowledge Model
(c) Model Training (d) Planning with WKM
input aY ro ou u a nr de yin o uth , e yo m ui d sed ele a o af ra m r co ho am ir. 1L ,o ao k ci an bg i nq eu ti c 1k , l ay agent model
output drawer 2, a drawer 1, a sofa 1… knowledge model
Task: put two newspapers in drawer.
Task Knowledge:
When trying to place multiple objects in a drawer, state knowledge base
(at, st, at+1)
you should first locate all the objects, then go to the
OAg be sn : t T: h g eo f rt io d gfr eid 1g e is 1 c losed TY ao su k a : r pe u i tn a t h ce le m anid ed gle g o inf a room… drawer one at a time, and place each object inside from agent model
State Knowledge: Your task is microwave. before closing the drawer. The action workflows are:
to … You are checking … Task Knowledge: 1)Locate all objects. from knowledge model
Agent: open fridge 1 You should first find an egg and 2)Go to the drawer.
Obs: The fridge 1 is open. In it … The workflows are: … 3)Place one object in/on the drawer. from environment
4)Close the drawer.
5)Repeat steps 2-4 for each object.
(b) State Knowledge (a) Task Knowledge at (1-γ)·pknow+γ·pagent
Summarization Synthesis Agent: go to sofa 1
Obs: On the sofa 1, you see a creditcard 2, a
newspaper 1. st
Task: put a clean egg in microwave State Knowledge: Your task is to put two go go
A Og be sn : t T: h
gE eox
f
p
rt
ioe
d
r gft
r e
iT
d
1r ga
e
isj e
1
cc lt oo sr ey
d A Og be sn : t
O:S nga
o
tm
h t
ep
o
l ce
c
od
o uu
T nnr tta
ee
rj re
tt
ooc ppto 11r ,y
y ou
n t h e e Aw re gs ep is na tp a :e
a
tnr aes
t k+
w ein 1s npd ear wa pw se pre ar 1.
p
Y o eo n ru 1i ta .
f
r re
o
mch se oc fk ai n 1g sofa 1 and t hpa
…
euk ate
t
t hpa
…
euk ate
t
Agent: open fridge 1 see a creditcard 2, a dishsponge Obs: You pick up the newspaper 1 from the sofa 1.
Obs: The fridge 1 is open. In it, 2… State Knowledge: Your task is to put two
you see a cup 3, a cup 1, a Agent: go to countertop 2 newspapers in drawer. You are checking sofa 1 and
lettuce 1… Obs: On the countertop 2, you
…… see a creditcard 1, a pen 1, a pen have found one newspaper. Next you should find State knowledge will not
Agent: put egg 2 in microwave 1 2, a newspaper 1… another newspaper. appear in the context of
Reward: 1.0 …… …… agent model during training
Reward: 0.0 Agent: put newspaper 2 in/on the drawer 1 and inference.
τw τl Reward: 1.0
Figure2: OverviewofourWKM.Wetrainaworldknowledgemodelontheknowledgesynthesizedbythe
agentmodelitselffrombothexpertandexploredtrajectories,providingpriortaskknowledgetoguideglobal
planninganddynamicstateknowledgetoassistlocalplanning.
Basedontheabove,thehistoricaltrajectoryh thatconsistsofalistofactionsandobservationsat
t
timetcanberepresentedas:
h =(u,a ,o ,a ,o ,...,a ,o ), (1)
t 0 0 1 1 t t
whereu ∈U isthetaskinstructionanda ∈A,o ∈Oaretheactionandtheobservation. Givena
task,thelanguageagentwithparameterθservesasthepolicymodelπ responsibleforgenerating
θ
theactiona basedonh ateachtimestept+1:
t+1 t
a ∼π (·|h ). (2)
t+1 θ t
Specifically,a ∼π (·|u)isgeneratedaccordingtothetaskinstructionu. Thewholetrajectoryτ
0 θ
concludeswhenthetaskiscompletedorexceedsthemaximumtimesteps. Thentheproductionof
theentiretrajectorywithtimelengthncanbemodeledas:
n
(cid:89)
π (τ|u)= π (a |h )π (a |u). (3)
θ θ t+1 t θ 0
t=0
Ultimately,thefinalrewardr(u,τ)∈[0,1]representingthetaskcompletionrateiscalculated. Note
thatwefollowaREACT-style[54]trajectorythatincludesrationalesbeforeeachaction. Weuseato
representtheactionwithrationalesforconvenience.
WorldKnowledgeModel. Worldknowledgemodelservesashumans’mentalcognitionofthe
physicalenvironment,moreintricatethanthewordknowledgemodelwhichLLM-poweredagent
modelsaretrainedtobe[61,10,52,13]. Our“world”herereferstothesimulatedenvironmentofthe
task. Basedonthestaticenvironmentofthetaskandthedynamicchangesduringinteractionwiththe
agent,wedefineworldknowledgeasacombinationofpriorglobalknowledgeanddynamiclocal
knowledge,correspondingtotheblindtrial-and-errorprobleminglobalplanningandthehallucinatory
actionissueinlocalplanningintraditionalagentmodels,respectively. Toattainpreciseandefficient
agentplanning,wedevelopaparametricWKMtosimulatethementalWKMofhumans.
3 Method
As shown in Figure 2, we steer the agent model to self-synthesize the task knowledge from the
comparison of expert and sampled trajectories (§3.1). Then we prompt the agent model to self-
summarizethestateknowledgebasedonhistoricalbehaviorandconstructastateknowledgebase
3(§3.2). ThegeneratedknowledgewillbeintegratedintotheexperttrajectoriesfortrainingtheWKM.
Afterthetrainingprocess(§3.3),weaugmenttheagentmodelwiththeworldknowledgemodelto
achieveeffectiveandaccurateplanning(§3.4).
3.1 TaskKnowledgeSynthesis
Thetaskknowledgeservesasthepriorknowledgetoguidetheagentmodel’sglobalplanningand
preventitfromdroppingintoblindtrial-and-error.
ExperiencedAgentExploration. Weprimarilyacquiretaskknowledgethroughthecomparisonof
preferencetrajectories(chosenvs. rejected). Inordertoimprovethequalityofrejectedtrajectories
andobtainmoretargetedtaskknowledge,weemployanexperiencedagentforexploration.Firstly,we
trainavanillalanguagemodelwithexperttrajectories4fromthetrainingsettoobtainanexperienced
agent. Subsequently,theexperiencedagentexploresthetrainingsettasksagaintogeneraterejected
trajectories. Ourpurposeistoextractsuperiortaskknowledgethatcannotbeacquiredsolelythrough
supervisedfine-tuningonchosentrajectories,thusfurthereffectivelyboostingtheagent’scapabilities.
SelfKnowledgeSynthesis. Withtheexperttrajectoriesasthechosenonesandthetrajectories
sampled from the experienced agent as the rejected ones, we prompt the agent model itself to
synthesizethetaskknowledge. SupposingKisthetaskknowledgespace:
κ∼π (·|ρ ,u,τ ,τ ), (4)
θ TaskKnow w l
whereκ∈Kisthetaskknowledge,ρ standsfortheprompttoinstructthetaskknowledge
TaskKnow
extraction,andτ ,τ arethechosenandrejectedtrajectoriesrespectively. Notethatgiventhesame
w l
tasku,τ andτ alwayssatisfyr(u,τ ) = 1 ≥ r(u,τ ). Evenwhenr(u,τ ) = r(u,τ ),westill
w l w l w l
considertrajectoriessampledfromtheexperiencedagentasrejectedones. Thisisbecauseexpert
trajectoriesoftenhaveshortersteplengths,enablingtheagenttolearnmoreknowledgeofefficient
planning. Fordetailedpromptsoftaskknowledgesynthesis,pleaserefertoAppendixH.1.
3.2 StateKnowledgeSummarization
Thestateknowledgeservesasthedynamicknowledgetoconstraintheagentmodel’slocalplanning
andpreventitfromgeneratinghallucinatoryactions. Weprompttheagentmodeltoself-summarize
state knowledge at each planning step based on the expert trajectories to guarantee quality. For
detailedpromptsofstateknowledgesummarization,pleaserefertoAppendixH.2. Supposingthe
promptusedtosummarizestateknowledgeisρ andthestateknowledges∈S isapartof
StateKnow
thestatespaceS,thegenerationofstateknowledgeattimetcanberepresentedas:
s ∼π (·|ρ ,h ). (5)
t θ StateKnow t
StateKnowledgeBaseConstruction. Toavoidconfusioncausedbyexcessiveadditionalinfor-
mation,insteadofexplicitlyconcatenatingthestateknowledgetothecontext,weconstructastate
knowledgebaseforretrieval(weanalyzein§4.3howexplicitstateknowledgemayaffecttheperfor-
manceofagentmodel). Wecombinethestateknowledges withthepreviousactiona andnext
t t
actiona fromtheexperttrajectorytoformaaction-state-actiontriplet(a ,s ,a ). Afteriterat-
t+1 t t t+1
ingthroughallexperttrajectories,weobtainaStateKnowledgeBaseB ={(s,a ,a )(i)}|B| ,
pre next i=1
wherea =a ,a =a ,and|B|isthesizeofthestateknowledgebase.
pre t next t+1
3.3 ModelTraining
Weintegratethegeneratedworldknowledgeintoexperttrajectoriesandtrainaworldknowledge
model. Theagentmodelneedstobere-trainedtoadapttotheincorporationoftaskknowledge. Note
thatouragentmodelandknowledgemodelarebothtrainedwithLoRAsharingthesamebackbone.
WelisttheexamplesoftrainingdataforboththeagentmodelandWKMinAppendixE.
4Fordetailsonhowtocollectexperttrajectories,pleaserefertoAppendixA.
4AgentModelTraining. GiventheexperttrajectoriesdatasetD = {(u,κ,τ )(i)}|D| withtask
w i=1
knowledgeκgeneratedin§3.1,wetraintheagentmodeltofollowthetaskknowledgetogenerate
actions. Underanauto-regressivemanner,thelossoftheagentmodelcanbeformulatedas:
L (π )=−E [π (τ |u,κ)] (6)
agent θ τw∼D θ w
SupposeX =(x ,x ,...,x )isthetokensequenceofthetrajectoryτ ,wehave:
1 2 |X| w
|X|
(cid:88)
π (τ |u,κ)=− (1(x ∈A)×logπ (x |u,κ,x )). (7)
θ w j θ j <j
j=1
Here1(x ∈ A)istheindicatorfunctiontomasktokensunrelatedtoactions. Pleasenotethatτ
j w
heredoesnotincludethestateknowledgementionedin§3.2.
WorldKnowledgeModelTraining. Themaindifferenceinthetrainingdatabetweentheagent
andknowledgemodelistheaddedstateknowledge. Giventheexperttrajectoriesdatasetwithboth
taskandstateknowledgeD′ ={(u,κ,τ′ )(i)}|D′|whereτ′ =(a ,o ,s ,...,a ,o ,s ),theloss
w i=1 w 0 0 0 n n n
oftheknowledgemodelπ canbeformulatedas:
ϕ
L (π )=−E [π (κ|u)π (τ′ |u,κ)] (8)
know ϕ κ,τ w′∼D′ ϕ ϕ w
SupposeX′ =(x′,x′,...,x′ )isthetokensequenceoftheexperttrajectorywithstateknowledge
1 2 |X′|
τ′ andY =(y ,y ,...,y )representsthetokensequenceofthetaskknowledgeκ,wehave:
w 1 2 |Y|
|Y|
(cid:88)
π (κ|u)=− logπ (y |u,y ) (9)
ϕ ϕ i <i
i=1
|X′|
π (τ′ |u,κ)=−(cid:88)(cid:0)1(x′ ∈S)×logπ (x′|u,κ,x′ )(cid:1) , (10)
ϕ w j ϕ j <j
j=1
where1(x ∈S)istheindicatorfunctiontomasktokensunrelatedtostateknowledge.
j
3.4 AgentPlanningwithWorldKnowledgeModel
Atinferencetime,theagentmodelplansontheevaluationtaskswiththeaidoftheworldknowledge
model. Weredefinethehistoricaltrajectoryh =(u,κ,a ,o ,a ,o ,...,a ,o ). Givenaspecific
t 0 0 1 1 t t
task instruction u, the knowledge model first generates the task knowledge κ ∼ π (·|u), then
ϕ
the agent model starts planning. Assuming the available action set A ⊆ A for the task u is
u
(α(1),α(2),...,α(|Au|)),atanytimet≥0,insteadofdirectlygeneratinganextactiona ∈A
u u u t+1 u
basedonh ,wefirstemploytheworldknowledgemodeltogeneratethecurrentstateknowledge
t
s ∼π (·|h )andleverages toquerythestateknowledgebaseB ={(s,a ,a )(i)}|B| . With
t ϕ t t pre next i=1
the state knowledge as the key, we retrieve N nearest triplets from where a = a based on
pre t
semanticsimilarityandcollectthecorrespondingnextactionsa . Wecounttheprobabilityof
next
eachactionp know(α u(i))= N Ni,whereN iistheoccurrencenumberofactionα u(i)inallthecollected
a . Therefore,wegettheprobabilityacquiredfromthestateknowledgebase:
next
| (cid:88)Au|
P (A )=(p (α(1)),p (α(2)),··· ,p (α(|Au|))), p (α(i))=1. (11)
know u know u know u know u know u
i=1
Afterward,wesampleallthelogitsofα(i),1≤i≤|A |fromtheagentmodelandapplyasoftmax
u u
functiontonormalizetheprobability. Wedefinetheprobabilityacquiredfromtheagentmodelas:
| (cid:88)Au|
P (A )=(p (α(1)),p (α(2)),··· ,p (α(|Au|))), p (α(i))=1. (12)
agent u agent u agent u agent u agent u
i=1
Finally,wedeterminethenextactionbycombiningtheabovetwoprobabilities:
a = argmax (γ·p (α(i))+(1−γ)·p (α(i))), (13)
t+1 agent u know u
α( ui)∈Au,1≤i≤|Au|
whereγ isthehyperparameterthatcontrolstheproportionofP (A ). Basedontheabove,we
agent u
enhance the agent planning by global guidance from task knowledge and local constraints from
stateknowledgegeneratedbyourWKM.DuetotheWKMandretrieval,theinferencestageincurs
additionaltimeoverheadcomparedtothepureagentmodel. Theapproximateratioisaround2.5:1.
5Table 1: MainResults. Thebestresultsaremarkedinboldandthesecond-bestresultsaremarkedwith
underline.Alltheprompt-basedbaselines(u)areevaluatedunderone-shotpromptingandallthefine-tuning-
basedbaselines(v)aretrainedthroughLoRA.RedrepresentsthechangesofWKMrelativetotheoptimal
resultsinthebaselines.WKMandagentmodelaredifferentLoRAssharingthesamebackbone.
ALFWorld ScienceWorld
Backbone Method WebShop
Seen Unseen Seen Unseen
GPT-3.5-Turbo 8.57 5.97 44.37 15.41 13.99
u REACT
GPT-4 44.29 38.05 62.76 67.32 65.09
u REACT 7.86 5.22 14.63 20.72 17.65
u Reflexion 11.56 6.00 16.64 21.07 18.11
v NAT 64.43 68.96 61.01 57.12 50.79
Mistral-7B
v ETO 66.84 71.43 64.09 58.17 51.85
v KNOWAGENT 70.44 70.72 61.28 59.32 47.24
WKM 73.57+3.13 76.87+5.44 65.48+1.39 62.12+2.80 53.62+1.77
u REACT 6.43 2.24 5.93 3.58 3.51
u Reflexion 7.14 2.99 7.71 4.94 3.93
v NAT 67.86 65.88 55.82 47.63 44.98
Gemma-7B
v ETO 66.43 68.66 62.67 50.44 47.84
v KNOWAGENT 69.29 67.60 58.80 48.55 45.28
WKM 70.71+1.42 70.40+1.74 63.75+1.08 53.68+3.24 49.24+1.40
u REACT 2.86 3.73 19.32 24.76 22.66
u Reflexion 4.29 4.48 22.73 27.23 25.41
v NAT 60.71 59.70 61.60 55.24 48.76
Llama-3-8B
v ETO 64.29 64.18 64.57 57.90 52.33
v KNOWAGENT 66.71 62.69 64.40 58.67 49.18
WKM 68.57+1.86 65.93+1.75 66.64+2.07 60.12+1.55 54.75+2.42
4 Experiments
4.1 ExperimentalSettings
DatasetsandMetrics. Weevaluateourmethodonthreereal-worldsimulatedplanningdatasets:
ALFWorld [41], WebShop [53], and ScienceWorld [50]. AlFWorld and ScienceWorld include
unseentaskstoevaluatetheagent’sgeneralizationability. TherewardofALFWorldisbinary0or
1,indicatingwhethertheagenthascompletedthetaskornot. WebShopandScienceWorldprovide
denserewardsfrom0to1tomeasurethecompletionlevelofthetask. Forallthedatasets,weapply
averagerewardasthefinalmetrics. PleaserefertoAppendixBfordetaileddatasetinformation.
ModelsandBaselines. Weevaluateonthreestate-of-the-artopen-sourcemodels: 1)Mistral-7B
[16],theMistral-7B-Instruct-v0.2version. 2)Gemma-7B[24],theGemma-1.1-7B-itversion. 3)
Llama-3-8B[25],theMeta-Llama-3-8B-Instructversion. Wecompareourmethodwithtwoprompt-
based baselines: REACT [54] and Reflexion [40]. Besides, we adopt two strong baselines that
introducerejectedtrajectoriesintothetrainingprocesstolearnfromexperience: NAT[49],learn
fromrejectedtrajectoriesthroughSFT,andETO[44],learnfromrejectedtrajectoriesthroughDPO
[36]. Moreover,wecomparewithaknowledge-augmentedplanningmethodKNOWAGENT. Wealso
includeChatGPT(gpt-3.5-turbo-0125)[27]andGPT-4(gpt-4-32K-0613)forcomparison. Allthe
prompt-basedbaselinesaretestedunderone-shotandallthefine-tuning-basedbaselinesaretrained
withLoRA[12]. PleaserefertoAppendixCforbaselinesandre-producingdetails.
TrainingandInferenceSetups. Wefine-tunetheproposedapproachwithLoRA[12]usingthe
LlamaFactory [62] framework. The learning rate is 1e-4 and the sequence length is 2048 for all
the models. The training epoch is 3 and the batch size is 32. We adopt the AdamW optimizer
[22]withacosinelearningscheduler. Duringinference,thenumberofretrievedaction-state-action
tripletsN issetto3000andtheP (A )weightγ issetto{0.4,0.5,0.7}. Allthetrainingand
agent u
inferenceexperimentsareconductedon8NVIDIAV10032GGPUswithin12hours. Pleasereferto
AppendixDfordetailedhyperparametersusedinourpaper.
6w/o all w/ state w/ task w/ task&state
80 80 70
76.87
75.37
70
67.8669.2973.57
69.4070.67
70 60
60.8162.12
65.48 55.04
60
63.57
60
62.4463.68
50
52.78 50.3251.5253.4253.62
56.98
50 50 40
seen unseen test seen unseen
ALFWorld WebShop ScienceWorld
Figure3: AblationStudyonMistral-7B.w/oallmeansthevanillaexperiencedagentmodeltrainingwithpure
experttrajectories.w/stateistestingagentmodelwithonlystateknowledgebaseconstraints.w/taskstandsfor
guidingagentmodelwithonlytaskknowledge.w/task&stateisourWKMwithbothtaskknowledgeguidance
andstateknowledgeconstraints.
Table 2: Average Steps. The maximum number of steps in Table 3: Hallucinatory Action Rates on
ALFWorld and WebShop is 40 and 10. In ScienceWorld, the ALFWorld. Wecalculatetheproportionof
numberofstepsrangesfrom10to120dependingonthetask trajectoriescontaininginvalidactionsregard-
type,withanaverageofaround40. lessoftheircorrectness.
ALFWorld ScienceWorld ALFWorld
Method WebShop Method
Seen Unseen Seen Unseen Seen Unseen
NAT 23.27 23.42 4.08 20.18 21.21 NAT 45.71% 50.00%
ETO 19.82 22.29 3.99 24.13 26.35 ETO 34.29% 36.57%
KNOWAGENT 18.51 24.56 4.01 21.06 24.74 KNOWAGENT 33.57% 44.78%
WKM 17.66 17.92 3.97 18.74 19.59 WKM 32.86% 29.85%
4.2 Results
MainResults. AsshowninTable1,forprompt-basedbaselinesonopen-sourcemodels,bothRE-
ACTandReflexionexhibitpoorperformance,farbehindourmethodandfine-tuning-basedbaselines
onvariousdatasets. GPT-3.5-TurboperformsordinarilyontwodatasetsotherthanWebShop,andit
evenfallsbehindMistral-7BandLlama-3-8B’sREACTperformanceonScienceWorld. However,
GPT-4exhibitsstrongperformanceacrossvariousdatasets. Nevertheless, ourapproach, through
LoRA training alone, surpasses GPT-4 on ALFWorld (44.29→73.57 on seen, 38.05→76.87 on
unseen)andWebShop(62.76→66.64). Forfine-tuning-basedbaselines,bothNATandETOfall
behindourmethod,implyingthatjustintegratingworldknowledgeforagentmodelsisworthmore
thanfurtherfussySFTorDPOonnegativeexamples. OurmethodalsoperformsbetterthanKNOWA-
GENTwhichbringshuman-designedfixedactionknowledgeandlongactionpathsintotrajectories.
ThissuggeststheeffectivenessofourWKMwhichisresponsibleforgeneratinginstance-leveltask
knowledgeandmaintainingimplicitactionconstraints. Furthermore,KNOWAGENT’sperformance
on unseen tasks is not as impressive as on seen tasks, while WKM can keep its advantage. This
phenomenonalsodemonstratesthegeneralizationabilityofWKM.
Approach Ablations. As shown in Figure 3, taking Mistral-7B as an example, we decompose
thekeycomponentsofWKMtoexaminetherolesofthetaskandstateknowledgeseparately. Ina
macroview,removingeachmoduleresultsinacleardropintheagent’sperformance,whichvalidates
the power of our world knowledge. Furthermore, the improvement through task knowledge (w/
task)ismorepronouncedthanthatthroughstateknowledge(w/state),suggestingthenecessityof
globalpriorknowledgeforagentplanning. Amoremicroobservationrevealsthattheimpactofstate
knowledgeismoresignificantonseentaskscomparedtounseentasks,whiletheinfluenceoftask
knowledgeissustainableacrossseenandunseentasks. Thismaybeattributedthatalthoughour
real-timestateknowledgeisgeneratedbyWKM,thestateknowledgebaseisbuiltonthetrainingset,
whichmayweakengeneralizationtosomeextent.
4.3 Analysis
Worldknowledgecanmitigateblindtrial-and-errorandreducehallucinatoryactions. We
comparethenumberofplanningstepsforeachdatasetbetweenthreestrongbaselinesandWKM
and calculate the average steps of each method. As depicted in Figure 10 (in Appendix), WKM
7
draweR
egarevAdemonstratestheabilitytocompleteasignificantproportionoftasksusingtheshortesttrajectory,
indicatingthatguidancefromworldknowledgecaneffectivelyreducetheagent’sblindtrial-and-error
in the environment. Taking a further perspective from an average standpoint in Table 2, it can
be observed that WKM exhibits lower average planning steps compared to other baselines. As
ALFWorldcanrespondtoinvalidactions,inTable3,wecountthepercentageofhallucinatoryactions
thatoccurredintrajectoriesfromALFWorldforeachmethod. Theresultsconfirmtheeffectiveness
ofourworldknowledgemodeltodecreasehallucinatoryactions. Furthermore,itisworthnotingthat
mostbaselinesshowaprominentincreaseintheaveragenumberofstepsandpercentageofinvalid
actionswhentransitioningfromseentaskstounseentasks,butWKMcanstillmaintainarelatively
lowlevel. Thisreflectslaterallythatourworldknowledgecanstilleffectivelyguidetheagentmodel
onunseentasks,highlightingtheknowledgegeneralizationbroughtbytheworldknowledgemodel.
Toseehowourworldknowledgeworks,pleaserefertoourcasestudyinAppendixF.
Ourinstance-levelknowledgecangeneralizebettertounseen
tasks. Tofurtherexplorethebenefitofusingaknowledgemodelto 80
Human
generateinstance-leveltaskknowledge,wecarefullysurveythetask
70 WKM w/o state
knowledgegeneratedbyourWKMandabstractitintodataset-level
knowledge for each dataset. Then we retrain the agent model to 60
adapttonewdataset-levelknowledge5. AsillustratedinFigure4,
50
wecomparetheperformanceofdataset-levelknowledgewithour
40
i
S
ln
ec
vs it eea lnn
c
kc ee nW- ol we ov
r
lle edl d.t gIa ets ck nak onn tbo oew nol le
b
yd
se
sg
r
ue
v
re( pW
d
astK
sh
eaM
sto
hw
u
u/
r
mo ms aot na
d
-t dee
l
e)
-
sgo
ie
gn
n
nA
e er
dL atF
e
kW
d
noio
n
wr sl
t
ld
a en
da cn ged
e- ALFWorld s
Aee Ln
FWorld
unseen
SciWorld
see Scn
iWorld
unseen
onseentasksbutalsoexhibitsevenmoreremarkableperformance
onunseentasks,withtheimprovementinperformanceonunseen
Figure4:Performanceofhuman-
designeddataset-levelknowledge
taskssignificantlygreaterthanthatonseentasks. Thisphenomenon
compared to WKM generated
straightlyreflectsthestronggeneralizationabilityofourknowledge
instance-levelknowledge.
modelcomparedtorigidlydesignedknowledgebyhumans.
Table4: Weak-guide-strong. Theknowl- Weak knowledge model guides strong agent model
edgemodelhereisbasedonMistral-7B. planning. In our main experiments, the knowledge
modelandagentmodelarebasedonthesamebackbone.
ALFWorld
Backbone Method
Here,weexploreonALFWorldwhatwillhappenifweuse
Seen Unseen
aweakknowledgemodeltoguideastrongagentmodel.
GPT-3.5-Turbo WRE KA MCT w/ostate 18 2. .5 87 6 85 .. 99 67 WechooseMistral-7Basthebackboneoftheknowledge
GPT-4
REACT 44.29 38.05 modelandChatGPTandGPT-4astheagentmodel. Since
WKMw/ostate 50.71 47.01 wecannotgetthetokendistributionfromOpenAIAPI,we
onlyapplytaskknowledgetotheagentmodel. AsexhibitedinTable4,theresultsofbothChatGPT
andGPT-4showdistinctadvancesafterbeingguidedbytheMistral-7Bworldknowledgemodel,
indicatingtheweakworldknowledgemodelalsocontainsknowledgethatthestrongmodelmaylack.
IntheeraofLLMs,thisinspiresuswithanewagentlearningparadigm: weak-guide-strong. Due
toitslightweightnature,theweakknowledgemodelcanflexiblyadjustitsparametersbasedonthe
needsoftheagentmodel,whichcanaddressthedifficultyoflargeagentmodelsinadaptingtonew
environmentsthroughfine-tuning.
ALFWorld
seen
100%
UnifiedWorldKnowledgeModelTraining. Wemixtheworld
knowledge collected from all three datasets and jointly train one 50%
ScienceWorld ALFWorld
singleworldknowledgemodeltoinvestigatetheeffectofmulti-task unseen unseen
0%
worldknowledgelearning. Figure5illustratestherelativeperfor-
mancecomparisonbetweenmulti-taskWKMandvariousbaselines,
from which we can observe that multi-task WKM not only does
ScienceWorld WebShop
not lead to performance degradation but also exhibits visible im- seen
provementscomparedtosingle-taskWKM,especiallyonWebShop ETO KnowAgent NAT
WKM-single-task WKM-multi-task
andScienceWorld. Similarto[57,58,3]whichendeavortotrain
Figure5: Relativeperformance
aunifiedagentmodelandachievestronggeneralizationabilityto
ofmulti-taskWKMcomparedto
held-outtasks,thisobservationinspiresuswiththepotentialoftrain-
variousbaselines.
ing a unified world knowledge model that can be applied to help
5Detailedmanuallydesigneddataset-levelknowledgepromptcanbefoundinAppendixH.3
8
draweR
egarevAvariousheld-inagentmodelsandalsogeneralizetoguideheld-outagentmodels. Amoredaring
ideaiswhetheraunifiedagentmodelcombinedwithaunifiedworldknowledgemodelisthekeyto
ArtificialGeneralIntelligence(AGI).
Explicit state knowledge will hurt the planning performance. WKM WKM w/o state Explicit State
To demonstrate the rationality of our choice to construct a state 80 73.57 76.87 75.37
knowledge base, we explore the effect of directly incorporating 70 69.29 65.43 66.69
stateknowledgeintothecontextoftheagentmodel(weretrainthe 60
agentmodeltofollowboththetaskandstateknowledge),asshown 50
in Figure 6. The performance of explicit state knowledge is far 40
ALFWorld seen ALFWorld unseen
inferiortoourapproachofretrievingfromastateknowledgebase Figure 6: Performance of ex-
andutilizingprobabilisticconstraints. Itevenperformsworsethan plicitstateknowledge.
whenweremovestateknowledgeandonlyincludetaskknowledge. Thisclearlyindicatesthatblindly
extendingpromptswithalargeamountofexplicitnaturallanguagefeedbackislose-more-than-gain
foragentplanning,andimplicitknowledgeconstraintsmaybesometimesmoreprudent.
5 RelatedWork
LLM Agents. LLMs have emerged as a promising avenue towards unlocking the potential of
ArtificialGeneralIntelligence,offeringrobustsupportforthedevelopmentofagentsystems[48,51,
8,63]. Existingworksinthisfieldmainlyfocusesonagentplanning[14,21,54,42],externaltools
harnessing[39,23,43,29,32,35,46],codegeneration[45,21,31,11],etc. Recently,therehasbeen
anincreasingfocusonendowingopen-sourceLLMswithagentfunctionalitiesthroughfine-tuning
[2,57,56,38,44,49]. However,theseapproachesrelyonblindlyfittingtheprobabilitiesoftokens
tolearnplanning,withouthavinganintimatecognitionoftheenvironment. Thelackofknowledge
canleadtotheagentblindlyattemptingtrial-and-errorandgeneratinghallucinatoryactions.
KnowledgeAugmentedAgentPlanning. Planning[15]isacrucialcapabilityforintelligentagents
toaccomplishreal-worldtasks,oftenrequiringagentstopossessrichknowledgeandenvironmental
commonsense. Fewworkshaveexploredthefieldofknowledge-augmentedagentplanning. [14,61,
5]utilizetherichparametricknowledgestoredinpre-trainedlanguagemodelstoassistagentplanners.
[7,20,59,64]designstructuredornaturallanguageknowledgetoregulatetheactions. However,
theabovestudiesrequirethemanualdesignoffixedprompttemplatesortaskprocedures,making
itchallengingtotransferacrossdifferenttaskenvironments. [63,55,6]proposetheautomationof
knowledge generation using language models. However, their knowledge either consists of only
globalworkfloworonlylocalactionprinciples. Incontrast,wetrainourworldknowledgemodelboth
onglobaltaskknowledgeandlocalstateknowledgetoassistagentplanning,andtheseknowledge
sourcesarederivedfromthemodel’sself-summaryratherthanhand-curated.
LLM-basedWorldModel. Worldmodelandagentmodeloftenco-occurinthedomainofrein-
forcementlearningandrobotics[13,9,19,37,26,4]. WithLLMscommonlydeemedasthemost
powerfulintelligentmachinesconstructedbyhumansthusfar,theLLM-backedworldmodelshave
beenproposed[61,10,13]. Inourpaper,weattempttoself-synthesizeworldknowledgeandtrainto
obtainaworldknowledgemodel. However,weconsiderourmodeltobeaworldknowledgemodel
ratherthanaworldmodelbasedonthereasonthatourmodelistemporarilyunabletoutilizesearch
algorithms(e.g. MCTS)inconjunctionwiththeagentmodeltomakepredictionsabouttheworld
andweleavethisforourfuturework.
6 ConclusionandFutureWork
Inthispaper,westrivetodevelopaparametricworldknowledgemodel(WKM)toaugmentlanguage
agentmodelplanning. OurWKMcangeneratepriortaskknowledgetoguideglobalplanningas
well as dynamic state knowledge to regulate local planning. Our extensive results show that our
world knowledge can work on both GPT-4 and state-of-the-art open-source models and achieve
superiorperformancecomparedtovariousstrongbaselines. Analyticalexperimentsvalidatethatour
WKMcan1)reducebrainlesstrial-and-errorandinvalidactions,2)generalizebettertounseentasks,
3)achieveweak-guide-strong,and4)beeffectivelyextendedtounifiedworldknowledgetraining.
9
draweR
egarevAPotential future directions include: 1) building a unified world knowledge model, 2) learning to
predicttheworldlikeaworldmodel,3)applyingtomulti-modalagentplanning,etc.
Limitations
Despite our best efforts, this paper still have some limitations: 1) Our primary intention behind
designingtheWKMistocompensateforthelackofworldknowledgeintheagentmodel. However,
determiningwhatalanguagemodelknowsanddoesn’tknowhasbeenanongoingchallengethat
remains unresolved. 2) It is widely acknowledged that world knowledge extends beyond textual
representations. Whileourworldknowledgeiscurrentlylimitedtotextualinformation,exploring
multi-modalworldknowledgemodelsisindeedoneofourimportantfuturetasks. 3)Ourworld
knowledgemodelcannotdynamicallyupdatewiththechangesoftheworldandfeedbackfromthe
agent. 4)Generatingworldknowledgecanintroduceadditionalinferenceoverhead.
References
[1] Robert Eamon Briscoe. Mental imagery and the varieties of amodal perception. Pacific
PhilosophicalQuarterly,92(2):153–173,2011.
[2] BaianChen,ChangShu,EhsanShareghi,NigelCollier,KarthikNarasimhan,andShunyuYao.
Fireact: Towardlanguageagentfine-tuning. CoRR,abs/2310.05915,2023.
[3] ZehuiChen,KuikunLiu,QiuchenWang,WenweiZhang,JiangningLiu,DahuaLin,KaiChen,
andFengZhao. Agent-flan: Designingdataandmethodsofeffectiveagenttuningforlarge
languagemodels. CoRR,abs/2403.12881,2024.
[4] AnnaDawidandYannLeCun. Introductiontolatentvariableenergy-basedmodels: Apath
towardsautonomousmachineintelligence. CoRR,abs/2306.02572,2023.
[5] Yan Ding, Xiaohan Zhang, Saeid Amiri, Nieqing Cao, Hao Yang, Andy Kaminski, Chad
Esselink, and Shiqi Zhang. Integrating action knowledge and llms for task planning and
situationhandlinginopenworlds. Auton.Robots,47(8):981–997,2023.
[6] YaoFu,Dong-KiKim,JaekyeomKim,SungryullSohn,LajanugenLogeswaran,Kyunghoon
Bae,andHonglakLee. Autoguide: Automatedgenerationandselectionofstate-awareguide-
linesforlargelanguagemodelagents. CoRR,abs/2403.08978,2024.
[7] JianGuan, WeiWu, ZujieWen, PengXu, HongningWang, andMinlieHuang. AMOR:A
recipe for building adaptable modular knowledge agents through process feedback. CoRR,
abs/2402.01469,2024.
[8] TaichengGuo,XiuyingChen,YaqiWang,RuidiChang,ShichaoPei,NiteshV.Chawla,Olaf
Wiest,andXiangliangZhang. Largelanguagemodelbasedmulti-agents: Asurveyofprogress
andchallenges. CoRR,abs/2402.01680,2024.
[9] DavidHaandJürgenSchmidhuber. Worldmodels. CoRR,abs/1803.10122,2018.
[10] ShiboHao,YiGu,HaodiMa,JoshuaJiahuaHong,ZhenWang,DaisyZheWang,andZhiting
Hu. Reasoningwithlanguagemodelisplanningwithworldmodel. InHoudaBouamor,Juan
Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages
8154–8173.AssociationforComputationalLinguistics,2023.
[11] SiruiHong, XiawuZheng, JonathanChen, YuhengCheng, JinlinWang, CeyaoZhang, Zili
Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and
ChenglinWu. Metagpt: Metaprogrammingformulti-agentcollaborativeframework. CoRR,
abs/2308.00352,2023.
[12] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
LuWang,andWeizhuChen. Lora: Low-rankadaptationoflargelanguagemodels. InThe
TenthInternationalConferenceonLearningRepresentations,ICLR2022,VirtualEvent,April
25-29,2022.OpenReview.net,2022.
10[13] ZhitingHuandTianminShu. Languagemodels,agentmodels,andworldmodels: TheLAW
formachinereasoningandplanning. CoRR,abs/2312.05230,2023.
[14] WenlongHuang,PieterAbbeel,DeepakPathak,andIgorMordatch. Languagemodelsaszero-
shotplanners: Extractingactionableknowledgeforembodiedagents. InKamalikaChaudhuri,
StefanieJegelka,LeSong,CsabaSzepesvári,GangNiu,andSivanSabato,editors,International
ConferenceonMachineLearning,ICML2022,17-23July2022,Baltimore,Maryland,USA,
volume162ofProceedingsofMachineLearningResearch,pages9118–9147.PMLR,2022.
[15] XuHuang,WeiwenLiu,XiaolongChen,XingmeiWang,HaoWang,DefuLian,YashengWang,
RuimingTang,andEnhongChen. UnderstandingtheplanningofLLMagents:Asurvey. CoRR,
abs/2402.02716,2024.
[16] AlbertQ.Jiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,DevendraSinghChap-
lot,DiegodeLasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,LucileSaulnier,
Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril,
ThomasWang,TimothéeLacroix,andWilliamElSayed. Mistral7b. CoRR,abs/2310.06825,
2023.
[17] PhilipNJohnson-Laird. Mentalmodelsandhumanreasoning. ProceedingsoftheNational
AcademyofSciences,107(43):18243–18250,2010.
[18] Philip Nicholas Johnson-Laird. Mental models: Towards a cognitive science of language,
inference,andconsciousness. HarvardUniversityPress,1983.
[19] LukaszKaiser,MohammadBabaeizadeh,PiotrMilos,BlazejOsinski,RoyH.Campbell,Konrad
Czechowski,DumitruErhan,ChelseaFinn,PiotrKozakowski,SergeyLevine,AfrozMohiuddin,
RyanSepassi,GeorgeTucker,andHenrykMichalewski. Modelbasedreinforcementlearning
for atari. In 8th International Conference on Learning Representations, ICLR 2020, Addis
Ababa,Ethiopia,April26-30,2020.OpenReview.net,2020.
[20] ZelongLi,WenyueHua,HaoWang,HeZhu,andYongfengZhang. Formal-llm: Integrating
formallanguageandnaturallanguageforcontrollablellm-basedagents. CoRR,abs/2402.00798,
2024.
[21] Lajanugen Logeswaran, Yao Fu, Moontae Lee, and Honglak Lee. Few-shot subgoal plan-
ning with language models. In Marine Carpuat, Marie-Catherine de Marneffe, and Iván
Vladimir Meza Ruíz, editors, Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies,
NAACL2022,Seattle,WA,UnitedStates,July10-15,2022,pages5493–5506.Associationfor
ComputationalLinguistics,2022.
[22] IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. In7thInternational
ConferenceonLearningRepresentations,ICLR2019,NewOrleans,LA,USA,May6-9,2019.
OpenReview.net,2019.
[23] PanLu,BaolinPeng,HaoCheng,MichelGalley,Kai-WeiChang,YingNianWu,Song-Chun
Zhu,andJianfengGao.Chameleon:Plug-and-playcompositionalreasoningwithlargelanguage
models.InAliceOh,TristanNaumann,AmirGloberson,KateSaenko,MoritzHardt,andSergey
Levine,editors,AdvancesinNeuralInformationProcessingSystems36: AnnualConferenceon
NeuralInformationProcessingSystems2023,NeurIPS2023,NewOrleans,LA,USA,December
10-16,2023,2023.
[24] ThomasMesnard,CassidyHardin,RobertDadashi,SuryaBhupatiraju,ShreyaPathak,Laurent
Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot,
AakankshaChowdhery,AdamRoberts,AdityaBarua,AlexBotev,AlexCastro-Ros,Ambrose
Slone,AmélieHéliou,AndreaTacchetti,AnnaBulanova,AntoniaPaterson,BethTsai,andetal.
Gemma: Openmodelsbasedongeminiresearchandtechnology. CoRR,abs/2403.08295,2024.
[25] Meta. Introducingmetallama3: Themostcapableopenlyavailablellmtodate,2024. https:
//ai.meta.com/blog/meta-llama-3/.
11[26] Thomas M. Moerland, Joost Broekens, Aske Plaat, and Catholijn M. Jonker. Model-based
reinforcementlearning: Asurvey. Found.TrendsMach.Learn.,16(1):1–118,2023.
[27] OpenAI. Chatgpt: Optimizing language models for dialogue, 2022. https://openai.
com/blog/chatgpt/.
[28] OpenAI. GPT-4technicalreport. CoRR,abs/2303.08774,2023.
[29] ShishirG.Patil,TianjunZhang,XinWang,andJosephE.Gonzalez. Gorilla: Largelanguage
modelconnectedwithmassiveapis. CoRR,abs/2305.15334,2023.
[30] RTPramod,MichaelCohen,KirstenLydic,JoshTenenbaum,andNancyKanwisher. Evidence
thatthebrain’sphysicsenginerunsforwardsimulationsofwhatwillhappennext. Journalof
Vision,20(11):1521–1521,2020.
[31] ChenQian,XinCong,ChengYang,WeizeChen,YushengSu,JuyuanXu,ZhiyuanLiu,and
MaosongSun. Communicativeagentsforsoftwaredevelopment. CoRR,abs/2307.07924,2023.
[32] ShuofeiQiao,HonghaoGui,HuajunChen,andNingyuZhang. Makinglanguagemodelsbetter
toollearnerswithexecutionfeedback. CoRR,abs/2305.13068,2023.
[33] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi
Tan,FeiHuang,andHuajunChen. Reasoningwithlanguagemodelprompting: Asurvey. In
AnnaRogers,JordanL.Boyd-Graber,andNaoakiOkazaki,editors,Proceedingsofthe61st
AnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),
ACL2023,Toronto,Canada,July9-14,2023,pages5368–5393.AssociationforComputational
Linguistics,2023.
[34] ShuofeiQiao,NingyuZhang,RunnanFang,YujieLuo,WangchunshuZhou,YuchenEleanor
Jiang,ChengfeiLv,andHuajunChen. AUTOACT:automaticagentlearningfromscratchvia
self-planning. CoRR,abs/2401.05268,2024.
[35] YujiaQin,ShihaoLiang,YiningYe,KunlunZhu,LanYan,YaxiLu,YankaiLin,XinCong,
XiangruTang,BillQian,SihanZhao,RunchuTian,RuobingXie,JieZhou,MarkGerstein,
Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to
master16000+real-worldapis. CoRR,abs/2307.16789,2023.
[36] RafaelRafailov,ArchitSharma,EricMitchell,ChristopherD.Manning,StefanoErmon,and
ChelseaFinn. Directpreferenceoptimization: Yourlanguagemodelissecretlyarewardmodel.
InAliceOh,TristanNaumann,AmirGloberson,KateSaenko,MoritzHardt,andSergeyLevine,
editors,AdvancesinNeuralInformationProcessingSystems36: AnnualConferenceonNeural
InformationProcessingSystems2023,NeurIPS2023,NewOrleans,LA,USA,December10-
16,2023,2023.
[37] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre,
SimonSchmitt,ArthurGuez,EdwardLockhart,DemisHassabis,ThoreGraepel,TimothyP.
Lillicrap,andDavidSilver. Masteringatari,go,chessandshogibyplanningwithalearned
model. Nat.,588(7839):604–609,2020.
[38] Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen,
Ji Zhang, and Fei Huang. Small llms are weak tool learners: A multi-llm agent. CoRR,
abs/2401.07324,2024.
[39] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.
Hugginggpt: SolvingAItaskswithchatgptanditsfriendsinhuggingface. InAliceOh,Tristan
Naumann,AmirGloberson,KateSaenko,MoritzHardt,andSergeyLevine,editors,Advances
in Neural Information Processing Systems 36: Annual Conference on Neural Information
ProcessingSystems2023, NeurIPS2023, NewOrleans, LA,USA,December10-16, 2023,
2023.
[40] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.
Reflexion: languageagentswithverbalreinforcementlearning. InAliceOh,TristanNaumann,
AmirGloberson,KateSaenko,MoritzHardt,andSergeyLevine,editors,AdvancesinNeural
Information Processing Systems 36: Annual Conference on Neural Information Processing
Systems2023,NeurIPS2023,NewOrleans,LA,USA,December10-16,2023,2023.
12[41] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and
MatthewJ.Hausknecht. Alfworld: Aligningtextandembodiedenvironmentsforinteractive
learning. In9thInternationalConferenceonLearningRepresentations,ICLR2021,Virtual
Event,Austria,May3-7,2021.OpenReview.net,2021.
[42] ChanHeeSong,BrianM.Sadler,JiamanWu,Wei-LunChao,ClaytonWashington,andYuSu.
Llm-planner: Few-shotgroundedplanningforembodiedagentswithlargelanguagemodels. In
IEEE/CVFInternationalConferenceonComputerVision,ICCV2023,Paris,France,October
1-6,2023,pages2986–2997.IEEE,2023.
[43] YifanSong,WeiminXiong,DaweiZhu,ChengLi,KeWang,YeTian,andSujianLi. Rest-
gpt: Connectinglargelanguagemodelswithreal-worldapplicationsviarestfulapis. CoRR,
abs/2306.06624,2023.
[44] YifanSong,DaYin,XiangYue,JieHuang,SujianLi,andBillYuchenLin. Trialanderror:
Exploration-basedtrajectoryoptimizationforLLMagents. CoRR,abs/2403.02502,2024.
[45] HaotianSun,YuchenZhuang,LingkaiKong,BoDai,andChaoZhang. Adaplanner: Adaptive
planningfromfeedbackwithlanguagemodels. InAliceOh,TristanNaumann,AmirGloberson,
Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information
ProcessingSystems36: AnnualConferenceonNeuralInformationProcessingSystems2023,
NeurIPS2023,NewOrleans,LA,USA,December10-16,2023,2023.
[46] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. Toolal-
paca: Generalized tool learning for language models with 3000 simulated cases. CoRR,
abs/2306.05301,2023.
[47] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas
Blecher,CristianCanton-Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,andetal. Llama
2: Openfoundationandfine-tunedchatmodels. CoRR,abs/2307.09288,2023.
[48] LeiWang,ChenMa,XueyangFeng,ZeyuZhang,HaoYang,JingsenZhang,ZhiyuanChen,
JiakaiTang,XuChen,YankaiLin,WayneXinZhao,ZheweiWei,andJirongWen. Asurveyon
largelanguagemodelbasedautonomousagents. FrontiersComput.Sci.,18(6):186345,2024.
[49] RenxiWang,HaonanLi,XudongHan,YixuanZhang,andTimothyBaldwin. Learningfrom
failure: Integratingnegativeexampleswhenfine-tuninglargelanguagemodelsasagents. CoRR,
abs/2402.11651,2024.
[50] RuoyaoWang,PeterA.Jansen,Marc-AlexandreCôté,andPrithvirajAmmanabrolu. Science-
world: Isyouragentsmarterthana5thgrader? InYoavGoldberg, ZornitsaKozareva, and
Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural
LanguageProcessing,EMNLP2022,AbuDhabi,UnitedArabEmirates,December7-11,2022,
pages11279–11298.AssociationforComputationalLinguistics,2022.
[51] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang,
Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong,
Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin,
Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng,
XipengQiu,XuanjingHuan,andTaoGui. Theriseandpotentialoflargelanguagemodelbased
agents: Asurvey. CoRR,abs/2309.07864,2023.
[52] JiannanXiang,TianhuaTao,YiGu,TianminShu,ZiruiWang,ZichaoYang,andZhitingHu.
Languagemodelsmeetworldmodels: Embodiedexperiencesenhancelanguagemodels. In
AliceOh,TristanNaumann,AmirGloberson,KateSaenko,MoritzHardt,andSergeyLevine,
editors,AdvancesinNeuralInformationProcessingSystems36: AnnualConferenceonNeural
InformationProcessingSystems2023,NeurIPS2023,NewOrleans,LA,USA,December10-
16,2023,2023.
[53] ShunyuYao,HowardChen,JohnYang,andKarthikNarasimhan. Webshop: Towardsscalable
real-worldwebinteractionwithgroundedlanguageagents. InSanmiKoyejo,S.Mohamed,
A.Agarwal,DanielleBelgrave,K.Cho,andA.Oh,editors,AdvancesinNeuralInformation
13ProcessingSystems35: AnnualConferenceonNeuralInformationProcessingSystems2022,
NeurIPS2022,NewOrleans,LA,USA,November28-December9,2022,2022.
[54] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and
Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh
InternationalConferenceonLearningRepresentations,ICLR2023,Kigali,Rwanda,May1-5,
2023.OpenReview.net,2023.
[55] YiningYe,XinCong,ShizuoTian,JiannanCao,HaoWang,YujiaQin,YaxiLu,HeyangYu,
HuadongWang,YankaiLin,ZhiyuanLiu,andMaosongSun. Proagent: Fromroboticprocess
automationtoagenticprocessautomation. CoRR,abs/2311.10751,2023.
[56] Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Chandu, Kai-Wei Chang, Yejin
Choi,andBillYuchenLin. Lumos: Learningagentswithunifieddata,modulardesign,and
open-sourcellms. CoRR,abs/2311.05657,2023.
[57] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang.
Agenttuning: Enablinggeneralizedagentabilitiesforllms. CoRR,abs/2310.12823,2023.
[58] JianguoZhang,TianLan,RitheshMurthy,ZhiweiLiu,WeiranYao,JuntaoTan,ThaiHoang,
Liangwei Yang, Yihao Feng, Zuxin Liu, Tulika Awalgaonkar, Juan Carlos Niebles, Silvio
Savarese,ShelbyHeinecke,HuanWang,andCaimingXiong. Agentohana: Designunifieddata
andtrainingpipelineforeffectiveagentlearning. CoRR,abs/2402.15506,2024.
[59] AndrewZhao,DanielHuang,QuentinXu,MatthieuLin,Yong-JinLiu,andGaoHuang. Expel:
LLMagentsareexperientiallearners. InMichaelJ.Wooldridge,JenniferG.Dy,andSriraam
Natarajan,editors,Thirty-EighthAAAIConferenceonArtificialIntelligence,AAAI2024,Thirty-
SixthConferenceonInnovativeApplicationsofArtificialIntelligence,IAAI2024,Fourteenth
SymposiumonEducationalAdvancesinArtificialIntelligence,EAAI2014,February20-27,
2024,Vancouver,Canada,pages19632–19642.AAAIPress,2024.
[60] WayneXinZhao,KunZhou,JunyiLi,TianyiTang,XiaoleiWang,YupengHou,YingqianMin,
BeichenZhang,JunjieZhang,ZicanDong,YifanDu,ChenYang,YushuoChen,ZhipengChen,
JinhaoJiang,RuiyangRen,YifanLi,XinyuTang,ZikangLiu,PeiyuLiu,Jian-YunNie,and
Ji-RongWen. Asurveyoflargelanguagemodels. CoRR,abs/2303.18223,2023.
[61] ZiruiZhao,WeeSunLee,andDavidHsu. Largelanguagemodelsascommonsenseknowledge
forlarge-scaletaskplanning. InAliceOh,TristanNaumann,AmirGloberson,KateSaenko,
MoritzHardt,andSergeyLevine,editors,AdvancesinNeuralInformationProcessingSystems
36: AnnualConferenceonNeuralInformationProcessingSystems2023,NeurIPS2023,New
Orleans,LA,USA,December10-16,2023,2023.
[62] YaoweiZheng,RichongZhang,JunhaoZhang,YanhanYe,ZheyanLuo,andYongqiangMa.
Llamafactory: Unifiedefficientfine-tuningof100+languagemodels. CoRR,abs/2403.13372,
2024.
[63] Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu,
JintianZhang,JingChen,RuipuWu,ShuaiWang,ShidingZhu,JiyuChen,WentaoZhang,
Ningyu Zhang, Huajun Chen, Peng Cui, and Mrinmaya Sachan. Agents: An open-source
frameworkforautonomouslanguageagents. CoRR,abs/2309.07870,2023.
[64] YuqiZhu,ShuofeiQiao,YixinOu,ShuminDeng,NingyuZhang,ShiweiLyu,YueShen,Lei
Liang,JinjieGu,andHuajunChen. Knowagent: Knowledge-augmentedplanningforllm-based
agents. CoRR,abs/2403.03101,2024.
14A ExpertTrajectoriesCollection
WemainlyusetheexperttrajectorieswithaREACT-style[54]collectedfrom[44]:
1. ALFWorld[41]. Thedatasetprovideshuman-annotatedtrajectories.
2. WebSahop[53]. Exceptforhuman-annotatedtrajectories,GPT-4isalsoappliedtogenerate
trajectorieswitharewardlargerthan0.7beingreserved.
3. ScienceWorld[50]. Thedatasetoffersheuristicalgorithmstosearchgoldentrajectoriesfor
eachsub-task.
Sincetheoriginalgoldentrajectoriesdonotcontainrationales,GPT-4isfurtherleveragedtogenerate
thecorrespondinginformation.
B DatasetInformation
We evaluate our method on three real-world simulated agent planning datasets: ALFWorld [41],
WebShop[53],andScienceWorld[50].
1. ALFWorldisahouseholddatasetrequiringtheagenttonavigatethroughtheroomand
manipulateobjects. Exceptforseentasks,AlFWorldalsoincludesunseentaskstoevaluate
the agent’s generalization ability. The reward of ALFWorld is binary 0 or 1, indicating
whethertheagenthascompletedthetaskornot.
2. WebShopisanonlineshoppingdatasetinawebsiteenvironment. Itprovidesdensefinal
rewardsfrom0to1tomeasurethecompletionlevelofthetask.
3. ScienceWorldisascientificreasoningdatasetwhichisatthelevelofastandardelementary
schoolsciencecurriculum. Italsopossessesbothseenandunseenpartsandadensereward
functionfrom0to1.
Forallthedatasets,weapplyaveragerewardasthefinalmetrics. Table5illustratesthestatisticsof
eachdataset.
Table5: Datasetstatistics.
Dataset Train Text-Seen Text-Unseen
ALFWorld 3,119 140 134
WebShop 1,824 200 -
ScienceWorld 1,483 194 211
C ComparedBaselines
Herewedetailedlyintroducethebaselineswecomparewithandourre-producedetails.
1. REACT[54]. ThefirstapproachincorporatesChain-of-Thought(COT)promptinginagent
planningtaskswithaformatofThought-Action-Observationloop. Inourpaper,weapply
one-shotpromptingforREACT6.
2. Reflexion [40]. A strong prompt-based baseline reinforces agent planning with verbal
feedback. Manuallydesignedpromptsareusedtoenabletheagenttoreflectonthehistorical
trajectoryandre-planbasedonthefeedback. Inourpaper,weutilizeone-shotprompting
forreflectionandselectthefirstreflectiterationasourresultduetolimitedcontext7.
3. NAT[49]. NATincludesnegativetrajectoriesbyemployingdifferentpromptsduringagent
fine-tuning. Whenevaluating,onlypositivepromptsareusedtoencouragethelanguage
6https://github.com/ysymyth/ReAct
7https://github.com/noahshinn/reflexion
15agenttogeneratecorrecttrajectories. AsitalsofollowstheREACT-styleformat,wedirectly
usethedefaultpositiveandnegativepromptsandtrainwithLoRAinourpaper8.
4. ETO[44].Anotherbaselineincludesnegativetrajectoriesduringagenttraining.Themethod
containstwotrainingphases,ofwhichthefirstphaseisbehaviorcloningwhichfine-tunes
theagentonexperttrajectories,andthesecondphaseislearningfromfailureswhichfurther
fine-tunestheagentthroughDirectPreferenceOptimization(DPO)[36]. Inourpaper,we
removetheone-shotpromptforfairnessandretainallthedefaulthyperparametersproposed
inETOexceptforLoRAtraining9.
5. KNOWAGENT[64]. KNOWAGENTisaknowledge-augmentedagentplanningbaselinethat
appliesactionknowledgeinthepromptandmaintainsanactionpathinthecontextduring
planningtoconstraintheagent’saction. Wedirectlyusethedefaultpromptmentionedin
KNOWAGENT for ALFWorld and carefully extend it to WebShop and ScienceWorld by
followingasimilarformat10.
Alltheprompt-basedbaselinesaretestedunderone-shotandallthefine-tuning-basedbaselinesare
trainedwithLoRA[12].
D Hyperparameters
ThedetailedhyperparametersweuseduringtrainingandinferenceareshowninTable6. Weemploy
identicalhyperparametersfordifferentmodels. Thetemperatureoftheagentmodelissetto0.0when
conductingexplorationand0.5whenintroducedintoWKM.ThetemperatureofWKMissetto0.0
forallthetime. TheP (A )weightγ issetto0.4forALFWorld,0.5forWebShop,and0.7for
agent u
SienceWorld.
Table6: Detailedhyperparametersusedinourpaper.
Name Value
lorar 8
loraalpha 16
loradropout 0.05
loratargetmodules q_proj,v_proj
cutofflen 2048
epochs 3
batchsize 32
batchsizeperdevice 4
gradientaccumulationsteps 2
learningrate 1e-4
warmupratio 0.03
temperature 0.0,0.5
retrievedstateknowledgeN 3000
P (A )weightγ 0.4,0.5,0.7
agent u
E TrainingDataExample
WeshowthetrainingdataexampleforagentmodelandworldknowledgemodelinFigure7and
Figure8. Duringtraining,transparenttokensdonotcontributetothecalculationoftheloss. Only
thepartsofthemodelresponsibleforoutput(agentmodel: purple,knowledgeworldmodel: green),
participateinthelosscalculation.
8https://github.com/Reason-Wang/NAT
9https://github.com/Yifan-Song793/ETO
10https://github.com/zjunlp/KnowAgent
16TrainingDataExampleforAgentModel
TaskInstruction: Youareinthemiddleofaroom. Lookingquicklyaroundyou,yousee
abed1,adesk1,adrawer5,adrawer4,adrawer3,adrawer2,adrawer1,adresser1,a
garbagecan1,alaundryhamper1,andashelf1. Yourtaskisto: examinethebookwiththe
desklamp.
TaskKnowledge: Whenyoucannotfindtheobjectrequiredforthetaskunderthedesklamp,
firstcheckallthedrawersandshelvesintheroom,thenrevisitthepreviouslocationsand
double-checkbeforegivingupandlookinginthegarbagecanorlaundryhamper. Theaction
workflowsareasfollows:1. Foreachdrawerandshelf,gotothelocationandopenittocheck
iftheobjectisinside. 2. Iftheobjectisnotfound,closethedrawerorshelfandmoveonto
thenextone. 3. Aftercheckingallthedrawersandshelves,revisitthepreviouslocationsand
double-checkiftheobjectmighthavebeenoverlooked. 4. Iftheobjectisstillnotfound,look
inthegarbagecanorlaundryhamperasalastresort.
Thought: Thetaskrequiresmetoexamineabookwiththedesklamp. First,Ineedtolocate
thebook.
Action: gotobed1
Observation: Onthebed1,youseeabook1,alaptop1,andapillow1.
Thought: Iseeabookonthebed. Ishouldpickituptoexamineitwiththedesklamp.
Action: takebook1frombed1
Observation: Youpickupthebook1fromthebed1.
Thought: NowthatIhavethebook,Ineedtofindthedesklamptoexaminethebookunder
itslight.
Action: gotodesk1
Observation: Onthedesk1,youseeanalarmclock1,abowl1,aCD2,amug3,amug2,
amug1,apen1,andapencil2.
Thought: Thedesklampisn’tonthedesk. Ineedtolookaroundtheroomtofindwherethe
desklampislocated.
Action: gotodresser1
Observation: Onthedresser1,youseeacellphone1,adesklamp1,andakeychain2.
Thought: I’vefoundthedesklamponthedresser. Ishoulduseittoexaminethebook.
Action: usedesklamp1
Figure7: TrainingDataExampleforAgentModel.
TrainingDataExampleforAgentModel
TaskInstruction: Youareinthemiddleofaroom. Lookingquicklyaroundyou,yousee
abed1,adesk1,adrawer5,adrawer4,adrawer3,adrawer2,adrawer1,adresser1,a
garbagecan1,alaundryhamper1,andashelf1. Yourtaskisto: examinethebookwiththe
desklamp.
TaskKnowledge: Whenyoucannotfindtheobjectrequiredforthetaskunderthedesklamp,
firstcheckallthedrawersandshelvesintheroom,thenrevisitthepreviouslocationsand
double-checkbeforegivingupandlookinginthegarbagecanorlaundryhamper. Theaction
workflowsareasfollows:1. Foreachdrawerandshelf,gotothelocationandopenittocheck
iftheobjectisinside. 2. Iftheobjectisnotfound,closethedrawerorshelfandmoveonto
thenextone. 3. Aftercheckingallthedrawersandshelves,revisitthepreviouslocationsand
double-checkiftheobjectmighthavebeenoverlooked. 4. Iftheobjectisstillnotfound,look
inthegarbagecanorlaundryhamperasalastresort.
Thought: Thetaskrequiresmetoexamineabookwiththedesklamp. First,Ineedtolocate
thebook.
Action: gotobed1
Observation: Onthebed1,youseeabook1,alaptop1,andapillow1.
StateKnowledge: Yourtaskistoexamineabookwiththedesklamp. Youhavenotfound
thedesklampyet,andhaveonlycheckedthebed1.
Thought: Iseeabookonthebed. Ishouldpickituptoexamineitwiththedesklamp.
Action: takebook1frombed1
17Observation: Youpickupthebook1fromthebed1.
StateKnowledge: Yourtaskistoexamineabookusingthedesklamp. Yourhavelocatedthe
bookonbed1andpickeditup.
Thought: NowthatIhavethebook,Ineedtofindthedesklamptoexaminethebookunder
itslight.
Action: gotodesk1
Observation: Onthedesk1,youseeanalarmclock1,abowl1,aCD2,amug3,amug2,
amug1,apen1,andapencil2.
StateKnowledge: Yourtaskistoexamineabookwiththedesklamp. Youhavelocatedthe
bookonbed1andpickeditup. Next,Youneedtofindthedesklamptoexaminethebook
underitslight.
Thought: Thedesklampisn’tonthedesk. Ineedtolookaroundtheroomtofindwherethe
desklampislocated.
Action: gotodresser1
Observation: Onthedresser1,youseeacellphone1,adesklamp1,andakeychain2.
StateKnowledge: Yourtaskistoexamineabookwiththedesklamp. Youhavelocatedthe
bookonthebedandpickeditup,nowyoufindadesklamponadresser.
Figure8: TrainingDataExampleforWorldKnowledgeModel.
F CaseStudy
InFigure9, welistthetrajectoriesofETOandourWKMwithinthesametaskinALFWorldto
illustratehowworldknowledgefunctions. Therationalesbeforeeachactionhavebeenomittedto
guaranteeaclearillustration. Thetaskistocleansomesoapbarandputitincabinet.
Initially,ETOblindlysearchesforthesoapbarinthecountertopandcabinet,introducinga
lotofirrelevantinformationandunnecessarycontext.Inthelaterstagesofplanning,ETOexperiences
thehallucinationandexecutestheputactionafterclosethecabinet, causingtheenvironment
tobecomeunrecognizableandresultinginacollapse. Onthecontrary,guidedbytaskknowledge,
WKMdirectlyidentifiedthepossiblelocationsofthesoapbarandsuccessfullyfounditinthe
first attempt. Subsequently, WKM efficiently completed the task with precision, adhering to the
constraintsofstateknowledge.
G WinRateofPlanningSteps
SeeFigure10.
H Prompts
Inthissection,weillustrateallthepromptsusedinourpaper. Partofourpromptsaretakenfrom
AutoGuide[6].
H.1 TaskKnowledgeSynthesisPrompt
SeeFigure11.
H.2 StateKnowledgeSummarizationPrompt
SeeFigure12.
H.3 Dataset-LevelKnowledgePrompt
SeeFigure13.
18CaseStudyofWKM
ETO WKM
TaskInstruction: Youareinthemiddleofa TaskInstruction: ...
room.Lookingquicklyaroundyou,youseea TaskKnowledge: Whenthetaskistoclean
cabinet4,acabinet3,acabinet2,acabinet1, andplaceanobject,youshouldfirstlocatethe
acountertop1,agarbagecan1,ahandtowel- object. Thepossiblelocationsofthesoapbar
holder2,ahandtowelholder1,asinkbasin2, are: toilet,sinbasin. Oncetheobjectisfound,
asinkbasin1,atoilet1,atoiletpaperhanger youshouldcleanitusingthesinkbasin,then
1,andatowelholder1. Yourtaskisto: clean put it in the cabinet. The action workflows
somesoapbarandputitincabinet. areasfollows: 1. Gototheplacewherethe
#Blindtrial-and-errorbegins. objectislocated. 2. Taketheobjectfromthe
Agent: gotocountertop1 place. 3. Cleantheobjectusingthesinkbasin.
Observation: Onthecountertop1,youseea 4. Gotothecabinet. 5. Puttheobjectin/on
candle3,acandle1,asoapbottle1,aspray- thecabinet.
bottle2,andatoiletpaper2. Agent: gototoilet1
Agent: gotocabinet1 Observation: Onthetoilet1,youseeasoap-
Observation: Thecabinet1isclosed. bar1,andaspraybottle3.
Agent: opencabinet1 StateKnowledge:Yourtaskistocleansome
Observation: Youopenthecabinet1. The soapbarandputitincabinet. Nowyouare
cabinet1isopen. Init,youseeasoapbottle checkingtoilet1andthereisasoapbar1.
2. Agent: takesoapbar1fromtoilet1
Agent: closecabinet1 Observation:Youpickupthesoapbar1from
Observation: Youclosethecabinet1. thetoilet1.
#Blindtrial-and-errorends. StateKnowledge:Yourtaskistocleansome
Agent: gototoilet1 soapbar and put it in cabinet. You have
Observation: Onthetoilet1,youseeasoap- checked the toilet 1 and found the soapbar
bar1,andaspraybottle3. 1.
Agent: takesoapbar1fromtoilet1 Agent: gotosinkbasin1
Observation:Youpickupthesoapbar1from Observation: On the sinkbasin 1, you see
thetoilet1. nothing.
Agent: gotosinkbasin1 StateKnowledge:Yourtaskistocleansome
Observation: On the sinkbasin 1, you see soapbar and put it in cabinet. You have
nothing. checked the toilet 1 and found the soapbar
Agent: cleansoapbar1withsinkbasin1 1. Nowyouareatthesinkbasin.
Observation: Youcleanthesoapbar1using Agent: cleansoapbar1withsinkbasin1
thesinkbasin1. Observation: Youcleanthesoapbar1using
Agent: gotocabinet1 thesinkbasin1.
Observation: Thecabinet1isclosed. StateKnowledge:Yourtaskistocleansome
Agent: opencabinet1 soapbar and put it in cabinet. You have
Observation: Youopenthecabinet1. The cleanedthesoapbar1usingthesinkbasin1.
cabinet1isopen. Init,youseeasoapbottle Agent: gotocabinet1
2. Observation: Thecabinet1isclosed.
Agent: closecabinet1 StateKnowledge:Yourtaskistocleansome
Observation: Youclosethecabinet1. soapbar and put it in cabinet. You have
#Hallucinatoryaction. cleanedthesoapbar1usingthesinkbasin1.
Agent: putsoapbar1in/oncabinet1 Nowyouarecheckingcabinet1.
Observation: Nothinghappens. Agent: opencabinet1
Agent: checkcabinet1 Observation: Youopenthecabinet1. Init,
Observation: Nothinghappens. youseeasoapbottle2.
Agent: gotoexit StateKnowledge:Yourtaskistocleansome
Observation: Nothinghappens. soapbar and put it in cabinet. You have
Agent: exit cleaned the soapbar 1 and opened the cab-
Observation: Nothinghappens. inet1.
...... Agent: putsoapbar1in/oncabinet1
Reward: 0.0 Reward: 1.0
Figure9: CasestudyforourWKM.
19WKM ETO KnowAgent NAT
ALFWorld seen 37.1 12.8 31.4 18.7
ALFWorld unseen 38.8 14.2 20.9 26.1
WebShop test 65.5 11.0 15.0 8.5
SciWorld seen 47.4 17.1 17.5 18.0
SciWorld unseen 50.5 14.4 17.0 18.1
0 25 50 75 100
Win Rate (%)
Figure10: WinRateofPlanningSteps. Wechoosethemethodwiththeshorteststepsforeachtaskand
calculatetheproportion.
PromptforTaskKnowledgeSynthesis
TaskKnowledge
PromptforSynthesis: Iwillprovideyouwithananalysisofbothasuccessfultrajectory
andanexploredtrajectoryforthesametask. Bycomparingthetwo,wecanidentifythekey
factorsthatcontributetosuccess. Basedonthisanalysis,youneedtogeneratetask-related
taskknowledgetohelpincreasethesuccessrateoffutureendeavors.
SuccessTrajectory: Success_T
ExploredTrajectory: Explored_T
The task knowledge should specify what to do in what task. Here is a task knowledge
example:
TaskKnowledgeExample
Youshouldmakeyouranswerconcise. Putyouranswerinthisformat: TaskKnowledge:
When... youshould(orshouldnot)... Theactionworkflowsare: ...
Figure11: PromptforTaskKnowledgeSynthesis.
PromptforStateKnowledgeSynthesis
StateKnowledge
PromptforSynthesis: You’llgetasegmentofatrajectoryofatext-basedtasktask. Your
taskistogenerateabriefandgeneralstateknowledgeofthenowtaskstatefollowing"State
Knowledge: ". Keepitwiseandgeneralforthesametask. Hereisanexample:
StateKnowledgeExample
Nowit’syourturn. Hereisthetrajectory:
Trajectory
Makesureyouroutputiswithin128tokens.
Putyouranswerinthisformat: StateKnowledge: ...
Figure12: PromptforStateKnowledgeSummarization.
20TaskKnowledgeexample
AlfworldTaskKnowledgeexample
Whenpickinganobject,heatit,andplaceit,youshouldfirstgotothepossiblelocationsof
theobject,thentaketheobject,heatitwithmicrowave,andputitinplace.
Theactionworkflowsareasfollows:
1)gotoreceptacle
2)takeobjectfromreceptacle
3)heatobjectwithreceptacle
4)gototheplacetoputtheobject
5)putobjectin/onreceptacle
WebshopTaskKnowledgeexample
Whenlookingforanobjectyouwanttobuy,youshouldfirstsearchwithrelevantkeywords
tailoredtotheproductyouarelookingfor,andthenclicktherelevanttagtoviewtheproduct
details,ifthedescriptionmatchesthecharacteristicsofthetargetitem,click[buynow].
Theactionworkflowsareasfollows:
1) search with keywords or examples, if you are searching for a laptop, you might
search[laptop,14-inch,IntelCorei7]
2)clickthemostrelevanttagtoviewthedetailedproductpage.
3)checktheproductdetailsonebyone,likecolor,size,type,andprice,andmakesurethe
priceiswithinbudget.
4)iffindtherightitems,click[buynow]tobuyit.
SciworldTaskKnowledgeexample
Whentaskedwithboilingapplejuice,focusonlocatingthekitchenfirst. Then,locatethe
applejuiceinthefridge. Activatethestove,pourtheapplejuiceintoametalpot,andmove
themetalpottothestove. Monitorthestoveuntiltheapplejuicereachesaboilingpoint.
Onceboiled,removethepotfromthestove.
Theactionworkflowsare:
1)teleporttothekitchen.
2)lookaroundtofindtheapplejuiceinthefridge.
3)activatethestove.
4)pourapplejuiceintoametalpot.
5)movethemetalpottothestove.
6)lookatstove.
7)examineapplejuicetoconfirmboiling.
8)repeatstep6,7untilapplejuiceisboiled.
Figure13: Dataset-LevelTaskKnowledgeExamples.
I EthicsStatement
Thisresearchwasconductedfollowingtheethicalstandardsandbestpractices. Allourexperiments
use publicly available datasets (as detailed in Appendix B), avoiding ethical concerns related to
privacy, confidentiality, or misuse of personal biological information. However, despite our best
efforts,itisnotavoidableifsomeonemaliciouslymodifiestheworldknowledgemodeltocontradict
theworld’sknowledgeandleadstheagenttoengageinunethicalbehavior.
21