Global Behavior of Learning Dynamics in Zero-Sum
Games with Memory Asymmetry
YumaFujimoto KaitoAriu
CyberAgent CyberAgent
TheUniversityofTokyo kaitoariu@gmail.com
SokenUniversity
fujimoto.yuma1991@gmail.com
KenshiAbe
CyberAgent
TheUniversityofElectro-Communications
abekenshi1224@gmail.com
Abstract
Thisstudyexaminestheglobalbehaviorofdynamicsinlearningingamesbetween
twoplayers,XandY.Weconsiderthesimplestsituationformemoryasymmetry
betweentwoplayers: XmemorizestheotherY’spreviousactionandusesreactive
strategies,whileYhasnomemory.Althoughthismemorycomplicatesthelearning
dynamics,wediscovertwonovelquantitiesthatcharacterizetheglobalbehavior
ofsuchcomplexdynamics. OneisanextendedKullback-Leiblerdivergencefrom
the Nash equilibrium, a well-known conserved quantity from previous studies.
The other is a family of Lyapunov functions of X’s reactive strategy. These
twoquantitiescapturetheglobalbehaviorinwhichX’sstrategybecomesmore
exploitative, and the exploited Y’s strategy converges to the Nash equilibrium.
Indeed, wetheoreticallyprovethatY’sstrategygloballyconvergestotheNash
equilibrium in the simplest game equipped with an equilibrium in the interior
of strategy spaces. Furthermore, our experiments also suggest that this global
convergence is universal for more advanced zero-sum games than the simplest
game. This study provides a novel characterization of the global behavior of
learningingamesthroughacoupleofindicators.
1 Introduction
Learning in games targets how multiple agents learn their optimal strategies in the repetition of
games[1]. Thesetofsuchplayers’beststrategiesisdefinedasNashequilibrium[2],whereevery
playerhasnomotivationtochangehis/herstrategy. However,thisequilibriumishardtocompute
in general because one’s best strategy depends on the others’ strategies. Indeed, the behavior of
multi-agentlearningiscomplicatedinzero-sumgames,whereplayersconflictintheirpayoffs. Even
whenplayerstrytolearntheiroptimalstrategiesthere,theirstrategiesoftencyclearoundtheNash
equilibriumandfailtoconvergetotheequilibrium.
Inordertounderstandsuchstrangebehaviors,whichareuniqueinmulti-agentlearning,thedynamics
of how multiple agents learn their strategies, say, learning dynamics, are frequently studied [3,
4, 5, 6]. The representative dynamics of interest are the replicator dynamics, which is based on
the evolutionary dynamics in biology [7, 8, 9, 10, 11]. These dynamics are also known as the
multiplicative weight updates (MWU) in its discrete-time version [12, 13]. Furthermore, their
connectiontootherrepresentativelearningdynamics,suchasgradientascent[14,15,16,17]and
Preprint.Underreview.
4202
yaM
32
]TG.sc[
1v64541.5042:viXraQ-learning[18,19,20],shouldbenoted. Suchreplicatordynamicsareknowntobecharacterized
by Kullbuck-Leibler (KL) divergence, which is the distance from the Nash equilibrium to the
players’presentstrategies. ThisKLdivergenceisconservedduringthelearningdynamics,andthe
distancefromtheNashequilibriumisinvariant. FollowtheRegularizedLeader(FTRL)isaclass
oflearningalgorithmsincludingthereplicatordynamicsandalsohasitsconservedquantity,which
isthesummationofdivergencesforalltheplayers[21,22]. Tosummarize,suchcomplexlearning
dynamicshavebeendiscussedbasedontheirconservedquantity.
In this study, we define memory as an agent’s ability to change its action choice depending on
the outcome of past games. By definition, this memory allows the agent to make more complex
andintelligentdecisions. Whenmemoryisintroducedintoanormal-formgame,theplayerscan
achieveawiderrangeofstrategiesastheNashequilibria(knownasFolktheorem[23]). Furthermore,
memoryisalsointroducedintolearningalgorithms,suchasreplicatordynamics[24,25,26,27]and
Q-learning[28,29,30,31,32]. Here,sincethismemorycausesfeedbackfromthepast,theglobal
dynamicsofsuchlearningalgorithmsbecomemorecomplex. Indeed,replicatordynamicsdiverge
fromtheNashequilibriumundersymmetricmemorylengthsbetweenplayers[26],whileconverging
underasymmetricmemorylengths[27].Here,KLdivergenceisnolongerusefultocapturetheglobal
dynamicsbecauseitincreasesordecreasesovertime. Theanalysisofthedynamicsinwith-memory
gamesislimitedtothelocal,linearizedstabilityanalysisinthevicinityofNashequilibrium[27].
Tosummarize,sincememorycruciallycomplicateslearningdynamics,theglobalbehaviorofthe
dynamicsisstillunexplored.
Thisstudyprovidesthefirsttheoreticalanalysisoftheglobalbehavioroflearninginwith-memory
games. We assume games where their memory structure is simplest and asymmetric; One side
adopts a reactive strategy that can memorize the other’s previous action [33, 34, 35, 24, 36, 25],
whiletheotherhasnomemory. Inordertocharacterizetheglobalbehaviorofsuchwith-memory
games,weextendKLdivergenceandprovethatsuchextendeddivergenceincreasesordecreases
Fig. 1w-2ithtimedependingonthereactivestrategy(seeFig.1A).Furthermore, weproposeafamilyof
Lyapunovfunctionsthatcharacterizethedynamicsofreactivestrategy(seeFig.1B).TheseLyapunov
functionsshowthatthewith-memorysidemonotonicallylearnstoexploittheno-memoryside. As
anapplicationofthesefunctions,weprovethattheconvergencefromarbitraryinitialstrategies,i.e.,
globalconvergence,occursinmatchingpennies. Wealsoexperimentallyconfirmthatsuchglobal
convergenceisobservedingeneralzero-sumgames.
A global behavior of D B global behavior of H
D H
3
H
2
X’s strategy H
1
Figure 1: A. Illustration of the global behavior of the conditional divergence, D(X,y). Three
trajectories (red, black, and blue) are plotted with the Nash equilibrium (the black star marker).
ThehorizontalandverticalaxesshowX’sstrategy(xst)andY’sstrategy(y )inmatchingpennies
1 1
(formulatedinFig.2). Thisdivergencedecreases(red: D˙ <0),cycles(black: D˙ =0),orincreases
(blue: D˙ >0)withtime. Thesethreelinesareplottedforthedifferentinitialstrategies,i.e.,X and
y. B.IllustrationoftheglobalbehaviorofthefamilyofLyapunovfunctions,H(X;δ). Thecolored
lineshowsatrajectory(frompurpletored)ofLyapunovfunctionsH ,H ,andH ,eachofwhich
1 2 3
isH(X;δ)forsomespecificδ. Thegraybrokenlinesaretheprojectionsoftheblacksolidlineto
H -H ,H -H ,andH -H planes. AllofH ,H ,andH monotonicallyincreasewithtime.
1 2 2 3 3 1 1 2 3
2
ygetarts
s’Y2 Preliminary
2.1 Settings
First,weformulatetwo-playernormal-formgames. Weconsidertwoplayers,denotedasXandY.
Fig. 2-2
X’sactionsaredenotedas{a } ,whileY’sare{b } . WhenXandYchoosea and
b
,theyobtainthepayoffsofui 1≤ ∈i≤ Rm aX
ndv
∈R,respecj tiv1 e≤ lj y≤ .m ThY us,alltheirpossiblepayoffi
sare
j ij ij
givenbythematrices,U :=(u ) andV :=(v ) . Here,whenV =−U holds,thegamesare
ij ij ij ij
calledzero-sum. Althoughourformulationoflearningalgorithmscanbeusedforgeneralgames,
thisstudyfocusesonzero-sumgames.
normal-form
Y
Y
game
y y
1 2 u u
conditional 11 12
u 11 u 12 x 1|j j ∈ 1 2
u u
U := X 21 22
u u x
|j
21 22 2
previous round
reactive strategy
present round memory
Figure2: Illustrationofgamesbetweenreactiveandzero-memorystrategies. Theareasurroundedby
themagentadottedlineshowsthenormal-formgame. Ineachround,Xchoosesactioni=1or2in
therow,followingitsstrategy,i.e.,theprobabilitydistributionofx=(x ,x ). Ontheotherhand,
1 2
Ychoosesactionj = 1or2inthecolumn,followingitsstrategy,i.e.,theprobabilitydistribution
of y = (y ,y ). Depending on their choices i and j, X receives a payoff u , given by a matrix
1 2 ij
formofU =(u ) =((u ,u ),(u ,u )). Furthermore,inzero-sumgames,Yreceives−u .
ij i,j 11 12 21 22 ij
Especiallyinthematchingpennies,theiractionsof1(2)correspondtothechoiceof“head”(“tail”)
ofacoin. Whentheirchoicesmatchi = j,Xwins,i.e.,u = u = 1(theorangeblocks). Else
11 22
whentheirchoicesmismatchi̸=j,Ywins,i.e.,u =u =−1(theblueblocks). Theareaoutside
12 21
ofthemagentadottedlineshowsthedifferenceduetoaneffectofmemory. Thegrayboxshowsthat
XmemorizesY’spreviousaction,representedasj =1or2. Thus,Xusesareactivestrategyandcan
chooseitsactionwiththeconditionalprobabilityofx andx forY’spreviousaction.
1|j 2|j
We assume that X can use reactive strategies, i.e., can change its action choice depending on
the other’s previous action. This reactive strategy is denoted as X := (x ) ∈
(cid:81) 1≤j≤mY∆mX−1,amatrixcomposedofm
Y
vectorseachofwhichareani e|j le1 m≤ ei n≤ tm oX f,1 a≤ mj≤ Xm −Y 1-
dimensionalsimplex. Here,x meanstheprobabilitythatXchoosesa intheconditionwhenY’s
i|j i
previousactionisb . Thus,Σ x =1shouldbesatisfiedforallj. Ontheotherhand,Yonlycan
j i i|j
useclassicalmixedstrategiesandchooseitsownactionwithoutreferencetothepreviousactions.
Thismixedstrategyisdenotedasy = (y j)
1≤j≤mY
∈ ∆mY−1, avectorwhichisanelementofa
(m −1)-dimensionalsimplex. Thus,Σ y =1shouldbesatisfied.
Y j j
2.2 Stationarystateandexpectedpayoff
Wenowdiscussthestationarystateandexpectedpayoffofrepeatedgames. SinceYdetermines
its action independent of the outcomes of previous rounds, X’s stationary strategy, defined as
xst := (xst) , is given by xst(x ,y) = Σ x y . Here, xst means the probability that X
i 1≤i≤mX i i j i|j j i
choosesa inthestationarystate.Furthermore,thestationarystateisdescribedasPst :=xst⊗ywith
i
useofxstandy. Last,X’sexpectedpayoffisgivenbyust(xst,y):=Σ Σ u pst =Σ Σ u xsty .
i j ij ij i j ij i j
2.3 Nashequilibrium
WenowdefinetheNashequilibriuminthenormal-formgame. Here,notethatthisequilibriumis
basedongameswithoutmemories,whereX’sstrategydoesnotrefertothepastgames,i.e.,x:=(x ) .
i i
3Byusingtheexpectedpayoffust(x,y)forgameswithoutmemories,theNashequilibrium(x∗,y∗)
isformulatedas
(cid:26) x∗ =argmax ust(x,y∗)
x . (1)
y∗ =argmin ust(x∗,y)
y
Fromthedefinition,ust(x,y)isthelinearfunctionforxandy,andtheNashequilibriumcondition
ischaracterizedbythegradientofsuchexpectedpayoffsas
(cid:26) ∂ust/∂x =Σ u y∗ =C (x∗ >0)
i j ij j i , (2)
∂ust/∂x =Σ u y∗ <C (x∗ =0)
i j ij j i
(cid:26) ∂ust/∂y =Σ u x∗ =C (y∗ >0)
j j ij i i . (3)
∂ust/∂y =Σ u x∗ >C (y∗ =0)
i j ij i i
Fromtheseconditions,foralliandj suchthatx∗ >0andy∗ >0hold,respectively,weobtain
i j
Σ u x∗ =Σ u y∗ =:u∗. (4)
i ij i j ij j
Letusinterpretthisequation. First,Σ u x∗ = u∗ meansthatwhenXtakesitsNashequilibrium
i ij i
strategy,itsownpayoffisfixedtou∗,independentofY’sstrategy. Ontheotherhand,Σ u y∗ =u∗
j ij j
similarlymeansthatY’sNashequilibriumstrategyfixesX’spayofftou∗. Inotherwords,eitherXor
YtakesitsNashequilibriumstrategy,theirpayoffsarefixed. Thisisthespecialpropertyinzero-sum
games.
2.4 Learningalgorithm: replicatordynamics
Letusdefinethereplicatordynamicsasarepresentativelearningalgorithm. X’sandY’sreplicator
dynamicsareformulatedas
(cid:18) dust dust(cid:19)
x˙ =x −Σ x , (5)
i|j i|j dx i i|jdx
i|j i|j
(cid:18) dust dust(cid:19)
y˙ =−y −Σ y . (6)
j j dy j j dy
j j
Here,followingthetheoremsin[26],X’sandY’sreplicatordynamicsincludethegradientforthe
expected payoff ust. Thus, the update of X’s strategy increases its payoff ust, while that of Y’s
strategydecreasestheother’spayoffust.
3 Novelquantitiescharacterizinglearningdynamics
ThissectionanalyzesthedynamicsofEqs.(5)and(6). First,wecomputeindetailthegradientterms,
whichappeartobecomplex. Next,asapreliminary,wedefinepositivedefinitematricesforsome
specialvectors. Basedonthisdefinition,weintroducetwoquantitiescharacterizingthedynamicsof
Eqs.(5)and(6): AnextendedKLdivergenceandafamilyofLyapunovfunctions.
3.1 Polynomialexpressionsoflearningdynamics
First,thegradienttermsinEqs.(5)and(6)arecomputedas
dust(xst(X,y),y) ∂xst(x ,y)∂ust(xst,y)
= i i (7)
dx ∂x ∂xst
i|j i|j i
=y Σ u y , (8)
j j′ ij′ j′
dust(xst(X,y),y) ∂ust(xst,y) ∂xst(x ,y)∂ust(xst,y)
= +Σ i i (9)
dy ∂y i ∂y ∂xst
j j j i
=Σ u xst+Σ x Σ u y . (10)
i ij i i i|j j′ ij′ j′
Here, we remark that Eqs. (5) and (6) are nonlinear functions of X and y, which is a feature of
learninginwith-memorygames. Notably,however,theseequationsarepolynomialexpressionswith
X andy. Suchpolynomialexpressionscannotbeseeninthegamesofothermemorylengths[26,27]
butarespecialinthegamesbetweenreactiveandnomemorystrategies.
43.2 Positivedefinitenessforzero-sumvectors
Next,letusintroduceadefinitenessofmatrices. Here,however,thisdefinitematrixisforvectors
whose elements are summed to 0, named “zero-sum vectors”. In mathematics, zero-sum vector
δ :=(δ ) satisfiesΣ δ =0butδ ̸=0.
k k k k
Definition1(Positivedefinitenessforzero-sumvectors). AsquarematrixM is“positivedefinite
forzero-sumvectors”whenforallvectorsδ ̸=0suchthatΣ δ =0,δ·(Mδ)<0holds.
k k
Thepositivedefinitenessforzero-sumvectorsconnectswithanordinarypositivedefinitenessbya
simpletransformationofamatrix(seeAppendixBfordetails).
3.3 ExtendedKullback-Leiblerdivergence
The first quantity is an extended version of divergence. Before considering the extension, we
introduce the classical version of divergence D , which is the function of X’s mixed strategies
c
(x:=(x i)
1≤i≤mX
∈∆mX−1)andY’smixedstrategies(y)as
D (x,y):=D (x∗∥x)+D (y∗∥y), (11)
c KL KL
D (p∗∥p):=p∗·logp∗−p∗·logp. (12)
KL
We now give an intuitive interpretation of this quantity. First, D (p∗∥p) is the KL divergence,
KL
meaningthedistancefromthereferencepointp∗ tothetargetpointp. Thus,D (x,y)meansthe
c
totaldistancefromtheNashequilibrium(x∗,y∗)tothecurrentstate(x,y).
Letusextendtheclassicaldivergencetothecaseofthisstudy,whereXreferstothepreviousaction
oftheotherandcanusereactivestrategiesX. Thisextendeddivergence,i.e.,D(X,y),isnamedthe
“conditional-sum”divergence,formulatedas
D(X,y):=Σ D (x∗∥x )+D (y∗∥y). (13)
j KL j KL
WenowremarkthedifferencebetweenD(X,y)andD (x,y). RecallthatX’sreactivestrategyis
c
definedas(x ) ,whichshowshowtochooseitsactionwiththeconditionthatYchoseb in
j 1≤j≤mY j
thepreviousround. Hence,D(X,y)representsthesummationofKLdivergencefromx∗tox for
j
alltheconditionsofj. Here,wealsoremarkthatwhenthereactivestrategydoesnotusememory,
i.e., x = xforallj, thisconditional-sumdivergencealsocapturesthebehavioroftheclassical
j
divergence(seeAppendixCfordetails).
Thisconditional-sumdivergencesatisfiesthefollowingtheorem(seeAppendixA.1foritsfullproof).
Theorem1(MonotonicdecreaseofDforpositivedefiniteXTU). IfXTU ispositivedefinitefor
zero-sumvector,D†(X;dy):=D˙(X,y)<0foralldy ̸=0.
ProofSketch. WecalculationD˙(X,y)inpractice. Inthecalculation,thecontributionofX’sgradient
(Eq. (8)) cancels out the contribution of the first term of the gradient of Y (Eq. (10)). (Here, we
remarkthatthesamecancelingoutalsooccursinthecalculationfortheconservationoftheclassical
divergenceD (x,y)ingameswithoutmemory.) However,thecontributionofthesecondtermof
c
Eq.(10)isspecialingamesofareactivestrategy. ByusingtheconstantpayoffconditionintheNash
equilibrium(Eqs.(4)),weobtain
D˙(X,y)=−dyTXTUdy(=:D†(X;dy)), (14)
wherewedefineddy :=y−y∗,whichmeansthedifferencefromY’sequilibriumstrategyandisa
zero-sumvector. Thus,whenXTU ispositivedefiniteforzero-sumvectors,dyTXTUdyisalways
positive,leadingtoD†(X;dy)<0foralldy ̸=0.
If XTU are always positive definite for zero-sum vectors, Thm. 1 gives the global behavior of
learningdynamicsasbelow.
Corollary1(ConvergenceofY’sstrategy). IfXTU continuestobepositivedefiniteforzero-sum
vectors,y =y∗holdsaftersufficientlylongtimepasses.
Proof. This corollary is straightforwardly proved by Thm. 1. If XTU is positive definite and if
y ̸=y∗,D†(X;dy)<0holds. Here,D(X,y)hasitslowerbound(min D(X,y)=0). Thus,
X,y
D†(X;dy)=0⇔y =y∗shouldbesatisfiedeventually.
53.4 FamilyofLyapunovfunctions
Furthermore,weintroduceaLyapunovfunction,whichcharacterizesthelearningdynamicsofX’s
reactivestrategy. Basedonanarbitraryzero-sumvectorδ :=(δ ) ,thisfunctionisdefinedas
i 1≤i≤mX
H(X;δ):=δTUlogXTδ. (15)
ThefollowingtheoremholdsforthisLyapunovfunction(seeAppendixA.2foritsfullproof).
Theorem 2 (Monotonic increase of H). For all δ such that Σ δ = 0, H(X;δ) monotonically
i i
increaseswithtime.
ProofSketch. ByusingEq.(4),wecalculate
H˙(X;δ)=(δTUdy)2(=:H†(y;δ)). (16)
ThismeansthatH†(y;δ)≥0forallδ.
LetusinterpretthefunctionofH(X;δ). Forsimplicity,weconsiderasimpleclassofzero-sum
vectors,δ =e −e forsomeiandi′ ̸=i,wheree denotestheunitvectorfori-thelement. Then,
i i′ i
wecalculatethefunctionasH(X;e −e )=u ·logx −u ·logx −u ·logx +u ·logx .
i i′ i i i i′ i′ i i′ i′
ThemonotonicincreaseofH(X;e −e )showsthatlogx becomescorrelatedtou ,andlogx to
i i′ i i i′
u . Here,x =(x ) representsthevectoroftheprobabilitiestochoosei-thactioninthecondition
i′ i i|j j
thattheotherchosej-thactioninthepreviousround. u =(u ) alsorepresentsthevectorofthe
i ij j
payoffsforactioni. Thus,thecorrelationbetweenlogx andu meansthatthelargeritsreward
i i
u is,themorefrequentlyXreturnsitsi-thactionagainsttheother’sj-thaction,inotherwords,X
ij
becomesmoreexploitative.
Thistheoremcharacterizesthestateoflearningdynamicsafterasufficientlylongtimepasses,as
follows.
Corollary2(ConvergenceofY’sstrategyordivergenceofH). Aftersufficientlylongtimepasses,
y =y∗holdsorH(X;δ)divergestoinfinityforallδ ̸=0suchthatΣ δ =0.
i i
Proof. SinceH(X;δ)≥0alwaysholdsforallδ,eitherH(X;δ)=0orH(X;δ)>0holdsinthe
finalstate. Theformerisequivalenttoy =y∗,whilethelatterisequivalenttoH(X;δ)→∞.
4 Globalbehavioroflearningdynamics
Now, Cors. 1 and 2 are utilized to capture the global behavior of the learning dynamics. One of
theglobalbehaviorsseeninthezero-sumgamesistheglobalconvergencetotheNashequilibrium
independentofX’sandY’sinitialstrategies. First,Cor.1guaranteesthatafterasufficientlylong
timepasses,eitherY’sstrategyconvergestotheequilibriumorH(X;δ)diverges. Sincetheformer
triviallyleadstosuchglobalconvergence,wenowconsiderthelattercondition,i.e.,H(X;δ) =
δTUlogXTδ →∞. Here,H(X;δ)=δTUlogXTδ →∞naivelyconnectsto−D†(X,dy)=
dyTXTUdy > 0 (explained in the next paragraph), meaning that XTU is positive definite for
zero-sumvectors. Ifso,Cor.2becomesapplicable,andeitherY’sstrategyconvergestotheNash
equilibrium.
WeexplaintheconnectionbetweenH(X;δ)=δTUlogXTand−D†(X,dy)=dyTXTUdy >
0. Here,H(X;δ)and−D†(X,dy)havetwocommonalities. Thefirstoneisthatδanddyareboth
zero-sumvectorsincommon. TheotheroneisthatbothH and−D†aregivenbytheproductofX’s
strategyX andthepayoffmatrixU. ThismeansthatthemoreX iscorrelatedtoU,thelargerboth
H and−D†are. Here,however,wealsoremarktwodifferencesofH from−D†;H considersthe
logarithminX’sstrategyX andthereversedorderintheproductofX andU.
Inthefollowing,weprovesuchglobalconvergenceofeitherplayer’sstrategyinthematchingpennies,
thesimplestgameequippedwithafull-supportequilibrium. Wealsoexperimentallyshowglobal
convergenceinmorecomplicatedgamesthanthematchingpennieslater.
4.1 Example: Matching-pennies
Let us define the matching-pennies game (see Fig. 2 for the illustration of its payoff matrix).
This game considers the action numbers of m = m = 2 and the payoff matrix of U =
X Y
6((u ,u ),(u ,u )) = ((+1,−1),(−1,+1)). The Nash equilibrium of this game is only
11 12 21 22
x∗ = y∗ = (1/2,1/2). This game is the simplest example of games that have a full-support
equilibrium. Inaddition,ithasbeenknownthatthereplicatordynamicsingameswithoutmemories
cycle around the Nash equilibrium and cannot reach the equilibrium. Nevertheless, the learning
dynamicsingamesofareactivestrategyachievetheconvergencetotheNashequilibriumforY’s
strategy. Thefollowingtheoremholds(seeAppendixA.3foritsfullproof).
Theorem 3 (Global convergence in matching-pennies). In the matching-pennies (U =
((+1,−1),(−1,+1))), Y’s strategy converges to the Nash equilibrium (y = y∗), independent
ofboththeplayers’initialstrategies.
ProofSketch. First,wedefineaspecialnotationintwo-actiongames;(x ,x ) =: (x ,1−x )
1|j 2|j j j
for j = 1,2. We also define the logit function of x as q := logx −log(1−x ). Here, note
j j j j
thatq > q ⇔ x > x . Bythedirectcalculation,weproveH(X;δ) ∝ q −q . Thus,Cor.2
1 2 1 2 1 2
showsthatafterasufficientlylongtimepasses,y =y∗holdsorH(X;δ)diverges,inotherwords,
H(X;δ) > 0 ⇔ q > q ⇔ x > x holds. Intheformercase, YobviouslyreachestheNash
1 2 1 2
equilibrium strategy, so we now assume the latter case (x > x holds). We can also prove that
1 2
x >x isequivalenttothepositivedefinitenessofXTU. BecauseXTU continuestoholdaftera
1 2
sufficientlylongtimepasses,y =y∗holdsafterafurtherlongtimepasses,meaningthatY’sstrategy
convergestoitsequilibrium.
5 Experimentalsupportsfortheory
Fig. 3-2
ThissectionexperimentallyshowstheglobalconvergencetotheNashequilibrium. First,wevisualize
themechanismofglobalconvergenceinmatchingpennies. FollowingThm.2,H(X;δ)increases
withtime,andXTU becomespositivedefinite. Afterthat,followingThm.1,thedistancefromthe
NashequilibriumD(X,y)decreaseswithtime. Next,wealsodemonstrateglobalconvergencefor
severalexamplesofmoreadvancedgamesthanmatchingpennies. Wehaveobservedtheglobal
convergenceinallzero-sumgamesasfarasweexperimented,eventhoughwecouldnotproveitin
generalzero-sumgames. AllthesimulationsinthissectionareperformedfollowingtheRunge-Kutta
fourth-ordermethodofEqs.(5)and(6)withasufficientlysmallstepsizeof5×10−2.
A B
y
q
2
x
2
q x
1 1
Figure3: A.Trajectoriesofq andq . Therainbowcontourplotindicatesthevalueofq −q . All
1 2 1 2
thetrajectoriesmonotonicallyincreaseq −q withtimeandconvergeintheareaofq > q in
1 2 1 2
theirfinalstates. B.Trajectoriesofthelearningdynamics. Theblackbrokenlinecorrespondstothe
regionofNashequilibria,xst =y =(1/2,1/2). Eachcoloredlineshowsatrajectoryofthelearning
dynamics. First,thecirclemarkersshowtheinitialstates. Followingthebluelines,thetrajectories
divergefromtheNashequilibria(D(X,y)increaseswithtime). However,thetrajectoriesstopto
divergeandswitchtoconvergetotheNashequilibria(D(X,y)decreases),followingtheredlines.
ThestarmarkersarethefinalstatesandcorrespondtooneoftheNashequilibria.
5.1 Visualizationofglobalconvergencemechanism
Letusseethatlearningdynamicsinthematchingpenniesconvergetotheequilibriumfollowing
the mechanism we provided. To analyze the matching pennies in which both the players have
twoactions, weusethenotationof(x ,x ) =: (x ,1−x ), q := logx −log(1−x ), and
1|j 2|j j j j j j
(y ,y )=:(y,1−y).
1 2
7Fig. 3A shows the dynamics of q and q . Here, the colors indicate the contour plot for q −
1 2 1
q , showing that H(X;δ) ∝ q −q monotonically increases with time and thus Thm. 2 holds.
2 1 2
Furthermore, we also see that X’s strategy reaches the region of q > q ⇔ x > x after a
1 2 1 2
sufficientlylongtimepasses. Intheregion,XTU ispositivedefinite,andthusThm.1isapplicable
aftersufficientlylongtimepasses.
Next,Fig.3Bplotstheglobalbehaviorofthelearningdynamics,whichisdescribedbythethree
parametersofx ,x ,andy. ThegraylineshowstheregionthatcorrespondstotheNashequilibrium,
1 2
i.e.,xst(X,y)=y =(1/2,1/2). Furthermore,thecoloredlinesshowexampletrajectoriesofthe
learningdynamics. ThebluepartofthelineshowsthatD(X,y)increasesatthebeginningofthe
learningdynamics. Thispartisintheregionofx <x ,followingThm.1. Afterthat,theredpart
Fig. 4 1 2
showsthatD(X,y)decreases,andthelearningdynamicsconvergetotheequilibrium. Thispartis
intheregionofx >x ,followingThm.1.
1 2
5.2 Globalconvergenceinmoreadvancedzero-sumgames
Theglobalconvergenceisobservedinmoreadvancedzero-sumgamesthanthematchingpennies.
Fig.4showsthreerepresentativeexamples. Inalltheexamples,webasicallyassumethatthereisno
dominant(pure-)strategy,whereX’si=σ(j)-thactionisadvantageousforY’sj-thaction,andvise
versa,forthepermutationσ(i.e.,σ(1)=2,σ(2)=3,σ(3)=4,andσ(4)=1). Inotherwords,the
payoffmatricessatisfyu =2andu =−2. Threepanelstakedifferentvaluesintheother
iσ(i) σ(j)j
elementsoftheirpayoffmatrix.
A random B C
0 −1
+2 −2 +2 −2 +2 −2 number +2 −2 ++22 −−22 ++22 −−22 +2 −2
−2 +2 −2 +2 −2 +2 −2 +2 −−22 ++22 −−22 ++22 −2 +2
−2 +2 −2 +2 −2 +2 −2 +2 −−22 ++22 −−22 ++22 −2 +2
+2 −2 +2 −2 +2 −2 +2 −2 ++22 −−22 ++22 −−22 +2 −2
initial y initial x
final y final x
Figure4: Globalconvergenceinthreeadvancedgames. Inallthegames,weconsidertherock-paper-
scissors-likegameextendedtothefour-actioncase,wherethesecond,third,fourth,andfirstactions
wintheother’sfirst,second,third,andfourthactions,respectively. Thewinnerreceivesthepayoff
of2(theorangeblocksinthematricesforthewinningofX),whilethelosersendsthepayoffof
2(theblueblocks). Wenowintroducethreevariantsfortheotherblocksinthepayoffmatrix. A.
Weseteachoftheotherblocksbyrandomnumbersin[−1,1](thegrayblocks). Then,Y’sstrategy
convergestotheuniqueNashequilibrium(theredstarmarker)independentofitsinitialstate(the
bluecirclemarkers). B.Weseteachoftheotherblocksby0,wherethepayoffmatrixdegenerates.
Y’s strategy converges to one of the Nash equilibria (the line consisting of the red star markers)
dependingonitsinitialstate. C.Onlytheblockfortheinteractionbetweenanactionissetto−1,
andtheothersare0. Ifso,X’sstrategyconvergestotheuniqueNashequilibrium(theorangestar
markers)independentofitsinitialstate(thegreencirclemarkers). Instead, Y’sstrategiesdonot
converge.
First,Fig.4Ashowsthecasewheretheotherelementsofthepayoffmatrixarerandomnumbers
followingtheuniformdistributionof[−1,1]. Inthiscase,thepayoffmatrixdoesnotdegenerate. In
otherwords,XhasnotwodifferentstrategiesthatgenerateequalpayoffsforallY’sstrategies. In
mathematics,thereexistsnoa = (a i)
1≤i≤mX
∈ RmX suchthatΣ ia iu
i
= 0. Weobservethatthe
singleNashequilibriumexists,andY’sstrategyalwaysconvergestoitsequilibriumindependentof
X’sandY’sinitialstrategies.
Second,Fig.4Bshowsthecasewherealltheotherelementstake0. Inthiscase,thepayoffmatrix
degenerates;Indeed,u +u =0holdsfor(i,i′)=(1,3),(2,4). ContinuousNashequilibriaare
i i′
8seenthere,x∗ =r (0,1/2,0,1/2)+(1−r )(1/2,0,1/2,0)andy∗ =r (0,1/2,0,1/2)+(1−
X X Y
r )(1/2,0,1/2,0)forall0≤r and0≤r ≤1. WeobservethatY’sstrategyconvergestooneof
Y X Y
itsequilibriadependingonX’sandY’sinitialstrategies.
Third,Fig.4Cshowsthecasewheretheotherelementstake0inprincipleexceptforu =−1. In
11
thiscase,thepayoffmatrixdegeneratesbecauseu +u =0holdsfor(i,i′)=(2,4). Furthermore,
i i′
the only Nash equilibrium exists on the boundary of strategy spaces, x∗ = (0,1/2,0,1/2), and
y∗ = (1/2,0,1/2,0). Asfarasweexperiment,whenthepayoffmatrixdegeneratesandonlythe
boundaryNashequilibriumexists,Y’sstrategydoesnotconvergetoitsequilibriumexceptionally.
Nevertheless,weobservethatX’sstrategyconvergestoitsequilibriuminsteadofY’s.
6 Conclusion
Thisstudyconsideredthesimplestsituationofmemoryasymmetrybetweentwoplayers;onlyplayer
X memorizes the other’s previous action, while player Y cannot. We formulated their learning
dynamics based on the replicator dynamics. Although the existence of memory complicates the
dynamics,wecapturedtheglobalbehaviorofthelearningdynamicsbyintroducingtwonewquantities.
Oneistheconditional-sumdivergence,whichisanextensionofthepreviousdivergencetothecase
ofgamesofareactivestrategy. Thetimechangeofthisextendeddivergenceischaracterizedby
thedefinitenessofXTU. TheotherisafamilyofLyapunovfunctions,characterizedbyUlogXT.
Thisisprovedtomonotonicallyincreasewithtime. Asavalidapplicationofthesetwoquantities,
we proved the global convergence in the learning dynamics in the simplest game equipped with
aninteriorNashequilibrium. Wefurtherexperimentallyobservedtheglobalconvergenceinmore
advanced zero-sum games. It is still a conjecture whether the learning dynamics in games of a
reactivestrategyconvergetotheNashequilibriumeveningeneralzero-sumpayoffmatrices. This
studyprovidesnovelandvalidindicatorstoanalyzedynamicsinlearningingames.
References
[1] DrewFudenbergandDavidKLevine. Thetheoryoflearningingames,volume2. MITpress,
1998.
[2] JohnFNashJr. Equilibriumpointsinn-persongames. ProceedingsoftheNationalAcademyof
Sciences,36(1):48–49,1950.
[3] KarlTuyls,PieterJan’THoen,andBramVanschoenwinkel.Anevolutionarydynamicalanalysis
ofmulti-agentlearninginiteratedgames. InAAMAS,volume12,pages115–153.Springer,
2006.
[4] LucianBusoniu,RobertBabuska,andBartDeSchutter. Acomprehensivesurveyofmultia-
gentreinforcementlearning. IEEETransactionsonSystems,Man,andCybernetics,PartC
(ApplicationsandReviews),38(2):156–172,2008.
[5] Karl Tuyls and Gerhard Weiss. Multiagent learning: Basics, challenges, and prospects. Ai
Magazine,33(3):41–41,2012.
[6] DaanBloembergen,KarlTuyls,DanielHennes,andMichaelKaisers. Evolutionarydynamicsof
multi-agentlearning: Asurvey. JournalofArtificialIntelligenceResearch,53:659–697,2015.
[7] PeterDTaylorandLeoBJonker. Evolutionarystablestrategiesandgamedynamics. Mathe-
maticalbiosciences,40(1-2):145–156,1978.
[8] DanielFriedman.Evolutionarygamesineconomics.Econometrica:JournaloftheEconometric
Society,pages637–666,1991.
[9] Tilman Börgers and Rajiv Sarin. Learning through reinforcement and replicator dynamics.
JournalofEconomicTheory,77(1):1–14,1997.
[10] JosefHofbauer,KarlSigmund,etal. Evolutionarygamesandpopulationdynamics. Cambridge
universitypress,1998.
9[11] MartinANowakandKarlSigmund. Evolutionarydynamicsofbiologicalgames. Science,
303(5659):793–799,2004.
[12] SanjeevArora,EladHazan,andSatyenKale. Themultiplicativeweightsupdatemethod: a
meta-algorithmandapplications. Theoryofcomputing,8(1):121–164,2012.
[13] JamesPBaileyandGeorgiosPiliouras. Multiplicativeweightsupdateinzero-sumgames. In
EC,pages321–338,2018.
[14] SatinderSingh,MichaelJKearns,andYishayMansour.Nashconvergenceofgradientdynamics
ingeneral-sumgames. InUAI,pages541–548,2000.
[15] Michael Bowling and Manuela Veloso. Multiagent learning using a variable learning rate.
ArtificialIntelligence,136(2):215–250,2002.
[16] MartinZinkevich. Onlineconvexprogrammingandgeneralizedinfinitesimalgradientascent.
InICML,pages928–936,2003.
[17] Michael Bowling. Convergence and no-regret in multiagent learning. In NeurIPS, pages
209–216,2004.
[18] ChristopherJCHWatkinsandPeterDayan. Q-learning. Machinelearning,8(3):279–292,1992.
[19] AamalAbbasHussain,FrancescoBelardinelli,andGeorgiosPiliouras.Asymptoticconvergence
andperformanceofmulti-agentq-learningdynamics. InAAMAS,pages1578–1586,2023.
[20] AamalHussain,FrancescoBelardinelli,andGeorgiosPiliouras. Beyondstrictcompetition:
approximateconvergenceofmulti-agentq-learningdynamics. InIJCAI,pages135–143,2023.
[21] PanayotisMertikopoulosandWilliamHSandholm. Learningingamesviareinforcementand
regularization. MathematicsofOperationsResearch,41(4):1297–1324,2016.
[22] PanayotisMertikopoulos,ChristosPapadimitriou,andGeorgiosPiliouras. Cyclesinadversarial
regularizedlearning. InSODA,pages2703–2717,2018.
[23] DrewFudenbergandEricMaskin. Thefolktheoreminrepeatedgameswithdiscountingor
withincompleteinformation. InAlong-runcollaborationonlong-rungames,pages209–230.
WorldScientific,2009.
[24] YumaFujimotoandKunihikoKaneko. Emergenceofexploitationassymmetrybreakingin
iteratedprisoner’sdilemma. PhysicalReviewResearch,1(3):033077,2019.
[25] YumaFujimotoandKunihikoKaneko. Exploitationbyasymmetryofinformationreference
in coevolutionary learning in prisoner’s dilemma game. Journal of Physics: Complexity,
2(4):045007,2021.
[26] Yuma Fujimoto, Kaito Ariu, and Kenshi Abe. Learning in multi-memory games triggers
complexdynamicsdivergingfromnashequilibrium. InIJCAI,2023.
[27] YumaFujimoto,KaitoAriu,andKenshiAbe. Memoryasymmetrycreatesheteroclinicorbitsto
nashequilibriuminlearninginzero-sumgames. InAAAI,2024.
[28] NaokiMasudaandHisashiOhtsuki. Atheoreticalanalysisoftemporaldifferencelearningin
theiteratedprisoner’sdilemmagame. Bulletinofmathematicalbiology,71:1818–1850,2009.
[29] WolframBarfuss. Reinforcementlearningdynamicsintheinfinitememorylimit. InAAMAS,
pages1768–1770,2020.
[30] YukiUsuiandMasahikoUeda. Symmetricequilibriumofmulti-agentreinforcementlearning
inrepeatedprisoner’sdilemma. AppliedMathematicsandComputation,409:126370,2021.
[31] JanuszMMeylahn,LarsJanssen,etal. Limitingdynamicsforq-learningwithmemoryonein
symmetrictwo-player,two-actiongames. Complexity,2022,2022.
10[32] MasahikoUeda. Memory-twostrategiesformingsymmetricmutualreinforcementlearning
equilibrium in repeated prisoners’ dilemma game. Applied Mathematics and Computation,
444:127819,2023.
[33] MartinNowak. Stochasticstrategiesintheprisoner’sdilemma. Theoreticalpopulationbiology,
38(1):93–112,1990.
[34] HisashiOhtsuki. Reactivestrategiesinindirectreciprocity. JournalofTheoreticalBiology,
227(3):299–314,2004.
[35] SeungKiBaek,Hyeong-ChaiJeong,ChristianHilbe,andMartinANowak.Comparingreactive
andmemory-onestrategiesofdirectreciprocity. Scientificreports,6(1):25676,2016.
[36] Laura Schmid, Christian Hilbe, Krishnendu Chatterjee, and Martin A Nowak. Direct reci-
procitybetweenindividualsthatusedifferentstrategyspaces. PLoSComputationalBiology,
18(6):e1010149,2022.
11Appendix
A Proofs
A.1 ProofofThm.1
Proof. ThedynamicsofD(X,y)arecalculatedas
x˙ y˙
D˙(X,y)=−Σ Σ x∗ i|j −Σ y∗ j
i j ix j jy
i|j j
(cid:18) ∂ust ∂ust(cid:19) (cid:18) ∂ust ∂ust(cid:19)
=−Σ Σ x∗ −Σ x −Σ y∗ −Σ y
i j i ∂x i i|j∂x j j ∂y j j ∂y
i|j i|j j j
=−Σ Σ x∗y (cid:0) Σ u y −Σ x Σ u y (cid:1) +Σ y∗(cid:0) Σ u xst−Σ y Σ u xst(cid:1)
i j i j j′ ij′ j′ i i|j j′ ij′ j′ j j i ij i j j i ij i
+Σ
y∗(cid:0)
Σ x Σ u y −Σ y Σ x Σ u y
(cid:1)
j j i i|j j′ ij′ j′ j j i i|j j′ ij′ j′
=−Σ Σ Σ x∗y u y +Σ Σ Σ x y u y +Σ Σ y∗u xst−Σ Σ y u xst
i j j′ i j ij′ j′ i j j′ i|j j ij′ j′ i j j ij i i j j ij i
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=:(A)=u∗ =:(B)=ust =:(C)=u∗ =:(C)=ust
+Σ Σ Σ y∗x u y −Σ Σ Σ y x u y
i j j′ j i|j ij′ j′ i j j′ j i|j ij′ j′
(cid:124) (cid:123)(cid:122) (cid:125)
=:(E)=−dyTXTUdy
=−u∗+ust+u∗−ust−dyTXTUdy
=−dyTXTUdy
(=:D†(X;dy)). (17)
Here,wecalculated(A)-(D)as
(A)=Σ Σ Σ x∗y u y
i j j′ i j ij′ j′
=Σ Σ y y (Σ x∗u )
j j′ j j′ i i ij′
=Σ Σ y y u∗
j j′ j j′
=u∗, (18)
(B)=Σ Σ Σ x y u y
i j j′ i|j j ij′ j′
=Σ Σ (Σ x y )u y
i j′ j i|j j ij′ j′
=Σ Σ xstu y
i j′ i ij′ j′
=ust, (19)
(C)=Σ Σ y∗u xst
i j j ij i
=Σ (Σ y∗u )xst
i j j ij i
=u∗Σ xst
i i
=u∗, (20)
(D)=Σ Σ y u xst
i j j ij i
=ust, (21)
(E)=Σ Σ Σ y∗x u y −Σ Σ Σ y x u y
i j j′ j i|j ij′ j′ i j j′ j i|j ij′ j′
=−Σ Σ Σ (y −y∗)x u y
i j j′ j j i|j ij′ j′
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125)
=:dyj =dy j′+y j∗ ′
=−Σ Σ Σ dy x u dy −Σ Σ Σ dy x u y∗
i j j′ j i|j ij′ j′ i j j′ j i|j ij′ j′
=−Σ Σ Σ dy x u dy −u∗Σ Σ y x
i j j′ j i|j ij′ j′ i j j i|j
=−Σ Σ Σ dy x u dy
i j j′ j i|j ij′ j′
=−dyTXTUdy. (22)
Here,notethatdy = y−y∗ iszero-sumvector. Thus,ifXTU ispositivedefiniteforzero-sum
vectors,D†(X;dy)≤0holdsindependentofdy,meaningthatD(X,y)monotonicallydecreases
withtime.
12A.2 ProofofThm.2
Proof. ByEqs.(5)and(8),weobtain
x˙
H˙(X,y)=Σ Σ Σ δ u i′|j δ
i i′ j i ijx i′
i′|j
(cid:18) ∂ust ∂ust(cid:19)
=Σ Σ Σ δ u −Σ x δ
i i′ j i ij ∂x i i|j∂x i′
i|j i|j
=Σ Σ Σ δ u y δ (Σ u y −Σ x Σ u y )
i i′ j i ij j i′ j′ i′j′ j′ i′′ i′′|j j′ i′′j′ j′
=Σ Σ Σ Σ δ u y δ u y −Σ Σ Σ Σ (Σ δ )δ u y x u y
i i′ j j′ i ij j i′ i′j′ j′ i i′′ j j′ i′ i′ i ij j i′′|j i′′j′ j′
(cid:124) (cid:123)(cid:122) (cid:125)
=0
=Σ Σ Σ Σ δ u y δ u y
i i′ j j′ i ij j i′ i′j′ j′
=(Σ Σ δ u y )2
i j i ij j
=(Σ Σ δ u dy +Σ Σ δ u y∗)2
i j i ij j i j i ij j
(cid:124) (cid:123)(cid:122) (cid:125)
=u∗Σiδi=0
=(δTUdy)2
(=:H†(y;δ)). (23)
ThisequationshowsthatH†(y;δ)≥0alwaysholds.Furthermore,H†(y;δ)=0forallδissatisfied
ifandonlyifdy =0⇔y =y∗.
A.3 ProofofThm.3
Proof. Now, by using the special properties of two-action games, let us simplify the notation of
X :={x } ,y :={y } ,andδ :={δ } . Fromthedefinition,Σ x =1⇔x +x =1for
i|j i,j j j i i i i|j 1|j 2|j
allj ∈ {1,2}holds,andthuswesimplydenoteitbyx =: x andx = 1−x . Inasimilar
1|j j 2|j j
manner,becausey +y =1holds,wesimplydenoteitbyy :=yandy =1−y. Furthermore,
1 2 1 2
sinceΣ δ = 0holds,wesimplydenoteitbyδ =: δ andδ = −δ. Intwo-actiongames,wecan
i i 1 2
write
δT =(δ δ )=(δ −δ), (24)
1 2
(cid:18) (cid:19) (cid:18) (cid:19)
logx logx logx logx
logX = 1|1 1|2 = 1 2 . (25)
logx logx log(1−x ) log(1−x )
2|1 2|2 1 2
Usingthesenotations,wecalculateH(X;δ)as
H(X;δ)=δTUlogXTδ
(cid:18) (cid:19)(cid:18) (cid:19)(cid:18) (cid:19)
1 −1 logx log(1−x ) δ
=(δ −δ) 1 1
−1 1 logx log(1−x ) −δ
2 2
(cid:18) (cid:19)
x x
=2δ2 log 1 +log 2
1−x 1−x
1 2
=2δ2(q −q ). (26)
1 2
Here,inthefinalline,weusedq := logx −log(1−x ) ⇔ x = exp(q )/(1+exp(q )). The
j j j j j j
dynamicsofH(X;δ)isalsocalculatedas
H†(y;δ)=(δTUdy)2
(cid:18) (cid:18)
1
−1(cid:19)(cid:18)
y−y∗
(cid:19)(cid:19)2
= (δ −δ)
−1 1 −y+y∗
=16δ2(y−y∗)2. (27)
Thisequationshowsthatforδ ̸=0,H†(y;δ)=0holdsifandonlyify =y∗. Thus,aftersufficiently
longtimepasses,y =y∗holdsorH(X;δ)divergestoinfinity. Ifso,H(X;δ)>0issatisfiedand
13isequivalenttoq >q ,whichisalsoequivalenttox >x fromthedefinition. Ifx >x holds,
1 2 1 2 1 2
D†(X;dy)<0issatisfiedbecausewecanderive
D†(X;dy)=−dyTXTUdy
(cid:18) (cid:19)(cid:18) (cid:19)(cid:18) (cid:19)
x 1−x u u y−y∗
=−(y−y∗ −y+y∗) 1 1 11 12
x 1−x u u −y+y∗
2 2 21 22
=−(y−y∗)2(x −x )(u −u −u +u ). (28)
1 2 11 12 21 22
SinceD(X,y)hasitslowerbound,D†(X;dy) = 0 ⇔ y = y∗ holdsaftersufficientlylongtime
passes. Thus,itwasprovedthatY’sstrategygloballyconvergestoitsNashequilibriumstrategy.
B Connectionwithordinarydefiniteness
Thisstudyconsideredpositivedefinitenesswithsomeconstraint,i.e.,forzero-sumvectors. However,
thisconstrainedpositivedefinitenessisrelatedtotheordinarilypositivedefinitenesswithoutconstraint,
introducedasfollows(describing“tilde”fordistinction).
Definition2(Positivedefiniteness). AsquarematrixM˜ ispositivedefinitewhenforallvectors
δ˜̸=0,δ˜·(M˜δ˜)>0.
Now, weshowhowthepositivedefinitenessforzero-sumvectorsistiedtotheordinarypositive
definiteness. First,freelychooseandfixkˆ ∈{1,··· ,m}. Fromanym×mmatrixM =(m ) ,
kk′ k,k′
wedefinea(m−1)×(m−1)matrixM˜ :=(m˜ ) by
kk′ k̸=kˆ,k′̸=kˆ
m˜ :=m −m −m +m . (29)
kk′ kk′ kkˆ kˆk′ kˆkˆ
Then,thepositivedefinitenessofM forzero-sumvectorsisequivalenttothepositivedefinitenessof
M˜ intheordinarydefinitionasfollows.
Theorem4(Equivalencetopositivedefiniteness). Ifandonlyifm×mmatrixM ispositivedefinite
forzero-sumvectors,(m−1)×(m−1)matrixM˜ ispositivedefinite.
Proof. For any (m−1)-dimensional vector δ˜ = (δ ) , we define a m-dimensional zero-sum
k k̸=kˆ
vectorδby
(cid:40)
−Σ δ˜ (k =kˆ)
δ := k̸=kˆ k . (30)
k δ˜ (k ̸=kˆ)
k
Conversely,itistriviallypossibletodefineδ˜fromanyδ. Thus,thereisone-to-onecorrespondence
betweenthem-dimensionalzero-sumvectorδandthe(m−1)-dimensional(general-sum)vectorδ˜.
Now,wecanproveδ·(Mδ)=δ˜·(M˜δ˜)asfollows.
(cid:88)(cid:88)
δ·(Mδ)= δ m δ
k kk′ k′
k k′
(cid:88) (cid:88) (cid:88) (cid:88)
= δ m δ + δ m δ + δ m δ +δ m δ
k kk′ k′ k kkˆ kˆ kˆ kˆk′ k′ kˆ kˆkˆ kˆ
k̸=kˆk′̸=kˆ k̸=kˆ k′̸=kˆ
= (cid:88) (cid:88) δ˜ m δ˜ −(cid:88) (cid:88) δ˜ m δ˜ −(cid:88) (cid:88) δ˜ m δ˜ +(cid:88) (cid:88) δ˜ m δ˜
k kk′ k′ k kkˆ k′ k kˆk′ k′ k kˆkˆ k′
k̸=kˆk′̸=kˆ k̸=kˆk′̸=kˆ k̸=kˆk′̸=kˆ k̸=kˆk′̸=kˆ
= (cid:88) (cid:88) δ˜ (m −m −m +m )δ˜
k kk′ kkˆ kˆk′ kˆkˆ k′
k̸=kˆk′̸=kˆ
= (cid:88) (cid:88) δ˜ m˜ δ˜
k kk′ k′
k̸=kˆk′̸=kˆ
=δ˜·(M˜δ˜). (31)
Here, in the third equal sign, we used Eq. (30). Thus, if and only if M is positive definite for
zero-sumvectors(i.e.,δ·(Mδ)>0),M˜ ispositivedefinite(i.e.,δ˜·(M˜δ˜)>0).
14C Connectiontotheclassicaldivergence
Thissectionisdedicatedtounderstandingtheconnectionbetweenourconditional-sumdivergence
D(X,y)andtheclassicaldivergenceD (x,y). Here,recallthatX’sreactivestrategyconsistsof
c
multiplevectorsX =(x ) ,wherex isthevectoroftheprobabilitydistributionofitsaction
j 1≤j≤mY j
choicewhentheotherchosej-thactioninthepreviousround. Ontheotherhand,intheclassical
divergence,X’smixedstrategyxisthesinglevectoroftheprobabilitydistributionofitsactionchoice
independentoftheother’spreviouschoice. Thus,ifthereactivestrategyofX satisfiesx =xfor
j
allj,itcorrespondstothemixedstrategyofx. Then,D˙(X,y)=D˙ (x,y)holds,asthefollowing
c
theoremshows.
Theorem5(Connectionwiththeclassicaltotaldivergence). WhenX satisfiesx = xforallj,
j
D˙(X,y)=D˙ (x,y)=0holds.
c
Proof. LetusprovethatD˙ =Σ D˙ (x∗∥x )+D˙ (y∗∥y)andD˙ =D˙ (x∗∥x)+D˙ (y∗∥y)
j KL j KL c KL KL
areequal. First,thetimeevolutionofY’sdivergence,i.e.,D˙ (y∗∥y),istriviallyequalbetween
KL
thesetwoequations. Thus,weproveΣ D˙ (x∗∥x )=D˙ (x∗∥x)below.
j KL j KL
x˙
Σ D˙ (x∗∥x )=−Σ Σ x∗ i|j
j KL j j i ix
i|j
=−Σ Σ x∗y (Σ u y −Σ x Σ u y )
j i i j j′ ij′ j′ i i|j j′ ij′ j′
(cid:124)(cid:123)(cid:122)(cid:125)
=xi
=−Σ x∗(Σ u y −Σ x Σ u y )
i i j′ ij′ j′ i i j′ ij′ j′
=D˙ (x∗∥x). (32)
KL
Inthefinalequalsign,weusedthereplicatordynamicsofX’sstrategyforgameswithoutmemories
as
(cid:18) (cid:19)
du (x,y) du (x,y)
x˙ =x st −Σ x st
i i dx i i dx
i i
=x (Σ u y −Σ x Σ u y ) (33)
i j ij j i i j ij j
Thus,weprovedD˙ =D˙ .
c
Inthisproof,weprovedD˙(X,y) = D˙ (x,y)separatelyforeachdivergenceofX’sstrategyand
c
Y’sstrategy.
Furthermore, note that D˙(X,y) = D˙ (x,y) holds only for a moment. Because the learning
c
dynamicsgivex˙ ̸=x˙,x graduallydiffersfromx. Inconclusion,D˙(X,y)=D˙ (x,y)becomes
j j c
unsatisfiedimmediately. (However,D˙ (x,y)=0continuestohold.)
c
D Computationalenvironment
Thesimulationspresentedinthispaperwereconductedusingthefollowingcomputationalenviron-
ment.
• OperatingSystem: macOSMonterey(version12.4)
• ProgrammingLanguage: Python3.11.3
• Processor: AppleM1Pro(10cores)
• Memory: 32GB
15