[
    {
        "title": "Brain Tumor Segmentation (BraTS) Challenge 2024: Meningioma Radiotherapy Planning Automated Segmentation",
        "authors": "Dominic LaBellaKatherine SchumacherMichael MixKevin LeuShan McBurney-LinPierre NedelecJavier Villanueva-MeyerJonathan ShapeyTom VercauterenKazumi ChiaOmar Al-SalihiJustin LeuLia HalaszYury VelichkoChunhao WangJohn KirkpatrickScott FloydZachary J. ReitmanTrey MullikinUlas BagciSean SachdevJona A. Hattangadi-GluthTyler SeibertNikdokht FaridConnor PuettMatthew W. PeaseKevin ShiueSyed Muhammad AnwarShahriar FaghaniMuhammad Ammar HaiderPranav WarmanJake AlbrechtAndrás JakabMana MoassefiVerena ChungAlejandro AristizabalAlexandros KarargyrisHasan KassemSarthak PatiMicah ShellerChristina HuangAaron ColeySiddharth GhantaAlex SchneiderConrad SharpRachit SalujaFlorian KoflerPhilipp LohmannPhillipp VollmuthLouis GagnonMaruf AdewoleHongwei Bran LiAnahita Fathi KazerooniNourel Hoda TahonUdunna AnazodoAhmed W. MoawadBjoern MenzeMarius George LinguraruMariam AboianBenedikt WiestlerUjjwal BaidGian-Marco ConteAndreas M. T. RauscheckerAyman NadaAly H. AbayazeedRaymond HuangMaria Correia de VerdierJeffrey D. RudieSpyridon BakasEvan Calabrese",
        "links": "http://arxiv.org/abs/2405.18383v1",
        "entry_id": "http://arxiv.org/abs/2405.18383v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18383v1",
        "summary": "The 2024 Brain Tumor Segmentation Meningioma Radiotherapy (BraTS-MEN-RT)\nchallenge aims to advance automated segmentation algorithms using the largest\nknown multi-institutional dataset of radiotherapy planning brain MRIs with\nexpert-annotated target labels for patients with intact or post-operative\nmeningioma that underwent either conventional external beam radiotherapy or\nstereotactic radiosurgery. Each case includes a defaced 3D post-contrast\nT1-weighted radiotherapy planning MRI in its native acquisition space,\naccompanied by a single-label \"target volume\" representing the gross tumor\nvolume (GTV) and any at-risk post-operative site. Target volume annotations\nadhere to established radiotherapy planning protocols, ensuring consistency\nacross cases and institutions. For pre-operative meningiomas, the target volume\nencompasses the entire GTV and associated nodular dural tail, while for\npost-operative cases, it includes at-risk resection cavity margins as\ndetermined by the treating institution. Case annotations were reviewed and\napproved by expert neuroradiologists and radiation oncologists. Participating\nteams will develop, containerize, and evaluate automated segmentation models\nusing this comprehensive dataset. Model performance will be assessed using the\nlesion-wise Dice Similarity Coefficient and the 95% Hausdorff distance. The\ntop-performing teams will be recognized at the Medical Image Computing and\nComputer Assisted Intervention Conference in October 2024. BraTS-MEN-RT is\nexpected to significantly advance automated radiotherapy planning by enabling\nprecise tumor segmentation and facilitating tailored treatment, ultimately\nimproving patient outcomes.",
        "updated": "2024-05-28 17:25:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18383v1"
    },
    {
        "title": "Hostile Counterspeech Drives Users From Hate Subreddits",
        "authors": "Daniel HickeyMatheus SchmitzDaniel M. T. FesslerPaul E. SmaldinoKristina LermanGoran MurićKeith Burghardt",
        "links": "http://arxiv.org/abs/2405.18374v1",
        "entry_id": "http://arxiv.org/abs/2405.18374v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18374v1",
        "summary": "Counterspeech -- speech that opposes hate speech -- has gained significant\nattention recently as a strategy to reduce hate on social media. While previous\nstudies suggest that counterspeech can somewhat reduce hate speech, little is\nknown about its effects on participation in online hate communities, nor which\ncounterspeech tactics reduce harmful behavior. We begin to address these gaps\nby identifying 25 large hate communities (\"subreddits\") within Reddit and\nanalyzing the effect of counterspeech on newcomers within these communities. We\nfirst construct a new public dataset of carefully annotated counterspeech and\nnon-counterspeech comments within these subreddits. We use this dataset to\ntrain a state-of-the-art counterspeech detection model. Next, we use matching\nto evaluate the causal effects of hostile and non-hostile counterspeech on the\nengagement of newcomers in hate subreddits. We find that, while non-hostile\ncounterspeech is ineffective at keeping users from fully disengaging from these\nhate subreddits, a single hostile counterspeech comment substantially reduces\nboth future likelihood of engagement. While offering nuance to the\nunderstanding of counterspeech efficacy, these results a) leave unanswered the\nquestion of whether hostile counterspeech dissuades newcomers from\nparticipation in online hate writ large, or merely drives them into\nless-moderated and more extreme hate communities, and b) raises ethical\nconsiderations about hostile counterspeech, which is both comparatively common\nand might exacerbate rather than mitigate the net level of antagonism in\nsociety. These findings underscore the importance of future work to improve\ncounterspeech tactics and minimize unintended harm.",
        "updated": "2024-05-28 17:12:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18374v1"
    },
    {
        "title": "The CoExplorer Technology Probe: A Generative AI-Powered Adaptive Interface to Support Intentionality in Planning and Running Video Meetings",
        "authors": "Gun WooParkPayod PandaLev TankelevitchSean Rintel",
        "links": "http://arxiv.org/abs/2405.18239v1",
        "entry_id": "http://arxiv.org/abs/2405.18239v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18239v1",
        "summary": "Effective meetings are effortful, but traditional videoconferencing systems\noffer little support for reducing this effort across the meeting lifecycle.\nGenerative AI (GenAI) has the potential to radically redefine meetings by\naugmenting intentional meeting behaviors. CoExplorer, our novel adaptive\nmeeting prototype, preemptively generates likely phases that meetings would\nundergo, tools that allow capturing attendees' thoughts before the meeting, and\nfor each phase, window layouts, and appropriate applications and files. Using\nCoExplorer as a technology probe in a guided walkthrough, we studied its\npotential in a sample of participants from a global technology company. Our\nfindings suggest that GenAI has the potential to help meetings stay on track\nand reduce workload, although concerns were raised about users' agency, trust,\nand possible disruption to traditional meeting norms. We discuss these concerns\nand their design implications for the development of GenAI meeting technology.",
        "updated": "2024-05-28 14:48:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18239v1"
    },
    {
        "title": "An Open-Source Reproducible Chess Robot for Human-Robot Interaction Research",
        "authors": "Renchi ZhangJoost de WinterDimitra DodouHarleigh SeyffertYke Bauke Eisma",
        "links": "http://arxiv.org/abs/2405.18170v1",
        "entry_id": "http://arxiv.org/abs/2405.18170v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18170v1",
        "summary": "Recent advancements in AI have sped up the evolution of versatile robot\ndesigns. Chess provides a standardized environment that allows for the\nevaluation of the influence of robot behaviors on human behavior. This article\npresents an open-source chess robot for human-robot interaction (HRI) research,\nspecifically focusing on verbal and non-verbal interactions. OpenChessRobot\nrecognizes chess pieces using computer vision, executes moves, and interacts\nwith the human player using voice and robotic gestures. We detail the software\ndesign, provide quantitative evaluations of the robot's efficacy and offer a\nguide for its reproducibility. The code and are accessible on GitHub:\nhttps://github.com/renchizhhhh/OpenChessRobot",
        "updated": "2024-05-28 13:30:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18170v1"
    },
    {
        "title": "Fatigue and mental underload further pronounced in L3 conditionally automated driving: Results from an EEG experiment on a test track",
        "authors": "Nikol FigalováHans Joachim BiegMichael SchulzJürgen PichenMartin BaumannLewis ChuangOlga Pollatos",
        "links": "http://dx.doi.org/10.1145/3581754.3584133",
        "entry_id": "http://arxiv.org/abs/2405.18114v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18114v1",
        "summary": "Drivers' role changes with increasing automation from the primary driver to a\nsystem supervisor. This study investigates how supervising an SAE L2 and L3\nautomated vehicle (AV) affects drivers' mental workload and sleepiness compared\nto manual driving. Using an AV prototype on a test track, the oscillatory brain\nactivity of 23 adult participants was recorded during L2, L3, and manual\ndriving. Results showed decreased mental workload and increased sleepiness in\nL3 drives compared to L2 and manual drives, indicated by self-report scales and\nchanges in the frontal alpha and theta power spectral density. These findings\nsuggest that fatigue and mental underload are significant issues in L3 driving\nand should be considered when designing future AV interfaces.",
        "updated": "2024-05-28 12:23:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18114v1"
    }
]