[
    {
        "title": "Classifying Overlapping Gaussian Mixtures in High Dimensions: From Optimal Classifiers to Neural Nets",
        "authors": "Khen CohenNoam LeviYaron Oz",
        "links": "http://arxiv.org/abs/2405.18427v1",
        "entry_id": "http://arxiv.org/abs/2405.18427v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18427v1",
        "summary": "We derive closed-form expressions for the Bayes optimal decision boundaries\nin binary classification of high dimensional overlapping Gaussian mixture model\n(GMM) data, and show how they depend on the eigenstructure of the class\ncovariances, for particularly interesting structured data. We empirically\ndemonstrate, through experiments on synthetic GMMs inspired by real-world data,\nthat deep neural networks trained for classification, learn predictors which\napproximate the derived optimal classifiers. We further extend our study to\nnetworks trained on authentic data, observing that decision thresholds\ncorrelate with the covariance eigenvectors rather than the eigenvalues,\nmirroring our GMM analysis. This provides theoretical insights regarding neural\nnetworks' ability to perform probabilistic inference and distill statistical\npatterns from intricate distributions.",
        "updated": "2024-05-28 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18427v1"
    },
    {
        "title": "Tensor Methods in High Dimensional Data Analysis: Opportunities and Challenges",
        "authors": "Arnab AuddyDong XiaMing Yuan",
        "links": "http://arxiv.org/abs/2405.18412v1",
        "entry_id": "http://arxiv.org/abs/2405.18412v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18412v1",
        "summary": "Large amount of multidimensional data represented by multiway arrays or\ntensors are prevalent in modern applications across various fields such as\nchemometrics, genomics, physics, psychology, and signal processing. The\nstructural complexity of such data provides vast new opportunities for modeling\nand analysis, but efficiently extracting information content from them, both\nstatistically and computationally, presents unique and fundamental challenges.\nAddressing these challenges requires an interdisciplinary approach that brings\ntogether tools and insights from statistics, optimization and numerical linear\nalgebra among other fields. Despite these hurdles, significant progress has\nbeen made in the last decade. This review seeks to examine some of the key\nadvancements and identify common threads among them, under eight different\nstatistical settings.",
        "updated": "2024-05-28 17:54:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18412v1"
    },
    {
        "title": "Explicit Formulae to Interchangeably use Hyperplanes and Hyperballs using Inversive Geometry",
        "authors": "Erik ThordsenErich Schubert",
        "links": "http://arxiv.org/abs/2405.18401v1",
        "entry_id": "http://arxiv.org/abs/2405.18401v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18401v1",
        "summary": "Many algorithms require discriminative boundaries, such as separating\nhyperplanes or hyperballs, or are specifically designed to work on spherical\ndata. By applying inversive geometry, we show that the two discriminative\nboundaries can be used interchangeably, and that general Euclidean data can be\ntransformed into spherical data, whenever a change in point distances is\nacceptable. We provide explicit formulae to embed general Euclidean data into\nspherical data and to unembed it back. We further show a duality between\nhyperspherical caps, i.e., the volume created by a separating hyperplane on\nspherical data, and hyperballs and provide explicit formulae to map between the\ntwo. We further provide equations to translate inner products and Euclidean\ndistances between the two spaces, to avoid explicit embedding and unembedding.\nWe also provide a method to enforce projections of the general Euclidean space\nonto hemi-hyperspheres and propose an intrinsic dimensionality based method to\nobtain \"all-purpose\" parameters. To show the usefulness of the\ncap-ball-duality, we discuss example applications in machine learning and\nvector similarity search.",
        "updated": "2024-05-28 17:43:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18401v1"
    },
    {
        "title": "A Note on the Prediction-Powered Bootstrap",
        "authors": "Tijana Zrnic",
        "links": "http://arxiv.org/abs/2405.18379v1",
        "entry_id": "http://arxiv.org/abs/2405.18379v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18379v1",
        "summary": "We introduce PPBoot: a bootstrap-based method for prediction-powered\ninference. PPBoot is applicable to arbitrary estimation problems and is very\nsimple to implement, essentially only requiring one application of the\nbootstrap. Through a series of examples, we demonstrate that PPBoot often\nperforms nearly identically to (and sometimes better than) the earlier PPI(++)\nmethod based on asymptotic normality$\\unicode{x2013}$when the latter is\napplicable$\\unicode{x2013}$without requiring any asymptotic characterizations.\nGiven its versatility, PPBoot could simplify and expand the scope of\napplication of prediction-powered inference to problems where central limit\ntheorems are hard to prove.",
        "updated": "2024-05-28 17:22:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18379v1"
    },
    {
        "title": "A Hessian-Aware Stochastic Differential Equation for Modelling SGD",
        "authors": "Xiang LiZebang ShenLiang ZhangNiao He",
        "links": "http://arxiv.org/abs/2405.18373v1",
        "entry_id": "http://arxiv.org/abs/2405.18373v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18373v1",
        "summary": "Continuous-time approximation of Stochastic Gradient Descent (SGD) is a\ncrucial tool to study its escaping behaviors from stationary points. However,\nexisting stochastic differential equation (SDE) models fail to fully capture\nthese behaviors, even for simple quadratic objectives. Built on a novel\nstochastic backward error analysis framework, we derive the Hessian-Aware\nStochastic Modified Equation (HA-SME), an SDE that incorporates Hessian\ninformation of the objective function into both its drift and diffusion terms.\nOur analysis shows that HA-SME matches the order-best approximation error\nguarantee among existing SDE models in the literature, while achieving a\nsignificantly reduced dependence on the smoothness parameter of the objective.\nFurther, for quadratic objectives, under mild conditions, HA-SME is proved to\nbe the first SDE model that recovers exactly the SGD dynamics in the\ndistributional sense. Consequently, when the local landscape near a stationary\npoint can be approximated by quadratics, HA-SME is expected to accurately\npredict the local escaping behaviors of SGD.",
        "updated": "2024-05-28 17:11:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18373v1"
    }
]