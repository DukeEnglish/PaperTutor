[
    {
        "title": "On the Origin of Llamas: Model Tree Heritage Recovery",
        "authors": "Eliahu HorwitzAsaf ShulYedid Hoshen",
        "links": "http://arxiv.org/abs/2405.18432v1",
        "entry_id": "http://arxiv.org/abs/2405.18432v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18432v1",
        "summary": "The rapid growth of neural network models shared on the internet has made\nmodel weights an important data modality. However, this information is\nunderutilized as the weights are uninterpretable, and publicly available models\nare disorganized. Inspired by Darwin's tree of life, we define the Model Tree\nwhich describes the origin of models i.e., the parent model that was used to\nfine-tune the target model. Similarly to the natural world, the tree structure\nis unknown. In this paper, we introduce the task of Model Tree Heritage\nRecovery (MoTHer Recovery) for discovering Model Trees in the ever-growing\nuniverse of neural networks. Our hypothesis is that model weights encode this\ninformation, the challenge is to decode the underlying tree structure given the\nweights. Beyond the immediate application of model authorship attribution,\nMoTHer recovery holds exciting long-term applications akin to indexing the\ninternet by search engines. Practically, for each pair of models, this task\nrequires: i) determining if they are related, and ii) establishing the\ndirection of the relationship. We find that certain distributional properties\nof the weights evolve monotonically during training, which enables us to\nclassify the relationship between two given models. MoTHer recovery\nreconstructs entire model hierarchies, represented by a directed tree, where a\nparent model gives rise to multiple child models through additional training.\nOur approach successfully reconstructs complex Model Trees, as well as the\nstructure of \"in-the-wild\" model families such as Llama 2 and Stable Diffusion.",
        "updated": "2024-05-28 17:59:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18432v1"
    },
    {
        "title": "Classifying Overlapping Gaussian Mixtures in High Dimensions: From Optimal Classifiers to Neural Nets",
        "authors": "Khen CohenNoam LeviYaron Oz",
        "links": "http://arxiv.org/abs/2405.18427v1",
        "entry_id": "http://arxiv.org/abs/2405.18427v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18427v1",
        "summary": "We derive closed-form expressions for the Bayes optimal decision boundaries\nin binary classification of high dimensional overlapping Gaussian mixture model\n(GMM) data, and show how they depend on the eigenstructure of the class\ncovariances, for particularly interesting structured data. We empirically\ndemonstrate, through experiments on synthetic GMMs inspired by real-world data,\nthat deep neural networks trained for classification, learn predictors which\napproximate the derived optimal classifiers. We further extend our study to\nnetworks trained on authentic data, observing that decision thresholds\ncorrelate with the covariance eigenvectors rather than the eigenvalues,\nmirroring our GMM analysis. This provides theoretical insights regarding neural\nnetworks' ability to perform probabilistic inference and distill statistical\npatterns from intricate distributions.",
        "updated": "2024-05-28 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18427v1"
    },
    {
        "title": "Hierarchical World Models as Visual Whole-Body Humanoid Controllers",
        "authors": "Nicklas HansenJyothir S VVlad SobalYann LeCunXiaolong WangHao Su",
        "links": "http://arxiv.org/abs/2405.18418v1",
        "entry_id": "http://arxiv.org/abs/2405.18418v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18418v1",
        "summary": "Whole-body control for humanoids is challenging due to the high-dimensional\nnature of the problem, coupled with the inherent instability of a bipedal\nmorphology. Learning from visual observations further exacerbates this\ndifficulty. In this work, we explore highly data-driven approaches to visual\nwhole-body humanoid control based on reinforcement learning, without any\nsimplifying assumptions, reward design, or skill primitives. Specifically, we\npropose a hierarchical world model in which a high-level agent generates\ncommands based on visual observations for a low-level agent to execute, both of\nwhich are trained with rewards. Our approach produces highly performant control\npolicies in 8 tasks with a simulated 56-DoF humanoid, while synthesizing\nmotions that are broadly preferred by humans. Code and videos:\nhttps://nicklashansen.com/rlpuppeteer",
        "updated": "2024-05-28 17:57:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18418v1"
    },
    {
        "title": "Why are Visually-Grounded Language Models Bad at Image Classification?",
        "authors": "Yuhui ZhangAlyssa UnellXiaohan WangDhruba GhoshYuchang SuLudwig SchmidtSerena Yeung-Levy",
        "links": "http://arxiv.org/abs/2405.18415v1",
        "entry_id": "http://arxiv.org/abs/2405.18415v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18415v1",
        "summary": "Image classification is one of the most fundamental capabilities of machine\nvision intelligence. In this work, we revisit the image classification task\nusing visually-grounded language models (VLMs) such as GPT-4V and LLaVA. We\nfind that existing proprietary and public VLMs, despite often using CLIP as a\nvision encoder and having many more parameters, significantly underperform CLIP\non standard image classification benchmarks like ImageNet. To understand the\nreason, we explore several hypotheses concerning the inference algorithms,\ntraining objectives, and data processing in VLMs. Our analysis reveals that the\nprimary cause is data-related: critical information for image classification is\nencoded in the VLM's latent space but can only be effectively decoded with\nenough training data. Specifically, there is a strong correlation between the\nfrequency of class exposure during VLM training and instruction-tuning and the\nVLM's performance in those classes; when trained with sufficient data, VLMs can\nmatch the accuracy of state-of-the-art classification models. Based on these\nfindings, we enhance a VLM by integrating classification-focused datasets into\nits training, and demonstrate that the enhanced classification performance of\nthe VLM transfers to its general capabilities, resulting in an improvement of\n11.8% on the newly collected ImageWikiQA dataset.",
        "updated": "2024-05-28 17:57:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18415v1"
    },
    {
        "title": "Don't Forget to Connect! Improving RAG with Graph-based Reranking",
        "authors": "Jialin DongBahare FatemiBryan PerozziLin F. YangAnton Tsitsulin",
        "links": "http://arxiv.org/abs/2405.18414v1",
        "entry_id": "http://arxiv.org/abs/2405.18414v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18414v1",
        "summary": "Retrieval Augmented Generation (RAG) has greatly improved the performance of\nLarge Language Model (LLM) responses by grounding generation with context from\nexisting documents. These systems work well when documents are clearly relevant\nto a question context. But what about when a document has partial information,\nor less obvious connections to the context? And how should we reason about\nconnections between documents? In this work, we seek to answer these two core\nquestions about RAG generation. We introduce G-RAG, a reranker based on graph\nneural networks (GNNs) between the retriever and reader in RAG. Our method\ncombines both connections between documents and semantic information (via\nAbstract Meaning Representation graphs) to provide a context-informed ranker\nfor RAG. G-RAG outperforms state-of-the-art approaches while having smaller\ncomputational footprint. Additionally, we assess the performance of PaLM 2 as a\nreranker and find it to significantly underperform G-RAG. This result\nemphasizes the importance of reranking for RAG even when using Large Language\nModels.",
        "updated": "2024-05-28 17:56:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18414v1"
    }
]