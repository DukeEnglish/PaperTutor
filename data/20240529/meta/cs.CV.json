[
    {
        "title": "DiG: Scalable and Efficient Diffusion Models with Gated Linear Attention",
        "authors": "Lianghui ZhuZilong HuangBencheng LiaoJun Hao LiewHanshu YanJiashi FengXinggang Wang",
        "links": "http://arxiv.org/abs/2405.18428v1",
        "entry_id": "http://arxiv.org/abs/2405.18428v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18428v1",
        "summary": "Diffusion models with large-scale pre-training have achieved significant\nsuccess in the field of visual content generation, particularly exemplified by\nDiffusion Transformers (DiT). However, DiT models have faced challenges with\nscalability and quadratic complexity efficiency. In this paper, we aim to\nleverage the long sequence modeling capability of Gated Linear Attention (GLA)\nTransformers, expanding its applicability to diffusion models. We introduce\nDiffusion Gated Linear Attention Transformers (DiG), a simple, adoptable\nsolution with minimal parameter overhead, following the DiT design, but\noffering superior efficiency and effectiveness. In addition to better\nperformance than DiT, DiG-S/2 exhibits $2.5\\times$ higher training speed than\nDiT-S/2 and saves $75.7\\%$ GPU memory at a resolution of $1792 \\times 1792$.\nMoreover, we analyze the scalability of DiG across a variety of computational\ncomplexity. DiG models, with increased depth/width or augmentation of input\ntokens, consistently exhibit decreasing FID. We further compare DiG with other\nsubquadratic-time diffusion models. With the same model size, DiG-XL/2 is\n$4.2\\times$ faster than the recent Mamba-based diffusion model at a $1024$\nresolution, and is $1.8\\times$ faster than DiT with CUDA-optimized\nFlashAttention-2 under the $2048$ resolution. All these results demonstrate its\nsuperior efficiency among the latest diffusion models. Code is released at\nhttps://github.com/hustvl/DiG.",
        "updated": "2024-05-28 17:59:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18428v1"
    },
    {
        "title": "GFlow: Recovering 4D World from Monocular Video",
        "authors": "Shizun WangXingyi YangQiuhong ShenZhenxiang JiangXinchao Wang",
        "links": "http://arxiv.org/abs/2405.18426v1",
        "entry_id": "http://arxiv.org/abs/2405.18426v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18426v1",
        "summary": "Reconstructing 4D scenes from video inputs is a crucial yet challenging task.\nConventional methods usually rely on the assumptions of multi-view video\ninputs, known camera parameters, or static scenes, all of which are typically\nabsent under in-the-wild scenarios. In this paper, we relax all these\nconstraints and tackle a highly ambitious but practical task, which we termed\nas AnyV4D: we assume only one monocular video is available without any camera\nparameters as input, and we aim to recover the dynamic 4D world alongside the\ncamera poses. To this end, we introduce GFlow, a new framework that utilizes\nonly 2D priors (depth and optical flow) to lift a video (3D) to a 4D explicit\nrepresentation, entailing a flow of Gaussian splatting through space and time.\nGFlow first clusters the scene into still and moving parts, then applies a\nsequential optimization process that optimizes camera poses and the dynamics of\n3D Gaussian points based on 2D priors and scene clustering, ensuring fidelity\namong neighboring points and smooth movement across frames. Since dynamic\nscenes always introduce new content, we also propose a new pixel-wise\ndensification strategy for Gaussian points to integrate new visual content.\nMoreover, GFlow transcends the boundaries of mere 4D reconstruction; it also\nenables tracking of any points across frames without the need for prior\ntraining and segments moving objects from the scene in an unsupervised way.\nAdditionally, the camera poses of each frame can be derived from GFlow,\nallowing for rendering novel views of a video scene through changing camera\npose. By employing the explicit representation, we may readily conduct\nscene-level or object-level editing as desired, underscoring its versatility\nand power. Visit our project website at: https://littlepure2333.github.io/GFlow",
        "updated": "2024-05-28 17:59:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18426v1"
    },
    {
        "title": "ViG: Linear-complexity Visual Sequence Learning with Gated Linear Attention",
        "authors": "Bencheng LiaoXinggang WangLianghui ZhuQian ZhangChang Huang",
        "links": "http://arxiv.org/abs/2405.18425v1",
        "entry_id": "http://arxiv.org/abs/2405.18425v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18425v1",
        "summary": "Recently, linear complexity sequence modeling networks have achieved modeling\ncapabilities similar to Vision Transformers on a variety of computer vision\ntasks, while using fewer FLOPs and less memory. However, their advantage in\nterms of actual runtime speed is not significant. To address this issue, we\nintroduce Gated Linear Attention (GLA) for vision, leveraging its superior\nhardware-awareness and efficiency. We propose direction-wise gating to capture\n1D global context through bidirectional modeling and a 2D gating locality\ninjection to adaptively inject 2D local details into 1D global context. Our\nhardware-aware implementation further merges forward and backward scanning into\na single kernel, enhancing parallelism and reducing memory cost and latency.\nThe proposed model, \\name{}, offers a favorable trade-off in accuracy,\nparameters, and FLOPs on ImageNet and downstream tasks, outperforming popular\nTransformer and CNN-based models. Notably, \\name{}-S matches DeiT-B's accuracy\nwhile using only 27\\% of the parameters and 20\\% of the FLOPs, running\n2$\\times$ faster on $224\\times224$ images. At $1024\\times1024$ resolution,\n\\name{}-T uses 5.2$\\times$ fewer FLOPs, saves 90\\% GPU memory, runs 4.8$\\times$\nfaster, and achieves 20.7\\% higher top-1 accuracy than DeiT-T. These results\nposition \\name{} as an efficient and scalable solution for visual\nrepresentation learning. Code is available at\n\\url{https://github.com/hustvl/ViG}.",
        "updated": "2024-05-28 17:59:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18425v1"
    },
    {
        "title": "3DitScene: Editing Any Scene via Language-guided Disentangled Gaussian Splatting",
        "authors": "Qihang ZhangYinghao XuChaoyang WangHsin-Ying LeeGordon WetzsteinBolei ZhouCeyuan Yang",
        "links": "http://arxiv.org/abs/2405.18424v1",
        "entry_id": "http://arxiv.org/abs/2405.18424v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18424v1",
        "summary": "Scene image editing is crucial for entertainment, photography, and\nadvertising design. Existing methods solely focus on either 2D individual\nobject or 3D global scene editing. This results in a lack of a unified approach\nto effectively control and manipulate scenes at the 3D level with different\nlevels of granularity. In this work, we propose 3DitScene, a novel and unified\nscene editing framework leveraging language-guided disentangled Gaussian\nSplatting that enables seamless editing from 2D to 3D, allowing precise control\nover scene composition and individual objects. We first incorporate 3D\nGaussians that are refined through generative priors and optimization\ntechniques. Language features from CLIP then introduce semantics into 3D\ngeometry for object disentanglement. With the disentangled Gaussians, 3DitScene\nallows for manipulation at both the global and individual levels,\nrevolutionizing creative expression and empowering control over scenes and\nobjects. Experimental results demonstrate the effectiveness and versatility of\n3DitScene in scene image editing. Code and online demo can be found at our\nproject homepage: https://zqh0253.github.io/3DitScene/.",
        "updated": "2024-05-28 17:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18424v1"
    },
    {
        "title": "Hierarchical World Models as Visual Whole-Body Humanoid Controllers",
        "authors": "Nicklas HansenJyothir S VVlad SobalYann LeCunXiaolong WangHao Su",
        "links": "http://arxiv.org/abs/2405.18418v1",
        "entry_id": "http://arxiv.org/abs/2405.18418v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18418v1",
        "summary": "Whole-body control for humanoids is challenging due to the high-dimensional\nnature of the problem, coupled with the inherent instability of a bipedal\nmorphology. Learning from visual observations further exacerbates this\ndifficulty. In this work, we explore highly data-driven approaches to visual\nwhole-body humanoid control based on reinforcement learning, without any\nsimplifying assumptions, reward design, or skill primitives. Specifically, we\npropose a hierarchical world model in which a high-level agent generates\ncommands based on visual observations for a low-level agent to execute, both of\nwhich are trained with rewards. Our approach produces highly performant control\npolicies in 8 tasks with a simulated 56-DoF humanoid, while synthesizing\nmotions that are broadly preferred by humans. Code and videos:\nhttps://nicklashansen.com/rlpuppeteer",
        "updated": "2024-05-28 17:57:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18418v1"
    }
]