[
    {
        "title": "Notes on Applicability of GPT-4 to Document Understanding",
        "authors": "Łukasz Borchmann",
        "links": "http://arxiv.org/abs/2405.18433v1",
        "entry_id": "http://arxiv.org/abs/2405.18433v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18433v1",
        "summary": "We perform a missing, reproducible evaluation of all publicly available GPT-4\nfamily models concerning the Document Understanding field, where it is\nfrequently required to comprehend text spacial arrangement and visual clues in\naddition to textual semantics. Benchmark results indicate that though it is\nhard to achieve satisfactory results with text-only models, GPT-4 Vision Turbo\nperforms well when one provides both text recognized by an external OCR engine\nand document images on the input. Evaluation is followed by analyses that\nsuggest possible contamination of textual GPT-4 models and indicate the\nsignificant performance drop for lengthy documents.",
        "updated": "2024-05-28 17:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18433v1"
    },
    {
        "title": "Why are Visually-Grounded Language Models Bad at Image Classification?",
        "authors": "Yuhui ZhangAlyssa UnellXiaohan WangDhruba GhoshYuchang SuLudwig SchmidtSerena Yeung-Levy",
        "links": "http://arxiv.org/abs/2405.18415v1",
        "entry_id": "http://arxiv.org/abs/2405.18415v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18415v1",
        "summary": "Image classification is one of the most fundamental capabilities of machine\nvision intelligence. In this work, we revisit the image classification task\nusing visually-grounded language models (VLMs) such as GPT-4V and LLaVA. We\nfind that existing proprietary and public VLMs, despite often using CLIP as a\nvision encoder and having many more parameters, significantly underperform CLIP\non standard image classification benchmarks like ImageNet. To understand the\nreason, we explore several hypotheses concerning the inference algorithms,\ntraining objectives, and data processing in VLMs. Our analysis reveals that the\nprimary cause is data-related: critical information for image classification is\nencoded in the VLM's latent space but can only be effectively decoded with\nenough training data. Specifically, there is a strong correlation between the\nfrequency of class exposure during VLM training and instruction-tuning and the\nVLM's performance in those classes; when trained with sufficient data, VLMs can\nmatch the accuracy of state-of-the-art classification models. Based on these\nfindings, we enhance a VLM by integrating classification-focused datasets into\nits training, and demonstrate that the enhanced classification performance of\nthe VLM transfers to its general capabilities, resulting in an improvement of\n11.8% on the newly collected ImageWikiQA dataset.",
        "updated": "2024-05-28 17:57:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18415v1"
    },
    {
        "title": "Don't Forget to Connect! Improving RAG with Graph-based Reranking",
        "authors": "Jialin DongBahare FatemiBryan PerozziLin F. YangAnton Tsitsulin",
        "links": "http://arxiv.org/abs/2405.18414v1",
        "entry_id": "http://arxiv.org/abs/2405.18414v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18414v1",
        "summary": "Retrieval Augmented Generation (RAG) has greatly improved the performance of\nLarge Language Model (LLM) responses by grounding generation with context from\nexisting documents. These systems work well when documents are clearly relevant\nto a question context. But what about when a document has partial information,\nor less obvious connections to the context? And how should we reason about\nconnections between documents? In this work, we seek to answer these two core\nquestions about RAG generation. We introduce G-RAG, a reranker based on graph\nneural networks (GNNs) between the retriever and reader in RAG. Our method\ncombines both connections between documents and semantic information (via\nAbstract Meaning Representation graphs) to provide a context-informed ranker\nfor RAG. G-RAG outperforms state-of-the-art approaches while having smaller\ncomputational footprint. Additionally, we assess the performance of PaLM 2 as a\nreranker and find it to significantly underperform G-RAG. This result\nemphasizes the importance of reranking for RAG even when using Large Language\nModels.",
        "updated": "2024-05-28 17:56:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18414v1"
    },
    {
        "title": "RACCooN: Remove, Add, and Change Video Content with Auto-Generated Narratives",
        "authors": "Jaehong YoonShoubin YuMohit Bansal",
        "links": "http://arxiv.org/abs/2405.18406v1",
        "entry_id": "http://arxiv.org/abs/2405.18406v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18406v1",
        "summary": "Recent video generative models primarily rely on carefully written text\nprompts for specific tasks, like inpainting or style editing. They require\nlabor-intensive textual descriptions for input videos, hindering their\nflexibility to adapt personal/raw videos to user specifications. This paper\nproposes RACCooN, a versatile and user-friendly video-to-paragraph-to-video\ngenerative framework that supports multiple video editing capabilities such as\nremoval, addition, and modification, through a unified pipeline. RACCooN\nconsists of two principal stages: Video-to-Paragraph (V2P) and\nParagraph-to-Video (P2V). In the V2P stage, we automatically describe video\nscenes in well-structured natural language, capturing both the holistic context\nand focused object details. Subsequently, in the P2V stage, users can\noptionally refine these descriptions to guide the video diffusion model,\nenabling various modifications to the input video, such as removing, changing\nsubjects, and/or adding new objects. The proposed approach stands out from\nother methods through several significant contributions: (1) RACCooN suggests a\nmulti-granular spatiotemporal pooling strategy to generate well-structured\nvideo descriptions, capturing both the broad context and object details without\nrequiring complex human annotations, simplifying precise video content editing\nbased on text for users. (2) Our video generative model incorporates\nauto-generated narratives or instructions to enhance the quality and accuracy\nof the generated content. It supports the addition of video objects,\ninpainting, and attribute modification within a unified framework, surpassing\nexisting video editing and inpainting benchmarks. The proposed framework\ndemonstrates impressive versatile capabilities in video-to-paragraph\ngeneration, video content editing, and can be incorporated into other SoTA\nvideo generative models for further enhancement.",
        "updated": "2024-05-28 17:46:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18406v1"
    },
    {
        "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive Inference Pass",
        "authors": "Ethan ShenAlan FanSarah M PrattJae Sung ParkMatthew WallingfordSham M. KakadeAri HoltzmanRanjay KrishnaAli FarhadiAditya Kusupati",
        "links": "http://arxiv.org/abs/2405.18400v1",
        "entry_id": "http://arxiv.org/abs/2405.18400v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18400v1",
        "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the $k$ most recent token\nembeddings from the drafts as input to the next decoding step of the language\nmodel. At every inference step we combine the $k$ drafts with the top-$k$\ntokens to get $k^2$ new drafts and cache the $k$ most likely options, using an\nn-gram interpolation with minimal compute overhead to filter out incoherent\ngenerations. Our experiments show that $k$ drafts from Superposed Decoding are\nat least as coherent and factual as Nucleus Sampling and Greedy Decoding\nrespectively, while being at least $2.44\\times$ faster for $k\\ge3$. In a\ncompute-normalized setting, user evaluations demonstrably favor text generated\nby Superposed Decoding over Nucleus Sampling. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
        "updated": "2024-05-28 17:40:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18400v1"
    }
]