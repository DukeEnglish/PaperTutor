[
    {
        "title": "DiG: Scalable and Efficient Diffusion Models with Gated Linear Attention",
        "authors": "Lianghui ZhuZilong HuangBencheng LiaoJun Hao LiewHanshu YanJiashi FengXinggang Wang",
        "links": "http://arxiv.org/abs/2405.18428v1",
        "entry_id": "http://arxiv.org/abs/2405.18428v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18428v1",
        "summary": "Diffusion models with large-scale pre-training have achieved significant\nsuccess in the field of visual content generation, particularly exemplified by\nDiffusion Transformers (DiT). However, DiT models have faced challenges with\nscalability and quadratic complexity efficiency. In this paper, we aim to\nleverage the long sequence modeling capability of Gated Linear Attention (GLA)\nTransformers, expanding its applicability to diffusion models. We introduce\nDiffusion Gated Linear Attention Transformers (DiG), a simple, adoptable\nsolution with minimal parameter overhead, following the DiT design, but\noffering superior efficiency and effectiveness. In addition to better\nperformance than DiT, DiG-S/2 exhibits $2.5\\times$ higher training speed than\nDiT-S/2 and saves $75.7\\%$ GPU memory at a resolution of $1792 \\times 1792$.\nMoreover, we analyze the scalability of DiG across a variety of computational\ncomplexity. DiG models, with increased depth/width or augmentation of input\ntokens, consistently exhibit decreasing FID. We further compare DiG with other\nsubquadratic-time diffusion models. With the same model size, DiG-XL/2 is\n$4.2\\times$ faster than the recent Mamba-based diffusion model at a $1024$\nresolution, and is $1.8\\times$ faster than DiT with CUDA-optimized\nFlashAttention-2 under the $2048$ resolution. All these results demonstrate its\nsuperior efficiency among the latest diffusion models. Code is released at\nhttps://github.com/hustvl/DiG.",
        "updated": "2024-05-28 17:59:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18428v1"
    },
    {
        "title": "Classifying Overlapping Gaussian Mixtures in High Dimensions: From Optimal Classifiers to Neural Nets",
        "authors": "Khen CohenNoam LeviYaron Oz",
        "links": "http://arxiv.org/abs/2405.18427v1",
        "entry_id": "http://arxiv.org/abs/2405.18427v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18427v1",
        "summary": "We derive closed-form expressions for the Bayes optimal decision boundaries\nin binary classification of high dimensional overlapping Gaussian mixture model\n(GMM) data, and show how they depend on the eigenstructure of the class\ncovariances, for particularly interesting structured data. We empirically\ndemonstrate, through experiments on synthetic GMMs inspired by real-world data,\nthat deep neural networks trained for classification, learn predictors which\napproximate the derived optimal classifiers. We further extend our study to\nnetworks trained on authentic data, observing that decision thresholds\ncorrelate with the covariance eigenvectors rather than the eigenvalues,\nmirroring our GMM analysis. This provides theoretical insights regarding neural\nnetworks' ability to perform probabilistic inference and distill statistical\npatterns from intricate distributions.",
        "updated": "2024-05-28 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18427v1"
    },
    {
        "title": "GFlow: Recovering 4D World from Monocular Video",
        "authors": "Shizun WangXingyi YangQiuhong ShenZhenxiang JiangXinchao Wang",
        "links": "http://arxiv.org/abs/2405.18426v1",
        "entry_id": "http://arxiv.org/abs/2405.18426v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18426v1",
        "summary": "Reconstructing 4D scenes from video inputs is a crucial yet challenging task.\nConventional methods usually rely on the assumptions of multi-view video\ninputs, known camera parameters, or static scenes, all of which are typically\nabsent under in-the-wild scenarios. In this paper, we relax all these\nconstraints and tackle a highly ambitious but practical task, which we termed\nas AnyV4D: we assume only one monocular video is available without any camera\nparameters as input, and we aim to recover the dynamic 4D world alongside the\ncamera poses. To this end, we introduce GFlow, a new framework that utilizes\nonly 2D priors (depth and optical flow) to lift a video (3D) to a 4D explicit\nrepresentation, entailing a flow of Gaussian splatting through space and time.\nGFlow first clusters the scene into still and moving parts, then applies a\nsequential optimization process that optimizes camera poses and the dynamics of\n3D Gaussian points based on 2D priors and scene clustering, ensuring fidelity\namong neighboring points and smooth movement across frames. Since dynamic\nscenes always introduce new content, we also propose a new pixel-wise\ndensification strategy for Gaussian points to integrate new visual content.\nMoreover, GFlow transcends the boundaries of mere 4D reconstruction; it also\nenables tracking of any points across frames without the need for prior\ntraining and segments moving objects from the scene in an unsupervised way.\nAdditionally, the camera poses of each frame can be derived from GFlow,\nallowing for rendering novel views of a video scene through changing camera\npose. By employing the explicit representation, we may readily conduct\nscene-level or object-level editing as desired, underscoring its versatility\nand power. Visit our project website at: https://littlepure2333.github.io/GFlow",
        "updated": "2024-05-28 17:59:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18426v1"
    },
    {
        "title": "ViG: Linear-complexity Visual Sequence Learning with Gated Linear Attention",
        "authors": "Bencheng LiaoXinggang WangLianghui ZhuQian ZhangChang Huang",
        "links": "http://arxiv.org/abs/2405.18425v1",
        "entry_id": "http://arxiv.org/abs/2405.18425v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18425v1",
        "summary": "Recently, linear complexity sequence modeling networks have achieved modeling\ncapabilities similar to Vision Transformers on a variety of computer vision\ntasks, while using fewer FLOPs and less memory. However, their advantage in\nterms of actual runtime speed is not significant. To address this issue, we\nintroduce Gated Linear Attention (GLA) for vision, leveraging its superior\nhardware-awareness and efficiency. We propose direction-wise gating to capture\n1D global context through bidirectional modeling and a 2D gating locality\ninjection to adaptively inject 2D local details into 1D global context. Our\nhardware-aware implementation further merges forward and backward scanning into\na single kernel, enhancing parallelism and reducing memory cost and latency.\nThe proposed model, \\name{}, offers a favorable trade-off in accuracy,\nparameters, and FLOPs on ImageNet and downstream tasks, outperforming popular\nTransformer and CNN-based models. Notably, \\name{}-S matches DeiT-B's accuracy\nwhile using only 27\\% of the parameters and 20\\% of the FLOPs, running\n2$\\times$ faster on $224\\times224$ images. At $1024\\times1024$ resolution,\n\\name{}-T uses 5.2$\\times$ fewer FLOPs, saves 90\\% GPU memory, runs 4.8$\\times$\nfaster, and achieves 20.7\\% higher top-1 accuracy than DeiT-T. These results\nposition \\name{} as an efficient and scalable solution for visual\nrepresentation learning. Code is available at\n\\url{https://github.com/hustvl/ViG}.",
        "updated": "2024-05-28 17:59:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18425v1"
    },
    {
        "title": "Why are Visually-Grounded Language Models Bad at Image Classification?",
        "authors": "Yuhui ZhangAlyssa UnellXiaohan WangDhruba GhoshYuchang SuLudwig SchmidtSerena Yeung-Levy",
        "links": "http://arxiv.org/abs/2405.18415v1",
        "entry_id": "http://arxiv.org/abs/2405.18415v1",
        "pdf_url": "http://arxiv.org/pdf/2405.18415v1",
        "summary": "Image classification is one of the most fundamental capabilities of machine\nvision intelligence. In this work, we revisit the image classification task\nusing visually-grounded language models (VLMs) such as GPT-4V and LLaVA. We\nfind that existing proprietary and public VLMs, despite often using CLIP as a\nvision encoder and having many more parameters, significantly underperform CLIP\non standard image classification benchmarks like ImageNet. To understand the\nreason, we explore several hypotheses concerning the inference algorithms,\ntraining objectives, and data processing in VLMs. Our analysis reveals that the\nprimary cause is data-related: critical information for image classification is\nencoded in the VLM's latent space but can only be effectively decoded with\nenough training data. Specifically, there is a strong correlation between the\nfrequency of class exposure during VLM training and instruction-tuning and the\nVLM's performance in those classes; when trained with sufficient data, VLMs can\nmatch the accuracy of state-of-the-art classification models. Based on these\nfindings, we enhance a VLM by integrating classification-focused datasets into\nits training, and demonstrate that the enhanced classification performance of\nthe VLM transfers to its general capabilities, resulting in an improvement of\n11.8% on the newly collected ImageWikiQA dataset.",
        "updated": "2024-05-28 17:57:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.18415v1"
    }
]