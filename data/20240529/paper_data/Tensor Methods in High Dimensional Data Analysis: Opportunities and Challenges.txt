Tensor Methods in High Dimensional Data Analysis: Opportunities
and Challenges
Arnab Auddy∗, Dong Xia† and Ming Yuan‡
∗University of Pennsylvania
†Hong Kong University of Science and Technology
‡Columbia University
(May 29, 2024)
Abstract
Largeamountofmultidimensionaldatarepresentedbymultiwayarraysortensorsarepreva-
lent in modern applications across various fields such as chemometrics, genomics, physics, psy-
chology, and signal processing. The structural complexity of such data provides vast new op-
portunities for modeling and analysis, but efficiently extracting information content from them,
bothstatisticallyandcomputationally,presentsuniqueandfundamentalchallenges. Addressing
these challenges requires an interdisciplinary approach that brings together tools and insights
from statistics, optimization and numerical linear algebra among other fields. Despite these
hurdles, significant progress has been made in the last decade. This review seeks to examine
some of the key advancements and identify common threads among them, under eight different
statistical settings.
1 Introduction
There is a long and rich history of the use of tensor methods in data analysis. See, e.g, Coppi and
Bolasco(1989);Kroonenberg(2008);McCullagh(2018)andreferencestherein. Indeed, manyofthe
most popular tensor decomposition methods originated from factor analysis of multiway data (see,
e.g.,Tucker,1966;CarrollandChang,1970;Harshman,1970). Theuseoftensormethods,however,
have enjoyed a renaissance of sorts in recent years. See, e.g., Cichocki et al. (2015); Sidiropoulos
et al. (2017); Bi et al. (2021). On the one hand, this is the direct consequence of collecting more
∗Arnab Auddy’s research was supported by NSF Grant DMS-2015285.
†Dong Xia’s research was supported by Hong Kong RGC grant GRF 16302020 and GRF 16300121.
‡Ming Yuan’s research was supported by NSF Grants DMS-2015285 and DMS-2052955.
1
4202
yaM
82
]TS.htam[
1v21481.5042:viXradata with more complex relational structure. For example, it is much more intuitive to think of a
videoasthreedimensionalarraythanamatrix,andanalyzeitassuch. Ontheotherhand,methods
of moments have emerged as the most popular approach to various high dimensional latent factor
models for which the maximum likelihood estimates are often difficult to compute; and tensors
arise naturally when considering higher order moments of a random vector.
Following the convention in the related literature, here and in what follows, we shall use the
term “tensor” loosely in that (i) we shall not make an effort to differentiate between tensors and
multiway arrays and (ii) when referring to tensors, we assume they are of order at least three so
that they can be differentiated from matrices or vectors, e.g., tensors of order two or one.
1.1 Why Tensor Methods
Tensor methods have presented numerous new opportunities for statistical modeling and data
analysisthatarenotavailableformatrix-basedtechniques. Conceptually, theyofferabetterwayto
organizedatawithcomplexrelationalstructuresandleadtoanalysiswithenhancedinterpretability.
From a modeling perspective, they ensure the identifiability of latent factors and other parameters
thatmaynototherwisebewelldefined. Finally, itisalsoworthwhiletousetensormethodsfroman
inferential point of view because the multilinear structure can help restrict the impact of sampling
error. Wenowuseprincipalcomponentanalysis(PCA)asanexampletoillustratethesekeymerits.
1.1.1 Conceptual Benefit – Interpretability
Most of the statistical techniques are designed for data matrices consisting of observations (rows)
for multiple variables (columns). They can be inadequate for analyzing data with richer and more
complex structures, and addressing research questions such as “Who Does What to Whom and
When” (Kroonenberg, 2008).
Consider, for example, applying PCA to the spa-
tiotemporalgeneexpressiondataofthehumanbrainfrom
Kang et al. (2011). The data, after preprocessing, con-
tains expression of 1087 genes over 10 different brain re-
gionsfor13developmentalperiods. Toapplytheclassical
PCA, one would be forced to discard the spatial tempo-
Figure 1: Loadings of the first principal
ral information and format the data into a matrix of 1087
components for different brain regions:
genes (rows) and 130 combinations of location and pe-
the darkness of the dots represent the
riod (columns). On the other hand, as demonstrated by
magnitude of the loadings. Reproduced
Liu et al. (2022), treating the data as third order tensor
from Liu et al. (2022).
enables us to delineate the growth pattern and spatial
2distribution of gene expression. In particular, as shown in Figure 1, the factor loadings of the first
principal components for different brain regions highlights the relevance of spatial proximity.
1.1.2 Benefit for Modeling – Identifiability
A remarkable property of tensors, first discovered by Kruskal (1977), is that nearly all low rank
tensors can be uniquely written as a sum of rank-one tensors. This is to be contrasted with the
fact that there are infinite many ways to write a low rank matrix as a sum of rank-one matrices.
The implication of such identifiability can be demonstrated in the comparison between PCA and
independent component analysis (ICA). The principal components (PC) can be identified as the
singular vectors of the covariance matrix, and likewise, the independent components (IC) are as-
sociated with the “singular vectors” of the fourth order cumulant tensor. Suppose that we observe
d independent random variables with mean zero, unit variance and nonzero excess kurtosis, up
to an unknown rotation. We cannot reconstruct these variables via PCA because the variance is
invariant to any rotation. We can however do so from its fourth order cumulant tensor which has
rank d with each component corresponding to one of the independent variables.
Figure 2: ICA vs PCA: κ (u⊤X) = E(u⊤X)4 3 and cov(u⊤X) as functions of u over the unit
4
−
circle.
As a more concrete example, consider the case of two independent random variables X =
(X ,X )⊤ with var(X) = I . The left panel of Figure 2 shows the excess kurtosis as a function
1 2 d
of u = (u ,u )⊤: f(u) := κ (u X +u X ) over the unit circle. There are only four maximizers
1 2 4 1 1 2 2
corresponding to the independent components X and X respectively. In contrast, cov(u⊤X)
1 2
± ±
remains constant over all u on the unit circle, as shown in the right panel.
1.1.3 Benefit for Inferences – Restricted Effect of Perturbation
Statistical analysis is based on noisy observations and subject to sampling error. Understanding
the effect of sampling error plays a central role in any inference procedure. A recent observation
is that, due to the inherent structural constraints, effect of sampling error on tensors can be more
3isolated than matrices which may allow us to conduct statistical inferences that are infeasible or
impossible for data matrices.
Original Data PCA components ICA components
0.50 2
0.25 1 1
0.00 0 0
−0.25 −1 −1
−0.50 −2
−0.50−0.25 0.00 0.25 0.50 −2 −1 0 1 2 −1 0 1
Original Data PCA components ICA components
0.2 1 1
0.0 0 0
−0.2 −1 −1
−0.50−0.25 0.00 0.25 0.50 −1 0 1 −1 0 1
Figure 3: Effect of eigengap on PCA and ICA.
Take the classical PCA as an example. The sample PCs are intrinsically linked together: how
well a sample PC approximates its population counterpart is determined by the estimation error
of the sample covariance matrix as well as how far its corresponding eigenvalue is from the others,
and its distribution generally depends on all eigenvalues and eigenvectors of the covariance matrix.
This means that inferences about one PC necessarily require information about all others and
therefore impractical. Both hindrances disappear when considering ICA or PCs with multilinear
structure. Figure 3 illustrates how eigengap may affect PCA but not ICA. The top left panel
plots 500 observations from U([ 0.5,0.5] [ 0.5/1.05,0.5/1.05]). Because the two coordinates are
− × −
independent with different variances, they are both the PCs and ICs. The top middle and right
panelsplotthetwosamplePCsandsampleICs,respectively. ItisclearthatthesamplePCsperform
poorlyevenwithsuchalargesamplesize. Thebottompanelsrepeatthesameexercisewithalarger
eigengap: observations are now sampled from U([ 0.5,0.5] [ 0.5/1.5,0.5/1.5]). Comparing the
− × −
two scenarios, the impact of eigengap on PCs is evident, whereas the ICs are virtually unaffected.
1.2 Challenges of Using Tensor Methods
Whilethereareplentyofreasonstousetensormethodsindataanalysis, howtodosoappropriately
andtakefulladvantageofthebenefitstheycanofferposemanyuniqueandfundamentalchallenges.
Conceptually, many standard notions and intuitions we have developed for matrices need to
be re-examined when extending to higher order tensors. One of the most notable examples is the
4best low rank approximation. In the case of matrices, the best low rank approximation can be
characterized by singular value decomposition thanks to Eckart-Young theorem (see e.g., Bhatia,
2013; Stewart and Sun, 1990). In the case of higher order tensors, this is much more subtle since
the best low rank approximation may not even exist! See, e.g., Hackbusch (2012). Even worse, this
is not a mere pathological exception but rather a prevalent phenomenon. Consider, for example,
a 2 2 2 tensor T whose (1,1,1) and (2,2,2) entries are 1 and other entries are 0. Suppose
× ×
that we observe X = T +σE where E is an iid standard normal ensemble and want to estimate
T knowing apriori that it has rank two. The maximum likelihood estimate (MLE) can then be
defined as the best rank-two approximation to X. Unfortunately, it can be derived that, for any
σ > 0, there is a nonzero probability that the MLE defined as such does not even exist.
Another ubiquitous challenge is computation. As noted by Hillar and Lim (2013), “most tensor
problems are NP hard”. This means that we need to be always mindful of the computational
feasibility of tensor methods when applying them to high dimensional data. Indeed one of the most
intriguing phenomena that is often observed for tensor methods is that statistically optimal proce-
dures are often found to be computationally infeasible, yet computationally tractable procedures
are known to be statistically suboptimal.
1.3 Organization
In this article, we shall review representative progresses that have been made to take advantage
of the benefits offered by tensors while tackling the challenges they bring about in eight statistical
settings. These examples are selected to illustrate how the opportunities and challenges of tensor
methods manifest in different ways and under different contexts. Given the fast growing literature,
the review is by no means exhaustive and the choices of examples necessarily reflect our personal
and oftentimes unintentional bias.
The rest of the article is organized as follows: we shall first briefly introduce some of the basic
notations as well as algebraic and algorithmic tools for tensors in the next section. Section 3
discusses how relevant tensor methods have been used, what the new challenges are, how they can
be addressed, and what the unique phenomena are in the eight statistical problems.
2 Background and Tools
2.1 Notation
Wedenotevectorsandmatricesbyboldfacedloweranduppercaselettersrespectively: e.g., v Rd
∈
and M Rd1×d2; and tensors by script-style letters, e.g., T . Entries of a matrix or tensor is
∈
5represented by upper-case letters with a set of indices, e.g., T is the (i ,...,i ) entry of an
i1i2···ip 1 p
pth order tensor T . We first list a set of basic notations and tensor operations that we shall use
throughout the paper.
2.1.1 Matricization
A p-th order tensor T Rd1×···×dp has the mode-k matricization k(T ) Rd k×(cid:81) j̸=k,j∈[p]dj with
∈ M ∈
elements ( (T )) = T where
Mk i kj i1...i k−1i ki k+1...ip
(cid:88) (cid:89)
j = 1+ (i 1)J with J = d .
l l l m
−
l∈[p],l̸=k m∈[l−1],m̸=k
2.1.2 Outer Product
For two tensors T Rd1×···×dp, T ′ Rd′ 1×···×d′ q, their outer product T T ′ is a (p+q)th order
∈ ∈ ◦
tensor whose entries are given by
(T T ′) = T T′ .
◦
i1...ipj1...jq i1...ip j1...jq
In particular, the outer product of two vectors is a matrix.
2.1.3 Multiplication
A tensor T Rd1×···×dp can be multiplied along its k-th mode by a matrix M Rm k×d k to get a
∈ ∈
new tensor T ′ := T
k
M Rd1×d2×...d k−1×m k×d k+1···×dp with elements
× ∈
(cid:88)
T′ = T M
i1...i k−1j ki k+1...ip i1...ip j ki k
i ∈[d ]
k k
for i [d ] for l = 1,...,p, l = k, and j [m ].
l l k k
∈ ̸ ∈
2.1.4 Norms
The inner product between two tensors T ,T ′ Rd1×···×dp is given by
∈
(cid:88)
T ,T ′ = T T′ .
⟨ ⟩
i1...ip i1...ip
i1,...,ip
The Frobenius norm of a tensor is then defined as T := T ,T 1/2. The spectral norm or
F
∥ ∥ ⟨ ⟩
operator norm of T is defined as T := sup T ,u u , where Sd−1 denotes the
∥ ∥ u k∈Sdk−1 ⟨ 1 ◦···◦ p ⟩
unit sphere in Rd.
62.2 Tensor Decompositions
Singular value decomposition (SVD) plays a central role in analyzing data formatted in matrices.
It turns out that there are multiple ways to generalize SVD to higher order tensors, most notably
the so-called CP decomposition and Tucker decomposition.
2.2.1 CP Decomposition
One of most popular approaches to tensor decomposition is the so-called Canonical Polyadic (CP)
decomposition(Hitchcock,1927;CarrollandChang,1970;Harshman,1970)thatexpressesatensor
as the sum of rank-one tensors, e.g., tensors that can be expressed as outer product of vectors. To
fix ideas, consider a third order tensor T Rd1×d2×d3. Its CP decomposition has the form
∈
r
(cid:88)
T = λ a b c . (1)
i i i i
◦ ◦
i=1
where λ is are scalars, and a
i
Sd1−1, b
i
Sd2−1 and c
i
Sd3−1 are unit vectors.
∈ ∈ ∈
CP Rank. The smallest integer r for which such a decomposition holds is called the CP-rank
of T . It is convenient to use the notation T = [λ;A,B,C] for CP decomposition as described
by Equation 1 where λ = (λ ,...,λ )⊤, A = [a a ... a ], and similarly B = [b b ... b ], and
1 r 1 2 r 1 2 r
C = [c c ... c ].
1 2 r
Orthogonally Decomposable Tensors. CP decomposition can be viewed as a generalization of
SVD. In the case of matrices, the factor matrices can also be made orthonormal without loss
of generality. For tensors this is generally not possible. But if a tensor does allow for a CP
decomposition with orthonormal factor matrices, e.g.,
a ,a = b ,b = c ,c = 0, i = j,
i j i j i j
⟨ ⟩ ⟨ ⟩ ⟨ ⟩ ∀ ̸
we say it is orthogonally decomposable (ODECO).
Uniqueness. Note that factors a , b and c in CP decomposition are not uniquely
i i i
{ } { } { }
defined. For example, we can always replace a by a and b by b , or by permuting the
i i i i
− −
indices. Interestingly, however, modulo this trivial ambiguity, they may be uniquely identified.
This remarkable observation was first made by J. B. Kruskal who showed (Kruskal, 1977) that if
k(A)+k(B)+k(C) 2r 1,
≥ −
then the CP decomposition of T given by Equation 1 is unique up to a permutation of the indices
1,...,r and sign changes of the factors. Here for a matrix M Rd×r, k(M) denotes its Kruskal
{ } ∈
rank, that is the maximum integer k such that any k columns of M are linearly independent. This
immediately implies that ODECO tensors are identifiable.
72.2.2 Tucker Decomposition
Tucker decomposition (Tucker, 1963) is another popular way to generalize SVD to tensors. More
specifically, the Tucker decomposition of a third order tensor T Rd1×d2×d3 can be written as:
∈
T = [C;U ,U ,U ] := C U U U ,
1 2 3 1 1 2 2 3 3
× × ×
where C Rr1×r2×r3 is a so-called core tensor, and U
q
Rdq×rq is the q-th mode component
∈ ∈
matrices, for q = 1,2,3. Note that Tucker decomposition is not unique. For example, for any
invertible matrix M Rr1×r1,
∈
T = [C;U ,U ,U ] = [C M;U M−⊤,U ,U ].
1 2 3 1 1 2 3
×
Higher Order SVD. Without loss of generality, we can always take component matrices U(q)s
to be orthonormal as a generalization of the matrix SVD. Note that
q(T ) = U
q
q(C)( q′̸=qU q′)⊤
M M ⊗
for q = 1,2,3 where stands for Kronecker product. De Lathauwer et al. (2000b) showed that we
⊗
can force the rows of q(C) to be orthogonal for all q = 1,2,3 so that U
q
Rdq×rq is the left
M ∈
singular matrix of (T ), and the ℓ norm of the rows of M (C) are the singular values. This is
q 2 q
M
the so-called higher order SVD (HOSVD).
Multilinear ranks. Tucker decomposition also corresponds to a new notion of rank for tensors.
In particular, we can take r to be the rank of (T ) and the p-tuple (r ,...,r ) is often referred
q q 1 p
M
to as the multilinear rank or Tucker rank of T . It is worth noting that, in general, r s are different.
q
2.3 Algorithms
In a nutshell, most of the tensor methods for data analysis can be reduced to finding a low-rank,
either CP rank or Tucker rank, tensor that fits the data well. This is often cast as minimizing a loss
function L( ) over tensors satisfying certain rank constraints. Such problems are generally highly
·
nonconvex and computationally challenging. We now discuss several popular practical strategies.
2.3.1 Alternating Minimization
The idea behind alternating minimization is that we can reparametrize a low rank tensor by its
decomposition and then optimize over the factors in an iterative fashion. Consider, for example,
optimizing L over (d 1,d 2,d 3,r 1,r 2,r 3), which is a certain subset of tensors in Rd1×d2×d3 with
F
multilinearranks(r ,r ,r ). Additionalstructuralinformationsuchassparsityorincoherencemay
1 2 3
also be incorporated in (d ,d ,d ,r ,r ,r ). The alternating minimization algorithm operates
1 2 3 1 2 3
F
8on the joint space induced by the tensor factorization form. By re-parametrizing a tensor T
∈
(d ,d ,d ,r ,r ,r ) using Tucker decomposition, the optimization problem can be equivalently
1 2 3 1 2 3
F
written as
min L(cid:0) [C;U ,U ,U ](cid:1) , s.t. C Rr1×r2×r3, U (d ,r ), k = 1,2,3.
1 2 3 k k k
∈ ∈ U
The feasible set (d k,r k) is a subset of Rd k×r k, predetermined according to particular applications
U
orforidentifiabilitypurposes. Thealternatingminimizationalgorithmisaniterativeprocedurethat
minimizes the loss function by alternately optimizing over individual parameters. The pseudocodes
for alternating minimization are provided in Algorithm 1. The alternating minimization algorithm
usually converges fast provided that a sufficiently good initialization is given and the sub-problems
in Equation 2 can be solved efficiently.
Algorithm 1 Alternating Minimization
Input: loss L(cid:0) [C;U 1,U 2,U 3](cid:1) ; feasible sets U k (d k,r k) Rd k×r k for k = 1,2,3
∈ U ⊂ ∀
Initialization: C (cid:98)[0]
∈
Rr1×r2×r3 and U(cid:98)[ k0]
∈
U(d k,r k) for ∀k = 1,2,3;
Set t 0.
←
while not converged do
Update by
(cid:16) (cid:17)
U(cid:98)[t+1]
argmin L
(cid:2)C (cid:98)[t];U,U(cid:98)[t] ,U(cid:98)[t](cid:3)
1 ←− 2 3
U∈U(d1,r1)
(cid:16) (cid:17)
U(cid:98)[t+1]
argmin L
(cid:2)C (cid:98)[t];U(cid:98)[t+1] ,U,U(cid:98)[t](cid:3)
(2)
2 ←− 1 3
U∈U(d2,r2)
(cid:16) (cid:17)
U(cid:98)[t+1]
argmin L
(cid:2)C (cid:98)[t];U(cid:98)[t+1] ,U(cid:98)[t+1] ,U(cid:3)
3 ←− 1 2
U∈U(d3,r3)
(cid:16) (cid:17)
C (cid:98)[t+1] argmin L (cid:2)C;U(cid:98)[t+1] ,U(cid:98)[t+1] ,U(cid:98)[t+1](cid:3)
←− C∈Rr1×r2×r3 1 2 3
Set t t+1.
←
end while
Return: T (cid:99)[t] = (cid:2)C (cid:98)[t];U(cid:98)[t] ,U(cid:98)[t] ,U(cid:98)[t](cid:3) .
1 2 3
Higher-order orthogonal iteration. The higher-order orthogonal iteration (HOOI) is a special
case of alternating minimization algorithm applied to the loss function L(T ) = X T 2 with
∥ − ∥F
the feasible set (d ,r ) = O , the collection of d r matrices with orthonormal columns.
k k d ,r k k
U k k ×
Here, X represents a d d d data tensor. The sub-problems Equation 2 admit an explicit
1 2 3
× ×
solution, for example,
(cid:16) (cid:17)
U(cid:98)[ 1t+1]
= SVD
r1
M1(cid:0)X ×2U(cid:98)[ 2t]⊤
×3U(cid:98)
3[t]⊤(cid:1)
,
9whereSVD ( )returnsthetop-r leftsingularvectorsofamatrix. HOOIwasinitiallyintroducedby
r
·
De Lathauwer et al. (2000a) to study the best low-rank approximation of a given tensor. Variants
of HOOI have been designed to accommodate additional structures. See, e.g., Allen (2012); Sun
et al. (2017); Zhang and Han (2019); Xia et al. (2021); Ke et al. (2019); Jing et al. (2021).
Power Iteration. Tensor power iteration (tens-PI) is a special case of HOOI used for finding the
best rank-one approximation of a given tensor. It most often appears in alternating minimization
for optimizing L over the set of CP-decomposable tensors. By re-parametrizing a tensor T using
(cid:0)(cid:80)r (cid:1)
CP-decomposition, we aim to minimize L λ a b c , where λ s are scalars and a ,b ,c s
i=1 i i ◦ i ◦ i i i i i
are unit vectors. The algorithm alternately minimizes the loss function over individual rank-one
components. This approach leads to the tensor power iteration algorithm if L(T ) = X T 2 for
∥ − ∥F
agivendatatensorX. Forinstance, giventheestimates(cid:8) λ(cid:98) i[t] ,a (cid:98)i[t] ,b(cid:98) i[t] , (cid:98)c i[t](cid:9)r
i=1
atthet-thiteration,
the alternating minimization algorithm updates by solving
(λ(cid:98) j[t+1] ,a (cid:98)[ jt+1] ,b(cid:98)[ jt+1] , (cid:98)c[ jt+1] )
←−
a λr ,g a,m b,i cn (cid:13) (cid:13) (cid:13)(cid:16) X −(cid:88)j i=− 11 λ(cid:98) i[t+1] a (cid:98)i[t+1] ◦b(cid:98) i[t+1] ◦(cid:98)c i[t+1]
−(cid:88)r i=j+1λ(cid:98) i[t] a (cid:98)i[t] ◦b(cid:98)[ it] ◦(cid:98)c i[t](cid:17) −λa ◦b ◦c(cid:13) (cid:13) (cid:13)2 F,
whichisoftensolvedbythetensorpoweriterationalgorithm. Tensorpoweriterationhasbeenused
in many works including Anandkumar et al. (2014a,b); Mu et al. (2015, 2017); Sharan and Valiant
(2017). In statistical applications, tens-PI is used with context aware smart initializations for SVD
of the covariance tensor in Liu et al. (2022), for learning mixture models in Anandkumar et al.
(2015), and for ICA in Auddy and Yuan (2023b). A truncated tens-PI has been used for sparse
CP decomposition in Sun et al. (2017). Tens-PI has been proven effective for tensor completion in
Jain and Oh (2014).
2.3.2 Gradient Descent
Another common strategy for optimizing over low rank tensors is gradient descent. To minimize
L over (d ,d ,d ,r ,r ,r ), gradient descent algorithms can operate directly on the space of
1 2 3 1 2 3
F
d d d tensors with multilinear ranks (r ,r ,r ). Multiple choices of gradients are available,
1 2 3 1 2 3
× ×
such as the standard (vanilla) gradient in Euclidean space and the Riemannian gradient in the
associated manifolds. The general architect of gradient descent algorithms can be described by
Algorithm 2. Here, Proj ( ) denotes the projection operator, which enforces that
F(d1,d2,d3,r1,r2,r3)
·
each update resides in (d ,d ,d ,r ,r ,r ).
1 2 3 1 2 3
F
Projected Gradient Descent. When the vanilla gradient is computed with G (cid:98)[t] := L(T (cid:99)[t]),
∇
it results in the projected gradient descent algorithm. The gradient G (cid:98)[t] is typically a full-rank
tensor, as is the gradient descent update T (cid:99)[t] η G (cid:98)[t]. The projection step entails finding a low-
− ·
10Algorithm 2 Gradient Descent
Input: loss L(T ); initializations T (cid:99)[0] (d 1,d 2,d 3,r 1,r 2,r 3); step size η > 0; t = 0
∈ F
while not converged do
Compute a certain type of gradient: G (cid:98)[t] grad(cid:0) L(T (cid:99)[t])(cid:1)
←(cid:16)− (cid:17)
Update by T (cid:99)[t+1] Proj T (cid:99)[t] η G (cid:98)[t]
←−
F(d1,d2,d3,r1,r2,r3)
− ·
Set t t+1
←
end while
Return: T (cid:99)[t].
rankapproximationofthegivenfull-ranktensor. Althoughfindingthebestlow-rankapproximation
is a challenge for tensors, a reasonably good approximation is sufficient to ensure the convergence
of Algorithm 2. As demonstrated in Chen et al. (2019b), HOSVD can serve as the projection step,
and Algorithm 2 converges at a linear rate, yielding a statistically optimal estimator under various
models.
Riemannian Gradient Descent. By viewing (d ,d ,d ,r ,r ,r ) as a Riemannian manifold,
1 2 3 1 2 3
F
one can take G (cid:98)[t] as the Riemannian gradient, which leads to the Riemannian gradient descent
algorithm (Edelman et al., 1998; Kressner et al., 2014). The Riemannian gradient has multilinear
ranks at most (2r ,2r ,2r ) ensuring that the gradient descent update has multilinear ranks at
1 2 3
most (3r ,3r ,3r ). This low-rank structure enables much faster computation for the subsequent
1 2 3
projection step. Riemannian gradient descent was investigated by Cai et al. (2022a) for robust
tensor decomposition and by Cai et al. (2022b) for noisy tensor completion.
Grassmannian and Joint Gradient Descent. Gradient descent algorithms can also operate on
the joint space induced by the tensor factorization form. To minimize L(cid:0) [C;U ,U ,U ](cid:1) , one can
1 2 3
computethegradientswithrespecttoC andU s, respectively. TheconstraintU O isoften
k k d ,r
∈ k k
imposedforidentifiabilitypurposes. ByviewingO astheGrassmannianmanifold,wecanutilize
d ,r
k k
the Grassmannian gradient to update U(cid:98)k, leading to the Grassmannian gradient descent algorithm
(Xia and Yuan, 2019; Lyu et al., 2023b). Instead of enforcing orthogonality constraints, Han et al.
(2022b) considers minimizing L(cid:0) [C;U ,U ,U ](cid:1) with the penalty λ (cid:80)3 U⊤U I 2 and
1 2 3 1 k=1∥ k k − r3∥F
proposes a vanilla gradient descent algorithm, jointly with respect to C and U s.
k
2.3.3 Overcoming Nonconvexity
Problems involving low rank tensors are usually highly nonconvex with many local optima. Either
alternatingminimizationorgradientdescentalgorithmsarepronetobestuckwithbadlocaloptima.
An intriguing and recurring phenomenon that is often observed is that with a good initialization,
one can usually obtain a statistically efficient estimate with an optimal signal-to-noise ratio. Yet to
11overcome the nonconvexity, a much higher signal-to-noise ratio is necessary. Nonetheless, several
strategies are often used to alleviate the challenge.
Spectral Initialization. Dependingonthe datageneratingmechanism, differenttypes ofspectral
initialization are commonly used. In particular, HOSVD was widely used for initialization when
considering tensors of low Tucker ranks. See, e.g., Zhang and Xia (2018); Cai et al. (2019); Mon-
tanari and Sun (2018); Ke et al. (2019). Both matricization, e.g., unfolding a tensor into matrices,
and random slicing, e.g., random linear combinations of all slices, are often used for ODECO ten-
sors and more generally in the context of CP decomposition. See, e.g., Anandkumar et al. (2014a,
2015);AuddyandYuan(2023b). XiaandYuan(2019)introducedaspectralinitializationapproach
based on a second-order U-statistics that has been shown to be effective in a number of settings
including tensor completion and regression (see, e.g., Cai et al., 2022b; Xia et al., 2021).
Convex Relaxations. Anotherpopularstrategytoovercomethenonconvexityisbyconvexrelax-
ation. Thisisusuallyaccomplishedbymatricizationandresorttomatrix-basedconvexapproaches.
See, e.g., Gandy et al. (2011); Mu et al. (2014); Raskutti et al. (2019). More powerful semidefi-
nite or sum-of-squares relaxation approaches are also commonly considered. See, e.g., Jiang et al.
(2015); Tomioka and Suzuki (2013); Hopkins et al. (2015); Ma et al. (2016); Schramm and Steurer
(2017).
Other Approaches. One common way to avoid being stuck at a local optimum is by adding
occasional and carefully designed jumps to the original iterative algorithms. See, e.g., Belkin et al.
(2018). Adding stochastic noise to gradients is another way to escape the trap of local optima (see,
e.g., Ge et al., 2015). The approximate message passing algorithm has been proven successful in
learning rank-one tensor PCA, as demonstrated by Richard and Montanari (2014).
3 Statistical Models
We now consider several specific tensor-related statistical problems. To fix ideas, unless otherwise
indicated, we shall focus our discussion on third order cubic tensors, e.g., T Rd×d×d. Oftentimes,
∈
more general results are available but we opt for this simplification for brevity.
3.1 Tensor SVD
Let us begin with a canonical example for studying tensor methods where we wish to recover the
decompositionofasignaltensorT fromanobservationcontaminatedwithnoise, e.g., X = T +E.
For simplicity, we assume that the noise E is an iid ensemble, e.g., its entries are independently
sampled from a common distribution.
123.1.1 Computational Gap
Tensor SVD provides a simple statistical model to understand the role of computational consider-
ations in tenor related problems.
Gaussian Noise. The simplest case is when the signal tensor T has rank one, e.g., T =
[λ;a,b,c] = λa b c where λ > 0, a,b,c Sd−1. When E has independent standard Gaussian
◦ ◦ ∈
entries, the maximum likelihood estimate (MLE) is given by
(λ(cid:98),a (cid:98)SVD,b(cid:98)SVD, (cid:98)c SVD) := argmin X γx y z 2 F. (3)
γ>0,x,y,z∈Sd−1∥ − ◦ ◦ ∥
Richard and Montanari (2014) showed that for λ ≳ √d, the MLE is indeed consistent, and con-
versely, if λ ≲ √d, then no consistent estimates of a,b,c exist.
However, as proved by Arous et al. (2019), the optimization problem on the right hand side
of Equation 3 can have exponentially, in d, many spurious local minima when λ d. This
≪
computationalchallengecanbeovercomeby,forexample,spectralinitializationaftermatricization.
Morespecifically,therighthandsideofEquation3canbeoptimizedbypoweriterationintializedby
HOSVD,e.g.,a[0],b[0] andc[0] aretheleadingleftsingularvectorof (X), (X)and (X)
1 2 3
M M M
respectively. This approach yields consistent estimate of T when λ ≳ d3/4. The same signal-
to-noise ratio requirement can also be achieved by a number of alternative algorithms including
approximate message passing (see, e.g., Richard and Montanari, 2014). Nonetheless, there remains
an additional cost in the signal-to-noise ratio required for consistent estimate for computationally
tractable methods.
General Noise. This observation goes beyond consistency and Gaussian noise. As shown by
Auddy and Yuan (2022), the gap between statistical efficiency and computational tractability can
be characterized by moment conditions of the entries of E. Specifically, consider the case λ = dξ
and E E α < for some α 4. Then the minimax optimal rate of convergence is given by
111
| | ∞ ≥
inf sup max sin∠(a (cid:101),a),sin∠(b(cid:101),b),sin∠( (cid:101)c,c) d1/2−ξ,
a (cid:101),b(cid:101), (cid:101)ca,b,c∈Sd−1 { } ≍
where the infimum is taken over all estimates based on X. This rate of convergence can be
attained, in particular, by a (cid:98)SVD,b(cid:98)SVD, (cid:98)c
SVD
when ξ > max 1/2,1/4+2/α . On the other hand,
{ }
computationally tractable algorithms they developed can only reach minimax optimality when
ξ > 3/4, thus creating a gap in signal-to-noise ratio requirement. See Figure 4. It is interesting to
note that for any α > 8, the gap is identical to the Gaussian case but narrows as α decreases to 4.
Low Tucker Rank Spikes. Generalizing beyond rank one case in a Tucker decomposition frame-
work, one may consider T = [C;U ,U ,U ] where C Rr×r×r is the core tensor and we wish to
1 2 3
∈
estimate the semi-orthonormal component matrices U Rd×r for q = 1,2,3. Under the Gaussian
q
∈
13Computationally Tractable and
sin∠(aˆ,a)≲p d1/2 −ξ
3
4
Computationally Intractable but
sin∠(aˆ SVD,a)≲p d1/2 −ξ
1
2
sin∠(aˆ SVD,a) 1
→
4 α 8
Figure 4: Statistical and computational tradeoff for tensor SVD: horizontal axis represents the
moment condition for noise, vertical axis corresponds to the signal strength. Reproduced and
modified from Auddy and Yuan (2022).
noise, Zhang and Xia (2018) showed that similar computational gap exists. More specifically, if
r ≲ √d and λ min := min q=1,2,3σ r( q(C)) ≳ λ comp d3/4, then
M ≍
(cid:16) (cid:17)
sinΘ U(cid:98)q,U q ≲ √d/λ min for q = 1,2,3,
where U(cid:98)q is the estimate computed by HOOI initialized via HOSVD. Furthermore, this rate is also
minimax optimal.
3.1.2 Perturbation Bounds
As in the matrix case, perturbation bounds play a crucial role in the analysis of low rank tensors
in general and tensor SVD in particular. For ODECO tensors, deterministic perturbation bounds
for singular values and vectors, in a spirit similar to classical results for matrices such as those
due to Weyl, Davis, Kahan and Wedin, can be established. More specifically, assume that T =
(cid:80)d i=1λ ia
i
◦b
i
◦c
i
is an ODECO tensor. Let T (cid:99)= (cid:80)d i=1λ(cid:98)ia
(cid:98)i
◦b(cid:98)i ◦(cid:98)c
i
be an (arbitrary) estimate of
T that is also ODECO. Auddy and Yuan (2023a) showed that there exists a numerical constant
C > 1 such that
max λ(cid:98)i λ i C T (cid:99) T
1≤i≤d| − | ≤ ∥ − ∥
and there is a permutation π : [d] [d] such that
→
C T (cid:99) T
max sin∠(a (cid:98)π(i),a i),sin∠(b(cid:98) π(i),b i),sin∠( (cid:98)c π(i),c i) ∥ − ∥.
{ } ≤ λ
i
Although these bounds are similar in spirit to those for matrices, there are also crucial distinctions.
In particular, the sinΘ theorems of Davis-Kahan-Wedin bound the perturbation effect on the ith
singular vector by C T (cid:99) T /min i′̸=i λ i′ λ i . The dependence on the gap between λ i and other
∥ − ∥ | − |
14
dgol/λgol
=:
ξsingular values is unavoidable for matrices. This is not the case for higher-order ODECO tensors
where perturbation affects the singular vectors in separation. Note that the permutation π of
indices is necessary because we do not require that λ s are distinct or sufficiently apart from each
i
other.
Furthermore, iftheestimationerrorissmallwhencomparedwithsignal, e.g., T (cid:99) T = o(λ i),
∥ − ∥
then we can take the constant in the above bounds to be one:
λ(cid:98) π(i) λ i T (cid:99) T
| − | ≤ ∥ − ∥
and
(cid:110) (cid:111) T (cid:99) T
max sin∠(a (cid:98)π(i),a i),sin∠(b(cid:98) π(i),b i),sin∠( (cid:98)c π(i),c i) ∥ − ∥(1+o(1)). (4)
≤ λ
i
Bothboundscanbeshowntobesharplyoptimal. SeeAuddyandYuan(2023a)forfurtherdetailed
discussion.
Applying these perturbation bounds one can see that the minimax optimal rates for estimating
ODECO T are essentially the same as those for rank-one spikes and can be attained, in particular,
byT (cid:99)= argminA
is ODECO
X A . Thisestimate, asintherank-onespikecase, isnotamenable
∥ − ∥
for computation. There is a fruitful line of recent research in developing computationally tractable
estimates and algorithm dependent bounds, which again incates a gap in performance between
computationlly tractable and intractable methods. See, e.g., Anandkumar et al. (2014a); Luo et al.
(2021); Janzamin et al. (2019).
3.2 Multiway PCA
Tensor SVD is closely related to multiway PCA. Traditional PCA is a useful tool for dimension
reduction but it does not account for the multiway structure when each observation consists of
a matrix or a possibly higher order array. Naively applying PCA to multiway observations by
“flattening” them into vectors is statistically inefficient and also lacks interpretability. To fix ideas,
let us focus here on the case when each observation is an array of dimension d d d.
× ×
Recall that, for a random vector X Rd, the first PC direction is the vector a Sd−1 which
∈ ∈
maximizes σ2(a) := Var( X,a ). Similarly, for a tensor X Rd×d×d, it is natural to define the
⟨ ⟩ ∈
leading multiway principal component U Rd×d×d as:
1
∈
U = argmax Var( X,a b c ).
1
a◦b◦c, a,b,c∈Sd−1 ⟨ ◦ ◦ ⟩
The constraint that U is rank one, i.e., U = u(1) u(2) u(3) for u(1) ,u(2) ,u(3) Sd−1, helps
1 1 1 ◦ 1 ◦ 1 1 1 1 ∈
ensure that U conforms to the multiway nature of X. Other PCs can be defined successively:
1
U = argmax Var( X,a b c ).
k
⟨ ◦ ◦ ⟩
a◦b◦c, a,b,c∈Sd−1,
a⊥a,b⊥b,c⊥c,l<k
l l l
15Once again, similar to classical PCA, the successive PCs are defined to maximize variance in the
directions orthogonal to previously found PCs U ,...,U . The sample multiway PCs can be
1 k−1
defined similarly with population variance now replaced by sample variances.
3.2.1 PCA or SVD
Wheneachobservationisavector,samplePCscanbeidentifiedwithSVDofthedatamatrixwhere
each row corresponds to an observation, after appropriate centering. See, e.g., Jolliffe (2002). As
such, one often uses the two terms, PCA and SVD, interchangeably. The equivalence, interestingly,
does not hold for multiway PCA in general even though the subtle differences between the two are
often mistakenly neglected.
It is easy to see that
σ2(U )U U + +σ2(U )U U (5)
1 1 1 r r r
◦ ··· ◦
is a rank-r greedy ODECO approximation to the covariance operator T = Cov(X), which is a
Σ
d d d sixth order array with entries σ = Cov(X ,X ). Likewise, the sample
× ···×
i1i2i3j1j2j3 i1i2i3 j1j2j3
counterpart of Equation 5 is a rank-r greedy ODECO approximation to the sample covariance
operator. In general, however, U s cannot be identified with best low rank approximations to the
k
data tensor X Rd×d×d×n where each mode-four slice is one observation.
n
∈
3.2.2 Asymptotic Independence
Ouyang and Yuan (2023) showed that there are many statistical benefits to consider multiway
PCA. Consider a spiked covariance model:
d
(cid:88)
X = σ θ U +σ E,
k k k 0
k=1
where (θ ,...,θ )⊤ N(0,I ) are the random factors, and U = u(1) u(2) u(3) Rd×d×d are rank
1 r ∼ r k k ◦ k ◦ k ∈
one principal components with u(q) ,u(q) = 1 if k = l and 0 otherwise, for q = 1,2,3. Moreover E
⟨ k l ⟩
is a noise tensor with independent N(0,1) entries. Under this model,
(cid:32) (cid:33) (cid:40) (cid:114) (cid:41)
(cid:16) (cid:17) σ σ2 d d
sin∠ U (cid:99)k,U
π(k)
≤
C
σ
0 + σ20 max n,
n
π(k) π(k)
forsomepermutationπ : [d] [d]. ThishighlightstwomainbenefitsofusingmultiwayPCsinstead
→
of using classical PCA after stringing the observations as long vectors. Firstly, the dimension
dependence is only through d, which is smaller than the real dimension of U , i.e., d3. Secondly,
k
the result does not require any eigengap, and allows for repeated σ values. The issue of repeated
k
singular values however leads to bias, since in this case it is difficult to identify which sample PC
16estimates which U . This problem can be overcome by a sample splitting scheme as described by
k
Ouyang and Yuan (2023), leading to an asymptotically normal estimate U (cid:99)k of U k. In particular,
in the fixed dimension case,
(cid:16) (cid:17)
√n vec(U(cid:98)(q)) vec(U(q)) d N (0,diag(Γ 1,...,Γ d)) as n ,
− → → ∞
(cid:16) (cid:17)
where Γ
k
= (cid:0) σ 02/σ k2+σ 04/σ k4(cid:1) I
d
−u( kq) (u k(q) )⊤ . Here U(cid:98)(q) = [u (cid:101)( 1q) u (cid:101)( 2q) ... u (cid:101)( dq) ] and U(q) =
(q) (q) (q)
[u u ... u ]. Notice that the asymptotic distribution of the bias corrected sample multi-
1 2 d
way PCs are normal distributions centered at the true multiway PCs. Perhaps more importantly,
the sample multiway PCs are asymptotically independent. Similar properties continue to hold in
the case of growing dimensions.
3.2.3 Principal Subspaces
One can also generalize the notion of principal subspaces to multiway data. For example, Tang
et al. (2023) introduced a spiked covariance model where
Cov(X) = T +σ2I ,
Σ0 d3
where
T 3 U = 0,
Σ0 ×k=1 k⊥
for some U O , and I is the sixth order tensor with entries I = 1 if j = j ,j =
k
∈
d,r d3 j1j2...j6 1 4 2
j ,j = j and I = 0 otherwise. In other words, the principal subspaces in each mode can
5 3 6 j1j2...j6
be characterizied by orthonormal matrices U ,U ,U Rd×r. In particular, they show that this
1 2 3
∈
is equivalent to
X = M +A U +A U +A U +Z,
1 1 1 2 2 2 3 3 3
× × ×
where M is a fixed mean, A Rr×d×d, A Rd×r×d and A Rd×d×r are random tensors with
1 2 3
∈ ∈ ∈
mean 0, and Z Rd×d×d is a noise tensor with mean 0, and covariance σ2I , and is uncorrelated
d3
∈
with random tensors A ,A ,A .
1 2 3
3.3 Independent Component Analysis
ICA is a popular method for separating data streams into independent source components and has
been extensively studied. In the ICA model, one observes a random vector X Rd where X can be
∈
written as a linear combination of independent sources. That is, there is a mixing matrix A Rd×d
∈
such that X = AS, where S = (S ,...,S )⊤ is a vector of independent random variables S for
1 d k
k = 1,...,d. Given n independent copies of X, the goal in ICA is to recover the unknown matrix
A. The problem is intimately related to tensor decomposition.
17To fix ideas, suppose that X are suitably centered and scaled, i.e., E(X) = E(S) = 0 and
Var(X) = Var(S) = I . In practice, this is often referred to as pre-whitening. In doing so, A
d
becomesanorthonormalrotationmatrix. TherotationmatrixAcanbeidentifiablefromobserving
X = AS if and only if all but one source S are non-Gaussian (Comon, 1992). ICA methods have
k
been developed to exploit the non-Gaussianity of X, particularly through a nonzero kurtosis. The
kurtosis of X sampled from a prewhitened ICA model has the form (see, e.g., Comon and Jutten,
2010):
d
(cid:88)
K (X) = E(X X X X) M = κ (S )a a a a , (6)
4 0 4 k k k k k
◦ ◦ ◦ − ◦ ◦ ◦
k=1
where M = E(Z Z Z Z) for Z N(0,I ), is a known 4-th order tensor. Moreover, κ (S ) =
0 d 4 k
◦ ◦ ◦ ∼
ES4 3 is the excess kurtosis of the k-th independent source S .
k − k
3.3.1 Perturbation Bounds
In light of Equation 6, we can derive an estimate of A from that of K (X). Perhaps the most
4
natural choice is the sample cumulant tensor:
n
K (cid:99) 4sample := n1 (cid:88) X i ◦X i ◦X i ◦X i −M 0.
i=1
As noted in Section 3.1.2, we can then proceed to estimate a s by the eigenvectors, denoted by a s,
k (cid:98)k
of an ODECO approximation of K (cid:99)sample . Using the perturbation bounds from Auddy and Yuan
4
(2023a), we immediately get
sin∠(a (cid:98)π(k),a k) ≲
∥K (cid:99) 4sample −K 4(X)
∥.
κ (S )
4 k
Interestingly though, K (cid:99) 4sample is inconsistent in that ∥K (cid:99) 4sample
−
K 4(X)
∥
̸→p 0 unless n ≳ d2.
Somewhat surprisingly, we can do better than the sample kurtosis. In particular, Auddy and Yuan
(2023b) introduced an estimate, denoted by K (cid:99)AY, such that
4
(cid:32)(cid:114) (cid:33)
d
K (cid:99) 4AY K 4(X) = O p ,
∥ − ∥ n
under mild regularity conditions. This immediately yields that the eigenvectors, denoted by aAYs,
(cid:98)k
of K (cid:99)AY satisfy
4
(cid:32)(cid:114) (cid:33)
d
sin∠(a (cid:98)A π(Y k),a k) = O
p
n
.
The estimate aAY is not only consistent for a whenever n d, but also minimax optimal in that
(cid:98)k k
≫
no other estimate converges at a faster rate over all possible mixing matrices. Unfortunately, K (cid:99)AY
4
is not amenable for computation and impractical for high dimensional problems.
183.3.2 Algorithm Dependent Error Bound
Inpractice,toestimatea fromthedecompositioninEquation6,onecanusetensorpoweriteration
k
[0]
on the sample kurtosis tensor, starting from some nontrivial initial estimators a . This idea was
(cid:98)k
first introduced as cumulant based FastICA by Hyvarinen (1999) and has since been studied by
numerous works in the literature, including Hyv¨arinen and Oja (2000); Hyvarinen et al. (2002);
Ollila (2009) in the fixed dimension case, and Anandkumar et al. (2014a, 2015); Belkin et al. (2013,
[0]
2018); Auddy and Yuan (2023b) in the growing dimension case. Given an initialization a , the
(cid:98)k
FastICA iteration takes the form
n
w := a[t] 1 (cid:88) X ,a[t] 3X ; a[t+1] := w/ w .
(cid:98) (cid:98)k
− 3n ⟨
i (cid:98)k
⟩
i (cid:98)k (cid:98) ∥(cid:98)
∥
i=1
Assuming that the initialization is nontrivial in that sin∠(a (cid:98)[0],a k) < 1 η for some fixed η > 0,
−
Auddy and Yuan (2023b) showed that after T ≳ logd iterations, we have
(cid:32)(cid:114) (cid:33)
d
[T]
sin∠(a (cid:98)π(k),a k) = O
p
n
.
provided n ≳ d2. The main challenge is, however, how to obtain a nontrivial initialization.
3.3.3 Initialization and Computational Gap
A common way to initialize is random slicing. Let K (cid:99)4 be an estimator of K 4(X). One can initialize
via the SVD of:
d
(cid:88)
K (cid:99)4 1,2M = κ 4(S k) a k a k,M a ka⊤ k +(K (cid:99)4 K 4) 1,2M. (7)
× ⟨ ◦ ⟩ − ×
k=1
Here M Rd×d has iid N(0,1) entries. By sampling M repeatedly, one can find M such that
∗
∈
the gap within the singular values κ 4(S k) a k a k,M ∗ is sufficiently larger than the error (K (cid:99)4
⟨ ◦ ⟩ −
K ) M . Thus performing an SVD of the matrix in Equation 7 provides unit vectors a[0]
4 ×1,2 ∗ (cid:98)k
[0]
satisfying sin∠(a
(cid:98)k
,a k) < 1
−
η for some fixed η > 0, after which one can apply the FastICA
iterations. Once a is estimated by a , we enforce the slicing matrices M to be perpendicular to
k (cid:98)k
thealreadyestimateddirections,i.e., a a ,M = 0,sothatthesingularvaluesκ (S ) a a ,M
(cid:98)k (cid:98)k 4 k k k
⟨ ◦ ⟩ ⟨ ◦ ⟩
are necessarily small in the directions close to a .
k
Anandkumaretal.(2015)chooseK (cid:99)4 = K (cid:99) 4sample inEquation7,andshowthatonehasnontrivial
initializations whenever n d3. Auddy and Yuan (2023b) improved this sample complexity
≫
through a refined kurtosis estimator that yields a nontrivial estimator whenever n d2. Through
≫
low degree polynomial algorithms, Auddy and Yuan (2023b) also provide evidence to show that the
19lower bound of n d2 is unavoidable in order to obtain a computationally feasible ICA procedure
≫
inthissetting. Similartotheothertensorrelatedproblems, thispresentsagapbetweenthesample
complexity for information theoretic feasibility, n d, and computational tractability, n d2.
≫ ≫
3.4 Mixture Models
Tensor-based methods are particularly useful in learning high dimensional mixture models. The
classical Gaussian mixture model (GMM) assumes a collection of observations is independently
sampled from a common distribution, denoted by GMM(cid:0) p, µ (cid:1) := (cid:80)m p N(µ ,I ),
{ j }j∈[m] j=1 j · j d
which is a mixture of isotropic d-dimensional normal distributions with mean vectors µ s. Here,
j
p sarethemixingprobabilitiessatisfying1⊤p = 1. HsuandKakade(2013)consideredtheadjusted
j
second and third order moments, which admit the following decomposition:
m
(cid:88)
M : = E(X X) I = p (µ µ )
2 d j j j
◦ − ◦
j=1
d
M : = E(X X X) (cid:88) E(cid:0) X e e +e X e +e e X(cid:1)
3 i i i i i i
◦ ◦ − ◦ ◦ ◦ ◦ ◦ ◦
i=1
m
(cid:88)
= p (µ µ µ ),
j j j j
◦ ◦
j=1
Tensor methods enjoy special advantages in learning GMM. For instance, the eigenvectors of M
3
can be uniquely defined even if p = p for some pair j = k. This advantage is unseen in its matrix
j k
̸
counterpart M .
2
3.4.1 Error Bounds
If µ , ,µ Rd are orthogonal to each other, M becomes an ODECO tensor. As in the
1 m 3
··· ∈
ICA case, we can derive an estimate for them from an estimate of M . In particular, let µ be
3 (cid:98)k
the eigenvectors of an ODECO approximation of the sample moment, denoted by M (cid:99)sample . The
3
perturbation bounds from Equation 4 immediately implies that
Msample M
sin∠(cid:0) µ (cid:98)π(k),µ k(cid:1) ∥ (cid:99) 3 − 3 ∥ for all k [m],
≤ p ∈
k
for some permutation π : [m] [m]. Furthermore, it converges to zero in probability if n
→ ≫
d3/2/(min p )2 undersuitablemomentconditions. Thissamplecomplexitycanbefurtherimproved
j j
using the same technique as Auddy and Yuan (2023b). Specifically, one can construct a moment
estimate in the same fashion as they did for estimating the kurtosis tensor, yielding an estimate of
µ s satisfying:
k
(cid:114)
(cid:0) (cid:1) 1 d
sin∠ µ (cid:98)π(k),µ k ≲ for all k [m]
p n ∈
k
20with high probability, attaining a rate which is also minimax optimal. However, as in the ICA case,
this estimating procedure is not computationally tractable.
Computationally tractable approaches based on power iteration have also been investigated by
(cid:8) (cid:9)
Anandkumar et al. (2014a, 2015) among others. Let (p ,µ ) be the output of tensor power
(cid:98)k (cid:98)k k∈[m]
iterationalgorithmappliedtoM (cid:99)sample withsequentialdeflationandalargenumberofinitializations
3
via random slicing. More specifically, by repeatedly sampling v N(0,I ), one can find v such
d ∗
∼
that the top singular vector µ (cid:98)[0] of M (cid:99)3 3 v ∗ satisfies sin∠(µ (cid:98)[0],µ k) < 1 η for some fixed η > 0
× −
and some k [m], thus acting as a nontrivial initialization for tensor power iteration. It was shown
∈
by Anandkumar et al. (2014a) that, with high probability, there exists a permutation π : [m] [m]
(cid:55)→
such that
(cid:114) (cid:114)
(cid:0) (cid:1) 1 d (cid:12) (cid:12) d
sin∠ µ (cid:98)π(k),µ k ≲ and (cid:12)p (cid:98)π(k) p k(cid:12) ≲ for all k [m]
p n − n ∈
k
provided n ≳ d2/(min kp k)2. Once again, we can see the gap in sample complexity.
3.4.2 Low rank mixture model
Matrix-valued observations routinely arise in diverse applications. A low-rank mixture model
(LrMM) was proposed by Lyu and Xia (2023) to investigate the computational and statistical lim-
its in estimating an underlying low-rank structure given a collection of matrix-valued observations.
Under the general LrMM, the matrix-valued observations X 1, ,X
n
Rd1×d2 are independently
··· ∈
sampled from a common distribution, denoted by (cid:80)m p N(M ,I I ), which is a mixture
j=1 j · j d1 ◦ d2
of matrix normal distributions with isotropic covariances and low-rank population center matrices
M js. By concatenating these matrices into a tensor X Rd1×d2×n, the expectation of X, con-
∈
ditioned on the latent labels, admits a low-rank Tucker decomposition. Variants of LrMM have
appeared in Chen et al. (2021b) for studying low-rank mixed regression, and in Mai et al. (2022);
Sun and Li (2019) for tensor clustering. Lyu and Xia (2023) showed that a polynomial-time algo-
rithm combining a tensor-based initialization method and a modified second order moment method
can achieve a statistically optimal rate in estimating the low-rank population center matrix under
the symmetric two-component LrMM. A low-rank Lloyd’s algorithm was proposed by Lyu and
Xia (2022) with a tensor-based initialization method, which achieves a minimax optimal clustering
error rate characterized by the separation strength between the population centers. The authors
also provided convincing evidence to support the existence of a statistical-to-computational gap in
LrMM for both estimation and clustering.
213.5 Tensor Completion
Tensor data often have missing values. Tensor completion (Liu et al., 2012) refers to the problem
of recovering a low-rank tensor by observing only a small subset of its entries. Let T Rd×d×d
∈
have multilinear ranks (r,r,r), and denote Ω [d] [d] [d] the locations where the entries of
⊂ × ×
T are observed. It is commonly assumed that T is incoherent to ensure the well-posedness of
the problem. For notational simplicity, we treat the incoherence parameter of T as a constant,
implying that d3/2 T / T = O(1). Denote by n := Ω the sample size.
max F
∥ ∥ ∥ ∥ | |
3.5.1 Convex Approaches
In the seminal work by Yuan and Zhang (2016), a convex program was introduced for exact tensor
completion by minimizing the tensor nuclear norm:
T (cid:99)YZ := argmin Y ∗, s.t. Ω(Y ) = Ω(T ), (8)
∥ ∥ P P
where (T ) is the operator which zeros out the entries of T except those in Ω, and the tensor
Ω
P
nuclear norm is defined by Y ∗ := sup ∥M∥≤1 Y ,M . Suppose that Ω is sampled uniformly from
∥ ∥ ⟨ ⟩
[d] [d] [d] with a cardinality n C (cid:0) r1/2d3/2+r2d(cid:1) log3d for a large absolute constant C > 0.
1 1
× × ≥
Yuan and Zhang (2016) shows that the solution to Equation 8 can exactly recover the underlying
tensor T with high probability.
Later,YuanandZhang(2017)introducedthetensorincoherentnorms,showingthatminimizing
the incoherent nuclear norm can, w.h.p., exactly recover an order-p tensor if the sample size Ω ≳
| |
r(p−1)/2d3/2log2d. Ghadermarzy et al. (2019) studied a tensor max-norm and further improved
the sample complexity, which nearly matches the information-theoretic lower bound. However,
these convex norms are, in general, computationally NP-hard. Barak and Moitra (2016) provided
evidence that no polynomial-time algorithms can consistently recover a rank-one d d d tensor
× ×
if the sample size Ω = o(d3/2). Another line of convex methods is based on semi-definite program
| |
(SDP). The sum-of-squares (SOS) method reformulates tensor completion as a large-scale SDP,
whose size depends on the degree of relaxation. Potechin and Steurer (2017) shows that SOS can
exactly recover a d d d tensor with CP-rank r w.h.p. if the sample size Ω ≳ rd3/2Polylog(d).
× × | |
Although the SOS method is polynomial-time computable in theory, in practice, its computation
is prohibitively intensive due to the need to solve very large-scale SDPs.
3.5.2 Nonconvex Approaches
Nonconvex methods for tensor completion usually admit much fast computation and consume less
memory and storage. In a nutshell, non-convex methods for completing a d d d tensor with
× ×
22multilinear ranks (r,r,r) aim to solve
(cid:13) (cid:13)
min (cid:13) (cid:0) [C;U ,U ,U ] T (cid:1)(cid:13) s.t. C Rr×r×r;U ,U ,U Rd×r. (9)
(cid:13) Ω 1 2 3 (cid:13) 1 2 3
P − F ∈ ∈
Alternating minimization and gradient descent algorithms are commonly used for finding a locally
optimalsolutiontoEquation9. Thesuccessofthesealgorithmscruciallydependontheavailability
of a warm initialization. Finding a warm initialization is often the most challenging step in tensor
optimizationproblem. SupposethatΩissampleduniformlywithreplacement. Thedataessentially
consistsofarandomsample (X ,Y ) : i [n] ofi.i.d. observationssatisfyingY = X ,T , where
i i i i
{ ∈ } ⟨ ⟩
X is sampled uniformly from the orthonormal basis := (cid:8) e e e : i,j,k [d](cid:9) . Xia and
i i j k
X ◦ ◦ ∈
Yuan (2019) proposed a second-order moment method, which applies spectral initialization to the
U-statistic:
d6
(cid:88)
Y Y (X ) ⊤(X ) for all k = 1,2,3, (10)
n(n 1) i j Mk i Mk j
− 1≤i̸=j≤n
which is an unbiased estimator of (T ) ⊤(T ). Xia and Yuan (2019) showed that the objective
Mk Mk
functioninEquation9behaveslikeaparabolaaroundtheoracleandstudiedtheGrassmanniangra-
dient descent algorithm for minimizing the objective function with spectral initialization obtained
from Equation 10. Let T (cid:99)XY denote the estimator delivered by their algorithm. Xia and Yuan
(2019) shows that if n C 1(cid:0) r7/2d3/2log7/2d+r7dlog6d(cid:1) , then T (cid:99)XY = T with high probability.
≥
Jain and Oh (2014) showed that, once well initialized, the alternating minimization algorithm
can exactly recover T with CP-rank r if n ≳ r5d3/2log4d. A spectral method was studied by
Montanari and Sun (2018) for both undercomplete (r d) and overcomplete (r d) regimes,
≤ ≫
which cannot exactly recover the underlying tensor.
3.5.3 Noisy tensor completion
Consider the noisy case where Y = T ,X +ε has a sub-Gaussian noise ε with a proxy-variance
⟨ ⟩
σ2. Noisy tensor completion was first studied by Barak and Moitra (2016) using the sum-of-
squares method, but their estimator is statistically sub-optimal. The sample mean T (cid:99)sample :=
d3n−1(cid:80)n i=1Y iX
i
is an unbiased estimator of T . The concentration inequality of T (cid:99)sample was
investigatedbyYuanandZhang(2016)andgeneralizedbyXiaetal.(2021). Forinstance, Theorem
1 of Xia et al. (2021) shows that T (cid:99)sample concentrates at T in the following manner:
(cid:13) (cid:13)T (cid:99)sample T (cid:13) (cid:13) Cα(cid:0) T max+σ(cid:1)
max(cid:26)(cid:18) d4(cid:19)1/2
,
d3(cid:27)
log5d,
− ≤ ∥ ∥ · n n
which holds with probability at least 1 d−α for any α 1 and C > 0 is an absolute constant.
− ≥
Note that concentration inequality holds for any given tensor T , whether it is full rank or not.
23It is worth noting that Xia and Yuan (2021) studied a concentration inequality for the case when
entries are sampled non-uniformly and without replacement.
WhiletheconcentrationinequalityforthesamplemeanestimatorT (cid:99)sample issharpinthetensor
spectral norm, it is typically full rank, and the error rate it achieves is sub-optimal in the Frobenius
norm. The regularized-HOOI algorithm was investigated by Xia et al. (2021) showing that the
resulting estimator, denoted by T (cid:99)XYZ, achieves the minimax optimal rate
d−3/2(cid:13) (cid:13)T (cid:99)XYZ −T (cid:13) (cid:13)
F ≤
C(cid:0) ∥T ∥max+σ(cid:1)
·(cid:18)
(rd+r
n3)logd(cid:19)1/2
,
which holds with high probability if the sample size n ≳ (r2d+rd3/2)Polylog(d) and the signal
strength satisfies λ min/σ ≳ d9/4n−1/2Polylog(d). The rate is minimax optimal with respect to the
sample size and the degrees of freedom, but is not proportional to noise standard deviation. If
T σ, the rate becomes sub-optimal.
max
∥ ∥ ≫
Statistically optimal estimators have also been investigated by Cai et al. (2019) and Cai et al.
(2022b) for noisy tensor completion under the CP and tensor-train model, respectively. A vanilla
gradientdescentalgorithmwasstudiedinCaietal.(2019)andCaietal.(2020)forcompletingaCP-
format tensor from noisy observations. By a sophisticated leave-one-out analysis, their estimator
is shown to be minimax optimal not only in Frobenius norm but also in max-norm.
3.6 Tensor Regression
Tensor regression is a versatile tool with applications in brain imaging , facial image analysis,
spatiotemporal learning, multitask learning, chemometrics and myriad other fields. See, e.g., Zhou
et al. (2013); Yu and Liu (2016).
Let us consider scalar responses depending linearly on tensor covariates:
Y = X ,T +ε , i = 1,...,n, (11)
i i i
⟨ ⟩
where Y is the real valued response, ε is the observation noise for i = 1,...,n. Further X and
i i i
T are the order-3 d d d dimensional tensors of design and the coefficients respectively, with
× ×
X ,T denotingtheirelement-wiseinnerproduct. Equation11suggestsaleastsquaresestimator:
i
⟨ ⟩
n
(cid:88)
T (cid:99):= argmin (Y
i
X i,A )2. (12)
A∈Rd×d×d
i=1
−⟨ ⟩
Stringing A into a d3 dimensional vector and using standard regression methods is statistically
inefficient. Instead we now describe some methods which benefit from the multiway nature of A
and take advantage of its low dimensional structures.
243.6.1 Convex approaches
A common approach of solving the constrained least squares problem in Equation 12 is to add a
convex penalty:
(cid:40) n (cid:41)
1 (cid:88)
T (cid:99):= argmin (Y
i
X i,A )2+τ (A) . (13)
A∈Rd×d×d 2n
i=1
−⟨ ⟩ R
The penalty ( ) is chosen based on the intended structural constraints. For elementwise
R ·
sparsity, one uses (T ) = vec(T ) = (cid:80) T . On the other hand for the sparsity of fibres,
R ∥ ∥1 i,j,k| ijk |
we use a group-based regularizer (T ) = (cid:80) T , akin to the group Lasso. It is also common
R i,j∥ ij. ∥2
to make low rank assumptions, e.g., via a low CP-rank r: which can be imposed by a tensor nuclear
norm penalty (T ) = T
∗
= maxS:∥S∥=1 T ,S . Low multirank assumptions can be placed
R ∥ ∥ ⟨ ⟩
similarly by penalizing through the sum of the nuclear norms of the matricizations (cid:80) (T ) .
q∥Mq ∥∗
Raskutti et al. (2019) provided a general framework for analyzing convex regularized tensor
regression estimators of the form Equation 13 with weakly decomposable penalties ( ). Within
R ·
this framework, they defined two key quantities:
(A)2 (cid:18) (cid:19)
s( ) = sup R and w ( ) = E sup A,G
A A∈A/{0} ∥A ∥2 F G S A∈S⟨ ⟩
where G Rd×d×d is a tensor with independent N(0,1) entries. Here, roughly speaking, denotes
∈ A
the low dimensional subspace of tensors in which we constrain T to be in. The first quantity s( )
A
then measures the intrinsic dimensionality of . The second quantity, called Gaussian width, is a
A
notion of the complexity of . When evaluated on B (1) := A : (A) 1 , this second quantity
R
S { R ≤ }
determines how large the penalty is with respect to the Frobenius norm. With these definitions,
R
Raskutti et al. (2019) proved that the solution T (cid:99)of Equation 13, with high probability satisfies
T (cid:99) T 2 F ≲ s( )τ2
∥ − ∥ A
provided τ ≳ σw G[B R(1)]/√n, under Gaussian designs and errors ε j N(0,σ2).
∼
When specialized to elementwise sparsity of the form (cid:80) 1(T = 0) s, the general bound
ijk ijk ̸ ≤
above boils down to ∥T (cid:99) −T |2 F ≲ σ2slog(d)/n, provided τ ≍ σ(cid:112) log(d)/n. When imposing a low
rank structure, say through the multirank condition max rank( (T )) : q = 1,2,3 r one
q max
{ M } ≤
can choose the penalty R(T ) = (cid:80) q∥Mq(T ) ∥∗. Then the solution T (cid:99)of Equation 13 satisfies
∥T (cid:99) −T ∥2 F ≲ σ2r maxd2/n, by taking τ ≍ σ(cid:112) d2/n.
3.6.2 Nonconvex approaches
While convex relaxations to the nonconvex tensor regression problem provide polynomial time
algorithms, they are often slow in practice. Moreover, a convex relaxation might involve quantities
25computingwhichareNP-hard. Thenuclearnorm T = sup A,T : A 1 , usedtoimpose
∗
∥ ∥ {⟨ ⟩ ∥ ∥ ≤ }
CP low rank-ness in tensor regression, is one such example. In stead, nonconvex projected gradient
descent algorithms for tensor regression are often beneficial for both computational and statistical
purposes.
Chen et al. (2019b) consider a generalized linear model for a scalar response Y and a covariate
tensor X Rd1×d2×d3, where the conditional distribution of Y on X is given by p(Y X,T ) =
∈ |
h(Y)exp Y X,T a( X,T ) where a( ) is strictly convex and differentiable log-partition
{ ⟨ ⟩ − ⟨ ⟩ } ·
function. This results in the negative log-likelihood objective function
n
1 (cid:88)
(A) = [a( X ,A ) Y X ,A logh(Y )].
i i i i
L n ⟨ ⟩ − ⟨ ⟩−
i=1
In this subsection, we consider projected gradient descent (PGD) to optimize the above objective
function. The PGD framework, introduced by Jain et al. (2014), can be applied to a general tensor
space as follows:
F (cid:16) (cid:17)
T (cid:99)[t+1] Proj T (cid:99)[t] η (cid:0)T (cid:99)[t](cid:1) .
F
←− − ∇L
In practice can be described based on low rank or sparsity assumptions. In Chen et al. (2019b)
F
the authors derive theoretical guarantees of the PGD estimator for general loss functions satisfying
restricted strong convexity and for collections of subspaces that allow for contractive projections.
Duetothenonconvexity, theperformanceofPGDdependsontheinitialization. Thisisreflectedin
the local Gaussian width w [ B (1)]. Here denotes the subspace where the PGD iteration
G 0 F 0
F ∩ F
is initialized, and B (1) is the unit ball of tensors in Frobenius norm. With these conditions, Chen
F
et al. (2019b) show that:
T (cid:99)[k] T
F
cw G[
0
B F(1)]/√n+δ
∥ − ∥ ≤ F ∩
for some constant c > 0, provided PGD is run for k ≳ log( T F/δ) iterations. Thus if we have
∥ ∥
n ≳ (w G[ 0 B F(1)])2 samples, the PGD estimator converges linearly to the true covariate tensor
F ∩
T .
Specializing to the linear model and the sparsity/low rank examples considered earlier, one can
compare the nonconvex and convex approaches. For illustrative purposes, we only consider the low
Tuckerrankregressionmodel, i.e., T hasmultirankatmost(r ,r ,r )withr := min r ,r ,r .
1 2 3 min 1 2 3
{ }
In this setting, Chen et al. (2019b) used an approximate projection operator defined through a low
rank projection on the matricizations. With this estimator, they showed that with n ≳ r mind2
samples, the PGD estimator T (cid:99)satisfies ∥T (cid:99) − T ∥2 F ≲ σ2r mind2/n after log(n) many iterations.
The PGD estimator improves on the convex relaxed estimator of Raskutti et al. (2019) by selecting
the optimal mode for matricization, whereas convex regularization takes an average of the three
matricizations, and is hence sub-optimal.
26Followingtheaboveworks,inZhangetal.(2020)theauthorsaddressedtheissueofscalabilityof
tensor regression to massive dimensions. Their method is based on importance sketching (ISLET)
where, depending on low rankness or sparsity requirements, HOOI or STAT-SVD (see Zhang and
Han (2019)) is used to determine the importance sketching directions. Then tensor regression
is applied on the dimension reduced tensors obtained from importance sketching. Due to the
dimension reduction, ISLET has immense computational as well as theoretical advantage over a
regression method applied on the entire tensor.
3.7 Higher Order Networks
Higherordernetworksmodelthejointinteractionsofmultipleentities,suchashypergraphnetworks
(Benson et al., 2016), multi-layer networks (Kivel¨a et al., 2014), and dynamic networks (Mucha
et al., 2010). These higher-order networks can be conveniently represented as higher-order tensors.
Consider a 3-uniform hypergraph on d vertices, where each hyperedge consists of exactly three
vertices. It is equivalently representable by the adjacency tensor A 0,1 d×d×d. Specifically, the
∈ { }
entry A = 1 if and only if the hyperedge (i ,i ,i ) is present. If the hyperedges are undirected,
i1i2i3 1 2 3
tthen he adjacency tensor is symmetric.
Clustering nodes in an undirected hypergraph network, often referred to as community detec-
tion, is of particular interest in many applications. Suppose there exist K groups among d vertices,
characterized by a binary matrix Z 0,1 d×K, where each row has exactly one entry being 1
∈ { }
and the others are 0s. Under the hypergraph stochastic block model (hSBM, Ghoshdastidar and
Dukkipati (2014)), the hyperedge connectivity is determined solely by the node community mem-
berships, summarized by a probability tensor P [0,1]K×K×K. The expected adjacency tensor
∈
A admits a Tucker decomposition as
E[A] = [C;Z,Z,Z].
Since the singular vectors of E[A] provide community membership information, one natural idea is
to estimate the population singular vectors by finding a low-rank approximation of A. If the oracle
cluster sizes are balanced, the singular vectors of E[A] are incoherent. Motived by this, Ke et al.
(2019) proposed to find a regularized tensor decomposition of A based on the regularized-HOOI
algorithm. They derived a concentration inequality of A using the incoherent operator norm:
A E[A](cid:13) (cid:13) := sup (cid:10)A E[A],a b c(cid:11)
∥ − o,δ − ◦ ◦
a,b,c∈Sd−1
∥c∥max≤δ
for any δ (0,1]. The conventional tensor operator norm corresponeds to the special case .
o,1
∈ ∥·∥
If the cluster sizes are balanced, one can focus on small values of δs, such as
O(cid:0) (K/d)1/2(cid:1)
. This
provides technical convenience for dealing with extremely sparse hypergraph networks.
27Suppose that the oracle clusters have balanced sizes and P/q is full rank with a bounded
e
condition number. Denote q := P , which characterizes the overall network sparsity level
e max
∥ ∥
. Indeed, d2q can be viewed as the average node degree. Ke et al. (2019) shows that if d2q
e e
≫
K2log2d, thentheregularizedregularized-HOOIandspectralclusteringalgorithmcanconsistently
recoverallthecommunitieswithprobabilityendingtooneasd . Itmeansthattheirmethodis
→ ∞
effectiveaslongasthetotalnumberofhyperedgesexceedsdlog3d, assumingK = O(1). Therefore,
the regularized tensor decomposition method can handle extremely sparse hypergraph networks.
It is worth noting that the method studied in Ke et al. (2019) can also handle node heterogeneity.
Regularized tensor decomposition has been investigated by Jing et al. (2021) for community de-
tection in multi-layer networks and by Lyu et al. (2023b) for general higher-order network analysis.
In particular, a sharp concentration inequality for the adjacency tensor with respect to incoherent
operator norm was developed by Jing et al. (2021). They show that the regularized-HOOI and
spectral clustering can consistently recover both the global and local community memberships for
multi-layer networks. Their theories allow the network sparsity to decrease when more layers are
observed. The methods are still effective even when some layers are disconnected graphs. More re-
cently, Lyu et al. (2023a) studied a two-step algorithm for clustering layers in a multi-layer network
and achieved the minimax optimal clustering error rate under a wide range of network sparsity
conditions.
The tensor block model (TBM) can be viewed as a generalization of hSBM. A rank-one sparse
TBM was proposed by Xia and Zhou (2019), assuming that the observed tensor A satisfies A =
λ1 1 1 + E, where C [d], and the noise tensor E has i.i.d. centered sub-Gaussian
C1
◦
C2
◦
C3 j
⊂
entries. Here, 1 denotes the binary vector with only the entries in C being equal to one. More
C
general TBMs have been investigated by Wang and Zeng (2019), Han et al. (2022a) and Zhou and
Chen (2023) for cluster analysis. In particular, Han et al. (2022a) proposed a higher-order spectral
clustering algorithm for finding clusters in each dimension, where a statistical-to-computational
gap was discovered. Their algorithm can exactly recover all the clusters under a suitable separation
condition.
3.8 Tensor Time Series
Tensor time series have become ubiquitous to date. For example, the monthly international trade
flow of different categories of commodities between countries produces a time series of tensor-
valued observations (Cai et al., 2022a; Chen et al., 2022). As elaborated by Rogers et al. (2013),
thechallengeintensortimeseriesanalysisistostudythedynamicswhilepreservingtheunderlying
tensor structures.
Tensor factor model (Chen et al., 2022) assumes the tensor-valued observations share common
28loadings and the dynamic is characterized by smaller-sized factor tensors, which is generalized from
matrix factor model (Chen et al., 2019a; Wang et al., 2019). Let X
t
Rd1×d2×d3,t [T] be a time
∈ ∈
series of tensors. Under tensor factor model, it is assumed that
X = (cid:2)F ;U ,U ,U (cid:3) +E ,
t t 1 2 3 t
where F
t
Rr1×r2×r3,t [T] are factor tensors. The Tucker decomposition preserves the tensor
∈ ∈
structure, while the factor tensor time series save the underlying dynamic. It is assumed that
the process F is weakly stationary and the lagged cross-products have fixed expectation in that
t
(T h)−1(cid:80)T F F converges to E[F F ] in probability. For simplicity, the tensor
− t=h+1 t−h ◦ t T−h ◦ T
E s are assumed to be white noise and mutually independent across time. Estimating procedures
t
under tensor factor model can be technically involved. Chen et al. (2022) proposed two estimators,
named TOPUP (time series outer-product unfolding procedure) and TIPUP (time series inner-
product unfolding procedure). For any k = 1,2,3, TOPUP and TIPUP calculate the mode-k lag-h
auto-correlations using outer and inner products as follows:
T T
V(O) := 1 (cid:88) (X ) (X ) and V(I) := 1 (cid:88) (X ) ⊤(X ),
k,h T h Mk t−h ◦Mk t k,h T h Mk t−h Mk t
− t=h+1 − t=h+1
respectively. Conditioned on F : t [T] , the left singular subspaces of EV(O) and EV(I)
{ t ∈ } k,h k,h
are both directly related to the loading matrix U . TOPUP and TIPUP then exploits multiple
k
auto-correlations differently where the former and latter ones take the left singular vectors of the
assembles
(cid:0) (O) (cid:1) (cid:0) (I) (cid:1)
(V ),h = 1, ,h and (V ),h = 1, ,h ,
M1 k,h ··· 0 M1 k,h ··· 0
respectively. FinitesamplepropertiesofTOPUPandTIPUPwereestablishedbyChenetal.(2022).
ACP-formattensorfactormodelwasproposedbyHanetal.(2021). Theauthorsintroducedanovel
initialization procedure based on composite PCA and proposed to refine the estimates by iterative
simultaneous orthogonalization. The computational and statistical performances of their algorithm
were investigated assuming that the factor time series is stationary and strongly α-mixing.
Autoregressive (AR) models are useful for predicting future observations in time series. Matrix
andtensortimeseriesARmodelshavebeenproposedbyChenetal.(2021a)andWangetal.(2024).
For instance, under a matrix time series AR(q) model, the observations X
t
Rd1×d2,t [T] are
∈ ∈
assumed to satisfy
X = A X B⊤+ +A X B⊤+E , t = q+1, ,T,
t 1 t−1 1 q t−q q t
··· ···
where q 1, A s and B s are unknown d d and d d matrices, and E s are stochastic noise.
k k 1 1 2 2 t
≥ × ×
Under a tensor AR(1) model, the observations X
t
Rd1×d2×d3,t [T] are assumed to satisfy
∈ ∈
X = X C C C +E , t = 2, ,T,
t t−1 1 1 2 2 3 3 t
× × × ···
29where the unknown matrix C is of size d d . Chen et al. (2021a) and Wang et al. (2024)
k k k
×
investigated the least squares method to estimate the unknown matrix parameters and proposed to
solveitbyresortingtothenearestKroneckerproductproblemorbyalternatingminimization. Both
estimatingproceduresarenon-convex, andonlylocallyoptimalsolutionscanbefound. Asymptotic
normality was established, under mild conditions, for the globally optimal solution to the least
squares minimization.
4 Concluding Remarks
Tensors methods are ubiquitous in modern statistical applications. In many instances, the nominal
complexity of the problem is much greater than the information content of the data and the main
challenge is to develop inferential methods to achieve both statistical and computational efficien-
cies. Over the past decade, considerable strides have been taken in the development of innovative
statistical techniques, efficient computational algorithms, and fundamental mathematical theory to
analyze and exploit information in these types of data. This review samples various representative
advancement to assess the statistical and computational performance of the maximum likelihood
andotheroptimizationmethods, convexregularization, gradientdecentandpoweriteration, tensor
unfolding, and their combinations in a wide range of tensor-related problems. A recurring theme
is the intricate interplay between two types of efficiencies – statistical and computational – with a
gap often existing between the performances of statistically and computationally efficient methods.
A deep understanding of this phenomenon has profound significance in statistical theory.
References
Genevera Allen. Sparse higher-order principal components analysis. In Artificial Intelligence and
Statistics, pages 27–36. PMLR, 2012.
Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor
decompositions for learning latent variable models. Journal of machine learning research, 15:
2773–2832, 2014a.
Animashree Anandkumar, Rong Ge, and Majid Janzamin. Guaranteed non-orthogonal tensor
decomposition via alternating rank-1 updates. arXiv preprint arXiv:1402.5180, 2014b.
Animashree Anandkumar, Rong Ge, and Majid Janzamin. Learning overcomplete latent variable
models through tensor methods. In Conference on Learning Theory, pages 36–112. PMLR, 2015.
30Gerard Ben Arous, Song Mei, Andrea Montanari, and Mihai Nica. The landscape of the spiked
tensor model. Communications on Pure and Applied Mathematics, 72(11):2282–2330, 2019.
Arnab Auddy and Ming Yuan. On estimating rank-one spiked tensors in the presence of heavy
tailed errors. IEEE Transactions on Information Theory, 68(12):8053–8075, 2022.
ArnabAuddyandMingYuan. Perturbationboundsfor(nearly)orthogonallydecomposabletensors
with statistical applications. Information and Inference: A Journal of the IMA, 2023a.
Arnab Auddy and Ming Yuan. Large dimensional independent component analysis: Statistical
optimality and computational tractability. arXiv preprint arXiv:2303.18156, 2023b.
Boaz Barak and Ankur Moitra. Noisy tensor completion via the sum-of-squares hierarchy. In
Conference on Learning Theory, pages 417–445. PMLR, 2016.
Mikhail Belkin, Luis Rademacher, and James Voss. Blind signal separation in the presence of
gaussian noise. In Conference on Learning Theory, pages 270–287. PMLR, 2013.
Mikhail Belkin, Luis Rademacher, and James Voss. Eigenvectors of orthogonally decomposable
functions. SIAM Journal on Computing, 47(2):547–615, 2018.
Austin R Benson, David F Gleich, and Jure Leskovec. Higher-order organization of complex net-
works. Science, 353(6295):163–166, 2016.
Rajendra Bhatia. Matrix analysis, volume 169. Springer Science & Business Media, 2013.
Xuan Bi, Xiwei Tang, Yubai Yuan, Yanqing Zhang, and Annie Qu. Tensors in statistics. Annual
review of statistics and its application, 8:345–368, 2021.
Changxiao Cai, Gen Li, H Vincent Poor, and Yuxin Chen. Nonconvex low-rank tensor completion
from noisy data. Advances in neural information processing systems, 32, 2019.
Changxiao Cai, H Vincent Poor, and Yuxin Chen. Uncertainty quantification for nonconvex tensor
completion: Confidence intervals, heteroscedasticity and optimality. In International Conference
on Machine Learning, pages 1271–1282. PMLR, 2020.
Jian-Feng Cai, Jingyang Li, and Dong Xia. Generalized low-rank plus sparse tensor estimation
by fast riemannian optimization. Journal of the American Statistical Association, pages 1–17,
2022a.
Jian-Feng Cai, Jingyang Li, and Dong Xia. Provable tensor-train format tensor completion by
riemannian optimization. Journal of Machine Learning Research, 23(123):1–77, 2022b.
31J Douglas Carroll and Jih-Jie Chang. Analysis of individual differences in multidimensional scaling
viaann-waygeneralizationofEckart-Youngdecomposition. Psychometrika,35(3):283–319,1970.
Elynn Y Chen, Ruey S Tsay, and Rong Chen. Constrained factor models for high-dimensional
matrix-variate time series. Journal of the American Statistical Association, 2019a.
Han Chen, Garvesh Raskutti, and Ming Yuan. Non-convex projected gradient descent for gener-
alized low-rank tensor regression. The Journal of Machine Learning Research, 20(1):172–208,
2019b.
RongChen,HanXiao,andDanYang. Autoregressivemodelsformatrix-valuedtimeseries. Journal
of Econometrics, 222(1):539–560, 2021a.
Rong Chen, Dan Yang, and Cun-Hui Zhang. Factor models for high-dimensional tensor time series.
Journal of the American Statistical Association, 117(537):94–116, 2022.
Yanxi Chen, Cong Ma, H Vincent Poor, and Yuxin Chen. Learning mixtures of low-rank models.
IEEE Transactions on Information Theory, 67(7):4613–4636, 2021b.
Andrzej Cichocki, Danilo Mandic, Lieven De Lathauwer, Guoxu Zhou, Qibin Zhao, Cesar Caiafa,
and Huy Anh Phan. Tensor decompositions for signal processing applications: From two-way to
multiway component analysis. IEEE signal processing magazine, 32(2):145–163, 2015.
Pierre Comon. Independent component analysis, 1992.
Pierre Comonand ChristianJutten. Handbook of Blind Source Separation: Independent component
analysis and applications. Academic press, 2010.
Renato Coppi and Sergio Bolasco. Multiway data analysis. North-Holland Publishing Co., 1989.
Lieven De Lathauwer, Bart De Moor, and Joos Vandewalle. On the best rank-1 and rank-
(r ,r ,...,r ) approximation of higher-order tensors. SIAM journal on Matrix Analysis and
1 2 n
Applications, 21(4):1324–1342, 2000a.
Lieven De Lathauwer, Bart De Moor, and Joos Vandewalle. A multilinear singular value decom-
position. SIAM journal on Matrix Analysis and Applications, 21(4):1253–1278, 2000b.
AlanEdelman,Tom´asAArias,andStevenTSmith. Thegeometryofalgorithmswithorthogonality
constraints. SIAM journal on Matrix Analysis and Applications, 20(2):303–353, 1998.
SilviaGandy,BenjaminRecht,andIsaoYamada. Tensorcompletionandlow-n-ranktensorrecovery
via convex optimization. Inverse problems, 27(2):025010, 2011.
32Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic
gradient for tensor decomposition. In Conference on learning theory, pages 797–842. PMLR,
2015.
Navid Ghadermarzy, Yaniv Plan, and O¨zgu¨r Yilmaz. Near-optimal sample complexity for convex
tensor completion. Information and Inference: A Journal of the IMA, 8(3):577–619, 2019.
DebarghyaGhoshdastidarandAmbedkarDukkipati.Consistencyofspectralpartitioningofuniform
hypergraphsunderplantedpartitionmodel. Advances in Neural Information Processing Systems,
27, 2014.
Wolfgang Hackbusch. Tensor spaces and numerical tensor calculus, volume 42. Springer, 2012.
Rungang Han, Yuetian Luo, Miaoyan Wang, and Anru R Zhang. Exact clustering in tensor block
model: Statistical optimality and computational limit. Journal of the Royal Statistical Society
Series B: Statistical Methodology, 84(5):1666–1698, 2022a.
Rungang Han, Rebecca Willett, and Anru R Zhang. An optimal statistical and computational
framework for generalized tensor estimation. The Annals of Statistics, 50(1):1–29, 2022b.
YuefengHan,Cun-HuiZhang,andRongChen. Cpfactormodelfordynamictensors. arXivpreprint
arXiv:2110.15517, 2021.
RHarshman. Foundationsoftheparafacprocedure: Modelandconditionsforanexplanatoryfactor
analysis. Technical Report UCLA Working Papers in Phonetics 16, University of California, Los
Angeles, Los Angeles, CA, 1970.
Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the ACM
(JACM), 60(6):45, 2013.
Frank L Hitchcock. The expression of a tensor or a polyadic as a sum of products. Journal of
Mathematics and Physics, 6(1-4):164–189, 1927.
Samuel B Hopkins, Jonathan Shi, and David Steurer. Tensor principal component analysis via
sum-of-square proofs. In Conference on Learning Theory, pages 956–1006, 2015.
Daniel Hsu and Sham M Kakade. Learning mixtures of spherical gaussians: moment methods
and spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical
Computer Science, pages 11–20, 2013.
AapoHyvarinen. Fastandrobustfixed-pointalgorithmsforindependentcomponentanalysis. IEEE
transactions on Neural Networks, 10(3):626–634, 1999.
33Aapo Hyv¨arinen and Erkki Oja. Independent component analysis: algorithms and applications.
Neural networks, 13(4-5):411–430, 2000.
Aapo Hyvarinen, Juha Karhunen, and Erkki Oja. Independent component analysis. Studies in
informatics and control, 11(2):205–207, 2002.
PrateekJainandSewoongOh. Provabletensorfactorizationwithmissingdata. Advances in Neural
Information Processing Systems, 27, 2014.
Prateek Jain, Ambuj Tewari, and Purushottam Kar. On iterative hard thresholding methods for
high-dimensional m-estimation. Advances in neural information processing systems, 27, 2014.
Majid Janzamin, Rong Ge, Jean Kossaifi, and Anima Anandkumar. Spectral learning on matrices
and tensors. Foundations and Trends® in Machine Learning, 12(5-6):393–536, 2019.
Bo Jiang, Shiqian Ma, and Shuzhong Zhang. Tensor principal component analysis via convex
optimization. Mathematical Programming, 150(2):423–457, 2015.
Bing-YiJing, TingLi, ZhongyuanLyu, andDongXia. Communitydetectiononmixturemultilayer
networks via regularized tensor decomposition. The Annals of Statistics, 49(6):3181–3205, 2021.
Ian T Jolliffe. Principal component analysis for special types of data. Springer, 2002.
Hyo Jung Kang, Yuka Imamura Kawasawa, Feng Cheng, Ying Zhu, Xuming Xu, Mingfeng Li,
Andr´e MM Sousa, Mihovil Pletikos, Kyle A Meyer, Goran Sedmak, et al. Spatio-temporal
transcriptome of the human brain. Nature, 478(7370):483–489, 2011.
Zheng Tracy Ke, Feng Shi, and Dong Xia. Community detection for hypergraph networks via
regularized tensor power iteration. arXiv preprint arXiv:1909.06503, 2019.
Mikko Kivel¨a, Alex Arenas, Marc Barthelemy, James P Gleeson, Yamir Moreno, and Mason A
Porter. Multilayer networks. Journal of complex networks, 2(3):203–271, 2014.
Daniel Kressner, Michael Steinlechner, and Bart Vandereycken. Low-rank tensor completion by
riemannian optimization. BIT Numerical Mathematics, 54:447–468, 2014.
Pieter M Kroonenberg. Applied multiway data analysis. John Wiley & Sons, 2008.
Joseph B Kruskal. Three-way arrays: rank and uniqueness of trilinear decompositions, with appli-
cation to arithmetic complexity and statistics. Linear algebra and its applications, 18(2):95–138,
1977.
34Ji Liu, Przemyslaw Musialski, Peter Wonka, and Jieping Ye. Tensor completion for estimating
missing values in visual data. IEEE transactions on pattern analysis and machine intelligence,
35(1):208–220, 2012.
Tianqi Liu, Ming Yuan, and Hongyu Zhao. Characterizing spatiotemporal transcriptome of the
human brain via low-rank tensor decomposition. Statistics in Biosciences, pages 1–29, 2022.
Yuetian Luo, Garvesh Raskutti, Ming Yuan, and Anru R Zhang. A sharp blockwise tensor per-
turbation bound for orthogonal iteration. The Journal of Machine Learning Research, 22(1):
8106–8153, 2021.
Zhongyuan Lyu and Dong Xia. Optimal clustering by lloyd algorithm for low-rank mixture model.
arXiv preprint arXiv:2207.04600, 2022.
Zhongyuan Lyu and Dong Xia. Optimal estimation and computational limit of low-rank gaussian
mixtures. The Annals of Statistics, 51(2):646–667, 2023.
ZhongyuanLyu,TingLi,andDongXia. Optimalclusteringofdiscretemixtures: Binomial,poisson,
block models, and multi-layer networks. arXiv preprint arXiv:2311.15598, 2023a.
Zhongyuan Lyu, Dong Xia, and Yuan Zhang. Latent space model for higher-order networks and
generalizedtensordecomposition. JournalofComputationalandGraphicalStatistics,pages1–17,
2023b.
Tengyu Ma, Jonathan Shi, and David Steurer. Polynomial-time tensor decompositions with sum-
of-squares. In2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS),
pages 438–446. IEEE, 2016.
Qing Mai, Xin Zhang, Yuqing Pan, and Kai Deng. A doubly enhanced em algorithm for model-
based tensor clustering. Journal of the American Statistical Association, 117(540):2120–2134,
2022.
Peter McCullagh. Tensor methods in statistics. Courier Dover Publications, 2018.
Andrea Montanari and Nike Sun. Spectral algorithms for tensor completion. Communications on
Pure and Applied Mathematics, 71(11):2381–2425, 2018.
Cun Mu, Bo Huang, John Wright, and Donald Goldfarb. Square deal: Lower bounds and improved
relaxations for tensor recovery. In International conference on machine learning, pages 73–81.
PMLR, 2014.
35Cun Mu, Daniel Hsu, and Donald Goldfarb. Successive rank-one approximations for nearly orthog-
onally decomposable symmetric tensors. SIAM Journal on Matrix Analysis and Applications, 36
(4):1638–1659, 2015.
Cun Mu, Daniel Hsu, and Donald Goldfarb. Greedy approaches to symmetric orthogonal tensor
decomposition. SIAM Journal on Matrix Analysis and Applications, 38(4):1210–1226, 2017.
Peter J Mucha, Thomas Richardson, Kevin Macon, Mason A Porter, and Jukka-Pekka Onnela.
Communitystructureintime-dependent,multiscale,andmultiplexnetworks. Science,328(5980):
876–878, 2010.
Esa Ollila. The deflation-based fastica estimator: Statistical analysis revisited. IEEE transactions
on Signal Processing, 58(3):1527–1541, 2009.
Jialin Ouyang and Ming Yuan. On the multiway principal component analysis. arXiv preprint
arXiv:2302.07216, 2023.
Aaron Potechin and David Steurer. Exact tensor completion with sum-of-squares. In Conference
on Learning Theory, pages 1619–1673. PMLR, 2017.
Garvesh Raskutti, Ming Yuan, and Han Chen. Convex regularization for high-dimensional mul-
tiresponse tensor regression. The Annals of Statistics, 47(3):1554–1584, 2019.
Emile Richard and Andrea Montanari. A statistical model for tensor pca. In Advances in Neural
Information Processing Systems, pages 2897–2905, 2014.
Mark Rogers, Lei Li, and Stuart J Russell. Multilinear dynamical systems for tensor time series.
Advances in Neural Information Processing Systems, 26, 2013.
Tselil Schramm and David Steurer. Fast and robust tensor decomposition with applications to
dictionary learning. In Conference on Learning Theory, pages 1760–1793. PMLR, 2017.
Vatsal Sharan and Gregory Valiant. Orthogonalized als: A theoretically principled tensor decom-
position algorithm for practical use. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pages 3095–3104. JMLR. org, 2017.
Nicholas D Sidiropoulos, Lieven De Lathauwer, Xiao Fu, Kejun Huang, Evangelos E Papalexakis,
and Christos Faloutsos. Tensor decomposition for signal processing and machine learning. IEEE
Transactions on Signal Processing, 65(13):3551–3582, 2017.
Gilbert W Stewart and Ji-guang Sun. Matrix perturbation theory. Academic Press, 1990.
36Will Wei Sun and Lexin Li. Dynamic tensor clustering. Journal of the American Statistical Asso-
ciation, 114(528):1894–1907, 2019.
Will Wei Sun, Junwei Lu, Han Liu, and Guang Cheng. Provable sparse tensor decomposition.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3):899–916, 2017.
Runshi Tang, Ming Yuan, and Anru R Zhang. Mode-wise principal subspace pursuit and matrix
spiked covariance model. arXiv preprint arXiv:2307.00575, 2023.
Ryota Tomioka and Taiji Suzuki. Convex tensor decomposition via structured schatten norm
regularization. Advances in neural information processing systems, 26, 2013.
LedyardRTucker. Implicationsoffactoranalysisofthree-waymatricesformeasurementofchange.
Problems in measuring change, 15:122–137, 1963.
Ledyard R Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31(3):
279–311, 1966.
DiWang, YaoZheng, andGuodongLi. High-dimensionallow-ranktensorautoregressivetimeseries
modeling. Journal of Econometrics, 238(1):105544, 2024.
Dong Wang, Xialu Liu, and Rong Chen. Factor models for matrix-valued high-dimensional time
series. Journal of econometrics, 208(1):231–248, 2019.
Miaoyan Wang and Yuchen Zeng. Multiway clustering via tensor block models. Advances in neural
information processing systems, 32, 2019.
Dong Xia and Ming Yuan. On polynomial time methods for exact low-rank tensor completion.
Foundations of Computational Mathematics, 19(6):1265–1313, 2019.
Dong Xia and Ming Yuan. Effective tensor sketching via sparsification. IEEE Transactions on
Information Theory, 67(2):1356–1369, 2021.
Dong Xia and Fan Zhou. The sup-norm perturbation of hosvd and low rank tensor denoising. The
Journal of Machine Learning Research, 20(1):2206–2247, 2019.
Dong Xia, Ming Yuan, and Cun-Hui Zhang. Statistically optimal and computationally efficient low
rank tensor completion from noisy entries. The Annals of Statistics, 49(1):76–99, 2021.
Rose Yu and Yan Liu. Learning from multiway data: Simple and efficient tensor regression. In
International Conference on Machine Learning, pages 373–381. PMLR, 2016.
37MingYuanandCun-HuiZhang. Ontensorcompletionvianuclearnormminimization. Foundations
of Computational Mathematics, 16(4):1031–1068, 2016.
Ming Yuan and Cun-Hui Zhang. Incoherent tensor norms and their applications in higher order
tensor completion. IEEE Transactions on Information Theory, 63(10):6753–6766, 2017.
Anru Zhang and Rungang Han. Optimal sparse singular value decomposition for high-dimensional
high-order data. Journal of the American Statistical Association, 114(528):1708–1725, 2019.
Anru Zhang and Dong Xia. Tensor svd: Statistical and computational limits. IEEE Transactions
on Information Theory, 64(11):7311–7338, 2018.
Anru R Zhang, Yuetian Luo, Garvesh Raskutti, and Ming Yuan. Islet: Fast and optimal low-rank
tensor regression via importance sketching. SIAM journal on mathematics of data science, 2(2):
444–479, 2020.
Hua Zhou, Lexin Li, and Hongtu Zhu. Tensor regression with applications in neuroimaging data
analysis. Journal of the American Statistical Association, 108(502):540–552, 2013.
YuchenZhouandYuxinChen. Heteroskedastictensorclustering. arXiv preprint arXiv:2311.02306,
2023.
38