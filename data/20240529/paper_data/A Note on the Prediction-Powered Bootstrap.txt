A Note on the Prediction-Powered Bootstrap
Tijana Zrnic
Department of Statistics and Stanford Data Science
Stanford University
Abstract
We introduce PPBoot: a bootstrap-based method for prediction-powered inference. PPBoot is appli-
cable to arbitrary estimation problems and is very simple to implement, essentially only requiring one
application of the bootstrap. Through a series of examples, we demonstrate that PPBoot often per-
formsnearlyidenticallyto(andsometimesbetterthan)theearlierPPI(++)methodbasedonasymptotic
normality—whenthelatterisapplicable—withoutrequiringanyasymptoticcharacterizations. Givenits
versatility,PPBootcouldsimplifyandexpandthescopeofapplicationofprediction-poweredinferenceto
problems where central limit theorems are hard to prove.
1 Introduction
Black-box predictive models are increasingly used to generate efficient substitutes for gold-standard labels
when the latter are difficult to come by. For example, predictions of protein structures are used as efficient
substitutesforslowandexpensiveexperimentalmeasurements[3,4,8],andlargelanguagemodelsareusedto
cheaply generate substitutes for scarce human annotations [5, 7, 14]. Prediction-powered inference (PPI) [1]
is a recent framework for statistical inference that combines a large amount of machine-learning predictions
with a small amount of real data to ensure simultaneously valid and statistically powerful conclusions.
While PPI [1] (and its improvement PPI++ [2]) offers a principled solution to incorporating black-box
predictions into the scientific workflow, its scope of application is still limited. The current analyses focus
on certain convex M-estimators such as means, quantiles, and GLMs to ensure tractable implementation.
Furthermore, applying PPI requires case-by-case reasoning: inference relies on a central limit theorem and
problem-specific plug-in estimates of the asymptotic variance. This makes it difficult for practitioners to
apply PPI to entirely new estimation problems.
WeintroducePPBoot: abootstrap-basedmethodforprediction-poweredinference,whichisapplicableto
arbitraryestimationproblemsandisverysimpletoimplement. PPBootdoesnotrequireanyproblem-specific
derivations or assumptions such as convexity. Across a range of practical examples, we show that PPBoot is
valid and typically at least as powerful as the earlier PPI [1] and PPI++ [2] methods. We also develop two
extensions of PPBoot: one incorporates power tuning [2], improving the power of basic PPBoot; the other
incorporates cross-fitting [15] when a good pre-trained model for producing the predictions is not available
a priori but needs to be trained or fine-tuned. Overall, PPBoot offers a simple and versatile approach to
prediction-powered inference.
Our approach to debiasing predictions is inspired by PPI(++) [1, 2], but differs in that our confidence
intervals are based on bootstrap simulations, rather than a central limit theorem with a plug-in variance
estimate. Furthermore, our approach enjoys broad applicability, going beyond convex M-estimators. A
predecessor of PPI, called post-prediction inference (postpi) [10], was motivated by inference problems in a
similarsetting, withlittlegold-standarddataandabundantmachine-learningpredictions. Likeourmethod,
the postpi method also leverages the bootstrap. However, PPBoot is quite different and has provable
guarantees for a broad family of estimation problems.
1
4202
yaM
82
]LM.tats[
1v97381.5042:viXraProblemsetup. Wehaveaccesstonlabeleddatapoints(X ,Y ), i∈[n],drawni.i.d. fromP=P ×P ,
i i X Y|X
and N unlabeled data points X˜ , i∈[N], drawn i.i.d. from the same feature distribution P . The labeled
i X
and unlabeled data are independent. For now, we also assume that we have a pre-trained machine learning
modelf thatmapsfeaturestooutcomes;weextendPPBootbeyondthisassumptioninSection4. Thus,f(X )
i
and f(X˜ ) denote the predictions of the model on the labeled and the unlabeled data points, respectively.
i
Furthermore, we use (X,Y) as short-hand notation for the whole labeled dataset, i.e. X = (X ,...,X )
1 n
and Y =(Y ,...,Y ); similarly, f(X)=(f(X ),...,f(X )). We use X˜,f(X˜), etc analogously.
1 n 1 n
Our goal is to compute a confidence interval for a population-level quantity of interest θ . For example,
0
we might be interested in the average outcome, θ = E[Y ], a regression coefficient obtained by regressing
0 i
Y on X, or the correlation coefficient between a particular feature and the outcome. We use θˆ(·) to denote
any“standard”(meaning, notprediction-powered)estimatorforθ thattakesasinputalabeleddataset. In
0
other words, θˆ(X,Y) is any standard estimate of θ . For example, if θ is a mean over the data distribution,
0 0
θ =E[g(X ,Y )] for some g, then θˆcould be the corresponding sample mean: θˆ(X,Y)= 1 (cid:80)n g(X ,Y ).
0 i i n i=1 i i
Unlike existing PPI methods, which focused on M-estimation, PPBoot does not place restrictions on θ and
0
can be applied as long as there is a sensible estimator θˆ.
2 PPBoot
We present a bootstrap-based approach to prediction-powered inference that is applicable to arbitrary es-
timation problems. The idea is very simple. Let B denote a user-chosen number of bootstrap iterations.
At every step b ∈ [B], we resample the labeled and unlabeled data with replacement; let (X∗,Y∗) and X˜∗
denote the resampled datasets. We then compute the bootstrap estimate for iteration b as
θ∗ =θˆ(X˜∗,f(X˜∗))+θˆ(X∗,Y∗)−θˆ(X∗,f(X∗)),
b
where θˆis any standard estimator for the quantity of interest. Finally, we apply the percentile method to
obtain the PPBoot confidence interval:
CPPBoot =(cid:16) quantile(cid:0) {θ∗}B ;α/2(cid:1) ,quantile(cid:0) {θ∗}B ;1−α/2(cid:1)(cid:17) ,
b b=1 b b=1
where α is the desired error level. We summarize PPBoot in Algorithm 1.
ThevalidityofCPPBoot followsfromthestandardvalidityofthebootstrap. Thekeyobservationisthatθ∗
b
is a consistent estimate if θˆis a consistent estimator: indeed, θˆ(X∗,Y∗) converges to θ and θˆ(X˜∗,f(X˜∗))−
0
θˆ(X∗,f(X∗)) simply estimates zero due to the fact that the labeled and unlabeled data follow the same
distribution. Furthermore,notonlyisourestimationstrategyconsistent,butitisalsoasymptoticallynormal
aroundθ whenθˆ(X,Y)yieldsasymptoticallynormalestimates(suchasinthecaseofM-estimation),under
0
only mild additional regularity. For mathematical details, we refer the reader to the work of Yang and
Ding [12], who propose a similar estimator for average causal effects relying on this fact. The asymptotic
normality implies that it would also be valid to compute CLT intervals centered at θˆPPBoot = θˆ(X˜,f(X˜))+
θˆ(X,Y) − θˆ(X,f(X)) with a bootstrap estimate of the standard error via θ∗, though we opted for the
b
percentile bootstrap for conceptual simplicity. Many other variants of PPBoot based on other forms of the
bootstrap are possible; see Efron and Tibshirani [6] for other options.
Algorithm 1 PPBoot
Input: labeled data (X,Y), unlabeled data X˜, model f, error level α∈(0,1), bootstrap iterations B ∈N
1: for b=1,...,B do
2: Resample (X∗,Y∗) and X˜∗ from (X,Y) and X˜ with replacement
3: Compute θ∗ =θˆ(X˜∗,f(X˜∗))+θˆ(X∗,Y∗)−θˆ(X∗,f(X∗))
b
Output: CPPBoot =(quantile(cid:0) {θ∗}B ;α/2(cid:1) ,quantile(cid:0) {θ∗}B ;1−α/2(cid:1) )
b b=1 b b=1
2Furthermore, we expect θ∗ to be more accurate than the classical estimate θˆ(X,Y) if the the machine-
b
learning predictions are reasonably accurate, since an accurate model f yields θˆ(X∗,f(X∗)) ≈ θˆ(X∗,Y∗),
and thus the bootstrap estimate is roughly θ∗ ≈ θˆ(X˜∗,f(X˜∗)). Since θˆ(X˜∗,f(X˜∗)) leverages N ≫ n data
b
points, we expect it to have far lower variability than θˆ(X,Y).
3 Applications
We evaluate PPBoot in a series of applications, comparing it to the earlier PPI and PPI++ methods [1, 2], as
wellas“classical”inference,whichusesthelabeleddataonly. Toavoidcherry-pickingexampleapplications,
we primarily focus on the datasets and estimation problems studied by Angelopoulos et al. [1]. To showcase
the versatility of our method, we run additional experiments with estimation problems not easily handled
by PPI. For now, we do not use power tuning in PPI++; we will return to power tuning in the next section.
Over 100 trials, we randomly split the data (which is fully labeled) into a labeled component of size n
and treat the remainder as unlabeled. The confidence intervals are computed at error level α = 0.1. We
report the interval width and coverage averaged over the 100 trials, for varying n. To compute coverage, we
takethevalueofthequantityofinterestonthewholedatasetasthegroundtruth. Wealsoplottheinterval
computed by each method for three randomly chosen trials, for a fixed n. We apply PPBoot with B =1000.
Each of the following applications defines a unique estimation problem on a unique dataset. We briefly
describe each application; for further details, we refer the reader to [1].
Galaxies. In the first application, we study galaxy data from the Galaxy Zoo 2 dataset [11], consisting
of human-annotated images of galaxies from the Sloan Digital Sky Survey [13]. The quantity of interest
is the fraction of spiral galaxies, i.e., the mean of a binary indicator Y ∈ {0,1} which encodes whether a
i
galaxy has spiral arms. We use the predictions by Angelopoulos et al. [1], which are obtained by fine-tuning
a pre-trained ResNet on a separate subset of galaxy images from the Galaxy Zoo 2. We show the results in
Figure 1. We observe that PPI and general PPBoot have essentially identical interval widths, significantly
outperforming classical inference based on a standard CLT interval. All methods approximately achieve the
nominal coverage.
AlphaFold. The next example concerns estimating a particular odds ratio between two binary variables:
phosphorylation, a regulatory property of a protein, and disorder, a structural property. This problem was
studied by Bludau et al. [4]. Since disorder is difficult to measure experimentally, AlphaFold [8] predictions
are used to impute the missing values of disorder. For the application of PPI, we apply the asymptotic
analysis based on the delta method provided in [2], as it is more powerful than the original analysis in [1].
Figure 2 shows the performance of the methods. General PPBoot performs similarly to PPI in terms of
interval width, though slightly worse. As expected, classical inference based on the CLT yields much larger
intervals than the other baselines. All methods achieve the desired coverage.
1.00
0.100
0.75
0.075
0.50 classical
0.050 PPI
0.25
PPBoot
0.025 0.00
0.20 0.25 0.30 500 1000 1500 500 1000 1500
fraction of spiral galaxies n n
Figure 1: Classical inference, PPI, and PPBoot, applied to estimating the fraction of spiral galaxies from
galaxy images.
3
htdiw
lavretni
egarevoc1.00
0.75 3
0.50 classical
2
PPI
0.25
PPBoot
1
0.00
1 2 3 4 200 300 400 500 600 300 400 500 600
odds ratio n n
Figure 2: Classical inference, PPI, and PPBoot, applied to estimating the odds ratio between protein phos-
phorylation and protein disorder with AlphaFold predictions.
1.00
1.00
0.75
0.75
0.50 classical
0.50
PPI
0.25
0.25 PPBoot
0.00
5.0 5.5 6.0 2000 4000 6000 2000 4000 6000
median gene expression n n
Figure 3: Classical inference, PPI, and PPBoot, applied to estimating the median gene expression with
transformer predictions.
Geneexpression. BuildingontheanalysisofVaishnavetal.[9],whotrainedastate-of-the-arttransformer
model to predict the expression level of a particular gene induced by a promoter sequence, Angelopoulos
et al. [1] computed PPI confidence intervals on quantiles that characterize how a population of promoter
sequencesaffectsgeneexpression. Theycomputedtheq-quantileofgeneexpression,forq ∈{0.25,0.5,0.75}.
We report the results for estimating the median gene expression with PPBoot, though our findings are not
substantially different for q ∈ {0.25,0.75}. See Figure 3 for the results. We observe that PPBoot leads to
substantially tighter intervals than PPI: PPBoot improves over PPI roughly as much as PPI improves over
classical CLT inference, all the while maintaining correct coverage.
Census. We investigate the relationship between socioeconomic variables in US census data, in particular
theAmericanCommunitySurveyPublicUseMicrodataSample(ACSPUMS)collectedinCaliforniain2019.
We study two applications: in the first we evaluate the relationship between age and income, and in the
second we evaluate the relationship between income and having health insurance. We use the predictions of
incomeandhealthinsurance,respectively,from[1],obtainedbytrainingagradient-boostedtreeonhistorical
censusdataincludingvariousdemographiccovariates,suchassex,age,education,disabilitystatus,andmore.
1.00
0.75
300
0.50 classical
PPI
0.25
200 PPBoot
0.00
1000 1200 1400 1000 1500 2000 1000 1500 2000
OLS coefficient n n
Figure 4: Classical inference, PPI, and PPBoot, applied to estimating the relationship between age and
income in US census data.
4
htdiw
lavretni
htdiw
lavretni
htdiw
lavretni
egarevoc
egarevoc
egarevoc1e 5
1.00
2.0
0.75
1.5 0.50 classical
PPI
1.0 0.25 PPBoot
0.00
1 2 3 500 1000 1500 500 1000 1500
logistic coefficient 1e 5 n n
Figure 5: Classical inference, PPI, and PPBoot, applied to estimating the relationship between income and
having health insurance in US census data.
1.00
0.10 0.75
0.50 classical
0.05
PPBoot
0.25
imputed
0.00 0.00
0.2 0.4 0.6 500 1000 1500 500 1000 1500
correlation coefficient n n
Figure 6: Classical inference, PPBoot, and the imputed approach, applied to estimating the relationship
between age and income in US census data.
In the first application, the target of inference is the ordinary least-squares (OLS) coefficient between age
and income, controlling for sex. In the second application, the target of inference is the logistic regression
coefficient between income and the indicator of having health insurance. We plot the results for linear and
logisticregressioninFigure4andFigure5,respectively. Inbothapplications,PPBootyieldssimilarintervals
to PPI, and both methods give smaller intervals than classical inference based on the CLT. In the second
application, PPBoot slightly undercovers, though that may be resolved by increasing B.
To show that PPBoot is applicable quite broadly, we also quantify the relationship between age and
income andincome and healthinsurance, respectively, usingthe Pearsoncorrelation coefficient. Priorworks
on PPI(++) do not study this problem, as the theory is not easy to apply to this estimand. To demonstrate
that this is a nontrivial problem, we also form confidence intervals via the “imputed” approach, which
naivelytreatsthepredictionsasrealdata,andshowthatitseverelyundercoversthetarget. (Forallprevious
applications, Angelopoulos et al. [1] demonstrated the lack of validity of the imputed approach.) We show
the results in Figure 6 and Figure 7, respectively. PPBoot yields smaller intervals than classical inference,
while maintaining approximately correct coverage. In this application, classical inference uses the classical
percentile bootstrap method. The imputed approach yields very small intervals and zero coverage.
1.00
0.10 0.75
0.50 classical
0.05
PPBoot
0.25
imputed
0.00 0.00
0.2 0.3 500 1000 1500 500 1000 1500
correlation coefficient n n
Figure 7: Classical inference, PPBoot, and the imputed approach, applied to estimating the relationship
between income and having health insurance in US census data.
5
htdiw
lavretni
htdiw
lavretni
htdiw
lavretni
egarevoc
egarevoc
egarevoc4 Extensions
We state two extensions of PPBoot that improve upon the basic method along different axes. The first one
is a strict generalization of PPBoot that handles a version of “power tuning” [2], leading to more powerful
inferences than the basic method. The second one extends PPBoot to problems where the predictive model
f is not available a priori, the setting studied in [15].
4.1 Power-tuned PPBoot
At a conceptual level, power tuning [2] is a way of choosing how much to rely on the machine-learning
predictions so that their use never leads to wider intervals. In particular, power tuning should enable
recovering the classical bootstrap when the predictions provide no signal about the outcome.
We define the power-tuned version of PPBoot by simply adding a multiplier λ∈R to the terms that use
predictions:
θ∗ =λ·θˆ(X˜∗,f(X˜∗))+θˆ(X∗,Y∗)−λ·θˆ(X∗,f(X∗)).
b
The parameter λ determines the degree of reliance on the predictions: λ = 1 recovers the basic PPBoot;
λ=0 recovers the classical bootstrap.
We will next show how to tune λ from data as part of PPBoot. Before doing so, we derive the optimal λ
that the tuning procedure will aim to approximate. One reasonable goal is to pick λ so that the variance of
the bootstrap estimates Var(θ∗) is minimized. A short calculation shows that the optimal tuning parameter
b
for this criterion equals:
(cid:16) (cid:17)
Cov θˆ(X∗,f(X∗)),θˆ(X∗,Y∗)
λ = .
opt Var(X∗,f(X∗))+Var(X˜∗,f(X˜∗))
To incorporate power tuning into PPBoot, at every bootstrap iteration b we perform an additional, inner
bootstrap where we draw b′ bootstrap samples from (X∗,Y∗) and X˜∗—denoted (X∗∗,Y∗∗) and X˜∗∗—and
compute
(cid:16) (cid:17)
C(cid:100)ov θˆ(X∗∗,f(X∗∗)),θˆ(X∗∗,Y∗∗)
λ∗ = ,
b V(cid:100)ar(X∗∗,f(X∗∗))+V(cid:100)ar(X˜∗∗,f(X˜∗∗))
where the empirical covariance and variances are computed on the b′ inner bootstrap draws of the data.
We evaluate the benefits of power tuning empirically. We revisit two applications: estimating the fre-
quency of spiral galaxy via computer vision and estimating the odds ratio between phosphorylation and
disorder via AlphaFold. The problem setup is the same as before, only here we additionally evaluate the
power-tuned versions of PPI and PPBoot. We plot the results in Figure 8 and Figure 9. As we saw before,
PPIandPPBootyieldintervalsofsimilarsize. Perhapsmoresurprisingly, thetunedversionsalsoyieldinter-
valsofsimilarsize,eventhoughthetuningproceduresaredifferentlyderived. Forexample,intheAlphaFold
application, tuned PPI has two tuning parameters, while tuned PPBoot has only one. Also, as expected, the
power-tuned procedures outperform their non-tuned counterparts.
4.2 Cross-PPBoot
Next, we extend PPBoot to problems where we do not have access to a pre-trained model f. We therefore
havethelabeleddata(X,Y)andtheunlabeleddataX˜, butnof. Thisisthesettingconsideredin[15], and
our solution will resemble theirs. It helps to think of PPBoot as an algorithm that takes (X,Y,f(X)) and
(X˜,f(X˜)) as inputs:
(cid:16) (cid:17)
CPPBoot =PPBoot (X,Y,f(X)),(X˜,f(X˜)) .
One obvious solution to not having f is to simply split off a fraction of the labeled data and use it to train
f. Once f is trained, we use the remainder of the labeled data and the unlabeled data to apply PPBoot. Of
61.00
0.025
0.75
PPI
0.020
0.50 PPI (tuned)
PPBoot
0.25
0.015 PPBoot (tuned)
0.00
0.25 0.26 0.27 2000 4000 6000 8000 2000 4000 6000 8000
fraction of spiral galaxies n n
Figure 8: PPI, PPBoot, and their tuned versions, applied to estimating the fraction of spiral galaxies from
galaxy images.
1.00
0.8
0.75
PPI
0.50 PPI (tuned)
0.6 PPBoot
0.25
PPBoot (tuned)
0.00
1.8 2.0 2.2 2.4 1000 2000 3000 4000 2000 3000 4000
odds ratio n n
Figure 9: PPI, PPBoot, and their tuned versions, applied to estimating the odds ratio between phosphoryla-
tion and protein disorder with AlphaFold predictions.
course, this data-splitting baseline may be wasteful because we do not use all labeled data at our disposal
for inference. Furthermore, we do not use all labeled data to train f either.
To remedy this problem, we define cross-PPBoot, a method that leverages cross-fitting similarly to
cross-PPI[15]. WepartitionthedataintoK folds,I ,...,I ,∪K I =[n]. Typically,K willbeaconstant
1 K j=1 j
such as K =10. Then, we train K models, f(1),...,f(K), using any arbitrary learning algorithm, such that
model f(j) is trained on all labeled data except fold I . Finally, we apply PPBoot, but for every point X in
j i
fold I , we use f(j)(X ) as the corresponding prediction, and for every unlabeled point X˜ , we use f¯(X˜ )=
j i i i
1 (cid:80)K f(j)(X˜ ) as the corresponding prediction. In other words, let f(1:K)(X)=(f(1)(X ),...,f(K)(X ))
K j=1 i 1 n
bethevectorofpredictionscorrespondingtoX ,...,X , andletf¯(x)= 1 (cid:80)K f(j)(x)denotetheaverage
1 n K j=1
model. Then, we have
(cid:16) (cid:17)
Ccross-PPBoot =PPBoot (X,Y,f(1:K)(X)),(X˜,f¯(X˜)) .
The cross-fitting prevents the predictions on the labeled data from overfitting to the training data.
Wenowshowempiricallythatcross-PPBootisamorepowerfulalternativetoPPBootwithdatasplitting
usedtotrainasinglemodelf. Wealsocomparecross-PPBootwithcross-PPI[15],sincebotharedesigned
for settings where a pre-trained model is not available. cross-PPI leverages cross-fitting in a similar way
as cross-PPBoot, however it computes confidence intervals based on a CLT rather than the bootstrap. As
before, to avoid cherry-picking, we focus on the applications studied by Zrnic and Cand`es [15].
We consider the applications to spiral galaxy estimation from galaxy images and estimating the OLS
coefficient between age and income in US census data. We use the same data and model-fitting strategy
as in [15]. See Figure 10 and Figure 11 for the results. In these two figures, PPI and PPBoot refer to the
data-splitting baseline, where we first train a model and then apply PPI and general PPBoot, respectively.
Qualitatively the takeaways are similar to the takeaways from the power tuning experiments. The use
of cross-fitting improves upon the basic versions of PPI and PPBoot, but again, somewhat surprisingly,
cross-PPI and cross-PPBoot lead to very similar intervals.
7
htdiw
lavretni
htdiw
lavretni
egarevoc
egarevoc1.00
0.012
0.75
PPI
0.010
0.50 cross-PPI
0.008 PPBoot
0.25
cross-PPBoot
0.006
0.00
0.250 0.255 0.260 0.265 0.270 10000 15000 20000 25000 30000 10000 15000 20000 25000 30000
fraction of spiral galaxies n n
Figure 10: PPI, cross-PPI, PPBoot, and cross-PPBoot, applied to estimating the fraction of spiral galaxies
from galaxy images.
1.00
150
0.75
PPI
100 0.50 cross-PPI
PPBoot
0.25
cross-PPBoot
50
0.00
950 1000 1050 1100 1150 10000 20000 30000 10000 20000 30000
OLS coefficient n n
Figure 11: PPI, cross-PPI, PPBoot, and cross-PPBoot, applied to estimating the relationship between age
and income in US census data.
Acknowledgements
T.Z. thanks Bradley Efron for many useful discussions that inspired this note.
References
[1] Anastasios N Angelopoulos, Stephen Bates, Clara Fannjiang, Michael I Jordan, and Tijana Zrnic.
Prediction-powered inference. Science, 382(6671):669–674, 2023.
[2] Anastasios N Angelopoulos, John C Duchi, and Tijana Zrnic. PPI++: Efficient prediction-powered
inference. arXiv preprint arXiv:2311.01453, 2023.
[3] Inigo Barrio-Hernandez, Jingi Yeo, Ju¨rgen J¨anes, Milot Mirdita, Cameron LM Gilchrist, Tanita Wein,
MihalyVaradi,SameerVelankar,PedroBeltrao,andMartinSteinegger. Clusteringpredictedstructures
at the scale of the known protein universe. Nature, 622(7983):637–645, 2023.
[4] Isabell Bludau, Sander Willems, Wen-Feng Zeng, Maximilian T Strauss, Fynn M Hansen, Maria C
Tanzer, Ozge Karayel, Brenda A Schulman, and Matthias Mann. The structural context of posttrans-
lational modifications at a proteome-wide scale. PLoS biology, 20(5):e3001636, 2022.
[5] Pierre Boyeau, Anastasios N Angelopoulos, Nir Yosef, Jitendra Malik, and Michael I Jordan. Autoeval
done right: Using synthetic data for model evaluation. arXiv preprint arXiv:2403.07008, 2024.
[6] Bradley Efron and Robert J Tibshirani. An introduction to the bootstrap. Chapman and Hall/CRC,
1994.
[7] Shuxian Fan, Adam Visokay, Kentaro Hoffman, Stephen Salerno, Li Liu, Jeffrey T Leek, and Tyler H
McCormick. Fromnarrativestonumbers: Validinferenceusinglanguagemodelpredictionsfromverbal
autopsy narratives. arXiv preprint arXiv:2404.02438, 2024.
8
htdiw
lavretni
htdiw
lavretni
egarevoc
egarevoc[8] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
KathrynTunyasuvunakool,RussBates,AugustinZˇ´ıdek,AnnaPotapenko,etal.Highlyaccurateprotein
structure prediction with alphafold. Nature, 596(7873):583–589, 2021.
[9] Eeshit Dhaval Vaishnav, Carl G de Boer, Jennifer Molinet, Moran Yassour, Lin Fan, Xian Adiconis,
DawnAThompson,JoshuaZLevin,FranciscoACubillos,andAvivRegev. Theevolution,evolvability
and engineering of gene regulatory dna. Nature, 603(7901):455–463, 2022.
[10] Siruo Wang, Tyler H McCormick, and Jeffrey T Leek. Methods for correcting inference based on
outcomes predicted by machine learning. Proceedings of the National Academy of Sciences, 117(48):
30266–30275, 2020.
[11] Kyle W Willett, Chris J Lintott, Steven P Bamford, Karen L Masters, Brooke D Simmons, Kevin RV
Casteels, Edward M Edmondson, Lucy F Fortson, Sugata Kaviraj, William C Keel, et al. Galaxy zoo
2: detailed morphological classifications for 304 122 galaxies from the sloan digital sky survey. Monthly
Notices of the Royal Astronomical Society, 435(4):2835–2860, 2013.
[12] Shu Yang and Peng Ding. Combining multiple observational data sources to estimate causal effects.
Journal of the American Statistical Association, 2019.
[13] Donald G York, J Adelman, John E Anderson Jr, Scott F Anderson, James Annis, Neta A Bahcall,
JA Bakken, Robert Barkhouser, Steven Bastian, Eileen Berman, et al. The sloan digital sky survey:
Technical summary. The Astronomical Journal, 120(3):1579, 2000.
[14] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
ZhuohanLi,DachengLi,EricXing,etal. JudgingLLM-as-a-judgewithMT-BenchandChatbotArena.
Advances in Neural Information Processing Systems, 36, 2024.
[15] TijanaZrnicandEmmanuelJCand`es. Cross-prediction-poweredinference. Proceedings of the National
Academy of Sciences (PNAS), 121(15), 2024.
9