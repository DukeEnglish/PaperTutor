Hierarchical World Models as
Visual Whole-Body Humanoid Controllers
NicklasHansen1 JyothirSV2 VladSobal2 YannLeCun2,3
XiaolongWang1∗ HaoSu1∗
1UCSanDiego 2NewYorkUniversity 3MetaAIFAIR
∗Equaladvising
Figure1.Visualwhole-bodycontrolforhumanoids.WepresentPuppeteer,ahierarchicalworld
modelforwhole-bodyhumanoidcontrolwithvisualobservations.Ourmethodproducesnaturaland
human-likemotionswithoutanyrewarddesignorskillprimitives,andtraverseschallengingterrain.
Abstract
Whole-bodycontrolforhumanoidsischallengingduetothehigh-dimensionalna-
tureoftheproblem,coupledwiththeinherentinstabilityofabipedalmorphology.
Learningfromvisualobservationsfurtherexacerbatesthisdifficulty. Inthiswork,
weexplorehighlydata-drivenapproachestovisualwhole-bodyhumanoidcontrol
based on reinforcement learning, without any simplifying assumptions, reward
design,orskillprimitives. Specifically,weproposeahierarchicalworldmodelin
whichahigh-levelagentgeneratescommandsbasedonvisualobservationsfora
low-levelagenttoexecute,bothofwhicharetrainedwithrewards. Ourapproach
produces highly performant control policies in 8 tasks with a simulated 56-DoF
humanoid,whilesynthesizingmotionsthatarebroadlypreferredbyhumans.
Codeandvideos: https://nicklashansen.com/rlpuppeteer
1 Introduction
Learning a generalist agent in the physical world is a long-term goal of many researchers in AI.
Among variant agent designs, humanoids stand out as versatile platforms capable of performing
a wide range of tasks, by integrating whole-body control and perception. However, this is a very
challenging problem due to the high-dimensional nature of the observation and action spaces, as
wellasthecomplexdynamicsofabipedalembodiment,anditmakeslearningsuccessfulyetnatural
whole-bodycontrollerswithreinforcementlearning(RL)extremelydifficult. Forexample,consider
the task shown in Figure 1, where a humanoid is rewarded for forward progress while jumping
overgaps. Tosucceedinthistask,ahumanoidneedstoaccuratelyperceivethepositionandlength
of oncoming floor gaps, while carefully coordinating full body motions such that it has sufficient
momentumandrangetoreachacrosseachgap.
Preprint.CorrespondencetoNicklasHansen<nihansen@ucsd.edu>.
4202
yaM
82
]GL.sc[
1v81481.5042:viXraDuetothesheercomplexityofsuchproblems,priorworkchoosetomakesimplifiedassumptions,
suchasusinglow-dimensional(privileged)observationsandactions[Heessetal.,2017,Pengetal.,
2018, Wagener et al., 2022, Jiang et al., 2023], or (learned) skill primitives [Merel et al., 2017,
2018b, Hasenclever et al., 2020, Cheng et al., 2024]. While these approaches are able to solve a
variety of tasks to some extent, we expect to find a solution that is entirely data-driven and relies
onasfewassumptionsaspossible. Inthispaper,weproposetodirectlylearnacontrolleronhigh-
dimensionalhumanoidrobotjoints,leveragingmodel-basedRLwithanexistinglarge-scalemotion
capture(MoCap)dataset[CMU,2003].
Weproposeadata-drivenRLmethodforvisualwhole-bodycontrolthatproducesnatural,human-
likemotionsandcansolvediversetasks. Ourapproach,dubbedPuppeteer,isahierarchicalJEPA-
style [LeCun, 2022] world model that consists of two distinct agents: a proprioceptive tracking
agentthattracksareferencemotionviajoint-levelcontrol,andavisualpuppeteeragentthatlearns
toperformdownstreamtasksbysynthesizinglower-dimensionalreferencemotionsforthetracking
agenttotrackbasedonvisualobservations.
Concretely,thetwoagentsaretrainedindependentlyintwoseparatestagesusingthemodel-based
RL algorithm TD-MPC2 [Hansen et al., 2024] as a learning backbone. First, a single tracking
worldmodelis(pre)trainedtotrackreferencemotionsfrompre-existinghumanMoCapdata[CMU,
2003] re-targeted to a humanoid embodiment [Wagener et al., 2022]. It learns a single model to
convertanyreferencekinematicmotiontophysicallyexecutableactions. Thisisadeparturefrom
previous work that often learns multiple low-level models [Merel et al., 2017, Hasenclever et al.,
2020]. Importantly, this tracking agent can be saved and reused across all downstream tasks. In
the second stage, we train a puppeteering world model that takes visual observation as inputs and
outputs the reference motion for the tracking agent based on the specified downstream task. The
puppeteeragentistrainedwithonlineenvironmentinteractionusingthefixedtrackingagent. Akey
featureofourframeworkisitsstrikingsimplicity: bothworldmodelsarealgorithmicallyidentical
(butdifferininputs/outputs)andcanbetrainedusingRLwithoutanybellsandwhistles. Different
fromatraditionalhierarchicalRLsetting,ourpuppeteeragent(high-levelpolicy)outputsgeometric
locationsforasmallnumberofend-effectorjointsinsteadofagoalembedding. Thetrackingagent
(low-levelpolicy)isthusonlyrequiredtolearnjoint-levelphysics. Thismakesthetrackingagent
easilysharableandgeneralizableacrosstasks,leadingtoanoverallsmallcomputationalfootprint.
To evaluate the efficacy of our approach, we propose a new task suite for visual whole-body hu-
manoidcontrolwithasimulated56-DoFhumanoid, whichcontainsatotalof8challengingtasks.
We show that our method produces highly performant control policies across all tasks compared
toasetofstrongmodel-freeandmodel-basedbaselines: SAC[Haarnojaetal.,2018],DreamerV3
[Hafneretal.,2023],andTD-MPC2[Hansenetal.,2024]. Furthermore,wefindthatmotionsgen-
erated by our method are broadly preferred by humans in a user study with 46 participants. We
concludethepaperbycarefullydissectinghoweachofourdesignchoicesinfluenceresults. Code
formethodandenvironmentsisavailableathttps://nicklashansen.com/rlpuppeteer. Our
maincontributionscanbesummarizedasfollows:
• Tasksuite.Weproposeanew,challengingtasksuiteforvisualwhole-bodyhumanoidcontrol
withasimulated56-DoFhumanoid. Thetasksuitehas8tasksintotal.
• Hierarchical world model. We propose a method for high-dimensional continuous control
thatleveragesalearnedhierarchicalworldmodelforplanning.
• Evaluating “naturalness” of controllers. We develop several metrics for quantifying how
naturalandhuman-likegeneratedmotionsareacrosstasksinoursuite,includinghumanpref-
erencesfromauserstudy.
• Analysis & ablations. We carefully ablate each of our design choices, analyze the relative
importanceofeachcomponent,andprovideactionableadviceforfutureworkinthisarea.
2 Preliminaries
Problem formulation. We model visual whole-body humanoid control as a reinforcement learn-
ing problem governed by an episodic Markov Decision Process (MDP) characterized by the tuple
(S,A,T,R,γ,∆)wheres ∈ S arestates,a ∈ Aareactions,S: S ×A (cid:55)→ S istheenvironment
transition(dynamics)function,R: S×A(cid:55)→Risascalarrewardfunction,γ isthediscountfactor,
and∆: S (cid:55)→{0,1}isanepisodeterminationcondition. Weimplicitlyconsiderbothproprioceptive
2MoCap tracking (RL) Downstream tasks (RL) +
Tracking Puppeteering
agent agent
Tracking Tracking Puppeteer
World Model World Model World Model
Save & Reuse
1. Pretrain tracking agent on MoCap data 2. Train puppeteering agent on downstream tasks
Figure2. Approach. Wepretrainatrackingagent(worldmodel)onhumanMoCapdatausingRL;
this agent takes proprioceptive information q and an abstract reference motion (command) c as
t t
input, andsynthesizesH low-levelactionsthattracksthereferencemotion. Wethentrainahigh-
levelpuppeteeringagentondownstreamtasksviaonlineinteraction; thisagenttakesbothstateq
t
andvisualinformationv asinput,andoutputscommandsforthetrackingagenttoexecute.
t
informationqandvisualinformationv aspartofstatessandwillonlymakethedistinctionclear
when necessary. We aim to learn a policy π: S (cid:55)→ A that maximizes discounted sum of rewards
(cid:104) (cid:105)
in expectation: E (cid:80)T γtr , r = R(s ,π(s )) for an episode of length T, while synthesiz-
π t=0 t t t t
ing motions that look “natural”. We informally define natural motions as policy rollouts that are
human-like,butdevelopseveralmetricsformeasuringthe“naturalness”ofpoliciesinSection4.
TD-MPC2. Webuilduponthemodel-basedreinforcementlearning(MBRL)algorithmTD-MPC2
[Hansenetal.,2024],whichrepresentsthestate-of-the-artincontinuouscontrolandhasbeenshown
tooutperformalternativesintaskswithhigh-dimensionalactionspaces[Hansenetal.,2022,2024,
Sferrazza et al., 2024]. Specifically, TD-MPC2 learns a latent decoder-free world model from en-
vironmentinteractionsandselectsactionsbyplanningwiththelearnedmodel. Allcomponentsof
the world model are learned end-to-end using a combination of joint-embedding prediction [Grill
etal.,2020],rewardprediction,andtemporaldifference[Sutton,1998]losses,withoutdecodingraw
observations. Duringinference,TD-MPC2followstheModelPredictiveControl(MPC)framework
forlocaltrajectoryoptimizationusingModelPredictivePathIntegral(MPPI)[Williamsetal.,2015]
as a derivative-free (sampling-based) optimizer. To accelerate planning, TD-MPC2 additionally
learnsamodel-freepolicypriorwhichisusedtowarm-startthesamplingprocedure.
3 AHierarchicalWorldModelforHigh-DimensionalControl
Weaimtolearnhighlyperformantand“natural”policiesforvisualwhole-bodyhumanoidcontrol
inadata-drivenmannerusinghierarchicalworldmodels. Akeystrengthofourapproachisthatit
cansynthesizehuman-likemotionswithoutanyexplicitdomainknowledge, rewarddesign, expert
demonstrations,norskillprimitives. Whilewefocusonhumanoidcontrolduetotheircomplexity,
our approach can in principle be applied to any embodiment. Our method, dubbed Puppeteer,
consistsoftwodistinctagents,bothofwhichareimplementedasTD-MPC2worldmodels[Hansen
et al., 2024] and trained independently. Figure 2 provides an overview of our method. The two
agentsaredesignedasfollows:
1. Alow-leveltrackingagentthattakesarobotproprioceptivestateq andanabstractcommand
t
c asinputattimet,andusesplanningwithalearnedworldmodeltosynthesizeasequenceof
t
H controlactions{a ,a ,...,a }that(approximately)obeystheabstractcommand.
t t+1 t+H
2. A high-level puppeteering agent that takes the same robot proprioceptive state q as input,
t
aswellas(optionally)auxiliaryinformationandmodalitiessuchasRGBimagesv ortask-
t
relevantinformation,andusesplanningwithalearnedworldmodeltosynthesizeasequence
ofH high-levelabstractcommands{c ,c ,...,c }forthelow-levelagenttoexecute.
t t+1 t+H
3A unique benefit of our approach is that a single tracking world model can be (pre)trained and
reused across all downstream tasks. The tracking and puppeteering world models are algorithmi-
callyidentical(butdifferininputs/outputs),andconsistofthefollowing6components:
Encoder z=h(s) ▷Encodesstateintoalatentembedding
Latentdynamics z′ =d(z,a) ▷Predictsnextlatentstate
Reward rˆ=R(z,a) ▷Predictsrewardrofastatetransition
(1)
Termination δˆ=D(z,a) ▷Predictsprobabilityoftermination
Terminalvalue qˆ=Q(z,a) ▷Predictsdiscountedsumofrewards
Policyprior aˆ =p(z) ▷Predictsanactiona∗thatmaximizesQ
wherezisalatentstate. BecauseweconsiderepisodicMDPswithterminationconditions,weaddi-
tionallyaddaterminationpredictionheadD(highlightedinEquation1)thatpredictstheprobability
ofterminationconditionedonalatentstateandaction. Useofterminationsignalsinthecontextof
planningwithaworldmodelrequiresspecialcareandhas,tothebestofourknowledge,notbeen
explored in prior work; we introduce a novel method for this in Section 3.3. In the following, we
describe the two agents and their interplay in the context of visual whole-body humanoid control.
MinorimplementationdetailsareprovidedinAppendixB.
3.1 Low-LevelTrackingWorldModel
We first train the low-level tracking world model independently
from the high-level agent and any potential downstream tasks.
We leverage pre-existing human MoCap data [CMU, 2003] re-
targeted to the 56-DoF “CMU Humanoid” embodiment [Tassa
et al., 2018] during training of the tracking model, which (as we
willlatershowempirically)implicitlyencodeshumanmotionpri-
ors. Specifically, we train our tracking world model by sam-
pling (s ,a ,r ,s ,...,r ) sequences from MoCapAct [Wa-
t t t t+1 H
generetal.,2022],anofflinedatasetthatconsistsofnoisy,subop-
timalrolloutsfromexistingpoliciestrainedtotrackreferencemo-
tions(836MoCapclips). Thisisincontrasttopriorliteraturethat Figure 3. MoCap tracking.
learnper-clippoliciesorskillprimitives[Heessetal.,2017,Merel The low-level tracking agent
etal.,2017,Hasencleveretal.,2020]usingexpertdemonstrations. istrainedtotrackrelativeend-
effector (head, hands, feet)
Observations include humanoid proprioceptive information q at
t positions of sampled refer-
timet, aswellasareferencemotion(command)c totrack. Dur-
. t encemotionsin3Dspace.
ingtrainingofthetrackingpolicy,weletc = (qref )where
t t+1...t+H
eachqref correspondstorelativeend-effector(head,hands,feet)positionsofthesampledreference
motion at a future timestep; during downstream tasks, we train the high-level agent to output (via
planning)commandscforthelow-levelagenttotrack. Figure3illustratesourlow-dimensionalref-
erence;thecontrollablehumanoidtracksend-effectorpositionsofareferencemotion. Welabel
all transitions using the reward function from Hasenclever et al. [2020]. To improve state-action
coverageofthetrackingworldmodel,wetrainwithacombinationofofflinedataandonlineinter-
actions,maintainingaseparatereplaybufferforonlineinteractiondataandsamplingoffline/online
datawitha50%/50%ratioineachgradientupdateasinFengetal.[2023].Wefindthistobecrucial
fortrackingperformancewhentrainingasingleworldmodelonalargenumberofMoCapclips.
3.2 High-LevelPuppeteeringWorldModel
We now consider training a high-level puppeteering world model via online interaction in down-
streamtasks. AsillustratedinFigure2,thepuppeteeringmodelistrained(usingdownstreamtask
rewards) to control the tracking model via commands c, i.e., we redefine commands to now be
the action space of the puppeteering agent. The tracking world model remains frozen (no weight
updates)throughoutthisprocess,whichallowsustoreusethesametrackingmodelacrossalldown-
stream tasks. Because the high-level agent uses planning for action selection, it natively supports
temporalabstractionbyoutputtingmultiplecommands(c ,c ,...,c )forthelow-levelagent
t t+1 t+H
to execute; we treat the number of low-level steps per high-level step as a hyperparameter k that
allowsustotradestrongmotionprior(largek)forcontrolgranularity(smallk).
4stand walk run
corridor hurdles walls gaps stairs
Figure4. Tasks. Wedevelop5visualwhole-bodyhumanoidcontroltaskswitha56-DoFsimulated
humanoid(bottom),aswellas3non-visualtasks(top). SeeAppendixCformoredetails.
3.3 PlanningwithTerminationConditions
We consider episodic MDPs with termination conditions. In the context of humanoid control, a
commonsuchterminationconditionisnon-footcontactwiththefloor.Useofterminationconditions
requires special care in the context of world model learning and planning, as both components
are used to simulate (latent) multi-step rollouts. We extend the world model of TD-MPC2 with a
terminationpredictionheadD,whichpredictstheprobabilityofterminationateachtimestep. This
terminationheadistrainedend-to-endtogetherwithallothercomponentsoftheworldmodelusing
.
L (θ)=L (θ)+αCE(δˆ,δ) (2)
Puppeteer TD-MPC2
whereδˆ,δarepredictedandground-truthterminationsignals,respectively,CEisthe(binary)cross-
entropyloss,andαisaconstantcoefficientbalancingthelosses.WeadditionallytruncateTD-targets
at terminal states during training. It is similarly necessary to truncate model rollouts and value
estimates during planning (at test-time). However, we only have access to predicted termination
signals at test-time, which can be noisy and consequently lead to high-variance value estimates
forlatentrollouts. Tomitigatethis, wemaintainacumulativeweighting(discount)oftermination
probabilitieswhenrollingoutthemodel(cappedat0),suchthatonlyasofttruncationisapplied.
4 Experiments
Our proposed method holds the promise of strong downstream task performance while still syn-
thesizing natural and human-like motions. To evaluate the efficacy of our method, we propose a
newtasksuiteforwhole-bodyhumanoidcontrolwithmulti-modalobservations(visionandpropri-
oceptiveinformation)basedonthe“CMUHumanoid”modelfromDMControl[Tassaetal.,2018].
Oursimulatedhumanoidhas56fullycontrollablejoints(A∈R56),andincludesbothhead,hands,
andfeet. Weaimtolearnhighlyperformantpoliciesinadata-drivenmannerwithouttheneedfor
embodiment-ortask-specificengineering(e.g.,rewarddesign,constraints,orauxiliaryobjectives),
whilesynthesizingnaturalandhuman-likemotions. Codeformethodandenvironmentsisavailable
athttps://nicklashansen.com/rlpuppeteer.
4.1 ExperimentalDetails
Tasks. Ourproposedtasksuiteconsistsof5vision-conditionedwhole-bodylocomotiontasks,and
an additional 3 tasks without visual input. We provide an overview of tasks in Figure 4; they are
designed with a high degree of randomization and include running along a corridor, jumping over
hurdlesandgaps,walkingupthestairs,andcircumnavigatingobstacles(walls). All5visualcontrol
tasksusearewardfunctionthatisproportionaltothelinearforwardvelocity,whilenon-visualtasks
reward displacement in any direction. Episodes are terminated at timeout (500 steps) or when a
5Stand Walk Run Corridor
400 400 400 200
200 200 200 100
0 0 0 0
Hurdles Walls Gaps Stairs
100
200 200 200
50
100 100 100
0 0 0 0
0 1M 2M 3M 0 1M 2M 3M 0 1M 2M 3M 0 1M 2M 3M
SAC DreamerV3 TD-MPC2 Puppeteer (ours)
Figure5. Learningcurves. Episodereturnasafunctionofenvironmentstepsonall8tasksfrom
ourproposedtasksuite. OurmethodmatchestheperformanceofTD-MPC2whileproducingmore
natural motions. We evaluate SAC and DreamerV3 only on proprioceptive tasks as they do not
achieveanymeaningfulperformance. Averageof3randomseeds;shadedareais95%CIs.
non-foot joint makes contact with the floor. We empirically observe that the TD-MPC2 baseline
degeneratestohighlyunrealisticbehaviorwithoutacontact-basedterminationcondition.
Implementation. We pretrain a single 5M parameter TD-MPC2
Human preference
world model to track all 836 CMU MoCap [CMU, 2003] reference
97.6%
motionsretargetedtotheCMUHumanoidmodel.Thetrackingagent
istrainedfor10Mstepsusingbothofflinedata(noisyrollouts)from
MoCapAct[Wageneretal.,2022]andonlineinteractionwithanew
referencemotionsampledineachepisode. Wesample50%ofeach
batchfromtheofflinedataset,and50%fromtheonlinereplaybuffer 0.4% 2.0%
for each gradient update; we did not experiment with other ratios.
TD-MPC2
The puppeteering agent is similarly implemented as a 5M parame-
Equal
ter TD-MPC2 world model, which we train from scratch via online
Ours
interaction on each downstream task. Observations include a 212-d
Figure 6. Human pref-
proprioceptive state vector and 64×64 RGB images from a third-
erence in humanoid mo-
person camera. Both agents act at the same frequency, i.e., we set
tions. Aggregate results
k = 1. Training the tracking world model takes approximately 12
from a user study (n = 46)
days,andtrainingthepuppeteeringworldmodeltakesapproximately
wherehumansarepresented
4days,bothonasingleNVIDIAGeForceRTX3090GPU.
withpairsofmotionsgener-
Baselines. Webenchmarkourmethodagainststate-of-the-artRLal- ated by TD-MPC2 and our
gorithms for continuous control, including (1) widely used model- method, and are asked to
free RL method Soft Actor-Critic (SAC) [Haarnoja et al., 2018] providetheirpreference.
which learns a stochastic policy and value function using a maxi-
mum entropy RL objective, (2) model-based RL method DreamerV3 [Hafner et al., 2020, 2021,
2023] which simultaneously learns a world model using a generative objective, and a model-free
policyinthelatentspaceofthelearnedworldmodel,and(3)model-basedRLmethodTD-MPC2
[Hansen et al., 2022, 2024] which learns a self-predictive (decoder-free) world model and selects
actions by planning with the learned world model. Both our method and baselines use the same
hyperparametersacrossalltasks. WeprovidefurtherimplementationdetailsinAppendixB.
4.2 MainResults
Wefirstpresentourmainbenchmarkresults,andthenanalyseandablateeachdesignchoice.
Benchmarkresults. Weevaluateourmethod,Puppeteer,andbaselinesonall8whole-bodyhu-
manoidcontroltasks. EpisodereturnasafunctionofenvironmentstepsisshowninFigure5. We
observethattheperformanceofourmethodiscomparabletothatofTD-MPC2acrossalltasks(ex-
ceptstairs),whereasSACandDreamerV3doesnotachieveanymeaningfulperformancewithinour
computationalbudgetof3Menvironmentsteps. Aswewillsoonreveal,TD-MPC2producesbetter
6hurdles−→
stairs−→
Figure 7. Qualitative results. Our hierarchical approach, Puppeteer, produces natural human
motions,whereasTD-MPC2trainedend-to-endoftenlearnshigh-performingbutunnaturalgaits.
policiesintermsofepisodereturnonthestairstask,butfarlessnaturalbehavior;thisphenomenon
iscommonlyreferredtoasrewardhacking[ClarkandAmodei,2016,Skalseetal.,2022].
“Naturalness” of humanoid controllers. We conduct a Table 1. Proxies for “naturalness”.
user study (n = 46) in which humans are shown pairs Evaluated on the hurdles task. eplen
of short (∼10s) clips of policy rollouts from TD-MPC2 denotes the average episode length
and our method, and are asked to provide their prefer- over the course of training; height is
ence.Participantsareundergraduateandgraduatestudents theaveragetorsoheight(gait)atendof
across multiple universities and disciplines. Results from training. Meanandstd. across3seeds.
thisstudyareshowninFigure6,andFigure7showstwo
eplen↑ height(cm)↑
sampleclipsfromthestudy. Whilebothmethodsperform
comparablyintermsofdownstreamtaskreward,asuper- TD-MPC2 70.7±5.5 85.9±4.7
majority of participants rate rollouts from our method as Ours 100.6±1.0 96±0.2
morenaturalthanthatofTD-MPC2, withonly4%ofre-
sponsesratingthemas“equallynatural”and0%ratingTD-MPC2asmorenatural. Thispreference
is especially pronounced in the stairs task, where TD-MPC2 achieves a higher asymptotic return
(higher forward velocity) but learns to “roll” up stairs as opposed to our method that walks. We
alsoreportseveralquantitativemeasuresofnaturalnessinTable1,whichstronglysupportouruser
studyresults. ThesefindingsunderlinetheimportanceofamoreholisticevaluationofRLpolicies
asopposedtosolelyrelyingonrewards. SeeAppendixAformoreresults.
4.3 Ablations&Analysis
We ablate each design choice of our method, including both the pretraining (tracking) and down-
streamtasklearningstages. OurexperimentalresultsaresummarizedinFigure8.
Pretraining(tracking).Ourmethodleveragesbothofflineandonlinedataduringpretrainingofthe
trackingworldmodel. Weablatethistrainingmixtureintwodistinctways: (i)usingonlyofflineor
onlinedata,and(ii)reducingthenumberofuniqueMoCapclipsseenduringtraining. Interestingly,
wefindthatleveragingbothdatasourcesleadstobettertrackingpoliciesoverall. Wehypothesize
that this is because offline data may help in learning to track especially difficult motions such as
jumpingandbalancingononeleg,whileonlinedataimprovesstate-actioncoverageandthusleads
toa morerobustworldmodel overall. Similarly, trainingonmore diverseMoCapclipsalso leads
tobettertrackingperformance. Trainingonall836MoCapclipsresultsinthebesttrackingworld
model,andweexpecttrackingtofurtherimprovewithavailabilityofmoreMoCapdata.
7
sruO
2CPM-DT
sruO
2CPM-DTPretraining(tracking) Downstreamtasks
Data mixture # MoCap clips Hierarchical planning High-level pretraining
61.7 43.0
60% 48.6 40%
50%
40% 35.1 50%
21.0 20%
20%
6.4
1.6 2.9
0% 0% 0% 0%
0 2M 4M 6M 8M 1% No planning 0 1M 2M
Offline 5% Planning high
Online 25% Planning low Scratch (default)
Offline + online 100% Planning low+high Pretrained
Figure 8. Ablations. Normalized score for various ablations of Puppeteer during pretraining
(left) and downstream tasks (right). Pretraining benefits from diverse data, as well as both pre-
existing (offline) data and online interactions. We also observe that planning is critical to whole-
bodyhumanoidcontrol. Meanacross3seeds;downstreamablationsareaveragedacross5tasks.
Generalization (gaps)
100%
50%
Training
(0.1, 0.4)
0%
0.1m 0.4m 0.8m 1.2m 0.0 0.5 1.0
Gap length
Visualizationofgaplengths
Puppeteer (ours)
Figure 9. Zero-shot generalization to larger gap lengths. (Left) Visualization of gap lengths.
Agentistrainedongaps[0.1,0.4]mandevaluatedongapsupto1.2m. (Right)Normalizedperfor-
manceasafunctionofgaplengthinthevisualgapstask. Meanof3seeds. Ourmethodachieves
non-trivialperformanceongapsupto3×thetrainingdata. CIsomittedforvisualclarity.
Downstream tasks. We conduct three ablations that help us better understand the impact of a
hierarchicalapproachtodownstreamtasks: (i)usingalearnedmodel-freepolicyinlieuofplanning
in either level of the hierarchy, (ii) pretraining of the high-level agent in addition to the low-level
agent,and(iii)evaluatingzero-shotgeneralizationtounseenenvironmentvariations(gaplengthin
the gaps task). The first two ablations are shown in Figure 8, and the latter is shown in Figure 9.
Wefindthatplanningatbothlevelsiscriticaltoeffectivewhole-bodyhumanoidcontrol,whichwe
conjectureisduetothehighdimensionalityoftheproblem;thisissupportedbyconcurrentstudies
onhigh-dimensionalcontinuouscontrolwithTD-MPC2[Hansenetal.,2024,Sferrazzaetal.,2024].
Next,wepretrainagentsonthecorridortaskandindependentlyfinetuneoneachvisualcontroltask.
Whilethespecificenvironmentsandmotionsdifferbetweentasks,wefindthatourmethodbenefits
substantially from finetuning. We conjecture that this is because the need to control a low-level
trackingagentissharedbetweenallhigh-levelagents,irrespectiveofthedownstreamtask. Finally,
weexplorethezero-shotgeneralizationabilityofourmethodtoharder,unseenvariationsofthegap
task. Interestingly, we observe that our method generalizes to gap lengths up to 3× the training
datawithoutadditionaltraining. Inlightoftheseresults,webelievethatfurtherinvestigationofthe
generalizationabilityofhierarchicalworldmodelswillbeapromisingdirectionforfutureresearch.
5 RelatedWork
Learningwhole-bodycontrollersforhumanoidsisalong-standingproblemattheintersectionof
themachinelearningandroboticscommunities. Humanoidsareofparticularinteresttothelearning
communitybecauseofthehigh-dimensionalnatureoftheproblem[Heessetal.,2017,Mereletal.,
2017, Peng et al., 2018, Merel et al., 2018b, Hasenclever et al., 2020, Wagener et al., 2022, Shi
etal.,2022,Caggianoetal.,2022,Sferrazzaetal.,2024],andtotheroboticscommunitybecauseit
isapromisingmorphologyforgeneral-purposeroboticagents[Grizzleetal.,2009,Lietal.,2023,
BostonDynamics, 2024, Unitree, 2024, Cheng et al., 2024]. Prior work predominantly focus on
learning control policies for individual tasks using model-free reinforcement learning algorithms,
manyofwhichleverageexpertdemonstrationsfromhumanMoCapdata[CMU,2003]re-targeted
totheirconsideredhumanoidembodiment[Wageneretal.,2022]and/orskillencoders[Heessetal.,
82017,Mereletal.,2018b,Hasencleveretal.,2020,Shietal.,2022]. Forexample, Wageneretal.
[2022] learn expert policies for each reference MoCap clip in the dataset, and then subsequently
distillsthemintoasinglepolicyviasupervisedlearning. Instead, wetrainasingletrackingworld
modelacrossallclipsintheMoCapAct[Wageneretal.,2022]dataset,usingonlynoisyrolloutsand
offlineRLtodoso. Concurrentwork,HumanoidBench[Sferrazzaetal.,2024],similarlyintroduces
awhole-bodycontrolbenchmarkusingthelessexpressiveUnitreeH1[Unitree,2024]embodiment.
Our contributions differ in two important ways: (1) we develop a method for synthesizing natural
human motions with a highly expressive humanoid model, while Sferrazza et al. [2024] focus on
benchmarkingexistingmethodsforonlineRL,and(2)ourproposedmethodadditionallytakesvi-
sion as input, whereas HumanoidBench considers tasks with privileged state information in their
experiments.
World models (and model-based RL more broadly) are of increasing interest to researchers due
totheirstrongempiricalperformanceinanonlineRLsetting[HaandSchmidhuber,2018,Hafner
et al., 2023, Hansen et al., 2024], as well as their promise of generalization to structurally similar
problem instances [Zhang et al., 2018, Zheng et al., 2022, Lee et al., 2022, LeCun, 2022, Sobal
etal.,2022,Brohanetal.,2023]. Existingmodel-basedRLalgorithmscanbroadlybecategorized
intoalgorithmsthatselectactionsbyplanningwithalearnedworldmodel[Ebertetal.,2018,Schrit-
twieseretal.,2020,Yeetal.,2021,SVetal.,2023,Hansenetal.,2024],andalgorithmsthatinstead
learnamodel-freepolicyusingimaginedrolloutsfromtheworldmodel[Kaiseretal.,2020,Hafner
etal.,2023]. WebuildupontheTD-MPC2[Hansenetal.,2024]worldmodel,whichusesplanning
andhasbeenshowntooutperformotherexistingalgorithmsforcontinuouscontrol[Hansenetal.,
2023a, Lancaster et al., 2023, Feng et al., 2023, Sferrazza et al., 2024]. In this work, we demon-
stratethatplanningiskeytosuccessinthehigh-dimensionalcontinuouscontrolproblemsthatwe
consider.
Hierarchical RL offers a framework for subdividing a complex learning problem into more ap-
proachable subproblems, often by, e.g., leveraging (learned or manually designed) skill primitives
[Pastor et al., 2009, Merel et al., 2017, 2018a, Shi et al., 2022] or facilitating learning over long
time horizons via temporal abstractions [Nachum et al., 2019, LeCun, 2022, Hafner et al., 2022,
Gumbsch et al., 2023, Chen et al., 2024]. Our method, Puppeteer, is also hierarchical in nature,
butdoesnotrelyonskillprimitivesnortemporalabstractionfortasklearning. Instead,welearna
singlelow-levelworldmodelthatcanbereusedacrossavarietyofdownstreamtasks, andinstead
introduceahierarchyintermsofdatasourcesandinputmodalities.
6 Conclusion
Puppeteerisadata-drivenmethodforlearninghighlyperformantvisualcontrolpoliciesforhigh-
dimensional continuous control such as humanoids. We demonstrate that our method consistently
produces motions that are considered natural and human-like by human evaluators compared to
existingmethods,andwefindthatourmethodisabletogeneralizezero-shottogaplengthsthatfar
exceedthetrainingdata. Theseresultsare,tothebestofourknowledge,unprecedentedinthearea
of whole-body humanoid control. However, we acknowledge that our contributions have several
limitationsthatmaynotbeobvious:
• Whileourproposedtasksuiteconsistsofchallengingvisualwhole-bodycontroltaskswitha
detailedhumanoidmodel,tasksprimarilyevaluatethevisio-locomotivecapabilitiesofcurrent
methods. However,humanoidsholdthepromiseofperforminghighlydexteroustasks(such
as, e.g., manipulating and carrying objects in unstructured environments alongside humans)
whichisnotcoveredbyourexistingsetoftasks. Weexpectdevelopmentofnewtaskswillbe
increasinglyimportantasalgorithmscontinuetoimprove,andwehopethatthereleaseofour
benchmarkwillhelpfacilitatethat.
• Our hierarchical approach currently consists of two levels, only one of which is pretrained
in the majority of our experiments. Our experiment on high-level pretraining suggests that
it can be beneficial to pretrain both levels, but we expect this conclusion to vary depending
on the choice of source and target tasks [Hansen et al., 2023b, Xu et al., 2023, Lin et al.,
2023]. Further research on how to pretrain and transfer the full hierarchical world model is
warranted.
9References
BostonDynamics. Atlas,2024. URLwww.bostondynamics.com/atlas.
AnthonyBrohan,NoahBrown,JusticeCarbajal,YevgenChebotar,XiChen,KrzysztofChoromanski,Tianli
Ding,DannyDriess,AvinavaDubey,ChelseaFinn,etal. Rt-2:Vision-language-actionmodelstransferweb
knowledgetoroboticcontrol. arXivpreprintarXiv:2307.15818,2023.
VittorioCaggiano,HuaweiWang,GuillaumeDurandau,MassimoSartori,andVikashKumar. Myosuite–a
contact-richsimulationsuiteformusculoskeletalmotorcontrol,2022.
ChangChen,FeiDeng,KenjiKawaguchi,CaglarGulcehre,andSungjinAhn. Simplehierarchicalplanning
withdiffusion. arXivpreprintarXiv:2401.02644,2024.
XuxinCheng,YandongJi,JunmingChen,RuihanYang,GeYang,andXiaolongWang.Expressivewhole-body
controlforhumanoidrobots. arXivpreprintarXiv:2402.16796,2024.
JackClarkandDarioAmodei. Faultyrewardfunctionsinthewild. OpenAIBlog,2016.
CarnegieMellonUniversityCMU. Carnegiemellonuniversitygraphicslabmotioncapturedatabase, 2003.
URLhttp://mocap.cs.cmu.edu.
FrederikEbert,ChelseaFinn,SudeepDasari,AnnieXie,AlexX.Lee,andSergeyLevine. Visualforesight:
Model-baseddeepreinforcementlearningforvision-basedroboticcontrol. ArXiv,abs/1812.00568,2018.
Yunhai Feng, Nicklas Hansen, Ziyan Xiong, Chandramouli Rajagopalan, and Xiaolong Wang. Finetuning
offlineworldmodelsintherealworld. ConferenceonRobotLearning,2023.
Jean-BastienGrill,FlorianStrub,FlorentAltch’e,CorentinTallec,PierreH.Richemond,ElenaBuchatskaya,
Carl Doersch, Bernardo Ávila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Ko-
ray Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-
supervisedlearning. AdvancesinNeuralInformationProcessingSystems,2020.
J.W.Grizzle,JonathanHurst,BenjaminMorris,Hae-WonPark,andKoushilSreenath. Mabel,anewrobotic
bipedalwalkerandrunner. In2009AmericanControlConference,pages2030–2036,2009. doi: 10.1109/
ACC.2009.5160550.
ChristianGumbsch,NoorSajid,GeorgMartius,andMartinVButz. Learninghierarchicalworldmodelswith
adaptivetemporalabstractionsfromdiscretelatentdynamics. InTheTwelfthInternationalConferenceon
LearningRepresentations,2023.
DavidHaandJürgenSchmidhuber. Recurrentworldmodelsfacilitatepolicyevolution. InAdvancesinNeural
InformationProcessingSystems31,pages2451–2463.CurranAssociates,Inc.,2018.
TuomasHaarnoja,AurickZhou,KristianHartikainen,G.Tucker,SehoonHa,JieTan,VikashKumar,Henry
Zhu,AbhishekGupta,P.Abbeel,andSergeyLevine. Softactor-criticalgorithmsandapplications. ArXiv,
abs/1812.05905,2018.
DanijarHafner,TimothyP.Lillicrap,JimmyBa,andMohammadNorouzi. Dreamtocontrol:Learningbehav-
iorsbylatentimagination. ArXiv,abs/1912.01603,2020.
DanijarHafner,TimothyLillicrap,MohammadNorouzi,andJimmyBa. Masteringatariwithdiscreteworld
models. InternationalConferenceonLearningRepresentations,2021.
Danijar Hafner, Kuang-Huei Lee, Ian Fischer, and Pieter Abbeel. Deep hierarchical planning from pixels.
AdvancesinNeuralInformationProcessingSystems,35:26091–26104,2022.
DanijarHafner,JurgisPasukonis,JimmyBa,andTimothyLillicrap.Masteringdiversedomainsthroughworld
models. arXivpreprintarXiv:2301.04104,2023.
NicklasHansen,XiaolongWang,andHaoSu. Temporaldifferencelearningformodelpredictivecontrol. In
ICML,2022.
NicklasHansen,YixinLin,HaoSu,XiaolongWang,VikashKumar,andAravindRajeswaran. Modem: Ac-
celeratingvisualmodel-basedreinforcementlearningwithdemonstrations. 2023a.
Nicklas Hansen, Zhecheng Yuan, Yanjie Ze, Tongzhou Mu, Aravind Rajeswaran, Hao Su, Huazhe Xu, and
XiaolongWang. Onpre-trainingforvisuo-motorcontrol: Revisitingalearning-from-scratchbaseline. In
InternationalConferenceonMachineLearning(ICML),2023b.
10NicklasHansen,HaoSu,andXiaolongWang.Td-mpc2:Scalable,robustworldmodelsforcontinuouscontrol,
2024.
LeonardHasenclever,FabioPardo,RaiaHadsell,NicolasHeess,andJoshMerel.Comic:Complementarytask
learning&mimicryforreusableskills.InInternationalConferenceonMachineLearning,pages4105–4115.
PMLR,2020.
NicolasHeess,DhruvaTb,SrinivasanSriram,JayLemmon,JoshMerel,GregWayne,YuvalTassa,TomErez,
ZiyuWang,SMEslami,etal. Emergenceoflocomotionbehavioursinrichenvironments. arXivpreprint
arXiv:1707.02286,2017.
Zhengyao Jiang, Yingchen Xu, Nolan Wagener, Yicheng Luo, Michael Janner, Edward Grefenstette, Tim
Rocktäschel, and Yuandong Tian. H-gap: Humanoid control with a generalist planner. arXiv preprint
arXiv:2312.02682,2023.
LukaszKaiser,MohammadBabaeizadeh,PiotrMilos,BlazejOsinski,RoyH.Campbell,K.Czechowski,D.Er-
han,ChelseaFinn,PiotrKozakowski,SergeyLevine,RyanSepassi,G.Tucker,andHenrykMichalewski.
Model-basedreinforcementlearningforatari. ArXiv,abs/1903.00374,2020.
PatrickLancaster,NicklasHansen,AravindRajeswaran,andVikashKumar. Modem-v2: Visuo-motorworld
modelsforreal-worldrobotmanipulation. arXivpreprint,2023.
YannLeCun. Apathtowardsautonomousmachineintelligenceversion0.9.2,2022-06-27. OpenReview,62,
2022.
Kuang-HueiLee,OfirNachum,MengjiaoYang,L.Y.Lee,DanielFreeman,WinnieXu,SergioGuadarrama,
Ian S. Fischer, Eric Jang, Henryk Michalewski, and Igor Mordatch. Multi-game decision transformers.
ArXiv,abs/2205.15241,2022.
ZhongyuLi,XueBinPeng,PieterAbbeel,SergeyLevine,GlenBerseth,andKoushilSreenath. Robustand
versatilebipedaljumpingcontrolthroughreinforcementlearning.InKostasE.Bekris,KrisHauser,SylviaL.
Herbert,andJingjinYu,editors,Robotics:ScienceandSystemsXIX,Daegu,RepublicofKorea,July10-14,
2023,2023.doi:10.15607/RSS.2023.XIX.052.URLhttps://doi.org/10.15607/RSS.2023.XIX.052.
XingyuLin,JohnSo,SashwatMahalingam,FangchenLiu,andPieterAbbeel. Spawnnet:Learninggeneraliz-
ablevisuomotorskillsfrompre-trainednetworks,2023.
JoshMerel, YuvalTassa, DhruvaTB,SriramSrinivasan, JayLemmon, ZiyuWang, GregWayne, andNico-
las Heess. Learning human behaviors from motion capture by adversarial imitation. arXiv preprint
arXiv:1707.02201,2017.
JoshMerel,ArunAhuja,VuPham,SaranTunyasuvunakool,SiqiLiu,DhruvaTirumala,NicolasHeess,and
GregWayne. Hierarchicalvisuomotorcontrolofhumanoids. arXivpreprintarXiv:1811.09656,2018a.
Josh Merel, Leonard Hasenclever, Alexandre Galashov, Arun Ahuja, Vu Pham, Greg Wayne, Yee Whye
Teh, and Nicolas Heess. Neural probabilistic motor primitives for humanoid control. arXiv preprint
arXiv:1811.11711,2018b.
OfirNachum,HaoranTang,XingyuLu,ShixiangGu,HonglakLee,andSergeyLevine. Whydoeshierarchy
(sometimes)worksowellinreinforcementlearning? arXivpreprintarXiv:1909.10618,2019.
PeterPastor,HeikoHoffmann,TamimAsfour,andStefanSchaal. Learningandgeneralizationofmotorskills
bylearningfromdemonstration.In2009IEEEInternationalConferenceonRoboticsandAutomation,pages
763–768,2009. doi:10.1109/ROBOT.2009.5152385.
Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. Deepmimic: Example-guided
deep reinforcement learning of physics-based character skills. ACM Trans. Graph., 37(4):143:1–143:14,
July 2018. ISSN 0730-0301. doi: 10.1145/3197517.3201311. URL http://doi.acm.org/10.1145/
3197517.3201311.
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt,
ArthurGuez,EdwardLockhart,DemisHassabis,ThoreGraepel,etal. Masteringatari,go,chessandshogi
byplanningwithalearnedmodel. Nature,588(7839):604–609,2020.
Carmelo Sferrazza, Dun-Ming Huang, Xingyu Lin, Youngwoon Lee, and Pieter Abbeel. Humanoidbench:
Simulatedhumanoidbenchmarkforwhole-bodylocomotionandmanipulation,2024.
Lucy Xiaoyang Shi, Joseph J. Lim, and Youngwoon Lee. Skill-based model-based reinforcement learning.
2022.
11JoarSkalse,NikolausHowe,DmitriiKrasheninnikov,andDavidKrueger. Definingandcharacterizingreward
gaming. AdvancesinNeuralInformationProcessingSystems,35:9460–9471,2022.
VladSobal,JyothirSV,SiddharthaJalagam,NicolasCarion,KyunghyunCho,andYannLeCun. Jointembed-
dingpredictivearchitecturesfocusonslowfeatures. arXivpreprintarXiv:2211.10831,2022.
R.Sutton. Learningtopredictbythemethodsoftemporaldifferences. MachineLearning,3:9–44,1998.
JyothirSV,SiddharthaJalagam,YannLeCun,andVladSobal. Gradient-basedplanningwithworldmodels.
arXivpreprintarXiv:2312.17227,2023.
YuvalTassa,YotamDoron,AlistairMuldal,TomErez,YazheLi,DiegodeLasCasas,DavidBudden,Abbas
Abdolmaleki,etal. Deepmindcontrolsuite. Technicalreport,DeepMind,2018.
Unitree. H1,2024. URLwww.unitree.com/h1.
Nolan Wagener, Andrey Kolobov, Felipe Vieira Frujeri, Ricky Loynd, Ching-An Cheng, and Matthew
Hausknecht. MoCapAct: A multi-task dataset for simulated humanoid control. In Advances in Neural
InformationProcessingSystems,volume35,pages35418–35431,2022.
GradyWilliams,AndrewAldrich,andEvangelosA.Theodorou. Modelpredictivepathintegralcontrolusing
covariancevariableimportancesampling. ArXiv,abs/1509.01149,2015.
YifanXu, NicklasHansen, ZiruiWang, Yung-ChiehChan, HaoSu, andZhuowenTu. Onthefeasibilityof
cross-tasktransferwithmodel-basedreinforcementlearning. 2023.
WeiruiYe,ShaohuaiLiu,ThanardKurutach,PieterAbbeel,andYangGao.Masteringatarigameswithlimited
data. AdvancesinNeuralInformationProcessingSystems,34:25476–25488,2021.
MarvinZhang,SharadVikram,LauraSmith,P.Abbeel,MatthewJ.Johnson,andSergeyLevine. Solar: Deep
structuredlatentrepresentationsformodel-basedreinforcementlearning. ArXiv,abs/1808.09105,2018.
QinqingZheng,AmyZhang,andAdityaGrover. Onlinedecisiontransformer. InICML,2022.
12A AdditionalQualitativeResults
corridor−→
gaps−→
walls−→
hurdles−→
13
sruO
2CPM-DT
sruO
2CPM-DT
sruO
2CPM-DT
sruO
2CPM-DTgaps−→
stairs−→
walls−→
B ImplementationDetails
MoCap dataset. We use the “small” offline dataset provided by MoCapAct [Wagener et al.,
2022], which is available at https://microsoft.github.io/MoCapAct. This dataset contains
20 noisy expert rollouts from each of 836 expert policies trained to track individual MoCap clips,
totalling(suboptimal)16,720trajectories. Trajectoriesarevariablelengthandarelabelledwiththe
CoMiC[Hasencleveretal.,2020]trackingrewardwhichweusethroughoutthiswork.
Puppeteer.WebaseourimplementationoffofTD-MPC2andusedefaultdesignchoicesandhyper-
parameterswheneverpossible. AllhyperparametersarelistedinTable2. Ourapproachintroduces
onlytwonewhyperparameterscomparedtopriorwork: losscoefficientforterminationprediction
(becauseourtasksuitehasterminationconditions; weaddthistotheTD-MPC2baselineaswell),
andthenumberoflow-levelstepstotakeperhigh-levelstep(temporalabstraction).
TD-MPC2. We use the official implementation available at https://github.com/
nicklashansen/tdmpc2,butmodifytheimplementationtosupportmulti-modalobservationsand
terminationconditionsasdiscussedinSection3.
SAC. We benchmark against the implementation from https://github.com/denisyarats/
pytorch_sacduetoitsstrongperformanceonlower-dimensionalDMControltasksaswellasits
popularityamongthecommunity. Wemodifytheimplementationtosupportearlytermination.
DreamerV3. We use the official implementation available at https://github.com/danijar/
dreamerv3,andusethedefaulthyperparametersrecommendedforlower-dimensionalDMControl
tasks.
14
sruO
2CPM-DT
sruO
2CPM-DT
sruO
2CPM-DTTable2. Listofhyperparameters. Weusethesamehyperparametersacrossalltasks,levels(high-
levelandlow-level),andacrossbothPuppeteerandTD-MPC2whenapplicable. Hyperparameters
uniquetoPuppeteerare highlighted.
Hyperparameter Value
Planning
Horizon(H) 3
Iterations 8
Populationsize 512
Policypriorsamples 24
Numberofelites 64
Temperature 0.5
Low-levelstepsperhigh-levelstep 1
Policyprior
Logstd. min. −10
Logstd. max. 2
Replaybuffer
Capacity 1,000,000
Sampling Uniform
Architecture
Encoderdim 256
MLPdim 512
Latentstatedim 512
Activation LayerNorm+Mish
NumberofQ-functions 5
Optimization
Update-to-dataratio 1
Batchsize 256
Joint-embeddingcoef. 20
Rewardpredictioncoef. 0.1
Valuepredictioncoef. 0.1
Terminationpredictioncoef. 0.1
Temporalcoef. (λ) 0.5
Q-fn. momentumcoef. 0.99
Policypriorentropycoef. 1×10−4
Policypriorlossnorm. Moving(5%,95%)percentiles
Optimizer Adam
Learningrate 3×10−4
Encoderlearningrate 1×10−4
Gradientclipnorm 20
Discountfactor 0.97
Seedsteps 2,500
15C TaskSuite
Weproposeabenchmarkforvisualwhole-bodyhumanoidcontrolbasedonthe“CMUHumanoid”
model from DMControl [Tassa et al., 2018]. Our simulated humanoid has 56 fully controllable
joints (A ∈ R56), and includes both head, hands, and feet. Our task suite consists of 5 vision-
conditionedwhole-bodylocomotiontasks(corridor,hurdles,walls,gaps,stairs),aswellas3tasks
thatuseproprioceptiveinformationonly(stand,walk,run). All8tasksareillustratedinFigure4.
Observationsalwaysincludeproprioceptiveinformation,aswellaseithervisualinputs(high-level
agent) or a command (low-level agent). The proprioceptive state vector is 212-dimensional and
consists of relative joint positions and velocities, body velocimeter and accelerometer, gyro, joint
torques, binary touch (contact) sensors, and orientation relative to world z-axis. Visual inputs are
64×64RGBimagescapturedbyathird-personcamera(asseeninFigure4), andcommandsare
15-dimensionalvectorswithvaluesin[−1,1].
D UserStudy
To compare the “naturalness” of policies learned by our method vs. TD-MPC2, we design a user
studyinwhichhumansareaskedtowatchshort(∼10s)pairsofclipsofsimulatedhumanoidmotions
generatedforeachofour5visualwhole-bodyhumanoidcontroltasks. Eachuserispresentedwith
2suchpairspertask,totalling10pairsperuser. Theconcreteinstructionsprovidedtousersareas
follows:
Instructions
Inthisstudy,youwillwatchpairsofshort(10seconds)clipsofsimulatedhumanoidmotions. For
eachpair,youareaskedtodeterminewhichofthetwoclipsappearmore"natural"and"human-like"
toyou,i.e.,whichcliplooksmorelikethebehaviorofarealhuman.
Usersarethenprovidedwitheachofthe10pairsofclips,andpromptedtoanswerquestionsofthe
form:
Q1: Whichofthefollowingtwomotionsappearmore"natural"and"human-like"?
1. ←LEFTismorenatural
2. →RIGHTismorenatural
3. LEFTandRIGHTareequallynatural
The order of clips is selected at random for each pair. Aggregate results from the user study are
provided in Table 3, and Figure 10 shows a sample question from the user study. Participants
are sourced from undergraduate and graduate student populations across multiple universities and
disciplines. Wedonotcollectpersonalorotherwiseidentifiableinformationaboutparticipants,and
allparticipantshaveprovidedwrittenconsenttouseoftheirresponsesforthepurposesofthisstudy.
16Figure10. Screenshotofaquestionfromtheuserstudy. Usersareshowntwoclipsside-by-side
andareaskedtoprovidetheirpreference.
Table 3. Results from the user study. We summarize results from our user study (n = 46) be-
low by reporting per-pair aggregate numbers. Higher is better ↑. Clips generated by our method,
Puppeteer,areconsideredmorenaturalbyasuper-majorityofparticipants.
Pair TD-MPC2 Equal Ours
Corridor
Pair1 0 0 46
Pair2 0 2 44
Hurdles
Pair1 0 0 46
Pair2 0 0 46
Walls
Pair1 1 2 43
Pair2 0 0 46
Gaps
Pair1 0 0 46
Pair2 0 0 46
Stairs
Pair1 0 2 44
Pair2 1 3 42
Aggregate 0.4% 2.0% 97.6%
17