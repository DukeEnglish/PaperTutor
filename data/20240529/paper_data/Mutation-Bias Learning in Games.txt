Mutation-Bias Learning in Games
JohannBauer˚ SheldonWest
Dept. ofMathematics Dept. ofComputerScience
City,UniversityofLondon,UK City,UniversityofLondon,UK
EduardoAlonso MarkBroom
Dept. ofComputerScience Dept. ofMathematics
City,UniversityofLondon,UK City,UniversityofLondon,UK
Abstract
Wepresenttwovariantsofamulti-agentreinforcementlearningalgorithmbased
onevolutionarygametheoreticconsiderations. Theintentionalsimplicityofone
variantenablesustoproveresultsonitsrelationshiptoasystemofordinarydif-
ferential equations of replicator-mutator dynamics type, allowing us to present
proofsonthealgorithm’sconvergenceconditionsinvarioussettingsviaitsODE
counterpart. The more complicated variant enables comparisons to Q-learning
basedalgorithms. WecomparebothvariantsexperimentallytoWoLF-PHCand
frequency-adjustedQ-learningonarangeofsettings,illustratingcasesofincreas-
ingdimensionalitywhereourvariantspreserveconvergenceincontrasttomore
complicatedalgorithms. Theavailabilityofanalyticresultsprovidesadegreeof
transferabilityofresultsascomparedtopurelyempiricalcasestudies,illustrating
thegeneralutilityofadynamicalsystemsperspectiveonmulti-agentreinforcement
learningwhenaddressingquestionsofconvergenceandreliablegeneralisation.
1 Introduction
Reinforcementlearningalgorithmshavebeenemployedinawiderangeofproblemsettingswithgreat
success,e.g.,[25],andforthesingle-agentcasetheconditionsforconvergenceof,e.g.,Q-learning
havebeenclarified,[28]. However,formulti-agentreinforcementlearning(MARL),questionsof
convergencearestillverymuchopen. Evensimpletwo-playersettings,e.g. theRock-Paper-Scissors
(RPS)game,canexhibitchaoticbehaviourundersimpledynamics,[23],andmakearigorousapriori
analysischallenging. Formorecomplicatedalgorithms,ananalysisbeyondexperimentalevaluation
isoftenhardlypossible. However,moregeneralanalysesarehighlyinformativeofwhyalgorithms
behaveinacertainwayandtheoreticalguaranteesforat leastthesimplestofsettingsarehighly
desirableinordertoassesshowreliablyMARLalgorithmswillgeneralisetosimilarsettings.
Inparticular,asMARLalgorithmsoftenleadtostochasticdiscrete-timedynamicsystems,insights
fromthefieldsoflearningdynamicsingamesandofevolutionarygametheory(EGT)havebeen
particularly relevant. EGT approaches and specifically the established replicator dynamics (RD)
haveinformedanumberofconstructionsoranalysesoflearningalgorithmsinmulti-agentsettings,
e.g., [15,17]. ThepotentialofEGTtoinformlearningalgorithmsisillustrated, asaparticularly
prominentexample,bythefactthattheWoLF-PHClearningalgorithm,[3],keepstrackofthepast
averagepolicy. InlightofRD,thisisparticularlyuseful,asthetime-averagepolicyinRDconverges
toaNashequilibriumunderself-playinzero-sumgames,e.g,[29,prop. 3.6,p. 92],providingan
intuitionforhowWoLF-PHCcanlearnNashequilibriainself-playinanumberofsettings.
˚{first name}.{last name}@city.ac.uk
4202
yaM
82
]GL.sc[
1v09181.5042:viXraContribution
Building on the relation between RD and a simple form of reinforcement learning, called Cross
learning[2,6],weformulatetwovariantsofanewreinforcementlearningalgorithm: Mutation-bias
learningwithdirectpolicyupdates(MBL-DPU)–aleastcomplexitymodificationofCrosslearning–
andmutation-biaslearningwithlogisticchoice(MBL-LC).Explicitlytakingintoaccountthestochas-
ticity of the problem, we prove that MBL-DPU can be approximated by a mutation-perturbed
replicatordynamics(RMD),specifiedin[1],anon-lineardynamicswhosestabilitypropertiescan
stillbestudiedanalyticallytoacertaindegree. AlthoughtheLyapunovstabilityandotherproper-
tiesofthecontinuous-timecasedonotalwaystransfertothediscrete-timelearningdynamics—a
prominentexampleistheRPSgame,[29]—weshowthatasymptoticstabilityinthecontinuouscase
doesimplytheconvergenceoftheMARLalgorithm. SimpleRDcannothaveasymptoticallystable
interioreuqilibria,e.g. [20,lemma1]. Hence,Crosslearningisunabletolearninteriorequilibria
and will quickly deviate from RD in cases of merely neutral stability, such as in RPS games. In
contrasttoRDandCrosslearning,RMDallowsinteriorequilibriatobeasymptoticallystable,[1],
enablingtheproposedMBLalgorithmtoovercomethisfundamentallimitationofCrosslearningand
approachinteriorNashequilibriaarbitrarilyclosely. Hence,wecanshowthatinthecaseofglobally
asymptoticallystableequilibria,MBLprocessesrevisitarbitraryneighbourhoodsofsuchequilibria
infinitely often almost surely, particularly in zero-sum games. In contrast to more complicated
algorithms,thesimplicityofMBLallowsananalyticapproachtothequestionofconvergenceof
MBLtoanε-equilibriuminagivengameapriori—beitzero-sumornot—andfurtherunderstanding
whenconvergenceshouldnotbeexpected,irrespectiveofparameterchoices.Toourknowledge,MBL
isamongthesimplestuncoupled,inthesenseof[3,7],algorithmsthatcanlearninteriorequilibria
andamongthefewsuchforwhichamoregeneralrigorousdynamicsystemanalysisisavailable.
Therestofthispaperproceedsasfollows: Afterrelatingourresultstotheliterature,westatethe
necessaryevolutionarygametheoreticpreliminaries. WethenintroducethetwoMBLvariants,MBL-
DPUandMBL-LC,whichdemonstratesanalternativeapproachtoincludethemutationperturbation
termclosertoQ-learninginspiredapproaches,andstatethepropositionsontherelationofMBL-DPU
toRMDandtheconvergencepropertiesofMBL-DPU.Wethenillustratethetheoreticalresultswith
numericalexperimentsinarangeoftwo-playergames,aswellasathree-playergame,andcompare
thebehavioursofthetwoMBLvariantstothoseoffrequency-adjustedQ-learning(FAQ),[10],and
Win-or-Learn-FastPolicy-Hill-Climbing(WoLF-PHC),[3],demonstratingtheutilityofarigorous
dynamicsystemanalysisinthestudyofMARLalgorithms.
Relatedwork
Alargerclassofstochasticreinforcementlearningrulesisrelatedtodeterministiccontinuous-time
systemsofRDtypein[21]. SystemsofRDtypewithadditionalperturbationshavebeenrelatedto
variouslearningrules,includingsuchwithentropyrelatedperturbationterms,[22],andexponential
learningbasedonalogitmodel,[14]. SomeanalysesfocusspecificallyonQ-learningbasedlearning
algorithms. Forinstance,[11]considersthestabilityandconvergencepropertiesofQ-learningin
thetwo-playersetting;however,theQ-valuesenterasexpectations,notasrandomvariables,and
thereforetheeffectsofstochasticityarenotconsidered—acrucialfactorinarigorousanalysis. A
similar approach is pursued by the frequency-adjusted Q-learning algorithm (FAQ) in [27] with
a corrected derivation given in [10]. However, both strands start from assumptions which have
notbeenproved,andthereforenotheoreticalguaranteescanbeinferred. Nonetheless,wechoose
FAQ-learning as a comparison, as [10] claims it to be linked to an ODE system similar to RMD
andasitisasufficientlysimpleuncoupledalgorithmveryclosetoQ-learning,makingitanatural
candidateforcomparison. Asasecondcandidateforcomparison,wechooseWoLF-PHC,[3],since
itsvariantWoLF-IGAisstronglylinkedtoadynamicsystemsperspectiveandWoLF-PHC,too,is
anuncoupledandrelativelysimplealgorithm,closetoQ-learning. Althoughitstheoreticalanalysis
ismorethoroughthanforFAQ,onlythetwo-playertwo-actionanalysisofWoLF-IGAisavailable.
BothalgorithmshavedemonstratedthattheyareabletolearnNashequilibriainsimplesettingsunder
self-play,wheresimpleralgorithmssuchasPolicy-Hill-Climbingwouldfail.
AseparateapproachtoMARLconvergenceanalysisispursuedviamultipletimescalesalgorithms,
whereQ-valueestimatesarelearnedquickerthanpolicychangesoccur,e.g.,[5]. Here,theconver-
genceanalysisrelatestosmoothedbest-responsedynamics. However,thetimescaleseparationresults
inafundamentallymorecomplicatedapproachandmorecomplicatedalgorithms. Forthecaseof
ε-greedymulti-agentQ-learningunderstochasticpayoffs,convergenceconditionsaregivenin[4].
2However,thisalgorithmoperatesonjointactions,whichrequiresagentstobeabletoobservethe
actionschosenbyallagents,andisthereforenotuncoupledinthesenseof[3].
Wedonottakeintoaccountproximalpolicyoptimization(PPO)algorithms,[24],forourcomparison,
sincetheyrequireanagenttoconstructanapproximationoftheactualtargetfunctionandsolvea
constrainedoptimisationproblemateachlearningstepwithasuitablesamplingstrategyin-between
learning and to keep track of a potentially large number of estimates. This results in a much
morecomplicatedalgorithmthananalysedhereandconvergenceanalysiseveninthesingle-agent
settingischallenging, e.g., [12]. WearenotawareofarigorousMARLconvergenceanalysisin
non-cooperativegames,althoughexperimentalresultsinthisdirectionexist,e.g.,[13]forn-player
RPSgameswithconvergenceonlyinverylimitedcases,or[19]extendingPPOtoWoLF-PPOin
experimentalstudiesofMatchingPenniesandtwo-playerRPS.
2 Preliminaries
Asouranalysisofmulti-agentlearningisformulatedinthesettingof(evolutionary)gametheory,we
giveshortdefinitionsofthemainconceptsemployedandreferthereadertothestandardliteraturefor
details[e.g.,8,29].
Finite normal-form games. A normal-form game is a tuple pP,A,rq, where P “ t1,...,Nu
representsthesetofplayers,A “ ˆ A whereA “ t1,...,n uisthesetofpurestrategiesof
iPP i i i
eachplayeri,2andr “pr q isafamilyoffunctionswithr :AÑRmappingthepurestrategy
i iPP i
profilesinAtothepayoffsofplayeri. ForeachplayeriPP,weassumethattheplayerchoosesa
purestrategyfromA iaccordingtosomřeprobabilitydistributionx ioverA i,i.e.,accordingtosome
tuplepx q PD :“tξ PRAi : ξ “1u. Wecallsuchanx themixedstrategyofplayer
ih hPAi i ě0 h h i
i.3 Wewillcallmixedstrategiessimplystrategies,wherethereisnodangerofconfusion.
Nashequilibrium. Wecallastrategyprofilex˚ :“px˚q PD :“ˆ D aNashequilibrium
i iPP iPP i
ifforallplayersiPP andallmixedstrategiesx PD ztx˚u,wehave
i i i
Err paq|x˚sěErr paq|px ,x˚ qs (2.1)
i i i ´i
wherepx ,x˚ qPDdenotesthemixedstrategyprofileforwhichpx ,x˚ q “x (@hPA )and
i ´i i ´i ih ih i
px ,x˚ q “x˚ (@j PPztiu,hPA ). TheequilibriumiscalledastrictNashequilibriumifthe
i ´i jh jh j
inequalityisstrictforalli P P. Thewell-knownintuitionofthisconceptisthatnoplayerhasan
incentivetodeviatefromtheNashequilibriumstrategygiventhatallotherplayersplaytheNash
equilibriumstrategyprofile,sinceforeachplayeriPP,x˚isabest-responsetox˚. Equivalently,
i
nopurestrategyhasahigherpayoffthantheNashequilibriumstrategy:
@iPP,hPA : Err paq|x˚sěErr paq|x˚,a “hs. (2.2)
i i i i
Asausefulrelaxationofthisconcept,wecallastrategyprofilepx˜ q PDanε-equilibriumif
i iPP
Dεą0@iPP,hPA : Err paq|x˜sěErr paq|x˜,a “hs´ε, (2.3)
i i i i
i.e. everypurestrategyisbyatmostεbetterthanpx˜ q ,andforallplayersi P P,px˜ q isan
i iPP i iPP
ε-best-responsetox˜.
Repeated games, learning and rationality. Given a finite normal-form game, we consider an
infinitely repeated game to be a repetition of the normal-form game for each round t P N. In
particular,assumingthatineachroundttheplayerschooseapurestrategyprofileaptqaccordingto
themixedstrategyprofilexptq“px ptqq ,thesepurestrategyprofilesdefineastochasticprocess
i iPP
taptqu tPN. Inturn,analgorithmwhichadaptsthemixedstrategyprofileineachroundt,definesa
potentiallystochasticprocesstxptqu tPN. Itisthisresultingprocessanditspropertieswhicharethe
focusofourconvergenceanalysis. Followingthedefinitiongivenby[3], wecallsuchaprocess
rational,ifaplayeri’smixedstrategytx iptqu tPN convergestoabest-responsewheneverallother
players’strategiesconvergetoastationarypolicy. Wecallaprocessε-rationalifitconvergesto
anε-best-response. Itisclearthatinthecaseofstationarypoliciesforallotherplayers,thefocal
2AisusuallydenotedS inthegametheoryliterature,andplayersareconceivedaspopulationsofpure
strategiesintheEGTliterature.Inthesimplestcase,purestrategiescorrespondtoactionsinthereinforcement
learningliterature.Weusetheterms‘player’and‘agent’synonymously.
3Thiswouldbereferredtoasapolicyinthereinforcementlearningliterature.
3playerfacesaMarkovdecisionprocessandthebest-responsestrategymaximisestheplayer’saverage
expectedpayoff. Inthesimplestcase,whereplayerscannotobserveotherplayers’actionsandhave
nomemory,asconsideredhere,theusualstatespaceandthestate-dependencyofpoliciesdisappear.
Replicator-mutatordynamics. Weconsiderthemulti-populationreplicator-mutatordynamics
formulated in [1], which is a special case of general replicator-mutator dynamics [e.g., 18]: For
alli P P,letM ą 0beamutationparameter,c P D ˝ (denotingtheinteriorofD )somefixed
i i i i
parameter and f
i
: D Ñ RAi a continuously differentiable fitness function. Then the replicator-
mutatordynamicsisgivenforiPP,hPA by
` ř i ˘
x9 ptq“x ptq f pxptqq´ x ptqf pxptqq `M pc ´x ptqq. (RMD)
ih ih ih k ik ik i ih ih
IncasethatM “0foralliPP,RMDreducestothestandardmulti-populationreplicatordynamics
i
(RD).Onepossible(andusual)conceptualisationofthefitnessofapurestrategyhPA istoassume
i
thatitistheexpectedpayoffofplayingh, givenallotherplayers’strategies, ormoreconcretely,
givenastrategyprofilexPDletthefitnessf satisfyf pxq“Err paq|x,a “hs. Itisclearthat
ih ih i i
allfitnessfunctionsarecontinuouslydifferentiableinthiscase.
Remark. TheequilibriaofRMD,alsocalledmutationequilibria,ingeneralarenotNashequilibria
oftheunderlyinggame. Instead,theyareε-equilibria,whereεdependsonpM q asshownin[1].
i iPP
3 Mutation-biaslearning
WecannowintroducethestochasticlearningrulesandspecifytheirrelationtoRMD.Weprovide
twovariantsofMBL:one, basedondirectpolicyupdates(MBL-DPU,alg. 1)–wherethepolicy
updatecorrespondstoCrosslearning, [6], withamutationbiasasaperturbationterm; theother,
basedonlogisticchoice(MBL-LC,alg. 2)–wherethepolicycorrespondstologisticchoicebasedon
action-valueestimateswhichareupdatedwithamutationbiasperturbation.
Algorithm1(MBL-DPU)MBLwithdirectpolicyupdateforgenericplayeriPP
1: Initialise: Chooselearningrateθ,mutationparametersM i ą0andc i PD i˝,initialx i PD i.
2: foralltimestdo
3: Selectstrategya i PA iwithprobabilitiesPrpa i “hq“x ih(@hPA i).
4: Observepayoffr iresulting "fromstrategyprofilepa jq jPP.
x `θp1´x qr `θM pc ´x q ifh“a ,
5: ForallhPA i,set: x ih Ð xih ´θx r `ih θMi pc ´i xih q ih otherwisi e.
ih ih i i ih ih
6: endfor
MBLwithdirectpolicyupdate(MBL-DPU). MBL-DPU,alg.1,isthesimplerofthetwovariants
withadirectpolicyupdateandnoestimationofQ-values.ItisanadditivelinearperturbationofCross
learningwithperturbationtermθM pc ´x q,line5,andbecomesidenticaltoCrosslearning,
i ih ih
[2,6], forM “ 0(@i P P). Inthissenseitcanbesaidtobealeastcomplexitymodificationof
i
Crosslearning,sinceonlyfewelementarycomputationsarerequiredinadditiontosimpleCross
learning. WenotethattheassumptioninCrosslearning,thatthepayoffsr berestrictedtor0,1sis
i
notnecessary. Itsufficesthatpayoffsarenon-negativeandbounded. Inthiscase,θhastobechosen
smallenoughtoensurewell-definitionofMBL-DPU.Notethatthisassumptionisnotrestrictivefor
finitegames,asboundednessistriviallysatisfiedforfinitegamesandnon-negativitycanbeensured
byaddingaconstantC toallpayoffsr ,affectingneithertheNashequilibrianorthedynamicsin
i i
thedeterministiclimit—astraightforwardpropertyofRDandRMD.
MBL with logistic choice (MBL-LC). Clearly, the simple perturbation in MBL-DPU can be
combinedwithawideclassoftransformationsonthepayoffswithoutaffectingtheadditivecharacter
oftheperturbation. Asomewhatmoreinvolvedpossibilitytocombinethemutation-likeperturbation
withapolicyupdateisbasedonaBoltzmanndistributionormultinomiallogisticchoice,asfrequently
encounteredinQ-learning. InMBL-LC,alg. 2,theperturbationaffectstheaction-valueupdates
insteadofthepolicy. Hence,thisversionmorecloselyresemblesthealgorithmsanalysedin[10,11],
andallowsaclosercomparisontoFAQ.Inparticular,restrictingtheadjustmentinline6byapplying
aminimumisparallelFAQ.Onecanseethatthelogisticchoicepolicycanstillbeexpressedasa
4Algorithm2(MBL-LC)MBLwithlogisticchoiceforgenericplayeriPP
1: Initialise: Chooselearningrateθ,M i ą0andc i PD i˝,Q i PRAi. Chooseβ ą0,τ ą0.
2: foralltimestdo
3: ForallhPA i,set: x ih Ð ř kPe Aτ iQ eih τQik.
4: Selectstrategya i PA iwithprobabilitiesPrpa i “hq“x ih(@hPA i).
5: Observepayoffr iresultingfromstrat!egypro)file´pa jq jPP. ¯
6: Forh“a i,set: Q ih ÐQ ih`min xβ ih,1 θ r i`M ixci ih
h
.
7: endfor
policyupdatewithmodifiedpayoffs:
"
x ih Ð xx ii hh ´`p x1 ih´ r˜ ix ihqr˜ i oif thh e“ rwa isi e, , with r˜ i “ xix ai ia pi ep τe ∆τ∆ QQ iai ia ´i´ 1q1 `q 1, (3.1)
whereQdenotesanaction-valuefunctionand∆Q denotestheupdateoftheaction-valueofthe
iai
chosen action a . From this it is clear that an intermediate approach could be using the simpler
i
MBL-DPUcombinedwithQ-learning,whichisequivalenttotransformingpayoffsaccordingly.
ConvergenceofMBL-DPU
Weaddressthequestionofconvergenceintwosteps. First,wedeterminewhetherthestochastic
processinducedbythelearningalgorithmcanbeapproximatedbyadeterministicdynamics. Second,
wetransfertheconvergencepropertiesofthedeterministicdynamicstothestochasticprocess. For
MBL-DPUwehavethefollowingconvergenceresult(provedinappendixA):
Proposition3.1. ForeverytimeT ă8,thefamilyofstochasticprocessestpXθ ptqq u induced
ih i,h tě0
byMBL-DPUconvergestoRMDinthesensethatforallεą0:
sup Prp}Xθpn q´Φpxp0q,Tq}ąεqÑ0 as θ Ñ0, (3.2)
xp0q θ
wheren θ ÑT forθ Ñ0,xp0qisa.s. theinitialstateofthestochasticprocessesandΦpxp0q,¨qis
θ
theuniquesolutionofRMDwithΦpxp0q,0q“xp0q.
Remark. As discussed in [2, 16], proposition 3.1 on its own does not yield an analysis of the
asymptoticbehaviourofthestochasticprocess. However,ifamutationequilibriumxM ofRMDis
asymptoticallystableandxp0qliesinthebasinofattractionofxM,thenwehaveΦpxp0q,TqÑxM
asT Ñ8. Hence,withtheasymptoticstabilityofxM,wehavethatforT largeenough,Φpxp0q,Tq
isarbitrarilyclosetoxM andtogetherwithproposition3.1,anyneighbourhoodofxM willbereached
bythelearningprocesstXθptqu withanarbitrarydegreeofcertaintyafterfinitelymanystepsfor
tě0
suitablechoiceofθ. Althoughthisdoesnotimplythattheprocessmustremaininthisneighbourhood
afterwards,itwillrevisittheneighbourhoodwitharbitraryprobabilitydependingonθ.
Attractingmutationlimits. In[1]itwasshownthateverygamehasatleastoneconnectedNash
equilibriumcomponentthatisapproximatedbymutationequilibriairrespectiveofthechoiceofthe
mutationparameterc,asM Ñ 0,calledamutationlimit. Furthermore,itwasshownthatforthe
gameofMatchingPenniestheNashequilibriumisapproximatedbyasymptoticallystablemutation
equilibria,warrantingthenameattractingmutationlimitforsuchNashequilibria. Thisimpliesthe
followingconsequence(provedinappendixA):
Proposition 3.2. If aunique Nash equilibrium x˚ P D˝ is anattractingmutation limit and U a
neighbourhoodofx˚,thenforeverymutationparametercPD˝thereareM ą0,θ ą0suchthat
thestochasticprocesstpXθptqqu tPN
0
inducedbyMBL-DPUvisitsU atafinitetimea.s.,i.e.,with
probability1thereisS PN 0withXθpSqPU. Infact,tpXθptqqu tPN
0
a.s. visitsU infinitelyoften.
IncontrasttoMBL-DPU,wedonothaveaproofofananalogousresultforMBL-LC,yet. In[10,11]
itisassumedthatFAQ,asimilarlogisticchoicelearningrulebasedonQ-learning,convergestoa
perturbationofthereplicatordynamics,albeitnoproofisgiven. Althoughitseemsplausiblefor
MBL-LCtobehavesimilarlytoMBL-DPU,theexperimentalresultsindicatethatMBL-LCislikely
moresensitivetothechoiceoflearningratethanMBL-DPU,sincethelogisticchoicecancausea
strongervarianceofthestrategyateachlearningstep,asindicatedinthemoredetailedresultsfor
MBL-LCinappendixB.Thelargervarianceinthelearningstepisalsothereasonwhyourproof
strategyisconsiderablymorechallengingforMBL-LC.
5Perturbationcreatesatrade-offbetweenaccuracyandspeed. WenotethatneitherMBL-DPU
norMBL-LCconvergetoaNashequilibriumbutonlytoanε-equilibriumandinparticular,thatboth
stayawayfromtheboundaryofD. ForMBL-DPUthisisclearfromthefactthattheequilibriaof
RMDarenotNashequilibriaandthattheboundaryofDisrepelling. ForMBL-LCthisisalsodue
totheexplorationparameterτ. Forthelatter,itisfurtherthecasethatτ cannotbelettoapproach
8 as this collides with the θ Ñ 0 limit and makes the time derivative of the policy unbounded.
Thisresultsinahighlyincreasedvarianceinthestochasticprocess,preventingeffectivelearningof
equilibria. Thisparticularaspectappliesalsotootherlogisticchoicebasedalgorithms,particularly
FAQ. However, if MBL-LC and FAQ indeed converge to the corresponding ODE systems, then
these include τ as a simple scaling parameter. Since constant positive rescalings do not change
thetrajectories,thesystemscanberescaledby1{τ insuchawaythatτ effectivelyregulatesthe
perturbation’sstrengthrelativetothereplicatordynamics. InthecaseofRMD,1{τ canbeabsorbed
bythemutationstrengthM. Thusanincreaseofτ hasthesameeffectasadecreaseofM which
resultsinallmutationequilibriamovingclosertoaNashequilibrium,asdesired. Areductioninthe
perturbationstrengthalsoresultsinalongertimetoapproachequilibriaandthiscreatesatrade-off
betweenaccuracyandspeedforbothMBL-LCandMBL-DPU.
4 Experimentalresults
Weillustratethetheoreticalresultsinanumberofexperimentalsettings: thePrisoner’sDilemma
(PD),MatchingPennies(MP),Rock-Paper-Scissors(RPS)with3,5and9availablestrategies,and
thethree-playerMatchingPennies(3MP)games. WecompareMBL-DPUandMBL-LCtoFAQ,
[10],andWoLF-PHC,[3]. Fordetailsonthegames’payoffsandfurtherexperiments,cf. appendixB.
Prisoner’sDilemma(PD). PDisanexampleofagamewithastrictNashequilibriumatavertex
ofthejointstrategyspaceD. ItisknownthatstrictNashequilibriaareasymptoticallystableunder
RD,e.g.,[29]. Inthiscase,plainCrosslearningwouldalsoconvergetotheNashequilibrium. Itwas
shownthatRMDdoesnotdestabiliseasymptoticallystableequilibriaofRD[1,lemma4.8]. Hence,
themutationequilibriumresultingfromthemutationperturbationremainsasymptoticallystableand,
withourresult,MBL-DPUalsolearnsanapproximationoftheNashequilibrium. Inthissense,PDis
theleastchallengingsettingintermsoftheeasewithwhichtheNashequilibriumcanbelearned. The
settingservesmainlytoillustratethefactthatthelearnedequilibriaofMBL-DPUandMBL-LCin
factlieawayfromtheboundaryNashequilibrium,inparticularsincemutationpushesthetrajectories
awayfromtheboundaryofD,incontrasttotheothertwoalgorithms. Withdecreasingmutation
strengthM,bothalgorithmsareabletobetterapproachtheNashequilibrium,aswouldbeexpected
fromRMD.ThiscasealsoillustratesthatthemoreelementaryMBL-DPUconvergesmoreslowly
thaneitherofMBL-LC,FAQ,orWoLF-PHC.Formoredetailsandfiguresonthisbenigncase,we
referthereadertoappendixB.1.
Zero-sumgames—MatchingPennies(MP). Asasecond,structurallydifferentcase,weconsider
zero-sumgameswhichhaveinteriorNashequilibria. Forthegamesconsideredhereitisstraight-
forwardtocheckthattheeigenvaluesoftheJacobianofRMDintheneighbourhoodoftheNash
equilibriumonlyhavenegativerealparts. Equivalently,onecancheckthattheeigenvaluesofthe
JacobianofRDarepurelyimaginaryintheneighbourhoodoftheNashequilibriumandconsider
thatRMDshiftstheeigenvaluestowardsthenegativehalf-plane,renderingtheNashequilibrium
anattractingmutationlimit. Withpropositions3.1and3.2,respectively,MBL-DPUisguaranteed
toconvergeinthesespecificcases,4 withageneralresultonconvergenceandstabilityofRMDin
zero-sumsettingsinpreparation. Infact,weobserveconvergenceintheMPsettingforMBL-DPU,
MBL-LC,aswellasourcomparisons,FAQlearningandWoLF-PHC,fig. 1. Thissettingillustrates
thatMBL-DPUovercomesthelimitationsofCrosslearningataminimalcostinincreasedcomplexity.
SimilartothePDsetting,MBL-DPUconvergesmoreslowlythanthemorecomplicatedalgorithms,
MBL-LC,FAQ,orWoLF-PHC.WithMPbeingaplanarsystemandthePoincaré-Bendixsontheorem,
thecomplexityofthesystemisstillrelativelysmall.
Zero-sumgames—Rock-Paper-Scissors(RPS). Forthehigherdimensionalsettings,i.e.,RPS
with3,5and9strategies,westillobserveconvergenceforMBL-DPU,fig. 2,asguaranteedbythe
4Inmorecomplexcaseswithmultipleequilibria,convergencedependsontheinitialstatelyinginthebasin
ofattractionofanequilibrium.
6Figure 1: Self-play on the
MP game; for 10 different
initial conditions. Each sub-
figure shows the ten trajecto-
riesintheprojectionontothe
first components of the play-
ers’strategies,inthiscasethe
‘defect’strategy,withthefirst
player on the horizontal axis
andthesecondonthevertical
(a)MBL-DPUwithM´1 “20. (b)MBL-LCwithM´1 “τ “20. axis. Points coloured yellow
correspondtoearlierpointsin
time, changing over orange
and violet to black for later
pointsintime. Thepositionof
the game’s Nash equilibrium
ismarkedwithabluecrossin
theprojectionplane.
(c)FAQwithτ “20. (d)WoLF-PHCwithinitiallearning
rate 10´1 for Q, win learning rate
1{2¨10´4.
(a)MBL-DPUonRPS-3. (b)MBL-DPUonRPS-5. (c)MBL-DPUonRPS-9.
Figure2: Self-playofMBL-DPUonRPS-3,RPS-5andRPS-9games,withM´1 “20.
Nashequilibriumbeinganattractingmutationlimit. Naturally,thetrajectoriesoftheresulting4,
8and16dimensionalsystemsappearlessintuitiveinthe2D-projection. ForMBL-LC,fig. 3,and
FAQ,fig. 4,weobserveconvergenceintheRPS-3case,butbothalgorithmsdeteriorateinhigher
dimensions,MBL-LCforRPS-9,fig. 3c,andFAQforRPS-5andRPS-9,figs. 4band4c,withboth
showingtheconvergenceregionsplittingupsuchthatsometrajectoriesstopapproximatingtheNash
equilibrium. Similarly,whileWoLF-PHCseemstoapproachtheNashequilibriuminRPS-3and
RPS-5,fig. 5,itlosestheabilitytolearntheNashequilibriumforRPS-9,fig. 5c,withtrajectories
seeminglygettingstuckneartheboundaryofD.
Three-playerMatchingPennies. Beyondthetwo-playercase,wecompareMBLinathree-player
MatchingPenniessettingintroducedin[9]. Inshort,thethreeplayershaveasharedpurestrategy
space,i.e. A “A “A ,withtwopurestrategies,whereplayer1wantstomatchplayer2,player
1 2 3
2wantstomatchplayer3,andplayer3wantsnottomatchplayer1. TheuniqueNashequilibrium
liesatthecenterofD. AllfouralgorithmsfailtolearntheNashequilibrium,fig. 6(MBL-LCnot
shown,cf. appendixB.3). Instead,theyseemtoapproachaseeminglystableperiodicorbit.
7(a)MBL-LConRPS-3. (b)MBL-LConRPS-5. (c)MBL-LConRPS-9.
Figure3: Self-playofMBL-LConRPS-3,RPS-5andRPS-9games,withM´1 “τ “20.
(a)FAQonRPS-3. (b)FAQonRPS-5. (c)FAQonRPS-9.
Figure4: Self-playofFAQ-learningonRPS-3,RPS-5andRPS-9games,withτ “20.
5 Discussion
Theexperimentalresultsillustratethedifficultiesinrelyingonexperimentalresultsalone. WoLF-
PHC,FAQandMBL-LCallshowquickerconvergenceinthosecaseswheretheyactuallydoconverge
andtheywouldseemthebetterchoicethanMBL-DPU.Notsurprisingly,thisisthecaseinPD,which
hasastrictNashequilibrium,andinMPwhichisaplanarsystemandcannotexhibittoocomplex
behaviours. However, weseethatbehavioursstartbecominglessclearwhenwemovetohigher
dimensionsintheRPSvariants. WhileallalgorithmsseemtoapproximatetheNashequilibrium
inRPS-3,weseeunexpectedbehaviourinRPS-5forFAQwithasplitupconvergenceregion. In
RPS-9 we see FAQ deteriorate further and MBL-LC now also failing to converge with a split in
theconvergenceregions. WoLF-PHCnowtoofailstolearntheNashequilibrium,withtrajectories
stallingorgettingstuckneartheboundary. InRPS-9noalgorithmexceptforMBL-DPU–thesimplest
amongthefour–managestoreliablyapproachtheNashequilibrium. Thislossofconvergencefor
themorecomplexalgorithmsisunexpected,sinceRPS-9doesnotfundamentallydifferfromRPS-3
inthegamestructureandthefailuretolearnwhenmovingfromRPS-3toRPS-9wouldbehardto
anticipateapriori. Incontrast,withtheresultsonMBL-DPUwehaveanindicationofhowwellit
willgeneralisetoastructurallycomparablebuthigherdimensionalscenario.
ThefailureofFAQ,WoLF-PHCandMBL-LCinRPS-9doesnotimplythattherearenoparameter
choicesthatcouldpotentiallyrestoretheconvergenceoftherespectivealgorithms. E.g.,tweakingthe
learningratesmightrestoreconvergenceinthesespecificcases,withoutguaranteeingconvergencein
higherdimensionalscenarios. However,theabsenceofanalyticaltoolsleavestheexistenceofsuch
parametervaluesanopenquestion. Evenwheresuchparameterchoicesexisttheproblemremains
potentiallyintractablewithoutanindicationofwheretolookforthemintheparameterspace—even
moresoforalgorithmswithmoreparameters.Togetherwiththeunpredictabilityoffailuretoconverge
whenmovingfromalowtoahigherdimensionalsetting,thisquestionsthereliabilityofalgorithms
thatseemtomakesenseintuitivelyandlookpromisinginsomeexperimentsbutforwhichwelack
fundamentalresults—particularlyforevenmorecomplicatedalgorithmsnotconsideredhere. In
thissituation,theutilityofthemathematicalguaranteesavailableforMBL-DPUbecomesobvious.
Givenapayoffstructure,conditionsforconvergencecanbecheckedbyanalysingtheODEsystem.
Inspecificcases,thisevenallowstheanalysisofclassesofsettings,suchastwo-playerzero-sum
8(a)WoLF-PHConRPS-3. (b)WoLF-PHConRPS-5. (c)WoLF-PHConRPS-9.
Figure5: Self-playofWoLF-PHC-learningonRPS-3,RPS-5andRPS-9games,withinitiallearning
rate10´1forQ,winlearningrate1{2¨10´4.
(a)MBL-DPU. (b)FAQ. (c)WoLF-PHC.
Figure 6: Self-play on 3MP by (a) MBL-DPU with M´1 “ 20, (b) FAQ with τ “ 20, and
(c)WoLF-PHCwithinitiallearningrate10´1forQ,winlearningrate1{2¨10´4.
games,forwhichwehavepreliminaryresultsthatRMDstabilisesequilibriaandallowsMBL-DPU
toconvergetotheneighbourhoodoftheNashequilibrium. Wefurtherunderstandwhereexactly
MBL-DPUisheadedandthatempiricalnon-convergencebecomeslesslikelywithsmallerlearning
rates. Thisgivesanindicationofwheretolookforasuitablelearningrate. Finally,whereMBL-DPU
failstoconverge,asin3MP,justastheotheralgorithms,theODEunderpinningmakesthisexpectable
and understandable, since an analysis of the corresponding RMD system quickly shows that the
JacobianofthesystemhaseigenvalueswithpositiverealpartsattheNashequilibrium,makingthe
equilibriumunstableforsufficientlysmallmutationstrengths. Thisdemonstratesthatsuchtheoretical
resultsenableustounderstandwhenagivenalgorithmisnotthebestchoiceforasetting,insteadof
searchingforparametervaluesthatmightormightnotrestoreconvergence,aswewouldbeforcedto
dootherwise.
ItshouldbenotedthatwehaveleftoutanymodificationstofurtherimproveMBL-DPU.Inparticular,
themutationstrengthwasfixed, whereasthetheoreticalperspectivemakesitquiteplausiblethat
mutationstrengthcanbechosenaccordingtoareductionschedule,startingwithhighmutationand
fast convergence and reducing mutation over time, increasing the accuracy with which the Nash
equilibriumisapproximated. Notefurtherthatthemutationstrengthislinkedtoameasureofthe
Nashconditionnotbeingsatisfied,sincetheequilibriaofRMDareε-equilibria. Hence,everyplayer
canusethecurrentviolationoftheNashcondition,i.e.,itsowndistancefromacurrentbest-response,
as a guide to adjust its mutation strength, e.g., by adjusting the mutation strength to be slightly
lower than the current violation of the Nash condition. We conjecture that this would result in
the system being driven towards a state that is not worse than the current state, as measured by
the Nash condition, while keeping the convergence speed as high as possible. We would expect
thistospeedupconvergenceandimprovethespeed-accuracytrade-off,makingMBL-DPUmore
attractive as a simple, predictable and theoretically founded MARL algorithm. Apart from such
practicalconsiderations,thecurrentanalysisstillleavesopenthequestionsofanalysingMBL-DPU’s
behaviourinnon-zero-sumgameswithoutstrictNashequilibriaanditsbehaviourinawiderrange
ofn-playersettingswithmorethantwoplayers. Additionally, aclarificationoftheconvergence
propertiesofMBL-LCwouldallowtodetermine, whetherasmallerlearningratewouldrecover
9convergence,sincethelogisticchoicepolicyshowsmuchlargervariancethanthedirectpolicyupdate
andmightthusbemoresensitivetothelearningrate. Furthermore,thecurrentanalysisislimitedto
statelessrepeatedgamesandanextensionoftheanalysistosettingswithstate-dependencywouldbe
desirable,e.g.,whereplayershavesomelimitedmemoryofopponents’pastplay.
References
[1] JohannBauer,MarkBroom,andEduardoAlonso. Thestabilizationofequilibriainevolutionarygame
dynamicsthroughmutation:Mutationlimitsinevolutionarygames. ProceedingsoftheRoyalSocietyA:
Mathematical,PhysicalandEngineeringSciences,475(2231):20190355,2019. doi:10.1098/rspa.2019.
0355.
[2] TilmanBörgersandRajivSarin. LearningThroughReinforcementandReplicatorDynamics. Journalof
EconomicTheory,77(1):1–14,1997. doi:10.1006/jeth.1997.2319.
[3] MichaelBowlingandManuelaVeloso. Multiagentlearningusingavariablelearningrate. Artificial
Intelligence,136(2):215–250,2002. doi:10.1016/S0004-3702(02)00121-2.
[4] ArchieC.Chapman, DavidS.Leslie, AlexRogers, andNicholasR.Jennings. ConvergentLearning
AlgorithmsforUnknownRewardGames. SIAMJournalonControlandOptimization,51(4):3154–3180,
2013. doi:10.1137/120893501.
[5] E.J.CollinsandDavidS.Leslie. Convergentmultiple-timescalesreinforcementlearningalgorithmsin
normalformgames. TheAnnalsofAppliedProbability,13(4):1231–1251,2003. doi: 10.1214/aoap/
1069786497.
[6] JohnG.Cross. AStochasticLearningModelofEconomicBehavior. TheQuarterlyJournalofEconomics,
87(2):239–266,1973. doi:10.2307/1882186.
[7] SergiuHartandAndreuMas-Colell. UncoupledDynamicsDoNotLeadtoNashEquilibrium. American
EconomicReview,93(5):1830–1836,2003. doi:10.1257/000282803322655581.
[8] JosefHofbauerandKarlSigmund. EvolutionaryGamesandPopulationDynamics. CambridgeUniversity
Press,Cambridge,1998.
[9] J.S.Jordan. ThreeProblemsinLearningMixed-StrategyNashEquilibria. GamesandEconomicBehavior,
5(3):368–386,1993. doi:10.1006/game.1993.1022.
[10] MichaelKaisersandKarlTuyls. FrequencyAdjustedMulti-agentQ-learning. InProceedingsofthe9th
InternationalConferenceonAutonomousAgentsandMultiagentSystems,AAMAS’10,pages309–316.
InternationalFoundationforAutonomousAgentsandMultiagentSystems,2010.
[11] ArdeshirKianercyandAramGalstyan. DynamicsofBoltzmannQlearningintwo-playertwo-action
games. PhysicalReviewE,85(4):041145,2012. doi:10.1103/PhysRevE.85.041145.
[12] QinghuaLiu,GellertWeisz,AndrásGyörgy,ChiJin,andCsabaSzepesvari. OptimisticNaturalPolicy
Gradient: A Simple Efficient Policy Optimization Framework for Online RL. Advances in Neural
InformationProcessingSystems,36:3560–3577,2023.
[13] Imre Gergely Mali and Gabriela Czibula. Policy-Based Reinforcement Learning in the Generalized
Rock-Paper-ScissorsGame. InESANN2023Proceedings,pages345–350,2023. doi:10.14428/esann/
2023.ES2023-92.
[14] MatteoMarsili,DamienChallet,andRiccardoZecchina. ExactsolutionofamodifiedElFarol’sbar
problem:Efficiencyandtheroleofmarketimpact. PhysicaA:StatisticalMechanicsanditsApplications,
280(3-4):522–553,2000. doi:10.1016/S0378-4371(99)00610-X.
[15] PanayotisMertikopoulosandWilliamH.Sandholm. LearninginGamesviaReinforcementandRegular-
ization. MathematicsofOperationsResearch,41(4):1297–1324,2016. doi:10.1287/moor.2016.0778.
[16] M.FrankNorman. MarkovProcessesandLearningModels. Numberv.84inMathematicsinScienceand
Engineering.AcademicPress,NewYork,1972.
[17] ShayeganOmidshafiei, ChristosPapadimitriou, GeorgiosPiliouras, KarlTuyls, MarkRowland, Jean-
Baptiste Lespiau, Wojciech M. Czarnecki, Marc Lanctot, Julien Perolat, and Remi Munos. α-Rank:
Multi-AgentEvaluationbyEvolution. ScientificReports,9(1),2019. doi:10.1038/s41598-019-45619-9.
10[18] KarenM.PageandMartinA.Nowak. UnifyingEvolutionaryDynamics. JournalofTheoreticalBiology,
219(1):93–98,2002. doi:10.1006/jtbi.2002.3112.
[19] DinoStephenRatcliffe,KatjaHofmann,andSamDevlin.WinorLearnFastProximalPolicyOptimisation.
In2019IEEEConferenceonGames(CoG),pages1–4, London, UnitedKingdom, 2019.IEEE. doi:
10.1109/CIG.2019.8848100.
[20] KlausRitzbergerandJorgenW.Weibull. EvolutionarySelectioninNormal-FormGames. Econometrica,
63(6):1371–1399,1995. doi:10.2307/2171774.
[21] AldoRustichini. OptimalPropertiesofStimulus—ResponseLearningModels. GamesandEconomic
Behavior,29(1-2):244–273,1999. doi:10.1006/game.1999.0712.
[22] Yuzuru Sato and James P. Crutchfield. Coupled replicator equations for the dynamics of learning in
multiagentsystems. PhysicalReviewE,67(1),2003. doi:10.1103/PhysRevE.67.015206.
[23] YuzuruSato,EizoAkiyama,andJ.DoyneFarmer. Chaosinlearningasimpletwo-persongame. Proceed-
ingsoftheNationalAcademyofSciences,99(7):4748–4751,2002. doi:10.1073/pnas.032086299.
[24] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy
OptimizationAlgorithms. arXiv:1707.06347,2017. doi:10.48550/arXiv.1707.06347.
[25] DavidSilver,JulianSchrittwieser,KarenSimonyan,IoannisAntonoglou,AjaHuang,ArthurGuez,Thomas
Hubert,LucasBaker,MatthewLai,AdrianBolton,YutianChen,TimothyLillicrap,FanHui,LaurentSifre,
GeorgevandenDriessche,ThoreGraepel,andDemisHassabis. MasteringthegameofGowithouthuman
knowledge. Nature,550(7676):354–359,2017. doi:10.1038/nature24270.
[26] GeraldTeschl. OrdinaryDifferentialEquationsandDynamicalSystems. AmericanMathematicalSociety,
Providence,RI,2012.
[27] KarlTuyls,PieterJan’THoen,andBramVanschoenwinkel. AnEvolutionaryDynamicalAnalysisof
Multi-AgentLearninginIteratedGames. AutonomousAgentsandMulti-AgentSystems,12(1):115–153,
2006. doi:10.1007/s10458-005-3783-9.
[28] ChristopherJ.C.H.WatkinsandPeterDayan. Q-Learning. MachineLearning,8:279–292,1992. doi:
10.1023/A:1022676722315.
[29] JörgenW.Weibull. EvolutionaryGameTheory. MITPress,Cambridge,Mass.,1995.
11A Proofs
Theproofsemployaresultprovedin[16,p. 118],whichwestateinthefollowingandthenproceed
toprovepropositions3.1and3.2.
A.1 Atheoremonlearningwithsmallsteps
Theresultfrom[16]weemployisphrasedinthefollowingterms: LetJ ĂR beaparameterset
ą0
withinfJ “ 0andN P N,suchthatforeveryθ P J,tXθu Ă I Ă RN isaMarkovprocess
n ně0 θ
withstationaryprobabilities. WedenotebyE xrŤX nθstheexpectedvalueofX nθ givenX 0θ “ x. Let
furtherI betheminimalclosedconvexsetwith I ĂI. Define
θ θ
Hθ “∆Xθ{θ
n n
andletwpx,θq,Spx,θq,spx,θqandrpx,θqforpx,θqPIˆJ begivenas:
wpx,θq“ErHθ|Xθ “xsPRN
n n
Spx,θq“ErpHθq2|Xθ “xsPRNˆN
n n
spx,θq“ErpHθ ´wpx,θqq2|Xθ “xs“Spx,θq´w2px,θqPRNˆN
n n
rpx,θq“Er}Hθ}3|Xθ “xsPR.
n n
?
wherex2 “xxT and}x}“ xTxforxPRN.
Wecannowstatetheorem8.1.1from[16,p. 118](omittingpart(C)):
TheoremA.1(Norman). Intheabovesituation,letthefollowingconditionsbesatisfied:
ThefamilyofsetspI q satisfies
θ θ
@xPI : lim inf }x´y}“0. (a.1)
θÑ0yPIθ
TherearefunctionswandsonI suchthat:
sup}wpx,θq´wpxq}POpθq, (a.2)
xPIθ
sup}spx,θq´spxq}Ñ0 for θ Ñ0, (a.3)
xPIθ
whereOreferstotheBachmann–Landaunotation.
Thefunctionwisdifferentiable,i.e.,thereisafunctionw1suchthatforallxPI:
}wpyq´wpxq´w1pxqpy´xq}
lim “0. (b.1)
yÑx }y´x}
yPI
Thefunctionw1isbounded:
sup}w1pxq}ă8. (b.2)
xPI
Thefunctionsw1andssatisfytheLipschitzcondition:
}w1pxq´w1pyq}
sup ă8, (b.3)
}x´y}
x,yPI,x‰y
}spxq´spyq}
sup ă8. (b.4)
}x´y}
x,yPI,x‰y
Thefunctionrisbounded:
sup rpx,θqă8. (c)
θPJ,xPIθ
Letfurtherforθ PJ andxPI ,µ px,θq“E rXθsandω px,θq“E r}Xθ ´µ px,θq}2s.
θ n x n n x n n
Inthiscase,thefollowinghold:
12(A) ω px,θqPOpθquniformlyinxPI andnθ ďT foranyT ă8.
n θ
(B) ForanyxPI,thedifferentialequation
f1ptq“wpfptqq
hasauniquesolutionfptq“fpx,tqwithfp0q“x. Foralltě0,wehavefptqPI,and
µ px,θq´fpx,nθqPOpθq
n
uniformlyinxPI andnθ ďT.
θ
RemarkA.2. Wenotethatparts(A)and(B)implythatforallεą0,
supPrp}Xθ ´fpx,Tq}ąεqÑ0
n
xPIθ
fornθ ÑT,θ Ñ0,andgiventhatXθ “xalmostcertainlyforallθ.
0
A.2 ConvergenceofMBL-DPU
We restate the simple reinforcement-mutation rule of MBL-DPU in the setting layed out above,
denoting the mixed strategies with an upper-case X to underscore that this is a random variable
anddenotingthedependenceonaparameterθ,denotingthewholefamilyofstochasticprocesses
astpXθ pnqq u . LetUpxq“pU pxqq bearandomvariablewhoseprobability
ih iPP,hPAi ně0 ih iPP,hPAi
distributiondependsonxPI withadiscrete,non-negativesupportwhichisindependentofx,and
letM ăM forsomeupperboundM ă8andalliPP.
i
ForaplayeriPP andachosenpurestrategyhPA ,theupdaterulethenisgivenasfollows:
i
` ˘ ` ˘
Xθ pn`1q“Xθ pnq`θ p1´Xθ pnqqU pXθpnqq `θM c ´Xθ pnq
ih ih ` ih ih ˘ `i ih ih ˘ (A.1)
Xθ pn`1q“Xθ pnq`θ p´Xθ pnqqU pXθpnqq `θM c ´Xθ pnq fork ‰h.
ik ik ik ih i ik ik
Wecannowshowproposition3.1,i.e.,thatthisruleindeedapproximatesRMDforθ Ñ 0inthe
senseofremarkA.2:
PropositionA.3. ThereisJ suchthatthefamilyofstochasticprocessestpXθ pnqq u
ih iPP,hPAi ně0
givenby(A.1)approximatesthereplicator-mutatordynamicsforθ Ñ0inthesenseofremarkA.2if
Xθp0qPI forallθ PJ.
Proof. TheproofproceedsbyshowingthattpXθ pnqq u satisfiestheconditionsoftheo-
ih iPP,hPAi ně0
remA.1. ForaplayeriPP andachosenstrategyhPA wehave:
i
Hθ pn`1q“∆Xθ pn`1q{θ “p1´Xθ pnqqU pXθpnqq`M pc ´Xθ pnqq
ih ih ih ih i ih ih
Hθ pn`1q“∆Xθ pn`1q{θ “´Xθ pnqU pXθpnqq`M pc ´Xθ pnqq fork ‰h
ik ik ik ih i ik ik
Notethatinthiscase,Hθ pn`1qisindependentofθifXθpnqisgiven,whichsimplifiestheanalysis.
ih
Letussetu pxq“ErU pXθpnqq|Xθpnq“xs,whereitisclearthatthereisnodependenceonn.
ih ih
Notethatuispolynomialinthecomponentsofxandhencesmooth.
Ś
Condition (a.1): In our case, I is given as the polyhedron D and I “ I for all θ and thus
i i θ
condition(a.1)issatisfied.ItremainstoshowthatindeedtpXθ pnqq u ĂI:NotethatU
ih iPP,hPAi ně0 ih
isadiscretenon-negativerandomvariableandthusboundedbysomeC ă8. Forθ ăpC`Mq´1,
wehaveθM ď 1. AssumethatXθ pnq “ x P I, thenforaplayeri P P andachosenstrategy
i ih
hPA wehave
i ` ˘
Xθ pn`1q“x `θ p1´x qU pn`1q`M pc ´x q
ih ih ih ih i ih ih
“x p1´θM q`θp1´x qU pn`1q`θM c ě0
ih i ih ih i ih
andforsomeotherpurestrategyk ‰h,wehave
` ˘
Xθ pn`1q“x `θ p´x qU pn`1q`M pc ´x q
ik ik´
`
ik ih ˘¯i ik ik
“x
ik
1´θlooUooiohooponooo`m1ooqoo`oooMooooin `θM ic
ik
ě0.
ď1
13ř
A simple calculation shows that Xθ pn ` 1q “ 1 if x P I. Thus we have that
k ik
tpXθ pnqq u ĂI ifXθp0qPI forallθandwecanchooseJ “p0,pC`Mq´1q.
ih iPP,hPAi ně0
Conditions(a.2)&(a.3): Considerfirstthefunctionw:
w px,θq“ErHθpnq|Xθpnq“xs
ih
“x p1´x qErU pn`1q|Xθpnq“xs`x M pc ´x q
ihÿ ih ih ih i ih ih
` x p´x qErU pn`1q|Xθpnq“xs`x M pc ´x q
ik ih ik ik i ih ih
k˜‰h ¸
ÿ
“x u pxq´ x u pxq `M pc ´x q
ih ih ik ik i ih ih
k
Itisclearthatwdoesnotdependonθandthatcondition(a.2)istriviallysatisfied. Similarly,Spx,θq
andspx,θqdonotdependonθandcondition(a.3)istriviallysatisfied.
Conditions (b.1)–(b.4): Since the function u is smooth, so is w. In particular, we have that
sup }w1pxq} ă 8 because I is compact and w1 is continuously differentiable, from which
xPI
followsthatw1satisfiestheLipschitz-condition(b.3)onI. Similarly,sissmoothandsatisfies(b.4).
Condition(c): Again, r doesnotdependonθ, andissmoothonI, whichiscompact. Thusitis
boundedonI andcondition(c)issatisfied.
Asaconsequence,wecanapplytheoremA.1tothefamilytXθpnqu andwithremarkA.2we
ně0
havethatforallεą0,
supPrp}Xθpnq´Φpx,Tq}ąεqÑ0
xPI
fornθ Ñ T,θ Ñ 0,andgiventhatXθp0q “ xforallθ,whereforalli P P andh P A ,Φisthe
i
uniquesolutionofthedifferentialequations
Φ9 px,tq“w pΦpx,tqq
ih ih ´ ÿ ¯
“Φ ptq u pΦpx,tqq´ Φ px,tqu pΦpx,tqq `M pc ´Φ px,tqq
ih ih ik ik i ih ih
k
withΦpx,0q“x.
Proposition A.4. Let xM be an equilibrium of (RMD) and U an open neighbourhood of xM.
If xM is globally asymptotically stable, then there is θ ą 0 such that the stochastic process
tpXθ pnqq u definedin(A.1)visitsU almostsurelyafterfinitelymanysteps.
ih iPP,hPAi ně0
Proof. LetΦpx,¨q:R ěŤ0 ÑDsatisfy(RMD)withΦpx,0q“xforallxPD. LetfurtherU1 ĂU
suchthatxM PU1and B pxqĂU forsomeδ ą0,whereB pxqdenotesanopenballwith
xPU1 δ δ
radiusδaroundx. AsxM isgloballyasymptoticallystable,thereisforeachxPDat1 ă8such
thatforalltąt1: Φpx,tqPU1.
ThisisbecausethereisaneighbourhoodV Ă U1 ofxM suchthat@x0 P V,t ą 0 : Φpx0,tq P U1
duetotheLyapunovstabilityofxM. SincexM isasymptoticallystable,foreveryxthereisatą0
suchthatΦpx,tqPV andhencethesolutionwillremaininU1afterwards.
Therefore,defineτ :D ÑRsuchthat:
τpxq“inftT ą0: Φpx,TqPVu
SincetheRHSof(RMD)iscontinuouslydifferentiablebyassumption,itisalsoLipschitzcontinuous.
Thus,Φiscontinuousinthefirstargumentandsoisτ asthefollowingargumentshows:
Let x P D and ε ą 0. Then there is t ą τpxq such that Φpx,sq P V for s P pτpxq,ts. Choose
1
s P pτpxq,tssuchthat|τpxq´s| ă ε . ThenΦpx,sq P V andthereisaneighbourhoodU ofx
1 x
suchthatforally PU ,Φpy,sqPV. Henceτpyqăsăτpxq`ε .
x 1
Wealsohaveτpyqąτpxq´ε duetothefollowing:
1
Considerd:“inft}Φpx,τpxq´ε q´v}:v PVuą0. NotethattheLipschitzconditionimplies
1
thatthereisLą0suchthatforalltą0andally PD
}Φpx,tq´Φpy,tq}ď}x´y}eLt
14andforalltPr0,τpxq´ε s,
1
}Φpx,tq´Φpy,tq}ď}x´y}eLpτpxq´ε1q
andw.l.o.g. wecanassumethat@y P U x,wehave}x´y}eLpτpxq´ε1q ă d 2. Thuswehaveforall
v PV
0ădď}Φpx,tq´v}“}Φpx,tq´Φpy,tq`Φpy,tq´v}
ď}Φpx,tq´Φpy,tq}`}Φpy,tq´v}
d
ď}x´y}eLpτpxq´ε1q`}Φpy,tq´v}ă `}Φpy,tq´v}
2
andsoforally P U , wehaveinft}Φpy,tq´v} : v P V,t P r0,τpxq´ε su ě d ą 0andthus
x 1 2
τpyq ą τpxq´ε . Soτ iscontinuousonD. LetthenT :“ sup τpxq ă 8. Notethatforall
1 xPD
xPDwehavethatforalltąT,Φpx,tqPU1andB pΦpx,tqqĂU.
δ
Letfurtherη ą0. ThenwithpropositionA.3,thereareθ ą0,n PNsuchthatforallxPD,
θ
PrpXθpn qPB pΦpx,TqqĂU|Xθp0q“xqąη
θ δ
andso
PrpXθpn qPUqąη.
θ
FromhereitiseasytoseethatthefirsthittimeofU fortXθptqu tPN
0
isalmostsurelyfinite,i.e.,the
earliesttimetforwhichXθptqPU: LetZpkq:“Xθpkn qfork PN andletS bethefirsthittime
θ 0
ofU fortZpkqu kPN 0,suchthatS isarandomvariablewithvaluesinN 0Yt8u. Clearlythefirsthit
timeofU fortXθptqu tPN
0
issmallerthanfortZpkqu kPN 0.
Wehavethatforallz PDandallk PN:
PrpZ PB pΦpz,TqqĂU|Z “zqąη
k`1 δ k
andhence
PrpZ PUqąη.
k`1
ThenwehaveforS,
PrpS ďk`1q“PrpS ďkq`p1´PrpS ďkqqPrpZ PUqąPrpS ďkqp1´ηq`η
k`1
andaquickinductionargumentyields:
` ˘
PrpS ďk`1qą1´p1´ηqk 1´p1´ηqPrpS “0q
Theprobabilityofafinitehittingtimeisthen:
PrpS PN q“ lim PrpS ďk`1qě1´ limp1´ηqkp1´p1´ηqPrpS “0qq“1
0
kÑ8 kÑ8
Inparticular,thehittingtimeofU fortXθptqu tPN
0
isfinitealmostsurely.
The previous proposition A.4 together with the consideration that an attracting mutation limit is
approximatedbyasymptoticallystablemutationequilibriaandtheimmediatelyfollowingcorollary
showproposition3.2:
Corollary A.5. If xM is a globally asymptotically stable equilibrium of (RMD) and U an open
neighbourhoodofxM,thenthereisθ ą0suchthatthestochasticprocesstXθpnqu definedin
ně0
(A.1)visitsU infinitelyoftenalmostsurely.
Proof. Considerforanyfinitet1 PN theprobabilitythattXθpnqu willnotvisitU afterwards.
0 ně0
ThisisclearlythesameastheprobabilitythattheprocesstZθpnqu inducedby(A.1)andstarting
ně0
inXθpt1q,i.e.,Zθp0q“Xθpt1qalmostsurely,willnotvisitU atall. ThepreviouspropositionA.4
showsthatthisprobabilityis0,whichconcludestheproof.
15B Specificationofexperimentsandfurtherresults
Thissectionprovidesthespecificationdetailsfortheexperimentalresultsofsection4andfurther
results for a broader rangeof parameter values. It is structured asfollows: Each game setting is
introducedwithitspayoffstructuretogetherwithfurtherresultsandashortdescriptionoftheresults,
in the order of Prisoner’s Dilemma (B.1), Matching Pennies (B.2.1), RPS-n games (B.2.2), and
three-player Matching Pennies (B.3). For the two-player settings, the payoff values are given as
matricesR andR ,givingthepayoffsforplayersoneandtworespectively,suchthatifplayerone
1 2
choosesthei-thpurestrategyfromA andplayertwochoosesthej-thpurestrategyfromA ,then
1 2
thepayoffsaregivenasr pi,jq“rR s andr pi,jq“rR s respectively. Theexperimentswere
1 1 ij 2 2 ij
runonasmallclusterofmulti-kernelCPUs, butwehavecheckedthattheycaneasilyberunon
personalhardware.
B.1 Prisoner’sDilemma
TheexperimentalresultsforthePrisoner’sDilemmaarebasedonthefollowingpayoffstructure:
ˆ ˙ ˆ ˙
1 5 1 0
R “ R “
1 0 3 2 5 3
ThisversionhasastrictuniqueNashequilibriumx˚at:
x˚ “p1 0qT x˚ “p1 0qT
1 2
MBL-DPU and MBL-LC. The experimental results (figures 7, 8) illustrate the behaviour of
MBL-DPUanditsconvergencefordifferentmutationstrengthsM. Inaccordancewithintuition,
convergenceisquickforhighmutationstrengthatthepriceofthemutationequilibriumbeingfurther
awayfromtheNashequilibrium. ForlowervaluesofM, wehavethatthemutationequilibrium
movesclosertotheNashequilibriumwhileconvergencebecomesslower. Incomparison,MBL-LC
(figures 9, 10) behaves similarly while converging much more quickly. An intuition for this is
providedwhenconsideringthatMBL-DPUcanbeviewedasalinearapproximationtoMBL-LCfor
smallτ.
FAQ-learning. For FAQ-learning (figures 11, 12), the role of τ corresponds to that of M´1 in
MBL.Wehavethat,similarlytobothMBLvariants,withincreasingvaluesofτ (i.e.,decreasing
valuesofM),thedynamicsapproachesaregionthatliesclosertotheNashequilibrium. Theintuition
hereisprovidedbythefactthatthedeterministiclimitofFAQisclaimedtobeareplicatordynamics
withaperturbativetermwhoseeffectdependsonτ andwhichpullsthesystemtowardsthecentreof
D. Furthermore,convergenceistheslowertheweakertheperturbativetermis,muchlikeinthetwo
MBLvariants. IncontrasttotheMBLvariants,FAQ-learningdefaultstotheusualQ-learningwhen
x ď β. ThiseffectivelyneutralisestherepellingdynamicsattheboundaryofD, whichwould
ih
otherwiseresultinverylarge(unbounded)changesintheQ-valuesforverylowvaluesofx . Note
ih
thatMBL-LChasx occurringinthedenominatortwiceandhenceretainstherepellingeffectatthe
ih
boundaryofD.
WoLF-PHC. Incontrasttotheotheralgorithms,WoLF-PHC(figure13)followsachosendirection
forsometimeuntilitisreplacedbyanewdirection,whichresultsinadiscretesequenceofdirections
andnon-smoothtrajectories. ConvergencetotheNashequilibriumoccursmuchfasterthanforthe
otheralgorithmsinthecaseofPD.However,strictNashequilibriaarealsoasymptoticallystablein
RDandthusPDisabasecasewhichillustratesthedifferentbehavioursinaclear-cutsituation,as
opposedtomorechallengingandambiguoussituationswithoutstrictNashequilibria.
16(a)τ “1,M “1 (b)τ “10,M “10´1 (c)τ “20,M “20´1
Figure7: MBL-DPUinself-playonthePDgamewithdifferentvaluesforτ (1,10,20)orM (1,
10´1,20´1)equivalently;θ “10´4;for10differentinitialconditions. Ineachsubfigure,theupper
graphshowsthetentrajectoriesintheprojectiononthefirstcomponentsoftheplayers’strategies,
inthiscasethe‘defect’strategy,withthefirstplayergivenonthehorizontalaxisandthesecond
playerontheverticalaxis. Pointscolouredyellowcorrespondtoearlierpointsintime,changingover
orangeandviolettoblackforlaterpointsintime. Thepositionofthegame’sNashequilibriumis
markedwithabluecrossintheprojectionplane. Thelowergraphshowsthestandarddeviationofall
componentsoftheplayers’strategiesforeachpointintimeoverthepast5000timesteps,foreachof
theteninitialconditions,colouredredandblueforthetwoplayers. Timeisgivenonthehorizontal
axis. ThestandarddeviationiscomputedwiththeusualEuclideanmetric.
(a)τ “30,M “30´1 (b)τ “35,M “35´1 (c)τ “40,M “40´1
Figure8:MBL-DPUinself-playonthePDgamewithdifferentvaluesforτ (30,35,40)orM (30´1,
35´1, 40´1) equivalently; θ “ 10´4; for 10 different initialisations. (See figure 7 for a detailed
explanationofthegraphs.)
17(a)τ “1,M “1´1 (b)τ “10,M “10´1 (c)τ “20,M “20´1
Figure9: MBL-LCinself-playonthePDgamewithdifferentvaluesforτ (1, 10, 20)orM (1,
10´1,20´1)equivalently;θ “5¨10´3;for10differentinitialisations. (Seefigure7foradetailed
explanationofthegraphs.)
(a)τ “30,M “30´1 (b)τ “35,M “35´1 (c)τ “40,M “40´1
Figure10:MBL-LCinself-playonthePDgamewithdifferentvaluesforτ (30,35,40)orM (30´1,
35´1,40´1)equivalently;θ “5¨10´3;for10differentinitialisations. (Seefigure7foradetailed
explanationofthegraphs.)
18(a)τ “1,M “1´1 (b)τ “10,M “10´1 (c)τ “20,M “20´1
Figure 11: FAQ in self-play on the PD game with different values for τ (1, 10, 20) or M (1,
10´1,20´1)equivalently;θ “5¨10´3;for10differentinitialisations. (Seefigure7foradetailed
explanationofthegraphs.)
(a)τ “30,M “30´1 (b)τ “35,M “35´1 (c)τ “40,M “40´1
Figure12: FAQinself-playonthePDgamewithdifferentvaluesforτ (30,35,40)orM (30´1,
35´1,40´1)equivalently;θ “5¨10´3;for10differentinitialisations. (Seefigure7foradetailed
explanationofthegraphs.)
19(a)Initiallearningrate10´1 for (b)Initiallearningrate10´1 for (c)Initiallearningrate10´2 for
Q.Winlearningrate10´2. Q.Winlearningrate1{2¨10´4. Q.Winlearningrate1{2¨10´4.
Figure13:WoLF-PHCinself-playonthePDgamewithdifferentlearningschedules;for10different
initialisations. Subgraph(a)hasahighconvergencespeedsuchthatonlydisconnectedpointscanbe
seen. (Seefigure7foradetailedexplanationofthegraphs.)
B.2 Zero-sumgames
For two-player zero-sum games, we have preliminary results showing that the Nash equilibrium
is an attracting mutation limit. While RD (and Cross learning) would not converge to interior
equilibria(withCrosslearningeventuallyapproachingtheboundary),RMDconvergestothemutation
equilibriumforeverychoiceofmutationprobabilities,cPD˝andM ą0,andsodoesMBL-DPU.
Stability is induced by the perturbative terms and their varying strengths have two effects which
havetobeweighedagainsteachother. Wedemonstratethegeneralideainthesimplesituationof
the Matching Pennies (MP) game. Further, we illustrate the changing behaviour when we grow
thestrategyspacebyconsideringdifferentversionsoftheRock-Paper-Scissorsgame,RPS-n,with
n“3,5,9,wherendenotesthenumberofstrategiesavailabletoeachplayer.
B.2.1 MatchingPennies
TheexperimentalresultsfortheMatchingPenniesgamearebasedonthefollowingpayoffstructure:
ˆ ˙ ˆ ˙
1 ´23{10 ´23{10 1
R “ R “
1 ´4{10 1 2 1 ´4{10
Nashequilibriumx˚at:
x˚ “p14{47 33{47qT x˚ “p33{47 14{47qT
1 2
TheMPgameisaparticularlysimplecaseofazero-sumgameandhenceprovidesaninformative
perspectiveonthebasiccharacteristicsofthedifferentalgorithms. Ingeneral,weseethatthelocation
ofthemutationequilibriumdependsonthemutationstrengthM,whileconvergenceisslowerfor
lowervaluesofM creatingatrade-offbetweenthese.
MBL-DPUandMBL-LC. ComparingMBL-DPUandMBL-LC,weseeagainthattheLC-variant
(figures16,17)approachesthemutationequilibriummorequicklythantheDPU-variant(figures
14,15). However,weseethattheDPU-variantexhibitsamuchsmallervariance,moreprecisely
standarddeviation,inthevicinityofthemutationequilibriumduetoitsslowerchange,withboth
20variantsroughlydifferingbyafactorbetween5and10(forM “40´1). Thisillustratesthestronger
effectthatsinglelargerpayoffshaveontheLC-variant,producingalargervariancenearthemutation
equilibrium.
FAQ-learning. ForFAQ-learning(figures18,19)weseeasimilarbehaviourasMBL-LC,however
withasmallervarianceneartheequilibriumforweakerperturbation(figure19). AswiththeMBL
variants, FAQ exhibits slower convergence for weaker perturbation with larger variance near its
(apparentlyasymptoticallystable)equilibrium. However,wealsoobservethatwithFAQ,solutions
cangettrappedneartheboundary(notethetrappedsolutionintheupperleftcornerinfigure19),
whichwedonotobservefortheMBLvariantsand haveprovednottobethecaseforMBL-DPU.
WoLF-PHC. Similartotheotheralgorithms,WoLF-PHC(figure20)followsspiral-liketrajectories
towardsaregionclosetotheNashequilibrium. Italsoshowsalowervariancenearthe(apparently
asymptoticallystable)equilibrium. However,WoLF-PHCemploysalearningrateschedulewhich
reducesthelearningrateovertimeandthusreducesvariance.5 OneshouldnotethatWoLF-PHCis
considerablymorecomplicatedasitreliesonareliablewaytoestimateaction-valuesaswellasa
long-termpopulationaverage. Itisclearthataplayerwouldrequiremoreresourcesforimplementing
WoLF-PHCthanfortheotheralgorithms.
(a)τ “1,M “1´1 (b)τ “10,M “10´1 (c)τ “20,M “20´1
Figure14: MBL-DPUinself-playontheMPgamewithdifferentvaluesforτ (1, 10, 20)orM
(1,10´1,20´1)equivalently;θ “10´4;for10differentinitialisations. (Seefigure7foradetailed
explanationofthegraphs.)
5ItwouldbepossibletoevaluateWoLF-PHCwithafixedlearningrateoruseareductionscheduleforthe
otheralgorithms.However,theformerwouldbeadeviationfromthecanonicalformulationofWoLF-PHCwhile
thelatterwouldnotbebasedonaprincipledapproach.Hence,thisheterogeneoussituationisanappropriate
basescenario.
21(a)τ “30,M “30´1 (b)τ “35,M “35´1 (c)τ “40,M “40´1
Figure 15: MBL-DPU in self-play on the MP game with different values for τ (30, 35, 40) or
M (30´1,35´1,40´1)equivalently;θ “ 10´4;for10differentinitialisations. (Seefigure7fora
detailedexplanationofthegraphs.)
(a)τ “1,M “1´1 (b)τ “10,M “10´1 (c)τ “20,M “20´1
Figure16: MBL-LCinself-playontheMPgamewithdifferentvaluesforτ (1,10,20)orM (1,
10´1,20´1)equivalently;θ “5¨10´3;for10differentinitialisations. (Seefigure7foradetailed
explanationofthegraphs.)
22(a)τ “30,M “30´1 (b)τ “35,M “35´1 (c)τ “40,M “40´1
Figure 17: MBL-LC in self-play on the MP game with different values for τ (30, 35, 40) or M
(30´1,35´1,40´1)equivalently;θ “ 5¨10´3;for10differentinitialisations. (Seefigure7fora
detailedexplanationofthegraphs.)
(a)τ “1,M “1´1 (b)τ “10,M “10´1 (c)τ “20,M “20´1
Figure 18: FAQ in self-play on the MP game with different values for τ (1, 10, 20) or M (1,
10´1,20´1)equivalently;θ “5¨10´3;for10differentinitialisations. (Seefigure7foradetailed
explanationofthegraphs.)
23(a)τ “30,M “30´1 (b)τ “35,M “35´1 (c)τ “40,M “40´1
Figure19: FAQinself-playontheMPgamewithdifferentvaluesforτ (30,35,40)orM (30´1,
35´1,40´1)equivalently;θ “5¨10´3;for10differentinitialisations. (Seefigure7foradetailed
explanationofthegraphs.)
(a)Initiallearningrate10´1 for (b)Initiallearningrate10´1 for (c)Initiallearningrate10´2 for
Q.Winlearningrate10´2. Q.Winlearningrate1{2¨10´4. Q.Winlearningrate1{2¨10´4.
Figure20:WoLF-PHCinself-playontheMPgamewithdifferentlearningschedules;for10different
initialisations. (Seefigure7foradetailedexplanationofthegraphs.)
24B.2.2 Zero-sumgameswithlargeractionspaces
TheexperimentalresultsfortheRPS-ngamesarebasedonthefollowingpayoffstructures.
RPS-3.
˜ ¸
0 ´2 3
R “ 2 0 ´2 R “´R
1 2 1
´1 2 0
Nashequilibriumx˚at:
x˚ “p2{7 11{35 2{5qT x˚ “p2{5 11{35 2{7qT
1 2
RPS-5.
¨ ˛
0 4 ´2 2 ´2
˚´4 0 2 ´1 1 ‹
˚ ‹
R “˚ 2 ´4 0 4 ´1‹ R “´R
1 ˝ ‚ 2 1
´4 1 ´4 0 2
2 ´1 1 ´2 0
Nashequilibriumx˚at:
x˚ “p11{61 510{2989 8{61 50{427 1198{2989qT
1
x˚ “p1{7 68{427 6{49 502{2989 174{427qT
2
RPS-9.
¨ ˛
0 2 1 3 1 ´1 ´1 ´2 ´1
˚´1 0 1 3 1 1 ´1 ´2 ´1‹
˚ ‹
˚´1 ´2 0 3 1 1 1 ´2 ´1‹
˚ ‹
˚´2 ´4 ´2 0 2 2 2 4 ´2‹
˚ ‹
R 1 “˚´1 ´2 ´1 ´3 0 1 1 2 1 ‹ R 2 “´R 1
˚ ‹
˚ 1 ´2 ´1 ´3 ´1 0 1 2 1 ‹
˚ 2 4 ´2 ´6 ´2 ´2 0 4 2 ‹
˝ ‚
1 2 1 ´3 ´1 ´1 ´1 0 1
1 2 1 3 ´1 ´1 ´1 ´2 0
Nashequilibriumx˚at:
x˚ “p1{8 1{8 1{8 1{16 1{8 1{8 1{16 1{8 1{8qT
1
x˚ “p3{22 3{44 3{22 1{22 3{22 3{22 3{22 3{44 3{22qT
2
WhileMPisaninformativeillustrationofthedifferentbehaviours,MPreducestoaplanardynamical
system,whichdoesnotallowmanycomplexbehaviours,asexemplifiedbythePoincaré-Bendixson
theorem,e.g.,[26,theorem7.16]holdingforplanarsystems. Hence,higher-dimensionalzero-sum
gamesallowafurtherunderstandingofthedifferencesbetweenthealgorithmsandshedlightonthe
effectoflargerstatespaceswhilepreservingtheneutralstabilityofinteriorequilibria. Weconsider
heretheRock-Paper-Scissorsgameofdifferentsizes(3,5and9actions).
MBL-DPUandMBL-LC. InRPS-3,MBL-DPU(figures21,22)showsasimilarbehaviourtoMP
withamarkeddependenceofthebehaviourofthevarianceonthevalueofM. Incontrast,MBL-LC
(figures23,24)showsamuchquickerconvergence,withthevariancedroppingaftersimilarnumbers
ofepisodes(around105)forallvaluesofM.AswithMBL-DPU,theresidualvarianceincreaseswith
weakermutation. ThisisinaccordancewiththeneutralstabilityoftheNashequilibrium,allowing
forlargerfluctuations.
In RPS-5, both MBL variants (figures 28, 29 for MBL-DPU and figures 30, 31 for MBL-LC)
showbehaviourssimilartotheirRPS-3counterparts. InRPS-9,MBL-DPU(figures35,36)again
showssimilarbehaviour,withslowerconvergencecomparedtoitsRPS-3andRPS-5counterparts.
Interestingly, MBL-LC (figures 37, 38) seems to have two distinct regions to which trajectories
evolve,suggestingapotentiallystrongersensitivitytothechoiceofθ.
25FAQ-learning. Like for MP, we see a quicker convergence for FAQ in RPS-3 (figures 25, 26)
comparedtotheMBLvariants,butwithtrajectoriessimilartothoseofMBL-LCwhenconsideringlow
valuesofM,inwhichcasethereplicatordynamicsmakesastrongercontributiontothetrajectories.
SimilartoMBL-LC,butalreadyinRPS-5,FAQshowstwodistinctregionstowhichtrajectories
evolvewhenperturbationisweak(figures32,33),whereastheformerdoesnotshowsuchasplitfor
RPS-5. InRPS-9,FAQshowssuchasplitforstrongerperturbationlevelsalreadyandshowseven
threedistinctsuchregionsforweakerperturbation(figures39,40).
WoLF-PHC. ForWoLF-PHC,weseeastillquickerconvergenceinRPS-3(figure27)thanforthe
otheralgorithms,similartotheMPcase. However,thebehaviourismuchlessclearinRPS-5(figure
34). Here,trajectoriesdonotconsistentlyapproachaspecificregion. Itispossiblethatthereduction
schedulesforthelearningrates,whichforceeachtrajectorytoconverge,leadtotrajectoriesstalling
prematurely. ThisbecomesevenmorepronouncedinRPS-9(figure41),whereWoLF-PHCseemsto
initiallymoveawayfromtheNashequilibriumandtogetstuckalongtheboundariesofD.
(a)τ “1,M “1´1 (b)τ “10,M “10´1 (c)τ “20,M “20´1
Figure21: MBL-DPUinself-playontheRPS-3gamewithdifferentvaluesforτ (1,10,20)orM
(1,10´1,20´1)equivalently;θ “10´4;for10differentinitialisations. (Seefigure7foradetailed
explanationofthegraphs.)
26(a)τ “30,M “30´1 (b)τ “35,M “35´1 (c)τ “40,M “40´1
Figure22: MBL-DPUinself-playontheRPS-3gamewithdifferentvaluesforτ (30,35,40)or
M (30´1,35´1,40´1)equivalently;θ “ 10´4;for10differentinitialisations. (Seefigure7fora
detailedexplanationofthegraphs.)
(a)τ “1,M “1´1 (b)τ “10,M “10´1 (c)τ “20,M “20´1
Figure23: MBL-LCinself-playontheRPS-3gamewithdifferentvaluesforτ (1,10,20)orM (1,
10´1,20´1)equivalently;θ “5¨10´3;for10differentinitialisations. (Seefigure7foradetailed
explanationofthegraphs.)
27(a)τ “30,M “30´1 (b)τ “35,M “35´1 (c)τ “40,M “40´1
Figure24: MBL-LCinself-playontheRPS-3gamewithdifferentvaluesforτ (30,35,40)orM
(30´1,35´1,40´1)equivalently;θ “ 5¨10´3;for10differentinitialisations. (Seefigure7fora
detailedexplanationofthegraphs.)
(a)τ “1,M “1´1 (b)τ “10,M “10´1 (c)τ “20,M “20´1
Figure 25: FAQ in self-play on the RPS-3 game with different values for τ (1, 10, 20) or M (1,
10´1,20´1)equivalently;θ “5¨10´3;for10differentinitialisations. (Seefigure7foradetailed
explanationofthegraphs.)
28(a)τ “30,M “30´1 (b)τ “35,M “35´1 (c)τ “40,M “40´1
Figure26: FAQinself-playontheRPS-3gamewithdifferentvaluesforτ (30,35,40)orM (30´1,
35´1,40´1)equivalently;θ “5¨10´3;for10differentinitialisations. (Seefigure7foradetailed
explanationofthegraphs.)
(a)Initiallearningrate10´1 for (b)Initiallearningrate10´1 for (c)Initiallearningrate10´2 for
Q.Winlearningrate10´2. Q.Winlearningrate1{2¨10´4. Q.Winlearningrate1{2¨10´4.
Figure27: WoLF-PHCinself-playontheRPS-3gamewithdifferentlearningschedules;for10
differentinitialisations. (Seefigure7foradetailedexplanationofthegraphs.)
29(a)τ “1,M “1´1 (b)τ “10,M “10´1 (c)τ “20,M “20´1
Figure28: MBL-DPUinself-playontheRPS-5gamewithdifferentvaluesforτ (1,10,20)orM
(1,10´1,20´1)equivalently;θ “10´4;for10differentinitialisations. (Seefigure7foradetailed
explanationofthegraphs.)
(a)τ “30,M “30´1 (b)τ “35,M “35´1 (c)τ “40,M “40´1
Figure29: MBL-DPUinself-playontheRPS-5gamewithdifferentvaluesforτ (30,35,40)or
M (30´1,35´1,40´1)equivalently;θ “ 10´4;for10differentinitialisations. (Seefigure7fora
detailedexplanationofthegraphs.)
30(a)τ “1,M “1´1 (b)τ “10,M “10´1 (c)τ “20,M “20´1
Figure30: MBL-LCinself-playontheRPS-5gamewithdifferentvaluesforτ (1,10,20)orM (1,
10´1,20´1)equivalently;θ “5¨10´3;for10differentinitialisations. (Seefigure7foradetailed
explanationofthegraphs.)
(a)τ “30,M “30´1 (b)τ “35,M “35´1 (c)τ “40,M “40´1
Figure31: MBL-LCinself-playontheRPS-5gamewithdifferentvaluesforτ (30,35,40)orM
(30´1,35´1,40´1)equivalently;θ “ 5¨10´3;for10differentinitialisations. (Seefigure7fora
detailedexplanationofthegraphs.)
31(a)τ “1,M “1´1 (b)τ “10,M “10´1 (c)τ “20,M “20´1
Figure 32: FAQ in self-play on the RPS-5 game with different values for τ (1, 10, 20) or M (1,
10´1,20´1)equivalently;θ “5¨10´3;for10differentinitialisations. (Seefigure7foradetailed
explanationofthegraphs.)
(a)τ “30,M “30´1 (b)τ “35,M “35´1 (c)τ “40,M “40´1
Figure33: FAQinself-playontheRPS-5gamewithdifferentvaluesforτ (30,35,40)orM (30´1,
35´1,40´1)equivalently;θ “5¨10´3;for10differentinitialisations. (Seefigure7foradetailed
explanationofthegraphs.)
32(a)Initiallearningrate10´1 for (b)Initiallearningrate10´1 for (c)Initiallearningrate10´2 for
Q.Winlearningrate10´2. Q.Winlearningrate1{2¨10´4. Q.Winlearningrate1{2¨10´4.
Figure34: WoLF-PHCinself-playontheRPS-5gamewithdifferentlearningschedules;for10
differentinitialisations. (Seefigure7foradetailedexplanationofthegraphs.)
(a)τ “1,M “1´1 (b)τ “10,M “10´1 (c)τ “20,M “20´1
Figure35: MBL-DPUinself-playontheRPS-9gamewithdifferentvaluesforτ (1,10,20)orM
(1,10´1,20´1)equivalently;θ “10´4;for10differentinitialisations. (Seefigure7foradetailed
explanationofthegraphs.)
33(a)τ “30,M “30´1 (b)τ “35,M “35´1 (c)τ “40,M “40´1
Figure36: MBL-DPUinself-playontheRPS-9gamewithdifferentvaluesforτ (30,35,40)or
M (30´1,35´1,40´1)equivalently;θ “ 10´4;for10differentinitialisations. (Seefigure7fora
detailedexplanationofthegraphs.)
(a)τ “1,M “1´1 (b)τ “10,M “10´1 (c)τ “20,M “20´1
Figure37: MBL-LCinself-playontheRPS-9gamewithdifferentvaluesforτ (1,10,20)orM (1,
10´1,20´1)equivalently;θ “5¨10´3;for10differentinitialisations. (Seefigure7foradetailed
explanationofthegraphs.)
34(a)τ “30,M “30´1 (b)τ “35,M “35´1 (c)τ “40,M “40´1
Figure38: MBL-LCinself-playontheRPS-9gamewithdifferentvaluesforτ (30,35,40)orM
(30´1,35´1,40´1)equivalently;θ “ 5¨10´3;for10differentinitialisations. (Seefigure7fora
detailedexplanationofthegraphs.)
(a)τ “1,M “1´1 (b)τ “10,M “10´1 (c)τ “20,M “20´1
Figure 39: FAQ in self-play on the RPS-9 game with different values for τ (1, 10, 20) or M (1,
10´1,20´1)equivalently;θ “5¨10´3;for10differentinitialisations. (Seefigure7foradetailed
explanationofthegraphs.)
35(a)τ “30,M “30´1 (b)τ “35,M “35´1 (c)τ “40,M “40´1
Figure40: FAQinself-playontheRPS-9gamewithdifferentvaluesforτ (30,35,40)orM (30´1,
35´1,40´1)equivalently;θ “5¨10´3;for10differentinitialisations. (Seefigure7foradetailed
explanationofthegraphs.)
(a)Initiallearningrate10´1 for (b)Initiallearningrate10´1 for (c)Initiallearningrate10´2 for
Q.Winlearningrate10´2. Q.Winlearningrate1{2¨10´4. Q.Winlearningrate1{2¨10´4.
Figure41: WoLF-PHCinself-playontheRPS-9gamewithdifferentlearningschedules;for10
differentinitialisations. (Seefigure7foradetailedexplanationofthegraphs.)
36B.3 Three-playerMatchingPennies
Further,weconsiderthebehaviouroftheMBLvariantsincomparisontoFAQlearningandWoLF-
PHCinathree-playerMatchingPennies(3MP)gameintroducedin[9],withpayoffsasgivenin
table1. ThesimilaritytothestandardMPgamebecomesclearwhenoneconsidersthatthepayoff
structurereflectsthefollowingidea: Thefirstplayerwantstomatchthesecondplayer’saction. The
secondplayerwantstomatchthethirdplayer’saction. However,thethirdplayerdoesnotwantto
matchthefirstplayer’saction. TheuniqueNashequilibriumfor3MPislocatedatthecentreofD.
Notethat,asinitiallyproposed,3MPisnotazero-sumgame.
H T H T
H p1,1,´1q p´1,´1,´1q H p1,´1,1q p´1,1,1q
T p´1,1,1q p1,´1,1q T p´1,´1,´1q p1,1,´1q
(a)Payoffswhenthethirdplayerchooses‘H’. (b)Payoffswhenthethirdplayerchooses‘T’.
Table1: Payofftuplesforthethree-playerMatchingPennies(3MP)gamewiththefirstplayer’s
actiondeterminingtherow,thesecondplayer’sactionthecolumn,andthethirdplayer’sactionthe
table.
In3MP,bothMBLvariants(figures42,43)showapparentlyasymptoticallystableperiodiclimit
behaviours,whichapproachtheboundaryofDasmutationdiminishes. Wefurtherseeaverysimilar
behaviourforFAQ(figure44)withτ´1showingananalogouseffecttoM inMBL,quitesimilarto
thetwo-playersettings. Likewise,WoLF-PHC(figure45)exhibitsapparentlyasymptoticallystable
trajectories,atleastintheprojectionontothefirstactionsofthefirsttwoplayers. Again,WoLF-PHC
showsareductionofvarianceovertime,presumablyduetodiminishinglearningrates. In[3],the
authorsshowthatWoLF-PHCconvergestotheNashequilibriumwhenδ {δ “ 3(asopposedto
l w
δ {δ “ 2). Since there is no established ODE approximation of WoLF-PHC that we are aware
l w
of,thereasonsforthisremainunclear. OneshouldalsonotethatwehavemadesurethattheNash
equilibriumisnotlocatedatthecentreofDinthetwo-playergamesbecausetheperturbationtermin
FAQhasitsequilibriumthereandconvergencemighteasilyhavebeencoincidental. For3MP,we
havenotmadeanysuchadaptationsandsomebehavioursmightchangewhentheNashequilibrium
ismovedawayfromthecentre.
37(a)τ “10,M “10´1 (b)τ “20,M “20´1 (c)τ “30,M “30´1
Figure 42: MBL-DPU in self-play on the 3MP game with different values for τ (10, 20, 30) or
M (10´1,20´1,30´1)equivalently;θ “ 10´4;for10differentinitialisations. (Seefigure7fora
detailedexplanationofthegraphs.)
(a)τ “10,M “10´1 (b)τ “20,M “20´1 (c)τ “30,M “30´1
Figure43: MBL-LCinself-playonthe3MPgamewithdifferentvaluesforτ (10,20,30)orM
(10´1, 20´1, 30´1) equivalently; θ “ 10´4; for 10 different initialisations. (See figure 7 for a
detailedexplanationofthegraphs.)
38(a)τ “10,M “10´1 (b)τ “20,M “20´1 (c)τ “30,M “30´1
Figure44: FAQinself-playonthe3MPgamewithdifferentvaluesforτ (10,20,30)orM (10´1,
20´1, 30´1) equivalently; θ “ 10´4; for 10 different initialisations. (See figure 7 for a detailed
explanationofthegraphs.)
(a)Initiallearningrate10´1 for (b)Initiallearningrate10´1 for (c)Initiallearningrate10´2 for
Q.Winlearningrate10´2. Q.Winlearningrate1{2¨10´4. Q.Winlearningrate1{2¨10´4.
Figure 45: WoLF-PHC in self-play on the 3MP game with different learning schedules; for 10
differentinitialisations. (Seefigure7foradetailedexplanationofthegraphs.)
39