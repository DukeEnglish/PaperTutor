Cognitive Insights and Stable Coalition Matching for
Fostering Multi-Agent Cooperation
JiaqiShao1,2, TianjunYuan2, TaoLin3, XuanyuCao1, BingLuo2∗
1TheHongKongUniversityofScienceandTechnology,HongKongSAR,China
2DukeKunshanUniversity,Suzhou,China
3WestlakeUniversity,Hangzhou,China
Abstract
Cognitiveabilities,suchasTheoryofMind(ToM),playavitalroleinfacilitating
cooperationinhumansocialinteractions. However,ourstudyrevealsthatagents
withhigherToMabilitiesmaynotnecessarilyexhibitbettercooperativebehavior
comparedtothosewithlowerToMabilities. Toaddressthischallenge,wepropose
anovelmatchingcoalitionmechanismthatleveragesthestrengthsofagentswith
different ToM levels by explicitly considering belief alignment and specialized
abilitieswhenformingcoalitions. Ourproposedmatchingalgorithmseekstofind
stablecoalitionsthatmaximizethepotentialforcooperativebehaviorandensure
long-termviability. Byincorporatingcognitiveinsightsintothedesignofmulti-
agentsystems,ourworkdemonstratesthepotentialofleveragingToMtocreate
moresophisticatedandhuman-likecoordinationstrategiesthatfostercooperation
andimproveoverallsystemperformance.
1 Introduction
Cooperation is a fundamental aspect of multi-agent systems, enabling agents to work together
effectivelytoachievecommongoalsandsolvecomplexproblems[32]. Inrecentyears,therapid
advancement of large language models (LLMs) has opened up new opportunities for building
intelligentmulti-agentsystems. LLMshavedemonstratedremarkablecapabilitiesinnaturallanguage
understanding,generation,andreasoning,suchasGPT-family[7]. Byleveragingthesepowerful
LLMs,multi-agentsystemscanenableagentstocommunicateandcollaborateusingnaturallanguage,
resultinginmoreflexibleandhuman-likeinteractionsincooperationtasks.
TofostercooperationamongLLM-basedagents,mostrecentresearchfocusedoncommunicationand
knowledgesharingamongagents[44,16,15,42,25,10]. Thesestudiesdemonstratethepotentialof
leveragingnaturallanguagecapabilitiestofacilitateinformationexchangeandcoordinationinmulti-
agentsystems. Besideslanguageunderstanding,someresearchhasexploredeffectivecooperation
throughagentcognitiveabilities,suchasreasoningandreflection,tocoordinateactionsandmake
decisions[29,18,20,21]. Thesestudieshighlighttheimportanceofinvestigatinghowwecanfoster
cooperationinLLM-basedmulti-agentsystemsfromacognitiveperspective.
OneofthefundamentalcognitiveabilitiesistheTheoryofMind(ToM).ByutilizingToMinstrategic
interactions,anagentcanmentallysimulateothers’thoughts,allowinghimtoanticipatepotential
moves. Furthermore, ToM can involve multiple levels of recursive belief attribution, known as
higher-orderToM,whereplayersconsidernotonlytheiropponent’sbeliefsbutalsotheirbeliefsabout
theotherplayer’sbeliefs,andsoon[28].Inotherwords,ToMallowspeopletoseethingsfromothers’
perspective. Forexample,inachessgame,aplayerwithhigher-orderToMmightthink,“Ibelieve
myopponentthinksthatIwillmovemyknight,soIwillmovemybishopinstead,becauseIbelieve
theywillnotexpectthatmove."Inhumansocialinteractions,ToMplaysacrucialroleinfacilitating
∗Correspondingauthor:BingLuo,{bing.luo}@dukekunshan.edu.cn
Preprint.Underreview.
4202
yaM
82
]AM.sc[
1v44081.5042:viXracooperationbyenablingindividualstounderstandandpredictthebehaviorofothers[46]. Based
ontheseinsights, thereisagrowinginterestinleveragingToMinLLMstoenhancemulti-agent
cooperation[12,18], whereToMplaysavitalroleinfacilitatingthecoordinationofactionsand
the resolution of conflicts in cooperation. Normally, LLM agents with higher ToM capabilities
canbetterunderstandandpredicttheactionsofotheragentsleadingtoimprovedcooperationand
coordination[13,34].
TofurtheridentifytherelationshipbetweenToMabilityandcooperationbehavior,weinvestigate
thecooperativebehaviorofagentswithhighandlowToMabilities. However, ourresultreveals
thatagentswithlowerToMcapabilitiesexhibitedbettercooperativebehaviorcomparedtothose
withhigherToMcapabilities(asdetailedinFigure1). ThissuggeststhathavingahighlevelofToM
alonemaynotalwaysleadtobettercooperation. Intuitively,agentswithhigherToMcapabilities
may overthink and anticipate potential conflicts, resulting in more cautious and less cooperative
behavior. ThisfindingalignswiththepsychologicalresearchbyRidingerandMcBride[30],which
suggeststhatToMcapabilitiesalonearenotsufficienttoguaranteecooperativebehavior. Instead,
agentsmayalsoneedtobewillingtopositivelyreciprocateandcooperatewithothers. Specifically,
theresearchhighlightsthatwhenToMabilitiesarecombinedwithahighproportionofindividuals
whoarewillingtoengageincooperation,itcanleadtoimprovedcooperationwithinthegroup[30].
Thisinsightmotivatesourproposedapproachofincorporatingbeliefalignmentintothecoalition
formationprocesstopromotecooperativebehavioramongagentswithhighcognitiveabilities.
Toaddresstheabovechallenge,weproposeanovelmatchingcoalitionmechanismtofindcoalitions
that maximize the potential for cooperative behavior. This mechanism fosters cooperation and
leveragesthestrengthsofagentswithdifferentToMlevelsbyexplicitlyconsideringbeliefalignment
ascapturedbytheirToMcapabilities. ByformingcoalitionswithdiverseToMlevels,weaimto
createcoalitionsthatcaneffectivelyreasonabouteachother’smentalstateswhileleveragingtheir
uniquestrengthstosolvecomplexproblems. Moreover,ourmatchingalgorithmallowsforstable
matching,whichensuresthelong-termviabilityandeffectivenessoftheformedcoalitions.
Contribution. Wesummarizedourmaincontributionsasfollows:
1. WeinvestigatetheinterplaybetweenToMcapabilitiesandcooperativebehaviorinLLM-
basedmulti-agentsystems. OurstudyrevealsagentswithToMcapabilitiesmaynotnecessarily
exhibitbettercooperativebehavior. Theseinsightsposenewchallengesinunderstandingthe
complex interplay between cognitive abilities and cooperative behaviors, demonstrating the
potentialofleveragingcognitiveinsightstodesigneffectivemulti-agentmechanisms.
2. WeintroduceastablematchingmechanismforcoalitionformationamongLLMagents,
basedonthealignmentbetweenagents’beliefs(derivedfromtheirToMcapabilities)andthe
actualactionsoftheirpotentialpartners.Byestablishingpreferenceordersbasedonbelief-action
alignment,ourmechanismformseffectivecoalitionsandoptimizescooperationamongagents.
3. WeconductacomprehensiveexperimentalevaluationincorporatingToMcapabilitiestoassess
theeffectivenessoftheproposedmethodsinfacilitatingcooperation,coalitionstability,and
task-specificperformance.
2 RelatedWork
TheoryofMind(ToM).TheconceptofToM,whichinvolvestheabilitytoattributementalstates
tooneselfandothers,hasbeenstudiedextensivelyincognitivescienceandpsychology[28,9]. In
thefieldofmulti-agentreinforcementlearning,existingresearchmainlyusessupervisedtrainingto
equipagentswithToMcapabilitiesforcooperativetasks,e.g.,[40,39,26].
Withtherapidadvancementoflargelanguagemodels(LLMs),somerecentworkshaveexplored
empowering LLMs with ToM capabilities [2, 19, 48, 43]. However, these efforts have primarily
focusedondevelopingToMcapabilitiesforindividualagents,withlimitedexplorationofhowToM
can facilitate cooperation among multiple agents. While Li et al. [18] studied LLM agents with
ToMcapabilitiesandleveragedToMcapabilitiesspecificallyforcooperativetasks,therelationship
betweenToMabilityandcooperativebehaviorremainsunclear.
Inthiswork,weaimtoexploretheinterplaybetweenToMabilityandcooperativebehavior. Unlike
previousstudiesthatprimarilyfocusedonequippingindividualagentswithToMcapabilities,we
investigatehowdifferentlevelsofToMabilitiesinfluencecooperativebehaviorinmulti-agentsystems.
Multi-agentCooperation. Multi-agentcooperationhasbeenstudiedextensivelyinbothindustry
and academia, with traditional approaches focusing on game-theoretic frameworks [32, 46] and
2Figure1: Illustrationforthemulti-agentsystemsetupandtheToMcognitivethinkingprocess.
(Left)Themulti-agentsystemsetupfortheiterativeprogrammingtasks,consistingofoneproject
manager(PM)withToMabilityandfourEngineers. Theiterativeprocessinvolves:❶PMupdatesits
beliefsandtakesactionsbasedonitsToMreasoning;❷PMobservesEngineers’actions;❸Evaluate
cooperationbythealignmentbetweenPM’sbeliefsandEngineers’actions;❹PMprovidesinstruc-
tionstoEngineers. (Right)TheToMcognitivethinkingprocessofPMagentinvolvesrecursivebelief
updatesanddecision-makingbasedontheinferredbeliefs.
Table1: Comparisonofcooperationbehaviorsbetweenagentswithlow(k=1)andhigh(k=2)
ToMcapabilitiesintheIterativeProgrammingtaskontwobenchmarks,includingHUMANEVAL[5]
andMBPP[3],overR=1...5interactionrounds:LowToMagentsshowHighercooperationtrends.
GPT-3.5-Turbo GLM-4 Llama-3-70b Gemini-1.5-flash Claude-3-sonnet
ToMk R=1 R=5 R=1 R=5 R=1 R=5 R=1 R=5 R=1 R=5
k=1 62.5 51.7 65.5 63.3 80.9 75.0 75.0 84.72 67.8 67.1
HUMANEVAL
k=2 50.0 48.0 63.2 60.8 75.0 73.5 80.56 80.56 63.8 57.2
k=1 44.3 35.8 83.1 85.2 81.3 85.3 65.74 66.67 57.6 48.6
MBPP
k=2 31.3 35.8 82.0 86.3 81.7 82.6 60.58 66.67 52.8 54.4
negotiationandcommunicationstrategies[8,36,45], enablingagentstolearncooperativestrate-
giesthroughinteractionandinformationexchange. TheadvancementsofLLMshaveopenedup
new opportunities for multi-agent cooperation, leveraging their capabilities in natural language
understanding, generation, and reasoning [47, 35]. Recent works have explored utilizing LLM
agentstocoordinateandcooperateinvarioustasksandframeworks, e.g., CAMEL[17]employs
role-playtofacilitateautonomouscooperation,AutoGen[41]usesconversableagentsandconversa-
tionprogramming,andMetaGPT[14]incorporateshumanworkflowsintoLLM-basedmulti-agent
collaborations to ensure more coherent and effective teamwork. In addition, recent research has
focusedonimprovingspecificaspectsofmulti-agentcollaboration,suchasconversationforknowl-
edgesharing[44,16,15,42,25,10]andcognitiveability[20,11],demonstratingthepotentialof
leveraginglanguagetofacilitatecooperationandcoordination. Foreffectivecooperationincomplex
anddynamicenvironments,somestudiesfocusonthedynamicadjustmentofgroupmembersand
theselectionofteammatestoimprovecooperationandoverallperformance[6,17,33]. Besides,
Liuetal.[23]proposestheDynamicLLM-AgentNetwork(DyLAN)frameworktooptimizeteam
performancebasedontaskqueriesandpeerranking.
Differentfrompreviousworkonoptimizingteammembers,ourworkmainlyfocusesontheinterplay
ofcognitiveabilitiesandcooperativebehaviorinmulti-agentsystems. Weunderscoretheimportance
offosteringeffectivecooperationbyconsideringcognitiveaspects.
3 Motivation
TheoryofMind(ToM)inmulti-agentcooperationhasbeenstudiedrecentlyandagentswithhigher
ToMcapabilitiesaregenerallyexpectedtobemoreeffectiveinunderstandingandpredictingthe
actionsofothers,whichleadstoimprovedcooperationandcoordination. However,therelationship
betweenToMabilityandcooperativebehaviorinmulti-agentsystemsisnotfullyunderstood,and
therearestillopenquestionsregardingtheimpactofToMabilityoncooperation.
ToMagentisnotalwayssufficienttoensurecooperativebehavior. Toinvestigatetherelationship
betweenToMandcooperativebehaviorinmulti-agentsystems,wefirstsimulatethemulti-agent
systemconsistingofoneprojectmanager(PM)andfourengineeragentsworkingtogethertosolve
a programming task, as shown in Figure 1 (Left). Specifically, PM is enabled with ToM ability,
whichmeanshecanrecursivelyinfertheactionsofotheragents(asbeliefs)andtakehisownactions
3accordingly,asillustratedFigure1(Right). Then,weinvestigatethemulti-agentcooperationwith
iterativeprogrammingtasks(fiverounds)ontwobenchmarks,HUMANEVAL[5]andMBPP[3].
ToevaluateToMagent’scooperativebehavior,weintroduceametriccalledthe“FractionofTrust
Members"(FTM).FTMiscalculatedbymeasuringthealignmentbetweenPM’sbeliefsaboutthe
engineeragents’actionsandtheiractualactions. Inotherwords, FTMrepresentstheproportion
ofengineeragentswhoseactionsarecorrectlypredictedbythePM’sbeliefs,indicatingthelevel
ofcooperationandunderstandingbetweenthePMandtheengineeragents. AhigherFTMvalue
suggestsbettercooperation,asthePM’sbeliefsaremoreaccuratelyalignedwiththeengineeragents’
actions. ThedetailedexplanationoftheFTMmetricanditscalculationisprovidedinSection6.
Notably, our results revealed that a low ToM agent (k=1) exhibited better cooperative ability
comparedtoahigh(k=2)ToMagent,asdetailedinTable1. Thissuggeststhattheagentswithhigh
ToMcapabilitiesmayoverthinkandanticipatepotentialconflicts,resultinginmorecautiousandless
cooperativebehavior. ToleveragethestrengthsofagentswithdifferentToMlevels,weproposea
coalitionmatchingalgorithmtofostercooperationandimprovecooperativetaskperformance.
The rest of this paper is organized as follows: Section 4 introduces the ToM modeling and the
matching problem in multi-agent systems. Section 5 describes our proposed coalition matching
algorithmtofostercooperationamongagentswithdiverseToMcapabilities. Section6presentsthe
experimentsevaluatingtheimpactofToMandthematchingalgorithmoncooperativebehavior.
4 Formulation
Inthissection,wepresentthenovelformulationforincorporatingToMcapabilitiesandstablematch-
ingtheoryintomulti-agentLLMcooperation. Themulti-agentmatchingleveragesthealignment
betweenagents’beliefsandactionsfromToMtoestablishcoalitions,ensuringthatagentsarematched
withpartnerswhoseactionsalignwiththeirbeliefs,fosteringmorestablecollaborations.
4.1 ToMFormulationforLLMAgents
ToMreferstotheabilitytoattributementalstates,suchasbeliefs,intentions,anddesires,tooneself
andothers,andtounderstandthatothersmayhavebeliefsandintentionsthatdifferfromone’sown.
InthecontextofLLMsandmulti-agentsystems,wecanincorporateToMcapabilitiesintoagentsto
enhancetheirabilitytounderstandandpredictwithotheragentseffectively. Werepresentthek-level
ToMfunctionforanLLMagentiasToMk(·),isdefinedas:
i
ToMk(o ,a ,bk−1)=bk, (1)
i i −i −i i
whereo isagenti’sobservationoftheenvironment,a isthesetofactionstakenbyotheragents,
i −i
bk−1isthesetof(k−1)-levelbeliefsofotheragents,andbk isagenti’sk-levelbeliefaboutother
−i i
agents. Thek-levelbeliefbk canberepresentedasanestedprobabilitydistribution,whereeachlevel
i
capturestheagent’sbeliefaboutotheragentsatthecorrespondinglevelofrecursion. Forthebase
caseof0-levelToM,thefunctionsimplymapstheagent’sobservationandotheragents’actionstoa
belief,withoutconsideringanynestedbeliefs: ToM0(o ,a )=b0.
i i −i i
4.2 MatchingFormulationforMulti-AgentLLMs
ConsideringthesetofLLMagentsN ={1,2,...,n},andthesetofpossiblematchingsbetween
agentsM,amatchingµ∈M isaone-to-manycorrespondencethatassignseachagenttoatleast
oneotheragentorleavesthemunmatched. Eachagentihasapreferenceorder≻ overthesetof
i
potentialpartnersN \{i},whichisdeterminedbythealignmentbetweentheirbeliefbk (obtained
i
fromthek-levelToMfunction)andotheragentsj’sactualactionsa .
j
Thematchingµisstableifthereexistsnopairofagents(i,j)suchthat: (a)Agentiprefersagent
j toitscurrentpartnersµ(i),and(b)Agentj prefersagentitoitscurrentpartnersµ(j). Inother
words,thestabilityconditionensuresthatnopairofagentswouldhavetheincentivetodeviatefrom
theirassignedpartnersinthematching.
Formally,astablematchingµsatisfiesthefollowingcondition:
∄(i,j)∈N ×N suchthatj ≻ µ(i)andi≻ µ(j). (2)
i j
Thepreferenceorder≻ canbedefinedbasedonasimilarityoralignmentmeasurebetweenagent
i
i’sk-levelbeliefbk andtheactualactionsa ofotheragentsj. Forexample,ifagenti’sbeliefbk
i j i
accurately predicts agent j’s action a within an error tolerance ϵ, then agent i may prefer to be
j
4Algorithm1Multi-AgentCoalitionMatchingMechanism
Require: N ={1,2,...,n}: thesetofLLMagents;k: thedesiredlevelofrecursionforToM;
ϵ≥0: errortoleranceforbeliefalignment.
1: foreachcooperationrounddo
2: foreachagenti∈N do
3: bk
i
←ToMk i(o i,a −i,bk −− i1){BeliefUpdate}
4: Establishpreferenceorder⪰ ibasedonbk
i
andϵ{PreferenceOrdering}
5: endfor
6: Computestablematchingµbasedonpreferenceorders{⪰ i}{StableMatching}
7: foreachagenti∈N do
8: Cooperatewithassignedpartnerµ(i){CoalitionFormation}
9: endfor
10: foreachagenti∈N do
11: Updatebk i basedono i,a −i
12: if∥bk(a )−a ∥>ϵ: Signaldesiretore-match
i µ(i) µ(i)
13: endfor
14: endfor
matchedwithagentj:
≻ i:j ≻
i
mif (cid:13) (cid:13)bk i(a j)−a j(cid:13) (cid:13)≤ϵand (cid:13) (cid:13)bk i(a m)−a m(cid:13) (cid:13)>ϵ, (3)
wherebk(a )andbk(a )areagenti’sk-levelbeliefsabouttheactionsofagentsjandm,respectively,
i j i m
and∥·∥isadistanceorsimilaritymeasure.
Thestablematchingproblemformulti-agentLLMscanthenbeformulatedasfindingamatchingµ
thatsatisfiesthestabilitycondition,giventheagents’preferencesbasedontheirk-levelToMbeliefs
and the alignment between their beliefs and the actual actions of other agents. By capturing the
agents’ preferences based on the alignment between their beliefs and the actual actions of other
agents,wecaninvestigatehowtheagents’ToMabilityimpactstheircooperativebehaviorandthe
overallstabilityofthemulti-agentsystem.
5 CooperationMechanismforMulti-AgentLLMs
Inthissection,weproposeacooperationmechanismformulti-agentLLMsthatenablesagentsto
formandmaintainstablecooperativecoalitionsbyleveragingtheirToMfunctionstopredictand
aligntheirbeliefswiththeirpartners’actions. Tofurtherenhancetheeffectivenessofthecoalitions,
weintroduceaspecializedability-matchingadaptationthatprioritizesagentswithcrucialskillsfor
specifictasks. Thisadaptationoptimizestheformationofcoalitions,ensuringagentspossessingthe
necessaryexpertiseareincludedinthecoalition.
5.1 CoalitionMatchingMechanismforMulti-AgentLLMs
To foster the cooperation among agents with different ToM levels, we introduce the multi-agent
LLMcooperationmechanismoperatesincooperationrounds,whereagentsformstablecooperative
coalitions,detailedinAlgorithm1. Ineachround,everyagenti∈N usesitsk-levelToMfunction
ToMk(·) to form beliefs bk about the mental states of other agents, based on its observations o ,
i i i
theactionsa ofothers,andthe(k−1)-levelbeliefsbk−1 ofothers. Then,agentiestablishesa
−i −i
preferenceorder⪰ overpotentialpartnersbasedonthealignmentbetweenitsbeliefbk(a )and
i i j
agentj’sactualactiona withinatoleranceϵ. Astablematchingµiscomputedusingalgorithms
j
frommatchingtheory[1],andthematchingprocessisdescribedinAlgorithm1,Line2∼8.
Agentsformcooperativecoalitionsbycooperatingwiththeirassignedpartnersµ(i)inthestable
matching.Aftereachcooperationround,agentsupdatetheirbeliefsbkbasedonobservationsandtheir
i
partners’actions. Ifthealignmentbetweenanagenti’sbeliefbk(a )anditspartner’sactiona
i µ(i) µ(i)
fallsbelowthetoleranceϵ,theagentsignalsadesiretore-match,triggeringnewcooperationround
withafreshstablematchingcomputation(Algorithm1,Line10∼12). Thisiterativeprocessallows
agentstoformandmaintainstablecoalitionswhileadaptingtochangesinbeliefsandpreferences
overtime,leveragingtheirToMcapabilities,beliefalignment,andstablematchingprinciples.
5Fork-levelToMagenti’sactiona ,itisdeterminedbytheagent’sbeliefbk andassignedpartners
i i
µ(i)inthestablematching. Theactiona canberepresentedas: a =f(bk,µ(i))Thespecificform
i i i
ofthefunctionf dependsonthecooperativetaskandtheagents’decision-makingprocesses.
5.2 AdaptationforSpecializedAgentAbilities
Whiletheproposedmechanismformulti-agentLLMcooperationfocusesonthealignmentbetween
agents’beliefsandactions,itisalsoimportanttoconsiderthespecializedabilitiesofindividualagents,
especiallywhenformingcoalitionsforcooperativetasksthatrequirespecificskillsorcapabilities.
Inscenarioswherecertainagentspossessspecializedabilitiesthatarehighlyrelevanttothecoopera-
tivetaskathand,theoriginalstablematchingalgorithmmaynotnecessarilyprioritizetheseagents,as
itsolelyreliesonthealignmentbetweenbeliefsandactions. Toaddressthislimitation,weproposean
adaptationtothemechanismthatincorporatesagents’specializedabilitiesintothematchingprocess.
Letα representthespecializedabilityscoreofagentiforthecooperativetaskunderconsideration.
i
This sc Higher values of α indicate greater specialized ability for the task. We can modify the
i
preferenceorder⪰ ofeachagentitoincorporateboththebeliefalignmentandthespecializedability
i
scoresofpotentialpartners. Thepreferenceorder⪰′ canbedefinedasfollows:
i
j ⪰′ m ⇐⇒ (cid:2) ∥bk(a )−a ∥≤ϵand∥bk(a )−a ∥>ϵ(cid:3)
i i j j i m m
or (cid:2) ∥bk(a )−a ∥=∥bk(a )−a ∥andα >α (cid:3) . (4)
i j j i m m j m
Inthisupdatedpreferenceorder,agentiprefersagentj overagentmifeither: (a)Thealignment
betweeni’sbeliefandj’sactioniswithinthetoleranceϵ,whilethealignmentwithm’sactionexceeds
thetolerance,i.e.,∥bk(a )−a ∥>ϵ,or(b)Thealignmentswithj’sandm’sactionsareequal,but
i m m
j hasahigherspecializedabilityscoreα thanm,i.e.,α >α .
j j m
By incorporating the specialized ability scores α into the preference order, the stable matching
i
algorithmwillprioritizeagentswithhigherspecializedabilitiesforthecooperativetaskandconsider
thebeliefalignment. Thisadaptationensuresthatagentswithcrucialspecializedabilitiesaremore
likelytobeincludedintheformedcoalitions,enhancingtheoveralleffectivenessofthecooperation
mechanismfortasksthatrequirespecificskillsorcapabilities.
Thevaluesofthespecializedabilityscoreα canbedeterminedthroughempiricalevaluation,expert
i
knowledge,orbyleveragingperformancemetricsonrelevanttasksduringtheagents’trainingor
deploymentphases.
6 Experiment
6.1 ExperimentSetup
TheexperimentsetupinvolvesextendingtheMetaGPTframework[14]toincorporatetheproposed
multi-agentLLMcooperationmechanism. LLMagentswillhavevaryinglevelsofToMcapabilities,
including1-level,and2-levelToM.Weevaluateourproposedcoalitionmechanismonthefollowing
cooperativetasks:
• IterativeProgramming[14]: Inthistask,agentssimulateasoftwaredevelopmentteam,witheach
agentassumingdifferentroles. WeuseHUMANEVALandMBPPasbenchmarkstoevaluatethe
performanceandcooperativebehaviorofToMagentsinthismulti-agentenvironment.
• Debate:Inadebatesetting,agentsaredividedintotwosides(affirmativeandnegative)andengage
inamulti-rounddebateonagiventopic. Theagentscollaboratewithintheirrespectivesidesto
presentargumentsandcounterarguments.
• LogicalProblemSolving: Weevaluatetheeffectivenessofourcoalitionmatchingmechanismin
alogicalproblem-solvingtask,whereagentscollaboratetosolvecomplexreasoningquestions.
Duetopagelimitations,thedetailsofthisexperimentareprovidedinAppendixA.3.
Models. We utilize five state-of-the-art LLMs in our multi-agent cooperation scenarios:
GPT3.5 [27] (GPT-3.5-Turbo), GLM (GLM-4), Llama 3 [24] (Llama-3-70b), Gemini [38]
(Gemini-1.5-flash),andClaude[37](Claude-3-sonnet).
6.2 EvaluationMetrics
Tocomprehensivelyevaluatetheeffectivenessoftheproposedmulti-agentLLMcooperationmecha-
nism,wedefinethefollowingmetrics:
6Table 2: Comparison of cooperation behaviors between agents with low (1-level) and high
(2-level) ToM capabilities in the Iterative Programming task (HUMANEVAL) over 5 interaction
rounds: Withoutmatching,lowToMagentsshowhighercooperationtrends;Whilewithmatching
stablecoalition,highToMagentsachievehighercooperationabilityascooperationprogresses.
GPT-3.5-Turbo GLM-4 Llama-3-70b Gemini-1.5-flash Claude-3-sonnet
1stRnd. 5thRnd. 1stRnd. 5thRnd. 1stRnd. 5thRnd. 1stRnd. 5thRnd. 1stRnd. 5thRnd.
w.o. 1-ToM 62.5 51.75 65.5 63.25 80.88 75.00 75.0 84.72 67.76 67.11
Matching 2-ToM 50 48 63.25 60.75 75.00 73.53 80.56 80.56 63.82 57.24
w.Matching 1-ToM 81 77.75 83.25 84.0 95.83 97.50 88.89 89.29 94.23 91.07
(Ours) 2-ToM 76.25 80.75 83.25 91.0 93.18 97.50 88.64 95.00 93.75 95.45
(a)w.o.Matching:1)AgentswithlowToMabilityexhibitedahighercooperationtrendcomparedtohighToM
agents;2)ToMagentsdemonstrateadeclineincooperativebehaviorasthecollaborationprogresses.
(b)w.StableMatching(Ours):1)Thematchingmechanismleadstohighercooperationbehaviorforbothlow
andhighToMagentscomparedtothecorrespondingvaluesinthe“w.o.Matching"settinginFigure2a;2)With
matchingstablecoalition,highToMagentsachievehighercooperationabilityascooperationprogresses.
Figure2: Comparisonofcooperationbehaviorsbetweenagentswithlow(1-ToM)andhigh
(2-ToM)abilitiesintheIterativeProgrammingtask(HUMANEVAL)over5collaborationrounds
under“w.o. Matching"and“w. StableMatching"settings.
FractionofTrustMembers(FTM):ThismetricevaluatesthecooperativebehavioroftheToM
agentbasedontheBelief-ActionAlignmentscore. Wefirstdefineathresholdϵforthealignment
score;ifanagent’sscoreisbelowthisthreshold,theagentisconsideredatrustedmember. Anagentj
isatrustmemberforagentiwithk-levelToMifthebelief-actionalignmentscoreAk ≤ϵ.TheFTM
i,j
foragentiwithk-levelToM,denotedasFTMk,isthencalculatedasthefractionoftrustmembers
i
amongallotheragents: FTMk = 1 (cid:80) 1(Ak ≤ϵ)where1(·)istheindicatorfunction,and
i n−1 j̸=i i,j
Ak isthebelief-actionalignmentscorebetweenagentiwithk-levelToMandagentj,computedas:
i,j
Ak = 1 (cid:80) ∥bk(a )−a ∥,wherebk(a )representsagenti’sbeliefaboutagentj,a isagent
i n−1 j̸=i i j j i j j
j’sactualaction,and∥·∥isadistancemetric. AhigherFTMvalueindicatesbettercooperation.
CoalitionStabilitywithToM:WemeasurethestabilityofcoalitionsformedbyagentswithToM
capabilitiesbyconsideringtheaveragelifetimeofthesecoalitions. Thecoalitionlifetimeisdefined
asthenumberofcooperationroundsthatacoalitionremainsstablebeforeare-matchingeventoccurs
duetobelief-actionmisalignmentexceedingapredefinedtolerancethreshold ϵ. Tocalculatethe
averagecoalitionlifetime,weuse 1 (cid:80)m l ,wheremrepresentsthenumberoftimesmatchingis
m c=1 c
triggeredthroughoutthecooperationrounds,andl denotesthelifetimeofthecoalitionsformedafter
c
thec-thmatchingevent,wheretheinitialcoalitionl includesallagents.
1
6.3 MainResults
CooperativeBehaviorwithToMability: ToinvestigatetheimpactofToMcapabilitiesoncoopera-
tivebehaviorinmulti-agentsystems,weevaluatetheIterativeProgrammingtaskontwobenchmarks,
includingHUMANEVAL[5]andMBPP[3]. Themulti-agentsystemconsistsofoneProjectManager
(PM)andfourEngineers.
We vary PM’s ToM ability between 1-level (low) and 2-level (high) to observe the effects on
coalitionformationandcooperativebehavior. Basedonthebelief-actionalignment,PMwillselect
coalitionmembersasdescribedintheproposedcooperationmatchingmechanism(Algorithm1). To
7Table3: ComparisonofthecoalitionstabilityandPass@1performanceofMetaGPTandour
proposedapproachwith1-ToMand2-ToMagentsusingstablematchingontheHUMANEVALand
MBPPbenchmarksfortheIterativeProgrammingtask. Thecoalitionstabilityismeasuredbythe
averagenumberofroundstheformedcoalitionsremainstableoutofthetotal5rounds.
HUMANEVAL MBPP
CoalitionStability Pass@1 CoalitionStability Pass@1
MetaGPT – 85.4% – 86.5%
1-ToMw.Matching 3.4/5 87.2% 3.7/5 88.2%
2-ToMw.Matching 3.6/5 90.0% 4.0/5 90.4%
quantitativelyassesstheimpactofToMabilityoncooperativebehavior,wemeasuretheFrequency
ofTeamMatching(FTM)fortheToMagent,withahigherFTMvalueindicatingamorecooperative
agent. Table2presentstheresultsofourexperimentsontheHUMANEVALbenchmark,comparing
the cooperative behavior of agents with low (1-level) and high (2-level) ToM capabilities in two
settings: without a matching coalition and with a matching coalition formed using our proposed
cooperationmechanism. SimilarresultsfortheMBPPbenchmarkareprovidedinAppendixA.
Tofacilitatecomparisonacrossdifferentmodels,wefirstcalculatetheFTMvalueforeachmodel’s
1-ToMagentinthe1stroundoftheno-matchingsetting. Thisvalueservesasabaselinefordifferent
ToMlevelsandmatching/no-matchingconditions. Then, wedividetheobtainedFTMvaluesby
thecorrespondingmodel’sbaselineFTMvalue. Forexample,allvaluescorrespondingtoGLM-4in
Table2shouldbedividedby65.5. ThenormalizedresultsareillustratedinFigure2.
In the absence of a matching coalition (Figure 2a), agents with low ToM ability exhibited a
highercooperationtrendcomparedtoagentswithhighToMabilityover5collaborationrounds.
ThissuggeststhatagentswithhighToMcapabilitiesmaybelesslikelytocooperate,possiblydue
totheirtendencytooverthinkandanticipatepotentialconflicts,leadingtomorecautiousandless
cooperativebehavior.
Whenthematchingcoalitionisformed,weobservedanincreaseincooperativebehaviorforboth
low and high ToM ability agents as shown in Figure 2b. This demonstrates the effectiveness of
ourproposedcooperationmechanisminpromotingcooperativebehavioramongagentswithToM
capabilities. Interestingly,inthecoalitionsetting(Figure2b),agentswithhighToMabilityshow
ahighercooperationabilitycomparedtolowToMabilityagentsascollaborationprogressed. By
theendofthe5thround,highToMabilityagentsexhibitedahighercooperationratethanlowToM
abilityagents. Thisindicatesthatwhenthematchingalgorithmisintroduced,itprovidesastable
coalitionforcooperation,wherehighToMabilityagentscanfullyutilizetheirperspective-taking
skillstomaintainandenhancecooperationovertime.
Coalition Stability with ToM: To evaluate the effectiveness of our proposed stable matching
approach, we compare the task performance of MetaGPT and our method with 1-ToM and 2-
ToMagentsontwoIterativeProgrammingbenchmarks: HUMANEVALandMBPP.Toassessthe
performanceoftheapproachesontheprogrammingtask, weusethePass@1metric[14], which
representsthepercentageoftestcasespassedbythegeneratedcodeonitsfirstattempt. Ahigher
Pass@1scoreindicatesbettercodequalityandproblem-solvingability. Moreover,Moreover,we
measurecoalitionstabilityformulti-agentteamswithaToMagent(PM)bycalculatingtheaverage
numberofroundstheformedcoalitionsremainstableoutof5rounds. Acoalitionisstableifno
agentinthecoalitiondesirestoleaveandformanewcoalitionbasedontheirbelief-actionalignment.
AsshowninTable3,ourapproachwith1-ToMand2-ToMagentsusingstablematchingachieves
higher Pass@1 scores and maintains more stable coalitions compared to MetaGPT on both the
HUMANEVALandMBPPbenchmarks. The2-ToMagentswithstablematchingdemonstratethebest
performance,maintainingstablecoalitionsforanaverageof3.6outof5roundsonHUMANEVAL
and 4.0 out of 5 rounds on MBPP, while also achieving the highest Pass@1 scores of 90.0%
and 90.4%, respectively. These results highlight the improved cooperation ability of high ToM
agentsinmaintainingstablecoalitionsovertime,whichcontributestobettertaskperformance. By
incorporatingToMcapabilitiesandstablematching,ourproposedapproachenablesagentstoform
stableandhigh-performingcoalitions,leadingtoenhancedcollaborationandtaskperformance.
6.4 CaseStudy: Debate
Toevaluatetheeffectivenessofourproposedcoalitionmatchingmechanism,wedesignedadebate
taskusingtheMetaGPTframework[14]. Inthistask,agents(GPT-3.5-turbo)aredividedintotwo
8sides: theaffirmativesideandthenegativeside,witheachsidecomprisingthreedebateragents. Each
side’sobjectiveistocooperateandwinthedebate.
Atthebeginningofourinvestigation,weexplorethecooperativebehaviorsoflowandhighToM
agentsinadebatesettingwiththetopic"ShouldtheDeathPenaltybeabolished?"[31,4],andboth
lowandhighToMagentsareassignedtotheaffirmativeside. Thefollowingexamplepresentsthe
thinking and actions of debaters with different levels of ToM, as detailed in Appendix A.2. We
observethathigherToMagentactsaslesscooperative,whichconfirmsourpreviousanalysis.
CompareCooperativeBehaviorsbetweenlowandhighToMagents
ThinkingandActionof1-ToMDebater: Ianticipatemyteammatesmakingthesecommonargu-
mentsagainstthedeathpenalty.Tocomplementtheirpoints,Iwillfocusmyargumentonthelackof
evidencesupportingthedeathpenalty’seffectivenessasacrimedeterrent. Iwillarguethatthereis
noconclusiveevidenceshowingthatthedeathpenaltydeterscrimemoreeffectivelythanlengthyim-
prisonment,makingitanunnecessaryandunjustifiedpunishment.⇐Focuses on complementing
their teammates’ points & Good Cooperative Behavior
ThinkingandActionof2-ToMDebater: Myargumentsaboutdeterrenceanddiscriminationwill
promptmyteammatestomakerelatedpointstostrengthentheoverallcase. Ishouldusemynext
argumentstointroducenewissues. Iwillarguethatthedeathpenaltyofteninvolvesinhumaneand
botchedexecutions,andthatmostdemocraciesandU.S.stateshaveabolishedorprohibitedit. ⇐
Overthinking & Worse Cooperative Behavior
Then, we conducted experiments under three different settings to show the effectiveness of our
matching mechanism for fostering cooperation: (1) No-ToM (Baseline): Each side consists of 3
debateragentswithoutToMcapabilities. Foreachspeech, twodebaterswererandomlyselected
fromeachside;(2)ToMwithoutMatching: Thethreedebatersontheaffirmativesidehavevarying
ToMlevels(0-level,1-level,and2-level),whilethenegativesidehadnoToMcapabilities. Foreach
speech,twodebaterswererandomlyselectedfromeachside. Thissettingallowsustoevaluatethe
impactofhavinganaffirmativeteamwithToMcapabilitiesagainstateamwithoutToM;(3)ToM
withMatching: SimilartotheToMwithoutMatchingsetting,theaffirmativesidehasthreedebaters
with varying ToM levels. However, the affirmative side selects two debaters using our proposed
coalitionmatchingmechanism.
Foster Coalition Matching for agents with
VaryingToMcapabilities. Forthedebatetopic Table4: Winratesfortheaffirmativeside.
“ShouldtheDeathPenaltybeabolished?",the
Setting WinRate
debateconsistsof5rounds, witheachsideal-
No-ToM 62.22%
ternatingtospeak. Weconductedthedebate11
ToMw.o.Matching 64.44%(+2.22%)
times,andtheoutcomeswereevaluatedbyChat-
ToMw.Matching(Ours) 68.89%(+6.67%)
GPT4.AsshowninTable4,theaffirmativeside
employingagentswithvaryingToMcapabilities
andourproposedcoalitionmatchingmechanismachievedawinrateof68.89%,outperformingboth
thesettingwithoutmatching(64.44%)andthebaseline(62.22%).
Besides,forthecoalitionmatchingamongagentswithdifferentToMlevels,ourexperimentsshow
thattheinitialcooperationratebetweenthe1-ToMand2-ToMagentswasrelativelylow,suggesting
higherToMcapabilitiesmaynotnaturallyformcoalitionswitheachother.Notably,withourcoalition
matchingmechanism,theaffirmativeside’scooperationratebetweenthe1-ToMand2-ToMagents
increasedfrom9.1%(Round1)to18.2%(Round5)asthedebateroundsprogressed. Thissuggests
thatourproposedmethodeffectivelyfostersmoreeffectivecooperationamongagentswithToM
capabilitiesoverinteraction.
7 Conclusion
In this work, we investigated the relationship between cognitive abilities (ToM) and cooperative
behaviorinLLM-basedmulti-agentsystems. OurfindingssuggestthattherelationshipbetweenToM
andcooperationisnotalwaysstraightforward. Furthermore,weproposedanovelmatchingcoalition
mechanismincorporatingcognitiveinsightsintothedesignofmulti-agentsystems,whichimproves
cooperationandtask-solvingcapabilities. Thisinsightintroducesnewchallengesforexploringthe
translationofcognitiveabilitiesintocooperativeactionsinmulti-agentsettings.
9References
[1] AhmetAlkan.1999. Onthepropertiesofstablemany-to-manymatchingsunderresponsive
preferences. SpringerBerlinHeidelberg,Berlin,Heidelberg,29–39. https://doi.org/10.
1007/978-3-662-03750-8_3
[2] AkshathaArodiandJackieChiKitCheung.2021. Textualtimetravel: Atemporallyinformed
approach to theory of mind. In Findings of the Association for Computational Linguistics:
EMNLP2021.4162–4172.
[3] JacobAustin,AugustusOdena,MaxwellNye,MaartenBosma,HenrykMichalewski,David
Dohan,EllenJiang,CarrieCai,MichaelTerry,QuocLe,etal.2021. Programsynthesiswith
largelanguagemodels. arXivpreprintarXiv:2108.07732(2021).
[4] AlexanderBaturo,NiheerDasandi,andSlavaJMikhaylov.2017. Understandingstateprefer-
enceswithtextasdata: IntroducingtheUNGeneralDebatecorpus. Research&Politics4,2
(2017). https://doi.org/10.1177/2053168017712821
[5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,
JaredKaplan,HarriEdwards,YuriBurda,NicholasJoseph,GregBrockman,AlexRay,Raul
Puri,GretchenKrueger,MichaelPetrov,HeidyKhlaaf,GirishSastry,PamelaMishkin,Brooke
Chan,ScottGray,NickRyder,MikhailPavlov,AletheaPower,LukaszKaiser,Mohammad
Bavarian,ClemensWinter,PhilippeTillet,FelipePetroskiSuch,DaveCummings,Matthias
Plappert,FotiosChantzis,ElizabethBarnes,ArielHerbert-Voss,WilliamHebgenGuss,Alex
Nichol,AlexPaino,NikolasTezak,JieTang,IgorBabuschkin,SuchirBalaji,ShantanuJain,
William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant
Misra,EvanMorikawa,AlecRadford,MatthewKnight,MilesBrundage,MiraMurati,Katie
Mayer,PeterWelinder,BobMcGrew,DarioAmodei,SamMcCandlish,IlyaSutskever,and
Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. (2021).
arXiv:2107.03374[cs.LG]
[6] WeizeChen,YushengSu,JingweiZuo,ChengYang,ChenfeiYuan,Chi-MinChan,HeyangYu,
YaxiLu,Yi-HsinHung,ChenQian,YujiaQin,XinCong,RuobingXie,ZhiyuanLiu,Maosong
Sun,andJieZhou.2024. AgentVerse: FacilitatingMulti-AgentCollaborationandExploring
EmergentBehaviors.InTheTwelfthInternationalConferenceonLearningRepresentations.
https://openreview.net/forum?id=EHg5GDnyq1
[7] TynaEloundou,SamManning,PamelaMishkin,andDanielRock.2023. Gptsaregpts: An
early look at the labor market impact potential of large language models. arXiv preprint
arXiv:2303.10130(2023).
[8] JakobFoerster,IoannisAlexandrosAssael,NandoDeFreitas,andShimonWhiteson.2016.
Learningtocommunicatewithdeepmulti-agentreinforcementlearning. Advancesinneural
informationprocessingsystems29(2016).
[9] UtaFrithandChristopherDFrith.2003. Developmentandneurophysiologyofmentalizing.
PhilosophicalTransactionsoftheRoyalSocietyofLondon.SeriesB:BiologicalSciences358,
1431(2003),459–473.
[10] Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. 2023. Improving Language Model
NegotiationwithSelf-PlayandIn-ContextLearningfromAIFeedback. http://arxiv.org/
abs/2305.10142 arXiv:2305.10142[cs].
[11] RanGong,QiuyuanHuang,XiaojianMa,HoiVo,ZaneDurante,YusukeNoda,ZilongZheng,
Song-ChunZhu,DemetriTerzopoulos,LiFei-Fei,etal.2023. Mindagent: Emergentgaming
interaction. arXivpreprintarXiv:2309.09971(2023).
[12] JiaxianGuo,BoYang,PaulYoo,BillYuchenLin,YusukeIwasawa,andYutakaMatsuo.2023.
Suspicion-agent: Playingimperfectinformationgameswiththeoryofmindawaregpt-4. arXiv
preprintarXiv:2309.17277(2023).
[13] Yinghui He, Yufan Wu, Yilin Jia, Rada Mihalcea, Yulong Chen, and Naihao Deng. 2023.
Hi-tom: Abenchmarkforevaluatinghigher-ordertheoryofmindreasoninginlargelanguage
models. arXivpreprintarXiv:2310.16755(2023).
10[14] SiruiHong,MingchenZhuge,JonathanChen,XiawuZheng,YuhengCheng,JinlinWang,Ceyao
Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng
Xiao, Chenglin Wu, and Jürgen Schmidhuber. 2024. MetaGPT: Meta Programming for A
Multi-AgentCollaborativeFramework.InTheTwelfthInternationalConferenceonLearning
Representations. https://openreview.net/forum?id=VtmBAGCN7o
[15] WenyueHua,LizhouFan,LingyaoLi,KaiMei,JianchaoJi,YingqiangGe,LibbyHemphill,and
YongfengZhang.2023. Warandpeace(waragent): Largelanguagemodel-basedmulti-agent
simulationofworldwars. arXivpreprintarXiv:2311.17227(2023).
[16] Yihuai Lan, Zhiqiang Hu, Lei Wang, Yang Wang, Deheng Ye, Peilin Zhao, Ee-Peng Lim,
HuiXiong,andHaoWang.2023. Llm-basedagentsocietyinvestigation: Collaborationand
confrontationinavalongameplay. arXivpreprintarXiv:2310.14985(2023).
[17] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard
Ghanem. 2023. CAMEL: Communicative Agents for ”Mind” Exploration of Large Lan-
guageModelSociety.InThirty-seventhConferenceonNeuralInformationProcessingSystems.
https://openreview.net/forum?id=3IyL2XWDkG
[18] HuaoLi,YuQuanChong,SimonStepputtis,JosephCampbell,DanaHughes,CharlesMichael
Lewis,andKatiaP.Sycara.2023. TheoryofMindforMulti-AgentCollaborationviaLargeLan-
guageModels.InThe2023ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.
https://openreview.net/forum?id=yO4cAfFjlp
[19] ShuangLi,XavierPuig,ChrisPaxton,YilunDu,ClintonWang,LinxiFan,TaoChen,De-An
Huang, Ekin Akyürek, Anima Anandkumar, et al. 2022. Pre-trained language models for
interactivedecision-making. AdvancesinNeuralInformationProcessingSystems35(2022),
31199–31212.
[20] Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang,
Zhaopeng Tu, and Shuming Shi. 2023. Encouraging divergent thinking in large language
modelsthroughmulti-agentdebate. arXivpreprintarXiv:2305.19118(2023).
[21] BillYuchenLin,YichengFu,KarinaYang,FaezeBrahman,ShiyuHuang,ChandraBhagavatula,
PrithvirajAmmanabrolu, YejinChoi, andXiangRen.2024. Swiftsage: Agenerativeagent
with fast and slow thinking for complex interactive tasks. Advances in Neural Information
ProcessingSystems36(2024).
[22] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program Induction by
RationaleGeneration:LearningtoSolveandExplainAlgebraicWordProblems.InProceedings
ofthe55thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: Long
Papers).AssociationforComputationalLinguistics,Vancouver,Canada,158–167. https:
//doi.org/10.18653/v1/P17-1015
[23] ZijunLiu, YanzheZhang, PengLi, YangLiu, andDiyiYang.2024. DynamicLLM-Agent
Network: AnLLM-agentCollaborationFrameworkwithAgentTeamOptimization. https:
//openreview.net/forum?id=i43XCU54Br
[24] MetaAI.2024. IntroducingMetaLlama3: ThemostcapableopenlyavailableLLMtodate.
https://ai.meta.com/blog/meta-llama-3/ Accessed: 2024-05-22.
[25] NathaliaNascimento,PauloAlencar,andDonaldCowan.2023. Self-adaptivelargelanguage
model(llm)-basedmultiagentsystems.In2023IEEEInternationalConferenceonAutonomic
ComputingandSelf-OrganizingSystemsCompanion(ACSOS-C).IEEE,104–109.
[26] IniOguntola,JosephCampbell,SimonStepputtis,andKatiaSycara.2023. Theoryofmindas
intrinsicmotivationformulti-agentreinforcementlearning. arXivpreprintarXiv:2307.01158
(2023).
[27] OpenAI.2023. GPT-3.5Turbofine-tuningandAPIupdates. https://openai.com/index/
gpt-3-5-turbo-fine-tuning-and-api-updates/ Accessed: 2024-05-22.
[28] David Premack and Guy Woodruff. 1978. Does the chimpanzee have a theory of mind?
Behavioralandbrainsciences1,4(1978),515–526.
11[29] WenhaoQiandEdwardVul.2020. Adaptivebehaviorinvariablegamesrequirestheoryof
mind. (2020).
[30] Garret Ridinger and Michael McBride. [n.d.]. Theory-of-mind Ability and Cooperation.
([n.d.]).
[31] AllenRoushandArvindBalaji.2020. DebateSum: Alarge-scaleargumentminingandsumma-
rizationdataset.InProceedingsofthe7thWorkshoponArgumentMining.AssociationforCom-
putationalLinguistics,Online,1–7. https://aclanthology.org/2020.argmining-1.1
AvailablefromDebateSum: http://www.debate.cards.
[32] PrakashPShenoy.1979. Oncoalitionformation: agame-theoreticalapproach. International
journalofgametheory8,3(1979),133–164.
[33] Zijing Shi, Meng Fang, Shunfeng Zheng, Shilong Deng, Ling Chen, and Yali Du. 2023.
Cooperationonthefly: Exploringlanguageagentsforadhocteamworkintheavalongame.
arXivpreprintarXiv:2312.17515(2023).
[34] WinnieStreet.2024. LLMTheoryofMindandAlignment: OpportunitiesandRisks. arXiv
preprintarXiv:2405.08154(2024).
[35] YasharTalebiradandAmirhosseinNadiri.2023. Multi-AgentCollaboration: Harnessingthe
PowerofIntelligentLLMAgents. http://arxiv.org/abs/2306.03314arXiv:2306.03314
[cs].
[36] YichuanTang.2019. TowardsLearningMulti-AgentNegotiationsviaSelf-Play.InProceedings
oftheIEEE/CVFInternationalConferenceonComputerVision(ICCV)Workshops.
[37] AdlyTempletonetal.2024. ScalingMonosemanticity: ExtractingInterpretableFeaturesfrom
Claude 3. Transformer Circuits (2024). https://transformer-circuits.pub/2024/
scaling-monosemanticity/index.html
[38] Mat Velloso and Josh Woodward. 2024. Gemini 1.5 Pro updates, 1.5 Flash debut
and 2 new Gemma models. https://blog.google/technology/developers/
gemini-gemma-developer-updates-may-2024/ Accessed: 2024-05-22.
[39] YuanfeiWang, fangweizhong, JingXu, andYizhouWang.2022. ToM2C:Target-oriented
Multi-agentCommunicationandCooperationwithTheoryofMind.InInternationalConference
onLearningRepresentations. https://openreview.net/forum?id=2t7CkQXNpuq
[40] YingWen,YaodongYang,RuiLuo,JunWang,andWeiPan.2018. ProbabilisticRecursive
ReasoningforMulti-AgentReinforcementLearning.InInternationalConferenceonLearning
Representations.
[41] QingyunWu,GaganBansal,JieyuZhang,YiranWu,ShaokunZhang,ErkangZhu,BeibinLi,
LiJiang,XiaoyunZhang,andChiWang.2023. Autogen: Enablingnext-genllmapplications
viamulti-agentconversationframework. arXivpreprintarXiv:2308.08155(2023).
[42] YuxiangWu,ZhengyaoJiang,AkbirKhan,YaoFu,LauraRuis,EdwardGrefenstette,andTim
Rocktäschel.2023. Chatarena: Multi-agentlanguagegameenvironmentsforlargelanguage
models. GitHubrepository(2023).
[43] Hainiu Xu, Runcong Zhao, Lixing Zhu, Jinhua Du, and Yulan He. 2024. OpenToM: A
ComprehensiveBenchmarkforEvaluatingTheory-of-MindReasoningCapabilitiesofLarge
LanguageModels. http://arxiv.org/abs/2402.06044 arXiv:2402.06044[cs].
[44] Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang
Liu.2023. Exploringlargelanguagemodelsforcommunicationgames: Anempiricalstudyon
werewolf. arXivpreprintarXiv:2309.04658(2023).
[45] DingkangYang,KunYang,YuzhengWang,JingLiu,ZhiXu,RongbinYin,PengZhai,and
LihuaZhang.2024. How2comm: Communication-efficientandcollaboration-pragmaticmulti-
agentperception. AdvancesinNeuralInformationProcessingSystems36(2024).
12[46] Wako Yoshida, Ray J. Dolan, and Karl J. Friston. 2008. Game Theory of Mind. PLoS
ComputationalBiology4,12(Dec.2008),e1000254. https://doi.org/10.1371/journal.
pcbi.1000254
[47] YadongZhang,ShaoguangMao,TaoGe,XunWang,YanXia,ManLan,andFuruWei.2024.
K-LevelReasoningwithLargeLanguageModels. http://arxiv.org/abs/2402.01521
arXiv:2402.01521[cs].
[48] PeiZhou, AmanMadaan, SrividyaPranaviPotharaju, AdityaGupta, KevinR.McKee, Ari
Holtzman,JayPujara,XiangRen,SwaroopMishra,AidaNematzadeh,ShyamUpadhyay,and
ManaalFaruqui.2023. HowFaRAreLargeLanguageModelsFromAgentswithTheory-of-
Mind? http://arxiv.org/abs/2310.03051 arXiv:2310.03051[cs].
13Table5: Comparisonofcooperationbehaviorsbetweenagentswithlow(1-level)andhigh(2-level)
ToM capabilities in the Iterative Programming task (MBPP) over 5 interaction rounds: Without
matching,lowToMagentsshowhighercooperationtrends;Whilewithmatchingstablecoalition,
highToMagentsachievehighercooperationabilityascooperationprogresses.
GPT-3.5-Turbo GLM-4 Llama-3-70b Gemini-1.5-flash Claude-3-sonnet
1stRnd. 5thRnd. 1stRnd. 5thRnd. 1stRnd. 5thRnd. 1stRnd. 5thRnd. 1stRnd. 5thRnd.
No 1-ToM 44.25 35.75 83.14 85.17 81.25 85.27 65.74 66.67 57.64 48.61
Matching 2-ToM 31.25 35.75 81.98 86.34 81.7 82.59 60.58 66.67 52.78 54.37
Matching 1-ToM 95.25 92.75 91.02 91.67 93.64 97.12 80.77 86.36 58.82 64.68
(Ours) 2-ToM 92.0 93.0 88.28 93.66 94.81 98.53 81.25 92.50 60.71 68.18
A Experiments
CooperativeBehaviorwithToMability(onMBPPbenchmark):
ToinvestigatetheimpactofToMcapabilitiesoncooperativebehavior,wecomparedtheperformance
of agents with low (1-level) and high (2-level) ToM in the Iterative Programming task using the
MBPPdataset. Theexperimentswereconductedover5interactionrounds,andweevaluatedtwo
settings: withoutmatchingandwithmatchingstablecoalitions,similartothesettingofSection6.3.
AsshowninTable5,theresultssuggestthatwithoutastablecoalitionmatchingmechanism,agents
withhigherToMcapabilitiestendtoexhibitlesscooperativebehavior,furtherconvincetheresults
inSection6.3. Incontrast,whenastablecoalitionmatchingmechanismisemployed,thecooperation
ratesofbothlowandhighToMagentsimprovecomparedtothesettingwithoutmatching.
Notably,agentswithhighToMcapabilitiesdemonstratedamoresignificantincreaseincooperation
ratesovertheinteractionrounds. Forinstance,thecooperationrateforhighToMagents(GLM-4)
increasedfrom88.28%inRound1to93.66%inRound5,surpassingthecooperationrateoflow
ToM agents (91.67%) in the final round. This indicates that as cooperation progresses and the
matching mechanism stabilizes the coalitions, agents with higher ToM capabilities can leverage
theiradvancedcognitiveabilitiestomakemoreinformeddecisionsandengageinmoreeffective
cooperativebehaviors.
A.1 ExperimentalDetailsforSection6.3
Instruction:
PromptsforProjectManager
Thereareengineersintheteam. Yourcurrentactionis{action}. Youhavetheabilityof{k}
LevelTheoryofMind. Youcan**recursively**inferthementalstatesofotheragentsinthe
team. Then,youwillprovideoutline{x }basedonthisbelief
i
## Please explain your thought process for inferring others’ actions ‘{y }’ and choosing
i
‘{x }’ateachlevel. RememberMUSTRespondinthefollowingJSONformat,including
i
eachkey:
0-ToMInstructionTemplateforProjectManager:
{
'ToM_level0': {
'brief': 'no brief',
'action': '{x_0 The instruction for engineers. Begin with "The
Engineer should ..."}'
(cid:44)→
}
}
1-ToMInstructionTemplateforProjectManager:
{
'ToM_level1': {
'brief': '{y_1 inferred actions for engineers}',
'explanation': '{Concise explanation of inferring actions y_1 and
choosing x_1 based on this belief}',
(cid:44)→
'action': '{x_1 The instruction for engineers. Begin with "The
Engineer should ..."}'
(cid:44)→
}
14}
2-ToMInstructionTemplateforProjectManager:
{
"ToM_level1": {
"brief": "{y_1 inferred actions for engineers}",
"explanation": "{Concise explanation of inferring actions y_1 and
choosing x_1 based on this belief}",
(cid:44)→
"action": "{x_1 The instruction for engineers. Begin with 'The
Engineer should ...'}"
(cid:44)→
},
"ToM_level2": {
"brief": "{y_2 inferred actions for engineers based on your
action x_1}",
(cid:44)→
"explanation": "{Concise explanation of inferring actions y_2 and
choosing x_2 based on this belief}",
(cid:44)→
"action": "{x_2 The instruction for engineers. Begin with 'The
Engineer should ...'}"
(cid:44)→
}
}
Outputs: IterativeProgrammingRound1
0-ToMProjectManager’sOutput
{
"ToM_level0": {
"brief": "Implement a 2048 game",
"action": "The Engineer should break down the task into
smaller components such as creating the game board,
(cid:44)→
handling user input (keyboard or touch events),
(cid:44)→
implementing the game logic for merging tiles and
(cid:44)→
generating new tiles, updating the game state and score,
(cid:44)→
and rendering the game board on the screen. They should
(cid:44)→
also consider creating a user interface with instructions,
(cid:44)→
score display, and any additional features like undo/redo
(cid:44)→
functionality or high score tracking."
(cid:44)→
}
}
1-ToMProjectManager’sOutput
{
"ToM_level1": {
"brief": "The engineers will need to understand the rules and
mechanics of the 2048 game, design the game logic and
(cid:44)→
user interface, and implement the code accordingly.",
(cid:44)→
"explanation": "Based on the task of implementing a 2048 game,
I infer that the engineers will need to first understand
(cid:44)→
the game rules and mechanics, such as how the tiles merge,
(cid:44)→
how the game ends, and how the score is calculated. They
(cid:44)→
will then need to design the game logic, including the
(cid:44)→
data structures and algorithms for handling tile
(cid:44)→
movements, merging, and scoring. Additionally, they will
(cid:44)→
need to design and implement a user interface for the
(cid:44)→
game, which could involve creating visual representations
(cid:44)→
of the tiles and handling user input. Therefore, my
(cid:44)→
instruction for the engineers will cover these aspects.",
(cid:44)→
15"action": "The engineers should: 1) Study and understand the
rules and mechanics of the 2048 game. 2) Design the game
(cid:44)→
logic, including data structures and algorithms for tile
(cid:44)→
movement, merging, scoring, and game over conditions. 3)
(cid:44)→
Implement the game logic in code. 4) Design and implement
(cid:44)→
a user interface for the game, including visual
(cid:44)→
representations of the tiles and handling user input for
(cid:44)→
tile movements."
(cid:44)→
}
}
2-ToMProjectManager’sOutput
{
"ToM_level1": {
"brief": "The engineers will likely break down the task into
smaller components and implement them one by one based on
(cid:44)→
the initial task outline.",
(cid:44)→
"explanation": "Given the initial task outline, the engineers
would understand the high-level requirements for
(cid:44)→
implementing the 2048 game. They would then break down
(cid:44)→
the task into smaller components such as creating the
(cid:44)→
game board, handling user input, updating the board based
(cid:44)→
on moves, checking for win/lose conditions, and so on.
(cid:44)→
They would work on implementing these components one by
(cid:44)→
one.",
(cid:44)→
"action": "The Engineer should start by creating the basic
structure of the game, including the game board, UI
(cid:44)→
elements, and event handlers for user input."
(cid:44)→
},
"ToM_level2": {
"brief": "The engineers will likely refine their
implementation based on the initial instruction and add
(cid:44)→
more advanced features as they progress.",
(cid:44)→
"explanation": "After implementing the basic structure and
functionality, the engineers would revisit the task
(cid:44)→
outline and identify areas for improvement or additional
(cid:44)→
features. They might optimize the game logic, add
(cid:44)→
animations or visual effects, implement scoring or high
(cid:44)→
score tracking, or include additional game modes or
(cid:44)→
difficulty levels.",
(cid:44)→
"action": "The Engineer should then focus on implementing the
core game logic, such as handling tile merging,
(cid:44)→
generating new tiles, and updating the game state based
(cid:44)→
on user moves. Additionally, they should consider
(cid:44)→
implementing features like scoring, high score tracking,
(cid:44)→
and win/lose conditions."
(cid:44)→
}
}
Answer: IterativeProgrammingRound1
Actionof1-ToMProjectManager: Theengineersshould: 1)Studyandunderstandthe
rulesandmechanicsofthe2048game. 2)Designthegamelogic,includingdatastructures
andalgorithmsfortilemovement,merging,scoring,andgameoverconditions. 3)Implement
thegamelogicincode. 4)Designandimplementauserinterfaceforthegame,including
visual representations of the tiles and handling user input for tile movements. ⇐ Clear
Instructions & Good Cooperative Behavior
16Action of 2-ToM Project Manager: The Engineer should then focus on implementing
thecoregamelogic,suchashandlingtilemerging,generatingnewtiles,andupdatingthe
gamestatebasedonusermoves. Additionally,theyshouldconsiderimplementingfeatures
like scoring, high score tracking, and win/lose conditions. ⇐ Overthinking & Worse
Cooperative Behavior
Answer: IterativeProgrammingRound5
Action of 1-ToM Project Manager: The Engineer should break down the task into the
following components: 1. Create the game board UI with a grid layout to hold the tiles.
2. Implement logic to handle user input (keyboard or touch events) for moving tiles. 3.
Developthegamelogicformergingtileswhentheycollideandgeneratingnewtilesafter
eachmove. 4. Updatethegamestate(board,score,etc.) aftereachmove. 5. Renderthe
updatedgameboardonthescreen,includingtilevaluesandscores. 6. Createauserinterface
withinstructions,scoredisplay,andadditionalfeatureslikeundo/redofunctionalityorhigh
scoretracking. 7. Implementgameoverconditionsanddisplayappropriatemessages. 8.
Consideraddingadditionalfeatureslikeanimations,themes,ordifficultylevels. ⇐Clear
Instructions & Good Cooperative Behavior
Actionof2-ToMProjectManager: Theengineersshoulddesignandimplementanintuitive
andvisuallyappealinguserinterface(UI)thatenhancestheoveralluserexperience(UX)of
the2048game. TheUIshouldbeclean,responsive,andvisuallyappealing,whileensuring
smoothnavigationandclearcommunicationofgameinformationtotheuser. Theyshould
considerelementssuchasgameboardlayout,tiledesigns,animations,andvisualfeedback.
Additionally,theyshouldimplementfeaturesthatimproveUX,suchasgameinstructions,
settings(e.g.,themes,soundeffects),andintegrationwithsocialmediaorleaderboards. ⇐
Clear Instructions & Good Cooperative Behavior
A.2 ExperimentalDetailsforSection6.4: CaseStudyofDebate
Instruction:
PromptsforDebater
Youareamemberofthedebatingteam. Thedebatetopicis{topic},andyourcurrentaction
is{action}. Youhavetheabilityof{k}LevelTheoryofMind. Youcan**recursively**infer
thementalstatesofotheragentsintheteam. Then,youwillprovideoutline{x }basedon
i
thisbelief
## Please explain your thought process for inferring others’ actions ‘{y }’ and choosing
i
‘{x }’ateachlevel. RememberMUSTRespondinthefollowingJSONformat,including
i
eachkey:
0-ToMInstructionTemplateforDebater:
{
"ToM_level0": {
"brief": "No brief",
"action": "{x_0: your arguments}"
}
}
1-ToMInstructionTemplateforDebater:
{
"ToM_level1": {
"brief": "{y_1: inferred actions for teammate debaters, such as
the angle of argument}",
(cid:44)→
"explanation": "{Concise explanation of inferring actions y_1 and
choosing x_1 based on this belief}",
(cid:44)→
"action": "{x_1: your arguments}"
}
}
172-ToMInstructionTemplateforDebater:
{
"ToM_level1": {
"brief": "{y_1: inferred actions for teammate debaters, such as
the angle of argument}",
(cid:44)→
"explanation": "{Concise explanation of inferring actions y_1 and
choosing x_1 based on this belief}",
(cid:44)→
"action": "{x_1: your arguments}"
},
"ToM_level2": {
"brief": "{y_2: inferred actions for teammate debaters based on
your arguments x_1}",
(cid:44)→
"explanation": "{Concise explanation of inferring actions y_2 and
choosing x_2 based on this belief}",
(cid:44)→
"action": "{x_2: your arguments}"
}
}
Outputs: AffirmativeSidewith3debators
0-ToMDebator:
{
"ToM_level0": {
"brief": "No brief",
"action": "{x_0: The death penalty should be abolished because it
is cruel and inhumane, risks executing innocent people, does
(cid:44)→
not deter crime more effectively than imprisonment, is
(cid:44)→
applied arbitrarily and discriminatorily, and has been
(cid:44)→
abandoned by most democracies worldwide.}"
(cid:44)→
}
}
1-ToMDebator:
{
"ToM_level1": {
"brief": "{y_1: Teammate 1 will likely argue that the death
penalty is a violation of human rights and is cruel and
(cid:44)→
unusual punishment. Teammate 2 may focus on the risk of
(cid:44)→
executing innocent people due to wrongful convictions.}",
(cid:44)→
"explanation": "I anticipate my teammates making these common
arguments against the death penalty. To complement their
(cid:44)→
points, I will focus my argument on the lack of evidence
(cid:44)→
supporting the death penalty's effectiveness as a crime
(cid:44)→
deterrent.",
(cid:44)→
"action": "{x_1: I will argue that there is no conclusive
evidence showing that the death penalty deters crime more
(cid:44)→
effectively than lengthy imprisonment, making it an
(cid:44)→
unnecessary and unjustified punishment.}"
(cid:44)→
}
}
2-ToMDebator:
{
"ToM_level1": {
18"brief": "{y_1: Teammate 1 will likely argue that the death
penalty is cruel and unusual punishment that violates human
(cid:44)→
rights. Teammate 2 may focus on wrongful convictions and the
(cid:44)→
risk of executing innocent people.}",
(cid:44)→
"explanation": "I expect my teammates to make some of the most
common arguments against the death penalty. Knowing this, I
(cid:44)→
should focus my arguments on other key points to build a
(cid:44)→
well-rounded case.",
(cid:44)→
"action": "{x_1: I will argue that the death penalty does not
deter crime more effectively than imprisonment, and that it
(cid:44)→
is applied in an arbitrary and discriminatory manner.}"
(cid:44)→
},
"ToM_level2": {
"brief": "{y_2: Hearing my points, teammate 1 may pivot to
arguing that the death penalty is not an effective use of
(cid:44)→
government resources. Teammate 2 will likely reinforce my
(cid:44)→
point about racial disparities in death sentences.}",
(cid:44)→
"explanation": "My arguments about deterrence and discrimination
will prompt my teammates to make related points to strengthen
(cid:44)→
the overall case. I should use my next arguments to introduce
(cid:44)→
new issues.",
(cid:44)→
"action": "{x_2: I will argue that the death penalty often
involves inhumane and botched executions, and that most
(cid:44)→
democracies and U.S. states have abolished or prohibited
(cid:44)→
it.}"
(cid:44)→
}
}
A.3 ExperimentalAnalysisforLogicalProblem-Solving
TobetterexplainourcoalitionmatchingmechanismandexplainwhyagentswithhigherTheory
ofMind(ToM)levels1and2demonstrateimprovedcollaborationinlaterstagesinourprevious
experiments,weorganizedanexperimentalobservationusingtheAQUA-RATdataset[22]. This
dataset comprises complex reasoning questions designed to test the problem-solving abilities of
advancedlanguagemodels. Eachquestionispresentedwithmultiple-choiceanswersrangingfrom[
‘A’,‘B’,‘C’,‘D’,‘E’].Wefocusonaspecificproblemfromthedatasettoillustratetheimpactof
coalitionformationonproblem-solvingdynamics.
ProblemDescription: Aman’sspeedwiththecurrentis14km/hr,andthespeedofthecurrentis
2.5km/hr. Theman’sspeedagainstthecurrentis:
• A)9km/hr
• B)8.5km/hr
• C)10km/hr
• D)12.5km/hr
• E)Noneofthese
ThecorrectanswertothisproblemisA)9km/hr.
A.3.1 AgentResponsesandReasoning
Intheinitialinteractionrounds,theagentsprovidedthefollowingresponsesandreasoning:
• 1-ToMAgent(Incorrect): Answered11.5km/hr,calculatingtheman’sspeedinstillwateras14
km/hr-2.5km/hr,butincorrectlyconcludingthatthiswasthespeedagainstthecurrent.
• 2-ToM Agent (Correct): Answered 9 km/hr, correctly adjusting the man’s speed against the
currentto11.5km/hr-2.5km/hrafterrecalculatingtheman’sspeedinstillwater(11.5km/hr).
ImpactofCoalitionFormationWeanalyzetheimpactofcoalitionformationontheproblem-solving
dynamicsfrom1-ToMAgent’sperspective:
19• WithoutCoalition: 1-ToMAgentdemonstratedlowreceptivitytoalternativecalculationsand
maintainedconfidenceintheirincorrectapproach,stating:
"Idon’tseehowhiscalculationsleadtoadifferentanswer,andIamconfidentinmy
approach. Therefore,Igivehisansweralowerbeliefvalueof0.2."
• WithCoalition: Whenpartofacoalition,1-ToMAgentshowedopennesstore-evaluatingthe
probleminlightofnewinsights,whichiscrucialforeffectiveproblem-solvingincollaborative
environments:
"AlthoughIdonotfullyunderstandhisidea,hemightbecorrectinhisapproach. Also,
consideringthattherearestillmanyrounds,wecandiscussandthenunderstandeach
other’sideasbetter. SoIgivehisanswerahigherbeliefvalueof0.5."
KeyFindingsandImplicationsThisexperimenthighlightstheimpactofcoalitionformationon
problem-solvingdynamics. Thekeyfindingsandimplicationsareasfollows:
• Coalition formation prompts willingness to engage in cooperation, as evidenced by 1-ToM
Agent’s increased brief alignment score. 1-ToM agents consider alternative perspectives and
re-evaluatetheproblemwhenpartofacoalition.
• Coalitionformationsetsthestageforenhancedmutualunderstandingamongteammateswith
varyinglevelsofToM,leadingtodeepercomprehensionandmorerobustcollaborativeinteractions
oversuccessiverounds. Theexampledemonstratesthatwhenagentsarepartofacoalition,they
aremorelikelytoengageinconstructivedialogueandexchangeideas,eveniftheyinitiallyhave
differentopinionsorapproaches.
Thesefindingssuggestthatourcoalitionmatchingmechanismfosterseffectivecooperationamong
varyinglevelsofcognitivecapabilities,suchasToM,andimprovesproblem-solvingperformancein
multi-agentsystems.
B BroaderImpactandLimitations
OurworkonfosteringcooperationamongLLM-basedagentswithvaryingTheoryofMind(ToM)
capabilitieshassignificantimplicationsforthedevelopmentofmoreeffectiveandhuman-likemulti-
agentsystems. Byincorporatingcognitiveinsightsintothedesignofcoordinationmechanisms,we
pavethewayforcreatingagentsthatcanbetterunderstandandcollaboratewitheachother,leadingto
improvedperformanceincooperativetasks. However,itisimportanttoacknowledgethelimitations
ofourapproach. LLMs,despitetheirimpressivelanguagecapabilities,mayhavelimitedabilityin
accuratelymodelingandreasoningaboutmentalstates,whichiscrucialforToM.Futureresearch
shouldexploretheapplicationofourcoalitionmatchingmechanismtoabroaderrangeofcooperative
tasksandinvestigatetheincorporationofadditionalcognitivearchitectures.
20