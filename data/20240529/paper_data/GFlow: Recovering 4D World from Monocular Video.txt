GFlow: Recovering 4D World from Monocular Video
ShizunWang XingyiYang QiuhongShen ZhenxiangJiang XinchaoWang‚àó
NationalUniversityofSingapore
shizun.wang@u.nus.edu xinchao@nus.edu.sg
teaser Frame 35
2D Tracking 3D Tracking
Still
Cluster Moving
Cluster
Zero-shot Segmentation Consistent Depth
A) Monocular Video Input Novel View Synthesis Scene Editing
w/o Camera Parameters B) Center points of Reconstructed Gaussians in GFlow C) Downstream Video Applications
Figure 1: A) Given a monocular video in the wild, B) our proposed GFlow can reconstruct the
underlying4Dworld,i.e.thedynamicscenerepresentedby3DGaussiansplatting[10]andassociated
GFlow
cameraposes. TheGaussiansaresplitintostillandmovingclustersandaredensifiedwithinGFlow.
C)FromGFlowwecanstraightforwardlyenablevariouscapacities,suchastracking,segmentation,
novelviewsynthesis,andediting. Bestviewedbyzoomingin.
Abstract
Reconstructing 4D scenes from video inputs is a crucial yet challenging task.
Conventionalmethodsusuallyrelyontheassumptionsofmulti-viewvideoinputs,
knowncameraparameters,orstaticscenes,allofwhicharetypicallyabsentunder
in-the-wild scenarios. In this paper, we relax all these constraints and tackle a
highly ambitious but practical task, which we termed as AnyV4D: we assume
onlyonemonocularvideoisavailablewithoutanycameraparametersasinput,
andweaimtorecoverthedynamic4Dworldalongsidethecameraposes. Tothis
end,weintroduceGFlow,anewframeworkthatutilizesonly2Dpriors(depth
andopticalflow)toliftavideo(3D)toa4Dexplicitrepresentation,entailinga
flowofGaussiansplattingthroughspaceandtime. GFlowfirstclustersthescene
into still and moving parts, then applies a sequential optimization process that
optimizes camera poses and the dynamics of 3D Gaussian points based on 2D
priorsandsceneclustering,ensuringfidelityamongneighboringpointsandsmooth
movementacrossframes. Sincedynamicscenesalwaysintroducenewcontent,
we also propose a new pixel-wise densification strategy for Gaussian points to
integratenewvisualcontent. Moreover,GFlowtranscendstheboundariesofmere
4D reconstruction; it also enables tracking of any points across frames without
the need for prior training and segments moving objects from the scene in an
unsupervisedway. Additionally,thecameraposesofeachframecanbederived
‚àóCorrespondingAuthor
Preprint.Underreview.
4202
yaM
82
]VC.sc[
1v62481.5042:viXrafromGFlow,allowingforrenderingnovelviewsofavideoscenethroughchanging
camerapose. Byemployingtheexplicitrepresentation,wemayreadilyconduct
scene-levelorobject-leveleditingasdesired,underscoringitsversatilityandpower.
Visitourprojectwebsiteat: littlepure2333.github.io/GFlow.
1 Introduction
Thequestforaccuratereconstructionof4Dworlds(3D+t)fromvideoinputsstandsattheforefrontof
contemporaryresearchincomputervisionandgraphics. Thisendeavoriscrucialfortheadvancement
ofvirtualandaugmentedreality,videoanalysis,andmultimediaapplications.Thechallengeprimarily
stems from capturing the transient essence of dynamic scenes and the often absent camera pose
information.Traditionalapproachesaretypicallysplitbetweentwotypes:onereliesonpre-calibrated
camera parametersor multi-view videoinputs toreconstruct dynamicscenes [35, 17, 30, 1, 3, 5,
12,15,16,25],whiletheotherestimatescameraposesfromstaticscenesusingmulti-viewstereo
techniques[2,6,32,14,34,36,27,26,31,4]. Thisdivisionhighlightsamissingpieceinthisfield:
Isitpossibletoreconstructdynamic3Dscenesandcameramotionfromasingle,uncalibrated
videoinputalong?
We name this task "Any Video-to-4D", or "AnyV4D" for short. Addressing this challenge is
particularlydifficultduetotheproblem‚Äôsinherentcomplexity. Attemptingtoreconstructdynamic
3Dworldsfromsingle-camerafootageinvolvesdecipheringapuzzlewheremultiplesolutionsmight
visuallyseemcorrectbutdonotadheretothephysicalcompositionofourworld. AlthoughNeRF-
basedmethodsattempttosolvethisproblem, theyfallshortofaccuratelycapturingthephysical
constraintsoftherealworld. Thislimitationstemsfromtheirimplicitrepresentation,whichstruggles
toencodetheunderlyingphysicalpropertiesofmaterialsandenforcereal-worldphysicalinteractions.
Recentdevelopmentsin3DGaussianSplatting(3DGS)[10]anditsextensions[35,17,40,39]into
dynamicsceneshaveemergedaspromisingalternatives. Thesetechniqueshaveshownpromisein
handlingthecomplexitiesassociatedwiththedynamicnatureofreal-worldscenesandtheintricacies
ofcameramovementandpositioning. Yet,theystilloperateundertheassumptionofaknowncamera
sequence[27,26]. Toovercometheselimitationsandunlockthefullpotentialofdynamicscene
reconstruction,weproposeanewapproachbasedonthefollowinginsight:
Given2DcharacterssuchasRGB,depthandopticalflowfromonevideo,weactuallyhaveenough
cluestomodelthe4D(3D+t)worldbehindthevideo(2D+t).
Based on this intuition, we introduce "GFlow", a novel framework that leverages the explicit
representationpowerof3DGaussianSplatting[10]andconceptualizesthevideocontentasafluid
flowofGaussianpointsthroughspaceandtime,effectivelyreconstructinga4Dworldwithoutdirect
camerainput.
ThekeytoGFlowisconductingsceneclusteringthatseparatesthesceneintostillandmovingparts,
followedbyasequentialoptimizationprocessthatseamlesslyintegratesprecisetuningofcamera
poseswithdynamicadjustmentsof3DGaussianpoints. Thisdualoptimizationutilizesdepthand
opticalflowpriorstoensurethateachvideoframeisaccuratelyrendered,mirroringthedynamic
fluxoftheoriginalscenewhileincorporatingnewvisualinformationthroughournewlydesigned
pixel-wisedensificationstrategy. Thisframeworknotonlymaintainscross-framerenderingfidelity
butalsoensuressmoothtransitionsandmovementsamongpoints,addressingthecriticalchallengeof
temporalcoherence.
Moreover, through our experiments, GFlow has demonstrated not just its potential as a tool for
3Dscenerecoverybutasatransformativeforceinvideoanalysisandmanipulation. Itsabilityto
trackanypointsacrossframesin3Dworldcoordinateswithoutpriortrainingandsegmentmoving
objectsfromthesceneinanunsupervisedmannerredefinesthelandscapeofvideounderstanding. By
employingexplicitrepresentationthrough3DGS,GFlowcanrenderenthrallingnewviewsofvideo
scenesbyeasilychangingcameraposesandeditingobjectsorentirescenesasdesired,showcasing
itsunparalleledversatilityandpower.
22 Preliminaries
2.1 3Dgaussiansplatting
3DGaussianSplatting(3DGS)[10]exhibitsstrongperformanceandefficiencyinrecentadvances
in3Drepresentation. 3DGSfitsasceneasasetofGaussians{G }frommulti-viewimages{V }
i k
and paired camera poses {P } in an optimization pipeline. Adaptive densification and pruning
k
of Gaussians are applied in this iterative optimization to control the total number of Gaussians.
Generally,eachGaussianiscomposedofitscentercoordinate¬µ ‚àà R3,3Dscales ‚àà R3,opacity
Œ± ‚àà R,rotationquaternionq ‚àà R4,andassociatedview-dependentcolorrepresentedasspherical
harmonicsc‚ààR3(d+1)2,wheredisthedegreeofsphericalharmonics.
These parameters can be collectively denoted by G, with G = {¬µ ,s ,Œ± ,q ,c } denoting the
i i i i i i
parameters of the i-th Gaussian. The core of 3DGS is its tile-based differentiable rasterization
pipeline to achieve real-time optimization and rendering. To render {G } into a 2D image, each
i
GaussianisfirstprojectedintothecameracoordinateframegiventhecameraposeP todetermine
i
thedepthofeachGaussian. Thencolors,depth,orotherattributesinpixelspacearerenderedin
parallelbyalphacompositionwiththedepthorderofadjacent3DGaussians. Specifically,inour
formulation,wedonotconsiderview-dependentcolorvariationsforsimplicity,thusthedegreeof
sphericalharmonicsissetasd=0,i.e.,onlytheRGBcolorc‚ààR3.
2.2 Cameramodel
Toprojectthe3Dpointcoordinates¬µ‚ààR3intothecameraview,weusethepinholecameramodel.
The camera intrinsics is K ‚àà R3√ó3 and the camera extrinsics which define the world-to-camera
transformationisE = [R|t] ‚àà R3√ó4. Thecamera-view2Dcoordinatesx ‚àà R2 arecalculatedas
dh(x) = KEh(¬µ), where d ‚àà R is the depth, and h(¬∑) represents the homogeneous coordinate
mapping.
3 Recovering4DWorldasGaussianFlow
Problemdefinition Thetask"AnyVideo-to-4D",shortfor"AnyV4D"isdefinedas: Givenaset
ofimageframestakenfromamonocularvideoinputwithoutanycameraparameters,thegoalisto
createamodelthatrepresentsthevideo‚Äôs4Dworld. Thismodelincludesthe3Dscenedynamicsand
thecameraposeforeachframe. Themainchallengeofthistaskisthecomplexityofvideodynamics,
whichrequiresimultaneouslyrecoveryofbothcameraposeandscenecontent.
Overview To deal with above challenges, we propose GFlow, a framework that represent the
videosthroughaflowof3DGaussiansplatting[10]. Atitessence,GFlowalternatelyoptimizesthe
cameraposeandGaussianpointsforeachframeinsequentialordertoreconstructthe4Dworld. This
processinvolvesclusteringGaussianpointsintomovingandstillcategories,alongwithdensification
ofGaussianpoints. Thecameraposesaredeterminedbasedonthestillpoints,whilethemoving
pointsareoptimizedtoaccuratelyrepresentdynamiccontentinthevideo.
Pipeline AsshowninFigure2,givenanimagesequence{I }T ofmonocularvideoinput,we
t t=0
firstutilizesoff-the-shelfalgorithms[38,37,32]toderivethecorrespondingdepth{D }T ,optical
t t=0
flow{F }T andcameraintrinsicK. TheinitializationoftheGaussiansisperformedusingthe
t t=0
prior-driveninitialization,asdescribedinSec3.2.1. ThenforeachframeI attimet,GFlowfirst
t
dividestheGaussianpoints{G } intostillcluster{Gs} andmovingcluster{Gm} inSec3.1.1.
i t i t i t
Theoptimizationprocessthenproceedsintwosteps. Inthefirststep,onlythecameraextrinsicsE is
t
optimized. ThisisachievedbyaligningtheGaussianpointswithinthestillclusterwiththedepthD
t
andopticalflowF ,inSec3.1.2. Followingthis,undertheoptimizedcameraextrinsicsE‚àó,the
t‚àí1 t
GaussianpointsG arefurtherrefinedusingconstraintsfromtheRGBI ,depthD ,opticalflowF ,
t t t t‚àí1
inSec3.1.3. Additionally,theGaussianpointsaredensifiedusingourproposedpixel-wisestrategy
toincorporatenewlyvisiblescenecontent,asoutlinedinSec3.2.2. Afteroptimizingthecurrent
frame,theprocedure‚Äîsceneclustering,cameraoptimization,andGaussianpointoptimization‚Äîis
repeatedforsubsequentframes.
3{ùêº!}
ùêπ% MSùê∂ ùê∂! !$ ‚Äô!% (& )&
%*+
Off-the-shelf Depth,
Optical Flow and
Camera Intrinsic
ùëÉ! ùëÉ!"# ùê∫! ùê∫!"#
{ùê∑!} {ùêπ!} ùêæ
B) Scene Clustering C1) Camera Optimization C2) Gaussians Optimization
A) Prior Association D) Next frame
Figure2: OverviewofGFlow. A)Givenamonocularvideoinputconsistingofimagesequence{I },
i
theassociateddepth{D },opticalflow{F }andcameraintrinsicK areobtainedusingoff-the-shelf
i i
prior. B)Foreachframe,GFLowfirstclusteringthesceneintostillpartGs andmovingpartGm.
i i
ThenoptimizationprocessinGFlowconsistsoftwosteps: C1)OnlythecameraposeP isoptimized
t
byaligningthedepthandopticalflowwithinthestillcluster. C2)Undertheoptimizedcamerapose
P‚àó,theGaussianpoints{G }areoptimizedanddensifiedbasedonRGB,depth,opticalflowand
t i
thetwosceneclusters. D)ThesameprocedureofstepsB,C1,andC2loopsforthenextframe. The
colorfulgraphicsunderthedashedlinerepresentthevariablesinvolvedintheoptimization.
3.1 AlternatingGaussian-cameraoptimization
Whilethe3DGaussianSplatting(3DGS)method[10]isadeptatmodelingstaticscenes,wehave
expandeditscapabilitiestorepresentvideosbysimulatingaflowofGaussianpointsthroughspace
andtime. Inthisapproach,theGaussianpointsareinitializedandoptimizeddirectlyinthefirstframe
asdescribedinSec3.2.1. Forsubsequentframes,weadoptaalternatingoptimizationstrategyforthe
cameraposes{P }andtheGaussians{G }. Thismethodallowsustoeffectivelyaccountforboth
i i
cameramovementanddynamicchangesinthescenecontent.
3.1.1 Sceneclustering
Inconstructingdynamicscenesthatincludebothcameraandobjectmovements,treatingthesescenes
asstaticcanleadtoinaccuraciesandlossofcrucialtemporalinformation. Tobettermanagethis,we
proposeamethodtoclusterthesceneintostillandmovingparts1,whichwillbeIncorporatedinthe
optimisationstage.
Attimet,WefirstemployK-Means[7]algorithmtoseparateGaussianpointsintotwogroupsbased
ontheirmovementsfromtheopticalflow[38,37]mapmapF . Weassumethestillpartcoversthe
t
mostareasinthevisualcontent. Sotheclusterwiththelargerareaintheimage,identifiedbyits
concavehull[20],isconsideredthestillcluster{Gs} ‚äÜ {G } . Theotheristhemovingcluster
i t i t
{Gm} ‚äÜ{G } .
i t i t
Inthefirstframe,weclustertheGaussianpointsusingthismethod. Forlaterframes,asnewpoints
areaddedthroughdensification,weneedtoupdatetheclusters. Existingpointskeeptheirprevious
labels. Fornewpoints,wefirstclusterallpointstogether. ThenewGaussianpointislabeledas‚Äòstill‚Äô
ifitscurrentbelongingclustersharethemostcommonGuassianpointswithlastframe‚Äôsstillcluster
{Gs} ;otherwise,it‚Äôslabeled‚Äòmoving‚Äô.
i t‚àí1
3.1.2 Cameraoptimization
Between two consecutive frames, we assume the camera intrinsic keep the same, while camera
extrinsicundergoaslighttransformation. Ourgoalistoidentifyandcapturethistransformationto
maintaingeometricconsistencyforelementsinthescenethatdonotmove.
1Forsimplicity,wetreatallmovingobjectsasasingleentityratherthanasseparate,distinctobjects.
4Westartwiththeassumptionthatthecameraintrinsic,denotedasK,isknownandderivedfroma
widely-usedstereomodel[32]. ThecameraextrinsicE = [R|t]consistsofarotationR ‚àà SO(3)
andatranslationt‚ààR3.
Foragivenframeattimet,weoptimizethecameraextrinsicE byminimizingtheerrorsinitsdepth
t
estimationandopticalflow. Duringthisoptimization,onlytheGaussianpointswithinthestillcluster
{Gs} areconsideredintheerrorcalculation.
i t
(cid:110) (cid:111)
E‚àó =argmin Œª L ({Gs} ,E )+Œª L ({Gs} ,E ) , (1)
t d dep i t t f flo i t t
Et
whereŒª andŒª areweightingfactorsfordepthlossL andflowlossL ,respectively.
d f dep flo
Depthloss. Thedepthloss,L ,iscalculatedusingtheL1lossbetweentheestimatedandactual
dep
depthsofGaussianpoints. Specifically,foreachGaussianpointG withinthecluster{Gs} attime
i,t i t
t,wedefined(G ;E )asitsdepthfromthecameraandx(G ;E )asitsprojected2Dcoordinatein
i,t t i,t t
theimage. ThecorrespondingdepthfromthedepthmapD atthesecoordinatesisD (x(G ;E )).
t t i,t t
Toaddressdiscrepanciesinscalebetweentheestimatedandactualdepths,thelossisnormalizedby
thesumofthesetwodepthvalues.
Theformalexpressionforthedepthloss,consideringallGaussianpointsinthestillcluster{Gs}
i t
withrespecttotheextrinsicE isgivenby:
t
L ({G } ,E )=
(cid:88) |d(G i,t;E t)‚àíD t(x(G i,t;E t))|
; (2)
dep i t t d(G ;E )+D (x(G ;E ))
i,t t t i,t t
‚àÄGi,t‚àà{Gi}t
Opticalflowloss. Theopticalflowloss,denotedbyL ,quantifiesthemeansquarederror(MSE)
flo
betweentheactualmovementsofGaussianpointsbetweenframesandtheexpectedopticalflow,
ensuringthetemporalsmoothnessoftheGaussianflow. Specifically,thelosscomparesthepositional
changeofaGaussianpoint,x(G ;E )‚àíx(G ;E ),withtheopticalflowF (x(G ;E ))
i,t t i,t‚àí1 t t‚àí1 i,t‚àí1 t
fromthepreviousframe:
L ({G } ,E )=
(cid:88) (cid:12) (cid:12)(cid:12) (cid:12)(cid:0)
x(G ;E )‚àíx(G ;E
)(cid:1)
‚àíF
(cid:0)
x(G ;E
)(cid:1)(cid:12) (cid:12)(cid:12) (cid:12)2
; (3)
flo i t t (cid:12)(cid:12) i,t t i,t‚àí1 t t‚àí1 i,t‚àí1 t (cid:12)(cid:12)
2
‚àÄGi,t‚àà{Gi}t
3.1.3 Gaussiansoptimization
Inthissection,wefocusonoptimizingGaussianpointsinasceneataspecifictimet,withoptimized
cameraextrinsicsE‚àó. Initially,weupdatethepositionsoftheGaussianpointstoreflectmovement
t
detectedinpreviousframe. Subsequently,weoptimizethesepointstoensuretheyalignwiththe
sceneintermsofRGBappearance,depth,andopticalflow.
Pre-optimizationgaussianrelocation. OncewehavetheoptimizedcameraextrinsicsE‚àó, we
t
proceedwithupdatingthepositionsofmovingGaussianpoints. Initially,wetrackthe2Dcoordinates
of moving Gaussian points from the previous frame {x(Gm )}. Using these coordinates, we
i,t‚àí1
calculatetheirmovementbasedonthepreviousframe‚Äôsopticalflowmap{F (x(Gm ))}and
t‚àí1 i,t‚àí1
updatetheircurrentposition:x(Gm)=x(Gm )+F (x(Gm )).Withtheupdatedcoordinates,
i,t i,t‚àí1 t‚àí1 i,t‚àí1
wethenextractthedepthfromthecurrentframe‚Äôsdepthmap{D (x(Gm))},andprojectthesepoints
t i,t
fromthecameraviewtoworldcoordinatesusingE‚àó. ThisstepensuresthatthemovingGaussian
t
pointsareaccuratelypositionedforsubsequentoptimization.
Optimization objectives The primary focus in optimizing Gaussian points is on minimizing
photometricloss, whichcombinesMSEandstructuralsimilarityindex(SSIM)[33]betweenthe
renderedimagefromGaussianpointsandtheactualframeimageI .
t
L ({G } )=L (R({G } ),I )+L (R({G } ),I ); (4)
pho i t mse i t t ssim i t t
5Here,R(¬∑)denotesthe3DGaussiansplattingrenderingprocess. Tomaintainthepositionalconsis-
tencyofstillGaussianpoints,astillnesslossisimplementedtominimizechangesintheir3Dcenter
{¬µ(G ,t)}coordinatesbetweenframes:
i
(cid:88)
L ({G } )= ||¬µ(G )‚àí¬µ(G )|| (5)
sti i t i,t i,t‚àí1
‚àÄGi,t‚àà{Gi}t
ThetotaloptimizationobjectiveofGaussianpointsistominimizeacompositeoflosses:
{G‚àó} =argmin(Œª L ({G })+Œª L ({Gm})+Œª L ({Gm})+Œª L ({Gs})) (6)
i t p pho t d dep t f flo t s sti t
{Gi}t
PhotometriclossisuniversallyappliedtoallGaussianpoints,whereasthedepthandopticalflow
lossesfocusspecificallyonthemovingcluster,andthestillnesslosstargetsthestationarycluster.
Thisapproachensuresatailoredoptimizationthataccountsforboththedynamicsandstabilityof
Gaussianpointsinthescene.
3.2 Gaussianinitializationanddensification
Buildingontheoptimizationprocessdescribedearlier,thissectionfocusesonhowweinitializeand
addGaussianpointsinaccordancewiththevideocontent.
3.2.1 Singleframeinitialization
Theoriginal3DGaussianSplatting[10]initializesGaussianpointsusingpointcloudsderivedfrom
Structure-from-Motion (SfM) [26, 27], which are only viable for static scenes with dense views.
However, our task involves dynamically changing scenes in both space and time, making SfM
infeasible.
Toaddressthis,wedevelopedanewmethodcalledprior-driveninitializationforsingleframes.
Thismethodfullyutilizesthetextureinformationanddepthestimationobtainedfromtheimageto
initializetheGaussianpoints.
Intuitively,imageareaswithmoreedgesusuallyindicatemorecomplextextures,somoreGaussian
points should be initialized in these areas. To achieve this, we use the edge map to guide the
initialization. GivenanimageI ‚àà RH√óW,weextractitstexturemapT ‚àà RH√óW usinganedge
detectionoperator,suchastheSobeloperator[8]. Wethennormalizethistexturemaptocreatea
probabilitymapP ‚ààRH√óW,fromwhichwesampleN toobtaintheir2Dcoordinates{x }N .
i i=1
Toobtaintheirpositioninthe3Dspace,weuseanoff-the-shelfmonoculardepthestimator[32]to
generatethedepthmapDofframeimageI,asitcanofferstronggeometricinformation. Thedepth
{d }N ofsampledpointscanberetrievedfromdepthmapDusing2Dcoordinates.
i i=1
The3Dcentercoordinate{¬µ }N ofGaussianpointsisinitializedbyunprojectingdepth{d }N
i i=1 i i=1
andcamera-view2Dcoordinates{x }N accordingtothepinholemodel,asdescribedinSec2.2.
i i=1
Thescale{s }N andcolor{c }N oftheGaussianpointsareinitializedbasedontheprobability
i i=1 i i=1
valuesandpixelcolorsretrievedfromtheimage,respectively.
3.2.2 Pixel-wiseGaussianpointdensification
3DGaussianSplatting[10],designedforstaticscenes,usesgradientthresholdingtodensifyGaussian
points;pointsexceedingagradientthresholdareclonedorsplitbasedontheirscale. However,this
methodstrugglesindynamicscenes,particularlywhencameramovementsrevealnewsceneareas
wherenoGaussianpointsexist.
Toaddressthis,weintroduceanewdensificationstrategythatleveragesimagecontent,specifically
targetingareasyettobefullyreconstructed. Ourapproachutilizesapixel-wisephotometricerrormap
E ‚ààRH√óW asthebasisfordensification. Thiserrormapisoptionallyenhancedbyanewcontent
pho
maskM ‚ààRH√óW,detectedviaaforward-backwardconsistencycheckusingbidirectionalflow
new
6CoDeF
Ours
Figure3: VisualcomparisonofreconstructionqualityontheDAVIS[24]dataset. Basedonexplicit
representation, 3DGS [10], our GFlow can recover dynamic scenes in high quality. In contrast,
CoDeF[19],whichisbasedonimplicitrepresentation,failstomodelhighlydynamicscenes. Better
viewedbyzoominginandincolor.
fromadvancedopticalflowestimators[37,38]. Thesemapsidentifyimageareasneedingadditional
Gaussianpoints.
Specifically, we zero out error map below a threshold œÑ, and convert the remaining data into a
normalizedprobabilitymapP ‚ààRH√óW. TodensifynewGaussianpoints,thesameinitialization
e
methoddescribedinSec3.2.1isadopted,withtheexceptionofsamplingfromP .Thenumberofnew
e
Gaussianpointsintroducedisp proportionatetothenon-zeroelementnumberofthresholdederror
1
map,andnomorethananotherportionp oftheexistingpointswithintheframe‚Äôsview,ensuring
2
controlledexpansionofthepointsetandtargeteddensificationwherenecessary.
4 Experiments
Datasets We conduct experiments on two challenging video datasets, DAVIS [24] and Tanks
and Temples [11] to evaluate the video reconstruction quality and camera pose accuracy. While
segmentationasaby-product, theobjectsegmentationaccuracyisalsoevulated. DAVISdataset
containsreal-worldvideosofabout30~100frameswithvariousscenariosandmotiondynamics.
The salient object segmentation mask is labeled. We evaluate video reconstruction quality and
objectsegmentationaccuracyonDAVIS2016valdataset. TanksandTemples[11]datasetcontains
videoswithcomplexcameraposemovement. Andthecorrespondingcameraposesarecomputed
by COLMAP algorithm [26, 27]. Following [2], we evaluate camera pose accuracy on 8 scenes
whichcoverbothindoorandoutdoorscenarios. Wealsoevaluatevideoreconstructionqualityon
thisdataset. Duetothelongdurationofvideosinthisdataset,wesample1frameevery4framesfor
efficientevaluation.
Evaluation metrics To quantitatively evaluation our GFlow‚Äôs capabilities, we adopt following
metrics. Forreconstructionquality,wereportstandardPSNR(PeakSignal-to-NoiseRatio),SSIM
(StructuralSimilarityIndex)andLPIPS[41](LearnedPerceptualImagePatchSimilarity)metrics.
Asforcameraposeaccuracy, wereportstandardvisualodometrymetrics[29,42], includingthe
Absolute Trajectory Error (ATE) and Relative Pose Error (RPE) of rotation and translation as in
[2,14].
Implementationdetails Allimagesequencesareresizedsothattheshortestsideis480pixels.
TheinitialnumberofGaussianpointsissetto40,000. ThecameraintrinsicsK areaveragedfrom
the camera intrinsics of all frames predicted by DUSt3R [32]. We use two Adam optimizers for
thetwooptimizationprocedures. ThelearningrateforGaussianoptimizationissetto1e-3,andfor
cameraoptimization,itissetto1e-4. Thenumberofoptimizationiterationsissetto500forGaussian
optimizationinthefirstframe,150forcameraoptimization,and300forGaussianoptimizationin
subsequentframes. Thecolorgradientissettozero,enforcingGaussianpointstomoveratherthan
lazilychangingcolor. Webalancethelosstermbysettinghyper-parametersŒª = 1, Œª = 1e-4,
p d
Œª =1,andŒª =1e-2. Densificationisconductedatthe150-thand300-thstepsinthefirstframe
s f
optimization. Forsubsequentframes, thedensificationofGaussianpointsoccursatthefirststep
7Table1: ReconstructionqualityresultsonDAVIS[24]dataset. AveragePSNR,SSIMandLPIPS
scoresonallvideosarereported.
DAVIS TanksandTemples
Method
PSNR‚Üë SSIM‚Üë LPIPS‚Üì PSNR‚Üë SSIM‚Üë LPIPS‚Üì
CoDeF[19] 24.8904 0.7703 0.2932 26.8442 0.8160 0.1760
GFlow(Ours) 29.5508 0.9387 0.1067 32.7258 0.9720 0.0363
w/osceneclustering 26.3831 0.9032 0.1478 32.4098 0.9706 0.0387
Table2: ObjectsegmentationresultsonDAVIS[24]dataset. Regionsimilarity(J),contouraccuracy
(F)andtemporalstability(T)[24]arereported. ItisworthnotingthatourGFlowframeworkwas
notspecificallytrainedforobjectsegmentation;thiscapabilityemergesasaby-product. One-shot/
zero-shotreferstowhetherthemaskpromptofthefirstframeisgiven.
One-shot Zero-shot
Method
J ‚Üë F ‚Üë J&F ‚Üë J ‚Üë F ‚Üë J&F ‚Üë
GFlow(Ours) 36.1 34.9 35.5 38.3 39.5 38.9
w/osceneclustering 15.1 11.6 13.4 - - -
withanewcontentmaskappliedandatthe100-thstepwithoutanymaskapplied. ThethresholdœÑ,
portionp ,portionp indensificationaresetto1e-2,20%and20%respectively. Allexperimentsare
1 2
conductedonasingleNVIDIARTXA5000GPU,takingtensofminutespervideo. Unlessotherwise
specified,theexperimentalsetupforevaluationisasmentionedabove. Notethatthedynamicswithin
each video could be distinct, so for better reconstruction, the hyperparameters could be tuned in
practice.
4.1 Evaluationresults
4.1.1 Videoreconstruction
Quantitativeresults Reconstructingthe4Dworld,particularlywithcameraandcontentmovement,
isanextremelychallengingtask. Aswearethefirsttoaddressthisproblem,wechooseCoDeF[19],
themethodclosesttotacklethistask,asourbaseline. CoDeFemploysimplicitrepresentationto
learnacanonicaltemplateformodelingmonocularvideo,whichlacksphysicalinterpretability,such
asestimatingcamerapose. AsshowninTable1,ourGFlowdemonstratessignificantadvantages
inreconstructionquality. Thisimprovementstemsfromitsexplicitrepresentation,whichcanadapt
positionsovertimewhilemaintainingvisualcontentcoherence.
Qualitativeresults ThevisualcomparisoninFigure3showsthatCoDeF[19]strugglestorecon-
structvideoswithsignificantmovementduetoitsrelianceonrepresentingavideoasacanonical
template. Sincetheessenceofacanonicaltemplateisa2Dimage,soCoDeFcannothandlecomplex
spatialrelationshipsanddynamicchangesaseffectivelyasourGFlow.
4.1.2 Objectsegmentation
Since GFlow drives the Gaussian points to follow the movement of the visual content, given an
initial one-shot mask prompt, all Gaussian points within this mask can propagate to subsequent
frames. Thispropagationformsanewmaskasaconcavehull[20]aroundthesepoints. Notably,this
capabilityisaby-productofGFlow,achievedwithoutextraintendedtraining. Theevaluationresults
areshowninTable2. Evenwithoutaninitialmaskprompt,GFlowcanstillgeneratehigh-quality
zero-shotsegmentationmasks. ThesemasksareproducedbasedonthemovingGaussianpoints
resultingfromsceneclusteringinanunsupervisedmanner,asdemonstratedinFigure1.
8A) Point tracking in 2D camera-view B) Point tracking in 3D world-coordinates
Figure4: PointtrackingvisualizationonDAVIS[24]dataset. A)showstrackinginthe2Dcamera-
viewwhichcontainsjointmotionofcameraandcontentmovement. B)showstrackinginthe3D
world-coordinateswhichonlypresentcontentmovement.
Table3: CameraposeestimationresultsonTanksandTemples[11]dataset. Absolutetrajectory
error(ATE)andrelativeposeerror(RPE)arereported. TheunitofRPE isindegrees,ATEisinthe
r
groundtruthscaleandRPE isscaledby100.
t
Method Time‚Üì RPE ‚Üì RPE ‚Üì ATE‚Üì
t r
GlobalOptimization
NoPe-NeRF[2] ~25hrs 0.080 0.038 0.006
BARF[14] - 1.046 0.441 0.078
NeRFmm[34] - 1.735 0.477 0.123
On-the-flyOptimization
GFlow(Ours) ~15mins 1.653 0.177 0.016
4.1.3 Pointtracking
ThetrackingtrajectoriesareillustratedinFigure4. DuetothenatureofGFlow,allGaussianpoints
canserveasquerytrackingpoints,enablingtrackinginboth2Dand3Dcoordinates. Inconventional
2Dtracking,thetrackingoccursinthecamera-view,whichincludesthejointmotionofboththe
cameraandthecontent. Incontrast,theGaussianpointsofGFlowresideinthe3Dworld-coordinates,
representingonlycontentmovement. Asaresult,some3Dtrackingtrajectoriestendtoremainin
theiroriginallocations,asshowninFigure4B),becausetheyarepartofthestillbackground.
4.1.4 Cameraposeestimation
Our method can reconstruct the 4D world along with corresponding camera poses. Since some
componentsofGFlowarespecificallydesignedfordynamicscenes,weslightlyadjustthesettingsfor
evaluatingcameraposeaccuracyontheTanksandTemplesdataset,whichconsistsofstaticscenes.
WemodifythesceneclusteringproceduretotreatallGaussianpointsasstatic,asthere‚Äôsnoneed
to distinguishmovingparts in a static scene. The resultsare shownin Table3. As an on-the-fly
optimizationmethod,weachievecomparableresultstoglobaloptimizationmethodsthatrepeatedly
observeeachview,whilerequiringsignificantlylesstime.
4.2 Ablationstudy
Effectofsceneclustering Oneofthebiggestchallengesinthistaskcouldbethedynamicnature
ofthescene,involvingbothcontentandcameramovements. Therefore,asakeycomponentofour
9framework,sceneclusteringplaysacrucialroleindistinguishingbetweenmovingandstaticparts.
AscanbeseenfromTable1,withoutsceneclustering,theframeworkdegeneratestostaticscene
reconstruction,compromisingthereconstructionqualityofdynamicscenes. Similarly,fromTable2,
weobservenegativeinfluencesonsegmentationwhensceneclusteringisnotapplied.
4.3 Downstreamvideoapplications
Various downstream applications could be extended from our GFlow framework. Since explicit
representationcanbeeasilyedited: Camera-levelmanipulation: Changingthecameraextrinsics
canrendernovelviewsofdynamicscenes. Whencombinedwithcameraintrinsics, itcancreate
visualeffectslikedollyzoom. Object-levelediting: WiththeclusterlabelsofmovingGaussian
points,wecanadd,remove,resize,orstylizethesepoints,allowingforpreciseobject-levelediting.
Scene-levelediting: Editingcanalsobeappliedtotheentirescene,enablingtheapplicationofvisual
effectsglobally,asillustratedinFigure1.
5 Relatedworks
4D reconstruction from video, also known as dynamic 3D scene reconstruction, NeRFs [18] are
implicitrepresentationsinitiallyproposedforreconstructingstaticscenes.Subsequentworksextended
NeRFstohandledynamicscenes[21,22,25,13],typicallyusinggrids,triplanes[3,5,28],orlearning
deformable fields to map a canonical template [19, 9]. Although some efforts have accelerated
the training speed of NeRFs [15, 23], achieving real-time rendering for dynamic scenes remains
challenging,especiallywithmonocularinput.
Recentdevelopmentsin3DGaussianSplatting(3DGS)[10]havesetnewrecordsinrenderingspeed.
Extensionsof3DGS[35,17,40,39]havebegunexploringdynamicscenereconstruction. However,
theystilloperateundertheassumptionofaknowncamerasequence[27,26].
Whileallthesemethodseitherrelyonpre-calibratedcameraparametersormulti-viewvideoinputsto
reconstructdynamicscenes[35,17,30,1,3,5,12,15,16,25],orestimatecameraposesfromstatic
scenesusingmulti-viewstereotechniques[2,6,32,14,34,36,27,26,31,4]. Thekeydifference
betweenourGFlowandtheseapproachesliesinourabilitytoperformdynamicscenereconstruction
fromasingle,unposedmonocularvideo.
6 Limitations
AlthoughGFloweffectivelyreconstructsthe4Dworldfrommonocularunposedvideoandenables
numerousapplications,severalkeychallengesremain: Ourapproachreliesonoff-the-shelfdepth
andopticalflowcomponents,whereinaccuraciescancompromisetheprecisionandfidelityofour
reconstruction. Specifically, inaccurate depth maps may lead to incorrect spatial placements of
Gaussianpoints,whileerroneousopticalflowcanresultinimpropermotionestimationanddynamic
scenerepresentation. Toaddresstheseissues,wecouldintegratemoreadvancedmulti-framestereo
methodstorefinedepthestimationandincorporatesemanticfeaturestobetterassociateandtrack
movingobjects. Additionally,thecurrentuseofK-Meansclusteringforsceneclusteringmaybe
inadequateforcomplexscenarios,suggestingtheneedforamoresophisticatedandcomprehensive
clusteringstrategy. Furthermore,ouron-the-flyonlineoptimizationcanintroduceandaccumulate
errors over time; thus, implementing a look-back or global optimization method could mitigate
theseaccumulatederrorsandenhanceoverallaccuracy. Addressingthesechallengesiscrucialfor
improvingtheprecisionandrobustnessofGFlowinreconstructingdynamic4Dscenes.
7 Conclusion
Wehavepresented"GFlow",anovelframeworkdesignedtoaddressthechallengingtaskofrecon-
structingthe4Dworldfrommonocularvideoinputs,termed"AnyV4D". Throughsceneclustering
andsequentialoptimizationofcameraandGaussianpoints,coupledwithpixel-wisedensification,
GFlowenablestherecoveryofdynamicscenesalongsidecameraposesacrossframes. Furthercapa-
bilitiessuchastracking,segmentation,editing,andnovelviewrendering,highlightingthepotential
forGFlowtorevolutionizevideounderstandingandmanipulation.
10References
[1] AayushBansal,MinhVo,YaserSheikh,DevaRamanan,andSrinivasaNarasimhan. 4dvisualizationof
dynamiceventsfromunconstrainedmulti-viewvideos. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pages5366‚Äì5375,2020.
[2] WenjingBian,ZiruiWang,KejieLi,Jia-WangBian,andVictorAdrianPrisacariu. Nope-nerf:Optimising
neuralradiancefieldwithnoposeprior. InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pages4160‚Äì4169,2023.
[3] AngCaoandJustinJohnson. Hexplane:Afastrepresentationfordynamicscenes. InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pages130‚Äì141,2023.
[4] DavidCharatan,SizheLi,AndreaTagliasacchi,andVincentSitzmann. pixelsplat:3dgaussiansplatsfrom
imagepairsforscalablegeneralizable3dreconstruction. arXivpreprintarXiv:2312.12337,2023.
[5] SaraFridovich-Keil,GiacomoMeanti,FrederikRahb√¶kWarburg,BenjaminRecht,andAngjooKanazawa.
K-planes: Explicit radiance fields in space, time, and appearance. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages12479‚Äì12488,2023.
[6] YangFu,SifeiLiu,AmeyKulkarni,JanKautz,AlexeiAEfros,andXiaolongWang. Colmap-free3d
gaussiansplatting. arXivpreprintarXiv:2312.07504,2023.
[7] JohnAHartiganandManchekAWong. Algorithmas136:Ak-meansclusteringalgorithm. Journalof
theroyalstatisticalsociety.seriesc(appliedstatistics),28(1):100‚Äì108,1979.
[8] NickKanopoulos,NageshVasanthavada,andRobertLBaker. Designofanimageedgedetectionfilter
usingthesobeloperator. IEEEJournalofsolid-statecircuits,23(2):358‚Äì367,1988.
[9] YoniKasten,DolevOfri,OliverWang,andTaliDekel. Layeredneuralatlasesforconsistentvideoediting.
ACMTransactionsonGraphics(TOG),40(6):1‚Äì12,2021.
[10] BernhardKerbl,GeorgiosKopanas,ThomasLeimk√ºhler,andGeorgeDrettakis. 3dgaussiansplattingfor
real-timeradiancefieldrendering. ACMTransactionsonGraphics,42(4):1‚Äì14,2023.
[11] ArnoKnapitsch,JaesikPark,Qian-YiZhou,andVladlenKoltun. Tanksandtemples: Benchmarking
large-scalescenereconstruction. ACMTransactionsonGraphics(ToG),36(4):1‚Äì13,2017.
[12] TianyeLi,MiraSlavcheva,MichaelZollhoefer,SimonGreen,ChristophLassner,ChangilKim,Tanner
Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. Neural 3d video synthesis
frommulti-viewvideo. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages5521‚Äì5531,2022.
[13] ZhengqiLi,QianqianWang,ForresterCole,RichardTucker,andNoahSnavely. Dynibar:Neuraldynamic
image-basedrendering. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages4273‚Äì4284,2023.
[14] Chen-HsuanLin, Wei-ChiuMa, AntonioTorralba, andSimonLucey. Barf: Bundle-adjustingneural
radiancefields. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages
5741‚Äì5751,2021.
[15] HaotongLin,SidaPeng,ZhenXu,TaoXie,XingyiHe,HujunBao,andXiaoweiZhou. High-fidelityand
real-timenovelviewsynthesisfordynamicscenes. InSIGGRAPHAsia2023ConferencePapers,pages
1‚Äì9,2023.
[16] Kai-EnLin,LeiXiao,FengLiu,GuoweiYang,andRaviRamamoorthi. Deep3dmaskvolumeforview
synthesisofdynamicscenes. InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision,pages1749‚Äì1758,2021.
[17] JonathonLuiten,GeorgiosKopanas,BastianLeibe,andDevaRamanan. Dynamic3dgaussians:Tracking
bypersistentdynamicviewsynthesis. arXivpreprintarXiv:2308.09713,2023.
[18] BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,RaviRamamoorthi,andRen
Ng. Nerf:Representingscenesasneuralradiancefieldsforviewsynthesis. CommunicationsoftheACM,
65(1):99‚Äì106,2021.
[19] HaoOuyang,QiuyuWang,YuxiXiao,QingyanBai,JuntaoZhang,KechengZheng,XiaoweiZhou,Qifeng
Chen,andYujunShen. Codef: Contentdeformationfieldsfortemporallyconsistentvideoprocessing.
arXivpreprintarXiv:2308.07926,2023.
11[20] Jin-SeoParkandSe-JongOh. Anewconcavehullalgorithmandconcavenessmeasureforn-dimensional
datasets. JournalofInformationscienceandengineering,28(3):587‚Äì600,2012.
[21] KeunhongPark,UtkarshSinha,JonathanTBarron,SofienBouaziz,DanBGoldman,StevenMSeitz,and
RicardoMartin-Brualla. Nerfies: Deformableneuralradiancefields. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,pages5865‚Äì5874,2021.
[22] KeunhongPark,UtkarshSinha,PeterHedman,JonathanTBarron,SofienBouaziz,DanBGoldman,
RicardoMartin-Brualla,andStevenMSeitz. Hypernerf:Ahigher-dimensionalrepresentationfortopologi-
callyvaryingneuralradiancefields. arXivpreprintarXiv:2106.13228,2021.
[23] SidaPeng,YunzhiYan,QingShuai,HujunBao,andXiaoweiZhou. Representingvolumetricvideos
asdynamicmlpmaps. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages4252‚Äì4262,2023.
[24] FedericoPerazzi,JordiPont-Tuset,BrianMcWilliams, LucVanGool, MarkusGross, andAlexander
Sorkine-Hornung. Abenchmarkdatasetandevaluationmethodologyforvideoobjectsegmentation. In
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages724‚Äì732,2016.
[25] AlbertPumarola,EnricCorona,GerardPons-Moll,andFrancescMoreno-Noguer.D-nerf:Neuralradiance
fieldsfordynamicscenes. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages10318‚Äì10327,2021.
[26] JohannesLSchonbergerandJan-MichaelFrahm. Structure-from-motionrevisited. InProceedingsofthe
IEEEconferenceoncomputervisionandpatternrecognition,pages4104‚Äì4113,2016.
[27] Johannes L Sch√∂nberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view
selectionforunstructuredmulti-viewstereo. InComputerVision‚ÄìECCV2016:14thEuropeanConference,
Amsterdam,TheNetherlands,October11-14,2016,Proceedings,PartIII14,pages501‚Äì518.Springer,
2016.
[28] RuizhiShao, ZerongZheng, HanzhangTu, BoningLiu, HongwenZhang, andYebinLiu. Tensor4d:
Efficientneural4ddecompositionforhigh-fidelitydynamicreconstructionandrendering. InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages16632‚Äì16642,2023.
[29] J√ºrgenSturm,NikolasEngelhard,FelixEndres,WolframBurgard,andDanielCremers. Abenchmarkfor
theevaluationofrgb-dslamsystems. In2012IEEE/RSJinternationalconferenceonintelligentrobotsand
systems,pages573‚Äì580.IEEE,2012.
[30] JiakaiSun,HanJiao,GuangyuanLi,ZhanjieZhang,LeiZhao,andWeiXing. 3dgstream: On-the-fly
trainingof3dgaussiansforefficientstreamingofphoto-realisticfree-viewpointvideos. arXivpreprint
arXiv:2403.01444,2024.
[31] FengruiTian,ShaoyiDu,andYueqiDuan. Mononerf:Learningageneralizabledynamicradiancefield
frommonocularvideos. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,
pages17903‚Äì17913,2023.
[32] ShuzheWang,VincentLeroy,YohannCabon,BorisChidlovskii,andJeromeRevaud. Dust3r:Geometric
3dvisionmadeeasy. arXivpreprintarXiv:2312.14132,2023.
[33] ZhouWang,AlanCBovik,HamidRSheikh,andEeroPSimoncelli. Imagequalityassessment:fromerror
visibilitytostructuralsimilarity. IEEEtransactionsonimageprocessing,13(4):600‚Äì612,2004.
[34] ZiruiWang,ShangzheWu,WeidiXie,MinChen,andVictorAdrianPrisacariu. Nerf‚Äì:Neuralradiance
fieldswithoutknowncameraparameters. arXivpreprintarXiv:2102.07064,2021.
[35] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian,
and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint
arXiv:2310.08528,2023.
[36] YitongXia,HaoTang,RaduTimofte,andLucVanGool. Sinerf:Sinusoidalneuralradiancefieldsforjoint
poseestimationandscenereconstruction. arXivpreprintarXiv:2210.04553,2022.
[37] HaofeiXu,JingZhang,JianfeiCai,HamidRezatofighi,andDachengTao. Gmflow: Learningoptical
flowviaglobalmatching. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pages8121‚Äì8130,2022.
[38] HaofeiXu,JingZhang,JianfeiCai,HamidRezatofighi,FisherYu,DachengTao,andAndreasGeiger.Uni-
fyingflow,stereoanddepthestimation. IEEETransactionsonPatternAnalysisandMachineIntelligence,
2023.
12[39] ZeyuYang,HongyeYang,ZijiePan,XiatianZhu,andLiZhang. Real-timephotorealisticdynamicscene
representationandrenderingwith4dgaussiansplatting. arXivpreprintarXiv:2310.10642,2023.
[40] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d
gaussiansforhigh-fidelitymonoculardynamicscenereconstruction. arXivpreprintarXiv:2309.13101,
2023.
[41] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectivenessofdeepfeaturesasaperceptualmetric. InCVPR,2018.
[42] ZichaoZhangandDavideScaramuzza. Atutorialonquantitativetrajectoryevaluationforvisual(-inertial)
odometry. In2018IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems(IROS),pages
7244‚Äì7251.IEEE,2018.
13