Don’t Forget to Connect!
Improving RAG with Graph-based Reranking
JialinDong BahareFatemi BryanPerozzi LinF.Yang AntonTsitsulin
UCLA GoogleResearch GoogleResearch UCLA GoogleResearch
Abstract
RetrievalAugmentedGeneration(RAG)hasgreatlyimprovedtheperformanceof
LargeLanguageModel(LLM)responsesbygroundinggenerationwithcontext
fromexistingdocuments. Thesesystemsworkwellwhendocumentsareclearly
relevant to a question context. But what about when a document has partial
information,orlessobviousconnectionstothecontext?Andhowshouldwereason
aboutconnectionsbetweendocuments? Inthiswork,weseektoanswerthesetwo
core questions about RAG generation. We introduce G-RAG, a reranker based
ongraphneuralnetworks(GNNs)betweentheretrieverandreaderinRAG.Our
methodcombinesbothconnectionsbetweendocumentsandsemanticinformation
(viaAbstractMeaningRepresentationgraphs)toprovideacontext-informedranker
forRAG.G-RAGoutperformsstate-of-the-artapproacheswhilehavingsmaller
computationalfootprint. Additionally,weassesstheperformanceofPaLM2asa
rerankerandfindittosignificantlyunderperformG-RAG.Thisresultemphasizes
theimportanceofrerankingforRAGevenwhenusingLargeLanguageModels.
1 Introduction
RetrievalAugmentedGeneration(RAG)[35]hasbroughtimprovementstomanyproblemsintext
generation. One example is Open-Domain Question Answering (ODQA) [40] which involves
answering natural language questions without limiting the domain of the answers. RAG merges
theretrievalandansweringprocesses,whichimprovestheabilitytoeffectivelycollectknowledge,
extractusefulinformation,andgenerateanswers. Eventhoughitissuccessfulinfetchingrelevant
documents,RAGisnotabletoutilizeconnectionsbetweendocuments. IntheODQAsetting,this
leadstothemodeldisregardingdocumentscontaininganswers,a.k.a.positivedocuments,withless
apparentconnectionstothequestioncontext. Wecanidentifythesedocumentsifweconnectthem
withpositivedocumentswhosecontextisstronglyrelevanttothequestioncontext.
Tofindconnectionsbetweendocumentsandselecthighlyrelevantones,thererankingprocessplays
avitalroleinfurthereffectivelyfilteringretrieveddocuments. Arobustrerankeralsobenefitsthe
readingprocessbyeffectivelyidentifyingpositivedocumentsandelevatingthemtoprominentranking
positions. Whenthereader’soutputperfectlymatchesoneofthegoldstandardanswers, itleads
toanincreaseinexact-matchperformancemetrics. Givenourpaper’semphasisonthereranking
aspect,ourperformancemetricsprimarilyfocusonrankingtasks,specificallyMeanTiedReciprocal
RankingandMHits@10. Thus,ourpaperfocusesonusingrerankingtoimproveRAG–asitisa
fundamentalbridgebetweentheretrievalandreadingprocesses.
Pre-trainedlangaugemodels(LMs)likeBERT[6],RoBERTa[23],andBART[19]havebeenwidely
used to enhance reranking performance by estimating the relevant score between questions and
documents. Recently,theAbstractMeaningRepresentation(AMR)graphhasbeenintegratedwith
aLMtoenhancethesystem’sabilitytocomprehendcomplexsemantics[42]. Whilethecurrent
rerankersexhibitadmirableperformance,certainlimitationspersist.
Preprint.Underreview.
4202
yaM
82
]LC.sc[
1v41481.5042:viXraQ&P AMRs DocumentGraph Reranker
…
…
…
… …
…
findconnections establishGNN
Figure1: G-RAGusestwographsforre-rankingdocuments: TheAbstractMeaningRepresentation
(AMR)graphisusedasfeaturesforthedocument-levelgraph. Documentgraphisthenusedfor
documentreranking.
Firstly,asmentionedabove,mostofthecurrentworksfailtocaptureimportantconnectionsbetween
differentretrieveddocuments. Somerecentwork[46]triestoincorporateexternalknowledgegraphs
toimprovetheperformanceofthereadingprocessinRAGbutatthecostofsignificantmemory
usageforknowledgegraphstorage. Theconnectionbetweendocumentshasnotbeenconsidered
inthererankingprocessyet. Secondly,eventhoughtheAMRgraphimprovestheunderstandingof
thecomplexsemantics,state-of-the-art[42]workintegratesredundantAMRinformationintothe
pre-trainedlanguagemodels. Thisextrainformationcancausepotentialoverfitting,inadditionto
increasesofincomputationaltimeandGPUcost. Thirdly,currentpapersutilizecommonpre-trained
languagemodelsasrerankerswhichareinsufficientgiventhefastpaceofLLMdevelopment. With
the recent breakthroughs from LLM, researchers are curious about how LLMs without perform
(withoutfine-tuning)onthererankingtask.
Toaddressthesechallengesandlimitations,weproposeamethodbasedondocumentgraphs,where
eachnoderepresentsadocument,andeachedgerepresentsthattherearecommonconceptsbetween
twodocuments. Weincorporatetheconnectioninformationbetweendifferentdocumentsintothe
edge features and update the edge features through the message-passing mechanism. For node
features,eventhoughweaimtoaddAMRinformationtocomposearicherunderstandingofcomplex
semantics,wewon’toverwhelminglyaddallAMR-relatedtokensasnode-levelfeatures. Instead,we
investigatethedeterminingfactorthatfacilitatesthererankertoidentifymorerelevantdocuments
andencodethiskeyfactortonodefeatures.
Moreover,insteadofusingthecross-entropylossfunctionduringthetraining,weapplypairwise
rankinglossinconsiderationoftheessentialaimofranking. Wealsoinvestigatetheperformance
ofapubliclyavailableLLM,i.e.,PaLM2[9]withdifferentversions,asarerankeronanODQA.
AccordingtothemoderateperformanceofPaLM2onrerankingtasks,weprovideseveralpotential
reasonsandemphasizetheirreplaceableroleofrerankermodeldesigntoimproveRAG.Theframe-
workofgraph-basedrerankingintheproposedG-RAGisillustratedinFig1.Toprovideaclearer
illustrationofourmethod’spipeline,pleaserefertoFig4intheAppendix.
Ourcontributionscanbesummarizedasfollows:
1. ToimproveRAGforODQA,weproposeadocument-graph-basedrerankerthatleverages
connectionsbetweendifferentdocuments. Whenthedocumentssharesimilarinformation
with their neighbor nodes, it helps the reranker to successfully identify the documents
containinganswercontextthatisonlyweaklyconnectedtothequestion.
2. We introduce new metrics to assess a wide range of ranking scenarios, including those
withtiedrankingscores. Themetricseffectivelyevaluatethisscenariobydiminishingthe
optimisticeffectbroughtbytiedrankings. Basedonthesemetrics,ourproposedmethod
outperformsstate-of-the-artandrequiresfewercomputationalresources.
3. WeassesstheperformanceofapubliclyavailableLLM(PaLM2[9])asareranker,exploring
variations across different model sizes. We find that excessive ties within the generated
rankingscoreshindertheeffectivenessofpre-trainedlargelanguagemodelsinimproving
RAGthroughreranking.
2
…
…
…2 RelatedWork
RAGinODQA. RAG[20,35]combinesinformationretrieval(viaDensePassageRetrieval,DPR
[16])andareadingprocessinadifferentiablemannerforODQA.Alineofliteraturefocuseson
developingrerankersforfurtherimprovingRAG.ApproacheslikemonoT5[30]andmonoELECTRA
[34]useproposedpre-trainedmodels. Moreover,Zhuangetal.[47]proposeafine-tunedT5version
asareranker. Morerecently,Parketal.[33]developarerankermodulebyfine-tuningthereader’s
neuralnetworksthroughapromptingmethod. However,theaboveapproachesneglecttoinvestigate
theconnectionsamongdocumentsandfailtoleveragethisinformationduringthererankingprocess.
These methods are prone to fail to identify the documents containing gold answers that may not
exhibitobviousconnectionstothequestioncontext. Toaddressthisissue, ourproposedmethod
is based on document graphs and is more likely to identify valuable information contained in a
documentifmostofitsneighboringdocumentnodesinthegraphsharesimilarinformation.
Graphs in ODQA. Knowledge graphs, which represent entities and their relations, have been
leveragedinODQA[46,15,1,5]toimprovetheperformanceofRAG.However,KG-basedmethods
requirelargeexternalknowledgebasesandentitymappingfromdocumentstotheentitiesinthe
knowledgegraph,whichwouldincreasethememorycost. Ourproposedmethoddoesnotdependon
externalknowledgegraphs. WhilerecentworkbyWangetal.[42]usesAMRgraphsgeneratedfrom
questionsanddocumentstoconstructembeddings,theirfocusremainsontext-levelrelationswithin
single document. In contrast, our approach uniquely leverages document graphs to characterize
cross-documentconnections,anovelapplicationwithintheRAGrerankingprocess.
AbstractMeaningRepresentation(AMR). AMR[4]servesasapromisingtoolforrepresenting
textualsemanticsthrougharooted,directedgraph. IntheAMRgraph,nodesrepresentbasicsemantic
unitslikeentitiesandconcepts,whileedgesdenotetheconnectionsbetweenthem. AMRgraphshave
morestructuredsemanticinformationcomparedtothegeneralformofnaturallanguage[2,29]. A
lineofliteraturehasintegratedAMRgraphsintolearningmodels. Recently,Wangetal.[42]have
appliedAMRtoODQAtodealwithcomplexsemanticinformation. Eventhoughtheperformance
ofthererankerandthereaderisimprovedin[42],theirmethodalsoincreasesthecomputational
timeandGPUmemorycost. ThisissuemayarisebyintegratingalltokensofAMRnodesandedges
withoutconscientiouslyselectingthekeyfactors.Toaddressthisissue,ourmethodaimstoinvestigate
thegraphstructureofAMRgraphsandidentifythekeyfactorsthatimprovetheperformanceofthe
reranker.
LLMsinReranking. LLMssuchasChatGPT[31],PaLM2[9],LLaMA[38],andGPT4[32],have
proventobecapableofprovidinganswerstoabroadrangeofquestionsduetotheirvastknowledge
repositories and chain-of-thought reasoning capability. With this breakthrough, researchers are
seekingtoexplorepotentialimprovementsthatLLMscanbringtoimproveRAGinODQA,suchas
[12,26]. Atthesametime,severalstudies[41,37]havescrutinizedtheefficacyofLLMsinQuestion-
Answering. Wangetal.[41]indicatesthesuperiorityoftheDPR[16]+FiD[13]approachoverLLM
inODQA.WhilesomepapershavedemonstratedimprovementsinLLMrerankingperformance,it’s
essentialtonotethattheseenhancementsofteninvolveadditionaltechniquessuchasaugmentedquery
generation[36]orconditionalrankingtasks[11],whichmaynotdirectlyalignwithourzero-shot
setting. Therecentpaper[28]demonstratesthatLLMisagoodfew-shotrerankerandinvestigates
differentscenarioswherezero-shotLLMsperformpoorly. Italsoprovideseffortstoaddressthese
challengesbycombiningvarioustechniques,suchasemployingsmallerlanguagemodels. Despite
theseinvestigations,thepotentialofLLMswithoutfine-tuningasrerankerstoimproveRAGremains
unexplored,asexistingstudiesoftentakepre-trainedlanguagemodelssuchasBERT[6],RoBERTa
[23],andBART[19]inthererankerrole.
3 ProposedMethod: G-RAG
G-RAG leverages the rich structural and semantic information provided by the AMR graphs to
enhancedocumentreranking. Section3.1detailshowweuseAMRgraphinformationandbuilda
graphstructureamongtheretrieveddocuments. Section3.2outlinesthedesignofourgraphneural
networkarchitectureforrerankingdocuments.
33.1 EstablishingDocumentGraphsviaAMR
In ODQA datasets we consider, one document is a text block of 100 words that come from the
textcorpus. Foreachquestion-documentpair, weconcatenatethequestionq anddocumentpas
“question:<questiontext><documenttext>”andthenexploitAMRBART[3]toparsethesequence
intoasingularAMRgraph. TheAMRgraphforquestionq anddocumentpisdenotedasG =
qp
{V,E},whereV andE arenodesandedges,respectively. Eachnodeisaconcept,andeachedge
is denoted as e = (s,r,d) where s,r,d represent the source node, relation, and the destination
node,respectively. Ourrerankeraimstorankamongthetop100documentsretrievedbyDPR[16].
Thus,givenonequestionqanddocuments{p ,··· ,p }withn=100,weestablishtheundirected
1 n
document graph G = {V,E} based on AMRs {G ,··· ,G }. For each node v ∈ V, it
q qp1 qpn i
correspondstothedocumentp . Forv ,v ∈ V,i ̸= j,ifthecorrespondingAMRG andG
i i j qpi qpj
have common nodes, there will be an undirected edge between v and v (with a slight abuse in
i j
notation)denotedase =(v ,v )∈E. WeremoveisolatednodesinG . Inthefollowing,wewill
ij i j q
constructthegraphneuralnetworksbasedonthedocumentgraphstopredictwhetherthedocument
isrelevanttothequestion. PleaserefertoAppendixAforAMRgraphstatistics,i.e.,thenumberof
nodesandedgesinAMRgraphs,ofthecommondatasetsinODQA.
3.2 GraphNeuralNetworksforReranking
FollowingSection3.1,weconstructagraphamongthen = 100retrieveddocumentsdenotedas
G giventhequestionq. WeaimtoexploitboththestructuralinformationandtheAMRsemantic
q
informationtoreranktheretrieveddocuments. Tointegratethesemanticinformationofdocuments,
thepre-trainedlanguagemodelssuchasBERT[6],andRoBERTa[23]arepowerfultoolstoencode
thedocumenttextsasnodefeaturesingraphneuralnetworks. EventhoughWangetal.[42]integrate
AMRinformationintoLMs,itincreasescomputationaltimeandGPUmemoryusage. Toaddress
this,weproposednodeandedgefeaturesforgraphneuralnetworks,whichsimultaneouslyexploit
thestructuralandthesemanticinformationofAMRbutavoidaddingredundantinformation.
3.2.1 GeneratingNodeFeatures
Our framework applies a pre-trained language model to encode all the n retrieved documents in
{p ,p ,··· ,p }givenaquestionq. ThedocumentembeddingisdenotedasX˜ ∈Rn×dwheredis
1 2 n
thehiddendimension,andeachrowofX˜ isgivenby
x˜ =Encode(p )fori∈{1,2,···n}. (1)
i i
SinceAMRbringsmorecomplexandusefulsemanticinformation,weintendtoconcatenatedoc-
ument text and corresponding AMR information as the input of the encoder. However, if we
integrate all the information into the embedding process as the previous work [42] did, it would
bring high computational costs and may lead to overfitting. To avoid this, we investigate the de-
terminingfactorthatfacilitatesthererankertoidentifymorerelevantdocuments. Bystudyingthe
structureofAMRsfordifferentdocuments, wenotethatalmosteveryAMRhasthenode“ques-
tion”, where the word “question" is included in the input of the AMR parsing model, given by
“question:<questiontext><documenttext>”. Thus,wecanfindthesinglesourceshortestpathstart-
ingfromthenode“question". Whenlistingeverypath,thepotentialconnectionfromthequestionto
theanswerbecomesmuchclearer. Bylookingintothenodescoveredineachpath,boththestructural
andsemanticinformationcanbecollected. Theembeddingenablesustoutilizethatinformationto
identifythesimilaritybetweenquestionanddocumentcontext.
Tobetterillustratethestructureoftheshortestpath,wealsoconductsomeexperimentstoshowthe
statisticoftheshortestpath,seeFig3inAppendix. Westudytheshortestsinglesourcepaths(SSSPs)
startingfrom“question”intheAMRgraphsofdocumentsfromthetrainsetofNaturalQuestion
(NQ)[18]andTriviaQA(TQA)[14]dataset. Theanalysisshowsthatcertainnegativedocuments
cannotestablishadequateconnectionstothequestioncontextwithintheirtext. Moreover,negative
documentsencounteranotherextremescenariowherepathscontainanabundanceofinformation
related to the question text but lack valuable information such as the gold answers. This unique
patternprovidesvaluableinsightthatcanbeutilizedduringtheencodingprocesstoimprovethe
rerankerperformance.
4Thus,theproposeddocumentembeddingisgivenbyX ∈Rn×dandeachrowofX canbegivenby,
fori∈{1,2,···n}:
x =Encode(concat(p ,a )), (2)
i i i
wherea isasequenceofwords,representingtheAMRinformationconcerningthedocumentp .
i i
Therearetwostepstogettherepresentationofa : 1)PathIdentification: Firstly,theshortestsingle
i
sourcepaths(SSSPs)aredeterminedstartingfromthenodelabeled"question"intheAMRgraph
G . Eachpathidentifiedshouldnotbeasubsetofanother. Forinstance,considerthefollowing
qpi
paths composed of node concepts: [‘question’, ‘cross’, ‘world-region’, ‘crucifix’, ‘number’, ‘be-
located-at’,‘country’,‘Spain’],[‘question’,‘cross’,‘religion’,‘Catholicism’,‘belief’,‘worship’];2)
NodeConceptExtraction: Subsequently,thenodeconceptsalongtheseidentifiedpathsareextracted
to construct a . In the example provided, a is formed as follows: "question cross world-region
i i
crucifixnumberbe-located-atcountrySpainreligionCatholicismbeliefworship". X ∈ Rn×d (2)
willbetheinitialnoderepresentationofgraphneuralnetworks.
3.2.2 EdgeFeatures
Besides the node features, we also adequately leverage edge features associated with undirected
edgesinAMR{G ,··· ,G }. LetEˆ ∈ Rn×n×l denotetheedgefeaturesofthegraph. Then,
qp1 qpn
Eˆ ∈Rl representsthel-dimensionalfeaturevectoroftheedgebetweenthenodev andnodev
ij· i j
i̸=j,andEˆ denotesthek-thdimensionoftheedgefeatureinEˆ . Inourframework,l=2and
ijk ij·
Eˆ isgivenby:
 Eˆ =0,noconnectionbetweenG andG ,
 ij· qpi qpj
Eˆ =#commonnodesbetweenG andG , (3)
ij1 qpi qpj
 Eˆ =#commonedgesbetweenG andG .
ij2 qpi qpj
WethennormalizetheedgefeatureEˆ toavoidtheexplosivescaleofoutputnodefeatureswhen
beingmultipliedbytheedgefeatureingraphconvolutionoperations. Thus,ourderivedfeatureE
isnormalizedonthefirstandseconddimension,respectively. Similaredgenormalizationhasalso
beenconsideredinthepaper[8]. E ∈Rn×n×l willbetheinitialedgerepresentationofgraphneural
networks.
3.2.3 RepresentationUpdate
Basedontheaboveinitialnodeandedgerepresentations,wearriveatupdatingrepresentationsin
thegraphneuralnetworks. GivenadocumentgraphG(V,E)with|V|=n,theinputfeatureofnode
v ∈V isdenotedasx0 ∈Rd,andtheinitialrepresentationoftheedgebetweennodevanduisgiven
v
bye0 ∈Rl withl=2. LetN(v)denotetheneighbornodesofthenodev ∈V. Therepresentation
uv
ofnodev ∈V atlayerℓcanbederivedfromaGNNmodelgivenby:
(cid:91)
xℓ =g(xℓ−1, f(xℓ−1,eℓ−1)), (4)
v v u uv
u∈N(v)
(cid:83)
wheref, andgarefunctionsforcomputingfeature,aggregatingdata,andupdatingnoderepre-
sentations,respectively. Specifically,thefunctionf appliesdifferentdimensionaledgefeaturesas
weightstothenodefeatures,givenby
l
(cid:88)
f(xℓ−1,eℓ−1)= eℓ−1(m)xℓ−1. (5)
u uv uv u
m=1
(cid:83)
We choose mean aggregator [17] as the operation . The parameterized function g is a non-
linear learnable function that aggregates the representation of the node and its neighbor nodes.
Simultaneously,therepresentationofedgestartingfromv ∈V atlayerℓisgivenby:
(cid:91)
eℓ =g(eℓ−1, eℓ−1). (6)
v· v· u·
u∈N(v)
53.2.4 RerankingScoreandTrainingLoss
GivenaquestionqanditsdocumentgraphG ={V,E},wehavetheoutputnoderepresentationsof
q
GNN,i.e.,xL,whereListhenumberofGNNlayers. Withthesameencoderin(2),thequestionqis
v
embeddedas
y=Encode(q). (7)
Thererankingscoreforeachnodev ∈V correspondingthedocumentp iscalculatedby
i i
s =y⊤xL, (8)
i vi
fori = 1,··· ,nand|V| = n. Thecross-entropytraininglossofdocumentrankingforthegiven
questionqis:
n (cid:32) (cid:33)
L
=−(cid:88)
y log
exp(s i)
(9)
q i (cid:80)n
exp(s )
i=1 j=1 j
wherey =1ifp isthepositivedocument,and0forthenegativedocument. Thecross-entropyloss
i i
mayfailtodealwiththeunbalanceddatainODQAwherethenumberofnegativedocumentsismuch
greaterthanthenumberofpositivedocuments. Besidesthecross-entropylossfunction,thepairwise
lossfunctionhasbeenapowerfultoolforranking[21]. Givenapairofscoress ands ,theranking
i j
lossisgivenby:
RL (s ,s ,r)=max(0,−r(s −s )+1), (10)
q i j i j
wherer =1ifdocumentishouldberankedhigherthandocumentj,andvice-versaforr =−1. We
conductexperimentsbasedonbothlossfunctionsandemphasizetheadvantageoftherankingloss
(10)overthecross-entropyloss(9).
4 Experiments
4.1 Setting
Datasets. We conduct experiments on two representative ODQA datasets Natural Ques-
tions(NQ)[18]andandTriviaQA(TQA)[14]. NQisderivedfromGoogleSearchQueriesandTQA
includesquestionsfromtriviaandquiz-leaguewebsites. Detaileddatasetstatisticsarepresented
inTable4inAppendixA.NotethatthegoldanswerlistsindatasetNQusuallyhavemuchfewer
elementsthanthedatasetTQA,whichleadstoamuchsmallernumberofpositivedocumentsfor
eachquestion.
WeuseDPR[16]toretrieve100documentsforeachquestionandgeneratetheAMRgraphforeach
question-documentpairusingAMRBART[3]. ThedatasetwithAMRgraphsisprovidedby[42]1.
PleaserefertoAppendixAformoredetailsontheAMRstatisticinformation. Weconductedour
experimentsonaTeslaA10040GBGPU,demonstratingthelowcomputationalneedsofG-RAG.
ModelDetails. FortheGNN-basedrerankingmodels,weadopta2-layerGraphConvolutional
Network [17] with hidden dimension chosen from {8,64,128} via hyperparameter-tuning. The
dropoutrateischosenfrom{0.1,0.2,0.4}. WeinitializetheGNNnodefeaturesusingpre-trained
models, e.g, BERT [6], GTE [22], BGE [44], Ember [24]. We base our implementaion of the
embeddingmodelontheHuggingFaceTransformerslibrary[43]. Fortrainingourframework,we
adopttheoptimizerAdamW[25]withthelearningratechosenfrom{5e−5,1e−4,5e−4}. Batch
sizeissetto5. Wesetthelearningratewarm-upwith1,000steps. Thenumberoftotaltrainingsteps
is50k,andthemodelisevaluatedevery10ksteps.
4.1.1 Metrics
EventhoughTop-Kaccuracy,wheretheground-truthrankingisbasedonDPRscores[16],iscom-
monlyusedinthemeasurementofreranking[7,13],thismetricisunsuitableforindicatingtheoverall
rerankingperformanceforallpositivedocuments.Moreover,withthepromisingdevelopmentofLLM
1https://github.com/wangcunxiang/Graph-aS-Tokens/tree/main
6inlearningtherelevancebetweentexts,DPRscoresmaylosetheiradvantageandfairness. Toaddress
thisissue,othermetricssuchasMeanReciprocalRank(MRR)andMeanHits@10(MHits@10)
areusedformeasuringthererankingperformance[42]. Tobespecific,TheMeanReciprocalRank
(MRR)scoreofpositivedocumentisgivenbyMRR= 1 (cid:80) ( 1 (cid:80) 1 ),whereQisthe
|Q| q∈Q |P+| p∈P+ rp
questionsetfromtheevaluatingdataset,P+isthesetofpositivedocuments,andr istherankof
p
documentpestimatedbythereranker. TheMHits@10indicatesthepercentageofpositivedocuments
thatarerankedintheTop10,givenbyMHits@10= 1 (cid:80) ( 1 (cid:80) I(r <=10)),where
|Q| q∈Q |P+| p∈P+ p
theindicationI(A)=0iftheeventAistrue,otherwise0.
Theabovemetricsworkwellformostcases,however,theymayfailtofairlycharacterizetheranking
performancewhentherearetiesinrankingscores,whichiscommoninrelevantscoresgeneratedby
LLMssuchasChatGPT[31],PaLM2[9],LLaMA[38],andGPT4[32]. PleaserefertoFig5inthe
Appendixforthedetailedpromptandresultsofrelevantscoresbetweenquestionsanddocuments.
Toaddresstiesintherankingscores,weproposevariantsofMRRandMHits@10. Denoter(t)asthe
p
rankofthedocumentpwithtties. Inotherwords,therelevantscorebetweenthequestionandthe
documentpisthesameasothert−1documents. ThevariantofMRRfortiedrankingisnamed
MeanTiedReciprocalRanking(MTRR),representedas
(cid:18)
1 (cid:88) 1 (cid:88) 1
MTRR= I(t=1)
|Q| |P+| r(t)
q∈Q p∈P+ p
(cid:19)
2
+ I(t>1) . (11)
r(t)+r(t)+t−1
p p
ThemetricMTRRaddressesthetiedrankr(t)estimatedbythererankerviaaveragingtheoptimistic
p
rankr(t)andthepessimisticrankr(t)+t−1. ThemetricsMRRandMTRRarethesamewhenthere
p p
isnorankingtie. ThevariantofMHits@10fortiedrankingisTiedMeanHits@10(TMHit@10).
DenoteH(p)asthesetthatincludesalltheranks{r(ti)}thatarehigherthantherankofdocumentp,
pi
i.e.,r(τ). Basedonthesenotations,wepresentthenewmetricas:
p
 
1 (cid:88) 1 (cid:88)
TMHits@10=  Hits@10(p), (12)
|Q| |P+|
q∈Q p∈P+
whereHits@10(p)isdefinedas
  0 (1, 0if −(cid:80) (cid:80)it ii t> i)/1 τ0 ,f ifor 0∀ <r p( (cid:80)t ii) i∈
t
iH <( 1p 0),
for∀r(ti) ∈H(p)andτ >1,
 11 ,0 o/ tτ h, ei rf wHp isi ( ep .)=∅andτ >10,
IftherearetiesintheTop-10ranking,themetricTMHit@10diminishingtheoptimisticeffectvia
dividingthehit-number(nogreaterthan10)bythenumberofties.
4.2 ComparingRerankerSystems
Wecompareourproposedalgorithmwithbaselinesasfollowswithfixedhyper-parametersandno
fine-tuning,wherethehiddendimensionis8,thedropoutrateis0.1,andthelearningrateis1e-4.
w/o reranker: Without an additional reranker, the ranking score is based on the retrieval scores
providedbyDPR[16]. BART:Thepre-trainedlanguagemodelBART[19]servesasthereranker.
BART-GST:Thismethodintegratesgraph-as-tokenintothepre-trainedmodel[42]. Foreachdataset,
weusethebestperformanceprovidedinthepaper. RGCN-S[42]stackstheRGCNmodelonthe
top of the transformer. Even though this method is based on graph neural networks, it doesn’t
relyonthedocumentgraphs,butconstructnodesinthegraphmodelbasedonthetextalignment
inquestion-documentpairs. MLP:Theinitialnodefeaturesareonlybasedondocumenttextas
describedin(1)withtheBERT[6]encoder. AfterthenodefeaturesgothroughMLP,wegetthe
relevantscoresvia(8)andtakethecross-entropyfunction(9)astrainingloss.GCN:Besidesupdating
noderepresentationsviaGCN,therestsettingisthesameasMLP.Wealsoconductexperimentswith
7differentGNNmodels. PleaserefertoAppendixBfordetails. G-RAG:Theinitialnodefeatures
arebasedondocumenttextandAMRinformationasdescribedin(2). Therestofthesettingisthe
sameasGCN.G-RAG-RL:Usingtherankinglossfunction andkeeptheothersettingthesameas
G-RAG.
NQ TQA
strategy MRR MH MRR MH
w/oreranker 20.2 18.0 37.9 34.6 12.1 12.3 25.5 25.9
BART 25.7 23.3 49.3 45.8 16.9 17.0 37.7 38.0
BART-GST 28.4 25.0 53.2 48.7 17.5 17.6 39.1 39.5
RGCN-S 26.1 23.1 49.5 46.0 — — — —
MLP 19.2 17.8 40.0 38.8 17.6 17.1 34.0 31.4
GCN 22.6 22.4 47.6 44.2 18.2 17.4 38.0 37.0
G-RAG 25.1 24.2 49.1 47.2 18.5 18.3 38.5 39.1
G-RAG-RL 27.3 25.7 49.2 47.4 19.8 18.3 42.9 39.4
Table1: Resultsonthedev/testsetofNQandTQAwithouthyperparameterfine-tuning.
TheresultsonMRRandMHits@10ontheNQandTQAdatasetsareprovidedinTable1. Notethat
theresultsonNQalwaysoutperformtheresultsonTQA,thisisduetoasmallernumberofpositive
documentsmakingiteasytoputmostofthepositivedocumentsintotheTop10. Generallyspeaking,
TQAisamorecomplexandrobustdatasetthanNQ.Modelswithgraph-basedapproaches,suchas
GCNandG-RAG,showcompetitiveperformanceacrossmetrics. Thesemethodshaveadvantages
overthebaselinemodels,i.e.,withoutrerankerandMLP.Inconclusion,basedonthesimulation
results,theproposedmethodG-RAG-RLemergesasastrongmodel,indicatingtheeffectivenessof
graph-basedstrategiesandthebenefitofpairwiserankinglossonidentifyingpositivedocuments. To
highlighttheadvantagesoftheproposedG-RAGoverstate-of-the-artbenchmarks,weconducted
experimentsacrossvariousembeddingmodelswithfine-tuningparameterinthenextsection.
NQ TQA
strategy MRR MH MRR MH
w/oreranker 20.2 18.0 37.9 34.6 12.1 12.3 25.5 25.9
BART 25.7 23.3 49.3 45.8 16.9 17.0 37.7 38.0
PaLM2XS 14.9 14.0 34.1 34.2 11.6 12.5 29.1 31.6
PaLM2L 18.6 17.9 40.7 39.7 12.7 12.9 34.7 35.6
G-RAG-RL 27.3 25.7 49.2 47.4 19.8 18.3 42.9 39.4
Table2: ResultsofPaLM2beingthereranker. SmallembeddingmodelsoutperformLLMsinthis
setting. Incomparison,G-RAG-RLconsiderablyimprovestheresultscomparedtobothlanguage
modeltypesbyleveragingconnectioninformationacrossdocuments. WeuseTiedMeanHits@10.
4.3 UsingdifferentLLMsasEmbeddingModels
ThefeatureencoderalwaysplaysavitalroleinNLPtasks. Betterembeddingmodelsaremorelikely
tofetchsimilaritiesacrosscontextsandhelpidentifyhighlyrelevantcontext. BesidestheBERT
modelusedinthestate-of-the-artreranker,manypromisingembeddingmodelshavebeenproposed
recently. Toevaluatetheeffectivenessofdifferentembeddingmodels,i.e.,BERT[6],GTE[22],BGE
[44],Ember[24],weconducttheexperimentsunderthesamesettingasG-RAG-RL.Theresultsare
presentedinTable3. Forconvenience,wedirectlyaddtworesultsfromSection4.2: BART-GST
andBERT.Emberperformsconsistentlywellacrossallevaluations. Inconclusion,Emberappears
tobethetop-performingmodel,followedcloselybyGTEandBGE,whileBART-GSTandBERT
showslightlylowerperformanceacrosstheevaluatedmetrics. Thusourfine-tuningresultisbased
onG-RAG-RLwithEmberastheembeddingmodel. Thegridsearchsettingforhyperparameteris
introducedinSection4.1. Weonlyrun10kiterationsforeachsettingandpickuptheonewiththe
bestMRR.Theresultwithhyperparametertuning,i.e.,Ember(HPs-T),isaddedinTable3. Even
thoughBART-GSTdemonstratescompetitiveperformanceinsomescenarios,itispronetooverfitting
8especiallyintermsofMRRontheNQdataset. However,theproposedmethods,i.e.,Emberand
Ember(HPs-T),aremorelikelytoavoidoverfittingandachievethehighestscoreacrossalltestsets.
NQ TQA
embedding MRR MH MRR MH
BART-GST 28.4 25.0 53.2 48.7 17.5 17.6 39.1 39.5
BERT 27.3 25.7 49.2 47.4 19.8 18.3 42.9 39.4
GTE 29.9 26.3 52.6 47.7 19.2 19.3 41.8 40.3
BGE 28.7 27.4 52.1 48.2 18.7 18.3 43.4 40.7
Ember 9.0 26.1 52.9 48.0 19.8 18.6 44.3 42.0
Ember(HPs-T) 28.9 27.7 51.1 50.0 20.0 19.4 41.6 41.4
Table3: G-RAGwithchangingtheembeddingmodel.
4.4 InvestigatingPaLM2Scores
Toevaluatetheperformanceoflargelanguagemodelsonthererankingtask,weconductzero-short
experimentsonthedev&testsetsoftheNQandTQAdatasets. AnexampleofLLM-generated
relevancescoreisillustratedinFigure5intheAppendix.
Ingeneral, weobservethatscoresgeneratedbyPaLM2areintegersbetween0and100thatare
divisibleby5. Thisoftenleadstotiesindocumentsrankings. Toaddressthetiesintherankingscore,
weusetheproposedmetricsMTRR(Eq.11)andTMHits@10(Eq.12)toevaluatetheperformanceof
rerankerbasedonPaLM2[9]. Fortheconvenienceofcomparison,wecopyw/o rerank,BART,
andG-RAGresultsfromSection4.2. Sincethereisnotiedrankingprovidedbyw/o rerankand
BART,theMRRandMHits@10havethesamevaluesasMTRRandTMhits@10,respectively.
TheperformanceresultsareprovidedinTable2. TheresultsdemonstratethatLLMswithzero-shot
learningdoesnotdowellinrerankingtasks. Thismaybecausedbytoomanytiesintherelevance
scores, especiallyforsmall-sizeLLMwheretherearemoreofthem. Thisresultemphasizesthe
importanceofrerankingmodeldesigninRAGevenintheLLMera. Morequalitativeexamplesbased
onPaLM2areprovidedinAppendixC.
WecomparetheresultsofbothapproacheswithG-RAGwhichbringsadditionalperspectivetothese
results. Leveragingtheinformationaboutconnectionsofentitiesacrossdocumentsanddocuments
themselvesbringssignificantimprovementsupto7percentagepoints.
5 Conclusions
Our proposed model, G-RAG, addresses limitations in existing ODQA methods by leveraging
implicitconnectionsbetweendocumentsandstrategicallyintegratingAMRinformation. Ourmethod
identifiesdocumentswithvaluableinformationsignificantlybetterevenwhenthisinformationis
only weakly connected to the question context. This happens because documents connected by
thedocumentgraphshareinformationthatisrelevantforthefinalanswer. WeintegratekeyAMR
informationtoimproveperformancewithoutincreasingcomputationalcost. Wealsoproposedtwo
metricstofairlyevaluatetheperformanceofawiderangeofrankingscenariosincludingtiedranking
scores. Furthermore,ourinvestigationintotheperformanceofPaLM2asarerankeremphasizes
thesignificanceofrerankermodeldesigninRAG,asevenanadvancedpre-trainedLLMmightface
challengesinthererankingtask.
Recently,paperssuchas[27,36]introducedmethodsforrerankinglistwisedocumentsusingLLMs.
Despitethis,ourproposedmetricMTRRremainsvalidforcomparisonwiththeirapproachesmea-
suredbyMRR(mentionedinpaper[27]). Thusourmethodhaspotentialforbroaderadoptionand
comparison with existing approaches. Additionally, we’re enthusiastic about investigating more
advancedtechniquestoefficientlyresolvetiesinrankingscoresproducedbyLLMs. Therearemore
directions for future study. For instance, designing more sophisticated models to better process
AMR information and integrating this information into node & edge features will bring further
improvementsinreranking. Further,whileapre-trainedLLMdoesnothaveimpressiveperformance
asarerankeritself,fine-tuningitmaybeextremelyusefulforenhancingtheperformanceofRAG
systems.
9References
[1] AkariAsai,KazumaHashimoto,HannanehHajishirzi,RichardSocher,andCaimingXiong.
Learning to retrieve reasoning paths over wikipedia graph for question answering. In 8th
InternationalConferenceonLearningRepresentations, ICLR2020, AddisAbaba, Ethiopia,
April26-30, 2020.OpenReview.net, 2020. URLhttps://openreview.net/forum?
id=SJgVHkrYDH. Citedonpage3.
[2] XuefengBai, YulongChen, LinfengSong, andYueZhang. Semanticrepresentationfordi-
alogue modeling. In Proceedings of the 59th Annual Meeting of the Association for Com-
putational Linguistics and the 11th International Joint Conference on Natural Language
Processing (Volume 1: Long Papers), pages 4430–4445, Online, August 2021. Associa-
tion for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.342. URL https:
//aclanthology.org/2021.acl-long.342. Citedonpage3.
[3] XuefengBai,YulongChen,andYueZhang.Graphpre-trainingforAMRparsingandgeneration.
InProceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics
(Volume1: LongPapers),pages6001–6015,Dublin,Ireland,May2022.AssociationforCom-
putational Linguistics. URL https://aclanthology.org/2022.acl-long.415.
Citedonpages4and6.
[4] LauraBanarescu,ClaireBonial,ShuCai,MadalinaGeorgescu,KiraGriffitt,UlfHermjakob,
Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. Abstract Meaning
Representationforsembanking. InProceedingsofthe7thLinguisticAnnotationWorkshopand
InteroperabilitywithDiscourse,pages178–186,Sofia,Bulgaria,August2013.Associationfor
ComputationalLinguistics. URLhttps://aclanthology.org/W13-2322. Citedon
page3.
[5] JoseOrtizCostaandAnaghaKulkarni. Leveragingknowledgegraphforopen-domainquestion
answering. In2018IEEE/WIC/ACMInternationalConferenceonWebIntelligence(WI),pages
389–394.IEEE,2018. Citedonpage3.
[6] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. BERT:Pre-trainingof
deepbidirectionaltransformersforlanguageunderstanding. InProceedingsofthe2019Confer-
enceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics: Human
LanguageTechnologies,Volume1(LongandShortPapers),pages4171–4186,Minneapolis,
Minnesota,June2019.AssociationforComputationalLinguistics. doi: 10.18653/v1/N19-1423.
URLhttps://www.aclweb.org/anthology/N19-1423. Citedonpages1,3,4,6,
7,and8.
[7] MichaelGlass,GaetanoRossiello,MdFaisalMahbubChowdhury,AnkitaNaik,PengshanCai,
andAlfioGliozzo. Re2G:Retrieve,rerank,generate. InProceedingsofthe2022Conference
of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, pages 2701–2715, Seattle, United States, July 2022. Association
for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.194. URL https://
aclanthology.org/2022.naacl-main.194. Citedonpage6.
[8] LiyuGongandQiangCheng. Exploitingedgefeaturesforgraphneuralnetworks. InProceed-
ingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages9211–9219,
2019. Citedonpage5.
[9] Google,RohanAnil,AndrewM.Dai,OrhanFirat,MelvinJohnson,DmitryLepikhin,Alexandre
Passos,SiamakShakeri,EmanuelTaropa,PaigeBailey,ZhifengChen,EricChu,JonathanH.
Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica
Moreira,MarkOmernick,KevinRobinson,SebastianRuder,YiTay,KefanXiao,Yuanzhong
Xu, YujingZhang, GustavoHernandezAbrego, JunwhanAhn, JacobAustin, PaulBarham,
JanBotha,JamesBradbury,SiddharthaBrahma,KevinBrooks,MicheleCatasta,YongCheng,
ColinCherry,ChristopherA.Choquette-Choo,AakankshaChowdhery,ClémentCrepy,Shachi
Dave,MostafaDehghani,SunipaDev,JacobDevlin,MarkDíaz,NanDu,EthanDyer,Vlad
Feinberg,FangxiaoyuFeng,VladFienber,MarkusFreitag,XavierGarcia,SebastianGehrmann,
LucasGonzalez,GuyGur-Ari,StevenHand,HadiHashemi,LeHou,JoshuaHowland,Andrea
Hu,JeffreyHui,JeremyHurwitz,MichaelIsard,AbeIttycheriah,MatthewJagielski,Wenhao
10Jia,KathleenKenealy,MaximKrikun,SnehaKudugunta,ChangLan,KatherineLee,Benjamin
Lee,EricLi,MusicLi,WeiLi,YaGuangLi,JianLi,HyeontaekLim,HanzhaoLin,Zhongtao
Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra,
MaysamMoussalem,ZacharyNado,JohnNham,EricNi,AndrewNystrom,AliciaParrish,
Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan
Richter,ParkerRiley,AlexCastroRos,AurkoRoy,BrennanSaeta,RajkumarSamuel,Renee
Shelby,AmbroseSlone,DanielSmilkov,DavidR.So,DanielSohn,SimonTokumine,Dasha
Valter,VijayVasudevan,KiranVodrahalli,XuezhiWang,PidongWang,ZiruiWang,TaoWang,
JohnWieting, YuhuaiWu, KelvinXu, YunhanXu, LintingXue, PengchengYin, JiahuiYu,
QiaoZhang,StevenZheng,CeZheng,WeikangZhou,DennyZhou,SlavPetrov,andYonghui
Wu. Palm2technicalreport,2023. Citedonpages2,3,7,and9.
[10] WillHamilton, ZhitaoYing, andJureLeskovec. Inductiverepresentationlearningonlarge
graphs. Advancesinneuralinformationprocessingsystems,30,2017. Citedonpage15.
[11] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and
WayneXinZhao. Largelanguagemodelsarezero-shotrankersforrecommendersystems. In
EuropeanConferenceonInformationRetrieval,pages364–381.Springer,2024. Citedonpage
3.
[12] DengrongHuang,ZizhongWei,AizhenYue,XuanZhao,ZhaoliangChen,RuiLi,KaiJiang,
BingxinChang,QilaiZhang,SijiaZhang,etal. Dsqa-llm: Domain-specificintelligentquestion
answeringbasedonlargelanguagemodel.InInternationalConferenceonAI-generatedContent,
pages170–180.Springer,2023. Citedonpage3.
[13] GautierIzacardandÉdouardGrave. Leveragingpassageretrievalwithgenerativemodelsfor
open domain question answering. In Proceedings of the 16th Conference of the European
ChapteroftheAssociationforComputationalLinguistics: MainVolume,pages874–880,2021.
Citedonpages3and6.
[14] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale
distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and
Min-YenKan,editors,Proceedingsofthe55thAnnualMeetingoftheAssociationforCom-
putational Linguistics (Volume 1: Long Papers), pages 1601–1611, Vancouver, Canada,
July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL
https://aclanthology.org/P17-1147. Citedonpages4and6.
[15] MingxuanJu, WenhaoYu, TongZhao, ChuxuZhang, andYanfangYe. Grape: Knowledge
graphenhancedpassagereaderforopen-domainquestionanswering. InFindingsofEmpirical
MethodsinNaturalLanguageProcessing,2022. Citedonpage3.
[16] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov,
DanqiChen,andWen-tauYih. Densepassageretrievalforopen-domainquestionanswering.
InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcess-
ing (EMNLP), pages 6769–6781, Online, 2020. Association for Computational Linguistics.
doi: 10.18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.
emnlp-main.550. Citedonpages3,4,6,7,and16.
[17] ThomasNKipfandMaxWelling. Semi-supervisedclassificationwithgraphconvolutional
networks. InICLR,2017. Citedonpages5,6,and15.
[18] TomKwiatkowski,JennimariaPalomaki,OliviaRedfield,MichaelCollins,AnkurParikh,Chris
Alberti,DanielleEpstein,IlliaPolosukhin,JacobDevlin,KentonLee,etal. Naturalquestions:a
benchmarkforquestionansweringresearch. TransactionsoftheAssociationforComputational
Linguistics,7:453–466,2019. Citedonpages4and6.
[19] MikeLewis,YinhanLiu,NamanGoyal,MarjanGhazvininejad,AbdelrahmanMohamed,Omer
Levy,VeselinStoyanov,andLukeZettlemoyer. BART:Denoisingsequence-to-sequencepre-
trainingfornaturallanguagegeneration,translation,andcomprehension. InProceedingsof
the58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages7871–7880,
Online,July2020.AssociationforComputationalLinguistics. doi: 10.18653/v1/2020.acl-main.
703. URLhttps://www.aclweb.org/anthology/2020.acl-main.703. Cited
onpages1,3,and7.
11[20] PatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,Naman
Goyal,HeinrichKüttler,MikeLewis,Wen-tauYih,TimRocktäschel,etal.Retrieval-augmented
generationforknowledge-intensivenlptasks. NeurIPS,33:9459–9474,2020. Citedonpage3.
[21] YunchengLi,YaleSong,andJieboLuo. Improvingpairwiserankingformulti-labelimageclas-
sification. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
pages3617–3625,2017. Citedonpage6.
[22] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang.
Towardsgeneraltextembeddingswithmulti-stagecontrastivelearning,2023. Citedonpages6
and8.
[23] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,OmerLevy,Mike
Lewis,LukeZettlemoyer,andVeselinStoyanov. Roberta: Arobustlyoptimizedbertpretraining
approach. arXivpreprintarXiv:1907.11692,2019. Citedonpages1,3,and4.
[24] Zheng Liu and Yingxia Shao. RetroMAE: Pre-training retrieval-oriented transformers via
maskedauto-encoder. arXivpreprintarXiv:2205.12035,2022. Citedonpages6and8.
[25] IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. In7thInternational
ConferenceonLearningRepresentations,ICLR2019,NewOrleans,LA,USA,May6-9,2019.
OpenReview.net,2019. URLhttps://openreview.net/forum?id=Bkg6RiCqY7.
Citedonpage6.
[26] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for
multi-stagetextretrieval. arXivpreprintarXiv:2310.08319,2023. Citedonpage3.
[27] XueguangMa,XinyuZhang,RonakPradeep,andJimmyLin. Zero-shotlistwisedocument
rerankingwithalargelanguagemodel. arXivpreprintarXiv:2305.02156,2023. Citedonpage
9.
[28] YuboMa,YixinCao,YongHong,andAixinSun. Largelanguagemodelisnotagoodfew-shot
informationextractor,butagoodrerankerforhardsamples! InFindingsoftheAssociationfor
ComputationalLinguistics: EMNLP2023,pages10572–10601,2023. Citedonpage3.
[29] Tahira Naseem, Austin Blodgett, Sadhana Kumaravel, Timothy J. O’Gorman, Young-Suk
Lee,JeffreyFlanigan,RamónFernándezAstudillo,RaduFlorian,SalimRoukos,andNathan
Schneider. Docamr: Multi-sentenceamrrepresentationandevaluation. InNorthAmerican
ChapteroftheAssociationforComputationalLinguistics,2021. Citedonpage3.
[30] RodrigoNogueira,ZhiyingJiang,RonakPradeep,andJimmyLin. Documentrankingwith
apretrainedsequence-to-sequencemodel. InFindingsoftheAssociationforComputational
Linguistics: EMNLP2020,pages708–718,2020. Citedonpage3.
[31] OpenAI. ChatGPT. https://openai.com/research/chatgpt.,. Citedonpages3
and7.
[32] OpenAI. GPT-4. https://openai.com/gpt-4,. Citedonpages3and7.
[33] EunhwanPark,Sung-MinLee,DearyongSeo,SeonhoonKim,InhoKang,andSeung-HoonNa.
Rink: reader-inheritedevidencererankerfortable-and-textopendomainquestionanswering. In
ProceedingsoftheAAAIConferenceonArtificialIntelligence,volume37,pages13446–13456,
2023. Citedonpage3.
[34] RonakPradeep,YuqiLiu,XinyuZhang,YilinLi,AndrewYates,andJimmyLin. Squeezing
water from a stone: a bag of tricks for further improving cross-encoder effectiveness for
reranking. InEuropeanConferenceonInformationRetrieval,pages655–670.Springer,2022.
Citedonpage3.
[35] Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib
Rana, and Suranga Nanayakkara. Improving the domain adaptation of retrieval augmented
generation(rag)modelsforopendomainquestionanswering. TransactionsoftheAssociation
forComputationalLinguistics,11:1–17,2023. Citedonpages1and3.
12[36] WeiweiSun,LingyongYan,XinyuMa,ShuaiqiangWang,PengjieRen,ZhuminChen,Dawei
Yin, andZhaochunRen. Ischatgptgoodatsearch? investigatinglargelanguagemodelsas
re-rankingagents. InProceedingsofthe2023ConferenceonEmpiricalMethodsinNatural
LanguageProcessing,pages14918–14937,2023. Citedonpages3and9.
[37] YimingTan,DehaiMin,YuLi,WenboLi,NanHu,YongruiChen,andGuilinQi. Canchatgpt
replacetraditionalkbqamodels? anin-depthanalysisofthequestionansweringperformanceof
thegptllmfamily. InInternationalSemanticWebConference,pages348–367.Springer,2023.
Citedonpage3.
[38] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timo-
théeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Open
andefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023. Citedon
pages3and7.
[39] PetarVelicˇkovic´,GuillemCucurull,ArantxaCasanova,AdrianaRomero,PietroLiò,andYoshua
Bengio. Graphattentionnetworks. InInternationalConferenceonLearningRepresentations,
2018. Citedonpage15.
[40] EllenM.VoorheesandDawnM.Tice. TheTREC-8questionansweringtrack. InM.Gavrilidou,
G. Carayannis, S. Markantonatou, S. Piperidis, and G. Stainhauer, editors, Proceedings of
the Second International Conference on Language Resources and Evaluation (LREC’00),
Athens,Greece,May2000.EuropeanLanguageResourcesAssociation(ELRA). URLhttp:
//www.lrec-conf.org/proceedings/lrec2000/pdf/26.pdf. Citedonpage1.
[41] CunxiangWang,SiruiCheng,ZhikunXu,BowenDing,YidongWang,andYueZhang. Evalu-
atingopenquestionansweringevaluation. arXivpreprintarXiv:2305.12421,2023. Citedon
page3.
[42] CunxiangWang,ZhikunXu,QipengGuo,XiangkunHu,XuefengBai,ZhengZhang,andYue
Zhang. ExploitingAbstractMeaningRepresentationforopen-domainquestionanswering. In
AnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki,editors,FindingsoftheAssociation
for Computational Linguistics: ACL 2023, pages 2083–2096, Toronto, Canada, July 2023.
Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.131. URL
https://aclanthology.org/2023.findings-acl.131. Citedonpages1,2,3,
4,6,and7.
[43] ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,Anthony
Moi,PierricCistac,TimRault,RémiLouf,MorganFuntowicz,etal. Huggingface’stransform-
ers: State-of-the-artnaturallanguageprocessing. arXivpreprintarXiv:1910.03771,2019. URL
https://arxiv.org/abs/1910.03771. Citedonpage6.
[44] ShitaoXiao,ZhengLiu,PeitianZhang,andNiklasMuennighoff. C-pack: Packagedresources
toadvancegeneralchineseembedding,2023. Citedonpages6and8.
[45] KeyuluXu,WeihuaHu,JureLeskovec,andStefanieJegelka. Howpowerfularegraphneural
networks? InInternationalConferenceonLearningRepresentations,2018. Citedonpage15.
[46] DonghanYu,ChenguangZhu,YuweiFang,WenhaoYu,ShuohangWang,YichongXu,Xiang
Ren, Yiming Yang, and Michael Zeng. KG-FiD: Infusing knowledge graph in fusion-in-
decoderforopen-domainquestionanswering. InACL,pages4961–4974,Dublin,Ireland,May
2022.AssociationforComputationalLinguistics. doi: 10.18653/v1/2022.acl-long.340. URL
https://aclanthology.org/2022.acl-long.340. Citedonpages2and3.
[47] HongleiZhuang,ZhenQin,RolfJagerman,KaiHui,JiMa,JingLu,JianmoNi,XuanhuiWang,
andMichaelBendersky. Rankt5: Fine-tuningt5fortextrankingwithrankinglosses. InSIGIR,
pages2308–2313,2023. Citedonpage3.
13Train Dev Test
NaturalQuestions 79168 8757 3610
TriviaQA 78785 8837 11313
Table4: DatasetStatistics.
A DatasetStatistics
InFig. 2,weillustratetheAMRgraphstatisticsinthedatasetsNaturalQuestions(NQ)andTriviaQA.
To better illustrate the structure of the shortest path, we also conduct some experiments to show
the statistic of the shortest path in the AMR graph, see Fig 3. We analyze the shortest single
sourcepaths(SSSPs)intheAMRgraphsofdocumentsandtrytoestablishtheconnectionbetween
questioncontextsanddocumentcontexts. TheanalysisrevealsanotabletrendintheAMRgraphsof
documents,indicatingthatcertainnegativedocumentscannotestablishadequateconnectionstothe
questioncontextwithintheirtext. Thispatternbringsinsightsintotheencodingprocesstoenhance
rerankingperformance.
(a)NQ-train (b)NQ-train (c)NQ-dev (d)NQ-dev
(e)NQ-test (f)NQ-test (g)TQA-train (h)TQA-train
(i)TQA-dev (j)TQA-dev (k)TQA-test (l)TQA-test
Figure2: NumberofnodesandedgesinAMRgraphsintrain/dev/testsetofdatasetNQandTQA.
14(a)NQ-Positive (b)NQ-Negative (c)TQA-Positive (d)TQA-Negative
Figure3: NumberofSSSPsAMRgraphsintrainsetofdatasetNQandTQA.
Figure4: ThepipelineofG-RAG.
B SimulationResultswithDifferentGNNModels.
BesidestheGCN[17]modelconsideredinthemainmanuscript,wecomparethesimulationresults
withdifferentGNNmodelsinthissection. Specifically,underthesamesettingastheGCNmodelin
Ember(HPs-T)fromTable3,weuseGAT[39]withadditionalparameternumberofheadsbeing8,
GraphSage[10]withtheaggregationchoicebeing‘lstm’,andGIN[45]withtheaggregationchoice
being‘mean’. ThecomparisonresultsareillustratedinTable5. Fortheconvenienceofcomparison,
wedirectlyaddtworesultsfromSection4.2,i.e.,BART-GSTandGCN(i.e.,Ember(HPs-T)inTable
3). ItshowsthattheGCNmodelstilloutperformsinmostcases. Thismaybeduetothedocument
graphsconsideredinourpaperbeingverysmall,whiletheadvancedGNNmodelusuallytargets
handlingthousands,ormillionsofnodesinthegraph. Besides,ourmodelhasalreadytakentheedge
featureintoconsideration,whichmayleadtooverfittingifintroducingmoreweightparameters.
NQ TQA
Embedding
MRR_dev MRR_test MH_dev MH_test MRR_dev MRR_test MH_dev MH_test
/Metric
BART-GST 28.4 25.0 53.2 48.7 17.5 17.6 39.1 39.5
GCN 28.9 27.7 51.1 50.0 20.0 19.4 41.6 41.4
GAT 28.1 27.1 52.3 47.2 19.1 18.9 43.0 41.0
GraphSage 29.8 26.5 52.3 47.2 19.6 18.4 42.9 39.7
GIN 28.4 27.8 50.2 48.5 19.7 18.9 42.2 39.3
Table5: ResultsofG-RAGwithdifferentGNNmodels. WeuseMeanHits@10.
C QualitativeExamples
Wetaketherankingscoresgivenbypalm2Lasabaselinetoinvestigatehowthegraph-basedmodel
brings benefits to reranking in Open-Domain Question Answering. Since TQA is a much more
complexdatasetwithmorepositivedocuments,wetakeanexamplefromTQA.
15Question: Ol’BlueEyesisthenicknameof?
GoldAnswer: [‘Sinatra(film)’,‘BiographyofFrankSinatra’,‘ColumbusDayRiot’,‘Life
ofFrankSinatra’,‘AVoiceinTime: 1939–1952’,‘Sinatra’,‘Biographyoffranksinatra’,
‘Ol’BlueEyes’,‘AVoiceinTime: 1939-1952’,‘Politicalbeliefsoffranksinatra’,‘Franck
Sinatra’,‘OldBlueEyes’,‘FrankSinatra’,‘FrankSinatraI’,‘FrancisAlbertFrankSinatra’,
‘FrancisA.Sinatra’,‘FrankSinatra,Sr.’,‘FrancisAlbertSinatra’,‘PoliticalbeliefsofFrank
Sinatra’, ‘Old blue eyes’, ‘Frank sanatra’, ‘Frank sinatra’, ‘Frank senatra’, ‘FBI Files on
FrankSinatra’,‘FrancisSinatra’]
NumberofPositivedocuments: 24positivedocumentsoutof100documents
ThefollowingareTop-10documentsgivenbytheproposedGNN-basedreranker. Eachdocument
isaccompaniedbyrelevantinformationaboutitsAMRgraph,includingthenumberofnodesand
edges,aswellasthecountofsingle-sourceshortestpaths(SSSPs)originatingfromthenodelabeled
“question". Ifthenode“question"isnotpresentintheAMRgraph,theSSSPscountisnotedas0.
Additionally,wepresentthecorrespondingscoreassignedbypalm2-Landitsrankbasedonthe
palm2reranker. TherankingassignedbytheretrieverDPRisalsoprovidedforreference. [16].
1st: Sinatra in 1998, for example, the building was bathed in blue light to represent the
singer’snickname"Ol’BlueEyes". AfteractressFayWray,whostarredin"KingKong",
diedinSeptember2004,thebuildinglightswereextinguishedfor15minutes. Thefloodlights
bathedthebuildinginred, white,andblueforseveralmonthsafterthedestructionofthe
WorldTradeCenterinSeptember2001,thenrevertedtothestandardschedule. OnJune4,
2002,theEmpireStateBuildingdonnedpurpleandgold(theroyalcolorsofElizabethII),in
thanksfortheUnitedKingdomplayingtheStarSpangledBanner
---------------------------------------------
AMRgraphinformation: #nodes: 51,#edges82,#SSSP:32
Scorebypalm2: 50/100,Rankbypalm2: 9/100
RankbyDPR:5/100
2nd: andactivelycampaignedforpresidentssuchasHarryS.Truman,JohnF.Kennedyand
RonaldReagan. Incrime,theFBIinvestigatedSinatraandhisallegedrelationshipwiththe
Mafia. WhileSinatraneverlearnedhowtoreadmusic,hehadanimpressiveunderstandingof
it,andheworkedveryhardfromayoungagetoimprovehisabilitiesinallaspectsofmusic.
Aperfectionist,renownedforhisdresssenseandperformingpresence,healwaysinsisted
onrecordinglivewithhisband. Hisbrightblueeyesearnedhimthepopularnickname"Ol’
BlueEyes". Sinatraledacolorfulpersonallife,and
---------------------------------------------
AMRgraphinformation: #nodes: 53,#edges75,#SSSP:34
Scorebypalm2: 100/100,Rankbypalm2: 1/100
RankbyDPR:1/100
3rd: claimedthatSinatrahadgrown"tiredofentertainingpeople,especiallywhenallthey
reallywantedwerethesameoldtuneshehadlongagobecomeboredby". Whilehewas
in retirement, President Richard Nixon asked him to perform at a Young Voters Rally in
anticipation of the upcoming campaign. Sinatra obliged and chose to sing "My Kind of
Town"fortherallyheldinChicagoonOctober20,1972. In1973,Sinatracameoutofhis
short-livedretirementwithatelevisionspecialandalbum. Thealbum,entitled"Ol’Blue
EyesIsBack",arrangedbyGordonJenkinsandDonCosta,
---------------------------------------------
AMRgraphinformation: #nodes: 52,#edges85,#SSSP:19
Scorebypalm2: 20/100,Rankbypalm2: 27/100
RankbyDPR:8/100
164th: State Police would attend, searching for organized crime members in the audience.
Duringa1979appearanceinProvidence,MayorBuddyCiancinamedSinatraanhonorary
firechief,completewithahelmetbearingthename"F.SINATRA"withnickname"Ol’Blue
Eyes"beneath. DavidBowie’sconcertonMay5,1978wasoneofthreerecordedforhislive
album"Stage". TheBeeGeesperformedtwosold-outconcertshereonAugust28–29,1979
aspartoftheirSpiritsHavingFlownTour. TheKinksrecordedmuchoftheirlivealbumand
video,"OnefortheRoad"attheCivicCenterSeptember23,1979.
---------------------------------------------
AMRgraphinformation: #nodes: 54,#edges67,#SSSP:0
Scorebypalm2: 50/100,Rankbypalm2: 9/100
RankbyDPR:6/100
5th: illness). PasettawastheproduceroftheElvisPresleyconcertspecial, "Alohafrom
HawaiiViaSatellite"inJanuary1973. Theshowstillholdstherecordforthemostwatched
television special in history; viewing figures are between 1 and 1.5 billion live viewers
worldwide. 1973alsosawPasettadirect"MagnavoxPresentsFrankSinatra"(alsoknownas
"Ol’BlueEyesIsBack"),thetelevisionspecialthatmarkedFrankSinatra’scomebackfrom
retirement. Pasettadiedina2015single-caraccident. ThevehicledrivenbyKeithStewart
collidedwithPasettashortlyafterStewarthadallowedhispassengerstodisembark. Marty
PasettaMartinAllen
---------------------------------------------
AMRgraphinformation: #nodes: 39,#edges59,#SSSP:39
Scorebypalm2: 50/100,Rankbypalm2: 9/100
RankbyDPR:3/100
6th: himfeelwealthyandimportant,andthathewasgivinghisverybesttotheaudience.
Hewasalsoobsessedwithcleanliness—whilewiththeTommyDorseybandhedeveloped
thenickname"LadyMacbeth",becauseoffrequentshoweringandswitchinghisoutfits. His
deepblueeyesearnedhimthepopularnickname"Ol’BlueEyes". ForSantopietro,Sinatra
wasthepersonificationofAmericainthe1950s: "cocky,eyeonthemainchance,optimistic,
andfullofthesenseofpossibility". BarbaraSinatrawrote,"AbigpartofFrank’sthrillwas
thesenseofdangerthatheexuded,anunderlying,ever-presenttensiononly
---------------------------------------------
AMRgraphinformation: #nodes: 44,#edges81,#SSSP:30
Scorebypalm2: 100/100,Rankbypalm2: 1/100
RankbyDPR:2/100
7th: wherehissuiteandthoseofhisentouragewereonthe23rdfloor. Histour,hisfirstin
Australiain15yearsandbilledas"Ol’BlueEyesIsBack,"wasscheduledtoincludetwo
showsinMelbourne,followedbythreeinSydney. Inhisfirstshow,accordingtonewsreports
from1974,Sinatrareferredonstagetothemediaas"parasites"and"bums"andtowomen
specificallyas"thebroadsofthepress,thehookersofthepress,"thenadding,"Imightoffer
themabuckandahalf,I’mnotsure."ThecharacterofRodBlueinthe
---------------------------------------------
AMRgraphinformation: #nodes: 32,#edges61,#SSSP:32
ScorebyPaLM2: 50/100,Rankbypalm2: 9/100
RankbyDPR:11/100
178th: RLPO,BBCConcertOrchestra(for"FridayNightIsMusicNight"),LahtiSymphony
Orchestra,NorthernSinfonia,theMelbourneSymphonyOrchestra,theAdelaideSymphony
OrchestrafortheAdelaideCabaretFestivalandtheRTÉConcertOrchestra. Hismostpopular
showistheinteractive"SinatraJukebox"where,"insteadofanhourofsongsandanecdote,
halfwaythroughmembersoftheaudiencewereinvitedtofillinrequestforms". Reviewing
theshow,"CabaretScenes"said,"IcanthinkofnoothersingertobetterpayhomagetoOl’
BlueEyesonhis100thbirthday."In2014heperformedonBBCRadio2withtheBBC
---------------------------------------------
AMRgraphinformation: #nodes: 51,#edges57,#SSSP:20
Scorebypalm2: 20/100,Rankbypalm2: 27/100
RankbyDPR:14/100
9th: ashistributeto"TheGreatAmericanSongbook". ThealbumhasOleg’svocalsand
arrangementsbyabigbandleaderPatrickWilliams(alateperiodFrankSinatrarecording
associate) and sound engineering by Al Schmitt, whose 60-year career yielded 150 gold
and platinum albums, 20 Grammy awards and who also recorded Ol’ Blue Eyes. "Bring
MeSunshine"wasproducedatthelegendaryCapitolRecordsstudiosinHollywood,CA.
Songwriter,CharlesStrousequoted: ""TheGreatAmericanSongbook,towhichIamproud
tobeacontributor,isoneofourgreatestculturalexports,Olegisalivingexampleofwhatan
---------------------------------------------
AMRgraphinformation: #nodes: 52,#edges68,#SSSP:12
Scorebypalm2: 50/100,Rankbypalm2: 9/100
RankbyDPR:27/100
10th: firstplacewinsTheFoundingDirectorisBenFerris(2004+). SydneyFilmSchool
runstwocourses: TheDiplomaofScreen&MediaandTheAdvancedDiplomaofScreen
&Media. SomeoftheaccoladesaffordedtoSydneyFilmSchoolgraduatesfortheirwork
include: BestStudentDocumentaryFilmatAntennaFilmFestival: "Ol’BlueEyes",Matt
Cooney Finalist at Bondi Short Film Festival: "Letters Home", Neilesh Verma Industry
AdvisoryBoard(IAB)PitchCompetitionwinner: "LotusSonny",GarySofarelliOpening
Nightscreening;BestAustralianAnimation&BestAustralianComposeratWorldofWomen
WOWFilmFestival,2012: "CameraObscura",MartaMaia
---------------------------------------------
AMRgraphinformation: #nodes: 58,#edges69,#SSSP:35
Scorebypalm2: 0/100,Rankbypalm2: 30/100
RankbyDPR:39/100
Byanalyzingtheaboveresult,wenotethatdocuments(suchas1st,2nd,and4th)containingexact
wordsfromthequestion(i.e.,thesewordsare“Ol’BlueEyes"and“nickname"inourexample)are
prioritizedatthetopbymostrankers. However, ifadocumentincludeswordvariationsorlacks
sufficientkeywords,itposesachallengeforthebaselinererankertoidentifyitsrelevance,seethe9th
and10thdocuments. Toaddressthisissue,theAMRgraphofdocumentsisusedinourmethodto
comprehendmoreintricatesemantics. TheSSSPsfromthe‘question’nodeintheAMRgraphalso
playthecrucialroleinuncoveringtheunderlyingconnectionsbetweenthequestionandthewordsin
thedocuments.
Anotherchallengingscenarioforthebaselinererankerariseswhenseveralkeywordsorevengold
answersarepresentinthedocumentsbutareweaklyconnected,makingrecognitiondifficult. For
example,inthe7th,8th,and9thdocumentsthereareboth“Ol’BlueEyes"and“Sinatra"whichare
goldanswers,yetthesewordsarenotdirectlylinkedasthesentence: “Ol’BlueEyesisthenickname
of“Sinatra". Instead,theconnectionbetweenthesetwowordsisveryloose. Luckily,the7th,8th,and
9thdocumentsareconnectedtothe1stdocumentinthedocumentgraphduetocommonnodeslike
’Sinatra’and’OlBlueEyes.’ The1stdocumentstandsoutasmoreeasilyidentifiableasapositive
document,givenitsincorporationofallkeywordsfromthequestions. Thesewordsnotonlyhavea
strongconnectionbutalsocollectivelycontributetoacohesiveanswertothequestion. Leveraging
thisinformationandemployingamessage-passingmechanism,wecanenablethe7th,8th,and9th
18documenttoadeptlydiscernpotentialkeywords. Consequently,thisapproachenhancestheirranking,
basedontheinsightsderivedfromthewell-connectedandinformation-rich1stdocument.
D ExamplesofLLM-generateRelevantScore
SomeexamplesofLLM-generaterelevantscoreareillustratedinFig5.
Input: Input: Input:
To what extent is the following passage relevant to the To what extent is the following passage relevant to the To what extent is the following passage relevant to the
given question? Please provide a score on a scale of 0 to given question? Please provide a score on a scale of 0 to given question? Please provide a score on a scale of 0 to
100. 100. 100.
Question: who sings does he love me with reba Question: where do the great lakes meet the ocean Question: what is the smallest prime number that is
greater than 30
Text: Red Sandy Spikadress of Reba McEntire … during Text: nations maintain coast guard vessels in the Great
a duet performance of "Does He Love You" with Linda Lakes. During settlement… canals an all-inland water Text: Euclid number …the first three primes are 2, 3, 5;
Davis…won entertainer of the year route was provided between New York City their product is 30… celebrated proof of the infinitude.
Output: 75 Output: 40 Output: 20
Figure5: ExamplesofLLM-generaterelevantscore.
19