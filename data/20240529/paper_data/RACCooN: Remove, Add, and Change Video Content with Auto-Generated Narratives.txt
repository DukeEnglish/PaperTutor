RACCooN: Remove, Add, and Change Video Content
with Auto-Generated Narratives
JaehongYoon∗ ShoubinYu∗ MohitBansal
UniversityofNorthCarolinaatChapelHill
{jhyoon, shoubin, mbansal}@cs.unc.edu
https://raccoon-mllm-gen.github.io/
Input Video
(1) Video-to-Paragraph Video Description
The scene unfolds on a sunny beach with a clear blue sky
overhead. A man in casual attire is seen walking across the
sandy terrain, accompanied by a large white dog on a
leash. Meanwhile, a small black cat strolls by, adding to
the relaxed beach atmosphere. Buildings line the
background, suggesting an urban beachfront setting……
Output Videos Identified Objects and Descriptions
[White Dog] A large, fluffy white dog, likely of a breed
Remove such as a Samoyed or a white Golden Retriever, is seen
3
w gra ol ok min eg
d
a alo nn dg es xid he
ib
t ih tse am fa rn ie.
n
T dh lye dd eo mg ea ap np oe ra .r s well- ✍
[Man] A casually dressed man in a red shirt and blue
Change j te ha en os w in n ea r ob fl u the e s wh hir itt e a dn od g .w Hh ei t we a lb ke sa wc ih th s ah o rer lt as x, e dli k ge al iy t,
indicating a leisurely outing.
[Cat] A small black cat with sleek fur, making a solitary
walk across the beach. This cat adds a sense of calm
independence to the scene.  
RACCooN ……
Add
+ [Woman] A casual woman in a white beach dress
and with a straw hat, walking beside a man, likely
accompanying him and his white dog. She moves
with a relaxed stride, suggesting a leisurely outing
together.
(2) Paragraph-to-Video
Figure1: OverviewofRACCooN,aversatileanduser-friendlyvideo-to-paragraph-to-videoframe-
work,enablesuserstoremove,add,orchangevideocontentviaupdatingauto-generatednarratives.
Abstract
Recentvideogenerativemodelsprimarilyrelyoncarefullywrittentextpromptsfor
specifictasks,likeinpaintingorstyleediting. Theyrequirelabor-intensivetextual
descriptionsforinputvideos,hinderingtheirflexibilitytoadaptpersonal/rawvideos
touserspecifications. ThispaperproposesRACCooN,aversatileanduser-friendly
video-to-paragraph-to-videogenerativeframeworkthatsupportsmultiplevideo
editingcapabilitiessuchasremoval,addition,andmodification,throughaunified
pipeline. RACCooNconsistsoftwoprincipalstages: Video-to-Paragraph(V2P)
andParagraph-to-Video(P2V).IntheV2Pstage,weautomaticallydescribevideo
scenes in well-structured natural language, capturing both the holistic context
andfocusedobjectdetails. Subsequently,intheP2Vstage,userscanoptionally
refine these descriptions to guide the video diffusion model, enabling various
modifications to the input video, such as removing, changing subjects, and/or
adding new objects. The proposed approach stands out from other methods
throughseveralsignificantcontributions: (1)RACCooNsuggestsamulti-granular
spatiotemporal pooling strategy to generate well-structured video descriptions,
capturing both the broad context and object details without requiring complex
human annotations, simplifying precise video content editing based on text for
users. (2) Our video generative model incorporates auto-generated narratives
∗EqualContribution.
4202
yaM
82
]VC.sc[
1v60481.5042:viXraorinstructionstoenhancethequalityandaccuracyofthegeneratedcontent. It
supportstheadditionofvideoobjects,inpainting,andattributemodificationwithin
aunifiedframework,surpassingexistingvideoeditingandinpaintingbenchmarks.
Theproposedframeworkdemonstratesimpressiveversatilecapabilitiesinvideo-to-
paragraphgeneration(upto9.4%p↑absoluteimprovementinhumanevaluations
againstthebaseline),videocontentediting(relative49.7%↓inFVD),andcanbe
incorporatedintootherSoTAvideogenerativemodelsforfurtherenhancement.
1 Introduction
Recentadvancesinvideogenerativemodels[67,19,11,41,6,5,4,63],includingSora[43],have
demonstratedremarkablecapabilitiesincreatinghigh-qualityvideos. Simultaneously,videoediting
models[15,45,60,74,62,78]havegainedsignificantattention,thankstotheirpromisingapplications
thatallowuserstomodifyvideosaccordingtouser-writtentextualinstructions,effectivelyaltering
videocontentandattributes. Despitetheseadvancements,significantchallengesremainindeveloping
aversatileanduser-friendlyframeworkthatfacilitateseasyvideomodificationforpersonaluse. The
primarychallengesinclude: 1)thecomplexityoftrainingaunifiedframeworkencompassingmultiple
videoeditingskills(e.g., remove, add, orchangeanobject). Trainingasinglemodeltoperform
variouseditingskillsishighlychallenging,andrecentvideoeditingmethodsoftenfocusonspecific
tasks,suchasbackgroundinpainting[74,62],orattributeediting[15,45,23]. 2)thenecessityfor
well-structuredtextualpromptsthataccuratelydescribevideosandcanbeeditedtosupportdiverse
videoeditingskills. Thequalityofpromptscriticallyinfluencesthemodels’capabilitiesandthe
qualityoftheiroutputs. Generatingdetailedpromptsistime-consumingandcostly,andthequality
variesdependingontheexpertiseoftheannotators. AlthoughMultimodalLargeLanguageModels
(MLLMs)[36,42,68,73]havebeenexploredforautomaticallydescribingvideos,theyoftenoverlook
criticaldetailsincomplexscenes. Thisoversightcompromisesthedevelopmentofaseamlesspipeline,
hinderingbothuserconvenienceandtheeffectivenessofvideogenerativemodels.
Totackletheselimitations,asshowninFigure1,weintroduceRACCooN:Remove,Add,andChange
Video Content with Auto-generated Narratives, a novel video-to-paragraph-to-video (V2P2V)
generative framework that facilitates diverse video editing capabilities based on auto-generated
narratives. RACCooNallowsfortheseamlessremovalandmodificationofsubjectattributes,aswell
astheadditionofnewobjectstovideoswithoutrequiringdenselyannotatedvideopromptsor
extensiveuserplanning. Ourframeworkoperatesintwomainstages: video-to-paragraph(V2P)and
paragraph-to-video(P2V).IntheV2Pstage,weintroduceanewvideodescriptiveframeworkbuilt
onapre-trainedVideo-LLMbackbone(PG-Video-LLaVA[42]). WefindthatexistingVideo-LLMs
effectivelycaptureholisticvideofeatures,yetoftenoverlookdetailedcuesthatarecriticalforaccurate
videoediting,asusersmaybeinterestedinalteringthesemissingcontexts. Toaddressthis,wepropose
anovelmulti-granularvideoperceptionstrategythatleveragessuperpixels[32,25]tocapturediverse
andinformativelocalizedcontextsthroughoutavideo. Wefirstextractfine-grainedsuperpixelsusinga
lightweightpredictor[69]andthenapplyoverlappingk-meansclustering[9,61,26]tosegmentvisual
scenesintovariouslevelsofgranularity. Thesuggestedlocalizedspatiotemporalsegmentationassists
theLLM’scomprehensionofobjects,actions,andeventswithinthevideo,enablingittogenerate
fluentanddetailednaturallanguagedescriptions. Next,intheP2Vstage,tointegratemultipleediting
capabilitiesintoasinglemodel,wefine-tunedavideoinpaintingmodelthatcanpaintvideoobjects
accuratelywithdetailedtext,objectmasks,andconditionvideo. Then,byutilizinguser-modified
promptsfromgenerateddescriptionsintheV2Pstage,ourvideodiffusionmodelcanaccuratelypaint
correspondingvideoregions, ensuringthattextualupdatesfrompromptsarereflectedinvarious
editingtasks. Moreover,tobettersupportourmodeltraining,wehavecollectedtheVideoParagraph
with Localized Mask (VPLM) dataset—a collection of over 7.2K high-quality video-paragraph
descriptionsand5.5kdetailedobjectdescriptionswithcorrespondingmasks,annotatedfromthe
publiclyavailableROVI[62]datasetusingGPT-4V[1].
WeemphasizethatRACCooNenhancesthequalityandversatilityofvideoeditingbyleveraging
detailed, automatically generated textual prompts that minimize ambiguity and refine the scope
of generation. We validate the extensive capabilities of the RACCooN framework in both V2P
generation,text-basedvideocontentediting,andvideogenerationonActivityNet[30],YouCook2[80],
UCF101 [50], DAVIS [44], and our proposed VPLM datasets. On the V2P side, RACCooN
outperformsseveralstrongvideocaptioningbaselines[31,42,36],particularlyimprovingbyaverage
2+9.1%p on VPLM and up to +9.4%p on YouCook2 compared to PG-VL [42], based on both
automaticmetricsandhumanevaluation. OntheP2Vside,RACCooNsurpassespreviousstrong
videoediting/inpaintingbaselines[15,45,60,74,62]overthreesubtasksofvideocontentediting
(remove, add, and change video objects) over 9 metrics. We also demonstrate that the proposed
RACCooN framework can enhance SoTA video generative models by leveraging detailed auto-
generatedtextualprompts. Wefurtherconductextensiveablationandvisualizationstovalidatethe
improvementquantitativelyandqualitatively. Ourcontributionsareasfollows:
1. WeintroduceRACCooN,aversatileanduser-friendlyvideo-to-paragraph-to-videogenerative
frameworkthatdescribestheinputvideowithdetailednaturallanguage,whichinturnenables
userstocreateoreditvideoswithouttheneedforanyhuman-annotatedinputprompts.
2. Wepresentamulti-granularpoolingstrategytocapturelocalvideocontexts,enhancingvideo
comprehensionbygeneratingfluentanddetaileddescriptions. Thisenablesuserstocreatenew
videosthatretainthevisualcharacteristicsoftheinputandfocusonspecificcontextediting.
3. WepresenttheVPLMdataset,whichcontains7.2Khigh-qualitydetailedvideoparagraphs,and
5.5Kobject-leveldetailedcaption-maskpairs,facilitatingtheaccurateV2PandP2V.
4. WeshowtheeffectivenessandefficiencyofRACCooNframeworkviaextensivequantitative
andqualitativeanalyses,comparingwithstrongbaselinesinvideocaptioningandediting.
2 RelatedWork
Video-to-ParagraphGeneration. Therecenttrendinvideo-languagetasksfocusesongenerat-
ing comprehensive textual descriptions for long and complex video content [49, 30, 56, 52, 64].
Vid2Seq[68]introducesanoveldenseeventcaptioningapproachfornarratedvideos,withtimetokens
andeventboundaries. Video-LLaVAvariants[33,42]presentalargemultimodalmodelintegrating
text,video,andaudioinputsforgenerativeandquestion-answeringtasks. Similarly,LLaVA-Next[76]
improveszero-shotvideounderstandingbytransferringmulti-imageknowledgethroughconcatenated
visualtokens. Whilethesemethodsareeffectiveinvideodescription,theyoftenmisskeycontextual
details[75,31]. OurRACCooNcapturesbothholisticandlocalizeddetailsbyleveraginglocalized
spatiotemporalinformation,enhancingvideoeditingandgenerationcapabilities.
Prompt-to-VideoEditing. Videoediting[5,38,10,29,59,77]involvesenhancing,modifying,or
manipulatingvideocontentfordesiredeffects. VideoComposer[60]offersamulti-sourcecontrollable
videogenerativeframework. TokenFlow[15]adaptstext-to-imagediffusionwithflowmatchingfor
consistenttext-drivenvideoediting. LGVI[62]integratesanMLLMforcomplexlanguage-based
videoinpainting. Thesemethodsoftenfocusonspecifictasksandmayinadvertentlyalterunrelated
regionsduetolimitedcontextualinformation. OurV2P2Vframeworkovercomestheselimitationsby
usingauto-generated,detaileddescriptionstointegratekeycontextsintodiverseeditingtasks.
3 Remove,Add,andChangeVideoContentwithAuto-GeneratedNarratives
Conditionalvideogenerationandeditingmodels,designedtoaccuratelyreflectuserrequirements
describedinthetext,oftenfacechallengeswithcomplexscenesduetolimited,high-leveldescriptions
andalackofdetailedvideounderstanding. DespiteimprovementsfromrecentadvancesinMLLMs,
these models still struggle to capture complex spatial-temporal dynamics, often omitting crucial
objectsanddetails. Trainingatext-to-videomodelwithsuchvaguepromptscompromisesoutput
specificity, and leads the model to generate average, arbitrary content that fails to capture user
instructions’nuances. Toaddresstheseissues,weintroduceRACCooN,auser-friendly,two-stage
video-to-paragraph-to-videoeditingframework. Initially,RACCooNgeneratesdetailed,structured
paragraphs from videos, capturing holistic content and key local objects through multi-granular
spatiotemporalinformation. Thesedetaileddescriptionsarethenusedforconditionalvideogeneration
andediting,enablinguserstoadd,remove,orchangevideoobjectsdirectlybyinteractingwiththe
generateddescriptions,thusenhancingthespecificityandrelevanceoftheoutput.
3.1 V2P:Auto-DescriptiveFrameworkwithMulti-GranularVideoPerception
MultimodalLLMforVideoParagraphGeneration. IntheV2Pstage,theRACCooNframework
generateswell-structured,detaileddescriptionsforbothholisticvideosandlocalobjects. Itemploys
3RACCooN: Video-to-Paragraph-to-Video Generative Framework
Stage 1: Video-to-Paragraph (V2P)
Spatial The video shows a woman walking in a
Pooling LLM park while talking to herself. She is
wearing a black dress and carrying a
Video yellow bag. …
Encoder LoRA
(optional) Layouts of <Obj> to be added:
Temporal LM {Frame 1: [0.2, 0.0, 0.5, 0.7], …
Projection Pooling Head
when the user doesn’t
Input provide layouts
Prompt: Describe the Video
Video S Pu rp ee dr ip ctix oe rl PoM oG liS ng + < O(o bp j>ti o ton a bl e) P adre dd ei dct layout of User edits
User edit:“add <Obj>”
Grounding Module l {a Fy ro au mt es : 1: [0.1, 0.1, 0.4, 0.25],
Inflated U-Net …
User edit: “remove <Obj>”
Video Noise
C
Decoder Generation User edit: “change the
<Attr> of <Obj>”
Stage 2: Paragraph-to-Video (P2V)
Figure2: IllustrationofRACCooNframework. RACCooNgeneratesvideodescriptionswiththe
threedistinctpooledvisualtokens,includingMulti-GranularSpatiotemporal(MGS)Pooling. Next,
userscaneditthegenerateddescriptionsbyadding,removing,ormodifyingwordstocreatenew
videos. Notethatforaddingobjecttasks,ifusersdonotprovidelayoutinformationfortheobjects
theywanttoadd,RACCooNcanpredictthetargetlayoutineachframe.
amultimodalLLMwiththreemaincomponents: avisualencoderE,amultimodalprojector,and
anLLM.Givenaninputvideox∈RF×C×H×W,whereF,C,H,andW representthenumberof
frames,channels,height,andwidth,respectively,weextractvideofeaturesusingthevisualencoder:
e=E(x)∈Rt×h×w×d. Here,t,h,w,andddenotetheencodedtemporaldimension,theheightand
widthofthetokens,andthefeaturedimension. Tounderstandcomplexvideoswithmultiplescenes,
weusethreepoolingstrategies: spatialpooling,temporalpooling,andmulti-granularspatiotemporal
pooling. Spatialpoolinges =Poolings(e)∈Rt×daggregatestokenswithinthesameframe,while
temporalpoolinget =Poolingt(e)∈R(h·w)×daveragesfeaturesacrossthetemporaldimensionfor
thesameregion. DespitethesestrategieshelpingtheLLMgraspthevideoholisticallyinspaceor
time, theyoftenoverlookcapturingkeyobjectsoractionslocalizedthroughoutthevideostream,
especiallyinuntrimmed,dynamic,multi-scenevideos.
Multi-GranularSpatiotemporalPooling. Toaddressthisissue,weintroduceanovelsuperpixel-
basedspatiotemporalpoolingstrategy,coinedmulti-granularspatiotemporalpooling(MGSpooling).
As illustrated in Figure 2 left top, this strategy is designed to capture localized information via
superpixels across spatial and temporal dimensions. Superpixels [32, 16, 69, 25] are small and
coherentclustersofpixelsthatsharesimilarcharacteristics,suchascolorortexture. Theseclusters
provideanefficientrepresentationofvisualscenesandareresilienttoframenoisesincetheyaverage
outthepixelvalueswithineachcluster,effectivelysmoothingoutvariationsinducedbynoise. Weuse
alightweightsuperpixelpredictorσ(·)[69]togeneratesuperpixelsacrossvideoframes,capturingthe
granularvisualityofeachlocalarea. However,duetotheirlimitedcoveragearea,thesefine-grained
visualfeaturesoftenfailtocaptureattribute-levelsemantics,suchasobjectsandactions[75,31].
Motivated by such importance of varying the compositions of multiple superpixels for different
contextsinvideounderstanding,weproposetheuseofoverlappingk-meansclustering[9,61]for
theobtainedvideosuperpixels,whichimprovesthegranularityfromfinetocoarse. Thisapproach
allowstheLLMtogatherinformativecuesaboutvariousobjectsandactions. Wefirstobtainthepixel
featuresandthesuperpixelindexvectorforthevideopixels: S,g = σ(x,ginit),whereginit isthe
inputsuperpixelindices,initializedbyaregion-basedgrid. Giventheaveragedpixelfeaturesofeach
superpixel,S¯ ∈R|g|×dp,whered sdenotesthepixelfeaturesize,wegeneratetheMGStokensel:
m=OKM(cid:0) S¯,k,v(cid:1) ∈{0,1}k×F×H×W,
(1)
el =AvgPool(m)⊗e∈Rk×d,
whereOKMrepresentstheoverlappingk-meansalgorithmwithkcentroidsandoverlapscalevfor
eachcluster. mdenotesthesetofbinarymasksforsuperpixels. ⊗denotestensormultiplication.
4WedescribethedetailedMGSprocessandablationofpoolingstrategiesintheAppendix. Next,
we concatenate the pooled video tokens and map them into the text embedding space using the
multimodallinearprojector. Combinedwiththeembeddingoftheencodedtexttokenepfromthe
textualprompt,theLLMgeneratesawell-structuredanddetaileddescriptionaofthevideo:
e=concat[es;el;et]·W⊤,
(cid:98) (2)
a=LLM(concat[ep;e]),
(cid:98)
whereW ∈Rd×d′ istheweightmatrixforlinearprojectionintothetextembeddingdimensiond′.
Wehighlightthatourvideodescriptionframeworkservesasanintegrated,user-interactivetoolfor
video-to-paragraphgenerationandvideocontentediting. (Figure2topright).
3.2 P2V:User-InteractiveVideoContentEditingwithAutomaticallyGeneratedDescriptions
Withthewell-structured,detailed,andobject-centricvideodescriptiongeneratedfromtheVideo-to-
Paragraphstage,userscan‘read’thevideodetailsandinteractivelymodifythecontentbyalteringthe
model-generateddescription. Thisapproachshiftsusers’focusfromlabor-intensivevideoobservation
tocontentediting. Wecategorizegeneralvideocontenteditingintothreeimportantsubtasks: (1)
VideoObjectAdding: addextraobjectstoavideo. (2)VideoObjectRemoving: deletetarget
objectsandre-generatetheobjectregionasthebackground. (3)VideoObjectChanging: change
objects’attributes(e.g.,color,textural,material). Manypreviousworkshavemadegreatprogress
invideoediting[62,15,45,78,13]butusuallyfocusononeofthesesubtasks. Inthispaper,we
proposeaunifiedgenerativemodelforvideocontenteditingthatintegratesallthosecrucialsubtasks.
Specifically,weformulatethesesubtasksastext-basedvideopaintingtasksandleverageasingle
videodiffusionmodelforadding,removing,andchangingvideoobjectsintheformofinpainting.
AsshowninFigure2bottom,ourvideodiffusionmodelprocessesinputvideox∈RF×C×H×W with
apredictedbinarymaskm′ ∈RF×1×H×W targetingspecificregionsformodification. Following
imageinpaintingtechniques[65,48],weapplythemask2tothevideotodesignatetheeditingregion.
The masked video is then encoded using a Variational Autoencoder (VAE [27]) to serve as the
generationcondition. Inthiscase,themodelcanbeinformedonwhichvideoregionshouldbeedited
thusenablinglocalizedvideoediting. Drivenbythedetaileddescription,thevideodiffusionmodel
canadd,remove,orchangevideocontentthatreflectsthetextprompts.
3.3 VPLMDatasetCollectionandRACCooNPipelineTraining
DatasetCollection. Weutilizevideodatasets[40,14]frompreviousvideoinpaintingwork[62].
Eachrawvideoisaccompaniedbymultipleinpaintedversionswithspecificobjectsremovedand
includesbinarymasksoftheseobjects. Althoughwell-annotatedwithobjectmasksandinpainted
backgrounds,thesedatasetslackdetaileddescriptionsofholisticvideoandspecificlocalobjects,
hinderingRACCooN’strainingforproducingwell-structuredcaptionsforvideoediting. Toaddress
this, we use GPT-4V [1] to annotate detailed video descriptions. We first re-arrange uniformly
sampled video frames into a grid-image [12] and add visual prompts by numbering each frame.
WethenaskGPT-4Vtogeneratedetailedcaptionsforboththeentirevideoandkeyobjects, ina
well-structuredformat. Next,wetrainV2PandP2Vstagesinourframeworkseparately(Figure2). In
theend,RACCooNcanautomaticallygeneratedetailed,well-structureddescriptionsforrawvideos
andadaptthesedescriptionsbasedonuserupdatesforvariousvideocontenteditingtasks.
MLLM Instructional Fine-tuning. To enable the MLLM to output detailed video descriptions
for content editing, we construct an instructional fine-tuning dataset based on VPLM with two
video-instruction[36]designs: (1)Forobjecteditingandremoval,theMLLMgeneratesstructured
videocaptionsidentifyingkeyobjectsintheoriginalvideox,usingannotateddescriptionsasthe
learningobjective. Thisallowsuserstoeditvideosdirectlyfromthesedescriptionswithoutexhaustive
analysis. (2) For object insertion, the MLLM provides not only detailed descriptions but also
frame-wiseplacementsuggestionsfornewobjects,enhancingitsutilityinvideoeditingbyavoiding
manualtrajectoryoutlining. Fortraining,weconvertvideoobjectsegmentationmasksintobounding
boxesbyselectingmaximalandminimalcoordinatesandfollowtheboxplanningstrategyusing
LLMs[34]. WeinputboxcoordinatesasasequenceofnumbersandtrainRACCooNframework
topredicttheselayoutsgiveninpaintedvideosx. Weperformparameter-efficientfine-tuningwith
(cid:98)
2Weuseimagegrounding[37]andvideotrackingmodels[8]astheoff-the-shelfmaskpredictorininference.
5Low-rankAdapters(LoRA[21])3 onthesemixeddatasetsusingstandardCEloss. Wefreezethe
visualencoderandLLMbackbone,updatingtheprojector,LoRAmodules,andLLMhead.
VideoDiffusionModelFine-tuning. Ourvideodiffusionmodelbuildsonthepriorimageinpainting
model [48], enhanced with temporal attention layers to capture video dynamics. The model is
designedtogeneratevideothatalignswithinputprompts,focusingonobject-centricvideocontent
editing. Tosupportthis,wedevelopatrainingdatasetofmask-object-descriptiontriples. Weuse
GPT-4toproducesingle-objectdescriptionsfromlong,detailedvideonarratives,framingthistaskas
amulti-choiceQAproblem. Next,forthethreevideoeditingsubtasks,wedesignspecificinput-output
combinations: (1)VideoObjectAddition: Inputs: inpaintedvideox,objectboundingboxesfrom
(cid:98)
segmentation masks m, and detailed object description p. Output: original video x. (2) Video
ObjectRemoval: Inputs: originalvideox,objectsegmentationmasksm,andafixedbackground
prompt. Output: inpaintedvideox. (3)VideoObjectChange: Inputs: originalvideox,object
(cid:98)
segmentationmasksm,andobjectdescriptionp. Output: originalvideox. Themodelisfine-tuned
inaparameter-efficientmannerfollowingthepriorwork[63],updatingonlythetemporallayersand
thequeryprojectionswithintheself-attentionandcross-attentionmodules. WeemploytheMSEloss
betweengeneratedandrandomnoise. SeeAppendixformoredetailsonthedatasetandtraining.
4 ExperimentalResults
4.1 ExperimentSetup
Tasks&Datasets: WeevaluateourRACCooNframeworkondiversevideodatasetsacrosstasks,
includingvideocaptioning(YouCook2[80],VPLM),text-basedvideocontentediting(DAVIS[44],
VPLM),andconditionalvideogeneration(ActivityNet[30],YouCook2[80],UCF101[50]).
Metrics: Foreachtask,weevaluateourapproachwithvariousmetrics. (1)VideoCaption: following
previousworks[68,81],weconductacomprehensivehumanevaluationandadoptgeneralmetrics
for our long video descriptions, including SPICE [2], BLEU-4 [55], and CIDEr [55]. (2) Video
Object Layout Planning: following the prior work [34], we evaluate the framework for object
layout planning by bounding box IoU, FVD [54], and CLIP-score [46]. (3) Text-based Video
ContentEditing: followingpriorworks[15,5,70],weevaluatetheframeworkforvideoediting
byCLIP-Text,CLIP-Frame,Qedit[70],andSSIM[20]. (4)ConditionalVideoGeneration: we
measureperformanceonFVD[54],CLIP-Score[46],andSSIM[20].
Implementation Details: In V2P generation, we set k = [20,25] and v = [5,6] for superpixel
clustering. WeuseCLIP-L/14@336[46]astheimageencoderandVicuna-1.5[79]astheLLM.
OurP2VmodelisstartedfromStableDiffusion-2.0-Inpainting[48]. WesplittheVPLMdatasets
intotrainandtestsets,withthetestsetcontaining50uniquevideo-paragraphpairs(forV2P)and
180 mask-object-description triples (for P2V).We manually annotate theediting prompts for the
object-changingsubtask. WequantitativelycompareRACCooNandotherbaselinesontheVLPM
testset. Tofocusongenerationresultsratherthangroundingability,weapplythesamegroundtruth
masksandcaptionstoallmethodsforP2Vevaluation. SeetheAppendixformoredetailsondatasets,
metrics,implementations,additionalhyperparameters&V2Pexperiments,andqualitativeanalysis.
4.2 Video-to-ParagraphGeneration
Video-ParagraphAlignment. WeconductedaquantitativeevaluationofourproposedRACCooN
framework’svideo-to-paragraphgenerationcapabilities,comparingitagainststrongbaselineswitha
focusonobject-centriccaptioningandobjectlayoutplanning. Theresults,summarizedinTable1,
showthatopen-sourcevideo-LLMs(e.g.,PG-VL,Video-Chat)whichhavesmallerLLMs(<13B
parameters),strugglewithobject-centriccaptioningandusuallyfailtogeneratelayoutplanning. This
isprimarilyduetotheirlackofinstructionalfine-tuningandinsufficientvideodetailmodelingwithout
multi-granularpooling. Incontrast,ourRACCooNframeworkdemonstratessuperiorperformancein
bothobject-centriccaptioningandcomplexobjectlayoutplanning,benefitingfromtheinstructional
tuning on our VPLM dataset. Additionally, our method achieves competitive performance with
proprietaryMLLMs(e.g.,Gemini1.5Pro,GPT-4o)inkeyobjectcaptioningandlayoutplanning,
demonstratingitssuperiorinstructionfollowingandgenerationquality.
3WeemployLoRAforqueryandvalueforeachself-attention.
6Table1: ResultsofSingleObjectPrediction Table 2: Results of Human Evaluation on
on VPLM test set. Metrics are abbreviated:S: YouCook2. We measure the quality of the de-
SPICE,B:BLEU-4,C:CIDEr. scription through four metrics: Logic Fluency
(Logic),LanguageFluency(Lang.),VideoSum-
Methods S B C IoU FVD CLIP mary(Summ.),andVideoDetails(Details). We
LLaVA[36]
1o 7p .4en-s 2o 7u .r 5ceM 18L .L 5Ms
- - -
reportthenormalizedscores∈[0,100].
Video-Chat[31] 18.2 25.3 19.1 - - -
PG-VL[42] 18.2 27.4 14.6 - - -
Methods Logic Lang. Summ. Details Avg.
Gemini1.5Pro[51] 19p .r 2opri 2e 3ta .5ryM 1L 1.L 0Ms 0.115 371.63 0.978 GroundTruth 66.7 42.2 41.7 72.2 55.7
GPT-4o[17] 20.6 28.0 37.4 0.179 447.67 0.977 PG-VL[42] 77.2 81.1 69.4 62.8 72.6
RACCooN 80.6 85.0 72.2 72.2 77.5
RACCooN 23.1 31.0 33.5 0.218 432.42 0.983
Input Video
RACCooN Generated Paragraphs
The video shows a woman walking down a busy city street at night. She is wearing a red dress and carrying a handbag. She stops to look at a
building and then walks away. The woman is also seen walking down a rain-soaked street and looking at a building. She then walks away again.
The video also shows a man walking down the street and looking at a building. The woman is also seen walking down a street and looking at a
building. The video ends with a woman walking down a street and looking at a building. Overall, the video captures the busy and bustling
atmosphere of a city at night. The woman's red dress and handbag stand out as she walks down the street, and her attention to the buildings she
looks at adds depth to the scene. The man walking down the street also adds to the overall sense of a busy urban environment.
Object Catalog:
1. Woman: A stylish woman wearing a black leather jacket, a long red dress, and black boots. She carries a black purse and has sunglasses on.
2. Traffic lights: There are traffic lights visible in the background, indicating that the street is a busy intersection.
3. Building: A large, multi-story structure with a prominent presence in the background. It might be a commercial or residential building.
4. Handbag: A small, handheld accessory carried by the woman. It could be used for holding personal belongings or as a fashionable accessory.
Figure3: QualitativeV2PexampleofourRACCooNonSoravideo.
HumanEvaluation&QualitativeExamples. Weconductedahumanevaluationtocompareour
auto-generatedcaptionswiththosefromastrongbaselineandhumanannotationsontenrandomly
selected YouCook2 videos (each three to five minutes long with multiple scenes and complex
viewpoints). FiveevaluatorsratedthesebasedonLogicFluency,LanguageFluency,VideoSummary,
andVideoDetails(detailsinAppendix). Theaveragescoresforeachcriterionandtheiroverallmean
areillustratedinTable2. OurmethodsignificantlysurpassedboththePG-VL-generatedandground
truthcaptionsinallmetrics,showinga4.9%pand21.8%pabsoluteimprovementrespectively,and
matchedthegroundtruthincapturingVideoDetailswitha9.4%penhancementoverthebaseline,
highlightingRACCooN’ssuperiorcapabilityincapturingvideodetails. Weadditionallyvisualize
descriptionsgeneratedbyourRACCooN.Weuseawell-knowngeneratedvideofromtheSora[43]
generateddemoexample. AsshowninFigure3,itdemonstratesourmodel’srobustcapabilityto
auto-describecomplexvideocontentwithouthumantextualinput.
4.3 VideoContentEditingwithRACCooN
QuantitativeEvaluation. AsshowninTable3,wequantitativelycomparethevideoeditingability
ofRACCooNwithstrongvideoeditingmodelsbasedoninpaintingorDDIM-inversion[18]across
threeobject-centricvideocontenteditingsubtasks: objectchanging,removal,andadding. Ingeneral,
RACCooNoutperformsallbaselinesacross9metrics. Forobjectchanging,RACCooNoutperforms
the best-performing baseline by 0.8% on CLIP-T, indicating better video-text alignment while
maintainingtemporalconsistency,asdemonstratedbycomparableCLIP-FandQeditscores. Note
thatLGVIisnotdesignedtoaltervideoattributesandtendstopreservevideocontentwithmarginal
change(i.e.,identicalinputandoutputvideos),resultinginimprovedCLIP-Fscores. Intheobject
removaltask,RACCooNshowssignificantimprovementsoverstrongbaselines(relatively+57.8%
FVD,+2.5%SSIM,+9.6%PSNR).Suchimprovementsaremaintainedintheadditiontask(relatively
7Table3: ResultsofVideoContentEditingonthreesub-tasksonVPLMtest. Wegrayoutmodels
thatconducttheDDIMinversionprocessandhaveadifferentfocusonourinpainting-basedmodel.
ChangeObject RemoveObject AddObject
Model
CLIP-T↑ CLIP-F↑ Qedit↑ FVD↓ SSIM↑ PSNR↑ FVD↓ SSIM↑ PSNR↑
Inversion-basedModels
FateZero[45] 25.18 94.47 1.01 1037.05 47.35 15.16 1474.80 47.65 15.45
TokenFlow[15] 29.25 96.23 1.31 1317.29 47.06 15.83 1373.20 49.95 15.95
Inpainting-BasedModels
InpaintAnything[74] 24.86 92.01 1.01 383.81 82.33 27.69 712.59 77.75 22.41
LGVI[62] 23.82 95.33 1.04 915.24 56.16 19.14 1445.43 47.93 16.09
VideoComposer[60] 27.61 94.18 1.25 827.04 47.34 17.55 1151.90 48.01 15.76
RACCooN 27.85 94.78 1.15 162.03 84.38 30.34 415.82 77.81 23.38
Remove Object Change Object (Type)
1
[ w p c t sC h tlae il en emi am at ir b r dsib in , in e lhg ygr a a ma nsni dn h odb s vo r G eei a g ar s une h pdge t wc r n lg ai f] i p r: r m e de eA b te sthi .n n es b g tk t a ori ol c ol p h ke c f, rad ik n grg r dn or ao e u a sc nsy h k s d t o .c h l c dl eil smi T ym b h ab ui e nn se i deg rr [ s g a T aW h l mr hoaa i inh n s dr ga dk tl s ie hts p h se er aS ee rd msh n kae a a rd n rir k so ick n gm e] et: r.i h an lIA ieya ft t e bm fi s .n o oha g d c bos ayws at l i ch c av ke pa ne rs od , g f e i s n opr sp t ua o o w nm wt a dh eet ni re dtw fd e ui mt lw s h f oph i voa ni ettl se sss . [ l b e c hiB hng aur th pailo h s pt dw eu i nb sn eiia r na sD o n s stw d to h a ing ec n ] a a d d fll[ lo e oR eyg rr n a et ec a r s rsc a t g o e ,c yo iac dt .n ro i ] s so: l. pn e lA a a I d y wm t s ins ie t g td h h ai t u e aa sm i els - p ns t l sai wuz eyr ae fd ogd uy fs, l [ b d c b f qG al uil eia s c ii m a ta tc em ei bkn ,n t s ic ana lt ggn ao, iP d r lfda eg aw n e en i ftd n h d ossa i r b t e] lee b i a tc n a c[ sp o cB l k aa a l sr onn t io s z. c ed w u ei yIa n rn .e t g e b i .r sB po o I ae sw n t ta h n cr o h t h] wb h a: e e n se sa A ,p r a l w a awl o ry na i o of dr t d uug h nele l ida ny s,
2
Add Object Change Object (Color)
+ d l e t se h tx o g aeg h[ s nB i l cbl awa a ei wti nc .t s ndk h h ws iD d g in tio hos hg ut e] ii t: n tn . sec t I rA t gt o yt nm a w gane e us d a e imi r tu os am s u r p t- ak r s i iini nncz g toe as sld l a p ao r lcb n a r l yaa o fi nc st udk ss l + s b b s a hp n el au eo c rr[ er a k eW
d
t k ld a oi eo in xe gn rm g n .es a i da ma sn u] p di: b j to eeu an matA bynt elt o s e aa.nc n i flea H o, os d rreu a ra n wp w l dil abn hy llk k io s l i ehnd ns d er gh se e .i t s wr S rhs t e h oe a a e ld ia lrr in nds d g i iw s s w o pl wt him lg ai iie tha t yd hen st [ b s d f p oW t l fuil iu nr tp pe no i t-p ri m ifne n ela gd od g
p
n pe adb a] sn rr: l i iu l em na ee sD n ga s d sr a f.s h e n oiShs s d ro ohs er anwe et n d h whs a g i e act ain lc re ga
k
e en .dTa s dd o- s c s go ina h ,ra s i i str zu hht ea h o esl o a w r o awd iiu ncz ia tt gto r hf i n k o si tt n p ira g ie nl oo nl d ky ff s [ p s a a tB oh c gla a t io iy lis nr ive tk t te yr s ee l y rt as c b a np e a deo pl b n rl t fglt u oai oP en cg rl g a u i rn s sy ega h e b a ir r s ow it n uh hi nn i ea dt pn e l p tW d a hoy sh s e.h w i i ti bt h Hr ie ai ot t e] s ne: ksa e enA s h td xh b i h mo am d ir b ls laa t e .irl ts le k fs,
21
Figure4: QualitativeComparisonbetweenRACCooNandotherbaselines. Baselinenamesare
abbreviated: VC:VideoComposer[60],I-A:InpaintingAnything[74],TF:TokenFlow[15]. We
underlinedvisualdetailsinourcaption. Bestviewedincolor. MorevisualizationsareinAppendix.
+41.6%FVD,+4.3%PSNR).Meanwhile,someDDIM-basedmodels(e.g.,TokenFlow[15])work
wellforspecifictasks(changeobjects), butdonothandleothertypesofediting. Incontrast, our
methodisanall-rounderplayerthatenablesdiversevideocontenteditingskills.
Visualization. InFigure4,wecomparevideosgeneratedbyRACCooNwithseveralSoTAbaselines
acrossthreevideocontenteditingtasks. Forobjectremoval,RACCooNdemonstratessuperiorresults,
naturallyandsmoothlyinpaintingthebackground,whereasVideoComposergeneratesunexpected
contentandLGVIfailstoaccuratelyremoveobjectsacrossframes. Forobjectaddition,comparedto
Inpainting-AnythingandVideoComposer,whichoftenmissobjectsorproducedistortedgenerations,
RACCooN generates objects with more fluent and natural motion, accurately reflecting caption
details (e.g., the collar of the dog, the pink shirt, and blue jeans for the woman). For changing
objects,ourmethodoutperformsinpainting-basedVideoComposerandinversion-basedTokenFlow.
8
tupnI
CV
IVGL
sruO
tupnI
CV
A-I
sruO
tupnI
CV
FT
sruO
tupnI
CV
FT
sruOTable5: ResultsofInversion-basedVideoEditingonDAVISVideo. Ourgeneratedparagraph
canbeintegratedwithdifferentSoTAinversion-basedvideoeditingmodels(e.g. TokenFlow[15]or
Fate-Zero[45]). attr. andins. indicateattribute-andinstance-levelediting.
CLIP-Text CLIP-Frame SSIM
Model
attr. ins. all attr. ins. all attr. ins. all
FateZero[45] 28.9 27.2 28.1 95.6 95.1 95.3 71.5 70.8 71.2
FateZero[45]+RACCooN 31.7 30.7 31.2 95.5 95.1 95.3 72.1 72.3 72.2
TokenFlow[15] 31.4 29.8 30.6 94.6 94.1 94.3 57.0 56.0 56.5
TokenFlow[15]+RACCooN 32.6 31.6 32.1 94.7 94.3 94.5 58.0 57.3 57.6
Table6: ResultsofConditionalVideoGenerationonthreedatasets. RACCooNframeworkcanbe
integratedwithdifferentvideogenerationmodels(e.g. VideoCrafter[7]orDynamiCrafter[66]).
ActivityNet YouCook2 UCF101
Model
FVD↓ CLIP↑ SSIM↑ FVD↓ CLIP↑ SSIM↑ FVD↓ CLIP↑ SSIM↑
VideoCrafter[7] 3743.62 9.58 22.26 4731.22 9.89 20.58 3556.06 9.81 18.32
VideoCrafter[7]+RACCooN 2357.41 10.53 24.02 3046.82 9.47 23.89 2208.90 9.60 22.32
DynamiCrafter[66] 1632.30 10.65 32.46 2059.93 11.95 37.22 1588.57 11.98 38.81
DynamiCrafter[66]+RACCooN 1536.63 10.69 32.86 1904.08 10.03 38.78 1573.27 9.76 39.83
RACCooNaccuratelyre-paintsobjectstoachieveobjecteditingforcolor(white→blue)andtype
(dog→RACCooN),whileothersstruggletomeetrequirements.
AblationStudies. AsshowninTable4,wefurther Table4: Ablationonvideoobjectchanging,
validatetheeffectivenessofcomponentsbyreplacing removing,andaddingwithdifferentinputs.
detaileddescriptionswithshortcaptions,andoracle
masks/planning boxes with model-generated ones.
Settings FVD↓ SSIM↑ PSNR↑
In adding objects, our detailed object descriptions
addobject
canbenefitgenerationbyprovidingaccuratedetails,
RACCooN 415.80 77.81 23.38
leading to improved quantitative results (relatively
w/odetailcaption 476.01 76.80 23.14
+14.4% FVD). We further replace GT boxes with w/ooracleplanning 969.95 76.65 21.21
boxespredictedbyRACCooN,andstillshowsuperior
performanceoverotherbaselinemethodswithoracle removeobject
boxes in Table 3. It demonstrates that our V2P RACCooN 162.03 84.38 30.34
w/ooraclemask 398.01 81.60 27.15
stagecanthusautomaticallygenerateplanningfroma
givenvideotoeliminateusers’labor. Next,inobject Setting CLIP-TCLIP-F Qedit
removalandchanging,wereplacetheoraclemasks
changeobject
withgrounding[37]andtracking[8]toolsgenerated RACCooN 27.85 94.78 1.15
mask, it shows marginally decrement for changing w/ooraclemask 27.23 94.33 1.14
objects,andourframeworkstillshowsstrongresults
overotherbaselinesinTable3withoraclemasks. It
suggeststhatourRACCooNframeworkiseffectiveandrobusttohandlediverseeditingskillsina
non-orcalesetting(PleaseseetheAppendix).
4.4 EnhancingInversion-basedVideoEditing&GenerationwithRACCooNParagraphs
Wefurthervalidatethatdetailedparagraphscanbenefitvideogenerativetasks(e.g.,inversion-based
videoeditingandvideogeneration). OurRACCooNcaptionscanbeintegratedwithoff-shelfSoTA
videoeditingandgenerationmodelstoenhancethem. SeeAppendixformoredetails.
Inversion-basedVideoEditing. OurRACCooNframeworkcansignificantlyenhancevideoediting
tasks. WeintegrateditintotwoSoTAmethods,TokenFlow[15]andFateZero[45],andcompared
theirperformancewithdifferenttextinputs. Thebaselineusedhuman-writtenshortcaptions,while
baseline+RACCooNusedgenerateddetailedcaptions. AsshowninTable5,integratingRACCooN
significantlyimprovedperformance. Theseresultsindicatethatdetailedcaptionsenhancetext-to-video
alignmentandlocalizedediting,whilepreservingtemporalconsistencyandquality.
9ConditionalVideoGeneration. WeintegratedourmethodwithSoTAvideogenerationmethods
(VideoCrafter[7](Text-to-Video)andDynamiCrafter[66](Image-Text-to-Video)). AslistedinTable6,
VideoCrafter’sgeneratedvideosoftenfailedtoalignwiththeGTcaptions,especiallyincomplex
videos. Incontrast,ourframeworkimprovedvideoqualityandalignmentwithauto-generateddetailed
paragraphs, achieving substantial improvements. Additionally, RACCooN aided DynamiCrafter,
whichuseskeyframesretrievedbycaptionsasanextracondition. Wenotedconsistentimprovements
inFVD,CLIP,andSSIMscoreswithRACCooN.Itunderscorestheeffectivenessofourmethodin
augmentingvideogenerationmodelswithautomateddescriptions.
5 Conclusion
OurproposedRACCooNframeworknewlyintroducesanauto-descriptivevideo-to-paragraph-to-
videogenerativeframework. RACCooNautomaticallygeneratesvideodescriptionsbyleveraginga
multi-granularspatiotemporalpoolingstrategy,enhancingthemodel’sabilitytorecognizedetailed,
localizedvideoinformation. RACCooNthenusestheseenricheddescriptionstoeditandgenerate
videocontent,offeringuserstheflexibilitytomodifycontentthroughtextualupdates,thuseliminating
theneedfordetailedvideoannotations. ThesevideoeditingandgenerationabilitiesofRACCooN
frameworkhighlightnotableeffectivenessandenableabroaderrangeofuserstoengageinvideo
creationandeditingtaskswithoutthefaithfullywrittentextualprompts.
Acknowledgement
WethankJaeminCho,HanLin,JialuLi,andEliasStengel-Eskinforthethoughtfuldiscussion. This
workwassupportedbyDARPAECOLEProgramNo. HR00112390060,NSF-AIEngageInstitute
DRL-2112635, DARPA Machine Commonsense (MCS) Grant N66001-19-2-4031, ARO Award
W911NF2110220, ONRGrantN00014-23-1-2356, andAccelerateFoundationModelsResearch
program. Theviewscontainedinthisarticlearethoseoftheauthorsandnotofthefundingagency.
References
[1] J.Achiam,S.Adler,S.Agarwal,L.Ahmad,I.Akkaya,F.L.Aleman,D.Almeida,J.Altenschmidt,
S.Altman,S.Anadkat,etal. Gpt-4technicalreport. arXivpreprintarXiv:2303.08774,2023.
[2] P.Anderson,B.Fernando,M.Johnson,andS.Gould. Spice: Semanticpropositionalimage
captionevaluation. InComputerVision–ECCV2016: 14thEuropeanConference,Amsterdam,
TheNetherlands,October11-14,2016,Proceedings,PartV14,pages382–398.Springer,2016.
[3] M.Bain,A.Nagrani,G.Varol,andA.Zisserman. Frozenintime: Ajointvideoandimage
encoderforend-to-endretrieval. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages1728–1738,2021.
[4] A.Blattmann,T.Dockhorn,S.Kulal,D.Mendelevitch,M.Kilian,D.Lorenz,Y.Levi,Z.English,
V.Voleti,A.Letts,etal. Stablevideodiffusion: Scalinglatentvideodiffusionmodelstolarge
datasets. arXivpreprintarXiv:2311.15127,2023.
[5] D.Ceylan,C.-H.P.Huang,andN.J.Mitra. Pix2video: Videoeditingusingimagediffusion. In
ProceedingsoftheInternationalConferenceonComputerVision(ICCV),2023.
[6] W.Chai,X.Guo,G.Wang,andY.Lu. Stablevideo: Text-drivenconsistency-awarediffusion
videoediting. InProceedingsoftheInternationalConferenceonComputerVision(ICCV),
2023.
[7] H. Chen, M. Xia, Y. He, Y. Zhang, X. Cun, S. Yang, J. Xing, Y. Liu, Q. Chen, X. Wang,
etal. Videocrafter1: Opendiffusionmodelsforhigh-qualityvideogeneration. arXivpreprint
arXiv:2310.19512,2023.
[8] Y.Cheng,L.Li,Y.Xu,X.Li,Z.Yang,W.Wang,andY.Yang. Segmentandtrackanything.
arXivpreprintarXiv:2305.06558,2023.
[9] G.Cleuziou. Ageneralizationofk-meansforoverlappingclustering. Rapporttechnique,2007.
10[10] P. Couairon, C. Rambour, J.-E. Haugeard, and N. Thome. Videdit: Zero-shot and spatially
awaretext-drivenvideoediting. arXivpreprintarXiv:2306.08707,2023.
[11] P.Esser,J.Chiu,P.Atighehchian,J.Granskog,andA.Germanidis.Structureandcontent-guided
video synthesis with diffusion models. In Proceedings of the International Conference on
ComputerVision(ICCV),2023.
[12] Q.Fan,R.Panda,etal. Animageclassifiercansufficeforvideounderstanding. arXivpreprint
arXiv:2106.14104,2,2021.
[13] X. Fan, A. Bhattad, and R. Krishna. Videoshop: Localized semantic video editing with
noise-extrapolateddiffusioninversion. arXivpreprintarXiv:2403.14617,2024.
[14] K.Gavrilyuk,A.Ghodrati,Z.Li,andC.G.Snoek. Actorandactionvideosegmentationfroma
sentence. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
pages5958–5966,2018.
[15] M.Geyer,O.Bar-Tal,S.Bagon,andT.Dekel. Tokenflow: Consistentdiffusionfeaturesfor
consistentvideoediting. arXivpreprintarXiv:2307.10373,2023.
[16] D. Giordano, F. Murabito, S. Palazzo, and C. Spampinato. Superpixel-based video object
segmentationusingperceptualorganizationandlocationprior. InProceedingsoftheIEEE
InternationalConferenceonComputerVisionandPatternRecognition(CVPR),2015.
[17] gpt4o. https://openai.com/index/hello-gpt-4o/. May2024.
[18] A.Hertz,R.Mokady,J.Tenenbaum,K.Aberman,Y.Pritch,andD.Cohen-Or.Prompt-to-prompt
imageeditingwithcrossattentioncontrol. arXivpreprintarXiv:2208.01626,2022.
[19] W. Hong, M. Ding, W. Zheng, X. Liu, and J. Tang. Cogvideo: Large-scale pretraining for
text-to-videogenerationviatransformers. arXivpreprintarXiv:2205.15868,2022.
[20] A.HoreandD.Ziou.Imagequalitymetrics: Psnrvs.ssim.In201020thinternationalconference
onpatternrecognition,pages2366–2369.IEEE,2010.
[21] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora:
Low-rankadaptationoflargelanguagemodels. InProceedingsoftheInternationalConference
onLearningRepresentations(ICLR),2022.
[22] E.Ilg,N.Mayer,T.Saikia,M.Keuper,A.Dosovitskiy,andT.Brox. Flownet2.0: Evolutionof
opticalflowestimationwithdeepnetworks. InProceedingsoftheIEEEconferenceoncomputer
visionandpatternrecognition,pages2462–2470,2017.
[23] H.JeongandJ.C.Ye. Ground-a-video: Zero-shotgroundedvideoeditingusingtext-to-image
diffusionmodels. arXivpreprintarXiv:2310.01107,2023.
[24] Y.Jo,S.Lee,A.S.Lee,H.Lee,H.Oh,andM.Seo. Zero-shotdensevideocaptioningbyjointly
optimizingtextandmoment. arXivpreprintarXiv:2307.02682,2023.
[25] T.-W.Ke,S.Mo,andX.Y.Stella. Learninghierarchicalimagesegmentationforrecognitionand
byrecognition. InProceedingsoftheInternationalConferenceonLearningRepresentations
(ICLR),2023.
[26] S. Khanmohammadi, N. Adibeig, and S. Shanehbandy. An improved overlapping k-means
clusteringmethodformedicalapplications. ExpertSystemswithApplications,2017.
[27] D.P.KingmaandM.Welling.Auto-encodingvariationalbayes.arXivpreprintarXiv:1312.6114,
2013.
[28] A.Kirillov,E.Mintun,N.Ravi,H.Mao,C.Rolland,L.Gustafson,T.Xiao,S.Whitehead,A.C.
Berg,W.-Y.Lo,P.Dollár,andR.Girshick. Segmentanything. arXiv:2304.02643,2023.
[29] D.Kondratyuk,L.Yu,X.Gu,J.Lezama,J.Huang,R.Hornung,H.Adam,H.Akbari,Y.Alon,
V.Birodkar,etal. Videopoet: Alargelanguagemodelforzero-shotvideogeneration. arXiv
preprintarXiv:2312.14125,2023.
11[30] R.Krishna,K.Hata,F.Ren,L.Fei-Fei,andJ.CarlosNiebles.Dense-captioningeventsinvideos.
InProceedingsoftheIEEEinternationalconferenceoncomputervision,pages706–715,2017.
[31] K.Li,Y.He,Y.Wang,Y.Li,W.Wang,P.Luo,Y.Wang,L.Wang,andY.Qiao. Videochat:
Chat-centricvideounderstanding. arXivpreprintarXiv:2305.06355,2023.
[32] Z.LiandJ.Chen. Superpixelsegmentationusinglinearspectralclustering. InProceedingsof
theIEEEInternationalConferenceonComputerVisionandPatternRecognition(CVPR),2015.
[33] B. Lin, B. Zhu, Y. Ye, M. Ning, P. Jin, and L. Yuan. Video-llava: Learning united visual
representationbyalignmentbeforeprojection. arXivpreprintarXiv:2311.10122,2023.
[34] H. Lin, A. Zala, J. Cho, and M. Bansal. Videodirectorgpt: Consistent multi-scene video
generationviallm-guidedplanning. arXivpreprintarXiv:2309.15091,2023.
[35] F. Liu, K. Lin, L. Li, J. Wang, Y. Yacoob, and L. Wang. Mitigating hallucination in large
multi-modalmodelsviarobustinstructiontuning.InProceedingsoftheInternationalConference
onLearningRepresentations(ICLR),2023.
[36] H.Liu,C.Li,Q.Wu,andY.J.Lee.Visualinstructiontuning.InAdvancesinNeuralInformation
ProcessingSystems(NeurIPS),2023.
[37] S.Liu,Z.Zeng,T.Ren,F.Li,H.Zhang,J.Yang,C.Li,J.Yang,H.Su,J.Zhu,etal. Grounding
dino: Marryingdinowithgroundedpre-trainingforopen-setobjectdetection. arXivpreprint
arXiv:2303.05499,2023.
[38] S. Liu, Y. Zhang, W. Li, Z. Lin, and J. Jia. Video-p2p: Video editing with cross-attention
control. arXivpreprintarXiv:2303.04761,2023.
[39] Z.Ma,J.Pan,andJ.Chai. World-to-words: Groundedopenvocabularyacquisitionthroughfast
mappinginvision-languagemodels. arXivpreprintarXiv:2306.08685,2023.
[40] A. Majumdar, A. Shrivastava, S. Lee, P. Anderson, D. Parikh, and D. Batra. Improving
vision-and-languagenavigationwithimage-textpairsfromtheweb. InComputerVision–ECCV
2020: 16thEuropeanConference,Glasgow,UK,August23–28,2020,Proceedings,PartVI16,
pages259–274.Springer,2020.
[41] K. Mei and V. Patel. Vidm: Video implicit diffusion models. In Proceedings of the AAAI
NationalConferenceonArtificialIntelligence(AAAI),2023.
[42] S. Munasinghe, R. Thushara, M. Maaz, H. A. Rasheed, S. Khan, M. Shah, and F. Khan.
Pg-video-llava: Pixelgroundinglargevideo-languagemodels. arXivpreprintarXiv:2311.13435,
2023.
[43] openai. https://openai.com/sora. 2024.
[44] J.Pont-Tuset,F.Perazzi,S.Caelles,P.Arbeláez,A.Sorkine-Hornung,andL.VanGool. The
2017davischallengeonvideoobjectsegmentation. arXivpreprintarXiv:1704.00675,2017.
[45] C.Qi,X.Cun,Y.Zhang,C.Lei,X.Wang,Y.Shan,andQ.Chen. Fatezero: Fusingattentions
forzero-shottext-basedvideoediting. arXivpreprintarXiv:2303.09535,2023.
[46] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,
P.Mishkin,J.Clark,etal.Learningtransferablevisualmodelsfromnaturallanguagesupervision.
InProceedingsoftheInternationalConferenceonMachineLearning(ICML),2021.
[47] T.Ren,S.Liu,A.Zeng,J.Lin,K.Li,H.Cao,J.Chen,X.Huang,Y.Chen,F.Yan,Z.Zeng,
H.Zhang,F.Li,J.Yang,H.Li,Q.Jiang,andL.Zhang. Groundedsam: Assemblingopen-world
modelsfordiversevisualtasks. arXivpreprintarXiv:2401.14159,2024.
[48] R.Rombach,A.Blattmann,D.Lorenz,P.Esser,andB.Ommer.High-resolutionimagesynthesis
withlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconferenceoncomputervision
andpatternrecognition,pages10684–10695,2022.
12[49] Z. Shen, J. Li, Z. Su, M. Li, Y. Chen, Y.-G. Jiang, and X. Xue. Weakly supervised dense
videocaptioning. InProceedingsoftheIEEEConferenceonComputerVisionandPattern
Recognition,pages1916–1924,2017.
[50] K.Soomro,A.R.Zamir,andM.Shah. Ucf101: Adatasetof101humanactionsclassesfrom
videosinthewild. arXivpreprintarXiv:1212.0402,2012.
[51] G.Team,R.Anil,S.Borgeaud,Y.Wu,J.-B.Alayrac,J.Yu,R.Soricut,J.Schalkwyk,A.M.
Dai,A.Hauth,etal. Gemini: afamilyofhighlycapablemultimodalmodels. arXivpreprint
arXiv:2312.11805,2023.
[52] Y. Tewel, Y. Shalev, R. Nadler, I. Schwartz, and L. Wolf. Zero-shot video captioning with
evolvingpseudo-tokens. arXivpreprintarXiv:2207.11100,2022.
[53] H.Touvron,L.Martin,K.Stone,P.Albert,A.Almahairi,Y.Babaei,N.Bashlykov,S.Batra,
P.Bhargava,S.Bhosale,etal. Llama2: Openfoundationandfine-tunedchatmodels. arXiv
preprintarXiv:2307.09288,2023.
[54] T.Unterthiner,S.vanSteenkiste,K.Kurach,R.Marinier,M.Michalski,andS.Gelly. Fvd: A
newmetricforvideogeneration. InICLRworkshop,2019.
[55] R.Vedantam,C.LawrenceZitnick,andD.Parikh. Cider: Consensus-basedimagedescription
evaluation. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
pages4566–4575,2015.
[56] B. Wang, L. Ma, W. Zhang, and W. Liu. Reconstruction network for video captioning. In
ProceedingsoftheIEEEInternationalConferenceonComputerVisionandPatternRecognition
(CVPR),2018.
[57] L.Wang,J.He,S.Li,N.Liu,andE.-P.Lim. Mitigatingfine-grainedhallucinationbyfine-tuning
largevision-languagemodelswithcaptionrewrites. InInternationalConferenceonMultimedia
Modeling.Springer,2024.
[58] T.Wang,R.Zhang,Z.Lu,F.Zheng,R.Cheng,andP.Luo. End-to-enddensevideocaptioning
withparalleldecoding. InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision,pages6847–6857,2021.
[59] W.Wang,Y.Jiang,K.Xie,Z.Liu,H.Chen,Y.Cao,X.Wang,andC.Shen. Zero-shotvideo
editingusingoff-the-shelfimagediffusionmodels. arXivpreprintarXiv:2303.17599,2023.
[60] X. Wang, H. Yuan, S. Zhang, D. Chen, J. Wang, Y. Zhang, Y. Shen, D. Zhao, and J. Zhou.
Videocomposer: Compositionalvideosynthesiswithmotioncontrollability. AdvancesinNeural
InformationProcessingSystems,36,2024.
[61] J.J.Whang,I.S.Dhillon,andD.F.Gleich.Non-exhaustive,overlappingk-means.InProceedings
ofthe2015SIAMinternationalconferenceondatamining,pages936–944.SIAM,2015.
[62] J. Wu, X. Li, C. Si, S. Zhou, J. Yang, J. Zhang, Y. Li, K. Chen, Y. Tong, Z. Liu, et al.
Towardslanguage-drivenvideoinpaintingviamultimodallargelanguagemodels. arXivpreprint
arXiv:2401.10226,2024.
[63] J. Z. Wu, Y. Ge, X. Wang, S. W. Lei, Y. Gu, Y. Shi, W. Hsu, Y. Shan, X. Qie, and M. Z.
Shou. Tune-a-video: One-shottuningofimagediffusionmodelsfortext-to-videogeneration. In
ProceedingsoftheInternationalConferenceonComputerVision(ICCV),2023.
[64] W.Wu,H.Luo,B.Fang,J.Wang,andW.Ouyang. Cap4video: Whatcanauxiliarycaptionsdo
fortext-videoretrieval? InProceedingsoftheIEEEInternationalConferenceonComputer
VisionandPatternRecognition(CVPR),2023.
[65] S.Xie,Z.Zhang,Z.Lin,T.Hinz,andK.Zhang. Smartbrush: Textandshapeguidedobject
inpaintingwithdiffusionmodel. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages22428–22437,2023.
13[66] J. Xing, M. Xia, Y. Zhang, H. Chen, X. Wang, T.-T. Wong, and Y. Shan. Dynamicrafter:
Animatingopen-domainimageswithvideodiffusionpriors. arXivpreprintarXiv:2310.12190,
2023.
[67] W.Yan,Y.Zhang,P.Abbeel,andA.Srinivas. Videogpt: Videogenerationusingvq-vaeand
transformers. arXivpreprintarXiv:2104.10157,2021.
[68] A.Yang,A.Nagrani,P.H.Seo,A.Miech,J.Pont-Tuset,I.Laptev,J.Sivic,andC.Schmid.
Vid2seq: Large-scalepretrainingofavisuallanguagemodelfordensevideocaptioning. In
ProceedingsoftheIEEEInternationalConferenceonComputerVisionandPatternRecognition
(CVPR),2023.
[69] F. Yang, Q. Sun, H. Jin, and Z. Zhou. Superpixel segmentation with fully convolutional
networks. InProceedingsoftheIEEEInternationalConferenceonComputerVisionandPattern
Recognition(CVPR),2020.
[70] X.Yang,L.Zhu,H.Fan,andY.Yang. Eva: Zero-shotaccurateattributesandmulti-objectvideo
editing. arXivpreprintarXiv:2403.16111,2024.
[71] Z.Yang,Y.Wei,andY.Yang.Associatingobjectswithtransformersforvideoobjectsegmentation.
InAdvancesinNeuralInformationProcessingSystems(NeurIPS),2021.
[72] Z. Yang and Y. Yang. Decoupling features in hierarchical propagation for video object
segmentation. InAdvancesinNeuralInformationProcessingSystems(NeurIPS),2022.
[73] S.Yu,J.Yoon,andM.Bansal. Crema: Multimodalcompositionalvideoreasoningviaefficient
modularadaptationandfusion. arXivpreprintarXiv:2402.05889,2024.
[74] T. Yu, R. Feng, R. Feng, J. Liu, X. Jin, W. Zeng, and Z. Chen. Inpaint anything: Segment
anythingmeetsimageinpainting. arXivpreprintarXiv:2304.06790,2023.
[75] H.Zhang,X.Li,andL.Bing. Video-llama: Aninstruction-tunedaudio-visuallanguagemodel
forvideounderstanding. InProceedingsoftheConferenceonEmpiricalMethodsinNatural
LanguageProcessing(EMNLP),2023.
[76] Y. Zhang, B. Li, h. Liu, Y. j. Lee, L. Gui, D. Fu, J. Feng, Z. Liu, and C. Li.
Llava-next: A strong zero-shot video understanding model. https://llava-vl.github.io/blog/
2024-04-30-llava-next-video/. April2024.
[77] Z. Zhang, B. Li, X. Nie, C. Han, T. Guo, and L. Liu. Towards consistent video editing
withtext-to-imagediffusionmodels. InAdvancesinNeuralInformationProcessingSystems
(NeurIPS),2023.
[78] Z.Zhang,B.Wu,X.Wang,Y.Luo,L.Zhang,Y.Zhao,P.Vajda,D.Metaxas,andL.Yu. Avid:
Any-lengthvideoinpaintingwithdiffusionmodel. arXivpreprintarXiv:2312.03816,2023.
[79] L.Zheng,W.-L.Chiang,Y.Sheng,S.Zhuang,Z.Wu,Y.Zhuang,Z.Lin,Z.Li,D.Li,E.Xing,
etal. Judgingllm-as-a-judgewithmt-benchandchatbotarena. AdvancesinNeuralInformation
ProcessingSystems,36,2024.
[80] L.Zhou,C.Xu,andJ.Corso. Towardsautomaticlearningofproceduresfromwebinstructional
videos. InProceedingsoftheAAAIConferenceonArtificialIntelligence,2018.
[81] L.Zhou,Y.Zhou,J.J.Corso,R.Socher,andC.Xiong. End-to-enddensevideocaptioningwith
maskedtransformer. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pages8739–8748,2018.
[82] Y. Zhou, C. Cui, J. Yoon, L. Zhang, Z. Deng, C. Finn, M. Bansal, and H. Yao. Analyzing
and mitigating object hallucination in large vision-language models. In Proceedings of the
InternationalConferenceonLearningRepresentations(ICLR),2024.
14Appendix
Inthisappendix,wepresentthefollowing:
• MoredetailsaboutVPLMdatasetcollection(SectionA.1),experimentalsetups(SectionA.2),
moreimplementationdetails(SectionA.3).
• LimitationsandNegativeSocietalImpactofRACCooN(Sec.B).
• Additionalanalysisincludingablations(Sec.C.2,Sec.C.1).
• AdditionalqualitativeexampleswithRACCooNonvideocontentediting(Sec.C.3).
A ExperimentalSetup
GPT4 Prompt
Please carefully analyze the provided video. Please carefully analyze the provided video. The video is
presented as frame sequences, and they are placed in a top-left to bottom-right order as the temporal order (the
top-left is the start frame, the bottom-right is the end frame, and there are frame indexes 1,2,3,4,5,6 on each
frame).
Please focus on two tasks (1) providing a short and well-organized holistic video description including main
objects, actions, and events; (2) identifying and cataloging (up to five) significant objects within this video.
Ensure that your analysis adheres to the structure outlined below, prioritizing objects that are clear, discernible,
and integral to the overall context of the video. Your descriptions should be rich in detail, capturing key
attributes such as color, shape, actions, styles, movements, and any distinct features. Strive for precision and
succinctness to aid in comprehension and ease of reference. SuperImage as Video
Format for Video Description (Ensure to avoid repetitive descriptions and avoid frame-by-frame
descriptions.):
Video Description: Drone view of waves crashing against the rugged cliffs along Big Sur's garay point beach.
The crashing blue waters create white-tipped waves, while the golden light of the setting sun illuminates the
r T s H
E
o e h xi ac g e
a
.k h
m
y Ts w t ph es a lh ie ey spo .
F
r id se
o
. r
r
ao mA p v
a
s if tm e r fwo oa m
r
ll t
L
hi t as h isl tea
t
icn nr aod gpa w t Id u
d
i r edt eh no s tw a it
f
hn il ei e g dt oh r Oa t th wh bo e
j
u eb s b cee te a a Nus ci t at yhs
m
i oi esn f
s
a t t
a
h hd ne er
d
ad cm ti o hs aa eta st ii rn tc c Da e f n ee, d saa ctn t,
r
h id w pe tg i irtr oh ue
n
ge t sgn h :e es d h c r l liu af nb f' db s s e e cr dy ag p c e eo s v o je fu r t ts t h it n eh ge P o ac u cli itf f f io' cs v e Ce rd o g t ahe se. t A b g
p
ha r
a a
c em
n
ba y
d
itr b au ae o tn t .o aa i Tf sk of he o r
it
er mr e
c
a e,s
l
t
i
na mt .a vp T
b
ipw rsh ori e ol nod a mvvli c eif d ee h re
n
i ans o
t
a g
i l
sodn a gec snt p eu sdi rtca eritr un nsyV ct
e
ea tii ,s urd as sre e ece uo n tr rdi ig rneD oea gs use g iw ns oi gn dc f i
n
tg er f h er i
d
awp dt hm t bi tti e oho ye n ps
m
da a
e
s ig n
m
nhi da so ica en w . t
t
i
g
hHnp rega ee n
e
at ghd nneea eimn rci t yn ala y alra 'se a
n
tl s dnu a s as k ti hsh te ut
e
r sg
r
ai sr n te lh
o
ae e
u
n nli dg sh t
1 b
H 1
. l
.i
a
n
PW c
t
ak so n. .m p d.
f
u
o
a(a Cr rn s o O: e n.A
b
tS i js n eht cuey te l swi is ifeh
n
a t h tw r hs eo ers m eu
V
n aa irg dn el e. a ooS s :tsh hee es r w a sne igdar n rs ie fa d ic b l ail npa tsc t ok i bc l k je e.a cSt th she )er wja ac lk ke st , c oa nl fo in dg e nr te ld y ad nre ds s c, a sa un ad l lb yl .a ck boots, and carries a o 1 t
p
fe af .
a
m cn G
t
eca pi
h
a at e eu n nr sr a dte
,
m P,
l b
ef ea oo gnn dc std yu ,a .as ani : nn dA dg s l ao a trI n rd o bg n aec e g nno , dn t d ai s f i s ai se e s ctr i od rv n oc a cO sit t a si ibo v t i ien j o te
s
nba c sln t wa
h
d N c i ot k haa
u
mn la c di n om ee d n rsa
s
s wl a e ac hn r rva i ed t ar e e ctD i. b o oe e n ns a tc e rr
a
r ffk si op n tr et o ti dswo . n sn I hts sf ao b rr
p
lai lt ys c k wd eo ithaci rl ise ts, e wy he
i te
… N i o Yn b ou s. j uim eg rch b t dste e r i i sn f c e t to ra h i c peeh y ta i c oao h nrb e soj e pb sc hajet r o c t uu t o' ln s df i aq a epu xlpe a cel r ey ga l, r e a r ie , nn n s cs cieu g l ar nai rn in ifd tg i yc ,i a ty s n do t esu tii ar tg e in lmd ,i e f ais oc nc ra dr n si p cc st e pei n o ew e cn . iis ft h ica i inr te y t ,h b eeo n t v sh i u d rc e io o nn . g c H yis i oge u h a l cin g ad h n t t rah eno cyr oo nru e sg tl rah ut, i c o tp n r tso hhv ei i pd osi b n b jg ee c tw tc sl e e iea nnr 2 e o ro. m r g lC eb a a ir n nor i e zi ad at nea t iirk moye n asr . l: y H cmA aen b r a eoa l l sd aiz ou nin l dwt g m e m a aa a rpl ise na taw n e d ne da aa a nr rai k cn n eg bd . e a s lt o li amg nh e dt sg lic gre r hiy p t- tu c, on p li o ofo s rr esm dib l pj ya a c i ndk tee snt , tw sif hyit oih n w gg i r nte h ge e hn i s official
y E no onu ts ur p r rm e o i vtn ihd de e a oc euc xto ppr lud at ni n c ag o ti n oto t na sy i no osu r r o n bd oje te es c sc t r s fi o p i rnt i t o htn h es e g. p er no ev raid tee dd ch ai pn tt is o. nE sn . sure the output length is less than 400 words. Do 3 t th r. a aB nt qa p um ro ilb iv tyo id o ae n F a do n ir sae t ps u it vr: a oD l ti ase t ln i c fs o e a r n ta hdn e d e e nl cu ri os c lh h o, i g nt ih g ce ah lf ao thbr eie ta ms tt efc o o orm f t thp her ei s p se a as n n dt ca a tl u.l, aT g rh yr ee . e an r eb aa m exb uo do e ss talks
Figure5: PipelineofourVPLMdatasetannotationwithGPT4V.Wefirstconvertavideoas
asuperimageandthengivesomein-contextexamplestopromptGPT-4Vtoannotatedetailedand
well-structuredvideodescriptions.
A.1 VPLMdatacollection.
As we mention in Section 3.3, to facilitate our model training, we start from open-source video
inpaintingdata[62]4tobuildahigh-qualitydatasetthatincludesthewell-structured,detailedcaption
forbothvideoandeachobjectinthevideo. Specifically,weleverageGPT-4V 5 toannotateeach
video. AsshowninFigure5,wefirstconvertavideotoasuperimage[12]byconcatenatinguniformly
sampledframes,andwealsodrawframeIDsoneachframeasavisualprompttopresenttemporal
order. ThenweprompttheGPT-4Vbyprovidingadetailedpromptwithin-contextexamples(left
ofFigure5). Inthiscase,weobtainedwell-structured,detailedcaptionsthatcontainbothholistic
videoandlocalobjects(bottomrightofFigure5). Toensuretheannotationquality,wesampled
1annotatedvideofromeach100batchesandthendidahumancross-check,andrefinedthebatch
annotationsaccordingtothesampledexample. Throughthispipeline,weobtained7.2Khigh-quality
qualitywell-structured,detailedvideodescriptionswithanaverageof238.0wordsforeachvideo.
Next,toobtainpairedobject-mask-descriptiontripletsforvideoinpaintingmodeltraining,webuild
an automatic detailed object caption and object name matching pipeline using GPT4. As in our
basedataset[62], wealreadyhaveclasslabelsforeachobjectmask, weframedthismatchingas
amulti-choiceQAtoaskGPT4whichobjectcaptioncaninFigure5matchedtothegivenobject
4MITLicense: https://choosealicense.com/licenses/mit/
5version1106
15classes. Wefurtherfilteredoutthetripletswithtoosmallmasks(<1%maskareas.) Inthiscase,
weobtained5.5Kobject-mask-descriptiontripletswithanaverageof37.2wordsforeachobjectto
supportourvideodiffusionmodeltraining.
A.2 BenchmarksandDatasetsDetails
As mentioned in the main paper (Section 4.1), we evaluate our proposed RACCooN on various
tasks. Forvideo-to-paragraphgeneration,wetestourmodelonthestandardvideocaptiondataset
YouCook2[80](validationset)aswellasourVPLMdataset. Wenexttestvideocontenteditingwith
threesubtasksonourVPLMdataset. RegardingtheexperimentsofincorporatingRACCooNwith
otherconditionalvideogenerationmodels,wetestRACCooNondiversevideosfromActivityNet,
YouCook2,andUCF101. Weuniformlyselected100videosfromthose3datasetstobuildthetest
bed. FortheexperimentsofincorporatingRACCooNwithothervideoeditingmodels,wefollow
thepreviouswork[15],andselect30uniquevideosfromtheDAVISdataset. Foreachvideo,we
annotatetwodifferenttypesofediting,attributeeditingandinstanceediting. Itleadsto60text-video
pairsinourvideoeditingevaluation. Wechooseobjectcaptionsthatcontainthesamekeywordsfor
editinginhumancaptionstorepresentthemodel-generatedcaption.
A.3 ImplementationDetails
Metrics: Weprovidemoredetailsaboutourmetrics. CLIP-Textmeasuresthesimilaritybetween
the edit prompt and the embedding of each frame in the edited video. CLIP-Frame computes
theaverageCLIPsimilaritybetweentheimageembeddingsofconsecutiveframestomeasurethe
temporalcoherency. SSIMmeasuresthestructuralsimilaritybetweentheoriginalandeditedvideo
forevaluatinglocalizedediting. Q =CLIP −T/Wrap−Err,itisacomprehensivescorefor
edit
videoeditingquality,whereWarp−Errcalculatesthepixel-leveldifferencebywarpingtheedited
videoframesaccordingtotheestimatedopticalflowofthesourcevideo,extractedbyFlowNet2.0[22].
Forlayoutplanning,wecomputetheFVDandCLIP-Imagesimilaritybetweenthegroundtruthand
thepredictedboundingbox.
𝒎 1 𝒎 𝑘
Superpixel
OKM
… A Pv oe or la ing ge ⊗
Predictor
ഥ𝑺 𝒆∈ℝ𝑡×ℎ×𝑤×𝑑 𝒆𝒍
Fine-grained superpixels𝒈 Grouping relevant superpixels
Input Video Obtaining MGS Pooled Tokens
and features 𝑺 into kclusters
Figure6: IllustrationofMGSpooling. WeobtainMGSpoolingtokensusingaspatiotemporalmask
mviaoverlappingk-meansclustering(OKM)ofaveragedsuperpixelfeaturesS¯.
More Details about Multi-granular Spatiotemporal Pooling. As mentioned Section 3.1, we
proposed a novel Multi-granular Spatiotemporal Pooling (MGS Pooling) to address the lack of
complex spatial-temporal modeling in video. We further provide a more intuitive visualization
forourproposedMGSPoolinginFigure6. Wefirstadoptalightweight165superpixelpredictor
thatgeneratessuperpixelsacrossvideoframes,thenusetheoverlappingk-meansclusteringforthe
obtainedvideosuperpixels. Inthiscase,wegatherinformativecuesaboutvariousobjectsandactions
forLLM.
HumanEvaluationonVideo-to-ParagraphGeneration. Weconductahumanevaluationonnine
randomlyselectedvideosfromtheYouCook2dataset. Videosarethree-tofive-minute-longand
containmultiplesuccessivesceneswithcomplexviewpoints. Weprovidethesevideostofourdifferent
annotatorswithgroundtruthcaptions,descriptionsgeneratedbyPG-Video-LLAVA,andourmethod,
RACCooN, where the captions/descriptions for each video are randomly shuffled. We leverage
four distinct human evaluation metrics: Logic Fluency (Logic), Language Fluency (Language),
VideoSummary(Summary)andVideoDetails(Details). Toavoidamisinterpretationofmethods’
capabilitiesduetorelativeevaluation,weinstructtheannotatorstoindependentlyratethequalityof
eachsetofcaptionsbasedonthesefourdifferentcriteria,bygivingascorefrom1to5(i.e.,choice:
[1,2,3,4,5]).
16Off-shelfVideoEditingModels. WeutilizeTokenFlow[15]andFateZero[45]asourvideoediting
tools. TokenFlowgeneratesahigh-qualityvideocorrespondingtothetargettext,whilepreserving
the spatial layout and motion of the input video. For SSIM computation, we compute SSIM for
region-of-no-interestsincewewanttokeepthoseregionsunchanged. wefirstmaskoutregionsof
interestwiththegroundtruthmaskprovidedbytheDAVISdataset,thewecomputeSSIMonmasked
imagesandconductmeanpoolingasthevideo-levelmetrics.
Off-shelfConditionalVideoGenerationModels. WeleveragebothVideoCrafter[7]andDynami-
Crafter[66]asourvideogenerationbackbone. VideoCrafterisoneoftheSoTAvideogeneration
models that can handle different input conditions (image, text). DynamiCrafter is based on the
open-sourceVideoCrafterandT2ILatentDiffusionmodel[48],andwastrainedonWebVid10M[3],it
providesbetterdynamicandstrongercoherence. WeadoptVideoCrafter-512andDynamiCrafter-512
variants. Foreachvideo,weuseCLIPsimilaritytoretrievemultiplekeyframescorrespondingtoeach
caption. Thosekeyframesresultinmultiplegeneratedvideoclipsviathevideogenerationmodel. For
FVDcomputation,weconductmeanpoolingoverthoseclipstorepresentavideo. Weusek =25
andv =6forgeneratedcaptionsinallexperiments. Theexperimentsareconductedonthe4×48GB
A6000GPUsmachine.
B LimitationsandBroaderImpact
Our proposed RACCooN framework has shown a remarkable ability to interpret input videos,
producingwell-structuredanddetaileddescriptionsthatoutperformstrongvideocaptioningbaselines
andevengroundtruths. However,ithasthepotentialtoproduceinaccuraciesorhallucination[35,
57,82,39]inthegeneratedtextoutputs. Inaddition,theperformanceofourproposedframework
inparagraphgeneration, videogeneration, andeditingisinfluencedbytheemployedpre-trained
backbones,includinganLLM[53],baseInpaintingModel[48],VideoDiffusionModel[66],and
Video Editor [15]. However, our key contributions are independent of these backbones, and we
emphasizethatRACCooN’scapabilitiescanbefurtherenhancedwithfutureadvancementsinthese
generativemodelbackbones.
LLM-empoweredvideodescriptionandphotorealisticvideocreation/editinginheritbiasesfromtheir
trainingdata,leadingtoseveralbroaderimpacts,includingsocietalstereotypes,biasedinterpretation
ofactions,andprivacyconcerns. Tomitigatethesebroaderimpacts,itisessentialtocarefullydevelop
andimplementgenerativeandvideodescriptionmodels,suchasconsideringdiversifyingtraining
datasets,implementingfairnessandbiasevaluationmetrics,andengagingcommunitiestounderstand
andaddresstheirconcerns.
C AdditionalAnalysis
C.1 Ablationstudy
TheeffectofkandvAsshowninTable7,wedidinitializedhyperparameterprobingexperimentson
ActivityNet-CapandYouCook2datasets. weobservethatallvariantsofourapproachwithvarying
kandvgenerallyachieveimprovedperformancecomparedtobaselinesintermsofmultiplevideo
captioningmetrics: SPICE,BLEU-4,METEOR,andROUGE.Thisresultdemonstratestheefficacy
ofourmulti-granularspatiotemporalpoolingapproachwithafinetocoarsesearchofvideocontexts
basedonsuperpixels. Inaddition,weobservethatRACCooNframeworkshowsasmallgapbetween
variantsineachdataset,highlightingtherobustnessofourapproachtothehyperparametersetupsand
datasets.
TheeffectofSuperpixelOverlapWeintroduceoverlappingk-meansclusteringtoaggregatevideo
superpixels,capturingavarietyofvisualcontextswhileallowingforpartialspatiotemporaloverlap.
Toinvestigatetheeffectofoursuggestedoverlappingapproach,wealsoevaluatethevariantofour
frameworkwithoutoverlap(i.e.,v =1)onvideo-to-paragraphgenerationtasksinTable7. Asshown,
ourapproachwithoverlap(i.e.,v >1)surpassesthenon-overlappingversionofRACCooNacross
variousscalesofvisualcontextsk,asindicatedbythevideocaptioningmetricsweevaluated. This
emphasizestheadvantageofpermittingoverlapinunderstandingvideocontexts,whichenhancesthe
inputvideo’scomprehensionbyallowingfordiverseandfluentinterpretationsoflocalvisualregions
withsurroundingsassociatedatthesametime.
17Table7: AblationofRACCooNforVideo-to-ParagraphGenerationonActivityNetandYouCook2.
Metricsareabbreviated: M:METEOR,B:BLEU-4,S:SPICE,R:ROUGE.v =1indicatestheversion
withoutsuperpixeloverlap. Wehighlightthehyperparametersetupusedinthemainexperiment.
ActivityNet YouCook2
Models k v
S B M R S B M R
PDVC[58] - - - 2.6 10.5 - - 0.8 4.7 -
Vid2Seq[68] - - 5.4 - 7.1 - 4.0 - 4.6 -
ZeroTA[24] - - 2.6 - 2.7 - 1.6 - 2.1 -
PG-VL[42] - - 13.6 13.9 14.2 18.1 6.2 16.5 8.6 15.8
1 13.5 13.9 14.2 18.1 6.3 16.9 8.7 15.9
2 13.7 14.6 14.4 18.2 6.4 17.5 8.7 16.1
20
4 13.6 14.3 14.3 18.2 6.6 16.2 8.8 16.0
5 13.8 15.0 14.5 18.4 6.4 17.9 8.8 16.2
1 13.6 14.1 14.3 18.0 6.1 16.9 8.6 15.9
RA (OC uC ro so )N 25 2
4
1 13 3. .8
6
11 44 .. 34 11 44 .. 33 11 88 .. 23 66 .. 64 11 76 .. 13 89 .. 90 11 66 .. 10
6 13.7 14.5 14.4 18.2 6.9 18.0 9.0 16.1
10 - - - - 6.4 16.5 8.7 16.1
1 13.6 14.1 14.3 18.0 6.3 16.5 8.8 16.0
30 2 13.7 14.2 14.2 18.1 6.6 17.1 9.0 16.2
4 13.5 14.5 14.3 18.2 6.6 18.1 8.8 16.1
6 13.6 14.4 14.4 18.2 6.4 17.2 8.8 16.2
Table8: RACCooNvariantswithdifferentgroundingmethodsforVideo-to-ParagraphGeneration
onYouCook2.
Method Localization Clustering SPICE BLEU-4 METEOR ROUGE
PG-VL[42] - - 6.2 16.5 8.6 15.8
SAM[28] - 6.4 16.9 8.7 15.9
GroundedSAM[47] - 6.5 16.5 8.7 16.1
Ours
SAM-Track[8] k-means 6.2 16.5 8.8 16.1
SAM-Track[8] overlappingk-means 6.5 17.4 9.0 16.1
Superpixel overlappingk-means 6.9 18.0 9.0 16.1
Forsimplicity,weusek = 25andv = 6forallexperimentsonconditionalvideogenerationand
videoeditingtasks,demonstratingtherobustnessofRACCooNforhyperparameters.
C.2 ComparisonwithPre-trainedGroundingModels
Wefurtherinvestigatetheapplicabilityofrecentpowerfulpre-trainedvisualgroundingmodels[28,8,
47]. Segment-Anything[28]andGroundedSAM[47]arestrongopen-endedobjectsegmentation
modelsforimages,andwedirectlycomputeourlocalizedgranulartokensbasedontheirsegmentation
masks. Weselect25segmentationmasksintotal,fromuniformlysampledframesineachvideofor
faircomparisonwithRACCooN(k =25). AsshowninTable8,thesevariantsofRACCooNachieve
improvedperformanceagainstthebest-performingbaseline,PG-VL,butareoftensuboptimalsince
theyfocusonregionalinformationandcannotcontaintemporalinformationofthevideos. Unlikethese
image-basedsegmentationmethods,SAM-Track[8]generatescoherentmasksofobservedobjects
oversuccessiveframesinvideos,byadoptingmultipleadditionalpre-trainedmodules,including
GroundingDino[37]andAOT[71,72]. WeadoptSAM-Tracktoinitializesuperpixelsinvideosand
conductoverlappingk-meansclustering(k =25). Here,weobservethatRACCooNwithSAM-Track
superpixelinitializationachievesreasonableperformance,andisbeneficialforvideoeditingtasks. It
enablesthemodeltocoherentlyedittargetedregionsinvideoswitheditedkeywords.
C.3 AdditionalVisualizations
Inthissection,weprovidemorequalitativeexamplesofvarioustasks,includingthreetypesofvideo
contentediting,ablationonremovingoracleplanningandGTmasks,enhancedvideoediting,and
conditionalvideogeneration.
18Remove, Add, and Change Object the videos. We provide more qualitative examples in this
Appendixacrossdifferenttypesofvideocontentediting,includingremoving(Figure7,Figure8,
Figure 9), adding (Figure 10, Figure 11, Figure 12), and changing/editing (Figure 13, Figure 14,
Figure 15). According to the visualization, our RACCooN generally outperforms other strong
baselines on all three subtasks. Our RACCooN can reflect the updated text description more
accurately,thusaidinginauser-friendlyvideo-generativeframework. Forexample,ourmethodcan
accuratelychangethecolorofthehat(red→blue),whichisaverysmallareainthevideo,whileother
methodsstruggletomeettherequirement.
Ablation Study Visualization. We illustrate extra visualizations for replacing orecle mask with
grounding&trackingtoolsgeneratedonesforvideoobjectremoval(Figure16and Figure17)and
changing(Figure18and Figure19),aswellasreplacingoracleobjectboxeswithourmodel-predicted
one(Figure20and Figure21). OurframeworkshowsrobustresultswithLLMplanningandoff-shelf
segmentationtools. Wefurthershowthefailurecasesofremovingandchangingobjectsmainlycome
fromthemissingmaskpredictionofthevideosegmentationmasks.
19[Observer]: A male wearing dark grey shorts, a black T-shirt with neon yellow logos,
and athletic shoes. He stands attentively with his arms crossed, watching the
shooter's performance.backrgound
[White Poodle]: A small, fluffy white poodle exhibiting a pampered and stylized
appearance with a rounded haircut characteristic of the breed's show grooming
standards. The dog moves with a deliberate prance, displaying the distinctive behavior
and training fit for a show animal.backrgound
Figure7: Morevisualizationofremovingvideoobjects
20
tupnI
CV
IVGL
sruO
tupnI
CV
IVGL
sruO[Seagull]: A white seagull with wings spread wide, showcasing gray tips. As it flies, its
yellow beak and black eye markings are noticeable, adding to its distinctive features.
The bird's underbelly is white, reflecting sunlight, while the light and smooth lines of
its body exemplify a strong yet graceful figure in mid-flight. backrgound
[Large Shark]: A sizable and powerful shark, possibly a Great White, with a grey and
white body. Its dorsal fin is tall and prominent, and it has a distinctly streamlined
shape. The shark conveys a sense of calm authority as it moves effortlessly through
the water.backrgound
Figure8: Morevisualizationofremovingvideoobjects
21
tupnI
CV
IVGL
sruO
tupnI
CV
IVGL
sruO[Snowboarder]: A person dressed in a dark snowsuit, possibly black or navy, with a
snowboard attached to their feet. They display confidence and skill as they
navigate down the slope, executing moves and jumps.backrgound
[Giant Panda]: A large, black and white panda with a distinct, dense coat. It is
shown playfully climbing and balancing on the wooden beams of its enclosure. It has
a round face, large black eye patches, and is quite agile for its size.backrgound
Figure9: Morevisualizationofremovingvideoobjects
22
tupnI
CV
IVGL
sruO
tupnI
CV
IVGL
sruO+ [Surfer]: An athlete wearing a dark wetsuit, possibly black or navy, showcasing
talent in balancing and steering on the waves. Their stance is wide and steady, knees
bent, arms outstretched for balance, and their posture exuding confidence.
+ [Man in Red Top]: The subject is a fit male wearing a bright red short-sleeved
shirt, navy blue athletic shorts, and black sports shoes. He displays athleticism and
coordination as he skillfully plays with the football.
Figure10: Morevisualizationofaddingvideoobjects
23
tupnI
CV
A-I
sruO
tupnI
CV
A-I
sruO+ [Large Shark]: A sizable and powerful shark, possibly a Great White, with a grey and
white body. Its dorsal fin is tall and prominent, and it has a distinctly streamlined shape.
The shark conveys a sense of calm authority as it moves effortlessly through the water.
+ [Basketball Player]: Athlete wearing a white basketball jersey with a prominent star
design, royal blue basketball shorts, and white sports sneakers. The player is focused on
performing dribbling exercises, showcasing control and agility.
Figure11: Morevisualizationofaddingvideoobjects
24
tupnI
CV
A-I
sruO
tupnI
CV
A-I
sruO+ [Seagull]: A white seagull with wings spread wide, showcasing gray tips. As it flies, its
yellow beak and black eye markings are noticeable, adding to its distinctive features. The
bird's underbelly is white, reflecting sunlight, while the light and smooth lines of its
body exemplify a strong yet graceful figure in mid-flight.
+ [Brown Dog]: An enthusiastic medium-sized brown dog with glossy fur and a lean
build. It remains undeterred by the splashes, focused on the man and possibly a tossed
object or on the interaction itself. Its tail is partially submerged, blending into the
blue water, and it moves swiftly, demonstrating agility and the enjoyment of water.
Figure12: Morevisualizationofaddingvideoobjects
25
tupnI
CV
A-I
sruO
tupnI
CV
A-I
sruO[Large Shark] [Orange Shark]: A sizable and powerful shark, possibly a Great White, with a grey
and white orange body. Its dorsal fin is tall and prominent, and it has a distinctly streamlined
shape. The shark conveys a sense of calm authority as it moves effortlessly through the water.
[Man] [Man in orange]: An athletic man wearing an orange shirt, black white shorts, and sports
shoes. He is running on a well-maintained playing area characterized by manicured green grass, a
smooth outfield, and a perimeter wall adorned with sponsorship banners and a scoreboard.
Figure13: Morevisualizationofeditingvideoobjects
26
tupnI
CV
FT
sruO
tupnI
CV
FT
sruO[Player 1 with Red Hat] [Player 1 with Blue Hat]: Male player on the near side of the court.
He is wearing a white shirt, dark shorts, and a red blue hat, and is playing with a yellow tennis
racket. He serves and returns the ball, demonstrating agility and competitive spirit in the game.
[Person]: Clad in a dark-colored jacket colorful, rainbow-striped jacket and pants, the person
assumes a casual yet attentive posture while walking the dog, indicative of a routine stroll.
Figure14: Morevisualizationofeditingvideoobjects
27
tupnI
CV
FT
sruO
tupnI
CV
FT
sruO[Baby Gorilla] [Baby Panda]: A young gorilla panda with black fur, portrayed in the act of
crawling. The gorilla's movement is slow and cautious, exhibiting natural curiosity as it explores
its surroundings.
[Dalmatian] [Dog]: A medium-sized, Dalmatian dog with white facial markings, a white chest,
and white-tipped paws. It has floppy ears, a long tail, and a gentle expression. The dog is
actively engaging with a ball, using its front paws and nose, displaying playful behavior.
Figure15: Morevisualizationofeditingvideoobjects
28
tupnI
CV
FT
sruO
tupnI
CV
FT
sruO[Snowboarder]: A person dressed in a dark snowsuit, possibly black or navy, with a
snowboard attached to their feet. They display confidence and skill as they
navigate down the slope, executing moves and jumps.backrgound
[Giant Panda]: A large, black and white panda with a distinct, dense coat. It is
shown playfully climbing and balancing on the wooden beams of its enclosure. It has
a round face, large black eye patches, and is quite agile for its size.backrgound
Figure16: Morevisualizationofeditingvideoobjects
29
elcarO
.w
ledoM
.w
elcarO
.w
ledoM
.w
tupnI
ksaM
ksaM
tupnI
ksaM
ksaM[Snowboarder]: A person dressed in a dark snowsuit, possibly black or navy, with a
snowboard attached to their feet. They display confidence and skill as they
navigate down the slope, executing moves and jumps.backrgound
[Giant Panda]: A large, black and white panda with a distinct, dense coat. It is
shown playfully climbing and balancing on the wooden beams of its enclosure. It has
a round face, large black eye patches, and is quite agile for its size.backrgound
Figure17: Morevisualizationofeditingvideoobjects
30
elcarO
.w
ledoM
.w
elcarO
.w
ledoM
.w
tupnI
ksaM
ksaM
tupnI
ksaM
ksaM[Baby Gorilla] [Baby Panda]: A young gorilla panda with black fur, portrayed in the act of
crawling. The gorilla's movement is slow and cautious, exhibiting natural curiosity as it explores
its surroundings.
[Brown Dog] [Raccoon]: A medium-sized, light brown dog raccoon with a sturdy build and
alert ears. Its tail wags enthusiastically as it leads the playful chase in the forest, displaying
a sense of happiness and energy.
Figure18: Morevisualizationofeditingvideoobjects
31
elcarO
.w
ledoM
.w
elcarO
.w
ledoM
.w
tupnI
ksaM
ksaM
tupnI
ksaM
ksaM[Large Shark] [Orange Shark]: A sizable and powerful shark, possibly a Great White, with a grey
and white orange body. Its dorsal fin is tall and prominent, and it has a distinctly streamlined
shape. The shark conveys a sense of calm authority as it moves effortlessly through the water.
[Woman]: Dressed in a casual outfit of blue denim shorts and a horizontally striped blue and
white T-shirt a dark red dinning dress. She accessorizes with pink flip-flops and is engaged
in the action of putting a leash on her dog, showing signs of preparing for a walk.
Figure19: Morevisualizationofeditingvideoobjects
32
elcarO
.w
ledoM
.w
elcarO
.w
ledoM
.w
tupnI
ksaM
ksaM
tupnI
ksaM
ksaM+ [Surfer]: An athlete wearing a dark wetsuit, possibly black or navy, showcasing
talent in balancing and steering on the waves. Their stance is wide and steady, knees
bent, arms outstretched for balance, and their posture exuding confidence.
+ [Giant Panda]: A large, black and white panda with a distinct, dense coat. It is
shown playfully climbing and balancing on the wooden beams of its enclosure. It has
a round face, large black eye patches, and is quite agile for its size.
Figure20: Morevisualizationofeditingvideoobjects
33
elcarO
.w
ledoM
.w
elcarO
.w
ledoM
.w
tupnI
tupnI
gninnalP
gninnalP
gninnalP
gninnalP+ [Skier in Blue and Green]: A skier wearing a blue jacket with green sleeves and blue
pants. They're equipped with skiing poles and skis that appear to be primarily white with
some red signage. Their helmet is a bright neon lime green with matching goggles
+ [Climber]: A woman in athletic wear, featuring a black tank top and matching
shorts, with her hair tied back. She shows focus and precision in her bouldering
technique.
Figure21: Morevisualizationofeditingvideoobjects
34
elcarO
.w
ledoM
.w
elcarO
.w
ledoM
.w
tupnI
tupnI
gninnalP
gninnalP
gninnalP
gninnalP