Explicit Formulae to Interchangeably use Hyperplanes
and Hyperballs using Inversive Geometry
Erik Thordsen erik.thordsen@tu-dortmund.de
Fakultät für Informatik, Data Mining
TU Dortmund University
Erich Schubert erich.schubert@tu-dortmund.de
Fakultät für Informatik, Data Mining
TU Dortmund University
Abstract
Many algorithms require discriminative boundaries, such as separating hyperplanes or hy-
perballs,orarespecificallydesignedtoworkonsphericaldata. Byapplyinginversivegeom-
etry, weshowthatthetwodiscriminativeboundariescanbeusedinterchangeably, andthat
general Euclidean data can be transformed into spherical data, whenever a change in point
distances is acceptable. We provide explicit formulae to embed general Euclidean data into
spherical data and to unembed it back. We further show a duality between hyperspherical
caps, i.e., the volume created by a separating hyperplane on spherical data, and hyperballs
and provide explicit formulae to map between the two. We further provide equations to
translate inner products and Euclidean distances between the two spaces, to avoid explicit
embedding and unembedding. We also provide a method to enforce projections of the gen-
eral Euclidean space onto hemi-hyperspheres and propose an intrinsic dimensionality based
method to obtain “all-purpose” parameters. To show the usefulness of the cap-ball-duality,
we discuss example applications in machine learning and vector similarity search.
1 Introduction
Somepracticalapplicationsinmachinelearningarespecificallydesignedtoworkonsphericalornon-spherical
data, such as HIOB by Thordsen & Schubert (2023). Extending these algorithms to non-spherical data can
requirealotofeffort. Inothercases,algorithmsarespecificallydesignedtoworkwithseparatinghyperplanes,
when one would perhaps desire to use hyperballs instead, such as SVMs which have been extended to use
balls for the Support Vector Data Description by Tax & Duin (2004). Again, redesigning the algorithms
can be quite difficult if not nigh impossible. In this paper, we will provide concise formulae to eliminate
both of these issues – at least if a locally-linear change in point distances is acceptable. We propose the use
of inversive geometry to transfer general Euclidean datasets into spherical data and provide the necessary
formulaetojumpbetweenhyperballsinthegeneralEuclideanandhyperplanesinthesphericalspace. Since
our motivation is primarily to make Euclidean spaces spherical (in order to be able to use machine learning
algorithms designed for spherical data), we will call the general Euclidean space the “original space” and
the spherical space the “embedding space”. While most of the proposed ideas have been well known for
low-dimensional spaces in the field of inversive geometry for a long time, dating back to the early 19th
century (Coolidge, 1947, p. 279), to our knowledge, no one has yet provided explicit formulae for the
embedding and unembedding functions for arbitrary-dimensional spaces. Neither did we find the necessary
formulae to map hyperballs to hyperplanes and vice versa and to translate inner products and Euclidean
distances between the two spaces. Most literature on inversive geometry is limited to three dimensions,
due to its usefulness in visualization techniques such as geographic maps. While it is often easy to show
that some surface type is mapped to another surface type, giving the exact values for ball centers and
radii requires quite exhausting algebraic transformations. Today’s machine learning methods, however,
often involve high-dimensional embeddings, and much of the computational improvements are focused on
1
4202
yaM
82
]GL.sc[
1v10481.5042:viXraFigure 1: Visualization of the (inverse) stereographic projection from one- and two-dimensional data to
two- and three-dimensional spheres. By offsetting the data to a plane in an additional dimension (blue) and
inverting the vector norms, i.e., mirroring each point with the unit sphere (green) around the origin (black),
we obtain a spherical distribution (orange).
optimizing the computation of inner products. This paper provides equations useful to the practically
inclined machine learning researcher, who can now with ease explore the possibilities of using spherical data
in their algorithms.
We provide formulae to “move” between general Euclidean and spherical spaces of one additional dimension
using arbitarily-dimensional stereographic projections. The (inverse) stereographic projection used for that
process is visualized in Figure 1, where the poles are plotted as arrows. We show that hyperspherical caps
given specific poles get mapped to hyperballs in the general Euclidean space, for which we provide explicit
formulaeforcenterandradius. Wealsoprovidetheinversemapandequationstotranslateinnerproductsand
Euclidean distances between the two spaces. We further show, that the result can be extended to arbitrary
poles, resulting in a map between hyperspherical caps and a specific subclass of hyperellipsoids. We provide
parameters to enforce projections of the general Euclidean space onto hemi-hyperspheres and propose an
intrinsic-dimensionality-basedmethodtoobtain“all-purpose”parameters. Toshowcasetheusefulnessofour
results, we include examples of how to use the provided methods in practical settings.
2 Hyperspherical Embeddings
Assuming an arbitrarily indexed dataset X ⊂ Rd, a unit-length inversion direction vector v ∈ Rd+1, and a
scale factor s∈R . We define the affine hyperplane embedding of X under v and s as
>0
(cid:20) s−XTv (cid:21)
H (X,v,s)7→ X, 1,...,d
A v
d+1
where [X,y] is the set of vectors x ∈ X extended by one dimension with coefficients y ∈ R. Above and
i i
in the rest of this text, formulae that involve sets of vectors are per-vector operations. One can easily see,
that for any vector x′ in H (X,v,s) holds ⟨x′,v⟩ = s, i.e., that all points in H (X,v,s) lie on an at least
A A
one-codimensional hyperplane. v is a normal vector of the hyperplane and whenever s̸=0, the hyperplane
is affine. Using the affine hyperplane embedding, we can define the affine inversion sphere embedding
S (X,v,s)⊂Rd+1 as
A
H (X,v,s)
S (X,v,s)7→ A
A ∥H (X,v,s)∥2
A
As per inversion theory, all points in S (X,v,s) lie on a hypersphere that includes the origin in Rd+1 and
A
whosecenterliesindirectionv. Thefurthestpointtotheoriginindirectionv ispreciselytheinvertedimage
2of sv, i.e., v. The center, thereby, is given as v and the radius is 1 . The definition of the affine inversion
s 2s 2s
sphereembeddingisanalogous totheinverse stereographic projectionin higherdimensionswhere v denotes
s
the north pole. Accordingly, we can define the inversion-based spherical embedding S(X,v,s) as
(cid:16) v (cid:17)
S(X,v,s)7→2s S (X,v,s)− =2sS (X,v,s)−v
A 2s A
All vectors in S(X,v,s) then must lie on a unit sphere around the origin in Rd+1. The choice of v and
s both affect how the data is distributed on the resulting unit sphere. Very small values in s move the
affine hyperplane closer to the origin, resulting in points clustering around −v, whereas very large values
result in points clustering around v. A more even spread of the data over the entire sphere is desirable in
most cases. One can find such an s by grid search over a reasonable interval of s values and investigate,
e.g.,theAngle-BasedIntrinsicDimensionality(ABID,Thordsen&Schubert,2022)oftheresultingspherical
distribution. In case X is very large, this can be estimated from a sample of X. Mean-centering X in
advance and scaling X to a mean vector norm of 1 typically results in a robust choice of s within [10−3,103]
in which an exponentially spaced grid with about 50 steps should find a suitable value. We can give the
inverse of S(X,v,s) as
!
Y +v
S−1(Y,v,s)=2s
∥Y +v∥2
1,...,d
where (.) denotes dropping the extra dimension d+1, such that S−1(S(X,v,s),v,s)=X.
1,...,d
3 Embedded Hyperspherical Caps and Hyperballs
In this section, we first focus on the simpler case where v =(0,...,0,1), which clearly is a useful choice and
preferred for applications where no other constraints have to be satisfied. For that v, the hyperspherical cap
resulting from intersecting a hyperplane with S(X,v,s) corresponds to a hyperball in the original space of
X whenever −v is not included in the cap. It also simplifies the (un-)embedding functions to
[X,s] Y
Se(X,v,s)=2s −v and Se−1(Y,v,s)=s 1,...,d .
∥X∥2+s2 1+Y
d+1
Toprove therelationbetween hypersphericalcaps andhyperballs, wefirstintroducethenecessarynotation.
Definition 1 (Hypherspherical Caps) The openhypersphericalcapC ofadirectionalunit-lengthvector
p∈Rd and a bias b∈(0,1) is the set of points on the unit sphere, whose dot product with p is greater than b,
C(p,b):={x∈S |⟨x,p⟩>b} .
d−1
The separating hyperplane has normal vector p and bias b. The closed hyperspherical cap is defined analo-
gously with ⟨x,p⟩≥b. The boundary of the cap is the difference between the closed and the open cap.
Definition 2 (Hyperball) The open hyperball B of a center c ∈ Rd and a radius r ∈ R is the set of
>0
points whose Euclidean distance to c is less than r, i.e.,
B(c,r):=(cid:8) x∈Rd |∥x−c∥<r(cid:9) .
The closedhyperballisdefinedanalogouslywith∥x−c∥≤r. The boundaryofthehyperballisthedifference
between the closed and the open cap.
Wecannowprovethedualityofhypersphericalcapsinthesphericalembeddingspaceandhyperballsinthe
original space. It is, again, important that −v is not included in the cap, i.e., that the directional vector p
and bias b of the cap satisfy b+p
d+1
> 0 and consequentially ⟨v,p⟩ < b, since Se−1(−v,v,s) is ill-defined
and corresponds to inifinity in all possible directions in the original space. From inversive geometry, it is
well known, that hyperspheres that do not contain the center of inversion get mapped to hyperspheres, e.g.,
3Theorem 3.4 of Lee (2020). Since we shift the center of the hyperball by v, the center of inversion ends
up at −v in the projected space. The boundaries of the cap is a hypersphere, thus when the cap does not
intersect−v, theboundaryintheoriginalspaceisalsoahypersphere. When−v iscontainedinthecap, the
“interior”ofthecapgetsprojectedtothe“outerior”ofthehypersphereinoriginalspace,i.e. thecomplement
of the contained hyperball. Conversely, if −v is neither on the boundary nor inside the cap, the cap gets
projected to a hyperball in the original space. Since all points in H (X,v,s) share the same (d+1)-th
A
coefficient, cutting that dimension does not affect the resulting hyperball. While proving the existence of
theresultinghyperballisrathertrivial, findingtheexplicitvaluesforcenterandradiusisnot. Theprooffor
theCap-Ball-Dualityisorientedalongtheboundaryofthecapandball, i.e., thehypersphereofintersection
of the unit d-sphere S and the hyperplane defined by p and b and the hypersphere around c with radius r
d
in the original space. The formulae for the center and the radius are not derived directly, but are based on
the intuition that the image under S must be shifted towards v to obtain the cap direction vector. Using
that intuition, exact values can be derived, resulting in a closed form existence proof.
Theorem 1 (Cap-Ball-Duality) For arbitrary p∈S and b∈(−1,1) with b+p >0, the image under
d d+1
S−1(_,v,s) of the hyperspherical cap C(p,b) is a hyperball in Rd whenever v = (0,...,0,1) ∈ Rd+1 and
s∈R . The center and radius of the hyperball are
>0
s
(cid:18) p−αv (cid:19) 2α 1−b2
c=S−1 ,v,s , and r =s , where α:= .
∥p−αv∥ b+p 2(b+p )
d+1 d+1
Proof 1 The proof is based on showing that points on the boundary of the cap are projected to points on the
boundary of the ball. For that, we can choose an arbitrary point on the boundary, start with the distance of
its projection under S−1 to the center of the ball, and show that it equals the radius of the ball specified in
the Theorem. By continuity, all points inside/outside the cap get projected to points inside/outside the ball.
The full proof can be found in the Appendix, since the derivation is quite technical and lengthy.
With the simplifying choice of v =(0,...,0,1), we can give an alternative representation of c, that does not
include S or S−1 as
p
c = s 1,...,d . (1)
q
1−p2 +(p −α)2+p −α
d+1 d+1 d+1
This “simplified” equation highlights that the center c of the hyperball is collinear to the first d coefficients
of the normal vector p of the cap. With that observation, we can choose β such that βc = p and derive
d+1
the opposite direction to find a hyperspherical cap in the embedding space given a hyperball.
Theorem 2 (Cap-Ball-Duality 2) For arbitrary center c ∈ Rd and radius r ∈ R , the image under
>0
S(_,v,s) of the hyperball B(c,r) is a hyperspherical cap on S whenever v = (0,...,0,1) ∈ Rd+1 and
d
s∈R . The direction p and bias b of the hyperspherical cap are
>0
q q
(cid:18) q (cid:19) s s2+β2∥c∥2r2−r2 1−β2∥c∥2
p= βc, 1−β2∥c∥2 , and b= ,
r2+s2
2s
where β := .
q
(∥c∥2+r2+s2)2−4∥c∥2r2
Proof 2 The proof follows from inverting the equations in Theorem 1 and inserting the chosen definitions
here to derive the proposed value of β. The full proof can again be found in the Appendix.
Using the two theorems, we can derive some auxiliary measures, that in applications will likely be of use.
First, we can show that for s≥max ∥x∥ all points will be projected to the (closed) hemi-hypersphere in
x∈X
direction v.
4Corollary 3 (Hemispherical Embedding) Given indexed data x ∈ X ⊂ Rd and v = (0,...,0,1) ∈
i
Rd+1. For any large enough scale s ≥ max ∥x∥, S(X,v,s) lies on the closed hemisphere in direction v,
x∈X
i.e., ∀xˆ∈S(X,v,s): ⟨xˆ,v⟩≥0. For any s>max ∥x∥, all embedded vectors lie on the open hemisphere.
x∈X
Proof 3 Choosing p := v and b := 0, we obtain a hyperball with center at the origin and radius s. Iff
s ≥ max ∥x∥, then ⟨x,v⟩ ≥ 0 for all x ∈ X, i.e., all points in S(X,v,s) lie on the closed hemi-
x∈X
hypersphere in direction v. Iff s > max ∥x∥, then ⟨x,v⟩ > 0 for all x ∈ X, i.e., all points in S(X,v,s)
x∈X
lie on the open hemi-hypersphere in direction v.
Next,wecangiveclosedformsfortheinnerproductandsquaredEuclideandistanceafter(un-)embeddingin
termsofthesamemeasuresintheoriginalspace. Thisopenstheembeddinguptokernel-basedapplications,
where the spherical embedding is implicitly applied to the kernel space and for corrections in distance
computations to account for the non-isometry of the embedding. The distance equations can also be used
to compute distances to, e.g., the hyperball centers implicitly in the embedding space.
Corollary 4 Given any s ∈ R and assuming v = (0,...,0,1) ∈ Rd+1. Let x,y ∈ Rd and xˆ =
>0
S(x,v,s),yˆ=S(x,v,s), then the following equations hold:
⟨x,x⟩−2⟨x,y⟩+⟨y,y⟩ 2s2∥x−y∥2
⟨xˆ,yˆ⟩=1−2s2 =1− (2)
(⟨x,x⟩+s2)(⟨y,y⟩+s2) (∥x∥2+s2)(∥y∥2+s2)
4s2∥x−y∥2
∥xˆ−yˆ∥2 = (3)
(∥x∥2+s2)(∥y∥2+s2)
⟨xˆ,yˆ⟩−xˆ yˆ ⟨xˆ,yˆ⟩−⟨xˆ,v⟩⟨yˆ,v⟩
⟨x,y⟩=s2 d+1 d+1 =s2 (4)
(1+xˆ )(1+yˆ ) (1+⟨xˆ,v⟩)(1+⟨yˆ,v⟩)
d+1 d+1
s2
∥x−y∥2 = ∥xˆ−yˆ∥2 (5)
(1+xˆ )(1+yˆ )
d+1 d+1
4s2
= ∥xˆ−yˆ∥2 (6)
(4−∥xˆ−v∥2)(4−∥yˆ−v∥2)
Proof 4 The proofs consist of inserting the definitions and simplifying the resulting equations. The deriva-
tions are given in the Appendix.
For moving from the spherical to the Euclidean space, we provide two formulae each, one using the last
coordinate and one using the direction vector. The formulae using the last coordinate is computationally
moreefficient,yet,theotherformulaeareabetterfitforkernel-basedapplications. Thelatterinconjunction
with Theorem 1 also provide exact solutions for arbitrary v when not discarding the last coordinate upon
unembedding. When discarding the last coordinate, as defined in S−1(_,v,s), the resulting balls – already
lying on a hyperplane orthogonal to v – get linearly projected to a slanted hyperplane – orthogonal to
(0,...,0,1) – resulting in one of the dimensions getting “squished”.
4 Embedded Hyperspherical Caps and Hyperellipsoids
For embedding directions v other than (0,...,0,1), the inverse image of a hyperspherical cap under
S−1(_,v,s) is an ellipsoid, with all but one identical radii (semi-axes). The single other radius is smaller
and its axis is collinear to v .
1,...,d
Theorem 5 (Cap-Ellipsoid-Duality) For arbitrary p,v ∈ Rd+1 and b ∈ R with ∥v∥ = 1 and b +
≥0
⟨p,v⟩>0, the image under S−1(_,v,s) of the hyperspherical cap C(p,b) is a hyperellipsoid in Rd whenever
v ̸=(0,...,0,1) and s∈R . The center of the hyperellipsoid is
>0
(cid:18) p−αv (cid:19) 1−b2
c=S−1 ,v,s where α:= .
∥p−αv∥ 2(b+⟨p,v⟩)
5(0,...,0,1)
d+1
v v
d+1
θ
j
π−θ
2
θ
v
sv 1,...,d
·
i
Figure 2: A visual proof that the radius in direction v is the only one varying from the radius in
1,...,d
the spherical case and that its scaling factor is v . i and j indicate arbitrary basis vectors orthogonal to
d+1
(0,...,0,1), not necessarily parallel to coordinate axes. The cosine of θ equals v .
d+1
The smallest radius and the corresponding direction are
s s
(1−b2)v2 (1−b2) v
r =s d+1 =s |v |, and a = 1,...,d
1 (b+⟨p,v⟩)2 (b+⟨p,v⟩)2 d+1 1 ∥v ∥
1,...,d
and the radius in all directions a orthogonal to a is
2 1
s
1−b2
r =s .
2 (b+⟨p,v⟩)2
Proof 5 The proof follows from Theorem 1, since the hyperspherical cap C(p,b) is the image under S of
the hyperball B(c′,r) in H (Rd,v,s) where the first d coefficients of c′ equal c. The hyperplane on which all
A
points in H (Rd,v,s) live is orthogonal to v and the final step of the inversive projection is to discard the
A
last coefficient of the embedded vectors, which can be represented as a linear projection on the hyperplane
with normal vector (0,...,0,1). The cosine between these two hyperplanes is ⟨v,(0,...,0,1)⟩=v and all
d+1
directions orthogonal to v are shared between the two hyperplanes. Accordingly, the radius in the direction
of v has to be scaled by the cosine between the two hyperplanes v , while the radius in all other directions
d+1
remains unchanged. A visual proof is given in Figure 2.
The Cap-Ball-Duality can then be interpreted as the limit case of the Cap-Ellipsoid-Duality, where the
difference between the “squished” and the other dimensions vanishes and the “squished” axis therefore
becomes arbitrary and ill-defined. We neither provide formulae for the inverse direction of the duality, nor
for dot products, and squared distances for the Cap-Ellipsoid-Duality. Whilst those functions should have a
closed form, they are likely very large and cover a much less useful case of ellipsoids with exactly one axis of
different length. The inclined reader can likely guess appropriate functions exploiting the observation, that
the original space is a slanted view of the spherical case and distances are compressed along the “squished”
axis, but proofing these guessed functions is certainly cumbersome. Otherwise, one can avoid the projection
to one less dimension in the unembedded space and use the Cap-Ball-Duality instead. When moving from
spherical data to a non-spherical representation, that may be a sufficient representation and the formulae
for products and distances for the Cap-Ball-Duality apply. Since the unembedding scales the space along
theembeddingdirection,itshouldbepossibletochainmultipleembeddingstoachieveanequivalentduality
between general ellipsoids and hyperspherical caps.
Hypothesis 6 (Generalized Cap-Ellipsoid-Duality) For arbitrary ellipsoids E ⊆ Rd with axes
a ,...,a andradiir ,...,r suchthatr ≤r foralli∈{1,...,d−1}, thereexistv ,...,v ,s ,...,s ,
1 d 1 d i d 1 d−1 1 d−1
6and b such that the repeated embedding of E is the hyperspherical cap C(v ,b), i.e.,
d−1
S(...(S(E,v ,s ),...),v ,s )=C(v ,b).
1 1 d−1 d−1 d−1
Further, a =(v ) for all i and a closed form for the center exists.
i i 1,...,d
5 Parameter Choice
To embed the data with the proposed embedding vector v =(0,...,0,1), we still need to choose the scaling
parameters. Thechoiceofthatparametercontrolsthedistributionofsamplesontheresultinghypersphere.
For infinitesimal s, the samples cluster around −v whereas for infinite s, the samples cluster around v.
Ideallyformostdown-streamapplications, thesampleswould“evenlydistribute”overthesphereasto, e.g.,
not provoke numerical instabilities. As a criterion for a good spread of the data, we propose to consider the
intrinsic dimensionality of the embedded data. The intrinsic dimensionality of the data is the number of
dimensions that are necessary to represent the space populated by the data. We consider that measure as a
good criterion, since it assigns the same value to the worst case on both ends of the value space of s and a
higher value everywhere inbetween. Intuitively, targeting a high intrinsic dimensionality can be motivated
bybestpreservingthedatacomplexity, asareductioninIDindicatesalossofstructuralinformation. Ifthe
entire dataset collapses to v or −v on the sphere, the intrinsic dimensionality should be minimum. For any
point inbetween, the intrinsic dimensionality should be larger with a likely maximum hopefully close to the
intrinsicdimensionalityoftheoriginalspace. Weexpectthatmaximumtoprovidethebestspreadingofthe
dataoverthesphereandthereforebethebestchoicefors. Sincetheembeddeddatawilllieonasphere, we
propose to employ the ABID estimator of Thordsen & Schubert (2022), which considers the pairwise cosine
distribution of samples. The ABID estimator is typically applied in a local manner, i.e., evaluated over a
small neighborhood around each point in the sample, commonly using the k-nearest neighbors. We here
instead suggest to use it as a global measure of intrinsic dimensionality, by aggregating it over the entire
(embedded) dataset. That implies that we consider the dataset to populate a linear subspace of the original
space. We thereby do not explicitly attempt to cover the local structure of the data, but rather the global
structure. The ABID estimator of Thordsen & Schubert (2023) is defined as
ABID(X)=E (cid:2) cos(x,y)2(cid:3)−1 (7)
x,y∈X
We are then interested in the value of s that maximizes the ABID estimate evaluated over the embedded
samples. We expect the ABID of embedded samples to be upper bounded by the ABID value on the
unembedded space plus one. We will later provide empirical evidence that these values can be expected
to be identical. In the unembedded space, we propose to mean-center the dataset, such that the cosines
are computed relative to the mean of the dataset. This should provide an as-large-as-possible value for the
ABID estimate, since if all points were shifted far away from the origin, all cosines would be maximized.
The data would then be almost indistinguishable from a single point. In the embedded space, the origin,
i.e., the center of the sphere, will be the point relative to which the cosines are computed. That results in
minimum values of 1 for the ABID estimate, since the center of the sphere virtually becomes part of the
dataset, which then lies on the line through the origin in direction of v. Solving the criteria for an optimal s
analytically, however, requires intricate knowledge of the distribution of the dataset, which we would rather
avoid, since it limits the parameter choice to specific distributions. Instead, we perform a parameter sweep
for s over a reasonable range. We propose to center this range on the mean vector norm of the samples in
the original space. Given two samples x and y and their respective embedded points x and y, the ratio of
b b
cosines prior and posterior to embedding equates
∥x∥2+∥y∥2−∥x−y∥2
cos(x,y)
= 2∥x∥∥y∥
cos(x b,y b) 2−∥ bx−by∥2
2
7Uniform ball Univariate normal MNIST
1 1 1
X) X) X)
S( 0.5 S( 0.5 S( 0.5
s
of
0
s
of
0
s
of
0
e e e
n n n
si−0.5 si−0.5 si−0.5
o o o
C C C
−1 −1 −1
−1 −0.5 0 0.5 1 −1 −0.5 0 0.5 1 −1 −0.5 0 0.5 1
Cosines of X Cosines of X Cosines of X
Figure3: Pairwisecosinesbeforeandaftersphericalembeddingofmean-centereddata. Fromeachdistribu-
tion we drew 500 samples. For the generated datasets we used 10 dimensional distributions. The visualized
cosines belong to 10k random sample pairs. The blue line indicates the identity function.
Which when inserting the solution for ∥x−y∥2 reduces to
b b
∥x∥2+∥y∥2−∥x−y∥2
=
(cid:18) (cid:19)
2−
4s2∥x−y∥2
∥x∥∥y∥
(∥x∥2+s2)(∥y∥2+s2)
(cid:16) (cid:17)(cid:16) (cid:17)(cid:16) (cid:17)
∥x∥2+∥y∥2−∥x−y∥2 ∥x∥2+s2 ∥y∥2+s2
=
(cid:18) (cid:16) (cid:17)(cid:16) (cid:17) (cid:19)
2 ∥x∥2+s2 ∥y∥2+s2 −4s2∥x−y∥2 ∥x∥∥y∥
The distribution of that term clearly depends on the distribution of the samples in the original space. If we
assert s to be the mean vector norm of the samples and approximate ∥x∥ and ∥y∥ by the same value, the
term further simplifies to
(cid:16) s2+s2−∥x−y∥2(cid:17)(cid:0) s2+s2(cid:1)(cid:0) s2+s2(cid:1)
∥x∥,∥y∥→s
= (cid:16) (cid:17)
2(s2+s2)(s2+s2)−4s2∥x−y∥2 s2
(cid:16) (cid:17)
2s2−∥x−y∥2 4s4
= =1
(cid:16) (cid:17)
8s4−4s2∥x−y∥2 s2
So as all vector norms approach s, the ratio of all cosines approaches 1 as well. Accordingly, in the limit,
the distribution of cosines in the embedded space should be identical to the distribution of cosines in the
unembedded space and thereby the ABID estimate should be identical. The limit case is of course likely
not observed, whereby that choice of s is only approximately optimal. In our experiments, however, this
choice for s was always close to the optimal value and off by less than a multiple of 10. Accordingly, a
logarithmically spaced grid from 0.1 to 10 times the mean vector norm of the samples in the original space
should be sufficient to cover the optimal value of s and a small number of grid values, e.g. 20, should suffice
to find a suitable value. In case of outliers with a very large vector norm, the initial value can be adjusted
to the median vector norm. As can be seen in Figure 4, the ABID estimates are capped at one plus the
dimensionality of the original space for synthetic datasets. The corresponding pairwise cosines before and
after embedding are visualized in Figure 3 and approximate the identity function.
6 Relevance to Machine Learning
The Cap-Ball-Duality provides an algorithmic solution to apply sphere-based algorithms to general Eu-
clidean space and to translate between separating hyperspheres and hyperplanes. The former allows to
expand spherical algorithms to general Euclidean space, such as ScaNN of Guo et al. (2020) or HIOB of
Thordsen&Schubert(2023),andtosolveball-basedproblemswithhyperplanes,suchastheminimalbound-
ing sphere problem. The latter allows to exchange the separating geometry, e.g., in support vector machines
8Uniform ball Univariate normal MNIST
30
10 10
D D D20
BI BI BI
A 5 A 5 A
10
0.01 0.1 1 10 0.1 1 10 100 100 1000 10k 100k
s s s
Figure 4: ABID estimates over spherical embeddings with varying s parameter. From each distribution we
drew 500 samples. For the generated datasets we used 10 dimensional distributions. The horizontal black
line indicates the ABID estimate of the original dataset plus one. The red vertical line indicates the mean
vector norm. The blue vertical lines indicate 0.1 and 10 times the mean vector norm, respectively.
1 Gaussian 3 Gaussians
s) s)
344 33
m m 1.5
1 1
n.
(
2 n.
(
1
u u
c.
f
c.
f
0.5
e e
D
d0
D
d
0
D D
V V−0.5
S S
0 1 2 3 4 0.9 1 1.1 1.2 1.3
spherical SVM dec. fun. (583ms) spherical SVM dec. fun. (339ms)
Figure 5: The decision function values for SVDDs and SVMs fitted to the original and embedded data,
respectively. Left shows the results for a Gaussian distribution in 10 dimensions, right for three Gaussian
blobs in 10 dimensions. The datasets of 5000 points each were iteratively shifted to the center of the SVM
induced sphere to obtain an equal shift in distances in all directions. The shifting process is included in the
displayed fitting times.
or spatial indexes, to work with balls instead of hyperplanes and vice-versa. The results from these adap-
tations are increasingly approximate for small biases/large radii, but allow for a rapid prototyping or cheap
approximation of algorithms in the respective other geometry.
Spherical Embedding Baseline Training Loss
1
5 Sph. Emb.
1 1 2 Baseline
ss 0.1
o
L 5
E
0 0 C 2
0.01
5
2
−1 −1
0 1000 2000 3000 4000
−1 0 1 2 −1 0 1 2 Epochs
Figure6: Thedecisionboundariesandtraininglossesforneuralnetworkstrainedwithsphericalembedding
and ReLU as first activation, respectively. The baseline network consisted of three fully connected layers,
with shapes 2×2, 2×16 and 16×2 with ReLU activations inbetween. The spherical embedding network
used the S with a learned s as first activation and therefore had a 3×16 second layer.
9Figure 7: Recall of ScaNN and Annoy spatial indexes for spherical and non-spherical data before and after
(un)embedding. Highlighted recall values are the best for each dataset and index. Shaded cells indicate
values that we would have expected to improve over the “Direct” columns.
ScaNN 10@10-recall Annoy 10@10-recall
Dataset d Spherical
Direct Emb. Unemb. Direct Emb. Unemb.
Normalized Blobs 50 Yes 72.78% 72.96% 26.83% 35.70% 35.99% 41.79%
Normalized Blobs 100 Yes 80.95% 80.95% 26.73% 26.10% 26.38% 30.18%
GloVe 100 Yes 89.95% 90.08% 54.84% 7.13% 7.15% 16.84%
NY Times 256 Yes 81.11% 80.80% 61.44% 14.50% 14.82% 21.52%
Last.fm 64 Yes 87.93% 86.35% 32.32% 16.99% 18.44% 3.89%
Blobs 50 No 38.93% 70.04% 28.13% 41.54% 34.23% 40.88%
Blobs 100 No 37.38% 74.99% 25.58% 30.11% 27.25% 30.48%
Fashion MNIST 784 No 99.85% 75.69% 65.82% 85.59% 66.84% 65.57%
SIFT 128 No 98.39% 84.05% 55.90% 35.74% 26.64% 32.88%
UsingalinearSVMtoseparatesphericallyembeddeddatafrom−v,forexample,givesacheapapproximation
for the order of scores of the Support Vector Data Descriptor (Tax & Duin, 2004, SVDD) – an adaptation
of the SVM designed to fit a bounding sphere around the data. The strong correlation between these two
measures as well as the significant speedup is displayed in Figure 5. While the QP-solver-based SVDD
implementation1 required about 1.5 minutes for each of the tasks, the spherical embedding-based SVM
required only half a second to solve multiple SVM tasks for proper fitting. The results of the SVM and
SVDD are not identical, but the order of the scores is mostly preserved.
Similarly, the performance of spatial indexes for approximate nearest neighbor search can be improved by
enforcingthegeometryonwhichtheyworkbest. The(un)embeddingofcourseintroducesanapproximation
error, though we expect it to be small enough for larger datasets to not significantly impact the recall of
already approximate approaches. The ScaNN index (Guo et al., 2020), for example, works best for cosine
distance, since it is based on product quantization. By embedding the data onto a hypersphere, we can
approximately translate the Euclidean distances of non-spherical data into cosine distances. The Annoy
index by Bernhardsson2 is based on projections to random interpoint axes in the dataset. That approach
essentially quantizes the squared Euclidean distance and we expect it to work better on non-spherical data.
Table 7 shows the 10@10-recall of the ScaNN and Annoy indexes on various datasets with and without
spherical embedding. The datasets were mostly taken from the ANN-Benchmarks3 and toy datasets of
10 (normalized) gaussian blobs were added. Non-spherical datasets were mean centered in advance. The
sphericalembeddingswerecomputedwithsequaltothemeanvectornorm. Forthesphericalunembeddings
thedatasetswerefirstnormalized,potentiallylosinginformationonnon-sphericaldatasets,andthenrotated
such that the mean vector was aligned with v = (0,...,0,1). The rotation then ensured, that the mean
direction gets mapped to the origin of the unembedded space. The unembedding then used a constant s
value of 1. We did not tune any of the parameters of ScaNN and Annoy, but used constant parameters for
all runs, since we were only interested in the relative improvement due to (un)embedding the data. The
recallofScaNNandAnnoyimprovesformostofthedatasetssignificantly,wherewe(un)embeddedthedata
from a less appropriate into a more appropriate space. Exceptions were the Last.fm, Fashion MNIST and
SIFTdatasets. Unembeddingnon-sphericaldataorembeddingsphericaldatacanatbestimprovetherecall
slightly. Thus, the recall does not always improve and identifying cases in which spherical (un)embedding
is advantageous remains a matter of future research, but in the cases where an improvement was expected
and observed, it was quite significant.
The spherical embedding could further be applied in neural networks as an activation function, to virtually
replace the dot products with soft distance measures to learned centers. A visual example is given in
1https://github.com/iqiukp/SVDD-Python
2https://github.com/spotify/annoy
3https://ann-benchmarks.com/
10Figure6,wheretwo(almost)identicalnetworksaretrainedonthemoonsdatasetwithandwithoutspherical
embeddingasthefirstactivation. Notonlydoesthesphericalembeddingnetworkconvergefaster,itproduces
“softer” decision boundaries, since the decisions in original space are based on distances to virtual centers
rather than hyperplanes. To allow for a shift of the center of inversion, a linear layer was added before the
sphericalembedding. Forthesphericalembeddingnetwork,thislayerwasinitializedwiththeidentitymatrix
and zero bias, yet randomly for the baseline model, that used a ReLU activation instead of the embedding.
These examples are some inspirations as to where the results of this paper can be leveraged. Aside from
potentially improving the quality of approaches tailored to either spherical or non-spherical data in various
machinelearningtasks,thesphericalembeddingaddsalmostnocomputationalcost. Translatingthecapsto
balls or vice-versa after training, allows to skip the (un)embedding process after training, although existing
libraries need to include that shortcut programmatically.
7 Conclusion
We have proposed spherical (un)embedding of data using arbitrarily-dimensional spherical inversion and
introduced an explicit recommendation for the directional vector v and a selection scheme for the scale
parameter s of these embeddings. Using these (un)embedding functions allows to apply algorithms to
other domains or to translate data into a more appropriate domain, achieving improved execution time or
quality on outlier detection, classification, and spatial indexing. We have further given explicit formulae
to translate between hyperspherical caps in the spherically embedded space and balls in the original space.
This allows to reduce the computational overhead of the (un)embedding process after training by avoiding
the (un)embedding altogether and instead evaluating distances or dot products in the task domain. By
providing closed forms for inner products and distances in the respective other space using only functions of
theoriginalspace, itispossibletoapplytheseresultstokernel-basedapproachesaswell. Thisarticleproves
the relevant formulae for future research on methods that exploit the Cap-Ball-Duality in machine learning
research.
References
Julian Lowell Coolidge. A history of geometrical methods. 1947. URL https://api.semanticscholar.
org/CorpusID:119437065.
Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Acceler-
ating large-scale inference with anisotropic vector quantization. In International Conference on Machine
Learning, 2020. URL https://arxiv.org/abs/1908.10396.
Nam-Hoon Lee. Geometry: from isometries to special relativity. 2020. URL https://api.
semanticscholar.org/CorpusID:219025032.
David M. J. Tax and Robert P. W. Duin. Support vector data description. Mach. Learn., 54(1):
45–66, 2004. doi: 10.1023/B:MACH.0000008084.60811.49. URL https://doi.org/10.1023/B:MACH.
0000008084.60811.49.
Erik Thordsen and Erich Schubert. ABID: angle based intrinsic dimensionality - theory and analysis. Inf.
Syst., 108:101989, 2022. doi: 10.1016/j.is.2022.101989.
Erik Thordsen and Erich Schubert. An alternating optimization scheme for binary sketches for cosine
similaritysearch. InOscarPedreiraandVladimirEstivill-Castro(eds.),SimilaritySearchandApplications
- 16th International Conference, SISAP 2023, A Coruña, Spain, October 9-11, 2023, Proceedings, volume
14289 of Lecture Notes in Computer Science, pp. 41–55. Springer, 2023. doi: 10.1007/978-3-031-46994-7\
_4. URL https://doi.org/10.1007/978-3-031-46994-7_4.
11A Appendix
Proof 6 (Proof of Theorem 1) Let S(x) and S−1(x) be shorthand for Se(x,v,s) and Se−1(x,v,s), respec-
tively. Let p and b be the directional unit-length vector and bias of an arbitrary hyperspherical cap satisfying
b+p >0. We choose c and r as described above. Since r decreases for increasing b, a smaller cap results
d+1
in a smaller ball and vice versa. By continuity of S, S−1, and our definition of r, it suffices to show, that
the boundary of the cap and the ball are images of each other under S and S−1. We, therefore, assume an
(cid:13) (cid:13)
arbitrary x ∈ S
d
with ⟨x,p⟩ = b and intend to show that (cid:13)S−1(x)−c(cid:13) = r. We will make use of the fact,
that for such x holds ∥x+v∥=2(1+x ) since
d+1
∥x+v∥2 = ⟨x+v,x+v⟩=⟨x,x⟩+⟨v,v⟩+2⟨x,v⟩=2(1+x ).
d+1
Let cˆ:=S(c)= p−αv , then
∥p−αv∥
(cid:13) (cid:13)S−1(x)−S−1(cˆ)(cid:13)
(cid:13)
(cid:13) (cid:13)
(cid:13) ! ! (cid:13)
(cid:13) x+v cˆ+v (cid:13)
=(cid:13)2s −2s (cid:13)
(cid:13) ∥x+v∥2 ∥cˆ+v∥2 (cid:13)
(cid:13) 1,...,d 1,...,d(cid:13)
(cid:13) (cid:13)
(cid:13) ! ! (cid:13)
(cid:13) x cˆ (cid:13)
=(cid:13)2s −2s (cid:13)
(cid:13) ∥x+v∥2 ∥cˆ+v∥2 (cid:13)
(cid:13) 1,...,d 1,...,d(cid:13)
(cid:13) (cid:13)
(cid:13)(cid:18) x cˆ (cid:19) (cid:13)
=s(cid:13) − (cid:13)
(cid:13) 1+x 1+cˆ (cid:13)
(cid:13) d+1 d+1 1,...,d(cid:13)
v
=su u tXd x2 i + cˆ2 i −2 x icˆ i
(1+x )2 (1+cˆ )2 (1+x )(1+cˆ )
d+1 d+1 d+1 d+1
i=1
s
∥x ∥2 ∥cˆ ∥2 ⟨x ,cˆ ⟩
=s 1,...,d + 1,...,d −2 1,...,d 1,...,d
(1+x )2 (1+cˆ )2 (1+x )(1+cˆ )
d+1 d+1 d+1 d+1
s
1−x2 1−cˆ2 ⟨x ,cˆ ⟩
=s d+1 + d+1 −2 1,...,d 1,...,d
(1+x )2 (1+cˆ )2 (1+x )(1+cˆ )
d+1 d+1 d+1 d+1
s
1−x 1−cˆ ⟨x ,cˆ ⟩
=s d+1 + d+1 −2 1,...,d 1,...,d
1+x 1+cˆ (1+x )(1+cˆ )
d+1 d+1 d+1 d+1
s
(1−x )(1+cˆ )+(1−cˆ )(1+x )−2⟨x ,cˆ ⟩
=s d+1 d+1 d+1 d+1 1,...,d 1,...,d
(1+x )(1+cˆ )
d+1 d+1
s
2−2x cˆ −2⟨x ,cˆ ⟩
=s d+1 d+1 1,...,d 1,...,d
(1+x )(1+cˆ )
d+1 d+1
s
2−2⟨x,cˆ⟩
=s
(1+x )(1+cˆ )
d+1 d+1
s
√ 1−⟨x,cˆ⟩
= 2s
(1+x )(1+cˆ )
d+1 d+1
At this point, we have reduced the formula as far as possible in terms of cˆ and now substitute cˆ with the
definition given in the Theorem.
s
√ ∥p−αv∥−⟨x,p−αv⟩
= 2s
(cid:0) (cid:1)
(1+x ) ∥p−αv∥+(p−αv)
d+1 d+1
12s
√ ∥p−αv∥−b+αx
= 2s d+1
(1+x )(∥p−αv∥+p −α)
d+1 d+1
√ v u p 1+α2−2αp −b+αx
= 2su d+1 d+1
t (cid:16)p (cid:17)
(1+x ) 1+α2−2αp +p −α
d+1 d+1 d+1
v u r (cid:16) (cid:17)2
u 1+ 1−b2 − 1−b2 p −b+ 1−b2 x
=√ 2su
u
2(b+pd+1) b+pd+1 d+1 2(b+pd+1) d+1
u r !
u (cid:16) (cid:17)2
t(1+x ) 1+ 1−b2 − 1−b2 p +p − 1−b2
d+1 2(b+pd+1) b+pd+1 d+1 d+1 2(b+pd+1)
√ v u p 4(b+p )2+(1−b2)2−4(b+p )(1−b2)p −2b(b+p )+(1−b2)x
= 2su d+1 d+1 d+1 d+1 d+1
t (cid:16)p (cid:17)
(1+x ) 4(b+p )2+(1−b2)2−4(b+p )(1−b2)p +2p (b+p )−(1−b2)
d+1 d+1 d+1 d+1 d+1 d+1
v q
=√ 2su u
u
b4+4b3p d+1+4b2p2 d+1+2b2+4bp d+1+1−2b2−2bp d+1+(1−b2)x d+1
t (cid:16)q (cid:17)
(1+x ) b4+4b3p +4b2p2 +2b2+4bp +1+2bp +2p2 −(1−b2)
d+1 d+1 d+1 d+1 d+1 d+1
s
√ (b2+2bp +1)−2b2−2bp +(1−b2)x
= 2s d+1 d+1 d+1
(cid:0) (cid:1)
(1+x ) (b2+2bp +1)+2bp +2p2 −(1−b2)
d+1 d+1 d+1 d+1
s
√ (1−b2)+(1−b2)x
= 2s d+1
(1+x )2(b+p )2
d+1 d+1
s
1−b2
=s
(b+p )2
d+1
=r
Thus all points from the boundary of the cap have a distance of r to the given center c, forming a sphere of
radius r.
Proof 7 (Proof of Theorem 2) Inserting the solution for α in Theorem 1 into the equation for r and
solving for b gives
q
s s2+(1−p2 )r2−p r2
d+1 d+1
b =
r2+s2
Solving the equation for r for α and substituting b with the solution for b gives
(cid:16) q (cid:17)
r2 p s+ s2+(1−p2 )r2
d+1 d+1
α =
2s(r2+s2)
Inserting that value for α in Equation (1) and solving for p gives
1,...,d
 q 
p s(r2+2s2)−r2 ∥p ∥2r2+s2
d+1 1,...,d
 v 
c

u
u
4∥p 1,...,d∥2s2(r2+s2)2 

 +u (cid:18) q (cid:19)2 
 t + p s(r2+2s2)−r2 ∥p ∥2r2+s2 
d+1 1,...,d
p =
1,...,d 2s2(r2+s2)
which we can solve for p by multiplying both vectors with p and solving
d+1 1,...,d
q
∥p ∥2(r2+s2)(s2−∥c∥2)+⟨c,p ⟩r2 ∥p ∥2r2+s2
1,...,d 1,...,d 1,...,d
p =
d+1 s(r2+2s2)⟨c,p ⟩
1,...,d
13Using the definition, that p =βc, we can give a closed form for p as
1,...,d d+1
q
β(r2+s2)(s2−∥c∥2)+r2 s2+∥c∥2β2r2
p =
d+1 s(r2+2s2)
q
Equating this solution with 1−β2∥c∥2, which is derived from the unit length of p, we obtain the final
solution for β that is given in the Theorem. The solution is unique whenever it exists, since the center of a
hyperball and direction vector of a cap are unique. The solution always exists, iff all terms are well-defined,
more specifically iff the term under the root in the solution for β is always positive. The term becomes
smallest for s→0 and in the limit s→0, the term becomes (c2−r2)2 which obviously is positive. Since we
demand s>0, the term is always positive and the solution for β is always well-defined.
Proof 8 (Proof of Corollary 4) In the proofs we will make use of the simplified definitions Se and Se−1
for the special case of v = (0,...,0,1). First we will derive the equations when moving from Euclidean to
spherical space and then we will derive the equations for the inverse direction.
Product, Euclidean to spherical
⟨xˆ,yˆ⟩
* +
[x,s] [y,s]
= 2s −v,2s −v
∥x∥2+s2 ∥y∥2+s2
* +
[x,s] [y,s]
= 2s ,2s
∥x∥2+s2 ∥y∥2+s2
* + * +
[x,s] [y,s]
− 2s ,v − 2s ,v +⟨v,v⟩
∥x∥2+s2 ∥y∥2+s2
* + ! !
x y 2s2 2s2
= 2s ,2s +
∥x∥2+s2 ∥y∥2+s2 ∥x∥2+s2 ∥y∥2+s2
2s2 2s2
− − +1
∥x∥2+s2 ∥y∥2+s2
4s2⟨x,y⟩ (cid:18) 2s2 (cid:19)(cid:18) 2s2 (cid:19)
= + −1 −1
(⟨x,x⟩+s2)(⟨y,y⟩+s2) ⟨x,x⟩+s2 ⟨y,y⟩+s2
4s2⟨x,y⟩+(cid:0) 2s2−(⟨x,x⟩+s2)(cid:1)(cid:0) 2s2−(⟨y,y⟩+s2)(cid:1)
=
(⟨x,x⟩+s2)(⟨y,y⟩+s2)
4s2⟨x,y⟩+4s4−2s2(⟨x,x⟩+s2)−2s2(⟨y,y⟩+s2)
= +1
(⟨x,x⟩+s2)(⟨y,y⟩+s2)
⟨x,x⟩−2⟨x,y⟩+⟨y,y⟩
= 1−2s2
(⟨x,x⟩+s2)(⟨y,y⟩+s2)
Distance, Euclidean to spherical
∥xˆ−yˆ∥2
= 2−2⟨xˆ,yˆ⟩
(cid:18) (cid:19)
⟨x,x⟩−2⟨x,y⟩+⟨y,y⟩
= 2−2 1−2s2
(⟨x,x⟩+s2)(⟨y,y⟩+s2)
4s2
= ∥x−y∥2
(∥x∥2+s2)(∥y∥2+s2)
14Product, spherical to Euclidean
(cid:28) (cid:29)
xˆ yˆ
⟨x,y⟩ = s 1,...,d ,s 1,...,d
1+xˆ 1+yˆ
d+1 d+1
⟨xˆ,yˆ⟩−xˆ yˆ
= s2 d+1 d+1
(1+xˆ )(1+yˆ )
d+1 d+1
Distance, spherical to Euclidean
∥x−y∥2
= ∥x∥2+∥y∥2−2⟨x,y⟩
⟨xˆ,xˆ⟩−xˆ2 ⟨yˆ,yˆ⟩−yˆ2 (cid:18) ⟨xˆ,yˆ⟩−xˆ yˆ (cid:19)
= s2 d+1 +s2 d+1 −2 s2 d+1 d+1
(1+xˆ )2 (1+yˆ )2 (1+xˆ )(1+yˆ )
d+1 d+1 d+1 d+1
(cid:18) (cid:19)
1−xˆ 1−yˆ ⟨xˆ,yˆ⟩−xˆ yˆ
= s2 d+1 +s2 d+1 −2 s2 d+1 d+1
1+xˆ 1+yˆ (1+xˆ )(1+yˆ )
d+1 d+1 d+1 d+1
(cid:18) (cid:19)
(1−xˆ )(1+yˆ )+(1+xˆ )(1−yˆ )
d+1 d+1 d+1 d+1
−2(⟨xˆ,yˆ⟩−xˆ yˆ )
= s2 d+1 d+1
(1+xˆ )(1+yˆ )
d+1 d+1
2−2⟨xˆ,yˆ⟩
= s2
(1+xˆ )(1+yˆ )
d+1 d+1
s2
= ∥xˆ−yˆ∥2
(1+xˆ )(1+yˆ )
d+1 d+1
15