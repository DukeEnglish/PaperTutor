A Hessian-Aware Stochastic Differential Equation for
Modelling SGD
Xiang Li† Zebang Shen† Liang Zhang† Niao He†
Abstract
Continuous-time approximation of Stochastic Gradient Descent (SGD) is a crucial tool to study
its escaping behaviors from stationary points. However, existing stochastic differential equation (SDE)
models fail to fully capture these behaviors, even for simple quadratic objectives. Built on a novel
stochastic backward error analysis framework, we derive the Hessian-Aware Stochastic Modified Equation
(HA-SME), an SDE that incorporates Hessian information of the objective function into both its drift and
diffusion terms. Our analysis shows that HA-SME matches the order-best approximation error guarantee
among existing SDE models in the literature, while achieving a significantly reduced dependence on the
smoothness parameter of the objective. Further, for quadratic objectives, under mild conditions, HA-SME
is proved to be the first SDE model that recovers exactly the SGD dynamics in the distributional sense.
Consequently, when the local landscape near a stationary point can be approximated by quadratics,
HA-SME is expected to accurately predict the local escaping behaviors of SGD.
1 Introduction
We consider unconstrained stochastic minimization problems of the form
minf(x):=Eξ[F(x;ξ)], (1)
x
where f :Rd R is a smooth function and ξ is a random vector. In this work, we study Stochastic Gradient
Descent (SGD→) [Robbins and Monro, 1951], the most popular approach for solving such problems, which
updates iteratively as follows:
x =x η F(x ;ξ ), (2)
k+1 k k k k
− ∇
where η >0 is the stepsize and F(x ;ξ ) is an unbiased stochastic gradient at x . In the absence of noise,
k k k k
SGD reduces to Gradient Desce∇nt (GD). The choice of the stepsize η is crucial, affecting aspects such as
k
convergence speed, training stability [Bengio, 2012, Nar and Sastry, 2018], and generalization error [Wilson
et al., 2017, Zhou et al., 2022].
Existingnon-convexoptimizationtheoryhaveestablishedthenon-asymptoticconvergenceofSGDtowards
a stationary point using a diminishing stepsize; see e.g., Drori and Shamir [2020], Fang et al. [2019], Ghadimi
and Lan [2013], Khaled and Richtárik [2023], Li and Orabona [2020], Sebbouh et al. [2021] and Yang et al.
[2024],tolistjustafew. Intuitively,thediminishingstepsizereducesthefluctuationcausedbySGD’sinherent
randomness, ensuring the objective descends in expectation. On the other hand, in practice a constant
stepsize, i.e. η η, is often preferred and yields better performance. This presents a challenge to classical
k
theory, as it fails≡to capture the non-diminishing role of noise in SGD dynamics. Additionally, it falls short in
differentiating between saddle points and local minima, as well as in identifying favorable stationary points —
those that not only are local minima but also exhibit good generalization.
†Department of Computer Science, ETH Zurich, Switzerland. xiang.li@inf.ethz.ch, zebang.shen@inf.ethz.ch,
liang.zhang@inf.ethz.ch,niao.he@inf.ethz.ch.
1
4202
yaM
82
]LM.tats[
1v37381.5042:viXra10
40 SGD 250 SGD
0 − HA-SME0 HA-SME
−20
−20
SME-1 200 SME-1
5
20 inS itME p- o2 in20
t
SME-2
150
0 0
100
0
40
20 20
40
0 0 50
0
5
− 10 5 0 5 10 0 2 4 6 8 10
− −
Ex numberofSGDupdates
(a)
f(cid:0) [x,y]⊺(cid:1)
=
1(cid:0) x2−y2(cid:1)
(b)
f(cid:0) [x,y]⊺(cid:1)
=
1(cid:0) x2+y2(cid:1)
2 2
Figure 1: Escaping behaviors of SGD and SDEs on quadratic functions. The left sub-figure demonstrates the
trajectories around a saddle point (0,0), while the right sub-figure represents the evolution of the function
value around a minimum. We set η =2.1, the initial point to (1,1), and the covariance of the additive noise
to I for both cases. We conducted the experiments 1000 times to estimate the expectation. For the right
sub-figure, we evaluated the SDEs at time stamps corresponding to η times the number of SGD updates. In
these two cases, the proposed HA-SME behaves similarly as SGD. However, SME-2 fails to escape the saddle
point in case (a), and both SME-1 and SME-2 fail to escape the minimum in case (b). We further provide in
Section 6.1 mathematical proofs for the failure cases of these two existing SMEs in approximating SGD.
TheabovelimitationsunderscoretheneedtounderstandacriticalaspectofSGDinnonconvexoptimization:
the transitions between different stationary points, beyond the traditional focus on convergence towards a
stationary point. The selection of stepsize plays a crucial role in SGD’s transition behaviors:
• Escaping from stationary points. At a stationary point, either a saddle point or a local minimum,
increasing the stepsize raises the likelihood of SGD to escape from it.
• Minima selection. Given a constant stepsize, SGD can rapidly exit local minima with a large Hessian
norm, also known as sharp minima, but dwell longer at flat minima where the norm is small.
The latter relates to the notable generalization capabilities of SGD, which in prevailing belief, tends to favor
flat minima over sharp ones [Keskar et al., 2017, Neyshabur et al., 2017, 2018]. These phenomena have been
empirically verified [Zhang et al., 2021] and theoretically investigated [Brandière and Duflo, 1996, Liu and
Yuan, 2023, Mertikopoulos et al., 2020, Pemantle, 1990]. Indeed, analyzing the link between stepsize and the
escaping dynamics of SGD has emerged as an important topic in non-convex optimization. In particular,
there is growing interest in using continuous-time dynamics, specifically Stochastic Differential Equations
(SDEs), to understand the mechanisms behind escaping from saddle points and minima selection [Hu et al.,
2019, Xie et al., 2022]. This involves quantifying the interplay between escaping speed, stepsizes, and local
sharpness [Hu et al., 2019, Ibayashi and Imaizumi, 2023, Nguyen et al., 2019, Xie et al., 2020, Zhu et al.,
2019]. A more detailed discussion of these works is provided in Section 1.1.
Existing continuous-time approximations of SGD and their failure modes. Previous study on
the escaping behavior of constant-stepsize SGD through its continuous-time approximation typically entails
two steps: (1) Proposing a proxy SDE model that approximately tracks the distributional evolution of the
SGD dynamics; (2) Analyzing the escaping behavior of the proposed SDE model to predict or interpret the
behavior of SGD. Evidently, whether the behavior of SGD can be precisely captured hinges on the accuracy
of the proxy models. Among existing SDE models, the second-order stochastic modified equation (SME-2) [Li
et al., 2017] provides the order-best approximation guarantee:
2
yE
)y,x(fE(cid:16) η (cid:17) (cid:112)
dX = f(X )+ f(X ) 2 dt+ ηΣ(X )dW , (3)
t t t t t
−∇ 4∥∇ ∥
where Σ(X ) denotes the covariance matrix of the stochastic gradient at X , W represents the standard
t t t
Wiener process, and η denotes the constant stepsize of SGD. However, we note that even for simple quadratic
objectives,SME-2andSGDshowdifferentbehaviorsinescapingfromsaddlepointsandminima: SME-2exhibits
locally stabilizing behavior while SGD with constant stepsize quickly escapes from the same local region,
as illustrated in Figure 1. Our study also identifies similar failure modes for other existing SDE models,
detailed in Section 6.1. This disparity in behavior suggests the inadequacy of existing SDEs in modeling the
escaping dynamics of SGD, even with high-order approximation such as SME-2. Consequently, this brings us
to a central question:
How to accurately model SGD in continuous time while preserving its escaping behaviors?
Our contributions. We develop a new Stochastic Backward Error Analysis (SBEA) to approximate
the dynamics of constant-stepsize SGD. Our analysis yields an SDE, termed “Hessian-Aware Stochastic
Modified Equation” (HA-SME), that integrates Hessian information into both its drift and diffusion terms.
By incorporating local curvature information, HA-SME effectively tracks the escape phenomena of SGD. The
well-posedness of HA-SME is assured under mild regularity assumptions. Moreover, under similar assumptions,
the distribution evolution of HA-SME exactly matches that of SGD on quadratic functions.
Our contributions are summarized as follows:
• Failure modes of existing SDE proxies. In this study, we identify several shortcomings of current
SDE approximations in capturing the escaping behaviors of SGD. We present concrete examples of
optimization problems, where SGD exhibits escaping behavior, while existing SDE approximations tend
to stabilize around the stationary points. We further pinpoint that this failure mode stems from the
negligence of higher-order terms involving Hessians in their derivations.
• A new Hessian-aware SDE proxy. We propose the SBEA framework, which enables us to design
HA-SME capable of encompassing more error terms involving gradients and Hessians than existing SDEs
in the error analysis. Specifically, the drift term and diffusion coefficient of HA-SME are defined by
two power series, respectively. Utilizing a generating function approach, we demonstrate that these
power series converge when the stepsize is below a certain constant threshold, and their limits possess
analytical expressions. We provide sufficient conditions to ensure HA-SME is well-posed.
• Approximation guarantees on general functions. We establish that HA-SME adheres to an
order-2 weak approximation error guarantee, aligning with the best-known results among existing SDE
approximations. To further differentiate HA-SME from existing SMEs, we conduct a fine-grained analysis
of the weak approximation error, taking into account of the smoothness parameter of the objective,
which we denote as λ. For convex functions, our findings indicate that HA-SME admits a uniform-in-time
approximation error of order (cid:0) η2(cid:1). In stark contrast, the error for SME-1 model is (ηλ), and for
SME-2, it is (cid:0) η2λ2(cid:1). Our resuO lts mark a notable improvement over the dependence onO the smoothness
parameter cOompared to existing SDE approximations.
• Exact recovery on quadratic functions. We then examine the quadratic functions, a common
simplificationusedtoapproximatethelocallandscapesofnon-linearobjectives. Weshowthataccurately
mirroring the SGD dynamics (linear in this setting) with a linear SDE (OU process) is fundamentally
impossible, due to a potential mismatch between the ranks of their covariance matrices. Nonetheless,
we establish that under certain mild conditions, i.e., when eigenvectors of the Hessian and the noise
covariance matrices align or a small stepsize is used, the extension of our proposed HA-SME into the
complex domain is well-founded, and this extension exactly replicates the constant-stepsize SGD
dynamics in a distributional sense. This marks the first work, to the best of our knowledge, in achieving
exact recovery of SGD using continuous-time models.
31.1 Related Work
Continuous-time approximation of GD and SGD. The connection between differential equations
and gradient-based methods has been extensively studied previously, such as in [Attouch et al., 2022, Bloch,
1994, Brown and Bartholomew-Biggs, 1989, Fiori and Bengio, 2005, Helmke and Moore, 2012, Krichene et al.,
2015, Muehlebach and Jordan, 2019, 2021, Schropp and Singer, 2000, Shi et al., 2021, Su et al., 2016]. For
GD, the most straightforward continuous-time approximation is the gradient flow
dX = f(X )dt. (4)
t t
−∇
WhilethisflowapproximatesGDaccuratelyasthestepsizeapproacheszero,theapproximationerrorbecomes
critical when the stepsize is large. Barrett and Dherin [2020] took a step further by employing backward error
analysis (detailed in Section 2.2) to derive a second-order continuous-time approximation for GD, as given by:
(cid:16) η (cid:17)
dX = f(X )+ f(X ) 2 dt. (5)
t t t
−∇ 4∥∇ ∥
However, this second-order model still has limitations, particularly in capturing certain critical behaviors of
GD in discrete-time, such as overshooting, i.e., the iterates oscillate around the minimum or even diverge
because of large stepsize. To tackle this issue, Rosca et al. [2022] introduced the Principle Flow (PF), a
continuous-timeapproximationforGDthatoperatesincomplexspaceandcanexactlymatchthediscrete-time
dynamics of GD on quadratics. This work is closely related to ours, and we will dive deeper in Section 2.3.
When taking noises into account, Mandt et al. [2015, 2016] introduced the following SDE dynamics for
approximating SGD (SME-1):
(cid:112)
dX = f(X )dt+ ηΣ(X )dW . (6)
t t t t
−∇
Later,Lietal.[2017]introducedSME-2(describedinEquation(3))andestablishedrigorousweakapproximation
for these SDEs. These SDEs are named according to the order of their weak approximation accuracy (see
precise definition in Definition 3.1). Additional contributions by Orvieto and Lucchi [2019] and Fontaine et al.
[2021] examined SDEs where the drift and diffusion terms are scaled by varying stepsizes. Their SDEs reduce
to SMEs when adopting constant stepsizes. Another line of work has considered Homogenized SGD, which
replaces the diffusion term in SME-1 with (cid:112) ηf(X ) 2f(X )/B [Paquette and Paquette, 2021, Paquette et al.,
t t
2022] or (cid:112) ηf(X ) 2f(X )/B [Mori et al., 2022], w∇ here B denotes the batch size and X the local minimum.
However, their at n∇alyses a∗ re limited to the least square loss, lacking approximation gu∗ arantees for general
non-convex objectives.
EscapingBehaviorsAnalysis Backinthe90s,Pemantle[1990]andBrandièreandDuflo[1996]showedthat
SGD with a small stepsize can avoid hyperbolic saddle points, i.e., λ (cid:0) 2f(x)(cid:1) <0 and det(cid:0) 2f(x)(cid:1) =0.
min
Recent studies [Liu and Yuan, 2023, Mertikopoulos et al., 2020] extend∇ed this understanding∇by show̸ing
SGD can escape strict saddle points (λ (cid:0) 2f(x)(cid:1) < 0). Additionally, with slight modifications to the
min
algorithm, such as adding artificial noises wh∇en certain conditions are met, Ge et al. [2015] and Jin et al.
[2017] established convergence of noisy SGD to approximate local minima.
In terms of minima escaping, leveraging continuous-time frameworks, Jastrzebski et al. [2017] highlighted
three crucial factors that influence the minima found by SGD: step-size, batchsize, and gradient covariance.
Zhu et al. [2019] focused on the role of anisotropic noise and concluded that such noise enables SGD to
evade sharp minima more effectively. Quantitatively, the time required for escaping has been theoretically
shown to exponentially depend on the inverse of the stepsize, using theory for random perturbations of
dynamical systems [Hu et al., 2019]. Subsequent work expanded on these ideas, considering heavy-tail noise
and utilizing Lévy-driven SDEs to examine SGD’s stability around minima [Nguyen et al., 2019]. While the
aforementioned research largely focused on parameter-independent gradient noise, Xie et al. [2020] analyzed
parameter-dependent anisotropic noise. Using the analysis for the classic Kramers’ escape problem, they
showed that compared to Stochastic Gradient Langevin Dynamics, SGD can quickly escape sharp minima,
with escape speed exponentially depending on the determinant of the Hessian. Their analysis assumes that
4the system first forms a stationary distribution around a basin before escaping. However, SGD typically
selects minima dynamically. New insights into the exponential escape time have been developed by employing
Large Deviation Theory in non-stationary settings [Ibayashi and Imaizumi, 2023].
2 Preliminaries
In this section, we first introduce the notations used throughout our analysis, and then review the Backward
Error Analysis (BEA), a technique instrumental to the development of our stochastic backward error analysis.
Additionally, we discuss the Principle Flow (PF) introduced by Rosca et al. [2022], which inspires the
derivation of our HA-SME model.
2.1 Notations
Uppercase variables, such as X, denote continuous-time variables, while lowercase variables like x represent
discrete-timevariables. ThefunctionX(x ,t)indicatesthesolutionofacontinuous-timeprocessstartingfrom
0
theinitialpointx afteratimedurationt. Formatrixnotation,weuse[A] tospecifytheelementatthe(i,j)-
0 i,j
th position in matrix A. For two matrices, A and B, of the same size, we denote A:B =tr(cid:0) A B(cid:1). Applying
⊤
the logarithmic function to a matrix and dividing between two matrices are by default element-wise. We use
thenotationI torepresenttheidentitymatrixand todenotetheHadamardproductbetweenmatrices. The
square root of a matrix, i.e., √A, is the matrix B s⊙uch that BB⊺ =A. In the context of functions, f is
Cm
d ise tfi hn eed sea ts o(cid:80)
f
a|lα
l|
m≤m -t| hD dα ifff e|∞ re, nta in ad blw ee fud ne cfi tin oe nsth oe ns Ret d.C Fbm or(cid:0) R cod m(cid:1) = ple{ xf n∈ umC bm er(cid:0) R xd ,(cid:1) R| e∥ (xf )∥C anm d< Im∞ (x} ), w exh te rr ae ct∥ C tm h∥ e(cid:0) R red a(cid:1)
l
and imaginary parts, respectively, and x denotes the complex conjugate of x. For random variables x and y,
(cid:104) ⊺(cid:105)
the covariance is represented as Cov[x,y]=E (x E(x))(y E(y)) . Lastly, we use W
t
to represent the
− −
standard Wiener process.
2.2 Backward Error Analysis (BEA)
Consider the GD dynamics1
x =x η f(x ), (7)
k+1 k k
− ∇
with constant stepsize η. Let X(cid:101) :[0, ) Rd be a continuous trajectory. The idea of BEA is to identify a
series of functions, denoted as g
i
:Rd∞ → Rd for i 0,1,... , which constitute a modified equation as:
→ ∈{ }
d
X(cid:101)
=G(cid:101)η(X(cid:101)):=(cid:88)∞
ηpg p(X(cid:101)), (8)
dt
p=0
such that X(cid:101)((k+1)η) closely approximates x k+1, given X(cid:101)(kη)=x k. To achieve this goal, we consider the
Taylor expansion of the above continuous-time process at time kη,
(cid:12)
(cid:88)∞ ηj djX(cid:101) (cid:12) (cid:88)∞ η2 (cid:88)∞ (cid:88)∞
X(cid:101)((k+1)η)=
j!
(dt)j(cid:12)
(cid:12) (cid:12)
=x k+η ηpg p(x k)+
2
ηp ∇g p(x k) ηpg p(x k)+
···
. (9)
j=0 X(cid:101)=xk p=0 p=0 p=0
By matching the powers of η in Equation (7) and Equation (9) up to the (p+1)-order, we solve for g
i
for i = 0,...,p. This procedure ensures that the leading error term between the continuous-time variable
X(cid:101)((k+1)η) and the discrete-time variable x k+1, given X(cid:101)(kη) = x k, is of the order (cid:0) ηp+2(cid:1). For a more
comprehensive exploration of BEA, please refer to Section IX of Hairer et al. [2006]. O
When p=0, the system reduces to gradient flow (Equation (4)) and then p=1, we recover Equation (5).
Increasing the order p in the above BEA procedure improves the accuracy of the approximation. The higher
1Similarargumentscanbemadeforotherdiscrete-timedynamics.
5order modified flows generated by BEA involve higher-order gradients of the objective function f. For
example, for p=2, according to BEA, we obtain the third-order Gradient Flow [Rosca et al., 2022]:
(cid:18) (cid:19)
η 1 1 ⊺
dX /dt= f(X ) 2f(X ) f(X ) η2 2f(X ) f(X )+ f(X ) 3f(X ) f(X ) .
t t t t t t t t t
−∇ − 2∇ ∇ − 3∇ ∇ 12∇ ∇ ∇
2.3 Principle Flow
While theoretically, one could solve for all component function g ’s to achieve an arbitrary target error order,
i
the resulting modified flow quickly becomes cumbersome due to the involvement of higher-order gradients.
Moreover, the resulting power series (w.r.t. the exponent of η) may not even be convergent [Hairer et al.,
2006]. In light of this limitation of BEA, Rosca et al. [2022] suggested the following term selection strategy:
Principle 1 (Term selection of PF, BEA). In BEA, when solving g ’s in Equation (8), derivatives of f(x)
i
up to the second order, i.e., terms containing pf(x) for p 3, are omitted.
∇ ≥
With this term selection scheme, the resulting power series not only converges (with convergence radius
η 2f(x) =1) but admits a concise form. The corresponding modified equation is:
∥∇ ∥
dX =U(X )log(I −ηΛ(X t)) U(X )⊺ f(X )dt, (10)
t t t t
ηΛ(X ) ∇
t
where for any fixed x, we denote 2f(x)=U(x)Λ(x)U(x)⊺ the eigen-decomposition of 2f(x). When the
stepsize is large, the logarithmic f∇unction could result in complex numbers, as elements o∇f I ηΛ might be
negative. Operating in the complex space is beneficial for modeling the divergent and oscillato−ry behaviors of
gradient descent Rosca et al. [2022]. As only higher-order gradients are omitted in Principle 1, PF provides
an exact characterization of the discrete GD dynamics on quadratic objectives.
3 Stochastic Backward Error Analysis
We extend the BEA approach to the stochastic domain, termed Stochastic Backward Error Analysis (SBEA),
providing a systematic approach for constructing SDEs that approximate stochastic discrete-time processes.
While we focus on the analysis for SGD, similar argument can be generalized to other dynamics.
3.1 Differences between BEA and SBEA
Compared with BEA, there are two major modifications in SBEA. First, SBEA includes an additional
Brownian motion term in its ansatz, compared to the one in Equation (8), with the diffusion coefficients to
be determined; Second, to evaluate the quality of the approximation, SBEA adopts the weak approximation
error (see Definition 3.1) as a metric in the distributional sense.
(1) Ansatz with Brownian motion. In BEA, we define the modified equation with a series of functions
{reg pi }rei ∈seN n. tT to hefu dr it ffh uer siom nod coel effith ce ier na tn sd oo fm tn he ess Bi rn owS nG iaD n,w me otin iot nro .d Su pc ee ca ifin co at lh lye ,r wse eri ce os n{sh idi e}ri ∈aN > h0 y, ph oi th: eR sid s→conR td in× ud o, ut so
-
time dynamics as follows:
dX = (cid:0) g (X)+ηg (X)+η2g (X)+ (cid:1) dt+(cid:112) ηh (X)+η2h (X)+ dW . (11)
0 1 2 1 2 t
··· ···
The rationale stems from the observation that the noise introduced in the ansatz undergoes continuous
modification by the drift term. Consequently, it is natural to employ a similar power series representation for
the diffusion coefficient2. While one can truncate the series h i i N to simplify the analysis, as we illustrate
inFigure2andtheoreticallyjustifyinSection6.1, suchtrunc{atio}n∈cancompromisetheapproximationquality
and lead to misaligned escaping behaviors.
2Wedonotincludeh0 intheansatzasSGDdegeneratestogradientflowwhenη→0,whichimpliesh0≡0.
6(2) Weak approximation error as quality metric. Our goal is to solve for the functions g ’s and h ’s
i i
so that the ansatz in Equation (11) approximates the SGD dynamics with high quality. Instead of seeking a
path-wise alignment like BEA, we focus on aligning the continuous-time and discrete-time dynamics in a
distributional sense, characterized by the following weak approximation error.
Definition 3.1 (Weak Approximation Error). Fixing T >0, we say a continuous-time process X(t)
t [0,T]
i ηs 0a >n 0or td he ar tp arw ee ia nk dea pp ep nr dox enim ta ot fio ηn (bto utth me as yeq du epen ence d{ox nk } T,if ufo ar na dn iy tsu d∈eriC vb a2( tp iv+ e1 s) )(cid:0) R sud c(cid:1) h, t th he ar te foe rxi as lt ls xC > R0 d∈ ,and
∈
(cid:12) (cid:12)
(cid:12) (cid:12)E[u(x k) x
0
=x] E[u(X(kη)) X(0)=x](cid:12)
(cid:12)
Cηp, (12)
| − | ≤
for all k =1,..., T/η and all 0<η <η .
0
⌊ ⌋
ThismetriciscommonlyusedforanalyzingdiscretizationerrorsofSDE,asreferencedintextbooks[Kloeden
and Platen, 2011, Milstein, 2013] and various studies on modeling SGD behavior with SDEs [Feng et al.,
2019, Hu et al., 2019, Li and Wang, 2022, Li et al., 2017].
Remark 3.1. We highlight the difference between SBEA and the weak backward error analysis introduced
by Debussche and Faou [2012]. The latter provides a framework for constructing a sequence of modified
Kolmogorovgeneratorswithincreasingorderstoapproximatediscrete-timeprocesses. However,sincethehigh-
order Kolmogorov generators involve high-order gradients, it does not yield an equivalent SDE representation
(recall that the Kolmogorov generator corresponding to an Itô SDE contains only second-order gradients).
Consequently, their framework is fundamentally different from our approach in terms of the final outcome.
3.2 Procedures in SBEA
We determine the components h i i N and g i i N by first calculating the semi-group expansions of the two
conditional expectations involv{ed}in∈Equati{on}(1∈2) after one step, i.e. k =1, and then match the resulting
terms according to the power of η. Such a one-step approximation analysis can be translated to the weak
approximation error guarantee via a martingale argument, as we will elaborate in Section 5.
Semi-group expansions. We expand the conditional expectation of the test function u after one SGD
step as:
E[u(x η F(x;ξ))
x]=u(x)+η(cid:88)∞
ηpΦ p(x), (13)
− ∇ |
p=0
where we use Φ to denote the aggregation of terms with ηp+1 in the expansion. Simple calculation shows
p
Φ (x)= f(x)⊺ u(x), Φ (x)= 1 2u(x):(cid:0) f(x) f(x)⊺ +Σ(x)(cid:1) , (14)
0 1
−∇ ∇ 2∇ ∇ ∇ ···
where Σ(x) denotes the covariance matrix of the stochastic gradient at x, defined as
(cid:104) ⊺(cid:105)
Σ(x)=E ( F(x;ξ) E[ F(x;ξ)])( F(x;ξ) E[ F(x;ξ)]) . (15)
∇ − ∇ ∇ − ∇
For the continuous-time system, by the semi-group expansion [Hille and Phillips, 1996], the conditional
expectation of a test function u under the SDE model yields:
E[u(X(η)) X(0)=x]=(cid:0) eη Lu(cid:1) (x)=u(x)+η u(x)+ 1 η2 2u(x)+ 1 η3 3u(x)+ , (16)
| L 2 L 6 L ···
where is corresponding the infinitesimal generator [Särkkä and Solin, 2019]. For the SDE described in
EquatioLn (11), the generator is given explicitly by
+ (cid:18) (cid:19) +
1 (cid:88)∞ 1 (cid:88)∞
=(g +ηg + ) + (ηh + ): 2 =g + ηi g + h : 2 = ηi ,
0 1 1 0 i i i
L ··· ·∇ 2 ··· ∇ ·∇ ·∇ 2 ∇ L
i=1 i=0
7where for the ease of notation, we denote
1
:=g and :=g + h : 2 for i 1.
0 0 i i i
L ·∇ L ·∇ 2 ∇ ≥
Gathering terms with power ηp in Equation (16) leads to the following lemma:
Lemma 3.1. Define the function
p+1
(cid:88) 1 (cid:88)
Ψ p(x)= ln ln lnu(x), (17)
n! L1L2 ···Ln
n=1 ln 0 n :(cid:80)n ln=p+1 n
{i≥ }i=1 i=1 i −
where denotes the composition of the operators and . We have that
ln ln ln ln
LjLj+1 Lj Lj+1
(cid:0) eη u(cid:1) (x)=u(x)+η(cid:88)∞ ηpΨ (x). (18)
L p
p=0
In the following, we determine the components g ’s and h ’s in an iterative manner.
p p
Identify components by iteratively matching terms. Ouraimistodetermineg ’sandh ’sbyaligning
i i
the terms from the discrete-time expansion (Equation (13)) with those from continuous time (Equation (18)),
according to the order of η. Note that every application of the generator on a function generates terms
with all orders of η. Consequently, p-th order terms in the continuous-time eLxpansion become complicated as
p grows, as evident in Lemma 3.1. Fortunately, we can identify g ’s and h ’s in an iterative manner. In the
p p
following discussion, g and h for i=0,1,...,p 1 are treated as known from our iterative construction,
i i
and we solve the unknowns g and h by matchin−g Φ and Ψ . We make the following two observations:
p p p p
[Ψ (x) (Equation (17)) is linear on g and h .] Recall that g and h are only contained linearly in
p p p p p
u, which appears only once in Ψ (x) by choosing n=1 and l1 =p.
Lp p 1
[TermsinΨ (x)withn>1containsonlyknownterms.] Forn>1,thetermsinEquation(17)willonly
p
include g and h for i p 1, since for any choice of the indices ln+1 n , we must have ln p 1 if
n>1. Ti hese teri ms are≤alr−eady determined by our iterative strat{egj y. }j=1 j ≤ −
Following Rosca et al. [2022], we also explore various term selection strategies, starting with the exact
matching case.
Principle 2 (Term selection of exact-matching, SBEA). When determining the component functions g and
p
h , all terms in Φ from Equation (13) must match the ones in Ψ from Equation (18). In particular, given
p p p
the arbitrariness of the test function, the coefficients of pu (p 1) should all be matched.
∇ ≥
With the above principle, we can identify the unknowns, g and h , for p 1.
p p
≤
Remark 3.2 (Recovering SME-2 by SBEA). By matching Ψ and Φ , we deduce that g (x)= f(x). By
0 0 0
matchingΨ withΦ ,wesolvethatg (x)= 1 2f(x) f(x)andh (x)=Σ(x). Thisleadstothe−f∇ormulation
of the SME-21 (Equat1 ion (3)). 1 −2∇ ∇ 1
3.3 Exact Matching Fails for Order η3 in SBEA
We have shown that for p=0,1, it is possible to exactly solve g and h . However, for p 2, adhering to
p p
Principle 2 can be infeasible. Specifically, following Principle 2 leads to an over-determin≥ed system when
determining the unknown component functions g and h for p 2. Recall that the test function u is chosen
p p
arbitrarily and let us consider all the possible occurrence of q≥ u(x), q =1,2,..., in Φ and Ψ .
p p
∇
8• For the continous-time dynamics, the term Ψ in Equation (17) can contain terms with factors qu(x)
p
for q ranging from 1 to p+1. For example, the term p+1u(x) can be generated from Equatio∇n (17)
with the choice of n=p+1 and ln =0 for all j 1,∇ ...,n ; the term u(x) can be generated with
j ∈{ } ∇
the choice n=1 and l1 =0.
1
• For discrete-time expansion as per Equation (13), the term Φ involves only pu(x).
p
∇
For exact matching, the coefficient of pu in Ψ must match that in Φ ; besides, for any q = p, the
p p
coefficientof quinΨ mustbezero. Thisl∇eadstoanover-determinedsystemcomprisingp+1condi̸tionsfor
p
2 free variabl∇es (g and h ) when p 2. In other words, the above iterative construction of the components
p p
g and h fails for p 2. The infea≥sibility result highlights a drastic difference between SBEA and BEA.
p p
Shardlow [2006] also≥noted such infeasibility when attempting to find stochastic modified flows to achieve
higher orders of weak approximation guarantees.
InspiredbyPF,weintroducearelaxedtermselectionscheme,comparedtoPrinciple2,sothatonlyasubset
of terms in Ψ (the continuous-time dynamics) and Φ (the discrete-time dynamics) match. Importantly, this
p p
term selection scheme should ensure that the resulting continuous-time dynamics still preserve the favorable
escaping behaviors of the discrete-time dynamics of interest. This will be the focus of the following section.
4 Hessian-Aware Stochastic Modified Equation
In this section, we first derive the proposed SDE, HA-SME, to model SGD using the idea of SBEA. Then we
discuss sufficient conditions for its well-posedness.
Due to the lack of degree of freedom, exactly matching all terms in Φ and Ψ for p 2 is in general
p p
infeasible. Instead, we adopt the following term selection principle. ≥
Principle 3 (Term selection in HA-SME, SBEA). When determining the components g and h in SBEA,
p p
ignore the terms that involve (r)f(x), (s)u(x) and (m)Σ(x) for r,s 3 and m 1.
∇ ∇ ∇ ≥ ≥
Under this principle of term selection, we obtain the following result.
Lemma 4.1. Under Principle 3, when applying SBEA on SGD, we have the following results:
1. The component functions g and h in Equation (11) are uniquely determined;
p p
2. The component functions g and h in Equation (11) admit the form
p p
g (x)= c (cid:0) 2f(x)(cid:1)p f(x), (19)
p p
· ∇ ∇
p 1
h p(x)= (cid:88)− a
k,p 1 k
(cid:0) 2f(x)(cid:1)k Σ(x)(cid:0) 2f(x)(cid:1)p −1 −k ; (20)
− − · ∇ ∇
k=0
3. The coefficients a and c in g and h satisfy
s,m s p p
{ } { } { } { }
+ +
log(1 x) (cid:88)∞ log(1 x)(1 y) (cid:88)∞
− = c xs and − − = a xsym.
s s,m
x xy (x+y)
s=0 − s,m 0
≥
We now derive the limit of the power series defined in Equation (11).
Theorem 4.1. Let the components g and h be determined by the SBEA framework applied on SGD
p p
{ } { }
under Principle 3. Denote the limits
(cid:88) (cid:88)
b(x)= ηpg (x) and (x)= ηph (x). (21)
p p
D
p 0 p 1
≥ ≥
9(cid:13) (cid:13)
Under the assumption that η <1/(cid:13) 2f(x)(cid:13), we have that the limits b(x) and (x) exist with
∇ D
b(x)=U(x)log(I −ηΛ(x)) U(x)⊺ f(x), (22)
ηΛ(x) ∇
⊺
[U ΣU] log(1 ηλ )(1 ηλ )
(x)=U(x)S(x)U(x) ,such that [S(x)] = i,j − i − j , (23)
D ⊤ i,j ηλ λ (λ +λ )
i j i j
−
⊺
where U(x) and Λ(x) are defined through the eigen-decomposition 2f(x)=U(x)Λ(x)U(x) . The diagonal
∇
elements of Λ(x) are denoted by λ (x). For conciseness, we omit the dependence of λ , U and Σ on x.
i i
The derivation of the above theorem is highly nontrivial, and represents one of the major contributions of
our paper. Its proof includes several critical steps: (1) Identifying the structure of the components g and h
p p
in the SBEA ansatz; (2) Determining the coefficient of the components g and h ; (3) Computing the limit of
p p
the resulting power series, which we defer to Appendix A.
The following lemma provides sufficient conditions for the existence of the diffusion coefficient (cid:112) (x).
D
Lemma 4.2. (x) is positive semi-definite at point x if either of the following two conditions holds:
D
(cid:13) (cid:13)
1. The Hessian 2f(x) and covariance matrix Σ(x) commute, and η <1/(cid:13) 2f(x)(cid:13).
∇ ∇
2. The covariance matrix Σ(x) is positive definite, and η satisfies
(cid:40) (cid:115) (cid:41)
1 λ (Σ(x)) √2
η min 1 1 min ,1 , (24)
≤ 2f(x) − − √dλ (Σ(x)) − 2
∥∇ ∥ max
where λ () and λ () denote the maximum and minimum eigenvalues of a matrix.
max min
· ·
The validity of the above two conditions is discussed at the end of this subsection. With Theorem 4.1 and
Lemma 4.2, we now present the proposed SDE model.
Definition 4.1. Let the assumptions in Theorem 4.1 and Lemma 4.2 hold for any x, then the drift
term b(x) and diffusion coefficient D(x) := (cid:112) (x) in Equation (21) are well-defined. We denote the
SDE as the Hessian-Aware Stochastic ModifiedDEquation (HA-SME):
dX =b(X )dt+D(X )dW . (25)
t t t t
Comparing with SME-1 in Equation (6) and SME-2 in Equation (3), both the drift term and the diffusion
coefficient of HA-SME incorporate the Hessian information, hence the name.
Notice that the drift term b(x) of HA-SME exactly corresponds to PF defined in Equation (10). A naive
idea would be to combine PF with the conventional diffusion coefficient √ηΣ as in SME-1 and SME-2. The
following remark comments on the drawback of such a construction.
Remark 4.1 (A naive SDE derived from PF). Adding the diffusion coefficient √ηΣ to the PF in Equation (10)
gives rise to the following SDE:
dX =U(X )log(I −ηΛ(X t)) U(X )⊺ f(X )dt+(cid:112) ηΣ(X )dW , (26)
t t t t t t
ηΛ(X ) ∇
t
which we refer to as Stochastic Principle Flow (SPF). Such a straightforward combination could lead to
escaping behaviors different from SGD. For example, when considering a multi-mode function, as illustrated
in Figure 2, SGD and our proposed HA-SME can easily escape the local minimum and find the global one,
whereas SPF remains trapped in the valley of the initial local minimum. A theoretical comparison of HA-SME
and SPF on general objective functions is provided in Section 5.2.
10SGD
2 f(x)
HA-SME
initialpoint
4 SPF
globalminimum
1
2
0
0
2 0 2 4 6 8 0 20 40 60 80 100
−
x timet
(a) function f(x) (b) expected value of x through time
t
Figure 2: The illustration depicts the failure case for SPF, a naive stochastic extension of PF. The objective
function is defined as f(x)= 1x2 for x<1; 1(x 2)2+1 for 1 x<3; and 1(x 5)2 1 for x 3. We
use dashed lines to indicate th2 e two minima.−T2 he i−nitial point is se≤t to 0, and we4 set− η =0− .992 9 and t≥he noise
variance to 1 for all methods. We run the simulation 100 times to estimate the expectation. SGD and HA-SME
escape the initial minimum and arrive near the global minimum, while SPF stays around the initial one. A
theoretical justification for the failure of SPF is presented in Section 6.1.
Remark 4.2 (Comparison with existing SMEs). By integrating Hessian into its diffusion coefficient, HA-SME
provides a nuanced correction to the SDE noise. This adjustment is crucial for capturing the true dynamics
of SGD, even when considering additive, state-independent noise models, such as F(x,ξ) = f(x)+ξ,
where ξ (0,Σ). While discrete-time models treat noise as state-independent, in∇continuous t∇ime, noise
is introd∼ucNed at infinitesimal time intervals subject to immediate transformation by the drift term, which
includes Hessian information. The necessity of incorporating the Hessian matrix into the diffusion term for
corrections is also underscored by the failure cases of existing SDE models discussed in Section 6.1.
Remark 4.3 (Discussion on conditions of Lemma 4.2). The first condition states that the eigenvectors of
2f(x) and Σ(x) are aligned. In the vicinity of minima, this condition finds support from both theoretical
∇analysis and empirical evidence, even in deep learning. Theoretically, Jastrzebski et al. [2017] showed that
Σ(x ) 2f(x ) when the model fits all the data at x , which is further empirically verified by Xie et al.
∗ ∗ ∗
≈∇
[2020]. For mean-square loss, Mori et al. [2022] and Paquette et al. [2022] derived Σ(x) 2f(x) 2f(x ) near
local minima x , where B is the mini-batch size. Wang and Wu [2024] further
theor≈eticaB lly∇justifi∗
ed the
∗
approximation for nonlinear networks. In addition, the approximation Σ(x) 2f(x ) is commonly used in
∗
local escaping analysis of SGD [Ibayashi and Imaizumi, 2023, Xie et al., 20≈20,∇2022, Zhu et al., 2019]. Our
requirement here is more relaxed, as we only need the eigenvectors to be the same, regardless of eigenvalues.
To guarantee the existence and uniqueness of a solution for an SDE, it is often sufficient to impose
regularity conditions on both the drift and diffusion terms. In this context, we show that, when Σ is positive
definite, conditions in Lemma 4.2 lead to the well-posedness of the proposed HA-SME model.
(cid:0) (cid:1)
Theorem 4.2. Assume f C3 Rd , Σ being positive definite, and that at least one of the two conditions in
∈ b
Lemma 4.2 is satisfied everywhere. HA-SME has a unique strong solution.
Remark 4.4. While for general smooth objectives, Lemma 4.2 requires a small stepsize, in the particular case
of quadratic functions with a constant noise covariance, the requirement of small stepsize is not necessary, as
will be discussed in Section 6. In this case, the diffusion coefficient is constant, and the drift term depends
linearly on X , naturally fulfilling the Lipschitz criterion. This is sufficient for proving the well-posedness of
t
SDEs, independent of the stepsize.
11
)x(f txE5 Approximation Error Analysis of HA-SME
In this section, we analyze the approximation errors of HA-SME in approximating SGD on general smooth
functions. Our first result establishes the order 2 weak approximation for HA-SME, as defined in Definition 3.1,
matching the order-best guarantee in the literature. Subsequently, we conduct a more fine-grained analysis,
elucidating the explicit dependence of the approximation error on the smoothness parameter of the objective
function. We observe a significant improvement of HA-SME over the existing SME models. In particular, for
convex objectives, the leading error term of HA-SME is independent of the smoothness parameter.
5.1 Weak Approximation Error Guarantee
Below, we show a weak approximation error guarantee on sufficiently smooth functions. The assumptions we
impose are common in deriving weak approximation errors for SDEs that approximate SGD [Feng et al.,
2017, Hu et al., 2019, Li et al., 2017].
(cid:0) (cid:1)
Theorem 5.1. Assuming for any ξ, F(;ξ) C7 Rd , HA-SME is an order 2 weak approximation of SGD.
· ∈ b
An order 2 weak approximation is the order-best approximation guarantee known for SGD in existing
literature. Typically, achieving weak approximation results requires bounding the (high-order) derivatives
of the drift and diffusion terms of the SDE. While this condition is readily met for existing SMEs, ensuring
boundedness for HA-SME presents a challenge, as the drift and diffusion terms in HA-SME are defined as limits
of power series and involve logarithmic components. Our proof builds upon the following lemma.
Lemma 5.1 (Regularity of the drift term and diffusion coefficient). Consider a fixed n 0. Assume for
that any ξ, we have F( ·;ξ)
∈
C bn+2(Rd). T (cid:13)hen, there exist (cid:13)s a constant η
0
> 0 such that,≥ for any η < η 0,
max [b(x)] < and max (cid:13)[D(x)D(x)⊺ ] (cid:13) < .
0 ≤i ≤d ∥ i ∥Cn ∞ 0 ≤i,j ≤d(cid:13) i,j(cid:13) Cn ∞
Remark 5.1. A similar proof shows that SPF also admits the order 2 weak approximation error. Although
SME-2, SPF, and HA-SME share the same order of weak approximation error, as demonstrated in Section 6,
these models can have drastically different behaviors near critical points. This discrepancy between the
practice and the above theory arises because the classical analysis solely emphasizes the dependence on the
stepsize, while neglecting other crucial factors, such as the norm of the Hessian matrix. To better differentiate
these models, a more fine-grained analysis is necessary, where the dependence on the problem-dependent
parameters is explicitly accounted for.
5.2 Fine-Grained Error Analysis with Hessian Dependence
Our subsequent analysis explicitly examines the dependence of the error on stepsize η, Lipschitz parameter
ws e:= dis ffu ep rx e∈nR td ia∥t∇e f th( ex) a∥p, pa rn od ximsm ao tio ot nhn ge us as rap na tr ea em se ot fer HAλ -S:= MEsu frp ox m∈R Sd M(cid:13) (cid:13) E∇-22f a( nx d)(cid:13) (cid:13) S. PT F,h hr io gu hg lh ighth tii ns gd ie tt sa ail ded vaa nn taa gly es ii ns,
modeling SGD.
(cid:0) (cid:1)
Theorem 5.2. Assume for any ξ, we have F(;ξ) C8 Rd , and for any 0 i,j d, we have [Σ()]
(cid:0) (cid:1) · ∈ b ≤ ≤ · i,j ∈
C b6 Rd . Let X(t) be the stochastic process described by HA-SME and {x
k
}
be the sequence generated by SGD.
There exists η
0
>0 such that for any η <η
0
and T >0, it holds that for all x Rd,
∈
sup E[u(x k) x
0
=x] E[u(X(kη)) X(0)=x] (cid:0)(cid:0) η2s3+η3s4λ3(cid:1) M(T)(cid:1) , (27)
| | − | |≤O
k=1,..., T/η
⌊ ⌋
where for any p ≥1, M(T):=η(cid:80) ⌊ kT =/ 0η ⌋−1(cid:80) 1 J 8(cid:12) (cid:12)DJuk(x)(cid:12) (cid:12) with uk(x)=E[u(x k) |x 0 =x].
≤| |≤ ∞
Note that M(T) characterizes how the regularity of the test function u deteriorates along the SGD
trajectory. By definition, this quantity is solely determined by the SGD dynamics, thus independent of the
SDE models.
Similarly, we derive the following results for SME-2 and SPF.
12Theorem 5.3. Under the same settings as Theorem 5.2,
1. when X(t) is described by SME-2, it holds that for all x Rd,
∈
sup E[u(x k) x
0
=x] E[u(X(kη)) X(0)=x]
(cid:0)(cid:0) η2(cid:0) s3+sλ2(cid:1) +η3s4λ3(cid:1) M(T)(cid:1)
,
| | − | |≤O
k=1,..., T/η
⌊ ⌋
2. when X(t) is described by SPF, it holds that for all x Rd,
∈
sup E[u(x k) x
0
=x] E[u(X(kη)) X(0)=x]
(cid:0)(cid:0) η2(cid:0) s3+λ(cid:1) +η3s4λ3(cid:1) M(T)(cid:1)
.
| | − | |≤O
k=1,..., T/η
⌊ ⌋
The above theorems implies that HA-SME demonstrates a clear improvement in terms of dependency on λ.
In particular, it eliminates the dependence on λ in the leading error term involving η2.
As is common in the finite-time approximation error analysis of SDE [Feng et al., 2017, Hu et al., 2019,
Li et al., 2017], we point out that M(T) could exhibit exponential growth in terms of T and λ in general.
However, in cases when the function is convex, the following result shows that these constants remain
independent of λ.
(cid:0) (cid:1) ⊺
Lemma 5.2. Assuming that for any ξ, F(;ξ) C9 Rd is convex, i.e., F(y;ξ) F(x;ξ) F(x;ξ) (y x)
· ∈ b − ≥∇ −
for any x,y Rd. There exists a constant η
0
such that, for any η <η 0, it holds that
∈
M(T) ( u ),
≤O ∥ ∥C8
where M(T) is defined in Theorem 5.2, and () hides terms that do not depend on η, λ or s.
O ·
The above lemma allows us to develop the following approximation guarantee.
(cid:0) (cid:1) (cid:0) (cid:1)
Theorem 5.4. Fixing T >0, assume that the test function satisfies u C8 Rd , F(;ξ) C9 Rd is convex
(cid:0) (cid:1) ∈ b · ∈ b
for any ξ, and for any 0 i,j d, [Σ()] C6 Rd . Let X(t) be the stochastic process described by
≤ ≤ · i,j ∈ b
HA-SME and x be the sequence generated by SGD. There exists a constant η >0 such that for any η <η ,
k 0 0
{ }
it holds that for all x Rd,
∈
sup E[u(x k) x
0
=x] E[u(X(kη)) X(0)=x]
(cid:0) η2s3(cid:1)
.
| | − | |≤O
k=1,..., T/η
⌊ ⌋
FollowingasimilarprooftogetherwithTheorem5.3,wecanestablishthattheupperboundsforSME-2and
SPF are (cid:0) η2(cid:0) s3+sλ2(cid:1)(cid:1) and (cid:0) η2(s3+λ)(cid:1), respectively. This difference partially explains the limitations
of existinOg SDE models in captOuring the escape dynamics as illustrated in Figures 1 and 2.
Intuitively, the advantage of HA-SME is inherent in its construction by SBEA. Even under Principle 3 of
term selection, HA-SME incorporates infinite sequences of terms associated with λ, (cid:8)(cid:0) 2f(x)(cid:1)p(cid:9) ∞ , in both
∇ p=0
its drift and diffusion terms. This is in stark contrast to SME-1, SME-2, and SPF, all of which truncate the
error series at a certain point. All three of these models can be recovered by SBEA with a proper truncated
series of g and h : SME-1 includes g , h , and h ; SME-2 includes g , g , h , and h ; and SPF includes
p p 0 0 1 0 1 0 1
g , h{ ,}and h{. W}e conjecture that the λ-dependence in the error analysis for SME-2 and SPF cannot be
{imi p}r∞i o= v0 ed.0
This
lim1
itation arises because error terms that exhibit such λ-dependence are inherently truncated
during their construction within the SBEA framework.
Remark 5.2. Since Theorem 5.4 addresses the global point-wise approximation, the dependence on the global
gradient norm of f appears unavoidable and cannot be neglected. However, to study the escaping behavior
of SGD near a critical point, we believe that the global gradient norm s can be relaxed to its local version.
This would be an interesting result because, in the vicinity of a critical point of a smooth function, the local
gradient norm can be regarded as a negligible constant. Consequently, the leading term in Theorem 5.4 would
be independent of both s and λ (whereas errors of existing SDE proxies always depend on λ).
13Remark 5.3. If we further assume that f(,ξ) is strongly-convex for any ξ, then M(T) can be shown to
be uniformly bounded w.r.t. T (the proo·f is provided in Appendix B.1), leading to a uniform-in-time
approximation guarantee3. A similar guarantee for SME-2 was established in the literature under the strong-
convexity assumption [Li and Wang, 2022], but without considering the explicit dependence on the problem
parameters s and λ.
6 Exact Recovery of SGD by HA-SME on Quadratics
The quadratic objective, despite its simplicity, holds significance in studying the behaviors of SGD. Its
relevance comes from not only its application in linear regression but also its use in modeling the local
curvature of complex models through second-order Taylor expansions. Moreover, insights from Neural
Tangent Kernel [Arora et al., 2019, Jacot et al., 2018] suggests that in regression tasks, the objectives of
sufficiently wide neural networks closely resemble quadratic functions. The convergence of SGD on quadratic
models has been extensively studied, for example, for constant stepsizes [Dieuleveut et al., 2020] and stepsize
schedulers [Ge et al., 2019, Pan et al., 2021]. Additionally, the exploration of locally escaping behaviors of
SGD often relies on a local quadratic assumption [Hu et al., 2019, Ibayashi and Imaizumi, 2023, Xie et al.,
2020, 2022, Zhu et al., 2019].
In this section, we aim to compare different SDEs in terms of their ability to approximate SGD on
quadratic functions. In this context, We first discuss the failure cases of existing SDE proxies (including SPF
in Equation (26)) and then illustrate how HA-SME can accurately match SGD under certain conditions.
6.1 Failure Cases for Existing SDEs
We now offer a theoretical justification for the phenomena observed in Figures 1 and 2 — specifically, why
SME-1, SME-2, and SPF fail in modeling SGD on quadratic functions. We restrict the noise in this analysis to
be additive and state-independent.
Assumption 6.1 (quadratic objecive4). The objective function f is defined as f(x) = 1x⊺ Ax, where
⊺ 2
A Rd ×d is a symmetric real matrix. We denote its eigen-decomposition by A=UΛU , where U Rd ×d
∈ ∈
satisfies U ⊤U =I
d
and Λ Rd ×d is diagonal.
∈
Assumption 6.2 (state-independent noise). The stochastic gradient satisfies
F(x,ξ)= f(x)+ξ, ξ (0,Σ),
∇ ∇ ∼N
where Σ is a constant positive semi-definite matrix.
With state-independent noise, the aforementioned SDEs applied to quadratics can be represented as
Ornstein–Uhlenbeck (OU) processes. For simplicity, this subsection considers isotropic noise, i.e. Σ=σ2I for
some scalar σ. We show that even within this highly simplified noise setting, existing SDEs may struggle
to accurately capture the escaping behaviors exhibited by SGD. The following proposition computes the
distributions of the iterates of SGD and the SDE proxies.
Proposition 6.1. Under Assumptions 6.1 and 6.2 with Σ=σ2I for some scalar σ, it holds that
1. The iterates of SGD (Equation (2)) with stepsize η η satisfy
k
≡
(cid:32) (cid:33)
k 1
x U(I
ηΛ)kU⊺
x , η2σ2
(cid:88)−
U(I
ηΛ)2mU⊺
.
k 0
∼N − −
m=0
3Ourresultassumesuniformlyboundedgradients,whichmaynotholdforstrongly-convexfunctions. However,thiscanbe
alleviatedbyrestrictingourdiscussiontoacompactset,andmodifyingHA-SMEtopreventitfromescapingthisset,akintothe
approachbyLiandWang[2022].
4Wefocusonthesimplyquadratic 1x⊺ Ax,whilenotingthatallresultsinthissectioncaneasilygeneralizetoanyquadratic
functionoftheform
1x⊺
Ax+bx+c.
2
2
142. The solution of SME-1 (Equation (6)) satisfies
(cid:18) (cid:19)
I exp( 2Λt) ⊺
X(x ,t) exp( At)x , ησ2U − − U .
0 0
∼N − 2Λ
3. The solution of SME-2 (Equation (3)) satisfies
(cid:32) (cid:0) (cid:0) (cid:1) (cid:1) (cid:33)
(cid:16) (cid:16) η (cid:17) (cid:17) I exp 2Λ+ηΛ2 t ⊺
X(x ,t) exp A+ A2 t x , ησ2U − − U .
0 ∼N − 2 0 2Λ+ηΛ2
4. The solution of SPF (Equation (26)), if η <1/ A , satisfies
∥ ∥
(cid:32) (cid:33)
X(x ,t) U(1
ηΛ)t/ηU⊺
x ,
η2σ2U1 −(1 −ηΛ)2t/η U⊺
.
0 0
∼N − 2log(1 ηΛ)
− −
Now we delve into the analysis of different escaping scenarios.
Escaping Saddle Points in Figure 1a Consider the function defined in Figure 1a, i.e., f([x,y]⊺ ) =
1(cid:0) x2 y2(cid:1), where the saddle point is at (0,0). According to Proposition 6.1, along the direction of y, the
2 −
iterate x k generated by SGD has a mean of (1+ηλ 2)ky 0 and a variance of η2σ2(cid:80)k m− =1 0(1+ηλ 2)2m, which
drifts away from 0 for any η > 0. Thus, SGD can successfully escape for any stepsize η > 0. However,
for SME-2, when η 2, as t , its stationary distribution is a Gaussian distribution with mean 0 and
covariance ησ2(cid:0) 2Λ≥ +ηΛ2(cid:1) −1→ . I∞ n such a regime of constant stepsize, the dynamics of SME-2 converges to
a stationary distribution around 0, while SGD escapes exponentially fast. This observation confirms the
numerical simulation presented in Figure 1a.
Escaping Minimum in Figure 1b Consider the objective f([x,y]⊺ )= 1(cid:0) x2+y2(cid:1) where all eigenvalues
2
of the Hessian are positive. From Proposition 6.1, SGD will escape from (0,0) if η >2 — both the mean and
covariance of the iterates grow exponentially. However, for SME-1 and SME-2, their stationary distributions are
zero-mean Gaussians with covariance ησ2U(2Λ) −1U⊺ and ησ2(cid:0) 2Λ+ηΛ2(cid:1) −1, respectively. This elucidates
why, in Figure 1b, their dynamics oscillate around the minimum.
Escaping Behavior on Bimodal Function in Figure 2 Consider the one-dimensional piece-wise
quadratic function depicted in Figure 2a. When we initialize the point within the range [ 1,1], the local
function is quadratic. At least for the first step, the behaviors of SGD and SPF should be accu−rately predicted
by Proposition 6.1. We consider small stepsize close to 1/ 2f(x) (which is 1 in this case). Due to the
additive noise, the iterates of SGD always have a variance n∥o∇smaller∥than η2σ2. Such a variance allows the
iterates to escape from the local basin and reach the basin around the global minimum at x=5. However,
after the first step, SPF would have a mean around 0 and variance also near 0 after time η. It can be shown
b exy ps lu aib ns sti wtu ht yin Sg Pt F= teη n, ds set tt oin sg taη yΛ clc ol so es le yt ao ro1 ui nn dP tr ho epo ins ii tti io an
l
m6. i1 nia mn ud mby inn oo uti rng simth ua lt atl ii om nz ,→a1 s1 sl− o hg( o(1 w1− −nz z) )2 in= F0 ig. uT rehi 2s
.
This phenomenon underscores the need for a more nuanced correction of the diffusion term to accurately
model the escaping behaviors of SGD in such settings.
6.2 Hardness of Approximating SGD with OU Process on Quadratics
Ideally, we would like to approximate SGD on quadratics exactly using SDEs, i.e., the iterates of SGD
share the same distribution as its continuous-time counterpart at time stamps kη for any natural number k.
However, this is not possible for general quadratic objective with arbitrary gradient noises, even if they are
additive and state-independent Gaussian noises. We provide a hard instance, demonstrating that no OU
15process (even extended to complex domain as elaborated below) can achieve this. We focus on the class of
OU process because the corresponding transition kernel is Gaussian, which matches the transition kernel of
SGD (when viewed as a Markov chain) under Assumptions 6.1 and 6.2.
Complex OU Process We note that the iterates of SGD in this context is always real-valued and follows
a Gaussian distribution, as outlined in Proposition 6.15. However, to allow the continuous-time proxy to have
the maximum capability of approximating SGD, we allow it to be complex-valued. The benefit of extending
to complex space has been observed by Rosca et al. [2022] when matching the dynamics of GD by PF.
A random vector, denoted by z = x+iy, is a complex Gaussian random vector if x and y are two
(possibly correlated) real Gaussian random vectors. In the real-valued case, a Gaussian random vector can be
characterized by its mean and covariance, while in the complex-valued case, one additional parameter called
(cid:104) (cid:105)
pseudo-covariance is involved. Formally, the covariance is defined as Γ(z)=E (z E[z])(z E[z])H and
− −
(cid:104) ⊺(cid:105)
pseudo-covariance is defined as C(z)=E (z E[z])(z E[z]) . We refer to Appendix C.1 for a detailed
− −
discussion of properties of complex Gaussian random vectors.
We now state the desideratum when matching the distribution of a real normal variable, denoted as z,
(cid:101)
and a complex normal variable z.
Desideratum 1. The distribution of a complex Gaussian random vector z is said to match the distribution
of real normal variable z if the following conditions hold:
(cid:101)
E[Re(z)]=E[z (cid:101)], E[Im(z)]=0, Γ(z)=C(z)=Cov[z (cid:101),z (cid:101)].
The above result is equivalent to the statement: z and Re(z) have the same distribution, and Im(z) have 0
(cid:101)
mean and 0 variance. Moreover, consider the setting described by Assumptions 6.1 and 6.2 so that the SGD
iterates are all real Gaussian random vectors. A complex OU process
dX =BX dt+DdW ,
t t t
is said to match the iterates of the SGD if the distributions of X matches those of the SGD iterates at all
t
corresponding time stamps, i.e. t=kη where k is the iteration index of SGD. Here B Cd ×d, D Cd ×m
∈ ∈
and W represents an m-dimensional standard Wiener process.
t
Proposition 6.2. Consider the setting described by Assumptions 6.1 and 6.2. One can construct real
matrices A and Σ such that, for any given stepsize η >0, there exists no complex OU process6 that matches
the iterates of SGD, in the sense of Desideratum 1.
The hard instance is constructed by choosing a full-rank matrix A and a degenerate covariance matrix
Σ such that the eigenvectors of Σ are not aligned with those of A. The distributional mismatch of the
SDE and SGD can be intuitively understood through the following argument: Starting from a deterministic
initialization, after one SGD step, the covariance of the iterates is η2Σ, which is rank-deficient; In contrast, in
continuous-timedynamics,thenoiseinjectedisrotatedbythemisalignedlineartransformationsfromthedrift
term and the covariance matrix quickly becomes full-rank. Please find the rigorous proof in Appendix C.4.
6.3 Exact Approximation From HA-SME
While in general, complex OU process cannot exactly model SGD on quadratic functions, HA-SME when
extended to complex-valued, achieves an exact match when the matrices A and Σ commute or when the
5Thisproposition,whileinitiallyframedforisotropicnoises,canbeeasilygeneralizedtoanisotropicnoises.
6InourformulationofcomplexOUprocess,weuserealBrownianmotionWt. Itshouldbenoted,however,thattheframework
canalsoincludecomplexBrownianmotion[Perret,2010]. ConsideracomplexBrownianmotiondefinedbyW(cid:102)t=W tr+iW ti,
w behe er qe uW ivatr lea nn td lyW exti pa rer se setw do asin dd Xep te =nd Ben Xt tr de ta +lB Dro (cid:2)w Inia in I(cid:3)m (cid:2)o dt Wio tn rs d. WC to i(cid:3)n ⊺s .equently,anySDEoftheformdXt=BXtdt+DdW(cid:102)t can
16stepsize is small enough. These conditions are slightly weaker than the sufficient conditions for the existence
of our HA-SME (Lemma 4.2).
The following lemma outline conditions for a complex OU process to match SGD on quadratics.
Lemma 6.1. Consider the same setting as Proposition 6.2, and denote the diagonal elements of Λ as
λ 1,λ 2,...,λ d. After time kη for k 0, the mean of X(kη) equals the mean of x k, i.e., E[X(kη)]=E[x k], if
and only if B =Ulog(1 −ηΛ)U⊺ . In a≥ ddition,
η
1. the covariance of X(kη) equals the covariance of x if and only if for all 0 i,j d,
k
≤ ≤
(cid:16) (cid:17)
⊺
(cid:104)(cid:0) U⊺ D(cid:1)(cid:0) U⊺ D(cid:1)H(cid:105) = [U ΣU] i,j log(1 −ηλ i)+log(1 −ηλ j) . (28)
i,j ηλ iλ j (λ i+λ j)
−
2. pseudo-covariance of X(kη) equals the covariance of x if and only if for all 0 i,j d,
k
≤ ≤
⊺
(cid:104)(cid:0) U⊺ D(cid:1)(cid:0) U⊺ D(cid:1)⊺(cid:105) = [U ΣU] i,j(log(1 −ηλ i)+log(1 −ηλ j)) . (29)
i,j ηλ iλ j (λ i+λ j)
−
Satisfying any of the existence conditions for HA-SME in Lemma 4.2 ensures the existence of matrix D
that adheres to the two required conditions in Lemma 6.1. Under these existence conditions, the matrices,
whose elements are defined in the RHS of Equation (28) and Equation (29), become the same real positive
semi-definitematrix. Bytakingthesquarerootsofthismatrix’seigenvalues, wecanconstructD thatsatisfies
both Equations (28) and (29), thereby achieving an exact match for SGD on quadratic functions.
Theorem 6.1. Under Assumptions 6.1 and 6.2, the solution of HA-SME exactly matches the iterates of SGD
if either of the following two conditions holds:
1. A and Σ commute, and ηλ =1 for all eigenvalues λ of A.
i i
̸
(cid:110) (cid:113) (cid:111)
2. Σ is positive definite, and η 1 min 1 1 λmin(Σ) ,1 √2 .
≤ ∥A
∥
− − √dλmax(Σ) − 2
Tothebestofourknowledge,thismarksthefirstinstanceofanSDEthatpreciselymirrorsthedistribution
of SGD, albeit restricted to quadratic functions.
Remark 6.1. The first condition in this theorem relaxes the constraints from Lemma 4.2 to allow larger
stepsizes. As noted in Rosca et al. [2022], complex flow is helpful for capturing the instabilities caused by
large stepsizes in GD on quadratics. This is also the case for HA-SME. When η <1/ A , HA-SME operates in
real space. However, when η >1/ A , imaginary components emerge due to the lo∥gar∥ithmic function.
∥ ∥
7 Conclusion and Future Work
In this work, we present HA-SME, an advancement over existing SDEs for approximating the dynamics of
SGD. Specifically, HA-SME offers improved theoretical approximation guarantees for general smooth objectives
over current SDEs in terms of the dependence on the smoothness parameter of the objective. For quadratic
objectives, HA-SME exactly recovers the dynamics of SGD under mild conditions. The primary innovation lies
in integrating Hessian information into both the drift and diffusion terms of the SDE, achieved by extending
backward error analysis to the stochastic setting. This integration preserves the interplay of noise and local
curvature in approximating SGD, allowing to better capture its escaping behaviors.
In the future, it would be interesting to further analyze the escaping behaviors of HA-SME. We anticipate
that this will provide a more accurate characterization of SGD’s exit time of a local region with respect
to the dependence on the Hessian. Second, applying our SBEA to other optimization algorithms, such as
momentum and adaptive gradient methods would be intriguing directions for further research.
17References
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact
computation with an infinitely wide neural net. In Advances in Neural Information Processing Systems,
volume 32, 2019. (Cited on page 14.)
Hedy Attouch, Zaki Chbani, Jalal Fadili, and Hassan Riahi. First-order optimization algorithms via inertial
systems with Hessian driven damping. Mathematical Programming, pages 1–43, 2022. (Cited on page 4.)
David GT Barrett and Benoit Dherin. Implicit gradient regularization. arXiv preprint arXiv:2009.11162,
2020. (Cited on page 4.)
Yoshua Bengio. Practical recommendations for gradient-based training of deep architectures. In Neural
Networks: Tricks of the Trade: Second Edition, pages 437–478. Springer, 2012. (Cited on page 1.)
Anthony Bloch. Hamiltonian and gradient flows, algorithms and control, volume 3. American Mathematical
Soc., 1994. (Cited on page 4.)
Odile Brandière and Marie Duflo. Les algorithmes stochastiques contournent-ils les pièges ? Annales de
l’I.H.P. Probabilités et statistiques, 32(3):395–427, 1996. (Cited on pages 2 and 4.)
Andrew A Brown and Michael C Bartholomew-Biggs. Some effective methods for unconstrained optimization
based on the solution of systems of ordinary differential equations. Journal of Optimization Theory and
Applications, 62:211–224, 1989. (Cited on page 4.)
Augustin Louis Baron Cauchy. Analyse algébrique, volume 1. Debure, 1821. (Cited on page 44.)
Arnaud Debussche and Erwan Faou. Weak backward error analysis for SDEs. SIAM Journal on Numerical
Analysis, 50(3):1735–1752, 2012. (Cited on page 7.)
Aymeric Dieuleveut, Alain Durmus, and Francis Bach. Bridging the gap between constant step size stochastic
gradient descent and Markov chains. The Annals of Statistics, 48(3):1348 – 1382, 2020. doi: 10.1214/
19-AOS1850. (Cited on page 14.)
Yoel Drori and Ohad Shamir. The complexity of finding stationary points with stochastic gradient descent.
In International Conference on Machine Learning, 2020. (Cited on page 1.)
Cong Fang, Zhouchen Lin, and Tong Zhang. Sharp analysis for nonconvex sgd escaping from saddle points.
In Conference on Learning Theory, 2019. (Cited on page 1.)
Yuanyuan Feng, Lei Li, and Jian-Guo Liu. Semi-groups of stochastic gradient descent and online principal
component analysis: properties and diffusion approximations. arXiv preprint arXiv:1712.06509, 2017.
(Cited on pages 12, 13, 38, 39, and 48.)
Yuanyuan Feng, Tingran Gao, Lei Li, Jian-Guo Liu, and Yulong Lu. Uniform-in-time weak error analysis for
stochastic gradient descent algorithms via diffusion approximation. arXiv preprint arXiv:1902.00635, 2019.
(Cited on page 7.)
Simone Fiori and Yoshua Bengio. Quasi-geodesic neural learning algorithms over the orthogonal group: A
tutorial. Journal of Machine Learning Research, 6(5), 2005. (Cited on page 4.)
Xavier Fontaine, Valentin De Bortoli, and Alain Durmus. Convergence rates and approximation results for
SGD and its continuous-time counterpart. In Conference on Learning Theory, 2021. (Cited on page 4.)
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic gradient
for tensor decomposition. In Conference on Learning Theory, 2015. (Cited on page 4.)
18Rong Ge, Sham M Kakade, Rahul Kidambi, and Praneeth Netrapalli. The step decay schedule: A near
optimal,geometricallydecayinglearningrateprocedureforleastsquares. InAdvancesinNeuralInformation
Processing Systems, volume 32, 2019. (Cited on page 14.)
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013. (Cited on page 1.)
Ernst Hairer, Marlis Hochbruck, Arieh Iserles, and Christian Lubich. Geometric numerical integration.
Oberwolfach Reports, 3(1):805–882, 2006. (Cited on pages 5 and 6.)
Uwe Helmke and John B Moore. Optimization and dynamical systems. Springer Science & Business Media,
2012. (Cited on page 4.)
EinarHilleandRalphSaulPhillips. Functional analysis and semi-groups,volume31. AmericanMathematical
Soc., 1996. (Cited on page 7.)
Wenqing Hu, Chris Junchi Li, Lei Li, and Jian-Guo Liu. On the diffusion approximation of nonconvex
stochastic gradient descent. Annals of Mathematical Sciences and Applications, 4(1):3–32, 2019. (Cited on
pages 2, 4, 7, 12, 13, and 14.)
Hikaru Ibayashi and Masaaki Imaizumi. Why does SGD prefer flat minima?: Through the lens of dynamical
systems. In When Machine Learning meets Dynamical Systems: Theory and Applications, 2023. (Cited on
pages 2, 5, 11, and 14.)
Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization
in neural networks. In Advances in Neural Information Processing Systems, volume 31, 2018. (Cited on
page 14.)
Stanisław Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and
Amos Storkey. Three factors influencing minima in SGD. arXiv preprint arXiv:1711.04623, 2017. (Cited
on pages 4 and 11.)
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points
efficiently. In International Conference on Machine Learning, 2017. (Cited on page 4.)
NitishShirishKeskar,DheevatsaMudigere,JorgeNocedal,MikhailSmelyanskiy,andPingTakPeterTang. On
large-batch training for deep learning: Generalization gap and sharp minima. In International Conference
on Learning Representations, 2017. (Cited on page 2.)
AhmedKhaledandPeterRichtárik. BettertheoryforSGDinthenonconvexworld. Transactions on Machine
Learning Research, 2023. ISSN 2835-8856. (Cited on page 1.)
P.E. Kloeden and E. Platen. Numerical Solution of Stochastic Differential Equations. Stochastic Modelling
and Applied Probability. Springer Berlin Heidelberg, 2011. (Cited on page 7.)
Walid Krichene, Alexandre Bayen, and Peter L Bartlett. Accelerated mirror descent in continuous and
discrete time. Advances in Neural Information Processing Systems, 2015. (Cited on page 4.)
Lei Li and Yuliang Wang. On uniform-in-time diffusion approximation for stochastic gradient descent. arXiv
preprint arXiv:2207.04922, 2022. (Cited on pages 7 and 14.)
Qianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and adaptive stochastic gradient
algorithms. In International Conference on Machine Learning, 2017. (Cited on pages 2, 4, 7, 12, and 13.)
Xiaoyu Li and Francesco Orabona. A high probability analysis of adaptive sgd with momentum. arXiv
preprint arXiv:2007.14294, 2020. (Cited on page 1.)
19Tsung-Yu Lin and Subhransu Maji. Improved bilinear pooling with CNNs. arXiv preprint arXiv:1707.06772,
2017. (Cited on page 35.)
Jun Liu and Ye Yuan. Almost sure saddle avoidance of stochastic gradient methods without the bounded
gradient assumption. arXiv preprint arXiv:2302.07862, 2023. (Cited on pages 2 and 4.)
Stephan Mandt, Matthew D Hoffman, David M Blei, et al. Continuous-time limit of stochastic gradient
descent revisited. In OPT workshop, NIPS, 2015. (Cited on page 4.)
Stephan Mandt, Matthew Hoffman, and David Blei. A variational analysis of stochastic gradient algorithms.
In International Conference on Machine Learning, 2016. (Cited on page 4.)
Panayotis Mertikopoulos, Nadav Hallak, Ali Kavis, and Volkan Cevher. On the almost sure convergence
of stochastic gradient descent in non-convex problems. In Advances in Neural Information Processing
Systems, volume 33, 2020. (Cited on pages 2 and 4.)
Grigorii Noikhovich Milstein. Numerical integration of stochastic differential equations, volume 313. Springer
Science & Business Media, 2013. (Cited on page 7.)
Takashi Mori, Liu Ziyin, Kangqiao Liu, and Masahito Ueda. Power-law escape rate of SGD. In International
Conference on Machine Learning, 2022. (Cited on pages 4 and 11.)
Michael Muehlebach and Michael I Jordan. A dynamical systems perspective on Nesterov acceleration. In
International Conference on Machine Learning, 2019. (Cited on page 4.)
Michael Muehlebach and Michael I Jordan. Optimization with momentum: Dynamical, control-theoretic,
and symplectic perspectives. Journal of Machine Learning Research, 22(73):1–50, 2021. (Cited on page 4.)
Kamil Nar and Shankar Sastry. Step size matters in deep learning. In Advances in Neural Information
Processing Systems, volume 31, 2018. (Cited on page 1.)
Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, and Nati Srebro. Exploring generalization in
deep learning. In Advances in Neural Information Processing Systems, volume 30, 2017. (Cited on page 2.)
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to spectrally-
normalized margin bounds for neural networks. In International Conference on Learning Representations,
2018. (Cited on page 2.)
Thanh Huy Nguyen, Umut Simsekli, Mert Gurbuzbalaban, and Gaël Richard. First exit time analysis of
stochasticgradientdescentunderheavy-tailedgradientnoise. InAdvances in Neural Information Processing
Systems, volume 32, 2019. (Cited on pages 2 and 4.)
Antonio Orvieto and Aurelien Lucchi. Continuous-time models for stochastic optimization algorithms. In
Advances in Neural Information Processing Systems, volume 32, 2019. (Cited on page 4.)
Rui Pan, Haishan Ye, and Tong Zhang. Eigencurve: Optimal learning rate schedule for SGD on quadratic
objectives with skewed hessian spectrums. arXiv preprint arXiv:2110.14109, 2021. (Cited on page 14.)
CourtneyPaquetteandElliotPaquette. Dynamicsofstochasticmomentummethodsonlarge-scale, quadratic
models. In Advances in Neural Information Processing Systems, volume 34, 2021. (Cited on page 4.)
Courtney Paquette, Elliot Paquette, Ben Adlam, and Jeffrey Pennington. Homogenization of SGD in
high-dimensions: Exact dynamics and generalization properties. arXiv preprint arXiv:2205.07069, 2022.
(Cited on pages 4 and 11.)
Robin Pemantle. Nonconvergence to unstable points in urn models and stochastic approximations. The
Annals of Probability, 18(2):698–712, 1990. (Cited on pages 2 and 4.)
20Christian Perret. The stability of numerical simulations of complex stochastic differential equations. PhD
thesis, ETH Zurich, 2010. (Cited on page 16.)
Herbert Robbins and Sutton Monro. A Stochastic Approximation Method. The Annals of Mathematical
Statistics, 22(3):400 – 407, 1951. doi: 10.1214/aoms/1177729586. (Cited on page 1.)
Mihaela Rosca, Yan Wu, Chongli Qin, and Benoit Dherin. On a continuous time model of gradient descent
dynamics and instability in deep learning. Transactions on Machine Learning Research, 2022. (Cited on
pages 4, 5, 6, 8, 16, 17, 27, 33, and 61.)
Peter Ross. Generalized hockey stick identities and n-dimensional blockwalking. The College Mathematics
Journal, 28(4):325, 1997. (Cited on page 30.)
Simo Särkkä and Arno Solin. Applied stochastic differential equations, volume 10. Cambridge University
Press, 2019. (Cited on pages 7, 51, and 56.)
Johannes Schropp and I Singer. A dynamical systems approach to constrained minimization. Numerical
functional analysis and optimization, 21(3-4):537–551, 2000. (Cited on page 4.)
Othmane Sebbouh, Robert M Gower, and Aaron Defazio. Almost sure convergence rates for stochastic
gradient descent and stochastic heavy ball. In Conference on Learning Theory, 2021. (Cited on page 1.)
Boris V Shabat. Introduction to complex analysis. Part II, translations of mathematical monographs, vol.
110. American Mathematical Society, Providence, RI, 1992. (Cited on page 46.)
Tony Shardlow. Modified equations for stochastic differential equations. BIT Numerical Mathematics, 46:
111–125, 2006. (Cited on page 9.)
Bin Shi, Simon S Du, Michael I Jordan, and Weijie J Su. Understanding the acceleration phenomenon via
high-resolution differential equations. Mathematical Programming, pages 1–70, 2021. (Cited on page 4.)
Willi-Hans Steeb and Yorick Hardy. Matrix calculus and Kronecker product: a practical approach to linear
and multilinear algebra. World Scientific Publishing Company, 2011. (Cited on page 35.)
WeijieSu,StephenBoyd,andEmmanuelJCandes. AdifferentialequationformodelingNesterov’saccelerated
gradient method: Theory and insights. Journal of Machine Learning Research, 17(153):1–43, 2016. (Cited
on page 4.)
Flemming Topsøe. Some bounds for the logarithmic function. Inequality theory and applications, 4:137, 2007.
(Cited on page 50.)
Mingze Wang and Lei Wu. A theoretical analysis of noise geometry in stochastic gradient descent. arXiv
preprint arXiv:2310.00692, 2024. (Cited on page 11.)
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal value of
adaptive gradient methods in machine learning. In Advances in Neural Information Processing Systems,
volume 30, 2017. (Cited on page 1.)
Zeke Xie, Issei Sato, and Masashi Sugiyama. A diffusion theory for deep learning dynamics: Stochastic
gradientdescentexponentiallyfavorsflatminima. InInternational Conference on Learning Representations,
2020. (Cited on pages 2, 4, 11, and 14.)
Zeke Xie, Xinrui Wang, Huishuai Zhang, Issei Sato, and Masashi Sugiyama. Adaptive inertia: Disentangling
the effects of adaptive learning rate and momentum. In International Conference on Machine Learning,
2022. (Cited on pages 2, 11, and 14.)
21Junchi Yang, Xiang Li, Ilyas Fatkhullin, and Niao He. Two sides of one coin: the limits of untuned SGD and
the power of adaptive methods. Advances in Neural Information Processing Systems, 36, 2024. (Cited on
page 1.)
ChiyuanZhang,SamyBengio,MoritzHardt,BenjaminRecht,andOriolVinyals. Understandingdeeplearning
(still) requires rethinking generalization. Commun. ACM, 64(3):107–115, feb 2021. doi: 10.1145/3446776.
(Cited on page 2.)
Yi Zhou, Yingbin Liang, and Huishuai Zhang. Understanding generalization error of SGD in nonconvex
optimization. Machine Learning, 111(1):345–375, 2022. doi: 10.1007/s10994-021-06056-w. (Cited on
page 1.)
Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochastic gradient
descent: Its behavior of escaping from sharp minima and regularization effects. In International Conference
on Machine Learning, 2019. (Cited on pages 2, 4, 11, and 14.)
22Contents
1 Introduction 1
1.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2 Preliminaries 5
2.1 Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.2 Backward Error Analysis (BEA) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.3 Principle Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3 Stochastic Backward Error Analysis 6
3.1 Differences between BEA and SBEA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.2 Procedures in SBEA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
3.3 Exact Matching Fails for Order η3 in SBEA . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4 Hessian-Aware Stochastic Modified Equation 9
5 Approximation Error Analysis of HA-SME 12
5.1 Weak Approximation Error Guarantee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
5.2 Fine-Grained Error Analysis with Hessian Dependence . . . . . . . . . . . . . . . . . . . . . . 12
6 Exact Recovery of SGD by HA-SME on Quadratics 14
6.1 Failure Cases for Existing SDEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
6.2 Hardness of Approximating SGD with OU Process on Quadratics . . . . . . . . . . . . . . . . 15
6.3 Exact Approximation From HA-SME . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
7 Conclusion and Future Work 17
A Construction of HA-SME 24
A.1 Proof Sketch of Lemma 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
A.2 Proof of Lemma A.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
A.2.1 Proof of Lemma A.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
A.2.2 Proof of the Functional Structures in Equations (33) and (34) . . . . . . . . . . . . . . 26
A.3 Proof of Lemma A.3 and the Recursive Expressions of c and a . . . . . . . . . . . . . 30
i i,j
A.4 Proof of Theorem 4.1. . . . . . . . . . . . . . . . . . .{. .}. . .{. . .}. . . . . . . . . . . . . . 33
A.5 Proof of the Well-Posedness of HA-SME . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
A.6 Helper Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
B Approximation Error Analysis 38
B.1 Proofs for Section 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
B.2 Helper Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
C Exact Match of SGD on Quadratics 49
C.1 Complex Normal Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
C.2 Helper Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
C.3 Proofs for Section 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
C.4 Proof for Proposition 6.2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
23A Construction of HA-SME
A.1 Proof Sketch of Lemma 4.1
We start with a sketch of the proof for Lemma 4.1, where we first determine the functional structure of the
componentsg ’sandh ’sintheSBEAansatzinEquation(11)andthendeterminetheirexactexpressionusing
i i
a generating function based approach. We prove via induction. The complexity in our analysis emerges from
identifying all possible terms generated by the expansion of in Equation (17). The collective
expansion of these operators unfolds as a multinomial series.
TLhl1 isLsl2 et·s··oLurln
work apart from the PF, as in the
latter, in the absence of diffusion terms, the expansion generates only a monomial. The rigorous proofs of the
statements made in the induction are provided afterwards.
Lemma 3.1 is the building block for our analysis, it can be seen from the following.
Proof for Lemma 3.1. In the semi-group expansion of Equation (16), consider the term 1ηn nu(x) for
1 n p+1. Since we already have ηn, to get ηp+1 , nu(x) must contribute ηp+1 n. Asn! cLontains ηi,
− i
we≤obt≤ain the conclusion. L L
Before providing the inductive proof, statement (1) in Lemma 4.1, i.e. the uniqueness of g and h , can
p p
be easily obtained from the following argument.
Proof of point (1) in Lemma 4.1. We first give examples for solving the first few g and h , then we use an
i i
argument of induction to finish the proof. For the terms associated with η0, Equation (13) and Equation (16)
already match, i.e., u(x). For terms associated with η, we can solve that g (x) = f(x) and h (x) = 0.
0 0
Considering terms with η2, we have for discrete-time (Equation (13)) −∇
η2 1 2u(x):(cid:0) f(x) f(x)⊺ +Σ(x)(cid:1) ,
· 2∇ ∇ ∇
and for continuous-time(Equation (16))
 
η2  g (x)⊺ u(x)+ 1 h (x): 2u(x)+1 ( f ( f u))(x) .
· 1 ∇ 2 1 ∇ 2 −∇ ·∇ −∇ ·∇ 
 
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
fromη u(x) from 1η2 2u(x)
L 2 L
Notethatthisholdsforanyproperu, sowegrouptermsassociatedwith uand 2uandmatchthetermsin
discrete-time and continuous-time. In this way, we obtain 2 equations, wh∇ich give∇s us the following g and h :
1 1
1
g (x)= 2f(x) f(x) and h (x)=Σ(x). (30)
1 1
−2∇ ∇
Now assume we already know g and h . We will proceed to solve g and h . This is done
{ i }1 i<p { i }1 i<p p p
by solving terms with η of order p+1≤. The terms a≤ssociated with ηp+1 are shown in Lemma 3.1. When
n=1, we will have linear terms with g and h , respectively. When n 2, since l +l + +l could only
p p 1 2 n
sum to p 1, we only have g and h for i<p. Given that we assume≥ g and h ··· are already
− i i { i }1 i<p { i }1 i<p
solved, by grouping terms with u and 2u, we obtain two linear equatio≤ns and can solv≤e for g and h .
p p
Note that the coefficients for g ∇and h i∇n these two linear equations are non-zero, therefore, the solution
p p
exists and is unique.
We now give a sketch of the inductive proof of the second statement in Lemma 4.1, on the functional
structure of the components h ’s and g ’s. Recall point (2) in Lemma 4.1: The components g and h admit
p p p p
the form
g (x)= c (cid:0) 2f(x)(cid:1)p f(x), (31)
p p
· ∇ ∇
24p 1
h p(x)= (cid:88)− a
k,p 1 k
(cid:0) 2f(x)(cid:1)k Σ(x)(cid:0) 2f(x)(cid:1)p −1 −k , (32)
− − · ∇ ∇
k=0
where c and a areconstantstobedetermined. LetqbetheinductionindexandletEquations(19)
k k,p 1 k
and (2{0) }be the{indu−ct−ion} hypothesis for q = p 1. Such a hypothesis clearly holds for q = 1 (recall the
calculation in Remark 3.2). The following lemma≥shows that the nice functional structures in Equations (19)
and (20) are preserved for q =p+1.
Lemma A.1. Under Principle 3, suppose that Equations (19) and (20) hold in step q =p. They also hold
for q =p+1.
Proof sketch. To establish the induction, we need to enumerate all possible terms generated by the expansion
in Equation (17). Viewing the application of each as a layer, this process entails a dual-stage selection
mechanism:
Lli
1. Within each layer we consider the terms generated from either ηlig
li
·∇
or η 2lih
li
: ∇2 in Lli.
2. For a fixed layer, we consider how the operators and 2 are applied on the subsequent layer. For
example, for the operator , it could be acting on∇ g t∇o yield a concrete function g , or it can
engage with the operator ∇ 2 from the subsequent layli e+ r1 to form the operator 3. ∇ li+1
∇ ∇
Finally,foreverypossiblesequence-of-selectionsmadeintheabovemechanism,anyremainingdifferential
operators j will be applied on the test function u. Hereafter, we use the typewriter font to emphasize that
the sequen∇ ce-of-selections is with respect to the above selection mechanism.
Unraveling all potential selection in Equation (17) is inherently difficult. However, Principle 3 allows us
to ensures the particular structures of g and h . For the following discussion, we introduce the concept
p p
of free-nabla-j: During unraveling of the operator compositions in Equation (17), a free-nabla-j is
generated if we obtain an operator j during any stage of the above selection mechanism.
We have the following result. ∇
Lemma A.2. Once free-nabla-j, for j 3, is generated, one can terminate the subsequent operator
≥
expansion as all the resulting terms will be excluded by Principle 3.
See an elaborated discussion in Appendix A.2. From this observation, the expansion of Equation (17) can
be significantly simplified and one can show that if the induction in Equations (19) and (20) holds for q =p,
g and h admit the following form:
p+1 p+1
g (x)= c (cid:0) 2f(x)(cid:1)p+1 f(x) (33)
p+1 p+1
· ∇ ∇
p
h p+1(x)= (cid:88) (a
k,p
k+b
k,p k
f(x) 2) (cid:0) 2f(x)(cid:1)k Σ(x)(cid:0) 2f(x)(cid:1)p −k . (34)
− − ·∥∇ ∥ · ∇ ∇
k=0
Further, a detailed examination of the combinations reveals the following result.
Lemma A.3. b 0.
k,p k
− ≡
The above results together establish the induction step of q =p+1.
Oncewehavedeterminethefunctionalstructureofg andh ,thenextstepistodeterminethecoefficients
p p
a and c . As we obtain the structures of g and h by induction, the coefficients admit recursion forms. To
i,j p p p
resolve these coefficients, we utilize an argument based on the generating functions. As a result, we obtain
point (3) in Lemma 4.1.
25A.2 Proof of Lemma A.1
To establish the inductive step in Lemma A.1, we prove the following points in this section.
(a) We first prove Lemma A.2, which allows us to significantly simplify the expansion of the compositions
of the generators ’s.
i
L
(b) We analyze all possible terms generated without incurring free-nabla-j, for j 3, and find that there
areonlythreepossibilities. ThisleadstothefunctionalstructuredescribedinEq≥uations(33)and(34).
(c) We further provide the recursive definitions of the coefficients c , a , and b . This allows us to
i i,j i,j
conclude that b 0 (Lemma A.3) and also it allows us t{o c}al{culat}e the l{imit}of the power series
k,p k
g and h , i.e−. po≡int (3) in Lemma 4.1.
p p
{ } { }
A.2.1 Proof of Lemma A.2
Let us recall the concept of free-nabla-j: During unraveling of the operator compositions in Equation (17),
a free-nabla-j is generated if we obtain an operator j during any stage of the selection mechanism
mentioned in the proof sketch of Lemma A.1. ∇
In the following, we show that once free-nabla-j for j 3 is generated in a sequence-of-selections
mentioned in the proof sketch of Lemma A.1, the term result≥ing from this sequence is excluded by Principle 3,
i.e. itcontainsatleastoneoftheterms (r)f(x), (s)u(x)and (m)Σ(x)forr,s 3andm 1. Toestablish
this result, there are two possibilities o∇nce free-∇ nabla-j for j∇ 3 is generated:≥ ≥
≥
1. Thefree-nabla-j operatoristobedirectlyappliedonthetestfunctionu. Inthiscase,alltheresulting
term from this sequence-of-selections contains (j)u(x), j 3, which is excluded by Principle 3.
∇ ≥
2. The free-nabla-j operator is to be applied on a generator , i.e. we encounter the term
i
L
1
(j)(ηig + ηih : 2). (35)
i i
∇ ·∇ 2 ∇
One can prove that (see the discussion below) all terms in the expansion of the above expression either
is excluded by Principle 3 or it contains free-nabla-q for q j7. Consequently, the order j does not
decrease after the operator free-nabla-j being applied on a≥generator .
i
L
One can repeat the case 2 until the free-nabla-j operator is directly applied on the test function u, which
reduces to case 1. From the above argument, all terms generated are to be excluded by Principle 3.
A.2.2 Proof of the Functional Structures in Equations (33) and (34)
According to Lemma A.2, we only need to consider the sequence-of-selections mentioned in the proof
sketch of Lemma A.1 such that no free-nabla-j operator is generated, for j 3. It turns out that there are
only three possible sequence-of-selections, as depicted in Figure 3. ≥
In this section, we elaborate on these three possible sequences, based on which we then establish the
functional structures in Equations (33) and (34). In the meantime, we also derive the recursive definitions of
the coefficients c , a , and b .
i i,j i,j
{ } { } { }
7Forsimplicity,wetakej=3asexample. Forj>3,thelogicissimilar. OnehasforthefirstterminEquation(35)
∇(3)(gi·∇)= gi·∇(4) +∇gi·∇(3)+ ∇2gi·∇(2)+∇3gi·∇ .
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
free-nabla-4 free-nabla-3 everytermcontains∇rf,r≥3
BytheproductruleofthedifferentialandbasedontheinductioninEquation(33),onecancheckthatintheexpansionsof
∇2gi and∇3gi,alltermscontain∇rf,forr≥3. Hence,thelasttwotermsintheaboveexpressionareexcludedbyPrinciple3.
ThesecondterminEquation(35)canbeexcludedwithasimilarargument.
26Ll1 ηl1g l1·∇ 1 2ηl1h l1 : ∇2 Ll1 ηl1g l1·∇ 1 2ηl1h l1 : ∇2
Ll1
ηl1g
l1·∇
1 2ηl1h
l1
: ∇2
Ll2 ηl2g l2·∇ 1 2ηl2h l2 : ∇2
L·· li·
ηlig li·∇ 1 2ηlih li : ∇2 Ll2
ηl2g
l2·∇
1 2ηl2h
l2
: ∇2
L·· li·
ηlig li·∇ 1 2ηlih li : ∇2 Lli+1 ηli+1g li+1·∇ 1 2ηli+1h li+1 : ∇2 Ll3
ηl3g
l3·∇
1 2ηl3h
l3
: ∇2
···
Lli+1 ηli+1g li+1·∇ 1 2ηli+1h li+1 : ∇2 Lli+2 ηli+2g li+2·∇ 1 2ηli+2h li+2 : ∇2 Lli ηlig li·∇ 1 2ηlih li : ∇2
··· ··· ···
Lln ηlng ln·∇ 1 2ηlnh ln : ∇2 Lln ηlng ln·∇ 1 2ηlnh ln : ∇2 Lln ηlng ln·∇ 1 2ηlnh ln : ∇2
(a) Sequence Type I (b) Sequence Type II (c) Sequence Type III
Figure 3: Illustrations of possible combinations in the construction of HA-SME. In constructing HA-SME, we
examine the product of , where each contains two terms. The red arrows indicate the
remainingSequencesafterLfil l1 tLerl2 in·g··oLul tn thosewithhighLel ri
-orderderivativesoff, u, orΣ, whichareexcludedin
the construction rules of HA-SME. These arrows originate from or 2 and lead to g or , demonstrating
how the from the previous operator is applied in the subseq∇uent∇operator. The
dali
shed∇arrows represent
the exclu∇sion of repeated operations.
Notation For the ease of the proof, we define for n ≥1, m ≥0 and sequence {c
i
}+ i=∞0,
ρ(cid:0) n,m, {c
i
}+ i=∞0(cid:1) = (cid:88) c l1c l2···c ln,
l1+l2+...ln=m
wi t. oe a. y t, hs t eu om clo oo eo ffikf a cal itl entp h to iss os fii sb xl t me hrc ioo num g cb (h xin g )a nent .i eo rn as tio nf gn fui nte cm tios nf sr .om Letse cq (u xe )n =ce (cid:80){c + i=i ∞} 0+ i= c∞0 ixs i,uc thh et nh ρat (cid:0) nth ,e ms ,u {m c
i
}o + if =∞0in (cid:1)d ii sce es quis ivm al. enA t
Solve for g We claim that we can solve g by matching terms with u. To show the claim, we observe
p p
that u terms could only be achieved by choosing g and never selecting∇ h for for all 1 i n. This is the
case o∇f our Sequence Type I illustrated in Figure 3al .i To see why this is theli case, we star≤t wi≤th , since we
want terms with u(x), we must select g instead of h . Next, when selecting from ,
wLeln
could also
only select g ,∇since if h is chosen,ln we would havl en h : 2(g u). The 2Ll on p− e1 ration must be
applied to gln− o1 therwise weln w−1 ill have 2u instead of u. Hln− ow1 ev∇er, wl hn e·n∇we apply∇the gradient operator
twice in g ,l tn here must be a third-orde∇r gradient factor∇, which we choose to exclude, popping up as we have
ln
g of form Equation (19). Following this logic recursively, we have that u terms contain only g (with
0ln
i p 1). Also note that the gradient operator in g must be app∇lied to g except
wheni
i=n
wh≤ere≤the−gradient operator is applied to u, otherwise weli w·il∇l have 2 operator andli f+ a1 ce the same issue as
when we selecting h . ∇
li
Since now in the u terms only contains g (with non-zero coefficient) without h , we can solve for g .
p p p
Again, since the grad∇ient operator from g (except i=n) could only be used for the next g , specifically
li li+1
applied on the f(x) factor of g otherwise we would have third-order gradient of f(x), we have n 1
factors of f(x)∇changed to 2f(li x+ )1 and only one remains. Therefore, g is of form Equation (19). −
p
The rem∇aining proof for fi∇nding c is almost the same as the proof for Theorem A.2 of Rosca et al. [2022].
p
The resulting c i is the coefficients of the Taylor expansion of c(x)= log( x1 −x), i.e., c(x)=(cid:80)+ i=∞ 0 c ixi.
Solve for h In the proof g , we have shown that the u terms allow us to solve g , and now we proceed
p p p
to solve the form of h by considering 2u terms. Aga∇in, to solve h , we need terms with the order of η
p p
∇
27summing up to p+1 and consider Lemma 3.1. Let us discuss what are the valid possibilities of selections
from such an expansion.
Let us call the selection between g and h the i-th step of selection. Assume we have the gradient
li li
operator q after the i-th step. If in the next step, we select g , since g has only one factor of f,
which me∇ans we can only apply the gradient operator once on tli h+ i1 s· g∇ , the ol ri d+ e1 r of the remaining gradi∇ent
li+1
operator passed to the next step is still at least q. If in the i+1-th step, we select h : 2 instead, all the
gradient operators should go to the next step (otherwise, if applied on h , factors wli+ e1 ex∇clude will emerge),
li+1
resulting in a q+2. Note that by our construction, in the end, the operator applied on u should not exceed
2. Therefore n∇o matter what we select from the first step, which produces at least , we should only select
g instead h for i>1. Because otherwise we would have at least 3 passing to u.∇According to the above
reli
asoning,
weli
have only two cases left: ∇
1. If we select g in the first step, which corresponds to Figure 3b, then in order to get 2u at the
end, we will
neel1
d at a step i with 2 i n, the gradient operator is not applied to g but∇passed to
the next step. For other steps, the op≤erat≤or is applied to g. This is the only way to
haveli
2u. Plugging
in the solution of g , we know that the resulting term in Equation (16) is of form ∇
i
p 1
ηp+1(cid:88)−
b
s,p 1
sTr(cid:110) f(x)(cid:0) 2f(x)(cid:1)s 2u(x)(cid:0) 2f(x)(cid:1)p −1 −s f(x)⊺(cid:111)
,
− − ∇ ∇ ∇ ∇ ∇
s=0
where b can be written as
s,p 1 s
− −
b
s,p 1 s
− −
p+1 n n i(cid:18) (cid:19)
= (cid:88) n1 !(cid:88)(cid:88)− n −
q
i ρ(cid:0) q+1,p −1 −s −q, {c
k
}+ k=∞0(cid:1) ρ(cid:0) n −1 −q,s+q −n+2, {c
k
}+ k=∞0(cid:1) , (36)
n=2 i=2q=0
where c = 1 , the coefficient associated with g . Note that the coefficient is for terms with 2f(x)
k −1+k i ∇
to the power of s on the left of 2u and p 1 s on the right. To see why this is the case, we have the
following selection procedure: ∇ − −
(a) Such terms can be found in ηn nu for n=2 to p+1.
n!L
(b) As mentioned before, we have at step 2 i n, the gradient operator is not applied to g . Note
that in this case the 2f(x) coming fro≤m g≤ will contribute to the RHS of 2u.
Thenli
we will
have for the remainin∇g steps of selection,
li
∇
2(cid:0)
g
(cid:0)
g ( g
u)(cid:1)(cid:1)
.
∇
li+1
·∇
li+2
·∇ ···
ln
·∇
(c) According to Lemma A.7, there are (cid:0)n −qi(cid:1) combinations such that the number of ∇g resulting in
the RHS of 2u is q.
∇
(d) For the RHS of 2u, we have g and q of g selected from the last step. Note that g would
contains (cid:0) 2f(x)∇(cid:1)k for k 0, anl di for the q o∇ f g, we obtain additional (cid:0) 2f(x)(cid:1)q becausk e of the
operato∇r. Therefore, th≥e total number of ∇ 2f(x) on the RHS of 2u∇comes from g and q of
∇ g and an additional q. Therefore, we select f∇rom q+1 of g, whose in∇dices sum up to
pli
1 s q
(∇so that the total order of f(x) is p 1 s q+q =p 1 s). − − −
∇ − − − − −
(e) F Fo or llot whe inL gH siS mo ilf a∇r l2 ou g, icw ae sh ta hv ee lg a1 s, t{s∇teg pl ,k}inki −= t1 2 ota an ld wn e −seli e−ctq 1o +f i∇g 2se +le ncted iacc qor =di nng t 1o St qep of1 gc ,.
whose indices sum up to s+q n+2. − − − − −
−
We note that for any matrix A, Tr A =Tr A⊺ , so the traces associated with b and b are the
s,m m,s
same for s,m 0. According to Le{mm}a A.4,{ b } +b =0 for m+s 1. Therefore, the coefficients
s,m m,s
cancel each oth≥er. For the case of b with s 1, Lemma A.4 also impli≥es b +b =0 = b =0.
s,s s,s s,s s,s
≥ ⇒
Therefore, in summary, in this whole case, there is no term generated.
282. If we select h in the first step, which corresponds to Figure 3c, then in order to get 2u at the
end, we will in
tl1
he following steps apply the gradient at g for k 2 at once and pass to th∇e next step
the other gradient operator. In this way, all terms of
Equlk
ation (1≥6) with order of η summing to p+1
and coming from 1 nu, has the following form:
n!L
1 ηp+1a˜ Tr(cid:110)(cid:0) 2f(x)(cid:1)i Σ(x)(cid:0) 2f(x)(cid:1)j 2u(x)(cid:0) 2f(x)(cid:1)k(cid:111) , (37)
i,j,k
2 ∇ ∇ ∇ ∇
for some absolute constants a˜ with i+j+k =p 1. The reason why we have this form is because
i,j,k
of our induction assumption and Lemma A.7. Th−e reason why i+j +k = p 1 is that to have
ηp+1, we must have l +l + +l = p+1 n. For the first step, we have− h . According to
the induction
assumpt1 ion,2
it
ca·n··offern
(l 1)-th−order of 2f(x). For the
remaininl1
g steps, we have
1
g , which can offer (l +1)-th order of − 2f(x). Therefor∇e, in total we have f(x) to the power of
∇ l
ls
1+l +l +
+ls
+n 1=p+1∇ n 1+n 1=p 1. According∇to the property of the
1 2 3 n
tra−ce operator, Eq·u·a·tion (37)−can also be w−ritt−en as − −
1 ηp+1a˜ Tr(cid:110)(cid:0) 2f(x)(cid:1)k+i Σ(x)(cid:0) 2f(x)(cid:1)j 2u(x)(cid:111) .
i,j,k
2 ∇ ∇ ∇
Now we try to match the ηp+1 terms in Equation (13) and Equation (16). Note that in Equation (13),
the term associated with ηp+1 for p 2 is 0, therefore we have the following equality for solving h :
p
≥
(cid:32) p+1 (cid:33)
1 ηp+1 Tr(cid:8) h (x)⊺ 2u(x)(cid:9) +(cid:88) a˜ Tr(cid:110)(cid:0) 2f(x)(cid:1)k+i Σ(x)(cid:0) 2f(x)(cid:1)j 2u(x)(cid:111) =0,
p i,j,k
2 ∇ ∇ ∇ ∇
n=2
which implies (since this should hold for any u)
p+1
h (x)=
(cid:88)
a˜
(cid:0) 2f(x)(cid:1)j Σ(x)(cid:0) 2f(x)(cid:1)k+i
.
p i,j,k
− ∇ ∇
n=2
So far, by induction, we have proved that h has the form of Equation (19) (regardless of the constants).
p
Next we will try to find these constants. We will stick to the notation in Lemma 4.1, i.e., using a for
i,j
the rest of the proof, i.e.,
p 1
h
p(x)=(cid:88)−
a
k,p 1 k
(cid:0) 2f(x)(cid:1)k Σ(x)(cid:0) 2f(x)(cid:1)p −1 −k
.
− − · ∇ ∇
k=0
Similar to the proof of Equation (36), we have
a
k,p 1 k
− −
p k p 1 k n 1(cid:18) (cid:19)
= −(cid:88) n1 !(cid:88) −(cid:88)− a l,r(cid:88)− n −
q
1 ρ(cid:0) q,k −q −l, {c
k
}+ k=∞0(cid:1) ρ(cid:0) n −1 −q,p −k −n+q −r, {c
k
}+ k=∞0(cid:1) .
n=2 l=0 r=0 q=0
(38)
The reason is that we have the following selection process:
(a) Such terms can be found in ηn nu for n=2 to p+1.
n!L
(b) We try to find term with
Tr(cid:110)(cid:0) 2f(x)(cid:1)p −1 −k Σ(cid:0) 2f(x)(cid:1)k 2u(cid:111)
,
∇ ∇ ∇
which can be found from Sequence
(cid:0) 2f(x)(cid:1)l Σ(cid:0) 2f(x)(cid:1)r :(cid:0) 2f(x)(cid:1)k −l 2u(cid:0) 2f(x)(cid:1)p −1 −k −r
.
∇ ∇ ∇ ∇ ∇
(cid:124) (cid:123)(cid:122) (cid:125)
hl1
29(c) wFo itr h(cid:0)
∇(cid:0)
2f 2f(x (x)(cid:1) )k (cid:1), r.w Te hca an
t
ih sa wve hy(cid:0)
∇
w2 ef h(x av)(cid:1) el sc uo mm min ag tif oro nm
s
oh
vl e1
r. lA als no dh
rl1
wc ia thn c co on et ffiri cb ieu nte tsto a(
∇
.f(x))p −1 −k
l,r
∇
(d) As mentioned before, we have for the selection at step 2 k n,
≤ ≤
2(g (g ( g u))).
∇
l2
·∇
l3
·∇ ···
ln
·∇
According to Lemma A.7, there are (cid:0)n −q1(cid:1) combinations such that the number of ∇g resulting in
the LHS of 2u is q.
∇
(e) For the LHS of Σ(x), we have q of g selected from the last step. The indices should sum up to
k q l, since we will generate ad∇ditional q of 2f(x) and h provides l. Therefore, we have
co−effic−ient ρ(cid:0) q,k −q −l, {c
k
}+ k=∞0(cid:1) similarly for th∇e RHS.
l1
A.3 Proof of Lemma A.3 and the Recursive Expressions of c and a
i i,j
{ } { }
Lemma A.4. Recall the recursive definition of b in Equation (36),
s,m
s+m+2 n n i(cid:18) (cid:19)
b
s,m
= (cid:88) n1 !(cid:88)(cid:88)− n −
q
i ρ(cid:0) q+1,m −q, {c
k
}+ k=∞0(cid:1) ρ(cid:0) n −1 −q,s+q −n+2, {c
k
}+ k=∞0(cid:1) ,
n=2 i=2q=0
where s,m 0, s+m 1, c = 1 . We have b +b =0.
≥ ≥ k −k+1 s,m m,s
Proof. Let c(x)=(cid:80)+ k=∞ 0c kxk, and we know that c(x)= log( x1 −x). Define
+
(cid:88)∞
b(x,y)= b xsym.
s,m
s,m 0
≥
Then we have
b(x,y)
+ s+m+2 n n i(cid:18) (cid:19)
= (cid:88)∞ xsym (cid:88) n1 !(cid:88)(cid:88)− n −
q
i ρ(cid:0) q+1,m −q, {c
k
}+ k=∞0(cid:1) ρ(cid:0) n −1 −q,s+q −n+2, {c
k
}+ k=∞0(cid:1)
s,m 0 n=2 i=2q=0
≥
+ + n n i(cid:18) (cid:19)
= (cid:88)∞ n1
!
(cid:88)∞ xsym(cid:88)(cid:88)− n −
q
i ρ(cid:0) q+1,m −q, {c
k
}+ k=∞0(cid:1) ρ(cid:0) n −1 −q,s+q −n+2, {c
k
}+ k=∞0(cid:1)
n=2 s+m n 2 i=2q=0
≥ −
+ + n 2n q(cid:18) (cid:19)
= (cid:88)∞ n1
!
(cid:88)∞ xsym(cid:88)− (cid:88)− n −
q
i ρ(cid:0) q+1,m −q, {c
k
}+ k=∞0(cid:1) ρ(cid:0) n −1 −q,s+q −n+2, {c
k
}+ k=∞0(cid:1)
n=2 s+m n 2 q=0 i=2
≥ −
+ + n 2(cid:18) (cid:19)
= (cid:88)∞ n1
!
(cid:88)∞ xsym(cid:88)− n q+− 11 ρ(cid:0) q+1,m −q, {c
k
}+ k=∞0(cid:1) ρ(cid:0) n −1 −q,s+q −n+2, {c
k
}+ k=∞0(cid:1)
n=2 s+m n 2 q=0
≥ −
+ + n 2(cid:18) (cid:19)
= (cid:88)∞ n1
!
(cid:88)∞ xsym(cid:88)− n −
q
1 ρ(cid:0) q,m −q+1, {c
k
}+ k=∞0(cid:1) ρ(cid:0) n −q,s+q −n+1, {c
k
}+ k=∞0(cid:1)
n=2 s+m n 2 q=1
≥ −
+ + n 2(cid:18) (cid:19)
= y1 (cid:88)∞ n1
!
(cid:88)∞ xsym+1(cid:88)− n −
q
1 ρ(cid:0) q,m −q+1, {c
k
}+ k=∞0(cid:1) ρ(cid:0) n −q,s+q −n+1, {c
k
}+ k=∞0(cid:1) ,
n=2 s+m n 2 q=1
≥ −
where in the forth equality, we used hockey-stick identity [Ross, 1997], and in the fifth equality, we replace q
with q 1. Note that
−
+ n 2(cid:18) (cid:19)
(cid:88)∞ xsym+1(cid:88)− n −
q
1 ρ(cid:0) q,m −q+1, {c
k
}+ k=∞0(cid:1) ρ(cid:0) n −q,s+q −n+1, {c
k
}+ k=∞0(cid:1) (39)
s+m n 2 q=1
≥ −
30is equivalent to
(yc(y)+xc(x))n −1c(x) (xc(x))n −1c(x). (40)
−
To see why this is the case, let us first looked at
(yc(y)+xc(x))n −1c(x).
To have ym+1, we can select yc(y) from (yc(y)+xc(x))n −1 for q times, which results in coefficients (cid:0)n −q1(cid:1)
a ρn (cid:0)d
n
−ρ(cid:0) qq ,, sm +− qq −+ n1 +, { 1c ,k {} c+ k k=∞ }0
+
k(cid:1) =∞. 0(cid:1)F .o Nr ox ts e, tw hae ts ie nle Ect qun a− tio1 n− (3q 9)o ,f thx ec( sx u) mfr oo fm q( sy tc a( ry t) s+ frox mc(x 1) ,) wn − h1 i, chan md eao nb sta win
e
havenotconsideredthecasewhereyc(y)isneverselectedin(yc(y)+xc(x))n −1. ThatiswhyinEquation(40),
we subtract (xc(x))n −1c(x).
Next, we proceed with
b(x,y)= 1 (cid:88)+ ∞ 1 (cid:16) (yc(y)+xc(x))n −1c(x) (xc(x))n −1c(x)(cid:17)
y n! −
n=2
+ +
=
1 (cid:88)∞ 1
(yc(y)+xc(x))n −1c(x)
1 (cid:88)∞ 1
(xc(x))n −1c(x). (41)
y n! − y n!
n=2 n=2
For the first part, we have
+
1 (cid:88)∞ 1
(yc(y)+xc(x))n −1c(x)
y n!
n=2
+
=
c(x) (cid:88)∞ 1
(yc(y)+xc(x))n
y(yc(y)+xc(x)) n!
n=2
c(x) (cid:16) (cid:17)
= eyc(y)+xc(x) 1 (yc(y)+xc(x)) ,
y(yc(y)+xc(x)) − −
where in the last equality we used Taylor expansion ex =(cid:80)+
∞
1xn. Plugging in the definition of c(x), we
n=0 n!
have
c(x) (cid:16) (cid:17)
eyc(y)+xc(x) 1 (yc(y)+xc(x))
y(yc(y)+xc(x)) − −
log(1 x)
= − ((1 x)(1 y) 1 log((1 x)(1 y)))
xylog((1 x)(1 y)) − − − − − −
− −
log(1 x)
= − (xy x y log((1 x)(1 y))).
xylog((1 x)(1 y)) − − − − −
− −
For the second part of Equation (41),
+ +
1 (cid:88)∞ 1
(xc(x))n −1c(x)=
1 (cid:88)∞ 1
(xc(x))n
y n! xy n!
n=2 n=2
1 (cid:16) (cid:17)
= exc(x) 1 xc(x)
xy − −
1
= (1 x 1 log(1 x))
xy − − − −
1
= ( x log(1 x)).
xy − − −
31Combining the two parts, we get
log(1 x) 1
b(x,y)= − (xy x y log((1 x)(1 y)))+ (x+log(1 x))
xylog((1 x)(1 y)) − − − − − xy −
− −
log(1 x) 1
= − (xy x y)+ .
xylog((1 x)(1 y)) − − y
− −
Now to prove the required result, we consider the generator function
b(x,y)+b(y,x)
log(1 x) 1 log(1 y) 1
= − (xy x y)+ + − (xy x y)+
xylog((1 x)(1 y)) − − y xylog((1 x)(1 y)) − − x
− − − −
1 1 1
= (xy x y)+ +
xy − − x y
=1.
Note that the coefficients of b(x,y)+b(y,x) for xsym with s+m 1 is b +b , which means
s,m m,s
≥
b +b =0.
s,m m,s
Lemma A.5. Define a =1 and for s,m 0 and s+m 1, recall the recursive definition in Equation (38),
0,0
≥ ≥
a
s,m
s+m+1 s m n 1(cid:18) (cid:19)
=
−
(cid:88) n1 !(cid:88)(cid:88) a l,r(cid:88)− n −
q
1 ρ(cid:0) q,s −q −l, {c
k
}+ k=∞0(cid:1) ρ(cid:0) n −1 −q,m+1 −n+q −r, {c
k
}+ k=∞0(cid:1) ,
n=2 l=0r=0 q=0
where c = 1 . Then we have
k −1+k
+
(cid:88)∞ log((1 x)(1 y))
a(x,y)= a xsym = − − .
s,m
xy x y
s,m 0 − −
≥
Proof. We begin with
a(x,y)
+
(cid:88)∞
= a xsym
s,m
s,m 0
≥
+ s+m+1 s m n 1(cid:18) (cid:19)
=1
−
(cid:88)∞ xsym (cid:88) n1 !(cid:88)(cid:88) a l,r(cid:88)− n −
q
1 ρ(cid:0) q,s −q −l, {c
k
}+ k=∞0(cid:1)
s,m 1 n=2 l=0r=0 q=0
≥
·ρ(cid:0) n −1 −q,m+1 −n+q −r, {c
k
}+ k=∞0(cid:1)
+ + s m n 1(cid:18) (cid:19)
=1 −(cid:88)∞ n1
!
(cid:88)∞ xsym(cid:88)(cid:88) a l,r(cid:88)− n −
q
1 ρ(cid:0) q,s −q −l, {c
k
}+ k=∞0(cid:1)
n=2 s+m n 1 l=0r=0 q=0
≥ −
·ρ(cid:0) n −1 −q,m+1 −n+q −r, {c
k
}+ k=∞0(cid:1)
Similar to the proof technique used in Lemma A.4, we note that
+ s m n 1(cid:18) (cid:19)
(cid:88)∞ xsym(cid:88)(cid:88) a l,r(cid:88)− n −
q
1 ρ(cid:0) q,s −q −l, {c
k
}+ k=∞0(cid:1) ρ(cid:0) n −1 −q,m+1 −n+q −r, {c
k
}+ k=∞0(cid:1)
s+m n 1 l=0r=0 q=0
≥ −
32is equivalent to
a(x,y)(xc(x)+yc(y))n −1,
where c(x)=(cid:80)+ k=∞ 0c kxk = log( x1 −x). To constitute xsym, we first select a l,rxlyr from a(x,y), and then select
q times xc(x) from (xc(x)+yc(y))n −1.
Then we have
+
a(x,y)=1
(cid:88)∞ 1
a(x,y)(xc(x)+yc(y))n −1
− n!
n=2
+
=1
a(x,y) (cid:88)∞ 1
(xc(x)+yc(y))n
− xc(x)+yc(y) n!
n=2
a(x,y) (cid:16) (cid:17)
=1 exc(x)+yc(y) 1 (xc(x)+yc(y)) .
− xc(x)+yc(y) − −
Plugging in the definition of c(x), we obtain
a(x,y)
a(x,y)=1 ((1 x)(1 y) 1 log((1 x)(1 y)))
− log((1 x)(1 y)) − − − − − −
− −
a(x,y)
=1 (xy x y log((1 x)(1 y))).
− log((1 x)(1 y)) − − − − −
− −
Solving this equation for a(x,y) gives us the desired result.
A.4 Proof of Theorem 4.1
Next, we need to show that b(x) and (x) is convergent. Solving for b(x) is similar to the proof of Rosca
et al. [2022, Theorem A.2]. Here, we pDresent the derivation of the diffusion term. We have
+ + p 1
(x)=(cid:88)∞ ηph p(x)=(cid:88)∞ ηp(cid:88)− a
k,p 1 k
(cid:0) 2f(x)(cid:1)k Σ(x)(cid:0) 2f(x)(cid:1)p −1 −k ,
D − − · ∇ ∇
p=0 p=0 k=0
where a is determined in Lemma 4.1. The above equation implies
k,p 1 k
− −
(cid:32) + p 1 (cid:33)
U⊺
U
=U⊺ (cid:88)∞ ηp(cid:88)−
a
k,p 1 k
(cid:0) 2f(x)(cid:1)k Σ(x)(cid:0) 2f(x)(cid:1)p −1 −k
U
D − − · ∇ ∇
p=0 k=0
+ p 1
(cid:88)∞ (cid:88)− ⊺
= ηp a ΛkU Σ(x)UΛp 1 k.
k,p 1 k − −
− − ·
p=0 k=0
Since Λ is a diagonal matrix, we have
+ p 1
(cid:2) U⊺ DU(cid:3)
i,j
=(cid:88)∞ ηp(cid:88)− a
k,p −1 −k
·λk
i
(cid:2) U⊺ Σ(x)U(cid:3) i,jλp j−1 −k
p=0 k=0
+ p 1
=η(cid:88)∞(cid:88)− a (ηλ )k(cid:2) U⊺ Σ(x)U(cid:3) (ηλ )p 1 k
k,p −1 −k · i i,j j − −
p=0k=0
+ +
=η(cid:88)∞ (cid:88)∞ a (ηλ )k(cid:2) U⊺ Σ(x)U(cid:3) (ηλ )p 1 k
k,p −1 −k · i i,j j − −
k=0p=k+1
33+ +
=η(cid:2) U⊺ Σ(x)U(cid:3) (cid:88)∞(cid:88)∞ a (ηλ )k(ηλ )q,
i,j k,q · i j
k=0q=0
where in the last equality we let q =p 1 k. Then according to Lemma 4.1, we have
− −
(cid:2) ⊺ (cid:3) (cid:2) ⊺ (cid:3)
U U =η U Σ(x)U a(ηλ ,ηλ ),
D i,j i,j i j
where a(x,y)= log(1 −x)(1 −y).
xy (x+y)
−
A.5 Proof of the Well-Posedness of HA-SME
Proof for Lemma 4.2. In the first condition, the matrix U⊺ ΣU becomes diagonal. We can check that the
diagonal elements are always positive definite, and by taking square root of the eigenvalues, we obtain
(cid:115)
D =U Λ(cid:101)ηlog(1 −ηΛ)2 U⊺ , (42)
(1 ηΛ)2 1
− −
where Σ=UΛ(cid:101)U⊺ and 2f(x)=UΛU⊺.
Forthesecondcond∇ition, weconsidersmallstepsizeregime. WewillproceedtoshowthattheRHSmatrix
of Equation (23), denoted as M is positive semi-definite, so that such a square root always exists. Consider
matrix K, whose (i,j)-th element is defined as
log(1 ηλ )+log(1 ηλ )
i j
K = − − .
i,j
(1 ηλ )(1 ηλ ) 1
i j
− − −
√
We consider small η, i.e., η < 1 − 22 , therefore, K is real. The matrix M can then be written as
2f(x)
∥∇ ∥
⊺ ⊺ (cid:0) ⊺ ⊺ (cid:1)
M =ηU ΣU K =ηU ΣU (1+K 1)=η U ΣU +U ΣU (K 1) ,
⊙ ⊙ − ⊙ −
where 1 is a all-one matrix. Now since η >0, to determine the positive semi-definiteness of M, it is sufficient
to determine positive semi-definiteness of U⊺ ΣU +U⊺ ΣU (K 1).
NotethatformatrixU⊺ ΣU,sinceU isorthogonal,itonl⊙ychan−gestheeigenspaceofΣ,butdoesnotchange
the eigenvalues. To see why this is the case, assume λ is a eigenvalue of Σ with v being the corresponding
eigenvector. Then we have
Σv =λv.
Then U 1v is a eigenvector for U⊺ ΣU with eigenvalue λ, since
−
⊺ ⊺ ⊺
U ΣUU 1v =U Σv =U λv =λU 1v.
− −
Going back to our problem, since U⊺ ΣU is positive definite, a sufficient condition for matrix U⊺ ΣU +
U⊺ ΣU (K 1) to be PSD is that
⊙ −
λ (cid:0) U⊺ ΣU(cid:1) =λ (Σ) U⊺ ΣU (K 1) . (43)
min min
≥∥ ⊙ − ∥
Let us look at U⊺ ΣU (K 1) , we know that
∥ ⊙ − ∥
(cid:12) (cid:12)
U⊺ ΣU (K 1) √d U⊺ ΣU sup(cid:12)[K 1] (cid:12)
∥ ⊙ − ∥ ≤ ∥ ∥
(cid:12)
−
i,j(cid:12)
i,j
(cid:12) (cid:12)
=√d
U⊺
ΣU
sup(cid:12) (cid:12)log((1 −ηλ i)(1 −ηλ j)) 1(cid:12)
(cid:12)
∥ ∥ (cid:12) (1 ηλ )(1 ηλ ) 1 − (cid:12)
i,j i j
− − −
34(cid:12) (cid:12)
=√dλ
max(Σ)sup(cid:12) (cid:12)log((1 −ηλ i)(1 −ηλ j)) 1(cid:12)
(cid:12),
(cid:12) (1 ηλ )(1 ηλ ) 1 − (cid:12)
i,j i j
− − −
where we used Lemma A.8 for the inequality.
According to Lemma A.6,
(cid:12) (cid:12)
sup(cid:12) (cid:12)log((1 −ηλ i)(1 −ηλ j)) 1(cid:12) (cid:12) sup1 (1 max ηλ i , ηλ j )2 1 (cid:0) 1 η(cid:13) (cid:13) 2f(x)(cid:13) (cid:13)(cid:1)2
(cid:12) (1 ηλ )(1 ηλ ) 1 − (cid:12)≤ − − {| | | |} ≤ − − ∇
i,j i j i,j
− − −
Plugging in back to Equation (43), a sufficient condition for M to be PSD is
λ min(Σ) √dλ max(Σ)(cid:16) 1 (cid:0) 1 η(cid:13) (cid:13) 2f(x)(cid:13) (cid:13)(cid:1)2(cid:17) .
≥ − − ∇
Rearranging the terms gives us the condition in the theorem.
Proof of Theorem 4.2. Toshowtheresult, itissufficienttoshowthattheeigenvaluesofD arelowerbounded
away from 0. For the first condition, we have an explicit form of D in Equation (42). Note that when
(cid:16) (cid:17)
the abstract value of all entries of ηΛ are smaller than 1, entries of log(1 ηΛ)2/ (1 ηΛ)2 1 are lower
− − −
bounded away from 0. Therefore, as long as eigenvalues of Σ are lower bounded, i.e., Σ is positive definite,
the eigenvalues of D are lower bounded from 0. For the second condition, Similar to the proof for Lemma 4.2,
we know that as long as the stepsize satisfies the condition, the eigenvalues of the diffusion coefficient are
positive and lower bounded.
Now, we show that why lower boundedness of eigenvalues of D is sufficient to prove the result. We need
to show that the drift term b and diffusion term D are Lipschitz. According to Lemma B.1, the drift term is
Lipschitz, and according to Lemma B.2, we know that DD⊺ is Lipschitz. Therefore, ∂[DD⊺ ] /∂x is upper
i,j k
bounded. Then according to Lin and Maji [2017, Equation (7)], we have
(cid:18) (cid:19)
vec ∂D i,j =(D I+I D) 1vec(cid:0) 1˜ (cid:1) ,
⊺ − i,j
∂DD ⊗ ⊗
where vec is the vectorization of matrices and denotes the Kronecker product. 1˜ is a matrix whose
i,j
(i,j)-th element is 1 and all other elements are 0⊗. Next, we get
⊺
∂D i,j (cid:88) ∂D i,j ∂[DD ] p,q
=
⊺
∂x ∂[DD ] ∂x
k p,q k
p,q
⊺
=(cid:88)(cid:2) (D I+I D) 1vec(cid:0) 1˜ (cid:1)(cid:3) ∂[DD ] p,q
⊗ ⊗ − i,j qd+p ∂x
k
p,q
(cid:12) ⊺ (cid:12)
(cid:88)(cid:13) (cid:13)(D I+I D) −1vec(cid:0) 1˜ i,j(cid:1)(cid:13) (cid:13)(cid:12) (cid:12)∂[DD ] p,q(cid:12) (cid:12)
≤ ⊗ ⊗ (cid:12) ∂x (cid:12)
k
p,q
(cid:12) ⊺ (cid:12)
(cid:88)(cid:13)
(cid:13)(D I+I D)
−1(cid:13) (cid:13)(cid:12) (cid:12)∂[DD ] p,q(cid:12)
(cid:12).
≤ ⊗ ⊗ (cid:12) ∂x (cid:12)
k
p,q
Itremainstoshowthattheeigenvaluesof(D I+I D) 1 isbounded. AccordingtoSteebandHardy[2011,
−
Theorem 2.15], the eigenvalues of D I +I⊗ D ar⊗e a+b a,b λ d , where λ are the eigenvalues
of D. Therefore, the eigenvalues of D⊗ I +⊗ I D ar{e lower|boun∈d{edi }sii n= c1 e}we have ti he eigenvalues of D
being lower bounded by a positive cons⊗tant. Th⊗is implies that eigenvalues of (D I+I D) 1 are upper
−
bounded, which concludes the proof. ⊗ ⊗
35A.6 Helper Lemmas
Lemma A.6. For any x,y R and √2 1 x,y 1 √2, it holds that
∈ 2 − ≤ ≤ − 2
(cid:12) (cid:12)
(cid:12) (cid:12)log((1 −x)(1 −y)) 1(cid:12) (cid:12) 1 (1 max x, y )2.
(cid:12)(1 x)(1 y) 1 − (cid:12)≤ − − {| | | |}
− − −
(cid:12) (cid:12)
Proof. Let z =(1 x)(1 y), then the LHS becomes (cid:12)logz 1(cid:12). We first study the function
− − (cid:12)z 1 − (cid:12)
−
logz
f(z)= 1,
z 1 −
−
whose gradient is given by
z 1 logz 1 1 logz
f (z)= −z − = − z − 0,
′
(z 1)2 (z 1)2 ≤
− −
where the last inequality comes from the fact logz 1 1 for z >0. Also note that f(1)=0, therefore, we
know that ≥ − z
(cid:40)
logz 1, z 1
f(z) = z 1 − ≤
| | 1− logz, z >1.
− z 1
−
(cid:12) (cid:12)
According to previous analysis, the maximum of (cid:12)logz 1(cid:12) would be at the minimum or maximum possible
(cid:12)z 1 − (cid:12)
−
value of z. Denoting m:=max x, y with 0 m 1 √2, then we know that
{| | | |} ≤ ≤ − 2
(cid:12) (cid:12)log((1 x)(1 y)) (cid:12) (cid:12) (cid:40) log(1 m)2 log(1+m)2 (cid:41)
(cid:12) − − 1(cid:12) max − 1,1
(cid:12)(1 x)(1 y) 1 − (cid:12)≤ (1 m)2 1 − − (1+m)2 1
− − − − − −
Next, we will show that for 0 m 1,
≤ ≤
log(1 m)2 log(1+m)2
− 1 1 .
(1 m)2 1 − ≥ − (1+m)2 1
− − −
We start with
(cid:32) (cid:33)
log(1 m)2 log(1+m)2
− 1 1
(1 m)2 1 − − − (1+m)2 1
− − −
2(m+2)log(1 m)+2(m 2)log(1+m) 2m(m+2)(m 2)
= − − − − .
m(m+2)(m 2)
−
Since m(m+2)(m 2)<0, it suffice to show that the numerator is less or equal than 0. We let
−
g(m)=2(m+2)log(1 m)+2(m 2)log(1+m) 2m(m+2)(m 2),
− − − −
whose gradient is
m+2 m 2
g ′(m)= +log(1 m)+ − +log(1+m) 3m2+4
m 1 − 1+m −
−
m+2 m 2
+ − 4m2+4
≤ m 1 1+m −
−
3m2(m2 3)
= − 0,
(1 m)(m+1) ≤
−
36wheretheinequalityholdsbecauselogz z 1forz >0. Therefore,for0 m 1,wehaveg(m) g(0)=0.
So far, we have proved that ≤ − ≤ ≤ ≤
(cid:12) (cid:12)log((1 x)(1 y)) (cid:12) (cid:12) log(1 m)2
(cid:12) − − 1(cid:12) − 1.
(cid:12)(1 x)(1 y) 1 − (cid:12)≤ (1 m)2 1 −
− − − − −
Next, we will prove that an upper bound is as follows
log(1 m)2
− 1 1 (1 m)2,
(1 m)2 1 − ≤ − −
− −
for 0 m 1 √2. With a change of variable for z =(1 m)2, to prove the above inequality is the same as
showi≤ng th≤e fo−llow2 ing for 1 z 1, −
2 ≤ ≤
logz logz logz+z2 3z+2
1 (1 z)= +z 2= − 0.
z 1 − − − z 1 − z 1 ≤
− − −
Then it is sufficient to show
h(z):=logz+z2 3z+2 0.
− ≥
We have
1 (z 1)(2z 1)
h ′(z)= +2z 3= − − 0.
z − z ≤
Therefore h(z) h(1)=0 for 1 z 1. The whole proof is finished.
≥ 2 ≤ ≤
Lemma A.7. Let u:Rd →R and {v i(x) }n i=1 be vector fields, i.e., v i :Rd →Rd. The terms in the result of
2(v (v (v ( v u)))),
1 2 3 n
∇ ·∇ ·∇ ·∇ ··· ·∇
that contain only 2u and v for i [1,n] are
i
∇ ∇ ∈
   
(cid:88) (cid:89) (cid:89)
 ∇v i ∇2u  ∇v i ,
S ⊆[1,n] i ∈S(cid:101) i ∈[1(cid:92) ,n] \S
where is the inner product between vectors, S(cid:101) is the ascending ordered set containing all elements from S, S(cid:98)
·
is the corresponding descending ordered set, and is the set difference operator.
\
Proof. We will prove this by induction. First, let us consider the case of n=1, then clearly, we have
2(v u)= v 2u+ 2u v + ,
1 1 1
∇ ·∇ ∇ ∇ ∇ ∇ C
where contains irrelevant terms, i.e., terms with higher order gradients of v and u. Now assume the
i
conclusCion holds true for n=1,...,m 1, then when n=m, we have
−
2(v (v (v ( v u))))
1 2 3 m
∇ ·∇ ·∇ ·∇ ··· ·∇
= v 2(v (v ( v u)))+ 2(v (v ( v u))) v +
1 2 3 m 2 3 m 1
∇ ∇ ·∇ ·∇ ··· ·∇ ∇ ·∇ ·∇ ··· ·∇ ∇ C
       
(cid:88) (cid:89) (cid:89) (cid:88) (cid:89) (cid:89)
= ∇v 1  ∇v i ∇2u  ∇v i +  ∇v i ∇2u  ∇v i ∇v 1+ C
S ⊆[2,m] i ∈S(cid:101) i ∈[2(cid:92) ,m] \S S ⊆[2,m] i ∈S(cid:101) i ∈[2(cid:92) ,m] \S
   
(cid:88) (cid:89) (cid:89)
=  ∇v i ∇2u  ∇v i + C,
S ⊆[1,m] i ∈S(cid:101) i ∈[1(cid:92) ,m] \S
which concludes the proof.
37Lemma A.8. We have for any A,B Rd ×d,
∈
A B √dsupA B .
i,j
∥ ⊙ ∥≤ ∥ ∥
i,j
Proof. Let e 1,e 2,...,e
d
be the standard basis of Rd. Then we have for any 1 k d,
≤ ≤
(A B)e supA Be supA B .
k i,j k i,j
∥ ⊙ ∥≤ ∥ ∥≤ ∥ ∥
i,j i,j
Then for any vector v Rd, which can be written as v =v 1e 1+v 2e 2+ , we have
∈ ···
d d
(cid:88) (cid:88)
(A B)v v (A B)e supA B v supA B √d v ,
k k i,j k i,j
∥ ⊙ ∥≤ | |∥ · ∥≤ ∥ ∥ | |≤ ∥ ∥ ∥ ∥
i,j i,j
k=1 k=1
where the last inequality we used Cauchy-Schwartz inequality.
B Approximation Error Analysis
B.1 Proofs for Section 5
Proof for Theorem 5.1. The proof follows the idea of Theorem 2.2 of Feng et al. [2017], however our proof
is more difficult since we need to deal with more complicated HA-SME. The assumption of F(x;ξ) <
c7
implies f(x) < and Σ(x) < . AccordingtoLemmaB.1,wehaveboundednessof∥ b(x) ∥,whe∞re
b(x) is t∥he dr∥ifc t7 ter∞m of H∥ A-SME∥.c A6 cco∞rding to Lemma B.1, we have boundedness of the d∥iffusio∥nC5 term of
⊺
HA-SME, i.e., ∥U(x)L(x)U(x) ∥C5 is upper bounded. Let un(x) defined the same as in Feng et al. [2017], i.e.,
η
un(x 0)=E[u(x n)]. According to Theorem 2.1 of Feng et al. [2017], for small enough η, we have boundedness
of un .
∥Sta∥rC ti6
ng both from un(x), we first measure the error between the discrete-time SGD and continuous-time
iterates after one-step. For the discrete-time SGD, by Taylor expansion, and boundedness of f(x;ξ) and
un(x) , we have ∇
∥ ∥C6
(cid:12) (cid:12)
(cid:12) (cid:12)un+1(x) un(x)+η f(x), un(x) 1 η2 E(cid:2) f(x;ξ) f(x;ξ)⊺(cid:3) : 2un(x)(cid:12) (cid:12)
(cid:12) − ⟨∇ ∇ ⟩− 2 ∇ ∇ ∇ (cid:12)
(cid:12) (cid:12)
=(cid:12) (cid:12)un+1(x) un(x)+η f(x), un(x) 1 η2(cid:0) f(x) f(x)⊺ +Σ(x)(cid:1) : 2un(x)(cid:12) (cid:12)
(cid:12) − ⟨∇ ∇ ⟩− 2 ∇ ∇ ∇ (cid:12)
(cid:0) η3(cid:1) . (44)
≤O
In continuous time, after time η, according to Lemma B.3, we have
(cid:12)
(cid:12) (cid:12)eη L un(x) η un(x)
η2 2un(x)(cid:12)
(cid:12) (cid:12) (η3). (45)
(cid:12) − − L − 2 L (cid:12)≤O
Note that different from previous SDEs (Equations (3) and (6)), now η un(x) and η2 2un(x) contains
infinite many terms and we need to consider their errors. We know that L 2 L
η ⊺
η un(x)=ηb(x) un(x)+ D(x)D(x) : 2un(x),
L ·∇ 2 ∇
where b(x) is the drift term of HA-SME. By Taylor expansion of un w.r.t. η and Lemma B.4, we have
L
(cid:12)
(cid:12)
η2
(cid:12)ηb(x) un(x)+η f(x) un(x)+ 2f(x) f(x) un(x)
(cid:12) ·∇ ∇ ·∇ 2 ∇ ∇ ·∇
38(cid:12)
η ⊺ 1 (cid:12)
+ U(x)L(x)U(x) : 2un(x) η2Σ(x): 2un(x)(cid:12) (η3).
2 ∇ − 2 ∇ (cid:12)≤O
Then we have
(cid:12) (cid:12) (cid:12)η un(x)+η f(x) un(x)+ η2 2f(x) f(x) un(x) 1 η2Σ(x): 2un(x)(cid:12) (cid:12) (cid:12) (cid:0) η3(cid:1) . (46)
(cid:12) L ∇ ·∇ 2 ∇ ∇ ·∇ − 2 ∇ (cid:12)≤O
Next, we look at the errors in η2 2un(x). By Taylor expansion of 2un w.r.t. η and Lemma B.4, we have
2 L L
(cid:12)
(cid:12)
(cid:12)η2
2un(x)
η2
2f(x) f(x) un(x)
η2
f(x) f(x)⊺ :
2un(x)(cid:12)
(cid:12) (cid:12) (cid:0) η3(cid:1) . (47)
(cid:12) 2 L − 2 ∇ ∇ ·∇ − 2 ∇ ∇ ∇ (cid:12)≤O
Combining Equations (45) to (47), we get
(cid:12)
(cid:12) (cid:12)eη Lun(x) un(x)+η f(x) un(x)
η2
Σ(x): 2un(x)
η2
f(x) f(x)⊺ :
2un(x)(cid:12)
(cid:12) (cid:12) (cid:0) η3(cid:1) .
(cid:12) − ∇ ·∇ − 2 ∇ − 2 ∇ ∇ ∇ (cid:12)≤O
Combining the above equation and Equation (44), we have
(cid:12) (cid:12)eη Lun(x) un+1(x)(cid:12) (cid:12) (cid:0) η3(cid:1) .
− ≤O
Denote
En = un(x) u(x,nη) .
∥ − ∥L∞
We obtain
(cid:13) (cid:13)
En+1 =(cid:13)un+1(x) −u(x,(n+1)η)+eη Lun(x) −eη Lun(x)(cid:13)
L∞
(cid:13) (cid:13) (cid:13) (cid:13)
≤(cid:13)eη L(un(x) −u(x,nη))(cid:13)
L∞
+(cid:13)un+1(x) −eη Lun(x)(cid:13)
L∞
=(cid:13) (cid:13)eη L(un(x) −u(x,nη))(cid:13) (cid:13)
L∞
+ O(cid:0) η3(cid:1)
un(x) u(x,nη) + (cid:0) η3(cid:1)
≤∥ − ∥L∞ O
=En+ (cid:0) η3(cid:1) ,
O
where the last inequality comes from the L contraction of et (see Lemma 2.1 of Feng et al. [2017]). Then
∞ L
we know that
En n (cid:0) η3(cid:1) = T (cid:0) η3(cid:1) (cid:0) η2(cid:1) .
≤ O ηO ≤O
Proof of Lemma 5.1. The lemma follows by Lemmas B.1 and B.2
Proof of Lemma 5.2. We will show the results for both the convex and strongly-convex (corresponding to
our Remark 5.3) settings. According to Feng et al. [2017], for any n 0, un+1(x) can be written as
≥
un+1(x)=E[un(x η F(x;ξ))].
− ∇
Let us check the first-order derivatives of un+1. Denoting y =x η F(x;ξ), we obtain
− ∇
un+1(x)=E(cid:2)(cid:0)
I η
2F(x;ξ)(cid:1) un(y)(cid:3)
.
∇ − ∇ ∇
391. If f() is strongly-convex, we obtain
·
(cid:13) (cid:13)
sup(cid:13) un+1(x)(cid:13) (1 ηµ)sup un(x) .
∇ ≤ − ∥∇ ∥
x x
With small η, we have ηsup (cid:13) (cid:13) 2F(x;ξ)(cid:13) (cid:13)<1, and the recursion form is a contraction:
x,ξ ∇
sup un(x) (1 µη)nsup u(x)
∥∇ ∥≤ − ∥∇ ∥
x x
e µηnsup u(x)
−
≤ ∥∇ ∥
x
(cid:0) e µηn u (cid:1) .
≤O − ∥ ∥C1
2. If f() is convex, we have
·
(cid:13) (cid:13)
sup(cid:13) un+1(x)(cid:13) sup un(x) ( u ).
∇ ≤ ∥∇ ∥≤O ∥ ∥C1
x x
The notation () does not depend on n, η or upper bounds for derivatives of F in () with orders higher
than the seconOd o·rder. O ·
Next, we consider the second-order gradients:
2un+1(x)=E(cid:2)(cid:0)
I η
2F(x;ξ)(cid:1) 2un(y)(cid:0)
I η
2F(x;ξ)(cid:1)
η
un(y)⊺ 3f(x)(cid:3)
.
∇ − ∇ ∇ − ∇ − ∇ ∇
Then we also obtain a recursion for the 2-norm of its vector form:
sup(cid:13) (cid:13)vec(cid:0) 2un+1(x)(cid:1)(cid:13)
(cid:13)
(48)
∇ 2
x
(cid:124) (cid:123)(cid:122) (cid:125)
an+1
=sup(cid:13)
(cid:13)
2un+1(x)(cid:13)
(cid:13)
(49)
∇ F
x
sup(cid:13) (cid:13)(cid:0)
I η
2F(x;ξ)(cid:1) 2un(y)(cid:0)
I η
2F(x;ξ)(cid:1)(cid:13)
(cid:13)
+ηsup(cid:13)
(cid:13)
un(y)⊺ 3f(x)(cid:13)
(cid:13)
≤ − ∇ ∇ − ∇ F ∇ ∇ F
x x
=sup(cid:13) (cid:13)vec(cid:0)(cid:0)
I η
2F(x;ξ)(cid:1) 2un(y)(cid:0)
I η
2F(x;ξ)(cid:1)(cid:1)(cid:13)
(cid:13)
+ηsup(cid:13)
(cid:13)
un(y)⊺ 3f(x)(cid:13)
(cid:13)
− ∇ ∇ − ∇ 2 ∇ ∇ F
x x
=su xp(cid:13) (cid:13) (cid:13)(cid:0) I −η ∇2F(x;ξ)(cid:1) ⊗2 vec(cid:0) ∇2un(y)(cid:1)(cid:13) (cid:13) (cid:13) 2+ηsu xp(cid:13) (cid:13) ∇un(y)⊺ ∇3f(x)(cid:13) (cid:13) F
≤su xp(cid:13) (cid:13) (cid:13)(cid:0) I −η ∇2F(x;ξ)(cid:1) ⊗2(cid:13) (cid:13) (cid:13) 2(cid:13) (cid:13)vec(cid:0) ∇2un(y)(cid:1)(cid:13) (cid:13) 2+ηsu xp(cid:13) (cid:13) ∇un(y)⊺ ∇3f(x)(cid:13) (cid:13) F
sup(cid:13)
(cid:13)I η
2F(x;ξ)(cid:13) (cid:13)2(cid:13) (cid:13)vec(cid:0) 2un(y)(cid:1)(cid:13)
(cid:13)
+ηsup(cid:13)
(cid:13)
un(y)⊺ 3f(x)(cid:13)
(cid:13) ,
≤ − ∇ 2 ∇ 2 ∇ ∇ F
x x
where is the Kronecker product, and we used that A B A B for matrices A and B.
⊗ ∥ ⊗ ∥2 ≤∥ ∥2∥ ∥2
1. If f() is strongly-convex, the recursion becomes
·
sup(cid:13) (cid:13)vec(cid:0) 2un+1(x)(cid:1)(cid:13) (cid:13) (1 ηµ)2sup(cid:13) (cid:13)vec(cid:0) 2un(x)(cid:1)(cid:13) (cid:13) +ηsup(cid:13) (cid:13) un(y)⊺ 3f(x)(cid:13) (cid:13) . (50)
∇ 2 ≤ − ∇ 2 ∇ ∇ F
x (cid:124) (cid:123)(cid:122) (cid:125) x x
(cid:124) (cid:123)(cid:122) (cid:125) c (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
an+1 an bn
According to our assumption and previous results, b (ηe µηn u ). Also we choose small η such
that 1 ηµ>0. It holds for the recursion that n ≤O − ∥ ∥C1
−
n
(cid:88)
a cn+1a + cn sb .
n+1 0 − s
≤
s=0
40In our case, it becomes
n
sup(cid:13) (cid:13)vec(cid:0) ∇2un(x)(cid:1)(cid:13) (cid:13)
2
≤(1 −ηµ)2nsup(cid:13) (cid:13)vec(cid:0) ∇2u(x)(cid:1)(cid:13) (cid:13) 2+(cid:88) (1 −ηµ)2(n −s) O(ηe −µηs ∥u ∥C1)
x x
s=0
n
≤(1 −ηµ)nsup(cid:13) (cid:13)vec(cid:0) ∇2u(x)(cid:1)(cid:13) (cid:13) 2+(cid:88) e −µη2(n −s) O(ηe −µηs ∥u ∥C1)
x
s=0
n
≤e −µηnsup(cid:13) (cid:13)vec(cid:0) ∇2u(x)(cid:1)(cid:13) (cid:13) 2+(cid:88) eµηs O(ηe −2µηn ∥u ∥C1)
x
s=0
=e −µηnsup(cid:13) (cid:13)vec(cid:0) ∇2u(x)(cid:1)(cid:13) (cid:13) 2+ eηµ e( ηn µ+1) 1−1 O(ηe −2µηn ∥u ∥C1)
x
−
≤e −µηnsup(cid:13) (cid:13)vec(cid:0) ∇2u(x)(cid:1)(cid:13) (cid:13) 2+ e eη ηµ µ(n+1 1) O(ηe −2µηn ∥u ∥C1)
x
−
≤e −µηnsup(cid:13) (cid:13)vec(cid:0) ∇2u(x)(cid:1)(cid:13) (cid:13) 2+ eηe µηµn 1O(ηe −2µηn ∥u ∥C1)
x
−
=e −µηnsup(cid:13) (cid:13)vec(cid:0) ∇2u(x)(cid:1)(cid:13) (cid:13) 2+ eηµ1 1O(ηe −µηn ∥u ∥C1)
x
−
≤e −µηnsup(cid:13) (cid:13)vec(cid:0) ∇2u(x)(cid:1)(cid:13) (cid:13) 2+ η1 µO(ηe −µηn ∥u ∥C1)
x
=e −µηnsup(cid:13) (cid:13)vec(cid:0) ∇2u(x)(cid:1)(cid:13) (cid:13) 2+ O(e −µηn ∥u ∥C1)
x
(e µηn u ), (51)
≤O − ∥ ∥C2
where we used that ex x+1.
≥
2. If f() is convex, we have
·
sup(cid:13) (cid:13)vec(cid:0) 2un+1(x)(cid:1)(cid:13)
(cid:13)
sup(cid:13) (cid:13)vec(cid:0) 2un(x)(cid:1)(cid:13)
(cid:13) + (η).
∇ 2 ≤ ∇ 2 O
x x
Therefore, we obtain
sup(cid:13) (cid:13)vec(cid:0) 2un(x)(cid:1)(cid:13)
(cid:13) (T u ).
∇ 2 ≤O ∥ ∥C2
x
Next, we proceed with induction.
1. If f() is strongly-convex, assume it holds that for any 0 < s p, we have sup vec( sun(x))
(e · µηn u ). Then, we study the p+1-th order gradient of≤ un+1: x∥ ∇ ∥F ≤
O − ∥ ∥Cs
(cid:13) (cid:13)vec(cid:0) ∇p+1un+1(x)(cid:1)(cid:13) (cid:13) 2 =(cid:13) (cid:13) (cid:13)(cid:0) I −η ∇2F(x;ξ)(cid:1) ⊗p+1 vec(cid:0) ∇p+1un(y)(cid:1)(cid:13) (cid:13) (cid:13) 2+η O(e −µηn ∥u ∥Cp).
The first term is the result by applying the operator p+1 times on u. For other terms, at least one
is applied on (cid:0) I η 2F(x;ξ)(cid:1), which res∇ults in a η and gradients of F higher than the second-order
(∇which is hidden in− (∇ )). Also, the gradients of u are at most the p-th order, which by induction are
already upper boundOed·. Therefore, similarly to Equation (50), we obtain
sup(cid:13) (cid:13)vec(cid:0) p+1un+1(x)(cid:1)(cid:13)
(cid:13)
∇ 2
x
(cid:13) (cid:13)
≤su xp(cid:13) (cid:13)(cid:0) I −η ∇2F(x;ξ)(cid:1) ⊗p+1 vec(cid:0) ∇p+1un(y)(cid:1)(cid:13)
(cid:13)
2+η O(e −µηn ∥u ∥cp)
≤sup(cid:13) (cid:13)(cid:0) I −η ∇2F(x;ξ)(cid:1)(cid:13) (cid:13)p 2+1 sup(cid:13) (cid:13)vec(cid:0) ∇p+1un(x)(cid:1)(cid:13) (cid:13) 2+η O(e −µηn ∥u ∥cp)
x x
41≤(1 −ηµ)p+1sup(cid:13) (cid:13)vec(cid:0) ∇p+1un(x)(cid:1)(cid:13) (cid:13) 2+η O(e −µηn ∥u ∥cp),
x
which can be then recursively bounded similarly to Equation (51), i.e.,
sup(cid:13) (cid:13)vec(cid:0) p+1un(x)(cid:1)(cid:13)
(cid:13)
∇ 2
x
≤e −µηnsup(cid:13) (cid:13)vec(cid:0) ∇p+1u(x)(cid:1)(cid:13) (cid:13) 2+ O(e −µηn ∥u ∥cp)
x
(e µηn u ).
≤O − ∥ ∥cp+1
Now we know that
(cid:88) (cid:12) (cid:12)DJun(x)(cid:12) (cid:12) ≤O(cid:0) e −µηn ∥u ∥Cp(cid:1) .
1 J p
≤| |≤
It follows
T/η 1 T/η 1
η⌊ (cid:88)⌋− (cid:88) (cid:12) (cid:12)DJuk(x)(cid:12) (cid:12) ≤η⌊ (cid:88)⌋− O(cid:0) e −µηk ∥u ∥Cp(cid:1)
k=0 1 J p k=0
≤| |≤
1 e ηµ T/η
− ⌊ ⌋
η − ( u )
≤ 1 e µη O ∥ ∥Cp
η− −
( u )
≤ 1 e µηO ∥ ∥Cp
− −
( u ).
≤O ∥ ∥Cp
2. If f() is convex, we assume for any 0<s p, sup vec( sun(x)) (Ts 1 u ). Similarly to
the s·trongly-convex case, we obtain ≤ x∥ ∇ ∥F ≤O − ∥ ∥Cs
(cid:13) (cid:13)vec(cid:0) ∇p+1un+1(x)(cid:1)(cid:13) (cid:13) 2 =(cid:13) (cid:13) (cid:13)(cid:0) I −η ∇2F(x;ξ)(cid:1) ⊗p+1 vec(cid:0) ∇p+1un(y)(cid:1)(cid:13) (cid:13) (cid:13) 2+η O(Tp −1 ∥u ∥Cp),
which implies
sup(cid:13) (cid:13)vec(cid:0) p+1un(x)(cid:1)(cid:13) (cid:13) (Tp u ).
∇ 2 ≤O ∥ ∥Cp+1
x
Further it holds that
T/η 1
η⌊ (cid:88)⌋− (cid:88) (cid:12) (cid:12)DJuk(x)(cid:12) (cid:12) (cid:0) Tp+1 u (cid:1) .
≤O ∥ ∥Cp
k=0 1 J p
≤| |≤
Proof for Theorem 5.2. Theassumptionof F(x;ξ) < implies f(x) < . AccordingtoLemmaB.1,
c8 c8
we have boundedness of b(x) , where∥ b(x) is∥the d∞rift term∥of HA∥ -SME.∞According to Lemma B.1,
∥ ∥C6 ⊺
we have boundedness of the diffusion term of HA-SME, i.e., ∥U(x)L(x)U(x) ∥C6 is upper bounded. Denote
η
Mn :=(cid:80) (cid:12) (cid:12)DJun(x)(cid:12) (cid:12).
p 1 J p
Similar≤t|o|t≤he proof for Theorem 5.1, we first have
(cid:13) (cid:13)
(cid:13) (cid:13)un+1 un+η f u 1 η2Σ: 2un(cid:13) (cid:13) (cid:0) η3s3Mn(cid:1) .
(cid:13) − ∇ ·∇ − 2 ∇ (cid:13) ≤O 3
L∞
42First, we expand eη un using Taylor’s expansion and Lemma B.3,
L
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:13)eη Lun −un −η Lun
−
1 2η2 L2un
−
31 !η3 L3un(cid:13) (cid:13)
(cid:13)
≤O(cid:0) η4λ3s4M 8n(cid:1) .
L∞
Then using Lemma B.4, we find the expansion of un, 2un and 3un, respectively.
L L L
(cid:18) (cid:19)
1 1
un = f un+η 2f f un+ Σ: 2un
L −∇ ·∇ −2∇ ∇ ·∇ 2 ∇
+ η2 (cid:18) 2(cid:0) 2f(cid:1)2 f un+ 1(cid:0) Σ 2f + 2fΣ(cid:1) : 2un(cid:19) + (cid:0) η3λ3sMn(cid:1) . (52)
2 −3 ∇ ∇ ·∇ 2 ∇ ∇ ∇ O 2
For 2un, we have
L
⊺ ⊺
2un = f(x) 2f un+ f 2un f
L ∇ ∇ ∇ ∇ ∇ ∇
(cid:18) (cid:19)
+η f⊺(cid:0) 2f(cid:1)2 un+ f⊺ 2f 2un f 1 Σ:(cid:0) 2un 2f + 2f 2un(cid:1) + (Mn)
∇ ∇ ∇ ∇ ∇ ∇ ∇ − 2 ∇ ∇ ∇ ∇ O 3
+
(cid:0) η2λ3s2Mn(cid:1)
.
O 4
For 3un, we have
L
3un = f⊺(cid:0) 2f(cid:1)2 un 3 f⊺ 2f( un)2+ (Mn)+ (cid:0) ηλ3s3Mn(cid:1) .
L −∇ ∇ ∇ − ∇ ∇ ∇ O 3 O 6
Summarizing the above, we obtain
(cid:13) (cid:13)eη Lun −un+1(cid:13) (cid:13)
L∞
≤O(cid:0) η3s3M 3n+η4λ3s4M 8n(cid:1) .
Following the last few steps in the proof of Theorem 5.1, we obtain
n 1
En (cid:88)− (cid:0) η3s3Mk+η4λ3s4Mk(cid:1)
≤ O 3 8
k=0
finish the proof.
Proof for Theorem 5.3. The proof follows similarly to the proof of Theorem 5.2. For SME-2, in the expansion
of un, i.e., Equation (52), the term
L
η2 (cid:18) 2(cid:0) 2f(cid:1)2 f un+ 1(cid:0) Σ 2f + 2fΣ(cid:1) : 2un(cid:19)
2 −3 ∇ ∇ ·∇ 2 ∇ ∇ ∇
is missing. Therefore, after cancellation of terms, there will be error terms of λ2s un and λ(cid:13) (cid:13) 2un(cid:13) (cid:13). For
SPF, in Equation (52), the term ∥∇ ∥ ∇
η2 (cid:18) 1(cid:0) Σ 2f + 2fΣ(cid:1) : 2un(cid:19)
2 2 ∇ ∇ ∇
is missing. Therefore, it results in additional error of λ(cid:13)
(cid:13)
2un(cid:13) (cid:13).
∇
Proof for Theorem 5.4. The proof is simply to combine the results from Theorem 5.2 and Lemma 5.2.
43B.2 Helper Lemmas
TL he em rem ea xiB st.1 co. nF so tar nn ts≥ η0, aa ns dsu Cme >f 0∈
,
bC obn th+2 in(R ded p) ea nn dd ende tn oo fte λλ an:= ds su ,p sx u∈cR hd t(cid:13) (cid:13) h∇ at2f fo( rx) a(cid:13) (cid:13) na yn ηds sa: t= isfs yu ip nx g∈ηRd <∥∇ ηf( ax n) d∥.
0 0
ηλ<C, the drift term of HA-SME, defined in in Equation (22), satisfies
max [b(x)] < (s) and max [b(x)] < (s+λ),
0 i d| i | O 0 i d∥ i ∥Cn O
≤≤ ≤≤
where [b(x)] is the i-th entry of b(x), and () hides the dependence on the upper bounds for the derivatives
i
O ·
of f higher than the second-order derivatives.
Proof. Recall the definition of the drifting term of HA-SME in Equation (22)
log(I ηΛ(x)) ⊺
b(x)=U(x) − U(x) f(x).
ηΛ(x) ∇
We first note that b(x) can be written as
b(x)=
(cid:88)∞ 1 ηp(cid:0) 2f(x)(cid:1)p
f(x).
− p+1 ∇ ∇
p=0
The (i,j)-th element of Ap for matrix A Rd ×d can be represented as
∈
d d d
(cid:88) (cid:88) (cid:88) (cid:88)
[Ap] = [A] [A] [A] [A] [A] .
i,j i,s1 s1,s2 s2,s3··· sp−2,sp−1 sp−1,j
s1=1 s2=1 s3=1 sp−1
Therefore, for any i, the i-th element of b(x) can be written as
[b(x)] =
(cid:88)∞ 1 ηp(cid:104)(cid:0) 2f(x)(cid:1)p f(x)(cid:105)
i
− p+1 ∇ ∇ i
p=0
d d d
= (cid:88)∞ 1 ηp (cid:88) (cid:2) 2f(x)(cid:3) (cid:88) (cid:2) 2f(x)(cid:3) (cid:88) (cid:2) 2f(x)(cid:3) [ f(x)]
− p+1 ∇ i,s1 ∇ s1,s2··· ∇ sp−1,sp ∇ sp
p=0 s1=1 s2=1 sp=1
d d d
(cid:88)∞ 1 (cid:88) (cid:88) (cid:88)
= ηp ∂ ∂ f(x) ∂ ∂ f(x) ∂ ∂ f(x)∂ f(x).
− p+1
i s1 s1 s2
···
sp−1 sp sp
p=0 s1=1 s2=1 sp=1
(cid:124) (cid:123)(cid:122) (cid:125)
(A)
Notice that Term (A) can be expanded as a summation of dp terms, each of which contains p+1 factors.
Note that the first-order and second-order partial derivatives are upper bounded by s and λ respectively, i.e.,
Dαf s for a =1 and Dαf λ for a =2. Therefore, we have
| |≤ | | | |≤ | |
(cid:88)∞ 1
[b(x)] ηpdpλps
i
| |≤ p+1
p=0
=s(cid:88)∞ 1
(ηdλ)p.
p+1
p=0
We get a power series in the above equation, and according to Cauchy–Hadamard theorem [Cauchy, 1821],
the convergence radius for g(x)=(cid:80)
∞
1 xp is
p=0 p+1
1 1
= =1.
(cid:12) (cid:12)1 1
limsup (cid:12) 1 (cid:12)p
p (cid:12)p+1(cid:12)
→∞
44Then we know that [b(x)] is upper bounded by (s) as long as
i
| | O
1
ηdλ<1 = ηλ< .
⇒ d
Next, we consider the first-order derivative of b(x). We denote the upper bound for higher order partial
derivatives as B, i.e., max Dαf B. Recall that in b(x), Term (A) contains dp terms and each
2<a n+2
term has p factors of second-|or|≤der de|rivat|iv≤es of f(x) and one factor of the first-order derivative of f(x). If
we take gradient of Term (A) w.r.t. x, each term, according to the product rule of gradients and by taking
derivatives w.r.t. each factor, will result in: (1) p terms, each consisting of one factors of ∂3f, p 1 factor of
∂2f, and one factor of ∂f; (2) 1 term consisting of p+1 factors of ∂2f. Therefore, we can boun−d
∂ [b(x)] (cid:88)∞ 1 ηpdp(cid:0) pBsλp 1+λp+1(cid:1)
j i −
| |≤ p+1
p=0
(cid:88)∞ p (cid:88)∞ 1
=s dpηB(ηλ)p 1+λ (dηλ)p
−
p+1 p+1
p=0 p=0
For the second summation, the convergence radius of the power series is the same as the upper bound for
[b(x)] , and it is bounded by (λ) for sufficiently small η and ηλ. For the first summation, comparing to the
i
upper bound of [b(x)] , the coOefficients of the power series are multiplied by p. However, this will not change
i
the convergence radius. It is known that for sequences a and b , if limsup a =A and lim b =B,
p p p p p p
then limsup (a b )=AB. Note that →∞ →∞
p p p
→∞
1
lim p p =1.
p | |
→∞
Therefore, by applying Cauchy–Hadamard theorem, multiplying the coefficients with p would not change the
radius of convergence. Therefore, if we have ηλ and ηB smaller than the convergence radius of (cid:80) 1 xp,
p 1+p
that is 1, the first summation in the upper bound of ∂ [b(x)] can be upper bounded by some constants. As
j i
a result, we can conclude that | |
∂ [b(x)] (s+λ).
j i
| |≤O
Following the same idea, we identify that after applying twice partial derivatives on b(x), i.e., ∂ ∂ [b(x)] ,
j k i
each term in Term (A) becomes:
p∂4f(cid:0) ∂2f(cid:1)p −1
∂f +p(p
1)(cid:0) ∂3f(cid:1)2(cid:0) ∂2f(cid:1)p −2
∂f
+(2p+1)∂3f(cid:0) ∂2f(cid:1)p
−
Thus, we have
∂ ∂ [b(x)] (cid:88)∞ 1 ηpdp(cid:0) p(p 1)B2sλp 2+pBsλp 1+(2p+1)Bλp(cid:1)
j k i − −
| |≤ p+1 −
p=0
(cid:88)∞ p(p 1) (cid:88)∞ p (cid:88)∞ 2p+1
=s − dp(Bη)2(λη)p −2+s dpBη(λη)p −1+B dp(λη)p.
p+1 p+1 p+1
p=0 p=0 p=0
By the same reason as what we did for the ∂ [b(x)] , the three summations converge, and are upper bounded
j i
by some constants not relying on λ.
For derivatives higher than the second ones, we can follow the same logic to show that as long as ηB and
ηλ is small enough, the derivatives are upper bounded and such bound does not rely on λ. Specifically, for
45the k-th order derivative of b(x) with k 2, we will have terms of the following form derived from each term
in Term (A): ≥
 
m
(cid:0) pk(cid:1) (cid:89) ∂qjf(cid:0) ∂2f(cid:1)s (∂f)r,
O
j=1
with m+s+r =p+1 and q 3, s p and r 1. As we only consider derivatives of b(x) up to some finite
j
order. We have finite such term≥s and≤they can≤be bounded using the idea presented previously.
Lemma B.2. For n
≥
0, assume f
∈
C
bn+2(cid:0) Rd(cid:1)
and [Σ]
i,j ∈
C
bn(cid:0) Rd(cid:1)
for all 1
≤
i,j
≤
d where [A]
i,j
is
the (i,j)-th entry of matrix A, then there exists η ,C > 0, such that for any η < η and ηλ < C, where
0 0
(cid:13) (cid:13)
λ pr: i= ncs iu plp ex ∈flR od w(cid:13) s∇ a2 tif s( fyx)(cid:13) and C,η 0 does not depend on λ, we have that the diffusion term D(x) of stochastic
(cid:13) (cid:13)
(cid:13)[D(x)D(x)⊺ ] (cid:13)
(cid:13) i,j(cid:13)
max Cn < (1),
0 i,j d η O
≤ ≤
where () hides the dependence on the upper bounds for the derivatives of f higher than the second-order
O ·
derivatives and max [Σ] .
0 ≤i,j ≤d ∥ i,j ∥Cn
Proof. The proof idea is similar to the proof of Lemma B.1, however, in this case, we do not have an explicit
form for the coefficients of the power series. The diffusion term can be represented as
p 1
D(x)D(x)⊺ =(cid:88)∞ ηp(cid:88)−
a
k,p 1 k
(cid:0) 2f(x)(cid:1)k Σ(x)(cid:0) 2f(x)(cid:1)p −1 −k
,
− − · ∇ ∇
p=1 k=0
where a are absolute constants such that by Taylor expansion at (0,0),
k,p 1 k
− −
+
log(1 x)(1 y) (cid:88)∞
g(x,y):= − − = a xsym.
s,m
xy (x+y)
− s,m 0
≥
For the power series above, we know that its convergence radius is greater or equal than 1 for both x
and y, meaning that x < 1 and y < 1 is a sufficient condition for it to converge. Also, according to the
Cauchy–Hadamard theorem for multiple variables [Shabat, 1992], the signs of the coefficients do affect the
convergence radius, therefore the convergence radius of
+
(cid:88)∞
g(x,y):= a xsym
(cid:101) s,m
| |
s,m 0
≥
is also greater or equal than 1. Now we look back into D(x)D(x)⊺. We note that the (i,j)-th element of
ApBAk for A,B Rd ×d can be written as
∈
d d d d d d
(cid:2) ApBAk(cid:3)
=
(cid:88)
[A]
(cid:88)
[A]
(cid:88)
[A]
(cid:88)
[S]
(cid:88)
[A]
(cid:88)
[A]
i,j i,s1 s1,s2··· sp−1,sp sp,m m,q1 q1,q2···
s1=1 s2=1 sp=1 m=1 q1=1 q2=1
d
(cid:88)
[A] [A] .
qk−2,qk−1 qk−1,j
qk−1=1
Therefore, we can derive that
(cid:2) ⊺(cid:3)
D(x)D(x)
i,j
46p 1 d d d d
=(cid:88)∞ ηp(cid:88)−
a
(cid:88) (cid:2) 2f(x)(cid:3) (cid:88) (cid:2) 2f(x)(cid:3) (cid:88) (cid:2) 2f(x)(cid:3) (cid:88)
[Σ(x)]
k,p −1 −k ∇ i,s1 ∇ s1,s2··· ∇ sk−1,sk sk,m
p=1 k=0 s1=1 s2=1 sk=1 m=1
d d d
(cid:88) (cid:2) 2f(x)(cid:3) (cid:88) (cid:2) 2f(x)(cid:3) (cid:88) (cid:2) 2f(x)(cid:3) (cid:2) 2f(x)(cid:3)
∇ m,q1 ∇ q1,q2··· ∇ qp−k−3,qp−k−2 ∇ qp−k−2,j
q1=1 q2=1 qp−k−2=1
p 1 d d d d
(cid:88)∞ (cid:88)− (cid:88) (cid:88) (cid:88) (cid:88)
= ηp a ∂ ∂ f(x) ∂ ∂ f(x) ∂ ∂ f(x) [Σ(x)]
k,p −1 −k i s1 s1 s2 ··· sk−1 sk sk,m
p=1 k=0 s1=1 s2=1 sk=1 m=1
d d d
(cid:88) (cid:88) (cid:88)
∂ ∂ f(x) ∂ ∂ f(x) ∂ ∂ f(x)∂ ∂ f(x).
m q1 q1 q2
···
qp−k−3 qp−k−2 qp−k−2 j
q1=1 q2=1 qp−k−2=1
Let us look at the term associated with a , i.e., everything after a in the above equation. It
k,p 1 k k,p 1 k
contains p 1 summations, thus resulting in− dp−1 terms. Each term has p fa−ct−ors consisting of one element
−
of Σ(x) and−others being second-order derivatives of f(x). Denoting S :=max [Σ(x)] , we can derive
i,j ∥ i,j ∥C6
(cid:12) (cid:12)(cid:2) D(x)D(x)⊺(cid:3) (cid:12) (cid:12) (cid:88)∞ ηp(cid:88)p −1 a dp 1Sλp 1
(cid:12) i,j(cid:12)
≤ |
k,p −1 −k
|
− −
p=1 k=0
p 1
=ηS(cid:88)∞ (cid:88)−
a
k,p 1 k
(ηdλ)p −1
| − − |
p=1k=0
p 1
=ηS(cid:88)∞ (cid:88)−
a
k,p 1 k
(ηdλ)k(ηdλ)p −1 −k
| − − |
p=1k=0
=ηS
(cid:88)∞
a (ηdλ)s(ηdλ)m.
s,m
| |
s,m 0
≥
According to our previous reasoning, the power series is convergent if
1
ηdλ<1 = ηλ< ,
⇒ d
and we have
(cid:13) (cid:13)
(cid:13)[D(x)D(x)⊺ ] (cid:13)
(cid:13) i,j(cid:13)
max C0 < .
i,j η ∞
Next, we consider the first-order derivative of D(x)D(x)⊺. Similar to the reasoning in the proof of
Lemma B.1, after taking derivatives there are pdp 1 terms associated with coefficients a , and each
− k,p 1 k
termhaspfactorsconsistingofonefactorbeingderivativesofΣ(x)orΣ(x)andothersbeingu−pt−othird-order
derivatives of f(x). Specifically, we have the following terms associated with a :
k,p 1 k
− −
(p
1)∂3f(cid:0) ∂2f(cid:1)p −2 Σ+(cid:0) ∂2f(cid:1)p −1
∂Σ.
−
Let B be the constant such that max Dαf B, and let m:=max Bη,λη . We have
2<a n+2anda=2
| |≤ | |̸ | |≤ { }
(cid:12) (cid:12) (cid:12)∂ k(cid:2) D(x)D(x)⊺(cid:3) i,j(cid:12) (cid:12)
(cid:12)
≤ηS(cid:88)∞ (cid:88)p −1 |a
k,p −1 −k
|(p −1)dp −1Bη(λη)p −2+ηS(cid:88)∞ (cid:88)p −1 |a
k,p −1 −k
|(dλη)p −1.
p=1k=0 p=1k=0
The second summation can be bounded the same way as before. For the first summation, it is upper bounded
by
p 1
ηS(cid:88)∞ (cid:88)−
a
k,p 1 k
(p 1)(dm)k(dm)p −1 −k
| − − | −
p=1k=0
47=ηS
(cid:88)∞
(s+m)a (dm)s(dm)m.
s,m
| |
s,m 0
≥
Nowwehaveapowerserieswithcoefficientsmultipliedby(s+m)comparedtothepreviousone. Howeverthis
would not change the convergence radius. Denote the convergence radius of g(x,y) as r and r . According
(cid:101) 1 2
to Cauchy–Hadamard theorem for multiple variables, we have
limsup |a s,mr 1sr 2m |s+1 m =1.
s+m
→∞
We note that r and r are also convergence radius for our new power series, since
1 2
sl +im ms →u ∞p |(s+m)a s,mr 1sr 2m |s+1 m = sl +im ms →u ∞p |a s,mr 1sr 2m |s+1 m
s
(cid:124)+l mim
→∞|
(cid:123)s (cid:122)+m |s+1 m
(cid:125)
=1
= limsup |a s,mr 1sr 2m |s+1 m =1.
s+m
→∞
Therefore a sufficient condition for the convergence of the power series is
1
max ηλ,ηB < .
{ } d
The proof is similar for higher-order derivatives. Every time we take derivative, the coefficients would be
multiplied by a factor of order s+m, however, that would not change the radius of convergence. Specifically,
for the k-th derivative of D(x)D(x)⊺ with k 2, terms associated with a have the following form:
k,p 1 k
≥ − −
 
m
(cid:0) pk(cid:1) (cid:89) ∂qjf(cid:0) ∂2f(cid:1)s ∂rΣ,
O
j=1
with m+s = p 1, q 3, s p 1 and r k. These terms can be bounded using similar idea of the
j
previous proof. − ≥ ≤ − ≤
Lemma B.3. Let be the infinitesimal generator for an SDE defined as
L
dX =b(X )dt+D(X )dW ,
t t t t
where b:Rd Rd and D :Rd Rd ×d. We have for any n 1 and t>0, it holds
→ → ≥
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13)
(cid:13)et Lu −n (cid:88)−1 t pp !Lpu(cid:13) (cid:13)
(cid:13)
(cid:13)
≤O(cid:16) tnm iax ∥b ∥C0∥b ∥n C−2(1 n−1)(cid:13) (cid:13)DD⊺(cid:13) (cid:13)n C2(n−1)∥u ∥C2n(cid:17) ,
p=0 L∞
(cid:13) (cid:13)
where b :=max [b] and DD⊺ :=max (cid:13)[DD⊺ ] (cid:13) .
∥ ∥Cm 0 ≤i ≤d ∥ i ∥Cm ∥ ∥Cm 0 ≤i ≤d(cid:13) i,j(cid:13) Cm
Proof. According to the Taylor expansion of et on t, we obtain for any x
L
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)et Lu(x) −n (cid:88) p=− 01 t pp !Lpu(x)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)=(cid:12) (cid:12) (cid:12) (cid:12)t nn ! ∂n (e ∂t L t)u n(x)(cid:12) (cid:12) (cid:12) (cid:12) t=s(cid:12) (cid:12) (cid:12) (cid:12),
for some 0 s t. According to Feng et al. [2017, Lemma 2.1], et is a contraction, therefore, we have
L
≤ ≤
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)et Lu(x) −n (cid:88) p=− 01 t pp !Lpu(x)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
L∞
=(cid:13) (cid:13) (cid:13) (cid:13)t nn ! ∂n (e ∂t L t)u n(x)(cid:12) (cid:12) (cid:12) (cid:12) t=s(cid:13) (cid:13) (cid:13) (cid:13) L∞
48tn
nu .
≤ n!∥L ∥L∞
Note that is linear operator and it will apply 2 operator Therefore, we obtain gradients of b and DD⊺
up to the 2L (n 1)-th order and gradients of u up∇to the 2n-th order. Note that we bound the first b using
b and the−following n 1 occurrences of b using b , since they have different dependence on λ for
∥ HA∥ -C S0 ME according to Lemm−a B.1. It would be more∥co∥nC v2 e( nn− ie1 n) t for fine-grained analysis later.
Lemma B.4. For n ≥0, assume f ∈C b2n(Rd) and [Σ(x)]
i,j
∈C b2n −2(cid:0) Rd(cid:1) for any 0 ≤i,j ≤d. Let
L
be the
infinitesimal generator for HA-SME, which depends on η. We have for any function u:Rd R, and integer
→
n,p>0,
(cid:13) (cid:13) (cid:13) (cid:13)∂ (p ∂L ηn )pu(cid:13) (cid:13) (cid:13)
(cid:13)
≤O(cid:0) λn+p −1sn ∥u ∥C2n(cid:1) ,
L∞
(cid:13) (cid:13)
tw hh eer de erλ iv: a= tis vu ep sx o∈fR fd(cid:13) o∇ th2 ef r( tx h) a(cid:13) n, s th: e= ss eu cp onx ∈dR -od r∥ d∇ erf d(x er) ∥ ivaa tn id veO
s
( a· n) dhi mde as xthe depen [Σde ]nce on
.
the upper bounds for
0 ≤i,j ≤d ∥ i,j ∥Cn
Proof. We note that when taking partial gradient of b w.r.t. η, the dependence on λ on the upper bounds for
(cid:13)(cid:104) (cid:105) (cid:13) (cid:13)(cid:104) (cid:105) (cid:13)
the derivatives, i.e., (cid:13) ∂b (cid:13) , would increase its order by 1. That is to say, if (cid:13) ∂kb (cid:13) (λq), then
(cid:13)(cid:104) (cid:105) (cid:13) (cid:13) ∂η i(cid:13) Cm (cid:13) (∂η)k i(cid:13) Cm ≤O
(cid:13) ∂k+1b (cid:13) (λq+1). The reason is that gradients w.r.t. x of b can be written as power series of η and
(cid:13) (∂η)k+1 i(cid:13) Cm ≤O
ηλ (see the proof of Lemma B.1). Whenever we taking partial derivative w.r.t. η, the order of η compared to
λ in each term is decreased by 1. Then we can take a λ factor out and the remaining series still converges
(the same reasoning as in the proof of Lemma B.1). The same logic applies to DD⊺. In nu, we will have at
most n 1 occurrences of gradients of b. Therefore, before taking derivatives w.r.t. ηL, the upper bounds
would b−e at most λn 1. After taking the derivative p times, the order of λ is at most n+p 1. Similar logic
−
applies to DD⊺ and corresponding Lemma B.2. −
C Exact Match of SGD on Quadratics
Before diving into the proof, we will introduce some basics about complex normal distribution and complex
OU process.
C.1 Complex Normal Distribution
Since our proposed SDE may operate in complex space, similar to PF, we provide here some basics about the
complex distribution, specifically the complex normal distribution. A complex random vector is defined by
two real random variables:
z =x+iy,
wherexandy aretworealrandomvectors,whichmaybecorrelated. Therandomvectorz iscalledacomplex
(cid:20) (cid:21)
x
normal vector if is a normal vector. In contrast to real-valued random vector, there are three parameters
y
that define a complex normal variable (µ,Γ,C),
CN
µ:=E[z] (Expectation)
(cid:104) (cid:105)
Γ:=E (z E[z])(z E[z])H (Covariance)
− −
(cid:104) ⊺(cid:105)
C :=E (z E[z])(z E[z]) (Pseudo-Covariance)
− −
49Similar to the real case, for z (µ,Γ,C), the following property holds
∼CN
Az+b
(cid:0) Aµ+b,AΓAH,ACA⊺(cid:1)
.
∼CN
If we look at the real and imaginary part separately, we have
Γ=Cov[x,x]+Cov[y,y]+i(Cov[y,x] Cov[x,y]),
− (53)
C =Cov[x,x] Cov[y,y]+i(Cov[y,x]+Cov[x,y]).
−
For the covariance of x and y, we have
1
Cov[x,x]= Re(Γ+C),
2 (54)
1
Cov[y,y]= Re(Γ C).
2 −
C.2 Helper Lemmas
Lemma C.1 (Topsøe [2007, Equation (22)]). For x R and x 0, it holds that
∈ ≥
x(6+x)
log(1+x) .
≤ 6+4x
Lemma C.2. For x R and 1 x 2, it holds that
∈ ≤ ≤
(5 8x)log(x 1) 16 8x.
− − ≥ −
Proof. Denote f(x)=(5 8x)log(x 1) 16+8x, and we have
− − −
8(x 1)log(x 1) 3
f ′(x)= − − − − .
x 1
−
Let g(x)=(x 1)log(x 1), then we get
− −
g (x)=1+log(x 1).
′
−
We know that the minimum of g(x) is obtained at x=1/e+1, i.e., g(x) 1/e. Therefore,
≥−
8/e 3
f ′(x) − 0.
≤ x 1 ≤
−
Hence f(x) is decreasing, and f(x) f(2) 0, which concludes the proof.
≥ ≥
Lemma C.3. For x R and 1 x 2, it holds that
∈ ≤ ≤
4x4log2(3/2) 5/2 3x.
− ≤ −
Proof. Denote f(x)= 4x4log2(3/2) 2.5+3x. We have
− −
f (x)= 16x3log2(3/2)+3,
′
−
and
f (x)= 48x2log2(3/2)<0.
′′
−
(cid:113)
Therefore, f ′(x) is decreasing and is 0 when x = 3 16log3 2(3/2). Hence, f(x) obtains its maximum at this
(cid:16)(cid:113) (cid:17)
point and f 3 3 <0, which completes the proof.
16log2(3/2)
50Lemma C.4. For x R and 1 x 2, it holds that
∈ ≤ ≤
x3(cid:0) 4log2(3/2) 6log(3/2)log(4/3)(cid:1) 0.09 0.1x.
− ≤ −
Proof. Denote f(x)=x3(cid:0) 4log2(3/2) 6log(3/2)log(4/3)(cid:1) 0.09+0.1x. We have
− −
f (x)=3x2(cid:0) 4log2(3/2) 6log(3/2)log(4/3)(cid:1) +0.1 f (1) 0,
′ ′
− ≤ ≤
since 4log2(3/2) 6log(3/2)log(4/3)<0. Therefore, f(x) is non-increasing and f(x) f(1) 0.
− ≤ ≤
Lemma C.5. Consider complex-valued OU process
dz =Azdt+DdW ,
t
⊺
where z Cd, A Cd ×d is diagonalizable with orthogonal basis, i.e., A=USU with diagonal matrix S and
∈ ∈
orthogonal matrix U, D Cd ×m, and W
t
Cm is a standard real Brownian motion. The covariance and
∈ ∈
pseudo-covariance at time t are
(cid:90) t
Γ(t)=U
exp(τS)U⊺ DDHUexp(cid:0) τS(cid:1) dτU⊺
0
(cid:90) t
⊺ ⊺ ⊺
C(t)=U exp(τS)U DD Uexp(τS)dτU .
0
Proof. Recall that the covariance and pseudo-covariance are closely related to 4 matrices, Cov[Re(z),Re(z)],
Cov[Im(z),Im(z)],Cov[Re(z),Im(z)]Cov[Im(z),Re(z)]. Therefore,westudythedynamicsofthesematrices.
We first notice that complex-valued OU process can be re-written as a real-valued SDE.
(cid:20) (cid:21) (cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21)
Re(z) Re(A) Im(A) Re(z) Re(D)
d
Im(z)
=
Im(A)
−
Re(A) Im(z)
dt+
Im(D)
dW t.
We assume z(0) is a deterministic variable, i.e., its covariance is 0, then according to Equation (6.20) of
Särkkä and Solin [2019],
(cid:34)(cid:20) (cid:21)(cid:20) (cid:21)⊺(cid:35) (cid:20) (cid:21)
Re(z(t)) E[Re(z(t))] Re(z(t)) E[Re(z(t))] Cov[Re(z),Re(z)] Cov[Re(z),Im(z)]
E − − =
Im(z(t)) E[Im(z(t))] Im(z(t)) E[Im(z(t))] Cov[Im(z),Re(z)] Cov[Im(z),Im(z)]
− −
(cid:90) t (cid:18) (cid:20) Re(A) Im(A)(cid:21)(cid:19)(cid:20) Re(D)(cid:21)(cid:18) (cid:18) (cid:20) Re(A) Im(A)(cid:21)(cid:19)(cid:20) Re(D)(cid:21)(cid:19)⊺
= exp τ − exp τ − dτ,
Im(A) Re(A) Im(D) Im(A) Re(A) Im(D)
0
(55)
where the exponential operator is matrix exponential.
(cid:18) (cid:20) (cid:21)(cid:19)
Re(A) Im(A)
Next, let us look at the matrix exp τ − , which has some special properties. Note
Im(A) Re(A)
that we have
Re(A)=URe(S)U⊺ , and Im(A)=UIm(S)U⊺ .
Therefore, we have
(cid:20) (cid:21) (cid:20) ⊺ ⊺(cid:21)
Re(A) Im(A) URe(S)U UIm(S)U
− = ⊺ − ⊺
Im(A) Re(A) UIm(S)U URe(S)U
(cid:20) (cid:21)(cid:20) (cid:21)(cid:20) ⊺ (cid:21)
U Re(S) Im(S) U
= − ⊺ .
U Im(S) Re(S) U
51(cid:20) (cid:21)
U
Note that the matrix is orthogonal, so by the definition of matrix exponential, we have
U
(cid:18) (cid:20) Re(A) Im(A)(cid:21)(cid:19) (cid:88)∞ τp (cid:20) Re(A) Im(A)(cid:21)p
exp τ − = −
Im(A) Re(A) p! Im(A) Re(A)
p=0
(cid:20) U (cid:21) (cid:88)∞ τp (cid:20) Re(S) Im(S)(cid:21)p(cid:20) U⊺ (cid:21)
= − ⊺
U p! Im(S) Re(S) U
p=0
(cid:20) (cid:21) (cid:18) (cid:20) (cid:21)(cid:19)(cid:20) ⊺ (cid:21)
U Re(S) Im(S) U
= exp τ − ⊺ .
U Im(S) Re(S) U
By the property of matrix exponential, if for matrices X and Y, we have XY =YX, then exp(X+Y)=
exp(X)exp(Y). We verify that this property holds for our case:
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
Re(S) Im(S) Re(S) Im(S)
− = + − ,
Im(S) Re(S) Re(S) Im(S)
and
(cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21)
Re(S) Im(S) Re(S)Im(S)
− = −
Re(S) Im(S) Re(S)Im(S)
(cid:20) (cid:21)(cid:20) (cid:21)
Im(S) Re(S)
= − .
Im(S) Re(S)
Therefore, we have
(cid:18) (cid:20) (cid:21)(cid:19) (cid:18) (cid:20) (cid:21)(cid:19) (cid:18) (cid:20) (cid:21)(cid:19)
Re(S) Im(S) Re(S) Im(S)
exp τ − =exp τ exp τ −
Im(S) Re(S) Re(S) Im(S)
(cid:20) (cid:21) (cid:18) (cid:20) (cid:21)(cid:19)
exp(τRe(S)) Im(S)
= exp τ − ,
exp(τRe(S)) Im(S)
Where the last equality holds because Re(S) is diagonal. Next, we will show that
(cid:18) (cid:20) (cid:21)(cid:19) (cid:20) (cid:21)
Im(S) cos(τIm(S)) sin(τIm(S))
exp τ − = − ,
Im(S) sin(τIm(S)) cos(τIm(S))
where the sin and cos operators are applied element-wise. To see this, let us write down the expansion of the
matrix exponential.
(cid:18) (cid:20) (cid:21)(cid:19)
Im(S)
exp τ −
Im(S)
(cid:20) (cid:21) (cid:20) (cid:21)
τIm(S) 1 τ2Im(S)2
=I+ − + −
τIm(S) 2 τ2Im(S)2
−
(cid:20) (cid:21) (cid:20) (cid:21)
1 τ3Im(S)3 1 τ4Im(S)4
+ + +
3! τ3Im(S)3 4! τ4Im(S)4 ···
−
(cid:20) (cid:21)
cos(τIm(S)) sin(τIm(S))
= − .
sin(τIm(S)) cos(τIm(S))
Now going back to Equation (55), we have
(cid:20) (cid:21)
Cov[Re(z),Re(z)] Cov[Re(z),Im(z)]
Cov[Im(z),Re(z)] Cov[Im(z),Im(z)]
52(cid:20)
U
(cid:21)(cid:90) t(cid:20)
exp(τRe(S))cos(τIm(S))
exp(τRe(S))sin(τIm(S))(cid:21)(cid:20) U⊺ Re(D)(cid:21)
= − ⊺
U exp(τRe(S))sin(τIm(S)) exp(τRe(S))cos(τIm(S)) U Im(D)
0
(cid:18)(cid:20) (cid:21)(cid:20) ⊺ (cid:21)(cid:19)⊺ (cid:20) ⊺ (cid:21)
exp(τRe(S))cos(τIm(S)) exp(τRe(S))sin(τIm(S)) U Re(D) U
− ⊺ dτ ⊺
exp(τRe(S))sin(τIm(S)) exp(τRe(S))cos(τIm(S)) U Im(D) U
(cid:20)
U
(cid:21)(cid:90) t(cid:20)
P(τ)
Q(τ)(cid:21) (cid:20) U⊺ (cid:21)
= dτ ⊺ ,
U M(τ) N(τ) U
0
where
⊺ ⊺
P(τ)=exp(τRe(S))cos(τIm(S))U Re(D)Re(D) Uexp(τRe(S))cos(τIm(S))
⊺ ⊺
exp(τRe(S))cos(τIm(S))U Re(D)Im(D) Uexp(τRe(S))sin(τIm(S))
−
⊺ ⊺
exp(τRe(S))sin(τIm(S))U Im(D)Re(D) Uexp(τRe(S))cos(τIm(S))
−
⊺ ⊺
+exp(τRe(S))sin(τIm(S))U Im(D)Im(D) Uexp(τRe(S))sin(τIm(S)),
⊺ ⊺
Q(τ)=exp(τRe(S))cos(τIm(S))U Re(D)Re(D) Uexp(τRe(S))sin(τIm(S))
⊺ ⊺
+exp(τRe(S))cos(τIm(S))U Re(D)Im(D) Uexp(τRe(S))cos(τIm(S))
⊺ ⊺
exp(τRe(S))sin(τIm(S))U Im(D)Re(D) Uexp(τRe(S))sin(τIm(S))
−
⊺ ⊺
exp(τRe(S))sin(τIm(S))U Im(D)Im(D) Uexp(τRe(S))cos(τIm(S)),
−
⊺ ⊺
M(τ)=exp(τRe(S))sin(τIm(S))U Re(D)Re(D) Uexp(τRe(S))cos(τIm(S))
⊺ ⊺
exp(τRe(S))sin(τIm(S))U Re(D)Im(D) Uexp(τRe(S))sin(τIm(S))
−
⊺ ⊺
+exp(τRe(S))cos(τIm(S))U Im(D)Re(D) Uexp(τRe(S))cos(τIm(S))
⊺ ⊺
exp(τRe(S))cos(τIm(S))U Im(D)Im(D) Uexp(τRe(S))sin(τIm(S)),
−
⊺ ⊺
N(τ)=exp(τRe(S))sin(τIm(S))U Re(D)Re(D) Uexp(τRe(S))sin(τIm(S))
⊺ ⊺
+exp(τRe(S))sin(τIm(S))U Re(D)Im(D) Uexp(τRe(S))cos(τIm(S))
⊺ ⊺
+exp(τRe(S))cos(τIm(S))U Im(D)Re(D) Uexp(τRe(S))sin(τIm(S))
⊺ ⊺
+exp(τRe(S))cos(τIm(S))U Im(D)Im(D) Uexp(τRe(S))cos(τIm(S)).
Now we have
(cid:20) ⊺ ⊺ (cid:21) (cid:34)(cid:82)t (cid:82)t (cid:35)
U Cov[Re(z),Re(z)]U U Cov[Re(z),Im(z)]U P(τ)dτ Q(τ)dτ
U⊺ Cov[Im(z),Re(z)]U U⊺ Cov[Im(z),Im(z)]U = (cid:82)0 t M(τ)dτ (cid:82)0 t N(τ)dτ .
0 0
Denote the covariance of z(t) as Γ(t). According to the definition of covariance for complex random variables,
we have
(cid:90) t (cid:90) t (cid:18)(cid:90) t (cid:90) t (cid:19)
⊺
U Γ(t)U = P(τ)dτ + N(τ)dτ +i M(τ)dτ Q(τ)dτ .
−
0 0 0 0
Letusdenotethei-theigenvalueofAasa ,i.e.,thei-thdiagonalelementofS. Sinceexp(τRe(S))cos(τIm(S))
i
and exp(τRe(S))sin(τIm(S)) are both diagonal, we have the (i,j)-th element of U⊺ Γ(t)U satisfies
(cid:2) ⊺ (cid:3)
U Γ(t)U
i,j
(cid:90) t
(cid:2) ⊺ ⊺ (cid:3)
= exp(τRe(a )+Re(a ))cos(τIm(a ))cos(τIm(a )) U Re(D)Re(D) U dτ
i j i j i,j
0
(cid:90) t
(cid:2) ⊺ ⊺ (cid:3)
+ exp(τRe(a )+Re(a ))sin(τIm(a ))sin(τIm(a )) U Re(D)Re(D) U dτ
i j i j i,j
0
(cid:90) t
(cid:2) ⊺ ⊺ (cid:3)
+i exp(τRe(a )+Re(a ))sin(τIm(a ))cos(τIm(a )) U Re(D)Re(D) U dτ
i j i j i,j
0
53(cid:90) t
(cid:2) ⊺ ⊺ (cid:3)
i exp(τRe(a )+Re(a ))cos(τIm(a ))sin(τIm(a )) U Re(D)Re(D) U dτ
− i j i j i,j
0
(cid:90) t
(cid:2) ⊺ ⊺ (cid:3)
exp(τRe(a )+Re(a ))cos(τIm(a ))sin(τIm(a )) U Re(D)Im(D) U dτ
− i j i j i,j
0
(cid:90) t
(cid:2) ⊺ ⊺ (cid:3)
+ exp(τRe(a )+Re(a ))sin(τIm(a ))cos(τIm(a )) U Re(D)Im(D) U dτ
i j i j i,j
0
(cid:90) t
(cid:2) ⊺ ⊺ (cid:3)
i exp(τRe(a )+Re(a ))sin(τIm(a ))sin(τIm(a )) U Re(D)Im(D) U dτ
− i j i j i,j
0
(cid:90) t
(cid:2) ⊺ ⊺ (cid:3)
i exp(τRe(a )+Re(a ))cos(τIm(a ))cos(τIm(a )) U Re(D)Im(D) U dτ
− i j i j i,j
0
(cid:90) t
(cid:2) ⊺ ⊺ (cid:3)
exp(τRe(a )+Re(a ))sin(τIm(a ))cos(τIm(a )) U Im(D)Re(D) U dτ
− i j i j i,j
0
(cid:90) t
(cid:2) ⊺ ⊺ (cid:3)
+ exp(τRe(a )+Re(a ))cos(τIm(a ))sin(τIm(a )) U Im(D)Re(D) U dτ
i j i j i,j
0
(cid:90) t
(cid:2) ⊺ ⊺ (cid:3)
+i exp(τRe(a )+Re(a ))cos(τIm(a ))cos(τIm(a )) U Im(D)Re(D) U dτ
i j i j i,j
0
(cid:90) t
(cid:2) ⊺ ⊺ (cid:3)
+i exp(τRe(a )+Re(a ))sin(τIm(a ))sin(τIm(a )) U Im(D)Re(D) U dτ
i j i j i,j
0
(cid:90) t
(cid:2) ⊺ ⊺ (cid:3)
+ exp(τRe(a )+Re(a ))sin(τIm(a ))sin(τIm(a )) U Im(D)Im(D) U dτ
i j i j i,j
0
(cid:90) t
(cid:2) ⊺ ⊺ (cid:3)
+ exp(τRe(a )+Re(a ))cos(τIm(a ))cos(τIm(a )) U Im(D)Im(D) U dτ
i j i j i,j
0
(cid:90) t
(cid:2) ⊺ ⊺ (cid:3)
i exp(τRe(a )+Re(a ))cos(τIm(a ))sin(τIm(a )) U Im(D)Im(D) U dτ
− i j i j i,j
0
(cid:90) t
(cid:2) ⊺ ⊺ (cid:3)
+i exp(τRe(a )+Re(a ))sin(τIm(a ))cos(τIm(a )) U Im(D)Im(D) U dτ.
i j i j i,j
0
Let us look at the terms associated with [U⊺ Re(D)Re(D)⊺ U] . According to properties of trigonometric
i,j
functions, we have
cos(x)cos(y)+sin(x)sin(y)=cos(x y)
−
sin(x)cos(y) cos(x)sin(y)=sin(x y),
− −
which implies the factor associated with [U⊺ Re(D)Re(D)⊺ U] is
i,j
(cid:90) t
exp(τRe(a )+Re(a ))cos(τIm(a ) τIm(a ))dτ
i j i j
−
0 (56)
(cid:90) t
+ iexp(τRe(a )+Re(a ))sin(τIm(a ) τIm(a ))dτ.
i j i j
−
0
Notethatforcomplexvariablez,Re(exp(z))=exp(Re(z))cos(Im(z))andIm(exp(z))=exp(Re(z))sin(Im(z)).
To see why this is the case, by Euler’s formula,
exp(z)=exp(Re(z)+iIm(z))=exp(Re(z))exp(iIm(z))=exp(Re(z))(cos(Im(z))+isin(Im(z))).
Therefore, going back to Equation (56), this factor is equal to
(cid:90) t
Re(exp(τ(Re(a )+Re(a )+i(Im(a ) Im(a )))))dτ
i j i j
−
0
54(cid:90) t
+ iIm(exp(τ(Re(a )+Re(a )+i(Im(a ) Im(a )))))dτ
i j i j
−
0
(cid:90) t
= exp(τ(Re(a )+Re(a )+i(Im(a ) Im(a ))))dτ
i j i j
−
0
(cid:90) t
= exp(τ(a +a ))dτ,
i j
0
where x is the complex conjugate of x.
Similarly, we can calculate the factors associated with [U⊺ Im(D)Re(D)⊺ U] , [U⊺ Re(D)Im(D)⊺ U] ,
i,j i,j
and [U⊺ Im(D)Im(D)⊺ U] .
i,j
(cid:2) ⊺ (cid:3)
U Γ(t)U
i,j
(cid:90) t (cid:90) t
(cid:2) ⊺ ⊺ (cid:3) (cid:2) ⊺ ⊺ (cid:3)
= U Re(D)Re(D) U exp(τ(a +a ))dτ i U Re(D)Im(D) U exp(τ(a +a ))dτ
i,j i j − i,j i j
0 0
(cid:90) t (cid:90) t
(cid:2) ⊺ ⊺ (cid:3) (cid:2) ⊺ ⊺ (cid:3)
+i U Im(D)Re(D) U exp(τ(a +a ))dτ + U Im(D)Im(D) U exp(τ(a +a ))dτ
i,j i j i,j i j
0 0
(cid:90) t
(cid:2) ⊺ ⊺ ⊺ ⊺ ⊺ ⊺ ⊺ ⊺ (cid:3)
= U Re(D)Re(D) U +U Im(D)Im(D) U +iU Im(D)Re(D) U iU Re(D)Im(D) U exp(τ(a +a ))dτ
− i,j i j
0
(cid:90) t
=(cid:2) U⊺ DDHU(cid:3)
exp(τ(a +a ))dτ.
i,j i j
0
Therefore, we can obtain that
(cid:90) t
Γ(t)=U
exp(τS)U⊺ DDHUexp(cid:0) τS(cid:1) dτU⊺
.
0
Similarly, for pseudo-covariance C, we have
(cid:90) t
⊺ ⊺ ⊺
C(t)=U exp(τS)U DD Uexp(τS)dτU .
0
C.3 Proofs for Section 6
Proof for Proposition 6.1. Solution for SGD Let us consider more general covariance matrix Σ instead of
σ2I. The iterates of SGD can be written as
x =x η(Ax +ξ )
k k 1 k 1 k 1
− − − −
=(I ηA)x ηξ
k 1 k 1
− − − −
=(I ηA)2x η(I ηA)ξ ηξ
k 2 k 2 k 1
− − − − − − −
...
k 1
=(I ηA)kx η
(cid:88)−
(I ηA)mξ .
0 k 1 m
− − − − −
m=0
Clearly, it is a linear combination of independent Gaussian variables, therefore x is also has a Gaussian
k+1
distribution. Moreover,
E[x k]=(I ηA)kx
0
=U(I ηΛ)kU⊺ x
0
− −
55k 1 k 1
Cov[x ,x ]=η2 (cid:88)− (I ηA)mΣ(I ηA)m =η2 (cid:88)− U(I ηΛ)mU⊺ ΣU(I ηΛ)mU⊺ .
k k
− − − −
m=0 m=0
Plugging in Σ=σ2I gives us the desired result.
The three SDEs considered applied on quadratics with Assumption 6.2 are OU processes. According to
Särkkä and Solin [2019, Section 6.2], for a linear time-invariant SDE
dX =FX dt+LdW ,
t t t
the solution would be a Gaussian variable and its mean and covariance satisfy
E[X(x 0,t)]=exp(Ft)x
0
(cid:90) t
⊺ ⊺
Cov[X(x ,t),X(x ,t)]= exp(F(t τ))LL exp(F(t τ)) dτ.
0 0
− −
0
In our case, since we assume isotropic noise, i.e., the noise covariance is σ2I, the diffusion coefficients of
the three SDEs we consider is also isotropic, i.e., √ησI. Under this condition, F and L commute, and the
covariance has closed form solution.
Solution for SME-1 The SDE SME-1 applied on the problem we consider is
dX = AX dt+√ησIdW .
t t t
−
Then the mean is
E[X(x 0,t)]=exp( At)x 0,
−
and the covariance is
(cid:90) t
Cov[X(x ,t),X(x ,t)]= exp(A(τ t))ησ2exp(A(τ t))dτ
0 0
− −
0
(cid:90) t
⊺ ⊺
=ησ2 Uexp(Λ(τ t))U Uexp(Λ(τ t))U dτ
− −
0
(cid:90) t
⊺
=ησ2U exp(Λ(τ t))exp(Λ(τ t))dτU
− −
0
(cid:90) t
⊺
=ησ2U exp(2Λ(τ t))dτU
−
0
I exp( 2Λt) ⊺
=ησ2U − − U .
2Λ
Solution for SME-2 The SDE SME-2 applied on the problem we consider is
η
dX = (A+ A2)X dt+√ησIdW .
t t t
− 2
Then the mean is
(cid:16) (cid:16) η (cid:17) (cid:17)
E[X(x 0,t)]=exp A+ A2 t x 0,
− 2
and the covariance is
(cid:90) t (cid:16)(cid:16) η (cid:17) (cid:17) (cid:16)(cid:16) η (cid:17) (cid:17)
Cov[X(x ,t),X(x ,t)]= exp A+ A2 (τ t) ησ2exp A+ A2 (τ t) dτ
0 0
2 − 2 −
0
(cid:90) t (cid:16)(cid:16) η (cid:17) (cid:17) ⊺ (cid:16)(cid:16) η (cid:17) (cid:17) ⊺
=ησ2 Uexp Λ+ Λ2 (τ t) U Uexp Λ+ Λ2 (τ t) U dτ
2 − 2 −
0
56(cid:90) t (cid:16)(cid:16) η (cid:17) (cid:17) (cid:16)(cid:16) η (cid:17) (cid:17) ⊺
=ησ2U exp Λ+ Λ2 (τ t) exp Λ+ Λ2 (τ t) dτU
2 − 2 −
0
(cid:90) t (cid:16) (cid:16) η (cid:17) (cid:17) ⊺
=ησ2U exp 2 Λ+ Λ2 (τ t) dτU
2 −
0
I
exp(cid:0) 2(cid:0)
Λ+
ηΛ2(cid:1) t(cid:1)
⊺
=ησ2U − 2(cid:0)−
Λ+
ηΛ2(cid:1)2 U .
2
Solution for SPF The SDE SPFapplied on the problem we consider is
log(1 ηΛ) ⊺
dX =U − U X dt+√ησIdW .
t t t
η
Then the mean is
(cid:18) (cid:19)
log(1 ηΛ) ⊺
E[X(x 0,t)]=exp U − U t x
0
η
=Uexp(cid:16)
log(1
ηΛ)t/η(cid:17) U⊺
x
0
−
=U(1
ηΛ)t/ηU⊺
x ,
0
−
and the covariance is
(cid:90) t (cid:18) log(1 ηΛ) ⊺ (cid:19) (cid:18) log(1 ηΛ) ⊺ (cid:19)
Cov[X(x ,t),X(x ,t)]= exp U − U (τ t) ησ2exp U − U (τ t) dτ
0 0
η − η −
0
(cid:90) t (cid:18) log(1 ηΛ) (cid:19) ⊺
=ησ2U exp 2 − (τ t) dτU
η −
0
(cid:18) (cid:18) (cid:18) (cid:19)(cid:19)(cid:19)
η 2log(1 ηΛ) ⊺
=ησ2U 1 exp − t U
2log(1 ηΛ) − η
− −
=η2σ2U(cid:18) 1 (cid:16)
1 (1
ηΛ)2t/η(cid:17)(cid:19) U⊺
.
2log(1 ηΛ) − −
− −
Proof for Desideratum 1. If the distribution of complex variable matches the real variable, then by the
definition of matching, we have
E[Re(z)]=E[z (cid:101)] E[Im(z)]=0
Cov[Re(z),Re(z)]=Cov[z,z] Cov[Im(z),Im(z)]=0.
(cid:101) (cid:101)
Then according to Equation (53), we get
Γ(z)=Cov[Re(z),Re(z)]+Cov[Im(z),Im(z)]+i(Cov[Im(z),Re(z)] Cov[Re(z),Im(z)])
−
=Cov[Re(z),Re(z)]=Cov[z,z]
(cid:101) (cid:101)
Γ(z)=Cov[Re(z),Re(z)] Cov[Im(z),Im(z)]+i(Cov[Im(z),Re(z)]+Cov[Re(z),Im(z)])
−
=Cov[Re(z),Re(z)]=Cov[z,z],
(cid:101) (cid:101)
asIm(z)=0deterministically. Next,weconsidertheoppositedirection. Themeansmatchtrivially. According
to Equation (54), we have
1
Cov[Re(z),Re(z)]= (Γ(z)+C(z))=Cov[z,z]
(cid:101) (cid:101)
2
1
Cov[Im(z),Im(z)]= (Γ(z) C(z))=0,
2 −
which completes the proof.
57C.4 Proof for Proposition 6.2
Proof. Consider function
(cid:20) (cid:21)
1 ⊺ 1 0
f(x)= x x,
2 0 1
−
and
(cid:20) (cid:21)
Σ Σ
Σ= 11 12 with Σ ,Σ ,Σ >0 and Σ Σ Σ2 =0.
Σ 12 Σ 22 11 12 22 11 22 − 12
Note that the last condition implies that Σ is positive semi-definite and not positive definite. It is easy to
(cid:20) (cid:21)
1 2
come up with such a Σ, e.g., .
2 4
According to Lemma 6.1, a necessary condition for matching the discrete-time iterates is that the RHS of
Equation (28) is positive semi-definite, otherwise it cannot be decomposed to (U⊺ D)(U⊺ D)H. Therefore, as
long as we can show that the determinant of Σ is negative, i.e., it contains a negative eigenvalue, then we can
claim that there is no such a decomposition, i.e., no linear SDE can match the discrete-time iterates.
The RHS matrix of Equation (28) in our case is
(cid:34) 2Σ11Re(log(1 −η)) Σ12(log(1 −η)+log(1+η))(cid:35)
η 2 η
Σ12(log(1+η− )+log(1 −η)) 2Σ22l− og(1+η) ,
η η+2
−
where we used x+x=2Re(x) to simplify the result. Then, our goal is to show
(cid:16) (cid:17)
Σ2 (log(1 η)+log(1+η)) log(1 η)+log(1+η)
4Σ 11Σ 22Re(log(1 η))log(1+η) 12 − −
− <0.
(η 2)(η+2) − η2
−
It is then sufficient to show the following:
(cid:16) (cid:17)
(log(1 η)+log(1+η)) log(1 η)+log(1+η) (η 2)(η+2)
Σ Σ
− − − > 11 22 =1. (57)
4η2Re(log(1 η))log(1+η) Σ2
− 12
To see why this is the case, notice that for all for all η >0, it holds that
Re(log(1 η))
− >0.
η 2
−
Next, we will discuss in three different cases, i.e., 0 < η < 1, 1 < η < 2 and η > 2, to prove the above
inequality. For the boundary case, one can check that the inequality holds when η 2.
→
When 0<η <1 In this case, we have Re(log(1 η))<0, therefore, Equation (57) is equivalent to
−
(cid:16) (cid:17)
4η2Re(log(1 η))log(1+η)>(log(1 η)+log(1+η)) log(1 η)+log(1+η) (η 2)(η+2)
− − − −
4(log(1 η)+log(1+η))(log(1 η)+log(1+η))>η2(log(1 η) log(1+η))(log(1 η) log(1+η)).
⇐⇒ − − − − − − (58)
When 0<η <1, we have log(1 η)=log(1 η), which implies the following equivalence of Equation (58):
− −
4(log(1 η)+log(1+η))2 >η2(log(1 η) log(1+η))2
− − −
4(cid:0) log(cid:0) 1 η2(cid:1)(cid:1)2 >η2(log(1 η) log(1+η))2
⇐⇒ − − −
582log(cid:0)
1
η2(cid:1)
>η(log(1+η) log(1 η))
⇐⇒ − − − −
ηlog(1+η) ηlog(1
η)+2log(cid:0)
1
η2(cid:1)
<0.
⇐⇒ − − −
(cid:124) (cid:123)(cid:122) (cid:125)
g(η)
Now we analyze the function g(η).
η η 4η
g (η)=log(1+η)+ log(1 η)+
′
1+η − − 1 η − 1 η2
(cid:18) (cid:19) − −
1+η 2η
=log
1 η − 1 η2
− −
(cid:18) (cid:19)
2η 2η
=log 1+
1 η − 1 η2
− −
(cid:16) (cid:17)
2η 6+ 2η
1 η 1 η 2η
− −
≤ 6+ 8η − 1 η2
1 η −
−
12η 8η2 2η
= −
(1 η)(6+2η) − 1 η2
− −
8η3
= <0,
−(1 η)(1+η)(6+2η)
−
where the first inequality is according to Lemma C.1. Now we know that g(η) is strictly decreasing over
0<η <1, therefore sup g(η)<g(0)=0.
0<η<1
When 1<η <2 In this case, we also obtain Equation (58). In addition, since log(1 η)=πi+log(η 1),
we have − −
log(1 η)log(1 η)=log2(1 η)+π2.
− − −
Therefore, Equation (58) is equivalent to
4(cid:0) log2(η 1)+π2+2log(η 1)log(1+η)+log2(1+η)(cid:1)
− −
>η2(cid:0) log2(η 1)+π2 2log(η 1)log(1+η)+log2(1+η)(cid:1)
− − −
4(log(η 1)+log(1+η))2 η2(log(η 1) log(1+η))2+π2(4 η2)>0 (59)
⇐⇒ − − − − −
(4 η2)(cid:0) log2(η 1)+log2(η+1)+π2(cid:1) +(8+2η2)log(η 1)log(1+η)>0.
⇐⇒ − − −
To prove the above inequality, it is sufficient to prove a lower bound of LHS is greater than 0.
(cid:0) 4 η2(cid:1)(cid:0) log2(η 1)+log2(η+1)+π2(cid:1) +(8+2η2)log(η 1)log(1+η)
− − −
>(cid:0) 4 η2(cid:1)(cid:0) log2(η 1)+log2(η+1)+π2(cid:1) +(8+2η2)log(η 1)η
− − −
(cid:0) 4 η2(cid:1)(cid:0) log2(η 1)+log2(η+1)+π2(cid:1) +(22η 12)log(η 1)
≥ − − − −
(6 3η)log2(η 1)+(cid:0) 4 η2(cid:1)(cid:0) log2(1+η)+π2(cid:1) +(22η 12)log(η 1)
≥ − − − − −
(6 3η)log2(η 1)+(cid:0) 4 η2(cid:1)(cid:0) (ηlog(3/2)+log(4/3))2+π2(cid:1) +(22η 12)log(η 1),
≥ − − − − −
(cid:124) (cid:123)(cid:122) (cid:125)
h(η)
where the first inequality is because log(1+η) < η for 1 < η < 2, and the second inequality holds as
2η3+2η 22η 12. This can be seen by noticing that 2η3+2η is convex on this interval and 22η 12 is the
linear int≤erpolat−ion of the endpoints. Similarly, 4 η2 is concave, and lower bounded by linear int−erpolation
6 3η, which gives us the third inequality. Th−e final inequality comes from the linear interpolation of
en−dpoints of concave function log(1+η).
59Next, we study the lower bound of h(η). By taking its derivative, we obtain
1
h(η)= (3(1 η)log2(η 1)+(16η 10)log(η 1) 4η4log2(3/2)
′
η 1 − − − − −
−
+η3(cid:0) 4log2(3/2) 6log(3/2)log(4/3)(cid:1) +η2(cid:0) 6log(3/2)log(4/3) 2log2(4/3) 2π2+8log2(3/2)(cid:1)
− − −
+η(2log2(4/3)+2π2 8log2(3/2)+8log(4/3)log(3/2)+22) 8log(4/3)log(3/2) 12)
− − −
1 ((16η 10)log(η 1) 4η4log2(3/2)+η3(cid:0) 4log2(3/2) 6log(3/2)log(4/3)(cid:1)
≤ η 1 − − − −
−
+η2(cid:0) 6log(3/2)log(4/3) 2log2(4/3) 2π2+8log2(3/2)(cid:1)
− −
+η(2log2(4/3)+2π2 8log2(3/2)+8log(4/3)log(3/2)+22) 8log(4/3)log(3/2) 12)
− − −
1 (16η 32 4η4log2(3/2)+η3(cid:0) 4log2(3/2) 6log(3/2)log(4/3)(cid:1)
≤ η 1 − − −
−
+η2(cid:0) 6log(3/2)log(4/3) 2log2(4/3) 2π2+8log2(3/2)(cid:1)
− −
+η(2log2(4/3)+2π2 8log2(3/2)+8log(4/3)log(3/2)+22) 8log(4/3)log(3/2) 12)
− − −
1 (16η 32+2.5 3η+η3(cid:0) 4log2(3/2) 6log(3/2)log(4/3)(cid:1)
≤ η 1 − − −
−
+η2(cid:0) 6log(3/2)log(4/3) 2log2(4/3) 2π2+8log2(3/2)(cid:1)
− −
+η(2log2(4/3)+2π2 8log2(3/2)+8log(4/3)log(3/2)+22) 8log(4/3)log(3/2) 12)
− − −
1
(16η 32+2.5 3η+0.09 0.1η
≤ η 1 − − −
−
+η2(cid:0) 6log(3/2)log(4/3) 2log2(4/3) 2π2+8log2(3/2)(cid:1)
− −
+η(2log2(4/3)+2π2 8log2(3/2)+8log(4/3)log(3/2)+22) 8log(4/3)log(3/2) 12).
− − −
wherethesecond,thirdandforthinequalitiesareaccordingtoLemmasC.2toC.4respectively. Theremaining
numerator is a quadratic function of η, and one can easily verify that it is less than 0. Therefore, we conclude
that h(η)<0. Hence h(x) h(2)=0.
′
≥
When η >2 In this case, our target becomes to prove Equation (59), however with the opposite sign of the
inequality.
4(log(η 1)+log(1+η))2 η2(log(η 1) log(1+η))2+π2(cid:0) 4 η2(cid:1)
− − − − −
4(log(η 1)+log(1+η))2 4(log(η 1) log(1+η))2+π2(cid:0) 4 η2(cid:1)
≤ − − − − −
=16log(η
1)log(1+η)+π2(cid:0)
4
η2(cid:1)
− −
<16(η
2)log(1+η)+π2(cid:0)
4
η2(cid:1)
− −
16(η
2)η(6+η) +π2(cid:0)
4
η2(cid:1)
≤ − 6+4η −
(8 2π2)η3+(32 3π2)η2+(8π2 96)η+12π2
= − − −
3+2η
(cid:0)(cid:0) (cid:1) (cid:0) (cid:1) (cid:1)
(x 2) 2π2 8 x2+ 7π2 48 x+6π2
= − − − − ,
3+2η
where the second inequality holds as log(x) < x 1 for x > 1 and the third inequality is according to
Lemma C.1. In addition, (cid:0) 2π2 8(cid:1) x2+(cid:0) 7π2 48(cid:1)− x+6π2 0, therefore, we have
− − ≥
4(log(η 1)+log(1+η))2 η2(log(η 1) log(1+η))2+π2(cid:0) 4 η2(cid:1) <0,
− − − − −
which concludes the proof.
60Proof for Lemma 6.1. To match the expectation, the argument is similar to the proof in Rosca et al. [2022,
Section A.6]. Here, we proceed to match the covariance and pseudo-covariance. Let us first look at the
continuous-time process. We denote the covairance of X as Γ(X). According to Lemma C.5, we have
(cid:2) ⊺ (cid:3)
U Γ(t)U
i,j
(cid:16) (cid:17)
[U⊺ ΣU] i,j log(1 −ηλ i)+log(1 −ηλ i) (cid:90) t (cid:18) log(1 ηλ i)(cid:19) (cid:32) log(1 ηλ j)(cid:33)
= exp τ − exp τ − dτ
ηλ λ (λ +λ ) η η
i j − i j 0
(cid:16) (cid:17)
⊺
η[U ΣU] i,j log(1 −ηλ i)+log(1 −ηλ i) (cid:90) t (cid:18) τ (cid:16) (cid:17)(cid:19)
= exp log(1 ηλ )+log(1 ηλ ) dτ
i j
(1 ηλ )(1 ηλ ) 1 η − −
− i − j − 0
(cid:16) (cid:17)
⊺
η[U ΣU] i,j log(1 −ηλ i)+log(1 −ηλ i) η (cid:18) (cid:18) t (cid:16) (cid:17)(cid:19) (cid:19)
= exp log(1 ηλ )+log(1 ηλ ) 1
i j
(1 −ηλ i)(1 −ηλ j) −1 log(1 −ηλ i)+log(1 −ηλ i) η − − −
η2[U⊺ ΣU] (cid:18) (cid:18) t (cid:16) (cid:17)(cid:19) (cid:19)
i,j
= exp log(1 ηλ )+log(1 ηλ ) 1
i j
(1 ηλ )(1 ηλ ) 1 η − − −
i j
− − −
⊺
η2[U ΣU] (cid:16) (cid:17)
i,j t
= ((1 ηλ i)(1 ηλ j))η 1 ,
(1 ηλ )(1 ηλ ) 1 − − −
i j
− − −
where we used the fact that for complex-valued variable z, it holds that exp(z)=exp(z). Now we consider
time stamp t=kη for k =1,2,..., where we have
⊺
(cid:2) U⊺ Γ(kη)U(cid:3) = η2[U ΣU] i,j (cid:16) ((1 ηλ )(1 ηλ ))k 1(cid:17)
i,j (1 ηλ )(1 ηλ ) 1 − i − j −
i j
− − −
k 1
=η2(cid:2) U⊺ ΣU(cid:3) (cid:88)− ((1 ηλ )(1 ηλ ))m.
i,j − i − j
m=0
Then we have
k 1
U⊺ Γ(kη)U =η2 (cid:88)− (I ηΛ)mU⊺ ΣU(I ηΛ)m,
− −
m=0
which implies
k 1
Γ(kη)=η2
(cid:88)−
U(I
ηΛ)mU⊺
ΣU(I
ηΛ)mU⊺
− −
m=0
k 1
=η2
(cid:88)−
(I ηA)mΣ(I ηA)m,
− −
m=0
which matches the covariance of discrete-time variable. The proof for matching pseudo-covariance is almost
the same.
61