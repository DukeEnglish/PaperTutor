On the Origin of Llamas: Model Tree Heritage
Recovery
EliahuHorwitz AsafShul YedidHoshen
SchoolofComputerScienceandEngineering
TheHebrewUniversityofJerusalem,Israel
https://vision.huji.ac.il/mother/
{eliahu.horwitz, asaf.shul, yedid.hoshen}@mail.huji.ac.il
Abstract
Therapidgrowthofneuralnetworkmodelssharedontheinternethasmademodel
weightsanimportantdatamodality. However,thisinformationisunderutilizedas
theweightsareuninterpretable,andpubliclyavailablemodelsaredisorganized.
InspiredbyDarwin’streeoflife,wedefinetheModelTreewhichdescribesthe
originofmodelsi.e.,theparentmodelthatwasusedtofine-tunethetargetmodel.
Similarly to the natural world, the tree structure is unknown. In this paper, we
introduce the task of Model Tree Heritage Recovery (MoTHer Recovery) for
discoveringModelTreesintheever-growinguniverseofneuralnetworks. Our
hypothesis is that model weights encode this information, the challenge is to
decode the underlying tree structure given the weights. Beyond the immediate
applicationofmodelauthorshipattribution,MoTHerrecoveryholdsexcitinglong-
termapplicationsakintoindexingtheinternetbysearchengines. Practically,for
eachpairofmodels,thistaskrequires: i)determiningiftheyarerelated,andii)
establishingthedirectionoftherelationship. Wefindthatcertaindistributional
propertiesoftheweightsevolvemonotonicallyduringtraining,whichenablesusto
classifytherelationshipbetweentwogivenmodels. MoTHerrecoveryreconstructs
entiremodelhierarchies,representedbyadirectedtree,whereaparentmodelgives
risetomultiplechildmodelsthroughadditionaltraining.Ourapproachsuccessfully
reconstructscomplexModelTrees,aswellasthestructureof“in-the-wild”model
familiessuchasLlama2andStableDiffusion.
1 Introduction
Thenumberanddiversityofneuralmodelssharedonlinehave
been growing at an unprecedented rate, establishing model
weights as an important data modality (12; 6; 26; 19). On Llama 2
the popular model repository Hugging Face alone there are
over600,000models,withthousandsmoreaddeddaily. Many
Code Llama Code
ofthesemodelsarerelatedthroughacommonancestor(i.e., Llama Chat Python Llama
foundationmodel)fromwhichtheywerefine-tuned. Wehy-
pothesizethatthemodelweightsmayencodethisrelationship. Llama 2
Code Llama
Motivated by Darwin’s tree of life (7), which describes the Model Tree Instruct
relationships between organisms, we analogously define the
ModelTreetodescribethehereditaryrelationsbetweenmodels.
Reminiscentofthenaturalworld,thestructureoftheModel Figure1: Llama2ModelTree: We
Treeisunknown. WethereforeproposethetaskofModelTree performMoTHerRecoveryin-the-
Heritage Recovery (MoTHer Recovery) for mapping Model wild, successfully recovering the
Treesintherapidlyevolvinguniverseofneuralnetworks. Llama2ModelTreewithperfectac-
curacy
Preprint.Underreview.
4202
yaM
82
]GL.sc[
1v23481.5042:viXraRecovering the Model Tree is important for several reasons. First, it establishes the lineage of a
model, determining whether one model originated from another (i.e., via fine-tuning). This can
helpresolvelegaldisputesovermodelauthorshiporwrongfuluseofproprietarytrainingdata. For
instance,afoundationmodelmaybetrainedonproprietarydataandpubliclyreleased. Byusingthe
graphcombinedwithmembershipinferenceattacks(32),onecouldidentifyfine-tunedversionsof
themodelandtakeappropriateaction. Inthelongterm,mappingthemodeluniverserepresentsan
excitingfieldofresearch,akintohowsearchenginesindextheinternet(2).
WebeginourexplorationoftheModelTreebystudyingtherelationshipbetweentheweightsof
relatedmodels. Ourmainfindingisthatoutliervaluesintheweightsofrelatedmodelsoftenindicate
theirrelationship. Wefindthatthedistancebetweentheseoutlierscorrelateswiththeirnodedistance
ontheirfamilytree. Wethenproceedtoexaminehowtheweightsofamodelevolveoverthecourse
oftraining. Weobservethatthenumberofweightoutlierschangesmonotonicallyoverthecourseof
training. Specifically,wedistinguishbetweenageneralizationstage(oftencalledpre-training)anda
specializationstage(oftenreferredtoasfine-tuning). Wefindthatduringgeneralization,thenumber
ofweightoutliersgrows,whileduringspecialization,itdiminishes.
With these observations, we attempt to recover a Model Tree for a set of models. This requires
determiningwhethereachpairofmodelsisdirectlyconnectedandestablishingthedirectionofthe
relationship. Weusetheoutlier-baseddistancetocreateapairwisedistancematrixandtheoutlier
monotonicitytocreateabinaryedgedirectionmatrix. Finally,weuseaminimumdirectedspanning
treealgorithmonthecombineddistancematrixtorecoverourModelTree. WeextendtheModel
TreetoaModelGraph,allowingustorecoverecosystemswithmultipletreesbyfirstclusteringthe
nodesbasedontheirpairwisedistances.
Wedemonstratetheefficacyofourmethodonacurateddatasetaswellasreal-worldecosystems
suchastheLlama2family(33;30)andtheStableDiffusionfamily(29). Weextensivelyablate
ourmethod,justifyingourtechnicalchoices. Finally,wediscussawayforwardtowardsrecovering
ModelGraphsatwebscalesuchastheHuggingFaceModelGraph.
Tosummarize,ourmaincontributionsare:
1. ProposingtheModelTreeandModelGraphdatastructuresfordescribingthehereditary
relationsbetweenmodels.
2. Uncoveringtheconnectionbetweenmodelweightoutliervaluesandtheirdistanceonthe
ModelGraph.
3. IntroducingthetaskofModelTreeHeritageRecoveryanddemonstratingtheeffectiveness
ofMoTHerrecoveryinreal-worldscenarios.
2 RelatedWorks
Modelweightprocessing. Althoughmodelweightsfullycharacterizethecapabilitiesofthemodel,
mostresearchisfocusedonmodelactivations(4;31;41;9). Earlyworks(14;15;1)visualizedthe
trajectoryofweightsusingprincipledcomponentanalysis(PCA).Eilertsenetal.(11)performed
alargeempiricalstudyofmodelweights,representingeachmodelasapointintheneuralweight
spaceandtrainingclassifierstopredictdifferenttraininghyper-parameters. Theseminalworkof
(24)proposedusingaHyperNettogenerateparametersofamodel,recentworks(12;39;36;28)
useddiffusionmodelstogeneratetheweightsofsmallnetworks. Morerecently, alineofworks
predictmodelaspectsperformanceorclassifymodelsbasedontheirweightsbyprojectingtheweight
intoanembeddingspace(21;6)orbyincorporatingpermutationinvariancepriorswhiletraininga
neuralnetworkontheweights(40;23;25;27;26;34). Recently,Horwitzetal.(19)demonstrateda
vulnerabilitywhereintheweightsofapre-trainedmodelcanberecoveredusingasmallnumberof
LoRAfine-tunedmodels. Overall,modelweightprocessingremainsanunderexploredareathatis
poisedtogrowinthecomingyears.
3 TheModelGraph
Definition. We introduce the Model Tree, a data structure for describing the origin of models
stemming from a base model (e.g., a foundation model). Consider a set of models V, where the
2basemodelv ∈V servesastherootnode. Everymodelv ∈V \{v },isfine-tunedfromanother
b b
model. Werefertothemodelfromwhichv wasfine-tunedasitsparent model,anddenoteitby
Pa(v). Conversely,werefertovasachildofPa(v). Aparentcanhavemultiplechildren(including
none),whileallmodelsexcepttheroothaveonlyoneparent. ThesetoftreeedgesisdenotedbyE,
whereeachdirectededgebetweenaparentanditschildisrepresentedase=(Pa(v),v). Overall,
theModelTreeT isdefinedbyitsnodesanddirectededges,T =(V,E). Wedenotebyd(u,v)the
numberofedgesontheshortestpathinT betweenthenodesuandv. ThetreeT isthesameas
+ +
ourtreeT exceptthatthedirectededgesarereplacedbyundirectedones.
AcollectionofModelTreesT ,...,T formsaforest,whichwecallaModelGraph. ThisModel
1 n
GraphisdefinedasG =(V =V ∪...∪V ,E =E ∪...∪E ). InaModelGraph,d(u,v)isonly
1 n 1 n
definedifu,v ∈T ,whenu∈T andv ∈T ,d(u,v)isundefined. Asthearchitectureofamodelis
i i j
givenbyitsweights,andsincedifferentarchitecturesnecessarilybelongtodifferenttrees,wecan
assumewithoutlossofgeneralitythatallv ∈V areofthesamearchitecture.
Taskdefinition. Duetothelargenumberanddiversityofmodels,thestructureoftheModelGraph
isunknownandisnon-trivialtoestimate. WethereforeintroducethetaskofModelTreeHeritage
Recovery(MoTHerRecovery)formappingthestructureoftheModelGraph.
Formally,givenasetofmodelsV,thegoalistorecoverthestructureoftheModelGraphG =(V,E)
basedsolelyontheweightsofthemodelsv ∈V. SinceaModelGraphisaforestofModelTrees,the
taskinvolvestwomainsteps: (i)ClusterthenodesintodifferentcomponentsT ,T ,...,whereeach
1 2
componentisaModelTreewithanunknownstructure. (ii)RecoverthestructureofeachModelTree
T . Essentially,aseachgraphisdefinedbyitsverticesandedges,thetaskistorecoverthedirected
i
edgesE usingtheweightsofv ∈V.
4 ModelGraphPriors
Despitetherecentgrowthinpublicmodels,andalthough 120
Full FT ( = 0.990)
modelweightsfullycharacterizethebehaviorofamodel, LoRA ( = 0.998)
0.95
modelweightsremainlargelyuninterpretable. Here,we 100
exploretwokeypropertiesofmodelweightsthatenable
ModelGraphrecovery. InSec.5,weutilizetheseproper- 0.90 80
tiesforourmethod,MoTHerRecovery.
60
0.85
4.1 Estimatingnodedistancefrommodelweights
40
0.80
Inthissection, weinvestigatetheuseofmodelweights
1 2 3 4
topredictthedistancebetweentwomodelsintheModel Model Tree Edge Distance
Tree. Thiswillhelpusdeterminewhethertwomodelsare
relatedviaanedge.
Figure 2: Weight Distance vs. Model
First,wedefinetheweightdistancebetweenmodels. Let Tree Edge Distance: Our weight dis-
uandv betwomodels,andu l andv l denotetheweight tancesℓ FT andℓ LoRA almostperfectly
matrixoflayerlofmodelsuandvrespectively, correlate with the number of edges be-
tweenmodelsinaModelTree. Thusis
agoodindicatorfordeterminingparent-
ℓ (u,v)=max(max(u −v )) (1)
FT l l
l∈L childrelation,i.e.,modelsthatwerefine-
tunedfromoneanother
whereLisasetofmodellayers.
Next, we study the weight distance ℓ (u,v) between pairs of models as a function of the edge
FT
distanced(u,v)betweentheirrespectivenodesontheModelTree. InFig.2,weplottherelationship
betweenthesetwodistances(ρ=0.99). Itisevidentthatnodeswithdirectparent-childconnections
(i.e.,modelsfine-tunedfromoneanother)havethelowestweightdistance. Weconcludethatalow
ℓ distancebetweentwomodelsishighlycorrelatedwithanedgebetweentheirnodesandvice
FT
versa. Notethatweuse“distance”inacolloquialsense,asmathematicallyitdoesnotsatisfythe
definitionsofadistance.
LoRAfine-tuning. LoRA(20)hasbecomethedominantmethodforparameter-efficientfine-tuning.
Whenfine-tuningamodelviaLoRA,theentiremodelisfrozen,andonlyasubsetofthelayerstunea
3
TF ARoLViT ViT
0.9 ResNet 10 2.92 ResNet 5.0
1.0
1.1 5 2.91 4.5
Step Step
Figure3: DirectionalWeightScore: Weplotthechangeinthedirectionalweightscorethroughout
thepre-training(generalization)stage(left)andfine-tuning(specialization)stage(right). Inallcases,
thedirectionalscoreisalmostmonotonic,indicatingtheincreasingnumberofweightoutliervalues
duringgeneralizationandthedecreasingnumberduringspecialization. TheResNet(18)andViT(8)
modelswerepre-trainedandfine-tunedindependentlyateachstage
newlow-rankmatrixforeachlayer. Consequently,amodelfine-tunedviarankrLoRAdiffersfrom
itsbasemodelbyamatrixofatmostrankrforeachlayer. Furthermore,twomodelsfine-tunedfrom
thesamebasemodelusingrankr andr LoRAsdifferfromeachotherbyamatrixofatmostrank
1 2
r +r perlayer. Weusethispropertytoprovideabetterestimateofthenodedistancebetween
1 2
LoRAmodelsanddefinetheLoRAweightdistanceas:
ℓ (u,v)=max(rank(u −v )) (2)
LoRA l l
l∈L
whereListhesetoffine-tunedLoRAweights. Inpractice,wecomputetherankusingsingularvalue
decomposition(SVD),wheretherankisthenumberofsingularvaluesgreaterthansomethresholdϵ.
Similartothefullfine-tuningcase(seeFig.2),alowLoRAweightdistancebetweentwomodelsis
highlycorrelatedwithanedgebetweentheirnodes.
4.2 Estimatingedgedirectionfromweights
Thedirectionofanedgebetweentwonodesuandvreflectswhethermodelvwastrainedfromu
orviceversa. TheweightdistancefromSec.4.1cannotdisentanglethedirectionasitresultsina
symmetricmatrix. Estimatingthedirectionofedgesrequiresastatisticoftheweightsthatevolve
monotonically during training. To this end, we use kurtosis (i.e., fourth moment) and define the
directionalweightscoreas:
(cid:104) (cid:105)
E (u −µ)4
(cid:88) l
k(u)= (3)
(cid:16) (cid:104) (cid:105)(cid:17)2
l∈L E (u l−µ)2
whereLisasetofmodellayers. Notethatthedirectionalscoreonlydefinesanorderbetweenrelated
nodes;unrelatednodesmayhaveverydifferentscores.
Tostudytheeffectivenessofthisscore,weobservehowtheweightsofamodelevolvethroughoutthe
trainingprocess.Interestingly,wefoundthatthetrainingprocesscanbecategorizedintotwostages:a
generalizationstageandaspecializationstage.AsseeninFig.3,whilethescoreismonotonicinboth
stages,itincreasesduringgeneralizationanddecreasesduringspecialization.Althoughgeneralization
isusuallyattributedtopre-trainingandspecializationtofine-tuning,thisisnotnecessarilyalwaysthe
case. Thetermfine-tuningistypicallyusedforanytrainingperformedaftertheinitialpre-training
stage. However,generalizationtrainingmayalsobeappliedtoanalreadypre-trainedmodel,weshow
suchanexampleinSec.6(StableDiffusion).
Intuition. ConsideramodelwithGaussian-basedweightinitialization,suchasKaiminginitial-
izationbyHeetal.(17). Duringthegeneralizationstage,tobetterencodelarge,general-purpose
datasets, theweightsofamodelwilllikelyrequireanincreasingnumberofdiversevalues. This
mayleadtoanincreaseinthenumberofoutliervaluesintheweightmatrix. Incontrast, during
thespecializationstage,theweightsofamodelwithalowervaluediversitywilllikelysufficeto
encompass the smaller, task-specific data. This may lead to a decrease in the number of outlier
valuesintheweightmatrix. Kurtosis,asameasureofthe“tailedness”ofadistribution,capturesthis
property.
4
TiV
teNseR
TiV
teNseRA B C
GPC PC2
A
A A
A B C A
B C B B
B C
A A C C
C B A
B C B C
PCS S3 (a) Input (b) Edge via (c) Root via weight
models weight distance directional score
Figure4: RecoveringaSimplifiedModelGraph: WeenumerateallpossibleModelGraphsofsize3
(left). Ontheright,wedemonstrateaModelGraphRecoveryprocess. (a)Asetof3modelswithno
priorknowledgeregardingtheirrelations. (b)Placeedgesbetweenthenodeswiththelowestweight
distance. (c)Designatethenodewiththehighestdirectionalweightscoreastheroot
5 ModelTreeHeritageRecovery
Giventheabovepriors,wenowdescribehowtorecoverthestructureofModelGraphs.Asawarm-up
problem,inSec.5.1westartbymanuallysolvingallthe3nodesModelGraphs. Then,inSec.5.2
wedescribeMoTHer,ourproposedalgorithmforrecoveringrealModelGraphs.
Inbothcases,wehaveaccesstothemodelweightsandassumethatwearegiventhetrainingstageof
eachnode(i.e.,generalizationorspecialization)1.EachModelGraphmaycontainoneormoreModel
Trees. ConnectednodeswithinaModelTreearederivedfromeachotherviaadditionaltrainingsteps.
Unlessotherwisespecified,weassumenopriorknowledgeregardingthemodelrelations.
5.1 Warm-up: AsimplifiedModelGraph
To recover a Model Graph of size 3, we place edges between the nodes with the lowest weight
distance and designate the node with the highest directional weight score as the root. Next, we
elaborateoneachofthepossiblesize3ModelGraphs.
Grandparent-Parent-Child(GPC). ThisModelTreeexhibitsa3-generationalrelationship,where
each node is derived from the previous one (see Fig. 4). To recover the underlying Model Tree
structure,weuseℓ toplaceedgesbetweennodepairswiththelowestweightdistances. Thisis
FT
motivatedbyouranalysisinSec.4.1andFig.2. Next,todeterminetheorderofthenodes,weusethe
directionalscoreshowninEq.3anddesignatethenodewiththehighestscoreastheroot. Combining
thesestepsfullyconstructstheGPCModelTree,weillustratethisprocessinFig.4. Notethatthe
aboveprocessassumesthespecializationtrainingstagesforall3models. Toadjusttheprocessfor
thegeneralizationstage,simplychangethesignofthedirectionalscore. Whenthetrainingstagesof
eachnodediffer,wechoosethesignofthedirectionalscoreaccordingtoeachnode’strainingstage.
Parent-Child-Child(PC2). ThisModelTreecontainsoneparentwithtwochildren(seeFig.4).
AsintheGPCcase,westartbyplacingtheedgesusingtheweightdistancedefinedinEq.1. Since
bothchildrenarederivedfromtheroot,thedirectionalscorewillpredictthenodewiththehighest
scoreastheroot(seeFig.4). DifferenttrainingstagesarehandledasintheGPCcase.
Parent-Child-Stranger(PCS). Unliketheprevioustriplets,herewehaveaModelGraphcomprised
oftwoModelTrees(seeFig.4). Torecoverthisstructure,wefirstidentifythenodewithnoedge
accordingtothepairwiseweightdistance. SincethatnodebelongstoadifferentModelTree,itwill
havealargerdistancethanasetthreshold,allowingustoclassifyitastheoddone. Withthenode
isolated,wecanfollowtheprotocolofthepreviousModelTreesforthetworemainingnodes.
Stranger-Stranger-Stranger(S3). Finally,herewehaveaModelGraphcomprising3ModelTrees
(seeFig.4). SimilartoPCS,wecanclusterthemintodifferentModelTreesbasedontheirlarge
distances,allowingustoidentifythestructure.
1Thisisareasonableassumptionaswegenerallyknowwhetheramodelisafoundationmodelwithgeneral
capabilitiesorafine-tunedspecializedmodel.
5MDST
+ Algorithm
(b) Pairwise (c) Find MDST
(a) Cluster
distance matrix 𝐷 & using merged
input models
directional matrix 𝐾 prior matrix 𝑀
Figure 5: MoTHer Recovery Overview: Our proposed Model Graphs and Model Trees are new
datastructuresfordescribingthehereditytrainingrelationsbetweenmodels. Inthesestructures,
heredityrelationsarerepresentedasdirectededges. WeintroducethetaskofMoTHerRecoveryits
goalistouncovertheunknownstructureofModelGraphsbasedontheweightsofasetofinput
models. Ouralgorithmworksasfollows: (a)ClusterintodifferentModelTreesbasedonthepairwise
weightdistances. (b)Foreachcluster,i)useℓ orℓ tocreateapairwisedistancematrixDfor
FT LoRA
placingedges,andii)createabinarydirectionalmatrixK basedonthekurtosistodeterminethe
edgedirection. (c)TorecoverthefinalModelTree,runaminimumdirectedspanningtree(MDST)
algorithm on the merged prior matrix M. The final recovered Model Graph is the union of the
recoveredModelTrees
5.2 MoTHerRecovery: ScalingupModelGraphRecovery
We present MoTHer, our method for recovering the structure of larger Model Graphs. Let
v ,...,v ∈V beasetofnodesrepresentingdifferentmodelsthatsharethesamearchitecture. For
1 n
simplicity,assumefornowthatallv ,...,v ∈T ,i.e.,allnodesarefromthesameModelTree,we
1 n
willlaterrelaxthisassumption.
OurgoalistorecovertheedgesE oftheModelTreeT,thisisdonebyplacingedgesbetweentwo
nodes, where one was trained from the other. As seen above, recovering the structure requires a
combinationoftheestimatedweightdistanceandtheedgedirection. LetD beaweightdistance
matrixandletK beabinarydirectionalmatrix,
(cid:26) (cid:26)
ℓ(v ,v ), ifi̸=j 1, ifk(v )<k(v )
D = i j , K = i j (4)
ij ∞, otherwise ij 0, otherwise
Toallowforbothgeneralizationandspecializationnoderelations,wedefineT asabinarymatrix
(cid:26)
1, ifgeneralization
T = (5)
ij 0, otherwise
ThefinaldistancematrixforrecoveringtheModelTreetakesintoaccountall3constraintsasfollows,
M =D+λ(K⊕T) (6)
ij
where⊕isabinaryXORandλregularizesthedirectionalscore,allowingforsomemistakes. Since
ℓ andℓ mayrangeinvalue,wedefineλtobeproportionaltoDwithλ=c·( 1 (cid:80)n D )
FT LoRA n2 i,j=1 ij
wherecissomeconstant.
WecanrecovertheModelTreefromM usingaminimumdirectedspanningtree(MDST)algorithm.
Inthispaper,weemploytheChu-Liu-Edmonds’algorithm(5;10),whichiterativelycontractscycles
inthegraphuntilformingatree. Thealgorithmproceedsasfollows: initially,ittreatseachnode
asatemporarytree. Then,itmergesthetemporarytreesviatheincomingedgewiththeminimum
weight. Subsequently,itidentifiescyclesintheremainingtemporarytreesandremovestheedgewith
thehighestweight. Thismergingprocesscontinuesuntilallcyclesareeliminated,resultinginthe
minimumdirectedspanningtree. ThealgorithmrunsinO(EV),however,fasterMDSTalgorithms
exist(13)withO(E+VlogV).
Multiplecomponents. Thusfarweassumedallv ,...,v arefromthesametree. Torelaxthis
1 n
assumptionweaddanadditionalclusteringstepbeforerunningtheabovealgorithm. Specifically,we
runaclusteringalgorithmonthepairwiseL distanceofv ,...,v andfindtheMDSTforeachof
1 1 n
theclustersindependently.
6- ImageNet
- ImageNet-21k
- MAE
- DINO
- MSN
Figure6: ViTTreeHeritageRecovery(VTHR)DatasetOverview: Ourdatasetcontains3Model
Graph splits. Each Model Graph contains 105 nodes across five Model Trees rooted at different
pre-trainedViTmodelsfoundonHuggingFace. Eachnoderepresentsamodelthatwasfine-tunedon
differentdatasetswithdifferentconfigurations. WevisualizethestructureofasingleModelGraph
6 Experiments
Experimentalsetup ToevaluatetheperformanceofMoTHerweconstructtheViTTreeHeritage
Recovery(VTHR)datasetcontaining3splits: i)FT:fullfine-tuning,ii)LoRA-V:LoRAfine-tuning
withvaryingranks,iii)(LoRA-F):LoRAfine-tuningwithfixedrank. EachcategorycontainsaModel
Graphwith105modelsin3levelsofhierarchy. TheModelGraphscomprisedof5ModelTrees
rootedbydifferentpre-trainedViTs(8)foundonHuggingFace. ThesecondlevelofeachModel
Tree contains 4 models fine-tuned on randomly chosen datasets from the VTAB benchmark(38).
Eachsecond-levelmodelhas4childnodes,fine-tunedwithrandomlysampledVTABdatasetswhile
ensuringtheyaredifferentthantheirparentmodel.Welabelallofthemodelsasspecialization-trained.
Thefine-tuningexhibitssomevariationinhyper-parameters,formoredetailsseeApp.A.Inaddition
toVTHR,weevaluateourmethodonin-the-wildModelTreesfoundonHuggingFace.
Weevaluatethemethodusingaccuracy,acorrectpredictionisonewhereboththeedgeplacement
anddirectionarecorrect. Inallourtests,MoTHerraninsecondstominutesevenonaCPU.Forfull
fine-tuning,beforerunningEq.1,wemin-maxnormalizetheweightsofeachlayeraccordingtothe
layerminimumandmaximumvalueofallmodelsinthatModelTree. Weusehierarchicalclustering
fromscipy(35)withthedefaulthyper-parametersassumingknowledgeofthenumberofclusters.
6.1 ViTTreeHeritageRecovery(VTHR)datasetresults
Fullfine-tuning. WestartwithasimplifiedsettingofrecoveringtheModelTreesindependently
(i.e.,aModelGraphwithasingleModelTree). WesetLfromEq.1tobeallthedenselayersof
themodel. For3outofthe5ModelTreesinthedataset,MoTHersuccessfullyreconstructsthetree
structurewithperfectaccuracy. TheothertwoModelTreessufferedfromawrongdirectionalscore,
whichresultedinanimperfectreconstruction,weshowthefullbreakdowninTab.1. Next,wetest
ourmethodontheentireModelGraph. Thisrequiresfirstclusteringthesetofnodes,asseeninTab.1,
MoTHerrecoveredtheModelGraphwithanaccuracyof0.89. Notably,theclusteringsucceeded
withperfectaccuracy,resultinginasimilarresultastheindependentModelTrees,inSec.6.3show
therelationbetweenthesizeoftheModelGraphandtheaccuracyoftheclustering.
LoRA fine-tuning. We proceed by testing MoTHer on the LoRA dataset splits, similar to the
previoussection,wefirstperformMoTHerrecoveryontheModelTreesindependently. WesetL
fromEq.2tobealltheLoRAfine-tunedvaluelayersofthemodel(bydefault,LoRAdoesnottrain
thedenselayers). AscanbeseeninTab.1,whenthedifferentmodelshavevaryingLoRAranks,we
successfullyreconstructallModelTreeswithperfectaccuracy. Incontrast,whenallmodelsusethe
samerank,thevariancebetweendifferentmodelsdecreases,resultinginaslightlyreducedaccuracy
foroneofthe5ModelTrees. WhenworkingwiththeentireModelGraph, thedifferenceinthe
LoRAmatrixbetweenmodelsfromdifferentModelTreesis(almost)guaranteedtobefullrank. As
before,theclusteringsucceededwithperfectaccuracy,seeTab.1fortheModelGraphresults.
ToablatewhethertheLoRA-basedℓ distancedefinedinEq.2isnecessary,werepeattheabove
LoRA
experimentwithℓ . Notusingthelow-rankdistancepriorreducestheresultsonLoRA-V by22%
FT
fromperfectaccuracyto0.78,demonstratingthesignificanceofℓ forLoRAfine-tunedmodels.
LoRA
7Table1: MoTHerResultsonVTHR:MoTHerRecoveryachieveshighaccuracybothforindividual
ModelTreesandentireModelGraphsacrossallVTHRsplits
VTHR ModelTreeRoot Model
Split ImageNet ImageNet-21k MAE DINO MSN Graph
FT 1.0 1.0 0.95 0.5 1.0 0.89
LoRA-F 1.0 1.0 1.0 1.0 0.8 0.96
LoRA-V 1.0 1.0 1.0 1.0 1.0 1.0
6.2 In-the-wildMoTHerrecovery
HavingshownwecanreconstructtheModelGraphstructurewithhighaccuracyontheVTHRdataset,
wenowattempttoreconstructtheModelTreeofin-the-wildmodelsfoundonHuggingFace. We
notethattheexamplesbelowareuniqueinthattheyprovidearelativelydetaileddescriptionoftheir
modelhierarchy. Wearenotawareofothermodelfamiliesthatprovidesuchadetaileddescription,
thisprovidesauniqueopportunityforustotestourmethodinareal-worldsituationwithground
truthmetricsbutalsoemphasizestherelevanceoftheMoTHerrecoverytask.
Llama2. TheLlamapapers(33;30)describeindetailthetraininghierarchyoftheirmodelsand
even include a figure that shows the model hierarchy. As such, we can evaluate MoTHer on the
Llamafamilycomparetothegroundtruththeyprovide. WeusetheofficialLlama2modelsprovided
byMeta,whichcontain5modelsin3levelsofhierarchy. Weusetheℓ distancedescribedabove,
FT
sincethefine-tunedmodelsarespecializedversionsofthefoundationmodel,welabeleachmodel
asaspecializationtrainedmodel. Ourmethodsuccessfullyreconstructsthecorrectstructureofthe
ModelTreewithperfectaccuracy,weplotthefullModelTreeinFig.1.
StableDiffusion TheHuggingFacemodelcardsforStableDiffusiondescribea4levelhierarchy
spanning5oftheirmodels. Weusetheℓ distancedescribedabove,however,sincethedifferent
FT
modelversionsarebetterandmoregeneralizedfoundationmodelswetreatthemasgeneralization
nodes. AsseeninFig.8,MoTHersuccessfullyreconstructsallbutasingleedge,incorrectlyplacing
“StableDiffusion1.4”asadescendentof“StableDiffusion1.3”,insteadofasibling. Notably,the
mistakeoccurredduetoawrongdistance,asthedirectionalscorereturnedthecorrectedgedirection.
6.3 Ablations
Robustnesstosimilarmodels. Foundationmodelsoftencomewithfine-tuning“recipes”,assuch,
manypubliclyavailablemodelsarealmostidenticaltoeachother,oftenwithjustadifferentseed.
WethereforetesttherobustnessofMoTHerwith3identicallyfine-tunedversionsofViTthatused
differentseeds. Inall3cases,thedistancebetweensiblingmodelswasgreaterthantotheparent
model,allowingustocorrectlyrecovertheModelTree.
RobustnesstosmallerModelTrees. WeablatewhetherMoTHerissensitivetothesizeofthe
ModelTree. WethereforesamplesubModelTreesofvaryingsize. Foreachofthe5ModelTrees,
eachexperimentwasrepeated10times,withanewlysampledrandomsubModelTreeeachtime
100
SD
80 v1.3
SD SD
60
v1.1 v1.2
40
GT SD SD
20 V VT TH HR R F LoT RA-V Stable Diffusion v1.4 v1.5
0
0.2 0.4 0.6 0.8 1.0
Figure8: StableDiffusionModelTree: Wesuc-
Node Percentage
cessfullyreconstructallbutasingleedge,where
Figure7: ClusteringRobustnesstoSmallModel theestimatedweightdistancewasincorrectbut
Graphs: Inbothcases,theclusteringworkswith thedirectionwascorrect
highaccuracyevenforsmallModelGraphs
8
ycaruccA(50subModelTreesforeachtestedsize). Indeed,thenumberofnodesinaModelTreesizedid
notimpacttheresults(seeTab.4). WenotethatwedidnottestMoTHeronhuge,webscaleModel
Graphs,inSec.7wediscussscalingMoTHerrecoverytowebscaleModelGraphs.
ClusteringrobustnesstosmallerModelGraphs Wenowtestwhethertheclusteringsucceedsfor
smallModelGraphs. AsseeninFig.7,evenwithaslittleas10models(across5ModelTrees),the
clusteringachieveshighaccuracy,indicatingMoTHerrecoverycouldbeperformedevenforsmall,
yetdiverseModelGraphs.
Otherdirectionalscores. Ourdirectionalscore(Eq.3)useskurtosistoestimatethedistributional
changeinoutliersofweightvalues.However,otherdirectionalscoresmaybefavorable.Wecompared
theperformanceusingthemean,variance,skewness,kurtosis,andentropy. Todoso,wefine-tuned
eachoftherootmodelsfromtheVTHRdatasetandextractedintermediateweightsthroughoutthe
trainingprocess. Thekurtosisistheonlymetricthatdemonstratedconsistentmonotonicityacrossthe
differentmodels(seeFig.14).
Effectoflayerstypes. Neuralnetworksoftencontainmultiplelayertypes(e.g.,linear,convolu-
tional,attention). Wethereforestudythechangeinthedirectionalscorefordifferentlayertypes
throughoutthefine-tuningprocess. Despitedifferenttypesoflayersexhibitingsimilartrendson
average,thedenselayerremainedconsistentacrossallModelTrees(seeFig.19).
7 DiscussionandLimitations
Training stage supervision. Throughout the paper we assumed the training stage is known in
advance. Whileformanyusecasesthisisareasonableassumption,findingamethodtoautomatically
inferthestagebasedonthemodelweightswouldallowforgreaterapplicabilityincaseswherethe
trainingstagesupervisionismissing. Alternatively, onecouldextendMoTHerwithmethodsfor
identifyingthedirectionofanedgethatdonotrelyonthetrainingstage.
MoTHerRecoveryatwebscale. ThispapermakesthefirststepinrecoveringtheModelGraph
of trained neural networks. However, recovering web scale Model Graphs requires significant
computational resources for storing the hundreds of thousands of models and computing their
distancematrices. Additionally,asModelTreescontinuallygrowandnewModelTreesareadded
totheModelGraph,manuallydocumentingthesechangesisinfeasible. Newalgorithmsarelikely
neededforefficientweightdistancecomputationandforcontinuouslyupdatingtheModelGraphwith
newmodelsandModelTrees. Projectingthemodelweightstosomelowerdimensionalembedding
spacecoulddecreasethecomputationalcostsandbemoresuitableforwebscaleModelGraphs.
Inferringgraphpropertiesfrommodelmetadata. MoTHerrecoveryisthefirststeptowards
indexingtheAImodeluniverse. Akintodigitalmapsoftheworld,onceaModelGraphhasbeen
constructed, additional layers that contain node metadata (e.g., training dataset, training hyper-
parameters,modelaffiliation)couldbeaddedtoit. Givenpartialdataonalimitednumberofnodes,
itmaybepossibletotrainaGraphNeuralNetwork(GNN)(16)orsimilarmethods(3;22;37)onthe
ModelGraphtoestimatetheattributesofothernodes.
8 Conclusion
Althoughmodelweightsfullycharacterizeamodel’scapabilities,theirlackofinterpretabilityhas
leftthemunder-explored. However,withthegrowingnumberofpubliclyavailablemodels,including
over600,000modelsonHuggingFace,modelweightsaresettobecomeasignificantdatamodality.
WeproposetheModelGraph,adatastructurefordescribingthehereditaryrelationsbetweenmodels.
TorecoverthestructureofaModelGraph,weintroducethetaskofMoTHerRecovery,whichaims
tomapoutunknownModelGraphs. Weidentifytwokeypropertiesofmodelweightsthatenable
therecoveryofModelGraphs. WevalidateourapproachbysuccessfullyreconstructingtheModel
Graphofin-the-wildproductionmodels. Takentogether,theModelGraphandMoTHerRecovery
makeanexcitingfirststeptowardunderstandingtheoriginofmodels.
9References
[1] JosephAntogniniandJaschaSohl-Dickstein. Pcaofhighdimensionalrandomwalkswithcomparisonto
neuralnetworktraining. AdvancesinNeuralInformationProcessingSystems,31,2018. 2
[2] SergeyBrinandLawrencePage. Theanatomyofalarge-scalehypertextualwebsearchengine. Computer
networksandISDNsystems,30(1-7):107–117,1998. 2
[3] JoanBruna,WojciechZaremba,ArthurSzlam,andYannLeCun. Spectralnetworksandlocallyconnected
networksongraphs. arXivpreprintarXiv:1312.6203,2013. 9
[4] HilaChefer,ShirGur,andLiorWolf. Transformerinterpretabilitybeyondattentionvisualization. In
ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages782–791,
2021. 2
[5] Yoeng-JinChu. Ontheshortestarborescenceofadirectedgraph. ScientiaSinica,14:1396–1400,1965. 6
[6] GuyDar,MorGeva,AnkitGupta,andJonathanBerant. Analyzingtransformersinembeddingspace.
arXivpreprintarXiv:2209.02535,2022. 1,2
[7] CharlesDarwin,JohnWyonBurrow,andJohnWyonBurrow. Theoriginofspeciesbymeansofnatural
selection:or,thepreservationoffavoredracesinthestruggleforlife. ALBurtNewYork,2009. 1
[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,etal.Animageisworth
16x16words:Transformersforimagerecognitionatscale. arXivpreprintarXiv:2010.11929,2020. 4,7
[9] AmilDravid,YossiGandelsman,AlexeiAEfros,andAssafShocher.Rosettaneurons:Miningthecommon
unitsinamodelzoo. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,
pages1934–1943,2023. 2
[10] JackEdmondsetal. Optimumbranchings. JournalofResearchofthenationalBureauofStandardsB,71
(4):233–240,1967. 6
[11] GabrielEilertsen,DanielJönsson,TimoRopinski,JonasUnger,andAndersYnnerman. Classifyingthe
classifier:dissectingtheweightspaceofneuralnetworks. arXivpreprintarXiv:2002.05688,2020. 2
[12] ZiyaErkoç,FangchangMa,QiShan,MatthiasNießner,andAngelaDai. Hyperdiffusion:Generatingim-
plicitneuralfieldswithweight-spacediffusion. InProceedingsoftheIEEE/CVFInternationalConference
onComputerVision,pages14300–14310,2023. 1,2
[13] Harold N Gabow, Zvi Galil, Thomas Spencer, and Robert E Tarjan. Efficient algorithms for finding
minimumspanningtreesinundirectedanddirectedgraphs. Combinatorica,6(2):109–122,1986. 6
[14] MarcusGallagherandTomDowns.Visualizationoflearninginneuralnetworksusingprincipalcomponent
analysis. InProcofIntConfonComputationalIntelligenceandMultimediaApplications, editedby:
Varma,B.andYao,X.,Australia,pages327–331.Citeseer,1997. 2
[15] Marcus Gallagher and Tom Downs. Weight space learning trajectory visualization. In Proc. Eighth
AustralianConferenceonNeuralNetworks,Melbourne,pages55–59.Citeseer,1997. 2
[16] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
Advancesinneuralinformationprocessingsystems,30,2017. 9
[17] KaimingHe, XiangyuZhang, ShaoqingRen, andJianSun. Delvingdeepintorectifiers: Surpassing
human-levelperformanceonimagenetclassification. InProceedingsoftheIEEEinternationalconference
oncomputervision,pages1026–1034,2015. 4
[18] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecognition.
InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages770–778,2016.
4
[19] EliahuHorwitz,JonathanKahana,andYedidHoshen.Recoveringthepre-fine-tuningweightsofgenerative
models. arXivpreprintarXiv:2402.10208,2024. 1,2
[20] EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,and
WeizhuChen. Lora:Low-rankadaptationoflargelanguagemodels. arXivpreprintarXiv:2106.09685,
2021. 3
10[21] ShaharKatz,YonatanBelinkov,MorGeva,andLiorWolf. Backwardlens:Projectinglanguagemodel
gradientsintothevocabularyspace. arXivpreprintarXiv:2402.12865,2024. 2
[22] ThomasNKipfandMaxWelling. Semi-supervisedclassificationwithgraphconvolutionalnetworks.
arXivpreprintarXiv:1609.02907,2016. 9
[23] Miltiadis Kofinas, Boris Knyazev, Yan Zhang, Yunlu Chen, Gertjan J Burghouts, Efstratios Gavves,
CeesGMSnoek,andDavidWZhang. Graphneuralnetworksforlearningequivariantrepresentationsof
neuralnetworks. arXivpreprintarXiv:2403.12143,2024. 2
[24] TaoKong,AnbangYao,YurongChen,andFuchunSun. Hypernet: Towardsaccurateregionproposal
generationandjointobjectdetection. InProceedingsoftheIEEEconferenceoncomputervisionand
patternrecognition,pages845–853,2016. 2
[25] DerekLim,HaggaiMaron,MarcTLaw,JonathanLorraine,andJamesLucas. Graphmetanetworksfor
processingdiverseneuralarchitectures. arXivpreprintarXiv:2312.04501,2023. 2
[26] AvivNavon,AvivShamsian,IdanAchituve,EthanFetaya,GalChechik,andHaggaiMaron. Equivariant
architecturesforlearningindeepweightspaces. InInternationalConferenceonMachineLearning,pages
25790–25816.PMLR,2023. 1,2
[27] AvivNavon,AvivShamsian,EthanFetaya,GalChechik,NadavDym,andHaggaiMaron. Equivariant
deepweightspacealignment. arXivpreprintarXiv:2310.13397,2023. 2
[28] WilliamPeebles,IlijaRadosavovic,TimBrooks,AlexeiAEfros,andJitendraMalik. Learningtolearn
withgenerativemodelsofneuralnetworkcheckpoints. arXivpreprintarXiv:2209.12892,2022. 2
[29] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-resolution
imagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pages10684–10695,2022. 2
[30] BaptisteRoziere,JonasGehring,FabianGloeckle,StenSootla,ItaiGat,XiaoqingEllenTan,YossiAdi,
JingyuLiu,TalRemez,JérémyRapin,etal. Codellama:Openfoundationmodelsforcode. arXivpreprint
arXiv:2308.12950,2023. 2,8
[31] RamprasaathRSelvaraju,MichaelCogswell,AbhishekDas,RamakrishnaVedantam,DeviParikh,and
DhruvBatra. Grad-cam: Visualexplanationsfromdeepnetworksviagradient-basedlocalization. In
ProceedingsoftheIEEEinternationalconferenceoncomputervision,pages618–626,2017. 2
[32] RezaShokri,MarcoStronati,CongzhengSong,andVitalyShmatikov. Membershipinferenceattacks
againstmachinelearningmodels. In2017IEEEsymposiumonsecurityandprivacy(SP),pages3–18.
IEEE,2017. 2
[33] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov, SoumyaBatra, PrajjwalBhargava, ShrutiBhosale, etal. Llama2: Openfoundationand
fine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023. 2,8
[34] ThomasUnterthiner,DanielKeysers,SylvainGelly,OlivierBousquet,andIlyaTolstikhin. Predicting
neuralnetworkaccuracyfromweights. arXivpreprintarXiv:2002.11448,2020. 2
[35] PauliVirtanen,RalfGommers,TravisE.Oliphant,MattHaberland,TylerReddy,DavidCournapeau,
EvgeniBurovski,PearuPeterson,WarrenWeckesser,JonathanBright,StéfanJ.vanderWalt,Matthew
Brett,JoshuaWilson,K.JarrodMillman,NikolayMayorov,AndrewR.J.Nelson,EricJones,Robert
Kern,EricLarson,CJCarey,˙IlhanPolat,YuFeng,EricW.Moore,JakeVanderPlas,DenisLaxalde,
JosefPerktold,RobertCimrman,IanHenriksen,E.A.Quintero,CharlesR.Harris,AnneM.Archibald,
Antônio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0:
FundamentalAlgorithmsforScientificComputinginPython. NatureMethods,17:261–272,2020. 7
[36] KaiWang,ZhaopanXu,YukunZhou,ZelinZang,TrevorDarrell,ZhuangLiu,andYangYou. Neural
networkdiffusion. arXivpreprintarXiv:2402.13144,2024. 2
[37] DanielWinter,NivCohen,andYedidHoshen. Classifyingnodesingraphswithoutgnns. arXivpreprint
arXiv:2402.05934,2024. 9
[38] XiaohuaZhai,JoanPuigcerver,AlexanderKolesnikov,PierreRuyssen,CarlosRiquelme,MarioLucic,
JosipDjolonga,AndreSusanoPinto,MaximNeumann,AlexeyDosovitskiy,etal. Alarge-scalestudyof
representationlearningwiththevisualtaskadaptationbenchmark. arXivpreprintarXiv:1910.04867,2019.
7
11[39] BaoquanZhang,ChuyaoLuo,DeminYu,XutaoLi,HuiweiLin,YunmingYe,andBowenZhang.Metadiff:
Meta-learningwithconditionaldiffusionforfew-shotlearning. InProceedingsoftheAAAIConferenceon
ArtificialIntelligence,pages16687–16695,2024. 2
[40] AllanZhou,KaienYang,KayleeBurns,AdrianoCardace,YidingJiang,SamuelSokota,JZicoKolter,and
ChelseaFinn. Permutationequivariantneuralfunctionals. AdvancesinNeuralInformationProcessing
Systems,36,2024. 2
[41] BoleiZhou,AdityaKhosla,AgataLapedriza,AudeOliva,andAntonioTorralba. Learningdeepfeatures
fordiscriminativelocalization. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pages2921–2929,2016. 2
12A DatasetDetails
ForallVTHRsplits,weusethefollowingmodelsastheModelTreerootstakenfromHuggingFace:
• https://huggingface.co/google/vit-base-patch16-224
• https://huggingface.co/google/vit-base-patch16-224-in21k
• https://huggingface.co/facebook/vit-mae-base
• https://huggingface.co/facebook/dino-vitb16
• https://huggingface.co/facebook/vit-msn-base
FortheFTsplit,topreventmodeloverfitting,weuselargerdatasetsof10Ksamplesratherthanthe
original1KusedintheVTABbenchmark. Eachmodelusesadifferentrandomlysampledseed. See
Tab.2foradditionalhyper-parameters.
Apartfromtherankandseeds,bothLoRA-FandLoRA-Vusethesamehyper-parameters. LoRA-F
usesafixedrankandalphaof16,LoRA-Vusesrankssampledrandomlyoutoftheoptionsshownin
Tab.3. BothusetheVTAB-1KdatasetsshowninTab.3andrandomseeds. SeeTab.2foradditional
hyper-parameters.
Table2: FullFine-tuningHyper-parameters
Name Value
lr 2e−4
batch_size 64
epochs 5
cifar100,svhn,patch_camelyon,
datasets
clevr-count,clevr-distance,dmlab
Table3: LoRAVaryingRanksFine-tuningHyper-parameters
Name Value
lora_rank(r) 8,16,32,64
lora_alpha(α) 8,16,32,64
lr 9e−3
batch_size 128
epochs 20
cifar100,caltech101,dtd,flower102,pet37,svhn,patch_camelyon,
datasets clevr-count,clevr-distance,dmlab,kitti,dsprites-location,dsprites-orientation,
smallnorb-azimuth,smallnorb-elevation
B Llama2andStableDiffusionModels
FortheLlama2modelsweusedthefollowingversionsfoundonHuggingFace:
• https://huggingface.co/meta-llama/Llama-2-7b-hf
• https://huggingface.co/meta-llama/CodeLlama-7b-hf
• https://huggingface.co/meta-llama/CodeLlama-7b-Instruct-hf
• https://huggingface.co/meta-llama/CodeLlama-7b-Python-hf
• https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
ForStableDiffusionweusedthefollowingversionsfoundonHuggingFace:
• https://huggingface.co/CompVis/stable-diffusion-v1-1
• https://huggingface.co/CompVis/stable-diffusion-v1-2
13Table 4: Robustness to smaller Model Trees: Accuracy over VTHR sub Model Trees, mostly
unaffectedbytreesize
%Model(#Models) 20%(4) 40%(8) 60%(13) 80%(17) 100%(21)
Accuracy 0.87±0.25 0.9±0.18 0.88±0.20 0.89±0.19 0.89
• https://huggingface.co/CompVis/stable-diffusion-v1-3
• https://huggingface.co/CompVis/stable-diffusion-v1-4
• https://huggingface.co/runwayml/stable-diffusion-v1-5
Figure9: Mean Figure10: Variance Figure11: Skewness
Figure12: Kurtosis Figure13: Entropy
Figure14: Otherdirectionalscores: Wecomputethedifferenttypesofdirectionalscoresthroughout
thefine-tuningprocess.thekurtosisistheonlymetricthatremainedconsistently(almost)monotonic.
EachcolorrepresentsadifferentModelTreeroot
11 .. 00 00 05 1.00 1.000 1.000
0.995 0.99 0.995 00 .. 99 99 05
0.990 0.98 0.990 0.985
0.985 0.97 0.985 0.980
0.980 0.980 0.975
0.975 0.96 0.975 0.970
0.95 0.965
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
Figure15: Query Figure16: Value Figure17: Key Figure18: Dense
Figure 19: Effect of Layer Type: We compute the directional score throughout the fine-tuning
process. TheDenselayeristheonlylayerthatremainedconsistently(almost)monotonic. Eachcolor
representsadifferentModelTreeroot
14