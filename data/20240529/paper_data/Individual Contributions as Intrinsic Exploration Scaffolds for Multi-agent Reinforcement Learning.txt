Individual Contributions as Intrinsic Exploration Scaffolds for Multi-agent
Reinforcement Learning
XinranLi1 ZifanLiu1 ShiboChen1 JunZhang1
Abstract Dayong, 2005) and package logistics (Seuken & Zilber-
stein,2007)toemergencyresponseoperations(Parkeretal.,
Inmulti-agentreinforcementlearning(MARL),
2016)androboticcontrolsystems(Swamyetal.,2020).The
effective exploration is critical, especially in
multi-agentsettingsintroduceuniquechallengesbeyondthe
sparserewardenvironments. Althoughintroduc-
singleagentreinforcementlearning(RL),suchastheneed
ingglobalintrinsicrewardscanfosterexploration
toaddressnon-stationarityandpartialobservability(Yuan
in such settings, it often complicates credit as-
etal.,2023a),aswellasthecomplexitiesinvolvedincredit
signment among agents. To address this diffi-
assignment(Foersteretal.,2018).
culty,weproposeIndividualContributionsasin-
trinsicExplorationScaffolds(ICES),anovelap- Despite the progress made by state-of-the-art algorithms
proachtomotivateexplorationbyassessingeach likeMADDPG(Loweetal.,2017),QMIX(Rashidetal.,
agent’scontributionfromaglobalview. Inpartic- 2020)andMAPPO(Yuetal.,2022),whichleveragethecen-
ular,ICESconstructsexplorationscaffoldswith tralizedtrainingdecentralizedexecution(CTDE)paradigm,
Bayesian surprise, leveraging global transition asignificantlimitationarisesinenvironmentswithsparse
information during centralized training. These rewards. Sparse rewards, common in real-world applica-
scaffolds,usedonlyintraining,helptoguidein- tions,presentasubstantialchallengeforpolicyexploration
dividualagentstowardsactionsthatsignificantly astheyprovidelimitedguidanceduringtraining. Classical
impact the global latent state transitions. Addi- exploration methods, such as ϵ-greedy, struggle in these
tionally,ICESseparatesexplorationpoliciesfrom environments,primarilyduetotheexponentiallygrowing
exploitationpolicies,enablingtheformertouti- state space and the necessity for coordinated exploration
lizeprivilegedglobalinformationduringtraining. amongagents(Liuetal.,2021). Toaddresssparserewards
Extensiveexperimentsoncooperativebenchmark in MARL, recent approaches have focused on augment-
taskswithsparserewards,includingGoogleRe- ingextrinsicrewardswithglobalintrinsicrewards. These
searchFootball(GRF)andStarCraftMulti-agent intrinsicrewardsaretypicallydesignedtofostercoopera-
Challenge (SMAC), demonstrate that ICES ex- tion(Wangetal.,2019)ordiversity(Lietal.,2021). While
hibitssuperiorexplorationcapabilitiescompared showingpromise,thesemethodssufferfromoneobstacle:
withbaselines. Thecodeispubliclyavailableat thenon-stationarynatureofintrinsicrewardsduringtrain-
https://github.com/LXXXXR/ICES. ing(Burdaetal.,2018)introducesadditionalcomplications
increditassignment. Furthermore,balancingintrinsicand
extrinsicrewardsoftendemandsconsiderabletunningeffort,
1.Introduction particularly in the absence of prior knowledge about the
extrinsicrewardfunctions(Yuanetal.,2023b).
Multi-agentreinforcementlearning(MARL)hasrecently
Toaddresstheaforementionedchallengesandimproveper-
gainedsignificantinterestintheresearchcommunity,pri-
formanceinMARLwithsparserewards,weproposeanew
marilyduetoitsapplicabilityacrossadiverserangeofprac-
explorationmethod,namedIndividualContributionasIn-
ticalscenarios. Numerousreal-worldapplicationsaremulti-
trinsic Exploration Scaffolds (ICES). The key idea is to
agentinnature,rangingfromresourceallocation(Ying&
take advantage of the CTDE paradigm and utilize global
1Department of Electronic and Computer Engineering, The information available during the training to construct in-
Hong Kong University of Science and Technology, Hong trinsicscaffoldsthatguidemulti-agentexploration. These
Kong SAR, China. Correspondence to: Shibo Chen <eeshi-
intrinsic scaffolds are specifically designed to encourage
bochen@ust.hk>.
individualactionsthathaveasignificantinfluenceonthe
Proceedings of the 41st International Conference on Machine underlying global latent state transitions, thus promoting
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by cooperativeexplorationwithouttheneedtolearnintrinsic
theauthor(s).
1
4202
yaM
82
]GL.sc[
1v01181.5042:viXraIndividualContributionsasIntrinsicExplorationScaffoldsforMulti-agentReinforcementLearning
credit assignment. Furthermore, these scaffolds, akin to actionscollectivelyformajointactionu∈Un,leadingto
physicalscaffoldsinconstruction,willbedismantledafter atransitiontothenextglobalobservations′ ∼ P(s′|s,u)
trainingtopreventanyeffectonexecutionlatency. andyieldingaglobalrewardr = R(s,u). Forclarity,we
refertothisglobalrewardastheextrinsicrewardr ,dis-
In particular, we make two key technical contributions. ext
tinguishingitfromtheagents’intrinsicmotivations. Each
Firstly, we shift the focus from global intrinsic rewards
agentkeepsalocalaction-observationhistorydenotedas
to individual contributions as the primary motivation for
h ∈ (Ω×U). Theteamobjectiveistolearnthepolicies
agentexploration. Thisapproacheffectivelycircumvents i
thatmaximizetheexpecteddiscountedaccumulatedreward
thecomplexitiesinvolvedincreditassignmentforglobal G =(cid:80) γtrt.
intrinsicrewards. Toencourageagentstoperformcooper- t t
ativeexploration,wecapitalizeonthecentralizedtraining
2.2.ConditionalVariationalAutoencoders(CVAEs)
toestimatetheBayesiansurpriserelatedtoagents’actions,
quantifyingtheirindividualcontributions. Thisisachieved CVAEs(Sohnetal.,2015)extendvariationalautoencoders
byemployingaconditionalvariationalautoencoder(CVAE) (VAEs)tomodelconditionaldistributions,adeptathandling
withtwoencoders. Secondly,weoptimizeexplorationand scenarios where the mapping from input to output is not
exploitationpoliciesseparatelywithdistinctRLalgorithms. one-to-one,butratherone-to-many(Sohnetal.,2015). The
Inthisway,theexplorationpoliciescanbegrantedaccessto generationprocessinaCVAEisasfollows: givenanob-
privilegedinformation,suchasglobalobservations,which servation x, a latent variable z is sampled from the prior
helpstoalleviatethenon-stationaritychallenge.Importantly, distributionp (z|x),andtheoutputyisgeneratedfromthe
θ
theseexplorationpoliciesserveastemporaryscaffolds,and conditionaldistributionp (y|x,z). Theobjectiveistomax-
θ
donotintrudeonthedecentralizednatureoftheexecution imize the conditional log-likelihood, which is intractable
phase. inpractice. Therefore,thevariationallowerboundismaxi-
mizedinstead,whichisexpressedas:
We evaluate the proposed ICES on two benchmark envi-
ronments: GoogleResearchFootball(GRF)andStarCraft
L (x,y;θ,ϕ)=D [q (z|x,y)∥p (z|x)]
CVAE KL ϕ θ
Multi-agentChallenge(SMAC),undersparserewardset-
+E [logp(y|x,z)],
tings. The empirical results and comprehensive ablation qϕ(z|x,y)
(1)
studiesdemonstrateICES’ssuperiorexplorationcapabili-
ties,notablyinconvergencespeedandfinalwinrates,when whereD denotestheKullback–Leibler(KL)divergence
KL
comparedwithexistingbaselines. andq (z|x,y)isanapproximationofthetrueposterior.
ϕ
2.Background 2.3.RelatedWorks
Inthis section, webrieflyintroduce thefully cooperative Besidesadaptingexplorationtechniquesfromsingle-agent
multi-agent task considered in this work and provide es- RL, considerable research has focused on developing ex-
sentialbackgroundonCVAEs, whichwillbeutilizedfor plorationmethodstailoredtomulti-agentsettings. Wecate-
constructingmeaningfulindividualcontributionassessment. gorizetheseeffortsintotwobroadtypes: global-leveland
Then, weprovideanoverviewofrelatedworksonmulti- agent-levelexploration.
agentexploration.
Global-levelExploration: Researchinthisdomainaims
toencourageexplorationintheglobalspace. Forinstance,
2.1.ProblemSetting
MAVEN(Mahajanetal.,2019)integrateshierarchicalcon-
trolbyintroducingalatentspacetoguideexploration. Stud-
Decentralized Partially Observable Markov Decision
iesbyLiuetal.(2021),Xuetal.(2023b)andJoetal.(2024)
Process(Dec-POMDP): Weconsiderafullycooperative
focusonreducingtheexplorationspacebyidentifyingkey
partiallyobservablemulti-agenttaskmodeledasadecen-
subspaces. MAGIC(Chenetal.,2022)adoptsgoal-oriented
tralizedpartiallyobservableMarkovdecisionprocess(Dec-
exploration for multi-stage tasks. Other approaches em-
POMDP)(Oliehoek&Amato,2016). TheDec-POMDPis
phasizeencouragingdesirablecollectivebehaviors,suchas
definedbyatupleM = ⟨S,U,P,R,Ω,O,n,γ⟩,wheren
the work by Chitnis et al. (2019) that emphasizes foster-
denotesthenumberofagentsandγ ∈(0,1]isthediscount
ingsynergisticbehaviorsamongagents, andLAIES(Liu
factor thatbalances the trade-offbetween immediateand
etal.,2023a),whichavoidslazyagentsbyencouragingdili-
long-termrewards.
gence. Additionally,methodslikeEMC(Zhengetal.,2021)
Attimestept, withtheglobalobservations ∈ S, agenti and MASER (Jeon et al., 2022) seek to enhance sample
receivesalocalobservationo i ∈Ωdrawnfromtheobser- efficiency by effectively utilizing existing experiences in
vationfunctionO(s,i). Subsequently,theagentselectsan replaybuffers,eitherbyreplayinghigh-rewardsequences
actionu i ∈U basedonitslocalpolicyπ i. Theseindividual orcreatingsubgoalsforcooperativeexploration.
2IndividualContributionsasIntrinsicExplorationScaffoldsforMulti-agentReinforcementLearning
Agent-levelExploration: Thiscategoryofresearchincor- rewardstotheoriginalcreditassignmentproblem.
poratesspecificobjectivesattheindividualagentlevel.EITI
andEDTI(Wangetal.,2019)focusonmaximizingthemu-
3.IndividualContributionsasIntrinsic
tual influence among agents’ state transitions and values.
ExplorationScaffolds
Lietal.(2021)promotesdiversebehaviorsamongagents
andXuetal.(2023a)proposestoencouragediversejoint In this work, we propose a novel approach of leveraging
policycomparedtohistoricalones. SMMAE(Zhangetal., individualcontributionsasintrinsicscaffoldstoenhanceex-
2023) fosters individual curiosity, while ADER (Kim & plorationinMARL.Itaimstofullyutilizeprivilegedglobal
Sung,2023)introducesanadaptiveentropy-regularization informationduringcentralizedtrainingwhileensuringde-
schemetoallowvariedlevelsofexplorationacrossagents. centralizedexecutionremainsunaffected.
Whileglobal-levelexplorationaidsinfosteringcooperative Thefollowingsubsectionswilladdressthreekeyquestions:
behaviors,theintegrationofglobalextrinsicandintrinsic 1) Why use individual contributions as intrinsic scaf-
rewards often complicates credit assignment, potentially folds? Section3.1examinestheadvantagesoffocusingon
hindering algorithm performance. Some methods (Chen thecontributionsofindividualagentsovercollectiveteam
etal.,2022;Liuetal.,2023a)alsorelyonparsingtheglobal effortsinenhancingexplorationstrategieswithinMARL.
observation,andthusrequirespecificdomainknowledge, 2)Howtoassessindividualcontributions? Section3.2
whichtherebylimitstheirapplicability. Incontrast,agent- describeshowourmethodsquantifyeachagent’simpacton
levelexplorationoffersamorestraightforwardapproachbut global latent state transitions using Bayesian surprise. 3)
mayresultinlesscoordinatedactionsamongagents. Our Howarethesescaffoldsutilizedeffectively? Section3.3
methodseekstocombinetheadvantagesofbothapproaches, elaboratesonhowexplorationandexploitationpoliciesare
assigning specific motivations to individual agents while optimizedwithdistinctobjectivesandstrategiestoutilize
leveragingglobalinformationtoshapethesemotivations. thesescaffoldseffectively,therebyenhancingexploration
withoutcompromisingtheoriginaltrainingobjectivesorthe
Beyondtheliteratureonexploration,wediscusstwoother
decentralizedexecutionstrategy.
researchlinesthatsharesimilartechniquestothoseusedin
thiswork:
3.1.FromGlobalIntrinsicRewardstoIndividual
IntrinsicRewardsforMorethanExploration: Inaddi-
Contributions
tiontoleveragingintrinsicrewardsforbetterexploration,
theconceptofintrinsicmotivationhasbeenappliedtoother Previousmethodslargelyrelyonformulatingaglobalin-
aspectsofMARL.Forexample,LIIR(Duetal.,2019)pro- trinsicreward,whichisthenaddedtoextrinsicrewardsto
posesutilizingintrinsicrewardstoexplicitlyassigncredits incentivize agents to explore. This strategy presents two
todifferentagents, resultinginanalgorithmofenhanced notabledrawbacks: Firstly,addingintrinsicrewardstothe
performance. Otherworksdesignintrinsicrewardstoincor- existingextrinsicrewardsalterstheoriginaltrainingobjec-
poratepreferencessuchassocialinfluences(Jaquesetal., tive. Intrinsicrewards,oftenlearnedandthusnon-stationary
2019), social diversity (McKee et al., 2020), and align- throughoutthetrainingphase(Burdaetal.,2018),willintro-
ment(Maetal.,2022;Zhangetal.,2024)intothelearned duceadditionalnon-stationarityintothetrainingobjective.
policies.
Secondly,likeglobalextrinsicrewards,globalintrinsicre-
CreditAssignment: Creditassignmentisakeychallenge wardsrequirecreditassignmentamongagents,ataskthat
in MARL, referring to how to allocate global rewards to becomesmorechallengingwiththenon-stationarynature
provideaccuratefeedbackforindividualagents(Yuanetal., of intrinsic rewards. These complications can be effec-
2023a). Several value decomposition methods, such as tivelybypassedbydirectlyprovidingagentswithindividual
VDN(Sunehagetal.,2018),QMIX(Rashidetal.,2020), intrinsic motivations. In our method, this is achieved by
andQTRAN(Sonetal.,2019),havebeenproposedtoim- utilizingprivilegedglobalinformationavailableonlyduring
plicitlyassigncreditsamongagentsfordiscreteactions,and thecentralizedtrainingphase,thusaddressingtheissuesof
alaterworkLICA(Zhouetal.,2020)tacklesthesameissue non-stationarityandcomplexcreditassignment.
in the continuous action domain. COMA (Foerster et al.,
ThisisfurtherverifiedbyempiricalablationstudiesinSec-
2018)takesadifferentapproachandusesthecounterfac-
tion4.3.
tualbaselinetoexplicitlymeasureeachagent’scontribution.
Morerecently,NA2Q(Liuetal.,2023b)proposesaninter-
3.2.AssessingIndividualContributionstoConstruct
pretablecreditassignmentframeworkbyexploitinggeneral-
IntrinsicScaffolds
izedadditivemodels. Unliketheseworksthataimtosolve
thecreditassignmentchallenge,ourworkfocusesonavoid- BayesianSurprisetoCharacterizeIndividualContribu-
ing extra complexity brought by non-stationary intrinsic tions: In this subsection, we assess the individual contri-
3IndividualContributionsasIntrinsicExplorationScaffoldsforMulti-agentReinforcementLearning
bution,denotedasri ,ofaspecificactionui executedby agentsandtheeffectsofnoisyTV.Consequently,weemploy
t,int t
agenti. Theobjectiveistoevaluatetheimpactofactionui theBayesiansurpriseasthemeasurementmethod. Follow-
t
onthegloballatentstatetransitions. ing previous works (Itti & Baldi, 2005; Mazzaglia et al.,
2022),weexpressthecontributionri asthemutualinfor-
t,int
mationbetweenthelatentvariablez andtheactionui,
t+1 t
whichisgivenas
ri =I(z ;ui|s ,u−i)
t,int t+1 t t t
=D
(cid:2)
p(z |s ,u )∥p(z |s
,u−i)(cid:3)
. (2)
KL t+1 t t t+1 t t
Thistermcapturesthediscrepancybetweentheactualand
counterfactuallatentstatedistributionsfromtheperspective
Figure 1: Dynamics model. The variable z denotes the of an individual agent i. In later sections, we omit the
latentstate. Thesolidlineindicatestheindividualcontri- subscript t where contextually clear, referring to ri
t,int
butionsofagenti’sactions,highlightedbythegreenarrow, simplyasri .
int
signifyingtheprimaryfocusofourmeasurement. Dashed
lines represent other influences on state uncertainties, CVAEtoEstimatetheBayesianSurprise: Forrobustesti-
includingtheactionsofotheragents(bluearrow),whichare mationofindividualcontributions,itisessentialtoidentify
excluded from agent i’s contribution assessment, and the alatentspaceforz tthatisbothcompactandinformative,ca-
environment’sinherentstochasticity(redarrows), known pableofreconstructingtheoriginalstatespace. Toachieve
asthenoisyTVproblem,whichweaimtomitigate. this, weresorttoCVAEowingtoitsabilitytoinduceex-
plicitpriordistributionsandperformprobabilisticinference,
anecessityinenvironmentswithinherentstochasticity,such
asGRF(Kurachetal.,2020).
We aim to estimate two specific priors: p (z |s ,u )
ψ t+1 t t
andp (z |s ,u−i). However,learningthesetwopriors
ϕ t+1 t t
independentlywillnotyieldsatisfactoryresults,asshown
laterinourablationstudies(Figure7). Thisisduetothe
KL Divergence potentialmisalignmentofthelatentspacescreatedbyeach
independentlytrainedpriors,renderingtheKL-divergence
measure less effective. Thus, to align the latent spaces,
weusetwoseparateencodersfortheseestimationswhile
utilizingashareddecoderforreconstruction.
TheCVAE’sarchitecture,depictedinFigure2,includesthe
followingcomponents:
Figure2: ModulesforBayesiansurpriseestimation. The
structure resembles the CVAE structure with separate
encoders and a shared decoder. The KL divergence PriorEncoders: p ψ(z t+1|s t,u t),
betweenestimatedpriorsisusedasintrinsiccontribution p (z |s ,u−i),
ϕ t+1 t t
measurements.
ReconstructionDecoder: p (s |z ),
θ t+1 t+1
LatentPosteriors: q (z |s ,u ,s ),
ψ t+1 t t t+1
q (z |s ,u−i,s ).
Toachievethis, wefirstdemonstratetheenvironmentdy- ϕ t+1 t t t+1
namicmodelinFigure1,illustratinghowstatetransitionun-
certaintiesareinfluencedbyseveralfactors. Theseinclude TrainingObjectiveforScaffolds: Thetrainingobjective
theimpactofagenti’saction,representedbyamutualin- oftheabovemodulesistomaximizethevariationallower
formationtermr ti ,int =I(z t+1;ui t|s t,u− t i);theinfluenceof boundoftheconditionallog-likelihood(Sohnetal.,2015),
otheragents’actions,denotedbyI(z ;u−i|s ,ui);and formalizedas:
t+1 t t t
theinherentenvironmentaluncertainties,expressedasthe
entropyH(s t|z t),knownasthenoisyTVproblem(Schmid- J(ψ,ϕ,θ)=−D KL[q ψ(z t+1|s t,u t,s t+1)∥p ψ(z t+1|s t,u t)]
huber,2010). Ourfocusisprimarilyontheindividualcon- −D (cid:2) q (z |s ,u−i,s )∥p (z |s ,u−i)(cid:3)
tributionri ,whichnecessitatesaspecificmeasurement KL ϕ t+1 t t t+1 ϕ t+1 t t
t,int +E [logp (s |z)]+E [logp (s |z)].
methodtoeffectivelydistinguishthecontributionofagent z∼qψ θ t+1 z∼qϕ θ t+1
i’sactionui andmitigatepotentialmisattributionsamong (3)
t
4IndividualContributionsasIntrinsicExplorationScaffoldsforMulti-agentReinforcementLearning
Forward Pass
Back Propagation
Mixing Network
Sampling Argmax
...
KL Divergence
...
Agent 1 Agent n
Explore Policy Exploit Q-Network
...
(a) (b) (c)
Figure3: NetworkarchitectureofICES.(a)Intrinsicexplorationscaffolds. (b)Agentarchitecture. (c)Overallarchitecture.
The red arrows denote the gradient flows guided by the global extrinsic reward r , and the yellow arrows denote the
ext
gradientflowsguidedbytheindividualscaffoldsri . Thetrainingobjectivesfortheexplorationpolicyandexploitation
int
policyaredecoupledwhilebothpoliciesarecombinedforactionselectionduringthetrainingphase.
3.3.DecouplingExplorationandExploitationPoliciesto ploitationduringtraining. Theexplorationandexploitation
UtilizeIntrinsicScaffolds actionsaregivenas:
IntheICESframework,weretainthetrainingobjectiveof ui ∼ν (τi,u,s), (5)
learningthetarget(exploitation)policyπ,aimingatmaxi- explore i
mizingthecumulativerewardG t =(cid:80) tγtrt. Concurrently, ui exploit =arg umaxQ i(τi,u), (6)
weadjustthebehaviorpolicyb = {b }n toenhanceex-
i i=1
ploration. Unliketheclassicalϵ-greedymethodadoptedby withQ (τi,·)representingthelocalQ-valuefunctionfor
i
mostoftheoff-policyworks,whereactionsareuniformly exploitation.
sampledifnotfollowingthetargetpolicy,ICESagentsprior-
OptimizationObjectives: Theoptimizationobjectivefor
itizeactionsthatsignificantlycontributetostatetransitions,
asidentifiedbyri . Theoverallarchitectureisdepictedin theexploitationpolicy,parameterizedbyζ,istominimize
int
theTD-errorloss:
Figure3,withdifferentgradientflowsdenotedbyredand
yellowarrowsforglobalextrinsicrewardsandindividual
L(ζ)=E
(cid:104)(cid:0)
ytot−Q (τ ,u ,s
;ζ)(cid:1)2(cid:105)
,
scaffolds,respectively. (τt,ut,st,rext,st+1)∼D tot t t t
(7)
Wedenotethetargetpolicyasπ ={π }n ,theexploration
i i=1
policyas{ν }n andthebehaviorpolicyderivedfromthe
i i=1 where ytot = r +γmax Q (τ ,u,s ;ζ−) and
abovetwopoliciesasb={b }n . ext u tot t+1 t+1
i i=1 ζ−aretheparametersofatargetnetworkasinDQN.
CombiningtheExplorationandExploitationPoliciesfor
The optimization objective for the exploration policy,
BehaviorPolicies: Asshowninpart(b)ofFigure3,action
parameterizedbyξ,istomaximizetheaverageindividual
selection involves both the exploration policy ν and the
i intrinsic scaffolds (not the episodic return objective) and
exploitationpolicyπ . Thebehaviorpolicyb isdetermined
i i explorationpolicyentropy:
asfollows:
ui ∼b i(cid:0) ui explore,ui exploit(cid:1) J i(ξ)=E νi[r ii nt]+βH(·|τi,s), (8)
(cid:40)
ui withprobabilityα where β is a hyperparameter to control the regu-
= explore , (4)
ui withprobability1−α larization weight for entropy maximization and
exploit H(·|τi,s) = −E lnν (·|τi,s;ξ) is the entropy
νi(ξ) i
whereαisahyperparameterbalancingexplorationandex- ofpolicyν atlocal-globalobservationpair(τi,s).
i
5IndividualContributionsasIntrinsicExplorationScaffoldsforMulti-agentReinforcementLearning
Thisapproachensuresthatwhilethetrainingoftheexploita- Algorithm1TrainingProcedureofICES
tion network remains centralized and retained, the explo- 1: Init: Scaffoldsparametersψ,ϕ,θ
rationnetworkbenefitsfromdecentralizedtrainingguided 2: Init: Explorationnetworksparametersξ,η
byindividualintrinsicscaffolds. Thisstrategycircumvents 3: Init: Exploitationnetworksparametersζ
the challenges in intrinsic credit assignment. Moreover, 4: Init: D =∅,step=0,θ− =θ
sinceexplorationpoliciesareemployedonlyduringtrain- 5: whilestep<step do
max
ing, they can utilize privileged information, such as the 6: t=0. Resettheenvironment.
globalobservations,formoreinformeddecision-making.
7: fort=1,2,...,episode_limitdo
REINFORCE with Baseline for Exploration Policy 8: fori=1,2,...,ndo
Training: Bydecouplingtheexplorationandexploitation, 9: Selectactionsui t ∼b i {▷Equation(4)}
wecanemploydistinctRLalgorithmstoupdateeachpolicy, 10: endfor
leveragingtheirrespectivestrengths. Fortheexploitation 11: (s t+1,o t+1,r t,ext)=env.step(u t)
policyupdate(denotedbytheredarrowsinFigure3),we 12: D =D∪(s t,o t,u t,r t,ext,s t+1,o t+1)
follow the previous work and use the DQN update with 13: endfor
value decomposition methods like QMIX (Rashid et al., 14: ifstep mod train_interval==0then
2020)orQPLEX(Wangetal.,2020). 15: ξ,η,ζ ←TrainPolicies(ψ,ϕ,ξ,η,ζ,D)
{▷Algorithm2}
Fortheexplorationpolicy,whosegradientisdenotedbythe 16: ψ,ϕ,θ ←TrainScaffolds(ψ,ϕ,θ,D)
yellowarrowsinFigure3,wepreferstochasticpoliciesover {▷Algorithm3}
deterministic ones for more diverse behaviors. Thus, we
17: endif
adoptapolicy-basedreinforcementlearningalgorithmwith 18: ifstep mod target_update_interval==0then
entropyregularizationwiththeobjectivefunctiongivenin 19: θ− =θ
Equation (8). To stabilize training, we introduce a value
20: endif
functionV(τi,s;η)asabaseline. Withthepolicygradient
21: endwhile
theorem(detailselaboratedinAppendixA.1),wearriveat 22: Output: Exploitationnetworksparametersζ
∇ J (ξ)=E (cid:2) A·∇ lnν(·|τi,s;ξ)(cid:3) , (9)
ξ i νi(ξ),(τi,s)∼D ξ
whereA=ri −V(τi,s;η)−β istheadvantagefunction
int
andV (τi,s)isupdatedbyminimizing randomseeds. Theshadedareasrepresent50%confidence
η
intervals. Detailed descriptions of network architectures
L(η)=E
(cid:104)(cid:0)
ri
−V(τi,s;η)(cid:1)2(cid:105)
. (10)
andtraininghyperparametersareavailableinAppendixB.
νi(ξ),(τi,s)∼D int FurtherexperimentalresultsareprovidedinAppendixC.
3.4.OverallICESTrainingAlgorithm 4.1.Settings
WesummarizetheoveralltrainingprocedureforICESin Benchmarks: In this work, we test ICES and baselines
Algorithm 1. In particular, we train a scaffolds network onwidelyusedbenchmarksofGRFandSMAC1insparse
(updated by Algorithm 3 in Appendix A.2) with param- rewardsettings,withdetailselaboratedinAppendixB.2.
eters ψ,ϕ,θ and two policy networks (updated by Algo-
Baselines: We implement our proposed ICES on top of
rithm2inAppendixA.2),includinganexplorationnetwork
QMIX(Rashidetal.,2020). FortheGRFenvironment,we
parametrizedbyξ,η andanexploitationnetworkparame-
add a curve combining ICES with QPLEX (Wang et al.,
terized by ζ. We utilize the scaffolds network to provide
2020) and denote the results as ICES-QPLEX. We com-
guidanceforexplorationnetworkupdates, andweutilize
pare ICES with six state-of-arts baselines: ADER (Kim
the exploration network to influence the action selection
&Sung,2023),MASER(Jeonetal.,2022),EMC(Zheng
processes,consequentlyinfluencingthelearningprocessof
etal.,2021)(builtuponQPLEX,denotedasEMC-QPLEX),
exploitationnetworks. Amongtheabovenetworks,onlythe
CDS(Lietal.,2021),MAVEN(Mahajanetal.,2019)and
exploitationnetworkwillbeusedforexecution.
QMIX(Rashidetal.,2020). Whereverpossible,weutilize
theofficialimplementationsofthesebaselinesfromtheir
4.Experiments
respectivepapers;incaseswheretheimplementationisnot
available,wecloselyfollowthedescriptionsprovidedinthe
In this section, we evaluate ICES on two multi-agent
papersandimplementthemontopofQMIX.
benchmark tasks: GRF and SMAC. Experiments in the
GRF domain are averaged over eight random seeds and 1WeuseSC2.4.10withdifficultyof7.Notethatperformance
experimentsintheSMACdomainareaveragedoverfive isnotalwayscomparableacrossversions.
6IndividualContributionsasIntrinsicExplorationScaffoldsforMulti-agentReinforcementLearning
ICES-QPLEX ICES ADER MASER EMC-QPLEX CDS MAVEN QMIX
100
0.6 0.6 0.8 0.6
80
0.4 0.4 0.6 0.4 60
40
0.2 0.2 0.2
0.4 20
0.0 0.0 0.0 0
0 1 Time2
steps
3 1e64 0 1 0. T2 ime2
steps
3 1e64 0.0 0.2 T0. i4 meste0 p.6
s
0.8 1e1 7.0 0.0 0.5 Tim1 e. s0
teps
1.5 1e2 6.0
(a) 3_vs_1_with_keeper (b) corner (c) counterattack_hard (d) 3m
0.0
0 1 2 3 4
100 100 Timestep1s00 1e6 100
80 80 80 80
60 60 60 60
40 40 40 40
20 20 20 20
0 0 0 0
0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0 1 2 3
Timesteps 1e6 Timesteps 1e6 Timesteps 1e6 Timesteps 1e6
(e) 8m (f) 2s3z (g) 2s_vs_1sc (h) 5m_vs_6m
Figure4: PerformancecomparisonwithbaselinesonGRFandSMACbenchmarksinsparserewardsettings.
4.2.BenchmarkresultsonGRFandSMAC ICES ICES w/ global-con ICES w/ int+ext
We present the comparative performance of ICES and
0.6 100
0.6
variousbaselinesinFigure4. Overall,ICESdemonstrates
80
superior performance over baselines constructed on 0.4
0.4 60
QMIX. When integrated with QPLEX (ICES-QPLEX),
40
it surpasses baselines built on QPLEX. This showcases 0.2 0.2
thatICESisabletofostereffectiveexplorationinMARL 20
training, without tampering with the original training 0.0 0 1 2 0.03 4 0 0.0 0.5 1.0 1.5 2.0
Timesteps 0.0 1e6 2.5 Timesteps 1e6
objective(discussedinSection3.3),thusleadingtoafast
Timesteps1e6
(a) 3_vs_1_with_keeper (b) 2s_vs_1sc
convergenceandenhancedfinalperformance. Challenges
inGRF,includingenvironmentalstochasticityandtheneed
for agent collaboration, are adeptly addressed by ICES. Figure5: Ablationsonindividualcontributions.
In particular, ICES filters out environmental noise using
Bayesian surprise and promotes team coordination by
constructingscaffoldsbasedonglobalstatetransitions. (as computingintrinsicscaffoldswithinacompactlatentspace,
discussedinSection3.2.) Itisalsoworthmentioningthat, ratherthantheextensiveoriginalstatespace.
among the baselines, EMC-QPLEX also shows notable
exploration capabilities, particularly in the early stages 4.3.AblationStudies
oftraining. Thissuggeststhatutilizingepisodicmemory,
Weconductthreesetsofablationstudiesregardingdifferent
as EMC-QPLEX does, could be a beneficial approach to
aspectsofourproposedICESwithonerepresentativetask
improvesampleefficiency,albeitdifferentfromourfocus
fromeachbenchmarktask.
ongeneratingmoreinformativeexplorationexperiences.
Thefirstsetofablationstudiesfocusesonindividualcon-
For SMAC tasks, ICES also demonstrates strong perfor-
tributions. We explore two distinct modifications to our
mancecomparedwithbaselines,wheremostofthebaselines
originalapproachwiththeresultspresentedinFigure5:
requiremoretrainingbudgettofindthewinningstrategy.
Forexample,inscenario2s_vs_1sc,ICESstartstoob- -ICESw/global-con: Insteadofusingtheindividualcon-
servewinningepisodeswhilebaselineshavenotafter2M tributionasscaffoldsforthecorrespondingagentfollowing
timesteps. In SMAC, the exploration challenges mainly Equation(2),weusethecollectivecontributionofallagents
arisefromthelargestatespace,whichICESaddressesby asaglobalscaffold. Here,thecontributionofallagentsis
7
serocS
degarevA
%
etaR
niW
tseT
serocS
degarevA
%
etaR
niW
tseT
serocS
degarevA
serocS
degarevA
%
etaR
niW
tseT
serocS
degarevA
serocS
degarevA
%
etaR niW
tseT
%
etaR
niW
tseT
%
etaR
niW
tseTIndividualContributionsasIntrinsicExplorationScaffoldsforMulti-agentReinforcementLearning
ICES ICES w/o s ICES w/ int+ext
0.8 0.8
=0.3 =0.02
0.6 100 0.6 =0.2 0.6 =0.01
0.6 =0.1 =0.002
80 0.4 =0.05 0.4 =0.001
0.4
0.4 60
0.2 0.2
40
0.2 0.2
20 0.0 0.0
0 1 2 3 4 0 1 2 3 4
0.0 0 1 02.0 3 4 0 0.0 0.5 1.0 1.5 2.0 Timesteps 1e6 Timesteps 1e6
Timesteps0 1e6 2 4 Timesteps 1e6 (a) Analysisonα (b) Analysisonβ
Timesteps 1e6
(a) 3_vs_1_with_keeper (b) 2s_vs_1sc
Figure 8: Hyperparameter analysis on the
Figure6: Ablationsondecouplingexplorationandexploita- 3_vs_1_with_keeperscenario.
tionpolicies.
- ICES w/o s: This variant, diverging from the approach
specifiedinEquation(5),excludestheglobalobservations
ICES ICES w/o CVAEs ICES w/o max-ent fromtheexplorationpolicy’sinputs.
ICES w/ 2 CVAEs
-ICESw/int+ext: Inthisvariant,individualscaffoldsare
directly summed up and used as global intrinsic rewards, 0.6 100
0.50 similartopreviousmethods(Lietal.,2021).
80
0.4
0.25 60 Figure 6 shows the detrimental impact on exploration ef-
40 fectivenesswhenexcludingglobalobservationinformation
0.2 0.00
0.02.5 20 fromtheexplorationpolicies(whilemaintainingseparate
0.0 Tim1ees6teps 0 networksforexplorationandexploitation). Thishighlights
0 1 2 3 4 0.0 0.5 1.0 1.5 2.0
Timesteps 1e6 Timesteps 1e6 thesignificanceofutilizingprivilegedinformationinexplo-
(a) 3_vs_1_with_keeper (b) 2s_vs_1sc rationpolicies. Particularlyinscenarioswithpronounced
partialobservability,suchasthoseencounteredintheSMAC
Figure7: Ablationsonotherdesignchoices. tasks, the lack of global information heavily deteriorates
theexploration,withICESachievingafinalwinningrate
of 60% while ICES w/o s only achieving 20%. Further-
estimatedbyr =I(z ;u |s ). more,theperformancefurtherdegradeswhenexploration
t,int t+1 t t
-ICESw/int+ext: Inthisvariant,individualscaffoldsare andexploitationpoliciesaremergedintoasinglenetwork.
directly summed up and used as global intrinsic rewards, Thissetofablationstudiesemphasizesthecriticalroleof
similartopreviousmethods(Lietal.,2021). decouplingexplorationandexploitationpolicies.
ResultsinFigure5indicatethatreplacingindividualcon- ThelastsetofablationsareforotherdesignchoicesinICES
tributions with global contributions hinders effective ex- andtheresultsaregiveninFigure7:
ploration. Notably,inthe3_vs_1_with_keepertask,
-ICESw/omax-ent: Thisvariationeliminatestheentropy
ICESachievesthefinalperformanceofover60%winning
regularizationbybysettingβ =0inEquation(8).
rate while ICES w/ global-con only archives 40%. This
-ICESw/oCVAEs: ContrarytoleveragingKL-divergence
highlightsthemisallocationofexplorationincentiveswhen
in the latent space as stated in Equation (2), this variant
contributionsarenotindividuallyattributed,particularlyin
directlycalculatestheintrinsiccontributionastheEuclidean
scenarioswherespecificagentsplaypivotalroles(e.g. the
distanceintheoriginalstatespace.
playerwiththeballinGRF).Moreover,furtherintegrating
-ICESw/2CVAEs: Insteadofemployingtwoencoders
thescaffoldsintoaglobalintrinsicrewardexacerbatesper-
andashareddecoder, thisvarianttrainstwoindependent
formancedegradation. Thiscouldbeattributedtotheadded
CVAEsforBayesiansurpriseestimation.
complexityofneedingtoassigncreditamongagentsofthis
new,non-stationaryintrinsicreward,complicatingthetrain- Figure7indicatesthateachofthesemodificationsleadsto
ingprocess. Thus,thissetofablationstudiesunderscores adeclineinperformanceintermsoffinalperformanceor
theeffectivenessofdirectlyassigningindividualscaffolds convergencespeed.
toagents.
4.4.HyperparameterAnalysis
The second set of ablation studies investigates the effect
of decoupling exploration and exploitation policies. We We further investigate the effect of different hyperparam-
conductedexperimentswithtwovariantswiththeresults eters on the performance of ICES, as shown in Figure 8.
showninFigure6: The hyperparameter α controls the tradeoff between the
8
serocS
degarevA
serocS
degarevA
serocS
degarevA
serocS
degarevA
%
etaR
niW
tseT
%
etaR
niW
tseT
serocS
degarevA
serocS
degarevAIndividualContributionsasIntrinsicExplorationScaffoldsforMulti-agentReinforcementLearning
Figure9: Visualizationinthecounterattack_hardscenario. Wevisualizesomekeyframesoftrainedpolicies,withour
teaminyellow. Theyellowdashedarrowsdenotetheplayermovementwhilethebluearrowsdenotetheballmovement.
Ontheleftbottomcorners,wevisualizetheintrinsicscaffoldsoftheagentholdingtheball,andredbarsdenoteactions
encouragedbyintrinsicscaffolds.
exploration policy and exploitation policy, while β deter- MARLgoalofmaximizingextrinsicrewardswhileenjoying
minesthebalancebetweenrandomexplorationanddirected thebenefitofcooperativeexploration.
exploration. Overall,withinareasonablerange,ICESper-
This paper has two main limitations. First, the proposed
forms competitively across different hyperparameter set-
ICESrequiresadditionalpolicynetworks,whichintroduce
tings,showcasingitsrobustness. However,achievingopti-
extratrainingcomplexitycomparedwithQMIXorQPLEX.
malperformancerequirespropertuningbasedonthespe-
Reducingsuchcomplexitymightbeworthexploringwhen
cifictaskathand.
scalingICEStocaseswithmoreagents. Second,thispaper
onlyconsidersone-steplatentstatetransitions,whichmay
4.5.Visualization
beinsufficientasexplorationguidanceinmorecomplicated
Wevisualizethefinaltrainedpoliciesalongsidesomeintrin- scenarios. For future work, we aim to incorporate time
sicscaffoldsofthecounterattack_hardscenarioin abstractioninICEStofurtherimproveitsapplicability.
Figure9. Weobservethatonthefirstfewtimesteps,player
8,whopossessestheball,movestowardstheright,aiming Acknowledgements
toapproachitsteammatepalyer7. Atthesametime,one
ofthehighestrewardedactionsidentifiedbytheintrinsic Wethanktheanonymousreviewersfortheirvaluablefeed-
scaffoldsisshort_pass,whichisbeneficialbecauseplayer7 backandsuggestions.
isclosertothegoal. Consequently,guidedbythisintrinsic
scaffold,player8executesapasstoplayer7. Subsequently, ImpactStatement
player7receivesthepassandmakesashot,resultingina
goal. Notably,rightbeforethegoal,player7isencouraged Thispaperpresentsworkwhosegoalistoadvancethefield
toshootorsprint,bothofwhicharegoodactioncandidates. ofReinforcementLearning. Therearemanypotentialsoci-
Thisvisualizationresultshowcaseshowintrinsicscaffolds etalconsequencesofourwork,noneofwhichwefeelmust
serve as a guiding mechanism, directing agents towards bespecificallyhighlightedhere.
actionsthatarebothexploratoryandstrategicallysound.
References
5.Conclusions
Burda,Y.,Edwards,H.,Pathak,D.,Storkey,A.,Darrell,T.,
andEfros, A.A. Large-scalestudyofcuriosity-driven
In this work, we investigate MARL with sparse rewards.
learning. InProceedingsoftheInternationalConference
To facilitate cooperative exploration among agents with-
onLearningRepresentations,2018.
outtamperingthetrainingobjective,weproposeICES.Its
key idea is to use estimations of individual contributions
Chen, X., Liu, X., Zhang, S., Ding, B., and Li, K. Goal
toencourageagentstochooseactionsthathavemoresig-
consistency:Aneffectivemulti-agentcooperativemethod
nificantimpactonthelatentstatetransitionduringtraining
formultistagetasks. InRaedt,L.D.(ed.),Proceedings
time. ICESofferstwomainbenefits: Firstly,withtheindi-
ofthe31stInternationalJointConferenceonArtificial
vidualcontributionestimatedbyBayesiansurprise,ICES
Intelligence,pp.172–178.ijcai.org,2022.
directlyassignstheexplorationcreditstoindividualagents.
Thisapproachbypassestheneedforcreditassignmentof Chitnis, R., Tulsiani, S., Gupta, S., andGupta, A. Intrin-
globalintrinsicrewardsandalleviatesthenoisyTVproblem sicmotivationforencouragingsynergisticbehavior. In
brought by stochastic environment transitions. Secondly, ProceedingsoftheInternationalConferenceonLearning
withthedistinctalgorithmsandobjectivestooptimizeex- Representations,2019.
plorationandexploitationpolicies,ICESretainstheoriginal
Du,Y.,Han,L.,Fang,M.,Liu,J.,Dai,T.,andTao,D. Liir:
9IndividualContributionsasIntrinsicExplorationScaffoldsforMulti-agentReinforcementLearning
Learningindividualintrinsicrewardinmulti-agentrein- Liu, I.-J., Jain, U., Yeh, R. A., and Schwing, A. Coop-
forcementlearning. InAdvancesinNeuralInformation erative exploration for multi-agent deep reinforcement
ProcessingSystems,volume32,2019. learning. InProceedingsoftheInternationalConference
onMachineLearning,pp.6826–6836.PMLR,2021.
Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., and
Whiteson, S. Counterfactual multi-agent policy gradi- Liu, Z., Zhu, Y., and Chen, C. NA2Q: Neural attention
ents. InProceedingsoftheAAAIConferenceonArtificial additivemodelforinterpretablemulti-agentq-learning.In
Intelligence,volume32,2018. ProceedingsoftheInternationalConferenceonMachine
Learning,pp.22539–22558.PMLR,2023b.
Hu, J., Wang, S., Jiang, S., and Wang, W. Rethinking
the implementation tricks and monotonicity constraint Lowe, R., Wu, Y. I., Tamar, A., Harb, J., Pieter Abbeel,
in cooperative multi-agent reinforcement learning. In O.,andMordatch,I. Multi-agentactor-criticformixed
Proceedingsofthe2ndBlogpostTrackatInternational cooperative-competitive environments. In Advances
ConferenceonLearningRepresentations,2023. in Neural Information Processing Systems, volume 30,
2017.
Itti, L.andBaldi, P. Bayesiansurpriseattractshumanat-
tention. InAdvancesinNeuralInformationProcessing
Ma,Z.,Wang,R.,Li,F.-F.,Bernstein,M.,andKrishna,R.
Systems,pp.547–554,2005.
Elign: Expectationalignmentasamulti-agentintrinsic
Jaques,N.,Lazaridou,A.,Hughes,E.,Gulcehre,C.,Ortega, reward. InAdvancesinNeuralInformationProcessing
P., Strouse, D., Leibo, J.Z., andDeFreitas, N. Social
Systems,volume35,pp.8304–8317,2022.
influence as intrinsic motivation for multi-agent deep
Mahajan,A.,Rashid,T.,Samvelyan,M.,andWhiteson,S.
reinforcement learning. In Proceedings of the Interna-
Maven: Multi-agentvariationalexploration. InAdvances
tionalConferenceonMachineLearning,pp.3040–3049.
in neural information processing systems, volume 32,
PMLR,2019.
2019.
Jeon, J., Kim, W., Jung, W., and Sung, Y. Maser: Multi-
Mazzaglia, P., Catal, O., Verbelen, T., and Dhoedt, B.
agent reinforcement learning with subgoals generated
Curiosity-drivenexplorationvialatentbayesiansurprise.
fromexperiencereplaybuffer. InProceedingsoftheIn-
InProceedingsoftheAAAIConferenceonArtificialIn-
ternationalConferenceonMachineLearning,pp.10041–
telligence,volume36,pp.7752–7760,2022.
10052.PMLR,2022.
McKee,K.R.,Gemp,I.,McWilliams,B.,Duèñez-Guzmán,
Jo,Y.,Lee,S.,Yeom,J.,andHan,S. FoX:Formation-aware
E.A.,Hughes,E.,andLeibo,J.Z. Socialdiversityand
exploration in multi-agent reinforcement learning. In
socialpreferencesinmixed-motivereinforcementlearn-
ProceedingsoftheAAAIConferenceonArtificialIntelli-
ing. InProceedingsofthe19thInternationalConference
gence,volume38,pp.12985–12994,2024.
onAutonomousAgentsandMultiAgentSystems,pp.869–
Kim,W.andSung,Y. Anadaptiveentropy-regularization 877,2020.
framework for multi-agent reinforcement learning. In
Oliehoek, F.A.andAmato, C. Aconciseintroductionto
ProceedingsoftheInternationalConferenceonMachine
decentralizedPOMDPs. Springer,2016.
Learning,pp.16829–16852.PMLR,2023.
Kurach,K.,Raichuk,A.,Stan´czyk,P.,Zaja˛c,M.,Bachem, Parker,J.,Nunes,E.,Godoy,J.,andGini,M. Exploiting
O.,Espeholt,L.,Riquelme,C.,Vincent,D.,Michalski, spatial locality and heterogeneity of agents for search
M.,Bousquet,O.,etal.Googleresearchfootball:Anovel andrescueteamwork. JournalofFieldRobotics,33(7):
reinforcementlearningenvironment. InProceedingsof 877–900,2016.
theAAAIconferenceonartificialintelligence,volume34,
Rashid, T., Samvelyan, M., De Witt, C. S., Farquhar, G.,
pp.4501–4510,2020.
Foerster,J.,andWhiteson,S. Monotonicvaluefunction
Li,C.,Wang,T.,Wu,C.,Zhao,Q.,Yang,J.,andZhang,C. factorisationfordeepmulti-agentreinforcementlearning.
Celebratingdiversityinsharedmulti-agentreinforcement TheJournalofMachineLearningResearch,21(1):7234–
learning. InAdvancesinNeuralInformationProcessing 7284,2020.
Systems,volume34,2021.
Samvelyan,M.,Rashid,T.,SchroederdeWitt,C.,Farquhar,
Liu, B., Pu, Z., Pan, Y., Yi, J., Liang, Y., and Zhang, D. G.,Nardelli,N.,Rudner,T.G.,Hung,C.-M.,Torr,P.H.,
Lazyagents: anewperspectiveonsolvingsparsereward Foerster,J.,andWhiteson,S. Thestarcraftmulti-agent
probleminmulti-agentreinforcementlearning. InPro- challenge. InProceedingsofthe18thInternationalCon-
ceedings of the International Conference on Machine ferenceonAutonomousAgentsandMultiAgentSystems,
Learning,pp.21937–21950.PMLR,2023a. pp.2186–2188,2019.
10IndividualContributionsasIntrinsicExplorationScaffoldsforMulti-agentReinforcementLearning
Schmidhuber, J. Formal theory of creativity, fun, and in- Xu, P., Zhang, J., Yin, Q., Yu, C., Yang, Y., and Huang,
trinsicmotivation(1990–2010). IEEETransactionson K. Subspace-awareexplorationforsparse-rewardmulti-
AutonomousMentalDevelopment,2(3):230–247,2010. agent tasks. In Proceedings of the AAAI Conference
onArtificialIntelligence,volume37,pp.11717–11725,
Seuken,S.andZilberstein,S. Improvedmemory-bounded 2023b.
dynamicprogrammingfordecentralizedpomdps. InPro-
ceedingsofthe23rdConferenceonUncertaintyinArtifi- Ying,W.andDayong,S. Multi-agentframeworkforthird
cialIntelligence,pp.344–351,2007. partylogisticsine-commerce. ExpertSystemswithAp-
plications,29(2):431–436,2005.
Sohn,K.,Lee,H.,andYan,X. Learningstructuredoutput
Yu, C., Velu, A., Vinitsky, E., Gao, J., Wang, Y., Bayen,
representationusingdeepconditionalgenerativemodels.
A., and Wu, Y. The surprising effectiveness of ppo in
InAdvancesinNeuralInformationProcessingSystems,
cooperativemulti-agentgames. InAdvancesinNeural
volume28,2015.
InformationProcessingSystems,volume35,pp.24611–
24624,2022.
Son, K., Kim, D., Kang, W.J., Hostallero, D.E., andYi,
Y. Qtran: Learningtofactorizewithtransformationfor Yuan, L., Zhang, Z., Li, L., Guan, C., and Yu, Y. A sur-
cooperativemulti-agentreinforcementlearning. InPro- vey of progress on cooperative multi-agent reinforce-
ceedings of the International Conference on Machine ment learning in open environment. arXiv preprint
Learning,pp.5887–5896.PMLR,2019. arXiv:2312.01058,2023a.
Sunehag,P.,Lever,G.,Gruslys,A.,Czarnecki,W.M.,Zam- Yuan,M.,Li,B.,Jin,X.,andZeng,W. Automaticintrin-
baldi,V.,Jaderberg,M.,Lanctot,M.,Sonnerat,N.,Leibo, sicrewardshapingforexplorationindeepreinforcement
J.Z.,Tuyls,K.,etal. Value-decompositionnetworksfor learning. InKrause, A., Brunskill, E., Cho, K., Engel-
cooperativemulti-agentlearningbasedonteamreward. hardt,B.,Sabato,S.,andScarlett,J.(eds.),Proceedingsof
InProceedingsofthe17thInternationalConferenceon theInternationalConferenceonMachineLearning,vol-
AutonomousAgentsandMultiAgentSystems,pp.2085– ume202ofProceedingsofMachineLearningResearch,
2087,2018. pp.40531–40554.PMLR,2023b.
Zhang, J., Zhang, Y., Zhang, X.S., Zang, Y., andCheng,
Swamy,G.,Reddy,S.,Levine,S.,andDragan,A.D. Scaled
J. Intrinsicactiontendencyconsistencyforcooperative
autonomy: Enabling human operators to control robot
multi-agentreinforcementlearning.InProceedingsofthe
fleets. In Proceedings of the 2020 IEEE International
AAAIConferenceonArtificialIntelligence, volume38,
ConferenceonRoboticsandAutomation,pp.5942–5948.
pp.17600–17608,2024.
IEEE,2020.
Zhang,S.,Cao,J.,Yuan,L.,Yu,Y.,andZhan,D.-C. Self-
Terry, J., Black, B., Grammel, N., Jayakumar, M., Hari,
motivatedmulti-agentexploration. InProceedingsofthe
A.,Sullivan,R.,Santos,L.S.,Dieffendahl,C.,Horsch,
2023 International Conference on Autonomous Agents
C.,Perez-Vicente,R.,etal. Pettingzoo: Gymformulti-
andMultiagentSystems,pp.476–484,2023.
agent reinforcement learning. In Advances in Neural
InformationProcessingSystems,volume34,pp.15032– Zheng, L., Chen, J., Wang, J., He, J., Hu, Y., Chen, Y.,
15043,2021. Fan, C., Gao, Y., and Zhang, C. Episodic multi-agent
reinforcementlearningwithcuriosity-drivenexploration.
Wang,J.,Ren,Z.,Liu,T.,Yu,Y.,andZhang,C. Qplex: Du- InAdvancesinNeuralInformationProcessingSystems,
plexduelingmulti-agentq-learning.InProceedingsofthe volume34,pp.3757–3769,2021.
InternationalConferenceonLearningRepresentations,
2020. Zhou,M.,Liu,Z.,Sui,P.,Li,Y.,andChung,Y.Y. Learning
implicitcreditassignmentforcooperativemulti-agentre-
Wang, T., Wang, J., Wu, Y., and Zhang, C. Influence- inforcementlearning. InAdvancesinNeuralInformation
based multi-agent exploration. In Proceedings of the ProcessingSystems,volume33,pp.11853–11864,2020.
InternationalConferenceonLearningRepresentations,
2019.
Xu, P., Zhang, J., and Huang, K. Exploration via joint
policydiversityforsparse-rewardmulti-agenttasks. In
Proceedingsofthe32ndInternationalJointConference
onArtificialIntelligence,pp.326–334,2023a.
11IndividualContributionsasIntrinsicExplorationScaffoldsforMulti-agentReinforcementLearning
A.ICESAlgorithmDetails
A.1.ICESExplorationPolicyGradient
HerewedetailthedeviationtoEquation(9). FromEquation(8),wehave:
J (ξ)=E [ri ]+βH(·|τi,s). (11)
i νi int
Then,weintroduceV (τi,s)asabaseline. SinceV (τi,s)isindependenttoν ,wecanrewritetheobjectiveas:
η η i
J (ξ)=E (cid:2) ri −V (τi,s)(cid:3) +βH(·|τi,s) (12)
i νi(ξ) int η
=E (cid:2) Ri (s,a)−V (τi,s)(cid:3) +βH(·|τi,s), (13)
s,τi∼d,νi(ξ) int η
wheredisthedistributionofτiands,andRi (·,·)istheintrinsicrewardfunction.
int
TakingthegradientofJ (ξ)withrespecttoξ,wehave:
i
∇ J (ξ)=∇ E (cid:2) Ri (s,a)−V (τi,s)(cid:3) +β∇ H(·|τi,s). (14)
ξ i ξ s,τi∼d,νi(ξ) int η ξ
Definer =E(cid:2) Ri (s,a)−V (τi,s)|s=s,a=a(cid:3) ,wecanrewritethegradientas:
sa int η
(cid:88) (cid:88)
∇ J (ξ)=∇ d(τi,s) ν (a|s)r +β∇ H(·|τi,s) (15)
ξ i ξ i,ξ sa ξ
τi,s a
(cid:88) (cid:88)
= d(τi,s) r ∇ ν (a|τi,s)+β∇ H(·|τi,s) (16)
sa ξ i,ξ ξ
τi,s a
=(cid:88) d(τi,s)(cid:88)
r ·ν
(a|τi,s)∇ ξν i,ξ(a|τi,s)
+β∇ H(·|τi,s) (17)
sa i,ξ ν (a|τi,s) ξ
i,ξ
τi,s a
(cid:88) (cid:88)
= d(τi,s) ν (a|τi,s)·r ∇ lnν (a|τi,s)+β∇ H(·|τi,s) (18)
i,ξ sa ξ i,ξ ξ
τi,s a
=E (cid:2)(cid:0) Ri (s,a)−V (τi,s)(cid:1) ·lnν (a|τi,s)(cid:3) +β∇ H(·|τi,s) (19)
s,τi∼d,νi(ξ) int η i,ξ ξ
=E (cid:2)(cid:0) Ri (s,a)−V (τi,s)−β(cid:1) ·lnν (a|τi,s)(cid:3) . (20)
s,τi∼d,νi(ξ) int η i,ξ
A.2.ICESTrainingDetails
WedetailthetrainingproceduresforthepolicynetworksandthescaffoldsnetworkmentionedinSection3.4withAlgorithm2
andAlgorithm3,respectively.
Algorithm2TrainPolicies: TrainingProcedureofICESPolicies(Section3.3)
1: Input: Scaffoldsparametersψ,ϕ,explorationnetworkparametersξ,η,exploitationnetworksparametersζ,replay
bufferD
2: Samplebatch∼D
3: ζ ←ζ−LearningRate·∇L(ζ) {▷Equation(7)}
4: fori=1,2,...,ndo
5: Calculateintrinsicscaffoldsr ti
,int
=D KL(cid:2) p ψ(z t+1|s t,u t)∥p ϕ(z t+1|s t,u−
t
i)(cid:3) {▷Equation(2)}
6: endfor
7: ξ
←ξ+LearningRate·(cid:80)n
i
∇J i(ξ) {▷Equation(8)}
8: η ←η−LearningRate·∇L(η) {▷Equation(10)}
9: Output: Updatedparametersξ,η,ζ
B.ExperimentDetails
B.1.EnvironmentalSettings
Google Research Football (GRF): We evaluate our proposed method ICES against baselines on three
GRF (Kurach et al., 2020) scenarios, namely academy_3_vs_1_with_keeper, academy_corner and
12IndividualContributionsasIntrinsicExplorationScaffoldsforMulti-agentReinforcementLearning
Algorithm3TrainScaffolds: TrainingProcedureofICESScaffolds(Section3.2)
1: Input: Scaffoldsparametersψ,ϕ,θ,replaybufferD
2: Samplebatch∼D
3: ψ ←ψ+LearningRate·∇J ψ(ψ,ϕ,θ)
4: ϕ←ϕ+LearningRate·∇J ϕ(ψ,ϕ,θ)
5: θ ←θ+LearningRate·∇J θ(ψ,ϕ,θ)
{▷Equation(3)}
6: Output: Updatedparametersψ,ϕ,θ
academy_counterattack_hard. The sparse reward settings are used for ICES, baselines, and ablations, where
therewardsareonlyobservedwhenscoringorlosingthegame(Lietal.,2021). Thedetailsoftherewardsettingisgivenin
Table1. Thisrewardstructurecallsforhighlevelsofcooperationamongagentsandisfurthercomplicatedbythestochastic
natureofopponents’policies. ForGRFtasks(Figures4(a)to4(c)),weplottheaveragescores(with1forwining,0foratie
and−1forlosing)oftestepisodeswithrespecttothetrainingtimesteps.
Table1: GRFrewards.
Event Reward
Ourteamscores +100
Opponentteamscores -1
Ourteamortheballreturnstoourhalf-court -1
StarCraftMulti-agentChallenge(SMAC): WefurtherassessourproposedmethodICESonfiveSMAC(Samvelyan
etal.,2019)scenarioswithsparserewardsettingsfollowingthepreviousworks(Jeonetal.,2022;Kim&Sung,2023).
Therewardsareonlygivenuponthedeathofunits(alliesorenemies),anddetailsarelistedinTable3. Weusefoureasy
tasksandonehardtaskforbenchmark,including3m,8m,2s3z,2s_vs_1scand5m_vs_6m,asspecifiedinTable2.
ForSMACtasks(Figures4(d)to4(h)),weplottheaveragewinrateoftestepisodesovertrainingtimesteps.
Table2: SMACchallenges.
Task AllyUnits EnemyUnits Type
3m 3Marines 3Marines homogeneous,symmetric
8m 8Marines 8Marines homogeneous,symmetric
2s3z 2Stalkers&3Zealots 2Stalkers&3Zealots heterogeneous,symmetric
2s_vs_1sc 2Stalkers 1SpineCrawler homogeneous,asymmetric
5m_vs_6m 5Marines 6Marines heterogeneous,asymmetric
B.2.ICESImplementationDetails
ForbothICESandbaselines,parametersharingamongagentsisadoptedtoimprovethesampleefficiencyaswellaslower
themodelcomplexity. ForICESspecifically,weremovetheϵ-greedyexplorationstrategyaftertheepsilonannealingtime,
sincetheexplorationpolicyisalreadyrandominitsnature.
ForGRF,weimplementICESbasedonthecodeframeworkPyMARL(Samvelyanetal.,2019). ForSMAC,weimplement
ICESbasedonthecodeframeworkPyMARL2(Huetal.,2023). Inthisenvironment,beforeagentsconductanymeaningful
exploration,theytendtoprolongtheepisodebyescapinginsteadofattackingtheenemies. Previousmethodsavoidthis
behaviorbynormalizingtheintrinsicrewardstobelessthanorequaltozero(Jeonetal.,2022;Joetal.,2024),whilewe
simplyadda−0.02steppenaltyasintrinsicrewards. ForbothGRFandSMACexperiments,defaulthyperparametersfrom
thecodeframeworksareused. ForICES-specifichyperparameters,welisttheminTable4. Inparticular,αannealsgradually
throughoutthetrainingprocess.
13IndividualContributionsasIntrinsicExplorationScaffoldsforMulti-agentReinforcementLearning
Table3: SMACrewards.
Event Reward
Allenemiesdie +200
Oneenemydies +10
Oneallydies -5
Table4: ICEShyperparameters.
Hyperparameter Benchmark Scenario Value
Actionembeddingdimension - - 4
Scaffoldslearningrate - - 0.0001
Scaffoldsgradientclipping - - 0.1
GRF - 0.001
Explorationagentlearningrate
SMAC - 0.01
academy_3_vs_1_with_keeper 0.2-0.05
GRF academy_corner 0.2-0.05
α academy_counterattack_hard 0.1-0.05
5m_vs_6m 0.1-0.05
SMAC
others 0.1-0.1
academy_3_vs_1_with_keeper 0.02
GRF academy_corner 0.05
β academy_counterattack_hard 0.05
5m_vs_6m 0.5
SMAC
others 0.1
B.3.Infrastructure
ExperimentsarecarriedoutonNVIDIAGeForceRTX3080GPUs.
C.AdditionalExperimentalResults
C.1.ICESonKAZ
SinceICESdonotrequireparticularparsingofstates,itcanbeeasilygeneralizedtopixel-basedMARLtasks. Therefore,we
furthertestICESonapixel-basedMARLbenchmarktaskknights_archers_zombies(KAZ)fromthepettingzoo
environments(Terryetal.,2021)withtheresultsshowninFigure10. Wecanseethatinpixel-basedMARLtasks,ICESis
alsoabletoimprovetheperformancebypromotingcooperativeexploration.
KnightsArchersZombies(KAZ)Environment: Inthisgame,wecontrol4agents(2knightsand2archers)withthegoal
tokillallzombiesthatappearonthescreen. Eachagentcanmoveandattacktokillzombies. Whenaknightattacks,it
swingsamaceinanarcinfrontofitscurrentheadingdirection. Whenanarcherattacks,itfiresanarrowinastraightlinein
thedirectionofthearcher’sheading. Werewardtheagentswith+1whenazombiedies.
InputsPreprocessing: Weusethepixel-basedlocalandglobalobaservationinthisenvironmentforICESandQMIX.
In particular, the global observation is represented by a 720×1280×3 pixel colored image, while the observation of
eachagentisrepresentedasa512×512×3pixelcoloredimagearoundtheagent. Astheinputspaceistoolargefor
RLalgorithmstolearnefficiently,weadoptsomenecessarypreprocessingtolocalandglobalobservationtoreducethe
dimensionofinputspace.
Thepreprocessingpipelineforlocalobservationisasfollows:ColorReduction(Onlytakethefirstcolorchannelanddiscard
14IndividualContributionsasIntrinsicExplorationScaffoldsforMulti-agentReinforcementLearning
QMIX
ICES
4
2
0
0 1 2 3 4
Timesteps 1e6
Figure10: PerformancecomparisononKAZbenchmark.
therest)→Resizeto64×64. Thepreprocessingpipelinefortheglobalobservationisasfollows: ColorReduction→Crop
thecentral720×1100pixels→Resizeto128×128.
NetworkArchitecture: WeusethecodeframeworkPyMARL2(Huetal.,2023)forthisexperiment. Sincetheinputsare
imagesratherthanvectors,weaddfeatureencodingblocksintheagentnetworkandthemixinghypernetworktoprocess
theinputimages. DetailsareprovidedinTable5andTable6,respectively. RNNlayersarenotusedforthisexperimentto
avoidextensiveGPUmemoryconsumption.
Table5: Observationencodingblocksinagentnetwork.
Layer Operator #Channels
1 Conv3×3&MaxPooling 32
2 Conv3×3&MaxPooling 64
3 Conv3×3&MaxPooling 128
4 Flatten&FC 128
Table6: Observationencodingblocksinmixinghypernetwork.
Layer Operator #Channels
1 Conv5×5&MaxPooling 32
2 Conv5×5&MaxPooling 64
3 Conv5×5&MaxPooling 128
4 Flatten&FC 128
For ICES, we use the same QMIX network for value functions and the CVAEs with network structure specified in
Table7andTable8. WedesignthenetworkarchitectureforCVAEherebasedonhttps://github.com/AntixK/
PyTorch-VAE.
Hyperparameters: WeusethedefaulthyperparametersinPyMARL2exceptforbatch_size = 4forbothQMIXand
ICES.ForICESspecifichyperparameters,welisttheminTable9.
15
sdraweR
degarevAIndividualContributionsasIntrinsicExplorationScaffoldsforMulti-agentReinforcementLearning
Table7: CVAEencoderinICES.
Layer Operator #Channels
1 Conv5×5 8
2 Conv5×5 16
3 Conv5×5 32
4 Conv3×3 64
5 Conv3×3 128
6 Flatten&FC 128
7 FC×2 64
Table8: CVAEdecoderinICES.
Layer Operator #Channels
1 FC×2 64
2 TransposedConv5×5 64
3 TransposedConv5×5 32
4 TransposedConv5×5 16
5 TransposedConv5×5 8
6 TransposedConv7×7 2
Table9: ICEShyperparametersusedinKAZexperiment.
Hyperparameter Value
Actionembeddingdimension 4
Scaffoldslearningrate 0.0001
Scaffoldsgradientclipping 0.1
Explorationagentlearningrate 0.001
α 0.1-0.05
β 0.1
16