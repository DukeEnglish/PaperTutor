An Open-Source Reproducible Chess Robot for Human-
Robot Interaction Research
May 2024
Renchi Zhang, Joost de Winter*, Dimitra Dodou, Harleigh Seyffert, Yke Bauke Eisma
Faculty of Mechanical Engineering, Delft University of Technology, Delft, Netherlands
*Corresponding author: j.c.f.dewinter@tudelft.nl
Abstract
Recent advancements in AI have sped up the evolution of versatile robot designs. Chess
provides a standardized environment that allows for the evaluation of the influence of robot
behaviors on human behavior. This article presents an open-source chess robot for human-
robot interaction (HRI) research, specifically focusing on verbal and non-verbal interactions.
OpenChessRobot recognizes chess pieces using computer vision, executes moves, and
interacts with the human player using voice and robotic gestures. We detail the software
design, provide quantitative evaluations of the robot's efficacy and offer a guide for its
reproducibility. The code and datasets are accessible on GitHub:
https://github.com/renchizhhhh/OpenChessRobot
Keywords: Artificial Intelligence, Chess, Human-robot Interaction, Open-source, Transfer
Learning
1. Introduction
Robots are becoming increasingly common across a variety of traditionally human-controlled
domains. Examples range from automated mowers that maintain community lawns to robots
in assembly lines and agricultural settings. Recent scientific advancements in AI have enabled
new opportunities for intelligent sensing, reasoning, and acting by robots. In particular, the
rapid development of large language models, such as ChatGPT, and vision-language models,
have lowered the barrier of human-to-robot communication by being able to transform text and
images into interpretable actions or vice versa.
As technology advances, it is likely that robots will attain greater capabilities and will be able
to tackle tasks previously within the exclusive realm of human expertise. This ongoing
evolution may also lead to closer and more productive interactions between humans and
robots. At the same time, integrating different AI-based robotic components remains a
challenge, and the human-robot interaction (HRI) field lags in terms of endorsing
reproducibility principles (Gunes et al., 2022). Encouraging transparent and reproducible
research, therefore, remains an ongoing task.
The use of chess as a testbed for evaluating the effect of technology on human perception
and behavior traces back to the 18th-century chess automaton Mechanical Turk (Standage,
2002). Furthermore, chess has played an important role in advancing the field of AI, starting
with Claude Shannon’s chess-playing algorithm (Shannon, 1950) to the success of IBM’s
Deep Blue (Campbell et al., 2002) and DeepMind’s self-play learning algorithm (Silver et al.,
2018). In this paper, we incorporate modern AI algorithms into the design of a chess-playing
robot to be used for studying HRI. HRI research may benefit from a chess-based setup
because the game of chess provides a controlled rule-based environment in which the impact
of robots on human players can be precisely measured.HRI-oriented studies with chess robots have typically employed chess robots to imitate human
behaviors and assess the resulting impact on the human opponent. Pereira et al. (2008) and
Leite et al. (2013) used the social robot iCat to play chess with children. This robot relies on
an electronic chess board as input and emits emotional responses and verbal utterances,
guided by an emotion model. Sajó et al. (2011) developed Turk-2, a multimodal chess robot
with human-like communication skills, while LC et al. (2021) explored human-robot and robot-
robot-human interaction using artistic intervention, where expressive robot arms played chess
and embodied distinct personalities.
When an electronic chessboard is not used, a camera is needed to determine if a move has
been made and/or what move it was. A common solution is a monocular top-view camera,
which has the advantage of not experiencing perspective-induced occlusion, making it
relatively easy to identify any changes in the chess position (Golz & Biesenbach, 2015;
Kołosowski et al., 2020; Larregay et al., 2018; Luqman & Zaffar, 2016; Srivatsan et al., 2020;
Thanh Tra et al., 2018). An exception to this concept is the Gambit chess robot by Matuszek
et al. (2011), which does not require a top view but uses a stereo camera instead. This robot
was unique because it classified the pieces by type and color, whereas other robots identify
and track move-by-move changes from the starting position (e.g., Golz & Biesenbach, 2015;
Kołosowski et al., 2020; Larregay et al., 2018; Siraj, 2017).
Other research focuses on the development of computer-vision methods for the detection of
the chessboard, and the subsequent classification of the pieces on the chessboard, without
always linking these computer-vision methods to a chess robot (Christie et al., 2017;
Czyżewski et al., 2020; Ding, 2016; Koray & Sümer, 2016; Ranasinghe et al., 2023; Schwenk
& Yuan, 2015; Xie et al., 2018). The detection of the chessboard typically involves line- or
edge-detection techniques (Chen & Wang, 2019; Czyżewski et al., 2020; Srivatsan et al.,
2020; Wölflein & Arandjelović, 2021; Xie et al., 2018), while the classification of chess pieces
commonly uses convolutional neural networks (CNN) (Mallasénn Quintana et al., 2020; Shin
et al., 2023; Wölflein & Arandjelović, 2021). Challenges in both cases lie in testing for
robustness under various conditions. A common technique to improve classification
performance is to verify if the move/position is legal or plausible according to a chess engine.
With such methods, it is possible to let the algorithm home in on the most probable
classification outcome (e.g., Czyżewski et al., 2020; Mallasénn Quintana et al., 2020;
Ranasinghe et al., 2023). Currently, a state-of-the-art method is that of Wölflein and
Arandjelović (2021), which used a diverse set of neural network models and achieved a per-
square classification accuracy of 99.8%.
Our contribution to the development of the chess robot centers on a software platform that
can be reproduced by others by using open-source robotics middleware. The chosen robot
arm is widely available in research, while other hardware components can be substituted if
required. The OpenChessRobot can detect pieces using computer vision and execute moves.
For verbal interaction, we rely on large language models. Specifically, by connecting ChatGPT
to the chess engine output, our robot interprets strategies and explains them to humans in
natural language. Non-verbally, the robot is able to express its evaluation of the game by either
nodding or shaking.
This paper first presents the hardware and software design of the chess robot. It then provides
an outline of its core modulus, which include Perception, Analysis & Evaluation, Motion
Planning & Execution, and Interaction. Module-specific experiments for performance
evaluation are presented as well. The paper concludes with a discussion of the limitations of
our robot and future research opportunities in HRI.2. Hardware and Software Architecture
Our chess-playing robot consists of the following hardware components (Figure 1):
• a Franka Emika Panda robot arm (Franka Emika, 2020) equipped with a Franka Hand
and a customized 3D-printed robot gripper. The robot arm has 7 degrees of freedom
and is a widely used Collaborative Robot, suited for safe human interaction. It operates
on v5.4.0 firmware.
• a ZED2 StereoLabs camera (StereoLabs, 2020).
• a keyboard.
• an external microphone and a speaker, connected to the PC.
• an NVIDIA Jetson Nano (NVIDIA, 2019).
• a Linux PC with Ubuntu 20.04, with a real-time kernel. It is equipped with an Intel I7-
8700K processor and an NVIDIA GTX2080 graphics card.
ZED2 stereo camera
Franka Emika Panda
(FEP) robotic arm
5-key keyboard
Nvidia Jetson Nano
Figure 1. The system at a glance. The whole system consists of a Franka Emika Panda robot
arm, a ZED2 stereo camera, and an NVIDIA Jetson Nano computing board.
Our setup includes a number of accessories. These comprise a chessboard (48 × 48 cm), a
set of chess pieces (model: Staunton No. 4; ranging in height from 4 to 7.8 cm, and ranging
in width from 2 to 4 cm), and four printed ArUco markers. Considering the robot arm’s range,
the chessboard should be positioned so that the farthest rank remains within 65 cm of the
robot. In Figure 1, the distance between the closest edge of the board and the robot is 16 cm.
The robot arm, the Jetson Nano, and the Linux PC connect to a local network via a router.
Communication between the arm and the control PC is realized through the Franka Control
Interface (Franka Robotics GmbH, 2023) by integrating the Libfranka (0.9.2) library for low-
level control and the franka_ros package for the Robot Operating System (ROS) (Quigley etal., 2009). The ZED2 camera is attached to the Franka Hand via a custom mount, and it
interfaces with the NVIDIA Jetson Nano through a USB connection. The Jetson Nano streams
the camera view to the PC through ZED SDK v3.5. Considering that our setup uses images
from a single camera, the ZED2 camera and Jetson Nano can be substituted with more
economical monocular alternatives.
The software architecture of the OpenChessRobot is built upon ROS Noetic (Open Robotics,
2020), which offers common robotics data formats and message-passing among software
modules. Figure 2 provides an overview of the software architecture, divided into four
modules: Perception, Analysis & Evaluation, Motion Planning & Execution, and Interaction.
The Perception module uses the ZED2 camera SDK to capture the chess game images and
relies on a neural-network based chess identifier to translate the game images into text
descriptions. The latter identifier requires PyTorch and CUDA runtime. The Analysis &
Evaluation module feeds the annotation of the chess game to a chess engine in order to get
predicted moves and their corresponding scores. In the Motion Planning & Execution module,
the predicted move, accompanied by the 3D chessboard localization results from the
Perception module, is used by the MoveIt! motion planner (Coleman et al., 2014) to plan and
execute chess moves.
The Interaction module takes the human inputs and manages the OpenChessRobot’s verbal
and non-verbal feedback. For the verbal feedback, a prompt wrapper, which combines the
user request and outputs from the Analysis & Evaluation module, serves as a client of
OpenAI’s ChatGPT API service and generates the responses to talk with the human.
Interaction
Initial Image Loca3 lD iz- ation C Lh oe cs as ti oG nr sid M Ma ok vi en sg
Verbal
Trained Piece Raw Game Feedback
Follow-up Detector Position
Images
Audio Play
Perception Speech
Engine
Wrapper Predicted Chess Move
Moves Encoder Planner
User Speech
Game request Recognition
Evaluation
Robot Posture
Chess Manager
Engine Processed
Position Prompt Wrapper & Text
OpenAI Service Output
Analysis & Evaluation Motion Planning & Execution
Communication via
Figure 2. The software architecture of four modules and communications between modules.
Within the Perception module, the camera images are used to determine the 3D positions of
the squares on the chessboard and identify individual pieces. The identified game position is
analyzed through the chess engine wrapper, yielding the best next move and an evaluation of
the current board position. The OpenChessRobot then enacts the chess move. Additionally,
the robot has the capability to respond through posture and speech.
3. Modules
The Perception module uses computer vision to identify the chessboard and the pieces; it can
distinguish between occupied and empty squares and recognize pieces and their color. The
perceived game position is sent to the Analysis & Evaluation module, which interfaces with a
chess engine to determine the best move. The Perception module also calculates the chesssquares’ real-world locations, which are sent to the motion planner for planning the robot’s
end-effector trajectory to execute chess moves provided by the Analysis & Evaluation module.
The OpenChessRobot runs a data collection pipeline (orange box in Figure 3) and a human-
robot gameplay pipeline (yellow box in Figure 3). The data collection pipeline is used to collect
real-world data of chess pieces for retraining the Perception module and adapting to a new
chessboard. The basic gameplay pipeline allows the robot to play the chess game with a
human from any game position.
Change pose All markers Yes 3D localization of Change pose Is data No
Start the robot to Hovering visible? the board grid to Ready collection?
No
Yes
Ask to adjust the board
Load piece
Generate a Executes a Change pose Take an image
settings on key
move list chess move to Ready & store the FEN
squares
Load chess
games No Is the list Yes
Store the data
traversed?
Whose turn? Robot rG eca om ge n ip tio os ni t aio nn d Is the output Yes Evaluate the FEN& Execute a Change pose
move validation valid? predict the move chess move to Ready
Human No
Warn the user
Wait for human
move and
confirmation
Figure 3. Flow chart of the OpenChessRobot execution using the core modules. The robot
uses computer vision to recognize the chessboard and pieces, engages with a chess engine
to determine moves, and executes chess moves for gameplay or data gathering. The chart
splits into two key workflows: one for collecting data to improve the robot’s perception of
different chessboards and chess pieces, and another for playing chess with a human.
3.1. Perception
The ability to perceive the chess pieces is a key function of the chess robot. While the
commonly used top view resolves the issue of piece occlusion, its practicality is limited
because of the limited perspective on the pieces. Adopting lower camera angles and dynamic
camera positions to observe the game supports the robot’s natural interaction with the human
player (see Figure 1). Additionally, our approach relies on RGB images captured by a single
camera rather than relying on point clouds (Matuszek et al., 2011), improving the simplicity
and reproducibility of our setup.
Figure 4 shows the Perception module of the OpenChessRobot. The module consists of two
distinct classifiers, one for occupancy and the other for piece classification.
3.1.1 3D chessboard localization
To manipulate the chess pieces, acquiring their 3D coordinates in the real world is needed.
Four ArUco markers are used as indicators of the 3D positions of the chessboard. An image
containing these markers, along with the marker length and the camera’s intrinsic and extrinsic
parameters and distortion coefficients, enables the computation of marker-to-camera
translation and rotation using OpenCV.
notiazilatiinI
noticelloC
ataD
yalP
emaGTraining and Inference of the Chess Perception Module
Data Collection Training/Fine-turning Inference
Simulation Occupancy
Classifier
Synthetic
Dataset ResNet 34
5,000
game states Preprocessing Piece
in FENs Classifier
Images from Isaac Sim Real-world Inception New game image
Dataset Net
Game
records Real-world
Raw
prediction
Piece Game
settings positions Change robot poses
on key in FEN
squares No
Legality check
Autonomous replay Real Images with FENs
Yes
Board 3D localization Valid Game analysis
Marker 3D Non-linear least Chess grid FEN
localization squares optimization localization
Figure 4. The Perception module of the OpenChessRobot relies on two distinct classifiers,
one for occupancy and the other for piece classification. To train these classifiers, we
synthesized chess positions in the NVIDIA Isaac Simulator (Makoviychuk et al., 2021) based
on previous chess games and assign ground truth labels, including the game positions
represented in Forsyth–Edwards Notation (FEN) and pixel coordinates of board corners. In
the real-world setup, the robot collects game images from the ready camera pose (Figure 3)
after autonomously placing pieces on key squares and replaying given games. These real-
world images are then used to fine-tune the classifiers that were initially trained on synthetic
datasets.
The board 3D localization is activated when the OpenChessRobot is initialized. The robot
moves to a hovering pose to observe the chessboard from above (gray box in Figure 3).
Although the robot works with low camera positions most of the time during the game, a
vertical camera angle is used for 3D chessboard localization to reduce errors caused by image
distortion. Then, a grid is formed for the chess squares using the actual square size and a
non-linear least squares algorithm is used to optimize the grid corner positions to be close to
the marker positions.
3.1.2 Chess piece detection
In the preceding section, we explained how the 3D coordinates of the squares are estimated.
The next task is to identify the chess pieces on each square.
The piece detection model is an extension of a CNN-based model from Wölflein and
Arandjelović (2021). We complemented their method by using a different synthetic dataset as
well as a real-world dataset to fine-tune the pretrained models in order to effectively handle
our real-world chess set.
To synthesize images of chess pieces on a board, we varied backgrounds, lighting conditions,
camera poses, and piece locations. A total of 5,000 game positions, randomly selected from
games played by grandmaster Bobby Fischer, were used. We synthetic images of selected
chess games using the NVIDIA Isaac Simulator (Makoviychuk et al., 2021). Ground truth
labels were created, consisting of the game positions represented in Forsyth-Edwards
Notation (FEN) and the positions of the board corners. Following a similar approach to
previous works (Mallasénn Quintana et al., 2020; Matuszek et al., 2011; Wölflein &Arandjelović, 2021), the images were cropped into sub-images, each corresponding to a
square on the chessboard. Two categories, “empty” or “occupied” were assigned for
occupancy classification, while 12 categories (representing the six types of chess pieces in
both black and white) were designated for piece classification.
To train the occupancy and piece classifiers with the synthetic data, 80% of the cropped
images were used as the training set, while the remaining 20% was evenly split between a
test set and a validation set. A ResNet-34 (He et al., 2016) and an InceptionV3 (Szegedy et
al., 2015) were trained independently on these two respective training datasets. This is
illustrated in the left portion of Figure 4.
To adapt to real world chess, a piece-square dataset was created by manually iterating all the
chess pieces on the board. During the iteration, the game positions were recorded in both
FEN notation and in images taken at 1080p resolution from the perspective of the robot in the
ready pose (Figure 3). A number of training datasets to fine-tune the piece classifier were
extracted from the piece-square dataset by using sub-images of chess pieces on key squares.
The trained models were evaluated on the remaining portion of the piece-square dataset.
Additionally, to evaluate the classifiers’ performance in games, a game dataset was collected
by the robot autonomously replaying the provided chess games. Two games played by the
grandmaster Michael Adams were used, with the game positions recorded both in FEN and
in images, following the same procedure used when collecting the piece-square set. Dataset
creation, training settings and evaluation are explained in more detail in the Section 5.1 and
Appendix B in the supplementary material.
The preceding chess detection model considered only the position of the chess game. It does
not verify the legality of the observed game position and does not possess knowledge of the
previous game position. Such information can potentially improve the accuracy of the
Perception module. Our legality check consists of two steps: first, it verifies if the game position
conflicts with the chess rules, such as detecting no king or too many pawns. If the game
position is invalid, it requests a new image–using the same camera pose but with potentially
better camera exposure–and reanalyzes it. The robot can also change the camera angle to
improve perception. More specifically, in case the predicted game states are detected as
illegal, the robot slightly moves to its side in order to capture an image from a different
perspective to improve the occupancy and piece classification.
In the second step of the legality check, the robot examines if the recognized chess move is
among the legal moves from the prior game position. When the inferred game position and
chess move prove to be legitimate, the robot adopts the inference and proceeds with
gameplay. If not, the robot will halt its operation and wait for manual correction by the operator.
3.2. Analysis & Evaluation
The processed chess FEN is forwarded to a chess engine wrapper, which uses the Universal
Chess Interface (UCI) protocol for interaction with standard engines. Our chess engine
wrapper builds upon an existing open-source Python wrapper for Stockfish (Zhelyabuzhsky,
2022). Our system integrates Stockfish 15 (Stockfish, 2022) as the default engine, using a
total of 10 CPU threads. The chess engine assigns scores to candidate moves, which are
monotonically related to the player’s win rate, should the engine play against an equally strong
opponent.
3.3. Motion Planning & Execution
For the planning of the OpenChessRobot’s motions, we rely on the MoveIt! motion planner
(Coleman et al., 2014). To determine the joint configurations, an inverse kinematics solver
named IKFast is used (MoveIt, 2021). This solver calculates suitable robot joint angles based
on the specified end-effector coordinates with respect to the robot’s base.The robot arm is equipped with a customized gripper (Figure 5), designed to secure the
underside of the piece, which is always cylindrical. The gripper is designed to be tolerant to
minor deviations between the anticipated pickup positions (square centers) and the actual
positions. Consequently, the human (or robotic) chess player is not bound to place a piece
exactly at the center of a square.
Figure 5. The customized 3D-printed gripper. This gripper has two fingers that can deal with
deviations of chess pieces if they are not in the center of a square.
There are two basic robot motions: a sliding motion (Figure 6, left) and a jumping motion
(Figure 6, right). If the path between the start and end position of a movement is unobstructed,
the robot grasps the piece and pushes it to the destination square. When the path is
obstructed, the robot raises the piece to leap over other pieces. Special movements like
capture, castling, and en passant are manually programmed as combinations of the two basic
motions.
Figure 6. The robot is executing planned trajectories to move chess pieces. Left: The robot
slides a pawn from the E2 square to the E4 square. Right: The robot lifts the knight on G1 to
jump over other pieces on its way.A chess move for the robot to execute is provided by the Analysis & Evaluation module in an
encoded text format, e.g., g1f310000. The first 4 characters indicate the chess move. Extra
information for a move is stored in the remaining characters. Specifically, the fifth to ninth
characters stand for whether a move is a jump, capture, castling, en passant, or promotion
move.
With the encoded string we create waypoints, using the 3D locations of the start square, the
destination square, as well as middle states for special motions. The middle states are
generated when the robot needs to lift pieces first rather than slide them on the board. The
motion planner receives waypoints and plans the trajectory of the end-effector using Rapidly-
exploring Random Trees (RRTConnect; Kuffner & LaValle, 2000), with a 3D box surrounding
the chessboard as a workspace constraint. After trajectory planning, velocity/acceleration
limits are enforced to the planned trajectory (Kunz & Stilman, 2013).
4. Interactive Modules
As the chess robot aims to be used for human-robot interaction research, providing different
modalities for human-robot communication is important. We implemented an interactive
gameplay pipeline based on the core modules, as represented in Figure 7. Specifically, based
on the human chess performance, the OpenChessRobot can move its head to express its
evaluation, and it can verbally express information about move qualities.
Change pose All markers Yes 3D localization of Change pose
Start the robot
to Hovering visible? the board grid to Ready
No
Ask to adjust the board
Whose turn? Robot rG eca om ge n ip tio os ni t aio nn d Is the output Yes E mv oa vlu ea &te pth ree d h icu t m tha en Executes a
valid?
move validation next move chess move
Human No Warn the user
thH eu rm oba on
t
g ca az me
e
o ran
?
Yes E mx oe tc iou nte A g mo oo vd e/b ?ad
feedback
No
Change pose
Human asking Yes Verbal feedback to Ready
questions? on the speaker
Execute
No motion
feedback
Wait for human
move and
confirmation
Figure 7. Flow chart of the OpenChessRobot execution using the core modules and Interactive
module. As an extension to core modules, the robot has an interactive gameplay pipeline. It
evaluates human moves using predefined criteria and provides feedback by adjusting its
posture. The system is also programmed to interpret human behaviors, such as gazing or
asking questions, to provide a responsive and engaging experience.
4.1. Verbal Interaction
In typical chess games, verbal communication does not hold a central role, although some
professional players explained the rationale and inner thoughts of chess moves in “Banter
Blitz”. Additionally, chess trainers commonly use verbal feedback to train their students.
noitazilaitinI
yalP
emaG
evitcaretnIOpenAI's ChatGPT has proven to be capable of generating human-like dialogues. However,
ChatGPT is unable to properly play chess on its own (Kuo et al., 2023). Our proposed solution
relies on combining the analysis from the chess engine with ChatGPT without model fine-
tuning.
Depending on the human player’s request, the OpenChessRobot can explain the last move
or the next move. The process is depicted in Figure 8. Capturing the user’s request is achieved
through voice recognition. To trigger the voice interaction, the robot should be in the ready
pose and waiting for the user’s move and before the player has made their move. The player
must vocalize specific keywords of “explain the (last) move” or “analyze/predict the (next)
move”.
System message: Observe Chess Board
Can you You are a helpful chess assistant. Your role is to help the user
explain why understand the reaso inn cs lb ue dh inin gd … …chess opening moves,
you made the
last move? User request in keywords:
“explain the last move”
Context input (Json): Detected Game in FEN:
“m”e ov va elu ”a :”ti co 5n ””
,
:
“
F”F Ea Nir ””
:
,
…
1rb2rk1/pp I3 npp f1 e/4 rp rq e1p d/8 L/1 aPP s1 tB 2 MP/P o5P v1 e/2 :Q c2P 52/3RK2R w K
“history”: …., “prediction: ….,
Chess Analysis and Evaluation:
CP: +1.2
Input audio: Output audio: Evaluation: Fair
History moves: c4 Qf6 …
from human synthetic Compact service request: Predicted moves: c5 Rb8 g3 b6 c6 Ba6….
System message + User request + Context input
User request Text stream
in keywords from OpenAI
Server Response in Streaming:
“The move c5”, Blue: information from the user
“will pave the way for potential expansion with c6, ” Green: information from the robot modules
…..
Golden: prompts for GPT and results from GPT
Figure 8. Verbal interaction. In this diagram, the user asks for an explanation of the last move.
Then a prompt consisting of a system message, user request, and processed game position
is provided to the ChatGPT API to generate the verbal feedback.
When explaining the last move, the move history and the current game FEN are sent to the
Analysis & Evaluation module. Alternatively, when the user seeks advice on next moves, the
system uses the chess engine to predict several top-ranked moves, along with their game
continuations. A wrapper integrates global instructions with the chess game information into a
prompt ready for submission to the OpenAI service. The global instructions, which remain the
same among different requests, are embedded in a system message (Table 1).
Table 1. Prompt and GPT-4 output to explain the last move (bishop c4)
Prompt:
System You are a helpful chess assistant. Your role is to help the user understand the
message reasons behind chess opening moves, including their benefits, weaknesses, and
overall strategic concepts. There are some instructions on how to respond to the
user. Instruction 1: You should always start your response with a short sentence or
few words to acknowledge that you've received the user's message and are
considering it. Instruction 2: The user prefers concise explanations, so please keep
your responses brief. Instruction 3: Your response should be informative and fluid,
avoiding bullet points or index numbers. Instruction 4: When analyzing a board,
mention one key reason for a move. Try to use the given history moves and
predicted future moves. The input will include a user question, a chess FEN for the
current game, the move to make, predicted future moves and a move history in a
dictionary format under the keys 'question', 'fen', 'move', 'evaluation', 'future' and
'history'.
Input:User {'question':'can you explain?',
message 'fen':'r1bqkbnr/pppp1ppp/2n5/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2
3', 'move':'Bc4', 'evaluation':'Excellent', \ 'future':'Bc4 Bc5 d3 Nf6 O-O d6 c3 O-O h3
h6 ',\ 'history': '1. e4 e5 2. Nf3 Nc6'}
GPT-4-0613:
Sure, I can assist with that. The move Bc4 in this position is known as the Italian Game. The
aim of this move is to control the center quickly with your pawn and knight and then put your
bishop on its most dangerous square. After this, both white and black usually castle and then
start a pawn storm against the opponent’s king. In this case, future moves predict a traditional
development of pieces with both sides castling kingside. The main goal is to have a solid
position and try to exploit tiny advantages.
With the prompt, the OpenAI service is requested to provide an analysis within the specified
chess context in a tutor-like tone. To reduce the latency in vocalizing the text responses from
the server while maintaining a natural flow in the synthetic voice, the feedback from the server
streaming in words/characters is organized in short sentences. A queue for sentences to play
sequentially is implemented.
4.2. Non-Verbal Interaction
In human-human communication, non-verbal cues play an important role. In professional
chess games, verbal exchanges are often sparse. Apart from the moves played, participants
may rely on behavioral cues to understand their opponents.
Posture feedback based on game evaluation
The posture feedback consists of basic nodding and shaking motions executed by the robot’s
hand (Figure 9). These gestures are triggered based on the chess engine’s evaluation.
Specifically, the robot enacts nodding or shaking gestures when the reduction in win
probability due to the latest move exceeds a predefined threshold.
Figure 9. Posture feedback of the robot nodding and shaking head after an evaluated good or
bad chess move, respectively.5. Experimental Evaluation
We performed the following assessments of the chess robot system: (1) Perception module:
accuracies of the piece classifiers, (2) Motion Planning & Execution module: grasping a piece
across different sections of a square.
5.1. Chess Perception
For the piece classifier of the chess perception module, several training settings were tested, using
two sets of key squares, different components of the training dataset, and varying training lengths.
The challenge was to identify a training setting that minimizes the number of training samples while
achieving good generalization performance.
The two sets of key squares were:
• squares, which include intersections between the key files A, E, H and the key ranks
1, 4, 8
• 3×3 squares, which contain intersections between the key files A, E, D, H and the key ranks
1, 4, 5, 8.
4×4
The piece-square dataset contains three subsets: default, rotated, and shifted:
• The default dataset (D) contains sub-images of 768 unique piece-square pairs (64 squares
12 pieces), with each pair occurring once.
• The rotated dataset (R) includes sub-images of four types of pieces: white king, white knight,
×black king and black knight. Kings were rotated twice by 45 degrees (64 squares 2
rotations), while knights were rotated four times (64 squares 4 rotations) to create different
shapes due to their rotational asymmetry (See Figure 10 right). Sixty-four samples of e×ach
rotated piece were randomly selected, forming the final rotat×ed dataset.
• The shifted dataset (S) contained sub-images with cropping windows shifted in four directions
(up, down, left, right). The displacement in each direction was ¼ square in length. For each
shift direction, all 768 unique piece-square pairs were extracted. The S dataset serves as a
test set with increased noise.
Additionally, the data recorded by the OpenChessRobot while replaying the games was used as
another test set (G) to evaluate the piece classifiers trained with piece images on the key squares.
Only unique piece-square pairs in each game are extracted, making up the G dataset.
Black knight at G7
default Rotated (135˚) Shifted (right)
White king at E5
default Rotated (90˚) Shifted (right)
Figure 10. The raw images to create the piece-square dataset. The upper half image is taken
without any rotations and the lower half image is taken with 135-degree piece rotations.Different training and test dataset compositions were extracted from these three datasets.
• The training set (d) contained sub-images of each piece on the key squares. The remaining
samples in the D dataset are divided into a validation set and a test set with a ratio of 20:80.
• The training set (r) was generated from the R dataset using pieces on the key squares with
the 90-degree rotation.
• Similarly, the training set (rr) contains all pieces on the key squares with all the rotations from
the R dataset. The corresponding test sets and validation sets of r and rr are created in the
same manner as the D dataset.
The fine-tuning was conducted in two phases: in the first phase, weights of most layers of the model
are frozen and only the last layer (head) is trained over epochs with a learning rate of 0.001. In the
second phase, all the model's weights are trained with a learning rate of 0.0001. The Adam optimizer
is used in both phases.
Following the aforementioned training conditions, the piece classifier models were evaluated on their
corresponding test set, as well as on the S and G datasets. The evaluation results are detailed in
Table 2. When comparing the two selections of key squares based on the model accuracies on
dataset G, it is evident that using squares provides more samples for fine-tuning and results in
good accuracies with better robustness. Additionally, including the rotated samples in the training
set also helps improve performan4ce×, b4ut it requires a longer training duration to be effective. Results
on the S dataset show that using squares provides better robustness against the noise caused
by piece displacements compared to using squares. While longer training durations and adding
4×4
rotations to the training set yield some improvements, these improvements are not substantial.
3×3
Regarding the occupancy classifier, it was found to achieve 100% accuracy on G dataset after
retraining for 50 epochs using piece sub-images at the squares and all the adjacent empty
squares.
4×4
Table 2. Piece classifier accuracies in different training conditions. The best accuracies on the
dataset S and G are underlined.
Key
3 3 4 4
squares
× ×
Epoch 500+500 1000+500 1000+1000 500+500 1000+500 1000+1000
Training
Test set d d+r d+rr d d+r d+rr d d+r d+rr d d+r d+rr d d+r d+rr d d+r d+rr
set
D-d 0.9924 0.9943 0.9943 0.9978 1.0000 1.0000
D+R-d-r 0.9794 0.9835 0.9863 0.9939 0.9939 0.9954
D+R-d-rr 0.9915 0.9887 0.9930 0.9984 0.9922 1.0000
S 0.9762 0.9704 0.9684 0.9704 0.9691 0.9759 0.9652 0.9688 0.9740 0.9837 0.9831 0.9844 0.9834 0.9883 0.9886 0.9814 0.9886 0.9844
G 0.9932 1.0000 0.9932 0.9865 0.9662 1.0000 0.9932 0.9865 1.0000 0.9865 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000
5.2. Grasping of Pieces
An experiment was performed in which the OpenChessRobot was tasked with grasping
various pieces positioned on different squares of the chessboard, repeated multiple times. To
reduce positional discrepancies, a compensation of 1 cm along the y-axis and 0.55 cm along
the x-axis (Figure 11) was applied as the system error.
Specifically, the evaluation focused on testing the grasp performance using the largest piece,
the king (4 cm in width, 7.8 cm in height), and the smallest piece, the pawn (2 cm in width, 4
cm in height). These pieces were positioned at each of the four corner squares (A1, A8, H1,
H8) and shifted from the edges of the squares to their centers (depicted in Figure 11). Twoshifting distances were used: (1) placing the piece tangent to the square’s edge or (2) 0.625
cm away from the edge, corresponding to 1/8 the length of a square. Each piece in each
square position was tested 10 times.
Y-axis
X-axis
Figure 11. The positions by shifting the piece at square H8. In the left image, the black pawn
is in the center without shifting. In the right image, the white king shifts to the left edge of the
square. The brown circle shows the center position. The white circles indicate the first shifting
distance (to the edge) and the yellow circles indicate the second shifting method
(intermediate).
Figure 12 shows the success rates for the different independent variables of the grasping
experiment. A distinction is made between three categories: 1) accurate grasping: a correct
grasp in 10 out of 10 trials, 2) remedied grasping: a correct grasp in 8 or 9 out of 10 trials
(usually correct grasping occurred here in such a way that the piece slid into the grasper’s
teeth), and 3) missed grasping: a correct grasp in 0 to 7 out of 10 trials. Table 3 shows the
corresponding results in numerical form. A trend can be noticed, whereby the grasping was
least successful when the piece was positioned toward the edge of the square or towards the
‘up’ direction, i.e., in the positive direction of the y-axis (see Figure 11), positioned away from
the base of the robot arm. It is hypothesized that this effect stems from image projection and
coordinate transformation errors in the 3D chessboard localization.
Figure 12. Results from the grasping experiment.
5.3. Human-Robot Chess Play
To assess the core components of the chess robot, a chess game putting a human against
the OpenChessRobot was conducted. The robot had control of the white pieces while the
human played with the black pieces. To determine its next move, Stockfish 15, with a search
depth set to 20, was used.
The OpenChessRobot initiated the game from its ready pose, executed a move, and
subsequently returned to its ready pose, awaiting the human player’s move. The human playerwas prompted to press the middle keyboard button (see Figure 1) upon completing their move.
The robot then captured an image, identified the human’s move, and made its subsequent
move.
Table 3. Piece grasping results. The percentages represent the number of conditions that
were marked as ‘accurate grasping’ or ‘remedied grasping’.
Overall Shift Distance Shift Direction
Without With
Piece Halfway Edge Left Right Up Down
Shift Shift
(n = 16) (n = 16) (n = 8) (n = 8) (n = 8) (n = 8)
(n = 4) (n = 32)
Pawn 100% 65% 75% 31% 75% 88% 0% 50%
King 100% 78% 100% 31% 63% 88% 50% 63%
To ensure a fair match, the human player also had access to Stockfish 15 to determine their
next best move. The game ended when a checkmate was achieved or when the robot and the
human repeated a set of specific positions, indicating a tied game. Throughout the experiment,
we recorded the time consumption of each robot module as it carried out its respective tasks.
The game ended in a draw after 97 moves. Out of these 97 moves, 7 involved capturing
pieces, wherein the robot first removed the captured piece before executing its move; 6 moves
required jumping pieces over others, resulting in longer execution times compared to moves
made on unobstructed paths. The majority, totaling 83 moves, followed the pick-and-slide
approach. The robot executed a castling move twice.
The time taken for the OpenChessRobot to execute its moves is depicted in the left plot of
Figure 13. On average, it took 7.33 seconds for the robot to complete a chess move, from its
initial position to the resumption of that position after the move. Capture moves, on average,
consumed 6.24 seconds. The right plot of Figure 13 shows the time allocation for move
detection, evaluation, and prediction. Move detection was typically completed within 1 second,
with an outlier attributed to camera failure. Evaluating a single move with Stockfish costs
around 1 second. Finding the next move with Stockfish generally consumed approximately 5
seconds, unless the current human move had already been seen when Stockfish searches for
the last move. In such cases, the search time was virtually zero, allowing the robot to execute
the next move immediately. The relatively large computation time can be explained by the fact
that we let Stockfish calculate to a high depth, something that is normally not necessary for
amateur-level play.
Figure 13. Left: Time consumption of different robot moves in the game. Right: Time
consumption of Perception (detect time), Stockfish evaluating a move (evaluate time), and
stockfish searching for the next move (engine time).6. Discussion and Conclusion
In this paper, we introduced an open-source cognitive robot designed for engaging in chess
matches with humans. While multiple chess robots have been developed over the years, none
have been made available as reproducible platforms, and most of them have not been
specifically designed for conducting HRI studies within a chess game context. Our robot takes
a unique approach by integrating robust robotic perception, evaluation of gameplay, and move
execution. Our development places emphasis on adaptability and reproducibility, rendering it
an accessible platform for researchers and enthusiasts alike.
We focused on the game of chess because of its value as a controlled experimental
environment where the controllable measurement of both human behavior and machine
performance is feasible. Chess also serves as a ‘battleground’ for Moravec’s Paradox, which
states that while machines excel at computational tasks (such as playing chess), they
traditionally struggle with tasks requiring human-like perception, motor control, and language
processing (Moravec, 1988). Our research platform aims to transcend Moravec’s Paradox by
integrating traditional chess engine capabilities with innovations in human-like perception and
the verbalization of moves using synthetic speech. This approach not only challenges
Moravec’s Paradox but also paves the way for a new line of research. Our platform signals a
step toward a new form of robotics that, in essence, revisits the conceptual foundation laid by
the Mechanical Turk in the 18th century (Standage, 2002).
Furthermore, the integration of verbal and non-verbal interactions is included to enhance the
depth of engagement between humans and the robot. By using a large language model for
verbal communication, we have created a solution that bridges this divide between traditional
robotic systems and human-like interaction. Future research may include more refined
linguistic possibilities by using large language models that are specifically trained or fine-tuned
on chess-related text databases (e.g., Feng et al., 2023).
Limitations and future research
Our system has been evaluated across its various modules. The evaluation details validate
our design and illuminate possibilities for further research with the system. In terms of
interaction methods, our emphasis was on demonstrating the system’s capabilities rather than
focusing on achieving the state-of-the-art in interactions. The chess engine evaluation is
currently only used for posture interactions. Additionally, there is a lack of human-subject
experiments that measure user acceptance of the interaction modules.
In the future, we intend to leverage this setup to study how AI-embodied robots influence
people during interactions. This will involve the robot communicating with humans through
emotional expressions and more natural verbal interactions. Extending the
OpenChessRobot's capabilities beyond the chess application to general physical assistance
is also an intriguing avenue to explore.
In conclusion, our cognitive chess robot represents a valuable contribution to the field of
human-robot interaction, representing a versatile platform for ongoing research. We envision
this system not merely as a game companion, but also as a catalyst for scientific research in
the area of human-robot interaction.
Funding
This project has received funding from the Cohesion Project of Faculty of Mechanical
Engineering, Delft University of Technology.
References
Campbell, M., Hoane, A. J., Jr., & Hsu, F.-h. (2002). Deep Blue. Artificial Intelligence, 134,
57–83. https://doi.org/10.1016/S0004-3702(01)00129-1Chen, A. T.-Y., & Wang, K. I.-K. (2019). Robust computer vision chess analysis and
interaction with a humanoid robot. Computers, 8, 14.
https://doi.org/10.3390/computers8010014
Christie, D. A., Kusuma, T. M., & Musa, P. (2017). Chess piece movement detection and
tracking, a vision system framework for autonomous chess playing robot. Proceedings of
the 2017 Second International Conference on Informatics and Computing, Jayapura,
Indonesia. https://doi.org/10.1109/IAC.2017.8280621
Coleman, D., Sucan, I., Chitta, S., & Correll, N. (2014). Reducing the barrier to entry of
complex robotic software: A MoveIt! case study. arXiv.
https://doi.org/10.48550/arXiv.1404.3785
Czyżewski, M. A., Laskowski, A., & Wasik, S. (2020). Chessboard and chess piece
recognition with the support of neural networks. Foundations of Computing and Decision
Sciences, 45, 257–280. https://doi.org/10.2478/fcds-2020-0014
Ding, J. (2016). ChessVision: Chess board and piece recognition (Technical report).
Stanford University.
https://web.stanford.edu/class/cs231a/prev_projects_2016/CS_231A_Final_Report.pdf
Franka Emika. (2020). Panda powertool.
https://web.archive.org/web/20200102112015/https://www.franka.de/technology
Franka Robotics GmbH. (2023). Franka Control Interface documentation.
https://frankaemika.github.io/docs
Feng, X., Luo, Y., Wang, Z., Tang, H., Yang, M., Shao, K., Mguni, D., Du, Y., & Wang, J.
(2023). ChessGPT: Bridging policy learning and language modeling. Advances in Neural
Information Processing Systems, 36. https://doi.org/10.48550/arXiv.2306.09200
Golz, J., & Biesenbach, R. (2015). Implementation of an autonomous chess playing
industrial robot. Proceedings of the 2015 16th International Conference on Research and
Education in Mechatronics, 53–56, Bochum, Germany.
https://doi.org/10.1109/REM.2015.7380373
Gunes, H., Broz, F., Crawford, C. S., Rosenthal-von der Pütten, A., Strait, M., & Riek, L.
(2022). Reproducibility in human-robot interaction: Furthering the science of HRI. Current
Robotics Reports, 3, 281–292.https://doi.org/10.1007/s43154-022-00094-5
He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition.
Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition,
770–778, Las Vegas, NV. https://doi.org/10.1109/CVPR.2016.90
Kołosowski, P., Wolniakowski, A., & Miatliuk, K. (2020). Collaborative robot system for
playing chess. Proceedings of the 2020 International Conference Mechatronic Systems
and Materials, Bialystok, Poland. https://doi.org/10.1109/MSM49833.2020.9202398
Koray, C., & Sümer, E. (2016). A computer vision system for chess game tracking. In L.
Cehovin, R. Mandeljc, & V. Štruc (Eds.), Proceedings of the 21st Computer Vision Winter
Workshop, Rimske Toplice, Slovenia.
Kuffner, J. J., & LaValle, S. M. (2000). RRT-connect: An efficient approach to single-query
path planning. Proceedings 2000 ICRA. Millennium Conference. IEEE International
Conference on Robotics and Automation. Symposia Proceedings, 2, 995–1001, San
Francisco, CA. https://doi.org/10.1109/ROBOT.2000.844730
Kunz, T., & Stilman, M. (2013). Time-optimal trajectory generation for path following with
bounded acceleration and velocity. In N. Roy, P. Newman, & S. Srinivasa (Eds.),
Robotics: Science and Systems VIII (pp. 209–216). MIT Press.
https://doi.org/10.7551/mitpress/9816.003.0032
Kuo, M. T., Hsueh, C. C., & Tsai, R. T. H. (2023). Large language models on the
chessboard: A study on ChatGPT’s formal language comprehension and complex
reasoning skills. arXiv. https://doi.org/10.48550/arXiv.2308.15118
Larregay, G., Pinna, F., Avila, L., & Morán, D. (2018). Design and implementation of a
computer vision system for an autonomous chess-playing robot. Journal of Computer
Science & Technology, 18, e01. https://doi.org/10.24215/16666038.18.e01
LC, R., Benayoun, M., Lindborg, P., Xu, H., Chung Chan, H., Man Yip, K., & Zhang, T.
(2021). Power chess: Robot-to-robot nonverbal emotional expression applied tocompetitive play. Proceedings of the 10th International Conference on Digital and
Interactive Arts, Aveiro, Portugal. https://doi.org/10.1145/3483529.3483844 .
Leite, I., Pereira, A., Mascarenhas, S., Martinho, C., Prada, R., & Paiva, A. (2013). The
influence of empathy in human–robot relations. International Journal of Human-Computer
Studies, 71, 250–260. https://doi.org/10.1016/j.ijhcs.2012.09.005
Luqman, H. M., & Zaffar, M. (2016). Chess brain and autonomous chess playing robotic
system. Proceedings of the 2016 International Conference on Autonomous Robot
Systems and Competitions, 211–216, Bragança, Portugal.
https://doi.org/10.1109/ICARSC.2016.27
Makoviychuk, V., Wawrzyniak, L., Guo, Y., Lu, M., Storey, K., Macklin, M., Hoeller, D.,
Rudin, N., Allshire, A., Handa, A., & State, G. (2021). Isaac Gym: High performance
GPU-based physics simulation for robot learning. arXiv.
https://doi.org/10.48550/arXiv.2108.10470
Mallasénn Quintana, D., Del Barrio García, A. A., & Prieto Matías, M. (2020).
LiveChess2FEN: A framework for classifying chess pieces based on CNNs. arXiv.
https://doi.org/10.48550/arXiv.2012.06858
Matuszek, C., Mayton, B., Aimi, R., Deisenroth, M. P., Bo, L., Chu, R., Kung, M., LeGrand,
L., Smith, J. R., & Fox, D. (2011). Gambit: An autonomous chess-playing robotic system.
Proceedings of the 2011 IEEE International Conference on Robotics and Automation,
4291–4297, Shanghai, China. https://doi.org/10.1109/ICRA.2011.5980528
Moravec, H. (1988). Mind children. Cambridge, MA: Harvard University Press.
MoveIt. (2021.). IKFast Kinematics Solver.
https://docs.ros.org/en/kinetic/api/moveit_tutorials/html/doc/ikfast/ikfast_tutorial.html
NVIDIA. (2019). NVIDIA Jetson Nano. https://www.nvidia.com/en-us/autonomous-
machines/embedded-systems/jetson-nano
Open Robotics. (2020). ROS Noetic installation instructions.
https://wiki.ros.org/noetic/Installation
Pereira, A., Martinho, C., Leite, I., & Paiva, A. (2008). iCat, the chess player: The influence
of embodiment in the enjoyment of a game. Proceedings of the 7th International Joint
Conference on Autonomous Agents and Multiagent Systems - Volume 3, 1253–1256,
Estoril, Portugal.
Quigley, M., Conley, K., Gerkey, B., Faust, J., Foote, T., Leibs, J., Berger, E., Wheeler, R., &
Ng, A. Y. (2009). ROS: An open-source Robot Operating System. Proceedings of the
ICRA Workshop on Open Source Software, Kobe, Japan.
Ranasinghe, P., Ranasinghe, P., & Ashan, V. (2023). ChessEye: An integrated framework
for accurate and efficient chessboard reconstruction. Proceedings of the 2023 Moratuwa
Engineering Research Conference, 177–182, Moratuwa, Sri Lanka.
https://doi.org/10.1109/MERCon60487.2023.10355515
Sajó, L., Ruttkay, Z., & Fazekas, A. (2011). Turk-2, a multi-modal chess player. International
Journal of Human-Computer Studies, 69, 483–495.
https://doi.org/10.1016/j.ijhcs.2011.04.001
Schwenk, A., & Yuan, C. (2015). Visual perception and analysis as first steps toward
human–robot chess playing. In G. Bebis, R. Boyle, B. Parvin, D. Koracin, I. Pavlidis, R.
Feris, T. McGraw, M. Elendt, R. Kopper, E. Ragan, Z. Ye, & G. Weber (Eds.), Advances
in visual computing. 11th International Symposium, ISVC 2015, Las Vegas, NV, USA,
December 14–16, 2015, Proceedings, Part II (pp. 283–292). Cham: Springer.
https://doi.org/10.1007/978-3-319-27863-6_26
Shannon, C. E. (1950). XXII. Programming a computer for playing chess. The London,
Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 41, 256–275.
https://doi.org/10.1080/14786445008521796
Shin, K., Yamsani, S., Mineyev, R., Chen, H., Gandi, N., Lee, Y. J., & Kim, J. (2023).
Exploring the capabilities of a general-purpose robotic arm in chess gameplay.
Proceedings of the 2023 IEEE-RAS 22nd International Conference on Humanoid Robots,
Austin, TX. https://doi.org/10.1109/Humanoids57100.2023.10375209Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre,
L., Kumaran, D., Graepel, T., Lillicrap, T., Simonyan, K., & Hassabis, D. (2018). A general
reinforcement learning algorithm that masters chess, shogi, and Go through self-play.
Science, 362, 1140–1144. https://doi.org/10.1126/science.aar6404
Siraj, D. S. (2017). JudgeMaster: Automated chess movement validator. Proceedings of the
2017 National Information Technology Conference, 112–118, Colombo, Sri Lanka.
https://doi.org/10.1109/NITC.2017.8285662
Srivatsan, R., Badrinath, S., & Lakshmi Sutha, G. (2020). Autonomous chess-playing
robotic arm using Raspberry PI. Proceedings of the 2020 International Conference on
System, Computation, Automation and Networking, Pondicherry, India.
https://doi.org/10.1109/ICSCAN49426.2020.9262351
Standage, T. (2002). The Turk: The life and times of the famous eighteenth-century chess-
playing machine. New York: Walker.
StereoLabs. (2020). Zed 2. https://www.stereolabs.com/products/zed-2
Stockfish. (2022). Stockfish 15.1. https://stockfishchess.org
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., & Wojna, Z. (2016). Rethinking the
inception architecture for computer vision. Proceedings of the 2016 IEEE Conference on
Computer Vision and Pattern Recognition, Las Vegas, NV.
https://doi.org/10.1109/CVPR.2016.308
Thanh Tra, N. L., Cong, P. T., & Anh, N. D. (2018). Design a chess movement algorithm and
detect the movement by images classification using Support Vector Machine classifier.
Proceedings of the 2018 4th International Conference on Green Technology and
Sustainable Development, 335–340, Ho Chi Minh City, Vietnam.
https://doi.org/10.1109/GTSD.2018.8595604
Wölflein, G., & Arandjelović, O. (2021). Determining chess game state from an image.
Journal of Imaging, 7, 94. https://doi.org/10.3390/jimaging7060094
Xie, Y., Tang, G., & Hoff, W. (2018). Chess piece recognition using oriented chamfer
matching with a comparison to CNN. Proceedings of the 2018 IEEE Winter Conference
on Applications of Computer Vision, 2001–2009, Lake Tahoe, NV.
https://doi.org/10.1109/WACV.2018.00221
Zhelyabuzhsky, I. (2022). Stockfish. https://pypi.org/project/stockfishSupplementary Material
Appendix A. Eye gaze tracking with HoloLens 2
In order to monitor the user’s eye gaze in real-time, the eye-tracking function of the HoloLens
2 can be used. It is able to determine if the eye gaze vector hits holograms. There are two
holograms, namely the Franka robot model and the chessboard model, overlaying on the
physical robot and the real chessboard. For this, we implement a digital twin of the robot that
follows all the movements of the real robot (see Figure A1). The user’s attention distribution
can be easily logged by monitoring the gaze-on-hologram status during the interaction.
Figure A1. A digital twin of the chess robot and a model of the chess set in the HoloLens 2.
The digital twin and the chessboard in this photo are manually placed next to the physical
robot only for the demonstration. In practice, the digital twin and the virtual board are set to be
transparent and not visible when the user wears the HoloLens 2.Appendix B. Dataset creation
A new dataset was collected by placing all the pieces (64 - 1 missing rook) on the board, and
then taking photos from the default camera pose. The corresponding labels in FEN format
were then created. After row movements, column movements, and rotations, each piece
appeared at least once at each square on the board, with rotations. The procedure for
creating the dataset is detailed below:
Starting position
The pieces were placed in the middle of the square manually. The camera pose was such
that pieces often occluded each other. The adjacent pieces of each piece had both white
neighbors and black neighbors.
Image in Dropbox: Data/piece_square_iteration/raw/img/img_C0_R0_r0.png
Label in
FEN:pppppppp/1nbqkbnr/PPPPPPPP/RNBQKBNR/PPPPPPPP/rnbqkbnr/pppppppp/RNBQ
KBNR
Figure 1. Starting position.
Taking photos of the default and rotated piece positions
At one game position, multiple photos were taken: one photo recorded the default setting.
Four additional photos recorded the pieces with rotations. Four types of pieces were rotated
because of their rotational asymmetry: white king, white knight, black king, black knight.
Kings were rotated twice and knights were rotated four times to create different shapes of
the pieces in the image. Each rotation was approximately 45 degrees.Default Rotated
Figure 2. Illustration of piece rotation.
Following the above procedure, at each chess pieces’ position, five images were taken
(unrotated, both the knight and king rotated twice, knight rotated twice more). Photos are
named as: img_CX_RY_[r0, r1, r2, r3, r4].png
Shifting the pieces by row
Then, the rows of pieces were moved towards the direction of increasing rank to make a
new chess piece position. Then five photos were taken at the new position with the same
method mentioned above. The row shifting was repeated 3 times (R0 > R1 > R2 > R3).
Image in Dropbox: Data/piece_square_iteration/raw/img/img_C0_R1_r0.png
Label in FEN:
1nbqkbnr/PPPPPPPP/RNBQKBNR/PPPPPPPP/rnbqkbnr/pppppppp/RNBQKBNR/pppppppp
Figure 3. Starting position, after one upward row shift.Shifting the pieces by column
After shifting the row three times, one additional row shift was realised (R3 > R0), in order to
recreate the R0 game position. After that, a column shift towards the (A > H) direction was
conducted. (C0 > C1), followed by taking photos at this new game position using the same
method mentioned above.
Figure 4. Starting position, after one rightward row shift.
Image in Dropbox: Data/piece_square_iteration/raw/img/img_C1_R0_r0.png
Label in FEN:
PPPPPPPP/rrnbqkbn/pppppppp/RRNBQKBN/pppppppp/r1nbqkbn/PPPPPPPP/RRNBQKBN
Creating the datasets
A total of 160 images were collected (4 row movements × 8 column movements × 5
rotations) based on which three datasets were created: default, rotated and shifted. Data can
be found at: Dropbox/Renchi Zhang/ChessRobot/Data/piece_square_iteration
The default dataset contained 64 sub-images of each unrotated piece. In total there are 64 ×
12 sub-images. For the black and white kings and queens, there is exactly one sub-image
per square from the collected images. For the other pieces, the 64 sub-images were
randomly selected from the raw dataset for each square.
The rotated dataset contained 64 sub-images of each rotated piece (bk, wk, bkn, wkn). Per
piece, one of the rotations was picked randomly from the possible rotations for each square.
The shifted dataset contained sub-images with their cropping windows shifted into four
directions (up, down, left, right). Currently, the displacement in each direction was ¼ square
in length. For each shift direction, 64 × 12 sub-images were extracted from the raw image
set with the unrotated pieces. The shifted dataset is currently not used in training but only in
tests.
Training data selection and train-val-test split
Several squares were selected as key squares for the board. Sub-images of pieces on these
squares were used to train the model; this model was then tested on the remaining squares
to show its generalization ability. Two sets of key squares were tested:
1. 3×3:
key_file: A, E, H;key_rank: 1, 4, 8;
training set: 9 × 12 sub-images;
2. 4×4:
key_file: A, E, D, H;
key_rank: 1, 4, 5, 8;
training set: 16 × 12 sub-images;
The training dataset (d) from the default dataset was generated using all sub-images on
these key squares.
A training dataset (r) was generated from the rotation dataset using all sub-images with 90-
degree rotation.
Another training dataset (rr) was generated from the rotation dataset using all sub-images
with both 90-degree and 180-degree rotations.
Then sub-images of the remaining squares were split into a validation set and a test set
using the ratio 20:80. The test sets corresponding to d, r, rr are referred to as d, r, rr.
Before feeding the training set to train the model, some image augmentation was applied on
each sub-image randomly. These parameters were set by trial-and-error.
Attributes Range
brightness/hue/saturation [-20%,20%]
contrast [-30%,30%]
scale [0.8,1.2]
translation [-0.03, 0.1]
Examples: