Classifying Overlapping Gaussian Mixtures in High
Dimensions: From Optimal Classifiers to Neural Nets
KhenCohen∗,YaronOz
RaymondandBeverlySacklerSchoolofPhysicsandAstronomy
Tel-AvivUniversity
Tel-Aviv69978,Israel
khencohen@mail.tau.ac.il
NoamLevi∗
ÉcolePolytechniqueFédéraledeLausanne(EPFL)
Switzerland
noam.levi@epfl.ch
Abstract
Wederiveclosed-formexpressionsfortheBayesoptimaldecisionboundariesin
binary classification of high dimensional overlapping Gaussian mixture model
(GMM)data,andshowhowtheydependontheeigenstructureoftheclasscovari-
ances, for particularly interesting structured data. We empirically demonstrate,
throughexperimentsonsyntheticGMMsinspiredbyreal-worlddata,thatdeep
neuralnetworkstrainedforclassification,learnpredictorswhichapproximatethe
derivedoptimalclassifiers. Wefurtherextendourstudytonetworkstrainedon
authenticdata, observingthatdecisionthresholdscorrelatewiththecovariance
eigenvectorsratherthantheeigenvalues,mirroringourGMManalysis. Thispro-
videstheoreticalinsightsregardingneuralnetworks’abilitytoperformprobabilistic
inferenceanddistillstatisticalpatternsfromintricatedistributions.
1 Introduction
Thereisawell-acceptedunderstandinginmachinelearningthatinherentcorrelationsorstatistical
structurewithinthedataplayacrucialroleinenablingeffectivemodeling. Thepresenceofstructure
inthedataprovidesimportantcontextthatmachinelearningalgorithmscanleverageforaccurateand
meaningfulknowledgeextractionandgeneralization. Harnessinganyinherentstructureiswidely
seenasanimportantfactorforachievingsuccessfullearningoutcomes.
Determining the characteristics of natural datasets that contribute to effective training and good
generalizationhastremendoustheoreticalandpracticalsignificance. Regrettably,real-worlddatasets
aretypicallyviewedassamplesdrawnfromsomeunknown,intricate,high-dimensionalunderlying
population,renderinganalysesthatdependonaccuratelymodelingthetruedistributionenormously
challenging. Connectingpropertiesofthedatatolearningoutcomesisdifficultbecauserealdatadoes
notreadilyyielditsgenerativeprocessorpopulationdistribution.
While fully characterizing natural data distributions remains difficult, progress can still be made
byconsideringsimplifiedmodelsthatcapturekeyaspectsofthetruedistribution. Oneapproach
istomodelthedataviathemomentexpansionoftheunderlyingpopulation. Forarandomvector
X ∈ Rd representingasinglesample,itsdistributioncanbedescribedbythemomentgenerating
∗EqualContribution
Preprint.Underreview.
4202
yaM
82
]LM.tats[
1v72481.5042:viXrafunctionM (t)=(cid:80)∞ tnE(Xn)/n!. Approximatingthisbyretainingafinitenumberofmoments
X n=0
providesatractablemodelofthedatadistribution.
AsimplestartingpointistheGaussianmodel,whichpositsthatadataset’sstatisticalpropertiescan
befullycapturedbyitsfirsttwomoments-themeanvectorandsamplecovariancematrix. This
assumeshigher-orderdependenciesarenegligiblesuchthatthedistributionischaracterizedsolelyby
itsfirstandsecondcentralstatisticalmoments.
Inthelastdecade,analyzingthebehaviorofmachinelearningalgorithmsonidealizedi.i.d. Gaussian
datasetshasemergedasanimportantareaofresearchinhigh-dimensionalstatistics[Donohoand
Tanner, 2009, Korada and Montanari, 2011, Monajemi et al., 2013, Candès et al., 2020, Bartlett
etal.,2020]. Studyingthesesimplifiedsyntheticdistributionshasprovidedvaluableinsights,suchas
neuralnetworkscalinglaws[Maloneyetal.,2022,Kaplanetal.,2020],universalconvergenceproper-
ties[Seddiketal.,2020]andanimprovedunderstandingofthe"DoubleDescent"phenomenon[Mon-
tanariandSaeed,2022].ExamininglearningonGaussianapproximationsofrealdatadistributionshas
helpedestablishfoundationalunderstandingsofalgorithmicbehaviorinhigh-dimensionalsettings.
Aparticularcaseofinterestisthatofdatawhichisdividedintoasetnumberofclasses. Incertain
instances,thedatacanbedescribedbyamixturemodel,whereeachsampleisgeneratedseparately
for each class. The simplest example of such distributions is that of a Gaussian Mixture Model
(GMM),onwhichwefocusinthiswork.
Gaussianmixturesareapopularmodelinhigh-dimensionalstatisticssince,besidesbeinganuniversal
approximator,theyoftenleadtomathematicallytractableproblems. Indeed,arecentlineofwork
has analyzed the asymptotic performance of a large class of machine learning problems in the
proportionalhigh-dimensionallimitundertheGaussianmixturedataassumption,seee.g. Maiand
Liao[2019],Mignaccoetal.[2020a],Taherietal.[2020],KiniandThrampoulidis[2021],Wangand
Thrampoulidis[2021],Refinettietal.[2021],Loureiroetal.[2021a].
Inlightoftheinsightsgainedbystudyingtheunsupervised,perspectiveonGMMclassification,we
focushereonacomplementarydirection. Namely,weconsiderthesupervisedsetting,whereasingle
neuralnetworkistrainedonadatasetgeneratedfromaGMM,toperformbinaryclassificationwhere
thetruelabelsaregiven. ToconnecttheGMMwithreal-worlddata,werestrictourselvestothecase
ofstrictlynon-linearlyseparableGMMs,suchthattheclassmeansdifferenceisnegligiblecompared
totheclasscovariancedifference. WerefertothissetupasoverlappingGMMclassification.
Ourmaincontributionsareasfollows:
• InSec.3,wederivetheBayesOptimalClassifiers(BOCs)anddecisionboundariesforoverlap-
pingGMMclassificationproblems,bothinthepopulationandtheempiricallimits.
• InSec.4,undertheassumptionsofcorrelatedfeaturesandHaardistributedeigenvectors,weare
abletoprovideapproximateclosedformequationsforthedecisionboundariesanddiscriminator
distributions,asafunctionoftheeigenvaluesandeigenvectorsofthedifferentclasscovariances.
• InSec.5,wepresentempiricalevidence,demonstratingthatsomenetworkstrainedtoperform
binaryclassificationonaGMM,approximatetheBOC.Wefurtherrelatetheseobservationsto
existingresults,namelyconvergenceofhomogeneousnetworkstoaKKTpoint.
• InSec.6,wedemonstrateempiricallythatforhighdimensionaldata,thecovarianceeigenvectors
andnottheeigenvaluesdeterminetheclassificationthresholdfordeepneuralnetworkstrained
onreal-worlddatasets,andprovideanexplanationinspiredbyourresultsonGMMs. InSec.7
wesummarizeourconclusionsandoutlook.
2 BackgroundandRelatedWork
TherehasbeensignificantworkaimedatunderstandingtheclassificationcapabilitiesofGaussian
mixturemodels(GMMs),thatis,recoveringtheclusterlabelforeachdatapointratherthanusing
labelsprovidedbyateacher. Forbinaryclassificationproblems,examplesincludemethodsproposed
byMaietal.[2019],Mignaccoetal.[2020b],andDengetal.[2022],withthelatteralsodemonstrating
anequivalencebetweenclassificationandsingle-indexmodels.Inmulti-classsettings,[Thrampoulidis
etal.,2020]analyzedtheperformanceofridgeregressionclassifiers. Themostgeneralresultsin
thisareaarefromGaussianMixGroup[Loureiroetal.,2021b],whichconsiderstheuseofGMMs
withanyconvexlossfunctionforclassification. AdditionalresultsregardingkernelsandGMMs
include[Couilletetal.,2018,LiaoandCouillet,2019,KammounandCouillet,2023,Refinettietal.,
2021],preciseasymptotics[Loureiroetal.,2021c]andtheconnectionbetweenGMMsandreal-world
20.10 0.0025
101
0.08 0.0020 102
0.06 0.0015 103
0.04 0.0010 104
0.02 0.0005 105
0.00 0.0000 106
120 100 80 60 40 20 0 20 40 4000 3000 2000 1000 0 1000 2000 3000 4000 3000 2000 1000 0 1000 2000 3000
Figure1: Probabilitydensityfunctionofβ(x),evaluatedonGMMswithdifferentclasscovariance
matrices. BlueandredbinsindicatesamplesdrawnfromtheclassesAandB,respectively. Solid
curvesrepresentthenumericalevaluationofEq.(3)asageneralizedχ2distribution. Dotsindicate
the values given by Eq. (5), as well as by Eq. (9). Left: β distribution for covariances with the
samebasisbutdifferentspectra. Center: β distributionforcovarianceswiththesamespectrumbut
differentrandombases. Right: β distributioncomparisonforcovarianceswithbothdifferentspectra
anddifferentbases,showningray. Here,wetakeα =0.5,∆α=−0.3,d=100.
A
visiondata[IngrossoandGoldt,2022]. Additionally,workhasbeendonetounderstandQuadratic
DiscriminantAnalysis(QDA)inhighdimensions[Elkhaliletal.,2017,GhojoghandCrowley,2019,
DasandGeisler,2021],whichwerelyheavilyuponinthiswork.
3 OverlappingGaussianMixturesinHighDimensions
Consider the task of performing binary classification on samples drawn from a two class GMM,
wheretheunderlyingGaussiandistributionshavesimilarmeansbutdifferentcovariancematrices.
WegeneratetwodatasetsD ,D ofequalsizeN,andthesamedatadimensionsx∈Rd. Here,each
A B
samplevectorisdrawnfromajointlynormaldistribution,eitherx∼N(µ ,Σ )orx∼N(µ ,Σ ),
A A B B
wherethetwomeansandcovariancematricesdistinguishbetweenthetwoclasses. Inthissetup,
weassignalabelvaluey =1tosamplesdrawnfromclassAandy =−1fortheB classsamples.
Throughoutthiswork,weconsiderthelargenumberoffeaturesandlargenumberofsampleslimit
d,N →∞,whiletheratiod/N = γ ∈ R+ isconstant. Wedenoteγ → 0asthepopulationlimit,
whileγ →1astheempiricallimit.
Inthefollowingsections,wediscusstheoptimalclassifiersobtainedfirstfromthepopulationand
thentheempiricaldatadistributions. Wethenapplyourresultstoasimplifiedmodeldatasetwhich
capturessomeofthepropertiesofreal-worlddatasets,anddistinguishtherolesofthecovariance
eigenvectorsandeigenvaluesforthiscase.
3.1 OptimalClassificationonPopulationData
TheBayes-optimalclassifier(BOC)assignseachsampletotheclassthatmaximizestheexpected
valuegainoritslogarithmdefinedas
G(X =x)=argmaxp(X =x|Y =y)p(Y =y)=argmaxlog(p )=argmaxβ , (1)
X|Y Y
Y=y Y=y Y=y
wherep(Y =y)=1/C istheclassdensitywithC thenumberofclassesinthecaseofabalanced
dataset. Here, p(X = x|Y = y) = exp(β ) is the conditional in-class density. The decision
Y
boundarybetweenanytwoclassesy,y′ ∈C isgivenwhenβ (X =x)=β (X =x),where
Y=y Y=y′
the probabilities of a sample being classified as y or y′ are equal. For a two-class GMM, this is
obtainedbysimplyequatingtheclassconditionalprobabilitydensities,i.e.,p =p ,leadingtothe
A B
quadraticdecisionrule,derivedinElkhaliletal.[2017]aswellasDasandGeisler[2021],topick
classAif
1
β(x)= (xTQx−2qTx+c)>0, Q=(Σ−1−Σ−1), q =(Σ−1µ −Σ−1µ ), (2)
2 B A B B A A
(cid:18) (cid:19)
|Σ |
c=µTΣ−1µ −µTΣ−1µ −log A ,
B B B A A A |Σ |
B
where|O|isthedeterminantofthematrixO. Thisquadraticβ(x)istheBayesclassifier,ortheBayes
decisionvariablethat,whencomparedtozero,maximizesexpectedgain.
NotethattheqTxterminEq.(2)determinesthelineardecisionboundary,whilexTQxdetermines
the nonlinear part. For the rest of this work, we consider the nonlinear, overlapping mixture
configuration,whereµ = µ = µ,whileΣ ̸= Σ ,asaproxyforthebehaviorofmanyreal
A B A B
worlddatasets[Schillingetal.,2021]. Withoutlossofgenerality,wemaysetµ=0,sincewecan
3
)(p )(p )(pFigure2: ComparisonbetweenBOCandquadraticnetworktrainedondatawithdifferentcovariance
spectrum. Left: accuracy, measured as the sigmoid function acting on the network/BOC output.
Center: network/BOCoutputdistribution. BlueindicatesnetworkresultswhileredistheBOC
prediction. Here,thenetworkistrainedwithd =100andN =200kto91.27%trainingaccuracy
h
and reaches 91.1% with α = 0.2 and α = 0.3. Right: Results for KKT convergence of a
A B
quadraticNNtrainedonanα=0.2correlatedGMM,wheretheclasscovariancessharethesame
spectrum but are given a different random basis. Red indicates the BOC prediction, blue is the
quadraticnetworkoutputbeforethesoftmaxfunction,whentrainedwithd =100,andgreenare
h
theKKTpredictionswithλ ∝1/N andd =100. Thedatadimensionsaresimilartothemaintext,
a h
i.e.,d=100,usingN =100ksamples,thenetworkreached100%trainingand99.9%testaccuracy.
alwaysshiftawaythemeanofthedistributions,resultinginthesimplerformofβ(x)
1 1 1(cid:88) a
β(x)= (xTQx+c)= (Tr(QxxT)+c)= q χ˜2− , (3)
2 2 2 i 1 2
i
where the last transition in Eq. (3) is valid since β is given by a sum of χ2 distributed random
variables,knownasthegeneralizedchi-squareddistribution[DasandGeisler,2021],wherea ∼
N(log(|Σ−1Σ |),0),q aretheeigenvaluesofΣ1/2QΣ1/2,andY =A,B. Theprobabilitydensity
B A i Y Y
function (PDF) of β does not have a closed analytic form, but can be obtained numerically by
variousmethods[Ferrari,2019,DasandGeisler,2021]. InFig.1,weshowtheβ-distributionfor
high-dimensionalGaussiandatawithdifferentcovariances,obtainedbothempiricallyfromEq.(3)
andbydrawingfromasumofχ2variablesmatchingEq.(3).
WhilethefullPDFofβ cannotbewritteninasimpleform,theempiricalexpectationvalueona
givensetofmeasurementscanbederived. Namely,givenasetofN samplesx ,i=1,...,N,taken
i
fromanormaldistributionN(0,M),M ∈Rd×d,theexpectationvalueoftheBayesclassifierβ on
thesesamplesissimply
⟨β ⟩= 1(cid:2) Tr(QM)−log(cid:0) |Σ−1||Σ |(cid:1)(cid:3) , (4)
M 2 B A
where⟨·⟩indicatesaveragingoverthedatadistribution. Itcanbeusedtodefinewhatitmeansfora
sampletobelongtoclassAorB,bycomputingtheaveragedistancebetweentheexpectationvalue
ofβ onanydatasetandtheclassvalues,givenby
⟨β ⟩=−d+Tr(Σ−1Σ )+c>0, ⟨β ⟩=d−Tr(Σ−1Σ )+c<0, (5)
A B A B A B
whereweaveragedoverN → ∞samplesfromtheA,B distributions,andc = −log(|Σ−1Σ |).
B A
WeshowthesevaluesasblackdotsinFig.1.
3.2 EmpiricalOptimalClassification
So far, we have assumed that we have access to the population covariances Σ ,Σ . Here, we
A B
comment on the classification problem changes when relaxing this assumption and consider the
empiricalcovarianceswhichareobtainedatfinitedimensiontosamplenumberratioγ =const.
In particular, we consider the case where we have access only to finitely sampled versions of
the population moments, but have infinitely many samples of this noisy distribution. Here, the
distributionsinquestionforclassesA,Barestillgaussian,buttheirrespectivemomentsarenotthe
populationones,implyingthattheBOCisdeformed. Thecovariancematriceswehaveaccesstoare
givenbymeasurements,thatdefinetheempiricalcovariancesforeachclassΣ = 1X XT ≃
√ N,Y N Y Y
Σ + γR , where X ∈ Rd×N is the design matrix, and R is a random matrix with O(1)
Y Y Y Y
4eigenvalues[BiroliandMézard,2023]2. WecanthereforerewritetheempiricalBOCas
√
xT (Q )x c √ (cid:18)|I+ γΣ R |(cid:19)
β (x)≃ N + N, Q ≃Q+ γ(R −R ), c =c−log √ B B , (6)
N 2 2 N B A N |I+ γΣ R |
A A
where we expand to leading order in γ = d. This implies that the empirical β distribution
N N
remainsageneralizedχ2withthesubstitutionofcoefficientstobetheeigenvaluesofΣ1/2Q Σ1/2
Y N Y
and the constant c is shifted by the log term. Therefore, the empirical deviation from the BOC
∆β =∥β −β∥isestimatedby
N N
√
∆β ≃ γ|Tr(R (xxT −Σ )−R (xxT −Σ ))|, (7)
N B B A A
√
whichdecreaseswith γ. ThissettingisexhibitedinFig.1,whichismeanttomimicthereal-world
setting,wherewedonothaveaccesstothepopulationcovariance,andwecanonlyestimatehow
closetheempiricalmomentsaretothepopulationones.
4 AnalysisforaToyModelofComplexData
InordertoconnecttheBOCwithrealworldclassificationtasks,wefocusonaspecificmodelingof
naturaldata. Here,weconsiderasimplemodelofacorrelatedGMMinspiredbytheneuralscaling
lawliterature[Maloneyetal.,2022,MontanariandSaeed,2022,LeviandOz,2023a]aswellasthe
signalrecoveryliterature[Loureiroetal.,2021d,2022]. Concretely,weassumeapowerlawscaling
spectrum,andadifferentbasismatrixforeachclass
Λ =i−1−αYδ , Σ =OTΛO , (8)
Y,ij ij Y Y Y
where δ is the d dimensional identity matrix and O is a random orthogonal matrix. In the
ij Y
followingsections,wedistinguishtheroleofeigenvaluesandeigenvectorsindeterminingtheBOC
foroverlappingGMMs.
4.1 DiagonalCorrelatedCovariances
Asafirstexample,weconsiderthecasewhereΣ ,Σ arebothdiagonalmatriceswithdifferent
A B
eigenvalues,suchthatO =O =I,andΣ =Λ ,Σ =Λ .Givenamodelofpowerlawscaling
A B A A B B
datasetAwithα ,wecandefinethepowerlawexponentofthedatasetB tobeα =α +∆α.
A B A
Here,wecanobtainexplicitexpressionsforβ as
1(cid:16) (cid:17) 1(cid:16) (cid:17)
⟨β ⟩= H(−∆α)−d−∆αlog(Γ(d+1)) , ⟨β ⟩= d−H(∆α)−∆αlog(Γ(d+1)) , (9)
A 2 d B 2 d
whereHi istheHarmonicnumber,andΓ(x)istheGammafunction. Inthelimitofd → ∞,one
d
(cid:16) (cid:16) (cid:17) (cid:17)
obtainsthat⟨β ⟩∝ 1 2d d∆α −1 −(2d∆α+∆α)log(d) atleadingorder. Thisisarapidly
A 4 ∆α+1
growingfunctionind,showingthatevenasmalldifferenceinspectracanbemagnifiedsimplyby
dimensionality,leadingtocorrectclassification.
4.2 RotatedCorrelatedCovariances
Next,weconsiderthecasewhereΣ ,Σ sharethesamespectrum,butarerotatedwithrespectto
A B
oneanother,suchthatΣ =OTΛO ,Σ =OTΛO . ThesampleexpectationfortheBOCreads
A A A B B B
⟨β ⟩=−⟨β ⟩=
1(cid:0) Tr(OTΛ−1OΛ)−d(cid:1)
, (10)
A B 2
whereO =O OT isitselfarandomorthogonalmatrix. Inthelimitofd→∞,wemayreplacethe
A B
expectationvalueoverthedatasetwithitsintegralwithrespecttotheHaarmeasureontherotation
group,andobtainaclosedformexpressionas
1(cid:20) 1 (cid:21) 1(cid:32) H(−1−α)H(1+α) (cid:33)
⟨β ⟩ = Tr(Λ−1)Tr(Λ)−d = d d −d , (11)
A O 2 d 2 d
wherethefinalexpressionisgivenforapowerlawscalingdataset. Inthelimitofd→∞,Eq.(11)
(cid:16) (cid:17)
growsas⟨β ⟩ ∝ 1 1 +1 d− dα+1ζ(α+1),whichisafastergrowingfunctionthanEq.(9)
A O 2 α(α+2) 2(α+2)
2Thisresultholdsinthelimitofd2 ≪N.Inthecaseofd≪N,Q isaWishartmatrix,whichrequires
N
morecarefultreatmentsuchasgiveninLiaoandCouillet[2019],Tiomokoetal.[2019],Zhangetal.[2018],
whichweleaveforfuturework.
5ResNet18 FC
85
100
95 80
90
75
85
80 70
75
65
70
65 60
60
55
55
1.5 1.4 1.3 1.2 1.1 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.5 1.4 1.3 1.2 1.1 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Figure3: TwoGaussianclassificationtask,withthesameeigenvectorsbutdifferenteigenvaluesbulk:
thefirstonewithα=0.5andthesecondwithα+∆α. ThemodelisFC,averagedover5training
runs-thesolidlinerepresentsthemeanaccuracyandshadedarearepresents1-sigmaerrorbars.
for∆α/α<1indicatingthatitrequiresalargedifferenceintherelativeeigenvaluescalingexponents
∆α/αtoshifttheBOCdistribution,whencomparedtotheeffectofabasisdifference. Weshowthis
effectexplicitlyinthedifferentpanelsofFig.1. Thisimpliesthattheeigenvectorsarelikelytoplaya
moresignificantrolethaneigenvaluesindatasetswhicharewellmodeledbytheabovesetup.
5 NeuralNetworksasNearlyOptimalClassifiers
HavingestablishedtheBOCbehavioronoverlappingGMMs,wecannowaskhowtheseconclusions
translatetothelearningprocessinsomeneuralnetworkarchitectures. Namely,NNshavebeenshown
toachieveoptimalclassificationinsomecases[Radhakrishnanetal.,2023]. Wedemonstratethat
NNsquitegenericallyapproximatethequadraticBOC,andprovidesomeintuitionastohowitmay
occurbyappealingtothedirectionalconvergenceofgradientdescenttoaKKTpoint.
5.1 NeuralNetworkClassifier
LetD = {(x ,y )}N ⊆ Rd ×{−1,1}beabinaryclassificationtrainingdataset. LetΦ(θ;·) :
a a a=1
Rd →Rbeaneuralnetworkparameterizedbyθ ∈Rp. Foralossfunctionℓ:R→Rtheempirical
lossofΦ(θ;·)onthedatasetDisL(θ):=(cid:80)N
ℓ(y Φ(θ;x )). Wefocusonthelogisticloss(a.k.a.
a=1 a a
binarycrossentropy),namely,ℓ(q)=log(1+e−q).
ThenetworkfunctionΦ(θ;·),canthereforebeinterpretedastheequivalentoftheBOCvariableβ in
theprevioussections. Withthisintuitioninmind,weexpectthatneuralnetworkstrainedtoperform
binaryclassificationshouldconvergetotheBOC,i.e.,Φ(θ;·) → β withsufficientwithsufficient
networkexpressivity,numberofsamplesandsuccessfuloptimizationdynamics.
Thesimplestnetworkarchitecturethatonemayemployisalinearclassifier,whereΦ(z)=z. Such
linearnetworksofanydepthcannotapproximatetheBOCgivenbyEq.(3),asitisfullynonlinear.
Next, weconsideranon-trivialcase, bytakingatwolayernetwork, withtheactivationfunction
σ(z) = z2,i.e. thequadraticactivation. Quadraticactivationsarenaturalinthecontextofsignal
processing,whereoftendetectorscanonlymeasuretheamplitude(andnotphase)ofasignal,e.g. the
phaseretrievalproblem[Jaganathanetal.,2015,Dongetal.,2023]. Ithasalsogainedinpopularity
recentlyasaprototypicalnon-convexoptimizationproblemwithstrictsaddles,e.g.Candesetal.
[2014],Chenetal.[2019],Arnaboldietal.[2024],Martinetal.[2023]. Ithasalsobeenshownsuch
two-layerblockscanbeusedtosimulatehigher-orderpolynomialneuralnetworksandsigmoidal
activatedneuralnetworks[Livnietal.,2014,SoltaniandHegde,2018]. Concretely,asufficiently
expressivenetworkfunctiontofullyrealizetheBOCis
(cid:88)(cid:88)
Φ=vT(Wx)2+b= v W W x x +b, (12)
i iµ iν µ ν
i µ,ν
whereW ∈Rdh×d,vT ∈Rdh areweightmatricesattheinputandhiddenlayers,d histhenumber
ofhiddenunits,andbisthebiasatthelastlayer. Inthissetup,apredictorfunctioncanbeeasily
matchedtotheBOCbymatching
6
)%(
ycaruccA
)%(
ycaruccAd
(cid:88) 1 c
v W W = Q , b= , (13)
i iµ iν 2 µν 2
i=1
wherethenumberofhiddenneuronscorrespondtotheminimalnumberrequiredtomatchtherankof
theBOCmatrixQ,henced ≥ d. Sincethisnetworkisexpressiveenoughtofullyreproducethe
h
BOC,ouranalysisinSec.3.1holds,andtheimportanceofeigenvaluesandeigenvectorsismanifest.
WeshowtheresultofaquadraticnetworkconvergingtotheBOCinFig.2whered=d =100.
h
5.2 Karush–Kuhn–Tucker(KKT)Convergence
Anothersourceofintuitionforthevalueofd maybefoundintheKKTconvergenceequation[Jiand
h
Telgarsky,2020,LyuandLi,2020],whichstates,inanutshell,thattheweightsofahomogeneous
neuralnetwork3,trainedwiththelogisticlossforbinaryclassification,usinganinfinitesimallearning
rateη →0andinfiniteiterationst→∞,willconvergeindirectiontoaKKTfixedpointgivenby
N
θ˜ =(cid:88) λ y ∇ Φ(θ˜;x ), (14)
a a θ a
a=1
whereθ˜ arethenetworkweightsatconvergence,x arethesamplesandy arethesamplelabels.
a a
Here,λ ∈R+,andarenonzeroonlyforsamplesonthedecisionboundary. Thefulltheoremand
a
requirementsareprovidedinApp.B.WecanapplyEq.(14)tothequadraticnetworkas
N N
v˜=(cid:88) λ y (W˜x )2, W˜ =(cid:88) λ y v˜Tx (Wx ), (15)
a a a a a a a
a=1 a=1
whereweassumenobiases(consistentwiththecaseofsimilarspectrumanddifferentbasis). We
posit,thatinhighdimensionsλ ∝N−1,sinceatlargedeverysampleinthedatasetshouldlieonthe
a
marginandcontributeanequalamount,andthedecisionhyper-surfaceareashouldbecomparableto
theentirevolume[Hsuetal.,2022]. Underthisansatz,theaboveequationscanbesolvednumerically,
and indeed approximate the BOC as d increases. We present evidence that the Bayes optimal
h
decision boundary can be realized by a two-layer neural network with quadratic activation, and
increasingthenumberofhiddenunitsincreasestheprobabilityofconvergingtothisminimaunder
gradientflowinFig.2.
6 ResultsExtendingtoRealisticDataandNetworks
In the previous section, we showed that small differences in eigenvalues and eigenvectors are
magnifiedbythedimension,makingclassificationeasier. Wefurtherdemonstratedthatdifferencesin
theeigenvectorsarefastergrowinginthedimensionofthedata,suggestingeigenvectorsplayamore
significantroleintheBayesiandecisionboundary. Next,wewillprovideclearevidencethatthese
conclusionshold,andextendtoreal-worlddatasets,byperformingasetoftestsrelatedtothevarious
propertiesofthecovariancematrices. Wetraintwocommonnetworkarchitecturestoperformbinary
classification, bothonGMMsandonrealimages, exploringtheirperformanceaswechangethe
covariancestructureofeachclass.
Concretely,weconsidertwopopularnetworkarchitectures: fullyconnected(FC)andconvolutional
neuralnetworks(CNN),testedontheCIFAR10[Krizhevsky,2012]andFashion-MNIST[Xiaoetal.,
2017]datasets. Foreachtrainingprocedure,themodelisoptimizedtoclassifysamplesfromtwo
classesinthegivendataset(class0versusclasses1-9aggregated).
Theoptimizationprocedureproceedsasfollows: foreachclass,thesamplesaresplitintotraining
andevaluationsubsets. Wethencomputethecovariancematrixofthetrainingandevaluationsubsets
separately for the first and second classes. New synthetic data is generated by sampling from a
multivariateGaussiandistributionwithzeromeanandthecorrespondingcovariancematrix. This
synthetic data is used to train the model to distinguish between the two classes (i.e. classify the
Gaussians). Whentrainedonrealimages,themodelistestedonrealimagesandnotgaussians.
Theoptimizationobjectiveisthebinarycross-entropyloss. TheFCarchitectureconsistsof3dense
layers with 2048 units each utilizing ReLU activations, followed by a softmax output layer. For
3Homogeneity,i.e.f(Lθ)=Laf(θ),wheref istheoutputofthenetwork,θarethenetworkparameters,
andL,a∈R,aquadraticnetworksatisfiesthisrequirementasitiscomposedofmonomials.
7Flip Eigenvalues, FC, CIFAR10 Flip Eigenvectors, FC, CIFAR10 Flip Eigenvalues, ResNet18, CIFAR10 Flip Eigenvectors, ResNet18, CIFAR10
Eigenvalue Threshold Eigenvector Threshold Eigenvalue Threshold Eigenvector Threshold
Flipping Point Analysis - Gaussian, Real, and Optimal
FC, FMNIST ResNet18, FMNIST FC, CIFAR10 ResNet18, CIFAR10
Gaussian Real Gaussian Real Gaussian Real BOC Gaussian Real BOC
102
101
100
0
1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9
Figure4: TheclassificationflippingtestontheFMNISTandCIFAR10datasetsbetweenclass0and
otherclasses. Toprow: trainingandtestsonGMMsgeneratedfromtheclasscovariancematrices.
Middlerow: trainingandtestsonrealimagesafterwhitening,rescalingandcoloring. Bottomrow:
eigenvectorthresholdattheflippingpoint,forFMNISTandCIFAR10,usingGMMsandrealimages,
wheneithertrainingwithaNNorpredictingwiththeBOC.Bluecolumnsindicatetheflippingpoint
onGMMclassification,redindicatesthesametestonrealimages,andgreenshowsthepredictions
oftheBOCgivenonlybyinformationonthecovariancematrices,asgivenbyEq.(3). TheBOCis
onlyshownforCIFAR10,duetonumericalinstabilities.
CNNs,weemployResNet-18[Heetal.,2015]. AllmodelsareoptimizedwiththeAdamoptimizer,a
learningrateof0.001,andCosineAnnealingLRschedulingover50epochswithabatchsizeof256.
Thesyntheticdatasetscontain1,280imagesperclass,split80-20fortrainingandvalidation. Our
computationalresourcewasasingleNVIDIAGeForceGTX1650GPU,with16GBRAM.
6.1 ∆αTestsonGMMs
Wefirstperformanexperimenttoquantitativelyevaluateanetwork’sabilitytoutilizecovariance
matrixeigenvaluescalingforclassificationtasks. Specifically,tworandomcovariancematricesare
constructedusingEq.(8),withonecovariancehasthespectralscalingofαandtheotherofα+∆α.
Critically,bothmatricessharethesamebasisofeigenvectors. Approximately500samplesarethen
drawnfromeachcovariancematrix.
Amodelistrainedtoclassifybetweenthetwoclassesdefinedbythedistinctcovariancematrices.
Fig.3showstheresultsforafullyconnectednetwork,aswellasforaResNet18architecture[He
etal.,2015]. Toassesstheperformancevariability,theresultsareaveragedoveranensembleoffive
independenttrainingruns.
Byvaryingonlythescalingexponentbetweenthecovarianceswhilekeepingtheeigenvectorsfixed,
this experiment protocol directly tests a network’s capability to leverage the eigenvalue scaling
impartedbythecovariancematrixfordiscriminativelearning. Thestabilityoftheclassifications
indicate the degree to which these deep models can extract and utilize the intrinsic geometrical
informationencodedinthecovariancedescriptors.
6.2 FlipTestsonGMMsConstructedfromRealData
Next,weconsidertheroleofeigenvectorsandeigenvaluesinclassifyingoverlappingGMMs,aswell
asrealimages. Itisknownthattheeigenvectorsandeigenvaluesofthedatacovariancematrixreveal
differentaspectsofthedata. Inordertounderstandwhichofthetwocontainspertinentinformation
forNNclassificationperformance,weconsiderthefollowingsetup: westudytwodatasets,with
covariance matrices C and C , and decompose each one of them to a set of eigenvectors and
1 2
8
ycaruccA
ycaruccA
tnioP
pilF
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccAeigenvalues:
(cid:8) λ1,v1(cid:9) ,(cid:8) λ2,v2(cid:9)
. Wethenconstructanewcovariancematrixbycombiningsome
i i i i
oftheeigenvaluesandeigenvectorsofC andC . Toquantifytheratioofeigenvectors/eigenvalues
1 2
coming from each class, we define two threshold indices τ for the eigenvalues and τ for the
λ v
eigenvectors,whereeachofthemisanintegerbetween0andthedimensionofC ,C ,givenbyd.
1 2
Thenewcovariancematrixreads:
C =VTΛV , (16)
τλ,τv
whereV =V andΛ=Λ arethenewbasis,andeigenvalues,respectively,whicharecomposed
τv τλ
byamixturebetweenthetwoclasses:
(cid:26) v1 i≤τ (cid:26) λ1 i≤τ
v = i , λ = i (17)
i v2 τ <i i λ2 τ <i.
i i
Thisprocessisdenotedasafliptest,illustratedinFig.8.
It is worth noting that when composing the rotation matrix V, we must ensure that it is nearly
orthogonaltomaintainthepropertiesofatruebasis,whichisnotguaranteedinourscheme. However,
wefindthatthismatrixisveryclosetoorthogonalinthefollowingsense: wedefinethefollowing
error,andrequiree≪1
e≡max1(cid:12) (cid:12)|V†V −I|(cid:12) (cid:12) , (18)
τ d F
where∥·∥ istheFrobeniusnorm.Eq.(18)setsanupperboundfortheerroroverallofthethresholds
F
τ ∈[0,d]. Wepracticallyfoundthate∼0.018,whilecomputingitforarandomlygeneratedmatrix
composedofunitnormvectorsgivese∼1.
InFig.4(toprow),weshowtheresultsoftrainingaFCandResNet18networkstoclassifyGMMs
constructedfromdifferentC ,C andtestedonsamplesdrawnfromC . Thebaselineresults
1 2 τλ,τv
arethatthenetworkclassifiessamplesascomingfromC ifC hasboththeeigenvaluesand
1 τλ,τv
eigenvectors of C . When constructing C with the eigenvectors of C and the eigenvalues
1 τλ,τv 1
of C , the network still classifies samples as coming from C . This is not the case when taking
2 1
the eigenvalues from C but some of the eigenvectors from C . Evidently, it requires a O(100)
1 2
eigenvectorsfromC toconvincethenetworkthatthesamplescamefromC .
2 2
InFig.4(bottomrow),weshowthatthisresultisarchitecturedependent,buttheexistenceofan
eigenvectorflippingpointpersiststhroughoutourexperiments. Wefurthercomparethisresultwith
theBOCprediction,showingthatindeedtheBOCrequiresmoreeigenvectorstobeflippedbefore
determiningthatsamplesbelonginC ratherthanC . ThisisshownindetailinFig.11.
2 1
WefurtheranalyzetheabovefliptestsfordifferentdatasizesusedtoconstructC andC . Wefind
1 2
thattheresultsdonotchangeaboveacertainnumberofsamples,asthecovariancematricesarewell
approximatedbeyondthatpoint. Fig.12showstheresults.
6.3 FlipTestsonRealData
Toensurethattheoverlappingmixturemodelholdsforreal-data,weexaminedtheeffectofaddingthe
vectorclassmeanscomputedfromtheCIFAR10andFMNISTdatasets. Weobservednoqualitative
changeintheresults,indicatingthattheassumptionofzeromeanisjustifiedinthesecases.
Inordertoperformthesameflippingtestsonrealimages,wesimplyperformawhiteningtransfor-
mation,followedbyarescaling,andthenrotatingtothebasisofchoice[Belhumeuretal.,1997].
Foranillustrationoftheresultingimagessee Fig.9andFig.10. Thisprocessensuresthattheclass
covariancematricesofthedatahavebeenmatchedtoourdesign,butinherentlyaffectsallthehigher
momentsoftheunderlyingdistributioninanunpredictableway. Interestingly,ascanbeseeninFig.4
(toprow),theoccurrenceofaclearflippingpointwhichdependsoneigenvectorsandnoteigenvalues
ismanifestinrealimages. InFig.4(bottomrow)weshowacomparisonbetweenthethresholds
forGMMsandrealimages,wheretheresultsindicateaclosesimilarity,particularlyforResNet18
trainedonCIFAR10,whichcanbeattributedtoitshighexpressivity.
7 ConclusionsandLimitations
WestudiedbinaryclassificationofhighdimensionaloverlappingGMMdataasaframeworktomodel
real-worldimagedatasetsandtoquantifytheimportancefortheclassificationtasksoftheeigenvalues
andtheeigenvectorsofthedatacovariancematrix. Weshowedthatdeepneuralnetworkstrained
forclassification,learnpredictorsthatapproximatetheBayesoptimalclassifiers,anddemonstrated
9that the decision thresholds for networks trained on authentic data correlate with the covariance
eigenvectors rather than the eigenvalues, compatible with the GMM analysis. Our results reveal
newtheoreticalinsightstotheneuralnetworks’abilitytoperformprobabilisticinferenceanddistill
statisticalpatternsfromcomplexdistributions.
Limitations: Firstly,wefocusedontheγ ≪1regimeoftheestimatedclasscovariances,whilein
manyreal-worldcases,theempiricallimitmaybemoreapplicableLeviandOz[2023b]. InSec.5.2,
our ansatz neglected the dependence of λ on d/N and the spectral density, which nonlinearly
determinesthenumberofpointswhichlieonthedecisionsurface. Itmightbepossibletoderive
asimilarresulttoHsuetal.[2022]bytakingafeaturemapapproachtothequadraticnet,butwe
postponethistofuturework. Finally,whilewequantifiedthesignificanceofthedetailedstructure
ofthedatacovariancematrix,i.e. itseigenvaluesandeigenvectorsandtheirrelativeimportancefor
classification,wearestillleftwiththeopenquestionregardingtherelativeimportanceofthehigher
momentsofthedistribution,whichweleaveforafuturestudy.
8 Acknowledgements
WewouldliketothankAmirGloberson,YohaiBar-Sinai,YueLuandBrunoLoureiroforuseful
discussions. NLwouldliketothankG-Researchfortheawardofaresearchgrant,aswellasthe
CERN-THdepartmentfortheirhospitalityduringvariousstagesofthiswork. TheworkofY.O.is
supportedinpartbyIsraelScienceFoundationCenterofExcellence.
References
DavidDonohoandJaredTanner. Observeduniversalityofphasetransitionsinhigh-dimensional
geometry, with implications for modern data analysis and signal processing. Philosophical
TransactionsoftheRoyalSocietyA:Mathematical,PhysicalandEngineeringSciences,367(1906):
4273–4293,2009.
SatishBabuKoradaandAndreaMontanari.Applicationsofthelindebergprincipleincommunications
andstatisticallearning. IEEETransactionsonInformationTheory,57(4):2440–2450,2011. doi:
10.1109/TIT.2011.2112231.
HatefMonajemi,SinaJafarpour,MatanGavish,DavidL.Donoho,SivaramAmbikasaran,Sergio
Bacallado,DineshBharadia,YuxinChen,YoungChoi,MainakChowdhury,SohamChowdhury,
Anil Damle, Will Fithian, Georges Goetz, Logan Grosenick, Sam Gross, Gage Hills, Michael
Hornstein,MilindaLakkam,JasonLee,JianLi,LinxiLiu,CarlosSing-Long,MikeMarx,Akshay
Mittal,AlbertNo,RezaOmrani,LeonidPekelis,JunjieQin,KevinRaines,ErnestRyu,Andrew
Saxe, DaiShi, KeithSiilats, DavidStrauss, GaryTang, ChaojunWang, ZoeyZhou, andZhen
Zhu. Deterministic matrices matching the compressed sensing phase transitions of gaussian
randommatrices. ProceedingsoftheNationalAcademyofSciences,110(4):1181–1186,2013. doi:
10.1073/pnas.1219540110.
Emmanuel J Candès, Pragya Sur, et al. The phase transition for the existence of the maximum
likelihoodestimateinhigh-dimensionallogisticregression. TheAnnalsofStatistics,48(1):27–42,
2020.
PeterL.Bartlett,PhilipM.Long,GáborLugosi,andAlexanderTsigler. Benignoverfittinginlinear
regression. ProceedingsoftheNationalAcademyofSciences,117(48):30063–30070,2020. ISSN
0027-8424. doi: 10.1073/pnas.1907378117.
AlexanderMaloney,DanielA.Roberts,andJamesSully. Asolvablemodelofneuralscalinglaws,
2022.
JaredKaplan, SamMcCandlish, TomHenighan, TomB.Brown, BenjaminChess, RewonChild,
ScottGray,AlecRadford,JeffreyWu,andDarioAmodei.Scalinglawsforneurallanguagemodels,
2020.
MohamedElAmineSeddik,CosmeLouart,MohamedTamaazousti,andRomainCouillet. Random
matrixtheoryprovesthatdeeplearningrepresentationsofgan-databehaveasgaussianmixtures,
2020.
10AndreaMontanariandBasilN.Saeed. Universalityofempiricalriskminimization. InProceedings
ofThirtyFifthConferenceonLearningTheory,volume178ofProceedingsofMachineLearning
Research,pages4310–4312.PMLR,02–05Jul2022.
Xiaoyi Mai and Zhenyu Liao. High-dimensional classification via empirical risk minimization:
Improvementsandoptimality. arXiv: 1905.13742,2019.
FrancescaMignacco,FlorentKrzakala,YueLu,PierfrancescoUrbani,andLenkaZdeborova. The
roleofregularizationinclassificationofhigh-dimensionalnoisygaussianmixture. InInternational
ConferenceonMachineLearning,pages6874–6883.PMLR,2020a.
Hossein Taheri, Ramtin Pedarsani, and Christos Thrampoulidis. Optimality of least-squares for
classificationingaussian-mixturemodels. In2020IEEEInternationalSymposiumonInformation
Theory(ISIT),pages2515–2520.IEEE,2020.
GaneshRamachandraKiniandChristosThrampoulidis. Phasetransitionsforone-vs-oneandone-vs-
alllinearseparabilityinmulticlassgaussianmixtures. InICASSP2021-2021IEEEInternational
ConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages4020–4024.IEEE,2021.
KeWangandChristosThrampoulidis.Benignoverfittinginbinaryclassificationofgaussianmixtures.
InICASSP2021-2021IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing
(ICASSP),pages4030–4034.IEEE,2021.
Maria Refinetti, Sebastian Goldt, Florent Krzakala, and Lenka Zdeborová. Classifying high-
dimensionalgaussianmixtures: Wherekernelmethodsfailandneuralnetworkssucceed,2021.
BrunoLoureiro,GabrieleSicuro,CedricGerbelot,AlessandroPacco,FlorentKrzakala,andLenka
Zdeborova. Learning gaussian mixtures with generalized linear models: Precise asymptotics
inhigh-dimensions. InAdvancesinNeuralInformationProcessingSystems,volume34,pages
10144–10157,2021a.
XiaoyiMai,ZhenyuLiao,andRomainCouillet. ALargeScaleAnalysisofLogisticRegression:
AsymptoticPerformanceandNewInsights.InICASSP2019-2019IEEEInternationalConference
onAcoustics, SpeechandSignalProcessing(ICASSP),pages3357–3361, May2019. doi: 10.
1109/ICASSP.2019.8683376. ISSN:2379-190X.
FrancescaMignacco,FlorentKrzakala,YueM.Lu,andLenkaZdeborová. Theroleofregularization
inclassificationofhigh-dimensionalnoisygaussianmixture. 2020b. doi: 10.48550/ARXIV.2002.
11544. URLhttps://arxiv.org/abs/2002.11544.
Zeyu Deng, Abla Kammoun, and Christos Thrampoulidis. A model of double descent for high-
dimensionalbinarylinearclassification. InformationandInference: AJournaloftheIMA,11(2):
435–495,June2022. ISSN2049-8772. doi: 10.1093/imaiai/iaab002. URLhttps://doi.org/
10.1093/imaiai/iaab002.
ChristosThrampoulidis,SametOymak,andMahdiSoltanolkotabi. Theoreticalinsightsintomulti-
classclassification: ahigh-dimensionalasymptoticview. InProceedingsofthe34thInternational
ConferenceonNeuralInformationProcessingSystems,NIPS’20,pages8907–8920,RedHook,
NY,USA,December2020.CurranAssociatesInc. ISBN9781713829546.
BrunoLoureiro,GabrieleSicuro,CédricGerbelot,AlessandroPacco,FlorentKrzakala,andLenka
Zdeborová. Learninggaussianmixtureswithgeneralisedlinearmodels: Preciseasymptoticsin
high-dimensions. 2021b. doi: 10.48550/ARXIV.2106.03791. URLhttps://arxiv.org/abs/
2106.03791.
RomainCouillet,ZhenyuLiao,andXiaoyiMai. Classificationasymptoticsintherandommatrix
regime. In201826thEuropeanSignalProcessingConference(EUSIPCO),pages1875–1879,
2018. doi: 10.23919/EUSIPCO.2018.8553034.
Zhenyu Liao and Romain Couillet. A large dimensional analysisof least squares support vector
machines. IEEE Transactions on Signal Processing, 67(4):1065–1074, February 2019. ISSN
1941-0476. doi: 10.1109/tsp.2018.2889954. URLhttp://dx.doi.org/10.1109/TSP.2018.
2889954.
11AblaKammounandRomainCouillet. Covariancediscriminativepowerofkernelclusteringmethods.
ElectronicJournalofStatistics,17(1):291–390,2023. doi: 10.1214/23-EJS2107. URLhttps:
//doi.org/10.1214/23-EJS2107.
BrunoLoureiro,GabrieleSicuro,CedricGerbelot,AlessandroPacco,FlorentKrzakala,andLenka
Zdeborová. Learninggaussianmixtureswithgeneralizedlinearmodels: Preciseasymptoticsin
high-dimensions.InM.Ranzato,A.Beygelzimer,Y.Dauphin,P.S.Liang,andJ.WortmanVaughan,
editors, AdvancesinNeuralInformationProcessingSystems, volume34, pages10144–10157.
Curran Associates, Inc., 2021c. URL https://proceedings.neurips.cc/paper_files/
paper/2021/file/543e83748234f7cbab21aa0ade66565f-Paper.pdf.
Alessandro Ingrosso and Sebastian Goldt. Data-driven emergence of convolutional structure in
neuralnetworks. ProceedingsoftheNationalAcademyofSciences,119(40),September2022.
ISSN1091-6490. doi: 10.1073/pnas.2201854119. URLhttp://dx.doi.org/10.1073/pnas.
2201854119.
KhalilElkhalil,AblaKammoun,RomainCouillet,TareqYAl-Naffouri,andMohamed-SlimAlouini.
Asymptoticperformanceofregularizedquadraticdiscriminantanalysisbasedclassifiers. In2017
IEEE27thInternationalWorkshoponMachineLearningforSignalProcessing(MLSP),pages
1–6.IEEE,2017.
BenyaminGhojoghandMarkCrowley. Linearandquadraticdiscriminantanalysis: Tutorial,2019.
Abhranil Das and Wilson S. Geisler. A method to integrate and classify normal distributions.
JournalofVision,21(10):1,September2021. ISSN1534-7362. doi: 10.1167/jov.21.10.1. URL
http://dx.doi.org/10.1167/jov.21.10.1.
AchimSchilling,AndreasMaier,RichardGerum,ClausMetzner,andPatrickKrauss. Quantifying
theseparabilityofdataclassesinneuralnetworks. NeuralNetworks,139:278–293,July2021.
ISSN0893-6080. doi: 10.1016/j.neunet.2021.03.035. URLhttp://dx.doi.org/10.1016/j.
neunet.2021.03.035.
AlbertoFerrari. Anoteonsumanddifferenceofcorrelatedchi-squaredvariables,2019.
GiulioBiroliandMarcMézard. Generativediffusioninverylargedimensions. JournalofStatistical
Mechanics: TheoryandExperiment,2023(9):093402,September2023. ISSN1742-5468. doi:
10.1088/1742-5468/acf8ba. URLhttp://dx.doi.org/10.1088/1742-5468/acf8ba.
Malik Tiomoko, Romain Couillet, Eric Moisan, and Steeve Zozor. Improved estimation of the
distancebetweencovariancematrices. InICASSP2019-2019IEEEInternationalConference
onAcoustics,SpeechandSignalProcessing(ICASSP),pages7445–7449,2019. doi: 10.1109/
ICASSP.2019.8682621.
GuodongZhang,ChaoqiWang,BowenXu,andRogerB.Grosse. Threemechanismsofweightdecay
regularization. CoRR,abs/1810.12281,2018. URLhttp://arxiv.org/abs/1810.12281.
NoamLeviandYaronOz. Theunderlyingscalinglawsanduniversalstatisticalstructureofcomplex
datasets. arXivpreprintarXiv:2306.14975,2023a.
Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mezard,
and Lenka Zdeborova. Learning curves of generic features maps for realistic datasets with a
teacher-studentmodel. InAdvancesinNeuralInformationProcessingSystems,volume34,2021d.
BrunoLoureiro,CedricGerbelot,HugoCui,SebastianGoldt,FlorentKrzakala,MarcMezard,and
LenkaZdeborova. Learningcurvesofgenericfeaturesmapsforrealisticdatasetswithateacher-
student model. Journal of Statistical Mechanics: Theory and Experiment, 2022(11):114001,
nov2022. doi: 10.1088/1742-5468/ac9825. URLhttps://doi.org/10.1088%2F1742-5468%
2Fac9825.
AdityanarayananRadhakrishnan,MikhailBelkin,andCarolineUhler.Wideanddeepneuralnetworks
achieveconsistencyforclassification. ProceedingsoftheNationalAcademyofSciences,120(14),
March2023. ISSN1091-6490. doi: 10.1073/pnas.2208779120. URLhttp://dx.doi.org/10.
1073/pnas.2208779120.
12KishoreJaganathan,YoninaC.Eldar,andBabakHassibi. Phaseretrieval: Anoverviewofrecent
developments,2015.
JonathanDong,LorenzoValzania,AntoineMaillard,Thanh-anPham,SylvainGigan,andMichael
Unser. Phaseretrieval: Fromcomputationalimagingtomachinelearning: Atutorial. IEEESignal
ProcessingMagazine,40(1):45–57,2023. doi: 10.1109/MSP.2022.3219240.
EmmanuelJ.Candes,XiaodongLi,andMahdiSoltanolkotabi. Phaseretrievalviawirtingerflow:
Theory and algorithms. CoRR, abs/1407.1065, 2014. URL http://arxiv.org/abs/1407.
1065.
YuxinChen,YuejieChi,JianqingFan,andCongMa. Gradientdescentwithrandominitialization:
fastglobalconvergencefornonconvexphaseretrieval. MathematicalProgramming,176(1–2):
5–37,February2019. ISSN1436-4646. doi: 10.1007/s10107-019-01363-6. URLhttp://dx.
doi.org/10.1007/s10107-019-01363-6.
LucaArnaboldi,FlorentKrzakala,BrunoLoureiro,andLudovicStephan. Escapingmediocrity: how
two-layernetworkslearnhardgeneralizedlinearmodelswithsgd,2024.
SimonMartin,FrancisBach,andGiulioBiroli. Ontheimpactofoverparameterizationonthetraining
ofashallowneuralnetworkinhighdimensions,2023.
RoiLivni,ShaiShalev-Shwartz,andOhadShamir. Onthecomputationalefficiencyoftrainingneural
networks,2014.
Mohammadreza Soltani and Chinmay Hegde. Towards provable learning of polynomial neural
networksusinglow-rankmatrixestimation. InAmosStorkeyandFernandoPerez-Cruz,editors,
ProceedingsoftheTwenty-FirstInternationalConferenceonArtificialIntelligenceandStatistics,
volume84ofProceedingsofMachineLearningResearch,pages1417–1426.PMLR,09–11Apr
2018. URLhttps://proceedings.mlr.press/v84/soltani18a.html.
ZiweiJiandMatusTelgarsky. Directionalconvergenceandalignmentindeeplearning,2020.
KaifengLyuandJianLi. Gradientdescentmaximizesthemarginofhomogeneousneuralnetworks,
2020.
DanielHsu,VidyaMuthukumar,andJiXu.Ontheproliferationofsupportvectorsinhighdimensions,
2022.
AlexKrizhevsky. Learningmultiplelayersoffeaturesfromtinyimages. UniversityofToronto,05
2012. URLhttps://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf.
HanXiao,KashifRasul,andRolandVollgraf. Fashion-mnist: Anovelimagedatasetforbenchmark-
ingmachinelearningalgorithms. arXiv: 1708.07747,2017. doi: 10.48550/ARXIV.1708.07747.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition,2015.
PeterN.Belhumeur,JoãoP.Hespanha,andDavidJ.Kriegman. Eigenfacesvs.fisherfaces: Recogni-
tionusingclassspecificlinearprojection. IEEETrans.PatternAnal.Mach.Intell.,19(7):711–720,
jul1997. ISSN0162-8828. doi: 10.1109/34.598228. URLhttps://doi.org/10.1109/34.
598228.
NoamLeviandYaronOz. Theuniversalstatisticalstructureandscalinglawsofchaosandturbulence.
arXivpreprintarXiv:2311.01358,2023b.
VardanPapyan,X.Y.Han,andDavidL.Donoho. Prevalenceofneuralcollapseduringtheterminal
phase of deep learning training. Proceedings of the National Academy of Sciences, 117(40):
24652–24663,September2020. ISSN1091-6490. doi: 10.1073/pnas.2015509117. URLhttp:
//dx.doi.org/10.1073/pnas.2015509117.
13A NeuralCollapseonOverlappingGMMData
Here,weexploretheconceptofNeuralCollapse(NC)onoverlappingGMMsgeneratedfromreal
data. Practically, we employ the metrics given in Papyan et al. [2020] to measure the NC when
trainingaResNet18ongaussianversionsofCIFAR10images. InFig.5,weshowthatNCoccurson
overlappingGMMsevenwhentheclassmeanvectorofthedatahasbeensettozero,indicatingthat
expressivenetworksareabletoperformnonlineartransformationsthatgenerateclassmeans,where
nonewerepresent,andatthefinallayerstillperformlinearclassification.
VariousmetricsofNeuralCollapse,asgivenbyPapyanetal.[2020],areshownforGMMsgenerated
fromtwoclassesofCIFAR10. Here,weuse5000samplesperclass,trainingaResNet18toperform
binary classification using cross-entropy loss, on GMMs generated from greyscale versions of
CIFAR10 images. Each sample has dimension 1024, and the training is done using the default
specificationsprovidedinthesupplementarycodeofPapyanetal.[2020]
Training Error NC4: Convergence to NCC Training Error NC4: Convergence to NCC
50 50 0.5
40 0.8 40 0.4
30 0.6 30 0.3
20 0.4 20 0.2
10 0.2 10 0.1
0 0.0 0 0.0
0 100 200 300 0 100 200 300 0 100 200 300 0 100 200 300
Epoch Epoch Epoch Epoch
NC1: Activation Collapse NC2: Equinorm NC1: Activation Collapse NC2: Equinorm
103 0.04 Class Means 103 0.04 Class Means
102 0.03 Classifiers 102 0.03 Classifiers
101 101
100 0.02 100 0.02
101 0.01 101 0.01
102 102
0.00 0.00
0 100 200 300 0 100 200 300 0 100 200 300 0 100 200 300
Epoch Epoch Epoch Epoch
NC2: Maximal Equiangularity NC3: Self Duality NC2: Maximal Equiangularity NC3: Self Duality
0.6 C Cl la as ss s ifM iee ra sns 1.5 0.6 C Cl la as ss s ifM iee ra sns 1.5
0.4 1.0 0.4 1.0
0.2 0.5 0.2 0.5
0.0 0.0 0.0 0.0
0 100 200 300 0 100 200 300 0 100 200 300 0 100 200 300
Epoch Epoch Epoch Epoch
Figure5: ResultsforneuralcollapseconvergencefortrainingResNet18ontwoclasses(0,7)sampled
fromGaussianversionsofCIFAR10data. Left: settingµ =µ =0. Right: µ ̸=µ ,asgiven
A B A B
bytheclassmeans. Thenetworkattains99.9%testaccuracyinbothcases. Weseenosubstantial
differenceinconvergencetoneuralcollapsemetrics,affirmingourclaimsinthemaintext.
B KKTformulation
Asdiscussedinthemaintext,wepositthattheapproachtowardsaBOCforaNNcanbeexplained
byconvergencetoaKKTpoint.
OurreasoningfollowsTheoremTheoremB.1below,whichholdsforgradientflow(i.e.,gradient
descentwithaninfinitesimallysmallstepsize). Beforestatingthetheorem,weneedthefollowing
definitions: (1) We say that gradient flow converges in direction to θ˜ if lim θ(t) = θ˜ ,
t→∞ ∥θ(t)∥ ∥θ˜∥
whereθ(t)istheparametervectorattimet;(2)WesaythatanetworkΦishomogeneousw.r.t. the
parametersθifthereexistsL>0suchthatforeveryλ>0andθ,xwehaveΦ(αθ;x)=λLΦ(θ;x).
Thus,scalingtheparametersbyanyfactorλ>0scalestheoutputsbyλL. Wenotethatessentially
anyfully-connectedorconvolutionalneuralnetworkwithReLUactivationsishomogeneousw.r.t.
theparametersθifitdoesnothaveanyskip-connections(i.e.,residualconnections)orbiasterms,
exceptpossiblyforthefirstlayer.
Theorem B.1 (Paraphrased from Lyu and Li [2020], Ji and Telgarsky [2020]). Let Φ(θ;·) be a
homogeneousReLUneuralnetwork.Considerminimizingthelogisticlossoverabinaryclassification
14
)%(
rorrE
gniniarT
}1-^bS
wS{rT
|)1-C(/1
+
soC|gvA
CCN
morf
hctamsiM
noitroporP
smroN
fo
gvA/dtS
2^||H
-
T^W||
)%(
rorrE
gniniarT
}1-^bS
wS{rT
|)1-C(/1
+
soC|gvA
CCN
morf
hctamsiM
noitroporP
smroN
fo
gvA/dtS
2^||H
-
T^W||dataset{(x ,y )}N usinggradientflow. Assumethatthereexiststimet suchthatL(θ(t ))<14.
a a a=1 0 0
Then,gradientflowconvergesindirectiontoafirstorderstationarypoint(KKTpoint)ofthefollowing
maximum-marginproblem:
1
min ∥θ′∥2 s.t. ∀i∈[n] y Φ(θ′;x )≥1. (19)
θ′ 2 a a
Moreover,L(θ(t))→0ast→∞.
The above theorem guarantees directional convergence to a first order stationary point (of the
optimizationproblem (Eq.(19))),whichisalsocalledKarush–Kuhn–Tuckerpoint,orKKTpoint
forshort. TheKKTapproachallowsinequalityconstraints,andisageneralizationofthemethodof
Lagrangemultipliers,whichallowsonlyequalityconstraints.
ThegreatvirtueofTheoremB.1isthatitcharacterizestheimplicitbiasofgradientflowwiththe
logisticlossforhomogeneousnetworks. Namely,eventhoughtherearemanypossibledirections
of θ thatclassifythedatasetcorrectly,gradientflowconvergesonlytodirectionsthatareKKT
∥θ∥
pointsofProblem(Eq.(19)). Inparticular,ifthetrajectoryθ(t)ofgradientflowundertheregime
ofTheoremB.1convergesindirectiontoaKKTpointθ˜,thenwehavethefollowing: Thereexist
λ ,...,λ ∈Rsuchthat
1 n
N
θ˜ =(cid:88) λ y ∇ Φ(θ˜;x ) (stationarity) (20)
a a θ a
a=1
∀a∈[N], y Φ(θ˜;x )≥1 (primalfeasibility) (21)
a a
λ ,...,λ ≥0 (dualfeasibility) (22)
1 N
∀a∈[N], λ =0ify Φ(θ˜;x )̸=1 (complementaryslackness) (23)
a a a
OurmaininsightisbasedonEq.(20),whichimpliesthattheparametersθ˜arealinearcombinations
ofthederivativesofthenetworkatthetrainingdatapoints. Wesaythatadatapointx isonthe
a
margin if y Φ(θ˜;x ) = 1 (i.e. |Φ(θ˜;x )| = 1) . Note that Eq. (23) implies that only samples
a a a
whichareonthemarginaffectEq.(20),sincesamplesnotonthemarginhaveacoefficientλ =0.
a
InsufficientlyhighdimensionweexpectnosamplestobeoffthemargininGMMclassification
thereforeallλ shouldbenonzeroandcontributeasimilaramount.
a
C DatasetProperties
WeshowtheeigenvaluesscalingstructureandtheInverseParticipationRatio(IPR)oftheeigenvectors.
Weseparatetheeigenvaluestothreeparts: I.Thelargeeigenvalues,II.Thebulkeigenvalues,andIII.
Thesmalleigenvalues,ascanbeseeninFig.6. Thesethreeregimesareseenalsoinrealdatasetsas
showninpreviouswork[LeviandOz,2023a,b]. Fig.6showstheeigenvaluesforeachoftheclasses
intheFMNISTandCIFAR10datasets,respectively.
TheIPRdefinedas:
(cid:32) (cid:33)−1
IPR≡ (cid:88) |v |4 , (24)
n
n
isusedtomeasurethelocalizationrate(entropy)ofavector. Fig.7showstheIPRoftheeigenvectors
ofthecovariancematricesforthedifferentclassesintheCIFAR10datasetandFMNIST.Ascanbe
seenfromtheFigure,thechangesintheeigenvaluesstructureandtheIPRissmall.
D Appendix-Flippingtestsexamples
Weincludethedifferentfiguresconcerningtheclassificationflippingtestsreferredtointhemaintext.
ThediagraminFig.8outlinesthestructureoftheflippingtests. InFig.9andFig.10weseerotated
imagesofFMNISTandCIFAR10,respectively. InFig.11,Fig.12andFigures13and14weplotthe
eigenvaluesandeigenvectorsthresholds.
4Thisensuresthatℓ(y Φ(θ(t );x ))<1foralli,i.e.atsometimeΦclassifieseverysamplecorrectly.
a 0 a
15Figure6: Left: Partitiontheeigenvectorindicestothreeregimes: thelargeeigenvalues,thebulk
andthesmalleigenvalues. Thetopfigureshowstheeigenvaluesversustheirindex,andthebottom
showthecorrespondingIPRversustheeigenvaluesindex. Center/Right: Theeigenvaluesofthe
covariance matrix for the different classes in CIFAR10/FMNIST dataset, with a comparison to
uncorrelateddata(blue). Theorangelinerepresentstheeigenvaluesofthecovariancematrixofthe
wholedatasetandthegreendashedlineshowstheaverageofthedifferentclasses. Weseethethree
regimesofeigenvalues: thelargeeigenvalues-upto±10firsteigenvalues,thebulkscaling-between
±10to±300,andthesmalleigenvalues-from±350.
CIFAR10 FMNIST
35 Class 1 Class 7 60 Class 1 Class 7
Class 2 Class 8 Class 2 Class 8
30 Class 3 Class 9 Class 3 Class 9
Class 4 Class 10 50 Class 4 Class 10
Class 5 Noise Class 5 Noise
25 Class 6 Class 6
40
20
30
15
20
10
5 10
0 0
0 20 40 60 80 0 10 20 30 40
Eigenvalue index Eigenvalue index
Figure7: ThehistogramoftheIPRofthecovariancematrixeigenvectorsforthedifferentclassesin
theCIFAR10dataset(left),andFMNIST(right),withacomparisontonoise(blue).
Thecodeforourexperimentsisprovdedinhttps://github.com/khencohen/FlippingsTests.
16
RPI RPIFigure 8: The classification flipping tests. We evaluate the model classification performance on
differentcovariancematrices. Left-buildinganewcovariancematrixbyflippingtheeigenvectorsup
toacertainthreshold. Right-buildinganewcovariancematrixbyflippingtheeigenvaluesuptoa
certainthreshold. Thismethodprovidesinformationabouttherelativeimportanceofthedifferent
eigenvectorsandeigenvaluesforthemodeltomakeaclassificationdecision.
Rotated images comparison. ResNet18, FMNIST
c0 vs c1 c0 vs c2 c0 vs c3 c0 vs c4 c0 vs c5 c0 vs c6 c0 vs c7 c0 vs c8 c0 vs c9
Figure9: ExamplesofrotatedimagesintheFMNISTdataset. Eachofthesamplesofclass0(shirt,
toprow),wasrotatedusingthecovariancematrixofanotherclass(bottomrow),andthemiddlerow
showstheresultoftherotation. Inallofourtests,foranyrotationofclass0sample,withclassc
basis,themodelclassifiedtherotatedimageasasamplefromclassc.
17Rotated images comparison. ResNet18, CIFAR10
c0 vs c1 c0 vs c2 c0 vs c3 c0 vs c4 c0 vs c5 c0 vs c6 c0 vs c7 c0 vs c8 c0 vs c9
Figure10:ExamplesofrotatedimagesintheCIFARdataset. Eachofthesamplesofclass0(airplane,
toprow),wasrotatedusingthecovariancematrixofanotherclass(bottomrow),andthemiddlerow
showstheresultoftherotation. Inallofourtests,foranyrotationofclass0sample,withclassc
basis,themodelclassifiedtherotatedimageasasamplefromclassc.
Flip eigenvectors, Classifier, CIFAR10 Flip eigenvalues, Optimal, CIFAR10
100 100
A = 0, 0
80 A = 0, 1 80 A = 0, 1
A = 0, 2
A = 0, 2
A = 0, 3
60 A = 0, 3 60 A = 0, 4
A = 0, 4
A = 0, 5
A = 0, 5
A = 0, 6
40 A = 0, 6 40
A = 0, 7
A = 0, 7
A = 0, 8
A = 0, 8
20 20 A = 0, 9
A = 0, 9
0 0
0 200 400 600 800 1000 0 200 400 600 800 1000
Threshold Threshold
Figure11: Theclassificationeigenvector(left)andeigenvalues(right)flippingtestontheoptimal
classifier. CIFAR10dataset,comparingclass0withotherclasses.
Flip Eigenvectors, FC, CIFAR10, (0, 5) Flip Eigenvectors, ResNet18, CIFAR10, (0, 5)
100
100
90 90
100
80 1 20 00 0 80 200
70 300 70 300
60 400 60 400
500 500
50 600 50 600
40 700 40 700
23 00 8 9 10 0 00 0
00
23 00 8 9 10 0 00 0
00
10 10
0
0
0 100 101 102 103 104 0 100 101 102 103 104
Eigenvector Threshold Eigenvector Threshold
Figure12: TheclassificationeigenvectorflippingtestontheGaussiandatasetbetweenclass0and
class5,fordifferentnumberofsamples. FCmodel(left)andResNet18(right),CIFAR10dataset.
18
ycaruccA
ycaruccA ycaruccA
ycaruccAFigure13: TheclassificationeigenvaluesflippingtestontherotatedFMNISTdatasetbetweenclass0
andotherclassesfortheFCmodel.
Figure14: TheclassificationeigenvaluesflippingtestontherotatedFMNISTdatasetbetweenclass0
andotherclassesfortheResNet18model.
19