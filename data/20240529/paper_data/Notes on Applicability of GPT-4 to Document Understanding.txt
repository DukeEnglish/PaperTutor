Notes on Applicability of GPT-4 to Document Understanding
LukaszBorchmann
Snowflake
lukasz.borchmann@snowflake.com
Abstract GPT-4Family. Asthecomparisonincludesmod-
elstrainedusingdifferentrecipesandarchitectures,
Weperformamissing,reproducibleevaluation
each should be considered and optimized sepa-
ofallpubliclyavailableGPT-4familymodels
rately. Theself-reportedresultsfromMarchwere
concerningtheDocumentUnderstandingfield,
achieved with release-day models with image in-
whereitisfrequentlyrequiredtocomprehend
textspacialarrangementandvisualcluesinad- putsfunctionallyavailabletoahandfulofselected
ditiontotextualsemantics. Benchmarkresults users. Afterward,modelswereincrementallyim-
indicatethatthoughitishardtoachievesatis- provedandreplacedwithnewervariants,including
factory results with text-only models, GPT-4 theJunereleaseofnewertextualGPT-4(with8k
VisionTurboperformswellwhenoneprovides
and32ktokenscontextwindows)andNovember
both text recognized by an external OCR en-
releasesof’turbo’variants(capableofconsuming
gineanddocumentimagesontheinput. Evalu-
upto128ktokens,includingtokenizedimages).
ationisfollowedbyanalysesthatsuggestpos-
siblecontaminationoftextualGPT-4models
andindicatethesignificantperformancedrop
forlengthydocuments.
1 Introduction
DocumentUnderstandingisthecapacitytoconvert
March June November
a document into meaningful information, which
commonlyinvolvesintegratingcluesrepresented Incremental model updates
by layout, non-textual elements, and text style
(Borchmann et al., 2021). The advent of LLMs
andlargemodelsabletoprocessdocumentimages
self-reported
motivates the inquiry of how well they perform
in this scenario compared to specialized models
2 Experiments
developedinparallel.
Though many tasks are commonly considered
We consider DocVQA (Mathew et al., 2020), In-
undertheumbrellatermofDocumentUnderstand-
fographicsVQA(Mathewetal.,2022),SlideVQA
ing, we limit the evaluation to Document Visual
(Tanakaetal.,2023),andDUDE(VanLandeghem
QuestionAnswering,whichisthemostconveni©e 2n023t Snowflake Inc. All Rights Reserved
et al., 2023) datasets, as they represent the entire
concerningbothLLM-basedchatassistantsandthe
spectrumofdocumenttypes:
fact every piece of information within the docu-
mentcanberequestedbyexpressingaquestionor
DocVQA InfoVQA DUDE SlideVQA
specifyinginstructionsinnaturallanguage.
Text-intensive ✓ ✗ ✓ ✗
Whereas the press release of the GPT-4 model Vision-intensive ✗ ✓ ✗ ✓
(OpenAI, 2023) mentioned scores on two docu- Multi-page ✗ ✗ ✓ ✓
mentVQAdatasets, thedetailsremainunknown,
andwhethersuchresultsareachievablewithcom- Thissectiondiscussesresultsforthebestprompt,
mercially available API is vague. We intend to image resolution, and OCR combinations deter-
bridgethisgapwithadetailedtechnicalreportand minedonvalidationsets(AppendixesA-E),aswell
reproducibleevaluationprocedure. asthecrucialfindingsofthesestudies.
4202
yaM
82
]LC.sc[
1v33481.5042:viXraTable1: Bestresultsachievedconcerningpromptandparametersoptimization(coveredinAppendixBandE).
ANLSscoresexceptforSlideVQA,wheretheexactmatchproposedbyauthorsisreported(bothare,infact,variants
oflooseaccuracy). SeeTable9intheAppendixfordetailsonthebestmodelsreferenced.
# Model Version Vision/Text DocVQA InfoVQA SlideVQA DUDE
1 TURBO 1106-vision-preview ✓/✗ 84.6 67.3 55.1 53.5
V
2 TURBO +OCR 1106-vision-preview ✓/✓ 87.4 71.9 57.3 53.3
V
3 TURBO+OCR 1106-preview ✗/✓ 78.2 54.3 45.9 48.1
4 8K+OCR 0613 ✗/✓ 77.5 54.5 41.4 48.0
5 32K+OCR 32k-0613 ✗/✓ 79.5 52.8 44.7 48.8
6 VISION (reported) ✓/✗ 88.4 75.1 — —
93.1 75.7 37.7 53.4
7 Bestmodel
Qwen-VL InternVL InstructDr GRAM
8 Humanperformance 98.1 97.2 89.8 74.8
Results. Comparison in Table 1 indicates that Counterintuitively,asimilaranalysisperformed
TURBO and TURBO +OCR modelsincorporat- onInfographicsVQAshowsthatgainsfromOCR
V V
ingvisualaspectsofthedocument(and,indirectly, textpresencearemostapparentacrossvisualarti-
itslayout)outperformheaviertext-onlymodels. facts.. We hypothesize it can be attributed to the
Thoughwecouldestablishstate-of-the-artper- richer interplay between textual and graphic ele-
formance on the SlideVQA and DUDE datasets, ments, e.g., here a higher proportion of evidence
results achieved on the well-established task of requires visual and textual content to be compre-
DocVQAseempoorcomparedtoscoresreported hendedsimultaneously(36%comparedto31%).
intheliterature.
EvidenceLocation. Ithasbeenshownthatlan-
Importantly, we show that the GPT-4 Vision
guage models don’t robustly use information in
model, which can consume both text and vision
long input contexts (Liu et al., 2023). Because
modalities,benefitsfromprovidingtextrecognized
theSlideVQAdatasetprovidesinformationonthe
by an external OCR engine as a part of the in-
positionoftherequestedinformationwithinadoc-
put. Despite best-effort optimization reported in
ument, we can investigate how the performance
Appendixes A-E, we could not match the undis-
of the model changes depending on the evidence
tilledGPT-4 VISION scoresinapixel-onlysetup.
location.
Giventheavailablepublicvision-and-textmodel,
ResultsinFigure1indicatetheprimacybiasfor
this level of performance seems achievable only
allofthemodels,i.e.,achievedscoresarehighest
whenimagesareaccompaniedbyrecognizedtext.
whenrelevantinformationoccursatthebeginning
oftheinput. Moreover,theysuggestrecencybias
3 ErrorAnalysis
for TURBO ,asthescoresslightlyimproveaswe
V
Weleveragethedatasets’diagnosticcategoriesand movetotheendoftheinputdocument.
metadatatoanalyzemodels’performancedepend-
Answer Type. As the DUDE dataset contains
ingontheinputandevidencefeatures.
questions requiring a list as an answer or state-
Evidence. Unsurprisingly, the observed advan- mentsthatitisnon-answerableconcerningthein-
tage of TURBO +OCR is echoed in scores de- putdocument,weanalyzetheperformanceinthese
V
pendingonhowtheinputdocumentrepresentsthe categories(Table3).
information. WeseethatOCR-providedtextsignif- Interestingly, though list answers performance
icantlyimprovesDocVQAresultsiftherequested followsthegeneralpattern,non-turbovariantsof
valueispresentinfreetextandtext-richelements GPT-4 appear significantly better in identifying
structuredinforms,lists,andtables(Table2). At non-answerable questions — even though their
the same time, improvement is less visible if the overallperformanceislowerduetolackofvisual
evidenceisprovidedasafigureorimage. cognition.Table2: Impactofanswerevidence(DocVQAandInfographicsVQA).WhenOCRtextisprovided,significant
improvementsinDocVQAoccurwithfreetext,forms,lists,andtables,whileinInfographicsVQA,gainsaremost
notablewithvisualartifacts,possiblyduetothericherinterplayoftextualandgraphicelements.
DocVQA InfographicsVQA
Model
Freetext Figure/Image Form Table/List Textual Figure/Visual/Map Table/List
TURBO 82.8 76.5 88.8 83.4 80.0 60.9 67.3
V
TURBO +OCR 87.1 77.4 92.0 87.6 82.0 68.4 71.8
V
TURBO+OCR 80.9 55.6 85.0 80.5 67.9 43.7 55.2
Table 1
8K+OCR 80.6 53.7 82.7 79.0 65.9 44.1 57.2
Page TURBOV TURBOV + OCR TURBO + OCR 8K + OCR 32K + OCR
32K+OCR 1 - 5 86.2 51.8 58,3 85.3 618,11.7 6650.,91 443,24.1 48,7 57.5
6 - 10 54,9 55,9 45,8 39,8 43,1
11 - 15 48,2 50,4 41,6 40,9 39,6
16 - 20 49,2 51,8 40,7 37,0 38,1
TURBO V TURBO V + OCR TURBO + OCR
60 60 61,1 60
58,3
54,9 55,9
50 48,2 49,2 50 50,4 51,8 50 50,1
45,8
40 40 40 41,6 40,7
30 30 30
1-5 6-10 11-15 16-20 1-5 6-10 11-15 16-20 1-5 6-10 11-15 16-20
32K + OCR 8K + OCR
60 60
50 50
48,7
40 43,1 39,6 40 43,4 39,8 40,9
38,1 37
30 30
1-5 6-10 11-15 16-20 1-5 6-10 11-15 16-20
Figure1: ScoresonSlideVQAdependingontheevidencelocation(bucketsoffivepages). Resultsrevealaprimacy
bias,withhigherscoreswhenrelevantinformationisatthebeginningoftheinput.
Table 3: Impact of expected answer types (DUDE). Non-turbo variants of GPT-4 demonstrate proficiency in
identifyingnon-answerablequestionsdespiteloweroverallperformancestemmingfromalackofvisualcognition,
whileallmodelsencounterchallengesingeneratinganswersforabstractivecasescomparedtoextractiveones.
Model Lists Non-answerable Abstractive Extractive
TURBO 56.5 57.9 48.1 66.1
V
TURBO +OCR 57.4 50.5 48.0 68.5
V
TURBO+OCR 47.8 52.9 30.4 63.3
8K+OCR 42.8 61.9 34.6 58.8
32K+OCR 47.5 63.3 29.9 63.2
1Table4: Impactofmentioningthedatasetname(guidedinstruction)comparedtothebaselineandmentioning
different datasets (misguided). Subset of 200 test sets’ documents (∼850 and ∼1,200 question-answer pairs).
Changeisanimprovementoveramaximumofbaselineandmisguidedvalues.
Model Dataset Baseline Misguided Guided Change
DocVQA 85.3 87.3 87.3 0.0
TURBO
V
InfoVQA 68.8 72.0 71.8 −0.2
DocVQA 81.1 80.2 82.8 +1.7
TURBO+OCR
InfoVQA 53.6 58.2 58.6 +0.4
DocVQA 77.7 78.6 82.4 +3.8
8K+OCR
InfoVQA 56.3 56.0 59.9 +3.6
DocVQA 82.2 82.7 85.3 +2.6
32K+OCR
InfoVQA 56.4 55.5 59.6 +3.2
Whencomparingextractivecases,wherethean- ful because mentioning even an irrelevant task
swer could be copied from the input document, couldimpacttheperformancebyaligningoutput
toabstractivecases,whereithastobegenerated, form with popular annotation conventions rather
weobservethatallmodelsstrugglewiththelatter thanend-userpreferences.
scenario. Results reported in Table 4 indicate we could
increase the performance of text-only models by
Concerning an operation required to provide an
adding the dataset name to the prompt by up to
answer,theanalysisofoverallscoreswasnotvery
3.8points. Atthesametime,usingadifferentQA
insightfulas,e.g.,countingmightinvolvecounting
datasetnamedoesnotleadtocomparableoutcomes
ofgraphicalartifacts,andcomparisonmayrequire
forallbut TURBO model. Concerningthecases
comparing visual parts of the input document. It V
whenoutputhaschangedasaresultofguidingand
didn’t change even when we considered only an-
ledtodifferentscores,40%isaresultofchanging
swerswithplain-textandtabularevidencewhere
wrongorimperfectanswersforanswersmatching
there was a greater chance for a fair comparison
thegoldstandardperfectly(casingincluded).
betweenmodels.
Thoughthepremisesconsideredhereareincon-
4 ContaminationAnalysis clusive, this suggests that both datasets might be
presentintrainingdataoftextualGPT-4variants,
Asthereisapossibilitythattrainortestsetsplits
andachievedscoresshouldbetakenwithagrain
ofconsidereddatasetswerepresentinthetraining
ofsalt.
data of GPT-4, there is a question of whether it
Thoughtheresultsofthevison-enhancedmodel
is a zero-shot performance or whether the scores
could be distinct because of the lack of contami-
wereinflatedduetodatacontamination. Thestudy
nation, it is possible it was either not exposed to
concernsDocVQAandInfographicsVQA,asthese
dataset names (more likely in the textual crawls)
twowerepublishedbeforeSeptember2022,andit
ordidnothaveanintensivetomemorizeanswers
ispossiblethattheywerepresentincrawlsusedto
(moresubstantialfortextualmodelssincesomeof
trainalloftheGPT-4models.
theanswersweregroundedonvisuallayerunavail-
We use a straightforward technique of guided
ableduringthetraining).
instruction (Golchin and Surdeanu, 2023) where
promptsareextendedwithinformationonthetar- 5 Limitations
get dataset, e.g., instead of ‘answer the question’
Itisimportanttoacknowledgecertainlimitations
weuse‘answerthequestionfromDocVQAdataset
thatshapethecontextofanalysis.
test split’ and check if it impacts the evaluation
score. Additionally, we extend the analysis with Only performance. Our analysis is limited to
misguidedinstructions,whereadifferentthaneval- the performance of the models, as expressed by
uateddatasetnameisused. Thisreferenceishelp- theirscoresonpopularbenchmarkdatasets. Thereare,however,othervitalaspectstoconsiderbefore theyarenegligible. Finally,wecannotguaranteea
deployingmodels,suchasdataprivacy,cost,legal lackofbehavioralchangesduringtheexperiment.
compliance,orbiasesthatthesecanexhibit.
Specific’understanding.’ Whendocumentpro-
Dataset selection. Though we sketched the ra- cessing systems are considered, the term ’under-
tionalebehindtheselectionofdatasetsformodels’ standing’ we use has a very narrow and specific
comparison, it is debatable. Different choices of meaning. Itdoesnotimplytheabilityofmodelsto
datasets could lead to different outcomes. More- reasonortheirintelligence.
over,theselectionimpactedresultsindirectly,e.g.,
These limitations provide important context and
we were limited to the source image sizes, some-
presentopportunitiesforfutureresearchtoaddress.
timesleadingtoupsamplingforhigher-resolution
configurations(itispossiblethatwithcontrolover 6 FutureConsiderations
thedigitization,onecanachievebetterresults,de-
Weanticipateafewpotentialareasforforthcoming
spitethefactourresultsindicatenogainfromreso-
studies,includingwaystoimproveanswerquality
lutionabove2kpixels).
anddimensionstomeasureperformanceacross.
Nofinetuning. Unlikemoststate-of-the-artrefer-
Textarrangement. Preservingtextreadingorder
ences, our evaluation procedure did not assume
anditsintegrity,aswellascomprehensionofspa-
finetuning on a train set of considered datasets.
tialarrangement,areallcrucialforDocumentUn-
Though some of these data could unintentionally
derstandingproblems. Thesecanbemanipulated
endupintheGPT-4trainingmix(seeSection4),
by OCR setup (e.g., Tesseract can represent text
finetuned models of this size could, in principle,
arrangementwithspaces,andAzuretoproducethe
vastlyimprovetheirperformance.
human-friendlyreadingorder)orrepresentedasa
Vast search space. Zero-shot performance of partofplaintextinput. Studyingtheimpactofdif-
LLMs depends heavily on used prompts. Since ferentinputtextrepresentationsandarrangements
thesecanbetestedandrefinedbasedmerelyonthe mightbeworthwhile,particularlywhenconsider-
author’sintuitionthatdoesnotobeyobviousand ingplain-textLLMs.
unambiguousstrategies,oneneverknowsifanother
Confidencecalibration. DocumentUnderstand-
prompt could lead to significantly higher scores.
ingsystemsarecommonlyconsideredinbusiness
Moreover,alloptimizationswereperformedona
processautomation,whereitiscrucialtohavewell-
subset of the validation set to reduce the cost of
calibrated confidence scores. Since the API of
experiments. It is possible that testing all combi-
consideredmodelsdoesnotprovideasingleconfi-
nationsofparametersonacompletedevsetwould
dencescoreforageneratedanswer,oneislimited
leadtootherchoices.
tomethodsbasedon,e.g.,drawingseveralanswers
HugeimpactofOCR. Theperformanceoftext- fromthemodel,per-tokenprobabilities,orthever-
only models depends on how the used OCR en- balized confidence (Lin et al., 2022). It is worth
ginepreservedandrepresentedthelocalstructure evaluatingsuchestimates’reliability.
of the document. Since even the best model can-
Multi-QA. Asdatasetsunderconsiderationcom-
not achieve perfect scores with imperfect inputs,
monlyhadafewquestionsposedtothesameinput
it is possible to achieve better results with differ-
document,itwouldbecost-efficienttoformulate
entinputrepresentationsorbettertextualcontent
promptstoextractmultiplevaluessimultaneously.
recognition.
Moreover,thoughtheimpactofsuchanapproach
Third-party dependency. As models we con- remains unknown, it can potentially increase the
siderareaccessibleonlyviaAPIswehavenocon- answer’squality(Dwojaketal.,2020).
trolover,someinevitableorhypotheticallimitsare
Weeagerlylookforwardtofurtherexplorationin
involved. First, it happened infrequently that the
theseareas.
question-documentpairwasrejectedbecauseofthe
provider’scontentfilteringpolicy,slightlydecreas-
7 Summary
ingtheoverallscore. Second,wetreatedOpenAI
andAzureAPIswiththesamemodelinterchange- WeassessedtheperformanceofGPT-4familymod-
ably, assuming that if there are some differences, elsindocumentunderstanding,extendingpreviousevaluationsbyincludingtheDUDEandSlideVQA for generic visual-linguistic tasks. arXiv preprint
datasets. Withadetailedanalysisoftheresultsand arXiv:2312.14238.
commentary,weprovidedpracticalandtheoretical
Tomasz Dwojak, Michał Pietruszka, Łukasz Borch-
insightsintothefield,inparticular: mann,JakubChłe˛dowski,andFilipGralin´ski.2020.
Fromdatasetrecyclingtomulti-propertyextraction
• demonstrated that even models of this parame- and beyond. In Proceedings of the 24th Confer-
tercountunderperforminimage-onlysetupand enceonComputationalNaturalLanguageLearning,
pages 641–651, Online. Association for Computa-
vastlybenefitfromprovidingtextinadditionto
tionalLinguistics.
theinputimage;
Shahriar Golchin and Mihai Surdeanu. 2023. Time
• revealed a primacy bias in model performance, Travel in LLMs: Tracing Data Contamination in
e.g.,thetendencytoperformsignificantlybetter LargeLanguageModels.
with requested information at the beginning of
StephanieLin,JacobHilton,andOwainEvans.2022.
inputdocuments;
Teaching models to express their uncertainty in
words.
• proposedanextensiontotheguidedinstruction
contaminationassessingtechnique; NelsonF.Liu,KevinLin,JohnHewitt,AshwinParan-
jape,MicheleBevilacqua,FabioPetroni,andPercy
• identifiedcategoriesofquestionscomparedmod- Liang. 2023. Lost in the Middle: How Language
ModelsUseLongContexts.
els underperform with and suggested prompts,
OCRs,andinputimageresolutionscanbeused Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthe-
toimprovetheirperformance. nisKaratzas,ErnestValveny,andCVJawahar.2022.
InfographicVQA. InProceedingsoftheIEEE/CVF
Importantly,incontrasttotheundisclosedsetupin WinterConferenceonApplicationsofComputerVi-
sion,pages1697–1706.
previousreports,weprovideallthedetails,ensur-
ingreproducibilitywithpubliclyavailableAPIs. Minesh Mathew, Ruben Tito, Dimosthenis Karatzas,
R Manmatha, and CV Jawahar. 2020. Document
VisualQuestionAnsweringChallenge2020. arXiv
References preprintarXiv:2008.08899.
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, OpenAI. 2023. GPT-4. https://openai.com/
SinanTan, PengWang, JunyangLin, ChangZhou, research/gpt-4.
and Jingren Zhou. 2023. Qwen-vl: A versatile
vision-languagemodelforunderstanding, localiza- Ryota Tanaka, Taichi Iki, Kyosuke Nishida, Kuniko
tion, text reading, and beyond. arXiv preprint Saito,andJunSuzuki.2024. Instructdoc: Adataset
arXiv:2308.12966. forzero-shotgeneralizationofvisualdocumentun-
derstandingwithinstructions.
Tsachi Blau, Sharon Fogel, Roi Ronen, Alona Golts,
RoyGanz,EladBenAvraham,AviadAberdam,Sha- RyotaTanaka,KyosukeNishida,KosukeNishida,Taku
har Tsiper, and Ron Litman. 2024. Gram: Global Hasegawa, Itsumi Saito, and Kuniko Saito. 2023.
reasoningformulti-pagevqa. SlideVQA:ADatasetforDocumentVisualQuestion
AnsweringonMultipleImages.
Łukasz Borchmann, Michał Pietruszka, Tomasz
Stanisławek, Dawid Jurkiewicz, Michał Turski, JordyVanLandeghem,RubènTito,ŁukaszBorchmann,
KarolinaSzyndler,andFilipGralin´ski.2021. DUE: Michał Pietruszka, Pawel Joziak, Rafal Powalski,
End-to-End Document Understanding Benchmark. DawidJurkiewicz,MickaelCoustaty,BertrandAnck-
In Proceedings of the Neural Information Process- aert,ErnestValveny,MatthewBlaschko,SienMoens,
ingSystemsTrackonDatasetsandBenchmarks,vol- and Tomasz Stanislawek. 2023. Document Under-
ume1.Curran. standing Dataset and Evaluation (DUDE). In Pro-
ceedingsoftheIEEE/CVFInternationalConference
Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, onComputerVision(ICCV),pages19528–19540.
ZhangweiGao,ErfeiCui,WenwenTong,Kongzhi
Hu, Jiapeng Luo, Zheng Ma, et al. 2024. How far
are we to gpt-4v? closing the gap to commercial
multimodalmodelswithopen-sourcesuites. arXiv
preprintarXiv:2404.16821.
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su,
Guo Chen, Sen Xing, Zhong Muyan, Qinglong
Zhang,XizhouZhu,LeweiLu,etal.2023. Internvl:
Scaling up vision foundation models and aligningA OrderofExperiments Sincethe TURBO consumesalow-resolution
V
representation of the entire input image and, op-
Forpracticalreasons,wedidnotoptimizeallcom-
tionally,high-resolutioncrops,weinvestigatethe
binationsofpromptsandinputs(imageproperties,
impactofhigh-resolutioncrops’presenceandun-
OCRs). Instead, experiments are performed in a
derlyingimagesize.
chosenorder,whichreducesthesearchspace(Fig-
Results in Table 12a indicate that the high-
ure2).
resolution variant with the image size of 2048px
We start by optimizing prompts on TURBO
V onthelongersideperformsrobustlybothwithand
assumingoriginalresolution(SlideVQA)orhigh-
without OCR output and independently from the
resolution images (2048px on the longer side for
dataset. Itwasoutperformedonlyoncebyasmall
therestofthedatasetsconsidered).
marginwithtwicethelowerresolution.
Thebest TURBO promptsareassumedduring
V Thisobservationholdsfor TURBO +OCR (Ta-
thecomparisonofOCRenginesfortheremaining V
ble12b),whereonecanadditionallyobservethat
models,aswellasinputimageformatandresolu-
the gain from text availability is vast in a low-
tionfor TURBO .
V resolutionregime.
Next, we optimize prompts for the rest of the
models, assuming the OCRs they perform best D OptimizingOCRs
with. Finally,wecalculateresultsinTable1,taking
Analogously to the previous studies, we evalu-
eachmodel’sbestprompts,OCRs,andinputimage
ateTesseract5.3.3,AzureCognitiveServices3.2
sizes.
(2022-04-30),andAmazonTextract(DetectDoc-
B OptimizingPromptforVisionModel ument Text 1.0) OCRs. Experiments in this sec-
tion assume Prompt 8 for DUDE, Prompt 2 for
DocVQA, InfographicsVQA, SlideVQA. We
DocVQA,andPrompt5fortherestofthedatasets
evaluatetheimpactofdifferentchoicesonasubset
considered.
of 50 documents from the validation set of each
Thoughthereisnouniversallysuperiorsolution,
dataset, i.e., approx. 200, 400, and 300 question-
Table 7 indicates that Textract is the best choice
answerpairs.
and the Tesseract is the worst for most datasets,
Considered variants, shown in Table 5, result
assumingthetext-onlyregime. Whenaccompanied
fromthefactthemodel(1)tendstoproduceboil-
byvision,theresultsbecomenoisier,andthemodel
erplatetextinadditiontothevalueexpectedcon-
seemstobenefitevenfromtheavailabilityoflow-
cerning the convention used in the dataset, (2) in
qualityOCR.
some cases, the model refrains from answering,
Note that this study assumed each model’s de-
whereasallofthequestionsinbothdatasetscanbe
faultsettings,anditispossiblethatonecanachieve
answeredbasedonthedocumentprovided.
different rankings of OCRs when manipulating,
Results indicate that Prompt 5 performs ro-
e.g., reading order parameters. Nevertheless, we
bustly across all datasets and is topped only on
expectthedefaultparametervaluestobeoptimized
theDocVQAdatasetbyanegligiblemargin.
bytheproviders,andfine-grainedOCRoptimiza-
DUDE. AsDUDEintroducesnoveltypesofan- tionisoutsidethescopeofthispaper.
swers, we perform prompt optimization for this
E OptimizingPromptforTextModels
dataset separately. We start with the most robust
promptfromTable5andconsidervariants(Table6) HavingtheOCRs,wecomparehowpromptsfrom
attemptedtocoverlistvaluesandstatementsthat SectionBbehaveontext-onlymodels. Thisleads
thequestioncannotbeansweredgroundedonthe to several changes, which is not unexpected con-
inputdocument. Weevaluatetheimpactofdiffer- cerningzero-shotsetup,whichisextremelysensi-
entchoicesonasubsetof30documentsfromthe tivetopromptchoice,andfactpromptsgenerally
validationset,i.e.,300question-answerpairs. do not transfer between different models, even if
they retain some architectural and training data
C OptimizingImageResolution
similarities.
Weoptimizetheinputimage,assumingthesame Thisconcludestheparameteroptimization,lead-
subsetsofvalidationsetsasintheprevioussection. ingtothefinalsetupinTable11.2. Optimize OCRs 3. Optimize Prompts
1. Optimize TURBO
V
assuming best TURBO V and Images assuming
Prompts and Images
Prompts and Images best OCRs
Optimize image
Optimize OCR
Optimize prompts
Figure 2: The order of experiments assumed to reduce the search space and not optimize all combinations of
promptsandinputs(imageproperties,OCRs).
Table5: Impactofdifferent TURBO promptsonDocVQA,InfographicsVQA,andSlideVQAvalidationscores.
V
# Prompt DocVQA InfoVQA SlideVQA
1 Answerthequestion:[TEXT] 2.1 0.0 0.0
2 Answerthequestion. Donotwriteafullsentence,justprovideavalue. 87.5 67.1 63.1
Question:[TEXT]
3 Answerthequestion.BeconciseandprovideavalueIamlookingforonly. 80.4 57.5 48.1
Question:[TEXT]
4 Answerthequestion. Donotwriteafullsentence,justprovideavalue. 85.0 67.1 61.4
Alwaystrytoprovideananswer.Question:[TEXT]
5 Answerthequestion.Donotwriteafullsentence,justprovideavalue.If 87.4 68.7 62.4
thevalueisunclear,guessitgivenaninputdocument.Question:[TEXT]
6 Replace[ANSWER]withavalueinthetemplategivenquestionanddoc- 85.1 61.8 59.7
ument. ←(cid:45) Question: [TEXT] ←(cid:45) Template: Based on the context, the
answertothequestionwouldbe"[ANSWER]".Table6: Impactofdifferent TURBO promptsonDUDEvalidationscores.
V
# Prompt DUDE
5 Answerthequestion.Donotwriteafullsentence,justprovideavalue.Ifthevalueisunclear,guessit 38.6
givenaninputdocument.Question:[TEXT]
7 Answerthequestion.Donotwriteafullsentence.ProvideavalueasaPythonlist.Ifthereisasingle 40.1
answer,theoutputshouldbeaone-elementlistlike["ANSWER"].Iftherearemultiplevalidanswers,
thelistwillhaveseveralelements,e.g.,["ANSWER1","ANSWER2"].Theoutputshouldbe["None"]
ifthevalueisunclear.Question:[TEXT]
8 Answerthequestion.Donotwriteafullsentence.ProvideavalueasaPythonlist.Ifthereisasingle 42.7
answer,theoutputshouldbeaone-elementlistlike["ANSWER"].Iftherearemultiplevalidanswers,
thelistwillhaveseveralelements,e.g.,["ANSWER1","ANSWER2"].Theoutputshouldbe["None"]
ifthevaluecannotbefoundinthedocument.Question:[TEXT]
9 ProvideavalueasaPythonlist.Ifthereisasingleanswer,theoutputshouldbeaone-elementlistlike 41.6
["John"].Iftherearemultiplevalidanswers,thelistwillhaveseveralelements,e.g.,["1997","1998"].
Theoutputshouldbe["None"]ifthevaluecannotbefoundinthedocument.Thevalueswearelooking
forarerelatedtothequestion:[TEXT]
10 Answerthequestion: [TEXT]←(cid:45)Donotwriteafullsentence. ProvideavalueasaPythonlist,e.g., 39.7
["ANSWER"].Iftherearemultiplevalidanswers,thelistwillhaveseveralelements,e.g.,["ANSWER
1","ANSWER2"].Theoutputshouldbe["None"]ifthevaluecannotbefoundinthedocument.
11 Answerthequestion:[TEXT]←(cid:45)Keeptheanswershort.Respond"None"ifnotsureabouttheanswer. 34.0
Iftherearemultiplevalidanswers,separatethemby"|."
12 Answerthequestion:[TEXT]←(cid:45)Donotwriteafullsentence,justprovideavalue.Respond"None"if 34.4
thevaluecannotbefoundinthedocument.Iftherearemultiplevalidvalues,separatethemby"|."
Table 7: Study of Tesseract 5.3.3, Azure Cognitive Services 3.2 (2022-04-30), and Amazon Textract (Detect
DocumentText1.0). Prompt2(DocVQA,SlideVQA),Prompt5(InfographicsVQA),andPrompt8(DUDE).
Model OCR DocVQA InfoVQA SlideVQA DUDE
Tesseract 87.4 72.7 62.4 43.9
TURBO +OCR Azure 87.5 71.0 62.7 45.2
V
Amazon 87.1 72.4 63.1 44.3
Tesseract 65.6 41.1 43.4 34.5
TURBO+OCR Azure 79.4 56.4 54.6 38.3
Amazon 82.1 58.9 51.2 37.6
Tesseract 64.6 40.5 36.9 31.3
8K+OCR Azure 81.7 57.0 44.7 36.7
Amazon 82.6 59.1 45.8 34.2
Tesseract 64.2 34.3 41.0 37.4
32K+OCR Azure 76.4 53.1 51.9 39.3
Amazon 76.9 56.4 52.9 39.2Table8: ImpactofdifferentpromptsonDocVQA,InfographicsVQA,andSlideVQAvalidationscoresassumingthe
bestOCRforeachdataset(selectedintheablationstudy).
Model Dataset Prompt1 Prompt2 Prompt3 Prompt4 Prompt5 Prompt6
DocVQA 1.0 82.1 67.0 78.7 80.6 84.7
TURBO+OCR InfographicsVQA 0.0 58.2 43.8 59.4 58.9 58.7
SlideVQA 0.0 54.6 44.7 52.5 54.2 54.9
DocVQA 3.4 82.6 16.7 79.0 80.1 80.6
8K+OCR InfographicsVQA 1.2 57.1 9.7 56.8 59.1 59.5
SlideVQA 0.0 45.8 1.4 44.1 45.1 46.8
DocVQA 3.0 76.9 8.2 76.8 77.8 80.4
32K+OCR InfographicsVQA 0.1 58.2 14.6 58.4 56.4 58.2
SlideVQA 0.0 52.9 11.2 49.2 48.8 52.5
Table9: Sourcesofhumanperformanceandassumedstate-of-the-artmodels.
Dataset State-of-the-art Humanperformance
DocVQA Qwen-VL-Max(Baietal.,2023) Mathewetal.(2020)
InfographicsVQA InternVL(Chenetal.,2023,2024) Mathewetal.(2022)
SlideVQA InstructDr(Tanakaetal.,2024) Tanakaetal.(2023)
DUDE GRAM(Blauetal.,2024) VanLandeghemetal.(2023)
Table10: ImpactofdifferentpromptsonDUDEvalidationscoresassumingthebestOCRforeachdataset(selected
intheablationstudy).
Model Prompt5 Prompt7 Prompt8 Prompt9 Prompt10 Prompt11 Prompt12
TURBO+OCR 34.2 37.9 38.3 38.8 36.6 34.0 30.6
8K+OCR 31.8 32.6 36.7 36.0 36.4 26.0 40.2
32K+OCR 33.1 36.0 39.3 34.8 34.5 33.9 33.5Table 12: Impact of different image sizes on scores.
Pixelsonthelongersideexceptforlowthatcorresponds
todetail: lowintheAPI(512pxsquare).
(a) TURBO
V
Dataset low 512 1024 2048 4096
Table11: Finaltestsetevaluationsetupoptimizedon
validationsets. DocVQA 50.5 69.3 85.2 87.5 87.2
InfoVQA 37.8 45.2 58.8 68.7 68.0
SlideVQA 47.8 62.7 63.1 64.7 61.0
Dataset Prompt OCR Image
DUDE 30.3 34.1 44.2 43.8 44.0
TURBO
V
DocVQA 2 — 2048px jpg (b) TURBO V+OCR
InfoVQA 5 — 2048px png
Dataset low 512 1024 2048 4096
SlideVQA 2 — 2048px jpg
DocVQA 80.6 86.4 86.5 87.5 87.1
DUDE 8 — 1024px jpg
InfoVQA 52.9 57.7 63.1 72.7 72.6
TURBO V+OCR SlideVQA 54.2 63.7 64.7 63.1 62.0
DocVQA 2 Azure 2048px jpg DUDE 35.2 35.2 44.6 45.2 43.8
InfoVQA 5 Tesseract 2048px png
SlideVQA 2 Amazon 1024px jpg
Table13: Guided(G)andmisguided(M)promptsused
DUDE 8 Azure 2048px jpg
intheContaminationAnalysis.
TURBO+OCR
2G AnswerthequestionfromDocVQAtestsplit
DocVQA 6 Amazon —
(Task 1 - Single Page Document VQA). Do
InfoVQA 4 Amazon — not write a full sentence, just provide a value.
Question:[TEXT]
SlideVQA 6 Azure —
2M AnswerthequestionfromSQuADtestsplit.Do
DUDE 9 Azure — not write a full sentence, just provide a value.
Question:[TEXT]
8K+OCR 4G AnswerthequestionfromInfographicsVQA
testsplit.Donotwriteafullsentence,justpro-
DocVQA 2 Amazon —
videavalue. Alwaystrytoprovideananswer.
InfoVQA 6 Amazon — Question:[TEXT]
SlideVQA 6 Amazon — 4M AnswerthequestionfromSQuADtestsplit.Do
notwriteafullsentence,justprovideavalue.Al-
DUDE 12 Azure — waystrytoprovideananswer.Question:[TEXT]
6G Replace [ANSWER] with a value in the tem-
32K+OCR plategivenquestionanddocument.←(cid:45)Source:
DocVQAtestsplit(Task1-SinglePageDocu-
DocVQA 6 Amazon —
mentVQA).←(cid:45)Question:[TEXT]←(cid:45)Template:
InfoVQA 4 Amazon — Basedonthecontext,theanswertothequestion
wouldbe"[ANSWER]".
SlideVQA 2 Amazon —
Replace[ANSWER]withavalueinthetemplate
DUDE 8 Azure — givenquestionanddocument.←(cid:45)Source:Info-
graphicsVQAtestsplit.←(cid:45)Question:[TEXT]
←(cid:45)Template:Basedonthecontext,theanswer
tothequestionwouldbe"[ANSWER]".
6M Replace [ANSWER] with a value in the tem-
plategivenquestionanddocument.←(cid:45)Source:
SQuAD test split. ←(cid:45) Question: [TEXT] ←(cid:45)
Template: Basedonthecontext,theanswerto
thequestionwouldbe"[ANSWER]".