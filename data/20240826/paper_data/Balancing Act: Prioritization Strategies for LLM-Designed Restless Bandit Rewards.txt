Balancing Act: Prioritization Strategies for LLM-Designed Restless Bandit
Rewards
ShresthVerma1*,NiclasBoehmer1*,LingkaiKong1,MilindTambe1,2
1HarvardUniversity
2GoogleResearchIndia
Abstract agentsaremoreorlesslikelytoreceivearesource.RMABs
havebeenusedinvariousdomainssuchasmachinemainte-
LLMsareincreasinglyusedtodesignrewardfunctionsbased
nance (Abbou and Makis 2019), anti-poaching (Qian et al.
onhumanpreferencesinReinforcementLearning(RL).We
2016a),andhealthcare(Ayeretal.2019;Vermaetal.2023).
focus on LLM-designed rewards for Restless Multi-Armed
In many of them, system organizers have evolving alloca-
Bandits,aframeworkforallocatinglimitedresourcesamong
tion priorities based on agents’ features that need to be in-
agents. In applications such as public health, this approach
empowers grassroots health workers to tailor automated al- corporated into the resource allocation process (Deardorff
location decisions to community needs. In the presence of etal.2018;Vermaetal.2024).Forinstance,inahealthcare
multiple agents, altering the reward function based on hu- program, a healthcare worker might want to change the al-
manpreferencescanimpactsubpopulationsverydifferently, location policy to prioritize low-income beneficiaries who
leading to complex tradeoffs and a multi-objective resource areathigherriskorolderbeneficiarieswhohavetransporta-
allocation problem. We are the first to present a principled tionbarriersforhealthcareaccess(Nelsonetal.2016;Syed,
method termed Social Choice Language Model for dealing
Gerber,andSharp2013)viathepreferenceprompt:Priori-
with these tradeoffs for LLM-designed rewards for multia-
tizelow-incomebeneficiariesandolderbeneficiaries.
gent planners in general and restless bandits in particular.
Thenovelpartofourmodelisatransparentandconfigurable Translating such human language prompts to effective
selection component, called an adjudicator, external to the and aligned reward functions is a general, non-trivial chal-
LLMthatcontrolscomplextradeoffsviaauser-selectedso- lenge in RL (Hazra et al. 2024; Cao et al. 2024; Ma et al.
cialwelfarefunction.Ourexperimentsdemonstratethatour 2023), e.g., as it is unclear how changes in the reward in-
modelreliablyselectsmoreeffective,aligned,andbalanced fluence the used policy downstream. However, the multia-
rewardfunctionscomparedtopurelyLLM-basedapproaches.
gent nature of the RMAB problem adds a new twist to the
problemofrewarddesigninRL:Itbecomesfundamentally
1 Introduction multi-objective. Consider the above example prompt ask-
ing for the prioritization of two subpopulations. As these
Rewardfunctionsplayafundamentalroleinthegeneration
subpopulationsmaycontaindifferentagents,selectingare-
of optimal policies for sequential decision-making via re-
wardfunctionwillmostlikelyinvolvetradingofftheinter-
inforcement learning. Previous work has shown that LLMs
ests of the low-income vs. older beneficiaries, making this
are an effective tool for designing reward functions that
a multi-objective problem. If this multi-objective nature is
canbeguidedandcustomizedviahumanlanguageprompts
ignored,aselectedrewardfunctionmightheavilyfavorone
(Ma et al. 2023; Cao et al. 2024; Kwon et al. 2023; Xie
of the two groups (e.g., leading to the allocation of many
et al. 2024a; Ma et al. 2024; Yu et al. 2023; Hazra et al.
resources to low-income beneficiaries, and no resources to
2024). We study the problem of designing high-quality re-
older ones). Another tradeoff that emerges because of the
ward functions aligned with human preference prompts in
multiagent nature of the problem is between aligning with
the context of multiagent optimization and planning prob-
the preference prompt and impacting agents not referenced
lems.WepresentatransparentframeworkaroundLLMsthat
intheprompt:Changingtherewardfunctionstoalignwith
constructseffective,aligned,andbalancedrewardfunctions
a prompt can lead to drastic changes in the number of re-
forcomplexhumanprompts.
sources distributed to subpopulations not referenced in the
We study the reward design problem for restless multi-
user prompt (e.g., in the example prompt, the selected re-
armed bandits (RMABs), a popular model in multiagent
ward might lead to taking away all resources from well-
systems for sequentially allocating a limited number of re-
educatedbeneficiaries).Ifnotadequatelybalanced,thiscan
sourcestoasetofagents(Whittle1988;Nin˜o-Mora2023).
causestrongsideeffectsunintendedbytheuser.
In RMABs, each agent is represented by an individual
Markov Decision Process including a reward function. By To our knowledge, we are the first to address the multi-
choosing these reward functions, one can control which objectivenatureofLLM-poweredrewarddesigninRMABs
in particular and multiagent planners in general. Closest to
1EqualContribution ourpaperistheworkbyBeharietal.(2024)whoproposed
4202
guA
22
]GL.sc[
1v21121.8042:viXraa fully LLM-based Decision-Language Model for RMABs DLM mixes generation with selection and does not explic-
to generate and select reward functions (as code) from hu- itlyaccountforthemulti-objectivenatureoftherewardde-
man language prompts. However, as argued in Sections 2 signproblem.Furthermore,incontrasttoourwork,theyfo-
and4,theDLMmodelisnotproperlyequippedtohandlethe cusonsmallRMABinstances(∼20arms).Throughoutthe
multi-objective nature of the problem, as the LLM selects paper, we will use a slightly modified variant of DLM ad-
functions in an unpredictable, hard-to-control and some- justedtooursetting(seeSectionC.2)asabaseline.
times(clearly)suboptimalwaythatdoesnotadequatelytake LLMs can also be integrated into model-based and ex-
intoaccountandbalancethedifferentobjectives. plainableRL,servingasworldmodelsimulators(Linetal.
We present a Social Choice Language Model (SCLM) 2023)orpolicyinterpreters(Das,Chernova,andKim2023).
thatdesignsrewardfunctions(asPythoncode)alignedwith They can also act as information processors, effectively
complex, multi-objective human language preferences; see managing environment data and task instructions (Basava-
Figure 2 for an overview of SCLM. Our pipeline separates tia, Murugesan, and Ratnakar 2024; Song et al. 2023) and
the generation of candidate reward functions in the gener- as direct decision makers (Li et al. 2022) to improve the
ator from the selection of one function in the adjudica- sampleefficiencyofofflineRL.Whilethisareaofresearch
tor. For the generator, we use LLM-powered evolutionary is rapidly evolving, there remains a significant gap in ad-
searchtogenerateapoolofrewardfunctions(Beharietal. dressingmulti-objective,multi-agentscenarioswithinLLM-
2024). In the transparent and customizable adjudicator, we enhancedRL,whichwebegintoaddressinthispaper.
take a new social choice perspective to address the multi-
Multi-ObjectiveReinforcementLearning(MORL) Re-
objectivenatureofourproblem:Wecreateascorercompo-
searchonMORLfocusesonlearningpoliciesthatmaximize
nentthatevaluatesthequalityofgeneratedrewardfunctions
(and balance between) multiple objective functions, typi-
accordingtothedifferentobjectives(e.g.,differentprioriti-
callyviascalarizingtheobjectivesintoasinglerewardfunc-
zationrequests).Subsequently,asocialwelfarefunctionag-
tion (Moffaert, Drugan, and Nowe´ 2013) or approximating
gregates these “alignment scores” to select the best reward
theParetofront(Roijersetal.2013;VanMoffaertandNowe´
function.Theusercanselectthesocialwelfarefunctionand
2014).Inthecontextofmultiagentsystems,MORLhasbeen
thereby has additional control over the preferred trade-off
usedasamethodtoensurethefairtreatmentoftheindivid-
betweenobjectives,e.g.,maximizingthesummedvs.mini-
ualagents(JiangandLu2019;Zimmeretal.2021;Fanetal.
mumalignmentofallobjectives.Wedemonstrateinourex-
2023).Closesttooursfromthislineofworkisthepaperby
periments that our SCLM leads to the selection of reward
Fanetal.(2023),whouseideasfromtheresourceallocation
functions significantly better aligned with complex, multi-
literaturetocombinemultipleobjectivesintoasingularnon-
objective prompts. Moreover, we also show how it can be
linearobjectivefunction.However,incontrasttoourpaper,
used to effectively mitigate the risks of using rewards de-
theirfocusliesonpolicylearningforsuchnon-linearobjec-
signed from human prompts: unintended effects for other
tivefunctions.Theyneitherconsiderrewarddesign,human
agents and the ineffective allocation of resources. SCLM
preferenceprompts,LLMs,norRMABs.
combines the generative power of LLMs to design reward
WerefertoSectionAforadditionalrelatedwork.
functions with the capabilities of social choice to handle
multi-objectivedecision-makingscenarios.
3 Preliminaries
2 RelatedWorks An instance of Restless Multi-Armed Bandits (RMAB) is
definedbyasetofN arms,atimehorizonT,andabudget
LLM-enhanced RL LLMs have emerged as a powerful K. We also refer to arms as agents. Each arm i ∈ [N] is
tool to enhance RL. Recent work has used LLMs to gen- anindependentlyevolvingMDPwithstatespaceS ,actions
i
erate reward functions based on natural language descrip- A ={0,1},transitionfunctionP :S ×A ×S →R ,
i i i i i ≥0
tions(Maetal.2023;Xieetal.2024b;Yuetal.2023).For andrewardfunctionR :S →R.Wereferto1astheactive
i i
instance, Goyal, Niekum, and Mooney (2019); Carta et al. action corresponding to pulling the arm (i.e., allocating a
(2022);Mirchandani,Karamcheti,andSadigh(2021);Hazra resource) and 0 as the passive action corresponding to not
etal.(2024)shaperewardsbytraininganRLagenttolearn pulling the arm. We focus on the popular case where each
andcompleteintermediatetasksguidedbylanguage,yetfo- MDPconsistsoftwostates,i.e.,S ={0,1}foralli∈[N],
i
cusingonverydifferent(non-multiagent)environments. yet our methodology applies to MDPs with arbitrary state
The work of Behari et al. (2024) is the first to present a spaces. We refer to 0 as the bad and 1 as the good state.
Decision-Language Model for generating reward functions For each step in which an agent is in the good state, they
for RMABs from human prompts. The model performs a derive a utility of 1, while they derive a utility of 0 in the
formofevolutionarysearchtofindrewardfunctionsaligned bad state. Accordingly, agents’ default reward function R∗
withthegivenpromptintwointerleavingphases:generation is R∗(s) = s. We assume that there is a set of categorical
andreflection.Inthegenerationphase,anLLMgeneratesa features.Eacharmisassociatedwithavalueofeachfeature.
setofrewardfunctions.Basedonrewardfunction’sperfor- Aglobalrewardfunctionisarewardfunctiondefinedover
mances,inthereflectionphase(Maetal.2023;Shinnetal. features, which induces a reward function for each arm by
2023), the LLM selects the function best aligned with the plugginginitsfeaturevalues(seeExample4.1).
prompt.Thisfunctionisthenincludedinthepromptforthe In each step within the time horizon T, the planner ob-
next generation phase or returned. In contrast to our work, serves the state of all arms and decides to pull a subset of
2at most K arm. As solving the RMAB problem optimally
is computationally intractable (Papadimitriou and Tsitsik-
lis1994),wemakeuseoftheverypopularstate-dependent
Whittleindex(Whittle1988;Nin˜o-Mora2023),whichtries
to quantify for each state of each arm the reward gain
achieved from applying the active action to the arm in this
state. In the Whittle index policy Π, in each step, we com-
pute the Whittle index for each arm (based on its current
state)andpullthearmswiththeK highestWhittleindices.
(a) Prompt: “Prioritize agents (b) Prompt: “Prioritize agents
Wewilluseitasthesolutionstrategyinthefollowing.
witholdageandagentswithlow with high age and agents with
ForaglobalrewardfunctionR,wewriteΠ(R)todenote
education” lowincome”
the Whittle index policy for R, i.e., the Whittle index pol-
icy for the instance where each agent uses the function R
Figure1:Tradeoffsbetweenprioritizationclauses.
afterpluggingintheirfeaturevaluesastheirreward.Were-
fertoΠ(R∗)asthedefaultpolicy.Toassessthequalityofa
of agents against each other. Generally, in the presence of
globalrewardfunctionR,weoftenconsidertheutilityfea- multiple agents and limited resources, each clause can be
turedistributionforsomefeatureX.Thisdistributionshows viewedasaseparateindependentobjectivethatwewantto
foreachvalueofthefeature,theexpectedutilitygenerated optimize,renderingthisamulti-objectiveproblem.
byarmswiththisfeaturevalueunderthepolicyΠ(R)(see
Toillustratethetradeoffdecisionswefacebetweendiffer-
Figure4ainSectionBforanexample).
entclauseswhenselectingarewardfunction,inFigure1,we
showtwoinstancesfromourexperimentsforapromptcon-
4 ProblemStatement&Challenges
sisting of two prioritization clauses. Every point represents
Weassumethatwearegivenahuman-languagepreference a LLM-designed reward function. The x and y axes repre-
prompt, concatenating one or multiple preference clauses. sentthequalityoftherewardfunctionfromtheperspective
Eachpreferenceclausespecifiesasingleoptimizationgoal. of the two prioritized subgroups where higher percentage
Weexplicitlyconsiderthreetypesofpreferenceclauses(yet values indicate more benefits (see Section 6.3 for details).
ourmethodologyextendstoarbitraryones): RewardfunctionsmarkedwithstarslieontheParetofronts
(nootheravailablefunctiondominatesthem).
• Give priority to agents with certain feature values, i.e.,
Inourexperiments,weobservethatDLMpicksfunctions
increasetheutilitygeneratedbytheseagents,
from very different parts of the Pareto frontier, potentially
• donotshifttheutilitydistributionforsomefeature,and
clearlyprioritizingonesubgroupoveranother(seeFigure1a
• maximizethesummedutilitygeneratedbyallagents. foranexample).Inmanyotherinstances,italsopickssub-
Wemostlyfocusonthefirsttypeandrefertothemasprior- optimalfunctions,i.e.,functionsthatdonotlieonthefron-
itizationclausesandprompts.Apreferencepromptisaset tier,thatmayevenharmoneofthesubgroupswhilestrongly
P = {p ,p ,...} of the involved preference clauses. We benefitingtheother(seeFigure1b).Thishighlightstherisks
1 2
call a prompt P singular if |P| = 1 and composite other- (andshortcomingsofDLM)innotaccountingforthemulti-
wise;ourfocusisonthelatter.Wecaninfluencetheutility objectivenatureoftheproblem,asitpicksrewardfunctions
agentsgeneratebyselectingasingleglobalrewardfunction thatareinefficient(i.e.,dominated)andunfair(i.e.,heavily
(inducingrewardfunctions(R ) forallagents). favoringoneclauseovertheother).
i i∈[n]
Ourgoalistocreateamodelthathandlesthetradeoffs
Example 4.1. Consider an RMAB instance with three bi-
posed by composite “multi-objective” prompts in a princi-
nary features A, B, and C. A preference prompt P could
pled, transparent, and customizable fashion and outputs a
be“PrioritizeagentswithA = 0andprioritizeagentswith
singleeffectiveandfairlyalignedglobalrewardfunction.
B = 1”, i.e., P = {“prioritize agents with A = 0”, “pri-
oritize agents with B = 1” }. Two possible global reward
functions for the prompt are R′(s) = s·(1−A)·B and 5 SocialChoiceLanguageModel(SCLM)
R′′(s)=s·(1−A)+s·B.ForfunctionR′′,therewardof
WeproposeaSocialChoiceLanguageModeltogeneratere-
anagentiwithA = 0andB = 1isR (s) = 2s,whilethe
i wardsfromhumanlanguagecompositepreferenceprompts
rewardofanagentj withA = 1andB = 1isR (s) = s.
j (seeFigure2foravisualization).Separatingthegeneration
Selecting R′′, agent i is more likely to receive a resource
andselectionofrewardfunctions,themodelconsistsoftwo
thanagentj,asthegoodstatecontributesmorerewardfori.
sequential components. The LLM-powered generator gen-
Wewanttodesignasingleglobalrewardfunctionthatis erates a set of candidate reward functions. Subsequently,
“well-aligned”withallclausesofthegivenhuman-language taking a social-choice-inspired viewpoint, the adjudicator
preferenceprompt.However,asclausescancontradicteach selects a reward function from the pool to be returned to
other, perfect alignment with all clauses becomes impossi- the user in two steps: First, a scorer model computes an
ble. For instance, if a prompt requests the prioritization of alignment score for each reward function with each prior-
two fully disjoint subpopulations, each resource will only itization clause (i.e., we judge each reward function from
benefit one of the two. When picking the reward function, theperspectiveofallrelevant“objectives”).Second,auser-
weneedtocarefullybalancetheinterestsofthetwogroups definedsocialwelfarefunctionaggregatesthesescoresintoa
3Figure2:OverviewofSCLM.Instep1,thepreferencepromptispassedtothegenerator,whichperformsanevolutionarysearch
tocreateapoolRofcandidaterewardfunctions.Instep2,thesefunctionsarepassedtotheadjudicatorwhereascorermodel
(e.g.,thesimulatororLLMscorer)computesthealignmentscores.Instep3,auser-definedsocialwelfarefunctionselectsa
rewardfunctionbasedonthealignmentscores.
“winning”candidaterewardfunction.Byselectingthesocial caninterprettherewardfunctionsasthecandidatesandthe
welfarefunction,theusercancontroltheareaofthePareto preferenceclausesinthepromptasthevoterswiththeirpref-
frontier from which reward functions get selected. While erencesoverthecandidatesreflectingtherewardfunction’s
weremarkthatourmodelcanalsobeusedtotacklemulti- alignment with the clause. This view gives rise to the fol-
objective issues arising when designing rewards in single- lowing strategy: Given a prompt P = {p ,p ,...,p }, we
1 2 ℓ
agentRL,thedetailsofourcomponents(e.g.,thereflection evaluateeachrewardfunctionR ∈ Rfromtheperspective
in the generator and the computation of alignment scores) of each preference clause p by computing an (alignment)
i
arespecifictothemultiagentnatureoftheRMABproblem. score s (R). s (R) measures the alignment of Π(R) with
i i
preference clause p , i.e., how much the voter representing
i
5.1 Generator p “likes” the candidate R. We describe at the end of this
i
Givenaprompt,ourgeneratorcreatesasetofcandidatere- sectionhowthesescoresarecomputed.
wardfunctions(asPythoncode)viaavariantofevolution-
SocialWelfareFunctions Socialwelfarefunctionsselect
arysearch(Beharietal.2024):Weproceedinmultiplesteps.
analternativebasedoninputpreferencesofvoters.Thepros
First,inputtingtheproblemdescription,featuredescriptions
and cons of individual social welfare functions have been
andthepreferenceprompt,weaskanLLMtogeneratecode
extensivelyresearchedanddebatedinsocialchoice(Arrow,
forarewardfunction.Werepeatthisqueryn timestoob-
p
Sen,andSuzumura2010;Rawls2017).Wemakeuseofcar-
tain a set R of n candidate reward functions. Afterwards,
p
dinalsocialwelfarefunctions(KeeneyandKirkwood1975)
foreachfunctionR∈Rwecomputetheutilityfeaturedis-
which take as input our alignment scores (s (R))
tributions of the policy Π(R) induced by the reward func- i i∈[ℓ],R∈R
and output the winning reward function. We consider the
tion R on the given RMAB instance (via repeatedly sim-
threearguablymostpopularsocialwelfarefunctions:
ulating the policy on the instance). Then, the prompt and
the set of candidate reward functions together with the as- Utilitarian Returntherewardfunctionmaximizingthesum
sociated utility feature distributions are passed to an LLM, ofitsscores,i.e.,argmax (cid:80) s (R).
whichisaskedtoselecttherewardfunctionR′fromRbest R∈R i∈[ℓ] i
alignedwiththeprompt(Maetal.2023;Shinnetal.2023). Nash Return the reward function maximizing the product
(cid:81)
Now, we repeat the whole process, this time including the ofitsscores,i.e.,argmax R∈R i∈[ℓ]s i(R).
selected policy R′ as a seed in the reward function genera-
Egalitarian Return the reward function maximizing its
tionprompts.Oncewehaveexecutedtheprocessn times,
r minimumscore,i.e.,argmax min s (R).
we add all generated n ·n candidate reward functions R R∈R i∈[ℓ] i
p r
tothepoolR(seeSectionC.2fordetails). Socialwelfarefunctionsalsoallowforassigningadifferent
importancetoclauses:Theusercouldsubmitanimportance
5.2 Adjudicator scorew foreachclausep ,whichcanbeeasilyincorporated
i i
Theadjudicatorselectsarewardfunctionfromagivenpool in the social welfare function, e.g., the Utilitarian welfare
(cid:80)
of candidate reward functions returned by the generator. functionbecomesargmax R∈R i∈[ℓ]w i·s i(R).
To handle the complex tradeoffs arising within composite Selectingthesocialwelfarefunctiongivesuscontrolover
promptsandtheresultingmulti-objectiveoptimizationprob- thetradeoffsbetweenobjectives:Bypickingtheegalitarian
lem,theadjudicatorfollowsasocialchoiceapproach.Social function, we ensure that one clause will not get prioritized
choice is a discipline at the intersection of economics, phi- overanother.Incontrast,theUtilitarianfunctionprioritizes
losophy, and mathematics and concerned with aggregating the summed alignment, allowing for mismatches between
the potentially contradicting preferences of a set of voters clauses;theNashfunctionstrikesabalancebetweenthetwo
into a fair compromise alternative from a given candidate functions.Further,theadjudicatormakestheselectionpro-
set(Arrow,Sen,andSuzumura2010;Moulin2004).Itthus cessmoretransparent,asthedifferentobjectives,theselec-
providesatheoreticallygroundedandwell-studiedmethod- tioncriterion,andtheperformanceofthecandidatereward
ologyforbalancingcompetinginterests.Inourproblem,we functionsregardingtheobjectivesbecomeexplicit.
4It remains to describe how the alignment scores s (R) a prioritization prompt is that it can sharply decrease the
i
are computed. We present two general methods to com- summed utility generated by all agents: The user might re-
pute alignment scores, which we will use for prioritiza- quest the prioritization of a subpopulation that does not
tionclauses.Subsequently,wediscusstwomorecustomized benefit much from receiving a resource, leading to severe
methods for the prevention of unintended utility shifts or drops in the summed utility generated by all agents. Users
dropsintotalgeneratedutility. can address this issue in our model by adding a clause
p =“maximizethetotalgeneratedutility”totheprompt.As
Simulator Scorer Model (SCLM-SIM) For each pref- i
thealignmentscores (R)ofp withsomerewardfunction
erence clause p ∈ P, we compute a reward function R i i
i i R we compute the summed utility generated by all agents
alignedwithp bycastingitasasingularprompttotheDLM
i under the policy Π(R) (computed via multiple simulations
pipeline(seeSectionC.2).ForeachR ∈R,wecomputeas
ofthepolicyonthegiveninstance).Weagainapply0-1nor-
s (R)theexpectedrewardaccordingtorewardfunctionR
i i malizationtoallscoress (R) forpromptp .
producedbypolicyΠ(R)(again,weapproximatethisquan- i R∈R i
tity by running multiple simulations). s (R) quantifies the
i 6 Experiments
qualityofthepolicyinducedbythecandidaterewardfunc-
tionRfromtheperspectiveofp (ascapturedbyR ).Asthe Wedescribeourtestset(Section6.1),thecomparedmethods
i i
scale of the reward functions can vary significantly among (Section6.2),andourexperimentalresultsbothfordealing
preference clauses, we normalize the scores by the perfor- withcompositeprioritizationprompts(Section6.3)andad-
manceofthedefaultpolicy,i.e.,wecompute
si(R)−si(R∗). ditionallyminimizingunintendedsideeffects(Section6.4).
si(R∗) Following the work of Behari et al. (2024), which consti-
LLM Scorer Model (SCLM-LLM) The Simulator tutesourmostimportantbaseline,weuseGeminiPro(Anil
ScorerModelassumesaccesstorewardfunctionscapturing etal.2023)astheLLMinourexperiments.
individualpreferenceclauseswell.Ifnowell-alignedreward
6.1 DatasetDescription
functions can be obtained, the performance of SCLM-SIM
can deteriorate because it can become very noisy. Another Synthetic Domain We create three synthetic datasets,
disadvantageofSCLM-SIMisthatthescoresinSCLM-SIM each with three features (A, B, and C). For some agent
are all computed via simulation, which can become com- and feature, we randomly sample the agent’s feature value
putationally quite costly. Motivated by this, we propose a between 0 and 1. Arm’s passive transition probabilities
quicker and more flexible LLM-based approach, where we (P(s,a = 0,s′) for s,s′ ∈ [0,1]) are sampled uniformly
prompt an LLM to rate the alignment of a candidate re- between 0 and 1. Our three datasets differ in how active
ward function with a preference clause. In particular, for transitionprobabilitiesaresampled.Foreachdataset,weset
each R ∈ R and p ∈ P, we use a prompt that includes a three-dimensional weight vector W ∈ [0,1]3 specifying
i
R,p ,andtheutilityfeaturedistributionsproducedbypol- howmucheachfeatureimpactstheeffectofapplyinganac-
i
icyΠ(R).WeasktheLLMtorankhowwellRalignswith tiveaction(seeSectionC.3fordetails).Foreachagent,letf
thepreferenceclausep onascalefrom1to5(seeSectionE denotetheirfeaturevalues;wesampletheiractivetransition
i
forprompttexts). probabilitiesfors,s′ ∈[0,1]asP(s,a=1,s′)=P(s,a=
0,s′)+δ, δ ∼N(∆,σ),where∆=W ·fT andσisahy-
Preventing Unintended Utility Shifts and Utility Drop
perparameter. Subsequently, we discretize the values of all
Aligning reward functions to a prioritization prompt might
featuresinto5differentequal-sizedbucketstobeconsistent
cause (unintended) utility shifts in other features (e.g., due
with the real-world domain. For each dataset, we sample 5
to feature correlations, shifting utility to low-income ben-
instancesaccordingtotheaboveprocedurewithN = 2100
eficiaries might shift it away from more educated ones).
arms,abudgetofB = 210,andatimehorizonofT = 12
See Section B for a concrete example. SCLM offers users
(to replicate the setup of the real-world domain described
theoptiontoexplicitlypreventtheseshiftsbyaddingaddi-
below where roughly 3%−10% of beneficiaries receive a
tionalclauses(“objectives”)totheprompt:Givenaprompt
resourceeveryweekforuptothreemonths).
P (e.g.,thepromptfromExample4.1),foreachfeaturenot
referencedintheprompt,theusercanaddanewpreference Real-WorldDomain ARMMAN(2024)isanon-profitin
clauserequestingaminimumshiftintheutilitydistribution Indiathatoperateslarge-scaleMaternalandChildCareMo-
ofthisfeature(e.g.,forExample4.1theycouldadd“donot bileHealthprogramsforunderservedcommunities.Oneof
change the utility distribution for feature C”). To compute their programs disseminates critical health information via
thealignmentscores (R)betweenarewardfunctionRand weekly automated voice messages. The goal of the NGO
i
aclausep =“minimizeutilityshiftforfeatureX”,wecom- is to maximize beneficiaries’ engagement, i.e., the num-
i
parefeatureX’sutilitydistributionunderthedefaultpolicy ber of messages they listen to. A limited number of bene-
with its utility distribution under the policy Π(R). Specifi- ficiaries are called by health workers every week to boost
cally,wequantifythedifferenceusingtheEarthmover’sdis- engagement. The problem of planning which beneficiaries
tance (EMD) between the two distributions. Afterward, we to call has been modeled and solved as an RMAB, where
apply0-1normalizationtoallscoress (R) forprompt the good/bad state corresponds to a high/low weekly en-
i R∈R
p ,whichareinputtothesocialwelfarefunction(alongwith gagementofthebeneficiary.Weuseanonymizeddatafrom
i
thealignmentscoresfortheotherclauses). a quality improvement study conducted in January 2022
Anotherpotentialriskofaligningarewardfunctionwith (Verma et al. 2023). For each beneficiary, we have access
5(a) Synthetic domain: sum % change (left) and minimum of % (b) Real-world domain: sum % change (left) and minimum of %
change(right)inutilityforthetwogroupsprioritized. changeinutilityforthetwogroupsprioritized.
Figure3:Resultscomparingthequalityofrewarddesignmethodsforcompositeprioritizationprompts.Resultsareaveraged
across180=12·15values:12compositepromptson15RMABinstances(from3datasets).Errorbarsrepresentstd-error.
to their income, education, and age level, which we use as preferencepromptandprovidestheproblemandfeature
ourthreefeatures.Beneficiaries’historiclistenershipvalues descriptionasadditionalcontextintheprompt.
are used to estimate their transition probabilities under the DLM This baseline implements the Decision-Language
passive action (Mate et al. 2022). One problem in estimat- ModelbyBeharietal.(2024)(seeSectionC.2).
ing transition probabilities under the active action is that
DLM-PromptEngg This is a modified version of DLM
duetothelimitednumberofservicecallsmade,suchtran-
wherewithinthereflectionprompt,weincludeexamples
sitionsarerare.Thus,activetransitionprobabilityestimates
forsingularqueriesofhowtheLLMshouldreasonover
arenoisy.Toalleviatethisissue,weusethefeaturesandpas-
thedifferentrewardfunctionchoices(seeSectionE).
sive transition probabilities from ARMMAN together with
the procedure to sample active transitions described in the 6.3 Results:CompositePrioritizationPrompts
synthetic domain. Again, we create datasets for three dif-
We analyze the performance on the 12 composite prompts
ferentweightvectorsW.Eachdatasetconsistsoffivesam-
which request the prioritization of two subpopulations (see
pled RMAB instances with N = 2100 arms, a budget of
SectionD.1foradditionalresults).
B =210andatimehorizonofT =12.
Evaluation Metrics As our goal is to fulfill the prefer-
Problem Instances Instances of our problem consist of
encesspecifiedbytheuser(incontrasttotheclassicgoalof
two parts: A preference prompt and an RMAB instance.
maximizingtotalutility),weneedtoquantifythealignment
We initially focus on prioritization prompts. Specifically,
ofthereturnedrewardfunctionwiththegivenpromptP to
foreachfeatureX,weconsidertwodifferentprioritization
evaluateourmodels.Duetothecomposite,multi-objective
clauses“Prioritizeagentswithlow/highvalueoffeatureX”.
natureofourprompts,westartbymeasuringthealignment
Thisgivesriseto6singularpromptsconsistingofonepriori-
of the returned reward function R with each prioritization
tizationclause,twoforeachfeature.Forcompositeprompts,
clausep ∈P inaseparateevaluationscoree (R).Forthis,
wetakeallcombinationsoftwofeaturesandthetwopriori- i i
weneedtoquantifyhowwellagivenrewardfunctionpriori-
tizationclausesforeachfeature(e.g.“Prioritizeagentswith
tizesthesubpopulationspecifiedintheprompt.However,as
high value of feature A and also prioritize agents with low
ourpromptsarewritteninhumanlanguage,thesesubpopu-
value of feature B”). This results in 3·4 = 12 composite
lationsarenotpreciselydefined(asthepromptsonlyspeak
prompts. For each domain, we run each prompt on the 15
ofagentswith“high”/“low”valueofsomefeatureX).No-
RMABinstancesfromthethreedatasets.
tably,onecouldthinkthatthescoress (R)computedinour
i
adjudicatorcouldbeusedasourevaluationscorese (R),as
6.2 Models&Baselines i
theymeasurehowwellarewardRalignswithaprioritiza-
WeanalyzefourdifferentvariantsofSCLMdifferinginthe
tion clause p . However, this would create an unfair com-
i
usedsocialwelfarefunction(UtilitarianorEgalitarian)and
petitiveadvantagefortheSCLMcomparedtoourbaselines
scorermodel(SimulatororLLM),e.g.,wedenoteasSCLM-
whodonothaveaccesstothesescores.
SIM-Egalitarian SCLM with the Simulator Scorer Model
Instead,wetakeadifferentapproachandassumethatthe
and the Egalitarian social welfare function.1 In our gener-
terms “low” and “high” in the input prompts refer to the
ator,wegenerate4candidaterewardfunctionsineachstep
most extreme feature values. Let p be some prompt prior-
i
andrun5iterationstogenerateatotalof20candidatereward
itizingagentswithahigh/lowvalueofsomefeatureX.As
functions. In addition, we consider several LLM-focused
theevaluationscoree (R),wecomputethesummedutility
i
baselines(seeSectionEfordetaileddescriptions):
generatedbytheagentswithhighest/lowestvalueofX un-
LLM-Zeroshot This baseline only queries the LLM once. derthepolicyΠ(R)normalizedbytheutilitygeneratedby
Itaskstoreturnarewardfunctionalignedwiththegiven theseagentsunderthedefaultpolicyΠ(R∗).2Reflectingthe
1WealsoranourexperimentsfortheNashwelfarefunction.Its 2In Section D.1, we also check how the results change if we
performancewasverysimilar,yetslightlyinferiortotheUtilitarian insteadinterpret“low”/“high”torefertothelowest/highesttwoor
welfarefunctioninallrelevantevaluationdimensions. threevalues.Weobserveverysimilartrends.
6Table 1: Results comparing different methods for aligning tor)isneeded.Finally,LLM-zeroshotconsistentlyperforms
with prioritization prompts while minimizing unintended the worst, which highlights the non-triviality of the reward
utilityshifts.Ahighersummedutilitychangeimpliesabet- design problem and the need for a guided extensive search
teralignment,whereaslessunintendedshiftisbetter. withintherewardfunctionsearchspace.
SyntheticDomain Real-WorldDomain 6.4 Results:UtilityShifts&CumulativeUtility
Summed%UtilityChange Unintended Summed%UtilityChange Unintended
inDesiredFeature(s) Shift inDesiredFeature(s) Shift As discussed in Section 5.2, we can also use our pipeline
DLM-OP -0.6±0.58 2.315±0.05 2.016±1.39 0.276±0.03
to prevent unintended side-effects of aligning reward func-
DLM-EP -1.882±0.87 2.346±0.08 -0.254±1.45 0.276±0.04
SCLM 4.854±0.55 0.12±0.01 6.01±1.8 0.062±0.02 tions with prioritization clauses, i.e., (i) shifts in the utility
feature distribution of features not included in the prompt
and(ii)dropsinthetotalgeneratedutility.Wefocuson(a)
multi-objectivenatureofourproblem,weconsidertwomet- hereandrelegatetheresultsfor(b),whichpaintaverysim-
rics for measuring the alignment of a reward R with a full ilar picture, to Section D.3. We analyze all 6 singular and
composite prompt: the sum and minimum of the % change 12 composite prompts, where we add additional clauses to
of the utility generated by the two prioritized groups under prevent shifts in the utility distribution of all features not
policy Π(R) compared to the default policy, i.e., the sum referencedintheprompt.WeuseSCLM-SIM-Utilitarianas
(resp.minimum)oftheevaluationscoresforR. ourmethod.Asbaselines,weconsiderDLMonlyprompted
withtheprioritizationclause(s)(calledDLM-OP)andDLM
Results In Figure 3, we show the averaged results from promptedwiththeprioritizationclause(s)andclause(s)ask-
thesyntheticandreal-worlddomain.Wedepicttheaverage ingfortheminimizingofutilityshifts(calledDLM-EP).
summed and minimum alignment with the two clauses of Tocomputethealignmentwiththeprioritizationclauses,
thecompositeprompt,i.e.,theminimum/summedchangein similartoSection6.3,wecomputetheaveragechangeinthe
theutilitygeneratedbytheprioritizedgroupofagents. utilitygeneratedbytheprioritizedsubpopulations.Toquan-
We start by focusing on SCLM with Simulator Scorer tify the unintended utility shifts, we compute the average
SCLM-SIM (green-shaded bars), our strongest method. On Earth mover’s distance between the utility feature distribu-
both domains, SCLM-SIM significantly outperforms all tionunderthecandidateanddefaultrewardfunctionforeach
baselines for both minimum and summed % change inde- featurenotincludedinoneoftheprioritizationclauses.
pendentofwhethertheUtilitarianorEgalitariansocialwel- Table 1 shows the results. Comparing DLM-OP and
farefunctionischosen.SCLM-SIM-Utilitarianoutperform- DLM-EP,wefindthataddingtheadditionalobjectivetothe
ingthebaselinesfortheminimumchangeandSCLM-SIM- prompt does not result in a better performance for the syn-
Egalitarian outperforming them for the summed change thetic and real-world domains. In contrast, SCLM chooses
highlights the advantages of the SCLM, as these objec- reward functions resulting in significantly higher utility in-
tives are not explicitly optimized by the respective mod- creases for the prioritized subpopulations and significantly
els, e.g., SCLM-SIM-Utilitarian aims at maximizing the fewerunintendedutilityshifts.ThefactthatSCLMperforms
summed change and not the minimum one, but still per- advantageously for both (conflicting) objectives highlights
forms well regarding the minimum change. This indicates the quality of the pipeline and its capabilities to effectively
thatSCLMindependentofthechosenwelfarefunctiondoes addressmultipleobjectives(ofdifferenttypes).
abetterjobatpickingeffectiveandalignedrewardfunctions
(ontheParetofront).ComparingSCLM-SIM-Utilitarianand 7 Discussion
SCLM-SIM-Egalitarian, the two methods exhibit a big dif-
We present a customizable Social Choice Language Model
ferenceunderthesummedchangecriterion,whilethediffer-
tohandlethemulti-objectivenatureofpreferencepromptsin
enceregardingtheminimumchangeismuchsmaller.Exam-
rewarddesignforRMABs.Weshowcasehowmethodsfrom
ining individual instances we find in Section D.1 that both
social choice can be used to improve the quality and trans-
functions lead to very different selections on the instance
parency of decision-making of LLM-based frameworks, as
level;unsurprisingly,theEgalitarianmethodcreatesrewards
we present an adjudicator component that makes the final
thatbenefitbothgroupsinamorebalancedfashion.
decision from options generated by the LLM. SCLM sig-
IfwereplacetheSimulationScorerwiththeLLMScorer,
nificantly improves the quality of the chosen reward func-
the performance of the SCLM decreases, but is still bet-
tions.WedemonstratethatSCLMcannotonlyhandlecom-
ter than all of our three baselines. The difference between
positeprioritizationpromptsbutarbitrarypromptscontain-
theLLMandSimulationScorerhighlightstheadvantageof
ing multiple objectives, e.g., balancing the prioritization of
theadditional informationacquiredthrough themore com-
subpopulationswiththetotalutilitygeneratedbyallagents.
plexandcomputationallyexpensivesimulationmethod.Re-
For future work, SCLM can be applied to other problems
garding the performance of the baselines, our DLM base-
from multiagent planning and reinforcement learning. Fur-
line with prompt engineering DLM-PromptEngg improves
ther,SCLMcaneasilybeextendedtohandlemultiplepref-
slightly upon the results of DLM in the synthetic domain,
erencepromptsspecifiedbydifferentusers.
whileinthereal-worlddomain,theirperformanceissimilar.
Thissuggeststhatpromptengineeringitselfisnotsufficient
to adequately deal with the multi-objective nature of com-
posite prompts; an external component (like our adjudica-
7References atic review of the literature. PLoS neglected tropical dis-
Abbou, A.; and Makis, V. 2019. Group Maintenance: A eases,12(2):e0006211.
Restless Bandits Approach. INFORMS J. Comput., 31(4): Fan, Z.; Peng, N.; Tian, M.; and Fain, B. 2023. Wel-
719–731. fare and Fairness in Multi-objective Reinforcement Learn-
Anil et al., R. 2023. Gemini: A Family of Highly Capable ing. In Proceedings of the 2023 International Conference
MultimodalModels. CoRR,abs/2312.11805. onAutonomousAgentsandMultiagentSystems,1991–1999.
ACM.
ARMMAN. 2024. ARMMAN: Advancing Reduction
in Mortality and Morbidity of Mothers, Children, and Fish, S.; Go¨lz, P.; Parkes, D. C.; Procaccia, A. D.; Rusak,
Neonates. https://armman.org/. G.; Shapira, I.; and Wu¨thrich, M. 2023. Generative Social
Choice. CoRR,abs/2309.01291.
Arrow,K.J.;Sen,A.;andSuzumura,K.2010. Handbookof
socialchoiceandwelfare. Elsevier. Ge,L.;Halpern,D.;Micha,E.;Procaccia,A.D.;Shapira,I.;
Ayer, T.; Zhang, C.; Bonifonte, A.; Spaulding, A. C.; and Vorobeychik,Y.;andWu,J.2024.AxiomsforAIAlignment
Chhatwal,J.2019. PrioritizinghepatitisCtreatmentinUS fromHumanFeedback. CoRR,abs/2405.14758.
prisons. OperationsResearch,67(3):853–873. Goyal,P.;Niekum,S.;andMooney,R.J.2019. Usingnat-
Bakker, M. A.; Chadwick, M. J.; Sheahan, H.; Tessler, urallanguageforrewardshapinginreinforcementlearning.
M. H.; Campbell-Gillingham, L.; Balaguer, J.; McAleese, arXivpreprintarXiv:1903.02020.
N.; Glaese, A.; Aslanides, J.; Botvinick, M. M.; and Sum- Hazra,R.;Sygkounas,A.;Persson,A.;Loutfi,A.;andMar-
merfield, C. 2022. Fine-tuning language models to find tires, P. Z. D. 2024. REvolve: Reward Evolution with
agreement among humans with diverse preferences. In Large Language Models for Autonomous Driving. CoRR,
Koyejo,S.;Mohamed,S.;Agarwal,A.;Belgrave,D.;Cho, abs/2406.01309.
K.;andOh,A.,eds.,AnnualConferenceonNeuralInforma-
Ho, J.; and Ermon, S. 2016. Generative adversarial imita-
tionProcessingSystems2022.
tion learning. Advances in neural information processing
Basavatia,S.;Murugesan,K.;andRatnakar,S.2024.STAR- systems,29.
LING: Self-supervised Training of Text-based Reinforce-
Jiang,J.;andLu,Z.2019. Learningfairnessinmulti-agent
ment Learning Agent with Large Language Models. arXiv
systems. Advances in Neural Information Processing Sys-
preprintarXiv:2406.05872.
tems,32.
Behari, N.; Zhang, E.; Zhao, Y.; Taneja, A.; Nagaraj, D.;
Keeney,R.L.;andKirkwood,C.W.1975. Groupdecision
andTambe,M.2024. ADecision-LanguageModel(DLM)
making using cardinal social welfare functions. Manage-
for Dynamic Restless Multi-Armed Bandit Tasks in Public
mentScience,22(4):430–437.
Health. CoRR,abs/2402.14807.
Kwon, M.; Xie, S. M.; Bullard, K.; and Sadigh, D. 2023.
Bloembergen, D.; Tuyls, K.; Hennes, D.; and Kaisers, M.
RewardDesignwithLanguageModels. InTheEleventhIn-
2015. Evolutionary dynamics of multi-agent learning: A
ternationalConferenceonLearningRepresentations,ICLR
survey. JournalofArtificialIntelligenceResearch,53:659–
2023,Kigali,Rwanda,May1-5,2023.OpenReview.net.
697.
Ladosz, P.; Weng, L.; Kim, M.; and Oh, H. 2022. Explo-
Cao, Y.; Zhao, H.; Cheng, Y.; Shu, T.; Liu, G.; Liang, G.;
rationindeepreinforcementlearning:Asurvey.Information
Zhao, J.; and Li, Y. 2024. Survey on Large Language
Fusion,85:1–22.
Model-EnhancedReinforcementLearning:Concept,Taxon-
omy,andMethods. CoRR,abs/2404.00282. Li, S.; Puig, X.; Paxton, C.; Du, Y.; Wang, C.; Fan, L.;
Carta,T.;Oudeyer,P.-Y.;Sigaud,O.;andLamprier,S.2022. Chen,T.;Huang,D.-A.;Akyu¨rek,E.;Anandkumar,A.;etal.
Eager: Asking and answering questions for automatic re- 2022. Pre-trainedlanguagemodelsforinteractivedecision-
ward shaping in language-guided rl. Advances in Neural making. Advances in Neural Information Processing Sys-
InformationProcessingSystems,35:12478–12490. tems,35:31199–31212.
Conitzer,V.;Freedman,R.;Heitzig,J.;Holliday,W.H.;Ja- Lin, J.; Du, Y.; Watkins, O.; Hafner, D.; Abbeel, P.; Klein,
cobs,B.M.;Lambert,N.;Mosse´,M.;Pacuit,E.;Russell,S.; D.;andDragan,A.2023. Learningtomodeltheworldwith
Schoelkopf,H.;Tewolde,E.;andZwicker,W.S.2024. So- language. arXivpreprintarXiv:2308.01399.
cialChoiceforAIAlignment:DealingwithDiverseHuman Ma,Y.J.;Liang,W.;Wang,G.;Huang,D.;Bastani,O.;Ja-
Feedback. CoRR,abs/2404.10271. yaraman, D.; Zhu, Y.; Fan, L.; and Anandkumar, A. 2023.
Dai,J.;andFleisig,E.2024.MappingSocialChoiceTheory Eureka: Human-Level Reward Design via Coding Large
toRLHF. CoRR,abs/2404.13038. LanguageModels. CoRR,abs/2310.12931.
Das,D.;Chernova,S.;andKim,B.2023.State2explanation: Ma, Y. J.; Liang, W.; Wang, H.; Wang, S.; Zhu, Y.; Fan,
Concept-based explanations to benefit agent learning and L.; Bastani, O.; and Jayaraman, D. 2024. DrEureka:
user understanding. Advances in Neural Information Pro- Language Model Guided Sim-To-Real Transfer. CoRR,
cessingSystems,36:67156–67182. abs/2406.01967.
Deardorff, K. V.; Rubin Means, A.; A´sbjo¨rnsdo´ttir, K. H.; Margolis, G. B.; and Agrawal, P. 2023. Walk these ways:
andWalson,J.2018. Strategiestoimprovetreatmentcover- Tuningrobotcontrolforgeneralizationwithmultiplicityof
ageincommunity-basedpublichealthprograms:asystem- behavior. InConferenceonRobotLearning,22–31.PMLR.
8Mate, A.; Madaan, L.; Taneja, A.; Madhiwalla, N.; Verma, Song, C. H.; Wu, J.; Washington, C.; Sadler, B. M.; Chao,
S.; Singh, G.; Hegde, A.; Varakantham, P.; and Tambe, M. W.-L.; and Su, Y. 2023. Llm-planner: Few-shot grounded
2022. Field study in deploying restless multi-armed ban- planning for embodied agents with large language models.
dits: Assisting non-profits in improving maternal and child In Proceedings of the IEEE/CVF International Conference
health. InProceedingsoftheAAAIConferenceonArtificial onComputerVision,2998–3009.
Intelligence,volume36,12017–12025.
Syed,S.T.;Gerber,B.S.;andSharp,L.K.2013. Traveling
Mirchandani,S.;Karamcheti,S.;andSadigh,D.2021. Ella: towardsdisease:transportationbarrierstohealthcareaccess.
Explorationthroughlearnedlanguageabstraction.Advances Journalofcommunityhealth,38:976–993.
inneuralinformationprocessingsystems,34:29529–29540. Vamplew, P.; Dazeley, R.; Berry, A.; Issabekov, R.; and
Moffaert, K. V.; Drugan, M. M.; and Nowe´, A. 2013. Dekker,E.2011.Empiricalevaluationmethodsformultiob-
Hypervolume-BasedMulti-ObjectiveReinforcementLearn- jective reinforcement learning algorithms. Machine learn-
ing. In Purshouse, R. C.; Fleming, P. J.; Fonseca, C. M.; ing,84:51–80.
Greco, S.; and Shaw, J., eds., 7th International Conference VanMoffaert,K.;andNowe´,A.2014. Multi-objectiverein-
onEvolutionaryMulti-CriterionOptimization,volume7811 forcementlearningusingsetsofparetodominatingpolicies.
ofLectureNotesinComputerScience,352–366.Springer. The Journal of Machine Learning Research, 15(1): 3483–
Moulin,H.2004. Fairdivisionandcollectivewelfare. MIT 3512.
press. Verma, S.; Singh, G.; Mate, A.; Verma, P.; Gorantla, S.;
Madhiwalla, N.; Hegde, A.; Thakkar, D.; Jain, M.; Tambe,
Nelson,L.A.;Mulvaney,S.A.;Gebretsadik,T.;Ho,Y.-X.;
M.;andTaneja,A.2023.Expandingimpactofmobilehealth
Johnson, K. B.; and Osborn, C. Y. 2016. Disparities in the
programs: SAHELI for maternal and child care. AI Mag.,
useofamHealthmedicationadherencepromotioninterven-
44(4):363–376.
tionforlow-incomeadultswithtype2diabetes. Journalof
the American Medical Informatics Association, 23(1): 12– Verma, S.; Zhao, Y.; Sanket Shah, N. B.; Taneja, A.;
18. and Tambe, M. 2024. Group Fairness in Predict-Then-
OptimizeSettingsforRestlessBandits.openreview.net/pdf?
Ng,A.Y.;Harada,D.;andRussell,S.1999. Policyinvari-
id=GJlZbpLWX3.
anceunderrewardtransformations:Theoryandapplication
torewardshaping. InIcml,volume99,278–287. Whittle, P. 1988. Restless bandits: Activity allocation in a
changingworld.Journalofappliedprobability,25(A):287–
Nin˜o-Mora, J. 2023. Markovian restless bandits and index
298.
policies:Areview. Mathematics,11(7):1639.
Xie, T.; Zhao, S.; Wu, C. H.; Liu, Y.; Luo, Q.; Zhong, V.;
Papadimitriou, C. H.; and Tsitsiklis, J. N. 1994. The com-
Yang,Y.;andYu,T.2024a. Text2Reward:RewardShaping
plexityofoptimalqueueingnetworkcontrol.InProceedings
withLanguageModelsforReinforcementLearning. InThe
of IEEE 9th annual conference on structure in complexity
Twelfth International Conference on Learning Representa-
Theory,318–322.IEEE.
tions,ICLR2024.OpenReview.net.
Qian, Y.; Zhang, C.; Krishnamachari, B.; and Tambe,
Xie, T.; Zhao, S.; Wu, C. H.; Liu, Y.; Luo, Q.; Zhong, V.;
M. 2016a. Restless Poachers: Handling Exploration-
Yang,Y.;andYu,T.2024b. Text2Reward:RewardShaping
Exploitation Tradeoffs in Security Domains. In Jonker,
withLanguageModelsforReinforcementLearning. InThe
C. M.; Marsella, S.; Thangarajah, J.; and Tuyls, K., eds.,
Twelfth International Conference on Learning Representa-
Proceedings of the 2016 International Conference on Au-
tions.
tonomousAgents&MultiagentSystems,Singapore,May9-
Yang, R.; Sun, X.; and Narasimhan, K. 2019. A gener-
13,2016,123–131.ACM.
alized algorithm for multi-objective reinforcement learning
Qian, Y.; Zhang, C.; Krishnamachari, B.; and Tambe, andpolicyadaptation. Advancesinneuralinformationpro-
M. 2016b. Restless poachers: Handling exploration- cessingsystems,32.
exploitation tradeoffs in security domains. In Proceedings
Yu, W.; Gileadi, N.; Fu, C.; Kirmani, S.; Lee, K.; Arenas,
ofthe2016InternationalConferenceonAutonomousAgents
M.G.;Chiang,H.L.;Erez,T.;Hasenclever,L.;Humplik,J.;
&MultiagentSystems,123–131.
Ichter,B.;Xiao,T.;Xu,P.;Zeng,A.;Zhang,T.;Heess,N.;
Rawls,J.2017.Atheoryofjustice.InAppliedethics,21–29. Sadigh,D.;Tan,J.;Tassa,Y.;andXia,F.2023. Languageto
Routledge. Rewardsfor RoboticSkill Synthesis. In Tan,J.; Toussaint,
Roijers, D. M.; Vamplew, P.; Whiteson, S.; and Dazeley, M.; and Darvish, K., eds., Conference on Robot Learning,
R. 2013. A survey of multi-objective sequential decision- volume229ofProceedingsofMachineLearningResearch,
making. JournalofArtificialIntelligenceResearch,48:67– 374–404.PMLR.
113. Zhang,C.B.C.;andRamponi,G.2023. HIP-RL:Halluci-
Shinn, N.; Cassano, F.; Gopinath, A.; Narasimhan, K.; and nated Inputs for Preference-based Reinforcement Learning
Yao,S.2023. Reflexion:languageagentswithverbalrein-
inContinuousDomains.InICML2023WorkshopTheMany
forcementlearning. InOh,A.;Naumann,T.;Globerson,A.;
FacetsofPreference-BasedLearning.
Saenko,K.;Hardt,M.;andLevine,S.,eds.,AnnualConfer- Zimmer,M.;Glanois,C.;Siddique,U.;andWeng,P.2021.
enceonNeuralInformationProcessingSystems2023. Learning fair policies in decentralized cooperative multi-
9agent reinforcement learning. In International Conference
onMachineLearning,12967–12978.PMLR.
10Appendix
A AdditionalRelatedWork
Multi-Objective Reinforcement Learning Real-world
decision-making often involves trade-offs between mul-
tiple, conflicting objectives, making multi-objective rein-
forcementlearning(MORL)essential.Acommonapproach
in MORL is to scalarize multiple objectives into a single
reward function, but this can oversimplify the problem. To
address this, Pareto-based methods identify Pareto-optimal
(a)Incomefeature. (b)Educationfeature.
solutionsthatreflectdifferenttrade-offsbetweenobjectives,
providing a more comprehensive decision-making frame-
Figure4:Utilityfeaturedistributionsforthedefaultreward
work (Roijers et al. 2013; Van Moffaert and Nowe´ 2014).
function (orange) and reward function returned for prompt
RecentadvancementsinMORLhaveintroducedalgorithms
“Prioritize agents with low income” (blue). The x-axis de-
that enhance scalability, handle non-linear objectives, and
pictsthevaluesofthefeatureandthey-axisthetotalutility
improve exploration strategies. For example, (Yang, Sun,
generatedbyallagentswiththisfeaturevalue.
and Narasimhan 2019) proposed an algorithm that adapts
policiesbasedonchangingobjectiveimportance.Addition-
ally, empirical methods have been developed to evaluate B AdditionalMaterialforSection4
MORLperformanceacrossscenarios(Vamplewetal.2011).
Unintended Utility Shifts Moving from the default re-
Inmulti-agentsystems,MORLaddscomplexityasagents
wardfunctiontoarewardfunctionalignedwithagiven(pri-
mustbalancetheirobjectiveswhileconsideringothers’.Spe-
oritization) prompt causes shifts within the distribution of
cialized approaches, such as extending Pareto-based meth-
resources and utility. Due to correlations between features,
ods tomulti-agent contexts,aim tofind Pareto-efficient so-
thismightchangemightleadtounintendedutilityshiftsfor
lutions for all agents involved (Bloembergen et al. 2015).
features not specified in the prompt. In Figure 4, we show
Moreover, the concept of fairness becomes important in
an instance of our experiments. We present the utility fea-
multi-agent MORL, as solutions must be equitable to
ture distribution for the two features income and education
all agents while still optimizing overall system perfor-
for two reward functions: The reward function selected by
mance. Researchers have begun to investigate fairness-
DLM for the prompt “ Prioritize agents with low income ”
aware MORL algorithms that explicitly consider the dis-
(orange)andthedefaultreward(blue).Whiletheutilitygen-
tribution of rewards among agents, ensuring that no sin-
eratedbylow-incomeagentsincreases,theutilitygenerated
gle agent disproportionately benefits at the expense of oth-
by highly educated agents decreases, a side-effect a user is
ers(JiangandLu2019;Zimmeretal.2021).
likely unaware of and that might conflict with their alloca-
tiongoals.
(Pre-LLM) Reward Function Design Designing effec-
tive reward functions for RL has long been a challenge C ImplementationDetails
due to the lack of robust learning signals. (Ladosz et al.
C.1 WhittleIndexPolicy
2022).PreviousapproachesforrewarddesigninRLinclude
manually specifying reward components and tuning their To formulate the Whittle Index Policy, first we define the
coefficients (Ng, Harada, and Russell 1999; Margolis and valuefunctionforanarmi∈[N]underthesubsidyλas
Agrawal 2023), and learning reward models from demon-
Vλ(s)= max Q (s,a ,λ). (1)
strations (Ho and Ermon 2016) or preferences (Zhang and i i i
a∈{0,1}
Ramponi 2023). While promising, these methods often de-
Here,Q (s,a ,λ)measurestheexpecteddiscountedcumu-
mand substantial domain expertise, large amounts of data, i i
lativefuturerewardwhereasubsidyλisaddedtothereward
orcomputationallyexpensiveprocesses.
whenthepassiveactionistaken.TheWhittleindexassoci-
atedtothestates isthendefinedas:
LLMsandSocialChoice Inouradjudicator,wetakeaso- i
cialchoiceperspectivetomodeltheproblemofselectinga W (s ):=inf{Q (s ,a =0,m)=Q (s ,a =1,m)}.
i i i i i i i i
finalrewardfunctioninawaythatbalancesdifferentobjec- m
tives against each other. Principles from social choice have Thewhittleindexisthusthevalueofsubsidysuchthatthe
informedLLM-basedalgorithminpreviousworks,albeitin passive (a = 0) and active (a = 1) actions have the same
theverydifferentcontextofcreatingaconsensusstatements value.Intuitively,thiscapturesthevalueoftakingactiveac-
foragroupofuserswithdiverseviews(Bakkeretal.2022; tiononanarm.
Fishetal.2023).Moreover,socialchoiceprinciplesarealso ToimplementtheWhittleIndexcomputation,weusethe
increasinglyusedtoinformthefine-tuningoflanguagemod- methodin(Qianetal.2016b)basedonbinarysearch.Addi-
els, including its alignment with human feedback (Bakker tionally,foralltheexperiments,wecachecomputedwhittle
et al. 2022; Conitzer et al. 2024; Ge et al. 2024; Dai and index for a given set of transition probabilities and reward
Fleisig2024). functionstoreducecomputationtime.
11C.2 DLMPipeline Table 2: Weight vector parameters for different synthetic
datasets.
In our work, we use a modified version of the DLM
pipeline(Beharietal.2024),whichemploystheWhittleIn-
FeatureA FeatureB FeatureC σ
dex policy as the planner for RMAB. Our approach differs
from Behari et al. (2024) in that we use the Whittle Index Dataset1 0.8 -1.5 1 0.1
policy specifically for simulating RMAB, whereas Behari Dataset2 10 -1.5 1 0.1
etal.(2024)usethePreFeRMABpolicy(ZhaoandBehariet Dataset3 1 -1.5 10 0.1
al.2023).Thismodificationallowsforfasterandmorestable
simulations and effectively decouples the learning problem
fromtheplanningproblem. 2. Foreveryarmi,uniformlysamplethethreefeaturesf ∼
Specifically,theDLMconsistsofthreecomponents.First, Uniform([0,1]3)
a user provides a natural language preference P. We then
3. Define a three-dimensional weight vector W ∈ [0,1]3
createapromptforLLMwhichincludesthecontextofthe
andastandarddeviationparameterσ.
RMABproblem,thepreferenceP,adescriptionoffeatures
4. Sampletheeffectofinterventionfromthenormaldistri-
available,andtheindexofthosefeaturesinthefeaturearray.
butionasδ ∼N(∆,σ),where∆=W ·fT.
Finally,theLLMisaskedtogeneratethePythoncodeofthe
rewardfunctionintextformat.Wedescribethepromptused 5. Calculate active transition probabilities as P(s,a =
inFigures6and7. 1,s′)=P(s,a=0,s′)+δ
The LLM is queried n p times to obtain n p reward func- 6. Calculate the complimentary probabilities as
tions.Foreachrewardfunction,wealsocomputethereward P (s,a,s′ = 0) = 1 − P (s,a,s′ = 1), ∀s ∈
i i
distribution over all the features. Next, given all the gener- {0,1},a∈{0,1}
atedrewardfunctionsandtheircorrespondingrewarddistri-
Themagnitudeoftheweightvaluedeterminestheextent
butions, we query the LLM to select the best reward func-
to which a feature influences the effect of the intervention,
tion.WedescribethepromptusedinFigures8and9.This
while the direction indicates whether a low or high feature
iscalledthereflectionstep.Thebestrewardfunctionisthen
value amplifies this effect. The standard deviation param-
used inside the prompt for next step of generating n re-
p eter,σ,controlsthespreadofsampledprobabilitiesaround
wardfunctions.Thisprocessisrepeatedn timestoobtain
r themeaneffectoftheintervention.Table2showstheWeight
n ·n rewardfunctions.Inallourrewardfunctiongenera-
p r vectorusedforthethreesyntheticdatasetsgeneratedinour
tionexperiments,wequerytheLLMn = 4timesandrun
p experiments. We consider weight values that describe the
the reflection loop n = 5 times resulting in 20 candidate
r followingscenarios:i)allweightsareroughlyequal(dataset
rewardfunctions.
1) ii) one of the features has much higher weight than the
AsanLLM,weusetheGeminiPromodelbyGoogle.We
othertwo(dataset2)andweswitchwhichfeaturehasmax-
querytheLLMusingpythonbasedAPIfromthegenerative-
imumweight(dataset3).
ai-pythonlibrary.
C.4 RealWorldDataset
C.3 SyntheticDatasetGeneration
As described in Section 6.1, one challenge in estimating
AnRMABproblemisdefinedbythetransitionprobabilities transition probabilities under active actions is the rarity of
ofMarkovDecisionProcessgoverningeveryarmandthere- such transitions due to the limited number of service calls.
wardfunctions.Weconsidera2-state,2-actionMarkovde- Consequently, the estimates for active transition probabili-
cisionprocessinallourexperiments.Thisresultsin8tran- tiestendtobenoisy.Tomitigatethisissue,wefollowapro-
sition probability parameters P i(s,a,s′) ∀s ∈ {0,1},a ∈ ceduresimilartothatinthesyntheticdomaintoconstructthe
{0,1},s′ ∈ {0,1} for every arm i. Out of these 8 param- datasets, but with some modifications. Specifically, instead
eters, we only need to define 4 parameters P i(s,a,s′ = ofuniformlysamplingpassiveprobabilitiesinstep1,weuse
1) ∀s ∈ {0,1},a ∈ {0,1} independently, and the rest the transition probabilities estimated from real-world data
4 parameters can be calculated as the compliment values (Vermaetal.2023).Next,instep2,weusetheattributesde-
P (s,a,s′ = 0) = 1−P (s,a,s′ = 1) ∀s ∈ {0,1},a ∈
i i scribingthereal-worldbeneficiariestodefinefeatures.Thus,
{0,1}.
insteadoffeaturesA,BandC,weusefeaturesAge,Income
To simulate the effects of conflicting preferences and andEducation.Finally,weusethesameweightvectorval-
trade-offs in a controlled setup, we configure the four tran- uesasinTable2togeneratethreerealworlddatasets.This
sition probability parameters to depend on the features de- allows us to generate multiple datasets with varying levels
scribing each arm. We consider each arm to be character- of effect of intervention based on features while still using
izedbyavectorofcontinuousfeaturesf,withthreevalues the realistic passive transition probabilities and feature dis-
rangingbetween0and1(f ∈[0,1]3).Weusethefollowing tributions.
setup to stochastically generate the 4 transition probability
parameters. C.5 Hyperparameters
1. For every arm i, uniformly sample the two passive WerunallexperimentswithN = 2100arms,B = 210as
probability parameters P (s,a = 0,s′ = 1) ∼ budget,T = 12weeksandγ = 0.9asdiscountfactor.For
i
Uniform(0,1), ∀s∈{0,1}. everydataset,5RMABinstancesaregeneratedbasedonthe
12different weight vectors as described in the section above. Figure1,eachpointcorrespondstoacandidaterewardfunc-
Additionally,weruneachRMABsimulationwith10differ- tionwiththeaxesmeasuringalignmentwiththetworespec-
entseeds.Weestimatethecumulativerewardsandfeature- tive clauses. (as described in the beginning of the section
level utility by calculating the mean of the discounted sum 6.3). The SCLM-Utilitarian model chooses a reward func-
ofrewardsoverT timestepsacrossall10seeds. tionfromtheParetofrontierthatshowsamuchstrongeref-
fectforthesecondclause(i.e.,theutilityincreaseforagents
C.6 ComputingResources withalowvalueoffeatureC ismorepronounced).Incon-
Allexperimentsarerunonamachinewith16GBRAMand trast, the reward function selected by SCLM-Egalitarian is
M1 chip CPU. For every RMAB instance, it took roughly much more balanced. DLM-based baselines fail to pick a
3 hours to generate 360 candidate reward functions (20 re- rewardfunctionfromtheParetofrontier.
ward functions for each of the 18 prompts). The primary
bottleneck in speed was the API call limits for using LLM D.3 Tradeoffsinpreferencesandcumulative
(GeminiPro). utility
Toshowthegeneralapplicabilityofourframework,wecon-
D AdditionalResults
sider prompts where the user can specify preferences for
D.1 CompositePrioritizationPrompts multipleattributesandalsospecifyapreferenceclausethat
prefers maximizing utility. For all 6 singular and 12 com-
In Tables 3 and 4, we show results aggregated for the syn-
positeprompts,weaddadditionalclausestomaximizeutil-
theticandrealworlddatasets.Specifically,weshowtheSum
ity (or minimize drop in utility). As a baseline, we con-
of % change in utility and Minimum of % change in util-
sidertwomodels:DLMpromptedonlybytheprioritization
ity not just for the highest/lowest value of a feature but for
clause (DLM-Prioritization Only) and DLM prompted by
top/bottom two and three buckets. We also include results
boththeprioritizationclause(s)andaclauserequestingthe
from the scorer model that optimizes for Nash Social Wel-
maximizationofoverallutility(DLM-ExtendedPrompt).
farefunction.
Overall, we observe that the Simulator-based scorer To assess alignment with prioritization clauses, we cal-
(SCLM-SIM)performsbestinallscenarios,bothwhenop- culate the average change in utility for the prioritized sub-
timizing for the Utilitarian objective or the Egalitarian ob- group, following the method outlined in Section 6.3. To
jective.Itisalsoworthnotingthatwhenoptimizingforone quantifythescoreformaximizingutility,weusetheutility
of the objectives (for instance, Utilitarian objective maxi- generated by the default reward function, normalizing this
mizes the sum of % change in utility), SCLM outperforms valuebetween0and1toensureallscorecomponentsareon
baselines in the objective it is not explicitly optimizing for the same scale. Finally, with scores for all prompt compo-
(for instance, Minimum of % change in utility). Lastly, we nents,wecanapplyauser-definedSocialWelfareFunction.
observe that optimizing for the Nash objective yields very Table5showstheresults.ComparingDLM-Prioritization
similarperformanceasoptimizingfortheUtilitarianobjec- OnlyandDLM-ExtendedPrompt,wefindthataddingthead-
tive. ditional objective to the prompt does not result in a better
performanceonboththesyntheticandreal-worlddomains.
D.2 EffectofSocialChoiceFunction Incontrast,SCLMchoosesrewardfunctionsresultinginsig-
nificantlyhigherutilityincreasesfortheprioritizedsubpop-
ulations and significantly higher utility (measured through
NormalizedUtilityScore)andlowerdropinutilityascom-
pared to default reward function. The fact that SCLM per-
formsadvantageouslyforboth(conflicting)objectiveshigh-
lightsthequalityofthepipelineanditscapabilitiestoeffec-
tivelyaddressmultipleobjectives(ofdifferenttypes).
E PromptTexts
In Figures 6-15, we show the prompts passed to LLM for
various experiments described in the paper. Specifically,
promptinFigures6,7showhowtogeneratearewardfunc-
tionincodeformbasedontheproblemdescription,thein-
Figure5:Comparisonofdifferentmethodsusingtheprompt
dicesoffeatures,andpreferencegoalsforsyntheticandreal-
“PrioritizeagentswithalowvalueforfeatureB andagents
worldproblemdomains,respectively.Figures8,9showhow
withalowvalueforfeatureC”.
theLLMispromptedtoselectthebestrewardfunctionfrom
those generated in the previous step for synthetic and real-
While both the Utilitarian and Egalitarian social welfare worlddomains,respectively.Together,thesepromptsestab-
function leads to good overall results, the choice between lishtheDLMbaselineinthepaper.LLM-Zeroshotbaseline
them significantly influences which reward function is se- hastheexactsamepromptasFigures6,7forsyntheticand
lectedattheinstancelevel.Figure5showsoneexamplein- realworlddomains.Theonlydifferenceisthatthereisnore-
stance for a prompt with two prioritization clauses. As in flectionstep,andthefirstrewardgeneratedbyLLMischo-
13Table3:ResultssummarycomparingdifferentrewardfunctionchoicestrategiesaggregatedacrossthethreesyntheticDatasets.
Cellsinboldindicatethetop2bestvalues(higherisbetter).DLM:Decision-LanguageModel,SCLM:SocialChoiceLanguage
Model,SCLM-SIM:SimulationbasedScorerModel,SCLM-LLM:LLMbasedScorerModel
SocialWelfareFunction Minimumof%changeinutility Minimumof%changeinutility
Method top/bottom1bucket top/bottom2buckets top/bottom3buckets top/bottom1bucket top/bottom2buckets top/bottom3buckets
LLM-zeroshot -0.266%±1.51% -0.228%±1.3% -3.269%±1.14% -15.16%±0.81% -13.249%±0.71% -12.485%±0.61%
DLM 3.879%±1.42% 5.379%±1.27% 1.89%±0.99% -7.844%±0.74% -6.249%±0.62% -6.761%±0.49%
DLM-PromptEngg 7.607%±1.53% 8.665%±1.44% 4.087%±1.24% -9.301%±0.82% -7.17%±0.81% -7.767%±0.71%
SCLM-SIM 28.944%±2.12% 21.936%±1.55% 13.654%±1.07% -2.278%±1.02% -3.579%±0.83% -3.559%±0.55%
Utilitarian
SCLM-LLM 14.348%±1.66% 10.304%±1.22% 6.333%±0.99% -3.973%±0.76% -4.428%±0.6% -4.817%±0.54%
SCLM-SIM 16.448%±1.95% 11.425%±1.34% 8.6%±0.94% -1.176%±1.01% -2.028%±0.71% -1.833%±0.49%
Egalitarian
SCLM-LLM 11.421%±1.62% 7.845%±1.21% 4.141%±0.92% -4.877%±0.73% -4.373%±0.59% -4.68%±0.5%
SCLM-SIM 28.262%±2.42% 20.416%±1.73% 11.102%±1.19% -4.053%±1.18% -5.408%±0.89% -5.261%±0.62%
Nash
SCLM-LLM 9.478%±1.68% 6.608%±1.23% 2.957%±1.02% -5.973%±0.77% -5.782%±0.6% -6.117%±0.54%
Table 4: Results summary comparing different reward function choice strategies aggregated across the three Real World
Datasets.Cellsinboldindicatethetop2bestvalues(higherisbetter).DLM:Decision-LanguageModel,SCLM:SocialChoice
LanguageModel,SCLM-SIM:SimulationbasedScorerModel,SCLM-LLM:LLMbasedScorerModel
SocialWelfareFunction Minimumof%changeinutility Minimumof%changeinutility
Method top/bottom1bucket top/bottom2buckets top/bottom3buckets top/bottom1bucket top/bottom2buckets top/bottom3buckets
LLM-zeroshot -0.893%±1.38% -5.541%±0.79% -10.436%±0.46% -7.285%±0.69% -7.003%±0.45% -6.971%±0.26%
DLM 6.219%±1.59% -0.485%±0.95% -8.733%±0.43% -5.317%±0.84% -6.454%±0.39% -7.674%±0.19%
DLM-PromptEngg 4.341%±1.65% -3.57%±0.91% -9.417%±0.57% -4.957%±0.94% -7.366%±0.5% -7.586%±0.37%
SCLM-SIM 21.502%±1.76% 10.643%±1.06% -3.434%±0.53% 2.206%±0.89% -1.203%±0.45% -4.423%±0.29%
Utilitarian
SCLM-LLM 8.711%±1.3% -0.574%±1.08% -8.285%±0.82% -6.512%±0.63% -6.457%±0.55% -6.756%±0.43%
SCLM-SIM 11.481%±1.61% 7.92%±1.01% -0.416%±0.37% 2.109%±0.75% 0.287%±0.4% -1.974%±0.22%
Egalitarian
SCLM-LLM 8.239%±1.67% -1.919%±0.98% -9.488%±0.48% -1.561%±0.8% -4.67%±0.39% -7.059%±0.2%
SCLM-SIM 20.579%±1.8% 10.195%±1.1% -4.376%±0.54% 2.09%±0.9% -1.568%±0.47% -4.886%±0.29%
Nash
SCLM-LLM -0.732%±1.25% -4.854%±0.95% -10.455%±0.68% -9.877%±0.65% -8.373%±0.53% -8.107%±0.36%
Table5:Resultscomparingdifferentrewardfunctionchoiceselectionstrategiesfordecidingtradeoffbetweenutilitymaximiza-
tionandpreferencealignment,aggregatedacrossthethreesyntheticdatasets(left)andthreerealworlddatasets(right).Social
ChoiceLanguageModelhasthehighestpercentchangeinrewardforthedesiredattributewhilealsoensuringthelowdropin
utilityascomparedtodefault.
Synthetic RealWorld
%ChangeinDesiredAttribute UtilityMetrics %ChangeinDesiredAttribute UtilityMetrics
Method top/bottom1bucket top/bottom2buckets NormalizedUtilityScore %dropinUtility top/bottom1bucket top/bottom2buckets NormalizedUtilityScore %dropinUtility
DLM-PrioritizationOnly -0.812±0.85 0.072±0.83 0.507±0.01 -4.867±0.21 5.678±0.67 1.177±0.43 0.327±0.01 -6.051±0.14
DLM-ExtendedPrompt -2.508±1.2 -1.089±1.26 0.548±0.02 -3.825±0.29 3.416±1.04 0.461±0.64 0.347±0.02 -5.689±0.25
SCLM 12.389±1.01 10.276±0.8 0.71±0.01 -1.141±0.16 9.058±0.61 4.868±0.4 0.532±0.01 -3.662±0.16
14senasthebestreward.
InFigure10and11,weshowhowthepromptisenhanced
with additional information to assist in selecting a reward
function.Theadditionalinformationishighlightedinbold.
Section6.4explainshowSCLMisusedtospecifyadditional
objectives, such as minimizing utility shifts in features not
included in the preference prompt and maximizing overall
utility.InFigures12,13weshowthepromptsfortheDLM-
EPbaselines,whichexplicitlyincludeinstructionstomini-
mizeutilityshiftsinunintendedfeatures.InFigures14,15,
weshowthepromptsthatexplicitlyincludeinstructionsfor
maximizingoverallutility.
15Generator:SyntheticDomain
Prompt
CreateaPythonrewardfunctionforRLinresourceallocationproblemforagents,withtheobjectiveofprioritizinghigher
states and: Focus on the agents with low value of feature A.. The function should use ’state’ (value is either 0,1) and
features ’agent feats’ (length 15 array) to direct the RL agent. Here is a description of the features you may use along
withtheindexin’agent feats’array:
IndexNameDataType
0.FeatureAbucket1-Binary
1.FeatureAbucket2-Binary
2.FeatureAbucket3-Binary
3.FeatureAbucket4-Binary
4.FeatureAbucket5-Binary
5.FeatureBbucket1-Binary
6.FeatureBbucket2-Binary
7.FeatureBbucket3-Binary
8.FeatureBbucket4-Binary
9.FeatureBbucket5-Binary
10.FeatureCbucket1-Binary
11.FeatureCbucket2-Binary
12.FeatureCbucket3-Binary
13.FeatureCbucket4-Binary
14.FeatureCbucket5-Binary
All buckets are in increasing order of the feature values. For example, ’Feature A bucket 1’ would consist of bucket of
lowestvaluesoffeatureAwhile’FeatureAbucket5’wouldconsistofhighestvalueoffeatureA.ThisistrueforFeature
A,BandC.
Your task: 1. Write a simple, single-line Python reward function. Exclude the word ’return’ and exclude non-standard
libraries. Format your code with triple $ signs: $$$[YOUR FUNCTION]$$$. Note that HIGHER states are always
preferred,soensurerewardincreasesasstateincreases.Makesurerewardisalwayspositiveandincreasingwithstate.
ExamplePrompt:PrioritizeagentsthathavelowfeatureAandhighfeatureC
Example Response: Python Code: ’$$$ state+state * ((agent feats[0] or agent feats[1]) and (agent feats[17] or
agent feats[18]oragent feats[19]))$$$’or’$$$state*(agent feats[0]or3*agent feats[19])$$$’or’$$$state+2*state
*((5*agent feats[0]+agent feats[1])andagent feats[19])$$$’Intheseexample,agent feats[0]andagent feats[1]repre-
sentagentswithlowvaluesforfeatureAandagent feats[17],agent feats[18],agent feats[19]representagentswithhigh
valuesforfeatureC
It is upto you to decide which features will represent a preference. For example low values could be the lowest feature
bucket,orlowerthreefeaturebucketsorsoon.Comeupwithauniquenewrewardforthespecifiedgoal:Focusonthe
agentswithlowvalueoffeatureA..Goal:FocusontheagentswithlowvalueoffeatureA.
Output
$$$2*state+state*(1*agent feats[0]+0.5*agent feats[1])$$$
Figure6:PromptpassedtotheLLMtogeneraterewardfunctionbasedontheproblemscenariosintheSyntheticDomain.
16Generator:RealWorldDomain
Prompt
Create a Python reward function for RL in phone call resource allocation to mothers in India, with the objective of
prioritizing higher states and: Focus on the young mothers by age. The function should use ’state’ (value is either 0,1)
andfeaturesagentfeats(length43array)todirecttheRLagent.Hereisadescriptionofthefeaturesyoumayusealong
withtheindexinagentfeatsarray:
IndexNameDataType
0.Ages10-20-Binary
1.Ages21-30-Binary
2.Ages31-40-Binary
3.Ages41-50-Binary
4.Ages51-60-Binary
5.Educationlevel1/7–illiterate-Binary
6.Educationlevel2/7–1-5thGradeCompleted-Binary
7.Educationlevel3/7–6-9thGradeCompleted-Binary
8.Educationlevel4/7–10thGradePassed-Binary
9.Educationlevel5/7–12thGradePassed-Binary
10.Educationlevel6/7–Graduate-Binary
11.Educationlevel7/7–Postgraduate-Binary
12.Incomebracket1(e.g.,0-5000)-Binary
13.Incomebracket2(e.g.,5001-10000)-Binary
14.Incomebracket3(e.g.,10001-15000)-Binary
15.Incomebracket4(e.g.,15001-20000)-Binary
16.Incomebracket5(e.g.,20001-25000)-Binary
17.Incomebracket6(e.g.,25001-30000)-Binary
18.Incomebracket7(e.g.,30000-999999)-Binary
Yourtask:
Writeasimple,single-linePythonrewardfunction.Excludetheword’return’andexcludenon-standardlibraries.Format
your code with triple $ signs: $$$[YOUR FUNCTION]$$$. Note that HIGHER states are always preferred, so ensure
reward increases as state increases. Make sure reward is always positive and increasing with state. Example Prompt:
Prioritizeagentsthatareolderandrich
ExampleResponse:
PythonCode:’$$$state*(agent feats[4]andagent feats[18])$$$’
Comeupwithauniquenewrewardforthespecifiedgoal:Focusontheyoungmothersbyage.Goal:Focusontheyoung
mothersbyage
Output
$$$state*agent feats[7]$$$
Figure7:PromptpassedtotheLLMtogeneraterewardfunctionbasedontheproblemscenariosintheRealWorldDomain.
17DLMChoice:SyntheticDomain
Prompt
MygoalwastocreateaPythonrewardfunctionforRLinresourceallocation,withtheobjectiveof:Focusontheagents
withlowvalueoffeatureA.Itriedseveralrewardfunctionsforthistask.
BelowaretherewardfunctionsIusedandtheircorrespondingrewarddistributions:
FunctionNumber0: RewardFunction:2*state+state*(1*agent feats[0]+0.5*agent feats[1])
Reflection:’
Category:AFeatureAbucket1:113.20
FeatureAbucket2:137.07
FeatureAbucket3:56.51
FeatureAbucket4:56.82
FeatureAbucket5:54.60
Category:BFeatureBbucket1:82.89
FeatureBbucket2:65.36
FeatureBbucket3:60.33
FeatureBbucket4:50.13
FeatureBbucket5:46.00
Category:CFeatureCBucket1:46.78
FeatureCBucket2:49.96
FeatureCBucket3:46.64
FeatureCBucket4:66.58
FeatureCBucket5:62.24
FunctionNumber1: RewardFunction:state*(agent feats[0]andnot(agent feats[1]oragent feats[2]))
Reflection:’Category:AFeatureAbucket1:177.84
FeatureAbucket2:54.76
FeatureAbucket3:55.93
FeatureAbucket4:57.00
FeatureAbucket5:55.29
Category:BFeatureBbucket1:64.47
FeatureBbucket2:58.76
FeatureBbucket3:56.96
FeatureBbucket4:51.11
FeatureBbucket5:50.13
Category:CFeatureCBucket1:49.80
FeatureCBucket2:51.69
FeatureCBucket3:48.40
FeatureCBucket4:65.89
FeatureCBucket5:60.00
Based on the above reward distributions and the given goal: Focus on those with high education., please identify the
FUNCTIONNUMBERofthemosteffectiverewardfunction.ProvideyouranswerEXACTLYINthefollowingformat:’The
bestrewardfunctionisatnumber:[FUNCTIONNUMBER]’.
Output:
Thebestrewardfunctionisatnumber:1
Figure 8: Prompt passed to the LLM to choose a reward function based on the context of problem scenario in the Synthetic
Domain,thegeneratedrewardfunctionsandtherewarddistributioncorrespondingtoeveryrewardfunction.
18DLMChoice:RealWorldDomain
Prompt
MygoalwastocreateaPythonrewardfunctionforRLinresourceallocation,withtheobjectiveof:Focusonthosewith
higheducation.Itriedseveralrewardfunctionsforthistask.Below,Ihavethegivenrewardfunction,andthecorrespond-
ingdistributionofrewardachievedacross20agentfeatures.
BelowaretherewardfunctionsIusedandtheircorrespondingrewarddistributions:
Function Number 0: Reward Function: -agent feats[5] -agent feats[6]-agent feats[7]-agent feats[8]-agent feats[9]-
agent feats[10]-agent feats[11]
Reflection:’
Category:AgeAges10-20:121.73
Ages21-30:421.04
Ages31-40:244.49
Ages41-50:64.11
Ages51-60:10.58
Category:IncomeIncomebracket1(e.g.,0-5000):126.82
Incomebracket2(e.g.,5001-10000):373.62
Incomebracket3(e.g.,10001-15000):234.87
Incomebracket4(e.g.,15001-20000):77.40
Incomebracket5(e.g.,20001-25000):35.58
Incomebracket6(e.g.,25001-30000):2.58
Incomebracket7(e.g.,30000-999999):11.09
Category:EducationIlliterate:39.91
1-5thGradeCompleted:157.84
6-9thGradeCompleted:281.36
10thGradePassed:197.64
12thGradePassed:103.18
Graduate:21.13
Postgraduate:60.89
FunctionNumber1: RewardFunction:state*agent feats[10]
Reflection:’Category:AgeAges10-20:134.22
Ages21-30:469.16
Ages31-40:270.44
Ages41-50:72.80
Ages51-60:11.96
Category:IncomeIncomebracket1(e.g.,0-5000):138.40
Incomebracket2(e.g.,5001-10000):414.44
Incomebracket3(e.g.,10001-15000):266.44
Incomebracket4(e.g.,15001-20000):85.33
Incomebracket5(e.g.,20001-25000):40.20
Incomebracket6(e.g.,25001-30000):2.80
Incomebracket7(e.g.,30000-999999):10.96
Category:EducationIlliterate:45.07
1-5thGradeCompleted:173.82
6-9thGradeCompleted:314.07
10thGradePassed:217.31
12thGradePassed:113.02
Graduate:29.36
Postgraduate:65.93
Based on the above reward distributions and the given goal: Focus on those with high education., please identify the
FUNCTIONNUMBERofthemosteffectiverewardfunction.ProvideyouranswerEXACTLYINthefollowingformat:’The
bestrewardfunctionisatnumber:[FUNCTIONNUMBER]’.
Output:
Thebestrewardfunctionisatnumber:1
Figure 9: Prompt passed to the LLM to choose a reward function based on the context of problem scenario in Real World
Domain,thegeneratedrewardfunctionsandtherewarddistributioncorrespondingtoeveryrewardfunction.
19DLMChoicewithPromptEngineering(DLM-PromptEngg):RealWorldDomain
Prompt
MygoalwastocreateaPythonrewardfunctionforRLinresourceallocation,withtheobjectiveof:Focusonthosewith
higheducation.Itriedseveralrewardfunctionsforthistask.Below,Ihavethegivenrewardfunction,andthecorrespond-
ingdistributionofrewardachievedacross20agentfeatures.
BelowaretherewardfunctionsIusedandtheircorrespondingrewarddistributions:
Function Number 0: Reward Function: -agent feats[5] -agent feats[6]-agent feats[7]-agent feats[8]-agent feats[9]-
agent feats[10]-agent feats[11]
Reflection:’
Category:AgeAges10-20:121.73
Ages21-30:421.04
Ages31-40:244.49
Ages41-50:64.11
Ages51-60:10.58
Category:IncomeIncomebracket1(e.g.,0-5000):126.82
Incomebracket2(e.g.,5001-10000):373.62
Incomebracket3(e.g.,10001-15000):234.87
Incomebracket4(e.g.,15001-20000):77.40
Incomebracket5(e.g.,20001-25000):35.58
Incomebracket6(e.g.,25001-30000):2.58
Incomebracket7(e.g.,30000-999999):11.09
Category:EducationIlliterate:39.91
1-5thGradeCompleted:157.84
6-9thGradeCompleted:281.36
10thGradePassed:197.64
12thGradePassed:103.18
Graduate:21.13
Postgraduate:60.89
FunctionNumber1: RewardFunction:state*agent feats[10]
Reflection:’Category:AgeAges10-20:134.22
Ages21-30:469.16
Ages31-40:270.44
Ages41-50:72.80
Ages51-60:11.96
Category:IncomeIncomebracket1(e.g.,0-5000):138.40
Incomebracket2(e.g.,5001-10000):414.44
Incomebracket3(e.g.,10001-15000):266.44
Incomebracket4(e.g.,15001-20000):85.33
Incomebracket5(e.g.,20001-25000):40.20
Incomebracket6(e.g.,25001-30000):2.80
Incomebracket7(e.g.,30000-999999):10.96
Category:EducationIlliterate:45.07
1-5thGradeCompleted:173.82
6-9thGradeCompleted:314.07
10thGradePassed:217.31
12thGradePassed:113.02
Graduate:29.36
Postgraduate:65.93
Figure10:EnhancedPromptpassedtotheLLMtochoosearewardfunctionbasedonthecontextoftheproblemscenarioin
theRealWorldDomain,thegeneratedrewardfunctions,therewarddistributionscorrespondingtoeveryrewardfunctionand
additionalexamplesonwhattolookatwhenchoosingarewardfunctionalignedwiththepreference.
20Basedontheaboverewarddistributionsandthegivengoal:Focusonthethosewithloweducation.,pleaseidentifytheFUNCTIONNUMBER
of the most effective reward function. You can look at the reward distributions for different features and based on them, judge the
effectivenessofthecorrepondingrewardfunction.Forinstance,ifthequerywantstoprioritizelowincomeagents,youshouldlookif
therewardsareindeedhighforlowincomefeatures.itisuptoyoutodecidewhichfeaturesdescribelowincomepreference.Provide
youranswerEXACTLYINthefollowingformat:’Thebestrewardfunctionisatnumber:[FUNCTIONNUMBER]’..
Output:
Thebestrewardfunctionisatnumber:1
Figure11:EnhancedPromptpassedtotheLLMtochoosearewardfunctionbasedonthecontextoftheproblemscenarioin
theRealWorldDomain,thegeneratedrewardfunctions,therewarddistributionscorrespondingtoeveryrewardfunctionand
additionalexamplesonwhattolookatwhenchoosingarewardfunctionalignedwiththepreference.
21DLMChoicewithExtendedPromptforMinimizingUtilityShifts(DLM-EP):RealWorldDomain
Prompt
MygoalwastocreateaPythonrewardfunctionforRLinresourceallocation,withtheobjectiveof:Focusontheyoung
mothers by age and also focus on those with low education. I tried several reward functions for this task. Below, I
havethegivenrewardfunction,andthecorrespondingdistributionofrewardachievedacross20agentfeatures.
BelowaretherewardfunctionsIusedandtheircorrespondingrewarddistributions:
FunctionNumber0: RewardFunction:state*(agent feats[0]oragent feats[1])and(agent feats[5]oragent feats[6])
Reflection:’
Category:AgeAges10-20:163.24
Ages21-30:547.98
Ages31-40:269.78
Ages41-50:72.11
Ages51-60:10.91
Category:IncomeIncomebracket1(e.g.,0-5000):154.40
Incomebracket2(e.g.,5001-10000):472.98
Incomebracket3(e.g.,10001-15000):293.53
Incomebracket4(e.g.,15001-20000):89.82
Incomebracket5(e.g.,20001-25000):40.84
Incomebracket6(e.g.,25001-30000):2.91
Incomebracket7(e.g.,30000-999999):9.53
Category:EducationIlliterate:66.47
1-5thGradeCompleted:257.87
6-9thGradeCompleted:312.69
10thGradePassed:224.22
12thGradePassed:113.53
Graduate:23.42
Postgraduate:65.82
FunctionNumber1: RewardFunction:state*(agent feats[0]oragent feats[1])*(agent feats[5]oragent feats[6])
Reflection:’Category:AgeAges10-20:163.24
Ages21-30:547.98
Ages31-40:269.78
Ages41-50:72.11
Ages51-60:10.91
Category:IncomeIncomebracket1(e.g.,0-5000):154.40
Incomebracket2(e.g.,5001-10000):472.98
Incomebracket3(e.g.,10001-15000):293.53
Incomebracket4(e.g.,15001-20000):89.82
Incomebracket5(e.g.,20001-25000):40.84
Incomebracket6(e.g.,25001-30000):2.91
Incomebracket7(e.g.,30000-999999):9.53
Category:EducationIlliterate:66.47
1-5thGradeCompleted:257.87
6-9thGradeCompleted:312.69
10thGradePassed:224.22
12thGradePassed:113.53
Graduate:23.42
Postgraduate:65.82
Additional Information - Rewards from Default reward function (Reward distribution from Default reward function.
Truncatedforbrevity.)
Figure12:EnhancedPromptpassedtotheLLMtochoosearewardfunctionbasedonthecontextoftheproblemscenarioin
theRealWorldDomain,thegeneratedrewardfunctions,therewarddistributionscorrespondingtoeveryrewardfunctionand
additionalinformationtominimizetheunintendedutilityshiftsindimensionsnotspecifiedinthepreference.
22Based on the above reward distributions and the given goal: Focus on the young mothers by age and also focus on those with low
education,pleaseidentifytheFUNCTIONNUMBERofthemosteffectiverewardfunction.Alsomakesurethatthatyouchooseareward
functionthatdoesnotcauseunintendedshiftsinreward.Unintendedshiftsinrewardheremeansthatthechosenrewardfunction
shouldn’tdrasticallychangethedistributioninrewardwithrespecttofeaturesnotspecifiedinthepromptForexample,iftheprompt
istopreferagentswithloweducation,thenthechosenrewardfunctionshouldn’tchangethedistributioninrewardw.r.tthedefault
reward distribution too much in the income feature buckets . Provide your answer EXACTLY IN the following format: ’The best reward
functionisatnumber:[FUNCTIONNUMBER]’.
Output:
Thebestrewardfunctionisatnumber:1
Figure13:Continued:EnhancedPromptpassedtotheLLMtochoosearewardfunctionbasedonthecontextoftheproblem
scenario in the Real World Domain, the generated reward functions, the reward distributions corresponding to every reward
functionandadditionalinformationtominimizetheunintendedutilityshiftsindimensionsnotspecifiedinthepreference.
23DLMChoicewithExtendedPromptforMaximizingOverallUtility(DLM-EP):RealWorldDomain
Prompt
MygoalwastocreateaPythonrewardfunctionforRLinresourceallocation,withtheobjectiveof:Focusontheyoung
mothers by age and also focus on those with low education. I tried several reward functions for this task. Below, I
havethegivenrewardfunction,andthecorrespondingdistributionofrewardachievedacross20agentfeatures.
BelowaretherewardfunctionsIusedandtheircorrespondingrewarddistributions:
FunctionNumber0: RewardFunction:state*(agent feats[0]oragent feats[1])and(agent feats[5]oragent feats[6])
Reflection:’
Category:AgeAges10-20:163.24
Ages21-30:547.98
Ages31-40:269.78
Ages41-50:72.11
Ages51-60:10.91
Category:IncomeIncomebracket1(e.g.,0-5000):154.40
Incomebracket2(e.g.,5001-10000):472.98
Incomebracket3(e.g.,10001-15000):293.53
Incomebracket4(e.g.,15001-20000):89.82
Incomebracket5(e.g.,20001-25000):40.84
Incomebracket6(e.g.,25001-30000):2.91
Incomebracket7(e.g.,30000-999999):9.53
Category:EducationIlliterate:66.47
1-5thGradeCompleted:257.87
6-9thGradeCompleted:312.69
10thGradePassed:224.22
12thGradePassed:113.53
Graduate:23.42
Postgraduate:65.82
FunctionNumber1: RewardFunction:state*(agent feats[0]oragent feats[1])*(agent feats[5]oragent feats[6])
Reflection:’Category:AgeAges10-20:163.24
Ages21-30:547.98
Ages31-40:269.78
Ages41-50:72.11
Ages51-60:10.91
Category:IncomeIncomebracket1(e.g.,0-5000):154.40
Incomebracket2(e.g.,5001-10000):472.98
Incomebracket3(e.g.,10001-15000):293.53
Incomebracket4(e.g.,15001-20000):89.82
Incomebracket5(e.g.,20001-25000):40.84
Incomebracket6(e.g.,25001-30000):2.91
Incomebracket7(e.g.,30000-999999):9.53
Category:EducationIlliterate:66.47
1-5thGradeCompleted:257.87
6-9thGradeCompleted:312.69
10thGradePassed:224.22
12thGradePassed:113.53
Graduate:23.42
Postgraduate:65.82
Figure14:EnhancedPromptpassedtotheLLMtochoosearewardfunctionbasedonthecontextoftheproblemscenarioin
theRealWorldDomain,thegeneratedrewardfunctions,therewarddistributionscorrespondingtoeveryrewardfunctionand
additionalinformationtomaximizetheoverallutility.
24Based on the above reward distributions and the given goal: Focus on the young mothers by age and also focus on those with low
education,pleaseidentifytheFUNCTIONNUMBERofthemosteffectiverewardfunction.Alsomakesurethatthatyouchooseareward
function which also maximizes the total reward. You can calculate this by adding up rewards in each feature bucket.. Provide your
answerEXACTLYINthefollowingformat:’Thebestrewardfunctionisatnumber:[FUNCTIONNUMBER]’.
Output:
Thebestrewardfunctionisatnumber:1
Figure15:Continued:EnhancedPromptpassedtotheLLMtochoosearewardfunctionbasedonthecontextoftheproblem
scenario in the Real World Domain, the generated reward functions, the reward distributions corresponding to every reward
functionandadditionalinformationtomaximizetheoverallutility.
25