JacNet: Learning Functions with Structured Jacobians
JonathanLorraine*12 SafwanHossain*12
Abstract Theneedtousederivativesofapproximatedfunctionsarises
inmanyscenarios(Vicoletal.,2022). Forexample,ingen-
Neuralnetworksaretrainedtolearnanapproxi-
erativeadversarialnetworks(Goodfellowetal.,2014),the
matemappingfromaninputdomaintoatarget
generatordifferentiatesthroughthediscriminatortoensure
domain. Incorporating prior knowledge about
thelearneddistributionisclosertothetruedistribution. In
truemappingsiscriticaltolearningausefulap-
somemulti-agentlearningalgorithms,anagentdifferenti-
proximation. Withcurrentarchitectures,itischal-
ates through how another agent responds (Foerster et al.,
lengingtoenforcestructureonthederivativesof
2018;Lorraineetal.,2021;2022b). Alternatively,hyperpa-
theinput-outputmapping. Weproposetousea
rameterscandifferentiatethroughaNNtoseehowtomove
neuralnetworktodirectlylearntheJacobianof
tominimizevalidationloss(MacKayetal.,2019;Lorraine
theinput-outputfunction,whichallowseasycon-
&Duvenaud,2018;Lorraineetal.,2020;2022a;Lorraine,
trolofthederivative. Wefocusonstructuringthe
2024;Mehtaetal.,2024;Raghuetal.,2021;Adam&Lor-
derivativetoallowinvertibilityandalsodemon-
raine,2019;Baeetal.,2024;Zhangetal.,2023).
stratethatotherusefulpriors,suchask-Lipschitz,
can be enforced. Using this approach, we can Webeginbygivingbackgroundontheproblemin§2and
learnapproximationstosimplefunctionsthatare discuss related work in § 3. Then, we introduce relevant
guaranteed to be invertible and easily compute theoryforouralgorithmin§4followedbyourexperimental
the inverse. We also show similar results for 1- resultsin§5.
Lipschitzfunctions.
1.1.Contributions
• Weproposeamethodforlearningfunctionsbylearning
1.Introduction
theirJacobian,andprovideempiricalresults.
Neuralnetworks(NNs)arethemainworkhorsesofmodern
• Weshowhowtomakeourlearnedfunctionsatisfyreg-
machinelearning,usedtoapproximatefunctionsinawide
ularityconditions-invertible,orLipschitz-bymaking
rangeofdomains. TwotraitsthatdriveNN’ssuccessare
theJacobiansatisfyregularityconditions
(1) they are sufficiently flexible to approximate arbitrary
functions, and (2) we can easily structure the output to
• WeshowhowthelearnedJacobiancansatisfyregular-
incorporatecertainpriorknowledgeabouttherange(e.g.,
ityconditionsviaappropriateoutputactivations.
softmaxoutputactivationforclassification).
NNflexibilityisformalizedbyshowingtheyareuniversal
2.Background
functionapproximators. Thismeansthatgivenacontinuous
functionyonaboundedintervalI,aNNapproximationˆy
θ This section sets up the standard notation used in § 4.
cansatisfy:∀x∈I,|y(x)−ˆy (x)|<ϵ.Horniketal.(1989)
θ Our goal is to learn a C1 function y(x) : X → Y. De-
show that NNs with one hidden layer and non-constant,
note d ,d as the dimension of X,Y respectively. Here,
boundedactivationfunctionsareuniversalapproximators. x y
we assume that x ∼ p(x) and y is deterministic. We
WhileNNscanachievepoint-wiseapproximationswithar-
will learn the function through a NN - ˆy (x) - param-
bitraryprecision,lesscanbesaidabouttheirderivativesw.r.t. θ
eterized by weights θ ∈ Θ. Also, assume we have a
theinput. Forexample,NNswithstep-functionactivations
bounded loss function L(y(x),ˆy (x)), which attains its
areflatalmosteverywhereandcannotapproximatearbitrary θ
minimum when y(x) = ˆy (x). Our population risk is
inputderivativesyetarestilluniversalapproximators. R(θ) = E [L(y(x),ˆy (x)θ )], andwewishtofindθ∗ =
p(x) θ
argmin R(θ). Inpractice,wehaveafinitenumberofsam-
*Equalcontribution 1UniversityofToronto2VectorInstitute.Cor- θ
plesfromtheinputdistributionD ={(x ,y )|i=1...n},
respondenceto:JonathanLorraine<lorraine@cs.toronto.edu>. i i
andweminimizetheempiricalrisk:
FirstworkshoponInvertibleNeuralNetworksandNormalizing
Flows(ICML2019),LongBeach,CA,USA
θˆ∗ =argminRˆ(θ)=argmin1/n(cid:88)
L(y i,ˆy θ(x i,θ))
θ θ
D
4202
guA
32
]GL.sc[
1v73231.8042:viXraJacNet:LearningFunctionswithStructuredJacobians
Itiscommontohavepriorknowledgeaboutthestructure verse. OtherreversiblearchitecturesincludeNICE(Dinh
ofy(x)whichwewanttobakeintothelearnedfunction. If etal.,2014),Real-NVP(Dinhetal.,2016),RevNet(Jacob-
weknowtheboundsoftheoutputdomain,properlystruc- sen et al., 2018), and Glow (Kingma & Dhariwal, 2018).
turingthepredictionsthroughoutputactivationisaneasy Richter-Powelletal.(2021)developcontinuationsonour
waytoenforcethis. Examplesincludeusingsoftmaxfor workshopmethod(Lorraine&Hossain,2019).
classification,orReLUforanon-negativeoutput.
Inthemoregeneralcase,wemaywanttoensureourlearned 4.Theory
functionsatisfiescertainderivativeconditions,asmanyfunc-
Wearemotivatedbytheideathatafunctioncanbelearned
tionclassescanbeexpressedinsuchaway. Forexample,a
bycombininginitialconditionswithanapproximateJaco-
functionislocallyinvertibleifitsJacobianhasanon-zero
bian. ConsideradeterministicC1functiony:X →Y that
determinantinthatneighborhood. Similarly,afunctionis
k-Lipschitzifitsderivativenormliesinside[−k,k].
wewishtolearn. LetJ xy :X →Rdx×dy betheJacobianof
yw.r.t. x. Wecanevaluatethetargetfunctionbyevaluat-
WeproposeexplicitlylearningthisJacobianthroughaNN ingthefollowinglineintegralwithsomeinitialcondition
J (x)parameterizedbyθandusinganumericalintegrator (x ,y =y(x )):
θ o o o
toevaluateˆy (x). Weshowthatwithasuitablechoiceof (cid:90)
θ
outputactivationforJ (x),wecanguaranteeourfunction y(x)=y + Jy(x)ds
θ o x
isgloballyinvertible,ork-Lipschitz. c(xo,x)
In practice, this integral is evaluated by parameterizing a
pathbetweenx andxandnumericallyapproximatingthe
3.RelatedWork o
integral. Notethatthechoiceofpathandinitialcondition
donotaffecttheresultbythefundamentaltheoremofline
Enforcingderivativeconditions:Thereisexistingworkon
integrals. We can write this as an explicit line integral
strictlyenforcingderivativeconditionsonlearnedfunctions.
for some path c(t,x ,x) from x to x parameterized by
Forexample,ifweknowthefunctiontobek-Lipschitz,one o o
t ∈ [0,1] satisfying c(0,x ,x) = x and c(1,x ,x) = x
methodisweightclipping(Arjovskyetal.,2017). Aniletal. o o o
(2018)recentlyproposedanarchitecturewhichlearnsfunc-
withd/dt(c(t,x o,x))=c′(t,x o,x):
tionsthatareguaranteedtobe1-Lipschitzandtheoretically (cid:90) t=1
y(x)=y + Jy(c(t,x ,x))c′(t,x ,x)dt
capableoflearningallsuchfunctions.Amosetal.(2017)ex- o x o o
t=0
plorelearningscalarfunctionsthatareguaranteedtobecon-
A simple choice of path is c(t,x ,x) = (1−t)x +tx,
o o
vex(i.e.,theHessianispositivesemi-definite)intheirinputs.
whichhasc′(t,x ,x)=x−x . Thus,toapproximatey(x),
o o
Whilethesemethodsguaranteederivativeconditions,they wecancombineinitialconditionswithanapproximateJy.
x
canbenon-trivialingeneralizingtonewconditions,limiting
Weproposetolearnanapproximate,JacobianJ (x):X →
θ
expressiveness, orinvolvingexpensiveprojections. Czar- Rdx×dy,withaNNparameterizedbyθ ∈ Θ. Fortraining,
neckietal.(2017)proposeatrainingregimethatpenalizes
considerthefollowingpredictionfunction:
thefunctionwhenitviolateshigher-orderconstraints. This
(cid:90) t=1
does not guarantee the regularity conditions and requires ˆy (x)=y + J (c(t,x ,x))c′(t,x ,x)dt
θ o θ o o
knowingtheexactderivativeateachsample-however,itis t=0
easytouse. Wecancomputetheempiricalrisk,Rˆ,withthisprediction
functionbychoosingsomeinitialconditions(x ,y )∈D,
Differentiating through integration: Training our pro- o o
a path c, and using numerical integration. To backpropa-
posedmodelrequiresback-propagatingthroughanumerical
gateerrorstoupdateournetworkparametersθ, wemust
integrationprocedure. Weleveragedifferentiablenumerical
backpropagatethroughthenumericalintegration.
integrators provided by Chen et al. (2018) who use it to
modelthedynamicsofdifferentlayersofanNNasordinary
differential equations. FFOJRD (Grathwohl et al., 2018) 4.1.DerivativeConditionsforInvertibility
usesthisfortrainingasitmodelslayersofareversiblenet-
Theinversefunctiontheorem(Spivak,1965)statesthata
workasanODE.Ourapproachdiffersinthatweexplicitly
functionislocallyinvertibleiftheJacobianatthatpointis
learntheJacobianofourinput-outputmappingandintegrate
invertible. Additionally, wecancomputetheJacobianof
alongarbitrarypathsintheinputoroutputdomain. f−1 bycomputingtheinverseoftheJacobianoff. Many
Invertible Networks: In Behrmann et al. (2018), an in- non-invertiblefunctionsarelocallyinvertiblealmostevery-
vertibleresidualnetworkislearnedwithcontractivelayers, where(e.g.,y =x2).
andanumericalfixedpointprocedureisusedtoevaluate
TheHadamardglobalinversefunctiontheorem(Hadamard,
theinverse. Incontrast, weneedanon-zeroJacobiande-
1906), is an example of a global invertibility criterion. It
terminantandusenumericalintegrationtoevaluatethein-
statesthatafunctionf : X → Y isgloballyinvertibleifJacNet:LearningFunctionswithStructuredJacobians
theJacobiandeterminantiseverywherenon-zeroandf is evaluatedlossislowerthanourtolerance. Inpractice,this
proper. A function is proper if whenever Y is compact, provides significant computational savings. Additionally,
f−1(Y)iscompact. Thisprovidesanapproachtoguarantee supposethecomputationalbottleneckisnumericalintegra-
globalinvertibilityofalearnedfunction. tion. Inthatcase,wemaybeabletoadaptivelyselectinitial
conditions (x ,y ) that are near our target, reducing the
o o
4.2.StructuringtheJacobian numberofevaluationstepsinourintegrator.
Byguaranteeingithasnon-zeroeigenvalues,wecouldguar-
4.5.Conservation
anteethatourJacobianforanRn →Rnmappingisinvert-
ible. Forexample,withasmallpositiveϵwecouldusean Whenourinputdomaindimensionalityd >1,weruninto
x
outputactivationof: complexities. Forsimplicity, assumed = 1, andweare
y
attemptingtolearnavectorfieldthatisthegradient. Vector
J′(x)=J (x)JT(x)+ϵI
θ θ θ fieldsthatarethegradientofafunctionareknownascon-
Here,J θ(x)J θT(x)isaflexiblePSDmatrix,whileadding servative. OurlearnedfunctionJ θ isavectorfieldbutisnot
ϵI makes it positive definite. A positive definite matrix necessarilyconservative. Assuch,J θ maynotbethegradi-
hasstrictlypositiveeigenvalues,whichimpliesinvertibility. entofanyscalarpotentialfunction,andthevalueofourline
However,thisoutputactivationrestrictsthesetofinvertible integraldependsonthepathchoice. Investigatingpotential
functionswecanlearn,astheJacobiancanonlyhavepos- problems and solutions to these problems is relegated to
itiveeigenvalues. Infuturework,wewishtoexploreless futurework.
restrictiveactivationswhilestillguaranteeinginvertibility.
5.Experiments
Manyotherregularityconditionsonafunctioncanbespeci-
fiedintermsofthederivatives. Forexample,afunctionis
Inourexperiments,weexplorelearninginvertible,andLip-
Lipschitzifthefunction’sderivativesarebounded,which
schitz functions with the following setup: Our input and
canbedonewithak-scaledtanhactivationfunctionasour outputdomainsareX = Y = R. WeselectL(y ,y ) =
1 2
outputactivation. Alternativelywecouldlearnacomplex
∥y −y ∥. Our training set consists of 5 points sampled
1 2
differentiablefunctionbysatisfying∂u/∂a=∂v/∂b,∂u/∂b=
uniformly from [−1,1], while our test set has 100 points
−∂v/∂a,whereu,v areoutputcomponentsanda,barein-
sampled uniformly from [−2,2]. The NN architecture is
put components. We focus on invertibility and Lipschitz
fullyconnectedwithasinglelayerwith64hiddenunits,and
becausetheyarecommoninmachinelearning.
outputactivationonournetworkdependsonthegradient
regularityconditionwewant. WeusedAdam(Kingma&
4.3.ComputingtheInverse Ba,2014)tooptimizeournetworkwithalearningrateof
0.01andallotherparametersatdefaults. Weusefullbatch
Once the Jacobian is learned, it allows easy computation
of f−1 by integrating the inverse Jacobian along a path gradientestimatesfor50iterations.
c(t,y o,y)intheoutputspace,givensomeinitialconditions Toevaluatethefunctionatapoint,weuseaninitialvalue,
(x o,y o =ˆy θ(x o)): parameterizealinearpathbetweentheinitialandterminal
(cid:90) t=1
x(y,θ)=x + (J (c(t,y ,y)))−1c′(t,y ,y)dt point, and use the numerical integrator from Chen et al.
o θ o o (2018). The path is trivially the interval between x and
t=0 o
IfthecomputationalbottleneckisinvertingJ ,wepropose x because d = 1, and our choice of the initial condition
θ x
tolearnamatrixwhichiseasilyinvertible(e.g.,Kronecker isx = 0. Weadaptivelyaltertheintegrator’stolerances,
o
factorsofJ ). startingwithloosetolerancesanddecreasingthembyhalf
θ
when the training loss is less than the tolerance, which
4.4.Backpropagation providessignificantgainsintrainingspeed.
Trainingthemodelrequiresback-propagatingthroughnu- Welearntheexponentialfunctiony(x)=exp(x)forthein-
mericalintegration. Toaddressthis,weconsidertherecent vertiblefunctionexperiment. Weuseanoutputactivationof
work of Chen et al. (2018), which provides tools to effi- J θ′(x)=J θ(x)J θ(x)T +ϵI forϵ=0.0001,whichguaran-
ciently back-propagate across numerical integrators. We teesanon-zeroJacobiandeterminant. Oncetrained,wetake
combine their differentiable integrators on intervals with thelearnedderivativeJ θ′(x)andcomputeitsinverse,which
autogradforourrectificationtermc′. Thisprovidesadif- bytheinversefunctiontheoremgivesthederivativeofthe
ferentiablenumericallineintegrator,whichonlyrequiresa inverse. Wecomputetheinverseofthepredictionfunction
usertospecifyadifferentiablepathandtheJacobian. byintegratingtheinverseofthelearnedderivative. Figure1
qualitativelyexploresthelearnedfunction,andthetopof
Theintegratorsallowausertospecifysolutiontolerance.
Figure3quantitativelyexploresthetrainingprocedure.
We propose annealing the tolerance tighter whenever theJacNet:LearningFunctionswithStructuredJacobians
Inputx Inputx
Figure2. Agraphofthe1-Lipschitztargetfunction|x|,thepre-
dictionfunctionatthetrainednetworkweights,andtheprediction
functionattheinitialnetworkweights. Notehowtheinitialpre-
dictionfunctionisinaccuratewhilethefinalpredictionfunction
matchesthetargetfunctionclosely. Additionally,notehowthe
learnedpredictionfunctionis1-Lipschitzatinitializationandafter
training.Weincludegraphsofanunconstrainedlearnedfunction
whosederivativeisnotboundedby[-1,1].Thisdoesnotgeneral-
izewelltounseendata.
Top: Learninginvertibleexp(x)
Inputy
Figure1. Agraphoftheinvertibletargetfunctionexp(x),thepre-
dictionfunctionatthetrainednetworkweights,andtheprediction
functionattheinitialnetworkweights.Theinitialpredictionfunc-
tionisinaccurate,whilethefinalonematchesthetargetfunction
closely.Furthermore,thelearnedinverseofthepredictionfunction
closelymatchesthetrueinversefunction.Weincludegraphsofan
unconstrainedlearnedfunctionwhoseJacobiancanbezero.Note
thattheunconstrainedfunctionisnotinvertibleeverywhere.
Bottom: LearningLipschitz|x|
FortheLipschitzexperiment,welearntheabsolutevalue
function y(x) = |x|, a canonical 1-Lipschitz example in
Aniletal.(2018). WeuseanoutputactivationonourNN
ofJ′(x) = tanh(J (x)) ∈ [−1,1],whichguaranteesour
θ θ
prediction function is 1-Lipschitz. Figure 2 qualitatively
exploresthelearnedfunction,andthebottomofFigure3
quantitativelyexploresthetrainingprocedure.
6.Conclusion
Wepresentatechniquetoapproximateafunctionbylearn-
ing its Jacobian and integrating it. This method is useful
whenguaranteeingthefunction’sJacobianproperties.Afew Iteration
examplesofthisincludelearninginvertible,Lipschitz,or Figure3. Agraphoftheempiricalriskortraininglossversustrain-
complexdifferentiablefunctions. Small-scaleexperiments ingiteration. Astrainingprogresses,wetightenthetoleranceof
are presented, motivating further exploration to scale up thenumericalintegratortocontinuedecreasingthelossatthecost
theresults. Wehopethatthisworkwillfacilitatedomain ofmorecomputationallyexpensiveiterations.Top:Thetraining
experts’easyincorporationofawidevarietyofJacobian dynamicsforlearningtheinvertibletargetfunction.Bottom:The
regularityconditionsintotheirmodelsinthefuture. trainingdynamicsforlearningtheLipschitztargetfunction.
ˆynoitciderP/ytegraT
xdetrevnI
θ
ˆynoitciderP/ytegraT
)θ(Rˆ
ksiRlaciripmE
)θ(Rˆ
ksiRlaciripmE
θJacNet:LearningFunctionswithStructuredJacobians
References Hadamard,J. Surlestransformationsponctuelles. Bull.Soc.
Math.France,34:71–84,1906.
Adam,G.andLorraine,J. Understandingneuralarchitec-
turesearchtechniques. arXivpreprintarXiv:1904.00438,
Hornik, K., Stinchcombe, M., and White, H. Multilayer
2019.
feedforwardnetworksareuniversalapproximators. Neu-
ralnetworks,2(5):359–366,1989.
Amos, B., Xu, L., and Kolter, J. Z. Input convex neural
networks. InProceedingsofthe34thInternationalCon-
Jacobsen, J.-H., Smeulders, A., and Oyallon, E. i-
ferenceonMachineLearning-Volume70,pp.146–155.
revnet: Deep invertible networks. arXiv preprint
JMLR.org,2017.
arXiv:1802.07088,2018.
Anil,C.,Lucas,J.,andGrosse,R.Sortingoutlipschitzfunc-
tion approximation. arXiv preprint arXiv:1811.05381, Kingma,D.P.andBa,J. Adam: Amethodforstochastic
optimization. arXivpreprintarXiv:1412.6980,2014.
2018.
Arjovsky,M.,Chintala,S.,andBottou,L. Wassersteingan. Kingma, D. P. and Dhariwal, P. Glow: Generative flow
arXivpreprintarXiv:1701.07875,2017. withinvertible1x1convolutions. InAdvancesinNeural
InformationProcessingSystems,pp.10215–10224,2018.
Bae,J.,Lin,W.,Lorraine,J.,andGrosse,R. Trainingdata
attributionviaapproximateunrolleddifferentation. arXiv Lorraine,J. ScalableNestedOptimizationforDeepLearn-
preprintarXiv:2405.12186,2024. ing. PhDthesis,UniversityofToronto(Canada),2024.
Behrmann, J., Duvenaud, D., and Jacobsen, J.-H. Invert-
Lorraine, J. and Duvenaud, D. Stochastic hyperparame-
ibleresidualnetworks. arXivpreprintarXiv:1811.00995,
teroptimizationthroughhypernetworks. arXivpreprint
2018.
arXiv:1802.09419,2018.
Chen,T.Q.,Rubanova,Y.,Bettencourt,J.,andDuvenaud,
Lorraine,J.andHossain,S. Jacnet:Learningfunctionswith
D. K. Neural ordinary differential equations. In Ad-
structuredjacobians. InICMLINNFWorkshop,2019.
vances in Neural Information Processing Systems, pp.
6571–6583,2018.
Lorraine,J.,Vicol,P.,andDuvenaud,D. Optimizingmil-
Czarnecki,W.M.,Osindero,S.,Jaderberg,M.,Swirszcz, lionsofhyperparametersbyimplicitdifferentiation. In
G.,andPascanu,R. Sobolevtrainingforneuralnetworks. International conference on artificial intelligence and
InAdvancesinNeuralInformationProcessingSystems, statistics,pp.1540–1552.PMLR,2020.
pp.4278–4287,2017.
Lorraine,J.,Vicol,P.,Parker-Holder,J.,Kachman,T.,Metz,
Dinh, L., Krueger, D., and Bengio, Y. Nice: Non-linear L.,andFoerster,J. Lyapunovexponentsfordiversityin
independent components estimation. arXiv preprint differentiablegames. arXivpreprintarXiv:2112.14570,
arXiv:1410.8516,2014. 2021.
Dinh,L.,Sohl-Dickstein,J.,andBengio,S. Densityesti-
Lorraine,J.,Anderson,N.,Lee,C.,DeLaroussilhe,Q.,and
mationusingrealnvp. arXivpreprintarXiv:1605.08803,
Hassen,M. Taskselectionforautomlsystemevaluation.
2016.
arXivpreprintarXiv:2208.12754,2022a.
Foerster, J., Chen, R. Y., Al-Shedivat, M., Whiteson, S.,
Lorraine,J.P.,Acuna,D.,Vicol,P.,andDuvenaud,D.Com-
Abbeel, P., and Mordatch, I. Learning with opponent-
plexmomentumforoptimizationingames. InInterna-
learningawareness. InProceedingsofthe17thInterna-
tionalConferenceonArtificialIntelligenceandStatistics,
tionalConferenceonAutonomousAgentsandMultiAgent
pp.7742–7765.PMLR,2022b.
Systems,pp.122–130.InternationalFoundationforAu-
tonomousAgentsandMultiagentSystems,2018.
MacKay, M., Vicol, P., Lorraine, J., Duvenaud, D., and
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Grosse,R. Self-tuningnetworks: Bileveloptimizationof
Warde-Farley,D.,Ozair,S.,Courville,A.,andBengio, hyperparametersusingstructuredbest-responsefunctions.
Y. Generative adversarial nets. In Advances in neural InInternationalConferenceonLearningRepresentations
informationprocessingsystems,pp.2672–2680,2014. (ICLR),2019.
Grathwohl, W., Chen, R. T., Betterncourt, J., Sutskever, Mehta,N.,Lorraine,J.,Masson,S.,Arunachalam,R.,Bhat,
I.,andDuvenaud,D. Ffjord: Free-formcontinuousdy- Z.P.,Lucas,J.,andZachariah,A.G. Improvinghyperpa-
namicsforscalablereversiblegenerativemodels. arXiv rameteroptimizationwithcheckpointedmodelweights.
preprintarXiv:1810.01367,2018. arXivpreprintarXiv:2406.18630,2024.JacNet:LearningFunctionswithStructuredJacobians
Raghu,A.,Lorraine,J.,Kornblith,S.,McDermott,M.,and
Duvenaud,D.K. Meta-learningtoimprovepre-training.
AdvancesinNeuralInformationProcessingSystems,34:
23231–23244,2021.
Richter-Powell,J.,Lorraine,J.,andAmos,B. Inputconvex
gradient networks. arXiv preprint arXiv:2111.12187,
2021.
Spivak,M. Calculusonmanifolds: amodernapproachto
classicaltheoremsofadvancedcalculus.Addison-Wesley
PublishingCompany,1965.
Vicol,P.,Lorraine,J.P.,Pedregosa,F.,Duvenaud,D.,and
Grosse, R. B. On implicit bias in overparameterized
bilevel optimization. In International Conference on
MachineLearning,pp.22234–22259.PMLR,2022.
Zhang,M.R.,Desai,N.,Bae,J.,Lorraine,J.,andBa,J. Us-
inglargelanguagemodelsforhyperparameteroptimiza-
tion. InNeurIPS2023FoundationModelsforDecision
MakingWorkshop,2023.