Double Descent: Understanding Linear Model
Estimation of Nonidentifiable Parameters and a Model
for Overfitting
Ronald Christensen
Department of Mathematics and Statistics
University of New Mexico*
August 26, 2024
Abstract
Weconsiderordinaryleastsquaresestimationandvariationsonleastsquaresestimation
such as penalized (regularized) least squares and spectral shrinkage estimates for problems
with p > n and associated problems with prediction of new observations. After the intro-
duction of Section 1, Section 2 examines a number of commonly used estimators for p>n.
Section 3 introduces prediction with p>n. Section 4 introduces notational changes to facili-
tatediscussionofoverfittingandSection5illustratesthephenomenonofdoubledescent. We
concludewithsomefinalcomments.
*RonaldChristensenisaDistinguishedProfessorintheDepartmentofMathematicsandofStatistics,University
of New Mexico, Albuquerque, NM, 87131. Thanks to Mohammad Hattab for codesigning and programing the
simulation.
1
4202
guA
32
]LM.tats[
1v53231.8042:viXra1 Introduction
Considerastandardlinearmodel
Y =Xβ +e, E(e)=0, Cov(e)=σ2I . (1.1)
n
Here Y is an observable random n vector, X is an observed n×p matrix, β is a p vector of un-
observable fixed parameters, and e is a unobservable random vector of errors. Linear models
are extremely rich and include such nonparametric regression methods as fitting polynomials,
trigonometric functions, splines, wavelets, reproducing kernels, and fake reproducing kernels
(likethehyperbolictangent),cf. Christensen(2019,Chapter1). Theyeasilyincorporategeneral-
izedadditivemodels. Wheneverr(X),therankofX,islessthan p,atleastsomeoftheparameters
in β are unidentifiable, thus making β unidentifiable. Of recent interest are problems in which
p>n,whichensuresthatnonidentifiabilityexistsinβ.
Weconsiderordinaryleastsquaresestimationandvariationsonleastsquaresestimationsuch
aspenalized(regularized)leastsquaresandspectralshrinkageestimatesforproblemswith p>n
and associated problems with prediction of new observations. After the introduction of Sec-
tion1,Section2examinesanumberofcommonlyusedestimatorsfor p>n. Section3introduces
prediction with p>n. Section 4 introduces notational changes to facilitate discussion of overfit-
ting and Section 5 illustrates the phenomenon of double descent. We conclude with some final
comments.
When p≤n, correct regression models should make valid predictions. When overfitting oc-
curs,thesepredictionscanoftenbeimprovedusingcorrectreducedmodelsorbiasedestimation
methods. Letting p→n typically causes overfitting. Because of identifiability issues, there is no
general guarantee that p>n models, even when correct and with little variability σ2, will make
validpredictionsinthesenseofpredictingvaluesclosetothemeanvalueoffutureobservables.
However, there are situations in which good predictions are more likely to occur and, with ran-
domly selected training and test data sets, one can check how well particular procedures are
working. In many cases, notably nonparametric multiple regression, the many different ap-
proachesavailableformodelingE(Y)providedifferentpossibilitiesforfindinganapproachthat
predictswell.
1.1 Ordinary Least Squares
Ordinaryleastsquares(OLS)estimatesminimize
∥Y −Xβ∥2 ≡(Y −Xβ)′(Y −Xβ).
It is well known that OLS estimates are determined as solutions to either of the following equa-
tions
X′Xβ =X′Y or Xβ =MY.
Thefirstofthesearethewellknownnormalequationsandcanbeobtainedbysettingderivatives
equal to 0. The second equation involves M, the unique (Euclidean) perpendicular projection
operator (ppo) onto C(X). C(X) is the column space of X, also known as the range space of X
and the span of the columns of X. M is unique, idempotent (MM =M), symmetric (M =M′), has
2ˆ
C(M)=C(X),andMX =X. Christensen(2020,Chapter2)[somewhatfancifully]referstoβ being
aleastsquaresestimateifandonlyifitsolvesthesecondequationinthepreviousdisplayasthe
Fundamental Theorem of Least Squares Estimation. When r(X)< p, the least squares estimates
ˆ
β arenotuniquelydefinedbutbytheFundamentalTheoremandtheuniquenessofM,thefitted
ˆ
valuesvectorXβ isunique.
1.2 Penalized Least Squares
Penalized(regularized)leastsquaresestimatesminimize,forsomenonnegativepenaltyfunction
P andnonnegativetuningparameterλ,
∥Y −Xβ∥2+λnP(β).
Typically, P(0) = 0. For any least squares estimates βˆ , uniquely define the sum of squares for
error(SSE)as
SSE ≡∥Y −Xβˆ ∥2
sothat
∥Y −Xβ∥2+λnP(β)=SSE+∥Xβˆ −Xβ∥2+λnP(β).
From this second form it is clear that any penalized least squares estimates have to be functions
ˆ ˆ
of the OLS estimate β and indeed of the least squares fitted values Xβ. Typically, incorporating
thepenaltyfunctionisenoughtomakethepenalizedleastsquaresestimatesunique.
1.3 Spectral Shrinkage Estimates
Considerthesingularvaluedecomposition(SVD)
1
X′X =VD(s )V′
j
n
wherein D(s ) is a p×p diagonal matrix of eigenvalues, the jth column of V is an eigenvector
j
of 1X′X corresponding to s , andV is an orthonormal matrix (V′V =I =VV′). Without loss of
n j p
generality,assumes ≥s ≥···≥s . Withr(X)=t,wehaves >0for j=1,...,t andift < p,then
1 2 p j
s =0for j=t+1,...,p. Infact,withV denotingthefirstrcolumnsofV andD [ψ(s )]adiagonal
j r r j
matrix consisting of the function ψ applied to the r largest eigenvalues, another singular value
decompositionis
1
X′X =VD (s )V′.
n t t j t
Here the columns of V comprise an orthonormal basis for C(X′X) =C(X′) so that N ≡VV′ =
t t t
X′(XX′)−X is the ppo ontoC(X′), cf. Christensen (2020, Section B.3). When p>>n, the compu-
tationaladvantagesbecomehugeforperforminganSVDon
1XX′
witheigenvectorsascolumns
n
ofamatrixU andthencomputingV fromU,e.g. Christensen(2020,Section13.3).
IfΨmapsthenonnegativerealsintothemselves,defineaspectralshrinkeras
(cid:18) (cid:19)
1
Ψ X′X ≡VD[Ψ(s)]V′.
i
n
3ThecorrespondingΨshrinkageestimateis
(cid:18) (cid:19)
1 1
βˆ ≡ Ψ X′X X′Y
Ψ
n n
(cid:18) (cid:19)
1 1
= Ψ X′X X′Xβˆ ,
n n
where the normal equations ensure that these are functions of the least squares estimates and
fitted values. We will see later that for spectral shrinkage estimates to actually shrink (a version
of)theleastsquaresestimatesweneedΨ(u)<1/u.
1.4 Identifiability
Christensen(2020,Section2.1)discussesidentifiabilityin(generalized)linearmodels. Itamounts
to the idea that identifiable functions are those that are functions of Xβ. It is convenient to de-
compose β into a component in C(X′) and a component perpendicular to C(X′) via β = Nβ +
(I−N)β. (Vectors u and w are perpendicular/orthogonal, denoted u⊥w, if and only if u′w=0.
C(X′)⊥ isthevectorspacecontainingallvectorsorthogonaltoC(X′).) Clearly,Nβ =X′(XX′)−Xβ
is a function of Xβ, hence identifiable and therefore is something that can be estimated. On the
other hand, whenever t < p so thatC(X′)̸=Rp, neither β nor the nontrivial parameter (I−N)β
is identifiable and cannot be uniquely estimated from data. To see this consider two parame-
ters β and β with β ̸=β but Nβ =Nβ , then Xβ =Xβ , so any functions of Xβ must be the
1 2 1 2 1 2 1 2
same without the parameters being the same, hence β is not a function of Xβ and not identi-
fiable. Similarly, under the same conditions, while Xβ = Xβ , the function (I−N)β displays
1 2
(I−N)β ̸=(I−N)β ,so(I−N)β isnotidentifiable.
1 2
The distribution of the dataY depends only on Nβ. The parameter (I−N)β has no effect on
the distribution of Y, so there is no information in the data about (I−N)β. Even a Bayesian
who puts a prior distribution on β thereby determines a conditional distribution for (I−N)β
givenNβ andthisconditionaldistributionwillbethesameinthepriorandtheposterior. More-
over, the distribution of any future observation will depend on its mean, and if that mean is not
identifiable, it will depend on (I−N)β which we have no way to learn about except via prior
information.
2 Nonidentifiable Estimation
Commonlyusedmethodsforestimatingnonidentifiablelinearmodelparametersincludepenal-
izedleastsquares,spectralshrinkageestimates,andtheminimumnorm(shortest)leastsquares
estimate, see Hastie et al. (2020) and Richards et al. (2020). We just saw that all penalized
least squares and spectral shrinkage estimates are functions of the OLS estimates, indeed of
the unique least squares fitted values. It follows immediately that they will, in particular, be
functionsoftheminimumnormleastsquaresestimates.
In this section we discuss ridge regression, minimum norm least squares, gradient descent
estimates, and principal component regression estimates. Along the way we provide simple
proofs of some well known facts: that minimum norm least squares estimates are unique, that
4when properly initialized, a convergence point of gradiant descent must be the minimum norm
OLS estimate, that the Moore-Penrose generalized inverse OLS estimate gives the minimum
norm OLS estimate, that letting the ridge parameter go to zero gives minimum norm OLS, and
thatusingalloftheworthwhileprincipalcomponentsgivesminimumnormOLS.
Ridge regression estimates were all the rage in the 1970s. They then went out of fashion for
a long time but have now experienced a rebirth with the existence of large data sets. Ridge
regressionestimatesaredefinedaspenalizedleastsquaresestimates,
(cid:26) (cid:27)
1
βˆ ≡argmin ∥Y −Xβ∥2+λ∥β∥2 .
λ n
β
NotethatthispenaltyfunctiondoesnotmakealotofsenseunlessthecolumnsofX areonsome
common scale, a statement that also applies to most other off-the-shelf penalty functions like
LASSOandelasticnet.
Ridgeestimateshaveawellknownalternativeformulationintermsofamatrixformulaused
below. We now show the unsurprising fact that ridge estimates are also spectral shrinkage esti-
mates. Choosethespectralshrinkerfunction
1
λ(u)≡ .
u+λ
Thescalarλ determinesthefunctionλ andviceversa. Writetheridgeregressionestimateas
βˆ ≡ (X′X+λnI )−1X′Y
λ p
(cid:18) (cid:19)−1
1 1
= X′X+λI X′Y
p
n n
= (cid:2) VD(s )V′+λVV′(cid:3)−1 1 X′Y
j
n
= (cid:2) VD(s +λ)V′(cid:3)−1 1 X′Y
j
n
1
= VD[1/(s +λ)]V′ X′Y
j
n
(cid:18) (cid:19)
1 1
≡ λ X′X X′Y.
n n
This also establishes that for any λ > 0, the inverse (X′X +λnI)−1 =VD[n/(s +λ)]V′ always
j
existssothatridgeestimatesarealwaysunique.
Ridge estimators are often compared to minimum norm least squares estimates, that is, the
least squares estimate βˆ with the smallest value of ∥βˆ ∥2 ≡ βˆ′βˆ . Any OLS estimate βˆ has βˆ =
Nβˆ +(I−N)βˆ and∥βˆ ∥2 =∥Nβˆ ∥2+∥(I−N)βˆ ∥2. SinceNX′ =X′,wegetXN =X andXβˆ =X(Nβˆ )
ˆ
so, by the Fundamental Theorem of Least Squares Estimation, Nβ is an OLS estimate that is no
ˆ
longerthanβ.
ThesearchforminimumnormleastsquaresestimatescannowberestrictedtothoseinC(X′).
IfthereisauniqueestimateinC(X′)wearedone. AsarguedinNosedal-Sanchezetal.(2012)and
inChristensen(2019,Section3.1),ifβˆ andβ˜ arebothOLSestimatesinC(X′),then(βˆ −β˜ )∈C(X′)
5but also from the Fundamental Theorem, X(βˆ −β˜ )=0, so (βˆ −β˜ )⊥C(X′), i.e. (βˆ −β˜ )∈C(X′)⊥.
ButtheintersectionofthetwospacesC(X′)andC(X′)⊥ isthezerovectorbecausetheonlyvector
ˆ ˜
orthogonaltoitselfisthezerovector. Henceβ =β.
ˆ ˆ ˆ ˆ
Denote the minimum norm OLS estimate β . For any OLS β we have β = Nβ. Another
m m
method of finding βˆ is to fit the model Y = XX′δ +e by ordinary least squares and take βˆ =
m m
X′δˆ . This works becauseC(X)=C(XX′), so they have the same ppo M and by the Fundamental
Theorem applied to this and model (1.1),
Xβˆ
=MY
=XX′δˆ
, so
X′δˆ
is an OLS estimate of β but
is obviously in C(X′). For any generalized inverse of X′X, say (X′X)−, it is well known that
βˆ =(X′X)−X′Y isanOLSestimate. TheMoore-PenrosegeneralizedinverseofX′X,say(X′X)+≡
VD (n/s )V′,hasthepropertythatC[(X′X)+]=C(V)=C(X′). Therefore,theMoore-PenroseOLS
t t j t t
estimate βˆ+ ≡(X′X)+X′Y ∈C(X′) must be the minimum norm least squares estimate. Defining
(cid:110)1/u ifu>0
the spectral shrinker function ξ(u) ≡ , the Moore-Penrose estimate is also the
0 ifu=0
spectralshrinkageestimate
βˆ =βˆ+ =βˆ
.
ξ m
We now show the often stated but rarely proven fact that, if the gradient (steepest) descent
algorithm is properly initialized and converges, it must be to the minimum norm least squares
estimate βˆ . Set up gradient descent as minimizing ∥Y −Xβ∥2 initialized at β with step size
m 0
η/n>0,then
η
β ≡β + X′(Y −Xβ ).
t+1 t t
n
Clearly, any convergence point (β =β ) satisfies the normal equations (X′Y =X′Xβ ) hence is
t+1 t t
a least squares estimate. Generally, if β ∈C(X′), then all β s are inC(X′) and any convergence
0 t
point has to be the unique least squares estimate inC(X′). So if β ∈C(X′), gradient descent can
0
ˆ
only converge at the minimum norm least squares estimate β . Obviously, the common choice
m
β =0∈C(X′). For a completely general β , a point of convergence would be the OLS estimate
0 0
ˆ
β +(I−N)β . If you had prior information on β, which seems like it would be difficult to elicit
m 0
when p>n,youcouldstartgradientdescentatyourbestpriorguessandlikelygetareasonable
OLSestimatethatisnotinC(X′).
Richardsetal.(2021)arguethattheentiresequenceofgradientdescentiterationsareinterest-
ingspectralshrinkageestimateswithβ definedbyapplyingthespectralshrinkerfunction
t
t−1
t(u)≡η ∑(1−ηu)r =(1−(1−ηu)t)/u.
r=0
Forthistoactuallyshrinkweneedηmax (s )<1.
j j
The simplest and most familiar model in which the parameters are nonidentifiable is the bal-
ancedone-wayanalysisofvariance,
y =µ+α +ε , i=1,...,a, j=1,...,N.
ij i ij
Forcomputerprogramstoprintoutatableofestimatedcoefficients,theyneedtoselectachoice
ofleastsquaresestimates. Correspondingtofourcommonsideconditions,fourcommonchoices
6ofOLSestimatesare
 0   y¯   y¯   y¯ 
·· 1· a·
y¯ y¯ −y¯ 0 y¯ −y¯
 1·  1· ··    1· a· 
βˆ 1 = y¯ 2·  βˆ 2 = y¯ 2·−y¯ ··  βˆ 3 = y¯ 2·−y¯ 1·  βˆ 4 =  . . .  .
 .   .   .   
 . .   . .   . .  y¯ −y¯ 
a−1· a·
y¯ y¯ −y¯ y¯ −y¯ 0
a· a· ·· a· 1·
Asshownintheappendix,theminimumnormOLSandridgeestimatorsare
 a y¯ 
 a+a 1y¯
··
 (cid:16)a+1+aλ ··
(cid:17)
y¯ − a y¯  1 y¯ − a y¯ 
ˆ  1· a+1 ·· ˆ 1+aλ 1· a+1+aλ ·· 
β m =  . . .  ; β λ =   (cid:16) . . . (cid:17)  .
y¯ a·− a+a 1y¯ ·· 1+1 aλ y¯ a·− a+1a +aλy¯ ··
Thebalancedadditivetwo-wayANOVAis
y =µ+α +η +ε , i=1,...,a, j=1,...,b, k=1,...,N.
ijk i j ijk
LettingJ denoteanr vectorof1s,onewell-knownchoiceofleastsquaresestimatesis
r
y¯  y¯ 
    1·· ·1·
µˆ y¯
··· y¯ y¯
βˆ ≡αˆ =Y¯ a−y¯ ···J a, where Y¯ a =  2 . .·· ; Y¯ b =  · . .2· .
ηˆ Y¯ −y¯ J  .   . 
b ··· b
y¯ y¯
a·· ·b·
NotethatJ′Y¯ =ay¯ andJ′Y¯ =by¯ . Asshownintheappendix,
a a ··· b b ···
 µˆ m  a+a b+b aby¯ ··· 
βˆ
m
≡αˆ m=Y¯ a− a+a+ b+ab aby¯ ···J a.
ηˆ m Y¯ b− a+b+ b+ab aby¯ ···J b
Notethatβˆ isaleastsquaresestimatebecauseyˆ =y¯ +y¯ −y¯ =µˆ +αˆ +ηˆ .
m ijk i·· ·j· ··· m mi mj
Consider again a general spectral shrinkage estimate. We now show that all spectral shrink-
ageestimateshaveβˆ ∈C(X′). ThenormalequationsgiveX′Y =X′Xβˆ and,recallingthats >0
Ψ m j
for j=1,...,t andwhent < p,s =0for j=t+1,...,p,weobtain
j
(cid:18) (cid:19)−1
1 1
βˆ ≡ Ψ X′X X′Y
Ψ
n n
=
VD(cid:2)
Ψ(s
)(cid:3) V′1 X′Xβˆ
j m
n
= VD(cid:2) Ψ(s )(cid:3) V′VD(s )V′ βˆ
j j m
= VD(cid:2) Ψ(s )(cid:3) D(s )V′ βˆ
j j m
= VD(cid:2) Ψ(s )s (cid:3) V′ βˆ
j j m
= VD (cid:2) Ψ(s )s (cid:3) V′ βˆ .
t t j j t m
7ThusspectralshrinkageestimatesarealwaysinC(X′). IfΨ(s )s =1, j=1,...t,thenβˆ =βˆ ,so
j j Ψ m
forspectral shrinkageestimates toshrinkthe leastsquares estimates,we wantΨ(u)<1/u. Such
spectral shrinkage estimates are shrinking the components of the vector γˆ
=V′βˆ
that we will
t t m
soonseetobetheOLSestimateoftheregressioncoefficientsγ intheprincipalcomponentregres-
t
sion modelY =(XV)γ +e. Incidentally, generalized ridge regression using the penalty function
t t
β′Qβ forpositivedefiniteQdoesnotseemtokeeptheestimatesinC(X′)unlessC(QX′)=C(X′).
Forstandardridgeregressionthepreviousparagraphgives
β =VD [s /(s +λ)]V′ βˆ .
λ t t j j t m
Itisnowimmediatethatastheridgeparameterλ convergesto0,theridgeestimateconvergesto
theminimumnormOLSestimate. Asλ convergesto0,s /(s +λ)convergesto1for j=1,...,t,
j j
soVD [s /(s +λ)]V′ convergestoVV′ =N andβ convergestoNβˆ =βˆ ,theuniqueminimum
t t j j t t t λ m m
norm OLS estimate. Moreover, the ridge estimates only substantially differ from the minimum
norm OLS estimates in eigenvector directions for which the eigenvalues are small. When s is
j
large, s /(s +λ) is close to one and when s is small, s /(s +λ) is close to zero. To the extent
j j j j j
that we can categorize all of the s values as either large or small, we will see shortly that the
j
ridge estimates approximate principal component regression. Recall that, ignoring the (critical)
issuesoflocationadjustmentandscaling,thecolumnsofXV constitutethet nontrivialprincipal
t
componentsassociatedwithX′X.
Another method used when p > n is principal component regression, e.g. Hattab, Jackson,
andHuerta(2019). Againignoringtheissuesoflocationadjustmentandscalingofthepredictor
variables,regressiononthefirstr≤t principalcomponentsfitsthemodelY =(XV )γ +ebyOLS.
r r
The model implicitly presupposes that β =V γ ∈C(V) =C(X′), so the estimate of β becomes
r r t
βˆ ≡V γˆ or,
Pr r r
βˆ = V [V′X′XV ]−1V′X′Y
Pr r r r r
= V [V′X′XV ]−1V′X′Xβˆ
r r r r
= V [V′VD (ns )V′V ]−1V′VD (ns )V′ βˆ
r r t t j r r t t j t
(cid:26) (cid:20) (cid:21)(cid:27)−1
I
= V [I ,0]D (ns ) r [I ,0]D (ns )V′ βˆ
r r t j 0 r t j t
= V (cid:8) D (ns )(cid:9)−1 D (ns )V′ βˆ
r r j r j r
= VV′ βˆ
r r
=
VV′Nβˆ
r r
= VV′ βˆ
r r m
which projects βˆ into the space spanned by the first r eigenvectors of X′X. Because it is a pro-
m
ˆ
jection of the minimum norm OLS estimate, β must also be unique. This seems not quite to
Pr
qualifyasaspectralshrinkageestimatebecausethenecessaryΨfunctionwouldneedtodepend
on the specific eigenvalues of
1X′X.
If, rather than specifying the number of principal compo-
n
nents r, you specify how large the eigenvalue has to be to be included, that would be a spectral
shrinkage estimate. Christensen (2020, Section 13.1) argues that principal components corre-
spondingtosmalleigenvaluesshouldbeexcludedfromtheanalysisbecausetheyareunreliable
8directions in the estimation space. Marquart’s generalized inverse regression is equivalent to
principalcomponentregression,againignoringissuesoflocationadjustmentandscaling.
Note thatC(XV)=C(X), so the t dimensional full modelY =(XV)γ +e is equivalent to the
t t t
originallinearmodelbutreparameterizesitsothattheuniqueestimateγˆ
=V′βˆ
determinesthe
t t m
minimum norm OLS estimate
βˆ
≡Vγˆ
=VV′βˆ
=
βˆ
. The variance-bias trade-off discussion
Pt t t t t m m
of Christensen (2020, Section 2.9) arguing that correct reduced models always give improved
estimation and that even incorrect reduced models often improve estimation applies directly
to the full model Y = (XV)γ +e and reduced models Y = (XV )γ +e. In particular, when the
t t r r
reducedmodelis(closeto)correct,theOLSestimateγˆ shouldbeanimprovementovertheOLS
r
γˆ. TypicallyY =(XV )γ +e is an incorrect model, so γˆ typically provides a biased estimate of γ
t r r r t
but may have smaller overall error than γˆ. These properties extend to βˆ ≡V γˆ being perhaps
t Pr r r
ˆ ˆ
biasedforNβ =NVγ butperhapsbetteroverallthanβ =Vγˆ =β .
t t Pt t r m
Finally,considertheLASSO.ItisdifficulttoexaminetheLASSOapplieddirectlytoβ butwe
consider a transformed LASSO. As discussed earlier, any penalized (regularized) least squares
estimatecanbefoundbyminimizing
1 1
∥Xβˆ −Xβ∥2+λP(β)= (βˆ −β)′X′X(βˆ −β)+λP(β).
m m m
n n
Supposewespecifyapenaltyfunctionoftheform
p
P(β)= ∑ψ(e′V′ β)
j
j=1
where ψ(u) ≥ 0, ψ(0) = 0, and e is a p-vector of 0s except for a 1 in the jth spot. If we now
j
reparameterizeandalsotransformtheestimatesby
γ ≡V′ β; γˆ ≡V′ βˆ ,
m m
thefunctionweneedtominimizebecomes
p
(γˆ −γ)′D(s )(γˆ −γ)+λ∑ψ(γ )= ∑(cid:2) s (γˆ −γ )2+λψ(γ )(cid:3) . (2.1)
m j m j j mj j j
j j=1
The advantage of this form is that we can minimize the overall function by minimizing each of
the pcomponentfunctions. Forany jwiths =0,thefunctionwillbeminimizedbytakingγ =0
j j
andweonlyneedtominimize
t
∑(cid:2) s (γˆ −γ )2+λψ(γ )(cid:3) .
j j j j
j=1
ArgumentssimilartothoseinChristensen(2019,Section2.3)showthat,withtheLASSOpenalty
ψ(u)=|u|,theestimateγˆ isdeterminedbyγˆ =0,for j=t+1,...,pandfor j=1,...,t,
L Lj

γˆ −λ/2s ifγˆ ≥λ/2s
 mj j mj j
γˆ = 0 if|γˆ |<λ/2s
Lj mj j
γˆ +λ/2s ifγˆ ≤−λ/2s
mj j mj j
9ˆ
Reverting back to the original parameterization, β ≡Vγˆ and, because of γˆ = 0 for j = t +
L L Lj
1,...,p,wehaveβˆ ∈C(V)=C(X′). ThisLASSOperformsbothvariableselectionandshrinkage
L t
estimationontheprincipalcomponentsregressionmodel.
In general, if we use γˆ • to denote the minimizer of (2.1) and denote the derivative of ψ(u) as
ψ
•
ψ (u)≡d uψ(u),thenγˆ • =0for j=t+1,...,pandfor j≤t theestimateγˆ • willtakeononeofthe
ψj ψj
•
uvaluesatwhichthederivativedoesnotexistoritwillbeasolutiontoγˆ =γ −(λ/2s )ψ (γ ).
mj j j j
ˆ
We just need to check which of these values minimize (2.1). In any case, β • ≡Vγˆ • will be in
ψ ψ
C(V t)=C(X′)becauseγˆ
•
=0for j=t+1,...,p.
ψj
2.1 Will these work?
In linear model theory it is common to be concerned with estimating linear functions of β, say
Λ′β for some known p×r matrix Λ. As we will see, the standard problem of predicting future
observations is a special case of this estimation problem. In traditional p ≤ n linear models
there has been little need to consider estimation of any such functions that are not identifiable.
Such functions are said to be “estimable” if there exists a matrix P such that Λ′ =P′X. With this
structure, Λ′β = P′Xβ is clearly a function of Xβ and is identifiable. Moreover, Λ′β = P′Xβ =
P′XNβ = Λ′(Nβ), so estimation involves Nβ but never involves the unidentifiable parameter
(I−N)β.
The estimation methods we have considered revolve around the minimum norm OLS es-
ˆ
timate β which provides an unbiased estimate of Nβ. To see unbiasedness note that N =
m
X′(XX′)−X and by the Fundamental Theorem, for any OLS estimate E(Xβˆ )=E(MY)=ME(Y)=
MXβ =Xβ,soinparticularwehave
E(βˆ )=E(Nβˆ )=E[X′(XX′)−Xβˆ ]=X′(XX′)−E(Xβˆ )=X′(XX′)−Xβ =Nβ.
m
We can possibly improve on this estimate by fitting reduced models or using other forms of
shrinkageestimationasdiscussedearlier.
Unfortunately, with p>n restricting Λ′β to estimable functions seems to be untenable, espe-
cially when Λ′β is taken to be the mean of some future observations. In problems for which
Λ′(I−N)β is substantially different from 0 there is simply no hope of ensuring that our esti-
mation methods will do a good job. We might be able to use prior information on β to inform
ourdecisionson(I−N)β butevenwith p≤nitisdifficulttoelicitmeaningfulpriorinformation
onβ andwith p>nitiswell-nighimpossible. Theremainderofthisdiscussionlooksforsituationsin
which Λ′β is somehow close to being estimable so that it is plausible that Λ′(I−N)β is not substantially
ˆ
differentfrom0andestimationmethodsbasedonβ maydoagoodjob.
m
3 Prediction
Typically, we want to use model (1.1) to predict the observationsY in a future n dimensional
f f
linearmodelwiththesameβ parameter,
Y =X β +e . (3.1)
f f f
10Model (1.1) was taken as a standard model with mean 0, homoscedastic, uncorrelated – now
independent – errors and the same assumptions are typically made about the predictive model.
Also Y and Y are assumed independent. When p ≤ n it is standard practice (cf., Christensen,
f
2020, Section 6.6 or Christensen, 2024) to assume that X β is estimable in model (1.1). Predictive
f
estimability means that there exists a matrix P such that X = P′X, which implies that C(X′) =
f f f f
C(X′P )⊂C(X′). ItfollowsthatforthesepredictionproblemsX (I−N)β =0andwedonotneed
f f
toworryabout(I−N)β. With p<n,predictiveestimabilityautomaticallyholdsforallregression
models(t = p)anddoesnotseemlikeanunduerestrictionformodelswithcategoricalvariables,
but requiring predictive estimability seems overly restrictive when considering models with
p>n.
˜ ˜
It is easily seen that any estimate of β from model (1.1), say β(Y), hasY and β(Y) indepen-
f
dent,sotheexpectedsquaredpredictionerrorsatisfies
E∥Y −X β˜ (Y)∥2 =E∥Y −X β∥2+E∥X β −X β˜ (Y)∥2.
f f f f f f
ThusthepredictionproblemreducestotheproblemofestimatingX β. Ifweknewwhatβ was,
f
thebestpossiblepredictorofY wouldbeX β.
f f
3.1 Partitioned Models
Nobody uses the earlier theory directly, it needs to be adapted to partitioned models. Chris-
tensen (2019, Chapter 2) discusses penalized least squares for general partitioned models but
here we restrict ourselves to the simplest and most common case. Suppose the vector of predic-
torvariablesx alwayshasaleadelementof1,sothatx′ =(1,z′). Write
x′  1 z′ 
1 1
. . .
X = .
.
=.
.
.
.
=[J n,Z].
x′ 1 z′
n n
Model(1.1)becomes
Y =J β +Zβ +e.
n 0 ∗
WritingJr ≡J J′,themodelcanbereparameterizedas
r r r
Y =J α+[I−(1/n)Jn]Zβ +e. (3.2)
n n ∗
Note that both (1/n)Jn and I−(1/n)Jn are ppos. [I−(1/n)Jn]Z = Z−J z¯′ has each column in Z
n n n n ·
corrected for its sample mean value. (More often than not the columns would also be scaled to
haveacommonlength.)
It is well known that the OLS estimate of α is y¯, the sample mean. OLS estimates of β are
· ∗
obtainedbysolvingthenormalequationsortheFundamentalTheoremequations
Z′[I−(1/n)Jn]Zβ =Z′[I−(1/n)Jn]Y; [I−(1/n)Jn]Zβ =M Y,
n ∗ n n ∗ ∗
whereM istheppoontoC{[I−(1/n)Jn]Z}. Define
∗ n
1 1
S ≡ Z′[I−(1/n)Jn]Z; S ≡ Z′[I−(1/n)Jn]Y.
zz n−1 n zy n−1 n
11Notethatthenormalequationscannowberewrittenas
S β =S .
zz ∗ zy
When(y,z′)′formarandomsamplefromsomepopulation,S andS arethestandardunbiased
i i zz zy
estimates of Cov(z)≡Σ and Cov(z,y)≡Σ . Penalty functions typically only involve β , so are
zz zy ∗
written P(β ) and penalized least squares estimates are found by minimizing, for any OLS
∗
ˆ
estimateβ ,
∗
1 (cid:110) (cid:111)
(βˆ −β )′ Z′[I−(1/n)Jn]Z (βˆ −β )+λP(β ).
n−1 ∗ ∗ n ∗ ∗ ∗
SpectralshrinkageestimatesinvolvetheSVDofS ,
zz
S =VD(s )V′
zz j
and
ˆ
β ≡Ψ(S )S .
∗Ψ zz zy
With N the ppo ontoC{Z′[I−(1/n)Jn]}=C{Z′[I−(1/n)Jn]Z}, the relevant decomposition of β
∗ n n ∗
intoidentifiableandnonidentifiablecomponentsisβ =N β +(I−N )β .
∗ ∗ ∗ ∗ ∗
The best possible prediction from this model for a future case y with predictor variables
fi
(features)z isE(y )=α+(z −z¯)′β . TheOLSestimateofthisis
fi fi fi · ∗
yˆ ≡y¯ +(z −z¯)′ βˆ .
fi · fi · ∗
ˆ
Better predictions may or may not be obtained by replacing β with a penalized estimate or a
∗
spectral shrinkage estimate or a principle components estimate. For the parameter α +(z −
fi
z¯)′β tobeestimableweneed(z −z¯)∈C(S )=C{Z′[I−(1/n)Jn]}.
· ∗ fi · zz n
Thereparameterizedversionofthepartitionedpredictivemodelis
Y =J α+[Z −J z¯′]β +e ,
f n f f n f · ∗ f
where z¯′ = (1/n)J′Z so that the columns of Z are location corrected by data from model (1.1)
· n f
n
ratherthanusing[I −(1/n )J f]Z .
n f f n f f
Itiseasytotellifanyfuturecaseisagoodcandidateformakingaprediction. Simplyregressz −z¯ on
fi ·
thecolumnsofZ′[I−(1/n)Jn]. Ifthisfithasthesumofsquarederrorscloseto0,thepredictivecaseisclose
n
to being estimable and is less likely to be thrown off by the nonidentifable quantity (z −z¯)′(I−N )β .
fi · ∗ ∗
Whatwewouldlikeissomeassurancethatallofthecasesatwhichwemightwanttopredictwillbegood
candidatesformakingaprediction.
Fitting high dimensional polynomials is notorious as a method for overfitting data. Suppose
we have a univariate predictor w. With 7 distinct w values we can fit a 6th degree polynomial
which has p = 7. However, suppose the w values form 3 widely separated clusters, say w ∈
{0,0.5,1,10,19,19.5,20}. One cannot meaningfully fit anything more than a parabola to such
dataandfitting4th,5th,or6thdegreepolynomilswouldbeconsideredoverfittingandcangive
wild predictions for w values between the clusters. Fortunately it is easy to see that trying to
predictat,say,w=5usingahighdegreepolynomialwouldbeapoorideabecausethepredictive
leverage, defined as Var(x′ βˆ )/σ2 = [SE(x′ βˆ )]2/MSE, will be large, cf. Christensen (2015, Section
f f
9.2).
12The predictive estimability issue is something different. To get good predictions we will al-
ways need n somewhat large but for illustration consider n=3, p−1=4, and z=(w,w2,w3,w4)
withw=−1,0,1. Forpredictionsconsiderz basedonw =0.5,1.5. Itisnottohardtoseethat
f f
 
  1 0
−1 1 −1 1
Z = 0 0 0 0 and
C(cid:8) Z′[I−(1/n)Jn](cid:9) =C 0 1
.
n 1 0
1 1 1 1
0 1
Obviously, we can get good predictions whenever w is close to any of −1,0,1 but for the rela-
f
tivelydifficultpredictionsatw =0.5,1.5,
f
   
1/2 3/2 0.5 1.5
Z′
f
−z¯ ·J 2′ = 

− 15 // 812 1 29 7/ /1 82  =  − 00 .1.4 1 3. .6 4  .
−29/48 95/48 −0.6 2.0
While neither of the vectors z −z¯ is all that close to being in C{Z′[I−(1/n)Jn]}, clearly the
fi · n
first column z −z¯, corresponding to w =0.5, is much closer to predictive estimability than is
f1 · f
.
z −z¯ with w =1.5. Of course what we really need is to have (z −z¯)′(I−N )β =0 but in the
f2 · f fi · ∗ ∗
.
absence of knowledge about (I−N )β , all we can do is hope that having (z −z¯)′(I−N ) = 0
∗ ∗ fi · ∗
will yield good predictions. Of course in this example if n≥5 with at least 5 distinct w values,
predictive estimability will always hold exactly for the 4th degree polynomial, regardless of
issuesofoverfitting.
3.2 Best Linear Prediction
When(y,z′)′ isarandomvector,thebestlinearpredictor(BLP)ofyfromzisthe“linearexpecta-
tion”
Eˆ(y|z)≡µ +(z−µ )′ β ,
y z ∗
where E(y)= µ , E(z)= µ , and β is any solution to Σ β =Σ , e.g. Christensen (2020, Section
y z ∗ zz ∗ zy
6.3). ThisresultholdswhetherornotΣ isnonsingular.
zz
When the (y,z′)′s form a random sample from some population, the obvious thing to do is
i i
to replace the unknown first and second order moments with their unbiased estimates: µˆ =y¯,
y ·
µˆ =z¯,Σˆ =S ,andΣˆ =S whichleadstotakingβˆ tobeanysolutiontothenormalequations,
z · zz zz zy zy ∗
ˆ
S β =S ,
zz ∗ zy
andthenaturalestimateoftheBLPis
yˆ≡y¯+(z−z¯)′ βˆ .
∗
These are precisely the predictions associated with the OLS estimates for the reparameterized
partitionedmodel(3.2).
Similar to the fact that for linear models, estimable functions depend only on N β ∈C(S )
∗ ∗ zz
andnoton(I−N )β ⊥C(S ),BLPsdependonlyonacertainpartofβ . Decomposeβ =β +β
∗ ∗ zz ∗ ∗ 1 2
13with β ∈C(Σ ) and β ⊥C(Σ ). Prediction depends only on β . To see this, as in Christensen
1 zz 2 zz 1
(2020,Lemma1.3.5)weknowthatforsomerandomvectorbwehave
Pr[(z−µ )=Σ b]=1.
z zz
Itthenfollowsthatwithprobabilityone,
(z−µ )′ β =b′Σ β =b′Σ (β +β )=b′Σ β =(z−µ )′ β . (3.3)
z ∗ zz ∗ zz 1 2 zz 1 z 1
Since (z−µ ) ∈C(Σ ) a.s., when sampling future observations (y ,z′ )′ from the same pop-
z zz fi fi
ulation as the (y,z′)′s, we might hope that it should be nearly true that (z −z¯)∈C(S ) which
i i fi · zz
means that predictive estimability is nearly true and we can make good predictions. In stan-
dard regression with p ≤ n it is generally assumed that r(Σ ) = p−1 so C(Σ ) = Rp−1. If the
zz zz
zs have an absolutely continuous distribution (which requires Σ to be nonsingular), then with
i zz
probability one r(S )=min(p−1,n−1), cf. Okamoto (1973), so if p≤n, then r(S )= p−1 and
zz zz
C(S )=Rp−1, so predictive estimability holds. Unfortunately, when p>n, with probability one
zz
r(S ) = n−1 < p−1 so C(S ) is strictly contained within Rp−1 =C(Σ ) and when p >> n the
zz zz zz
spaceswillnotevenbeclose.
For p>n one can simply replace S with a more appropriate estimate. There is a wide litera-
zz
ture on such methods for estimating Σ , cf. Fan, Liao, and Liu (2016) or Lam (2020) but, as Lam
zz
points out, achieving consistent estimation requires one to impose a model for Σ . If the model
zz
is true, the predictions might be good but, for example, spectral shrinkers (implicitly) provide
estimates of Σ (as defined here they estimate the precision matrix Σ−1) and, as we have seen,
zz zz
evenspectralshrinkersthatgivenonsingularestimatesofanonsingularΣ (likethatassociated
zz
withridgeregression)donotsolvethepredictiveestimabilityproblemforspectralshrinkagees-
timates. Spectral shrinkage estimates remain estimates withinC(S ), so they still estimate N β
zz ∗ ∗
rather than all of β . In fact, the same can be said for any estimate of Σ that takes the form
∗ zz
Σ˜ =VDV′ foranydiagonalmatrixD.
zz
In the next section we argue that for complicated models with large p, the covariance matrix
Σ cansometimesbewellapproximatedbyamatrixofmuchlowerrankthan p−1. Thisinturn
zz
suggests that it should be nearly true that (z −z¯)∈C(S ), hence ensuring that nonestimability
fi · zz
isaaminorproblemandthatpredictionsshouldtypicallyworkwell.
Allofthisrequiresustosamplethe(y ,z′ )′sfromthesamepopulationasthe(y,z′)′s. While
fi fi i i
in practical prediction problems this may be a questionable assumption, when randomly divid-
ing a complete set of data into training and test data we ensure that the same model will apply
to both parts. While this does not ensure thatC(X′)⊂C(X′), i.e., predictive estimability, it does
f
assurethatforfixed pwithlargenandn ,
f
S →Σ ←S ,
zz zz fzz
inprobability. HereS ≡ 1 Z′[I −(1/n )Jn f]Z . Ourhopeforpredictiveestimabilityisthat
fzz n f−1 f n f f n f f
. . .
C(Σ )=C(S )=C([Z −J z¯′]′[Z −J z¯′])=C([Z −J z¯′]′)⊂C(S )=C(Σ )
zz fzz f n f · f n f · f n f · zz zz
The second approximate equality stems from having z¯ replaced by z¯ in computing the esti-
f· ·
mated covariance matrix of the future predictors. The subset relation is what we need to have
in order to get predictive estimability. Unfortunately, for p > n > n these approximations are
f
unconvincing.
143.3 Recapitulation
Torecap,ifmodel(1.1)isagoodmodel,evenwhen p>nwehavetoolstogetgoodestimatesof
N β . Anytimethatpredictiveestimabilityholdsinmodel(3.1),thatis,if
∗ ∗
C([Z −J z¯′]′[Z −J z¯′])=C([Z −J z¯′]′)⊂C(S ),
f n f · f n f · f n f · zz
good estimation of N β is sufficient for good prediction. With p > n we can probably only
∗ ∗
hope that predictive estimability will be approximately true in which case we can hope that
[Z −z¯′J ](I−N )β is close to 0 so that it does not have a deleterious affect on predictions.
f · n f ∗ ∗
We can easily check when predictive estimability is approximately true by doing a multivariate
regressionof[Z −J z¯′]′ onZ′[I−(1/n)Jn].
f n f · n
When the predictive model (3.1) is sampled from the same population as (1.1), the fact that
C([Z −J µ′]′) ⊂C(Σ ) with probability one gives us hope that C([Z −J z¯′]′) ⊂C(S ) is ap-
f n f z zz f n f · zz
proximately true so that predictive estimability is approximately true. When both n and n are
f
large, sample estimates from each model should be close to their population equivalents, so not
only does S approximate Σ but so do both S and (1/n )[Z −J z¯′]′[Z −J z¯′]. Again this
zz zz fzz f f n f · f n f ·
gives us a basis to hope that predictive estimability may be approximately true but for p > n
these approximations are complicated by rank deficiencies in the estimators as illustrated ear-
lier. As we will see in the next subsection, anytime r(Σ ) << n, predictive estimability should
zz
holdapproximately.
3.4 Reduced Rank Covariance Matrices
Supposer(Σ )=t−1< p−1. Underthisassumptionandrandomsamplingofthezs,thematrix
zz i
X in Section 1 has r(X) =t (with probability 1) as assumed earlier. A plausible model for this
phenomenonistheexistenceofarandomt−1vectorwwithabsolutelycontinuousdistribution,
E(w)=0,nonsingularCov(w)=Σ ,anda p−1×t−1fixedmatrixRofrankt−1withz−µ =Rw.
ww z
Clearly
Σ =RΣ R′.
zz ww
Rewritingourindependentdataasz −µ =Rw,leadstowriting
i z i
Z =WR′+J µ′.
n z
Then
1 1
S = Z′[I−(1/n)Jn]Z = RW′[I−(1/n)Jn]WR′ =RS R′.
zz n−1 n n−1 n ww
Regardless of the size of p, if n is large relative to t−1, then S will be a good estimate of Σ ,
ww ww
sowithRfixed,evenifRisunknown,S willbeagoodestimateofΣ .
zz zz
Since(z−µ )∈C(Σ )a.s.,whensamplingfutureobservations(y ,z′ )′ fromthesamepopula-
z zz fi fi
tionasthe(y,z′)′s,wecannowexpectittobenearlytruethat(z −z¯)∈C(S ),sothatpredictive
i i fi · zz
estimabilitywillnearlyholdandallourpredictionsshouldworkwell. Butthisonlyholdswhen
nissubstantiallylargerthant−1.
153.5 Spiked Covariance Matrices
Our hope is that the argument of the previous subsection remains true when the reduced rank
covariance structure is only approximately true. In practice we anticipate observing r(S ) =
zz
n−1< p−1 but Section 4 argues that rankt−1<<n reduced rank models are often reasonable
approximations when p > n. Spiked covariance models for Σ provide rank p−1 models that
zz
approximate the behavior of rank t −1 covariance structures. Spiked models have been the
subjectofconsiderableresearchinanasymptoticscenariowith p/n→γ. Heret isnolongerr(X).
Here most often r(X) is either n or p depending on whether γ >1 (p>n ) or γ <1 (p<n). The
standard asymptotic setting where everything works like we want is γ =0. (All the theoretical
workIhaveseenusessimplifyingassumptionsratherthanthepartitionedmodel.)
The original spike model for the eigenvalues of Σ has a finite number t−1 of large eigen-
zz
values with the remaining eigenvalues being constant. Generalizations of this model focus on
weakening the assumption that the remaining eigenvalues are constant. One more realistic
model is the generalized spike model of Bai and Yao (2012). While this model is quite com-
plicated and Dey and Lee (2019) comment on the limited work performed on this generalized
spike model, Dey and Lee have illustrations that seem to suggest that the model is consistent
witht−1 large eigenvalues and then exponential decay of the the remaining eigenvalues. (This
comment is for fixed n and p. The asymptotics often require the spike eigenvalues to grow
with p which complicates the meaning of exponential decay of the nonspike eigenvalues.) Jung
(2022) considers a somewhat different generalization of the spike model that also seems consis-
tent with exponential decay for fixed n and p. One reason to emphasize exponential decay of
the eigenvalues is that Thibeault, Allard, and Desrosiers (2024) have found that spikes together
with exponential decay of the remaining eigenvalues characterize wide classes of complicated
modelsacrossvariousdisciplines.
The main result of the spike covariance theory is that for γ >0 the first t−1 eigenvectors of
S converge to biased versions of the firstt−1 eigenvectors of Σ . This in turn implies that the
zz zz
first sample principle components converge to biased versions of the corresponding population
principle components. Changing his notation to agree with ours, Jung (2022) states, “the first
t−1sample andprediction [principlecomponent] scoresare comparableto thetrue scores. The
asymptoticrelationtellsthatforlarge p,thefirstt−1samplescores... convergetothetruescores
..., uniformly rotated and scaled for all data points.” (My italics.) The point of principle component
regressionisreducingdimensionwhileretainingasmuchaspossibleoftheinformationinthez
vector. As discussed in Christensen (2019, Section 14.1), any nonsingular linear transformation
ofagroupofprinciplecomponentscontainsthesame(linear)informationaboutzastheoriginal
principle components. Since rotation and scaling are linear transformations, for the purpose of
doing principle component regression on r≤t−1 principle components, the asymptotic bias of
thesampleprinciplecomponentsrelativetothepopulationprinciplecomponentsisirrelevant.
Althoughtheworkonthegeneralizedspikemodelsseemstofocusonthevalueoftheeigen-
vectors associated with the spikes, equally important for our purposes is that we not be leaving
valuable information on the table by ignoring the nonspike eigenvalues. Jung, Lee, and Ahn
(2018) indicate that for the original spike model “the remaining estimated [principle compo-
nent]scoresaremostlyaccumulatednoise”. Ifso,theyshouldbeofnouseinpredictingy. Other
work on spike models also suggests that the nonspike sample eigenvectors are uninformative
about the corresponding population eigenvectors. (Specifically, that the sample and population
16eigenvectorsareasymptoticallyorthogonal.)
WriteaSVD
Σ =V˜D(s˜ )V˜′; V˜ =[V˜ ,...,V˜ ]
zz j 1 p−1
p−1 p−1
As discussed, for example, in Christensen (2019, Section 14.1), ∑ j=r+1s˜ j/∑ j=1s˜ j measures the
relative amount of (linear) information in z lost by using only the first r population principle
components. Under exponential decay, this can be a relatively small amount. Under a spike
model the amount of information retained from considering only the spike principal compo-
nents is ∑t j− =1 1s˜ j/∑p j=− 11 s˜
j
which can be the vast majority of the information. But unfortunately,
there is no way to guarantee that the little information given up on z by using only the spike
components is not the most relevant information for prediction. As indicated earlier, one can
argue that the information being given up on z is not reliable. With training and test data we
can try our procedures and see if they work. When performing nonparametric multiple regres-
sionusinglinearcombinationsofspanning(orbasis)functions,intheoryanychoiceofspanning
functionsshouldwork(polynomials,trigfunctions,wavelets,etc.),butinpracticedifferentbasis
functions work better or worse on different data and they have different nonestimable parame-
ters(I−N )β .
∗ ∗
Again, the data only allow us to estimate N β , so any time (z −z¯)′(I−N )β ̸= 0 our pre-
∗ ∗ fi · ∗ ∗
dictions can be off. In the p > n linear model, we can never guarantee that we will do a good
job of predicting in the absence of predictive estimability because we cannot guarantee that
(z −z¯)′(I−N )β will be small. In particular problems it may work out. Similarly, best lin-
fi · ∗ ∗
ear prediction from z is equivalent to best linear prediction from all p−1 population principle
components and there is no guarantee that the population principle components with small
eigenvalues are not important for predicting y. However, if we are incapable of estimating the
small eigenvalue population principle components, it becomes irrelevant whether they are use-
ful for predicting y. We can only use as predictors the sample principle components that give
usmeaningfulinformation aboutthestructureofΣ . Ifthatis enoughtogivegood predictions,
zz
thatisfantastic.
Similartotheprevioussubsection,ifthespikemodelisclosetobeingarankt−1<<nmodel,
thelinearmodelshoulddisplayapproximatepredictiveestimability. If,asJung’s(2022)Theorem
.
1 asymptotics suggest, C(V˜ )=C(V )⊂C(S )=C(N ) whereV andV˜ now contain the
t−1 t−1 zz ∗ t−1 t−1
.
first t−1 columns ofV andV˜ , thenC(S )⊥ ⊂C(V )⊥ =C(V˜ )⊥, and one can argue that (z−
zz t−1 t−1
. .
z¯)′(I−N ) = 0 because that occurs if and only if ∥(I−N )(z−z¯)∥2 = 0 and, using the standard
∗ ∗
resultontheexpectedvalueofaquadraticform,
.
∥(I−N )(z−z¯)∥2 ≤∥(I−V V′ )(z−µ)∥2 =E∥(I−V˜ V˜′ )(z−µ)∥2
∗ t−1 t−1 t−1 t−1
p−1
=E(z−µ)′(I−V˜ V˜′ )(z−µ)=tr(cid:2) (I−V˜ V˜′ )V˜D(s˜)V˜(cid:3) = ∑ s˜ =. 0,
t−1 t−1 t−1 t−1 j
j=t
where the last approximate equality is the definition of what it means for the spike model to
be close to a rank t −1 model. Of course it is actually easy to check approximate predictive
estimability rather than relying on this argument. It also bears repeating that we will never
knowt.
174 A Model for Overfitting
One common procedure leading to linear models with more parameters than observations is
performingnonparametricmultipleregression. Forexample,ifyouobservethevectorx′=(1,z′),
fitting an r−1 degree interactive polynomial model in each component of z leads to the linear
model
r−1 r−1
E(y|z)= ∑ ··· ∑ β zk 1···zk p−1,
k 1...k p−1 1 p−1
k 1=0 k p−1=0
which is the sum of rp−1 terms. The same idea applies when fitting trig functions, splines, or
waveletsinwhichcase,usingdefinitionsoffunctionsφ appropriatefortheapplication,
h
r−1 r−1
E(y|z)= ∑ ··· ∑ β φ (z )···φ (z ).
k 1...k
p−1
k
1
1 kp p−1
k 1=0 k p−1=0
Here most often φ ≡1. The number of terms rp−1 quickly gets out of hand. If you measure just
0
p−1=5variablesinzanduser=8functionsφ tomodelthecurveineachdimension,thenthe
h
totalnumberofparametersintheconstructedlinearmodelisnearly33,000.
In general we assume that there is some underlying p−1 vector of predictor variables z and
thatforzthereissomevectorvaluedfunctionφ(z)takingvaluesinRs forwhich
E(y|z)=β +φ(z)′ β . (4.1)
0 ∗
In this model, our concern is estimation when s+1>n rather than with p>n. In this model we
canplausiblyhave p<<n. Thekeyideaisthatalimitednumberofactualrandomvariablesinz
aredrivingalinearexplanatorymodelbasedonφ(z)ofveryhighdimensionandthatthisfunc-
tionalrelationshipdrivestheapplicabilityofthecovariancemodelsdiscussedinSubsections3.3
and3.4,whereintherolesoft−1and p−1arenowbeingplayedby p−1ands,respectively.
Asillustratedfornonparametricmultipleregression,weoftenhaves>> p. Whilesmayneed
tobelargetogetagoodfittingmodel,oftenonepickssmuchlargerthanitneedstobetoensure
that the (unknown) relevant features of E(y|z) are being captured. If model (4.1) involves such
overfitting it is incumbent upon us to adjust for that by using some form of shrinkage estimate
like: fitting reduced models (e.g., principle components with r <n), penalized least squares, or
spectralshrinkageestimates.
For nonparametric multiple regression, z is observed and φ is known, so φ(z) is observed.
Moreover, for nonparametric multiple regression, we know that by taking s sufficiently large
with appropriate φ s, model (4.1) will be approximately correct. However, in general, we do not
h
needtoseezorknowφ weonlyneedtobeabletoobserveφ(z). Inageneralproblemwithalarge
number s of observable predictors, we can imagine that there are p−1 unobserved underlying
variableszthatdrivetheentireproblemandthatoursobservationsφ(z)areunknownfunctions
ofthoseunobservedunderlyingvariables.
WriteE[φ(z)]≡µ ,Cov[φ(z)]≡Σ andCov[φ(z),y]≡Σ ,thenthebestlinearpredictoris
φ φφ φy
Eˆ[y|φ(z)]=µ +[φ(z)−µ ]′ β
y φ ∗
18whereβ isanysolutionofΣ β =Σ . AfirstorderTaylor’sexpansionofφ(z)providesalinear
∗ φφ ∗ φy
(actuallyaffine)approximationtoφ(z)intermsofz,
.
φ(z)=φ(µ )+[d φ(µ )](z−µ ). (4.2)
z z z z
WhilewewillcontinuetousethisTaylor’sapproximation,thefundamentalideasimplyrequires
areasonablelinearapproximationofφ(z)basedonz. Basedonthelinearapproximation,
.
′
Σ =[d φ(µ )]Σ [d φ(µ )] .
φφ z z zz z z
If (4.2), or any other similar linear approximation, happens to hold exactly, then the reduced
rank covariance structure of Subsection 3.3 applies. If (4.2), or any other similar linear approx-
imation, applies, then the spike covariance structure of Subsection 3.4 seems plausible. Again,
althoughnotdirectlyrelevanttothisproblem,Thibeaultetal.(2024)foundthespikemodelwith
exponentialdecayapplicabletoawidevarietyofcomplexsystems.
ThatΣ andS canbeverynearlysingulariswell-known. Eveninjustonedimensionwith
φφ φφ
p < n, fitting high degree polynomials are notorious for their numerical difficulties due to the
collinearity in X′X and Z′[I−(1/n)Jn]Z. Hardly anyone fits y =β +β z +···+β z5+ε because
n i 0 1 i 5 i i
of high correlations among the predictors. One is far more likely to fit y =γ +γ (z −z¯)+···+
i 0 1 i ·
γ (z −z¯)4+β (z −z¯)5+ε but even then collinearity problems often persist (leading some to
4 i · 5 i · i
fit orthogonal polynomials). The point is that when fitting the raw or even the mean corrected
polynomial for this somewhat complex model, Σ and S will be very nearly singular. Unfor-
φφ φφ
tunately,aswewillseebyexampleinthenextsection,thisnearsingularitymaynotbesufficient
toensurethatt−1<<nasrequiredforastronghopeofapproximatepredictiveestimability.
5 Double Descent
Double descent is a phenomenon associated with the prediction errors from fitting increasingly
large models to a fixed set of data. We revert to the notation of Section 1 rather than Section 4.
The idea in our linear models is that n is fixed and p is increasing. The phenomenon is the
empirical observation that often when p is small we under fit the data, as p increases we get
increasinglybetterpredictionsbutas papproachesnwebegintooverfitthemodelandgetworse
resultstothepointthatwhen p=nweareofteninterpolatingthedata(yˆ =y)andgettingpoor
i i
predictions. The double in double descent is the observation that as p gets bigger than n, we
againseeimprovedpredictiveperformance.
The unexpressed catch is that as p increases, we need to specify the changing model matrix
X, which we now refer to as X . To observe double descent, you need some dross among the
p
sequenceofmodels.
Doubledescentisreallyaboutunderfitting,ratherthanoverfitting. Theideaisthatfor p<n,
the X models have a limited capability for modeling E(y|x) and this capability is at first fully
p
exploited and then subjected to overfitting as p → n. The second descent, when p > n, comes
fromusingincreasinglycomplexX thatactuallygivegoodapproximationstoE(y|x).
p
For example, we know that standard methods for nonparametric multiple regression can ap-
proximateanycontinuousE(y|x)butthatgettingagoodapproximationoftenrequires p>n,so,
in the absence of predictive estimability, we cannot be sure that our parameter estimates will
19yieldgoodpredictions. Ofcoursewhentestdataareavailable,wecanactuallyseewhich,ifany,
specific procedures work well. Another problem is that we do not know the structure of E(y|x),
so we do not know how large to make p in order to catch the appropriate structure, so we often
pick a p that overfits. Of course this is not an issue when looking specifically at the double de-
scent phenomenon, wherein one fits each X model in the sequence. Even with p>n we would
p
be inclined to stop fitting when the predictive fits started getting worse, although there is no
reasontobelievethataphenomenonoftripledescentcouldnotbepossible.
Hastie et al. (2022) provide a theoretical discussion of this phenomenon with numerous ref-
erences. Here we merely illustrate the double descent phenomenon with a simple simulated
example. Online supplemental material includes a more extensive discussion of our example
and our actual code. We now revert to the notation from Section 4 with p−1 original predictor
variablesandnonparametricmultipleregressionmodelswithatotalofs+1predictors.
We (Mohammad Hattab and I) generated data according to a multidimensional noninterac-
tive symmetric fourth degree polynomial E(y|z)=∑p−1 z4−2z2. Figure 1 shows the form of the
j=1 j j
regression function in each variable. The idea is to fit a sequence of polynomial models using
least squares, that the intercept and linear terms will be of little use, the quadratic term should
have some imperfect utility in explaining the tail behavior, the number of sample observations
will be chosen to cause a cubic polynomial to interpolate the training data, but we will con-
tinue to fit correct fourth and fifth degree polynomials with s+1>n using Moore-Penrose least
squaresestimates(minimumnormleastsquaresestimates).
From Figure 1, if you sample predictor values primarily from the center of this curve, say
between−1.5and1.5,aflatlinedoesaremarkablygoodjobofpredictionandfittingaquadratic
will be of little help. Many distributions for the predictor variables that are symmetric with a
mode at 0, like (multivariate) normals andt(df) distributions, fit this pattern. You have to have
substantial interest in values out near ±2 before fitting a quadratic starts to help. To achieve
this we initially used independentU(−2,2) distributions for the predictor variables and finally
settled on sampling correlated data with U(−2,2) marginal distributions for both the training
andtestdata.
Thesequenceofmodelstobefittedinvolvenointeractionsandincorporateaninterceptonly
model, a model that adds all linear terms in the various dimensions, then all quadratics, then
cubics, then quartics, then quintics, so our sequence involves only six models and our sequence
ofmodelshavenumbersofparameterss+1=1,p,1+2(p−1),...,1+5(p−1).
Wetrainedonn=1+3(p−1)observationssothatn=s+1whenfittingthecubicpolynomial
and we took a test sample size of n = 101. With truth being a symmetric 4th degree, fitting
f
the linear and cubic terms should be useless, the quadratic terms should model useful aspects
of the underlying truth and reduce both training error and prediction error relative to the lin-
ear terms. The cubic terms overfit and interpolate the data, so reduce training error to 0 but
raise the prediction error by incorporating and estimating useless terms. With p > n we incor-
porate the true quartic features of the model. The 4th degree model captures enough of the true
model to improve prediction beyond the quadratic model. The quintic model again constitutes
overfitting.
Fromr=1,...,1000differentY vectorsandforeachmodelwecomputedasimulationestimate
of the squared deviation between our predictions and the best predictions, i.e., we simulated
20Figure1: FunctionalformofE(y|x )=x4−2x2.
j j j
E [E(y |x )−x′ βˆ (Y)]2 using
x f,Y f f f
(cid:40) (cid:41)
1 1000 1 n f (cid:104) (cid:105)2
PMSE ≡ ∑ ∑ E(y |x )−x′ βˆ .
1000 n fi fi fi r
r=1 f i=1
We have a relatively small sample of predictions, n =101, but the goal is really to get accurate
f
predictionsratherthananaccurateestimateoftheexpectedsquaredpredictiveestimationerror.
Wealsocomputedasimulationestimateofthemeansquaredpredictionbiases,
1 n f
(cid:34)
1 1000
(cid:35)2
Bias2 = ∑ E(y |x )− ∑ x′ βˆ .
n fi fi 1000 fi r
f i=1 r=1
When we are fitting a correct linear model with s+1>n (in our simulations, either the quartic
orquinticmodels),thisbecomes
Bias2 = 1 ∑n f
(cid:32)
x′ β − 1 1 ∑000 x′ βˆ
(cid:33)2
=. 1 ∑n f (cid:0) x′ β −x′ Nβ(cid:1)2 = 1 ∑n f (cid:2) x′ (I−N)β(cid:3)2 ,
n fi 1000 fi r n fi fi n fi
f i=1 r=1 f i=1 f i=1
so when there are fewer observations than model terms, this squared bias term involves the
partsoftheBestPredictorthatweareincapableofestimating.
Table 1 contains simulation results for fitting polynomial models with 50 predictors, on the
left with the number of observations equal to the number of parameters in the cubic model and
ontherightwithtwicethatnumberofobservationsminus1. Withdifferentrandomselectionsof
predictorvariablesthenumbersinthesetablescanchangeafairamountbutthepatternsremain
pretty consistent. The table on the left displays the classic pattern of double descent with a big
21drop-offinPMSEforfittingthequadraticmodel,ahugeincreaseforfittingthecubicmodelthat
interpolates the data, a huge drop-off for fitting the correct quartic model even though there are
fewer observations than predictors, and then a moderate increase due to overfitting with the
quintic model. Most of the prediction error is due to estimation bias except when fitting the
optimalquarticmodelinwhichcasebiasisstillsubstantial. Ofcoursetheorderofmagnitudeof
thebiasproblemforthecorrectquinticmodelismuchlessthanthebiasfortheincorrectmodels.
The bias tells us that neither of the two correct models has β all that close to Nβ, so, although
thecorrectmodelsgreatlyimprovepredictions,thepredictionsleavemuchtobedesired.
Table1: ErrorinEstimatingBestPredictors: Polynomialmodelswith p−1=50,n =101.
f
n=151 n=301
Model PMSE Bias2 s+1 Model PMSE Bias2
M 3905.59 3905.59 1 M 4254.91 4254.91
0 0
M 4800.50 4799.97 51 M 5384.33 5384.12
1 1
M 364.01 361.91 101 M 260.05 259.55
2 2
M 82360.71 80054.88 151 M 483.90 482.80
3 3
M 12.30 6.81 201 M 2.32 0.0021
4 4
M 59.84 53.82 251 M 6.55 0.0060
5 5
The table on the right displays traditional regressions with more observations than param-
eters. With the larger sample size, estimability holds for all predictions. The pattern in PMSE
remains the same for models up to the quadratic but the cubic model is no longer an interpola-
tion model and is no longer severely overfitted. The cubic term is still useless, so it constitutes
overfitting relative to the useful but imperfect quadratic model. The quality of predictions for
the correct quartic and quintic models is much better with the larger sample size because the
parameter estimates are now theoretically unbiased. The quintic model is overfitted so behaves
somewhatworse. Asbefore,formodelsofdegreelessthan4,mostofthepredictionerrorisdue
toestimationbias.
For n = 151, of the 200 eigenvalues in S for the optimal fourth degree model, the largest
zz
49 accounted for 90% of the total and the largest 86 accounted for 99% of the total. While the
covariance matrix is very nearly singular, it does not have t−1 << n as discussed in Section 3
for getting everything close to predictive estimability. For n=301, of the 200 eigenvalues in S
zz
fortheoptimalfourthdegreemodel,thelargest59accountedfor90%ofthetotalandthelargest
96accountedfor99%ofthetotal.
Finally, we also examined much larger models with 500 predictor variables. Again, it takes
roughly half of the eigenvalues to account for 99% of the variability in the quartic model. The
resultspresentedinTable2arequalitativelycomparabletothoseinTable1. Thenumbersbeing
reportedareaverages,sothelargernumbersfortheincorrectmodelswith500predictorsrather
than 50 predictors, suggest that wrong models are a bigger problem with more predictors. It
seemsevenmoreclearfromthelargerprobleminTable2thatfittingaappropriatemodelisfarmore
important than having enough observations to fit that model using traditional methods, as desirable as
itistohaveadditionalobservations.
22Table2: ErrorinEstimatingBestPredictors: Polynomialmodelswith p−1=500,n =101.
f
n=1501 n=3001
Model PMSE Bias2 s+1 Model PMSE Bias2
M 266827.6 266827.6 1 M 275783 275783
0 0
M 333943.4 333942.9 501 M 293307 293307
1 1
M 20047.2 20045.3 1001 M 10027 10026
2 2
M 109707117. 109701539. 1501 M 15293 15292
3 3
M 12.3 7.1 2001 M 2.2 0.0023
4 4
M 46.6 41.1 2501 M 5.6 0.0066
5 5
6 Final Comments
When p>nwecanalwaysdecomposeβ intonontrivialcomponentsNβ ∈C(X′)and(I−N)β ∈
C(X′)⊥. Nβ is identifiable and estimable and the minimum norm least squares estimate is un-
biased for it. (I−N)β has no effect on the distribution ofY, so it is impossible to learn about it
from the data. The parameter (I−N)β can only be informed a priori. Many popular estimation
methodsfocusonestimatingNβ astheyprovideestimatesthatonlyexistinC(X′).
In the models considered, the best predictor is always the expected value of the future obser-
vation to be predicted. When these quantities are estimable, they are functions of Nβ and stan-
dardresultsonpredictionandestimationapply. Forpredictionmeansthatarenotestimable,we
have considered sampling models for the rows of X that suggest that most future observations
will have means that are close to being estimable, so that there is hope (but no assurance) that
the unidentifiable parameter (I−N)β will have little effect on the best predictor. Regardless of
thesamplingmodel,itiseasytocheckwhetherthemeanofafutureobservationisclosetobeing
estimable. When the overall data are randomly divided into training and test subsets, it is also
easytocheckwhether(I−N)β isplayingalargerolebecausethatwouldcausepoorpredictions
inthetestdata.
Asimplewaytoproceedintheseproblemsistoreplacemodel(1.1)withtheprinciplecompo-
nentmodelusingtheprinciplecomponentsforallthepositiveeigenvaluesofS . Thismodelcan
zz
beexploredwiththeusualarrayofreducedmodelsandbiasedestimationtechniquesbutishin-
dered by the fact that, if there are no replications in the rows of X, typically it would interpolate
thedataandgiveSSE =0.
With p>n,typicallytherewillbeinsufficientdegreesoffreedomforerrortoobtainareliable
estimate of σ2 from model (1.1). As discussed in Christensen (2024) [but eliminating his typos],
if you have test data and predictive estimability, you can use (Y −X βˆ )′[I+X (X′X)−X′]−1(Y −
f f f f f
X βˆ )/n as an unbiased estimate of σ2. Under normality this has the usual relationship to a
f f
χ2(n )distributionbutwillnotbeindependentoftypicalmeansquaresforhypothesesregarding
f
model(1.1). Withoutpredictiveestimability,onecouldchecktoseeifthereareenoughpredictive
cases with estimable means to give an adequate estimate of variance or, if that is not the case,
one could see if there are sufficient cases with either predictive estimability or near predictive
estimabilitytogetausefulideaofthevariancefromthisprocedure.
Another option is that if sample principle components corresponding to small but positive
eigenvalues have no relation to their corresponding population principle components, they
23could be dropped from the model and used for estimating error. The result seems likely to
be biased. Alternatively, if, as one typically would, we have adjusted the z variable to have
a common scale, we can probably drop the n−t principle components with the smallest esti-
mated regression coefficients. In my (admittedly limited) experience, this is pretty much what
theLASSOwoulddototheprinciplecomponentmodel.
When the original model (1.1) is over fitted, the various biased estimation methods may sug-
gest reductions that can be made in the original model. For example, if (1.1) is a 6th degree
polynomial in many predictor variables being fitted by principle components and the estimates
of β suggest that the 5th and 6th degree terms are not important, then one could try fitting only
a4thdegreepolynomial.
Our discussion has focused on highly over-parameterized linear models. Neural networks
areexamplesofhighlyover-parameterizednonlinearregressionmodels.
References
Bai, Z.D. and Yao, J. (2012). On sample eigenvalues in a generalized spiked population model,
JournalofMultivariateAnalysis,106,167-177.
Christensen,R.(2015). AnalysisofVariance,Design,andRegression: LinearModelingforUnbalanced
Data,SecondEdition. ChapmanandHall/CRCPres,BocaRaton,FL.
Christensen, Ronald (2019). Advanced Linear Modeling: Statistical Learning and Dependent Data,
ThirdEdition. Springer-Verlag,NewYork.
Christensen,Ronald(2020). PlaneAnswerstoComplexQuestions: TheTheoryofLinearModels,Fifth
Edition. Springer,NewYork.
Christensen, Ronald (2024). Comment on “Forbidden Knowledge and Specialized Training: A
Versatile Solution for the Two Main Sources of Overfitting in Linear Regression,” by Rohlfs
(2023),TheAmericanStatistician,78,131-133,DOI:10.1080/00031305.2023.2277156
Dey,RounakandLee,Seunggeun(2019). Asymptoticpropertiesofprincipalcomponentanalysis
and shrinkage-bias adjustment under the generalized spiked population model. Journal of
MultivariateAnalysis,173,145-164.
Fan, J., Liao, Y., and Liu, H. (2016). An overview of the estimation of large covariance and
precisionmatrices. TheEconometricsJournal,19(1),C1-C32.
Hastie, Trevor; Montanari, Andrea; Rosset, Saharon; and Tibshirani, Ryan J. (2022). Surprises in
highdimensional ridgeless least squares interpolation. The Annals of Statistics, 50, 949-986.
https://doi.org/10.1214/21-AOS2133.
Hattab,MohammadW.;Jackson,CharlesS.;andHuerta,Gabriel(2019). Analysisofclimatesen-
sitivity via high-dimensional principal component regression, Communications in Statistics:
CaseStudies,DataAnalysisandApplications,DOI:10.1080/23737484.2019.1670119
Hellton, K.H. and Thoresen, M.(2017). When and why are principal component scores a good
toolforvisualizinghigh-dimensionaldata? ScandinavianJournalofStatistics,44,581-816.
Jung, Sungkyu (2022). Adjusting systematic bias in high dimensional principal component
scores. StatisticaSinica,32,939-959.
24Jung, S.; Lee, M.H.; and Ahn, J. (2018). On the number of principal components in high dimen-
sions. Biometrika,105,389-402.
Lam, Clifford (2020). High-dimensional covariance matrix estimation. WIREs Computational
Statistics,12(2).
Nosedal-Sanchez, A.; Storlie, C.B.; Lee, T.C.M.; Christensen, R. (2012). Reproducing kernel
Hilbertspacesforpenalizedregression: Atutorial. TheAmericanStatistician,66,50-60.
Okamoto, M. (1973). Distinctness of the eigenvalues of a quadratic form in a multivariate sam-
ple. AnnalsofStatistics,1,763-765.
Richards, Dominic; Dobriban, Edgar; and Rebeschini, Patrick (2021). Comparing Classes of
Estimators: When does Gradient Descent Beat Ridge Regression in Linear Models? ArXiv:
2108.11872.
Thibeault,V.;Allard,A.;andDesrosiers,P.(2024). Thelow-rankhypothesisofcomplexsystems.
NaturePhysics,20,294-302.
Appendix
EXAMPLE: Findminimumnormandridgeestimatesinoverparameterizedbalancedone-way
ANOVAwithagroups. LetJc beanr×cmatrixof1sandJ ≡J1.
r r r
aN N N ··· N
N N 0 ··· 0
X′X
=
  N 0 N . . .


=(cid:20)
aN NJ
a′(cid:21)
.


. .
.
. .
.
... 0

NJ a NI a
N 0 ··· 0 N
Therankofthis(a+1)×(a+1)matrixisaandavectorintheorthogonalcomplementisdefined
by
(cid:20)
aN
NJ′(cid:21)(cid:20)
1
(cid:21)
a =0.
NJ NI −J
a a a
WithabasisfortheorthogonalcomplementwecanwritetheppoontoC(X′)as
(cid:20)
1
(cid:21)(cid:18)(cid:20)
1
(cid:21)′(cid:20)
1
(cid:21)(cid:19)−1(cid:20)
1
(cid:21)′
1
(cid:20)
1
−J′(cid:21)
N =I− =I− a
−J
a
−J
a
−J
a
−J
a
a+1 −J
a
J aa
and,sinceβˆ =[0,y¯ ,...,y¯ ]′ isanOLSestimate,theuniqueminimumnormestimateis
1· a·
 a y¯ 
a+1 ··
y¯ − a y¯
ˆ  1· a+1 ··
Nβ =
.
.
 . . 
y¯ − a y¯
a· a+1 ··
OtherchoicesforOLSgivenearlieryieldthesameresult.
25Ridgeregressionestimatesrequire
(cid:20) a+λ J′ (cid:21)
X′X+(λN)I =N a ,
J (1+λ)I
a a
where, for convenience, the ridge parameter is λN rather than λ. Using the standard formula
fortheinverseofapartitionedmatrix(e.g.,Christensen,2020,ExerciseB.21),
1 (cid:20) a+λ J′ (cid:21)−1 1 (cid:34) λ(a1 ++ 1λ +λ) λ(a+− 11 +λ)J a′ (cid:35)
a = (cid:16) (cid:17) .
N J a (1+λ)I a N λ(a+− 11 +λ)J a 1+1
λ
I+ λ(a+1 1+λ)J aJ a′
Theridgeregressionestimateis
(X′X+λNI)−1X′Y =
 a y¯ 
ay¯  a+1+λ ··
·· (cid:16) (cid:17)
(cid:34) λ(a1 ++ 1λ +λ) λ(a+− 11 +λ)J a′ (cid:35)  y¯ 1·   1+1 λ y¯ 1·− a+1a +λy¯ ··  
(cid:16) (cid:17)  . = 
λ(a+− 11 +λ)J a 1+1 λ I+ λ(a+1 1+λ)J aJ a′  . .    (cid:16) . . . (cid:17) 
y¯ a· 1 y¯ − a y¯
1+λ a· a+1+λ ··
Notethatforλ =0thesearetheminimumnormestimates.
Minimum norm OLS is minimizing ∥β∥2 subject to ∥Y −Xβ∥2 =∥Y −Xβˆ ∥2 whereas ridge is
minimizing∥Y −Xβ∥2 subjectto∥β∥2 =K whereK isdeterminedbyλ.
EXAMPLE: Findtheminimumnormestimateinabalancedadditivetwo-wayANOVA,
y =µ+α +η +ε , i=1,...,a, j=1,...,b,k=1,...,N.
ijk i j ijk
Themodelmatrixcanbewritten
X ={[J ⊗J ⊗J ],[I ⊗J ⊗J ],[J ⊗I ⊗J ]}
a b N a b N a b N
Onewell-knownchoiceofleastsquaresestimatesis
y¯  y¯ 
    1·· ·1·
µˆ y¯
··· y¯ y¯
βˆ =αˆ =Y¯ a−y¯ ···J a, where Y¯ a =  2 . .·· ; Y¯ b =  · . .2· .
ηˆ Y¯ −y¯ J  .   . 
b ··· b
y¯ y¯
a·· ·b·
NotethatJ′Y¯ =ay¯ andJ′Y¯ =by¯ .
a a ··· b b ···
Wenowgothroughthecomputationstofindtheminimumnormestimate.
 abN J′bN J′aN
a b
X′X =bNJ a bNI a J abN 
aNJ JaN aNI
b b b
26DefineRimplicitlywithC(R)=C(X′)⊥ via
 
1 1
0=X′X−J
a
0 =X′XR.
0 −J
b
WenowfindNβˆ whereN =I−R(R′R)−1R′.
(cid:20) (cid:21) (cid:20) (cid:21)
1+a 1 1 1+b −1
R′R= , (R′R)−1 = ,
1 1+b a+b+ab −1 1+a
1 (cid:20) b −(1+b)J′ J′ (cid:21)
(R′R)−1R′ = a b ,
a+b+ab a J′ −(1+a)J′
a b
(cid:20) (cid:21)
1 by¯
(R′R)−1R′ βˆ = ··· ,
a+b+ab ay¯
···
   
y¯ by¯ +ay¯
(cid:110) (cid:111) ··· 1 ··· ···
βˆ m =Nβˆ =βˆ −R (R′R)−1R′ βˆ =Y¯ a−y¯ ···J a−  −by¯ ···J a 
a+b+ab
Y¯ −y¯ J −ay¯ J
b ··· b ··· b
or
 µˆ 0  a+a b+b aby¯ ··· 
βˆ
m
=αˆ 0=Y¯ a− a+a+ b+ab aby¯ ···J a.
ηˆ 0 Y¯ b− a+b+ b+ab aby¯ ···J b
27