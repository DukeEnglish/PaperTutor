Multi-Layer Transformers Gradient Can be Approximated in
Almost Linear Time
Yingyu Liang∗ Zhizhou Sha† Zhenmei Shi‡ Zhao Song§ Yufa Zhou¶
Abstract
The quadratic computational complexity in the self-attention mechanism of popular trans-
formerarchitecturesposessignificantchallengesfortrainingandinference,particularlyinterms
of efficiency and memory requirements. Towards addressing these challenges, this paper intro-
duces anovelfastcomputationmethodforgradientcalculationinmulti-layertransformermod-
els. Our approach enables the computation of gradients for the entire multi-layer transformer
modelinalmostlineartimen1+o(1),wherenistheinputsequencelength.
Thisbreakthroughsig-
nificantly reduces the computational bottleneck associated with the traditional quadratic time
complexity. Our theory holds for any loss function and maintains a bounded approximation
error across the entire model. Furthermore, our analysis can hold when the multi-layer trans-
former model contains many practical sub-modules, such as residual connection, casual mask,
and multi-head attention. By improving the efficiency of gradient computation in large lan-
guage models, we hope that our work will facilitate the more effective training and deployment
of long-context language models based on our theoretical results.
∗ yingyul@hku.hk. The University of Hong Kong. yliang@cs.wisc.edu. University of Wisconsin-Madison.
† shazz20@mails.tsinghua.edu.cn. Tsinghua University.
‡ zhmeishi@cs.wisc.edu. University of Wisconsin-Madison.
§ zsong@adobe.com, magic.linuxkde@gmail.com. AdobeResearch.
¶ yufazhou@seas.upenn.edu. University of Pennsylvania.
4202
guA
32
]GL.sc[
1v33231.8042:viXraContents
1 Introduction 3
1.1 Key background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 Our contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2 Related Work 5
3 Preliminary 6
3.1 Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.2 Loss function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Close forms of gradient components . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Main Result 8
4.1 Fast computing for single layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4.2 Fast computing for multi-layer transformers . . . . . . . . . . . . . . . . . . . . . . . 9
4.3 Beyond the previous works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
5 Technical Overview 10
5.1 Low-rank approximation for attention matrix . . . . . . . . . . . . . . . . . . . . . . 11
5.2 Accelerating gradient computation of T (X) . . . . . . . . . . . . . . . . . . . . . . . 11
i
5.3 Accelerating gradient computation of W and W . . . . . . . . . . . . . . . . . . . 12
i Vi
5.4 Accelerating gradient computation for multi-Layer transformers . . . . . . . . . . . . 12
6 Extension 13
6.1 Multi-head attention and residual connections . . . . . . . . . . . . . . . . . . . . . . 13
6.2 Causal attention mask . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
6.3 Prompt tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
6.4 Synergy with system-level attention acceleration . . . . . . . . . . . . . . . . . . . . 14
7 Conclusion 14
A More Related Work 16
B Discussion and Extension Details 17
B.1 Multi-head attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
B.2 Residual connection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
B.3 Causal attention mask . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B.4 System-level attention acceleration . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B.5 Prompt tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
C Preliminary on Gradient Calculation 19
C.1 Basic math facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
C.2 Close form of three gradient components . . . . . . . . . . . . . . . . . . . . . . . . . 20
C.3 Basic notations for computing gradients . . . . . . . . . . . . . . . . . . . . . . . . . 21
C.4 Low rank representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C.5 Bounded entries of matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
1D Matrix View 25
D.1 Gradient of s(X) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
D.2 Gradient on T (X) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
i
D.3 Matrix view of C(X) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
D.4 Matrix view of gradient on T (X) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
i
D.5 Matrix view of each term in gradient on T (X) . . . . . . . . . . . . . . . . . . . . . 32
i
D.6 Components of gradient on T (X) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
i
E Fast Computation for gradient on T(X) 39
E.1 Fast computation for B (X) term . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
6
E.2 Fast computation for B (X) term . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
7
E.3 Fast computation for B (X) term . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
8
E.4 Fast computation for B (X) term . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
2
E.5 Fast computation for B (X) term . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
4
E.6 Putting everything together . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
F Fast Computation for Gradient on W 50
F.1 Key concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
F.2 Gradient of s(X) on W . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
F.3 Gradient of L(X) on W . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
F.4 Fast computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
G Fast Computation for Gradient on W 54
V
G.1 Gradient of s(X) on W . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
V
G.2 Gradient of L(X) on W . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
V
G.3 Fast computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
H Gradient Approximation for Entire Model 59
H.1 Computation time for G . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
i
H.2 Fast computation for single-layer transformer . . . . . . . . . . . . . . . . . . . . . . 62
H.3 Fast computation for multi-layer transformer . . . . . . . . . . . . . . . . . . . . . . 63
I Causal Attention Mask 66
I.1 Tools from previous work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
I.2 Fast computation with causal mask . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
J Residual Connection 71
J.1 Key concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
J.2 Analysis of the residual connection . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
J.3 Analysis for the entire model with the residual connection . . . . . . . . . . . . . . . 73
K Multi-head Attention 75
21 Introduction
Large Language Models (LLMs), such as ChatGPT [SZK+22], GPT-4 [AAA+23], Claude [Ant24],
Llama [TLI+23, LT24], and others, have demonstrated immense potential to enhance various as-
pects of our daily lives, e.g., conversation AI [LCT+24], AI agent [XCG+23, CYL+24], search
AI [Ope24], AI assistant [MWY+23, ZKAW23, KHC+24, FJL+24] and many so on. One of the
most emergent abilities of LLMs is dealing with long-context information, a format that is cru-
cial for recording material like academic papers, official reports, legal documents, and so on.
LLMs have proven adept at tackling long-context tasks, including Retrieval Augmented Gener-
ation (RAG) [LPP+20, GXG+23], zero-shot summarization [LSH+23, ZLD+24, CAM24, ZJV+24],
and maintaining very long-term conversations [XSW21, XGW+22, MLT+24], and so on. This
proficiency has necessitated the development of long-context modeling capabilities within LLMs.
LLMs are mainly based on Transformer architecture, the key module of which is the self-
attention mechanism. In attention computation, we will compute the attention score between each
pair of tokens, which is the complexity bottleneck during long context training and inference. In
detail, we need to spend O(n2d) runningtime for each self-attention block, which is quadratic in n,
where n is the length of the context token and d is the hidden feature dimension of the model. For
example, LLaMA 3.1 405B [LT24], one of the cutting-edge LLMs, supports n=128k and d= 4096
taking30.84M GPUtraininghours,whichunderscorestheneedformoreefficient trainingprocesses
for such extensive context models. Given the extensive context lengths of LLMs, this quadratic
scaling results in several critical challenges: (1) a marked decrease in training efficiency [HLZ+23,
LYL+23, HTH+24]; (2) substantial memory requirements to accommodate the large quadratic
gradient matrices [LWXZ22, LYG+23]; and (3) significant energy usage, which in turn contributes
to higher carbon dioxide emissions [SZM+23, SCZ+24].
One seminar work [AS23] showed that the self-attention inference can be approximated in
almost linear time. However, this result is for the inference time but does not address the main
challenge, which is the expensive computation in the training time. In this work, we address this
main challenge, by proving that the quadratic computational complexity in the back-propagation
of self-attention can be approximated in almost linear time. This suggests we may be able to save
the substantial resources required for training Large Language Models.
1.1 Key background
We first introduce a basic background, starting with defining the softmax function.
Definition 1.1 (Softmax). Let z Rn. We define Softmax: Rn Rn satisfying
∈ →
Softmax(z) := exp(z)/ exp(z),1 .
n
h i
Here we apply exp to a vector entry-wise.
Then, we introduce the definition of a single self-attention layer.
Definition 1.2 (Single layer self-attention). Let n be the number of tokens and d be the hidden
dimension size. Let X Rn×d denote the input data matrix. Let Q := XW ,K := XW ,V :=
Q K
∈
XW Rn×d. Let W := W W⊤ Rd×d be the key-query weight matrix, and let W Rd×d be
V ∈ Q K ∈ V ∈
the value weight matrix. The self-attention function Attn(Q,K,V) is:
Attn(Q,K,V) = Softmax(QK⊤/d)V.
We remark that we apply Softmax to each row of the n n matrix.
×
3The above Attn function takes Q,K,V as parameters, and can be re-parameterized by input
matrix X and then re-written in the following sense
Attn(X) := f(X) X W ,
V
· ·
where (1) A := exp(XWX⊤/d) Rn×n and exp is applied element-wise, (2) D := diag(A1 )
n
Rn×n, and (3) f(X):= D−1A R∈ n×n is the attention matrix. ∈
∈
In contemporary LLMs, the architecture typically incorporates multiple layers of transformers.
Consequently, in order to design a fast training algorithm for the entire model, it is imperative
to examine the self-attention layers within these multi-layer transformer structures. We provide a
formal definition for multi-layer self-attention as follows.
Definition 1.3 (Multi-layer transformer). Let m denote the number of transformer layers in the
model. Let X be the input sentence. Let g denote components other than self-attention in the i-th
i
transformer layer. Let Attn denote the self-attention module in the i-th transformer layer (see also
i
Definition 1.2). We define an m-layer transformer as
F (X) := g Attn g Attn g Attn g (X),
m m m m−1 m−1 1 1 0
◦ ◦ ◦ ◦···◦ ◦ ◦
where denotes function composition.
◦
In Definition 1.3, the g includes the layer norm, MLP, residual connection, dropout, positional
i
encoding,multi-headconcatenation, andotheroperations. Allforwardandbackwardcomputations
ofthesemodulescanberuninlineartimewithrespectton. Thus,inthiswork, wemainlyfocuson
theacceleration ofself-attention module. Specifically,asshowninDefinition1.2,then nattention
×
matrix f(X) dominates the computational complexity, introducing a quadratic bottleneck. In the
exact computation case, if the attention matrix is full rank, no acceleration is possible. However,
by compromising negligible accuracy, designing a fast sub-quadratic algorithm becomes feasible.
Fortunately, by employing the polynomial kernel approximation method from [AA22], we can ap-
proximate the attention matrix and achieve an almost linear time n1+o(1) algorithm, effectively
breaking the quadratic bottleneck.
1.2 Our contributions
We now state our main contributions as follows:
Theorem 1.4 (Main result, informal version of Theorem 4.2). Let n be the number of tokens, and
d be the hidden dimension size. We assume d = O(logn) and each number in matrices can be
written using O(logn) bits. There exists an algorithm (Algorithm 1) that can compute the gradient
of multi-layer self-attention (see also Definition 1.3) in almost linear time n1+o(1)d= n1+o(1), where
the approximation error of entire model can be bounded in 1/poly(n).
Our assumption is mild when the context length n is large, as we usually see feature dimension
d as a constant, which is also used in [AS23, AS24a]. Our results indicate that large language
models (LLMs) can be trained in almost linear time n1+o(1) and maintain a robust approximation
guarantee, while the traditional way takes Ω(n2) time. This advancement is realized through the
application of polynomial kernel approximation [AS23, AS24a]. To be more specific, by leveraging
the inherent sparsity within the dense attention matrix, we are able to perform efficient low-rank
approximation, thereby significantly accelerating the computation of the dense matrices. Our
framework is applicable to any loss function, making it universally applicable. Furthermore, our
4analysis can hold when the multi-layer transformer model contains many practical sub-modules,
such as residual connection, casual mask, and multi-head attention (Section 6).
Numerous studies, including Flash Attention [DFE+22, Dao23, SBZ+24], quantization tech-
niques [JKC+18, XLS+23, LTT+24], and sparsity approaches [HJK+24,KKF+23, MCW+24], have
empirically focused on accelerating attention mechanisms. However, theoretically, these methods
arestillconstrainedbyquadratictimecomplexity. Inthisstudy,weintroduceaninnovativeacceler-
ation technique (Algorithm 1) that effectively overcomes this quadratic bottleneck, backed by solid
theoretical foundations (Theorem 4.2). Moreover, this new method is designed to be seamlessly
integrated with existing approaches to further enhance their performance (see Section 6.4).
Our contributions are as follows:
• We introduce a fast computation method that allows the gradient of each self-attention layer
tobeapproximatedinalmostlineartimen1+o(1) with1/poly(n)error,breakingthequadratic
time complexity bottleneck (Theorem 4.1).
• We extend our single-layer results to module-wise gradient computation so that our Algo-
rithm 1 can provide approximated gradient computation in m n1+o(1) time for m-layer trans-
·
former. Furthermore, the approximation of the gradient diverges from the exact gradient by
an error of 1/poly(n) when considered across the entire model (Theorem 4.2).
• Additionally, our analysis can hold when the multi-layer transformer model contains resid-
ual connection, casual mask, and multi-head attention. Our results can be applied to any
gradient-basedalgorithm,e.g.,training,fullfine-tuning,prompt-tuning,andsoon(Section6).
Roadmap. Our paper organized as follows. Section 2 offers the related work. Section 3
provides essential conceptions and key definitions across the whole paper. Section 4 presents our
primary findings, where we articulate our novel algorithm that is capable of calculating gradients
across the entire model in almost linear time. In Section 5, we explain the techniques we employ,
including low-rank approximation, some tricks for accelerating computation of gradients, and an
analysis of the approximation error. Section 6 provides various extensions of our algorithm. Lastly,
we conclude this paper in Section 7.
2 Related Work
Attention mechanism. Attention mechanisms, including self-attention and cross-attention, are
pivotaltechniquesemployedinstate-of-the-artneuralnetworks. Sinceitwasintroducedin[VSP+17],
it has gained widespread adoption across various domains. In particular, it is integral to decoder-
only LLMs [RWC+19] and the Vision Transformer (ViT) architecture [DBK+20]. The former
has been instrumental in the remarkable success of LLMs, while the latter has significantly ad-
vanced the field of computer vision, encompassing applications such as image generation [RBL+22,
WSD+23,WXZ+24], detection [LMGH22, ZLC+21], segmentation [ZTT+22,TDC+23], andlayout
generation [GLA+21, CZY23, WCZ+23]. Moreover, attention mechanism can be integrated into
multi-modal models [XGH+21, ZHJL24, LSSZ24b, WMS+24], math reasoning [LLS+24a], diffusion
models[SSDK+20,LSSZ24c],differentialprivacy[BEPP22,SSC+22,WCY+23,LSSZ24a,SAMB24]
and many other techniques.
Long-context modeling in LLMs. As LLMs grow in size and capability, in-context learning
(ICL) [MLH+22, SWXL24, XSL24] has become a preferred method for directing these models to
perform a variety of tasks, as opposed to the resource-intensive process of fine-tuning. Nonetheless,
5research has indicated that longer prompts can impair LLMs performance due to the limitation on
maximum sequence length during pre-training [LZD+24]. Consequently, extending the maximum
sequence length during pre-training and fine-tuning stages is imperative. Enhancing training effi-
ciency is crucial given the prevalent use of the Transformer architecture in LLMs, which incurs a
quadratic computational cost relative to sequence length. Addressing this challenge, some stud-
ies have explored continued fine-tuning of LLMs with extended context lengths [TSP+24], while
othershaveexperimentedwiththeinterpolation andextrapolation capabilities ofpositionalembed-
ding [CWCT23, PAA+23, SAL+24]. However, these approaches have not fundamentally addressed
the core issue: the quadratic computational cost associated with sequence length in the attention
mechanism [KWH23, FCA23]. In this study, we delve into accelerating the attention mechanism,
thereby addressing the long-context modeling issue at its essence.
Attentionacceleration. Attentionmechanismhasfacedcriticismduetoitsquadratictimecom-
plexity with respect to context length, a concern exacerbated by the increasing length in modern
large language models (LLMs) such as GPT-4 [AAA+23], Claude [Ant24], Llama [TLI+23, LT24],
etc. Nevertheless, this limitation can be circumvented by employing polynomial kernel approxima-
tion techniques [AA22], which enable the derivation of a low-rank representation of the attention
matrix. This innovation significantly accelerates both the training and inference processes of a
single attention layer, achieving almost linear time complexity [AS23, AS24a], while our work sup-
ports both training and inference for any multi-layer transformer. Furthermore, this approach can
be extended to higher-order attention mechanisms, i.e., tensor attention, maintaining almost linear
time complexity during both training and inference [AS24b, LSSZ24b]. Moreover, there are other
theoretical approaches. For instance, [LLS+24b] introduces the conv-basis method to accelerate
attention computation. [HJK+24] proposes a near-linear time algorithm under the assumptions of
uniform softmax column norms and sparsity.
3 Preliminary
We first introduce some basic notation used in this paper in Section 3.1. In Section 3.2, we define
the loss function. Then, to better understand our proofs, we provide a closed-form analysis of the
three gradient components in a single self-attention transformer layer in Section 3.3.
3.1 Notations
For any positive integer n, we use [n] to denote set 1,2, ,n . For two vectors x Rn and
y Rn, we use x,y to denote the inner product bet{ ween· x·· ,y. } Namely, x,y = n ∈ x y . We
∈ h i h i i=1 i i
usee todenoteavector whereonly i-th coordinateis 1, andother entries are0. For each a,b Rn,
i
we use a b Rn to denote the Hardamard product, i.e. the i-th entry of (a bP ) is a b f∈ or all
i i
⊙ ∈ ⊙
i [n]. We use 1 to denote a length-n vector where all the entries are ones. We use A to
n ∞
∈ k k
denote the ℓ norm of a matrix A Rn×d, i.e. A := max A . We use poly(n) to
∞ ∞ i∈[n],j∈[d] i,j
∈ k k | |
denote polynomial time complexity with respective to n.
3.2 Loss function
The loss function is the optimization objective in the training of LLMs, and we define it as follows.
Definition 3.1 (Loss function L(X)). For some input matrix X Rn×d, we define the one-unit
∈
loss function ℓ(X) : Rn×d R, for any j [n],k [d]. Furthermore, we define the overall loss
j,k
→ ∈ ∈
6function L(X), such that
n d
L(X) = ℓ(X)
j,k
j=1k=1
XX
Remark 3.2. Typically, in Definition 3.1, the most widely used loss function in the LLM training
procedure is the cross-entropy loss function, which can also be viewed as a summation of one unit
loss function.
The output matrix of the multi-layer transformer needs to pass an additional linear layer to map
the hidden dimension d to the vocabulary size d . Assuming d is a constant, the weight matrix
voc voc
dimensions for this additional MLP layer are d d . The probability tensor Y Rn×dvoc is
voc pred
× ∈
the final output. We denote the ground truth as Y Rn×dvoc corresponding to Y . According to
gt pred
∈
the cross-entropy loss definition, the formula is expressed as
n dvoc
L (X) = (Y ) log((Y ) )
cross−entropy gt j,k pred j,k
−
j=1k=1
XX
where the summation iterates over all elements, and the ground truth (Y ) = 1 for the correct
gt j,k
class and 0 otherwise.
3.3 Close forms of gradient components
In traininglarge language models (LLMs), updatingthe modelnecessitates computing thegradient
of weights for every layer. Consequently, it becomes essential to derive the closed-form expressions
for all corresponding gradient components with respect to the weights of the query, key, and value
matrices in the transformer model. We first define some intermediate variables before detailing
these gradient components in each self-attention transformer layer.
Definition 3.3 (Intermediate variables T ). Let m denote the number of transformer layers in
i
the model. Let m-layer self-attention transformer be defined as Definition 1.3. Let d denote the
hidden dimension. Let n denote the sequence length. Let X Rn×d be the input sentence. Let
∈
g denote components other than self-attention in the i-th transformer layer. Let Attn denote the
i i
self-attention module in the i-th transformer layer (see also Definition 1.2).
For i 0,1,2, ,m , we define T (X) Rn×d be the intermediate variable (hidden states)
i
∈ { ··· } ∈
output by i-th layer self-attention transformer. Namely, we have
g (X), i = 0;
0
T (X) =
i ((g
i
Attn i)(T i−1(X)), i [m].
◦ ∈
Here, we use to denote function composition.
◦
Then, we are ready to introduce the close forms of the three gradient components in a single
self-attention transformer layer. Notably, according to the chain rule, the gradient of the k-th
transformer layer in LLMs depends on the gradient components from the (k +1)-th transformer
layer. The gradient can be calculated for every transformer layer by combining the upstream and
local gradients. The close forms of the gradients for each layer in multi-layer transformers are
formalized in the following lemma (Lemma 3.4).
7Lemma 3.4 (Close form of gradient components, informal version of Lemma C.3). Let L(X) be
defined as Definition 3.1. Let W = W W⊤,W Rd×d denote the attention weight in the
i Qi Ki Vi ∈
i-th transformer layer. Let T (X) denote the intermediate variable output by i-th self-attention
i
transformer layer (see Definition 3.3). Let G Rn×d denote the gradient matrix resulting from
i
∈
dL(X)
the application of the chain rule up to the function g , i.e., G = . For j [n],k [d],
i i dAttn i(Ti−1(X)) ∈ ∈
let G (j,k) denote the (j,k)-th entry of G , let dAttn i(Ti−1(X))j,k Rn×d denote the gradient of
i i dTi−1(X) ∈
(j,k)-th entry of Attn (T (X)). Then, we can show that
i i−1
• Part 1.
dL(X) n d dAttn (T (X))
i i−1 j,k
= G (j,k) .
i
dT (X) · dT (X)
i−1 i−1
j=1k=1
XX
• Part 2.
dL(X) n d dAttn (T (X))
i i−1 j,k
= G (j,k) .
i
dW · dW
i i
j=1k=1
XX
• Part 3.
dL(X) n d dAttn (T (X))
i i−1 j,k
= G (j,k) .
i
dW · dW
Vi
j=1k=1
Vi
XX
Based on the above close forms of three gradient components, we can move on to our main
results smoothly.
4 Main Result
Now, we present our main findings. We will work through this section in the following order:
In Section 4.1, we delineate the computational efficiency of our gradient calculation methods in
each single layer. In Section 4.2, we introduce our main theorem (Theorem 4.2) for multi-layer
transformer by integrating the preceding results and provide our main algorithm (Algorithm 1).
Section 4.3 discusses how we transcend the previous works.
4.1 Fast computing for single layer
In the case of single-layer attention, we provide our theorem that state the three gradient compo-
nents can be calculated in almost linear time with negligible error.
Theorem 4.1 (Single-layer gradient approximation). We assume d = O(logn) and each number
in matrices can be written using O(logn) bits. Let L(X) be defined as Definition 3.1. Suppose we
have a single-layer self-attention transformer model (m = 1 in Definition 1.3). We can approximate
one-layer self-attention for three gradient components, i.e. dL(X) , dL(X) and dL(X) , in n1+o(1) time
dX dW dWV
with 1/poly(n) error.
Proof. We finish the proof by Lemma 5.1, 5.2 and 5.3.
8Algorithm 1 Almost Linear Time (ALT) Multi-layer Transformer Gradient Approximation Algo-
rithm
1: datastructure ALTGrad ⊲ Theorem 4.1 and 4.2
2: members
3: n R: the length of input sequence
∈
4: d R: the hidden dimension
∈
5: m R: the number of transformer layers
∈
6: L(X) R: the loss function ⊲ Definition 3.1
∈
7: T i Rn×d: the output of i-th transformer layer
∈
8: Attn i Rn×d: the output that pass i-th attention layer
9: W i,W∈ Vi
∈
Rd×d : the weight matrices in i-th transformer layer
10: end members
11:
12: procedure
SingleGrad(dL(X)
) ⊲ Theorem 4.1
dTi
13: Compute G i = d dL A( ttX n i) via Lemma 5.4 ⊲ n1+o(1) time
14: Compute D 6,D 7,D 8,D 2,D 4 via Lemma E.5, E.6, E.8, E.10 ⊲ n1+o(1) time
dL(X)
15: /* Approximate , Lemma 5.1 */
dTi−1
16: g t D 6+e D 7e +De 8+e D 2+e D 4 ⊲ n1+o(1) time
← dL(X)
17: /* Approximate , Lemma 5.2 */
dWi
18: eConstreuct Ue 3,V 3 evia Leemmae5.2 ⊲ n1+o(1) time
19: g w
←
(T i⊤ −1U 3) ·(V 3⊤T i−1) ⊲ n1+o(1) time
dL(X)
20: /* Approximate , Lemma 5.3 */
21: Ce onstruct U 1,V 1
vdW iaVi
Lemma C.12 ⊲ n1+o(1) time
22: g v
←
(T i⊤ −1U 1) ·(V 1⊤G i) ⊲ n1+o(1) time
23: resper ee ct tu ivr en ly.g t,g w,g v ⊲ g t,g w,g v are the approximated d dL T( i−X 1) ,d dL W(X i) ,d dL W(X Vi) in next iteration,
24: end procedeuree e e e e
25:
26: procedure MultiGrad(L(X)) ⊲ Theorem 4.2
dL(X)
27: Compute ⊲ O(nd) time
dTm
dL(X)
28: g t ← dTm
29: for i = m 1 do
→
30: e g t,g w,g v SingleGrad (g t)
←
31: Optimize W i via g w using optimizer
32: Oepetimeize W Vi via g v using oeptimizer
33: end for e
34: end procedure e
35: end datastructure
4.2 Fast computing for multi-layer transformers
Based on the results demonstrated in previous sections, we are ready to introduce our main result:
the whole transformer model can be approximated in almost linear time.
Theorem 4.2 (Main result, formal version of Theorem 1.4). Let m denote the number of trans-
former layers. We assume d= O(logn) and each number in matrices can be written using O(logn)
9bits. We can show that, for any i [m], all the gradient components (see also Lemma 3.4) of the
∈
i-th layer can be computed by Algorithm 1 in almost linear time n1+o(1), and the approximation
error of entire m layer transformer model can be bounded by 1/poly(n).
Proof. The theorem can be proved by directly combining Theorem 4.1 and Lemma 5.5.
Theorem 4.2 demonstrates that, during the training of a multi-layer transformer model, at
each training iteration, the gradient computation for the weight matrices of each layer can be
performed in almost linear time n1+o(1). This result supports the feasibility of fast training for
any transformer-based large language models (LLMs). In Algorithm 1, we illustrate the process of
back-propagating gradients from the m-th transformer layer back to the first layer. This algorithm
highlights the significance of the gradient with respect to the intermediate variables T (X). Due to
i
the application of the chain rule in gradient computation, the gradient of T (X) is indispensable
i
for determining the gradients of the weight matrices W and W at the i-th layer. Consequently,
i Vi
by iteratively computing the gradient for T (X), we systematically propagate the gradient through
i
to the initial transformer layer.
Additionally, our algorithm is capable of computing the gradient with respect to the inputdata
X. Therefore, our algorithm also supports fast prompt tuning. For a more in-depth discussion on
this topic, please refer to Section 6.
4.3 Beyond the previous works
Ouralgorithmexhibitssignificantadvancementsovertwobrilliantrelatedpriorstudies,namely[AS23]
and [AS24a]. In [AS23], the authors proposed an almost linear time algorithm for computing the
forward process of the attention mechanism. In contrast, [AS24a] introduced an almost linear time
algorithm for the backward of attention mechanism. However, [AS24a] has the following limita-
tions: (1) only computing gradients for a single layer of the attention mechanism, which cannot
extend to multiple layers; (2) calculating gradients with respect to a specific loss, namely the ℓ
2
loss; (3) computing gradients only for the weight matrix W (as defined in Definition 1.2), but
i
ignore other crucial components such as the MLP layer following attention computation and the
activation function.
In our work, we have the following improvements beyond [AS24a]: (1) we enable almost linear
time gradient computation across an entire transformer layer, incorporating both the MLP layer
and the activation function; (2) our algorithm supports gradient calculation for any loss function
L(X) (see Definition 3.1); (3) we extend the gradient calculation to include not only W but also
i
T (X) and W . These advancements collectively demonstrate a substantial leap forward from the
i Vi
methodologies in [AS23] and [AS24a].
5 Technical Overview
This section provides a brief overview of the proof techniques used throughout this paper. Sec-
tion 5.1 delves into intuition under the low-rank approximation applied to the attention matrix
f(X). Section 5.2 elaborates on calculating gradients with respect to the intermediate variable
(hidden states) T in almost linear time. In Section 5.3, we detail the process for computing gradi-
i
ents for the weight matrices W and W within transformer layers in almost linear time. Finally,
V
in Section 5.4, we demonstrate how to generalize our findings from single-layer transformers to
multi-layer transformers, including an analysis of the runtime and approximation error across the
entire model.
105.1 Low-rank approximation for attention matrix
In this section, we delve into the crucial technique behindour work: the low-rank approximation of
the attention matrix, which is achieved through the polynomial method [ACSS20, AA22]. Drawing
inspiration from [AS23], the intuition of this approximation lies in the fact that the attention
matrix f(X) Rn×n (as defined in Definition 1.2), also referred to as the similarity matrix in
∈
attention mechanism, can be effectively approximated by low-rank matrices U ,V Rn×k1, where
1 1
∈
k = no(1). The naive method for calculating the attention matrix f(X) has a time complexity of
1
O(n2), whereas the input data X Rn×d contains only d n = n1+o(1) entries. This discrepancy
∈ ·
suggests the potential of using low-rank representations of f(X) to design a fast algorithm.
An example of how to use the low-rank representations is the attention forward (Attn(X) :=
f(X)V in Definition 1.2) as in [AS23]: approximating f(X) along does not lead to fast algorithm,
since U V⊤ still requires n nentries. But by using the structure of the whole, we can do it faster.
1 1 ×
By expressing f(X) as U V⊤, the attention forward becomes U V⊤ V . It is well known that
1 1 1 1
n×k1k1×n n×d
differentmultiplication sequencescanleadtodramatically differentnumbersofoperationsrequired,
so the order of matrix multiplications matters. We first pe| rf{ oz r} m|{ Vz} ⊤| V{z} Rk1×d and this cost
1 ∈
O(k nd)= n1+o(1) time. Then we perform U V⊤V costing O(nk d) = n1+o(1) time.
1 1 1 1
This method significantly reduces the computation time of the attention forward from O(n2) to
almost linear time, n1+o(1). Driven by thistechniqueandanalyzing theclose formsof thegradients,
we can extend the acceleration to the gradient of the entire model.
5.2 Accelerating gradient computation of T (X)
i
Based on the low-rank approximation method mentioned in Section 5.1, we can compute the gra-
dient of L(X) with respect to the intermediate variable T (X), which denotes the output of the
i
i-th transformer layer. This computation is critical as it enables us to calculate gradients for other
gradient components because of the chain rule.
Extending to any kind of loss function. According to the findings in [DSXY23], the gradient
dL(X)
can be decomposed into five components, namely C (X),C (X),C (X),C (X),C (X), as
dTi(X) 6 7 8 2 4
detailed in LemmaD.1. However, thegradient resultpresented in[DSXY23]is tailored to aspecific
loss function, the ℓ loss, limiting its applicability to a narrow range of scenarios. In this study, we
2
extend the scope of their findingsby extending them to apply to any loss function L(X), as defined
dL(X)
in Definition 3.1. By incorporating G = , we derive a closed-form expression for the
i dAttn i(Ti−1(X))
gradient of L(X) with respect to T (X), which is detailed in Section D.2.
i
Accelerating the gradient computation. To accelerate the gradient computation for T (X),
i
we need the matrix form of the gradients, as discussed in Section D. This approach is essential
for understanding the underlying mechanisms when applying the low-rank approximation tech-
nique in gradient calculations. Subsequently, using that technique, we can accelerate the gradient
dL(X)
computation for (Lemma 5.1). By individually applying this technique to each of the five
dTi(X)
terms, we demonstrate that each term can be computed in almost linear time, n1+o(1), as shown in
Sections E.1, E.2, E.3, E.4, and E.5.
The next step is to aggregate these terms, as described in Section E.6. Since all five terms are
n d matrices, the summation of these terms remains almost linear in complexity. Conclusively,
×
we are safe to argue that for any single-layer transformer, the gradient computation with respect
to the input tensor can be performed in almost linear time n1+o(1), as stated in Lemma 5.1.
11The statement made for a single transformer layer can be readily generalized to any layer
within an m-layer transformer model. For instance, consider the intermediate variables T (X) and
i
T (X) (as defined in Definition 3.3), where T (X) = (g Attn )(T (X)). Given the gradient
i−1 i i i i−1
◦
dL(X)
, as established in the previous paragraph, we can compute the gradient with respect to
dTi(X)
T (X), namely dL(X) , in almost linear time n1+o(1). For a multi-layer transformer model, the
i−1 dTi−1(X)
above process can beconducted recursively. Thus, wecan compute thegradient of theloss function
L(X) on any T (X) in almost linear time n1+o(1).
i
Lemma 5.1 (Fast computation for
dL(X)
, informal version of Lemma E.11). Let L(X) be de-
dTi(X)
fined as Definition 3.1. Let m denote the number of self-attention transformer layers (see Defi-
nition 1.3). Let T (X) denote the intermediate variable output by i-th self-attention transformer
i
layer (see Definition3.3). Weshow that dL(X) can be approximated inn1+o(1) time, with 1/poly(n)
dTi(X)
approximation error.
5.3 Accelerating gradient computation of W and W
i Vi
InSection5.2,wedetailedthefastcomputationofgradientsforintermediatevariablesT (X). Given
i
that W is definedas the productW W⊤ (see Definition 1.2), with W and W representing the
i Qi Ki Qi Ki
query and key weight matrices, respectively, the gradients of W and W represent all trainable
i Vi
weight matrices in a transformer layer. Consequently, by determiningthe gradients for W and W
i Vi
across each layer, we achieve almost linear time gradient back-propagation throughout multi-layer
transformer models.
Fast gradient computation. The prior study in [AS24a] demonstrated that the gradient of W
i
can be computed in almost linear time. We extend their findings by adapting their approach to ac-
commodateany loss functionL(X) (as definedinDefinition 3.1)andfurthergeneralize theirresults
to include the gradient computation for both W and W in each transformer layer (Lemma 5.2
i Vi
and 5.3).
Lemma 5.2 (Fast computation for
dL(X)
, informal version of Lemma F.5). Let L(X) be defined as
dWi
Definition 3.1. Let m denote the number of self-attention transformer layers (see Definition 1.3).
For any i [m], let W = W W⊤,W Rd×d denote the attention weight in the i-th transformer
∈ i Qi Ki Vi ∈
layer. We show that dL(X) can be approximated in n1+o(1) time, with 1/poly(n) approximation
dWi
error.
Lemma 5.3 (Fast computation for
dL(X)
, informalversion of LemmaG.4). Let L(X) be defined as
dWVi
Definition 3.1. Let m denote the number of self-attention transformer layers (see Definition 1.3).
For any i [m], let W = W W⊤,W Rd×d denote the attention weight in the i-th transformer
∈ i Qi Ki Vi ∈
layer. We show that dL(X) can be approximated in n1+o(1) time, with 1/poly(n) approximation
dWVi
error.
5.4 Accelerating gradient computation for multi-Layer transformers
In this section, our focus turns to extending the single-layer transformer result from the previous
section to a multi-layer transformer.
12Running time analysis. We derive the closed-form gradient for the non-attention components
within a transformer layer, namely the g function defined in Definition 1.3. With the closed-form
i
gradient of g established in Lemma H.1, we then demonstrate in Lemma 5.4 that the gradient
i
computation for g can also be achieved in almost linear time. Given that the number of layers m
i
is much smaller than n, we can treat m as a constant. Consequently, with respect to running time,
since the computation time for gradients on each layer is n1+o(1), we only need to iteratively repeat
this procedure for m time. Therefore, the overall running time for computing gradients across the
entire model is m n1+o(1).
·
Lemma 5.4 (Computation time for G , informal version of Lemma H.2). Let T (X) be defined
i i
as Definition 3.3, i.e. T (X) = (g Attn )(T (X)). Let G Rn×d denote the gradient matrix
i i i i−1 i
◦ ∈
dL(X)
resulting from the application of the chain rule up to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
Assume we already have dL(X) . Assuming for any Z Rn×d, we have g (Z) Rn×d, and g (Z)=
dTi(X) ∈ i ∈ i
φ(Z W ), where W Rd×d and φ : R R denotes any element-wise activation function. Let φ′
g g
· ∈ →
denote the derivative of φ. Then, we show that G can be computed in n1+o(1) time.
i
Error propagation analysis. Here, we consider the approximation error. In our setting, the
approximationerrororiginates fromthelow-rankapproximationoftheattention matrix,asdetailed
in Lemma C.12. As discussed in previous sections, the approximation error in each layer can be
bounded by 1/poly(n). Then, we only need to focus on how error propagates in different layers.
We first prove that our 1/poly(n) approximation error statement holds for a single-layer trans-
former, as evidenced in Lemma H.3. Subsequently, through mathematical induction and leveraging
the results of error propagation over the gradient of g , we can show that the approximation error
i
canbeboundedby1/poly(n)foranym-layer transformer(Lemma5.5),wherethenumberoflayers
m is considered as a constant.
Lemma 5.5 (Multi-layer transformer gradient approximation, informal version of Theorem H.4).
Let L(X) be defined as Definition 3.1. Let X be defined as Definition 1.2. Suppose we have a
m-layer transformer (see Definition 1.3). Then, for any i [m], we can show that,
∈
• Part 1: running time. Our algorithm can approximate dL(X) , dL(X) , and dL(X) in
dTi−1(X) dWi dWVi
n1+o(1) time.
• Part 2: error bound. The approximation of the entire transformer model can be bounded
dL(X)
by 1/poly(n). Namely, our algorithm output g satisfies g 1/poly(n).
k − dX k∞ ≤
6 Extension e e
Here, we explore potential enhancements to our algorithm. In Section 6.1, we show the potential
extension of multi-head attention and residual connections. In Section 6.2, we offer possibilities to
incorporate causal masked attention in almost linear time. In Section 6.3, we show our algorithm
can accelerate the prompt tuning process. In Section 6.4, we give the possible integration with
system-level attention acceleration techniques.
6.1 Multi-head attention and residual connections
Multi-head attention and residualconnections are important components in attention mechanisms.
While these components were not involved in our initial analysis for simplicity, incorporating them
13into our algorithm is straightforward, as detailed in Sections B.1 and B.2. Our algorithm maintains
the capability to compute gradients for multi-layer transformers with multi-head attention and
residualconnectioninalmostlineartime,suggestingthatitcanbereadilyadaptedtomorepractical
transformer models.
The detailed analysis of incorporating residual connection with our framework can be found in
Section J and Lemma J.3. For the synergy with multi-head attention, we provide comprehensive
analysis in Section K and Lemma K.2.
6.2 Causal attention mask
Thecausalattention maskiscriticaltopreventtransformersfrom“cheating” duringtrainingbyen-
suringfutureinformationisnotused. Thefull-rankcharacteristicofthecausalattentionmaskposes
challenges for low-rank approximations. Nevertheless, we have identified a method to accelerate
the computation of causal masked attention by exploiting its inherent properties, as demonstrated
in [LLS+24b], remaining almost linear time complexity. A comprehensive explanation is provided
in Section B.3.
More detailed analysis of causal attention can be found in Section I and Lemma I.7 and I.8.
6.3 Prompt tuning
Prompttuning(orprefixlearning)isaprevalentapproachinparameter-efficientfine-tuning(PEFT),
which requires the calculation of gradients on input data X. Given our algorithm’s ability to com-
putegradientsforintermediatevariablesT inapproximatelylineartime,wecansimilarlyaccelerate
i
thegradient computation for inputdataX, thusenhancingtheefficiency of theprompttuningpro-
cess. Additional details are provided in Section B.5.
6.4 Synergy with system-level attention acceleration
Many contemporary works focus on system-level acceleration of attention mechanisms, often by
leveraging caching and mitigating I/O bottlenecks. Our algorithm has the potential to integrate
with such advancements. By combining our theoretical improvements in computation time (from
O(n2) to n1+o(1)) with system-level optimizations, the overall efficiency of attention mechanism
computation may increase dramatically. We leave the implementation of our method on GPU as
future work since there are several coding challenges. More details can be found in Section B.4.
7 Conclusion
The attention mechanism in transformer models inherently has quadratic time complexity with
respect to the input token length. In this work, we propose a novel Algorithm 1, which can
approximately train a multi-layer transformer model in almost linear time, introducing only a
small error. Moreover, our algorithm is compatible with any loss function, practical sub-modules
(residual connection, casual mask, multi-head attention), and gradient-based algorithm and can be
seamlessly integrated with other system-level acceleration techniques. While we lack enterprise-
scale computational resources for training large language models, our theoretical findings suggest
that we can accelerate the training of LLMs in practice.
14Acknowledgement
Research is partially supported by the National Science Foundation (NSF) Grants 2023239-DMS,
CCF-2046710, and Air Force Grant FA9550-18-1-0166.
15Appendix
Roadmap. In Section A, we provide further related works of this paper. In Section B, we
provide a detailed discussion about several potential extensions of our framework. In Section C, we
introduce basic notations and concepts used in our paper, along with the low-rank approximation
technique introduced in [AS23] and [AS24a]. In Section D, we provide details about how we
integratethegradientofT (X)intomatrixform. InSectionE,weexplainhowtoapplythelow-rank
i
approximation technique to accelerate thecomputation for the gradient on T (X). In Section F,we
i
extend the result of [AS24a] to arbitrary loss functions and accelerate the computation of gradient
onW viathelow-rankapproximationtechnique. InSectionG,wecalculatethegradientonW and
V
accelerate the computation of the gradient on W . In Section H, with the help of math induction,
V
we analyze the time complexity and the approximation error across the entire model. In Section I,
wediscusshowourframeworkcanexpandtoanattention mechanismwithacausalattention mask.
In Section J, we provide details about how to integrate our framework with attention mechanism
withtheresidualconnection. InSectionK,wearguethat,withtheadditionofmulti-headattention,
our algorithm can still achieve almost linear time gradient computation.
A More Related Work
Attentiontheory. [BCB14]introducedattentionmechanismsinNLP,enhancingencoder-decoder
architecturewithvariable-lengthvectorstoimprovemachinetranslation. Buildingonthis,[LPM15]
developed local and global attention variants, further refining NLP tasks. Attention mechanisms
found diverse applications: [XBK+15] applied them to image captioning, [VSP+17]’s Transformer
model revolutionized NLP by capturing word relationships, and [VCC+17] incorporated atten-
tion into graph neural networks. Recent Large Language Model research has focused extensively
on attention computation [DMS23, AS23, ZHDK23, CLP+20, LSZ23, BSZ23, KKL20]. Studies
by [ZHDK23, CLP+20, KKL20] use Locality Sensitive Hashing for attention approximation, with
[ZHDK23] offering efficient dot-product attention. [BSZ23] and [AS23] explore static and dynamic
attention calculations, while [LSZ23] investigates hyperbolic regression regularization. [DMS23]
proposesalgorithms forreducingattention matrixdimensionality inLLMs. Attention hasalso been
examined from optimization andconvergence perspectives [LLR23,GMS23,SZKS21,ZKV+20], in-
vestigating word co-occurrence learning [LLR23], regression problems with exponential activation
functions [GMS23], attention mechanism evolution during training [SZKS21], and the impact of
heavy-tailed noise on stochastic gradient descent [ZKV+20]. Theoretical explorations of attention
variants include quantum attention [GSYZ23], tensor attention [AS24b, LSSZ24b], and differen-
tially private attention [LSSZ24a, GSY23, LSSS24].
More methods for model acceleration. Varioustechniqueshavebeendevelopedformodelac-
celeration. One approach involves modifying modelarchitectures to enable faster inference, such as
Mamba[GD23,DG24],LinearizingTransformers[ZBKR24,MVK+24],PolySketchFormer[KMZ23],
and the Hopfield Model [HCW+24, HCL+24, WHHL24, XHH+24, HLSL24, WHL+24, HYW+23]
and so on. Another line of work is to prune the weights in a neural network to reduce running
time and memory consumption [HPTD15, FC18, LAT19, WZG19, BGOFG20, BMBE20, LZ20,
TKYG20, CJD+21, HABN+21, HCI+21, JCR+22, FA22, FA23, SLBK24, LLSS24]. In addition,
specific techniques have been developed to accelerate LLM generation, including KV-Cache com-
pression [GZL+23, LDLG23, XTC+23, ZSZ+24, LWD+23, DYZ+24, XJD+24] and speculative de-
coding [LCH+24, SCY+24, ESL+24].
16B Discussion and Extension Details
In Section B.1, we argue that our framework can easily adapt to the multi-head attention mecha-
nism. In Section B.2, we introduce how to integrate residual connection to our framework. In Sec-
tion B.3, we detail the integration of the causal attention mask into our algorithm. In Section B.4,
we discuss the possibility of the synergy between our theoretical side attention acceleration and the
existing system-level attention acceleration mechanism. In Section B.5, we show how to expedite
prompt tuning using our results.
B.1 Multi-head attention
The multi-head attention mechanism was first introduced by [VSP+17]. This innovation allows
a token to simultaneously attend to multiple positions within the same layer, thereby enriching
the model’s capacity for capturing various dependencies. However, this enhanced capability comes
with an increase in the size of the attention matrix f(X) from 1 n n to h n n, where h
× × × ×
is the number of attention heads. To mitigate the computational burden, each head’s vector is
derived by splitting the original vector, reducing the dimensionality of each head to d := d/h. To
h
summarize, the key distinctions between multi-head and single-head attention are (1) an enlarged
attention matrix f(X) and (2) a reduced dimensionality d within each attention head.
h
Enlarged attention matrix. As previously discussed, the attention matrix’s dimensionality
increases with the number of heads, h. Despite this expansion, the application of the low-rank
approximation technique, as outlined in Section 5.1, ensures that the computation time for the
attention matrix remains almost linear. Specifically, for a constant number of heads h in the
multi-head mechanism, the time complexity for computing f(X) Rh×n×n is h n1+o(1) = n1+o(1).
∈ ·
Reduced dimensionality. Another differentiating factor of multi-head attention is the lower
dimensionality processed by each head, i.e. d := d/h, compared the fulld in single-head attention.
h
This reduction ensures that the gradient computation time does not increase with the introduction
of multiple attention heads.
We provide comprehensive analysis of the synergy of our algorithm with multi-head attention
in Section K. We first prove in Lemma K.2, with the addition of multi-head attention, the gradient
over the attention mechanism can be computed in almost linear time. Then, we further prove that
for any multi-layer transformer, with multi-head attention, the gradient can becomputed in almost
linear time as well.
B.2 Residual connection
Residual connection is a pivotal technique in deep neural network architectures, effectively ad-
dressing issues such as vanishing and exploding gradients during training process, and facilitating
faster convergence of the model. Residual connection is also integrated into the standard attention
mechanism. Formally, given the intermediate variable T (X) output by the i-th transformer layer
i
as definedin Definition 3.3, we providethe formal definition of residualconnection in Definition J.1
and J.2. Since the residual connection only brings an additional add operation to each component
and with T (X) belonging to the space Rn×d, the residual connection introduces only a marginal
i
computational overhead of O(n d) per layer. Consequently, the total computational cost for each
·
layer is O(n d)+n1+o(1) = n1+o(1). Hence, by intuition, the inclusion of residual connections does
·
not compromise the overall complexity of our method.
17The detailed analysis is provided in Section J, where we first prove in Lemma J.3, that if the
gradient over one structure can be computed in almost linear time, then with the addition of the
residual connection, the gradient can also be computed in almost linear time. Then we use math
induction to extend our result to the entire multi-layer transformer model.
B.3 Causal attention mask
In transformer training, attention mask is a crucial component, designed to prevent a given token
from attending to future tokens in the sequence. Causal attention mask is a widely used attention
mask, which is configured as a lower triangular matrix, where elements on or below the main
diagonal are ones, with all other entries being zeros.
Now we describe how to incorporate this into our algorithm. Let M 0,1 n×n represent the
∈ { }
causal attention mask (see Definition I.2). Let f(X) := D−1(M A) where A = exp(XWX⊤/d)
⊙
and D := diag((M A) 1 ). Lemma I.1 reveals that A has a low-rank representation given by
n
U V⊤. Using Lemm⊙ a I.3· , we know (M (U V⊤b)) v for any vector v Rn can be computed in
0 0 ⊙ 0 0 · ∈
almost linear time.
To integrate the causal mask into the gradient computation within each transformer layer, we
firstfindall instances that have the structureof f(X) H or (f(X) (UV⊤)) H, whereH,U,V are
· ⊙ ·
low rank matrices. Then, we replace f(X) with f(X) in these instances. More detailed analysis of
causal attention can be found in Section I. To be more specific, we group the gradient components
forT ,W ,W intotwocategories, onefordotprobduct(LemmaI.7),anotherforHadamardproduct
i i Vi
(Lemma I.8). After showing each component can be calculated in almost linear time, the overall
gradient computation remains n1+o(1) time. Thus, our framework can seamlessly accommodate
causal attention masks.
B.4 System-level attention acceleration
The attention computing acceleration involves a two-pronged strategy that leverages both system-
level improvements (e.g. Flash Attention [DFE+22, Dao23, SBZ+24]) and the theoretical time
complexity improvements (e.g. our work and [HJK+24]).
Numerous efforts have been made in the literature to accelerate attention calculations at the
system level. For instance, Flash Attention [DFE+22, Dao23, SBZ+24] targets the I/O bottleneck
inherent in attention mechanisms. Studies such as block-wise parallel decoding [SSU18] focus on
implementing parallel decoding within transformer models to enhance inference speed. Addition-
ally, recent advancements in the field of speculative decoding, such as Medusa [CLG+24], leverage
a smaller, more efficient model to generate predictions, with the larger model only responsible for
validating, the smaller model’s outputs [LKM23].
Despitetheseinnovations,theaforementionedmethodsdonotaddressthefundamentalquadratic
time complexity O(n2) of the attention mechanisms. This presents an opportunity to complement
our low-rank approximation technique, with these system-level optimizations, thereby achieving an
even greater acceleration in attention computation. For instance, we could design an I/O-aware
algorithm for Algorithm 1, similar to the approach taken by Flash Attention, to effectively leverage
GPU acceleration.
To implement our algorithm practically on GPU, we have some coding challenges to fix: (1)
we need to define some new tensor operations in PyTorch, e.g. Eq. (5), Eq. (8); (2) we need to
systematically re-implement some back-propagation function of the current PyTorch function; (3)
we need to implement some CUDA function to run our algorithm in parallel for the casual mask,
see discussion in Section B.3. We may leave this as our future work.
18B.5 Prompt tuning
Prompt tuning, as introduced by various studies [LL21, LARC21, LJF+22, MLG24, HSK+24,
LSSY24], has emerged as a parameter-efficient fine-tuning strategy for large language models
(LLMs). Specifically,prompttuninginvolvesadjusting“softprompts”conditionedonfrozenLLMs.
This method requires relatively small number of tuneable parameters compared with fine-tuning
the entire LLMs, making it a popular choice for conserving training resources, including data and
computational power.
Theanalysisrevealsthattheessenceofprompttuninginvolvescomputinggradientswithrespect
to the soft prompts X across the entire model. In both prompt tuning and full fine-tuning, the
p
quadratic O(n2) computational complexity of gradient calculation remains the same due to the
self-attention mechanism inherent in LLMs.
In this work, leveraging the low-rank approximation technique discussed in Section 5.1, our
algorithm (Algorithm 1) efficiently computes gradients on soft prompts X over the entire model
p
in almost linear time. This suggests that our method is universal and can also be applied within
traditional prompt tuning frameworks.
C Preliminary on Gradient Calculation
In Section C.1, we list several useful math facts used in the following sections of this paper. In
Section C.2, we provide the close forms of the gradient components. In Section C.3, we introduce
some mathematical definitions to facilitate understanding of gradient calculations. In Section C.4,
we list some low rank approximation technique introduced in [AS23] and [AS24a]. In Section C.5,
we demonstrate that the entries of matrices defined in Section C.3 are bounded.
Notations. For two vectors x Rn and y Rn, we use x,y to denote the inner product
n∈ ∈ h i
between x,y. Namely, x,y = x y . We use e to denote a vector where only i-th coordinate
h i i=1 i i i
is 1, and other entries are 0. For each a,b Rn, we use a b Rn to denote the Hardamard
P ∈ ⊙ ∈
product, i.e. the i-th entry of (a b) is a b for all i [n]. We use 1 to denote a length-n vector
i i n
⊙ ∈
where all the entries are ones. We use A to denote the ℓ norm of a matrix A Rn×d, i.e.
∞ ∞
k k ∈
A := max A . We usepoly(n) to denote polynomial time complexity with respective
∞ i∈[n],j∈[d] i,j
k k | |
to n.
C.1 Basic math facts
In this section, we provide some useful basic math facts,
Fact C.1. Let x,y,z Rn. Then we have
∈
• x y,z = x⊤diag(y)z.
h ⊙ i
• x,(y z) = y,(x z) = z,(y x)
h ⊙ i h ⊙ i h ⊙ i
• x,y = x y,1 .
n
h i h ⊙ i
Then, we introduce a classical folklore used for the Hadamard product of two matrices.
Fact C.2 (Folklore, [AS24a]). Let U ,V Rn×k1. Let U ,V Rn×k2. Then we have
1 1 2 2
∈ ∈
( U V⊤) ( U V⊤)= (U U )(V V )⊤
1 1 ⊙ 2 2 1 ⊘ 2 1 ⊘ 2
n×k1k1×n n×k2k2×n n×k1k2 k1k2×n
|{z}|{z} |{z}|{z} | {z }| {z }
19Here, given U Rn×k1 and U Rn×k2, the U U Rn×k1k2 is the row-wise Kronecker
1 2 1 2
∈ ∈ ⊘ ∈
product, i.e., (U U ) := (U ) U for all i [n], l [k ] and l [k ].
1
⊘
2 i,l1+(l2−1)k1 1 i,l1 i,l2
∈
1
∈
1 2
∈
2
C.2 Close form of three gradient components
In this section, we show how to derive the close form for the gradient components within each layer
of a multi-layer transformer.
Lemma C.3 (Close form of gradient components, formal version of Lemma 3.4). If we have the
below conditions,
• Let L(X) be defined as Definition 3.1.
• Let W := W W⊤ Rd×d be the key-query weight matrix, W Rd×d be the value weight
i Qi Ki ∈ Vi ∈
matrix for the i-th transformer layer.
• Let T (X) denote the intermediate variable output by i-th self-attention transformer layer (see
i
Definition 3.3).
• Let G Rn×d denote the gradient matrix resulting from the application of the chain rule up
i
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
• For i [n],j [d], let G (i ,j ) denote the (i ,j )-th entry of G , let dAttn i(Ti−1(X))i2,j2
2 ∈ 2 ∈ i 2 2 2 2 i dTi−1(X) ∈
Rn×d denote the gradient of (i ,j )-th entry of Attn (T (X)).
2 2 i i−1
Then, we can show that
• Part 1.
dL(X) n d dAttn (T (X))
= G (i ,j )
i i−1 i2,j2.
i 2 2
dT (X) · dT (X)
i−1 i−1
i X2=1j X2=1
• Part 2.
dL(X) n d dAttn (T (X))
= G (i ,j )
i i−1 i2,j2.
i 2 2
dW · dW
i i
i X2=1j X2=1
• Part 3.
dL(X) n d dAttn (T (X))
= G (i ,j )
i i−1 i2,j2.
i 2 2
dW · dW
Vi
i X2=1j X2=1
Vi
Proof. We have
• L(X) R.
∈
• Attn (T (X)) Rn×d,T (X) Rn×d.
i i−1 i−1
∈ ∈
• W Rd×d,W Rd×d.
i
∈
Vi
∈
Therefore, we have
20• dL(X) Rn×d, dAttn i(Ti−1(X)) R(n×d)×(n×d).
dTi−1(X) ∈ dTi−1(X) ∈
• dL(X) Rd×d, dAttn i(Ti−1(X)) R(n×d)×(d×d).
dWi ∈ dWi ∈
• dL(X) Rd×d, dAttn i(Ti−1(X)) R(n×d)×(d×d).
dWVi ∈ dWVi ∈
Then, simply applying chain rule, we can get the final results.
C.3 Basic notations for computing gradients
Before we move on to compute gradients, we need to define some useful notations.
We begin with introducing the index for a matrix.
Definition C.4 (Simplified notations). For any matrix Z Rn×d, for i [n],j [d], we have
∈ ∈ ∈
following definitions:
• Let Z and Z(i,j) denote the (i,j)-th entry of Z.
i,j
scalar
• Let |Z{z} and Z(i, ) denote the i-th row of Z.
i,∗
∗
d×1
• Let |Z{z} and Z( ,j) denote the j-th column of Z.
∗,j
∗
n×1
Then,|w{ze}define the exponential matrix in the attention mechanism.
Definition C.5 (Exponential function u). If we have the below conditions,
• Let X Rn×d
∈
• Let W := W W⊤ Rd×d
Q K ∈
We define u(X) Rn×n as follows
∈
u(X) := exp(XWX⊤)
Then, we introduce the summation vector of the aforementioned exponential matrix.
Definition C.6 (Sum function of softmax α). If we have the below conditions,
• Let X Rn×d
∈
• Let u(X) be defined as Definition C.5
We define α(X) Rn as follows
∈
α(X) := u(X) 1
n
·
Then, with the help of the summation vector, we are ready to normalize the exponential matrix
and get the softmax probability matrix.
Definition C.7 (Softmax probability function f). If we have the below conditions,
• Let X Rn×d
∈
21• Let u(X) Rn×n be defined as Definition C.5
∈
• Let α(X) Rn be defined as Definition C.6
∈
We define f(X) Rn×n as follows
∈
f(X) := diag(α(X))−1u(X)
where we define f(X)⊤ Rn is the j -th row of f(X).
j0 ∈ 0
Besides the probability matrix introduced above, we introducethe value matrix in thefollowing
definition.
Definition C.8 (Value function h). If we have the below conditions,
• Let X Rn×d
∈
• Let W Rd×d
V
∈
We define h(X) Rn×d as follows
∈
h(X) = XW
V
Then, we introduce s(X) to represent the output of the attention mechanism.
Definition C.9 (Self-attention output s). If we have the below conditions,
• Let f(X) be defined as Definition C.7
• Let h(X) be defined as Definition C.8
We define s(X) Rn×d as follows
∈
s(X) = f(X)h(X)
Then, we introduce q(X) and p(X) to facilitate the calculation of the gradient on W.
Definition C.10 (Definition of q(X)). If we have the below conditions,
• Let h(X) Rn×d be defined as in Definition C.8.
∈
• Let G Rn×d denote the gradient matrix resulting from the application of the chain rule up
i
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
• For i [n],j [d], let G (i ,j ) denote the (i ,j )-th entry of G .
2 2 i 2 2 2 2 i
∈ ∈
We define q(X) Rn×n as
∈
q(X) = G h(X)⊤.
i
n×d d×n
where we define q(X)⊤ Rn is the j -th row of|{qz(}X|).{z }
j0 ∈ 0
Definition C.11 (Definition of p(X), Definition C.5 in [AS24a]). For every index j [n], we
0
∈
define p(X) Rn as
j0
∈
p(X) := (diag(f(X) ) f(X) f(X)⊤)q(X)
j0 j0
−
j0 j0 j0
where we have p(X) Rn×n and we define p(X)⊤ Rn is the j -th row of p(X).
∈ j0 ∈ 0
Furthermore, we define p (X) = f(X) q(X) and p (X) = diag(p (X) 1 )f(X). Additionally,
1 2 1 n
⊙ ·
we can calculate p(X) as
p(X) = p (X) p (X)
1 2
−
22C.4 Low rank representations
Using [AS23]’s polynomial method techniques, we can obtain the following low-rank representation
result.
Lemma C.12 (Low rank representation to f, Section 3 of [AS23], Lemma D.1 of [AS24a]). For
any A = o(√logn), there exists a k = no(1) such that: Let X Rn×d and W Rd×d be a square
1
∈ ∈
matrix. It holds that XW R, X R, then there are two matrices U ,V Rn×k1 such
∞ ∞ 1 1
k k ≤ k k ≤ ∈
that U V⊤ f(X) ǫ/poly(n). Here f(X) = D−1exp(XWX⊤) (see also Definition C.7) and
k 1 1 − k∞ ≤
we define D = diag(exp(XWX⊤)1 ) (see also Definition C.6). Moreover, these matrices U ,V
n 1 1
can be explicitly constructed in n1+o(1) time.
A similar technique can be applied to s(X).
Lemma C.13 (Low rank representation to s). Let d = O(logn). Assume that each number in the
n d matrices h(X) Rn×d can be written using O(logn) bits. Let n d matrix s(X) Rn×d
× ∈ × ∈
be defined as Definition C.9. Then, there are two matrices U ,V Rn×k1 we have U V⊤h(X)
1 1 ∈ k 1 1 −
s(X) ǫ/poly(n).
∞
k ≤
Proof. We can show that
U V⊤h(X) s(X) = U V⊤h(X) f(X)h(X)
k 1 1 − k∞ k 1 1 − k∞
= (U V⊤ f(X))h(X)
k 1 1 − k∞
n×n n×n n×d
n U V⊤ f(X) h(X)
≤
k| {1z 1} −|{z} | k∞{z k} k∞
n×n n×n n×d
n U V⊤ f(X) poly(n)
≤
k|1{z1} −|{z} k∞ ·|{z}
n×n n×n
ǫ/poly(n)
| {z } |{z}
≤
where the 1st step is from the choice of s(X), the 2nd step comes from AC BC = (A B)C
− −
holds for any matrices A, B, and C, the 3rd step is because of basic linear algebra, the 4th step
is due to each number in h(X) can be written using O(log(n)) bits, the fifth step follows from
U V⊤ f(X) ǫ/poly(n).
k 1 1 − k∞ ≤
We can also get a low-rank representation of p (x) and p (x).
1 2
Lemma C.14 (Low rank representation to p (X), Lemma D.4 of [AS24a]). Let k = no(1). Let
1 1
k = no(1). Assume that p (X) := f(X) q(X). Assume U ,V Rn×k1 approximates the f(X)
2 1 1 1
such that U V⊤ f(X) ǫ/poly(n).⊙ Assume U ,V Rn×k2∈ approximates the q(X) Rn×n
k 1 1 − k∞ ≤ 2 2 ∈ ∈
such that U V⊤ q(X) ǫ/poly(n). Then there are matrices U ,V Rn×k3 such that
k 2 2 − k∞ ≤ 3 3 ∈
U V⊤ p (X) ǫ/poly(n). The matrices U ,V can be explicitly constructed in n1+o(1) time.
k 3 3 − 1 k∞ ≤ 3 3
Lemma C.15 (Low rank representation p (X), Lemma D.5 of [AS24a]). Let k = no(1). Let k =
2 1 2
no(1). Letk = no(1). Assumethatp (X)isann nwhere j -throwp (X) = f(X) f(X)⊤q(X)
4 2 × 0 2 j0 j0 j0 j0
for each j [n]. Assume U ,V Rn×k1 approximates the f(X) such that U V⊤ f(X)
0 ∈ 1 1 ∈ k 1 1 − k∞ ≤
ǫ/poly(n). Assume U ,V Rn×k2 approximates the q(X) Rn×n such that U V⊤ q(X)
2 2 ∈ ∈ k 2 2 − k∞ ≤
ǫ/poly(n). Then there are matrices U ,V Rn×k4 such that U V⊤ p (X) ǫ/poly(n). The
4 4 ∈ k 4 4 − 2 k∞ ≤
matrices U ,V can be explicitly constructed in n1+o(1) time.
4 4
23C.5 Bounded entries of matrices
In this section, we provide proof that entries of matrices are bounded.
We begin with the exponential matrix f(X).
Lemma C.16 (Bounded entries of f(X)). If we have the below conditions,
• Let f(X) Rn×n be defined in Definition C.7.
∈
Then, we can show that
f(X) 1
∞
k k ≤
Proof. By Definition C.7, we have
f(X)= diag(α(X))−1u(X)
By Definition C.6, we have
α(X) = u(X)1
n
Combining above two equations, we have
f(X) 1
∞
k k ≤
A similar analysis can be applied to h(X) and s(X) as well.
Lemma C.17 (Bounded entries of h(X)). If we have the below conditions,
• Let X Rn×d,W,W Rd×d be defined in Definition 1.2.
V
∈ ∈
• Assuming each entry of X,W,W can be re represented using O(log(n)) bits.
V
• Let h(X) Rn×d be defined in Definition C.8.
∈
Then, we can show that
h(X) poly(n)
∞
k k ≤
Proof. By Definition C.8, we have
h(X) := XW
V
Then, we have
h(X) = XW
∞ V ∞
k k k k
n X W
∞ V ∞
≤ k k k k
poly(n)
≤
where the 1st step is from the definition of h(X), the 2nd step comes from basic linear algebra, the
3rd step is because of each entry in X and W can be represented by O(log(n)) bits.
V
Lemma C.18 (Bounded entries of s(X)). If we have the below conditions,
24• Let X Rn×d,W,W Rd×d be defined in Definition 1.2.
V
∈ ∈
• Assuming each entry of X,W,W can be re represented using O(log(n)) bits.
V
• Let s(X) Rn×d be defined in Definition C.9.
∈
Then, we can show that
s(X) poly(n)
∞
k k ≤
Proof. By Definition C.9, we have
s(X) = f(X)h(X)
n×d n×n n×d
Then, we have |{z} |{z}|{z}
s(X) = f(X)h(X)
∞ ∞
k k k k
n f(X) h(X)
∞ ∞
≤ k k k k
poly(n)
≤
where the 1st step is from the definition of c(X), the 2nd step comes from basic linear algebra, the
3rd step is because of Lemma C.16, C.17.
D Matrix View
dL(X)
In this section, we dive into analyzing the gradient of .
dTi−1(X)
In Section D.1, we give the gradient of s(X) with respective to X. In Section D.2, we show
the close form of the gradient on T (X) via the chain rule. In Section D.3, we integrate each
i
C (X) to its corresponding matrix term B (X). In Section D.4, applying the similar technique
i i
used in the previous section, we integrate the gradient on T (X) into its corresponding matrix
i
view. In Section D.5, we further apply matrix integration on each matrix term in the gradient on
T (X) calculated in the previous section. In Section D.6, we give the matrix view of all gradient
i
components.
D.1 Gradient of s(X)
In this section, we give the gradient of s(X) with respective to X.
The results from [DSXY23] give the gradient of c(X). By chain rule, the gradient of s(X) is
equivalent to the gradient of c(X) from [DSXY23], since c(X) = s(X) B where B is a constant
−
matrix.
Lemma D.1 (Gradient of s(X) , Lemma B.16 in [DSXY23]). If we have the below conditions,
i0,j0
• Let s(X) Rn×d be defined as Definition C.9
∈
Then, we have
• Part 1. For all i = i [n], j ,j [d],
0 1 0 1
∈ ∈
ds(X)
i0,j0
= C (X)+C (X)+C (X)+C (X)+C (X)
1 2 3 4 5
dX
i1,j1
where we have definitions:
25– C (X) := s(X) f(X) W ,X
1
−
i0,j0
·
i0,i0
·h
j1,∗ i0,∗
i
– C (X) := s(X) f(X) ,XW
2
−
i0,j0
·h
i0,∗ ∗,j1i
– C (X) := f(X) h(X) W ,X
3 i0,i0
·
i0,j0
·h
j1,∗ i0,∗
i
– C (X) := f(X) (XW ),h(X)
4
h
i0,∗
⊙
∗,j1 ∗,j0i
– C (X) := f(X) (W )
5 i0,i0
·
V j1,j0
• Part 2. For all i = i [n], j ,j [d],
0 1 0 1
6 ∈ ∈
ds(X)
i0,j0
= C (X)+C (X)+C (X)
6 7 8
dX
i1,j1
where we have definitions:
– C (X) := s(X) f(X) W ,X
6
−
i0,j0
·
i1,i0
·h
j1,∗ i0,∗
i
∗ This is corresponding to C (X)
1
– C (X) := f(X) h(X) W ,X
7 i1,i0
·
i1,j0
·h
j1,∗ i0,∗
i
∗ This is corresponding to C (X)
3
– C (X) := f(X) (W )
8 i1,i0
·
V j1,j0
∗ This is corresponding to C (X)
5
D.2 Gradient on T (X)
i
In the Lemma D.2, we use the chain rule to calculate the close form of the gradient on T (X).
i
Lemma D.2 (Gradient for T (X)). If we have the below conditions,
i
• Let Attn be defined as Definition 1.2.
i
• Let T (X) Rn×d be defined as Definition 3.3.
i
∈
• Let s(X) be defined as Definition C.9.
• Let G Rn×d denote the gradient matrix resulting from the application of the chain rule up
i
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
• For i [n],j [d], let G (i ,j ) denote the (i ,j )-th entry of G .
2 2 i 2 2 2 2 i
∈ ∈
Then, we can show that, for i [n], j [d], we have
1 1
∈ ∈
n d
dL(X) ds(X)
= G (i ,j )
i0,j0
i 0 0
dT (X) · dX
i−1 i1,j1
i X0=1j X0=1
i1,j1
Proof. By Lemma C.3, we have
dL(X) n d dAttn (T (X))
= G (i ,j )
i i−1 i2,j2.
i 2 2
dT (X) · dT (X)
i−1 i−1
i X2=1j X2=1
By Definition 1.2 and Definition C.9, we have
Attn (T (X)) = s(T (X))
i i−1 i−1
26Therefore, by combining above two equations and substituting variable T (X) = X, we have
i−1
n d
dL(X) ds(X)
= G (i ,j )
i0,j0
i 0 0
dT (X) · dX
i−1 i1,j1
i X0=1j X0=1
i1,j1
D.3 Matrix view of C(X)
Inthis section, we willprovide thematrix view of C (X) R, for i 6,7,8,2,4 . We will consider
i
∈ ∈ { }
each C (X) one by one. We begin with C (X).
i 6
Lemma D.3 (Matrix view of C (X)). If we have the below conditions,
6
• Let C (X,i ,j ):= s(X) f(X) W ,X be defined as in Lemma D.1.
6 1 1
−
i0,j0
·
i1,i0
·h
j1,∗ i0,∗
i
• We define amatrix B (X) Rn×d. For alli [n],j [d], letB (i ,j )denote the (i ,j )-th
6 1 1 6 1 1 1 1
∈ ∈ ∈
entry of B (X). We define B (i ,j ) = C (X,i ,j ).
6 6 1 1 6 1 1
Then, we can show that
B (X) = s(X) f(X) (W X )⊤
6
−
i0,j0 ∗,i0
·
i0,∗
n×d 1×1 n×1 1×d
Proof. We have | {z } | {z }| {z }| {z }
C (X,i ,j )= s(X) f(X) W ,X
6 1 1
−
i0,j0
·
i1,i0
·h
j1,∗ i0,∗
i
= s(X) f(X) X⊤ W
−
i0,j0
·
i1,i0
·
i0,∗ j1,∗
where the 1st step is from the choice of C (X), the 2nd step comes from a,b =a⊤b holds for any
6
h i
a,b Rd.
∈
We have
B (X)(i , ) = s(X) f(X) W X
6 1
∗ −
i0,j0 i1,i0 i0,∗
d×1 1×1 1×1 d×d d×1
Then, we have | {z } | {z }| {z }|{z} |{z}
B (X) = s(X) f(X) (W X )⊤
6
−
i0,j0 ∗,i0
·
i0,∗
n×d 1×1 n×1 1×d
| {z } | {z }| {z }| {z }
A similar analysis procedure can also be applied on C (X).
7
Lemma D.4 (Matrix view of C (X)). If we have the below conditions,
7
• Let C (X,i ,j ):= f(X) h(X) W ,X be defined as in Lemma D.1.
7 1 1 i1,i0
·
j0,i1
·h
j1,∗ i0,∗
i
• We define amatrix B (X) Rn×d. For alli [n],j [d], letB (i ,j )denote the (i ,j )-th
7 1 1 7 1 1 1 1
∈ ∈ ∈
entry of B (X). We define B (i ,j ) = C (X,i ,j ).
7 7 1 1 7 1 1
27Then, we can show that
B (X) = (f(X) h(X) ) (W X )⊤
7 ∗,i0
⊙
∗,j0
· ·
i0,∗
n×d n×1 1×d
Proof. We have | {z } | {z } | {z }
C (X,i ,j )= f(X) h(X) W ,X
7 1 1 i1,i0
·
i1,j0
·h
j1,∗ i0,∗
i
= f(X) h(X) W⊤ X
i1,i0
·
i1,j0
·
j1,∗ i0,∗
where the 1st step is from the choice of C (X), the 2nd step comes from a,b =a⊤b holds for any
7
h i
a,b Rd.
∈
We have
B (X)(i , ) = f(X) h(X) W X
7 1
∗
i1,i0
·
i1,j0
· ·
i0,∗
Then, we have
B (X) = (f(X) h(X) ) (W X )⊤
7 ∗,i0
⊙
∗,j0
· ·
i0,∗
n×d n×1 1×d
| {z } | {z } | {z }
Then, we provide an analysis of C (X).
8
Lemma D.5 (Matrix view of C (X)). If we have the below conditions,
8
• Let C (X,i ,j ):= f(X) (W ) be defined as in Lemma D.1.
8 1 1 i1,i0
·
V j1,j0
• We define amatrix B (X) Rn×d. For alli [n],j [d], letB (i ,j )denote the (i ,j )-th
8 1 1 8 1 1 1 1
∈ ∈ ∈
entry of B (X). We define B (i ,j ) = C (X,i ,j ).
8 8 1 1 8 1 1
Then, we can show that
B (X) = f(X) (W )⊤
8 ∗,i0 V ∗,j0
n×d n×1 1×d
Proof. We have | {z } | {z }| {z }
C (X,i ,j ) = f(X) (W )
8 1 1 i1,i0
·
V j1,j0
where the 1st step is from the choice of C (X).
7
We have
B (X)(i , ) = f(X) (W )
8 1
∗
i1,i0
·
V ∗,j0
Then, we have
B (X) = f(X) (W )⊤
8 ∗,i0 V ∗,j0
n×d n×1 1×d
| {z } | {z }| {z }
28Now, we consider C (X).
2
Lemma D.6 (Matrix view of C (X)). If we have the below conditions,
2
• Let C (X,j ):= s(X) f(X) ,XW be defined as in Lemma D.1.
2 1
−
i0,j0
·h
i0,∗ ∗,j1i
• We define a matrix B (X) Rd. For all j [d], the j -th entry of B (X) is defined as
2 1 1 2
∈ ∈
C (X,j ).
2 1
Then, we can show that
B (X) = s(X) W⊤ X⊤ f(X)
2
−
i0,j0 i0,∗
d×1 1×1 d×d d×n n×1
Proof. We have | {z } | {z }|{z}|{z} | {z }
C (X,j )= s(X) f(X) ,XW
2 1
−
i0,j0
·h
i0,∗ ∗,j1i
= s(X) (XW )⊤f(X)
−
i0,j0
·
∗,j1 i0,∗
= s(X) W⊤ X⊤ f(X)
−
i0,j0 ∗,j1 i0,∗
1×1 1×d d×n n×1
|{z}
where the 1st step is from the choice of| C ({ Xz ), t} h| e{ sz ec} ond s| tep{ fz ollo} ws from a,b = a⊤b, for any
2
h i
a,b Rn.
∈
Then, we have
B (X) = s(X) W⊤ X⊤ f(X)
2
−
i0,j0 i0,∗
d×1 1×1 d×d d×n n×1
| {z } | {z }|{z}|{z} | {z }
Finally, we analyze C (X), which is the last term we need to compute.
4
Lemma D.7 (Matrix view of C (X)). If we have the below conditions,
4
• Let C (X,j ):= f(X) (XW ),h(X) be defined as in Lemma D.1.
4 1
h
i0,∗
⊙
∗,j1 ∗,j0i
• We define a matrix B (X) Rd. For all j [d], the j -th entry of B (X) is defined as
4 1 1 4
∈ ∈
C (X,j ).
4 1
Then, we can show that
B (X) = W⊤ X⊤ (f(X) h(X) )
4 i0,∗
⊙
∗,j0
d×1 d×d d×n n×1
Proof. We have | {z } |{z}|{z} | {z }
C (X,j ) = f(X) (XW ),h(X)
4 1
h
i0,∗
⊙
∗,j1 ∗,j0i
= f(X) h(X) ,(XW )
h
i0,∗
⊙
∗,j0 ∗,j1
i
= (XW )⊤(f(X) h(X) )
∗,j1 i0,∗
⊙
∗,j0
where the 1st step is from the choice of C (X), the 2nd step comes from Fact C.1, and the last
4
step follows from basic linear algebra.
29D.4 Matrix view of gradient on T (X)
i
Since we have got the matrix view of each C (X) term in the previous section, we can get the
i
matrix view of the gradient on T (X) in Lemma D.8.
i
Lemma D.8 (Matrix view of single entry of gradient). If we have the below conditions,
• Let s(X) be defined as Definition C.9.
• Let G Rn×d denote the gradient matrix resulting from the application of the chain rule up
i
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
• For i [n],j [d], let G (i ,j ) denote the (i ,j )-th entry of G .
2 2 i 2 2 2 2 i
∈ ∈
• Let B (X),B (X),B (X) Rn×d be defined in Lemma D.3, Lemma D.4, and Lemma D.5
6 7 8
∈
• Let B (X),B (X) Rd be defined in Lemma D.6 and Lemma D.7.
2 4
∈
For any i [n],j [d], we have
0 0
∈ ∈
ds(X)
G (i ,j ) i0,j0 = G (i ,j ) (B (X)+B (X)+B (X)+ e (B (X)+B (X))⊤)
i 0 0
· dX
i 0 0
·
6 7 8 i0 2 4
1×1 n×d n×1 1×d
Proof. By Lemma D.1, we ha|ve {z } | {z } |{z}| {z }
• Part 1. For all i = i [n], j ,j [d],
0 1 0 1
∈ ∈
ds(X)
i0,j0
= C (X)+C (X)+C (X)+C (X)+C (X) (1)
1 2 3 4 5
dX
i1,j1
• Part 2. For all i = i [n], j ,j [d],
0 1 0 1
6 ∈ ∈
ds(X)
i0,j0
= C (X)+C (X)+C (X) (2)
6 7 8
dX
i1,j1
Since for any i [n],j [d], let G (i ,j )
ds(X)i0,j0
denote the (i ,j )-th entry of G (i ,j )
1 ∈ 1 ∈ i 0 0 · dXi1,j1 1 1 i 0 0 ·
ds(X)i0,j0,
we consider the following two cases:
dX
• Case 1. The i -th row of G (i ,j ) ds(X)i0,j0.
0 i 0 0 · dX
• Case 2. The other n 1 rows of G (i ,j ) ds(X)i0,j0 where i = i .
− i 0 0 · dX 1 6 0
We first consider Case 1.
Recall that the matrix view of C (X),C (X) R are B (X),B (X) Rd, and the matrix view
2 4 2 4
∈ ∈
of C (X),C (X),C (X) R are B (X),B (X),B (X) Rn×d, respectively.
6 7 8 6 7 8
∈ ∈
For k 6,7,8 , we use B (X)(s, ) Rd to denote the s-th row of B (X).
k k
∈ { } ∗ ∈
We use (G (i ,j ) ds(X)i0,j0)(i , ) Rd to denote the i -th row of G (i ,j ) ds(X)i0,j0.
i 0 0 · dX 0 ∗ ∈ 0 i 0 0 · dX
SinceC (X),C (X),C (X)arethecorrespondingpartsofC (X),C (X),C (X),andbyEq.(1),
6 7 8 1 3 5
then we can have the following
ds(X)
(G (i ,j )
i0,j0)(i
, ) = G (i ,j ) (B (X)(i , )+B (X)(i , )+B (X)(i , )+B (X)+B (X))
i 0 0 0 i 0 0 6 0 7 0 8 0 2 4
· dX ∗ · ∗ ∗ ∗
1×1 d×1
| {z } | {z }
30We then consider Case 2.
For k 6,7,8 , we use B (X)(= s, ) R(n−1)×d to denote the matrix B (X) with the s-th
k k
∈ { } 6 ∗ ∈
row removed.
Similarly, we use (G (i ,j ) ds(X)i0,j0)(= i , ) R(n−1)×d to denote the matrix G (i ,j )
i 0 0 · dX 6 0 ∗ ∈ i 0 0 ·
ds(X)i0,j0
with the i -th row removed.
dX 0
By Eq. (2), we have
ds(X)
(G (i ,j )
i0,j0)(=
i , ) = G (i ,j ) (B (X)(= i , )+B (X)(= i , )+B (X)(= i , ))
i 0 0 0 i 0 0 6 0 7 0 8 0
· dX 6 ∗ · 6 ∗ 6 ∗ 6 ∗
1×1 d×(n−1)
| {z } | {z }
Combining Case 1 and Case 2 together, we have
ds(X)
G (i ,j ) i0,j0 = G (i ,j ) (B (X)+B (X)+B (X)+ e (B (X)+B (X))⊤)
i 0 0
· dX
i 0 0
·
6 7 8 i0 2 4
1×1 n×d n×1 1×d
| {z } | {z } |{z}| {z }
Then, we have the matrix view of T (X) gradient.
i
Lemma D.9 (Matrix view of T (X) gradient). If we have the below conditions,
i
• Let L(X) be defined as Definition 3.1.
• Let T(X) be defined as Definition 3.3.
• Let G Rn×d denote the gradient matrix resulting from the application of the chain rule up
i
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
• For i [n],j [d], let G (i ,j ) denote the (i ,j )-th entry of G .
2 2 i 2 2 2 2 i
∈ ∈
• Let B (X),B (X),B (X) Rn×d be defined in Lemma D.3, Lemma D.4, and Lemma D.5
6 7 8
∈
• Let B (X),B (X) Rd be defined in Lemma D.6 and Lemma D.7.
2 4
∈
Then, we have
n d
dL(X)
= G (i ,j ) (B (X)+B (X)+B (X)+ e (B (X)+B (X))⊤)
dT (X)
i 0 0
·
6 7 8 i0 2 4
i−1
i X0=1j X0=1
1×1 n×d n×1 1×d
Proof. By Lemma D.8, we h|ave{z } | {z } |{z}| {z }
ds(X)
G (i ,j ) i0,j0 = G (i ,j ) (B (X)+B (X)+B (X)+ e (B (X)+B (X))⊤)
i 0 0
· dX
i 0 0
·
6 7 8 i0 2 4
1×1 n×d n×1 1×d
Then, by Lemma C.3 we h|ave{z } | {z } |{z}| {z }
dL(X) n d dAttn (T (X))
= G (i ,j )
i i−1 i2,j2.
i 2 2
dT (X) · dT (X)
i−1 i−1
i X2=1j X2=1
After combining the above two equations, we are done.
31D.5 Matrix view of each term in gradient on T (X)
i
Inthissubsection, wereducethedoublesummation toamatrix productforeasy andclear analysis.
We first work on the B term.
6
Lemma D.10 (Matrix view of B (X) term). If we have the below conditions,
6
• Let B (X) = s(X) f(X) (W X )⊤ be defined in Lemma D.3.
6
−
i0,j0 ∗,i0
·
i0,∗
n×d 1×1 n×1 1×d
• We|de{fizne}z (|X) {zRn×}n|, w{hzich}s|atisfi{ezs }
6
∈
z (X) = (G (i , )⊤s(X) )f(X)
6 ∗,i0 i 0
∗
i0,∗ ∗,i0
n×1 1×d d×1 n×1
| {z } | {z }| {z } | {z }
• Let f(X) Rn×n be defined in Definition C.7.
∈
• Let W Rd×d be defined in Definition 1.2.
∈
• Let G Rn×d denote the gradient matrix resulting from the application of the chain rule up
i
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
• For i [n],j [d], let G (i ,j ) denote the (i ,j )-th entry of G .
2 2 i 2 2 2 2 i
∈ ∈
Then we have
n d
G (i ,j )B (X) = z (X) X W⊤
i 0 0 6 6
−
i X0=1j X0=1 1×1 n×d n×n n×d d×d
Proof. | {z }| {z } | {z }|{z}|{z}
n d n d
G (i ,j )B (X) = G (i ,j )s(X) f(X) (W X )⊤
i 0 0 6
−
i 0 0 i0,j0 ∗,i0
·
i0,∗
i X0=1j X0=1 i X0=1j X0=1
1×1 1×1 n×1 1×d
n d
| {z }| {z }| {z }| {z }
= ( G (i ,j )s(X) )f(X) (W X )⊤
−
i 0 0 i0,j0 ∗,i0
·
i0,∗
i X0=1 j X0=1
1×1 1×1 n×1 1×d
n
= (G
(i|
,
){ ⊤z s(X}|
)
{z )f(} X|
)
{z (W}|
X
{ )z⊤ }
−
i 0
∗
i0,∗ ∗,i0
·
i0,∗
i X0=1
1×d d×1 n×1 1×d
n
= (G| (i{z , )⊤}| s(X{z ) } )f| (X{z ) } X| ⊤ { Wz ⊤ }
−
i 0
∗
i0,∗ ∗,i0 i0,∗
i X0=1 1×d d×1 n×1 1×d d×d
where the 1st step is from the choice of B
(X|
),
t{ hz
e
2n} d| ste{ pz co} m| es{ frz om}|ba{zsi}c| a{ lz g}
ebra, the 3rd step
6
is because of a⊤b = d a b holds for any a,b Rd, the 4th step is due to (AB)⊤ = B⊤A⊤ for
i=1 i · i ∈
any matrices A and B.
P
Recall that we have z (X) = (G (i , )⊤s(X) )f(X) .
6 ∗,i0 i 0
∗
i0,∗ ∗,i0
n×1 1×d d×1 n×1
| {z } | {z }| {z } | {z }
32Then, we have
n
(G (i , )⊤s(X) )f(X) X⊤ W⊤
−
i 0
∗
i0,∗ ∗,i0 i0,∗
i X0=1 1×d d×1 n×1 1×d d×d
=
n z| (X{ )z } X|
⊤
{z W⊤} | {z }|{z}|{z}
−
6 ∗,i0 i0,∗
i X0=1 n×1 1×d d×d
= z 6(X|) X{z W}⊤ |{z}|{z}
−
n×n n×d d×d
where the 1st step is from the
c|h{ozice}| o{ fz z}| ({ Xz}
), the 2nd step comes from basic linear algebra.
6
Then, we can get the matrix view of B (X) term.
7
Lemma D.11 (Matrix view of B (X) term). If we have the below conditions,
7
• Let B (X) = (f(X) h(X) ) (W X )⊤ be defined in Lemma D.4.
7 ∗,i0
⊙
∗,j0
· ·
i0,∗
n×d n×1 1×d
• We|de{fizne}z (|X) Rn×{nz, which s}ati|sfies{z }
7
∈
z (X) = f(X) (h(X)G (i , )).
7 ∗,i0 ∗,i0⊙ i 0
∗
n×1 n×1 n×d d×1
| {z } | {z } |{z}| {z }
• Let X Rn×d,W Rd×d be defined in Definition 1.2.
∈ ∈
• Let G Rn×d denote the gradient matrix resulting from the application of the chain rule up
i
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
• For i [n],j [d], let G (i ,j ) denote the (i ,j )-th entry of G .
2 2 i 2 2 2 2 i
∈ ∈
Then we have
n d
G (i ,j )B (X) = z (X) X W⊤
i 0 0 7 7
i X0=1j X0=1 1×1 n×d n×n n×d d×d
Proof. We have | {z }| {z } | {z }|{z}|{z}
n d n d
G (i ,j )B (X) = G (i ,j )(f(X) h(X) ) (W X )⊤
i 0 0 7 i 0 0 ∗,i0
⊙
∗,j0
· ·
i0,∗
i X0=1j X0=1
1×1 n×d
i X0=1j X0=1
1×1 n×1 1×d
n d
| {z }| {z } | {z }| {z } | {z }
= (f(X) ( G (i ,j )h(X) )) (W X )⊤
∗,i0⊙ i 0 0 ∗,j0
· ·
i0,∗
i X0=1
n×1
j X0=1
1×1 n×1 1×d
n
=
(f| (X{z
)
} (h(X)| G{ (z
i ,
} )|
))
{ (z X⊤} W|⊤) {z }
∗,i0⊙ i 0
∗ ·
i0,∗
i X0=1
n×1 n×d d×1 1×d
where the 1st step is from the choice of B|(X{)z, th}e 2n|d{zst}e|p c{ozme}s fro|m ba{szic al}gebra, the 3rd step
7
is because of basic linear algebra.
33Recall that we have z (X) = f(X) (h(X)G (i , )).
7 ∗,i0 ∗,i0⊙ i 0
∗
n×1 n×1 n×d d×1
Then we have
| {z } | {z } |{z}| {z }
n
(f(X) (h(X)G (i , ))) (X⊤ W⊤)
∗,i0⊙ i 0
∗ ·
i0,∗
i X0=1
n×1 n×d d×1 1×d
n
=
z| (X{z
)
} X⊤|{z W} ⊤| {z } | {z }
7 ∗,i0 i0,∗
i X0=1 n×1 1×d d×d
= z 7(X|) X{z W}⊤ |{z}|{z}
n×n n×d d×d
where the 1st step is from th|e {czho}ic| e{z o} f| z{z (} X), the 2nd step comes from basic linear algebra.
7
Then, we consider B (X).
8
Lemma D.12 (Matrix view of B (X) term). If we have the below conditions,
8
• Let B (X) = f(X) (W )⊤ be defined in Lemma D.5.
8 ∗,i0 V ∗,j0
n×d n×1 1×d
• Let | G{z R}n×d| de{ nz ote}|the{gzrad}ient matrix resulting from the application of the chain rule up
i
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
• For i [n],j [d], let G (i ,j ) denote the (i ,j )-th entry of G .
2 2 i 2 2 2 2 i
∈ ∈
Then we have
n d
G (i ,j )B (X) = f(X) G W⊤
i 0 0 8 i V
i X0=1j X0=1 1×1 n×d n×n n×d d×d
Proof. We have | {z }| {z } |{z}|{z}|{z}
n d n d
G (i ,j )B (X) = G (i ,j )f(X) (W )⊤
i 0 0 8 i 0 0 ∗,i0 V ∗,j0
i X0=1j X0=1
1×1 n×d
i X0=1j X0=1
1×1 n×1 1×d
n d
| {z }| {z } | {z }| {z }| {z }
= f(X) ( G (i ,j )(W )⊤ )
∗,i0 i 0 0 V ∗,j0
i X0=1
n×1
j X0=1
1×1 1×d
n
=
| f(X{z
)
}
G
(i|
,
)⊤{z W⊤}| {z }
∗,i0 i 0
∗
V
i X0=1
n×1 1×d d×d
= f(X) G W⊤
| {iz }V| {z }|{z}
n×n n×d d×d
where the 1st step is from the choice of B
(X|),{tzh}e| 2{ nz d}|s{tze}p
comes from basic algebra, the 3rd step
8
is because of basic linear algebra, the 4th step is due to basic linear algebra.
34Now, we can do the matrix view of B (X) term.
2
Lemma D.13 (Matrix view of B (X) term). If we have the below conditions,
2
• Let B (X) = s(X) W⊤ X⊤ f(X) be defined in Lemma D.6
2
−
i0,j0 i0,∗
d×1 1×1 d×d d×n n×1
• Let G| {z R}n×d|den{ozte t}h| e{ gz r} a| d{ iz e} n|t m{aztri}x resulting from the application of the chain rule up
i
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
• For i [n],j [d], let G (i ,j ) denote the (i ,j )-th entry of G .
2 2 i 2 2 2 2 i
∈ ∈
• We define z (X) Rn×n, which satisfies
2
∈
z (X) = (G (i , )⊤s(X) )f(X)
2 i0,∗ i 0
∗
i0,∗ i0,∗
n×1 1×d d×1 n×1
| {z } | {z }| {z } | {z }
• Let X Rn×d,W Rd×d be defined in Definition 1.2
∈ ∈
Then we have
n d
G (i ,j ) e B (X)⊤ = z (X) X W
i 0 0 i0 2
−
2
i X0=1j X0=1 1×1 n×1 1×d n×n n×d d×d
Proof. We have | {z }|{z}| {z } | {z }|{z}|{z}
n d n d
G (i ,j ) e B (X)⊤ = G (i ,j )s(X) e f(X)⊤ X W
i 0 0 i0 2
−
i 0 0 i0,j0 i0 i0,∗
i X0=1j X0=1 1×1 n×1 1×d i X0=1j X0=1 1×1 1×1 n×1 1×n n×d d×d
| {z }|{z}| {z } n d | {z }| {z }|{z}| {z }|{z}|{z}
= ( G (i ,j )s(X) ) e f(X)⊤ X W
−
i 0 0 i0,j0 i0 i0,∗
i X0=1 j X0=1 1×1 1×1 n×1 1×n n×d d×d
n
= (G (i| , ){ ⊤z s(X}| ) {z ) e} | f{ (z X}| )⊤ {z X}|{ Wz}|{z}
−
i 0
∗
i0,∗ i0 i0,∗
i X0=1 1×d d×1 n×1 1×n n×d d×d
=
n |
e
({ Gz (i} ,| )⊤{z s(X} )|{z} )f| (X{z )⊤}| X{z}| W{z}
−
i0 i 0
∗
i0,∗ i0,∗
i X0=1 n×1 1×d d×1 1×n n×d d×d
where the 1st step is from the choice of B
(X| ){ ,z} th|
e
2n{ dz st} ep| c{ oz me}
s
f|rom{zba}s| ic{z a} lg| e{ bz}
ra, the 3rd
2
step is because of a⊤b = d a b holds for any a,b Rd, the 4th step is due to (AB)⊤ = B⊤A⊤
i=1 i · i ∈
holds for any matrix A,B.
P
Recall that we have z (X) = (G (i , )⊤s(X) )f(X) .
2 i0,∗ i 0
∗
i0,∗ i0,∗
n×1 1×d d×1 n×1
Then, we have
| {z } | {z }| {z } | {z }
n
e (G (i , )⊤s(X) )f(X)⊤ X W
−
i0 i 0
∗
i0,∗ i0,∗
i X0=1 n×1 1×d d×1 1×n n×d d×d
|{z} | {z }| {z } | {z }|{z}|{z}
35n
= e z (X)⊤ X W
−
i0 2 i0,∗
i X0=1 n×1 1×n n×d d×d
= z 2(X|){z}X
|
W
{z
}|{z}|{z}
−
n×n n×d d×d
where the 1st step is from th|e c{hzo}ic| e{z o} f| z{z (} X), the 2nd step comes from basic linear algebra.
2
Finally, we do a similar analysis for the term B (X). Then, we get all the matrix views we
4
need.
Lemma D.14 (Matrix view of B (X) term). If we have the below conditions,
4
• Let B (X) = W⊤ X⊤ (f(X) h(X) ) be defined in Lemma D.7.
4 i0,∗
⊙
∗,j0
d×1 d×d d×n n×1
• Let |G{z R}n×d|{ dz e} n| o{ tz e} t|he gradie{nzt matrix r}esulting from the application of the chain rule up
i
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
• For i [n],j [d], let G (i ,j ) denote the (i ,j )-th entry of G .
2 2 i 2 2 2 2 i
∈ ∈
• We define z (X) Rn×n, which satisfies
4
∈
z (X) = f(X) (h(X)G (i , ))
4 i0,∗ i0,∗
⊙
i 0
∗
n×1 n×1 n×1
| {z } | {z } | {z }
Then we have
n d
G (i ,j ) e B (X)⊤ = z (X) X W
i 0 0 i0 4 4
i X0=1j X0=1 1×1 n×1 1×d n×n n×d d×d
Proof. We have | {z }|{z}| {z } | {z }|{z}|{z}
n d
G (i ,j ) e B (X)⊤
i 0 0 i0 4
i X0=1j X0=1
1×1 n×1 1×d
n d
| {z }|{z}| {z }
= G (i ,j ) e (f(X)⊤ h(X)⊤ ) X W
i 0 0 i0 i0,∗⊙ ∗,j0
i X0=1j X0=1 1×1 n×1 1×n n×d d×d
n | {z }|{z}d | {z }|{z}|{z}
= e (f(X)⊤ ( G (i ,j )h(X)⊤ )) X W
i0 i0,∗⊙ i 0 0 ∗,j0
i X0=1 n×1 1×n j X0=1 1×1 1×n n×d d×d
n
=
| e{z} (| f(X{z )⊤} (h(X| )G{ (z
i ,
} )| )⊤){z X} W|{z}|{z}
i0 i0,∗⊙ i 0
∗
i X0=1 n×1 1×n 1×n n×d d×d
n
=
| e{z} z| (X{z )⊤} X|
W
{z } |{z}|{z}
i0 4 i0,∗
i X0=1 n×1 1×n n×d d×d
= z 4(X|){z}X
|
W
{z
}|{z}|{z}
n×n n×d d×d
| {z }|{z}|{z}
36where the 1st step is from the choice of B (X), the 2nd step comes from basic algebra, the 3rd step
4
is because of basic linear algebra, the 4th step is due to the choice of z (X), the 5th step follows
4
from basic linear algebra.
D.6 Components of gradient on T (X)
i
Definition D.15 (Definition of D ). If we have the below conditions,
k
• For k 6,7,8 , let B (X) Rn×d be defined as Lemma D.3, D.4, and D.5, respectively.
1
∈ { }
k1
∈
• For k 2,4 , let B (X) Rd×1 be defined as Lemma D.6 and D.7, respectively.
2
∈ { }
k2
∈
• Let G Rn×d denote the gradient matrix resulting from the application of the chain rule up
i
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
We define D Rn×d as follows:
k
∈
• For k 6,7,8 , we define
1
∈ { }
n d
D := G (i ,j )B (X)
k1 i 0 0 k1
i X0=1j X0=1
1×1 n×d
| {z }| {z }
• For k 2,4 , we define
2
∈ { }
n d
D := G (i ,j ) e B (X)⊤
k2 i 0 0 i0 k2
i X0=1j X0=1
1×1 n×1 1×d
| {z }|{z}| {z }
Definition D.16 (Definition of K). If we have the below conditions,
• Let s(X) Rn×d be defined as Definition C.9.
∈
• Let G Rn×d denote the gradient matrix resulting from the application of the chain rule up
i
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
We define K Rn, where for each i [n], we define
0
∈ ∈
K = G (i , )⊤s(X)
i0 i 0
∗
i0,∗
1×1 1×d d×1
Furthermore, we have |{z} | {z }| {z }
K = (G s(X)) 1
i d
⊙
n×1 n×d d×1
Lemma D.17 (Close form of D ). If|{ wz e} hav|e the{zbelow}|c{ozn}ditions,
k
• Let X Rn×d,W Rd×d be defined as Definition 1.2.
∈ ∈
• For k 6,7,8,2,4 , let D Rn×d be defined as Definition D.15.
k
∈ { } ∈
• For k 6,7,2,4 , let z (X) Rn×n be defined as Lemma D.10, D.11, D.13, and D.14,
3
∈ { }
k3
∈
respectively.
37• Let K Rn be defined as Definition D.16.
∈
• We define z (X) Rn×n, which satisfies
6
∈
z (X) = f(X)diag(K).
6
n×n n×n n×n
| {z } |{z}| {z }
• We define z (X) Rn×n, which satisfies
7
∈
z (X) = f(X) (h(X) G⊤)
7 i
⊙
n×n n×n n×d d×n
| {z } |{z} |{z}|{z}
• We define z (X) Rn×n, which satisfies
2
∈
z (X) = diag(K)f(X)
2
n×n n×n n×n
| {z } | {z }|{z}
• We define z (X) Rn×n, which satisfies
4
∈
z (X) = f(X) ( G h(X)⊤)
4 i
⊙
n×n n×n n×d d×n
| {z } |{z} |{z}| {z }
Then, we can show that the close forms of D can be written as follows:
k
• D = z (X) X W⊤.
6 6
−
n×n n×d d×d
• D = z (|X{)z }X|{z W}|⊤{ .z}
7 7
n×n n×d d×d
• D = f|({Xz)}| G{z} W|{⊤z} .
8 i V
n×n n×d d×d
• D = |{zz}(X|{)z}X|{z}W .
2 2
−
n×n n×d d×d
• D = z (|X{)z }X|{z}W|{.z}
4 4
n×n n×d d×d
Proof. We fi|n{iszh}t|h{ez}p|ro{zo}f by parts.
• By Lemma D.10, we have the close form of D .
6
• By Lemma D.11, we have the close form of D .
7
• By Lemma D.12, we have the close form of D .
8
• By Lemma D.13, we have the close form of D .
2
• By Lemma D.14, we have the close form of D .
4
38E Fast Computation for gradient on T (X )
In this section, we give an almost linear time n1+o(1) algorithm for each B (X) term. Namely, we
i
consider B (X),B (X),B (X),B (X),B (X) in Section E.1, E.2, E.3, E.4, and E.5, respectively.
6 7 8 2 4
E.1 Fast computation for B (X) term
6
Before we introduce the almost linear time algorithm for B (X) term, we need to introduce the
6
accelerated algorithm for the key component term, z (X), in Lemma E.2.
6
We first compute K, which is defined in Definition D.16
Lemma E.1 (Computation time for K). If we have the below conditions,
• Let K Rn be defined as Definition D.16.
∈
Then, we can show that K can be computed in O(n d) time.
·
Proof. Since for each i [n], we have
0
∈
K = G (i , )⊤s(X)
i0 i 0
∗
i0,∗
1×1 1×d d×1
Then, we have that it takes O(d)| t{ iz m} e fo| r ca{ lz cula} t|ing{zeac}h entry.
Since there are total n entries in K, the overall computation time for K is O(n d).
·
We now compute z (X).
6
Lemma E.2 (Fast computation for z (X)). If we have the below conditions,
6
• Let X Rn×d,W,W Rd×d be defined in Definition 1.2.
V
∈ ∈
• Let G Rn×d denote the gradient matrix resulting from the application of the chain rule up
i
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
• Assuming each entry of X,W,W ,G can be re represented using O(log(n)) bits.
V i
• Let z (X) Rn×n be defined in Lemma D.10.
6
∈
Then, for some k = no(1), there are matrices U ,V Rn×k6 such that U V⊤ z (X)
6 6 6 ∈ k 6 6 − 6 k∞ ≤
ǫ/poly(n). The matrices U ,V can be constructed in n1+o(1) time.
6 6
Proof. Recall in Lemma D.10, we have define z (X) satisfying the following equation
6
z (X) = (G (i , )⊤s(X) )f(X) (3)
6 ∗,i0 i 0
∗
i0,∗ ∗,i0
n×1 1×d d×1 n×1
Recall that K Rn has b|een{zdefi}ned|in D{zefin}it|ion{zD.1}6.| B{yzLe}mma E.1, we have K can be
∈
computed in O(n d) time.
·
We also have
z (X) = f(X)diag(K)
6
n×n n×n n×n
| {z } |{z}| {z }
39By Lemma C.12, we have U ,V Rn×k1 such that
1 1
∈
U V⊤ f(X) ǫ/poly(n)
k 1 1 − k∞ ≤
Let U = U , V =diag(K)V .
6 1 6 1
We have V = diag(K) V can be computed in nk time.
6 1 1
n×n n×k1
The overall running time for constructing U and V is n1+o(1).
| {z }|{z} 6 6
Then, we consider the error bound.
We have
U V⊤ z (X) = U V⊤diag(K) f(X)diag(K)
k 6 6 − 6 k∞ k 1 1 − k∞
n U V⊤ f(X) diag(K)
≤ k 1 1 − k∞ k k∞
n(ǫ/poly(n)) diag(K)
∞
≤ k k
ǫ/poly(n)
≤
where the 1st step is from the choice of U , V , the 2nd step comes from basic linear algebra, the
6 6
3rd step is because of Lemma C.12, the 4th step is due to diag(K) poly(n).
∞
k k ≤
Then, we are ready to introduce the almost linear time algorithm for B (X) term.
6
Lemma E.3 (Fast computation for B (X) term). If we have the below conditions,
6
• Let X Rn×d,W,W Rd×d be defined in Definition 1.2.
V
∈ ∈
• Assuming each entry of X,W,W ,G can be re represented using O(log(n)) bits.
V i
• Let B (X) Rn×n be defined in Lemma D.3.
6
∈
• We define D Rn×d, where D := n d G (i ,j )B (X).
6 ∈ 6 i0=1 j0=1 i 0 0 6
• Let G Rn×d denote the gradient mPatrix rPesulting from the application of the chain rule up
i
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
• For i [n],j [d], let G (i ,j ) denote the (i ,j )-th entry of G .
2 2 i 2 2 2 2 i
∈ ∈
Then, we can show that, there is an algorithm to approximate D in n1+o(1) time, and it can
6
achieve ǫ/poly(n) accuracy.
Namely, the algorithm output D satisfying
6
D D ǫ/poly(n)
e 6 6 ∞
k − k ≤
Proof. Recall that in Lemma D.10, we have defined z (X) Rn×n, which satisfies
e 6
∈
z (X) = (G (i , )⊤s(X) )f(X)
6 ∗,i0 i 0
∗
i0,∗ ∗,i0
n×1 1×d d×1 n×1
And, in that Lemma, we |also{zhav}e | {z }| {z } | {z }
n d
G (i ,j )B (X) = z (X) X W⊤
i 0 0 6 6
−
i X0=1j X0=1 1×1 n×d n×n n×d d×d
| {z }| {z } | {z }|{z}|{z}
40Let U ,V Rn×k6 be defined as Lemma E.2.
6 6
∈
Let z (X) = U V⊤.
6 6 6
By Lemma E.2, we have
e
z (X) z (X) ǫ/poly(n) (4)
6 6 ∞
k − k ≤
Proof of running time.
e
We compute in the following way:
• Compute V⊤ X , which takes n1+o(1) time.
6
k6×n n×d
• Compute V|{⊤z} X|{ Wz} ⊤, which takes n1+o(1) time.
6
k6×d d×d
• Compute |U{z }V| ⊤{z X} W⊤, which takes n1+o(1) time.
6 6
n×k6 k6×d
Therefore, th| e{z o} v|eral{lzrun}ning time is n1+o(1).
Proof of error bound.
We have
z (X)XW⊤ z (X)XW⊤
6 6 ∞
k − k
d n z (X) z (X) X W
6 6 ∞ ∞ ∞
≤ · k − k k k k k
e
d n(ǫ/poly(n)) X W
∞ ∞
≤ · k k k k
e
ǫ/poly(n)
≤
where the 1st step is from basic linear algebra, the 2nd step comes from Eq.(4), the 3rd step is
because of W poly(n) and X poly(n).
∞ ∞
k k ≤ k k ≤
E.2 Fast computation for B (X) term
7
Similar to the analysis process of B (X) term, we first provide the almost linear time algorithm for
6
z (X), then provide that algorithm for B (X).
7 7
Lemma E.4 (Fast computation for z (X)). If we have the below conditions,
7
• Let z (X) Rn×n be defined in Lemma D.11.
7
∈
• By Lemma C.12, let U ,V be the low rank approximation of f(X), such that U V⊤
1 1 k 1 1 −
f(X) ǫ/poly(n).
∞
k ≤
• Let X Rn×d,W,W Rd×d be defined in Definition 1.2.
V
∈ ∈
• Assuming each entry of X,W,W ,G can be re represented using O(log(n)) bits.
V i
• Let G Rn×d denote the gradient matrix resulting from the application of the chain rule up
i
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
• For i [n],j [d], let G (i ,j ) denote the (i ,j )-th entry of G .
2 2 i 2 2 2 2 i
∈ ∈
41Then, for some k = no(1), there are matrices U ,V Rn×k7 such that U V⊤ z (X)
7 7 7 ∈ k 7 7 − 7 k∞ ≤
ǫ/poly(n). The matrices U ,V can be constructed in n1+o(1) time.
7 7
Proof. RecallthatinLemmaD.11,wehavedefinedz (X) Rn×n, wherethei -thcolumnofz (X)
7 0 7
∈
satisfies
z (X) = f(X) (h(X)G (i , ))
7 ∗,i0 ∗,i0⊙ i 0
∗
n×1 n×1 n×d d×1
which is equivalent to | {z } | {z } |{z}| {z }
z (X) = f(X) (h(X) G⊤)
7 i
⊙
n×n n×n n×d d×n
By Lemma C.12, we know
f(X)| :={z U} V⊤|{ iz
s
}
a
go| od{z a} p| p{ rz o}
ximation for f(X).
1 1
We choose U = U h(X) and V = V G , where U ,V Rn×k1d.
7 1 7 1 i 7 7
⊘ ⊘ ∈
Proof of running time. e
For U = U h(X), since U Rn×k1,h(X) Rn×d, constructing U takes O(ndk ) =
7 1 1 7 1
⊘ ∈ ∈
O(n1+o(1)) time.
Similarly, constructing V takes O(n1+o(1)) time.
7
Proof of error bound.
Using Fact C.2, we have
U V⊤ z (X) = U V⊤ f(X) (h(X)G⊤)
k 7 7 − 7 k∞ k 7 7 − ⊙ i k∞
= (U h(X))(V G )⊤ f(X) (h(X)G⊤)
1 1 i i ∞
k ⊘ ⊘ − ⊙ k
= (U V⊤) (h(X)G⊤) f(X) (h(X)G⊤)
k 1 1 ⊙ i − ⊙ i k∞
= f(X) (h(X)G⊤) f(X) (h(X)G⊤)
i i ∞
k ⊙ − ⊙ k
d h(X) G ǫ/poly(n)
∞ i ∞
≤ ke k k k ·
ǫ/poly(n) (5)
≤
where the 1st step is from the definition of z (X), the 2nd step comes from the choice of U and V ,
7 7 7
the3rdstepisbecauseofFact C.2, the4th stepisduetothedefinitionoff(X), the5th stepfollows
from f(X) f(X) ǫ/poly(n),thesixthstepfollows fromLemmaC.17and G poly(n).
∞ i ∞
k − k ≤ k k ≤
e
e
Then, we can do similarly fast computation for B term.
7
Lemma E.5 (Fast computation for B (X) term). If we have the below conditions,
7
• Let B (X) Rn×d be defined in Lemma D.4.
7
∈
• We define D Rn×d, where D := n d G (i ,j )B (X).
7 ∈ 7 i0=1 j0=1 i 0 0 7
• Let X Rn×d,W,W Rd×d,B RPn×d bePdefined in Definition 1.2.
V
∈ ∈ ∈
• Assuming each entry of X,W,W ,G can be re represented using O(log(n)) bits.
V i
• Let G Rn×d denote the gradient matrix resulting from the application of the chain rule up
i
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
• For i [n],j [d], let G (i ,j ) denote the (i ,j )-th entry of G .
2 2 i 2 2 2 2 i
∈ ∈
42Then, we can show that, there is an algorithm to approximate D in n1+o(1) time, and it can
7
achieve ǫ/poly(n) accuracy.
Namely, the algorithm output D satisfies
7
e D 7 D 7 ∞ ǫ/poly(n)
k − k ≤
Proof. In Lemma D.11, we have e
n d
G (i ,j )B (X) = z (X) X W⊤
i 0 0 7 7
i X0=1j X0=1 1×1 n×d n×n n×d d×d
Let U ,V Rn×k7 be defined in| Lem{z ma} E| .4{ .z } | {z }|{z}|{z}
7 7
∈
Let z (X) := U V⊤.
7 7 7
By Lemma E.4, we have
e
z (X) z (X) ǫ/poly(n) (6)
7 7 ∞
k − k ≤
Proof of running time.
e
We compute in the following way:
• Compute V⊤ X , which takes n1+o(1) time.
7
k7×n n×d
• Compute V|{⊤z} X|{ Wz} ⊤, which takes n1+o(1) time.
7
k7×d d×d
• Compute |U{z }V| ⊤{z X} W⊤, which takes n1+o(1) time.
7 7
n×k7 k7×d
Therefore, th| e{z o} v|eral{lzrun}ning time is n1+o(1).
Proof of error bound.
We have
z (X)XW⊤ z (X)XW⊤
7 7 ∞
k − k
d n z (X) z (X) X W
7 7 ∞ ∞ ∞
≤ · k − k k k k k
e
d n(ǫ/poly(n)) X W
∞ ∞
≤ · k k k k
e
ǫ/poly(n)
≤
where the 1st step is from basic linear algebra, the 2nd step comes from Eq. (6), the 3rd step is
because of W poly(n) and X poly(n).
∞ ∞
k k ≤ k k ≤
E.3 Fast computation for B (X) term
8
Then, we can do fast computations on B (X) term.
8
Lemma E.6 (Fast computation for B (X) term). If we have the below conditions,
8
• Let B (X) Rn×d be defined in Lemma D.5.
8
∈
43• We define D Rn×d, where D := n d G (i ,j )B (X).
8 ∈ 8 i0=1 j0=1 i 0 0 8
• Let X Rn×d,W,W Rd×d be defiPned inPDefinition 1.2.
V
∈ ∈
• Assuming each entry of X,W,W ,G can be re represented using O(log(n)) bits.
V i
• Let G Rn×d denote the gradient matrix resulting from the application of the chain rule up
i
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
• For i [n],j [d], let G (i ,j ) denote the (i ,j )-th entry of G .
2 2 i 2 2 2 2 i
∈ ∈
Then, we can show that, there is an algorithm to approximate D in n1+o(1) time, and it can
8
achieve ǫ/poly(n) accuracy.
Namely, the algorithm output D satisfies
8
D D ǫ/poly(n)
e 8 8 ∞
k − k ≤
Proof. Recall that in Lemma D.12, we have
e
n d
G (i ,j )B (X) = f(X) G W⊤
i 0 0 8 i V
i X0=1j X0=1 1×1 n×d n×n n×d d×d
Let f(X) := U V⊤ denote the ap| pro{ xz ima} ti| on{z of} f(X| ){ .z}|{z}|{z}
1 1
By Lemma C.12, we have
e
f(X) f(X) ǫ/poly(n) (7)
∞
k − k ≤
Proof of running time.
e
We compute in the following way:
• Compute V⊤ G , which takes n1+o(1) time.
1 i
k1×n n×d
• Compute V|{⊤z}G|{ Wz}⊤, which takes n1+o(1) time.
1 i V
k1×d d×d
• Compute |U{z V}|⊤{Gz}W⊤, which takes n1+o(1) time.
1 1 i V
n×k1 k1×d
Therefore, th|e{zo}v|eral{lzrun}ning time is n1+o(1).
Proof of error bound.
We have
f(X)G W⊤ f(X)G W⊤
i V i V ∞
k − k
d n f(X) f(X) G W
∞ i ∞ V ∞
≤ e· k − k k k k k
d n(ǫ/poly(n)) G W
i ∞ V ∞
≤ · e k k k k
ǫ/poly(n)
≤
where the 1st step is from basic linear algebra, the 2nd step comes from Eq.(7), the 3rd step is
because of G poly(n) and W poly(n).
i ∞ V ∞
k k ≤ k k ≤
44E.4 Fast computation for B (X) term
2
Then, we provide the proof of how to do fast computation on B (X).
2
Lemma E.7 (Fast computation for z (X)). If we have the below conditions,
2
• Let z (X) Rn×n be defined as in Lemma D.13.
2
∈
• Let X Rn×d,W,W Rd×d be defined in Definition 1.2.
V
∈ ∈
• Assuming each entry of X,W,W ,G can be re represented using O(log(n)) bits.
V i
• Let G Rn×d denote the gradient matrix resulting from the application of the chain rule up
i
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
• For i [n],j [d], let G (i ,j ) denote the (i ,j )-th entry of G .
2 2 i 2 2 2 2 i
∈ ∈
Then, for some k = no(1), there are matrices U ,V Rn×k9 such that U V⊤ z (X)
9 9 9 ∈ k 9 9 − 2 k∞ ≤
ǫ/poly(n). The matrices U ,V can be constructed in n1+o(1) time.
9 9
Proof. Recall that in Lemma D.13, we have defined z (X) Rn×n, where the i -th row of z (X)
2 0 2
∈
satisfies
z (X) = (G (i , )⊤s(X) )f(X)
2 i0,∗ i 0
∗
i0,∗ i0,∗
n×1 1×d d×1 n×1
Recall that K Rn has b|een{dzefi}ned i|n D{ezfinit}io|n D{z.16}. | {z }
∈
By Lemma E.1, we have K can be computed in O(n d) time.
·
We also have
z (X) = diag(K)f(X)
2
n×n n×n n×n
By Lemma C.12, let U ,V be t|he{zlo}w r|ank{zap}p|ro{xzim} ation of f(X), such that U V⊤
1 1 k 1 1 −
f(X) ǫ/poly(n).
∞
k ≤
Let U = diag(K)U , V = V .
9 1 6 1
We have U = diag(K) U can be computed in nk time.
9 1 1
n×n n×k1
The overall running time for constructing U and V is n1+o(1).
| {z }|{z} 9 9
Then, we consider the error bound.
We have
U V⊤ z (X) = diag(K)U V⊤ diag(K)f(X)
k 9 9 − 2 k∞ k 1 1 − k∞
n U V⊤ f(X) diag(K)
≤ k 1 1 − k∞ k k∞
n(ǫ/poly(n)) diag(K)
∞
≤ k k
ǫ/poly(n) (8)
≤
where the 1st step is from the choice of U , V , the 2nd step comes from basic linear algebra, the
6 6
3rd step is because of Lemma C.12, the 4th step is due to diag(K) poly(n).
∞
k k ≤
Lemma E.8 (Fast computation for B (X) term). If we have the below conditions,
2
45• Let B (X) Rn×d be defined in Lemma D.6.
2
∈
• We define D Rn×d, where D := n d G (i ,j ) e B (X)⊤.
2 ∈ 2 i0=1 j0=1 i 0 0 i0 2
P P 1×1 n×1 1×d
• Let X Rd×n,W,W Rd×d,B Rn×d be define|d in{zDefi}n|i{tzio}n| 1.{2z. }
V
∈ ∈ ∈
• Assuming each entry of X,W,W ,B,G can be re represented using O(log(n)) bits.
V i
• Let G Rn×d denote the gradient matrix resulting from the application of the chain rule up
i
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
• For i [n],j [d], let G (i ,j ) denote the (i ,j )-th entry of G .
2 2 i 2 2 2 2 i
∈ ∈
Then, we can show that, there is an algorithm to approximate D in n1+o(1) time, and it can
2
achieve ǫ/poly(n) accuracy.
Namely, the algorithm output D satisfies
2
D D ǫ/poly(n)
2 2 ∞
e k − k ≤
Proof. In Lemma D.13, we have
e
n d
G (i ,j ) e B (X)⊤ = z (X) X W
i 0 0 i0 2
−
2
i X0=1j X0=1 1×1 n×1 1×d n×n n×d d×d
Let U ,V Rn×k9 be defined| in{zLem}m|{az}E|.7.{z } | {z }|{z}|{z}
9 9
∈
Let z (X) := U V⊤.
2 9 9
By Lemma E.7, we have
e
z (X) z (X) ǫ/poly(n) (9)
2 2 ∞
k − k ≤
Proof of running time.
e
We compute in the following way:
• Compute V⊤ X , which takes n1+o(1) time.
9
k9×n n×d
• Compute V|{⊤z}X|{ Wz} , which takes n1+o(1) time.
9
k9×d d×d
• Compute |U{z }V|⊤{z X} W, which takes n1+o(1) time.
9 9
n×k9 k9×d
Therefore, th|e{zo}v|era{lzl ru}nning time is n1+o(1).
Proof of error bound.
We have
z (X)XW z (X)XW
2 2 ∞
k − k
d n z (X) z (X) X W
2 2 ∞ ∞ ∞
≤ · k − k k k k k
e
d n(ǫ/poly(n)) X W
∞ ∞
≤ · k k k k
e
ǫ/poly(n)
≤
where the 1st step is from basic linear algebra, the 2nd step comes from Eq.(9), the 3rd step is
because of W poly(n) and X poly(n).
∞ ∞
k k ≤ k k ≤
46E.5 Fast computation for B (X) term
4
Finally, our analysis shows that we can do fast computations for B (X) term. After that, we
4
showed that all terms can be computed quickly.
Lemma E.9 (Fast computation for z (X)). If we have the below conditions,
4
• Let z (X) Rn×n be defined in Lemma D.14.
4
∈
• Let X Rn×d,W,W Rd×d be defined in Definition 1.2.
V
∈ ∈
• Assuming each entry of X,W,W ,G can be re represented using O(log(n)) bits.
V i
• Let G Rn×d denote the gradient matrix resulting from the application of the chain rule up
i
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
• For i [n],j [d], let G (i ,j ) denote the (i ,j )-th entry of G .
2 2 i 2 2 2 2 i
∈ ∈
Then, for some k = no(1), there are matrices U ,V Rn×k10, let z (X) := U V⊤, such
10 10 10 ∈ 4 10 10
that z (X) z (X) ǫ/poly(n). The matrices U ,V can be constructed in n1+o(1) time.
4 4 ∞ 10 10
k − k ≤
Proof. In Lemma D.14, we have defined z (X) Rn×n, where the i -th colue mn of z (X) satisfies
4 0 4
e ∈
z (X) = (f(X) (h(X)G (i , )))
4 i0,∗ i0,∗
⊙
i 0
∗
n×1 n×1 n×1
which is equivalent to | {z } | {z } | {z }
z (X) = (f(X) G h(X)⊤)
4 i
⊙
n×n n×n n×d d×n
By Lemma C.12, let U ,V b| e{ tz h} e low|{ rz an} k a| p{z p} ro| xi{ mz a} tion of f(X), such that U V⊤
1 1 k 1 1 −
f(X) ǫ/poly(n).
∞
k ≤
We choose U = U G and V = V h(X), where U ,V Rn×k1d.
10 1 i 10 1 10 10
⊘ ⊘ ∈
Proof of running time.
For U = U G , since U Rn×k1,G Rn×d, constructing U takes O(ndk )= O(n1+o(1))
10 1 i 1 i 10 1
⊘ ∈ ∈
time.
Similarly, constructing V takes O(n1+o(1)) time.
10
Proof of error bound.
Let f(X) := U V⊤.
1 1
Using Fact C.2, we have
e
z (X) z (X)
4 4 ∞
k − k
= U V⊤ f(X) (G h(X)⊤)
k 10 10 − ⊙ i · k∞
e
= (U G )(V h(X))⊤ f(X) (G h(X)⊤)
1 i 1 i ∞
k ⊘ ⊘ − ⊙ · k
= (U V⊤) (G h(X)⊤) f(X) (G h(X)⊤)
k 1 1 ⊙ i · − ⊙ i · k∞
where the 1st step is from the definition of z (X),z (X), the 2nd step comes from the choice of
4 4
U and V , the 3rd step is because of Fact C.2.
10 10
e
(U V⊤) (G h(X)⊤) f(X) (G h(X)⊤)
k 1 1 ⊙ i · − ⊙ i · k∞
47= U V⊤ f(X) G h(X)⊤
k 1 1 − k∞ k i · k∞
d (ǫ/poly(n)) h(X) G
∞ i ∞
≤ · k k k k
ǫ/poly(n)
≤
where the 1st step is from basic linear algebra, the 2nd step comes from U V f(X)
1 1 ∞
k − k ≤
ǫ/poly(n), the 3rd step is because of Lemma C.17 and G poly(n).
i ∞
k k ≤
Lemma E.10 (Fast computation for B (X) term). If we have the below conditions,
4
• Let B (X) Rn×d be defined in Lemma D.7.
4
∈
• We define D Rn×d, where D := n d G (i ,j ) e B (X)⊤.
4 ∈ 4 i0=1 j0=1 i 0 0 i0 4
P P 1×1 n×1 1×d
• Let X Rn×d,W,W Rd×d be defined in Defini|tion{z1.2.}|{z}| {z }
V
∈ ∈
• Assuming each entry of X,W,W ,G can be re represented using O(log(n)) bits.
V i
• Let G Rn×d denote the gradient matrix resulting from the application of the chain rule up
i
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
• For i [n],j [d], let G (i ,j ) denote the (i ,j )-th entry of G .
2 2 i 2 2 2 2 i
∈ ∈
Then, we can show that, there is an algorithm to approximate D in n1+o(1) time, and it can
4
achieve ǫ/poly(n) accuracy.
Namely, the algorithm output D satisfies
4
e D 4 D 4 ∞ ǫ/poly(n)
k − k ≤
Proof. In Lemma D.14, we have e
n d
G (i ,j ) e B (X)⊤ = z (X) X W
i 0 0 i0 4 4
i X0=1j X0=1 1×1 n×1 1×d n×n n×d d×d
Let z (X) := U V⊤. | {z }|{z}| {z } | {z }|{z}|{z}
4 10 10
By Lemma E.9, we have
e
z (X) z (X) ǫ/poly(n) (10)
4 4 ∞
k − k ≤
Proof of running time.
e
We compute in the following way:
• Compute V⊤ X , which takes n1+o(1) time.
10
k10×n n×d
• Compute V|{⊤zX} |{ Wz} , which takes n1+o(1) time.
10
k10×d d×d
• Compute |U{z }V|{ ⊤z X} W, which takes n1+o(1) time.
10 10
n×k10 k10×d
|{z} | {z }
48Therefore, the overall running time is n1+o(1).
Proof of error bound.
We have
z (X)XW z (X)XW
4 4 ∞
k − k
d n z (X) z (X) X W
4 4 ∞ ∞ ∞
≤ · k − k k k k k
e
d n(ǫ/poly(n)) X W
∞ ∞
≤ · k k k k
e
ǫ/poly(n)
≤
where the 1st step is from basic linear algebra, the 2nd step comes from Eq.(10), the 3rd step is
because of W poly(n) and X poly(n).
∞ ∞
k k ≤ k k ≤
E.6 Putting everything together
After we have analyzed each B (X) term in the previous section, we put them together in this
i
section, to analyze the overall running time and error bound of the gradient of L(X) on T (X) in
i
Lemma E.11.
Lemma E.11 (Fast computation for
dL(X)
, formal version of Lemma 5.1). If we have the below
dTi−1(X)
conditions,
• Let L(X) be defined as Definition 3.1.
• Let m denote the number of self-attention transformer model (see Definition 1.3).
• For any i [m], let T (X) be defined as Definition 3.3.
i
∈
• Let X Rn×d,W,W Rd×d be defined in Definition 1.2.
V
∈ ∈
• Assuming each entry of X,W,W ,G can be re represented using O(log(n)) bits.
V i
• Let G Rn×d denote the gradient matrix resulting from the application of the chain rule up
i
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
• Assume G can be computed in n1+o(1) time.
i
We can show that dL(X) can be approximated in n1+o(1) time, with 1/poly(n) approximation
dTi−1(X)
error. Namely, our algorithm can output g in n1+o(1) time, which satisfies
t
dL(X)
g t e ∞ 1/poly(n)
k − dT (X)k ≤
i−1
Proof. By Lemma D.9, we have e
n d
dL(X)
= G (i ,j ) (B (X)+B (X)+B (X)+ e (B (X)+B (X))⊤)
dT (X)
i 0 0
·
6 7 8 i0 2 4
i−1
i X0=1j X0=1
1×1 n×d n×1 1×d
= | D{iz } | {z } |{z}| {z }
i∈{2,4,6,7,8}
X
wherethe1ststepisfromLemmaD.9,the2ndstepcomesfromthedefinitionofD ,D ,D ,D ,D .
6 7 8 2 4
49Then, by Lemma E.3, E.5, E.6, E.8, E.10, we have D ,D ,D ,D ,D Rn×d can be approxi-
6 7 8 2 4
∈
mated in n1+o(1) time, with up to ǫ/poly(n) error.
Namely, for i 2,4,6,7,8 , let D Rn×d denote the approximated version of D, we have
i
∈ { } ∈
eD
i
D
∞
ǫ/poly(n)
k − k ≤
Let g = D . e
t i∈{2,4,6,7,8} i
Proof of running time.
P
Theerunning time for ge
t
= i∈{2,4,6,7,8}D
i
is 5nd.
Therefore, the overall running time for computing g is n1+o(1).
P t
Proof of error bouned. e
We have e
dL(X)
g = (D D )
t ∞ i i ∞
k − dT (X)k k − k
i−1
i∈{2,4,6,7,8}
X
e (De D )
i i ∞
≤ k − k
i∈{2,4,6,7,8}
X
e
ǫ/poly(n)
≤
dL(X)
where the 1st step is from the definition of g and , the 2nd step comes from basic algebra,
t dTi−1(X)
the 3rd step is because of D D ǫ/poly(n).
i ∞
k − k ≤
Then, choose ǫ = 1/poly(n), we have e
e
dL(X)
g 1/poly(n)
t ∞
k − dT (X)k ≤
i−1
e
F Fast Computation for Gradient on W
In Section F.1, we introduce some essential notations used in this section. In Section F.2, we offer
the gradient of s(X) on W, which is equivalent to the gradient of the output of the attention
mechanism on W. In Section F.3, we illustrate the gradient of L(X) on W. In Section F.4, we
introduce the almost linear time algorithm for calculating the gradient of L(X) on W, along with
the error bound analysis.
F.1 Key concepts
Definition F.1 (Definition of A, [AS24a]). Let A ,A Rn×d be two matrices. Suppose that
1 2
A = A A Rn2×d2 . We define A Rn×d2 be a n d∈ 2 size sub-block from A. Note that there
1
⊗
2
∈
j0
∈ ×
are n such sub-blocks.
Remark F.2. Note that the A ,A matrices in Definition F.1 is X in our setting. Since in
1 2
[AS24a], they consider a more general setting, where A ,A can be difference matrices, while in
1 2
our problem, we consider self-attention. Therefore, in our paper, we have A = A = X.
1 2
50F.2 Gradient of s(X) on W
We begin with introducing the close form of the gradient of s(X).
[AS24a] proved the close form of the gradient of c(X) = s(X) B with respect to W for a
−
constant matrix B. By chain rule, this is equivalent to the gradient of s(X) with respect to W.
Lemma F.3 (Gradient of s(X) on W, Lemma B.1 in [AS24a]). If we have the below conditions,
• Let A be defined as Definition F.1. For every i [d2], define A Rn to be the i-th column
for A Rn×d2 . ∈
j0,i
∈
j0
∈
• Let f(X),h(X),s(X) be defined as Definition C.7, C.8, C.9.
• Let W Rd×d be defined as Definition 1.2. Let w Rd2 denote the vector representation of
∈ ∈
W.
Then, for each i [d2], we have For each j [n], for every i [d]
0 0
∈ ∈ ∈
ds(X)
j0,i0 = A f(X) ,h(X) f(X) ,h(X) A ,f(X)
dw h
j0,i
⊙
j0 i0i−h j0 i0i·h j0,i j0i
i
F.3 Gradient of L(X) on W
Differingfromtheℓ loss functionusedin[AS24a], ourframework supportsarbitrarylossfunctions.
2
Therefore, we use Lemma F.4 to illustrate the gradient of L(X) on W.
Lemma F.4 (Gradient of L(X) on W). If we have the below conditions,
• Let L(X) be defined as Definition 3.1.
• Let W Rd×d,X Rn×d be Defined as Definition 1.2.
∈ ∈
• Let p(X) be defined as Definition C.11.
Then, we can show that
dL(X)
= X⊤ p(X) X
dW · ·
i
Proof. By Lemma F.3, we have, for each i [d2], we have For each j [n], for every i [d]
0 0
∈ ∈ ∈
ds(X)
j0,i0 = A f(X) ,h(X) f(X) ,h(X) A ,f(X) (11)
dw h
j0,i
⊙
j0 i0i−h j0 i0i·h j0,i j0i
i
n×1 n×1 n×1 n×1 n×1 n×1 n×1
By Fact C.1, we have |{z} | {z } | {z } | {z } | {z } |{z} | {z }
A f(X) ,h(X) = A⊤ diag(f(X) )h(X)
h
j0,i
⊙
j0 i0i j0,i j0 i0
and
f(X) ,h(X) f(X) ,A = A⊤ f(X) f(X)⊤h(X)
h
j0 i0i·h j0 j0,i
i
j0,i j0 j0 i0
By Eq. (11), for each i [d2], we have For each j [n], for every i [d], we have
0 0
∈ ∈ ∈
ds(X)
j0,i0 = A⊤ (diag(f(X) ) f(X) f(X)⊤)h(X)
dw j0,i j0 − j0 j0 i0
i
51which implies,
ds(X)
j0,i0 = A⊤ (diag(f(X) ) f(X) f(X)⊤)h(X) (12)
dW j0 j0 − j0 j0 i0
d2×n n×n n×1
|{z}| {z }| {z }
By Lemma C.3, for i [m], we have
∈
dL(X) n d dAttn (T (X))
= G (i ,j )
i i−1 i2,j2.
(13)
i 2 2
dW · dW
i i
i X2=1j X2=1
By the definition of s(X) (Definition C.9), we have
s(X) = Attn (T (X))
i i−1
Combining Eq. (12) and Eq. (13), for each i [m], we have
∈
n d
dL(X)
= G (j ,i ) A⊤ (diag(f(X) ) f(X) f(X)⊤)h(X) (14)
dW i 0 0 · j0 j0 − j0 j0 i0
i
j X0=1i X0=1
1×1 d2×n n×n n×1
| {z } |{z}| {z }| {z }
Recall that we have defined q(X) in Definition C.10,
d
q(X) := G (j ,i ) h(X) (15)
j0 i 0 0
·
i0
i X0=1
Recall that p(x) Rn is define as Definition C.11,
j0
∈
p(x) := (diag(f(x) ) f(x) f(x)⊤)q(x) . (16)
j0 j0
−
j0 j0 j0
Then, we have
dL(X)
dW
i
n d
= G (j ,i ) A⊤ (diag(f(X) ) f(X) f(X)⊤)h(X)
i 0 0
·
j0 j0
−
j0 j0 i0
j X0=1i X0=1
1×1 d2×n n×n n×1
n
= A⊤ | (dia{ gz (f(} X| ){z )}| f(X) f(X)⊤{z )q(X) }| {z }
j0 j0
−
j0 j0 j0
j X0=1
d2×n n×n n×1
n
=
A|{ ⊤z} p|
(X)
{z }| {z }
j0 j0
j X0=1
= X⊤ p(X) X
d×n n×n n×d
where the 1st step
i| s{z fr} o|m{zE}q| .{ (z 1}
4), the 2nd step comes from Eq. (15), the 3rd step is because of
Eq. (16), the 4th step is due to the tensor tricks.
52F.4 Fast computation
Finally, weintroducethealmost linear timealgorithm andits erroranalysis ofthegradientof L(X)
on W in Lemma F.5.
Lemma F.5 (Fast computation for
dL(X)
). If we have the below conditions,
dWi
• Let L(X) be defined as Definition 3.1.
• Let m denote the number of self-attention transformer layers (see Definition 1.3).
• For any i [m], let W = W W⊤ denote the attention weight in the i-th transformer layer.
∈ i Qi Ki
We can show that dL(X) can be approximated in n1+o(1) time, with 1/poly(n) approximation
dWi
error. Namely, our algorithm can output g in n1+o(1) time, which satisfies
w
dL(X)
g w e ∞ 1/poly(n)
k − dW k ≤
i
Proof. Recall by Lemma C.14, C.15e, we have defined p (X),p (X) Rn×n.
1 2
∈
In thoseLemmas, wehave p (X),p (X) have low rank approximation U V⊤ and U V⊤, respec-
1 2 3 3 4 4
tively.
By the definition of p(X) (Definition C.11), we have
p(X) = p (X) p (X) (17)
1 2
−
Then, by Lemma F.4, we have
dL(X)
dW
i
= X⊤p(X)X
= X⊤(p (X) p (X))X
1 2
−
where the 1st step is from Lemma F.4, the 2nd step comes from Eq. (17).
Let p (X),p (X) denote the low rank approximations for p (X),p (X), respectively.
1 2 1 2
Proof of running time. We first compute X⊤p (X)X in following order
1
e e
• Compute X⊤ U , which takes n1+o(1) time.
3
e
d×n n×k3
• Compute | X{z⊤} U|{zV}⊤, which takes n1+o(1) time.
3 3
d×k3 k3×n
• Compute X| {⊤z U} V|{⊤z}X , which takes n1+o(1) time.
3 3
d×n n×d
The overall runn|ing{ztime}| fo{z r} X⊤p (X)X is n1+o(1).
1
Similarly, the overall running time for X⊤p (X)X is n1+o(1).
2
SinceX⊤p 1(X)X,X⊤p 2(X)Xe Rd×d,thecomputationtimeforX⊤(p 1(X) p 2(X))X isO(d2).
∈ −
Therefore, the overall running time for X⊤ e(p 1(X) p 2(X))X is n1+o(1).
−
Proof ofeerror bouned. e e
We consider the error for X⊤p 1(X)X first. e e
e 53X⊤p (X)X X⊤p (X)X
1 1 ∞
k − k
= X⊤(p (X) p (X))X
1 1 ∞
k − k
e
n2 X 2 p (X) p (X)
≤ k k∞k 1 − 1 k∞
e
n2(ǫ/poly(n)) X 2
≤ k k∞
e
ǫ/poly(n) (18)
≤
where the 1st step is from basic algebra, the 2nd step comes from basic linear algebra, the 3rd step
is because of p (X) p (X) ǫ/poly(n), the 4th step is due to X poly(n).
1 1 ∞ ∞
k − k ≤ k k ≤
Similarly, we can have
e
X⊤p (X)X X⊤p (X)X ǫ/poly(n) (19)
2 2 ∞
k − k ≤
Therefore, we have
e
X⊤p(X)X X⊤p(X)X
∞
k − k
= X⊤p (X)X X⊤p (X)X +X⊤p (X)X X⊤p (X)X
1 1 2 2 ∞
k − − k
e
X⊤p (X)X X⊤p (X)X + X⊤p (X)X X⊤p (X)X
1 1 ∞ 2 2 ∞
≤ k − k k − k
e e
(ǫ/poly(n))+(ǫ/poly(n))
≤
e e
= ǫ/poly(n)
where the 1st step is from basic algebra, the 2nd step comes from triangle inequality, the 3rd step
is because of Eq. (18) and Eq. (19), the 4th step is due to basic algebra.
Then, we choose ǫ = 1/poly(n), we have
dL(X)
g 1/poly(n)
w ∞
k − dW k ≤
i
e
G Fast Computation for Gradient on W
V
In Section G.1, we introduce the close form of the gradient of s(X) on W . In Section G.2, we
V
provide the close form of the gradient of L(X) on W . In Section G.3, based on the close form
V
calculated in the previous section, we introduce the almost linear time algorithm for computing the
gradient of L(X) on W .
V
G.1 Gradient of s(X) on W
V
Since s(X) = f(X)h(X), we begin with considering the gradient of h(X) on W in Lemma G.1.
V
Lemma G.1 (Gradient of h(X) on W ). If we have the below conditions,
V
• Let h(X) be defined as Definition C.8.
• Let W be defined as Definition 1.2.
V
54Then, for any i [n],j [d] and any i ,j [d], we have
0 0 1 1
∈ ∈ ∈
dh(X) X j = j
i0,j0
=
i0,i1 0 1
d(W V) i1,j1 (0 j 0 = j 1
6
Proof. Since h satisfies
i0,j0
h = X⊤ (W ) ,
i0,j0 i0,∗ V ∗,j0
we have h only depends on (W ) .
i0,j0 V ∗,j0
Hence, we have, for j = j ,
0 1
6
dh(X)
i0,j0
= 0
d(W )
V i1,j1
For j = j case, we have
0 1
dh(X)
i0,j0
= X
d(W )
i0,i1
V i1,j0
Combining the result in the previous Lemma and the chain rule, we can have the gradient of
s(X) on W in Lemma G.2.
V
Lemma G.2 (Gradient of s(X) on W ). If we have the below conditions,
V
• Let s(X) be defined as Definition C.9.
• Let W be defined as Definition 1.2.
V
Then, for any i [n],j [d] and any i ,j [d], we have
2 2 1 1
∈ ∈ ∈
• Part 1.
ds(X) f(X)⊤ X j = j
i2,j2 = i2,∗ ∗,i1 2 1
d(W V) i1,j1 (0 j 2 = j 1
6
• Part 2.
ds(X)
i2,j2 = X⊤ f(X) e⊤
dW i2,∗ j2
V
d×n n×1 1×d
d×d
|{z}
| {z }|{z}
| {z }
Proof. Proof of Part 1.
By Definition C.9, we have
s(X) := f(X)⊤ h(X) (20)
i2,j2 i2,∗ ∗,j2
Therefore, s(X) is only depends on h(X) , which further means s(X) is only depends
i2,j2 ∗,j2 i2,j2
on (W ) .
V ∗,j2
55Hence, for j = j , we have
1 2
6
ds(X)
i2,j2
= 0
d(W )
V i1,j2
We consider j = j case.
1 2
By, Eq. (20), we can derive that
ds(X)
i2,j2
= f(X) (21)
dh(X)
i2,i3
i3,j2
By chain rule, we have
ds(X)
i2,j2
d(W )
V i1,j2
d
ds(X) dh(X)
=
i2,j2 i3,j2
dh(X) d(W )
i X3=1
i3,j2 V i1,j2
d
dh(X)
= f(X)
i3,j2
i2,i3d(W
)
i X3=1
V i1,j2
d
= f(X) X
i2,i3 i3,i1
i X3=1
= f(X)⊤ X (22)
i2,∗ ∗,i1
where the 1st step is from chain rule, the 2nd step comes from Eq. (21), the 3rd step is because of
Lemma G.1, the 4th step is due to basic linear algebra.
Proof of Part 2.
By Eq (22), we have
ds(X)
i2,j2 = X⊤ f(X)
d(W )
i2,∗
V ∗,j2
d×n n×1
d×1
|{z}
| {z }
which implies | {z }
ds(X)
i2,j2 = X⊤ f(X) e⊤
dW i2,∗ j2
V
d×n n×1 1×d
d×d
|{z}
| {z }|{z}
| {z }
G.2 Gradient of L(X) on W
V
Since we have already got the close form of the gradient of s(X) on W , we can easily extend it
V
and get the close form of the gradient of L(X) on W in Lemma G.3.
V
Lemma G.3 (Gradient of L(X) on W ). If we have the below conditions,
V
• Let L(X) be defined as Definition 3.1.
56• Let W be defined as Definition 1.2.
V
Then, we can show that
dL(X)
= X⊤ f(X) G
i
dW
Vi
d×n n×n n×d
d×d
|{z}|{z}|{z}
Proof. We slightly abuse the notation|, u{zsin}g W to represent V in Lemma G.1, G.2.
V i
By Lemma G.2, we have
ds(X)
i2,j2 = X⊤ f(X) e⊤ (23)
dW i2,∗ j2
V
d×n n×1 1×d
d×d
|{z}
| {z }|{z}
By Lemma C.3, we have | {z }
dL(X) n d dAttn (T (X))
= G (i ,j )
i i−1 i2,j2.
(24)
i 2 2
dW · dW
Vi
i X2=1j X2=1
Vi
By Definition C.9 and Definition 1.2, we have
s(X) = Attn (T (X))
i i−1
Therefore, combining Eq. (23) and Eq. (24), we have
dL(X)
dW
Vi
n d
= G (i ,j ) X⊤ f(X) e⊤
i 2 2 i2,∗ j2
i X2=1j X2=1 1×1 d×n n×1 1×d
n
| {z
}|d{z}
| {z }|{z}
= X⊤ f(X) G (i ,j ) e⊤
i2,∗ i 2 2 j2
i X2=1 d×n n×1 j X2=1 1×1 1×d
=
n | X{z ⊤} | f(X{z
)
}
G
(i|
,
){ ⊤z }|{z}
i2,∗ i 2
∗
i X2=1 d×n n×1 1×d
= X⊤|f{(zX} |) G
{iz }| {z }
d×n n×n n×d
where the 1st step is from Eq. (2| 3{ )z} an|d{zE}q|.{(z2}4), the 2nd step comes from basic algebra, the 3rd
step is because of basic linear algebra, the 4th step is due to basic linear algebra.
G.3 Fast computation
Finally, we can introduce our almost linear time algorithm for computing the L(X) gradient on
W .
V
Lemma G.4 (Fast computation for
dL(X)
). If we have the below conditions,
d(WV)i
57• Let L(X) be defined as Definition 3.1.
• Let m denote the number of self-attention transformer layers (see Definition 1.3).
• For any i [m], let W Rd×d denote the attention weight in the i-th transformer layer.
∈
Vi
∈
We can show that dL(X) can be approximated in n1+o(1) time, with 1/poly(n) approximation
dWVi
error. Namely, our algorithm can output g in n1+o(1) time, which satisfies
v
dL(X)
g v e ∞ 1/poly(n)
k − dW k ≤
Vi
Proof. Recall in Lemma C.12, U V⊤e is the low rank approximation of f(X).
1 1
Let f(X) := U V⊤ denote the low rank approximation of f(X).
1 1
Recall in Lemma G.3, we have
e
dL(X)
= X⊤ f(X) G
i
dW
Vi
d×n n×n n×d
d×d
|{z}|{z}|{z}
Proof of running time. | {z }
We compute X⊤f(X)G in following order
i
• Compute X⊤ U , which takes n1+o(1) time.
·e 1
d×n n×k1
• Compute | X{z⊤} U|{z}V⊤, which takes n1+o(1) time.
· 1 · 1
d×k1 k1×n
• Compute | X⊤{z U} |V{⊤z} G , which takes d2 n time.
· 1 · 1 · i ·
d×n n×d
The overall r|unnin{zg tim}e i|s{nz}1+o(1).
Proof of error bound.
We have
X⊤ f(X) G X⊤ f(X) G
i i ∞
k · · − · · k
= X⊤ (f(X) f(X)) G
i ∞
k · − · ek
n2 X f(X) f(X) G
∞ ∞ i ∞
≤ k k k −e k k k
n2(ǫ/poly(n)) X G
∞ i ∞
≤ k ke k k
ǫ/poly(n)
≤
where the 1st step is from basic algebra, the 2nd step comes from basic linear algebra, the 3rd
step is because of f(X) f(X) ǫ/poly(n), the 4th step is due to X poly(n) and
∞ ∞
k − k ≤ k k ≤
G poly(n).
i ∞
k k ≤
Let g =X⊤ f(X) G . e
v i
· ·
We choose ǫ = 1/poly(n). Then, we have
e e
dL(X)
g 1/poly(n)
v ∞
k − dW k ≤
Vi
e
58H Gradient Approximation for Entire Model
InSectionH.1,weintroducethecloseformofG andarguethatG canbecomputedinalmostlinear
i i
time n1+o(1). In Section H.2, we provide the almost linear time algorithm for gradient computing
on a single-layer transformer. In Section H.3, with the help of math induction, we introduce the
almost linear time algorithm for computing the gradient of the multi-layer transformer, along with
its approximation error.
H.1 Computation time for G
i
Here we consider g in Definition 1.3 as a linear layer with an arbitrary non-linear activation φ.
i
Since g can be viewed as a composition of an MLP and an activation function, we begin with
i
analyzing the T
i
gradient on Attn i.
Lemma H.1 (Gradient of T on Attn ). If we have the below conditions,
i i
• Let T (X) be defined as Definition 3.3.
i
• Assuming for any Z Rn×d, we have g (Z) Rn×d, and g (Z) = φ(ZW ), where W Rd×d
i i g g
∈ ∈ ∈
and φ : R R denotes any element-wise activation function. Let φ′ denote the derivative of
→
φ.
• We simplify the notation, using T and Attn to represent T (X) and Attn (T (X)), respec-
i i i i i−1
tively.
• For any matrix Z Rn×d, we use Z(i,j) to denote the (i,j)-th entry of Z.
∈
Then, we can show that, for any i ,i [n],j ,j [d],
4 5 4 5
∈ ∈
• Part 1.
φ′(Attn (i , )⊤W ( ,j ))W (j ,j ) i = i
i 4 g 4 g 5 4 4 5
dT (i ,j ) ∗ ∗
i 4 4
=
dAttn (i ,j )  1×1 1×1
i 5 5  0
| {z }| {z }
i
4
6= i
5

• Part 2. 
dT (i ,j )
i 4 4 = φ′(Attn (i , )⊤W ( ,j )) e W ( ,j )⊤
dAttn i 4 ∗ g ∗ 4 i4 g ∗ 4
i
1×1 n×1 1×d
n×d
| {z }|{z}| {z }
| {z }
Proof. Proof of Part 1.
By the definition of T (Definition 3.3), for i [d],j [n], we have
i 4 4
∈ ∈
T (i ,j ) = φ(Attn (i , )⊤W ( ,j ))
i 4 4 i 4 g 4
∗ ∗
Therefore, for any i = i , we have
5 4
6
dT (i ,j )
i 4 4
= 0
dAttn (i ,j )
i 5 5
Then, we consider i = i case.
4 5
59By basic calculus, we have
dT (i ,j )
i 4 4 = φ′(Attn (i , )⊤W ( ,j ))W (j ,j )
dAttn (i ,j ) i 4 ∗ g ∗ 4 g 5 4
i 4 5
1×1 1×1
Combining two equations mentioned|above, we{hzave the resu}l|t for{zPar}t 1.
Proof of Part 2.
By result of Part 1, for i = i , we have
5 4
dT (i ,j )
i 4 4 = φ′(Attn (i , )⊤W ( ,j ))W (j ,j )
dAttn (i ,j ) i 4 ∗ g ∗ 4 g 5 4
i 4 5
1×1 1×1
which implies | {z }| {z }
dT (i ,j )
i 4 4 = φ′(Attn (i , )⊤W ( ,j ))W ( ,j )
dAttn (i , ) i 4 ∗ g ∗ 4 g ∗ 4
i 4
∗
1×1 d×1
By result of Part 1, for i = i , we |have {z }| {z }
5 4
6
dT (i ,j )
i 4 4
= 0
dAttn (i , )
i 5
∗
By basic linear algebra, combining the two equations mentioned above, we have
dT (i ,j )
i 4 4 = φ′(Attn (i , )⊤W ( ,j )) e W ( ,j )⊤
dAttn i 4 ∗ g ∗ 4 i4 g ∗ 4
i
1×1 n×1 1×d
| {z }|{z}| {z }
Then, we can argue that the computation for G can be done in almost linear time n1+o(1).
i
Lemma H.2 (Computation time for G , formal version of Lemma 5.4). If we have the below
i
conditions,
• Let G Rn×d denote the gradient matrix resulting from the application of the chain rule up
i
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
• Assuming we already have dL(X) .
dTi(X)
• Assuming for any Z Rn×d, we have g (Z) Rn×d, and g (Z) = φ(ZW ), where W Rd×d
i i g g
∈ ∈ ∈
and φ : R R denotes any element-wise activation function. Let φ′ denote the derivative of
→
φ.
• We simplify the notation, using T and Attn to represent T (X) and Attn (T (X)), respec-
i i i i i−1
tively.
• For any matrix Z Rn×d, we use Z(i,j) to denote the (i,j)-th entry of Z.
∈
Then, we can show that G can be computed in n1+o(1) time.
i
60dL(X)
Proof. Let g := , and for any i [n],j [d], let g (i ,j ) denote the (i ,j )-th entry of
Ti dTi 4 ∈ 4 ∈ Ti 4 4 4 4
g .
Ti
Similarly, for any i [n],j [d], let T (i ,j ) denote the (i ,j )-th entry of T .
5 5 i 5 5 5 5 i
∈ ∈
We can have
dL(X)
G =
i dAttn
i
dL(X) dT
i
=
dT · dAttn
i i
dT
i
= g
Ti · dAttn
i
n d
dT (i ,j )
i 4 4
= g (i ,j )
Ti 4 4 · dAttn
i
i X4=1j X4=1
where the 1st step is from the definition of G , the 2nd step comes from chain rule, the 3rd step is
i
because of the definition of g , the 4th step is due to chain rule.
Ti
n d
dT (i ,j )
i 4 4
g (i ,j )
Ti 4 4 · dAttn
i
i X4=1j X4=1
n d
= g (i ,j )φ′(Attn (i , )⊤W ( ,j )) e W ( ,j )⊤
Ti 4 4 i 4
∗
g
∗
4 i4 g
∗
4
i X4=1j X4=1
1×1 1×1 n×1 1×d
n d
| {z }| {z }|{z}| {z }
= e g (i ,j )φ′(Attn (i , )⊤W ( ,j ))W ( ,j )⊤
i4 Ti 4 4 i 4
∗
g
∗
4 g
∗
4
i X4=1
n×1
j X4=1
1×1 1×1 1×d
n
= | e{z} (W | (g { (z i , } )| φ′(Attn (i{z , )⊤W )))⊤}| {z }
i4 g Ti 4
∗ ⊙
i 4
∗
g
i X4=1
n×1 d×d d×1 d×1
= (g Ti| ⊙{zφ}′( |A {t zt }n i|W g{)z)W}g⊤ | {z } (25)
n×d d×d
where the 1st step is| from Le{ mz ma H.1} ,|t{hze} 2nd step comes from basic algebra, the 3rd step is
because of basic linear algebra, the 4th step is due to basic linear algebra.
By Eq. (25), we have the close form of G .
i
We can compute G in the following order
i
• Compute (g φ′(Attn W )), which takes n d time.
Ti
⊙
i g
·
n×d
• Compute |(g φ′({Azttn W )})W⊤, which takes d2 n time.
Ti
⊙
i g g
·
n×d d×d
Therefore, the o| verall ru{ nz ning tim} e|f{ozr}G is n1+o(1).
i
61H.2 Fast computation for single-layer transformer
In this section, we dive into the computation time and approximation error of the gradient of a
single-layer transformer. We demonstrate in the following Lemma that the gradient of a single-
layer transformer can be computed in almost linear time n1+o(1), and its error can be bounded by
1/poly(n).
Lemma H.3 (Single-layer transformer gradient approximation). If we have the below conditions,
• Let L(X) be defined as Definition 3.1.
• Let X be defined as Definition 1.2.
• Let the gradient matrix G Rn×d be defined as G = dL(X) .
i ∈ i dAttn i(Ti−1(X))
• For i [n],j [d], let G (i ,j ) denote the (i ,j )-th entry of G .
2 2 i 2 2 2 2 i
∈ ∈
• Assuming for any Z Rn×d, we have g (Z) Rn×d, and g (Z)= φ(Z W ), where W Rd×d
i i g g
∈ ∈ · ∈
and φ : R R denotes any element-wise activation function. Let φ′ denote the derivative of
→
φ.
• Suppose we have a single-layer transformer (see Definition 1.3).
Then, we can show that,
• Part 1: running time. Our algorithm can approximate dL(X) in n1+o(1) time.
dX
• Part 2: error bound. The approximation error of the single-layer transformer can be
bounded by 1/poly(n). Namely, our algorithm output g satisfies
1
dL(X)
g 1/peoly(n)
1 ∞
k − dX k ≤
Proof. By Definition 1.3, a single-layeretransformer has following structure:
g Attn g (X)
1 1 0
◦ ◦
By the definition of G , we have
i
dL(X)
G =
1 dAttn (T (X))
1 0
dL(X) dT (X)
1
= (26)
dT (X) · dAttn (T (X))
1 1 0
By Lemma H.2, we have G can be computed in n1+o(1) time.
1
Proof of Part 1: running time.
For less confusion, in this part of the proof, we ignore the approximation error temporarily.
Since we have got G , we use methods mentioned in Lemma E.11, F.5, G.4 to compute
1
dL(X) ,dL(X) ,dL(X) , respectively, which takes n1+o(1) time for each.
dT0(X) dW1 dWV1
Then, since we have dL(X) , again by Lemma H.2, we have dL(X) can be computed in n1+o(1)
dT0(X) dX
time.
Therefore, the overall running time is n1+o(1).
Proof of Part 2: error bound.
62Then, we move on to the error bound.
By Lemma H.2 and Eq. (26), there is no approximation error when computing G .
1
dL(X) dL(X) dL(X)
ByLemmaE.11,F.5,G.4,wehavethereis1/poly(n)approximationerroron , , ,
dT0(X) dW1 dWV1
respectively.
dL(X) dL(X) dL(X)
Let g ,g ,g denote the approximation results of , , , respectively.
t0 w1 v1 dT0(X) dW1 dWV1
We have
e e e
dL(X)
g 1/poly(n) (27)
k
t0
− dT
(X)k∞
≤
0
and e
dL(X)
g 1/poly(n)
k
w1
− dW
k∞
≤
1
and e
dL(X)
g 1/poly(n)
k
v1
− dW
k∞
≤
V1
Let G = g dT0(X) denote theeapproximated version of G .
0 t0 · dX 0
We have
e e
G G
0 0 ∞
k − k
dL(X) dT (X)
0
= (g )
ket0
− dT (X) · dX
k∞
0
dL(X) dT (X)
0
n ed g
≤ · k
t0
− dT
(X)k∞
k dX
k∞
0
dT (X)
0
n d(1e/poly(n))
∞
≤ · k dX k
1/poly(n)
≤
where the 1st step is from the definition of G , the 2nd step comes from basic linear algebra, the
0
3rd step is because of Eq. (27), the 4th step is due to each entry can be written by O(logn) bits.
Let g = G . e
1 0
Therefore, we have
e e
dL(X)
g 1/poly(n)
1 ∞
k − dX k ≤
e
H.3 Fast computation for multi-layer transformer
Sincewehavealreadydemonstratedthatalmostlineartimegradientcomputationcanbeappliedto
a single-layer transformer, with the help of math induction, we can easily generalize that result to
themulti-layer transformer. InthefollowingLemma, wedisplaythatthegradientofthemulti-layer
transformer can be computed in almost linear time, and its approximation error can be bounded
by 1/poly(n).
Lemma H.4 (Multi-layer transformer gradient approximation, formal version of Lemma 5.5). If
we have the below conditions,
63• Let L(X) be defined as Definition 3.1.
• Let X be defined as Definition 1.2.
• Let G Rn×d denote the gradient matrix resulting from the application of the chain rule up
i
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
• For i [n],j [d], let G (i ,j ) denote the (i ,j )-th entry of G .
2 2 i 2 2 2 2 i
∈ ∈
• Let gradient components for each layer be computed according to Lemma E.11, F.5, G.4.
• Assuming for any Z Rn×d, we have g (Z) Rn×d, and g (Z)= φ(Z W ), where W Rd×d
i i g g
∈ ∈ · ∈
and φ : R R denotes any element-wise activation function. Let φ′ denote the derivative of
→
φ.
• Suppose we have a m-layer transformer (see Definition 1.3).
Then, we can show that,
• Part 1: running time. Our algorithm can approximate dL(X) in n1+o(1) time.
dX
• Part 2: error bound. The approximation error of the multi-layer transformer can be
bounded by 1/poly(n). Namely, our algorithm output g satisfies
dL(X)
g
∞
1/peoly(n)
k − dX k ≤
Proof. We use math induction to proveethis Lemma.
Step 1: Proof of a single-layer transformer.
Firstly, by Lemma H.3, we have that for one-layer transformer, our conclusion is established.
Step 2: Assumption for k-layer transformer.
Secondly, we assume for any k, for k-layer transformer model, we have
• Our algorithm can approximate dL(X) in O(n1+o(1)) time.
dX
• The approximation error of the k-layer transformer can be bounded by 1/poly(n). Namely,
our algorithm output g satisfies
dL(X)
e g ∞ 1/poly(n)
k − dX k ≤
Step 3: Proof of (k+1)-layer treansformer.
Thirdly, we consider the (k+1)-layer transformer model.
Without loss of generality, we assume that the additional transformer layer is added at the
beginning of the model.
Namely, let F denote a k-layer transformer model. We have
k
F (X) = g Attn g Attn g (X)
k k k 1 1 0
◦ ◦···◦ ◦ ◦
Let the (k+1)-layer transformer model have the following structure:
F (X) = F Attn g(X) (28)
k+1 k
◦ ◦
Let T := g(X).
0
By assumption, we have
64• dL(X) can be approximated in n1+o(1) time.
dAttn(T0)
• Let g denote the approximated version of dL(X) . We have
k dAttn(T0)
dL(X)
e g 1/poly(n) (29)
k k − dAttn(T )k∞ ≤
0
Step 3.1: Proof of the runnineg time for (k+1)-layer transformer
For less confusion, in this part of the proof, we ignore the approximation error temporarily.
By the assumption, we have dL(X) can be approximated in n1+o(1) time.
dAttn(T0)
dL(X)
We compute in following order:
dX
• Since we already have dL(X) , by Lemma E.11, the computation time for dL(X) is n1+o(1).
dAttn(T0) dT0
• Since we have dL(X) , by Lemma H.2, the computation time for dL(X) is n1+o(1).
dT0 dX
Therefore, for (k+1)-layer transformer, the overall running time for dL(X) is n1+o(1).
dX
Step 3.2: Proof of the error bound for (k+1)-layer transformer
dL(X)
By Lemma E.11, during the process of solving the approximated version of , the approx-
dg(X)
imation error will not be magnified by more than poly(n).
dL(X)
Let g denote the approximated version of , we have
t0 dg(X)
dL(X)
e g
k
t0
−
dg(X)k∞
dL(X)
peoly(n) g
k ∞
≤ k − dT(X)k
1/poly(n) (30)
≤ e
where the 1st step is from the above statement, the 2nd step comes from Eq. (29), the 3rd step is
because of basic algebra.
Then, we consider
dL(X) dL(X) dg(X)
= (31)
dX dg(X) · dX
dL(X)
Recall that we have g = . Then, we have
dX
dL(X)
e g
∞
k − dX k
dL(X) dg(X)
= k(eg
t0
−
dg(X))
· dX
k∞
dL(X) dg(X)
n ed g
≤ · k
t0
−
dg(X)k∞
k dX
k∞
dg(X)
n d(1e/poly(n))
∞
≤ · k dX k
1/poly(n)
≤
where the 1st step is from Eq. (31), the 2nd step comes from basic linear algebra, the 3rd step is
because of Eq. (30), the 4th step is due to each entry can be written by O(logn) bits.
65Step 4: Use math induction.
Sofar, withtheassumptionthatourstatement holdsunderk-layer transformer,wehave proved
that our statement still holds under (k+1)-layer transformer.
Therefore, by math induction, our statement holds for any m-layer transformer.
I Causal Attention Mask
This section will discuss how to combine the causal attention mask with our framework. We argue
thatevenwiththecausalattention mask,wecanalsoachievealmostlineartimegradientcomputing
for the multi-layer transformer.
InSection I.1,weintroduceessentialtools fromliteraturetodealwiththecausalmaskaddedon
the attention matrix. In Section I.2, we show that with the addition of causal mask, our framework
can still achieve almost linear time gradient computation.
I.1 Tools from previous work
Firstly, we restate a classical low-rank approximation method in the literature.
Lemma I.1 (Low-rank approximation, [AS23]). Suppose Q,K Rn×d, with Q R, and
∞
∈ k k ≤
K R. Let A := exp(QK⊤/d) Rn×n. For accuracy parameter ǫ (0,1), there is a positive
∞
k k ≤ ∈ ∈
integer g bounded above by
log(1/ǫ)
g = O max ,R2 ,
log(log(1/ǫ)/R)
(cid:16) n o(cid:17)
and a positive integer r bounded above by
2(g+d)
r
≤ 2g
(cid:18) (cid:19)
such that: There is a matrix A Rn×n that is an (ǫ,r)-approximation of A Rn×n. Furthermore,
∈ ∈
the matrices U and V defining A can be computed in O(n r) time.
0 0
·
e
Then, we provide the formal definition for the causal attention mask.
e
Definition I.2 (Causal attention mask, [LLS+24b]). We define the causal attention mask as M
∈
0,1 n×n, where M = 1 if i j and M = 0 otherwise.
i,j i,j
{ } ≥
In previous work [LLS+24b], they point out there exists an algorithm (Algorithm 2) that can
calculate low-rank matrices (with the causal attention mask) multiplication with any vector v in
almost linear time. We restate their results in Lemma I.3.
LemmaI.3(Fastcomputationforcausalattentionmaskontensor,[LLS+24b]). LetM 0,1 n×n
∈ { }
be a causal attention mask defined in Definition I.2. Let U ,V Rn×k. Let v Rn. Then, there
0 0
∈ ∈
exists an algorithm (see Algorithm 2) whose output satisfies that
Y = (M (U V⊤))v,
⊙ 0 0
which takes O(nk) time.
We extend their results to the multiplication of matrix with no(1) columns.
66Algorithm 2 Causal attention mask algorithm, Algorithm 4 in [LLS+24b]
1: procedure CausalMask(U 0 Rn×k,V 0 Rn×k,v Rn) ⊲ Lemma I.3
∈ ∈ ∈
2: c 0 0 k
←
3: for j = 1 n do
→
4: b j
←
(V 0⊤) j v j ⊲ Let (V 0⊤) j denote the j-th row of V 0
∈
Rn×k
k×1 scalar
5: c j c j−1+ b j
← | {z }|{z}
k×1 k×1
6: end for
|{z} |{z}
7: for j = 1 n do
→
8: Y j
←
h(U 0⊤) j, c j
i
k×1 k×1
9: end for
10: return Y | {z } |{z} ⊲ Y Rn
∈
11: end procedure
Lemma I.4 (Fast computation for causal attention mask on matrix). If we have the below condi-
tions,
• Let M 0,1 n×n be a causal attention mask defined in Definition I.2.
∈{ }
• Let U ,V Rn×k where k = no(1).
0 0
∈
• Let H Rn×kH where k
H
= no(1).
∈
Then, there exists an algorithm, whose output satisfies that
Z = (M (U V⊤))H,
⊙ 0 0
which takes n1+o(1) time.
Proof. For j [k ], let H Rn denote the j-th column of H.
H ∗,j
∈ ∈
By Lemma I.3, we can compute (M (U V⊤))H in O(nk) time.
⊙ 0 0 ∗,j
There are k columns in total. Therefore, the overall running time is O(nkk ) = O(n no(1)
H H
· ·
no(1)) = n1+o(1).
I.2 Fast computation with causal mask
Wecaneasilychangealllow-rankmatricesmultiplication tothealgorithmmentionedinLemmaI.4.
Then, our framework can support the causal attention mask and still achieves almost linear time
gradient computing for the multi-layer transformer.
The causal mask directly affects the attention matrix, so it’s necessary to define the attention
matrix with the causal mask applied.
Definition I.5. Let M 0,1 n×n be a causal attention mask defined in Definition I.2. We define
∈ { }
attention matrix with causal mask as:
f(X) := D−1(M A)
⊙
where A:= exp(XWX⊤/d) and D := dbiag((M A) 1 n).
⊙ ·
67After analyzing the components of gradients on T (X),W ,W in Section E, F and G, we cate-
i i Vi
gorize them into two groups: one involving the dot product and the other involving the Hadamard
product of the attention matrix. Then, we can show f(X)H and (f(X) (UV⊤))H for low rank
⊙
matrices U,V,H can be approximated in almost linear time.
b b
Lemma I.6. If we have the below conditions,
• Let f(X) be defined in Definition I.5.
• Let U,V Rn×k where k =no(1).
b
∈
• Let H Rn×kH where k
H
= no(1).
∈
Then, approximating the following takes n1+o(1) time:
• Part 1. f(X)H
• Part 2. (f(X) (UV⊤))H
b
⊙
Proof. From Definition I.5, we know
b
f(X) := D−1(M A)
⊙
where D := diag((M A) 1 n). b
⊙ ·
By Lemma I.1, U V⊤ is a good approximation for A. Then, we can approximate f(X) by:
0 0
D−1(M ⊙(U 0V 0⊤)) b
where D := diag((M (U V⊤)) 1 ).
⊙ 0 0 · n
Using Lemma I.3, we know (M (U V⊤)) v for any vector v Rn can be computed in almost
⊙ 0 0 · ∈
linear time.
We begin by examining the normalization matrix D−1. Calling Lemma I.3, we compute (M
⊙
(U V⊤)) 1 in almost linear time. Then, it takes O(n) time to make (M (U V⊤)) 1 diagonal.
0 0 · n ⊙ 0 0 · n
Given that D is diagonal, its inverse D−1 can be determined in O(n) time. Thus, we can compute
D−1 in almost linear time.
Proof of Part 1. H can be viewed as a combination of k vectors, each of size n. Calling
H
Lemma I.4, we can compute (M (U V⊤))H in n1+o(1) time.
⊙ 0 0
Finally, we computeD−1(M (U V⊤))H, which takes n1+o(1) time since D−1 is diagonal. The
⊙ 0 0
n×n
n×kH
overall gradient computation remains n1+o(1) time.
|{z}| {z }
Proof of Part 2. The proof for this part involves Fact C.2. We can show
((D−1(M (U V⊤))) (UV⊤))H
⊙ 0 0 ⊙
= ((M (D−1U V⊤)) (UV⊤))H
⊙ 0 0 ⊙
= (M ((D−1U V⊤) (UV⊤)))H
⊙ 0 0 ⊙
= (M ((D−1U ) U)(V V)⊤)H
0 0
⊙ ⊘ ⊘
where the 1st step is from D(A B) = (DA) B = A (DB) for diagonal matrix D Rm×m and
⊙ ⊙ ⊙ ∈
A,B Rm×n, the 2nd step comes from (A B) C = A (B C) for A,B,C Rm×n, and the
∈ ⊙ ⊙ ⊙ ⊙ ∈
last step follows from Fact C.2.
Let U := (D−1U ) U and V := V V.
M 0 M 0
⊘ ⊘
68For U , we compute D−1 U which takes nk time. We then compute (D−1U ) U which
M 0 0
⊘
n×n n×k n×k n×k
takes O(nk2) time.
|{z}|{z} | {z } |{z}
For V , we compute V V which takes O(nk2) time.
M 0
⊘
n×k n×k
We now have (M (U V⊤)H. Calling Lemma I.4, we finish the proof.
⊙ |{M z}M|{z}
We now prove for gradient components that have dot product.
Lemma I.7 (Components for dot product). If we have the below conditions,
• Let f(X) be defined in Definition I.5.
• Let G Rn×d denote the gradient matrix resulting from the application of the chain rule up
bi
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
• Let D = f(X)diag(K)XW⊤ be defined in Lemma D.17.
6
−
• Let D = diag(K)f(X)XW be defined in Lemma D.17.
2
−
• Let D = f(X)G W⊤ be defined in Lemma D.17.
8 i V
• Let g := X⊤f(X)G be the gradient on W and defined in Lemma G.3.
v i Vi
Then, we can show the following can be approximated in almost linear time:
• Part 1. D = f(X)diag(K)XW⊤
6
−
• Part 2. D = diag(K)f(X)XW
b2 b
−
• Part 3. D = f(X)G W⊤
b8 i Vb
• Part 4. gbv := Xb⊤f(X)G
i
Proof. Proof of Part 1. For D , we compute diag(K) X first, which takes nd time.
b b 6
n×n n×d
Then, we compute f(X)diab g(K)X using Part 1. of Lemma I.6, which takes n1+o(1) time.
| {z }|{z}
n×n n×d
Finally, we computeb f(X)diag(K)X W⊤, which takes n1+o(1) time.
|{z}| {z }
n×d d×d
b
Proof of Part 2. F |or D 2, {w ze com }p|u{tze}f(X) X using Part 1. of Lemma I.6, which takes
n×n n×d
n1+o(1) time. b b
|{z}|{z}
Then, we compute diag(K)f(X)X, which takes nd time.
n×n n×d
After that, we compute diagb (K)f(X)X W , which takes n1+o(1) time.
| {z }| {z }
n×d d×d
b
Proof of Part 3. For D |8, we {c zompute }|in{zt}he following steps:
We compute f(X) G using Part 1. of Lemma I.6, which takes n1+o(1) time.
i
b
n×n n×d
b
|{z}|{z}
69Then, we compute f(X)G W⊤, which takes n d2 time.
i V
·
n×d d×d
Proof of Part 4. Fbor g , we compute in the following steps:
v
| {z }|{z}
We compute f(X) G using Part 1. of Lemma I.6, which takes n1+o(1) time.
i
n×n n×d b
Then, we comb pute X⊤ f(X)G , which takes n d2 time.
|{z}|{z} i ·
d×n n×d
b
We then prove for g|r{azd}i|ent{zcom}ponents that have Hadamard product.
Lemma I.8 (Components for Hadamard product). If we have the below conditions,
• Let f(X) be defined in Definition I.5.
• Let G Rn×d denote the gradient matrix resulting from the application of the chain rule up
bi
∈ dL(X)
to the function g , i.e., G = .
i i dAttn i(Ti−1(X))
• Let D = (f(X) (h(X)G⊤))XW⊤ be defined in Lemma D.17.
7 ⊙ i
• Let D = (f(X) (G h(X)⊤))XW be defined in Lemma D.17.
4 i
⊙
• Let g := X⊤p(X)X = X⊤(p (X) p (X))X be the gradient on W and defined in Defini-
w 1 2 i
−
tion C.11 and Lemma F.5 where p (X) = f(X) q(X) and p (X) = diag(p (X) 1 )f(X).
1 2 1 n
⊙ ·
Then, we can show the following can be approximated in almost linear time:
• Part 1. D = (f(X) (h(X)G⊤))XW⊤
7 ⊙ i
• Part 2. D = (f(X) (G h(X)⊤))XW
b4 b i
⊙
• Part 3. g := X⊤(p (X) p (X))X where p (X) = f(X) q(X) and p (X) = diag(p (X)
bw b 1 2 1 2 1
− ⊙ ·
1 )f(X).
n
b b b b b b
Proof. Proof of Part 1. For D , we can compute (f(X) (h(X)G⊤)) X using Part 2. of
b 7 ⊙ i
n×n n×d
Lemma I.6, which takes n1+o(1) tbime. b
| {z }|{z}
We then compute (f(X) (h(X)G⊤))X W⊤, which takes nd2 time.
i
⊙
n×d d×d
Proof of Part 2 |.b For D 7{,
z
we can }co|m{zp}ute (f(X) ⊙(G ih(X)⊤)) X using Part 2. of
n×n n×d
Lemma I.6, which takes n1+o(1b) time. b
| {z }|{z}
We then compute (f(X) (G h(X)⊤))X W , which takes nd2 time.
i
⊙
n×d d×d
Proof of Part 3. b For g , we consider X⊤p (X)X first. Based on Definition C.10, we have
| w {z }|{z}1
p (X) = f(X) q(X) = f(X) (G h(X)⊤). We then compute (f(X) (G h(X)⊤))X using Part
1 i i
⊙ ⊙ ⊙
2. of Lemma I.6, which takbes n1+o(1) time. Aftber that, we compute X⊤ (f(X) (G ih(X)⊤))X,
⊙
b b b b d×n n×d
which takes nd2 time. b
|{z}| {z }
Now we consider X⊤p (X)X. By definition, p (X) = diag(p (X) 1 )f(X). We first compute
2 2 1 n
·
p (X) 1 = (f(X) (G h(X)⊤)) 1 usingPart 2. of Lemma I.6, which takes n1+o(1) time. Mean-
1 n i n
· ⊙ ·
while, we compute f(X)Xb using Part 1. of Lembma I.6, whichbtakes n1+ob(1) time. We then have
b b
b
70diag(p (X) 1 )f(X)X, which takes nd time. Finally, we compute X⊤ diag(p (X) 1 )f(X)X,
1 n 1 n
· ·
n×n n×d d×n n×d
whichbtakes nd2 tbime. b b
| {z }| {z } |{z}| {z }
Together, X⊤p (X)X X⊤p (X)X takes d2 time.
1 2
−
d×d d×d
b b
Thus, we s|how{tzhat o}ur |frame{wzork}can support causal attention masks.
J Residual Connection
Inthissection, wediscusshowtoadaptourframeworktotheattention mechanismwiththeresidual
connection.
In Section J.1, we provide a formalized definition of the two residual connections used in the
attention mechanism. In Section J.2, we argue that with the addition of the residual connection,
the gradient over the attention mechanism can be computed in almost linear time n1+o(1) and the
approximation error can be bound by 1/poly(n). In Section J.3, we use math induction to show
that the gradient over the entire transformer with the residual connection can also be computed in
almost linear time n1+o(1).
J.1 Key concepts
Recall that in Definition 3.3, we have defined T (X) Rn×d as the intermediate variable output
i
∈
by the i-th transformer layer. For simplicity, we use T to represent T (X) in the rest part of this
i i
section. Namely, we have
T = (g Attn )(T )
i i i i−1
◦
Then, we consider adding the residual connection to our framework. Note that there are two
residual connection operations in one transformer layer. We first define the residual connection
over the Attn in Definition J.1.
i
Definition J.1 (Residual connection over Attn i). If we have the below conditions,
• Let T be defined as Definition 3.3.
i
• Let Attn be defined as Definition 1.2.
i
We define Z Rn×d as the output with the residual connection of Attn . Namely, we have
i i
∈
Z = T +Attn (T )
i i−1 i i−1
Then, we consider the second residual connection over the MLP layer g , where we have the
i
formal definition for this in Definition J.2.
Definition J.2 (Residual connection over g ). If we have the below conditions,
i
• Let the multi-layer transformer be defined as Definition 1.3.
• Let the intermediate variable T be defined as Definition 3.3.
i
• Let g denote the components other than self-attention in the i-th transformer layer.
i
71• Let Z Rn×d be defined as Definition J.1.
i
∈
Then T , the output of i-th layer transformer with the residual connection, should have the
i
following form:
T = Z +g (Z )
i i i i
J.2 Analysis of the residual connection
In the previous section, we have defined the two residual connection operations.
In this section, we argue that if the gradient computation can be done in almost linear time
without the residual connection, then with the addition of the residual connection, the gradient
computation can also be completed in almost linear time.
Lemma J.3 (Analysis of the residual connection). If we have the below conditions,
• Let L(X) be defined as Definition 3.1.
• Let Y Rn×d and X Rn×d denote the output and input of the residual connection,
R R
∈ ∈
respectively.
• Let H :Rn×d Rn×d denote some layer in the transformer, such as MLP, Attn, etc.
→
• Suppose the residual connection can be written as
Y = X +H(X ).
R R R
• Assuming we have dL(X) Rn×d, then we can calculate dL(X)dH(XR) in almost linear time
dYR ∈ dYR dXR
n1+o(1).
Then, we can show that,
• dL(X) can be calculated in almost linear time n1+o(1).
dXR
• If dL(X) has 1/poly(n) approximation error, then the approximation error on dL(X) is still
dYR dXR
1/poly(n).
Proof. By the chain rule, we have
dL(X) dL(X) dY
R
=
dX dY dX
R R R
dL(X) dH(X )
R
= (I + )
dY dX
R R
dL(X) dL(X)dH(X )
R
= + (32)
dY dY dX
R R R
where the 1st step is from the chain rule, the 2nd step comes from basic calculus, the 3rd step is
because of basic algebra.
By the assumption, we already have
dL(X)
, and
dL(X)dH(XR)
can be computed in almost linear
dYR dYR dXR
time n1+o(1).
The addition operation between
dL(X)
and
dL(X)dH(XR)
takes n d time.
dYR dYR dXR ·
Therefore, the overall running time for dL(X) is n1+o(1).
dXR
72Then, we consider the approximation error.
By Eq. (32) and basic linear algebra, the approximation error will not be magnified by more
than (n dpoly(n)+1). Since (n dpoly(n)+1)(1/poly(n)) = poly(n), the approximation error
· ·
dL(X)
on can be bounded by 1/poly(n).
dXR
J.3 Analysis for the entire model with the residual connection
In the previous section, we have shown that, with the addition of the residual connection on a
single component, the gradient computation time can still be done in almost linear time. We will
apply this finding to the entire model.
We begin by single layer proof.
Lemma J.4 (Fast gradient computation for single-layer transformer with residual connection). If
we have the below conditions,
• Let L(X) be defined as Definition 3.1.
• Let X Rn×d be defined as Definition 1.2.
∈
• Suppose we have a single-layer transformer (see Definition 1.3).
• Let the residual connection be defined as Definition J.1 and J.2.
Then, we can show that,
• Part 1: running time. Our algorithm can approximate dL(X) in n1+o(1) time.
dX
• Part 2: error bound. The approximation error of the single-layer transformer with the
residual connection can be bounded by 1/poly(n). Namely, our algorithm output g satisfies
r1
dL(X)
g 1/poly(n) e
k
r1
− dX
k∞
≤
Proof. We use T to represent T (X) foer simplicity. By the definition of T (see also Definition 3.3),
i i i
we have the following equations
T = g (X)
0 0
Follow Definition J.1 and J.2, we have
Z = T +Attn (T )
1 0 1 0
and
T = Z +g (Z )
1 1 1 1
Then we calculate the gradient by the following steps:
• Step 1: Calculate dL(X). By thedefinition of L(X) (see also Definition 3.1), we have dL(X)
dT1 dT1
can be computed in n d time.
·
• Step 2: Calculate dL(X). By Lemma H.2, the assumption in Lemma J.3 is satisfied.
dZ1
Therefore, we have dL(X) can be computed in almost linear time n1+o(1).
dZ1
73• Step 3: Calculate dL(X). By Lemma E.11, the assumption in Lemma J.3 is satisfied.
dT0
dL(X)
Hence, can be computed in almost linear time. By Lemma E.11, the approximation
dT0
error is 1/poly(n).
• Step 4: Calculate dL(X). By Lemma H.2, dL(X) can be computed in n1+o(1). The approx-
dX dX
imation error is (n d)(1/poly(n)) =(1/poly(n)).
·
Tosumup,wecanshowthattheoverallrunningtimefor dL(X) isn1+o(1) andtheapproximation
dX
error is 1/poly(n).
Let g be the output of Step 4. Then we are done.
r1
e
We now prove for multi-layer.
Lemma J.5 (Fast gradient computation for multi-layer transformer with residual connection). If
we have the below conditions,
• Let L(X) be defined as Definition 3.1.
• Let X Rn×d be defined as Definition 1.2.
∈
• Let the residual connection be defined as Definition J.1 and J.2.
• Suppose we have a m-layer transformer (see Definition 1.3).
Then, we can show that,
• Part 1: running time. Our algorithm can approximate dL(X) in n1+o(1) time.
dX
• Part 2: error bound. The approximation error of the m-layer transformer with the residual
connection can be bounded by 1/poly(n). Namely, our algorithm output g satisfies
r
dL(X)
g 1/poly(n) e
r ∞
k − dX k ≤
Proof. We use math induction in this peroof.
Step 1: Proof of a single-layer transformer.
Firstly, by Lemma J.4, we have the statement holds for a single-layer transformer.
Step 2: Assumption for k-layer transformer.
Secondly, we assume for any k, for k-layer transformer model, we have
• Part 1: running time. Our algorithm can approximate dL(X) in O(n1+o(1)) time.
dX
• Part 2: error bound. The approximation error of the k-layer transformer can be bounded
by 1/poly(n). Namely, our algorithm output g satisfies
dL(X)
g e 1/poly(n)
∞
k − dX k ≤
Step 3: Proof of (k+1)-layer treansformer.
Thirdly, we consider the (k+1)-layer transformer model.
Let F denote a k-layer transformer with the residual connection.
k
74Then, the entire model can be written as
(F g )(X)
k 0
◦
By the definition of T , we have
i
T = g (X)
0 0
Then, by definition of Z (see also Definition J.1), we have
i
Z = T +Attn (T )
1 0 1 0
By Definition J.2, we have
T = Z +g (Z )
1 1 1 1
Without loss of generality, we assume that the additional transformer layer is added at the
beginning of the model. Then, the (k+1)-layer transformer model has the following structure:
F (X) = F (T )
k+1 k 1
dL(X)
By the assumption for k-layer transformer, we have can be computed in almost linear
dT1
time n1+o(1) and the approximation error can be bounded by 1/poly(n).
dL(X)
We apply similar proof of Lemma J.4, then we can show that, we can compute in almost
dX
linear time n1+o(1) and the approximation error can be bounded by 1/poly(n).
K Multi-head Attention
Following the notation used in Section B.1, we use h to denote the number of heads, and d = d/h
h
to denote the dimension of each head.
Definition K.1 (Multi-head attention). If we have the below conditions,
• Let h denote the number of heads.
• Let d denote the hidden dimension. Let d = d/h denote the dimension of each attention
h
head.
• Let Q,K,V Rn×d be defined as Definition 1.2.
∈
• Let f(X) be defined as Definition C.7.
• Let s(X) be defined as Definition C.9.
The multi-head attention can be formalized as follows:
• Step 1. Split the hidden dimension d of Q,K,V Rn×d into h parts. Then, for each l [h],
∈ ∈
we have Q l,K l,V
l
Rn×d h.
∈
• Step 2. For each l [h], calculate the attention matrix f := Softmax(Q K⊤/d ) Rn×n,
∈ l l l h ∈
and calculate the corresponding attention result s
l
:= f lV
l
Rn×d h.
∈
75• Step 3. Concatenate s l Rn×d h together, then we have the final multi-head attention output
∈
s Rn×d.
∈
Then, we dive into the analysis of the gradient computation process over the attention mecha-
nism with multi-head attention.
Lemma K.2 (Analysis of the multi-head attention). If we have the below conditions,
• Let Attn(X) be defined as Definition 1.2.
• Let multi-head attention mechanism be defined as Definition K.1.
• Let Y ,X Rn×d denote the output and input of the multi-head attention, respectively.
m m
∈
Then, we can show that,
• dL(X) can be calculated in almost linear time n1+o(1).
dXm
• If dL(X) has 1/poly(n) approximation error, then the approximation error on dL(X) is still
dYm dXm
1/poly(n).
Proof. Following the notations used in Definition K.1, for l [h], we use s l Rn×d h to denote the
∈ ∈
output by each attention head. And we use s Rn×d to denote the concatenated version of the
∈
output of the multi-head attention.
By the chain rule and the definition of L(X) (see also Definition 3.1), we have
dL(X) dL(X) dY ds
m
=
dX dY · ds dX
m m m
h
dL(X) dY ds
m l
=
dY · ds dX
m m
l=1
X
where the 1st step is from the chain rule, the 2nd step comes from s Rn×d is the concatenated
∈
version of s
l
Rn×d h.
∈
We calculate the gradient in the following steps:
• Step 1: Calculate dL(X). By the definition of L(X) (Definition 3.1), we have that dL(X)
dYm dYm
can be calculated in n d time.
·
• Step 2: Calculate dL(X) dYm. Since we already have dL(X) , by Lemma H.2, we have
dYm · ds dYm
dL(X) dYm can be computed in almost linear time n1+o(1).
dYm · ds
• Step 3: Calculate dL(X) dYm h ds l . Foreach l [h], byLemmaE.11, dL(X) dYm ds l
dYm · ds l=1 dXm ∈ dYm · ds ·dXm
can be computed in n1+o(1). Since the number of heads h can be viewed as a constant here,
P
it takes n1+o(1) time to compute the gradients on h heads.
Therefore, the overall running time for dL(X) is n1+o(1).
dXm
Then, we consider the error bound.
dL(X)
By assumption, there is 1/poly(n) approximation error on . For each l [h], the approx-
dYm ∈
imation error will not be magnified by more than n2 d d poly(n) on dL(X) dYm ds l .
· · h · dYm · ds · dXm
dL(X)
Then, since there is total h heads, the approximation error on can be bound by
dXm
h n2 d d poly(n) (1/poly(n)) = 1/poly(n)
h
· · · · ·
76Similar to the proof of Lemma H.3 and H.4, we apply Lemma K.2 to deal with the multi-head
dL(X)
attention in each transformer layer. Then, we can show that can be computed in almost
dX
linear time n1+o(1) and the approximation error can be bounded by 1/poly(n).
77References
[AA22] Amol Aggarwal and Josh Alman. Optimal-degree polynomial approximations for ex-
ponentials and gaussian kernel density estimation. In Proceedings of the 37th Compu-
tational Complexity Conference, pages 1–23, 2022.
[AAA+23] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-
cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anad-
kat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
[ACSS20] Josh Alman, Timothy Chu, Aaron Schild, and Zhao Song. Algorithms and hardness
for linear algebra on geometric graphs. In 2020 IEEE 61st Annual Symposium on
Foundations of Computer Science (FOCS), pages 541–552. IEEE, 2020.
[Ant24] Anthropic. The claude 3 model family: Opus, sonnet, haiku. https://www-
cdn.anthropic.com, 2024.
[AS23] Josh Alman and Zhao Song. Fast attention requires bounded entries. Advances in
Neural Information Processing Systems, 36, 2023.
[AS24a] Josh Alman and Zhao Song. The fine-grained complexity of gradient computation for
training large language models. arXiv preprint arXiv:2402.04497, 2024.
[AS24b] Josh Alman and Zhao Song. How to capture higher-order correlations? generalizing
matrix softmax attention to kronecker computation. In The Twelfth International
Conference on Learning Representations, 2024.
[BCB14] Dzmitry Bahdanau, KyunghyunCho, andYoshua Bengio. Neuralmachinetranslation
by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
[BEPP22] Rouzbeh Behnia, Mohammadreza Reza Ebrahimi, Jason Pacheco, and Balaji Pad-
manabhan. Ew-tune: A framework for privately fine-tuning large language models
with differential privacy. In 2022 IEEE International Conference on Data Mining
Workshops (ICDMW), pages 560–566. IEEE, 2022.
[BGOFG20] Davis Blalock, JoseJavier Gonzalez Ortiz, JonathanFrankle, andJohnGuttag. What
is the state of neural network pruning? Proceedings of machine learning and systems,
2:129–146, 2020.
[BMBE20] Brian Bartoldson, Ari Morcos, Adrian Barbu, and Gordon Erlebacher. The
generalization-stability tradeoff in neural network pruning. Advances in Neural In-
formation Processing Systems, 33:20852–20864, 2020.
[BSZ23] JanvandenBrand,ZhaoSong,andTianyiZhou. Algorithmandhardnessfordynamic
attention maintenance in large language models. arXiv preprint arXiv:2304.02207,
2023.
[CAM24] Anshuman Chhabra, Hadi Askari, and Prasant Mohapatra. Revisiting zero-shot ab-
stractive summarization in the era of large language models from the perspective of
position bias. arXiv preprint arXiv:2401.01989, 2024.
78[CJD+21] Tianyi Chen, Bo Ji, Tianyu Ding, Biyi Fang, Guanyi Wang, Zhihui Zhu, Luming
Liang, Yixin Shi, Sheng Yi, and Xiao Tu. Only train once: A one-shot neural network
trainingandpruningframework. Advances in Neural Information Processing Systems,
34:19637–19651, 2021.
[CLG+24] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen,
and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple
decoding heads. arXiv preprint arXiv:2401.10774, 2024.
[CLP+20] Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan Lingjie Li, Tri Dao,
Zhao Song, Anshumali Shrivastava, and Christopher Re. Mongoose: A learnable
lsh framework for efficient neural network training. In International Conference on
Learning Representations, 2020.
[CWCT23] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending
context window of large language models via positional interpolation. arXiv preprint
arXiv:2306.15595, 2023.
[CYL+24] Weize Chen, Ziming You, Ran Li, Yitong Guan, Chen Qian, Chenyang Zhao, Cheng
Yang, Ruobing Xie, Zhiyuan Liu, and Maosong Sun. Internet of agents: Weav-
ing a web of heterogeneous agents for collaborative intelligence. arXiv preprint
arXiv:2407.07061, 2024.
[CZY23] Shang Chai, Liansheng Zhuang, and Fengying Yan. Layoutdm: Transformer-based
diffusion model for layout generation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 18349–18358, 2023.
[Dao23] Tri Dao. Flashattention-2: Faster attention with better parallelism and work parti-
tioning. arXiv preprint arXiv:2307.08691, 2023.
[DBK+20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua
Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recogni-
tion at scale. arXiv preprint arXiv:2010.11929, 2020.
[DFE+22] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R´e. Flashattention:
Fast and memory-efficient exact attention with io-awareness. Advances in Neural
Information Processing Systems, 35:16344–16359, 2022.
[DG24] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient
algorithms through structured state space duality. arXiv preprint arXiv:2405.21060,
2024.
[DMS23] Yichuan Deng, Sridhar Mahadevan, and Zhao Song. Randomized and deterministic
attention sparsification algorithms for over-parameterized feature dimension. arXiv
preprint arXiv:2304.04397, 2023.
[DSXY23] Yichuan Deng, Zhao Song, Shenghao Xie, and Chiwun Yang. Unmasking transform-
ers: A theoretical approach to data recovery via attention weights. arXiv preprint
arXiv:2310.12462, 2023.
79[DYZ+24] Harry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie Chi, and Beidi
Chen. Get more with less: Synthesizing recurrence with kv cache compression for
efficient llm inference. arXiv preprint arXiv:2402.09398, 2024.
[ESL+24] Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti,
Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, et al.
Layer skip: Enablingearly exit inferenceand self-speculative decoding. arXiv preprint
arXiv:2404.16710, 2024.
[FA22] EliasFrantarandDanAlistarh. Optimalbraincompression: Aframeworkforaccurate
post-training quantization and pruning. Advances in Neural Information Processing
Systems, 35:4475–4488, 2022.
[FA23] EliasFrantarandDanAlistarh. Sparsegpt: Massivelanguagemodelscanbeaccurately
pruned in one-shot. In International Conference on Machine Learning, pages 10323–
10337. PMLR, 2023.
[FC18] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse,
trainable neural networks. arXiv preprint arXiv:1803.03635, 2018.
[FCA23] Quentin Fournier, Ga´etan Marceau Caron, and Daniel Aloise. A practical survey on
faster and lighter transformers. ACM Computing Surveys, 55(14s):1–40, 2023.
[FJL+24] Tao Feng, Chuanyang Jin, Jingyu Liu, Kunlun Zhu, Haoqin Tu, Zirui Cheng, Guanyu
Lin, and Jiaxuan You. How far are we from agi. arXiv preprint arXiv:2405.10313,
2024.
[GD23] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state
spaces. arXiv preprint arXiv:2312.00752, 2023.
[GLA+21] Kamal Gupta, Justin Lazarow, Alessandro Achille, Larry S Davis, Vijay Mahade-
van, and Abhinav Shrivastava. Layouttransformer: Layout generation and completion
with self-attention. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 1004–1014, 2021.
[GMS23] Yeqi Gao, Sridhar Mahadevan, and Zhao Song. An over-parameterized exponential
regression. arXiv preprint arXiv:2303.16504, 2023.
[GSY23] Yeqi Gao, Zhao Song, and Xin Yang. Differentially private attention computation.
arXiv preprint arXiv:2305.04701, 2023.
[GSYZ23] Yeqi Gao, Zhao Song, Xin Yang, and Ruizhe Zhang. Fast quantum algorithm for
attention computation. arXiv preprint arXiv:2307.08045, 2023.
[GXG+23] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai,
Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language
models: A survey. arXiv preprint arXiv:2312.10997, 2023.
[GZL+23] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao.
Model tells you what to discard: Adaptive kv cache compression for llms. arXiv
preprint arXiv:2310.01801, 2023.
80[HABN+21] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste.
Sparsity in deep learning: Pruning and growth for efficient inference and training in
neural networks. Journal of Machine Learning Research, 22(241):1–124, 2021.
[HCI+21] Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Joseph Naor, and Daniel
Soudry. Accelerated sparse neuraltraining: A provable and efficient method to findn:
m transposable masks. Advances in neural information processing systems, 34:21099–
21111, 2021.
[HCL+24] Jerry Yao-Chieh Hu, Pei-Hsuan Chang, Haozheng Luo, Hong-Yu Chen, Weijian Li,
Wei-Po Wang, and Han Liu. Outlier-efficient hopfield layers for large transformer-
based models. In Forty-first International Conference on Machine Learning (ICML),
2024.
[HCW+24] Jerry Yao-Chieh Hu, Bo-Yu Chen, Dennis Wu, Feng Ruan, and Han Liu. Nonpara-
metric modern hopfield models. arXiv preprint arXiv:2404.03900, 2024.
[HJK+24] Insu Han, Rajesh Jayaram, Amin Karbasi, Vahab Mirrokni, David Woodruff, and
Amir Zandieh. Hyperattention: Long-context attention in near-linear time. 2024.
[HLSL24] Jerry Yao-Chieh Hu, Thomas Lin, Zhao Song, and Han Liu. On computational limits
of modern hopfield models: A fine-grained complexity analysis. In Forty-first Inter-
national Conference on Machine Learning (ICML), 2024.
[HLZ+23] Nan He, Hanyu Lai, Chenyang Zhao, Zirui Cheng, Junting Pan, Ruoyu Qin, Ruofan
Lu,RuiLu,YunchenZhang,GangmingZhao,etal. Teacherlm: Teachingtofishrather
than giving the fish, language modeling likewise. arXiv preprint arXiv:2310.19019,
2023.
[HPTD15] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and
connections for efficient neural network. Advances in neural information processing
systems, 28, 2015.
[HSK+24] Jerry Yao-Chieh Hu, Maojiang Su, En-Jui Kuo, Zhao Song, and Han Liu. Com-
putational limits of low-rank adaptation (lora) for transformer-based models. arXiv
preprint arXiv:2406.03136, 2024.
[HTH+24] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng,
Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the po-
tential of small language models with scalable training strategies. arXiv preprint
arXiv:2404.06395, 2024.
[HYW+23] Jerry Yao-Chieh Hu, Donglin Yang, Dennis Wu, Chenwei Xu, Bo-Yu Chen, and Han
Liu. On sparse modern hopfield model. In Thirty-seventh Conference on Neural
Information Processing Systems (NeurIPS), 2023.
[JCR+22] Tian Jin, Michael Carbin, Dan Roy, Jonathan Frankle, and Gintare Karolina Dziu-
gaite. Pruning’seffectongeneralizationthroughthelensoftrainingandregularization.
Advances in Neural Information Processing Systems, 35:37947–37961, 2022.
[JKC+18] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew
Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of
81neural networks for efficient integer-arithmetic-only inference. In Proceedings of the
IEEE conference on computer vision and pattern recognition, pages 2704–2713, 2018.
[KHC+24] Tzu-Sheng Kuo, Aaron Lee Halfaker, Zirui Cheng, Jiwoo Kim, Meng-Hsin Wu, Tong-
shuang Wu, Kenneth Holstein, and Haiyi Zhu. Wikibench: Community-driven data
curation for ai evaluation on wikipedia. In Proceedings of the CHI Conference on
Human Factors in Computing Systems, pages 1–24, 2024.
[KKF+23] Eldar Kurtic, Denis Kuznedelev, Elias Frantar, Michael Goin, and Dan Alistarh.
Sparse finetuning for inference acceleration of large language models. arXiv preprint
arXiv:2310.06927, 2023.
[KKL20] Nikita Kitaev, L ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient trans-
former. arXiv preprint arXiv:2001.04451, 2020.
[KMZ23] Praneeth Kacham, Vahab Mirrokni, and Peilin Zhong. Polysketchformer: Fast trans-
formers via sketches for polynomial kernels. arXiv preprint arXiv:2310.01655, 2023.
[KWH23] Feyza Duman Keles, PruthuviMahesakya Wijewardena, and Chinmay Hegde. On the
computational complexity of self-attention. In International Conference on Algorith-
mic Learning Theory, pages 597–619. PMLR, 2023.
[LARC21] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-
efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods
in Natural Language Processing, pages 3045–3059, 2021.
[LAT19] NLee,TAjanthan,andPTorr.Snip: single-shotnetworkpruningbasedonconnection
sensitivity. In International Conference on Learning Representations. Open Review,
2019.
[LCH+24] Minghan Li, Xilun Chen, Ari Holtzman, Beidi Chen, Jimmy Lin, Wen-tau Yih, and
Xi Victoria Lin. Nearest neighbor speculative decoding for llm generation and attri-
bution. arXiv preprint arXiv:2405.19325, 2024.
[LCT+24] Na Liu, Liangyu Chen, Xiaoyu Tian, Wei Zou, Kaijiang Chen, and Ming Cui. From
llmtoconversational agent: Amemoryenhancedarchitecturewithfine-tuningoflarge
language models. arXiv preprint arXiv:2401.02777, 2024.
[LDLG23] Yucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. Compressing context to en-
hance inference efficiency of large language models. arXiv preprint arXiv:2310.06201,
2023.
[LJF+22] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie
Tang. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and
tasks. In Proceedings of the 60th Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers). Association for Computational Linguis-
tics, 2022.
[LKM23] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers
via speculative decoding. In International Conference on Machine Learning, pages
19274–19286. PMLR, 2023.
82[LL21] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for
generation. In Proceedings of the 59th Annual Meeting of the Association for Compu-
tational Linguistics and the 11th International Joint Conference on Natural Language
Processing (Volume 1: Long Papers), pages 4582–4597, 2021.
[LLR23] Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic struc-
ture: Towards a mechanistic understanding. In International Conference on Machine
Learning, pages 19689–19729. PMLR, 2023.
[LLS+24a] Chenyang Li, Yingyu Liang, Zhenmei Shi, Zhao Song, and Tianyi Zhou. Fourier cir-
cuits in neural networks: Unlocking the potential of large language models in mathe-
matical reasoning and modular arithmetic. arXiv preprint arXiv:2402.09469, 2024.
[LLS+24b] Yingyu Liang, Heshan Liu, Zhenmei Shi, Zhao Song, and Junze Yin. Conv-basis: A
new paradigm for efficient attention inference and gradient computation in transform-
ers. arXiv preprint arXiv:2405.05219, 2024.
[LLSS24] Xiaoyu Li, Yingyu Liang, Zhenmei Shi, and Zhao Song. A tighter complexity analysis
of sparsegpt. arXiv preprint arXiv:2408.12151, 2024.
[LMGH22] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision
transformer backbones for object detection. In European conference on computer
vision, pages 280–296. Springer, 2022.
[LPM15] Minh-ThangLuong,HieuPham,andChristopherDManning. Effective approachesto
attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.
[LPP+20] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,
Naman Goyal, Heinrich Ku¨ttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨aschel, et al.
Retrieval-augmented generation forknowledge-intensive nlptasks. Advances in Neural
Information Processing Systems, 33:9459–9474, 2020.
[LSH+23] Yixin Liu, Kejian Shi, Katherine S He, Longtian Ye, Alexander R Fabbri, Pengfei Liu,
Dragomir Radev, and Arman Cohan. On learning to summarize with large language
models as references. arXiv preprint arXiv:2305.14239, 2023.
[LSSS24] Yingyu Liang, Zhizhou Sha, Zhenmei Shi, and Zhao Song. Differential privacy mech-
anisms in neural tangent kernel regression. arXiv preprint arXiv:2407.13621, 2024.
[LSSY24] YingyuLiang, ZhenmeiShi,ZhaoSong,andChiwunYang. Towardinfinite-longprefix
in transformer. arXiv preprint arXiv:2406.14036, 2024.
[LSSZ24a] Yingyu Liang, Zhenmei Shi, Zhao Song, and Yufa Zhou. Differential privacy of cross-
attention with provable guarantee. arXiv preprint arXiv:2407.14717, 2024.
[LSSZ24b] Yingyu Liang, Zhenmei Shi, Zhao Song, and Yufa Zhou. Tensor attention train-
ing: Provably efficient learning of higher-order transformers. arXiv preprint
arXiv:2405.16411, 2024.
[LSSZ24c] Yingyu Liang, Zhenmei Shi, Zhao Song, and Yufa Zhou. Unraveling the smooth-
ness properties of diffusion models: A gaussian mixture perspective. arXiv preprint
arXiv:2405.16418, 2024.
83[LSZ23] Zhihang Li, Zhao Song, and Tianyi Zhou. Solving regularized exp, cosh and sinh
regression problems. arXiv preprint arXiv:2303.15725, 2023.
[LT24] AI@MetaLlamaTeam. Thellama3herdofmodels.arXivpreprint arXiv:2407.21783,
2024.
[LTT+24] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang,
Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware
weight quantization for on-device llm compression and acceleration. Proceedings of
Machine Learning and Systems, 6:87–100, 2024.
[LWD+23] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali
Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual
sparsity for efficient llms at inference time. In International Conference on Machine
Learning, pages 22137–22176. PMLR, 2023.
[LWXZ22] Dacheng Li, Hongyi Wang, Eric Xing, and Hao Zhang. Amp: Automatically finding
model parallel strategies with heterogeneity awareness. Advances in Neural Informa-
tion Processing Systems, 35:6630–6639, 2022.
[LYG+23] Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, and Xipeng Qiu. Adalomo: Low-memory
optimization with adaptive learning rate. arXiv preprint arXiv:2310.10195, 2023.
[LYL+23] Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, and Xipeng Qiu. Full
parameterfine-tuningforlargelanguage modelswithlimited resources. arXiv preprint
arXiv:2306.09782, 2023.
[LZ20] Tianlin Liu and Friedemann Zenke. Finding trainable sparse networks through neural
tangent transfer. In International Conference on Machine Learning, pages 6336–6347.
PMLR, 2020.
[LZD+24] Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. Long-context llms
struggle with long in-context learning. arXiv preprint arXiv:2404.02060, 2024.
[MCW+24] Da Ma, Lu Chen, Pengyu Wang, Hongshen Xu, Hanqi Li, Liangtai Sun, Su Zhu,
ShuaiFan, and KaiYu. Sparsity-accelerated trainingfor large language models. arXiv
preprint arXiv:2406.01392, 2024.
[MLG24] Jesse Mu, Xiang Li, and Noah Goodman. Learning to compress prompts with gist
tokens. Advances in Neural Information Processing Systems, 36, 2024.
[MLH+22] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Ha-
jishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes
in-contextlearningwork? InProceedings ofthe 2022Conference onEmpirical Methods
in Natural Language Processing, pages 11048–11064, 2022.
[MLT+24] Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbi-
eri, and Yuwei Fang. Evaluating very long-term conversational memory of llm agents.
arXiv preprint arXiv:2402.17753, 2024.
[MVK+24] Jean Mercat, Igor Vasiljevic, Sedrick Keh, Kushal Arora, Achal Dave, Adrien
Gaidon, and Thomas Kollar. Linearizing large language models. arXiv preprint
arXiv:2405.06640, 2024.
84[MWY+23] Amama Mahmood, Junxiang Wang, Bingsheng Yao, Dakuo Wang, and Chien-Ming
Huang. Llm-powered conversational voice assistants: Interaction patterns, opportuni-
ties, challenges, and design guidelines. arXiv preprint arXiv:2309.13879, 2023.
[Ope24] OpenAI. Searchgpt prototype. https://openai.com/index/searchgpt-prototype, 2024.
[PAA+23] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella
Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, et al. Rwkv:
Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023.
[RBL+22] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¨rn Om-
mer. High-resolution image synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–
10695, 2022.
[RWC+19] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
Language models are unsupervised multitask learners. 2019.
[SAL+24] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
Roformer: Enhanced transformer with rotary position embedding. Neurocomputing,
568:127063, 2024.
[SAMB24] Tanmay Singh,HarshvardhanAditya, Vijay KMadisetti, andArshdeepBahga. Whis-
pered tuning: Data privacy preservation in fine-tuning llms through differential pri-
vacy. Journal of Software Engineering and Applications, 17(1):1–22, 2024.
[SBZ+24] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri
Dao. Flashattention-3: Fastandaccurateattentionwithasynchronyandlow-precision.
arXiv preprint arXiv:2407.08608, 2024.
[SCY+24] Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, and Beidi Chen. Tri-
force: Lossless acceleration of long sequence generation with hierarchical speculative
decoding. arXiv preprint arXiv:2404.11912, 2024.
[SCZ+24] Jovan Stojkovic, Esha Choukse, Chaojie Zhang, Inigo Goiri, and Josep Torrellas.
Towards greener llms: Bringing energy-efficiency to the forefront of llm inference.
arXiv preprint arXiv:2403.20306, 2024.
[SLBK24] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective
pruningapproach for large language models. In The Twelfth International Conference
on Learning Representations, 2024.
[SSC+22] Weiyan Shi, Ryan Shea, Si Chen, Chiyuan Zhang, Ruoxi Jia, and Zhou Yu. Just fine-
tune twice: Selective differential privacy for large language models. In Proceedings
of the 2022 Conference on Empirical Methods in Natural Language Processing, pages
6327–6340, 2022.
[SSDK+20] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Er-
mon, and Ben Poole. Score-based generative modeling through stochastic differential
equations. arXiv preprint arXiv:2011.13456, 2020.
85[SSU18] Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for
deep autoregressive models. Advances in Neural Information Processing Systems, 31,
2018.
[SWXL24] ZhenmeiShi,JunyiWei, ZhuoyanXu,andYingyuLiang. Whylargerlanguagemodels
do in-context learning differently? arXiv preprint arXiv:2405.19592, 2024.
[SZK+22] JohnSchulman,BarretZoph,ChristinaKim,JacobHilton, JacobMenick, JiayiWeng,
Juan Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, et al. Chatgpt:
Optimizing language models for dialogue. OpenAI blog, 2(4), 2022.
[SZKS21] Charlie Snell, Ruiqi Zhong, Dan Klein, and Jacob Steinhardt. Approximating how
single head attention learns. arXiv preprint arXiv:2103.07601, 2021.
[SZM+23] Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael
Jones, William Bergeron, Jeremy Kepner, Devesh Tiwari, and Vijay Gadepally. From
words to watts: Benchmarking the energy costs of large language model inference. In
2023 IEEE High Performance Extreme Computing Conference (HPEC), pages 1–9.
IEEE, 2023.
[TDC+23] Hans Thisanke, Chamli Deshan, Kavindu Chamith, Sachith Seneviratne, Rajith Vi-
danaarachchi, and Damayanthi Herath. Semantic segmentation using vision trans-
formers: A survey. Engineering Applications of Artificial Intelligence, 126:106669,
2023.
[TKYG20] HidenoriTanaka, Daniel Kunin,Daniel L Yamins, and SuryaGanguli. Pruningneural
networks withoutany data by iteratively conservingsynaptic flow. Advances in neural
information processing systems, 33:6377–6389, 2020.
[TLI+23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timoth´ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal
Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
arXiv:2302.13971, 2023.
[TSP+24] Szymon Tworkowski, Konrad Staniszewski, Miko laj Pacek, Yuhuai Wu, Henryk
Michalewski, and Piotr Mi lo´s. Focused transformer: Contrastive training for context
scaling. Advances in Neural Information Processing Systems, 36, 2024.
[VCC+17] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio,
and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903,
2017.
[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in
neural information processing systems, 30, 2017.
[WCY+23] Yuntao Wang, Zirui Cheng, Xin Yi, Yan Kong, Xueyang Wang, Xuhai Xu, Yukang
Yan, Chun Yu, Shwetak Patel, and Yuanchun Shi. Modeling the trade-off of privacy
preservation and activity recognition on low-resolution images. In Proceedings of the
2023 CHI Conference on Human Factors in Computing Systems, pages 1–15, 2023.
86[WCZ+23] Yilin Wang, Zeyuan Chen, Liangjun Zhong, Zheng Ding, Zhizhou Sha, and Zhuowen
Tu. Dolfin: Diffusion layout transformers without autoencoder. arXiv preprint
arXiv:2310.16305, 2023.
[WHHL24] Dennis Wu, Jerry Yao-Chieh Hu, Teng-Yun Hsiao, and Han Liu. Uniform memory
retrieval with larger capacity for modern hopfield models. In Forty-first International
Conference on Machine Learning (ICML), 2024.
[WHL+24] Dennis Wu, Jerry Yao-Chieh Hu, Weijian Li, Bo-Yu Chen, and Han Liu. STanhop:
Sparse tandem hopfield model for memory-enhanced time series prediction. In The
Twelfth International Conference on Learning Representations (ICLR), 2024.
[WMS+24] Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang, and Neel Joshi. Is
a picture worth a thousand words? delving into spatial reasoning for vision language
models. arXiv preprint arXiv:2406.14852, 2024.
[WSD+23] Zirui Wang, Zhizhou Sha, Zheng Ding, Yilin Wang, and Zhuowen Tu. Tokencompose:
Grounding diffusion with token-level supervision. arXiv preprint arXiv:2312.03626,
2023.
[WXZ+24] Yilin Wang, Haiyang Xu, Xiang Zhang, Zeyuan Chen, Zhizhou Sha, Zirui Wang, and
Zhuowen Tu. Omnicontrolnet: Dual-stage integration for conditional image genera-
tion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 7436–7448, 2024.
[WZG19] Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before
training by preserving gradient flow. In International Conference on Learning Repre-
sentations, 2019.
[XBK+15] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan
Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural im-
age caption generation with visual attention. In International conference on machine
learning, pages 2048–2057. PMLR, 2015.
[XCG+23] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming
Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large
language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023.
[XGH+21] Hu Xu, Gargi Ghosh, Po-Yao Huang, Prahal Arora, Masoumeh Aminzadeh,
Christoph Feichtenhofer, Florian Metze, and Luke Zettlemoyer. Vlm: Task-
agnostic video-language model pre-training for video understanding. arXiv preprint
arXiv:2105.09996, 2021.
[XGW+22] Xinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu Niu, Hua Wu, Haifeng Wang, and
Shihang Wang. Long time no see! open-domain conversation with long-term persona
memory. arXiv preprint arXiv:2203.05797, 2022.
[XHH+24] Chenwei Xu, Yu-Chao Huang, Jerry Yao-Chieh Hu, Weijian Li, Ammar Gilani, Hsi-
Sheng Goan, and Han Liu. Bishop: Bi-directional cellular learning for tabular data
withgeneralizedsparsemodernhopfieldmodel. InForty-first International Conference
on Machine Learning (ICML), 2024.
87[XJD+24] Yuhui Xu, Zhanming Jie, Hanze Dong, Lei Wang, Xudong Lu, Aojun Zhou, Amrita
Saha, Caiming Xiong, and Doyen Sahoo. Think: Thinner key cache by query-driven
pruning. arXiv preprint arXiv:2407.21018, 2024.
[XLS+23] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.
Smoothquant: Accurate and efficient post-training quantization for large language
models.InInternational ConferenceonMachineLearning,pages38087–38099. PMLR,
2023.
[XSL24] Zhuoyan Xu, Zhenmei Shi, and Yingyu Liang. Do large language models have com-
positional ability? an investigation into limitations and scalability. In ICLR 2024
Workshop on Mathematical and Empirical Understanding of Foundation Models, 2024.
[XSW21] Jing Xu, Arthur Szlam, and Jason Weston. Beyond goldfish memory: Long-term
open-domain conversation. arXiv preprint arXiv:2107.07567, 2021.
[XTC+23] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient
streaming language models with attention sinks. arXiv preprint arXiv:2309.17453,
2023.
[ZBKR24] Michael Zhang,KushBhatia, HermannKumbong,andChristopherR´e. Thehedgehog
& the porcupine: Expressive linear attentions with softmax mimicry. arXiv preprint
arXiv:2402.04347, 2024.
[ZHDK23] Amir Zandieh, Insu Han, Majid Daliri, and Amin Karbasi. Kdeformer: Accelerating
transformers via kernel density estimation. In International Conference on Machine
Learning, pages 40605–40623. PMLR, 2023.
[ZHJL24] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models
for vision tasks: A survey. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2024.
[ZJV+24] ChenyangZhao, XueyingJia, Vijay Viswanathan, TongshuangWu, andGraham Neu-
big. Self-guide: Better task-specific instruction following via self-synthetic finetuning.
arXiv preprint arXiv:2407.12874, 2024.
[ZKAW23] Jieyu Zhang, Ranjay Krishna, Ahmed H Awadallah, and Chi Wang. Ecoassistant:
Using llm assistant more affordably and accurately. arXiv preprint arXiv:2310.03046,
2023.
[ZKV+20] Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank
Reddi, Sanjiv Kumar, and Suvrit Sra. Why are adaptive methods good for attention
models? Advances in Neural Information Processing Systems, 33:15383–15393, 2020.
[ZLC+21] Zixiao Zhang, Xiaoqiang Lu, Guojin Cao, Yuting Yang, Licheng Jiao, and Fang
Liu. Vit-yolo: Transformer-based yolo for object detection. In Proceedings of the
IEEE/CVF international conference on computer vision, pages 2799–2808, 2021.
[ZLD+24] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and
TatsunoriBHashimoto. Benchmarkinglargelanguagemodelsfornewssummarization.
Transactions of the Association for Computational Linguistics, 12:39–57, 2024.
88[ZSZ+24] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai,
Zhao Song, Yuandong Tian, Christopher R´e, Clark Barrett, et al. H2o: Heavy-hitter
oracle for efficient generative inference of large language models. Advances in Neural
Information Processing Systems, 36, 2024.
[ZTT+22] Bowen Zhang, Zhi Tian, Quan Tang, Xiangxiang Chu, Xiaolin Wei, Chunhua Shen,
et al. Segvit: Semantic segmentation with plain vision transformers. Advances in
Neural Information Processing Systems, 35:4971–4982, 2022.
89