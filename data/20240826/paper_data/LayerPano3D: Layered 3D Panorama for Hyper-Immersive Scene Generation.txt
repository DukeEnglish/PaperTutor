1
L P 3D: Layered 3D Panorama for
AYER ANO
Hyper-Immersive Scene Generation
Shuai Yang∗, Jing Tan∗, Mengchen Zhang, Tong Wu(cid:0), Yixuan Li, Gordon Wetzstein, Senior
Member, IEEE, Ziwei Liu, Member, IEEE, and Dahua Lin(cid:0)
Abstract—3Dimmersivescenegenerationisachallengingyetcriticaltaskincomputervisionandgraphics.Adesiredvirtual3Dscene
should1)exhibitomnidirectionalviewconsistency,and2)allowforfreeexplorationincomplexscenehierarchies.Existingmethods
eitherrelyonsuccessivesceneexpansionviainpaintingoremploypanoramarepresentationtorepresentlargeFOVscene
environments.However,thegeneratedscenesuffersfromsemanticdriftduringexpansionandisunabletohandleocclusionamong
scenehierarchies.Totacklethesechallenges,weintroduceLAYERPANO3D,anovelframeworkforfull-view,explorablepanoramic3D
scenegenerationfromasingletextprompt.Ourkeyinsightistodecomposeareference2Dpanoramaintomultiplelayersatdifferent
depthlevels,whereeachlayerrevealstheunseenspacefromthereferenceviewsviadiffusionprior.LAYERPANO3Dcomprises
multiplededicateddesigns:1)weintroduceanoveltext-guidedanchorviewsynthesispipelineforhigh-quality,consistentpanorama
generation.2)WepioneertheLayered3DPanoramaasunderlyingrepresentationtomanagecomplexscenehierarchiesandliftitinto
3DGaussianstosplatdetailed360-degreeomnidirectionalsceneswithunconstrainedviewingpaths.Extensiveexperiments
demonstratethatourframeworkgeneratesstate-of-the-art3Dpanoramicsceneinbothfullviewconsistencyandimmersiveexploratory
experience.WebelievethatLAYERPANO3Dholdspromiseforadvancing3Dpanoramicscenecreationwithnumerousapplications.
IndexTerms—3DSceneGeneration,PanoramaGeneration,DiffusionModels,NeuralRendering
✦
1 INTRODUCTION driftissue:longsequentialsceneexpansioneasilyproduces
incoherent results as the out-paint artifacts accumulate
THedevelopmentofspatialcomputing,includingvirtual
through iterations, hampering the global consistency and and mixed reality systems, greatly enhances user en-
harmonyofthegeneratedscene.
gagement across various applications, and drives demand
Another branch of methods [10], [11], [12], [13], [14],
for explorable, high-quality 3D environments. We contend
[15] employs Equirectangular Panorama to represent 360◦,
thatadesiredvirtual3Dsceneshould1)exhibithigh-quality
large field of view (FOV) environments in 2D. However,
and consistency in appearance and geometry across the
full 360◦ ×180◦ view; 2) allow for free exploration among the absence of large-scale panoramic datasets hinders the
capabilityofpanoramagenerationsystems,resultinginlow-
complex scene hierarchies with clear parallax. In recent
resolutionimageswithsimplestructuresandsparseassets.
years, many approaches in 3D scene generation [1], [2], [3]
Moreover, 2D panorama [10], [11], [13] does not allow for
wereproposedtoaddresstheseneeds.
free scene exploration. Even when lifted to a panoramic
One branch of works [4], [5], [6], [7], [8], [9] seeks
scene [16], the simple spherical structure fails to provide
to create extensive scenes by leveraging a “navigate-and-
complex scene hierarchies with clear parallax, leading to
imagine” strategy, which successively applies novel-view
occluded spaces that cause blurry renderings, ambiguity,
rendering and outpaints unseen areas to expand the scene.
andgapsinthegenerated3Dpanorama.Somemethods[17]
However, this type of approaches suffer from the semantic
typicallyuseinpainting-baseddisocclusionstrategytofillin
theunseenspaces,buttheyrequirespecific,predefinedren-
• S.YangiswithShanghaiJiaoTongUniversityandShanghaiAILabora-
dering paths tailored for each scene, limiting the potential
tory,Shanghai201203,China.
E-mail:yang shuai@sjtu.edu.cn forfreeexploration.
• J.Tan,T.WuandY.LiarewiththeMultimediaLaboratory,theChinese To this end, we present LAYERPANO3D, a novel frame-
UniversityofHongKong,HongKongSAR. work that leverages Multi-Layered 3D Panorama for full-
E-mail:{tj023,wt020,ly122}@ie.cuhk.edu.hk
viewconsistentandfreeexploratoryscenegenerationfrom
• M. Zhang is with Zhejiang University, Zhejiang and Shanghai AI
Laboratory,Shanghai,China. text prompts. The main idea is to create a Layered 3D
E-mail:zhangmengchen@zju.edu.cn Panorama by first generating a reference panorama and
• G.WetzsteiniswiththeDepartmentofElectricalEngineering,Stanford
treatingitasamulti-layeredcomposition,whereeachlayer
University,Stanford,CA,UnitedStates.
E-mail:gordon.wetzstein@stanford.edu depictsscenecontentataspecificdepthlevel.Inthisregard,
• Z.LiuiswithS-Lab,NanyangTechnologicalUniversity,Singapore. it allows us to create complex scene hierarchies by placing
E-mail:ziwei.liu@ntu.edu.sg occludedassetsindifferentdepthlayersatfullappearance.
• D.LiniswiththeMultimediaLaboratory,theChineseUniversityofHong
Our contributions are two-fold. First, to generate high-
Kong,HongKongSAR,andShanghaiAILaboratory,Shanghai,China,
andCentreofPerceptualandInteractiveIntelligence,HongKongSAR. qualityandcoherent360◦×180◦ panoramas,weproposea
E-mail:dhlin@ie.cuhk.edu.hk novel text-guided anchor view synthesis pipeline. By fine-
*:equalcontribution,(cid:0):correspondingauthor. tuning a T2I model [18] to generate 4 orthogonal perspec-
4202
guA
32
]VC.sc[
1v25231.8042:viXra2
Reference Panorama
A vibrant city avenue,
bustling traffic,
towering skyscrapers,
Cartoon style
Fig.1.OverviewofLAYERPANO3D.Guidedbysimpletextprompts,LAYERPANO3Dleveragesmulti-layered3Dpanoramatocreate
hyper-immersivepanoramicscenewith360◦×180◦ coverage,enablingfree3Dexplorationamongcomplexscenehierarchies.
tive views as anchors, we prevent semantic drifts during 3Dgeneration [20].Thesetechnologieshaverevolutionized
panorama generation, while ensuring a consistent horizon digital content creation in fields like entertainment, design,
level across all views. Furthermore, the anchor views en- andvirtualreality.
richthepanoramabyincorporatingcomplexstructuresand
detailed features derived from large-scale, pre-trained per- 2.1 3DRepresentation
spective image generators. Second, we introduce the Lay-
Early neural rendering methods [21], [22], [23], [24] typi-
ered 3D Panorama representation as a general solution to
cally incorporate a multi-layer perceptron (MLP) to model
handleocclusionfordifferenttypesofsceneswithcomplex
3D scenes as continuous functions. NeRF [25], being the
scene hierarchies, and lift it to 3D Gaussians [19] to enable
representative MLP-based method, achieves state-of-the-
free 3D exploration. By leveraging pre-trained panoptic
art novel view synthesis performance due to the volume
segmentation prior and K-Means clustering, we streamline
rendering and inductive bias nature of MLPs. However, as
an automatic layer construction pipeline to decompose the
theMLPneedstoevaluateonlargeamountofpointsalong
reference panorama into different depth layers. The un-
everycameraray,therenderingprocessbecomesextremely
seen space at each layer is synthesized with a finetuned
exhaustive.Subsequentmethods[19],[26],[27],[28],[29]im-
panoramainpainter[11].
proveonfastertrainingandrenderingbyemployingsparse,
Extensive experiments demonstrate the effectiveness
advanced scene representation. Among these, 3D Gaussian
of LAYERPANO3D in generating hyper-immersive lay-
Splatting (3DGS) [19] has recently emerged as a rising star
ered panoramic scene from a single text prompt. LAY-
withtransformativereal-timerenderingandhigh-definition
ERPANO3D surpasses state-of-the-art methods in creating
resolutions.Itexplicitlyparameterizesthe3Dscenewith3D
coherent,plausible,text-aligned2Dpanoramaandfull-view
Gaussians,whereeachGaussianisoptimizedtorepresenta
consistent,explorable3Dpanoramicenvironments.Further-
volume and projected to 2D for rasterization. The discrete
more, our framework streamlines an automatic pipeline
nature of 3D Gaussians enables flexible scene editing and
without any scene-specific navigation paths, providing
composition for 3DGS. Therefore, our approach employs
more user-friendly interface for non-experts. We believe
3DGS as scene representation to facilitate multi-layer scene
thatLAYERPANO3Deffectivelyenhancestheaccessibilityof
creation.
full-view,explorableAIGC3Denvironmentsforreal-world
applications.
2.2 3DSceneGeneration
Duetotherecentsuccessofdiffusionmodels,3Dscenegen-
2 RELATED WORKS
erationhasalsoachievedsomedevelopment.Scenescape[7]
In recent years, advancements in AI, particularly deep and DiffDreamer [30], for example, explore perpetual view
learning, along with increased computational power and generation through the incremental construction of 3D
large datasets, have driven rapid development in 2D and scenes. One major branch of work employ step-by-step3
inpaintingfrompre-definedtrajectories.Text2Room[6]cre- process. In Stage III (Sec. 3.3), the Layered 3D Panorama
ates room-scale 3D scenes based on text prompt, utiliz- is lifted to 3D Gaussians in a cascaded manner to enable
ing textured 3D meshes for scene representation. Similarly, free3Dexploration.
LucidDreamer [4] and WonderJourney [5] can generate
domain-free3DGaussiansplattingscenesfromiterativein-
3.1 ReferencePanoramaGeneration
painting.However,thislineofworkoftensufferfromthese-
manticdriftissue,resultinginunrealisticscenefromartifact Text-Guided Anchor View Synthesis. We start by gener-
accumulationandinconsistentsemantics.Whilesomeother ating four orthogonal anchor views to establish the fun-
approaches[3],[31],[32]endeavortointegrateobjectswith damental geometric structure and visual appearance. This
environments, they yield relatively low quality of compre- is achieved by fine-tuning a pre-trained perspective T2I
hensive scene generation. Recently, our concurrent works, diffusionmodel.
DreamScene360 [16] and HoloDreamer [17] also employ For data preparation, we construct a high-quality RGB
panoramaaspriortoconstructpanoramicscenes.However, panoramadatasetconsistingof911outdoorimagessourced
they only achieve the 360◦ ×180◦ field of view at a fixed from the web and 9684 images from Matterport3D [38].
viewpoint based on a single panorama of low-quality and For each panorama, we use llava-v1.5-7b [39] to generate
simple structure, and do not support free roaming within captionsastextpromptsandprojectfourorthogonalviews
the scene. In contrast, our framework leverages Multi- without overlap at a fixed FOV (60◦), elevation (0◦) and
Layered 3D Panorama representation to construct high- four azimuths (0◦,90◦,180◦,270◦). This setting can effec-
quality, fully enclosed scenes that enable unconstrained tively avoid perspective distortion and fisheye effect, and
navigationpathsin3Dscene. allow further refinement of consistency in the subsequent
synthesisstages.Additionally,wearrangethesefourviews
intoa2×2gridimageinafixedorderfortheeaseoftraining
2.3 PanoramaGeneration
anddrawawhitecross-linewithacertainpixelsizebetween
PanoramagenerationmethodsareoftenbasedonGANsor thegridsasasplit.
diffusionmodels.Earlyinthisfield,withthedifferentforms Compared with object-level multi-view generation, it
ofdeepgenerativeneuralnetworks,GAN-basedpanorama is harder for scene-level generation to diffuse more views
generationmethodsexploremanypathstoimprovequality with high quality and consistency in a single pass. Multi-
and diversity. Among them, Text2Light [15] focuses on view cross-attention-based methods [10], [11] learn high-
HDR panoramic images by employing a text-conditioned levelsemanticsattheexpenseofoverfittingtomonotonous
global sampler alongside a structure-aware local sampler. patterns due to the scarcity of panorama data. Other ap-
However,trainingGANsischallengingandtheyencounter proaches [1] require huge computation and time in both
the issue of mode collapse. Recently, some studies have trainingandinferencestagesandgeneraterelativelysimple
utilized diffusion models to generate panoramas. MVDif- scenes with limited assets. Therefore, inspired by [40], we
fusion [10] generates eight perspective views with multi- directlyfine-tuneStable-Diffusion-XL[18]toefficientlygen-
branch UNet but the resulting closed-loop panorama only erate high-quality anchor views as image grids for subse-
captures the 360◦ × 90◦ FOV. The image generated from quent panorama generation, free of complex regularization
MultiDiffusion [33] and Syncdiffusion [34] is more like a techniques.Thistext-guidedanchorviewsynthesispipeline
long-range image with wide horizontal angle as they do can scale positively with the potency of the underlying
not integrate camera projection models. PanoDiff [35] can T2I model and unlock the capability of the base model
generate 360◦ panorama from one or more unregistered without additional knowledge through lightweight fine-
NarrowField-of-View(NFoV)imageswithposeestimation tuning. Furthermore, unlike the clean background in the
and controlling partial FOV LDM, while the quality and object-levelgeneration,scenebackgroundsareoftendiverse
diversityofresultsarelimitedbythescarcityofpanoramic andcomplex.Thus,insteadofsimplyinitializingeachsub-
image training data like most other methods [14], [36], gridwitha2DGaussianblobasiscommonlydoneinobject-
[37]. In contrast, our model can generate Multi-Layered 3D based multi-view approaches, we initialize the image grids
Panorama for immersive, high-quality, and coherent scene with random colored noise and draw a white cross-line
generationfromtextprompts. between the grids during the inference process, mimicking
thetrainingdata.
EquirectangularPanoramaSynthesis.EquirectangularPro-
3 METHOD
jection (ERP) is a method that maps a 3D sphere onto
Thegoalofourworkistocreateapanoramicsceneguided a 2D map. In ERP, the lines of longitude are mapped to
by text prompts, that encompasses a complete 360◦×180◦ verticallineswithconstantspacing,andthelinesoflatitude
field of view from various viewpoints within an extensive are mapped to horizontal lines with constant spacing. This
range in the scene, while allowing for unconstrained tra- mapping relationship is neither area-preserving nor angle-
jectory for immersive exploration. LAYERPANO3D consists preservingandthemappingoflatitudeleadstostrongdis-
threestages.InStage I(Sec.3.1),weproposeatext-guided tortionsinthepolarregions.Toobtainadetailed,consistent
anchorviewsynthesispipelinepairedwithpanoramicout- andplausiblepanorama,wefirstgeneratea360◦×90◦FOV
painting to generate high-quality, consistent panorama as panoramatoobtaintheprinciplestructureandcontent,then
reference.InStageII(Sec.3.2),withthereferencepanorama, successivelyextenditto360◦×180◦ FOVpanorama.
we construct our Layered 3D Panorama representation by Withthetext-guidedanchorviewsynthesis,wehavethe
iterative layer decomposition, completion and alignment anchor perspective images [Ii ]4 ∈ Rh×w×3 of a scene
anc i=14
Multi-layer Panorama Construction Panoramic 3D Scene Optimization Created 3D Scene
Layer 0
Pano Base
Inpaint Gaussian
Gaussian
Selector
Layer 1
Pano Layer
Inpaint Gaussian
Gaussian
Layer 2 Selector
Pano Layer
Inpaint Gaussian
... G Seau les cs ti oan
r
Layer N
Layer Layer
Decompose Gaussian
PanoramaDepth Estimation Layer Alignment
Reference Panorama Generation Input Text
2D Panorama (360×180 FOV) 2D Panorama (360×90 FOV) A coastal road with a sign
pointing to a
lighthouse, waves
crashing on the
shore,
Oil Painting
style.
Fig.2. Pipeline Overview of LAYERPANO3D. Our framework consists of three stages, namely reference panorama generation,
multi-layer panorama construction and panoramic 3D scene optimization. LAYERPANO3D streamlines an automatic generation
pipelinewithoutanymanualeffortstodesignscene-specificnavigationpathsforexpansionorcompletion.
capturedbyfourorthogonalcamerainthehorizontalplane. Following [14], [33], we adopt a circular blending strategy
Basedonthemappingruleand60◦FOV,theseanchorviews in both denoising and VAE decoding stage, as illustrated
canbedirectlyprojectedtoformtheincomplete360◦×60◦ in Figure 3. A circular closed loop is created by extending
FOV panorama without overlap. Here, we assume that the rightmost side of the panorama with the pixels in the
the elevation angle of the anchors are ϕ anc = 0◦ with leftmost parts. Per-pixel adaptive weights W k are utilized
azimuthangles{θi }={0◦,90◦,180◦,270◦}.Then,weset toblendthedenoisedpatchesmappedfromextensionsand
anc
a viewpoint sequence with {(θ j,ϕ j)}n
j=1
that allows for a from the leftmost parts to generate the leftmost panorama.
360-degree rotation around the scene in 90◦ FOV to project Similarly,intheVAEtileddecodingstage,toavoidtilingar-
theperspectiveviewsIˆ j andmasksM j: tifacts,weensurethateachtileoverlapsandblendstogether
[Iˆ j,M j]=ERP−1(Pano,[θ j,ϕ j,FOV]), (1) t do ecf oo drm er.a Ts hm ero eo foth re,o eu atp chut pd ixu ee loto fte ha ec ph at nil oe rau msi ang isa cod mif pfe ur te en dt
where the ERP−1 is the inverse ERP from panorama to asaweightedaverageofallitsmappedpatchupdatesZ k,t
perspective views in 3D. We further apply a text-guided ateachdenoisingstep.
inpainter F inpaint [41] to synthesize the missing pixels in F−1 (W )
t th he at,m Iˆa js ik se ad ddre itg ioio nn as llyM pj reo -f prth oce ei sn seco dm wp il te hte Lav Mie aw [s 42Iˆ ]j. beN foo rte
e
Ψ(P t)=(cid:88)
k
(cid:80) mFm ma −p a1,k p,m(k
W m)
⊙F m− a1 p,k(Φ(Z k,t)), (3)
i cn op na tein xt ti un ag lkto noc wle la en dgu ep : the masked pixel distribution with whereF m− a1 p,k isimagespacemappingfrompatchZ k toits
corresponding latent in P; Ψ and Φ denote the diffusion
I j =F inpaint(LaMa(Iˆ j,M j)). (2) process at panorama-level and patch-level. Finally, we can
get a high-quality result as reference for the subsequent
Throughiterativeprojectionandinpainting,thePano
360×60
layeredpanoramaconstruction.
isextendedtoPano 360×90.
TheextensionfromPano 360×90 toPano 360×180 ismore
3.2 Multi-LayerPanoramaConstruction
difficult,asthepolarregionsexhibitmostdistortedpatterns
that cannot simply be synthesized through perspective im- We introduce the Layered 3D Panorama representation
age inpainting. Therefore, we fine-tune [43] on panoramic based on the following assumption: “an enclosed 3D scene
image-caption pairs for polar outpainting. Another chal- contains a background and various assets positioned in front of
lenge is to enforce continuity between the leftmost and it”.Inthisregard,usingLayered3DPanoramafor3Dscene
rightmost sides of the synthesized image during outpaint- generation is a general approach to handle occlusion for
ing, as it is a crucial characteristic [44] of 360◦ panoramas. varioustypesofscene.Tocreateacompletescenefromthe
Panorama
Outpaint
eludoM
RS
Panorama
Synthesis
Equuirectangular Multi-View
Diffusion5
distant objects, maintaining their visual clarity and texture
Circular Blending detailsevenwhenobservedfromacloserperspective.
Layer Alignment. Given the Layered 3D RGB Panorama
[P l]N l=0, we perform the depth prediction and alignment to
ensure consistency in a shared space. To begin with, we
apply the 360MonoDepth [50], to first estimate the layer
N (reference panorama) as the reference depth PN . The
depth
W = 2H Extend Block predicted depth P depth inherently contains extremely small
values from normalization, causing peak-like artifacts un-
Adaptive Weights
0 1 der the reference camera position. Consequently, we post-
Fig. 3. The circular blending Strategy. During the inference
processthedirectlyoutputdisparitymapPˆ
dispwiththefol-
process, to synthesize a 360◦ seamless panorama, adaptive lowingtransformationtomitigatethegeometrydistortion:
weights are utilized to blend the leftmost part of the latent with (cid:16) (cid:17)
therightmostpartindenoisingandVAEdecodingstage. D = max(Pˆ )−min(Pˆ ) /4,
bias disp disp
referencepanorama,weproposetodecomposeitintoN+1 Pˆ =max(Pˆ )−Pˆ +D , (4)
layers along the depth dimension. As shown in Figure 2, depth disp disp bias
P =Pˆ / max(Pˆ ).
these layers, arranged from farthest to nearest, represent depth depth depth
boththescenebackground(layer0)andthelayoutssituated
Toalignthelayerdepthin3Dspace,wefinditinfeasible
behindtheobservationpoint.
tosimplycomputeaglobalshiftandscaleasin [4],[6]due
Layer Decomposition. As shown in Figure 2, the reference
tothenonlinearnatureofERP.Therefore,weleveragedepth
panorama is decomposed by first identifying the scene inpainting model F depth from [51] to directly restore depth
assets and then cluster these assets in different layers ac- valuesbasedontheinpaintedRGBpixels.F depth harnesses
cordingtodepth.First,weemployanoff-the-shelfpanoptic
strong generalizability from large-scale diffusion prior and
segmentation model [45] pretrained on ADE20K [46] to
synthesizesinpainteddepthvaluesatanalignedscalewith
automatically find all scene assets visible in the reference
the base depth. We start from reference panoramic depth
panorama. A good layer decomposition requires that the PN toimplementstep-by-steprestorationfromlayerN−
depth
layerassetsshareasimilardepthlevelwithinlayersandare 1tolayer0:
distant from assets in other layers. In this sense, we assign
eachassetadepthvalueandapplyK-Meanstoclusterthese P dl
epth
=F depth(P l,M l⊙P dl+ ep1 th), (5)
masks into different groups. Given the reference panorama
depth map, the depth value for each asset mask is deter- where, in layer l, the inpainted panorama P l and masked
minedbycalculatingthe75thpercentileofthedepthvalues depthmapM l⊙P dl+ ep1 th areprovidedasinputstoF depth for
restoration.
within the masked region. According to the depth values,
theassetsareclusteredintoN groupsfromlayer0toN−1
and are merged into layer masks to guide the subsequent 3.3 Panoramic3DGaussianSceneOptimization
layercompletion.
3D Scene Initialization. To enable free 3D exploration, we
Layer Completion. With the layer mask, we focus on com-
lift the Layered 3D Panorama to 3D Gaussians [19], where
pleting the unseen content caused by asset occlusion. In
theGaussiansareinitializedfromthelayered3Dpanoramic
order to synthesize background pixels instead of creating
point clouds. Considering the intrinsic spherical structure
new elements, we finetune PanFusion [11] with the ERP-
of panorama, we can easily transform an equirectangular
perspective aware cross-attention as the panoramic back- image P ∈ RH×W×3 into 3D point cloud S(θ,ϕ,P depth).
groundinpainter.Specifically,ateachlayer,ourmodeltakes
Eachpixel(u,v)isrepresentedasa3Dpointandtheangles
thelayermaskM l,thereferencepanorama,andthe“empty
θ,ϕarecomputedasθ =(2u/W−1)π,ϕ=(2v/H−1)π/2.
scene, nothing” [47] prompt as input, and output co-
Then, the corresponding 3D coordinates (X,Y,Z) from
herentcontentatthemaskedarea.Theinpaintedpanorama
thedepthvalueP depth(θ u,ϕ v)arederivedasfollows:
at layer l is denoted P l and is used as supervision to
thesubsequentpanoramic3DGaussiansceneoptimization. X =P depth(θ u,ϕ v)cosϕ vcosθ u,
Note that, we additionally apply SAM [48] to extend the Y =P depth(θ u,ϕ v)sinϕ v, (6)
layer mask, based on the inpainted panorama from the
Z =P (θ ,ϕ )cosϕ sinθ .
depth u v v u
previouslayer,toeliminateunwantednewgenerationsfrom
inpainting. Basedonthistransformation,wecanextractthepointcloud
Moreover, to enable free rendering in 3D, where ob- foreachlayerpanoramatoinitialize3DGaussians.
servers can examine scenes from varying distances, the Drastic depth changes at layout edges introduce noisy
unprocessed textures of distant assets may appear blurred stretched outliers that would turn into artifacts during
astheobserverapproaches.Therefore,distantlayersrequire scenerefinement.Therefore,weproposeanoutlierremoval
higher resolution to preserve texture details at different module that specifically targets stretched point removal
viewpoints. To address this, Super Resolution (SR) mod- usingheuristicpointcloudfilteringstrategies.Asstretched
ule [49] is employed to enhance the resolution of the lay- points are usually sparsely distributed in space, we design
ered panorama from layer 0 (background layer) to layer N the point filtering strategy based on its distance from the
(referencepanorama),achievinga2×upscaleinresolution. neighbors. First, we filter out all points with the minimum
SR processing significantly improves the texture quality of distancetoneighborsoverthresholdβ 1.Then,weeliminate
H6
𝐠%
𝐩%
𝐠&
𝐩& 𝑑’ 𝐠𝟏<𝑑 𝐩"
Layer 0
𝑑’𝐠!>𝑑𝐩!
𝐨
Layer N
𝑑!"#,%𝑑!"#,& a
f
arc cot tzi iv vee
en
g pga oau
iu
ns
s
ts
s
i cia
a
ln
on
ud
𝐩’
𝐠’
𝑑’𝐠"<𝑑𝐩"
(a) (b) (c) (d)
Fig.4.IllustrationoftheGaussianSelector.Giventhenewassetpointcloud,theGaussianSelectoridentifiestheactiveGaussiansfornext
layer’soptimization.(a)OptimizedGaussiansatlayerlpartiallyblocksnewassetpointcloudinlayerl+1.(b)Bycomputingtheviewingdirections
andthedistancetocamera,GaussianSelectoridentifiestheblockingGaussians.(c)ThemarkedGaussiansarelabeledasactive,andmerged
withthenewpointsforoptimization.(d)ShowedaspecialcasewheretheoptimizedGaussiancenterisbehindthenewpoints,butalsoidentifies
asanactiveGaussianbecauseitsvolumescaleoverlapswiththepoints(case1).
pointswithveryfewneighbors.Theideaistocalculatethe where s is the scaling of the Gaussian g. If d g1 > d p1
numberofneighborsofeachpointwithinagivenradiusand and d′ g1 < d p1 as in Figure 4(d), the referred Gaussian is
drop the points where their number of neighbors is below also marked active. For efficient memory storage and fast
thresholdβ 2.Tospeedupthecalculation,wemapthepoints look-up, we hash the distance vectors into a 3D grid. The
into3Dgrids,thenremoveallpointswithingridsthathave mapping function from vector coordinates to grid indices
lessthanβ 2 numberofneighbors. writes:f(p)=ceil(β 3log(p+1)).
3D Scene Refinement.Duringscenerefinement,wedevise
two types of Gaussian training schemes for varying scene
content:thebaseGaussianforreconstructingthesceneback-
4 EXPERIMENTS
groundandthelayerGaussianforoptimizingscenelayouts. 4.1 ImplementationDetails
Additionally, a Gaussian selector module is introduced be-
In the anchor view synthesis stage, we train the model
tweenlayerGaussianstofacilitatescenecomposition.
startingfromSDXL-base-1.0[18]withabatchsizeof16and
In scene refinement, the base Gaussian model is ini-
learningrateof10−5 foronly10Kiterationsonthe11kdata
tialized on a whole of the background point cloud, and
pairs.Thetrainingisdoneusing4NVIDIAA100GPUsfor
the layer Gaussian model initiates on and optimizes the
7 hours. For inference, we initialize four 512 × 512 grids
foreground assets. In practice, we project the layer mask
with random colored noise, subsequently superimposing
M l ontopointcloudsandusethemaskedpointstoinitiate
a white cross-line with a pixel width of 10 to ensure that
Gaussians. The optimized Gaussians from previous layers
the final inference result can be accurately segmented into
arefrozentoavoidunwantedmodification.Inthisway,the
four grids of size 507 × 507. In the reference panorama
scenebackgroundisoptimizedonceinthebaseGaussianto
synthesisstage,weleveragetheStableDiffusionInpainting
reduceunnecessarycomputationandconflictsofGaussians
pipeline [41] from huggingface [52] as the inpainter and
insubsequentlayers.
repurposedDiffusion360[43]astheoutpaintertosynthesize
We observe that the quality of the optimized scene
360◦×180◦ FOVpanorama.
is easily hampered by unaligned layers, and sometimes
In the layered panorama construction stage, we employ
F depth does fail to produce perfectly aligned layer depths.
OneFormer [45] to obtain the panoptic segmentation map
Gaussiansatlayerlcouldspanintounwanteddepthlevels
forthereferencepanorama.Backgroundcategoriesareman-
and block assets in the subsequent layer, as illustrated in
ually determined (i.e. sky, floor, ceiling, etc.) to filter out
Figure4(a).Tohandlethisissue,weintroducetheGaussian
backgroundcomponentsinassetmasks.Generally,weclus-
selector module to detect these conflicted Gaussians, re-
ter all asset masks into N = 3 layers via KNN and merge
activate them from frozen, and optimize them away from
all masks within each layer to form a unified layer mask.
theblockage.First,theselectorcomputesthedistancevector
With the obtained layer mask, we combine our fine-tuned
fromthecameracentero=(0,0,0)toeachnewpointp,as
panorama inpainting model with LaMa [42] to achieve
in Figure 4(b). The absolute distance from asset points p
multi-layer completion and apply 360MonoDepth [50] to
and scene Gaussians g to the camera is denoted as d p and
predictthereferencepanoramadepth.
d g respectively:
Inthe3Dpanoramicsceneoptimizationstage,weliftthe
d p =||p−o|| 2, d g =||g−o|| 2. (7) panorama RGBD into 3D point clouds. For scene initializa-
ByexaminingallGaussiansthatonthesameraywithasset tion, we set β 1 to 0.0001 and β 2 to 4 based on empirical
practice. These point clouds are used to initialize the base
pointsbutatacloserdistance:p/d
p
= g /d g, d
g
< d p,
Gaussian model and the layer Gaussian model. During
we mark them as active (Figure 4(c)). We also observe
the scene refinement stage, we optimize the base Gaussian
that despite the Gaussian center being behind the asset
model for 3,000 iterations, then the layer Gaussians each
points, if the scaling of the Gaussian covers the point, then
for 2,000 iterations. The training objective for base and
the Gaussian is still affecting the overall rendering quality.
layerGaussianistheL1lossandD-SSIMtermbetweenthe
Hence,weupdatethegaussiandistanceas
ground-truthviewsandtherenderedviews.Weuseasingle
d′ =d′ −max{s(g)}, (8) 80G A100 GPU for reconstruction and the reconstruction
g g7
Text2light
PanFusion
LDM3D-
pano
Diffusion360
Ours
Cartoon Style Van Gogh Style Black and white color painting Style Oil painting Style
A vibrant city avenue, bustling traffic, towering skyscrapers
Fig. 5. Qualitative comparisons in Panorama Generation. We compare 2D panorama generation methods across three
dimensions:Creativity (red),Resolution(green),andSphericalStructure(blue).LAYERPANO3Dproduceshigh-resolution,creative,
andconsistentpanoramaswhileaccommodatingmulti-stylegenerationrequirements.
timeforeachlayeris1.5minutesonaveragefor1024×1024 sourceitsofficialcode,wereproduceitsmodelforcompar-
resolutioninputs. ison.
4.3 QualitativeComparison
4.2 ComparisonMethods.
2D Panorama Generation.Weshowsomequalitativecom-
Toevaluatetheperformanceofourapproachinthecontext parisons with several state-of-the-art panorama generation
oftext-driven3Dpanoramicscenegeneration,Wecompare works in Figure 5. Text2light ignores style prompts due
with previous methods in two phases: 2D Panorama Gen- to being trained on a realistic HDRI dataset based on the
eration and 3D Panoramic Scene Reconstruction. For 2D VQGAN structure, and the components in the generated
PanoramaGeneration,wechoosefourapproachestocheck panorama are relatively simple. The results by PanFusion
the quality and creativity of 2D panorama: Text2light [15] areambiguousandlowinquality,whileitcanadapttostyle
adopts the VQGAN [53] structure to synthesize an HDR changes due to its operation in panorama and perspective
panorama image from text in a two-stage auto-regressive domains. LDM3D-pano is insensitive to style prompts and
manner. Panfusion [11] designs a novel dual-branch dif- the results are low-resolution. The instances generated by
fusion model to generate a 360-degree image from a Diffusion360 appear to be of higher quality compared to
text prompt. Diffusion360 [43] directly uses the Dream- theabove,butdonothavetheinherentsphericalstructural
Booth[54]trainingonthepanoramadataset[55].LDM3D- propertyofpanoramas,andthestyleseemstobethesame.
pano [12] extends LDM3D [56] to generate a realistic RGB Thisisbecausethemodelismerelytrainedonthepanorama
panorama and its corresponding depth. For 3D Panoramic dataset based on Dreambooth [54] without injecting any
Scene Reconstruction, we select three methods to compare prior knowledge. Our method achieves the highest quality
the 3D performance of scenes: LucidDreamer [4] starts both in consistency and diversity, presenting creative and
from a single image and a text prompt to create a 3D-GS reasonablegenerations.
scene. Text2Room [6] generates a textured 3D mesh scene 3D Panoramic Scene Reconstruction. To more precisely
with preset camera trajectory. Dreamscene360 [16] creates evaluate the quality of 3D panoramic scene reconstruction,
panoramic 3DGS scene with full 360◦ coverage from text we present qualitative comparisons with Text2Room [6],
prompts. Note that, since DreamScene360 does not open- LucidDreamer [4], and DreamScene360 [16] across two di-8
Rendered Panorama View 1 View 2 View 3 View 4
A vibrant city
avenue, bustling
traffic, towering
skyscrapers,
Cartoon style
Fig.6.Qualitativecomparisonsinfull360°×180°Scene.Wecomparethepanoramaandmultipleviewsofthescenegenerated
byfourmethods. LAYERPANO3D exhibitsmoreconsistent contentwithfull360◦×180◦coverage.
mensions.Forfull360◦×180◦viewconsistency,weinputan 4.4 QuantitativeComparison
imagewithatextprompttoguidescenecreationandrender
2D Panorama Generation. We adopt three metrics for
multiple views from the center of the scene. This approach
quantitativecomparisons:1)Intra-Style[34],[57]evaluates
synthesizes the texture of the panoramic environment, al-
the coherence of the generated panoramas; 2) FID [58]
lowingforamoreintuitiveassessmentaftergeneration.As
evaluatesbothfidelityanddiversity;3)CLIP[59]measures
showninFigure6,LucidDreamer[4]andText2room[6]fail
thecompatibilityofresultsandinputprompts.Weselected
to cover the full 360◦ × 180◦ view, resulting in semantic
73 prompts to generate panoramas for each method and
incoherenceandartifactsduetotheirsuccessiveinpainting-
averagedtheresultsforallmetrics.Moreover,auserstudyis
basedstrategy.DreamScene360[16]supportsa360◦×180◦
alsoconductedtofurtherevaluatethequalityofpanoramas,
view at a single fixed viewpoint, but it deviates from the where we project 4 views at a fixed FOV (90◦) to the user
inputconditions,andthequalityofthegeneratedresultsis
for sorting. As shown in Table 1, our method achieves
relatively low. In contrast, our model excels in maintaining
the best scores in both FID and CLIP metrics, indicating
full 360◦ × 180◦ view consistency while demonstrating
its high fidelity and strong compatibility with the input
superiorcontentcreativity.Toevaluatethecapabilityforfree
prompts.TheIntra-Styleresultsdemonstratethatourmodel
trajectoryrendering,wedesignazigzagtrajectorytoguide
achieves global coherence across the image, consistently
thecamera’smovementthroughthescene,withnovelview
maintainingtheoverallstyle.WhileText2lighthasasmaller
renderingssampledalongthetrajectoryforcomparison.As
Intra-Style score, this is primarily due to its tendency to
illustrated in Figure 7, we showed 3 random samples from
generate monotonous results with extensive uniform color
this fixed flythrough trajectory. Compared with the other
block backgrounds. We also attained outstanding results
three methods, our model achieves a more complete 3D
inaestheticmetrics,marginallytrailingDiffusion360.How-
scene with consistent textures and a reasonable geometric
ever, as discussed in Section 4.3, panoramas generated by
structure.
Diffusion360lacktheinherentsphericalstructuralproperty,
resultinginanunreasonablefinalperspectiveprojection.
sruO
063enecSmaerD
remaerddicuL
mooR2txeT9
Input View
&Render
Trajectory
View1 View2 View3 View1 View2 View3
Fig. 7. Qualitative comparisons in Scene Free Exploration. We show the novel view renderings along a zigzag trajectory to
compare the capability of free trajectory rendering. Our method is able to maintain high-quality content rendering and does not
showdistortionorgapsinunseenspace,whichshowstheabilityof LAYERPANO3D tocreatehyper-immersivepanoramicscenes.
Moreover,weconductedauserstudycomparinggener- SSIM and LPIPS [63]. In addition, we render four orthog-
ated panoramas, evaluating them across three dimensions: onalviews,eachdistributedatafixedFOV(90◦),elevation
Coherence (internal consistency), Plausibility (plausibil- (0◦) and four equidistant azimuths (0◦,90◦,180◦,270◦) re-
ity of perspective views), and Compatibility (align with spectively, to predictperspective field. Building upon [64],
prompts).Following[60],weusetheAverageUserRanking we independently predict camera parameters and evaluate
(AUR) as a preference metric where users rank each result the horizon tilt problem by calculating the Roll-Mean and
on a scale of 1 to 5 (lower is worse). We invite 29 users in- Roll-Var, i.e. the mean and variance of the elevation an-
cludinggraduatestudentsthatexpertisein3Dandaverage gles across these views. As shown in Table 2, our method
userstorankthe60resultsfrom5methodsindividually.The surpasses the existing methods in both novel view qual-
resultsinTable1demonstratethatourmethodoutperforms ity metrics (NIQE and BRISQUE) and 3D reconstruction
othersbyasignificantmarginacrossalldimensions. metrics (PSNR, SSIM, and LPIPS). Furthermore, we con-
ducted another user study to further evaluate the quality
3D Panoramic Scene Reconstruction. Following [2], we
of the generated 3D panoramic scene from two aspects: 1)
adopttwowidely-usednon-referenceimagequalityassess-
360◦ × 180◦ view consistency and 2) free path rendering
mentmetrics,NIQE[61]andBRISQUE[62],toevaluatethe
quality. For the first aspect, we render 60 frames in a
capability of free trajectory rendering, i.e. the novel view
360-degree view at the 0-degree and 45-degree elevation
qualityalonganimmersivenavigationpathinthe3Dscene.
respectivelyforevaluation.Forthesecondaspect,weselect
Furthermore,wealsomeasurethe3Dreconstructionquality
the same trajectory as in Figure 7 to render navigation
on training views via classic reconstruction metrics: PSNR,
sruO
063enecSmaerD
remaerDdicuL
mooR2txeT10
Input Masked Panorama Ours LaMa-Inpainting Stable-Diffusion-Inpainting
Fig.8.AnalysisontheLayerCompletionInpainting.Wepresentthepanoramainpaintingresultsforthreemethodsguidedby
thesametextprompt:“empty scene, nothing”[47].Ourmodeleffectivelyhandlescomplexscenarios,deliveringclearresults
withconsistentandcoherentstructures.
TABLE1
QuantitativecomparisonwithSoTAmethodson2DPanoramaGeneration.Boldindicatesthebestresult,andunderlineindicatesthe
second-bestresult.
UserStudy(AUR)
Method FID↓ Intra-Style↓ CLIP↑
Coherence↑ Plausibility↑ Compatibility↑
Text2light[15] 286.90 0.31 18.69 2.95 2.47 1.89
Panfusion[11] 283.80 18.66 21.22 3.06 3.03 3.26
Diffusion360[43] 274.03 3.70 21.65 3.08 3.22 3.24
LDM3D-pano[12] 298.64 8.05 20.58 2.56 2.84 2.87
Ours 255.60 2.24 22.08 3.35 3.44 3.73
TABLE2
QualitativecomparisonwithSoTAmethodson3DPanoramicScene.Boldindicatesthebestresult.
Appearance Geometry UserStudy(AUR)
Method
NIQE↓ BRISQUE↓ PSNR↑ SSIM↑ LPIPS↓ Roll-Mean↓ Roll-Var↓ 360◦×180◦↑ Free-path↑
Text2room[6] 5.117 44.598 30.437 0.890 0.033 1.877 1.566 1.76 2.28
LucidDreamer[4] 5.877 54.230 31.020 0.973 0.029 2.775 2.167 1.85 1.29
DreamScene360[16] 5.083 39.403 28.925 0.941 0.050 1.696 2.524 2.94 2.90
Ours 4.374 39.006 40.797 0.980 0.012 0.540 0.028 3.45 3.53
View0 View1 View0 View1 View0 View1
w/ Circular Blending Strategy w/o Circular Blending Strategy
Fig.9.AblationontheCircularBlendingStrategy.Thecircular View2 View3 View2 View3 View2 View3
blending strategy ensures the continuity between the leftmost Layer 0 Layer 1, Layer 1,
andrightmostsidesofthepanorama,whichpreservesthecru- w/ Gaussian Selector w/o Gaussian Selector
cialcharacteristicof360-degreepanoramas. Fig. 10. Ablation on the Gaussian Selector. Our Gaussian
Selector resolves the optimization conflicts between layers by
re-activating the Gaussians from the previous layer that blocks
videosforevaluation.Weinvite43usersincludinggraduate newassets.WiththeGaussianSelector,themergedGaussians
areoptimizedtofaithfullyreconstructtheground-truthpanorama
studentsthatexpertisein3Dandaverageuserstorankthe
views.
40resultsfrom4methodsindividually.Theaverageranking
is shown in Table 2. Our LAYERPANO3D achieves the best
performanceinboth360◦×180◦ viewconsistencyandfree
large-scale inpainting. Stable Diffusion tends to produce
pathrenderingqualityamongallfourapproaches.
distorted new elements due to the domain gap between
perspectiveandpanoramicimages.Incontrast,thankstothe
4.5 AnalysisandAblativeStudy panoramafinetuning,ourmoduledeliverscleaninpainting
AblationonCircularBlending.Figure9demonstratesthat resultswithcoherentandplausiblestructuresinthemasked
thecircularblendingstrategyeffectivelymaintainsgeomet- regions.
ric continuity, contributing to the generation of a seamless Ablation on Gaussian Selector. Our Gaussian selector is
360-degree panorama. Without this design, the rightmost proposed to select the part of Gaussians that appears in
andleftmostsidesofthepanoramacouldforminconsistent the front of newly added scene assets. By selecting these
patterns,resultinginadiscontinuouspanorama. Gaussians and re-activating them in the optimization, the
Analysis on Layer Completion Inpainting.Wediscussthe model achieves accurate appearance and geometry at the
effectiveness of our fine-tuned panorama inpainter in layer current layer. As shown in Figure 10, the leftmost column
completion.Wecomparetheinpaintingresultsamongthree isthesceneGaussiansatlayer0.Whenaddingthebuilding
approaches: LaMa [42], Stable Diffusion inpainting model assetsatthefirstlayer,theskyGaussiansfromtheprevious
and our proposed inpainter. As illustrated in Figure 8, layerpartiallyblockthebuildingassets(rightcolumn).After
LaMa produces inconsistent texture and blurry artifacts at using the Gaussian selector to select and optimize the sky11
Reference Panorama
A
B
A
C
D
C B
D
Ours (multi-layered panorama) Single panorama
Fig.11.Analysis on Panorama Rendering at Off-center Viewpoints. We show that LAYERPANO3D ’s robustness in rendering
360◦×180◦ consistentpanoramaatvariousviewpointswithinalargerangeofspaceinthegeneratedscene.Comparedwithour
single-layerpanoramabaseline,LAYERPANO3Drenderingsexhibithigh-qualitycontentwithoutanyholesorgapsfromocclusion.
Gaussians, these Gaussians learn to either be translucent roamingin3Dspace.WebelievethatLAYERPANO3Dholds
and pruned for low opacity or move to be a part of the promise to advance high-quality, explorable 3D scene cre-
buildingassets.Thereforeinthemiddlecolumn,weobserve ationinbothacademiaandindustry.
a consistent scene with no obvious blockage of the new Limitations and Future Works. LAYERPANO3D leverages
buildingassetsthankstotheGaussianSelector. good pre-trained prior to construct panoramic 3D scene,
Analysis on Panorama Renderings at Off-center View- i.e., panoramic depth prior for 3D lifting. Therefore, the
points.InFigure11,wedemonstratethatLAYERPANO3Dis createdscenemightcontainartifactsfrominaccuratedepth
robust to render consistent panorama images at various estimation. With advancements in more robust panorama
locations besides the original camera location in the center. depthestimation,wehopetocreatehigh-qualitypanoramic
Wesamplefourcameralocationsoncirculartrajectorieson 3Dsceneswithfinerassetgeometry.
thehemispherecenteredattheoriginandrender24viewsat
(−45◦,0◦,45◦)elevationtocomposenewpanoramaimages.
Byevaluatingpanoramarenderingsatnewviewpoints,we REFERENCES
show that our generated panoramic scene is 360◦ × 180◦
consistentandenclosed,robusttovariousviewpointsatany [1] R. Gao, A. Holynski, P. Henzler, A. Brussee, R. Martin-Brualla,
angle. Compared to the single-layered 3D panorama, our P. Srinivasan, J. T. Barron, and B. Poole, “Cat3d: Create any-
thing in 3d with multi-view diffusion models,” arXiv preprint
multi-layered 3D panorama exhibits no gaps or holes from
arXiv:2405.10314,2024.
the scene occlusion, demonstrating our capability for free
[2] H. Li, H. Shi, W. Zhang, W. Wu, Y. Liao, L. Wang, L. hang Lee,
3Dexplorationinthegeneratedscenes. and P. Zhou, “Dreamscene: 3d gaussian-based text-to-3d scene
generationviaformationpatternsampling,”2024.
[3] Q.Zhang,C.Wang,A.Siarohin,P.Zhuang,Y.Xu,C.Yang,D.Lin,
5 CONCLUSION B.Zhou,S.Tulyakov,andH.-Y.Lee,“Scenewiz3d:Towardstext-
guided3dscenecomposition,”2023.
In this paper, we propose LAYERPANO3D, a novel frame- [4] J.Chung,S.Lee,H.Nam,J.Lee,andK.M.Lee,“Luciddreamer:
Domain-free generation of 3d gaussian splatting scenes,” CoRR,
workthatgenerateshyper-immersivepanoramicscenefrom
vol.abs/2311.13384,2023.
a single text prompt. Our key contributions are two-fold. [5] H.Yu,H.Duan,J.Hur,K.Sargent,M.Rubinstein,W.T.Freeman,
First, we propose the text-guided anchor view synthe- F. Cole, D. Sun, N. Snavely, J. Wu, and C. Herrmann, “Won-
derjourney: Going from anywhere to everywhere,” CoRR, vol.
sis pipeline to generate detailed and consistent reference
abs/2312.03884,2023.
panorama. Second, we pioneer the Layered 3D Panorama
[6] L. Ho¨llein, A. Cao, A. Owens, J. Johnson, and M. Nießner,
representation to show complex scene hierarchies at multi- “Text2room:Extractingtextured3dmeshesfrom2dtext-to-image
ple depth layers, and lift it to Gaussians to enable free 3D models,”arXivpreprintarXiv:2303.11989,2023.
[7] R. Fridman, A. Abecasis, Y. Kasten, and T. Dekel, “Scenescape:
exploration. Extensive experiments show the effectiveness
Text-drivenconsistentscenegeneration,”2023.
of LAYERPANO3D in generating 360◦ × 180◦ consistent
[8] J. Zhang, X. Li, Z. Wan, C. Wang, and J. Liao, “Text2nerf: Text-
panorama at various viewpoints and enabling immersive driven3dscenegenerationwithneuralradiancefields,”2024.12
Reference Panorama Layer 0 Layer 1 Layer 2
A bustling city street with modern buildings and cars under a vibrant sky.
A coastal road with signs, cliffs, and the ocean under a bright blue sky.
A rural landscape with a country road, trees, and a cottage under a blue sky.
Sheep grazing in a tranquil landscape with trees and distant mountains.
An elegant library room with bookshelves, a desk, and decorative lighting.
Comfortable living room, fireplace, sofa, coffee table, bookshelf, large windows.
Fig.12.Additionalresultsof LAYERPANO3DonDiverseGeneration.LAYERPANO3Dgeneratesvarioushyper-immersiveLayered3DPanorama
thatcovercityscape,landscapeandindoorenvironments.13
[9] H.Ouyang,K.Heal,S.Lombardi,andT.Sun,“Text2immersion: [31] D.Cohen-Bar,E.Richardson,G.Metzer,R.Giryes,andD.Cohen-
Generative immersive scene with 3d gaussians,” arXiv preprint Or, “Set-the-scene: Global-local training for generating control-
arXiv:2312.09242,2023. lablenerfscenes,”2023.
[10] S. Tang, F. Zhang, J. Chen, P. Wang, and Y. Furukawa, “Mvd- [32] A. Vilesov, P. Chari, and A. Kadambi, “Cg3d: Compositional
iffusion: Enabling holistic multi-view image generation with generationfortext-to-3dviagaussiansplatting,”2023.
correspondence-awarediffusion,”2023. [33] O. Bar-Tal, L. Yariv, Y. Lipman, and T. Dekel, “Multidiffusion:
[11] C. Zhang, Q. Wu, C. C. Gambardella, X. Huang, D. Phung, Fusingdiffusionpathsforcontrolledimagegeneration,”2023.
W. Ouyang, and J. Cai, “Taming stable diffusion for text to 360 [34] Y. Lee, K. Kim, H. Kim, and M. Sung, “Syncdiffusion: Coherent
panoramaimagegeneration,”inProceedingsoftheIEEE/CVFCon- montageviasynchronizedjointdiffusions,”2023.
ferenceonComputerVisionandPatternRecognition,2024,pp.6347– [35] J. Wang, Z. Chen, J. Ling, R. Xie, and L. Song, “360-
6357. degree panorama generation from few unregistered nfov
[12] G.B.M.Stan,D.Wofk,E.Aflalo,S.-Y.Tseng,Z.Cai,M.Paulitsch, images,” in Proceedings of the 31st ACM International Conference
andV.Lal,“Ldm3d-vr:Latentdiffusionmodelfor3dvr,”2023. on Multimedia. ACM, Oct. 2023. [Online]. Available: http:
[13] G.Wang,Y.Yang,C.C.Loy,andZ.Liu,“Stylelight:Hdrpanorama //dx.doi.org/10.1145/3581783.3612508
generationforlightingestimationandediting,”2022. [36] J.LiandM.Bansal,“Panogen:Text-conditionedpanoramicenvi-
[14] H.Wang,X.Xiang,Y.Fan,andJ.-H.Xue,“Customizing360-degree ronmentgenerationforvision-and-languagenavigation,”2023.
panoramasthroughtext-to-imagediffusionmodels,”2023. [37] T. Wu, C. Zheng, and T.-J. Cham, “Panodiffusion: 360-degree
[15] Z.Chen,G.Wang,andZ.Liu,“Text2light:Zero-shottext-driven panoramaoutpaintingviadiffusion,”2024.
hdrpanoramageneration,”ACMTransactionsonGraphics(TOG), [38] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner,
vol.41,no.6,pp.1–16,2022. M.Savva,S.Song,A.Zeng,andY.Zhang,“Matterport3D:Learn-
[16] S.Zhou,Z.Fan,D.Xu,H.Chang,P.Chari,T.Bharadwaj,S.You, ingfromRGB-Ddatainindoorenvironments,”InternationalCon-
Z.Wang,andA.Kadambi,“Dreamscene360:Unconstrainedtext- ferenceon3DVision(3DV),2017.
to-3dscenegenerationwithpanoramicgaussiansplatting,”arXiv [39] H. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,”
preprintarXiv:2404.06903,2024. 2023.
[17] H.Zhou,X.Cheng,W.Yu,Y.Tian,andL.Yuan,“Holodreamer: [40] J. Li, H. Tan, K. Zhang, Z. Xu, F. Luan, Y. Xu, Y. Hong,
Holistic3dpanoramicworldgenerationfromtextdescriptions,” K. Sunkavalli, G. Shakhnarovich, and S. Bi, “Instant3d: Fast
arXivpreprintarXiv:2407.15187,2024. text-to-3d with sparse-view generation and large reconstruction
[18] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, model,”CoRR,2023.
J.Mu¨ller,J.Penna,andR.Rombach,“SDXL:improvinglatentdif- [41] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,
fusionmodelsforhigh-resolutionimagesynthesis,”CoRR,2023. “High-resolutionimagesynthesiswithlatentdiffusionmodels,”in
[19] B.Kerbl,G.Kopanas,T.Leimku¨hler,andG.Drettakis,“3dgaus- ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
siansplattingforreal-timeradiancefieldrendering,”ACMTrans. recognition,2022,pp.10684–10695.
Graph.,vol.42,no.4,pp.139:1–139:14,2023. [42] R. Suvorov, E. Logacheva, A. Mashikhin, A. Remizova,
[20] R. Po, W. Yifan, V. Golyanik, K. Aberman, J. T. Barron, A. H. A.Ashukha,A.Silvestrov,N.Kong,H.Goka,K.Park,andV.Lem-
Bermano,E.R.Chan,T.Dekel,A.Holynski,A.Kanazawaetal., pitsky,“Resolution-robustlargemaskinpaintingwithfouriercon-
“Stateoftheartondiffusionmodelsforvisualcomputing,”arXiv volutions,”2021.
preprintarXiv:2310.07204,2023. [43] M.Feng,J.Liu,M.Cui,andX.Xie,“Diffusion360:Seamless360
[21] M.Oechsle,S.Peng,andA.Geiger,“Unisurf:Unifyingneuralim- degreepanoramicimagegenerationbasedondiffusionmodels,”
plicitsurfacesandradiancefieldsformulti-viewreconstruction,” 2023.
inProceedingsoftheIEEE/CVFInternationalConferenceonComputer [44] T.HaraandT.Harada,“Sphericalimagegenerationfromasingle
Vision,2021,pp.5589–5599. normalfieldofviewimagebyconsideringscenesymmetry,”2020.
[22] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Love- [45] J.Jain,J.Li,M.T.Chiu,A.Hassani,N.Orlov,andH.Shi,“One-
grove, “Deepsdf: Learning continuous signed distance functions former: One transformer to rule universal image segmentation,”
forshaperepresentation,”inProceedingsoftheIEEE/CVFconference in Proceedings of the IEEE/CVF Conference on Computer Vision and
oncomputervisionandpatternrecognition,2019,pp.165–174. PatternRecognition,2023,pp.2989–2998.
[23] Y.Xiangli,L.Xu,X.Pan,N.Zhao,A.Rao,C.Theobalt,B.Dai,and [46] B.Zhou,H.Zhao,X.Puig,S.Fidler,A.Barriuso,andA.Torralba,
D.Lin,“Bungeenerf:Progressiveneuralradiancefieldforextreme “Sceneparsingthroughade20kdataset,”inProceedingsoftheIEEE
multi-scale scene rendering,” in European conference on computer conferenceoncomputervisionandpatternrecognition,2017,pp.633–
vision. Springer,2022,pp.106–122. 641.
[24] J. T. Barron, B. Mildenhall, M. Tancik, P. Hedman, R. Martin- [47] L. Zhang and M. Agrawala, “Transparent image layer
Brualla, and P. P. Srinivasan, “Mip-nerf: A multiscale represen- diffusion using latent transparency,” 2024. [Online]. Available:
tation for anti-aliasing neural radiance fields,” in Proceedings of https://arxiv.org/abs/2402.17113
theIEEE/CVFInternationalConferenceonComputerVision,2021,pp. [48] A.Kirillov,E.Mintun,N.Ravi,H.Mao,C.Rolland,L.Gustafson,
5855–5864. T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo et al., “Segment
[25] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ra- anything,”arXivpreprintarXiv:2304.02643,2023.
mamoorthi, and R. Ng, “Nerf: Representing scenes as neural [49] T.Yang,P.Ren,X.Xie,andL.Zhang,“Pixel-awarestablediffusion
radiance fields for view synthesis,” Communications of the ACM, forrealisticimagesuper-resolutionandpersonalizedstylization,”
vol.65,no.1,pp.99–106,2021. arXivpreprintarXiv:2308.14469,2023.
[26] A.Chen,Z.Xu,A.Geiger,J.Yu,andH.Su,“Tensorf:Tensorialra- [50] M. Rey-Area, M. Yuan, and C. Richardt, “360monodepth: High-
diancefields,”inEuropeanConferenceonComputerVision. Springer, resolution360degmonoculardepthestimation,”inProceedingsof
2022,pp.333–350. theIEEE/CVFConferenceonComputerVisionandPatternRecognition,
[27] S. Fridovich-Keil, A. Yu, M. Tancik, Q. Chen, B. Recht, and 2022,pp.3762–3772.
A. Kanazawa, “Plenoxels: Radiance fields without neural net- [51] Z.Liu,H.Ouyang,Q.Wang,K.L.Cheng,J.Xiao,K.Zhu,N.Xue,
works,” in Proceedings of the IEEE/CVF Conference on Computer Y.Liu,Y.Shen,andY.Cao,“Infusion:Inpainting3dgaussiansvia
VisionandPatternRecognition,2022,pp.5501–5510. learning depth completion from diffusion prior,” arXiv preprint
[28] T. Mu¨ller, A. Evans, C. Schied, and A. Keller, “Instant neural arXiv:2404.11613,2024.
graphicsprimitiveswithamultiresolutionhashencoding,”ACM [52] runwayml, “stable-diffusion-inpainting model card,”
TransactionsonGraphics(ToG),vol.41,no.4,pp.1–15,2022. 2022. [Online]. Available: https://huggingface.co/runwayml/
[29] Q.Xu,Z.Xu,J.Philip,S.Bi,Z.Shu,K.Sunkavalli,andU.Neu- stable-diffusion-inpainting
mann,“Point-nerf:Point-basedneuralradiancefields,”inProceed- [53] P. Esser, R. Rombach, and B. Ommer, “Taming transformers for
ings of the IEEE/CVF Conference on Computer Vision and Pattern high-resolutionimagesynthesis,”2021.
Recognition,2022,pp.5438–5448. [54] N.Ruiz,Y.Li,V.Jampani,Y.Pritch,M.Rubinstein,andK.Aber-
[30] S.Cai,E.R.Chan,S.Peng,M.Shahbazi,A.Obukhov,L.V.Gool, man, “Dreambooth: Fine tuning text-to-image diffusion models
andG.Wetzstein,“Diffdreamer:Towardsconsistentunsupervised forsubject-drivengeneration,”2023.
single-view scene extrapolation with conditional diffusion mod- [55] J. Xiao, K. A. Ehinger, A. Oliva, and A. Torralba, “Recognizing
els,”inICCV. IEEE,2023,pp.2139–2150. scene viewpoint using panoramic place representation,” in 201214
IEEEConferenceonComputerVisionandPatternRecognition. IEEE, Tong Wu received her Ph.D. degree from the
2012,pp.2695–2702. MultimediaLaboratoryattheChineseUniversity
[56] G. B. M. Stan, D. Wofk, S. Fox, A. Redden, W. Saxton, J. Yu, ofHongKongin2024.Beforethat,shereceived
E.Aflalo,S.-Y.Tseng,F.Nonato,M.Muller,andV.Lal,“Ldm3d: herBachelor’sdegreefromTsinghuaUniversity,
Latentdiffusionmodelfor3d,”2023. Beijing, in 2020. Her research focuses on 3D
[57] L. A. Gatys, A. S. Ecker, and M. Bethge, “Image style transfer reconstructionandGeneration.Shehasserved
using convolutional neural networks,” in Proceedings of the IEEE as a regular reviewer for CVPR, ICCV, ECCV,
conferenceoncomputervisionandpatternrecognition,2016,pp.2414– NeurIPS,ICLRandAAAI.
2423.
[58] M.Heusel,H.Ramsauer,T.Unterthiner,B.Nessler,andS.Hochre-
iter,“Ganstrainedbyatwotime-scaleupdateruleconvergetoa
localnashequilibrium,”inNIPS,2017,pp.6626–6637.
[59] J.Hessel,A.Holtzman,M.Forbes,R.L.Bras,andY.Choi,“Clip-
score:Areference-freeevaluationmetricforimagecaptioning,”in
EMNLP(1). AssociationforComputationalLinguistics,2021,pp. YixuanLireceivedtheB.Sc.degreefromNan-
7514–7528. jingUniversity,Nanjing,China,in2019,andthe
[60] L.Zhang,A.Rao,andM.Agrawala,“Addingconditionalcontrol M.Sc.degreefromNanjingUniversity,Nanjing,
totext-to-imagediffusionmodels,”inProceedingsoftheIEEE/CVF China,in2022.SheiscurrentlypursuingaPh.D
InternationalConferenceonComputerVision,2023,pp.3836–3847. degreeintheChineseUniversityofHongKong,
[61] A. Mittal, R. Soundararajan, and A. C. Bovik, “Making a “com- Hong Kong. Her research area is 3D vision,
pletely blind” image quality analyzer,” IEEE Signal processing especially 3D scene reconstruction and gener-
letters,vol.20,no.3,pp.209–212,2012. ation, with a recent focus on neural rendering
[62] A. Mittal, A. K. Moorthy, and A. C. Bovik, “No-reference image for large-scale scenes. She has served as a
quality assessment in the spatial domain,” IEEE Transactions on reviewerforCVPR,ICCV,ECCV,NeurIPS,IJCV.
imageprocessing,vol.21,no.12,pp.4695–4708,2012.
[63] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang,
“Theunreasonableeffectivenessofdeepfeaturesasaperceptual
metric,”inProceedingsoftheIEEEconferenceoncomputervisionand
patternrecognition,2018,pp.586–595.
[64] L. Jin, J. Zhang, Y. Hold-Geoffroy, O. Wang, K. Matzen, Gordon Wetzstein (Senior Member, IEEE) is
M. Sticha, and D. F. Fouhey, “Perspective fields for single currently an associate professor of EE and, by
image camera calibration,” 2023. [Online]. Available: https: courtesy,ofCSatStanfordUniversity,Stanford,
//arxiv.org/abs/2212.03239 California, leading the Stanford Computational
Imaging Lab and a faculty co-director of the
StanfordCenterforImageSystemsEngineering.
Shuai Yang received the B.Sc. degree from
TongjiUniversity,Shanghai,China,in2023.He
is currently pursuing a Ph.D. degree at School
of Electronic Information and Electrical Engi-
neering in Shanghai Jiao Tong University. His
research interests lie in the field of 3D Vision,
particularlycontentcreationofthe3Dsceneand Ziwei Liu (Member, IEEE) received the PhD
object. degree from the Chinese University of Hong
Kong.Heiscurrentlyanassistantprofessorwith
Nanyang Technological University, Singapore.
Previously,hewasaseniorresearchfellowwith
theChineseUniversityofHongKongandapost-
doctoral researcher with the University of Cali-
fornia, Berkeley. His research interests include
revolvesaroundcomputervision,machinelearn-
JingTanreceivedtheB.Sc.degreeandM.Sc.
ing, and computer graphics. He has published
degreefromNanjingUniversity,Nanjing,China,
extensivelyontop-tierconferencesandjournals
in 2020 and 2023. She is currently pursuing
inrelevantfields,includingCVPR,ICCV,ECCV,NeurIPS,ICLR,ICML,
a Ph.D. degree in the Multimedia Laboratory,
TPAMI,TOGandNature-MachineIntelligence.Heistherecipientof
the Chinese University of Hong Kong. Her re-
Microsoft Young Fellowship, Hong Kong PhD Fellowship, ICCV Young
searchfocusesonperceivingandrecreatingthe
ResearcherAwardand HKSTPBestPaperAward.Healsoservesas
worldsystemfromunderstandingtheforeground
anAreaChairofICCV,NeurIPS,andICLR.
eventstothecreationofthe3Dscenesurround-
ings. She has served as a reviewer for CVPR,
ICCV,ECCV,NeurIPS,ICLRandIJCV.
DahuaLinreceivedtheBEngdegreefromthe
UniversityofScienceandTechnologyofChina,
Hefei,China,in2004,theMPhildegreefromthe
Chinese University of Hong Kong, Hong Kong,
Mengchen Zhang received her B.Sc. degree in 2006, and the PhD degree from the Mas-
fromNanjingUniversity,Nanjing,China,in2023. sachusetts Institute of Technology, Cambridge,
SheiscurrentlypursuingaPh.D.degreeatZhe- MA,USA,in2012.From2012to2014,hewas
jiang University, Zhejiang, China. Her research aresearchassistantprofessorwithToyotaTech-
interestslieinthefieldof3DVision,particularly nologicalInstituteatChicago,Chicago,IL,USA.
6Dposeestimationand3Dgeneration. He is currently an associate professor with the
DepartmentofInformationEngineering,Chinese
UniversityofHongKong(CUHK),andtheleadingscientistwithShang-
haiAILaboratory.