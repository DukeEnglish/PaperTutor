MME-RealWorld: Could Your Multimodal LLM
Challenge High-Resolution Real-World Scenarios
that are Difficult for Humans?
Yi-FanZhang1,5,♠,HuanyuZhang1,5,HaochenTian1,5,ChaoyouFu2,†
ShuangqingZhang2,JunfeiWu1,5,FengLi3,KunWang4,5,QingsongWen6,†
ZhangZhang1,5,†,LiangWang1,5,RongJin7,TieniuTan1,2,5
1CASIA,2NJU,3HKUST,4NTU,5UCAS,6SquirrelAILearning,7MetaAI
♠ProjectLeader †CorrespondingAuthor
https://mme-realworld.github.io/
Abstract
ComprehensiveevaluationofMultimodalLargeLanguageModels(MLLMs)has
recentlygarneredwidespreadattentionintheresearchcommunity. However,we
observe that existing benchmarks present several common barriers that make it
difficulttomeasurethesignificantchallengesthatmodelsfaceintherealworld,
including:1)smalldatascaleleadstoalargeperformancevariance;2)relianceon
model-basedannotationsresultsinrestricteddataquality;3)insufficienttaskdif-
ficulty,especiallycausedbythelimitedimageresolution. Totackletheseissues,
weintroduceMME-RealWorld. Specifically,wecollectmorethan300Kimages
frompublicdatasetsandtheInternet,filtering13,366high-qualityimagesforan-
notation. This involves the efforts of professional 25 annotators and 7 experts
inMLLMs, contributingto29,429question-answerpairsthatcover43subtasks
across 5 real-world scenarios, extremely challenging even for humans. As far
asweknow,MME-RealWorldisthelargestmanuallyannotatedbenchmark
to date, featuring the highest resolution and a targeted focus on real-world
applications. We further conduct a thoroughevaluation involving 28 prominent
MLLMs, such as GPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet. Our results
show that even the most advanced models struggle with our benchmarks, where
noneofthemreach60%accuracy. Thechallengesofperceivinghigh-resolution
images and understanding complex real-world scenarios remain urgent issues to
beaddressed. ThedataandevaluationcodearereleasedinourProjectPage.
1 Introduction
In recent years, we have witnessed a significant flourish of Multimodal Large Language Models
(MLLMs)[15,43,73]. AprimaryobjectivebehinddesigningMLLMshasbeentodevelopgeneral
intelligentagentscapableofcomprehensivelyperceivinghumanqueriesandenvironmentalstitua-
tionsthroughtheintegrationofvariousmultimodalsensorydata. Consequently,aplethoraofcom-
prehensiveevaluationbenchmarkshaveemergedtorigorouslyassessmodelcapabilities. However,
somecommonconcernsalsoarise:
• Data Scale. Many existing benchmarks contain fewer than 10K Question-Answer (QA)
pairs, such as MME [17], MMbench [45], MMStar [10], MM-Vet [70], TorchStone [5],
andBLINK[20]. ThelimitednumberofQAcanleadtolargeevaluationfluctuations.
Email:yifanzhang.cs@gmail.com
4202
guA
32
]VC.sc[
1v75231.8042:viXraOCR in the Wild Remote Sensing
W ( ( (A B Ch ) ))a 0 00t 2 22is 8 03 8 28th 0 50e 5 56 p 5 56h 7 77o 7 77n 2 52e 2 33 number of HOP INN? W i ( (n A
B
h )t )ha RYet e ei ms l dl i .t odh wde . lc eo al ro er a o of f t h the e e px ic ca tv ua reto ?r
( (D E)) T0 n2 uh3 me8 bi0 m e r5 a .5 g7 e 7 d2 o3 es not feature the ( ( (C D E)) ) TB G hl ra eec e ik mn. . age does not feature the
color.
What's the content of the old man speaking in
the frame on the fourth row in the comic page?
(A)DON'T WAIT FOR US. WE'LL FIND Howmanyaircraftarethereinthe
OUROWNRABBITHOLE. picture?
(B)I SEE YOU BROUGHT WILLOW (A)1
LIFGOOD WITH YOU. (B)2
(C)HER NAME IS MARIAN DREWS! (C)3
(D)WE CAN MANAGE. (D)5
(E)Theimagedoesnotfeaturethecontent. (E)0
Video Monitoring Autonomous Driving
This image shows the front
What is the total number of motors view of the ego car. What is
and cars in the image? thefuturestateofthewhite
(A)26 suvin the middle?
(B)80 (A)Turn right.
(C)133 (B)Turn left.
(D)92 (C)Stationary.
(E)The image does not feature the (D)Keepgoingstraight.
objects (E)The image does not
feature the object
What will the truck do in the What is the traffic light on the right?
image? (A)yellow
(A)Stopping (B)red
(B)Keepmoving (C)green
(C)Turn left (D)changing/off
(D)Turn right (E)The image does not feature the
(E)The image does not feature the traffic light
object.
Diagram and Table
What'sthepercentageofCAPEXwhendirectcostsriseto35340000iftheoutcomeisat What is the data of Merchant Black Prices in 2038 in the diagram Real Energy Prices?
6250644? (A)40.00 to 60.00（B)20.00 to 40.00(C) 60.00 to 80.00(D) 80.00 to 100.00
(A) 12.6% (B) 10.3(C)11.9%(D)11.1(E)The image does not feature theDATA. (E) The image does not feature the data.
Figure1: DiagramofMME-RealWorld. Ourbenchmarkcontains5real-worlddomains,covering
43perceptionandreasoningsubtasks. EachQApairoffers5options. Wehighlightandmagnifythe
imagepartsrelevanttothequestioninaredboxforbettervisibility.
• Annotation Quality. While some benchmarks, such as MMT-Bench [69] and SEED-
Bench [32], are relatively larger in scale, their annotations are generated by LLMs or
MLLMs. This annotation process is inherently limited by the performance of the used
models. In our benchmark, for example, the best-performing model, InternVL-2, merely
achieves50%accuracy. Consequently,relyingonmodelswouldinevitablyintroducesig-
nificantnoise,compromisingthequalityoftheannotations.
• Task Difficulty. To date, the top performance of some benchmarks has reached the ac-
curacy of 80%-90% [50, 49, 56, 45, 34], and the performance margin between advanced
MLLMs is narrow. This makes it challenging to verify the benefits or improvements of
advancedmodelsandtodistinguishwhichoneissignificantlybetter.
In light of these concerns, we propose a new benchmark named MME-RealWorld. We first pay
attention to a series of well-motivated families of datasets, considering images from sources such
asautonomousdriving,remotesensing,videosurveillance,newspapers,streetviews,andfinancial
charts. Thesescenariosaredifficultevenforhumans,wherewehopethatMLLMscanreallyhelp.
Consideringthesetopics,wecollectatotalof13,366high-resolutionimagesfrommorethan300K
publicandinternetsources. Theseimageshaveanaverageresolutionof2,000×1,500,containing
richimagedetails. 25professionalannotatorsand7expertsinMLLMsareparticipatedtoannotate
andcheckthedataquality,andmeanwhileensuringthatallquestionsarechallengingforMLLMs.
Note that most questions are even hard for humans, requiring multiple annotators to answer and
double-checktheresults. AsshowninFig.2(a),MME-RealWorldfinallycontains29,429annota-
2tions for 43 sub-class tasks, where each one has at least 100 questions. 28 advanced MLLMs are
evaluated on our benchmark, along with detailed analysis. We conclude the main advantages of
MME-RealWorldcomparedtoexistingcounterpartsasfollows:
• DataScale.Withtheeffortsofatotalof32volunteers,wehavemanuallyannotated29,429
QA pairs focused on real-world scenarios, making this the largest fully human-annotated
benchmarkknowntodate.
• Data Quality. 1) Resolution: Many image details, such as a scoreboard in a sports
event,carrycriticalinformation. Thesedetailscanonlybeproperlyinterpretedwithhigh-
resolutionimages,whichareessentialforprovidingmeaningfulassistancetohumans. To
thebestofourknowledge,MME-RealWorldfeaturesthehighestaverageimageresolution
amongexistingcompetitors. 2)Annotation: Allannotationsaremanuallycompleted,with
aprofessionalteamcross-checkingtheresultstoensuredataquality.
• TaskDifficultyandReal-WorldUtility. TheperformanceofdifferentMLLMsisshown
inFig.2(b),inwhichwecanseethateventhemostadvancedmodelshavenotsurpassed
60%accuracy. Additionally,asillustratedinFig.1,manyreal-worldtasksaresignificantly
more difficult than those in traditional benchmarks. For example, in video monitoring, a
model needs to count the presence of 133 vehicles, or in remote sensing, it must identify
andcountsmallobjectsonamapwithanaverageresolutionexceeding5000×5000.
• MME-RealWorld-CN. Existing Chinese benchmark [45] is usually translated from its
Englishversion. Thishastwolimitations: 1)Question-imagemismatch. Theimagemay
relate to an English scenario, which is not intuitively connected to a Chinese question.
2) Translation mismatch [58]. The machine translation is not always precise and perfect
enough. We collect additional images that focus on Chinese scenarios, asking Chinese
volunteersforannotation. Thisresultsin5,917QApairs.
2 RelatedWork
MultimodalBenchmark. WiththedevelopmentofMLLMs, anumberofbenchmarkshavebeen
built. For instance, MME [17] constructs a comprehensive evaluation benchmark that includes a
total of 14 perception and cognition tasks. All QA pairs in MME are manually designed to avoid
dataleakage,andthebinarychoiceformatmakesiteasytoquantify. MMBench[45]containsover
3,000multiple-choicequestionscovering20differentabilitydimensions,suchasobjectlocalization
and social reasoning. It introduces GPT-4-based choice matching to address the MLLM’s lack of
instruction-followingcapabilityandanovelcircularevaluationstrategytoimprovetheevaluationro-
bustness.Seed-Bench[34]issimilartoMMEandMMBenchbutconsistsof19,000multiple-choice
questions. The larger sample size allows it to cover more ability aspects and achieve more robust
results. SEED-Bench-2[33]expandsthedatasetsizeto24,371QApairs,encompassing27evalua-
tiondimensionsandfurthersupportingtheevaluationofimagegeneration. MMT-Bench[69]scales
upthedatasetevenfurther,including31,325QApairsfromvariousscenariossuchasautonomous
driving and embodied AI. It encompasses evaluations of model capabilities such as visual recog-
nition, localization, reasoning, and planning. Additionally, other benchmarks focus on real-world
usage scenarios [20, 48, 6] and reasoning capabilities [70, 5, 23]. However, there are widespread
issues,suchasdatascale,annotationquality,andtaskdifficulty,inthesebenchmarks,makingithard
toassessthechallengesthatMLLMsfaceintherealworld.
MLLMs. This field has undergone significant evolution [68, 19], initially rooted in BERT-based
language decoders and later incorporating advancements in LLMs. MLLMs exhibit enhanced ca-
pabilities and performance, particularly through end-to-end training techniques, by leveraging ad-
vanced LLMs such as GPTs [53, 7], LLaMA [62, 63], Alpaca [59], PaLM [14, 2], BLOOM [52],
Mistral [28], and Vicuna [13]. Recent model developments, including Flamingo [3], PaLI [29],
PaLM-E [16], BLIP-2 [36], InstructBLIP [15], Otter [31], MiniGPT-4 [75], mPLUG-Owl [67],
LLaVA[43],Qwen-VL[4],andVITA[18],bringuniqueperspectivestochallengessuchasscaling
pre-training,enhancinginstruction-followingcapabilities,andovercomingalignmentissues. How-
ever,theperformanceofthesemodelsinthefaceofrealscenarioshasoftennotbeenrevealed.
High-resolution MLLMs. Empirical studies have shown that employing higher resolution is an
effectivesolutionformanytasks[4,41,40,51]. ApproacheslikeLLaVA-Next[42]segmenthigh-
resolutionimagesintomultiplepatches,encodingeachoneindependentlybeforeconcatenatingall
3Eng CN
VV ereV hhe i ih c cV i l lce e eP h l e C Ie i cd nE ole te x us
e
P t L i n nr se i od tt ta e i ie cn n os na t ngCr t cii ea o oC n u no nl Eo tr x i iR nsO e t gc er oi ne gn cnt ea it ti io on n PeM rcu elt pi- tC il oa Mnss C oou nnt iin tVgIn oten i rtio dn i P nered oi gctio nO bject P roperty Contact Information OCP rod R Tu c it h R& n e Adver et Wis ie l mme I
dnt
dent oity tIn efo r Bom Sa oti k eo , n nM Sia sgp in& na gP go e st S Ce & cr he aO nt reh aer cR teT ec Oe Sroxt ps Dg b Cjn ai iU et t o ai in co l gt ad on l re C rr aR s o R mt eua l e nn a Pctd ti oi i en on g rgg n n cs eith pii o tp in
on
2 MM CI i Cn anI iC aI C n i mt Cn - ml n h iGeSt a o GCM bt a re bPu go P ee rt nr Td rVo n M imr - Ln aid e n - Ln V aMV ni-4e e mV nn13 V -Ll t o -i--L 32. -- 8255 - 2 3 42 B. .5 4 B5 B 5 5 4 4 4 4 4 4 4 4A 0 0 6 6 3 3 3 2 1 0c . . . . . . . . . .c 3 1 6 5 6 2 1 8 4
4
2 MinI Y SCI C S iCn -Iln Mh l GSt a ol GiM -t i Mae Pu gMo V oee tr Td Vo n nEL mr -n Ed e n k-Ln V - -V i- 4e 3 ee m 1V n1 3 8Ll t o3 y4 i-L 2. B -- 5 5 BB- 32 4B 5 4 4 4 4 3 3 3 3 3A 5 7 7 5 2 9 8 8 8 7c . . . . . . . . . .c 8 9 0 8 0 8 9 8 5
2
Pedes Et Sgr ioi ga R nIn eln a l aI t ten iA Rn eot lte t t n ai tsn e io n h ot in ti Rni po es lo (n ahi tn iE p o 2 ( nsP RO h) ei l2 ap
t
iO ( o) E ns2 hiV p) O( bjE e2
ct
T) Counting TrafA
fic
u St igo nalno Drm i Mulo tiv -Pi eu dests n
ria n
g Motion M ulti-V
ehicle
M
otionV ehiD cT
le
Mi a ota ioIb ndg el nr te ita
y
Pm F ero cr
e
pe tcF ioao nstr ie nc ga C (s TtoC ai bm no lg ep )m C (a Dp ra iaial s gC c r i rou i aa s nT ml il a o (c )a Tt nu ib a ol ( bl na De let i( )iP aDo ge n i rr a ac ( g mTe rp a )abt mi lo e )n ) LLm
M
LG LLD aG Pe
i
LaY VS e nLS Pm
V
aIe
i
AM UlT
-
Vl Aii -pi M Gn -G-M AVo S
-
Ni 4 en N-E L- 1eE mo D1 ek .-- e
e
5- -. 3 xe 1 o i58 km
x n
-3y 4 tc- 1-B
t
ii -B Op B -V 3n
-
77r 8i BLw 2o BBl
B
3 3 3 3 3 3 3 3
3 2
2
28 6 5 3 2 1 1 1
0 9
8
7. . . . . . . .
. .
.
.8 3 6 9 5 5 5 4
8 5
6
5
LL mM L MGC IL DC a
T
GP eia na iV en a L
n
e
Pm mtV ei m A U
i
xe T-A Cb i pG r n tb - G
-P
Sr- Nn Mi 4e rN - Mi -L ea im oD 1e oaM ee n
-
-.x nn oi 5
V k
m-x n t k-- c3 - -t i 82 i- eO p-
2
V4 n- 77 B. r y8
.
i5 LB w 52 oBB lB 3 3 3 3 3 3 2 2
2 2
2
26 5 4 3 3 0 8 8
7 7
6
5. . . . . . . .
. .
.
.5 7 9 9 6 6 3 1
9 6
4
9
ShareGPT4V-13B 27.2 ShareGPT4V-13B 25.9
(a) Real-WorldTasks (b) Leaderboard
Figure 2: Task Categories (left). Our benchmark spans 5 key domains and 43 subtasks highly
related to real-world scenarios, including 13,366 high-resolution images and 29,429 annotations.
Model Performance (right). Average accuracies of advanced MLLMs are shown across both the
EnglishandChinesesplitsofthedataset.
localpatchtokenswiththeoriginalglobalimagetokens,albeitatanescalatedcomputationalcost.
Othermodels,suchasMonkey[40]andLLaVA-UHD[64],alsosplitimagesintopatchesbutsubse-
quentlycompressthemtoavoidredundanttokens. Mini-Genimi[39]comprisestwinencoders,one
forhigh-resolutionimagesandtheotherforlow-resolutionvisualembedding.Theyworkinanatten-
tionmechanism,wherethelow-resolutionencodergeneratesvisualqueries,andthehigh-resolution
counterpartprovidescandidatekeysandvaluesforreference. Conv-LlaVA[22]employsConvNeXt
insteadofViTasthevisionencoder.Cambrian[61]usesasetoflearnablelatentqueriesthatinteract
with multiple vision features via cross-attention layers. Additionally, SliME [73] stresses the im-
portanceofglobalfeatures,compressingthelocalimagepatchestwicebutpreservingalltheglobal
context. Although many of these models focus on improving resolution, none have been tested in
a rigorous high-resolution benchmark, often providing only intuitive examples that lack informa-
tiveness and convincing results. Our proposed benchmark offers a rigorous evaluation to test the
capabilitiesinunderstandinghigh-resolutionimages.
3 MME-RealWorld
In this section, we outline the data collection process, question annotation procedure, and provide
astatisticaloverviewofeachdomainandsubtaskinMME-RealWorldanditsChineseversion. We
visualizedifferenttasksfromthe5imagedomainsinFig.1. Detailedinformationondatasources,
evaluationtasks,andvisualizedresultscanbefoundinSec.A.
3.1 InstructionandCriterion
For eachquestion, we manually construct four options, withone being thecorrect answerand the
otherthreebeingthetextsappearingintheimageoroptionssimilartothecorrectone. Thisgreatly
enhances the difficulty, forcing the model to deeply understand the details of the image. We also
provideanadditionalchoiceE,whichallowsthemodeltorejectforansweringbecausethereisno
4
Pedestrian
MotionTable1: PromptsettingofMME-RealWorld.
[Image][Question]Thechoicesarelistedbelow:
(A)[Choice A]
(B)[Choice B]
(C)[Choice C]
(D)[Choice D]
(E)[Choice E]
Selectthebestanswertotheabovemultiple-choicequestionbasedontheimage. Respondwith
onlytheletter(A,B,C,D,orE)ofthecorrectoption.
Thebestansweris:
right answer. We try to use the model’s default prompt for multiple-choice questions, but if the
modeldoesnothavethedefaultprompt,weuseacommonpromptasshowninTab.1.
EvaluationMetric. Wefirstapplyarule-basedfiltertotheanswersgeneratedbyMLLM,aligning
themwiththegivenansweroptionsandcheckingforcorrectnessagainstthegroundtruth. Letthe
dataset be denoted as D = {D = {T }Td }D , where each domain D consists of T subtasks.
d t t=1 d=1 d d
Foreachsubtask, wecalculatetheaccuracyacrossallannotations. Foreachdomain, wecompute
twometrics: 1)AverageAccuracy(Avg). theweightedaverageaccuracyacrossallsubtasks,given
by(cid:80)Td
Avg(T )×|T |/|D |,where|·|istheinstancenumbercontainedinoneset,and2)Class-
t=1 t t d
based Average Accuracy (Avg-C). the unweighted average accuracy across subtasks, given by
(cid:80)Td
Avg(T )/T . Similarly,fortheentiredataset,wereporttheoverallAverageAccuracyacross
t=1 t d
allsamples,andtheclass-basedaverageaccuracyacrossdomains.
3.2 DataCollectionandAnnotation
Optical Character Recognition in the Wild (OCR). It is specifically designed to evaluate the
model’sabilitytoperceiveandunderstandtextualinformationinthereal-world. Wemanuallyse-
lecte3,293imageswithcomplexscenesandrecognizabletextinformationfrom150,259imagesin
existinghigh-resolutiondatasetsasourimagesources. Theseimagesspanvariouscategoriessuch
asstreetscenes,shops,posters,books,andcompetitions. Thevolunteersareworkedforannotation,
each with at least a foundational understanding of multimodal models, to independently generate
questionsandanswers. Theseannotationsaresubsequentlyreviewedandfurtherrefinedbyanother
volunteers. Based on the image annotations, we categorize these 3,297 images into 5 perception
tasks, totaling 5,740 QA pairs: contact information and addresses, identity information, products
andadvertisements,signageandothertext,aswellasnaturaltextrecognitioninelevationmapsand
books. Additionally, there are two reasoning tasks with 500 QA pairs: 1) scene understanding of
theentireimage,whichrequiresthemodeltolocateandcomprehendimportanttextsuchascompe-
titionresults,and2)characterunderstanding,focusingoncomicsorposterswherethemodelneeds
toanalyzerelationshipsandpersonalitiesbasedondialogueorpresentation.
RemoteSensing(RS).Theimageshaveawiderangeofapplicationsinreal-worldscenarios. Some
imagespossessextremelyhighquality,withindividualimagesizesreachingupto139MBandcon-
tainingveryrichdetails,whichmakesitdifficultevenforhumanstoperceivespecificobjects. We
manuallyselect1,298high-resolutionimagesfromover70,000publicremotesensingimages,en-
suringthateachimageisofhighquality,withsufficientresolutionandrichdetail. Oneprofessional
researcherisinvolvedinannotatingthedata,andanotherresearcherchecksandimprovestheannota-
tions,resultingin3,738QApairs. Thereare3perceptiontasks: objectcounting,colorrecognition,
andspatialrelationshipunderstanding.
DiagramandTable(DT).Althoughtherearealreadysomedatasetsrelatedtotableandchartun-
derstanding, they mostly feature simple scenes. We focus on highly complex chart data, such as
financial reports, which contain extensive numerical information and mathematical content, pre-
senting new challenges for MLLMs. We filter 2,570 images from the internet, with annotations
performed by two volunteer and reviewed by another one. We categorize these annotations into 4
tasks based on the question format: 1) Diagram and Table Perception (5,433 QA pairs): involve
locatingspecificvaluesofelementswithinthediagramsandtables;2)DiagramReasoning(250QA
pairs): includetaskssuchasidentifyingthemaximumandminimumvaluesinachart,performing
5simplecalculations,andpredictingtrends;and3)tableReasoning(250QApairs): focusonsimple
calculations related to specific elements, understanding mathematical concepts like maximum and
minimumvalues,andlocatingcorrespondingelements.
AutonomousDriving(AD).Itdemandsextensivegeneralknowledgeandembodiedunderstanding
capability.Weemphasizechallengingdrivingscenariosthatinvolvedistantperceptionsandintricate
interactionsamongdynamictrafficagents.Specifically,wemanuallyselectasubsetof2,715images
fromover40,000front-viewimagescapturedbyonboardcamerasinopen-sourcedatasets. These
images cover a diverse range of weather conditions, geographic locations, and traffic scenarios.
Besides,avolunteercarefullyannotateseachimage,andtheotheroneconductsathoroughreview,
resulting in 3,660 QA pairs for perception tasks and 1,334 QA pairs for reasoning tasks. The
perception tasks include objects identification, object attribute identification, and object counting
for traffic elements such as vehicle, pedestrian, and signals. The latter is categorized into 3 main
tasks: 1)IntentionPrediction: focusonpredictingdrivingintentionofadesignatedtrafficagentin
theshort-termfuture. 2)InteractionRelationUnderstanding: involvereasoningaboutegovehicle’s
reactiontoothertrafficelements,andtheinteractionsbetweentheseelements. 3)DriverAttention
Understanding: requirereasoningaboutthetrafficsignalthatthedrivershouldpayattentionto.
Monitoring(MO).Theimagesarefromvariousapplicationscenariosforpublicsafety,e.g.,streets,
shopping malls, and expressway intersections. We focus on complex high-resolution monitoring
images that include many real-world challenges, like scale variations and out-of-view, as possible
which could test whether the model handles them robustly in practice. Specifically, 1,601 high-
resolutionimagesaremanuallyselectedfromover10,000publicdatasetimages,whicharecaptured
from a broad range of cameras, viewpoints, scene complexities, and environmental factors across
dayandnight. Intermsofannotations,twovolunteersmanuallyannotateeachimagecarefully,and
multi-stagecarefulinspectionsandmodificationsareperformedbyanotherone. Whentheserefined
imageannotationsarecompleted,1,601imagesarecategorizedinto3mainperceptiontasks,total-
ing2,196QApairs,includingobjectcountingandlocation,andattributerecognition. Furthermore,
3 reasoning tasks are well-designed with 498 QA pairs: 1) calculate the sum of different objects,
whichrequiresthemodeltoperceivevariousobjectsandcalculatetheirtotalnumberaccurately;2)
intentionreasoning,focusingonreasoningthenextrouteandturnofthespecificobject;3)attribute
reasoning,focusingonreasoningthespecificmaterialsandfunctionsofthegivenobjects.
3.3 MME-RealWorld-CN
ThetraditionalgeneralVQAapproach[45]usesatranslationenginetoextendQApairsfromEn-
glishtoChinese.However,itmayfacevisual-textualmisalignmentproblems[58],failingtoaddress
complexitiesrelatedtonuancedmeaning,contextualdistortion,languagebias,andquestion-typedi-
versity. Additionally,askingquestionsinChineseaboutimagescontainingonlyEnglishtextsisnot
intuitive for benchmarking Chinese VQA capabilities. By contrast, we follow the steps below to
constructahigh-qualityChinesebenchmark:
• Selection. For video monitoring, autonomous driving, and remote sensing, many images
do not contain English information. Therefore, we select a subset of the aforementioned
questionpairs,double-checkingtoensuretheydonotcontainanyEnglishinformation.
• Translation. Translatethequestionsandanswersbyfourprofessional researchers, allof
whomarefamiliarwithbothEnglishandChinese.
• Collection. For diagrams and tables, since the original images often contain English in-
formation(e.g.,legends/captions),wecollectadditional300tablesand301diagramsfrom
theInternet,wherethecontentsareinChinese. Thisdataisfurtherannotatedbyonevol-
unteer,resultingin301×4QApairs,wherethetasktypeisthesameasdiagramandtable
inMME-RealWorld. Similarly,forOCRinthewild,wealsocollectadditional939images
forallthesubtasks.
In total, MME-RealWorld-CN has 1,889 additional images and total 5,917 QA pairs, which is
a smaller version of MME-RealWorld, but it retains similar task types, image quality, and task
difficulty. TheexamplescanbeseeninFig.13.
3.4 QualityControlandAnalysis
6During the annotation process, we impose the Table 2: Comparison of benchmarks. MME-
following requirements on annotators: 1. We RealWorld is the largest fully human-annotated
ensurethatallquestionscanbeansweredbased dataset, featuring the highest average resolution
on the image (except for specially constructed andthemostchallengingtasks.
questions where the correct option is “E”), FullyHuman Average LLaVA-1.5-7B
meaning that humans can always find the an- B Vie zn Wch izmark #Q 8A 00-P 0air Anno ×tation C ×N 1R 2e 2s 4o ×lu 1t 2io 2n
4
Perfo 5r 0m .0ance
swerswithintheimage.Thisapproachprevents R Tee xa tl VW Qo ArldQA 57 76 35
4
✓× ×× 1 95 83 56 ×× 78 66 83 58-
.2
forcingannotatorstoprovideanswersbasedon M MM ME Bench 2 33 27 14 7 ✓ ✓ ✓× 1 51 16 21 ×× 28 74 00 67 46 .. 30
low-qualityimagesorimagescontainingvague MMStar 1500 × × 512×375 30.3
ScienceQA 21000 × × 378×249 71.6
information. 2. The area of the object be- ChartQA 32719 × × 840×535 -
MM-Vet 218 × × 1200×675 31.1
ing questioned in each image must not exceed Seed-Bench 19242 × × 1024×931 66.1
SEED-Bench-2-Plus 2300 × × 1128×846 36.8
1/10 of the total image area. This ensures MMT-Bench 32325 × × 2365×377 49.5
MathVista 735 × × 539×446 26.1
that the object is not overly prominent, pre- TouchStone 908 × × 897×803 -
VisIT-Bench 1159 × × 765×1024 -
ventinghumansfromeasilyidentifyingthean- BLINK 3807 × × 620×1024 37.1
CV-Bench 2638 × × 1024×768 -
swer at first glance. 3. Each annotation is MME-RealWorld 29429 ✓ ✓ 2000x1500 24.9
cross-checkedbyatleasttwoprofessionalmul-
timodalresearcherstoensureaccuracyandpreventannotationerrorscausedbyhumanbias.
ThecomparisonofbenchmarksisshowninTab.2. ThemaximalresolutionofMME-RealWorldis
42,177,408 pixels, with dimensions of 5304×7952. The average resolution is 3,007,695 pixels,
equivalent to an image size of approximately 2000×1500. This resolution is significantly higher
thanthatofexistingbenchmarks. Forinstance, thehighestbenchmark, MME,hasanaverageres-
olution of 975,240 pixels, corresponding to an image size of about 1161×840. The exceptional
image quality and our strict, fully human annotation process make our tasks the most challenging
among all benchmarks. This is evidenced by the baseline model LLaVA-1.5-7B achieving an ac-
curacy of just 24.9%, significantly lower than on other benchmarks. Although some benchmarks
mayapproachourlevelofdifficulty,thisisprimarilyduetotheinherentcomplexityoftheirtasks.
Forinstance, MathVistafocusesonpuremathematicalproblems, andMM-Vetinvolvesmulti-step
reasoning—bothofwhicharenaturallychallengingandresultinlowerbaselineperformance. How-
ever, the majority of our tasks are centered on real-world perception problems. This means that,
currentMLLMsstillstruggletoeffectivelyaddresshuman-levelperceptualchallenges.
4 Experiments
We evaluate a total of 24 open-source MLLMs, including Qwen-VL-Chat [4], LLaVA, LLaVA-
Next[30],TextMonkey[46],mPLUG-DocOwl1.5[25],ShareGPT4V[11],MiniGPT-v2[9],Mon-
key[40],OtterHD[31],Cambrian-1[61],Mini-Gemini-HD[39],MiniCPM-V2.5[26],DeepSeek-
VL [47], YI-VL-34B1, SliME [73], CogVLM22, InternLM-XComposer2.5 [72], InternVL-Chat
V1-5, andInternVL-2[12], aswellas4close-sourceMLLMs, including, GPT-4o3, GPT-4o-mini,
Gemini1.5pro[60],andClaude3.5Sonnet4.
4.1 ResultsonMME-RealWorld
4.1.1 Perception
Tab.3presentstheperceptioncapabilitiesofdifferentmodelsacross5domains. Overall,InternVL-
2demonstratesthestrongestperceptionabilities,outperformingotherclosed-sourcemodels. How-
ever,theperformancevariesacrossdifferenttasks,withsomekeyobservationsasfollows:
1. GPT-4o performs best in real-world OCR tasks, achieving 77% accuracy, but its performance
drops significantly in more challenging tasks, falling behind other top-ranked models. This trend
is also observed in other closed-source models, such as Gemini-1.5-Pro and GPT-4o-mini, which
perform well in OCR tasks but struggle significantly in other real-world tasks. There are three
possible reasons: 1) Close-source models often have limitations on the maximum image size and
resolutionwhenuploadinglocalimages.Forexample,Claude3.5Sonnethasamaximumresolution
1https://huggingface.co/01-ai/Yi-VL-34B
2https://github.com/THUDM/CogVLM2
3https://openai.com/index/hello-gpt-4o/
4https://www.anthropic.com/news/claude-3-5-sonnet
7Table3:Experimentalresultsontheperceptiontasks. Modelsarerankedaccordingtotheiraver-
ageperformance. Rowscorrespondingtoproprietarymodelsarehighlightedingrayfordistinction.
“OCR”, “RS”, “DT”, “MO”, and “AD” each indicate a specific task domain: Optical Character
RecognitionintheWild,RemoteSensing,DiagramandTable,Monitoring,andAutonomousDriv-
ing, respectively. “Avg” and “Avg-C” indicate the weighted average accuracy and the unweighted
averageaccuracyacrosssubtasksineachdomain.
Method LLM Perception
TaskSplit OCR RS DT MO AD Avg Avg-C
#QApairs 5740 3738 5433 2196 3660 20767 20767
InternVL-2 InternLM2.5-7B-Chat 73.92 39.35 62.80 53.19 35.46 55.82 52.94
Claude3.5Sonnet - 72.47 25.74 67.44 32.19 40.77 52.90 47.72
InternLM-XComposer2.5 InternLM2-7B 69.25 36.12 63.92 39.48 33.63 52.47 48.48
InternVL-Chat-V1.5 InternLM2-Chat-20B 71.51 33.55 55.83 51.16 31.42 51.36 48.69
Mini-Gemini-34B-HD Nous-Hermes-2-Yi-34B 69.55 40.40 44.36 39.61 32.70 48.05 45.32
MiniCPM-V2.5 Llama3-8B 66.79 27.69 52.81 38.70 34.15 47.37 44.03
Cambrian-1-34B Nous-Hermes-2-Yi-34B 66.45 38.63 40.44 45.98 33.61 46.68 45.02
GPT-4o - 77.69 28.92 46.68 33.93 22.43 46.43 41.93
CogVLM2-llama3-Chat Llama3-8B 69.97 28.76 47.51 33.74 30.22 45.84 42.04
Cambrian-1-8B Llama3-8B-Instruct 58.68 40.05 32.73 47.68 38.52 43.82 43.53
SliME-8B Llama3-8B 53.45 42.27 29.34 40.62 33.66 40.29 39.87
Gemini-1.5-pro - 67.62 13.99 39.90 31.11 26.64 39.63 35.85
GPT-4o-mini - 62.51 6.69 44.23 26.50 24.18 37.12 32.82
Monkey Qwen-7B 54.63 24.99 32.51 28.01 29.67 36.30 33.96
mPLUG-DocOwl1.5 Llama-7B 51.15 23.71 29.34 24.97 28.28 33.71 31.49
DeepSeek-VL DeepSeek-LLM-7b-base 49.55 25.49 23.38 26.97 33.39 33.14 31.76
SliME-13B Vicuna-13B 50.58 25.82 20.93 24.73 27.16 31.50 29.84
Mini-Gemini-7B-HD Vicuna-7B-v1.5 42.02 31.30 22.31 34.15 24.81 31.07 30.92
YI-VL-34B Yi-34B-Chat 44.95 31.62 15.99 34.85 28.31 30.97 31.14
LLaVA-Next Llama3-8B 47.94 25.42 26.63 19.46 18.66 30.14 27.62
LLaVA-Next Qwen-72B 37.07 29.13 27.68 29.37 17.98 29.01 28.25
LLaVA1.5-13B Vicuna-13B 44.10 23.27 20.17 20.45 26.12 28.42 26.82
ShareGPT4V-13B Vicuna-13B 44.55 23.06 20.17 19.26 26.12 28.38 26.63
MiniGPT-v2 Llama2-7B-Chat 39.02 23.33 20.41 19.26 25.96 26.94 25.60
ShareGPT4V-7B Vicuna-7B 39.39 22.10 20.08 19.13 26.04 26.73 25.35
LLaVA1.5-7B Vicuna-7B 38.69 22.12 20.08 19.13 26.04 26.54 25.21
Qwen-VL-Chat Qwen-7B 32.37 15.14 15.59 22.13 15.08 20.75 20.06
TextMonkey Qwen-7B 37.30 11.69 5.93 16.14 14.26 18.18 17.06
limitof8Kandamaximumimagequalityof5MB,whileGPT-4oandGemini-proallowupto20MB.
Thisrestrictstheinputofsomehigh-qualityimages,aswehavetocompresstheimagesforupload.
2)Close-sourcemodelstendtobemoreconservative. Weobservethattheproportionofresponses,
where closed-source models output “E” indicating that the object in question is not present in the
image,ishigh. Thissuggeststhatthesemodelsmayadoptaconservativeresponsestrategytoavoid
hallucinations or to provide safer answers. 3) Closed-source models sometimes refuse to answer
certainquestions. Duetodifferentinput/outputfilteringstrategies,somesamplesareconsideredto
involveprivacyorharmfulcontentandarethereforenotanswered.
2. Models allowing higher resolution input, such as Mini-Gemini-HD and SliME, demonstrate a
significant advantage over models directly using vision encoders like CLIP, such as ShareGPT4V
andLLaVA1.5.Atthesamemodelsize,thesemodelsconsistentlyimproveacrossdifferentsubtasks.
Thishighlightsthecriticalimportanceofhigh-resolutionimageprocessingforaddressingcomplex
real-worldtasks.
3. Therearealsonotabletrendsacrossdifferentdomains. Remotesensingtasksinvolveprocessing
extremelylargeimages,demandingadeepercomprehensionofimagedetails. Modelsthatfocuson
high-resolutioninput,suchasCambrian-1,Mini-Gemini-HD,andSliME,outperformothermodels
inthesetasks. Additionally,modelstrainedonlargeamountsofchartdataexhibitimprovedpercep-
tioncapabilitiesforcomplexcharts. Forinstance,SliMEandLLaVA1.5havelimitedandrelatively
simplechartdataintheirtrainingsets,resultingininferiorperformanceinthiscategorycompared
tomorerecentmodels.
4.1.2 Reasoning
ExperimentalresultsonthereasoningtasksareshowninTab.4.Intermsofreasoningability,Claude
3.5 Sonnet distinguishes itself as the top performer across most domains, particularly outpacing
8the second-place GPT-4o by 16.4% in chart-related tasks. The closed-source model GPT-4o also
performs well, trailing slightly behind the second-place InternVL-2 but even outperforming it in
severaldomains. Mostopen-sourcemodelsperformpoorly,withtraditionalbaselinemethodssuch
asLLaVA1.5andQwen-VL-Chatyieldingresultsclosetorandomguessing.Furthermore,reasoning
tasks are more challenging than perception tasks. Even the top-ranked model fails to achieve an
average accuracy above 45%, with class-based accuracy not exceeding 50%. This indicates that
currentmodelsstillhaveasignificantgaptobridgetoreachhuman-levelreasoningcapabilities.
Table4: Experimentalresultsonthereasoningtasks. Modelsarerankedaccordingtotheiraver-
ageperformance. Rowscorrespondingtoproprietarymodelsarehighlightedingrayfordistinction.
“OCR”, “RS”, “DT”, “MO”, and “AD” each indicate a specific task domain: Optical Character
RecognitionintheWild,RemoteSensing,DiagramandTable,Monitoring,andAutonomousDriv-
ing, respectively. “Avg” and “Avg-C” indicate the weighted average accuracy and the unweighted
averageaccuracyacrosssubtasksineachdomain.
Method LLM Reasoning
TaskSplit OCR DT MO AD Avg Avg-C
#QApairs 500 500 498 1334 2832 2832
Claude3.5Sonnet - 61.90 61.20 41.79 31.92 44.12 49.20
InternVL-2 InternLM2.5-7B-Chat 57.40 39.00 43.57 29.84 38.74 42.45
GPT-4o - 61.40 44.80 36.51 26.41 37.61 42.28
CogVLM2-llama3-Chat Llama3-8B 54.00 32.80 41.16 31.18 37.25 39.79
InternVL-Chat-V1-5 InternLM2-Chat-20B 56.80 35.40 37.35 28.94 36.48 39.62
Cambrian-1-8B Llama3-8B-Instruct 53.20 27.40 42.37 30.73 36.16 38.43
SliME-8B Llama3-8B 53.20 29.40 36.14 31.55 35.80 37.57
MiniCPM-V2.5 Llama3-8B 44.00 31.80 36.95 31.03 34.50 35.95
SliME-13B Vicuna-13B 41.00 39.00 33.13 30.80 34.46 35.98
InternLM-XComposer2.5 InternLM2-7B 53.40 41.00 17.67 29.99 33.90 35.52
GPT-4o-mini - 47.00 39.80 25.81 26.79 32.48 34.85
YI-VL-34B Yi-34B-Chat 42.40 26.00 31.33 31.55 32.45 32.82
LLaVA-Next Llama3-8B 55.20 23.40 21.08 30.73 32.06 32.60
Mini-Gemini-34B-HD Nous-Hermes-2-Yi-34B 59.20 39.20 20.48 22.84 31.73 35.43
Gemini-1.5-pro - 52.70 33.20 28.33 19.20 29.19 33.36
Monkey Qwen-7B 27.20 20.80 27.31 33.04 28.84 27.09
DeepSeek-VL DeepSeek-LLM-7b-base 45.20 23.80 16.67 27.31 27.98 28.25
LLaVA-Next Qwen-72B 17.20 34.20 27.31 29.69 27.86 27.10
Cambrian-1-34B Nous-Hermes-2-Yi-34B 55.00 36.00 19.48 16.07 27.06 31.64
mPLUG-DocOwl1.5 Llama-7B 42.60 19.80 20.48 26.04 26.88 27.23
Mini-Gemini-7B-HD Vicuna-7B-v1.5 35.40 24.60 25.90 23.29 26.12 27.30
LLaVA1.5-13B Vicuna-13B 30.20 20.80 27.51 24.78 25.51 25.82
ShareGPT4V-13B Vicuna-13B 26.00 20.80 27.31 24.55 24.63 24.67
LLaVA1.5-7B Vicuna-7B 26.00 20.60 25.90 24.18 24.17 24.17
ShareGPT4V-7B Vicuna-7B 24.15 20.60 26.10 24.18 23.88 23.76
MiniGPT-v2 Llama2-7B-Chat 30.00 20.40 16.87 23.66 23.01 22.73
Qwen-VL-Chat Qwen-7B 28.60 13.60 16.47 24.63 21.95 20.83
TextMonkey Qwen-7B 30.40 2.20 4.42 20.01 15.96 14.26
4.2 ResultsonMME-RealWorld-CN
ResultsofperceptiontasksandreasoningtasksarepresentedinTab.5andTab.6,respectively. The
modelsshowdifferentperformancescomparedtotheMME-RealWorldEnglishversion.
1)InternVL-2significantlyoutperformsexistingmodelsinbothperceptionandreasoningtasksin
theChineseversion, evensurpassingitsperformanceontheEnglishversion, indicatingthatithas
beenspecificallyoptimizedforChinesedata.
2) There is a substantial difference in how models handle Chinese and English data, with some
modelsperformingmuchworseinChinesescenarios,particularlyinreasoningtasks. Forinstance,
GPT-4o and GPT-4o-mini show a performance drop of nearly 10%. However, some models seem
to excel in Chinese-related tasks. Notably, models based on Llama3-8B generally achieve strong
resultsinbothChineseperceptionandreasoningtasks,suchasSliMEandCogVLM2.Thissuggests
thatLlama3-8BmaybeaneffectiveLLMbackboneforChinesetasks.
9Table 5: Experimental results on the perception tasks of MME-RealWorld-CN. Models are
rankedaccordingtotheiraverageperformance.Rowscorrespondingtoproprietarymodelsarehigh-
lightedingrayfordistinction. “OCR”,“RS”,“DT”,“MO”,and“AD”eachindicateaspecifictask
domain: Optical Character Recognition in the Wild, Remote Sensing, Diagram and Table, Moni-
toring, and Autonomous Driving, respectively. “Avg” and “Avg-C” indicate the weighted average
accuracyandtheunweightedaverageaccuracyacrosssubtasksineachdomain.
Method LLM Perception
TaskSplit OCR RS DT MO AD Avg Avg-C
#QApairs 1908 300 602 500 700 4010 4010
InternVL-2 InternLM2.5-7B-Chat 69.92 41.33 71.63 53.19 34.14 59.70 54.04
InternVL-Chat-V1-5 InternLM2-Chat-20B 60.59 32.00 60.12 32.40 32.14 49.90 43.45
Claude3.5Sonnet - 54.44 32.67 74.09 25.00 32.43 48.25 43.73
SliME-8B Llama3-8B 53.93 41.33 58.25 29.20 31.29 46.60 42.80
GPT-4o - 55.90 23.67 54.86 25.20 21.14 43.44 36.15
YI-VL-34B Yi-34B-Chat 51.41 34.33 49.52 25.20 27.71 42.45 37.63
SliME-13B Vicuna-13B 50.63 17.33 48.49 17.80 33.23 40.69 33.50
Cambrian-1-34B Nous-Hermes-2-Yi-34B 48.11 33.79 44.34 27.60 26.43 40.13 36.05
CogVLM2-llama3-Chat Llama3-8B 46.12 22.00 39.48 24.80 34.14 38.57 33.31
Mini-Gemini-34B-HD Nous-Hermes-2-Yi-34B 41.82 38.28 40.60 27.80 34.29 38.31 36.56
LLaVA-Next Llama3-8B 40.62 31.67 37.49 35.40 27.29 36.50 34.49
Gemini-1.5-pro - 48.32 12.33 39.78 25.20 17.57 36.10 28.64
Monkey Qwen-7B 40.46 26.55 41.12 19.20 35.86 36.07 32.64
InternLM-XComposer2.5 InternLM2-7B 39.26 38.33 38.88 19.40 33.57 35.66 33.89
Mini-Gemini-7B-HD Vicuna-7B-v1.5 39.66 17.24 39.29 16.80 28.29 33.09 28.26
Cambrian-1-8B Llama3-8B-Instruct 32.71 35.86 30.28 27.60 35.57 32.44 32.40
mPLUG-DocOwl1.5 LLama-7B 33.33 18.62 31.83 25.60 28.43 30.19 27.56
LLaVA-Next Qwen-72B 32.76 23.67 28.69 34.60 23.14 30.02 28.57
MiniCPM-V2.5 Llama3-8B 33.23 16.67 31.67 20.40 26.00 28.89 25.59
DeepSeek-VL DeepSeek-LLM-7b-base 27.10 25.44 26.02 21.60 35.71 27.63 27.17
TextMonkey Qwen-7B 31.24 11.38 30.76 19.60 26.71 27.44 23.94
GPT-4o-mini - 29.56 7.33 31.79 22.00 24.00 26.32 22.94
Qwen-VL-Chat Qwen-7B 27.36 15.00 27.89 24.29 27.36 26.13 24.38
ShareGPT4V-13B Vicuna-13B 27.94 17.59 27.57 16.80 28.14 25.75 23.61
LLaVA1.5-13B Vicuna-13B 27.52 17.33 26.25 17.00 28.66 25.45 23.35
MiniGPT-v2 Llama2-7B-Chat 26.78 19.31 27.05 14.40 29.43 25.18 23.39
ShareGPT4V-7B Vicuna-7B 26.73 17.24 25.75 16.60 28.14 24.86 22.89
LLaVA1.5-7B Vicuna-7B 26.36 16.67 25.75 16.60 28.14 24.64 22.70
4.3 Fine-grainedAnalysisandFindings
Existing Models Still Lacking in Image Detail Perception. Fig. 3 displays the frequency with
whichvariousmodelschoose“E”astheiranswer. Wecompare4close-sourcemodelswiththebest-
performing open-source model, InternVL-2. During our annotation process, the frequency of “E”
answers does not exceed 5% of the overall data, meaning it represents only a small portion of the
totalQApairs. However,nearlyallmodelsshowamuchhigherfrequencyof“E”outputsthanthe
actualnumberof“E”instancespresentinourbenchmark. Thisindicatesthatmostmodels’visual
perceptionmodulesfailtoidentifytheobjectsintheimagescorrespondingtoourquestions.
Limitations of MLLMs in Understanding Dynamic Information. In combination with the re-
sults from autonomous driving and monitoring tasks, we observe that MLLMs exhibit significant
deficienciesinunderstanding,predicting,andreasoningaboutthedynamicinformationofobjects,
suchaspredictingthesteeringofacar. Althoughtheinputtothesemodelsisasingleframeimage
ratherthanavideo,thereremainsaconsiderablegapbetweentheirperformanceandthatofhumans.
Therefore,itseemsthattheseMLLMsarestillfarfromhavingthecapabilitytobeworldmodels.
ComputationEfficiency. Thereisasignificantdisparityincomputationefficiencyamongdifferent
modelswhenprocessinghigh-resolutionimages. Forexample,usingmodelssimilartoLLMs(e.g.,
Vicuna-13B),thecomputationalrequirementsforhandlingimagesexceeding1024×1024resolution
areasfollows: LLaVA1.5requires16.37TFLOPS,SliMErequires40.82TFLOPS,whileLLaVA-
NextandMini-Gemini-HDrequire78.37and87.59TFLOPS,respectively.LLaVA-NextandSliME
employdynamicchunkingandencodingofimages,whileMini-Gemini-HDusesahigher-resolution
vision encoder and significantly increases the number of vision tokens, resulting in a computation
cost approximately 5 times that of LLaVA1.5. Additionally, existing methods have inherent lim-
itations in handling high-resolution images. For example, Mini-Gemini-HD resizes images larger
than 672×672 to this size, causing a loss of more details. Moreover, we observe interesting phe-
10Table 6: Experimental results on the reasoning tasks of MME-RealWorld-CN. Models are
rankedaccordingtotheiraverageperformance.Rowscorrespondingtoproprietarymodelsarehigh-
lightedingrayfordistinction.“OCR”,“DT”,“MO”,and“AD”eachindicateaspecifictaskdomain:
OpticalCharacterRecognitionintheWild,DiagramandTable,MonitoringandAutonomousDriv-
ing, respectively. “Avg” and “Avg-C” indicate the weighted average accuracy and the unweighted
averageaccuracyacrosssubtasksineachdomain.
Method LLM Reasoning
TaskSplit OCR DT MO AD Avg Avg-C
#QApairs 207 602 298 800 1907 1907
InternVL-2 InternLM2.5-7B-Chat 61.90 61.20 41.79 31.92 44.12 49.20
Claude3.5Sonnet - 57.40 39.00 43.57 29.84 38.74 42.45
SliME-8B Llama3-8B 61.40 44.80 36.51 26.41 37.61 42.28
InternVL-Chat-V1-5 InternLM2-Chat-20B 54.00 32.80 41.16 31.18 37.25 39.79
CogVLM2-llama3-Chat Llama3-8B 56.80 35.40 37.35 28.94 36.48 39.62
YI-VL-34B Yi-34B-Chat 53.20 27.40 42.37 30.73 36.16 38.43
Monkey Qwen-7B 53.20 29.40 36.14 31.55 35.80 37.57
Mini-Gemini-7B-HD Vicuna-7B-v1.5 44.00 31.80 36.95 31.03 34.50 35.95
Mini-Gemini-34B-HD Nous-Hermes-2-Yi-34B 41.00 39.00 33.13 30.80 34.46 35.98
LLaVA-Next Llama3-8B 53.40 41.00 17.67 29.99 33.90 35.52
Cambrian-1-8B Llama3-8B-Instruct 47.00 39.80 25.81 26.79 32.48 34.85
SliME-13B Vicuna-13B 42.40 26.00 31.33 31.55 32.45 32.82
LLaVA-Next Qwen-72B 55.20 23.40 21.08 30.73 32.06 32.60
InternLM-XComposer2.5 InternLM2-7B 59.20 39.20 20.48 22.84 31.73 35.43
GPT-4o - 52.70 33.20 28.33 19.20 29.19 33.36
DeepSeek-VL DeepSeek-LLM-7b-base 27.20 20.80 27.31 33.04 28.84 27.09
Cambrian-1-34B Nous-Hermes-2-Yi-34B 45.20 23.80 16.67 27.31 27.98 28.25
LLaVA1.5-13B Vicuna-13B 17.20 34.20 27.31 29.69 27.86 27.10
ShareGPT4V-13B Vicuna-13B 55.00 36.00 19.48 16.07 27.06 31.64
MiniCPM-V2.5 Llama3-8B 42.60 19.80 20.48 26.04 26.88 27.23
LLaVA1.5-7B Vicuna-7B 35.40 24.60 25.90 23.29 26.12 27.30
ShareGPT4V-7B Vicuna-7B 30.20 20.80 27.51 24.78 25.51 25.82
Qwen-VL-Chat Qwen-7B 26.00 20.80 27.31 24.55 24.63 24.67
GPT-4o-mini - 26.00 20.60 25.90 24.18 24.17 24.17
MiniGPT-v2 Llama2-7B-Chat 24.15 20.60 26.10 24.18 23.88 23.76
mPLUG-DocOwl1.5 LLama-7B 30.00 20.40 16.87 23.66 23.01 22.73
TextMonkey Qwen-7B 28.60 13.60 16.47 24.63 21.95 20.83
Gemini-1.5-pro - 30.40 2.20 4.42 20.01 15.96 14.26
6000
Model
5000 Total QA pairs
Claude 3.5 Sonnet
4000 GPT-4o
GPT-4o-mini
3000 Gemini-1.5-pro
InternVL-2
2000 QA pairs with answer "E"
1000
0
OCR (P) RS (P) DT (P) MO (P) AD (P) OCR (R) DT (R) MO (R) AD (R)
Figure3: Frequencyofoutputtinganswer“E”fordifferentmodelsacrossvariousdomains. The
notationinparenthesesindicatesthetasktype: PforperceptionandRforreasoning. ThetotalQA
pairsandthosewithanswer“E”arealsopresentedforcomparison.
11
selpmaS
#2000
0 881 853 394 1684 1500 0 628 678 589 2002 0 685 427 239 2273 2000
696 0 957 418 1651 1250 1117 0 863 654 2107 1500 895 0 460 244 2064 1500
1000
520 801 0 444 1593 750 827 670 0 667 2072 1000 772 777 0 294 1859 1000
497 667 1004 0 1349 500 648 498 728 0 1838 675 720 418 0 1598
500 500
55 71 67 39 0 250 48 41 39 33 0 32 45 22 16 0
0 0 0
A B C D E A B C D E A B C D E
Predicted Labels Predicted Labels Predicted Labels
(a) Claude3.5Sonnet (b) GPT-4o (c) Cambrian-1-34B
2500
0 1051 1226 313 801 0 1011 419 235 456 0 947 723 484 1074 1000
2500
2000
2961 0 1247 276 848 2000 2523 0 325 231 568 1104 0 704 524 965 800
1500
2684 1194 0 256 803 1500 2343 1114 0 226 534 920 888 0 574 848 600
2512 1090 1177 0 702 1000 2214 936 323 0 445 1000 811 726 708 0 773 400
500 500 200
323 97 88 48 0 334 114 35 13 0 83 93 66 52 0
0 0 0
A B C D E A B C D E A B C D E
Predicted Labels Predicted Labels Predicted Labels
(d) Monkey (e) mPLUG-DocOwl1.5 (f) InternVL-2
Figure4: Distributionofincorrecchoices. Thematrixrevealsdistinctresponsebehaviorsamong
differentMLLMs. Largermodelstendtoselectthesaferoption“E”,whilesmallermodelsexhibita
biastowardthefirstoption“A”.InternVL-2,however,showsauniqueuniformerrordistribution.
nomenainclosed-sourcemodelsregardingimageresolution. Forinstance,GPT-4o-miniusesover
10,000tokensforsomelargeimages,whichisabout10timesmorethanotherclosed-sourcemod-
els,althoughitsperformancedoesnotsignificantlysurpassothermodels. Overall,wecurrentlylack
methodsthatcanefficientlyhandlehigherresolutionimageswithlowercomputationaloverhead.
Analyzing Incorrect Choices. We investigate the distribution of incorrect choices across a range
of models, as shown in Fig. 4. We can see that MLLMs show different response strategies when
dealingwithquestionsimbuedwithuncertainty. Largermodelsgenerallyadoptamoreconservative
approach,oftenoptingforthesaferresponse“E”,asillustratedfromFig.4(a)to4(c). Incontrast,
smaller MLLMs often lean towards the first option—usually option “A”—in similar situations, as
showninFig.4(d)and4(e). Notably,InternVL-2presentsauniquedistributionofincorrectchoices
thatisremarkablyuniform,whichmayaccountforitsexceptionalperformanceinourevaluation.
Instruction Following Abilities. As described in Sec. 3.1, our prompts specify that the model
should directly select and output a single answer. In this regard, closed-source models generally
performbetter,withoutputsbeingmoreconciseanddirectlyalignedwiththeinstructions.However,
wehaveobservedthatsomeopen-sourcemodelsdonotstrictlyadheretoourqueriesandgeneratea
significantamountofadditionalanalysis.Sometimes,theyevenproduceoutputsthatareexcessively
verbose,continuinguntilthetokencountreachesthepredefinedmaximumlimit. Thisindicatesthat
theopen-sourcemodelshavealotofroomforoptimizationintheabilityofinstructionfollowing.
Fordetailedresultsandanalysisofalldomainsandsubtasks,pleaserefertoAppendixSec.B.
5 Conclusion
In this paper, we have introduced MME-RealWorld, a comprehensive benchmark designed to ad-
dresskeylimitationsinexistingevaluationsofMLLMs,suchasdatascale,annotationquality,and
task difficulty. As the largest purely human-annotated dataset with the highest resolution to date,
MME-RealWorld benefits from the participation of 32 annotators, ensuring high data quality and
minimal individual bias. Most QA pairs focus on real-world scenarios, such as autonomous driv-
ing and video surveillance, which have significant applicability. Furthermore, we propose MME-
RealWorld-CN,abenchmarkspecificallyfocusedonChinesescenarios,ensuringthatallimagesand
12
hturT
dnuorG
hturT
dnuorG
A
B
C
D
E
A
B
C
D
E
hturT
dnuorG
hturT
dnuorG
A
B
C
D
E
A
B
C
D
E
hturT
dnuorG
hturT
dnuorG
A
B
C
D
E
A
B
C
D
EquestionsarerelevanttoChinesecontexts. Ourevaluationofawiderangeofmodelsrevealssignifi-
cantperformancegaps,highlightingthecurrentmodels’shortcomingsincompleximageperception
andunderscoring,andtheneedforfurtheradvancements.
References
[1] EirikurAgustssonandRaduTimofte. Ntire2017challengeonsingleimagesuper-resolution:
Datasetandstudy. InCVPR,2017.
[2] RohanAnil,AndrewMDai,OrhanFirat,MelvinJohnson,DmitryLepikhin,AlexandrePas-
sos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical
report. arXivpreprintarXiv:2305.10403,2023.
[3] AnasAwadalla,IrenaGao,JoshGardner,JackHessel,YusufHanafy,WanrongZhu,Kalyani
Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-
source framework for training large autoregressive vision-language models. arXiv preprint
arXiv:2308.01390,2023.
[4] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,Chang
Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile
abilities. arXivpreprintarXiv:2308.12966,2023.
[5] ShuaiBai, ShushengYang, JinzeBai, PengWang, XingxuanZhang, JunyangLin, Xinggang
Wang, Chang Zhou, and Jingren Zhou. Touchstone: Evaluating vision-language models by
languagemodels. arXivpreprintarXiv:2308.16890,2023.
[6] Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh
Gardner, Rohan Taori, and Ludwig Schimdt. Visit-bench: A benchmark for vision-language
instructionfollowinginspiredbyreal-worlduse. arXivpreprintarXiv:2308.06595,2023.
[7] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language
modelsarefew-shotlearners. NeurIPS,2020.
[8] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu,
Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal
datasetforautonomousdriving. InCVPR,2020.
[9] JunChen,DeyaoZhu,XiaoqianShen,XiangLi,ZechunLiu,PengchuanZhang,Raghuraman
Krishnamoorthi,VikasChandra,YunyangXiong,andMohamedElhoseiny. Minigpt-v2: large
languagemodelasaunifiedinterfaceforvision-languagemulti-tasklearning. arXivpreprint
arXiv:2310.09478,2023.
[10] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan,
Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-
languagemodels? arXivpreprintarXiv:2403.20330,2024.
[11] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and
Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv
preprintarXiv:2311.12793,2023.
[12] ZheChen,JiannanWu,WenhaiWang,WeijieSu,GuoChen,SenXing,MuyanZhong,Qin-
glong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai.
Internvl: Scalingupvisionfoundationmodelsandaligningforgenericvisual-linguistictasks.
arXivpreprintarXiv:2312.14238,2023.
[13] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,
SiyuanZhuang,YonghaoZhuang,JosephEGonzalez,etal. Vicuna: Anopen-sourcechatbot
impressinggpt-4with90%*chatgptquality. Seehttps://vicuna.lmsys.org(accessed14April
2023),2023.
[14] AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,Adam
Roberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal. Palm:
Scalinglanguagemodelingwithpathways. JMLR,2023.
13[15] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng
Wang, Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose
vision-languagemodelswithinstructiontuning. NeurIPS,2024.
[16] DannyDriess,FeiXia,MehdiSMSajjadi,CoreyLynch,AakankshaChowdhery,BrianIchter,
Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied
multimodallanguagemodel. arXivpreprintarXiv:2303.03378,2023.
[17] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,JinruiYang,
Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: A comprehensive evaluation benchmark for
multimodallargelanguagemodels. arXivpreprintarXiv:2306.13394,2023.
[18] ChaoyouFu,HaojiaLin,ZuweiLong,YunhangShen,MengZhao,YifanZhang,XiongWang,
DiYin,LongMa,XiawuZheng,etal.Vita:Towardsopen-sourceinteractiveomnimultimodal
llm. arXivpreprintarXiv:2408.05211,2024.
[19] ChaoyouFu,RenruiZhang,HaojiaLin,ZihanWang,TiminGao,YongdongLuo,YuboHuang,
ZhengyeZhang,LongtianQiu,GaoxiangYe,etal. Achallengertogpt-4v? earlyexplorations
ofgeminiinvisualexpertise. arXivpreprintarXiv:2312.12436,2023.
[20] XingyuFu,YushiHu,BangzhengLi,YuFeng,HaoyuWang,XudongLin,DanRoth,NoahA
Smith,Wei-ChiuMa,andRanjayKrishna. Blink: Multimodallargelanguagemodelscansee
butnotperceive. arXivpreprintarXiv:2404.12390,2024.
[21] HaoxiangGao,YaqianLi,KaiwenLong,MingYang,andYiqingShen. Asurveyforfounda-
tionmodelsinautonomousdriving. arXivpreprintarXiv:2402.01105,2024.
[22] ChunjiangGe,SijieCheng,ZimingWang,JialeYuan,YuanGao,JunSong,ShijiSong,Gao
Huang,andBoZheng. Convllava: Hierarchicalbackbonesasvisualencoderforlargemulti-
modalmodels. arXivpreprintarXiv:2405.15738,2024.
[23] Xiaotian Han, Quanzeng You, Yongfei Liu, Wentao Chen, Huangjie Zheng, Khalil Mrini,
XudongLin,YiqiWang,BohanZhai,JianboYuan,HengWang,andHongxiaYang. Infimm-
eval:Complexopen-endedreasoningevaluationformulti-modallargelanguagemodels,2023.
[24] DongyangHou,ZelangMiao,HuaqiaoXing,andHaoWu. V-rsir: Anopenaccessweb-based
imageannotationtoolforremotesensingimageretrieval. IEEEAccess,2019.
[25] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang,
QinJin,FeiHuang,etal. mplug-docowl1.5: Unifiedstructurelearningforocr-freedocument
understanding. arXivpreprintarXiv:2403.12895,2024.
[26] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei
Fang,YuxiangHuang,WeilinZhao,etal. Minicpm: Unveilingthepotentialofsmalllanguage
modelswithscalabletrainingstrategies. arXivpreprintarXiv:2404.06395,2024.
[27] XinyuJia,ChuangZhu,MinzhenLi,WenqiTang,andWenliZhou. Llvip: Avisible-infrared
paireddatasetforlow-lightvision. InICCV,2021.
[28] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile
Saulnier,etal. Mistral7b. arXivpreprintarXiv:2310.06825,2023.
[29] Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton
Lozhkov,ThomasWang,SiddharthKaramcheti,AlexanderRush,DouweKiela,etal.Obelics:
Anopenweb-scalefiltereddatasetofinterleavedimage-textdocuments. NeurIPS,2024.
[30] BoLi,KaichenZhang,HaoZhang,DongGuo,RenruiZhang,FengLi,YuanhanZhang,Ziwei
Liu, andChunyuanLi. Llava-next: Strongerllmssuperchargemultimodalcapabilitiesinthe
wild,May2024.
[31] BoLi,YuanhanZhang,LiangyuChen,JinghaoWang,JingkangYang,andZiweiLiu. Otter:
A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726,
2023.
14[32] BohaoLi,YuyingGe,YiChen,YixiaoGe,RuimaoZhang,andYingShan.Seed-bench-2-plus:
Benchmarkingmultimodallargelanguagemodelswithtext-richvisualcomprehension. arXiv
preprintarXiv:2404.16790,2024.
[33] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying
Shan. Seed-bench-2: Benchmarking multimodal large language models. arXiv preprint
arXiv:2311.17092,2023.
[34] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-
bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint
arXiv:2307.16125,2023.
[35] HongyangLi,YangLi,HuijieWang,JiaZeng,PinlongCai,HuilinXu,DahuaLin,JunchiYan,
FengXu, LuXiong, etal. Open-sourceddataecosysteminautonomousdriving: thepresent
andfuture. arXivpreprintarXiv:2312.03408,2023.
[36] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-
image pre-training with frozen image encoders and large language models. arXiv preprint
arXiv:2301.12597,2023.
[37] KaicanLi,KaiChen,HaoyuWang,LanqingHong,ChaoqiangYe,JianhuaHan,YukuaiChen,
WeiZhang,ChunjingXu,Dit-YanYeung,etal. Coda: Areal-worldroadcornercasedataset
forobjectdetectioninautonomousdriving. InECCV,2022.
[38] Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu.
Multimodalarxiv: Adatasetforimprovingscientificcomprehensionoflargevision-language
models,2024.
[39] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu,
Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision
languagemodels. arXivpreprintarXiv:2403.18814,2024.
[40] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang
Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large
multi-modalmodels. arXivpreprintarXiv:2311.06607,2023.
[41] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual
instructiontuning. arXivpreprintarXiv:2310.03744,2023.
[42] HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,ShengShen,andYongJaeLee.
Llava-next: Improvedreasoning,ocr,andworldknowledge,2024.
[43] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. arXiv
preprintarXiv:2304.08485,2023.
[44] J.Liu,D.Liu,W.Yang,S.Xia,X.Zhang,andY.Dai. Acomprehensivebenchmarkforsingle
imagecompressionartifactsreduction. InarXiv,2019.
[45] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhang,WangboZhao,YikeYuan,
JiaqiWang,ConghuiHe,ZiweiLiu,etal.Mmbench:Isyourmulti-modalmodelanall-around
player? arXivpreprintarXiv:2307.06281,2023.
[46] Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, and Xiang Bai.
Textmonkey:Anocr-freelargemultimodalmodelforunderstandingdocument. arXivpreprint
arXiv:2403.04473,2024.
[47] HaoyuLu,WenLiu,BoZhang,BingxuanWang,KaiDong,BoLiu,JingxiangSun,Tongzheng
Ren,ZhuoshuLi,YaofengSun,etal. Deepseek-vl:towardsreal-worldvision-languageunder-
standing. arXivpreprintarXiv:2403.05525,2024.
[48] YujieLu,DongfuJiang,WenhuChen,WilliamYangWang,YejinChoi,andBillYuchenLin.
Wildvision: Evaluating vision-language models in the wild with human preferences. arXiv
preprintarXiv:2406.11069,2024.
15[49] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa:
A benchmark for question answering about charts with visual and logical reasoning. arXiv
preprintarXiv:2203.10244,2022.
[50] MineshMathew,DimosthenisKaratzas,andCVJawahar. Docvqa: Adatasetforvqaondocu-
mentimages. InWACV,2021.
[51] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp
Dufter,DhrutiShah,XianzhiDu,FutangPeng,FlorisWeers,etal. Mm1: Methods,analysis
&insightsfrommultimodalllmpre-training. arXivpreprintarXiv:2403.09611,2024.
[52] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman,
Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al.
Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786,
2022.
[53] OpenAI. Gpt-4technicalreport. 2023.
[54] Enna Sachdeva, Nakul Agarwal, Suhas Chundi, Sean Roelofs, Jiachen Li, Mykel Kochen-
derfer, Chiho Choi, and Behzad Dariush. Rank2tell: A multimodal driving dataset for joint
importancerankingandreasoning. InWACV,2024.
[55] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Ping
Luo,AndreasGeiger,andHongyangLi. Drivelm: Drivingwithgraphvisualquestionanswer-
ing. arXivpreprintarXiv:2312.14150,2023.
[56] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi
Parikh,andMarcusRohrbach. Towardsvqamodelsthatcanread. InCVPR,2019.
[57] Xian Sun, Peijin Wang, Zhiyuan Yan, F. Xu, Ruiping Wang, W. Diao, Jin Chen, Jihao Li,
YingchaoFeng,TaoXu,M.Weinmann,S.Hinz,ChengWang,andK.Fu. Fair1m: Abench-
mark dataset for fine-grained object recognition in high-resolution remote sensing imagery.
ISPRS,2021.
[58] JingqunTang,QiLiu,YongjieYe,JinghuiLu,ShuWei,ChunhuiLin,WanqingLi,Mohamad
Fitri Faiz Bin Mahmood, Hao Feng, Zhen Zhao, et al. Mtvqa: Benchmarking multilingual
text-centricvisualquestionanswering. arXivpreprintarXiv:2405.11985,2024.
[59] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin,
Percy Liang, and Tatsunori B Hashimoto. Stanford alpaca: An instruction-following llama
model,2023.
[60] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui
Yu,RaduSoricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyof
highlycapablemultimodalmodels. arXivpreprintarXiv:2312.11805,2023.
[61] ShengbangTong,EllisBrown,PenghaoWu,SanghyunWoo,ManojMiddepogu,SaiCharitha
Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: A fully
open,vision-centricexplorationofmultimodalllms. arXivpreprintarXiv:2406.16860,2024.
[62] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timo-
théeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Open
andefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
[63] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Open
foundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
[64] Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua,
Zhiyuan Liu, and Gao Huang. LLaVA-UHD: an lmm perceiving any aspect ratio and high-
resolutionimages. arXivpreprintarXiv:2403.11703,2024.
[65] QinhongYang,DongdongChen,ZhentaoTan,QiankunLiu,QiChu,JianminBao,LuYuan,
GangHua,andNenghaiYu. Hq-50k:Alarge-scale,high-qualitydatasetforimagerestoration.
arXivpreprintarXiv:2306.05390,2023.
16[66] Zhenjie Yang, Xiaosong Jia, Hongyang Li, and Junchi Yan. Llm4drive: A survey of large
languagemodelsforautonomousdriving. arXive-prints,pagesarXiv–2311,2023.
[67] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang,
Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large
languagemodelswithmultimodality. arXivpreprintarXiv:2304.14178,2023.
[68] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A
surveyonmultimodallargelanguagemodels. arXivpreprintarXiv:2306.13549,2023.
[69] KainingYing,FanqingMeng,JinWang,ZhiqianLi,HanLin,YueYang,HaoZhang,Wenbo
Zhang, YuqiLin, ShuoLiu, JiayiLei, QuanfengLu, RunjianChen, PengXu, RenruiZhang,
HaozheZhang, PengGao, YaliWang, YuQiao, PingLuo, KaipengZhang, andWenqiShao.
Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision-language
modelstowardsmultitaskagi,2024.
[70] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao
Wang,andLijuanWang. Mm-vet: Evaluatinglargemultimodalmodelsforintegratedcapabil-
ities. InICML,2024.
[71] KaihaoZhang,DongxuLi,WenhanLuo,WenqiRen,BjörnStenger,WeiLiu,HongdongLi,
andMing-HsuanYang. Benchmarkingultra-high-definitionimagesuper-resolution. InICCV,
2021.
[72] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao,
ShuangruiDing,SongyangZhang,HaodongDuan,HangYan,etal. Internlm-xcomposer: A
vision-languagelargemodelforadvancedtext-imagecomprehensionandcomposition. arXiv
preprintarXiv:2309.15112,2023.
[73] Yi-FanZhang,QingsongWen,ChaoyouFu,XueWang,ZhangZhang,LiangWang,andRong
Jin. Beyond llava-hd: Diving into high-resolution large multimodal models. arXiv preprint
arXiv:2406.08487,2024.
[74] Yunsong Zhou, Linyan Huang, Qingwen Bu, Jia Zeng, Tianyu Li, Hang Qiu, Hongzi Zhu,
MinyiGuo,YuQiao,andHongyangLi. Embodiedunderstandingofdrivingscenarios. arXiv
preprintarXiv:2403.04593,2024.
[75] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-
hancingvision-languageunderstandingwithadvancedlargelanguagemodels. arXivpreprint
arXiv:2304.10592,2023.
[76] PengfeiZhu,LongyinWen,DaweiDu,XiaoBian,HengFan,QinghuaHu,andHaibinLing.
Detectionandtrackingmeetdroneschallenge. T-PAMI,2021.
17MME-RealWorld
————Appendix————
Contents
1 Introduction 1
2 RelatedWork 3
3 MME-RealWorld 4
3.1 InstructionandCriterion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.2 DataCollectionandAnnotation . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.3 MME-RealWorld-CN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.4 QualityControlandAnalysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
4 Experiments 7
4.1 ResultsonMME-RealWorld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4.1.1 Perception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4.1.2 Reasoning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4.2 ResultsonMME-RealWorld-CN . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
4.3 Fine-grainedAnalysisandFindings . . . . . . . . . . . . . . . . . . . . . . . . . 10
5 Conclusion 12
A DataCollectionandTaskSplit 19
A.1 OCRintheWild . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.2 DiagramandTable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
A.3 Remotesensing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
A.4 AutonomousDriving . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
A.5 Monitoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
B ExperimentalResultsonAllTaskSplits 25
18A DataCollectionandTaskSplit
A.1 OCRintheWild
Data Characteristics. The data is from real-world street scenes and high-resolution images of
product advertisements. Text is dense or difficult to detect and requires careful observation to be
identified.
A.1.1 DataSourcesandAnnotationProcess
DataSources. Wemanuallyselectimageswithcomplexscenesandrecognizabletextinformation
from existing high-resolution datasets for our test images. The open-source datasets used include
DIV2K and Flickr2K [1], which offer paired high-resolution RGB images and their correspond-
ing downscaled low-resolution RGB images by a factor of two. In our approach, we exclusively
utilizehigh-resolutionimages,selectingandpreservingimageswithcomplexscenesandcontexts.
Additionally,weincludetheLIU4K[44]dataset,whichcontains2,000imageswithresolutionsof
atleast3K,mostrangingfrom4Kto6K.Thisdatasetprovidesabundantmaterialsfortestingand
evaluatingperformanceon4K/8Kdisplaydevices, featuringdiverseandcomplexlow-levelsignal
distributionsandbackgrounds. Wealsoincorporatetwolarge-scaleUltra-High-Definitiondatasets,
UHD4KandUHD8K[71],whichcollectivelycontain23,000images. Thesedatasetscatertovar-
ious low-level image enhancement tasks, including image super-resolution (SR), image deraining
(Derain),low-lightimageenhancement(LLIE),andimagereflectionremoval(IRR).Finally,weuse
HQ-50K[65],alarge-scale,high-qualityimagerestorationdatasetcontaining50,000high-quality
images. HQ-50K stands out for its large scale, high resolution, varying compression rates, rich
texturedetails,andsemanticdiversity.
Annotation. 20volunteersannotatethequestionandanswerpairs. 3expertsaretaskedwithcheck-
ingandcorrectingtheannotationstoensurequality.
A.1.2 EvaluationDimensionsandBenchmarkStatistics
The evaluation of models in real-world complex scenes involves their ability to recognize and un-
derstandtext,enablingustoascertaintheircapacitytocomprehendandprocesstextualinformation
withinvisualcontent,therebyenhancingtheoverallpracticalityandreliabilityofintelligentsystems.
Specifically, Optical Character Recognition (OCR) in complex contexts comprises five perception
tasksandtworeasoningtasks. Forperceptiontasks,
1. Contact information and addresses (Fig. 5(a)). Recognizing telephone numbers, names of
countries/cities/streets,andbuildings(469imagesand577QApairs).
2. ProductandAdvertisementPerception(Fig.5(b)). Identifyingproductnames/pricesoradver-
tisementsofshopsorbrands(803imagesand1,588QApairs).
3. Identity Information Perception (Fig. 5(c)). Recognizing license numbers or ID cards of
cars/humans(852QApairs).
4. OtherkindofSmallTextonSignalsorIndicatorsPerception(Fig.5(d)). Recognizingsmall
textonindicators,signals,andsimilarobjects(626imagesand1,198QApairs).
5. Book, Map and Poster Perception (Fig. 5(e)). Recognizing dialogues/information on posters
andspecificlocationsinvolvingacountry/regiononmaps(785imagesand1,555QApairs).
Additionally,ourtworeasoningtasksinclude:
1. SceneRecognition(Fig.6(a)). Understandingthemeaningofscenesinimages,suchaspredict-
ingtheoutcomeofagamebasedonthescoreboardorwhatmighthappeninthefuturebasedonthe
scene,inferringthetimebylookingataclock,orcalculatingobjectprices(250imagesand250QA
pairs).
2.CharactersUnderstanding(Fig.6(b)).Understandingthepertinentcharacteristicsofcharacters
in a poster or comic, including their relationships, emotions, intentions, or quantities (250 images
and250QApairs).
Note that although we have 3,293 unique images, some tasks use overlapping image sets, so the
totalnumberofimageslistedinallthetasksisnotexactly3,293.
19What is the content of the sign on the top of the tall building
What is the phonenumber of HOP INN? withthebluewindowsinthedistance?
( ( ( ( (A B C D E) ) )) )0 0 T0 02 2 2 2 h3 8 0 3 e8 8 2 8 i0 0 5 0 m 6 5 a5 56 5 g5 57 7 e7 7 7 7 7 7 d2 2 5 2 o3 2 3 3 es not feature the number. ( ( ( (A B C D) ) ) ) V T L TA I I IR B RL A R AE N AS NRO AII IN NT TE ER RN NA AT TIO ION NAL ALHO HT OE TL EL
(E) The image does not feature the content.
(a) PhoneandAddressPerception. (b) ProductandAdvertisement.
What is the license plate number of From left to right, what's the text in the
thegreensedanontheleft? first big blue sign above the road?
(A) HZK-777 (A) Dresden Cottbus 13
(B) HZK-789 (B) Berlin-ZentrumBin.-Schonefeld113
( ( (C D E)) ) T lB 9 i8 chB e9 e- n 9 iN sm1 eP8 a .59 ge does not feature the ( ( (C D E)) ) TD B her r ee L s id i mne a-n Z g C e en o dtl r ol ub emu s s nB1 oi3 tn f. e-S ac tuh ro en te hf ee l td ex1 t1 .3
(c) Human/CarLicense. (d) OtherkindofsmallTextonSignalsorIndicators.
What's thecontent of the old man speaking in the frame
onthefourthrowinthecomicpage?
(A) DON'T WAIT FOR US. WE'LL FIND OUR
OWNRABBITHOLE.
(B) I SEE YOU BROUGHT WILLOW LIFGOOD
WITHYOU.
(C) HER NAME IS MARIAN DREWS!
(D) WECANMANAGE.
(E) The image does not feature the content.
(e) Book,MapandPoster.
Figure5: DataExamplesforPerceptionTasksinOCRintheWild
What might be the emotions of these characters?
W ( (A Bh ))at 34ti ::m 00 00eisitnow? ( (A B) ) H Ana gp rp yy ..
(C) 5:00 (C) Sad.
(D) 3:30 (D) Excited
(E) The image does not feature the time. (E) Theimagedoesnotfeaturetheemotion.
(a) SceneRecognition. (b) CharactersUnderstanding.
Figure6: DataExamplesforReasoningTasksinOCRintheWild
A.2 DiagramandTable
DataCharacteristics.Diagramsandtableswithrichcontentpresentsignificantchallengesforrapid
localizationandanalysis,evenforhumanresearchers.Thesetasksdemandahighlevelofperceptual
capability.
20What'sthepercentageofCAPEXwhendirectcostsriseto
35340000iftheoutcomeisat6250644? What is the data of Merchant Black Prices in 2038 in the
(A) 12.6% diagram Real Energy Prices?
(B) 10.3% (A) 40.00 to 60.00
(C)11.9% (B) 20.00to40.00
(D)11.1% (C)60.00 to 80.00
(E) Theimagedoesnotfeaturethedata. (D)80.00 to 100.00
(E) The image does not feature the data.
(a) TablePerception. (b) DiagramPerception.
In which year does the value of
'Gross Profit' first exceed 300,000,
according to the 'Income
Statement'table?
(A) 2030 Which year displays the lowest value of 'EBITDA Margin
(B) 2031 %', according to the 'Income Statement' chart?
( ( (C D E)) ) T t2 2 h0 0 h e3 3 e n2 3 i um ma bg ee r.doesnotfeature ( ( ( ( (A B C D E) )) ) )2 T2 2 20 0 0 0 h2 2 2 2 e5 6 7 8
image does not feature the number.
(c) TableReasoning. (d) DiagramReasoing
Figure7: DataExamplesforDiagramandTableTasks
A.2.1 DataSourcesandAnnotationProcess
Althoughthereareexistingdatasetsforevaluatingdiagramsandtables,suchasChartQA[49]and
someopen-sourcescientificchartdatalikeArxivQA[38],weobservethatthesedatasetsoftenhave
relativelylowimageresolutionsandlimitedcontentrichness. Consequently,theyarerelativelyeasy
forhumanstointerpretquickly, whichdoesnotalignwiththedesigngoalsofourbenchmark. To
address this, we source complex diagram data from the internet, such as detailed financial reports
with large charts. Analyzing these large charts poses significant perceptual challenges, even for
humans,andthusbetteralignswiththeobjectivesofourbenchmark.
Annotation. 20 volunteers are involved to generate question and answer pairs for the perception
task. Additionally, one expert researcher is responsible for generating reasoning annotations. To
ensurehigh-qualityannotations,threeexpertsareassignedtoreviewandcorrecttheannotations.
A.2.2 EvaluationDimensionsandBenchmarkStatistics
Theabilityofmultimodalmodelstoperceiveandunderstanddiagramandtabledatahaslongbeena
focusofresearch. InourDiagramandTabledomain,wehaveelevatedthedifficultyleveltoapoint
whereevenhumansfinditchallengingtosolveeasily. Wehavecollected2,570imagesand5,933
annotations,categorizingtheannotationsintothefollowingfourtypes:
1. Table Perception (Fig. 7(a)). Identifying specific elements within a table by using the given
tablename,horizontalaxiscoordinates,andrelatedlocationinformationtodeterminethevalueof
elementsinspecificpositions(4,018QApairs).
2. Diagram Perception (Fig. 7(b)). Identifying specific elements within a diagram by using the
providedlegendortitle,alongwithspecificlocationinformation,todeterminethevalueofelements
ortheintervalstheybelongto(1,415QApairs).
Additionally,ourtworeasoningtasksinclude:
1. Table Reasoning (Fig. 7(c)). This involves tasks that go beyond simple perception, such as
comparing the values of two elements in specific positions within a table, filtering the table based
ongivenconditions,ordeterminingthemaximumandminimumvalues(174QApairs).
2.DiagramReasoning(Fig.7(d)).Similartotablereasoning,butreasoningwithdiagramsinvolves
distinguishingspecificcolorsinthelegendandassessingtheheightofcurvesorbars(326QApairs).
21A.3 Remotesensing
Data Characteristics. From real remote sensing data, some images have extremely high quality,
withindividualimagesizesreachingupto139MBandcontainingrichdetails.
A.3.1 DataSourcesandAnnotationProcess
We select high-resolution images from public remote sensing datasets with rich information. For
example, the FAIR1M dataset [57] focuses on fine-grained object recognition and detection using
high-resolution(0.3−0.8m)RGBimagesfromGaogen(GF)satellitesextractedviaGoogleEarth.
Itcontains15,000imagesannotatedwithrotatedboundingboxesacross5maincategories(ships,
vehicles,airplanes,courts,androads),furtherdividedinto37sub-categories. ThePotsdamdataset5
datasetincludes38patchesoftrueorthophotos(TOP)extractedfromlargermosaics. VGoogle[24],
VBing [24], and VArcGIS [24] datasets, derived from Google Earth, Bing World Imagery, and
ArcGISWorldImageryrespectively, eachfeature38classeswithatotalofapproximately59,000
imagesperdataset. Eachclasscontainsatleast1,500images,withspatialresolutionsrangingfrom
0.07to38.22meters.
Annotation.Forallthequestionsinthissubsection,20volunteersmanuallycreatethequestionsand
answers,whileanotherexpertreviewsthequalityofthequestionstoensuretheymeettherequired
standards.
A.3.2 EvaluationDimensionsandBenchmarkStatistics
Remotesensingimageshaveawiderangeofapplicationsinreal-worldscenarios. Duringthecon-
struction of our dataset, we observe that many tasks are challenging for humans. For example,
counting the number of airplanes in Fig. 8(a) requires careful observation and counting by human
annotators. Automatingthisprocesswithmultimodallargemodelswouldbehighlyvaluableforre-
motesensingapplications. Weselectatotalof1,298high-qualityimagesanddesignthreespecific
taskstailoredforremotesensingimages:
1. ObjectCounting(Fig.8(a)). Taskinvolvescountingspecificobjectssuchasairplanes,ships,or
buildingswithinagivenimage(1,255QApairs).
2. Color Recognition (Fig. 8(b)). Task involves identifying and describing the colors of specific
objectsintheimage(1,226QApairs).
3.Spatial Relationship Understanding (Fig. 8(c)). Understanding both the absolute spatial rela-
tionshipsandrelativespatialrelationshipsbetweenobjectsintheimages(1,257QApairs).
A.4 AutonomousDriving
DataCharacteristics. Thefront-viewdrivingdatasarerecordedusingonboardcameraswithvar-
ious sensor configurations. The images encompass diverse weather conditions (e.g., sunny, night,
rainy, etc.), geographic locations (e.g., US, SG, CN), and complex traffic scenarios (e.g., urban,
highway,etc.).
A.4.1 DataSourcesandAnnotationProcess
Data Sources. We select high-quality images from large open-source driving datasets, each with
distinctadvantages. TheRank2Telldataset[54]rankstheimportancelevelofsurroundingobjects
for driving safety. Additionally, it provides dense annotations of semantic, spatial, and relational
attributes with bounding boxes for approximately 2,600 frames captured at intersections, and it
stitches images from three cameras to deliver a wide field of view (FOV). To enhance the relia-
bility of autonomous driving systems, the CODA dataset [37] collects 1,500 driving scenes, each
containing object-level corner cases, and labels more than 30 novel categories (e.g., garbage bag,
concreteblock,etc.).Itfocusesonevaluatingperformanceofperceptionsystemsindetectingout-of-
distribution(OOD)objectscomparedtocommontrafficelements. ThenuScenesdataset[8],oneof
themostpopularreal-worldautonomousdrivingdatasets,providesabundant3Dperceptionannota-
tionswithasemanticmapandCANbusexpansion[35].BasedonnuScenes[8],DriveLM-nuScenes
5https://paperswithcode.com/dataset/isprs-potsdam
22[55]linksapproximately4,800keyframeswithdrivingbehaviorsandmotionsbyformulating3P
reasoning (perception, prediction, planning) as a series of rich question-answer pairs in a directed
graph.
Annotation. Forallthequestionsinthissubsection, aprofessionalresearchermanuallygenerates
thequestionsandanswersbasedonthesourcedatasets’labels,achievingtheirnon-ambiguity,chal-
lengeandcomplexity. Anotherexpertreviewsthequalityofthequestionstoensuretheymeetthe
requiredstandards.
A.4.2 EvaluationDimensionsandBenchmarkStatistics
Vision-centric autonomous driving is one of the most significant applications of artificial intelli-
gence. However,unresolvedissuesremain,includingbothobject-levelandtask-levelcornercases,
aswellassafe-criticalandhuman-likeplanning[66]. MLLMswithgeneralknowledgeandtheabil-
ityofdrivingscenariosembodiedunderstanding[21,74]areseenasapromisingsolutiontoachieve
Level4autonomousdriving. Specifically,wehavedesignedthreemainperceptiontasksandthree
mainreasoningtasks,whicharefurthersubdividedintoatotaloffifteensub-tasks. Itisworthnot-
ingthat,astraditionaldetectiontasksinautonomousdrivinghavelargelybeenaddressedbymodern
perceptionmodels,ourfocusisshiftingtowardsperceptionchallengesinvolvingsmallordistantob-
jects,specificallythosethatoccupylessthan1/100ofthetotalimagearea. Meanwhile,LLMsmust
possessextensivedrivingexpertiseandevenadeepunderstandingof3Dspatialconceptsinorderto
effectivelyaddressthecomplexreasoningchallenges. Forperceptiontasks:
1. Object Identification (Fig. 9(a)). Describing the main traffic elements in front of the ego car
includingtheircategoriesandcorrespondingquantities(1,101imagesand1,101QApairs).
2. Object Attribute Identification. Task involves identifying the attribute of a specific object
accordingtoitsappearanceandlocation(atotalof454imagesand523QApairs), anddescribing
theattributesofallobjectswithinaspecificcategory(atotalof1,167imagesand1,315QApairs)
intrafficscenarios. Intermsofsub-tasks,theformerincludesthevisualattributeofatrafficsignal
(Fig. 9(e), 157 images and 201 QA pairs) and the motion attribute of a pedestrian (Fig. 9(f), 152
images and 164 QA pairs) or a vehicle (145 images and 158 QA pairs), and the latter includes
themotionattributesofmultiplepedestrians(Fig.9(c), 493imagesand492QApairs)orvehicles
(Fig.9(d),674imagesand823QApairs).
3. Object Counting (Fig. 9(b)). Counting special traffic elements in the given image, such as
cars,trucks,trafficsignals,etc.,especiallysomenovelobjectscomparedtotraditionalautonomous
drivingtaskssuchasgarbagebags,dogs,concreteblocks,etc. (647imagesand720QApairs).
Furthermore,reasoningtasksareasfollows:
1. Intention Prediction. Task involves predicting the intention of a designated traffic agent in
the given image (a total of 582 images and 614 QA pairs). In terms of sub-tasks, it contains fine-
grainedbehaviorpredictionoftheegovehicle(Fig.10(a),304imagesand304QApairs)andfuture
intentionofapedestrian(95imagesand103QApairs)oravehicle(Fig.10(b),183imagesand207
QApairs).
2. InteractionRelationUnderstanding. Taskinvolvesreasoningtheinteractionrelationbetween
two specific traffic elements (a total of 444 images and 513 QA pairs). In terms of sub-tasks, it
containstheegovehicle’sreactiontoaspecificobject(Fig.10(e)),whichisfurthercategorizedinto
threecategories: pedestrian(102imagesand106QApairs),vehicle(95imagesand101QApairs),
and traffic signal (81 images and 105 QA pairs). Additionally, another sub-task is predicting the
interactionsbetweentheaforementionedobjects,excludingtheegovehicle(Fig.10(d),166images
and201QApairs).
3. DriverAttentionUnderstanding(Fig.10(c)). Reasoningthetrafficsignalthatthedrivershould
pay attention to in the given front view image, such as yellow light, speed limit sign, no parking
sign,etc. (217imagesand217QApairs).
23A.5 Monitoring
A.5.1 DataSourcesandAnnotationProcess
Data Characteristics. Monitoring images are captured from different cameras (e.g., drone-
equipped cameras, fixed surveillance cameras, infrared cameras), viewpoints (arbitrary and fixed
viewpoints),scenecomplexities(e,g.,streets,shoppingmalls,intersections,campus,etc.),anden-
vironmentalfactors(dayandnight).
Weselecthigh-resolutionimagesfrompublicmonitoringimagedatasetswithmanyreal-worldchal-
lenges. Forexample,theVisDronedataset[76]bringsseveralchallenges,e.g.,viewpointvariations,
scalevariations,andout-of-view,etc.Additionally,itsdatasetcontains263videoclipswith179,264
framesand10,209staticimages,whicharecapturedviavariousdrone-equippedcamerasacrossvar-
iouscategories(e.g.,pedestrian,people,bicycle,car,van,truck,tricycle,awning-tricycle,bus,and
motor),density(e.g.,sparseandcrowdedscenes)andenvironments(e.g.,urbanandruralregions).
Additionally,Theseconddataset6,iscollectedindiverseenvironments(e.g.,street,mall,elevator,
etc.) forcrowddensitypredictiontask,features3,000imageswithonlypersoncategory,captured
byfixedsurveillancecameras. Thisdatasetishighlydiversefromthecameraviewpoints(lowalti-
tude,highaltitude,fisheye,etc.),scalesize,andscenecomplexities. TheLLVIPdataset[27],which
isavisible-infraredpaireddatasetforlow-lightvision, contains30,976imagestakeninbinocular
cameras,andcontainsalargenumberofpedestrians. Weonlyselectmanuallyinfraredimagesfrom
ittotestthemodel’srobustnessondifferentmodals.
Annotation. Forallthequestionsinthissubsection,twoprofessionalresearchersmanuallycreate
the questions and answers, and another expert reviews the quality of the questions to ensure they
meettherequiredstandards.
A.5.2 EvaluationDimensionsandBenchmarkStatistics
Monitoringimagesarewidelyappliedinreal-worldscenariostoincreasepublicsafety. Analyzing
the monitoring images accurately with MLLMs would be highly valuable for public safety and
crowdmanagement. Specifically,wehavedesignedthreemaintasksformonitoringimages:
1. ObjectCounting(Fig.11(a)). Taskinvolvescountingspecificobjectssuchaspedestrians,cars,
ortrucksinthegivenmonitoringimages(atotalof1,600imagesand1,600QApairs). Notedthat,
when the count of a specific object is equal to zero, this object counting task can be transformed
well into the object existence task (Fig. 11(b), for judging whether a specific object exists in the
givenimages. Thus,theobjectexistencetaskcanberegardedasaspecialcaseofthecountingtask.
Additionally, we categorize this task into two sub-tasks for vehicle counting (608 images and 608
QApairs)andpersoncounting(992imagesand992QApairs),respectively.
2. Object Location (Fig. 11(c)). Task involves judging the location of the specific vehicles, like
cars,ortrucksinthegivenmonitoringimages(atotalof136imagesand136QApairs).
3.AttributeRecognition.Taskinvolvesidentifyinganddescribingtheattributesofspecificobjects,
e.g.,colorrecognition(Fig.11(d))andorientationperception(Fig.11(e)),inthemonitoringimages
(atotalof460imagesand460QApairs). Additionally,wecategorizethistaskintotwosub-tasks
forthevehicle(352imagesand352QApairs)andpersonattributerecognitiontasks(108images
and108QApairs),respectively.
Inaddition,therearethreeseasoningtasksdescribedasfollows.
1. CalculatetheSumofDifferentObjects(Fig.12(a)). Countingvariousobjectsandcalculating
theirtotalnumberaccurately(300imagesand300QApairs).
2. IntentionReasoning(Fig.12(b)). Reasoningthenextrouteandturnofthespecificobject(98
imagesand98QApairs).
3. Attribute Reasoning (Fig. 12(c)). Reasoning the specific materials and functions of the given
objects, such as inferring the function of the dustbin via its appearance (100 images and 100 QA
pairs).
6This dataset is publicly accessible through the AI Studio website at
https://aistudio.baidu.com/datasetdetail/28831.
24Table7: ExperimentalresultsontheOCR in the Wildtasksarecategorizedasfollows: “Prod-
uct” represents products and advertisements; “B & M & P” represents books, maps, and posters;
“Contact” denotes contact information and addresses; “Identity” pertains to identity information;
and“Signage”referstosignageandothertext. Modelsarerankedaccordingtotheiraverageperfor-
manceonperceptiontasks,fromhighesttolowest. Rowscorrespondingtoproprietarymodelsare
highlightedingrayfordistinction.
Perception Reasoning
Method LLM
Product B&M&P Contact Identity Signage Avg Avg-C Scene Character Avg Avg-C
GPT-4o - 79.65 79.23 74.88 73.66 77.38 77.69 76.96 64.80 58.00 61.40 61.40
InternVL-2 InternLM2.5-7b-Chat 72.21 80.58 72.10 73.47 68.70 73.92 73.41 56.00 58.80 57.40 57.40
Claude3.5Sonnet - 71.89 83.67 61.15 64.64 69.70 72.47 70.21 62.60 61.20 61.90 61.90
InternVL-Chat-V1-5 InternLM2-Chat-20B 69.83 75.56 71.75 73.36 67.03 71.51 71.51 57.60 56.00 56.80 56.80
CogVLm2-llama3-Chat LLama3-8B 70.35 66.82 74.00 76.41 67.03 69.97 70.92 58.80 49.20 54.00 54.00
Mini-Gemini-34B-HD Nous-Hermes-2-Yi-34B 72.14 79.04 64.30 55.16 66.61 69.55 67.45 60.80 57.60 59.20 59.20
InternLM-XComposer2.5 InternLM2-7B 65.34 71.83 66.90 78.29 65.69 69.25 69.61 50.40 56.40 53.40 53.40
Gemini-1.5-pro - 65.92 66.37 74.41 70.19 66.36 67.62 68.65 56.60 48.80 52.70 52.70
MiniCPM-V2.5 LLama3-8B 64.69 69.52 71.58 68.90 62.19 66.79 67.38 48.80 39.20 44.00 44.00
Cambrian-1-34B Nous-Hermes-2-Yi-34B 67.65 77.30 62.22 50.47 64.19 66.45 64.37 54.00 56.00 55.00 55.00
GPT-4o-mini - 62.32 68.87 54.11 62.56 58.51 62.51 61.27 52.80 41.20 47.00 47.00
Cambrian-1-8B LLama3-8B-Instruct 59.18 67.27 55.98 52.93 52.25 58.68 57.52 52.80 53.60 53.20 53.20
Monkey Qwen-7B 55.58 54.47 53.38 59.98 50.42 54.63 54.77 32.40 22.00 27.20 27.20
SliME-8B LLama3-8B 55.97 57.30 41.25 55.05 49.92 53.45 51.90 55.60 50.80 53.20 53.20
mPLUG-DocOwl1.5 LLaMA-7B 54.62 52.22 59.10 63.03 32.99 51.15 52.39 46.80 38.40 42.60 42.60
SliME-13B Vicuna-13B 52.25 46.50 46.97 53.76 53.17 50.58 50.53 45.60 36.40 41.00 41.00
DeepSeek-VL DeepSeek-LLM-7b-base 53.72 55.06 31.72 44.13 49.42 49.55 46.81 48.80 41.60 45.20 45.20
LLaVA-Next LLama3-8B 50.77 49.45 38.30 38.73 53.51 47.94 46.15 58.00 52.40 55.20 55.20
YI-VL-34B Yi-34B-Chat 48.33 50.55 33.97 37.91 43.57 44.95 42.87 46.80 38.00 42.40 42.40
ShareGPT4V-13B Vicuna-13B 46.92 40.51 40.38 46.01 47.66 44.55 44.30 31.20 20.80 26.00 26.00
LLaVA1.5-13B Vicuna-13B 45.51 40.39 39.69 47.54 46.74 44.10 43.97 36.80 23.60 30.20 30.20
Mini-Gemini-7B-HD Vicuna-7B-v1.5 40.05 44.44 42.98 44.37 39.32 42.02 42.23 38.00 32.80 35.40 35.40
ShareGPT4V-7B Vicuna-7B 40.50 35.43 37.61 42.14 41.99 39.39 39.53 25.60 22.70 24.15 24.15
MiniGPT-v2 Llama2-7B-Chat 40.05 34.73 38.99 41.08 41.82 39.02 39.33 36.40 23.60 30.00 30.00
LLaVA1.5-7B Vicuna-7B 39.67 34.92 37.26 40.26 41.90 38.69 38.80 30.80 21.20 26.00 26.00
TextMonkey Qwen-7B 38.12 31.96 44.89 45.19 33.89 37.30 38.81 36.00 24.80 30.40 30.40
LLaVA-Next Qwen-72B 43.58 45.72 20.28 14.91 41.24 37.07 33.15 17.60 16.80 17.20 17.20
Qwen-VL-Chat Qwen-7B 32.73 37.62 27.38 33.22 26.88 32.37 31.57 36.40 20.80 28.60 28.60
B ExperimentalResultsonAllTaskSplits
OCRinTheWildPerformance. Tab.7displaystheperformanceofvariousmodelsonreal-world
OCR tasks. Generally speaking, when image resolution is high, the more advanced models still
demonstratecommendableOCRcapabilities. However, thisdoesnotimplythatourtaskisoflow
difficulty. TheaverageaccuracyratesofQwen-VLandthebasicLLaVAmodelonperceptiontasks
areonlyslightlybetterthanrandomguessing. Inthistask,thegapbetweenopen-sourceandclosed-
sourcemodelsisnotsignificant.GPT-4oranksfirstinoverallperformance,whileClaude3.5Sonnet
leadsinreasoningtasks.
DiagramandTable. Tab.8presentstheresultsforthediagramdomain, wheresomeofthemore
advancedmodelsperformrelativelywell,withthreemodelsachievinganaverageaccuracyofover
60%. Reasoning tasks, however, have proved to be more challenging. Only Claude 3.5 Sonnet
manage to exceed 60% accuracy, standing out significantly, with the second-ranked InternLM-
XComposer2.5 trailing by 20%. Additionally, models like LLaVA-Next, which have performed
well on existing benchmarks like chartQA, show noticeably weaker performance on our dataset,
underscoringthehigherdifficultyoftheourbenchmark.
Remote Sensing. Tab. 9 presents the performance of various models on remote sensing tasks. It
isevidentthatmodelsperformingwellonremotesensingdatatypicallyeitheremployspecialhan-
dlingforhigh-resolutionimages(e.g.,Mini-Gemini-HD,SliME,Cambrian)orhavevisionencoders
designedtosupporthigh-resolutioninputs(e.g.,InternVL).Amongthese,SliMEachievesthehigh-
est performance due to its support for the largest resolution. However, even the top-performing
model, SliME-8B, shows poor performance on counting tasks with extremely large images, with
only30%accuracy. Someclosed-sourcemodelsperformevenworse,withGPT-4o-miniachieving
only2%accuracy. Thishighlightsthehighdemandsofremotesensingdataonresolutionanddetail
perception.
Autonomous Driving. Tab. 10 and Tab. 11 show the perception and reasoning performance of
variousmodelsinautonomousdrivingscenarios. Asacriticalapplicationarea,autonomousdriving
remainsachallengeforMLLMs,withnomodelcurrentlycapableofreliablyaddressingtaskssuch
asintentprediction,trafficlightrecognition,andobjectcountingsolelythroughtext. OnlyClaude
3.5Sonnetachieveanaverageperceptionaccuracyexceeding40%. Reasoningtasksareevenmore
difficult, witheventhemostadvancedmodelsachievingonlyaround30%accuracy. Autonomous
25Table8: ExperimentalresultsontheDiagram and Tabletasks. Modelsarerankedaccording
totheiraverageperformance. Rowscorrespondingtoproprietarymodelsarehighlightedingrayfor
distinction.
Perception Reasoning
Method LLM
Diagram Table Avg Avg-C Diagram Table Avg Avg-C
Claude3.5Sonnet - 71.31 66.08 67.44 68.70 60.92 61.35 61.20 61.14
InternLM-XComposer2.5 InternLM2-7B 69.05 62.12 63.92 65.59 43.10 39.88 41.00 41.49
InternVL-2 InternLM2.5-7B-Chat 68.83 60.68 62.80 64.76 42.53 37.12 39.00 39.83
InternVL-Chat-V1-5 InternLM2-Chat-20B 61.55 53.81 55.83 57.68 37.36 34.36 35.40 35.86
MiniCPM-V2.5 LLama3-8B 57.81 51.05 52.81 54.43 26.44 34.66 31.80 30.55
CogVLm2-llama3-Chat LLama3-8B 51.52 46.09 47.51 48.81 31.61 33.44 32.80 32.53
GPT-4o - 47.35 46.44 46.68 46.90 44.25 45.09 44.80 44.67
Mini-Gemini-34B-HD Nous-Hermes-2-Yi-34B 47.63 43.21 44.36 45.42 38.51 39.57 39.20 39.04
GPT-4o-mini - 46.22 43.54 44.23 44.88 38.51 40.49 39.80 39.50
Cambrian-1-34B Nous-Hermes-2-Yi-34B 43.67 39.30 40.44 41.49 37.93 34.97 36.00 36.45
Gemini-1.5-pro - 41.41 39.37 39.90 40.39 35.63 31.90 33.20 33.77
Cambrian-1-8B LLama3-8B-Instruct 36.25 31.48 32.73 33.87 27.59 27.30 27.40 27.45
Monkey Qwen-7B 34.98 31.63 32.51 33.31 18.39 22.09 20.80 20.24
mPLUG-DocOwl1.5 LLama-7B 30.74 28.85 29.34 29.80 18.39 20.55 19.80 19.47
SliME-8B LLama3-8B 29.75 29.19 29.34 29.47 29.89 29.14 29.40 29.52
LLaVA-Next Qwen-72B 27.77 27.65 27.68 27.71 36.78 32.82 34.20 34.80
LLaVA-Next LLama3-8B 26.64 26.63 26.63 26.64 22.99 23.62 23.40 23.31
DeepSeek-VL DeepSeek-LLM-7b-base 23.67 23.27 23.38 23.47 22.99 24.23 23.80 23.61
Mini-Gemini-7B-HD Vicuna-7B-v1.5 21.20 22.70 22.31 21.95 27.01 23.31 24.60 25.16
SliME-13B Vicuna-13B 19.28 21.40 20.93 20.34 38.51 39.26 39.00 38.89
MiniGPT-v2 Llama2-7B-Chat 18.59 21.06 20.41 19.83 20.69 20.25 20.40 20.47
LLaVA1.5-13B Vicuna-13B 18.30 20.83 20.17 19.57 22.41 19.94 20.80 21.18
ShareGPT4V-13B Vicuna-13B 18.37 20.81 20.17 19.59 21.84 20.25 20.80 21.05
LLaVA1.5-7B Vicuna-7B 18.30 20.71 20.08 19.51 21.84 19.94 20.60 20.89
ShareGPT4V-7B Vicuna-7B 18.30 20.71 20.08 19.51 21.84 19.94 20.60 20.89
YI-VL-34B Yi-34B-Chat 15.90 16.03 15.99 15.97 23.56 27.30 26.00 25.43
Qwen-VL-Chat Qwen-7B 18.42 14.56 15.59 16.49 14.94 12.88 13.60 13.91
TextMonkey Qwen-7B 6.71 5.65 5.93 6.18 3.45 1.53 2.20 2.49
Table 9: Experimental results on the Remote Sensing tasks. Models are ranked according to
their average performance. Rows corresponding to proprietary models are highlighted in gray for
distinction.
Method LLM Color Count Position Avg Avg-C
SliME-8B LLama3-8B 45.66 28.63 52.19 42.27 42.16
Mini-Gemini-34B-HD Nous-Hermes-2-Yi-34B 41.12 21.29 58.31 40.40 40.24
Cambrian-1-8B LLama3-8B-Instruct 38.01 20.55 61.10 40.05 39.89
InternVL-2 InternLM2.5-7B-Chat 47.41 25.69 44.63 39.35 39.24
Cambrian-1-34B Nous-Hermes-2-Yi-34B 37.05 22.76 55.69 38.63 38.50
InternLM-XComposer2.5 InternLM2-7B 45.34 17.62 44.95 36.12 35.97
InternVL-Chat-V1-5 InternLM2-Chat-20B 34.10 17.86 48.29 33.55 33.42
YI-VL-34B Yi-34B-Chat 34.02 19.00 41.53 31.62 31.52
Mini-Gemini-7B-HD Vicuna-7B-v1.5 37.29 22.43 33.97 31.30 31.23
LLaVA-Next Qwen-72B 33.86 23.08 30.31 29.13 29.08
GPT-4o - 34.18 15.17 37.07 28.92 28.81
CogVLm2-llama3-Chat LLama3-8B 37.69 18.35 29.99 28.76 28.68
MiniCPM-V2.5 LLama3-8B 37.69 11.50 33.49 27.69 27.56
SliME-13B Vicuna-13B 30.04 18.43 28.80 25.82 25.76
Claude3.5Sonnet - 31.87 18.11 27.95 25.74 25.98
DeepSeek-VL DeepSeek-LLM-7b-base 29.40 7.34 39.30 25.49 25.35
LLaVA-Next LLama3-8B 30.04 20.55 25.74 25.42 25.44
Monkey Qwen-7B 22.07 16.97 35.72 24.99 24.92
mPLUG-DocOwl1.5 LLaMA-7B 27.81 16.39 26.81 23.71 23.67
MiniGPT-v2 Llama2-7B-Chat 23.35 20.15 26.41 23.33 23.30
LLaVA1.5-13B Vicuna-13B 26.22 16.88 26.57 23.27 23.22
ShareGPT4V-13B Vicuna-13B 25.58 16.97 26.49 23.06 23.01
LLaVA1.5-7B Vicuna-7B 23.11 16.88 26.25 22.12 22.08
ShareGPT4V-7B Vicuna-7B 23.03 16.88 26.25 22.10 22.05
Qwen-VL-Chat Qwen-7B 16.97 11.50 16.87 15.14 15.11
Gemini-1.5-pro - 13.39 8.32 20.13 13.99 13.95
TextMonkey Qwen-7B 6.93 2.04 25.86 11.69 11.61
GPT-4o-mini - 5.82 2.61 11.54 6.69 6.66
26Table 10: Experimental results on the Autonomous Driving perception tasks. Models are
rankedaccordingtotheiraverageperformance.Rowscorrespondingtoproprietarymodelsarehigh-
lightedingrayfordistinction.
Motion
Method LLM Identity Vehicle Multi-vehicle Pedestrain Multi-pedestrain TrafficSignal ObjectCount Avg Avg-C
Claude3.5Sonnet - 58.66 18.45 35.48 32.32 31.64 37.31 33.19 40.77 49.72
Cambrian-1-8B LLama3-8B-Instruct 56.77 50.00 33.29 32.32 14.00 35.82 33.06 38.52 47.65
InternVL-2 InternLM2.5-7B-Chat 46.68 39.24 30.98 34.76 17.24 37.81 34.58 35.46 41.07
MiniCPM-V2.5 LLama3-8B 44.96 41.77 30.86 31.71 19.88 37.31 29.17 34.15 39.56
SliME-8B LLama3-8B 44.50 51.90 28.68 29.27 15.21 29.35 33.61 33.66 39.08
InternLM-XComposer2.5 InternLM2-7B 46.23 48.10 26.61 32.93 11.76 40.30 32.50 33.63 39.93
Cambrian-1-34B Nous-Hermes-2-Yi-34B 43.96 38.61 31.96 32.93 12.37 27.36 33.89 33.39 38.68
DeepSeek-VL DeepSeek-LLM-7b-base 44.05 63.29 25.88 36.59 18.05 39.30 27.22 33.39 38.72
Mini-Gemini-34B-HD Nous-Hermes-2-Yi-34B 39.78 42.41 36.09 31.10 16.63 17.41 31.53 32.70 36.24
InternVL-Chat-V1-5 InternLM2-Chat-20B 40.42 39.87 26.97 27.44 18.66 32.34 30.14 31.42 35.92
CogVLm2-llama3-Chat LLama3-8B 33.15 36.71 29.77 31.71 19.27 43.78 28.19 30.22 31.69
Monkey Qwen-7B 35.97 60.76 24.30 37.20 18.26 32.34 24.72 29.67 32.82
YI-VL-34B Yi-34B-Chat 36.24 41.77 29.60 31.71 20.89 16.92 19.17 28.31 32.28
mPLUG-DocOwl1.5 LLama-7B 26.74 60.76 24.79 31.10 22.72 43.28 26.53 28.28 27.51
SliME-13B Vicuna-13B 26.61 46.84 24.54 32.32 17.65 43.28 27.50 27.16 26.89
Gemini-1.5-pro - 32.61 10.13 30.23 8.54 16.02 10.45 31.31 26.64 29.63
LLaVA1.5-13B Vicuna-13B 23.25 31.65 24.91 31.10 25.96 36.32 26.80 26.12 24.69
ShareGPT4V-13B Vicuna-13B 23.25 31.01 24.91 31.10 25.96 36.82 26.81 26.12 24.69
LLaVA1.5-7B Vicuna-7B 23.25 31.01 24.91 31.10 25.96 35.32 26.81 26.04 24.65
ShareGPT4V-7B Vicuna-7B 23.25 31.01 24.91 31.10 25.96 35.32 26.81 26.04 24.65
MiniGPT-v2 Llama2-7B-Chat 23.71 53.16 22.36 28.66 20.49 35.32 28.06 25.96 24.84
Mini-Gemini-7B-HD Vicuna-7B-v1.5 27.25 60.76 23.57 26.83 14.81 36.32 17.78 24.81 26.03
GPT-4o-mini - 19.07 45.57 24.67 23.78 11.36 40.30 31.11 24.18 21.63
GPT-4o - 15.26 23.42 25.39 26.22 9.94 41.29 32.22 22.43 18.85
LLaVA-Next LLama3-8B 21.44 41.77 22.36 29.88 9.23 22.39 8.06 18.66 20.05
LLaVA-Next Qwen-72B 19.26 26.58 26.37 29.88 12.58 16.42 5.97 17.98 18.62
Qwen-VL-Chat Qwen 9.26 35.44 15.43 23.17 8.32 34.83 16.39 15.08 12.17
TextMonkey Qwen-7B 8.54 37.34 22.72 15.85 16.23 14.93 6.39 14.26 11.40
Table11:ExperimentalresultsontheAutonomous Drivingreasoningtasks.Modelsareranked
according to their average performance on perception tasks, from highest to lowest. Rows corre-
spondingtoproprietarymodelsarehighlightedingrayfordistinction.
Intention Relation Attention
Method LLM Avg Avg-C
Ego Pedestrian Verhicle Ego2P Ego2T Ego2V O2O Signal
Monkey Qwen-7B 28.62 56.31 30.43 27.36 22.86 32.67 11.94 58.06 33.04 33.48
Claude3.5Sonnet - 26.32 32.04 24.64 23.58 25.71 20.79 24.38 65.90 31.92 30.59
YI-VL-34B Yi-34B-Chat 28.26 46.60 33.33 21.70 24.76 31.68 15.42 49.77 31.55 31.45
SliME-8B LLama3-8B 28.29 39.81 33.33 24.53 19.05 22.77 10.45 63.59 31.55 30.37
CogVLm2-llama3-Chat LLama3-8B 30.26 25.24 25.60 35.85 20.95 28.71 18.41 56.22 31.18 30.27
MiniCPM-V2.5 LLama3-8B 24.01 37.86 31.88 20.75 30.48 15.84 26.87 53.00 31.03 30.19
SliME-13B Vicuna-13B 25.00 41.75 28.99 28.30 21.90 24.75 25.87 48.39 30.80 30.64
LLaVA-Next LLama3-8B 32.89 49.51 33.82 28.30 25.71 24.75 7.96 43.32 30.73 30.78
Cambrian-1-8B LLama3-8B-Instruct 25.00 41.75 35.27 23.58 23.81 16.83 11.44 60.37 30.73 29.86
InternVL-2 InternLM2.5-7B-Chat 24.01 43.69 32.85 22.64 28.57 21.78 21.89 43.78 29.84 29.89
LLaVA-Next Qwen-72B 30.59 52.43 35.27 23.58 27.62 29.70 8.96 35.48 29.69 30.37
InternVL-Chat-V1-5 InternLM2-Chat-20B 25.99 32.04 31.88 16.98 22.86 25.74 9.45 57.14 28.94 27.89
DeepSeek-VL DeepSeek-LLM-7b-base 30.26 17.48 27.05 22.64 25.71 24.75 6.97 51.15 27.31 25.92
GPT-4o-mini - 11.51 19.42 24.64 22.64 28.57 17.82 31.34 54.84 26.79 26.40
GPT-4o - 17.11 19.42 27.54 15.09 20.00 22.77 16.92 60.83 26.41 25.12
mPLUG-DocOwl1.5 LLama-7B 20.72 26.21 30.43 19.81 31.43 25.74 12.94 41.94 26.04 26.14
LLaVA1.5-13B Vicuna-13B 23.36 18.45 24.15 26.42 23.81 22.77 25.37 30.41 24.78 24.39
Qwen-VL-Chat Qwen 20.39 21.36 20.77 16.04 23.81 17.82 16.92 50.69 24.63 23.60
ShareGPT4V-13B Vicuna-13B 23.36 17.48 26.09 25.47 27.63 22.77 25.37 26.27 24.55 24.33
Cambrian-1-34B Nous-Hermes-2-Yi-34B 14.14 24.27 22.71 24.53 30.48 31.68 10.95 46.54 24.40 25.52
LLaVA1.5-7B Vicuna-7B 23.36 17.48 26.09 25.47 27.62 22.77 25.37 23.96 24.18 24.03
ShareGPT4V-7B Vicuna-7B 23.36 17.48 26.09 25.47 27.62 22.77 25.37 23.96 24.18 24.03
InternLM-XComposer2.5 InternLM2-7B 25.33 36.89 34.30 17.92 26.67 25.74 23.88 44.24 24.03 28.78
MiniGPT-v2 Llama2-7B-Chat 23.68 25.24 28.02 28.30 22.86 21.78 2.49 37.33 23.66 23.71
Mini-Gemini-7B-HD Vicuna-7B-v1.5 21.05 6.80 14.49 19.81 15.24 25.74 15.42 54.38 23.29 21.80
Mini-Gemini-34B-HD Nous-Hermes-2-Yi-34B 14.14 22.33 21.74 23.58 31.43 30.69 10.45 47.00 22.84 24.91
TextMonkey Qwen-7B 9.54 24.27 23.67 24.53 17.14 20.79 11.44 35.94 20.01 20.81
Gemini-1.5-pro - 13.49 18.45 28.02 6.60 6.67 6.93 23.88 32.72 19.20 17.33
drivingisinherentlyahigh-riskdomainthatdemandsveryhighaccuracyforpracticaldeployment.
Thisindicatesthatmorepowerfulmultimodalmodelswith3Dspatialpredictionandunderstanding
ability, or specialized fine-tuning on domain-specific datasets for driving expertise, are necessary
beforeMLLMscanbeeffectivelyappliedinthisfield.
Monitoring Performance. Tab. 12 presents the performance of various models under monitoring
scenarios. As can be observed, the monitoring task poses a high degree of difficulty. Traditional
models like Qwen-VL and LLaVA have an accuracy rate of around 20%, which is nearly equiva-
lenttorandomguessing. Open-sourcemodelssignificantlyoutperformclosed-sourcemodels. For
instance,InternVL-2hasanaverageaccuracyrateof53.19onperceptiontasks,greatlysurpassing
GPT-4o’s 33.93. We notice that closed-source models such as GPT-4o have a high frequency of
answering“E”,withover35%ofresponseschoosing“E”. Thissuggeststhatclosed-sourcemodels
may be more inclined to refrain from responding when the answer is uncertain. Furthermore, we
find that while most models perform reasonably well on counting tasks, they struggle with tasks
27Table12: ExperimentalresultsontheMonitoringtasks. Modelsarerankedaccordingtotheir
averageperformance. Rowscorrespondingtoproprietarymodelsarehighlightedingrayfordistinc-
tion.
Perception Reasoning
Method LLM Vehicle Pedestrain Avg Avg-C Calculate Intention Property Avg Avg-C
Counting Location Attribute Counting Attribute
InternVL-2 InternLM2.5-7B-Chat 70.07 25.74 28.98 59.68 12.04 53.19 41.62 51.67 21.43 41.00 43.57 38.03
InternVL-Chat-V1-5 InternLM2-Chat-20B 72.53 23.53 27.27 55.24 7.41 51.16 39.52 39.33 26.53 42.00 37.35 35.95
Cambrian-1-8B LLama3-8B-Instruct 62.01 29.41 20.45 55.44 7.41 47.68 37.07 46.00 29.59 44.00 42.37 39.86
Cambrian-1-34B Nous-Hermes-2-Yi-34B 51.32 33.09 26.14 55.14 12.96 45.98 37.44 11.33 18.37 45.00 19.48 24.90
SliME-8B LLama3-8B 60.53 33.82 28.98 34.48 31.48 40.62 38.32 32.33 40.82 43.00 36.14 38.72
Mini-Gemini-34B-HD Nous-Hermes-2-Yi-34B 53.95 17.65 22.73 43.45 7.41 39.61 30.80 11.67 17.35 50.00 20.48 26.34
InternLM-XComposer2.5 InternLM2-7B 52.63 13.24 17.61 46.98 0.93 39.48 28.48 13.67 13.27 34.00 17.67 20.31
MiniCPM-V2.5 LLama3-8B 62.66 16.91 22.73 36.49 4.63 38.70 30.35 36.00 35.71 41.00 36.95 37.57
YI-VL-34B Yi-34B-Chat 56.25 8.09 23.86 32.47 5.56 34.85 26.85 28.00 28.57 44.00 31.33 33.52
Mini-Gemini-7B-HD Vicuna-7B-v1.5 47.86 13.97 25.00 34.07 13.89 34.15 28.16 21.00 24.49 42.00 25.90 29.16
GPT-4o - 50.66 15.44 19.89 34.17 6.48 33.93 26.76 4.00 13.27 41.00 19.42 19.42
CogVLm2-llama3-Chat LLama3-8B 48.19 26.47 22.16 32.36 12.04 33.74 29.16 40.00 40.82 45.00 41.16 41.94
Claude3.5Sonnet - 50.99 33.77 11.93 33.37 8.33 32.19 28.43 34.37 18.37 44.00 32.25 32.25
Gemini-1.5-pro - 52.63 9.56 10.80 31.05 10.08 31.11 24.21 11.67 13.27 27.00 17.31 17.31
LLaVA-Next Qwen-72B 57.89 10.39 27.27 15.32 28.70 29.37 28.16 23.33 38.78 28.00 27.31 30.04
Monkey Qwen-7B 42.76 40.44 21.02 21.77 9.26 28.01 27.21 25.33 21.43 39.00 27.31 28.59
DeepSeek-VL DeepSeek-LLM-7b-base 43.91 7.35 17.33 24.70 9.26 26.97 21.59 5.33 19.39 48.00 16.67 24.24
GPT-4o-mini - 44.57 8.82 8.52 26.71 3.70 26.50 19.80 7.33 8.16 19.00 11.50 11.50
mPLUG-DocOwl1.5 LLaMA-7B 34.87 19.12 26.42 21.27 6.48 24.97 22.19 10.00 33.67 39.00 20.48 27.56
SliME-13B Vicuna-13B 20.56 22.79 23.30 29.33 12.96 24.73 22.28 33.00 26.53 40.00 33.13 33.18
Qwen-VL-Chat Qwen-7B 37.66 16.18 21.88 14.62 12.04 22.13 20.75 14.67 17.35 21.00 16.47 17.67
LLaVA1.5-13B Vicuna-13B 14.47 22.06 21.59 24.29 12.96 20.45 19.30 27.67 23.47 31.00 27.51 27.38
LLaVA-Next LLama3-8B 46.71 0.00 22.16 4.13 23.15 19.46 19.27 13.67 46.94 18.00 21.08 26.20
ShareGPT4V-13B Vicuna-13B 14.31 22.06 15.62 23.99 12.04 19.26 17.88 27.33 24.49 30.00 27.31 27.27
MiniGPT-v2 Llama2-7B-Chat 13.98 22.06 15.34 24.40 11.11 19.26 17.69 13.67 19.39 24.00 16.87 19.02
LLaVA1.5-7B Vicuna-7B 14.31 22.06 15.62 23.79 11.11 19.13 17.67 27.33 23.47 24.00 25.90 24.93
ShareGPT4V-7B Vicuna-7B 14.31 22.06 15.62 23.79 11.11 19.13 17.67 27.33 23.47 25.00 26.10 25.27
TextMonkey Qwen-7B 39.47 6.62 7.10 8.17 0.00 16.14 12.92 0.67 4.08 16.00 4.42 6.92
relatedtospatialrelationshipjudgmentandattributerecognition. Moreover,relatedreasoningtasks
alsoposeahighlevelofdifficulty,withnomodelachievinganaccuracyrateover40%todate. In
combinationwiththeresultsfromautonomousdrivingtasks,weobservethatMLLMsexhibitsig-
nificantdeficienciesinunderstanding, predicting, andreasoningaboutthedynamicinformationof
objectsin2Dor3Dspace. Althoughtheinputtothesemodelsisasingleframeimageratherthan
atemporalsequenceofvideoframes,thereremainsaconsiderablegapbetweentheirperformance
andthatofhumans. Forhumans,whopossessrichexperientialknowledgeofdynamics,itisnotdif-
ficulttoinferthefuturestatesofobjectsfromasingleimageinunambiguoussituations. Therefore,
modernMLLMsarestillfarfromhavingthecapabilitytofunctionasworldmodels.
28Howmany
aircraft are
thereinthe
picture?
(A) 1
(B) 2
(C) 3
(D) 5
(E) 0
(a) ObjectCounting.
What is the color of
the excavator in the
middle area of the
picture?
(A) Yellow.
(B) Red.
(C) Black.
(D) Green.
(E) The image does
not feature the
color.
(b) ColorRecognition.
Where is the Y-fork cross
inthepicture?
(A) In the upper left
areaofthepicture.
(B) In the upper right
areaofthepicture.
(C) In the lower right
areaofthepicture.
(D) In the left-center
areaofthepicture.
(E) The image does not
featuretheposition.
(c) SpatialRelationshipUnderstanding.
Figure8: DataExamplesforPerceptionTasksinRemoteSensing
29This image shows the front
view of the ego car. What
are objects to the front of
the ego car?
(A) There is one bus,
many cars, and one
pedestrian.
(B) There are two
traffic cones and How many dogs are
one construction there on the road or
vehicle. around the road?
( (C D) ) T c b T b t cra aa ah h ar rr re e fs sr r fr r ,ii ie ea ce e a n r ra a c n. sdr r o d,e e no m oem tn w na sea e,no n my by ua sn .y ( ( ( ( (BA C D E)) )) ) 1 2 3 4 T noh te fi em aa tug re e d tho ee s
(E) All the above object
answers are wrong.
(a) ObjectIdentification. (b) ObjectCounting.
T v i p f ( ( ( ( (s r A B C D Ei eh oe t d ) ) ))i )w nhs e te s i o om ts rf f T p s a T p s a M a t M a ft i Ta h ea t tt e r e r r ra t h wa ah ag r hh at e e e a ed dn ru n n ee o te ee n ne e e us m m m me s d d ye e s s ys o ie
r
t i it t mo o o o ag h o eh g n nf pr ro p f v v v ro o va o i g i gt ae f teea ai i ih it w ht , d gn n nc d nh t c w n ne eh sa ea es a g g g ee a g s s t he s nrr . . , s a
o
r .t d t ie a a. th d a? n brl or r rW ee i nt d ji ete e ao
e
a h d itf s h n
c
n nwr r t ta s no geh s .o t on e .e tt T v i t e ( ( ( ( (s h A B C D Ei gh e at o ) )) )i )w h ts ce a i o a m rsf re T m a T a a T a a a t ft T Ta h e?at r r r r r rh hwh t ag o rh ht e e e e e eo r ru ee te o v r e e e up m p m p p e s e t i e e es ih n ro a a a ae mo o o ag h ee f g r o r o r rf v v ro o o k k k k a, f f ttf ei i t fw hh e e eer gn nac h t t d d t do eh h mde es a g gne h . . ,n e e . r , , od oc et d c at.h a a ba c c v oa cm noWe n na a i jr er a n ed f s d dr rsf sa h r cg r s st a n sh ta t t no . .w w r yet on e o o tt
(c) MotionAttributeIdentificationofMultiplePedestri-(d) MotionAttributeIdentificationofMultipleVehicles.
ans.
What type of the traffic signal
(excluding traffic lights) is on the left?
(A) speed limit sign
(B) construction work
(C) stop sign
(D) no parking
(E) The image does not feature
the object
(e) VisualAttributeIdentificationofaSpecificTrafficSignal
What is motion of the pedestrian
wearing black top on the right?
(A) walking on the sidewalk
(B) jaywalking (illegally crossing
not at pedestrian crossing)
(C) standing
(D) waiting to cross
(E) The image does not feature
the object
(f) MotionAttributeIdentificationofaSpecificPedestrian
Figure9: DataExamplesforPerceptionTasksinAutonomousDriving
30Predict the behavior of the
ego vehicle.?
(A) The ego vehicle is
going straight. The ego
vehicle is driving with
normal speed. This image shows the
(B) The ego vehicle is front view of the ego
steering to the right. car. What is the future
The ego vehicle is state of the white suv
( (C D) ) d T s l d Tel rr h hi f igi t ee vv .h i i Te n etn lg gh gyg o oe fs s av t e vl ee sgo ee th o .w hri ic ivl n cly ee g l. h e i t i s o c i sl t e h ie s i ( ( ( (n BA C D )) )t )he T T S Km tu u ei ad r r etnnd i po l lr ge nei? f oag t irh . nyt g. .
slightly steering to the straight.
right. The ego vehicle (E) The image does
is driving very fast. not feature the
(E) All the above answers object
are wrong.
(a) IntentionPredictionofEgoVehicle. (b) IntentionPredictionofaSpecificVehicle.
Based on the observations of the
white truck in the middle, what are
possible actions to be taken by the
T frh oi ns t i vm iea wge o s fh to hw e s e gth oe p m (Ae id )de ds lt er T?i a hWn e h ac ar cto t is i oss n i tn h ig se t trh oee a r esr oo mna a?d i ni n the
car. What is the traffic stationary, the reason is
signal that the ego that there is no safety issue.
v ae tth ei nc tl ie o nsh to ou ?ld pay (B) T g ao vh o e a i dha ec aat i cdo o,n lt lhi is se i t oro e n a a .c soce nl e isr a tote and
(A) Slow down. (C) The action is to remain
(B) Speed limit 40. stationary, and the reason
(C) No parking. is to follow traffic rules.
( (D E)) R T nohe td e f il emi ag a th ugt r. e e d tho ee s ( (D E)) T t t Th rh haee ef fr ia ie c mc a t r asi uo go ln en e s dii .ss o tt eoo s ft noa olk l te o f w en ao t tn h ue e r, e
object. the object.
(c) DriverAttentionUnderstanding (d) InteractionRelationbetweenTrafficElements
What should the ego vehicle do when
encountering the black sedan on the
left??
(A) accelerating/speeding up
(B) yielding
(C) stopping
(D) following
(E) The image does not feature
the object
(e) InteractionRelationbetweenEgoVehicleandTrafficElements
Figure10: DataExamplesforReasoningTasksinAutonomousDriving
31What is the number of awning-
tricycles in the image?
(A) 44
(B) 98
(C) 49
What is the number of (D) 72
cars in the image? (E) The image does not feature
(A) 84 the awning-tricycles.
(B) 77
(C) 11
(D) 59
(E) The image does not
feature the cars.
(a) ObjectCountinginMonitoringImages. (b) ObjectExistenceinMonitoringImages.
Where is the bicycle in the image? What color is the van in the image?
(A) The upper left corner (A) Blue
(B) The lower left corner (B) Red
(C) The upper right corner (C) Silver
(D) The lower right corner (D) Black
(E) The image does not feature the (E) The image does not feature the
bicycle. van.
(c) ObjectLocationinMonitoringImages. (d) ColorRecognitioninMonitoringImages.
What is the orientation of the
tricycle in the image?
(A) The right of the image
(B) The bottom of the image
(C) The left of the image
(D) The top of the image
(E) The image does not feature the
tricycle.
(e) OrientationPerceptioninMonitoringImages.
Figure11: DataExamplesforPerceptionTasksinMonitoringImages
32What is the total number of motors
and cars in the image?
(A) 26
(B) 80
(C) 133
(D) 92
(E) The image does not feature the
objects.
(a) ReasoningTaskofCalculatingtheSumofDifferentObjectsinMonitoringIm-
ages.
What will the truck do in the image?
(A) Stopping
(B) Keep moving
(C) Turn left
(D) Turn right
(E) The image does not feature the
object.
(b) ReasoningtaskofIntentionoftheSpecialObjectinMonitoringImages.
What's the yellow truck for in the
image?
(A) Express delivery
(B) Water supply
(C) Food supply
(D) Wood supply
(E) The image does not feature the
object.
(c) ReasoningtaskofAttributeoftheSpecialObjectinMonitoringImages.
Figure12: DataExamplesforReasoningTasksinMonitoringImages
33合并资产负债表中
2024年3月31日货币 股东人数中2023年
资金是多少? Q2是多少?
(A) 175893714.34 (A) 6263
(B) 175893714 (B) 48
(C) 175893714.35 (C) 50
(D) 175893714.33 (D) 36
(E) 表中没有指出 (E) 表中没有指出
货币资金. 该值.
Figure13: DataExamplesforPerceptionTasksinMME-RealWorld-CN
34