Domain-specific long text classification from sparse
relevant information
CéliaD’Cruza,*,Jean-MarcBerederb,**,FrédéricPreciosoa,*** andMichelRiveilla,****
aUniversitéCôted’Azur,CNRS,Inria,I3S,France
bCHUdeNice,France
Abstract. Large Language Models have undoubtedly revolution- domain,wherewordswhicharesimilaringeneralEnglishcannotbe
izedtheNaturalLanguageProcessingfield,thecurrenttrendbeing consideredassimilarbythemodelbecausetheyaretwodistinctcat-
topromoteone-model-for-alltasks(sentimentanalysis,translation, egories(ofcrime)inlegaltexts,asforinstancetheftandfraud[24].
etc.).However,thestatisticalmechanismsatworkinthelargerlan- Suchconfigurationsareobviouschallengesforverylargelanguage
guagemodelsstruggletoexploittherelevantinformationwhenitis modelspre-trainedonahugeamountofdataingeneralEnglish.
verysparse,whenitisaweaksignal.Thisisthecase,forexample, Wefacetheexactsameproblemswiththetechnicaljargonpresent
for the classification of long domain-specific documents, when the inthemedicalreports.Patientmedicalreportscontainasignificant
relevance relies on a single relevant word or on very few relevant amountofinformationondiversehealthcharacteristicsinstructured
wordsfromtechnicaljargon.Inthemedicaldomain,itisessentialto textsuchaskey-valuepairs(e.g.labresults)orinunstructuredfree
determinewhetheragivenreportcontainscriticalinformationabout text(e.gmedicalhistory).However,specificinformationcontainedin
apatient’scondition.Thiscriticalinformationisoftenbasedonone freetextcanbehardtoretrieve.Reportscanoftenbeseveralpages
orfewspecificisolatedterms.Inthispaper,weproposeahierarchical long, and similar terms in general English can appear in different
modelwhichexploitsashortlistofpotentialtargettermstoretrieve placesofthesamereportconfusingthelanguagemodelonthefinal
candidatesentencesandrepresentthemintothecontextualizedem- relevanceofthereport.Examplescanbefoundintheliterature:al-
beddingofthetargetterm(s)theycontain.Apoolingoftheterm(s) thoughanteriorandventralaresimilarinmeaning,anteriorisusedto
embedding(s) entails the document representation to be classified. indicatespatialrelationshipsofbodypartstoeachotherthroughout
Weevaluateourmodelononepublicmedicaldocumentbenchmark thebody,butventralismostoftenusedtoindicatearelationshipto
inEnglishandononeprivateFrenchmedicaldataset.Weshowthat theanteriorabdominalwall;theuseofupperratherthansuperiorhas
ournarrowerhierarchicalmodelisbetterthanlargerlanguagemodels evolvedfromcommonusageratherthanfollowingastrictanatomi-
forretrievingrelevantlongdocumentsinadomain-specificcontext. caldesignation.Forinstance,upperlipanduppereyelidareproper
clinicaldesignationsratherthansuperiorliporsuperioreyelid,but
superiorpoleofthekidneyistheproperterminologyforthe“upper”
1 Motivationsandrelatedworks
or“top”ofthekidney.
With the impact of Large Language Models (LLMs) over a wider We also face the dual phenomenon that the wording in free text
communitythantheNaturalLanguageProcessing(NLP)one,even maywidelydifferfromonereporttoanother,bothinthelinguistic
beyond the boundaries of the scientific community, there is a ten- structureandinthemedicalterms.Differentmedicalterms,aswell
dencytoexplorethefullcapabilitiesofthesenewmodelsinpartic- asLatinform,ormedicalabbreviations(sometimesevencodes),can
ularforeveryNLPtask.Itseemsthat"biggerisalwaysbetter"and beusedtodescribesimilarpatient’scondition[17,16]:astheniaor
"one-model-for-all-tasks" (e.g. the foundation models) are the new weakness,arthralgiaorjointpain,epistaxisornosebleed,etc.
trendymottosinNLP. Wedonotevenmentiontyposthatcantransformileum(thelast
sectionofthesmallintestinebetweenthejejunumandthecolon)into
Domain-specific and technical jargon issues Despite some re-
ilium(apartofthepelvicbone),orperoneal(pertainingtothelateral
markable successes of LLMs, when investigating the potential of
aspectoftheleg)intoperineal(pertainingtothepelvicfloor).
these approaches for specific tasks, a very recent work [24] has
Alloftheseissuesonarigoroususeofaprecisemedicalterminol-
shown that smaller language models are more appropriate in a
ogyandtheirconsequencesonhealthcarearewelldocumentedand
domain-specificcontext,especiallywhenhighaccuracyisexpected takenseriouslyintrainingprogramsinmedicine1.
becauseweareconsideringahigh-stakesdomain,suchaslawintheir
Thiscontextexplainswhyourfirstattemptstoinvestigaterecent
paper or medicine in our work. In their work, the authors consider
advancesininformationretrievalbasedonlanguagemodelshavenot
morespecificallyseveralalgorithmsbasedonBERTbackbone[5],
beenconclusive[10,20].Still,themethodsproposedinthesepapers
RoBERTa[14],DistilBERT[22].Theauthorsexplaintheimpactof
arebasedonvariantsofBERTbackbone(theaforementionedones,
considering a technical jargon, called “Legalese language" in Law
orALBERT[12],ERNIE[33]),asforthepreviouslycitedpapers.
∗CorrespondingAuthor.Email:celia.dcruz@inria.fr
∗∗CorrespondingAuthor.Email:bereder.jm@gmail.com 1 European Medicine Agency, Medical terms simplifier,
∗∗∗CorrespondingAuthor.Email:frederic.precioso@inria.fr https://www.ema.europa.eu/en/documents/other/ema-medical-terms-
∗∗∗∗CorrespondingAuthor.Email:michel.riveill@inria.fr simplifier_en.pdf
4202
guA
32
]LC.sc[
1v35231.8042:viXraLong text classification issues It took only a few years for to30),fromthesemanticfieldoftheinformationtoberetrieved.
Transformer-basedmodels[29]tobecomestate-of-theartfortexten- Forreproducibilitysake,weprovidealternativestogeneratethis
coding,drasticallyimprovingtextclassificationperformances.How- short list from publically available external resources. We show
ever,whenfocusingontheaforementionedencodersmostofthem thatourdeepmodelisnotmuchimpactedbythesizeofanauto-
are limited to 512 tokens as the maximum input sequence length, maticallygeneratedlistevenifitcontainsextrauselessterms.
limitingtheamountoftextthatcanbetakenintoaccount.Thesetext • Ahierarchicaldeepmodelbased,first,ontheBERTembeddingof
embeddingmethodshavebeenmostlyusedforshorttextclassifica- thepotentiallyrelevanttargettermscontainedinthefilteredsen-
tion,whilemedicalreportscanbeseveralpageslong. tences,thenontheattentionweightaverageofthesetermembed-
Forclassifyinglongtexts,severalstrategieshavebeenproposed. dingstoresultinthedocumentembeddingthatisfinallyclassified.
The most basic strategies involve indiscriminately truncating long • Anextensiveevaluationandablationstudyconductedagainstall
documents,thuscompletelydiscardinglargetextpartsthatcancon- the state-of the-art approaches (other hierarchical deep models,
tainrelevantinformation.Forinstance,takingintoaccountonlythe non-hierarchicalmodels,long-rangetransformers),showingalso
beginning,ortheend,orarandompartofamedicalreport. therobustnessofourmodelwithrespecttothequalityoftheshort
Other models involve selecting key sentences from documents listoftargettermsusedduringthefilteringphase,anddemonstrat-
suchasCogLTX[6]andBert+TextRank[19]. ingtheoverallperformanceofoursolution.
Other approaches adapt existing pre-trained transformer models
byusinghierarchicalstrategies[31,7,18,4],dividingthewholetext InthispaperwedetailboththeprivateFrenchcorpusandthepub-
intosmallpartsthatareiterativelyfedtothetransformermodel. licEnglishbenchmarkusedforevaluation(Sec.2),describeourap-
Anotherstrategyinvolvestheuseoflong-rangeTransformers[25, proach(Sec.3),andtesttheeffectivenessofourapproachagainstref-
26]thatemployvariousstrategiestoreducequadraticself-attention erenceapproachesonthetwobenchmarks(Sec.4).
complexity, thus enabling them to have a significantly higher limit
inthenumberoftokensthatcanbeprocessed(e.g.Longformer[2],
2 Datasets
Reformer[11],BigBird[32]).However,mostofthesedevelopments
havebeenprimarilymadeforEnglish.
2.1 PublicEnglishdatasetforsmokingstatus
Finally, the most recent approaches that have been proposed are
based on extracting embedding layers from decoder architectures
WeusethepubliclyavailableEnglishdataset"2006-Smoking"[28],
suchasLlama2[27].
where the classification task is to identify patient smoking status
However,whentheobjectiveistofocusonaspecificcriticalde-
from medical discharge records. This dataset was released as part
tail, as opposed to topic classification, relevant information repre- oftheNationalNLPClinicalChallenges(n2c2,previouslyi2b2)2.
sents only a small portion of all the information present in the re-
Thedatasetconsistsof502de-identifieddischargesummariesan-
ports.Therelevantinformationmayappearafewtimesinareport,
notatedbypulmonologiststoclassifypatientreportsintofivecate-
orevenonlyonce.Additionally,theavailabletrainingcorpussizecan
gories:PastSmoker(patientisaformersmokerwhohasnotsmoked
bequitelimited.Inthiscontext,mostoftheexistingmodelsabovedo
foratleastoneyear),CurrentSmoker(patientiscurrentlyasmoker
notdemonstratehigherperformancesforourpeculiarclassification
orwasasmokerwithinthepastyear),Smoker (aCurrentoraPast
whilerequestingsignificantlybiggercomputationresources.
Smokerbutthereportdoesnotprovideenoughinformationtoclas-
Privacyandcomputationresourcesconcerns In[23],Schickand sifyaseither),Non-Smoker (patienthasneverbeenasmoker),Un-
Schützeshowthatsmalllanguagemodels(ALBERT,RoBERTa)can known(thereportdoesnotmentionanythingaboutsmoking).There
befew-shotlearnersaslargermodelshaveshown.Theauthorsmen- isanaverageof874wordsperreport,withamaximumof3292.
tion that they achieve similar performances as larger LLMs with Some reports do not give any indication on the patient smoking
“language models that are much “greener” in that their parame- status.Inotherreports,whilethisinformationisprovided,itismen-
tercountisseveralordersofmagnitudesmaller”.In[9],Hsiehetal. tionedonceoratveryfewplaces,thereforerepresentingonlyatiny
emphasizeshowlargeLLMsare"challengingtodeployinrealworld fractionofareport.Moreover,thewordingindicatingthepatientsta-
applications due to their sheer size" and that their "computational tus varies from one report to another. For instance, both sentences
requirementsarefarbeyondaffordableformostproductteams,es- "Sheisaformersmoker"and"Nicotineabuse,quitinthe80s"indi-
peciallyforapplicationsthatrequirelowlatencyperformance". catesthatthepatientisapastsmoker.
This concern is particularly relevant in our case since working Theprovideddatasetcontains104reportsastestingdata(11Past
on medical personal data prevents from outsourcing computations Smoker, 11 Current Smoker, 3 Smoker, 16 Non-Smoker, 63 Un-
usingcloudsolutions,andalmostnohealthcenterhostscomputation known)and398reportsastrainingdata(36PastSmoker,35Current
resources is able to run very large language models on a regular Smoker,9Smoker,66Non-Smoker,252Unknown).
basis.Designingalightermodelisthereforeoneofourobjectives,
andtodothis,relyingononeofthealgorithmsintheBERTfamily
2.2 Frenchdatasetfromcolorectalcancerpatients
isareasonablechoice.
TheFrenchdataset3consistsof198de-identifiedFrenchmedicalre-
In this paper, we present our deep learning approach designed
portsfromcoloncancerpatientsobtainedfromNiceUniversityHos-
toclassifymedicalreports,documentsrangingfromonetoseveral
pital.Allidentifyinginformation-includingpatients’anddoctors’
pagesinlength,focusingoncriticalbutsparsespecificinformation,
names, birthdays, ID numbers, locations - has been removed. The
swampedwithnoisyandconfusingcontent.
Weproposethreemaincontributions: 2WebsitefortheNationalNLPClinicalChallenges(n2c2)https://n2c2.dbmi.
hms.harvard.edu/
• Afilteringphasetoreducetherepresentativenessgapbetweenrel- 3 Thisdatasetisprivateandhasbeencreatedunderanagreementbetween
evantandirrelevantcontentbasedonashortlistoftargetterms(up NiceUniversityHospitalandUniversitéCôted’Azur.anonymitywasverifiedthroughtheuseofregexrulesandeachre- 3.2.1 Englishdatasetrelevantterms
portwascarefullyreadmultipletimesbymedicalexperts.Themedi-
Thelistofrelevanttermsaimsatidentifyingthepatientsmokingsta-
calrecordscovertheperiodfromthebeginningof2017totheendof
tusintheEnglishdataset. Wetesttwodifferentapproaches.Inthe
2021.Theyrangefromonepagetoafewpageslong(572wordson
first approach, the relevant terms are automatically extracted from
averageperreport,maximum2115)withapre-determinedsequence
the website relatedwords.io4 that gathers, from a single source tar-
ofsections.However,thetextofeachsectionisfreelywritten.Oneof
get term, a list of related terms sorted by relevance. In our case,
themaintypesofreportsisfollow-upletterswithsectionsdescribing
since we want to classify the smoking status of patients (Current
thepatient’sco-morbidities,cancerhistory,previoustreatments,bi-
Smoker,PastSmoker,Smoker,Non-Smoker,Unknown),wechoose
ologicalparameters,etc.Otherreporttypesarepathologicalreports,
thesourcetargetterm"smoker".Inthesecondapproach,wedonot
surgicalreports,imagingresults,etc.
useanyexternalresource;weusetheX-Class[30]keywordexpan-
Thosereportsmaycontaininformationaboutthelateralityofthe
sionmethodwiththesourcetargetterm"smoker"toretrieverelevant
primary colon tumor (classified as either "left", "right", or "no in-
termsrelatedto"smoker"withinthedatasetfromthetrainingdocu-
formation"),whetherasurgeryforthecancerhasalreadybeenper-
mentthemselves.
formed (classified as either "yes" or "no / no information"), and
We remove English stop words from the aforementioned list of
whetherachemotherapyhasalreadybeeninitiated(classifiedasei-
relevanttargettermsusingthePythonlibrarystopwordsiso5thatre-
ther"yes"or"no/noinformation").Somereportsdonotindicateany
moves terms such as "not", "non", "very". Therefore, the final list
ofthisinformation,whileotherreportsindicateseveralinstancesof
containsthesourcetargetterm"smoker"anditsrelatedtermsfrom
thisinformationinoneormoresections.
therelatedwords.iowebsiteminusthestopwords.
OurdatasethasbeenannotatedbyasurgicaloncologistandaDeep
Thisfinaltargettermlistisrankedbytherelevanceoftheterms
Learning(DL)expertintwoways:theexactlocationsoftherelevant
tothewords"smoker"inbothrelatedwords.iowebsiteandX-Class
informationineachreport,aswellastheoverallclassificationofthe
methods.However,wecanvarythelistsizebyremovingtheterms
informationatthereportlevel.Asanexample,forthelateralityofthe
rankedasleastrelevantinordertostudythesensitivityofourmodel
primarycolontumor,ifareportindicates“Thepatienthasatumor
tothetargettermlist,aswedidinourexperiments.
intheleftcolonandametastasisintherightlobeoftheliver.”,only
theword“left”isannotatedasindicatingthelateralityoftheprimary
colontumor,sincetheword“right”doesnotindicatethelateralityof 3.2.2 Frenchdatasetrelevantterms
theprimarytumor,itrefersinsteadtoametastasis.Thewholereport
isclassifiedas“left”forthelaterality. The list of relevant target terms for each classification task – lat-
Outof198reports,thedocumentlabeldistributionforlaterality erality (of the primary colon tumor), surgery (performed), and
is67(right),81(left),and50(no).Thedistributionforsurgeryis44 chemotherapy(initiated)–toidentifyrelevantinformationfromthe
(no)and154(yes)andforchemotherapyitis145(no)and53(yes). FrenchdatasethasbeenprovidedbythemedicalexpertsfromNice
AlthoughtheprivateFrenchmedicaldatasetcannotbeshared,we UniversityHospital,whoconsideredthatlistasmorerelevantthan
still present the results as it is interesting in the sense that the list
usingaFrenchversionofrelatedwords.io6.Thethreelistsprovided
ofrelevantkeywordswasmanuallydefinedbymedicalexperts,and bytheexpertsrangefromapproximately10to20terms.Thoselists
thattheclassificationtasklateralityisparticularlychallengingsince are much smaller than the ones extracted from relatedwords.io be-
"left"and"right"aretargettermsbutarealsoeverywhereinreports. causethosethreelistshavebeencarefullytailoredtoeachclassifica-
tiontask,whicharequitespecific.Thoselistsprovidedbytheexperts
arenotrankedbyrelevance.Wesimulatevariablelistsizesbymanu-
3 Approach allyaddingtermsthatareoflittlerelevancetotheclassificationtask
(namely"augmentedvocabulary"),ormanuallydeletingsomerele-
3.1 Overview vanttermsfromthoselists(namely"reducedvocabulary").
For contexts where the relevant information appears quite sparsely 3.3 Filtering
inthedocuments,modelsthatindiscriminatelytakeintoaccountthe
Oncethelistofrelevanttargettermshasbeenestablished,wesearch
wholetextwilllikelystruggletofocusonthefewrelevantpartstoac-
inthedocumentsforallinstancesofthosetargetterms.
curatelyclassifythedocuments.Theideaofourapproachistoonly
CaseA:ifadocumentdoesnotcontainanytargetterm,itiscat-
keep the potentially relevant parts of a document while discarding
egorized as "no / no information" regarding laterality, surgery, or
most of the remaining probably irrelevant parts, therefore enabling
chemotherapy, or "unknown" regarding the patient smoking status.
ourtransformer-baseddeeplearningmodeltofocusonlyonafrac-
Nofurtheractionistaken.
tionofadocument.
CaseB:iftargettermsarepresent,weextractallsentencesfrom
The relevant parts of a document are retrieved by building a list
thedocumentsthatincludethesetargetterms.Thesesentencespro-
oftargettermspotentiallyrelevanttotheclassificationtask,andthen
videacontext.Thesetargettermsmayberelevant,becausetheyactu-
byextractingallthesentencescontainingoneormoreofthosetarget
allyprovideexplicitinformationonthelabelofthedocumentforone
terms.Figure1illustratesourapproach.
ofthetargetclasses(e.g.thelaterality,theoccurrenceofasurgery,or
theinitiationofchemotherapy),buttheymayalsobeirrelevant(“past
3.2 Relevanttargettermlist shouldersurgeryafterroadaccident”doesnotindicateasurgeryfor
cancer,“theuseofachemotherapytreatmentwilllaterbediscussed”
Wecompilealistoftargettermsrelevanttoeachclassificationtask. 4https://relatedwords.io
Itshouldbenotedthatthosetargettermscanbesinglewordsorcom- 5https://pypi.org/project/stopwordsiso/
posedofseveralwords. 6Forinstance:https://www.rimessolides.com/motscles.aspx?m=chirurgieClassification head
+ Loss
Compile a list of relevant target terms related to a given
classification task Document embedding
Averageof embeddings weightedby W
i i
……
W W
Does the document 1 N
contain at least a term
from the list? Softmax
Optional
NO YES S S
1 N loss
Sharedlinearlayer …… Sharedlinearlayer
Extractall sentences
No further action, classify the
containingatargetterm
document as “no” / “unknown” fromthe list Potentialentity …… Potentialentity
embedding embedding
1 N
Transformer encoder …… Transformer encoder
Sentence withpotential Sentence withpotential
……
entity entity
1 N
Figure1. Architectureofourapproach
does not indicate that a chemotherapy has already been initiated). inafewsentences.Therefore,asecondfilteringphasecanbeoption-
The target term "smoker" can be used in sentences such as "she is allyperformedbysubmittingthesentencesinthetrainingsetinitially
a former smoker", "she is not a smoker", "current smoker", which selectedfromthefirstfilteringphasetotheexpertsforfurtherman-
doesnotleadtothesamesmokingstatuslabel.Fromnowon,wecall ualannotationtodeterminewhetherthetermswithintheirsentence
entitiesthetargettermsinthecontextoftheirsurroundingsentence. areindeedrelevantornot.However,toreducetheneedofinterac-
tionswiththeexperts,anactivelearningstrategycanbeemployedto
annotateonlyasmallfractionofthedocumentsduringthissecond
3.4 Modelarchitecture
filteringphase.
Ourinitialmodelarchitecturecanbeextendedbyaddingasigmoid
3.4.1 Mainarchitecture
functionaftertheS scoresfollowedbyanoptionalbinarycrossen-
i
tropylosstoleveragetheannotationofthesecondoptionalfiltering
Thefilteringdoneinthepreviousstepretainedthesentencescontain-
phaseattrainingtime.Table7showstheimprovedperformanceon
ingtherelevantvocabulary.Weobtainacontextualizedembedding
ourthreedocumentclassificationtasksinourFrenchdatasetthathas
ofeachtargettermbypassingthesentencetowhichitbelongstoa
beenfurtherannotatedforthesecondoptionalfilteringphase.
transformerencoderandselectingspecificallytheembeddingasso-
ciatedtothetargetterm.
Someentitiesmayactuallybeirrelevanttotheclassificationtask, 4 Results
while others are highly relevant. The model calculates an attention 4.1 BaselinemodelandDLcontenders
weightW foreachtargettermtogivemoreweightstorelevantenti-
i
ties.Thisisdonebypassingeachtargettermembeddingtoashared WebenchmarkourapproachonbothourFrenchandEnglishdatasets
linear layer that outputs a score S and normalizing all the scores againstMachineLearning(ML)andDLarchitecturesdesignedfor
i
withasoftmaxlayer. longtextclassification.
Thedocumentembeddingistheproductsumofeachentityem-
• MLmodel.Logisticregression(LR)whereweremovedbefore-
bedding and their attention weight. This vector is then used in the
handstopwordsandpunctuationsfromthereports,welemmatized
classificationheadthatincludesalinearlayerfollowedbyasoftmax
andchangedthetexttolowercase,andappliedTF-IDF.
layerwithcross-entropylossfunction.
• SentenceselectionDLmodel.Bert+TextRank[19].
• Flat DL model. "SAN"-like model [8] that performs classifica-
3.4.2 Modelextension tiononthedocumentembeddingobtainedbysimplyperforming
aweightedaverageofallsubwordembeddings.
Whileafirstfilteringphasehasbeenperformedtoselectpotentially • Hierarchical DL models. 1) "HiSAN"-like model [8] with a
relevant sentences based on the presence of target terms, it might Transformerencodertoreplacetheirnon-contextualembeddings
stillbechallengingincertaincasestoobtainverygoodclassification andwesplitthetextsintoblocksof512tokens.2)"Transformer
results.Forinstance,thelateralityclassificationtaskischallenging overCLS"withmaxpooling[3].
because the target terms "right" and "left" may very often appear • Long-rangetransformers.1)Clinical-LongFormerusingglobal
within a report but be relevant to the primary tumor laterality only attention only on the first token [13]. 2) Clinical-BigBird usingLaterality Surgery Chemotherapy
Balanced Balanced Balanced
Accuracy Accuracy Accuracy
Accuracy Accuracy Accuracy
LR 0.556 0.542 0.843 0.753 0.874 0.872
HiSAN 0.526 0.490 0.847 0.761 0.866 0.841
TransformeroverCLS 0.561 0.543 0.866 0.737 0.851 0.820
SAN 0.579 0.544 0.860 0.866 0.839 0.834
Bert+TextRank 0.689 0.662 0.878 0.787 0.865 0.862
Ours 0.782 0.741 0.940 0.892 0.925 0.875
Table1. ModelcomparisonsonindividualtasksonourFrenchdataset
sparseattention[13].3)Llama27B[27]usingtheembeddingof
Smokingstatus
thelasttoken.Ifatextexceeds4096tokens,onlythebeginningof
thetextupto4096tokensisusedasinputtothemodels. Balanced
Accuracy
Accuracy
LR 0.596 0.215
4.2 Experimentalsetup
Bert+TextRank 0.604 0.253
WeusethetestsetprovidedintheNationalNLPClinicalChallenges SAN 0.817 0.551
(n2c2),fortheEnglish"2006-Smoking"dataset.Weapplya5-fold
TranformeroverCLS 0.848 0.624
cross-validationprocedureontheremainingtraining/validationset
andwereporttheaverageresultsonthetestset.Weapplya5-fold HiSAN 0.823 0.577
cross-validation procedure on the French dataset where the sum of
Llama2 0.829 0.560
eachtestfoldcoverstheentiredataset.
ExceptforMLandlong-rangetransformerapproaches,weusethe LongFormer 0.813 0.599
Bio+DischargeSummaryBERT[1]pretrainedmodelfromHugging BigBird 0.875 0.652
Facelibrary7astheTransformerencoderfortheEnglishdataset,and
Ours(N=30)
theCamemBERT-basepretrainedmodel[15]fromHuggingFaceli- 0.900 0.665
brary8 as the Transformer encoder for our French dataset. We use relatedWords.io
PySBD[21]toidentifysentenceboundaries.
Ours(N=20)
For each classification task and for each architecture, we train a 0.887 0.668
separate model. Our evaluation metrics are the accuracy9 and bal- X-Class
ancedaccuracy10.InallDLexperiments,weusethesameoptimizer Table2. ModelcomparisonsontheEnglishdataset."Ours"correspondsto
(AdamW), effective batch size (4), learning rate (2e-5), early stop ourapproachwiththefirstNmostrelevanttargettermsextractedfromthe
patience(5)andlabelsmoothing(0.1). relatedwords.iowebsiteorX-Classmethod
4.3.2 Robustnessofthevocabularylist
4.3 Experimentalresults
BoththeFrenchandEnglishdatasetsshowthatourapproachisnot
verysensitivetonoiseinthetargettermlist,namelythevocabulary
4.3.1 Comparisonwithdiversetypesofapproach
list.Theinclusionofadditionalunnecessarytermsortheomissionof
non-criticaltermsdoesnothaveamajorimpactontheresults.
We compare our approach with different types of models as illus- Table3shows,ontheEnglishdatatset,theimpactofthevariation
trated in Table 1 on the French dataset and Table 2 on the English ofthenumberNofmostrelevanttargettermsgeneratedeitherbythe
dataset. Our approach shows substantially better results than ML, relatedwords.iowebsiteortheX-Classkeywordexpansionmethod.
flat,orothersentenceselectionmodels.Likewise,ournarrowerhi- Withtherelatedwords.ioapproachontheEnglishdataset,thefirst
erarchicalmodel,whichusesafractionofthetextbyonlyselecting N = 10 target terms does not include some critical terms such as
potentiallyrelevantsentences,demonstratesbetterresultsthanother "smoking", hence resulting in poor results. The first N = 20 target
hierarchicalmodelswherethewholetextisindiscriminatelyfediter- termsdoesinclude"smoking"aswellas"nonsmoker",thereforesub-
ativelytoaBert-likemodel.Long-rangetransformersallowtheentire stantiallyimprovingtheresults.ThefirstN=30targettermsincludes
documentstobeprocessedallatonce,buttheyarecomputationally additionally relevant terms and little noise, giving the best results.
expensiveandourlighterapproachshowsbetterperformances. Increasing N only slightly decreases the performances as noise in-
creaseswithlessrelevantterms.
7https://huggingface.co/emilyalsentzer/Bio_Discharge_Summary_BERT With the X-Class target term expansion method on the English
8https://huggingface.co/camembert-base
dataset, the first N = 10 terms already includes critical terms such
9https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_
as"smoking","tobacco","cigarette",thereforealreadyshowingvery
score.html
10 https://scikit-learn.org/stable/modules/generated/sklearn.metrics. goodresults.UsingN=20showssimilarresults,andusingaddition-
balanced_accuracy_score.html allylessrelevanttermsonlyslightlydecreasestheperformance.relatedwords.io X-Class
Balanced Balanced
Accuracy Accuracy
Accuracy Accuracy
Ours(N=10) 0.788 0.489 0.887 0.658
Ours(N=20) 0.894 0.655 0.887 0.668
Ours(N=30) 0.900 0.665 0.879 0.659
Ours(N=40) 0.887 0.665 0.867 0.639
Ours(N=50) 0.887 0.658 0.871 0.640
Table3. VariationofthenumberNofmostrelevanttargettermsontheEnglishdataset
"Ours" list adds the target terms related to anatomical parts of the
Laterality
colon that indicates the laterality (“caecum”, “sigmoid”, etc.). Our
Balanced "augmentedvocabulary"listcontainsafewadditionaltermsthatare
Accuracy
Accuracy relatedtothecolonbutarenotrelevantinthereportswehave,such
as"transverse"(middlepartofthecolon,neitherleftorright).
Ours 0.782 0.741
Furthermore,ourapproachisnotverysensitivetothemethodused
Augmented tofindthetargetterms:interviewingexpertsfromthedomain,using
0.752 0.701
theX-Classkeywordexpansionmethod,orusingtherelatedwords.io
vocabulary
website yield strong results as long as the vocabulary lists contain
Reduced
enoughcriticallyrelevantterms.
0.764 0.723
vocabulary
Table4. Vocabularylistvariationsforthelateralityclassificationtask
4.3.3 Impactofthesparsityoftherelevantinformation
Surgery Modelsthatindiscriminatelyconsiderthewholetextmaystruggleto
classifydocumentswhentherelevantinformationappearssparsely
Balanced
Accuracy andtherestofthetextoffersverylittlehints.Theimpactthedegree
Accuracy
ofsparsityisbestillustratedbyTable1ontheFrenchdataset.Forthe
Ours 0.940 0.892 lateralitytask,thestructureortypeofreports(e.g.follow-upletters,
imagingreports,etc.)provideverylimitedcluesonthelateralityof
Augmented
0.927 0.874 the colon tumor, and the information of the colon tumor laterality
vocabulary appearsatmostonlyinafewsentencesinreportsthatcanbeafew
Reduced pages long, making it challenging for other models to classify the
0.942 0.887 documents,whereasourapproachshowsalargeimprovement.How-
vocabulary
ever,forthechemotherapyandsurgerytasks,certaintypesofreports
Table5. Vocabularylistvariationsforthesurgeryclassificationtask
offersomeclues.Forinstance,pathologicalreportsareoftenrelated
toasurgeryandsometypesoffollow-uplettersareusuallyrelated
Chemotherapy toachemotherapy.Inthiscase,ourmodelstillshowsbetterperfor-
mance,buttheimprovementoverothermodelsislesssubstantial.
Balanced
Accuracy
Accuracy
4.3.4 Optionalsecondlossfunction
Ours 0.925 0.875
Reduced Thecolontumorlateralityclassificationtaskisparticularlychalleng-
0.921 0.871
vocabulary ing since "right" and "left" terms appear everywhere in the reports
andareoftennotrelevant(e.g.“right”or“left”inatextmayrefers
FurtherReduced
0.902 0.841 tothecolontumorlateralitywhichisrelevant,butalsotothesideofa
vocabulary lungmetastasiswhichisirrelevant).Tocopewiththisissue,wehave
Table6. Vocabularylistvariationsforthechemotherapyclassificationtask thusevaluatedtheimpactofafurtherrefinementprocessinvolvinga
secondtargetedannotationofthesentencesselectedthroughthefirst
Tables4,5and6showtheimpactofthevariationofthevocabulary relevanttermlistfiltering,inthetrainingsetonly.Wehaveusedthis
listsontheFrenchdataset."Reducedvocabulary"listsonlyinclude refinedannotationwithanadditionallossfunctiontohelpthemodel
themostrelevanttargettermsthatarepresentinalltherelevantdoc- trainingonmoreprecisely“relevant”/“nonrelevant”termsandsen-
uments,"Ours"listsincludeadditionallyrelevantterms,while"aug- tencesfromthetext.Sincethisrequireextrahumanoperations,we
mentedvocabulary"listsincludeadditionaltermsthatareoflittlerel- onlyevaluatedthisprocessontheFrenchdataset.
evance.Thesupplementarymaterialdetailsthevocabularylistsused Trainingthemodelwithbothlosses(theinitialclassificationone,
intheFrenchdataset.Forinstance,inthecaseofthelateralityclas- andtherefinementfilteringonewhereallfilteredsentenceshavebeen
sificationtask,our"reducedvocabulary"listonlycontainsthe"left" furtherannotatedinthetrainingset),ourapproachshows,inTable7,
and"right"targettermsastheyarethemostpresentinthereports. significantimprovementsforallthreeclassificationtaskscomparedLaterality Surgery Chemotherapy
Balanced Balanced Balanced
Accuracy Accuracy Accuracy
Accuracy Accuracy Accuracy
Ours 0.782 0.741 0.940 0.892 0.925 0.875
Ours+2ndloss 0.904 0.891 0.979 0.970 0.940 0.901
Table7. Documentclassificationresultsofourmodelwiththeoptionalsecondlossfunction
tousingonlytheclassificationlossfunction,especiallyforthelat- References
erality classification task as expected but still significantly also for
[1] E.Alsentzer,J.Murphy,W.Boag,W.-H.Weng,D.Jindi,T.Naumann,
surgeryandchemotherapy.
andM.McDermott. PubliclyavailableclinicalBERTembeddings. In
Aninterestingfutureworkwouldbedesigninganactivelearning A.Rumshisky,K.Roberts,S.Bethard,andT.Naumann,editors,Pro-
strategytoannotateonlyasmallsubsetoffilteredsentences,request- ceedingsofthe2ndClinicalNaturalLanguageProcessingWorkshop,
pages 72–78, Minneapolis, Minnesota, USA, June 2019. Association
inglittleworkfromdomainexpertsforasignificantimprovement.
forComputationalLinguistics.
[2] I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-
document transformer. CoRR, abs/2004.05150, 2020. URL https:
5 Conclusion
//arxiv.org/abs/2004.05150.
[3] X.Dai,I.Chalkidis,S.Darkner,andD.Elliott.Revisitingtransformer-
Inthispaper,wepresentanapproachtoidentifyiflongdocuments based models for long document classification. In Y. Goldberg,
areusefulforexpertsbasedonrelevantinformationwhichappears Z. Kozareva, and Y. Zhang, editors, Findings of the Association
for Computational Linguistics: EMNLP 2022, Abu Dhabi, United
sparselyinthosedocuments,asitisthecaseforinstanceinmedical
Arab Emirates, December 7-11, 2022, pages 7212–7230. Associa-
reports.Wetacklethistaskconsideringitasabinaryclassification tion for Computational Linguistics, 2022. doi: 10.18653/V1/2022.
task:thedocumentanalyzedcontainsornottherelevantinformation. FINDINGS-EMNLP.534.
Traditional ML and DL models designed for long text classifi- [4] X.Dai,I.Chalkidis,S.Darkner,andD.Elliott.RevisitingTransformer-
basedModelsforLongDocumentClassification. InFindingsofthe
cation struggle in this context. Our model is based on hierarchical
AssociationforComputationalLinguistics:EMNLP2022,pages7212–
representationscombiningtoken-levelembeddingswithasentence- 7230,AbuDhabi,UnitedArabEmirates,2022.AssociationforCompu-
levelrepresentationintoadocumentembeddingallowingtoclassify tationalLinguistics. doi:10.18653/v1/2022.findings-emnlp.534. URL
https://aclanthology.org/2022.findings-emnlp.534.
preciselythedocumentasrelevantorirrelevant.Thisspecificstruc-
[5] J.Devlin,M.Chang,K.Lee,andK.Toutanova. BERT:pre-training
ture combining different embedding levels of the text outperforms ofdeepbidirectionaltransformersforlanguageunderstanding. CoRR,
longrangetransformersaswellashierarchicaldeepmodelsthathave abs/1810.04805,2018.URLhttp://arxiv.org/abs/1810.04805.
beenproposedintheliteratureforlongdocumentclassification. [6] M. Ding, C. Zhou, H. Yang, and J. Tang. Cogltx: Applying
BERT to long texts. In H. Larochelle, M. Ranzato, R. Had-
TextembeddinginourmodelisbasedonBERTvariants,nowa-
sell, M. Balcan, and H. Lin, editors, Advances in Neural Informa-
dayssocalledsmalllanguagemodels,andoutperformsmorerecent tion Processing Systems 33: Annual Conference on Neural Informa-
LLM-based embeddings, hence confirming recent recurrent results tionProcessingSystems2020,NeurIPS2020,December6-12,2020,
insimilarcontextsthatlargerLLMsarenotalwaysbetter. virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
96671501524948bc3937b4b30d0e57b9-Abstract.html.
Weevenproposeahuman(expert)intheloopmechanismforcon-
[7] S.Gao,M.T.Young,J.X.Qiu,H.-J.Yoon,J.B.Christian,P.A.Fearn,
texts where the vocabulary is confusing based on too general and G.D.Tourassi,andA.Ramanthan.Hierarchicalattentionnetworksfor
broadtargetterms,improvingtheperformancesevenmore.Ourap- informationextractionfromcancerpathologyreports. Journalofthe
AmericanMedicalInformaticsAssociation,25(3):321–330,Mar.2018.
proachoutperformsSOTAonbothaveryspecificFrenchdatasetand
ISSN1067-5027,1527-974X. doi:10.1093/jamia/ocx131. URLhttps:
amoreclassicpublicEnglishbenchmark,andcaneasilybeextended //academic.oup.com/jamia/article/25/3/321/4636780.
tootherclassificationtasksanddatasets. [8] S.Gao,J.X.Qiu,M.Alawad,J.D.Hinkle,N.Schaefferkoetter,H.-
In future work, we plan to investigate the extra load of refining J.Yoon,B.Christian,P.A.Fearn,L.Penberthy,X.-C.Wu,L.Coyle,
G. Tourassi, and A. Ramanathan. Classifying cancer pathology re-
annotationofselectedsentencesthroughanactivelearningstrategy.
portswithhierarchicalself-attentionnetworks. ArtificialIntelligence
inMedicine,101:101726,Nov.2019. ISSN09333657. doi:10.1016/j.
artmed.2019.101726. URLhttps://linkinghub.elsevier.com/retrieve/pii/
6 Ethicalconsiderations S0933365719303562.
[9] C.-Y.Hsieh,C.-L.Li,C.-K.YEH,H.Nakhost,Y.Fujii,A.J.Ratner,
Ourworkholdsthepotentialtopreventhealthcareprofessionalsfrom R.Krishna,C.-Y.Lee,andT.Pfister. Distillingstep-by-step!outper-
unintentionallyoverlookingcriticaldetailsinpatients’files,therefore forming larger language models with less training data and smaller
modelsizes.InThe61stAnnualMeetingOfTheAssociationForCom-
potentiallyenhancingclinicaldecision-making.However,ourmodel
putationalLinguistics,2023.
isnotflawless,andthemistakesitmakescouldconverselynegatively [10] V.Karpukhin,B.Oguz,S.Min,P.Lewis,L.Wu,S.Edunov,D.Chen,
impactclinicaldecisions.Hence,ourworkshouldcomplement,not andW.-t.Yih. Densepassageretrievalforopen-domainquestionan-
replace,expertjudgment.Thisresearchshouldbeseenasastepto- swering. InProceedingsofthe2020ConferenceonEmpiricalMeth-
ods in Natural Language Processing (EMNLP), pages 6769–6781,
wardsfacilitatedinformationextractionwhileemphasizingtheirre-
Online, Nov. 2020. Association for Computational Linguistics. doi:
placeabilityofhumanexpertiseinmedicalcontexts. 10.18653/v1/2020.emnlp-main.550.
Moreover,ourprivateFrenchdatasetisde-identifiedtoensurepa- [11] N.Kitaev,L.Kaiser,andA.Levskaya. Reformer:Theefficienttrans-
former. In 8th International Conference on Learning Representa-
tientconfidentialityandcomplieswithGeneralDataProtectionReg-
tions,ICLR2020,AddisAbaba,Ethiopia,April26-30,2020.OpenRe-
ulationandthestrictrulesofprocessingmedicaldatainFrance.The view.net,2020.
rawdataarenotprocessedoutofthehospital,butconcernsoverdata [12] Z.Lan,M.Chen,S.Goodman,K.Gimpel,P.Sharma,andR.Soricut.
privacyandpotentialmisusecouldstillberaised.Weareawareof ALBERT:AliteBERTforself-supervisedlearningoflanguagerepre-
sentations. In8thInternationalConferenceonLearningRepresenta-
thoseconcernsandallthisworkisconductedunderastrictprivacy
tions,ICLR2020,AddisAbaba,Ethiopia,April26-30,2020.OpenRe-
andconfidentialityprocedureofthehospital. view.net,2020.[13] Y.Li,R.M.Wehbe,F.S.Ahmad,H.Wang,andY.Luo. Acompara- https://doi.org/10.48550/arXiv.2307.09288.
tivestudyofpretrainedlanguagemodelsforlongclinicaltext. J.Am. [28] O. Uzuner, I. Goldstein, Y. Luo, and I. Kohane. Identifying Patient
MedicalInformaticsAssoc.,30(2):340–347,2023. SmokingStatusfromMedicalDischargeRecords.JournaloftheAmer-
[14] Y.Liu,M.Ott,N.Goyal,J.Du,M.Joshi,D.Chen,O.Levy,M.Lewis, icanMedicalInformaticsAssociation,15(1):14–24,Jan.2008. ISSN
L.Zettlemoyer,andV.Stoyanov.Roberta:ArobustlyoptimizedBERT 1067-5027,1527-974X.doi:10.1197/jamia.M2408.
pretrainingapproach. CoRR,abs/1907.11692,2019. URLhttp://arxiv. [29] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
org/abs/1907.11692. Gomez,L.u.Kaiser,andI.Polosukhin. Attentionisallyouneed. In
[15] L. Martin, B. Muller, P. J. Ortiz Suárez, Y. Dupont, L. Romary, I.Guyon,U.V.Luxburg,S.Bengio,H.Wallach,R.Fergus,S.Vish-
E.DeLaClergerie,D.Seddah,andB.Sagot. CamemBERT:aTasty wanathan,andR.Garnett,editors,AdvancesinNeuralInformationPro-
FrenchLanguageModel.InProceedingsofthe58thAnnualMeetingof cessingSystems,volume30.CurranAssociates,Inc.,2017.
theAssociationforComputationalLinguistics,pages7203–7219.As- [30] Z. Wang, D. Mekala, and J. Shang. X-class: Text classification
sociationforComputationalLinguistics,2020. doi:10.18653/v1/2020. with extremely weak supervision. In K. Toutanova, A. Rumshisky,
acl-main.645. URLhttps://www.aclweb.org/anthology/2020.acl-main. L. Zettlemoyer, D. Hakkani-Tür, I. Beltagy, S. Bethard, R. Cotterell,
645. T.Chakraborty,andY.Zhou,editors,Proceedingsofthe2021Confer-
[16] K.McCaffery,B.Nickel,R.Moynihan,J.Hersch,A.Teixeira-Pinto, enceoftheNorthAmericanChapteroftheAssociationforComputa-
L.Irwig,andA.Barratt. Howdifferentterminologyforductalcarci- tionalLinguistics:HumanLanguageTechnologies,NAACL-HLT2021,
nomainsituimpactswomen’sconcernandtreatmentpreferences:aran- Online,June6-11,2021,pages3043–3053.AssociationforComputa-
domisedcomparisonwithinanationalcommunitysurvey. BMJOpen, tionalLinguistics,2021. doi:10.18653/V1/2021.NAACL-MAIN.242.
5(11),2015. ISSN2044-6055. doi:10.1136/bmjopen-2015-008094. URLhttps://doi.org/10.18653/v1/2021.naacl-main.242.
URLhttps://bmjopen.bmj.com/content/5/11/e008094. [31] Z.Yang,D.Yang,C.Dyer,X.He,A.Smola,andE.Hovy. Hierar-
[17] B.Nickel,A.Barratt,T.Copp,R.Moynihan,andK.McCaffery.Words chicalAttentionNetworksforDocumentClassification. InProceed-
do matter: a systematic review on how different terminology for the ings of the 2016 Conference of the North American Chapter of the
sameconditioninfluencesmanagementpreferences. BMJOpen,7(7), Association for Computational Linguistics: Human Language Tech-
2017. ISSN2044-6055. doi:10.1136/bmjopen-2016-014129. URL nologies,pages1480–1489,SanDiego,California,2016.Association
https://bmjopen.bmj.com/content/7/7/e014129. for Computational Linguistics. doi: 10.18653/v1/N16-1174. URL
[18] R.Pappagari,P.Z˙elasko,J.Villalba,Y.Carmiel,andN.Dehak. Hier- http://aclweb.org/anthology/N16-1174.
archicaltransformersforlongdocumentclassification. InIEEEAuto- [32] M.Zaheer,G.Guruganesh,K.A.Dubey,J.Ainslie,C.Alberti,S.On-
maticSpeechRecognitionandUnderstandingWorkshop,ASRU2019, tañón, P. Pham, A. Ravula, Q. Wang, L. Yang, and A. Ahmed. Big
Singapore,December14-18,2019,pages838–844.IEEE,2019. bird:Transformersforlongersequences.InH.Larochelle,M.Ranzato,
[19] H.H.Park,Y.Vyas,andK.Shah. Efficientclassificationoflongdoc- R.Hadsell,M.Balcan,andH.Lin,editors,AdvancesinNeuralInforma-
umentsusingtransformers. InS.Muresan,P.Nakov,andA.Villavi- tionProcessingSystems33:AnnualConferenceonNeuralInformation
cencio,editors,Proceedingsofthe60thAnnualMeetingoftheAsso- ProcessingSystems2020,NeurIPS2020,December6-12,2020,virtual,
ciationforComputationalLinguistics(Volume2:ShortPapers),ACL 2020.
2022, Dublin, Ireland, May 22-27, 2022, pages 702–709. Associa- [33] Z.Zhang,X.Han,Z.Liu,X.Jiang,M.Sun,andQ.Liu. ERNIE:en-
tion for Computational Linguistics, 2022. doi: 10.18653/V1/2022. hanced language representation with informative entities. In A. Ko-
ACL-SHORT.79.URLhttps://doi.org/10.18653/v1/2022.acl-short.79. rhonen, D. R. Traum, and L. Màrquez, editors, Proceedings of the
[20] Y.Qu,Y.Ding,J.Liu,K.Liu,R.Ren,W.X.Zhao,D.Dong,H.Wu, 57thConferenceoftheAssociationforComputationalLinguistics,ACL
andH.Wang. Rocketqa:Anoptimizedtrainingapproachtodensepas- 2019,Florence,Italy,July28-August2,2019,Volume1:LongPapers,
sageretrievalforopen-domainquestionanswering. InProceedingsof pages1441–1451.AssociationforComputationalLinguistics,2019.
the2021ConferenceoftheNorthAmericanChapteroftheAssociation
forComputationalLinguistics:HumanLanguageTechnologies,pages
5835–5847,2021.
[21] N.SadvilkarandM.Neumann. Pysbd:Pragmaticsentenceboundary
disambiguation. CoRR,abs/2010.09657,2020. URLhttps://arxiv.org/
abs/2010.09657.
[22] V.Sanh,L.Debut,J.Chaumond,andT.Wolf. Distilbert,adistilled
version of bert: smaller, faster, cheaper and lighter. arXiv preprint
arXiv:1910.01108,2019.
[23] T.SchickandH.Schütze.It’snotjustsizethatmatters:Smalllanguage
modelsarealsofew-shotlearners. InProceedingsofthe2021Confer-
enceoftheNorthAmericanChapteroftheAssociationforComputa-
tionalLinguistics:HumanLanguageTechnologies,pages2339–2352,
2021.
[24] R.Sheik,K.SivaSundara,andS.J.Nirmala.Neuraldataaugmentation
forlegaloverrulingtask:Smalldeeplearningmodelsvs.largelanguage
models.NeuralProcessingLetters,56(2):121,2024.
[25] Y. Tay, M. Dehghani, S. Abnar, Y. Shen, D. Bahri, P. Pham, J. Rao,
L.Yang,S.Ruder,andD.Metzler. Longrangearena:Abenchmark
forefficienttransformers. In9thInternationalConferenceonLearn-
ingRepresentations,ICLR2021,VirtualEvent,Austria,May3-7,2021.
OpenReview.net,2021.
[26] Y.Tay,M.Dehghani,D.Bahri,andD.Metzler.EfficientTransformers:
ASurvey. ACMComputingSurveys,55:1–28,July2023. ISSN0360-
0300,1557-7341. doi:10.1145/3530811. URLhttps://dl.acm.org/doi/
10.1145/3530811.
[27] H.Touvron,L.Martin,K.Stone,P.Albert,A.Almahairi,Y.Babaei,
N.Bashlykov,S.Batra,P.Bhargava,S.Bhosale,D.Bikel,L.Blecher,
C.Canton-Ferrer,M.Chen,G.Cucurull,D.Esiobu,J.Fernandes,J.Fu,
W.Fu,B.Fuller,C.Gao,V.Goswami,N.Goyal,A.Hartshorn,S.Hos-
seini,R.Hou,H.Inan,M.Kardas,V.Kerkez,M.Khabsa,I.Kloumann,
A.Korenev,P.S.Koura,M.Lachaux,T.Lavril,J.Lee,D.Liskovich,
Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog,
Y.Nie,A.Poulton,J.Reizenstein,R.Rungta,K.Saladi,A.Schelten,
R.Silva,E.M.Smith,R.Subramanian,X.E.Tan,B.Tang,R.Taylor,
A.Williams,J.X.Kuan,P.Xu,Z.Yan,I.Zarov,Y.Zhang,A.Fan,
M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and
T. Scialom. Llama 2: Open foundation and fine-tuned chat models.
CoRR,abs/2307.09288,2023.doi:10.48550/ARXIV.2307.09288.URLSupplementaryMaterial:listoftargettermsusedin ∗ folfiri
theFrenchdataset ∗ folfirinox
• Targettermlistusedforthelateralityclassificationtask: ∗ folfoxiri
– "Ours"containstheaforementionedvocabularyandthefollow-
– The"reducedvocabulary"listonlycontains:
ingadditionalterms:
∗ left
∗ oxaliplatin
∗ right
∗ cetuximab
– "Ours"containstheaforementionedvocabularyandthefollow-
∗ erbitux
ingadditionalterms:
∗ bevacizumab
∗ wordsderivedfrom"caecum":caecum,cecal
∗ xelox
∗ words derived from "sigmoid": sigmoid, rectosigmoid, sig-
∗ capox
moidectomy,etc.
∗ capecitabine
∗ iliac
∗ 5-FU
∗ bauhin
∗ ascending
∗ descending
∗ wordsderivedfrom"ileo":ileo,ileocecal,ileocolic,etc.
∗ cmfromtheanalmargin
– The"augmentedvocabulary"listcontainstheaforementioned
vocabularyandthefollowingadditionalterms:
∗ transverse
∗ splenic
∗ hepatic
∗ pelvic
∗ lumbar
• Targettermlistusedforthesurgeryclassificationtask:
– The"reducedvocabulary"listonlycontains:
∗ wordsderivedfrom"surgery":surgery,surgical
∗ wordsderivedfrom"operation":operation,operated,operat-
ing
∗ words ending with "ectomy": colectomy, sigmoidectomy,
hepatectomy,metastasectomy,etc.
– "Ours"containstheaforementionedvocabularyandthefollow-
ingadditionalterms:
∗ wordsderivedfrom"resection":"resection","resected"
∗ exenteration
∗ cytoreduction
– The"augmentedvocabulary"listcontainstheaforementioned
vocabularyandthefollowingadditionalterms:
∗ wordsendingwith"otomy":laparotomy,etc
∗ wordsendingwith"ostomy":colostomy,ileostomy,etc.
∗ excision
∗ ablation
∗ exeresis
• Targettermlistusedforthechemotherapyclassificationtask:
– The"furtherreducedvocabulary"listonlycontains:
∗ chemotherapy
– The "further reduced vocabulary" list contains the aforemen-
tionedvocabularyandthefollowingadditionalterms:
∗ folfox