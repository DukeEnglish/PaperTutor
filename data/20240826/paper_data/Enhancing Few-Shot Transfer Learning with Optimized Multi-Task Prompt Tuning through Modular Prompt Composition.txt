Enhancing Few-Shot Transfer Learning with
Optimized Multi-Task Prompt Tuning through
Modular Prompt Composition
Ahmad Pouramini1* and Hesham Faili1†
1*School of Electrical and Computer Engineering, College of
Engineering, University of Tehran, North Kargar Street, Tehran,
515-14395, Tehran, Iran.
*Corresponding author(s). E-mail(s): ahmad.pouramini@ut.ac.ir;
Contributing authors: hfaili@ut.ac.ir;
†These authors contributed equally to this work.
Abstract
In recent years, multi-task prompt tuning has garnered considerable attention
for its inherent modularity and potential to enhance parameter-efficient transfer
learningacrossdiversetasks.Thispaperaimstoanalyzeandimprovetheperfor-
mance of multiple tasks by facilitating the transfer of knowledge between their
corresponding prompts in a multi-task setting. Our proposed approach decom-
poses the prompt for each target task into a combination of shared prompts
(source prompts) and a task-specific prompt (private prompt). During train-
ing, the source prompts undergo fine-tuning and are integrated with the private
prompt to drive the target prompt for each task. We present and compare mul-
tiple methods for combining source prompts to construct the target prompt,
analyzing the roles of both source and private prompts within each method. We
investigate their contributions to task performance and offer flexible, adjustable
configurations based on these insights to optimize performance. Our empirical
findingsclearlyshowcaseimprovementsinaccuracyandrobustnesscomparedto
theconventionalpracticeofprompttuningandrelatedworks.Notably,ourresults
substantiallyoutperformothermethodsinthefieldinfew-shotsettings,demon-
strating superior performance in various tasks across GLUE benchmark, among
othertasks.Thisachievementisattainedwithasignificantlyreducedamountof
training data, making our method a promising one for few-shot settings. ∗
∗
We plan to release our code publicly on GitHub upon request by the reviewer and for the
benefit of the broader public audience.
1
4202
guA
32
]IA.sc[
1v72231.8042:viXraKeywords:NaturalLanguageProcessing,Pre-trainedLanguageModels,
Prompt-Tuning,TransferLearning
1 Introduction
Presently, transfer learning has become the dominant paradigm of natural language
processing [15]. The prevalent technique involves pretraining a model on an extensive
corpus of raw data and subsequently fine-tuning it for specific tasks using a limited
dataset. Despite its success, this approach suffers from a series of limitations, such as
negative interference [14] and catastrophic forgetting [6].
Asaremedytoaddressthesechallenges,theconceptofmodulardeeplearninghas
gained traction [12]. Within this framework, computation units are designed as self-
contained, parameter-efficient modules. These modules are selectively engaged and
then combined to generate the ultimate output. For instance, incorporating adapter
modules, task-specific layers [5], or task-specific embeddings allow for the model to
focus more on certain tasks or to better separate the representations for different
tasks [2]. The main advantages of a modular neural architecture are positive transfer,
compositionality, and parameter efficiency [12].
Prompt-Tuning is a parameter efficient method for fine-tuning a pretrained lan-
guagemodelondownstreamtasks[7,9,10].Itcanalsoserveasamethodformodular
design to specialize different prompts for different tasks [12]. From this perspective,
task-specific prompt tokens, after encoded and optimized by the model’s embedding
layer,correspondstomodularparametersthatelicitthedesiredbehaviour.Theknowl-
edge distilled within these soft prompts can subsequently be transferred to related
tasks or combined to provide the necessary knowledge for tackling new tasks [17].
Yet, the effective utilization of this extensive cross-task knowledge within a multitask
learning framework has remained elusive.
In this paper, we explore task transferability within the context of multi-task
prompt tuning. We introduce a specialized framework called ComPT that combines
multiple shared source prompts and task-specific prompts to drive the target prompt
for each task within a multi-task learning setting. Our objective is to enhance overall
performancebyfacilitatingknowledgeexchangebetweenthesetasks.Weproposesev-
eral methods for combining these prompts and evaluate their effectiveness in solving
diverse tasks.
In the following sections, we outline the specifics of our approach and describe our
proposedmethods.Subsequently,wepresent ourexperimentalresults andconduct an
in-depthanalysisoftheresultsandcontributingfactorsaffectingperformance.Finally,
wereviewrelatedworks,compareourmethodwithexistingapproaches,andconclude
the paper.
2 Method
Figure 1 illustrates the ComPT approach. We train N tasks in multi-task settings.
These tasks use M source prompts, denoted as P , to formulate a target prompt
s
2Fig. 1 OverviewofComPT.
P for each distinct target task t. In addition to source prompts, a private prompt,
t
denoted as P is dedicated for each task. The final target prompt is obtained by
u
combining the source prompts and then composing them with the private prompt.
This is achieved by activating and combining the shared source prompts through a
trainable attention module, which assigns the weights w from the source prompt s
ts
to the target task t. Training is conducted using multi-task learning across all tasks.
Learningasharedpromptspaceinmulti-tasksettingscanposepracticalchallenges,as
itnecessitateslearningcommonalitiesacrossvarioussourcetaskswhilesimultaneously
minimizing interference. The source and private prompts are initially randomized.
However, in practice, prompts can also be initialized using prompts obtained from
a pre-training stage. In this study, we opt to simultaneously learn the prompts and
the attention weights, which resulted in high performance, even when compared to
approaches involving prompt initialization. In the following sections, we describe the
details of our approach.
2.1 Prompt Training
Before delving into further details of our method, let us first review the process of
prompt tuning. Prompt tuning typically entails optimizing the input prompt of a
language model LM to enhance its performance on a specific task.
We begin with a template T = x,p ,...,p , comprising the input sequence x =
1 m
[x ,...,x ] and prompt tokens p . The placement of prompt tokens can occur either
1 l i
beforeoraftertheinputtokens.Inourexperiments,wepositionedtheprompttokens
after the input tokens, yielding slightly better results. Prompt tuning then entails
mapping this template to a set of trainable prompt embeddings as follows:
{emb(x),h ,...h } (1)
0 m
We denote a soft prompt as P = [h ,...,h ] ∈ Rm×d, where m is the prompt
1 m
length, and d is the dimensionality of the underlying language model LM. This soft
prompt can be considered as a continuous representation of the sequence of prompt
tokens that is used to condition the language model.
3To optimize the soft prompt, we use differential optimization to maximize the
conditional probability of the language model’s output y given the input sequence x
and the prompt P:
maxp (y |[P;x]) (2)
θ
P
During training, only the prompt embeddings are updated, while the rest of the
model remains frozen. To stabilize the training of the soft prompts and enhance their
capacity to store and adapt to different tasks, we employed a multilayer neural net-
work with one hidden layer and a non-linear activation function. We refer to this
neuralnetworkasthepromptencoderandprovidemoredetailsaboutitinsubsequent
sections.
h =Encoder(h ) (3)
i i
Thepromptencoderisappliedtotheembeddingofeachprompttokenindividually
and modifies it to a new embedding h , which is fed to the language model LM.
i
2.2 Target Prompt Decomposition
In contrast to vanilla prompt tuning, the prompts in our approach result from com-
biningmultiplesourceprompts.Inotherwords,wedecomposethetargetpromptinto
severalsourcepromptsthataresharedamongtasks.Thebackpropagationmechanism
updatesthesesourcepromptsandparametersfortheircombinationtoobtainanopti-
maltargetprompt.Inthecontextofaspecifictargettask,thetargetpromptemerges
as a fusion of the shared source prompts along with a trainable task-specific prompt,
which we denote as the private prompt.
Precisely, let S represent the set of shared source prompts, and let P denote the
u
private prompt corresponding to the target task t. In this context, the computation
of the target prompt P is as follows:
t
(cid:32) (cid:33)
(cid:77)
P =P ◦ w ·P (4)
t u ts s
s∈S
The weights w are determined by an attention mechanism implemented through
ts
an attention module R, which dynamically assigns weights to the shared prompts.
This mechanism controls the contribution of each source prompt to a specific task
basedonitsrelevancetothetargettask,whileaprivatepromptcapturestask-specific
nuances. If none of the shared source prompts are useful for a target task, the target
prompt primarily relies on the private prompt for its formation.
The composition of the source prompts can be achieved using various functions,
with ⊕ denoting the composition function. In this paper, we explored summation and
4concatenationaspotentialcompositionfunctionsforthecombingthesourceprompts.
The resulting combination is then composed with the private prompt using the ◦
function. For composing the source prompts into the private prompt, we considered
both summation and multiplication. This framework is generalizable, allowing for the
exploration of various methods. Here, we cover three specific cases as follows, each
with their corresponding names listed after the formula:
 (cid:0)(cid:80) (cid:1)
P + w ·P , if ◦=mul,⊕=sum:SSUM
 u s∈S ts s
(cid:0)(cid:80) (cid:1)
P = P ∗ w ·P , if ◦=mul,⊕=sum:MSUM (5)
t u s∈S ts s
P
∗(cat w ·P ), if ◦=mul,⊕=concat:MCAT
u s∈S ts s
In the case of concatenation (MCAT), the length of the target prompt is equal to
the sum of the lengths of the source prompts. For the other cases, the source prompts
and the target prompt have equal lengths. The multiplication of source prompts to
the target prompt of each task can model the projection of the source prompts onto
the prompt for each task. Meanwhile, the summation of the target prompt to the
combination of source prompts can serve to complement and augment the source
prompts. We will delve deeper into the functions and implications of these operations
in the Discussion section 5.
To ensure that the weights associated with each shared source accurately reflect
theirrelevancetoatargettaskduringthetrainingprocess,wenormalizetheseweights
using the softmax function. After calculating the target prompt P , the contributing
t
elements are updated by optimizing to the following objective function.
maxp (y |[P ;x]) (6)
θ t
Pt,R
Inmulti-tasklearning,thelossforeachprivatepromptP iscomputedonlywhen
u
theinputsamplecorrespondingtothetaskisfedtothemodel.Incontrast,theshared
source prompt P is updated at each iteration for all input samples. As a result, the
s
attention weights for the shared prompt are adjusted based on the updated content
of these source prompts. This dynamic adjustment can facilitate both positive and
negativeknowledgetransferamongtasks,whichwillbeexploredinmoredetailinthe
Discussion section 5.
2.3 Source-Target Attentions
The attention module R governs the influence of each source prompt on the target
prompt by computing attention weights w ,...,w for the shared source prompts.
1t mt
Theseweightscanbelearnedasfreeparametersorthroughaprobabilitydistribution.
Specifically, weemployedthe RelaxedBernoullidistribution to formulatethese atten-
tion weights, as it demonstrated efficacy and learning stability in our experimental
evaluations compared to using free parameters.
5TheRelaxedBernoullidistributionrepresentsacontinuousrelaxationofthetradi-
tionalBernoullidistribution,facilitatingstochasticgradientcomputationthroughthe
reparameterization trick [11].
The Relaxed Bernoulli distribution is defined as follows:
(cid:34) (cid:35)
σ(w )u 1/τ
wˆ =σ log i,j where u∼Uniform(0,1) (7)
i,j (1−σ(w ))(1−u)
i,j
Here, a temperature parameter τ ∈ (0,∞) controls the degree of approximation,
w are the learnable logits, and wˆ are the resulting sampled attention weights from
ij ij
the distribution used during training. At inference time, the learned weights w are
ij
utilized.
Fast and Slow Learning. Learning the stochastic attention module may intro-
ducevariouschallenges,includingissuesliketraininginstabilityandoverfitting[1,12].
Moreover, balancing the learning of source and private prompts to achieve optimal
performance poses another challenge. To address these challenges, we prioritize rapid
learning for the attention module by employing a higher learning rate, enabling it
to quickly adapt by reusing existing prompts for the current task. Additionally, we
prioritize the learning of source prompts over private prompts to facilitate greater
information sharing through the source prompts. We will further analyze the impact
of these parameters in the Discussion section 5.
2.3.1 Inference
Duringinference,weloadallsharedsourcepromptsandprivateprompts,aswellasthe
shared attention module only once. The target prompt for a given task is derived by
applying Equation 5. Unlike during training, at inference time, the learned attention
module, which includes both positive and negative weights (logits of the Relaxed
Bernoulli distribution), is directly utilized without the need for additional sampling.
By applying softmax over these learned scores, probability distributions are obtained.
After obtaining the target prompt, it is concatenated to the input instance xand
fed to the model. The model generates the output y by utilizing its internal attention
mechanism to consider both the learned target prompt and the input data.
In the following sections, we will present the results obtained by applying each
method to various tasks and provide a comprehensive comparison among them.
3 Experiments
3.1 Tasks
We assess the performance of proposed methods across various tasks, including eight
tasksfromtheGLUEbenchmark[18]andsixtasksfromotherdatasets.Thesedatasets
are publicly available on Hugging Face’s model hub.1
Our evaluation involves two distinct experiments. In Experiment 1, the GLUE
tasksencompassMNLI,QNLI,RTE(NLI),COLA(grammaticalacceptability),SST2
1Retrievedfromhttps://huggingface.co/datasets/glue
6(sentiment analysis), STSB (semantic similarity), MRPC, and QQP (paraphrase
detection).
ForExperiment2,weincludeMultiNLI,SciTail(NLI),IMDB,Yelp-Polarity(sen-
timent analysis), PAWS, and MRPC (paraphrase detection). We collectively refer to
this group of tasks as SET2 tasks.
3.2 Implementation Details
We utilize the T5-base language model [13] as our base model, which is commonly
employedinrelatedworks.Thepromptlengthisaneffectiveparameter,andweinves-
tigated shorter lengths (20 tokens), which proved more effective compared to longer
prompts. Specifically, for MCAT, shorter lengths such as 10 tokens proved to be more
stable and efficient.
Inthequestforanoptimalpromptencoder,weexperimentedwithdifferentarchi-
tecturesandactivationfunctions.Initially,wetestedanLSTM-basedencoder,similar
to [10], but found superior performance with an MLP-based encoder. Further explo-
ration led us to conclude that the GELU activation function yielded slightly better
results compared to RelU and SiLU. Our chosen prompt encoder, a two-layer MLP,
generates prompt embeddings of size d corresponding to each token. This MLP com-
prisesafullyconnectedlayerwithahiddensizeofd(thelanguagemodel’sembedding
size), followed by a GELU activation function, and another fully connected layer
responsible for producing the final prompt embedding.
FortheRelaxedBernoullidistributiongenerator,westartedwithaninitialtemper-
ature of 5 and linearny reduced it using annealing to 1e-3 during entire training. The
attention module was trained with a learning rate of 0.1, while the prompt encoders
for source prompts were trained with a learning rate of 0.05 and the prompt encoders
for the private prompts were trained with a learning rate of 0.02.
Table 1 ComparativeperformanceonchosentasksfromGLUEdatasetsusingproposedmethods
infew-shotsettings.Theperformanceiscomputedacross3distinctseeds.
method k-shot Avg. cola mnli mrpc qnli qqp rte sst2 stsb
8 63.35 35.6 35.3 75.2 72.4 76.5 47.7 81.3 82.9
PT 16 63.29 34.1 28.7 77.9 60.3 83.1 48.5 89.4 84.4
32 70.15 71.5 45.3 84.3 52.2 79.5 48.7 93.1 86.5
8 74.32 67.0 58.8 80.3 87.7 85.2 57.3 72.9 85.3
MCAT 16 78.52 65.7 70.3 84.2 87.1 86.7 59.7 87.7 86.8
32 80.40 68.9 71.6 83.8 90.9 87.5 64.0 90.2 86.2
8 76.03 68.9 63.5 82.2 87.5 87.3 59.2 74.4 85.3
MSUM 16 78.43 67.9 69.5 84.0 86.6 87.7 55.6 88.4 87.8
32 80.35 67.7 75.4 83.3 89.9 87.0 62.4 90.1 87.0
8 74.38 68.0 61.7 85.1 74.9 86.9 60.9 73.3 84.2
SSUM 16 79.47 68.3 72.5 85.8 82.1 87.5 65.5 88.3 85.8
32 81.04 71.6 78.1 83.3 90.4 83.3 65.4 88.4 87.7
7Table 2 ComparativeperformanceontasksinSET2usingproposedmethodsinfew-shotsettings.
Theperformanceiscomputedacross3distinctseeds.
method k-shot Avg. imdb mrpc multinli paws scitail yelp-polarity
8 64.4 84.9 83.0 40.7 34.8 59.5 83.6
PT 16 65.4 87.3 83.8 39.1 35.2 59.6 87.1
32 66.5 88.4 80.0 25.3 52.4 59.6 93.1
8 70.6 76.8 81.4 59.1 53.9 68.1 84.6
MCAT 16 72.6 78.3 83.5 66.5 52.9 65.8 88.5
32 77.1 79.3 83.5 71.9 62.9 76.7 88.3
8 71.2 80.5 81.0 59.5 54.8 68.5 82.7
MSUM 16 74.0 74.9 79.3 69.1 60.0 71.5 89.2
32 77.3 80.7 83.0 73.3 63.6 74.1 89.1
8 71.4 77.7 85.1 61.1 54.7 64.2 85.5
SSUM 16 70.0 71.8 85.0 62.5 55.3 62.3 82.8
32 77.3 79.7 83.0 70.9 67.3 75.3 87.4
4 Results
In the following sections, we will present the results of the methods, and then provide
ananalysisoftheimplicationsarisingfromtheinvolvingfactorsinDiscussionsection.
4.1 Comparison of Proposed Methods
In order to evaluate the effectiveness of the methods discussed in Section 2, we per-
formed a series of experiments in a few-shot learning scenario. For each task, we
utilized sample sizes of 8, 16, 32 for few-shot settings. To ensure the reliability of the
results, each experiment was repeated three times with different random seeds.
Table1and2presentaverageperformanceoftheproposedmethodsacrossalltasks
intwoexperiments,offeringvaluableinsightsintotheireffectiveness.Inthefollowing,
we will conduct a comparative study of these methods.
4.1.1 Few-shot Efficiency
A consistent finding across all scenarios is that the proposed methods substantially
outperform prompt tuning in few-shot settings. This highlights the effectiveness of
incorporating source prompts and training them in multi-task settings. The adoption
ofmulti-tasksettingsfacilitatestheexchangeofknowledgeamongthesetasks,further
enhancing overall performance. This characteristic proves advantageous in few-shot
learning contexts where data availability is constrained.
According to the tables, the major enhancement compared to prompt-tuning is
observed for tasks such as QNLI, RTE, and MNLI in GLUE tasks, as well as PAWS,
MNLI, and SciTail in SET2 tasks.
8Table 3 Comparisonofproposedmethodson5000DataSamplesforGLUETasks
label Avg. nsp weights cola mnli mrpc qnli qqp rte sst2 stsb
MSUM 85.96 10 learned 81.0 82.5 86.5 92.3 90.0 73.7 92.5 89.2
SSUM 86.01 10 learned 81.8 82.1 86.0 92.8 89.6 73.3 93.2 89.3
MCAT 85.89 3 learned 80.5 83.1 87.0 92.7 89.6 72.2 93.0 89.0
Table 4 Comparisonofproposedmethodson5000DataSamplesforSET2Tasks
label Avg. nsp weights imdb mrpc multinli paws scitail yelp
MSUM 88.35 9 learned 85.8 86.0 83.5 87.9 94.1 92.8
SSUM 88.53 10 learned 86.4 86.8 83.7 88.8 92.7 92.8
MCAT 88.23 3 const 86.0 87.2 82.9 88.2 91.9 93.2
4.1.2 More Training data
We have expanded our experiments to include a larger training dataset. The results
of our proposed methods, utilizing 5000 samples for each task, are displayed in Table
3 for GLUE tasks and Table 4 for tasks in SET2.
Whileourdesignedmethodsdemonstratesignificantimprovementsinperformance
in few-shot settings, they also achieve state-of-the-art results with a larger training
dataset, outperforming the overall performance compared to the related works, which
are detailed in the Related Works section 6.
5 Discussion
In this section, we explore the pivotal roles played by source and private prompts
in our proposed methods, emphasizing the control and balance required to achieve
desirable results. We delve into how these prompts contribute to task performance
and knowledge transfer, examining strategies to optimize their impact for enhanced
model performance.
5.1 Role of Source Prompts
5.1.1 Number of Source Prompts
Figure 2 illustrates the impact of the number of source prompts on the performance
ofproposedmethods.Itisevidentthatfewersourcepromptsgenerallyleadtogreater
effectiveness across all methods. However, MSUM and SSUM can perform well even
with higher numbers of source prompts. MSUM demonstrates greater robustness to
variations in the number of source prompts and training data, while MCAT?s per-
formance consistently declines with an increasing number of source prompts. Unlike
MSUM and SSUM, which maintain a fixed prompt length, MCAT experiences an
increase in target prompt length, potentially adversely affecting its performance.
Additionally, in cases of MSUM and SSUM, increasing the number of source
promptstocertainvaluesdecreasesthevarianceinperformanceacrossdifferenttrain-
ingdatainfew-shotsettings,asevidencedbythestandarddeviationinmultipleruns,
as shown in Figure 2.
9Fig. 2 On the left, the average performance of methods (MSUM, SSUM, MCAT) on GLUE tasks
(top) and SET2 tasks (bottom) trained with 8 samples using three random seeds across various
numbersofsourceprompts.Ontheright,thecorrespondingstandarddeviationofthreeexperiments
ateachdatapoint.
The optimal number of source prompts depends on task characteristics. As
observed in Figure 2, for GLUE tasks, fewer or even a single source prompt performs
better. Conversely, tasks in SET2 require two or three source prompts to perform
better.Conflictingtasksgenerallyrequiremoresourcepromptstoeffectivelyseparate
conflictinginformation,whereastaskswithmoresimilaritiesbenefitfromacondensed
number of source prompts, facilitating increased information sharing. We will delve
deeper into this analysis in the next section.
Overall, MSUM facilitates the utilization of more source prompts to scale the
number of tasks and training data without compromising performance.
5.1.2 Contribution of Source Prompts
Toassesstheimpactofeachsourcepromptonthefinalresults,weisolatedindividual
promptsandmeasuredtheirperformanceonatestsetof100samples.Specifically,we
retained one specific source prompt and masked the others, then evaluated the test
data.Inthefollowingsections,wewillanalyzetheresultsobtainedfromthistechnique
for each of the proposed methods.
SSUM
Figure 3 illustrates the scores associated with each source prompt for the SSUM
method, employing one, two, and three source prompts for two experiments. In these
figures,eachcolumnrepresentstheresultswhenonlythecorrespondingsourceprompt
is retained, while the private prompt and other source prompts are masked. The col-
umn labeled all src shows the results when all source prompts are retained, while
10Fig.3 PerformancecomparisonoftheSSUMmethodonGLUEtasks(top)andSET2tasks(bottom)
trained with 16 samples using one, two, and three source prompts. Each column represents the
performancewhenretainingaspecificpromptandignoringtheothers.The”all src”columnignores
the private prompt, while ”total” includes all prompts. The last row indicates the average score of
eachcolumn.
the private prompt is ignored. The private column displays results when only the
private prompt is retained. On the right side of each figure, the results for including
all source prompts and the private prompt are shown labeled as total.
As shown, certain tasks like SST-2 and CoLA mainly rely on the private prompt.
These tasks exhibit the least similarity with the rest of the tasks based on the input
and output. According to Table 1, these tasks also derive the least benefit from the
proposed multi-tasking prompt tuning.
On the other hand, tasks such as STS-B, QNLI, and MRPC primarily rely on the
source prompts, while tasks like QQP, RTE, and MNLI use a combination of source
and private prompts. It is noteworthy that the final score for these tasks is higher
when employing both source and private prompts together, compared to using either
source or private prompts alone, highlighting the crucial role of source prompts in
enhancing performance. Similarly, according to the results from Table 1, these tasks
benefit the most from multi-task prompt tuning.
Wetheninvestigatedthepredictionsmadeusingisolatedsourceorprivateprompts.
Table5showsthepredictionsforMNLIwhenusingasinglesourceprompt.Asevident
fromthetable,thesourcepromptmainlypredictstheentailmentclass,alabelshared
11Table 5 PredictionsfortheMNLItaskusingthesourceprompt,theprivateprompt,andtheir
combinationintheSSUMmethod.
method preds contradiction entailment neutral total acc.
contradiction 1 0 0
source entailment 27 29 36 0.30
not-entailment 4 0 2
precision 3% 100% nan
contradiction 32 24 36
private neutral 0 5 2 0.34
precision 100% nan 5%
contradiction 19 1 4
combined entailment 2 10 2 0.62
neutral 11 18 32
precision 59% 34% 84%
Table 6 PredictionsfortheRTEtaskusingthesourceprompt,theprivateprompt,andtheir
combinationintheSSUMmethod.
method preds entailment not-entailment total acc.
entailment 6 2
source not-entailment 43 48 0.54
precision 12% 96%
private entailment 49 50 0.48
precision 100% nan
entailment 43 27
combined not-entailment 6 23 0.66
precision 88% 46%
with the QNLI and RTE tasks, which are the dominant tasks in the source part (See
Figure 3-1).
In contrast, the private prompt mainly predicts the contradiction class. This
illustrates the complementary roles of the source and private prompts. Notably, the
combinedpromptachieveshigherscoresthanbothsourceandprivatepromptsbypre-
dictingallthreeclasses.AsimilarbutoppositetrendcanbeobservedfortheRTEtask
(refertoTable6),wherethesourcepartpredominantlypredictsnot-entailmentand
the private part predicts entailment. This contrasting pattern aids in distinguishing
between MNLI and RTE on the shared source part.
While the labels for STS-B, MRPC, and QNLI differ, these tasks show a stronger
relianceonsourceprompts,whichmaybeduetotheirinherentrelationships.Eachtask
involvesanalyzingtwosentences:STS-Bassessessentencesimilarity,QNLIdetermines
if one sentence entails another, and MRPC identifies paraphrases.
Toverifythisrelationship,weconductedaseriesofexperimentswhereweevaluated
the test data of one task using prompts generated for another task. Figure A1 shows
theresultsfortasksQNLIandSTSB,aswellasRTEandSTSB.Thepromptsderived
for STSB and QNLI, which mainly rely on the source part, can effectively classify the
12othertaskbyconvertingthelabels.It’snoteworthythat,basedonthefigure,although
QNLI is similar to RTE, its correlation with STSB is lower. Generally, according to
the scores, QNLI and STSB are relatively easier tasks to solve.
While QQP is similar to QNLI, it exhibits conflict with QNLI as the main predic-
tion for this task is not-entailment on the source part (refer to Table A1). However,
similar to RTE, it uses the private prompt to compensate for the source prompt by
predicting mainly the duplicate class. In other words, the not-entailment on the
source part acts more like the not-duplicate class. The final score is higher than
when employing either the source or private prompt alone, implying an implicit com-
bination and conversion to predict the proper classes. This again demonstrates how
QQP can benefit from QNLI, despite the tasks having distinct labels.
Fig.4 PerformancecomparisonoftheMSUMmethod(top)andMCATmethod(bottom)onGLUE
taskstrainedwith16samplesusingone,two,andthreesourceprompts.Eachcolumnrepresentsthe
performance when retaining a specific source prompt and ignoring the other source prompts, with
the”total”columnincludesallprompts.Thelastrowindicatestheaveragescoreofeachcolumn.
13Fig.5 TheaverageperformanceofproposedmethodsusingconstantandvariableweightsonGLUE
tasksacrossvariousnumberoftrainingdata(top)andvariousnumbersofsourceprompts(bottom)
using16samples.
MSUM
Figure 4 shows the results for MSUM method. Here, ignoring either private prompt
or source prompt lead to zero performance. So, each columns shows the result of the
corresponding source prompt combined with the private prompt. As shown, the dif-
ferences in individual source prompts? performances within MSUM are consistently
minimal. It is evident that in MSUM, all source prompts contribute similarly to the
final performance. However, minor discrepancies among different source prompts sug-
gest that certain prompts excel in predicting specific labels or resolving conflicts.
Overall, the performance achieved by combining all source prompts with the private
prompt surpasses that of individual prompts alone. This observation underscores the
limited adverse effect of increasing the number of source prompts on the performance
and robustness of MSUM.
MCAT
Figure 4 (bottom) illustrates the results for the MCAT method. In this approach, we
isolatedasourcepromptbymaskingitusingthemodel’sbuilt-inattentionmechanism.
Unlike MSUM, the results obtained from MCAT reveal that different source prompts
canplaydistinctandcomplementaryrolesinshapingthefinalscore.Specifically,tasks
like RTE and MNLI rely on different source prompts, whereas similar tasks such as
QNLIandSTSBsharecommonsourceparts.However,inMCAT,thefinalscoretends
to be closer to the maximum achievable scores with these prompts, which could be
attributed to the model’s built-in attention mechanism effectively adjusting over the
source parts.
5.1.3 The Effect of Learned Weights
The question arises: what would happen if we omit the attention module and assign
constant or uniform attention to all source prompts? Figure 5 compares the results
14Fig. 6 Analysis of individual prompt scores for tasks in SET2 with constant weights (left) and
variableweights(right),alongsidecorrespondinglearnedweights.
obtainedfromemployingconstantandvariableweightslearnedbytheattentionmod-
ule.Notably,thenumberofsourcepromptswasfixedat3forMSUMand2forMCAT
and SSUM, which yielded the best results in experiments. Across all methods, con-
stant weights perform better when data is limited. However, as the amount of data
increases, the learned weights achieve higher scores. This suggests that in few-shot
settings, the available data may not be sufficient for effectively adjusting the weights,
but with more data, the learned weights can lead to improved results.
We also investigated the role of weights when the number of source prompts
increases.Wehypothesizedthatweightscouldhelpspecializedifferentsourceprompts
to resolve conflicting tasks. As observed from Figure 5, using learned weights is effec-
tiveforSSUMbutlesseffectiveforMCATasthenumberofsourcepromptsincreases.
ForMSUM,bothlearnedandconstantweightscloselytrackeachotheracrossdifferent
numbers of source prompts.
InMCAT,sincesourcepromptsareconcatenated,thein-builtattentionmechanism
of the model can adjust attention accordingly, making variable weights less effective.
Conversely,inSSUM,theweightedpromptsareaveragedandcombinedwithaprivate
prompt to shape the target prompt, allowing weights to activate and emphasize each
source prompt effectively.
To demonstrate the effectiveness of weights, we analyzed the impact of isolated
sourceprompts.Figure6comparestheresultsofemployingtwosourcepromptsacross
SET2 tasks when the weights are constant and learned.
As observed, using constant weights leads to similar contributions from all source
prompts. However, using variable weights allows for the separation of conflicting task
groupssuchasMRPCandPAWS(Paraphrasing),andYelp-PolarityandIMDB(Senti-
mentAnalysis),resultinginimprovedperformanceonthesetasks.Thelearnedweights,
represented as logits used for the RelaxedBernoulli distribution, were also shown in
the figure, demonstrating how this separation occurs.
5.2 Balancing the Impact of Source and Private Prompts
To achieve better results, we can balance the impact of source and private prompts
byemployingdifferentlearningratesforeachpart.Figure7showstheperformanceof
the proposed methods across different learning rates of the private prompt at 30 and
15Fig. 7 Performancecomparisonofproposedmethodsacrossdifferentlearningratesfortheprivate
prompt on GLUE tasks using 16 samples (top), 50 epochs (top-left), and 30 epochs (top-right),
alongsidethecorrespondingscoresforindividualsourceandprivatepromptsatspecificlearningrates
using the SSUM method (middle) at 50 epochs, and the corresponding scores of source prompts in
theMSUMmethod(bottom)at30epochs.
50 epochs, with the learning rate of the source prompt fixed at 0.05. Additionally, the
figure displays the scores of isolated source and private prompts at different learning
rates for SSUM and MSUM methods.
Adjusting the learning rates for the private prompt relative to the source prompt
can influence the attention towards or away from the source prompts and impact
the final score. Higher learning rates for the source prompt compared to the private
prompt typically result in better performance, emphasizing the importance of source
prompts. This effect is especially noticeable in SSUM and MCAT methods.
As demonstrated earlier, tasks like MNLI, RTE, and QQP derive the most benefit
from the complementary roles of source and private prompts. The individual prompt
16scores highlight that when attention is concentrated on source prompts, the overall
performanceofthesetasks,asindicatedbythesourcepromptandtotalscore,ishigher.
RTE benefits significantly from the shared source space, particularly when the
private learning rate is lower (e.g., plr=0.01). However, in terms of epochs, the source
prompts in this scenario require more time to adjust the weights and demonstrate
their effectiveness.
TheperformanceoftheMSUMmethodtendstoimprovewheneitherthelearning
rate of the source prompt or the private prompt is higher. Conversely, it shows lower
performance when the learning rates of the source and private prompts are equal. To
demonstratethiseffect,thebottomheatmapinFigure7displaysthescoresofisolated
sourcepromptscombinedwiththeprivatepromptat30epochswithdifferentlearning
rates.Asobserved,whenthelearningratesareequal,thescoresaresignificantlylower.
The results improve at 50 epochs (refer to Figure A2). However, even at that point,
the score with a lower learning rate of the private prompt, particularly for tasks like
RTE and MNLI, remains higher compared to other cases. It’s important to note that
higher learning rates for private prompts primarily enhance the score of tasks such as
SST-2, which rely mainly on the private prompt.
Table 7 ComparisonofproposedMethodsonSET2tasksbyincludingMNLIasadditionaltask
label All include imdb mrpc multinli paws scitail yelp
72.7 mnli 77.0 88.0 65.0 49.0 70.0 87.0
SSUM
70.5 — 75.0 89.0 61.0 51.0 64.0 83.0
75.0 mnli 78.0 89.0 77.0 47.0 72.0 87.0
MSUM
72.3 — 75.0 88.0 67.0 47.0 66.0 91.0
74.8 mnli 83.0 88.0 72.0 53.0 68.0 85.0
MCAT
71.3 — 81.0 88.0 58.0 47.0 67.0 87.0
5.3 Modularity
In our approach, the selection of tasks for multi-task prompt tuning can signifi-
cantlyimpactperformance.Additionally,ourmethodallowstheinitializationofsource
prompts with pre-trained prompts obtained from a training stage using individual
tasks.
Our method maintains its modular nature, enabling the inclusion or exclusion of
additional source tasks to further enhance the performance of the target tasks.
For instance, when considering the GLUE tasks, excluding SST-2 from the set of
source tasks can be a justifiable decision due to its limited utility or contribution to
the other tasks. Conversely, the inclusion of tasks like MNLI or STS-B has proven
effective and beneficial for other related tasks.
As another example, we added the MNLI task to SET2 and compared the perfor-
mance of the tasks with and without MNLI. The results are presented in Table 7. As
observed, the inclusion of MNLI boosts the performance in all methods, particularly
for tasks such as MultiNLI and SciTail, which are closely related to MNLI.
176 Related Works
This work lies in the line of parameter-efficient transfer learning for pretrained lan-
guage models. These approaches focus on fine-tuning a subset of parameters to adapt
pretrainedlanguagemodelstodownstreamtasks.Thefine-tunedparameterscantake
on various forms, such as lightweight neural adapters positioned between PTM lay-
ers [3], soft prompts appended to input examples [7], hidden states [8], bias terms
integratedwithinPTMparameters[20],orlow-rankmatricesincorporatedintoatten-
tion weights [4]. Our work is closely aligned with the realms of transfer learning and
modular design facilitated through the utilization of soft prompts.
Vu et al. [17] introduced SPoT, which delves into the enhancement of prompt
tuning performance across multiple tasks via soft prompt transfer. SPoT offers two
distinct approaches. In the first approach, it initiates by learning a generic prompt
across one or more source tasks using multi-task learning. This learned prompt then
serves as the foundation for initializing prompts in target tasks. Alternatively, in the
second approach, SPoT individually learns separate prompts for various source tasks.
It subsequently identifies an optimal source prompt and employs it to initialize the
training of a target prompt, thus enabling efficient knowledge transfer.
Similarly, Wang et al. [19] introduced MPT (Multitask Prompt Tuning), which
employs a similar approach by learning a single transferable prompt through multi-
task learning. This transferable prompt is learned via prompt decomposition and
distillation.Subsequently,low-rankupdatesareincorporatedintothissharedprompt,
facilitating its efficient adaptation to each distinct downstream target task.
Sunetal.[16]exploredtheconceptofmodularpromptswithinthescopeoffew-shot
learning.Theirmethodology,namedMulti-taskPre-trainedModularPrompt(MP2),
is comprised of two stages. In the first stage, modular prompts are pre-trained along-
side a trainable router with multi-task learning. This router facilitates the selection
and activation of source prompts for various target tasks, Subsequently, in the second
stage,asubsetofpromptsisactivatedandfine-tunedtoalignwithdownstreamtasks.
Notably, the initial phase of their method resembles our methodology. However,
there is a distinction: in their approach, the obtained source prompts serve as pre-
trained prompts for the subsequent stage. In contrast, in our approach, learning and
utilizing the source prompts are merged into a single stage. More importantly, we
employ a private prompt for each task, which is crucial for balancing the knowledge
from shared source tasks and a specific task knowledge.
Another related study to our work is conducted by Asai et al., named ATTEMPT
[1]. They first procure source prompts as encodings derived from extensive source
tasks.Theirmethodologyinvolvestraininganattentionmoduletointerpolatebetween
thesourcepromptsandanewlyinitializedtargetpromptforeachinstancewithinthe
targettask, aconcept analogousto theprivateprompts inour approach.One notable
distinction is that their attention module operates on an instance-wise basis, with
learning occurring for each input independently.
Additionally, they maintain the source prompts in a frozen state during target
prompt training, whereas our results emphasize that fine-tuning the source prompts
18Table 8 ComparisonwiththerelatedworksonGLUEtasks
method/ task mnli qqp qnli sst2 stsb mrpc rte cola avg.
ComPT-SSUM 82.1 89.6 92.8 93.2 89.3 86.0 73.3 81.8 86.0
FineTuning 86.8 91.6 93.0 94.6 89.7 90.2 71.9 61.8 84.9
PT 81.3 89.7 92.8 90.9 89.5 68.1 54.7 10.6 72.2
Spot 85.4 90.1 93.0 93.4 90.0 79.7 69.8 57.1 82.3
ATTEMPT 84.3 90.3 93.0 93.2 89.7 85.7 73.4 57.4 83.4
ATTEMPT-m 83.8 90.0 93.1 93.7 90.8 86.1 79.9 64.3 85.2
MPT 84.3 90.0 93.0 93.3 90.4 89.2 82.7 63.5 85.8
holds greater potential for enhancing performance, especially within few-shot scenar-
ios. Notably, in this context, all tasks are equipped to exchange knowledge during
training.
Unlike the methods discussed above, our approach stands out by guiding the cre-
ation of the target prompts through the incorporation of multiple source prompts
and a task-specific prompt throughout the training process. Additionally, we intro-
duced multiple methods to combine these prompts and compared their mechanisms
and advantages. We also investigated an approach of concatenating modular source
prompts, which is a novel technique.
Furthermore, unlike the multi-stage approaches used in these methods, our
approach operates within a single stage, performing knowledge transfer concurrently
with learning the target prompt for each task. While it is possible to initialize the
source prompts using pre-trained prompts, we achieve higher results by training all
prompts in a single stage, as detailed in the next section.
6.1 Comparison with Related Works
In Table 8, we conducted a comparison between our method and the reviewed
related works, which include vanilla prompt tuning (PT) [7], SPOT, ATTEMPT, and
MPT, across 8 GLUE tasks. ’ATTEMPT-m’ refers to the multi-tasking version of
ATTEMPT.Thereportednumericalvaluesaredirectlycitedfromtherespectivepub-
lishedpapers.It’sworthnotingthatMPT2 evaluatedtheirapproachonChinesetasks,
which led us to exclude them from this particular comparison.
Forourcomparison,weutilizedthetestsetsofthetasks.However,duetoresource
constraints,weconductedtestingonasubsetof5000samplesforourbest-performing
configurations. Despite this limitation, our overall performance outperforms these
other methods, and our scores are close to the top scores for various tasks. Neverthe-
less,weconsiderthemainadvantageofourapproachtobeinfew-shotsettings,where
it boosts the performance of tasks through knowledge sharing among related tasks. 2
2Inapossiblerevision,weintendtorunsomeoftherelatedworksinfew-shotsettingsandcompareour
scoresinthatsetting.
197 Conclusion
Inthisstudy,weexploredtherealmoftasktransferabilitywithinthecontextofmulti-
task prompt tuning, focusing on the potential for knowledge transfer among multiple
tasks trained with a mixture of soft prompts.
Our investigation revealed that the strategic use of source prompts, private
prompts,andattentionmodulesplaysapivotalroleindrivingsharedknowledgeacross
tasks and enhancing the performance of related tasks. This enhancement is particu-
larly critical in scenarios where data is scarce, as observed in few-shot settings. Our
modular approach, including the selection of tasks, determination of the number of
source prompts, method for combining source prompts into private prompts, and bal-
ancingtheircontributionsthroughlearningrates,amplifiestheresilienceandflexibility
of the approach.
These insights serve as valuable stepping stones for future research endeavors,
highlighting the value of multi-task prompt tuning as a versatile, modularity-driven
strategy for enhancing knowledge transfer and enabling parameter-efficient transfer
learning.
Declarations
• Competing Interests: The authors declare no competing interests.
• Authors’ Contributions:
– Hesham Faili: Supervision, Conceptualization, Methodology, Resources, Valida-
tion, Writing - Review & Editing.
– Ahmad Pouramini: Conceptualization, Investigation, Data Curation, Methodol-
ogy, Writing - Original Draft Preparation.
• Ethical and Informed Consent for Data Used: This study did not involve
human or animal subjects. All data are publicly available and cited appropriately.
Informed consent was not required.
• Data Availability and Access: Data are available on a public repository. Code
will be publicly available on GitHub: https://github.com/puraminy/ComPT
References
[1] Asai A, Salehi M, Peters ME, et al (2022) Attentional Mixtures of Soft Prompt
Tuning for Parameter-efficient Multi-task Knowledge Sharing. arXiv preprint
arXiv:220511961 URL http://arxiv.org/abs/2205.11961, arXiv:2205.11961
[2] Houlsby N, Giurgiu A, Jastrzebski S, et al (2019) Parameter-efficient transfer
learning for nlp. In: International Conference on Machine Learning, PMLR, pp
2790–2799
[3] Houlsby N, Giurgiu A, Jastrzebski S, et al (2019) Parameter-efficient transfer
learning for NLP. In: International Conference on Machine Learning, pp 2790–
2799
20[4] Hu HD, Zhu Q, Liu Z, et al (2021) Language model with adaptively computed
attention mechanisms. In: ACL-IJCNLP 2021, pp 2293–2304
[5] Karimi Mahabadi R, Henderson J, Ruder S (2021) Compacter: Efficient low-
rank hypercomplex adapter layers. Advances in Neural Information Processing
Systems 34:1022–1035
[6] Kirkpatrick J, Pascanu R, Rabinowitz N, et al (2017) Overcoming catastrophic
forgetting in neural networks. Proceedings of the national academy of sciences
114(13):3521–3526
[7] LesterB,Al-RfouR,ConstantN(2021)Thepowerofscaleforparameter-efficient
prompt tuning. In: Proceedings of the 2021 Conference on Empirical Methods in
NaturalLanguageProcessing.AssociationforComputationalLinguistics,Online
and Punta Cana, Dominican Republic, pp 3045–3059, https://doi.org/10.18653/
v1/2021.emnlp-main.243, URL https://aclanthology.org/2021.emnlp-main.243
[8] Li X, Liang P (2021) Prompting improves zero-shot learning for emergent enti-
ties. In: Proceedings of the 2021 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pp 5134–5145
[9] Li XL, Liang P (2021) Prefix-tuning: Optimizing continuous prompts for gen-
eration. ACL-IJCNLP 2021 - 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing, Proceedings of the Conference pp 4582–4597.
https://doi.org/10.18653/v1/2021.acl-long.353, URL http://arxiv.org/abs/2101.
00190, arXiv:2101.00190
[10] Liu X, Zheng Y, Du Z, et al (2021) Gpt understands, too. arXiv preprint
arXiv:210310385
[11] Maddison CJ, Mnih A, Teh YW (2017) The concrete distribution: A continuous
relaxationofdiscreterandomvariables.In:InternationalConferenceonLearning
Representations
[12] Pfeiffer J, Ruder S, Vuli´c I, et al (2023) Modular deep learning. arXiv preprint
arXiv:230211529
[13] Raffel C, Shazeer N, Roberts A, et al (2020) Exploring the limits of transfer
learning with a unified text-to-text transformer. Journal of Machine Learning
Research 21:1–67. arXiv:1910.10683
[14] Ruder S, Bingel J, Augenstein I, et al (2017) Learning what to share between
loosely related tasks. In: Proceedings of the 2017 Conference on Empirical Meth-
ods in Natural Language Processing, Association for Computational Linguistics,
pp 1939–1949
21[15] RuderS,PetersME,SwayamdiptaS,etal(2019)Transferlearninginnaturallan-
guage processing. In: Proceedings of the 2019 conference of the North American
chapter of the association for computational linguistics: Tutorials, pp 15–18
[16] Sun T, He Z, Zhu Q, et al (2022) Multi-Task Pre-Training of Modular Prompt
for Few-Shot Learning. arXiv preprint arXiv:221007565 URL http://arxiv.org/
abs/2210.07565, arXiv:2210.07565
[17] Vu T, Wang T, Munkhdalai T, et al (2020) Exploring and predicting transfer-
ability across NLP tasks. In: Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pp 7882–7926
[18] Wang A, Singh A, Michael J, et al (2018) Glue: A multi-task benchmark and
analysisplatformfornaturallanguageunderstanding.In:Proceedingsofthe2018
EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks
for NLP, pp 353–355
[19] Wang Z, Panda R, Karlinsky L, et al (2023) Multitask Prompt Tuning Enables
Parameter-Efficient Transfer Learning. arXiv preprint arXiv:230302861 1:1–16.
URL http://arxiv.org/abs/2303.02861, arXiv:2303.02861
[20] Zaken A, Mordokovitz E, Berdicesky G (2022) Bias correction of pre-trained
models. In: International Conference on Learning Representations
22Appendix A Analyze of Predictions
A.1 QQP Predictions
Table A1 provides the predictions for QQP task using only the source prompt, only
the private prompt, and their combination in the SSUM method.
Table A1 PredictionsfortheQQPtaskusingthesourceprompt,theprivateprompt,andtheir
combinationintheSSUMmethod.
method preds duplicate not-duplicate total acc.
duplicate 1 0
source entailment 7 0 0.8
not-duplicate 0 7
not-entailment 30 54
precision 3.0% 11.0%
duplicate 38 33
target not 0 1 0.38
not- 0 4
not-duplicate 0 6
not-uplicate 0 17
precision 100.0% 10.0%
duplicate 38 13
combined not-duplicate 0 48 0.87
precision 100.0% 79.0%
A.2 Assessing Task Relationships
Figure A1 presents the relationships between different tasks by employing prompts
obtained from one task to generate predictions for another task.
A.3 Effect of learning rate of private prompt
Figure A2 presents the performance of the MSUM method across different learning
rates.
23Fig.A1 EmployingpromptsobtainedfromtaskAtogeneratepredictionsfortaskB.TheANOVA
testwasusedtoassesstaskdependencies.Ontheleft,QNLIandRTEpromptsareusedtoevaluate
STSB.Ontheright,STSBpromptwasusedtoevaluateQNLIandRTE.
Fig.A2 PerformancescoresofsourcepromptsintheMSUMmethodatspecificlearningratesusing
50epochs.
24