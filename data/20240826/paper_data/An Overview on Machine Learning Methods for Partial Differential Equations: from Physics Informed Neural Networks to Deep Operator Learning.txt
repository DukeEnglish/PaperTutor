An Overview on Machine Learning Methods for
Partial Differential Equations: from Physics Informed
Neural Networks to Deep Operator Learning
Lukas Gonon1, Arnulf Jentzen2,3, Benno Kuckuck4,
Siyu Liang5,6,7, Adrian Riekert8, Philippe von Wurstemberger9,10
1 Department of Mathematics, Imperial College London,
United Kingdom; e-mail: l.gonon@imperial.ac.uk
2 School of Data Science and Shenzhen Research Institute
of Big Data, The Chinese University of Hong Kong, Shenzhen
(CUHK-Shenzhen), China; e-mail: ajentzen@cuhk.edu.cn
3 Applied Mathematics: Institute for Analysis and Numerics,
Faculty of Mathematics and Computer Science, University of
Münster, Germany; e-mail: ajentzen@uni-muenster.de
4 Applied Mathematics: Institute for Analysis and Numerics,
Faculty of Mathematics and Computer Science, University of
Münster, Germany; e-mail: bkuckuck@uni-muenster.de
5 School of Mathematics and Statistics,
Nanjing University of Science and Technology,
Nanjing, China; e-mail: liangsiyu@njust.edu.cn
6 School of Data Science, The Chinese University
of Hong Kong, Shenzhen (CUHK-Shenzhen), China
7 Mathematisches Institut, Ludwig-Maximilians-Universität München, Germany
8 Applied Mathematics: Institute for Analysis and Numerics,
Faculty of Mathematics and Computer Science, University of
Münster, Germany; e-mail: ariekert@uni-muenster.de
9 School of Data Science, The Chinese University
of Hong Kong, Shenzhen (CUHK-Shenzhen),
China; e-mail: philippevw@cuhk.edu.cn
10 Risklab, Department of Mathematics,
ETH Zurich, Switzerland;
e-mail: philippe.vonwurstemberger@math.ethz.ch
August 26, 2024
1
4202
guA
32
]AN.htam[
1v22231.8042:viXraAbstract
The approximation of solutions of partial differential equations (PDEs) with numerical algo-
rithms is a central topic in applied mathematics. For many decades, various types of methods
for this purpose have been developed and extensively studied. One class of methods which has
received a lot of attention in recent years are machine learning-based methods, which typically
involve the training of artificial neural networks (ANNs) by means of stochastic gradient descent
typeoptimizationmethods. WhileapproximationmethodsforPDEsusingANNshavefirstbeen
proposed in the 1990s they have only gained wide popularity in the last decade with the rise of
deep learning. This article aims to provide an introduction to some of these methods and the
mathematical theory on which they are based. We discuss methods such as physics-informed
neural networks (PINNs) and deep BSDE methods and consider several operator learning ap-
proaches.
Contents
1 Introduction 3
1.1 Fully-connected feedforward artificial neural networks (ANNs) . . . . . . . . . . . . . 4
2 Machine learning approximation methods for PDEs based on residual formula-
tions 4
2.1 Basic reformulation result for machine learning methods for PDEs I . . . . . . . . . . 4
2.2 Physics-informed neural networks (PINNs) . . . . . . . . . . . . . . . . . . . . . . . . 6
2.2.1 PINNs for general boundary value PDE problems . . . . . . . . . . . . . . . . 6
2.2.2 PINNs for time-dependent initial value PDE problems . . . . . . . . . . . . . 9
2.2.3 PINNs for free boundary Stefan problems . . . . . . . . . . . . . . . . . . . . 10
3 Machine learning approximation methods for PDEs based on stochastic Feynman-
Kac-type representations 13
3.1 Basic reformulation result for machine learning methods for PDEs II . . . . . . . . . . 13
3.2 Stochastic representations (Feynman-Kac formulas) for PDEs . . . . . . . . . . . . . 16
3.3 Deep Kolmogorov methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
3.4 Deep BSDE methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
3.4.1 Uniqueness for solutions of BSDEs . . . . . . . . . . . . . . . . . . . . . . . . 23
3.4.2 Reformulating semilinear PDEs as infinite-dimensional stochastic optimization
problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
4 Operator learning methods 28
4.1 The canonical data-driven operator learning framework . . . . . . . . . . . . . . . . . 29
4.2 Neural operator architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
4.2.1 Fully-connected feed-forward neural operators . . . . . . . . . . . . . . . . . . 30
4.2.2 Architectures based on convolutional neural networks . . . . . . . . . . . . . . 31
4.2.2.1 Simple periodic convolutional neural operators . . . . . . . . . . . . . 32
4.2.2.2 Simple convolutional neural operators with encoder-decoder architec-
ture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
4.2.3 Integral kernel neural operators . . . . . . . . . . . . . . . . . . . . . . . . . . 35
4.2.4 Fourier neural operators (FNOs) . . . . . . . . . . . . . . . . . . . . . . . . . . 36
4.2.5 Deep Operator Networks (DeepONets) . . . . . . . . . . . . . . . . . . . . . . 38
24.3 Physics-informed neural operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
4.4 Other deep operator learning approaches . . . . . . . . . . . . . . . . . . . . . . . . . 41
4.5 Numerical results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
4.5.1 Viscous Burgers equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
4.5.2 Allen-Cahn equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
4.5.3 Reaction-diffusion equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
1 Introduction
The approximation of solutions of partial differential equations (PDEs) with numerical algorithms
is a central topic in applied mathematics. For many decades, various types of methods for this
purpose have been developed and extensively studied such as finite difference methods (FDMs), finite
element methods (FEMs), and spectral methods. One class of methods which has received a lot of
attention in recent years are machine learning-based methods, which typically involve the training of
artificial neural networks (ANNs) by means of stochastic gradient descent (SGD)-type optimization
methods. While approximation methods for PDEs using ANNs have first been proposed in the
1990s (cf., e.g., [16,43,53]) they have only gained wide popularity in the last decade with the rise
of deep learning (cf., e.g., [19,32,75,82] for some seminal modern references). This article aims to
provide an introduction to some of these methods and the mathematical theory on which they are
based. There are already several other excellent survey articles on this topic and we refer, e.g., to
[1,3,4,6–8,10–12,18,20,24,25,28,31,34,35,37,38,41,44,45,47,52,63,64,66–68,70,77–79,81,84,89,90,93].
In this article we consider two different types problems that machine learning methods for PDEs
seek to address. The first type of problems focuses on solving single instances of PDEs while the
second type of problems is concerned with the approximation of solution operators corresponding to
PDEs. Machine learning methods for the first type of problems generally involve reformulating PDE
approximation problems as stochastic optimization problems which can be solved using SGD-type
optimization methods. In this article we discuss two main classes of such methods and distinguish
between them based on how this reformulation is achieved. Methods in the first class make use of
residualsofPDEs, thatis, thedifferencebetweentheleft-handsidesandtheright-handsidesofPDEs
forsolutioncandidates,andarediscussedinSection2. Ontheotherhand,methodsinthesecondclass
make use of Feynman-Kac formulas which provide stochastic representations of solutions of parabolic
PDEs, and are discussed in Section 3. For both classes, we identify abstract reformulation results
that allow the use of residuals and Feynman-Kac formulas, respectively, to recast PDE problems
as suitable stochastic optimization problems (cf. Sections 2.1 and 3.1). We illustrate how these
reformulation results can be applied to various types of PDEs. Specifically, we present reformulations
which underpin methods such as physics–informed neural networks (PINNs) (cf., Section 2.2), deep
Kolmogorov methods (cf., Section 3.2), and deep backward stochastic differential equation (BSDE)
methods (cf., Section 3.2).
In Section 4 we discuss operator learning methods for PDEs where the aim is to learn solution
operators of PDEs which corresponds to the second type of problems mentioned above. For instance,
such operators could be mappings which assign to all suitable initial values or boundary conditions
the corresponding PDE solutions. We rigorously define several architectures which make it possible
to learn such mappings from function spaces to function spaces and discuss how they can be trained.
In the remainder of this introduction we now introduce in the next subsection the definition of
fully-connected feedforward ANNs which we will use throughout this article.
31.1 Fully-connected feedforward artificial neural networks (ANNs)
The definitions in this subsection are strongly inspired by [41, Chapter 1].
Definition 1.1 (Affine linear functions). Let d,m,n ∈ N, s ∈ N , θ = (θ ,...,θ ) ∈ Rd satisfy
0 1 d
d ≥ s + mn + m. Then we denote by Aθ,s : Rn → Rm the function which satisfies for all x =
m,n
(x ,...,x ) ∈ Rn that
1 n
    
θ θ ··· θ x θ
s+1 s+2 s+n 1 s+mn+1
 θ θ ··· θ x  θ 
s+n+1 s+n+2 s+2n 2 s+mn+2
    
Aθ,s (x) =  θ s+2n+1 θ s+2n+2 ··· θ s+3nx 3+θ s+mn+3. (1)
m,n   . .
.
. .
.
... . .
.
   . .
.
    . .
.
 
    
θ θ ··· θ x θ
s+(m−1)n+1 s+(m−1)n+2 s+mn n s+mn+m
Definition 1.2 (Multi-dimensional versions of one-dimensional functions). Let ψ: R → R be a
function. Then we denote by M ψ: (∪ d∈N∪
d1,d2,...,d
d∈NRd1×d2×...×d d) → (∪ d∈N∪
d1,d2,...,d
d∈NRd1×d2×...×d d)
the function which satisfies for all d ∈ N, d ,d ,...,d ∈ N, x = (x ) ∈
1 2 d i1,i2,...,i d (i1,i2,...,i d)∈(×d k=1{1,2,...,d k})
Rd1×d2×...×d d, y = (y i1,i2,...,i d) (i1,i2,...,i d)∈(×d k=1{1,2,...,d k}) ∈ Rd1×d2×...×d d with ∀i 1 ∈ {1,2,...,d 1}, i 2 ∈
{1,2,...,d }, ..., i ∈ {1,2,...,d }: y = ψ(x ) that
2 d d i1,i2,...,i
d
i1,i2,...,i
d
M (x) = y. (2)
ψ
Definition 1.3 (Realizations of fully-connected feedforward ANNs). Let L ∈ N\{1}, l ,l ,...,l ∈
0 1 L
N, for every j ∈ {0,1,...,L} let d ∈ N satisfy d = (cid:80)j l (l +1), let a: R → R be a function,
j 0 j k=1 k k−1
and let θ ∈ RdL. Then we denote by Nl0,l1,...,lL: Rl0 → RlL the function which satisfies
a,θ
Nl0,l1,...,lL = Aθ,dL−1 ◦M ◦Aθ,dL−2 ◦...◦M ◦Aθ,d0 (3)
a,θ lL,lL−1 a lL−1,lL−2 a l1,l0
(cf. Definitions 1.1 and 1.2).
2 Machine learning approximation methods for PDEs based
on residual formulations
In this section we discuss machine learning approximation methods for PDEs which are based on
minimizing residuals of PDEs. We first introduce in Section 2.1 an abstract reformulation result upon
whichmanymachinelearningmethodsforPDEsrelytoreformulatePDEsasstochasticminimization
problems. Thereafter, in Section 2.2 we discuss PINNs which are one of the most famous machine
learning approximation methods for PDEs based on residual formulations.
2.1 Basic reformulation result for machine learning methods for PDEs I
Machine learning generally involves the training of ANNs through SGD-type methods. Applying
SGD-type methods to train ANNs requires the formulation of a suitable training objective given by a
parametric expectation. Therefore, to develop machine learning techniques for PDEs, it is necessary
to reformulate PDE problems into stochastic minimization problems, where the objective function is
given by a parametric expectation.
In this section we introduce in Corollary 2.2 an abstract reformulation result upon which many
machine learning methods for PDEs rely to reformulate PDE problems into such stochastic mini-
mization problems.
4Proposition 2.1. Let S and V be non-empty sets with S ⊆ V, let (Ω,F,P) be a probability space,
for every v ∈ V let L : Ω → [0,∞] be a random variable, assume for all v ∈ V that P(L = 0) = 1
v v
if and only if v ∈ S, and let l: V → [0,∞] satisfy for all v ∈ V that l(v) = E[L]. Then
v
(i) for all v ∈ V it holds that
l(v) = inf l(w) (4)
w∈V
if and only if v ∈ S and
(ii) there exists v ∈ V such that
l(v) = inf l(w). (5)
w∈V
Proof of Proposition 2.1. Observe that the assumption that for all v ∈ V it holds that P(L = 0) = 1
v
if and only if v ∈ S ensures that for all v ∈ S it holds that
P(L = 0) = 1. (6)
v
The assumption that for all v ∈ V it holds that l(v) = E[L] therefore implies that for all v ∈ S it
v
holds that
l(v) = E[L] = 0. (7)
v
Combining this and the fact that for all w ∈ V it holds that l(w) ≥ 0 shows that for all v ∈ S it
holds that
0 ≤ inf l(w) ≤ l(v) = 0. (8)
w∈V
Hence, we obtain for all v ∈ S that
l(v) = inf l(w) = 0. (9)
w∈V
Combining this and the assumption that S ̸= ∅ proves item (ii). Note that (9) and the assumption
that S ̸= ∅ demonstrate that
inf l(w) = 0. (10)
w∈V
Therefore, we obtain that for all v ∈ V with l(v) = inf l(w) it holds that l(v) = 0. The
w∈V
assumption that for all v ∈ V it holds that l(v) = E[L] hence establishes that for all v ∈ V with
v
l(v) = inf l(w) it holds that
w∈V
E[L] = 0. (11)
v
Therefore, we obtain that for all v ∈ V with l(v) = inf l(w) it holds that
w∈V
P(L = 0) = 1. (12)
v
The assumption that for all v ∈ V it holds that P(L = 0) = 1 if and only if v ∈ S hence ensures
v
that for all v ∈ V with l(v) = inf l(w) it holds that v ∈ S. Combining this with (9) implies
w∈V
item (i). The proof of Proposition 2.1 is thus complete.
Corollary 2.2 (Basic reformulation result for machine learning methods for PDEs I). Let S and V
be non-empty sets with S ⊆ V, let (Ω,F,P) be a probability space, let (R,R) be a measurable space,
let R: Ω → R be a random variable, let N ∈ N, for every v ∈ V let U : R → RN be measurable,
v
assume for all v ∈ V that P(U (R) = 0) = 1 if and only if v ∈ S, and let l: V → [0,∞] satisfy1 for
v
all v ∈ V that l(v) = E[∥U (R)∥2]. Then
v
1Observe that for all d ∈ N, v = (v ,...,v ), w = (w ,...,w ) ∈ Rd it holds that ∥v∥2 = (cid:80)d |v |2 and
1 d 1 d i=1 i
⟨v,w⟩=(cid:80)d
v w .
i=1 i i
5(i) for all v ∈ V it holds that
l(v) = inf l(w) (13)
w∈V
if and only if v ∈ S and
(ii) there exists v ∈ V such that
l(v) = inf l(w). (14)
w∈V
Proof of Corollary 2.2. Note that Proposition 2.1 (applied with V ↶ V, S ↶ S, (L) ↶
v v∈V
(∥U (R)∥2) in the notation of Proposition 2.1) shows item (i) and item (ii). The proof of Corol-
v v∈V
lary 2.2 is thus complete.
2.2 Physics-informed neural networks (PINNs)
In this section we discuss PINNs. The term PINNs and the corresponding methodology was intro-
duced in [73–75] for two different types of problems. The first type of problem is called data-driven
solutions of PDEs, where to goal is to approximate solutions of PDEs using ANNs. The second type
of problem is called data-driven discovery of PDEs, where the goal is to estimate parameters of PDEs
based on observation data. In this section we consider PINNs for the first type of problem, i.e., the
approximation of solutions of PDEs.
2.2.1 PINNs for general boundary value PDE problems
In this section we introduce a PINNs methodology for general boundary value PDE problem in
Method description 2.7 below. To motivate Method description 2.7 we introduce in Theorem 2.4 a
result which allows reformulating solutions of general boundary value PDE problems as solutions of
stochastic minimization problems. The proof of Theorem 2.4 relies on the abstract reformulation
result for PDEs in Corollary 2.2 above and on Lemma 2.3 below.
Lemma 2.3. Let d,δ ∈ N, let D ⊆ Rd, let f ∈ C(D,Rδ), let (Ω,F,P) be a probability space, let
X: Ω → D be a random variable, assume for all x ∈ D, ε ∈ (0,∞) that P(∥X −x∥ < ε) > 0, and
assume P(f(X) = 0) = 1. Then it holds for all x ∈ D that
f(x) = 0. (15)
Proof of Lemma 2.3. We establish (15) by a contradiction. For this let x ∈ D satisfy f(x) ̸= 0.
Observe that the fact that ∥f(x)∥ > 0 and the fact that R ∋ y (cid:55)→ ∥f(y)∥ ∈ R is continuous prove
that there exists η ∈ (0,∞) which satisfies for all y ∈ {z ∈ D: ∥z −x∥ < η} that
∥f(y)∥ > 0. (16)
Next note that (16) and the assumption that P(f(X) = 0) = 1 demonstrate that
0 = 1−P(f(X) = 0) = P(f(X) ̸= 0) = P(∥f(X)∥ > 0) ≥ P(∥X −x∥ < η). (17)
Furthermore, observe that the assumption that for all y ∈ D, ε ∈ (0,∞) it holds that P(∥X −y∥ <
ε) > 0 establishes that P(∥X −x∥ < η) > 0. Combining this with (17) ensures that
0 = P(∥f(X)∥ > 0) ≥ P(∥X −x∥ < η) > 0. (18)
The proof of Lemma 2.3 is thus complete.
6Theorem 2.4 (PINNs for general boundary value PDE problems). Let d,δ,b ∈ N, let D ⊆ Rd, let
V ⊆ C(D,Rδ), let W ⊆ ∂D, let D: V → C(D,Rδ) and B: V → C(W,Rb) be functions, let u ∈ V
satisfy for all x ∈ D, y ∈ W that
(D(u))(x) = 0 and (B(u))(y) = 0, (19)
let (Ω,F,P) be a probability space, let X: Ω → D and Y: Ω → W be independent random variables,
assume for all X,Y ∈ {Z ⊆ Rd: Z is open} with X ∩D ̸= ∅ and Y∩W ̸= ∅ that P({X ∈ X}∩{Y ∈
Y}) > 0, let l: V → [0,∞] satisfy for all v ∈ V that
l(v) = E(cid:2) ∥(D(v))(X)∥2 +∥(B(v))(Y)∥2(cid:3) , (20)
and let v ∈ V. Then the following two statements are equivalent:
(i) It holds that l(v) = inf l(w).
w∈V
(ii) It holds for all x ∈ D, y ∈ W that (D(v))(x) = 0 and (B(v))(y) = 0.
Proof of Theorem 2.4. Throughout this proof let S ⊆ V satisfy
S = (cid:8) v ∈ V : (cid:0) ∀x ∈ D,y ∈ W: ∥(D(v))(x)∥ = ∥(B(v))(y)∥ = 0(cid:1)(cid:9) (21)
and for every v ∈ V let U : D×W → Rδ+b satisfy for all x ∈ D, y ∈ W that
v
U (x,y) = (cid:0) (D(v))(x),(B(v))(y)(cid:1) . (22)
v
Note that the assumption that (Ω,F,P) is a probability space implies that Ω ̸= ∅. This and the fact
that X: Ω → D and Y: Ω → W are functions show that
D ̸= ∅ and W ̸= ∅. (23)
Next observe that (21) and (22) prove that for all v ∈ S it holds that P(U (X,Y) = 0) = 1.
v
Furthermore, note that the assumption that for all X,Y ∈ {Z ⊆ Rd: Z is open} with X ∩ D ̸= ∅
and Y ∩W ̸= ∅ it holds that P({X ∈ X}∩ {Y ∈ Y}) > 0 demonstrates that for all open X ⊆ Rd
with X ∩D ̸= ∅ it holds that P({X ∈ X}) > 0. Moreover, observe that (22) establishes that for all
v ∈ V with P(U (X,Y) = 0) = 1 it holds that
v
P(cid:0) (D(v))(X) = 0(cid:1) = P(cid:0) (B(v))(Y) = 0(cid:1) = 1. (24)
Combining this with the fact that for all open X ⊆ Rd with X ∩D ̸= ∅ it holds that P({X ∈ X}) > 0
and Lemma 2.3 (applied for every v ∈ V with d ↶ d, δ ↶ δ, f ↶ (D ∋ x (cid:55)→ (D(v))(x) ∈ Rδ) in the
notation of Lemma 2.3) ensures that for all v ∈ V, x ∈ D with P(U (X,Y) = 0) = 1 it holds that
v
(D(v))(x) = 0. (25)
In addition, note that the assumption that for all X,Y ∈ {Z ⊆ Rd: Z is open} with X ∩ D ̸= ∅
and Y ∩ W ̸= ∅ it holds that P({X ∈ X} ∩ {Y ∈ Y}) > 0 implies that for all open Y ⊆ Rd with
Y ∩W ̸= ∅ it holds that P({Y ∈ Y}) > 0. Combining this and Lemma 2.3 (applied for every v ∈ V
with d ↶ d, δ ↶ b, f ↶ (W ∋ y (cid:55)→ (B(v))(y) ∈ Rb) in the notation of Lemma 2.3) shows that for
all v ∈ V, y ∈ W with P(U (X,Y) = 0) = 1 it holds that
v
(B(v))(y) = 0. (26)
7This and (25) prove that for all v ∈ V with P(U (X,Y) = 0) = 1 it holds that v ∈ S. The
v
assumption that u ∈ V therefore demonstrates that S ̸= ∅ and V ̸= ∅. Combining this with (25),
(26), and Corollary 2.2 (applied with R ↶ D × W, N ↶ δ + b, R ↶ (X,Y), (U ) ↶ (U ) ,
v v∈V v v∈V
S ↶ S, V ↶ V, (Ω,F,P) ↶ (Ω,F,P) in the notation of Corollary 2.2) establishes (item (i) ←→
item (ii)). The proof of Theorem 2.4 is thus complete.
Corollary 2.5 (PINNs for Laplace equations). Let d,b ∈ N, let D ⊆ Rd, let f ∈ C(∂D,R),
u ∈ C2(D,R) satisfy for all x ∈ D, y ∈ ∂D that
(∆u)(x) = 0 and u(y) = f(y), (27)
let (Ω,F,P) be a probability space, let X: Ω → D and Y: Ω → ∂D be independent random variables,
assume for all X,Y ∈ {Z ⊆ Rd: Z is open} with X ∩D ̸= ∅ and Y∩∂D ̸= ∅ that P({X ∈ X}∩{Y ∈
Y}) > 0, let l: C2(D,R) → [0,∞] satisfy for all v ∈ C2(D,R) that
l(v) = E(cid:2) |(∆v)(X)|2 +|v(Y)−f(Y)|2(cid:3) , (28)
and let v ∈ C2(D,R). Then the following two statements are equivalent:
(i) It holds that
l(v) = inf l(w). (29)
w∈C2(D,R)
(ii) It holds for all x ∈ D, y ∈ ∂D that
(∆v)(x) = 0 and v(y) = f(y). (30)
Proof of Corollary 2.5. Observe that Theorem 2.4 (applied with d ↶ d, δ ↶ 1, b ↶ b, D ↶ D,
W ↶ ∂D, V ↶ C2(D,R), D ↶ (C2(D,R) ∋ u (cid:55)→ ∆u ∈ C(D,R)), B ↶ (C2(D,R) ∋ v (cid:55)→
v| −f ∈ C(∂D,R)) in the notation of Theorem 2.4) ensures (item (i)←→item (ii)). The proof of
∂D
Corollary 2.5 is thus complete.
Lemma 2.6. Let d,p,L ∈ N, θ ∈ Rd, a ∈ Cp(R,R), l ,l ,...,l ∈ N satisfy d = (cid:80)L l (l +1).
0 1 L k=1 k k−1
Then Nl0,l1,...,lL ∈ Cp(Rl0,RlL) (cf. Definition 1.3).
a,θ
Proof of Lemma 2.6. Note that item (i) in Lemma 6.1.5, item (i) in Lemma 6.1.6, and item (i) in
6.1.7 in [41] imply that Nl0,l1,...,lL ∈ Cp(Rl0,RlL) (cf., for instance, the proofs of Lemma 10.13.2,
a,θ
Lemma 10.13.3, and Corollary 10.13.4 in [41]). The proof of Lemma 2.6 is thus complete.
Methoddescription2.7(PINNmethodsforgeneralboundaryvaluePDEproblems2). Letd,δ,b,p ∈
N, let D ⊆ Rd, let W ⊆ ∂D, let D: Cp(D,Rδ) → C(D,Rδ) and B: Cp(D,Rδ) → C(W,Rb) be func-
tions, let u ∈ Cp(D,Rδ) satisfy for all x ∈ D, y ∈ W that
(D(u))(x) = 0 and (B(u))(y) = 0, (31)
let a ∈ Cp(R,R), d,L ∈ N, l ,l ,...,l ∈ N satisfy l = d, l = δ, and d = (cid:80)L l (l +1), let
0 1 L 0 L k=1 k k−1
L: Rd ×D×W → R satisfy for all θ ∈ Rd, x ∈ D, y ∈ W that
L(θ,x,y) = ∥(D(Nl0,l1,...,lL| ))(x)∥2 +∥(B(Nl0,l1,...,lL| )(y)∥2 (32)
a,θ D a,θ D
2Note that for every k,d,δ ∈ N and every D ⊆ Rd it holds that Ck(D,Rδ) = {f ∈ C(D,Rδ): (∃g ∈
Ck(Rd,Rδ): g| =f)}.
D
8(cf. Definition 1.3 and Lemma 2.6), let G: Rd × D × W → Rd satisfy for all x ∈ D, y ∈ W,
θ ∈ {ϑ ∈ Rd: L(·,x,y) is differentiable at ϑ} that G(θ,x,y) = (∇ L)(θ,x,y), let (Ω,F,P) be a
θ
probability space, let M,M ∈ N, let X : Ω → D, m ∈ {1,2,...,M}, be i.i.d. random variables, let
m
Y : Ω → W, m ∈ {1,2,...,M}, be i.i.d. random variables, for every n ∈ N, m ∈ {1,2,...,M} let
m
ξ : Ω → {1,2,...,M} be a random variable, assume that (X ) , (Y ) , and
n,m m m∈{1,2,...,M} m m∈{1,2,...,M}
(ξ ) are independent, let (γ ) ⊆ R, and let Θ: N × Ω → Rd satisfy for all
n,m (n,m)∈N×{1,2,...,M} n n∈N 0
n ∈ N that
0
(cid:34) (cid:35)
M
γ (cid:88)
Θ = Θ − n G(Θ ,X ,Y ) . (33)
n n−1
M
n−1 ξn,m ξn,m
m=1
Remark 2.8 (Explanations for Method description 2.7). Loosely speaking, in Method description 2.7
we think for sufficiently large N ∈ N of the ANN realization Nl0,l1,...,lL| as an approximation
a,ΘN D
Nl0,l1,...,lL| ≈ u (34)
a,ΘN D
of the solution u ∈ Cp(D,Rδ) of the boundary value PDE problem in (31).
Remark 2.9. In Method description 2.7 we employ fully-connected feedforward ANNs as defined in
Definition 1.3 to approximate PDE solutions. Many other architectures could be used; for example,
random feature networks where only the last layer is learned (cf., e.g., [26] for results analyzing such
methods).
2.2.2 PINNs for time-dependent initial value PDE problems
InthissectionweintroduceaPINNsmethodologyforgeneraltime-dependentinitialvaluePDEprob-
lems in Method description 2.11 below. This section proceeds in an analogous way to Section 2.2.1:
We motivate Method description 2.11 with a reformulation result in Corollary 2.10 which allows
recasting time-dependent initial value PDE problems as stochastic minimization problems.
Corollary 2.10 (PINNsfortime-dependentinitialvaluePDEproblems). Let T ∈ (0,∞), d,δ,b ∈ N,
let D ⊆ Rd, let ϕ ∈ C(D,Rδ), let V ⊆ C([0,T] × D,Rδ), let D: V → C([0,T] × D,Rδ) and
B: V → C([0,T]×∂D,Rb) be functions, let u ∈ V satisfy for all t ∈ [0,T], x ∈ D, y ∈ ∂D that
(D(u))(t,x) = 0, (B(u))(t,y) = 0, and u(0,x) = ϕ(x), (35)
let (Ω,F,P) be a probability space, let X: Ω → D, Y: Ω → ∂D, and T: Ω → [0,T] be independent
random variables, assume for all r ∈ (0,T), s ∈ (r,T), X,Y ∈ {Z ⊆ Rd: Z is open} with X ∩D ̸= ∅
and Y ∩∂D ̸= ∅ that P({X ∈ X}∩{Y ∈ Y}∩{r < T < s}) > 0, and let l: V → [0,∞] satisfy for
all v ∈ V that
l(v) = E(cid:2) ∥(D(v))(T,X)∥2 +∥(B(v))(T,Y)∥2 +∥v(0,X)−ϕ(X)∥2(cid:3) . (36)
Then
(i) for all v ∈ V it holds that
l(v) = inf l(w) (37)
w∈V
if and only if it holds for all t ∈ [0,T], x ∈ D, y ∈ ∂D that
(D(v))(t,x) = 0, (B(v))(t,y) = 0, and v(0,x) = ϕ(x) (38)
and
9(ii) there exists v ∈ V such that
l(v) = inf l(w). (39)
w∈V
Proof of Corollary 2.10. Observe that Theorem 2.4 (applied with d ↶ d + 1, δ ↶ δ, b ↶ b, D ↶
[0,T]×D, W ↶ [0,T]×∂D in the notation of Theorem 2.4) shows items (i) and (ii). The proof of
Corollary 2.10 is thus complete.
Method description 2.11 (PINN-SGD methods for time-dependent initial value PDE problems3).
Let T ∈ (0,∞), d,δ,b,p ∈ N, let D ⊆ Rd, let ϕ ∈ C(D,Rδ), let D: Cp([0,T]×D,Rδ) → C([0,T]×
D,Rδ) and B: Cp([0,T] × D,Rδ) → C([0,T] × ∂D,Rb) be functions, let u ∈ Cp([0,T] × D,Rδ)
satisfy for all t ∈ [0,T], x ∈ D, y ∈ ∂D that
(D(u))(t,x) = 0, (B(u))(t,y) = 0, and u(0,x) = ϕ(x), (40)
let a ∈ Cp(R,R), d,L ∈ N, l ,l ,...,l ∈ N satisfy l = d+1, l = δ, and d = (cid:80)L l (l +1),
0 1 L 0 L k=1 k k−1
let L: Rd ×[0,T]×D×∂D → R satisfy for all θ ∈ Rd, t ∈ [0,T], x ∈ D, y ∈ ∂D that
L(θ,t,x,y) = ∥(D(Nl0,l1,...,lL| ))(t,x)∥2 +∥(B(Nl0,l1,...,lL| ))(t,y)∥2
a,θ [0,T]×D a,θ [0,T]×D (41)
+∥Nl0,l1,...,lL(0,x)−ϕ(x)∥2
a,θ
(cf. Definition 1.3 and Lemma 2.6), let G: Rd × [0,T] × D × ∂D → Rd satisfy for all t ∈ [0,T],
x ∈ D, y ∈ ∂D, θ ∈ {ϑ ∈ Rd: L(·,x,y) is differentiable at ϑ} that G(t,θ,x,y) = (∇ L)(t,θ,x,y), let
θ
(Ω,F,P) be a probability space, let M,M ∈ N, let X : Ω → D, m ∈ {1,2,...,M}, be i.i.d. random
m
variables, let Y : Ω → ∂D, m ∈ {1,2,...,M}, be i.i.d. random variables, let T : Ω → [0,T],
m m
m ∈ {1,2,...,M}, be i.i.d. random variables, for every n ∈ N, m ∈ {1,2,...,M} let ξ : Ω →
n,m
{1,2,...,M} be a random variable, assume that (X ) , (Y ) , (T ) ,
m m∈{1,2,...,M} m m∈{1,2,...,M} m m∈{1,2,...,M}
and (ξ ) are independent, let (γ ) ⊆ R, and let Θ: N ×Ω → Rd satisfy for all
n,m (n,m)∈N×{1,2,...,M} n n∈N 0
n ∈ N that
0
(cid:34) (cid:35)
M
γ (cid:88)
Θ = Θ − n G(Θ ,T ,X ,Y ) . (42)
n+1 n
M
n−1 ξn,m ξn,m ξn,m
m=1
Remark 2.12 (Explanations for Method description 2.11). Loosely speaking, in Method descrip-
tion 2.11 we think for sufficiently large N ∈ N of the ANN realization Nl0,l1,...,lL| as an approx-
a,ΘN [0,T]×D
imation
Nl0,l1,...,lL| ≈ u (43)
a,ΘN [0,T]×D
of the solution u ∈ Cp([0,T]×D,Rδ) of the time-dependent initial value PDE problem in (40).
2.2.3 PINNs for free boundary Stefan problems
In this section we introduce a PINNs methodology for free boundary Stefan problems in Method
description 2.14 below. This section proceeds in an analogous way to Sections 2.2.1 and 2.2.1: We
motivate Method description 2.14 with a reformulation result in Theorem 2.13 which allows recasting
free boundary Stefan problems as stochastic minimization problems. The Stefan problem considered
in this section and the corresponding PINNs methodology in Method description 2.14 are based
on [85].
3Note that for every k,d,δ ∈ N and every D ⊆ Rd it holds that Ck(D,Rδ) = {f ∈ C(D,Rδ): (∃g ∈
Ck(Rd,Rδ): g| =f)}.
D
10Theorem 2.13 (PINNs for free boundary Stefan problems). Let T,ψ ∈ (0,∞), ϕ,g,h ,h ∈
0 1
C(R,R), letV = C1,2([0,T]×R,R)×C([0,T],R), letu = (u ,u ) = ((u (t,x)) ,(u (t)) ) ∈
1 2 1 (t,x)∈[0,T]×R 2 t∈[0,T]
V, let D = {(t,x) ∈ (0,T)×(0,∞): x < u (t)}, assume for all (t,x) ∈ D that
2
∂u1(t,x) = ∂2u1(t,x), ∂u1(t,0) = g(t), u (t,u (t)) = h (t), (44)
∂t ∂x2 ∂x 1 2 0
∂u1(t,u (t)) = h (t), u (0,x) = ϕ(x), and u (0) = ψ, (45)
∂x 2 1 1 2
let (Ω,F,P) be a probability space, let T: Ω → R and X: Ω → R be random variables which satisfy
for all non-empty open U ⊆ D that 0 < P((T,X) ∈ U) ≤ P((T,X) ∈ D) = 1, and let l: V → [0,∞]
satisfy for all v = (v ,v ) ∈ V that
1 2
l(v) = E(cid:2) |∂v1(T,X)− ∂2v1(T,X)|2 +|∂v1(T,0)−g(T)|2 +|v (T,v (T))−h (T)|2
∂t ∂x2 ∂x 1 2 0 (46)
+|∂v1(T,v (T))−h (T)|2 +|v (0,X)−ϕ(X)|2 +|v (0)−ψ|2(cid:3) .
∂x 2 1 1 2
Then
(i) for all v = (v ,v ) ∈ V it holds that
1 2
l(v) = inf l(w) (47)
w∈V
if and only if it holds for all (t,x) ∈ D that
∂v1(t,x) = ∂2v1(t,x), ∂v1(t,0) = g(t), v (t,v (t)) = h (t), (48)
∂t ∂x2 ∂x 1 2 0
∂v1(t,v (t)) = h (t), v (0,x) = ϕ(x), and v (0) = ψ, (49)
∂x 2 1 1 2
and
(ii) there exists v ∈ V such that
l(v) = inf l(w). (50)
w∈V
Proof of Theorem 2.13. Throughout this proof for every v ∈ V let U : D → R6 satisfy for all
v
(t,x) ∈ D that
U (t,x) = (cid:0) ∂v1(t,x)− ∂2v1(t,x), ∂v1(t,0)−g(t),v (t,v (t))−h (t),
v ∂t ∂x2 ∂x 1 2 0
∂v1(t,v (t))−h (t),v (0,x)−ϕ(x),v (0)−ψ(cid:1) (51)
∂x 2 1 1 2
and let S ⊆ V satisfy
S = {v ∈ V : sup ∥U (t,x)∥ = 0}. (52)
(t,x)∈D v
Note that (52), the assumption that P((T,X) ∈ D) = 1, the fact that {ϕ,g,h ,h } ⊆ C(R,R), and
0 1
the fact that [0,T]×R ⊇ D prove that for all v ∈ S it holds that P(U (T,X) = 0) = 1. Next observe
v
that (51) demonstrates that for all v ∈ V with
P(U (T,X) = 0) = 1 (53)
v
it holds that v (0) = ψ and
2
P(cid:0) ∂v1(T,X) = ∂2v1(T,X)(cid:1) = P(cid:0) v (0,X) = ϕ(X)(cid:1) = P(cid:0) ∂v1(T,0) = g(T)(cid:1)
∂t ∂x2 1 ∂x (54)
= P(cid:0) v (T,v (T)) = h (T)(cid:1) = P(cid:0) ∂v1(T,v (T)) = h (T)(cid:1) = 1.
1 2 0 ∂x 2 1
11Combining this, the fact that for all v ∈ V with (53) it holds that v (0) = ψ, the assumption that
2
for all non-empty open U ⊆ D it holds that P((T,X) ∈ U) > 0, and Lemma 2.3 (applied with d ↶ 2,
δ ↶ 6, f ↶ (D ∋ (t,x) (cid:55)→ U (t,x) ∈ R6) in the notation of Lemma 2.3) establishes that for all
v
(t,x) ∈ D, v ∈ V with P(U (T,X) = 0) = 1 it holds that
v
U (t,x) = 0. (55)
v
Hence, we obtain that for all v ∈ V with P(U (T,X) = 0)) = 1 it holds that v ∈ S. Combining
v
this and Corollary 2.2 (applied with N ↶ 6, R ↶ (T,X), (U ) ↶ (U ) , S ↶ S, V ↶ V,
v v∈V v v∈V
(Ω,F,P) ↶ (Ω,F,P) in the notation of Corollary 2.2) ensures items (i) and (ii). The proof of
Theorem 2.13 is thus complete.
Methoddescription2.14(PINNsforfreeboundaryStefanproblems). LetT,ψ ∈ (0,∞), ϕ,g,h ,h ∈
0 1
C(R,R), letV = C1,2([0,T]×R,R)×C([0,T],R), letu = (u ,u ) = ((u (t,x)) ,(u (t)) ) ∈
1 2 1 (t,x)∈[0,T]×R 2 t∈[0,T]
V, let D = {(t,x) ∈ (0,T)×(0,∞): x < u (t)}, assume for all (t,x) ∈ D that
2
∂u1(t,x) = ∂2u1(t,x), ∂u1(t,0) = g(t), u (t,u (t)) = h (t), (56)
∂t ∂x2 ∂x 1 2 0
∂u1(t,u (t)) = h (t), u (0,x) = ϕ(x), and u (0) = ψ, (57)
∂x 2 1 1 2
let a ∈ C2(R,R), d ,d ,L ,L ∈ N, l1,l1,...,l1 ,l2,l2,...,l2 ∈ N satisfy for all j ∈ {1,2} that
1 2 1 2 0 1 L1 0 1 L2
lj = 3−j, lj = 1, d = (cid:80)Lj lj(lj +1), for every n ∈ N let L : Rd1+d2 ×Ω → R satisfy for all
0 Lj j k=1 k k−1 n
θ = (θ ,...,θ ) ∈ Rd1+d2, (t,x) ∈ D that
1 d1+d2
L(θ,t,x) = (cid:16) |(∂ Nl 01,l 11,...,l L1 1 )(t,x)−( ∂2 Nl 01,l 11,...,l L1 1 )(t,x)|2 +|( ∂ Nl 01,l 11,...,l L1 1 )(t,0)−g(t)|2
∂t a,(θ1,θ2,...,θd1) ∂x2 a,(θ1,θ2,...,θd1) ∂x a,(θ1,θ2,...,θd1)
l1,l1,...,l1 l2,l2,...,l2
+|N 0 1 L1 (t,N 0 1 L2 (t))−h (t)|2
a,(θ1,θ2,...,θd1) a,(θd1+1,θd1+2,...,θd1+d2) 0
l1,l1,...,l1 l2,l2,...,l2
+|( ∂ N 0 1 L1 )(t,N 0 1 L2 )−h (t)|2
∂x a,(θ1,θ2,...,θd1) a,(θd1+1,θd1+2,...,θd1+d2 1
l1,l1,...,l1 l2,l2,...,l2 (cid:17)
+|N 0 1 L1 (0,x)−ϕ(x)|2 +|N 0 1 L2 (0)−ψ|2 ,
a,(θ1,θ2,...,θd1) a,(θd1+1,θd1+2,...,θd1+d2)
(58)
(cf. Definition 1.3 and Lemma 2.6), let G: Rd1+d2×[0,T]×R → Rd1+d2 satisfy for all t ∈ [0,T], x ∈ R,
θ ∈ {ϑ ∈ Rd1+d2: L(·,t,x) is differentiable at ϑ} that G(t,θ,x,y) = (∇ L)(t,θ,x,y), let (Ω,F,P) be
θ
a probability space, let M,M ∈ N, let T : Ω → D, m ∈ {1,2,...,M}, be i.i.d. random variables,
m
let X : Ω → ∂D, m ∈ {1,2,...,M}, be i.i.d. random variables, for every n ∈ N, m ∈ {1,2,...,M}
m
let ξ : Ω → {1,2,...,M} be a random variable, assume that (T ) , (X ) , and
n,m m m∈{1,2,...,M} m m∈{1,2,...,M}
(ξ n,m)
(n,m)∈N×{1,2,...,M}
are independent, let (γ n)
n∈N
⊆ R, and let Θ: N
0
×Ω → Rd1+d2 satisfy for all
n ∈ N that
0
(cid:34) (cid:35)
M
γ (cid:88)
Θ = Θ − n G(Θ ,T ,X ,) . (59)
n+1 n
M
n−1 ξn,m ξn,m
m=1
Remark 2.15 (Explanations for Method description 2.14). Loosely speaking, in Method descrip-
tion 2.14 we think for sufficiently large N ∈ N for all θ = (θ ,...,θ ): Ω → Rd1+d2 with θ = Θ
1 d1+d2 N
l1,l1,...,l1 (cid:12) l2,l2,...,l2 (cid:12)
of the ANN realizations N 0 1 L1 (cid:12) and N 0 1 L2 (cid:12) as an approximation
a,(θ1,θ2,...,θd1) [0,T]×R a,(θd1+1,θd1+2,...,θd1+d2) [0,T]
(cid:0) Nl 01,l 11,...,l L1
1
(cid:12)
(cid:12) ,
Nl 02,l 12,...,l L2
2
(cid:12)
(cid:12)
(cid:1)
≈ (u ,u )
(60)
a,(θ1,θ2,...,θd1) [0,T]×R a,(θd1+1,θd1+2,...,θd1+d2) [0,T] 1 2
of the solution u ∈ V of the Stefan problem in (56) and (57).
123 Machine learning approximation methods for PDEs based
on stochastic Feynman-Kac-type representations
InthissectionwediscussmachinelearningapproximationmethodsforPDEswhichemployFeynman-
Kac-type formulas in their derivation. We first introduce in Section 3.1 a refinement of the abstract
reformulation result in Section 2.1 involving conditional expectations. We then recall linear and
nonlinear Feynman-Kac formulas in Section 3.2 upon which the machine learning methods discussed
inthissectionarebased. Thereafter, inSection3.3wediscussmethodswhichrelyonlinearFeynman-
Kac formulas and in Section 3.4 we discuss methods which rely on nonlinear Feynman-Kac formulas.
3.1 Basic reformulation result for machine learning methods for PDEs II
As discussed in Section 2.1, machine learning methods for PDEs generally involve the transformation
of PDE problems into stochastic optimization problems. In Corollary 2.2 in Section 2.1 we intro-
duced a basic reformulation result which can be used to derive such transformations. Corollary 2.2 is
howevernotsufficientforsomeofthetransformationsconsideredinSection3whichinvolveFeynman-
Kac-typeformulas. Itisthesubjectofthissectiontointroducearefinedabstractreformulationresult
in Theorem 3.3 below which involves conditional expectations and is suitable for the derivation of
some of the machine learning methods for PDEs based on Feynman-Kac-type formulas. To prove
Theorem 3.3 we need two elemantary and well-known results presented in Proposition 3.1 and Corol-
lary 3.2 below (cf., e.g., [48, Theorem 8.20]). Furthermore, in Corollary 3.4 below we demonstrate
how the reformulation result in Corollary 2.2 above can be derived as a consequence of the more
general result in Theorem 3.3 below.
Proposition 3.1. Let (Ω,F,P) be a probability space, let G ⊆ F be a sigma-algebra, let N ∈ N, and
let X: Ω → RN be a random variable with E[∥X∥2] < ∞. Then
(i) it holds for all G-measurable Y : Ω → RN that
E(cid:2) ∥X −E[X|G]∥2(cid:3) ≤ E(cid:2) ∥X −E[X|G]∥2(cid:3) +E(cid:2) ∥E[X|G]−Y∥2(cid:3) = E(cid:2) ∥X −Y∥2(cid:3) , (61)
(ii) it holds for all G-measurable Y : Ω → RN with P(Y = E[X|G]) < 1 that
E(cid:2) ∥X −E[X|G]∥2(cid:3) < E(cid:2) ∥X −Y∥2(cid:3) , (62)
and
(iii) it holds that
E(cid:2)
∥X
−E[X|G]∥2(cid:3)
= inf
E(cid:2)
∥X
−Y∥2(cid:3)
.
(63)
Y:Ω→RN
Y is G-measurable
Proof of Proposition 3.1. Throughout this proof let e ,e ,...,e ∈ RN satisfy e = (1,0,...,0),
1 2 N 1
e = (0,1,0,...,0), ..., e = (0,...,0,1). Note that Jensen’s inequality for conditional expectations
2 N
(see, for example, Klenke [48, Theorem 8.20]), the tower property for conditional expectations, and
the assumption that E[∥X∥2] < ∞ imply that
(cid:20) (cid:21) (cid:20) (cid:21)
E(cid:2) ∥E[X|G]∥2(cid:3) = E (cid:80)N |⟨e ,E[X|G]⟩|2 = E (cid:80)N (cid:12) (cid:12)E(cid:2) ⟨e ,X⟩|G(cid:3)(cid:12) (cid:12)2
n n
n=1 n=1 (64)
(cid:20) (cid:21)
N
≤ E (cid:80) E(cid:2) |⟨e ,X⟩|2|G(cid:3) ≤ E(cid:2) |E[∥X∥|G]|2(cid:3) = E[E[∥X∥2|G]] = E(cid:2) ∥X∥2(cid:3) < ∞.
n
n=1 13The tower property for conditional expectations therefore shows that for all G-measurable Y : Ω →
RN it holds that
E[∥X −Y∥2] = E[∥(X −E[X|G])+(E[X|G]−Y)∥2]
= E(cid:2) ∥X −E[X|G]∥2 +2⟨X −E[X|G],E[X|G]−Y⟩+∥E[X|G]−Y∥2(cid:3)
(65)
=
E(cid:2)
∥X
−E[X|G]∥2(cid:3) +2E(cid:2)
⟨X
−E[X|G],E[X|G]−Y⟩(cid:3) +E(cid:2) ∥E[X|G]−Y∥2(cid:3)
(cid:104) (cid:105)
= E(cid:2) ∥X −E[X|G]∥2(cid:3) +2E E(cid:2) ⟨X −E[X|G],E[X|G]−Y⟩(cid:12) (cid:12)G(cid:3) +E(cid:2) ∥E[X|G]−Y∥2(cid:3) .
Hence, we obtain that
E[∥X −Y∥2]
= E(cid:2) ∥X −E[X|G]∥2(cid:3) +2E(cid:2) ⟨E[X −E[X|G]|G],E[X|G]−Y⟩(cid:3) +E(cid:2) ∥E[X|G]−Y∥2(cid:3)
(66)
=
E(cid:2)
∥X
−E[X|G]∥2(cid:3) +2E(cid:2) ⟨E[X|G]−E[X|G],E[X|G]−Y⟩(cid:3) +E(cid:2) ∥E[X|G]−Y∥2(cid:3)
=
E(cid:2)
∥X
−E[X|G]∥2(cid:3) +E(cid:2) ∥E[X|G]−Y∥2(cid:3)
.
This proves item (i). Observe that item (i) demonstrates items (ii) and (iii). The proof of Proposi-
tion 3.1 is thus complete.
Corollary 3.2. Let (Ω,F,P) be a probability space, let G ⊆ F be a sigma-algebra, let N ∈ N, and
let X = (X ,...,X ): Ω → RN be a random variable with E[∥X∥2] < ∞. Then
1 N
(cid:32) (cid:33)
N
E(cid:2) ∥X −E[X|G]∥2(cid:3) = (cid:88) inf E(cid:2) |X −Y|2(cid:3) . (67)
n
Y:Ω→R
n=1 Yis G-measurable
Proof of Corollary 3.2. Note that Proposition 3.1 (applied for every n ∈ {1,2,...,N} with N ↶ 1,
X ↶ X in the notation of Proposition 3.1) establishes that for all n ∈ {1,2,...,N} it holds that
n
E[|X −E[X |G]|2] = inf E(cid:2) |X −Y|2(cid:3) . (68)
n n n
Y:Ω→R
YisG-measurable
Combining this with the fact that E(cid:2) ∥X −E[X|G]∥2(cid:3) = (cid:80)N E[|X −E[X |G]|2] ensures (67). The
n=1 n n
proof of Corollary 3.2 is thus complete.
Theorem 3.3 (Basic reformulation result for machine learning methods for PDEs II). Let V and S
be non-empty sets with S ⊆ V, let (Ω,F,P) be a probability space, for every k ∈ {1,2} let (R ,R )
k k
be a measurable space, for every k ∈ {1,2} let R : Ω → R be a random variable, let N ∈ N,
k k
for every v ∈ V let U : R → RN be measurable, let ϕ: R × R → RN be measurable, assume
v 1 1 2
E[∥ϕ(R ,R )∥2] < ∞, assume for all v ∈ V that
1 2
P(U (R ) = E[ϕ(R ,R )|R ]) = 1 (69)
v 1 1 2 1
if and only if v ∈ S, and let l: V → [0,∞] satisfy for all v ∈ V that
l(v) = E[∥ϕ(R ,R )−U (R )∥2]. (70)
1 2 v 1
Then
(i) for all v ∈ V it holds that
l(v) = inf l(w) (71)
w∈V
if and only if v ∈ S and
14(ii) there exists v ∈ V such that
l(v) = inf l(w). (72)
w∈V
Proof of Theorem 3.3. Throughout this proof let G ⊆ F be the sigma-algebra generated by R .
1
Observe that (70) implies that for all v,w ∈ V with P(U (R ) = E[ϕ(R ,R )|R ]) = P(U (R ) =
v 1 1 2 1 w 1
E[ϕ(R ,R )|R ]) = 1 it holds that
1 2 1
l(v) = E[∥ϕ(R ,R )−U (R )∥2] = E[∥ϕ(R ,R )−E[ϕ(R ,R )|R ]∥2]
1 2 v 1 1 2 1 2 1 (73)
= E[∥ϕ(R ,R )−U (R )∥2] = l(w).
1 2 w 1
Next note that item (ii) in Proposition 3.1 (applied for every w ∈ V with (Ω,F,P) ↶ (Ω,F,P),
G ↶ G, N ↶ N, X ↶ (Ω ∋ ω (cid:55)→ ϕ(R (ω),R (ω)) ∈ RN), Y ↶ (Ω ∋ ω (cid:55)→ U (R (ω)) ∈ RN) in the
1 2 w 1
notationofProposition3.1)and(70)showthatforallv,w ∈ V withP(U (R ) = E[ϕ(R ,R )|R ]) = 1
v 1 1 2 1
and P(U (R ) = E[ϕ(R ,R )|R ]) < 1 it holds that
w 1 1 2 1
l(v) = E[∥ϕ(R ,R )−U (R )∥2] = E[∥ϕ(R ,R )−E[ϕ(R ,R )|R ]∥2]
1 2 v 1 1 2 1 2 1 (74)
< E[∥ϕ(R ,R )−U (R )∥2] = l(w).
1 2 w 1
Combining this and (73) proves that for all v,w ∈ V with P(U (R ) = E[ϕ(R ,R )|R ]) = 1 it holds
v 1 1 2 1
that
l(v) ≤ l(w). (75)
Therefore, we obtain that for all v ∈ V with P(U (R ) = E[ϕ(R ,R )|R ]) = 1 it holds that
v 1 1 2 1
l(v) = inf l(w). (76)
w∈V
Combining this with (69) demonstrates that for all v ∈ S it holds that
l(v) = inf l(w). (77)
w∈V
Furthermore,observethat(74)and(76)establishthatforallv,w ∈ V withP(U (R ) = E[ϕ(R ,R )|R ]) =
v 1 1 2 1
1 and P(U (R ) = E[ϕ(R ,R )|R ]) < 1 it holds that
w 1 1 2 1
inf l(z) = l(v) < l(w). (78)
z∈V
Moreover, note that (69) and the assumption that S ̸= ∅ ensure that there exists v ∈ V such
that P(U (R ) = E[ϕ(R ,R )|R ]) = 1. Combining this with (78) implies that for all w ∈ V with
v 1 1 2 1
P(U (R ) = E[ϕ(R ,R )|R ]) < 1 it holds that
w 1 1 2 1
inf l(z) < l(w). (79)
z∈V
Hence,weobtainthatforallv ∈ V with l(v) = inf l(z)itholdsthatP(U (R ) = E[ϕ(R ,R )|R ]) =
z∈V v 1 1 2 1
1. This and (69) show that for all v ∈ V with l(v) = inf l(z) it holds that v ∈ S. Combining
z∈V
this with (77) proves item (i). Observe that item (i) and the assumption that S ̸= ∅ demonstrate
item (ii). The proof of Theorem 3.3 is thus complete.
Note that the next result is identical to Corollary 2.2 in Section 2.1.
15Corollary 3.4 (Basic reformulation result for machine learning methods for PDEs I). Let S and V
be non-empty sets with S ⊆ V, let (Ω,F,P) be a probability space, let (R,R) be a measurable space,
let R: Ω → R be a random variable, let N ∈ N, for every v ∈ V let U : R → RN be measurable,
v
assume for all v ∈ V that P(U (R) = 0) = 1 if and only if v ∈ S, and let l: V → [0,∞] satisfy for
v
all v ∈ V that l(v) = E[∥U (R)∥2]. Then
v
(i) for all v ∈ V it holds that
l(v) = inf l(w) (80)
w∈V
if and only if v ∈ S and
(ii) there exists v ∈ V such that
l(v) = inf l(w). (81)
w∈V
Proof of Corollary 3.4. Note that Theorem 3.3 (applied with V ↶ V, S ↶ S, (R ,R ) ↶ (R,R),
1 1
(R ,R ) ↶ (R,R), R ↶ R, R ↶ R, N ↶ N, (U ) ↶ (U ) , ϕ ↶ (R×R ∋ (x,y) (cid:55)→ 0 ∈ RN)
2 2 1 2 v v∈V v v∈V
in the notation of Theorem 3.3) establishes items (i) and (ii). The proof of Corollary 3.4 is thus
complete.
3.2 Linear and nonlinear stochastic representations (Feynman-Kac for-
mulas) for PDEs
In this section we recall well-known Feynman-Kac formulas for PDEs. We first prove a nonlinear
Feynman-Kac formula for semilinear parabolic PDEs in Proposition 3.5 and thereafter obtain as a
consequence linear Feynman-Kac formulas for heat PDEs in Corollaries 3.8, 3.9, and 3.10.
Proposition 3.5 (Nonlinear Feynman-Kac Formula). Let T ∈ (0,∞), d ∈ N, let µ: [0,T]×Rd →
Rd, σ: [0,T]×Rd → Rd×d, and f: [0,T]×Rd×R×Rd → R be continuous, let u ∈ C1,2([0,T]×Rd)
satisfy4 for all t ∈ [0,T], x ∈ Rd that
(cid:0) (cid:1)
∂u(t,x)+ 1Tr (σ(t,x)[σ(t,x)]∗(Hess u)(t,x) + ∂u(t,x)µ(t,x)
∂t 2 x ∂x
+f(cid:0) t,x,u(t,x),[σ(t,x)]∗(∇ u)(t,x)(cid:1) = 0, (82)
x
let (Ω,F,P) be a probability space with a normal5 filtration (F ) , let B: [0,T] × Ω → Rd be
t t∈[0,T]
a standard (F ) -Brownian motion, let X: [0,T] × Ω → Rd be an (F ) -adapted stochastic
t t∈[0,T] t t∈[0,T]
process with continuous sample paths which satisfies that for all t ∈ [0,T] it holds P-a.s. that
(cid:90) t (cid:90) t
X = X + µ(s,X )ds+ σ(s,X )dB , (83)
t 0 s s s
0 0
and let Y : [0,T]×Ω → R and Z: [0,T]×Ω → Rd be (F ) -adapted stochastic processes with con-
t t∈[0,T]
tinuous sample paths which satisfy for all t ∈ [0,T] that Y = u(t,X ) and Z = [σ(t,X )]∗(∇ u)(t,X ).
t t t t x t
Then for all t ∈ [0,T], s ∈ [0,t] it holds P-a.s. that
(cid:90) t (cid:90) t
Y −Y + f(r,X ,Y ,Z )dr− (Z )∗dB = 0. (84)
t s r r r r r
s s
4Observe that for every d∈N and every d×d-matrix M ∈Rd×d it holds that M∗ is the transpose of M.
5Note that for every T ∈(0,∞) and every filtered probability space (Ω,F,(F ) ,P) it holds that (F ) is
t t∈[0,T] t t∈[0,T]
a normal filtration on (Ω,F,P) if and only if it holds for all t ∈ [0,T) that {A ∈ F: P(A) = 0} ⊆ F = (∩ F )
t s∈(t,T] s
(cf., for instance, Prévôt & Röckner [71, Definition 2.1.11]).
16Proof of Proposition 3.5. Observe that (83) and Itô’s formula (cf., for example, Klenke [48, Theo-
rem 25.27]) ensure that for all t ∈ [0,T], s ∈ [0,t] it holds P-a.s. that
Y = u(t,X )
t t
(cid:90) t
= u(s,X )+ ∂u(r,X )+ ∂u(r,X )µ(r,X )dr
s ∂r r ∂x r r (85)
s
(cid:90) t (cid:90) t
(cid:0) (cid:1)
+ ∂u(r,X )σ(r,X )dB + 1 Tr (σ(r,X )[σ(r,X )]∗(Hess u)(r,X ) dr.
∂x r r r 2 r r x r
s s
Combining this with (82) implies that for all t ∈ [0,T], s ∈ [0,t] it holds P-a.s. that
(cid:90) t
(cid:2) (cid:0) (cid:1)
Y = u(s,X )− 1Tr (σ(r,X )[σ(r,X )]∗(Hess u)(r,X ) + ∂u(r,X )µ(r,X )
t s 2 r r x r ∂x r r
s
(cid:90) t
(cid:0) (cid:1)(cid:3)
+f r,X ,u(r,X ),[σ(r,X )]∗(∇ u)(r,X ) dr+ ∂u(r,X )µ(r,X )dr
r r r x r ∂x r r
s
(cid:90) t (cid:90) t
+ ∂u(r,X )σ(r,X )dB + 1 Tr(cid:0) (σ(r,X )[σ(r,X )]∗(Hess u)(r,X )(cid:1) dr (86)
∂x r r r 2 r r x r
s s
(cid:90) t (cid:90) t
(cid:0) (cid:1)
= Y − f r,X ,u(r,X ),[σ(r,X )]∗(∇ u)(r,X ) dr+ ∂u(r,X )σ(r,X )dB
s r r r x r ∂x r r r
s s
(cid:90) t (cid:90) t
(cid:0) (cid:1)
= Y − f r,X ,Y ,Z dr+ (Z )∗dB .
s r r r r r
s s
This shows (84). The proof of Proposition 3.5 is thus complete.
Lemma 3.6 (Right continuity of the induced filtration). Let T ∈ (0,∞), let (Ω,F,P) be a probability
space, let (F ) be a filtration on (Ω,F,P), and for every t ∈ [0,T] let G ⊆ F satisfy G =
t t∈[0,T] t t
∩ F . Then
s∈(t,T]∪{T} s
(i) it holds for all t ∈ [0,T] that F ⊆ G ,
t t
(ii) it holds for all s,t ∈ [0,T] with s < t that F ⊆ G ⊆ F ⊆ G , and
s s t t
(iii) it holds for all t ∈ [0,T] that
G = ∩ G . (87)
t s∈(t,T]∪{T} s
Proof of Lemma 3.6. Note that the fact that for all t ∈ [0,T] it holds that G = (∩ F ) ⊇ F
t s∈(t,T]∪{T} s t
proves item (i). Observe that the assumption that for all t ∈ [0,T] it holds that G = ∩ F
t s∈(t,T]∪{T} s
demonstrates that for all t,r ∈ [0,T] with t < r it holds that
G = (∩ F ) = (∩ F ) ⊆ (∩ F ) ⊆ F . (88)
t s∈(t,T]∪{T} s s∈(t,T] s s∈(t,r] s r
This and item (i) establish item (ii). Note that item (ii) and the fact that G = F ensure that for
T T
all t ∈ [0,T], r ∈ (t,T]∪{T} it holds that
(∩ G ) ⊆ (∩ G ) ⊆ G ⊆ F . (89)
s∈(t,T]∪{T} s s∈(t,(r+t)/2]∪{T} s (r+t)/2 r
Combining this with item (i) implies that for all t ∈ [0,T] it holds that
(∩ F ) ⊆ (∩ G ) ⊆ (∩ F ) = G . (90)
s∈(t,T]∪{T} s s∈(t,T]∪{T} s r∈(t,T]∪{T} r t
The proof of Lemma 3.6 is thus complete.
17Proposition 3.7 (Construction of a normal filtration). Let T ∈ (0,∞), let (Ω,F,P) be a probability
space, let (F ) be a filtration on (Ω,F,P), for every t ∈ [0,T] let G ⊆ F be the sigma-algebra
t t∈[0,T] t
generated by F ∪{A ∈ F: P(A) = 0}, and for every t ∈ [0,T] let H ⊆ F satisfy H = ∩ G .
t t t s∈(t,T]∪{T} s
Then
(i) it holds that (H ) is a normal filtration on (Ω,F,P) and
t t∈[0,T]
(ii) it holds for all normal filtrations (I ) on (Ω,F,P) with ∀t ∈ [0,T]: F ⊆ I that ∀t ∈
t t∈[0,T] t t
[0,T]: H ⊆ I .
t t
Proof of Proposition 3.7. First, observe that the assumption that for all t ∈ [0,T] it holds that
G ⊆ F is the sigma-algebra generated by F ∪ {A ∈ F: P(A) = 0} shows that for all t ∈ [0,T] it
t t
holds that
{A ∈ F: P(A) = 0} ⊆ G . (91)
t
Item (i) in Lemma 3.6 (applied with (F ) ↶ (G ) , (G ) ↶ (H ) in the notation
t t∈[0,T] t t∈[0,T] t t∈[0,T] t t∈[0,T]
of Lemma 3.6) therefore proves that for all t ∈ [0,T] it holds that
{A ∈ F: P(A) = 0} ⊆ H . (92)
t
Combining this with items (ii) and (iii) in Lemma 3.6 demonstrates item (i). Note that for all normal
filtrations (I ) with ∀t ∈ [0,T]: F ⊆ I it holds for all t ∈ [0,T] that
t t∈[0,T] t t
(cid:0)F ∪{A ∈ F: P(A) = 0}(cid:1) ⊆ I and I = ∩ I . (93)
t t t s∈(t,T]∪{T} s
This and the fact that for all t ∈ [0,T] it holds that G ⊆ F is the sigma-algebra generated by
t
F ∪ {A ∈ F: P(A) = 0} establish that for all normal filtrations (I ) on (Ω,F,P) with ∀t ∈
t t t∈[0,T]
[0,T]: F ⊆ I it holds that ∀t ∈ [0,T]: G ⊆ I . Combining this with (93) ensures that for all normal
t t t t
filtrations (I ) on (Ω,F,P) with ∀t ∈ [0,T]: F ⊆ I it holds that
t t∈[0,T] t t
∀t ∈ [0,T]: H = (∩ G ) ⊆ (∩ I ) = I . (94)
t s∈(t,T]∪{T} s s∈(t,T]∪{T} s t
The proof of Proposition 3.7 is thus complete.
√
Corollary 3.8. Let T,ρ ∈ (0,∞), d ∈ N, ϱ = 2ρ, let u ∈ C1,2([0,T]×Rd) have at most polynomi-
ally growing partial derivatives, assume for all t ∈ [0,T], x ∈ Rd that
∂u(t,x) = ρ∆ u(t,x), (95)
∂t x
let (Ω,F,P) be a probability space, and let B: [0,T]×Ω → Rd be a standard Brownian motion. Then
it holds for all t ∈ [0,T], x ∈ Rd that
u(t,x) = E(cid:2) u(0,x+ϱB )(cid:3) . (96)
t
Proof of Corollary 3.8. Throughout this proof let (F ) be the normal filtration generated by
t t∈[0,T]
B (cf., for instance, Proposition 3.7). Proposition 3.5 (applied for every T ∈ (0,T], x ∈ Rd with
T ↶ T , d ↶ d, µ ↶ ([0,T ] × Rd ∋ (t,v) (cid:55)→ 0 ∈ Rd), σ ↶ ([0,T ] × Rd ∋ (t,v) (cid:55)→ ϱI ∈ Rd×d),
d
f ↶ ([0,T ]×Rd ×R×Rd ∋ (t,v,y,z) (cid:55)→ 0 ∈ R), u ↶ ([0,T ]×Rd ∋ (t,v) (cid:55)→ u(T −t,v) ∈ R),
(Ω,F,P) ↶ (Ω,F,P), (F ) ↶ (F ) , X ↶ ([0,T ]×Ω ∋ (t,ω) (cid:55)→ x+ϱB (ω) ∈ Rd) in the
t t∈[0,T] t t∈[0,T] t
18notation of Proposition 3.5) implies that for all T ∈ (0,T], t ∈ [0,T ], s ∈ [0,t], x ∈ Rd it holds P-a.s.
that
(cid:90) t
u(T −t,x+ϱB ) = u(T −s,x+ϱB )− ϱ[(∇ u)(r,x+ϱB )]∗dB . (97)
t s x r r
s
Hence, we obtain that for all T ∈ (0,T], x ∈ Rd it holds P-a.s. that
(cid:90) T
u(0,x+ϱB ) = u(T ,x)− ϱ[(∇ u)(r,x+ϱB )]∗dB . (98)
T x r r
0
Therefore, we obtain that for all t ∈ [0,T], x ∈ Rd it holds P-a.s. that
(cid:90) t
u(0,x+ϱB ) = u(t,x)− ϱ[(∇ u)(r,x+ϱB )]∗dB . (99)
t x r r
0
Next observe that the assumption that u ∈ C1,2([0,T]×Rd) has at most polynomially growing partial
derivatives shows that there exists c ∈ (0,∞) such that for all r ∈ [0,T], x ∈ Rd it holds that
∥(∇ u)(r,x)∥ ≤ c(1+∥x∥c). (100)
x
Combining this with the fact that for all c ∈ (0,∞) it holds that sup E[∥B ∥c] < ∞ proves that
r∈[0,T] r
for all t ∈ [0,T], x ∈ Rd it holds that
(cid:20)(cid:90) t (cid:21)
E ∥ϱ(∇ u)(r,x+ϱB )∥2dr < ∞. (101)
x r
0
This demonstrates that for all t ∈ [0,T], x ∈ Rd it holds that
(cid:34) (cid:35)
(cid:12)(cid:90) t (cid:12)2
E (cid:12) (cid:12) ϱ[(∇ u)(r,x+ϱB )]∗dB (cid:12) (cid:12) < ∞. (102)
x r r
(cid:12) (cid:12)
0
This and (99) establish that for all t ∈ [0,T], x ∈ Rd it holds that
E(cid:2) |u(0,x+ϱB )|(cid:3) < ∞. (103)
t
Combining this with (99), (101), and (102) ensures that for all t ∈ [0,T], x ∈ Rd it holds that
E(cid:2) u(0,x+ϱB )(cid:3) = u(t,x). (104)
t
This implies (96). The proof of Corollary 3.8 is thus complete.
√
Corollary 3.9. Let T,ρ ∈ (0,∞), d ∈ N, ϱ = 2ρ, let u ∈ C1,2([0,T]×Rd) have at most polynomi-
ally growing partial derivatives, let φ: Rd → R be a function, assume for all t ∈ [0,T], x ∈ Rd that
u(0,x) = φ(x) and
∂u(t,x) = ρ∆ u(t,x), (105)
∂t x
let (Ω,F,P) be a probability space, let B: Ω → Rd be a standard normal random variable, let ξ: Ω →
Rd and τ: Ω → [0,T] be random variables, assume that B, ξ, and τ are independent, and let G ⊆ F
be the sigma-algebra generated by τ and ξ. Then it holds P-a.s. that
√
u(τ,ξ) = E(cid:2) φ(ϱ τB+ξ)|G(cid:3) . (106)
19Proof of Corollary 3.9. Throughout this proof let (Ω˜ ,F˜ ,P˜ ) be a probability space and let B: [0,T]×
Ω˜ → Rd be a standard Brownian motion. Note that Corollary 3.8 (applied with T ↶ T, d ↶ d,
ρ ↶ ρ, u ↶ u, (Ω,F,P) ↶ (Ω˜ ,F˜ ,P˜ ), B ↶ B in the notation of Corollary 3.8), the fact that for all
x ∈ Rd it holds that u(0,x) = φ(x), and (105) show that for all t ∈ [0,T], x ∈ Rd it holds that
(cid:90) (cid:90)
u(t,x) = u(cid:0) 0,x+ϱB (ω)(cid:1)P˜ (dω) = φ(x+ϱB (ω))P˜ (dω). (107)
t t
Ω˜ Ω˜
√
Combining this and the fact that for all t ∈ [0,T], A ∈ B(Rd) it holds that P˜ (B ∈ A) = P( tB ∈
t
A) proves that for all t ∈ [0,T], x ∈ Rd it holds that
(cid:90) (cid:90) √ √
u(t,x) = φ(x+ϱB (ω))P˜ (dω) = φ(x+ tB(ω))P(dω) = E(cid:2) φ(x+ϱ tB)(cid:3) . (108)
t
Ω˜ Ω
Next observe that the assumption that u ∈ C1,2([0,T]×Rd) and the assumption that G is the sigma-
algebra generated by τ and ξ demonstrate that Ω ∋ ω (cid:55)→ u(τ(ω),ξ(ω)) ∈ R is G/B(R)-measurable.
Furthermore, note that the factorization lemma for conditional expectations, for example, in [40,
Corollary 2.9] (applied with (Ω,F,P) ↶ (Ω,F,P), G ↶ G, (X,X) ↶ (Rd,B(Rd)), (Y,Y) ↶ ([0,T]×
Rd,B([0,T]×Rd)), X ↶ B, Y ↶ (Ω ∋ ω (cid:55)→ (τ(ω),ξ(ω)) ∈ [0,T]×Rd), Φ ↶ (Rd ×[0,T]×Rd ∋
√
(x,t,y) (cid:55)→ φ(ϱ tx+y) ∈ R), ϕ ↶ u in the notation of [40, Corollary 2.9]) and (108) establish that
for all A ∈ G it holds that
√
E(cid:2) u(τ,ξ)id (cid:3) = E(cid:2) φ(ξ +ϱ τB)id (cid:3) . (109)
A A
Combining this with the fact that Ω ∋ ω (cid:55)→ u(τ(ω),ξ(ω)) ∈ R is G/B(R)-measurable ensures (106).
The proof of Corollary 3.9 is thus complete.
√
Corollary 3.10. Let T,ρ ∈ (0,∞), d ∈ N, ϱ = 2Tρ, let u ∈ C1,2([0,T] × Rd) have at most
polynomially growing partial derivatives, let φ: Rd → R be a function, assume for all t ∈ [0,T],
x ∈ Rd that u(0,x) = φ(x) and
∂u(t,x) = ρ∆ u(t,x), (110)
∂t x
let (Ω,F,P) be a probability space, let B: Ω → Rd be a standard normal random variable, let ξ: Ω →
Rd be a random variable, assume that B and ξ are independent, and let G ⊆ F be the sigma-algebra
generated by ξ. Then it holds P-a.s. that
u(T,ξ) = E(cid:2) φ(ϱB+ξ)|G(cid:3) . (111)
Proof of Corollary 3.10. ObservethatCorollary3.9(appliedwithT ↶ T,d ↶ d,x ↶ x,(Ω,F,P) ↶
(Ω,F,P), B ↶ B, ξ ↶ ξ, τ ↶ (Ω ∋ ω (cid:55)→ T ∈ [0,T]) in the notation of Corollary 3.9) implies (111).
The proof of Corollary 3.10 is thus complete.
3.3 Deep Kolmogorov methods
In this section we discuss the deep Kolmogorov methods introduced in [2] for solving linear Kol-
mogorov PDEs. Specifically, we present two reformulation results upon which deep Kolmogorov
methods are based. The first reformulation result in Theorem 3.11 casts terminal values of a heat
PDEsassolutionsofinfinite-dimensionalstochasticoptimizationproblems. Thesecondreformulation
result in Theorem 3.12 casts the entire solution of a heat PDEs as a solution of infinite-dimensional
stochastic optimization problem. Obtaining deep Kolmogorov methods from these reformulation
20results consists of replacing these infinite-dimensional stochastic optimization problems by finite-
dimensional stochastic optimization problems over the parameters of ANNs and thereafter applying
an SGD-type method to solve the finite-dimensional stochastic optimization problems (see, e.g., [2]
and [3, Section 2]).
Theorem 3.11 (Deep Kolmogorov methods for terminal values of heat PDEs). Let T,ρ ∈ (0,∞),
√
d ∈ N, ϱ = 2Tρ, let φ: Rd → R be a function, let u ∈ C1,2([0,T]×Rd,R) have at most polynomially
growing partial derivatives, assume for all t ∈ [0,T], x ∈ Rd that u(0,x) = φ(x) and
∂u(t,x) = ρ∆ u(t,x), (112)
∂t x
let (Ω,F,P) be a probability space, let B: Ω → Rd and ξ: Ω → Rd be standard normal random
variables, assume that B and ξ are independent, let V = C(Rd,R), and let l: V → [0,∞] satisfy for
all v ∈ V that
l(v) = E(cid:2) |φ(ϱB+ξ)−v(ξ)|2(cid:3) . (113)
Then
(i) for all v ∈ V it holds that
l(v) = inf l(w) (114)
w∈V
if and only if it holds for all x ∈ Rd that v(x) = u(T,x) and
(ii) there exists v ∈ V such that
l(v) = inf l(w). (115)
w∈V
Proof of Theorem 3.11. Throughout this proof let λ: B(Rd) → [0,∞] be the Lebesgue-Borel mea-
sure, let S ⊆ V satisfy
S = (cid:8) v ∈ V : sup |u(T,x)−v(x)| = 0(cid:9) , (116)
x∈Rd
and let G ⊆ F be the sigma-algebra generated by ξ. Corollary 3.10 (applied with u ↶ u, φ ↶ φ,
(Ω,F,P) ↶ (Ω,F,P), B ↶ B, ξ ↶ ξ, G ↶ G in the notation of Corollary 3.10) and (116) show that
for all v ∈ S it holds P-a.s. that
√
v(ξ) = E(cid:2) φ(ϱ TB+ξ)|G(cid:3) . (117)
Hence, we obtain that for all v ∈ S it holds that
√
P(v(ξ)−E[φ(ϱ TB+ξ)|G] = 0) = 1. (118)
√
Next note that Corollary 3.10 proves that for all v ∈ V with P(v(ξ)−E[φ(ϱ TB+ξ)|G] = 0) = 1 it
holds P-a.s. that
u(T,ξ) = v(ξ). (119)
Combining this with the assumption that ξ: Ω → Rd is a standard normal random variable demon-
√
strates that for all v ∈ V with P(v(T,ξ)−E[φ(ϱ TB+ξ)|G] = 0) = 1 it holds that
λ(cid:0) {x ∈ Rd : u(T,x) ̸= v(x)}(cid:1) = 0. (120)
The fact that Rd ∋ x (cid:55)→ u(T,x) − v(x) ∈ R is continuous establishes that for all v ∈ V with
√
P(v(T,ξ) − E[φ(ϱ TB + ξ)|G] = 0) = 1 it holds that sup |u(T,x) − v(x)| = 0. Theorem 3.3
x∈Rd √
(applied with (U ) ↶ (v) , R ↶ ξ, R ↶ B, R ↶ Rd, R ↶ Rd, ϕ ↶ ((ξ,B) (cid:55)→ φ(ϱ TB+ξ)
v v∈V v∈V 1 2 1 2
in the notation of Theorem 3.3) therefore ensures items (i) and (ii). The proof of Theorem 3.11 is
thus complete.
21Theorem 3.12 (Deep Kolmogorov methods for full solutions of heat PDEs). Let d ∈ N, T,ρ ∈
√
(0,∞), ϱ = 2ρ, let φ: Rd → R be a function, let u ∈ C1,2([0,T]×Rd,R) have at most polynomially
growing partial derivatives, assume for all t ∈ [0,T], x ∈ Rd that u(0,x) = φ(x) and
∂u(t,x) = ρ∆ u(t,x), (121)
∂t x
let (Ω,F,P) be a probability space, let B: Ω → Rd and ξ: Ω → Rd be standard normal random
variables, let τ: Ω → [0,T] be a continuous uniformly distributed random variable, assume that B, τ,
and ξ are independent, let V = C1,2([0,T]×Rd,R), and let l: V → [0,∞] satisfy for all v ∈ V that
√
l(v) = E(cid:2) |φ(ϱ τB+ξ)−v(τ,ξ)|2(cid:3) . (122)
Then
(i) for all v ∈ V it holds that
l(v) = inf l(w) (123)
w∈V
if and only if it holds for all t ∈ [0,T], x ∈ Rd that v(0,x) = φ(x) and
∂v(t,x) = ρ∆ v(t,x), (124)
∂t x
and
(ii) there exists v ∈ V such that
l(v) = inf l(w). (125)
w∈V
Proof of Theorem 3.12. Throughout this proof for every v ∈ V let F : [0,T] × Rd → R satisfy for
v
all t ∈ [0,T], x ∈ Rd that
F (t,x) = |v(0,x)−φ(x)|2 +|∂v(t,x)−∆ v(t,x)|2, (126)
v ∂t x
let S ⊆ V satisfy
S = {v ∈ V : sup F (t,x) = 0}, (127)
(t,x)∈[0,T]×Rd v
and let G ⊆ F be the sigma-algebra generated by τ and ξ. Corollary 3.9 (applied for every v ∈ S
with u ↶ v, φ ↶ φ, (Ω,F,P) ↶ (Ω,F,P), B ↶ B, τ ↶ τ, ξ ↶ ξ, G ↶ G in the notation of
Corollary 3.9) implies that for all v ∈ S it holds P-a.s. that
√
v(τ,ξ) = E(cid:2) φ(ϱ τB+ξ)|G(cid:3) . (128)
√
Hence, we obtain that for all v ∈ S it holds that P(v(τ,ξ)−E[φ(ϱ τB+ξ)|G] = 0) = 1. Combining
Corollary 3.9 and the assumption that for all t ∈ [0,T], x ∈ Rd it holds that u(0,x) = φ(x) and
∂u(t,x) = ρ∆ u(t,x) shows that it holds P-a.s. that
∂t x
√
u(τ,ξ) = E(cid:2) φ(ϱ τB+ξ)|G(cid:3) . (129)
√
Therefore, we obtain that for all v ∈ V with P(v(τ,ξ) − E[φ(ϱ τB + ξ)|G] = 0) = 1 it holds
P-a.s. that u(τ,ξ) = v(τ,ξ). Combining this with the assumption that ξ: Ω → Rd is a standard
normal random variable and the assumption that τ: Ω → [0,T] is a continuous uniformly distributed
√
random variable proves that for all v ∈ V with P(v(τ,ξ) − E[φ(ϱ τB + ξ)|G] = 0) = 1 it holds
for a.e. (t,x) ∈ [0,T] × Rd that u(t,x) = v(t,x). The assumption that u ∈ V hence demonstrates
√
that for all v ∈ V, (t,x) ∈ [0,T] × Rd with P(v(τ,ξ) − E[φ(ϱ τB + ξ)|G] = 0) = 1 it holds that
u(t,x) = v(t,x). Theorem 3.3 (applied with U ↶ v, R ↶ (τ,ξ), R ↶ B, R ↶ [0,T] × Rd,
√ v 1 2 1
R ↶ Rd, ϕ ↶ ((τ,ξ),B) (cid:55)→ φ(ϱ τB + ξ) in the notation of Theorem 3.3) therefore establishes
2
item (i) and item (ii). The proof of Theorem 3.12 is thus complete.
223.4 Deep BSDE methods
In this section we discuss the deep BSDE methods introduced in [19,32] for the approximation
of semilinear parabolic PDEs. First, we present some well-known uniqueness results for solutions
of BSDEs in Section 3.4.1. Thereafter, we introduce a reformulation result for semilinear parabolic
PDEsinSection3.4.2whichcaststerminalvaluesofsemilinearparabolicPDEsassolutionsofinfinite-
dimensionalstochasticoptimizationproblemsinvolvingBSDEs. ObtainingdeepBSDEmethodsfrom
this reformulation result then consists of discretizing the involved BSDEs, replacing the unknown
functionsintheBSDEsbyANNs, andapplyingSGD-typemethodstotheresultingfinite-dimensional
stochastic optimization problems.
3.4.1 Uniqueness for solutions of BSDEs
Lemma 3.13 (Uniqueness for solutions for BSDEs). Let T,M ∈ (0,∞), d ∈ N, let (Ω,F,P) be a
probability space with a normal filtration (F ) , let ξ: Ω → Rd be F -measurable with E[∥ξ∥2] <
t t∈[0,T] 0
∞, let B: [0,T] × Ω → Rd be a standard (F ) -Brownian motion, let µ: [0,T] × Rd → Rd,
t t∈[0,T]
σ: [0,T] × Rd → Rd×d, g: Rd → R, and f: [0,T] × Rd × R × Rd → R be Lipschitz continuous,
let X: [0,T] × Ω → Rd be an (F ) -adapted stochastic process with continuous sample paths
t t∈[0,T]
(w.c.s.p.), which satisfies that for every t ∈ [0,T] it holds P-a.s. that
(cid:90) t (cid:90) t
X = ξ + µ(s,X )ds+ σ(s,X )dB , (130)
t s s s
0 0
and assume (cid:82)T E(cid:2) ∥X ∥2(cid:3) ds < ∞, for every k ∈ {1,2} let Yk: [0,T]×Ω → R and Zk: [0,T]×Ω → Rd
0 s
be (F ) -adapted stochastic processes with continuous sample paths, which satisfy that for every
t t∈[0,T]
t ∈ [0,T] it holds P-a.s. that
(cid:90) T (cid:90) T
Yk −g(X )− f(s,X ,Yk,Zk)ds+ (Zk)∗dB = 0, (131)
t T s s s s s
t t
and assume
(cid:82)T E(cid:2)
|Y
|2(cid:3)
ds < ∞. Then
0 s
P(cid:0) Y1 = Y2(cid:1) = P(cid:0) Z1 = Z2(cid:1) = 1. (132)
Proof of Lemma 3.13. Observe that (131) ensures that for all s ∈ [0,T] it holds P-a.s. that
Y1 −Y2
s s
(cid:90) T (cid:104) (cid:105) (cid:90) T (133)
(cid:2) (cid:3)
= f(t,X ,Y1,Z1)−f(t,X ,Y2,Z2) dt− Z1 −Z2 dB .
t t t t t t t t t
s s
Note that Hutzenthaler et al. [39, Lemma 3.1] (applied with Y ↶ Y1 −Y2,
A ↶ f(·,X,Y1,Z1)−f(·,X,Y2,Z2), Z ↶ Z1 −Z2 in the notation of (46) in [39]) implies that for
· · · · · ·
all s ∈ [0,T], λ ∈ (0,∞) it holds that
(cid:104) (cid:90) T (cid:105)
E eλs|Y1 −Y2|2 + eλt∥Z1 −Z2∥2(cid:12) (cid:12)F dt
s s t t s
s (134)
≤
E(cid:104)(cid:90) T eλt
∥f(t,X ,Y1,Z1)−f(t,X ,Y2,Z2)∥2dt(cid:12) (cid:12)F
(cid:105)
.
λ t t t t t t s
s
23Taking the expectation and for all s ∈ [0,T], λ ∈ (0,∞) it holds that
(cid:104) (cid:90) T (cid:105)
E eλs|Y1 −Y2|2 + eλt∥Z1 −Z2∥2dt
s s t t
s
(cid:104)(cid:90) T eλt (cid:105)
≤ E ∥f(t,X ,Y1,Z1)−f(t,X ,Y2,Z2)∥2dt (135)
λ t t t t t t
s
(cid:104)(cid:90) T Leλt (cid:105)
≤ E (cid:0) |Y1 −Y2|2 +∥Z1 −Z2∥2(cid:1) dt ,
λ t t t t
s
where L is the Lipschitz constant of f. Let λ > 2LT +1 and we immediately have for all s ∈ [0,T],
N ∈ N it holds that
(cid:20)(cid:90) T eλt (cid:21) 1 (cid:90) T
E(cid:2) eλs|Y1 −Y2|2(cid:3) ≤ E |Y1 −Y2|2dt = E(cid:2) eλt|Y1 −Y2|2(cid:3) dt. (136)
s s 2T t t 2T t t
s s
Induction hence shows that for all N ∈ N, t ∈ [0,T] it holds that
0
E(cid:2)
eλt0|Y1
−Y2|2(cid:3)
≤
(cid:104) 1 (cid:105)2(cid:90) T (cid:90) T
E(cid:2)
eλt2|Y1
−Y2|2(cid:3)
dt dt
t0 t0 2T t2 t2 2 1
t0 t1
≤
(cid:104) 1 (cid:105)N+1(cid:90) T (cid:90) T (cid:90) T ...(cid:90) T (cid:90) T
E(cid:2)
eλs|Y1
−Y2|2(cid:3)
dsdt ... dt dt dt
2T s s N 3 2 1
t0 t1 t2 tN−1 tN
≤
(cid:104) 1 (cid:105)N+1(cid:90) T (cid:90) T (cid:90) T ...(cid:90) T (cid:90) T
E(cid:2)
eλs|Y1
−Y2|2(cid:3)
dsdt ... dt dt dt
2T s s N 3 2 1
t0 t1 t2 tN−1 t0
(137)
(cid:34) (cid:35)
=
(cid:104) 1 (cid:105)N+1(cid:20)(cid:90) T
E(cid:2)
eλs|Y1
−Y2|2(cid:3)
ds(cid:21) (cid:90) T (cid:90) T (cid:90) T ...(cid:90) T
dt ... dt dt dt
2T s s N 3 2 1
t0 t0 t1 t2 tN−1
≤
(cid:104) 1 (cid:105)N+1(cid:20)(cid:90) T
E(cid:2)
eλs|Y1
−Y2|2(cid:3)
ds(cid:21) (T −t 0)N
2T s s N!
t0
≤
(cid:104) 1 (cid:105)N+1(T −t 0)N
eλT
(cid:90) T
E(cid:2)
|Y1
−Y2|2(cid:3)
ds.
2T N! s s
t0
Therefore, we obtain that for all t ∈ [0,T] it holds that
E(cid:2) eλt|Y1 −Y2|2(cid:3) = 0. (138)
t t
Hence, we obtain that for all t ∈ [0,T] it holds that
P(cid:0) Y1 = Y2(cid:1) = 1. (139)
t t
The fact the every k ∈ {1,2} it holds that Yk is a stochastic process with continuous sample paths
therefore shows that
P(cid:0) Y1 = Y2(cid:1) = 1. (140)
Combining this with (135) proves that it holds that
(cid:104)(cid:90) T (cid:105)
E eλt∥Z1 −Z2∥2dt = 0, (141)
t t
s
24hence for a.e. t ∈ [0,T] it holds that
P(cid:0) Z1 = Z2(cid:1) = 1. (142)
t t
The fact that Z is a stochastic process with continuous sample paths demonstrates that for all
t ∈ [0,T] it holds that
P(cid:0) Z1 = Z2(cid:1) = 1, (143)
t t
and
P(cid:0) Z1 = Z2(cid:1) = 1. (144)
The proof of Lemma 3.13 is thus complete.
Lemma 3.14. Let v ∈ C (Rd,R), V ∈ C ([0,T] × Rd,Rd), let f: [0,T] × Rd × R × Rd → R be
b b
Lipschitz continuous, let (Ω,F,(F ) ,P) be a filtered probability space, let X: [0,T] × Ω → Rd
t t∈[0,T]
be an (F ) -adapted stochastic process with continuous sample paths, and for every k ∈ {1,2} let
t t∈[0,T]
Yk: [0,T]×Ω → R be a stochastic process with continuous sample paths which satisfies that for all
t ∈ [0,T] it holds P-a.s. that
(cid:90) t (cid:90) t
Yk = v(X )− f(cid:0) s,X ,Yk,V(s,X )(cid:1) ds+ [V(s,X )]∗dB . (145)
t 0 s s s s s
0 0
Then P(Y1 = Y2) = 1.
Proof of Lemma 3.14. Observe that (145) establishes that for all t ∈ [0,T] it holds P-a.s. that
(cid:12) (cid:90) t (cid:12)
(cid:12) (cid:12) (cid:12) (cid:2) (cid:0) (cid:1) (cid:0) (cid:1)(cid:3) (cid:12)
(cid:12)Y1 −Y2 (cid:12) = (cid:12)− f s,X ,Y1,V(s,X ) −f s,X ,Y2,V(s,X ) ds(cid:12)
t t (cid:12) s s s s s s (cid:12)
0 (146)
(cid:90) t
(cid:12) (cid:12)
≤ L (cid:12)Y1 −Y2 (cid:12)ds.
s s
0
Combining this with Gronwall’s inequality and the fact that it holds P-a.s. that Y1 = Y2 ensures
0 0
that P(Y1 = Y2) = 1. Combining this with the fact that for every k ∈ {1,2} Yk has continuous
t t
sample paths implies that
P(cid:0) Y1 = Y2(cid:1) = 1. (147)
The proof of Lemma 3.14 is thus complete.
3.4.2 Reformulating semilinear PDEs as infinite-dimensional stochastic optimization
problems
Theorem 3.15. Let T ∈ (0,∞), d ∈ N, let µ: [0,T]×Rd → Rd, σ: [0,T]×Rd → Rd×d, g: Rd → R,
and f: [0,T] × Rd × R × Rd → R be Lipschitz continuous, let u ∈ C1,2([0,T] × Rd) satisfy for all
t ∈ [0,T], x ∈ Rd that u(T,x) = g(x) and
(cid:0) (cid:1)
∂u(t,x)+ 1Tr (σ(t,x)[σ(t,x)]∗(Hess u)(t,x) + ∂u(t,x)µ(t,x)
∂t 2 x ∂x
+f(cid:0) t,x,u(t,x),[σ(t,x)]∗(∇ u)(t,x)(cid:1) = 0, (148)
x
let (Ω,F,(F ) ,P) be a filtered probability space, let B: [0,T]×Ω → Rd be a standard (F ) -
t t∈[0,T] t t∈[0,T]
Brownian motion, let X: [0,T]×Ω → Rd be an (F ) -adapted stochastic processes with continuous
t t∈[0,T]
sample paths which satisfies that for all t ∈ [0,T] it holds P-a.s. that
(cid:90) t (cid:90) t
X = X + µ(s,X )ds+ σ(s,X )dB , (149)
t 0 s s s
0 0
25and for6 every v ∈ C (Rd,R), V ∈ C ([0,T] × Rd,Rd) let Yv,V : [0,T] × Ω → R be a stochastic
b b
process with continuous sample paths which satisfies that for all t ∈ [0,T] it holds P-a.s. that
(cid:90) t (cid:90) t
Yv,V = v(X )− f(cid:0) s,X ,Yv,V,V(s,X )(cid:1) ds+ [V(s,X )]∗dB . (150)
t 0 s s s s s
0 0
Then
(i) for all v ∈ C (Rd,R), V ∈ C ([0,T]×Rd,Rd) it holds that
b b
E(cid:2) |Yv,V −g(X )|2(cid:3) = inf E(cid:2) |Yw,W −g(X )|2(cid:3) (151)
T T T T
w∈C (Rd,R),
b
W∈C ([0,T]×Rd,Rd)
b
if and only if
(cid:16) (cid:17) (cid:16) (cid:17)
P v(X ) = u(0,X ) = P ∀t ∈ [0,T]: V(t,X ) = [σ(t,X )]∗(∇ u)(t,X ) = 1 (152)
0 0 t t x t
and
(ii) there exist v ∈ C (Rd,R), V ∈ C ([0,T]×Rd,Rd) such that
b b
E(cid:2) |Yv,V −g(X )|2(cid:3) = inf E(cid:2) |Yw,W −g(X )|2(cid:3) = 0. (153)
T T T T
w∈C (Rd,R),
b
W∈C ([0,T]×Rd,Rd)
b
Proof of Theorem 3.15. Throughout this proof let
S = {(v,V) ∈ C (Rd,R)×C ([0,T]×Rd,Rd):
b b
(cid:16) (cid:17) (cid:16) (cid:17)
P v(X ) = u(0,X ) = P ∀t ∈ [0,T]: V(t,X ) = [σ(t,X )]∗(∇ u)(t,X ) = 1}, (154)
0 0 t t x t
let c ∈ [0,∞] satisfy
c = inf E(cid:2) |Yw,W −g(X )|2(cid:3) , (155)
T T
w∈C (Rd,R),
b
W∈C ([0,T]×Rd,Rd)
b
let v ∈ C (Rd,R), V∈ C ([0,T]×Rd,Rd) satisfy for all t ∈ [0,T], x ∈ Rd that
b b
v(x) = u(0,x) and V(t,x) = [σ(t,x)]∗(∇ u)(t,x) (156)
x
and let Y: [0,T]×Ω → R and Z: [0,T]×Ω → Rd satisfy for all t ∈ [0,T] that
Y = u(t,X ) and Z = V(t,X ). (157)
t t t t
Observe that (v, V) ∈ S. Therefore, we obtain that S is not empty. Note that (148), (149), (157),
and Proposition 3.5 show that for all s,t ∈ [0,T] it holds P-a.s. that
(cid:90) t (cid:90) t
Y −Y + f(r,X ,Y ,Z )dr− (Z )∗dB = 0. (158)
t s r r r r r
s s
6Notethatforallmetricspaces(E,d)and(E,δ)itholdsthatC (E,E)isthesetofallboundedcontinuousfunctions
b
from E to E.
26Hence, we obtain that
(cid:90) t (cid:90) t
Y −u(0,X )+ f(s,X ,Y ,Z )ds− (Z )∗dB = 0. (159)
t 0 s s s s s
0 0
Next observe that combining (158) with the fact that u(T,x) = g(x) proves that for all t ∈ [0,T] it
holds P-a.s. that
(cid:90) T (cid:90) T
Y −g(X )− f(s,X ,Y ,Z )ds+ (Z )∗dB = 0. (160)
t T s s s s s
t t
Furthermore, note that (150) demonstrates that for all (v,V) ∈ S it holds P-a.s. that
(cid:90) t (cid:90) t
Yv,V −u(0,X )+ f(s,X ,Yv,V,Z )ds− (Z )∗dB = 0. (161)
t 0 s s s s s
0 0
Combining this with (159) and Lemma 3.14 establishes that for all (v,V) ∈ S it holds that
E(cid:2) |Yv,V −g(X )|2(cid:3) = E(cid:2) |Y −g(X )|2(cid:3) = E(cid:2) |u(T,X )−g(X )|2(cid:3) = 0. (162)
T T T T T T
Observe that (162), (151) and the fact that S is not empty ensure that for all v ∈ C (Rd,R),
b
V ∈ C ([0,T]×Rd,Rd) with E(cid:2) |Yv,V −g(X )|2(cid:3) = c it holds that
b T T
E(cid:2) |Yv,V −g(X )|2(cid:3) = 0. (163)
T T
Therefore, we obtain that for all v ∈ C (Rd,R), V ∈ C ([0,T]×Rd,Rd) with E(cid:2) |Yv,V −g(X )|2(cid:3) = c
b b T T
that P(Yv,V = g(X )) = 1. Combining this with (150) implies that for all t ∈ [0,T], v ∈ C (Rd,R),
T T b
V ∈ C ([0,T]×Rd,Rd) with E(cid:2) |Yv,V −g(X )|2(cid:3) = c it holds P-a.s. that
b T T
(cid:90) T (cid:90) T
Yv,V = g(X )+ f(cid:0) s,X ,Yv,V,V(s,X )(cid:1) ds− [V(s,X )]∗dB . (164)
t T s s s s s
t t
Lemma 3.13 and (160) hence show that for all t ∈ [0,T], v ∈ C (Rd,R), V ∈ C ([0,T] × Rd,Rd)
b b
with E(cid:2) |Yv,V − g(X )|2(cid:3) = c it holds P-a.s. that (cid:0) Yv,V,V(·,X)(cid:1) = (cid:0) Y,Z(cid:1). Therefore, we obtain
T T ·
that for all s ∈ [0,T], v ∈ C (Rd,R), V ∈ C ([0,T]×Rd,Rd) with E(cid:2) |Yv,V −g(X )|2(cid:3) = c it holds
b b T T
P-a.s. that v(X ) = u(0,X ), V(s,X ) = Z = [σ(s,X )]∗(∇ u)(s,X ), for all s ∈ [0,T]. Hence,
0 0 s s s x s
we obtain that for all v ∈ C (Rd,R), V ∈ C ([0,T] × Rd,Rd) with E(cid:2) |Yv,V − g(X )|2(cid:3) = c, it
b b T T
holds that (v,V) ∈ S. Corollary 2.2 (applied with S ↶ S, V ↶ C (Rd,R) × C ([0,T] × Rd,Rd),
b b
(U ) ↶ (U ) , (Ω,F,P) ↶ (Ω,F,P) in the notation of Corollary 2.2) proves items (i) and (ii).
v v∈V v v∈V
The proof of Theorem 3.15 is thus complete.
Corollary 3.16. Let T ∈ (0,∞), d ∈ N, let µ: [0,T]×Rd → Rd, σ: [0,T]×Rd → Rd×d, g: Rd → R,
and f: [0,T] × Rd × R × Rd → R be Lipschitz continuous, let u ∈ C1,2([0,T] × Rd) satisfy for all
t ∈ [0,T], x ∈ Rd that u(T,x) = g(x) and
(cid:0) (cid:1)
∂u(t,x)+ 1Tr (σ(t,x)[σ(t,x)]∗(Hess u)(t,x) + ∂u(t,x)µ(t,x)
∂t 2 x ∂x
+f(cid:0) t,x,u(t,x),[σ(t,x)]∗(∇ u)(t,x)(cid:1) = 0, (165)
x
27let (Ω,F,(F ) ,P) be a filtered probability space, let B: [0,T]×Ω → Rd be a standard (F ) -
t t∈[0,T] t t∈[0,T]
Brownian motion, let X: [0,T]×Ω → Rd be an (F ) -adapted stochastic process with continuous
t t∈[0,T]
sample paths which satisfies that for all t ∈ [0,T] it holds P-a.s. that
(cid:90) t (cid:90) t
X = X + µ(s,X )ds+ σ(s,X )dB , (166)
t 0 s s s
0 0
assume for all ε ∈ (0,∞), x ∈ Rd that P(∥X − x∥ < ε) > 0, and for every v ∈ C (Rd,R),
0 b
V ∈ C ([0,T]×Rd,Rd) let Yv,V : [0,T]×Ω → R be a stochastic process with continuous sample paths
b
which satisfies that for all t ∈ [0,T] it holds P-a.s. that
(cid:90) t (cid:90) t
Yv,V = v(X )− f(cid:0) s,X ,Yv,V,V(s,X )(cid:1) ds+ [V(s,X )]∗dB . (167)
t 0 s s s s s
0 0
Then
(i) there exist v ∈ C (Rd,R), V ∈ C ([0,T]×Rd,Rd) which satisfy
b b
E(cid:2) |Yv,V −g(X )|2(cid:3) = inf E(cid:2) |Yw,W −g(X )|2(cid:3) = 0 (168)
T T T T
w∈C (Rd,R),
b
W∈C ([0,T]×Rd,Rd)
b
and
(ii) it holds for all x ∈ Rd that v(x) = u(0,x).
Proof of Corollary 3.16. Note that item (ii) in Theorem 3.15 demonstrates that there exist
v ∈ C (Rd,R), V ∈ C ([0,T]×Rd,Rd) which satisfy
b b
E(cid:2) |Yv,V −g(X )|2(cid:3) = inf E(cid:2) |Yw,W −g(X )|2(cid:3) = 0. (169)
T T T T
w∈C (Rd,R),
b
W∈C ([0,T]×Rd,Rd)
b
This establishes item (i). Observe that item (i) in Theorem 3.15 and (169) ensure that
P(cid:0) v(X ) = u(0,X )(cid:1) = 1. (170)
0 0
Combining this and Lemma 2.3 (applied with f ↶ (Rd ∋ x (cid:55)→ v(x)−u(0,x) ∈ R) in the notation of
Lemma 2.3) implies item (ii). The proof of Corollary 3.16 is thus complete.
4 Operator learning methods
In this section we provide an introduction to operator learning methods. Roughly speaking, the
field of operator learning is concerned with the approximation of mappings which map functions to
functions, a type of mappings usually called operators. In the context of PDEs, operators naturally
arise, for example, as mappings from boundary conditions or initial values of PDEs to corresponding
PDE solutions or terminal values. To approximately learn such operators it is necessary to develop
new types of trainable models which take functions as inputs and outputs. We refer to such models
as neural operators.
InSection4.1wepresentabasicmethodfortrainingsuchneuraloperatorsusingevaluationsofthe
targetoperator. InSection4.2wediscussprecisedefinitionsofdifferenttypesofneuraloperators. We
28firstconsiderneuraloperatorswhichrelyoninterpolationtomovebetweendiscretedataandfunctions
and are based on classical ANNs such as fully connected feed-forward ANNs (see Section 4.2.1) and
convolutional neural networks (CNNs) (see Section 4.2.2). We then present a selection of recent
and advanced neural operators that are directly formulated in function space without making use
of interpolation such as integral kernel neural operators (IKNOs) (see Section 4.2.3), Fourier neural
operators (FNOs) (see Section 4.2.4), and deep operator networks (DeepONets) (see Section 4.2.5).
In Section 4.3 we introduce the physics–informed neural operator (PINO) methodology for operators
related to PDEs, which combines the data-driven operator learning approach in Section 4.1 with the
PINNs methodology in Section 2.2. In Section 4.4 we survey additional operator learning methods
from the literature. Lastly, in Section 4.5 we present numerical simulations which illustrate the
performance of the neural operators discussed in this section.
4.1 The canonical data-driven operator learning framework
In Method description 4.1 below we illustrate how a neural operator can be trained using the SGD
optimization method when evaluations of the target operator are available as training data.
Method description 4.1 (General data-driven operator learning). Let d,d,M ∈ N, D ⊆ Rd,
D ⊆ Rd, let I ⊆ L0(D;R), O ⊆ L0(D;R) be vector spaces, let |||·|||: O → [0,∞) be measurable, let
S: I → O, N: Rd ×I → O, and L: Rd ×I → R satisfy for all θ ∈ Rd, i ∈ I that
L(θ,i) = |||(N(i))−(S(i))|||2, (171)
θ
let G: Rd × I → Rd satisfy for all θ ∈ Rd, i ∈ I with L(·,i): Rd → R is differentiable at θ that
G(θ,i) = (∇ L)(θ,i), let (Ω,F,P) be a probability space, let I : Ω → I, n,m ∈ N, be random
θ n,m
variables, let (γ ) ⊆ R, and let Θ: N ×Ω → Rd satisfy for all n ∈ N that
n n∈N 0 0
(cid:34) (cid:35)
M
γ (cid:88)
Θ = Θ − n G(Θ ,I ) . (172)
n+1 n n n,m
M
m=1
Remark 4.2 (Explanations for Method description 4.1). We think of S as the target operator that
we want to approximate, we think of Nas the neural operator that we want to use to approximate S,
we think of |||·||| as a norm or seminorm on the output space O used to measure the distance between
training samples and the output of the neural operator (cf., e.g., (228), (232), and (237) for concrete
examples), we think of (I ) as the training samples for the optimization procedure, and we
n,m (n,m)∈N2
think of Θ as an SGD process for the loss L with generalized gradient G, learning rates (γ ) ,
n n∈N
constantbatchsizeM, initialvaluesΘ , and data(I ) . Forsufficiently largen ∈ N
0 n,m (n,m)∈N×{1,2,...,M}
we hope that N ≈ S.
Θn
For simplicity in Method description 4.1 we choose to describe the plain-vanilla SGD method with
constant learning rates for the learning process in (172). In practice, however, typically other more
advanced SGD-type optimization methods are employed. For example, in our numerical simulations
in Section 4.5 we use an adaptive trainig procedure based on the Adam optimizer which is described
in [42, Appendix A.2]. Moreover, in typical applications the target operator S is not known exactly
and needs to be replaced with approximations of it to compute reference solutions in the loss function
in (171).
294.2 Neural operator architectures
In this section we introduce various neural operator architectures. In the treatment of several types
of neural operators, we will for simplicity restrict ourselves to the case of periodic functions. In
these situations, we will make use of the modulo operation in Definition 4.3 below and of multi-linear
periodic interpolation in Definition 4.5 below.
Definition 4.3 (Modulo operators). Let a ∈ N. Then we denote by mod : R → [0,a) the function
a
which satisfies for all b ∈ R that
mod (b) = min({b−ka ∈ R: k ∈ Z}∩[0,a)). (173)
a
Lemma 4.4 (Properties of multi-linear periodic interpolators). Let d ∈ N, d ,d ,...,d ∈ N,
1 2 d
x = (x
i1,i2,...,i
d)
(i1,i2,...,i d)∈(×d k=1{0,1,2,...,d k})
∈ R(d1+1)×(d2+1)×...×(d d+1) and let I: [0,1]d → R satisfy for
all y = (y ,y ,...,y ) ∈ [0,1]d that
1 d d
(cid:34) (cid:35)
d
(cid:88) (cid:89)
I(y) = (1−|d y −i |)1 (d y −i ) x . (174)
k k k [−1,1] k k k i1,i2,...,i d
(i1,i2,...,i d)∈(×d k=1{0,1,...,d k}) k=1
Then
(i) it holds for all (i ,i ,...,i ) ∈ (×d {0,1,...,d }) that I(i1, i2,..., i d) = x and
1 2 d k=1 k d1 d2 d d i1,i2,...,i d
(ii) it holds that I ∈ C([0,1]d,R).
Proof of Lemma 4.4. Note that(174) implies items (i) and (ii). The proof of Lemma 4.4 is thus
complete.
Definition 4.5 (Multi-linear periodic interpolators). Let d ∈ N, d ,d ,...,d ∈ N. Then we say
1 2 d
that I is the periodic multi-linear interpolator on [0,1]d with spatial discretization (d ,d ,...,d )
1 2 d
if and only if it holds that I: Rd1×d2×...×d d → C([0,1]d,R) is the function which satisfies for all
x = (x
i1,i2,...,i
d)
(i1,i2,...,i d)∈(×d k=1{0,1,2,...,d k−1})
∈ Rd1×d2×...×d d, x = (x
i1,i2,...,i
d)
(i1,i2,...,i d)∈(×d k=1{0,1,2,...,d k})
∈
R(d1+1)×(d2+1)×...×(d d+1), y = (y ,y ,...,y ) ∈ [0,1]d with ∀(i ,i ,...,i ) ∈ (×d {0,1,2,...,d }):
1 d d 1 2 d k=1 k
x = x that
i1,i2,...,i d modd1(i1),modd2(i2),...,moddd(i d)
(cid:34) (cid:35)
d
(cid:88) (cid:89)
(I(x))(y) = (1−|d y −i |)1 (d y −i ) x (175)
k k k [−1,1] k k k i1,i2,...,i d
(i1,i2,...,i d)∈(×d k=1{0,1,...,d k}) k=1
(cf. Lemma 4.4 and Definition 4.3).
4.2.1 Fully-connected feed-forward neural operators
In this section we introduce basic neural operator models based on vanilla fully-connected feed-
forward ANNs. In order to use fully-connected feed-forward ANNs to define models which take
functions as inputs and outputs, we will employ evaluations on grids and multi-linear interpolation
to move between discrete data and functions. We will first introduce fully-connected feed-forward
ANN models which take multi-dimensional matrices as inputs and outputs (see Definition 4.6 below)
and then use these models to define corresponding neural operator models (see Definition 4.8 below).
Roughly speaking, we think of the multi-dimensional matrices that serve as inputs and outputs of
the fully-connected feed-forward ANN models as discretizations of functions on a grid.
30Definition 4.6 (Fully-connected feed-forward ANN models). Let d,L ∈ N, l ,l ,...,l ∈ N,
0 1 L
d ,d ,...,d ∈ N, a ∈ C(R,R) satisfy l = d d ···d = l and let F: Rd1×d2×...×d d → Rd1d2···d d
1 2 d 0 1 2 d L
satisfy for all A = (A i1,i2,...,i d) (i1,i2,...,i d)∈(×d k=1{1,2,...,d k}) ∈ Rd1×d2×...×d d, x = (x 1,...,x d1d2···d d) ∈ Rd1d2···d d
×d
with ∀(i ,i ,...,i ) ∈ ( {1,2,...,d }):
1 2 d k=1 k
A = x (176)
i1,i2,...,i d (i1−1)d2d3···d d+(i2−1)d3d4···d d+...+(i d−1−1)d d+i d
that F(A) = x. Then we call Nthe fully-connected feed-forward ANN model with input-output shape
(d ,d ,...,d ), architecture (l ,l ,...,l ), and activation function a if and only if it holds that
1 2 d 0 1 L
(cid:16) (cid:17)
N: ×L Rl ℓ×l ℓ−1 ×Rl ℓ ×Rd1×d2×...×d d → Rd1×d2×...×d d (177)
ℓ=1
isthefunctionwhichsatisfiesforallΦ = ((W ,B )) ∈ (cid:0)×L Rl ℓ×l ℓ−1×Rl ℓ(cid:1) , A ∈ Rd1×d2×...×d d,
ℓ ℓ ℓ∈{1,2,...,L} ℓ=1
x ∈ Rl0, x ∈ Rl1, ..., x ∈ RlL with ∀ℓ ∈ {1,2,...,L−1}:
0 1 L
x = F(A), x = M (W x +B ), and x = W x +B (178)
0 ℓ a ℓ ℓ−1 ℓ L L L−1 L
that N(A) = F−1(x ) (cf. Definition 1.2).
Φ L
Remark 4.7 (Explanations for Definition 4.6). In (178) we think of Φ as the parameters of the
ANN, we think of A as the input of the ANN, we think of x as a reshaped version of the input A,
0
for every ℓ ∈ {1,2,...,L − 1} we think of x as the output of the ℓ-th layer of the ANN given the
ℓ
input A, and we think of the output F−1(x ) of the ANN as a reshaped version of the output of x
L L
of the last layer of the ANN.
Definition 4.8 (Periodicfully-connectedfeed-forwardneuraloperators). Let d,L ∈ N, l ,l ,...,l ∈
0 1 L
N, d ,d ,...,d ∈ N, a ∈ C(R,R) satisfy l = d d ···d = l , let I: Rd1×d2×...×d d → C([0,1]d,R) be
1 2 d 0 1 2 d L
the periodic multi-linear interpolator on [0,1]d with spatial discretization (d ,d ,...,d ), and let N
1 2 d
be the fully-connected feed-forward ANN model with input-output shape (d ,d ,...,d ), architecture
1 2 d
(l ,l ,...,l ), and activation function a (cf. Definitions 4.5 and 4.6). Then we call N the peri-
0 1 L
odic fully-connected feed-forward neural operator on [0,1]d with spatial discretization (d ,d ,...,d ),
1 2 d
architecture (l ,l ,...,l ), and activation function a if and only if it holds that
0 1 L
N: (cid:0)×L Rl ℓ×l ℓ−1 ×Rl ℓ(cid:1) ×C([0,1]d,R) → C([0,1]d,R) (179)
ℓ=1
is the function which satisfies for all f ∈ C([0,1]d,R), Φ = ((W ,B )) ∈ (×L Rl ℓ×l ℓ−1×Rl ℓ)
ℓ ℓ ℓ∈{1,2,...,L} ℓ=1
that
(cid:16) (cid:16) (cid:17)(cid:17)
N Φ(f) = I N Φ (cid:0) f(i1/ d1,i2/ d2,...,i d/ d d)(cid:1) (i1,i2,...,i d)∈(×d k=1{0,1,...,d k−1}) . (180)
4.2.2 Architectures based on convolutional neural networks
In this section, we illustrate with two basic examples how CNNs can be used to construct neural
operators. Since CNNs make use of the spatial structure of their input, they are a natural approach
in the context of operator learning, in particular when the action of the neural operator should be
based on the values of the input function sampled on a grid. For more background on CNNs, cf.,
e.g., [27, Chapter 9].
314.2.2.1 Simple periodic convolutional neural operators In this section we present a simple
example of a neural operator architecture based on discrete periodic convolutions (cf. Definition 4.9).
Roughly speaking, the discrete periodic convolution operation allows to define CNNs with the prop-
erty that the shape of the feature maps stays the same on each layer (cf. Remark 4.12). Like in
the case of fully-connected feed-forward ANNs in Section 4.2.1, we will first introduce CNN models
which take multi-dimensional matrices as inputs and outputs (see Definition 4.11 below) and then
use these models to define corresponding neural operator models (see Definition 4.13 below). Such
simple periodic neural operators were employed by [46] as a part of their proposed operator learning
methodology.
Definition 4.9 (Discrete periodic convolutions). Let d ∈ N, a ,a ,...,a ,w ,w ,...,w ∈ N , A =
1 2 d 1 2 d 0
(A
i1,i2,...,i
d)
(i1,i2,...,i d)∈(×d k=1{0,1,...,a k−1})
∈ Ra1×a2×...×a d, W = (W
j1,j2,...,j
d)
(j1,j2,...,j d)∈(×d k=1{−w k,−w k+1,...,w k})
∈
R(2w1+1)×(2w2+1)×...×(2w d+1) satisfy for all k ∈ {1,2,...,d} that
a ≥ 2w +1. (181)
k k
Then we denote by A⊛W = ((A⊛W) i1,i2,...,i d) (i1,i2,...,i d)∈(×d k=1{0,1,...,a k−1}) ∈ Ra1×a2×...×a d the tensor
×d
which satisfies for all (i ,i ,...,i ) ∈ {0,1,...,a −1} that
1 2 d k=1 k
(cid:88)w1 (cid:88)w2 (cid:88)w d
(A⊛W) = ... A W (182)
i1,i2,...,i d moda1(i1+j1),moda2(i2+j2),...,modad(i d+j d) j1,j2,...,j d
j1=−w1j2=−w2 j d=−w
d
(cf. Definition 4.3).
Definition 4.10 (One tensor). Let d ∈ N, d ,d ,...,d ∈ N. Then we denote by Id1,d2,...,d d =
1 2 d
(I id 11 ,, id 22 ,, .. .. .. ,, id dd) (i1,i2,...,i d)∈(×d k=1{1,2,...,d k}) ∈ Rd1×d2×...×d d the tensor which satisfies for all i 1 ∈ {1,2,...,d 1},
i ∈ {1,2,...,d }, ..., i ∈ {1,2,...,d } that
2 2 d d
Id1,d2,...,d d = 1. (183)
i1,i2,...,i
d
Definition 4.11 (Simple periodic CNN models). Let d,L ∈ N, l ,l ,...,l ∈ N, d ,d ,...,d ∈ N,
0 1 L 1 2 d
(w ) ⊆ N , a ∈ C(R,R) satisfy for all ℓ ∈ {1,2,...,L}, k ∈ {1,2,...,d} that
ℓ,k (ℓ,k)∈{1,2,...,L}×{1,2,...,d} 0
l = 1 = l and d ≥ 2w + 1. Then we call N the simple periodic CNN model with input-output
0 L k ℓ,k
shape (d ,d ,...,d ), channel structure (l ,l ,...,l ), kernel sizes (w ) , and
1 2 d 0 1 L ℓ,k (ℓ,k)∈{1,2,...,L}×{1,2,...,d}
activation function a if and only if it holds that
(cid:16) (cid:17)
N: ×L (cid:0)R(2w ℓ,1+1)×(2w ℓ,2+1)×...×(2w ℓ,d+1)(cid:1)l ℓ×l ℓ−1 ×Rl ℓ ×Rd1×d2×...×d d → Rd1×d2×...×d d (184)
ℓ=1
isthefunctionwhichsatisfiesforallΦ = (((W ) ,(B ) )) ∈
ℓ,n,m (n,m)∈{1,2,...,l }×{1,2,...,l } ℓ,n n∈{1,2,...,l } ℓ∈{1,2,...,L}
ℓ ℓ−1 ℓ
×L ((Rw ℓ,1×w ℓ,2×...×w ℓ,d)l ℓ×l ℓ−1 ×Rl ℓ), x = (x ,...,x ) ∈ (Rd1×d2×...×d d)l0, x = (x ,...,x ) ∈
ℓ=1 0 0,1 0,l0 1 1,1 1,l1
(Rd1×d2×...×d d)l1, ..., x = (x ,...,x ) ∈ (Rd1×d2×...×d d)lL−1 with ∀ℓ ∈ {1,2,...,L−1},
L−1 L−1,1 L−1,lL−1
n ∈ {1,2,...,l }: x = M (cid:0) B Id1,d2,...,d d +(cid:80)l ℓ−1 x ⊛W (cid:1) that
ℓ ℓ,n a ℓ,n m=1 ℓ−1,m ℓ,n,m
N(x ) = B Id1,d2,...,d d +(cid:80)lL−1 x ⊛W (185)
Φ 0,1 L,1 m=1 L−1,m L,1,m
(cf. Definitions 1.2, 4.9, and 4.10).
32Remark 4.12 (Explanations for Definition 4.11). In (185) we think of Φ as the parameters of the
CNN, we think of x as the input of the CNN, and for every ℓ ∈ {1,2,...,L − 1} we think of
0
x as the output of the ℓ-th layer of the CNN given the input x consisting of the l feature maps
ℓ 0 ℓ
x ,x ,...,x ∈ Rd1×d2×...×d d.
ℓ,1 ℓ,2 ℓ,l
ℓ
Definition 4.13 (Simple periodic convolutional neural operators). Let d,L ∈ N, l ,l ,...,l ∈ N,
0 1 L
d ,d ,...,d ∈ N, (w ) ⊆ N , a ∈ C(R,R) satisfy for all ℓ ∈ {1,2,...,L},
1 2 d ℓ,k (ℓ,k)∈{1,2,...,L}×{1,2,...,d} 0
k ∈ {1,2,...,d} that l = 1 = l and d ≥ 2w +1, let I: Rd1×d2×...×d d → C([0,1]d,R) be the periodic
0 L k ℓ,k
multi-linear interpolator on [0,1]d with spatial discretization (d ,d ,...,d ), and let N the simple peri-
1 2 d
odic CNN model with input-output shape (d ,d ,...,d ), channel structure (l ,l ,...,l ), kernel sizes
1 2 d 0 1 L
(w ) , and activation function a (cf. Definitions 4.5 and 4.11). Then we call N
ℓ,k (ℓ,k)∈{1,2,...,L}×{1,2,...,d}
the simple periodic convolutional neural operator on [0,1]d with spatial discretization (d ,d ,...,d ),
1 2 d
channel structure (l ,l ,...,l ), kernel sizes (w ) , and activation function a if
0 1 L ℓ,k (ℓ,k)∈{1,2,...,L}×{1,2,...,d}
and only if it holds that
(cid:16) (cid:17)
N: ×L (cid:0)R(2w ℓ,1+1)×(2w ℓ,2+1)×...×(2w ℓ,d+1)(cid:1)l ℓ×l ℓ−1 ×Rl ℓ ×C([0,1]d,R) → C([0,1]d,R) (186)
ℓ=1
is the function which satisfies for all f ∈ C([0,1]d,R), Φ ∈ (cid:0)×L (Rw ℓ,1×w ℓ,2×...×w ℓ,d)l ℓ×l ℓ−1×Rl ℓ(cid:1) that
ℓ=1
(cid:16) (cid:16) (cid:17)(cid:17)
N Φ(f) = I N Φ (cid:0) f(i1/ d1,i2/ d2,...,i d/ d d)(cid:1) (i1,i2,...,i d)∈(×d k=1{0,1,...,d k−1}) . (187)
4.2.2.2 Simple convolutional neural operators with encoder-decoder architecture A
common technique for designing CNNs is to use so-called encoder-decoder architectures. This means
that the network architecture is divided into two segments: an encoder segment followed by a decoder
segment. Roughly speaking, in the encoder part, the shape of the feature maps is successively
contracted while the number of channels (i.e. the number of feature maps) is increased. In the
decoder part, this operation is reversed, in the sense that successively the shape of the feature maps
is expanded while the number of channels is decreased. For an explanation of the encoder-decoder
architecture in a mathematical context, see Remark 4.17.
Inthis sectionweintroduce asimplewayto achievean encoder-decoder architecture by employing
full stride convolutions (cf. Definition 4.14) to contract the shape of the feature maps, and utilizing
transposed convolutions (cf. Definition 4.15 and [91]) to expand the shape of the feature maps.
Similar to Section 4.2.2.1 and Section 4.2.1, we will first introduce CNN models which take multi-
dimensional matrices as inputs and outputs (see Definition 4.16 below) and then use these models
to define corresponding neural operator models (see Definition 4.18 below). This approach is very
similar to the CNNs used in the operator learning method in [30].
Definition 4.14 (Discrete convolutions with full stride). Let d ∈ N, a ,a ,...,a ,b ,b ,...,b ,
1 2 d 1 2 d
w 1,w 2,...,w
d
∈ N, A = (A
i1,i2,...,i
d)
(i1,i2,...,i d)∈(×d k=1{0,1,...,a k−1})
∈ Ra1×a2×...×a d, W =
(W j1,j2,...,j d) (j1,j2,...,j d)∈(×d k=1{0,1,...,w k−1}) ∈ Rw1×w2×...×w d satisfy for all k ∈ {1,2,...,d} that
b = a /w . (188)
k k k
Then we denote by A∗W = ((A∗W) i1,i2,...,i d) (i1,i2,...,i d)∈(×d k=1{0,1,...,b k−1}) ∈ Rb1×b2×...×b d the tensor
which satisfies for all (i ,i ,...,i ) ∈ ×d {0,1,...,b −1} that
1 2 d k=1 k
w (cid:88)1−1w (cid:88)2−1 w (cid:88)d−1
(A∗W) = ... A W . (189)
i1,i2,...,i
d
i1w1+j1,i2w2+j2,...,i dw d+j
d
j1,j2,...,j
d
j1=0 j2=0 j d=0
33Definition 4.15 (Discrete transposed convolutions with full stride). Let d ∈ N, a ,a ,...,a ,
1 2 d
b 1,b 2,...,b d,w 1,w 2,...,w
d
∈ N, B = (B
i1,i2,...,i
d)
(i1,i2,...,i d)∈(×d k=1{0,1,...,a k−1})
∈ Rb1×b2×...×b d, W =
(W j1,j2,...,j d) (j1,j2,...,j d)∈(×d k=1{0,1,...,w k−1}) ∈ Rw1×w2×...×w d satisfy for all k ∈ {1,2,...,d} that
a = b w . (190)
k k k
Then we denote by B∗W = ((B∗W) i1,i2,...,i d) (i1,i2,...,i d)∈(×d k=1{0,1,...,a k−1}) ∈ Ra1×a2×...×a d the tensor
which satisfies for all (i ,i ,...,i ) ∈ ×d {0,1,...,a −1}, (j ,j ,...,j ) ∈ ×d {0,1,...,b −1}
1 2 d k=1 k 1 2 d k=1 k
that
(B∗W) = B W . (191)
i1w1+j1,i2w2+j2,...,i dw d+j
d
i1,i2,...,i
d
j1,j2,...,j
d
Definition4.16(SimpleCNNmodelswithencoder-decoderarchitecture). Letd,L ∈ N, l ,l ,...,l ∈
0 1 L
N, (d ) ⊆ N, (w ) ⊆ N, a ∈ C(R,R) satisfy for all
ℓ,k (ℓ,k)∈{0,1,...,L}×{1,2,...,d} ℓ,k (ℓ,k)∈{1,2,...,L}×{1,2,...,d}
ℓ ∈ {1,2,...,L}, k ∈ {1,2,...,d} that l = 1 and
0
d = d /w . (192)
ℓ,k ℓ−1,k ℓ−1,k
Then we call N the simple CNN model with encoder-decoder architecture with input-output shape
(d ,d ,...,d ), channel structure (l ,l ,...,l ,l ,l ,...,l ,l ), kernel sizes
0,1 0,2 0,d 0 1 L−1 L L−1 1 0
(w ) , and activation function a if and only if it holds that
ℓ,k (ℓ,k)∈{1,2,...,L}×{1,2,...,d}
(cid:18)
(cid:16) (cid:17)
N: ×L (Rw ℓ,1×w ℓ,2×...×w ℓ,d)l ℓ×l ℓ−1 ×Rl ℓ ×
ℓ=1
(cid:19)
(cid:16) (cid:17)
×L (Rw ℓ,1×w ℓ,2×...×w ℓ,d)l ℓ−1×l ℓ ×Rl ℓ−1 ×Rd0,1×d0,2×...×d 0,d → Rd0,1×d0,2×...×d 0,d (193)
ℓ=1
isthefunctionwhichsatisfiesforallΦ = (((W ) ,(B ) )) ∈
ℓ,n,m (n,m)∈{1,2,...,l }×{1,2,...,l } ℓ,n n∈{1,2,...,l } ℓ∈{1,2,...,L}
ℓ ℓ−1 ℓ
×L ((Rw ℓ,1×w ℓ,2×...×w ℓ,d)l ℓ×l ℓ−1×Rl ℓ), Φ = (((W ) ,(B ) )) ∈
ℓ=1 ℓ,n,m (n,m)∈{1,2,...,l ℓ−1}×{1,2,...,l ℓ} ℓ,n n∈{1,2,...,l ℓ−1} ℓ∈{1,2,...,L}
×L ((Rw ℓ,1×w ℓ,2×...×w ℓ,d)l ℓ−1×l ℓ×Rl ℓ−1), x = (x ,...,x ) ∈ (Rd0,1×d0,2×...×d 0,d)l0, x = (x ,...,x ) ∈
ℓ=1 0 0,1 0,l0 1 1,1 1,l1
(Rd1,1×d1,2×...×d 1,d)l1, ..., x = (x ,...,x ) ∈ (RdL,1×dL,2×...×d L,d)lL, x = (x ,...,x ) ∈
L L,1 L,lL L L,1 L,lL
(RdL,1×dL,2×...×d L,d)lL, x = (x ,...,x ) ∈ (RdL−1,1×dL−1,2×...×d L−1,d)lL−1, ... , x =
L−1 L−1,1 L−1,lL−1 0
(x ,...,x ) ∈ (Rd0,1×d0,2×...×d 0,d)l0 with ∀ℓ ∈ {1,2,...,L},n ∈ {1,2,...,l }:
0,1 0,l0 ℓ
(cid:16) (cid:17)
x = M B Id ℓ,1,d ℓ,2,...,d ℓ,d +(cid:80)l ℓ−1 x ∗W , x = x , (194)
ℓ,n a ℓ,n m=1 ℓ−1,m ℓ,n,m L L
(cid:16) (cid:17)
and x ℓ−1,n = M a1 (1,L](ℓ)+idR1 {1}(ℓ) B ℓ,nId ℓ−1,1,d ℓ−1,2,...,d ℓ−1,d +(cid:80)l mℓ =1x ℓ,m∗W ℓ,n,m (195)
that N (x ) = x (cf. Definitions 1.2, 4.10, 4.14, and 4.15).
(Φ,Φ) 0,1 0,1
Remark 4.17 (ExplanationsforDefinition4.16). In (194) and (195) we think of Φ as the parameters
of encoder CNN, we think of Φ as the parameters of decoder CNN, we think of x as the input of
0
the encoder CNN, for every ℓ ∈ {1,2,...,L−1} we think of x as the output of the ℓ-th layer of the
ℓ
encoder CNN given the input x consisting of the l feature maps x ,x ,...,x ∈ Rd ℓ,1×d ℓ,2×...×d ℓ,d
0 ℓ ℓ,1 ℓ,2 ℓ,l
ℓ
with successively contracting shape(d ,d ,...,d ), we thinkof x = x as the output ofthe encoder
ℓ,1 ℓ,2 ℓ,d L L
CNN and simultaneously the input of the decoder CNN, and for every ℓ ∈ {L−1,L−2,...,1} we think
of x as the output of the ℓ-th layer of the encoder CNN given the input x consisting of the l feature
ℓ L ℓ
maps x ,x ,...,x ∈ Rd ℓ,1×d ℓ,2×...×d ℓ,d with successively expanding shape (d ,d ,...,d ).
ℓ,1 ℓ,2 ℓ,l ℓ,1 ℓ,2 ℓ,d
ℓ
34Definition 4.18 (Simple convolutional neural operator with encoder-decoder architecture). Let
d,L ∈ N, l ,l ,...,l ∈ N, (d ) ⊆ N, (w ) ⊆ N, a ∈
0 1 L ℓ,k (ℓ,k)∈{0,1,...,L}×{1,2,...,d} ℓ,k (ℓ,k)∈{1,2,...,L}×{1,2,...,d}
C(R,R) satisfy for all ℓ ∈ {1,2,...,L}, k ∈ {1,2,...,d} that l = 1 and
0
d = d /w , (196)
ℓ,k ℓ−1,k ℓ−1,k
let I: Rd0,1×d0,2×...×d 0,d → C([0,1]d,R) be the periodic multi-linear interpolator on [0,1]d with spatial
discretization (d ×d ×...×d ), and let N be the simple CNN model with encoder-decoder architec-
0,1 0,2 0,d
ture with input-output shape (d ,d ,...,d ), channel structure (l ,l ,...,l ,l ,l ,...,l ,l ),
0,1 0,2 0,d 0 1 L−1 L L−1 1 0
kernel sizes (w ) , and activation function a (cf. Definitions 4.5 and 4.16). Then
ℓ,k (ℓ,k)∈{1,2,...,L}×{1,2,...,d}
we call N the simple convolutional neural operator with encoder-decoder architecture on [0,1]d with
spatial discretization (d ,d ,...,d ), channel structure (l ,l ,...,l ,l ,l ,...,l ,l ), kernel
0,1 0,2 0,d 0 1 L−1 L L−1 1 0
sizes (w ) , and activation function a if and only if it holds that
ℓ,k (ℓ,k)∈{1,2,...,L}×{1,2,...,d}
(cid:18)
(cid:16) (cid:17)
N: ×L (Rw ℓ,1×w ℓ,2×...×w ℓ,d)l ℓ×l ℓ−1 ×Rl ℓ ×
ℓ=1
(cid:19)
(cid:16) (cid:17)
×L (Rw ℓ,1×w ℓ,2×...×w ℓ,d)l ℓ−1×l ℓ ×Rl ℓ−1 ×C([0,1]d,R) → C([0,1]d,R) (197)
ℓ=1
is the function which satisfies for all f ∈ C([0,1]d,R), Φ ∈ (cid:0)×L (Rw ℓ,1×w ℓ,2×...×w ℓ,d)l ℓ×l ℓ−1 × Rl ℓ(cid:1) ,
ℓ=1
Φ ∈ (cid:0)×L (Rw ℓ,1×w ℓ,2×...×w ℓ,d)l ℓ−1×l ℓ ×Rl ℓ−1(cid:1) that
ℓ=1
(cid:16) (cid:16) (cid:17)(cid:17)
N (Φ,Φ)(f) = I N (Φ,Φ) (cid:0) f(i1/ d0,1,i2/ d0,2,...,i d/ d 0,d)(cid:1) (i1,i2,...,i d)∈(×d k=1{0,1,...,d 0,k−1}) . (198)
4.2.3 Integral kernel neural operators
In this section we introduce IKNOs, an early architecture for operator learning developed in [56].
The novel feature of IKNOs compared to the neural operators introduced in the previous sections is
that their definition is decoupled from a specific discretization of the function space.
The IKNOs introduced in this section are not immediately implementable numerically as they
require the computation of an integral in (201). When numerically evaluating IKNOs, [56] propose to
approximate the integral using a discretization technique based on message passing graph networks,
which can be applied with different discretization grids, and thus call their developed neural operator
graph kernel neural operator (GKNO). In [58] this discretization methodology is further improved
with a multi-scale graph structure and the resulting neural operator is named multipole graph neural
operator (MGNO).
Definition 4.19 (Integral kernel neural operators). Let d,n,d ,d ,d ,L ∈ N, D ∈ B(Rd), let
P K Q
P: Rd P×Rd×R → Rn, K: Rd K×Rd×Rd×R×R → Rn×n, Q: Rd Q×Rn → R, and a: R → R be
measurable functions, and let ν : B(D) → [0,∞), x ∈ D, be measures which satisfy for all B ∈ B(D)
x
that D ∋ x (cid:55)→ ν (B) ∈ [0,∞) is measurable. Then we call Na integral kernel neural operator on the
x
domain D, with lifting model P, kernel model K, projection model Q, integration measures (ν ) ,
x x∈D
length L, and activation function a if and only if it holds that
N: Rd P ×(Rn×n)L ×(Rd K)L ×Rd Q ×L0(D;R) → L0(D;R) (199)
is a function which satisfies for all P ∈ Rd P, W = (W ,W ,...,W ) ∈ (Rn×n)L, K = (K ,K ,
1 2 L 1 2
...,K ) ∈ (Rd K)L, Q ∈ Rd Q, f ∈ L0(D;R), v ,v ,...,v ∈ L0(D;Rn) with ∀x ∈ D,l ∈ {1,2,
L 0 1 L
35...,L}:
(cid:90)
v (x) = P (x,f(x)), (cid:12) (cid:12)K (cid:0) x,y,f(x),f(y)(cid:1) v (y)(cid:12) (cid:12)ν (dy) < ∞, (200)
0 P K l−1 x
l
D
(cid:18) (cid:90) (cid:19)
and v (x) = M W v (x)+ K (cid:0) x,y,f(x),f(y)(cid:1) v (y)ν (dy) (201)
l a l l−1 K l−1 x
l
D
that
N (f) = Q (v ) (202)
(P,W,K,Q) Q L
(cf. Definition 1.2).
Remark 4.20 (Choice of (ν ) in Definition 4.19). Assume the setting in Definition 4.19 and
x x∈D
let r ∈ (0,∞). In [56] it is suggested to choose the integration measures (ν ) for every x ∈ D
x x∈D
as the Lebesgue measure ν (dy) = 1 dy supported on a Ball with radius r around x.
x {z∈D:∥z−x∥≤r}
However, [56] also indicates that this choice can be refined further if knowledge of the target operator
is available.
4.2.4 Fourier neural operators
In this section we introduce FNOs as derived in [51,57]. Roughly speaking, [51,57] motivate FNOs by
considering IKNOs with kernel models (cf. Kin Definition 4.19) which only depend on the distance
of the two input points. The authors propose a parametrization of the kernels’ action in Fourier
space, thereby transforming the integral operation in state space into a multiplication operation in
Fourier space.
We introduce FNOs in two steps. First, we illustrate the idea of FNOs in Definition 4.23 by
presenting a version of FNOs using Fourier transforms and inverse Fourier transforms in L2-spaces,
whicharenotexactlyimplementableonacomputer. Secondly,wepresentanimplementableversionof
FNOs based on discrete Fourier transforms and discrete inverse Fourier transforms in Definition 4.26,
We begin with the standard definitions of Fourier transforms and inverse Fourier transforms in
Definitions 4.21 and 4.22 below. For their well-definedness see, e.g., [21, Section 3.4].
Definition 4.21 (Fourier transforms). Let d,n ∈ N, f = (f ) ∈ L2([0,1]d;Cn). Then
j j∈{1,2,...,n}
we denote by FT(f) = (FT (f)) ∈ L2(Zd;Cn) the function which satisfies for all k ∈ Zd,
j j∈{1,2,...,n}
j ∈ {1,2,...,n} that
(cid:90)
(FT (f))(k) = f (x)exp(−2πi⟨k,x⟩)dx. (203)
j j
[0,1]d
Definition 4.22 (Inverse Fourier transforms). Let d,n ∈ N, c = (c ) ∈ L2(Zd;Cn). Then
j j∈{1,2,...,n}
we denote by IFT(f) = (IFT (f)) ∈ L2([0,1]d;Cn) the equivalence class of function which
j j∈{1,2,...,n}
satisfies for all j ∈ {1,2,...,n} that
(cid:88)
IFT j(f) = c j(k)exp(2πi⟨k,(·)⟩). (204)
k∈Zd
Definition 4.23 (FNOs). Let d,n,d ,d ,d ,L ∈ N and let P: Rd P×Rd×R → Cn, R: Rd R×Zd →
P R Q
Cn×n, Q: Rd Q × Cn → R, and a: C → C be measurable functions. Then we call N an FNO with
36lifting model P, Fourier kernel model R, projection model Q, length L, and activation function a if
and only if it holds that
N: Rd P ×(Cn×n)L ×(Rd R)L ×Rd Q ×L0([0,1]d;R) → L0([0,1]d;R) (205)
isafunctionwhichsatisfiesforallP ∈ Rd P, W = (W ,W ,...,W ) ∈ (Cn×n)L, R = (R ,R ,...,R ) ∈
1 2 L 1 2 L
(Rd R)L, Q ∈ Rd Q, f ∈ L0([0,1]d;R), v ,v ,...,v ∈ L2([0,1]d;Cn) with ∀x ∈ [0,1]d,l ∈ {1,2,
0 1 L
...,L}:
v (x) = P (x,f(x)), R FT(v ) ∈ L2(Zd;Cn), (206)
0 P R l−1
l
(cid:16) (cid:17)
and v (x) = M W v (x)+(cid:2) IFT(cid:0) R FT(v )(cid:1)(cid:3) (x) (207)
l a l l−1 R l−1
l
that
N (f) = Q (v ) (208)
(P,W,R,Q) Q L
(cf. Definitions 1.2, 4.21, and 4.22).
Definition4.24(DiscretizedFouriertransform). Letd,N,n ∈ N, f = (f ) ∈ L0([0,1]d;Cn).
j j∈{1,2,...,n}
Then we denote by DFT (f) = (DFT (f)) ∈ L0({0,1,...,N −1}d;Cn) the function which
N N,j j∈{1,2,...,n}
satisfies for all k ∈ {0,1,...,N −1}d, j ∈ {1,2,...,n} that
1 (cid:88) (cid:16) (cid:17)
(DFT (f))(k) = f (r )exp −2πi⟨k,r⟩ . (209)
N,j Nd j N N
r∈{0,1,...,N−1}d
Definition 4.25 (Inverse discretized Fourier transform). Let d,N,n ∈ N, v = (v ) ∈
j j∈{1,2,...,n}
L0({0,1,...,N − 1}d;Cn). Then we denote by IDFT (f) = (IDFT (f)) ∈ L0([0,1]d;Cn)
N N,j j∈{1,2,...,n}
the function which satisfies for all x ∈ [0,1]d, j ∈ {1,2,...,n} that
(cid:88)
IDFT (f)(x) = v (k)exp(2πi⟨k,x⟩).
N,j j (210)
k∈{0,1,...,N−1}d
Definition 4.26 (Discretized FNOs). Let d,N,n,d ,d ,L ∈ N, let P: Rd P × Rd × R → Cn,
P Q
Q: Rd Q×Cn → R, and a: C → C be measurable functions. Then we call Nthe discretized FNO with
spatial discretization N, lifting model P, projection model Q, length L, and activation function a if
and only if it holds that
N: Rd P ×(Cn×n)L ×((Cn×n){0,1,...,N−1}d)L ×Rd Q ×L0([0,1]d;R) → L0([0,1]d;R) (211)
isthefunctionwhichsatisfiesforallP ∈ Rd P, W = (W ,W ,...,W ) ∈ (Cn×n)L, R = (R ,R ,...,R ) ∈
1 2 L 1 2 L
((Cn×n){0,1,...,N−1}d)L, Q ∈ Rd Q, f ∈ L0([0,1]d;R), v ,v ,...,v ∈ L0([0,1]d;Cn) with ∀x ∈ [0,1]d,l ∈
0 1 L
{1,2,...,L}:
v (x) = P (x,f(x)) and v (x) = M (cid:0) W v (x)+(cid:2) IDFT (cid:0) R DFT (v )(cid:1)(cid:3) (x)(cid:1) (212)
0 P l a l l−1 N l N l−1
that
N (f) = Q (v ) (213)
(P,W,R,Q) Q L
(cf. Definitions 1.2, 4.24, and 4.25).
37Remark 4.27. In the following we provide some explanations and remarks for Definition 4.26.
a) Note that in (212) we have that R ∈ (Cn×n){0,1,...,N−1}d is a function from {0,1,...,N −1}d
l
to Cn×n and DFT (v ) ∈ L0({0,1,...,N − 1}d;Cn) is a function from {0,1,...,N −1}d to
N l−1
Cn. Hence R DFT (v ) is a function from {0,1,...,N −1}d to Cn.
l N l−1
b) Note that in Definition 4.26 instead of using an abstract kernel model as in Definition 4.23, we
explicitly specify the action of the parameters (R ,R ,...,R ) on the Fourier modes in (212)
1 2 L
(cf. (207) in Definition 4.26). In [51, top of page 23] and [57, top of page 6] experiments with a
more general kernel model for FNOs have been performed, which yielded either same or worse
accuracy for more computational effort. Thus, the version presented in Definition 4.26 is the
way in which FNOs are typically employed.
c) InapplicationscenariosofFNOs whenoneisonlyinterestedinfunctionvalueson anequidistant
grid, the DFT and IDFT can be implemented based on the fast Fourier transform (FFT) and
inverse fast Fourier transform (IFFT), which significantly decreases the computational effort.
d) A discretized FNO with spatial discretization N ∈ N can also be used when the input function
is given through discretized values on a grid with M ∈ N gird points in each dimension and
M ≥ N. In this case in (212) DFT should be replaced with DFT composed with an operation
N M
to truncate the excess frequencies.
e) For brevity and simplicity we have presented FNOs in Definition 4.26 with complex valued
hidden functions (cf. v ,v ,...,v in (211)). For reasons of computational efficiency or when
0 1 L
there is a theoretical motivation, one might want to ensure that the hidden functions are real-
valued. To achieve this, a few modifications to Definition 4.26 can be made:
• Modify the codomain of P to Rn,
• change the domain and codomain of a to R, and
• in the definition of N, impose additional restrictions on R = (R ,R ,...,R ) ∈
1 2 L
((Cn×n){0,1,...,N−1}d)L suchthatforalll ∈ {1,2,...,L}, k = (k ,k ,...,k ) ∈ {0,1,...,N−
1 2 d
1}d we have that
R (k) = [R (mod (−k ),mod (−k ),...,mod (−k ))]∗. (214)
l l N 1 N 2 N d
These modifications make it possible to utilize the real FFT and inverse real FFT in applications
based on equidistant grids.
4.2.5 Deep Operator Networks (DeepONets)
In this section we discuss DeepONets which were introduced in [61]. DeepONets are motivated by the
fact that they possess universal approximation properties for operators (cf. [61, Theorem 1]) similar
to the universal approximation properties of fully-connected feedforward ANNs for functions on finite
dimensional spaces (cf. [13,36]).
Definition 4.28 (Unstacked DeepONets). Let d,d ,d ,m,p ∈ N, let D be a set, let x ,x ,...,x ∈
B T 1 2 m
D and let B: Rd B×Rm → Rp and T: Rd T×Rd → Rp be measurable functions. Then we call Nthe
38unstacked DeepONet neural operator with sensors x ,x ,...,x , branch model B, and trunk model
1 2 m
Tif and only if it holds that
N: Rd B ×Rd T×L0(D;R) → L0(Rd;R) (215)
is the function which satisfies for all B ∈ Rd B, T = Rd T, u ∈ L0(D;R), y ∈ Rd that
(N (u))(y) = (cid:10) B (u(x ),u(x ),...,u(x )),T(y)(cid:11) . (216)
(B,T) B 1 2 m T
Remark 4.29 (Stacked DeepONets). In [61] two versions of DeepONets are introduced: Stacked
DeepONets and unstacked DeepONets. Stacked DeepONets are a special case of the unstacked Deep-
ONets defined in Definition 4.28 when, roughly speaking, each output component of the branch model
B only depends on a distinct set of the d model parameters.
B
4.3 Physics-informed neural operators
The PINOs methodology was introduced in [57] to adapt the PINNs methodology of [45,75] to the
task of operator learning. Roughly speaking, the PINO methodology expands the general framework
for operator learning described in Method description 4.1 by incorporating an additional loss term
corresponding to the residual of the underlying PDE.
In this section we formulate the PINO methodology in the context of two frameworks. Firstly,
in Method description 4.30, we consider the case of a general abstract PDE, possibly with boundary
conditions (cf. Section 2.2.1). Secondly, in Method description 4.33, we specify the PINO method-
ology to a more specific case of a time-dependent initial value PDE with boundary conditions (cf.
Section 2.2.2).
Methoddescription4.30(PINOmethodsforboundaryvaluePDEproblems). Letd,d,d,p,M ,
Data
M ∈ N, λ ,λ ∈ [0,∞), D ⊆ Rd, D ⊆ Rd, D ⊆ Rd, I ⊆ C(D,R), let D: I ×
PDE Data PDE
Cp(D,R) → C(D,Rd), S: I → Cp(D,R), N: Rd × I → Cp(D,R), L : Rd × I × D → R,
Data
and L : Rd ×I ×D → R satisfy for all θ ∈ Rd, i ∈ I, x ∈ D, y ∈ D that
PDE
L (θ,i,x) = |(N(i))(x)−(S(i))(x)|2 and L (θ,i,y) = (cid:13) (cid:13)(cid:0) D(i,N(i))(cid:1) (y)(cid:13) (cid:13)2 , (217)
Data θ PDE θ
let (Ω,F,P) be a probability space, let I : Ω → I, k,n,m ∈ N, be random variables, let X : Ω →
k,n,m n,m
D, n,m ∈ N, be random variables, let Y : Ω → D, n,m ∈ N, be random variables, for every n ∈ N
n,m
let R : Rd ×Ω → R satisfy for all θ ∈ Rd that
n
(cid:104) (cid:105) (cid:104) (cid:105)
R (θ) = λData (cid:80)MData L (θ,I ,X ) + λPDE (cid:80)MPDE L (θ,I ,Y ) , (218)
n MData m=1 Data 1,n,m n,m MPDE m=1 PDE 2,n,m n,m
foreveryn ∈ Nlet G : Rd×Ω → Rd satisfyforallθ ∈ Rd, ω ∈ {w ∈ Ω: R (·,w) is differentiable at θ}
n n
that G (θ,ω) = (∇ R )(θ,ω), let (γ ) ⊆ R, and let Θ: N ×Ω → Rd satisfy for all n ∈ N that
n θ n n n∈N 0 0
Θ = Θ −γ G (Θ ). (219)
n+1 n n n n
Remark 4.31 (Explanations for Method description 4.30). We think of D as the residual of a
parametric PDE with boundary conditions, we think of S as (an approximation of) the solution
operator corresponding to the parametric PDE in the sense that for every parameter i ∈ I we have
that
D(i,S(i)) = 0, (220)
39we think of I as the set of admissible parameters of the PDE, we think of D as the domain of the
functions which act as parameters for the PDE, we think of D as the domain of the solution of
the PDE, we think of D as the domain on which the PDE and the boundary conditions are jointly
evaluated, we think of Nas the neural operator that we want to utilize to learn the operator S, we
think of Θ as an SGD optimizing process for the parameters of the model N, for every n ∈ N we think
of R as the empirical risk employed in the n-th step of the SGD optimization procedure Θ, we think
n
of L as the loss coming from the deviation between the model and the target operator, we think of
Data
L as the loss coming from the deviation of the model from the PDE and its boundary conditions,
PDE
we think of λ and λ as the factors which scale the losses L and L in the empirical risk
Data PDE Data PDE
evaluations, we think of M and M as the number of samples for the losses L and L in
Data PDE Data PDE
the empirical risk evaluations, we think of (I ) as the PDE parameters for which reference
1,n,m (n,m)∈N2
solutions (S(I )) are computed in empirical risk evaluations, we think of (X ) as
1,n,m (n,m)∈N2 n,m (n,m)∈N2
the points at which the model is compared to the reference solutions (S(I )) in empirical
1,n,m (n,m)∈N2
risk evaluations, we think of (I ) as the PDE parameters for which the residual of the PDE
2,n,m (n,m)∈N2
anditsboundaryconditionsarecomputedinempiricalriskevaluations,andwethinkof(Y )
n,m (n,m)∈N2
as the points at which the residual of the PDE and its boundary conditions are evaluated in empirical
risk evaluations. For sufficiently large n ∈ N we hope that N ≈ S.
Θn
Remark 4.32 (Applications and extensions of Method description 4.30). Being the squared residual
of a PDE, the term L in Method description 4.30 typically involves the derivation of the neural
PDE
operator Nwith respect to the input space variables. In [57] different ways to compute this derivation
are proposed, including automatic derivation and, in the case of FNOs, derivation in Fourier space.
Moreover, in [57] an extension of Method description 4.30 is proposed which aims to further
combine the PINO methodology with the pure PINNs methodology in [45]. Roughly speaking, after
an initial training phase as described in Method description 4.30 they suggest using the PINNs
methodology (i.e., only using L as loss) for a fixed i ∈ I to improve the approximation for this
PDE
specific instance of the parametric PDE.
Method description 4.33 (PINO methods for time-dependent initial value PDE problems). Let
T ∈ (0,∞), d,p,M ,M ,M ,M ∈ N, λ ,λ ,λ ,λ ∈ [0,∞), D ⊆ Rd,
Data PDE Boundary Init Data PDE Boundary Init
I ⊆ C(D,R), let D: Cp([0,T]×D,R) → C([0,T]×D,R), B: Cp([0,T]×D,R) → C([0,T]×∂D,R),
S: I → Cp([0,T]×D,R), N: Rd ×I → Cp([0,T]×D,R), L : Rd ×I ×[0,T]×D → [0,∞),
Data
L : Rd×I×[0,T]×D → [0,∞), L : Rd×I×∂D → [0,∞), and L : Rd×I×D → [0,∞),
PDE Boundary Init
satisfy for all θ ∈ Rd, i ∈ I, t ∈ [0,T], x ∈ D, y ∈ ∂D that
(cid:12)(cid:0) (cid:1) (cid:12)2 (cid:12)(cid:0) (cid:1) (cid:12)2 (221)
L (θ,i,t,x) = (cid:12) N(i) (t,x)−(S(i))(t,x)(cid:12) , L (θ,i,t,x) = (cid:12) D(N(i)) (t,x)(cid:12) ,
Data θ PDE θ
(cid:12)(cid:0) (cid:0) (cid:1)(cid:1) (cid:12)2 (cid:12)(cid:0) (cid:1) (cid:12)2 (222)
L (θ,i,t,y) = (cid:12) B N(i) (t,y)(cid:12) , and L (θ,i,y) = (cid:12) N(i) (t,x)−i(x)(cid:12) ,
Boundary θ Init θ
let (Ω,F,P) be a probability space, let I : Ω → I, k,n,m ∈ N, be random variables, let X : Ω →
k,n,m n,m
D, k,n,m ∈ N, be random variables, let T : Ω → [0,T], k,n,m ∈ N, be random variables, let
n,m
Y : Ω → ∂D, k,n,m ∈ N, be random variables, for every n ∈ N let R : Rd ×Ω → R satisfy for
n,m n
all θ ∈ Rd that
(cid:20) (cid:21) (cid:20) (cid:21)
R (θ) = λData
M (cid:80)Data
L (θ,I ,T ,X ) + λPDE
M (cid:80)PDE
L (θ,I ,T ,X )
n MData Data 1,n,m 1,n,m 1,n,m MPDE PDE 2,n,m 2,n,m 2,n,m
m=1 m=1
(cid:20) (cid:21) (cid:20) (cid:21)
λData
M (cid:80)Data
L (θ,I ,T ,Y ) + λPDE
M (cid:80)PDE
L (θ,I ,Y ) ,
MData Boundary 3,n,m 3,n,m 3,n,m MPDE Init 4,n,m 4,n,m
m=1 m=1
(223)
40foreveryn ∈ Nlet G : Rd×Ω → Rd satisfyforallθ ∈ Rd, ω ∈ {w ∈ Ω: R (·,w) is differentiable at θ}
n n
that G (θ,ω) = (∇ R )(θ,ω), let (γ ) ⊆ R, and let Θ: N ×Ω → Rd satisfy for all n ∈ N that
n θ n n n∈N 0 0
Θ = Θ −γ G(Θ ). (224)
n+1 n n n
Remark 4.34 (Explanations for Method description 4.33). We think of Das the residual of a time-
dependent parametric PDE with time horizon T, we think of B as the residual of the boundary
conditions of the PDE, we think of S as (an approximation of) the solution operator corresponding
to the PDE in the sense that for every parameter i ∈ I we have that
(S(i))(0,·) = i, D(i,S(i)) = 0, and B(S(i)) = 0, (225)
we think of I as the set of admissible initial conditions of the time-dependent PDE, we think of D as
the spatial domain of the PDE, we think of Nas the neural operator that we want to utilize to learn
the operator S, we think of Θ as an SGD optimizing process for the parameters of the model N, for
every n ∈ N we think of R as the empirical risk employed in the n-th step of the SGD optimization
n
procedure Θ, we think of L as the loss coming from the deviation between the model and the
Data
target operator, we think of L as the loss coming from the deviation of the model from the PDE,
PDE
wethinkof L asthelosscomingfromthedeviationofthemodelfromtheboundaryconditions
Boundary
of the PDE, we think of L as the loss coming from the deviation of the model from the initial
Init
conditions of the PDE, we think of λ , λ , λ , and λ as the factors which scale the
Data PDE Boundary Init
losses L , L , L , and L in the empirical risk evaluations, we think of M , M ,
Data PDE Boundary Init Data PDE
M , and M as the number of samples for the losses L , L , L , and L in
Boundary Init Data PDE Boundary Init
the empirical risk evaluations, and we think of (I ) , (X ) , (T ) ,
k,n,m (k,n,m)∈N3 k,n,m (k,n,m)∈N3 k,n,m (k,n,m)∈N3
and (Y ) as random variables which specify the training data for the SGD optimization
k,n,m (k,n,m)∈N3
procedure. For sufficiently large n ∈ N we hope that N ≈ S.
Θn
4.4 Other deep operator learning approaches
In this section we list a selection of other promising deep operator learning approaches in the liter-
ature. We refer, e.g., to [55] for a generalization of FNOs to more complicated geometries, we refer,
e.g., to [5] for an extension of FNOs by using Clifford layers where calculations are performed in
higher-dimensional non-commutative Clifford algebras, we refer, e.g., to [54] for a generalization of
DeepONets to a more sophisticated nonlinear architecture, we refer, e.g., to [86] for physics-informed
DeepONets, we refer, e.g., to [65] for a combination of DeepONets with spectral methods, we refer,
e.g., to [59] for an approach to learn the solution operator of a time-evolution PDE for long time
horizons based on approximating the time propagation of the PDE for short time steps using a vari-
ant of DeepONet, we refer, e.g., to [29] for a variant of DeepONets for learning solution operators to
stiff initial value problems, we refer, e.g., to [83] for a variant of DeepONets capable of taking several
functions as input, we refer, e.g., to [49] for a variant of DeepONets which uses latent representations
of the high-dimensional input and output functions, we refer, e.g., to [87] for asymptotic-preserving
convolutional DeepONets for linear transport equations, we refer, e.g., to [17] for DeepONets ap-
plied to singularly perturbed problems, we refer, e.g., to [92] for DeepONets applied to evolutionary
problems based on approximating the initial condition with a DeepONet and then applying a time
evolution to the parameters of the DeepONet, we refer, e.g., to [62] for a comparison between the
DeepONet and FNO methodologies, we refer, e.g., to [72] for a spatio-spectral neural operator, we
refer, e.g., to [88] for the Koopman neural operator, we refer, e.g., to [22,23] for deep learning-based
41reduced order models for parametric PDE problems, we refer, e.g., to [14,15,80] for approximation
rates and convergence results for neural operators, we refer, e.g., to [69] for a generalization of Deep-
ONets to operator learning on Wasserstein spaces, we refer, e.g., to [60] for a method to learn the
entire flow map associated to ordinary differential equations (ODEs) by training a different ANN
in each time-step and combining these ANNs with classical Runge-Kutta methods on different time
scales, we refer, e.g., to [30,33,46,76,94] for operator learning architectures based on CNNs, we refer,
e.g., to [9,50] for estimates for approximation and generalization errors in network-based operator
learning for PDEs, and we refer, e.g., to [5, Appendix D] and [41, Section 1.7.4] for other literature
overviews on operator learning approaches.
4.5 Numerical results
In this section we numerically test some of the operator learning models discussed in Section 4.2 in
the case of four operators related to parametric PDE problems and compare their performance with
classical numerical methods. Specifically, we consider the numerical approximation of an operator
mapping initial values to terminal values of the viscous Burgers equation in Section 4.5.1, the numer-
ical approximation of operators mapping initial values to terminal values of one and two-dimensional
Allen-Cahn equations in Section 4.5.2, and the numerical approximation of an operator mapping
source terms to terminal values of a reaction-diffusion equation in Section 4.5.3.
In every considered problem, all models are trained using the same training and validation set
with the Adam optimizer with adaptive learning rates starting at 1 instead of the SGD optimizer
1000
considered in Method description 4.1 (cf. [42, Appendix A.2] for a detailed description of our adaptive
training procedure). Moreover, we repeat the training of every operator learning model several times
with different initializations and select the best performing trained model over all training runs as the
approximation for that architecture. In every considered problem, the L2-errors of all methods are
approximated using a Monte Carlo (MC) approximation based on the same test set. The parameters
chosen to generate train, validation, and test sets, as well as additional hyperparameters for each
problem are listed in Table 5.
All the simulations were run on a remote machine on https://vast.ai equipped with an
NVIDIA GeForce RTX 3090 GPU with 24 GB RAM and an AMD Ryzen 5950X 16-core CPU
with 16 GB of total system RAM. As the evaluation time of models on GPUs can be highly vari-
able, we report the average evaluation time over 1000 test set evaluations for each method in Ta-
bles 1, 2, 3, and 4. The code for all numerical simulations is available at https://github.com/
deeplearningmethods/deep-learning-pdes.
4.5.1 Viscous Burgers equation
In this section we present numerical simulations for the approximation of an operator mapping initial
values to terminal values of the viscous Burgers equation with periodic boundary conditions. We
introduce below the viscous Burgers equation in conservative form and the corresponding operator.
Specifically, assume Method description 4.1, let T = 1, c = 1 , for every g ∈ H2 ((0,2π);R) let
10 per
u : [0,T] → H2 ((0,2π);R) be a mild solution of the PDE
g per
(cid:0) ∂ u (cid:1) (t,x) = c(∆ u )(t,x)− 1(cid:0) ∂ u2(cid:1) (t,x), (t,x) ∈ [0,T]×(0,2π), u (0) = g (226)
∂t g x g 2 ∂t g g
with periodic boundary conditions, assume I = O = H2 ((0,2π);R), and assume that the operator
per
S: I → O we want to approximate is given for all g ∈ I by
S(g) = u (T). (227)
g
42Estimated Averageevaluationtime Number
Precomputation
Method L2-error for214 testsamples oftrainable
time(ins)
in(229) over1000runs(ins) parameters
ANN(arch.:(128,256,512,256,128)) 0.017112 0.0013 328832 81
ANN(arch.:(128,256,1024,1024,256,128)) 0.013393 0.0049 1641088 107
ANN(arch.:(128,512,2048,2048,512,128)) 0.011996 0.0177 6427776 125
ANN(arch.:(128,1024,4096,8192,4096,1024,128)) 1.253236 0.2027 75778176 220
PeriodicCNN(arch.:(1,50,50,1),kernelsizes:(51,51,51)) 0.009627 0.0295 132701 170
PeriodicCNN(arch.:(1,50,50,50,1),kernelsizes:(41,41,41,41)) 0.005746 0.0427 209251 220
PeriodicCNN(arch.:(1,50,100,100,50,1),kernelsizes:(31,31,31,31,31)) 0.005837 0.2077 623401 337
PeriodicCNN(arch.:(1,100,200,200,100,100,1),kernelsizes:(31,31,31,31,31,31)) 0.014760 0.5068 2796901 790
Enc.-Dec.CNN(arch.:(1,8,32,64),kernelsizes:(2,4,2)) 0.132870 0.0034 10417 55
Enc.-Dec.CNN(arch.:(1,8,32,64,128),kernelsizes:(4,2,2,4)) 0.053636 0.0042 75153 86
Enc.-Dec.CNN(arch.:(1,8,32,64,128,256),kernelsizes:(4,2,2,4,2)) 0.008925 0.0059 206609 172
Enc.-Dec.CNN(arch.:(1,8,32,64,128,256,512,512),kernelsizes:(2,2,2,2,2,2,2)) 0.007097 0.0241 1748465 196
FNO(nr.modes:8,width:20,depth:4) 0.005961 0.0561 28965 207
FNO(nr.modes:16,width:20,depth:4) 0.004915 0.0572 41765 223
FNO(nr.modes:16,width:30,depth:4) 0.003713 0.0707 84935 226
FNO(nr.modes:16,width:30,depth:5) 0.003560 0.0797 102065 199
DeepONet(trunkarch.:(128,200,200,100),brancharch.:(1,50,100)) 0.113293 0.0007 91300 78
DeepONet(trunkarch.:(128,200,200,300),brancharch.:(1,50,100,300)) 0.055702 0.0008 161800 101
DeepONet(trunkarch.:(128,300,600,1000,500),brancharch.:(1,100,300,500)) 0.130578 0.0046 1501800 293
DeepONet(trunkarch.:(128,300,600,2000,1000),brancharch.:(1,200,500,1000)) 0.103735 0.0112 4024200 77
FDM(5CrankNicolsontimesteps) 0.023180 0.0068 0 0
FDM(10CrankNicolsontimesteps) 0.003055 0.0112 0 0
FDM(15CrankNicolsontimesteps) 0.001134 0.0155 0 0
FDM(20CrankNicolsontimesteps) 0.000633 0.0199 0 0
FEM(5CrankNicolsontimesteps) 0.024776 0.1000 0 0
FEM(10CrankNicolsontimesteps) 0.003532 0.2000 0 0
FEM(15CrankNicolsontimesteps) 0.001503 0.2499 0 0
FEM(20CrankNicolsontimesteps) 0.000867 0.3158 0 0
Spectral(5CrankNicolsontimesteps) 0.024959 0.0147 0 0
Spectral(10CrankNicolsontimesteps) 0.003515 0.0276 0 0
Spectral(15CrankNicolsontimesteps) 0.001455 0.0404 0 0
Spectral(20CrankNicolsontimesteps) 0.000815 0.0532 0 0
Table 1: Comparison of the performance of different methods for the approximation of the operator
in (227) mapping initial values to terminal values of the viscous Burgers equation in (226).
Moreover, let I: Ω → I be a N(0,106(10id −∆ )−6)-distributed random initial value where ∆ is
I x x
the Laplace operator on L2((0,2π);R) with periodic boundary conditions, fix a space discretization
N = 128, and assume for all h ∈ O that
 
2π (cid:88) (cid:90) 2π
|||h|||2 =  (h(2πx))2  ≈ (h(x))2dx. (228)
N
0
x∈{0,1,...,N−1}
N N N
We measure the quality of approximations S˜ : I → O by means of the L2-error
(cid:18) (cid:20)(cid:90) 2π (cid:21)(cid:19)1/2
(cid:0)E(cid:2) |||S˜ (I)−S(I)|||2(cid:3)(cid:1)1/2 ≈ E (cid:0) S˜ (I)(x)−u (T,x)(cid:1)2 dx . (229)
I
0
We test five different types of architectures for the neural operator N. Specifically, we test
ANNs with Gaussian Error Linear Unit (GELU) activation function (cf. Section 4.2.1, rows 1-4 in
Table1, andFigure2a), simpleperiodicconvolutionalneuraloperators(cf.Section4.2.2.1, rows5-8in
Table 1, and Figure 2b), simple convolutional neural operators with encoder-decoder architecture (cf.
Section 4.2.2.2, rows 9-12 in Table 1, and Figure 2c), FNOs (cf. Section 4.2.4, rows 13-16 in Table 1,
and Figure 2g), and DeepONets (cf. Section 4.2.5, rows 17-20 in Table 1, and Figure 2e). We also
evaluatethreedifferentclassicalapproximationmethods. Specifically, weevaluateFDMswithCrank-
Nicolsonexplicitmidpointlinearly implicit Runge-Kutta (LIRK)discretizationintime(cf.rows21-24
in Table 1 and Figure 2d), FEMs with Crank-Nicolson explicit midpoint LIRK discretization in time
(cf. rows 25-28 in Table 1 and Figure 2f), and spectral-Galerkin methods with Crank-Nicolson explicit
midpoint LIRK discretization in time (cf. rows 29-32 in Table 1 and Figure 2h). The performance of
all considered methods is summarized in Table 1 and graphically illustrated in Figure 1. In addition,
some approximations for a randomly chosen test sample are shown in Figure 2.
43ANN (arch.: (128, 256, 512, 256, 128))
100 A AN NN N ( (a ar rc ch h. .: : ( (1 12 28 8, , 2 55 16 2, , 1 20 02 44 8, , 1 20 02 44 8, , 2 55 16 2, , 1 12 28 8) )) )
ANN (arch.: (128, 1024, 4096, 8192, 4096, 1024, 128))
Periodic CNN (arch.: (1, 50, 50, 1), kernel sizes: (51, 51, 51))
Periodic CNN (arch.: (1, 50, 50, 50, 1), kernel sizes: (41, 41, 41, 41))
Periodic CNN (arch.: (1, 50, 100, 100, 50, 1), kernel sizes: (31, 31, 31, 31, 31))
Periodic CNN (arch.: (1, 100, 200, 200, 100, 100, 1), kernel sizes: (31, 31, 31, 31, 31, 31))
Enc.-Dec. CNN (arch.: (1, 8, 32, 64), kernel sizes: (2, 4, 2))
Enc.-Dec. CNN (arch.: (1, 8, 32, 64, 128), kernel sizes: (4, 2, 2, 4))
101 Enc.-Dec. CNN (arch.: (1, 8, 32, 64, 128, 256), kernel sizes: (4, 2, 2, 4, 2))
Enc.-Dec. CNN (arch.: (1, 8, 32, 64, 128, 256, 512, 512), kernel sizes: (2, 2, 2, 2, 2, 2, 2))
FNO (nr. modes: 8, width: 20, depth: 4)
FNO (nr. modes: 16, width: 20, depth: 4)
FNO (nr. modes: 16, width: 30, depth: 4)
FNO (nr. modes: 16, width: 30, depth: 5)
DeepONet (trunk arch.: (128, 200, 200, 100), branch arch.: (1, 50, 100))
DeepONet (trunk arch.: (128, 200, 200, 300), branch arch.: (1, 50, 100, 300))
102 DeepONet (trunk arch.: (128, 300, 600, 1000, 500), branch arch.: (1, 100, 300, 500))
DeepONet (trunk arch.: (128, 300, 600, 2000, 1000), branch arch.: (1, 200, 500, 1000))
FDM (5 Crank Nicolson timesteps)
FDM (10 Crank Nicolson timesteps)
FDM (15 Crank Nicolson timesteps)
FDM (20 Crank Nicolson timesteps)
FEM (5 Crank Nicolson timesteps)
FEM (10 Crank Nicolson timesteps)
103 F FE EM M ( (1 25 0 C Cr ra an nk k N Ni ic co ol ls so on n t ti im me es st te ep ps s) )
Spectral (5 Crank Nicolson timesteps)
Spectral (10 Crank Nicolson timesteps)
103 Avera1 g0 e2 evaluation time 101 S Sp pe ec ct tr ra al l ( (1 25 0 C Cr ra an nk k N Ni ic co ol ls so on n t ti im me es st te ep ps s) )
Figure 1: Graphical illustration of the performance of the methods in Table 1.
4.5.2 Allen-Cahn equation
In this section we present numerical simulations for the approximation of operators mapping initial
values to terminal values of one and two-dimensional Allen-Cahn equations. We introduce below the
considered Allen-Cahn equations and the corresponding operators.
Specifically, assume Method description 4.1, let d ∈ {1,2}, T = 3, c = 2 , for every g ∈
1000
H2 ((0,1)d;R) let u : [0,T] → H2 ((0,1)d;R) be a mild solution of the PDE
per g per
(cid:0) ∂ u (cid:1) (t,x) = c(∆ u )(t,x)−(u (t,x))3 +(u (t,x)), (t,x) ∈ [0,T]×(0,1)d, u (0) = g (230)
∂t g x g g g g
with periodic boundary conditions, assume I = O = H2 ((0,1)d;R), and assume that the operator
per
S: I → O we want to approximate is given for all g ∈ I by
S(g) = u (T). (231)
g
√
Moreover, let I: Ω → I be a N(0,25 ∗ 106( 5000id −∆ )−4 − 0.8id )-distributed random initial
I x I
value where ∆ is the Laplace operator on L2((0,1)d;R) with periodic boundary conditions, fix a
x
space discretization N ∈ N, and assume for all h ∈ O that
 
(cid:90)
1 (cid:88)
|||h|||2 =  (h(x))2  ≈ (h(x))2dx. (232)
Nd
(0,1)d
x∈{0,1,...,N−1}d
N N N
We measure the quality of approximations S˜ : I → O by means of the L2-error
(cid:18) (cid:20)(cid:90) (cid:21)(cid:19)1/2
(cid:0)E(cid:2) |||S˜ (I)−S(I)|||2(cid:3)(cid:1)1/2 ≈ E (cid:0) S˜ (I)(x)−u (T,x)(cid:1)2 dx . (233)
I
(0,1)d
We test five different types of architectures for the neural operator N. Specifically, we test ANNs
with GELU activation function (cf. Section 4.2.1, rows 1-3 in Table 2, and rows 1-3 in Table 3),
simpleperiodicconvolutionalneuraloperators(cf.Section4.2.2.1, rows4-6inTable2, androws4-6in
Table3), simpleconvolutionalneuraloperatorswithencoder-decoderarchitecture(cf.Section4.2.2.2,
rows 7-9 in Table 2, and rows 7-9 in Table 3), FNOs (cf. Section 4.2.4, rows 10-12 in Table 2, and
rows 10-12 in Table 3), and DeepONets (cf. Section 4.2.5, rows 13-15 in Table 2, and rows 13-15 in
44
rorre-2L
detamitsE1.0 ANN (arch.: (128, 256, 512, 256, 128))
ANN (arch.: (128, 256, 1024, 1024, 256, 128))
ANN (arch.: (128, 512, 2048, 2048, 512, 128))
0.8 A I Rn eN p fN u e t r( ea v nar cc lu eh e . s: o(1 lu2 t8 io, n1024, 4096, 8192, 4096, 1024, 128)) 1.0 P P Pe e er r ri i io o od d di i ic c c C C CN N NN N N ( ( (a a ar r rc c ch h h. . .: : : ( ( (1 1 1, , , 5 5 50 0 0, , , 5 5 10 0 0, , 0 1 5 , ) 0 1, , 0 k 1 0e ) ,r , 5n k 0e e ,l r s 1ni )ez ,le kss ei: z r( e n5 s e1 : l , ( s 5 4 iz1 1 e, , s5 4 : 1 1 () , 3 ) 14 ,1 3, 141 , ) 3) 1, 31, 31))
0.6 0.9 Periodic CNN (arch.: (1, 100, 200, 200, 100, 100, 1), kernel sizes: (31, 31, 31, 31, 31, 31))
Input value
Reference solution
0.4 0.8
0.7
0.2
0.6
0.0
0.5
0 1 2 3 4 5 6 0 1 2 3 4 5 6
(a) ANN plots (b) CNN periodic plots
1.0 FNO (nr. modes: 8, width: 20, depth: 4)
FNO (nr. modes: 16, width: 20, depth: 4)
FNO (nr. modes: 16, width: 30, depth: 4)
0.9 FNO (nr. modes: 16, width: 30, depth: 5)
Input value
1.0 Enc.-Dec. CNN (arch.: (1, 8, 32, 64), kernel sizes: (2, 4, 2)) Reference solution
E En nc c. .- -D De ec c. . C CN NN N ( (a ar rc ch h. .: : ( (1 1, , 8 8, , 3 32 2, , 6 64 4, , 1 12 28 8) , , 2 k 5e 6r )n ,e kl es ri nze es l : s i( z4 e, s2 : , ( 2 4,, 24 ,) ) 2, 4, 2)) 0.8
0.9 Enc.-Dec. CNN (arch.: (1, 8, 32, 64, 128, 256, 512, 512), kernel sizes: (2, 2, 2, 2, 2, 2, 2))
Input value
Reference solution
0.8 0.7
0.7
0.6
0.6
0.5 0.5
0 1 2 3 4 5 6 0 1 2 3 4 5 6
(c) CNN encoder-decoder plots (d) FNO plots
1.0 FDM (5 Crank Nicolson timesteps)
FDM (10 Crank Nicolson timesteps)
FDM (15 Crank Nicolson timesteps)
0.9 FDM (20 Crank Nicolson timesteps)
Input value
1.0 DeepONet (trunk arch.: (128, 200, 200, 100), branch arch.: (1, 50, 100)) Reference solution
DeepONet (trunk arch.: (128, 200, 200, 300), branch arch.: (1, 50, 100, 300)) 0.8
DeepONet (trunk arch.: (128, 300, 600, 1000, 500), branch arch.: (1, 100, 300, 500))
0.9 DeepONet (trunk arch.: (128, 300, 600, 2000, 1000), branch arch.: (1, 200, 500, 1000))
Input value
0.8 Reference solution 0.7
0.7
0.6
0.6
0.5
0.5
0 1 2 3 4 5 6 0 1 2 3 4 5 6
(e) DeepONet plots (f) FDM plots
1.0 F FE EM M ( (5 1 0C Cra rn ak n kN Nic io cl os lo sn o nt i tm imes et se tp es p) s) 1.0 Spectral (5 Crank Nicolson timesteps)
0.9 F F I RnE E epM M fu e t r( ( e1 2 v n5 0 a c lC C u ee r r sa a on n lk k u tN N ioi ic c no ol ls so on n t ti im me es st te ep ps s) ) 0.9 S S S I Rnp p p ep fe e e u ec c c t rt t t er r r v na a a al l l cl u e( ( (1 1 2 e s0 5 0 o C C C lur r r ta a a ion n n nk k k N N Ni i ic c co o ol l ls s so o on n n t t ti i im m me e es s st t te e ep p ps s s) ) )
0.8 0.8
0.7 0.7
0.6 0.6
0.5 0.5
0 1 2 3 4 5 6 0 1 2 3 4 5 6
(g) FEM plots (h) Spectral plots
Figure 2: Example approximation plots for a randomly chosen initial value for the viscous Burgers
equation in (226).
45Number
Estimated Timefor1024 Precomputation Numberof
Method oftrainable
L2-error evaluations(ins) time(ins) trainsteps
parameters
ANN(arch.:(64,256,512,256,64)) 0.009313 0.0012 296000 37 5600
ANN(arch.:(64,1024,4096,1024,64)) 0.006792 0.0236 8525888 50 6400
ANN(arch.:(64,1024,4096,8192,4096,1024,64)) 0.007311 0.2068 75647040 281 6400
PeriodicCNN(arch.:(1,50,50,1),kernelsizes:(31,31,31)) 0.004389 0.0423 80701 68 8800
PeriodicCNN(arch.:(1,50,50,50,1),kernelsizes:(21,21,21,21)) 0.002613 0.0412 107251 559 8000
PeriodicCNN(arch.:(1,50,100,100,50,1),kernelsizes:(21,21,21,21,21)) 0.001767 0.1040 422401 203 8000
Enc.-Dec.CNN(arch.:(1,4,16,32,128),kernelsizes:(4,2,2,2)) 0.047838 0.0019 18953 46 5600
Enc.-Dec.CNN(arch.:(1,4,16,32,128,256),kernelsizes:(4,2,2,2,2)) 0.008215 0.0035 150409 65 6800
Enc.-Dec.CNN(arch.:(1,8,16,32,64,128,256),kernelsizes:(2,2,2,2,2,2)) 0.005648 0.0051 175377 71 6400
FNO(nr.modes:16,width:20,depth:4) 0.004416 0.0298 41765 123 6000
FNO(nr.modes:16,width:30,depth:4) 0.004275 0.0435 84935 109 4800
FNO(nr.modes:16,width:30,depth:5) 0.003606 0.0498 102065 137 5200
DeepONet(trunkarch.:(64,2048,1024,512,256),brancharch.:(1,128,256)) 0.267403 0.0087 2920704 16 2400
DeepONet(trunkarch.:(64,2048,2048,1024,512),brancharch.:(1,128,256,512)) 0.140765 0.0205 7117312 34 4400
DeepONet(trunkarch.:(64,4096,8192,4096,4096,2048),brancharch.:(1,512,1024,2048)) 0.271463 0.2550 95184896 202 4800
Spectral(8CrankNicolsontimesteps) 0.055654 0.0092 0 0 0
Spectral(16CrankNicolsontimesteps) 0.000670 0.0171 0 0 0
Spectral(24CrankNicolsontimesteps) 0.000283 0.0249 0 0 0
Table 2: Comparison of the performance of different methods for the approximation of the operator
in (231) mapping initial values to terminal values of the Allen-Cahn equation in (230) in the case
d = 1.
ANN (arch.: (64, 256, 512, 256, 64))
ANN (arch.: (64, 1024, 4096, 1024, 64))
ANN (arch.: (64, 1024, 4096, 8192, 4096, 1024, 64))
Periodic CNN (arch.: (1, 50, 50, 1), kernel sizes: (31, 31, 31))
Periodic CNN (arch.: (1, 50, 50, 50, 1), kernel sizes: (21, 21, 21, 21))
101 Periodic CNN (arch.: (1, 50, 100, 100, 50, 1), kernel sizes: (21, 21, 21, 21, 21))
Enc.-Dec. CNN (arch.: (1, 4, 16, 32, 128), kernel sizes: (4, 2, 2, 2))
Enc.-Dec. CNN (arch.: (1, 4, 16, 32, 128, 256), kernel sizes: (4, 2, 2, 2, 2))
Enc.-Dec. CNN (arch.: (1, 8, 16, 32, 64, 128, 256), kernel sizes: (2, 2, 2, 2, 2, 2))
FNO (nr. modes: 16, width: 20, depth: 4)
FNO (nr. modes: 16, width: 30, depth: 4)
FNO (nr. modes: 16, width: 30, depth: 5)
DeepONet (trunk arch.: (64, 2048, 1024, 512, 256), branch arch.: (1, 128, 256))
DeepONet (trunk arch.: (64, 2048, 2048, 1024, 512), branch arch.: (1, 128, 256, 512))
102 DeepONet (trunk arch.: (64, 4096, 8192, 4096, 4096, 2048), branch arch.: (1, 512, 1024, 2048))
Spectral (8 Crank Nicolson timesteps)
Spectral (16 Crank Nicolson timesteps)
Spectral (24 Crank Nicolson timesteps)
103
103 102 101
Average evaluation time
Figure 3: Graphical illustration of the performance of the methods in Table 2.
Table 3). We also evaluate spectral-Galerkin methods with Crank-Nicolson explicit midpoint LIRK
discretization in time (cf. rows 16-18 in Table 2 and rows 16-18 in Table 3). The performance of
all considered methods in the case d = 1 is summarized in Table 2 and graphically illustrated in
Figure 3. The performance of all considered methods in the case d = 2 is summarized in Table 3
and graphically illustrated in Figure 5. In addition, some approximations for a randomly chosen test
samples are shown in Figures 4 and 6.
4.5.3 Reaction-diffusion equation
In this section we present numerical simulations for the approximation of an operator mapping source
terms to terminal values of a reaction-diffusion equation. The considered operator is inspired by the
reaction-diffusion equation in [61, Section 4.3]. We introduce below the considered reaction-diffusion
equation and the corresponding operator.
Specifically, assumeMethoddescription4.1, letT = 1, c = 5 , k = 2, foreveryg ∈ H2 ((0,2);R)
100 per
let u : [0,T] → H2 ((0,2);R) be a mild solution of the PDE
g per
(cid:0) ∂ u (cid:1) (t,x) = c(∆ u )(t,x)+k(cid:0) u (t,x)−(u (t,x))3(cid:1) +g(x), (234)
∂t g x g g g
(t,x) ∈ [0,T]×(0,2), u (0) = 0 (235)
g
46
rorre-2L
detamitsE1.0 ANN (arch.: (64, 256, 512, 256, 64))
ANN (arch.: (64, 1024, 4096, 1024, 64))
ANN (arch.: (64, 1024, 4096, 8192, 4096, 1024, 64))
0.5 I Rn ep fu et r ev na clu ee solution 1.0 P P Pe e er r ri i io o od d di i ic c c C C CN N NN N N ( ( (a a ar r rc c ch h h. . .: : : ( ( (1 1 1, , , 5 5 50 0 0, , , 5 5 10 0 0, , 0 1 5 , ) 0 1, , 0 k 1 0e ) ,r , 5n k 0e e ,l r s 1ni )ez ,le kss ei: z r( e n3 s e1 : l , ( s 3 2 iz1 1 e, , s3 2 : 1 1 () , 2 ) 2 1,1 , 2 121 , ) 2) 1, 21, 21))
Input value
0.5 Reference solution
0.0
0.0
0.5
0.5
1.0
1.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
(a) ANN plots (b) CNN periodic plots
1.0 FNO (nr. modes: 16, width: 20, depth: 4)
FNO (nr. modes: 16, width: 30, depth: 4)
FNO (nr. modes: 16, width: 30, depth: 5)
Input value
1.0 E En nc c. .- -D De ec c. . C CN NN N ( (a ar rc ch h. .: : ( (1 1, , 4 4, , 1 16 6, , 3 32 2, , 1 12 28 8) , , 2 k 5e 6r )n ,e kl es ri nze es l : s i( z4 e, s2 : , ( 2 4, , 22 ,) ) 2, 2, 2)) 0.5 Reference solution
Enc.-Dec. CNN (arch.: (1, 8, 16, 32, 64, 128, 256), kernel sizes: (2, 2, 2, 2, 2, 2))
Input value
0.5 Reference solution 0.0
0.0
0.5
0.5
1.0
1.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
(c) CNN encoder-decoder plots (d) FNO plots
1.0 Spectral (8 Crank Nicolson timesteps)
Spectral (16 Crank Nicolson timesteps)
Spectral (24 Crank Nicolson timesteps)
Input value
0.5 Reference solution
DeepONet (trunk arch.: (64, 2048, 1024, 512, 256), branch arch.: (1, 128, 256))
1.0 D D Ine e pe e up p t O O vN N ale e ut t e ( (t tr ru un nk k a ar rc ch h. .: : ( (6 64 4, , 2 40 04 98 6, , 2 80 14 98 2, , 1 40 02 94 6, , 5 41 02 9) 6, , b 2r 0a 4n 8c )h , a br rc ah n. c: h( 1 a, r c1 h2 .8 : , ( 2 15 , 6 5, 1 5 21 , 2 1) 0) 24, 2048)) 0.0
0.5 Reference solution
0.0 0.5
0.5
1.0 1.0
1.5
2.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
(e) DeepONet plots (f) FDM plots
Figure 4: Example approximation plots for a randomly chosen initial value for the Allen-Cahn
equation in (230) in the case d = 1.
Number
Estimated Timefor1024 Precomputation Numberof
Method oftrainable
L2-error evaluations(ins) time(ins) trainsteps
parameters
ANN(arch.:(4096,4096,4096,4096)) 0.337982 0.0040 50343936 70 2400
ANN(arch.:(4096,4096,8192,4096,4096)) 0.383713 0.0084 100683776 61 1200
ANN(arch.:(4096,4096,16384,16384,4096,4096)) 0.826986 0.0357 436252672 44 400
PeriodicCNN(arch.:(1,10,10,10,10,10,10,10,10,1),kernelsizes:(5,5,5,5,5,5,5,5,5,5,5)) 0.005406 0.0477 18081 562 5600
PeriodicCNN(arch.:(1,10,10,10,10,10,10,10,10,1),kernelsizes:(5,5,5,5,9,11,9,5,5,5,5)) 0.002320 0.0875 38881 765 7600
PeriodicCNN(arch.:(1,15,15,15,15,15,15,15,15,1),kernelsizes:(5,5,5,5,9,11,9,5,5,5,5)) 0.002529 0.1302 87046 1024 6800
Enc.-Dec.CNN(arch.:(1,128,256,512,1024),kernelsizes:(4,2,2,1)) 0.080153 0.0074 2366209 66 3200
Enc.-Dec.CNN(arch.:(1,128,256,512,1024,2048),kernelsizes:(4,2,2,2,1)) 0.078474 0.0084 9709313 112 3200
Enc.-Dec.CNN(arch.:(1,128,256,512,1024,2048,4096,4096),kernelsizes:(2,2,2,2,2,2,1)) 0.789321 0.0368 122966785 162 1600
FNO(nr.modes:16,width:20,depth:4) 0.008941 0.0637 473765 715 9600
FNO(nr.modes:16,width:30,depth:4) 0.011683 0.0804 1056935 807 8400
FNO(nr.modes:16,width:30,depth:5) 0.012527 0.0915 1317065 363 3600
DeepONet(trunkarch.:(4096,2048,1024,512,256),brancharch.:(2,128,256)) 0.471387 0.0013 11178368 56 4400
DeepONet(trunkarch.:(4096,2048,2048,1024,512),brancharch.:(2,128,256,512)) 0.320657 0.0020 15374976 68 4000
DeepONet(trunkarch.:(4096,4096,8192,4096,4096,2048),brancharch.:(2,512,1024,2048)) 0.876878 0.0134 111700480 29 400
Spectral(20CrankNicolsontimesteps) 0.002917 0.0385 0 0 0
Spectral(25CrankNicolsontimesteps) 0.000867 0.0479 0 0 0
Spectral(30CrankNicolsontimesteps) 0.000511 0.0573 0 0 0
Table 3: Comparison of the performance of different methods for the approximation of the operator
in (231) mapping initial values to terminal values of the Allen-Cahn equation in (230) in the case
d = 2.
47100 ANN (arch.: (4096, 4096, 4096, 4096))
ANN (arch.: (4096, 4096, 8192, 4096, 4096))
ANN (arch.: (4096, 4096, 16384, 16384, 4096, 4096))
Periodic CNN (arch.: (1, 10, 10, 10, 10, 10, 10, 10, 10, 1), kernel sizes: (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5))
Periodic CNN (arch.: (1, 10, 10, 10, 10, 10, 10, 10, 10, 1), kernel sizes: (5, 5, 5, 5, 9, 11, 9, 5, 5, 5, 5))
Periodic CNN (arch.: (1, 15, 15, 15, 15, 15, 15, 15, 15, 1), kernel sizes: (5, 5, 5, 5, 9, 11, 9, 5, 5, 5, 5))
Enc.-Dec. CNN (arch.: (1, 128, 256, 512, 1024), kernel sizes: (4, 2, 2, 1))
Enc.-Dec. CNN (arch.: (1, 128, 256, 512, 1024, 2048), kernel sizes: (4, 2, 2, 2, 1))
Enc.-Dec. CNN (arch.: (1, 128, 256, 512, 1024, 2048, 4096, 4096), kernel sizes: (2, 2, 2, 2, 2, 2, 1))
101 FNO (nr. modes: 16, width: 20, depth: 4)
FNO (nr. modes: 16, width: 30, depth: 4)
FNO (nr. modes: 16, width: 30, depth: 5)
DeepONet (trunk arch.: (4096, 2048, 1024, 512, 256), branch arch.: (2, 128, 256))
DeepONet (trunk arch.: (4096, 2048, 2048, 1024, 512), branch arch.: (2, 128, 256, 512))
DeepONet (trunk arch.: (4096, 4096, 8192, 4096, 4096, 2048), branch arch.: (2, 512, 1024, 2048))
Spectral (20 Crank Nicolson timesteps)
Spectral (25 Crank Nicolson timesteps)
102 Spectral (30 Crank Nicolson timesteps)
103
102 101
Average evaluation time
Figure 5: Graphical illustration of the performance of the methods in Table 3.
with periodic boundary conditions, assume I = O = H2 ((0,2);R), and assume that the operator
per
S: I → O we want to approximate is given for all g ∈ I by
S(g) = u (T). (236)
g
Moreover, let I: Ω → I be a N(0,108(100id −∆ )−4 − 0.8id )-distributed random source term
I x I
where ∆ is the Laplace operator on L2((0,2);R) with periodic boundary conditions, fix a space
x
discretization N = 128, and assume for all h ∈ O that
 
2 (cid:88) (cid:90) 2
|||h|||2 =  (h(2x))2  ≈ (h(x))2dx. (237)
N
0
x∈{0,1,...,N−1}
N N N
We measure the quality of approximations S˜ : I → O by means of the L2-error
(cid:18) (cid:20)(cid:90) 2 (cid:21)(cid:19)1/2
(cid:0)E(cid:2) |||S˜ (I)−S(I)|||2(cid:3)(cid:1)1/2 ≈ E (cid:0) S˜ (I)(x)−u (T,x)(cid:1)2 dx . (238)
I
0
We test five different types of architectures for the neural operator N. Specifically, we test
ANNs with GELU activation function (cf. Section 4.2.1, rows 1-3 in Table 4, and Figure 8a), simple
periodic convolutional neural operators (cf. Section 4.2.2.1, rows 4-6 in Table 4, and Figure 8b),
simple convolutional neural operators with encoder-decoder architecture (cf. Section 4.2.2.2, rows
7-9 in Table 4, and Figure 8c), FNOs (cf. Section 4.2.4, rows 10-12 in Table 4, and Figure 8d),
and DeepONets (cf. Section 4.2.5, rows 13-15 in Table 4, and Figure 8e). We also evaluate FDMs
with Crank-Nicolson explicit midpoint LIRK discretization in time (cf. rows 16-18 in Table 4 and
Figure 8f). The performance of all considered methods is summarized in Table 4 and graphically
illustrated in Figure 7. In addition, some approximations for a randomly chosen test sample are
shown in Figure 8.
48
rorre-2L
detamitsEInput value Terminal value
0.8 2 0.8 0.75
0.50
0.6 1 0.6 0.25
0.4 0 0.4 0. 00 .0 25
0.2 1 0.2 0.50
0.75
2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8
x1 x1
arch.: arch.: arch.:
(4096, 4096, 4096, 4096) (4096, 4096, 8192, 4096, 4096) (4096, 4096, 16384, 16384, 4096, 4096)
1.0 1.0 0.04
0.8 0.5 0.8 0.8
0.5 0.02
0.6 0.0 0.6 0.6
0.0 0.00
0.5
0.4 1.0 0.4 0.5 0.4 0.02
0.2 0.2 1.0 0.2
1.5 0.04
0.0 2.0 0.0 1.5 0.0
0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8
x1 x1 x1
arch.: (1, 10, 10, 10, 10, 10, 10, 10, 10, 1), arch.: (1, 10, 10, 10, 10, 10, 10, 10, 10, 1), arch.: (1, 15, 15, 15, 15, 15, 15, 15, 15, 1),
kernel sizes: (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5) kernel sizes: (5, 5, 5, 5, 9, 11, 9, 5, 5, 5, 5) kernel sizes: (5, 5, 5, 5, 9, 11, 9, 5, 5, 5, 5)
1.0
0.75 0.8 0.8 0.8
0.5 0.5 0.50
0.6 0.6 0.6 0.25 0.0 0.0 0.00
0.4 0.4 0.4 0.25
0.2 0.5 0.2 0.5 0.2 0.50
0.75
0.0 1.0 0.0 1.0 0.0 1.00
0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8
x1 x1 x1
arch.: (1, 128, 256, 512, 1024), arch.: (1, 128, 256, 512, 1024, 2048),a rch.: (1, 128, 256, 512, 1024, 2048, 4096, 4096),
kernel sizes: (4, 2, 2, 1) kernel sizes: (4, 2, 2, 2, 1) kernel sizes: (2, 2, 2, 2, 2, 2, 1)
1.0 1.0 0.0
0.8 0.8 0.8
0.5 0.5 0.2
0.6 0.6 0.6
0.0 0.0 0.4
0.4 0.4 0.4
0.2 0.5 0.2 0.5 0.2 0.6
0.0 1.0 0.0 1.0 0.0 0.8
0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8
x1 x1 x1
nr. modes: 16, width: 20, depth: 4 nr. modes: 16, width: 30, depth: 4 nr. modes: 16, width: 30, depth: 5
1.0 1.0 1.0
0.8 0.8 0.8
0.5 0.5 0.5
0.6 0.6 0.6
0.0 0.0 0.0
0.4 0.4 0.4
0.2 0.5 0.2 0.5 0.2 0.5
0.0 1.0 0.0 1.0 0.0 1.0
0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8
x1 x1 x1
trunk arch.: (4096, 2048, 1024, 512, 256), trunk arch.: (4096, 2048, 2048, 1024, 51tr2u)n, k arch.: (4096, 4096, 8192, 4096, 4096, 2048),
branch arch.: (2, 128, 256) branch arch.: (2, 128, 256, 512) branch arch.: (2, 512, 1024, 2048)
1.0
0.8 0.5 0.8 1.0 0.8 0.10
0.5 0.15
0.6 0.0 0.6 0.6
0.0 0.20
0.4 0.5 0.4 0.4 0.25
0.5
0.2 1.0 0.2 0.2 0.30
1.0
0.0 1.5 0.0 0.0 0.35
0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8
x1 x1 x1
20 Crank Nicolson timesteps 25 Crank Nicolson timesteps 30 Crank Nicolson timesteps
1.00
0.75 0.75 0.75
0.8 0.8 0.8
0.50 0.50 0.50
0.6 0.25 0.6 0.25 0.6 0.25
0.00 0.00 0.00
0.4 0.25 0.4 0.25 0.4 0.25
0.2 0.50 0.2 0.50 0.2 0.50
0.75 0.75 0.75
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8
x1 x1 x1
Figure 6: Example approximation plots for a randomly chosen initial value for the Allen-Cahn
equation in (230) in the case d = 2.
49
noitulos
ecnerefeR
NNA
NNC
cidoireP
NNC
.ceD-.cnE
ONF
teNOpeeD
lartcepS
2x
2x
2x
2x
2x
2x
2x
2x
2x
2x
2x
2x
2x
2x
2x
2x
2x
2x
2x
2xEstimated Averageevaluationtime Number
Precomputation
Method L2-error for214 testsamples oftrainable
time(ins)
in(238) over1000runs(ins) parameters
ANN(arch.:(128,512,1024,512,128)) 0.003341 0.0036 1181824 102
ANN(arch.:(128,1024,4096,1024,128)) 0.002926 0.0239 8657024 114
ANN(arch.:(128,1024,4096,8192,4096,1024,128)) 0.007312 0.2133 75778176 383
PeriodicCNN(arch.:(1,50,50,50,1),kernelsizes:(41,41,41,41)) 0.000992 0.0431 209251 292
PeriodicCNN(arch.:(1,50,100,100,50,1),kernelsizes:(31,31,31,31,31)) 0.000837 0.2079 623401 782
PeriodicCNN(arch.:(1,100,200,200,100,100,1),kernelsizes:(31,31,31,31,31,31)) 0.001662 0.5113 2796901 2992
Enc.-Dec.CNN(arch.:(1,4,16,32,128),kernelsizes:(4,2,2,4)) 0.107532 0.0024 35337 78
Enc.-Dec.CNN(arch.:(1,4,16,32,128,256),kernelsizes:(4,2,2,4,2)) 0.005597 0.0040 166793 207
Enc.-Dec.CNN(arch.:(1,16,32,64),kernelsizes:(2,4,2)) 0.236782 0.0043 12513 47
FNO(nr.modes:16,width:20,depth:4) 0.003437 0.0572 41765 289
FNO(nr.modes:16,width:30,depth:4) 0.002949 0.0708 84935 319
FNO(nr.modes:16,width:30,depth:5) 0.002612 0.0797 102065 336
DeepONet(trunkarch.:(128,200,200,300),brancharch.:(1,50,100,300)) 0.030712 0.0008 161800 131
DeepONet(trunkarch.:(128,300,600,1000,500),brancharch.:(1,100,300,500)) 0.032898 0.0048 1501800 130
DeepONet(trunkarch.:(128,300,600,2000,1000),brancharch.:(1,200,500,1000)) 0.029042 0.0115 4024200 181
FDM(4Crank-Nicolsontimesteps) 0.032469 0.0049 0 0
FDM(8Crank-Nicolsontimesteps) 0.004785 0.0084 0 0
FDM(16Crank-Nicolsontimesteps) 0.001153 0.0153 0 0
Table 4: Comparison of the performance of different methods for the approximation of the operator
in (236) mapping source terms to terminal values of the reaction-diffusion equation in (234).
ANN (arch.: (128, 512, 1024, 512, 128))
ANN (arch.: (128, 1024, 4096, 1024, 128))
ANN (arch.: (128, 1024, 4096, 8192, 4096, 1024, 128))
Periodic CNN (arch.: (1, 50, 50, 50, 1), kernel sizes: (41, 41, 41, 41))
Periodic CNN (arch.: (1, 50, 100, 100, 50, 1), kernel sizes: (31, 31, 31, 31, 31))
101 Periodic CNN (arch.: (1, 100, 200, 200, 100, 100, 1), kernel sizes: (31, 31, 31, 31, 31, 31))
Enc.-Dec. CNN (arch.: (1, 4, 16, 32, 128), kernel sizes: (4, 2, 2, 4))
Enc.-Dec. CNN (arch.: (1, 4, 16, 32, 128, 256), kernel sizes: (4, 2, 2, 4, 2))
Enc.-Dec. CNN (arch.: (1, 16, 32, 64), kernel sizes: (2, 4, 2))
FNO (nr. modes: 16, width: 20, depth: 4)
FNO (nr. modes: 16, width: 30, depth: 4)
FNO (nr. modes: 16, width: 30, depth: 5)
DeepONet (trunk arch.: (128, 200, 200, 300), branch arch.: (1, 50, 100, 300))
DeepONet (trunk arch.: (128, 300, 600, 1000, 500), branch arch.: (1, 100, 300, 500))
DeepONet (trunk arch.: (128, 300, 600, 2000, 1000), branch arch.: (1, 200, 500, 1000))
FDM (4 Crank-Nicolson time steps)
102 FDM (8 Crank-Nicolson time steps)
FDM (16 Crank-Nicolson time steps)
103
103 102 101
Average evaluation time
Figure 7: Graphical illustration of the performance of the methods in Table 4.
50
rorre-2L
detamitsE1.5 ANN (arch.: (128, 512, 1024, 512, 128))
ANN (arch.: (128, 1024, 4096, 1024, 128))
ANN (arch.: (128, 1024, 4096, 8192, 4096, 1024, 128))
1.0 I Rn ep fu et r ev na clu ee solution 1.5 P Pe er ri io od di ic c C CN NN N ( (a ar rc ch h. .: : ( (1 1, , 5 50 0, , 5 10 0, 0 5 , 0 1, 0 1 0) , , 5 k 0e ,r 1n )e ,l ks eiz re ns e: l ( s4 iz1 e, s4 : 1 (, 3 14 ,1 , 3 14 ,1 ) 3) 1, 31, 31))
Periodic CNN (arch.: (1, 100, 200, 200, 100, 100, 1), kernel sizes: (31, 31, 31, 31, 31, 31))
0.5 1.0 I Rn ep fu et r ev na clu ee solution
0.5
0.0
0.0
0.5
0.5
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
(a) ANN plots (b) CNN periodic plots
1.5 FNO (nr. modes: 16, width: 20, depth: 4)
FNO (nr. modes: 16, width: 30, depth: 4)
FNO (nr. modes: 16, width: 30, depth: 5)
1.5 Enc.-Dec. CNN (arch.: (1, 16, 32, 64), kernel sizes: (2, 4, 2)) 1.0 I Rn ep fu et r ev na clu ee solution
Enc.-Dec. CNN (arch.: (1, 4, 16, 32, 128), kernel sizes: (4, 2, 2, 4))
Enc.-Dec. CNN (arch.: (1, 4, 16, 32, 128, 256), kernel sizes: (4, 2, 2, 4, 2))
1.0 I Rn ep fu et r ev na clu ee solution 0.5
0.5
0.0
0.0
0.5 0.5
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
(c) CNN encoder-decoder plots (d) FNO plots
1.5 FDM (4 Crank-Nicolson time steps)
FDM (8 Crank-Nicolson time steps)
FDM (16 Crank-Nicolson time steps)
1.0 Input value
Reference solution
1.5 DeepONet (trunk arch.: (128, 200, 200, 300), branch arch.: (1, 50, 100, 300))
DeepONet (trunk arch.: (128, 300, 600, 1000, 500), branch arch.: (1, 100, 300, 500))
1.0 D I Rn ee p fe u ep t r eO v nN a cle u et e s( otr lu un tik o narch.: (128, 300, 600, 2000, 1000), branch arch.: (1, 200, 500, 1000)) 0.5
0.5 0.0
0.0
0.5
0.5
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
(e) DeepONet plots (f) FDM plots
Figure 8: Example approximation plots for a randomly chosen initial value for the reaction-diffusion
equation in (234).
5152
noisuffid-noitcaeR
)2=d(
nhaC-nellA
)1=d(
nhaC-nellA
sregruB
)3.5.4
noitceS
.fc(
)2.5.4
noitceS
.fc(
)2.5.4
noitceS
.fc(
)1.5.4
noitceS
.fc(
821
46
46
821
)N(
noisnemid
rep
spets
ecaps
#
gniniarT
652
46
821
4201
ezis
hctaB
004
004
004
004
.lave
rorre
.lav
neewteb
spets
.rt
#
79.0
79.0
79.0
69.0
ecnarelot
tnemevorpmI
3
1
3
5
ledom
rep
snur
.rt
fo
#
noslociN-knarC/MDF
noslociN-knarC/lartcepS
noslociN-knarC/lartcepS
noslociN-knarC/lartcepS
.los
ecnerefer
rof
mhtiroglA
tniopdim
ticilpxe
tniopdim
ticilpxe
tniopdim
ticilpxe
tniopdim
ticilpxe
tes
gniniarT
812
012
812
812
selpmas
#
215
821
821
215
noisnemid
rep
spets
ecaps
#
0001
0001
0001
0001
spets
emit
#
tes
noitadilaV
412
92
412
412
selpmas
#
4201
652
652
4201
noisnemid
rep
spets
ecaps
#
0051
0041
0041
0051
spets
emit
#
tes
tseT
412
92
412
412
selpmas
#
4201
652
652
4201
noisnemid
rep
spets
ecaps
#
0051
0041
0041
0051
spets
emit
#
.5.4
noitceS
ni
snoitalumis
laciremun
eht
ni
sledom
eht
fo
gniniart
eht
rof
sretemaraprepyH
:5
elbaTAcknowledgements
This project has been partially funded by the National Science Foundation of China (NSFC) under
grant number 12250610192. This project has also been partially funded by the Deutsche Forschungs-
gemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy EXC 2044-
390685587, Mathematics Münster: Dynamics-Geometry-Structure. This project has also been par-
tially funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) in the
frame of the priority programme SPP 2298 ’Theoretical Foundations of Deep Learning’ – Project
no. 464123384. This project has also been partially funded by the Fundamental Research Funds
for the Central Universities in China, No. 30924010943. We gratefully acknowledge Thomas Kruse
for several useful comments regarding the analysis of solutions of backward stochastic differential
equations.
References
[1] Barbara, I., Masrour, T., and Hadda, M. Train a deep neural network by minimizing an
energy function to solve partial differential equations: A review. 272–286.
[2] Beck, C., Becker, S., Grohs, P., Jaafari, N., and Jentzen, A. Solving the Kolmogorov
PDE by means of deep learning. J. Sci. Comput. 88, 3 (2021), Paper No. 73, 28.
[3] Beck, C., Hutzenthaler, M., Jentzen, A., and Kuckuck, B. An overview on deep
learning-based approximation methods for partial differential equations. Discrete Contin. Dyn.
Syst. Ser. B 28, 6 (2023), 3697–3746.
[4] Blechschmidt, J., and Ernst, O. G. Three ways to solve partial differential equations with
neural network—a review. GAMM-Mitt. 44, 2 (2021), Paper No. e202100006, 29.
[5] Brandstetter, J., Berg, R. v. d., Welling, M., and Gupta, J. K. Clifford Neural
Layers for PDE Modeling. arXiv:2209.04934 (2022).
[6] Brenner, M. Machine learning for partial differential equations. P61–003.
[7] Brunton, S. L., and Kutz, J. N. Machine learning for partial differential equations.
arxiv:2303.17078 (2023).
[8] Cai, S., Mao, Z., Wang, Z., Yin, M., and Karniadakis, G. E. Physics-informed neural
networks (PINNs) for fluid mechanics: a review. Acta Mech. Sin. 37, 12 (2021), 1727–1738.
[9] Chen, K., Wang, C., and Yang, H. Deepoperatorlearninglessensthecurseofdimensionality
for PDEs. arXiv:2301.12227 (2023).
[10] Chen, Z. Mathematical Analysis of High-Dimensional Algorithms and Models. PhD thesis,
Duke University, 2023.
[11] Chessari, J., Kawai, R., Shinozaki, Y., and Yamada, T. Numericalmethodsforbackward
stochastic differential equations: A survey. Probab. Surv. 20 (2023), 486–567.
[12] Cuomo, S., Schiano Di Cola, V., Giampaolo, F., Rozza, G., Raissi, M., and Pic-
cialli, F. Scientific machine learning through physics-informed neural networks: where we are
and what’s next. J. Sci. Comput. 92, 3 (2022), Paper No. 88, 62.
53[13] Cybenko, G. Approximation by superpositions of a sigmoidal function. Math. Control Signals
Systems 2, 4 (1989), 303–314.
[14] De Ryck, T., and Mishra, S. Genericboundsontheapproximationerrorforphysics-informed
(and) operator learning. arXiv:2205.11393 (2022).
[15] Deng, B., Shin, Y., Lu, L., Zhang, Z., and Karniadakis, G. E. Approximation rates of
DeepONets for learning operators arising from advection–diffusion equations. Neural Networks
153 (2022), 411–426.
[16] Dissanayake, M. W. M. G., and Phan-Thien, N. Neural-network-basedapproximationsfor
solving partial differential equations. Commun. Numer. Methods Engrg. 10, 3 (1994), 195–201.
[17] Du, T., Huang, Z., and Li, Y. Approximation and generalization of DeepONets for learning
operators arising from a class of singularly perturbed problems. arXiv:2306.16833 (2023).
[18] E., W. A mathematical perspective of machine learning. 914–955.
[19] E, W., Han, J., and Jentzen, A. Deep learning-based numerical methods for high-
dimensional parabolic partial differential equations and backward stochastic differential equa-
tions. Commun. Math. Stat. 5, 4 (2017), 349–380.
[20] E, W., Han, J., and Jentzen, A. Algorithms for solving high dimensional PDEs: from
nonlinear Monte Carlo to machine learning. Nonlinearity 35, 1 (2022), 278–310.
[21] Einsiedler, M., and Ward, T. Functional analysis, spectral theory, and applications, vol.276
of Graduate Texts in Mathematics. Springer, Cham, 2017.
[22] Fresca, S., Dede’, L., and Manzoni, A. A comprehensive deep learning-based approach to
reduced order modeling of nonlinear time-dependent parametrized PDEs. J. Sci. Comput. 87,
2 (2021), Paper No. 61, 36.
[23] Fresca, S., and Manzoni, A. POD-DL-ROM: enhancing deep learning-based reduced order
models for nonlinear parametrized PDEs by proper orthogonal decomposition. Comput. Methods
Appl. Mech. Engrg. 388 (2022), Paper No. 114181, 27.
[24] Germain, M., Pham, H., and Warin, X. Neural networks-based algorithms for stochastic
control and PDEs in finance. arXiv:2101.08068 (2021).
[25] Ghosh, P., Pateras, J., and Rana, P. A taxonomic survey of physics-informed machine
learning. Appl. Sci. 13 (2023).
[26] Gonon, L. Random feature neural networks learn Black-Scholes type PDEs without curse of
dimensionality. J. Mach. Learn. Res. 24 (2023), Paper No. [189], 51.
[27] Goodfellow, I., Bengio, Y., and Courville, A. Deep learning. Adaptive Computation
and Machine Learning. MIT Press, Cambridge, MA, 2016.
[28] Gopalakrishna, G. Essays in macro-finance and deep learning. Tech. rep., EPFL, 2023.
[29] Goswami, S., Jagtap, A. D., Babaee, H., Susi, B. T., and Karniadakis, G. E. Learning
stiff chemical kinetics using extended deep neural operators. arXiv:2302.12645 (2023).
54[30] Guo, X., Li, W., and Iorio, F. Convolutional Neural Networks for Steady Flow Approx-
imation. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (New York, NY, USA, 2016), KDD ’16, Association for Computing
Machinery, p. 481–490.
[31] Gupta, J., Jayaprakash, B., Eagon, M., Selvam, H. P., Molnar, C., Northrop, W.,
Shekhar, S., et al. A survey on solving and discovering differential equations using deep
neural networks. arXiv:2304.13807 (2023).
[32] Han, J., Jentzen, A., and E, W. Solving high-dimensional partial differential equations
using deep learning. Proc. Natl. Acad. Sci. USA 115, 34 (2018), 8505–8510.
[33] Heiß, C., Gühring, I., and Eigel, M. Multilevel CNNs for parametric PDEs.
arXiv:2304.00388 (2023).
[34] Herrmann, L., and Kollmannsberger, S. Deep learning in deterministic computational
mechanics. arXiv:2309.15421 (2023).
[35] Herrmann, L., and Kollmannsberger, S. Deep learning in computational mechanics: a
review. Computational Mechanics (2024), 1–51.
[36] Hornik, K., Stinchcombe, M., and White, H. Multilayer feedforward networks are uni-
versal approximators. Neural Networks 2, 5 (1989), 359–366.
[37] Hu, R., and Lauriere, M. Recent developments in machine learning methods for stochastic
control and games. arXiv:2303.10257 (2023).
[38] Huang, S., Feng, W., Tang, C., and Lv, J. Partial differential equations meet deep neural
networks: A survey. arXiv:2211.05567 (2022).
[39] Hutzenthaler, M., Kruse, T., and Nguyen, T. A. On the speed of convergence of Picard
iterations of backward stochastic differential equations. Probab. Uncertain. Quant. Risk 7, 2
(2022), 133–150.
[40] Jentzen, A., Kuckuck, B., Neufeld, A., and von Wurstemberger, P. Strong error
analysis for stochastic gradient descent optimization algorithms. arXiv:1801.09324 (2018).
[41] Jentzen, A., Kuckuck, B., and von Wurstemberger, P. Mathematical Introduction to
Deep Learning: Methods, Implementations, and Theory. arXiv:2310.20360 (2023).
[42] Jentzen, A., Riekert, A., and von Wurstemberger, P. Algorithmically designed arti-
ficial neural networks (ADANNs): Higher order deep operator learning for parametric partial
differential equations. arXiv: 2302.03286 (2023).
[43] Jianyu, L., Siwei, L., Yingjian, Q., and Yaping, H. Numerical solution of elliptic partial
differential equation using radial basis function neural networks. Neural Netw. 16, 5-6 (2003),
729–734.
[44] Karlsson Faronius, H. Solving Partial Differential Equations With Neural Networks. PhD
thesis, Uppsala University, 2023.
55[45] Karniadakis, G. E., Kevrekidis, I. G., Lu, L., Perdikaris, P., Wang, S., and Yang,
L. Physics-informed machine learning. Nature Reviews Physics 3, 6 (2021), 422–440.
[46] Khoo, Y., Lu, J., and Ying, L. Solving parametric PDE problems with artificial neural
networks. European J. Appl. Math. 32, 3 (2021), 421–435.
[47] Kianiharchegani, E. Data-Driven Exploration of Coarse-Grained Equations: Harnessing
Machine Learning. PhD thesis, he University of Western Ontario, 2023.
[48] Klenke, A. Probability theory—a comprehensive course. Universitext. Springer, Cham, [2020]
©2020. Third edition [of 2372119].
[49] Kontolati, K., Goswami, S., Karniadakis, G. E., and Shields, M. D. Learninginlatent
spaces improves the predictive accuracy of deep neural operators. arXiv:2304.07599 (2023).
[50] Kovachki, N., Lanthaler, S., and Mishra, S. On universal approximation and error
bounds for Fourier neural operators. J. Mach. Learn. Res. 22 (2021), Paper No. [290], 76.
[51] Kovachki, N., Li, Z., Liu, B., Azizzadenesheli, K., Bhattacharya, K., Stuart,
A., and Anandkumar, A. Neural operator: Learning maps between function spaces.
arXiv:2108.08481 (2021).
[52] Kumar, H., and Yadav, N. Deep learning algorithms for solving differential equations: a
survey. Journal of Experimental & Theoretical Artificial Intelligence (2023), 1–46.
[53] Lagaris, I. E., Likas, A., and Fotiadis, D. I. Artificial neural networks for solving ordinary
and partial differential equations. IEEE Trans. Neural Networks 9, 5 (1998), 987–1000.
[54] Lanthaler, S., Molinaro, R., Hadorn, P., and Mishra, S. Nonlinear reconstruction for
operator learning of PDEs with discontinuities. arXiv:2210.01074 (2022).
[55] Li, Z., Huang, D. Z., Liu, B., and Anandkumar, A. Fourier neural operator with learned
deformations for PDEs on general geometries. arXiv:2207.05209 (2022).
[56] Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A.,
and Anandkumar, A. Neural Operator: Graph Kernel Network for Partial Differential Equa-
tions. arXiv:2003.03485 (2020).
[57] Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A.,
and Anandkumar, A. Fourier neural operator for parametric partial differential equations.
In International Conference on Learning Representations (2021).
[58] Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Stuart, A., Bhattacharya, K.,
and Anandkumar, A. Multipole graph neural operator for parametric partial differential
equations. Advances in Neural Information Processing Systems 33 (2020), 6755–6766.
[59] Liu, L., and Cai, W. DeepPropNet–a recursive deep propagator neural network for learning
evolution PDE operators. arXiv:2202.13429 (2022).
[60] Liu, Y., Kutz, J. N., and Brunton, S. L. Hierarchicaldeeplearningofmultiscaledifferential
equation time-steppers. Philos. Trans. Roy. Soc. A 380, 2229 (2022), Paper No. 20210200, 17.
56[61] Lu, L., Jin, P., Pang, G., Zhang, Z., and Karniadakis, G. E. Learning nonlinear
operators via DeepONet based on the universal approximation theorem of operators. Nat. Mach.
Intell. 3, 3 (2021), 218–229.
[62] Lu, L., Meng, X., Cai, S., Mao, Z., Goswami, S., Zhang, Z., and Karniadakis, G. E.
A comprehensive and fair comparison of two neural operators (with practical extensions) based
on FAIR data. Comput. Methods Appl. Mech. Engrg. 393 (2022), Paper No. 114778, 35.
[63] Lu, L., Meng, X., Mao, Z., and Karniadakis, G. E. DeepXDE: a deep learning library
for solving differential equations. SIAM Rev. 63, 1 (2021), 208–228.
[64] Ludkovski, M. Statistical machine learning for quantitative finance. Annu. Rev. Stat. Appl.
10 (2023), 271–295.
[65] Meuris, B., Qadeer, S., and Stinis, P. Machine-learning-based spectral methods for partial
differential equations. Scientific Reports 13, 1 (2023), 1739.
[66] Mishra, C., and Gupta, D. Deep machine learning and neural networks: An overview. IAES
International Journal of Artificial Intelligence 6, 2 (2017), 66.
[67] Pagès, G. Introduction to Part V. Machine Learning and Applied Mathematics: a Game of
Hide-and-Seek? Cambridge University Press, 2023, p. 343–353.
[68] Pateras, J., Rana, P., and Ghosh, P. A taxonomic survey of physics-informed machine
learning. Applied Sciences 13, 12 (2023), 6892.
[69] Pham, H., and Warin, X. Mean-field neural networks: learning mappings on Wasserstein
space. arXiv:2210.15179 (2022).
[70] Pratama, D. A., Bakar, M. A., Man, M., and Mashuri, M. ANNs-
based method for solving partial differential equations: a survey. Available at:
https://www.preprints.org/manuscript/202102.0160/v1 (2021).
[71] Prévôt, C., and Röckner, M. A concise course on stochastic partial differential equations,
vol. 1905 of Lecture Notes in Mathematics. Springer, Berlin, 2007.
[72] Rafiq, M., Rafiq, G., Jung, H.-Y., and Choi, G. S. SSNO:Spatio-spectralneuraloperator
for functional space learning of partial differential equations. IEEE Access 10 (2022), 15084–
15095.
[73] Raissi, M., Perdikaris, P., and Karniadakis, G. E. Physics Informed Deep Learning
(Part I): Data-driven Solutions of Nonlinear Partial Differential Equations. arXiv:1711.10561
(2017).
[74] Raissi, M., Perdikaris, P., and Karniadakis, G. E. Physics Informed Deep Learning
(Part II): Data-driven Discovery of Nonlinear Partial Differential Equations. arxiv:1711.10566
(2017).
[75] Raissi, M., Perdikaris, P., and Karniadakis, G. E. Physics-informed neural networks:
a deep learning framework for solving forward and inverse problems involving nonlinear partial
differential equations. J. Comput. Phys. 378 (2019), 686–707.
57[76] Raonić, B., Molinaro, R., Rohner, T., Mishra, S., and de Bezenac, E. Convolutional
Neural Operators. arXiv:2302.01178 (2023).
[77] Rizvi, S. H. M., and Abbas, M. From data to insight, enhancing structural health monitoring
using physics-informed machine learning and advanced data collection methods. Engineering
Research Express 5, 3 (2023), 032003.
[78] Ruf, J., and Wang, W. Neural networks for option pricing and hedging: a literature review.
arXiv:1911.05620 (2019).
[79] Schmidhuber, J. Deep learning in neural networks: An overview. Neural networks 61 (2015),
85–117.
[80] Schwab, C., Stein, A., and Zech, J. Deep Operator Network Approximation Rates for
Lipschitz Operators. arXiv:2307.09835 (2023).
[81] Sharma, P., Chung, W. T., Akoush, B., and Ihme, M. A review of physics-informed
machine learning in fluid mechanics. Energies 16, 5 (2023), 2343.
[82] Sirignano, J., and Spiliopoulos, K. DGM: a deep learning algorithm for solving partial
differential equations. J. Comput. Phys. 375 (2018), 1339–1364.
[83] Tan, L., and Chen, L. Enhanced DeepONet for modeling partial differential operators con-
sidering multiple input functions. arXiv:2202.08942 (2022).
[84] Tanyu, D. N., Ning, J., Freudenberg, T., Heilenkötter, N., Rademacher, A., Iben,
U., and Maass, P. Deep learning methods for partial differential equations and related pa-
rameter identification problems. arXiv:2212.03130 (2022).
[85] Wang, S., and Perdikaris, P. Deep learning of free boundary and Stefan problems. J.
Comput. Phys. 428 (2021), Paper No. 109914, 24.
[86] Wang, S., Wang, H., and Perdikaris, P. Learning the solution operator of parametric
partial differential equations with physics-informed DeepONets. Science advances 7, 40 (2021),
eabi8605.
[87] Wu, K., Yan, X.-b., Jin, S., and Ma, Z. Asymptotic-preserving convolutional DeepONets
capture the diffusive behavior of the multiscale linear transport equations. arXiv:2306.15891
(2023).
[88] Xiong, W., Huang, X., Zhang, Z., Deng, R., Sun, P., and Tian, Y. Koopman neural
operator as a mesh-free solver of non-linear partial differential equations. arXiv:2301.10022
(2023).
[89] Yadav, S. Data Driven Stabilization Schemes for Singularly Perturbed Differential Equations.
PhD thesis, Indian Institute of Science Bangalore–560 012 (India, 2023.
[90] Yunus, R. B., Abdul Karim, S. A., Shafie, A., Izzatullah, M., Kherd, A., Hasan,
M. K., and Sulaiman, J. Anoverviewondeeplearningtechniquesinsolvingpartialdifferential
equations. Intelligent Systems Modeling and Simulation II: Machine Learning, Neural Networks,
Efficient Numerical Algorithm and Statistical Methods (2022), 37–47.
58[91] Zeiler, M. D., Krishnan, D., Taylor, G. W., and Fergus, R. Deconvolutional net-
works. In2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
(2010), pp. 2528–2535.
[92] Zhang, J., Zhang, S., Shen, J., and Lin, G. Energy-dissipative evolutionary deep operator
neural networks. arXiv:2306.06281 (2023).
[93] Zhou, M. Deep Learning Method for Partial Differential Equations and Optimal Problems. PhD
thesis, Duke University, 2023.
[94] Zhu, Y., and Zabaras, N. Bayesian deep convolutional encoder-decoder networks for surro-
gate modeling and uncertainty quantification. J. Comput. Phys. 366 (2018), 415–447.
59