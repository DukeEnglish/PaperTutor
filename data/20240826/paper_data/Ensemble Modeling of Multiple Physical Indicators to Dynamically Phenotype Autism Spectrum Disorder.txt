Ensemble Modeling of Multiple Physical Indicators to Dynamically Phenotype
Autism Spectrum Disorder
MarieHuynh* AaronKline* SaimouryaSurabhi* KaitlynDunlap*
OnurCezmiMutlu* MohammadmahdiHonarmand* ParnianAzizian*
PeterWashington† DennisP.Wall*
Abstract healthoutcomes;however,despitethepotentialforreliable
diagnosisasearlyas16–24months,theaverageageofdi-
Early detection of autism, a neurodevelopmental disor- agnosisis4.5years[14,21]. Diagnosticprocessestypically
der marked by social communication challenges, is cru- involve long waitlists and assessments, resulting in an av-
cial for timely intervention. Recent advancements have eragedelayoftwoyears[6]. Currentdiagnosesrelyonin-
utilized naturalistic home videos captured via the mo- person behavioral assessments, which are costly and sub-
bile application GuessWhat. Through interactive games jective, lacking definitive medical tests or biomarkers [5].
played between children and their guardians, GuessWhat This subjective process introduces variability and the po-
has amassed over 3,000 structured videos from 382 chil- tentialformisdiagnosisinfluencedbyclinicianexperience,
dren, both diagnosed with and without Autism Spectrum training,andsocialbiases[16].
Disorder (ASD). This collection provides a robust dataset Digital phenotyping using naturalistic home videos of-
for training computer vision models to detect ASD-related fersapromisingapproachforfasterandmoreobjectivedi-
phenotypic markers, including variations in emotional ex- agnosisofASDandotherdevelopmentalconditions.
pression, eye contact, and head movements. We have de-
veloped a protocol to curate high-quality videos from this 2.RelatedWork
dataset, forming a comprehensive training set. Utilizing
this set, we trained individual LSTM-based models using 2.1.DigitalPhenotypingofAutism
eyegaze,headpositions,andfaciallandmarksasinputfea-
Computer vision tools have shown promise in identify-
tures,achievingtestAUCsof86%,67%,and78%,respec-
ing multiple phenotypes such as emotion, eye movement,
tively. To boost diagnostic accuracy, we applied late fu-
and posture tracking in children through video analysis
sion techniques to create ensemble models, improving the
[11,20,27]. However,thesetoolsoftenlackstructureddata
overall AUC to 90%. This approach also yielded more
onchildrenwithautism, andpredictivemodelsusingindi-
equitable results across different genders and age groups.
vidualphenotypeshaverarelybeenassessedforfairnessor
Our methodology offers a significant step forward in the
integratedintoensemblemodels.
earlydetectionofASDbypotentiallyreducingthereliance
Eye tracking has been studied extensively for diagnos-
on subjective assessments and making early identification
ing ASD in children, with recent machine learning algo-
moreaccessiblyandequitable.
rithmsanalyzingeyegazepatternstodifferentiatebetween
ASDandneurotypical(NT)children[7,12,25]. Inameta-
analysisofeye-trackingbasedMLmodelsfordistinguish-
1.Introduction
ingbetweenASDandNTindividuals,Weietal.[7]reported
AutismSpectrumDisorder(ASD)isacomplexneurode- apooledclassificationaccuracyof81%,specificityof79%,
velopmental condition that affects approximately 1 in 36 and sensitivity of 84%. However, Takahashi and Hashiya
children across diverse ethnic, racial, and socioeconomic [23]reportedhighdegressofvariationineye-trackingdata
backgrounds, highlighting its widespread impact and the collection, emphasizing the need for additional data regu-
needforcomprehensivestrategiestoaddressit[15]. Chil- larization methods as well as incorporation of other data
dren with ASD face significant challenges in communica- modalities.
tion,socialinteractions,repetitivebehaviors,andrestricted Other modalities such as head movement [28], facial
interests,oftenleadingtoprofounddifficultiesineveryday expression [2,8], finger movement [22], and facial en-
functioning and development [9]. The impact of ASD ex- gagement[25]haveshownvaluableforidentifyingautism.
tendsbeyondindividualhealth,imposingasubstantialeco- Nonetheless, challenges related to data variability, gener-
nomicburdenwithan estimatedlifetimesocialcostofap- alizability across diverse populations, and integration with
proximately$3.6millionperaffectedindividual[3]. otherdiagnosticmeasuresremain.
Earlyinterventionsarecrucialasthedevelopmentalgap Buildinguponthisgroundwork,weusedeeptime-series
betweenchildrenwithASDandneurotypical(NT)children modelssuchasLSTMandGRUtoanalyzeeyegaze,face,
widensovertime[4]. Earlydiagnosiscanleadtoimproved and head features to predict ASD. Our approach will in-
cludeanadvancedfeatureengineeringpipelineandexplore
*{mahuynh,akline,mourya,kaiti.dunlap,
fusionmethodstoenhancepredictivepowerandmodelro-
cezmi,mhonar,azizian,dpwall}@stanford.edu,StanfordUniversity
†pyw@hawaii.edu,UniversityofHawaii bustness.
4202
guA
32
]VC.sc[
1v55231.8042:viXra2.2.DataFusionForASDClassification 3.2.FilteringPipeline
Recentstudieshavefocusedonthepotentialofdatafu- Theinstabilityofhomevideoscancreatenoiseanddata
sion for improving ASD classification performance. Pe- drifts that reduce the performance of the models. Further-
rochon et al. [19] recently evaluated an autism screening more,somevideosmaybefeature-poor,orthechildofin-
digitalapplicationadministeredduringpediatricwell-child terest may be too far away to extract features of interest,
visitsforchildrenaged17–36months. Theiralgorithmin- etc. Rigorous filtering and feature engineering are needed
tegratedmultipledigitalphenotypes,achievinganAUROC to account for these limitations and build a minimally vi-
of0.90,sensitivityof87.8%,specificityof80.8%,negative abletrainingsetforourtask. Ourfilteringpipelinecanbe
predictivevalueof97.8%, andpositivepredictivevalueof summarizedinthreestepsasillustratedinFigure2.
40.6%. Thesefindingsunderscorethepotentialofcombin- Thecriteriaforthevideosareasfollows: (1)theymust
ingdatasourcestoimprovediagnosticoutcomes. be of high quality, suitable for feature extraction, and (2)
Furtherextendingthislineofwork,Thompsonetal.[24] each video must focus on the child of interest. To en-
investigatedtheintegrationofacousticandlinguisticmark- sure these criteria, we filtered our videos using the Ama-
erswithvisualdata,reportingenhancementsinthesystem’s zon Rekognition Video API for face detection [1], which
abilitytopredictASDtraitsinvariedsocialcontexts. The provides estimates for sharpness, brightness, head pose,
integrationofphysiologicaldatahasbeenfurtherexplored facial landmarks, and face size for each video. For the
byNakamuraetal.[17],whodemonstratedthatcombining first criterion, we selected videos that guaranteed clarity
heartratevariabilityandskinconductancewithtraditional through sharpness and brightness metrics, featured a suf-
behavioral assessments could offer a more comprehensive ficiently large face for detecting eye-gazing features, had
understandingofASD. the eyes open for more than 70% of the video’s duration,
and maintained the face predominantly facing the camera
Our research builds on these foundations, focusing on
(as indicated by the head pose). To meet the second crite-
mobile videos captured in a home environment during
rion,weselectedvideoswherethefacewasproportionally
gameplay.Thisapproachoffersascalableandopenmethod
large enough for reliable feature extraction and where the
for dynamic data collection, crucial for conditions like
presenceofmultiplefaceswasminimal,ensuringthefocus
autismthatarenotstatic[26]. Itenablescontinuousmoni-
remainedonasinglechild. Thisfiltereddataset,referredto
toringandauthenticbehavioraldata,potentiallyfacilitating
asDatasetA,includes2123videosfeaturing288children,
earlier and more accurate diagnoses and personalized in-
asshowninTable1.
terventions. Moreover, ourworkemphasizesfusingmulti-
pleindicators—suchaseyegaze,headpositions,andfacial
Numberofvideos NumberofChildren
landmarks—extractedfromthesevideostoconstructrobust
diagnostic models. By analyzing video data in real-world ASD 2007 245
settings,weaimtobridgethegapbetweenclinicalpractice NT 116 43
andeverydayenvironments,offeringapracticalsolutionfor Total 2123 288
widespreadautismscreeninganddiagnosis.
Table1.DistributionofASDLabels-DatasetA
3.Dataset
3.3.BiasandImbalances
3.1.DataSourceandType
Ourdatasetcontainsasignificantimbalancewithrespect
Asahome-baseddigitaltherapeuticforchildrenwithde-
toASDandNTclasses,asreflectedinTable 1. DatasetA
velopmentaldelays,themobileapplicationGuessWhat[10]
hasan ASDto NTratio of17:1. Furthermore, someusers
providesarichdatasetofvideostotrainnewmodelstophe-
in the ASD class are more represented than others due to
notype autism digitally. To date, GuessWhat has amassed
moregameplay,ascanbeseeninFigure1.
more than 3,000 highly structured videos of 382 children
Sinceweonlyhave116videosforNTchildren(cf.Table
agedfrom2to12yearsoldwithandwithoutanautismdi-
1), we manually inspected all the NT videos to filter out
agnosis.
thoseofpoorqualityorthatdidnotadheretothespecified
constraints. Duringthisreview,7videoswereidentifiedas
NumberofVideosperChild
invalidforreasonssuchastheparentplayingandthechild
holding the phone, siblings playing one after the other in
102 ASD
thesamevideo,etc.
NT
Toavoidanybiastowardsaparticularchildandmitigate
theimbalancebetweenASDandNT,weunder-sampledthe
101
ASDclassbykeepingatmosttwovideoswiththehighest
quality (mean of sharpness and brightness) for each child.
100 WekeptallofourdataforNTchildren(sinceitisaminority
0 20 40 60 80 class). We obtained dataset B, which contains 700 videos
NumberofVideos (cf. Figure2).
3.4.FeatureExtraction
Figure1.PresenceofSuperusersintheASDClass.Somechildren
withASDdominatethedatawithdozensofvideos.
The input of our models consists of a sequence of k
frames, whereeachframecontainsafeaturevectorofsize
2
)elacsgol(tnuoCDatasetA DatasetB DatasetC
(N=2123) (N=700) (N=688)
RawVideos
(N=3113) ASD/NTvideos=17:1 ASD/NTvideos=5:1 ASD/NTvideos=5:1
Male/Femalevideos=3:1 Male/Femalevideos=3:1 Male/Femalevideos=3:1
245childrenwithASD 245childrenwithASD 243childrenwithASD
43NTchildren 43NTchildren 42NTchildren
VideoQuality, Balance Discardvideos
%ofFace, ASD/NT, withunsufficient
FaceSize, Balance featuresafter
HeadPose Super-users featureengineering
Figure2.KeyFilteringSteps
d (d being the dimension of features for a given modality, Withinvideos,weidentifyperiodsofmissingdata,illus-
d ∈ {2,7,60} for the eye gazing, head and face modality, tratedinFigure6.
respectively). Hence,ourinputisamultivariatetime-series Missing eye gaze data occurs when the child’s eyes are
denoted as [X1,...,Xk] where Xi = [xi,...,xi] and our closed, not facing the camera, or when the camera angle
1 d
output is a binary label Y, where Y ∈ {0,1} (0: NT, 1: is off. These instances can be informative, reflecting the
ASD). child’s interaction challenges or meaningful movements.
GuessWhat videos have an average frame rate of 28 Conversely, periods without face detection or poorly cen-
frames per second and average length of 90 seconds. We tered cameras provide no useful information. We exclude
down-sampledthedatasetBvideosto10framespersecond these uninformative windows and concatenate informative
and utilized AWS Rekognition [1] to extract frame-level segmentstoenhancefeaturecontinuity.
eye gaze, head pose, and facial features, as illustrated in Toreduceinputlength, weaveragedfeatureseverytwo
Figure3. Rekognition-providedconfidencescoresdemon- frames, for an effective frame rate of 5 fps, as shown in
stratedameanconfidenceof79.4foreyeand99.12forface Figure4. Wethennormalizedthefeaturesandrepresented
and head detection features for every frame with a face. missingframesasavectoroftokens(-1)asshowninFigure
After extraction, we dropped any video with less than 15 5., in order to incorporate missingness as a feature in the
seconds of features extracted and obtained a high-quality temporaldatastructure.
4.2.ModelTraining
Dataset C with 688 videos (cf. Figure 2). Per-video and
per-child demographics of the final dataset are detailed in
The resulting dataset was split at the child level into
Table 2.
training, validation, and test sets to prevent data leakage,
especially for children with multiple videos. Split details
Demographic VideoLevel ChildLevel are summarized in Table 3. To ensure fairness and repre-
ASD NT Total ASD NT Total sentativeness,westratifiedthesplitsbyagegroup(1-4,5-8,
Gender 9-12)andgender(Male,Female,Other),summarizedinTa-
ble4. Thisapproachmayresultinanunconventionaldata
Male 426 53 479 178 21 199
distributionbutensuresconsistentandaccuraterepresenta-
Female 124 55 179 53 21 74
tionacrossthesedemographicfactors.
NA 30 0 30 12 0 12
Age
Numberof Numberof
1-4 251 51 302 101 18 119 Videos Children
ASD NT ASD NT
5-8 194 33 227 84 14 98
Train(62.2%) 363 65 148 23
9-12 135 24 159 58 10 68 TrainUpsampled 363 148 97 23
Val(18%) 105 19 49 8
Location
Test(19.8%) 112 24 46 11
Unknown 258 90 348 109 32 141
UnitedStates 277 4 281 114 3 117 Table3. StatisticsofTrain/Val/TestSplits. Thesevideoshavethe
OutsideUS 45 14 59 20 7 27 threemodalities(eye,face,head).
Table 2. Demographics Statistics at the Video Level and the
Demographic TRAIN TEST VAL
ChildLevel
Age
1-4 198 58 46
4.ExperimentalSetup 5-8 137 44 46
9-12 93 34 32
4.1.DataPreprocessing
Gender
Mobiledataisinherentlynoisy,requiringrobustfeature Male 286 99 94
Female 119 34 26
engineering to extract informative sequences for learning.
None/Other 23 3 4
Ourfeatureengineeringpipeline, depictedinFigure4, ad-
Location
dressesseveralchallenges.
Unknown 225 68 55
Atthestartandendofvideos,ourfeaturesofinterestare UnitedStates 166 60 55
oftenmissingduetocamerastabilizationorgameinitiation OutsideUS 37 8 14
withthe child. Tomitigatethis, we truncateframeswhere
nofaceisdetected. Table4.DemographicsoftheSplits
3Video(90seconds)
...
Frames(10fps)
EyeGazing 2features Nofeatures ...
(AWSRekognition) Yaw, Detected
Pitch
60features
FaceLandmarks Eyes,Nose, Nofeatures ...
(AWSRekognition) Mouth,Face Detected
Shape
HeadLandmarks 7features Nofeatures ...
(AWS BoundingBox, Detected
Rekognition) HeadPose
Output [x1 1,...,x1 d] [NA,...,NA] ... [xk 1−1,...,xk d−1] [xk 1,...,xk d]
Figure3.FeatureExtractionScheme.Everyfeaturevectorcomeswithaconfidencescorerangingfrom0to100.
RawInput(10fps)
Truncateunstable
framesatthe
beginningandtheend
Deletewindows
wherethecamera
ismoving
Concatenate
informativewindowsof
features
X1=[x11,...,x1d] X2=[x21,...,x2d] X3=[x31,...,x3d] X4=[x41,...,x4d] X5=[x51,...,x5d] X6=[x61,...,x6d]
Downsamplingby
averaging
Z1=[(x11+x21)/2,...,(x1d+x2d)/2] Z2=[(x31+x41)/2,...,(x3d+x4d)/2] Z3=[(x51+x61)/2,...,(x5d+x6d)/2]
PreprocessedInput(5
fps) Z1=[z11,...,zd1] Z2=[z12,...,zd2] Z3=[z13,...,zd3]
Figure4.KeyFeatureEngineeringSteps
We trained LSTM and GRU models for the binary pre- ity. The intermediate fusion model concatenates the final
diction tasks using eye gazing, face, and head features, all hiddendimensionsofthepre-trainedmodelsandprocesses
implemented in PyTorch. We hypothesized that fusing all them through a multi-layer perceptron, with both models
three modalities (eye, head, face) would enhance predic- using binary cross-entropy with class weights as their loss
tive power, we tested two ensemble models: late fusion function. Given the dataset class imbalance, we also ex-
andintermediatefusion[18]. Forlatefusion, weaveraged ploredutilizingfocalloss[13]butfoundperformancesim-
the scores and used a linear layer, concatenating logit out- ilar to unmodified binary cross-entropy. We implemented
puts followed by a linear layer, to predict ASD probabil- earlystoppingwiththedefaultparametersof3epochsand
rangesfrom0to1 rangesfrom-180to180
Figure5.NormalizationandMissingnessExamplesforHeadFeatures.
4Table7summarizesthebestfusionmodels’performance
metrics. The late fusion models outperformed the inter-
mediate fusion model. The late fusion-averaging model
showedthehighestAUCandbalancedprecisionandrecall.
The intermediate fusion model had low AUC and preci-
sion,indicatinginadequacy. Narrowerconfidenceintervals
Figure6.DiscontinuitiesFactors for the late fusion models suggest consistent performance
and lower variability. The averaging model’s high macro-
averagedF1-scoreindicatesthebestoverallperformance.
deltaof0.001.
5.3.Fairnessevaluation
5.ResultsandDiscussion
Weevaluatedourmodelsforageandgendersensitivity,
excluding geographic locations due to insufficient demo-
5.1.EffectofFeatureEngineering
graphic parity differences and equalized odds differences,
Table 5 shows the impact of feature engineering on the where a lower parity difference indicates more evenly dis-
performance of three models (Eye, Head, Face) by com- tributed positive outcomes across groups, while a lower
paring their AUC and F1-scores before and after applying equalizedoddsdifferenceindicatesmoreevenlydistributed
the pipeline. The Eye model shows the most significant errorrates.
improvementthoughitsF1-scoreslightlydecreases,indicat- The Eye Model performs well 8for age groups 1-4 and
ingsometrade-offs. TheFacemodelexhibitsminimalim- 9-12 but struggles with age group 5-8, showing moderate
provement,withitsAUCmodestlyrisingandtheF1-score fairness issues (Demographic Parity Difference: 0.1732,
decreasing,showinglimitedeffectivenessofthefeatureen- Equalized Odds Difference: 0.1569). The Face and Head
gineering. Models perform well for age group 1-4 but poorly for age
group9-12,withsignificantfairnesschallenges(Equalized
Model AUCScore F1-Score(MA) Odds Difference: 0.7460). The Late Fusion (Avg) model
Eye(Raw) 0.66 0.66
shows improved performance and fairness across all age
Eye(After) 0.86 0.73
Head(Raw) 0.66 0.66 groups(DemographicParityDifference: 0.1826,Equalized
Head(After) 0.78 0.63 OddsDifference: 0.1250).
Face(Raw) 0.63 0.69
Face(After) 0.67 0.63 Regardinggender,theEyeModelperformsslightlybet-
ter for females 9, with balanced fairness metrics (Demo-
Table5. EffectofFeatureEngineeringPipelineonModelPerfor- graphic Parity Difference: 0.1078, Equalized Odds Differ-
mance.Eachmodelisthebestmodeltunedoutof40trials,chosen
ence: 0.0268). The Face and Head Models show more
onvalF1-score.
gender disparities, with lower performance for females
and higher fairness differences (Demographic Parity Dif-
ference: 0.2888,EqualizedOddsDifference: 0.4196). The
5.2.Performancecomparisonofourmodels
LateFusion(Avg)modelimprovesgenderperformanceand
The eye and head models achieved strong predictive fairness (Demographic Parity Difference: 0.2071, Equal-
power with test AUCs of 0.86 and 0.78, respectively (Fig- izedOddsDifference: 0.0769),whiletheLateFusion(Lin-
ure7d). Thefaciallandmarksmodelhadmoderatepredic- ear)modeloffersthebestbalanceofperformanceandfair-
tivepowerwithatestAUCof0.67. Table6summarizesthe ness (Demographic Parity Difference: 0.1979, Equalized
performancemetricsforeachmodel. Odds Difference: 0.0769). Given the importance of early
TheconfidenceintervalsinTable6wereobtainedusing diagnosis, the Late Fusion (Linear) model is the most bal-
bootstrapping. We resampled the test set 1000 times, cal- ancedandfairoptionforbothageandgendergroups. Fair-
culating each sample’s metrics. The 2.5th and 97.5th per- ness mitigation techniques applied to the Head and Face
centilesofthesemetricsprovidea95%confidenceinterval. Models for the age group 9-12 led to marginal improve-
The eye model outperforms the facial landmarks and ments,sowefocusedonthefairerandmoreeffectiveLate
head pose models, with narrower confidence intervals in- FusionMethods.
dicatingconsistentperformance. Theeyemodel’shighF1-
5.4.NetBenefitAnalysis
scorereflectsitsbalancedprecisionandrecall,demonstrat-
ingthebestoverallperformance. TheNetBenefitAnalysiscurve(Figure8)comparesdif-
Combining the three modalities enhanced predictive ferent models across various thresholds, illustrating their
power. The late fusion models, particularly the averag- performance in terms of net benefit. High sensitivity en-
ing method (test AUC of 0.90) and the linear method (test sures most children with ASD are correctly diagnosed for
AUCof0.84),performedstrongly. Theintermediatefusion timely interventions, while high specificity prevents incor-
modelperformedpoorly(testAUCof0.55). rectdiagnosesofNTchildren, avoidingunnecessaryinter-
We also tested Two-by-two feature combinations. For ventions.Thisbalanceensuresareliableandpracticaldiag-
latefusionbyaveraging,theEye+Headmodelachievedan nosticprocess,efficientlyallocatingresources.
AUCof0.87,Face+Head0.78,andEye+Face0.87.Forlate TheLateFusion(Avg)modelconsistentlyoffershigher
fusion with a linear layer, Eye+Head achieved an AUC of netbenefits, withtheLateFusion(Linear)andEyeModel
0.82,Face+Head0.67,andEye+Face0.90. TheEye+Face performingwellatvaryingthresholds. TheFaceandHead
combinationexcelledinbothmethods,leveragingeyegaze Modelsshowlowernetbenefits.Thedataset’sskewedASD
andfacialfeaturesforrobustpredictions. prevalenceaffectsthegeneralizabilityofthesefindings.
5Metric EyeGazing FacialLandmarks HeadPose
AUCscore 0.86[0.79,0.92] 0.67[0.55,0.78] 0.78[0.69,0.86]
Accuracy 0.79[0.72,0.86] 0.75[0.68,0.82] 0.75[0.68,0.82]
Recall(MA) 0.84[0.76,0.91] 0.65[0.55,0.76] 0.65[0.55,0.76]
Recall(WA) 0.79[0.72,0.86] 0.75[0.68,0.82] 0.75[0.68,0.82]
Precision(MA) 0.72[0.65,0.79] 0.62[0.53,0.70] 0.62[0.53,0.70]
Precision(WA) 0.89[0.84,0.92] 0.79[0.71,0.86] 0.79[0.71,0.86]
F1-score(MA) 0.73[0.65,0.81] 0.63[0.53,0.71] 0.63[0.53,0.71]
F1-score(WA) 0.82[0.75,0.87] 0.77[0.69,0.83] 0.77[0.69,0.83]
Table6.PerformanceMetricsfortheBestModelofEachTask.Thedefaultthresholdchosenforclassificationwas0.5.
(a)ReceiverOperationCharacteristicCurves(EyeandFusion (b)ReceiverOperationCharacteristicCurves-Combinations
Models). (Average).
(c)ReceiverOperationCharacteristicCurves-Combinations (d) Receiver Operation Characteristic Curves (Individual
(Linear). Models).
Figure7.ComprehensiveROCCurvesAnalysis
tomatedpreprocessingpipelineiscrucialforhandlingchal-
lengessuchasdetectingwhenachildistoofarorcloseto
thecamera,managingmultiplefaces,andtrackingdifferent
children in videos. As we continue to collect more data,
an age-centric approach will allow us to tailor games and
featurestodifferentagegroups,therebyenhancingbothen-
gagementandtheinformativenessofthefeaturesextracted.
Moreover, expanding our models to incorporate additional
modalities, such as speech, will provide us with a richer
understandingofsubjects. Additionally, focusingontime-
series pre-training and improving interpretability will en-
Figure8. NetBenefitAnalysis. Eachlineonthegraphshowsa hance robustness and transparency, making models more
model’snetbenefit(y-axis)againstthresholdvalues(x-axis).The reliable and understandable. To ensure fairness, it is cru-
dark green line (”all”) represents treating all as ASD, while the cial to collect skin color data and generalize across differ-
greenline(”none”)representstreatingnone,servingasbaselines. entskintones,therebypromotingequityandreducingbias
inourpredictiveoutcomes.
6.LimitationsandFutureDirections
References
Ourcurrentmodelsfaceseverallimitations. First,incor-
poratingaccelerometerdatacanmitigatedatadrifts,ensur- [1] Amazon Rekognition Developer Guide. https://
ingmorereliablepredictions. Secondly,developinganau- docs.aws.amazon.com/rekognition/latest/
6LateFusion LateFusion LateFusion
Metric (Eye,Head,Face) (Eye,Head,Face) (Eye,Face)
Averaging Linear Linear
AUCscore 0.90[0.84,0.95] 0.84[0.77,0.91] 0.90[0.83,0.95]
Accuracy 0.82[0.75,0.89] 0.84[0.78,0.91] 0.89[0.83,0.93]
Recall(MA) 0.88[0.81,0.92] 0.89[0.82,0.94] 0.85[0.76,0.93]
Recall(WA) 0.82[0.75,0.89] 0.84[0.78,0.91] 0.89[0.84,0.93]
Precision(MA) 0.75[0.67,0.82] 0.76[0.69,0.85] 0.80[0.72,0.89]
Precision(WA) 0.90[0.87,0.93] 0.91[0.88,0.94] 0.90[0.85,0.94]
F1-score(MA) 0.77[0.69,0.85] 0.79[0.71,0.88] 0.82[0.74,0.90]
F1-score(WA) 0.84[0.78,0.90] 0.86[0.80,0.92] 0.89[0.83,0.93]
Table7. PerformanceMetricsfortheBestFusionModels. The95%intervalswereobtainedbybootstrappingthetestset. Thedefault
thresholdchosenforclassificationwas0.5.
Demographic Equalized
Model Accuracy Recall Precision ROCAUC F1Score Parity Odds
Difference Difference
EyeModel
1-4 0.8276 0.8235 0.9767 0.8403 0.8936
5-8 0.7045 0.6667 0.9600 0.7708 0.7869 0.1732 0.1569
9-12 0.8529 0.8000 1.0000 0.9000 0.8889
FaceModel
1-4 0.8276 0.8235 0.9767 0.8403 0.8936
5-8 0.7273 0.7500 0.9000 0.6875 0.8182 0.1711 0.7460
9-12 0.6471 0.8400 0.7241 0.4756 0.7778
HeadModel
1-4 0.8276 0.8235 0.9767 0.8403 0.8936
5-8 0.7273 0.7500 0.9000 0.6875 0.8182 0.1711 0.7460
9-12 0.6471 0.8400 0.7241 0.4756 0.7778
LateFusion(Avg)
1-4 0.8621 0.8431 1.0000 0.9216 0.9149
5-8 0.7727 0.7500 0.9643 0.8125 0.8438 0.1826 0.1250
9-12 0.8235 0.7600 1.0000 0.8800 0.8636
LateFusion(Linear)
1-4 0.8621 0.8431 1.0000 0.9216 0.9149
5-8 0.8182 0.8056 0.9667 0.8403 0.8788 0.1531 0.1250
9-12 0.8529 0.8000 1.0000 0.9000 0.8889
LateFusion(Eye+Face)
1-4 0.9655 0.9608 1.0000 0.9804 0.9800
5-8 0.8182 0.8611 0.9118 0.7431 0.8857 0.1389 0.3750
9-12 0.8529 0.8800 0.9167 0.8289 0.8980
Table8.SummaryofModelFairnessacrossAgeGroups.
APIReference/Welcome.html, 2023. Accessed: [10] HaikKalantarian,PeterWashington,JesseySchwartz,Jena
2023-06-04. 2,3,9 Daniels,NickHaber,andDennisPWall. Guesswhat? to-
[2] Agnik Banerjee, Onur Cezmi Mutlu, Aaron Kline, wardsunderstandingautismfromstructuredvideousingfa-
Saimourya Surabhi, Peter Washington, and Dennis Paul cialaffect.Journalofhealthcareinformaticsresearch,3:43–
Wall. Training and profiling a pediatric facial expression 66,2019. 2
classifierforchildrenonmobiledevices: machinelearning [11] AnishLakkapragada,AaronKline,OnurCezmiMutlu,Kel-
study. JMIRformativeresearch,7:e39917,2023. 1 ley Paskov, Brianna Chrisman, Nathaniel Stockham, Peter
[3] JanetCakir,RichardEFrye,andStephenJWalker.Thelife- Washington,andDennisPaulWall. Theclassificationofab-
timesocialcostofautism: 1990–2029. ResearchinAutism normalhandmovementtoaidinautismdetection: Machine
SpectrumDisorders,72:101502,2020. 1 learningstudy. JMIRBiomedicalEngineering,7(1):e33771,
[4] Geraldine Dawson and Kathleen Zanolli. Early interven- 2022. 1
tion and brain plasticity in autism. In Autism: Neural Ba- [12] Sidrah Liaqat, Chongruo Wu, Prashanth Reddy Duggi-
sis and Treatment Possibilities: Novartis Foundation Sym- rala, Sen-ching Samson Cheung, Chen-Nee Chuah, Sally
posium251,volume251,pages266–280.WileyOnlineLi- Ozonoff, and Gregory Young. Predicting asd diagnosis in
brary,2003. 1 childrenwithsyntheticandimage-basedeyegazedata. Sig-
[5] MichaelBFirst. Diagnosticandstatisticalmanualofmental nalProcessing:ImageCommunication,94:116198,2021. 1
disorders,andclinicalutility,2013. 1 [13] Tsung-YiLin,PriyaGoyal,RossGirshick,KaimingHe,and
[6] ElizaGordon-Lipkin,JessicaFoster,andGeorginaPeacock. PiotrDolla´r. Focallossfordenseobjectdetection. InPro-
Whittlingdownthewaittime:exploringmodelstominimize ceedingsoftheIEEEinternationalconferenceoncomputer
thedelayfrominitialconcerntodiagnosisandtreatmentof vision,pages2980–2988,2017. 4
autismspectrumdisorder. PediatricClinics,63(5):851–859, [14] CatherineLord,SusanRisi,PamelaSDiLavore,CoryShul-
2016. 1 man,AudreyThurm,andAndrewPickles.Autismfrom2to
[7] Merve Gu¨lu¨, Fahrettin Bu¨yu¨kbayraktar, C¸ag˘lar Ko¨rog˘lu, 9yearsofage. Archivesofgeneralpsychiatry, 63(6):694–
Rengin Gu¨ndog˘du, and Aydın Akan. Machine learning- 701,2006. 1
basedpredictionofautismspectrumdisorderfromeyetrack- [15] Matthew J Maenner, Zachary Warren, Ashley Robinson
ingdata:Asystematicreview. ExpertSystemswithApplica- Williams, Esther Amoakohene, Amanda V Bakian, Deb-
tions,164:113938,2021. 1 orah A Bilder, Maureen S Durkin, Robert T Fitzgerald,
[8] Ming Jiang, Sunday M Francis, Diksha Srishyla, Christine SarahMFurnier,MichelleMHughes,etal. Prevalenceand
Conelea, Qi Zhao, and Suma Jacob. Classifying individ- characteristicsofautismspectrumdisorderamongchildren
uals with asd through facial emotion recognition and eye- aged8years—autismanddevelopmentaldisabilitiesmoni-
tracking. In 2019 41st Annual International Conference toringnetwork,11sites,unitedstates,2020.MMWRSurveil-
of the IEEE Engineering in Medicine and Biology Society lanceSummaries,72(2):1,2023. 1
(EMBC),pages6063–6068.IEEE,2019. 1 [16] CarlaAMazefskyandDonaldPOswald.Thediscriminative
[9] ChrisPlauche´ Johnson,ScottMMyers,etal. Identification abilityanddiagnosticutilityoftheados-g,adi-r,andgarsfor
and evaluation of children with autism spectrum disorders. childreninaclinicalsetting. Autism,10(6):533–549,2006.
Pediatrics,120(5):1183–1215,2007. 1 1
7Demographic Equalized
Model Accuracy Recall Precision ROCAUC F1Score Parity Odds
Difference Difference
EyeModel
Female 0.8235 0.7826 0.9474 0.8458 0.8571
Male 0.7778 0.7558 0.9848 0.8394 0.8553 0.1078 0.0268
FaceModel
Female 0.6765 0.6522 0.8333 0.6897 0.7317
Male 0.7677 0.8372 0.8889 0.5725 0.8623 0.2888 0.4196
HeadModel
Female 0.6765 0.6522 0.8333 0.6897 0.7317
Male 0.7677 0.8372 0.8889 0.5725 0.8623 0.2888 0.4196
LateFusion(Avg)
Female 0.8235 0.7391 1.0000 0.8696 0.8500
Male 0.8182 0.8023 0.9857 0.8627 0.8846 0.2071 0.0769
LateFusion(Linear)
Female 0.8529 0.7826 1.0000 0.8913 0.8780
Male 0.8384 0.8256 0.9861 0.8743 0.8987 0.1979 0.0769
LateFusion(Eye+Face)
Female 0.8235 0.8261 0.9048 0.8221 0.8636
Male 0.9091 0.9302 0.9639 0.8497 0.9467 0.2207 0.1041
Table9. SummaryofModelFairnessconcerningGender. TheclassOther/Noneisignoredsinceweonlyhaveasamplesizeof3inthe
testsetwhichisnotrepresentative.
[17] Yoshi Nakamura and et al. Integrating physiological data [27] ZhongZhao,ZhipengZhu,XiaobinZhang,HaimingTang,
withtraditionalbehavioralassessmentsforacomprehensive JiayiXing,XinyaoHu,JianpingLu,andXingdaQu. Identi-
understanding of autism spectrum disorders. Autism Re- fyingautismwithheadmovementfeaturesbyimplementing
searchandTreatment,16(4):560–577,2024. 2 machinelearningalgorithms. JournalofAutismandDevel-
[18] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, opmentalDisorders,pages1–12,2021. 1
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming [28] ZhongZhao,ZhipengZhu,XiaobinZhang,HaimingTang,
Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, JiayiXing,XinyaoHu,JianpingLu,andXingdaQu. Identi-
AndreasKopf,EdwardYang,ZacharyDeVito,MartinRai- fyingautismwithheadmovementfeaturesbyimplementing
son, AlykhanTejani, SasankChilamkurthy, BenoitSteiner, machinelearningalgorithms. JournalofAutismandDevel-
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An opmentalDisorders,pages1–12,2022. 1
imperative style, high-performance deep learning library.
InAdvancesinNeuralInformationProcessingSystems32,
pages8024–8035.CurranAssociates,Inc.,2019. 4
[19] SamPerochon,JMatiasDiMartino,KimberlyLHCarpen-
ter, Scott Compton, Naomi Davis, Brian Eichner, Steven
Espinosa, Lauren Franz, Pradeep Raj Krishnappa Babu,
GuillermoSapiro,etal. Earlydetectionofautismusingdig-
italbehavioralphenotyping.NatureMedicine,29(10):2489–
2497,2023. 2
[20] GuillermoSapiro,JordanHashemi,andGeraldineDawson.
Computer vision and behavioral phenotyping: an autism
casestudy,2019. 1
[21] Liming Shen, XuKun Liu, Huajie Zhang, Jing Lin,
Chengyun Feng, and Javed Iqbal. Biomarkers in autism
spectrumdisorders:Currentprogress.ClinicaChimicaActa,
502:41–54,2020. 1
[22] Roberta Simeoli, Nicola Milano, Angelo Rega, and Da-
vide Marocco. Using technology to identify children with
autism through motor abnormalities. Frontiers in Psychol-
ogy,12:635696,2021. 1
[23] YukiTakahashiandKazuoHashiya. Advancedvariationsin
eye-trackingmethodologiesforautismdiagnostics. Journal
ofAutismandDevelopmentalDisorders, 49(3):1234–1250,
2019. 1
[24] MarkThompsonetal. Integrationofacousticandlinguistic
markerswithvisualdataforautismspectrumdisorderpre-
diction. Journal of Behavioral and Developmental Neuro-
science,12(1):101–115,2024. 2
[25] Maya Varma, Peter Washington, Brianna Chrisman, Aaron
Kline,EmilieLeblanc,KelleyPaskov,NateStockham,Jae-
Yoon Jung, Min Woo Sun, and Dennis P Wall. Identifica-
tionofsocialengagementindicatorsassociatedwithautism
spectrum disorder using a game-based mobile application.
medRxiv,pages2021–06,2021. 1
[26] Peter Washington, Natalie Park, Parishkrita Srivastava,
Catalin Voss, Aaron Kline, Maya Varma, Qandeel Tariq,
Haik Kalantarian, Jessey Schwartz, Ritik Patnaik, et al.
Data-drivendiagnosticsandthepotentialofmobileartificial
intelligencefordigitaltherapeuticphenotypingincomputa-
tionalpsychiatry. BiologicalPsychiatry: CognitiveNeuro-
scienceandNeuroimaging,5(8):759–769,2020. 2
8A.Appendix
AllthresholdsaresummarizedinTable10
FilteringCriteria Variables Thresholds
Sharpness Sharpness(from0to100)>4
Quality
Brightness Brightness(From0to100)>20
NoFaceProportion NoFaceDetection(0to1)<0.6
FaceDetection MultiFaceProportion MultiFaceDetection(0to1)<0.3
FaceSize FaceSize(0to100)>0.01
Pitch(from-180to180)<45
HeadPose(Pitch,Roll,Yaw) Roll(from-180to180)<45
EyeVisibility
EyesConfidence Yaw(from-180to180)<45
EyeConfidence(0to100)>75
Table10. FilteringThresholdsbasedonFeaturesExtractedusingAWS[1]foreachvideo. Thesecriteriaconstitutetheconditionsfora
videotobeconsidered.
Table11.HyperparameterSearchSpacefortheIndividualModels
Hyperparameter RangeSearched
Model {LSTM,GRU,CNN+LSTM,CNN+GRU}
Hiddensize
{16,32,64}
(ofLSTM/GRU)
BatchSize {32,48,64,100}
NumofLayers [4,8]
DropoutProbability [0.1,0.3]
LearningRate [1e-4,1e-1]
WeightDecay [1e-5,1e-2]
Optimizer Adam
LossFunction Cross-Entropy,FocalLoss
Table12.HyperparameterSearchSpacefortheIntermediateFusionModels
Hyperparameter RangeSearched
BatchSize {16,32,64}
LearningRate [1e−4,1e−1]
FirstHiddenSize {128,192,256}
SecondHiddenSize {32,64,128}
ThirdHiddenSize {32,64}
Optimizer Adam
LossFunction Cross-Entropy
Table13.BestHyperparametersforDifferentTasksChosenBasedonValidationLoss.
Hyperparameter EyeGazing HeadPose FacialLandmarks
Model LSTM LSTM LSTM
HiddenSize 64 32 48
BatchSize 64 48 48
NumofLayers 8 4 4
DropoutProbability 0.265894 0.194464 0.179506
LearningRate 0.0324491 0.000336268 0.0464146
WeightDecay 1.15693e-05 3.82511e-05 3.66175e-05
Optimizer Adam Adam Adam
LossFunction Cross-Entropy Cross-Entropy Cross-Entropy
9Hyperparameter Value
batch size 16
learning rate 0.07165411551018012
first hidden size 256
second hidden size 32
third hidden size 64
num epochs 15
Table14.HyperparametersforIntermediateFusion
Hyperparameter Value
batch size 32
learning rate 0.0005439380832835521
num epochs 11
Table15.HyperparametersforLateFusion(Linear)
Algorithm1:TruncateWindow
Input :[X1,...,Xm]withXi=[xi,...,xi]wheremdenotesthenumberofframesanddthedimensionofthe
1 d
featureset.
Output:[Xa,...,Xb]withXi=[xi,...,xi]whereadenotesthefirstnon-Noneindex,bthelastnon-Noneindex,
1 d
anddthedimensionofthefeatureset.
1 FunctionTruncateWindow:
2
a←−1;
3
b←−1;
4
fori←1tomdo
5
ifXi≠Nonethen
6
a←i;
7 break;
8
fori←mto1step−1do
9
ifXi≠Nonethen
10
b←i;
11 break;
12
ifa≠−1andb≠−1then
13
return[Xa,...,Xb];
14 else
15 return[]; // All frames are None
10Algorithm2:CreateWindows
input :[X1,...,Xm]withXi=[xi,...,xi]wheremdenotesthenumberofframesanddthedimensionofthe
1 d
featureset.
input :s,thenumberofsecondsfordetermininguninformativemissingness(e.g.,s=2).
input :fps,framespersecondofthevideo.
output:ListofcontinuouswindowswithnomorethanssecondsofnofacedetectionwhereeachwindowW isa
sublist[Xa,...,Xb]suchthatXi=[xi,...,xi].
1 d
1
MaxMissing←s×fps
2
CurrentWindow←[]
3
AllWindows←[]
4
CountMissing←0
5
fori←1tomdo
6
ifXi≠[None,...,None]then
7 if CountMissing>MaxMissingthen
8
ifCurrentWindow≠[]then
9 AllWindows.append(
10 Truncate(CurrentWindow));
11
CurrentWindow←[];
12
CountMissing←0;
13
CurrentWindow.append(Xi);
14
CountMissing←0;
15 else
16
CountMissing←CountMissing+1;
17
ifCountMissing≤MaxMissingthen
18
CurrentWindow.append(Xi);
19
ifCurrentWindow≠[]then
20 AllWindows.append(Truncate(CurrentWindow));
21 returnAllWindows
Algorithm3:ConcatenateWindows
input :windows,listofcontinuouswindowswithnomorethanssecondsofnofacedetectionwhereeachwindow
W isasublist[Xa,...,Xb]suchthatXi=[xi,...,xi]
1 d
input :s,minimalnumberofsecondsoffeaturesforawindowtobeconsidered
input :fps,frameratepersecondofthevideo
output:Concatenatedlistofinformativewindows[Xa,...,Xb]withXi=[xi,...,xi]wheredthedimensionofthe
1 d
featureset.
1
ConcatenatedWindow←[]
2 forwindowinwindowsdo
3
if(len(window)≥(s×fps))ConcatenatedWindow.append(window)
4 returnConcatenatedWindow
11Logits
EyeModel O 1
(O +O +O )
FaceModel O 2 O final = 1 32 3
HeadModel O 3
Logits
Late EyeModel O 1
Fusion
r
e
y
FaceModel O 2 a L O final
r
a
e
n
iL
HeadModel O 3
FinalHidden
Dimensions
H
EyeModel 1
Intermediate FaceModel H 2 P L O final
Fusion M
HeadModel H 3
Figure9.LateandIntermediateFusionSchemes.
12