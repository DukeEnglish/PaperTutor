Improving Equivariant Model Training via Constraint
Relaxation
StefanosPertigkiozoglou∗ EvangelosChatzipantazis∗ ShubhenduTrivedi
UniversityofPennsylvania UniversityofPennsylvania Independent
pstefano@seas.upenn.edu vaghat@seas.upenn.edu shubhendu@csail.mit.edu
KostasDaniilidis
UniversityofPennsylvania
Archimedes,AthenaRC
kostas@cis.upenn.edu
Abstract
Equivariantneuralnetworkshavebeenwidelyusedinavarietyofapplicationsdue
totheirabilitytogeneralizewellintaskswheretheunderlyingdatasymmetries
are known. Despite their successes, such networks can be difficult to optimize
and require careful hyperparameter tuning to train successfully. In this work,
weproposeanovelframeworkforimprovingtheoptimizationofsuchmodelsby
relaxingthehardequivarianceconstraintduringtraining:Werelaxtheequivariance
constraintofthenetwork’sintermediatelayersbyintroducinganadditionalnon-
equivariancetermthatweprogressivelyconstrainuntilwearriveatanequivariant
solution. Bycontrollingthemagnitudeoftheactivationoftheadditionalrelaxation
term,weallowthemodeltooptimizeoveralargerhypothesisspacecontaining
approximateequivariantnetworksandconvergebacktoanequivariantsolutionat
theendoftraining. Weprovideexperimentalresultsondifferentstate-of-the-art
networkarchitectures,demonstratinghowthistrainingframeworkcanresultin
equivariantmodelswithimprovedgeneralizationperformance.
1 Introduction
Theexplicitincorporationoftask-specificsymmetryinthedesignandimplementationofeffective
andparameter-efficientneuralnetwork(NN)modelshasmaturedintoarationalandattractiveNN
designmeta-formalisminrecentyears—thatofgroupequivariantconvolutionalneuralnetworks
(GCNNs)(Cohen&Welling,2016;Ravanbakhshetal.,2017;Estevesetal.,2018;Kondor&Trivedi,
2018;Cohenetal.,2019;Maronetal.,2019;Weiler&Cesa,2019;Bekkers,2020;Villaretal.,2021;
Xuetal.,2022;Pearce-Crump,2023). GCNNsinvolveusingthemachineryofgroupandrepresenta-
tiontheorytocomposelayersthatareequivarianttotransformationsoftheinput. Suchnetworks,
withhard-codedsymmetry,haveproventobesuccessfulacrossawidevarietyoftasks,whileoften
affordingsignificantdataefficiency. Suchtasks/domainsinclude: RNAstructure(Townshendetal.,
2021),proteinstructure(Baeketal.,2021;Jumperetal.,2021),moleculegeneration(Satorrasetal.,
2021),medicalimaging(Winkels&Cohen,2019),naturallanguageprocessing(Gordonetal.,2020;
Petrache&Trivedi,2024),computervision(Chatzipantazisetal.,2023),robotics(Zhuetal.,2022;
Ordoñez-Apraezetal.,2024),densityfunctionaltheory(Gongetal.,2023),particlephysics(Bo-
gatskiy et al., 2020), lattice gauge theories (Boyda et al., 2021) amongst many others. GCNNs
nowhavealsomaturedenoughtohaveawell-developedtheory. Thisincludesbothprescriptive(or
architectural)theoryanddescriptiveanalysis. Ingeneral,GCNNsparticularlystandoutindomains
∗EqualContribution
Preprint.Underreview.
4202
guA
32
]GL.sc[
1v24231.8042:viXrawithdatascarcity,orwithahighdegreeofsymmetry(Kufeletal.,2023;Boydaetal.,2021),orinthe
physicalscienceswhererespectingexplicitsymmetriescouldbedictatedbyphysicallaws,violating
whichcouldleadtophysicallyimplausiblepredictions.
Despite the successes of group equivariant models, there are several outstanding challenges that
don’tyethavegeneralsatisfactorysolutions. Wediscusstwothathaveattractedrecentattention.
Thefirstchallenge—theprimarymotivationofourpaper—hastodowiththecommonobservation
thatequivariantnetworkscanbedifficulttotrain(Wangetal.,2024;Kondoretal.,2018;Liao&
Smidt,2023). Thereasonsforthisgeneraldifficultyarenotwell-understood,butitseemstooccurin
partbecausethetrainingdynamicsofsuchnetworkscanbenotablydifferentfromnon-equivariant
architectures. Forinstance,ifaGCNNoperatesentirelyinFourierspace(Bogatskiyetal.,2020;
Kondoretal.,2018;Xuetal.,2022), mostoftheusualintuitionabouttrainingNNmodelsdoes
notapply. Further, dependingonthelevelofequivarianceerrortoleranceforatask, theinternal
layerscouldbecomputationallyintensive,andinvolvee.g. higher-ordertensorproducts. Notably,the
abovedifficultyarisesevenwhenthemodeliscorrectlyspecifiedi.e. themodelandthedataencode
thesamesymmetry. ThesecondchallengewithGCNNs,hastodowiththedownsidesofworking
withexactequivariancewhenthedataitselfmighthavesome(possibly)relaxedsymmetry. This
hasrecentlyledtoaspurtofworkondevelopingmoreflexiblenetworksthatcanvarytheamount
ofequivariancedependingonthetask(Finzietal.,2021;Romero&Lohit,2022;vanderOuderaa
etal.,2022;Wangetal.,2022;Huangetal.,2023;Petrache&Trivedi,2023). Suchmodelsgenerally
improveaccuracyandwillsometimesalsosimplifytheoptimizationprocessasaside-effect. Broadly,
proposedsolutionsinvolveaddingadditionalregularizationtermsthatpenalizeforrelaxationerrors,
solvingfortheproblemofmodelmis-specification(Petrache&Trivedi,2023).
However,eventhoughthereisnowworkonrelaxed2equivariantnetworksthataddressesmodelmis-
specification,existingworksdon’tfocusonimprovingtheoptimizationprocessdirectly. Inthispaper,
wetakeasteptowardsexaminingthisquestioninmoredetail.Wemakethecasethatevenifweassume
thatthemodeliscorrectlyspecified,relaxingtheequivarianceconstraintduringoptimizationand
thenprojectingbacktotheequivariantspacecanitselfhelpinimprovingperformance. Weconjecture
thataprimereasonfortheoptimizationdifficultyofGCNNs,ascomparedtonon-equivariantmodels,
isthattheirsolution-spacemightbetooseverelyconstrained. Wederiveregularizationtermsthat
encourageeachlayertooperateinalargerhypothesisspaceduringtraining—thanbeingconstrained
toonlybeintheintertwinerspace—whileencouragingequivariantsolutions. Aftertheoptimization
iscomplete,weprojectthesolutionbackontothespaceofequivariantsolutions. Theapproachcan
alsobeadaptedtobetteroptimizeapproximatelyequivariantnetworksinasimilarmanner. Thefocus
ofourworkthusdistinguishesitfromworksonrelaxedequivariance—wearenotconcernedwith
mis-specification,butratherwithisolatingtheoptimizationprocessitself.
Belowwesummarizethemaincontributionsofourwork:
• Wepresentanoveltrainingframeworkthatcanimprovetheperformanceofequivariant
neuralnetworksbyrelaxingtheequivarianceconstraintduringtrainingandprojectingback
tothespaceofequivariantmodelsduringtesting.
• Wepresentaformulationthatextendsexistingequivariantneuralnetworkarchitecturesto
beapproximatelyequivariant. Weshowhowtrainingontherelaxednetworkcanimprove
theperformanceofitsequivariantsubnetwork.
• Weprovideexperimentalevidenceshowcasinghowourframeworkimprovestheperfor-
manceofexistingstate-of-the-artequivariantarchitectures.
2 RelatedWork
Thereislittlepriorworkonprovidinggeneralproceduresforimprovingtheoptimizationprocessfor
equivariantneuralnetworksdirectly. Elesedy&Zaidi(2021)sketchedaprojectedgradientmethodto
constructequivariantnetworksandsuggestedaregularizationschemethatcouldbeusedtoimplement
approximateequivariance. However,thiswasproposedasanasideinthepaper(sections7.2and
7.3),withoutempiricalortheoreticalbacking. Ourworkalsoinvolvesaprojectedgradientprocedure.
However,theregularizationschemethatweproposeissubstantiallydifferent.
2Weuse“relaxed”toencompassnotionsofpartialandapproximateequivariance(Petrache&Trivedi,2023).
2Workonapproximateandpartialequivariance(Finzietal.,2021;Romero&Lohit,2022;vander
Ouderaaetal.,2022;Wangetal.,2022;Huangetal.,2023;Petrache&Trivedi,2023;Wangetal.,
2023)seemssuperficiallyrelatedtoours,butcomeswithadifferentmotivation. Suchmethodsaim
to match data symmetry with model symmetry and are not explicitly concerned with improving
optimization.Asaresult,theyaredesignedtoaddresstaskswitheitherinherentrelaxedsymmetriesor
taskswheretheunderlyingrelaxedsymmetryismisspecified. Contrarytothat,ourmethodfocuseson
caseswheretheunderlyingsymmetryisknownexactly,andtherelaxationoftheequivariantconstraint
isusedonlyduringtrainingasawaytoimprovetheoptimization. TheworksofFinzietal.(2021);
vanderOuderaaetal.(2022);Gruveretal.(2023);Ottoetal.(2024);Petrache&Trivedi(2023)are
nonethelessrelevantsincetheyprovidemethodsformeasuringrelaxedequivariance,comprisingof
regularizationschemesthatarerelatedtothoseusedinourpaper,sincewealsoneedmeasuresof
relaxation. Infact,theworkofGruveretal.(2023)directlyinspiresonecomponentofourmethod.
Onthetheoreticalside,Petrache&Trivedi(2023)studiedgeneralization-approximationtradeoffsin
approximately/fullyequivariantCNNsinaverygeneralsetting,characterizingtheeffectofmodel
mis-specificationonperformance. Theyquantifyequivarianceasimprovingthegeneralizationerror,
andthealignmentofdataandmodelsymmetriesasimprovingtheapproximationerror. Theyleave
theimpactofimprovingtheoptimizationerrorforfuturework. Whilewedonotprovidetheoretical
results,ourworkcouldbeseenasfocusingonoptimizationerrorcomponentoftheclassicalpicture3.
Maileetal.(2023)proposedwhattheycallanequivariancerelaxationmorphism,whichreparamter-
izesanequivariantlayertooperatewithequivarianceconstraintsonasubgroup,butwiththegoalof
architecturesearch. Flinth&Ohlsson(2023)provideananalysisoftheoptimizationdynamicsof
equivariantmodelsandcomparethemtonon-equivariantmodelsfedwithaugmenteddata. However,
theydon’tusetheanalysistoprovideinsightsonimprovingtheoptimizationprocedureitself.
Severalresearchershaverecentlytriedtocircumventoptimizationdifficultiesinotherways. For
instance,Mondaletal.(2023)suggestsusingequivariance-promotingcanonicalizationfunctionson
topoflargepre-trainedmodels. Theworkof Basuetal.(2023b)operateswithasimilarmotivation
butwithoutcanonicalization. Yetanotherrepresentativeofworkwithafine-tuningmotivation,but
inadifferentcontextis(Basuetal.,2023a). Finally,simplifyingequivariantnetworkswithheavy
equivariantlayersandimprovingtheirscalabilityisanactiveareaofworkandisrelatedtoeasing
optimization. Suchworksusuallyemploytoolsfromrepresentationtheory,tensoralgebra,orexploit
samplingtheoremsovercompactgroupsandtheirhomogeneousspaces,suchasPassaro&Zitnick
(2023);Luoetal.(2024);Cobbetal.(2021);Ocampoetal.(2023).
3 Method
Tointroduceourproposedoptimizationframework,wefirstclearlydefinetheequivariantconstraint
thatthemodelsweaimtotrainmustsatisfy. Assumeafunctionf : Rn → Rm andagroupG4
acting on the input and output spaces via the general linear representations ρ : G → GL(Rn),
in
ρ :G→GL(Rm). ThenthefunctionissaidtobeequivarianttotheactionofgroupGifforall
out
g ∈Gitsatisfiesthefollowingconstraint:
f(ρ (g)x)=ρ (g)f(x), forallx∈Rn (1)
in out
Assumingweuseaneuralnetworktoapproximatethefunctionabove,thedefinitionofequivariance
asstateddoesn’timposespecificconstraintsontheindividuallayersofthenetwork. Nevertheless,
most of the current state-of-the-art equivariant architectures are a composition of simpler layers
eachoneofwhichisconstrainedtobeequivariant. Inthiscase,theoverallmodelistheresultofa
compositionf =f ◦f ◦...f ◦f ofsimplerequivariantlayersf :V →V ,whereV ,V
N N−1 2 1 i i i+1 i i+1
aretheinputandoutputspacesonwhichthegroupGactswiththecorrespondingrepresentations
ρ ,ρ (assumingV =Rn,V =Rmaretheinputandoutputspacesrespectively).
Vi Vi+1 1 N
Inthisworkwefocusonafamilyofmodelsasdescribedabove—thataredefinedthroughacompo-
sitionofsimplerequivariantlinearlayers. Duringstandardtrainingthelinearlayersareoptimized
overthesetofintertwinersH ,i.e. thesetoflinearmapsbetweentherepresentations(V ,ρ )and
i i Vi
(V ,ρ )thathavetheequivariancepropertyasstatedinEquation1. Thesetofintertwinersis
i+1 Vi+1
onlyasubsetofthesetofallpossiblelinearmapsfromV toV ,andasaresult,theyhaveareduced
i i+1
3Characterizingmodelperformanceasgeneralizationerror+approximationerror+optimizationerror
4WeassumehenceforththatwearealwaysdealingwithMatrixLiegroups.
3Standard Training Training via Constraint Relaxation
Train and Test on Train on relaxed Project to Equivariant Space
Constrained Equivariant Space Approximate Equivariant Space during Testing
Equivariant Equivariant Linear Layer Equivariant Linear Layer
Layer Layer Layer
Equivariant Equivariant Linear Layer Equivariant Linear Layer
Layer Layer Layer
Figure1: StandardtrainingofequivariantNNsisconstrainedtoalimitedparameterspacewhich
canresultinachallengingtrainingprocess. Weproposetorelaxtheseequivariantconstraintsduring
training,allowingoptimizationoverabroaderspaceofapproximatelyequivariantmodels. During
testing,weprojectthetrainedmodelbacktotheconstrainedspace—arrivingatanequivariantmodel
withenhancedperformancecomparedtoequivalentmodelstrainedwiththestandardprocess.
numberoffreeparameters. Weproposetofacilitatetrainingoverthisconstrainedspacebyrelaxing
theconstraintimposedontheintermediatelayersandoptimizingoveralargerhypothesisspaceH˜
whichisasupersetofthesetofequivariantmodelsH ⊂ H˜. Atrivialapproachistocompletely
relaxtheconstraintandsolelyoptimizeoverthelargersetcontainingallmodels. Theproblemwith
suchanapproachisthatitcompletelyabandonstheconceptofequivarianceandalltheattendant
generalizationbenefits. Consequently,toexpandthehypothesisspacewhilekeepingthebenefitsof
equivariantmodels,weneedarelaxationsuchthat:
• Given a non-equivariant model f ∈ H˜, we can efficiently return to an equivariant one
f¯∈H.
(cid:82)
• The relaxed model has a small equivariance error P = E ∥ρ (g)f(x) −
ee x∼p(x) G out
f(ρ (g)x)∥dg.Thisimpliesthatalthoughweextendthespaceofmodelsweoptimizeover,
in
wedonotdivergetoofarawayfromthespaceofequivariantsolutions.
• After we project back to the equivariant space, the error of the projection P =
pe
E
(cid:2) ∥f(x)−f¯(x)∥(cid:3)
is also small. This ensures that while we optimize the less
x∼p(x)
constrained model, we can return to the equivariant one without sacrificing the overall
performance.
Thefirstobjectivecanbesatisfiedbydefininganintermediatelayeroftheform:
f(x)=f (x)+θWx, θ ≥0andf ∈H,W ∈R|Vout|×|Vin| (2)
e e
whereH isthesetcontainingallpossibleequivariantsolutions. Hereitiseasytoseethatwecan
returntoanequivariantmodelbysettingθ =0,whichwerefertoasprojectiontotheequivariant
space. TheformulationofthelinearlayeraboveissimilartotheoneusedintheResidualPathway
Priors(RPP)(Finzietal.,2021). NotethatinRPP,thevalueofθremainsconstantandactsasaprior
onthelevelofequivarianceweexpectfromagiventaskanddataset. Contrastedtothat,inthiswork
weaimtocontrolthevalueofθinordertoactivelychangethelevelofequivarianceduringtraining
andprojectbacktotheequivariancespaceduringinference.
Forthesecondobjective,weneedtoutilizeametricthatmeasurestherelativedistanceofthemodel
fromthespaceofequivariantmodelsH. ItwasobservedbyGruveretal.(2023)thataneasyway
tomeasurehowmuchamodelsatisfiestheequivariantconstraintsisbyusingthenormoftheLie
derivative. Wepresentdetailsinthenextsection.
43.1 LieDerivativeRegularizationTerm
AssumewearegivenamatrixLiegroupGactingonavectorspaceV throughitsrepresentation
ρ:G→GL(V). ForthegivengroupthereexistsacorrespondingLiealgebragwiththeproperty
that for A ∈ g, eA ∈ G. Additionally, there exists a corresponding Lie algebra representation
dρ:g→gl(V)suchthatρ(etA)=edρ(A)t.
IfwetakethederivativeoftheactionofagroupelementetA ∈Gatt=0wegettheLiederivative:
(cid:12) (cid:12)
d(cid:12)
(cid:12) ρ(etA)=
d(cid:12)
(cid:12) edρ(A)t =dρ(A) (3)
dt(cid:12) dt(cid:12)
t=0 t=0
Assumethatthefollowinggrouprepresentationactonthevectorspaceoffunctionsas:
ρ (g)[f]=ρ (g)−1◦(f ◦ρ (g)) (4)
in−out out in
AsobservedbyGruveretal.(2023)theliederivativeoftheaboveactioniszeroforallequivariant
functionsf,sinceforallg ∈Gtheactionρ (g)[f]=f istheidentitymap. Asaconsequence,
in−out
wecanusethenormoftheLiederivativeasametrictocomputehowmuchafunctionf deviates
fromtheequivariantconstraintofEquation1. ForthelinearrelaxationtermWxthatweintroduced
inEquation2,wehavethattheLiederivativecanbecomputedinastraightforwardmanneras:
(cid:12) (cid:12)
L A(W)= dd t(cid:12) (cid:12)
(cid:12)
ρ out(e−At)Wρ in(eAt)= dd t(cid:12) (cid:12)
(cid:12)
e−dρout(A)tWedρin(A)t
t=0 t=0
=−dρ (A)W +Wdρ (A)
out in
Asaresult,wecanmeasurethedegreethatalinearlayersatisfiestheequivariantconstraintatapoint
x,bycomputingthenormoftheLiederivativeatthatpointforeachoneofthegeneratorsofthe
group. ForexampleinthecasewhereGisthegroupof3Drotations(G=SO(3)),wecancompute
theLiederivativeforeachgenerator:
(cid:32)0 0 0 (cid:33) (cid:32) 0 0 1(cid:33) (cid:32)0 −1 0(cid:33)
J = 0 0 −1 ,J = 0 0 0 ,J = 1 0 0
x y z
0 1 0 −1 0 0 0 0 0
Duringtraining, givenaninputdistributionp(x), wecompute, foreachlinearlayerWx, theLie
derivativeregularizationterm:
 
(cid:88)
L ld(W)=E x∈p(x) ∥L A(W)x∥
A∈{Ji}
Althoughtheaboveregularizationapplieswhenthesymmetrygroupweareconsideringisamatrix
LieGroup,asweshowintheexperimentsofSection4.4,wecanalsodefineasimilarregularizerfor
thecaseofdiscretefinitegroups. Insuchacase,foragivenlinearlayerwithweightsW andinput
x,wecomputethesumofthenormsofthedifferenceL (W)x = (ρ(g )W −Wρ(g ))xforall
gj j j
generatorsg ofthediscretefinitegroupunderconsideration.
j
AsdiscussedinOttoetal.(2024)andshowninFigure3(a),theinclusionoftheaboveregularization
termsencouragesequivariantsolutionsandpreventsthemodelfromdivergingawayfromthespace
ofequivariantmodels. Moreover,Figure2showshowtheinclusionofthisregularizationhelpsthe
overalltrainingandresultsinaperformanceimprovementofthefinaltrainedmodel.
3.2 ReducingtheProjectionError
Whileweoptimizeoveralargerhypothesisspace,wealwaysaimtoreturntoanequivariantmodel
aftertheendoftraining. UsingtheparametrizationinEq. 2wecanalwaysdothatbysettingθtobe
equaltozero. Althoughaftertheprojectiontheresultingmodelisguaranteedtobeequivariant,it
mightbefarfromtheoriginalrelaxedversion,meaningitmighthavealargeprojectionerrorP .
pe
Specifically,foranindividualrelaxedlayertheprojectionerroris:
P =E
(cid:2) ∥f(x)−f¯(x)∥(cid:3)
=E [∥f (x)+θWx−f (x)∥]
pe x∼p(x) x∼p(x) e e
=E [∥θWx∥]
x∼p(x)
5Asaresult,toensurethatP remainslowweintroduceasecondregularizationtermonthenorm
pe
∥Wx∥. Additionally,toreducethecontributionofθontheprojectionerror,weproposetoschedule
itsvaluebyslowlydecreasingitduringthelastphaseoftraining. Specifically,weapplyacyclic
schedulingwheregivenN totalnumberofepochs,thevalueofθatepochiis:
E
(cid:40)
2i ifi<N /2
θ i = 2N −E 2i ifi≥NE /2
NE E
InFigure2weshowhowboththeadditionalregularizationtermonthenormoftheactivation∥Wx∥,
andtheschedulingofθ,affecttheperformanceofourframework.
3.3 TrainingObjective
Overall, given a task with a corresponding loss L and a data distribution D, our framework
task
optimizesoverthefollowingtrainingobjective:
(cid:34) N (cid:32) (cid:33)(cid:35)
(cid:88) (cid:88)
L=E L (f(x),y)+λ ∥W f (x)∥+ ∥L (W )f (x)∥ (5)
(x,y)∼D task reg i i−1 A i i−1
i=1 A∈Ji
whereW istheweightmatrixoftheithadditiveunconstrainedlinearlayerandf (x)istheoutput
i i−1
ofthe(i−1)thlayer(withf correspondingtotheinput).
0
Duringtraining,wecontroltheamplitudeoftheadditiverelaxationtermbyschedulingθasdescribed
inSection3.2. Duringinference,weevaluateonlyontheequivariantpartofthemodelbysetting
θ = 0. Thus as shown in Figure 1, after the end of training, the resulting model has the same
parametercountandmodelarchitectureasthebaselinemodelwithoutanyadditionaladditivelayers.
3.4 Relaxingtheconstraintsofdifferentequivariantarchitectures
Inthissection,weconsideraselectionofdifferentequivariantarchitectures,andusethemtoillustrate
howwecouldapplyourproposedoptimizationframework:
VectorNeurons(Dengetal.,2021)InVectorNeurons,theprimarylinearlayerprocessesfeaturesof
theformX ∈RN×3. Itachievesequivariancebyapplyingaleftmultiplicationwithaweightmatrix
f(X)=WX. Thisoperationmixesonlytherowsoftheinputfeaturematrixandasaresultwhen
theinputfeaturesrotatebyR,theoutputalsorotatessincef(XR)=WXR=f(X)R.
Toapplyourproposedrelaxationweaddalinearlayerthatallowsthemixingofalltheelements
of the input feature matrix. We can achieve this by unrolling the feature matrix into a vector of
dimension(nm)andthenafterapplyinganunconstraintedlinearlayer,rollitbacktoafeaturematrix.
Sotheoverallrelaxedlayerhastheform:
f(X)=W X+θuvec[Wvec(x)]
e
wherevec,uvecarethecorrespondingunrollingandrollingoperations.
SEGNN(Brandstetteretal.,2021)andEquiformer(Liao&Smidt,2023): Theintermediate
representationofbothSEGNNandEquiformeraresteerablevectorspacesthattransformaccording
toarepresentationofSO(3). Inparticular,bothmodelsprocessacollectionoftypeltensorsthat
transformaccordingtotheWigner-DmatricesD(l)(g)ofthecorrespondingtypel. Theinteraction
betweentensorsofdifferenttypescanbedoneusingtheClebsch-Gordan(CG)tensorproductwhich
isabilinearoperatorthatcombinestwoinputvectorsxl1,xl2 oftypesl 1andl 2andreturnsatensor
(xl1 ⊗xl2)l oftypelasfollows:
(xl1 ⊗xl2)l =
(cid:88)l1 (cid:88)l2
C(l,m) x(l1)x(l2)
m (l1,m1)(l2,m2) m1 m2
m1=−l1m2=−l2
where x( ml1 1), x( ml2 2) are the mt 1h, mt 2h elements of tensors x(l1), x(l2) and C (( ll 1,m ,m)
1)(l2,m2)
are the
correspondingCGcoefficients. Inthisoperation,theCGcoefficientsrestrictthepossibleinteraction
6between elements of different types of vectors. We relax the equivariant constraint by adding
anunconstrainedlinearlayerthatcanmixtheelementsfromallthetensorsusedasintermediate
representations, independent of their type. In Equiformer we add such a linear layer in the feed-
forwardnetworkofthetransformerblock. SimilarlyinSEGNN,weaddittothelayerthatreceives
theaggregatedmessagesfromalltheneighborsofanodeandupdatesthenodefeatures.
Approximately Equivariant Steerable Convolutions (Wang et al., 2022): In this work, the
authorsdesignedapproximateequivariantsteerableconvolutionallayers. Weapplyourmethodby
incorporating an additional unconstrained convolutional kernel. Since this task contains discrete
symmetrygroups,namelydiscreterotationsandscalings,wereplacetheLiederivativeregularizer
withthecorrespondingonefordiscretegroups,describedinSection3.1.
4 Experiments
4.1 EquivariantPointCloudClassification
Wefirstevaluateouroptimizationframeworkbytrainingdifferentnetworksonthetaskofpointcloud
classification. WeusetheequivariantvariantsofPointNet(Qietal.,2016)andDGCNN(Wangetal.,
2019)whichwereproposedbyDengetal.(2021). WetrainontheModelNet40dataset(Changetal.,
2015),whichcontains12311pointcloudsfrom40differentclasses. Wecomparewiththestandard
trainingofthesenetworksusingthesamehyperparameterconfigurationasemployedinDengetal.
(2021). Duringbothtrainingandtesting,wesub-sampletheinputpointcloudsto300points.
ToapplyourmethodwerelaxtheVectorNeuronslinearlayerbyfollowingthemethodologydescribed
in Section 3.4. For both networks we set the regularization term λ = 0.01, which is a value
reg
weuseinallofthefollowingexperiments. Weprovideamoredetaileddescriptionofthetraining
parametersinAppendixA.1. Furthermore,inAppendixA.2wedescribestheprocessofchoosing
thehyperparameterλ andshowthemethod’srobustnesstoitsvalue. Figure2showcaseshow
reg
applyingourproposedframeworkbenefitsthetrainingofbothnetworks. Specifically,forthecase
ofthesmallerandlessperformantPointNet,wecanseeanevenlargerperformanceincreaseover
the baseline. These results show how the performance benefits of our optimization framework
increase in smaller under-parametrized networks, an effect we investigate further in Section 4.2.
InAppendixA.3weprovideadditionaldetailsonthecomputationalandmemoryoverheadofour
proposedoptimization,showcasingthatwhileadditionalparametersareintroducedduringtraining
theoverheadinthetrainingtimeislimited.
Ablationsontheregularizationtermsandθscheduling: Inadditiontothetrainingcurvesofour
methodandofthebaseline,Figure2showstheaccuracyofourproposedoptimizationprocedure
whenweremovesomeoftheproposedregularizationtermsortheschedulingofθ. Weobservethat
withoutanyregularizationbothmodelsdivergefromthespaceofequivariantsolutions. Asaresult,
duringinferencewhenθ = 0theirprojectionerrorP becomeslarger, resultinginasignificant
pe
dropintestaccuracy. Similarly,withouttheLiederivativeregularizer,thefinaltestaccuracyofboth
networkvariantsdrops. Insuchcases,Wxisunconstrainedandcanlearntoextractnon-equivariant
featuresthattheequivariantpartf isnotabletolearninanystageofthetraining. Thiseffectcan
e
alsobeobservedinfigure3(a)showingthetotalLiederivativeofthenetworkwhenitistrainedwith
andwithouttheliederivativeregularizationterm. NotincludingtheLiederivativeregularization
allowsthenetwork, especiallyinthebeginningoftraining, tooptimizeoversolutionswithlarge
equivarianceerror. Finally,forbothnetworks,weobservethatθscheduling,asdescribedinSection
3.2,canbenefittrainingcomparedtofixingθtoaconstantlowvalue. InAppendixA.4weprovide
additional results showcasing how contrary to our method a model with a constant θ (without θ
scheduling)hasasignificantdropinperformanceafteritisprojectedintotheequivariantspace.
4.2 ScalingonDifferentModelandDatasetSizes
Tobetterunderstandhowthemodelanddatasetsizesaffectourproposedoptimizationframework,
wetrainmodelsofvariabledepthondifferentnumbersoftrainingsamples. Asabaselinemodelwe
usetheSteerableE(3)GNN(SEGNN)(Brandstetteretal.,2021)andwetrainitonthetaskofNbody
particlesimulation(Kipfetal.,2018). Thistaskconsistsofpredictingthepositionof5electrically
chargedparticlesafter1000timestepswhengivenasinputtheirinitialpositions, velocities, and
charges.
7Figure2: TestaccuracyonModelNet40classification,duringtrainingofequivariantPointNetand
DGCNNusingabaselinetrainingprocessanddifferentversionsofourmethod. Theaccuracyis
computedfortheequivariantmodels,i.e. forthemodelsaftertheyareprojectedintheequivariant
space.
(a) (b)
Figure3: (a)NormofthetotalLiederivativeoftherelaxedPointNetmodeltrainedwithandwithout
theLiederivativeregularizationterm. ForthecomputationoftheLiederivativeweusethemethod
proposedinGruveretal.(2023).(b)ValueoftheLiederivativeregularizationtermforeachindividual
layeroftherelaxedPointNetmodelwhilewetrainusingourframeworkandwithLiederivative
regularizationweightsettoλ =0.01
reg
Figure4(a)showsthemeanaveragetesterrorachievedbynetworksofdifferentsizes,bothwhen
trainedwithastandardoptimization,andwhentrainedwithourproposedframework. Wecanobserve
thatforallsizesourmethodachievesbettergeneralizations. Thegapbetweenourmethodandthe
baselinebecomesgreaterinthecasesofsmallernetworks,aphenomenonthatwealsoobservedinthe
pointcloudclassificationexperimentsinSection4.1. Thus,ourframework,byrelaxingtheconstraint
andintroducingadditionaldegreesoffreedom,canhelptheoveralloptimization,especiallyinmodels
withalimitednumberofparameters. Additionally,figure4(b)showsthatwhenwefixthemodel
sizeandincreasethedatasetsizeourmethodisabletoscalebetterthanthebaseline. Inbothcases,
wecanobservethatthetrainingofthebaselinehasamuchlargervarianceandishighlydependent
ontherandominitializationofthelayers. Onthecontrary,ourmethodresultsinamoreconsistent
trainingwithasmallervariancebetweentherandomtrials.
4.3 MolecularDynamicsSimulation
To evaluate our framework in a challenging task using a complex network architecture, we train
Equiformer(Liao&Smidt,2023)onthetaskofmoleculardynamicssimulationsforasetofmolecules
providedaspartoftheMD17dataset(Chmielaetal.,2017). Thegoalofthistaskistopredictthe
energyandforcesfromdifferentconfigurationsofapre-specifiedmolecule. FollowingLiao&Smidt
(2023),foreachmoleculeweuseonly950differentconfigurationsfortrainingwhichsignificantly
increasesthetaskdifficulty. Foralltrainingrunsweusethesamevalueofλ = 0.01asinthe
reg
previousexperimentsandfortherestofthehyperparameters,weusethesameconfigurationasthe
oneproposedinLiao&Smidt(2023). InTable1weshowthatthemeanabsoluteerrorofenergy
8(a) (b)
Figure4: MeanAverageErrorontheNbodyparticlesimulationfor(a)differentmodelsizes,(b):
differentdatasetsizes.
andforcepredictionachievedbyEquiformer,bothwhenitistrainedusingstandardtraining,and
whenitistrainedwithourproposedoptimizationframework. Withoutanyadditionalhyperparameter
tuning,ourframeworkisabletoprovideimprovementsontheperformanceofEquiformerevenfor
thischallengingdata-scarcetask.
Table 1: MAE of Equiformer trained with and without our optimization framework on a set of
moleculesfromtheMD17dataset. TheenergyisreportedinmeVandtheforceinmeV/Åunits
Aspirin Benzene Ethanol Salicylicacid
Methods Energy Forces Energy Forces Energy Forces Energy Forces
Equiformer 5.3 7.2 2.2 6.6 2.2 3.1 4.5 4.1
Equiformer+Ours 5.2 7.1 2.2 6.6 2.0 2.9 4.1 4.1
4.4 OptimizingApproximatelyEquivariantNetworks
Finally, we show how our framework can be beneficial, not only for the optimization of exactly
equivariantnetworks,butalsoforapproximateequivariantones. Weapplyourmethodontopofthe
approximateequivariantsteerableCNNsproposedinWangetal.(2022). Althoughthesemodelsare
notprojectedbacktotheequivariantspace,theyarestillregularizedtostaywithinsolutionswith
smallequivarianterror.Themaindifferencewithourframeworkisthatintheapproximateequivariant
setting,theequivariantrelaxationremainsthesamethroughouttraining. Onthecontrary,wepropose
toprogressivelyconstrainthemodelbymodulatingthevalueoftheunconstrainedtermbyslowly
decreasing the value of θ from Equation 2. As a result by applying our optimization framework
ontopofthestandardtrainingoftheapproximateequivariantkernels,wetesthowprogressively
introducingadditionalconstraintsthroughouttrainingcanhelptheperformanceofthenetwork.
Weevaluateourmethodonthetaskof2DsmokeflowpredictiondescribedinWangetal.(2022). The
inputsofthemodelaresequencesofsuccessive64×64cropsofasmokesimulationgeneratedby
PhiFlow(Holletal.,2020). Thedesiredoutputisapredictionofthevelocityfieldforthenexttime
step. Weevaluateintwodifferentsettings: the"Future"settingwhereweevaluateonthesamepartof
thesimulationbutwepredictfuturetimestepsnotincludedinthetraining,andthe"Domain"setting
whereweevaluateonthesametimestepsasintrainingbutindifferentspatiallocations. Thedata
arecollectedfromsimulationswithdifferentinflowpositionsandbuoyantforces. Fortherotational
symmetrycase,whilethedirectionoftheinflowandofthebuoyantforceissymmetricto90◦degrees
rotations(C symmetrygroup),thebuoyancyfactorchangesforthedifferentdirectionsmakingthe
4
tasknotsymmetric. Forthescalingsymmetry,thesimulationsaregeneratedwithdifferentspatial
andtemporalsteps,withthebuoyantfactorchangingacrossscales.
9Table2: RMSEerroronthesyntheticsmokeplumedatasetwithapproximaterotationalandscale
symmetries. In the "Future" evaluation we train and evaluate the models in the same simulation
location but we test for later time steps in the simulation from the ones used in training. In the
"Domain"evaluationwetrainandevaluatethemodelsonthesametimestepsbutondifferentspatial
locationsinthesimulation.
Model MLP Conv Equiv Rpp Lift RSteer RSteer+Ours
Future 1.38±0.06 1.21±0.01 1.05±0.06 0.96±0.10 0.82±0.08 0.80±0.00 0.79±0.01
Rotation
Domain 1.34±0.03 1.10±0.05 0.76±0.02 0.83±0.01 0.68±0.09 0.67±0.01 0.58±0.00
Future 2.40±0.02 0.83±0.01 0.75±0.03 0.81±0.09 0.85±0.01 0.70±0.01 0.62±0.02
Scale
Domain 1.81±0.18 0.95±0.02 0.87±0.02 0.86±0.05 0.77±0.02 0.73±0.01 0.67±0.01
InadditiontotheapproximateequivariantsteerableCNNs(RSteer),wecomparewithasimpleMLP,
withanon-equivariantconvolutionalnetwork(ConvNet),aswellaswithanequivariantconvolutional
network(Equiv)(Weiler&Cesa,2019)andwithtwoadditionalapproximateequivariantnetworks
RPP (Finzi et al., 2021) and LIFT (Wang et al., 2021) that are trained using a standard training
procedure.InTable4.4weseethatbyapplyingouroptimizationframeworktheresultingapproximate
equivariantmodeloutperformsallotherbaselinesinbothcasesofapproximaterotationalandscale
symmetry. These results indicate that starting from an unconstrained model and progressively
increasing the applied constraints can benefit optimization even in the case where at the end of
training we stay in the space of approximate equivariant models and do not project back to the
equivariantspace.
5 Conclusion
In this work, we focus on the optimization of equivariant NNs. We proposed a framework for
improvingtheoveralloptimizationofsuchnetworksbyrelaxingtheequivarianceconstraintand
optimizingoveralargerspaceofapproximatelyequivariantmodels. Weshowcasetheimportanceof
utilizingregularizationduringtrainingtoensurethattherelaxedmodelsstayclosetothespaceof
equivariantsolutions. Aftertraining,weprojectbacktotheequivariantspacearrivingatamodel
thatrespectsthedatasymmetries,whileretainingitshighperformanceonthetask. Weevaluateour
proposedframeworkanditsindividualcomponentsoveravarietyofdifferentequivariantnetwork
architecturesandtrainingtasks,andwereportthatitcanconsistentlyprovideperformancebenefits
overthestandardtrainingprocedure. Atheoreticalanalysisofourapproach,possiblywithappealto
empiricalprocesstheory(Pollard,1990)tocontroltheoptimizationerror,isleftforfuturework.
Acknowledgements
Wegratefullyacknowledgesupportbythefollowinggrants:NSFFRR2220868,NSFIIS-RI2212433,
AROMURIW911NF-20-1-0080,andONRN00014-22-1-2677.
References
Baek, M., Dimaio, F., Anishchenko, I. V., Dauparas, J., Ovchinnikov, S., Lee, G. R., Wang, J.,
Cong, Q., Kinch, L. N., Schaeffer, R. D., Millán, C., Park, H., Adams, C., Glassman, C. R.,
DeGiovanni,A.M.,Pereira,J.H.,Rodrigues,A.V.,vanDijk,A.A.,Ebrecht,A.C.,Opperman,
D.J.,Sagmeister,T.,Buhlheller,C.,Pavkov-Keller,T.,Rathinaswamy,M.K.,Dalwadi,U.,Yip,
C.K.,Burke,J.E.,Garcia,K.C.,Grishin,N.V.,Adams,P.D.,Read,R.J.,andBaker,D. Accurate
predictionofproteinstructuresandinteractionsusingathree-trackneuralnetwork. Science,373:
871–876,2021.
Basu,S.,Katdare,P.,Sattigeri,P.,Chenthamarakshan,V.,Campbell,K.D.,Das,P.,andVarshney,L.R.
Efficientequivarianttransferlearningfrompretrainedmodels. InNeuralInformationProcessing
Systems,2023a. URLhttps://api.semanticscholar.org/CorpusID:258740850.
Basu,S.,Sattigeri,P.,Ramamurthy,K.N.,Chenthamarakshan,V.,Varshney,K.R.,Varshney,L.R.,
andDas,P.Equi-tuning:Groupequivariantfine-tuningofpretrainedmodels.InWilliams,B.,Chen,
Y.,andNeville,J.(eds.),Thirty-SeventhAAAIConferenceonArtificialIntelligence,AAAI2023,
Thirty-FifthConferenceonInnovativeApplicationsofArtificialIntelligence,IAAI2023,Thirteenth
10SymposiumonEducationalAdvancesinArtificialIntelligence,EAAI2023,Washington,DC,USA,
February7-14,2023,pp.6788–6796.AAAIPress,2023b. doi:10.1609/AAAI.V37I6.25832. URL
https://doi.org/10.1609/aaai.v37i6.25832.
Bekkers, E. J. B-spline cnns on lie groups. In 8th International Conference on Learning Repre-
sentations,ICLR2020,AddisAbaba,Ethiopia,April26-30,2020.OpenReview.net,2020. URL
https://openreview.net/forum?id=H1gBhkBFDH.
Bogatskiy,A.,Anderson,B.,Offermann,J.,Roussi,M.,Miller,D.,andKondor,R. Lorentzgroup
equivariantneuralnetworkforparticlephysics. InInternationalConferenceonMachineLearning,
pp.992–1002.PMLR,2020.
Boyda,D.,Kanwar,G.,Racanière,S.,Rezende,D.J.,Albergo,M.S.,Cranmer,K.,Hackett,D.C.,
andShanahan,P.E. Samplingusingsu(n)gaugeequivariantflows. PhysicalReviewD,103(7):
074504,2021.
Brandstetter,J.,Hesselink,R.,vanderPol,E.,Bekkers,E.,andWelling,M. Geometricandphysical
quantitiesimprovee(3)equivariantmessagepassing. 2021.
Chang,A.X.,Funkhouser,T.,Guibas,L.,Hanrahan,P.,Huang,Q.,Li,Z.,Savarese,S.,Savva,M.,
Song,S.,Su,H.,Xiao,J.,Yi,L.,andYu,F. ShapeNet: AnInformation-Rich3DModelRepository.
Technical Report arXiv:1512.03012 [cs.GR], Stanford University — Princeton University —
ToyotaTechnologicalInstituteatChicago,2015.
Chatzipantazis, E., Pertigkiozoglou, S., Dobriban, E., and Daniilidis, K. $\mathrm{SE}(3)$-
equivariant attention networks for shape reconstruction in function space. In The Eleventh In-
ternationalConferenceonLearningRepresentations,2023. URLhttps://openreview.net/
forum?id=RDy3IbvjMqT.
Chmiela,S.,Tkatchenko,A.,Sauceda,H.E.,Poltavsky,I.,Schütt,K.T.,andMüller,K.-R. Machine
learningofaccurateenergy-conservingmolecularforcefields. ScienceAdvances,3(5):e1603015,
2017. doi: 10.1126/sciadv.1603015. URLhttps://www.science.org/doi/abs/10.1126/
sciadv.1603015.
Cobb,O.,Wallis,C.G.R.,Mavor-Parker,A.N.,Marignier,A.,Price,M.A.,d’Avezac,M.,and
McEwen, J. Efficient generalized spherical {cnn}s. In International Conference on Learning
Representations,2021. URLhttps://openreview.net/forum?id=rWZz3sJfCkm.
Cohen,T.andWelling,M. Groupequivariantconvolutionalnetworks. InICML,2016.
Cohen,T.S.,Geiger,M.,andWeiler,M. AgeneraltheoryofequivariantCNNsonhomogeneous
spaces. InWallach,H.,Larochelle,H.,Beygelzimer,A.,dAlché-Buc,F.,Fox,E.,andGarnett,R.
(eds.),AdvancesinNeuralInformationProcessingSystems,volume32.CurranAssociates,Inc.,
2019.
Deng,C.,Litany,O.,Duan,Y.,Poulenard,A.,Tagliasacchi,A.,andGuibas,L. Vectorneurons: a
generalframeworkforso(3)-equivariantnetworks. arXivpreprintarXiv:2104.12229,2021.
Elesedy,B.andZaidi,S.Provablystrictgeneralisationbenefitforequivariantmodels.InInternational
conferenceonmachinelearning,pp.2959–2969.PMLR,2021.
Esteves, C., Allen-Blanchette, C., Makadia, A., and Daniilidis, K. Learning so (3) equivariant
representationswithsphericalcnns. InProceedingsoftheEuropeanConferenceonComputer
Vision(ECCV),pp.52–68,2018.
Finzi,M.,Benton,G.,andWilson,A.G. Residualpathwaypriorsforsoftequivarianceconstraints.
InAdvancesinNeuralInformationProcessingSystems,volume34,2021.
Flinth,A.andOhlsson,F. Optimizationdynamicsofequivariantandaugmentedneuralnetworks,
2023.
Gong,X.,Li,H.,Zou,N.,Xu,R.,Duan,W.,andXu,Y. Generalframeworkfore(3)-equivariant
neuralnetworkrepresentationofdensityfunctionaltheoryhamiltonian. NatureCommunications,
14(1):2848,2023.
11Gordon,J.,Lopez-Paz,D.,Baroni,M.,andBouchacourt,D. Permutationequivariantmodelsfor
compositionalgeneralizationinlanguage. InICLR,2020.
Gruver,N.,Finzi,M.A.,Goldblum,M.,andWilson,A.G. Theliederivativeformeasuringlearned
equivariance. InTheEleventhInternationalConferenceonLearningRepresentations,2023. URL
https://openreview.net/forum?id=JL7Va5Vy15J.
Holl, P.M., Um, K., andThuerey, N. phiflow: Adifferentiablepdesolvingframeworkfordeep
learningviaphysicalsimulations. InWorkshoponDifferentiableVision,Graphics,andPhysicsin
MachineLearningatNeurIPS,2020.
Huang, N., Levie, R., and Villar, S. Approximately equivariant graph networks. ArXiv,
abs/2308.10436,2023.
Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool,
K.,Bates,R.,Žídek,A.,Potapenko,A.,etal. Highlyaccurateproteinstructurepredictionwith
alphafold. Nature,596(7873):583–589,2021.
Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In 3rd International
Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
ConferenceTrackProceedings,2015.
Kipf, T., Fetaya, E., Wang, K.-C., Welling, M., and Zemel, R. Neural relational inference for
interacting systems. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International
ConferenceonMachineLearning,volume80ofProceedingsofMachineLearningResearch,pp.
2688–2697.PMLR,10–15Jul2018. URLhttps://proceedings.mlr.press/v80/kipf18a.
html.
Kondor,R.andTrivedi,S. Onthegeneralizationofequivarianceandconvolutioninneuralnetworks
totheactionofcompactgroups. InICML,2018.
Kondor,R.,Lin,Z.,andTrivedi,S.Clebsch–gordannets:afullyfourierspacesphericalconvolutional
neuralnetwork. AdvancesinNeuralInformationProcessingSystems,31,2018.
Kufel,D.,Kemp,J.,andYao,N.Y.Approximately-invariantneuralnetworksforquantummany-body
physics. 2023. URLhttps://api.semanticscholar.org/CorpusID:268031561.
Liao,Y.-L.andSmidt,T.Equiformer:Equivariantgraphattentiontransformerfor3datomisticgraphs.
InInternationalConferenceonLearningRepresentations,2023. URLhttps://openreview.
net/forum?id=KwmPfARgOTD.
Loshchilov,I.andHutter,F. Decoupledweightdecayregularization. InInternationalConferenceon
LearningRepresentations,2019. URLhttps://openreview.net/forum?id=Bkg6RiCqY7.
Luo,S.,Chen,T.,andKrishnapriyan,A.S. Enablingefficientequivariantoperationsinthefourier
basisviagaunttensorproducts. arXivpreprintarXiv:2401.10216,2024.
Maile, K., Wilson, D. G., and Forré, P. Equivariance-aware architectural optimization of neural
networks,2023.
Maron,H.,Ben-Hamu,H.,Shamir,N.,andLipman,Y. Invariantandequivariantgraphnetworks.
ArXiv,abs/1812.09902,2019.
Mondal, A. K., Panigrahi, S. S., Kaba, S.-O., Rajeswar, S., and Ravanbakhsh, S. Equivariant
adaptation of large pretrained models. In Thirty-seventh Conference on Neural Information
ProcessingSystems,2023. URLhttps://openreview.net/forum?id=m6dRQJw280.
Ocampo,J.,Price,M.A.,andMcEwen,J. ScalableandequivariantsphericalCNNsbydiscrete-
continuous(DISCO)convolutions. InTheEleventhInternationalConferenceonLearningRepre-
sentations,2023. URLhttps://openreview.net/forum?id=eb_cpjZZ3GH.
Ordoñez-Apraez, D., Turrisi, G., Kostic, V., Martin, M., Agudo, A., Moreno-Noguer, F., Pon-
til, M., Semini, C., and Mastalli, C. Morphological symmetries in robotics. arXiv preprint
arXiv:2402.15552,2024.
12Otto,S.E.,Zolman,N.,Kutz,J.N.,andBrunton,S.L. Aunifiedframeworktoenforce,discover,and
promotesymmetryinmachinelearning,2024. URLhttps://arxiv.org/abs/2311.00212.
Passaro,S.andZitnick,C.L. Reducingso(3)convolutionstoso(2)forefficientequivariantgnns. In
InternationalConferenceonMachineLearning,pp.27420–27438.PMLR,2023.
Pearce-Crump, E. Brauer’s group equivariant neural networks. In International Conference on
MachineLearning,pp.27461–27482.PMLR,2023.
Petrache,M.andTrivedi,S. Approximation-generalizationtrade-offsunder(approximate)group
equivariance. InOh,A.,Naumann,T.,Globerson,A.,Saenko,K.,Hardt,M.,andLevine,S.(eds.),
AdvancesinNeuralInformationProcessingSystems,volume36,pp.61936–61959.CurranAs-
sociates,Inc.,2023. URLhttps://proceedings.neurips.cc/paper_files/paper/2023/
file/c35f8e2fc6d81f195009a1d2ae5f6ae9-Paper-Conference.pdf.
Petrache, M. and Trivedi, S. Position paper: Generalized grammar rules and structure-based
generalizationbeyondclassicalequivarianceforlexicaltasksandtransduction. arXivpreprint
arXiv:2402.01629,2024.
Pollard,D. Empiricalprocesses:Theoryandapplications. InNSF-CBMSRegionalConferenceSeries
inProbabilityandStatistics,pp.i–86.JSTOR,1990.
Qi,C.R.,Su,H.,Mo,K.,andGuibas,L.J. Pointnet: Deeplearningonpointsetsfor3dclassification
andsegmentation. 2016. URLhttp://arxiv.org/abs/1612.00593. citearxiv:1612.00593.
Ravanbakhsh, S., Schneider, J. G., and Póczos, B. Equivariance through parameter-sharing. In
Precup,D.andTeh,Y.W.(eds.),Proceedingsofthe34thInternationalConferenceonMachine
Learning,ICML2017,Sydney,NSW,Australia,6-11August2017,volume70ofProceedingsof
MachineLearningResearch,pp.2892–2901.PMLR,2017. URLhttp://proceedings.mlr.
press/v70/ravanbakhsh17a.html.
Romero, D. W. and Lohit, S. Learning partial equivariances from data. Advances in Neural
InformationProcessingSystems,35:36466–36478,2022.
Satorras,V.G.,Hoogeboom,E.,Fuchs,F.,Posner,I.,andWelling,M. E(n)equivariantnormalizing
flowsformoleculegenerationin3d. ArXiv,abs/2105.09016,2021.
Townshend,R.J.L.,Eismann,S.,Watkins,A.M.,Rangan,R.,Karelina,M.,Das,R.,andDror,R.O.
Geometricdeeplearningofrnastructure. Science,373:1047–1051,2021.
vanderOuderaa,T.F.A.,Romero,D.W.,andvanderWilk,M. Relaxingequivarianceconstraints
withnon-stationarycontinuousfilters. InAdvancesinNeuralInformationProcessingSystems
(NeurIPS),volume35, Dec2022. doi: 10.48550/ARXIV.2204.07178. URLhttps://arxiv.
org/abs/2204.07178.
Villar, S., Hogg, D. W., Storey-Fisher, K., Yao, W., and Blum-Smith, B. Scalars are universal:
Equivariantmachinelearning,structuredlikeclassicalphysics. AdvancesinNeuralInformation
ProcessingSystems,34:28848–28863,2021.
Wang,D.,Walters,R.,Zhu,X.,andPlatt,R. Equivariant$q$learninginspatialactionspaces. In
5thAnnualConferenceonRobotLearning,2021. URLhttps://openreview.net/forum?id=
IScz42A3iCI.
Wang,R.,Walters,R.,andYu,R. Approximatelyequivariantnetworksforimperfectlysymmetric
dynamics. InInternationalConferenceonMachineLearning.PMLR,2022.
Wang,R.,Walters,R.,andSmidt,T. Relaxedoctahedralgroupconvolutionforlearningsymmetry
breakingin3dphysicalsystems. InNeurIPS2023AIforScienceWorkshop,2023. URLhttps:
//openreview.net/forum?id=B8EpSHEp9j.
Wang,R.,Hofgard,E.,Gao,H.,Walters,R.,andSmidt,T.E. Discoveringsymmetrybreakingin
physicalsystemswithrelaxedgroupconvolution,2024. URLhttps://arxiv.org/abs/2310.
02299.
13Wang,Y.,Sun,Y.,Liu,Z.,Sarma,S.E.,Bronstein,M.M.,andSolomon,J.M. Dynamicgraphcnn
forlearningonpointclouds. ACMTransactionsonGraphics(TOG),2019.
Weiler, M. and Cesa, G. General E(2)-Equivariant Steerable CNNs. In Conference on Neural
InformationProcessingSystems(NeurIPS),2019.
Winkels,M.andCohen,T. Pulmonarynoduledetectioninctscanswithequivariantcnns. Medical
imageanalysis,55:15–26,2019.
Xu, Y., Lei, J., Dobriban, E., and Daniilidis, K. Unified fourier-based kernel and nonlinearity
designforequivariantnetworksonhomogeneousspaces. InChaudhuri,K.,Jegelka,S.,Song,L.,
Szepesvári,C.,Niu,G.,andSabato,S.(eds.),InternationalConferenceonMachineLearning,
ICML2022,17-23July2022,Baltimore,Maryland,USA,volume162ofProceedingsofMachine
LearningResearch,pp.24596–24614.PMLR,2022. URLhttps://proceedings.mlr.press/
v162/xu22e.html.
Zhu, X., Wang, D., Biza, O., Su, G., Walters, R., and Platt, R. Sample efficient grasp learning
usingequivariantmodels. CoRR,abs/2202.09468,2022. URLhttps://arxiv.org/abs/2202.
09468.
14A Appendix/SupplementalMaterial
A.1 TrainingDetails
Inthissection,weprovideadditionaldetailsfortheapplicationofourframeworkintheexperiments
presentedinthiswork. Wefixtheweightoftheregularizationtermtobeλ = 0.01forallthe
reg
experiments. Wearrivethattheabovevalueforthehyperparameterλ byperforminggrid-search
reg
using cross validation, as described in more detail in Section A.2. Additionally, except on the
correspondingablationstudy,weusetheschedulingofthevalueofθasdescribedinSection3.2. The
variancereportedisover5randomtrialsofthesameexperimentwithdifferentseeds. Werunallthe
experimentsonNVIDIAA40GPUs. Forthemodel-specifictrainingdetails:
• PointCloudClassification:WeuseasbaselinestheVN-PointNetandVN-DGCNNnetwork
architecturesdescribedintheworkofDengetal.(2021). FortherelaxedversionofVN-
PoitNetwetrainfor250epochsusingtheAdamoptimizer(Kingma&Ba,2015),withan
initiallearningrateof10−3,thatwedecreaseevery20epochsbyafactorof0.7,andweight
decayequalto10−4. FortherelaxedversionofVN-DGCNNwetrainfor250epochsusing
stochasticgradientdescent, withaninitiallearningrateof10−3, thatwedecreaseusing
cosineannealing,andweightdecayequalto10−4. Thebatchsizeusedwas32.
• Nbodyparticlesimulation: WetraintherelaxedversionofSEGNN(Brandstetteretal.,
2021)for1000epochsusingAdamoptimizer(Kingma&Ba,2015)withalearningrateof
5∗10−4,aweightdecayof10−12 andbatchsizeof100. WereportthetestMAEforthe
modelatthetrainingepochthatachievedtheminimumvalidationerror.
• MolecularDynamicsSimulation: WetraintherelaxedversionofEquiformer(Liao&
Smidt,2023)for1500epochsusingAdamWoptimizer(Loshchilov&Hutter,2019)with
aninitiallearningrateof10−5,thatwedecreaseusingcosineannealing,andwithweight
decay equal to 10−6. The batch size used was 8. We use the network variant with max
representationtypesettoL =2
max
• ApproximatelyEquivariantSteerableConvolution: Wetraintheapproximatelyequiv-
ariantsteerableconvolutionproposedinWangetal.(2022)afterweapplyouradditional
relaxation. Wemodifythesamearchitectureusedintheoriginalworkwhichcontains5
layersofapproximateequivariantsteerableconvolutions. Wetrainfor1000epochsusing
theAdamoptimizer(Kingma&Ba,2015). Weuseaninitiallearningrateof10−4,thatwe
decreaseateachepochby0.95. Weperformearlystopping,wherethestoppingcriterionis
thatthemeanvalidationscoreofthelast5epochsexceedsthemeanvalidationscoreofthe
previous5epochs.
A.2 ChoiceofHyperparameters
Inalltheexperiments,apartfromtheweightλ oftheproposedregularizationterm,weusethe
reg
samehyperparametersastheonesusedbythebaselinemethodswecomparewith. Forthechoiceof
λ weperformhyperparametergridsearchusingcross-validationwithan80%-20%splitofthe
reg
originaltrainingsetofModelNet40intotrainingandvalidation. Figure5showcasestheperformance
ofaVN-Pointnetmodeltrainedwithourmethodonthe80%trainingsplitandevaluatedonthe20%
validation split for different values of λ . We observed that the best value of λ is relatively
reg reg
robustacrosstasks,soweperformedanextensivehyperparametersearchforthetaskofpointcloud
classification,andweusedthefoundvalueλ =0.01acrossallothertasks.
reg
A.3 ComputationalandMemoryOverheadofproposedmethod
Thecomputationaloverheadintroducedbyourmethodonlyaffectsthetrainingprocess. During
inference,aftertheprojectiontotheequivariantspace,theretrievedmodelhasthesamearchitecture
as the corresponding base model to which we apply our method on, thus it also has the same
computationalandmemoryrequirements. InTable3weshowthecostofourmethod,intermsof
trainingtimeandnumberofadditionalparameters. Whileourproposedmethodintroducesadditional
parametersduringtraining,duetotheparallelnatureoftheunconstrainednon-equivariantterm,the
overheadintrainingtimecanbelimitedgivenenoughmemoryresources. Asaresult, whilethe
additionalparametersareapproximatelythreetimestheparametersofthebasemodeltheincreasein
thetrainingtimeisbelow10%ofthebasetrainingtime.
15Figure5: ModelNet40classificationaccuracyonthevalidationsetusingourproposedmethodwith
differentvaluesofλ . ThebasemodelusedwastheVN-PointNet. Themodelwastrainedona
reg
splitofthetrainingsetcontaining80%ofthetrainingdata. Theother20%ofthedatawereheldout
asthevalidationsetusedtoevaluatethemodel.
Table3: AdditionalNumberofparametersandComputationalOverheadintroducedbytheproposed
method
NumberofParameters AdditionalParameters TimeperEpoch TimeperEpoch
Model
(BaseModel) (Ours) (BaseModel) (Ours)
VN-PointNet 1.9M 6.4M 75s 80s
VN-DGCNN 1.8M 6.2M 148s 154s
Equiformer 3.4M 10M 52s 57s
A.4 AblationonθSchedulingandEquivariantprojection
Duringthelaterstagesoftrainingourproposedθschedulingslowlydecreasesthelevelofrelaxation
oftheequivariantconstraint,bringingthemodelclosertotheequivariantspace. Thisprocessallows
themodeltosmoothlytransitionfromtherelaxedequivariantcasetotheexactequivariantone. In
Figure6weshowacomparisonoftheperformanceofamodeltrainedwithourproposedθscheduling
andamodeltrainedwithaconstantθbeforeandafteritisprojectedtotheequivariantspace. While
theperformanceoftherelaxedequivariantmodelwithconstantθisclosetotheperformanceachieved
by our method, we can observe a sudden drop in performance once it is projected back to the
equivariantspace. Ontheotherhand,ourproposedschedulingofθallowsthemodeltoreturntothe
equivariantspacebytheendoftrainingwithoutshowcasingsuchasignificantperformancedrop.
Figure6: ComparisonoftheperformanceonModelNet40classification(300points),foramodel
trained with our method and a model trained with a constant value of θ, for which the level of
equivarianterroriscontrolledonlybytheregularizationterm. Forthelattermethodweshowresults
bothbeforeandaftertheprojectiontotheequivariantspace.
16A.5 Limitations
AswedescribeinSection3,ourworkfocusesontheassumptionthattheequivariantNNsatisfiesthe
equivariantconstraintbyimposingitinallofitsintermediatelayers. Althoughthisassumptionis
generalenoughtocoveralargerangeofstate-of-the-artequivariantarchitecture,itdoesn’tapplyto
allpossibleequivariantnetworkssinceitispossibletoensureoverallconstraintsatisfactionusinga
differentmethodology. AdditionallytheproposedregularizersinSection3.1canbeappliedtotasks
wherethesymmetrygroupisamatrixLiegrouporadiscretefinitegroup. Extendingourproposed
frameworktoarbitrarysymmetrygroupsisafutureresearchquestionthatisnotaddressedinthis
paper.
A.6 BroaderImpact
Thispaperfocusesonthequestionofimprovingtheoptimizationofequivariantneuralnetworks.
Suchequivariantnetworksarecurrentlyusedtosolvetasksindifferentfields–rangingfromcomputer
visiontocomputationalchemistry. Asaresult,itsbroadersocietalimpactishighlydependentonthe
specificnetworkitenablesoptimizing.
17