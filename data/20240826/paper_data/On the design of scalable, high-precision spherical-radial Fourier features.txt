On the design of scalable, high-precision
spherical-radial Fourier features
Ayoub Belhadji†∗, Qianyu Julie Zhu†, and Youssef Marzouk†
† Massachusetts Institute of Technology
August 26, 2024
Abstract
Approximation using Fourier features is a popular technique for scaling kernel methods to large-scale
problems, with myriad applications in machine learning and statistics. This method replaces the integral
representation of a shift-invariant kernel with a sum using a quadrature rule. The design of the latter is
meant to reduce the number of features required for high-precision approximation. Specifically, for the
squared exponential kernel, one must design a quadrature rule that approximates the Gaussian measure
on Rd. Previous efforts in this line of research have faced difficulties in higher dimensions. We introduce
a new family of quadrature rules that accurately approximate the Gaussian measure in higher dimensions
by exploiting its isotropy. These rules are constructed as a tensor product of a radial quadrature rule and
a spherical quadrature rule. Compared to previous work, our approach leverages a thorough analysis of
the approximation error, which suggests natural choices for both the radial and spherical components.
We demonstrate that this family of Fourier features yields improved approximation bounds.
1 Introduction
Fourier features provide a powerful technique to overcome the scalability issues inherent in traditional kernel
methods. The very first instance of this approach was proposed by Rahimi and Recht [17], which consists in
approximating the integral that appears in the Bochner representation of a shift-invariant positive definite
kernel κ:Rd×Rd →R:
(cid:90) 1 (cid:88)M
κ(x,y)= cos(⟨ω,x−y⟩)dΛ(ω)≈ cos(⟨ω ,x−y⟩), (1)
M j
Rd
j=1
whereΛisaprobabilitymeasuresupportedonRd andω ,...,ω arei.i.d.samplesfromΛ. Inparticular,
1 M
in the case of the squared exponential kernel, the distribution Λ corresponds to a Gaussian distribution
on Rd. However, several works have pointed out that the vanilla Monte Carlo approximation in (1) is
sub-optimal. For instance, quasi-Monte Carlo rules were shown to improve upon the Monte Carlo method in
low-dimensional domains [22, 2]. Alternatively, Gaussian quadrature was shown to significantly improve upon
Monte Carlo methods in [9, 18]. However, attempting to adapt this approach to high dimensions through
tensorization has yielded poor empirical results. In contrast, spherical-radial quadrature rules appear to offer
better empirical performance [16]. This construction makes use of the isotropy of the Gaussian distribution
and is based on the transformation
(cid:16) 2σ π2(cid:17)d/2(cid:90) φ(ω)e−σ2∥ 2ω∥2
dω
=(cid:16) 2σ π2(cid:17)d/2(cid:90) (cid:90) φ(rn)rd−1e−σ2 2r2
dπ Sd−1(n)dr (2)
Rd R
+
Sd−1
where ϕ(ω)=cos(⟨ω,x−y⟩). The r.h.s. of (2) is then approximated by a tensor product of two quadrature
rules: a radial quadrature rule and a spherical quadrature rule. While this approach yields strong empirical
∗Correspondingauthor: abelhadj@mit.edu
1
4202
guA
32
]LM.tats[
1v13231.8042:viXraresults, there has been little systematic study of how to design the two quadrature rules. Indeed, the analysis
given in [16] was restricted to radial quadrature rules of order 3 and the spherical quadrature rules were
restricted to a very specific family of constructions. Moreover, the importance of the bandwidth σ was
overlooked.
Inthiswork,weproposearefinedanalysisofspherical-radialFourierfeatureswhenusedinapproximating
the squared exponential kernel. Our approach makes use of a decomposition of the approximation error that
shows the error of each component (spherical or radial). Moreover, we use a change of variable ξ =σ2r2/2 in
(2) to obtain a slightly different transformation
√
(cid:16) 2σ π2(cid:17)d/2(cid:90) φ(ω)e−σ2∥ 2ω∥2
dω =
1 2(cid:16) 21 π(cid:17)d/2(cid:90) (cid:90) φ(cid:16) σ2ξ n(cid:17)
ξd/2−1e−ξdπ Sd−1(n)dξ. (3)
Rd R
+
Sd−1
Using this modification, we show that we can take advantage of the Gaussian quadrature associated to the
weight function
1
p (ξ)= ξd/2−1e−ξχ (ξ), (4)
Ξ Γ(d/2) [0,+∞[
for which the algorithmic construction is classical. Moreover, this approach allows us to study the importance
of the number of nodes in the radial quadrature rule by studying the coefficients of the function ξ (cid:55)→
√
(cid:82) (cid:0) (cid:1)
cos ⟨ 2ξn,x−y⟩/σ dπ (n)intheorthonormalbasisformedbythegeneralizedLaguerrepolynomials,
Sd−1 Sd−1
which is a well-studied family of orthogonal polynomials. Similarly, we study the coefficients of the functions
√
n(cid:55)→cos(cid:0) ⟨ 2ξn,x−y⟩/σ(cid:1) in the basis of the spherical harmonics defined on Sd−1. This refined analysis of
the coefficients enables us to highlight the difference in approximation error between two families of spherical
quadrature rules: one based on Monte Carlo (SR-MC) on Sd−1 and the second based on i.i.d. orthogonal
matrices (SR-OMC). Combining the two analyses allows us to derive upper bounds for the expected value of
the squared approximation error that show the importance of the number of both the radial quadrature rule
and the spherical quadrature rule, the bandwidth σ, and the dimension d. Contrary to [16], our analysis
gives insight on how these parameters influence the approximation error.
Besidesthesetheoreticalfindings,ournumericalsimulationsshowthatSR-OMCdeliversstrongempirical
results, especially when the number of features M is close to the dimension d. Moreover, the numerical
simulations show how some interpolative quadrature rules on Sd−1 could yield even better quadrature rules
that are worth studying in the future.
Notation Denote by κ the Gaussian kernel on Rd×Rd defined by
σ
κ
σ(x,y):=e−∥x 2− σy 2∥2
. (5)
Moreover, denote by Λ the Gaussian density corresponding to the Fourier transform of κ , given by
σ σ
Λ
σ(ω):=(cid:16) 2σ π2(cid:17)d/2 e−σ2∥ 2ω∥2
. (6)
We denote a quadrature rule Q defined on the space of continuous function on some domain X as a measure:
Q =
(cid:80)M
w δ , where the w are the weights and the n are the nodes. Given a quadrature rule of
i=1 i ni i i
non-negative weights, defined on Rd, define the approximate kernel κˆ :Rd×Rd →R associated to Q to be
M
(cid:88) (cid:0) (cid:1)
κˆ(x,y):= w cos ⟨n ,x−y⟩ . (7)
i i
i=1
This article is structured as follows. Section 2 is dedicated to the details of our constructions along with
the main theoretical results. Section 3 contains an overview of existing Fourier feature approximations built
using quadrature rules. Numerical simulations illustrating the theoretical results are gathered in Section 4.
22 Main results
In this section, we detail our theoretical contributions. In Section 2.1, we define the family of quadrature
rules that we adopt for constructing Fourier features and present the main theoretical results. In Section 2.2,
we justify our choices of the radial and spherical quadrature rules, along with their respective approximation
errors.
2.1 A family of spherical-radial quadrature rules
We start with the construction of the approximate kernel κˆ, for which a study of the approximation error
will be given later. Let QR =(cid:80)MRa δ be the Gaussian quadrature associated to (4). Given a spherical
i=1 i ξi
quadrature rule QS = (cid:80)MS b δ , where b ∈ R and θ ∈ Sd−1, we consider the approximate kernel κˆ
j=1 j θj j + j
associated to the quadrature rule
√
(cid:88)MR(cid:88)MS
a b δ ; r :=
2ξ
i. (8)
i j riθj i σ
i=1j=1
The calculation of the weights a and the nodes ξ can be performed classically through the eigende-
i i
composition of the tridiagonal Jacobi matrix associated with the orthogonal polynomials with respect to
p , which coincide with generalized Laguerre polynomials. This classical approach is briefly reviewed in
Ξ
Appendix A.1. One could argue for the use of the Gauss-Hermite quadrature rule in the initial spherical-
radial transformation (2). However, this approach becomes highly unstable in higher dimensions due to the
calculation of the products rd−1, where the r are the non-negative nodes of the Gauss-Hermite quadrature,
i i
which can take extremely large values as the dimension d increases. Otherwise, we could use the Gaussian
quadrature associated to the density ∝rd−1e−r2/2 in the initial transformation (2). However, this entails the
manipulation of polynomials which are orthogonal with respect to this density and for which few properties
are known.
2.2 On the design of the spherical-radial quadrature rule
As was shown in [16], there are multiple ways to design a radial-spherical quadrature rule. In this section, we
discuss a principled way to design such a quadrature rule, which justifies the choice adopted in Section 2.1.
An important ingredient in the analysis is the following definition.
Definition 1. Let x,y ∈Rd. Define f on Rd by
x−y
(cid:0) (cid:1)
f (ω):=cos ⟨ω,x−y⟩ , (9)
x−y
and its spherical average f¯ defined on R by
x−y +
√
(cid:90) (cid:16) 2ξ (cid:17)
f¯ (ξ):= f n dπ (n). (10)
x−y x−y σ Sd−1
Sd−1
The crux of our approach is the observation that the squared error
(cid:12) (cid:12)κ(x,y)−κˆ(x,y)(cid:12) (cid:12)2 =(cid:12) (cid:12) (cid:12)(cid:90) f x−y(ω)dΛ σ(ω)−(cid:88)MR(cid:88)MS a ib jf x−y(cid:0) r iθ j(cid:1)(cid:12) (cid:12) (cid:12)2
Rd
i=1j=1
is upper bounded by the sum of two terms
√
2(cid:16)(cid:12) (cid:12)(cid:90) f¯ (ξ)p (ξ)dξ−(cid:88)MR a f¯ (ξ )(cid:12) (cid:12)2 + (cid:88) a (cid:12) (cid:12)f¯ (ξ )−(cid:88)MS b f (cid:16) 2ξ iθ (cid:17)(cid:12) (cid:12)2(cid:17) .
(cid:12) x−y Ξ i x−y i (cid:12) i(cid:12) x−y i j x−y σ j (cid:12)
R
+ i=1 i∈[MR] j=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
radial error spherical error
In other words, to upper bound the approximation error |κ(x,y)−κˆ(x,y)|2, it is sufficient to establish
upper bounds for the approximation errors of both the radial and spherical quadrature rules. The design of
an efficient quadrature rules boils down to understanding the structure and the smoothness of the integrands.
We discuss this point in detail next.
32.2.1 The radial quadrature rule
The radial quadrature rule has to be tailored to the functions f¯ , whose expression is given in the following
x−y
result.
Proposition 1. For x,y ∈Rd, we have
+∞
f¯ (ξ)= (cid:88) β (d,x−y)ξn, (11)
x−y n
n=0
where
(−∥x−y∥2/(2σ2))nΓ(cid:0)d(cid:1)
β (d,x−y):= 2 . (12)
n Γ(n+1)Γ(d/2+n)
In other words, f¯ is an analytic function despite the fact that the expression (10) involves a
x−y
dependency on the square root of ξ. The Gaussian quadrature is an effective method for approximating
uni-dimensional integrals that involve ‘simple’ weight functions and smooth integrands. In our case, we have
to deal with the weight function (4), for which the generalized Gauss-Laguerre quadrature rule offers the
most natural quadrature rule. We show in the following, that the functions f¯ are well approximated in the
x−y
orthonormal basis of the generalized Laguerre polynomials (ℓα m) m∈N, where α:=d/2−1, which is orthogonal
with respect to the scalar product ⟨.,.⟩ . A definition of these polynomials is given in Appendix A.1.
pΞ
Proposition 2. Let x,y ∈Rd. We have
(cid:115)
(cid:12) (cid:12)⟨f¯ x−y,ℓα m⟩ pΞ(cid:12) (cid:12)≤ m!Γ(m1 +d/2)c2me−c2 ; c:= ∥x √− 2σy∥ . (13)
In particular, we have
∀M ∈N,
(cid:88)+∞
m|⟨f¯ ,ℓα⟩ |≤
c2 (cid:16) c2 (cid:17)2M−1
, (14)
x−y m pΞ (cid:112) Γ(d/2) 2M −1
m=2M+1
and ∃L>0 which is independent of the choice of M , x and y such that
R
(cid:12) (cid:12)(cid:90)
f¯ (ξ)p
(ξ)dr−(cid:88)MR
a f¯ (ξ
)(cid:12)
(cid:12)≤L
c2 (cid:16) c2 (cid:17)2MR−1
. (15)
(cid:12) R x−y Ξ i x−y i (cid:12) (cid:112) Γ(d/2) 2M R−1
+ i=1
The proof of Proposition 2 is given in Appendix B.2.
2.2.2 The spherical quadrature rule
For the design of the spherical quadrature rule, we adopt a strategy similar to that in Section 2.2.1, and we
studyhowwellthefunctionsf , seenasfunctionsdefinedonSd−1,areapproximatedintheorthonormal
ri(x−y)
basis of the spherical harmonics. The latter is the most natural basis for representing functions on Sd−1. We
start with the following result.
Proposition 3. Let x,y ∈Rd, and r ∈R . We have
+
+∞
(cid:88) x−y
f (n)= N(d,k)λ P (⟨v,n⟩), v := (16)
r(x−y) k k ∥x−y∥
k=0
where the P are the Gegenbauer polynomials defined in Appendix A.2, and the λ are defined as
k k
Γ(d/2) (cid:90) 1
λ := √ cos(r∥x−y∥t)P (t)(1−t2)(d−3)/2dt, (17)
k πΓ(d/2−1/2) k
−1
and satisfies
Γ((d−1)/2)
(cid:18) r∥x−y∥(cid:19)k
∀k ∈N, |λ |≤ . (18)
k Γ(k+(d−1)/2) 2
The proof of Proposition 3 is given in Appendix B.3. In the following, we study the choice of the
spherical quadrature rule.
4Figure 1: Monte Carlo on Sd−1 versus Orthogonal Monte Carlo on Sd−1.
Dimensions from left to right: 2, 4, 8, 16, 32.
Monte Carlo on Sd−1 Taking the spherical weights b =1/M and the spherical nodes θ ,...,θ to be
j S 1 MS
i.i.d. samples from the uniform distribution on Sd−1, we get the following result.
Proposition 4. Let x,y ∈Rd, and let r ∈R . The expected squared error under Monte Carlo sampling on
+
Sd−1 applied to the function f is bounded as follows
r(x−y)
(cid:12)(cid:90) 1 (cid:88)MS (cid:12)2 1 (cid:88)+∞
E(cid:12) f (n)dπ (n)− f (θ )(cid:12) = N(d,k)λ2. (19)
(cid:12) r(x−y) Sd−1 M r(x−y) j (cid:12) M k
Sd−1 S S
j=1 k=2
In particular, we have
E(cid:12) (cid:12) (cid:12)(cid:90) f r(x−y)(n)dπ Sd−1(n)− M1 (cid:88)MS f r(x−y)(θ j)(cid:12) (cid:12) (cid:12)2 ≤ 2M1 (cid:16)r2∥ dx −− 1y∥2(cid:17)2 er2∥ dx −− 1y∥2 . (20)
Sd−1 S S
j=1
The proof of Proposition 4 is given in Appendix B.4. The upper bound (20) converges to 0 at the
√
typical Monte Carlo rate, and the constant depends on the ratio r∥x−y∥/ d−1. Now, remember that in
(cid:112)
our construction, r ≤ 2ξ /σ, where ξ is the largest node of the Gaussian quadrature associated to the
MR MR
weight function defined by (4). By Theorem 6.31.2. in [21] we have
d−2
∀i∈[M ],ξ ≤4M +2 +2=4M +d, (21)
R i R 2 R
and we get the following result.
Theorem 2.1. Let κ be the squared exponential kernel of bandwidth σ, and let κˆ be the empirical kernel of the
spherical-radial Fourier feature map, as defined in Section 2.1, associated to the Monte Carlo approximation
on Sd−1. Then, we have
E(cid:12) (cid:12)κ(x,y)−κˆ(x,y)(cid:12) (cid:12)2 ≤2(cid:32) L
(cid:112)
c2 (cid:16) c2 (cid:17)2MR−1 + 8 (cid:18) 4M R+d(cid:19)2 c4e4(4M d−R 1+d)c2(cid:33) , (22)
Γ(d/2) 2M R−1 M S d−1
√
where c=∥x−y∥/( 2σ), and L is the same constant as in Proposition 2.
The upper bound in (22) underscores the importance of the parameters M , M and σ. Increasing the
R S
number of nodes in the radial quadrature, M , reduces the radial error. However, to prevent a significant
R
rise in spherical error, M should be kept as small as possible. Furthermore, the spherical part of the
R
approximation error depends on the ratio (4(4M +d)c2)/(d−1), which scales as c2 for a fixed value of M
R R
and large value of d. In the following, we study how we can improve the design of the spherical quadrature
rule.
The orthogonal Monte Carlo quadrature on Sd−1 In this section, we assume that M is a multiple
S
of d, and we take θ ,...,θ to be the columns of the matrix obtained by the concatenation of M /d
1 MS S
orthonormal i.i.d. random matrices B ,...,B from the Haar distribution of O (R), so that the columns
1 MS/d d
of each B follows the distribution of the uniform distribution on Sd−1. By taking uniform weights equal to
ℓ
1/M , we define a quadrature rule that we call the orthogonal Monte Carlo quadrature on Sd−1. For such a
S
construction, we have the following result.
5Proposition 5. Let x,y ∈Rd, and let r ∈R . The expected squared error under the orthogonal Monte Carlo
+
quadrature on Sd−1 applied to the function f is bounded as follows
r(x−y)
(cid:12)(cid:90) 1 (cid:88)MS (cid:12)2 3 (cid:88)+∞
E(cid:12) f (n)dπ (n)− f (θ )(cid:12) ≤ N(d,k)λ2. (23)
(cid:12) r(x−y) Sd−1 M r(x−y) j (cid:12) M k
Sd−1 S S
j=1 k=4
In particular, we have
E(cid:12) (cid:12) (cid:12)(cid:90) f r(x−y)(n)dπ Sd−1(n)− M1 (cid:88)MS f r(x−y)(θ j)(cid:12) (cid:12) (cid:12)2 ≤ 16M1 (r2∥ dx −− 1y∥2 )4er2∥ dx −− 1y∥2 . (24)
Sd−1 S S
j=1
The upper bound in Proposition 5 converges to 0 at the same rate as in (19), which is 1/M . However,
S
the involved constant is lower, which is crucial in our context where M =m d, and m is not very large.
S S S
Indeed, the ratio between the r.h.s. of (24) and the r.h.s of (20) is equal to
1(cid:16)r2∥x−y∥2(cid:17)2
, (25)
8 d−1
and we get an improvement over Theorem 2.1 (the Monte Carlo approximation) when ∥x−y∥2 ≤
2(d−1)r−2. Figure 1 illustrates this improvement for ∥x−y∥=1 and for d∈{2,4,8,16,32}. Proposition 5
leads to the following result.
Theorem 2.2. Let κ be the squared exponential kernel of bandwidth σ, and let κˆ be the empirical kernel of
the spherical-radial Fourier feature map, as defined in Section 2.1, associated to the orthogonal Monte Carlo
quadrature on Sd−1. Then, we have
(cid:32) (cid:33)
E(cid:12) (cid:12)κ(x,y)−κˆ(x,y)(cid:12) (cid:12)2 ≤2 L
(cid:112)
c2 (cid:16) c2 (cid:17)2MR−1 + 2 (cid:16)4M R+d(cid:17)4 c8e4(4M d−R 1+d)c2 . (26)
Γ(d/2) 2M R−1 M S d−1
Despite the fact that the approximation error of this method converges to zero at a rate similar to
the Monte Carlo rate, the numerical simulations presented in Section 4 demonstrate the superiority of this
method. Compared to Theorem 2.1, the error term depends on the ratio c raised to the power of 4, which
yields a significant improvement when c≤1. The proof of Proposition 5 is given in Appendix B.5.
The optimal kernel quadrature The previous constructions, which make use of uniform weights, can be
combined with a procedure that optimizes the weights in some sense. Given a configuration of nodes, one
could seek the weights that minimizes the worst-case approximation error (WCE) of the quadrature on the
unit ball of an RKHS H associated to a positive definite kernel κ :Sd−1×Sd−1 →R. This quantity is
S Sd−1
equal to
(cid:13) (cid:88)MS (cid:13)
(cid:13)µ− w κ (θ ,.)(cid:13) (27)
(cid:13) i Sd−1 i (cid:13)
i=1
HS
(cid:82)
where µ∈H is defined by µ(.):= κ (.,θ)dπ (θ). The vector wˆ of the weights w that minimizes
Sd−1 Sd−1 Sd−1 i
(27) is given by wˆ = K S(θ)−1µ(θ), where K S(θ) := (κ Sd−1(θ i,θ j))
i,j∈[MS]
∈ RMS×MS, and µ(θ) :=
(µ(θ i))
i∈[MS]
∈RMS. When κ
Sd−1
is rotation-invariant, µ is a constant function with an explicit formula. The
study of the convergence of the resulting quadrature rule, also called the optimal kernel-based quadrature, was
conducted for random configurations of points [3, 10, 5, 6, 4]. In particular, if the kernel κ is the squared
Sd−1
exponential kernel, the defined quadrature rule is guaranteed to converge at an exponential rate. However,
the design of this quadrature rule requires a sharp theoretical analysis that allows tuning the bandwidth of
the kernel κ , and the resulting weights are not guaranteed to be positive.
Sd−1
3 Related work
In this section we compare our constructions with existing work.
6Vanilla random Fourier features (RFF) The frequencies used in the initial form of Fourier features are
M i.i.d. samples from the distribution Λ . The study of the approximation error of this method gave birth to
σ
an abundant literature [19, 20]; see [14] for a survey. In particular, these works are concerned with the study
of the uniform error bound
sup |κ(x,y)−κˆ(x,y)|, (28)
∥x−y∥≤B
for a given B >0. The uniform error bound was proved when M =Ω(d/σ) with constants that depends on
log(B). In practice, when M gets closer to the dimension d, the approximation deteriorates strongly, as we
will show in Section 4.
QMC Fourier features Methods based on quasi-Monte Carlo rules make use of the fact that the density
Λ can be factorized Λ =(cid:81)d Λ , and take the i-th component of the frequency ω ∈Rd to be equal to
σ σ i=1 σ,i m
Φ−1(v m,i), where (v m,.) m∈N∗ is a low-discrepancy sequence, and Φ is the cumulative distribution function of
the Gaussian [22, 2, 13]. As shown in [13], when (v m,.) m∈N∗ corresponds to the Halton sequence, we get a
uniform bound of the form
(cid:16)log(M)d(cid:17)
sup |κ(x,y)−κˆ(x,y)|=O , (29)
M
∥x−y∥≤B
which is a typical rate of convergence for QMC rules: the improvement over the Monte Carlo only happens
when M is exponential on the dimension d. This is corroborated by numerical simulations that show that
there is no advantage in using QMC Fourier features in high dimensional settings.
Stochastic spherical rules and orthogonal Fourier features Another class of closely-related methods
are stochastic spherical rules (SSR) introduced by Munkhoeva et al. in [16], which are based on the spherical-
radial decomposition [11]. The corresponding quadrature rule writes, for a continuous function φ:Rd →R,
as follows
1
(cid:88)M˜ (cid:88)MR (cid:88)MS
a b
φ(r itQ itz is)+φ(−r itQ itz is)
, (30)
M˜ ir is 2
it=1ir=1is=1
where the z ∈Sd−1, and the r are i.i.d. samples from the distribution supported on the real line, and the
is it
Q are i.i.d. samples from the Haar distribution of O (R). This construction (30) was shown to extend
it d
orthogonal Fourier features (ORF), which is a popular sampling scheme that reduces the approximation error
by enforcing orthogonal structure on the features[23, 8]. There are multiple ways of choosing the weights a
ir
and b . The adopted strategy in [16] was to calculate the weights in such a way that the quadrature rule is
is
exact for low-degree polynomials, so that the quadrature (30) yield an unbiased estimator of the integral
(cid:82)
φ(ω)Λ(ω)dω [16]. While they showed this scheme performs better than state-of-the-art methods, no
Rd
comprehensive analysis has been done to derive the error bound when the number of radial nodes is larger
than 3. Moreover, the design of the z was restricted: the z were taken to belong to a regular d-simplex, and
i i
the sensitivity of the method with respect to this choice is not clear.
4 Numerical simulations
Inthissection,wepresentnumericalsimulationsthatcorroborateourtheoreticalfindings. First,inSection4.1,
we study the influence of the choice of the spherical quadrature rule along with the influence of the
order of the radial quadrature rule. Then, in Section 4.2, we compare our constructions with existing
methods for entry-wise and spectral kernel matrix approximation. Finally, in Section 4.3, we compare
our constructions with existing methods for two learning tasks. The code for this paper is available at
https://github.com/qianyu-zhu/SRFF.
4.1 The importance of the spherical and radial designs
In this section, we study the influence of the design of the spherical quadrature rule and the number of nodes
oftheradialquadraturerule. Forthispurpose,weconducttwoexperimentsonthedatasetPowerplant(d=4).
The first experiment compares vanilla RFF and stochastic spherical-radial quadratures (SSR) as defined in
7[16], to six spherical-radial (SR) quadrature rules as defined in Section 2.1: (1) SR-MC takes QS to be Monte
Carlo on Sd−1; (2) SR-OMC takes QS to be Orthogonal Monte Carlo on Sd−1 as defined in Section 2.2.2; (3)
SR-SOMC takes QS to be symmetrized Orthogonal Monte Carlo on Sd−1, where M is an even integer and
S
the θ are the columns of the matrix obtained by the concatenation of B ,−B ,...,B ,−B
i 1 1 MS/(2d) MS/(2d)
and the B are i.i.d. matrices from the Haar distribution of O (R); (4) SR-OKQ-MC is the optimal kernel
i d
quadrature (OKQ) associated with the nodes of SR-MC, where the kernel κ is taken to be the Gaussian
Sd−1
kernelonSd−1;(5)SR-OKQ-OMCistheoptimalkernelquadratureassociatedwiththenodesofSR-OMC;(6)
SR-OKQ-SOMCistheoptimalkernelquadratureassociatedwiththenodesofSR-SOMC.Forthisexperiment,
we randomly select 5000 samples from the dataset and report the relative error ∥K−Kˆ∥ /∥K∥ averaged
Fr Fr
over 20 runs. The kernel bandwidth is chosen to be σ =2.83.
Figure 2 shows the results for the different
quadrature methods as the number of features in-
creases. We observe that SR-MC has a similar per-
formance compared to RFF, while optimizing the
weights in SR-OKQ-MC gives slightly better results.
On the other hand, SR-OMC yields significantly bet-
ter results compared to RFF, and slightly better
thanSSR.Thesymmetrizationoftheorthogonalma-
trix in SR-SOMC yields slightly worse performance
thanSR-OMC.Finally,optimizingtheweightsinSR-
OKQ-OMC leads to better results when the number
of features is large, and combining the optimization
of weights and the symmetrization of the nodes in
SR-OKQ-SOMC yields the best performance com-
pared to every other method. The experiment shows
that enforcing orthogonality in the spherical quadra-
ture rule notably reduces the approximation error,
Figure 2: Relative error of different kernel
which is in line with our theoretical analysis in Sec-
approximation schemes for the dataset Powerplant.
tion 2.2.2. Moreover, it shows that optimizing the
Shaded regions indicate sample standard deviation of
weightsthroughOKQsignificantlyimprovestherates
the relative error, computed over 20 independent runs
of convergence and yields exponential convergence,
of each method. Radial nodes M are fixed and the
as predicted by existing theoretical results in the R
number of spherical nodes M changes.
literature. S
(a) σ=0.28 (b) σ=0.57 (c) σ=2.83
Figure 3: Relative error of kernel approximation schemes for increasing number of radial nodes M .
R
In the second experiment, we investigate the
impact of the number of nodes of the radial quadrature rule. Figure 3 illustrates the relative error when
SR-OMC is used to approximate the kernel matrix corresponding to 5000 random samples from Powerplant.
We conduct experiments with M ∈{1,2,3,5,7} using three different values of σ. For σ =0.28, the relative
R
error corresponding to larger M decreases consistently but with a worse initial constant, eventually reaching
R
a saturation point. This can be explained by the trade-off between spherical and radial error in (26). Initially,
8the spherical error dominates, but as the total number of features increases, the radial error, becomes more
significant, and the error curve plateaus for a fixed value of M . The plateau is higher and appears earlier for
R
smaller M . A similar behavior is observed when σ =2.83, but this time the optimal M is equal to 2 when
R R
M .M ≤103. For a fixed dataset and σ, the optimal value of M is achieved when the spherical and radial
S R R
errors are balanced. This experiment shows the importance of the choice of M and its dependency on the
R
kernel bandwidth σ. Further analysis on synthetic datasets and other real datasets is done in Appendix D.1.
4.2 Kernel approximation on real-world datasets
In this section, we compare our construction SR-OMC to existing methods: (1) random Fourier features
(RFF) [17]; (2) orthogonal random features (ORF) [23]; (3) QMC-based Fourier features (QMC) [2]; and
(4) stochastic spherical-radial quadratures (SSR) [16], on 4 datasets: Powerplant (d=4,σ =1.41), Letter
(d=16,σ =1.0), USPS (d=256,σ =11.31), and MNIST (d=784,σ =10.58). We study two quantities:
the relative Frobenius norm d (K,Kˆ) = ∥K −Kˆ∥ /∥K∥ , and a quantity evaluating spectral deviation
F F F
G(K,Kˆ)=∥K−1 2KˆK−1 2 −Id∥ 2.
Figure 4 illustrates the relative Frobenius error for various methods as a function of the number of
features. Across all scenarios, SR-OMC and SSR consistently achieve superior accuracy compared to other
state-of-the-art methods. In low-dimensional cases, ORF exhibits larger errors than SR-OMC and SSR,
followedbyQMCandRFF.Asthedimensionalityincreases,ORF’sperformanceconvergestothatofSR-OMC
and SSR, while QMC and RFF continue to exhibit significantly larger errors.
Regardingthespectraldeviationerror, Figure5demonstratesthatourmethodconsistentlyoutperforms
others across all datasets. As the dimensionality increases, OMC initially exhibits comparable approximation
power on the Powerplant dataset but incurs a larger error on the LETTER dataset. ORF and SSR do not
maintain stable spectral approximations on either dataset. In high-dimensional settings, all four methods
perform similarly on the USPS and MNIST datasets, with our method showing a slight advantage. Figure 5
implies that SR-OMC offers numerical guarantees for spectral approximation, and exploring the underlying
theory presents a promising direction for future research.
Although our method, SR-OMC, has a relative Frobenius error similar to that of SSR, it offers greater
flexibility and robustness in terms of radial quadrature rule. Compared to the radial design in [16] which
includesexactly3nodes(theoriginandtwosymmetricrandomnodes),ourmethodallowsforeasyadjustment
oftheradialdesignbasedonchangesinthekernelbandwidth, datasetdimension, andtotalnumberofFourier
features. Thisadaptabilityenhancesouralgorithm’srobustnessacrossdifferentproblemsettings. Particularly
for high-dimensional datasets, minimizing the number of Fourier features is crucial to reduce computational
complexity. Our method operates efficiently with minimal feature count of d, whereas SSR needs at least
2(d+1) features.
(a) Powerplant, d=4 (b) Letter, d=16 (c) USPS, d=256 (d) MNIST, d=784
M =2,σ=1.41 M =1,σ=1.0 M =1,σ=11.31 M =1,σ=10.58
R R R R
Figure 4: Kernel approximation error on 4 datasets. SSR has slightly different bins on the x-axis due to its
specific spherical-radial construction.
4.3 Prediction on real-world datasets
To evaluate our method in practical learning tasks, we compare the performance of different kernel-
approximation schemes on a regression task on Powerplant, and classification tasks on Letter and USPS
9(a) Powerplant, d=4 (b) Letter, d=16 (c) USPS, d=256 (d) MNIST, d=784
M =2,σ=1.41 M =1,σ=1.0 M =1,σ=11.31 M =1,σ=10.58
R R R R
Figure 5: Kernel approximation error on 4 datasets. SSR has slightly different bins on the x-axis due to its
specific spherical-radial construction.
respectively. For the regression task, we implement support vector regression (SVR) and report its R2 score
(R2 =1 indicates that the regression predictions perfectly fit the data); while for the classification task, we
train a support vector classifier (SVC) and test its prediction accuracy.
Figure 6 compares the result of different methods along with the standard deviation computed from 10
repeats. We observe that SR-OMC and SSR both achieve similar high-precision results. ORF demonstrates
comparable performance to SR-OMC and SSR on high-dimensional dataset USPS but shows inferior perfor-
mance on the others. On the contrary, QMC behaves the best on Powerplant but the performance drops
significantly when dimension gets bigger.
Combine the numerical analysis in Section 4.2 and Section 4.3, we summarize that the prediction
power of different methods does not fully align with kernel approximation accuracy in general. Our method,
SR-ORC, consistently performs the best in all dimensions for both kernel approximation and prediction. SSR
achieves comparable results to our method but admits notable spectral deviation in low-dimension kernel
approximation tasks and requires 2 times more features for similar performance. ORF performs well in
high-dimensional settings, while QMC behaves the opposite.
(a) R2 error for Powerplant (b) Accuracy for LETTER (c) Accuracy for USPS
d=4,M =2,σ=1.41 d=16,M =1,σ=1.0 d=256,M =1,σ=11.31
R R R
Figure 6: Performance of SVM on regression and prediction tasks using different kernel approximation
schemes.
5 Conclusion
In this work, we provide an exhaustive study of spherical-radial Fourier features for approximating the
squared exponential kernel when Gaussian quadrature is used in the radial part. Our analysis is based on a
decomposition of the approximation error into spherical and radial components, allowing us to quantify the
contribution of each term. In particular, our analysis highlights the interplay between the number of nodes in
both quadratures and the bandwidth of the kernel. We also show that the approximation error is sensitive
10to the the choice of the spherical quadrature rule; specifically, we demonstrate that enforcing orthogonality
in the nodes of the spherical quadrature rule yields better results compared to Monte Carlo on Sd−1, both
theoretically and empirically. Finally, our numerical simulations suggest that optimizing the weights of the
spherical quadrature rule further improves the quality of approximation. Proving uniform approximation
bounds on compact sets, and extending these results to other shift-invariant kernels, is left for future work.
6 acknowledgement
AB, QZ, and YMM are grateful for support from the US Department of Energy (DOE), Office of Science,
Office of Advanced Scientific Computing Research (ASCR), under award number DE-SC0023188.
11References
[1] K. Atkinson and W. Han. Spherical harmonics and approximations on the unit sphere: an introduction,
volume 2044. Springer Science & Business Media, 2012.
[2] H. Avron, V. Sindhwani, J. Yang, and M. Mahoney. Quasi-monte carlo feature maps for shift-invariant
kernels. Journal of Machine Learning Research, 17(120):1–38, 2016.
[3] F. Bach. On the equivalence between kernel quadrature rules and random feature expansions. The
Journal of Machine Learning Research, 18(1):714–751, 2017.
[4] A.Belhadji. Ananalysisofermakov-zolotukhinquadratureusingkernels. AdvancesinNeuralInformation
Processing Systems, 34:27278–27289, 2021.
[5] A. Belhadji, R. Bardenet, and P. Chainais. Kernel quadrature with DPPs. In Advances in Neural
Information Processing Systems 32, pages 12907–12917. 2019.
[6] A. Belhadji, R. Bardenet, and P. Chainais. Kernel interpolation with continuous volume sampling.
Proceedings of the 37th International Conference on Machine Learning, pages 725–735, 2020.
[7] M.R. Capobianco and G Criscuolo. Quadrature rules on unbounded interval.
[8] Krzysztof M Choromanski, Mark Rowland, and Adrian Weller. The unreasonable effectiveness of
structured random orthogonal embeddings. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing
Systems, volume 30. Curran Associates, Inc., 2017.
[9] T. Dao, C. De Sa, and C. Ré. Gaussian quadrature for kernel features. Advances in neural information
processing systems, 30, 2017.
[10] M. Ehler, M. Gräf, and Ch. J. Oates. Optimal monte carlo integration on closed manifolds. Statistics
and Computing, 29(6):1203–1214, 2019.
[11] A. Genz and J. Monahan. A stochastic algorithm for high-dimensional integrals over unbounded regions
with gaussian weight. Journal of computational and applied mathematics, 112(1-2):71–81, 1999.
[12] S. Goldstein, J. L. Lebowitz, R. Tumulka, and N. Zanghi. Any orthonormal basis in high dimension is
uniformly distributed over the sphere. 2017.
[13] Z. Huang, J. Sun, and Y. Huang. Quasi-monte carlo features for kernel approximation. 2024.
[14] F. Liu, X. Huang, Y. Chen, and Johan A.K. Suykens. Random features for kernel approximation:
A survey on algorithms, theory, and beyond. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 44(10):7128–7148, 2021.
[15] C. Müller. Spherical harmonics, volume 17. Springer, 2006.
[16] MarinaMunkhoeva, YermekKapushev, EvgenyBurnaev, andIvanOseledets. Quadrature-basedfeatures
for kernel approximation. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates,
Inc., 2018.
[17] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. Platt, D. Koller,
Y.Singer,andS.Roweis,editors,Advances in Neural Information Processing Systems,volume20.Curran
Associates, Inc., 2007.
[18] P. F. Shustin and H. Avron. Gauss-legendre features for gaussian process regression. Journal of Machine
Learning Research, 23(92):1–47, 2022.
[19] B. Sriperumbudur and Z. Szabó. Optimal rates for random fourier features. Advances in neural
information processing systems, 28, 2015.
12[20] Danica J. Sutherland and Jeff G. Schneider. On the error of random fourier features. ArXiv,
abs/1506.02785, 2015.
[21] G. Szego. Orthogonal polynomials, volume 23. American Mathematical Soc., 1939.
[22] J. Yang, V. Sindhwani, H. Avron, and M. Mahoney. Quasi-monte carlo feature maps for shift-invariant
kernels. In International Conference on Machine Learning, pages 485–493. PMLR, 2014.
[23] Felix Xinnan X Yu, Ananda Theertha Suresh, Krzysztof M Choromanski, Daniel N Holtmann-Rice,
and Sanjiv Kumar. Orthogonal random features. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and
R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates,
Inc., 2016.
13A Orthogonal families
A.1 Generalized Laguerre polynomials and the associated Gaussian quadrature
The family of generalized Laguerre polynomials orthogonal with respect to the weight function p (x) =
α
Γ(α+1)−1xαe−xχ (x) are given explicitly by the formula
[0,+∞[
n (cid:18) (cid:19)
(cid:88) 1 n+α
Lα(x)= (−x)i. (31)
n i! n−i
i=0
More precisely, we have
(cid:90) Γ(n+α+1)
Lα(x)Lα(x)p (x)dx= δ . (32)
n m α n! mn
R
+
Moreover, by Lemma C.4 we have the following identity holds for general α′
(cid:90)
xα′−1Lα(x)e−xχ
(x)dx=(cid:18) α−α′+n(cid:19)
Γ(α′). (33)
n [0,+∞[ n
R
+
After proper scaling, we can derive an orthonormal basis of L (p )
2 α
(cid:115)
n!
ℓα(x):= Lα(x), (34)
n Γ(n+α+1) n
such that
(cid:90)
ℓα(x)ℓα(x)p (x)dx=δ . (35)
n m α mn
R
+
These polynomials satisfy the following recurrence
(cid:115)
2n+1+α−x n(n+α)
ℓα (X)= ℓα(X)− ℓα (X). (36)
n+1 (cid:112) (n+1)(n+α+1) n (n+1)(n+α+1) n−1
In particular, the Gaussian quadrature associated to this family of orthogonal polynomials can be calculated
by the eigendecomposition of the Jacobi matrix
√
 1+α 1+α 0 ··· 0 0 
√
(cid:112)
 1+α 3+α 2×(2+α) ··· 0 0 
 (cid:112) 
 0 2×(2+α) 5+α ··· 0 0 
J = 

. .
.
. .
.
. .
.
... . .
.
. .
.
 

 (cid:112) 
 0 0 0 ··· 2n−1+α (n+1)(n+1+α)
(cid:112)
0 0 0 ··· (n+1)(n+1+α) 2n+1+α
Moreover, they satisfy the identity
(cid:115)
(cid:90)
xα′−1ℓα(x)e−xχ (x)dx=
n! (cid:18) α−α′+n(cid:19)
Γ(α′). (37)
n [0,+∞[ Γ(n+α+1) n
R
+
A.2 Spherical harmomics
We define Y :Sd−1 →R to be the constant function equal to 1. For any k ∈N∗, denote by {Y :Sd−1 →
0,1 k,i
R,i=1,...,N(d,k)} the family of spherical harmonics of exact degree k, where
(cid:18) (cid:19)
(2k+d−2) k+d−3
N(d,k):= , (38)
k d−2
14and we adopt the notation N(d,0):=1.
We refer to [15] for an explicit construction of this family of functions.
The family (Y ) forms an orthonormal basis in L (π ). In particular, we have for k,k′ ∈N, and for
k,i 2 Sd−1
(i,i′)∈[N(d,k)]×[N(d,k′)], we have
⟨Y ,Y ⟩ =δ δ . (39)
k,i k′,i′ π Sd−1 k,k′ i,i′
Moreover, they satisfy the addition formula
N(d,k)
(cid:88)
∀k ∈N, ∀x,y ∈Sd−1, Y (x)Y (y)=N(d,k)P (⟨x,y⟩), (40)
k,i k,i k
i=1
where P is k-th Gegenbauer polynomial of parameter α=d/2−1 defined by
k
(cid:16) 1(cid:17)k Γ(cid:0)d−1(cid:1) (cid:18) d(cid:19)k
P k(t)= −
2
Γ(cid:0)
k+2 d−1(cid:1)(1−t2)3− 2d
dt
(1−t2)k+d− 23 , (41)
2
see1 Theorem 2.9 in [1]. For instance, we have
P (t)=1,
0
P (t)=t,
1
P (t)=
1 (cid:0) dt2−1(cid:1)
. (42)
2 d−1
Moreover, by Section 2.7.3. in [1], the polynomials satisfy the recursion relation
2k+d−2 k
P (t)= tP (t)− P (t). (43)
k+1 k+d−2 k k+d−2 k−1
In particular, using (43) along with (42) we prove that for k ∈N, P is an even function while P is an
2k 2k+1
odd function. In addition, by inequality (2.39) in [1], we have
∀k ∈N,∀t∈[−1,1], |P (t)|≤1, (44)
k
and using the addition formula (40), and the orthogonality of the spherical harmonics (39), we prove that
∀v ∈Sd−1, (cid:90) P (cid:0) ⟨v,θ⟩(cid:1)2 dπ (θ)= 1 . (45)
k Sd−1 N(d,k)
Sd−1
Finally, for a function φ:R→R, we have the Hecke-Funk formula
(cid:90) V (cid:90) 1
∀v ∈Sd−1, φ(⟨v,n⟩)Y (n)dπ (n)= d−2Y (v) φ(t)P (t)(1−t2)(d−3)/2dt, (46)
k,i Sd−1 V k,i k
Sd−1 d−1 −1
see Theorem 2.22 in [1].
B Proofs
B.1 Proof of Proposition 1
When x−y =0, we have for ξ ∈R and n∈Sd−1
+
√ √
(cid:16) 2ξ (cid:17) (cid:16)(cid:10) 2ξ (cid:11)(cid:17)
f n =cos n,x−y =1, (47)
x−y σ σ
1In[1],theauthorsoptedfortheun-normalized uniformmeasureonSd−1,hencethedifferencebetween2.24in[1]and(40)
15so that f¯ (ξ)=β (d,x−y), by observing that
x−y 0
√ √
V
πΓ(cid:0)d−1(cid:1)
2π(d−1)/2 Γ(d/2)
πΓ(cid:0)d−1(cid:1)
β (d,x−y)= d−2 2 = 2 =1. (48)
0 V Γ(cid:0)d(cid:1) Γ((d−1)/2) 2πd/2 Γ(cid:0)d(cid:1)
d−1 2 2
Now, assume that x−y ̸=0, and define
1
v := (x−y)∈Sd−1. (49)
∥x−y∥
We have, for ξ ∈R and n∈Sd−1,
+
√ √ √
(cid:16) 2ξ (cid:17) (cid:16)(cid:10) 2ξ (cid:11)(cid:17) (cid:16) 2ξ (cid:17)
f
x−y σ
n =cos
σ
n,x−y =cos
σ
∥x−y∥⟨n,v⟩ =φ√ 2ξ∥x−y∥(⟨n,v⟩), (50)
σ
where, for a given γ >0, the function φ :R→R is defined as follows
γ
φ (t):=cos(γt). (51)
γ
Thus, by Hecke-Funk formula (46), we have
(cid:90)
f¯ x−y(ξ)= φ√ 2ξ∥x−y∥(⟨n,v⟩)dσ(n)
Sd−1 σ
V (cid:90) 1
= d−2 φ√ (t)(1−t2)(d−3)/2dt
V 2ξ∥x−y∥
d−1 −1 σ
√
V (cid:90) 1 (cid:16) 2ξ (cid:17)
= d−2 cos ∥x−y∥t (1−t2)(d−3)/2dt. (52)
V σ
d−1 −1
Now, observe that
√ √
(cid:16) 2ξ (cid:17) (cid:88)∞ (−1)n( 2ξ∥x−y∥t)2n (cid:88)+∞
cos ∥x−y∥t = = α (ξ)t2n, (53)
σ σ2n(2n)! n
n=0 n=0
√
(−1)n( 2ξ∥x−y∥)2n
α (ξ):= .
n (2n)!σ2n
To switch the summation and integral in Equation (52), we show that
√
(cid:90) 1 (cid:88)+∞ (cid:90) 1 (cid:88)+∞ ( 2ξ∥x−y∥)2n
|α (ξ)||t|2n(1−t2)(d−3)/2dt≤ (1−t2)(d−3)/2dt (54)
n (2n)!σ2n
−1n=0 −1n=0
√
(cid:90) 1 (cid:16) 2ξ (cid:17)
≤ cosh ∥x−y∥t (1−t2)(d−3)/2dt (55)
σ
−1
(cid:90) 1
≤cosh(1) (1−t2)(d−3)/2dt (56)
−1
Γ(1/2)Γ(d/2−1/2)
=cosh(1) <∞, (57)
Γ(d/2)
where the last equality is proved using the identity
1 (cid:90) 1 Γ(α+1/2)Γ(β+1)
∀α>− ,∀β >−1, t2α(1−t2)βdt=B(α+1/2,β+1)= , (58)
2 Γ(α+β+3/2)
−1
where B is the Beta function.
By Fubini-Tonelli Theorem, the integration in (52) can be decomposed into sequence of integration,
with each term computed as follows
16√
(cid:90) 1 (−1)n( 2ξ∥x−y∥)2n (cid:90) 1
α (ξ)t2n(1−t2)(d−3)/2dt= t2n(1−t2)(d−3)/2dt
n (2n)!σ2n
−1 −1
√
(−1)n( 2ξ∥x−y∥)2nΓ(n+1/2)Γ((d−1)/2)
= , (59)
(2n)!σ2n Γ(n+d/2)
since
(cid:90) 1 Γ(n+1/2)Γ((d−1)/2)
t2n(1−t2)(d−3)/2dt= , (60)
Γ(n+d/2)
−1
which again follows from the identity (58).
Now, observe that
nfactors
(cid:122) (cid:125)(cid:124) (cid:123) √
Γ(n+1/2) (2n−1)·(2n−3)····1·Γ(1/2) Γ(1/2) π
= = = . (61)
(2n)! 2n(2n)! 2n·2n·n! 22nn!
Thus, (59) yields
(cid:90) 1 (−1)n∥x−y∥2n2nΓ(n+1/2)Γ((d−1)/2)
α (ξ)t2n(1−t2)(d−3)/2dt= ξn
n σ2n (2n)! Γ(n+d/2)
−1
√
π(−∥x−y∥2/(2σ2))nΓ(cid:0)d−1(cid:1)
= 2 ξn. (62)
Γ(n+1)Γ(cid:0)d +n(cid:1)
2
Combining (52), (53), and (62), we get
+∞
f¯ (ξ)= (cid:88) β (d,x−y)ξn, (63)
x−y n
n=0
where √
V
π(−∥x−y∥2/(2σ2))nΓ(cid:0)d−1(cid:1)
β (d,x−y):= d−2 2 . (64)
n V Γ(n+1)Γ(d/2+n)
d−1
Now, by using the fact that Vd−2 = √ Γ(d/2) , we get
Vd−1 πΓ(d/2−1/2)
(−∥x−y∥2/(2σ2))nΓ(cid:0)d(cid:1)
β (d,x−y)= 2 . (65)
n Γ(n+1)Γ(d/2+n)
Moreover, the infinite series in (64) is absolutely convergence because the ratio
√ √
(cid:12)β (d,x−y)ξn+1(cid:12) π∥x−y∥2ξ/(2σ2) π∥x−y∥2ξ/(2σ2)
(cid:12) n+1 (cid:12)= ≤ (66)
(cid:12) β (d,x−y)ξn (cid:12) (n+1)(d/2+n) n2
n
converges to 0 at the rate n−2. Therefore, the series converge.
B.2 Proof of Proposition 2
In the following we look for an expression of the coefficient γ (d,x−y) defined as
m
(cid:90)
γ (d,x−y):=⟨f¯ ,ℓα⟩ = f¯ (ξ)ℓα(ξ)p (ξ)dξ, α=d/2−1. (67)
m x−y m pΞ x−y m Ξ
R
+
By definition Proposition 1, we have
+∞
∀ξ ∈R , f¯ (ξ)= (cid:88) β (d,x−y)ξn. (68)
+ x−y n
n=0
17In order to switch the order of integral and summation in Equation (67), we claim that
(cid:88)+∞(cid:90) (cid:12) (cid:12) (cid:88)+∞
(cid:12)β (d,x−y)ξnℓα(ξ)p (ξ)(cid:12)dξ =: ηn <∞. (69)
(cid:12) n m Ξ (cid:12)
R
n=0 + n=0
To prove the claim, we use Cauchy Schwarz inequality
(cid:32) (cid:33)1/2(cid:32) (cid:33)1/2
(cid:90) (cid:12) (cid:12) (cid:90) (cid:90)
(cid:12)ξnℓα(ξ)p (ξ)(cid:12)dξ ≤ ξ2np (ξ)dξ [ℓα(ξ)]2p (ξ)dξ (70)
(cid:12) m Ξ (cid:12) Ξ m Ξ
R R R
+ + +
=[(α+2n)!]1/2 (71)
Plug in β , we obtain a sequence ζn that bounds ηn from above
n
(cid:90) (cid:12) (cid:12)
η ≤|β (d,x−y)| (cid:12)ξnℓα(ξ)p (ξ)(cid:12)dξ (72)
n n (cid:12) m Ξ (cid:12)
R
√ +
V
π(∥x−y∥2/(2σ2))nΓ(cid:0)d−1(cid:1)
= d−2 2 [(α+2n)!]1/2 =:ζ . (73)
V Γ(n+1)Γ(d/2+n) n
d−1
We can always find N large enough such that the ratio between consecutive ζ satisfies
n
(cid:12) (cid:12) (∥x−y∥2/(2σ2))Γ(n+1)Γ(d/2+n)[(α+2n+2)!]1/2
(cid:12)ζ /ζ (cid:12)= (74)
(cid:12) n+1 n(cid:12) Γ(n+2)Γ(d/2+n+1) [(α+2n)!]1/2
∥x−y∥2/(2σ2)
= (α+2n+2)1/2(α+2n+1)1/2 (75)
(n+1)(d/2+n)
∥x−y∥2/(2σ2)
≤ (α+2n+2)≤1/2 (76)
n2
for any n≥N. The ratio test shows that the sequence {ζ } is summable. {η } is positive and dominated
n n n n
by {ζ } hence is also summable, which proves the claim.
n n
With the claim, we employ Fubini-Tonelli Theorem to move the summation out
+∞ (cid:90) +∞ (cid:90)
(cid:88) (cid:88)
γ (d,x−y)= β (d,x−y) ξnℓα(ξ)p (ξ)dξ = β (d,x−y) ξnℓα(ξ)p (ξ)dξ, (77)
m n m Ξ n m Ξ
R R
n=0 + n=m +
where the last equality follows from the fact that ℓα is orthogonal to all polynomials with degree less than m
m
with respect to the inner product ⟨.,.⟩ .
Now, by Lemma C.4 the coefficiep nΞ t (cid:82) ξnℓα(ξ)p (ξ)dξ writes
R m Ξ
+
(cid:90) (cid:90)
ξnℓα(ξ)p (ξ)dξ =Γ(α+1)−1 ξnℓα(ξ)ξαe−ξdξ
m Ξ m
R R
+ +
(cid:115)
(cid:18) (cid:19)
m−n−1 Γ(m+1) Γ(α+1+n)
= (78)
m Γ(m+α+1) Γ(α+1)
where
(cid:18) (cid:19)
m−n−1 (m−n−1)(m−n−2)...(−n)
:= (79)
m m!
is equal to
(−1)m(cid:0)n(cid:1)
when n≥m.
m
By definition of β (d,x−y) in (12), we have
n
(−1)nΓ(d/2) ∥x−y∥
β (d,x−y)= c2n, c:= √ . (80)
n Γ(d/2+n)Γ(n+1) 2σ
18Combining (78), (79), and (80), we get
(cid:90)
β (d,x−y) ξnℓα(ξ)p (ξ)dξ (81)
n m Ξ
R
+
(cid:115)
(−1)nc2nΓ(d/2) (cid:18) n(cid:19) Γ(m+1) Γ(d/2+n)
= (−1)m
Γ(d/2+n)Γ(n+1) m Γ(m+d/2) Γ(d/2)
(cid:115)
(−1)n+mc2n 1
= . (82)
(n−m)! m!Γ(m+d/2)
Thus, by summing over n, we get
(cid:12) ∞ (cid:90) (cid:12)
(cid:12) (cid:12)γ m(d,x−y)(cid:12) (cid:12)=(cid:12) (cid:12)
(cid:12)
(cid:88) β n(d,x−y)
R
ξnℓα m(ξ)p Ξ(ξ)dξ(cid:12) (cid:12)
(cid:12)
(83)
n=m +
(cid:12)
(cid:12)
(cid:88)∞ (−1)n+mc2n(cid:115)
1
(cid:12)
(cid:12)
=(cid:12) (cid:12) (84)
(cid:12) (n−m)! m!Γ(m+d/2)(cid:12)
n=m
(cid:115)
1
(cid:12)
(cid:12)
(cid:88)∞ (−1)nc2n(cid:12)
(cid:12)
= (cid:12) (cid:12) (85)
m!Γ(m+d/2)(cid:12) (n−m)!(cid:12)
n=m
=(cid:115)
1
c2m(cid:12)
(cid:12)
(cid:12)(cid:88)∞ (−c2)n(cid:12)
(cid:12)
(cid:12) (86)
m!Γ(m+d/2) (cid:12) n! (cid:12)
n=0
(cid:115)
1
=
c2me−c2
. (87)
m!Γ(m+d/2)
Thus, for M ∈N, we have
(cid:88)+∞ √ (cid:88)+∞
m|γ (d,x−y)|≤ m|γ (d,x−y)| (88)
m m
m=2M m=2M
(cid:115)
+∞
=
(cid:88)
m
1 c2me−c2
m!Γ(m+d/2)
m=2M
≤
c2 (cid:88)+∞ 1
c2(m−1)e−c2
, (89)
(cid:112)
Γ(d/2) (m−1)!
m=2M
where we have use the fact that
Γ(m+d/2)=(d/2+m−1)···(d/2+1)·(d/2)Γ(d/2)≥m!Γ(d/2). (90)
(cid:124) (cid:123)(cid:122) (cid:125)
mfactor
Now, by Lemma C.1, we have
(cid:88)+∞ 1 c2me−c2
≤
1 (cid:0) c2(cid:1)2M−1 ec2 e−c2 ≤(cid:16) c2 (cid:17)2M−1
. (91)
m! (2M −1)! 2M −1
m=2M−1
Now to prove (15), we use Lemma C.2 which states that there exists a constant L that is independent
of M such that
R
(cid:12)(cid:90) (cid:88)MR (cid:12) (cid:88)+∞ √
∀φ∈L (p ), (cid:12) φ(ξ)p (ξ)dξ− a φ(ξ )(cid:12)≤L m|⟨φ,ℓα⟩ |. (92)
2 Ξ (cid:12) Ξ i i (cid:12) m pΞ
R
+ i=1 m=2MR
19B.3 Proof of Proposition 3
Thefunctionf iscontinuousonSd−1. Thus,byTheorem2.30in[1],f decomposesinthespherical
r(x−y) r(x−y)
harmonics basis as follows
+∞N(d,k)
(cid:88) (cid:88)
f (n)= ⟨f ,Y ⟩ Y (n), (93)
r(x−y) r(x−y) k,i π Sd−1 k,i
k=0 i=1
wheretheconvergenceholdsuniformlyonSd−1. Inthefollowing,wecalculatethecoefficients⟨f ,Y ⟩ .
r(x−y) k,i π Sd−1
When r(x−y)=0, we have
∀n∈Sd−1, f (n)=cos(⟨r(x−y),n⟩)=1=Y (n), (94)
r(x−y) 0,1
so that
∀k ∈N, ∀i∈[N(d,k)], ⟨f ,Y ⟩ =δ . (95)
r(x−y) k,i π Sd−1 k,0
Now, assume that r(x−y) ̸= 0. Define v := (x−y)/∥x−y∥, and β := r(x−y). Let k ∈ N and
i∈[N(d,k)]. We have
(cid:90) (cid:90)
⟨f ,Y ⟩ = f (n)Y (n)dπ (n)= φ (⟨v,n⟩)Y (n)dπ (n),
r(x−y) k,i π Sd−1 r(x−y) k,i Sd−1 β k,i Sd−1
Sd−1 Sd−1
where the function φ :R→R is defined by φ (t)=cos(βt). Using the Hecke-Funk formula (46), we get
β β
(cid:90)
⟨f ,Y ⟩ = φ (⟨v,n⟩)Y (n)dπ (n) (96)
r(x−y) k,i π Sd−1 β k,i Sd−1
Sd−1
V (cid:90) 1
= d−2Y (v) φ (t)P (t)(1−t2)(d−3)/2dt. (97)
V k,i β k
d−1 −1
Therefore, using the addition formula (40), we have for n∈Sd−1
N (cid:88)(d,k)
⟨f ,Y ⟩ Y (n)=
V
d−2
(cid:90) 1
φ (t)P
(t)(1−t2)(d−3)/2dtN (cid:88)(d,k)
Y (v)Y (n) (98)
r(x−y) k,i π Sd−1 k,i V β k k,i k,i
i=1 d−1 −1 i=1
=N(d,k)P (⟨v,n⟩)λ , (99)
k k
where
V (cid:90) 1 V (cid:90) 1
λ := d−2 cos(βt)P (t)(1−t2)(d−3)/2dt= d−2 cos(r∥x−y∥t)P (t)(1−t2)(d−3)/2dt. (100)
k V k V k
d−1 −1 d−1 −1
Using the identity Vd−2 = √ Γ(d/2) concludes the proof of (16).
Vd−1 πΓ(d/2−1/2)
Now, we move to the proof of the upper bound (18). First, observe that when k is an odd integer P is
k
an odd function, and we get λ =0, and the bound (18) holds. In the following, assume that k is an even
k
integer. By Lemma 11 in [15], we have for any k times differentiable function φ:R→R
(cid:90) 1 (cid:16)1(cid:17)k Γ((d−1)/2) (cid:90) 1
φ(t)P k(t)(1−t2)(d−3)/2dt=
2 Γ(k+(d−1)/2)
φ(k)(t)(1−t2)k+d− 23 dt. (101)
−1 −1
By taking φ(t)=φ (t)=cos(βt), we have φ(k)(t)=(−1)k/2βkcos(βt), and we get
β
Γ(d/2) (cid:16)β(cid:17)k Γ((d−1)/2) (cid:90) 1
λ
k
=(−1)k/2√
πΓ(d/2−1/2) 2 Γ(k+(d−1)/2)
cos(βt)(1−t2)k+d− 23 dt. (102)
−1
Now, observe that
(cid:12) (cid:12) (cid:12)(cid:90) 1 cos(βt)(1−t2)k+d− 23 dt(cid:12) (cid:12) (cid:12)≤(cid:90) 1 (cid:12) (cid:12)cos(βt)(cid:12) (cid:12)(1−t2)k+d− 23 dt≤(cid:90) 1 (1−t2)k+d− 23 dt. (103)
−1 −1 −1
20Moreover, we have
(cid:90) 1 Γ(1/2)Γ(k+ d−1)
(1−t2)k+d− 23 dt=B(1/2,k+(d−1)/2)= 2 , (104)
Γ(k+ d)
−1 2
where B is the Beta function. Since k is an even integer, we have
Γ(k+ d−1) (cid:0) k+(d−1)/2−1(cid:1) ...(cid:0) (d−1)/2+1(cid:1)(cid:0) (d−1)/2(cid:1) Γ((d−1)/2) Γ((d−1)/2)
2 = ≤ . (105)
Γ(k+ d) (cid:0) k+d/2−1(cid:1) ...(cid:0) d/2−1(cid:1)(cid:0) d/2(cid:1) Γ(d/2) Γ(d/2)
2
Thus √
(cid:90) 1 Γ((d−1)/2) πΓ((d−1)/2)
(1−t2)k+d 2−3
dt≤Γ(1/2) = . (106)
Γ(d/2) Γ(d/2)
−1
Combining (102), (103), (104) and (106), we get
(cid:12) (cid:12) (cid:16)β(cid:17)k Γ((d−1)/2)
(cid:12)λ k(cid:12)≤
2
Γ(k+(d−1)/2), (107)
which concludes the proof of (18).
B.4 Proof of Proposition 4
Define the approximation error
(cid:12)(cid:90) 1 (cid:88)MS (cid:12)
E (f ):=(cid:12) f (n)dπ (n)− f (θ )(cid:12). (108)
S r(x−y) (cid:12) r(x−y) Sd−1 M r(x−y) j (cid:12)
Sd−1 S
j=1
Let r ∈R∗ and x,y ∈Rd. When r(x−y)=0, f =Y ≡1 and
+ r(x−y) 0,1
(cid:12) 1 (cid:88)+∞
EE (f )2(cid:12) =0= N(d,k)λ2, (109)
S r(x−y) (cid:12) r(x−y)=0 M S k
k=2
since λ =0 for k ∈N∗.
k
Now, assume that r(x−y)̸=0. By Proposition 3, we have
+∞
(cid:88) x−y
∀n∈Sd−1, f (n)= N(d,k)λ P (⟨v,n⟩), v := . (110)
r(x−y) k k ∥x−y∥
k=0
For k =0, we have by Hecke-Funk formula (46)
V (cid:90) 1 (cid:90)
λ = d−2 cos(r∥x−y∥t)P (t)(1−t2)(d−3)/2dt= f (n)dπ (n), (111)
0 V 0 r(x−y) Sd−1
d−1 −1 Sd−1
since Y (v)=1 for v ∈Sd−1. Moreover, since P is an odd function, we have
0,1 1
V (cid:90) 1
λ = d−2 cos(r∥x−y∥t)P (t)(1−t2)(d−3)/2dt=0. (112)
1 V 1
d−1 −1
Thus
E (f )= 1 (cid:12) (cid:12)(cid:88)+∞ N(d,k)λ (cid:88)MS P (cid:0) ⟨v,θ ⟩(cid:1)(cid:12) (cid:12). (113)
S r(x−y) M (cid:12) k k j (cid:12)
S
k=2 j=1
Therefore
E (f )2 = 1
(cid:88)+∞ (cid:88)+∞
N(d,k )N(d,k )λ λ
(cid:88)MS (cid:88)MS
P (cid:0) ⟨v,θ ⟩(cid:1) P (cid:0) ⟨v,θ ⟩(cid:1) . (114)
S r(x−y) M2 1 2 k1 k2 k1 j1 k2 j2
S k1=2k2=2 j1=1j2=1
21Now, let k ,k ∈N\{0,1}. When k ̸=k , either j ̸=j so that θ and θ are independent, and we get
1 2 2 1 1 2 j1 j2
EP (cid:0) ⟨v,θ ⟩(cid:1) P (cid:0) ⟨v,θ ⟩(cid:1) =E P (cid:0) ⟨v,θ ⟩(cid:1)E P (cid:0) ⟨v,θ ⟩(cid:1) =0, (115)
k1 j1 k2 j2 θj1 k1 j1 θj2 k2 j2
or j =j , and we get
1 2
(cid:90)
EP (cid:0) ⟨v,θ ⟩(cid:1) P (cid:0) ⟨v,θ ⟩(cid:1) = P (cid:0) ⟨v,n⟩(cid:1) P (cid:0) ⟨v,n⟩(cid:1) dπ (n)=0, (116)
k1 j1 k2 j2 k1 k2 Sd−1
Sd−1
(cid:82)
using the addition formula (40) and the fact that Y (n)Y (n)dπ (n)=0, for i ∈ [N(d,k )]
Sd−1 k1,i1 k2,i2 Sd−1 1 1
and i ∈[N(d,k )] when k ̸=k . Thus
2 2 1 2
E
(cid:88)MS (cid:88)MS
P (cid:0) ⟨v,θ ⟩(cid:1) P (cid:0) ⟨v,θ ⟩(cid:1) =0. (117)
k1 j1 k2 j2
j1=1j2=1
When k =k =k, we use the addition formula (40) to prove that
1 2
1
N (cid:88)(d,k1)N (cid:88)(d,k2)
P (⟨v,θ ⟩)P (⟨v,θ ⟩)= Y (v)Y (v)Y (θ )Y (θ ).
k1 j1 k2 j2 N(d,k )N(d,k ) k1,i1 k2,i2 k1,i1 j1 k2,i2 j2
1 2
i1=1 i2=1
Moreover, since k is non-negative, we have
EY (θ )Y (θ )=EY (θ )Y (θ )=δ δ . (118)
k1,i1 j1 k2,i2 j2 k,i1 j1 k,i2 j2 i1,i2 j1,j2
Therefore,
EP (cid:0) ⟨v,θ ⟩(cid:1) P (cid:0) ⟨v,θ ⟩(cid:1) = 1 δ , (119)
k1 j1 k2 j2 N(d,k) j1,j2
since
N(d,k)
(cid:88)
Y (v)Y (v)=N(d,k)P (1)=N(d,k). (120)
k,i k,i k
i=1
Thus
E
(cid:88)MS (cid:88)MS
P (cid:0) ⟨v,θ ⟩(cid:1) P (cid:0) ⟨v,θ ⟩(cid:1) = M S . (121)
k1 j1 k2 j2 N(d,k)
j1=1j2=1
Finally, observe that | 1 (cid:80)K1 (cid:80)K2 N(d,k )N(d,k )λ λ (cid:80)MS (cid:80)MS P (cid:0) ⟨v,θ ⟩(cid:1) P (cid:0) ⟨v,θ ⟩(cid:1) | is
M S2 k1=2 k2=2 1 2 k1 k2 j1=1 j2=1 k1 j1 k2 j2
bounded by
(cid:88)K1 (cid:88)K2 (cid:16)(cid:88)+∞
(cid:17)2
N(d,k )N(d,k )λ λ ≤ N(d,k)λ <+∞, (122)
1 2 k1 k2 k
k1=2k2=2 k=2
for K ,K ∈N\{0,1}. Indeed, by (44), for k ,k ∈N\{0,1}, we have
1 2 1 2
(cid:12)
(cid:88)MS (cid:88)MS
(cid:0) (cid:1) (cid:0) (cid:1)(cid:12)
(cid:88)MS (cid:88)MS
(cid:0) (cid:1) (cid:0) (cid:1)
(cid:12) P k1 ⟨v,θ j1⟩ P k2 ⟨v,θ j2⟩ (cid:12)≤ |P k1 ⟨v,θ j1⟩ P k2 ⟨v,θ j2⟩ | (123)
j1=1j2=1 j1=1j2=1
(cid:88)MS (cid:88)MS
≤ 1=M2. (124)
S
j1=1j2=1
Combining (117) and (121), and using dominated convergence theorem, we get (19).
Next, we prove that
(cid:88)+∞ N(d,k)λ2
k
≤ 21(cid:16)r2∥ dx −− 1y∥2(cid:17)2 er2∥ dx −− 1y∥2 . (125)
k=2
22For this we make use of (18) which states that
Γ((d−1)/2)
(cid:18) r∥x−y∥(cid:19)k
|λ |≤ , (126)
k Γ(k+(d−1)/2) 2
so that
(cid:16) Γ((d−1)/2) (cid:17)2(cid:18) r∥x−y∥(cid:19)2k
N(d,k)λ2 ≤N(d,k) . (127)
k Γ(k+(d−1)/2) 2
In the following, we derive an upper bound for
Γ2((d−1)/2)
N(d,k) . (128)
Γ2(k+(d−1)/2)
For this purpose, we write
Γ((d−1)/2) Γ((d−1)/2)
= (129)
Γ(k+(d−1)/2) (k+(d−1)/2−1)(k+(d−1)/2−2)···((d−1)/2)Γ((d−1)/2)
(cid:124) (cid:123)(cid:122) (cid:125)
kfactors
2k
= . (130)
(2k+d−3)(2k+d−5)···(d+1)(d−1)
Thus, we get
Γ2((d−1)/2) 22k
= (131)
Γ2(k+(d−1)/2) (2k+d−3)2(2k+d−5)2···(d+1)2(d−1)2
22k
≤ (132)
(2k+d−3)(2k+d−4)(2k+d−5)···(d−1)(d−1)
22k(d−2)!
≤ . (133)
(d−1)(2k+d−3)!
Moreover, we have by (38)
(2k+d−2)(k+d−3)!
N(d,k)= , (134)
k!(d−2)!
so that
Γ2((d−1)/2) (2k+d−2)(k+d−3)! 22k(d−2)!
N(d,k) ≤
Γ2(k+(d−1)/2) k!(d−2)! (d−1)(2k+d−3)!
22k (2k+d−2)
≤
k!(d−1)(2k+d−3)...(k+d−2)
22k
≤ , (135)
k!(d−1)k
since, when k ≥2, we have
(d+2k−2)
(136)
(d−1)(d+2k−3)...(d+k−2)
(d+2k−2)(d+k−3)
= (137)
(d−1)(d+2k−3)...(d+k−2)(d+k−3)
1 (d+2k−2)(d+k−3) 1
= (138)
d−1 (d+2k−3)(d+k−2) (d+2k−4)...(d+k−1)(d+k−3)
(cid:124) (cid:123)(cid:122) (cid:125)
≤1
1 1
≤ (139)
d−1(d+2k−4)...(d+k−1)(d+k−3)
(cid:124) (cid:123)(cid:122) (cid:125)
k−1terms
1
≤ . (140)
(d−1)k
23Therefore, by (127), we have
(cid:88)+∞ (cid:88)+∞ 22k (cid:18) r∥x−y∥(cid:19)2k (cid:88)+∞ 1 (r∥x−y∥)2k
N(d,k)λ2 ≤ = (141)
k k!(d−1)k 2 k! (d−1)k
k=2 k=2 k=2
(cid:88)+∞ 1(cid:32) r2∥x−y∥2(cid:33)k
= . (142)
k! d−1
k=2
Finally, by Lemma C.1, for d≥2 we get
(cid:88)+∞ N(d,k)λ2
k
≤ 1 2(cid:16)r2∥ dx −− 1y∥2(cid:17)2 er2∥ dx −− 1y∥2 . (143)
k=2
B.5 Proof of Proposition 5
Let r ∈R∗ and x,y ∈Rd. When r(x−y)=0, the proof of (23) follows the same steps as in (B.4).
+
In the following, we assume that r(x−y)̸=0, and we make use of (114)
E (f )2 = 1
(cid:88)+∞ (cid:88)+∞
N(d,k )N(d,k )λ λ
(cid:88)MS (cid:88)MS
P (cid:0) ⟨v,θ ⟩(cid:1) P (cid:0) ⟨v,θ ⟩(cid:1) , (144)
S r(x−y) M2 1 2 k1 k2 k1 j1 k2 j2
S k1=2k2=2 j1=1j2=1
proved in Appendix B.4.
Now, let k ,k ∈N\{0,1}. We treat two cases: k ̸=k and k =k .
1 2 1 2 1 2
In the case k ̸=k , we have
1 2
E
(cid:88)MS (cid:88)MS
P (cid:0) ⟨v,θ ⟩(cid:1) P (cid:0) ⟨v,θ ⟩(cid:1) =
(cid:88)MS (cid:88)MS
EP (cid:0) ⟨v,θ ⟩(cid:1) P (cid:0) ⟨v,θ ⟩(cid:1) . (145)
k1 j1 k2 j2 k1 j1 k2 j2
j1=1j2=1 j1=1j2=1
Let (j ,j ) ∈ [M ]2. The random variable θ follows the uniform distribution on Sd−1, so that by the
1 2 S j1
Hecke-Funk formula (46), we have
(cid:90) V (cid:90) 1
E P (⟨v,θ ⟩)= P (⟨v,n⟩)dπ (n)= d−2 P (t)(1−t2)(d−3)/2dt=0. (146)
θj1 k1 j1
Sd−1
k1 Sd−1 V
d−1 −1
k1
Now, when θ and θ come from two different orthonormal matrices they are independent and we have
j1 j2
EP (cid:0) ⟨v,θ ⟩(cid:1) P (cid:0) ⟨v,θ ⟩(cid:1) =E P (cid:0) ⟨v,θ ⟩(cid:1)E P (cid:0) ⟨v,θ ⟩(cid:1) =0. (147)
k1 j1 k2 j2 θj1 k1 j1 θj2 k2 j2
Otherwise, they are not independent, and we make use of Lemma C.3, and we still get
EP (cid:0) ⟨v,θ ⟩(cid:1) P (cid:0) ⟨v,θ ⟩(cid:1) =0. (148)
k1 j1 k2 j2
Thus
E
(cid:88)MS (cid:88)MS
P (cid:0) ⟨v,θ ⟩(cid:1) P (cid:0) ⟨v,θ ⟩(cid:1) =0. (149)
k1 j1 k2 j2
j1=1j2=1
In the case k =k =k ̸=2. Let j ,j ∈[M ]2. When θ and θ come from two different orthonormal
1 2 1 2 S j1 j2
matrices, they are independent, and we have
EP (cid:0) ⟨v,θ ⟩(cid:1) P (cid:0) ⟨v,θ ⟩(cid:1) =0. (150)
k j1 k j2
Otherwise, we use again Lemma C.3, and we get
EP (cid:0) ⟨v,θ ⟩(cid:1) P (cid:0) ⟨v,θ ⟩(cid:1) ≤ 2 (cid:90) P (cid:0) ⟨v,θ⟩(cid:1)2 dπ (θ). (151)
k j1 k j2 d−1 k Sd−1
Sd−1
24Thus
E (cid:88)MS (cid:88)MS P (cid:0) ⟨v,θ ⟩(cid:1) P (cid:0) ⟨v,θ ⟩(cid:1) ≤ M S(cid:16)2d(d−1) +d(cid:17)(cid:90) P (⟨v,θ⟩)2dπ (θ)
k j1 k j2 d d−1 k Sd−1
Sd−1
j1=1j2=1
(cid:90)
≤3M P (⟨v,θ⟩)2dπ (θ). (152)
S k Sd−1
Sd−1
In the case k =k =2. We have
1 2
E
(cid:88)MS (cid:88)MS
P (cid:0) ⟨v,θ ⟩(cid:1) P (cid:0) ⟨v,θ ⟩(cid:1)
=E(cid:16)(cid:88)MS
P (cid:0) ⟨v,θ ⟩(cid:1)(cid:17)2 . (153)
k j1 k j2 2 j
j1=1j2=1 j=1
Now, we prove that
∀v ∈Sd−1,
(cid:88)MS
P (cid:0) ⟨v,θ ⟩(cid:1) =0, (154)
2 j1
j=1
so that
∀v ∈Sd−1, E
(cid:88)MS (cid:88)MS
P (cid:0) ⟨v,θ ⟩(cid:1) P (cid:0) ⟨v,θ ⟩(cid:1) =0. (155)
k j1 k j2
j1=1j2=1
For this purpose, observe that, by definition of P given by 41, we have
2
1 (cid:16) (cid:17)
P (t)= dt2−1 , (156)
2 d−1
so that for m∈[M /d−1], we have
S
(1+m)·d (1+m)·d 1+m
(cid:88) 1 (cid:16) (cid:88) (cid:88) (cid:17) 1 (cid:16) (cid:17)
P (⟨v,θ ⟩)= d ⟨v,θ ⟩2− 1 = d∥v∥2−d =0, (157)
2 j d−1 j d−1
j=1+m·d j=1+m·d j=1+m·d
where we have used the fact that for m∈[M /d−1], (θ ,...,θ ) forms an orthonormal basis of
S 1+m·d (1+m)·d
Rd. By summing (157) over m∈[M /d−1], we prove (154).
S
Finally, combining (144), (149), (152) and (155), and the fact that λ =0 when k is odd, we get
k
EE(f )2 ≤ 3 (cid:88)+∞ N(d,k)2λ2(cid:90) P (cid:0) ⟨v,θ⟩(cid:1)2 dπ (θ). (158)
r(x−y) M k k Sd−1
S Sd−1
k=4
Moreover, by (45) we have
(cid:90) (cid:0) (cid:1)2 1
P ⟨v,θ⟩ dπ (θ)= . (159)
k Sd−1 N(d,k)
Sd−1
Therefore
+∞
3 (cid:88)
EE(f )2 ≤ N(d,k)λ2. (160)
r(x−y) M k
S
k=4
The proof of (24) follows the same steps as in Appendix B.4 by making use of (127) and (135) and
Lemma C.1.
C Lemmata
C.1 An upper bound on the truncation of the exponential function
Lemma C.1. For M ∈N, we have
(cid:12) (cid:88)+∞ 1 (cid:12) xM+1
∀x∈R , (cid:12) xm(cid:12)≤ ex. (161)
+ (cid:12) m! (cid:12) (M +1)!
m=M+1
25Proof. Let φ be the exp function. Using the Taylor’s Theorem with integral form of remainder, we get for
x∈R ,
+
(cid:88)M xm (cid:90) x (x−t)M
φ(x)= φ(m)(0) + φ(M+1)(t) dt. (162)
m! M!
m=0 0
So that
(cid:12) (cid:88)M xm(cid:12) (cid:12)(cid:90) x (x−t)M (cid:12)
(cid:12)φ(x)− φ(m)(0) (cid:12)=(cid:12) φ(M+1)(t) dt(cid:12)
(cid:12) m!(cid:12) (cid:12) M! (cid:12)
m=0 0
(cid:12)(cid:90) x (x−t)M (cid:12)
≤ sup |φ(M+1)(t)|(cid:12) dt(cid:12)
(cid:12) M! (cid:12)
t∈[0,x] 0
xM+1
≤ex .
(M +1)!
We conclude, by observing that
(cid:12) (cid:88)+∞ xm(cid:12) (cid:12) (cid:88)M xm(cid:12)
(cid:12) (cid:12)=(cid:12)φ(x)− φ(m)(0) (cid:12). (163)
(cid:12) m!(cid:12) (cid:12) m!(cid:12)
m=M+1 m=0
C.2 Error bound for the generalized Gauss-Laguerre quadrature rule
Lemma C.2. There exists L>0 such that for any M ∈N∗, we have for φ∈L (p ),
2 Ξ
(cid:88)+∞ √ (cid:12)(cid:90) (cid:88)M (cid:12) (cid:88)+∞ √
m|⟨φ,ℓα⟩ |<+∞ =⇒ (cid:12) φ(ξ)p (ξ)dξ− a φ(ξ )(cid:12)≤L m|⟨φ,ℓα⟩ |, (164)
m pΞ (cid:12) Ξ i i (cid:12) m pΞ
R
m=2M + i=1 m=2M
where the (ℓα n) n∈N is family of normalized generalized Laguerre polynomials, and the a 1,...,a M and the
ξ ,...,ξ are respectively the weights and the nodes of the Gaussian quadrature associated to the weight
1 M
function p .
Ξ
Proof. First, we prove the existence of a universal constant L>0 that satisfies
M
(cid:88)
∀m∈N, | a ℓα(ξ )|≤L. (165)
i m i
i=1
For this purpose, we use Theorem 2.2 in [7] that shows the existence of a universal constant L′ >0 that
does not depend on M such that
(cid:12) (cid:12)(cid:90) +∞
φ(ξ)p
(ξ)dξ−(cid:88)M
a φ(ξ
)(cid:12) (cid:12)≤L′(cid:90) +∞ |φ′ (ξ)(cid:112)
ξp (ξ)|dξ. (166)
(cid:12) Ξ i i (cid:12) Ξ
0 i=1 0
Now, take φ(ξ):=ℓα(ξ). By using (166), we get
m
(cid:12) (cid:12)(cid:90) +∞
ℓα(ξ)p
(ξ)dξ−(cid:88)M
a ℓα(ξ
)(cid:12) (cid:12)≤L′(cid:90) +∞ |ℓ′α(ξ)(cid:112)
ξp (ξ)|dξ
(cid:12) m Ξ i m i (cid:12) m Ξ
0 i=1 0
(cid:18)(cid:90) +∞ (cid:19)1/2(cid:18)(cid:90) +∞ (cid:19)1/2
≤L′ p (ξ)dξ ℓ′α(ξ)2ξp (ξ)dξ
Ξ m Ξ
0 0
(cid:18)(cid:90) +∞ (cid:19)1/2
=L′ ℓ′α(ξ)2ξp (ξ)dξ . (167)
m Ξ
0
26Now, for the un-normalized generalized Laguerre polynomial, we have
L′α(x)=−Lα+1(x),
(168)
m m−1
thus
(cid:115)
m! √
ℓ′α(ξ)=− Lα+1(ξ)=− mℓα+1(ξ). (169)
m Γ(m+α+1) m−1 m−1
(cid:90) +∞ Γ(α+2)
ℓ′α(ξ)2ξp
(ξ)dξ = m=(α+1)m. (170)
m Ξ Γ(α+1)
0
Therefore, by the fact that (cid:82)+∞ ℓα(ξ)p (ξ)dξ =0 when m>0, we have
0 m Ξ
(cid:12)(cid:88)M (cid:12) (cid:112)
(cid:12) a ℓα(ξ )(cid:12)≤L′ (α+1)m, (171)
(cid:12) i m i (cid:12)
i=1
√
and we take L= α+1L′.
Now, let φ∈L (p ), and write
2 Ξ
+∞
(cid:88)
φ= ⟨φ,ℓα⟩ ℓα. (172)
m pΞ m
m=0
By the exactness of the quadrature for polynomials of order smaller than 2M −1, we get
(cid:12)(cid:90) (cid:88)M (cid:12) (cid:88)+∞ (cid:88)M (cid:88)+∞ √
(cid:12) φ(ξ)p (ξ)dξ− a φ(ξ )(cid:12)≤ |⟨φ,ℓα⟩ || a ℓα(ξ )|≤L m|⟨φ,ℓα⟩ |.
(cid:12) Ξ i i (cid:12) m pΞ i m i m pΞ
R
+ i=1 m=2M i=1 m=2M
C.3 A formula of covariance under orthogonal Monte Carlo
Lemma C.3. Let θ ,...,θ be the columns of a random orthogonal matrix sampled from the Haar distribution
1 d
on O (R). Let k,k′ ∈N∗. When k ̸=k′, we have
d
EP (⟨v,θ ⟩)P (⟨v,θ ⟩)=0. (173)
k i k′ j
Otherwise,
2 (cid:90)
∀v ∈Sd−1, EP (⟨θ ,v⟩)P (⟨θ ,v⟩)≤ P (v,θ)2dπ (θ). (174)
k i k j d−1 k Sd−1
Sd−1
Proof. Let i,j ∈ [d] such that i ≠ j. Since θ and θ are the columns of an orthogonal matrix, they are
i j
orthogonal and they follow marginally the uniform distribution on Sd−1. Now, given θ , the random variable
i
θ follows the uniform distribution on Sd−1∩θ⊥, denoted π . Therefore, given ψ ,ψ :Sd−1 →R, we
j i Sd−1(θ⊥) 1 2
i
have
(cid:90) (cid:90)
Eψ (θ )ψ (θ )= ψ (θ ) ψ (θ )dπ (θ )dπ (θ )
1 i 2 j 1 i 2 j Sd−1∩θ⊥ j Sd−1 i
Sd−1 Sd−1∩θ⊥ i
i
(cid:90)
= ψ (θ )(Tψ )(θ )dπ (θ ), (175)
1 i 2 i Sd−1 i
Sd−1
where T :L (π )→L (π ) is defined by
2 Sd−1 2 Sd−1
(cid:90)
(Tψ)(θ):= ψ (θ′)dπ (θ′). (176)
2 Sd−1∩θ⊥
Sd−1∩θ⊥
Now, let k,k′ ∈N∗. By (175), we have
(cid:90)
∀v ∈Sd−1, EP (⟨v,θ ⟩)P (⟨v,θ ⟩)= P (v,θ )(TP (v,.)(θ ))dπ (θ ). (177)
k i k′ j k i k′ i Sd−1 i
Sd−1
27By Theorem 3 in [12], the self-adjoint operator T is bounded and has pure point spectrum. Moreover, T
is a multiple of the identity on P the space of harmonic homogeneous polynomials of degree ℓ. Therefore
ℓ
TP (⟨v,.⟩) is a homogenous polynomial of degree k′. Thus, when k ̸=k′, we have
k′
(cid:90)
P (v,θ )(TP (v,.)(θ )dπ (θ )=0. (178)
k i k′ i Sd−1 i
Sd−1
Ontheotherhand,whenk =k′,weuseagainTheorem3in[12]whichstatesthatP (v,.)isaneigenfunction
k′
of T. Moreover, when d≥4 the corresponding eigenvalue is smaller than (d−1)−1, so that
(cid:90) 1 (cid:90)
P (v,θ )(TP (v,.)(θ )dπ (θ )≤ P (v,θ )2dπ (θ ). (179)
k i k′ i Sd−1 i d−1 k i Sd−1 i
Sd−1 Sd−1
Finally, for d=2 and d=3, the corresponding eigenvalue is smaller or equal to 1, so that
(cid:90) 2 (cid:90)
P (v,θ )(TP (v,.)(θ )dπ (θ )≤ P (v,θ )2dπ (θ ). (180)
k i k′ i Sd−1 i d−1 k i Sd−1 i
Sd−1 Sd−1
C.4 Integral involving generalized Laguerre polynomials
Lemma C.4. The generalized Laguerre polynomials satisfies the following equation
(cid:90) (cid:18) α−α′+n(cid:19)
xα′−1e−xLα(x)dx= Γ(α′) (181)
n n
R+
for arbitrary α,α′ ∈N.
Proof. The generalized Laguerre polynomial can be written into
x−αex dn
Lα(x)= (e−xxn+α). (182)
n n! dxn
Integration by parts n times yields
(cid:90) 1 (cid:90) dn
xα′−1e−xLα(x)dx= xα′−1−α (e−xxn+α)dx (183)
n n! dxn
R+ R+
1 (cid:20) dn−1 (cid:21)∞ α′−1−α(cid:90) dn−1
IB =P xα′−1−α (e−xxn+α) − xα′−2−α (e−xxn+α)dx (184)
n! dxn−1 n! dxn−1
0 R+
(cid:124) (cid:123)(cid:122) (cid:125)
=0
conduct integration by parts n times (185)
(α′−1−α)...(α′−n−α)(cid:90)
=0+(−1)n e−xxα′−1dx (186)
n!
R+
(cid:18) α−α′+n(cid:19)
= Γ(α′). (187)
n
where
(cid:18) (cid:19)
m−n−1 (m−n−1)(m−n−2)...(−n)
:= (188)
m m!
is equal to 0 when 0≤n≤m−1 and
(−1)m(cid:0)n(cid:1)
when n≥m.
m
28D Additional experiments
D.1 Radial rule analysis
When designing the spherical-radial rule, a key part is to find the optimal number of radial quadrature nodes.
To answer this question, we numerically computed the optimal number of radial nodes for different kernel
bandwidth and dataset dimension, which is explained well by Theorem 2.2. When the number of nodes is too
small, all the quadrature nodes concentrated in several hyperspheres and ignored other regions. On the other
hand, when the number of nodes is too big, most of the quadrature weights are approaching zero and have
trivially to the evaluation.
In Figure 7 and Figure 8, we showed the error in relative Frobenius norm for approximating 5000×5000
kernel matrices constructed from synthetic and real datasets. The result is averaged over 100 runs. When
dimension is small (4 or 8) and total number of features is within 1000, the optimal number of radial nodes is
2. When dimension ≈16, the optimal number of radial nodes becomes 1 when feature number goes beyond
100. When dimension is bigger (>16), the optimal number of radial nodes is 1. In kernel approximation and
prediction tasks, we use this as a guide to choose radial design, which yields desirable result.
Why we observe such change of optimal design when dimension increases? Recall that the upper bound
for the approximation error given in Equation (22) consists of two parts: the radial error and the spherical
error. Keep M fixed, the spherical error dominates when the number of spherical nodes M is small; when
R S
M grows, thesphericalerrordecayswith1/M sowewouldseetheerrordecaysasfeaturenumberincreases.
S S
But after some time the error reaches a plateau because the radial error stays unchanged. When dimension is
higher, the spherical error is bigger and we need to take the number of features very large to observe the
plateau.
When we keep the number of total features fixed and increases M , the increase in the spherical error
R
is faster than the decay in the radial error, hence in general when M ≥ 2, larger M yields bigger error.
R R
The idea that we only need one radial node in high dimension is in line with the concentration of Gaussian
measure in high dimension.
D.2 Maximum error and dataset diameter
Figure 9 shows the maximum error of a dataset of diameter D. The synthetic dataset consists of 1k uniform
samples from the hypersphere of radius D/2. As we can see from the experiment, the error given by our
spherical-radial algorithm (SR-OKQ-SOMC) with optimal kernel quadrature and symmetrized orthogonal
Monte Carlo nodes (details explained in Section 4.1) is smaller than other methods by a large margin and
admits modest increase with dataset diameter when d=4. Without optimal kernel quadrature, our method
SR-OMC behaves similarly to SSR, followed by QMC and ORF. The vanilla RFF has much bigger error.
In higher dimension, SR-OMC and RFF have the smallest error, followed by SR-OKQ-SOMC and ORF.
Although QMC has 1/N convergence rate in theory, it has inferior behavior compared with other methods
until the number of features is beyond a threshold, which hard to achieve in practice.
In the experiment we set the spherical kernel bandwidth σS = 1 in SR-OKQ-SOMC for d = 4 and
d=16. This parameter can be tuned bigger in high dimension to improve the algorithm performance, and
we leave the optimal selection of σS for future work.
29small σ medium σ large σ
Figure 7: The approximation error w.r.t. total number of features under different combination of kernel
bandwidth and dataset dimension. From top to bottom, dimension increases from 4 to 32; from left to right,
kernel bandwidth increases from 0.4∗d1/4 to 20∗d1/4.
30small σ medium σ large σ
Figure 8: The approximation error w.r.t. total number of features under different combination of kernel
bandwidth and dataset dimension. From top to bottom are 4 real datasets.
31(a) d=4 (b) d=16
Figure 9: Relation between maximum error and dataset diameter for different kernel approximation schemes.
The maximum approximation error is computed over 102 uniformly-ditsributed samples on the sphere of
radius D.
32