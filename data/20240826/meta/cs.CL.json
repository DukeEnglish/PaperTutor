[
    {
        "title": "Domain-specific long text classification from sparse relevant information",
        "authors": "Célia D'CruzJean-Marc BerederFrédéric PreciosoMichel Riveill",
        "links": "http://arxiv.org/abs/2408.13253v1",
        "entry_id": "http://arxiv.org/abs/2408.13253v1",
        "pdf_url": "http://arxiv.org/pdf/2408.13253v1",
        "summary": "Large Language Models have undoubtedly revolutionized the Natural Language\nProcessing field, the current trend being to promote one-model-for-all tasks\n(sentiment analysis, translation, etc.). However, the statistical mechanisms at\nwork in the larger language models struggle to exploit the relevant information\nwhen it is very sparse, when it is a weak signal. This is the case, for\nexample, for the classification of long domain-specific documents, when the\nrelevance relies on a single relevant word or on very few relevant words from\ntechnical jargon. In the medical domain, it is essential to determine whether a\ngiven report contains critical information about a patient's condition. This\ncritical information is often based on one or few specific isolated terms. In\nthis paper, we propose a hierarchical model which exploits a short list of\npotential target terms to retrieve candidate sentences and represent them into\nthe contextualized embedding of the target term(s) they contain. A pooling of\nthe term(s) embedding(s) entails the document representation to be classified.\nWe evaluate our model on one public medical document benchmark in English and\non one private French medical dataset. We show that our narrower hierarchical\nmodel is better than larger language models for retrieving relevant long\ndocuments in a domain-specific context.",
        "updated": "2024-08-23 17:54:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.13253v1"
    },
    {
        "title": "Data Exposure from LLM Apps: An In-depth Investigation of OpenAI's GPTs",
        "authors": "Evin JaffYuhao WuNing ZhangUmar Iqbal",
        "links": "http://arxiv.org/abs/2408.13247v1",
        "entry_id": "http://arxiv.org/abs/2408.13247v1",
        "pdf_url": "http://arxiv.org/pdf/2408.13247v1",
        "summary": "LLM app ecosystems are quickly maturing and supporting a wide range of use\ncases, which requires them to collect excessive user data. Given that the LLM\napps are developed by third-parties and that anecdotal evidence suggests LLM\nplatforms currently do not strictly enforce their policies, user data shared\nwith arbitrary third-parties poses a significant privacy risk. In this paper we\naim to bring transparency in data practices of LLM apps. As a case study, we\nstudy OpenAI's GPT app ecosystem. We develop an LLM-based framework to conduct\nthe static analysis of natural language-based source code of GPTs and their\nActions (external services) to characterize their data collection practices.\nOur findings indicate that Actions collect expansive data about users,\nincluding sensitive information prohibited by OpenAI, such as passwords. We\nfind that some Actions, including related to advertising and analytics, are\nembedded in multiple GPTs, which allow them to track user activities across\nGPTs. Additionally, co-occurrence of Actions exposes as much as 9.5x more data\nto them, than it is exposed to individual Actions. Lastly, we develop an\nLLM-based privacy policy analysis framework to automatically check the\nconsistency of data collection by Actions with disclosures in their privacy\npolicies. Our measurements indicate that the disclosures for most of the\ncollected data types are omitted in privacy policies, with only 5.8% of Actions\nclearly disclosing their data collection practices.",
        "updated": "2024-08-23 17:42:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.13247v1"
    },
    {
        "title": "Which Prosodic Features Matter Most for Pragmatics?",
        "authors": "Nigel G. WardDivette MarcoOlac Fuentes",
        "links": "http://arxiv.org/abs/2408.13240v1",
        "entry_id": "http://arxiv.org/abs/2408.13240v1",
        "pdf_url": "http://arxiv.org/pdf/2408.13240v1",
        "summary": "We investigate which prosodic features matter most in conveying prosodic\nfunctions. We use the problem of predicting human perceptions of pragmatic\nsimilarity among utterance pairs to evaluate the utility of prosodic features\nof different types. We find, for example, that duration-related features are\nmore important than pitch-related features, and that utterance-initial features\nare more important than utterance-final features. Further, failure analysis\nindicates that modeling using pitch features only often fails to handle\nimportant pragmatic functions, and suggests that several generally-neglected\nacoustic and prosodic features are pragmatically significant, including\nnasality and vibrato. These findings can guide future basic research in\nprosody, and suggest how to improve speech synthesis evaluation, among other\napplications.",
        "updated": "2024-08-23 17:29:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.13240v1"
    },
    {
        "title": "Multi-Layer Transformers Gradient Can be Approximated in Almost Linear Time",
        "authors": "Yingyu LiangZhizhou ShaZhenmei ShiZhao SongYufa Zhou",
        "links": "http://arxiv.org/abs/2408.13233v1",
        "entry_id": "http://arxiv.org/abs/2408.13233v1",
        "pdf_url": "http://arxiv.org/pdf/2408.13233v1",
        "summary": "The quadratic computational complexity in the self-attention mechanism of\npopular transformer architectures poses significant challenges for training and\ninference, particularly in terms of efficiency and memory requirements. Towards\naddressing these challenges, this paper introduces a novel fast computation\nmethod for gradient calculation in multi-layer transformer models. Our approach\nenables the computation of gradients for the entire multi-layer transformer\nmodel in almost linear time $n^{1+o(1)}$, where $n$ is the input sequence\nlength. This breakthrough significantly reduces the computational bottleneck\nassociated with the traditional quadratic time complexity. Our theory holds for\nany loss function and maintains a bounded approximation error across the entire\nmodel. Furthermore, our analysis can hold when the multi-layer transformer\nmodel contains many practical sub-modules, such as residual connection, casual\nmask, and multi-head attention. By improving the efficiency of gradient\ncomputation in large language models, we hope that our work will facilitate the\nmore effective training and deployment of long-context language models based on\nour theoretical results.",
        "updated": "2024-08-23 17:16:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.13233v1"
    },
    {
        "title": "Enhancing Few-Shot Transfer Learning with Optimized Multi-Task Prompt Tuning through Modular Prompt Composition",
        "authors": "Ahmad PouraminiHesham Faili",
        "links": "http://arxiv.org/abs/2408.13227v1",
        "entry_id": "http://arxiv.org/abs/2408.13227v1",
        "pdf_url": "http://arxiv.org/pdf/2408.13227v1",
        "summary": "In recent years, multi-task prompt tuning has garnered considerable attention\nfor its inherent modularity and potential to enhance parameter-efficient\ntransfer learning across diverse tasks. This paper aims to analyze and improve\nthe performance of multiple tasks by facilitating the transfer of knowledge\nbetween their corresponding prompts in a multi-task setting. Our proposed\napproach decomposes the prompt for each target task into a combination of\nshared prompts (source prompts) and a task-specific prompt (private prompt).\nDuring training, the source prompts undergo fine-tuning and are integrated with\nthe private prompt to drive the target prompt for each task. We present and\ncompare multiple methods for combining source prompts to construct the target\nprompt, analyzing the roles of both source and private prompts within each\nmethod. We investigate their contributions to task performance and offer\nflexible, adjustable configurations based on these insights to optimize\nperformance. Our empirical findings clearly showcase improvements in accuracy\nand robustness compared to the conventional practice of prompt tuning and\nrelated works. Notably, our results substantially outperform other methods in\nthe field in few-shot settings, demonstrating superior performance in various\ntasks across GLUE benchmark, among other tasks. This achievement is attained\nwith a significantly reduced amount of training data, making our method a\npromising one for few-shot settings.",
        "updated": "2024-08-23 17:01:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.13227v1"
    }
]