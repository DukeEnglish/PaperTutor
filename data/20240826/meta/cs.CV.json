[
    {
        "title": "MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?",
        "authors": "Yi-Fan ZhangHuanyu ZhangHaochen TianChaoyou FuShuangqing ZhangJunfei WuFeng LiKun WangQingsong WenZhang ZhangLiang WangRong JinTieniu Tan",
        "links": "http://arxiv.org/abs/2408.13257v1",
        "entry_id": "http://arxiv.org/abs/2408.13257v1",
        "pdf_url": "http://arxiv.org/pdf/2408.13257v1",
        "summary": "Comprehensive evaluation of Multimodal Large Language Models (MLLMs) has\nrecently garnered widespread attention in the research community. However, we\nobserve that existing benchmarks present several common barriers that make it\ndifficult to measure the significant challenges that models face in the real\nworld, including: 1) small data scale leads to a large performance variance; 2)\nreliance on model-based annotations results in restricted data quality; 3)\ninsufficient task difficulty, especially caused by the limited image\nresolution. To tackle these issues, we introduce MME-RealWorld. Specifically,\nwe collect more than $300$K images from public datasets and the Internet,\nfiltering $13,366$ high-quality images for annotation. This involves the\nefforts of professional $25$ annotators and $7$ experts in MLLMs, contributing\nto $29,429$ question-answer pairs that cover $43$ subtasks across $5$\nreal-world scenarios, extremely challenging even for humans. As far as we know,\nMME-RealWorld is the largest manually annotated benchmark to date, featuring\nthe highest resolution and a targeted focus on real-world applications. We\nfurther conduct a thorough evaluation involving $28$ prominent MLLMs, such as\nGPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet. Our results show that even the\nmost advanced models struggle with our benchmarks, where none of them reach\n$60\\%$ accuracy. The challenges of perceiving high-resolution images and\nunderstanding complex real-world scenarios remain urgent issues to be\naddressed. The data and evaluation code are released at\nhttps://mme-realworld.github.io/ .",
        "updated": "2024-08-23 17:59:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.13257v1"
    },
    {
        "title": "How Diffusion Models Learn to Factorize and Compose",
        "authors": "Qiyao LiangZiming LiuMitchell OstrowIla Fiete",
        "links": "http://arxiv.org/abs/2408.13256v1",
        "entry_id": "http://arxiv.org/abs/2408.13256v1",
        "pdf_url": "http://arxiv.org/pdf/2408.13256v1",
        "summary": "Diffusion models are capable of generating photo-realistic images that\ncombine elements which likely do not appear together in the training set,\ndemonstrating the ability to compositionally generalize. Nonetheless, the\nprecise mechanism of compositionality and how it is acquired through training\nremains elusive. Inspired by cognitive neuroscientific approaches, we consider\na highly reduced setting to examine whether and when diffusion models learn\nsemantically meaningful and factorized representations of composable features.\nWe performed extensive controlled experiments on conditional Denoising\nDiffusion Probabilistic Models (DDPMs) trained to generate various forms of 2D\nGaussian data. We found that the models learn factorized but not fully\ncontinuous manifold representations for encoding continuous features of\nvariation underlying the data. With such representations, models demonstrate\nsuperior feature compositionality but limited ability to interpolate over\nunseen values of a given feature. Our experimental results further demonstrate\nthat diffusion models can attain compositionality with few compositional\nexamples, suggesting a more efficient way to train DDPMs. Finally, we connect\nmanifold formation in diffusion models to percolation theory in physics,\noffering insight into the sudden onset of factorized representation learning.\nOur thorough toy experiments thus contribute a deeper understanding of how\ndiffusion models capture compositional structure in data.",
        "updated": "2024-08-23 17:59:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.13256v1"
    },
    {
        "title": "Ensemble Modeling of Multiple Physical Indicators to Dynamically Phenotype Autism Spectrum Disorder",
        "authors": "Marie HuynhAaron KlineSaimourya SurabhiKaitlyn DunlapOnur Cezmi MutluMohammadmahdi HonarmandParnian AzizianPeter WashingtonDennis P. Wall",
        "links": "http://arxiv.org/abs/2408.13255v1",
        "entry_id": "http://arxiv.org/abs/2408.13255v1",
        "pdf_url": "http://arxiv.org/pdf/2408.13255v1",
        "summary": "Early detection of autism, a neurodevelopmental disorder marked by social\ncommunication challenges, is crucial for timely intervention. Recent\nadvancements have utilized naturalistic home videos captured via the mobile\napplication GuessWhat. Through interactive games played between children and\ntheir guardians, GuessWhat has amassed over 3,000 structured videos from 382\nchildren, both diagnosed with and without Autism Spectrum Disorder (ASD). This\ncollection provides a robust dataset for training computer vision models to\ndetect ASD-related phenotypic markers, including variations in emotional\nexpression, eye contact, and head movements. We have developed a protocol to\ncurate high-quality videos from this dataset, forming a comprehensive training\nset. Utilizing this set, we trained individual LSTM-based models using eye\ngaze, head positions, and facial landmarks as input features, achieving test\nAUCs of 86%, 67%, and 78%, respectively. To boost diagnostic accuracy, we\napplied late fusion techniques to create ensemble models, improving the overall\nAUC to 90%. This approach also yielded more equitable results across different\ngenders and age groups. Our methodology offers a significant step forward in\nthe early detection of ASD by potentially reducing the reliance on subjective\nassessments and making early identification more accessibly and equitable.",
        "updated": "2024-08-23 17:55:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.13255v1"
    },
    {
        "title": "LayerPano3D: Layered 3D Panorama for Hyper-Immersive Scene Generation",
        "authors": "Shuai YangJing TanMengchen ZhangTong WuYixuan LiGordon WetzsteinZiwei LiuDahua Lin",
        "links": "http://arxiv.org/abs/2408.13252v1",
        "entry_id": "http://arxiv.org/abs/2408.13252v1",
        "pdf_url": "http://arxiv.org/pdf/2408.13252v1",
        "summary": "3D immersive scene generation is a challenging yet critical task in computer\nvision and graphics. A desired virtual 3D scene should 1) exhibit\nomnidirectional view consistency, and 2) allow for free exploration in complex\nscene hierarchies. Existing methods either rely on successive scene expansion\nvia inpainting or employ panorama representation to represent large FOV scene\nenvironments. However, the generated scene suffers from semantic drift during\nexpansion and is unable to handle occlusion among scene hierarchies. To tackle\nthese challenges, we introduce LayerPano3D, a novel framework for full-view,\nexplorable panoramic 3D scene generation from a single text prompt. Our key\ninsight is to decompose a reference 2D panorama into multiple layers at\ndifferent depth levels, where each layer reveals the unseen space from the\nreference views via diffusion prior. LayerPano3D comprises multiple dedicated\ndesigns: 1) we introduce a novel text-guided anchor view synthesis pipeline for\nhigh-quality, consistent panorama generation. 2) We pioneer the Layered 3D\nPanorama as underlying representation to manage complex scene hierarchies and\nlift it into 3D Gaussians to splat detailed 360-degree omnidirectional scenes\nwith unconstrained viewing paths. Extensive experiments demonstrate that our\nframework generates state-of-the-art 3D panoramic scene in both full view\nconsistency and immersive exploratory experience. We believe that LayerPano3D\nholds promise for advancing 3D panoramic scene creation with numerous\napplications.",
        "updated": "2024-08-23 17:50:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.13252v1"
    },
    {
        "title": "Re-evaluation of Face Anti-spoofing Algorithm in Post COVID-19 Era Using Mask Based Occlusion Attack",
        "authors": "Vaibhav SundharamAbhijit SarkarA. Lynn Abbott",
        "links": "http://arxiv.org/abs/2408.13251v1",
        "entry_id": "http://arxiv.org/abs/2408.13251v1",
        "pdf_url": "http://arxiv.org/pdf/2408.13251v1",
        "summary": "Face anti-spoofing algorithms play a pivotal role in the robust deployment of\nface recognition systems against presentation attacks. Conventionally, full\nfacial images are required by such systems to correctly authenticate\nindividuals, but the widespread requirement of masks due to the current\nCOVID-19 pandemic has introduced new challenges for these biometric\nauthentication systems. Hence, in this work, we investigate the performance of\npresentation attack detection (PAD) algorithms under synthetic facial\nocclusions using masks and glasses. We have used five variants of masks to\ncover the lower part of the face with varying coverage areas (low-coverage,\nmedium-coverage, high-coverage, round coverage), and 3D cues. We have also used\ndifferent variants of glasses that cover the upper part of the face. We\nsystematically tested the performance of four PAD algorithms under these\nocclusion attacks using a benchmark dataset. We have specifically looked at\nfour different baseline PAD algorithms that focus on, texture, image quality,\nframe difference/motion, and abstract features through a convolutional neural\nnetwork (CNN). Additionally we have introduced a new hybrid model that uses CNN\nand local binary pattern textures. Our experiment shows that adding the\nocclusions significantly degrades the performance of all of the PAD algorithms.\nOur results show the vulnerability of face anti-spoofing algorithms with\nocclusions, which could be in the usage of such algorithms in the post-pandemic\nera.",
        "updated": "2024-08-23 17:48:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.13251v1"
    }
]