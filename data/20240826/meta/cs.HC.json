[
    {
        "title": "Demonstration of Wheeler: A Three-Wheeled Input Device for Usable, Efficient, and Versatile Non-Visual Interaction",
        "authors": "Md Touhidul IslamNoushad SojibImran KabirAshiqur Rahman AmitMohammad Ruhul AminSyed Masum Billah",
        "links": "http://dx.doi.org/10.1145/3672539.3686749",
        "entry_id": "http://arxiv.org/abs/2408.13173v1",
        "pdf_url": "http://arxiv.org/pdf/2408.13173v1",
        "summary": "Navigating multi-level menus with complex hierarchies remains a big challenge\nfor blind and low-vision users, who predominantly use screen readers to\ninteract with computers. To that end, we demonstrate Wheeler, a three-wheeled\ninput device with two side buttons that can speed up complex multi-level\nhierarchy navigation in common applications. When in operation, the three\nwheels of Wheeler are each mapped to a different level in the application\nhierarchy. Each level can be independently traversed using its designated\nwheel, allowing users to navigate through multiple levels efficiently.\nWheeler's three wheels can also be repurposed for other tasks such as 2D cursor\nmanipulation. In this demonstration, we describe the different operation modes\nand usage of Wheeler.",
        "updated": "2024-08-23 15:44:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.13173v1"
    },
    {
        "title": "Wheeler: A Three-Wheeled Input Device for Usable, Efficient, and Versatile Non-Visual Interaction",
        "authors": "Md Touhidul IslamNoushad SojibImran KabirAshiqur Rahman AmitMohammad Ruhul AminSyed Masum Billah",
        "links": "http://dx.doi.org/10.1145/3654777.3676396",
        "entry_id": "http://arxiv.org/abs/2408.13166v1",
        "pdf_url": "http://arxiv.org/pdf/2408.13166v1",
        "summary": "Blind users rely on keyboards and assistive technologies like screen readers\nto interact with user interface (UI) elements. In modern applications with\ncomplex UI hierarchies, navigating to different UI elements poses a significant\naccessibility challenge. Users must listen to screen reader audio descriptions\nand press relevant keyboard keys one at a time. This paper introduces Wheeler,\na novel three-wheeled, mouse-shaped stationary input device, to address this\nissue. Informed by participatory sessions, Wheeler enables blind users to\nnavigate up to three hierarchical levels in an app independently using three\nwheels instead of navigating just one level at a time using a keyboard. The\nthree wheels also offer versatility, allowing users to repurpose them for other\ntasks, such as 2D cursor manipulation. A study with 12 blind users indicates a\nsignificant reduction (40%) in navigation time compared to using a keyboard.\nFurther, a diary study with our blind co-author highlights Wheeler's additional\nbenefits, such as accessing UI elements with partial metadata and facilitating\nmixed-ability collaboration.",
        "updated": "2024-08-23 15:39:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.13166v1"
    },
    {
        "title": "Avatar Visual Similarity for Social HCI: Increasing Self-Awareness",
        "authors": "Bernhard HilpertClaudio Alves da SilvaLeon ChristidisChirag BhuvaneshwaraPatrick GebhardFabrizio NunnariDimitra Tsovaltzi",
        "links": "http://arxiv.org/abs/2408.13084v1",
        "entry_id": "http://arxiv.org/abs/2408.13084v1",
        "pdf_url": "http://arxiv.org/pdf/2408.13084v1",
        "summary": "Self-awareness is a critical factor in social human-human interaction and,\nhence, in social HCI interaction. Increasing self-awareness through mirrors or\nvideo recordings is common in face-to-face trainings, since it influences\nantecedents of self-awareness like explicit identification and implicit\naffective identification (affinity). However, increasing self-awareness has\nbeen scarcely examined in virtual trainings with virtual avatars, which allow\nfor adjusting the similarity, e.g. to avoid negative effects of\nself-consciousness. Automatic visual similarity in avatars is an open issue\nrelated to high costs. It is important to understand which features need to be\nmanipulated and which degree of similarity is necessary for self-awareness to\nleverage the added value of using avatars for self-awareness. This article\nexamines the relationship between avatar visual similarity and increasing\nself-awareness in virtual training environments. We define visual similarity\nbased on perceptually important facial features for human-human identification\nand develop a theory-based methodology to systematically manipulate visual\nsimilarity of virtual avatars and support self-awareness. Three personalized\nversions of virtual avatars with varying degrees of visual similarity to\nparticipants were created (weak, medium and strong facial features\nmanipulation). In a within-subject study (N=33), we tested effects of degree of\nsimilarity on perceived similarity, explicit identification and implicit\naffective identification (affinity). Results show significant differences\nbetween the weak similarity manipulation, and both the strong manipulation and\nthe random avatar for all three antecedents of self-awareness. An increasing\ndegree of avatar visual similarity influences antecedents of self-awareness in\nvirtual environments.",
        "updated": "2024-08-23 14:11:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.13084v1"
    },
    {
        "title": "VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints",
        "authors": "Jinghua TangLiyun ZhangYu LuDian DingLanqing YangYiChao ChenMinjie BianXiaoshan LiGuangtao Xue",
        "links": "http://arxiv.org/abs/2408.13019v1",
        "entry_id": "http://arxiv.org/abs/2408.13019v1",
        "pdf_url": "http://arxiv.org/pdf/2408.13019v1",
        "summary": "Emotion recognition can enhance humanized machine responses to user commands,\nwhile voiceprint-based perception systems can be easily integrated into\ncommonly used devices like smartphones and stereos. Despite having the largest\nnumber of speakers, there is a noticeable absence of high-quality corpus\ndatasets for emotion recognition using Chinese voiceprints. Hence, this paper\nintroduces the VCEMO dataset to address this deficiency. The proposed dataset\nis constructed from everyday conversations and comprises over 100 users and\n7,747 textual samples. Furthermore, this paper proposes a multimodal-based\nmodel as a benchmark, which effectively fuses speech, text, and external\nknowledge using a co-attention structure. The system employs contrastive\nlearning-based regulation for the uneven distribution of the dataset and the\ndiversity of emotional expressions. The experiments demonstrate the significant\nimprovement of the proposed model over SOTA on the VCEMO and IEMOCAP datasets.\nCode and dataset will be released for research.",
        "updated": "2024-08-23 12:14:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.13019v1"
    },
    {
        "title": "A Survey on Drowsiness Detection -- Modern Applications and Methods",
        "authors": "Biying FuFadi BoutrosChin-Teng LinNaser Damer",
        "links": "http://arxiv.org/abs/2408.12990v1",
        "entry_id": "http://arxiv.org/abs/2408.12990v1",
        "pdf_url": "http://arxiv.org/pdf/2408.12990v1",
        "summary": "Drowsiness detection holds paramount importance in ensuring safety in\nworkplaces or behind the wheel, enhancing productivity, and healthcare across\ndiverse domains. Therefore accurate and real-time drowsiness detection plays a\ncritical role in preventing accidents, enhancing safety, and ultimately saving\nlives across various sectors and scenarios. This comprehensive review explores\nthe significance of drowsiness detection in various areas of application,\ntranscending the conventional focus solely on driver drowsiness detection. We\ndelve into the current methodologies, challenges, and technological\nadvancements in drowsiness detection schemes, considering diverse contexts such\nas public transportation, healthcare, workplace safety, and beyond. By\nexamining the multifaceted implications of drowsiness, this work contributes to\na holistic understanding of its impact and the crucial role of accurate and\nreal-time detection techniques in enhancing safety and performance. We\nidentified weaknesses in current algorithms and limitations in existing\nresearch such as accurate and real-time detection, stable data transmission,\nand building bias-free systems. Our survey frames existing works and leads to\npractical recommendations like mitigating the bias issue by using synthetic\ndata, overcoming the hardware limitations with model compression, and\nleveraging fusion to boost model performance. This is a pioneering work to\nsurvey the topic of drowsiness detection in such an entirely and not only\nfocusing on one single aspect. We consider the topic of drowsiness detection as\na dynamic and evolving field, presenting numerous opportunities for further\nexploration.",
        "updated": "2024-08-23 11:15:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.12990v1"
    }
]