[
    {
        "title": "JacNet: Learning Functions with Structured Jacobians",
        "authors": "Jonathan LorraineSafwan Hossain",
        "links": "http://arxiv.org/abs/2408.13237v1",
        "entry_id": "http://arxiv.org/abs/2408.13237v1",
        "pdf_url": "http://arxiv.org/pdf/2408.13237v1",
        "summary": "Neural networks are trained to learn an approximate mapping from an input\ndomain to a target domain. Incorporating prior knowledge about true mappings is\ncritical to learning a useful approximation. With current architectures, it is\nchallenging to enforce structure on the derivatives of the input-output\nmapping. We propose to use a neural network to directly learn the Jacobian of\nthe input-output function, which allows easy control of the derivative. We\nfocus on structuring the derivative to allow invertibility and also demonstrate\nthat other useful priors, such as $k$-Lipschitz, can be enforced. Using this\napproach, we can learn approximations to simple functions that are guaranteed\nto be invertible and easily compute the inverse. We also show similar results\nfor 1-Lipschitz functions.",
        "updated": "2024-08-23 17:21:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.13237v1"
    },
    {
        "title": "Double Descent: Understanding Linear Model Estimation of Nonidentifiable Parameters and a Model for Overfitting",
        "authors": "Ronald Christensen",
        "links": "http://arxiv.org/abs/2408.13235v1",
        "entry_id": "http://arxiv.org/abs/2408.13235v1",
        "pdf_url": "http://arxiv.org/pdf/2408.13235v1",
        "summary": "We consider ordinary least squares estimation and variations on least squares\nestimation such as penalized (regularized) least squares and spectral shrinkage\nestimates for problems with p > n and associated problems with prediction of\nnew observations. After the introduction of Section 1, Section 2 examines a\nnumber of commonly used estimators for p > n. Section 3 introduces prediction\nwith p > n. Section 4 introduces notational changes to facilitate discussion of\noverfitting and Section 5 illustrates the phenomenon of double descent. We\nconclude with some final comments.",
        "updated": "2024-08-23 17:19:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.13235v1"
    },
    {
        "title": "On the design of scalable, high-precision spherical-radial Fourier features",
        "authors": "Ayoub BelhadjiQianyu Julie ZhuYoussef Marzouk",
        "links": "http://arxiv.org/abs/2408.13231v1",
        "entry_id": "http://arxiv.org/abs/2408.13231v1",
        "pdf_url": "http://arxiv.org/pdf/2408.13231v1",
        "summary": "Approximation using Fourier features is a popular technique for scaling\nkernel methods to large-scale problems, with myriad applications in machine\nlearning and statistics. This method replaces the integral representation of a\nshift-invariant kernel with a sum using a quadrature rule. The design of the\nlatter is meant to reduce the number of features required for high-precision\napproximation. Specifically, for the squared exponential kernel, one must\ndesign a quadrature rule that approximates the Gaussian measure on\n$\\mathbb{R}^d$. Previous efforts in this line of research have faced\ndifficulties in higher dimensions. We introduce a new family of quadrature\nrules that accurately approximate the Gaussian measure in higher dimensions by\nexploiting its isotropy. These rules are constructed as a tensor product of a\nradial quadrature rule and a spherical quadrature rule. Compared to previous\nwork, our approach leverages a thorough analysis of the approximation error,\nwhich suggests natural choices for both the radial and spherical components. We\ndemonstrate that this family of Fourier features yields improved approximation\nbounds.",
        "updated": "2024-08-23 17:11:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.13231v1"
    },
    {
        "title": "Amortized Bayesian Multilevel Models",
        "authors": "Daniel HabermannMarvin SchmittLars KühmichelAndreas BullingStefan T. RadevPaul-Christian Bürkner",
        "links": "http://arxiv.org/abs/2408.13230v1",
        "entry_id": "http://arxiv.org/abs/2408.13230v1",
        "pdf_url": "http://arxiv.org/pdf/2408.13230v1",
        "summary": "Multilevel models (MLMs) are a central building block of the Bayesian\nworkflow. They enable joint, interpretable modeling of data across hierarchical\nlevels and provide a fully probabilistic quantification of uncertainty. Despite\ntheir well-recognized advantages, MLMs pose significant computational\nchallenges, often rendering their estimation and evaluation intractable within\nreasonable time constraints. Recent advances in simulation-based inference\noffer promising solutions for addressing complex probabilistic models using\ndeep generative networks. However, the utility and reliability of deep learning\nmethods for estimating Bayesian MLMs remains largely unexplored, especially\nwhen compared with gold-standard samplers. To this end, we explore a family of\nneural network architectures that leverage the probabilistic factorization of\nmultilevel models to facilitate efficient neural network training and\nsubsequent near-instant posterior inference on unseen data sets. We test our\nmethod on several real-world case studies and provide comprehensive comparisons\nto Stan as a gold-standard method where possible. Finally, we provide an\nopen-source implementation of our methods to stimulate further research in the\nnascent field of amortized Bayesian inference.",
        "updated": "2024-08-23 17:11:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.13230v1"
    },
    {
        "title": "An Overview on Machine Learning Methods for Partial Differential Equations: from Physics Informed Neural Networks to Deep Operator Learning",
        "authors": "Lukas GononArnulf JentzenBenno KuckuckSiyu LiangAdrian RiekertPhilippe von Wurstemberger",
        "links": "http://arxiv.org/abs/2408.13222v1",
        "entry_id": "http://arxiv.org/abs/2408.13222v1",
        "pdf_url": "http://arxiv.org/pdf/2408.13222v1",
        "summary": "The approximation of solutions of partial differential equations (PDEs) with\nnumerical algorithms is a central topic in applied mathematics. For many\ndecades, various types of methods for this purpose have been developed and\nextensively studied. One class of methods which has received a lot of attention\nin recent years are machine learning-based methods, which typically involve the\ntraining of artificial neural networks (ANNs) by means of stochastic gradient\ndescent type optimization methods. While approximation methods for PDEs using\nANNs have first been proposed in the 1990s they have only gained wide\npopularity in the last decade with the rise of deep learning. This article aims\nto provide an introduction to some of these methods and the mathematical theory\non which they are based. We discuss methods such as physics-informed neural\nnetworks (PINNs) and deep BSDE methods and consider several operator learning\napproaches.",
        "updated": "2024-08-23 16:57:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.13222v1"
    }
]