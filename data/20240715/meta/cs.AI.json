[
    {
        "title": "Adaptive Prediction Ensemble: Improving Out-of-Distribution Generalization of Motion Forecasting",
        "authors": "Jinning LiJiachen LiSangjae BaeDavid Isele",
        "links": "http://arxiv.org/abs/2407.09475v1",
        "entry_id": "http://arxiv.org/abs/2407.09475v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09475v1",
        "summary": "Deep learning-based trajectory prediction models for autonomous driving often\nstruggle with generalization to out-of-distribution (OOD) scenarios, sometimes\nperforming worse than simple rule-based models. To address this limitation, we\npropose a novel framework, Adaptive Prediction Ensemble (APE), which integrates\ndeep learning and rule-based prediction experts. A learned routing function,\ntrained concurrently with the deep learning model, dynamically selects the most\nreliable prediction based on the input scenario. Our experiments on large-scale\ndatasets, including Waymo Open Motion Dataset (WOMD) and Argoverse, demonstrate\nimprovement in zero-shot generalization across datasets. We show that our\nmethod outperforms individual prediction models and other variants,\nparticularly in long-horizon prediction and scenarios with a high proportion of\nOOD data. This work highlights the potential of hybrid approaches for robust\nand generalizable motion prediction in autonomous driving.",
        "updated": "2024-07-12 17:57:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09475v1"
    },
    {
        "title": "FairyLandAI: Personalized Fairy Tales utilizing ChatGPT and DALLE-3",
        "authors": "Georgios MakridisAthanasios OikonomouVasileios Koukos",
        "links": "http://arxiv.org/abs/2407.09467v1",
        "entry_id": "http://arxiv.org/abs/2407.09467v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09467v1",
        "summary": "In the diverse world of AI-driven storytelling, there is a unique opportunity\nto engage young audiences with customized, and personalized narratives. This\npaper introduces FairyLandAI an innovative Large Language Model (LLM) developed\nthrough OpenAI's API, specifically crafted to create personalized fairytales\nfor children. The distinctive feature of FairyLandAI is its dual capability: it\nnot only generates stories that are engaging, age-appropriate, and reflective\nof various traditions but also autonomously produces imaginative prompts\nsuitable for advanced image generation tools like GenAI and Dalle-3, thereby\nenriching the storytelling experience. FairyLandAI is expertly tailored to\nresonate with the imaginative worlds of children, providing narratives that are\nboth educational and entertaining and in alignment with the moral values\ninherent in different ages. Its unique strength lies in customizing stories to\nmatch individual children's preferences and cultural backgrounds, heralding a\nnew era in personalized storytelling. Further, its integration with image\ngeneration technology offers a comprehensive narrative experience that\nstimulates both verbal and visual creativity. Empirical evaluations of\nFairyLandAI demonstrate its effectiveness in crafting captivating stories for\nchildren, which not only entertain but also embody the values and teachings of\ndiverse traditions. This model serves as an invaluable tool for parents and\neducators, supporting them in imparting meaningful moral lessons through\nengaging narratives. FairyLandAI represents a pioneering step in using LLMs,\nparticularly through OpenAI's API, for educational and cultural enrichment,\nmaking complex moral narratives accessible and enjoyable for young, imaginative\nminds.",
        "updated": "2024-07-12 17:46:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09467v1"
    },
    {
        "title": "Human-like Episodic Memory for Infinite Context LLMs",
        "authors": "Zafeirios FountasMartin A BenfeghoulAdnan OomerjeeFenia ChristopoulouGerasimos LampourasHaitham Bou-AmmarJun Wang",
        "links": "http://arxiv.org/abs/2407.09450v1",
        "entry_id": "http://arxiv.org/abs/2407.09450v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09450v1",
        "summary": "Large language models (LLMs) have shown remarkable capabilities, but still\nstruggle with processing extensive contexts, limiting their ability to maintain\ncoherence and accuracy over long sequences. In contrast, the human brain excels\nat organising and retrieving episodic experiences across vast temporal scales,\nspanning a lifetime. In this work, we introduce EM-LLM, a novel approach that\nintegrates key aspects of human episodic memory and event cognition into LLMs,\nenabling them to effectively handle practically infinite context lengths while\nmaintaining computational efficiency. EM-LLM organises sequences of tokens into\ncoherent episodic events using a combination of Bayesian surprise and\ngraph-theoretic boundary refinement in an on-line fashion. When needed, these\nevents are retrieved through a two-stage memory process, combining\nsimilarity-based and temporally contiguous retrieval for efficient and\nhuman-like access to relevant information. Experiments on the LongBench dataset\ndemonstrate EM-LLM's superior performance, outperforming the state-of-the-art\nInfLLM model with an overall relative improvement of 4.3% across various tasks,\nincluding a 33% improvement on the PassageRetrieval task. Furthermore, our\nanalysis reveals strong correlations between EM-LLM's event segmentation and\nhuman-perceived events, suggesting a bridge between this artificial system and\nits biological counterpart. This work not only advances LLM capabilities in\nprocessing extended contexts but also provides a computational framework for\nexploring human memory mechanisms, opening new avenues for interdisciplinary\nresearch in AI and cognitive science.",
        "updated": "2024-07-12 17:34:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09450v1"
    },
    {
        "title": "The $μ\\mathcal{G}$ Language for Programming Graph Neural Networks",
        "authors": "Matteo BelenchiaFlavio CorradiniMichela QuadriniMichele Loreti",
        "links": "http://arxiv.org/abs/2407.09441v1",
        "entry_id": "http://arxiv.org/abs/2407.09441v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09441v1",
        "summary": "Graph neural networks form a class of deep learning architectures\nspecifically designed to work with graph-structured data. As such, they share\nthe inherent limitations and problems of deep learning, especially regarding\nthe issues of explainability and trustworthiness. We propose $\\mu\\mathcal{G}$,\nan original domain-specific language for the specification of graph neural\nnetworks that aims to overcome these issues. The language's syntax is\nintroduced, and its meaning is rigorously defined by a denotational semantics.\nAn equivalent characterization in the form of an operational semantics is also\nprovided and, together with a type system, is used to prove the type soundness\nof $\\mu\\mathcal{G}$. We show how $\\mu\\mathcal{G}$ programs can be represented\nin a more user-friendly graphical visualization, and provide examples of its\ngenerality by showing how it can be used to define some of the most popular\ngraph neural network models, or to develop any custom graph processing\napplication.",
        "updated": "2024-07-12 17:27:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09441v1"
    },
    {
        "title": "Let Me DeCode You: Decoder Conditioning with Tabular Data",
        "authors": "Tomasz SzczepańskiMichal K. GrzeszczykSzymon PłotkaArleta AdamowiczPiotr FudalejPrzemysław KorzeniowskiTomasz TrzcińskiArkadiusz Sitek",
        "links": "http://arxiv.org/abs/2407.09437v1",
        "entry_id": "http://arxiv.org/abs/2407.09437v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09437v1",
        "summary": "Training deep neural networks for 3D segmentation tasks can be challenging,\noften requiring efficient and effective strategies to improve model\nperformance. In this study, we introduce a novel approach, DeCode, that\nutilizes label-derived features for model conditioning to support the decoder\nin the reconstruction process dynamically, aiming to enhance the efficiency of\nthe training process. DeCode focuses on improving 3D segmentation performance\nthrough the incorporation of conditioning embedding with learned numerical\nrepresentation of 3D-label shape features. Specifically, we develop an\napproach, where conditioning is applied during the training phase to guide the\nnetwork toward robust segmentation. When labels are not available during\ninference, our model infers the necessary conditioning embedding directly from\nthe input data, thanks to a feed-forward network learned during the training\nphase. This approach is tested using synthetic data and cone-beam computed\ntomography (CBCT) images of teeth. For CBCT, three datasets are used: one\npublicly available and two in-house. Our results show that DeCode significantly\noutperforms traditional, unconditioned models in terms of generalization to\nunseen data, achieving higher accuracy at a reduced computational cost. This\nwork represents the first of its kind to explore conditioning strategies in 3D\ndata segmentation, offering a novel and more efficient method for leveraging\nannotated data. Our code, pre-trained models are publicly available at\nhttps://github.com/SanoScience/DeCode .",
        "updated": "2024-07-12 17:14:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09437v1"
    }
]