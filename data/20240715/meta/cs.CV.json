[
    {
        "title": "Adaptive Prediction Ensemble: Improving Out-of-Distribution Generalization of Motion Forecasting",
        "authors": "Jinning LiJiachen LiSangjae BaeDavid Isele",
        "links": "http://arxiv.org/abs/2407.09475v1",
        "entry_id": "http://arxiv.org/abs/2407.09475v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09475v1",
        "summary": "Deep learning-based trajectory prediction models for autonomous driving often\nstruggle with generalization to out-of-distribution (OOD) scenarios, sometimes\nperforming worse than simple rule-based models. To address this limitation, we\npropose a novel framework, Adaptive Prediction Ensemble (APE), which integrates\ndeep learning and rule-based prediction experts. A learned routing function,\ntrained concurrently with the deep learning model, dynamically selects the most\nreliable prediction based on the input scenario. Our experiments on large-scale\ndatasets, including Waymo Open Motion Dataset (WOMD) and Argoverse, demonstrate\nimprovement in zero-shot generalization across datasets. We show that our\nmethod outperforms individual prediction models and other variants,\nparticularly in long-horizon prediction and scenarios with a high proportion of\nOOD data. This work highlights the potential of hybrid approaches for robust\nand generalizable motion prediction in autonomous driving.",
        "updated": "2024-07-12 17:57:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09475v1"
    },
    {
        "title": "StyleSplat: 3D Object Style Transfer with Gaussian Splatting",
        "authors": "Sahil JainAvik KuthialaPrabhdeep Singh SethiPrakanshul Saxena",
        "links": "http://arxiv.org/abs/2407.09473v1",
        "entry_id": "http://arxiv.org/abs/2407.09473v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09473v1",
        "summary": "Recent advancements in radiance fields have opened new avenues for creating\nhigh-quality 3D assets and scenes. Style transfer can enhance these 3D assets\nwith diverse artistic styles, transforming creative expression. However,\nexisting techniques are often slow or unable to localize style transfer to\nspecific objects. We introduce StyleSplat, a lightweight method for stylizing\n3D objects in scenes represented by 3D Gaussians from reference style images.\nOur approach first learns a photorealistic representation of the scene using 3D\nGaussian splatting while jointly segmenting individual 3D objects. We then use\na nearest-neighbor feature matching loss to finetune the Gaussians of the\nselected objects, aligning their spherical harmonic coefficients with the style\nimage to ensure consistency and visual appeal. StyleSplat allows for quick,\ncustomizable style transfer and localized stylization of multiple objects\nwithin a scene, each with a different style. We demonstrate its effectiveness\nacross various 3D scenes and styles, showcasing enhanced control and\ncustomization in 3D creation.",
        "updated": "2024-07-12 17:55:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09473v1"
    },
    {
        "title": "Let Me DeCode You: Decoder Conditioning with Tabular Data",
        "authors": "Tomasz SzczepańskiMichal K. GrzeszczykSzymon PłotkaArleta AdamowiczPiotr FudalejPrzemysław KorzeniowskiTomasz TrzcińskiArkadiusz Sitek",
        "links": "http://arxiv.org/abs/2407.09437v1",
        "entry_id": "http://arxiv.org/abs/2407.09437v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09437v1",
        "summary": "Training deep neural networks for 3D segmentation tasks can be challenging,\noften requiring efficient and effective strategies to improve model\nperformance. In this study, we introduce a novel approach, DeCode, that\nutilizes label-derived features for model conditioning to support the decoder\nin the reconstruction process dynamically, aiming to enhance the efficiency of\nthe training process. DeCode focuses on improving 3D segmentation performance\nthrough the incorporation of conditioning embedding with learned numerical\nrepresentation of 3D-label shape features. Specifically, we develop an\napproach, where conditioning is applied during the training phase to guide the\nnetwork toward robust segmentation. When labels are not available during\ninference, our model infers the necessary conditioning embedding directly from\nthe input data, thanks to a feed-forward network learned during the training\nphase. This approach is tested using synthetic data and cone-beam computed\ntomography (CBCT) images of teeth. For CBCT, three datasets are used: one\npublicly available and two in-house. Our results show that DeCode significantly\noutperforms traditional, unconditioned models in terms of generalization to\nunseen data, achieving higher accuracy at a reduced computational cost. This\nwork represents the first of its kind to explore conditioning strategies in 3D\ndata segmentation, offering a novel and more efficient method for leveraging\nannotated data. Our code, pre-trained models are publicly available at\nhttps://github.com/SanoScience/DeCode .",
        "updated": "2024-07-12 17:14:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09437v1"
    },
    {
        "title": "Rethinking temporal self-similarity for repetitive action counting",
        "authors": "Yanan LuoJinhui YiYazan Abu FarhaMoritz WolterJuergen Gall",
        "links": "http://arxiv.org/abs/2407.09431v1",
        "entry_id": "http://arxiv.org/abs/2407.09431v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09431v1",
        "summary": "Counting repetitive actions in long untrimmed videos is a challenging task\nthat has many applications such as rehabilitation. State-of-the-art methods\npredict action counts by first generating a temporal self-similarity matrix\n(TSM) from the sampled frames and then feeding the matrix to a predictor\nnetwork. The self-similarity matrix, however, is not an optimal input to a\nnetwork since it discards too much information from the frame-wise embeddings.\nWe thus rethink how a TSM can be utilized for counting repetitive actions and\npropose a framework that learns embeddings and predicts action start\nprobabilities at full temporal resolution. The number of repeated actions is\nthen inferred from the action start probabilities. In contrast to current\napproaches that have the TSM as an intermediate representation, we propose a\nnovel loss based on a generated reference TSM, which enforces that the\nself-similarity of the learned frame-wise embeddings is consistent with the\nself-similarity of repeated actions. The proposed framework achieves\nstate-of-the-art results on three datasets, i.e., RepCount, UCFRep, and\nCountix.",
        "updated": "2024-07-12 17:03:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09431v1"
    },
    {
        "title": "SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers",
        "authors": "Shraman PramanickRama ChellappaSubhashini Venugopalan",
        "links": "http://arxiv.org/abs/2407.09413v1",
        "entry_id": "http://arxiv.org/abs/2407.09413v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09413v1",
        "summary": "Seeking answers to questions within long scientific research articles is a\ncrucial area of study that aids readers in quickly addressing their inquiries.\nHowever, existing question-answering (QA) datasets based on scientific papers\nare limited in scale and focus solely on textual content. To address this\nlimitation, we introduce SPIQA (Scientific Paper Image Question Answering), the\nfirst large-scale QA dataset specifically designed to interpret complex figures\nand tables within the context of scientific research articles across various\ndomains of computer science. Leveraging the breadth of expertise and ability of\nmultimodal large language models (MLLMs) to understand figures, we employ\nautomatic and manual curation to create the dataset. We craft an\ninformation-seeking task involving multiple images that cover a wide variety of\nplots, charts, tables, schematic diagrams, and result visualizations. SPIQA\ncomprises 270K questions divided into training, validation, and three different\nevaluation splits. Through extensive experiments with 12 prominent foundational\nmodels, we evaluate the ability of current multimodal systems to comprehend the\nnuanced aspects of research articles. Additionally, we propose a\nChain-of-Thought (CoT) evaluation strategy with in-context retrieval that\nallows fine-grained, step-by-step assessment and improves model performance. We\nfurther explore the upper bounds of performance enhancement with additional\ntextual information, highlighting its promising potential for future research\nand the dataset's impact on revolutionizing how we interact with scientific\nliterature.",
        "updated": "2024-07-12 16:37:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09413v1"
    }
]