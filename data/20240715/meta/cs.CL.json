[
    {
        "title": "Weight Block Sparsity: Training, Compilation, and AI Engine Accelerators",
        "authors": "Paolo D'AlbertoTaehee JeongAkshai JainShreyas ManjunathMrinal SarmahSamuel Hsu Yaswanth RapartiNitesh Pipralia",
        "links": "http://arxiv.org/abs/2407.09453v1",
        "entry_id": "http://arxiv.org/abs/2407.09453v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09453v1",
        "summary": "Nowadays, increasingly larger Deep Neural Networks (DNNs) are being\ndeveloped, trained, and utilized. These networks require significant\ncomputational resources, putting a strain on both advanced and limited devices.\nOur solution is to implement {\\em weight block sparsity}, which is a structured\nsparsity that is friendly to hardware. By zeroing certain sections of the\nconvolution and fully connected layers parameters of pre-trained DNN models, we\ncan efficiently speed up the DNN's inference process. This results in a smaller\nmemory footprint, faster communication, and fewer operations.\n  Our work presents a vertical system that allows for the training of\nconvolution and matrix multiplication weights to exploit 8x8 block sparsity on\na single GPU within a reasonable amount of time. Compilers recognize this\nsparsity and use it for both data compaction and computation splitting into\nthreads. Blocks like these take full advantage of both spatial and temporal\nlocality, paving the way for fast vector operations and memory reuse. By using\nthis system on a Resnet50 model, we were able to reduce the weight by half with\nminimal accuracy loss, resulting in a two-times faster inference speed. We will\npresent performance estimates using accurate and complete code generation for\nAIE2 configuration sets (AMD Versal FPGAs) with Resnet50, Inception V3, and\nVGG16 to demonstrate the necessary synergy between hardware overlay designs and\nsoftware stacks for compiling and executing machine learning applications.",
        "updated": "2024-07-12 17:37:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09453v1"
    },
    {
        "title": "Human-like Episodic Memory for Infinite Context LLMs",
        "authors": "Zafeirios FountasMartin A BenfeghoulAdnan OomerjeeFenia ChristopoulouGerasimos LampourasHaitham Bou-AmmarJun Wang",
        "links": "http://arxiv.org/abs/2407.09450v1",
        "entry_id": "http://arxiv.org/abs/2407.09450v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09450v1",
        "summary": "Large language models (LLMs) have shown remarkable capabilities, but still\nstruggle with processing extensive contexts, limiting their ability to maintain\ncoherence and accuracy over long sequences. In contrast, the human brain excels\nat organising and retrieving episodic experiences across vast temporal scales,\nspanning a lifetime. In this work, we introduce EM-LLM, a novel approach that\nintegrates key aspects of human episodic memory and event cognition into LLMs,\nenabling them to effectively handle practically infinite context lengths while\nmaintaining computational efficiency. EM-LLM organises sequences of tokens into\ncoherent episodic events using a combination of Bayesian surprise and\ngraph-theoretic boundary refinement in an on-line fashion. When needed, these\nevents are retrieved through a two-stage memory process, combining\nsimilarity-based and temporally contiguous retrieval for efficient and\nhuman-like access to relevant information. Experiments on the LongBench dataset\ndemonstrate EM-LLM's superior performance, outperforming the state-of-the-art\nInfLLM model with an overall relative improvement of 4.3% across various tasks,\nincluding a 33% improvement on the PassageRetrieval task. Furthermore, our\nanalysis reveals strong correlations between EM-LLM's event segmentation and\nhuman-perceived events, suggesting a bridge between this artificial system and\nits biological counterpart. This work not only advances LLM capabilities in\nprocessing extended contexts but also provides a computational framework for\nexploring human memory mechanisms, opening new avenues for interdisciplinary\nresearch in AI and cognitive science.",
        "updated": "2024-07-12 17:34:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09450v1"
    },
    {
        "title": "ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to Identify Likely Toxic Prompts",
        "authors": "Amelia F. HardyHoujun LiuBernard LangeMykel J. Kochenderfer",
        "links": "http://arxiv.org/abs/2407.09447v1",
        "entry_id": "http://arxiv.org/abs/2407.09447v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09447v1",
        "summary": "Typical schemes for automated red-teaming large language models (LLMs) focus\non discovering prompts that trigger a frozen language model (the defender) to\ngenerate toxic text. This often results in the prompting model (the adversary)\nproducing text that is unintelligible and unlikely to arise. Here, we propose a\nreinforcement learning formulation of the LLM red-teaming task which allows us\nto discover prompts that both (1) trigger toxic outputs from a frozen defender\nand (2) have low perplexity as scored by the defender. We argue these cases are\nmost pertinent in a red-teaming setting because of their likelihood to arise\nduring normal use of the defender model. We solve this formulation through a\nnovel online and weakly supervised variant of Identity Preference Optimization\n(IPO) on GPT-2 and GPT-2 XL defenders. We demonstrate that our policy is\ncapable of generating likely prompts that also trigger toxicity. Finally, we\nqualitatively analyze learned strategies, trade-offs of likelihood and\ntoxicity, and discuss implications. Source code is available for this project\nat: https://github.com/sisl/ASTPrompter/.",
        "updated": "2024-07-12 17:33:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09447v1"
    },
    {
        "title": "Open (Clinical) LLMs are Sensitive to Instruction Phrasings",
        "authors": "Alberto Mario Ceballos ArroyoMonica MunnangiJiuding SunKaren Y. C. ZhangDenis Jered McInerneyByron C. WallaceSilvio Amir",
        "links": "http://arxiv.org/abs/2407.09429v1",
        "entry_id": "http://arxiv.org/abs/2407.09429v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09429v1",
        "summary": "Instruction-tuned Large Language Models (LLMs) can perform a wide range of\ntasks given natural language instructions to do so, but they are sensitive to\nhow such instructions are phrased. This issue is especially concerning in\nhealthcare, as clinicians are unlikely to be experienced prompt engineers and\nthe potential consequences of inaccurate outputs are heightened in this domain.\n  This raises a practical question: How robust are instruction-tuned LLMs to\nnatural variations in the instructions provided for clinical NLP tasks? We\ncollect prompts from medical doctors across a range of tasks and quantify the\nsensitivity of seven LLMs -- some general, others specialized -- to natural\n(i.e., non-adversarial) instruction phrasings. We find that performance varies\nsubstantially across all models, and that -- perhaps surprisingly --\ndomain-specific models explicitly trained on clinical data are especially\nbrittle, compared to their general domain counterparts. Further, arbitrary\nphrasing differences can affect fairness, e.g., valid but distinct instructions\nfor mortality prediction yield a range both in overall performance, and in\nterms of differences between demographic groups.",
        "updated": "2024-07-12 17:00:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09429v1"
    },
    {
        "title": "Mitigating Entity-Level Hallucination in Large Language Models",
        "authors": "Weihang SuYichen TangQingyao AiChangyue WangZhijing WuYiqun Liu",
        "links": "http://arxiv.org/abs/2407.09417v1",
        "entry_id": "http://arxiv.org/abs/2407.09417v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09417v1",
        "summary": "The emergence of Large Language Models (LLMs) has revolutionized how users\naccess information, shifting from traditional search engines to direct\nquestion-and-answer interactions with LLMs. However, the widespread adoption of\nLLMs has revealed a significant challenge known as hallucination, wherein LLMs\ngenerate coherent yet factually inaccurate responses. This hallucination\nphenomenon has led to users' distrust in information retrieval systems based on\nLLMs. To tackle this challenge, this paper proposes Dynamic Retrieval\nAugmentation based on hallucination Detection (DRAD) as a novel method to\ndetect and mitigate hallucinations in LLMs. DRAD improves upon traditional\nretrieval augmentation by dynamically adapting the retrieval process based on\nreal-time hallucination detection. It features two main components: Real-time\nHallucination Detection (RHD) for identifying potential hallucinations without\nexternal models, and Self-correction based on External Knowledge (SEK) for\ncorrecting these errors using external knowledge. Experiment results show that\nDRAD demonstrates superior performance in both detecting and mitigating\nhallucinations in LLMs. All of our code and data are open-sourced at\nhttps://github.com/oneal2000/EntityHallucination.",
        "updated": "2024-07-12 16:47:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09417v1"
    }
]