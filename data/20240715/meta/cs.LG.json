[
    {
        "title": "Adaptive Prediction Ensemble: Improving Out-of-Distribution Generalization of Motion Forecasting",
        "authors": "Jinning LiJiachen LiSangjae BaeDavid Isele",
        "links": "http://arxiv.org/abs/2407.09475v1",
        "entry_id": "http://arxiv.org/abs/2407.09475v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09475v1",
        "summary": "Deep learning-based trajectory prediction models for autonomous driving often\nstruggle with generalization to out-of-distribution (OOD) scenarios, sometimes\nperforming worse than simple rule-based models. To address this limitation, we\npropose a novel framework, Adaptive Prediction Ensemble (APE), which integrates\ndeep learning and rule-based prediction experts. A learned routing function,\ntrained concurrently with the deep learning model, dynamically selects the most\nreliable prediction based on the input scenario. Our experiments on large-scale\ndatasets, including Waymo Open Motion Dataset (WOMD) and Argoverse, demonstrate\nimprovement in zero-shot generalization across datasets. We show that our\nmethod outperforms individual prediction models and other variants,\nparticularly in long-horizon prediction and scenarios with a high proportion of\nOOD data. This work highlights the potential of hybrid approaches for robust\nand generalizable motion prediction in autonomous driving.",
        "updated": "2024-07-12 17:57:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09475v1"
    },
    {
        "title": "Beyond Euclid: An Illustrated Guide to Modern Machine Learning with Geometric, Topological, and Algebraic Structures",
        "authors": "Sophia SanbornJohan MatheMathilde PapillonDomas BuracasHansen J LillemarkChristian ShewmakeAbby BerticsXavier PennecNina Miolane",
        "links": "http://arxiv.org/abs/2407.09468v1",
        "entry_id": "http://arxiv.org/abs/2407.09468v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09468v1",
        "summary": "The enduring legacy of Euclidean geometry underpins classical machine\nlearning, which, for decades, has been primarily developed for data lying in\nEuclidean space. Yet, modern machine learning increasingly encounters richly\nstructured data that is inherently nonEuclidean. This data can exhibit\nintricate geometric, topological and algebraic structure: from the geometry of\nthe curvature of space-time, to topologically complex interactions between\nneurons in the brain, to the algebraic transformations describing symmetries of\nphysical systems. Extracting knowledge from such non-Euclidean data\nnecessitates a broader mathematical perspective. Echoing the 19th-century\nrevolutions that gave rise to non-Euclidean geometry, an emerging line of\nresearch is redefining modern machine learning with non-Euclidean structures.\nIts goal: generalizing classical methods to unconventional data types with\ngeometry, topology, and algebra. In this review, we provide an accessible\ngateway to this fast-growing field and propose a graphical taxonomy that\nintegrates recent advances into an intuitive unified framework. We subsequently\nextract insights into current challenges and highlight exciting opportunities\nfor future development in this field.",
        "updated": "2024-07-12 17:48:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09468v1"
    },
    {
        "title": "Weight Block Sparsity: Training, Compilation, and AI Engine Accelerators",
        "authors": "Paolo D'AlbertoTaehee JeongAkshai JainShreyas ManjunathMrinal SarmahSamuel Hsu Yaswanth RapartiNitesh Pipralia",
        "links": "http://arxiv.org/abs/2407.09453v1",
        "entry_id": "http://arxiv.org/abs/2407.09453v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09453v1",
        "summary": "Nowadays, increasingly larger Deep Neural Networks (DNNs) are being\ndeveloped, trained, and utilized. These networks require significant\ncomputational resources, putting a strain on both advanced and limited devices.\nOur solution is to implement {\\em weight block sparsity}, which is a structured\nsparsity that is friendly to hardware. By zeroing certain sections of the\nconvolution and fully connected layers parameters of pre-trained DNN models, we\ncan efficiently speed up the DNN's inference process. This results in a smaller\nmemory footprint, faster communication, and fewer operations.\n  Our work presents a vertical system that allows for the training of\nconvolution and matrix multiplication weights to exploit 8x8 block sparsity on\na single GPU within a reasonable amount of time. Compilers recognize this\nsparsity and use it for both data compaction and computation splitting into\nthreads. Blocks like these take full advantage of both spatial and temporal\nlocality, paving the way for fast vector operations and memory reuse. By using\nthis system on a Resnet50 model, we were able to reduce the weight by half with\nminimal accuracy loss, resulting in a two-times faster inference speed. We will\npresent performance estimates using accurate and complete code generation for\nAIE2 configuration sets (AMD Versal FPGAs) with Resnet50, Inception V3, and\nVGG16 to demonstrate the necessary synergy between hardware overlay designs and\nsoftware stacks for compiling and executing machine learning applications.",
        "updated": "2024-07-12 17:37:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09453v1"
    },
    {
        "title": "Human-like Episodic Memory for Infinite Context LLMs",
        "authors": "Zafeirios FountasMartin A BenfeghoulAdnan OomerjeeFenia ChristopoulouGerasimos LampourasHaitham Bou-AmmarJun Wang",
        "links": "http://arxiv.org/abs/2407.09450v1",
        "entry_id": "http://arxiv.org/abs/2407.09450v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09450v1",
        "summary": "Large language models (LLMs) have shown remarkable capabilities, but still\nstruggle with processing extensive contexts, limiting their ability to maintain\ncoherence and accuracy over long sequences. In contrast, the human brain excels\nat organising and retrieving episodic experiences across vast temporal scales,\nspanning a lifetime. In this work, we introduce EM-LLM, a novel approach that\nintegrates key aspects of human episodic memory and event cognition into LLMs,\nenabling them to effectively handle practically infinite context lengths while\nmaintaining computational efficiency. EM-LLM organises sequences of tokens into\ncoherent episodic events using a combination of Bayesian surprise and\ngraph-theoretic boundary refinement in an on-line fashion. When needed, these\nevents are retrieved through a two-stage memory process, combining\nsimilarity-based and temporally contiguous retrieval for efficient and\nhuman-like access to relevant information. Experiments on the LongBench dataset\ndemonstrate EM-LLM's superior performance, outperforming the state-of-the-art\nInfLLM model with an overall relative improvement of 4.3% across various tasks,\nincluding a 33% improvement on the PassageRetrieval task. Furthermore, our\nanalysis reveals strong correlations between EM-LLM's event segmentation and\nhuman-perceived events, suggesting a bridge between this artificial system and\nits biological counterpart. This work not only advances LLM capabilities in\nprocessing extended contexts but also provides a computational framework for\nexploring human memory mechanisms, opening new avenues for interdisciplinary\nresearch in AI and cognitive science.",
        "updated": "2024-07-12 17:34:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09450v1"
    },
    {
        "title": "The $μ\\mathcal{G}$ Language for Programming Graph Neural Networks",
        "authors": "Matteo BelenchiaFlavio CorradiniMichela QuadriniMichele Loreti",
        "links": "http://arxiv.org/abs/2407.09441v1",
        "entry_id": "http://arxiv.org/abs/2407.09441v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09441v1",
        "summary": "Graph neural networks form a class of deep learning architectures\nspecifically designed to work with graph-structured data. As such, they share\nthe inherent limitations and problems of deep learning, especially regarding\nthe issues of explainability and trustworthiness. We propose $\\mu\\mathcal{G}$,\nan original domain-specific language for the specification of graph neural\nnetworks that aims to overcome these issues. The language's syntax is\nintroduced, and its meaning is rigorously defined by a denotational semantics.\nAn equivalent characterization in the form of an operational semantics is also\nprovided and, together with a type system, is used to prove the type soundness\nof $\\mu\\mathcal{G}$. We show how $\\mu\\mathcal{G}$ programs can be represented\nin a more user-friendly graphical visualization, and provide examples of its\ngenerality by showing how it can be used to define some of the most popular\ngraph neural network models, or to develop any custom graph processing\napplication.",
        "updated": "2024-07-12 17:27:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09441v1"
    }
]