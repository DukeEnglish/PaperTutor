[
    {
        "title": "Meta-Analysis with Untrusted Data",
        "authors": "Shiva KaulGeoffrey J. Gordon",
        "links": "http://arxiv.org/abs/2407.09387v1",
        "entry_id": "http://arxiv.org/abs/2407.09387v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09387v1",
        "summary": "[See paper for full abstract] Meta-analysis is a crucial tool for answering\nscientific questions. It is usually conducted on a relatively small amount of\n``trusted'' data -- ideally from randomized, controlled trials -- which allow\ncausal effects to be reliably estimated with minimal assumptions. We show how\nto answer causal questions much more precisely by making two changes. First, we\nincorporate untrusted data drawn from large observational databases, related\nscientific literature and practical experience -- without sacrificing rigor or\nintroducing strong assumptions. Second, we train richer models capable of\nhandling heterogeneous trials, addressing a long-standing challenge in\nmeta-analysis. Our approach is based on conformal prediction, which\nfundamentally produces rigorous prediction intervals, but doesn't handle\nindirect observations: in meta-analysis, we observe only noisy effects due to\nthe limited number of participants in each trial. To handle noise, we develop a\nsimple, efficient version of fully-conformal kernel ridge regression, based on\na novel condition called idiocentricity. We introduce noise-correcting terms in\nthe residuals and analyze their interaction with a ``variance shaving''\ntechnique. In multiple experiments on healthcare datasets, our algorithms\ndeliver tighter, sounder intervals than traditional ones. This paper charts a\nnew course for meta-analysis and evidence-based medicine, where heterogeneity\nand untrusted data are embraced for more nuanced and precise predictions.",
        "updated": "2024-07-12 16:07:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09387v1"
    },
    {
        "title": "Graph Neural Network Causal Explanation via Neural Causal Models",
        "authors": "Arman BehnamBinghui Wang",
        "links": "http://arxiv.org/abs/2407.09378v1",
        "entry_id": "http://arxiv.org/abs/2407.09378v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09378v1",
        "summary": "Graph neural network (GNN) explainers identify the important subgraph that\nensures the prediction for a given graph. Until now, almost all GNN explainers\nare based on association, which is prone to spurious correlations. We propose\n{\\name}, a GNN causal explainer via causal inference. Our explainer is based on\nthe observation that a graph often consists of a causal underlying subgraph.\n{\\name} includes three main steps: 1) It builds causal structure and the\ncorresponding structural causal model (SCM) for a graph, which enables the\ncause-effect calculation among nodes. 2) Directly calculating the cause-effect\nin real-world graphs is computationally challenging. It is then enlightened by\nthe recent neural causal model (NCM), a special type of SCM that is trainable,\nand design customized NCMs for GNNs. By training these GNN NCMs, the\ncause-effect can be easily calculated. 3) It uncovers the subgraph that\ncausally explains the GNN predictions via the optimized GNN-NCMs. Evaluation\nresults on multiple synthetic and real-world graphs validate that {\\name}\nsignificantly outperforms existing GNN explainers in exact groundtruth\nexplanation identification",
        "updated": "2024-07-12 15:56:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09378v1"
    },
    {
        "title": "HiPPO-Prophecy: State-Space Models can Provably Learn Dynamical Systems in Context",
        "authors": "Federico Arangath JosephKilian HaefeliNoah LinigerCaglar Gulcehre",
        "links": "http://arxiv.org/abs/2407.09375v1",
        "entry_id": "http://arxiv.org/abs/2407.09375v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09375v1",
        "summary": "This work explores the in-context learning capabilities of State Space Models\n(SSMs) and presents, to the best of our knowledge, the first theoretical\nexplanation of a possible underlying mechanism. We introduce a novel weight\nconstruction for SSMs, enabling them to predict the next state of any dynamical\nsystem after observing previous states without parameter fine-tuning. This is\naccomplished by extending the HiPPO framework to demonstrate that continuous\nSSMs can approximate the derivative of any input signal. Specifically, we find\nan explicit weight construction for continuous SSMs and provide an asymptotic\nerror bound on the derivative approximation. The discretization of this\ncontinuous SSM subsequently yields a discrete SSM that predicts the next state.\nFinally, we demonstrate the effectiveness of our parameterization empirically.\nThis work should be an initial step toward understanding how sequence models\nbased on SSMs learn in context.",
        "updated": "2024-07-12 15:56:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09375v1"
    },
    {
        "title": "Learning Distances from Data with Normalizing Flows and Score Matching",
        "authors": "Peter SorrensonDaniel Behrend-UriarteChristoph SchnörrUllrich Köthe",
        "links": "http://arxiv.org/abs/2407.09297v1",
        "entry_id": "http://arxiv.org/abs/2407.09297v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09297v1",
        "summary": "Density-based distances (DBDs) offer an elegant solution to the problem of\nmetric learning. By defining a Riemannian metric which increases with\ndecreasing probability density, shortest paths naturally follow the data\nmanifold and points are clustered according to the modes of the data. We show\nthat existing methods to estimate Fermat distances, a particular choice of DBD,\nsuffer from poor convergence in both low and high dimensions due to i)\ninaccurate density estimates and ii) reliance on graph-based paths which are\nincreasingly rough in high dimensions. To address these issues, we propose\nlearning the densities using a normalizing flow, a generative model with\ntractable density estimation, and employing a smooth relaxation method using a\nscore model initialized from a graph-based proposal. Additionally, we introduce\na dimension-adapted Fermat distance that exhibits more intuitive behavior when\nscaled to high dimensions and offers better numerical properties. Our work\npaves the way for practical use of density-based distances, especially in\nhigh-dimensional spaces.",
        "updated": "2024-07-12 14:30:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09297v1"
    },
    {
        "title": "Variational Inference via Smoothed Particle Hydrodynamics",
        "authors": "Yongchao Huang",
        "links": "http://arxiv.org/abs/2407.09186v1",
        "entry_id": "http://arxiv.org/abs/2407.09186v1",
        "pdf_url": "http://arxiv.org/pdf/2407.09186v1",
        "summary": "A new variational inference method, SPH-ParVI, based on smoothed particle\nhydrodynamics (SPH), is proposed for sampling partially known densities (e.g.\nup to a constant) or sampling using gradients. SPH-ParVI simulates the flow of\na fluid under external effects driven by the target density; transient or\nsteady state of the fluid approximates the target density. The continuum fluid\nis modelled as an interacting particle system (IPS) via SPH, where each\nparticle carries smoothed properties, interacts and evolves as per the\nNavier-Stokes equations. This mesh-free, Lagrangian simulation method offers\nfast, flexible, scalable and deterministic sampling and inference for a class\nof probabilistic models such as those encountered in Bayesian inference and\ngenerative modelling.",
        "updated": "2024-07-12 11:38:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.09186v1"
    }
]