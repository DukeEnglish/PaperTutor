GNN with Model-based RL for Multi-agent Systems
HanxiaoChen,HarbinInstituteofTechnology,hanxiaochen@hit.edu.cn
Abstract. Multi-agentsystems(MAS)constituteasignificantroleinexploring
machineintelligence and advanced applications. In order to deeply investigate
complicated interactions within MAS scenarios, we originally propose “GNN
for MBRL” model, which utilizes astate-spaced Graph Neural Networks with
Model-based Reinforcement Learning to address specific MAS missions(e.g.,
Billiard-Avoidance,AutonomousDrivingCars).Indetail,wefirstlyusedGNN
model to predict future states and trajectories of multiple agents, then applied
theCross-EntropyMethod(CEM)optimizedModelPredictiveControltoassist
theego-agentplanningactionsandsuccessfullyaccomplishcertainMAStasks.
Keywords:Multi-agentSystems,GraphNeuralNetworks,CrossEntropy
Method,ModelPredictiveControl
1 Introduction
1.1 Purpose
Vision-based mechanisms have been explored well in diverse reinforcement learning
applicationssuchasplayingnovel Atarivideogamesfromrawpixels[1],controlling
simulated autonomous vehicles with high-dimensional image observations [2] and
completing robotic manipulation tasks (e.g., grasping, door-opening) based on state
representations extracted from complicated vision data [3, 4]. However, it has been
empiricallyobserved that reinforcement learningfrom high dimensional observations
such as raw pixels is sample-inefficient [5] and time-consuming. Furthermore, it's
widely accepted that learning policies from physical state based features is
significantlymoreefficientandexplicitthanlearningfromvisual pixels.Thus,inthis
research project we focused on learning control policies from states and explored to
apply a graph neural network dynamics model to predict future states in multi-agent
systems, then utilize the Cross-Entropy Method optimized model-based controller to
implement motion planning for the ego-agent and successfully accomplish specific
MASmissions,suchasmulti-billiardavoidanceorself-drivingcarscenarios(Fig.1).
Fig.1.Gym-billiard&Gym-CarlaEnvironments.1.2 Background
Inspired by [6], which presents STOVE, a creative state-space model for videos that
explicitly reasons about multi-objectsand their positions, velocities, and interactions,
our program aims to curate another “GNN for MBRL” model based on the proposed
multi-billiard simulator (Fig. 1(a)) for sample efficient model-based control in MAS
taskswithheavilyinteractingagents.Obviously,autonomousdrivingisacomplicated
multi-agent system that requires the ego-agent to consider situations of surrounding
agents and then conduct further motion planning. For this application scenario, gym-
carla(Fig.1(b))providedby[7]canbeutilizedfordeeperexploration.Therefore,we
firstly design and validate our “GNN for MBRL” model on MAS billiard avoidance
scenariowhichexploresmorepossibilitiesofgraphneuralnetworksandmodel-based
reinforcementlearning,thentrytotransfersuchapromisingframeworktoreal-world
complicatedself-drivingapplications.
GraphNeuralNetworks.GNNisproposedtoestablishrepresentationsofnodesand
edgesingraphdata,evendemonstratednotablesuccessesinmultipleapplicationslike
recommended systems, social network prediction, and natural language processing.
Having observed significant potentials of GNN utilized in physics systems, we could
perform GNN-based reasoning on objects and relations in a simplified but effective
waybyrepresentingobjectsasnodesandrelationsasedges.
One successful GNN instance is the novel state-space model STOVE [6],which
is constructed by an image reconstruction model SuPAIR [8] and a GNN dynamics
model for inference, accelerating and regularizing training to push the limits of
unsupervisedlearningofphysicalinteractions.Thustwocomponentsarecombinedin
acompositionalmannertoyieldthestate-spacepredictivemodel:
p(x,z) p(z )p(x |z ) p(z |z )p(x |z ) (1)
0 0 0 t t t1 t t
where x means image observation, zdenotes object states. And the interface between
twocomponentsarethelatentpositionsandvelocitiesofmultipleagents.Toinitialize
states, they model p(z ,z)using simple uniform and Gaussian distributions. STOVE
0 1
modelistrainedongivenvideosequencesxbymaximizingtheevidencelowerbound
(ELBO): [logp(x,z)logq(z|x)].
q(z|x)
Exceptforbasicvideomodelingandprediction,STOVEextendstheirstructured
and object-aware video model into reinforcement learning (RL) tasks, allowing it to
be utilized for search or planning. According to their empirical evidence, an actor
basedonMonte-Carlotreesearch(MCTS)ontopofSTOVEiscompetitivetomodel-
freeapproachessuchasProximalPolicyOptimization(PPO)onlyrequiringafraction
ofsamples.InspiredbysuchRLexperiments,weconsideredtoapplytheGNNmodel
directly on states instead of high-dimensional visual data to improve the sample
efficiency and predict agents' future states well, then combine it with another model-
based RL method such as Model Predictive Control (MPC). Since our motivation is
learningtheGNNdynamicsmodelwithlow-levelstates,intheexperimentwedesign
to train our model with ground truth states of video sequence data for multi-agent
systems instead of inefficient visual data which needs to be firstly reconstructed by
SuPAIRmodeltoextractstatesintheoriginalSTOVE.Model-basedReinforcementLearning.Model-basedRL haslong beenviewed asa
potential remedy to the prohibitive sample complexity of model-free RL. Formally,
model-based RL [9] consists of two primary steps: (1) Learning a dynamics model
p(s |s,a)which can generate future states based on current states and actions. (2)
t1 t
Conducting the planning algorithm to learn a global policy or value function and act
the environment well. STOVE utilized Monte-Carlo tree search (MCTS) to obtain a
policybasedonthe world modelwhich isleveragedasasimulator forplanning.And
they discovered MCTS combined STOVE could exactly outperform the model-free
PPOalgorithminthemulti-billiardsavoidancetask.
2 Method
2.1 Framework
Fig.2intuitivelyexplainstwoimportantstagesinour“GNNforMBRL”method:(1)
GNNdynamicsmodeltrainingstage,whichwillbeinputtheofflinerecordedvideo
sequencesdataorlow-dimensionalstatesandtrainedforvideoprediction;(2)Motion
planning stage with CEM-based Model Predictive Control (MPC), that we utilize a
feed-back control algorithm integrated with a Cross-Entropy Method optimizer to
interact with the billiard environment and plan reasonable actions to control the ego-
agentwithpurposeofavoidingcollisionsbetweenotheragents.
Also, we have marked two different cases in the GNN dynamics training stage,
one is the “Action-conditioned case” belonging to the original STOVE model-based
control approach that trains GNN align with an object reconstruction SuPAIR model
on visual observation data, another one is “Supervised RL case” that we primarily
proposedtoaddressRLtasksdirectlyonlow-levelstateswithouttheprocessedvisual
observation information. But the framework for two cases is firstly training GNN
dynamics models for multi-agent future states prediction, then integrates the trained
GNN model into the following model-based RL section to control the ego agent for
correctmotionplanning(e.g.,avoidingbilliardcollisions,planningtraffictrajectories).
Fig.2.GNNforMBRLwholepipeline.Transferredtothe secondpart,MPCisafeedbackcontrolalgorithmthat applies
a model to make predictions about future outputs of a certain process. It possesses
severaladvantageslikehandlingmulti-inputmulti-output(MIMO)systemswithmore
constraints and it can easily incorporate future reference information into the control
problemtoimprove controller’sperformance.Therefore,we establishedacontinuous
versionofmulti-billiardenvironmentbasedonoriginaldiscreteSTOVEscenariosfor
datacollection.Andit’spossibleforustocombinethepreviouslytrainedGNNmodel
withMPCandinvestigateifthismethodcansuccessfullyaddressMAStasks.
2.2 DataGeneration
STOVE[6]proposedastructuredobject-awarephysicspredictionmodelbasedonthe
heavily interacting billiards simulation environments with diverse modes including
avoidance,billiards,gravity,andmulti-billiards(Fig.3).However,theirenvironment
script is just attached to the whole Github repository. Interested by such amazing
multi-agent systems, firstly we wrapped them into the gym environment style “gym-
billiard” (Fig. 1(a)) which can be easily imported by Python API, even assist other
researcherstounderstandthisphysicalsystemanddesignefficientalgorithms.
Fig.3.Multiplescenariosinthegym-billiardenvironment(left);Avoidancetask(right).
Toemphasize,ourprojectespeciallyfocusesontheinterestingavoidancebilliard
scenario,whichmeanstheredballservesastheego-objectandtheRLagentneedsto
control it to avoid collisions between other balls. In the original STOVE paper, the
ego-balliscontrolledbynineactions,whichcorrespondtomovinginoneoftheeight
(inter)cardinal directions and staying at rest. Also, a negative reward of -1 is given
wheneverthe redballcollideswithoneofthe others. Since STOVEtrains exactlyon
video data, so firstly we obtained the avoidance sequences datasets by applying
"generate_billiards_w_actions" function in the envs.py. Separately, we generated
1000sequencesoflength100fortraining(avoidance_train.pkl)and300sequencesof
length100fortesting(avoidance_test.pkl)witharandomactionselectionpolicy.Itis
alsopossible toapplythe Monte Carloactionpolicytogeneratevideosequence data.
Furthermore, the pixel resolution of such dataset was 32*32 and we set the mass of
theballtobe2.0with1.0radiussizeintheenvironmentconfigfile.
However, we discover that most model-based RL environments often provide
continuous actions for agents with the action space 2 rather than 9 discrete choices.
Thus, we are motivated to make changes in envs.py to get continuous action values,
where the red ball can be controlled by (1,2) dimensional numpy values ranged in (-
2,2). In this case, each value in actions means the acceleration of the ball in x and y
directions. Similar to the discrete mode, we produced continuous datasets with the
random action policy following the uniform distribution within the range of (-2,2).
Furthermore,we evenobtainedthe avoidance gifand founditperformedanexcellent
video for three interactive balls. Table 1 shows basic information (e.g., action_space,
averagerewards)oftwodifferentdatasets.Aswecansee,averagerewardsoftraining
andtestingdatainthecontinuousmodearesmallerthanthatofthediscretecondition,which means three balls have more collisions and they heavily interact with each
other in the continuous environment. More detailedly, eachpickle file restoresimage
observations, actions, states, dones, rewards for training and testing sequence data in
thedictionaryform,whichwillbeappliedinthefollowingphase.
Table1.Basiccomparisonsofthecontinuousanddiscretedatasets.
Data Action Actions AverageRewards AverageRewards
Mode space dtype oftrainingdata oftestingdata
Discrete 9 One-hotmode -17.276 -16.383
Continuous 2 Numpyarray -18.93 -18.71
2.3 GNNDynamicsModelTraining
As demonstrated in our training pipeline (Fig. 2), we intended to utilize a supervised
learning method which performs training on the ground-truth states rather than high-
dimensional image data to improve the sample efficiency, then combines the saved
GNN model with CEM optimized MPC to predict future states of billiards in the
interactiveAvoidanceenvironmentandreplanego-agent'sactions.Thenwewilltrain
two different cases on both Discrete and Continuous Avoidance datasets: (1)Action-
conditioned case, where [6] has extended STOVE to reinforcement learning (RL)
settings with two changes including a conditional prediction based on state & action,
and a reward prediction to yield the distribution p(z,r |z ,a ). (2) Supervised RL
t t t1 t1
case, where we consider the real states (batch_size, frame_number, 3, 4) just
including object positions and velocities as the input of GNN dynamics model for
physics predictions by replacing the SuPAIR-inferred states. Thus, the model can
directly learn to predict future states of multiple agents instead of firstly extracting
agentstaterepresentationfromvisualdatawithSuPAIRmodel.
2.4 GNNwithMBRL
After training and saving GNN dynamics model, we design to follow the traditional
Model-based RL pipeline to combine the trained GNN model with CEM-optimized
Model Predictive Control (MPC) approach [9] to investigate its performance on the
interactive continuous gym-billiard avoidance task. Thus, the saved GNN model is
integratedintotheModel-basedRLalgorithmlooptopredictfuturestatesofmultiple
agentsfrom the learned real transitions inthe MASenvironment, then MPCsearches
for an optimal action sequence for our ego-agent within the MAS scenario under the
learnedmodelandexecutesthefirstactionofthatsequence,discardingtheremaining
actions.Typicallythissearchisrepeatedaftereverystepinthegym-billiardscenario,
to account for any prediction errors and rewards by the model and to get feedback
fromtheinteractiveenvironment.
Forfurtheranalysisandvalidationforourproposed“GNNwithMBRL”method,
we firstly conducted experiments on discrete ball datasets with MCTS as in STOVE
[6]andsavedsomeempiricalvideos.Secondly,forthecontinuouscase,wecreatively
implemented the trained GNN dynamics model with the CEM optimized MPC and
comparetheperformanceswithrandomandground_truthsituations.3 Results
3.1 GNNTrainingResults
While collecting discrete and continuous train or test sequence data via our revised
gym-billiard API environment, we save them in pickle files which restore image
observations,actions,states,dones,rewardsformulti-billiardsinthe dictionaryform.
After that, we will train GNN dynamics models in two different cases (1) Action-
conditioned case, which follows [6] to train GNN on given video sequences with a
visualreconstructionmodelSuPAIR.(2)SupervisedRLcase,whereweconsiderreal
states (batch_size, frame_number, 3, 4) just including object positions and velocities
for multi-agents as the input of GNN dynamics model for physics predictions by
replacing the SuPAIR-inferred states. And we conduct training in two conditions for
500epochsandrestoreimportantmodelparameterfiles.
Certainly training time for the Supervised condition (~8 hours) is far less than
the Action-conditioned case (~25 hours). But the training processes for Discrete and
Continuous situations did not show too much difference. And one of our important
discoveries is that the novel GNN model can work exactly well on the new action
space 2 as for 9 discrete actions, even with no changes in the original GNN network
architecture. Thus, this GNN dynamics framework can perfectly achieve the unity
trainingforbothDiscreteandContinuousbilliardavoidanceenvironments.
Intuitively, Fig. 4 is the screenshot for the folder that we obtained after training
twocases,where"checkpoints"savedthemodelparameters,"gifs"containedthereal,
generatedreconstructedorrolloutvideos,"states"includedsomecrucialfilesofstates
orrewardsaftercertaintrainingtimesteps.Inaddition,thegenerateddatacanreferto:
https://pan.baidu.com/s/1evwHqrtVJE5EM46wJHxaZg(key:hru9).
Fig.4.Screenshotfortrainingresultsoftwocases.
Fig. 5 represents what we’ve obtained in the performance.csv of two different
cases after training. Obviously, the Action-conditioned training situation has restored
morenumericalmetricslikeelbo,reward,min_ll,v_errorthantheSupervisedRLcsv
file. Another interesting thing for both cases is that they save such logging files in a
jointtrainingandtestingstyle,whichmeansthe trainingandtestingmetricsforGNN
will appear repeatedly. From each individual Action-conditioned file, you can find
each row corresponds to different types, where "z", "z_sup", "z_dyn" belongs to the
SuPAIR image model's training results, but the "z_roll", "z_sup_roll", "z_dyn_roll"
denotetestingconsequencesfortheGNNrolloutvideosequences.Selectedfromsuchmetrics, wefocused on4differentvalues: reward,elbo,error,andv_error,since here
therewardmeanstheMSElossbetweentheGNNpredictedrewardsandgroundtruth
rewards instead of the traditional award definition. So the lower the reward is, the
model will be trained better. Elbo is very important because the GNN is trained on
givenvideosequencesxbymaximizingtheevidencelowerbound(ELBO).Errorand
v_error separately denote the position and velocity MSE loss between the predicted
andtruevalues.
Fig.5.Performance.csvpartial-shownfilesfor4differenttrainingcases.
Action-conditioned case. GNN training results for Action-conditioned condition are
presentedinFig.6andhereweshowthe"z"typeofdatatoexplaineachmetricsince
it contains the full information of "z_sup" and "z_dyn". Obviously, we discover the
reward MSE loss is decreasing in both continuous and discrete conditions, but the
continuousreward errordeclined from0.48to0,whereasthe discretefrom0.16to0.
As our GNN model trainingmotivation ismaximizing ELBO and the trainingcurves
indeedincreasethismetricsignificantlyfrom450to3600.
Fig.6.TrainingcurvesforAction-conditionedcase.
Inaddition,we payattentiontothe positionpredictionerror(Error)andvelocity
error(V_error)sincethesetwovariablesdeterminetheactualstatesofMASsystems.
Surelytheybothdecreasewhiletrainingbutwealsofindthecontinuouspositionerror
isclosetothe discreteone,butthe velocityerrorshows alargeamount ofdifference,
where the continuous V_error drops from 0.65 to 0.05 but the discrete one decreasesfrom 0.07 to 0.01. This seems a little strange but it's related to our new action space
definition: two values of (1, 2) numpy array range in (-2, 2) separately means the
accelerationinx&ydirections,soitmaycausesmuchgapordisparityinthevelocity
prediction.Accordingtothegeneratedreal,recon,androlloutgifs,wediscoveredthat
they are reasonable and the billiards in rollout videos run much slowly than others.
Generally speaking, such four metrics training curves exactly meet the criteria of a
reasonableGNNdynamicsmodelforthefollowingRLtask.
Supervised RLcase. As shown inFig. 2 pipeline, we directlyinput the ground truth
statesand actions toteachthe GNN dynamics modelpredicting futurepossible states
well. Similarly, the performance.csv records the total, prediction, and reconstruction
errors of the system iteratively for training and testing. As the Supervised RL case
(Fig. 7) does not require any image reconstruction operations on true states, the
Reconstruction_error always equals to 0. "Prediction_error" is another important
metric to evaluate the model prediction ability. Compared with the continuous case,
the discrete one performs much better since it drops the loss from 1.1 to 0.1. For the
"Total_error" which is connected to multiple factors, we find the continuous loss
keepsin a stable range but the discrete case firstly shows anapparent downtrend and
then keeps stable. To further validate the training results, we checked the generated
realandrolloutgifs.Obviously,alltherolloutvideosmaketheballsrunmuchslowly
butthe ego-redballexactlyreflectsreasonableavoidance ability.Therefore,basedon
comprehensive analysis, we considered that the trained Supervised RL model can be
utilizedforthefollowingmodel-basedreinforcementlearningcontrolphase.
Fig.7.TrainingcurvesforSupervisedRLcase.
3.2 GNNwithMBRL
In this section, we provide experimental results of the integration of GNN dynamics
modelandModel-basedcontrolmethodsonthegym-billiardavoidanceMAStask.At
first,we followthe“Model-basedControl”frameworkinSTOVE[6]ondiscreteball
datasets with MCTS and obtained qualitative videos for visualization. Secondly, for
the continuous datasets, we creatively integrated our trained GNN dynamics model
into the CEM optimized MPC method and compare their performances with random
andground_truthsituations.STOVEwithMCTS(Discrete).Wesuccessfullyre-implementedthecombinationof
STOVE with MCTS on our generated discrete datasets. Also, we’ve changed the
mass of multiple ball agents from 1.0 to 2.0 to produce two different pickle files and
respectively trained the GNN dynamics models as the proposed Action-conditioned
training cases. In detail, while running MCTS, we used the GNN model to predict
future sequences for 100 paralleled interactive gym-billiard avoidance environments
with the running length of 100, and the maximal rollout depth for each tree search
equals to 10. For this experiment, we focused on the mean collision rate of 100
environments and Fig. 8 clearly shows the calculated collision rate of each single
scenario. Also, we saved 100 interesting gifs after running MCTS to help us observe
how the red ego-ball interact with other agents. Surely, this red ball performs much
better to avoid collisions with MCTS and achieves lower mean collision rate for
paralleledenvironments.
Fig.8.STOVEwithMCTSrewarddistribution.
GNN with CEM based MPC (Continuous). To achieve our proposed “GNN with
MPC”approach,weloadthesavedsupervised-RLcaseGNNckptfilesandapplythe
model parameters for prediction. Another challenging problem for implementation is
appearedondatastructurefortheconnectionoftrainedmodelandtheMPCinterface,
we must ensure that the GNN model can exactly conduct predictions based on the
currentstateswithoutbugs.Surprisingly,weaddressedthisissuewellbycheckingthe
coderowtorow,changingthecudadeviceconfigtocpuandmanyvariables'dtypein
bothsupairvised/dynamics.pyandvideo_prediction/dynamics.py.
Aftersolvingabovetoughissues,westartedtointegratetheGNNtrainedmodels
into the classical CEM optimized MPC, and conducted experiments on two different
billiardenvironmentswith different epochs and horizons, even compute the "reward"
withother twobaselines(random+ground_truth).Here,"reward" meansthe average
collisions happening in each epoch with different horizons since we make some
changesfortheenvs.pyinmpcfoldersothatapositiverewardof1isgivenwhenever
the red ball collides with other balls. Table 2 collected different reward (similar to
collision rate) values for our GNN_MPC model with two training baselines (random
&ground_truth). To emphasize, the "random" case means that the futureactions will
begeneratedrandomlyinsteadofusingtheMPCcontrollerchosenactionstoconduct
motion planning. Whereas, the "ground_truth" denotes applying the true interaction
billiard environment to generate next states rather than via the GNN model. Another
importantthingisthatthe"m=1"versiontaskdiffersalittlefromthat"m=2"sincethe
"m=1" GNN model is initially trained on the old continuous datasets which generatethe red ball's movement not as flexible as that in the new "m=2" training data. Thus
we call it the "Old_env" to contrast with the "New_env". According to Table 2, we
discoverthatcollisionratesinthe"GNN_MPC"casearemuchlowerthan"Random",
evenmuchclosetothe"ground_truth"condition.
Table2.Reward(CollisionRate)fortwoenvswithdifferentbaselines.
Envs Epochs Horizons GNN_MPC Random Ground_truth
m=1 100 50 0.0558+0.0012 0.2790+0.025 0.0707+0.066
m=1 50 100 0.0565+0.0008 0.3543+0.0445 0.0408+0.0392
m=2 100 50 0.0648+0.001 0.2420+0.0178 0.0505+0.0480
m=2 50 100 0.0455+0.0008 0.2690+0.0350 0.0612+0.0575
In addition, we've recorded each epoch's "reward" values in the csv files for 4
cases mentioned in Table 2 and plot them together in Fig. 9. As we can see, the
plotting results correspond to our calculated collision rewards and 1.0 means that the
ego-objectcollideswithotherballsinallhorizonsofthewholeepoch.Soifthevalue
ofaveragereturnsismuchlower,themethod'sperformanceismuchbetter.Fromthis
perspective, we surprisingly find our new proposed "GNN_MPC" model ("STOVE")
exceeds the "Random" case significantly, which means the MPC controller performs
really well to choose a high-quality action. Since the results of "GNN_MPC" are
extremely close to the "Ground_truth" case, it is further indicated and validated that
ourtrained GNN dynamics modelwoksexactly well to predictfuture statesofmulti-
objectsystemsasthesametotheground_truthinteractiveenvironment.
Fig.9.GNNwithMPCexperimentalresults.4 Conclusions
In summary, we proposed “GNN for MBRL” idea which combines the novel
graphneuralnetwork(GNN)dynamicsmodelwithCEMoptimizedModelPredictive
Control (MPC) on the gym-billiard avoidance MAS task. Furthermore, we not only
conducted extensive experiments on “Action-conditioned” case as in STOVE with
MCTSviaoriginaldiscretedatasets,butalsoexploredandevaluated“SupervisedRL”
GNNdynamicsmodelwithCEMoptimizedMPConourrevisedcontinuousdatasets.
According to empirical results, we discovered that our creative idea performs well to
predictfuturevideosequencesandcontroltheego-agenttoexactlyaddresscertainRL
tasks,thusthismodelmaybeappliedandextendedonmuchcomplicatedmulti-agent
systemssuchasthegym-carlaautonomousdrivingenvironment.
References
1. HHafner D, Lillicrap T, Ba J, et al. Dream to control: Learning behaviors by latent
imagination[J].arXivpreprintarXiv:1912.01603,2019.
2. Liang X, Wang T, Yang L, et al. Cirl: Controllable imitative reinforcement learning for
vision-based self-driving[C]//Proceedings of the European Conference on Computer
Vision(ECCV).2018:584-599.
3. ChenH. Robotic manipulation with reinforcement learning, state representation learning,
and imitation learning (student abstract)[C]//Proceedings of the AAAI Conference on
ArtificialIntelligence.2021,35(18):15769-15770.
4. Chen H, Wang J, Meng M Q H. Kinova gemini: Interactive robot grasping with visual
reasoningandconversationalAI[C]//2022IEEEInternationalConferenceonRoboticsand
Biomimetics(ROBIO).IEEE,2022:129-134.
5. KaiserL,BabaeizadehM,MilosP,etal.Model-basedreinforcementlearningforatari[J].
arXivpreprintarXiv:1903.00374,2019.
6. Kossen J, Stelzner K, Hussing M, et al. Structured object-aware physics prediction for
videomodelingandplanning[J].arXivpreprintarXiv:1910.02425,2019.
7. Chen J, Li S E, Tomizuka M. Interpretable end-to-end urban autonomous driving with
latent deep reinforcement learning[J]. IEEE Transactions on Intelligent Transportation
Systems,2021.
8. Stelzner K, Peharz R, Kersting K. Faster attend-infer-repeat with tractable probabilistic
models[C]//InternationalConferenceonMachineLearning.PMLR,2019:5966-5975.
9. BharadhwajH,XieK,ShkurtiF.Model-predictivecontrolviacross-entropyandgradient-
basedoptimization[C]//LearningforDynamicsandControl.PMLR,2020:277-286.