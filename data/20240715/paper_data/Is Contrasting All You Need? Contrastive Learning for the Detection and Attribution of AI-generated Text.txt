Is Contrasting All You Need? Contrastive Learning for
the Detection and Attribution of AI-generated Text
LucioLaCavaa,*,DavideCostaa,1 andAndreaTagarellia,**
aDIMESDept.,UniversityofCalabria,87036Rende(CS),Italy
Abstract. The significant progress in the development of Large thereisagrowingriskofmisinformation,manipulation,anddecep-
LanguageModelshascontributedtoblurringthedistinctionbetween tion[51,9].WithouteffectivemeansofdistinguishingbetweenAI-
human and AI-generated text. The increasing pervasiveness of AI- generatedandhuman-generatedtext,usersmayunwittinglyconsume
generatedtextandthedifficultyindetectingitposesnewchallenges andpropagatefalseormisleadinginformation,underminingthein-
for our society. In this paper, we tackle the problem of detecting tegrityofpublicdiscourseanddecision-makingprocesses.
and attributing AI-generated text by proposing WhosAI, a triplet- Furthermore,theriseofAItextgenerationbringsethicalandso-
networkcontrastivelearningframeworkdesignedtopredictwhether cietalconcernsaboutauthorship,intellectualpropertyrights,andac-
a given input text has been generated by humans or AI and to un- countability.AsAIsystemsbecomeincreasinglyproficientatmim-
veiltheauthorshipofthetext.Unlikemostexistingapproaches,our ickinghumanlanguageandcreativity[8],questionsariseregarding
proposed framework is conceived to learn semantic similarity rep- theownershipandattributionofAI-generatedcontent.Withoutclear
resentationsfrommultiplegeneratorsatonce,thusequallyhandling guidelinesandmechanismsforidentifyingtheoriginoftext,issues
bothdetectionandattributiontasks.Furthermore,WhosAIismodel- mightalsoariseaboutplagiarism,copyrightinfringement,andlegal
agnosticandscalabletothereleaseofnewAItext-generationmodels responsibility,posingchallengestoestablishednormsinintellectual
byincorporatingtheirgeneratedinstancesintotheembeddingspace propertylawanddigitalcontentcreation.2
learnedbyourframework.ExperimentalresultsontheTuringBench
benchmarkof200Knewsarticlesshowthatourproposedframework Relatedwork. Theremarkableboostinhuman-liketextgeneration
achievesoutstandingresultsinboththeTuringTestandAuthorship performances achieved by Large Language Models (LLMs) in re-
Attributiontasks,outperformingallthemethodslistedintheTuring- cent years has determined a rising challenge in detecting whether
Benchbenchmarkleaderboards. and to what extent texts have been generated by humans or ma-
chines [21, 49, 39]. In this context, the “watermarking” paradigm
rapidly gained attention [25, 53, 28, 50], as it allows embedding
1 Introduction specific signals into generated texts that remain invisible to hu-
mansbutarealgorithmicallydetectable.Statisticallearningmethods
In recent years, advancements in artificial intelligence (AI) have
also offer advanced solutions for detecting the authorship of texts.
revolutionizedvariousdomains,includingnaturallanguageprocess-
Theseincludeprobabilisticmodels[31,1,47,14],logrankinforma-
ing (NLP), leading to the emergence of sophisticated text genera-
tion[38],perplexity[44],discoursemotifs[24],andotherstatistical
tion models. These AI-powered systems are capable of generating
approaches[16,40,45]
human-like text, ranging from simple sentences to complex narra-
Morerecently,wehavewitnessedtheemergenceofdeeplearning
tives, with remarkable fluency and coherence [20]. Such advance-
todetectorattributeAI-generatedcontent,whichstandsasapromis-
mentshaveattractedattentionfromtheresearchcommunityaswell
ingbodyofresearch.ResearchershavebeenexploitingLLMstode-
asindustryandsocietyatlarge,offeringopportunitiesforenhancing
tectgeneratedtext[18,46],usingChatGPTitselfasadetector[2],
communication,creativity,andproductivity[7].
orcombiningLLMswithtopologicalaspects[43].
However, a pressing challenge comes alongside these advance-
A very recent trend involves leveraging contrastive learning to
ments:distinguishingbetweenAI-generatedtextandhuman-written
handle textual information. Indeed, despite its origins in the com-
text.AsAItextgenerationmodelscontinuetoimproveinsophisti-
puter vision domain, contrastive representation learning has been
cationandrealism,theabilitytodifferentiatebetweenAI-generated
proven particularly effective in NLP contexts to improve research
andhuman-generatedcontentbecomesincreasinglycrucial[37].The on semantic similarity related problems, such as text classifica-
implicationsoffailingtodiscernbetweenthesetwosourcesoftext
tion[33,10],spottinghate-speech[23],unveilingintents[55],and
areprofoundandmultifaceted,spanningvariousaspectsofsociety,
eventuallydetectingAI-generatedtextthroughdomainadaptation[3]
suchasthepreservationoftruth,authenticity,andtrustworthinessin
ordomainadversarialtraining[4].
onlinecommunication.WiththeproliferationofAI-generatedcon-
DespitetheadvancementsinresearchondetectionofAItextgen-
tent on social media, news platforms, and other digital channels, eration, each of the above mentioned approaches faces significant
challenges. Watermarking approaches are conditioned by the con-
∗lucio.lacava@dimes.unical.it
∗∗CorrespondingAuthor,andrea.tagarelli@unical.it
1 Davide Costa was affiliated with the DIMES Dept. of the University of 2CanonereallyspotifthetextintheIntroductionwaswrittenbyahuman
Calabriauntilthetimeofsubmissionofthispaper. oranAItext-generationmodel?cretepossibilityofwatermarkingagiventext,leavingthedetection (ii)Amulti-classclassificationtask,knownasAuthorshipAttribu-
ofnon-watermarkedtextsanopenissue.Statisticallearningmethods tion(AA),whichrequirestopredictexactlywhoistheauthorofa
typically require access to the models’ internals or to information text,choosingbetweenahumanoranAItext-generator.
thatmightbeunavailable,limitingtheirapplicability.Yet,moreim- Inlinewiththerelatedliterature,oursettingdoesnotdifferentiate
portantly for the sake of comparison with our proposed approach, between human authors in either task (i.e., ‘human’ always corre-
existingcontrastive-learning-basedmethodsrequireorarebetterde- spondstooneclassinC),whereastheidentityofaparticularAItext-
velopedwhenlearningaseparatemodelforeachAIgenerator. generatormustbeunveiledfortheAuthorshipAttributiontaskonly,
thereforeM−1categoriesareavailablethatcorrespondtoeitherany
Contributions. Inlightoftheaboveremarks,weaimtofillagapin
AItext-generator(forTuringTest)oraspecificAItext-generator(for
detectingandattributingAI-generatedtextbyproposingWhosAI,a
AuthorshipAttribution).
novellearningframeworkthatleveragesdeeplycontextualizeddense
Itisworthemphasizingthatthesetasks,particularlytheAuthor-
representationsoftextualdataascoreofacontrastivetripletlearn-
shipAttribution,poseinprincipletwochallenges:
ingarchitecturetoaddressbinary/multi-classpredictiontasksofau-
thorshipattributionoftextswrittenbyhumansorAItext-generation • First, the available AI text-generator categories might not nec-
models.ThekeyideaunderlyingWhosAIistointegratethepowerof essarily be regarded as classes of document representations that
Transformer-basedpretrainedlanguagemodels(PLMs)intoaframe- unlikely share their linguistic feature subspaces; roughly speak-
workofsimilaritylearningoptimizingacontrastivetripletlossfunc- ing,twotextsgeneratedbydifferentAImodels,oreventhesame
tiontolearndeepsemanticsubspacesthatmaximizethecohesiveness modelwithdifferentparametrization,couldbehardlydistinguish-
ofgroupsofsimilartextsandtheseparationofgroupsofdissimilar ableformeachother.Thisparticularlyholdsinoursettingsince
texts.ComparedtoexistingsolutionsfordetectingAI-generatedtext, theAItextgenerationisassumedtobeopen-ended.
WhosAIfeaturesthefollowingkeyadvantages: • Second, and more importantly, it is supposed that the number
ofAItext-generatorswillkeepgrowing,howeveraclassification
• WhosAI does not require editing texts (unlike in watermarking
modeltrainedtorecognizetheauthorshipofatextwouldhaveto
methods),oraccessingAIgenerationmodels’internals,anddoes
be retrained every time a new AI text-generator is added to the
notmakeanyassumptiononlinguisticfeaturesthatmightbeex-
documentdatabase.
hibited by particular text generators, or on any degree of open-
endednessintextgeneration,thusWhosAIcandealwithonany The above challenges can be faced if the problem under study is
typeoftexts; switched to a similarity learning problem as a core component for
• WhosAIisconceivedtobeversatilew.r.t.theparticularPLMused theultimategoalofbinary/multi-classprediction.
atthecoreofthelearningframework,andisgeneral-purpose,as
itdoesnotrequiretrainingseparatemodelsfordifferenttasksor
3 Background
evengenerators,overcomingamajorissueofexistingapproaches
basedoncontrastivelearning;
• The contrastive learning approach in WhosAI makes is model- Transformer-based Pre-trained Language Models (PLMs) are
thewell-establishedNLPtoolstobuilddeeplycontextualizedtext-
agnostic and scalable to the release of new AI text-generators;
indeed,isitsufficienttoaddnewdatatothetrainingsettoenable representationlearningmodels.GivenatextdataDi ∈ D,atoken
theproposedframeworktogeneralizetonewtextgenerators.
sequence Ti = [τ i,1,...,τ i,|Ti|] is produced as initial representa-
tionofDi throughatokenizationprocesstypicallyassociatedwith
The significance of WhosAI has been demonstrated based on a aPLM.Eachtokensequenceisdeeplycontextualizedbymappingit
thorough evaluation on the widely recognized TuringBench bench- ontoadense,relativelylowdimensionalspaceofsizef,basedonthe
markdataset,comprising200Karticlesthatareeitherhuman-written PLM.TheresultingoutputisthetokenembeddingsofDi,denoted
orgeneratedby19differentAItext-generationmodels.Inthischal- asPLM(Ti)∈Rf×|Ti|.Eventually,apoolingfunctionpooling(·)is
lengingcontext,WhosAIachievesexcellentresultsintermsofboth appliedtothetokenembeddingsofeachobjectDi toyieldasingle
classificationperformanceandinternalvaliditycriteria,outperform- embeddingvectorh iofsizef:
ing all the methods appearing in the benchmark’s leaderboard, for
boththeTuringTestandAuthorshipAttributiontasks. h i =pooling(PLM(Ti))∈Rf. (1)
Typically, this pooled output is an average embedding over all to-
2 ProblemStatement kenembeddingsofadataobject.Theembeddingsh arecommonly
i
Wearegivenasetofdiscretelabels(categories)C ={cj}M j=1,with referredtoassentenceembeddings.
M ≥ 2,andacollectionoftextdataobjectsD = {Di}N i=1,such Similarity Learning. The deeply contextualized representations
thateachtextobjectinD isassignedtooneofthecategoriesinC. produced by a PLM lend themselves particularly suited to enable
Thesemanticsofsuchcategoriesrefertoinformationontheorigi- semanticcomparisonsbetweentheinputtextobjects.Inthisrespect,
natorofawrittentext,whichisassumedtobeeitherahumanora wewanttoexplicitlymodelandleveragethesimilarityspaceinduced
machine,i.e.,anAImodelfortextgeneration;hence,inthissetting, from the sentence embeddings. Similarity learning aims to train a
theauthorshipsofthetextsinDareaprioriknown. modeltodistinguishbetweensimilaranddissimilarpairsofobjects.
The problem we are interested in is generally to learn a model, More specifically, if we consider objects whose relative similarity
supervisedlytrainedon(cid:2)D,C(cid:3),thatcanpredictthecategoryfromC followsapredefinedorder–i.e.,foranytripletofobjects,thefirst
for any given text data whose authorship is unknown. Specifically, objectisassumedtobemoresimilartothesecondobjectthantothe
weaddresstwosupervisedlearningproblems:(i)Abinaryclassifi- thirdobject–thegoalbecomestolearnacontrastivelossfunction,
cation task, known as Turing Test (TT), which requires to predict sothatitfavorssmalldistancesbetweenpairsofobjectslabeledas
whethertheauthorofatextisahumanoranAItext-generator,and similar, and large distances for pairs labeled as dissimilar. This isFigure1. OverviewofourproposedWhosAIlearningframework,attrainingtime(left)andinferencetime(right).
certainlyourcasesinceitisexpectedthatahuman-writtentexttobe separation between classes corresponding to different text-creators
similartoanotherhuman-writtentextthananAI-generatedtext,or inthelearnedspace,and(iii)enhancingtherobustnessoftheframe-
textsgeneratedbythesameAImodeltobesimilartoeachotherthan workbycorruptingtheinputtextualdata.
totextsgeneratedfromotherAImodels.
Training. Our training process starts with mining triplets
ContrastivelearningisoftenperformedbyusingaSiameseNet- (cid:2)D(a),D(p),D(n)(cid:3) of text data objects from D to be fed into our
workarchitecture[5],whichcontainstwoPLMinstancessharingthe
triplet network. Such triplets are formed in such a way that, for a
sameweightswhilebeingtrainedinparallelontwoinputobjectsto givenanchorD(a),D(p)andD(n)areselectedaspositiveandnega-
computecomparableoutputs.Whenusingacontrastivetripletloss, tivesample,respectively,i.e.,suchthatc(a) =c(p)andc(a) (cid:6)=c(n),
SiameseNetworkiscommonlyreferredtoasTripletNetwork.
wheresymbolsc(·) arehereusedtodenotethecategoryassociated
withananchor,positiveornegativeobject.
4 TheWhosAIFramework Theembeddingsh(a),h(p),h(n) oftheanchor,positiveandneg-
ativeobjects,respectively,arenextcomputedaccordingtoEq.1.It
Overview. WeproposeWhosAI,adeeplearningframeworkforthe shouldbenotedthatthetextannotations,i.e.,theirassociatedcate-
detectionandattributionofopen-endedtextsgeneratedbyAImodels gories,arenotrequiredwhencomputingtheembeddings,sincethe
vs.human-writtentexts.Figure1showsaschematicillustrationof PLMisanunsupervisedlearner.
themaincomponentsanddataflowsofWhosAI.
Givenatriplet,theTripletNetworkcomputesthedistancebetween
WhosAIisconceivedtobetrainedontextdatawithassociatedla-
theembeddingoftheanchorobjectandtheembeddingoftheposi-
belsexpressingauthorshipaseitherhumanoranAItext-generation tiveobject(positivepair),andthedistancebetweentheembedding
model. The framework is comprised of three key elements: (i) a oftheanchorobjectandtheembeddingofthenegativeobject(nega-
PLM, which is charge of learning deeply contextualized represen- tivepair).Thetripletlossminimizesthedistancebetweenananchor
tations(embeddings)ofthetextdata,inanunsupervisedfashion,(ii) and a positive, both having the same category, and maximizes the
a Triplet Network architecture, which is designed to perform con- distancebetweentheanchorandanegativeofadifferentcategory:
trastivelearningtoinduceasimilarityspaceofthePLMembeddings,
and(iii)anearestcentroidclassificationmodel,whichisinchargeof L= X max(d(h(a),h(p))−d(h(a),h(n))+λ,0) (2)
predictingtheauthorshipcategoryforanyquerytext. (cid:2)D(a),D(p),D(n)(cid:3)
Duringthetrainingphase,WhosAIbuildsadeepsemanticrepre-
sentationspacewherebydifferentregionscorrespondtofeaturesof whered(·,·)isadistancefunctionandλ∈R+isamarginbetween
human-writtentextsaswellasdistinctAItext-generators.Thecon- positiveandnegativepairs.Thislossdefinesthetripletconstraintas
trastive learning strategy allows for capturing the underlying simi- therequirementthatthedistanceofnegativepairsshouldbelarger
larity structure and relations within the data objects, such that the thanthedistanceofpositivepairs.
deeplycontextualizedembeddingsproducedbyaPLMencoderwill Inference. Atinferencetime,WhosAIexploitsanoff-linestepthat
be grouped together when they correspond to the same author and
consistsinprecomputingthecentroidsinDforeachcategoryc ∈
k
w Mi oll reb oe vek re ,p at ss ae bp ya pra rt oe dd ucw t,h te hn et sh imey ilac ro ir tyre ls ep ao rn nd edto spd ai cf efe fr ae cn it lita au teth so tr hs e. C, defined as c
k
= (1/|D k|) PDi∈Dkh i, where D
k
denotes the
subsetofDcontainingdataobjectsofcategoryc .
learningofthedecisionboundaryforourclassificationobjectiveof GivenapreviouslyunseendataobjectD,Whk
osAIcomputesits
determiningtheclassofpreviouslyunseentexts;inthissetting,our
embeddingh(Eq.1),whichisthencomparedtoeachofthecentroids
choiceofanearestcentroidclassifierturnsouttobeahighlyefficient
insuchawaythatDisassignedtothecategoryc k∗ thatcorresponds
yeteffectivewaytoperformauthorshipprediction.
totheleastdistantcentroid:
WhosAI is designed to be versatile and modular. Versatility
mainlyreferstothepossibilityofchoosingalternativePLMsascore k∗ =argmin d(h,c ). (3)
k=1..M k
component of the Triplet Network, variants of the Triplet Network
architecture, and alternative (instance-based) classification models.
4.1 Optimizations
Moreover,WhosAIismodularinthatenhancedmethodsareconsid-
eredtoimprovespecificaspectsoftheframework.Inparticular,these We discuss here a set optimization techniques as enhancements of
enhancementsinclude(i)improvingtheefficiencyandgeneralization key components in WhosAI, namely improved triplet mining, dy-
capabilities of the contrastive learning component, (ii) refining the namicmarginscheduling,anddatacorruption.lowmargin,λ ,tofacilitatemanageablegradientsduringearlyop-
min
timization;infact,atearlystage,amodelcanexhibitsomediscrimi-
nativeability,however,largemarginsduringthisstagewouldleadto
excessivelylargegradients,hinderinglearning.Astheoptimization
progressesandthedistanceconstraintsareenforced,theimportance
ofloss-basedgradientsgraduallydiminishes.Topreventstagnation,
the margin is hence periodically increased by λ∆ every δ training
steps.AvisualrepresentationoftheprocessisshowninFigure2.
BasedonEq.6,ourlossfunctionbecomesdynamicbyintegrating
dynamicmarginschedulingwiththetripletloss:
Figure2. Ontheleft,anexampleofviolationatearlytraining-stageofthe
tripletconstraint,asthedistanceofthenegativepairisnotlargerthanthe L(t) = X max(d(h(a),h(p))−d(h(a),h(n))+λ(t),0).
distanceofthepositivepair.Asthetrainingprogresses(midandright),the (cid:2)D(a),D(p),D(n)(cid:3)
marginbetweenpositiveandnegativepairsdynamicallyincreases,thus (7)
strengtheningthefulfillmentofthetripletconstraint.
Data Corruption. In the context of language modeling, different
strategiesofdataaugmentationcanbeusedinordertogeneratenew
Improving Triplet Mining. A straightforward implementation of
trainingexamplesbyperturbingexistinginputsequences.Thegen-
the triplet mining process involves gathering triplets before each
eraleffectistoincreasethediversityandvariabilityofthetraining
trainingepochandfeedingbatchesofthesetripletsintotheTriplet
Network,essentiallyasan“offline”process.However,thisapproach data,therebyimprovingthemodel’sabilitytogeneralizeandrobust-
ness to different input variations. In this regard, we focus on the
might have two main drawbacks: (i) not all generated triplets may
processofremovingindividualtokens,orgroupsoftokens,froma
contain the valuable information needed for minimizing the loss
giveninputsequenceandobservingtheimpactonthemodel’sout-
(Eq.2),and(ii)tripletsregardedas“informative”inanearlierstage
put.Specifically,weconsiderthefollowingoperations,whichhave
of training might quickly become “uninformative” as the model’s
previously shown to be effective in improving the performance of
weightsundergoupdates.
PLMsinseveraltasks(e.g.,[19,27]):
Withinthisview,itbecomescrucialforthetripletminingprocess
toprioritizeanonlineidentificationofthemostinformativetriplets
• tokendeletion:giventhetokensequenceTi = [τ i,1,...,τ i,|Ti|],
foreachtrainingepoch.Theseshouldbethemostunexpectedones, atokenτ
i,j
isremovedwithprobabilityp∼U[0,1];
i.e.,tripletsthatmostviolatethemarginconstraintsenforcedbythe • spancropping:giventhetokensequenceTi = [τ i,1,...,τ i,|Ti|],
lossfunction.Thisstrategycanimprovetheminingprocessasiten- atokenτ i,j isselectedasastartingindexwithprobabilityps ∼
hances the generalization capabilities and training stability, and it U[0,1];then,foreachsampledstartingindex,aspansizesz ∼
makes training more efficient by avoiding the inclusion of the un- U[0,|Ti|×pspan]isalsosampled,wherepspanindicatestherel-
informativetriplets. ativesizeofthespanw.r.t.theoverallsequencesize.Finally,the
The above requirements can effectively be fulfilled by the pair sampledspansoftokensaredeletedfromthesequence.
mining scheme adopted in the multi-similarity miner method [48].
Itisimportanttonotethattheseoperationsaimtoremovespecific
Essentially, the pair mining consists in sampling informative pairs
(sequencesof)tokensfromourinputtextinsteadofmaskingthem.
throughtherelativesimilaritybetweenthenegativeandpositivepairs
sharing a common anchor. More specifically, a negative pair is se-
Accordingly,ourPLMsarenotrequiredtoreconstructthemissing
tokens,asinmasking-basedtasks.
lected as one having lower distance than the hardest positive pair
(i.e.,theonewiththehighestdistance):
5 ExperimentalMethodology
d(h(a),h(n))<maxd(h(a),h(p))+ε. (4)
5.1 Data
D(p)
Apositivepairisselectedasonehavinghigherdistancethanthe We used the publicly available benchmark dataset TuringBench
hardestnegativepair(i.e.,theonewiththelowestdistance): [42,41],whichcontains200Knewsarticles,where10Karehuman-
written and the other ones are machine-generated news articles
d(h(a),h(p))> mind(h(a),h(n))−ε. (5) equallydistributedover19differentAItext-generationmodels.From
D(n)
the human-written articles, originally collected from sources like
DynamicMarginScheduling. Anotherimprovementweconsider CNNandwithtypicallengthof200-400words,thetitleswereused
istomakethetrainingofWhosAIprogressivelyharder.Specifically, to prompt the 19 AI text-generators to generate 10K articles each.
bydynamicallyincreasingthemarginλinourlossfunction(Eq.2), Table1summarizesthemaincharacteristicsofthedataset,providing
werequirethemodeltofocusonhardernegativepairsasthetraining detailsforthevarioussubsetsofdataassociatedwiththehumancat-
goeson,inordertoproduceanenhancedseparationbetweenclasses. egoryandeachoftheAItext-generatorcategories.Itshouldbenoted
Tothisaim,inspiredbycurriculum-basedlearning[17],werevise
thatTuringBenchcomeswithapre-definedsplitintotrain,validation
thelossfunctionwithadynamicmarginthatfollowsalinearsched- andtestsets.Wewillfollowthissetting,soastofullycomparewith
uledependentonthetrainingsteptimet≥0,whichisdefinedas: previousandfutureevaluationstudiesonTuringBench.
λ(t) =λ min+λ∆(tmodδ), (6) 5.2 AssessmentCriteriaandModelSettings
whereλ
min
∈R+istheinitialmargin,λ∆ ∈R+denotesthemargin TovalidatetheperformanceofWhosAIindetectingandattributing
increment,andδrepresentsthestepsizeoftheincrement.Theratio- AI-generatedtext,weresorttostandardstatisticsbasedonthecon-
naleofthisformulaisasfollows.Webeginwithaninitial,relatively fusion matrices derived from testing WhosAI predictions w.r.t. theTable1. MaincharacteristicsoftheTuringBench.Tableadaptedfrom[42].
Generationmodel Avg#Words Avg.#Sentences
Ref. Params
(subset) perdocument perdocument
Human – 232.7 15.0 –
GPT-1 [34] 316.7 10.5 117M
GPT-2small [35] 118.6 4.0 124M
GPT-2medium [35] 120.9 4.2 355M
GPT-2large [35] 119.7 4.1 774M
GPT-2xl [35] 117.8 4.1 1.5B
GPT-2PyTorch NA 178.9 7.03 344M
GPT-3 [6] 129.5 5.0 175B
GROVERbase [54] 299.2 9.4 124M
GROVERlarge [54] 286.3 8.7 355M
GROVERmega [54] 278.9 9.2 1.5B
CTRL [22] 398.1 20.0 1.6B
XLM [26] 387.8 4.2 550M
XLNETbase [52] 226.1 11.6 110M Figure3. TuringTestevaluation:barchartoftheaverageF 1scorefrom
XLNETlarge [52] 415.8 4.3 340M theTuringBenchLeaderboard,foreachTuringBenchsubset(i.e.,generator).
FAIRwmt19 [32] 221.2 14.6 656M ThehorizontalreddashedlinecorrespondstotheF 1scoreachievedby
FAIRwmt20 [11] 100.6 5.1 749M WhosAI(best-performingsetting)overtheentireTuringBenchtestset.
TRANSFORMERXL [12] 211.7 9.8 257M
PPLMdistil [13] 156.9 10.7 82M
PPLMgpt2 [13] 188.9 11.9 124M
ground-truth under the Turing Test task and w.r.t. the ground-truth
under the Author Attribution task, respectively. These include the
weighted average (i.e., averaging over the support-weighted mean
perclass)ofprecision(P),recall(R),andF -score(F ).
1 1
We also account for distance-based quantitative criteria that ex-
presshowwellthelearnedspacealignswiththepredefinedcatego-
rizationofthetrainingtexts,intermsofcompactnesswithinsame-
categorygroupsofobjectsandseparationbetweengroupsofobjects
of different categories. To this purpose, by denoting with sim(·,·) Figure4. TuringTestevaluation:2DUMAPvisualizationofthesemantic
spaceproducedbyWhosAIbefore(left)andaftertraining(right).
thecosinesimilarityfunction,wecalculatetheaveragepairwisesim-
Colorsdenotehuman(red)vs.AI-generated(blue)texts.
ilarityoftheembeddingsofobjectssharingthesamecategory:
6 Results
1
intra(D)= X X sim(h i,h j), (8)
|D k| Weorganizeourpresentationoftheresultsintothreeparts:thefirst
ck∈CDi,Dj∈Dk
two discuss quantitative and qualitative results on the Turing Test
andtheaveragepairwisesimilarityoftheembeddingsofobjectsbe-
(TT)andAuthorshipAttribution(AA)tasks,respectively,achieved
longingtotwodifferentcategories:
byWhosAI(best-performingsetting)andcompetitors,whereasthe
inter(D)=
|D
h|1
|D k|
X X sim(h i,h j). (9) t gh ii er sd op nar tt hf eo pc eu rs fe os rmon anev ca el ou fat Wing hoth se Ai Im
.
pactofouroptimizationstrate-
ch,ck∈CDi∈Dh,Dj∈Dk
Following the most widely used approaches to sentence embed-
6.1 TuringTest
ding[36],weusedBERT[15]asourreferencePLM,leveragingits
publicly available bert-base-uncased implementation hosted We start with evaluating WhosAI on the binary classification task,
ontheHuggingFaceplatform.3Thishasatotalof110Mparameters, i.e., TT, aimed at recognizing whether a given piece of text origi-
distributed across 12 Transformer-encoder layers with 12 attention nates from a human or any AI text-generator. As reported in Fig-
heads,avocabularyof32Ktokens,maximumlengthofcontextset ure3,theofficialTuringBenchleaderboard4 presentstheF -scores
1
to512tokens,andembeddingsizef setto768.WeusedtheAdamW for the TT under a One-vs-One approach, whereby one side of the
optimizer with learning rate of 1.0E-5, (β 1,β 2)=(0.9,0.99), and a comparisondenotes“human”andtheotheronecorrespondstoeach
weightdecay,usefulforregularization,of0.01.Wesetthebatchsize oftheavailableAI-generatorsinTuringBench.Itcanbenoticedthat
to32elements,andthenumberofstepsforeachtrainingprocedureto somegeneratorsaremoreeasilydetectablethanothers,resultingin
30K.Furthermore,weemployalinearlearningrateschedulerwitha substantialdisparitiesintermsofaverageweightedF -scores.
1
3000-stepwarm-up.Asconcernsthedynamicmarginscheduling,we Bycontrast,WhosAIisabletolearnadeepsemanticspaceforthe
linearlydistributemarginupdatesduringthetrainingprocess,byset- wholesetofgeneratorsatonce.Asamajorresult,WhosAIachieves
tingλ min =0.1,andδ =750,whichimpliesλ∆ = 37 05 00 00 =0.025. animpressiveF 1-scoreof0.999onthewholeTTtestsetsuppliedby
Forthedatacorruptionoperations,wesettheprobabilityp = 0.05 theTuringBenchbenchmark,settinganewbestperformanceonthe
for the token deletion (TD) function, and both the probabilities ps Turing Test. Our remarkable F 1-score is further corroborated by a
andpspan to0.05forthespancropping(SC)function.Throughout qualitativeanalysisbasedonthevisualizationprovidedinFigure4:5
ourwork,weusedthecosinedistancedefinedas1−sim(·,·). whileatthebeginningofthetrainingthesemanticrepresentationdi-
Our experiments were carried out on a double 56-core Intel(R) rectlyinducedbythePLMdoesnotadequateseparatethehumanand
Xeon(R) Gold 6258R CPU, with 256GB RAM and two NVIDIA
GeForceRTX3090s,OSUbuntuLinux22.04LTS. 4Availableathttps://turingbench.ist.psu.edu/
5Figures4–5wereobtainedbytransformingtheWhosAIembeddingsusing
3https://huggingface.co/google-bert/bert-base-uncased UMAP[30]defaultparameterswith2componentsandcosinedistance.Table2. AuthorshipAttributionevaluation:ResultsachievedbyWhosAI
vs.thetop-5modelsfromtheTuringBenchLeaderboard
(https://turingbench.ist.psu.edu/).
Detectionmethod P R F 1
WhosAI 0.990 0.990 0.990
RoBERTa 0.821 0.813 0.811
BERT 0.803 0.802 0.800
BERTAA 0.780 0.775 0.776
OpenAIdetector 0.781 0.781 0.774
SVM(3-grams) 0.712 0.722 0.715
Table3. AuthorshipAttributionevaluation:Summaryofper-category
resultsachievedbyWhosAI.
Figure5. AuthorshipAttributionevaluation:2DUMAPvisualizationof
thesemanticspaceproducedbyWhosAI(left)andSBERT(right).
Generationmodel(class) P R F 1 support
Colorsdenotehuman(blue)andthevariousAItextgenerators.
Human 0.999 0.992 0.995 975
AIsubspaces,thefinaltrainedWhosAIshowsitsabilitytolearnper- GPT-1 1.000 1.000 1.000 993
GPT-2xl 0.950 0.970 0.960 993
fectlytorecognizethetwoclassesfortheTuringTest.Thiscouples GPT-3 0.977 0.959 0.968 894
withtheremarkableresultsreportedinTable4,wherebytheaverage GROVERmega 0.999 0.997 0.998 894
CRTL 0.998 0.999 0.999 1000
pairwisesimilaritybetweenembeddingsofobjectssharingthesame
XLM 1.000 1.000 1.000 973
category,resp.belongingtodifferentcategories,isof0.931,resp.- XLNETlarge 0.999 1.000 1.000 1000
0.808.Thesehighlightanotablecoherencewithineachcategoryand FAIRwmt20 0.983 0.986 0.984 993
aclearseparationbetweencategories.
TRANSFORMERXL 0.996 0.994 0.995 991
PPLMgpt2 0.990 0.992 0.991 975
Overall 0.990 0.989 0.989 10681
6.2 AuthorshipAttribution
6.3 SensitivityAnalysis
We discuss our evaluation of WhosAI on the Author Attribution
(AA) task, aimed at deciding the authorship of a text, being a hu- TuringTest. Table4reportstheresultsachievedbyWhosAIonthe
manoroneoftheAItext-generatorsinTuringBench. TTtaskbyvaryingtheframeworksettingsaccordingtothevarious
Ourfirstremarkablefindingderivesfromacomparisonbetween optimizationsdiscussedinSection4.1.
WhosAI results against those reported on the TuringBench leader- Atfirstglance,wenoticethatWhosAIcansolvetheTTtaskal-
board for the AA task, whose top-5 best-performing models are mostperfectly–withP,R,F alwaysabove0.996–regardlessof
1
showninTable2.RoBERTa[29]withamulti-classclassificationset- specificoptimizations.Thisremarkablefinding,coupledwiththevi-
tingturnsouttobethebestmodelintheleaderboardfortheAAtask, sualevidenceofthesemanticspacerepresentationdisplayedinFig-
withaF scoreof0.811,followedbyotherBERT-basedapproaches, ure4,indicatethatWhosAIexcelsindistinguishingbetweenhuman
1
aswellastheofficialOpenAIdetectorandmachinelearning-based authors and AI text-generators, even when equipped with the sim-
models. The winner method from the leaderboard is however out- plestconfiguration.
performedbyWhosAI,whichachievesastrikingaverageweighted While the classification performance criteria have indeed only
F score, precision and recall of 0.990, thus demonstrating almost slight fluctuations by varying the framework settings, different be-
1
perfectcapabilitiesofauthorshipprediction. haviors of WhosAI appear to be more evident in terms of com-
Table3offersinsightsintothepredictionperformanceofWhosAI pactness(intra)and,especially,separation(inter),withtheformer
w.r.t.eachofthegeneratorcategoriescorrespondingtothelargest- consistently above 0.846 and the latter that can vary from 0.35 to
sizeversionsoftheAImodels.Resultsshowextremelyrobustness -0.81. In particular, applying data corruption techniques can affect
ofWhosAI,asitachievesF scoreatleast0.960,andabove0.99in thedistance-basedcriteria:infact,byusingeithertokendeletionand
1
7outof10cases.Itshouldalsobenoticedthatprecisionandrecall span cropping, we notice a worse (i.e., higher) similarity between
arealwayscomparableorveryclosetoeachother,thusindicatingan embeddingsofobjectspertainingtodifferentcategories,whereasthe
equalcapabilityofavoidingbothtypesofstatisticalerrors. similaritybetweenembeddingsofobjectsfromthesamecategoryre-
AspreviouslyfoundfortheTuringTesttask,thestrikingF scores mainscoherent.Thissuggeststhatcorruptingthedataininputtothe
1
achievedbyWhosAIcouplewithanevidenceofhighestcohesive- TTpredictormightimpactparticularlyonsomeofthetokensthatare
nessandseparationofthesubspacesassociatedwiththevarioustext discriminativeofthetext-generators,beinghumanorAImodels.
authorships,asvisuallyshowninFig.5(left);quantitatively,thiscor- MoreimportantlyforourTTevaluation,weassessedtheeffecton
responds to an average pairwise similarity between embeddings of theWhosAIperformanceduetothepresenceoftextsthatweregen-
objectssharingthesamecategory,resp.belongingtodifferentcate- eratedbythesameAIarchitectureyetwithdifferentparametersizes
gories,of0.938,resp.-0.012(cf.Table5). (cf.Table1).Tothisaim,wefocusedoncomparingtheperformance
ItisworthnotingthattheoutstandingperformancebyWhosAIin of WhosAI when keeping all instances generated by the same AI
theAAtaskisnotpairedbyastate-of-the-artsentence-embedding text-generation architecture, and only the subsets corresponding to
methodforsemantic-similarity-relatedtaskslikeSBERT[36],based eitherthelargestmodelorthesmallestmodelofthatAIarchitecture
onaSiamesenetworkusingBERTatitscore:indeed,asshownin availableinTuringBench.
Fig.5(right),theintra-classcohesivenessandinter-classseparation AsreportedinTable4,thevariationofthemodelsubsetmainly
ofthesemanticspacelearnedbySBERTareclearlyworsethanthose impactsontheseparationbetweenembeddingsofobjectspertaining
achievedbyWhosAI. todifferentclasses:keepingallinstancesfromdifferentlysizedmod-Table4. TuringTestevaluation:ResultsbyvaryingthesettingofWhosAI.
Mostpreferablesettingisbolded.
Triplet Dynamic Data Generator P R F inter intra
Mining Margin Corrupt. subset 1
✗ ✗ ✗ All 0.999 0.999 0.999 -0.808 0.931
✓ ✗ ✗ All 0.999 0.999 0.999 -0.805 0.914
✓ ✓ ✗ All 0.999 0.999 0.999 0.275 0.941
✓ ✓ ✗ Largest 0.999 0.999 0.999 0.050 0.964
✓ ✓ SC Largest 0.998 0.998 0.998 0.275 0.892
✓ ✓ TD Largest 0.999 0.999 0.999 0.287 0.955
✓ ✓ ✗ Smallest 0.998 0.998 0.998 0.147 0.951
✓ ✓ SC Smallest 0.996 0.996 0.996 0.353 0.846
✓ ✓ TD Smallest 0.998 0.998 0.998 0.321 0.958
Figure6. Cosinesimilaritybetweencentroidsofthedifferentcategories
forall(left),resp.thelargest(right),generators.Darkercolorsindicate
Table5. AuthorshipAttributionevaluation:Resultsbyvaryingthesetting highersimilarity.
ofWhosAI.Mostpreferablesettingisbolded.
7 ConclusionsandFutureWork
Triplet Dynamic Data Generator P R F inter intra
Mining Margin Corrupt. subset 1 We tackled the challenge of detecting and attributing AI-generated
✗ ✗ ✗ All 0.769 0.775 0.763 -0.030 0.921 textthroughWhosAI,anovelPLM-basedframeworkthatleverages
✗ ✓ ✗ All 0.767 0.774 0.761 -0.031 0.923 contrastivelearningtoinduceasemanticsimilarityspacetextswrit-
✓ ✓ ✗ All 0.782 0.789 0.779 0.208 0.891
tenbyhumansorAItext-generationmodels.Thissimilarityspaceis
✓ ✓ ✗ Largest 0.990 0.990 0.990 -0.012 0.938
✓ ✓ SC Largest 0.985 0.985 0.985 0.048 0.938 efficientlyexploitedatinferencetimebymeansofanearestcentroid
✓ ✓ TD Largest 0.989 0.989 0.989 0.028 0.938 classifiertopredicttheauthorshipofunlabeledtexts.Extensiveex-
✓ ✓ ✗ Smallest 0.989 0.989 0.989 0.003 0.933
✓ ✓ SC Smallest 0.988 0.988 0.988 0.069 0.939 perimentationonthewell-knownTuringBenchdatasethasrevealed
✓ ✓ TD Smallest 0.991 0.991 0.991 0.038 0.936 state-of-the-artperformancesofWhosAIonbothTTandAAtasks.
Furthermore,WhosAIcomeswithseveralkeyadvantages:(i)itcan
beappliedstraightforwardlywithoutalteringtextsoraccessingmod-
els from the same architecture can bring additional discriminative
els’internals,(ii)itcanbeadaptedtoanumberofAItext-generators
information helping WhosAI better separate the human-generated
without needing model-specific adjustments, and (iii) it is model-
texts from the ones generated by all AI models. Conversely, main-
agnosticandscalableforeasyintegrationofnovelAItext-generators.
tainingonlyeitherthelargest-modelorsmallest-modelsubsetsmight
Remarkably,suchempiricalevidenceofoutstandingperformanceof
lead to slightly improved intra-class cohesiveness, hinting at a re-
WhosAIholdsdespiteourchoiceofPLMintheexperimentaleval-
ducednoiseaffectingcharacterizingtokens.
uationreferstothebaselineBERTmodel.
AuthorshipAttribution. Analogouslytothepreviousanalysis,Ta- There are important directions to explore. Particularly, we will
ble 5 summarizes the results achieved by WhosAI on the AA task evaluate WhosAI on other types of written texts than those avail-
based on different settings. We notice that WhosAI obtains an F
1
ableinTuringBench.WeaimtocompareWhosAIwithadvancedyet
scoreabove0.980,alsowithremarkabledistance-basedscores,in6 commerciallylicensedAIdetectiontools(e.g.,GPTZero).Also,we
out of 9 configurations, which correspond to testing on texts from willinvestigateexplainabilityaspectsofWhosAIinordertounveil
a particular model-size variant of an AI text generator, rather than whichfeaturesaredeterminanttocharacterizeandwhichtodiscrim-
testingonalltextsfromthedifferentvariantsofamodel. inatetextoriginators.
This prompted us to investigate the similarity between the cen-
RemarksonthecarbonfootprintofWhosAI. Asasupplemen-
troids of the subsets corresponding to the various categories (i.e.,
taryanalysis,weinvestigatedtheenvironmentalimpactofWhosAI.
modelinstances),asreportedinFig.6(left).Notably,lookingatthe
BasedonourselectedreferencePLM,whichhas109.48Mparame-
diagonalblocksintheheatmap,WhosAIconsistentlylearnsidentical
ters,weestimateaninferencecostof290.17GFLOPSforasingle-
centroids(i.e.,cosinesimilarityequalto1)forthedifferentinstances
elementbatchwithasequencelengthof512,andatrainingcostof
ofthesameAIgenerationarchitecture,regardlessoftheparameter
835.8 PFLOPS assuming 32 batch size, 512 sequence length, and
size.Whilethissuggeststhatasingleinstanceofagivenarchitecture
30K training steps. With an average training time for all WhosAI
maybeenoughinacontrastivesettingtocharacterizeawholefam-
configurationsof∼8hoursona350WNvidiaGeForceRTX3090
ilyofAIgenerators,itintroduceslotsofnoisewhendistinguishing
GPU, we estimate an energy consumption of 2.8 kWh per training
betweentwoormoreinstancesofthesamearchitecture.
run.Withanaveragecarbonefficiencyfactorof0.432kg/kWh,asin-
If we focus on the largest-model variants of the AI generators,
gletrainingrunisassociatedwith1.21kgofCO emissions,which
whichshouldmakeourAAtaskmorechallengingasitissupposed 2
extendsto50.4kWhofconsumedenergyand21.78kgofCO equiv-
thatmoreparametersenablethemodeltocapturemoreknowledge 2
alentemissionsforrunningallofourexperiments.6
providingitwithbettergenerationcapabilities,westillfindinFig.6
(right) low values of inter-class similarity. Analogous results (not Acknowledgements
shown) are achieved for the smallest model instances, which may AT, resp. LLC, was supported by project “Future Artificial Intelli-
bepreferableinresource-constrainedscenarios.
genceResearch(FAIR)”spoke9(H23C22000860006),resp.project
Furthermore, considering the impact of the different optimiza-
SERICS(PE00000014),bothundertheMURNationalRecoveryand
tions,thetripletminingandthedynamicmarginschedulingleadto
ResiliencePlanfundedbytheEU-NextGenerationEU.
performance improvements, while the data corruption methods ap-
pear to worsen the separation of the learned embedding subspaces 6 For this analysis, we used calflops (https://github.com/MrYxJ/
byaffectingtokenscrucialfordiscriminatingtext-generators. calculate-flops.pytorch)andimpact(https://github.com/mlco2/impact).References V. Stoyanov, and L. Zettlemoyer. BART: denoising sequence-to-
sequencepre-trainingfornaturallanguagegeneration,translation,and
[1] G.Bao,Y.Zhao,Z.Teng,L.Yang,andY.Zhang. Fast-detectgpt:Ef- comprehension.InProc.ACLConf.,pages7871–7880,2020.
ficient zero-shot detection of machine-generated text via conditional [28] B.Li,M.Zhang,P.Zhang,J.Sun,andX.Wang.Resilientwatermarking
probabilitycurvature.arXiv:2310.05130,2023. forllm-generatedcodes.arXiv:2402.07518,2024.
[2] A.BhattacharjeeandH.Liu.FightingFirewithFire:CanChatGPTDe- [29] Y.Liu,M.Ott,N.Goyal,J.Du,M.Joshi,D.Chen,O.Levy,M.Lewis,
tectAI-generatedText?SIGKDDExplor.Newsl.,25(2):14–21,2024. L.Zettlemoyer,andV.Stoyanov. Roberta:Arobustlyoptimizedbert
[3] A.Bhattacharjee,T.Kumarage,R.Moraffah,andH.Liu.ConDA:Con- pretrainingapproach,2019.
trastive domain adaptation for AI-generated text detection. In Proc. [30] L.McInnes,J.Healy,andJ.Melville. Umap:Uniformmanifoldap-
IJCNLPConf.,pages598–610,2023. proximationandprojectionfordimensionreduction,2020.
[4] A. Bhattacharjee, R. Moraffah, J. Garland, and H. Liu. Eagle: [31] E.Mitchell,Y.Lee,A.Khazatsky,C.D.Manning,andC.Finn. De-
A domain generalization framework for ai-generated text detection. tectgpt:Zero-shotmachine-generatedtextdetectionusingprobability
arXiv:2403.15690,2024. curvature.InProc.ICMLConf.,pages24950–24962.PMLR,2023.
[5] J.Bromley,I.Guyon,Y.LeCun,E.Säckinger,andR.Shah. Signature [32] N. Ng, K. Yee, A. Baevski, M. Ott, M. Auli, and S. Edunov. Face-
verificationusingasiamesetimedelayneuralnetwork. InProc.NIPS bookfair’swmt19newstranslationtasksubmission.arXiv:1907.06616,
Conf.,pages737–744,1993. 2019.
[6] T.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal, [33] L.Pan,C.-W.Hang,A.Sil,andS.Potdar. Improvedtextclassification
A.Neelakantan,P.Shyam,G.Sastry,A.Askell,etal.Languagemodels viacontrastiveadversarialtraining. InProc.AAAIConf.,volume36,
arefew-shotlearners.Proc.NIPSConf.,33:1877–1901,2020. pages11130–11138,2022.
[7] S.Bubeck,V.Chandrasekaran,R.Eldan,J.Gehrke,E.Horvitz,E.Ka- [34] A.Radford,K.Narasimhan,T.Salimans,I.Sutskever,etal.Improving
mar,P.Lee,Y.T.Lee,Y.Li,S.Lundberg,H.Nori,H.Palangi,M.T. languageunderstandingbygenerativepre-training.2018.
Ribeiro,andY.Zhang. Sparksofartificialgeneralintelligence:Early [35] A.Radford,J.Wu,R.Child,D.Luan,D.Amodei,I.Sutskever,etal.
experimentswithgpt-4.arXiv:2303.12712,2023. Languagemodelsareunsupervisedmultitasklearners. OpenAIblog,1
[8] L.L.Cava,D.Costa,andA.Tagarelli.OpenModels,ClosedMinds?On (8):9,2019.
AgentsCapabilitiesinMimickingHumanPersonalitiesthroughOpen [36] N.ReimersandI.Gurevych. Sentence-BERT:Sentenceembeddings
LargeLanguageModels.arXiv:2401.07115,2024. usingSiameseBERT-networks.InProc.EMNLP-IJCNLPConf.,pages
[9] C. Chen and K. Shu. Can LLM-Generated Misinformation Be De- 3982–3992,2019.
tected?arXiv:2309.13788,2024. [37] V.S.Sadasivan,A.Kumar,S.Balasubramanian,W.Wang,andS.Feizi.
[10] J. Chen, R. Zhang, Y. Mao, and J. Xu. Contrastnet: A con- Canai-generatedtextbereliablydetected?arXiv:2303.11156,2023.
trastive learning framework for few-shot text classification. In Proc. [38] J. Su, T. Y. Zhuo, D. Wang, and P. Nakov. Detectllm: Leveraging
AAAI-IAAI-EAAIConf.,pages10492–10500,2022. logrankinformationforzero-shotdetectionofmachine-generatedtext.
[11] P.-J. Chen, A. Lee, C. Wang, N. Goyal, A. Fan, M. Williamson, arXiv:2306.05540,2023.
and J. Gu. Facebook ai’s wmt20 news translation task submission. [39] R. Tang, Y.-N. Chuang, and X. Hu. The science of detecting llm-
arXiv:2011.08298,2020. generatedtext.CommunicationsoftheACM,67(4):50–59,2024.
[12] Z.Dai,Z.Yang,Y.Yang,J.Carbonell,Q.Le,andR.Salakhutdinov. [40] E.Tulchinskii,K.Kuznetsov,K.Laida,D.Cherniavskii,S.Nikolenko,
Transformer-XL:Attentivelanguagemodelsbeyondafixed-lengthcon- E.Burnaev,S.Barannikov,andI.Piontkovskaya. Intrinsicdimension
text.InProc.ACLConf.,pages2978–2988,2019. estimationforrobustdetectionofAI-generatedtexts. InProc.NIPS
[13] S.Dathathri,A.Madotto,J.Lan,J.Hung,E.Frank,P.Molino,J.Yosin- Conf.,2023.
ski,andR.Liu. Plugandplaylanguagemodels:Asimpleapproachto [41] A. Uchendu, T. Le, K. Shu, and D. Lee. Authorship attribution for
controlledtextgeneration.arXiv:1912.02164,2019. neuraltextgeneration.InProc.EMNLPConf.,pages8384–8395,2020.
[14] Z.Deng,H.Gao,Y.Miao,andH.Zhang. Efficientdetectionofllm- [42] A.Uchendu,Z.Ma,T.Le,R.Zhang,andD.Lee.TURINGBENCH:A
generatedtextswithabayesiansurrogatemodel. arXiv:2305.16617, benchmarkenvironmentforTuringtestintheageofneuraltextgener-
2023. ation.InFindingsoftheEMNLPConf.,pages2001–2016,2021.
[15] J.Devlin,M.Chang,K.Lee,andK.Toutanova. BERT:pre-training [43] A.Uchendu,T.Le,andD.Lee.Toproberta:Topology-awareauthorship
ofdeepbidirectionaltransformersforlanguageunderstanding.InProc. attributionofdeepfaketexts.arXivpreprintarXiv:2309.12934,2023.
NAACL-HLTConf.,pages4171–4186,2019. [44] C. Vasilatos, M. Alam, T. Rahwan, Y. Zaki, and M. Maniatakos.
[16] S. Gehrmann, H. Strobelt, and A. Rush. GLTR: Statistical detec- Howkgpt: Investigating the detection of chatgpt-generated univer-
tionandvisualizationofgeneratedtext. InProc.ACLConf.:System sity student homework through context-aware perplexity analysis.
Demonstrations,pages111–116,2019. arXiv:2305.18226,2023.
[17] G.HacohenandD.Weinshall. Onthepowerofcurriculumlearningin [45] S. Venkatraman, A. Uchendu, and D. Lee. Gpt-who: An informa-
trainingdeepnetworks.InProc.ICMLConf.,pages2535–2544,2019. tion density-based machine-generated text detector. arXiv preprint
[18] D. Ippolito, D. Duckworth, C. Callison-Burch, and D. Eck. Auto- arXiv:2310.06202,2023.
maticdetectionofgeneratedtextiseasiestwhenhumansarefooled. [46] V.Verma,E.Fleisig,N.Tomlin,andD.Klein. Ghostbuster:Detecting
arXiv:1911.00650,2019. textghostwrittenbylargelanguagemodels.arXiv:2305.15047,2023.
[19] G.Izacard,M.Caron,L.Hosseini,S.Riedel,P.Bojanowski,A.Joulin, [47] P. Wang, L. Li, K. Ren, B. Jiang, D. Zhang, and X. Qiu. Seqxgpt:
andE.Grave.Unsuperviseddenseinformationretrievalwithcontrastive Sentence-levelai-generatedtextdetection.arXiv:2310.08903,2023.
learning.Trans.Mach.Learn.Res.,2022. [48] X.Wang,X.Han,W.Huang,D.Dong,andM.R.Scott.Multi-similarity
[20] M.Jakesch,J.T.Hancock,andM.Naaman. Humanheuristicsforai- loss with general pair weighting for deep metric learning. In Proc.
generatedlanguageareflawed. ProceedingsoftheNationalAcademy CVPRConf.,pages5022–5030,2019.
ofSciences,120(11):e2208839120,2023. [49] J. Wu, S. Yang, R. Zhan, Y. Yuan, D. F. Wong, and L. S. Chao. A
[21] G.Jawahar,M.Abdul-Mageed,andL.V.Lakshmanan. Automaticde- surveyonllm-gerneratedtextdetection:Necessity,methods,andfuture
tectionofmachinegeneratedtext:Acriticalsurvey.arXiv:2011.01314, directions.arXiv:2310.14724,2023.
2020. [50] X.Xu,Y.Yao,andY.Liu. Learningtowatermarkllm-generatedtext
[22] N.S.Keskar,B.McCann,L.R.Varshney,C.Xiong,andR.Socher.Ctrl: viareinforcementlearning.arXiv:2403.10553,2024.
Aconditionaltransformerlanguagemodelforcontrollablegeneration. [51] K.-C.YangandF.Menczer.Anatomyofanai-poweredmalicioussocial
arXiv:1909.05858,2019. botnet.arXiv:2307.16336,2023.
[23] Y. Kim, S. Park, and Y.-S. Han. Generalizable implicit hate speech [52] Z.Yang,Z.Dai,Y.Yang,J.Carbonell,R.R.Salakhutdinov,andQ.V.
detectionusingcontrastivelearning. InProc.COLINGConf.,pages Le. Xlnet:Generalizedautoregressivepretrainingforlanguageunder-
6667–6679,2022. standing.Proc.NIPSConf.,32,2019.
[24] Z.M.Kim,K.H.Lee,P.Zhu,V.Raheja,andD.Kang. Threadsof [53] K.Yoo,W.Ahn,J.Jang,andN.Kwak. Robustmulti-bitnaturallan-
subtlety:Detectingmachine-generatedtextsthroughdiscoursemotifs. guagewatermarkingthroughinvariantfeatures. InProc.ACLConf.,
arXiv:2402.10586,2024. pages2092–2115,2023.
[25] J.Kirchenbauer,J.Geiping,Y.Wen,J.Katz,I.Miers,andT.Goldstein. [54] R.Zellers,A.Holtzman,H.Rashkin,Y.Bisk,A.Farhadi,F.Roesner,
Awatermarkforlargelanguagemodels. InProc.ICMLConf.,pages andY.Choi.Defendingagainstneuralfakenews.InProc.NIPSConf.,
17061–17084,2023. 2019.
[26] G.LampleandA.Conneau.Cross-linguallanguagemodelpretraining. [55] J. Zhang, T. Bui, S. Yoon, X. Chen, Z. Liu, C. Xia, Q. H. Tran,
arXiv:1901.07291,2019. W. Chang, and P. Yu. Few-shot intent detection via contrastive pre-
[27] M.Lewis,Y.Liu,N.Goyal,M.Ghazvininejad,A.Mohamed,O.Levy,
trainingandfine-tuning.arXiv:2109.06349,2021.