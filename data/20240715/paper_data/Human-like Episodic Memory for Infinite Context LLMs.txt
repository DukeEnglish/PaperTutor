Preprint
HUMAN-LIKE EPISODIC MEMORY FOR INFINITE CONTEXT LLMS
ZafeiriosFountas1,MartinABenfeghoul1,*,AdnanOomerjee1,*,FeniaChristopoulou1,
GerasimosLampouras1,HaithamBou-Ammar1,2andJunWang2
1HuaweiNoah’sArkLab,London,UK
2UniversityCollegeLondon,UK
{zafeirios.fountas,adnan.ebrahim.oomerjee,efstathia.christopoulou}@huawei.com
{gerasimos.lampouras,haitham.ammar}@huawei.com
martin.antoine.benfeghoul@h-partners.com
jun.wang@ucl.ac.uk
ABSTRACT
Largelanguagemodels(LLMs)haveshownremarkablecapabilities,butstillstrugglewithprocessing
extensivecontexts,limitingtheirabilitytomaintaincoherenceandaccuracyoverlongsequences.
In contrast, the human brain excels at organising and retrieving episodic experiences across vast
temporalscales,spanningalifetime. Inthiswork,weintroduceEM-LLM,anovelapproachthat
integrateskeyaspectsofhumanepisodicmemoryandeventcognitionintoLLMs,enablingthem
toeffectivelyhandlepracticallyinfinitecontextlengthswhilemaintainingcomputationalefficiency.
EM-LLM organises sequences of tokens into coherent episodic events using a combination of
Bayesiansurpriseandgraph-theoreticboundaryrefinementinanon-linefashion. Whenneeded,these
eventsareretrievedthroughatwo-stagememoryprocess,combiningsimilarity-basedandtemporally
contiguousretrievalforefficientandhuman-likeaccesstorelevantinformation. Experimentsonthe
LongBenchdatasetdemonstrateEM-LLM’ssuperiorperformance,outperformingthestate-of-the-art
InfLLMmodelwithanoverallrelativeimprovementof4.3%acrossvarioustasks,includinga33%
improvementonthePassageRetrievaltask. Furthermore, ouranalysisrevealsstrongcorrelations
betweenEM-LLM’seventsegmentationandhuman-perceivedevents,suggestingabridgebetween
thisartificialsystemanditsbiologicalcounterpart. ThisworknotonlyadvancesLLMcapabilities
inprocessingextendedcontextsbutalsoprovidesacomputationalframeworkforexploringhuman
memorymechanisms,openingnewavenuesforinterdisciplinaryresearchinAIandcognitivescience.
1 INTRODUCTION
Forcontemporarypre-trainedlargelanguagemodels(LLMs),thecontextwindowservesastheprimarymechanism
to incorporate domain-specific, private, or common up-to-date information. However, despite their remarkable
and ever-expanding capabilities, LLMs still exhibit significant limitations when tasked with processing extensive
contexts (Liu et al., 2024a). These limitations stem from inherent challenges in Transformer-based architectures.
RecentstudieshaveshownthatTransformersstrugglewithextrapolatingtocontextslongerthantheirtrainingwindow
size(Kazemnejadetal.,2024). Ontopofthis,employingsoftmaxattentionoverextendedtokensequencesrequires
substantialcomputationalresourcesforeachtokengeneration,andtheresultingattentionembeddingsriskbecoming
excessivelynoisyandlosingtheirdistinctiveness(Tworkowskietal.,2023).
Tomitigatethosechallenges,recentworkshavefocusedonretrieval-basedmethods,eitherintheformofin-context
augmentation(e.g.,RAG-basedtechniques(Lewisetal.,2020;Gaoetal.,2024))orviaretrievalofpreviously-inferred
key-valuepairs(KV)withinindividualattentionheads(Wuetal.,2022;Tworkowskietal.,2023;Bertschetal.,2023).
Notably,state-of-the-artperformanceisachievedwhenKVpairsareinitiallyorganisedintonon-overlappingsegments
and then retrieved together as one block of sequential tokens (Xiao et al., 2024a). While such techniques present
interestingavenuesofresearch,resultsstillindicateasignificantgapbetweentheperformanceofLLMsinshort-vs
long-contexttasks,evenwhenexistinglong-contextarchitecturesareemployed(Liuetal.,2024a).
Thisworktacklestheabovechallengesandattemptstobridgethisperformancegapbytakinginspirationfromthe
algorithmic interpretation of episodic memory in the human brain – the memory system responsible for encoding,
storing,andretrievingpersonalexperiencesandevents. Thebrainmakessenseofitscontinuousexperienceinthe
realworldbysegmentingitintodiscreteepisodicevents(Clewettetal.,2019;Zacks,2020),whichareorganisedina
∗EqualContribution.
1
4202
luJ
21
]IA.sc[
1v05490.7042:viXraPreprint
hierarchicalandnested-timescalestructure(Baldassanoetal.,2017)andstoredinlong-termmemory. Notably,the
boundariesbetweensucheventsaretheaccesspointswhenitcomestomemoryretrieval(Michelmannetal.,2023a)
andarewidelybelievedtocorrespondtopointsintimewithhighpredictionerrorsbetweenthebrain’sgenerativemodel
anditsrawsensoryinput(a.k.a.,surprise). Inthiscontext,surprisereferstomomentswhenthebrain’spredictions
aboutincomingsensoryinformationaresignificantlyviolated,leadingtoamismatchbetweenwhatisexpectedand
whatisactuallyperceived. Theseinstancesofhighsurprisearethoughttosignalimportantchangesintheenvironment
ornarrative, promptingthebraintosegmenttheongoingexperienceintodistinctevents(Zacksetal.,2007;2011;
Roseboometal.,2019;Sinclairetal.,2021;Fountasetal.,2022). Oncesegmentedandstored,thebraincanrecall
episodic memories based on their similarity to its current experience, recency, original temporal order, and their
proximitytootherrecalledmemories(temporalasymmetryandcontiguity,HowardandKahana,2002).
Followingtheseinsights,weproposeanovelarchitecture,EM-LLM,thatintegratescrucialaspectsofeventcognition
andepisodicmemoryintoTransformer-basedLLMs. Formemoryformation,wesegmentthesequenceofthetokens
presentedtotheunderlyingLLMintoindividualmemoryunitsrepresentingepisodicevents. Theboundaries, and
thusthesizeofthoseevents,areinitiallydetermineddynamically,basedonthelevelofsurpriseofthemodelduring
inference,andthenrefinedtomaximisecohesionwithinmemoryunitsandseparationofmemorycontentacrossthem
(seeSection3.2). Thisrefinementprocessleveragesgraph-theoreticmetrics,treatingthesimilaritybetweenattention
keys (the learned representations used in Transformer self-attention mechanisms) as a weighted adjacency matrix,
andaimstoenhancethemodel’sabilitytoefficientlyrecallrelevantinformationwhenaddressingcomplextaskswith
extendedcontexts. Importantly,thismemoryformationprocessincursminimaladditionalcomputationalcost,withthe
surprise-basedsegmentationrequiringnoextracomputationandtherefinementstephavingacomplexityofO(kn),
wherekistypicallyverysmallcomparedtothenumberoftokensn. Withthisefficientmemoryformationprocess,by
groupingsimilarinformationinsingleunits,weminimisethenumberofunitsneededtorecalldetailsaroundspecific
events. Formemoryrecall,ourapproachintegratessimilarity-basedretrievalwithmechanismsthatfacilitatetemporal
contiguityandasymmetryeffects. Byretrievingandbufferingsalientmemoryunits,ourmodelleveragesandenhances
therecentlydiscoveredpropensityofLLMstoexhibithuman-likepatternsinsequentialinformationretrieval(Ji-An
et al., 2024). This method not only ensures efficient access to pertinent information but also mimics the temporal
dynamicsfoundinhumanfreerecallstudies(suchasHowardandKahana,2002),furtherenhancingthemodel’sability
tohandlecomplextasksthatrequirenuancedtemporalreasoning.
Toproveourhypotheses,wefirstemployaseriesofhuman-annotatedpodcastscripts,whereweshowthatinformation
inLLMattentionheadscanbesemanticallygroupedinawaythatcorrelateswiththeeventstructureperceivedby
humans.Therefore,LLM-perceivedsurprisecanindeedserveasaproxyforthecognitivesignalsthatdrivehumanevent
segmentation,asconfirmedbypreviousworks(Kumaretal.,2023). Then,usingthelong-contextPG-19dataset(Rae
etal.,2020),whichcomprisesadiversecorpusofEnglishbooks,weevaluatetheeffectivenessofbothstepsinour
segmentationmethodforgroupingrelevantinformation,andassesstheperformanceofdifferentrefinementobjectives.
Finally,weshowthatourmethodisscalableandsignificantlyoutperformsthestate-of-the-artmodelInfLLM(Xiao
etal.,2024a)onthewidely-usedLongBenchbenchmark(Baietal.,2023)forlong-contexttasks,achievinganoverall
relativeimprovementof4.3%,includingasubstantial33%improvementonthePassageRetrievaltask.
2 RELATED WORK
2.1 LONG-CONTEXTINLLMS
Recently,severalapproacheshavebeenproposedtoextendthecontextwindowofTransformer-basedmodels. Those
includemethodsthataddressthelimitedrepresentationalcapacityofsoftmaxattention,anditsquadraticcomputational
andmemorycost(Katharopoulosetal.,2020;Munkhdalaietal.,2024). Othermethodstargetthepoorextrapolation
oftypicalpositionalencodingstoout-of-distribution(OOD)contextlengths(Kazemnejadetal.,2024). Thelatter
isevidentinmostwidelyusedmethods,includingtheoriginalabsolutepositionalencodings(Vaswanietal.,2017)
and the more recent relative positional encodings, such as the popular Rotary Positional Embeddings (RoPE) (Su
etal.,2024). Toaddressthis,someapproachesproposescalingoftherotationangles(Chenetal.,2024a)orthebase
constant(Xiongetal.,2023;Liuetal.,2024b;Pengetal.,2024;Dingetal.,2024). Others,scalepositionswithout
affectingtheembeddingfunction(Pressetal.,2021;Chenetal.,2023;Jinetal.,2024),exploringalternativestrategies
suchasKERPLE(Chietal.,2022)andFIRE(Lietal.,2024)oradoptmechanismsfromcertainLMslikeT5(Raffel
etal.,2020).
Concerningcomputationalefficiencyanddilutedattention,successfulapproachesproposemethodsforgeneralimprove-
mentstotheefficiencyofTransformersthroughoptimisedcomputations(Dao,2024;Hanetal.,2024a;Aminabadi
etal.,2022;Kwonetal.,2023;Liuetal.,2024c;Brandonetal.,2023)orcompressiontechniques(Nawrotetal.,2024;
Zhangetal.,2023),aswellastrainingmethodstailoredforlong-contextscenarios(Zhuetal.,2024;Chenetal.,2024b).
2Preprint
Level 1: episodic attention Level 2: softmax attention on selected groups (contiguous view)
Normalised
event
Group A B C Group D
Figure1: Group-basedk-NNretrievalcanbeseenasaformofhierarchicalepisodicattention. Initially,k =4groups
oftokensareselected(left)andthenusedforsoftmaxattention(right),asifallothersimilarityscoreswereforcedtobe
zero(non-shadedareasoftheleftcurve). Thisframeworkcansupportmultiplelevelsofepisodicattention.
Anotherdirectionistheutilisationofretrieval-basedmethods,thevastmajorityofwhichreliesonavectordatabase
thatkeepsakey-valuecacheandscalableapproximationsofk-nearestneighbors(k-NNs)toperformlookups(Wuetal.,
2022;Tworkowskietal.,2023;Bertschetal.,2023). Interestingly,sinceusingakey-valuecachewithk-NNlookup
canbeseenasanapproximationofapplyingsoftmaxattentiontothefulltokensequence(seeAppendixA.4),k-NN
retrievalmethodscanbeusedwithoutanyfine-tuning(Bertschetal.,2023). Foranexceptionthatdoesnotrelyon
k-NNs,seeWangetal.(2023).
Arecentandinterestingvariantofk-NNretrievalinvolvesretrievinglargegroupsoftokens,ratherthanindividualones.
ModelsthatrelyonthisapproachincludeSLED(Ivgietal.,2023)andthemorerecentInfLLM(Xiaoetal.,2024a),
whichachievesstate-of-the-artperformanceonlong-contextbenchmarks. InfLLMsegmentstheentirecontextlength
intofixed-sizememoryunitsandemploysk-NNlookupusingthetokenswiththehighestaccumulatedscoresperunit.
Thiscanbeseenasaformofhierarchicalattention,asillustratedinFig.1. Whilegroup-basedretrievalrepresentsa
promisingdirection,ourapproachsignificantlyadvancesthisconceptbydynamicallydeterminingtokengroupingsina
mannerakintohumanmemoryformation,addressingafundamentallimitationofInfLLM’sfixed-sizesegmentation
andenablingmoreadaptiveandcontext-sensitiveprocessingofextendedinformation.
2.2 NEURALMODELSOFEPISODICMEMORYANDEVENTCOGNITION
Theconceptofepisodicmemory,centraltoourapproach,hasbeenextensivelystudiedinboththeoreticalneuroscience
andmachinelearning. Neuralmodelsofepisodicmemorycapturehumanbehaviourandneuroimagingdata,providing
insightsintohowthebrainprocessesandstoresexperiencesandsuggestinglinksbetweenmemory,efficientrepresenta-
tionsandnavigationofphysicalandconceptualspaces(Gershmanetal.,2012;BennaandFusi,2021). Inmachine
learning,episodicmemory-inspiredapproacheshaveyieldedsignificantimprovementsacrossvariousdomains. For
instance,episodiccontrolhasenhancedreinforcementlearningagents’performanceandlearningspeed(Blundelletal.,
2016;Pritzeletal.,2017;Coda-Fornoetal.,2024). Inaddition,modelsofmemoryconstructionandconsolidation
havebeensuccessfulinalleviatingcatastrophicforgettinginneuralnetworks(Kirkpatricketal.,2017;Lopez-Pazand
Ranzato,2017;Chaudhryetal.,2019;Buzzegaetal.,2020;Prabhuetal.,2020),includingLLMs(Dasetal.,2024),and
appeartoexplainkeyfeaturesofhumanmemory,suchasimaginationandfuturethinking(SpensandBurgess,2024).
Thesemodelshaverevealedkeyaspectsofepisodicmemory,particularlyindescribinghowexperiencesaresegmented
intoevents,andwhennewmemoriesareencodedandretrieved(Luetal.,2022). Surpriseplaysacriticalroleinthis
process,triggeringeventboundariesandmemoryformation(Fountasetal.,2022;Kumaretal.,2023). Thisevent-based
structureisdeeplyintertwinedwithourperceptionoftime(Roseboometal.,2019;Shermanetal.,2022),highlighting
theinterdependenceofmemoryandtemporalcognition. Thisinsighthashelpedgenerativemodelsforvideo(Zakharov
etal.,2022a;b)andreinforcementlearning(Zakharovetal.,2021)tocapturetemporaldynamicsmoreaccurately. In
termsofmemoryretrieval,studiesinhumanfreerecallhaveshownadistinctiveincreasedlikelihoodofretrievingitems
encodedclosetogetherintime(temporalcontiguity)andinsuccession(temporalasymmetry)(seeFig.2A).Recently,it
wasshownthatattentionheadsintransformer-basedLLMsthatareassociatedwithin-contextlearning,alreadyexhibit
thesamedynamicretrievalbehaviour(Ji-Anetal.,2024)(Fig.2B)includingbothcontiguityandasymmetryeffects.
Therefore,transformershavetheinherentabilitytoactasepisodicmemoryretrievalmodels,ifprovidedwiththeright
informationwithintheircontextwindow. Ourworkleveragestheseconceptsofsurprise-basedeventsegmentation
andLLMs’inherenttemporalcontiguityandasymmetryeffectstoenableanewgenerationofInfiniteContext-Length
LLMs,capableofprocessingandunderstandinginformationovervastlyextendedtimescales.
3Preprint
A C
surprise
1 ...
event 1 event 2 ...
2 ...
initial episodic memory local
Distance from recalled item tokens context
kNN
B
3 ... contigious contigious ...
event event
dequeue
enqueue
4 ...
initial contiguity buffer similarity buffer local
tokens context
Distance from recalled token
Figure2:(A)Exampleofthetemporalcontiguityandasymmetryeffectinhumanfreerecall. Dataaveragedoverseveral
largefreerecallstudies(adoptedfromHowardandKahana,2002). (B)TheattentionscoresofaGPT2headaveraged
overalltokenstested. FigureadoptedfromJi-Anetal.(2024). (C)Schematicillustratingourproposedprocessfor
memoryformationandretrieval: ①Inputsequencewithsurprise-basedsegmentation(purplearrowsindicatehigh
surprise). ②Formationofepisodicmemories: inputissegmentedintoeventsandstored,withinitialtokensandlocal
contextpreserved. Notethattheboundaryrefinementprocessisnotshownhereforclarity. ③Memoryretrievalvia
k-NNsearch,selectingcontiguouseventsfromepisodicmemory. ④Finalcontextwindowstructure,comprisinginitial
tokens,contiguitybuffer(populatedbyneighboringevents),similaritybuffer(fromk-NNretrieval),andlocalcontext.
3 EM-LLM: LLM WITH EPISODIC MEMORY
3.1 ARCHITECTURE
EM-LLMisdesignedtobeapplieddirectlytopre-trainedLLMs,enablingthemtohandlecontextlengthssignificantly
largerthantheiroriginaltraininglength. Ourarchitecturedividesthecontextintothreedistinctgroups: initialtokens,
evicted tokens, and local context. This structure, while incorporating insights from recent work on token block
retrieval(Xiaoetal.,2024a),introducesnovelelementsinspiredbyhumanepisodicmemory.
Thelocalcontextrepresentsthemostrecenttokens,maximisinginformationaboutthecurrenttask,andfitswithinthe
typicalcontextwindowoftheunderlyingLLM.Thisgrouputilisesfullsoftmaxattentionandplaysarolesimilartothe
focusofattentionincognitivemodelsofworkingmemory,holdingthemostimmediatelyrelevantinformationforthe
currenttask(Cowan,2001). Theevictedtokenstypicallycomprisethemajorityofpasttokensinalong-contextscenario,
extendingfarbeyondtheLLM’soriginaltraininglength. Thesetokensaremanagedbyourproposedmemorymodel
functioningsimilarlytoshort-termepisodicmemoryinthebrain. Finally,followingpreviouswork,wealsomaintaina
groupof128initialtokensintheLLMcontext. Theseactasattentionsinksandhelprecovertheperformanceofwindow
attention,asfirstobservedbyXiaoetal.(2024b);Hanetal.(2024b)andlateradoptedbyXiaoetal.(2024a). For
retrievedtokens,whicharethereforediscontinuousandoutsidethelocalcontext,weassignafixedpositionembedding
asinRaffeletal.(2020);Xiaoetal.(2024a). ThisarchitectureenablesEM-LLMtoeffectivelyprocessandutilise
information from positions outside its pre-trained local context window, while maintaining the underlying LLM’s
performancecharacteristics.
3.2 MEMORYFORMATIONVIASURPRISE
InthecontextofLLMs,wedefineepisodicmemoryastheorganised,event-basedcollectionofpastkey-valuepairs,
analogous to the latent representations of personal experiences in human memory. Just as unexpected or novel
informationplaysacrucialroleinhumanmemoryformation,wepositthatanalogousindicatorsofnoveltyinLLMs
canserveasaneffectiveproxyforidentifyingsignificant“events”withinthemodel’sexperience. InBayesianterms,
surpriseisquantifiedbythenegativelog-likelihoodofobservingthecurrent,ground-truthtokengiventheprevious
tokens in an auto-regressive model, with high values indicating the unpredictability or novelty of each new token
within the context according to the model, i.e., it is “surprised” by the next token. Following work on cognitive
4
esnopser
lanoitidnoC
ytilibaborp
erocs
noitnetta
egarevAPreprint
modelling(Roseboometal.,2019;Fountasetal.,2022),weemployathresholdingmechanismtoperformaninitial
identificationofeventboundaries(usedforthefirsttimeinLLMs). Formally, atokenx isconsideredapotential
t
boundaryifitssurprisevalueexceedsathresholdT:
−logP(x |x ,...,x ;θ)>T with T =µ +γσ (1)
t 1 t−1 t−τ t−τ
whereµ andσ2 arethemeanandvarianceofsurpriseforawindowoffsetτ,andγ isascalingfactor. The
t−τ:t t−τ:t
choiceofthresholdT iscriticalinbalancingthegranularityofsegmentationwiththemodel’ssensitivitytocontextual
shifts. IftheT istoohigh,wewillidentifyveryfeweventboundaries,especiallyifthelocalcontextcontainsfew
surprisingtokens. Conversely,alowT resultsinfrequentboundaryidentification. Usingamovingwindowensures
thatT adaptstocontextualshifts,minimizingtheneedformanualtuningwhilemaintainingcontroloverthreshold
sensitivityviaγ. Wealsoexploredafixedthresholdapproach(T =T ),thoughourprimaryfocusremainedonthe
fixed
dynamicthresholdduetoitsadaptabilitytovaryingcontexts. Thisinitialsegmentationresultsinasetofpotentialevent
boundariesB =b ,b ,...,b ,whereeachb representstheindexofatokenexceedingthesurprisethreshold. These
1 2 k i
boundariesserveasthestartingpointforoursubsequentrefinementprocess,whichaimstooptimisetheintra-event
coherenceandinter-eventdistinctivenessoftheresultingmemorysegments.
3.3 BOUNDARYREFINEMENT
Whilesurprise-basedsegmentationprovidesaneffectiveinitialestimateofeventboundaries,wemakethekeyobserva-
tionthattheutilityofelementswithinaneventduringmemoryrecalldependsontheirlikelihoodofbeingutilisedbythe
currentquery. Therefore,wetheorisethatmemoryrecallwillbemostefficientwithhighintra-eventsimilaritybetween
keyswhilemaintaininglowinter-eventsimilarity. Forinstance,seethesimilarityofgroupsinFig.1. Tofurtherensure
this,weintroduceaboundaryrefinementstepwhichlookstooptimisethisobjective. Suchanobjectiveistypically
optimisedinthecontextofgraph-clustering,hencewewillexpressthisrefinementprocessinagraph-theoreticmanner.
Toachievethis,wetreatthesimilaritymatrixbetweenallkeysofanattentionheadhwithinthelocalcontextwindow
fortokensx ,x ,...,x asanadjacencymatrix. WedefinetheadjacencymatrixAhas
1 2 n
Ah =sim(Kh,Kh), (2)
ij i j
whereKhandKharethekeyvectorscorrespondingtotokensx andx ,respectively. Thesimilarityfunctionmeasures
i j i j
theclosenessoftwokeyvectors;inourimplementation,weusedotproductsimilarityKhT ·Khduetoitseffectiveness
i j
incapturingsemanticrelationshipsinhigh-dimensionalspaces(Vaswanietal.,2017)andtoalignwiththemechanism
ofself-attentioninTransformers.
Toevaluatethequalityofpotentialboundaries,wedefineametricfunctionf(A,B):Rn×n×{1,...,n}k →R. This
functionquantifiesthecohesionwithineventsandseparationbetweeneventsbasedonthegraphstructurerepresentedby
thesimilaritymatrixAandeventboundariesB. Weexperimentedwithtwowidely-acceptedgraph-clusteringmetrics:
modularityandconductance(Miasnikofetal.,2018). Modularity(NewmanandGirvan,2004)providesameasureof
thequalityofaparticulardivisionofanetworkintocommunities,withhighervaluesindicatinghigheredgedensity
intheidentifiedclusterwhencomparedtothedensityofedgesexpectedinarandomcluster. Asouredgeweights
representthesimilaritybetweentwotokens,weseektomaximisethismetric. Modularityisdefinedas:
f (Ah,B)=
1 (cid:88)(cid:34)
Ah −
(cid:80) iAh
ij
·(cid:80) jAh ij(cid:35)
δ(c ,c ) (3)
M 4m ij 2m i j
i,j
wheremisthetotaledgeweightinthegraph,c isthecommunity(episodicevent)towhichnodeiisassigned,andδis
i
theKroneckerdeltafunction. Conductance,ontheotherhand,measuresthefractionoftotalweightededgescutbya
givencommunityboundary,andisdefinedas:
f (Ah,B)=min
(cid:80) i∈S,j∈/SAh
ij , withvol(S)= (cid:88) A , vol(V \S)= (cid:88) A (4)
C S∈V min(vol(S),vol(V \S)) ij ij
i∈S,j∈S i∈/S,j∈/S
whereS = {b ,b +1,...,b }isasubsetofallnodesV = {b ,b +1,...,b }intheinducedgraph,withb ∈ B.
i i i+1 1 1 k i
Lowerconductancevaluesindicatebettercommunitystructure. Ourboundaryrefinementalgorithmiterativelyadjusts
theinitialsurprise-basedboundariestooptimisethesemetricfunctions. Whileourbestresultswereachievedusing
modularity,wealsoincludecomparisonswithconductance-basedboundaryrefinementtoprovideacomprehensive
analysis. TheoverallprocesscanbesummarizedinAlgorithm1.
ThisalgorithmfirstidentifiesinitialboundariesbasedonthesurprisethresholdT,thenrefinestheseboundariesby
findingtheoptimalpositionβˆbetweeneachpairofconsecutiveinitialboundaries(α,β)thatoptimisesthechosen
5Preprint
Algorithm1EventsegmentationinKVcache
Input: tokens: Listoftokensinthesequence
Input: T: Thresholdforsurprisaltoidentifyinitialboundaries
Input: f: Metricfunctiontoevaluatepotentialboundaries
Output: B: Listoffinalboundarypositions
1: B ←[iforiinrange(length(tokens))if−log(P(tokens[i]))>T] ▷Boundaryidentification
2: foriinrange(length(B))do
3: α,β =B[i],B[i+1]
4: B[i+1]←argmax βˆ∈(α,β]f(A,{α,βˆ}) ▷Boundaryrefinement
5: endfor
6: returnB
metric function f (either maximising modularity or minimising conductance). This process ensures that the final
segmentation(1)capturespointsofhighsurpriseand(2)optimisesforcoherentinformationgrouping. Theboundary
identificationstepincursnegligiblecomputationalcost,asitonlyevaluatesexistingLLMoutputs. Thetimecomplexity
ofAlgorithm1isdominatedbytheboundaryrefinementstep,whichhasanoverallcomplexityofO(kn),wherek
isthenumberofinitialboundariesandnisthesequencelength. Adetailedanalysisofthealgorithm’scomplexity,
includingthecomputationofmodularityandconductancemetrics,isprovidedinAppendixA.2. Despitethismodest
computational overhead, the resulting improvement in segment quality leads to significant performance gains in
downstreamtasks,particularlythoserequiringcomplextemporalreasoning.
3.4 MEMORYRETRIEVAL
Wheninferringanewtoken,anumberofepisodiceventsareselectedandbecomeapartofthe(extended)context
window of the underlying LLM. Our memory retrieval process employs a two-stage mechanism to select relevant
episodiceventsfortheLLM’scontextwindow(Fig.2C).First,weretrievek eventsusingk-NNsearchbasedondot
s
productsimilaritybetweenthecurrentqueryandrepresentativetokensofeachevent. Theserepresentatives,selected
as per Xiao et al. (2024a), are the most influential tokens within each event. For large memory stores, we utilise
approximatek-NN(Douzeetal.,2024)tomaintainefficiency. Thesek events,retrievedbasedontheirsimilarityto
s
thecurrentquery,formapartoftheLLM’scontextwindowthatwerefertoasthesimilaritybuffer.
Thesecondstageofourretrievalprocessintroducesanotherbuffer,whichwerefertoasthecontiguitybuffer,designed
tomaintaintemporalcontext. Implementedasaqueueofsizek ,thisbufferpromotestemporalrelationshipsinretrieval.
c
Whenaneventisretrieved,wealsoenqueueitsneighboringevents(within±npositionsintheoriginalsequence)into
thisbuffer. ThismechanismenablestheLLM’s“induction”attentionheadstoexhibitthecontiguityandasymmetry
effectsdiscussedinSection2.2. Thequeuestructureallowsforanaturaldecayoftemporalcontextasneweventsare
processed,witholderorrepeatedeventsbeingdequeuedasnewonesareadded. Intotal,k =k +k +2eventsare
s c
addedtothecontextwindow,strikingabalancebetweenrelevanceandtemporalrelationshipsinamanneranalogousto
humanepisodicmemoryretrieval.
4 EXPERIMENTS
4.1 PERFORMANCEOFEM-LLMONLONG-CONTEXTTASKS
Aspreviouslymentioned,InfLLMis,atthetimeofwriting,consideredtoachievestate-of-the-artperformanceon
long-contextbenchmarks(∞-Bench,LongBench),aswellasbeingtheonlymethodwhichusesgroup-basedk-NN
retrievalinLLMsonsuchbenchmarks. We,therefore,employthismodelasourbaselineforcomparisonwithourown
methodsonshortcontextwindows(4K+2K,asinXiaoetal.,2024a).
ResultsontheLongBenchdataset(Table1)showthatourmethodisabletoimproveonInfLLMinallbutonetask,
withthebestmethodachievinganoverallincreaseinperformanceof1.8percentagepoints(arelativeimprovement
of4.3%). Notethatthetableshowsthebestsinglemethodintermsofoverallperformanceforeachablation. Looking
atindividualtaskperformanceacrossallexperiments,weareabletobeatInfLLMinalltasks(seeAppendixA.1).
Interestingly,weseeanespeciallylargejumpinperformanceonthePassageRetrievaltaskacrossallablations,withup
toa33%improvementonInfLLM.Thistaskrequiresthemodeltoidentifytheoriginalparagraphfromasummary,a
challengingtaskthatteststhemodel’sabilitytoaccuratelyrecallawiderangeofdetailedinformationfromalarge
contextconcurrently. Thesubstantialimprovementonthistaskhighlightstheeffectivenessofoureventsegmentation
methodinenhancinglong-termmemoryrecallandretrievalaccuracyinLLMs. Additionally,ourmethodachieves
6Preprint
EM-LLM
Task InfLLM MaxImp.
S SM S+C SM+C
NarrativeQA 22.12 +1.49% 21.32 21.13 21.80 22.45
MultiNews 26.70 -0.30% 26.52 26.54 26.69 26.62
Qasper 29.33 +0.17% 28.99 29.38 29.11 28.68
TREC 69.00 +2.17% 70.00 70.00 70.50 70.50
MultiFieldQA 47.42 +0.42% 47.49 47.39 47.46 47.62
TriviaQA 86.67 +1.10% 86.93 87.62 87.35 87.47
HotpotQA 36.56 +9.38% 39.99 39.01 39.05 38.90
SAMSum 42.52 +0.87% 42.34 42.13 42.89 42.48
2WikiMQA 22.31 +6.41% 23.74 22.75 22.65 23.46
PassageRetrieval 64.00 +33.47% 85.42 78.92 84.67 84.08
Musique 17.68 +6.17% 17.58 17.82 17.93 18.77
LCC 56.67 +0.63% 54.90 57.03 54.79 56.79
GovReport 31.03 +1.90% 31.24 31.62 31.34 31.43
RepoBench-P 52.97 +1.34% 50.76 53.68 51.34 52.86
QMSum 23.49 +2.13% 23.82 23.20 23.99 23.47
Avg. score: 41.90 +4.30% 43.40 43.22 43.44 43.70
Table1: EM-LLMperformanceonLongBenchcomparedtoourbaselineInfLLM.S:surprisethreshold,SM:surprise
threshold + refinement with modularity, S+C: surprise threshold + contiguity buffer, SM+C: surprise threshold +
refinementwithmodularity+contiguitybuffer. MaxImp. showsthemaximumrelativeimprovementoverInfLLM
acrossallEM-LLMvariants.
anotable9.38%improvementontheHotpotQAtask, whichinvolvescomplexreasoningovermultiplesupporting
documents,furtheremphasisingthemodel’sabilitytoprovideexplanationsforanswers.
4.2 HUMANSANDLLMSURPRISECLUSTERSIMILARTOKENSTOGETHER
AsmentionedinSection3.2,weemploymodularityandconductanceastworefinementobjectivesinourboundary
refinementalgorithm,duetotheirqualitiesinassessingtheintra-andinter-eventsimilaritiesbetweenindividualtokens.
Wewillnowusesuchmetricstocomparevariouseventsegmentationmethods,includinghumaneventsegmentationdata.
Additionally,weintroduceonefurther,simplemetricforthisexperiment: theratiobetweenintra-andinter-community
similarity(I/IS),calculatedforeachheadandcommunityS asfollows:
(cid:88) (cid:88) intra
intra= A , inter= A , I/IS≡ (5)
ij ij
inter
i∈S,j∈S i∈S,j∈/S
Kumaretal.(2023)foundstrongcorrelationsbetweenhuman-perceivedeventsandpredictionerrorsacross3short
podcasts(7-30minutesonaverage),whenprocessingthecorrespondingtranscriptwithanLLM.Takingadvantageof
suchhumandataandresultsfrompreviousworksonthisdataset(Michelmannetal.,2021;Lositskyetal.,2016),we
comparethesegmentationqualityandcorrelationwithhumandataforeachofourmethods(3)usingoursimilarity
metrics.
AsshowninFig.3A,human-perceivedeventsachievesignificantlyhigherscoresinsimilaritymetricscomparedto
fixed or random events, suggesting that surprise is indeed an important factor for humans in their own perception
ofevents. Furthermore,surprise-onlysegmentation(S)achievesverysimilarresultstohumans,whiletheaddition
ofourrefinementalgorithm(SM, SC, FM, FC)significantlyimprovesperformance. Fig.3Bfurthershowsthat
surprise-basedmethods(S,SM,SC),consistentlyidentifyeventboundariesthatareclosesttothoseperceivedby
humans.
4.3 COMPARINGSEGMENTATIONMETHODS
LookingatTable2,itisclearthatsurprise-basedsegmentationwithrefinement(SM,SC)providesthebestresultsin
termsofeventsimilaritymetrics,regardlessofthebaseLLMused. Whilethesurprise-onlymethod(S)achievessome
goodresults,weobservethatrefinementisespeciallyadepttoimprovingthisperformancewithregardstoourmetrics,
asitisdirectlyoptimisingforsuchanobjective. Interestinglyhowever,thefixed-basedrefinementmethods(FM,FC)
donotreachthesameperformanceastheirsurprise-basedcounterparts,furthershowingthattheinitialsegmentation
withasurprisethresholdiscrucialtoachievingthebestpossiblebalanceinintra-/inter-similaritywithourmethods.
7Preprint
Figure3: Comparisonofhumaneventsegmentationwithdifferentcomputationalsegmentationmethodsintwohuman-
annotatedaudiodatasets(seeAppendixA.3). (A)DifferenceinmetricsforthecohesionandseparationofKVcacheof
LLaMA2attentionheads.Thegraphsreportthedifferenceofeachmethodwiththecorrespondingrandomsegmentation.
(B)Distancebetweenhumanreportsanddifferentmethods. Inbothsetsofresults,fixedmethods(F,FM,FC)perform
worsethantheirsurprise-basedcounterparts(S,SM,SC)withInfLLM’smethod(F)performingworsethanrandom.
LLM Metric F FM FC S SM SC
Mod↑ -2.3±4.1 29.2±44.0 6.7±25.9 18.6±29.6 39.9±55.5 29.5±42.7
Mistral-
Con↓ 9.1±8.7 -16.9±6.7 -12.5±9.6 -23.6±9.4 -24.6±9.3 -27.6±9.8
7B
I/IS↑ -4.3±4.0 31.2±21.4 3.7±14.9 17.9±17.0 35.3±27.7 21.6±22.4
Mod↑ -1.1±4.3 13.4±19.5 0.6±7.3 8.7±16.0 18.7±26.4 11.5±19.4
LLaMA2-
Con↓ 11.9±9.8 -18.8±7.4 -13.7±10.9 -29.5±10.2 -29.7±10.1 -33.3±10.3
7B
I/IS↑ -3.8±3.7 20.7±184.7 -1.1±6.8 15.0±880.0 25.0±19.9 16.5±15.4
Mod↑ -1.6±3.6 18.9±25.6 0.9±11.8 13.1±21.5 27.0±35.6 18.3±28.5
LLaMA3-
Con↓ 11.3±9.5 -20.3±6.9 -14.6±11.4 -29.7±9.2 -30.6±9.2 -33.9±9.6
8B
I/IS↑ -3.8±3.1 24.5±13.9 -1.1±5.8 15.7±11.0 28.1±16.1 16.4±12.2
Table 2: Comparison with graph-theoretic metrics in the KV cache of different LLMs and segmentation methods
using the PG-19 dataset. Reported values are the difference with the corresponding random segmentation. Mod:
modularity×105,Con: Conductance,I/IS:intra/inter-similarity×103. Hyper-parameters: Surprisethreshold=0.001
4.4 SIMILARITY,CONTIGUITY,RECENCYANDTEMPORALORDER
AsdemonstratedinTables1and2,alongwithFig.3,eachofourablationsshowvariouspositiveimprovementson
InfLLM(alsoseeAppendixA.1).AsmentionedinSection4.3,refinementhasastrongpositiveimpactinimprovingour
similaritymetrics. ThisisseentotranslatewelltomodelperformanceinTable1,achievingthebestperformanceina
thirdofthetasks,aswellasagreeingwithhumandata(Fig.3). Theeffectsofcontiguityarealsoclearlydemonstratedin
thistable,withtheadditionofourcontiguitybufferachievingthebestperformanceonthreetasks,andthesecond-best
overall score. Furthermore, these methods are shown to be generally complementary, achieving the best overall
performancewhencombined.
However,thefactthatcertaintasksstillappeartobenefitmorefromeithersurprise-only,refinement,orcontiguity,
isaninterestingresult. Thisislikelyduetothenatureofthetasksandthevaryingimportanceofcontiguityacross
thesetasks. Forinstance,inSupplementaryFig.5,theMultiNewstaskscoreshigherthanourbaselineonlyforaratio
of70%contiguitytosimilaritybuffers. Wherecontiguityisnotcrucial,addingsuchabuffertoourcontextwindow
alsoreducesthesizeofthesimilaritybuffer,andthereforeprovidespotentiallylessdirectlyrelevantevents. Thisis
compatiblewithourownfindingsthatacontiguitybufferthatisasbigorsmallerthanthesimilaritybufferyieldsthe
bestresults,suggestingthatthesimilaritybuffer,isstillthemostcrucialpartofourapproach. Thisisespeciallythe
8Preprint
casewhencombinedwithrefinement,whichweexpectisduetotheimprovedsimilarityofrefinedevents,hencefurther
reducingtheneedforcontiguousevents.
5 DISCUSSION
Humanstudies Thesurprise-basedsegmentationandboundaryrefinementprocessesinEM-LLMmirrorkeyaspects
ofhumaneventperceptionandmemoryformation. Ourapproachalignswiththeoriesproposingthathumanssegment
continuous experiences into discrete events based on prediction errors or moments of surprise (Zacks et al., 2007;
Fountas et al., 2022). This segmentation process is crucial for organising and later retrieving episodic memories
efficiently. Indeed,significantcorrelationshavebeenfoundbetweenhumaneventsegmentationandpredictionerrorsin
bothLLMs(Kumaretal.,2023)andvideomodels(Fountasetal.,2022;Mariolaetal.,2022). Ourresultsaddtothis
growingbodyofevidence,demonstratingthatLLM-basedsurprisecanserveasaproxyforhumaneventsegmentation,
in multiple levels of hierarchical abstraction, and that the resulting event structure in EM-LLM’s attention heads
correlatesstronglywithhuman-perceivedevents. Thisfindingcreatesamoredirect,low-levelconnectionbetweenLLM
mechanismsandhumancognitiveprocesses. Furthermore,ourmodel’suseofbothsimilarity-basedandtemporally
contiguous retrievalmechanisms parallels human memoryretrieval patterns, allowing forthe expressionof robust
phenomenafoundinhumanmemoryresearch(HowardandKahana,2002).
Furthermore,ourmodel’suseofbothsimilarity-basedandtemporallycontiguousretrievalmechanismsparallelshuman
memoryretrievalpatterns. Thetemporalcontiguityeffect,whereitemsexperiencedclosetogetherintimeareoften
recalledtogether,isarobustphenomenoninhumanmemoryresearch(HowardandKahana,2002). Furtherexperiments
could deepen our understanding of the connections between EM-LLM and human episodic memory. Following
Michelmannetal.(2023b),onepotentialdirectionistotestwhetherthetimingoftheeventboundariesorthedegreeof
modularityperlevelthatourmethodproducesiscloseronaveragetothehumanconsensus,thanindividualhuman
subjects. Second,wecanexplorethelevelatwhichdifferentratiosofthecontiguitybufferallowthehumanbiases
presentedinFig.2AandtheanalysisinJi-Anetal.(2024)tobemoreeasilyreproduced. Finally,wecouldinvestigate
howskewingeventrecallbasedonrecencyandoriginally-recordedsurpriseaffectsmodelperformanceandtowhat
extentitproducesbiasedbehaviourfoundinstudiesoffreerecall.
Inaddition,thearchitectureofEM-LLM,withitsdifferentiatedcontexthandlingdescribedinSection3.1,invitesfurther
interestingcomparisonstocognitivemodelsofhumanmemorybeyondepisodic. Thegroupoftokensformingthelocal
context,whichholdthemostrecentandtask-relevantinformation,sharecharacteristicswiththeconceptofworking
memory. Forinstance,Baddeley(2003)’sinfluentialmodelofworkingmemory,whichpositsalimited-capacitysystem
for temporary information storage and manipulation, bears similarities to our local context functionality. Yet, the
analogyisnotperfect. Ourbroadercontextwindow,includingbothlocalcontextandretrievedmemories,mightbe
moreaccuratelycomparedtoEricssonandKintsch(1995)’sconceptoflong-termworkingmemory,whichproposesa
mechanismforrapidaccesstorelevantinformationinlong-termmemory,extendingbeyondthetraditionalcapacity
limitsofworkingmemory. Alternatively,ourarchitecturealignswellwithCowan(2001)’sembedded-processesmodel,
whereourlocalcontextcouldbelikenedtothelimited-capacity“focusofattention”withinworkingmemory,whilethe
fullcontextwindowparallelstheactivatedportionoflong-termmemory. Futureworkcouldexploretheseanalogies
more deeply, providing a flexible test-bed for rapidly exploring hypotheses about human memory, and potentially
informingdebatesaboutcapacitylimitsinworkingmemory. Additionally,inspiredbythemulti-componentnature
ofBaddeley’smodel,onemightexploretheintegrationofmodality-specificbufferswithinEM-LLMtoenhanceits
performanceonmulti-modaltasks.
Machinelearning Inrefiningeventboundaries,weutilizedmodularityandconductanceasmetricsforevaluating
communitystructureinthesimilaritygraphofattentionkeys. Whileeffectiveinourexperiments,weacknowledge
thatnumerousothermethodsforgraphclusteringandsequencesegmentationcouldpotentiallybeapplied(Fortunato,
2010;Yangetal.,2016). Ourchoicewasmotivatedbytheirestablishedtheoreticalfoundationsandcomputational
efficiency,thoughcomparativestudiessuggestperformancecanvarybasedonnetworkcharacteristics(Yangetal.,
2016). Interestingly,oursurprise-basedinitialboundarydetectionsharessimilaritieswithBayesianonlinechange-point
detection(AdamsandMacKay,2007),suggestingpotentialavenuesforintegratingtimeseriesanalysistechniquesinto
LLMcontextprocessing. Futureworkcouldexplorewhethermoresophisticatedsegmentationorclusteringalgorithms
couldimproveEM-LLM’sperformance,particularlyforextremelylongcontextsorstreamingdatascenarios. Such
investigationscouldenhanceourmodelandcontributetounderstandinghowinformationisstructuredandprocessedin
LLMs,bridgingthegapbetweentraditionalsequenceanalysisandLLMcontextprocessing.
Looking ahead, several more avenues for future research emerge from this work. One promising direction is to
extendoursurprise-basedsegmentationandboundaryrefinementprocessestooperateateachlayeroftheTransformer
independently. Thiscouldleadtomorenuancedandhierarchicalrepresentationsofepisodicmemories, following
9Preprint
theunderlyingsemanticstructureoftheinputmoreclosely. Additionally,exploringhowEM-LLMcouldbeutilised
toenableimaginationandfuturethinkinghasgreatpotentialforadvancingmodel-basedreinforcementlearningand
continuallearningtechniquesinLLMs. Byleveragingitsevent-basedstructuretosimulatepotentialfuturescenarios
or recall past experiences in novel contexts, EM-LLM could enhance an LLM’s ability to plan, adapt, and learn
continuouslyfromnewinformation.
6 CONCLUSION
Inthiswork,weintroducedEM-LLM,anovelandflexiblearchitecturethatintegrateskeyaspectsofhumanepisodic
memory and event cognition into transformer-based language models. Our approach enables LLMs to effectively
processandutiliseinformationfromvastlyextendedcontexts,farbeyondtheiroriginaltraininglengths. Bycombining
surprise-basedeventsegmentationwithgraph-theoreticboundaryrefinement,andatwo-stagememoryretrievalprocess,
EM-LLMdemonstratessuperiorperformanceonlong-contexttaskscomparedtostate-of-the-artmodels. Crucially,our
methodrequiresnopre-trainingandcanbereadilyappliedtoexistingLLMs,offeringapromisingpathtowardsvirtually
infinite context windows. This capability has the potential to revolutionise how we interact with LLMs, enabling
continuous,personalizedinteractionsoverextendedperiods. Furthermore,theflexibilityofourframeworksuggestsit
couldserveasaviablealternativetotraditionalretrieval-augmentedgeneration(RAG)techniques,especiallywhen
combinedwithefficientcompressionmethodstoreducethememoryrequirementsforthemodel’sKVcache.
Inconclusion,EM-LLMrepresentsasignificantstepforwardinthedevelopmentoflanguagemodelswithextended
context-processingcapabilities. Bybridginginsightsfromcognitivesciencewithmachinelearning,ourapproachnot
onlyenhancestheperformanceofLLMsonlong-contexttasksbutalsoprovidesascalablecomputationalframework
fortestinghypothesesabouthumanmemory. Wehopethisstudywillinspirethecommunitytoexpandresearchonthe
intersectionbetweenLLMsandhumanmemorymechanisms.
REFERENCES
NelsonFLiu,KevinLin,JohnHewitt,AshwinParanjape,MicheleBevilacqua,FabioPetroni,andPercyLiang. Lostin
themiddle: Howlanguagemodelsuselongcontexts. TransactionsoftheAssociationforComputationalLinguistics,
12:157–173,2024a.
AmirhosseinKazemnejad,InkitPadhi,KarthikeyanNatesanRamamurthy,PayelDas,andSivaReddy. Theimpactof
positionalencodingonlengthgeneralizationintransformers. AdvancesinNeuralInformationProcessingSystems,
36,2024.
SzymonTworkowski,KonradStaniszewski,MikołajPacek,YuhuaiWu,HenrykMichalewski,andPiotrMiłos´. Focused
transformer:Contrastivetrainingforcontextscaling. InThirty-seventhConferenceonNeuralInformationProcessing
Systems,2023. URLhttps://openreview.net/forum?id=s1FjXzJ0jy.
PatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,NamanGoyal,HeinrichKüttler,
MikeLewis,Wen-tauYih,TimRocktäschel,etal. Retrieval-augmentedgenerationforknowledge-intensivenlptasks.
AdvancesinNeuralInformationProcessingSystems,33:9459–9474,2020.
YunfanGao,YunXiong,XinyuGao,KangxiangJia,JinliuPan,YuxiBi,YiDai,JiaweiSun,QianyuGuo,MengWang,
andHaofenWang. Retrieval-augmentedgenerationforlargelanguagemodels: Asurvey,2024.
Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. In
InternationalConferenceonLearningRepresentations,2022. URLhttps://openreview.net/forum?id=
TrjbxzRcnf-.
Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R. Gormley. Unlimiformer: Long-range transformers
withunlimitedlengthinput. InThirty-seventhConferenceonNeuralInformationProcessingSystems,2023. URL
https://openreview.net/forum?id=lJWUJWLCJo.
ChaojunXiao,PengleZhang,XuHan,GuangxuanXiao,YankaiLin,ZhengyanZhang,ZhiyuanLiu,SongHan,and
MaosongSun. Infllm: Unveilingtheintrinsiccapacityofllmsforunderstandingextremelylongsequenceswith
training-freememory,2024a.
DavidClewett,SarahDuBrow,andLilaDavachi. Transcendingtimeinthebrain: Howeventmemoriesareconstructed
fromexperience. Hippocampus,29(3):162–183,2019.
JeffreyMZacks. Eventperceptionandmemory. Annualreviewofpsychology,71:165–191,2020.
10Preprint
Christopher Baldassano, Janice Chen, Asieh Zadbood, Jonathan W Pillow, Uri Hasson, and Kenneth A Norman.
Discoveringeventstructureincontinuousnarrativeperceptionandmemory. Neuron,95(3):709–721,2017.
Sebastian Michelmann, Uri Hasson, and Kenneth A. Norman. Evidence that event boundaries are access points
for memory retrieval. Psychological Science, 34(3):326–344, 2023a. doi:10.1177/09567976221128206. URL
https://doi.org/10.1177/09567976221128206. PMID:36595492.
JeffreyMZacks,NicoleKSpeer,KhenaMSwallow,ToddSBraver,andJeremyRReynolds. Eventperception: a
mind-brainperspective. Psychologicalbulletin,133(2):273,2007.
JeffreyMZacks,ChristopherAKurby,MichelleLEisenberg,andNayiriHaroutunian. Predictionerrorassociatedwith
theperceptualsegmentationofnaturalisticevents. Journalofcognitiveneuroscience,23(12):4057–4066,2011.
Warrick Roseboom, Zafeirios Fountas, Kyriacos Nikiforou, David Bhowmik, Murray Shanahan, and Anil K Seth.
Activityinperceptualclassificationnetworksasabasisforhumansubjectivetimeperception.Naturecommunications,
10(1):267,2019.
AlyssaH.Sinclair,GraceM.Manalili,IvaK.Brunec,R.AlisonAdcock,andMorganD.Barense. Predictionerrors
disrupthippocampalrepresentationsandupdateepisodicmemories.ProceedingsoftheNationalAcademyofSciences,
118(51):e2117625118,2021. doi:10.1073/pnas.2117625118. URLhttps://www.pnas.org/doi/abs/10.
1073/pnas.2117625118.
ZafeiriosFountas,AnastasiaSylaidi,KyriacosNikiforou,AnilK.Seth,MurrayShanahan,andWarrickRoseboom. A
PredictiveProcessingModelofEpisodicMemoryandTimePerception. NeuralComputation,34(7):1501–1544,06
2022. ISSN0899-7667. doi:10.1162/neco_a_01514. URLhttps://doi.org/10.1162/neco_a_01514.
MarcWHowardandMichaelJKahana. Adistributedrepresentationoftemporalcontext. Journalofmathematical
psychology,46(3):269–299,2002.
LiJi-An,CoreyY.Zhou,MarcusK.Benna,andMarceloG.Mattar. Linkingin-contextlearningintransformersto
humanepisodicmemory,2024.
ManojKumar,ArielGoldstein,SebastianMichelmann,JeffreyMZacks,UriHasson,andKennethANorman.Bayesian
surprisepredictshumaneventsegmentationinstorylistening. Cognitivescience,47(10):e13343,2023.
Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. Compressive
transformersforlong-rangesequencemodelling. InInternationalConferenceonLearningRepresentations,2020.
URLhttps://openreview.net/forum?id=SylKikSYDH.
YushiBai,XinLv,JiajieZhang,HongchangLyu,JiankaiTang,ZhidianHuang,ZhengxiaoDu,XiaoLiu,AohanZeng,
LeiHou,YuxiaoDong,JieTang,andJuanziLi. Longbench: Abilingual,multitaskbenchmarkforlongcontext
understanding. arXivpreprintarXiv:2308.14508,2023.
AngelosKatharopoulos,ApoorvVyas,NikolaosPappas,andFrançoisFleuret. Transformersarernns: Fastautoregres-
sivetransformerswithlinearattention. InInternationalconferenceonmachinelearning,pages5156–5165.PMLR,
2020.
TsendsurenMunkhdalai,ManaalFaruqui,andSiddharthGopal. Leavenocontextbehind: Efficientinfinitecontext
transformerswithinfini-attention,2024.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,andIllia
Polosukhin. Attentionisallyouneed. Advancesinneuralinformationprocessingsystems,30,2017.
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced
transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. ISSN 0925-2312.
doi:https://doi.org/10.1016/j.neucom.2023.127063. URLhttps://www.sciencedirect.com/science/
article/pii/S0925231223011864.
GuanzhengChen,XinLi,ZaiqiaoMeng,ShangsongLiang,andLidongBing. CLEX:Continuouslengthextrapolation
forlargelanguagemodels. InTheTwelfthInternationalConferenceonLearningRepresentations,2024a. URL
https://openreview.net/forum?id=wXpSidPpc5.
WenhanXiong,JingyuLiu,IgorMolybog,HejiaZhang,PrajjwalBhargava,RuiHou,LouisMartin,RashiRungta,
Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. arXiv
preprintarXiv:2309.16039,2023.
11Preprint
Xiaoran Liu, Hang Yan, Chenxin An, Xipeng Qiu, and Dahua Lin. Scaling laws of roPE-based extrapolation. In
TheTwelfthInternationalConferenceonLearningRepresentations,2024b. URLhttps://openreview.net/
forum?id=JO7k0SJ5V6.
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context window extension
of large language models. In The Twelfth International Conference on Learning Representations, 2024. URL
https://openreview.net/forum?id=wHBfxhZu1u.
YiranDing,LiLynaZhang,ChengruidongZhang,YuanyuanXu,NingShang,JiahangXu,FanYang,andMaoYang.
Longrope: Extendingllmcontextwindowbeyond2milliontokens. arXivpreprintarXiv:2402.13753,2024.
OfirPress,NoahASmith,andMikeLewis. Trainshort,testlong: Attentionwithlinearbiasesenablesinputlength
extrapolation. arXivpreprintarXiv:2108.12409,2021.
ShouyuanChen,ShermanWong,LiangjianChen,andYuandongTian. Extendingcontextwindowoflargelanguage
modelsviapositionalinterpolation. arXivpreprintarXiv:2306.15595,2023.
HongyeJin,XiaotianHan,JingfengYang,ZhimengJiang,ZiruiLiu,Chia-YuanChang,HuiyuanChen,andXiaHu.
Llmmaybelonglm: Self-extendllmcontextwindowwithouttuning,2024.
Ta-ChungChi, Ting-HanFan, PeterRamadge, andAlexanderRudnicky. KERPLE:Kernelizedrelativepositional
embeddingforlengthextrapolation. InAliceH.Oh,AlekhAgarwal,DanielleBelgrave,andKyunghyunCho,editors,
AdvancesinNeuralInformationProcessingSystems,2022. URLhttps://openreview.net/forum?id=
hXzOqPlXDwm.
ShandaLi,ChongYou,GuruGuruganesh,JoshuaAinslie,SantiagoOntanon,ManzilZaheer,SumitSanghai,Yiming
Yang, Sanjiv Kumar, and Srinadh Bhojanapalli. Functional interpolation for relative positions improves long
contexttransformers. InTheTwelfthInternationalConferenceonLearningRepresentations,2024. URLhttps:
//openreview.net/forum?id=rR03qFesqk.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi,and
PeterJ.Liu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer. JournalofMachine
LearningResearch,21(140):1–67,2020. URLhttp://jmlr.org/papers/v21/20-074.html.
Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth Inter-
national Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=
mZn2Xyh9Ec.
Insu Han, Rajesh Jayaram, Amin Karbasi, Vahab Mirrokni, David Woodruff, and Amir Zandieh. Hyperattention:
Long-contextattentioninnear-lineartime. InTheTwelfthInternationalConferenceonLearningRepresentations,
2024a. URLhttps://openreview.net/forum?id=Eh0Od2BJIM.
Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji
Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, and Yuxiong He. Deepspeed-inference: enabling efficient
inferenceoftransformermodelsatunprecedentedscale. InProceedingsoftheInternationalConferenceonHigh
PerformanceComputing,Networking,StorageandAnalysis,SC’22.IEEEPress,2022. ISBN9784665454445.
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao
Zhang,andIonStoica. Efficientmemorymanagementforlargelanguagemodelservingwithpagedattention. In
Proceedingsofthe29thSymposiumonOperatingSystemsPrinciples,SOSP’23,page611–626,NewYork,NY,
USA,2023.AssociationforComputingMachinery. ISBN9798400702297. doi:10.1145/3600006.3613165. URL
https://doi.org/10.1145/3600006.3613165.
HaoLiu,MateiZaharia,andPieterAbbeel. Ringattentionwithblockwisetransformersfornear-infinitecontext. In
TheTwelfthInternationalConferenceonLearningRepresentations,2024c. URLhttps://openreview.net/
forum?id=WsRHpHH4s0.
WilliamBrandon,AniruddhaNrusimha,KevinQian,ZacharyAnkner,TianJin,ZhiyeSong,andJonathanRagan-Kelley.
Stripedattention: Fasterringattentionforcausaltransformers. arXivpreprintarXiv:2311.09431,2023.
PiotrNawrot,AdrianŁan´cucki,MarcinChochowski,DavidTarjan,andEdoardoMPonti. Dynamicmemorycompres-
sion: Retrofittingllmsforacceleratedinference. arXivpreprintarXiv:2403.09636,2024.
12Preprint
ZhenyuZhang, YingSheng, TianyiZhou, TianlongChen, LianminZheng, RuisiCai, ZhaoSong, YuandongTian,
ChristopherRe,ClarkBarrett,ZhangyangWang,andBeidiChen. H2o: Heavy-hitteroracleforefficientgenerative
inferenceoflargelanguagemodels. InThirty-seventhConferenceonNeuralInformationProcessingSystems,2023.
URLhttps://openreview.net/forum?id=RkRrPp7GKO.
Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. PoSE: Efficient context
windowextensionofLLMsviapositionalskip-wisetraining. InTheTwelfthInternationalConferenceonLearning
Representations,2024. URLhttps://openreview.net/forum?id=3Z1gxuAQrA.
YukangChen,ShengjuQian,HaotianTang,XinLai,ZhijianLiu,SongHan,andJiayaJia. LongloRA:Efficientfine-
tuningoflong-contextlargelanguagemodels. InTheTwelfthInternationalConferenceonLearningRepresentations,
2024b. URLhttps://openreview.net/forum?id=6PmJoRfdaK.
WeizhiWang,LiDong,HaoCheng,XiaodongLiu,XifengYan,JianfengGao,andFuruWei. Augmentinglanguage
modelswithlong-termmemory. InThirty-seventhConferenceonNeuralInformationProcessingSystems,2023.
URLhttps://openreview.net/forum?id=BryMFPQ4L6.
MaorIvgi,UriShaham,andJonathanBerant. Efficientlong-textunderstandingwithshort-textmodels. Transactions
oftheAssociationforComputationalLinguistics, 11:284–299, 2023. doi:10.1162/tacl_a_00547. URLhttps:
//aclanthology.org/2023.tacl-1.17.
SamuelJGershman,ChristopherDMoore,MichaelTTodd,KennethANorman,andPerBSederberg. Thesuccessor
representationandtemporalcontext. NeuralComputation,24(6):1553–1568,2012.
MarcusKBennaandStefanoFusi. Placecellsmaysimplybememorycells: Memorycompressionleadstospatial
tuningandhistorydependence. ProceedingsoftheNationalAcademyofSciences,118(51):e2018422118,2021.
Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z Leibo, Jack Rae, Daan
Wierstra,andDemisHassabis. Model-freeepisodiccontrol. arXivpreprintarXiv:1606.04460,2016.
AlexanderPritzel,BenignoUria,SriramSrinivasan,AdriàPuigdomènechBadia,OriolVinyals,DemisHassabis,Daan
Wierstra,andCharlesBlundell. Neuralepisodiccontrol. InDoinaPrecupandYeeWhyeTeh,editors,Proceedingsof
the34thInternationalConferenceonMachineLearning,volume70ofProceedingsofMachineLearningResearch,
pages2827–2836.PMLR,06–11Aug2017.
JulianCoda-Forno,ChangminYu,QinghaiGuo,ZafeiriosFountas,andNeilBurgess. Leveragingepisodicmemoryto
improveworldmodelsforreinforcementlearning. InMemoryinArtificialandRealIntelligence(MemARI)Workshop
at36thConferenceonNeuralInformationProcessingSystems(NeurIPS2022),2024.
JamesKirkpatrick, RazvanPascanu, NeilRabinowitz, JoelVeness, GuillaumeDesjardins, AndreiARusu, Kieran
Milan,JohnQuan,TiagoRamalho,AgnieszkaGrabska-Barwinska,etal. Overcomingcatastrophicforgettingin
neuralnetworks. Proceedingsofthenationalacademyofsciences,114(13):3521–3526,2017.
DavidLopez-PazandMarc'AurelioRanzato. Gradientepisodicmemoryforcontinuallearning. InI.Guyon,U.Von
Luxburg,S.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett,editors,AdvancesinNeuralInformation
ProcessingSystems,volume30.CurranAssociates,Inc.,2017. URLhttps://proceedings.neurips.cc/
paper_files/paper/2017/file/f87522788a2be2d171666752f97ddebb-Paper.pdf.
ArslanChaudhry,Marc’AurelioRanzato,MarcusRohrbach,andMohamedElhoseiny. Efficientlifelonglearningwith
a-GEM. InInternationalConferenceonLearningRepresentations,2019. URLhttps://openreview.net/
forum?id=Hkf2_sC5FX.
PietroBuzzega,MatteoBoschini,AngeloPorrello,DavideAbati,andSIMONECALDERARA. Darkexperiencefor
generalcontinuallearning: astrong,simplebaseline. InH.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,and
H.Lin,editors,AdvancesinNeuralInformationProcessingSystems,volume33,pages15920–15930.CurranAs-
sociates,Inc.,2020. URLhttps://proceedings.neurips.cc/paper_files/paper/2020/file/
b704ea2c39778f07c617f6b7ce480e9e-Paper.pdf.
AmeyaPrabhu,PhilipH.S.Torr,andPuneetK.Dokania. Gdumb: Asimpleapproachthatquestionsourprogressin
continuallearning. InAndreaVedaldi,HorstBischof,ThomasBrox,andJan-MichaelFrahm,editors,Computer
Vision–ECCV2020,pages524–540,Cham,2020.SpringerInternationalPublishing.
13Preprint
PayelDas,SubhajitChaudhury,ElliotNelson,IgorMelnyk,SarathSwaminathan,SihuiDai,AurélieLozano,Georgios
Kollias,VijilChenthamarakshan,SohamDan,etal. Larimar: Largelanguagemodelswithepisodicmemorycontrol.
arXivpreprintarXiv:2403.11901,2024.
Eleanor Spens and Neil Burgess. A generative model of memory construction and consolidation. Nature Human
Behaviour,pages1–18,2024.
QihongLu,UriHasson,andKennethANorman. Aneuralnetworkmodelofwhentoretrieveandencodeepisodic
memories. elife,11:e74445,2022.
MaxineTSherman,ZafeiriosFountas,AnilKSeth,andWarrickRoseboom. Trial-by-trialpredictionsofsubjective
timefromhumanbrainactivity. PLOSComputationalBiology,18(7):e1010223,2022.
AlexeyZakharov,QinghaiGuo,andZafeiriosFountas. Variationalpredictiveroutingwithnestedsubjectivetimescales.
InInternationalConferenceonLearningRepresentations,2022a. URLhttps://openreview.net/forum?
id=JxFgJbZ-wft.
AlexeyZakharov,QinghaiGuo,andZafeiriosFountas. Long-horizonvideopredictionusingadynamiclatenthierarchy.
arXivpreprintarXiv:2212.14376,2022b.
AlexeyZakharov,MatthewCrosby,andZafeiriosFountas. Episodicmemoryforsubjective-timescalemodels. InICML
2021WorkshoponUnsupervisedReinforcementLearning,2021. URLhttps://openreview.net/forum?
id=30lZDhrjonR.
NelsonCowan. Themagicalnumber4inshort-termmemory: Areconsiderationofmentalstoragecapacity. Behavioral
andbrainsciences,24(1):87–114,2001.
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models
withattentionsinks. InTheTwelfthInternationalConferenceonLearningRepresentations,2024b. URLhttps:
//openreview.net/forum?id=NG7sS51zVF.
Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. LM-infinite: Zero-shot
extremelengthgeneralizationforlargelanguagemodels. InKevinDuh,HelenaGomez,andStevenBethard,editors,
Proceedingsofthe2024ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:
HumanLanguageTechnologies(Volume1: LongPapers),pages3991–4008,MexicoCity,Mexico,June2024b.
AssociationforComputationalLinguistics. URLhttps://aclanthology.org/2024.naacl-long.222.
PierreMiasnikof,AlexanderShestopaloff,AnthonyBonner,andYuriLawryshyn. AStatisticalPerformanceAnalysisof
GraphClusteringAlgorithms,pages170–184. 052018. ISBN978-3-319-92870-8. doi:10.1007/978-3-319-92871-
5_11.
MarkEJNewmanandMichelleGirvan. Findingandevaluatingcommunitystructureinnetworks. PhysicalreviewE,
69(2):026113,2004.
MatthijsDouze,AlexandrGuzhva,ChengqiDeng,JeffJohnson,GergelySzilvasy,Pierre-EmmanuelMazaré,Maria
Lomeli,LucasHosseini,andHervéJégou. Thefaisslibrary. 2024.
SebastianMichelmann,AmyRPrice,BobbiAubrey,CamillaKStrauss,WernerKDoyle,DanielFriedman,PatriciaC
Dugan,OrrinDevinsky,SashaDevore,AdeenFlinker,etal. Moment-by-momenttrackingofnaturalisticlearning
anditsunderlyinghippocampo-corticalinteractions. Naturecommunications,12(1):5394,2021.
OlgaLositsky,JaniceChen,DanielToker,ChristopherJHoney,MichaelShvartsman,JordanLPoppenk,UriHasson,
and Kenneth A Norman. Neural pattern change during encoding of a narrative predicts retrospective duration
estimates. elife,5:e16070,2016.
Alberto Mariola, Zafeirios Fountas, Lionel Barnett, and Warrick Roseboom. Event segmentation in continuous,
naturalisticvideosfrommodel-based,data-driven,andhumanperspectives. 2022.
SebastianMichelmann,ManojKumar,KennethANorman,andMariyaToneva. Largelanguagemodelscansegment
narrativeeventssimilarlytohumans. arXivpreprintarXiv:2301.10297,2023b.
AlanBaddeley. Workingmemory: lookingbackandlookingforward. Naturereviewsneuroscience,4(10):829–839,
2003.
14Preprint
KAndersEricssonandWalterKintsch. Long-termworkingmemory. Psychologicalreview,102(2):211,1995.
SantoFortunato. Communitydetectioningraphs. Physicsreports,486(3-5):75–174,2010.
ZhaoYang,RenéAlgesheimer,andClaudioJTessone. Acomparativeanalysisofcommunitydetectionalgorithmson
artificialnetworks. Scientificreports,6(1):30750,2016.
RyanPrescottAdamsandDavidJCMacKay. Bayesianonlinechangepointdetection. arXivpreprintarXiv:0710.3742,
2007.
MarkEJNewman. Fastalgorithmfordetectingcommunitystructureinnetworks. PhysicalReviewE—Statistical,
Nonlinear,andSoftMatterPhysics,69(6):066133,2004.
Victor M. Panaretos and Yoav Zemel. Statistical aspects of wasserstein distances. Annual Review of Statistics
andItsApplication, 6(Volume6, 2019):405–431, 2019. ISSN2326-831X. doi:https://doi.org/10.1146/annurev-
statistics-030718-104938. URLhttps://www.annualreviews.org/content/journals/10.1146/
annurev-statistics-030718-104938.
15Preprint
A APPENDIX / SUPPLEMENTAL MATERIAL
A.1 SUPPLEMENTARYFIGURES
NarrativeQA Qasper MultiFieldQA HotpotQA
1 40
39
22
1 2 48 38
29
12 37
33 36
21 2
47 35
1 2
28
10 20 30 40 50 10 20 30 40 50 60 70 10 20 30 40 50 60 10 20 30 40 50
2WikiMQA Musique GovReport QMSum
32
24
23 18
31
23
22 17
10 20 30 40 50 60 10 20 30 40 50 10 20 30 40 50 10 20 30 40 50 60
MultiNews TREC TriviaQA SAMSum
S S+C InfLLM 71 88
SM SM+C
43
27
70
87
42
26
69
10 20 30 40 50 60 20 30 40 50 60 10 20 30 40 50 10 20 30 40 50 60
PassageRetrieval LCC RepoBench-P Average scores of all tasks
86
85 57
84
83
82
81 53
80
79 78
77 43
777 456 56
52
73
72
71
70
69
68 51
67
66 56 55 42
64
63
10 20 30 40 50 60 20 30 40 50 20 30 40 50 20 30 40 50 60
Mean event size Mean event size Mean event size Mean event size
Figure4: AblationstudyinLongBench. ComparisonofEM-LLMperformancefordifferentcombinationsofmodel
features(representedbydifferentcolours)anddifferentvaluesofγ (thethreshold’sscalingfactor). Modelvariantsare
alignedonthex-axisbasedontheaveragenumberofblocksizethatemergesforeachcase. Theγ valuesforeach
modelvariantareshowninthefirstsub-plot. ThecorrespondingInfLLMperformanceisalsoshown.
16
erocs
ksaT
erocs
ksaT
erocs
ksaT
erocs
ksaTPreprint
NarrativeQA Qasper MultiFieldQA HotpotQA
1
48
40
22
1
39
2
1 38 21
29
2
37
2 36
20
16 18 20 22 24 26 15.0 17.5 20.0 22.5 25.0 27.5 17.5 20.0 22.5 25.0 27.5 30.0 16 18 20 22 24 26
2WikiMQA Musique GovReport QMSum
19 24
23
18
31
16 18 20 22 24 26 16 18 20 22 24 26 16 18 20 22 24 26 15.0 17.5 20.0 22.5 25.0 27.5
MultiNews TREC TriviaQA SAMSum
27 Contiguity ratio: 0.3 43
Contiguity ratio: 0.5
Contiguity ratio: 0.7
InfLLM 70
87
69
17.5 20.0 22.5 25.0 27.5 30.0 16 18 20 22 24 26 16 18 20 22 24 26 15.0 17.5 20.0 22.5 25.0 27.5
PassageRetrieval LCC RepoBench-P Average scores of all tasks
85 53
84
83
82
81
80
777 789 56 43
77 56 52
74 73
72
71
70
69
68
666 567 55 51 42
64
63
14 16 18 20 22 24 26 26 28 30 32 34 36 26 28 30 32 34 36 16 18 20 22 24 26 28
Mean event size Mean event size Mean event size Mean event size
Figure5: AblationstudyinLongBench. ComparisonofEM-LLMperformancefordifferentratiosofthecontiguityand
similaritybuffers(representedbydifferentcolours)anddifferentvaluesofγ. Modelvariantsarealignedonthex-axis
basedontheaveragenumberofblocksizethatemergesforeachcase. Theγ valuesforeachmodelvariantareshown
inthefirstsub-plot. ThecorrespondingInfLLMperformanceisalsoshown.
17
erocs
ksaT
erocs
ksaT
erocs
ksaT
erocs
ksaTPreprint
A.2 COMPLEXITYANALYSISOFEM-LLMALGORITHM
Here,weprovideadetailedanalysisofthecomputationalcomplexityofourAlgorithm1,focusingontheboundary
refinementstepandthecalculationofmodularityandconductancemetrics.
BoundaryRefinementStep Theboundaryrefinementstepinvolvesfindingtheoptimalpositionβˆbetweeneach
pairofconsecutiveinitialboundaries(α,β)thatoptimizesthechosenmetricfunctionf. Thisstephasthefollowing
components:
Iterationoverinitialboundaries: O(k),wherekisthenumberofinitialboundaries. Foreachpairofboundaries,we
computethemetricfunctionf forallpositionsbetweenαandβ. Intheworstcase,thisisO(n)operationsperboundary
pair.
Therefore,theoverallcomplexityofthisstepisO(kn).
Metric Function Computation The metric functions (modularity or conductance) are computed at the level of
individualmemoryunits. Foramemoryunitofsizem:
• Modularity: Thenaivecomputationinvolvessummingoverallpairsofnodeswithintheunit,resultingina
worst-casecomplexityofO(m2). However,inpractice,thesimilaritygraphisoftensparse,meaningmany
nodepairshavenegligiblesimilarity. Leveragingthissparsity,moreefficientimplementationscanachieveO(l)
complexity,wherelisthenumberofnon-zerosimilarityedgeswithintheunit(Newman,2004). Typically,lis
muchsmallerthanm2,especiallyforlargerunits,leadingtosignificantcomputationalsavings.
• Conductance: Thisrequirescomputingthesumofedgeweightscrossingtheboundaryandthetotalvolumeof
theunit,whichcanbedoneinO(m)time.
Giventhatmistypicallymuchsmallerthannandvariesforeachunit,wecanconsidertheaverageunitsizem¯ and
averagenumberofnon-zerosimilarityedges¯l. Thetotalcomplexityforcomputingmetricsacrossallunitsisthen
O(k¯l)formodularity(whichintheworstcaseisO(km¯2),buttypicallymuchlower)orO(km¯)forconductance.
OverallComplexity Combiningtheboundaryrefinementstepandmetriccomputation,theoverallcomplexityis:
Formodularity: O(kn+km¯2)Forconductance: O(kn+km¯)
Sincetypicallym¯ ≪n,thedominantterminbothcasesisO(kn). Therefore,weexpresstheoverallcomplexityofour
algorithmasO(kn).
A.3 ANALYSISOFHUMANDATA
ThehumandatareleasedaspartofKumaretal.(2023)usedGaussiansmoothingontheaveragesignalacrossparticipants
todefineaprobabilitydistributionoflikelyeventboundarypositionswithrespecttotimestampsinthepodcast. Inorder
tocalculateoursimilaritymetrics,asshowninFig.3A,weneedtoexpressthisdataintermsofdiscreteeventpositions
withrespecttotokensinthetranscript. Forfaircomparison,wethereforeidentifiedhuman-annotatedpositionsby
selectingasmanyofthemostlikelypositionsinthedistributionasourinitialsurprise-basedeventsegmentationhad
identifiedinthetranscript. InthesameprocessusedbyKumaretal.(2023),wethenusedtheirprovidedwordonset
timestotranslatethesetimestampstotokenpositions,allowingustocalculateoursimilaritymetrics.
InFig.3B,weuseWassersteindistanceinordertocomparetherelativepositionsofeventboundariesbetweenhuman
annotations and those found by our own methods. Wasserstein distance is a versatile metric used to compare two
probabilitydistributions(PanaretosandZemel,2019). Weusedsuchametrictobettercapturetheuncertaintypresent
inthehumandata,andfoundittogivemoremeaningfulresultsthanstandardcorrelationordiscretedistancemetrics,
whichshowedverylittledifferencesbetweenmethods. Inordertocalculatesuchametric,wethereforeneedtoconvert
our own discrete boundary positions to a distribution across token positions. We did so by defining a Mixture of
Gaussians(MoG),witheachGaussiancorrespondingtoasingleposition. Notethat,forfaircomparisonwithhuman
data,weapplythesameprocesstothediscreteversionofthehuman-annotatedpositionsdescribedabove,andusethis
forcomparison.
A.4 APPROXIMATEEQUIVALENCEOFK-NEARESTNEIGHBOURSANDSOFTMAXATTENTION
Herewewillattempttoshowthatusingak-NNretrievalinakey-valuecacheaspartoftheattentionmechanismin
transformersisanapproximationofapplyingsoftmaxattentionovertheentiresequenceoftokens.
18Preprint
LetqbeaqueryvectorandK ={k ,k ,...,k }thesetofkeyvectorsinatransformermodelwithdimensionality
1 2 n
d. Eachkeyk hasacorrespondingvaluevectorv ,withV ={v ,v ,...,v }. Thesoftmaxattentionweightsa are
i i 1 2 n i
definedas:
a =
exp(q·k id−1 2)
(6)
i (cid:80)n j=1exp(q·k
j
d− 21)
Theoutputvectoruiscomputedas:
n
(cid:88)
u= a v (7)
i i
i=1
Inthek-NNapproach, asubsetK′ ofsizek isselected, containingkeysnearesttoq. Theapproximatedattention
weightsa′ overthissubsetare:
i
a′ =
exp(q·k id− 21)
fork ∈K′ (8)
i (cid:80) j∈K′exp(q·k
j
d−1 2) i
Theapproximateoutputvectoru′is:
(cid:88)
u′ = a′v (9)
i i
ki∈K′
ASSUMPTIONS
1. ExponentialDominance: Theexponentialfunctioninthesoftmaxissharplypeaked,implyingthatkeyswith
thehighestsimilaritiestoqcontributesignificantlymoretothesumthanothers.
2. Representativenessofk-NNSubset: ThesubsetK′capturesthemajorityoftheattentionweightfromthefull
setK.
Lemma1: Dominanceofk-NNSubset IfK′consistsofthekkeyswiththehighestdotproductsq·k ,then:
i
n
(cid:88) exp(q·k
j
d−1 2)≥α(cid:88) exp(q·k
j
d− 21) (10)
j∈K′ j=1
forsomeα≈1,typicallyverycloseto1.
Proof: Thisfollowsfromtheexponentialdominanceassumptionandthenatureoftheexponentialfunction,whichis
sharplypeaked.
Lemma2: ApproximationofOutputVector GiventhedominanceofK′asshowninLemma1,theapproximate
outputu′effectivelyrepresentsthefulloutputu:
∥u′−u∥≤ϵ (11)
whereϵisasmallerrorterm.
Proof: Followsfromtheweightedsumstructureofuandu′,usingtheboundsestablishedinLemma1.
Giventhelemmasandunderthestatedassumptions,thek-NNretrievalmechanismwithinakey-valuecacheeffectively
approximatesthesoftmaxattentionmechanismintransformers. Thisproofhighlightstheefficiencyversusaccuracy
trade-offinherentinusingapproximatemethodslikek-NNretrieval.
19