ICML2024NextGenerationofSequenceModelingArchitecturesWorkshop
HiPPO-Prophecy: State-Space Models can Provably Learn Dynamical
Systems in Context
FedericoArangathJoseph* FARANGATH@ETHZ.CH
KilianHaefeli* KHAEFELI@ETHZ.CH
NoahLiniger* NLINIGER@ETHZ.CH
ETHZurich
CaglarGulcehre CAGLAR.GULCEHRE@EPFL.CH
EPFL
Abstract
Thisworkexploresthein-contextlearningcapabilitiesofStateSpaceModels(SSMs)andpresents,
tothebestofourknowledge,thefirsttheoreticalexplanationofapossibleunderlyingmechanism.
WeintroduceanovelweightconstructionforSSMs,enablingthemtopredictthenextstateofany
dynamicalsystemafterobservingpreviousstateswithoutparameterfine-tuning. Thisisaccom-
plishedbyextendingtheHiPPOframeworktodemonstratethatcontinuousSSMscanapproximate
thederivativeofanyinputsignal. Specifically,wefindanexplicitweightconstructionforcontinuous
SSMsandprovideanasymptoticerrorboundonthederivativeapproximation. Thediscretizationof
thiscontinuousSSMsubsequentlyyieldsadiscreteSSMthatpredictsthenextstate. Finally,we
demonstratetheeffectivenessofourparameterizationempirically. Thisworkshouldbeaninitial
steptowardunderstandinghowsequencemodelsbasedonSSMslearnincontext.
1. Introduction
In-context learning (ICL) refers to a model’s ability to solve tasks unseen during training, only
basedoninformationprovidedincontext,withoutupdatingitsweights. ICLhasgainedsignificant
attentionsinceBrownetal.[6]demonstratedthattransformermodels[23]trainedinlargeanddiverse
languagecorporacanlearnincontextwithoutbeingexplicitlytrainedforit. Morespecifically,they
showed that given a sequence of input-output pairs from an unseen task, the model can predict
theoutputcorrespondingtoanewinput. WewillrefertothistypeofICLasfew-shotin-context
learningtoemphasizethepresenceofinput-outputpairsin-context. Subsequently,to[6],therehas
beenavarietyofempirical[9,17,20,27]aswellastheoretical[1,2,4,19,24,25,28–30]works
studyingthefew-shotICLcapabilitiesoftransformermodelsandthemechanismsunderlyingthem.
Nonetheless,therestillexistsagapbetweenthestudiedfew-shotICLsettingsandICLcapabilities
thatemergeinmodernsequencemodels,trainedautoregressivelyonsequentialdata[3,26]. Previous
worksby[3,26]aimtoclosethisgapbystudyingtheICLcapabilitiesofautoregressivelytrained
transformermodelstopredictthenextvaluef ofanunseensequence,whenprovidedwithvalues
k+1
f in-context. WetermthismodeoflearningfromcontextasautoregressiveICL.
≤k
Concurrentlytothesestudies,thedevelopmentofdeepstatespacemodelssuchasS4byGuetal.[13]
hassparkedaresurgenceofrecurrentsequencemodels,resultinginafamilyofmodels,whichwe
*EqualContribution
© F.A.Joseph,K.Haefeli,N.Liniger&C.Gulcehre.
4202
luJ
21
]GL.sc[
1v57390.7042:viXraHIPPO-PROPHECY: STATE-SPACEMODELSCANPROVABLYLEARNDYNAMICALSYSTEMSINCONTEXT
refertoasgeneralizedstatespacemodels(GSSMs)[7,8,11,14,16,21]. GSSMsofferapromising
alternative to transformers by addressing two of their fundamental shortcomings, namely length
generalization and quadratic computational cost (flops) with respect to the sequence length [11].
Similarlytotransformer-basedmodels,GSSMshaveempiricallybeenshowntobecapableofICL
[3,10,18,22]. Despitetheirpotential,ourtheoreticalunderstandingofthemechanismsunderlying
ICLinGSSMsisstilllimited. Tothebestofourknowledge,thisworkprovidesthefirstexplanation
ofhowGSSMscanperformautoregressiveICL.ForthisweconsiderSSMsonthetaskofpredicting
thenextstateofanydynamicalsystem,givenasequenceofpreviousstates. Ourcontributionscanbe
summarizedasfollows.
• Extending the HiPPO framework to show that SSMs can approximate the next state of any
dynamicalsystemuptofirstorderfromasequenceofpreviousstatesthroughanexplicitweight
construction. (§3)
• Anasymptoticboundontheerrorincurredwhenapproximatingthederivativeoftheinputsignal
withacontinuousSSMparametrizedwithourproposedconstructionfortheFouTbasis. (§3)
• Anexperimentalevaluationofourweightconstructionondifferentfunctionclasses,modelsizes,
andcontextlengths. (§4)
2. SSMsandHiPPOTheory
SSMsmapaninputsignalu(t) Rtoanoutputsignaly(t) Rviaahiddenstatex(t) RN and
∈ ∈ ∈
takethefollowingform:
(cid:40)
x˙(t) = Ax(t)+Bu(t)
(1)
y(t) = Cx(t)+Du(t)
Onekeypropertyofthesemodelsistheirabilitytomemorizepastinformation,u(s)fors t,in
≤
theirhiddenstatex(t). ThiscapabilitywasestablishedintheHiPPOtheory[12,15]. HiPPOtheory
considersaHilbertspacespannedbytheorthogonalbasisfunctions p (t,s) ,equippedwith
n n≥0
(cid:82)t { }
a measure ω(t,s) and an inner product f,g = f(s)g(s)ω(t,s)ds. The theory proposes a
⟨ ⟩ω −∞
parametrizationforAandBsuchthatthehiddenstatex (t)representstheprojectionoftheinput
n
signalu ontothebasisp (t,s)i.e. x (t) = u ,p . OneofthefundamentalresultsinHiPPO
≤t n n ≤t n ω
⟨ ⟩
theoryisthatinthelimitofaninfinitehiddenstatesizeN andforanappropriatechoiceofA
→ ∞
andB,itispossibletoreconstructtheinputsignaluptotimetfromthehiddenstatex(t):
∞
(cid:88)
u(s) = x (t)p (t,s) s t (2)
n n
∀ ≤
n=0
Inthiswork,weconsiderthreespecificHiPPOparameterizationsofAandB: LegTandLegSbased
onLegendrepolynomialsandFouT,whichisbasedonaFourierbasis. Theexplicitconstructionsfor
AandBaregiveninAppendixA.
3. SolvingDynamicalSystemswithSSMs
Asalludedtointheintroduction,westudySSMsonautoregressiveICL,predictingthenextvaluef
k+1
givenf incontext. ForSSMs,thiscorrespondstopredictingu afteriterativelyobservingthe
≤k k+1
firstkvaluesofthesequenceu . Toprogresstowardsthisgoal,weconsideracontinuousrelaxation
≤k
2HIPPO-PROPHECY: STATE-SPACEMODELSCANPROVABLYLEARNDYNAMICALSYSTEMSINCONTEXT
oftheproblemwherewemaptheindicestoinstancesincontinuoustime,i.e.,k t,k+1 t+∆t.
→ →
Inthefollowing,wedenotewithu(t)theinputincontinuoustimeandwithu itsdiscretecounterpart.
k
Our aim in the continuous setting is, therefore, to predict u(t+∆t), for which we consider the
followingintegralexpression:
(cid:90) t+∆t
u(t+∆t) = u(t)+ u˙(s)ds (3)
t
Tosolvethis,wetakemultiplesteps: (1)weapproximateu˙(t)byconstructingaspecificparameteri-
zationforCandDforcontinuousSSMsandageneralbasis p resultinginamodelsatisfying
n n≥0
{ }
u˙(t) y(t) = Cx(t)+Du(t). Subsequently, (2) we show how to approximate the integral via
≈
discretization,bringingusbacktothediscreteSSMsettingandouroriginalproblemofautoregressive
ICL.Finally,(3),weprovideanasymptoticboundontheerrorincurredbyapproximatingu˙(t)with
afinitehiddenstate.
(1) ByevaluatingEquation2attimet,weget: u(t) =
(cid:80)∞
x (t)p (t,t). Undersometechnical
n=0 n n
assumptionswecanexchangetheserieswiththederivative†. Notingthatp (t,t)isaconstantwith
n
respecttot,wegetu˙(t) =
(cid:80)∞
x˙ (t)p (t,t). Throughthis,weestablishaweightconstruction
n=0 n n
inProposition1,suchthatthecontinuousSSMapproximatesthegradientoftheinputsignal. The
followingpropositionisfortheLegTandFouTbases. InAppendixB.2wefurtherprovidetheresult
fortheLegSbasis.
Prop.1(ConstructionofCandDforLegTandFouT) If we choose C =
(cid:80)N
A p (t,t)
j k=0 kj k
and D =
(cid:80)N
B p (t,t) and A, B and p (t,t) as in HiPPO LegT or FouT, then the output
k=0 k k k
y(t) =: u˙ (t)isanapproximationofu˙(t)basedonN basisfunctions.
N
Proof We first assume an infinite hidden state size N = , then use the definition of x˙(t) and
∞
truncatetheseriestoobtaintheresult.
 
∞ ∞ ∞
(cid:88) (cid:88) (cid:88)
u˙(t) = x˙ k(t)p k(t,t) =  A kjx j(t)+B ku(t)p k(t,t)
k=0 k=0 j=0
N (cid:32) N (cid:33) (cid:32) N (cid:33)
(cid:88) (cid:88) (cid:88)
A p (t,t) x (t)+ B p (t,t) u(t)
kj k j k k
≈
j=0 k=0 k=0
N
(cid:88)
=: C x (t)+Du(t)
j j
j=0
(2) SincewecannotsolvetheintegralinEquation3inclosedform,weapproximateitusingthe
bilinear method, which then brings us back to the discrete SSM setting and our original problem
ofautoregressiveICL.StartingfromEquation3,weapproximateu˙(t) Cx(t)+Du(t)andthen
≈
†Iff(x)=(cid:80)∞ f (x),thenif(cid:80)∞ f˙ (x)convergesabsolutelywehavethatf˙(x)=(cid:80)∞ f˙ (x).Weassume
n=0 n n=0 n n=0 n
thisassumptionholdsthroughouttherestofthepaper.
3HIPPO-PROPHECY: STATE-SPACEMODELSCANPROVABLYLEARNDYNAMICALSYSTEMSINCONTEXT
apply the bilinear method. Here, C and D are the parametrizations of HiPPO LegT or FouT, as
definedinProposition1.
(cid:90) t+∆t
u(t+∆t) u(t) Cx(s)+Du(s)ds
− ≈
t
∆t(cid:16) (cid:17)
Cx(t)+Du(t)+Cx(t+∆t)+Du(t+∆t) .
≈ 2
Rearranging the equation, then applying the discrete-time mapping t k,t + ∆t k + 1 as
→ →
previouslydescribed,andapproximatingx x forsmall∆t,weobtain:
k k+1
≈
(cid:18) D∆t(cid:19)−1(cid:20)(cid:18) D∆t(cid:19)
∆t
(cid:21)
u = 1 1+ u + C(x +x )
(cid:98)k+1 k k k+1
− 2 2 2
(cid:18) D∆t(cid:19)−1(cid:20)(cid:18) D∆t(cid:19) (cid:21)
1 1+ u +∆tCx
k k+1
≈ − 2 2
= Cx +Du
k+1 k
Thisyieldsadiscrete-timesystem,wherethehiddenstateevolutionisgivenbyx = Ax +Bu .
k+1 k k
Here,A = (I ∆tA)−1(I + ∆tA)andB = ∆t(I ∆tA)−1BarethediscretizedversionsofA
− 2 2 − 2
andB,respectively[12]. Thecompletediscretizedsystemisexpressedas:
(cid:40)
x = Ax +Bu
k+1 k k
(4)
u = Cx +Du
(cid:98)k+1 k+1 k
Crucially, theabovesystemallowsustopredictthevalueofthefutureinputstateu basedon
(cid:98)k+1
thehiddenstatex (whichisafunctionofu )andtheinputu ,andhenceperformautoregressive
k <k k
ICL.Unlikeclassicalmachinelearning,whichrequirestrainingforspecificdynamicalsystems,our
parametrizationpredictsfuturestatesofarbitrarysequenceswithouttask-specificfine-tuning.
(3) Tofurtherinvestigatetheproposedparametrizationforthecontinuous-timeSSM,weprovidean
asymptoticboundontheerrorincurredwhenapproximatingu˙(t)withu˙ (t)whichistheoutputof
N
thecontinuousSSMwithafinitehiddenstateofdimensionN. Forthis,weconsideranalternative
constructionoftheFouTbasisinProposition2simplifyingtheanalysis. TheproofofProposition2
isanalogoustothatofProposition1andcanbefoundinAppendixD.
(cid:40)
0 ifk = 0ork odd
Prop.2(AlternativeFouTConstruction) Ifwechoose,C =
k
2√2πn otherwise
−
D = 0andA,Bandp (t,s)asinHiPPOFouTandifu(t)hask-thboundedderivativesforsome
k
k 3,thentheoutputy(t) =: u˙ (t)isanapproximationofu˙(t)basedonN basisfunctions.
N
≥
Usingthis,weshowthattheerror u˙(t) u˙ (t) ,decreasespolynomiallyinthehiddenstatesizeN
N
| − |
andlinearlydependsontheLipschitzconstantLofthe(k 1)-thderivative.
−
Thm.1(ApproximationError) Ifuhask-thboundedderivativesforsomek 3,i.e. u(k)(t)
≥ | | ≤
L t then it holds that for the choice of A and B in HiPPO FouT and C and D as in Prop. 2:
∀
u˙(t) u˙ (t) (L/Nk−2)
N
| − | ∈ O
TheproofofthisTheoremcanbefoundinAppendixD.Fromthisresult,wecanderiveaCorollary
(cid:82)t
fortheerrorofpredictingu(t)usingu (t) = u˙ (s)dsinthecontinuoussetting:
N 0 N
4HIPPO-PROPHECY: STATE-SPACEMODELSCANPROVABLYLEARNDYNAMICALSYSTEMSINCONTEXT
Cor.1(ApproximationError) UnderthesameassumptionsandparametrizationasinTheorem1,
wehavethat: u(t) u (t)
(cid:0) Lt/Nk−2(cid:1)
N
| − | ∈ O
(cid:82)t
Wefurthernotethathavingu (t) = u˙ (s)dsreflectshowwecalculateu inpractice,with
N 0 N k+1
thedifferencethatherewedonotconsideranapproximationoftheintegral. Inparticular,u (t)can
N
beseenastheequivalentcontinuousversionofourestimatoruˆ .
k
4. Experiments
WeperformathoroughexperimentalevaluationoftheweightconstructionpresentedinEquation
4. For this, we unroll the model step-by-step to predict u given u and evaluate the perfor-
k+1 ≤k
mance using (θ) = 1 (cid:80)T f (u ,x ) u , where θ = A,B,C,D and f is the
L T−Ts k=Ts| θ k k − k+1 | { } θ
parametrizedSSM.Unlessspecifiedotherwise,weuseT = 104 andT = T/2,providingthemodel
s
withsufficientlylongcontext.
OrdinaryDifferentialEquation Wecompareourmodel’sabilitytopredictthenextstateofan
adaptedVanderPolOscillatorinonevariable: u˙(t) = µ(1 u(t)2)sin(t),withN = 65. Asseen
−
in1(a)subfigure,bothLegTandFouTachievelowererrorinregionsoflowercurvature,consistent
withthedependenceontheLipschitzconstantestablishedinTheorem1.
White Signal and Filtered Noise Process Following Gu et al. [12] we use band-limited and
low pass filtered white noise 1(f)subfigure, which we refer to as White Signal a Filtered Noise
respectively(see[5]). Foreach,weusethreeprogressivelyhardersetupsinwhichthehigh-frequency
informationisincreased. ThefrequencycontentiscontrolledbyαforFilteredNoiseandbyγ for
WhiteNoise. Smallerαandlargerγ correspondtoincreasedhigh-frequencycontent. Wetestthe
modeloverdifferenthiddenstatesizesrangingfrom1to96inincrementsof5. Wefindthatusinga
largerhiddenstatedimensionisbeneficialuptoacertainN,afterwhichforLegTthereisminimal
tonofurtherbenefit. ForFouT,performanceinitiallyworsenswhenincreasingN beforeimproving.
Thisisbecause,forlowN,themodelessentiallycopiesitsinput,whichissurpassedforlargerN.
Furthermore,theerrorislowerwithlesshigh-frequencycontent.
LearningfromContext Toempiricallydemonstrateourmodels’useofcontext,weexaminehow
theerrorscaleswithincreasedcontextlength. In1(e)subfigure,wefindthatforallhiddenstatesizes
N,themodelgetsbetterwithalongercontext. Furthermore,moreexpressivemodelswithlarger
hiddenstatesexhibitlongeroscillations,requiringmoresamplestostabilize. Intuitively,largerN
correspondstobiggerfunctionclassesthatthemodelcanrepresent.
ConstructionsasInitialisation In1(d)subfigure,wecomparetheperformanceofSSMlayersin
differentsettings. (I)InitializingA,B,C,DatconstructionandtrainingC,D. (II)FixingA,B,C,
Datconstruction. (III)FixingA,Batconstruction,standardGaussianinitializationandtrainingof
C,D. (IV)InitializingA,B,C,Datconstructionandtrainingallofthem. Wetrainthemodelson
amixeddataset,consistingofWhiteSignal,LegendrePolynomials,andsumsofsinefunctions(see
AppendixE.3forfurtherdetails). Weevaluatethesemodelson3hold-outdatasets,namelyFiltered
Noise,aholdoutmixeddataset,linearfunctions,andtheVanderPoloscillatorfrom1(a)subfigure.
WeobservethatinitializingtheSSMwithourparametrization(I,II,IV)leadstoenhancedpredictive
performanceoverrandominitialization(III). Moreso,trainingthemodelusinggradientmethods
(I,III,IV)doesnotresultinincreasedperformanceoverourweightconstruction(II).
5HIPPO-PROPHECY: STATE-SPACEMODELSCANPROVABLYLEARNDYNAMICALSYSTEMSINCONTEXT
1 LegTα=0.05 LegTα=0.3 FouTα=0.1 LegTγ=1 LegTγ=5 FouTγ=2
LegTα=0.1 FouTα=0.05 FouTα=0.3 LegTγ=2 FouTγ=1 FouTγ=5
10 10−1 11 00 −− 32
− 5×10−3 ×10−1
2
11 00 −− 32 111 000 −−− 654
10−4 10−7
0 0
0 2000 4000 6000 8000 10000 0 20 40 60 80 100 0 20 40 60 80 100
Time N N
(a)FouTandLegTonVanderPol (b)ErrordependenceonN,α (c)ErrordependenceonN,γ
100 I
II
101 −0 10 N N= =1 36
1
N N= =4 66
1
N=76 1
10−1
I IVII 11 00 −− 32
0
10−4 1
10−2 10−5
10−6
10−3 10−7
0 50 100 150 200 250 300
0
0.0 0.5 1.0 1.5 2.0 2.5 3.0
Linear VanderPol Mixed FilteredNoise t t ×104
(d)Constructionvs. trainedLSSM (e)LegTerrordependenceont (f)FilteredNoise&WhiteSignal
Figure1: Empiricalevaluationofourweightconstruction. (a)subfigureVanderPoloscillatorso-
lutionanderrorsofFouTandLegTweightconstruction. (b)subfigureErrordependence
onN andαfortheFilteredNoisedataset(meanacross1kfunctionsand1std. plotted).
(c)subfigure Error dependence on N and γ for the White Signal dataset (mean across
1k functions and 1 std. plotted). (d)subfigure Performance comparison of weight con-
struction for LegT: (I) Initializing A,B,C,D at construction and training C,D. (II)
Fixing A,B,C,D at construction. (III) Fixing A,B at construction, standard Gaus-
sianinitializationandtrainingofC,D. (IV)InitializingA,B,C,Datconstructionand
training all of them (mean over 3 random seeds and error bars correspond to min and
max). (e)subfigureErrordependenceofLegTonthecontextlengtht = T ontheWhite
SignalDataset(meanacross1kfunctionsand1std. plotted). (f)subfigureExamplesofthe
FilteredNoise(α = 0.05)andWhiteSignal(γ = 2)datasets
5. Conclusions
Inthiswork,weproposeanovelSSMconstructionthatcanpredictthenextstateofanydynamical
systemfromitshistorywithoutfine-tuningitsparameters. Tothebestofourknowledge,thisisthe
firsttime it wastheoreticallyshownthatSSMs canperformautoregressive ICL.We findthatour
weightconstructionallowsSSMstoeffectivelyleveragecontexttomakepredictionsandthatitcan
serveasagoodinitialization.
ThisworkservesasaninitialsteptowardsunderstandingtheICLcapabilitiesofGSSMsandopens
severalavenuesforfutureresearch. InvestigatinghowgatingmechanismsinmodernGSSMslike
Mamba[11]andGriffin[7]affectICLcapabilitiesisonepotentialdirection. Anotherisexamining
theimpactoffullyconnectedlayersandnon-linearitiesfollowingSSMblocks. Lastly,weobserved
instabilitieswhenpredictingmultiplestepsintothefuture. Exploringmethodstoimprovemulti-step
predictionsandunderstandingtheinstabilitiescouldalsobevaluable.
6
noituloS
rorrE
TuoFrorrE TgeLrorrE
rorrE
rorrE
rorrE
langiSetihWesioNderetliFHIPPO-PROPHECY: STATE-SPACEMODELSCANPROVABLYLEARNDYNAMICALSYSTEMSINCONTEXT
References
[1] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to
implementpreconditionedgradientdescentforin-contextlearning,2023.
[2] EkinAkyu¨rek,DaleSchuurmans,JacobAndreas,TengyuMa,andDennyZhou. Whatlearning
algorithmisin-contextlearning? investigationswithlinearmodels,2023.
[3] Ekin Akyu¨rek, Bailin Wang, Yoon Kim, and Jacob Andreas. In-context language learning:
Architecturesandalgorithms,2024.
[4] YuBai,FanChen,HuanWang,CaimingXiong,andSongMei. Transformersasstatisticians:
Provablein-contextlearningwithin-contextalgorithmselection,2023.
[5] TrevorBekolay,JamesBergstra,EricHunsberger,TravisDeWolf,TerrenceStewart,Daniel
Rasmussen,XuanChoo,AaronVoelker,andChrisEliasmith. Nengo: aPythontoolforbuilding
large-scalefunctionalbrainmodels,2014. ISSN1662-5196.
[6] TomB.Brown,BenjaminMann,NickRyder,MelanieSubbiah,JaredKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,Alec
Radford,IlyaSutskever,andDarioAmodei. Languagemodelsarefew-shotlearners,2020.
[7] SohamDe,SamuelL.Smith,AnushanFernando,AleksandarBotev,GeorgeCristian-Muraru,
Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume
Desjardins,ArnaudDoucet,DavidBudden,YeeWhyeTeh,RazvanPascanu,NandoDeFreitas,
andCaglarGulcehre. Griffin: Mixinggatedlinearrecurrenceswithlocalattentionforefficient
languagemodels,2024.
[8] DanielY.Fu,TriDao,KhaledK.Saab,ArminW.Thomas,AtriRudra,andChristopherRe´.
Hungryhungryhippos: Towardslanguagemodelingwithstatespacemodels,2023.
[9] ShivamGarg,DimitrisTsipras,PercyLiang,andGregoryValiant. Whatcantransformerslearn
in-context? acasestudyofsimplefunctionclasses,2023.
[10] Riccardo Grazzi, Julien Siems, Simon Schrodi, Thomas Brox, and Frank Hutter. Is mamba
capableofin-contextlearning?,2024.
[11] AlbertGuandTriDao. Mamba: Linear-timesequencemodelingwithselectivestatespaces,
2023.
[12] AlbertGu,TriDao,StefanoErmon,AtriRudra,andChristopherRe´. Hippo: Recurrentmemory
withoptimalpolynomialprojections. InH.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,
andH.Lin, editors,AdvancesinNeuralInformationProcessingSystems, volume33, pages
1474–1487.CurranAssociates,Inc.,2020.
[13] AlbertGu,KaranGoel,andChristopherRe´.Efficientlymodelinglongsequenceswithstructured
statespaces,2022.
7HIPPO-PROPHECY: STATE-SPACEMODELSCANPROVABLYLEARNDYNAMICALSYSTEMSINCONTEXT
[14] Albert Gu, Ankit Gupta, Karan Goel, and Christopher Re´. On the parameterization and
initializationofdiagonalstatespacemodels,2022.
[15] AlbertGu,IsysJohnson,AmanTimalsina,AtriRudra,andChristopherRe´. Howtotrainyour
hippo: Statespacemodelswithgeneralizedorthogonalbasisprojections,2022.
[16] RaminHasani,MathiasLechner,Tsun-HsuanWang,MakramChahine,AlexanderAmini,and
DanielaRus. Liquidstructuralstate-spacemodels,2022.
[17] JannikKossen,YarinGal,andTomRainforth. In-contextlearninglearnslabelrelationshipsbut
isnotconventionallearning,2024.
[18] ChrisLu,YannickSchroecker,AlbertGu,EmilioParisotto,JakobFoerster,SatinderSingh,and
FeryalBehbahani. Structuredstatespacemodelsforin-contextreinforcementlearning,2023.
[19] ArvindMahankali,TatsunoriB.Hashimoto,andTengyuMa. Onestepofgradientdescentis
provablytheoptimalin-contextlearnerwithonelayeroflinearself-attention,2023.
[20] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan,BenMann,AmandaAskell,YuntaoBai,AnnaChen,TomConerly,DawnDrain,
DeepGanguli,ZacHatfield-Dodds,DannyHernandez,ScottJohnston,AndyJones,Jackson
Kernion,LianeLovitt,KamalNdousse,DarioAmodei,TomBrown,JackClark,JaredKaplan,
SamMcCandlish,andChrisOlah. In-contextlearningandinductionheads,2022.
[21] AntonioOrvieto,SamuelLSmith,AlbertGu,AnushanFernando,CaglarGulcehre,Razvan
Pascanu,andSohamDe. Resurrectingrecurrentneuralnetworksforlongsequences,2023.
[22] JonghoPark, JaeseungPark, ZheyangXiong, NayoungLee, JaewoongCho, SametOymak,
KangwookLee,andDimitrisPapailiopoulos. Canmambalearnhowtolearn? acomparative
studyonin-contextlearningtasks,2024.
[23] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,
LukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed,2023.
[24] MaxVladymyrov,JohannesvonOswald,MarkSandler,andRongGe. Lineartransformersare
versatilein-contextlearners,2024.
[25] Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Joa˜o Sacramento, Alexander
Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by
gradientdescent,2023.
[26] Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas
Zucchet,NinoScherrer,NolanMiller,MarkSandler,BlaiseAgu¨erayArcas,MaxVladymy-
rov, Razvan Pascanu, and Joa˜o Sacramento. Uncovering mesa-optimization algorithms in
transformers,2023.
[27] JerryWei,JasonWei,YiTay,DustinTran,AlbertWebson,YifengLu,XinyunChen,Hanxiao
Liu,DaHuang,DennyZhou,andTengyuMa. Largerlanguagemodelsdoin-contextlearning
differently,2023.
8HIPPO-PROPHECY: STATE-SPACEMODELSCANPROVABLYLEARNDYNAMICALSYSTEMSINCONTEXT
[28] Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter L.
Bartlett. Howmanypretrainingtasksareneededforin-contextlearningoflinearregression?,
2024.
[29] Ruiqi Zhang, Spencer Frei, and Peter L. Bartlett. Trained transformers learn linear models
in-context,2023.
[30] RuiqiZhang,JingfengWu,andPeterL.Bartlett. In-contextlearningofalineartransformer
block: Benefitsofthemlpcomponentandone-stepgdinitialization,2024.
9HIPPO-PROPHECY: STATE-SPACEMODELSCANPROVABLYLEARNDYNAMICALSYSTEMSINCONTEXT
AppendixA. ParametrizationsforLegT,LegSandFOuT
A.1. HiPPo-LegT
Asmentionedinthemaintext,HiPPO-LegTusesLegendrepolynomialsasitsbasisfunctions. In
particular, we have that ω(t,s) = 1I (s) and p (t,s) = √2n+1P (2s−t +1) where P is
θ [t−θ,t] n n θ n
then-thLegendrepolynomial. ThisleadstothefollowingchoiceforAandB:
(cid:40)
1 ( 1)n−k(2n+1) ifn k 1
A = − ≥ B = (2n+1)( 1)n (5)
nk n
θ (2n+1) ifn < k θ −
A.2. HiPPo-LegS
HiPPO-LegS also uses Legendre polynomials as its basis functions. However, here we have that
ω(t,s) = 1I (s) and p (t,s) = √2n+1P (2s 1) where P again is the n th Legendre
t [0,t] n n t − n −
polynomial. NotethatwhereasthemeasureofLegTistranslationinvariant,themeasureofLegS
is not. This causes the system to become Time-Varying and in particular this leads to having
x˙(t) = 1Ax(t)+ 1Bu(t)withthefollowingchoiceforAandB:
−t t

√2n+1√2k+1 ifn > k


A nk = (n+1) ifn = k B n = √2n+1 (6)

0 o.w.
A.3. HiPPO-FouT
HiPPO-FouT, differently from LegT and LegS uses the classical Fourier basis and in particular
(cid:104)
it assumes p (t,s) = √2 1 cos(2π[1 (t s)]) sin(2π[1 (t s)]) cos(4π[1 (t
n n≥0
{ } − − − − − −
(cid:105)
s)]) sin(4π[1 (t s)]) ... andω(t,s) = I (s). ThisleadstothefollowingchoiceofAand
[t−1,t]
− −
B:

 2 ifn = k = 0
 −

  2√2 ifn = 0andk oddork = 0andnodd 
 − 2 ifn = 0
 
 4 ifnoddandk odd 
A
nk
= − B
n
= 2√2 ifnodd (7)
2πn ifn k = 1andk odd 
  − 0 o.w.

  2πk ifk n = 1andnodd
 − −

0 o.w.
AppendixB. Proofs
B.1. ConstructionofCandDforLegTandFouT
Prop.1(ConstructionofCandDforLegTandFouT) If we choose C =
(cid:80)N
A p (t,t)
j k=0 kj k
and D =
(cid:80)N
B p (t,t) and A, B and p (t,t) as in HiPPO LegT or FouT, then the output
k=0 k k k
y(t) =: u˙ (t)isanapproximationofu˙(t)basedonN basisfunctions.
N
10HIPPO-PROPHECY: STATE-SPACEMODELSCANPROVABLYLEARNDYNAMICALSYSTEMSINCONTEXT
ProofWefirstassumethatthehiddenstatehasinfinitedimension(i.e. N = ),suchthatwehave
∞
perfectreconstructionofu˙(t):
∞
(cid:88)
u˙(t) = x˙ (t)p (t,t)
k k
k=0
 
∞ ∞
(cid:88) (cid:88)
=  A kjx j(t)+B ku(t)p k(t,t) bythedefinitionofx˙(t)
k=0 j=0
 
N N
(cid:88) (cid:88)
 A kjx j(t)+B ku(t)p k(t,t) approximatingtofinitehiddendimension
≈
k=0 j=0
N (cid:32) N (cid:33) (cid:32) N (cid:33)
(cid:88) (cid:88) (cid:88)
= A p (t,t) x (t)+ B p (t,t) u(t)
kj k j k k
j=0 k=0 k=0
N
(cid:88)
= C x (t)+Du(t)
j j
j=0
Therefore,wehave:
N
(cid:88)
C = A p (t,t) (8)
j kj k
k=0
N
(cid:88)
D = B p (t,t) (9)
k k
k=0
B.2. ConstructionofCandDforLegS
Prop.3(LegSconstruction) IfwechooseC = (cid:80)N 1A p (t,t)andD = (cid:80)N 1B p (t,t)
j k=0 t kj k k=0 t k k
andA,Bandp (t,t)asinHiPPOLegS,thentheoutputy(t)isanapproximationofu˙(t)basedon
k
N basisfunctions.
ProofNowAandBrepresenttheparametrizationsofHiPPOLegS.Theproofexactlyfollowsthe
samestepsastheoneabove,byfirstassuminginfinitehiddendimensionandthenapproximatingto
finitedimensionN.
∞
(cid:88)
u˙(t) = x˙ (t)p (t,t)
k k
k=0
 
∞ ∞
(cid:88) (cid:88) 1 1
=  A kjx j(t)+ B ku(t)p k(t,t)
t t
k=0 j=0
 
N N
(cid:88) (cid:88) 1 1
 A kjx j(t)+ B ku(t)p k(t,t)
≈ t t
k=0 j=0
11HIPPO-PROPHECY: STATE-SPACEMODELSCANPROVABLYLEARNDYNAMICALSYSTEMSINCONTEXT
N (cid:32) N (cid:33) (cid:32) N (cid:33)
(cid:88) (cid:88) 1 (cid:88) 1
= A p (t,t) x (t)+ B p (t,t) u(t)
kj k j k k
t t
j=0 k=0 k=0
N
(cid:88)
= C x (t)+Du(t)
j j
j=0
Therefore,wehave:
N
(cid:88) 1
C = A p (t,t) (10)
j kj k
t
k=0
N
(cid:88) 1
D = B p (t,t) (11)
k k
t
k=0
AppendixC. Discretization
C.1. DiscretizationofCandD
Thetrapezoid(orbilinear)methodisoneofthemostwidelyusedmethodsinnumericalanalysis,
usedingeneraltoapproximateintegralsby:
(cid:90) b b a(cid:0) (cid:1)
f(s)ds − f(a)+f(b) (12)
≈ 2
a
Inpractice,weareapproximatingtheintegralbetweenaandbwiththeareaofthetrapezoidwith
heightb aandparallelsidesf(a)andf(b),whichleadstoagoodapproximationifb aissmall.
− −
Inoursetting,wehavethat:
(cid:90) t+∆t
u(t+∆t) u(t) = u˙(s)ds (13)
−
t
(cid:90) t+∆t
= Cx(s)+Du(s)ds byusingourconstructionforu˙(s) (14)
t
∆t(cid:16) (cid:17)
Cx(t)+Du(t)+Cx(t+∆t)+Du(t+∆t) (15)
≈ 2
whereinthelaststepweusedthetrapezoidrulepresentedabove. Then,byrearrangingtheterms,we
get:
(cid:18) D∆t(cid:19)−1(cid:20)(cid:18) D∆t(cid:19)
∆t
(cid:21)
u(t+∆t) = 1 1+ u(t)+ C(x(t)+x(t+∆t)) (16)
− 2 2 2
(cid:18) D∆t(cid:19)−1(cid:20)(cid:18) D∆t(cid:19) (cid:21)
1 1+ u(t)+∆tCx(t+∆t) (17)
≈ − 2 2
= Cx(t+∆t)+Du(t) (18)
12HIPPO-PROPHECY: STATE-SPACEMODELSCANPROVABLYLEARNDYNAMICALSYSTEMSINCONTEXT
where we approximated x(t) x(t + ∆t) and where C = ∆t(1 D∆t/2)−1C and D =
(1
D∆t/2)−1 (1+D∆t/2).≈ −
−
Wethenmapthediscretizedtimesteptoindices,e.g. ift k thenwemapt+∆t k+1,which
→ →
leadsto:
u = Cx +Du
k+1 k+1 k
NotethatthisdiscretizationschemeworksforallHiPPOLegT,LegSandFouT.
C.2. DiscretizationofContinuousLinearSystems
Givenacontinuoustimedynamicallinearsystemoftheform:
(cid:40)
x˙(t) = Ax(t)+Bu(t)
y(t) = Cx(t)+Du(t)
Inordertobeabletousethissysteminpracticeandexperimentsweneedtodiscretizeit. Thereare
manymethodsusedtothisend,likeForwardEuler,BilinearorFoH.Byemployingthem,wearrive
atadiscretetimedynamicalsystemoftheform:
(cid:40)
x = Ax +Bu
k+1 k k
(19)
y = Cx +Du
k k+1 k
Where,inthecaseofthebilinearmethod,whichweuseinthiswork,wehaveA = (I ∆tA)−1(I+
− 2
∆tA),B = ∆t(I ∆tA)−1B,C = (I ∆tA)−⊤C⊤ andD = D+ 1C⊤B.
2 − 2 − 2 2
NotethatinoursettingthiswouldimplydiscretizingCandDtwice,onceforapproximatingu˙(t)
topredictthenextvalueofu(t)(seeAppendixC.1)andoncefordiscretizingthefullsystemasin
Equation19. Wefindthatonlyemployingthefirstoneworksbetterinpracticethenusingbothof
them. Hence,inourwork,wediscretizeAandBaccordingtothebilinearmethodandCandD
onlyaccordingtothediscretizationshowninAppendixC.1.
AppendixD. AlternativeFouTconstruction
Lemma1 Ifu(t)hask-thboundedderivatives,i.e. u(k)(t) L t,thenwehavethat xs(t)
n
| | ≤ ∀ | | ≤
L and xc(t) L .
(2πn)k | n | ≤ (2πn)k
ProofFromTheorem7in[15],wehavethatifuhask-thboundedderivatives,itholdsthat:
|xs n(t)
| ≤
(cid:12) (cid:12) (cid:12) (cid:12)(2π1
n)k
(cid:90) 1 u(k)(t)p n(s −t)dt(cid:12) (cid:12) (cid:12)
(cid:12)
0
≤
(2π1
n)k
(cid:90) 1 (cid:12) (cid:12)u(k)(t)(cid:12) (cid:12)(cid:12)
(cid:12)p n(s
−t)(cid:12)
(cid:12)dt
0
L
≤ (2πn)k
Similarly,underthesameassumptions,itholdsthat:
|xc n(t)
| ≤
(cid:12) (cid:12) (cid:12) (cid:12)(2π1
n)k
(cid:90) 1 u(k)(t)p n(s −t)dt(cid:12) (cid:12) (cid:12)
(cid:12)
0
13HIPPO-PROPHECY: STATE-SPACEMODELSCANPROVABLYLEARNDYNAMICALSYSTEMSINCONTEXT
≤
(2π1
n)k
(cid:90) 1 (cid:12) (cid:12)u(k)(t)(cid:12) (cid:12)(cid:12)
(cid:12)p n(s
−t)(cid:12)
(cid:12)dt
0
L
≤ (2πn)k
(cid:40)
0 ifk = 0ork odd
Prop.2(AlternativeFouTConstruction) Ifwechoose,C =
k
2√2πn otherwise
−
D = 0andA,Bandp (t,s)asinHiPPOFouTandifu(t)hask-thboundedderivativesforsome
k
k 3,thentheoutputy(t) =: u˙ (t)isanapproximationofu˙(t)basedonN basisfunctions.
N
≥
ProofAsalways,westartfrom:
∞
(cid:88)
u(s) = x (t)p (t,s) s t
n n
∀ ≤
n=0
whereinthecaseofFourierbasis,wedenote:
p =
√2(cid:2)
1 cos(2πt) sin(2πt) cos(4πt) sin(4πt)
...(cid:3)⊤
n n≥0
{ }
x(t) = (cid:2) x (t) xc(t) xs(t) xc(t) xs(t) ...(cid:3)⊤ R2N+1
0 1 1 2 2
∈
Hence,wehavethatforalls t:
≤
∞ ∞
u(s) = √2x
(t)+√2(cid:88) xc(t)cos(cid:0) 2πn(cid:2)
(t
s)+1(cid:3)(cid:1) +√2(cid:88) xs(t)sin(cid:0) 2πn(cid:2)
(t
s)+1(cid:3)(cid:1)
0 n n
− −
n=1 n=1
Assuminguhask-thboundedderivativeforsomek 3,itfollowsfromLemma1thatboth
(cid:80)∞ nxc(t)sin(cid:0) 2πn(cid:2)
(t
s)+1(cid:3)(cid:1) and(cid:80)∞ nxs≥ (t)cos(cid:0) 2πn(cid:2)
(t
s)+1(cid:3)(cid:1)
absolutelyconverge
n=1 n − n=1 n −
andhencewecanexchangetheserieswiththederivativeandthenwehaveforalls < t:
∞ ∞
u˙(s) =
2√2π(cid:88) nxc(t)sin(cid:0) 2πn(cid:2)
(t
s)+1(cid:3)(cid:1) 2√2π(cid:88) nxs(t)cos(cid:0) 2πn(cid:2)
(t
s)+1(cid:3)(cid:1)
n n
− − −
n=1 n=1
Byassumingcontinuityofu˙ (whichholdssinceuisk-timesdifferentiablewithk 3),wecantake
≥
thelimitassgoestottoextendthederivativeandget:
∞ N
(cid:88) (cid:88)
u˙(t) = 2√2π nxs(t) C x (t)
n k k
− ≈
n=1 k=0
Thus,wehavethat:
(cid:40)
0 ifk = 0ork odd
C =
k
2√2πk otherwise
−
D = 0
14HIPPO-PROPHECY: STATE-SPACEMODELSCANPROVABLYLEARNDYNAMICALSYSTEMSINCONTEXT
D.1. ApproximationError
Thm.1(ApproximationError) Ifuhask-thboundedderivativesforsomek 3,i.e. u(k)(t)
≥ | | ≤
L t then it holds that for the choice of A and B in HiPPO FouT and C and D as in Prop. 2:
∀
u˙(t) u˙ (t) (L/Nk−2)
N
| − | ∈ O
ProofLet’srecallthedefinitionofu˙ :
N
N−1
(cid:88)
u˙ (t) := √2 2πnxs(t)
N n
−
n=1
UsingLemma1,wecannowproceedtoourbound. Assumingk 3toguaranteeconvergenceof
≥
theinfiniteseries,weget:
(cid:12) (cid:12)
∞
(cid:12) (cid:88) (cid:12)
u˙(t) u˙ (t) = (cid:12) √2 2πnxs(t)(cid:12)
N (cid:12) n (cid:12)
| − | −
(cid:12) (cid:12)
n=N
∞
(cid:88)
2√2π n xs(t)
n
≤ | |
n=N
∞
(cid:88) L
2√2π n
≤ (2πn)k
n=N
(cid:18) (cid:19)
L
∈ O Nk−2
Cor.1(ApproximationError) UnderthesameassumptionsandparametrizationasinTheorem1,
wehavethat: u(t) u (t)
(cid:0) Lt/Nk−2(cid:1)
N
| − | ∈ O
(cid:82)t
ProofThisboundsimplyfollowsfromTheorem1andthefactthatu (t) = u˙ (s)ds:
N 0 N
(cid:12)(cid:90) t (cid:12)
(cid:12) (cid:12)
u(t) u N(t) = (cid:12) u˙(t) u˙ N(t)dt(cid:12)
| − | (cid:12) − (cid:12)
0
(cid:90) t
u˙(t) u˙ (t) dt
N
≤ | − |
0
(cid:18) (cid:19)
Lt
∈ O Nk−2
(cid:82)t
Note that having u (t) = u˙ (s)ds reflects how we calculate u(t+∆t) in practice, with the
N 0 N
differencethatherewedonotconsideranapproximationoftheintegral.
AppendixE. AdditionalExperimentalDetails
E.1. AdditionalExperiments
E.1.1. WHITE SIGNAL AND FILTERED NOISE
The White Signal and Filtered Noise datasets were generated via Nengo’s [5] White Signal and
FilteredNoisefunctions. TofurtherinvestigatetheperformanceofFouTandLegT,wesample100
15HIPPO-PROPHECY: STATE-SPACEMODELSCANPROVABLYLEARNDYNAMICALSYSTEMSINCONTEXT
Figure2: (Up)FunctionssampledfromFilteredNoisewithα = 0.05,0.1and0.3(fromlefttoright)
and predictions from LegT and FouT. (Bottom) Functions sampled from White Signal
withγ = 0.3,1and2(fromlefttoright)andpredictionsfromLegTandFouT.Inboth
casesweuseN = 65
LegT(N =33) FouT(N =33) LegT(N =65) FouT(N =65)
WhiteSignal(γ=0.3) 3.5±4.2(1e−11) 6.8±5.8(1e−8) 1.2±1.4(1e−11) 6.9±5.8(1e−8)
WhiteSignal(γ=1) 2.9±2.8(1e−7) 2.1±1.2(1e−6) 2.0±2.5(1e−10) 2.1±1.2(1e−6)
WhiteSignal(γ=2) 1.2±0.6(1e−5) 8.6±3.7(1e−6) 6.3±5.4(1e−7) 8.7±3.8(1e−6)
FilteredNoise(α=0.05) 2.1±0.2(1e−3) 1.7±0.1(1e−3) 2.8±0.3(1e−3) 1.5±0.1(1e−3)
FilteredNoise(α=0.1) 2.4±0.3(1e−4) 1.9±0.2(1e−4) 2.6±0.3(1e−4) 1.8±0.2(1e−4)
FilteredNoise(α=0.3) 5.0±1.0(1e−6) 6.4±1.5(1e−6) 4.1±0.6(1e−6) 6.2±1.5(1e−6)
Table1: MSEofLegTandFouTonWhiteSignalandFilteredNoisewithN = 33andN = 65.
differentfunctionsatrandomandevaluatebothweightconstructions. Wereportmeanandstandard
deviationoftheMSEforthetwoconstructionsinTable1. WecomparestatedimensionN = 33
andN = 65with10’000timestepspersignal. ForWhiteSignal,wecomparecut-offfrequencies
γ = 0.3, 1, and 2. For Filtered Noise, we use the Alpha filter [5] with parameters α = 0.05,
0.1, and 0.3. As γ increases or α decreases, the resulting functions become more oscillatory and
challengingtoapproximate(SeeFigure2). Notethatwehaveshiftedthegroundtruthfunctionup
by0.1todistinguishitfromthemodel’spredictions. FunctionsfromtheWhiteSignalprocessare
smootherandeasiertoapproximate,whileFilteredNoisegeneratesrougher,discontinuousfunctions.
Unsurprisingly,ourmodelsperformbetteronWhiteSignaldata.
E.1.2. APPROXIMATING DIFFERENTIAL EQUATIONS FROM PHYSICS
Inthissection,weprovideadditionalresultsontheperformanceofourSSMconstructiontopredict
thenextvalueofadynamicalsystemgovernedbyanODE.Forthis,weagainconsiderthemodified
VanderPoloscillatorasdescribedinthemaintextandprovideadditionalresultsfortheBernoulli
equation. TheBernoulliequationtakesthefollowingform:
u˙(t)+P(t)u(t) = Q(t)u(t)n (20)
WeuseP(t) = cos(5t),Q(t) = sin(t)andn = 1. Weplotthetruesolutionandthepredictionof
2
LegTandFouTinFigure3(whereagainweshiftthetruefunctionby0.1toavoidoverlapping)and
reporttheperformanceofthe twomethodsinTable2. Again, wecompareperformancefor state
dimensionN = 33andN = 65. WenoticethatLegTsignificantlyoutperformsFouTbyatleastone
orderofmagnitudeforbothN = 33andN = 65.
16HIPPO-PROPHECY: STATE-SPACEMODELSCANPROVABLYLEARNDYNAMICALSYSTEMSINCONTEXT
Bernoulli VanderPol
LegT(N =33) 1.8(1e 8) 6.4(1e 6)
− −
FouT(N =33) 3.0(1e 7) 6.6(1e 6)
− −
LegT(N =65) 1.7(1e 10) 4.4(1e 8)
− −
FouT(N =65) 3.0(1e 7) 6.6(1e 6)
− −
Table2: MSEofLegTandFouTonBernoulli’sDifferentialEquationandVanderPol’sOscillator
withN = 33andN = 65
Figure3: (Left)SolutionandPredictionforVanderPol’sOscillatorwithN = 65.
(Right)SolutionandPredictionforBernoulli’sDifferentialEquationwithN = 65
WenowrecalltheequationfortheVanderPol’soscillatorthatweusedinthemaintext:
u˙(t) = µ(1 u(t)2)sin(t) (21)
−
whereµisahyperparameter. Inourexperiments,weuseµ = 7andN = 17. Weplotthesolution
and our predictions in Figure 3 and Table 2. We notice that for N = 33, Legt and FouT perform
comparablywhereasforN = 65LegToutperformsFouT.WeremarkthatbylookingatFigure3,we
canseethatthesolutionfortheVanderPol’sOscillatorisverysimilartoasquarewave. Notethatfor
thisfunctionitisveryhardtopredictthenextvalueduetotheverysteepsuddenincrease/decrease
initsvaluefollowingaflatregion. Hence,itdoesnotcomeasasurprisetoseethattheperformance
forbothLegTandFouTismuchhigher(byatleastoneorderofmagnitudeacrossallthedifferent
valuesofN thatwetested)ontheBernoulli’sequation,whichissmoother.
E.2. Ablationsonbehaviorofpredictionsforincreasingstatedimension
In this section, we aim to validate the intuition that if we increase the state dimension N, the
performanceofourparametrizationalsoincreasesasdepictedin1(e)subfigure. Thisisbecause,as
wehaveshowntheoretically,wecanexpressu˙ asaninfiniteseriesoftermswhichwetruncateafter
N terms. AsN increases, onehenceexpectstheapproximationtogetbetter. However, thisonly
holdsinthecontinuoussetting. Inthediscretesettinginstead,whichweconsiderinourexperiments,
wearelimitedbytheresolutionwithwhichwesampleourinputsignalu(t).
Forthisexperiment,wetakeWhiteSignalwithγ = 1,2and5andFilteredNoisewithα = 0.05,
0.1and0.3andcomparetheperformanceasN increasesforbothLegTandFouT.WeletN vary
from1to96withastepsizeof5. InbothWhiteSignalandFilteredNoise,theperformanceofLegT
rapidlydecreasesandthenseemstostabilize. ForFouTinstead,theperformancegetsworsefirst
andthendecreasesagainwithN. Aswebrieflymentionedinthemaintext,wehypothesizethisis
duetothefactthatforlowN themodelsperformscopying,i.e. itpredictsthecurrenttimestepas
thenextone. Ifthesignalisnottoodiscountinuous,copyingisastrategythatresultsinalowerror
17HIPPO-PROPHECY: STATE-SPACEMODELSCANPROVABLYLEARNDYNAMICALSYSTEMSINCONTEXT
sincethecurrentvalueofthesignalandthenextarefairlyclose. However,asN increases,themodel
startsperformingbetterthancopyinganditeffectivelymanagestopredictthenextvalueoftheinput
signal.
E.3. ComparingdifferentInitalizations
Inthissection,weprovidesomefurtherinformationontheresultsobtainedin1(d)subfigure. Inthe
experiment,weuseaMixedDatasettotrainthemodelconsistingof: Sumsofrandomlydrawnsines
of frequencies between 0 and 50, Randomly Drawn White Signal’s using a frequency uniformly
drawnbetween[0.3,1.5]andrandomLegendrepolynomialsuptodegree15. Weempiricallyfound
thisdatamixturetobebeneficialforthemodeltonotoverfittoomuchtoaspecificfunctionclass.
ThemodelisthenevaluatedonLinearfunctionswithslopesrangingfrom[ 10,10],theVanderPol
−
Function,theaforementionedMixedDatasetandFilteredNoisefunctions. Specifically,weconsider
threelearningsettings:
1. InitializethemodelparamatersA,BasproposedbyHiPPO-LegTandrandomlysampleC
andDfrom (0,I). TrainingC&Donnextvaluepredictions.
N
2. InitializethemodelparamatersA,BasproposedbyHiPPO-LegTandinitializeCandDto
beourproposedC,D. TrainingC&Donnextvaluepredictions.
3. InitializethemodelparamatersA,BasproposedbyHiPPO-LegTandinitializeCandDto
beourproposedC,D. TrainingallofA,B,C&Donnextvaluepredictions.
ForallmodelweuseN = 32asthisshowedagoodtrade-offbetweenperformanceandefficiency.
Themodelsaretrainedonabatch-sizeof128performing1000epochs(8000gradientsteps). Our
findingsarethefollowing: Thebestgeneralizingoverallmodelisourexplicitweightconstruction
alongwithinitializingtoourconstructionsofC,Dandtrainingonallparameters. Mostimportantly
the model initialized with HiPPO A,B and gaussian C,D performs much worse and seems to
struggle in finding a weight construction that lets it adapt optimally to predicting the next signal
valuefromonlyitspreviousobservations. Thissuggeststhatourweightconstructionscouldserveas
anintializationschemethatcouldallowthemodeltobetteradapttocontext.
18