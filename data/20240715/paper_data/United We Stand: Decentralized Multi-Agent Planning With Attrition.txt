United We Stand: Decentralized Multi-Agent
Planning With Attrition
NhatNguyena,*,DuongNguyena,GianlucaRizzob andHungNguyena
aTheUniversityofAdelaide,Australia
bHESSOValais,Switzerland,andtheUniversityofFoggia,Italy.
Abstract. Decentralizedplanningisakeyelementofcooperative Process(Dec-POMDP)[34].However,thecomputationalcomplex-
multi-agent systems for information gathering tasks. However, de- ity of Dec-POMDPs presents a significant hurdle, making direct
spite the high frequency of agent failures in realistic large deploy- optimal solution search infeasible in polynomial-time [5]. Several
ment scenarios, current approaches perform poorly in the presence sampling-basedplanningalgorithmshavealsobeenproposedtoim-
of failures, by not converging at all, and/or by making very ineffi- prove computational efficiency, particularly for special classes of
cientuseofresources(e.g.energy).Inthiswork,weproposeAttrita- Dec-POMDPs, such as multi-robot active perception [49]. A first
bleMCTS(A-MCTS),adecentralizedMCTSalgorithmcapableof familyofapproachestoaddressthisisgivenbypoint-basedmeth-
timelyandefficientadaptationtochangesinthesetofactiveagents. ods,whichscalewell,buttheymaynotcovertheentirebeliefspace
Itisbasedontheuseofaglobalrewardfunctionfortheestimation well [35, 42]. Another set of algorithms is based on policy search
ofeachagent’slocalcontribution,andregretmatchingforcoordina- [40, 1] based on a parameterized policy representation. They can
tion.Weevaluateitseffectivenessinrealisticdata-harvestingprob- handleproblemswithlargeactionspaces,buttheymaygetstuckin
lemsunderdifferentscenarios.Weshowboththeoreticallyandex- localoptimaorrequiremanysamples[1].Thus,attentionhasturned
perimentally that A-MCTS enables efficient adaptation even under towardsalgorithmsbasedonMonteCarlotreesearch(MCTS),due
high failure rates. Results suggest that, in the presence of frequent to their ability to effectively explore long planning horizons, their
failures, our solution improves substantially over the best existing anytime nature [25], and their excellent performance in decentral-
approachesintermsofglobalutilityandscalability. ized settings [11, 7, 15], effectively overcoming the limitations of
otherapproaches.
Inmanypresent-dayMASapplicationscenarios,thedepartureof
1 Introduction
agentsfromthesystem(henceforthdenotedasattrition,anddueto
Cooperative multi-agent systems (MAS) are systems where multi- e.g.failuresorenergydepletion)isaverycommonfeature.However,
ple agents (such as autonomous vehicles/drones) work together to all of the main approaches to decentralized MAS planning assume
achieve a common goal such as maximizing a shared utility [17]. agentsarealwaysavailableandactivelycontributingtothejointplan-
Theseagentscancommunicateandcoordinatewitheachother,either ningprocess.Whenappliedtoscenarioswithattrition,theyperform
directlyorindirectly,tosolvecomplextasksthatarebeyondasin- inaheavilysuboptimalmannerandtheyoftendonotconvergeatall
gleagent’scapabilities.Examplesaredroneswarmsforautonomous [14,33].Inswarmrobotics,forinstance,agentattritionduetorobot
aerial surveillance or disaster relief operations, or teams of robots failures,damage,orenergydepletionaffectstheoverallswarmbe-
thatcollaboratetoexploreunknownenvironments,harvestdatafrom haviorandtaskcompletion.Designingrobustalgorithmsfordecen-
sensors,ormanipulateobjects,amongothers[47]. tralizedMASplanningcapableofeffectivelyhandlingagentlossis
CentralizedapproachesforaddressingtheMASplanningproblem criticalfortheirsuccessfuldeploymentmanyinpracticalscenarios.
donotscalewiththenumberofagents,astheamountofcomputa- The common approaches for scenarios with attrition are based
tionalresourcesrequiredtosolveitquicklybecomesprohibitive.In onperiodicallyresettingagents’learnedbehavior,andrestartingthe
addition,theamountofinformationexchangerequiredforacentral- learningprocess[2,38,21].Involatilesettingswithfrequentfailures,
ized planner to manage all agents can be unfeasibly high in large- suchafeaturemaysignificantlyhampertheoverallperformanceof
scale settings, particularly in remote areas and disaster scenarios theactiveperceptiontask,byslowingdowntheconvergencerateand
[44].Thus,recentresearchhasfocusedondecentralizedapproaches by keeping the system far from adapting and thus from achieving
foronlineMASplanning[49].Indeed,theyofferenhancedrobust- optimal operating conditions. Therefore, how to efficiently and ef-
ness,reducedcomputationalburden,andlowercommunicationload, fectivelyperformonlineMASplanninginthepresenceofattrition,
particularlyoninfrastructure-basedcommunicationssuchascellular whileachievingfastconvergence,isakeyopenissue.
radioaccessnetworks [11]. In this paper, we develop a novel decentralized planning algo-
The main challenge in decentralized approaches for coopera- rithmthatachievesbothoftheseobjectives.Ourapproachisbased
tive MAS is optimizing agents’ actions in a distributed manner to onMCTSandtheuseoftheglobalrewardinsteadofthelocalone
maximize a global reward function. This problem can typically be intheestimationofeachagent’slocalcontribution.Moreover,itex-
modeled as a Decentralized Partially Observable Markov Decision ploitsregretmatching(RM)[22]tocoordinatetheactionsbetween
agents.Weprovethatourapproachguaranteesthattheaveragejoint
∗CorrespondingAuthor.Email:nhatdaoanh.nguyen@adelaide.edu.au
4202
luJ
11
]AM.sc[
1v45280.7042:viXraaction of all agents converges to a Nash equilibrium (NE) if every agentsmayfailabruptly,alloftheabove-mentionedapproachesdo
agentappliesthesameRMprocedureinanycooperativegamewith notapply,astheydonotallowadjustingtoattritioninagents’popula-
a submodular utility function. Arriving at an NE guarantees no di- tions.Inthepresentwork,weshowthatthesuboptimalityofcurrent
verginginterestbetweentheagents,andthereforeitensuresthatall Dec-MCTSalgorithmsisduetotheusageofthemarginalcontribu-
participantscomeintoaself-enforcingagreementtoeffectivelyco- tion combined with the submodular properties of the global utility
ordinatetheiractiondecisionsinadecentralizedmanner.Themain functions.
contributionsofourworkare: AnotherbodyofliteratureonrelatedworksconcernsOpenAgent
• WedevelopAttritableMCTS(A-MCTS),anewonlinedecentral- Systems (OASYS), in which agents can enter and leave over time.
ized planning algorithm, based on MCTS and Regret Matching, MostsolutionstoOASYSareeitherfullyorpartiallyoffline,i.e.,of-
that can quickly and efficiently adapt the plan to settings where flineplanningwithonlineexecution[12,9]oronlineplanningwith
agentsfail,evenathighrates. precomputedofflinepolicies[18].Thisisnotfeasibleinapplications
• Weshowthat,bymodulatingtheutilityfunctionforeachagent, withsignificantsourcesofuncertainty,particularlywhentheenviron-
undertheassumptionofsubmodularity,successiveiterationsare ment’sstateormentalmodelsoftheagentsareunknowninadvance,
guaranteedtoimprovejointpolicies,andeventuallyleadtocon- andwhentheagents’failuresoccurabruptlyduringexecution.Are-
vergenceofouralgorithm.Weproveastrongconvergenceresult centwork[23]proposedafullyonlineapproachthatleveragescom-
forapproximatingapure-strategyNashequilibriuminafullydis- municationbetweenagentsrelatedtotheirpresencetopredicttheac-
tributedfashion. tionsofothers.Thisapproachassumesthatagentscancommunicate
• We evaluate our proposed approach in several information- theirexistenceimplicitly.Inscenarioswherethecommunicationis
gatheringscenarioswithattrition.Resultssuggestthat,inthepres- intermittentoragentsvanishwithoutwarning,theunforeseeablefail-
enceoffrequentfailures,oursolutionimprovessubstantiallyover urecansignificantlyimpactcoordinationandplanning,jeopardizing
thebestexistingapproachesintermsofglobalutilityandscalabil- theoverallperformance.Inaddition,thecomputationalcomplexity
ity. of modeling each other’s presence and predicting their actions can
makethesystemcomputationallyintractableonalargescale.Thus,
itisn’tdirectlyequippedtohandlesuddenagentfailuresandmayre-
2 RelatedWork
quirefurtherrefinementtobeviableinlarge-scaleorhighlydynamic
environments.
Information-gathering problems are often modeled as sequential
Ourpaperfocusesonproblemswhereagentsexperiencehardfail-
decision-makingproblems[48].Whentherearemultipleagents,de-
uresinanabruptandunforeseeablefashion.Thisunpredictablena-
centralizedinformationgatheringcanbeviewedasadecentralized
turemakestheexistingtechniquesnotdirectlyapplicable.Therefore,
POMDP[5].ThedominantapproachtoDec-POMDPistofirstsolve
moreresearchisneededtoenhancetherobustnessandresilienceof
the centralized, offline planning over the joint multi-agent policy
multi-agentsystemsinsuchuncertainattritionscenarios.Ourwork
space, and then push these policies to agents to execute them in
explicitlytacklesthischallengebyprovidinganewapproachforon-
a decentralized fashion [34]. When the state of the environment or
linedecentralizedplanningformulti-agentsthatdoesnotrequirepre-
agentsisnotknownaheadoftime,theseapproachesbecomeinfea-
computedofflinepoliciesandmentalmodels.Instead,agentsreason
sible.FullydecentralizedDec-POMDPsolversexist[43].However,
about the actions and existence of others using directly communi-
theyrequiresignificantmemoryandincurhighcomputationalcom-
catedinformation.Weemployacomputational-effectivegame-based
plexityduetotherequirementstocomputeandstoreallthereachable
techniquetocoordinateagents,enablingadaptivedecision-makingin
jointstateestimations[26].
thepresenceofpeerfailureswhileensuringfastconvergenceinpoly-
Recently, simultaneous distributed approaches based on MCTS
nomialtimerelativetosystemsize.
havegainedsignificantinterestduetotheirflexibilityintradingoff
computationtimeforaccuracy.Thekeyideaistousetheuppercon-
fidencebound(UCB)[25]forplanningthebestcourseofaction.To 3 ProblemFormulation
implementcooperationbetweenagents,thesemethodsusuallykeep
apredefinedmodeloftheteammates,whichcanbeheuristicorma- In this paper, we consider a set N of N autonomous agents mov-
chine learning trained [11, 15, 10]. However, as they are based on ing within a given area of space. We consider a set of R regions
trainedknowledge,theyareunsuitableforonlineplanninginsettings ofinterestinagivenarea,whereR k isthek-thelementoftheset.
thatchangeunpredictably,suchasindisastrousenvironments. We assume each region is a sphere with an equal radius, however,
Toaddressthis,newapproachesbasednotonaprioriknowledge theformulationcouldextendtomorecomplexmodels.Withoutloss
aboutagents’behavior,butoninformationsharingamongthem,have of generality, we assume agents move along an undirected graph
beenproposed(Dec-MCTS[7]).AkeyaspectofDec-MCTSandall G = (V,E) that is placed in the same space as the regions of in-
itssubsequentvariations[27,28,33]isthateachagentisassigneda terest.Eachvertexv i ∈ V representsalocation,andeachedgee ij
localutilityfunction,whichdoesnotmeasurethetotalteamreward representsafeasibleroutefromvertexv itov j.Akeypropertyofthis
butthecontributionofthatagentonly.Todealwithanyuncertainty graphisthatittraversesatleastonceeveryregionofinterest.This
thatarisesduringthemission,Dec-MCTSalgorithmsallowforon- graph typically models constraints to agent trajectories due to the
linereplanningduringexecution,byhavingagentsupdatetheirbe- morphology of the monitored environment, presence of obstacles,
liefsaboutthesystem. regions of interest distribution, and characteristics of agent move-
Underhighuncertaintyscenarios,agrowingbodyofliteraturere- ments,amongothers.Thespecificwayinwhichthegraphisderived
viewintheareaofmulti-dronesystems[45,29,16,20]exploresthe isthusapplicationandcontext-dependent[33].Thegraphisdefined
significantchallengesposedbyagentattrition–thelossorremoval atthebeginningofthemission,itdoesnotchangeovertimeanditis
ofindividualdrones(duetomechanicalfailures,environmentalfac- knownbyallagents.
tors,andhumanerrors),andhighlightstheneedtoaddressattrition The path of agent n, denoted as pn, is an ordered list of edges
for robust system performance. In systems with attrition in which pn = (en,en,...), such that two adjacent edges in the path are
1 2connected by a vertex of the graph. With p = (p1,...,pn,...,pN) Constraint(2)derivesfromimposingthatthetotalpathlengthfor
we denote the joint paths of every agent. Let B denote the maxi- the agent n to be less than the travel budget B available to each
mumpathlengthofeachagent,whichequalsthenumberofedges agent.Intuitively,ourgoalistofindapathforeachagentsuchthat
an agent can traverse. Such a maximum value is derived from the theglobalutilityassociatedwithallregionsobservedbyallagents
agent’sspeed,butitmayalsocapturevariousconstraints,e.g.dueto during the mission is maximized, while a subset F of agents fail.
finite storage capacity, among others. To any path pn we associate Suchanoptimizationproblemcannotbesolvedefficiently.Indeedit
acostb(pn),equaltothenumberoftraversededges.AregionR iseasytoseethatProblem1isavariantofthewell-knownNP-hard
k
isobservedifitistraversedbyapathofanagent.EveryregionR travelingsalesmanproblem.
k
is associated with a utility U(R ), which models the value of the
k
information that agents may collect from it. For ease of analytical 4 AttritableMCTSwithRegretMinimization
treatment, we assume that it takes one unit of time to traverse any
edgeandthatanyexchangeofinformationamongagentsisinstan- Inthissection,wefirstgiveabriefintroductiontoMonteCarloTree
taneous.Notehoweverthatourapproachcanbeeasilyextendedto Search and its most popular decentralized version. We then show
accountfornonzeroexchangeduration,aswellasforedgetraversal the root cause of the inefficiency of existing decentralized MCTS
timesthatdifferamongedgesandagents. approacheswithattrition.
Finally, we assume each agent can exchange information at any MCTSisanexcellentapproachforonlineplanningproblems[25].
pointintimewithanyotheragent.Thismodelsscenariosinwhich ThetreeT n foragentnisdefinedsuchthateachnodesofthetree
agents have a wireless interface to a cellular access network. We representsastateandeachedgeastartingfromthatnoderepresents
assume the information exchange to be instantaneous, independent an available action. A branch from the root node to another node
from the amount of information shared, and reliable, with no loss. representsavalidactionsequence.Thetreeisincrementallygrown
Intheexperimentalsection,werelaxthisassumptionandinvestigate viaafour-stepprocess:selection,expansion,rollout,andbackprop-
the impact ofnonidealities in information sharing on theeffective- agation. Decentralized Monte Carlo Tree Search (Dec-MCTS) [7]
nessofourapproach. extendsthepowerofMCTStoMASusingintentionsharing.Specif-
ically,agentnmaintainsaprobabilitymassfunctionq (x )overthe
n n
setofallpossibleactionsequencesX ,wherex ∈X isaprimitive
3.1 Multi-agentplanningwithattrition n n n
actionsequence.Theintentionsofotheragentsexceptagentnarede-
We denote the information-gathering task as a mission, for which notedbyq −nandX −n.Bytakingaprobabilisticsamplingfromthe
eachagentperformsindependentactionstoachieveacollectivegoal communicatedintention,eachagentcanreconstructtheglobalutil-
- maximizing the global utility for the whole team. Each agent n ity.Tocreatebettercoordination,ratherthanoptimizingdirectlyfor
plansitspathpnandcoordinateswithothersinadecentralizedman- the global utility U
g
of the entire team, each agent n instead opti-
ner while satisfying the given budget constraint B on path length. mizesforalocalmarginalcontributionutilityfunctionU n.Thatis,
This formulation of the information gathering problem generalizes agentnestimatestherolloutscoreforexecutingx nas:
many multi-agent path planning problems, such as team orienteer-
F (x )=U (x ,x )=U (x ,x )−U (x ), (4)
ingproblem[6].Weconsiderascenario,inwhichmissionplanning n n n n −n g n −n g −n
isperformedinadecentralizedmannerforscalabilityandcomputa-
whereU (x )istheglobalutilitywithoutthecontributionofagent
g −n
tionalfeasibility,asmentionedinSection1.Thus,eachagentplans
n.
itspathwhileconsideringthepotentialactionsofotheragentsand
We now analyze Dec-MCTS asymptotic behavior when agents
theteam’stotalutility.WeconsiderscenarioswhereasubsetFofthe
fail.Weareparticularlyinterestedinsubmodularrewardfunctions,
Nagentsfailduringthemission.Wefocusonhard failures,where
frequentlyarisingindatacollectionproblems[13,39].Submodular
agentsinterruptrewardcollectionandinformationexchange.Weas-
setfunction,whichisdefinedinDefinition1,satisfiesthediminish-
sumethesetofagentsthatfailFisunknowninadvanceandthetime
ing returns property. Regarding the information-gathering problem
atwhichtheyfailtobedeterminedbyanyarbitrarycriteriaordistri-
discussedinthispaper(seeSection3),themarginalgainofaddinga
bution.Therefore,oursolutiondoesnotrelyonknowingitssizeand
newlocationtothesetofvisitedlocationsdecreasesasthenumber
probabilitydistribution.Intheoccurrenceofafailure,alltheutility
oflocationsvisitedincreases.
collectedbythefailedagentislost,i.e.itisnotconsideredanymore
inthecomputationoftheglobalutilityofthemission.Thismodels Definition1(Submodularsetfunction). Letg : 2Ω → Rbeaset
atypicalsetupininformationgathering,inwhichdatacollectedby function where 2Ω is the power set of Ω. Then g is a submodular
agentsisrelayedtodatasinksonlyattheendofthemission. functionifforeveryX,Y ⊆ ΩwithX ⊆ Y andeveryx ∈ Ω\Y
Our goal is to provide an efficient planning and coordination thefollowinginequalityholds
mechanism that can quickly adapt to agent failures and maximize
theglobalutilityofthemissionwithintheagent’sbudgetconstraint. g(X∪x)−g(X)≥g(Y ∪x)−g(Y).
LetP = (P1,...,Pn,...,PN),withPn denotethesetofallpossi-
In particular, at iteration t, let x denote the chosen action se-
blepathsoflengthB whichstartsatagentnstartingposition.We n
quenceofagentnandx denotethecombinedsampledactionse-
definethefollowingproblem: −n
quences of other agents. Assume that at the next iteration t+1, a
Problem1. (Multi-agentplanninginattritionsettings) subsetofagentsfails.Letx′ bethecombinedsampledactionse-
−n
quencesofallagentsexceptagentnandthelostagents(i.e.,thatis
maximizeU g(p) (1) x′ ⊆x ).
p∈P −n −n
Proposition 1. If the global objective function U is submodular,
Subjectto: b(pn)≤B, ∀n∈N (2) then F(t+1)(x∗) ≥ F(t)(x∗) by the diminishingg return property
n n n n
0≤|F|≤N (3) duetosubmodularity,whereF (x )isdefinedin(4).
n nSynchronizethebestresponsejointpolicyxBR
^
X
3 xR 3M
X^ xR 1M
1
^ xRM
X 2
2
GrowthesearchtreeusingthebestresponsejointpolicyxBR SolvethecooperativegameusingRegretMatching
andcommunicatethebestactionsX^ andcommunicatethesolutionxRM
Figure1. OverviewoftheA-MCTSalgorithm.AgentsincrementallygrowthesearchusingthebestresponsepolicyxBRandcommunicatetheirbestactions
Xˆ.RegretMatchingisthenusedtocomputedistributivelyajointpolicyforthecooperativegame.Thesesolutionsaresynchronizedandthemostpayoff-
dominantischosenasthebestresponsepolicyxBR.
Proposition 1 states that if some agents fail during the mission, Algorithm1A-MCTSalgorithmforagentn
the remaining agents would mistakenly perceive that the contribu-
Input: GlobalobjectivefunctionU ,actionsbudgetB
g
tion of their previous actions increases. Hence, they would not be Output: bestactionsequencex∗ foragentn
aware of the actual reduction of the global utility and update their n
plans1.Fixingthisissuerequiresbothanew,context-awarewayto
1: T n←InitilizeMCTSTree
2: whilecomputationbudgetnotmetdo
compute the reward and a new way to coordinate the actions with 3: Xˆ n←SelectSubsetFrom(T n)
other agents on this new reward function. In the next section, we 4: (Xˆ,Xˆ −n)←CommunicateandUpdate(Xˆ n)
presentourproposedalgorithmforthemulti-agentplanninginattri-
5:
xBR←RegretMatchingCoordination(Xˆ)
tionsettingsproblem1.
6: forfixednumberofiterationsdo
7: x n←D-UCTSelect,Expand&Rollout(T n,B)
4.1 OverviewoftheA-MCTSalgorithm 8: F n←U g(x n,xB −nR)
9: T n←Backpropagation(T n,F n)
We develop Attritable MCTS (A-MCTS), an online decentralized 10: x∗ n←BestNextAction(T n)
MCTS algorithm that quickly adapts to agents’ attrition and effi- 11: returnx∗
n
cientlycoordinatesactionbetweentheremainingagents.Itsperfor-
mance mainly relies on two key factors, including the joint-utility-
Moreprecisely,givenasetofallpossibleactionsequencesofall
guideddecentralizedtreesearch,andthebestresponsepolicygiven
agentsX =(X ,X ),A-MCTSwillperiodicallycomputea“best
thesharedintentionsofothers.EachagentrunsA-MCTSdistribut- n −n
response"setofjointactionsequencesthatmaximizethejointutil-
edlytoplanforitselfapaththatisexpectedtomaximizethetotalutil-
ity for all participants xBR := {xBR,xBR} (Line 5). Each agent
ityofthewholemission.Agentsthenexecutethefirstplannedaction n −n
willthenassumeotheragentscoordinatelydeterminetheirpolicies
andobserveanychanges.Afterthat,theyperformreplanningfrom
following such “best response" xBR and uses such information to
theirnewstateandupdatetheplannedpathsbasedonnewlyavailable −n
computetheutilityforitsactionsequenceselectionwhilegrowing
information.Thesearchtreemaybeprunedbyremovingallchildren
the MCTS tree (Line 8). In general, the cardinality of X can be
of the root except the selected branch. This cycle of planning and n
verylargeanditgrowsexponentially.Toreducethecomputationand
executioncontinuesuntilthetravelbudgetexpires.Thepseudo-code
communication requirements, we consider only those dynamically
ofA-MCTSforagentnisshowninAlgorithm1. updatedsubsetsXˆ ⊆ X ofthemostpromisingactionsequences.
The tree T n of agent n is incrementally built over its action se- The set Xˆ is chon sen asn the best rollouts of M fixed nodes in the
quencesspaceX whileconsideringthepossiblebehaviorsofothers n
n searchtreeT withthehighestdiscountedempiricalaverage(Line
X (Line 6-9). In the selection phase, the discounted upper con- n
−n 3).Wethendefinethefollowingproblem:
fidenceboundonTree(D-UCT)[7]isappliedtohandletheabrupt
changesinrewardvaluescausedbytheactionsofotheragents. Problem2. (Bestjointpolicyformulti-agentplanning)
Thekeyideaofourproposedalgorithmistohavethesearchtrees
ofeveryagentbeguidedbythesameutilityofthejointactionse- maximize U (x ,x ,...,x ) (5)
g 1 2 N
quences. This is achieved by letting all agents optimize their local (x1,x2,...,xN)
actionsusingtheglobalutilityU directly(Line8).Eachagentcan
g
thendecideitspathx nindependentlytomaximizeU g andbeaware Subjectto: x ∈Xˆ , ∀n∈N (6)
ofthechangeintheglobalrewardsimmediatelyiftherearefailures n n
in the system. However, the uncertainty in other agents’ plans has The objective is to find an action profile (x ,x ,...,x ) that
1 2 N
alsobeenshowntodegradetheoverallperformancewhenusingthe maximizes the global utility U (·). Such an optimization problem
g
globalobjectivefunctiontooptimizelocalactions[46].Toovercome
cannotbesolvedefficiently.IndeeditisNP-hardtomaximizeasub-
thisissue,weproposetoleteachagentimproveitspolicyiteratively
modularfunction[37].SeekingaNashequilibrium(NE)(whereeach
whileassumingotherskeeptheirpoliciesfixed.
agentpolicyisthebestresponsetotheothers)thatachievesagood
efficiencycomparedtotheoptimalsolutionismoreaccessible[36].
1ForanalysisofDec-MCTSbehaviorwhenagentfailuresoccuraftertheal-
gorithmhasconverged,pleaseseeAppendixAintheSupplementaryMa- A greedy algorithm is usually employed to find an approximation
terial solution [31]. However, we will show later with simulations thatgreedy solutions can be substantially suboptimal even in scenarios Algorithm2RegretMatchingCoordinationalgorithm
with few agents. In the following section, we provide a distributed
Input: Global objective function U , joint compressed action se-
g
regret-basedsolutiontoProblem2thatquicklyandefficientlycom- quencessetXˆ
putesanNEjointpolicyformulti-agentsystems,regardlessoftheir Output: BestresponsejointactionsequencesxBR
complexity.
1: Everyagentn∈N performsthefollowingsteps2−9
2: InitializeRtozeroesandptouniformlyrandom
3: fort=1,2,... do
4.2 RegretMatchingForCooperativeCoordination
4:
x(t)←Sample(Xˆ,p)
5: foreachx nm ∈Xˆdo
Inthispaper,weconsiderthedistributedsolutionoftheoptimization 6: R nm←R nm+U g(x nm,x( −t) n)−U g(x(t))
problem 2 where each agent decides its path based on local infor- 7: Updatep(x nm)usingEq.(7)
mationandlimitedcommunicationfromitspeers.Weaimtodesign 8: xR nM←argmax xim∈Xˆ i[p(x im)],∀i∈N
a decision-making method that is capable of operating and adapt- 9: xRM ←CommunicateandUpdate(xRM)
−n n
ingwithoccasionalcommunicationorless,whereeveryagentacts 10: returnxBR←argmax (xR nM,xR −M n)[U g(xR nM, xR −M n )]
solelybasedonitslocalobservationanddoesnotneedtoconstantly
communicateeverydecisionwiththeothers.Thisistoguaranteethat
4.3 AnalysisandDiscussion
thealgorithmcaneffectivelyhandletheagentattritionsituationde-
scribedinSection3.1.Themaindifficultyhereishowtoensurethe Ithasbeenshownin[4]thatthereexistsnopolynomialtimealgo-
independentdecisionsoftheagentsleadtojointlyoptimaldecisions rithmtocomputeapureNEinmultiplayernonzero-sumstochastic
forthegroup.Toaddressthischallenge,weformulatetheproblemof games.Hence,weemployanapproximatemethodoffindingtheNE
findingforeachagentanactionsequencethatcollectivelymaximizes by proposing a decentralized Nash selection method based on Re-
thejointutilityasamulti-agentcooperativegame.Wethenproposea gretMatchingformakingchoicesinamultiplayermatrixgamefor-
distributedmechanism,whereeveryagentindependentlysimulatesa mulatedateachdecision-makingstate.RegretMatchingisaregret-
multi-playercooperativegamebasedonthelocalinformationavail- based algorithm for learning strategies in games, and is often used
able to itself and solves the game by self-play. For this purpose, a tocomputecorrelatedequilibriainmulti-playerrepeatedgameswith
gametheorylearningalgorithmbasedontheRegretMatchingtech- imperfectinformation.Althoughtheregretmatchingtechniquehas
nique[22]isemployedtoapproximatetheNashequilibriumofthe been widely used for non-cooperative games, its application in co-
game. operative games, such as the problem studied in our paper with a
LetXˆ = (Xˆ ,Xˆ )denotethejointsetofactionsequencesthat submodular utility function, has only been recently explored [32].
n −n
aresharedbetweenallagents,andx denotetheactionsequencem Inthiswork,byleveragingthesubmodularitypropertyofthejoint
nm
ofagentn.Inourapproach,periodically,everyagentindependently objectivefunction,weemployRegretMatchingasaself-playtech-
constructsamatrixgameinwhichthesetofplayerscontainsallthe niquetoindependentlylearnaNash-basedstrategyforeachplayer.
active agents and the set of actions is Xˆ. At this stage, each agent WetheoreticallyproveastrongerresultofconvergenceusingRegret
appliestheRegretMatching(RM)procedureasproposedin[22]to Matchingtoanapproximatepure-strategyNashsolution(seeDefi-
itsestimatedmatrixgametocomputeabestresponsejointdecision. nition2),ratherthanthecommonly-usedcorrelatedequilibrium,in
Thepseudo-codeofourRMgameisshowninAlgorithm2. gameswhereplayers’utilityfunctionsaresubmodular.
TofurtherimprovetheperformanceofRMincooperativesettings,
Definition 2 (Pure-Strategy Nash Equilibrium). A pure-strategy
welettheagentsusetheglobalutilitytocalculatetheregretsinstead
Nashequilibrium(PSNE)isajointactionprofilex∗ =(x∗,x∗ )∈
ofthelocalutility.Ateachiterationt,anactionx ∈X issampled n −n
nm Xˆ if for all n ∈ N and all x ∈ Xˆ such that: U (x∗,x∗ ) ≥
foreachagentbasedonaprobabilitydistribution.Letpdenotethis n n n n −n
U (x ,x∗ ).
probabilitydistributionwherep(x )istheprobabilityforx and n n −n
nm nm
(cid:80)M j=1p(x nj) = 1,∀n ∈ N.Withx(t) := {x( nt),x −(t) n},wedenote Theorem2. ThebestresponsejointdecisionxBR computedusing
the sampled set at iteration t, where x(t) is the sample action for RM,undertheassumptionofsubmodularutilityfunctions,isaPSNE
n
agentnandx(t) isthesampledactionsforallagentsexceptagent solution of the matrix-game representation generated by the set of
n. We then de− fin ne the regret of agent n for not taking action m at bestfeasiblepathsXˆ n ⊆X nchosenbyeveryagentateachdecision
iterationtasR(t) = U (x ,x(t))−U (x(t)).DenoteRasthe point2.
nm g nm −n g
cumulative regret matrix where an element R is the regret for
nm
x nmandR+
nm
=max{R nm,0}.Then,theprobabilitydistribution 5 ExperimentalEvaluation
pusedatthenextiterationwillbeupdatedas
ToassessourA-MCTSalgorithm,weconsiderthetaskofdatacol-
 lection from underwater wireless sensor networks (UWSN). Such
p(x nm)= (cid:80)M mR =+ n 1m R+ nm if(cid:80)M m=1R+ nm >0, (7) tasksusuallycallforacollaborationofmultipleautonomousunder-
 1 otherwise. watervehicles(AUVs)totraversetheenvironmentandgatherdata
M
fromsensors.Thescenarioconsistsof200randomlydistributedsen-
sors in a 4000 m × 4000 m plane, with a transmission radius of
DenotethejointdecisioncomputedusingRMbyagentn,whichis
50 m (typical for UWSN, e.g. [8]). The graph G of feasible paths
thesetofactionsequences,oneperagent,thathasthehighestprob-
abilityp(x ),asxRM.Similarly,letxRM bethecomputedsetfor is constructed using a probabilistic roadmap with a Dubins path
nm n −n
model [24]. This model employs curves to refine the straight-line
allagentsexceptagentn.Finally,thesesetsareexchangedbetween
everyagent,andthemostpayoff-dominantsolutionischosenasthe 2Forproofandanalysis,pleaseseeAppendixBandCintheSupplementary
bestresponsejointdecisionxBR =(xBR,xBR). Material
n −nFigure2. Impactofdifferentparametersonthealgorithms’performanceatthemissionend.Failuresintensity(thefractionofagentsthatfail)(a);Planning
time(b);Numberofexchangedcomponents(c);Actionsbudget(d),Numberofagents(e);Numberofrewards(f),andCommunicationfailureprobability(g).
Resultsarewith95%confidenceinterval.
segments connecting waypoints and is extensively utilized for rep- thebalancebetweenexplorationandexploitation).Unlessotherwise
resentingmotionconstraintspertinenttovehicle-likenonholonomic stated,eachagentcompressesitstreeintoasetof10possiblepaths
robotssuchasAUVs[3].ThegraphGconsistsof400verticesand andexchangesitwithitsteammatesevery50planningiterations.Un-
anaverageof19000edges. lessotherwisestated,weassume20agentsmoveinthegraph,witha
WebenchmarkA-MCTSagainstthefollowingbaselines: budgetBof9actions.Thesevaluesarechosenastheyhaveproven
• CentralizedMCTS(Central-MCTS):Asinglesearchtreeisbuilt toenableahightotalutilityscoreinthevastmajorityofscenarios
foralloftheM harvestingagentswiththeactionsofagentmare consideredinourexperiments.
attreedepth(m,m+M,m+2M,...). Tomodelattritioninthepopulationofagents,weassumethatev-
• Dec-MCTS[7]:Itisthestate-of-the-artdecentralizedmulti-agent eryagenthasthesameprobabilityoffailingduringthemissionand
planning.Init,agentsbuildtheirsearchtreewithamarginalcon- thatthetimeatwhicheachfailuretakesplaceisdistributeduniformly
tributionutilityfunctionandadaptthesametreeafterchurnsoc- atrandomthroughoutthemissionduration.Thekeymetricweuseto
cur. evaluatetheperformanceoftheconsideredalgorithmsistheInstan-
• Dec-MCTSwithreset(Dec-MCTS-Reset):LikeDec-MCTS,each taneousrewardcoverage(IRC),i.e.thefractionofavailablerewards
agent builds its search tree with a marginal contribution utility covered(i.e.collected)atagiventime.
function. Whenever churns occur, the tree of each agent is re-
set.Thisvariantisconsideredtoshowthatresettingthetreesfre-
5.1 PerformanceBenchmarking
quentlycouldhampertheoverallperformanceofthealgorithm.
• Dec-MCTS with global utility (Dec-MCTS-Global: Agents build
Inthefirstevaluationofouralgorithm’sadaptabilitytofailures,we
theirsearchtreewithaglobalutilityfunctionandadaptthesame
examine the impact of the failure intensity (i.e. fractions of agents
tree after churns occur. This variant is considered to show that
that fail during the mission) on the IRC at the mission end, illus-
alteringtheutilityfunctionalonewouldnotenhanceperformances
trated in Figure 2a. As expected, all algorithms experience perfor-
againstchurns.
mancedeclineswithincreasingintensity,reflectingreducedreward
• A-MCTS with greedy optimization (Greedy-MCTS): In this
coverageduetofewerremainingagentsinthesystem.Notably,with
scheme, we replace the RM Coordination (Algorithm 2) in our
over50%ofagentsfailing,Dec-MCTS-Resetsurpassesthenon-reset
A-MCTSwithagreedyalgorithm,inwhicheveryagentsequen-
versionduetothesmallersystemsizewhichrequiresfeweriterations
tiallypickstheactionsthatdeliverthehighestimmediaterewards
forexploration.Conversely,largersystemsnecessitatemoretimefor
forcollaboration.Thisvariantisconsideredtoshowthatgreedy
agentstolearnabouttheenvironment,hencefrequentresetshamper
solutionscanbesubstantiallysuboptimalinMAScoordination.
thealgorithm’sperformance.
Forallalgorithms,eachplanningphaseconsistsof500iterations,
Tofurtherelaborateonthismatter,weassessedtheimpactofplan-
the discounting factor is set to 0.9, and the exploration parameter
ningtimeontheIRCatthemissionend.AsFigure2bshows,other
is set to 0.4 (i.e., within the ranges recommended in [7] to ensure
baselinealgorithmsimprovedasplanningtimeincreased,withDec-MCTS-Resetstartingtooutperformthenon-resetwithplanningtime 5.2 TradeOffBetweenCommunicationLossand
largerthan750iterations.A-MCTS,ontheotherhand,requiressig- AttritionforA-MCTSAnalysis
nificantly less computational time yet still achieves the highest re-
wardsregardlessofthefailureintensityandrates,thusprovingitself
asagoodsolutionforonlinereplanning.
Thenumberofpathsexchangedbetweenagentssignificantlyin-
fluencessystemcomplexity.Increasedinformationexchangepoten-
tially leads to better algorithm performance, albeit at the expense
ofgreatercomputationalresourcesandtime.Toexaminethistrade-
off,inFig.2cweevaluatedtheimpactofdifferentnumbersofex-
changedcomponentsontheIRCatthemissionend.Asexpected,our
proposed algorithm’s performance improved with more exchanged
components,whilediscountedalgorithmsshowednobenefit.Indeed,
withmoreexchangedcomponentstheutilityofthejointpolicyfound
byregretmatchingimprovestoo.However,giventhefinitenumber
ofoptimalpoliciesinamulti-agentgame,escalatingthenumberof
componentseventuallyyieldsdiminishingreturns.
Astheaboveresultsshow,resettingthetreewouldnotconsistently
leadtoimprovementbecausetheplanningprocessinvolvesinitialex-
plorationinwhichagentstakerandomactionstolearnrewarddistri- Figure3. Impactofallowedinter-agentcommunicationlossontheperfor-
bution.Resettingwithoutsufficientplanningtimeresultsinsubopti- manceofA-MCTSattheendofthemission.
maljointpolicies.Additionally,theuseofthemarginalcontribution
Inourapproach,repeatedcommunicationlossisusedasanindi-
utilitycombinedwithasubmodularrewardfunctionhampersagents’
cationofattrition.However,inpracticalsettings,inter-agentcommu-
abilitytorecognizeglobalrewardreductionandhenceadapttofail-
nicationcanbeunreliableandintermittent.Ifthealgorithmismore
uresefficiently.Moreover,samplingotheragents’actionsequences
sensitivetocommunicationloss,itcanmistakenlytreatdelayedmes-
increasesvarianceinglobalutilityestimationanddegradestheco-
sagesasagentfailures.
ordinationquality.Byassumingthatthepoliciesofotheragentsare
Tobetterunderstandsuchimpact,inthissection,westudyhowtol-
fixed,A-MCTScanovercomethisinstabilityissueandadapttoagent
erancetocommunicationlossaffectstheperformanceofA-MCTS.
failuresbetter.Indeed,withregretmatchingaidsindiscoveringbetter
Specifically,weparameterizethistolerancelevelbythenumberof
jointpoliciesandthusprovidesbetterguidancefortheexploration-
times an agent must experience communication loss with another
exploitationofthesearchtree,ourmethodexhibitssuperiorperfor-
agentbeforetreatingitasattrition.Fig.3showstheIRCattheend
mancesinallcases.
ofthemissionagainstdifferentnumbersofallowedinter-agentcom-
Inanothersetofexperiments,weevaluatedtheimpactofaction
municationloss.Withupto5allowedmessagesloss,A-MCTSstill
budget B, the number of agents N, the number of rewards, and
showsnonoticeabledegradation.However,asthealgorithmismore
thecommunicationfailureprobability,foradefaultfailureintensity
communication loss tolerant, the performance declines. This is ex-
of50%.AsFig.2dshows,A-MCTSmanagedtooutperformDec-
pected because the remaining agents can not recognize churns fast
MCTSsubstantiallydespitethedifficultyofdecentralizedplanning
enoughandadaptefficiently.
withagrowingactionsbudget.Furthermore,aswedoubledthebud-
get of the action, the superiority of A-MCTS over the other tech-
niquestripled.A similarbehaviorisexhibited bythesystemwhen 6 Conclusions
we vary the number of agents. As shown in Fig. 2e, with a small
Achieving efficient coordination in multi-agent planning for infor-
numberofagents,thedifferencesbetweenouralgorithmandthedis-
mationgatheringisacriticalchallengeinpracticalsettingswithhigh
countedmethodsgrowto20%withanincreasingnumberofagents.
attrition rates. In this work, we proposed a new approach to tackle
Intheconsideredsettings,wealsoassesstheimpactofthedensity
thisissue.Ourproposedalgorithm,AttritableMCTS(A-MCTS),ef-
ofsensorsonthealgorithms’performancebyvaryingthenumberof
fectivelycoordinatesactionsamongagentswhileadaptingtoagent
sensorswithinthesamearea.AsshowninFig.2f,theIRCdeclines
failuresbyallowingallagentstojointlyoptimizetheglobalutilitydi-
asmoresensorsareintroducedinthesystem.Naturally,withanin-
rectlywithanewcoordinationtechniquebasedonregretmatching.
creasingnumberofsensorstheareathatmustbecoveredbyagents
OurempiricalevaluationdemonstratesthatA-MCTSimprovessub-
expandsaswell.Nevertheless,A-MCTSshowsbetterscalabilityas
stantiallyoverthebestexistingapproachesintermsofglobalutility
itconsistentlyoutperformsothermethods.
andscalabilityinscenarioswithfrequentagentfailures.Asafollow-
TheeffectivenessofcooperativeMASisnotablyinfluencedbythe
up,weintendtoextendA-MCTStomoredynamicsystemswhere
qualityofinter-agentcommunication.Tounderstandbettertheim-
newagentscanjoinandthecommunicationisprobabilistic.Another
pactofsuchlimitations,weevaluatedthealgorithms’performances
lineofinquiryistostudytheperformanceofouralgorithminprob-
under different communication failure probabilities between each
lemswithinter-agentdependency,wheretheactionsofanagentcan
agentpairduringamission.AsshowninFig.2g,thereisnonotable
onlybeenabledbytheactionsofanother.
decline in A-MCTS performance even when half of the communi-
cationisdisrupted,anditcontinuestooutperformbaselinemethods
with severely unstable communication. This highlights A-MCTS’s
abilitytoenableefficientcooperationamongagentsinhostileenvi-
ronmentswithrestrictedcommunication.References [27] M.Li,W.Yang,Z.Cai,S.Yang,andJ.Wang. Integratingdecision
sharingwithpredictionindecentralizedplanningformulti-agentcoor-
[1] C.Amato,D.S.Bernstein,andS.Zilberstein. Optimizingfixed-size dinationunderuncertainty.InIJCAI,pages450–456,2019.
stochasticcontrollersforpomdpsanddecentralizedpomdps. InInter- [28] M.Li,Z.Cai,W.Yang,L.Wu,Y.Xu,andJ.Wang. Dec-sgts:De-
nationalConferenceonAutonomousAgentsandMultiagentSystems, centralizedsub-goaltreesearchformulti-agentcoordination. InAAAI,
page459–466.InternationalFoundationforAutonomousAgentsand volume35,pages11282–11289,2021.
MultiagentSystems,2010. [29] F.F.Lizzio,E.Capello,andG.Guglieri. Areviewofconsensus-based
[2] O.AvnerandS.Mannor. Concurrentbanditsandcognitiveradionet- multi-agentuavimplementations.JournalofIntelligent&RoboticSys-
works.InECMLPKDD,pages66–81.Springer,2014. tems,106(2):43,2022.
[3] B.BarskyandT.DeRose. Geometriccontinuityofparametriccurves: [30] J.R.Marden,G.Arslan,andJ.S.Shamma. Cooperativecontroland
threeequivalentcharacterizations. IEEEComput.Graph.Appl.,9(6): potentialgames.IEEETransactionsonSystems,Man,andCybernetics,
60–69,1989.doi:10.1109/38.41470. PartB(Cybernetics),39(6):1393–1407,2009.
[4] K.BergandT.Sandholm. Exclusionmethodforfindingnashequilib- [31] G.L.Nemhauser,L.A.Wolsey,andM.L.Fisher. Ananalysisofap-
riuminmultiplayergames. InProceedingsoftheAAAIConferenceon proximationsformaximizingsubmodularsetfunctions—i. Mathemat-
ArtificialIntelligence,volume31,2017. icalprogramming,14(1):265–294,1978.
[5] D.S.Bernstein,R.Givan,N.Immerman,andS.Zilberstein. Thecom- [32] D. Nguyen, L. White, and H. Nguyen. Social optimum equilib-
plexityofdecentralizedcontrolofmarkovdecisionprocesses. Math. riumselectionfordistributedmulti-agentoptimization. arXivpreprint
Oper.Res.,27(4):819–840,2002. arXiv:2307.13242,2023.
[6] G.Best,J.Faigl,andR.Fitch. Onlineplanningformulti-robotactive [33] N.Nguyen,D.Nguyen,J.Kim,G.Rizzo,andH.Nguyen.Multi-agent
perceptionwithself-organisingmaps. AutonomousRobots,42(4):715– data collection in non-stationary environments. In IEEE 23rd WoW-
738,2018. MoM,pages120–129.IEEE,2022.
[7] G.Best,O.M.Cliff,T.Patten,R.R.Mettu,andR.Fitch. Dec-mcts: [34] F.A.OliehoekandC.Amato. Aconciseintroductiontodecentralized
Decentralizedplanningformulti-robotactiveperception. Int.J.Robot. POMDPs.Springer,2016.
Res.,38(2-3):316–337,2019. [35] J.Pineau,G.Gordon,andS.Thrun. Point-basedvalueiteration:An
[8] S.Cai,Y.Zhu,T.Wang,G.Xu,A.Liu,andX.Liu. Datacollection anytimealgorithmforpomdps.IJCAI,3:1025–1032,2003.
inunderwatersensornetworksbasedonmobileedgecomputing.IEEE [36] G.Qu,D.Brown,andN.Li. Distributedgreedyalgorithmformulti-
Access,7:65357–65367,2019. agenttaskassignmentproblemwithsubmodularutilityfunctions. Au-
[9] M.Chandrasekaran,A.Eck,P.Doshi,andL.Soh. Individualplanning tomatica,105:206–215,2019.
inopenandtypedagentsystems. InProceedingsoftheThirty-Second [37] N.RezazadehandS.S.Kia.Distributedstrategyselection:Asubmodu-
ConferenceonUncertaintyinArtificialIntelligence,pages82–91,2016. larsetfunctionmaximizationapproach.Automatica,153:111000,2023.
[10] S.Choudhury,J.K.Gupta,P.Morales,andM.J.Kochenderfer.Scalable [38] J.Rosenski,O.Shamir,andL.Szlak. Multi-playerbandits–amusical
anytimeplanningformulti-agentmdps. InAAMAS,pages341–349, chairs approach. In International Conference on Machine Learning,
2021. pages155–163.PMLR,2016.
[11] D.Claes,F.Oliehoek,H.Baier,K.Tuyls,etal. Decentralisedonline [39] Y.Satsangi,S.Whiteson,F.A.Oliehoek,andM.T.Spaan. Exploit-
planningformulti-robotwarehousecommissioning. InAAMAS,pages ingsubmodularvaluefunctionsforscalingupactiveperception. Au-
492–500,2017. tonomousRobots,42(2):209–233,2018.
[12] J. Cohen, J.-S.Dibangoye, and A.-I. Mouaddib. Open decentralized [40] S.SeukenandS.Zilberstein.Memory-boundeddynamicprogramming
pomdps. In2017IEEE29thInternationalConferenceonToolswith fordec-pomdps. InIJCAIProceedings-InternationalJointConference
ArtificialIntelligence(ICTAI),pages977–984.IEEE,2017. onArtificialIntelligence,volume22,page2009–2015,2011.
[13] M.CorahandN.Michael. Efficientonlinemulti-robotexplorationvia [41] M.Sewak.Deepreinforcementlearning.Springer,2019.
distributedsequentialgreedyassignment.InRobotics:ScienceandSys- [42] G. Shani, J. Pineau, and R. Kaplow. A survey of point-based
tems,volume13,2017. pomdpsolvers. InAutonomousAgentsandMulti-AgentSystems,page
[14] G.CybenkoandR.A.Hallman. Attritablemulti-agentlearning. In 353–386.Springer,2013.
Disrupt.Sci.Technol.V,volume11751,page117510L.International [43] M. T. Spaan, G. J. Gordon, and N. Vlassis. Decentralized planning
SocietyforOpticsandPhotonics,2021. underuncertaintyforteamsofcommunicatingagents.InAAMAS,pages
[15] A. Czechowski and F. A. Oliehoek. Decentralized mcts via learned 249–256,2006.
teammatemodels.InIJCAI,pages450–456,2020. [44] P.A.Trodden.Robustdistributedcontrolofconstrainedlinearsystems.
[16] C.Dinelli,J.Racette,M.Escarcega,S.Lotero,J.Gordon,J.Montoya, PhDthesis,UniversityofBristol,2009.
C.Dunaway,V.Androulakis,H.Khaniani,S.Shao,etal. Configura- [45] R.Williams,B.Konev,andF.Coenen.Multi-agentenvironmentexplo-
tionsandapplicationsof multi-agenthybriddrone/unmannedground rationwithar.drones. InAdvancesinAutonomousRoboticsSystems:
vehicle for underground environments: A review. Drones, 7(2):136, 15th Annual Conference, TAROS 2014, Birmingham, UK, September
2023. 1-3,2014.Proceedings15,pages60–71.Springer,2014.
[17] A.Dorri,S.S.Kanhere,andR.Jurdak.Multi-agentsystems:Asurvey. [46] D.H.Wolpert,S.R.Bieniawski,andD.G.Rajnarayan. Probability
IeeeAccess,6:28573–28593,2018. collectivesinoptimization.HandbookofStatistics,31:61–99,2013.
[18] A.Eck,M.Shah,P.Doshi,andL.-K.Soh. Scalabledecision-theoretic [47] J.XieandC.-C.Liu.Multi-agentsystemsandtheirapplications.Jour-
planninginopenandtypedmultiagentsystems. InProceedingsofthe nalofInternationalCouncilonElectricalEngineering,7(1):188–197,
AAAI Conference on Artificial Intelligence, volume 34, pages 7127– 2017.
7134,2020. [48] X.Yao,X.Wang,F.Wang,andL.Zhang. Pathfollowingbasedon
[19] A.GarivierandE.Moulines. Onupper-confidenceboundpoliciesfor waypointsandreal-timeobstacleavoidancecontrolofanautonomous
switching bandit problems. In Algorithmic Learning Theory, pages underwatervehicle.Sensors,20(3):795,2020.
174–188.Springer,2011. [49] K.Zhang,Z.Yang,andT.Bas¸ar.Multi-agentreinforcementlearning:A
[20] A.Goeckner,X.Li,E.Wei,andQ.Zhu.Attrition-awareadaptationfor selectiveoverviewoftheoriesandalgorithms. HandbookofReinforce-
multi-agentpatrolling.arXivpreprintarXiv:2304.01386,2023. mentLearningandControl,pages321–384,2021.
[21] M.K.HanawalandS.J.Darak. Multiplayerbandits:Atrekkingap-
proach. IEEETransactionsonAutomaticControl,67(5):2237–2252,
2021.
[22] S. Hart and A. Mas-Colell. A simple adaptive procedure leading to
correlatedequilibrium.Econometrica,68(5):1127–1150,2000.
[23] A.Kakarlapudi,G.Anil,A.Eck,P.Doshi,andL.-K.Soh. Decision-
theoreticplanningwithcommunicationinopenmultiagentsystems. In
UncertaintyinArtificialIntelligence,pages938–948.PMLR,2022.
[24] L.E.Kavraki,P.Svestka,J.-C.Latombe,andM.H.Overmars. Prob-
abilisticroadmapsforpathplanninginhigh-dimensionalconfiguration
spaces.IEEETrans.Robot.Automat.,12(4):566–580,1996.
[25] L. Kocsis, C. Szepesvári, and J. Willemson. Improved monte-carlo
search.Univ.Tartu,Estonia,Tech.Rep,1,2006.
[26] M.LauriandF.Oliehoek.Multi-agentactiveperceptionwithprediction
rewards.NeurIPS,33,2020.A TechnicalResults Lemma3. AssumethattheDec-MCTSalgorithmhasconvergedon
alltheagentsattimestepτ andthattheglobalobjectivefunction
A.1 DetailsofDiscountedUpperConfidenceBound 0
U issubmodular.Then
onTree(D-UCT) g
X(t+1)(x∗,γ)≥X(t)(x∗,γ), ∀t≥τ .
ConsideranarbitrarynodeswithasetofchildnodesA (s).When- n n n n 0
n
eversisvisited,thechildnodes′ ∈A n(s)withthelargestD-UCB Lemma3essentiallystatesthatonceDec-MCTSconverges,theD-
ischosenas UCBscorecalculatedbyagentnforitsconvergedactionsequence
a(t)(s)=argmaxX(t)(s′), (8) x isnon-decreasingevenifitdetectsthatsomeoftheotheragents
n n n
s′∈An(s) havefailed.Hence,italwayspicksandupdatesthesameactionse-
where X(t)(s′) is the D-UCB score for node s′ at iteration t. quence(i.e.,theseriesofactionsthathasthehighestD-UCBscores
n
X(t)(s′)isupdatedusingtheD-UCBalgorithm[19]asfollows. ateachcorrespondingdecisionnode).
n √
First,letγ ∈ (1/2,1)beadiscountingfactorandC p > 1 8an Letc,p ∈ x∗ n betwonodesintheconvergedactionsequenceof
explorationconstant,theupperboundconfidenceforchildnodes′is agent i, with c being the child node of p. After the algorithm con-
calculatedas vergesatiterationτ 0,bydefinition,thenodescandparegoingto
beselectedrepeatedly.Thus,atiterationt,thediscountednumberof
X(t)(s′,γ):=F¯(t)(s′,γ)+c(t)(s′,γ), (9)
n n n timescisvisitedcanbewrittenas
w anh der ce
(
nt)F¯
(n
s(t ′) ,( γs )′, iγ s) thi es et xh pe loav rae tr ia og ne be om nup si .rical reward for choosing s′,
N n(t)(c,γ)=γt−τ0 N
c(τ0)+t (cid:88)−τ0
γτ
Denote the discounted number of times the child node s′ of the τ=0 (15)
parentnodeshasbeenvisitedas =γt−τ0 N(τ0)+
1−γt−τ0+1
,
c 1−γ
N n(t)(s′,γ):=(cid:88)t τ=1γt−τ1(cid:110)
a(
nτ)(s)=s′(cid:111), (10)
withtheconstantN(τ0)isthediscountednumberoftimesnodecis
c
where1(cid:110) an(τ)(s)=s′(cid:111)istheindicatorfunctionthatreturns1ifnodes′ c ah to its ee rn ata iot nτ 0 t. iI snaddition,thediscountednumberoftimespisvisited
wasselectedatroundτ and0otherwise.
LetF n(τ)betherolloutscoreatiterationτ ≤tandN n(t)(s,γ)be
N(t)(p,γ)=γt−τ0
N(τ0)+t (cid:88)−τ0
γτ
thediscountednumberoftimestheparentnodeshasbeenvisited. n p
Thenattimet,theaveragerewardfornodes′iscomputedas τ=0 (16)
1−γt−τ0+1
F¯ n(t)(s′,γ)= N(t)(1 s′,γ)(cid:88)t τ=1γt−τF n(τ)1(cid:110)
a(
nτ)(s)=s′(cid:111), (11) =γt−τ0 N p(τ0)+ 1−γ ,
n
withtheconstantN(τ0)isthediscountednumberoftimesnodepis
andexplorationbonusas p
chosenatτ .Finally,theaccumulatedrolloutscoreforcatiteration
(cid:118) 0
c( nt)(s′,γ)=2C p(cid:117) (cid:117) (cid:116)lo Ng nN (t)n( (t s) ′( ,s γ, )γ) . (12) t (cid:88)tis
γt−τF n(τ)1(cid:110)
a( nτ)(p)=c(cid:111)
=γt−τ0F(τ0)+L
t (cid:88)−τ0
γτ
τ=1 τ=0 (17)
A.2 AnalysisofDec-MCTSPerformanceinAttrition 1−γt−τ0+1
Settings =γt−τ0F(τ0)+L ,
1−γ
Asshownin[7],Dec-MCTShasvanishingregretandconvergesas withtheconstantF(τ0)istheaccumulatedrolloutscoreforcatτ 0,
t → ∞. We prove here the behavior of Dec-MCTS after conver-
andListherolloutscoreforcateveryiterationsuptoτ asgivenin
0
gence. Recall that by convergence we mean each agent stays with
(13).Forbrevityofnotations,wedenotethefollowingvalues
thesameactionsequence(i.e.,theUCBscoreforeachactioninsuch
sequenceisthehighestatthatcorrespondingdecisionnode). N =γt−τ0 N(τ0)+
1−γt−τ0+1
,
Assume that Dec-MCTS converges at iteration τ (finite) for all c c 1−γ
0
a qg ue en nt cs e. oA ft ai gte er na tti no ,n at nd> x∗ −τ 0 n, dle et nox t∗ n esd te hn eo cte ont vh ee rgc eo dnv ae cr tg ioe nd sa ec qti uo en ncs ee s- N p =γt−τ0 N p(τ0)+ 1− 1γ −t− γτ0+1 ,
(18)
of every other agent except agent n. The rollout score received by
1−γt−τ0+1
agentnforexecutingtheactionsequencex∗ givenbythemarginal F =γt−τ0 F(τ0)+L ,
n c 1−γ
utilitywillthenbeaconstantL:
A=F(t+1)(x∗).
F(t)(x∗)=U (x∗,x∗ )=U (x∗,x∗ )−U (x∗ )=L. (13) n n
n n n n −n g n −n g −n
Atiterationt+1whenfailuresoccur,thevaluesforthediscounted
Assumethatatthenextiterationt+1,asubsetofagentsbecomes
numbersoftimesnodecandparechosen,andtheaccumulatedroll-
unavailableduetofailures.Letx′ bethecombinedsequencesof
−n outscoreforccanbeupdatedas
actionstakenbyallagentsexceptagentiandthelostagents.Thatis
x′ ⊆x∗ . N n(t+1)(c,γ)=N c+γt,
−n −n
N(t+1)(p,γ)=N +γt,
andtherolloutscoreagentireceivesforthesameactionsequence n p (19)
no Fw (i ts
+1)(x∗)=U (x∗,x′ )=U (x∗,x′ )−U (x′ ). (14)
(cid:88)t+1
γt+1−τF n(τ)1(cid:110) a( nτ)(p)=c(cid:111) =F cγ+A,
n n n n −n g n −n g −n τ=1withAistherolloutscoreforcatiterationt+1asgivenabove. thegameweseethattheD-UCBscoresforAgent1fluctuateuntilit
TheinequalityofLemma3nowcanbewrittenas convergesto{0.4607,0.5072}(after135iterationsinourexample).
TheempiricalaveragerewardofGoRight isestimatedbyAgent1
(cid:115) (cid:114)
F γ+A 2log(N +γt) F 2log(N ) by dividing its contribution (one diamond) to the global utility (11
c +c p ≥ c +c p
N +γt p N +γt N p N diamonds),whilethediscountednumberofvisitsisapproximately
c c c c
4,yieldingtheasymptoticscoreof0.5072.TheD-UCBscoreforGo
F γ+A F
⇔ c − c (20) Down(thesub-optimalaction)isnon-deterministicdependingonthe
N +γt N
c c randomchoicesmadebythetwoagentsduringtheinitialtransient.
(cid:32)(cid:115) (cid:114) (cid:33)
2log(N +γt) 2log(N ) Thisvalueisnotupdatedinconvergenceasthatbranchofthesearch
+c p − p ≥0.
p N +γt N treeisnotsampled,duetotheMCTSselectionpolicy.
c c
Using the marginal contribution as the utility improves stability
Let and convergence speed, but it causes issues when agents fail, as
(cid:32)(cid:115) (cid:114) (cid:33) shown.Atiteration150,Agent2fails(orleavesthegame).Inthis
f(t)= F cγ+A −F c+c 2log(N p+γt) − 2log(N p) case,eveniftheoptimalchoiceforAgent1wouldbeGoDown(due
N c+γt N c p N c+γt N c tothehigheramountofdiamonds),itstickswithGoRight.Thishap-
(21) pensbecauseboththeexplorationbonusandthelocalcontributionto
be a funtion of time t over the set of fixed paramters theoverallhypotheticalglobalrewardremainthesame,despitethe
{γ,c ,F ,N ,N }. realglobalrewardhasbeenreduced.
p c c p
Itcanbeverifiedthatf(t)isanincreasingfunctionasthederiva-
tiveoff(t)ispositivefort ≫ τ 0.Inaddition,ast ≫ τ 0,thein- B ProofofTheorem1
equalityof(20)becomes:
BeforegoingthroughtheproofofTheorem1,wefirstprovetheex-
F γ+A F
c ≥ c istenceofatleastonePSNEintheformulatedgame.
N N
c c
(cid:18) 1−γt−τ0+1(cid:19) Lemma4. Afinitecoordinationgamewillalwayshaveatleastone
⇔ A≥F c(1−γ)= γt−τ0F(τ0)+L
1−γ
(1−γ)PSNE,ifmaximizingplayers’localutilitiescorrespondstomaximiz-
ingtheglobalobjective,i.e.,theplayers’localutilityfunctionssat-
⇔ A≥L. isfy,∀x ,x′ ∈Xˆ , ∀x ∈Xˆ , ∀n∈N ,
n n n −n −n
The last inequality follows from the assumption that the global U (x ,x )−U (x′,x )>0 ⇒ Φ(x ,x )−Φ(x′,x )>0,
n n −n n n −n n −n n −n
utility function U is submodular. That is, having failures as time
g (22)
t+1 implies there are fewer agents collecting rewards, hence the whereΦ(·)isafunctionthatrepresentstheglobalobjective.
localutilityforagentiincreases(orremainsthesame).ThusA≥L.
SinceProposition1givesthatFτ+1(x∗)≥Fτ(x∗),thereexists Proof. Everyfinitecoordinationgameinwhichtheglobalobjective
n n n n
aτ forwhichf(t)isnon-negativeforsomet≫τ .Thisimpliesthe functionisalignedwiththelocalutilityfunctionsoftheplayers,that
0 0
UCBscoresforeachnodeintheactionssequencex∗ willremainthe is,satisfiesthepropertyasin(22),isageneralizedordinalpotential
n
highest.Thus,agentiwillcontinuetoselectx∗ afterfailures.This game[30].LetϕbeapotentialfunctionofacoordinationgameG.
n
concludestheproof. ThentheequilibriumsetofGcorrespondstothesetoflocalmaxima
ofϕ.Thatis,anactionprofilex=(x ,x )isaNEpointforGif
n −n
anonlyifforeveryn∈N,
ϕ(x)≥ϕ(x′,x ), ∀x′ ∈Xˆ .
Agent 1 n −n n n
Consider x∗ = (x∗,x∗ ) ∈ Xˆ for which ϕ(x∗) is maxi-
n −n
mal (which is true by definition for a finite set Xˆ), then for any
Agent 2 x′ =(x′,x ):
n −n
(a)
11 1.5 ϕ(x∗ n,x∗ −n)>ϕ(x′ n,x −n)⇔U n(x∗ n,x∗ −n)>U(x′ n,x −n).
Go Down
Go Right
Hence,thegamepossessesapurestrategyNE.
1
We now proceed with the main proof. It is well known that, for
anyfinitematrixgame,ifallplayersapplythesameRegretMatching
0.5 policytheempiricaldistributionofallplayers’jointactionconverges
1
tothesetofCoarseCorrelatedEquilibria(CCE)[22].Weprovea
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Iteration Iteration stronger result of convergence to a PSNE under the assumption of
(b) (c)
submodularutilityfunctions.
Figure4. Diamondscollectiongame(a),numberofdiamondscol-
Asweformulatetheproblemofmulti-agentinformationgather-
lected(b),andD-UCBscoreforeachactionofAgent1(c).
ingasmaximizationofasubmodularfunction,theconsideredmatrix
Let’s illustrate the significant implication of the lemma through gamegeneratedbytheactiveagentsandtheircorrespondingsetsof
anexample.Consideragrid-worlddiamondscollectiongame[41], bestfeasiblepathsateachdecisionpointsatisfiesthefollowingtwo
wheretwoagentsplayinateamusingDec-MCTS.Anexploration properties:
(cid:88)
factorC p andadiscountingfactorγ forDec-MCTSarechosenas • Property1: λ nU n(x)isconcaveinx,
0.5 and 0.75 respectively. As shown in Figure 4, when simulating n∈N
detcelloC
sdnomaiD
serocS
BCU• Property2:U (x ,x )isconvexinx , simplyfollowthebestcomputedNashequilibriumtoselecttheirde-
n n −n −n
wherex := (x ,x )denotesapurejointactioninwhichagentn cisions.InAppendixD,wedemonstrateviaextensivesimulationre-
n −n
choosespathx andtheotheragentsselectx .Thecombinationof sultsthatourapproximateNash-basedapproachachievesanoverall
n −n
thetwopropertiesimpliesthatplayern’slocalutilityfunctionU (·) good efficiency compared to the optimal solution and substantially
n
isconcaveinx givenx isfixed. improvesoverthemaincompetingapproaches,bothintermsofcon-
n −n
LetxbeaCCEoftheconsideredgame,andletx¯ = E [x],we vergencespeedandglobalutilityachieved.
π
thenprovethatx¯isapurestrategyNEofthegame.Withoutlossof
generality,assumethatλ = 1, ∀n ∈ N.AsxisaCCEpoint,it
n C.2 ThedistributedandparallelexecutionofRegret
satisfies
Matching
E[U (x)]≥E(cid:2) U (x′,x )(cid:3) , (23)
n n n −n
In multi-agent systems where agent loss and unreliable communi-
for every n ∈ N and every action x′ ∈ Xˆ . Also, since x¯ ∈ Xˆ,
n n cation are expected to occur, a single point of failure is often un-
usingProperty2wehave
acceptable.Moreover,acentralizedapproachthatrequiresaglobal
E(cid:2) U (x′,x (cid:3) ≥U (cid:0) x′,E[x ](cid:1) =U (x′,x¯ ). (24) viewofthegameisoftenintractableduetotheexponentialgrowth
n n −n n n −n n n −n
ofthegame’ssizeandcomplexity.Therefore,wedeviseadistributed
Combining(23)and(24)yields approachforexecutingthecoordinationalgorithm.Inourapproach,
whenanagentexperiencesalossofcommunicationwithotherteam-
E[U n(x)]≥U n(x′ n,x¯ −n). (25) mates,theagentcanpredicttheotheragent’sbehaviorsbysimulat-
ingtheirdecisionchoicesaccordingtotheinformationreceivedpre-
Replacingx′ =x¯ andthensummingoveralln∈N
n n viously.However,whencommunicationlossoccursrepeatedlyover
(cid:88) (cid:88) (cid:88) a certain number of times, it is treated as an agent attrition situa-
E[U (x)]≥ U (x¯ ,x¯ )= U (x¯).
n n n −n n tion and the remaining agents simply form a new game in which
n∈N n∈N n∈N
thesetofplayersonlycontainstheactiveagents.Ontheonehand,
UsingProperty1implies thedistributedexecutionofRMallowstheplayerstoindependently
learnandadapttheirstrategiesbasedonlocalinformation,without
(cid:34) (cid:35)
(cid:88) (cid:104) (cid:105) (cid:88) (cid:88) (cid:16) (cid:17) theneedforacentralauthorityorsynchronouscommunication.On
E U (x) =E U (x) ≤ U E[x] .
n n n
theotherhand,theparallelexecutionofRM(theagentsexecutethe
n∈N n∈N n∈N
samealgorithmsinparallel)helpsbyexploringdifferentNEpossi-
Therefore bilitiestoavoidlocaloptimaandidentifythemostpayoff-dominant
(cid:88) (cid:104) (cid:105) (cid:88)
E U n(x) = U n(x¯). one.
n∈N n∈N
Thus,U n(x¯)=E[U n(x)]foreveryn,and(25)becomes C.3 ComputationalcomplexityofAlgorithm2
U n(x¯)≥U n(x′ n,x¯ −n). Regret Matching was proven to guarantee a convergence rate of
(cid:112)
O(1/ (T)afterT iterations[22].WediscussherehowAlgorithm
foreveryx′ ∈Xˆ .Therefore,x¯isapureNashequilibrium.Thisim-
n n 2scalesconcerningthesizeoftheproblemandthenumberofavail-
pliesthatthetimeaverageofthejointactionofallplayersconverges
ableactionsforeachagenttochoosefrom.Formatrixgames,where
toaPSNEsolution.
eachplayerhasafinitesetofactionsandthepayoffsaregivenbya
matrix,theRMalgorithmcanbeimplementedinpolynomialtime.
C FurtherAnalysisofRegretMatching Specifically,amatrixgamewithNplayersandatmostMactionsper
CoordinationAlgorithm playerhasMN actioncombinationintotal.Eachplayerhasonelo-
calutility(orpayoff)foreachactioncombinationandthusitrequires
C.1 RationalebehindNashequilibriumsolution
N×MN integernumberstorepresentallpossibleplayers’utilities.
Nash equilibrium is particularly useful in situations where agents Therefore,asthenumberofplayersandthenumberofactionsper
haveincompleteinformationaboutthestrategiesofotheragents,i.e., playerincrease,thesizeofthegametreegrowsexponentially,mak-
coordinationwhenagentscannotmaintainperfectcommunication. ingitintractabletocomputetheentiretreeinmemoryortimeusing
Insuchcooperativesituationswithlimitedcommunication,NEpro- acentralizedapproach.
videsaneffectivewaytofindasetofstrategiesforeachagentthatis Incontrast,usingourproposeddistributedapproachaspresented
robusttouncertaintyandincompleteinformation.InaNashsolution, inAlgorithm2,thecomputationalcomplexityrequiredforthecom-
noagentcanimproveitsoutcomebyunilaterallydeviatingfromthe putationofanNEsolutionisreducedsignificantly.Inparticular,at
NE strategy. Thus, NE is a stable outcome for the agents involved eachlearningtimestep,eachRMplayerlearnsonlyitsutilityvector
intheplanningprocesswherealltheagentssharethesamecommon (ofsizeatmostequaltoM)forupdatingitsactiondecisionpolicyin
objective.Yet,aremainingchallengeisthatthereoftenexistsmul- thenexttimestep.Asaresult,thetotalamountofqueriesovertime
tipleNashequilibriaandthushowtomakesurethecombinationof required by each player to run the algorithm will scale according
these individual Nash-based strategies, which each agent indepen- to O(M ×T), where T is the number of iterations until conver-
dentlycomputes,definesanoptimalequilibrium.Theselectionofa gence. Note that in the implementation of our proposed approach,
goodNashequilibriumamongthemanyoptions,knownasanequi- eachagenthastodothesamecalculationsforothersimulatedplay-
libriumselectionproblem,remainsanopenquestionforfurtherin- ers.Thus,thetotalcomputationalcomplexityofoursolutionwould
vestigation.Withinthescopeofthiswork,toaddressthisdilemma, scale as ∼ O(N ×M ×T), which is linearly proportional to the
weproposethattheagentssynchronizetheirreachedNashpointsto numberofagents,thenumberofagent’sactions,andthenumberof
identify the most payoff-dominant Nash solution. The agents then iterations until convergence. Consequently, our proposed approach100 80 80 80
Dec-MCTS Dec-MCTS Dec-MCTS
Dec-MCTS-Global 70 Dec-MCTS-Global 70 Dec-MCTS-Global 70
80 Greedy-MCTS Dec-MCTS-Reset Dec-MCTS-Reset
A-MCTS 60 Greedy-MCTS 60 Greedy-MCTS 60
A-MCTS A-MCTS
60 50 50 50
40 40 40
40 30 30 30
Dec-MCTS
20 20 20 Dec-MCTS-Global
20 Dec-MCTS-Reset
10 10 10 Greedy-MCTS
A-MCTS
0 0 0 0
1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9
Actions Taken Actions Taken Actions Taken Actions Taken
(a) (b) (c) (d)
Figure5. EvolutionoverthemissionoftheInstantaneousRewardCoverage(IRC)intheForcedFailuresettingfordifferenttimesofattrition:noattrition(a),
attritionafter2actions(b),attritionafter4actions(c),andattritionafter6actions(d).Resultsarewith95%confidence.
Table1. Optimalityanalysisofregretmatching.
NumberofAgents NumberofComponents ActionsperComponent
2 3 4 5 6 10 11 12 13 14 7 9 11 13 15
PFO(%) 90 85 65 45 40 45 40 45 45 40 65 40 40 30 50
RNO 0.97 0.96 0.97 0.97 0.97 0.97 0.97 0.98 0.98 0.97 0.98 0.97 0.98 0.98 0.98
canconvergetoaNEsolutioninadistributedandscalableway,mak- MCTSperformingthebestinallcasesastheregretmatchingmethod
ingitmoresuitable,effective,andpracticalinreal-worldscenarios, allowingtheagentstodiscoverbetterjointpoliciesandthusprovides
wheretheplayersmayhaveaccesstodifferentandasynchronousin- betterguidancefortheexploration-exploitationofthesearchtree.It
formation. isalsointerestingtonotethatthesuperiorityofA-MCTScompared
toDec-MCTSisslightlyreduced(from15%to10%)asattritionoc-
curslater.Thisisexpectedaswhensomeagentsfailedinthefinal
D AdditionalExperimentalResuls
stage of the mission, the remaining agents would not have enough
actionbudgetlefttorecoverthelostrewards.
D.1 TimeofAttritionAnalysis
To perform a baseline evaluation of our algorithm, we consider a D.2 ACloserLookAtRegretMatchingBehavior
settingwithnoattritionandmeasurethetaskperformanceintermsof
instantaneousrewardcoveragethroughoutthemission.Asshownin Inthissection,westudytheoptimalityoftheNashequilibriumpoint
Fig.5a,underthisstaticenvironment,althoughtheIRCofA-MCTS computedbytheregretmatchingalgorithminA-MCTSinthecon-
appearedtobelessthanDec-MCTSinitially,itendedupcomparable textofthemulti-agentunderwaterdatacollectionproblem(Problem
andevenslightlyoutperformedthestate-of-the-artattheendofthe 2).Weusethefollowingtwometricstoevaluatetheperformanceof
mission.Thisshowsthatourproposedalgorithmcandiscoverpaths ouralgorithm:
thatguaranteemorelong-termrewardsandthusisalsoagoodfitfor • ProbabilityofFindingOptimalpolicy(PFO):Probabilitythatthe
multi-agentcoordinatedinformationgatheringingeneralsettings. Nashpolicyofourregretmatchingisoptimalinagivensetting.
Toevaluateouralgorithm’sadaptabilitytofailures,weconsidered
T
tt ih oe nsFo hr ac ve ed bF ea enilu tare kes ne ,tt hin alg f, oi fn thw eh aic gh ena tf ste (cr ha oss ep nec ai tfi rc ann du om mb )e br eo cf omac e- PFO= T1 (cid:88) 1 {p(t)=p∗(t)}
t=1
unavailable.Specifically,Fig.5b,c,anddshowstheinstantaneous
• RatiobetweenNashpolicyandOptimalpolicy(RNO):Ratiobe-
rewardcoveragewithattritionattheearlystage(e.g.,after2actions),
tweentheutilityoftheNashpolicyofourregretmatchingandthe
middlestage(e.g.,after4actions),andlaterstage(e.g.,after6ac-
optimalinagivensetting.
tions) of the mission respectively. As these figures show, resetting
the tree for replanning produced no significant benefits compared
U (p(t))
to those that adopted the same tree. This is because every MCTS RNO= g
U (p∗(t))
g
processstartswiththeexplorationphasewhereagentsintentionally
takerandomactionstolearntherewarddistribution.Assuch,reset- Theoptimalstrategyiscomputedusinganexhaustivesearchalgo-
ting the tree without sufficient planning would cause the produced rithm.Table1showstheresultsofthisstudywithadefaultnumber
jointpolicyfromthisperiodtobesub-optimal.Inaddition,asDec- ofagentsof6,numberofcomponentsperagentof10,andnumber
MCTS uses the marginal contribution as the utility function, it is ofactionspercomponentof9.Asexpected,theprobabilityoffind-
unabletorecognizethereductionoftheglobalrewardandhenceis ingtheoptimalstrategydecreasessignificantlywhenweincreasethe
unabletoadapttofailuresefficiently.Indeed,thegapbetweenitand number of agents. Indeed, with every added agent, the size of the
Dec-MCTS-Globalishalvedcomparedtothecasewithnofailure. gameincreasesexponentially,thuspotentiallycausingregretmatch-
However, using the global utility function alone is not enough, as ingtogetstuckatlocal-optimalpoints.Thesamebehaviorwasnot
samplingotheragents’actionsequencesintroducesalotofvariance observed when we increased the number of components per agent
intheestimationoftheglobalutility.Byassumingthatthepoliciesof orthenumberofactionspercomponent.Regardlessofthis,incases
otheragentsarefixed,bothA-MCTSandGreedy-MCTScanover- whereA-MCTSdidnotfindtheoptimalstrategy,itstillsustainably
comethisinstabilityissueandadapttoagentfailuresbetter,withA- achievedaratioof98%comparedtotheoptimal.
)%(
CRI
)%(
CRI
)%(
CRI
)%(
CRI