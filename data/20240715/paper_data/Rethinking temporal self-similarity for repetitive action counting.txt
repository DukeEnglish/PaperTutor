RETHINKINGTEMPORALSELF-SIMILARITYFORREPETITIVEACTIONCOUNTING
YananLuo*1,JinhuiYi*1,YazanAbuFarha2,MoritzWolter1,JuergenGall1,3
1UniversityofBonn 2BirzeitUniversity 3LamarrInstituteforMachineLearningandArtificialIntelligence
ABSTRACT PreviousApproaches
Counting repetitive actions in long untrimmed videos is a
challengingtaskthathasmanyapplicationssuchasrehabili-
tation. State-of-the-artmethodspredictactioncountsbyfirst
generating a temporal self-similarity matrix (TSM) from the
Temporal
PerFrame PeriodLength
sampled frames and then feeding the matrix to a predictor SampledVideoFrames Self-Similarity
Embeddings orDensityMap Matrix
network. The self-similarity matrix, however, is not an op-
OurApproach
timalinputtoanetworksinceitdiscardstoomuchinforma-
tionfromtheframe-wiseembeddings. Wethusrethinkhowa
TSM can be utilized for counting repetitive actions and pro-
poseaframeworkthatlearnsembeddingsandpredictsaction
startprobabilitiesatfulltemporalresolution. Thenumberof
repeated actions is then inferred from the action start proba-
bilities. IncontrasttocurrentapproachesthathavetheTSM PerFrame Temporal ActionStart
FullVideoFrames Self-Similarity
as an intermediate representation, we propose a novel loss Embeddings Probabilities
Matrix
based on a generated reference TSM, which enforces that Fig.1: WhilepreviousworksuseTSMsasintermediaterep-
the self-similarity of the learned frame-wise embeddings is resentation,welearnarepresentationwheretheTSMiscon-
consistent with the self-similarity of repeated actions. The sistentwiththeground-truth.
proposedframeworkachievesstate-of-the-artresultsonthree
datasets,i.e.,RepCount,UCFRep,andCountix. [5, 6] extended this paradigm and learn multiple correlation
matrices. Foralltheseapproaches,theframe-featuresarere-
Index Terms‚Äî Repetition Counting, Temporal Self-
duced to TSMs and the periodicity or a density map are es-
Similarity
timated from the TSMs. While self-similarity of features is
animportantcueforrepeatingactions,theTSMdiscardstoo
1. INTRODUCTION
muchinformationfromtheframe-wisefeaturesanditisthus
not the optimal input for a prediction network. We thus re-
Repetitive actions are ubiquitous in the real world, ranging
think the way how a temporal self-similarity matrix is used
from natural phenomena like glacial and oceanic currents to
forclass-agnosticrepetitioncountingandproposeadifferent
fundamentalbiologicalprocesseslikehumanheartbeat[1,2,
approach of utilizing self-similarity as shown in the bottom
3]. Inthispaper,weaddressthetaskofclass-agnosticrepeti-
rowofFig.1.
tiveactioncounting,whereinthegoalistopredictthenumber
Instead of reducing features to a TSM and predicting a
ofrepetitionsofanactionthatarecarriedoutinavideo.Such
density map of actions from the TSM, we predict the start
ataskhasmanypracticalapplicationssuchasguidingpeople
ofeachrepeatedactionfromtheframe-wiseembeddingsdi-
duringtheirphysicalexercises.
rectly. Inordertolearnembeddingswheretheself-similarity
In order to count repetitive actions, [4] introduced the
ishighforrepeatedactions,weintroduceanewlossthaten-
temporal self-similarity matrix (TSM) which computes sim-
forces consistency between the self-similarity of the learned
ilarities between frame-wise features. As shown in the top
frame-wise embeddings and the self-similarity of repeated
rowofFig.1,theTSMisusedasintermediaterepresentation
actions. To this end, we construct a target TSM from the
topredictframe-wiseperiodlengthandclassifyper-framebi-
ground-truth annotations and aim at a high embedding sim-
naryperiodicityjointly,whicharethenmergedforcounting.
ilarity when actions are repeated. We evaluate the approach
ThisworkhasbeensupportedbytheprojectiBehave(receivingfund- on three datasets, namely RepCountA [5], UCFRep [7], and
ingfromtheprogramme‚ÄúNetzwerke2021‚Äù,aninitiativeoftheMinistryof Countix [4], where our approach achieves state-of-the-art
CultureandScienceoftheStateofNorthrhineWestphalia),theERCCon-
results. In particular, in terms of Off-By-One Accuracy
solidatorGrantFORHUE(101044724),andtheChineseScholarshipCouncil
(202108440041).*denotesequalcontribution. (OBOA), our approach outperforms the state of the art by a
4202
luJ
21
]VC.sc[
1v13490.7042:viXra
Encoder
Encoder
Predictor
Predictor
ActionStart
Periodicitylarge margin. Overall, the contributions of this work can be 3. RACNETMODEL
summarizedasfollows:
Inthissection,weintroduceourRepetitiveActionCounting
‚Ä¢ Weproposeaframeworkforrepetitiveactioncounting network(RACnet). Wecasttheactioncountingtaskasaction
thatworksonthefulltemporalresolutionanddoesnot start prediction. During inference, the action count can be
haveaTSMbottleneck. directlyestimatedbycountingthenumberofactionstartsthat
are predicted by the network. Given a video with T frames
‚Ä¢ We introduce a novel loss based on a generated refer-
V = [v ,v ,...,v ], we want to predict per-frame action
ence TSM to enforce the self-similarity of the learned 1 2 T
startprobabilitiesA=[a ,a ,...,a ]. Fromtheactionstart
frame-wise embeddings to be consistent with the self- 1 2 T
probabilities,wetheninferthenumberofactionrepetitions.
similarityofrepeatedactions.
The proposed network consists of three modules, as
shown in Fig. 2. The Feature Encoder produces per-frame
2. RELATEDWORK actionembeddingsandtheTemporalRepetitionConstrain
Module(TRC)forcesthelearnedembeddingstohaveaself-
similaritystructurethatrespectsactionrepetitionsintheinput
Earlyactioncountingmodelswerebuiltwithhard-codedcon-
video. The Action Start Predictor (ASP) predicts for each
straintsandassumedauniformactionperiod[8]. Theirmod-
frame the probability that an action starts. In the following,
eling process typically involved a two-stage approach. The
weexplaineachmoduleindetail.
first step encompassed the segmentation and tracking of ob-
jects [9, 10, 8, 11] within the video frames, effectively iso-
lating the objects of interest. Subsequently, spectral or fre- 3.1. FeatureEncoder
quencyfeatureengineeringtechniques[12,13]wereapplied
tothesegmentedobjects[14],generatingvariousoutputs[10, Our encoder is composed of two main parts: spatial feature
15, 13] for counting. These early methods are limited to extractorandlocalfeatureaggregator.
action cycles of equal length, which rarely happens in real- Feature extractor. We use a pre-trained Video Swin Trans-
worldscenarios. former[21]toextractper-framespatialfeaturesfortheinput
To detect various periodicity, previous works [16, 4, 5] video. Foreachframev i intheinputvideo,weextractafea-
proposed to take input video frames at different time scales. ture tensor with dimension 7 √ó 7 √ó 768. To retain context
Despite the efficacy demonstrated by these approaches, they information and temporary consistency, we extract features
are constrained by the limited number of frames as input, fromfull-resolutionvideos.
renderingthemimpracticalforprocessingextendedvideose- Local feature aggregator. To enrich the features with tem-
quences. The recent approach [6] uses full-resolution as in- poral context as in [4, 5], we feed the full-resolution spatial
put, but it relies on a strided convolution of the input video. featuresoftheinputvideotoa3Dconvolutionlayerwithsize
Technically,theirnetworkdoesnotseeafull-resolutionvideo 3√ó3√ó3. Then, we apply a 2D spatial pooling to get the
for training. Multi-striding or scaling makes inference com- featureembeddingsx iforeachframeintheinput.
plicated,andthetemporalinformationfromtheoriginaldata
is lost due to sampling. In contrast to these approaches, our
3.2. TemporalRepetitionConstrainModule(TRC)
workfocusesonfullresolutionwithoutanysortofframesam-
pling to estimate the repetitive actions more accurately. The Given the per-frame embeddings [x ,...,x ] from the en-
1 T
concurrent work [17] integrates object detection and multi- codernetwork,weconstructatemporalself-similaritymatrix
pathtransformerstoregressthedensitymap. S. The matrix S is a T √óT matrix, where the element s
ij
[4] proposed the RepNet architecture which relies on a represents the similarity between frame i and frame j in the
Temporal Self-similarity Matrix (TSM) [18, 19] as its only inputvideo. Tocalculatethesimilarityscores ,wecompute
ij
intermediate layer. Following this line of research, [5] pro- the similarity function f(i,j) between frame i and frame j,
posed the TransRAC architecture which constructs multi- which is the negative of the Hamming distance between the
scale-sample inputs and regresses the density map from the correspondingframeembeddings,followedbyrow-wisemin-
intermediate correlation matrices constructed by adopting maxnormalization.
multi-head attention [20]. Based on TransRAC, [6] pro- Reference TSM. To generate the reference TSM S , we
ref
posed to compute correlation matrices from refined features assumethatframeembeddingsofoneactionmustalignwith
extracted by a temporal convolution network (TCN). How- frameembeddingsofallotherrepetitions.I.e.,theembedding
ever, using a TSM or correlation matrices as an exclusive ofthefirstframeinoneactionshouldhaveahighsimilarity
intermediate layer causes information loss and introduces a withtheembeddingofthefirstframeinallrepetitions. This
bottleneck. Different from previous approaches, we learn alsoholdsforthelastframeineachrepetition.Forframesbe-
embeddings where the temporal self-similarity is consistent tweenthestartandendofanaction,weallowmany-to-many
withthestructureoftherepetitionsintheinputvideo. alignmentastheserepetitionsmighthavedifferentdurations.SSE
VSwinFeature Loss
Extractor
Action Start Predictor (ASP)
Ground Truth [(ùë†,ùëí),‚Ä¶]
//Offline ! !
Frame-wise
tReCo
Local Feature Similarity Loss
Aggregator ùë† !"
Per Frame
Full Video Frames Embeddings ùë• ! Temporal Reference
Self-Similarity Matrix
TSM
(TSM)
Feature Encoder
TemporalRepetition Constrain Module(TRC)
Fig.2:RACnetarchitecture.Ourapproachconsistsofthreemodules:TheFeatureEncodergeneratesper-frameembeddings
fromfull-resolutionvideos. Itispre-trainedandfrozen. TheTemporalRepetitionConstrainModulegeneratesatemporal
self-similaritymatrixasanauxiliarytask, wherethetemporalRepetitionConstrain(tReCo)lossisproposedtoenforcecon-
sistency between the self-similarities of the features and the repeated actions. The SMS-TCN network in the Action Start
Predictor (ASP) generates per-frame action start probabilities and the sum-of-squared loss (SSE) is used for training. The
numberofrepetitionsintheinputvideosiscalculatedasthenumberofframesthatcorrespondtoanactionstart.
TogetthefinalreferenceTSMmatrix,wefollowthefollow-
ingsteps:
1. DefineaT√óT matrixandinitializeitwithzeros,except
forthediagonalwhichisinitializedtoone.
2. Set s to one if both i and j correspond to either the
ij
startofanactionrepetitionoritsend. Fig. 3: Examples of generated reference temporal self-
3. For each pair of action repetitions, we set all the ele- similarity matrices for three different videos. Left: The ac-
ments that lie on the line connecting locations ii‚Ä≤ and tions start later in the video. Middle: There is a long break
jj‚Ä≤toone,whereiisthestartofthefirstrepetitionand untiltheactionsarecontinued. Right: Theactionshavedif-
jisitsend,wherei‚Ä≤,j‚Ä≤correspondstothestartandend ferentdurations.
ofthesecondrepetitions. isdefinedasanobjectivefunction:
4. Smooththesimilaritymatrix.
T,T
(cid:88)
‚Ñì (S,S )= (S ‚àíS )2¬∑1 , (1)
tReCo ref ij refij SrefijÃ∏=0
Fig. 3 shows three examples of the produced reference i,j=1,1
TSM. The matrix depicts the temporal structure of the in-
whereweonlyconsiderthenon-zeroelementsinS since
ref
put video and shows where the repetitions start and end. It
we only want to enforce the self-similarity of features when
also exposes the periods where no actions are carried out in
actions are repeated. For other parts of the video, self-
the video. For example, Fig. 3 (a) shows an action with late
similarities can occur as well, e.g., when a person makes
starts,(b)revealsthatthereisalonginterruptionintheaction,
a break and stands around. We thus do not penalize self-
and(c)illustratestherepetitionswithdifferentdurations. By
similaritieswheretheactionisnotperformed.
usingsuchareferenceTSMmatrix,wesoft-constrainthenet-
work to learn features that share the self-similarity structure
3.3. ActionStartPredictor(ASP)
ofrepeatedactions.
Temporal repetition consistency loss. In order to guide The Action Start Predictor (ASP) module takes the frame-
thenetworktolearnfeatureswheretheself-similarityofthe wise embeddings [x ,...,x ] from the encoder network as
1 T
features reflects the repetitions of the action, we propose a input,andpredictstheprobabilityofeachframebeingastart
temporal Repetition Consistency loss (tReCo loss) on the frameforanactionrepetition.
generated TSM S . Specifically, the sum-of-squared error Sigmoid-Multi-StageTCN(SMS-TCN).Weadoptavariant
ref
between the generated TSM S and the reference TSM S ofthemulti-stagetemporalconvolutionalnetwork(MS-TCN)
ref
SMS-TCN[22] for the action prediction. Specifically, the activation school. TheaveragedurationofvideosinpartAis30.67sec-
function of each stage is replaced by a sigmoid function to onds,whichis4-5timeslargerthantheotherdatasets. Rep-
output the frame-wise action start probabilities [a ,...,a ]. Countistheonlydatasetthathasfine-grainedannotationsof
1 T
As the stage progresses, the predicted probabilities are re- startandendforeachactioncycle. PartBisnotreleasedyet,
finedduetoincreasedtemporalreceptivefields,whichenable soallexperimentswillbeconductedonPartA.Forcompar-
the assimilation of contextual information across numerous isons,weusethetestsubsetwhichhas152videos.
frames. In contrast to MS-TCN, only the output of the last UCFRep.Thisdatasetprovidesthenumberofactionsfor
stageisusedforthelosscalculation. eachvideo. AllthevideosaretakenfromUCF101[24]. The
Startpredictionloss. Asatargetforthismodule,wedefine average durationof thevideos is 8.15seconds. For compar-
a Gaussian around each frame that corresponds to an action isons,weusethevalidationsetwhichhas105videos.
start. The loss function for the action start prediction is the Countix. This dataset is a subset of Kinetics [25] and
sum-of-squarederror onlycontainsthesegmentsofrepeatedactionswiththecorre-
spondingcountannotations.Thetestdataoriginallyconsisted
‚Ñì
(A,AÀÜ)=(cid:88)T
(a ‚àíaÀÜ )2, (2)
of 2719 videos, but only 1692 videos are still available. We
sse i i reportalltheresultsonthe1692videos.Theaverageduration
i=1 ofthevideosis6.13seconds.
whereaÀÜ isthepredictedprobabilityofframeibeinganac-
i
tionstart,andaisthetargetvalue. 4.2. EvaluationMetrics
Forevaluation,weusethecommonmetricsusedbystate-of-
3.4. Loss the-artapproaches[4,5]:Off-ByOneAccuracy(OBOA)and
MeanAbsoluteError(MAE).
Thefinallossfunctiontotrainourmodelhastwoparts
Off-By-One Accuracy (OBOA). If the difference between
thepredictedcountandthegroundtruthislessthanorequal
‚Ñì=‚Ñì +Œª‚Ñì , (3)
sse tReCo to 1, the prediction is considered as correct, otherwise as
wrong.
where‚Ñì definedin(2)isthelossforpredictingtheaction
sse
MeanAbsoluteError(MAE).Thismetriccalculatestheab-
start probabilities, and ‚Ñì defined in (1) is the temporal
tReCo
solutedifferencebetweenthepredictedcountandtheground-
self-similarityloss. WesetŒª=1.0e‚àí5 tobalancetheimpact
truthcount,normalizedbytheground-truthcount.
ofthetwolosses.
4.3. ImplementationDetails
3.5. Inference
For feature extraction, we use Video Swin Transformer [21]
Duringinference,wefeedthevideosintotheencodernetwork
pre-trained on Kinetics400 [25]. As in previous works, it is
togettheper-frameembeddings,whicharefedintotheaction
frozenforafaircomparison.Wetrainourmodelwithalearn-
startpredictor(ASP)togettheper-framepredictedprobabil- ingrateof6.4√ó10‚àí5anduseADAMoptimizerwithabatch
ities of action start. Note that the Temporal Repetition Con-
sizeof16.Wetrainedourmodelsfor100epochsonanNvidia
straintModule(TRC)isnotusedduringinferenceandisonly
RTXA6000with48GBmemoryinlessthan9hours. Infer-
utilizedduringtraining.
encetakes6secondspervideoonaverage. Theprominence1
Togetthenumberofrepetitionsoftheactionintheinput
thresholdissetto0.2.
video,weneedtocountthenumberofframesthatcorrespond
toanactionstartinthepredictedoutput. Togettheseframes,
4.4. ComparisionwiththeState-of-the-Art
wefirstfindallthelocalpeaksinthepredictedprobabilities.
Thenweonlykeepthoseframeswithaprominence[23]value We compare the proposed approach with state-of-the-art ap-
higherthanathreshold. proaches on the RepCountA [5] dataset in Table 1. Tran-
srac[5]2andME-Rac[17]3donotcomputeMAEandOBOA
based on the count of action repetitions, but on the density
4. EXPERIMENTS
map that is generated from the ground-truth. Since a com-
parison based on density maps is not accurate and does not
4.1. Dataset
allow to compare methods that use a different temporal res-
We use three large-scale datasets to evaluate our approach: olution, we report the result of Transrac using the standard
RepCountA[5],UCFRep[7],andCountix[4].
1Please refer to supplementary material for more details:
RepCountA. It is the largest repetitive action counting
https://sigport.org/sites/default/files/docs/ICIP24RACnetsupp.pdf
dataset with two parts. Part A consists of 1041 videos from 2https://github.com/SvipRepetitionCounting/TransRAC
YouTube,andpartBcontains410videosrecordedinalocal 3https://github.com/yicheng-2019/ME-RACMethod MAE‚Üì OBOA‚Üë InferTime(s) Distance MAE‚Üì OBOA‚Üë
TransRac*[5] 0.4431 0.2913 1.1194 Euclideandistance 0.5661 0.3333
Lietal.*[6] 0.4103 0.3267 - Correlation 0.6307 0.2200
ME-Rac*[17] 0.3529 0.4018 -
Self-attention 0.7459 0.1730
RepNet[4] 0.9950 0.0134 0.4656 Hammingdistance 0.4441 0.3933
Zhangetal.[26] 0.8786 0.1554 -
TransRac[5] 0.6099 0.2763 1.1194 Table4: Impactofdifferentsimilarity/distancemeasuresfor
RACnet(ours) 0.4441 0.3933 6.1689 calculatingtheTSMontheRepCountAdataset.
Table 1: Comparison to state-of-the-art approaches on the
RepCountAdataset.*denotesadifferentevaluationprotocol. Stride MAE‚Üì OBOA‚Üë
3 0.5216 0.3467
UCFRep Countix 2 0.5420 0.3133
1 0.4441 0.3933
Method MAE‚Üì OBOA‚Üë MAE‚Üì OBOA‚Üë
Table5:ImpactofthetemporalresolutionontheRepCountA
Lietal.*[6] 0.4608 0.3333 - -
dataset.
RepNet[4] 0.9985 0.0090 0.8441 0.1600
TransRac[5] 0.6401 0.3240 0.5804 0.3782
RACnet(ours) 0.5260 0.3714 0.5278 0.3924
withrespecttobothmetrics. Thisshowsthebenefitofadding
Table 2: Generalization on the UCFRep and Countix
anadditionallosstoensurethattheself-similarityofthefea-
datasets. All models are trained on the training set of Rep-
turesisconsistentwiththestructureoftherepeatedactions.
CountA.*denotesadifferentevaluationprotocol.
Impactofthesimilaritymeasure.Tocalculatethepredicted
TSM,weusethenegativeHammingdistancebetweenpairsof
frameembeddings.InTable4,wecompareittoothervariants
protocol proposed by [4]. The results show a major differ-
such as Euclidean distance, correlation, and self-attention.
enceinMAE,butasmalldifferenceinOBOAiftheprotocol
TheHammingdistanceperformsbest.
is changed. The approaches [6, 17] are based on TransRac.
Impactoftemporalresolution. InTable5,weevaluatethe
Whencomparingtomethodsusingthesameprotocol,ourap-
impactofthetemporalresolution.Byincreasingthesampling
proachoutperformsthestateoftheartbyalargemargin. The
stride,wedecreasethetemporalresolution. Theresultsshow
concurrentwork[17]achievesalowerMAEandOBOA,but
that using the full temporal resolution, i.e., stride 1, gives a
the numbers are not comparable due to the different proto-
substantialimprovementforallmetrics.
cols. Wealsoincludetheaverageinferencetimeforavideo
Impact of full resolution. Previous methods either sub-
formethodswithavailablesourcecode.Ourmethodisslower
sampletheinputframesorfixthenumberofinputframesto
than [4, 5] since it uses the full temporal resolution. We ex-
afewframes. Wecomparetheperformanceofourapproach
pect that the concurrent work [17] is much slower than our
with other methods when the full resolution of the videos
approach since it uses two stages model, where objects are
is used in Table 6. Compared to Tables 1 and 2, using the
firstdetectedineachvideoframe.
full resolution improves the performance of RepNet but de-
We also compare the generalization performance of our
teriorates the performance of TransRac and [6]. Even if the
approachwithothermethods. Inthissetup,allthemodelsare
fulltemporalresolutionisusedbyallmethods,ourapproach
trainedonthetrainingsetofRepCountA[5]andevaluatedon
UCFRep[7]andCountix[4].AsshowninTable2,ourmodel
generalizesverywelltounseenvideosonbothdatasets.
Dataset Methods MAE‚Üì OBOA‚Üë
Lietal.*[6] 0.4366 0.3000
4.5. AblationStudies RepNet[4] 0.8283 0.2933
RepCountA
Transrac[5] 0.5064 0.1866
Impact of the tReCo loss. Table 3 shows the results of our
RACnet(ours) 0.4441 0.3933
model trained with and without the tReCo loss on the Rep-
RepNet[4] 0.6028 0.1158
CountA dataset. Using the tReCo loss improves the results
Countix Transrac[5] 0.5483 0.3712
RACnet(ours) 0.5278 0.3924
RepNet[4] 0.6654 0.2476
MAE‚Üì OBOA‚Üë
UCFRep Transrac[5] 0.5987 0.2952
w/otReColoss 0.4571 0.3667 RACnet(ours) 0.5260 0.3714
wtReColoss 0.4441 0.3933
Table 6: Impact of using full temporal resolution of the
Table3:ImpactofthetReColossontheRepCountAdataset. videosontheRepCountA,CountixandUCFRepdataset.Prediction MAE‚Üì OBOA‚Üë Repetitive action: push up Interruption: rest
periodicity 1.0510 0.1667
actionstart 0.4441 0.3933
Table7:Comparisonofpredictingactionstartandperiodicity
ontheRepCountAdataset.
outperformstheseapproachesonalldatasets.
Predicting action start vs. periodicity. Our approach pre-
dictsthestartofanaction. InTable7,wecompareittopre-
dicting the periodicity with our approach. While the start of Fig.5: Exampleofincorrectannotation. Fromtoptobottom:
an action is defined by a Gaussian with very small variance, several key frames, 1D PCA of feature embeddings, ground
theperiodicityislesspeaked.Theresultsshowthatpredicting truthactionstart,predictedactionstartprobabilities.
thestartoftheactionisbetterthantheperiodicity.
action: pommel horse
4.6. Visualization
Visualizations of TSM. Fig. 4 shows visualizations of the
temporal self-similarity matrix (TSM). Fig. 4 (a) is a video
ground truth: 25
fromRepCountAwithalongbreakbetweentheactions. Dif-
ferent from the TSM of RepNet in (b), the TSM of our ap-
proach(d)istrainedtocapturetherepetitiveactionstructure
(c).
Visualizationsofactionstartprobabilities. Fig.5showsan
examplefromRepCountA[5]. Thevideocontains4sessions
ofpush-ups. Thefirstthreesessionscontain15push-upsand prediction: 26
the last session, which was not annotated, contains 10 push-
ups. Exceptforawrongpeakatthebeginning,ourapproach
recognizeseachpush-up. Weshowthe1DPCAofper-frame
featureembeddings,theactionstartannotationsoftheground
truth, and the predicted action start probabilities of our ap-
proach,whichresultsin55countedrepetitions. The1DPCA
showstheregularchangesinactionsandfindsthebreaksbe- Fig.6: Exampleofpredictedactionstartprobabilities. Note
tween the repetitions. For comparison, RepNet predicts 17 thatthelastactionismissingintheannotationbutvisiblein
andTransRacpredicts14repetitionsforthisvideo. thepommelhorsevideo.
Example of prediction. Fig. 6 shows an example of action
startpredictionsforavideofromRepCountA.Ourapproach
successfully localizes the start of each pommel horse action 5. CONCLUSION
and predicts the correct number of action repetitions. Note
thatthelastactionismissingintheannotationbutvisiblein In this paper, we proposed a framework for repetitive action
thevideo. countinginbothshortandlongvideos. Ourframeworkcasts
the problem into action start prediction and calculates the
number of actions by counting the number of frames that
correspond to a repetition start. In contrast to previous ap-
proaches,wefeedthefull-resolutionsequencestoourmodel
and do not use the temporal similarity matrix (TSM) as an
(a) (b) (c) (d) intermediaterepresentation. Instead,weproposedatemporal
repetition constrain loss that forces the learned frame-wise
Fig.4: VisualizationsofTSMs. (a)Examplevideo. (b)TSM
embeddingstocapturetherepetitiveconsistencyoftheaction.
of RepNet. (c) Reference TSM. (d) TSM of our approach.
Theproposedlossimprovestheaccuracyofpredictingaction
Yellowindicateshighsimilarityandbluestandsforlowsim-
counts. The proposed framework achieves state-of-the-art
ilarity.
resultsonthreedatasets.6. REFERENCES [15] Ashwin Thangali and Stan Sclaroff, ‚ÄúPeriodic motion
detection and estimation via space-time sampling,‚Äù in
[1] JohnGideonandSimonStent, ‚ÄúThewaytomyheartis WACV,2005.
throughcontrastivelearning:Remotephotoplethysmog-
raphyfromunlabelledvideo,‚Äù inICCV,2021. [16] OfirLevyandLiorWolf, ‚ÄúLiverepetitioncounting,‚Äù in
ICCV,2015.
[2] MihaiFieraru,MihaiZanfir,SilviuCristianPirlea,Vlad
[17] YichengQiu,LiNiu,andFengSha,‚ÄúMultipath3d-conv
Olaru,andCristianSminchisescu, ‚ÄúAifit: Automatic3d
encoder and temporal-sequence decision for repetitive-
human-interpretable feedback models for fitness train-
action counting,‚Äù Expert Systems with Applications,
ing,‚Äù inCVPR,2021.
2024.
[3] Yunhua Zhang, Ling Shao, and Cees GM Snoek,
[18] ImranNJunejo,EmilieDexter,IvanLaptev,andPatrick
‚ÄúRepetitive activity counting by sight and sound,‚Äù in
Perez, ‚ÄúView-independentactionrecognitionfromtem-
CVPR,2021.
poralself-similarities,‚Äù PAMI,2010.
[4] Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson,
[19] ChirazBenAbdelkader, RossCutler, HarshNanda, and
Pierre Sermanet, and Andrew Zisserman, ‚ÄúCounting
Larry Davis, ‚ÄúEigengait: Motion-based recognition of
outtime:Classagnosticvideorepetitioncountinginthe
peopleusingimageself-similarity,‚Äù inAVBPA,2001.
wild,‚Äù inCVPR,2020.
[20] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
[5] HuazhangHu,SixunDong,YiqunZhao,DongzeLian, Uszkoreit,LlionJones,AidanNGomez,≈ÅukaszKaiser,
ZhengxinLi,andShenghuaGao, ‚ÄúTransrac: Encoding and Illia Polosukhin, ‚ÄúAttention is all you need,‚Äù
multi-scale temporal correlation with transformers for NeurIPS,2017.
repetitiveactioncounting,‚Äù inCVPR,2022.
[21] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan
[6] Jianing Li, Bowen Chen, Zhiyong Wang, and Honghai Wei, Zheng Zhang, Stephen Lin, and Baining Guo,
Liu, ‚ÄúFull resolution repetition counting,‚Äù in ICIRA, ‚ÄúSwintransformer: Hierarchicalvisiontransformerus-
2023. ingshiftedwindows,‚Äù inICCV,2021.
[7] Huaidong Zhang, Xuemiao Xu, Guoqiang Han, and [22] YazanAbuFarhaandJurgenGall,‚ÄúMs-tcn:Multi-stage
Shengfeng He, ‚ÄúContext-aware and scale-insensitive temporal convolutional network for action segmenta-
temporalrepetitioncounting,‚Äù inCVPR,2020. tion,‚Äù inCVPR,2019.
[23] AdamHelman,TheFinestPeaks-ProminenceandOther
[8] Michail Vlachos, Philip Yu, and Vittorio Castelli, ‚ÄúOn
MountainMeasures, TraffordPublishing,2005.
periodicitydetectionandstructuralperiodicsimilarity,‚Äù
2005.
[24] Khurram Soomro, Amir Roshan Zamir, and Mubarak
Shah, ‚ÄúUcf101: A dataset of 101 human actions
[9] ScottSatkinandMartialHebert, ‚ÄúModelingthetempo-
classes from videos in the wild,‚Äù arXiv preprint
ralextentofactions,‚Äù inECCV,2010.
arXiv:1212.0402,2012.
[10] ErikPogalin, ArnoldWMSmeulders, andAndrewHC
[25] WillKay,JoaoCarreira,KarenSimonyan,BrianZhang,
Thean, ‚ÄúVisualquasi-periodicity,‚Äù inCVPR,2008.
ChloeHillier,SudheendraVijayanarasimhan,FabioVi-
ola, TimGreen, TrevorBack, PaulNatsev, etal., ‚ÄúThe
[11] FangLiuandRosalindWPicard, ‚ÄúFindingperiodicity
kinetics human action video dataset,‚Äù arXiv preprint
inspaceandtime,‚Äù inICCV,1998.
arXiv:1705.06950,2017.
[12] Alexia Briassouli and Narendra Ahuja, ‚ÄúExtraction
[26] Huaidong Zhang, Xuemiao Xu, Guoqiang Han, and
and analysis of multiple periodic motions in video se-
Shengfeng He, ‚ÄúContext-aware and scale-insensitive
quences,‚Äù TPAMI,2007.
temporalrepetitioncounting,‚Äù inCVPR,2020.
[13] Ross Cutler and Larry S. Davis, ‚ÄúRobust real-time
periodic motion detection, analysis, and applications,‚Äù
TPAMI,2000.
[14] Costas Panagiotakis, Giorgos Karvounas, and Antonis
Argyros, ‚ÄúUnsuperviseddetectionofperiodicsegments
invideos,‚Äù inICIP,2018.