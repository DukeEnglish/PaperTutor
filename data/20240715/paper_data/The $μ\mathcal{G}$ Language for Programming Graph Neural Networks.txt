The 𝜇G LanguageforProgrammingGraphNeuralNetworks
MATTEOBELENCHIA,FLAVIOCORRADINI,MICHELAQUADRINI,andMICHELELORETI,Uni-
versityofCamerino,Italy
Graphneuralnetworksformaclassofdeeplearningarchitecturesspecificallydesignedtoworkwithgraph-structureddata.Assuch,
theysharetheinherentlimitationsandproblemsofdeeplearning,especiallyregardingtheissuesofexplainabilityandtrustworthiness.
Wepropose𝜇G,anoriginaldomain-specificlanguageforthespecificationofgraphneuralnetworksthataimstoovercometheseissues.
Thelanguage’ssyntaxisintroduced,anditsmeaningisrigorouslydefinedbyadenotationalsemantics.Anequivalentcharacterization
intheformofanoperationalsemanticsisalsoprovidedand,togetherwithatypesystem,isusedtoprovethetypesoundnessof𝜇G.
Weshowhow𝜇Gprogramscanberepresentedinamoreuser-friendlygraphicalvisualization,andprovideexamplesofitsgenerality
byshowinghowitcanbeusedtodefinesomeofthemostpopulargraphneuralnetworkmodels,ortodevelopanycustomgraph
processingapplication.
CCSConcepts:•Computingmethodologies→Neuralnetworks;Parallelprogramminglanguages;•Mathematicsofcomputing
→Graphtheory;•Theoryofcomputation→Programsemantics;Typetheory;•Softwareanditsengineering→Formalmethods;
Formallanguagedefinitions.
AdditionalKeyWordsandPhrases:GraphNeuralNetworks,Domain-SpecificLanguage,GraphDeepLearning
ACMReferenceFormat:
MatteoBelenchia,FlavioCorradini,MichelaQuadrini,andMicheleLoreti.2018.The𝜇GLanguageforProgrammingGraphNeural
Networks.J.ACM37,4,Article111(August2018),32pages.https://doi.org/XXXXXXX.XXXXXXX
1 INTRODUCTION
Deeplearningmodelsareattheforefrontofartificialintelligenceresearchtoday.Amongthem,artificialneuralnetworks
arethemostcommonlyusedclassofmodelsforawiderangeofdifferenttasks,includingnaturallanguageprocessing,
computervision,softwareengineering,andmanymore[29].Forapplicationswheretheinputscanberepresentedas
graphs,thegraphneuralnetwork[10](GNN)modelhasbeenthearchitectureofchoice,andhasachievedstate-of-the-art
performanceonmanytasks.
Despitethesepromisingadvancements,deeplearningmodels,includinggraphneuralnetworks,faceanumberof
issues.Thesesystemsaredifficulttoengineerwith,andarenotoriouslyhardertodebugandinterpretcomparedto
traditionalsoftwaresystems[23].Anotherissueisthelackofguaranteesthatthesesystemsofferregardingtheiroutputs,
whichtimeandagainhavebeenshowntobeeasilyfoolable,notonlyusingso-called“adversarialexamples”[17],
butalsomoregenerallyinunpredictableandsurprisingways[3,12].Furthermore,thesesystemsactlikeablack-box
andareopaque,inthesensethathumanusersareunabletounderstandhowtheyhavereachedtheirconclusions[4].
Authors’address:MatteoBelenchia,matteo.belenchia@unicam.it;FlavioCorradini,flavio.corradini@unicam.it;MichelaQuadrini,michela.quadrini@
unicam.it;MicheleLoreti,michele.loreti@unicam.it,UniversityofCamerino,ViaAndreaD’Accorso16,Camerino,Macerata,Italy,62032.
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenot
madeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitationonthefirstpage.Copyrightsforcomponents
ofthisworkownedbyothersthantheauthor(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,toposton
serversortoredistributetolists,requirespriorspecificpermissionand/orafee.Requestpermissionsfrompermissions@acm.org.
©2018Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
ManuscriptsubmittedtoACM
ManuscriptsubmittedtoACM 1
4202
luJ
21
]LF.sc[
1v14490.7042:viXra2 Belenchiaetal.
Finally,thereisaverystrongbiasindeeplearningresearchagainsttheusageofpriorknowledgeevenwhensuch
usageiswarranted,andeventhen,itisdifficulttofigureouthowtointegratesuchknowledgeinthesesystems[24].
Inthispaper,wetacklealltheseissuesbyproposinganewgraphneuralnetworkspecificationlanguage,called𝜇G
(pronouncedas“mee-gee”).Agraphneuralnetworkin𝜇GisbuiltupasacompositionofsimplerGNNsfollowing
theformalrulesofthelanguage.TheseGNNsarebuiltupfromthebaseterms,specifyingfunctionstobeappliedto
thesinglenodesintermsofthemselvesand/ortheirneighbors,whichcanthenbecomposedsequentially,inparallel,
selectedaccordingtoaBooleancondition,oriterated.
Thelanguage’ssyntaxisintroducedusingacontext-freegrammar,andthemeaningofeachtermisformallyand
rigorouslydefinedbyadenotationalsemantics.Lateron,wealsointroduceastructuraloperationalsemantics,andprove
thatbothsemanticsareequivalent,i.e.,theydefinethesamegraphneuralnetworkoperations.Usingtheoperational
semanticsandafterhavingdefinedatypesystemfor𝜇G,weprovethetypesoundnessofthelanguage.Thetype
soundnessof𝜇Gguaranteesthateverywell-typed𝜇GprogramproperlydefinesaGNNwiththecorrectoutputtype.
Thelanguagecanbeusedbothintextualformandingraphicalform.Thegraphicalrepresentationof𝜇Gprograms
isintroducedasamoreuser-friendlyapproachtousethelanguage,asitkeepstheflowofinformationandthetypeof
labelseasiertofollowandreasonabout.
Ourlanguageisframeworkandhardware-agnostic,as𝜇Gcouldbeimplementedindifferentways.Weoptedto
implement𝜇GinTensorFlowsothatitcanuseitsautomaticdifferentiationcapabilitiesandallowtheGNNsweprogram
tobeexecutableonCPUs,GPUs,orTPUswithoutchangingtheirimplementation.Furthermore,theseGNNscan
interoperatewithanyotherTensorFlowfeatureasiftheywereTensorFlowmodelsthemselves.
Weclaimthat𝜇Ghelpswiththeaforementionedproblemsofdeeplearning.Forthematterofexplainabilityand
interpretability,using𝜇G helpsbymakingthecomputationsperformedmoreexplicit,bytakingtheformofa𝜇G
expressionwhichhasclearlydefinitesemanticsandtypes.Indeed,theusermightevendefinefunctionsandcomputations
usingthesameterminologyofthespecificdomaininwhichtheGNNisgoingtobeusedin,makingthepurposeof
eachtermeasiertounderstand.Some,orall,partsoftheGNNcanbedefinedtobeinherentlyinterpretable,byvirtue
ofbeingdefinedexplicitlybasedontheavailabledomainknowledge.Furthermore,determiningthenodes,edges,or
labelsthatcontributedtoaspecificpredictionbecomesamenabletostaticanalysistechniques[7].
Asfortheissuesoftrustworthiness,theuseofaformallanguagelike𝜇GallowstheformalverificationofGNNs
similarlytothatofotherprogramminglanguages,e.g.,byabstractinterpretation,symbolicexecution,data-flowanalysis,
andsoon.Inparticular,weareinterestedintheproblemofformallyverifyingsafetypropertiesofGNNs,e.g.output
reachabilityproperties[18].Theverificationofanoutputreachabilitypropertyforafunction𝑓 givenaninputregion
𝑋 andanoutputregion𝑌 consistsincheckingwhetherforallinputs𝑥 ∈𝑋,𝑓(𝑥) ∈𝑌.Asfarasweknow,nosolution
forthisproblemhasbeenproposedforgraphneuralnetworksspecifically[33].Theverificationofpropertiesofthis
kindcanbeusedtoprovetherobustnessofaGNNagainstadversarialexamples,whichiscriticalwhensuchmodels
aredeployedinsafety-criticalsystems,whereguaranteesofcorrectfunctioningareparamount.
Finally,itiseasytoincludepriorknowledgewhendefiningaGNNin𝜇G.Thebasictermsofthelanguageneednot
makeuseofneuralnetworklayersasistypicallythecaseforthemorepopulargraphneuralnetworkmodels,butcanin
generaluseanykindoffunction,notnecessarilydependingontrainableparameters.Thiswayisalsopossibletodefine
aGNNwhichdoesnotrequiretrainingatall,aswedidinapreviousworkwhereweused𝜇Gformodelchecking[6].
Prior,orinnate,knowledgecanbefreelymixedwithoptimizablefunctioninordertobuildhybrid,neural-symbolic
systems[22,25,32].
ManuscriptsubmittedtoACMThe𝜇GLanguageforProgrammingGraphNeuralNetworks 3
Contributions. Ourmaincontributionsare:
• Wedefinethesyntax𝜇Glanguage,andprovideitsdenotationalandoperationalsemantics.
• Weprovetheequivalenceofthedenotationalandoperationalsemanticsof𝜇G.
• Wedefinethetypingrulesof𝜇Gandproveitstypesoundness.
• Weshowhow𝜇Gprogramscanberepresentedgraphically.
• Wedemonstratehowtouse𝜇Gtoprogramsomeofthemostpopulargraphneuralnetworkmodels.
Structureofthepaper. WesurveytherelatedworkinSection2,whileweintroducethenecessarynotationand
preliminaryinformationinSection3and4.Thenthe𝜇GlanguageisdescribedindetailinSection5byshowingits
syntaxandsemantics.Aproofofequivalenceofthedenotationalandoperationalsemanticsof𝜇G isinSection6,
followedbyaproofoftypesoundnessinSection7.Thegraphicalrepresentationof𝜇Gprogramsisintroducedin
Section8.Abriefdiscussionontheimplementationof𝜇G isinSection9.Finally,weevaluate𝜇G byshowingits
applicationinCTLmodelcheckinganditsusagetodefinesomegraphneuralnetworkmodelsinSection10,and
delineatethefuturedirectionofourworkintheconclusions.
2 RELATEDWORK
Manydomainspecificlanguages(DSL)havebeendevelopedovertheyearstoeasethedevelopmentofmachinelearning
applications[28].Farfrombeingacompletesurvey,wewilldiscusshereonlythemostrecentonesorthosethatare
mostrelevantforourwork.
DeepDSL[37]isalanguagebasedonScalathatallowsthedefinitionofneuralnetworksthroughvariableassignments.
Variablescanbeassignedcomputationalmodulessuchasconvolutionallayers,poolinglayers,activationfunctionsand
soon.Neuralnetworksaredefinedbyvariableassignmentswheretheright-handsideisthefunctioncompositionof
othercomputationalmodules.ThecodeiscompiledtoJavaandusesJCudatocommunicatewithCUDAandcuDNN.
AiDSL [13] is a language for the specification of supervised feed-forward neural networks using a model driven
engineeringapproach.ThecodeiscompiledintoEncog,amachinelearningframeworkinJava.SEMKIS-DSL[19]is
anotherlanguagedevelopedusingamodeldrivenengineeringapproach,wherethemainfocusisshiftedfromthe
specificationoftheneuralnetworkarchitecturetotherequirementsthattheinputdatasetandtheneuralnetwork’s
outputsmustsatisfy.Forthetimebeingtherearenocompilationtargetsforthelanguage,soitisnotpossibleto
automaticallyproduceaneuralnetworkfromspecification.Ontheotherhand,Diesel[11]actsatafinerlevelofdetail,
allowingthespecificationofcommonlinearalgebraandneuralnetworkcomputationsthataretypicallyimplemented
bylibrariessuchascuBLASandcuDNN.Itusesapolyhedralmodeltoscheduleoperationsandperformoptimizations,
andthesourcecodeiscompileddirectlyintoCUDAcode.Sofar,noneoftheselanguagestakeintoconsiderationthe
definitionofgraph-structuredinputsorthetypicaloperationsthatcharacterizegraphneuralnetworks.
ADSLwheregraphsarefirst-classobjectsisOptiML[31],whichsupportsthespecificationofmachinelearning
modelstoberunonheterogenoushardware(i.e.theCPUorGPU).Thelanguagecanbeusedtoimplementanymachine
learningmodelthatcanbeexpressedusingtheStatisticalQueryModel[20],andthecodecanbecompiledtoScala,
C++,orCUDAcode.GraphobjectsinOptiMLsupportoperationsexpressedintermsofverticesandedges,andgraph
operationsarespecifiedintermsofverticesandtheirneighboursinthegraph.Furthermore,OptiMLhasafixedpoint
operatorwithacustomizablethresholdvaluejustlike𝜇G.AnotherDSLthatsupportsgraphsisStreamBrain[27],which
isalanguageforthespecificationofBayesianConfidencePropagationNeuralNetworks.Thismodeltakesininputa
graphwhereeverynodeisarandomvariableandedgesrepresentthedependenciesbetweenvariables.StreamBrain
ManuscriptsubmittedtoACM4 Belenchiaetal.
hasaPythoninterfaceanditssyntaxissimilartothatofKeras,wherethemodelisseenasastackoflayers,anditcan
compiletoOpenMP,OpenCL,andCUDA.StreambrainisbasedonNumPyandsupportsCPUs,GPUsandFPGAs.
Despitethenativesupportforgraphs,neitherOptiMLnorStreamBrainfullysupportthegraphneuralnetwork
model,withthefirstbeinglimitedtostatisticalinferenceproblems,andthelattertoaspecifickindofneuralarchitecture
andgraphlabeling.Therefore,wecanconcludethat,atthetimeofwriting,nodomainspecificlanguagehasbeen
developedspecificallyforgraphneuralnetworkmodelsandtasks.
3 BACKGROUNDANDNOTATION
Graphsandtheirlabelings. AdirectedgraphisapairG=(V,E)whereVisacollectionofvertices(whichwealso
refertoasnodes)andEisacollectionoforderedpairsofvertices,callededges.Forany𝑢,𝑣 ∈V,whenever(𝑢,𝑣) ∈E
wesaythat𝑢isapredecessorof𝑣 andthat𝑣 isasuccessorof𝑢.Wealsosaythat(𝑢,𝑣)isanincomingedgefor𝑣 and
←− ←−
anoutgoingedgefor𝑢.Moreover,let𝑁 G(𝑣)denotethesetofpredecessorsof𝑣,formally𝑁 G(𝑣) = {𝑢 | (𝑢,𝑣) ∈ E},
→− →−
while𝑁 G(𝑢)denotesthesetofsuccessorsof𝑢,namely𝑁 G(𝑢)={𝑣 | (𝑢,𝑣) ∈E}.Similarly,let𝐼 G(𝑣)denotethesetof
incomingedgesfor𝑣,formally𝐼 G(𝑣)={(𝑢,𝑣) |𝑢,𝑣 ∈V},andlet𝑂 G(𝑢)denotethesetofoutgoingedgesfor𝑢,formally
𝑂 G(𝑢)={(𝑢,𝑣) |𝑢,𝑣 ∈V}.
Sometimesitisusefultoassociatenodesandedgeswithvalues.GivenagraphG = (V,E),wecanconsiderits
node-labelingandedge-labeling.Theformerisafunction𝜂 :V →𝑇
V
associatingeachnode𝑣 ∈Vwithavalueinthe
set𝑇 V.Similarly,anedge-labelingisafunction𝜉 : E →𝑇
E
thatmapsanedgetoitslabelintheset𝑇 E.Let𝑉 ⊆ V
(resp.𝐸 ⊆E),welet𝜂(𝑉)(resp.𝜉(𝐸))denotethemulti-setoflabelsassociatedtotheelementsof𝑉 (resp.𝐸)by𝜂(resp.
𝜉).Likewise,welet(𝜂,𝜉)(𝐸)denotethemulti-setoftuples(𝜂(𝑢),𝜉((𝑢,𝑣)),𝜂(𝑣))foreach(𝑢,𝑣) ∈𝐸.
Encodingofgraphsandlabels. Onewaytorepresentgraphsandtheirlabelingfunctionsonacomputerisintheform
ofamatrixofnodefeaturesX,anadjacencymatrixAandamatrixofedgefeaturesE.ThenodefeaturesmatrixX
storesthefeaturesassociatedwitheachnodeinthegraph.The𝑖-throwofthematrixcontainsavalue𝑥
𝑖
∈𝑇
V
that
representstheinformationassociatedwiththe𝑖-thvertexofthegraphinagivenordering.TheadjacencymatrixA
encodesthearchitectureofthegraph,witheachnon-zeroelement𝑎
𝑖𝑗
∈Rdenotingthepresenceofanedgefromanode
𝑖toanode𝑗.TheedgefeaturesmatrixEstoresthefeaturesassociatedwitheachedge,likethenodefeaturesmatrix.
The𝑖-throwofEisavalue𝑒
𝑖
∈𝑇
E
thatrepresentstheinformationassociatedwiththe𝑖-thedgeintherow-major(or
anyother)orderingoftheedgesinA.
GraphNeuralNetworks. Agraphneuralnetworkisadeeplearningmodelthatoperatesongraph-structureddata.
IntroducedbyScarsellietal.[30],itemergedtoovercomethelimitationsofgraphkernelmethods[26].GNNsgeneralize
manyotherclassesofdeeplearningarchitectures,asotherdeeplearningmodelscanbeseenasaparticularcase
ofgraphneuralnetworks[8].ConvolutionalNeuralNetworks(CNNs),forexample,canbeseenasagraphneural
networkswhereinputscanonlybe1-dimensionalsequences(suchastexts)or2-dimensionalgrids(suchasimages),
whicharebothparticularinstancesofgraphs.Graphneuralnetworks,moregenerally,canlearnonanynon-Euclidean
structurerepresentableasgraphs,whichcontrarytogridssuchasimages,canhavedifferingshapes,numberofnodes
andconnectivityproperties.
Graphneuralnetworkscanbeusedformanytasks,whichcanberoughlycategorizedaccordingtothesubjectof
prediction:nodes,edges,orentiregraphs.Aswithothermachinelearningmodels,therearetwomainkindsoftasks,
namelyclassificationandregression,wherethegoalistopredicttheclassorsomenumericalquantityassociatedwith
thenodesinthegraph,edges,ortheentiregraph.
ManuscriptsubmittedtoACMThe𝜇GLanguageforProgrammingGraphNeuralNetworks 5
Overtheyears,manytypesofgraphneuralnetworkvariantshavebeendeveloped[38].Amongthese,wemention
GraphConvolutionalNetworks[21],GraphIsomorphismNetworks[36],andGraphAttentionNetworks[34].Graph
neuralnetworkshavebeenappliedtothemostdisparatedomains[38],spanningcomputervision,naturallanguage
processing,particlephysics,chemistry,combinatorialoptimization,recommendersystems,trafficforecastingandgraph
mining.
Themostgeneralformofgraphneuralnetwork,whichsubsumesmostoftheGNNvariantsthathavebeendeveloped
overtheyears[8],isthemessagepassingneuralnetwork[5,14](MPNN).AtypicalMPNNcomprisesafixednumber
ofconvolutionallayersstackedinsequence,eachofthemupdatingthelabelsofthenodes.Aconvolutionoperation
computesthenewnodelabels𝑥′fromthecurrentnodelabels𝑥 accordingtothefollowingequation:
𝑖 𝑖
(cid:16) (cid:17)
𝑥 𝑖′ =𝜓 𝑥 𝑖,𝜎 𝑗∈𝑁 G(𝑖)𝜑(𝑥 𝑗,𝑒 𝑗𝑖,𝑥 𝑖) (1)
where𝑁 (𝑖)isthesetofneighborsofnode𝑖,𝜎isapermutation-invariantfunctionand𝜓,𝜑are(usually)optimizable
G
functions.Thefunction𝜑generatesthemessagesentfromnode𝑗tonode𝑖,whichinthesimplestcasemightsimplybe
𝑥 .Thefunction𝜎aggregatesthemultisetofmessagesobtainedby𝜑inawaythatisindependentoftheordertheyare
𝑗
evaluatedandisusuallyanon-learnablefunctionsuchasthesum,productormean,but,moregenerally,itcanalsobe
definedasaneuralnetwork.Thefunction𝜓 updatesthelabelofnode𝑖byconsideringboththecurrentlabel𝑥 andthe
𝑖
valuecomputedby𝜎.
GraphneuralnetworkscomposedbylayersdescribedbyEquation1arepermutation-equivariantfunctionsthattake
ininputagraphG,anedge-labeling𝜉,andanode-labeling𝜂toreturnanewnode-labeling𝜂′.Thischaracterization
leavesoutsometypesofgraphneuralnetworks,namely,thegraphneuralnetworksthatusepoolinglayers(which
thereforechangethearchitectureofthegraphbyremovingelementsfromVandE)andthegraphneuralnetworks
thatlearnedgefeaturesratherthannodefeatures(whichthereforeproduceanewedge-labeling𝜉′instead).Atany
rate,thestandardgraphneuralnetworkmodelwehavejustdescribedisgeneralenoughtoencompassthesetwo
specialcasesaswell.Agraphneuralnetworkthatusespoolinglayerscanberepresentedbyallowingthenode-labeling
functiontolabeleachnodewithanadditionalBooleanvaluethatspecifieswhetherthenodehasbeendeletedornot
andbyrulingthatanedgeisvalidifandonlyifboththesourceanddestinationnodeshavenotbeendeleted.Agraph
neuralnetworkthatlearnsedgefeaturescaninsteadbemodeledbyreifyingedgesintonodesandbyhavingthegraph’s
node-labelingfunctionlabeleachnodewithanadditionalBooleanvaluethatspecifieswhetherthenoderepresentsan
edgeoranodeoftheoriginalgraph.
4 THEDOMAINOFGRAPHNEURALNETWORKS
Inthissection,weintroducethenecessarydefinitionsandtheoremsthatcharacterizegraphneuralnetworksinour
work.Weelaboratefurtheronthenotionofnode-labelingfunctionandformalizethegraphneuralnetworkasa
transformationbetweennode-labelingfunctions,thenmoveontoprovethatthesetofgraphneuralnetworksisa
chaincompletepartiallyorderedset.
4.1 Node-labelingfunctions
Wedenotethesetofalllabelingfunctionswithco-domain𝑇 as𝐻[𝑇],andwesaythatthissetspecifiesitstype.As
examples,labelingfunctionsoftype𝐻[B]mapverticestoBooleanvalues,whilefunctionsoftype𝐻[N𝑘]with𝑘 ∈N
mapverticesto𝑘-tuplesofnaturalnumbers.Eachtypealsoincludesabottomnode-labelingfunction⊥𝐻 =𝜆𝑣.undef
ManuscriptsubmittedtoACM6 Belenchiaetal.
thatisundefinedforeverynode.Then(𝐻[𝑇],⊑𝐻)isapartiallyorderedset,where⊥𝐻 isthebottomelementand⊑𝐻
isthepointwiseorderingrelationsuchthat,forall𝑣 ∈V
𝜂1 ⊑𝐻 𝜂2 ⇐⇒ 𝜂1(𝑣)=𝑡 =⇒ 𝜂2(𝑣)=𝑡
Wealsoconsidertheparallelcompositionofnode-labelingfunctions𝜂1and𝜂2,andwedenoteitas𝜂1|𝜂2,tospecify
thenode-labelingfunction𝜆𝑣.(𝜂1(𝑣),𝜂2(𝑣))thatmapsverticesto(possiblynested)pairsofnodelabels.Theinverse
operationisgivenbytheprojectionfunctions𝜋 𝐿,𝜋
𝑅
:𝐻[𝑇 ×𝑇] →𝐻[𝑇]suchthat
𝜋 𝐿(𝜂1|𝜂2)=𝜂1
𝜋 𝑅(𝜂1|𝜂2)=𝜂2
Wecall𝜋 𝐿theleftprojectionand𝜋 𝑅therightprojection.Adequatecompositionofthesetwoprojectionfunctionscanbe
usedtoobtainanynestednode-labelingfunction.
4.2 Graphneuralnetworks
Given a graph G together with its edge-labeling function 𝜉, we define a graph neural network to be a function
𝜙
G,𝜉
:𝐻[𝑇1] →𝐻[𝑇2]thatmapsanode-labelingfunctiontoanothernode-labelingfunction.Wedenotethesetofsuch
graphneuralnetworksasΦ G,𝜉[𝑇1,𝑇2]or,moresuccinctly,asΦ G,𝜉.ThesubscriptG,𝜉indicatesthateachgraphneural
networkisparametrizedbyagraphandanedge-labelingfunction.Likewiseforthenode-labelingfunctions,thesetof
graphneuralnetworksΦ G,𝜉[𝑇1,𝑇2]isapartiallyorderedsetwithapoint-wiseorderingrelation⊑Φ
G,𝜉[𝑇1,𝑇2]
suchthat,
forall𝜂 ∈𝐻[𝑇1]
𝜙1 ⊑Φ G,𝜉[𝑇1,𝑇2] 𝜙2 ⇐⇒ 𝜙1(𝜂)=𝜂′ =⇒ 𝜙2(𝜂)=𝜂′
Whenthetypesareclearfromthecontextornotrevelant,wedropthesubscriptandsimplywrite⊑Φ .
G,𝜉
Next,wedefinetheunderlyingrelationofaGNNasthesetofinput-outputtupleswhichcharacterizetheGNN.
Buildingonthisconcept,wealsospecifytheirsequentialandparallelcomposition.
Definition4.1. Givenagraphneuralnetwork𝜙 :Φ G,𝜉[𝑇1,𝑇2]wedefineitsunderlyingrelation,denotedby𝑟𝑒𝑙(𝜙),as
𝑟𝑒𝑙(𝜙)={(𝜂1,𝜂2) ∈𝐻[𝑇1]×𝐻[𝑇2] |𝜙(𝜂1)=𝜂2}
Thesequentialcomposition,orsimplycomposition,ofrelations𝐴:𝐻[𝑇1]×𝐻[𝑇2]and𝐵:𝐻[𝑇2]×𝐻[𝑇3],denotedby
𝐴∗𝐵,isdefinedas
𝐴∗𝐵={(𝜂1,𝜂3) |∃𝜂2 ∈𝐻[𝑇2] :(𝜂1,𝜂2) ∈𝐴∧(𝜂2,𝜂3) ∈𝐵}
Theparallelcomposition,orconcatenation,ofrelations𝐴:𝐻[𝑇1]×𝐻[𝑇2]and𝐵:𝐻[𝑇1]×𝐻[𝑇3],denotedby𝐴⌢𝐵,is
definedas
𝐴⌢𝐵={(𝜂1,(𝜂2,𝜂3)) | (𝜂1,𝜂2) ∈𝐴∧(𝜂1,𝜂3) ∈𝐵}
ThefollowinglemmadefinestherelationshipbetweentherelationsandtheorderingofGNNs.
Lemma4.2. GiventwoGNNs𝜙1,𝜙2wehave
𝜙1 ⊑Φ 𝜙2 ⇐⇒ 𝑟𝑒𝑙(𝜙1) ⊆𝑟𝑒𝑙(𝜙2)
G,𝜉
Proof. First we show that𝜙1 ⊑Φ 𝜙2 =⇒ 𝑟𝑒𝑙(𝜙1) ⊆ 𝑟𝑒𝑙(𝜙2). Since𝜙1(𝜂) = 𝜂′ =⇒ 𝜙2(𝜂) = 𝜂′, any
G,𝜉
(𝜂,𝜂′) ∈𝑟𝑒𝑙(𝜙1)isalsoamemberof𝑟𝑒𝑙(𝜙2).
ManuscriptsubmittedtoACMThe𝜇GLanguageforProgrammingGraphNeuralNetworks 7
Fromtheotherdirection,supposethat𝑟𝑒𝑙(𝜙1) ⊆𝑟𝑒𝑙(𝜙2).Since(𝜂,𝜂′) ∈𝑟𝑒𝑙(𝜙1) =⇒ (𝜂,𝜂′) ∈𝑟𝑒𝑙(𝜙2),wehave
that∀(𝜂,𝜂′) ∈𝑟𝑒𝑙(𝜙1),𝜙1(𝜂)=𝜂′ =⇒ 𝜙2(𝜂)=𝜂′. □
Aswasthecaseforthenode-labelingfunctions,weconsidertheparallelcompositionofgraphneuralnetworks
𝜙1:Φ G,𝜉[𝑇,𝑇1]and𝜙2:Φ G,𝜉[𝑇,𝑇2],andwedenoteitas𝜙1×𝜙2,todefinethegraphneuralnetwork
𝜆𝑒.𝜆𝑣.(𝜙1(𝑒)(𝑣),𝜙2(𝑒)(𝑣))
withtypeΦ G,𝜉[𝑇,𝑇1×𝑇2]thatconcatenatesthenode-labelingsgeneratedby𝜙1and𝜙2.Thentherelationshipbetween
theparallelcompositionoflabelingfunctionsandtheparallelcompositionofgraphneuralnetworksishighlightedby
theequation:
𝜙1×𝜙2(𝜂)=𝜙1(𝜂)|𝜙2(𝜂)
Next,weprovethatthesetofgraphneuralnetworksofagiventypeΦ G,𝜉[𝑇1,𝑇2]withtheorderingrelation⊑Φ
G,𝜉
is
achaincompletepartiallyorderedset(ccpo).
Theorem4.3. Thesetofgraphneuralnetworksisachaincompletepartiallyorderedset (Φ G,𝜉,⊑Φ G,𝜉),withleast
element⊥Φ :𝐻[𝑇1] →𝐻[𝑇2]definedby
G,𝜉
⊥Φ (𝜂)=undef
G,𝜉
Proof. Firstweshowthat⊑Φ fulfilstherequirementstoapartialorder.Wehavethat𝜙 ⊑Φ 𝜙because𝜙(𝜂)=𝜂′
G,𝜉 G,𝜉
implies𝜙(𝜂)=𝜂′,thereforewesatisfythereflexivityrequirement.Thentoprovetransitivity,weassumethat𝜙1 ⊑Φ 𝜙2
G,𝜉
and𝜙2 ⊑Φ 𝜙3andshowthat𝜙1 ⊑Φ 𝜙3.If𝜙1(𝜂)=𝜂′,wegetthat𝜙2(𝜂)=𝜂′from𝜙1 ⊑Φ 𝜙2.Thenwealsohave
G,𝜉 G,𝜉 G,𝜉
𝜙3(𝜂) =𝜂′ from𝜙2 ⊑Φ 𝜙3.Lastly,toprovethattheorderingisanti-symmetric,weassumethat𝜙1 ⊑Φ 𝜙2and
G,𝜉 G,𝜉
𝜙2 ⊑Φ 𝜙1andweneedtoshowthat𝜙1=𝜙2.If𝜙1(𝜂)=𝜂′,thenby𝜙1 ⊑Φ 𝜙2wehavethat𝜙2(𝜂)=𝜂′.Likewiseif
G,𝜉 G,𝜉
𝜙1(𝜂)isundefined,then𝜙2(𝜂)mustbeundefinedaswell,otherwise𝜙2(𝜂)=𝜂′and𝜙2 ⊑Φ 𝜙1areincontradiction.
G,𝜉
Therefore𝜙1and𝜙2areequalonany𝜂.
Toseethat (Φ G,𝜉,⊑Φ G,𝜉) isaccpo,weneedtoshowthatforallchains𝑌,theleastupperbound(cid:195)𝑌 exists.Let
𝑟𝑒𝑙((cid:195)𝑌)=(cid:208){𝑟𝑒𝑙(𝜙) |𝜙 ∈𝑌}.Wefirstshowthat(cid:208){𝑟𝑒𝑙(𝜙) |𝜙 ∈𝑌}indeedspecifiesagraphneuralnetwork.Thatis,
whenever(𝜂,𝜂′)and(𝜂,𝜂′′)aremembersof𝑋 =(cid:208){𝑟𝑒𝑙(𝜙) |𝜙 ∈𝑌},then𝜂′ =𝜂′′.If(𝜂,𝜂′) ∈𝑋,thentheremustbe
a𝜙 ∈𝑌 suchthat𝜙(𝜂) =𝜂′,andsimilarlyfor(𝜂,𝜂′′) ∈𝑋,theremustbea𝜙′ ∈𝑌 suchthat𝜙′(𝜂) =𝜂′′.Since𝑌 isa
chain,theneither𝜙 ⊑Φ 𝜙′or𝜙′ ⊑Φ 𝜙.Inanycase,thismeansthat𝜙(𝜂)=𝜙′(𝜂)and𝜂′ =𝜂′′.Nextweshowthat
G,𝜉 G,𝜉
(cid:195)𝑌 aswedefineditisanupperboundof𝑌.Let𝜙beamemberof𝑌.Clearly,𝜙 ⊑Φ (cid:195)𝑌,because𝑟𝑒𝑙(𝜙) ⊆𝑟𝑒𝑙((cid:195)𝑌).
G,𝜉
Lastly,weprovethat(cid:195)𝑌 istheleastupperboundof𝑌.Let𝜙1 beanupperboundof𝑌.Bydefinition,𝜙 ⊑Φ 𝜙1
G,𝜉
forall𝜙 ∈𝑌,and𝑟𝑒𝑙(𝜙) ⊆ 𝑟𝑒𝑙(𝜙1).Thenitmustalsobethecasethat(cid:208){𝑟𝑒𝑙(𝜙) | 𝜙 ∈𝑌} ⊆ 𝑟𝑒𝑙(𝜙1),andtherefore
𝑟𝑒𝑙((cid:195)𝑌) ⊆𝑟𝑒𝑙(𝜙1).Then(cid:195)𝑌 ⊑Φ 𝜙1anditistheleastupperboundof𝑌.
G,𝜉
Thelaststepoftheproofistoshowthat⊥Φ
G,𝜉
istheleastelementofΦ G,𝜉.ItisindeedamemberofΦ
G,𝜉
and
⊥Φ ⊑Φ 𝜙 forall𝜙,since⊥Φ (𝜂)=𝜂′implies(vacuously)that𝜙(𝜂)=𝜂′. □
G,𝜉 G,𝜉 G,𝜉
5 THE𝜇GLANGUAGEFORGRAPHNEURALNETWORKS
Inthissection,weintroducethesyntaxof𝜇Gasaprogramminglanguageforthedefinitionofgraphneuralnetworks.
Afterspecifyingitssyntax(Definition5.1),weshowitsdenotationalsemantics(Definition5.2),andstructuraloperational
semantics(Definition5.8).
ManuscriptsubmittedtoACM8 Belenchiaetal.
Definition5.1(Syntaxof𝜇G). GivenasetSoffunctionsymbols,wedefineanalgebraofgraphneuralnetworks
withthefollowingabstractsyntax:
N ::=𝜄 |𝜓 |◁𝜑 𝜎 |▷𝜑 𝜎 |N1;N2 |N1||N2 |N1⊕N2 |N★
with𝜑,𝜎,𝜓 ∈S.Theoperatorprecedencerulesgivenby★>; > || >⊕andparenthesesareintroducedtothesyntax
tomakethemeaningofexpressionsunambiguous.
GivenagraphGandanedge-labeling𝜉,themeaningofa𝜇Gexpressionisagraphneuralnetwork,afunction
betweennode-labelingfunctions.Theterm𝜄 representstheapplicationoftheidentity GNNthatleavesthenode
labelsunaltered.Anotherofthebasic𝜇Gtermsisthefunctionapplication𝜓.ThisrepresentstheGNNthatappliesthe
𝜑 𝜑
functionreferencedby𝜓.Moreover,thepre-imageterm◁
𝜎
andthepost-imageterm▷
𝜎
defineaGNNthatcomputesthe
labelingofanodeintermsofthelabelsofitspredecessorsandsuccessors,respectively.TwoGNNscanbecomposedby
sequentialcompositionN1;N2andparallelcompositionN1||N2.ThechoiceoperatorN1⊕N2allowstorundifferent
GNNsaccordingtothevaluesofanode-labelingfunction.Finally,thestaroperatorN★isusedtoprogramrecursive
behavior.
5.1 Denotationalsemantics
Havingdefinedthesyntax,wearenowreadytointroducethedenotationalsemanticsof𝜇G.
Definition5.2. (Denotationalsemantics)GivenagraphGandanedge-labelingfunction𝜉,wedefinethesemantic
interpretationfunctionS𝑑𝑠[[·]]G,𝜉 :N →Φ
G,𝜉
on𝜇GformulasN byinductioninthefollowingway:
S𝑑𝑠[[𝜄]]G,𝜉 =𝑖𝑑
S𝑑𝑠[[𝜓]]G,𝜉 (𝜂)=𝜆𝑣.𝑓 𝜓(𝜂(V),𝜂(𝑣))
S𝑑𝑠[[◁𝜑 𝜎]]G,𝜉 (𝜂)=𝜆𝑣.𝑓 𝜎(𝑓 𝜑((𝜂,𝜉)(𝐼 G(𝑣))),𝜂(𝑣))
S𝑑𝑠[[▷𝜑 𝜎]]G,𝜉 (𝜂)=𝜆𝑣.𝑓 𝜎(𝑓 𝜑((𝜂,𝜉)(𝑂 G(𝑣))),𝜂(𝑣))
S𝑑𝑠[[N1;N2]]G,𝜉 =S𝑑𝑠[[N2]]G,𝜉 ◦S𝑑𝑠[[N1]]G,𝜉
S𝑑𝑠[[N1||N2]]G,𝜉 =S𝑑𝑠[[N1]]G,𝜉 ×S𝑑𝑠[[N2]]G,𝜉
S𝑑𝑠[[N1⊕N2]]G,𝜉 =𝑐𝑜𝑛𝑑(𝜋 𝐿,S𝑑𝑠[[N1]]G,𝜉 ◦𝜋 𝑅,S𝑑𝑠[[N2]]G,𝜉 ◦𝜋 𝑅)
S𝑑𝑠[[N★ ]]G,𝜉 =𝐹𝐼𝑋(𝜆𝜙.𝑐𝑜𝑛𝑑(𝜆𝑒.𝜆𝑣.S𝑑𝑠[[N]]G,𝜉 (𝑒)(𝑣)≃𝜖 𝑒(𝑣),𝑖𝑑,𝜙◦S𝑑𝑠[[N]]G,𝜉 ))
forany𝜓,𝜑,𝜎 ∈S.Thefunctions𝑐𝑜𝑛𝑑and𝐹𝐼𝑋 aredefinedas:
𝑐𝑜𝑛𝑑(𝑡,𝑓1,𝑓2)(𝜂)=𝑓1(𝜂) if𝑡(𝜂)=𝜆𝑣.True
𝑓2(𝜂) otherwise

𝐹𝐼𝑋(𝑓)=(cid:196) {𝑓𝑛 (⊥Φ ) |𝑛 ≥0}
G,𝜉
where𝑓0 =𝑖𝑑and𝑓𝑛+1 =𝑓 ◦𝑓𝑛.Foranylabeltype𝑇 andanyrealvalue𝜖 ∈R,wedefineabinarypredicate≃𝜖 such
that
𝑥 ≃𝜖 𝑦 ⇐⇒ |𝑥−𝑦| ≤𝜖
ManuscriptsubmittedtoACMThe𝜇GLanguageforProgrammingGraphNeuralNetworks 9
Additionally,weprovidethesemanticsforatermoftheformN1⊗N2whichwillbeusefullaterintheproofofthe
equivalencewiththestructuraloperationalsemantics
S𝑑𝑠[[N1⊗N2]]G,𝜉 =(S𝑑𝑠[[N1]]G,𝜉 ◦𝜋 𝐿)×(S𝑑𝑠[[N2]]G,𝜉 ◦𝜋 𝑅)
Inthefollowingparagraphs,weclarifythemeaningof𝜇Gexpressions.
Identityapplication. Theterm𝜄isevaluatedastheidentitygraphneuralnetworkthatreturnstheinputnode-labeling
functionasis.
Function application. A function symbol𝜓1,𝜓2,... ∈ S is evaluated as the graph neural network that maps a
node-labelingtoanewnode-labelingbyapplyingthecorrespondingfunction𝑓1,𝑓2,...onbothlocalandglobalnode
information.Thelocalinformationisthelabelofeachindividualnode,whiletheglobalinformationisthemultisetof
thelabelsofallthenodesinthegraph.Thegraphneuralnetworkweobtainappliesa(possiblytrainable)function𝑓 to
thesetwopiecesofinformation.Twoparticularcasesariseifthefunctionignoreseitherofthetwoinputs.If𝑓 ignores
theglobalinformation,theGNNreturnsanode-labelingfunction𝜂 ,apurelylocaltransformationofthenodelabels.
𝑙
Ontheotherhand,if𝑓 ignoresthelocalinformation,theGNNreturnsanode-labelingfunction𝜂 thatassignsto
𝑔
eachnodealabelthatsummarizestheentiregraph,emulatingwhatintheGNNliteratureisknownasaglobalpooling
operator[16].
Pre-imageandPost-Image. Thepre-image◁andthepost-image▷,togetherwithfunctionsymbols𝜑,𝜎 ∈ Sare
evaluatedasthegraphneuralnetworksS𝑑𝑠[[◁𝜑 𝜎]]G,𝜉 andS𝑑𝑠[[▷𝜑 𝜎]]G,𝜉.Inthecaseofthepre-image,foranysymbol
𝜑 ∈Sthecorrespondingfunction𝑓 generatesamessagefromtuples(𝜂(𝑢),𝜉((𝑢,𝑣)),𝜂(𝑣))foreach(𝑢,𝑣) ∈𝐼 G(𝑣).Then
foranysymbol𝜎 ∈Sthecorrespondingfunction𝑔generatesanewlabelforanode𝑣fromthemultisetofincoming
messagesfor𝑣 obtainedfrom𝑓 andthecurrentlabel𝜂(𝑣).Thefunctions𝑓 and𝑔maybetrainable.Thecaseofthe
post-imageisanalogous,withthedifferencethat𝑓 isappliedtotuples(𝜂(𝑣),𝜉((𝑣,𝑢)),𝜂(𝑢))foreach(𝑣,𝑢) ∈𝑂 (𝑣)
G
instead.
Sequentialcomposition. AnexpressionoftheformN1;N2isevaluatedasthegraphneuralnetworkresultingfrom
thefunctioncompositionofS𝑑𝑠[[N2]]G,𝜉 andS𝑑𝑠[[N1]]G,𝜉.
Parallel composition. An expression of the form N1||N2 is evaluated as the graph neural network that maps a
node-labelingfunctiontothenode-labelingfunctionobtainedfromtheparallelcompositionofS𝑑𝑠[[N1]]G,𝜉 and
S𝑑𝑠[[N2]]G,𝜉.
Choice. Thechoice operatorN1 ⊕N2 appliedto 𝜇G formulasN1,N2 isevaluatedasthegraphneuralnetwork
S𝑑𝑠[[N1]]G,𝜉 iftheleftprojectionoftheinputnode-labeling𝜂′ =𝜋 𝐿(𝜂)isanode-labelingfunctionsuchthat∀𝑣 ∈
G,𝜂′(𝑣)=True.Otherwise,itisevaluatedasthegraphneuralnetworkS𝑑𝑠[[N2]]G,𝜉.Inanycase,theselectedGNNis
givenininputtherightprojectionoftheinputnode-labelingfunction.
Fixedpoints. ThestaroperatorN★,orthefixedpointoperator,appliedtoa𝜇GformulaN isevaluatedasthegraph
neuralnetworkthatthatmapsanode-labelingfunction𝜂toanewnode-labelingfunction𝜂′thatisthefixedpointof
N computedstartingby𝜂.Inotherwords,thesequence
ManuscriptsubmittedtoACM10 Belenchiaetal.
𝜂0=𝜂
𝜂1=S𝑑𝑠[[N]]G,𝜉 (𝜂0)
𝜂2=S𝑑𝑠[[N]]G,𝜉 (𝜂1)
.
.
.
𝜂 𝑖 =S𝑑𝑠[[N]]G,𝜉 (𝜂 𝑖−1)
iscomputeduntilweobtainalabeling𝜂′suchthatS𝑑𝑠[[N]]G,𝜉 (𝜂′)=𝜂′.The𝐹𝐼𝑋 functionusedinthedefinition
requiresthattheinputfunctionalisacontinuousfunction.Inordertoprovethat,wefirsthavetoshowthatfunction
composition◦andthefunction𝑐𝑜𝑛𝑑arecontinuous.Later,inSection6,wewillalsorequirethecontinuityofparallel
composition×toprovetheequivalenceofthedenotationalandoperationalsemantics.Thefollowinglemmasproveall
theseresults.
Lemma5.3. Functioncomposition◦iscontinuousinbothitsarguments.
Proof. Weprovethat◦iscontinuousinthefirstargument.Theproofofcontinuityinthesecondargumentis
analogous.Let𝜙0beagraphneuralnetworkandlet𝐹(𝜙)=𝜙◦𝜙0.Westartbyprovingthat𝐹ismonotone.If𝜙1 ⊑Φ 𝜙2,
G,𝜉
then𝑟𝑒𝑙(𝜙1) ⊆𝑟𝑒𝑙(𝜙2),andwecanconcludethat
𝑟𝑒𝑙(𝜙0)∗𝑟𝑒𝑙(𝜙1) ⊆𝑟𝑒𝑙(𝜙0)∗𝑟𝑒𝑙(𝜙2)
andtherefore𝐹(𝜙1) ⊑Φ 𝐹(𝜙2).
G,𝜉
Nextweprovethecontinuityof𝐹.Let𝑌 beanon-emptychain,then
(cid:196) (cid:196)
𝑟𝑒𝑙(𝐹( 𝑌))=𝑟𝑒𝑙(( 𝑌)◦𝜙0)
(cid:196)
=𝑟𝑒𝑙(𝜙0)∗𝑟𝑒𝑙( 𝑌)
(cid:216)
=𝑟𝑒𝑙(𝜙0)∗ {𝑟𝑒𝑙(𝜙) |𝜙 ∈𝑌}
(cid:216)
= {𝑟𝑒𝑙(𝜙0)∗𝑟𝑒𝑙(𝜙) |𝜙 ∈𝑌}
(cid:196)
=𝑟𝑒𝑙( {𝐹(𝜙) |𝜙 ∈𝑌})
Thus𝐹((cid:195)𝑌)=(cid:195){𝐹(𝜙) |𝜙 ∈𝑌}. □
Lemma5.4. Parallelcomposition×iscontinuousinbothitsarguments.
Proof. Weprovethat×iscontinuousinthefirstargument.Theproofofcontinuityinthesecondargumentis
analogous. Let𝜙0 be a graph neural network and let 𝐹(𝜙) = 𝜙 ×𝜙0. We start by proving that 𝐹 is monotone. If
𝜙1 ⊑Φ 𝜙2,then𝑟𝑒𝑙(𝜙1) ⊆𝑟𝑒𝑙(𝜙2),andwecanconcludethat
G,𝜉
𝑟𝑒𝑙(𝜙0)⌢𝑟𝑒𝑙(𝜙1) ⊆𝑟𝑒𝑙(𝜙0)⌢𝑟𝑒𝑙(𝜙2)
ManuscriptsubmittedtoACMThe𝜇GLanguageforProgrammingGraphNeuralNetworks 11
andtherefore𝐹(𝜙1) ⊑Φ 𝐹(𝜙2).Next,weprovethecontinuityof𝐹.Let𝑌 beanon-emptychain,then
G,𝜉
(cid:196) (cid:196)
𝑟𝑒𝑙(𝐹( 𝑌))=𝑟𝑒𝑙(( 𝑌)|𝜙0)
(cid:196)
=𝑟𝑒𝑙( 𝑌)⌢𝑟𝑒𝑙(𝜙0)
(cid:216)
= {𝑟𝑒𝑙(𝜙) |𝜙 ∈𝑌}⌢𝑟𝑒𝑙(𝜙0)
(cid:216)
= {𝑟𝑒𝑙(𝜙)⌢𝑟𝑒𝑙(𝜙0) |𝜙 ∈𝑌}
(cid:196)
=𝑟𝑒𝑙( {𝐹(𝜙) |𝜙 ∈𝑌})
Thus𝐹((cid:195)𝑌)=(cid:195){𝐹(𝜙) |𝜙 ∈𝑌}.
□
Lemma5.5. Thefunction𝑐𝑜𝑛𝑑iscontinuousinitsfirstargument.
Proof. Let𝜙0,𝜙1 beGNNs,andlet𝐹(𝜙) =𝑐𝑜𝑛𝑑(𝜙,𝜙0,𝜙1).Westartbyprovingthat𝐹 ismonotone.Wehaveto
showthatif𝜙2 ⊑Φ 𝜙3,then𝐹(𝜙2) ⊑𝐹(𝜙3).Weconsideranarbitrarynode-labelingfunction𝜂andshowthat
G,𝜉
𝐹(𝜙2)(𝜂)=𝜂′ =⇒ 𝐹(𝜙3)(𝜂)=𝜂′
If𝜙2(𝜂) =𝜆𝑣.True,then𝐹(𝜙2)(𝜂) =𝜙0(𝜂),andfrom𝜙2 ⊑Φ 𝜙3,wegetthat𝐹(𝜙3)(𝜂) =𝜙0(𝜂).Let’sconsiderthe
G,𝜉
case𝜙2(𝜂)≠𝜆𝑣.True.Then𝐹(𝜙2)(𝜂)=𝜙1(𝜂)andthesameistruefor𝐹(𝜙3)(𝜂)sotheresultisimmediate.
Toprovethecontinuityof𝐹.Let𝑌 beanon-emptychain,andwewillonlyshowthat𝐹((cid:195)𝑌) ⊑Φ (cid:195){𝐹(𝜙) |𝜙 ∈𝑌}
G,𝜉
sincefromthemonotonicityof𝐹 wejustprovedwecanconclude(cid:195){𝐹(𝜙) |𝜙 ∈𝑌}⊑Φ 𝐹((cid:195)𝑌).
G,𝜉
Thenlet’sassume𝐹((cid:195)𝑌)(𝜂)=𝜂′andwehavetodeterminea𝜙 ∈𝑌 suchthat𝐹(𝜙)(𝜂)=𝜂′.If((cid:195)𝑌)(𝜂)=𝜆𝑣.True,
wehave𝐹((cid:195)𝑌)(𝜂) =𝜙1(𝜂)anditmustbethecasethat(𝜂,𝜆𝑣.True) ∈𝑟𝑒𝑙((cid:195)𝑌).Butsince𝑟𝑒𝑙((cid:195)𝑌) = (cid:208){𝑟𝑒𝑙(𝜙) |
𝜙 ∈𝑌}theremustexista𝜙 ∈𝑌 suchthat𝜙(𝜂)=𝜆𝑣.Trueand𝐹(𝜙)(𝜂)=𝜙1(𝜂).Thecasewhere((cid:195)𝑌)(𝜂)≠𝜆𝑣.True
isanalogous. □
Lemma5.6. Thefunction𝑐𝑜𝑛𝑑iscontinuousinitssecondandthirdarguments.
Proof. Weprovethat𝑐𝑜𝑛𝑑iscontinuousinthesecondargument.Theproofofcontinuityinthethirdargumentis
analogous.Let𝜙 𝑡,𝜙0beGNNs,andlet𝐹(𝜙)=𝑐𝑜𝑛𝑑(𝜙 𝑡,𝜙,𝜙0).Westartbyprovingthat𝐹 ismonotone.
If𝜙 𝑡(𝜂)=𝜆𝑣.True,then𝐹(𝜙1)(𝜂)=𝜙1(𝜂),andfrom𝜙1 ⊑Φ 𝜙2,wegetthat𝜙1(𝜂)=𝜂′ =⇒ 𝜙2(𝜂)=𝜂′.Therefore
G,𝜉
𝐹(𝜙2)(𝜂)=𝜙2(𝜂)=𝜂′provesourresult.Let’sconsiderthecase𝜙 𝑡(𝜂)≠𝜆𝑣.True.Then𝐹(𝜙1)(𝜂)=𝜙0(𝜂)andthesame
istruefor𝐹(𝜙2)(𝜂)sotheresultisimmediate.
Toprovethecontinuityof𝐹.Let𝑌 beanon-emptychain,andwewillonlyshowthat𝐹((cid:195)𝑌) ⊑Φ (cid:195){𝐹(𝜙) |𝜙 ∈𝑌}
G,𝜉
sincefromthemonotonicityof𝐹 wejustprovedwecanconclude(cid:195){𝐹(𝜙) |𝜙 ∈𝑌}⊑Φ 𝐹((cid:195)𝑌).
G,𝜉
Thenlet’sassume𝐹((cid:195)𝑌)(𝜂) =𝜂′andwehavetodeterminea𝜙 ∈𝑌 suchthat𝐹(𝜙)(𝜂) =𝜂′.If𝜙 𝑡(𝜂) ≠𝜆𝑣.True,
wehave𝐹((cid:195)𝑌)(𝜂) =𝜙0(𝜂)andforall𝜙 ∈𝑌 wehave𝐹(𝜙)(𝜂) =𝜙0(𝜂).If𝜙 𝑡(𝜂) = 𝜆𝑣.True,wehave𝐹((cid:195)𝑌)(𝜂) =
((cid:195)𝑌)(𝜂) =𝜂′ andtherefore (𝜂,𝜂′) ∈ 𝑟𝑒𝑙((cid:195)𝑌).Butthen,since𝑟𝑒𝑙((cid:195)𝑌) = (cid:208){𝑟𝑒𝑙(𝜙) | 𝜙 ∈ 𝑌},theremustexista
𝜙 ∈𝑌 suchthat𝜙(𝜂)=𝜂′andthen𝐹(𝜙)(𝜂)=𝜂′. □
Finally,wecanshowthatthefunctionalwepassto𝐹𝐼𝑋 isindeedacontinuousfunction.
Theorem5.7. Thefunctional𝐹(𝜙)=𝑐𝑜𝑛𝑑(𝜆𝑒.S𝑑𝑠[[N]]G,𝜉 (𝑒) (cid:27)𝑒,𝑖𝑑,𝜙◦S𝑑𝑠[[N]]G,𝜉 )iscontinuous.
ManuscriptsubmittedtoACM12 Belenchiaetal.
Proof. Wecansee𝐹 asthecompositionoftwofunctions
𝐹 =𝜆𝑓.𝑐𝑜𝑛𝑑(𝜆𝑒.S𝑑𝑠[[N]]G,𝜉 (𝑒) (cid:27)𝑒,𝑓,𝑖𝑑)◦𝜆𝑓.𝑓 ◦S𝑑𝑠[[N]]G,𝜉
andsincethefunctioncompositionofcontinuousfunctionsisitselfcontinuous,weconcludeitscontinuityfromthe
continuityoffunctioncomposition(Lemma5.3)and𝑐𝑜𝑛𝑑(Lemma5.6). □
5.2 Operationalsemantics
Forthestructuraloperationalsemantics,weconsidercomputationsoftheform
⟨N,𝜂⟩
whereN isa𝜇Gexpressionand𝜂isanode-labelingfunction.ThetransitionrulesareshowninTable1.Foranylabel
type𝑇 andanyrealvalue𝜖 ∈R,thefunction𝑓
≃𝜖
:(𝑇 ×𝑇)★×(𝑇 ×𝑇)→Bassociatedto𝜓
≃𝜖
inTable1isdefinedas
𝑓 ≃𝜖(𝑋,𝑥)=𝜋 𝐿(𝑥)≃𝜖 𝜋 𝑅(𝑥)
Finally,wecandefinethesemanticinterpretationfunctiononthestructuraloperationalsemantics.
Definition5.8(Structuraloperationalsemantics). GivenagraphGandanedge-labelingfunction𝜉,wedefinethe
semanticinterpretationfunctionS𝑠𝑜𝑠[[·]]G,𝜉 :N →Φ
G,𝜉
suchthat
S𝑠𝑜𝑠[[N]]G,𝜉
(𝜂)=𝜂′ if⟨N,𝜂⟩→★
G,𝜉
𝜂′
undef otherwise

TherulesPAR1andPAR2inTable1maketheevaluationof𝜇Gexpressionsnon-deterministic.Thenexttheorem
showsthatnomattertheorderofapplicationofthestructuralsemanticsrules,theobtainedGNNisthesame.
Theorem5.9(Confluence). If⟨N,𝜂⟩→
G,𝜉
⟨N1,𝜂1⟩and⟨N,𝜂⟩→
G,𝜉
⟨N2,𝜂2⟩,theneitherN1=N2and𝜂1=𝜂2or
thereexistsatermN′andalabelingfunction𝜂′suchthat⟨N1,𝜂1⟩→
G,𝜉
⟨N′,𝜂′⟩and⟨N2,𝜂2⟩→
G,𝜉
⟨N′,𝜂′⟩.
Proof. ByinductiononthestructureofN.TheonlyinterestingcaseisthatforexpressionsoftheformN1⊗N2,
whererulesPAR1 andPAR2 canbeappliedinanyorder.Supposethat ⟨N1 ⊗N2,𝜂⟩ →
G,𝜉
⟨N 1′ ⊗N2,𝜂1|𝜋 𝑅(𝜂)⟩ by
rulePAR1andthat⟨N1⊗N2,𝜂⟩ →
G,𝜉
⟨N1⊗N 2′,𝜋 𝐿(𝜂)|𝜂2⟩byrulePAR2.ThentherequiredtermisN′ =N 1′⊗N 2′
and the labeling function is𝜂′ = 𝜂1|𝜂2, because ⟨N1 ⊗ N 2′,𝜋 𝐿(𝜂)|𝜂2⟩ →
G,𝜉
⟨N 1′ ⊗ N 2′,𝜂1|𝜂2⟩ by rule PAR1 and
⟨N 1′⊗N2,𝜂1|𝜋 𝑅(𝜂)⟩→
G,𝜉
⟨N 1′⊗N 2′,𝜂1|𝜂2⟩byrulePAR2. □
5.3 Languagemacros
Wecanenrich𝜇Gwithanumberofmacrosthatsimplifiesthejobofprogrammingagraphneuralnetwork.Inthis
section,wedescribethemeanstodefinevariables,functions,andshortcutsforthedefinitionofif-then-elseandwhile
loopexpressions.SomeoftheseextensionsrequireustointroduceasetofvariablesymbolsX::=𝑋 |𝑌 |𝑍 |··· tothe
language’ssyntax.Wewillalsousefunctionsymbols𝑝 and𝑝 todenoteleftandrightprojections.
𝐿 𝑅
Variableassignments. Itisusefulsometimestoassignanentireexpressiontoasinglelabel,sothatwheneverthatlabel
occursinaprogram,thereferredexpressionissubstitutedtoit.Forthispurpose,weintroducevariableassignments
intheformof letexpressions.Aletexpressionhastheformlet X = N in N′.Theintuitivemeaningofsuch
expressionisthatalloccurrencesofthevariablesymbolXintheexpressionN′aresubstitutedwiththeexpression
ManuscriptsubmittedtoACMThe𝜇GLanguageforProgrammingGraphNeuralNetworks 13
[ID] ⟨𝜄,𝜂⟩→ 𝜂
G,𝜉
[APPLY] ⟨𝜓,𝜂⟩→
G,𝜉
⟨𝜄,𝜆𝑣.𝑓 𝜓([𝜂(𝑢) |𝑢 ∈V],𝜂(𝑣))⟩
𝜑 →−
[PREIMG] ⟨◁ 𝜎,𝜂⟩→
G,𝜉
⟨𝜄,𝜆𝑣.𝑓 𝜎([𝑓 𝜑(𝜂(𝑢),𝜉((𝑣,𝑢)),𝜂(𝑣)) |𝑢 ∈ 𝑁(𝑣)],𝜂(𝑣))⟩
𝜑 ←−
[POSTIMG] ⟨▷ 𝜎,𝜂⟩→
G,𝜉
⟨𝜄,𝜆𝑣.𝑓 𝜎([𝑓 𝜑(𝜂(𝑢),𝜉((𝑢,𝑣)),𝜂(𝑣)) |𝑢 ∈ 𝑁(𝑣)],𝜂(𝑣))⟩
⟨N1,𝜂⟩→
G,𝜉
⟨N 1′,𝜂′⟩
[SEQ1]
⟨N1;N2,𝜂⟩→
G,𝜉
⟨N 1′;N2,𝜂′⟩
[SEQ2] ⟨𝜄;N2,𝜂⟩→
G,𝜉
⟨N2,𝜂⟩
[SPLIT] ⟨N1||N2,𝜂⟩→
G,𝜉
⟨N1⊗N2,𝜂|𝜂⟩
⟨N1,𝜋 𝐿(𝜂)⟩→
G,𝜉
⟨N 1′,𝜂′⟩
[PAR1]
⟨N1⊗N2,𝜂⟩→
G,𝜉
⟨N 1′⊗N2,𝜂′|𝜋 𝑅(𝜂)⟩
⟨N2,𝜋 𝑅(𝜂)⟩→
G,𝜉
⟨N 2′,𝜂′⟩
[PAR2]
⟨N1⊗N2,𝜂⟩→
G,𝜉
⟨N1⊗N 2′,𝜋 𝐿(𝜂)|𝜂′⟩
[MERGE] ⟨𝜄⊗𝜄,𝜂⟩→ ⟨𝜄,𝜂⟩
G,𝜉
[CHOICE1] ⟨N1⊕N2,𝜂⟩→
G,𝜉
⟨N1,𝜋 𝑅(𝜂)⟩if𝜋 𝐿(𝜂)=𝜆𝑣.True
[CHOICE2] ⟨N1⊕N2,𝜂⟩→
G,𝜉
⟨N2,𝜋 𝑅(𝜂)⟩if𝜋 𝐿(𝜂)=𝜆𝑣.True
[STAR] ⟨N★,𝜂⟩→
G,𝜉
⟨((𝜄||N);𝜓 ≃𝜖||𝜄);(𝜄⊕N;N★),𝜂⟩
Table1. Structuraloperationalsemanticsof𝜇G
N.Thissubstitutionispurelysyntactical,andletexpressionsaresimplyrewrittenasN′ .Clearly,multiplelet
[N/X]
expressionscanbechained,sothatwecanwritelet X1 = N1,X2 = N2,...,X𝑘 = N𝑘 in N asashorthandfor
let X1=N1 in (let X2=N2 in (...(let X𝑘 =N𝑘 in N)...)).
Functiondefinitions. Similarlytovariableassignments,wealsoconsiderthedefinitionoffunctions.Adefexpression
hastheformdef X(X1,...X𝑘){N} in N′.ItsintuitivemeaningisthatwheneverthevariablesymbolXfollowedby
aparenthesizedlistofvalues(N1,...,N𝑘)(whichfromnowonwerefertoasacallexpression)occursinN′theentire
callexpressionissubstitutedwiththeexpressionN inwhicheachvariablesymbolX1,...,X𝑘 hasbeensubstituted
withthecorrespondingexpressionN1,...,N𝑘.Asinthecaseofvariables,thissubstitutionispurelysyntacticalis
rewrittenas
N′
[X(N1,...,N𝑘)/let X1=N1,...,X𝑘=N𝑘 in N]
ManuscriptsubmittedtoACM14 Belenchiaetal.
If-then-elseselection. Atypicalprogramminglanguageconstructistheif-then-elseselectionoperatorthatevaluatesa
Booleanconditionandexecutesoneoutoftwobranchesaccordingly.In𝜇Gthisoperatorcanbeimplementedasa
macrousingthechoiceoperator.Weconsideragraphneuralnetwork𝜙1:Φ G,𝜉[𝑇1,B]denotedbyanexpressionN1to
provideaBooleannode-labelingfunction,andthenexecutethegraphneuralnetworkdenotedbyN2iftheBoolean
labelingisTrueforeverynode,otherwisetheGNNdenotedbyN3isexecutedinstead.Thenwecanintroducetheterm
if N1 then N2 else N3asashorthandfor(N1||𝜄);(N2⊕N3).
Fixpointswithvariables. Oftentimeswemightwanttoincludeconstanttermsinourfixpointcomputations,thatis,
includeGNNswhoseoutputsdonotdependonthecurrentiteratesolution,butonlyontheinitialnodelabelsasreceived
bythestaroperator.Tothisend,weintroduceamacroexpressionfix X =N,let Y1 =N1,...,Y𝑘 =N𝑘 in N′
wherethevariablesymbolXisthefixpointvariableandN istheGNNthatcomputestheinitialvalueforthefixpoint
computation.ThevariablesY𝑖 for𝑖 ∈1,...,𝑘denotethevalueswewanttopre-computeandstayconstantduringthe
computation.Toclarifythisbehaviorformally,weshowhowthismacrocanbeexpressedinthelanguage.Werestrict
theplacementofvariablesymbolsX,Ytonotappearontheright-handsideofasequentialcompositionexpression.
Then,thismacrocanbeimplementedas
((N1||N2||···||N𝑘)||N);
((𝑝1||𝑝2||···||𝑝 𝑘)||N [′ 𝑝1/Y1,...,𝑝𝑘/Y𝑘,𝑝𝑅/X])★;
𝑝
𝑅
wheretheterms𝑝 denoteacompositionofleftandrightprojectionsfunctionstoobtainthenode-labelingproducedby
𝑖
S𝑑𝑠[[N𝑖]]G,𝜉 intheparallelcompositionofN𝑖 for𝑖 ∈1,...,𝑘.Indeed,itisevenpossibletostaticallyinferthelargest
possiblesetofsub-expressionsN𝑖 thatcanbepre-computed,andthereforewecansimplywritefix X=N in N′to
havethesameeffect.Thismacrothenessentiallyimplements𝜇-calculusstylefixpointcomputations,withtheadded
flexibilityofbeingabletosetanyinitialvalueforthefixpointcomputation.
Inthecasethereisnosuchsub-expressionN𝑖,themacroreducesto
N;N★
[𝜄/X]
Fixediterationloops. Thebasicfixpointoperatoriteratesuntiltheoutputlabelingisconsideredequaltotheinput
labeling.Attimestheprogrammermightwanttoinsteaditerateforafixednumberofsteps,thatis,executethebody
ofthefixpointexpressiononlyfor𝑘 ≥ 1steps.Forthispurpose,weexpandonthepreviousmacrotointroduce
repeat X=N in N′ for 𝑘with𝑘 ∈N+thatisashorthandforthe𝜇Gexpression
(fix X=𝑧𝑒𝑟𝑜||N in
if X;𝑝 𝐿;< 𝑘 then
(X;𝑝 𝐿;𝑠𝑢𝑐𝑐)||N [′
X;𝑝𝑅/X]
else
X);𝑝
𝑅
where𝑧𝑒𝑟𝑜denotesaconstantfunctionthatmapsallnodelabelsto0,< denotesaBooleanfunctionthatmapsnodes
𝑘
toTrueiftheir(integer)labelissmallerthan𝑘,and𝑠𝑢𝑐𝑐denotesthesuccessorfunctionovernaturalnumbers.Clearly,
ManuscriptsubmittedtoACMThe𝜇GLanguageforProgrammingGraphNeuralNetworks 15
wecanalsohavearepeatmacrowithouthavingtospecifyavariableandaninitialcondition:repeat N′ for 𝑘can
berewrittenasrepeat X=𝜄 in N′ for 𝑘.
6 EQUIVALENCEOFOPERATIONALANDDENOTATIONALSEMANTICS
Boththedenotationalsemanticsandoperationalsemanticsof𝜇Gdefineamappingfromnode-labelingfunctionsto
node-labelingfunctions,i.e.,agraphneuralnetwork.Inthenexttheorem,weshowthattheydefinethesamegraph
neuralnetwork.
Theorem6.1(Eqivalenceofdenotationalandstructuraloperationalsemantics). Forany𝜇Gexpression
N,anygraphGandanyedge-labelingfunction𝜉,wehavethat
S𝑑𝑠[[N]]G,𝜉 =S𝑠𝑜𝑠[[N]]G,𝜉
Proof. SincebothfunctionsreturnGNNs,whicharemembersofapartiallyorderedsetΦ withorderingrelation
G,𝜉
⊑Φ ,itissufficienttoshowthatforany𝜇GexpressionN:
G,𝜉
•
S𝑠𝑜𝑠[[N]]G,𝜉
⊑Φ
G,𝜉
S𝑑𝑠[[N]]G,𝜉
•
S𝑑𝑠[[N]]G,𝜉
⊑Φ
G,𝜉
S𝑠𝑜𝑠[[N]]G,𝜉
Lemma6.2andLemma6.3provethatboththeseconditionsaresatisfiedbythesemanticsof𝜇G. □
Lemma6.2. ForeveryexpressionN of𝜇G,wehaveS𝑠𝑜𝑠[[N]]G,𝜉 ⊑Φ
G,𝜉
S𝑑𝑠[[N]]G,𝜉 .
Proof. WeshallprovethatforanyexpressionN andanynode-labelingfunction𝜂,𝜂′
⟨N,𝜂⟩→★
G,𝜉
𝜂′ =⇒ S𝑑𝑠[[N]]G,𝜉 (𝜂)=𝜂′ (2)
Inordertodothat,wewillshowthat
⟨N,𝜂⟩→
G,𝜉
𝜂′ =⇒ S𝑑𝑠[[N]]G,𝜉 (𝜂)=𝜂′ (3)
⟨N,𝜂⟩→
G,𝜉
⟨N′,𝜂′⟩ =⇒ S𝑑𝑠[[N]]G,𝜉 (𝜂)=S𝑑𝑠[[N′]]G,𝜉 (𝜂′) (4)
IfEquation3andEquation4hold,theproofofEquation2isastraightforwardinductiononthelength𝑘 ofthe
derivationsequence ⟨N,𝜂⟩ →𝑘 𝜂′.TheproofofEquation3andEquation4isbyinductionontheshapeofthe
G,𝜉
derivationtrees.
ThecaseID: Wehave⟨𝜄,𝜂⟩→
G,𝜉
𝜂,andsinceS𝑑𝑠[[𝜄]]G,𝜉 (𝜂)=𝜂,theresultfollows.
ThecaseAPPLY: Wehave⟨𝜓,𝜂⟩→
G,𝜉
⟨𝜄,𝜆𝑣.𝑓 𝜓([𝜂(𝑢) |𝑢 ∈V],𝜂(𝑣))⟩,andsince
S𝑑𝑠[[𝜓]]G,𝜉 (𝜂)=𝜆𝑣.𝑓 𝜓([𝜂(𝑢) |𝑢 ∈V],𝜂(𝑣))
=S𝑑𝑠[[𝜄]]G,𝜉 (𝜆𝑣.𝑓 𝜓([𝜂(𝑢) |𝑢 ∈V],𝜂(𝑣)))
theresultfollows.
ThecasePREIMG: AnalogoustocaseAPPLY.
ThecasePOSTIMG: AnalogoustocaseAPPLY.
ManuscriptsubmittedtoACM16 Belenchiaetal.
ThecaseSEQ1: Assumethat⟨N1;N2,𝜂⟩→
G,𝜉
⟨N 1′;N2,𝜂′⟩because⟨N1,𝜂⟩→
G,𝜉
⟨N 1′,𝜂′⟩.Byinductionhypoth-
esiswehavethatS𝑑𝑠[[N1]]G,𝜉 (𝜂)=S𝑑𝑠[[N 1′]]G,𝜉 (𝜂′).Thenweget
S𝑑𝑠[[N1;N2]]G,𝜉 (𝜂)=S𝑑𝑠[[N2]]G,𝜉 (S𝑑𝑠[[N1]]G,𝜉 (𝜂))
=S𝑑𝑠[[N2]]G,𝜉 (S𝑑𝑠[[N 1′]]G,𝜉 (𝜂′))
=S𝑑𝑠[[N 1′;N2]]G,𝜉 (𝜂′)
ThecaseSEQ2: Wehavethat⟨𝜄;N2,𝜂⟩→
G,𝜉
⟨N2,𝜂⟩.Thenweget
S𝑑𝑠[[𝜄;N2]]G,𝜉 (𝜂)=S𝑑𝑠[[N2]]G,𝜉 (S𝑑𝑠[[𝜄]]G,𝜉 (𝜂))
=S𝑑𝑠[[N2]]G,𝜉 (𝜂)
ThecaseSPLIT: Wehave⟨N1||N2,𝜂⟩→
G,𝜉
⟨N1⊗N2,𝜂|𝜂⟩.Thenweget
S𝑑𝑠[[N1||N2]]G,𝜉 (𝜂)=S𝑑𝑠[[N1]]G,𝜉 ×S𝑑𝑠[[N2]]G,𝜉 (𝜂)
=(S𝑑𝑠[[N1]]G,𝜉 ◦𝜋 𝐿)×(S𝑑𝑠[[N2]]G,𝜉 ◦𝜋 𝑅)(𝜂|𝜂)
=S𝑑𝑠[[N1⊗N2]]G,𝜉 (𝜂|𝜂)
ThecasePAR1: Wehave⟨N1⊗N2,𝜂⟩→
G,𝜉
⟨N 1′⊗N2,𝜂′|𝜋 𝑅(𝜂)⟩because⟨N1,𝜋 𝐿(𝜂)⟩→
G,𝜉
⟨N 1′,𝜂′⟩.Byinduction
hypothesiswehavethatS𝑑𝑠[[N1]]G,𝜉 (𝜋 𝐿(𝜂))=S𝑑𝑠[[N 1′]]G,𝜉 (𝜂′).Thenweget
S𝑑𝑠[[N1⊗N2]]G,𝜉 (𝜂)=(S𝑑𝑠[[N1]]G,𝜉 ◦𝜋 𝐿)×(S𝑑𝑠[[N2]]G,𝜉 ◦𝜋 𝑅)(𝜂)
=S𝑑𝑠[[N1]]G,𝜉 (𝜋 𝐿(𝜂))|S𝑑𝑠[[N2]]G,𝜉 (𝜋 𝑅(𝜂))
=S𝑑𝑠[[N 1′]]G,𝜉 (𝜂′)|S𝑑𝑠[[N2]]G,𝜉 (𝜋 𝑅(𝜂))
=(S𝑑𝑠[[N 1′]]G,𝜉 ◦𝜋 𝐿)×(S𝑑𝑠[[N2]]G,𝜉 ◦𝜋 𝑅)(𝜂′|𝜋 𝑅(𝜂))
=S𝑑𝑠[[N 1′⊗N2]]G,𝜉 (𝜂′|𝜋 𝑅(𝜂))
ThecasePAR2: AnalogoustocasePAR1.
ThecaseMERGE: Wehavethat⟨𝜄⊗𝜄,𝜂⟩→ ⟨𝜄,𝜂⟩.Thenweget:
G,𝜉
S𝑑𝑠[[𝜄⊗𝜄]]G,𝜉 (𝜂)=(S𝑑𝑠[[𝜄]]G,𝜉 ◦𝜋 𝐿)×(S𝑑𝑠[[𝜄]]G,𝜉 ◦𝜋 𝑅)(𝜂)
=S𝑑𝑠[[𝜄]]G,𝜉 (𝜋 𝐿(𝜂))|S𝑑𝑠[[𝜄]]G,𝜉 (𝜋 𝑅(𝜂))
=𝜋 𝐿(𝜂)|𝜋 𝑅(𝜂)
=𝜂
=S𝑑𝑠[[𝜄]]G,𝜉 (𝜂)
ThecaseCHOICE1: Wehave⟨N1⊕N2,𝜂⟩→
G,𝜉
⟨N1,𝜋 𝑅(𝜂)⟩because𝜋 𝐿(𝜂)=𝜆𝑣.True.Thenweget
S𝑑𝑠[[N1⊕N2]]G,𝜉 (𝜂)=𝑐𝑜𝑛𝑑(𝜋 𝐿,S𝑑𝑠[[N1]]G,𝜉 ◦𝜋 𝑅,S𝑑𝑠[[N2]]G,𝜉 ◦𝜋 𝑅)(𝜂)
=S𝑑𝑠[[N1]]G,𝜉 (𝜋 𝑅(𝜂))
ThecaseCHOICE2: AnalogoustoCHOICE1.
ManuscriptsubmittedtoACMThe𝜇GLanguageforProgrammingGraphNeuralNetworks 17
ThecaseSTAR: Wehave
⟨N★,𝜂⟩→
G,𝜉
⟨((𝜄||N);𝜓 ≃𝜖||𝜄);(𝜄⊕N;N★ ),𝜂⟩
Let𝐹 =𝜆𝜙.𝑐𝑜𝑛𝑑(𝜆𝑒.𝜆𝑣.S𝑑𝑠[[N]]G,𝜉 (𝑒)(𝑣)≃𝜖 𝑒(𝑣),𝑖𝑑,𝜙◦S𝑑𝑠[[N]]G,𝜉 ),thenweget
S𝑑𝑠[[N★ ]]G,𝜉 (𝜂)=𝐹𝐼𝑋(𝐹)(𝜂)
=𝐹(𝐹𝐼𝑋(𝐹))(𝜂)
=𝑐𝑜𝑛𝑑(𝜆𝑒.𝜆𝑣.S𝑑𝑠[[N]]G,𝜉 (𝑒)(𝑣)≃𝜖 𝑒(𝑣),𝑖𝑑,𝐹𝐼𝑋(𝐹)◦S𝑑𝑠[[N]]G,𝜉 )(𝜂)
=𝑐𝑜𝑛𝑑(𝜆𝑒.𝜆𝑣.S𝑑𝑠[[N]]G,𝜉 (𝑒)(𝑣)≃𝜖 𝑒(𝑣),𝑖𝑑,S𝑑𝑠[[N★ ]]G,𝜉 ◦S𝑑𝑠[[N]]G,𝜉 )(𝜂)
=𝑐𝑜𝑛𝑑(S𝑑𝑠[[(N||𝜄);𝜓 ≃𝜖]]G,𝜉,S𝑑𝑠[[𝜄]]G,𝜉,S𝑑𝑠[[N;N★ ]]G,𝜉 )
=𝑐𝑜𝑛𝑑(𝜋 𝐿,S𝑑𝑠[[𝜄]]G,𝜉 ◦𝜋 𝑅,S𝑑𝑠[[N;N★ ]]G,𝜉 ◦𝜋 𝑅)◦S𝑑𝑠[[(N||𝜄);𝜓 ≃𝜖||𝜄]]G,𝜉 (𝜂)
=S𝑑𝑠[[𝜄⊕N;N★ ]]G,𝜉 ◦S𝑑𝑠[[((𝜄||N);𝜓 ≃𝜖||𝜄)]]G,𝜉 (𝜂)
=S𝑑𝑠[[((𝜄||N);𝜓 ≃𝜖||𝜄);(𝜄⊕N;N★ )]]G,𝜉 (𝜂)
ThiscompletesourproofofLemma6.2. □
Lemma6.3. ForeveryexpressionN of𝜇G,wehaveS𝑑𝑠[[N]]G,𝜉 ⊑Φ
G,𝜉
S𝑠𝑜𝑠[[N]]G,𝜉 .
Proof. WeproceedbystructuralinductionontheexpressionN.
Thecase𝜄: Immediate,asS𝑑𝑠[[𝜄]]G,𝜉 (𝜂)=S𝑠𝑜𝑠[[𝜄]]G,𝜉 (𝜂).
Thecase𝜓: Thiscaseissimpleaswell,sinceS𝑑𝑠[[𝜓]]G,𝜉 (𝜂)=S𝑠𝑜𝑠[[𝜓]]G,𝜉 (𝜂)byapplyingruleAPPLYfollowed
byruleID.
𝜑
Thecase◁ 𝜎: Analogoustocase𝜓,butinsteadusingtherulesPREIMGandID.
𝜑
Thecase▷ 𝜎: Analogoustocase𝜓,butinsteadusingtherulesPOSTIMGandID.
ThecaseN1;N2: ByinductionhypothesiswehaveS𝑑𝑠[[N1]]G,𝜉 ⊑Φ
G,𝜉
S𝑠𝑜𝑠[[N1]]G,𝜉 andS𝑑𝑠[[N2]]G,𝜉 ⊑Φ
G,𝜉
S𝑠𝑜𝑠[[N2]]G,𝜉.Bythemonotonicityof◦inbothitsarguments(Lemma5.3)andtheinductionhypothesisweget
that
S𝑑𝑠[[N1;N2]]G,𝜉 =S𝑑𝑠[[N2]]G,𝜉 ◦S𝑑𝑠[[N1]]G,𝜉
⊑Φ
S𝑠𝑜𝑠[[N2]]G,𝜉 ◦S𝑠𝑜𝑠[[N1]]G,𝜉
G,𝜉
Tocontinuetheproof,weneedtoprovethatif ⟨N1,𝜂⟩ →𝑘
G,𝜉
𝜂′ then ⟨N1;N2,𝜂⟩ →𝑘
G,𝜉
⟨N2,𝜂′⟩.Theproof
isbyinductiononthederivationlength𝑘.For𝑘 = 1wehave ⟨N1,𝜂⟩ →
G,𝜉
𝜂′,whichmeansthatN1 = 𝜄
and𝜂 = 𝜂′. Therefore, by applying rule SEQ2 we get ⟨𝜄;N2,𝜂⟩ →
G,𝜉
⟨N2,𝜂′⟩. For the inductive step, let’s
assumethelemmaholdsfor𝑘andprovethatitholdsfor𝑘+1.Weassumethat⟨N1,𝜂⟩→𝑘 G+ ,𝜉1𝜂′,whichmeans
that⟨N1,𝜂⟩ →
G,𝜉
⟨N 1′,𝜂1⟩forsomeN 1′,𝜂1,and⟨N 1′,𝜂1⟩ →𝑘
G,𝜉
𝜂′.Thereforethefirstrulewecanapplyfor
thesequentialcompositionofN1 andN2 isSEQ1 with⟨N1,𝜂⟩ →
G,𝜉
⟨N 1′,𝜂1⟩asitspremise,andwegetthe
transition ⟨N1;N2,𝜂⟩ →
G,𝜉
⟨N 1′;N2,𝜂1⟩. Now we apply the induction hypothesis ⟨N 1′,𝜂1⟩ →𝑘
G,𝜉
𝜂′ =⇒
⟨N 1′;N2,𝜂1⟩→𝑘
G,𝜉
⟨N2,𝜂′⟩toobtainthedesiredresult,havingperformed𝑘+1stepsintotal.Usingthisresult,
ManuscriptsubmittedtoACM18 Belenchiaetal.
wecanconcludethat
S𝑠𝑜𝑠[[N2]]G,𝜉 ◦S𝑠𝑜𝑠[[N1]]G,𝜉
⊑Φ
S𝑠𝑜𝑠[[N1;N2]]G,𝜉
G,𝜉
asrequired.
ThecaseN1||N2:
ByinductionhypothesiswehaveS𝑑𝑠[[N1]]G,𝜉
⊑Φ
G,𝜉
S𝑠𝑜𝑠[[N1]]G,𝜉 andS𝑑𝑠[[N2]]G,𝜉
⊑Φ
G,𝜉
S𝑠𝑜𝑠[[N2]]G,𝜉.Bythemonotonicityof×inbothitsarguments(Lemma5.4)andtheinductionhypothesis,we
getthat
S𝑑𝑠[[N1||N2]]G,𝜉 =S𝑑𝑠[[N1]]G,𝜉 ×S𝑑𝑠[[N2]]G,𝜉
⊑Φ
S𝑠𝑜𝑠[[N1]]G,𝜉 ×S𝑠𝑜𝑠[[N2]]G,𝜉
G,𝜉
Tocontinuetheproof,wehavetoshowthatif⟨N1,𝜂1⟩→𝑘 G1
,𝜉
𝜂 1′and⟨N2,𝜂2⟩→𝑘 G2
,𝜉
𝜂 2′,then⟨N1⊗N2,𝜂1|𝜂2⟩→𝑘 G1 ,+ 𝜉𝑘2
𝜂 1′|𝜂 2′.For𝑘1 = 𝑘2 = 1,wehavethatN1 = N2 = 𝜄,𝜂1 = 𝜂 1′ and𝜂2 = 𝜂 2′.Hence,weget ⟨𝜄 ⊗𝜄,𝜂1|𝜂2⟩ →
G,𝜉
⟨𝜄,𝜂1|𝜂2⟩ →
G,𝜉
𝜂1|𝜂2 by applying rule MERGE followed by rule ID and performing 1 + 1 = 2 steps. For
the inductive step, let’s assume the implication holds for 𝑘1,𝑘2 and prove it for 𝑘1 + 1,𝑘2 + 1. We have
⟨N1,𝜂1⟩ →
G,𝜉
⟨N 1′,𝜂 1′′⟩ →𝑘 G1
,𝜉
𝜂 1′ and ⟨N2,𝜂2⟩ →
G,𝜉
⟨N 2′,𝜂 2′′⟩ →𝑘 G2
,𝜉
𝜂 2′ forsomeN 1′,N 2′,𝜂 1′′,𝜂 2′′.Thenwe
canperformtwosteps⟨N1 ⊗N2,𝜂1|𝜂2⟩ →
G,𝜉
⟨N 1′ ⊗N2,𝜂 1′′|𝜂2⟩ →
G,𝜉
⟨N 1′ ⊗N 2′,𝜂 1′′|𝜂 2′′⟩byapplyingrules
[PAR1]and[PAR2].Thenbyapplyingtheinductionhypothesiswegetourresult⟨N 1′⊗N 2′,𝜂 1′′|𝜂 2′′⟩→𝑘 G1 ,+ 𝜉𝑘2 𝜂 1′|𝜂 2′
in2+𝑘1+𝑘2=(𝑘1+1)+(𝑘2+1)steps.Usingthisresult,wecanconcludethat
S𝑠𝑜𝑠[[N2]]G,𝜉 ×S𝑠𝑜𝑠[[N1]]G,𝜉 ⊑Φ S𝑠𝑜𝑠[[N1⊗N2]]G,𝜉 ◦(𝑖𝑑×𝑖𝑑)
G,𝜉
=S𝑠𝑜𝑠[[N1||N2]]G,𝜉
asrequired.
ThecaseN1⊗N2:
ByinductionhypothesiswehaveS𝑑𝑠[[N1]]G,𝜉
⊑Φ
G,𝜉
S𝑠𝑜𝑠[[N1]]G,𝜉 andS𝑑𝑠[[N2]]G,𝜉
⊑Φ
G,𝜉
S𝑠𝑜𝑠[[N2]]G,𝜉.Bythemonotonicityof◦(Lemma5.3)andthatof×inbothitsarguments(Lemma5.4),andthe
inductionhypothesis,wegetthat
S𝑑𝑠[[N1⊗N2]]G,𝜉 =(S𝑑𝑠[[N1]]G,𝜉 ◦𝜋 𝐿)×(S𝑑𝑠[[N1]]G,𝜉 ◦𝜋 𝑅)
⊑Φ
G,𝜉
(S𝑠𝑜𝑠[[N1]]G,𝜉 ◦𝜋 𝐿)×(S𝑠𝑜𝑠[[N2]]G,𝜉 ◦𝜋 𝑅)
=S𝑠𝑜𝑠[[N1⊗N2]]G,𝜉
ThecaseN1⊕N2: WehavethatS𝑑𝑠[[N1⊕N2]]G,𝜉 =𝑐𝑜𝑛𝑑(𝜋 𝐿,S𝑑𝑠[[N1]]G,𝜉 ◦𝜋 𝑅,S𝑑𝑠[[N2]]G,𝜉 ◦𝜋 𝑅).Byinduction
hypothesis we have that S𝑑𝑠[[N1]]G,𝜉 ⊑Φ
G,𝜉
S𝑠𝑜𝑠[[N1]]G,𝜉 and S𝑑𝑠[[N2]]G,𝜉 ⊑Φ
G,𝜉
S𝑠𝑜𝑠[[N2]]G,𝜉. By the
monotonicityof◦(Lemma5.3)andthatof𝑐𝑜𝑛𝑑initssecondandthirdarguments(Lemma5.6),andtheinduction
hypothesiswehavethat
S𝑑𝑠[[N1⊕N2]]G,𝜉 =𝑐𝑜𝑛𝑑(𝜋 𝐿,S𝑑𝑠[[N1]]G,𝜉 ◦𝜋 𝑅,S𝑑𝑠[[N2]]G,𝜉 ◦𝜋 𝑅)
⊑Φ
G,𝜉
𝑐𝑜𝑛𝑑(𝜋 𝐿,S𝑠𝑜𝑠[[N1]]G,𝜉 ◦𝜋 𝑅,S𝑠𝑜𝑠[[N2]]G,𝜉 ◦𝜋 𝑅)
ManuscriptsubmittedtoACMThe𝜇GLanguageforProgrammingGraphNeuralNetworks 19
NowfromtherulesCHOICE1andCHOICE2itfollowsthat
S𝑠𝑜𝑠[[N1⊕N2]]G,𝜉 (𝜂)=S𝑠𝑜𝑠[[N1]]G,𝜉 (𝜋 𝑅(𝜂))if𝜋 𝐿(𝜂)=𝜆𝑣.True
S𝑠𝑜𝑠[[N1⊕N2]]G,𝜉 (𝜂)=S𝑠𝑜𝑠[[N2]]G,𝜉 (𝜋 𝑅(𝜂))if𝜋 𝐿(𝜂)≠𝜆𝑣.True
therefore
𝑐𝑜𝑛𝑑(𝜋 𝐿,S𝑠𝑜𝑠[[N1]]G,𝜉 ◦𝜋 𝑅,S𝑠𝑜𝑠[[N2]]G,𝜉 ◦𝜋 𝑅)=S𝑠𝑜𝑠[[N1⊕N2]]G,𝜉
ThecaseN★ : WehavethatS𝑑𝑠[[N★]]G,𝜉 =𝐹𝐼𝑋(𝐹),where
𝐹 =𝜆𝜙.𝑐𝑜𝑛𝑑(𝜆𝑒.𝜆𝑣.S𝑑𝑠[[N]]G,𝜉 (𝑒)(𝑣)≃𝜖 𝑒(𝑣),𝑖𝑑,𝜙◦S𝑑𝑠[[N]]G,𝜉 )
Westartbyprovingthatif 𝑓 : 𝐷 → 𝐷 isacontinuousfunctiononaccpo (𝐷,⊑) andif𝑑 ∈ 𝐷 issuchthat
𝑓(𝑑) ⊑ 𝑑,then𝐹𝐼𝑋(𝑓) ⊑ 𝑑.Toprovethis,werecallthatbythemonotonicityof 𝑓 itmustbethecasethat
𝑓(𝑓(𝑑)) ⊑ 𝑓(𝑑),andmoregenerally,that𝑓𝑛+1(𝑑) ⊑ 𝑓𝑛(𝑑),∀𝑛 ≥0.Therefore𝑑isanupperboundofthechain
{𝑓𝑛(𝑑) |𝑛 ≥0}.Then,since⊥⊑𝑑,wehavethat𝑓𝑛(⊥) ⊑ 𝑓𝑛(𝑑),∀𝑛 ≥0and𝑑isalsoanupperboundforthe
chain{𝑓𝑛(⊥) |𝑛 ≥0}.Then𝐹𝐼𝑋(𝑓)=(cid:195){𝑓𝑛(⊥) |𝑛 ≥0}⊑𝑑sincetheleastfixedpointof𝑓 istheleastupper
boundofthatchain.
Thisresultprovesthat𝐹𝐼𝑋(𝐹) ⊑ S𝑠𝑜𝑠[[N★]]G,𝜉,ontheconditionthatweshowthat𝐹(S𝑠𝑜𝑠[[N★]]G,𝜉 ) ⊑
S𝑠𝑜𝑠[[N★]]G,𝜉.Wecanprovethisbyfirstshowingthat
S𝑠𝑜𝑠[[N★ ]]G,𝜉 =S𝑠𝑜𝑠[[((N||𝜄);𝜓 ≃𝜖||𝜄);(𝜄⊕N;N★ )]]G,𝜉
⊒S𝑠𝑜𝑠[[(𝜄⊕N;N★ )]]G,𝜉 ◦S𝑠𝑜𝑠[[((N||𝜄);𝜓 ≃𝜖||𝜄)]]G,𝜉
=𝑐𝑜𝑛𝑑(𝜋 𝐿,S𝑠𝑜𝑠[[𝜄]]G,𝜉 ◦𝜋 𝑅,S𝑠𝑜𝑠[[N;N★ ]]G,𝜉 ◦𝜋 𝑅)◦S𝑠𝑜𝑠[[((N||𝜄);𝜓 ≃𝜖||𝜄)]]G,𝜉
=𝑐𝑜𝑛𝑑(S𝑠𝑜𝑠[[(N||𝜄);𝜓 ≃𝜖]]G,𝜉,S𝑠𝑜𝑠[[𝜄]]G,𝜉,S𝑠𝑜𝑠[[N;N★ ]]G,𝜉 )
⊒𝑐𝑜𝑛𝑑(S𝑠𝑜𝑠[[(N||𝜄);𝜓 ≃𝜖]]G,𝜉,S𝑠𝑜𝑠[[𝜄]]G,𝜉,S𝑠𝑜𝑠[[N★ ]]G,𝜉 ◦S𝑠𝑜𝑠[[N]]G,𝜉 )
TheinductionhypothesisgivesusthatS𝑑𝑠[[N]]G,𝜉 ⊑S𝑠𝑜𝑠[[N]]G,𝜉,andbythemonotonicityof◦(Lemma5.3)
and𝑐𝑜𝑛𝑑(Lemmas5.5and5.6)weget
S𝑠𝑜𝑠[[N★ ]]G,𝜉 ⊒𝑐𝑜𝑛𝑑(S𝑠𝑜𝑠[[(N||𝜄);𝜓 ≃𝜖]]G,𝜉,S𝑠𝑜𝑠[[𝜄]]G,𝜉,S𝑠𝑜𝑠[[N★ ]]G,𝜉 ◦S𝑠𝑜𝑠[[N]]G,𝜉 )
⊒𝑐𝑜𝑛𝑑(S𝑑𝑠[[(N||𝜄);𝜓 ≃𝜖]]G,𝜉,𝑖𝑑,S𝑠𝑜𝑠[[N★ ]]G,𝜉 ◦S𝑑𝑠[[N]]G,𝜉 )
=𝐹(S𝑠𝑜𝑠[[N★ ]]G,𝜉
)
ThiscompletesourproofofLemma6.3.
□
7 TYPESOUNDNESS
Wesaythata𝜇Gtermiswell-typedwheneveritcanbetypedwiththerulesofTable2.Theserulesguaranteethat
anywell-typed𝜇Gtermdefinesagraphneuralnetwork.Next,wewillbeusingthesetypingrulestoprovethetype
soundnessoftheoperationalsemanticsof𝜇G.Provingthetypesafetyof𝜇GensuresthattheGNNsweprogramwithit
handlethedatatypescorrectlyandproduceresultsoftheexpectedtype.Concretely,thismeansthatwell-typedGNNs
canalwaysprogresstothenextstepofcomputationandthataftereachstepthetypeoftheoutputispreserved.
ManuscriptsubmittedtoACM20 Belenchiaetal.
[T-ID] 𝜄:Φ G,𝜉[𝑇,𝑇]
Γ⊢𝑓
𝜓
:𝑇 1∗×𝑇1→𝑇2
[T-PSI]
𝜓 :Φ G,𝜉[𝑇1,𝑇2]
Γ⊢𝑓
𝜑
:𝑇1×𝑇
𝑒
×𝑇1→𝑇2 Γ⊢𝑓
𝜎
:𝑇 2∗×𝑇1→𝑇3
[T-PRE]
𝜑
◁
𝜎
:Φ G,𝜉[𝑇1,𝑇3]
Γ⊢𝑓
𝜑
:𝑇1×𝑇
𝑒
×𝑇1→𝑇2 Γ⊢𝑓
𝜎
:𝑇 2∗×𝑇1→𝑇3
[T-POST]
𝜑
▷
𝜎
:Φ G,𝜉[𝑇1,𝑇3]
N1:Φ G,𝜉[𝑇1,𝑇2] N2:Φ G,𝜉[𝑇2,𝑇3]
[T-SEQ]
N1;N2:Φ G,𝜉[𝑇1,𝑇3]
N1:Φ G,𝜉[𝑇,𝑇1] N2:Φ G,𝜉[𝑇,𝑇2]
[T-PAR]
N1||N2:Φ G,𝜉[𝑇,𝑇1×𝑇2]
N1:Φ G,𝜉[𝑇1,𝑇 1′] N2:Φ G,𝜉[𝑇2,𝑇 2′]
[T-PROD]
N1⊗N2:Φ G,𝜉[𝑇1×𝑇2,𝑇 1′×𝑇 2′]
N1:Φ G,𝜉[𝑇1,𝑇2] N2:Φ G,𝜉[𝑇1,𝑇2]
[T-CHOICE]
N1⊕N2:Φ G,𝜉[B×𝑇1,𝑇2]
N :Φ G,𝜉[𝑇,𝑇]
[T-STAR]
N★:Φ G,𝜉[𝑇,𝑇]
Table2. Typingrulesof𝜇G.
Theorem7.1(Typesoundness). IfN :Φ G,𝜉[𝑇1,𝑇2],𝜂 :𝐻[𝑇1]and⟨N,𝜂⟩→★
G,𝜉
𝜂′,then𝜂′ :𝐻[𝑇2].
Proof. FollowsfromLemma7.2(Progress)andLemma7.3(Preservation). □
Lemma7.2(Progress). SupposethatN isawell-typedtermN :Φ G,𝜉[𝑇1,𝑇2]andthat𝜂 :𝐻[𝑇1],theneitherN =𝜄or
thereexistsN′,𝜂′suchthat⟨N,𝜂⟩→ ⟨N′,𝜂′⟩.
G,𝜉
Proof. Byinductionontypingderivations.ItsufficestoshowthateitherN =𝜄orthereexistsanoperationalrule
thatcanbeapplied.
CaseT-ID: Immediate,asN =𝜄.
CaseT-PSI: WehaveN =𝜓 andthereforecanuseruleAPPLY.
𝜑
CaseT-PRE: WehaveN =◁ 𝜎 andthereforecanuserulePREIMG.
𝜑
CaseT-POST: WehaveN =▷ 𝜎 andthereforecanuserulePOSTIMG.
CaseT-SEQ: WehaveN =N1;N2whereN1 :Φ G,𝜉[𝑇1,𝑇]andN2 :Φ G,𝜉[𝑇,𝑇2].ByinductionhypothesisN1is
either𝜄oritispossibletoapplyatransitionrule.Intheformercase,wecanapplyruleSEQ2,whileinthelatter
casewecanapplyruleSEQ1.
CaseT-PAR: WehaveN = N1||N2 whereN1 : Φ G,𝜉[𝑇,𝑇1] andN2 : Φ G,𝜉[𝑇,𝑇2].Inthiscase,wecanuserule
SPLIT.
ManuscriptsubmittedtoACMThe𝜇GLanguageforProgrammingGraphNeuralNetworks 21
CaseT-PROD: WehaveN =N1⊗N2whereN1:Φ G,𝜉[𝑇1,𝑇 1′]andN2:Φ G,𝜉[𝑇2,𝑇 2′].ByinductionhypothesisN1
andN2areeither𝜄orisitpossibletoapplyatransitionrule.Then,
• ifN1=𝜄andN2=𝜄,wecanapplyruleMERGE.
• ifN1≠𝜄andN2=𝜄,wecanapplyrulePAR1.
• ifN1=𝜄andN2≠𝜄,wecanapplyrulePAR2.
• ifN1≠𝜄andN2≠𝜄,wecanapplyeitherrulePAR1orrulePAR2.
CaseT-CHOICE: We have N = N1 ⊕ N2 : Φ G,𝜉[B×𝑇1,𝑇2] where N1 : Φ G,𝜉[𝑇1,𝑇2] and N2 : Φ G,𝜉[𝑇1,𝑇2].
Furthermore,wehavethat𝜂 :𝐻[B×𝑇1].If𝜋 𝐿(𝜂)istheconstantnode-labelingfunctionthatmapseverynode
toTrue,weapplyruleCHOICE1.InanyothercaseweapplyruleCHOICE2.
CaseT-STAR: WehaveN =N 1★whereN1:Φ G,𝜉[𝑇,𝑇].WecanapplyruleSTAR.
□
Lemma7.3(Preservation). IfN :Φ G,𝜉[𝑇1,𝑇2],𝜂 :𝐻[𝑇1],and⟨N,𝜂⟩→
G,𝜉
𝛾
• if𝛾 =⟨N′,𝜂′⟩,thenN′ :Φ G,𝜉[𝑇′,𝑇2]and𝜂′ :𝐻[𝑇′].
• if𝛾 =𝜂′,then𝜂′ :𝐻[𝑇2].
Proof. Byinductionontypingderivations.
CaseT-ID: WehaveN =𝜄:Φ G,𝜉[𝑇,𝑇]and𝜂 :𝐻[𝑇].TheonlyapplicableruleisIDbywhich⟨𝜄,𝜂⟩→
G,𝜉
𝜂 :𝐻[𝑇].
CaseT-PSI: Wehave𝜓 :Φ G,𝜉[𝑇1,𝑇2]and𝜂 :𝐻[𝑇1].TheonlyrulethatcanbeappliedisAPPLYwhichgivesN′ =𝜄
and𝜂′ =𝜆𝑣.𝑓 𝜓([𝜂(𝑢) |𝑢 ∈V],𝜂(𝑣)).Then𝜂′hastype𝐻[𝑇2]and𝜄hastypeΦ G,𝜉[𝑇2,𝑇2]byruleT-ID.
𝜑
CaseT-PRE: Wehave◁ 𝜎 :Φ G,𝜉[𝑇1,𝑇3]and𝜂 :𝐻[𝑇1].TheonlyrulethatcanbeappliedisPREIMGwhichgives
→−
N′ = 𝜄 and𝜂′ = 𝜆𝑣.𝑓 𝜎([𝑓 𝜑(𝜂(𝑢),𝜉((𝑣,𝑢)),𝜂(𝑣)) | 𝑢 ∈ 𝑁(𝑣)],𝜂(𝑣)).Then𝜂′ hastype𝐻[𝑇3] and𝜄 hastype
Φ G,𝜉[𝑇3,𝑇3]byruleT-ID.
CaseT-POST: AnalogoustoT-PRE,butusingrulePOSTIMGinstead.
CaseT-SEQ: WehaveN1;N2 : Φ G,𝜉[𝑇1,𝑇3] and𝜂 : 𝐻[𝑇1].Wealsohavesub-derivationswithconclusionsthat
N1:Φ G,𝜉[𝑇1,𝑇2]andN2:Φ G,𝜉[𝑇2,𝑇3].Therearetwopossiblerulesthatcanbeapplied,SEQ1andSEQ2.
• InthecaseofSEQ1,wehaveN′ =N 1′;N2.ByapplyingtheinductionhypothesisonN1wegetthat𝜂′ :𝐻[𝑇]
andN 1′ :Φ G,𝜉[𝑇,𝑇2]forsomenode-labelingtype𝐻[𝑇].ThenwecanapplyruleT-SEQonN 1′;N2togetits
typeΦ G,𝜉[𝑇,𝑇3].
• InthecaseofSEQ2,thenN1 =𝜄,𝑇2 =𝑇1andwehaveN′ =N2whichweknowhastypeΦ G,𝜉[𝑇1,𝑇3],and
𝜂′ =𝜂whichweknowhastype𝐻[𝑇1].
CaseT-PAR: WehaveN1||N2:Φ G,𝜉[𝑇,𝑇1×𝑇2]and𝜂 :𝐻[𝑇].Wealsohavesub-derivationswithconclusionsthat
N1:Φ G,𝜉[𝑇,𝑇1]andN2:Φ G,𝜉[𝑇,𝑇2].TheonlyrulethatcanbeappliedisSPLIT.Then𝜂′ =𝜂|𝜂 :𝐻[𝑇 ×𝑇]and
N′ =N1⊗N2whichbyruleT-PRODhastypeΦ G,𝜉[𝑇 ×𝑇,𝑇1×𝑇2].
CaseT-PROD: WehaveN1⊗N2 :Φ G,𝜉[𝑇1×𝑇2,𝑇 1′×𝑇 2′]and𝜂 :𝐻[𝑇1×𝑇2].Wealsohavesub-derivationswith
conclusionsthatN1 : Φ G,𝜉[𝑇1,𝑇 1′] andN2 : Φ G,𝜉[𝑇2,𝑇 2′].Therearethreepossiblerulesthatcanbeapplied,
PAR1,PAR2,andMERGE.
• InthecaseofPAR1,wehaveN′ =N 1′⊗N2and𝜂′ =𝜂1|𝜋 𝑅(𝜂).ByapplyingtheinductionhypothesisonN1
wegetthat𝜂1:𝐻[𝑇]andN 1′ :Φ G,𝜉[𝑇,𝑇 1′]forsomenode-labelingtype𝐻[𝑇].ThenwecanapplyruleT-Prod
onN 1′⊗N2togetitstypeΦ G,𝜉[𝑇 ×𝑇2,𝑇 1′×𝑇 2′],and𝜂′ :𝐻[𝑇 ×𝑇2].
• ThecaseofPAR2isanalogoustothatofPAR1.
ManuscriptsubmittedtoACM22 Belenchiaetal.
• InthecaseofMERGE,wehaveN′ = 𝜄 and𝜂′ = 𝜂.Furthermore,thisrulecouldhavebeenappliedonlyif
N1=N2=𝜄,whichmeansthat𝑇1=𝑇 1′and𝑇2=𝑇 2′.ThenbyruleT-Idwegetthat𝜄:Φ G,𝜉[𝑇 1′×𝑇 2′,𝑇 1′×𝑇 2′].
CaseT-CHOICE: We have N1 ⊕N2 : Φ G,𝜉[B×𝑇1,𝑇2] and𝜂 : 𝐻[B×𝑇1]. We also have sub-derivations with
conclusionsthatN1 : Φ G,𝜉[𝑇1,𝑇2] andN2 : Φ G,𝜉[𝑇1,𝑇2].Therearetwopossiblerulesthatcanbeapplied,
CHOICE1andCHOICE2.
• IncaseCHOICE1,wehave𝜂′ =𝜋 𝑅(𝜂):𝐻[𝑇1]andN′ =N1whichweknowtohavetypeΦ G,𝜉[𝑇1,𝑇2].
• IncaseCHOICE2,wehave𝜂′ =𝜋 𝑅(𝜂):𝐻[𝑇1]andN′ =N2whichweknowtohavetypeΦ G,𝜉[𝑇1,𝑇2].
CaseT-STAR: WehaveN★:Φ G,𝜉[𝑇,𝑇]and𝜂 :𝐻[𝑇].WealsohaveasubderivationwiththeconclusionthatN :
Φ G,𝜉[𝑇,𝑇].TheonlyrulethatcanbeappliedisSTAR.Then𝜂′ =𝜂 :𝐻[𝑇]andN′ =((𝜄||N);𝜓(cid:27)||𝜄);(𝜄⊕N;N★).
ByapplyingthetypingruleswecanconcludethatthistermalsohastypeΦ G,𝜉[𝑇,𝑇].
□
8 AGRAPHICALREPRESENTATION
Inthissection,wepresentagraphicalrepresentationfor𝜇Gprogramsandexpressions.Usingsuchgraphicalnotation
makesiteasiertounderstandandcommunicatethestructureandpurposeofa𝜇Gprogram,astheprogrammermight
easilylosetrackoflabeltypesandsizeswhendevelopingintextualform.Thebasictermsof𝜇Gcanberepresented
graphicallyasboxes,asshowninFigure1,whilethecompositetermsofthelanguageareshowninFigure2.Eachterm
hasexactlyoneinputnode-labelingandoneoutputnode-labeling,andthegraphicalrepresentationisbuilttop-down
startingfromthemain(usuallycomposite)termandsubstitutingeachboxwitheitherabasictermoracompositeone;
inthislastcase,thisprocedureisrepeatedrecursivelyuntilallboxesinthefigurearebasicterms.Asanillustrative
example,let𝜓1,𝜓2,𝜓3,𝜑,𝜎befunctionsymbols.Figure3showshowwecanrepresentgraphicallythe𝜇Gexpression
𝜑
(𝜓1||𝜓2);𝜓3;◁ 𝜎.
Fig.1. Graphicalrepresentationofbasic𝜇Gexpressions.
Inthegraphicalrepresentationfor𝜇Gexpressionswehaveusedsomeauxiliaryfunctionswhichwedescribeinturn:
• Thefunction•:𝐻[𝑇1] →𝐻[𝑇1]×𝐻[𝑇1],•(𝜂)=(𝜂,𝜂)mapsanode-labelingfunctiontoatuplecontainingtwo
copiesofitself.
• Thefunction◦:𝐻[𝑇1]×𝐻[𝑇2] →𝐻[𝑇1×𝑇2],
◦(𝜂1,𝜂2)=𝜂 𝜂1
2
i if f𝜂 𝜂2 1= =𝜂 𝜂⊥
⊥
𝜂1|𝜂2
otherwise

mapstwonode-labelingfunctionstoanewnode-labelingfunctionwhichlabelseachnodewiththeconcatenation
oftheirlabels.Ifoneoftheinputlabelingfunctionsistheemptylabeling,◦simplyreturnstheotherfunction.
ManuscriptsubmittedtoACMThe𝜇GLanguageforProgrammingGraphNeuralNetworks 23
(a) The sequential composition of (b)Theparallelcompositionofgraphneu-
graphneuralnetworks𝜙1and𝜙2. ralnetworks𝜙1and𝜙2.
(c)Thechoiceoperatorforthegraphneuralnet-
works𝜙1and𝜙2.
(d)Thestargraphneuralnetworkthatcomputesthefixedpointof𝜙1.
Fig.2. Graphicalrepresentationofthepossiblecompositionsof𝜇Gexpressions.
Fig.3.
Examplegraphicalrepresentationofthe𝜇Gexpression(𝜓1||𝜓2);𝜓3;◁𝜑
𝜎.
• Thefunction⋄:𝐻[B×𝑇1] →𝐻[𝑇1]×𝐻[𝑇1],
⋄(𝜂)=(𝜋 𝑅(𝜂),𝜂 ⊥) if𝜋 𝐿(𝜂)=𝜆𝑣.True
(𝜂 ⊥,𝜋 𝑅(𝜂)) otherwise

takesaBooleannode-labelingfunctionandanotherlabelingfunctionandpropagatesitinoutputasthefirstor
secondelementofatuple.IftheBooleanlabelingfunctionmapseverynodetoatruevalue,theotherlabeling
functionispropagatedasthefirstelementofthetuple(tobereceivedininputbytheGNNtobeexecutedwhen
thetestissatisfied),otherwiseitisreturnedasthesecondelementofthetuple.
ManuscriptsubmittedtoACM24 Belenchiaetal.
9 IMPLEMENTATION
The𝜇GlanguagehasbeenimplementedasaPythonlibrarycalledlibmg,whichwedescribedindetailinaprevious
work[7].ThelibrarywasdevelopedontopofSpektral[15],agraphneuralnetworklibrarybasedonTensorflow[1].
ThemainadvantageofusingaframeworksuchasTensorFlowisthat𝜇G programsarecompiledtographneural
networksthatcanbeexecutedseamlesslyonCPUs,GPUs,orinadistributedsettingwithoutchangingthesourcecode.
The functionalities of libmg include the typical language support tools in the form of a parser, unparser and
normalizerfor𝜇Gexpressions.Themainfunctionalitythatisprovidedconsistsinthe𝜇Gcompilerthattransformsa
𝜇GprogramintoaTensorFlowModelinstance.Thecompilercanbesetuptoautomaticallymemoizesub-expressions
thatoccurmultipletimesinthesameprogram.Thelibraryalsoimplementsbasicgraphvisualizationfunctions,that
allowstoviewinanywebbrowsertheinputoroutputgraphs,orintermediaterepresentationproducedbya𝜇Gmodel.
Finally,asimpleexplanatoryalgorithmisprovidedthatcomputesthesub-graphthatwasusedbya𝜇G modelto
computethelabelofagivennode.
Usinglibmg,the𝜓,𝜑,and𝜎 functionsaredefinedbyinstantiatingthecorrespondingclassesandarestoredin
dictionaries.Thesedictionaries,whichcontainamappingbetweenfunctionnamesandthefunctionsthemselves,are
usedtocreatea𝜇Gcompilerinstance.The𝜇Gcompilercanthenparse𝜇Gprogramscontainingthefunctionnames
definedinitsdictionaries.
ThemodelsgeneratedbythecompilercanbetrainedbyTensorFlowasusual.Onlyinthecaseofthestaroperator,
thecorrespondinglayerimplementsacustomgradientcomputation.Inthatcaseimplicitdifferentiationisusedasto
makethecomputationofthegradienttractable.
10 EVALUATION
Weevaluate𝜇Gbothintermsofitsexpressiveness,meaningthatitcanbeusedtodefinemost,ifnotall,ofthegraph
neuralnetworkarchitecturesthathavebeendevelopedovertheyears,andalsoasaspecificationlanguageforthe
developmentofgraphneuralnetworksforspecifictasks.InSection10.1wedescribeanupdatedversionoftheCTL
modelcheckerdevelopedusing𝜇Gintroducedinourpreviouswork[6,7].Then,inSection10.2,weshowhowto
definesomeofthemorewell-knownGNNarchitecturesusingourlanguage.
10.1 ModelChecking
Weevaluated𝜇GinapreviousworkbyusingittodefineaGNNthatperformsCTLmodelcheckingonKripkestructures.
Forthatusecase,thesetoffunctionsthatweusedisshowninTable3,whileinTable4weshowthetranslationfunction
fromCTLtothe𝜇GexpressiondenotingtheGNNthatverifiesit.
The𝜇Gmodelcheckerimplementationwascomparedtotwootherexplicit-statemodelcheckers,pyModelChecking1
andmCRL2[9].WeusedsomeofthebenchmarksfromtheModelCheckingContest2022[2],namelythoseforwhich
wecouldexplicitlygeneratethereachabilitygraphonourmachine,showninTable5.The𝜇Gmodelcheckerwas
setuptoeitherverifyalltheformulasinparallel(thefullsetup),oronebyoneaswouldhavebeenthecaseforthe
othermodelcheckers(thesplitsetup).Intheformercase,therewouldhavebeenadditionaladvantagesduetothe
memoizationofcommonsub-formulasacrossmultipleformulas.TheexperimentalresultsareshowninFigure4,and
wenoticethatforthelargerKripkestructuresthereisanevidentspeedadvantageof𝜇Gcomparedtotheothermodel
checkers(insomeinstances,byanorderofmagnitude).
1https://github.com/albertocasagrande/pyModelChecking
ManuscriptsubmittedtoACMThe𝜇GLanguageforProgrammingGraphNeuralNetworks 25
(cid:40)
True if𝑝 ∈𝑥
𝜓
𝑝
:(2𝐴𝑃)★×2𝐴𝑃 →B 𝜓 𝑝(𝑋,𝑥)= (∀𝑝 ∈𝐴𝑃)
False otherwise
𝜓
𝑡𝑡
:(2𝐴𝑃)★×2𝐴𝑃 →B 𝜓 𝑡𝑡(𝑋,𝑥)=True
𝜓
𝑓𝑓
:(2𝐴𝑃)★×2𝐴𝑃 →B 𝜓 𝑓𝑓(𝑋,𝑥)=False
𝜓 ¬:(B)★×B→B 𝜓 ¬(𝑋,𝑥)=¬𝑥
𝜓 ∧:(B2)★×B2→B 𝜓 ∧(𝑋,𝑥)=𝑥1∧𝑥2
𝜓 ∨:(B2)★×B2→B 𝜓 ∨(𝑋,𝑥)=𝑥1∨𝑥2
𝜑
𝑖𝑑
:B×()×B→B 𝜑 𝑖𝑑(𝑖,𝑒,𝑗)=𝑖
𝜎 ∨:B★×B→B 𝜎 ∨(𝑀,𝑥)=(cid:212) 𝑖𝑀
𝑖
Table3. The𝜓,𝜑,and𝜎functionsusedforCTLmodelchecking.Thesubscriptsindicatethefunctionsymbolsthatwillbeusedinthe
𝜇Gexpressions.Foreach𝑝 ∈𝐴𝑃,weconsideracorrespondingfunction𝜓𝑝.
M(𝑝)=𝑝
M(¬𝜙)=M(𝜙);¬
M(𝜙1∧𝜙2)=(M(𝜙1)||M(𝜙2));∧
M(EX𝜙)=M(𝜙);▷𝑖𝑑
∨
M(EG𝜙)=fix 𝑋 =𝑡𝑡 in (M(𝜙)||𝑋;▷𝑖𝑑 );∧
∨
M(E𝜙1U𝜙2)=fix 𝑋 =𝑓𝑓 in (M(𝜙2)||(M(𝜙1)||𝑋;▷𝑖 ∨𝑑 );∨);∧
Table4. RecursivedefinitionofthetranslationfunctionM:𝜙→N.
ID PetriNet #nodesinRG #edgesinRG
1 RobotManipulation-PT-00001 110 274
2 TokenRing-PT-005 166 365
3 Philosophers-PT-000005 243 945
4 RobotManipulation-PT-00002 1430 5500
5 Dekker-PT-010 6144 171530
6 BART-PT-002 17424 53328
7 ClientsAndServers-PT-N0001P0 27576 113316
8 Philosophers-PT-000010 59049 459270
9 Referendum-PT-0010 59050 393661
10 SatelliteMemory-PT-X00100Y0003 76358 209484
11 RobotManipulation-PT-00005 184756 1137708
12 Dekker-PT-015 278528 16834575
13 HouseConstruction-PT-00005 1187984 7191110
Table5. Petrinetmodelsusedfortheexperiments,forwhichwereportedherethenumberofnodesandedgesinthereachability
graph(RG).
10.2 SpecificationofGraphNeuralNetworks
Inthissection,weshowhowtodefinesomeofthemostpopulargraphneuralnetworkmodelsin𝜇G.Asisthecasefor
anyprogramminglanguage,therearemanydifferentwaystoobtainthesameresult,thereforetheimplementations
ManuscriptsubmittedtoACM26 Belenchiaetal.
Fig.4. Executiontimesof𝜇G(splitandfullsetups),pyModelChecking,andmCRL2across13PetrinetmodelsfromtheModel
CheckingContest2022.They-axisisonalogarithmicscale.OnthetwelfthPetrinet,theredbarindicatesthatthe𝜇Gmodelranout
ofmemoryafterthepre-processing(tracing)step.
thatweshowhereareonlyintendedtohighlightthegeneralityofthelanguage.Forspecificusecases,itisuptothe
programmertochoosethemostuseful𝜇Gimplementation.
Graph Convolutional Networks. A Graph Convolutional Network [21] (GCN) performs a graph convolution by
multiplyingthenodefeatureswithamatrixofweights,normalizedaccordingtothedegreeofthenodesandassigning
greaterimportancetothefeaturesreceivedfromnodeswithfewerneighbors.Thenewnodelabels𝑥′ ∈ R𝑚 are
𝑖
computedfromthecurrentnodelabels𝑥
𝑖
∈R𝑛accordingtotheequation
𝑥 Θ
𝑥 𝑖′ =𝑓 (cid:169)
(cid:173)
∑︁ √︁𝑑𝑒𝑔(𝑖)𝑗 √︁𝑑𝑒𝑔(𝑗)(cid:170)
(cid:174)
(cid:171)𝑗∈𝑁 G(𝑖)∪{𝑖}
(cid:172)
where𝑑𝑒𝑔(𝑖)isthedegreeofnode𝑖,Θ∈R𝑛×𝑚isamatrixoftrainableweights,and𝑓 isanactivationfunction.Theset
ofneighborsofanodeincludeboththepredecessorsandthesuccessors,astheGCNwasoriginallythoughtforthe
caseofundirectedgraphs.ToimplementtheGCN,wemakeuseofthefunctionsshowninTable6.Westartbydefining
a𝜇Gexpressiontocomputethedegreeofeachnode.Werecallthatthedegreeofanodeisthenumberofitsoutgoing
edges(theoutdegree)plusthenumberofincomingedges(theindegree),withselfloopscountingasboth.Itiseasyto
computethisnumberin𝜇G:weuse◁1 tocomputetheindegreeand▷1 tocomputetheoutdegree,thenweaddthem.
+ +
(◁1 +||▷1 +);+2
ManuscriptsubmittedtoACMThe𝜇GLanguageforProgrammingGraphNeuralNetworks 27
𝜓
𝑁𝑁
:(R𝑛)★×R𝑛 →R𝑚 𝜓 𝑁𝑁(𝑋,𝑥)=𝑓(𝑥𝑇Θ)
𝜓
+2
:(R2)★×R2→R 𝜓 +2(𝑋,𝑥)=𝑥1+𝑥2
𝜓
+2𝑛
:(R2𝑛)★×R2𝑛 →R𝑛 𝜓 +2𝑛(𝑋,𝑥)=𝑥1+𝑥2
𝜑1:R𝑛×R×R𝑛 →R 𝜑1(𝑖,𝑒,𝑗)=1
𝜑 𝑑𝑔𝑛 :R𝑛+1×R×R𝑛+1→R𝑛 𝜑 𝑑𝑔𝑛(𝑖,𝑒,𝑗)= √ 𝑖1𝑖2√ 𝑗1
𝜎 +:R★×R𝑛 →R 𝜎 +(𝑀,𝑥)=1+(cid:205) 𝑖𝑀
𝑖
𝜎 𝑑𝑔+:(R𝑛)★×R𝑛+1→R𝑛 𝜎 𝑑𝑔+(𝑀,𝑥)= 𝑥𝑥 12 +(cid:205) 𝑖𝑀 𝑖
Table6. FunctionsusedinthedefinitionoftheGCN.
Inthestepsthatfollow,weneedthedegreeofnodesbutalsotheoriginallabels,thereforeweuseparallelcomposition
with𝜄tohaveboththesevalues.
(◁1 +||▷1 +);+2||𝜄
Thenwecompute(cid:205) 𝑗∈𝑁 G(𝑖)∪{𝑖} √ 𝑑𝑒𝑔(𝑖𝑥 )√𝑗
𝑑𝑒𝑔(𝑗)
bysummingthepre-imageandthepost-imageusing𝑑𝑔𝑛and𝑑𝑔+
((◁1 +||▷1 +);+2||𝜄);(◁𝑑 𝑑𝑔 𝑔𝑛 +||▷𝑑 𝑑𝑔 𝑔𝑛 +);+2𝑛
Notethatboth𝜑
𝑑𝑔𝑛
and𝜎 𝑑𝑔+expectnodelabels𝑙 ∈R𝑛+1,where𝑙1 ∈Risthedegreeofthenodeand𝑙2 ∈R𝑛 isthe
actuallabelofthenode.Finally,wemultiplythenodelabelswiththeweightmatrixΘfollowedbytheapplicationof
theactivationfunction𝑓,i.e.,theclassicdenseneuralnetworklayer.Puttingallofthistogether,the𝜇Gexpressionthat
implementsaGCNlayeris
((◁1 +||▷1 +);+2||𝜄);(◁𝑑 𝑑𝑔 𝑔𝑛 +||▷𝑑 𝑑𝑔 𝑔𝑛 𝑟+);+2𝑛;𝑁𝑁
GraphAttentionNetworks. AGraphAttentionNetwork[34](GAT),liketheGCN,transformsthenodelabelsusinga
learnableweightmatrixΘ∈R𝑛×𝑚.Thenitemploysaself-attentionmechanism𝑎:R𝑚×R𝑚 →R,sharedbyallthe
nodes,thatgiventhelabelsofnode𝑗 andnode𝑖suchthat(𝑗,𝑖) ∈E,itindicatestheimportanceofnode𝑗’slabelsto
node𝑖.Thisvalueisthennormalizedusingthesoftmaxfunction.Intheoriginalformulation,theattentionmechanismis
asingle-layerfeedforwardneuralnetworkwhichusesaweightvectora∈R2𝑚andtheLeakyReLUactivationfunction.
Therefore,theattentioncoefficients𝛼 aregivenby
𝑗𝑖
𝑒𝑥𝑝(cid:16) 𝐿𝑒𝑎𝑘𝑦𝑅𝑒𝐿𝑈 (cid:16) a𝑇(𝑥 𝑖Θ,𝑥 𝑗Θ)(cid:17)(cid:17)
𝛼 𝑗𝑖 = (cid:205) 𝑘∈← 𝑁− G(𝑖)∪{𝑖}𝑒𝑥𝑝(cid:0)𝐿𝑒𝑎𝑘𝑦𝑅𝑒𝐿𝑈 (cid:0) a𝑇(𝑥 𝑖Θ,𝑥 𝑘Θ)(cid:1)(cid:1)
Giventheseattentioncoefficients,theGATlayercomputesthenewnodelabels𝑥 𝑖′ ∈R𝑚fromthecurrentlabels𝑥
𝑖
∈R𝑛
accordingtotheequation
𝑥 𝑖′ =𝑓 (cid:169)
(cid:173)
∑︁ 𝛼 𝑗𝑖𝑥 𝑗Θ(cid:170)
(cid:174)
(cid:173) (cid:174)
←−
(cid:171)𝑗∈𝑁 G(𝑖)∪{𝑖}
(cid:172)
where𝑓 istheactivationfunction.Theauthorsthenextendthisbasicmechanismbyemployingmulti-headattention,
thatis,theydefine𝐾 independentattentionheadsandweightmatriceswhoseoutputsarethenconcatenated.Thus,we
get
𝑥 𝑖′ =(cid:13) (cid:13) (cid:13)𝑘𝐾 =1𝑓 (cid:169) (cid:173)
(cid:173)
∑︁ 𝛼𝑘 𝑗𝑖𝑥 𝑗Θ𝑘(cid:170) (cid:174)
(cid:174)
←−
(cid:171)𝑗∈𝑁 G(𝑖)∪{𝑖}
(cid:172)
ManuscriptsubmittedtoACM28 Belenchiaetal.
𝜓𝜓 𝑁 :𝑁 (𝑖 R: 𝑛( +R 1𝑛 )★)★ ×× RR 𝑛𝑛 +1→ →R R𝑚
𝑛
𝜓 𝜓𝑁 (𝑁 𝑋𝑖 ,( 𝑥𝑋 ), =𝑥) 𝑥=
1
𝑓(𝑥Θ𝑖)
/ / 𝑥2
𝜑
𝑎𝑡𝑡∗𝑖
:R𝑛×R×R𝑛 →R𝑛 𝜑 𝑎𝑡𝑡∗𝑖(𝑖,𝑒,𝑗)=𝑒𝑥𝑝(𝐿𝑒𝑎𝑘𝑦𝑅𝑒𝐿𝑈(a𝑖 ·(𝑖𝑇Θ𝑖,𝑗𝑇Θ𝑖)))·𝑖
𝜑
𝑎𝑡𝑡𝑖
:R𝑛×R×R𝑛 →R 𝜑 𝑎𝑡𝑡𝑖(𝑖,𝑒,𝑗)=𝑒𝑥𝑝(𝐿𝑒𝑎𝑘𝑦𝑅𝑒𝐿𝑈(a𝑖 ·(𝑖𝑇Θ𝑖,𝑗𝑇Θ𝑖)))
𝜎
𝑡−𝑎𝑡𝑡𝑖
:R★×R𝑛 →R 𝜎 𝑡−𝑎𝑡𝑡𝑖(𝑀,𝑥)=𝑒𝑥𝑝(𝐿𝑒𝑎𝑘𝑦𝑅𝑒𝐿𝑈(a𝑖 ·(𝑥𝑇Θ𝑖,𝑥𝑇Θ𝑖)))+(cid:205) 𝑗𝑀
𝑗
𝜎
𝑛−𝑎𝑡𝑡𝑖
:(R𝑛)★×R𝑛 →R𝑛 𝜎 𝑛−𝑎𝑡𝑡𝑖(𝑀,𝑥)=𝑒𝑥𝑝(𝐿𝑒𝑎𝑘𝑦𝑅𝑒𝐿𝑈(a𝑖 ·(𝑥𝑇Θ𝑖,𝑥𝑇Θ𝑖)))·𝑥+(cid:205) 𝑗𝑀
𝑗
Table7. FunctionsusedinthedefinitionofGATconvolutions.
ThefunctionsneededtoimplementtheGATlayerareshowninTable7.First,wecomputetheunnormalizedattention
coefficientsandmultiplythemwiththenodelabels,usingtheterm◁𝑎𝑡𝑡∗1 .Thentonormalizethem,wecompute
𝑛−𝑎𝑡𝑡1
inparallelthetotalattentionvaluesforincomingedgesateachnodeusing◁𝑎𝑡𝑡1 ,anddividethelabelsofthefirst
𝑡−𝑎𝑡𝑡1
expressionbythisexpression
(◁𝑎𝑡𝑡∗1 ||◁𝑎𝑡𝑡1 );/
𝑛−𝑎𝑡𝑡1 𝑡−𝑎𝑡𝑡1
Atthispoint,wesimplyhavetomultiplybytheweightmatrixandapplytheactivationfunctiontogetthebasicGAT
layer
(◁ 𝑛𝑎𝑡 −𝑡 𝑎∗ 𝑡1 𝑡1||◁ 𝑡𝑎 −𝑡𝑡 𝑎1 𝑡𝑡1);/;𝑁𝑁1
Nowweshowhowtousemulti-headattention.Wesavethepreviousexpressionin𝑘 differentvariablesusinglet
expressions,suchthateachexpressiongetsitsownattentionmechanismandweightmatrix.Thenthebodyofthelet
expressionissimplytheparallelcompositionofthesevariables
let 𝐺𝐴𝑇1=(◁ 𝑛𝑎𝑡 −𝑡 𝑎∗ 𝑡1 𝑡1||◁ 𝑡𝑎 −𝑡𝑡 𝑎1 𝑡𝑡1);/;𝑁𝑁1,
.
.
.
𝐺𝐴𝑇 𝑘 =(◁ 𝑛𝑎𝑡 −𝑡 𝑎∗ 𝑡𝑘 𝑡𝑘||◁ 𝑡𝑎 −𝑡𝑡 𝑎𝑘 𝑡𝑡𝑘);/;𝑁𝑁 𝑘 in
𝐺𝐴𝑇1||𝐺𝐴𝑇2||···||𝐺𝐴𝑇
𝑘
GraphIsomorphismNetworks. AGraphIsomorphismNetwork[36](GIN)generalizestheWeisfeiler-Lehmantest[35]
andthushasthegreatestdiscriminativepoweramongGNNsfordeterminingwhethertwographsarenotisomorphic.
TheGINconvolutionconsistsinmultiplyingeachnodelabelby(1+𝜖),where𝜖 ∈Risatrainableparameter,thenadding
thesumofthelabelsoftheneighbornodes.Theobtainedvaluesarethenpassedininputtoamultilayerperceptron
(MLP),thatis,itgetsmultipliedwithalearnableweightmatrixΘ𝑖 followedbytheapplicationofanactivationfunction
𝑓
𝑖
twoormoretimesinsequence.Formally,theGINcomputesthenewnodelabels𝑥 𝑖′ ∈R𝑚 fromthecurrentlabels
𝑥
𝑖
∈R𝑛as
𝑥 𝑖′ =𝑀𝐿𝑃((1+𝜖)·𝑥 𝑖 + ∑︁ 𝑥 𝑗)
←−
𝑗∈𝑁 G(𝑖)
InTable8weshowthefunctionsweneedtoimplementtheGINlayerin𝜇G.Weuseparallelcompositiontocompute
theproductofthenodelabelswith1+𝜖andthesumofthelabelsoftheneighborsofeachnode,thenweaddthem
using+.
(∗𝜖||◁𝑖𝑑 );+
+
Finally,weputinsequencetwodenseneuralnetworklayerswithweightmatricesΘ1 ∈ R𝑛×𝑛′,Θ2 ∈ R𝑛′×𝑚 and
activationfunctions𝑓1,𝑓2,asinthecaseoftheoriginalGINarticle.Therefore,the𝜇Gexpressionthatimplementsa
ManuscriptsubmittedtoACMThe𝜇GLanguageforProgrammingGraphNeuralNetworks 29
𝜓
𝑁𝑁1
:(R𝑛)★×R𝑛 →R𝑛′ 𝜓 𝑁𝑁1(𝑋,𝑥)=𝑓1(𝑥Θ1)
𝜓
𝑁𝑁2
:(R𝑛′)★×R𝑛′ →R𝑚 𝜓 𝑁𝑁2(𝑋,𝑥)=𝑓2(𝑥Θ2)
𝜓
∗𝜖
:(R𝑛)★×R𝑛 →R𝑛 𝜓 ∗𝜖(𝑋,𝑥)=(𝑤
𝜖
+1)·𝑥
𝜓 +:(R2𝑛)★×R2𝑛 →R𝑛 𝜓 +(𝑋,𝑥)=𝑥1+𝑥2
𝜑
𝑖𝑑
:R𝑛×R×R𝑛 →R𝑛 𝜑 ∗(𝑖,𝑒,𝑗)=𝑖
𝜎 +:(R𝑛)★×R𝑛 →R𝑛 𝜎 +(𝑀,𝑥)=(cid:205) 𝑖𝑀
𝑖
Table8. FunctionsusedinthedefinitionofGINconvolutions.
𝜓
𝑀𝐿𝑃2
:(R𝑛+𝑘)★×R𝑛+𝑘 →R𝑚 𝜓 𝑀𝐿𝑃2(𝑋,𝑥)=𝑓4(𝑓3(𝑥Θ3)Θ4)
𝜓0:(R𝑛)★×R𝑛 →R𝑘 𝜓0(𝑋,𝑥)=0𝑘
𝜑
𝑀𝐿𝑃1
:R𝑛+𝑘 ×R×R𝑛+𝑘 →R𝑘 𝜑 𝑀𝐿𝑃1(𝑖,𝑒,𝑗)=𝑓2(𝑓1((𝜋 𝐿(𝑗),𝑒,𝑖)Θ1)Θ2)
𝜎 +:(R𝑘)★×R𝑘 →R𝑘 𝜎 +(𝑀,𝑥)=(cid:205) 𝑖𝑀
𝑖
Table9. FunctionsusedinthedefinitionoftheoriginalGNNconvolution.
GINis
(∗𝜖||◁𝑖 +𝑑 );+;𝑁𝑁1;𝑁𝑁2
TheOriginalGraphNeuralNetworkModel. ThegraphneuralnetworkmodelwasfirstintroducedbyScarsellietal.
[30].Thenewnodelabels𝑥′ ∈R𝑚 oftheoriginalGNNmodelarecomputedasatwo-stepprocess.Inthefirststepthe
𝑖
new“state”𝑠 𝑖′ ∈R𝑘 ofeachnodeiscomputedfromthecurrentnodelabels𝑥
𝑖
∈R𝑛,theedgelabels𝑒
𝑗𝑖
∈R𝑙,andthe
neighbornodes’labels𝑥
𝑗
∈R𝑛andstates𝑠
𝑗
∈R𝑘 asthefixedpointoftheequation
∑︁
𝑠 𝑖 = ℎ(𝑥 𝑖,𝑒 𝑗𝑖,𝑥 𝑗,𝑠 𝑗)
←−
𝑗∈𝑁 G(𝑖)
whereℎ istypicallyatwo-layerperceptronwithweightmatricesΘ1 ∈ R(2𝑛+𝑙+𝑘)×𝑛′,Θ2 ∈ R𝑛′×𝑘 andactivation
functions𝑓1,𝑓2.Then,thesecondstepconsistsintheapplicationofafunction𝑔tothestateandlabelsofeachnodeto
obtainthefinalnodelabels
𝑥 𝑖′ =𝑔(𝑥 𝑖,𝑠 𝑖)
whereinthiscasetoo𝑔isatwo-layerperceptron,withweightmatricesΘ3 ∈R(𝑛+𝑘)×𝑚′,Θ4 ∈R𝑚′×𝑚 andactivation
functions 𝑓3,𝑓4.InTable9weshowthefunctionsrequiredtoimplementthisGNNlayer.Thefirststepconsistsin
initializingthenodestates.Forthisexample,weinitializethenodestatestoanarrayof𝑘zerosandweappendittothe
currentnodelabelsusingtheidentitytermandparallelcomposition
(𝜄||0)
After this step, we compute the fixed point of the states using a two-layer perceptron to generate messages and
aggregatingthemthroughsummation
(𝜄||0);(𝜄||◁𝑀𝐿𝑃1)★
+
Onceafixedpointforthestatesisreached,weuseanothermulti-layerperceptrontoobtainthefinalnodelabelsand
obtain
(𝜄||0);(𝜄||◁𝑀 +𝐿𝑃1)★;𝑀𝐿𝑃2
ManuscriptsubmittedtoACM30 Belenchiaetal.
11 CONCLUSION
Wepresentedthesyntaxandsemanticsof 𝜇G,anovelprogramminglanguageforthedefinitionofgraphneural
networks,whereagraphneuralnetworkischaracterizedasahigher-orderfunction𝜙 :𝐻[𝑇1] →𝐻[𝑇2].Weproved
thetypesoundnessof𝜇Gandshownagraphicalformalismfortherepresentationof𝜇Gprograms.Thelanguagewas
implementedasaPythonlibrarybuiltontheTensorFlowframework.Weevaluated𝜇GontheapplicationofCTLmodel
checkingandasameanstospecifysomeofthemostpopulargraphneuralnetworkmodels.
Infutureimplementations,weplantoexpandthelanguageinanumberofways.Atypicalgraphneuralnetwork
operationthatisstilloverlookedin𝜇Gispooling.Apoolingoperatorchangestheunderlyinggraphbyreducingthe
numberofnodes,producingacoarsenedversionofagraph.Graphpoolingoperatorscanbegenerallydescribedin
termsoftheoperationsofselection,reduction,andconnection[16].Theselectionoperatorgroupsnodestogether,the
reductionoperatoraggregatesthenodesineachgroupintoasinglenode,andfinallytheconnectionoperatorgenerates
theedgestolinkthenewlygeneratednodes.Theintroductionofpoolingoperatorsrequirescarefulconsiderationof
howtheycanbecomposedwiththeexistingterms,butitwouldallow𝜇Gtotacklegraph-leveltasksexplicitly.
Themainpurposefordeveloping𝜇Gistobeeventuallyabletoperformformalverificationofgraphneuralnetworks
andgenerateexplanationsfortheiroutputs.Forthetimebeing,therearestillnoproceduresthatallowtheverification
ofoutputreachabilitypropertiesofGNNs[33].Inthefuture,weaimtodefineanabstractinterpretationapproachfor
thesetasks.
REFERENCES
[1] MartínAbadi,AshishAgarwal,PaulBarham,EugeneBrevdo,ZhifengChen,CraigCitro,GregS.Corrado,AndyDavis,JeffreyDean,MatthieuDevin,
SanjayGhemawat,IanGoodfellow,AndrewHarp,GeoffreyIrving,MichaelIsard,YangqingJia,RafalJozefowicz,LukaszKaiser,ManjunathKudlur,
JoshLevenberg,DandelionMané,RajatMonga,SherryMoore,DerekMurray,ChrisOlah,MikeSchuster,JonathonShlens,BenoitSteiner,Ilya
Sutskever,KunalTalwar,PaulTucker,VincentVanhoucke,VijayVasudevan,FernandaViégas,OriolVinyals,PeteWarden,MartinWattenberg,Martin
Wicke,YuanYu,andXiaoqiangZheng.2015.TensorFlow:Large-ScaleMachineLearningonHeterogeneousSystems. https://www.tensorflow.org/
Softwareavailablefromtensorflow.org.
[2] ElvioAmparore,BernardBerthomieu,GianfrancoCiardo,SilvanoDalZilio,FrancescoGallà,LomMessanHillah,FrancisHulin-Hubard,PeterGjøl
Jensen,LoïgJezequel,FabriceKordon,DidierLeBotlan,TorstenLiebke,JeroenMeijer,AndrewMiner,EmmanuelPaviot-Adet,JiříSrba,Yann
Thierry-Mieg,TomvanDijk,andKarstenWolf.2019.Presentationofthe9thEditionoftheModelCheckingContest.InToolsandAlgorithmsforthe
ConstructionandAnalysisofSystems,DirkBeyer,MariekeHuisman,FabriceKordon,andBernhardSteffen(Eds.).SpringerInternationalPublishing,
Cham,50–68. https://doi.org/10.1007/978-3-030-17502-3_4
[3] AnishAthalye,LoganEngstrom,AndrewIlyas,andKevinKwok.2018.SynthesizingRobustAdversarialExamples. arXiv:1707.07397[cs.CV]
[4] EnricoBarbieratoandA.Gatti.2024.TheChallengesofMachineLearning:ACriticalReview.ELECTRONICS13(2024),1–30. https://doi.org/10.
3390/electronics13020416
[5] PeterW.Battaglia,JessicaB.Hamrick,VictorBapst,AlvaroSanchez-Gonzalez,ViniciusZambaldi,MateuszMalinowski,AndreaTacchetti,David
Raposo,AdamSantoro,RyanFaulkner,CaglarGulcehre,FrancisSong,AndrewBallard,JustinGilmer,GeorgeDahl,AshishVaswani,KelseyAllen,
CharlesNash,VictoriaLangston,ChrisDyer,NicolasHeess,DaanWierstra,PushmeetKohli,MattBotvinick,OriolVinyals,YujiaLi,andRazvan
Pascanu.2018.Relationalinductivebiases,deeplearning,andgraphnetworks. https://doi.org/10.48550/arXiv.1806.01261arXiv:1806.01261[cs,stat].
[6] MatteoBelenchia,FlavioCorradini,MichelaQuadrini,andMicheleLoreti.2023. ImplementingaCTLModelCheckerwith𝜇G,aLanguage
forProgrammingGraphNeuralNetworks.InFormalTechniquesforDistributedObjects,Components,andSystems(LectureNotesinComputerScience),
MariekeHuismanandAntónioRavara(Eds.).SpringerNatureSwitzerland,Cham,37–54. https://doi.org/10.1007/978-3-031-35355-0_4
[7] MatteoBelenchia,FlavioCorradini,MichelaQuadrini,andMicheleLoreti.2024.libmg:aPythonLibraryforProgrammingGraphNeuralNetworks
inµG.(2024). AcceptedwithminorrevisionsforpublicationtoScienceofComputerProgramming.
[8] MichaelM.Bronstein,JoanBruna,TacoCohen,andPetarVeličković.2021. GeometricDeepLearning:Grids,Groups,Graphs,Geodesics,and
Gauges. arXiv:2104.13478[cs.LG]
[9] OlavBunte,JanFrisoGroote,JeroenJ.A.Keiren,MauriceLaveaux,ThomasNeele,ErikP.deVink,WiegerWesselink,AntonWijs,andTimA.C.
Willemse.2019.ThemCRL2ToolsetforAnalysingConcurrentSystems.InToolsandAlgorithmsfortheConstructionandAnalysisofSystems,Tomáš
VojnarandLijunZhang(Eds.).SpringerInternationalPublishing,Cham,21–39. https://doi.org/10.1007/978-3-030-17465-1_2
ManuscriptsubmittedtoACMThe𝜇GLanguageforProgrammingGraphNeuralNetworks 31
[10] GabrieleCorso,HannesStark,StefanieJegelka,TommiJaakkola,andReginaBarzilay.2024.Graphneuralnetworks.NatureReviewsMethodsPrimers
4,1(07Mar2024),17. https://doi.org/10.1038/s43586-024-00294-7
[11] VenmugilElango,NormRubin,MaheshRavishankar,HariharanSandanagobalane,andVinodGrover.2018.Diesel:DSLforlinearalgebraand
neuralnetcomputationsonGPUs.InProceedingsofthe2ndACMSIGPLANInternationalWorkshoponMachineLearningandProgrammingLanguages
(MAPL2018).AssociationforComputingMachinery,NewYork,NY,USA,42–51. https://doi.org/10.1145/3211346.3211354
[12] KevinEykholt,IvanEvtimov,EarlenceFernandes,BoLi,AmirRahmati,ChaoweiXiao,AtulPrakash,TadayoshiKohno,andDawnSong.2018.
RobustPhysical-WorldAttacksonDeepLearningModels. arXiv:1707.08945[cs.CR]
[13] VicenteGarcíaDíaz,JordánEspada,B.PelayoGarcía-Bustelo,andJuanCuevaLovelle.2015.TowardsaStandard-basedDomain-specificPlatform
toSolveMachineLearning-basedProblems.InternationalJournalofArtificialIntelligenceandInteractiveMultimedia3(Jan.2015),6–12. https:
//doi.org/10.9781/ijimai.2015.351
[14] JustinGilmer,SamuelS.Schoenholz,PatrickF.Riley,OriolVinyals,andGeorgeE.Dahl.2017.NeuralmessagepassingforQuantumchemistry.In
Proceedingsofthe34thInternationalConferenceonMachineLearning-Volume70(ICML’17).JMLR.org,Sydney,NSW,Australia,1263–1272.
[15] DanieleGrattarolaandCesareAlippi.2020.GraphNeuralNetworksinTensorFlowandKeraswithSpektral.IEEEComput.Intell.Mag.16(2020),
99–106. https://doi.org/10.1109/mci.2020.3039072
[16] DanieleGrattarola,DanieleZambon,FilippoMariaBianchi,andCesareAlippi.2022.UnderstandingPoolinginGraphNeuralNetworks.IEEE
TransactionsonNeuralNetworksandLearningSystems35,2(July2022),2708–2718. https://doi.org/10.1109/TNNLS.2022.3190922ConferenceName:
IEEETransactionsonNeuralNetworksandLearningSystems.
[17] SicongHan,ChenhaoLin,ChaoShen,QianWang,andXiaohongGuan.2023.InterpretingAdversarialExamplesinDeepLearning:AReview.ACM
Comput.Surv.55,14s,Article328(jul2023),38pages. https://doi.org/10.1145/3594869
[18] XiaoweiHuang,DanielKroening,WenjieRuan,JamesSharp,YouchengSun,EmeseThamo,MinWu,andXinpingYi.2020.ASurveyofSafetyand
TrustworthinessofDeepNeuralNetworks:Verification,Testing,AdversarialAttackandDefence,andInterpretability. http://arxiv.org/abs/1812.08342
arXiv:1812.08342[cs].
[19] BenjaminJahić,NicolasGuelfi,andBenoîtRies.2023.SEMKIS-DSL:ADomain-SpecificLanguagetoSupportRequirementsEngineeringofDatasets
andNeuralNetworkRecognition.Information14,4(April2023),213. https://doi.org/10.3390/info14040213Number:4Publisher:Multidisciplinary
DigitalPublishingInstitute.
[20] MichaelKearns.1998.Efficientnoise-tolerantlearningfromstatisticalqueries.J.ACM45,6(Nov.1998),983–1006. https://doi.org/10.1145/293347.
293351
[21] ThomasN.KipfandMaxWelling.2017.Semi-SupervisedClassificationwithGraphConvolutionalNetworks. arXiv:1609.02907[cs.LG]
[22] LuísC.Lamb,Arturd’AvilaGarcez,MarcoGori,MarceloO.R.Prates,PedroH.C.Avelar,andMosheY.Vardi.2020.GraphNeuralNetworksMeet
Neural-SymbolicComputing:ASurveyandPerspective.InProceedingsoftheTwenty-NinthInternationalJointConferenceonArtificialIntelligence,
IJCAI-20,ChristianBessiere(Ed.).InternationalJointConferencesonArtificialIntelligenceOrganization,Yokohama,Yokohama,Japan,4877–4884.
https://doi.org/10.24963/ijcai.2020/679Surveytrack.
[23] GaryMarcus.2018.DeepLearning:ACriticalAppraisal. arXiv:1801.00631 http://arxiv.org/abs/1801.00631
[24] GaryMarcus.2018.Innateness,AlphaZero,andArtificialIntelligence. arXiv:1801.05667[cs.AI]
[25] GaryMarcus.2020.TheNextDecadeinAI:FourStepsTowardsRobustArtificialIntelligence. https://arxiv.org/abs/2002.06177v3arXiv:2002.06177.
[26] ChristopherMorris,MatthiasFey,andNilsKriege.2021.ThePoweroftheWeisfeiler-LemanAlgorithmforMachineLearningwithGraphs.In
ProceedingsoftheThirtiethInternationalJointConferenceonArtificialIntelligence.ijcai.org,Montreal,Canada,4543–4550. https://doi.org/10.24963/
ijcai.2021/618ConferenceName:ThirtiethInternationalJointConferenceonArtificialIntelligence{IJCAI-21}ISBN:9780999241196Place:Montreal,
CanadaPublisher:InternationalJointConferencesonArtificialIntelligenceOrganization.
[27] ArturPodobas,MartinSvedin,StevenW.D.Chien,IvyB.Peng,NareshBalajiRavichandran,PawelHerman,AndersLansner,andStefanoMarkidis.
2021. StreamBrain:AnHPCFrameworkforBrain-likeNeuralNetworksonCPUs,GPUsandFPGAs.InProceedingsofthe11thInternational
SymposiumonHighlyEfficientAcceleratorsandReconfigurableTechnologies(Online,Germany)(HEART’21).AssociationforComputingMachinery,
NewYork,NY,USA,Article8,6pages. https://doi.org/10.1145/3468044.3468052
[28] IvensPortugal,PauloAlencar,andDonaldCowan.2016. APreliminarySurveyonDomain-SpecificLanguagesforMachineLearninginBig
Data.In2016IEEEInternationalConferenceonSoftwareScience,TechnologyandEngineering(SWSTE).IEEE,BeerSheva,Israel,108–110. https:
//doi.org/10.1109/SWSTE.2016.23
[29] IqbalH.Sarker.2021.DeepLearning:AComprehensiveOverviewonTechniques,Taxonomy,ApplicationsandResearchDirections.SNComputer
Science2,6(18Aug2021),420. https://doi.org/10.1007/s42979-021-00815-1
[30] FrancoScarselli,MarcoGori,AhChungTsoi,MarkusHagenbuchner,andGabrieleMonfardini.2009.TheGraphNeuralNetworkModel.IEEE
TransactionsonNeuralNetworks20,1(Jan.2009),61–80. https://doi.org/10.1109/TNN.2008.2005605ConferenceName:IEEETransactionsonNeural
Networks.
[31] ArvindK.Sujeeth,HyoukJoongLee,KevinJ.Brown,HassanChafi,MichaelWu,AnandR.Atreya,KunleOlukotun,TiarkRompf,andMartin
Odersky.2011.OptiML:animplicitlyparalleldomain-specificlanguageformachinelearning.InProceedingsofthe28thInternationalConferenceon
InternationalConferenceonMachineLearning(ICML’11).Omnipress,Madison,WI,USA,609–616.
[32] ZacharySusskind,BryceArden,LizyK.John,PatrickStockton,andEugeneB.John.2021.Neuro-SymbolicAI:AnEmergingClassofAIWorkloads
andtheirCharacterization. arXiv:2109.06133 https://arxiv.org/abs/2109.06133
ManuscriptsubmittedtoACM32 Belenchiaetal.
[33] MarcoSälzerandMartinLange.2022.FundamentalLimitsinFormalVerificationofMessage-PassingNeuralNetworks. https://openreview.net/
forum?id=WlbG820mRH-
[34] PetarVeličković,GuillemCucurull,ArantxaCasanova,AdrianaRomero,PietroLiò,andYoshuaBengio.2018. GraphAttentionNetworks.
arXiv:1710.10903[stat.ML]
[35] BorisYulievichWeisfeilerandAndreyAleksandrovichLeman.1968.TheReductionofaGraphtoCanonicalFormandtheAlgebrawhichAppears
therein.Nauchno-TechnicheskayaInformatsia9(1968),12–16.
[36] KeyuluXu,WeihuaHu,JureLeskovec,andStefanieJegelka.2019.HowPowerfulareGraphNeuralNetworks? arXiv:1810.00826[cs.LG]
[37] Tian Zhao, Xiaobing Huang, and Yu Cao. 2017. DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning.
arXiv:1701.02284[cs.PL]
[38] JieZhou,GanquCui,ShengdingHu,ZhengyanZhang,ChengYang,ZhiyuanLiu,LifengWang,ChangchengLi,andMaosongSun.2020.Graph
neuralnetworks:Areviewofmethodsandapplications.AIOpen1(Jan.2020),57–81. https://doi.org/10.1016/j.aiopen.2021.01.001
Received20February2007;revised12March2009;accepted5June2009
ManuscriptsubmittedtoACM