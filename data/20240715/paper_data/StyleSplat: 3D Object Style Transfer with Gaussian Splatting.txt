StyleSplat: 3D Object Style Transfer with
Gaussian Splatting
Sahil Jain⋆, Avik Kuthiala⋆, Prabhdeep Singh Sethi, and Prakanshul Saxena
Carnegie Mellon University
Abstract. Recentadvancementsinradiancefieldshaveopenednewav-
enues for creating high-quality 3D assets and scenes. Style transfer can
enhance these 3D assets with diverse artistic styles, transforming cre-
ative expression. However, existing techniques are often slow or unable
to localize style transfer to specific objects. We introduce StyleSplat,
a lightweight method for stylizing 3D objects in scenes represented by
3D Gaussians from reference style images. Our approach first learns a
photorealistic representation of the scene using 3D Gaussian splatting
while jointly segmenting individual 3D objects. We then use a nearest-
neighborfeaturematchinglosstofinetunetheGaussiansoftheselected
objects, aligning their spherical harmonic coefficients with the style im-
agetoensureconsistencyandvisualappeal.StyleSplatallowsforquick,
customizable style transfer and localized stylization of multiple objects
withinascene,eachwithadifferentstyle.Wedemonstrateitseffective-
ness across various 3D scenes and styles, showcasing enhanced control
and customization in 3D creation.
Keywords: 3D Gaussian Splatting · Style Transfer
1 Introduction
Breakthroughs in radiance field generation have revolutionized the capture and
representation of 3D scenes, allowing for unprecedented levels of detail and re-
alism. The ability to seamlessly transfer artistic styles to objects extracted from
theseradiancefieldsoffersatransformativeapproachforindustriessuchasgam-
ing, virtual reality, and digital art. This technology not only enhances creative
expression but also significantly reduces the time and effort required to produce
visuallystunning3Dassets,pushingtheboundariesofwhatispossibleindigital
content creation.
Theemergenceof3DGaussiansplatting(3DGS)[14]hasintroducedapower-
fulmethodforrepresentingradiancefields,offeringtheadvantageoffasttraining
and rendering while preserving high-quality details. Prior to this advancement,
neural radiance field (NeRF) based methods [2,25,26] have been extensively
utilized for reconstructing detailed scenes, and many techniques [19,21,35] aim
at transferring artistic styles to individual objects within NeRFs. However, the
slowtrainingandrenderingspeedsofNeRFsposesignificantchallengesfortheir
practical use.
⋆ equal contribution
4202
luJ
21
]VC.sc[
1v37490.7042:viXra2 S. Jain et al.
Fig.1: WeintroduceStyleSplat,anapproachforlightweight,customizable,andlocal-
ized stylization of 3D objects from reference style images. Our approach first learns
a photorealistic representation of the scene with 3D Gaussian splatting while jointly
segmenting the scene into individual 3D objects using 2D masks. We then employ a
nearest-neighborfeaturematchinglosstofinetuneandstylizetheuser-specifiedobjects
using the provided style images.
Approaches have also been introduced in the 3DGS paradigm [23,29] that
enable the transfer of style in real time. However, they apply the style glob-
ally to entire scenes without providing mechanisms for localized application to
individual objects. Furthermore, although text-based approaches [31] can fa-
cilitate object-specific style transfer through text instructions using an image-
conditioned diffusion model, the reliance on textual descriptions introduces am-
biguities, particularly in accurately conveying specific colors, styles, or textures
within 3D scenes.
Inthispaper,weintroduceStyleSplat,alightweightmethodforstylizing3D
objects in scenes represented using 3D Gaussians from arbitrary style images.
Our approach allows for the stylization of multiple objects within a scene, each
with a different style image. StyleSplat consists of three steps: 2D mask gener-
ation and tracking, 3D Gaussian training & segmentation and finally, 3D style
transfer. In the first step, we utilize off-the-shelf image segmentation and track-
ing models to obtain consistent 2D masks across the complete scene to generate
temporallycoherentmaskidentifiers.Then,thesemasksareusedassupervision
for segmenting the 3D Gaussians into distinct objects while jointly optimizing
their geometry and color, allowing for accurate object selection. Finally, we use
a nearest-neighbor feature matching loss to finetune the selected Gaussians by
aligning their spherical harmonic coefficients with the provided style image to
achieve consistency and visual appeal. This method provides accurate and fo-
cused stylization, resulting in more customized and high-quality outcomes.StyleSplat: 3D Object Style Transfer with Gaussian Splatting 3
Our method produces visually pleasing results across diverse scenes and
datasets, highlighting its versatility with different artistic styles.
2 Related Work
Styletransferon3Dsceneshasattractedattentioninrecentyears,withnotable
contributionsemergingforbothNeRFand3DGSrepresentations.Recently,sev-
eral techniques have also been introduced for object editing within the 3DGS
framework. Here, we review these prior efforts to provide context for our pro-
posed approach.
2.1 Style Transfer
Image style transfer, a long-standing challenge in computer vision, involves op-
timizingacontentimageandareferencestyleimagetocreateanewimage.The
resulting image maintains the content of the original while adopting the style of
the reference. Early works [7,8] utilize VGG-Net [30] to extract multi-level fea-
tures anditerativelyoptimize a Gram matrixloss and a content loss torender a
new image. Since then, numerous works have been proposed to improve various
aspects of style transfer, including faster stylization [10,13,32] and improving
semanticconsistencyandtexturepreservation[9,17,22].Severalapproacheshave
beenproposedtoconstrainthestyletransfertospecificobjectsintheimage.[4]
simultaneously segments and stylizes individual objects in an image and uses
a Markov random field for anti-aliasing near the object boundaries for seam-
less blending. [18] performs localized style transfer in real-time by generating
object masks using a fast semantic segmentation deep neural network. [27] uti-
lizes Segment Anything (SAM) [16] to generate object masks and contain the
style transfer to specific objects and uses a novel loss function to ensure smooth
boundaries between the stylized and non-stylized parts of the image.
2.2 Style Transfer in Radiance Fields
Severalworkshavebeenintroducedtoextendstylizationto3Dscenes.ARF[37]
presents a method for transferring artistic styles to 3D scenes represented by
neural radiance fields. Their approach effectively incorporates artistic features
intorenderedimageswhilemaintainingmulti-viewconsistencybyusinganearest
neighbor feature matching (NNFM) loss for 3D stylization instead of the Gram
matrixloss.However,ARFappliesstylechangestotheentirescene,modifyingall
objectswithintheview,whichmightnotbedesirableinapplicationswherestyle
transfer is intended for specific objects. Building upon ARF, several approaches
wereintroducedtolocalizestyletransfertouser-specifiedobjects.ARF-Plus[21]
introduces perceptual controls for scale, spatial area, and perceived depth and
uses semantic segmentation masks to ensure that stylization is applied only
to selected areas. S2RF [19] leverages Plenoxel’s [28] grid based representation
for the 3D scene and a masked NNFM (mNNFM) loss to constrain the style4 S. Jain et al.
transfer only to desired areas. CoARF [35] extends S2RF by adding a semantic
componenttostyletransferusingLSeg[20]featuresinadditiontoVGGfeatures.
However,thesemethodssufferfromslowrenderingspeedsduetotheirrelianceon
raymarching.Inthe3DGSparadigm,StyleGaussian[23]andGSS[29]introduce
approaches for scene stylization. Both methods seek to generate novel views of
a scene using unseen style images at test time, after being trained on a large
dataset of style images. Similar to ARF [37], these approaches are limited to
stylizing the entire scene.
2.3 Object Editing in 3D Gaussian Splatting
AcommonapproachforlocalizededitinginGaussiansplattinginvolvesappend-
ing additional features to each Gaussian to encode semantic information. These
features are optimized by rendering feature maps similar to RGB rasterization
and using 2D feature maps or segmentation masks from foundation models as
guidance. GaussianEditor [5] adds a scalar feature to each Gaussian to identify
whether the Gaussian is in the editing region of interest (ROI). It then uses 2D
grounded segmentation masks to optimize this feature. Gaussian Grouping [33]
appends a feature vector to each Gaussian and uses masks from SAM [16] and
DEVA [6] as guidance. Similarly, Feature 3DGS [38] distills LSeg [20] and SAM
featuresintoeachGaussianforpromptableediting.Alltheseapproachesdemon-
strate text-guided editing as a downstream task. However, textual descriptions
in style transfer can be ambiguous, making it difficult to accurately specify par-
ticularcolors,styles,ortextureswithin3Dscenes.Severaldiffusion-basedmeth-
ods have also been proposed. Instruct-GS2GS [31] utilizes a 2D diffusion model
to modify the appearance of 3D objects with text instructions but it fails to
constrain the changes to the specified object. TIP-Editor [39] personalizes a dif-
fusion model using LoRA and accepts both an image and a text prompt with a
3Dboundingboxforlocalediting.Fine-tuningtypicallyrequires5to25minutes
with this approach. By contrast, our method is lightweight and achieves results
in less than a minute.
2.4 Concurrent Work
Severalinterestingworkshaveemergedthatfocusonlocalizedimage-conditioned
editing of 3D Gaussians. One such work is StylizedGS [36], which emphasizes
scene stylization while allowing for spatial control through the use of 2D masks,
allowing different styles for different regions. However, since the approach uses
only2Dmasks,itfacessignificantlimitations.SinceeachGaussianhasavolume,
alphablendingmayinadvertentlyincludeneighboringGaussiansinthecomputa-
tion graph during rasterization, causing unintended style transfer to other parts
ofthescene.Thisresultsintheinabilitytopreciselystylizeasingleobjectwhile
leaving the rest of the scene unchanged. Another notable work is ICE-G [11].
Unlike our method, ICE-G copies the style image to the ROI for a single 2D
view. It then employs SAM and DINO [3] to propagate the style to multipleStyleSplat: 3D Object Style Transfer with Gaussian Splatting 5
views, effectively updating the data set with the desired style. Fine-tuning for
style transfer is subsequently performed on this updated dataset.
3 Method
Given a reference style image, a set of posed images, and objects specified by
the user, we aim to achieve fast novel view synthesis such that the objects cor-
responding to the user input are stylized according to the reference style image.
Our approach involves three steps: 2D mask generation & object tracking, 3D
Gaussian training & segmentation, and 3D style transfer. Fig. 2 provides a brief
overview of our method.
Fig.2: Our approach for StyleSplat. We first use an off-the-shelf segmentation and
tracking model [6] to generate view-consistent 2D object masks. Then, we use the
multi-view images to learn the geometry and color of 3D Gaussians while simultane-
ously learning a per Gaussian feature vector. These feature vectors are decoded into
objectlabelsusingalinearclassifiertocollecttheGaussianscorrespondingtotheuser-
specifiedobjects.TheSHcoefficientsoftheseselectedGaussiansarefinetunedtoalign
with the style image using NNFM loss.
3.1 Preliminary: 3D Gaussian Splatting
3D Gaussians [14] is an explicit 3D scene representation. Each 3D Gaussian is
characterized by a covariance matrix Σ and a center point µ , which is referred
to as the mean value of the Gaussian as:
G(x)=e− 21xTΣ−1x (1)6 S. Jain et al.
For differentiable optimization, the covariance matrix Σ can be decomposed
into a scaling matrix S and a rotation matrix:
Σ =RSSTRT (2)
When rendering novel views, differential splatting is employed for the 3D Gaus-
sians within the camera planes. As introduced by [40], using a viewing trans-
form matrix W and the Jacobian matrix J of the affine approximation of the
projective transformation, the covariance matrix Σ′ in camera coordinates can
be computed as:
Σ′ =JWΣWTJT (3)
In summary, each 3D Gaussian is characterized by the following attributes:
position X ∈ R3, color defined by spherical harmonic (SH) coefficients C ∈ Rk
(where k represents number of SH coefficents), opacity α ∈ R, rotation factor
r ∈ R4, and scaling factor s ∈ R3. Specifically, for each pixel, the color and
opacity of all the Gaussians are computed using the Gaussian’s representation
Eq. 1. The blending of N ordered points that overlap the pixel is given by the
formula:
i−1
(cid:89)
C =Σc α (1−α ) (4)
i i i
j=1
Here, c ,α represents the density and color of this point computed by a 3D
i i
Gaussian G with covariance Σ multiplied by an optimizable per-point opacity
and SH color coefficients.
3.2 2D Mask Generation & Object Tracking
Before segmenting the 3D scene, we need to acquire accurate 2D segmentation
masksontheentiresequence.Thesemasksneedtobetemporallycoherenttoen-
surethatthedifferentclassindicescorrespondtothesameobjectacrossframes.
WetreatthecapturedimagesasavideosequenceandmakeuseofaDEcoupled
VideosegmentationApproach(DEVA)[6]withastrongzero-shotsegmentation
model (SAM) [16] to get temporally coherent masks.
3.3 3D Gaussian Training & Segmentation
We follow the approach of Gaussian Grouping [33] to jointly train and segment
3D Gaussians. More specifically, each 3D Gaussian is given a learnable compact
featurevectoroflength16.Theseencodingsareoptimizedsimilartothespherical
harmonic coefficients in the 3DGS pipeline. For a given view, the feature vector
for a single pixel is evaluated as follows:
i−1
(cid:88) (cid:89)
E = e α′ (1−α′)
id i i j
i∈N j=iStyleSplat: 3D Object Style Transfer with Gaussian Splatting 7
where e′ is the feature vector for the ith Gaussian, and α′ is the influence of
i i
the ith Gaussian on the current pixel, evaluated similar to [34]. The rendered
E for each pixel is then passed through a classifier to provide a class label. A
id
cross-entropy loss is used between the predicted class labels vs the class labels
obtainedinthefirststage.Additionally,aspatialconsistencylossisaddedwhich
ensures that the feature vectors for the top-k nearest 3D Gaussians are similar.
Once this stage is completed, all Gaussians in the scene corresponding to the
same object have similar feature vectors.
3.4 3D Style Transfer
Once we have obtained a scene representation with segmented 3D Gaussians,
we use the learned feature vectors to perform style transfer on user-specified
objects.TogetthemaskIDsofspecificobjectsinthescene,theusercanspecify
aboundingboxaroundtheobjectorutilizeGroundingDINO[24]toextractthe
IDsusingatextprompt.WeselecttheGaussianscorrespondingtotheobjectsof
interestbypassingthefeaturevectorsthroughthetrainedclassifierandfiltering
outGaussianswithactivationslessthanathreshold.Wealsoperformstatistical
outlierremoval,eliminatingGaussianswhosepositionsdeviatesignificantlyfrom
their neighbors compared to the average for the scene. We then freeze all the
propertiesoftheselectedGaussiansandenablegradientsonlyfortheirSHcoef-
ficients. For each training view, we apply the nearest neighbor feature matching
(NNFM)lossbetweentheVGGfeaturesoftherenderedimageandthereference
style image. The NNFM loss minimizes the cosine distance between the VGG
feature of each pixel in the render with its nearest neighbor in the style image
and is given by:
1 (cid:88)
L = min(F (i)·F (j))
NNFM N j r s
i
where F are the VGG features for the render, F are the features for the style
r s
image, and N is the number of pixels in the rendered image. Since only the
SH coefficients of the user-specified object are trainable, the style transfer is
contained to a single object.
4 Results
Toassessourmethod,weperformqualitativeevaluationsonmultiplereal-world
scenes. We visually demonstrate the effectiveness of our method by successfully
applying different styles to various objects in a diverse selection of scenes. This
section highlights how our 3D segmentation approach for localized stylization
prevents leakages (Sec. 4.2), its performance in both single-object (Sec. 4.3)
and multi-object settings (Sec. 4.4), and scale control (Sec. 4.5). Finally, we
qualitatively compare our approach with S2RF (Sec. 4.6).8 S. Jain et al.
(a) GroundTruth (b) 2DSegmentation (c) 3DSegmentation
Fig.3:Effectof3Dsegmentationonlocalizedstyletransfer.Thefirstcolumnshowsthe
initial 3D object. The second column demonstrates the limitations of using a masked
loss similar to previous radiance field-based approaches [19,21,35]. 2D masks can be
inconsistentacrossviewsandintroduceerrors,leadingtoartifactsindifferentpartsof
the scene due to incorrect Gaussians being modified. The third column illustrates the
benefits of training with a collection of noisy masks to learn a view-consistent feature
vector per Gaussian, effectively correcting these errors and avoiding leakage.
4.1 Implementation Details
We evaluate our approach on a range of real-world scenes from diverse datasets,
including NeRF [25], MipNerf360 [2], LERF [15], and InstantNGP [26]. These
datasetsprovidevariouschallengingenvironmentstotesttheeffectivenessofour
method. Additionally, we use style images from the WikiArt dataset [1], which
offers a wide variety of artistic styles, demonstrating the versatility of our style
transfer technique.
For all scenes, we begin by running the 3D Gaussian and segmentation
pipeline for an initial 30,000 iterations. Following this, we freeze the parame-
ters of all the Gaussians, restricting further optimization to only the spherical
harmonic (SH) coefficients of the Gaussians that correspond to the selected ob-
ject.Thestyletransferoptimizationisthenperformedusing25%ofthetraining
images, running for 500 to 1,000 iterations depending on the complexity of the
scene. This targeted optimization process is highly efficient, taking less than a
minute to complete on a single NVIDIA A100 GPU.
4.2 Object selection
Previous radiance field based approaches [19,21,35] use a 2D masked loss in the
imagespacetolocalizethestyletransfer.However,2Dmaskscanbeinconsistent
across views and contain errors, leading to incorrect Gaussians being stylized
(as demonstrated in Fig. 3). Although this is not a problem in neural sceneStyleSplat: 3D Object Style Transfer with Gaussian Splatting 9
(a)GroundTruth (b) DEVAMasks (c) Features (d) Object (e) Stylized
Fig.4:3Dsegmentationresults.Figure(a)showsthegroundtruthimage,(b)displays
the masks extracted using SAM and DEVA, (c) visualizes the learned feature vectors
of all objects in the scene, (d) presents the extracted object, and (e) illustrates the
final stylized result.
representations, this manifests as artifacts in the final stylized scene for 3DGS.
Using a 3D segmentation approach leads to robustness against 2D mask errors.
The results of our object selection approach are illustrated in Fig. 4. The
masks provided by DEVA are shown in Fig. 4b and Fig. 4c visualizes the learnt
per-Gaussian feature vectors for the bear and pinecone scenes from the Instant-
NGP and NeRF datasets respectively. The feature vectors are visualized as the
first three principal component analysis (PCA) components of the original 16-
dimensional vectors. We can observe that the approach provides an effective
wayto select3D objectsin the scene,confiningthe style transferto the selected
object.10 S. Jain et al.
Ground Truth Venetian Canal South Ledges Lizard Story
Fig.5: Shows single object style transfer on the bear and pinecone scenes with style
imagesofdifferentartisticstylesandcomposition.Ourapproachlocalizesstyletransfer
to the selected objects, without affecting the background.
4.3 Single object style transfer
In this case, we focus on stylizing a single object from the scene. Fig. 5 demon-
strates that our method effectively confines the style transfer to the selected
object, stylizing it according to the provided artwork. In this figure, we use
three style images: Venetian Canal, South Ledges, and Lizard Story. We show-
case our results on two object-centric scenes - bear, and pinecone. The results
demonstrate precise style transfer. For example, in the bear scene, the geome-
try and texture (curvature and shadows) are faithfully preserved. The adaptive
nature of our style transfer, facilitated by the NNFM loss function, is evident
in the pinecone scene, where dark regions adopt a blue hue reminiscent of the
Venetian Canal, while bright areas take on an orange tone.
4.4 Multi-object style transfer
Ourmethodextendstostylizingmultipleobjectswithinascene,whereweselect
two distinct objects and apply different style images to each. Fig. 6 illustrates
the results for these scenes, showcasing both single-object and multi-object ap-
plicationsofourstyletransfermethodacrossscenesfeaturingnumerousobjects.
The first half of the image shows the counter scene from the MipNerf [2]
dataset. We stylize the ‘mitten’ and the ‘flower pot’ in this scene. From three
differentviewpoints,itisevidentthatourstylizationapproachishighlycontrol-
lable, allowing us to accurately select and stylize objects while preserving their
geometry.StyleSplat: 3D Object Style Transfer with Gaussian Splatting 11
Starry Night Venetian Canal Starry + Venetian
Ground Truth Mitten Stylized Flower Pot Stylized Both Stylized
Ground Truth Apple Stylized Tabletop Stylized Both Stylized
Fig.6: Multi-object style transfer. The style images above are applied to selected
objects highlighted in the two scenes (counter and figurines). In columns 2 & 3 we
show stylizing of individual objects and in column 4 we see both the stylized objects
together.
In the second half of the figure, we stylize the ‘green apple’ and the entire
‘tabletop’ in the figurines scene from the LERF [15] dataset. Our approach is12 S. Jain et al.
Ground Truth Style Image
Layers: [1, 3] Layers: [6, 8] Layers: [11, 13, 15]
Scale: 0.25 Scale: 0.5 Scale: 0.75
LR: 0.025 LR: 0.05 LR: 0.075
Fig.7: Visualisation of scale and intensity control. The scale of the style pattern can
becontrolledbychangingthesizeofthestyleimageorchangingthelayersforfeature
extraction. Intensity can be controlled by varying the learning rate.
effective at stylizing the ‘tabletop’ even though it is not in the foreground and
is occluded by various objects. The stylization occurs seamlessly, making the
stylized ‘tabletop’ appear as if the style is an inherent property of the surface.
4.5 Scale Control
Previous works [12,21] have shown that the scale of the style patterns can be
controlled by two parameters - the receptive field of the VGG features and the
sizeofthestyleimage.Thereceptivefieldcanbecontrolledbyvaryingthelayers
of the VGG network which are used to extract features for the NNFM loss. The
sizeofthestyleimagecanbecontrolledbyadownscalingfactor(scale=1means
that the style image is used in its original resolution).
In Fig. 7, we present experimental results exploring these scale parameters
and their impact on stylization. We can observe that as the scale of the style
imageisdecreased,therepetitionofthepatternincreases.Asimilareffectcanbe
observed with the network layer selection. The features from the deeper layersStyleSplat: 3D Object Style Transfer with Gaussian Splatting 13
Fig.8: Qualitative comparison of our method with S2RF on the flower scene.
have larger receptive fields, giving an effect similar to downscaling the style
image. It is interesting to note that if we use layers [1,3] which are somewhat
early in the network, the features extracted from those layers for style transfer
are not able to accurately learn patterns in the style image.
Additionally, we show that the intensity of style transfer can be controlled
by the learning rate, with a larger rate giving stronger style transfer.
4.6 Qualitative Comparison
We evaluate our approach in comparison to S2RF [19], which also focuses on
localized stylization of specific objects rather than entire 3D scenes. However,
S2RFintroducesdiscolorationinnon-targetareas,likelyduetoitsuseofagrid-
basedrepresentationwhichsharesparametersacrosslocalregions.Thisparame-
tersharingcaninadvertentlyaffectneighboringregions,whichisundesirablefor
this task. In contrast, our method utilizes segmented 3D Gaussians to precisely
isolate style transfer to the target object, leaving other Gaussians unaffected.
This distinction is evident in the first two rows in Fig. 8, which illustrates the
differences between the two approaches when transferring style on two specific
flowers. Our method also performs better in preserving content of the ground
truth images. In the last two rows, we observe that in the stylized image gener-
ated by S2RF, the shape of the individual flowers is difficult to discern, whereas
our approach achieves a more realistic style transfer result.14 S. Jain et al.
Additionally, S2RF achieves an average rendering speed of 15 FPS whereas
ourapproachachieves100+FPSduetoitsrelianceonthe3DGSrepresentation.
5 Limitations
While our method provides efficient and controllable stylization of 3D Gaus-
sian splats, it has certain limitations. Firstly, geometric artifacts arising from
the initial 3DGS reconstruction process can occasionally affect the quality of
the final stylized scenes. Additionally, the use of the Segment Anything Model
for view-specific segmentation can sometimes struggle with generating detailed
masksfromparticularangles,leadingtotheunintendedmergingofobjectparts.
Consequently, edits intended for specific areas might inadvertently impact ad-
jacent regions. Although these issues are rare in most scenarios, it can occur in
complex scenes or areas with poorly defined boundaries.
6 Conclusion
In this work, we introduce StyleSplat, a lightweight technique to stylize 3D ob-
jectsusingareferencestyleimageinthe3DGaussianSplattingparadigm.Style-
Splat leverages off-the-shelf image segmentation and tracking models to obtain
consistent 2D masks. These masks are then used as supervision for segment-
ing 3D Gaussians into distinct objects while jointly optimizing their geometry
and color. Finally, a nearest-neighbor feature matching loss is used to finetune
the selected Gaussians by aligning their spherical harmonic coefficients with the
provided style image. After the initial training and segmentation, StyleSplat
takes less than a minute to perform stylization, allowing fast experimentation.
We showcase the effectiveness of our method on a variety of scenes and styles,
highlighting its suitability for artistic endeavors.
References
1. WikiArt.org - Visual Art Encyclopedia — wikiart.org. https://www.wikiart.
org/, [Accessed 30-06-2024]
2. Barron, J.T., Mildenhall, B., Verbin, D., Srinivasan, P.P., Hedman, P.: Mip-
nerf 360: Unbounded anti-aliased neural radiance fields. CoRR abs/2111.12077
(2021), https://arxiv.org/abs/2111.12077
3. Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., Joulin,
A.: Emerging properties in self-supervised vision transformers (2021), https://
arxiv.org/abs/2104.14294
4. Castillo, C., De, S., Han, X., Singh, B., Yadav, A.K., Goldstein, T.: Son of zorn’s
lemma:Targetedstyletransferusinginstance-awaresemanticsegmentation(2017),
https://arxiv.org/abs/1701.02357
5. Chen, Y., Chen, Z., Zhang, C., Wang, F., Yang, X., Wang, Y., Cai, Z., Yang, L.,
Liu, H., Lin, G.: Gaussianeditor: Swift and controllable 3d editing with gaussian
splatting. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR). pp. 21476–21485 (June 2024)StyleSplat: 3D Object Style Transfer with Gaussian Splatting 15
6. Cheng,H.K.,Oh,S.W.,Price,B.,Schwing,A.,Lee,J.Y.:Trackinganythingwith
decoupled video segmentation (2023)
7. Gatys, L.A., Ecker, A.S., Bethge, M.: A neural algorithm of artistic style (2015),
https://arxiv.org/abs/1508.06576
8. Gatys,L.A.,Ecker,A.S.,Bethge,M.:Imagestyletransferusingconvolutionalneu-
ralnetworks.2016IEEEConferenceonComputerVisionandPatternRecognition
(CVPR) pp. 2414–2423 (2016), https://api.semanticscholar.org/CorpusID:
206593710
9. Gu, S., Chen, C., Liao, J., Yuan, L.: Arbitrary style transfer with deep feature
reshuffle (2018), https://arxiv.org/abs/1805.04103
10. Huang,X.,Belongie,S.:Arbitrarystyletransferinreal-timewithadaptiveinstance
normalization (2017), https://arxiv.org/abs/1703.06868
11. Jaganathan, V., Huang, H.H., Irshad, M.Z., Jampani, V., Raj, A., Kira, Z.: Ice-g:
Image conditional editing of 3d gaussian splats (2024)
12. Jing,Y.,Liu,Y.,Yang,Y.,Feng,Z.,Yu,Y.,Tao,D.,Song,M.:Strokecontrollable
fast style transfer with adaptive receptive fields (2018), https://arxiv.org/abs/
1802.07101
13. Johnson,J.,Alahi,A.,Fei-Fei,L.:Perceptuallossesforreal-timestyletransferand
super-resolution (2016), https://arxiv.org/abs/1603.08155
14. Kerbl, B., Kopanas, G., Leimkuehler, T., Drettakis, G.: 3d gaussian splatting for
real-timeradiancefieldrendering.ACMTransactionsonGraphics(TOG)42,1–
14 (2023), https://api.semanticscholar.org/CorpusID:259267917
15. Kerr, J., Kim, C.M., Goldberg, K., Kanazawa, A., Tancik, M.: Lerf: Language
embedded radiance fields (2023), https://arxiv.org/abs/2303.09553
16. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,
Whitehead, S., Berg, A.C., Lo, W.Y., Dollár, P., Girshick, R.: Segment anything
(2023)
17. Kolkin,N.,Salavon,J.,Shakhnarovich,G.:Styletransferbyrelaxedoptimaltrans-
port and self-similarity (2019), https://arxiv.org/abs/1904.12785
18. Kurzman,L.,Vazquez,D.,Laradji,I.:Class-basedstyling:Real-timelocalizedstyle
transferwithsemanticsegmentation(2019),https://arxiv.org/abs/1908.11525
19. Lahiri,D.,Panse,N.,Kumar,M.:S2rf:Semanticallystylizedradiancefields(2023)
20. Li, B., Weinberger, K.Q., Belongie, S., Koltun, V., Ranftl, R.: Language-driven
semantic segmentation (2022), https://arxiv.org/abs/2201.03546
21. Li,W.,Wu,T.,Zhong,F.,Oztireli,C.:Arf-plus:Controllingperceptualfactorsin
artistic radiance fields for 3d scene stylization (2023), https://arxiv.org/abs/
2308.12452
22. Liao,J.,Yao,Y.,Yuan,L.,Hua,G.,Kang,S.B.:Visualattributetransferthrough
deep image analogy (2017), https://arxiv.org/abs/1705.01088
23. Liu, K., Zhan, F., Xu, M., Theobalt, C., Shao, L., Lu, S.: Stylegaussian: Instant
3d style transfer with gaussian splatting (2024)
24. Liu,S.,Zeng,Z.,Ren,T.,Li,F.,Zhang,H.,Yang,J.,Li,C.,Yang,J.,Su,H.,Zhu,
J., et al.: Grounding dino: Marrying dino with grounded pre-training for open-set
object detection. arXiv preprint arXiv:2303.05499 (2023)
25. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,
R.: Nerf: Representing scenes as neural radiance fields for view synthesis (2020),
https://arxiv.org/abs/2003.08934
26. Müller, T., Evans, A., Schied, C., Keller, A.: Instant neural graphics primitives
with a multiresolution hash encoding. ACM Trans. Graph. 41(4), 102:1–102:15
(Jul 2022). https://doi.org/10.1145/3528223.3530127, https://doi.org/10.
1145/3528223.353012716 S. Jain et al.
27. Psychogios, K., Helen, L., Melissari, F., Bourou, S., Anastasakis, Z., Zahariadis,
T.: Samstyler: Enhancing visual creativity with neural style transfer and segment
anything model (sam). IEEE Access PP, 1–1 (01 2023). https://doi.org/10.
1109/ACCESS.2023.3315235
28. SaraFridovich-KeilandAlexYu,Tancik,M.,Chen,Q.,Recht,B.,Kanazawa,A.:
Plenoxels: Radiance fields without neural networks. In: CVPR (2022)
29. Saroha, A., Gladkova, M., Curreli, C., Yenamandra, T., Cremers, D.: Gaussian
splatting in style. arXiv preprint arXiv:2403.08498 (2024)
30. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition (2015), https://arxiv.org/abs/1409.1556
31. Vachha,C.,Haque,A.:Instruct-gs2gs:Editing3dgaussiansplatswithinstructions
(2024), https://instruct-gs2gs.github.io/
32. Wu, X., Hu, Z., Sheng, L., Xu, D.: Styleformer: Real-time arbitrary style transfer
viaparametricstylecomposition.In:2021IEEE/CVFInternationalConferenceon
Computer Vision (ICCV). pp. 14598–14607 (2021). https://doi.org/10.1109/
ICCV48922.2021.01435
33. Ye, M., Danelljan, M., Yu, F., Ke, L.: Gaussian grouping: Segment and edit any-
thing in 3d scenes (2023)
34. Yifan, W., Serena, F., Wu, S., Öztireli, C., Sorkine-Hornung, O.: Differentiable
surface splatting for point-based geometry processing. ACM Transactions on
Graphics 38(6), 1–14 (Nov 2019). https://doi.org/10.1145/3355089.3356513,
http://dx.doi.org/10.1145/3355089.3356513
35. Zhang, D., Fernandez-Labrador, C., Schroers, C.: Coarf: Controllable 3d artistic
style transfer for radiance fields (2024), https://arxiv.org/abs/2404.14967
36. Zhang,D.,Chen,Z.,Yuan,Y.J.,Zhang,F.L.,He,Z.,Shan,S.,Gao,L.:Stylizedgs:
Controllablestylizationfor3dgaussiansplatting.arXivpreprintarXiv:2404.05220
(2024)
37. Zhang, K., Kolkin, N., Bi, S., Luan, F., Xu, Z., Shechtman, E., Snavely, N.: Arf:
Artistic radiance fields (2022)
38. Zhou,S.,Chang,H.,Jiang,S.,Fan,Z.,Zhu,Z.,Xu,D.,Chari,P.,You,S.,Wang,Z.,
Kadambi,A.:Feature3dgs:Supercharging3dgaussiansplattingtoenabledistilled
feature fields. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR). pp. 21676–21685 (June 2024)
39. Zhuang,J.,Kang,D.,Cao,Y.P.,Li,G.,Lin,L.,Shan,Y.:Tip-editor:Anaccurate
3deditorfollowingbothtext-promptsandimage-prompts(2024),https://arxiv.
org/abs/2401.14828
40. Zwicker, M., Pfister, H., Baar, J., Gross, M.: Surface splatting. Proceedings of
theACMSIGGRAPHConferenceonComputerGraphics2001(082001).https:
//doi.org/10.1145/383259.383300