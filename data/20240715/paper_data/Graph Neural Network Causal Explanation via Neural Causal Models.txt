Graph Neural Network Causal Explanation
via Neural Causal Models
Arman Behnam1 and Binghui Wang1
Illinois Institute of Technology, Chicago IL 60616, USA
abehnam@hawk.iit.edu bwang70@iit.edu
Abstract. Graphneuralnetwork(GNN)explainersidentifytheimpor-
tant subgraph that ensures the prediction for a given graph. Until now,
almost all GNN explainers are based on association, which is prone to
spuriouscorrelations.WeproposeCXGNN,aGNNcausalexplainervia
causalinference.Ourexplainerisbasedontheobservationthatagraph
often consists of a causal underlying subgraph. CXGNN includes three
mainsteps:1)Itbuildscausalstructureandthecorrespondingstructural
causalmodel(SCM)foragraph,whichenablesthecause-effectcalcula-
tion among nodes. 2) Directly calculating the cause-effect in real-world
graphs is computationally challenging. It is then enlightened by the re-
centneuralcausalmodel(NCM),aspecialtypeofSCMthatistrainable,
anddesigncustomizedNCMsforGNNs.BytrainingtheseGNNNCMs,
thecause-effectcanbeeasilycalculated.3)Ituncoversthesubgraphthat
causally explains the GNN predictions via the optimized GNN-NCMs.
Evaluation results on multiple synthetic and real-world graphs validate
thatCXGNNsignificantlyoutperformsexistingGNNexplainersinexact
groundtruth explanation identification1.
Keywords: Graph neural network explanation · Neural causal model
1 Introduction
Graph is a pervasive data type that represents complex relationships among
entities. Graph Neural Networks (GNNs) [6,13,15,44], a mainstream learning
paradigm for processing graph data, take a graph as input and learn to model
the relation between nodes in the graph. GNNs have demonstrated state-of-the-
art performance across various graph-related tasks such as node classification,
link prediction, and graph classification, to name a few [42].
Explainable GNN provides a human-understandable way of the prediction
outputted by a GNN. Given a graph and a label (correctly predicted by a GNN
model), a GNN explainer aims to determine the important subgraph (called
explanatorysubgraph)thatisabletopredictthelabel.VariousGNNexplanation
methods[3,5,8,9,14,18,21,26–28,30,33,39,40,46–50]havebeenproposed,wherein
almost all of them are based on associating the prediction with a subgraph that
1 Code is available at https://github.com/ArmanBehnam/CXGNN
4202
luJ
21
]GL.sc[
1v87390.7042:viXra2 Arman Behnam and Binghui Wang
has the maximum predictability (more details see Section 2). However, recent
studies [7,32,41] show that association-based explanation methods are prone
to biased subgraphs as the valid explanation due to spurious correlations in
the training data. For instance, when the groundtruth explanatory subgraph is
the House-motif, it often occurs with the Tree bases. Then a GNN may not
learn the true relation between the label and the House-motif, but the Tree
base, due to it being easier to learn. We argue a truly explainable GNN should
uncover the intrinsic causal relation between the explanatory subgraph and the
label, which we call the causal explanation [4,10,29]. Note that a few GNN
explanation methods [19,20,32] are motivated by the causality concepts, e.g.,
Granger causality [11]2, but they are not causal explanations in essence.
OurGNNcausalexplainer:Inthispaper,wetakethefirststeptopropose
a GNN explainer via causal inference [24], which focuses on understanding and
quantifyingcause-and-effectrelationsbetweenvariablesinthetaskofinterest.In
the context of GNN causal explanation, we base on a common observation that
a graph often consists of a causal subgraph and a non-causal counterpart [7,19,
20,32,38,41].Thengivenagraphandits(predicted)label,weaimtoidentifythe
causal explanatory subgraph that causally yields such prediction. Our key idea
is that the causal explainer should be able to identify the causal interactions
among nodes/edges and interpret the label based on these interactions.
Specifically,weproposeaGNNcausalexplainer,calledCXGNN,thatconsists
ofthreesteps.1)Wefirstdefinecausalstructure(w.r.t.areferencenode)forthe
graph, which admits structure causal models (we call GNN-SCM). Such GNN-
SCM enables interventions to calculate cause and effects among nodes via do-
calculus [24]. 2) In real-world graphs, however, it is computationally challenging
toperformdo-calculuscomputationduetoalargenumberofnodesandedges.To
address it, we are inspired by the recent neural causal model (NCM) [43], which
is a special type of SCM that can be trainable. We prove that, for each GNN-
SCM, we can build a family of the respective GNN-NCMs. We then construct
a parameterized GNN-NCM such that when it is optimized, the cause-effects
defined on the GNN-NCM are easily calculated. 3) We finally determine the
causal explanatory subgraph. To do so, we first introduce the node expressivity
that reflects how well the reference node is in the causal explanatory subgraph.
ThenweiterateallnodesintheinputgraphandidentifythetrainedGNN-NCM
leading to the highest node expressivity. The underlying causal structure of this
GNN-NCM is then the causal explanatory subgraph.
We evaluate CXGNN on multiple synthetic and real-world graph datasets
withgroundtruthexplanations,andcomparethemwithstate-of-the-artassociation-
based and causality-inspired GNN explainers. Our results show CXGNN signifi-
cantlyoutperformsthebaselinesinexactlyfindingthegroundtruthexplanations.
Our contributions are summarized below:
– We propose the first GNN causal explainer CXGNN.
2 Granger causality can only identify that one variable helps predict another, but it
does not tell you which variable is the cause and which is the effect.Graph Neural Network Causal Explanation 3
– We leverage the neural-causal connection, design the GNN neural causal
models and train them to identify the causal explanatory subgraph.
– CXGNN shows superiority over the state-of-the-art GNN explainers.
2 Related Work
Association-based explainable GNN: Almost all existing GNN explainers
arebasedonassociation.Thesemethodscanberoughlyclassifiedintofivetypes.
(i)Decomposition-based methods [8,27] considerthepredictionofaGNNmodel
as a score and decompose it backward layer-by-layer until it reaches the input.
The score of different parts of the input can be used to explain its importance
to the prediction. (ii) Gradient-based methods [3,27] take the gradient of the
prediction with respect to the input to show the sensitivity of a prediction to
the input. The sensitivity can be used to explain the input for that prediction.
(iii) Surrogate-based methods [5,14,26,33,50] replace the GNN model with a
simple and interpretable surrogate one. (iv) Generation-based methods [18,30,
40,47] use generative models or graph generators to generate explanations. (v)
Perturbation-based methods [9,21,28,39,46,48,49] aim to find the important
subgraphs as explanations by perturbing the input. State-of-the-art explainers
from (iii)-(iv) show better performance than those from (i) and (ii).
Causality-inspired explainable GNN: Recent GNN explainers [7,19,20,32,
38,41]aremotivatedbycausality.Thesemethodsarebasedonacommonobser-
vation that a graph consists of the causal subgraph and its non-causal counter-
part. For instance, OrphicX [20] uses information-theoretic measures of causal
influence [2], and proposes to identify the (non)causal factors in the embedding
space via information flow maximization. CAL [32] introduces edge and node
attention modules to estimate the causal and non-causal graph features.
CXGNNvs.causality-inspiredexplainers.ThekeydifferenceliesinCXGNN
focusesonidentifyingthecausalexplanatorysubgraph bydirectlyquantifyingthe
cause-and-effect relations among nodes/edges in the graph. Instead, causality-
inspired explainers are inspired by causality concepts to infer the explanatory
subgraph, but they inherently do not provide causal explanations.
3 Preliminaries
In this section, we provide the necessary background on GNNs and causality to
understandthiswork.Forbrevity,wewillconsiderGNNsforgraphclassification.
Notations: We denote a graph as G=(V,E), where V and E are the node set
and edge set, respectively. v ∈ V(G) represents a node and e ∈ E(G) is an
u,v
edge between u and v. Each graph G is associated with a label y ∈Y, with Y
G
the label domain. An uppercase letter X and the corresponding lowercase one x
indicatearandomvariableanditsvalue,respectively;boldXandxdenoteaset
of random variables and its corresponding values, respectively. We use Dom(X)
to denote the domain of X, and P(X) as a probability distribution over X.4 Arman Behnam and Binghui Wang
Graph neural network (GNN):AGNNisamulti-layerneuralnetworkthat
operatesonthegraphanditerativelylearnsnode/graphrepresentationsviames-
sage passing. A GNN mainly uses two operations to compute node representa-
tions in each layer. Assume a node v’s representations in the (l−1)-th layer is
learnt and denoted as h(l−1). In the l-th layer, the message between two con-
v
nectednodesuandvisdefinedasml =MSG(hl−1,hl−1,e ).Theaggregated
u,v u v u,v
message for node u is then defined as u’s representation in the current layer l:
hl = AGG(ml :v ∈N (u)). Assume L layers of computation, the final repre-
u u,v 1
sentation for v is z =h(L) and Z={z } . GNN can add a predictor on top
v v v v∈V
of Z to perform graph-relevant tasks. For instance, when graph classification is
the task of interest, GNNs use Z to predict the label for a whole graph.
Structural Causal Model (SCM):SCMs[23]providearigorousdefinitionof
cause-effectrelationsbetweenrandomvariables.AnSCMMisafour-tupleM≡
(U,V,F,P(U)), where U is a set of exogenous (or latent) variables determined
by factors outside the model and they are the only source of randomness in an
SCM; V is a set {V ,V ,...,V } of n endogenous (or observable) variables of
1 2 n
interest determined by other variables within the model, i.e., in U∪V; F is set
of functions (define causal mechanisms) {f ,f ,...,f } such that each f is
(cid:83)
V1 V2 Vn Vi
a mapping function from U Pa to V , where U ⊆U and Pa ⊆V\V
Vi Vi i Vi Vi i
is the parent of V . That is, v ←f (pa ,u ) and F forms a mapping from U
i i Vi Vi Vi
to V. P(U) is the probability function over the domain of U. With SCM, one
canperforminterventionstofindcausesandeffectsanddesignamodelthathas
the capability of predicting the effect of interventions.
Definition 1 (InterventionandCausalEffects). Interventionsarechanges
made to a system to study the causal effect of a particular variable or treatment
on an outcome of interest. An SCM M induces a set of interventional distri-
butions over V, one for each intervention do(X = x) (short for do(x)), which
forces the value of variable X⊆V to be x. Then for each Y ⊆V:
pM(y|do(x))= (cid:88) P(u). (1)
{u|Yx(u)=y}
In words, an intervention forcing a set of variables X to take values x is
modeled by replacing the original mechanism f for each X ∈ X with its cor-
X
responding value in x. The impact of the intervention x on an outcome variable
Y is called potential response Y (u), which expresses causal effects and is the
x
solution for Y after evaluating: F :={f :V ∈V\X}∪{f ←x:X ∈X}.
x Vi i X
One possible strategy to estimate the underlying SCM of a task is using
its observational inputs and outputs. However, a critical issue is that causal
properties are provably impossible to recover solely from the joint distribution
over the input graphs and labels [24]. In this paper, we are inspired by the
emergingNeural Causal Model (NCM) [43],whichisaspecialtypeofSCMthat
is amenable to gradient descent-based optimization.
Neural Causal Model (NCM):ANCM[43]M(cid:99)(θ)overvariablesV withpa-
rameters θ ={θ Vi;V
i
∈V} is an SCM estimation as: M(cid:99)(θ)≡(U(cid:98),V,F(cid:98),P(U(cid:98))),Graph Neural Network Causal Explanation 5
where 1) U(cid:98) ⊆ {U(cid:98)C;C⊆V}, with each U(cid:98) associated with some subset of vari-
ables C ⊆ V, and Dom(U(cid:98)) = [0,1]; 2) F(cid:98) = {f(cid:98)Vi : V
i
∈ V}, with each
f(cid:98)Vi a neural network parameterized by θ
Vi
that maps U(cid:98)Vi ∪Pa
Vi
to V i, and
U(cid:98)Vi ={U(cid:98)C :V
i
∈C}; 3) P(U(cid:98)):U(cid:98) ∼Unif(0,1),∀U(cid:98) ∈U(cid:98).
[43] shows that NCM is proved to be as expressive as SCM, and hence
all NCMs are SCMs. However, expressiveness does not mean the learned NCM
modelhasthesameempiricalobservationsastheSCMmodel.Toensureequiva-
lence,thereshouldbeanecessarystructuralassumptiononNCMs,calledcausal
structure consistency. More details are referred to [43] and Appendix B.
4 GNN Causal Explanation via NCMs
In this section, we propose our GNN causal explainer, CXGNN, for explaining
graph classification. Our explainer also utilizes the common observation that a
graphconsistsofacausalsubgraphandanon-causalcounterpart[7,19,20,32,38,
41]. The overview of CXGNN is shown in Figure 10 in Appendix and all proofs
are deferred to Appendix C.
4.1 Overview
Given a graph G = (V,E) and a ground truth or predicted label by a GNN
model, our causal explainer bases on causal learning and identifies the causal
explanatory subgraph (denoted as Γ) that intrinsically yields the label.
Our CXGNN consists of three key steps: 1) define the causal structure G
for the graph G and the respective SCM M(cid:99)(G) (we call GNN-SCM) to enable
causal effect calculation via interventions; 2) However, directly calculating the
causal effect in real graphs is computationally challenging. We then construct
andtrainafamilyofparameterizedGNNneuralcausalmodelM(cid:99)(G,θ))(wecall
GNN-NCM), a special type of GNN-SCM that is trainable. 3) We uncover the
causal explanatory subgraph (denoted as Γ) based on the trained GNN-NCM
that best yields the graph label. Next, we will illustrate step-by-step in detail.
4.2 Causal Structure and Induced SCM on a Graph
In the context of causality, the problem of GNN explanation can be solved by
causeandeffectidentificationamongnodesandtheirconnectionsinagraph.In-
terventionsenableustointerpretthecausalrelationbetweennodes.Toperform
interventions on a graph, one often needs to first define the causal structure for
this graph, which involves the observable and latent variables.
Observable and latent variables in a graph: Given a G = (V,E). For
each node v ∈ V, there are both known and unknown effects from other nodes
and edges on v, which we call observable variables (denoted as V ) and latent
v
variables (denoted as U ), respectively. With it, we define a congruent causal
v
structure for enabling the graphs to admit SCMs.6 Arman Behnam and Binghui Wang
Definition 2 (Causal structure of a graph). Consider a graph G=(V,E),
we define the causal structure G of G as a subgraph that centers on a reference
node v and accepts the SCM structure:
(cid:110) (cid:111)
G(G)= V ={y }∪{y :v ∈N (v)},U ={U :v ∈N (v)}∪{U :e ∈E} ,
v v vi i ≤k v vi i ≤k v,vi v,vi
(2)
where v is to be learnt (see Section 4.4), y is the node v ’s label, N (v) means
vi i ≤k
nodes within the k-hop neighbors of v, U is v ’s latent variable, called node
vi i
effect; and U the edge e latent variable, called edgeeffect. In practice, we
v,vi v,vi
canspecifyU andU asrandomvariable,e.g.,fromaGaussiandistribution.
vi v,vi
With a causal structure for a graph, we can build the corresponding SCM in
the following theorem:
Theorem 1 (GNN-SCM). For a GNN operating on a graph G, there exists
an SCM M(G) w.r.t. the causal structure G of the graph G.
Appendix A shows an example on how to compute the causal effects on a
toy graph via a SCM truth table.
4.3 GNN Neural Causal Model
In reality, it is computationally challenging to build a truth table for variables
in GNN-SCM and perform do-calculus computation due to the large number of
nodes/edges in real-world graphs. Such a challenge impedes the calculation of
causaleffects.Toaddressit,wearemotivatedbyestimatingthecausaleffectvia
NCM(seeSection3).Specifically,Definition6inAppendixshows:toensurethe
equivalencebetweenNCMandSCM,NCMisrequiredtobeG-constrained.How-
ever, the general G-constrained NCM cannot be directly applied in our setting.
To this end, we first define a customized G-constrained GNN-NCM as below:
Definition 3 (G-ConstrainedGNN-NCM(constructive)). LetGNN-SCM
M(G,θ) be induced from the causal structure G(G) on a graph G. Then GNN-
NCM M(cid:99)(G,θ) will be constructed based on the causal structure G(G).
This construction ensures that any inferences made by M(cid:99)NCM(G,θ) respect
the causal dependencies as captured by G(G). Note that M(cid:99)(G,θ) represents
a family of GNN-NCMs since the parameters θ of the neural networks are not
specifiedbytheconstruction.Next,weproposeaconstructionofaG-constrained
GNN-NCM, following Definition 3.
GNN Neural Causal Model Construction One should consider the sound
and complete structure of GNN-NCMs that are consistent with Definition 2.
Here,wedefinethegeneralGNN-NCMstructureasshowninbelowEquation3,
which is an instantiation of Theorem 2.

V:=V(G)
M(cid:99)(G,θ)=
fU(cid:98) (cid:98)vi: (= u(cid:98)v{ iU(cid:98) ,u(cid:98)vi v, iv ,vi j∈ ):V =(G ar} g,P k∈m( {U(cid:98)
a
0,x) 1}:= Tk{ ,U v(cid:98) iv +i ∼ (cid:40)
l
lU
o
on
g
g(i σf 1( (0 −f, f1 σv)
i
(} f(u(cid:98)∪
fv
v{
i
iT
,
(u(cid:98)u(cid:98)k, vvv iii
, ,v
u(cid:98)∼
j v;
iN
θ ,v
v(
i
j0
)
;,
)
θ1 vi)
f
i):
k
)k )=∈ if1{ k0 =,1 0} ,}
F(cid:98):={f(cid:98)vi(u(cid:98)vi,u(cid:98)vi,vj)}
(3)Graph Neural Network Causal Explanation 7
Algorithm 1 GNN Neural Causal Model Training
Input:ThecausalstructureG(includingareferencenodev,itswithink-hopneighbors
N (v), and set of latent variables U ), node label y
≤k v v
Output: An optimized GNN-NCM M(cid:99)(G,θ∗) for the causal structure G centered at v
1: Build the GNN-NCM M(cid:99)(G,θ) based on G and Eqn. 3
2: for each node v ∈N (v) do
i ≤k
3: Calculate pM(cid:99)(G,θ)(y |do(v )) via Eqn. 4
v i
4: end for
5: Calculate pM(cid:99)(G,θ)(y ) via Eqn. 5
v
6: Calculate the loss L(M(cid:99)(G,θ);v) via Eqn. 6
7: Minimize the loss to reach the GNN-NCM M(cid:99)(G,θ∗)
Theorem 2 (GNN-NCM). Given causal structure G of a graph G and the
underlying GNN-SCM M(G), there exists a G-constrained GNN-NCM M(cid:99)(G,θ)
that enables any inferences consistent with M(G).
In Equation 3, V are the nodes in the causal structure G(G); each T is a
vi
standard Gaussian random variable; each ff is a feed-forward neural network
vi
onv parameterizedbyθ (noteonerequirementofff isitcouldapproximate
i vi vi
any continuous function), and σ is sigmoid activation function. The parameters
{θ } are not yet specified and must be learned through training the NCM.
vi
Training Neural Networks for GNN-NCMs We now compute the causal
effects on a target node v. Based on Definition 1 and the constructed GNN-
NCM M(cid:99)(G,θ) in Equation 3, the causal effect on v of an intervention do(v i)
(v ∈ N (v)) is pM(cid:99)(G,θ)(y |do(v )). This do-calculus then can be calculated as
i 1 v i
the expected value of nodes and edges affects values for v shown below:
pM(cid:99)(G,θ)(y
v
|do(v i))=E p(u(cid:98)v)(cid:104) (cid:89) f(cid:98)vi(u (cid:98)vj,u (cid:98)v,vj)(cid:105)
(v,vj)∈E(G)
1 (cid:88) (cid:89)
≈
|N (v)|
f(cid:98)vi(u (cid:98)vj,u (cid:98)v,vj). (4)
≤k
vi∈N≤k(v)(v,vj)∈E(G)
Thenonecancalculatetheprobabilityofthetargetnodelabely astheexpected
v
value of all the effects from the neighbor nodes on v:
pM(cid:99)(G,θ)(y v)=E p(uˆv)(cid:104) f(cid:98)v(cid:105) ≈ |N1 (v)||Y1
|
(cid:88) (cid:88) pM(cid:99)(G,θ)(y
v
=y|do(v i)) (5)
1
y∈Yvi∈N1(v)
ThetrueGNN-SCMinducesacausalstructurethatencodesconstraintsover
theinterventionaldistributions.Wenowfirstinvestigatethefeasibilityofcausal
inferencesintheclassofG-constrainedGNN-NCMs.Thesemodelsapproximate
the likelihood of the observed data based on the graph’s latent variables. The
cross-entropylossmeasuresthediscrepancybetweenthetargetnode’slabelpre-
diction and its true label. Inspired by [43], we define the GNN-NCM loss as:
L(M(cid:99)(G,θ);v)=− (cid:88) y vlog(pM(cid:99)(G,θ)(y v)) (6)
yv∈Y8 Arman Behnam and Binghui Wang
Algorithm 2 CXGNN: GNN Causal Explainer
Input: Graph G with label, and expressivity threshold δ
Output: Explanatory subgraph Γ
1: for each node v∈V(G) do
2: Build G based on the reference node v
v
3: TraintheGNN-NCMM(cid:99)(G v,θ v∗)viaAlg.1andcalculatethenodeexpressivity
exp v(M(cid:99)(G v,θ v∗))
4: end for
5: Find v∗ =argmax v∈V(G)exp v(M(cid:99)(G v,θ v∗));
6: Return the explanatory subgraph Γ induced by G v∗
To train neural networks for GNN-NCMs, one should generate samples from
the GNN-SCM. If provided, it is the specific realization of the interventions.
Specifically, GNN-NCMs are trained on node effects uˆ and edge effects uˆ
vi vi,vj
on the target node, as shown in Equation 3, and should specify f(cid:98)vi(uˆ vi,uˆ vi,vj).
Then a model, denoted as θ∗, is achieved by minimizing the GNN-NCM loss:
θ∗ ∈argminL(M(cid:99)(G,θ);v) (7)
θ
Details of training GNN-NCMs are shown in Algorithm 1. Basically, this algo-
rithm takes the causal structure G with respect to a reference node v as input
and returns an optimized GNN-NCM model M(cid:99)(G,θ∗).
4.4 Realizing GNN Causal Explanation
The remaining question is: how to find the causal explanatory subgraph Γ from
agraphGtocausallyexplainGNNpredictions?Theanswerisusingthetrained
GNN-NCMs M(cid:99)(G,θ∗). Before that, the first step is to clarify a node’s role in
GNN-NCMs for explanation.
Theorem 3 (Node explainability). Let a prediction for a graph G be ex-
plained.Anodev ∈Giscausallyexplainable,ifpM(cid:99)(G(G),θ)(y )canbecomputed.
v
The G-constrained GNN-NCM is trained on interventions and can interpret
the GNN predictions. Moreover, the information extracted from interventions
canbeusedforinterpretingnodes.Specifically,wedefineexpressivitytomeasure
the information for an explainable node.
Theorem 4 (Explainable node expressivity). An explainable node v has
expressivity defined as exp v(M(cid:99)(G,θ))=(cid:80) yvy vpM(cid:99)(G,θ)(y v).
In other words, the node expressivity reflects how well the node is in the
causal explanatory subgraph. Now we are ready to realize GNN causal explana-
tion based on learned GNN-NCMs. Given a graph G, we start from a random
node v, and build the causal structure G centered on v. By Algorithm 1, we can
reach an optimized GNN-NCM M(cid:99)(G,θ∗) and obtain the v’s expressivity.
WerepeatthisprocessforallnodesinthegraphGandfindthenodev∗ with
the associated M(cid:99)(G,θ∗) yielding the highest expressivity exp v∗(M(cid:99)(G,θ∗)). The
underlying subgraph of the causal structure centered by v∗ is then treated as
thecausalexplanatorysubgraphΓ.Algorithm2describesthelearningprocess.Graph Neural Network Causal Explanation 9
Table 1: Dataset statistics.
Avg. #nodesAvg. #edges#test graphs
BA+House 11.97 18.17 500
BA+Grid 15.96 24.20 500
BA+Cycle 10.0 10.5 500
Tree+House 12 13 500
Tree+Cycle 13 13.50 500
Tree+Grid 24 27 500
Benzene 20.48 21.73 100
Fluoride carbonyl 20.66 22.03 100
5 Experiments
5.1 Experimental Setup
Datasets: Following prior works [19,46], we use six synthetic datasets, and
two real-world datasets with groundtruth explanation for evaluation. Dataset
statistics are shown in Table 1.
– Synthetic graphs: 1) BA+House: This graph stems from a base random
Barabási-Albert(BA)graphattachedwitha5-node“house"-structuredmo-
tif as the groundtruth explanation; 2) BA+Grid: This graph contains a
base random BA graph and is attached with a 9-node “grid" motif as the
groundtruth explanation; 3) BA+Cycle: A 6-node “cycle" motif is ap-
pendedtorandomlychosennodesfromthebaseBAgraph.The“cycle"motif
is the groundtruth explanation; 4) Tree+House: The core of this graph is
a balanced binary tree. The 5-node “house" motif, as the groundtruth ex-
planation, is attached to random nodes from the base tree. 5) Tree+Grid:
Similarly, binary tree a the core graph and a 9-node “grid" motif as the
groundtruth explanation is attached; 6) Tree+Cycle: A 6-node “cycle"
motif, the groundtruth explanation, is appended to nodes from the binary
tree. The label of the synthetic graph is decided by the label of nodes in
the groundtruth explanation. Following existing works [19,46], a node v’s
label y is set to be 1 if v is in the groundtruth, and 0 otherwise. Hence, in
v
thesegraphs,thebasegraphactsasthenon-causalsubgraphthatcancause
the spurious correlation, while the attached motif can be seen as the causal
subgraph, as it does not change across graphs and decides the graph label.
– Real-world graphs: Weusetworepresentativereal-worldgraphdatasetswith
groundtruth[1].1) Benzene:itincludes12,000moleculargraphsextracted
from the ZINC15 [31] database and the task is to identify whether a given
molecule graph has a benzene ring or not. The groundtruth explanations
are the nodes (atoms) forming the benzene ring. 2) Fluoride carbonyl:
This dataset contains 8,671 molecular graphs with two classes: a positive
classmeansamoleculegraphcontainsafluoride(F-)andacarbonyl(C=O)
functional group. The groundtruth explanation consists of combinations of
fluoride atoms and carbonyl functional groups within a given molecule.10 Arman Behnam and Binghui Wang
Models and parameter setting: In CXGNN, we use a feedforward neural
network to parameterize GNN-NCM. The neural network consists of an input
layer, two fully connected hidden layers, and an output layer. ReLU activation
functions is used in all hidden layers, while a softmax activation function is ap-
plied to the output layer. The input to the network is the target node v’s node
effects and edge effects (see Equation 2), whose values are sampled from a stan-
dard Gaussian distribution, and the output is the predicted causal effect on v.
ThedetailedhyperparametersareshowninAppendixD.1.Thehyperparameters
in the compared GNN explainers are optimized based on their source code.
BaselineGNNexplainers:WecompareCXGNNwithbothassociation-based
andcausality-inspiredGNNexplainers.Wechoose4representativeones:gradient-
based Guidedbp [12], perturbation-based GNNExplainer [46], surrogate-based
PGMExplainer [33], and causality-inspired GEM [19], RCExplainer [38], and
OrphicX [20]. We use the public source code of these explainers for comparison.
The causality-inspired explainers are inspired by causality concepts to infer the
explanatory subgraph, but they inherently do not provide causal explanations.
Evaluation metrics: Given a set of testing graphs G. For each test graph
G ∈ G, we let its groundtruth explanatory subgraph be Γ and the estimated
G
explanatory subgraph by a GNN explainer be Γ. We use two common metrics,
i.e.,graphexplanationaccuracyandexplanationrecallfromtheliterature[1].In
addition, to justify the superiority of our causal explainer, we introduce a third
metric groundtruth match accuracy, which is the most challenging one.
– Graphexplanationaccuracy:ForagraphG,thegraphexplanationaccu-
racyisdefinedasthefractionofnodesintheestimatedexplanatorysubgraph
Γ that are contained in the groundtruth Γ , i.e., |V(Γ)∩V(Γ )|/|V(Γ )|.
G G G
We then report the average accuracy across all testing graphs.
– Graphexplanationrecall:DifferentGNNexplainersoutputtheestimated
explanatorysubgraphwithdifferentnodesizes.Whentwoexplainersoutput
thesamenumberofnodesinΓ ,theonewithasmallernodesizeshouldbe
G
treatedashavingabetterquality.Toaccountforthis,weusetheexplanation
recall metric that is defined as |V(Γ)∩V(Γ )|/|V(Γ)| for a given graph G.
G
We then report the average recall across all testing graphs.
– Groundtruth match accuracy:ForatestinggraphG,wecounta1ifthe
estimated Γ and groundtruth Γ exactly match, i.e., Γ =Γ, and 0 other-
G G
wise. In other words, the groundtruth match accuracy of all testing graphs
G is defined as (cid:80) 1[Γ =Γ]/|G|, where 1[·] is an indicator function.
G∈G G
5.2 Results on Synthetic Datasets
Comparison results: Table 2 shows the results of all the compared GNN ex-
plainers on the 6 synthetic datasets with 500 testing graphs and 3 metrics. We
have several observations. In terms of explanation accuracy, CXGNN performs
comparable or slightly worse than causality-inspired methods. This is because,
to ensure high accuracy, the estimated explanatory subgraph of these methods
should have a large size. This can be reflected by the explanation recall, whereGraph Neural Network Causal Explanation 11
Table 2: Comparison results on the synthetic datasets. B.H.: BA+House; B.G.:
BA+Grid; B.C.: BA+Cycle; T.H.: Tree+House; T.G.: Tree+Grid; T.C.: Tree+Cycle.
Graph explanation accuracy (%)
B.H. B.G. B.C. T.H. T.C. T.G.
GNNExp. [46] 75.60 76.16 75.13 77.24 71.60 72.18
PGMExp. [33] 61.60 44.98 63.07 58.28 49.90 37.42
Guidedbp [12] 60.00 0.00 66.67 0.00 0.00 0.00
GEM [19] 98.2 88.19 97.91 96.23 95.51 86.96
RCExp. [38] 100.00 88.89 100.00100.00100.00100.00
OrphicX [20] 88.00 89.00 55.65 96.20 100.00 99.93
CXGNN 100.0 100.00 83.33 100.0 82.67 100.00
Graph explanation recall (%)
B.H. B.G. B.C. T.H. T.C. T.G.
GNNExp. [46] 37.62 52.72 45.08 32.18 33.05 40.60
PGMExp. [33] 30.80 31.14 37.84 24.28 23.93 21.05
Guidedbp [12] 12.40 17.94 16.18 5.99 12.98 15.38
GEM [19] 39.18 50.86 45.40 38.75 34.65 41.20
RCExp. [38] 100.00 60.00 89.52 45.45 46.60 39.13
OrphicX [20] 98.08 97.71 60.00 41.38 59.22 40.61
CXGNN 100.0 60.55 90.00 61.67 68.15 49.05
Groundtruth match accuracy (%)
B.H. B.G. B.C. T.H. T.C. T.G.
GNNExp. [46] 0.20 2.20 2.20 0.80 0.40 0.20
PGMExp. [33] 1.00 0.00 0.00 3.80 0.00 0.00
Guidedbp [12] 1.00 0.6 0.6 0.6 0.2 0.6
GEM [19] 0.80 6.00 6.00 2.50 1.20 1.00
RCExp. [38] 100.00 0.00 49.60 0.00 0.00 0.00
OrphicX [20] 39.00 43.00 5.00 1.40 21.00 33.00
CXGNN 100.0 44.00 67.60 99.40 61.20 46.00
explanation recall is significantly reduced. Overall, the causality-inspired meth-
ods obtain higher accuracies than purely association-based methods.
More importantly, CXGNN drastically outperforms all the compared GNN
explainersintermsofgroundtruthmatch.Suchabigdifferencedemonstratesall
the association-based and causality-inspired GNN explainers are insufficient to
uncovertheexactgroundtruth.TheisduetoexistingGNNexplainersinherently
learning from correlations among nodes/edges in the graph, and capturing spu-
rious correlations. Instead, our causal explainer CXGNN can do so much more
accurately.Thisverifiesthecausalexplainerindeedcanintrinsicallyuncoverthe
causal relation between the explanatory subgraph and the graph label.
Visualization results: Figure 1 visualizes the explanations results of some
testing graphs in the four synthetic datasets. We note that there are different
ways for the groundtruth subgraph to attach to the base synthetic graph. We
can see CXGNN’s output exactly matches the groundtruth in these cases, while
the existing GNN explainers cannot. One reason could be that existing GNN
explainers are sensitive to the spurious relation.
Loss curve: Figure 2 shows the loss curves to train our GNN-NCM on a set
of nodes, where some nodes are in the groundtruth and some are not from12 Arman Behnam and Binghui Wang
(a) BA+House (b) BA+Grid (c) BA+Cycle (d)Tree+House (e) Tree+Grid (f) Tree+Cycle
Fig.1: Visualizing explanation results (subgraph containing the red nodes) by our
CXGNN on synthetic graphs.
Fig.2:LosscurvesoftrainingtheGNN-NCMsonthegroundtruthnodes(greencurves)
andnon-groundtruthones(redcurves)ontworandomchosengraphsfromBA+House.
More examples in other datasets are shown in Appendix D.
Fig.3: Node expressivity distributions on two unsuccessful graphs from BA+Cycle.
Greenbarscorrespondtonodesthatareinthegroundtruth,whileredbarscorrespond
to nodes that are not. More examples in other datasets are shown in Appendix D.
BA+House. We can see the loss decreases stably for groundtruth nodes, while
the loss for nodes not from the groundtruth are relatively high. This reflects
ourdesignedGNN-NCMmakesiteasiertolearngroundtruthnodes.Thatbeing
said, CXGNN indeed tends to find the causal subgraph.
Node expressivity distribution: We notice CXGNN still misses finding the
groundtruth explanatory subgraph for some graphs. One possible reason could
be that, theoretically, our GNN-SCM can always uncover the causal subgraph,
but practically, it is challenging to train the optimal one. Here, we randomlyGraph Neural Network Causal Explanation 13
Table 3: Comprehensive comparison results on the real-world datasets.
Exp. Acc. (%)Exp. Recall (%)GT Match Acc. (%)
Method Benzene F.C. Benzene F.C. Benzene F.C.
GNNExp. [46] 66.05 44.44 18.88 14.42 0.00 0.00
PGMExp. [33] 33.33 17.78 7.51 4.98 0.00 0.00
Guidedbp [12] 0.00 0.00 9.06 8.00 0.00 0.00
GEM [19] 71.98 46.22 19.80 14.57 0.00 0.00
RCExp. [38] 0.20 0.05 10.85 2.01 0.00 0.00
OrphicX [20] 47.63 11.14 30.31 10.01 3.40 5.50
CXGNN 73.46 66.67 21.35 16.43 66.67 75.00
Fig.4: Explanationresults(subgraphcontainingtheyellownodes)byourCXGNNon
real-worldgraphs.TheleftandrighttwographsareinBenzeneandF.C.,respectively.
Fig.5:LosscurvesoftrainingtheGNN-NCMsonthegroundtruthnodes(greencurves)
andnon-groundtruthones(redcurves)ontwographsfromthetworeal-worlddatasets,
respectively. More examples are shown in Appendix D.
select 2 such unsuccessful graphs in BA+Cycle and plot their distributions on
the node expressivity in Figure 3. We observe that, though the groundtruth
nodes are not always having the best expressivity, they are still at the top.
5.3 Results on Real-World Datasets
Comparison results: Table 3 shows the results of all the compared explainers
on the real-world datasets and three metrics. We have similar observations as
those in Table 2. Especially, no existing explainers can even find one exactly
matched groundtruth. Particularly, the explanation subgraphs produced by the
twocausality-inspiredbaselinescancoverthemajorityoralmostallgroundtruth
insyntheticdatasets(hencehighaccuracy),andthesizesoftheexplanationsub-
graphs are slightly larger than those of the groundtruth (hence relatively large14 Arman Behnam and Binghui Wang
Fig.6: Nodeexpressivitydistributionsontwounsuccessfulgraphsfromthereal-world
datasets, respectively. Green bars correspond to nodes that are in the groundtruth,
while red bars correspond to nodes that are not. More examples are in Appendix D.
recall).However,thecausality-inspiredbaselinesarenotgoodatexactlymatch-
ing the groundtruth, i.e., groundtruth match accuracy is low overall. Note also
thattheexactmatchofCXGNNisalsolargelyreduced(about30%).Onepossi-
blereasonisthatthegroundtruthexplanationinreal-worldgraphsisnoteasyto
define or even inaccurate. For instance, in MUTAG, both NO and NH motifs
2 2
are considered as the "mutagenic" groundtruth in the literature. However, [19]
found 32% of non-mutagenic graphs contain NO or NH , implying inaccurate
2 2
groundtruth. Here, we propose to also use an approximate groundtruth match
accuracy, where we require the estimated subgraph to be a subset and its size is
nolessthan60%ofthegroundtruth.Withthisnewalternativemetric,itsvalue
is much larger (i.e., 67% and 75%) on the two datasets.
Visualization results: Figure 4 visualizes the explanation results of some
graphs in the real-world datasets. We observe the explanatory subgraphs found
by CXGNN approximately/exactly match the groundtruth.
Loss curve: Figure 5 shows the loss curves to train our GNN-NCM on a set of
groundtruth and non-groundtruth ones. Similarly, the loss decreases stably for
groundtruth nodes, while not for non-groundtruth ones. Again, this implies our
GNN-NCM tends to find the causal subgraph.
Nodeexpressivitydistribution:Werandomlyselectsomeunsuccessfulgraphs
in real-world datasets and plot their distributions on the node expressivity in
Figure 6. Still, though the groundtruth nodes do not always achieve the best
expressivity, they are at the top.
6 Conclusion
GNNexplanation,i.e.,identifyingtheinformativesubgraphthatensuresaGNN
makes a particular prediction for a graph, is an important research problem.
ThoughvariousGNNexplainershavebeenproposed,theyareshowntobeprone
to spurious correlations. We propose a causal GNN explainer based on the fact
that a graph often consists of a causal subgraph and fulfills the goal via causal
inference. We then propose to train GNN neural causal models to uncover the
causalexplanatorysubgraph.Infuturework,wewillstudytherobustnessofour
CXGNN under the adversarial graph perturbation attacks [17,22,34–37,45].Graph Neural Network Causal Explanation 15
Acknowledgements
We thank all the anonymous reviewers for their valuable feedback and con-
structive comments. Behnam and Wang are partially supported by the Ama-
zon Research Award, the National Science Foundation (NSF) under Grant Nos.
2216926, 2241713, 2331302, and 2339686. Any opinions, findings conclusions,
and recommendations expressed in this material are those of the author(s) and
do not necessarily reflect the views of the funding agencies.
References
1. Agarwal, C., Queen, O., Lakkaraju, H., Zitnik, M.: Evaluating explainability for
graph neural networks. Scientific Data 10(1), 144 (2023)
2. Ay, N., Polani, D.: Information flows in causal networks. Advances in complex
systems 11(01), 17–41 (2008)
3. Baldassarre, F., Azizpour, H.: Explainability techniques for graph convolutional
networks. arXiv preprint arXiv:1905.13686 (2019)
4. Beckers, S.: Causal explanations and xai. In: Conference on Causal Learning and
Reasoning. pp. 90–109. PMLR (2022)
5. Duval,A.,Malliaros,F.D.:Graphsvx:Shapleyvalueexplanationsforgraphneural
networks.In:MachineLearningandKnowledgeDiscoveryinDatabases.Research
Track:EuropeanConference,ECMLPKDD2021,Bilbao,Spain,September13–17,
2021, Proceedings, Part II 21. pp. 302–318. Springer (2021)
6. Dwivedi,V.P.,Joshi,C.K.,Luu,A.T.,Laurent,T.,Bengio,Y.,Bresson,X.:Bench-
marking graph neural networks. Journal of Machine Learning Research (2023)
7. Fan,S.,Wang,X.,Mo,Y.,Shi,C.,Tang,J.:Debiasinggraphneuralnetworksvia
learning disentangled causal substructure. Advances in Neural Information Pro-
cessing Systems 35, 24934–24946 (2022)
8. Feng,Q.,Liu,N.,Yang,F.,Tang,R.,Du,M.,Hu,X.:Degree:Decompositionbased
explanation for graph neural networks. arXiv preprint arXiv:2305.12895 (2023)
9. Funke, T., Khosla, M., Rathee, M., Anand, A.: Zorro: Valid, sparse, and stable
explanationsingraphneuralnetworks.IEEETransactionsonKnowledgeandData
Engineering (2022)
10. Geiger, A., Wu, Z., Lu, H., Rozner, J., Kreiss, E., Icard, T., Goodman, N., Potts,
C.: Inducing causal structure for interpretable neural networks. In: International
Conference on Machine Learning. pp. 7324–7338. PMLR (2022)
11. Granger, C.W.: Investigating causal relations by econometric models and cross-
spectral methods. Econometrica: journal of the Econometric Society pp. 424–438
(1969)
12. Gu, J., Tresp, V.: Saliency methods for explaining adversarial attacks. arXiv
preprint arXiv:1908.08413 (2019)
13. Hamilton, W., Ying, Z., Leskovec, J.: Inductive representation learning on large
graphs. In: Advances in Neural Information Processing Systems. pp. 1024–1034
(2017)
14. Huang, Q., Yamada, M., Tian, Y., Singh, D., Chang, Y.: Graphlime: Local in-
terpretable model explanations for graph neural networks. IEEE Transactions on
Knowledge and Data Engineering (2022)
15. Kipf, T.N., Welling, M.: Semi-supervised classification with graph convolutional
networks. arXiv preprint arXiv:1609.02907 (2016)16 Arman Behnam and Binghui Wang
16. Kipf, T.N., Welling, M.: Semi-supervised classification with graph convolutional
networks. In: International Conference on Learning Representations (2017)
17. Li,J.,Pang,M.,Dong,Y.,Jia,J.,Wang,B.:Graphneuralnetworkexplanationsare
fragile.In:Proceedingsofthe41stInternationalConferenceonMachineLearning
(2024)
18. Li,W.,Li,Y.,Li,Z.,Hao,J.,Pang,Y.:Dagmatters!gflownetsenhancedexplainer
for graph neural networks. arXiv preprint arXiv:2303.02448 (2023)
19. Lin,W.,Lan,H.,Li,B.:Generativecausalexplanationsforgraphneuralnetworks.
In: International Conference on Machine Learning. pp. 6666–6679. PMLR (2021)
20. Lin, W., Lan, H., Wang, H., Li, B.: Orphicx: A causality-inspired latent variable
model for interpreting graph neural networks. In: Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition.pp.13729–13738(2022)
21. Luo,D.,Cheng,W.,Xu,D.,Yu,W.,Zong,B.,Chen,H.,Zhang,X.:Parameterized
explainer for graph neural network. Advances in neural information processing
systems 33, 19620–19631 (2020)
22. Mu, J., Wang, B., Li, Q., Sun, K., Xu, M., Liu, Z.: A hard label black-box ad-
versarial attack against graph neural networks. In: Proceedings of the 2021 ACM
SIGSAC Conference on Computer and Communications Security (CCS) (2021)
23. Pearl, J.: Causality. Cambridge university press (2009)
24. Pearl,J.,Glymour,M.,Jewell,N.P.:Causalinferenceinstatistics:Aprimer.John
Wiley & Sons (2016)
25. Pearl, J., Mackenzie, D.: The book of why: the new science of cause and effect.
Basic books (2018)
26. Pereira,T.,Nascimento,E.,Resck,L.E.,Mesquita,D.,Souza,A.:Distilln’explain:
explaining graph neural networks using simple surrogates. In: International Con-
ference on Artificial Intelligence and Statistics. pp. 6199–6214. PMLR (2023)
27. Pope, P.E., Kolouri, S., Rostami, M., Martin, C.E., Hoffmann, H.: Explainabil-
ity methods for graph convolutional neural networks. In: Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition. pp. 10772–
10781 (2019)
28. Schlichtkrull, M.S., Cao, N.D., Titov, I.: Interpreting graph neural networks for
NLP with differentiable edge masking. In: International Conference on Learning
Representations (2021), https://openreview.net/forum?id=WznmQa42ZAx
29. Schwab, P., Karlen, W.: Cxplain: Causal explanations for model interpretation
under uncertainty. Advances in neural information processing systems 32 (2019)
30. Shan, C., Shen, Y., Zhang, Y., Li, X., Li, D.: Reinforcement learning enhanced
explainer for graph neural networks. Advances in Neural Information Processing
Systems 34, 22523–22533 (2021)
31. Sterling,T.,Irwin,J.J.:Zinc15–liganddiscoveryforeveryone.Journalofchemical
information and modeling 55(11), 2324–2337 (2015)
32. Sui,Y.,Wang,X.,Wu,J.,Lin,M.,He,X.,Chua,T.S.:Causalattentionforinter-
pretable and generalizable graph classification. In: Proceedings of the 28th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining. pp. 1696–1705
(2022)
33. Vu, M., Thai, M.T.: Pgm-explainer: Probabilistic graphical model explanations
forgraphneuralnetworks.Advancesinneuralinformationprocessingsystems33,
12225–12235 (2020)
34. Wang, B., Gong, N.Z.: Attacking graph-based classification via manipulating the
graph structure. In: Proceedings of the 2019 ACM SIGSAC Conference on Com-
puter and Communications Security (CCS) (2019)Graph Neural Network Causal Explanation 17
35. Wang, B., Jia, J., Cao, X., Gong, N.Z.: Certified robustness of graph neural net-
works against adversarial structural perturbation. In: Proceedings of the 27th
ACM SIGKDD Conference on Knowledge Discovery & Data Mining. pp. 1645–
1653 (2021)
36. Wang, B., Li, Y., Zhou, P.: Bandits for structure perturbation-based black-box
attackstographneuralnetworkswiththeoreticalguarantees.In:IEEE/CVFCon-
ference on Computer Vision and Pattern Recognition (CVPR) (2022)
37. Wang, B., Pang, M., Dong, Y.: Turning strengths into weaknesses: A certified ro-
bustnessinspiredattackframeworkagainstgraphneuralnetworks.In:IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) (2023)
38. Wang, X., Wu, Y., Zhang, A., Feng, F., He, X., Chua, T.S.: Reinforced causal
explainer for graph neural networks. IEEE Transactions on Pattern Analysis and
Machine Intelligence 45(2), 2297–2309 (2022)
39. Wang, X., Wu, Y., Zhang, A., He, X., Chua, T.S.: Towards multi-grained ex-
plainabilityforgraphneuralnetworks.AdvancesinNeuralInformationProcessing
Systems 34, 18446–18458 (2021)
40. Wang, X., Shen, H.W.: Gnninterpreter: A probabilistic generative model-level ex-
planation for graph neural networks. arXiv preprint arXiv:2209.07924 (2022)
41. Wu, Y.X., Wang, X., Zhang, A., He, X., Chua, T.S.: Discovering invariant ratio-
nales for graph neural networks. arXiv preprint arXiv:2201.12872 (2022)
42. Wu,Z.,Pan,S.,Chen,F.,Long,G.,Zhang,C.,Philip,S.Y.:Acomprehensivesur-
veyongraphneuralnetworks.IEEEtransactionsonneuralnetworksandlearning
systems 32(1), 4–24 (2020)
43. Xia,K.,Lee,K.Z.,Bengio,Y.,Bareinboim,E.:Thecausal-neuralconnection:Ex-
pressiveness, learnability, and inference. Advances in Neural Information Process-
ing Systems 34, 10823–10836 (2021)
44. Xu,K.,Hu,W.,Leskovec,J.,Jegelka,S.:Howpowerfularegraphneuralnetworks?
In: International Conference on Learning Representations (2019)
45. Yang, H., Wang, B., Jia, J., et al.: Gnncert: Deterministic certification of graph
neural networks against adversarial perturbations. In: The Twelfth International
Conference on Learning Representations (2024)
46. Ying, Z., Bourgeois, D., You, J., Zitnik, M., Leskovec, J.: GNNExplainer: Gener-
atingexplanationsforgraphneuralnetworks.In:AdvancesinNeuralInformation
Processing Systems (2019)
47. Yuan, H., Tang, J., Hu, X., Ji, S.: Xgnn: Towards model-level explanations of
graph neural networks. In: Proceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining. pp. 430–438 (2020)
48. Yuan,H.,Yu,H.,Wang,J.,Li,K.,Ji,S.:Onexplainabilityofgraphneuralnetworks
viasubgraphexplorations.In:Proceedingsofthe38thInternationalConferenceon
Machine Learning (ICML). pp. 12241–12252 (2021)
49. Zhang, S., Liu, Y., Shah, N., Sun, Y.: Gstarx: Explaining graph neural networks
withstructure-awarecooperativegames.AdvancesinNeuralInformationProcess-
ing Systems 35, 19810–19823 (2022)
50. Zhang, Y., Defazio, D., Ramesh, A.: Relex: A model-agnostic relational model
explainer.In:Proceedingsofthe2021AAAI/ACMConferenceonAI,Ethics,and
Society. pp. 1042–1049 (2021)18 Arman Behnam and Binghui Wang
Appendix
A A GNN-SCM Example
The causal structure of a graph is a subgraph centering on a reference node and
accepts the SCM structure via Definition 2. The goal of causality in a graph
is to identify the subgraph with the maximum explainable node expressivity as
Theorem4thatcausallyexplainsGNNpredictions.Below,weuseatoyexample
graph to show how our explainer captures the causality in this graph.
We use a toy example to demonstrate SCMs and
the intervention process in GNNs. Figure 7 shows a
graphGthatcontainsfournodesA,B,C,andD,and
A B D
threeedgesA-B,A-C,andB-D.InGNNs,theseedges
contain messages passing between two nodes. For ex-
ample, the message between two nodes A and B in C
thel-thlayeroftheGNNisml =MSG(hl−1,hl−1,
A,B A B
e ).Ifthereexistsanedge,itmeansthatthereisan Fig.7: A toy example
A,B
interaction between nodes that has a specific value in
eachlayerl.Ifthereisnoedge,twonodesdon’tshare
amessage.IfweconsidernodeAasthereferencenodev,thenodesB,andC are
in the 1-hop neighbors N (v), and node D is in the 2-hop neighbors N (v).
≤1 ≤2
This GNN-SCM induces its causal structure G from Graph G, as discussed in
Definition 2.
A.1 GNN-SCM construction
Following[23],webuildaGNN-SCMM(G)thatlearnsfromthecausalstructure
G.Theendogenousvariablesarenodelabels{y ;v ∈A,B,C,D}.Theexogenous
v
variables in G are reference node A’s states: u ,u (in this example we con-
A1 A2
sider binary states A and A ), edges effects on reference node A: u ,u ,
1 2 A,B A,C
and neighbor nodes’ effects on reference node A: u ,u ,u . All of these la-
B C D
tentvariablesareassumedtoacceptthesameprobabilityP(U)asaprobability
function defined over the domain of U since we don’t want to input new specific
information. F is a set of functions based on the observable and latent variables
discussed above. One should consider that u and u are not independent.
vi vj
As discussed in Theorem 1, we construct the GNN-SCM M(G) based on
graph G as M(G)=:(U,V,F,P(U)), where:Graph Neural Network Causal Explanation 19
 
u ,u
M(G)=VU
: :=
=

{

Auu
f
f
f,
A
A
ABA
A
B
(
(
(1
, ,B
B
B
C,u
C,
C
,,
,uA
u,
C
u,A
D2
AAu
,, 1C
D
D
1}
,,
U, uu
A
AA,
2
2D
1
)
),
=
=u
u
A=
(
(2
(
()
(
({
¬
¬0
=
B, C1 f}
⊕
⊕A(
U
UB
A
A,
1
1u
)
)A
∨
∨1,
U
Uu
A
AA
,
,2
B
C) ))∧ ⊕⊕f
uuA
AA(C
22
)),u A1,u A2)∧f A(D)
F
P(: U=
)
:=f
f
f
fA
B
C
D
(cid:40)(
((
(D
P
=uu
uCB
D
()
Pu,,
)=
uu
A
(=
uAA
1¬
),,
¬CBD
=u))
D
)==
P
=(¬¬
u
Puu
ACB
(2
u)∧∧
=
)¬¬
P
=uu
AA
(
Pu,, CB
A
(u,B) )= =P P( (u
uA,C
))
=1/8
A,D B C D
(8)
A, B, C, P(U)
U =0 U =1
B B
U =0 UU ==10 U =1
C CC C
U =0UUU ===010UUU ===110U =1
A,B AAA,,,BBB AAA,,,BBB A,B
U =0UUUU ====1000UUUUUU ======110100UUUU ====1110U =1
A,C AAAA,,,,CCCC AAAAAA,,,,,,CCCCCC AAAA,,,,CCCC A,C
U =0UUUUU =====10000UUUUUUUUUU ==========1101001000UUUUUUUUUU ==========1110110100UUUUU =====11110U =1
A1 AAAAA11111 AAAAAAAAAA1111111111 AAAAAAAAAA1111111111 AAAAA11111 A1
U =0UUUUUU ======100000UUUUUUUUUUUUUUU ===============110100100010000UUUUUUUUUUUUUUUUUUUU ====================11101101001101001000UUUUUUUUUUUUUUU ===============000001111111111UUUUUU ======111110U =1
A2 AAAAAA222222 AAAAAAAAAAAAAAA222222222222222 AAAAAAAAAAAAAAAAAAAA22222222222222222222 AAAAAAAAAAAAAAA222222222222222 AAAAAA222222 A2
Fig.8: Logic tree for example’s SCM
In the GNN-SCM M(G), the set of functions F should be the exact form of
the interactions between variables. The argument of each function is the input
thatthisfunctionwilldooneofthelogicaloperationsOR,AND,NOT,orXOR20 Arman Behnam and Binghui Wang
onthem.Forexample,f (B,C,D,u ,u )=f (B,u ,u )∧f (C,u ,u )∧
A A1 A2 A A1 A2 A A1 A2
f (D)calculatesthevalueofeffectsonreferenceobservablevariableAwhenwe
A
assumethatallobservablevariablesB,C,D,andeachstateA ,orA arecause
1 2
of reference observable variable A to accept a specific value A , or A . In this
1 2
setting, all of these variables should be feasible and have value. For simplicity,
we consider all observable variables as binary, and the probability of these states
as uniform distribution.
In the graph provided, intervention do(C = 1) means forcing the value of
node C’s label to be 1, and the probability P(A = 1|do(C = 1)) determines
the respective causal effect for a treatment A = 1. In the GNN-SCM M(G),
this effect is denoted as u . Since C has an edge to the reference node A,
C
there is also a latent variable u , and its causal effect can be calculated by
A,C
P(do(C =1)|e =1).Inthisexample,thereisonenodeD thatdoesnothave
A,C
an edge to A. So, here we just calculate its causal effect on node A by latent
variable u .
D
Forthecausaleffectcalculation,weneedatruthtableshowinginducedvalues
of M(G). The logic tree we used for this table is shown in Figure 8, which has
seven layers since there are seven distinct latent variables(u , u , u , u ,
B C A,B A,C
u ,u )eachacceptingthevalueof0(it’snotthecause)or1(itisthecause).
A1 A2
A.2 SCM tables
By interpreting the variables and their values from the logic tree, the truth
table will be in four different states. If none of the 1-hop neighborhood nodes’
observablevariableaffectsreferencenodeA,ifoneofthem(nodeBorC)hasan
effect,orotherwisebothofthemaffectthereferencenode.ProbabilitiesinP(U)
are labeled from p to p for convenience, which are 7 binary variables(layers
0 63
in the logic tree).
In all provided table rows, u = 0 and D = 1, meaning D is not the cause
D
andthelatentvariableofitis1.However,thereisnoedgebetweennodesAand
D, we need to mention this in our calculations. For simplicity, we just showed
the cases that u = 0, but there are the same truth tables with u = 1 and
D D
D =0byprobabilitiesp top .Giventheprobabilitiesfromthetruthtables,
64 127
we can define them as follows:
P(U):=Unif(0,1) =⇒ p =p =p =...=p =...=p =1/128
0 1 2 63 127
A.3 GNN-SCM results
The capability of the tables shows that our specified GNN-SCM M(G) can
calculate all queries from each PCH layer [25]. In continue, we will calculate
an example for each layer:
An association layer query such as P(A=1|C =1) which is the probability
of observable variable A to be 1 given observable variable C to be 1, can be
computed as:Graph Neural Network Causal Explanation 21
u u u u u u u B C D A P(U)
B C D A,B A,C A1 A2
0 0 0 0 0 0 0 1 1 1 ¬(B∧C) p
0
0 0 0 0 0 1 0 1 1 1 (B∧C) p
1
0 0 0 0 0 0 1 1 1 1 (B∧C) p
2
0 0 0 0 0 1 1 1 1 1 ¬(B∧C) p
3
0 0 0 1 0 0 0 0 1 1 1 p
4
0 0 0 1 0 1 0 0 1 1 1 p
5
0 0 0 1 0 0 1 0 1 1 0 p
6
0 0 0 1 0 1 1 0 1 1 0 p
7
0 0 0 0 1 0 0 1 0 1 1 p
8
0 0 0 0 1 1 0 1 0 1 1 p
9
0 0 0 0 1 0 1 1 0 1 0 p
10
0 0 0 0 1 1 1 1 0 1 0 p
11
0 0 0 1 1 0 0 0 0 1 1 p
12
0 0 0 1 1 1 0 0 0 1 1 p
13
0 0 0 1 1 0 1 0 0 1 0 p
14
0 0 0 1 1 1 1 0 0 1 0 p
15
Table 4: Example’s SCM truth table where u =0, and u =0 (node B and C are
B C
the cause of node A to get a specific value). Probabilities in P(U) are labeled from p
0
to p for convenience.
15
u u u u u u u B C D A P(U)
B C D A,B A,C A1 A2
1 0 0 0 0 0 0 0 1 1 ¬(B∧C) p
16
1 0 0 0 0 1 0 0 1 1 (B∧C) p
17
1 0 0 0 0 0 1 0 1 1 (B∧C) p
18
1 0 0 0 0 1 1 0 1 1 ¬(B∧C) p
19
1 0 0 1 0 0 0 0 1 1 1 p
20
1 0 0 1 0 1 0 0 1 1 1 p
21
1 0 0 1 0 0 1 0 1 1 0 p
22
1 0 0 1 0 1 1 0 1 1 0 p
23
1 0 0 0 1 0 0 0 0 1 1 p
24
1 0 0 0 1 1 0 0 0 1 1 p
25
1 0 0 0 1 0 1 0 0 1 0 p
26
1 0 0 0 1 1 1 0 0 1 0 p
27
1 0 0 1 1 0 0 0 0 1 1 p
28
1 0 0 1 1 1 0 0 0 1 1 p
29
1 0 0 1 1 0 1 0 0 1 0 p
30
1 0 0 1 1 1 1 0 0 1 0 p
31
Table 5: Example’s SCM truth table where u =1, and u =0 (only node C is not
B C
thecauseofnodeAtogetaspecificvalue).ProbabilitiesinP(U)arelabeledfromp
16
to p for convenience.
31
P(A=1,C=1)
P(A=1|C=1)=
P(C=1)
p1+p2+p4+p5+p17+p18+p20+p21
= =0.5
p0+p1+p2+p3+p4+p5+p6+p7+p16+p17+p18+p19+p20+p21+p22+p23
(9)22 Arman Behnam and Binghui Wang
u u u u u u u B C D A P(U)
B C D A,B A,C A1 A2
0 1 0 0 0 0 0 1 0 1 ¬(B∧C) p
32
0 1 0 0 0 1 0 1 0 1 (B∧C) p
33
0 1 0 0 0 0 1 1 0 1 (B∧C) p
34
0 1 0 0 0 1 1 1 0 1 ¬(B∧C) p
35
0 1 0 1 0 0 0 0 0 1 1 p
36
0 1 0 1 0 1 0 0 0 1 1 p
37
0 1 0 1 0 0 1 0 0 1 0 p
38
0 1 0 1 0 1 1 0 0 1 0 p
39
0 1 0 0 1 0 0 1 0 1 1 p
40
0 1 0 0 1 1 0 1 0 1 1 p
41
0 1 0 0 1 0 1 1 0 1 0 p
42
0 1 0 0 1 1 1 1 0 1 0 p
43
0 1 0 1 1 0 0 0 0 1 1 p
44
0 1 0 1 1 1 0 0 0 1 1 p
45
0 1 0 1 1 0 1 0 0 1 0 p
46
0 1 0 1 1 1 1 0 0 1 0 p
47
Table 6: Example’s SCM truth table where u =0, and u =1 (only node B is not
B C
thecauseofnodeAtogetaspecificvalue).ProbabilitiesinP(U)arelabeledfromp
32
to p for convenience.
47
u u u u u u u B C D A P(U)
B C D A,B A,C A1 A2
1 1 0 0 0 0 0 0 0 1 ¬(B∧C) p
48
1 1 0 0 0 1 0 0 0 1 (B∧C) p
49
1 1 0 0 0 0 1 0 0 1 (B∧C) p
50
1 1 0 0 0 1 1 0 0 1 ¬(B∧C) p
51
1 1 0 1 0 0 0 0 0 1 1 p
52
1 1 0 1 0 1 0 0 0 1 1 p
53
1 1 0 1 0 0 1 0 0 1 0 p
54
1 1 0 1 0 1 1 0 0 1 0 p
55
1 1 0 0 1 0 0 0 0 1 1 p
56
1 1 0 0 1 1 0 0 0 1 1 p
57
1 1 0 0 1 0 1 0 0 1 0 p
58
1 1 0 0 1 1 1 0 0 1 0 p
59
1 1 0 1 1 0 0 0 0 1 1 p
60
1 1 0 1 1 1 0 0 0 1 1 p
61
1 1 0 1 1 0 1 0 0 1 0 p
62
1 1 0 1 1 1 1 0 0 1 0 p
63
Table 7: Example’s SCM truth table where u =1, and u =1 (node B and C are
B C
notthecauseofnodeAtogetaspecificvalue).ProbabilitiesinP(U)arelabeledfrom
p to p for convenience.
48 63
An intervention layer query such as P(A=1|do(C =1)) which is the proba-
bilityofAbeing1afterthisinterventiononC.Itseekstounderstandthecausal
effect of setting C to 1 on outcome A . do(C = 1) represents an intervention
where the variable C is actively set to 1 to control the variable directly, essen-Graph Neural Network Causal Explanation 23
tially breaking its usual causal edges with other variables. This query can be
computed as:
P(A=1|do(C=1))=P(A=(B∧C)|C=1)∨P(A=1|C=0)
p1+p2+p17+p18
=
p0+p1+p2+p3+p4+p5+p6+p7+p16+p17+p18+p19+p20+p21+p22+p23
p24+p25+p28+p29+p36+p37+p40+p41+p44+p45+p52+p53+p56+p57+p60+p61
+
p8+p9+...+p14+p15+p24+p25+p26+p27+p28+p29+p30+p31+p32+...+p63
=0.25+0.33=0.58 (10)
Fortheimplementation,weneedtheobserveddatageneratedfromthegraph.
One generated data can be: {A : True,B : False,C : False,D : True,u :
A1
1,u : 0,u : 1,u : 1,u : 0,u : 1,u : 0,A|C : 1}. The Fraction of
A2 A,B A,C B C D
samples that satisfy each function of the GNN-SCM M(G): {f :0.156458,f :
A B
0.249319,f : 0.25009,f : 0.50059}. These values show the range and cen-
C D
tral tendency of the probabilities across the 500 trials, indicating a degree of
variability in the outcomes based on the random generation of the probabilities.
A.4 Example’s GNN-NCM
Withrespecttoliterature,anNCMisasexpressiveasanSCM,andallNCMsare
SCMs. The causal diagram constraints are the bias between SCMs and NCMs.
InourspecifiedGNN-SCMM(cid:99)(G),andGNN-NCMM(cid:99)(G,θ)),allcausalinforma-
tion(observable,andlatentvariables)comesfromthecausalstructuredefinedin
Definition. 2. The respective GNN-NCM M(cid:99)(G,θ) is constructed as a proxy of
the exact GNN-SCM M(G). Based on Equation. 3, reference node A is chosen
as the target node for causal structure G. This GNN-NCM M(cid:99)(G,θ) is an induc-
tivebiastypeoftheGNN-SCMasM(cid:99)(G,θ)=:⟨U(cid:98),V,F(cid:98),P(U(cid:98))⟩.Theconstruction
of the corresponding GNN-NCM that induces the same distributions for our
example dataset is as follows:

M(cid:99)(G,θ))= PFVU(cid:98)
(cid:98)
(:
::
U(cid:98)=
==
){


{
:A
=U(cid:98)
f
f
f
f(cid:98)
(cid:98)
(cid:98)
(cid:98),
?A
B
C
D} B,
(
((
(D
U
AA
A, (cid:98)C
,,U ,)(cid:98)
UU,
U(cid:98)(cid:98)
(cid:98)=D=
))
)?}
==
=[0
??
?,1]
(11)
We know the observable variables V values, but there is no clue about the
exact values of latent variables U, so we have to estimate them by functions F(cid:98)
basedontheinformationinthegivencausalstructureG.First,wehavetobuild
the causal structure G given example graph G.
(cid:8)
G(G)= V ={A,B,C,D}},
v
U =(cid:8) U :{u ,u ,u ,u ,u }∪{U :{u ,u }(cid:9) (12)
v vi B C D A1 A2 v,vi A,B A,C24 Arman Behnam and Binghui Wang
Fig.9: Result of GNN-NCM M(cid:99)(G,θ)) for toy example
Second, given the probabilities from the truth tables, we have:
P(U(cid:98)):=Unif(0,1) =⇒ p
0
=p
1
=p
2
=...=p
63
=1/256
At last, based on Algorithm 1, we train the GNN-NCM to find the functions.
The GNN-NCM extends the GNN-SCM to utilize the power of neural networks
in capturing complex patterns in the dataset. The process begins with sampling
from a prior distribution to simulate the unobserved confounders in the causal
process. Since the reference node in the GNN-SCM was node A, we assign value
1 to it and want to see how GNN-NCM finds the causal effects on this node in
graph G. The result of the Algorithm 2, are:
WhenthetargetnodeisA,theexpectedprobabilityis0.24082797765731812
When the target node is B, the expected probability is 0.2353899081548055
WhenthetargetnodeisC,theexpectedprobabilityis0.18209974467754364
WhenthetargetnodeisD,theexpectedprobabilityis0.12958189845085144
Hence, the final causal explanatory subgraph Γ based on the results is the
GNN-NCM M(cid:99)(G,θ)) with target node A is:
(cid:8)
G(G)= V ={A=(B∧C),B =1,C =1,D =0}},
v
(cid:8)
U = U :{u =0,u =0,u =1,u =0,u =1}
v vi B C D A1 A2
(cid:9)
∪{U :{u =1,u =1} (13)
v,vi A,B A,C
The GNN-NCM M(cid:99)(G,θ)) structure is:
 (cid:110)
M(cid:99)(G,θ))= FVU P(cid:98)
(cid:98)
(:
::
U(cid:98)=
==
∪
){


{
:A
=U
U
f
f
f
f(cid:98)
(cid:98)
(cid:98)
(cid:98)A
B
C
D=v
v
(cid:40)i
(
(,
(
(v
U
P
=AA
A(
(cid:98):
i
B
,,
(,{
):
PuUU
Uu
(cid:98)(cid:98)
(cid:98){
=∧
A
(B
u
))
)
u1CA
f ==(cid:98)
=
)=
A, )B
=,
(
ff
f0
(cid:98)(cid:98)
(cid:98)B
u (cid:98)
CB
D
)=, PAu
((
=(=
(u1
u
(cid:98)(cid:98)C
1
u
(cid:98)
u,
CB
PD=1
Au=
,
(2=A
=
=C
0
u)0
,
,C,
0
=0u
1(cid:98)=u
,
),
)A=
uD
u (cid:98)(cid:98)
P
=1
2
AA,1
(=
=
,D
,
Pu}
CB(cid:111)
A
(1
1=
=
u,=,
,
Bu
u (cid:98)
)0
11A
)A}
))
=1 =,B=
P
P=
(0 (u,
u1
Au
,
,A
u (cid:98)
C
)2
A
),= C1
=
=}
1 1)
/8
(14)
A,D B C DGraph Neural Network Causal Explanation 25
B More Background on Causality
According to the literature, causality interprets the information by the Pearl
Causal Hierarchy (PCH) layers [25].
Definition 4 (PCH layers). The PCH layers L for i ∈ 1,2,3 are: L asso-
i 1
ciation layer, L intervention layer, and L counterfactual layer.
2 3
Definition 5 (G-Consistency). LetG bethecausalstructureinducedbySCM
M∗. For any SCM M, we say M is G-consistent w.r.t M∗ if M imposes the
same constraints over the interventional distributions as the true M∗.
Definition 6 (G-Constrained NCM). Let G be the causal structure induced
by SCM M∗. We can construct NCM M(cid:99) as follows: 1) Choose U(cid:98) s.t. U(cid:98)C ∈U(cid:98),
where any pair (V ,V ) ∈ C is connected with a bidirected arrow in G and is
i j
maximal; 2) For each V ∈ V, choose Pa(V ) ⊆ V s.t. for every V ∈ V,
i i j
V ∈ Pa(V ) iff there is a directed arrow from V to V in G. Any NCM in this
j i j i
family is said to be G-constrained.
Theorem 5. Any G-constrained NCM M(cid:99)(θ) is G-consistent.
C Proofs
Inthissection,weprovideproofsofthetheoremsinthemainbodyofthepaper.
C.1 Proof of Theorem 1
Theorem 1 (GNN-SCM). For a GNN operating on a graph G, there exists
an SCM M(G) w.r.t. the causal structure G of the graph G.
Proof. A GNN is a neural network operating on a graph G = (V,E) including
set of nodes V and set of edges E. Recall the GNN background in Section 3, the
GNN learning mechanism for a node v in the l-th layer can be summarized as:

node embeddings hl−1 from previous layer l−1, for u∈{v}∪N(v)
 v
GNN(v)≡ message ml =MSG(hl−1,hl−1,e ) for current layer l
u,v u v u,v
aggregated message hl =AGG(ml |u∈N(v)) for current layer l
v u,v
(15)
where the above process is iteratively performed k times for a k-layer GNN.
In doing so, each node v will leverage the information from all its within k
neighborhoods. We denote the k-layer GNN learning for v as:
(cid:8) (cid:9)
GNN(G)= node embed: {h }∪{h :u∈N (v)},message: {m ,u∈N (v)} ,
v u ≤k u,v ≤k
where h is v’ node feature and we omit the dependence on node v for notation
v
simplicity.26 Arman Behnam and Binghui Wang
Bydefinitionfromliterature,anSCMMisafour-tupleM≡(U,V,F,P(U)).
In this specification of ours, an SCM M(G) is a G-consistent four-tuple model
based on a set of observable variables V, a set of latent variables U, a set of
functions F, and the probability of latent variables P(U). Recall in Definition
2, where the causal structure G(G) of a given graph G (centered on a reference
node v) was defined. Now, the correspondence of the GNN learning on G and
the causal structure G centered on a node v can be written as:

obs.vars:{yv}∪{yvi :vi∈N≤k(v)}≡nodeembed.{hv}∪{hu:u∈N≤k(v)}
G(G)≡GNN(G) ⇐⇒ pla rt o. bv aa br is l: it{ yU ov fi la: tv ei nt∈ vN ar≤ iak b( lv e) s} P∪ ({ UU )v ≡,vi D: omev (, {vi hv∈ })E}≡ msg:{mu,v,u∈N≤k(v)}
(16)
Hence, there exists a GNN-SCM M(G) that induces the causal structure G
of G, as below:
 
G(G)=n edo gd ee ss ee tt EV (( GG ))
⇐⇒
M(G)≡U V: :{ {U yvv }i ∪:v {i yv∈ i;N vi≤ ∈k(v N) ≤} k∪ (v{ )U }v,vi :ev,vi ∈E}
n edo gd ee ee ffff ee cc tt == {{ UU vv ,i v}
i}
PF (: U{ )f :1, Pf (2 U,. v. i. ), ,f Pn (} U∈ v,F vi; )f(U)→V
(17)
C.2 Proof of Theorem 2
Theorem 2 (GNN-NCM). Given causal structure G of a graph G and the
underlying GNN-SCM M(G), there exists a G-constrained GNN-NCM M(cid:99)(G,θ)
that enables any inferences consistent with M(G).
Proof. From the literature, we know there exists a SCM M that includes exact
valuesofobservableandlatentvariablesthroughstudyingthecausesandeffects
within the SCM structure. First, we show a lemma that demonstrates the in-
heritance of neural causal models (NCMs) (see its definition in Section B) from
SCMs, which are built upon Definition 5, Definition 6 and Theorem 5.
Lemma 1 ( [43]). All NCMs M(cid:99)(θ) (parameterized by θ) are SCMs (i.e.,
M(cid:99)(θ)≺M). Further, any G-constrained M(cid:99)(θ) (see Definition 6) has the same
empirical observations as the SCM M, which means G-constrained NCMs can
be used for generating any distribution associated with the SCMs.
By Lemma 1, we know a G-constrained NCM M(cid:99)(θ) inherits all properties of
the respective SCM M and ensures causal inferences via G-constrained NCM.
In our context, we need to build the corresponding G-constrained GNN-NCM
M(cid:99)(G,θ) for the GNN-SCM defined in Equation 17. With it, we ensure all G-
constrained GNN-NCMs M(cid:99)(G,θ) are GNN-SCMs (M(cid:99)(G,θ)≺M(G)), meaning
these GNN-NCMs can be used for performing causal inferences on the causal
structure G(G). First, based to the four-tuple SCM M≡(U,V,F,P(U)), a G-
constrained NCM M(cid:99)(θ) = (U(cid:98),V,F(cid:98)(θ),P(U(cid:98))) can be defined. In our scenario,
we can define the set of functions F(cid:98)(θ) of the G-constrained GNN-NCM as:Graph Neural Network Causal Explanation 27
F(cid:98)(θ)={f(cid:98)vi(u (cid:98)vi,u (cid:98)vi,vj;θ vi):v
i
∈V(G)}≈{f 1,f 2,...,f n}∈F; f(U vi)→v i,
From Theorem 1, there exists a GNN-SCM M(G) for a GNN operating on
a graph G. Also the causal structure G(G) in Definition 2 naturally satisfies
Definition 6. Then, with respect to Equation 3, our G-constrained GNN-NCM
M(cid:99)(G,θ) based on underlying GNN-SCM M(G) is defined as:

U V(cid:98) : := ={ {U y(cid:98)v }i ∪:v {i y∈ ;N v≤ ∈k( Nv)}∪ (v{ )U(cid:98) }v,vi :e v,vi ∈E}
M(G) ⇐⇒ M(cid:99)(G,θ)≡ v vi i ≤k
PF(cid:98) (( Uθ (cid:98)) ):= :={ {f(cid:98)
Uv (cid:98)i
v( iu
(cid:98)v
∼i, Uu
(cid:98) nvi i, fv (j
0) ,( 1θ
)vi
:) v: iv
∈i
V∈ }V ∪}
{T
k,vi
∼N(0,1)}
C.3 Proof of Theorem 3
Theorem 3 (Node explainability). Let a prediction for a graph G be ex-
plained.Anodev ∈Giscausallyexplainable,ifpM(cid:99)(G(G),θ)(y )canbecomputed.
v
Proof. Inagraphclassificationtask,GNNpredictsagraphlabely foragraph
(cid:98)G
G with label y . The GNN explanation measures how accurately did the GNN
G
classifythegraphbyfindingthegroundtruthexplanationΓ inthegraphG.In
G
otherwords,thegraphexplanationdemandsthenodesinΓ shouldbeasaccu-
G
rate as possible. That is, if y = y ;∀v ∈ Γ , then GNN explanation explained
v (cid:98)v G
G’s prediction accurately.
BasedonTheorem2,G-constrainedGNN-NCMM(cid:99)(G,θ)inducescausalstruc-
ture G based on the reference node v ∈ V(G). So, the trained G-constrained
GNN-NCM estimates node effect U , and edge effect U defined in Equa-
vi v,vi
tion 2. Note that all the effects are respective to the reference node v, and if v
changes,thecausalstructureGwillbealsochanged,andasaresultG-constrained
GNN-NCM will be completely different.
According to Equation. 5, a G-constrained GNN-NCM M(cid:99)(G,θ) calculates
pM(cid:99)(G,θ)(y ) as the expected value of all the causal effects from the neighbor
v
nodes v , i.e. do(v ), on the reference node v:
i i
(cid:16) (cid:17)
∀v ∈V(G),(∃v ∈N (v)) =⇒ pM(cid:99)(G(G),θ)(y )≥0 (18)
i ≤k v
As the expected value was calculated for v, we can explain v’s node label based
on the outcome of pM(cid:99)(G(G),θ)(y ).
v
C.4 Proof of Theorem 4
Theorem 4 (Explainable node expressivity). An explainable node v has
expressivity defined as exp v(M(cid:99)(G,θ))=(cid:80) yvy vpM(cid:99)(G,θ)(y v).28 Arman Behnam and Binghui Wang
Proof. We know there is a value for the expected effect on an explainable node
v ∈ V(G) as pM(cid:99)(G(G),θ)(y ). This probability was calculated by the trained G-
v
constrained GNN-NCM M(cid:99)(G,θ).
Forourpurpose,weonlyconsidertheassociationandinterventionlayers.The
associationlayerisabouttheobservableinformationprovidedbythedata,while
the intervention layer in this paper is an explanation via doing interventions.
Association Layer L : G=(V,E), y ∈Y, Γ
1 G G
(19)
Intervention Layer L : do(v )=(y |y =x), U , U
2 i v vi vi v,vi
The explanation methods using information in the association layer is called
association-basedexplanation.Instead,theG-constrainedGNN-NCMM(cid:99)(G(G),θ)
istrainedoninterventions—anodev intheneighborhoodofthereferencenode
i
vprovidesthecausalexplanationinformationbasedontheinterventiondo(v )—
i
and leveraging this interventional layer information can causality interpret the
GNNpredictions. Toalignthis intrinsicexplanation informationfromcausal ef-
fects,weintroducethetermexpressivity.Rememberinprobabilitytheory,where
the expected value of a random variable provides a measure of the central ten-
dency of a probability distribution. For a discrete random variable Y with a
probability distribution p(y), the expected value of Y , denoted E(Y), is defined
as: E(Y )=(cid:80) y·p(y), where y ranges over all possible values of Y, and p(y) is
v y
theprobabilitythatY takesthevaluey.AccordingtoEquation5,pM(cid:99)(G(G),θ)(y )
v
includesallthecausaleffectfromtheneighhornodesony .Hence,theexpected
v
value of of the random variable node label Y will be defined as:
v
(cid:88)
E (Y )= y ·pM(cid:99)(G(G),θ)(y ),
L2 v v v
yv
where the subscript L means the expectation leverages the interventional layer
2
information.Notethatthisexpectedvalueisonlyfeasibleforthereferencenode
(i.e., v) upon which the G-constrained GNN-NCM M(cid:99)(G,θ) was built. This ex-
pected value is treated as the expressivity of the explainable node v that is
denoted as exp (M(cid:99)(G,θ)).
v
D Experiments
D.1 More experimental setup
CXGNN: The hyperparameter settings were determined through a systematic
searchandvalidationprocesstooptimizethemodel’sperformance.Thefollowing
hyperparameters were selected based on cross-validation:
– Learning rate: We test learning rates—0.001, 0.01, 0.1—to find the optimal
value, and the learning rate of 0.01 yielded the best results.Graph Neural Network Causal Explanation 29
Fig.10: Overview.Greennodeisthereferencenodev,BluenodesandRednodesare
node v’s 1-hop and 2-hop neighbors.
– Numberofhiddenlayers:Weconsideredarchitectureswith1,2,and3hidden
layers. A network with 2 hidden layers outperformed the others in terms of
both accuracy and convergence speed.
– Hiddenlayersize:Wetestedvarioushiddenlayersizes,including32,64,and
128 neurons per layer. A hidden layer size of 64 neurons struck a balance
between model complexity and performance.
– Batch size: We tested different batch sizes, ranging from 32 to 128. A batch
size of 64 was found to be suitable.
– Inaddition,asthebaselineGNNisGCN[16]thatisa2-layerneuralnetwork.
We hence use k =2 in CXGNN.
GNNExplainer: Its hyperparameters are detailed as follows:
– A dictionary to store coefficients for the entropy term and the size term for
learning edge mask and node feature mask. The chosen settings are:
• edge: entropy: 1.0, and size: 0.005
• feature: entropy: 0.1, and size: 1.0
– The number of epochs for training the explanation model: 200.
– Learning Rate: Used in the Adam optimizer for training, set to 0.01.
– The number of hops to consider when explaining a node prediction. It is
equal to the number of layers in the GNN.
PGMexplainer: It is for explaining predictions made by GNNs using Proba-
bilistic Graphical Models (PGMs), provides these hyperparameters:
– Number of perturbed graphs to generate: 10 for graph-level explanations
– How node features are perturbed: mean, for graph-level explanations.
– The probability that a node’s features are perturbed: 0.5
– The threshold for the chi-square independence test: 0.05
– Threshold for the difference in predicted class probability: 0.1
– Number of nodes to include in PGM: all nodes given by the chi-square test
are kept.30 Arman Behnam and Binghui Wang
Guidedbp: It is a form of Guided Backpropagation for explaining graph pre-
dictions, contains several hyperparameters and method-specific parameters:
– The loss function used to train the model: cross entropy.
– The number of hops for the k-hop subgraph, is implicitly set to the number
of layers in the GNN (i.e., k =2 in our results).
GEM: The GEM method has the below hyperparameters:
– Optimization Parameters:
• Learning Rate: 0.1, Gradient Clipping: 2, Batch size: 20, Number of
Epochs: 100, Optimizer: "Adam"
– Model Parameters:
• Hidden Dimension: 20, Output Dimension: 20, Number of Graph Con-
volution Layers: 2
– Explainer Parameters:
• Iterations to find alignment matrix: 1000, Number of mini-batches: 10
RCExp: The reinforced causal explainer for graph neural networks has the
below hyperparameters:
– Optimization Parameters:
• Learning Rate: 0.01, Weight Decay: 0.005, Number of Epochs: 100, Op-
timizer: "Adam"
– Model Parameters:
• HiddenDimension:64,32,OutputDimension:2,NumberofGraphCon-
volution Layers: 2
– Explainer Parameters:
• Output size of edge action rep generator: 64, Edge attribute dimension:
32
Orphicx: The causality-inspired latent variable Model for interpreting graph
neural networks has the below hyperparameters:
– Optimization Parameters:
• Learning Rate: 0.0005, Weight Decay: 0.01, Number of Epochs: 100,
Optimizer: "Adam", Early Stopping Patience: 20
– Model Parameters:
• Hidden Dimension: 32, Decoder Hidden Dimension: 16, Dropout rate:
0.5, Output Dimension: 108, Number of Graph Convolution Layers: 2
– Explainer Parameters: Number of causal factors: 5
D.2 More experimental results
Moreresultsonthesyntheticgraphsandreal-worldgraphsintermsoflosscurves
and node expressivity distributions are shown in the Figure 11-Figure 18.Graph Neural Network Causal Explanation 31
Fig.11: More results on BA+House. Top 3 figures: Loss curves of training the GNN-
NCMsonthegroundtruthnodes(greencurves)andnon-groundtruthones(redcurves);
Bottom 3 figures: Node expressivity distributions on three unsuccessful graphs. Green
bars(redbars)correspondtonodesthatare(NOT)inthegroundtruth.Samemeaning
for all the below figures.
Table8:Dominanttimecomplexityofthecomparedexplainers.N,E,d,h,T,L,K are
#nodes,#edges,#nodefeatures,#neurons,#trainingepochs,#layers,and#samples.
h=64≪d=∼1000.
GNNExp. O(T ∗L∗N ∗d2)
PGMExp.O(T ∗L∗N ∗d2+K∗(N +E))
Guidedbp O(T ∗L∗N)
GEM O(T ∗L∗N ∗d2)
RCExp. O(T ∗L∗(N +E))
OrphicX O(T ∗L∗N ∗d2)
CXGNN O(T ∗L∗N ∗h2)
E Complexity Analysis
WithinaGNN-NCM,wetrainafeed-forwardnetwork.WithanL-layernetwork
and each layer has h neurons, by training K epochs, the time complexity for a
graph with n nodes is O(T ∗L∗N ∗h2). Note that training GNN-NCMs for
all nodes independently can be easily paralleled via multi-threads/processors.
WealsoshowthedominantcomplexityofcomparedGNNexplainersinTable8.
Though computing GNN-NCM per node, we can see CXGNN is still more effi-
cientthanmostoftheSOTAexplainers(GNNExp.,PGMExp.,OrphicX,GEM).
F Discussion
Potential risk of overfitting. In our experiments, we tuned the number of
hidden layers and hidden neurons and observed that deeper/wider networks
indeed could cause overfitting. Through hyperparameters tuning, we found 232 Arman Behnam and Binghui Wang
Fig.12: More results on BA+Grid.
Fig.13: More results on BA+Cycle.
Fig.14: More results on Tree+House.Graph Neural Network Causal Explanation 33
Fig.15: More results on Tree+Grid.
Fig.16: More results on Tree+Cycle.
Fig.17: More results on Benzene.34 Arman Behnam and Binghui Wang
Fig.18: More results on Fluoride Carbonyl.
hidden layers with each layer having 64 neurons that can well balance between
model complexity and performance.
Practical issue of applying CXGNN to large graphs. We admit directly
running CXGNN in large graphs could have a scalability issue. One solution
to speed up the computation is using multi-threads/processors as all nodes can
be run independently in CXGNN. Note that all existing GNN explainers also
face the same scalability issue, even worse than ours as shown in Table 8. We
acknowledgeitisvaluablefutureworktodesignscalableGNNcausalexplainers.
True causal subgraph is not present. Our explainer and causality-inspired
onesareallbasedonthecommonassumptionthatagraphconsistsofthecausal
subgraph that interprets the prediction. If real-world applications do not satisfy
this assumption, all these explainers may not work well.
ComplexitycomparisonbetweenNCMandnotusingNCM(i.e.,SCM).
Computing the cause-effect in a graph via SCM is computationally intractable.
The complexity is exponential to the number of node/edge latent variables. In-
stead, training an NCM to learn the cause-effect is in the polynomial time.