Meta-Analysis with Untrusted Data
†
Shiva Kaul Geoffrey J. Gordon *
skkaul@cs.cmu.edu ggordon@cs.cmu.edu
Abstract
Meta-analysis a crucial tool for answering scientific questions. It is usually conducted on a
relatively small amount of “trusted” data, ideally from well-conducted, randomized, controlled
trials. Each trial i has features X i (describing its intervention and population), a true (unob-
served) causal effect U i, and an observed effect Y i ∼N(U i,V i), where V i is within-trial variance
duetoalimitednumberofparticipants. Givennewfeaturesx, thetaskistopredictaninterval
whichcontainsthetrueeffectuwithhighprobability. Traditionalalgorithmsignorethefeatures
(which induce heterogeneous effects) because the amount of trusted data is too small to train
a predictor of u given x. They make poor predictions when there is a lot of heterogeneity. In
this paper, we consider using untrusted data drawn from large observational databases, related
scientificliteratureandpracticalexperience. The(seeminglyaudacious)goalistomaintainrig-
orouspredictionofcausaleffects,despitepotentialconfoundinganderrorsintheuntrusteddata.
We achieve this goal with two new algorithms based on conformal prediction. This methodol-
ogy fundamentally produces rigorous prediction intervals, even when incorporating untrusted
data, but it does not address the problem of handling noise: we seek intervals for the new u,
but observe only Y and V. The first algorithm is appropriate when the trials are very large
(i.e.V i ≈0),someta-analysisisclosetonoise-freeprediction. Thisalgorithmusesanunderlying
noise-free conformal predictor (which takes U and predicts an interval for u) and determines
its worst-case predictions over all plausible U(cid:98) near Y. The second algorithm is designed for the
morepractical,challengingcaseofsmallertrials. Givenanewtrial’sxandv,wecanconformally
predictanintervalfory ∼N(u,v);theproblemis,wedon’tknowv. Atanypossiblev,weshow
√
theintervalfory canhaveO( v)width“shavedoff”toproduceanintervalforu. Then,wetake
the worst-case v: larger v leads to a larger interval for y, but more of it can be shaved. In order
for both of these algorithms to work, the underlying conformal prediction algorithm must be
analytically tractable and capable of incorporating prior information (the untrusted data). For
these purposes, we use fully-conformal kernel ridge regression. Under a novel condition called
idiocentricity (whichcanbeensuredbytakingtheridgeparameterλreasonablylarge),weshow
that fully-conformal KRR is simple and efficient.
On multiple biomedical datasets, we show that conformal meta-analysis resolves the challenge
of heterogeneity, delivering much tighter intervals than traditional algorithms. Furthermore, it
improves the rigor of such predictions, since traditional algorithms are based on large-sample
approximations with no finite-sample guarantees. It demonstrates qualitative improvements
in a case study using real data from a published meta-analysis on amiodarone. Overall, this
paper charts a radically new course for meta-analysis and evidence-based medicine, where het-
erogeneity and untrusted data are embraced to deliver more nuanced and precise predictions.
†
Computer Science Department, Carnegie Mellon University, Pittsburgh PA 15213
*Machine Learning Department, Carnegie Mellon University, Pittsburgh PA 15213
11 Introduction
A systematic review of a scientific question formally collects relevant, reliable evidence and answers
the question as precisely as the evidence allows. Roughly 30,000 systematic reviews are currently
published every year, either as standalone scientific papers or as part of clinical practice guide-
lines, by thousands of academic, professional, and regulatory organizations [Hoffmann et al., 2021].
Systematic reviews of the following questions each have over 2000 citations:
• Do anti-TNF antibodies for rheumatoid arthritis increase the risk of serious infections and
malignancies? [Bongartz et al., 2006]
• Does bariatric surgery improve or resolve the clinical and laboratory manifestations of type
2 diabetes mellitus? [Buchwald et al., 2009]
• Can oral and topical pharmacotherapies treat neuropathic pain? [Finnerup et al., 2015]
• Does exercise-based cardiac rehabilitation improve cardiovascular mortality in patients with
coronary heart disease? [Anderson et al., 2016]
• How effective is nicotine replacement therapy (gum, patches, spray, etc.) in achieving absti-
nence from smoking? [Hartmann‐Boyce et al., 2018]
Systematic reviews adhere to highly-scrutinized methodology [Higgins et al., 2019, Schünemann
et al., 2013, Page et al., 2021] and are widely considered to be the pinnacle of empirical evidence
[Guyatt et al., 1995, Murad et al., 2016]. They have a decisively influential role in healthcare and
relatedfields, especiallyincontentioussituationswheredifferentpartiesdisagreeorhavecompeting
interests. This is because systematic reviews are designed to be rigorous and unbiased, in a broad
sense [Sackett, 1979]: they should yield reliably correct answers, unblemished by personal opinions,
conflicts of interest, unproven assumptions, and/or confounding of causation by correlation.
Meta-analysis is the statistical core of most systematic reviews. A key goal of meta-analysis1 is to
learn, from the collected evidence, a predictor C of causal effect: given features x of a treatment
(encompassingitspopulation, intervention, comparisonandoutcomemeasure), thepredictedinter-
valC(x) ⊆ R should, withhighprobability, containthetrueeffectuofthetreatment. Asdescribed
here, meta-analysis models heterogeneity in the treatment and (in turn) its effect. For example,
changing the age of patients or the dosage of a drug corresponds to a change in x, which would
lead to a possibly different prediction C(x) of a different u. Unfortunately, prevalent meta-analysis
algorithms do not model heterogeneity, treating its consequences as inexplicable random noise in
u. This is because the complexity of meta-analysis is profoundly constrained by the stringent ex-
pectations placed upon systematic reviews: only a limited fraction of “trusted” evidence is allowed
in meta-analysis, which limits the kind of analysis that can actually be conducted.
Specifically,toavoidtheconfoundingbiasesofobservationalstudies,meta-analysisis(ideally)based
solely upon well-conducted, randomized, controlled trials. These allow causal questions (e.g. “what
is the effect of administering this drug?”) to be reliably answered. On average, about 10-20 RCTs
are included in the meta-analysis of a systematic review [Hoffmann et al., 2021]. Note that modern
meta-analyses, especially network meta-analyses, can be much larger; for example, recent meta-
analyses of glaucoma treatments, antipsychotics and antidepressants included 114, 402 and 522
trials, respectively [Li et al., 2016, Huhn et al., 2019, Cipriani et al., 2018]. Regardless, these
data represent a tiny fraction of the accumulated human experience with the empirical phenomena
1Meta-analysis also involves tasks such as estimating parameters with confidence intervals; see Section 2.2.
2STANDARD
monolithic prediction interval regardless of specific circumstances
META-ANALYSIS
POPULATION: acute AF patients dn oa mrr ao iw n trusted data
INTERVENTION: IV, moderate dosage
COMPARISON: only placebo
OUTCOME: normal rhythm within 1 day
How effectively does amiodarone
restore normal sinus rhythm to
patients with atrial fibrillation?
Figure 1: How meta-analysis presently answers scientific questions. The fundamental issues are that (1)
untrusteddata,constitutingthemajorityoftheevidencehierarchy[Guyattetal.,1995],arenotutilized,and
(2)heterogeneityinthetreatments(andtheireffects)isnotexplained. Toavoidheterogeneity,meta-analyses
arelimitedtorelativelynarrowdomainsX. OnlytrusteddatarelevanttoX areincluded. Thissmallamount
of data makes meta-analysis primitive and inflexible: it returns a single prediction interval C which does not
change based on specific treatment circumstances, such as the age of the patients or the dose of the drug.
of interest. Observational studies drawn from large databases now routinely involve millions of
patients [Hripcsak et al., 2016, All of Us Research Program Investigators, 2019]. The personal
experiences and opinions of practitioners, though less formal, are even more expansive.
Due to the paucity of trusted data, there is little hope in learning the complex relationship between
x and u. Present algorithms ignore x, so their (monolithic) prediction interval C(x) must be
wide enough to accommodate nearly all x. This inflexibility makes it difficult to establish good
scientific evidence in fields with heterogeneous treatments. For example, in exercise science, the
treatment effect may strongly depend on a large number of variables such as frequency, duration,
equipment, technique, age, and diet [Rippetoe, 2017, Ferreira et al., 2010]. In most psychological
research, the variation in u is attributable primarily to between-study heterogeneity rather than
within-study sampling variance [Stanley et al., 2018]. Some researchers believe that modeling
heterogeneityisessentialtoprogressinbehavorialscience,heraldingsuchstatisticaladvancementas
a “heterogeneity revolution” [Bryan et al., 2021]. They echo the hope of using covariates expressed
by DerSimonian and Laird [1986] in their seminal paper on random-effects meta-analysis.
1.1 Conformal Prediction
This paper demonstrates that untrusted data — with all its possible confounding, biases, and even
outrighterrors—canbeincorporatedintometa-analysiswhileremainingrigorousandunbiased. In
fact, this paper offers stronger, provable guarantees while weakening the assumptions traditionally
employed in meta-analysis. The solution is based upon conformal prediction [Vovk et al., 2005,
Shafer and Vovk, 2008, Lei and Wasserman, 2014]. The core idea of conformal prediction is that
the residual of a future trial probably isn’t extremely large compared to residuals of past trials.
3Given x, the conformal prediction interval C(x) simply excludes those u which would lead to an
extreme residual for a (hypothetical) future trial (x,u). Though only the trusted data are used for
these residual comparisons, the untrusted data can be incorporated into the underlying learning
algorithm producing the residuals. If they help produce an accurate predictor — that is, one with
small residuals — then the resulting interval C(x) is tight, because small changes to u dramatically
increase the rank of the residual. If the untrusted data don’t align with the trusted data, then C(x)
widens, but it remains correct, as it contains u with the required probability.
Whileconformalpredictionaptlymanagestheinclusionofuntrusteddata, therearetwounresolved
challenges when applying it to meta-analysis. The first challenge is noise: though we aim to predict
true effects u, the empirically observed effects y are blurred by limited trial sizes. Mathematically,
this is modeled as Gaussian noise y ∼ N(u,v), where v may depend on x and u. This noise is
curiouslychallengingtomanage,sincesmall(highnoise)studiescandifferfundamentallyfromlarge
(low noise) studies. This reflects difficulties in clinical practice, where large-scale trials routinely
fail to confirm the results of smaller ones [Ioannidis, 2005, Komajda et al., 2010, Manson et al.,
2019]. This problem is serious enough that meta-analyses attempt to detect it using funnel plots
[Light and Pillemer, 1984], whose validity is controversial [Lau et al., 2006].
The second challenge arises from limited sample size2 of n ≤ 500. When conformal prediction
is applied in practice, the residuals are usually computed on a held-out sample of data, to avoid
invalid comparisons between the n training residuals and the ‘fresh’ test residual for the (n+1)’th
trial. Such split conformal prediction is not feasible when n is small, since we cannot afford to
partition the data. A less common, but more efficient, approach is full conformal prediction. This
turns the test residual into a training residual, making the comparisons valid again, by running the
learning algorithm on all n+1 samples — for all hypothetically possible values of u. This may
pose a severe computational burden, and complicates efforts to handle noise.
1.2 Our Contributions
This paper resolves the aforementioned challenges of applying conformal prediction, giving rise to
conformal meta-analysis. It culminates in two meta-analysis algorithms, which both consist of the
followinglayers: (1)arepresentationofuntrusteddataasapriorprobabilitydistributionoverplau-
sible relationships between x and u, (2) a learning algorithm which takes the prior and the training
data, and returns an updated posterior distribution, (3) an efficient and simple implementation of
full conformal prediction, operating upon residuals produced by the learning algorithm, and (4) a
strategy for handling noise, exploiting the simplicity of the underlying conformal prediction.
Layers (1) and (2) are familiar: the untrusted data are represented as a Gaussian process, defined
by a mean function µ(x) and a kernel function κ(x,x(cid:48)). The learning algorithm is Gaussian process
regression, also known as Bayesian inference with a conjugate Gaussian process prior and a nor-
mal likelihood, or kernel ridge regression (KRR). This algorithm is capable of arbitrarily-powerful
nonlinear regression and can incorporate essentially any source of untrusted data. It has a hyper-
parameter λ corresponding to either the variance of the likelihood or the ridge penalty. Thus, layer
(3) amounts to fully-conformal KRR. Burnaev and Nazarov [2016] already derived fully-conformal
KRR for general λ. Though their algorithm is computationally efficient, it returns a general pre-
diction set (a union of disjoint intervals and isolated singletons) which isn’t amenable to analytic
2Modeling heterogeneity allows more expansive questions to be reviewed, generally leading to larger n.
4Prediction interval containing true effect with high probability CONFORMAL
Specific (e.g. 400 mg oral QID for 60+ males with persistent AF) META-ANALYSIS
POPULATIONS: any age, AF of any duration, … dob mro aa ind trusted data
INTERVENTIONS: any dosage, IV or oral, …
COMPARISONS: placebo, standard care, …
OUTCOMES: normal rhythm within 1 day, ... untrusted data
How effectively does amiodarone
restore normal sinus rhythm to
patients with atrial fibrillation?
Figure2: Thispaperchangeshowmeta-analysisanswersscientificquestions. First,arelativelybroaddomain
X for the meta-analysis is determined, possibly through further interaction with the user. This allows more
expansive questions which include more data. Next, both trusted and untrusted data relevant to X are
retrieved. Conformal meta-analysis takes these and produces not just a single interval, but a predictive
model C. Given specific treatment circumstances x, the model predicts an interval C(x) which, under
standard assumptions, contains the true effect with high probability.
reasoning. We show that, for practically sensible settings of λ, these complexities can be elimi-
nated. This is because sufficiently high λ makes KRR idiocentric: as u varies, the residual for
(x,u) changes more than the other residuals. Under this condition, and some mild approximation,
fully-conformal KRR can be simplified to computing quantiles in two lists.
If we had observed true effects U rather than noisy Y, then just running fully-conformal KRR
would yield a satisfactory interval. We don’t know U, but we do know the variances V, so we
know that, outside a rare event of probability δ, U lies in some region U around Y. Thus, the
statistical problem of covering u reduces to a purely computational problem: bound the intervals
generated by all plausible U(cid:98) within U. This can be formulated as an integer quadratic program,
whose notional computational content is somewhat daunting: an infinite-dimensional regression is
run an infinite number of times, for each possible u in the interval, and then the range of such
intervals is bounded over infinitely many U(cid:98). Fortunately, due to the structure of fully-conformal
KRR, the program can be bounded by a simple matrix norm computation. This bound is tight
for V ≈ 0. However, due to this algorithm’s somewhat blunt and pessimistic statistical analysis, it
suffers when meta-analysis departs from regression (i.e. there are trials having V i (cid:29) 0).
The second algorithm is more robust to trials of limited size. Rather than reasoning about the
worstpossibletraininginputU whenconformallypredictinguasC(x),thesecondalgorithmadopts
the complementary strategy: it reasons about the worst possible test input v when conformally
predicting y as C(x,v). In the latter approach, both V and v are presented to fully-conformal
KRR, so its residuals can incorporate noise-correcting terms. More noise tends to produce larger
residuals, so we naturally subtract more from residuals with higher V i. This approach effectively
reduces the importance of small trials, whereas the previous algorithm had to consider a large
5range for their possible true effects. A parameter η > 0 controls the extent of noise correction.
η = 1 subtracts the full expectation of the noise. Smaller η is less corrective, but has an advantage:
√
the interval C(x,v) is only ηv wider than C(x,0). η therefore controls the aggressiveness of the
√
following strategy: since we don’t know v, just return C(x,0), “shaving” the ηv growth around it.
Since shaving might inadvertently cut off some u, fully-conformal KRR must be run with a higher
underlying confidence level, which requires larger n. Thus, conformal meta-analysis is feasible with
a small number of large trials, but not a small number of small trials.
Our experiments have two goals: (1) to quantify how much conformal meta-analysis could improve
predictions when used, as intended, with large amounts of untrusted data, and (2) to more qualita-
tively assess, before such data are available, how it would impact the experience of producing and
consuming systematic reviews. At a high level, we find that conformal meta-analysis could improve
how the medical community interacts with evidence.
2 Preliminaries
These are the predictive goals of meta-analysis.
Predicting Effects. Let P be a distribution over features x ∈ X, true effects u ∈ R and variances
v > 0. For i ∈ {1,...,n}, let (X i,U i,V i) be exchangeable samples from P . Let Y i = U i+E i, where
E i ∼ N(0,V i) are independent. Let µ : X → R and κ : X ×X (cid:55)→ R be fixed (with respect to P ) mean
and positive-definite kernel functions, respectively. From µ, κ, X, Y, and V, produce a prediction
band C such that P(u ∈ C(x)) ≥ 1−α for a desired confidence level α ∈ (0,1).
Predicting Trials. Sameasabove, exceptC alsotakesv, andshouldsatisfyP(y ∈ C(x,v)) ≥ 1−α,
where y = u+(cid:15) for independent (cid:15) ∼ N(0,v).
Thefirsttaskismorepracticallyusefulandtechnicallyinvolved. However,sinceuisnotobservable,
but y is, the second task is more easily verifiable. It is not immediately clear which task is more
challenging, in the sense of needing wider intervals. On one hand, y has inherently more variance
than u. On the other, the prediction of u is made without knowing v, which might otherwise
distinguish between small and large trials having characteristically different u. The rest of this
section is a review, thoroughly describing the origin and purpose of these tasks. It can be skipped
by readers interested solely in the novel technical developments of this paper.
2.1 Outcomes and Effects
Let x ∈ X be features describing a treatment. This consists of the prospectively-set criteria of its
population, intervention, comparison, and measure of outcome, commonly abbreviated as PICO
[Richardson et al., 1995]. For example, x may include the duration of an exercise program and
the minimum age of its participants. It may also include auxiliary information that was collected
passively and retrospectively, though (as described in the next section) this may complicate the
interpretation of the meta-analysis. x does not have to be numerical; it can be, for example, a
published document describing a clinical trial. The number of participants in such a trial should
not be intentionally encoded in x, since a treatment should be applicable to any number of people.
However, avoiding implicit, unintentional correlations between trial design and trial size may be
difficultorimpossible. Letξ encodefactorswhichinfluencethetreatment,butareneithercontrolled
nor observed. For example, the effect of an exercise program may surreptitiously depend on the
6altitude of the training facility or the jobs of the participants.
In the Neyman-Rubin framework of potential outcomes [Neyman, 1923, Rubin, 1974], for a single
participant denoted by ρ, ρ(1) ∈ R is the outcome when assigned the treatment, and ρ(0) ∈ R
is the outcome when assigned the comparison. Each outcome may be a final measurement (such
as the amount of strength gained after training), or its change from a baseline measurement, or
the logarithm of the ratio of final to baseline. The difference ρ(1)−ρ(0) is the individual effect of
the treatment. The potential outcomes framework is challenging because we cannot observe both
terms in ρ(1)−ρ(0), since each participant is assigned to either the treatment or the comparison.
The conditional average treatment effect (CATE), denoted by u, quantifies the expected difference
between the treatment and comparison for a new participant:
u(x,ξ) = E ρ(ρ(1)−ρ(0) | x,ξ) (1)
2.2 Different Goals of Meta-Analysis
The CATE is the predictive target of meta-analysis. With high probability (typically 95%, with
α = 0.05), the CATE should lie within the predicted interval:
P (u(x,ξ) ∈ C(x)) ≥ 1−α (2)
C,x,ξ
Ratherthanpredictingrelativelyspecific,tangibleeffects,meta-analysisoftenfocusesonestimating
more abstract, harder-to-verify quantities. Meta-analyses usually report a confidence interval CI ⊂
R
which, with high probability, should contain the average treatment effect (ATE, also known as
the summary effect or grand mean):
P (ATE ∈ CI) ≥ 1−α where ATE = E u(x,ξ)
CI x,ξ
Whereas the confidence interval merely needs to capture the ATE, the prediction interval must
capture most of the dispersion around it. (Formally, a prediction interval covers a random variable,
and its coverage probability must also account for the randomness of that variable, whereas a
confidence interval covers a fixed value). In the presence of significant heterogeneity, the confidence
interval is much tighter than the prediction interval, and has little chance of capturing the effect
of a future treatment. Due to this potentially unintuitive behavior, and the possibility of instilling
overconfidence in evidence about the treatment, many prominent researchers encourage systematic
reviews to report prediction intervals [IntHout et al., 2016, Riley et al., 2011, Borenstein, 2024].
According to some researchers, the relative ease of corroborating (or refuting) predictions makes
them essential for scientific rigor and reproducibility [Billheimer, 2019].
Theseproblemsareexacerbatedbytheintroductionoffeatures(x)andlargernumbersoftrials(n),
as proposed in this paper. Since confidence intervals are tighter than prediction intervals, it may
be technically tempting to use untrusted priors to analogously tighten intervals for ATE. However,
when considering many trials with substantially different features, ATE becomes a useless quantity
[Simonsohn et al., 2022, Subramanian et al., 2018, Gould, 2010]. It is arguably misleading to use
features within a statistical analysis but to simultaneously obfuscate their existence in the reported
statistic. This is why prediction intervals are presently the preferred solution concept.
While prediction intervals avoid some of the unintuitive pitfalls of confidence intervals, it is impor-
tant to note that the predictive guarantee (2) has subtleties of its own. It is a mixed observational-
causal guarantee: coverage does not hold for all x, just for most x according to some probability
7distribution. For example, if we observe only trials with patients younger than 70, then coverage
may not hold for those older than 70. Furthermore, the probabilistic guarantee is marginal. For
example, if α = 0.05, then it is possible for coverage to be 99% for patients younger than 60 and
only 80% for patients between 60 and 70, so long as the average is at least 95%.
Theguarantee(2)ismostreliablewhenthedistributionoverxisexplicitlyspecifiedbyagenerative
model. If trial designs are actually chosen according to this distribution, and x consists solely of
prospectively-set, controllable variables, then it is easy to sample future x for which the coverage
guarantee holds. If x includes retrospectively-collected information, or the trials are designed
according to unspecified criteria, then the guarantee becomes less meaningful.
2.3 Randomized Controlled Trials (RCTs)
AnRCTenrollsmparticipantswithpotentialoutcomesρ 1,...,ρ m. Uniformlyatrandom,itassigns
m 0 of them to group 0 (the comparison), and the remaining m 1 to group 1 (the treatment). Most
RCTs do not report individual outcomes. Rather, they report the mean and (corrected) variance
of the comparison outcomes are reported as y(0) and v(0). The same statistics are reported for the
treatment outcomes as y(1) and v(1). These are combined into y, the difference in means, and v, a
sum of the observed standard errors [Deeks and Higgins, 2010]. These statistics are defined as:
1 (cid:88)
y(g) = ρ (g) y = y(1)−y(0)
i
m
g
iingroupg
1 (cid:88) v(0) v(1)
v(g) = (ρ (g)−y(g))2 v = +
i
m −1 m m
g 0 1
iingroupg
Condensing the data into y and v has the following rationale. It can be shown that y is an unbiased
estimate of the CATE:
E(y | x) = E(u | x)
Thus, as the RCT enrolls a very large number of participants, y converges to u. This is the primary
reason why RCTs are so valuable. v is an estimate of y’s variance around u, under conditions
discussed in the next section.
2.4 Random-Effects Model of the Data
Meta-analysis is conducted upon n trials, each with data X i ∈ X, Y i ∈ R and V i > 0 for i =
1,...,n. As discussed above, each trial’s Y i is centered around U i, but varies around it due to
its limited number of participants. Because Y i is a sample average, by the central limit theorem,
it is asymptotically normally distributed around U i. The random-effects model of meta-analysis
[DerSimonian and Laird, 1986, Higgins et al., 2009] asserts, as a simplifying assumption, that Y i is
exactly (not just asymptotically) normally distributed around U i with true variance equal to the
observed one. That is, Y i ∼ N(U i,V i). This can be written in a way that highlights a key difference
between the standard random-effects model and this paper’s model:
Y i(X i,ξ i) = ATE+ U i(X i,ξ i)−ATE + N(0,V i) (3)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
between-trialheterogeneity within-trialvariation
The first and last terms are the same in both models. The random-effects model asserts that
the middle term U i −ATE ∼ N(0,ν) where ν (often denoted by τ2) is called the heterogeneity
8variance. By contrast, in this paper, U i depends on the features X i, and may also involve arbitrary
(non-Gaussian) noise through ξ i. Thus, this paper eliminates a potentially-unrealistic assumption.
2.5 Untrusted Data as a Probability Distribution
Independently of RCTs, practitioners and researchers often possess deep intuitions about the
CATE. These intuitions arise from the lower levels of the evidence hierarchy: observational studies,
individually-published cases, hands-on experience, and personal belief [Murad et al., 2016]. It is
difficult to rigorously infer causation from such untrusted (or “real-world”) data, since they are
observational and may have deeply-embedded biases. Nonetheless, it is often found that untrusted
dataagreewithRCTs[BensonandHartz,2000]. Retrospectively,Toewsetal.[2024]foundtheratio
of risk-ratios between RCTs and observational studies to be approximately 1.08. The prospective
RCT-DUPLICATE trial found their Pearson correlation to be 0.82 [Wang et al., 2023], with much
of the discrepancy attributable to readily-identified factors [Heyard et al., 2024].
Since untrusted data originates from different kinds of sources and experiences, it does not share
the form of RCTs. A modern approach to capturing disparate, large quantities of knowledge is
to pretrain foundation models. Such models are already being developed for healthcare [Moor
et al., 2023, Singhal et al., 2023, Tu et al., 2024]. Concretely, this learns an embedding φ(x) which
maps features x into a Euclidean space having inner product κ(x,x(cid:48)) = φ(x)Tφ(x(cid:48)). On top of
this embedding, a linear predictor of the CATE can be trained as µ(x) = wTφ(x). Practically, this
representation(µ,κ)encompassesnearlyeveryusefulwayofpredictingtheCATE.Mathematically,
this representation constructs a Gaussian process, a probability distribution over functions f :
X (cid:55)→ R , with higher weight placed on f which could plausibly approximate the CATE [Kanagawa
et al., 2018, Williams and Rasmussen, 2006]. In this probabilistic perspective, µ(x) = E f(x) and
f
κ(x,x(cid:48)) = E (f(x)−µ(x))(f(x(cid:48))−µ(x(cid:48))). Gaussian processes are often used as prior probability
f
distributionsinBayesianinference[Gelmanetal.,1995]. Inourapproach,unlikeBayesianinference,
the prior distribution is not assumed to be correct.
An important restriction is that µ and κ are fixed with respect to P . In practical terms, this means
the outcomes of the trials are not reincorporated into µ and κ. Otherwise, the trials could trivially,
erroneously serve as their own reality check. Thus, although µ and κ are completely untrusted in
terms of their veracity and utility, their provenance (especially the data used to generate them)
must be clearly understood. Practices such as preregistration and data transparency can facilitate
this understanding [Munafò et al., 2017]. Importantly, this assumption is about the processes used
toincludedata,whichareunderourcontrol. Itisnotaboutthecomplexphenomenawhichgenerate
the data itself. It is much weaker than the assumptions of ignorability and positivity which are
made in causal inference.
2.6 Standard Meta-Analysis Algorithms
As previously mentioned, prevalent algorithms for meta-analysis ignore the features x; in the par-
lance of the field, they perform mean-effect prediction rather than meta-regression. Thus, they
simply return a single prediction interval C ⊂ R rather than a prediction band. Because the model
(3) is not analytically solvable, there is no exact, rigorous frequentist prediction interval. Instead,
there are many different formulae [Veroniki et al., 2019, Nagashima et al., 2021], each involving
approximations which hold only as n → ∞. Most of the prediction intervals have this form:
(cid:113)
(cid:91) (cid:91)
C = ATE±t νˆ+V(cid:100)ar(ATE) (4)
9(cid:91)
In this expression, the variance estimates νˆ and V(cid:100)ar(ATE) are usually algorithm-specific. More
generally, t is the 1− α quantile of a Student t distribution with n−1 degrees of freedom. A(cid:91) TE is
2
an estimate of ATE, usually based upon inverse-variance weighting:
(cid:44)
(cid:91) (cid:88) (cid:88) 1
ATE = w iY i w i where w i = for each i = 1,...,n (5)
V +νˆ
i
i i
Inpractice,themostwidely-usedpredictionintervalisbasedontheclassicalheterogeneityestimator
(cid:91)
νˆ of DerSimonian and Laird [1986], and an estimator V(cid:100)ar(ATE) proposed by Higgins et al. [2009].
When n is small, experimental evidence indicates this interval is too small to satisfy (2) with the
desired probability 1−α.
Proposition 1 (ClassicalPredictionInterval). Assumethemodel(3)withU i ∼ N(ATE,ν). Define
the following quantities within (4):
νˆ = Q
S
1− +( Sn 2− /S1 1) V(cid:100)ar(A(cid:91) TE) = ((cid:88)
i
w i)2 Y¯ = (cid:80) (cid:80)n i=
n
i=1 1V Vi− i−1Y 1i Q = (cid:88) i=n 1V i−1(Y i−Y¯)2 S
r
= (cid:88) i=n 1V i−r
Then C, as defined in (4), approximately satisfies (2) as n → ∞.
Partlett and Riley [2017] proposed an alternative prediction interval based upon restricted maxi-
mumlikelihood(REML)andHartung-Knapp-Sidik-Jonkman(HKSJ)estimators[Nagashimaetal.,
(cid:91) (cid:91)
2021]. REML obtains νˆ and ATE as the maximizers of a log-likelihood function (cid:96)(νˆ,ATE) which
is filtered to remove influences from irrelevant variables [Viechtbauer, 2005]. It is not concave,
so it cannot be maximized by standard algorithms. However, its stationary points ∂(cid:96) / dνˆ = 0
(cid:91) (cid:91)
(for fixed ATE) and ∂(cid:96) / dATE = 0 (for fixed νˆ) have closed-form expressions, so it is amenable
(cid:91)
to alternating maximization. The following estimator V(cid:100)ar(ATE) was developed independently by
HartungandKnapp[2001]andSidikandJonkman[2003]. CochraneStatisticalMethodsandother
groups endorse the use of HKSJ [IntHout et al., 2014, Veroniki, 2022, Veroniki et al., 2019].
Proposition 2 (REML+HKSJ Prediction Interval). Assume the model (3) with U i ∼ N(ATE,ν).
(cid:91)
Initialize νˆ = 0. Alternate the updates to ATE and w in (5) with the following update of νˆ, until a
fixed point is approximately reached:
νˆ ←
(cid:80)n i=1w i2(( (cid:80)Y i n−A w(cid:91) T 2E)2−V i)
+
(cid:80)n1
w
V(cid:100)ar(A(cid:91)
TE) =
(cid:88)n ( (Y ni −− 1A(cid:91) )T (cid:80)E)2 ww i
i=1 i i=1 i i=1 j j
Then C, as defined in (4), approximately satisfies (2) as n → ∞.
In addition to these frequentist intervals, Bayesian intervals for u can also be obtained [Smith
et al., 1995, Gelman et al., 1995]. These begin with prior distributions over ATE and ν. Improper
(i.e. unnormalized) uniform priors are a default uninformative choice [Röver, 2017]. Using the
random-effects model as a likelihood, Bayes’ theorem obtains the posterior distribution over ATE
and ν, which induces a (normal) posterior distribution over u. From this posterior distribution,
a prediction interval for u can be derived. Such intervals can be highly sensitive to the choice
of uninformative prior, which is partially why Bayesian methods are less common in systematic
reviews [Hamaguchi et al., 2021]. Nonetheless, there are some circumstances where the flexibility
ofBayesianmethodsisdesirable. Forexample,theBayesianapproachcanbeextendedtopredicting
trials. The posterior distribution for future y ∼ N(u,v) is just u’s posterior with v more variance.
10Proposition 3 (Bayesian Trial Prediction). Let the prior distribution over ATE be improper
uniform. Assumethe likelihood(3) with U i |ATE,ν ∼ N(ATE,ν). Then, recalling (5), the posterior
predictive distribution conditioned on ν is y | ν = νˆ ∼ N
(cid:16) A(cid:91) TE,((cid:80)
iw
i)−1+νˆ+v(cid:17)
. [Röver, 2017]
2.7 The Ethics of Meta-Analysis
Healthcare is important, uncertain, and sometimes controversial. Evidence-based medicine was
introducedtohelpresolvesomeoftheseissues,butitinvolvescontroversyofitsown. Itunavoidably
privileges certain kinds of experiences and opinions over others. This paper does not introduce
these problems, but it does operate in their midst. Let us examine how these problems could be
ameliorated or aggravated by our approach.
Currently,meta-analysisinevidence-basedmedicineishighlyexclusionary. The“lowerlevels”ofthe
evidence hierarchy are deprecated in favor of RCTs in an effort to preserve rigor and eliminate bias.
However, this introduces some bias of its own. For example, RCTs are expensive to conduct. Any
methodology that substantially prefers RCTs may be substantially influenced by funding agencies
and associated institutions [Lundh et al., 2017]. Furthermore, RCTs are not ethical to conduct
in many situations [Morris and Nelson, 2007]. Conformal meta-analysis recognizes that RCTs are
especially valuable, but it holistically incorporates data of less rarified origin. Even when our
methods do not lead to quantitative improvements, they are arguably more fair, inclusive, and
comprehensive. They could ameliorate concerns that evidence-based medicine limits the autonomy
of healthcare professionals [Armstrong, 2007].
However, conformal meta-analysis introduces additional computational and statistical complexity
into the process of meta-analysis. This complexity could be exploited by bad actors, with negative
societal consequences. For example, a malicious meta-analyst could sneak RCT data into their
prior to arrive at essentially whichever conclusions they desire. To prevent such harms from oc-
curring, any rigorous conclusions derived from conformal meta-analysis need to be accompanied by
safeguards on the handling of data.
3 Related Work
Causal inference from observational data. Performingrandomized, controlledtrialsisnotthe
onlywaytoestimatecausaleffects. Aftermakingappropriateassumptions,causalinferencescanbe
extracted from observational data [Imbens and Rubin, 2015, Pearl, 2009, Spirtes et al., 2001]. This
is an extensive research endeavor encompassing fields such as economics, public policy and online
advertising; we mention some of the most relevant work here. The survey by Colnet et al. [2024]
discusses various approaches to integrating RCTs with observational data. To estimate the CATE,
causal forests [Wager and Athey, 2018] and metalearners [Künzel et al., 2019] combine machine
learning techniques with causal reasoning. The most widespread assumption of such methods is
ignorability, or unconfoundedness. It requires that, having observed the features x, the treatment
assigned to a participant is independent of their potential outcomes ρ(0) and ρ(1). That is, there
are no unmeasured variables outside of x that could bias treatment towards different participants.
Another widespread assumption is positivity, or overlap: for every x, both the treatment and the
comparison have a chance of being assigned.
Suchstrong,unprovenassumptionsareplausibleinmanycircumstances,buttheyarenotappropri-
11ate for systematic reviews. At some point, assumptions must be tested; systematic reviews, more
confirmatory than exploratory in nature, often serve this crucial purpose. Nevertheless, conformal
meta-analysis allows these methods to be (indirectly) used in systematic reviews, without any con-
cerns about their unproven assumptions. These methods can ideally be used to extract better µ
and κ from the untrusted data. Indeed, taming severe biases in this data likely requires aggressive
assumptions of some kind. Thus, conformal meta-analysis doesn’t replace these methods; rather,
it expands their domain of application to more scientific settings.
Conformal prediction with label noise. Previous works have examined how to conformally
predict the underlying u while observing only noisy Y 1,...,Y n. It is often empirically observed
that conformal prediction can be obliviously robust to label noise, in the sense that C(x), without
any involvement of V or v, manages to covers u without any loss in confidence. However, provable
guarantees remain elusive. Feldman et al. [2023] show that if C(x) always contains the median
of u | x, then C(x) covers u with no loss in confidence. This is a very strong assumption in
meta-analysis, as it essentially posits that the relationship between x and u has been globally
determined, and the main difficulty of conformal prediction is to account for the uncertainty driven
by the unobserved variables ξ. Most approaches to (non-obliviously) handling noise involve some
modification to split conformal prediction. In classification, the (discrete) labels may be noisy
because they are the majority vote from some underlying probability distribution, which reflects
uncertainty over the true class. Stutz et al. [2023] adapt split conformal prediction to account for
this uncertainty by sampling multiple labels from the underlying distribution. Sesia et al. [2023]
and Penso and Goldberger [2024] modify split conformal prediction to estimate the amount of over
(or under) coverage of C(x). Unfortunately, splitting the data is not feasible in meta-analysis,
where n is small. Label noise should be distinguished from label shift, when the training Y 1,...,Y n
are sampled from a different distribution than the test y [Podkopaev and Ramdas, 2021].
Meta-regression. A meta-regression fits the observed effects Y i as a (typically linear) function of
the features X i [Stanley and Jarrell, 1989]. Meta-regression is usually conducted to diagnose which
featuresareresponsibleforheterogeneity. Itcanalsogenerateusefulhypothesesforfutureresearch,
by identifying which features are associated with higher or lower effects. While meta-regression
and conformal-meta-analysis are similar in form, there are a number of crucial differences. Most
importantly, unlike conformal-meta analysis, meta-regression does not offer predictive guarantees
fornewx; thefittothedataispost-hocandinterpretive[Bakeretal.,2009,ThompsonandHiggins,
2002]. The (non-predictive) statistical task in meta-regression is to determine which features have
a statistically significant relationship with the effect [Huizenga et al., 2011]. To limit spurious
findings, meta-regression is typically performed on a small number of prespecified features. By
contrast, conformal meta-analysis fits powerful, nonlinear models on a potentially large number
of features. In conformal meta-analysis, the regression, as embodied by the prediction band C, is
presented as the main result, not just an adjunct diagnostic.
Bayesian priors. Conformal meta-analysis takes a prior probability distribution, along with
trial data, and makes predictions from a posterior distribution — a process that mirrors Bayesian
inference [Gelman et al., 1995]. A crucial difference is that Bayesian inference assumes the prior
to be correct, whereas conformal meta-analysis does not. When Bayesian methods are employed
in meta-analysis, only uninformative priors are used for ATE. Even with this limitation, the choice
of uninformative prior can seriously impact the empirical validity of the ensuing meta-analysis
[Hamaguchi et al., 2021]. However, informative priors for the heterogeneity variance ν have been
developed, and are more widely accepted [Rhodes et al., 2016, Lilienthal et al., 2024].
12Uniform confidence bands. Prediction intervals also should not be confused with uniform
confidence bands, which offer the following stronger guarantee, and do not involve unobserved ξ:
P (for all x ∈ X, u(x) ∈ C(x)) ≥ 1−α
C
Such bands have been developed for Gaussian process regression in the context of online opti-
mization, where new points x are sequentially, adaptively chosen to minimize uncertainty about u
[Srinivas et al., 2009, Chowdhury and Gopalan, 2017, Fiedler et al., 2021, Neiswanger and Ramdas,
2021]. Since subsequent x are chosen adaptively using the band, it is essential for the band to hold
for arbitrary x rather than just randomly-sampled x. Strictly speaking, these bands are correct for
arbitrary µ and κ. However, their widths depend on the smoothness of u, as quantified by its norm
in the reproducing kernel Hilbert space induced by κ. Since u is unknown, this quantity is also
unknown. As a practical matter, when µ and κ can range from very good to very poor, the band is
either very wide or unknown. Though conformal meta-analysis only offers prediction intervals with
marginal coverage guarantees, their width and coverage do not depend on unknown quantities.
Utilizing unlabeled data. Trusted labels are generally considered a scarce resource in machine
learning, especially compared to unlabeled data (i.e. x sampled from the marginal distribution of
P
). Unlabeled data are commonly used to pretrain large foundation models [Dahl et al., 2011, Dai
and Le, 2015]. Semi-supervised learning studies how to rigorously use unlabeled data to improve
predictions [Balcan and Blum, 2010]. Angelopoulos et al. [2023] recently proposed prediction-
powered inference as an approach to safely tighten confidence intervals by using unlabeled data
along with a prior derived from separate, untrusted data. In this approach, (1) the unlabeled
data and prior (which is temporarily treated as correct) are used to estimate the parameter, (2)
concentration inequalities are applied to bound the estimation error arising from limited unlabeled
data, and (3) the labeled data are used to correct the estimation error due to inaccuracy of the
prior. Subsequently, Zrnic and Candès [2024] proposed cross-prediction-powered inference, which
has similar goals but does not depend upon an untrusted prior. Instead, it splits the data (as in
cross-validation) to train a predictor. Such methods have been used to improve out-of-distribution
causal inference [Demirel et al., 2024]. However, these methods are not directly applicable to
predictive meta-analysis, in which there are no unlabeled data. Furthermore, these methods are
designed to produce confidence intervals rather than prediction intervals.
Safely using untrusted data. Various endeavors in statistics and machine learning involve
making predictions that are rigorously guaranteed, even though they use untrusted data. To
some extent, all these techniques manage to circumvent the “garbage-in, garbage-out” principle.
PAC-Bayesian generalization theory formalizes inductive bias as an (untrusted) prior probability
distribution[Shawe-TaylorandWilliamson,1997,McAllester,1998,Seeger,2002]. Itsgeneralization
bounds are tight when the prior and data align, so that a learning algorithm (producing a posterior
distribution)canfitthedatawithoutdivergingfarfromtheprior. WhilePAC-Bayesisaveryuseful
theoretical tool, conformal prediction bounds are quantitatively tighter, especially when n is small.
Instatistics,anuntrustedpriordistributioncanbeusedtodefineane-value,anonnegativestatistic
whosemeanisatmostone[NeiswangerandRamdas,2021]. Usingitsreciprocalasanunnormalized
density leads to e-posteriors, which can be used as the basis for valid inferences and decisions
[Grünwald, 2023]. To derive confidence intervals with conditional coverage guarantees, likelihood-
freeinferencemethodscanexploituntrustedpriorinformation[Masseranoetal.,2023]. Incomputer
science, algorithms can be infused with untrusted predictions, also called side information, advice,
or hints [Mitzenmacher and Vassilvitskii, 2022]. When the predictions are good, the algorithms
run faster; when the predictions are bad, the algorithms retain acceptable worst-case performance.
13A prototypical example is binary search, which can be modified to run in O(1) time given a good
prediction of the target’s index, and in O(logn) time no matter how bad the prediction was.
4 Fully-Conformal Idiocentric Ridge
1 def precomputations(M, K, U, m, k, kₒ, α):
2 n = len(M)
3 I = eye(n)
4 I_ = eye(n+1)
5 M_ = append(M,m)
6 K_ = block([[K, k[:, newaxis]], [k, kₒ]])
7 λ = amax(diag(K_))
8 t_ = solve(K_/λ + I_, M_)
9 Q_ = solve(K_+λ*I_, K_)
10 Q = Q_[:-1,:-1]
11 q = Q_[-1,:-1]
12 qₒ = Q_[-1,-1]
13
14 A = -q
15 a = 1-qₒ
16 B = U - Q@U - t_[:-1]
17 b = -q@U - t_[-1]
18 # a is already positive; flip signs (wlog) so that a,A_i >= 0
19 B *= sign(A) + (A == 0)
20 A *= sign(A)
21 S = sqrt(λ*diag(Q))
22 s = sqrt(λ*qₒ)
23
24 τ = ceil((1-α)*(n+1)).astype(int32)
25 return Q, q, qₒ, t_, A, a, B, b, S, s, λ, n, I, τ
Algorithm 1: Python / NumPy code for common linear-algebraic computations described in Section 4. In
this code, and the code throughout the paper, some elisions and deoptimizations are made for readability.
In particular, import statements are omitted.
This section studies standard, noise-free conformal prediction: given observations X and U, we
seek a prediction interval C(x) for u. We fully conformalize kernel ridge regression (KRR) under
the condition of idiocentricity, as defined below. Our approach actually generalizes beyond KRR
to all linear smoothers, which includes methods such as k-nearest neighbors, Nadaraya-Watson
kernel regression, and smoothing splines [Buja et al., 1989]. We obtain a fast, simple algorithm for
(approximate) full conformal prediction which enjoys the following theoretical guarantee. Section 5
will use it to develop meta-analysis algorithms.
Theorem 1 (Fully-Conformal Idiocentric KRR). Let (X i,U i) ∼ P (for i = 1,...,n) as well as
(x,u) ∼ P be exchangeable. Let µ and κ be fixed with respect to P . Algorithm 2 returns [u −,u +]
satisfying P(u ∈ [u −,u +]) ≥ 1−α.
144.1 Kernel Ridge Regression, a Linear Smoother
Let M and K be the mean and kernel function applied to the training features:
M = [µ(X ),...,µ(X )]T ∈ Rn K = [κ(X ,X )] ∈ Rn×n
1 n i j 1≤i,j≤n
√
This has normalized residuals |U i −M i|/ K ii for i = 1,...,n. Given a parameter λ ∈ R , KRR
learns the following posterior on the training features:
M(cid:99)= (K(cid:98)/λ)U +(K/λ+I)−1M K(cid:98) = λ(K +λI)−1K
In full conformal prediction, KRR is applied to the training set (X,U) augmented by (x,u). We
will use bars to denote this augmentation, so X¯ = [X;x], U¯ = [U;u]. Let m = µ(x), k =
[κ(X 1,x),...,κ(X n,x)]T, k 0 = κ(x,x), and:
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
I 0 K k Q q
I¯= K¯ = Q¯ := (K¯ +λI¯)−1K¯ =
0 1 kT k qT q
0 0
Then, the augmented posterior mean is:
t¯= [t;t0]
(cid:122) (cid:125)(cid:124) (cid:123)
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
M(cid:99)
= Q¯
U
+(K¯/λ+I¯)−1
M
mˆ u m
So the differences between the observations and posterior means are:
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
U −M(cid:99)
= (I¯−Q¯)
U
−t¯=
(I −Q)U −qu
−t¯=
Au+B
u−mˆ u −qTU +(1−q )u au+b
0
with the abbreviations:
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
A −q B I −Q
= = U −t¯
a 1−q b −qT
0
The augmented posterior kernel matrix is
λQ¯
. Therefore the posterior residuals are:
|A u+B | |au+b|
i i
R i = r = (6)
S s
i
√ √
with the final abbreviations S i = λQ ii and s = λq 0. Conveniently, these residuals are absolute
valuesofaffinefunctionsinu. Itiseasytoseethatresidualsofthisformaresharedbythefollowing
broader class of learning algorithms.
Definition 1 (Symmetric Linear Smoothers). A learning algorithm is symmetric if permuting its
inputs (X i,U i) permutes its training predictions M(cid:99)i. A linear smoother’s predictions M(cid:99) are linear
functions of the observations U, though they may be nonlinear in X.
4.2 Full Conformal Prediction Under Idiocentricity
Full conformal prediction is based on the following observation.
15residuals
disjoint in u if and only if
Figure 3: A visualization of how general conformal prediction (left) simplifies for linear smoothers (middle)
and even further simplifies under idiocentricity (right). The full conformal interval (7) intersects multiple
regions where r ≤R i. In each of the plots, these residuals (vertical axis) are shown as a function of the test
example’s u (horizontal axis). In general, these regions have no structure and must be determined through
exhaustive retraining at every u. For linear smoothers (middle), these regions simplify considerably, but may
still be disjoint and/or unbounded. Under idiocentricity (right), these regions are just bounded intervals.
Geometrically, idiocentricity ensures a larger slope for r than for any R i.
Proposition 4 (FullConformalPrediction). Let (X i,U i) ∼ P (for i = 1,...,n) as well as (x,u∗) ∼
P be exchangeable. Let [R;r] be the residuals of a symmetric learning algorithm on [X;x] and [U;u].
Given any α ∈ (0,1), let:
C(x) = {u : r is among the τ smallest of R 1,...,R n} for τ = (cid:100)(1−α)(n+1)(cid:101) (7)
Then P(u∗ ∈ C(x)) ≥ 1−α. [Vovk et al., 2005]
Burnaev and Nazarov [2016] derived an algorithm for computing KRR’s C(x). We dramatically
simplify the algorithm under the following condition.
Definition 2 (Idiocentricity). The residuals R 1,...,R n,r are idiocentric if:
|∂r/du|
lim > 1 for all i = 1,...,n
u→±∞ |∂R i/du|
This condition means that changing the test example’s effect u changes its own residual more than
it changes the residuals of other examples. By exchangeability and symmetry, this applies to all
the examples, not just the test one. For linear smoothers, idiocentricity simplifies to the following
condition.
Lemma 1 (Linear Idiocentricity). The residuals in (6) are idiocentric if
|a|
>
|Ai|
for all i.
s Si
First, let us show how this condition simplifies C(x).
(cid:104) (cid:105)
Lemma 2. For i = 1,...,n, let L i = − SS iaib +− ss AB ii, − SS iaib −+ ss AB ii , noting these endpoints may not be
sorted. If KRR is idiocentric, then its prediction set (7) simplifies to:
C(x) = {u : u is outside less than τ of the L 1,...,L n}
161 def conformal_krr(M, K, U, m, k, kₒ, α):
2 _,_,_,_,A,a,B,b,S,s,_,n,_,τ = precomputations(M, K, U, m, k, kₒ, α)
3
4 if τ <= n:
5 us = vstack([
6 (-S*b - s*B) / (S*a + s*A),
7 (-S*b + s*B) / (S*a - s*A)
8 ])
9 us = sort(us, axis=0)
10 u = sort(us[1])[τ-1]
p
11 u = flip(sort(us[0]))[τ-1]
n
12 return u , u
n p
13 else:
14 return -inf, inf
Algorithm 2: Python code forfully-conformalized idiocentric kernelridge regression. It is mildly approximate
due to the simplification introduced by Lemma 3. This algorithm can be used for plain regression outside
the context of meta-analysis. Presently, it is used as a subroutine for Algorithm 3.
Proof. Since the residuals defined in (6) are absolute values, we can flip the signs of b and B i to
standardize on a,A i ≥ 0. Since a/s > A i/S i ≥ 0, the condition r ≤ R i is equivalent to u ∈ L i.
We slightly loosen the defining condition of C(x) to obtain an even simpler algorithm.
Lemma 3. In the notation of Lemma 2, let u + be (the minimal value) above τ of the L i, and let
u − be (the maximal value) below τ of the L i. Then C(x) ⊆ [u −,u +].
Proof. Theupperendpointu + ismetwhen,forτ ofthei ∈ {1,...,n},wehaveu + ≤ L i oru + ≥ L i.
Ignore the first possibility, which becomes more unlikely as u + increases, for a potentially looser
but nonetheless valid interval. A similar argument justifies u −.
KRR is idiocentric when λ is set sufficiently large. The following upper bound is sometimes loose,
but works well throughout this paper. We note that the optimal setting of λ for regression may
not coincide with the optimal setting for conformal prediction. For example, λ = 0 (known as
interpolation or ridgeless regression) can be a good learning algorithm [Hastie et al., 2022, Liang
and Rakhlin, 2020], but it is useless for full conformal prediction, since its residuals are all zero.
Theorem 2. KRR is idiocentric if λ ≥ maxiK¯ ii = max{K 11,...,K nn,k 0}.
Proof. Recalling (1), we seek to prove:
|q | |1−q | |q | |1−q |
i 0 i 0
√ < √ ⇐⇒ √ <
Q q Q ·q q
ii 0 ii 0 0
Since Q is positive definite, by the Cauchy-Schwartz inequality:
(cid:112) (cid:112)
|q | = |(cid:104)f ,f (cid:105)| ≤ ||f ||·||f || = ||f ||2·||f ||2 = Q ·q
i i 0 i 0 i 0 ii 0
17Thus, it suffices to show that 1 < 1− q0q0, that is, 0 < q 0 < 1 2. Since Q¯ is positive definite, q 0 > 0 is
obvious. To establish q 0 < 1 2, let us examine the constraints on the last row of Q¯ . By the original
definition of
Q¯
, taking just the last column of
K¯
:
(cid:20) (cid:21) (cid:20) (cid:21)
q k
= (K¯ +λI¯)−1
q w
0
Expanding and multiplying by both sides:
(cid:18)(cid:20) (cid:21) (cid:19)(cid:20) (cid:21) (cid:20) (cid:21)
K k q k
+λI¯ =
kT k q k
0 0 0
Expanding again:
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
K k q k
q+ q +λ =
kT k 0 q k
0 0 0
This finally leads to the constraints:
(K +λI)q = (1−q )k
0
kTq+λq = (1−q )k
0 0 0
Invertingthefirstequationtosolveforq = (1−q 0)(K+λI)−1k andpluggingintothesecondyields:
(1−q )kT(K +λI)−1k+λq = (1−q )k
0 0 0 0
If we take λ = k 0 then:
(1−q )kT(K +k I)−1k = (1−2q )k
0 0 0 0
(cid:88)n k˜ i2
=
1−2q
0
k
0
λ +k 1−q
i 0 0
i=1
The left hand side is positive, so in order for the right hand to be positive, it is necessary that
q 0 < 1 2, as originally desired. To ensure λ (and KRR overall) remain symmetric, this analysis must
be applied to any permutation of the data. Thus, λ should be larger than any diagonal entry of
K¯ , not just k 0.
To prove Theorem 1, use the λ of Theorem 2 to earn the simplified interval of Lemma 3, which is
supported by the coverage guarantee of Proposition 4.
5 Conformal Meta-Analysis Algorithms
The main results of this section are the following algorithms for predicting causal effects.
Theorem 3 (Conformally Predicting Clean Effects). Let δ > 0. Under the assumptions for
Predicting Effects, Algorithm 3 returns [u −,u +] satisfying P(u ∈ [u −,u +]) ≥ (1−α)(1−δ).
Theorem 4 (Conformally Predicting Effects). Let η > 0. Under the assumptions for Predicting
Effects, Algorithm 5 returns [u −,u +] satisfying P(u ∈ [u −,u +]) ≥ 1− α
(cid:112)
.
(1−α)erfc η/2
18Figure 4: A high-level illustration of Theorem 3. It determines a region U which contains U with high
probability. Each U(cid:98) ∈ U induces a different interval C(x;U(cid:98)). By covering them all in an outer interval
[u −,u +], the conformal coverage guarantee of C(x;U) (highlighted in red) is inherited.
In the second algorithm (Algorithm 5), setting η = 0 (i.e. disabling noise correction) obtains
confidence 1−2α, which is just a slight loss from 1−α when α ≈ 0. (For example, 0.95 confidence
1−α
drops to 0.9473, which probably doesn’t change τ = (cid:100)(1−α)(n+1)(cid:101)). This setting is appropriate
whenV ≈ 0,i.e.thetrialsallhavealargenumberofparticipants. Bysettingη = 2·inverfc( 1 )2,
c(1−α)
the confidence drops to 1 − c · α. More noise correction is conceptually more appropriate when
analyzing both small and large trials. However, the loss of confidence means larger n is needed,
which may not be a worthwhile tradeoff. Conformal prediction is usable only when τ ≤ n; with
c = 2, a final confidence of 95% requires n ≥ 40. This is twice the n needed for η = 0.
While the overhead at η = 0 is not practically important, it indicates either the second algorithm,
or its analysis, are suboptimal. When meta-analysis is very close to regression (i.e. V ≈ 0),
the original 1−α coverage should be smoothly recovered. The first algorithm (Algorithm 3) has
this ideal behavior. This algorithm directly builds upon Algorithm 2 and demonstrates there is
no inherent overhead in meta-analysis. Furthermore, its guarantee (Theorem 3) still holds in the
much more challenging situation where the noise vector E (in Y = U +E) is not iid normal, but is
adversariallychosenwithinsomesetE. However,becauseofthisadversarialpessimism,itsintervals
become loose as V grows above zero. Thus, we recommend Algorithm 5 for practical application.
Inadditiontothesealgorithmsforpredictingeffects,Section5.2presentsanalgorithmforpredicting
trials, i.e. covering y given x and v. This is just an adaptation of fully-conformal KRR (Section 4)
which takes into account both V and v, incorporating them into the residuals to control for noise.
This algorithm is developed primarily as a prelude to Algorithm 5, but it is also used in the case
study of Section 7.
Theorem 5 (Conformally Predicting Trials). Let η > 0. Under the assumptions for Predicting
Trials, Algorithm 4 returns [y −,y +] satisfying P(y ∈ [y −,y +]) ≥ 1−α.
5.1 Predicting Clean Effects
The proof of Theorem 3 decouples into a relatively blunt statistical component (Lemma 4) and a
more complicated computational one (Lemma 5). It builds upon Theorem 1’s guarantee that fully-
191 def predict_clean_effect(M, K, Y, V, m, k, kₒ, α, δ):
2 # get "plain" conformal interval for y, having observed Y
3 y ,y = conformal_krr(M, K, Y, m, k, kₒ, α)
n p
4 # ω is additional endpoint width due to noise
5 # ω -> 0 if V -> 0 quickly enough
6 Q,q,_,_,A,a,_,_,S,s,_,n,I,τ = precomputations(M, K, Y, m, k, kₒ, α)
7 ρ = chi2.ppf(1-δ, n)
8 G = vstack([
9 (-S*q[newaxis, :] + s*(I-Q)) / (S*a + s*A),
10 (-S*q[newaxis, :] - s*(I-Q)) / (S*a - s*A) ])
11 ω = amax(norm(G * sqrt(ρ*V), axis=1))
12 # return widened interval
13 u , u = y -ω, y +ω
n p n p
14 return u , u
n p
Algorithm 3: Python code for conformal meta-analysis with nearly no noise (i.e. “clean” effects observed in
very large trials). As the noise vanishes (i.e. V →0), this algorithm reduces to Algorithm 2 because ω →0.
conformal KRR’s interval covers u. Let C(x;U(cid:98)) denote this interval when U(cid:98) is given as training
data. If some outer interval C(cid:98)(x) contains all C(x;U(cid:98)) over a plausible set of U(cid:98) including U, then
of course C(cid:98)(x) contains C(x;U) and inherits its coverage. Lemma 4 shows the uncertainty over
U falling in that plausible set separates from fully-conformal KRR’s uncertainty over u, given U.
This follows from E’s independence from U given V.
Lemma 4 (Cover All Possibilities). Let C(x;U) = C(x) be the KRR interval from Lemma 2 when
computed on the true U. Let C(cid:98)(x) contain all intervals induced by the ellipsoid E:
(cid:40) (cid:88)n E2 (cid:41)
(cid:91)
E = E : i ≤ ρ C(cid:98)(x) = C(x;U +E−E)
V i (cid:124) (cid:123)(cid:122) (cid:125)
i=1 E∈E Y
Let ρ > 0 be chosen so that P E(E ∈ E | V) ≥ 1−δ. Then P(u ∈ C(cid:98)(x)) ≥ (1−α)(1−δ).
Proof. In the following, let rest denote X,U,x,u.
P(u ∈ C(cid:98)(x)) ≥ P(u ∈ C(x),C(x) ⊆ C(cid:98)(x)) (partial probability)
(cid:16) (cid:17)
= E VE
rest
1(u ∈ C(x))·P E(C(x) ⊆ C(cid:98)(x) | V,rest) (total probability)
≥ E VE rest(1(u ∈ C(x))·(1−δ)) (see below)
= (1−δ)P (u ∈ C(x)) (total probability)
rest,V
≥ (1−δ)(1−α) (conformal prediction)
A sufficient condition for C(x;U) ⊆ C(cid:98)(x) is that E = E for some E ∈ E, i.e. that E belongs to the
ellipsoid. Note that C(cid:98)(x) depends on U but this condition does not. Thus:
P E(C(x) ⊆ C(cid:98)(x) | V,rest) ≥ P E(E ∈ E | V,rest) (sufficient condition)
= P E(E ∈ E | V) (conditional independence)
20≥ 1−δ (assumption)
This lemma doesn’t make any smoothness assumptions on how C(x;Uˆ) changes as U(cid:98) varies away
from U; it relies on the coverage of exactly C(x;U), but not of any slight perturbation C(x;U(cid:98)).
Furthermore, the lemma does not strongly depend on the distribution of E, just that we know a
set E which captures it with probability 1−δ. For Gaussian noise, this is an ellipsoid.
The previous lemma converts the statistical problem of covering u into the purely computational
problem of bounding the endpoints of C(cid:98)(x). The next lemma shows that we can just extend the
“naive” interval, which plugs in Y for U, with some additional width ω at the endpoints. ω is easy
to compute, and drops to zero if V (and δ) go to zero. This indicates the algorithm retains optimal
confidence as meta-analysis becomes close to plain regression.
Lemma 5 (Extra Width). Let [y −,y +] be the interval returned by Algorithm 2 when given Y in
place of U. Let ρ > 0 and G ∈ R2n×n be defined by Lemma 4 and (10), respectively. Let ω be the
√
maximum (cid:96) 2 norm of any row of G diag ρV. Then C(cid:98)(x) ⊆ [y −−ω,y ++ω].
As the first step of proving this lemma, we allow two separate optimizations for the upper and
lower endpoints of C(cid:98)(x). The following lemma performs this (trivially valid) split.
Lemma 6. Recall the intervals L i from Lemma 2. C(cid:98)(x) ⊆ [uˆ −,uˆ +] where:
uˆ − := minmax{bottom n−τ +1 lower endpoints of L 1,...,L n} (8)
E∈E
uˆ + := maxmin{top n−τ +1 upper endpoints of L 1,...,L n} (9)
E∈E
The 2n endpoints (both lower and upper) of the L i can be expressed as F +GE, which highlights
their linear dependence on the noise E.
Lemma 7. The 2n endpoints of L 1,...,L n are the entries of F + GE, where F ∈ R2n and
G ∈ R2n×n are defined below in (10). (Thus, if E = 0, then the bottom and top n−τ +1 endpoints
in F are exactly {y −,y +} returned by Algorithm 2 when given Y in place of U).
Proof. The L i are defined in terms of equations from Section 4 and Lemma 2, which we summarize
here. In terms of the variables B,b,U(cid:98) and E and constants a,A,s,S,Q,q,t¯,Y and V:
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
−S b−sB −S b+sB B I −Q
L i = S ai +sA i , S ai −sA i where b = −qT U(cid:98) −t¯ and U(cid:98) = Y −E
i i i i
To simplify notation, we can eliminate the variables B,b, and U(cid:98) the endpoints directly in terms of
E. Specifically, L = [F(0)+G(0)E,F(1)+G(1)E], where:
S qTY +S t −s(I −Q)Y +st −S qT +s(I −Q)
(0) i i 0 (0) i
F = G = (10)
i S a+sA i S a+sA
i i i i
S qTY +S t +s(I −Q)Y −st −S qT −s(I −Q)
(1) i i 0 (1) i
F = G =
i S a−sA i S a−sA
i i i i
F = [F(0);F(1)] G = [G(0);G(1)]
21Letusfocusonupperboundinguˆ + from(9); thesameprooftechniquewillapplytolowerbounding
uˆ − from (8). The maximization of uˆ + involves two aspects: choosing n−τ +1 coordinates (cor-
responding to large values in F) and then optimizing E so F +GE is large in those coordinates.
The program is difficult because the largest coordinates of F may not coincide with the largest
obtainable by GE — for example, it might be worthwhile to choose an initially large F i even if
(GE) i cannot become very large. We will allow the adversary to decouple these choices, allowing
them to add the largest-possible (GE) i to the largest coordinates of F. This loosens the interval,
but greatly simplifies it as well.
Lemma 8. Let y + be the upper endpoint returned by Algorithm 2 when given Y in place of U.
Recall the definition of uˆ + in (9). Then uˆ + ≤ u + := y ++ω, where:
ω := maxmax(GE) i such that E ∈ E
E i
Proof. As discussed above, uˆ + is produced by taking some initial upper endpoint F i and choosing
E to maximize (GE) i. y + is the top-(n−τ +1) endpoint, so y + +ω = u + exceeds most of the
possible F +(GE) i: it is not possible for any initially-smaller endpoint F i ≤ y + to become larger
than u + following the introduction of noise, since then F i+(GE) i ≤ F i+ω ≤ y ++ω = u +. Even
at the n−τ initially-larger endpoints F i ≥ y +, it may be possible for u + to exceed F i+(GE) i. But
this just means the upper bound is looser than necessary.
Now, the excess width ω can be easily computed with basic linear algebra. The following lemma
concludes the proof of Lemma 5.
√
Lemma 9. ω is the maximum (cid:96) 2 norm of any row of G diag ρV.
Proof. Because E is symmetric around the origin, an absolute value can be introduced to the
definition of ω without affecting its value, which makes it an (cid:96) ∞ norm:
ω = max||GE|| ∞ such that E ∈ E
E
Recalling that E is an ellipsoid, perform a change of variables to transform it to a unit (cid:96) 2 ball,
yielding the following equivalent program:
(cid:112)
ω = max||G diag( ρV)E|| ∞ such that ||E|| 2 ≤ 1
E
This is the definition of the 2 → ∞ operator norm, which is computed as described in the lemma
statement. A different geometry for E would result in a different norm bound on h. For example,
√
box constraints for E would result in h being bounded by the ∞ → ∞ norm of G diag ρV, i.e. the
maximum (cid:96) 1 norm of any row. Thus, this simple proof technique (much like Lemma 4) can be
extended to different kinds of noise.
Finally, we need to calculate the threshold ρ, in terms of δ and V i, so that E ∈ E with high
probability. By a change of variables to standard normals, ρ equals the 1 − δ quantile of the
chi-square distribution with n degrees of freedom. This completes the proof of Theorem 3.
221 # capital arguments for training trials, lowercase are for test
2 def predict_trial(M, K, Y, V, m, k, kₒ, v, α, η):
3 # standard linear algebra for KRR
4 Q,q,_,_,A,a,B,b,S,s,_,_,I,τ = precomputations(M, K, Y, m, k, kₒ, α)
5
6 if τ <= n: # enough training trials for conformal prediction
7 # compute interval L_i = G_i±H_i for each training trial
8 # y in L_i corresponds to r <= R_i for residuals
9 D = square(I-Q) @ V
10 d = square(q) @ V
11 S2, s2 = square(S), square(s)
12 a2A2 = a**2*S2 - A**2*s2
13 ρ = η*(D*s2 - d*S2 - a2A2*v)
14 G = (A*B*s2 -a*b*S2) / a2A2
15 H = sqrt(maximum(0, s2*S2*(A*b - a*B)**2 - ρ*a2A2)) / a2A2
16 L , L = G-H, G+H
n p
17 # return extreme quantiles of L_i's upper/lower endpoints
18 y = sort(L )[τ-1]
p p
19 y = flip(sort(L ))[τ-1]
n n
20 return y , y
n p
21 else: # not enough training trials
22 return -inf, inf
Algorithm 4: Python code for conformal prediction of trials. This is a variant of Algorithm 2 which incorpo-
rates terms correcting for noise in the observed effects.
5.2 Predicting Trials
This section proceeds analogously to Section 4, deriving residuals for full conformal prediction.
However, nowthedataareoftheform(x,y,v)ratherthan(x,u). HereisaversionofProposition4
adapted to the meta-analytic data.
Proposition 5 (Full Conformal Prediction). Let (X i,Y i,V i) ∼ P (for i = 1,...,n) as well as
(x,y∗,v) ∼ P be exchangeable. Let [R;r] be the residuals of a symmetric learning algorithm on
[X;x], [Y;y] and [V;v]. Given any α ∈ (0,1), let τ = (cid:100)(1−α)(n+1)(cid:101) and:
C(x,v) = {y : r is among the τ smallest of R 1,...,R n} (11)
Then P(y∗ ∈ C(x,v)) ≥ 1−α. [Vovk et al., 2005]
Now that V and v are observed, noise-correcting terms based upon them can be incorporated into
the residuals. In the following residuals, A i,a,B i,b,S i, and s are the same as in Section 4. For
η > 0, Z i and z subtract off an η multiple of the expected impact of the noise. To simplify the
calculation of these expectations, we adopt the squared loss in lieue of the absolute loss:
(M(cid:99)i−Y i)2−ηZ
i
(A iy+B i)2−ηZ
i
(mˆ −y)2−ηz (ay+b)2−ηz
R = = r = =
i S2 S2 s2 s2
i i
23To determine Z i and z, decompose the differences between the observations and the posterior
means. As before, denote augmentation with overlines, as in E¯= [E;(cid:15)].
(cid:20) (cid:21)
Y −M(cid:99)
= (I¯−Q¯)(U¯ +E¯−M¯)−t¯
y−mˆ
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
=
U −M(cid:99)
−(I¯+Q¯)E¯=
U −M(cid:99)
+
(I −Q)E −q(cid:15)
u−mˆ u−mˆ −qTE +(1−q )(cid:15)
0
Now, calculate the mean squared error with respect to E i ∼ N(0,V i) and (cid:15) ∼ N(0,v):
E (Y i−M(cid:99)i)2 = E (U i−M(cid:99)i+(e i−Q i)TE −q i(cid:15))2
 2
(cid:88)
= (U i−M(cid:99)i)2+E (1−Q ii)E i− Q i,jE
j
−q i(cid:15)
j(cid:54)=i
Zi
(cid:122) (cid:125)(cid:124) (cid:123)
(cid:88)
= (U i−M(cid:99)i)2+(1−Q ii)2V i+ Q2 i,jV j+ q i2 v
(cid:124)(cid:123)(cid:122)(cid:125)
j(cid:54)=i
A2
(cid:124) (cid:123)(cid:122) (cid:125) i
Di
z
(cid:122) (cid:125)(cid:124) (cid:123)
(cid:88)
E (y−mˆ)2 = E (u−mˆ −qTE +(1−q )(cid:15))2 = (u−mˆ)2+ q2V +(1−q )2v
0 j j 0
(cid:124) (cid:123)(cid:122) (cid:125)
j
a2
(cid:124) (cid:123)(cid:122) (cid:125)
d
As before, we use idiocentricity to simplify the residual comparisons.
Theorem 6. For i = 1,...,n, let ρ i = η(Z is2−zS i2). Define the intervals L i = G i±H i, where:
(cid:113)
(cid:0) (cid:1)
G i =
(A
ai
SB
)i
2s2 −− (Aab sS )i2
2 and H i =
max 0, s2S i2(A (i ab S− )a 2B −i) (2 A− sρ )2i((aS i)2−(A is)2)
i i i i
If KRR is idiocentric, then its full conformal prediction set (11) simplifies to:
C(x,v) = {y : y is outside less than τ of the L 1,...,L n}
Proof. r ≤ R i rewrites to S i2(ay +b)2 +ρ i ≤ s2(A iy +B i)2. Under idiocentricity, which ensures
a/s > A i/S i ≥ 0, this is equivalent to y ∈ L i.
To prove Theorem 5, use the λ of Theorem 2 to earn the simplified interval of Theorem 6, which is
supported by the coverage guarantee of Proposition 5.
5.3 Predicting Effects
Theorem 5 guarantees that C(x,v) usually covers y ∼ N(u,v). We will use this guarantee to derive
intervals C(x) that usually cover u. We don’t have a v to plug into C(x,v), so we have to dig into
how C(x,v) works. The claim of Theorem 4 is that C(x,0) covers u just slightly less often than
it covers y, so long as the level of noise correction η is not too high. This holds because of two
counterbalancing properties of C(x,v) that hold for all v ≥ 0.
241 def predict_effect(M, K, Y, V, m, k, kₒ, α, η):
2 return predict_trial(M, K, Y, V, m, k, kₒ, 0, α, η)
Algorithm 5: Python code for conformal meta-analysis with small trials, deferring entirely to Algorithm 4.
Figure5: Ahigh-levelsketchofC(x,0)’scoverageofu,whenηissmallenough(left)versustoolarge(right).
The gray dots are u, and its distributions conditioned on various v are shown. C(x,0) is the dark green bar;
√
as v increases, C(x,v) increases by ηv, and that growth (in yellow) is shaved. The orange curves convey
the spread of |N(0,v)|. With good η (left), C(x,v) grows slowly compared to |N(0,v)|, which naturally
pushes in the u (on average) as v increases. Thus, C(x,0) is wide enough to contain most of the u, no
matterwhatv is. Ontheright, whenη islarge, C(x,v)adaptsmoredynamicallytov, soC(x,0)issmaller.
Too many u in the yellow region are shaved.
The first property is that most of the spread of |N(0,v)| can be shaved from the edges of C(x,v)
without losing too many u. This is possible because, in meta-analysis, we care only about small
α, ideally around 0.05. Since C(x,v) covers y with high probability, there are only a few u closer
than |N(0,v)| to the ends of C(x,v) — otherwise, bad flips of the noise could push too many y out
of the interval, which would violate the coverage guarantee of C(x,v). While this logic indicates
shaving is a conceptually feasible strategy, it remains an abstract possibility, since we don’t know
√
v, and don’t know how much to shave. (It should intuitively be O( v), but constants matter).
The second property is that making η smaller limits the growth of C(x,v). We mean this in a
completely formulaic sense — we have reasonably concrete expressions for the endpoints of C(x,v),
√
and the following Lemma 10 shows they widen by ηv. When η = 0, C(x,v) doesn’t depend on
v at all. In other words, when noise correction is disabled, C(x,0) must completely internalize the
impact of noise, yielding a relatively wide interval. Larger settings of η allow C(x,v) to grow more
with v, allowing (relatively) thin intervals at small v. To concretely realize the shaving strategy, we
just need to set η small enough so that, as a function of v, the shaveable region within C(x,v) grows
as fast as C(x,v) itself. This allows us to obliviously use the baseline C(x,0). The conditional
distribution v | x is arbitrary and unknown, but any probability mass on v > 0 simply pushes more
u within C(x,0).
√
The fact that C(x,v) grows proportionally to v to capture the noise is not only intuitive, it is
necessary. Most well-behaved learning algorithms should yield conformal intervals which grow (on
average) at roughly this rate. Our ability to prove an exact growth rate, in the next lemma, relies
on the simplicity of full conformal prediction for idiocentric linear smoothers.
Lemma 10 (Normal Interval Growth). Let C(x,v) be the interval from Theorem 6. For all η ≥ 0
√
and v > 0, C(x,v) ⊆ C(x,0)± ηv.
25Proof. The interval for y depends on v only through ρ i:
1 (cid:122) (cid:125)(cid:124) (cid:123)
ρ = Z s2−zS2 = D s2−dS2−((aS )2−(A s)2)v
η i i i i i i i
Under idiocentricity, a/s > A i/S i. Thus, the bracketed term above is positive, ρ i decreases with
v, the square-root radius in L i (which subtracts ρ i) increases with v, an √d the denom √inator i √n L i
is positive. Dividing by the denominator, the radius H i is of the form ...+ηv ≤ √...+ ηv.
Neither the center G i of L i nor the other elided terms in the radius depend on v; the ηv term is
the only one which involves v.
The rest of the proof of Theorem 4 doesn’t depend on either idiocentricity or linear smoothers.
Lemma 11 formalizes the first property described above: most u are contained within C(x,v) by a
margin that grows with v. Finally, Lemma 12 shows that C(x,v) can be shaved down to C(x,0),
with η determining the loss in coverage of u.
Lemma 11 (Pay For Room). Recall y = u+(cid:15) for (cid:15) ∼ N(0,v). Let w = [u−(cid:15),u+(cid:15)], with possibly
unsorted endpoints. If P(y ∈ C(x,v)) ≥ 1−α, then P(w ⊆ C(x,v)) ≥ (1−2α)/(1−α).
Proof. AbbreviateC(x,v) = C. Thekeypropertywerepeatedlyuseisthatyisoneoftheendpoints
of w chosen uniformly at random, conditionally independent of the other data. If w (cid:54)⊆ C, then
either both of its endpoints are not in C, or exactly one of them isn’t. In the former case, y clearly
isn’t in C; in the latter, it isn’t with probability 1. Let gray be the event that exactly one of w’s
2
endpoints is outside of C. First, we prove that:
P(gray) ≤ 2α (12)
Let near denote both of w’s endpoints are in C, and far that neither are in C, so that near,gray,far
partition the probability space. By total probability, and the aforementioned reasoning about y:
P(y ∈ C) = (1−P(gray)−P(far))P(y ∈ C | near)+P(far)P(y ∈ C | far)+P(gray)P(y ∈ C | gray)
1
= (1−P(gray)−P(far))(1)+P(far)(0)+P(gray)
2
1
≤ 1−P(gray)+P(gray)
2
Combining this with the assumption yields (12). Next:
P(y ∈ C | w (cid:54)⊆ C) = P(gray)P(y ∈ C | gray) (only nonzero case)
1
= P(gray) (symmetry)
2
≤ α (12)
With this inequality, the original claim follows from:
1−α ≤ P(y ∈ C) (assumption)
= P(w ⊆ C,y ∈ C)+(1−P(w ⊆ C))P(y ∈ C | w (cid:54)⊆ C) (total probability)
≤ P(w ⊆ C,y ∈ C)+(1−P(w ⊆ C))α (proved above)
= P(w ⊆ C)+(1−P(w ⊆ C))α (y ∈ w)
Note this proof required (cid:15) to be symmetric, zero mean, and conditionally independent given its
variance v, but not necessarily normally distributed.
26Lemma 12 (Shaving). If P(w ⊆ C(x,v)) ≥ 1−2α, then P(u ∈ C(x,0)) ≥ 1− α .
1−α (1−α)erfc(cid:112) η/2
Proof. Abbreviate C = C(x,v) and C(cid:101) = C(x,0). For the first inequality of the following block,
the worst case is obtained when u is exactly one of the endpoints of C(cid:101) (say, the upper endpoint
c˜ +), since that maximizes the distance from the endpoint of C (say, c +), and therefore maximizes
probability that w will still remain within C.
P(w ⊆ C | u (cid:54)∈ C˜) ≤ P(c˜ +|(cid:15)| ≤ c )
+ +
√
= P(|(cid:15)| ≤ ηv) (Lemma 10)
(cid:114)
η
= erf (normal distribution)
2
Thus, the desired claim follows from total probability and some rearranging:
1−2α
≤ P(w ⊆ C) (assumption)
1−α
= P(u ∈ C(cid:101))P(w ⊆ C | u ∈ C(cid:101))+P(u (cid:54)∈ C(cid:101))P(w ⊆ C | u (cid:54)∈ C(cid:101)) (total probability)
(cid:112)
≤ P(u ∈ C(cid:101))+(1−P(u ∈ C(cid:101)))erf η/2 (proved above)
6 Simulations
We performed four types of simulations on three biomedical datasets from the Penn Machine
Learning Benchmark [Olson et al., 2017]. These regression datasets define K and Y; we generated
synthetic M and V according to parameters prior error ≥ 0 and effect noise ≥ 0, respectively. These
paramters are formally defined in Appendix A.1. prior error of 0.2,0.9, and 3.0 roughly correspond
to good, okay, and bad priors derived from untrusted data. effect noise between roughly 0 and
10 captures the practical range of large to small trials, but we may use larger values to stress-test
algorithms. CMAabbreviatesAlgorithm5 with η = 0. Wecompareitto thestate-of-the-artHKSJ
method, which is described in Section 2.6.
Simulation 1: This investigates when conformal meta-analysis is superior to traditional meta-
analysis. For different settings of prior error, we compare the widths of the intervals obtained by
differentmeta-analysisalgorithms. TheonlysituationinwhichHKSJiscompetitivewithconformal
meta-analysis is when the prior is bad and the number of trials is small/moderate. Otherwise,
conformal meta-analysis is superior, sometimes achieving intervals that are dramatically thinner
than those of HKSJ.
Simulation 2: Thisexperimentcheckswhetherthedesired95%confidencelevelisstillachievedas
effect noise increases. Conformal meta-analysis succeeds, whereas HKSJ fails badly. On the other
datasets, HKSJ sometimes drops below 80% confidence. This deficiency is present at all settings
of effect noise, though it aggravates at higher values. This simulation shows that conformal meta-
analysis has a rigorous coverage guarantee, and HKSJ does not. It should be noted that HKSJ was
developedtoimprovethecoverageguaranteeofthemoreprevalentHiggins-Thompson-Spiegelhalter
method.
Simulation 3: This experiment compares different instantiations of Algorithm 5: one with η = 0,
and the other with η = 0.4015, with α adjusted so both ultimately seek a 90% confidence level.
27width Simulation1(badprior) width Simulation1(okayprior) width Simulation1(goodprior)
2000 2000 2000
1500 1500 1500
HKSJ HKSJ HKSJ
CMA
1000 1000 1000
CMA
500 500 500
n n CnMA
20 50 100 200 500 20 50 100 200 500 20 50 100 200 500
cowviedrtahgeSimulation1Si(mbaudlaptiroionr)2(smalln) width Simulation1(okcaovyeprargieor) SimuwlaidttihonS2im(lualragteionn)1(goodprior)
70 70 70
610. 60 1. 60
0.45 900 5 45 00 CMA 0.95 45 00 CMA
03 .0 9 CH MKS AJ 30 HKSJ 0.9 HKSJ 30 HKSJ HKSJ
20 20 CMA 20
10 10 effect 10 CeMffAect
00 100 200 n 300 0 400 noise 0 n100 0 200 300 400 nnoise
20 50 100 200 500 20 50 100 200 500 20 50 100 200 500
ccoowvvieedrrtaahggeeSimulation1SSi(imbmauudllaapttiriooionnr)23((ssmmaallllnn)) width Simulation1(okccaoovvyeeprraarggieeor) SSiimmuuwllaaidtttiihoonnS23im((lluaalrraggteeionnn))1(goodprior)
215.00 100 . HKSJ 2500 CMA 1.0 10 . HKSJ 2500 HKSJCMA
200.0905 2000 η>0 0.95 2000 η>0
0.95 0.95
1500 1500 η=0 1500 η=0
100.009.009 CMA 1000 0.09.09 1000
500 500 eeffffeecHctKtSJ CMA 500 eeffffeecHctKtSJ
00 20100 40 200 60n 300 80 410000nnooisisee 00 2n0100 40 200 60 300 80 410000CnnMnoAoisisee
20 50 100 200 500 20 50 100 200 500 20 50 100 200 500
ccoovvw eerrid aat ggh ee SSS iimmim uuu llaala ttiit ooio nnn 234 ((ss( msmm aaa llllll nnn ))) ccoovvw eerrid aat ggh ee SSS iimmim uuu llaala ttiit ooio nnn 234 ((ll( aala rrggrg eee nnn )))
5000 η=0 5000 η=0
1 0034. ..0 990010 5500. 00
η
CP >r Mio
0
ACr
MA
1 0034. ..0 990010 5500. 00 ηP >rio 0CrMA
2000 2000 CMA
0 1.09 0.0900 eeH fpffK fereS icHJ
octKtrSJ
0 1.09 0.0900
eeH fpffK fereS icHJ octK trSJ
000 120100 2 40 2003 60 4 300 80 5 4100060nnoeoirsirseoer 000 120100 2 40 2003 60 4 300 80 5 4100060nnoeoirsirseoer
widthSimulation5(minisculenoise) width Simulation5(verylownoise) width Simulation5(lownoise)
covweridatghe SSimimuulalatitoionn34(s(smmaallllnn)) covweridatghe SSimimuulalatitoionn34(l(alargrgeenn))
800 800 800
100 100 CMA-CL
17.000080 700 ηP>rio0r 1.00 80 700 ηP>rio0r
06.900560 600 ηC=M0A 0.9560 CMA-CL 600 ηC=M0A
40 40
05.9000 CMA 500 HKSJ 0.90 CMA 500 CMHKASJ
20 CMA-CL epffreioctr 20 epffreioctr
00 20 120 40 2 40 60 3 8060n 4 80 5 201060 neorirsoer40 0060 8012n 0 2 40 320 60 4 40 80 560 180060n neorirsoer
wFiditghuSrimeul6a:tionR5es(muilntisscuolfenaolilses)imulations ownidthaSsiminuglalteione5xe(vmerpylloawrndoaistea)set. See ApwpidethndixSimAu.la1tiofnor5(cloownngoriusee)nt results
width Simulation4(smalln) width Simulation4(largen)
10o0n the other datasets, as well as prec10i0se descriptions of the effect noise an1d00prior error parameters. OveCMraA-lCl,L
5000 5000
48 0c0 0o0nformal meta-analysis can deliver 8m0uch tightPeriorr inter4v00a0ls thanCtMrAa-CdLitiona80l methods (Simulation 1), ePvrioern
360t00h0ough traditional methods have wea6k0coverageHgKSuJarant30e0e0s (Simulation 2),60whereas our algorithms, or their
HKSJ
124 00 a0 00 n 00 alyses, have (overly) stron Cg MAg -Cu Laran4t0ees (SimuClMaAtion 3 12 ) 00 . 00 00 Algorithm 5, not40just a good prior, is essentiaClMAto
2 t0 h 0is p 2e 0rfo 1rman 4c 0e 2(Sim 60ul 3ati 8o 0nCnM4A) 4. Algo20 r 5ithm 203 6caep nrr rio obr r4e 0 bette 060r whe 8n 01CnMthAere i 2s2 v0 ery l 3i 2t 0tle noi 4se 40(Sim 5u 60lation 8065CnM)epA.rr rio or r
widthSimulation5(minisculenoise) width Simulation5(verylownoise) width Simulation5(lownoise)
W70 i0 th the higher setting of η, over70 -0 coverage is consistently demons70 t0 rated. This suggests thC aM tA-C tL he
600 600 CMA-CL 600
analysis of Section 5 can be improved, at least in some settings.
500 500 500
400 400 400
Simulation 4: Our approCC a nMM cAA h-CLassumes that, in many fieldsC, nMiAt should be possible to developC nMgAood
20 40 60 80 20 40 60 80 20 40 60 80
priors from large volumes of untrusted data. However, if these priors are indeed very accurate, it
is unclear whether using KRR (upon just n trials) is worth the complexity, and possible statistical
overhead, over just using the prior as a fixed predictor. (This is conceptually equivalent to using
an extremely large ridge parameter λ, or performing split conformal using all the training data
for calibration.) This simulation indicates there is no such overhead: our fully-conformal intervals
are strictly superior to those derived from a fixed prior. Thus, unless assumptions stronger than
exchangeabilityareusedtoderivepredictionintervals, learningissuperiortomerevalidation. Note
that, when prior error is high, HKSJ becomes superior.
28Thomas et al. (2004)
Kochiadakis et al. (2007)
Balla et al. (2011)
Karacaglar et al. (2019)
Figure 7: Prediction intervals for new observed effects y (black dots) produced by traditional meta-analysis
(light blue) and conformal meta-analysis (magenta, thin). On average, they are comparable in width (1.34
and1.31, respectively). Conformalmeta-analysismanagestocoverthediscrepanttrialofBallaetal.[2011].
Note that the prior for conformal meta-analysis was produced post-hoc, having already seen the analysis of
Letelier et al. [2003] and the results therein. Thus, these intervals should not be interpreted as quantitative
evidence, but merely as qualitative illustrations of the behavior of conformal meta-analysis.
Simulation 5: Algorithm 5 is recommended for practical use, which is why it is used throughout
therestoftheexperiments. Nonetheless, Algorithm3waspresentedasasuperioralternativeinthe
(somewhat theoretical) scenario when the effects are “clean” (i.e. the trials are very large). This
simulation experimentally confirms that Algorithm 3 can be superior in this setting. When there
is essentially no effect noise, Algorithm 3 (labeled as CMA-CL) obtains lower width. However, its
intervals become substantially worse as effect noise or n increase.
7 Case Study: Amiodarone
We revisit the systematic review of Letelier et al. [2003], which assessed the effectiveness of amio-
darone for atrial fibrillation (AF) patients. Its outcome measure is the relative risk of normal sinus
rhythm; that is, the probability of restoring normal rhythm when administered amiodarone, di-
vided by the probability of restoration with placebo. The review involved n = 21 trials, which we
use as training data. For test data, we identify 4 trials that were published after the review, but
would have met its inclusion criteria [Thomas et al., 2004, Kochiadakis et al., 2007, Balla et al.,
2011, Karaçağlar et al., 2019]. Per the Predicting Trials task, we compare traditional meta-analysis
(the Bayesian algorithm of Proposition 3, described in Section 2.6) with conformal meta-analysis
(Algorithm 4, with η = 1).
Our goal is not to make any scientific claims about amiodarone, nor to reassess its evidence base;
that would require following a formal, preregistered protocol. Though we temper our quantitative
findings (depicted in Figure 7), we find them qualitatively interesting. Conformal meta-analysis
manages to correctly predict all 4 trials, whereas traditional meta-analysis suffers a misprediction.
This is not statistically convincing, but it aligns with the fact that conformal meta-analysis has
a rigorous coverage guarantee, whereas traditional algorithms do not. (See Section 2.6 for more
details). It is interesting to observe that not all of the conformal intervals overlap; by contrast, tra-
ditional intervals all inherently overlap. This suggests users of conformal meta-analysis could enjoy
predictions that are meaningfully responsive to the details of their proposed treatment, perhaps
distinguishing between effective and ineffective ones.
Appendix A.2 describes how we conducted the conformal meta-analysis. We highlight some ways
it differed from the usual process. The first change is training a prior on helpful data that would
29otherwise be ignored. We identify 8 trials that did not meet the inclusion criteria, since they
were not placebo controlled. To generate pseudo-effects for these trials, we need to understand
the placebo effect. This leads to the second major change, which is holistically including the
perspectives of practitioners. The critique of Slavik and Zed [2004], written by two doctors of
pharmacy, gave estimates for the placebo effect on sinus rhythm (i.e. spontaneous conversion) in
differentcircumstances. Weusetheseestimatestogeneratethepseudo-effects. Finally,arguablythe
biggestchangeinvolvesLLMs. Inordertoextractfeaturesfromtrials,wegivetheirpublishedPDFs
to LLMs (specifically, GPT-4 and Claude) along with a prompt including example output. Next,
parsing code (also written by LLMs) converts the textual features to numerical (x,y,v). Thus,
LLMs can be used to aid meta-analysis, much as meta-analysis serves as a question-answering
system. This experience, and the results of the paper overall, reflect positively on the following
dilemma: can language models be used to rigorously answer scientific questions?
8 Conclusion
This paper resolves two longstanding problems with meta-analysis: (1) how to derive rigorous con-
clusionsfromtheuntrusted, lowerlevelsoftheevidencehierarchy, and(2)howtomanagepervasive
heterogeneity when pooling together randomized controlled trials. It develops fundamentally more
powerful meta-analysis algorithms based upon novel insights about full conformal prediction in
the presence of noise. We believe that our approach has the potential to increase the accuracy of
evidence synthesis, enhance the development of clinical practice guidelines, and ultimately improve
patientoutcomes. Furthermore, webelieveourapproachcouldmakeevidence-basedmedicinemore
harmoniousandinclusive,bydeprecatingpotentiallydivisive,controversialdata-exclusionpractices
which are presently rationalized as necessary for statistical rigor.
However, further work must be conducted before this potential can be realized. Conformal meta-
analysisdependsuponpriorsderivedfromuntrusteddata,butsuchpriorshaveyettobedeveloped.
Conformal meta-analysis should ideally be paired with ongoing efforts to develop foundation mod-
els from large healthcare databases. From a technical perspective, this paper is meant to initiate
the study of conformal meta-analysis, not to definitively solve it. The simulations indicate that the
intervalsofAlgorithm3andAlgorithm5areunnecessarilyloose; amorefine-grained, thoroughsta-
tisticalexaminationofmeta-analysisiswarranted. Also, itshouldbepossibletoobtaincomparably
tight intervals under weaker assumptions. Specifically, the assumption of exchangeability could be
relaxed, as in previous work on conformal prediction [Barber et al., 2023, Gibbs and Candès, 2024].
References
All of Us Research Program Investigators. The “all of us” research program. New England Journal
of Medicine, 381(7):668–676, 2019.
Lindsey Anderson, David R Thompson, Neil Oldridge, Ann-Dorthe Zwisler, Karen Rees, Nicole
Martin, and Rod S Taylor. Exercise-based cardiac rehabilitation for coronary heart disease.
Cochrane Database of Systematic Reviews, (1), 2016.
Anastasios N Angelopoulos, Stephen Bates, Clara Fannjiang, Michael I Jordan, and Tijana Zrnic.
Prediction-powered inference. Science, 382(6671):669–674, 2023.
30David Armstrong. Professionalism, indeterminacy and the ebm project. BioSocieties, 2(1):73–84,
2007.
W. L. Baker, C. Michael White, J. C. Cappelleri, J. Kluger, C. I. Coleman, and From the Health
Outcomes, Policy, and Economics (HOPE) Collaborative Group . Understanding heterogeneity
in meta-analysis: the role of meta-regression. International Journal of Clinical Practice, 63
(10):1426–1434, 2009. doi: https://doi.org/10.1111/j.1742-1241.2009.02168.x. URL https:
//onlinelibrary.wiley.com/doi/abs/10.1111/j.1742-1241.2009.02168.x.
Maria-Florina Balcan and Avrim Blum. A discriminative model for semi-supervised learning. Jour-
nal of the ACM (JACM), 57(3):1–46, 2010.
IdrizBalla,ElizanaPetrela,andAnestiKondili. Pharmacologicalconversionofrecentatrialfibrilla-
tion: arandomized, placebo-controlledstudyofthreeantiarrhythmicdrugs/yenibaslayanatriyal
fibrilasyonun ilaçla sinüs ritmine döndürülmesi: Üç antiaritmik ilaçla gerçeklestirilen randomize,
plasebo-kontrollü çalisma. The Anatolian Journal of Cardiology, 11(7):600, 2011.
Rina Foygel Barber, Emmanuel J Candes, Aaditya Ramdas, and Ryan J Tibshirani. Conformal
prediction beyond exchangeability. The Annals of Statistics, 51(2):816–845, 2023.
KjellBensonandArthurJHartz. Acomparisonofobservationalstudiesandrandomized,controlled
trials. New England Journal of Medicine, 342(25):1878–1886, 2000.
Dean Billheimer. Predictive inference and scientific reproducibility. The American Statistician, 73
(sup1):291–295, 2019.
Tim Bongartz, Alex J Sutton, Michael J Sweeting, Iain Buchan, Eric L Matteson, and Victor
Montori. Anti-tnf antibody therapy in rheumatoid arthritis and the risk of serious infections
and malignancies: systematic review and meta-analysis of rare harmful effects in randomized
controlled trials. Jama, 295(19):2275–2285, 2006.
Michael Borenstein. Avoiding common mistakes in meta-analysis: Understanding the distinct roles
of q, i-squared, tau-squared, and the prediction interval in reporting heterogeneity. Research
Synthesis Methods, 15(2):354–368, 2024. doi: https://doi.org/10.1002/jrsm.1678. URL https:
//onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1678.
Christopher J Bryan, Elizabeth Tipton, and David S Yeager. Behavioural science is unlikely to
change the world without a heterogeneity revolution. Nature human behaviour, 5(8):980–989,
2021.
HenryBuchwald,RhondaEstok,KyleFahrbach,DeirdreBanel,MichaelDJensen,WalterJPories,
JohnPBantle,andIsabellaSledge.Weightandtype2diabetesafterbariatricsurgery: systematic
review and meta-analysis. The American journal of medicine, 122(3):248–256, 2009.
Andreas Buja, Trevor Hastie, and Robert Tibshirani. Linear smoothers and additive models. The
Annals of Statistics, pages 453–510, 1989.
Evgeny Burnaev and Ivan Nazarov. Conformalized kernel ridge regression. In 2016 15th IEEE
international conference on machine learning and applications (ICMLA), pages 45–52. IEEE,
2016.
Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In International
Conference on Machine Learning, pages 844–853. PMLR, 2017.
31AndreaCipriani,JulianPTHiggins,JohnRGeddes,andGeorgiaSalanti. Conceptualandtechnical
challenges in network meta-analysis. Annals of internal medicine, 159(2):130–137, 2013.
Andrea Cipriani, Toshi A Furukawa, Georgia Salanti, Anna Chaimani, Lauren Z Atkinson, Yusuke
Ogawa, Stefan Leucht, Henricus G Ruhe, Erick H Turner, Julian PT Higgins, et al. Comparative
efficacyandacceptabilityof21antidepressantdrugsfortheacutetreatmentofadultswithmajor
depressive disorder: a systematic review and network meta-analysis. The Lancet, 391(10128):
1357–1366, 2018.
Bénédicte Colnet, Imke Mayer, Guanhua Chen, Awa Dieng, Ruohong Li, Gaël Varoquaux, Jean-
Philippe Vert, Julie Josse, and Shu Yang. Causal inference methods for combining randomized
trials and observational studies: a review. Statistical science, 39(1):165–191, 2024.
George E Dahl, Dong Yu, Li Deng, and Alex Acero. Context-dependent pre-trained deep neural
networks for large-vocabulary speech recognition. IEEE Transactions on audio, speech, and
language processing, 20(1):30–42, 2011.
AndrewMDaiandQuocVLe. Semi-supervisedsequencelearning. Advances in neural information
processing systems, 28, 2015.
Jonathan J Deeks and Julian PT Higgins. Statistical algorithms in review manager 5. Statistical
methods group of the Cochrane Collaboration, 1(11), 2010.
Ilker Demirel, Ahmed Alaa, Anthony Philippakis, and David Sontag. Prediction-powered general-
ization of causal inferences. In Forty-first International Conference on Machine Learning, 2024.
URL https://openreview.net/forum?id=QKnWXX3aVm.
Rebecca DerSimonian and Nan Laird. Meta-analysis in clinical trials. Controlled Clinical Trials,
7(3):177–188, 1986. ISSN 0197-2456. doi: https://doi.org/10.1016/0197-2456(86)90046-2. URL
https://www.sciencedirect.com/science/article/pii/0197245686900462.
Shai Feldman, Bat-Sheva Einbinder, Stephen Bates, Anastasios N Angelopoulos, Asaf Gendler,
and Yaniv Romano. Conformal prediction is robust to dispersive label noise. In Conformal and
Probabilistic Prediction with Applications, pages 624–626. PMLR, 2023.
Manuela L. Ferreira, Rob J.E.M. Smeets, Steven J. Kamper, Paulo H. Ferreira, and Luciana A.C.
Machado. Can We Explain Heterogeneity Among Randomized Clinical Trials of Exercise for
Chronic Back Pain? A Meta-Regression Analysis of Randomized Controlled Trials. Physical
Therapy, 90(10):1383–1403, 10 2010. ISSN 0031-9023. doi: 10.2522/ptj.20090332. URL https:
//doi.org/10.2522/ptj.20090332.
Christian Fiedler, Carsten W Scherer, and Sebastian Trimpe. Practical and rigorous uncertainty
bounds for gaussian process regression. In Proceedings of the AAAI conference on artificial
intelligence, volume 35, pages 7439–7447. AAAI, 2021.
Nanna B Finnerup, Nadine Attal, Simon Haroutounian, Ewan McNicol, Ralf Baron, Robert H
Dworkin, Ian Gilron, Maija Haanpää, Per Hansson, Troels S Jensen, et al. Pharmacotherapy for
neuropathic pain in adults: a systematic review and meta-analysis. The Lancet Neurology, 14
(2):162–173, 2015.
Andrew Gelman, John B Carlin, Hal S Stern, and Donald B Rubin. Bayesian data analysis.
Chapman and Hall/CRC, 1995.
32Isaac Gibbs and Emmanuel J Candès. Conformal inference for online prediction with arbitrary
distribution shifts. Journal of Machine Learning Research, 25(162):1–36, 2024.
Stephen Jay Gould. The median isn’t the message. Ceylon Medical Journal, 49(4), 2010.
Peter D Grünwald. The e-posterior. Philosophical Transactions of the Royal Society A, 381(2247):
20220146, 2023.
Gordon H. Guyatt, David L. Sackett, John C. Sinclair, Robert Hayward, Deborah J. Cook,
Richard J. Cook, Eric Bass, Hertzel Gerstein, Brian Haynes, Anne Holbrook, Roman Jaeschke,
Andreas Laupacls, Virginia Moyer, and Mark Wilson. Users’ Guides to the Medical Litera-
ture: IX. A Method for Grading Health Care Recommendations. JAMA, 274(22):1800–1804,
12 1995. ISSN 0098-7484. doi: 10.1001/jama.1995.03530220066035. URL https://doi.org/
10.1001/jama.1995.03530220066035.
Yuta Hamaguchi, Hisashi Noma, Kengo Nagashima, Tomohide Yamada, and Toshi A Furukawa.
Frequentist performances of bayesian prediction intervals for random-effects meta-analysis. Bio-
metrical Journal, 63(2):394–405, 2021.
J Hartmann‐Boyce, SC Chepkin, W Ye, C Bullen, and T Lancaster. Nicotine replacement therapy
versus control for smoking cessation. Cochrane Database of Systematic Reviews, (5), 2018. ISSN
1465-1858. doi: 10.1002/14651858.CD000146.pub5. URL https://doi.org//10.1002/
14651858.CD000146.pub5.
Joachim Hartung and Guido Knapp. On tests of the overall treatment effect in meta-analysis with
normally distributed responses. Statistics in medicine, 20(12):1771–1782, 2001.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-
dimensional ridgeless least squares interpolation. Annals of statistics, 50(2):949, 2022.
RachelHeyard,LeonhardHeld,SebastianSchneeweiss,andShirleyVWang. Designdifferencesand
variation in results between randomised trials and non-randomised emulations: meta-analysis
of rct-duplicate data. BMJ Medicine, 3(1), 2024. doi: 10.1136/bmjmed-2023-000709. URL
https://bmjmedicine.bmj.com/content/3/1/e000709.
Julian PT Higgins, Simon G Thompson, and David J Spiegelhalter. A re-evaluation of random-
effects meta-analysis. Journal of the Royal Statistical Society Series A: Statistics in Society, 172
(1):137–159, 2009.
Julian PT Higgins, James Thomas, Jacqueline Chandler, Miranda Cumpston, Tianjing Li,
Matthew J Page, and Vivian A Welch. Cochrane Handbook for Systematic Reviews of Inter-
ventions. John Wiley & Sons, Chichester, UK, 2nd edition, 2019.
Falk Hoffmann, Katharina Allers, Tanja Rombey, Jasmin Helbach, Amrei Hoffmann, Tim Mathes,
andDawidPieper. Nearly80systematicreviewswerepublishedeachday: observationalstudyon
trendsin epidemiology and reporting overtheyears2000-2019. Journal of Clinical Epidemiology,
138:1–11, 2021.
George Hripcsak, Patrick B Ryan, Jon D Duke, Nigam H Shah, Rae Woong Park, Vojtech Huser,
Marc A Suchard, Martijn J Schuemie, Frank J DeFalco, Adler Perotte, et al. Characterizing
treatment pathways at scale using the ohdsi network. Proceedings of the National Academy of
Sciences, 113(27):7329–7336, 2016.
33Maximilian Huhn, Adriani Nikolakopoulou, Johannes Schneider-Thoma, Marc Krause, Myrto
Samara, Natalie Peter, Thomas Arndt, Lio Bäckers, Philipp Rothe, Andrea Cipriani, et al.
Comparative efficacy and tolerability of 32 oral antipsychotics for the acute treatment of adults
with multi-episode schizophrenia: a systematic review and network meta-analysis. The Lancet,
394(10202):939–951, 2019.
Hilde M Huizenga, Ingmar Visser, and Conor V Dolan. Testing overall and moderator effects in
random effects meta-regression. British Journal of Mathematical and Statistical Psychology, 64
(1):1–19, 2011.
Guido W Imbens and Donald B Rubin. Causal inference in statistics, social, and biomedical
sciences. Cambridge university press, 2015.
Joanna IntHout, John PA Ioannidis, and George F Borm. The hartung-knapp-sidik-jonkman
method for random effects meta-analysis is straightforward and considerably outperforms the
standard dersimonian-laird method. BMC medical research methodology, 14:1–12, 2014.
Joanna IntHout, John PA Ioannidis, Maroeska M Rovers, and Jelle J Goeman. Plea for routinely
presenting prediction intervals in meta-analysis. BMJ open, 6(7):e010247, 2016.
John PA Ioannidis. Contradicted and initially stronger effects in highly cited clinical research.
Jama, 294(2):218–228, 2005.
Motonobu Kanagawa, Philipp Hennig, Dino Sejdinovic, and Bharath K Sriperumbudur. Gaus-
sian processes and kernel methods: A review on connections and equivalences. arXiv preprint
arXiv:1807.02582, 2018.
Emir Karaçağlar, İlyas Atar, Süleyman Özbiçer, Atilla Sezgin, Salih Özçobanoğlu, Ayse Canan
Yazici, Bülent Özin, and Haldun Müderrisoğlu. Amiodarone versus direct current cardioversion
intreatmentofatrialfibrillationaftercardiacsurgery. Turkish Journal of Clinics and Laboratory,
10(1):26–32, 2019.
George E Kochiadakis, Nikos E Igoumenidis, Michail E Hamilos, Maria E Marketou, Gregory I
Chlouverakis, and Panos E Vardas. A comparative study of the efficacy and safety of pro-
cainamide versus propafenone versus amiodarone for the conversion of recent-onset atrial fibril-
lation. The American journal of cardiology, 99(12):1721–1725, 2007.
Michel Komajda, John JV McMurray, Henning Beck-Nielsen, Ramon Gomis, Markolf Hanefeld,
Stuart J Pocock, Paula S Curtis, Nigel P Jones, and Philip D Home. Heart failure events with
rosiglitazone in type 2 diabetes: data from the record clinical trial. European heart journal, 31
(7):824–831, 2010.
Sören R Künzel, Jasjeet S Sekhon, Peter J Bickel, and Bin Yu. Metalearners for estimating het-
erogeneous treatment effects using machine learning. Proceedings of the national academy of
sciences, 116(10):4156–4165, 2019.
Joseph Lau, John PA Ioannidis, Norma Terrin, Christopher H Schmid, and Ingram Olkin. The
case of the misleading funnel plot. Bmj, 333(7568):597–600, 2006.
Jing Lei and Larry Wasserman. Distribution-free prediction bands for non-parametric regression.
Journal of the Royal Statistical Society Series B: Statistical Methodology, 76(1):71–96, 2014.
Luz M Letelier, Kamol Udol, Javier Ena, Bruce Weaver, and Gordon H Guyatt. Effectiveness of
34amiodarone for conversion of atrial fibrillation to sinus rhythm: a meta-analysis. Archives of
Internal Medicine, 163(7):777–785, 2003.
Tianjing Li, Kristina Lindsley, Benjamin Rouse, Hwanhee Hong, Qiyuan Shi, David S Friedman,
Richard Wormald, and Kay Dickersin. Comparative effectiveness of first-line medications for
primary open-angle glaucoma: a systematic review and network meta-analysis. Ophthalmology,
123(1):129–140, 2016.
Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel “Ridgeless” regression can gen-
eralize. The Annals of Statistics, 48(3):1329 – 1347, 2020. doi: 10.1214/19-AOS1849. URL
https://doi.org/10.1214/19-AOS1849.
Richard J Light and David B Pillemer. Summing up. Harvard University Press, 1984.
Jona Lilienthal, Sibylle Sturtz, Christoph Schürmann, Matthias Maiworm, Christian Röver, Tim
Friede, and Ralf Bender. Bayesian random-effects meta-analysis with empirical heterogeneity
priors for application in health technology assessment with very few studies. Research Synthesis
Methods, 15(2):275–287, 2024.
Andreas Lundh, Joel Lexchin, Barbara Mintzes, Jeppe B Schroll, and Lisa Bero. Industry spon-
sorship and research outcome. Cochrane database of systematic reviews, (2), 2017.
JoAnnEManson,NancyRCook,I-MinLee,WilliamChristen,ShariSBassuk,SamiaMora,Heike
Gibson, David Gordon, Trisha Copeland, Denise D’Agostino, et al. Vitamin d supplements and
preventionofcancerandcardiovasculardisease. New England Journal of Medicine, 380(1):33–44,
2019.
Luca Masserano, Tommaso Dorigo, Rafael Izbicki, Mikael Kuusela, and Ann B Lee. Simulator-
basedinferencewithwaldo: Confidenceregionsbyleveragingpredictionalgorithmsandposterior
estimators for inverse problems. Proceedings of Machine Learning Research, 206, 2023.
DavidAMcAllester. Somepac-bayesiantheorems. InProceedings of the eleventh annual conference
on Computational learning theory, pages 230–234, 1998.
Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. Communications of
the ACM, 65(7):33–35, 2022.
Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan M Krumholz, Jure Leskovec,
Eric J Topol, and Pranav Rajpurkar. Foundation models for generalist medical artificial intelli-
gence. Nature, 616(7956):259–265, 2023.
Marilyn C Morris and Robert M Nelson. Randomized, controlled trials as minimal risk: an ethical
analysis. Critical care medicine, 35(3):940–944, 2007.
Marcus R Munafò, Brian A Nosek, Dorothy VM Bishop, Katherine S Button, Christopher D
Chambers, Nathalie Percie du Sert, Uri Simonsohn, Eric-Jan Wagenmakers, Jennifer J Ware,
and John Ioannidis. A manifesto for reproducible science. Nature human behaviour, 1(1):1–9,
2017.
M Hassan Murad, Noor Asi, Mouaz Alsawas, and Fares Alahdab. New evidence pyramid. BMJ
Evidence-Based Medicine, 21(4):125–127, 2016.
Kengo Nagashima, Hisashi Noma, and Toshi A. Furukawa. pimeta: an r package of prediction
intervals for random-effects meta-analysis. arXiv, 2021.
35Willie Neiswanger and Aaditya Ramdas. Uncertainty quantification using martingales for misspec-
ified gaussian processes. In Algorithmic learning theory, pages 963–982. PMLR, 2021.
Jersey Neyman. Sur les applications de la théorie des probabilités aux experiences agricoles: Essai
des principes. Roczniki Nauk Rolniczych, 10(1):1–51, 1923.
Randal S Olson, William La Cava, Patryk Orzechowski, Ryan J Urbanowicz, and Jason H Moore.
Pmlb: alargebenchmarksuiteformachinelearningevaluationandcomparison. BioDatamining,
10:1–13, 2017.
Matthew J Page, Joanne E McKenzie, Patrick M Bossuyt, Isabelle Boutron, Tammy C Hoffmann,
Cynthia D Mulrow, Larissa Shamseer, Jennifer M Tetzlaff, Elie A Akl, Sue E Brennan, et al.
Theprisma2020statement: anupdatedguidelineforreportingsystematicreviews. International
journal of surgery, 88:105906, 2021.
Christopher Partlett and Richard D Riley. Random effects meta-analysis: coverage performance of
95% confidence and prediction intervals following reml estimation. Statistics in medicine, 36(2):
301–317, 2017.
J.Pearl.Causality.Causality: Models,Reasoning,andInference.CambridgeUniversityPress,2009.
ISBN 9780521895606. URL https://books.google.com/books?id=f4nuexsNVZIC.
Coby Penso and Jacob Goldberger. A conformal prediction score that is robust to label noise.
arXiv preprint arXiv:2405.02648, 2024.
Aleksandr Podkopaev and Aaditya Ramdas. Distribution-free uncertainty quantification for classi-
fication under label shift. In Uncertainty in artificial intelligence, pages 844–853. PMLR, 2021.
Kirsty M. Rhodes, Rebecca M. Turner, Ian R. White, Dan Jackson, David J. Spiegelhalter, and
Julian P. T. Higgins. Implementing informative priors for heterogeneity in meta-analysis us-
ing meta-regression and pseudo data. Statistics in Medicine, 35(29):5495–5511, 2016. doi:
https://doi.org/10.1002/sim.7090. URL https://onlinelibrary.wiley.com/doi/abs/
10.1002/sim.7090.
W Scott Richardson, Mark C Wilson, Jim Nishikawa, and Robert S Hayward. The well-built
clinical question: a key to evidence-based decisions. ACP journal club, 123(3):A12–A13, 1995.
Richard D Riley, Julian P T Higgins, and Jonathan J Deeks. Interpretation of random effects
meta-analyses. BMJ, 342, 2011. ISSN 0959-8138. doi: 10.1136/bmj.d549. URL https://www.
bmj.com/content/342/bmj.d549.
Mark Rippetoe. Starting Strength: Basic Barbell Training. Aasgaard Company, 3rd edition, 2017.
ISBN 9780982522738.
Christian Röver. Bayesian random-effects meta-analysis using the bayesmeta r package. arXiv
preprint arXiv:1711.08683, 2017.
DonaldBRubin. Estimatingcausaleffectsoftreatmentsinrandomizedandnonrandomizedstudies.
Journal of educational Psychology, 66(5):688, 1974.
David L Sackett. Bias in analytic research. In The case-control study consensus and controversy,
pages 51–63. Elsevier, 1979.
H Schünemann, J Brożek, G Guyatt, and A Oxman, editors. GRADE handbook for grading quality
of evidence and strength of recommendations. The GRADE Working Group, 2013.
36MatthiasSeeger. Pac-bayesiangeneralisationerrorboundsforgaussianprocessclassification. Jour-
nal of machine learning research, 3(Oct):233–269, 2002.
Matteo Sesia, YX Wang, and Xin Tong. Adaptive conformal classification with noisy labels. arXiv
preprint arXiv:2309.05092, 2023.
Glenn Shafer and Vladimir Vovk. A tutorial on conformal prediction. Journal of Machine Learning
Research, 9(3), 2008.
JohnShawe-TaylorandRobertCWilliamson. Apacanalysisofabayesianestimator. InProceedings
of the tenth annual conference on Computational learning theory, pages 2–9, 1997.
Kurex Sidik and Jeffrey N Jonkman. On constructing confidence intervals for a standardized mean
difference in meta-analysis. Communications in Statistics-Simulation and Computation, 32(4):
1191–1203, 2003.
Uri Simonsohn, Joseph Simmons, and Leif D Nelson. Above averaging in literature reviews. Nature
Reviews Psychology, 1(10):551–552, 2022.
KaranSinghal,TaoTu,JurajGottweis,RorySayres,ElleryWulczyn,LeHou,KevinClark,Stephen
Pfohl, Heather Cole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering
with large language models. arXiv preprint arXiv:2305.09617, 2023.
Richard S Slavik and Peter J Zed. Intravenous amiodarone for conversion of atrial fibrillation:
Misled by meta-analysis? Pharmacotherapy: The Journal of Human Pharmacology and Drug
Therapy, 24(6):792–798, 2004.
Teresa C Smith, David J Spiegelhalter, and Andrew Thomas. Bayesian approaches to random-
effects meta-analysis: a comparative study. Statistics in medicine, 14(24):2685–2699, 1995.
PeterSpirtes, ClarkGlymour, andRichardScheines. Causation, prediction, and search. MITpress,
2001.
Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian pro-
cess optimization in the bandit setting: No regret and experimental design. arXiv preprint
arXiv:0912.3995, 2009.
T. D. Stanley and Stephen B. Jarrell. Meta-regression analysis: A quantitative method of lit-
erature surveys. Journal of Economic Surveys, 3(2):161–170, 1989. doi: https://doi.org/10.
1111/j.1467-6419.1989.tb00064.x. URL https://onlinelibrary.wiley.com/doi/abs/
10.1111/j.1467-6419.1989.tb00064.x.
Tom D Stanley, Evan C Carter, and Hristos Doucouliagos. What meta-analyses reveal about the
replicability of psychological research. Psychological bulletin, 144(12):1325, 2018.
David Stutz, Abhijit Guha Roy, Tatiana Matejovicova, Patricia Strachan, Ali Taylan Cemgil, and
Arnaud Doucet. Conformal prediction under ambiguous ground truth. Transactions on Machine
Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=
CAd6V2qXxc.
SV Subramanian, Rockli Kim, and Nicholas A Christakis. The “average” treatment effect: A
constructripeforretirement.acommentaryondeatonandcartwright. Socialscience&medicine,
210:77–82, 2018.
37Stuart P Thomas, Duncan Guy, Elisabeth Wallace, Roselyn Crampton, Pat Kijvanit, Vicki Eipper,
David L Ross, and Mark J Cooper. Rapid loading of sotalol or amiodarone for management of
recent onset symptomatic atrial fibrillation: a randomized, digoxin-controlled trial. American
heart journal, 147(1):E3, 2004.
Simon G. Thompson and Julian P. T. Higgins. How should meta-regression analyses be undertaken
and interpreted? Statistics in Medicine, 21(11):1559–1573, 2002. doi: https://doi.org/10.1002/
sim.1187. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.1187.
Ingrid Toews, Andrew Anglemyer, John LZ Nyirenda, Dima Alsaid, Sara Balduzzi, Kathrin Grum-
mich, Lukas Schwingshackl, and Lisa Bero. Healthcare outcomes assessed with observational
study designs compared with those assessed in randomized trials: a meta-epidemiological study.
Cochrane Database of Systematic Reviews, (1), 2024.
Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang,
AndrewCarroll,CharlesLau,RyutaroTanno,IraKtena,AnilPalepu,BasilMustafa,Aakanksha
Chowdhery, Yun Liu, Simon Kornblith, David Fleet, Philip Mansfield, Sushant Prakash, Re-
nee Wong, Sunny Virmani, Christopher Semturs, S. Sara Mahdavi, Bradley Green, Ewa Domi-
nowska, Blaise Aguera y Arcas, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias,
Karan Singhal, Pete Florence, Alan Karthikesalingam, and Vivek Natarajan. Towards gen-
eralist biomedical ai. NEJM AI, 1(3):AIoa2300138, 2024. doi: 10.1056/AIoa2300138. URL
https://ai.nejm.org/doi/abs/10.1056/AIoa2300138.
Areti Angeliki Veroniki. Random-effects meta-analysis methods in revman (cochrane statistical
editor training 2022). YouTube video, 2022. URL https://www.youtube.com/watch?v=
4gsaUl5uh70.
AretiAngelikiVeroniki,DanJackson,RalfBender, OliverKuss, DeanLangan, JulianP.T.Higgins,
Guido Knapp, and Georgia Salanti. Methods to calculate uncertainty in the estimated overall
effect size from a random-effects meta-analysis. Research Synthesis Methods, 10(1):23–43, 2019.
doi: https://doi.org/10.1002/jrsm.1319. URL https://onlinelibrary.wiley.com/doi/
abs/10.1002/jrsm.1319.
Wolfgang Viechtbauer. Bias and efficiency of meta-analytic variance estimators in the random-
effects model. Journal of Educational and Behavioral Statistics, 30(3):261–293, 2005.
Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning in a random
world, volume 29. Springer, 2005.
Stefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment effects using
random forests. Journal of the American Statistical Association, 113(523):1228–1242, 2018.
Shirley V Wang, Sebastian Schneeweiss, Jessica M Franklin, Rishi J Desai, William Feldman,
Elizabeth M Garry, Robert J Glynn, Kueiyu Joshua Lin, Julie Paik, Elisabetta Patorno, et al.
Emulation of randomized clinical trials with nonrandomized database analyses: results of 32
clinical trials. Jama, 329(16):1376–1385, 2023.
Christopher KI Williams and Carl Edward Rasmussen. Gaussian processes for machine learning,
volume 2. MIT press Cambridge, MA, 2006.
HyeSunYun, DavidPogrebitskiy, IainJMarshall, andByronCWallace. Automaticallyextracting
numerical results from randomized controlled trials with large language models. arXiv preprint
arXiv:2405.01686, 2024.
38Tijana Zrnic and Emmanuel J Candès. Cross-prediction-powered inference. Proceedings of the
National Academy of Sciences, 121(15):e2322083121, 2024.
A Appendix
A.1 Simulation Details and Full Results
The simulations were performed using three partially-synthetic biomedical datasets from the Penn
Machine Learning Benchmark [Olson et al., 2017]: 1196_BNG_pharynx, 1201_BNG_breastTumor,
and 1193_BNG_lowbwt. We randomly subsample training data (X,U) as well as test data (x,u).
The kernel matrix K is generated using either the Gaussian or Laplace kernel as κ. For con-
sistency across datasets having different scales, a parameter effect noise > 0 is introduced, and
the distribution of V is constructed to satisfy effect noise = E(V i)2/E|U i|. Specifically V i ∼
Exp(1)·(cid:112) effect noise·E|U i|. Similarly, to produce prior means M of varying quality, a parameter
prior error > 0 is introduced, and the distribution of M satisfies MSE(M,U) = prior error·V(U).
Furthermore, the difference between M and U should not be purely random — otherwise, using
KRR to explain this difference would be hopeless. Instead, we generate a random offset func-
tion f˜(x) = (cid:80) ig iκ(x˜ i,x) for random held-out data x˜ i and g i ∼ N(0,1). Since f˜ is an RKHS
element generated from random data, there is some hope in approximating it using the training
data. Letting F˜ be f˜ applied to the training features, we generate M = pF˜ + (1 − p)U where
(cid:113)
p = prior error·V(U)/MSE(U,F˜).
All simulations are averaged over 32 random splits. Intervals are computed for between 256 and
768 test data in each run. Due to the efficiency of our proposed algorithms, all experiments are
capable of running on a free Google Colab instance.
width Simulation1(badprior) width Simulation1(okayprior) width Simulation1(goodprior)
2000 2000 2000
1500 1500 1500
HKSJ HKSJ HKSJ
CMA
1000 1000 1000
CMA
500 500 500
n n CnMA
20 50 100 200 500 20 50 100 200 500 20 50 100 200 500
width Simulation1(badprior) width Simulation1(okayprior) width Simulation1(goodprior)
70 70 70
60 60 60
50 50 50
40 40 40
30 CH MKS AJ 30 HKSJ 30 HKSJ
20 20 CMA 20
10 10 10 CMA
0 n 0 n 0 n
20 50 100 200 500 20 50 100 200 500 20 50 100 200 500
width Simulation1(badprior) width Simulation1(okayprior) width Simulation1(goodprior)
2500 HKSJ 2500 HKSJ 2500 HKSJ
2000 2000 2000
1500 1500 1500
1000 CMA 1000 1000
500 500 CMA 500
n n CnMA
20 50 100 200 500 20 50 100 200 500 20 50 100 200 500
Simulation 1: Rows are different datasets; the different columns, from left to right, set prior error equal to
3.0, 0.9, and 0.2, respectively. α=0.1 and effect noise=0.5 were used.
39coverage Simulation2(smalln) coverage Simulation2(largen)
1. 1.
CMA
0.95 0.95 CMA
0.9 HKSJ 0.9 HKSJ
effect effect
0 100 200 300 400 noise 0 100 200 300 400 noise
coverage Simulation2(smalln) coverage Simulation2(largen)
1. CMA 1. CMA
0.95 0.95
0.9 0.9
effecHtKSJ effecHtKSJ
0 100 200 300 400 noise 0 100 200 300 400 noise
coverage Simulation2(smalln) coverage Simulation2(largen)
1. 1.
CMA CMA
0.95 0.95
0.9 0.9
HKSJ
effecHtKSJ effect
0 100 200 300 400 noise 0 100 200 300 400 noise
Simulation 2: Rows are different datasets. n = 50 and n = 200 are used in the left and right columns,
respectively. prior error is set low to 0.2.
coverage Simulation3(smalln) coverage Simulation3(largen)
1.00 1.00
η>0 η>0
0.95 0.95
η=0 η=0
0.90 0.90
effect effect
0 20 40 60 80 100 noise 0 20 40 60 80 100 noise
coverage Simulation3(smalln) coverage Simulation3(largen)
η=0 η=0
1.00 1.00
η>0 η>0
0.95 0.95
0.90 0.90
effect effect
0 20 40 60 80 100 noise 0 20 40 60 80 100 noise
coverage Simulation3(smalln) coverage Simulation3(largen)
1.00 η>0 1.00 η>0
0.95 η=0 0.95 η=0
0.90 0.90
effect effect
0 20 40 60 80 100 noise 0 20 40 60 80 100 noise
Simulation 3: Rows are different datasets; n = 50 and n = 200 are used in the left and right columns,
respectively. α=0.1 and prior error=0.1 were used.
40width Simulation4(smalln) width Simulation4(largen)
5000 5000
4000 Prior 4000 Prior
3000 CMA 3000
2000 2000 CMA
HKSJ
1000 1000 HKSJ
prior prior
0 1 2 3 4 5 6 error 0 1 2 3 4 5 6 error
width Simulation4(smalln) width Simulation4(largen)
100 100
80 Prior 80 Prior
60 CMA 60 CMA
40 40
HKSJ HKSJ
20 prior 20 prior
0 1 2 3 4 5 6 error 0 1 2 3 4 5 6 error
width Simulation4(smalln) width Simulation4(largen)
5000 5000
Prior Prior
4000 4000
3000 HKSJ 3000
HKSJ
2000 2000
CMA
1000 1000 CMA
prior prior
0 1 2 3 4 5 6 error 0 1 2 3 4 5 6 error
Simulation 4: Rows are different datasets; n = 16 and n = 200 are used in the left and right columns,
respectively. A low effect noise=0.02 was set, along with α=0.1.
widthSimulation5(minisculenoise) width Simulation5(verylownoise) width Simulation5(lownoise)
800 800 800
CMA-CL
700 700 700
600 600 CMA-CL 600
500 CMA 500 CMA 500 CMA
CMA-CL
n n n
20 40 60 80 20 40 60 80 20 40 60 80
widthSimulation5(minisculenoise) width Simulation5(verylownoise) width Simulation5(lownoise)
100 100 100 CMA-CL
80 80 CMA-CL 80
60 60 60
40 40 40
CMA-CL
20 CnMA 20 CnMA 20 CnMA
20 40 60 80 20 40 60 80 20 40 60 80
widthSimulation5(minisculenoise) width Simulation5(verylownoise) width Simulation5(lownoise)
700 700 700 CMA-CL
600 600 CMA-CL 600
500 500 500
400 400 400
CMA CMA CMA
CnMA-CL n n
20 40 60 80 20 40 60 80 20 40 60 80
Simulation 5: Rows are different datasets; effect noise is set to 0.00000005, 0.1, and 2 in the left, middle
and right columns, respectively. A low confidence α=0.25 is set.
41A.2 Case Study Details
We follow the meta-analysis process illustrated in Figure 2. First, we determine the domain X of x.
Helpfully, Letelier et al. [2003] identified 10 potentially-relevant features, such as mean age, mean
AF duration, and amiodarone therapy protocol (e.g. “IV, 5 mg/kg in 30 min + 10 mg/kg in 20
h” or “Oral, 600 mg/d for 3 wk”). In order to extract these features from the trial, we give their
published PDFs to a publicly-available language model, along with a prompt including example
output. This extraction is fairly reliable, echoing the experience of Yun et al. [2024]. Next, parsing
code (also written by the language model) converts the extracted textual features to numerical
vectors x. As exemplified in Figure 10, this parsing can be tedious and error-prone, even with a
state-of-the-art LLM. Our final predictions end up relying on just three features: total amiodarone
dosage in the first 24 hours, whether mean AF duration was above or below 48 hours, and the
number of patients (which is a sensible feature when predicting trials rather than effects).
In lieue of a powerful pretrained foundation model, we base µ and κ on the critique of Slavik and
Zed [2004]. They describe how multiple sources of heterogeneity, such as dosage, could impact
the effect of amiodarone. Most importantly, amiodarone has a relatively slow course of action,
whereas patients with recent-onset AF (usually defined as an AF duration of less than 48 hours)
have a high chance of spontaneously reverting to normal sinus rhythm. (Letelier et al. [2003] also
noted this pattern). With recent-onset AF, median spontaneous conversion rates are “11% at 2
hours after admission, 18% at 3 hours, 25% at 4 hours, 31% at 6 hours, 39% at 8 hours, 38% at
12 hours, 58% at 24 hours, and 67% at 48 hours.”. This compares to only 0–8% within the first
72 hours for patients with persistent AF. We identify 8 further trials which compared amiodarone
to an active comparison. We compute pseudo-effects (as relative risk) by taking the ratio of the
observedprobabilityofconversionunderamiodarone,overtheaforementionedestimatedprobability
ofspontaneousconversionovertime. Suchindirectcomparisonisreminiscentofhownetworkmeta-
analysisworks[Ciprianietal.,2013]. WetrainedaReLUdeepnetworkuponthe3relevantfeatures
in these synthetically-labeled data.
42Can you extract the following features from the attached PDF paper? I gave example values, from another paper, which
should be replaced with the actual values in this paper. The only relevant outcome is conversion to normal sinus
rhythm. Also, create a new key like "Results": [a, b, c, d] where a is the number of amiodarone patients converted to
sinus rhythm, b is the total number of amiodarone patients, c is the number of comparison patients converted to
sinus rhythm, and d is the total number of comparison patients. Answer as JSON.
{"Name": "Villani et al.11 (Italy) 2000", "Features": { "Amiodarone Therapy Protocol": "Oral, 400 mg/d for 1 mo", "
Comparison Treatment": "Oral digoxin, 0.25 mg/d or oral diltiazem hydrochloride 180- 360 mg/d for 1 mo", "Time to
Outcome Measure": "1 mo", "Number of Amiodarone Patients": "44", "Number of Control Patients": "30", "Fraction with
CV Disease": "47", "Mean Left Atrium Size, mm": "50", "Mean AF Duration": "17 wk", "Mean Age": "58", "Fraction Male":
"67", "Adequate Concealment of Treatment": "No", "Follow-up Fraction": "100", "Masked Patients": "Yes", "Masked
Caregiver": "no", "Masked Assessor": "no" }}
Figure 8: Prompt used to extract relevant data from trial PDFs.
In the attached JSON list, each element represents a study described by the "Features" attribute. Convert these
features to real numbers so they can be provided to a learning algorithm.
* amiodarone treatment should be the total dosage, in milligrams, which is given over the first 24 hours. If the
dosage is specified per kg bodyweight, then take into account the average bodyweight of the patients.
* comparison treatment should be converted to [0,1], where 0 denotes placebo and 1 an intensive, high dose comparison
regimen.
* if the fraction of male patients is unknown, just assume it is 0.5.
* fraction with CV disease and followup fraction were reported as integers, so for example 78 should be converted to
0.78.
* number of control and amiodarone patients should be just copied over as integers
* mean AF duration and time to outcome measure should be converted to -1 for <= 48 hours and 1 for > 48 hours
* mean left atrium size and mean age should be rescaled to \[-1,1\] where 0 is the average of the feature, -1 is the
minimum, and 1 is the maximum
* the boolean features should be rescaled to \[-1, 1\], where -1 means false, 1 means true, and 0 means not present
or not confident.
* include the same keys for all the studies, using the original key names.
Answer as JSON; no further explanation is necessary.
Figure 9: Prompt used to convert extracted data to numerical features.
431 def parse_dosing_protocol(protocol):
2 if protocol is None or protocol.lower() == 'not specified':
3 return 0
4
5 weight = 70 # Average body weight in kg
6 total_mg = 0 # Initialize total milligrams
7
8 # Normalize and break down the protocol into components
9 protocol = protocol.lower().replace('over', 'in').replace('plus', ',')
10 phases = protocol.split('+')
11
12 for phase in phases:
13 parts = phase.split(',')
14 for part in parts:
15 part = part.strip()
16 tokens = part.split()
17 dose = 0
18 rate_based = False
19 duration = 24 # Default duration is 24 hours unless specified
20
21 # Parse the dose and units
22 for i, token in enumerate(tokens):
23 try:
24 # Attempt to convert token to float to find numeric values
25 potential_dose = float(token)
26
27 # Check for units immediately following the numeric value
28 if i + 1 < len(tokens):
29 unit = tokens[i + 1]
30 if 'g' in unit and 'mg' not in unit:
31 potential_dose *= 1000 # Convert grams to milligrams
32 elif 'mg/kg' in unit:
33 potential_dose *= weight # Convert to total mg for given
(cid:44)→ weight
34
35 # Determine if the dose is time-bound
36 if 'hour' in unit or 'h' in unit or 'min' in unit:
37 rate_based = True # The dose is a rate per time
38 duration = extract_duration(part)
39 if 'min' in unit:
40 duration /= 60 # Convert minutes to hours
41 dose = potential_dose
42 break
43 except ValueError:
44 continue # Not a number, move to next token
45
46 # Apply the dose calculation based on the duration and whether it's
(cid:44)→ rate-based
47 if rate_based:
48 total_mg += min(duration, 24) * dose # Apply the rate up to 24 hours
49 elif 'day' in part:
50 if 'first' in part or '1 day' in part or '1 week' in part:
51 total_mg += dose # Apply if it specifies the first day or week
52 else:
53 total_mg += dose # Single dose or calculated for the duration
54
55 return total_mg
Figure10: PythoncodegeneratedbyGPT-4toparse4a4ndconvertamiodaronetherapyprotocols. Generating
this code required multiple rounds of interaction with the language model. This code still has mild bugs,
which are left untouched to accurately convey contemporary expectations about in-context parsing.