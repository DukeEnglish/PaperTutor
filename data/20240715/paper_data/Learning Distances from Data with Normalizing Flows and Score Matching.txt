Learning Distances from Data with Normalizing Flows
and Score Matching
PeterSorrenson∗ DanielBehrend-Uriarte
HeidelbergUniversity HeidelbergUniversity
ChristophSchnörr UllrichKöthe
HeidelbergUniversity HeidelbergUniversity
Abstract
Density-baseddistances(DBDs)offeranelegantsolutiontotheproblemofmet-
riclearning. BydefiningaRiemannianmetricwhichincreaseswithdecreasing
probabilitydensity,shortestpathsnaturallyfollowthedatamanifoldandpointsare
clusteredaccordingtothemodesofthedata. Weshowthatexistingmethodsto
estimateFermatdistances,aparticularchoiceofDBD,sufferfrompoorconver-
genceinbothlowandhighdimensionsduetoi)inaccuratedensityestimatesand
ii)relianceongraph-basedpathswhichareincreasinglyroughinhighdimensions.
To address these issues, we propose learning the densities using a normalizing
flow,agenerativemodelwithtractabledensityestimation,andemployingasmooth
relaxationmethodusingascoremodelinitializedfromagraph-basedproposal.
Additionally,weintroduceadimension-adaptedFermatdistancethatexhibitsmore
intuitive behavior when scaled to high dimensions and offers better numerical
properties. Ourworkpavesthewayforpracticaluseofdensity-baseddistances,
especiallyinhigh-dimensionalspaces.
1 Introduction
Metric learning aims to determine a distance metric that accurately measures the similarity or
dissimilarity between data points. Typically, data points are mapped into a space where simple
metricslikeEuclideandistancecanbeapplied. Althoughthismethodiscomputationallyefficient,it
islimitedbytheconfigurationspossibleinEuclideanspace,wherearbitrarydistancerelationships
betweensetsofpointscannotberepresented.
AmoreflexiblebutcomputationallyexpensiveapproachinvolvesdefiningaRiemannianmetricinthe
dataspaceandsolvingforgeodesicdistances. Thisallowsforarbitrarydistancerelationships,butit
requireschoosinganappropriateRiemannianmetric. Fermatdistances[Groismanetal.,2022],which
areatypeofdensity-baseddistance[Bousquetetal.,2003],offeranelegantsolution: themetric
shouldbeconformal(amultipleoftheidentitymatrix)andinverselyproportionaltotheprobability
density. Inthisway,geodesicspassthroughhigh-densityregionsofthedata,respectinganyinherent
manifoldstructure. ThisapproachisconsistentwithFermat’sprincipleofleasttimeinoptics,where
theinversedensityfunctionslikearefractiveindex.
Toaddressthecomputationalchallengeofcomputinggeodesicswiththismetric,previousworkhas
approximatedtrajectoriesusingshortestpathsinagraphofnearestneighbors[Bijraletal.,2012].
TheedgeweightsareapproximatedbyraisingEuclideandistancestoanappropriatepower,with
theoretical analysis supporting the consistency of this approximation. However, in practice, we
∗Correspondencetopeter.sorrenson@iwr.uni-heidelberg.de.
Preprint.Underreview.
4202
luJ
21
]GL.sc[
1v79290.7042:viXraFigure1: Groundtruthandestimatedgeodesicsona2-dimensionalspiraldataset. Thegreenline
showsagroundtruthgeodesic,solvedviarelaxationonthetruedensity(Algorithm1). Thedashed
orangelineisanestimateforthegeodesic,foundastheshortestpathinanearestneighborgraph
withedgeweightscalculatedfromanormalizingflowdensity(Algorithm2).
observe that convergence quickly plateaus, showing no significant improvement with increasing
samplesize.
Byemployingmoreaccurateedgeweightsobtainedthroughnormalizingflows,weachievemuch
fasterconvergence. Additionally,previousapproachesdidnotaccountforthelimitationsofgraph-
basedmethodsinhigherdimensions,wherepathsbecomeincreasinglyroughduetothecurseof
dimensionality. To address this, we use a relaxation scheme to smooth trajectories and maintain
satisfactoryconvergence. Wefindthattrainingmodelsviascorematchingismoreeffectiveforthis
purposethanusingnormalizingflows.
Insummary,ourcontributionsarethreefold:First,weintroducetheuseofnormalizingflowstoobtain
moreaccurateedgeweightsforestimatingFermatdistances,significantlyimprovingconvergence
rates. Second,weaddressthelimitationsofgraph-basedmethodsinhigh-dimensionalspacesby
implementingasmoothrelaxationschemeusingscorematching,whichbettermaintainsthecontinuity
andsmoothnessofthetrajectories. Finally,weproposeadimension-adaptedFermatdistancethat
scalesintuitivelywithincreasingdimensionalityandexhibitssuperiornumericalproperties,paving
thewayforpracticalapplicationsofdensity-baseddistancesincomplex,high-dimensionaldatasets.
Throughtheseinnovations,wedemonstratethepracticalityandeffectivenessofourapproach,opening
upanewpossibilityformetriclearningusingdensity-baseddistancesinhigh-dimensionalspaces.
2 Relatedwork
Bousquetetal.[2003]introducestheideaofchangingthegeometryofthedataspacebyuseofa
conformaldensity-basedmetricinordertogetmoreinformativedistancefunctions. Howeverthey
donotattempttocalculategeodesicsordistancesimpliedbythisgeometry,insteadlinkingittoa
modifiedSVMalgorithmusedforsemi-supervisedlearning. SajamaandOrlitsky[2005]alsouses
adensity-basedmetricforsemi-supervisedlearning. Theyestimatedensitiesusingkerneldensity
estimationandconstructagraphofnearestneighbordistancesfromwhichtheycomputegeodesics.
Bijraletal.[2012]continuesthislineofworkbyapproximatingthedensityasinverselyproportional
toapoweroftheEuclideandistancebetweencloseneighbors. Hwangetal.[2016]givesconvergence
guaranteesforthepower-weightedshortestpathdistancestogeodesicdistancesunderthedensity-
dependent metric. Several other papers give similar results, and some practical implementations
[Chuetal.,2020,Littleetal.,2022,Groismanetal.,2022,Moscovichetal.,2017,Alamgirand
VonLuxburg,2012,MckenzieandDamelin,2019,Littleetal.,2020].Termsusedarepower-weighted
2shortestpathdistances(PWSPD)[Littleetal.,2022]andFermatdistances[Groismanetal.,2022,
Trillosetal.,2023].
Onecrucialcontributionofourworkistocheckthatproposedmethodsactuallyconvergeinpractice,
regardlessoftheoreticalguarantees. Previousworkdoesnotcompareshortestpathstogroundtruth
geodesics,evenforauniformdistribution,wheregeodesicsaresimplystraightlines. Weaddressthis
majorshortcomingbycomputingthegroundtruthgeodesicsforproblemswithknowndensityand
findthatexistingmethodsshowverypoorconvergence.
3 Background
3.1 Riemanniangeometry
Riemanniangeometryofferspowerfultoolsforanalyzingcurvedspaces,whichcanbeparticularly
usefulforunderstandingandoptimizingovernon-Euclideanspacessuchasmanifolds.Afundamental
objectinthisfieldisthemetrictensor,wheredifferentchoicesofmetriccanleadtovastlydifferent
geometricstructures,suchaspositivecurvature(e.g.,aspherewhichclosesinonitself)ornegative
curvature(e.g.,hyperbolicspacewhichexpandsexponentiallyquickly),aswellasmorecomplicated
mixturesofthetwo.
Moreformally,aRiemannianmanifoldMisasmoothn-dimensionalspacethatlocallyresembles
Rn. At each point p on the manifold, there is an associated tangent space T , which is a linear
p
approximation of the manifold at p. The metric tensor g is a smoothly varying positive-definite
bilinearformonthetangentspace. Itdefinesaninnerproductinthetangentspace,allowingusto
measurelengthsandangles.
(cid:112)
Foratangentvectorv ∈ T ,thenormisdefinedas∥v∥ = g(v,v),providinganotionoflength
p
(cid:82) (cid:112)
inthetangentspace. Thelengthofasmoothcurveγ(t)onthemanifoldisgivenby g(γ˙,γ˙)dt,
whereγ˙ isthederivativeofγ(t)withrespecttot. Thisintegralmeasuresthetotallengthofthecurve
bysumminguptheinfinitesimallengthsalongthecurve.
The distance between two points on the manifold is defined as the minimal length of a curve
connectingthepoints. Thecurvesthatminimizethislengthareknownasgeodesics,whicharethe
generalizationofstraightlinesinEuclideanspacetocurvedspaces.
3.2 Fermatdistances
Density-baseddistances(DBDs)provideawaytodefinedistancesbetweenpointsinadistribution
byutilizingtheunderlyingprobabilitydensityfunction. Thisapproachcanbeparticularlyusefulfor
taskssuchasclusteringandpathfindingindataanalysisandmachinelearning. Ifdesignedwell,a
DBDcancapturedesirablepropertiessuchas:
1. Shortestpathsbetweenpointsshouldpassthroughsupportedregionsofthedata. Ifthereis
aregionwithnodatabetweentwopoints,theshortestpathshoulddeviatearoundittopass
throughregionswithmoredata.
2. Datashouldclusteraccordingtoitsmodes. Iftwodatapointsbelongtodifferentmodes
withlittledataconnectingthem,thedistancebetweenthemshouldbehigh.
OnewaytodefineaDBDthatsatisfiesthesepropertiesisthroughFermatdistances. Namedafter
Fermat’sprincipleofleasttimeinoptics,thesedistancesmodeltheshortestpathsbetweenpoints
byconsideringtheinverseprobabilitydensity(oramonotonicfunctionofit)asanalogoustothe
refractiveindex.
InthelanguageofRiemanniangeometry,wedefineametrictensoroftheform
⟨u,v⟩
g(u,v)= , (1)
f(p)2
wherepistheprobabilitydensity,f isamonotonicfunction,and⟨·,·⟩istheEuclideaninnerproduct.
Thisformulationensuresthatregionswithhigherprobabilitydensityhavelower"refractiveindex,"
guiding shortest paths through denser regions of the data. In practice, it is convenient to choose
3Unscaled(β=1) Scaled(β=1/D)
1.0 D=2 1.0 D=2
D=100 D=100
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Figure 2: Geodesics of the standard normal distribution, projected into 2d, taken between two
√ √
orthogonalpointsatdistance Dfromtheorigin. Thepathsarescaledby Dtoenablecomparison.
(Left) Unscaled metric (same temperature for all dimensions) leads to sharply curved geodesics
in high dimensions. (Right) Scaled metric (temperature equal to dimension) leads to consistent
geodesicsinalldimensions(notethatthelinesareoverlapping).
f(p)=pβ,whereβ isaninversetemperatureparameterthatcontrolstheinfluenceofthedensityon
themetric. SinceourmetricenactsasimplescalingoftheEuclideaninnerproduct,itisaconformal
metric,whichlocallypreservesanglesandsimplifieskeygeometricobjectssuchasthegeodesic
equationandscalarcurvature.
Thelengthofasmoothcurveγ(t)isthereforegivenby
(cid:90) ∥γ˙(t)∥
L(γ)= dt (2)
p(γ(t))β
3.2.1 Fermatdistanceswhichscalewithdimension
If β = 1, this leads to unintuitive behavior in higher dimensions. Consider the standard normal
distributioninDdimensions. Duetorotationalsymmetry,thegeodesicconnectingx andx will
1 2
lieintheplanespannedbythesetwopointsandtheorigin. Withoutlossofgenerality,supposethis
planealignswiththefirsttwoaxisdirections. Thedensityrestrictedtothisplaneisthesameasinthe
two-dimensionalcase,uptoaconstantfactor. Therefore,geodesicsintheD-dimensionalstandard
normaldistributionarethesameasinthetwo-dimensionalstandardnormaldistribution,oncewe
accountfortherotationintotheplaneofx andx .
1 2
However,despitethissimilarity,thetypicaldistancefromtheoriginofpointssampledfromtheD-
√
dimensionalstandardnormaldistributionismuchlarger,withameanof D. Asaresult,geodesics
willstartandendinverylow-densityregionsrelativetothetwo-dimensionalstandardnormal. This
meansthatmostofthetrajectoryisspentmovingtowardsandthenoutwardsfromthehigh-density
regionneartheorigin.
Toaddressthisissue,ifwescalethetemperaturelinearlywithdimension(β = 1/D),theresults
becomeconsistentwiththetwo-dimensionalcase(seeFig.2).Scalingthetemperaturewithdimension
alsoincreasesnumericalstability. Thisisbecausetheeffectoftheprobabilitydensityisreduced,
leadingtomoreEuclidean-likegeodesics. ThiseffectisclearlyvisibleinFig.2,wherethegeodesics
aremuchclosertostraightlinesinthescaledcase.
3.3 Geodesicequation
Fromthemetric,wecancomputethegeodesicequation.Thisisthesecond-orderdifferentialequation
whichallpathsofminimumlength(geodesics)mustsatisfy.Inourcase,wehavetherelativelysimple
solutionforatrajectoryγ(t):
γ¨−2β(s(γ)·γ˙)γ˙ +βs(γ)∥γ˙∥2 =0 (3)
wheres(x)= ∂logp(x) isthescoreofthedistributionand∥·∥istheordinaryEuclideannorm.
∂x
4Algorithm1Onestepofrelaxation. φ={φ }n isthecurrentstateofthetrajectory,sisthescore
i i=0
function,β theinversetemperature,γ ∈[0,1)thesmoothingparameterandα∈(0,1]thestepsize.
Thisstepshouldbeiterateduntilconvergence,whichcanbemeasuredviathedifferencebetweenφ
andφ′.
functionRELAXATIONSTEP(φ,s,β,γ,α)
v ← 1(φ −φ ) ▷i=1,...,n−1
i 2 i+1 i−1
φ′ ← 1(φ +φ )+ β (cid:0) s(φ )∥v ∥2−(s(φ )·v )v (cid:1)
i 2 i+1 i−1 2 i i i i i
φ′ ←(1−γ)φ′ + γ(φ′ +φ′ )
i i 2 i+1 i−1
φ′ ←αφ′ +(1−α)φ
i i i
returnφ′
endfunction
Thegeodesicequationhasthepropertythatthespeedisconstantwithrespecttothemetric,meaning
(cid:112)
g(γ˙,γ˙) is constant. In other words, the geodesic distance traveled in a given time interval is
thesameatallpointsofthecurve. Ifγ passesfromahigh-probabilityregiontoalow-probability
one, the distances in the low-probability region could be orders of magnitude larger than in the
high-probabilityregion. Thismakesnumericallysolvingtheequationschallengingduetothevastly
differentEuclideanvelocitiesalongthetrajectory.
ItisconvenienttoreparameterizetheequationsuchthattheEuclideanvelocityisconstantinstead,as
thisismucheasiertohandlenumerically. Thisisespeciallyusefulforsolvingviarelaxation,where
havingequal-lengthsegmentsleadstomuchmorestablebehavior. Makingthischangeresultsina
verysimilarequation(removingonlythefactorof2inthesecondterm):
φ¨−β(s(φ)·φ˙)φ˙ +βs(φ)∥φ˙∥2 =0 (4)
SeeAppendixA.1forderivationsoftheseresults.
4 Methods
4.1 Groundtruthdistancesthroughrelaxation
Arelaxationschemeallowsonetosolveadifferentialequationconstrainedbyboundaryconditions
byiterativelyupdatingthesolutionsuchthatitbetterobeysthedifferentialequation. Wedividethe
interval[0,1]intoequaltimestepsofsizeh = 1/nandwriteφ = φ(ih). Initializethevaluesof
i
thecurveφatt=ihfori=0,...,n,withtheconstraintthatφ andφ arethechosenendpoints.
0 n
Theinitialcurvecouldbethestraightlineconnectingtheendpoints,oramoreinformativeguessif
additionalinformationisavailable.
Equation(4)leadstothefollowingrelaxationscheme. Define
φ −φ
v = i+1 i−1 (5)
i 2
thenupdatewith
φ′ = φ i+1+φ i−1 + β (cid:0) s(φ )∥v ∥2−(s(φ )·v )v (cid:1) (6)
i 2 2 i i i i i
SeeAppendixA.2foraderivation.
4.1.1 Implementation
In practice, some additional features are needed to ensure stable convergence of the relaxation
scheme, especially when the score function is not particularly smooth. Firstly, we found that
oscillatoryfeaturescommonlyarise. Toaddressthis,weusesmoothingaftertheupdate,returninga
valuewhichisinterpolatedbetweenthevalueatthenodeandtheaverageofitsneighbors. Secondly,
forrougherscorefunctionsweuseastepsizebetween0and1,where0correspondstonoupdateand
1correspondstotheordinaryrelaxationstep. SeeAlgorithm1.
54.2 WeightedgraphalgorithmbasedonEuclideandistance
Ifwehavealargenumberofsamplesfromp(x), wecanapproximatethedensitybetweenclose
neighborsx andx asconstantandhencetheshortestpathbetweenthemasastraightline. The
1 2
densityisroughlyproportionaltotheinverseoftheEuclideandistance,raisedtothepowerd,where
distheintrinsicdimensionofthedistribution[Bijraletal.,2012]:
1
p(γ)∝ (7)
∥x −x ∥d
1 2
Hencethedistancebetweenx andx accordingtothemetricisapproximatelyproportionaltoa
1 2
poweroftheEuclideandistance
dist(x ,x )∝∥x −x ∥βd+1 (8)
1 2 1 2
Bijraletal.[2012]thereforeproposestoapproximateshortestpathsbetweenmoredistantpointsby
constructingak-nearestneighborsgraphofthesampleswithedgeweightsgivenby∥x −x ∥βd+1
1 2
andapplyingDijkstra’salgorithm.
Hwangetal.[2016]givesconsistencyguaranteesforthisestimate,provingthatascaledversionofit
convergestothegroundtruthdistancewitharateofexp(−θ n1/(3d+2)),withnthesamplesize,d
0
theintrinsicdimension,andθ apositiveconstant. Groismanetal.[2022]additionallyshowthatthe
0
shortestpathsconvergetogeodesicsinalargesamplesizelimit.
Whilethisapproachisappealinginitssimplicity,canbeeasilyimplementedusingstandardlibraries,
andcomeswiththeoreticalguarantees,thedensityestimateistoopoortoprovideaccurateestimates
ofFermatdistancesinpractice. Intheexperimentalsection5.1.1weshowthatpathscomputedwith
thisapproachdonotconvergetothegroundtruthgeodesicsassamplesizeincreases,eveninvery
simple2-dimensionaldistributionssuchasthestandardnormal.
Weareunsurewhyweobservesuchalargedeviationfromthetheoreticalprediction. Itispossible
that the unknown constant θ is very small, leading to extremely slow convergence, despite the
0
exponentialfunction. Weleaveinvestigationintothispointtofuturework.
4.3 Normalizingflows
Normalizingflows[RezendeandMohamed,2015,Kobyzevetal.,2020,Papamakariosetal.,2021]
areasetoftechniquestolearnaprobabilitydensityviaaparameterizeddiffeomorphismf which
θ
mapsfromdatatolatentspace. Theprobabilitydensityisobtainedthroughthechangeofvariables
formula:
p θ(x)=π(f
θ(x))(cid:12) (cid:12)
(cid:12)
(cid:12)det(cid:18) ∂ ∂f xθ(cid:19)(cid:12) (cid:12)
(cid:12)
(cid:12)−1
(9)
whereπisasimplelatentdistribution. Acommonstrategytodesignnormalizingflowsisthrough
couplingblocks[Dinhetal.,2014,2016,Durkanetal.,2019],wheref isthecompositionofaseries
θ
ofblocks,eachwithatriangularJacobian. Thismakesthedeterminanteasytocalculate,andhence
theaboveformulaistractable.
Normalizingflowsaretrainedbyminimizingthenegativelog-likelihoodonatrainingset:
L=E [−logp (x)] (10)
pdata(x) θ
4.4 Density-weightedgraphalgorithm
Inordertobetterestimateprobabilitydensities,weuseanormalizingflowtrainedonthedataand
directlyusethedensitypredictedbythemodel.Inordertoapproximatethegeodesicdistancebetween
closeneighbors,weassumethatthegeodesicconnectingthemisapproximatelyastraightline,and
estimateitslengthbyadiscreteapproximation:
S
dist(x ,x
)≈(cid:88)∥y i−y i−1∥
(11)
1 2 p (y )
θ i−1/2
i=1
Herethey arepointswhichinterpolatex andx withy = x andy = x , andy isthe
i 1 2 0 1 S 2 i−1/2
midpointofy andy .
i−1 i
6Algorithm2Constructionofadensity-weightedgraphwhichcansubsequentlybeusedtocompute
shortest paths and distances between points. X is the data matrix, p the probability density, β
theinversetemperature,kthenumberofneighbors,andS thenumberofsegmentswithwhichto
approximatetheintegral. Thecomputationscanalsobeperformedinthelogspace,returninglog
edgeweights,whichismorenumericallystableinhigherdimensions.
functionDENSITYGRAPH(X,p,β,k,S)
G ←KNN(X,k) ▷Constructk-nearestneighborgraph
for(l,m)∈G do ▷Iterateoveredges
y ←(1− i)X + iX ▷Fori=0,...,S
i S l S m
y ← 1(y +y ) ▷Fori=1,...,S
i−1/2 2 i−1 i
G ←(cid:80)S ∥y −y ∥/p(y )β
lm i=1 i i−1 i−1/2
endfor
returnG
endfunction
Thisallowsustotakeak-nearestneighborsgraphofthedataandreplacetheedgeweightsbythe
approximatedistance. Thisresultsinasparsegraph,towhichwecanapplyDijkstra’salgorithmto
estimateshortestpathsanddistancesbetweenanytwopoints. SeeAlgorithm2.
4.5 Relaxationwithalearnedscoremodel
Thescorefunctionisthegradientofthelog-likelihoodwithrespecttospatialinputs,i.e.,s(x) =
∇ logp(x). Notethedifferencefromthedefinitioncommonlyusedinstatistic,wherethederivative
x
iswithrespecttomodelparameters.
Score matching [Hyvärinen, 2005] is a family of methods to estimate this function from data,
including variants such as sliced score matching [Song et al., 2020a], denoising score matching
[Vincent,2011]anddiffusionmodels[Songetal.,2020b].
Sliced score matching We can learn the score of p(x) by minimizing the following objective
(knownasthescorematchingobjective):
E [tr(∇ s (x))+ 1∥s (x)∥2] (12)
p(x) x θ 2 θ
Inhighdimensionsthetracetermisexpensivetoevaluateexactly,soslicedscorematchingusesa
stochasticapproximation(theHutchinsontraceestimator[Hutchinson,1989]):
E [vT∇ s (x)v+ 1∥s (x)∥2] (13)
p(x)p(v) x θ 2 θ
wherevissampledfromanappropriatedistribution,typicallystandardnormal.
SinceonlythescoreisneededfortherelaxationalgorithmdescribedinSection4.1,wecanusea
trainedscoremodeltoperformrelaxationtoapproximategeodesics. Inpractice,weinitializethe
pathfromagraph-basedmethodtospeedupconvergenceandavoidlocalminima.
5 Experiments
Performancemetric Aftersolvingforashortestpathφbetweentwopointsx andx ,wecompute
1 2
itslengthL(φ)byusingthegroundtruthdensityfunction,thencomparethisvaluetotheground
truthdistancebetweenthetwopoints:
L(φ)
M(φ)=log (14)
dist(x ,x )
1 2
Provided we have access to the ground truth density, this performance metric can be calculated
independentlyofhowφisobtained,andhasavalueofzeroonlyifφisthegroundtruthgeodesic.
Themetricwillotherwisetakepositivevalues.
7Normal GMM
0.5
0.4
TrueDensity 0.3
NormalizingFlow
Power
0.2
0.1
0.0
0 50000 100000 150000 200000 0 50000 100000 150000 200000
Samples Samples
Circle Spiral 2Spirals
0.5
0.4
0.3
0.2
0.1
0.0
0 50000 100000 150000 200000 0 50000 100000 150000 200000 0 50000 100000 150000 200000
Samples Samples Samples
Figure3: Thelengthofshortestpathsinpower-weighteddistancegraphsdonotconvergetothe
ground truth with increasing sample size (green). In contrast, learned and ground-truth density
weightedgraphs(orangeandgreen)doconverge,atsimilarrates.
5.1 Scalingthedataset
Inthissectionweperformexperimentsonasetof5two-dimensionaldatasetsforwhichweknow
the ground truth density (see Fig. 3). Details on how the datasets were created can be found in
AppendixB.
5.1.1 Power-weightedgraphpathsdonotconvergewithincreasingsamplesize
Wefirstestimatepathsusingthepower-weightedgraphmethod[Bijraletal.,2012],appliedtoa
k-nearestneighborgraph. Wefindthatinall5datasets,increasingthesamplesizeleadstosaturating
performance(seeFig.3,greenlines). InFig.6intheappendixweshowthatincreasingthenumber
ofneighborsdoesnotimproveperformance.
5.1.2 Poorconvergenceisduetopoordensityestimation
Inordertoinvestigatetherootcauseofthepoorconvergence, wedesignedothergraphmethods
whichusethenearestneighborestimatorforthedensity,butdonotusepower-weightedgraphedges.
Weuse4variantsforthedensityestimate(seeAppendixC.1). Wefindthatall4haveverysimilar
performancetothepower-weightedgraph,whereastheequivalentestimatorsusingthegroundtruth
densityquicklyconverge. SeeAppendixC.1forplotssupportingthisclaim.
5.1.3 Learningthedensityleadstoconvergentpaths
Since the problem lies in poor density estimation, we hope to fix the problem by better density
estimation. To this end, we use a normalizing flow to learn the density from samples, then use
Algorithm 2 to construct a graph over which we compute shortest paths. We find that the paths
obtained converge at close to the same rate as paths obtained by an equivalent method using the
ground truth densities (Fig. 3 orange and blue lines). Full experimental details can be found in
AppendixB.
8
rorrEgoLnaeM
rorrEgoLnaeMUnscaled Scaled
1.50
1.25
1.00 TrueDensity
NormalizingFlow
0.75 Power
ScoreModelRelaxation
0.50
0.25
0.00
5 10 15 20 25 5 10 15 20 25
Dimension Dimension
Figure4: Meanlogerrorwithasamplesizeof200,000. Graph-basedpaths(thefirstthreemethods)
converge very slowly in higher dimensions with increasingly worse performance as dimension
increases. Whenusingthescaledmetric(β =1/D),convergenceimprovesovertheunscaledmetric,
butallgraph-basedmethodsperformsimilarlyinhigherdimensions. Doingrelaxationwithalearned
scorefunction(redline)leadstomuchfasterconvergence,avoidingthecurseofdimensionality.
5.2 Scalingthedimension
Machine learning datasets often lie in high-dimensional spaces, so it is crucial that any method
appliedtothemscaleswell,avoidingthecurseofdimensionalityasmuchaspossible. Wetestthis
bytakingoneofourtwo-dimensionaldatasets,namelythestandardnormal,andscalingitupto25
dimensions. Weperformexperimentsbothforthescaledandunscaledversionofthemetric.
5.2.1 Graph-basedpathsdonotconvergewellinhigherdimensions
Our first finding is that all graph-based methods show declining performance as the dimension
increases,forbothunscaledandscaledmetrics(Fig.4). Thiscanbestraightforwardlyexplained
bythecurseofdimensionality: withincreasingdimensionbutfixedsamplesize,itisincreasingly
unlikely that points in the dataset lie close to the true trajectory. The shortest path in the graph
becomesincreasinglyroughincontrasttothesmoothgroundtruthtrajectory.
5.2.2 Relaxationusingalearnedscorefunctionimprovesconvergencerates
Inordertoobtainsmootherpaths,weturntothesamerelaxationalgorithmweusetosolveforground
truthgeodesics. Insteadofusingthegroundtruthscorefunction,weusealearnedscorefunction. An
obviouschoicewouldbetouseautomaticdifferentiationtogetthescorefromatrainednormalizing
flow,butwefindtheresultingscoresaretoonoisytobeuseful,inhibitingreliableconvergenceof
therelaxationscheme. Instead,welearnascoremodelwithslicedscorematching,whichresultsin
asmootherfunctionsuitableforperformingrelaxation. Fullexperimentaldetailscanbefoundin
AppendixB.
Wefindthatthisfixestheconvergenceproblems,bothfortheunscaledandscaledmetrics,though
convergenceismorereliableinthescaledcase(Fig.4). PleaseseeFig.8andFig.9intheappendix
formoredetailedevidence.
6 Conclusion
Inconclusion,ourstudyfocusesonunderstandingtheconvergencepropertiesofvariousalgorithms
to ground-truth geodesics by employing toy distributions with known densities. We investigate
thestandardnormaldistributionindimensionshigherthan2,highlightingthechallengesfacedby
graph-basedmethodseveninthissimplescenario.Ourfindingsdemonstratesignificantimprovements
throughtheuseofnormalizingflowsandscoremodels,yetfurtherresearchisneededtoaddressmore
9
rorrEgoLnaeMcomplexdistributionsandreal-worlddatawheretheunderlyingdensityisunknown. Futurework
shouldaimtounifytherolesofnormalizingflowsandscoremodelsintoasinglecohesivemodel,
enhancingthemethod’sapplicabilityandefficiency. Additionally,adeepertheoreticalanalysisis
required to understand why our proposed methods converge effectively, while previous methods
withconsistencyguaranteesdonot. Thisworklaysthegroundworkforpracticalapplicationsof
density-baseddistances,settingthestageformorerobustandscalablesolutionsinmetriclearning.
References
MortezaAlamgirandUlrikeVonLuxburg. Shortestpathdistanceinrandomk-nearestneighbor
graphs. arXivpreprintarXiv:1206.6381,2012.
LyntonArdizzone,TillBungert,FelixDraxler,UllrichKöthe,JakobKruse,RobertSchmier,and
Peter Sorrenson. Framework for Easily Invertible Architectures (FrEIA), 2018-2022. URL
https://github.com/vislearn/FrEIA.
AvleenSBijral,NathanRatliff,andNathanSrebro. Semi-supervisedlearningwithdensitybased
distances. arXivpreprintarXiv:1202.3702,2012.
OlivierBousquet,OlivierChapelle,andMatthiasHein. Measurebasedregularization. Advancesin
NeuralInformationProcessingSystems,16,2003.
TimothyChu,GaryLMiller,andDonaldRSheehy. Exactcomputationofamanifoldmetric,via
Lipschitzembeddingsandshortestpathsonagraph. InProceedingsoftheFourteenthAnnual
ACM-SIAMSymposiumonDiscreteAlgorithms,pages411–425.SIAM,2020.
Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components
estimation. arXivpreprintarXiv:1410.8516,2014.
LaurentDinh,JaschaSohl-Dickstein,andSamyBengio. DensityestimationusingRealNVP. arXiv
preprintarXiv:1605.08803,2016.
ConorDurkan,ArturBekasov,IainMurray,andGeorgePapamakarios.Neuralsplineflows.Advances
inneuralinformationprocessingsystems,32,2019.
PabloGroisman,MatthieuJonckheere,andFacundoSapienza. NonhomogeneousEuclideanfirst-
passagepercolationanddistancelearning. Bernoulli,28(1):255–276,2022.
CharlesRHarris,KJarrodMillman,StéfanJVanDerWalt,RalfGommers,PauliVirtanen,David
Cournapeau,EricWieser,JulianTaylor,SebastianBerg,NathanielJSmith,etal. Arrayprogram-
mingwithNumPy. Nature,585(7825):357–362,2020.
JohnDHunter. Matplotlib: A2dgraphicsenvironment. Computinginscience&engineering,9(03):
90–95,2007.
Michael F Hutchinson. A stochastic estimator of the trace of the influence matrix for Laplacian
smoothingsplines. CommunicationsinStatistics-SimulationandComputation,18(3):1059–1076,
1989.
SungJinHwang,StevenBDamelin,andAlfredOHeroIII. Shortestpaththroughrandompoints.
2016.
AapoHyvärinen. Estimationofnon-normalizedstatisticalmodelsbyscorematching. Journalof
MachineLearningResearch,6(4),2005.
IvanKobyzev,SimonJDPrince,andMarcusABrubaker. Normalizingflows: Anintroductionand
reviewofcurrentmethods. IEEEtransactionsonpatternanalysisandmachineintelligence,43
(11):3964–3979,2020.
AnnaLittle,MauroMaggioni,andJamesMMurphy. Path-basedspectralclustering: Guarantees,
robustness tooutliers, andfast algorithms. Journal ofmachine learningresearch, 21(6):1–66,
2020.
10AnnaLittle,DanielMcKenzie,andJamesMMurphy.Balancinggeometryanddensity:Pathdistances
onhigh-dimensionaldata. SIAMJournalonMathematicsofDataScience,4(1):72–99,2022.
DanielMckenzieandStevenDamelin. PowerweightedshortestpathsforclusteringEuclideandata.
arXivpreprintarXiv:1905.13345,2019.
AmitMoscovich,ArielJaffe,andNadlerBoaz. Minimax-optimalsemi-supervisedregressionon
unknownmanifolds. InArtificialIntelligenceandStatistics,pages933–942.PMLR,2017.
George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji
Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. Journal of
MachineLearningResearch,22(57):1–64,2021.
AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performancedeeplearninglibrary. Advancesinneuralinformationprocessingsystems,32,
2019.
FabianPedregosa,GaëlVaroquaux,AlexandreGramfort,VincentMichel,BertrandThirion,Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn:
MachinelearninginPython. theJournalofmachineLearningresearch,12:2825–2830,2011.
DaniloRezendeandShakirMohamed. Variationalinferencewithnormalizingflows. InInternational
conferenceonmachinelearning,pages1530–1538.PMLR,2015.
SajamaandAlonOrlitsky. Estimatingandcomputingdensitybaseddistancemetrics. InProceedings
ofthe22ndinternationalconferenceonMachinelearning,pages760–767,2005.
YangSong,SahajGarg,JiaxinShi,andStefanoErmon. Slicedscorematching: Ascalableapproach
todensityandscoreestimation. InUncertaintyinArtificialIntelligence,pages574–584.PMLR,
2020a.
YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,andBen
Poole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. arXivpreprint
arXiv:2011.13456,2020b.
NicolásGarcíaTrillos,AnnaLittle,DanielMcKenzie,andJamesMMurphy.Fermatdistances:Metric
approximation,spectralconvergence,andclusteringalgorithms. arXivpreprintarXiv:2307.05750,
2023.
PascalVincent. Aconnectionbetweenscorematchinganddenoisingautoencoders. Neuralcomputa-
tion,23(7):1661–1674,2011.
PauliVirtanen,RalfGommers,TravisEOliphant,MattHaberland,TylerReddy,DavidCournapeau,
EvgeniBurovski,PearuPeterson,WarrenWeckesser,JonathanBright,etal.SciPy1.0:fundamental
algorithmsforscientificcomputinginPython. Naturemethods,17(3):261–272,2020.
A Derivations
A.1 Geodesicequation
Thegeodesicequationwhichcharacterizespathsofzeroaccelerationγ is
γ¨i+Γi γ˙jγ˙k =0 (15)
jk
whereΓi aretheChristoffelsymbolsofthesecondkind
jk
(cid:18) (cid:19)
1 ∂g ∂g ∂g
Γi = gil lj + lk − jk (16)
jk 2 ∂xk ∂xj ∂xl
andgil istheinverseofg,i.e. suchthatgilg =δi.
lj j
11Ourmetrichastheformg =λ2δ ,meaningthat
ij ij
1 (cid:18) ∂λ2 ∂λ2 ∂λ2 (cid:19)
Γi = δil δ + δ − δ (17)
jk 2λ2 ∂xk lj ∂xj lk ∂xl jk
(cid:18) (cid:19)
∂logλ ∂logλ ∂logλ
=δil δ + δ − δ (18)
∂xk lj ∂xj lk ∂xl jk
∂logλ ∂logλ ∂logλ
= δi + δi − δilδ (19)
∂xk j ∂xj k ∂xl jk
Inthiscaseλ=p−β andhence ∂logλ =−β∂logp =−βs,wheres= ∂logp isthescorefunction
∂x ∂x ∂x
oftheprobabilitydensity. Asaresult
Γi =−βs δi −βs δi +βs δilδ (20)
jk k j j k i jk
Substitutingintothegeodesicequationleadsto
γ¨i−βs γ˙iγ˙k−βs γ˙jγ˙i+βs δilδ γ˙jγ˙j =0 (21)
k j l jk
whichcanbewritteninvectornotationas
γ¨−2β(s(γ)·γ˙)γ˙ +βs(γ)∥γ˙∥2 =0 (22)
Let f : [0,1] → [0,1] be a strictly increasing diffeomorphism. Given a curve γ : [0,1] → Rn,
considerthecurveφ:[0,1]→Rndefinedsuchthatφ(u)=γ(f−1(u))orequivalentlyφ(f(t))=
γ(t). Thederivativesofthetwocurvesarerelatedby
φ˙f˙=γ˙ (23)
and
φ¨f˙2+φ˙f¨=γ¨ (24)
where derivatives of φ are with respect to u = f(t). By substituting these expressions into the
geodesicequation,wearriveattheequivalentequationforφ:
φ¨f˙2+φ˙f¨−2β(s(γ)·φ˙f˙)φ˙f˙+βs(γ)∥φ˙f˙∥2 =0 (25)
WewouldlikeφtohaveaconstantEuclideanspeed(themagnitudebutnotnecessarilythedirection
ofthevelocityisconstant),meaning
(cid:18) (cid:19)
d 1
∥φ˙∥2 =φ˙ ·φ¨=0 (26)
du 2
Multiplyingthegeodesicequationbyφ˙,thisisequivalenttorequiring
∥φ˙∥2f¨−2β(s(γ)·φ˙)∥φ˙∥2f˙2+β(s(γ)·φ˙)∥φ˙∥2f˙2 =0 (27)
implyingthat
f¨=β(s(γ)·φ˙)f˙2 (28)
Resubstitutingintothegeodesicequationanddividingbyf˙2leadstotheresult:
φ¨−β(s(φ)·φ˙)φ˙ +βs(φ)∥φ˙∥2 =0 (29)
A.2 Relaxationscheme
Thecentralfinitedifferenceapproximationsforthefirstandsecondderivativesare
φ −φ
φ˙ ≈ i+1 i−1 (30)
i 2h
and
φ −2φ +φ
φ¨ ≈ i+1 i i−1 (31)
i h2
Bysubstitutingthisintothedifferentialequationwehave
φ −2φ +φ
i+1 i i−1 −β(s(φ )·φ˙ )φ˙ +βs(φ )∥φ˙ ∥2 ≈0 (32)
h2 i i i i i
12Figure5: The5two-dimensionaldatasets.
whereφ˙ isestimatedusingfinitedifferences. Wecanrearrangethisto
i
φ ≈ φ i+1+φ i−1 + h2β (cid:0) s(φ )∥φ˙ ∥2−(s(φ )·φ˙ )φ˙ (cid:1) (33)
i 2 2 i i i i i
andusetheequationasanupdaterule,updatingeachpositionofthecurveexcepttheendpointsat
eachiteration.
Dividingbyverysmallhcouldleadtonumericalinstability. Wecanavoiddividingandmultiplying
byhbythefollowingupdaterule. Firstdefine
φ −φ
v = i+1 i−1 (34)
i 2
thenupdatewith
φ = φ i+1+φ i−1 + β (cid:0) s(φ )∥v ∥2−(s(φ )·v )v (cid:1) (35)
i 2 2 i i i i i
B ExperimentalDetails
Software libraries We used the numpy [Harris et al., 2020] and scipy [Virtanen et al., 2020]
Python libraries for basic numerical and graph operations and matplotlib [Hunter, 2007] for
plotting. We used scikit-learn [Pedregosa et al., 2011] to compute nearest neighbor graphs
andpytorch[Paszkeetal.,2019]fortrainingneuralnetworks. WeusedFrEIA[Ardizzoneetal.,
2018-2022]forconstructingnormalizingflows.
Two-dimensionaldatasets Weusethefollowingtwo-dimensionaldatasets. Allareimplemented
asGaussianmixturemodels. ThefirsttwoareexplicitlyGaussian,thefinalthreeareGMMswith
50componentsfittedusingscikit-learntodatageneratedbyaddingnoisetocertaingeometric
structures.
• Standardnormal
13• Gaussianmixturewith3components
• Circle
• 1spiral
• 2spirals
Normalizingflowtraining WeusedtheFrEIAlibrary[Ardizzoneetal.,2018-2022]todesign
normalizing flows using spline coupling blocks [Durkan et al., 2019]. We used the following
hyperparameters:
• Blocks: 12fortwo-dimensionaldata,5forstandardnormalinDdimensions
• Splinebins: 10
• Domainclampinginthesplines: 5
• ActNormbetweenblocks
• Noiseof10−5addedtothetrainingdata
• LR=5×10−4
• Weightdecay=10−6
• Fully-connectedcouplingblocksubnetswith3hiddenlayersof64dimensionseach,with
ReLUactivationandBatchNorm
• Trainingiterations(asafunctionoftrainingsetsizen): 2500×2n//1000
• Batchsize=256ifn>400,else⌊n/3⌋
• Adamoptimizer
Scoremodeltraining Weusefullyconnectednetworkstopredictthescore,with6hiddenslayer
withthewidthdependingonthedimensionoftheinput. Weusethefollowinghyperparameters:
• Hiddenlayerwidth(asafunctionofinputdimensionD)
– 3≤D ≤5: 128
– 6≤D ≤8: 170
– 9≤D ≤25: 200
• LR=10−3
• Weightdecay=10−6
• Softplusactivationfunction
• Trainingiterations(asafunctionoftrainingsetsizen): 2500×2n//1000
• Batchsize=256ifn>400,else⌊n/3⌋
• Adamoptimizer
Inallcases,thechosenmodelcheckpointwasthatwhichachievedthelowesttrainingloss.
Computeresources Forthetwo-dimensionaldatasets,weperformedallexperimentsonamachine
withaNVIDIAGeForceRTX2070GPUand32GBofRAM.Ittookapproximately24hourstorun
allexperiments.
Forthehigherdimensionalstandardnormaldistributions,weperformedallexperimentsonamachine
withtwoGPUs: aNVIDIAGeForceRTX2070andaNVIDIAGeForceRTX2080TiRev. A.The
machinehas32GBofRAM.Ittookapproximately24hourstorunallexperiments.
Inadditionwedidsomepreliminaryexperimentsonthefirstmachine,totallingapproximately48
hoursofcomputetime.
14Normal GMM
0.5
k=20 0.4
k=50
k=100
0.3
TrueDensity
NormalizingFlow
Power 0.2
0.1
0.0
0 50000 100000 150000 200000 0 50000 100000 150000 200000
Samples Samples
Circle Spiral 2Spirals
0.5
0.4
0.3
0.2
0.1
0.0
0 50000 100000 150000 200000 0 50000 100000 150000 200000 0 50000 100000 150000 200000
Samples Samples Samples
Figure6: Increasingthenumberofnearestneighborsdoesnotsignificantlyimprovetheconvergence
ofthepower-weightedgraphmethodontwo-dimensionaldatasets.
C Additionalexperimentalresults
C.1 Convergenceofalternativegraphedgeweightings
Wetryoutotheredgeweights,asdescribedinthemaintext. Giventhenearest-neighbordensity
estimatespˆ(X )∝(min ∥X −X ∥)−d fordatapointsX ,weapproximatethegraphedgesby
l m l m l
G = ∥X −X ∥/p˜((X +X )/2) where p˜is a density estimate for the midpoint. We use 4
lm l m l m
variants:
1. Inverseofmean: p˜((X +X )/2)=(pˆ(X )+pˆ(X ))/2
l m l m
2. Meanofinverse: 1/p˜((X +X )/2)=(1/pˆ(X )+1/pˆ(X ))/2
l m l m
3. Max: p˜((X +X )/2)=max(pˆ(X ),pˆ(X ))
l m l m
4. Min: p˜((X +X )/2)=min(pˆ(X ),pˆ(X ))
l m l m
Wefindthatall4haveverysimilarperformancetothepower-weightedgraph,whereastheequivalent
estimatorsusingthegroundtruthdensityquicklyconverge. SeeFig.7.
15
rorrEgoLnaeM
rorrEgoLnaeM0.6
0.6
TrueDensity TrueDensity
InverseofMean(Estimated) 0.5 MeanofInverse(Estimated)
0.5
InverseofMean(True) MeanofInverse(True)
0.4 0.4
0.3 0.3
0.2 0.2
0.1 0.1
0.0 0.0
102 103 104 105 102 103 104 105
Samples Samples
TrueDensity TrueDensity
0.6 Max(Estimated) 0.5 Min(Estimated)
Max(True) Min(True)
0.5 0.4
0.4
0.3
0.3
0.2
0.2
0.1
0.1
0.0 0.0
102 103 104 105 102 103 104 105
Samples Samples
Figure7:Alternativeedgeweightingsbasedonthenearestneighbordensityestimatordonotconverge.
Thesameweightingsbasedonthegroundtruthdensitydoconverge. Experimentsperformendonthe
GMMtwo-dimensionaldataset.
16
rorrEgoLnaeM
rorrEgoLnaeM
rorrEgoLnaeM
rorrEgoLnaeMTrueDensity UnscaledMetric
NormalizingFlow
Power
ScoreModelRelaxation
Dimension3 Dimension4 Dimension5
2.0
1.5
1.0
0.5
0.0
0 50000 100000 150000 200000 0 50000 100000 150000 200000 0 50000 100000 150000 200000
Samples Samples Samples
Dimension6 Dimension7 Dimension8
2.0
1.5
1.0
0.5
0.0
0 50000 100000 150000 200000 0 50000 100000 150000 200000 0 50000 100000 150000 200000
Samples Samples Samples
Dimension15 Dimension20 Dimension25
2.0
1.5
1.0
0.5
0.0
0 50000 100000 150000 200000 0 50000 100000 150000 200000 0 50000 100000 150000 200000
Samples Samples Samples
Figure8: Detailedplotofconvergencebehaviorinhigherdimensions,unscaledmetric. Weseethat
allgraph-basedmethodsplateauinperformance,whereastherelaxation-basedmethodcontinuesto
showgoodconvergence.
17
rorrEgoLnaeM
rorrEgoLnaeM
rorrEgoLnaeMTrueDensity ScaledMetric
NormalizingFlow
Power
ScoreModelRelaxation
Dimension3 Dimension4 Dimension5
1.0
0.8
0.6
0.4
0.2
0.0
0 50000 100000 150000 200000 0 50000 100000 150000 200000 0 50000 100000 150000 200000
Samples Samples Samples
Dimension6 Dimension7 Dimension8
1.0
0.8
0.6
0.4
0.2
0.0
0 50000 100000 150000 200000 0 50000 100000 150000 200000 0 50000 100000 150000 200000
Samples Samples Samples
Dimension15 Dimension20 Dimension25
1.0
0.8
0.6
0.4
0.2
0.0
0 50000 100000 150000 200000 0 50000 100000 150000 200000 0 50000 100000 150000 200000
Samples Samples Samples
Figure9: Detailedplotofconvergencebehaviorinhigherdimensions,scaledmetric. Weseethat
allgraph-basedmethodsplateauinperformance,whereastherelaxation-basedmethodcontinuesto
showgoodconvergence.
18
rorrEgoLnaeM
rorrEgoLnaeM
rorrEgoLnaeM