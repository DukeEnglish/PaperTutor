1
Beyond Euclid:
An Illustrated Guide to Modern Machine Learning
with Geometric, Topological, and Algebraic Structures
Sophia Sanborn∗1 Johan Mathe∗2 Mathilde Papillon∗1 Domas Buracas3 Hansen J Lillemark3
Christian Shewmake3,5 Abby Bertics1 Xavier Pennec4 Nina Miolane1,2,3
1University of California, Santa Barbara 2Atmo, Inc. 3New Theory AI 4Universite´ Coˆte d’Azur & Inria
5University of California, Berkeley
Abstract—The enduring legacy of Euclidean geometry cal patterns of neurons in natural and artificial neural
underpinsclassicalmachinelearning,which,fordecades, networks.
hasbeenprimarilydevelopedfordatalyinginEuclidean
space. Yet, modern machine learning increasingly en-
This non-Euclidean revolution was part of a greater
counters richly structured data that is inherently non-
Euclidean. This data can exhibit intricate geometric, trend towards generalization and abstraction in 19th
topological and algebraic structure: from the geometry and 20th century mathematics. In addition to expand-
of the curvature of space-time, to topologically complex ing the realm of geometry, mathematicians proceeded
interactions between neurons in the brain, to the alge-
to define more abstract notions of space, freed from
braic transformations describing symmetries of physical
rigid geometric concepts like distances and angles.
systems. Extracting knowledge from such non-Euclidean
data necessitates a broader mathematical perspective. Thisgaverisetothefieldoftopology,whichexamines
Echoing the 19th-century revolutions that gave rise to the properties of a space that are preserved under con-
non-Euclidean geometry, an emerging line of research is tinuoustransformationssuchasstretchingandbending.
redefining modern machine learning with non-Euclidean By abstracting away from the rigidity of geometric
structures. Its goal: generalizing classical methods to
structures, topology emphasizes more general spatial
unconventional data types with geometry, topology, and
algebra.Inthisreview,weprovideanaccessiblegateway properties such as continuity and connectedness. In-
to this fast-growing field and propose a graphical tax- deed, two structures that look very different from a
onomy that integrates recent advances into an intuitive geometricperspectivemaybeconsideredtopologically
unifiedframework.Wesubsequentlyextractinsightsinto
equivalent. The famous of example of this is a donut
current challenges and highlight exciting opportunities
and coffee mug, which are topologically equivalent
for future development in this field.
sinceonecanbecontinuouslydeformedintotheother.
This notion of abstract equivalence was supported by
Index Terms—Geometric Deep Learning, Geometry,
Topology, Algebra, Machine Learning the simultaneous development of the field of abstract
algebra, which examines the symmetries of an ob-
ject—the transformations that leave its fundamental
I. INTRODUCTION
structureunchanged.Thesemathematicalideasquickly
For nearly two millennia, Euclid’s Elements of Ge- found applications in the natural sciences, and revolu-
ometry formed the backbone of our understanding tionized how we model the world.
of space and shape. This ‘Euclidean’ view of ge-
ometry—characterized by flat planes and straight A similar revolution is now unfolding in machine
lines—remained unquestioned until the 19th century. learning. In the last two decades, a burgeoning body
Only then, did mathematicians venture “beyond” to of research has expanded the horizons of machine
develop the principles of non-Euclidean geometry on learning, moving beyond the flat, Euclidean spaces
curved spaces. Their pioneering work revealed that traditionally used in data analysis to embrace the
there is no singular, true geometry. Instead, Euclidean rich variety of structures offered by non-Euclidean
geometry is but one in a mathematical universe of geometry, topology, and abstract algebra. This move-
geometries, each of which can be used to illuminate ment includes the generalization of classical statistical
different structures in nature—from the mechanics of theory and machine learning in the field of Geometric
celestial bodies embracing the curvature of spacetime Statistics(Guiguietal.,2023,Pennec,2006)aswellas
to the topologically and algebraically complex electri- deeplearningmodelsinthefieldsofGeometric,(Bron-
4202
luJ
21
]GL.sc[
1v86490.7042:viXra2
stein et al., 2021), Topological (Bodnar, 2023, Hajij Topology,geometryandalgebraarebranchesofmath-
et al., 2023a), and Equivariant (Cohen et al., 2021) ematics that study the properties of abstract spaces.
Deep Learning. In the 20th century, non-Euclidean In machine learning, data can possess explicit spatial
geometry radically transformed how we model the structure—such as an image of a brain scan, or a
world with pen and paper. In the 21st century, it is rendering of a protein surface. Even when data is not
poised to revolutionize how we model the world with overtly spatial, a dataset can be naturally conceptual-
machines. izedasasetofsamplesdrawnfromanabstractsurface
embeddedinahigh-dimensionalspace.Understanding
This review article provides an accessible introduction
the “shape” of data—that is, the shape of the space to
to the core concepts underlying this movement. We
which this data belongs—can give important insights
organizethemodelsinthisbodyofliteratureintoaco-
into the patterns of relationships that give data its
herenttaxonomydefinedbythemathematicalstructure
meaning.
of both the data and the machine learning model. In
so doing, we clarify distinctions between approaches Topology, geometry and algebra each provide a differ-
and we highlight challenges and high-potential areas ent lens and set of tools for studying the properties
of research that are as-of-yet unexplored. We begin of data spaces and their “shapes,” summarized in
by introducing the essential mathematical background the textbox below. Topology lends the most abstract,
in Section II, and turn to an analysis of mathematical flexible perspective and considers spaces as stretchy
structure in data in Section III before introducing structures that can be continuously deformed so long
our ontology of machine learning and deep learning as connectivity and continuity are preserved. Topology
methods in Section IV and V. We explore the as- thus studies the relationships between points. Geom-
sociated landscape of open-source software libraries etry allows us to quantify familiar properties such as
in Section VI, and delve into the movement’s key distancesandangles,inotherwords:performmeasure-
application domains in Section VII. Accordingly, this ments on points. Algebra provides the tools to study
reviewarticlerevealshowthisnewframeworkforma- the symmetries of an object—the transformations that
chine learning—born from the elegant mathematics of can be applied while leaving its fundamental structure
geometry, topology, and algebra—has been developed, invariant.
implemented, and adapted to propose transformative
solutions to real-world challenges.
A. Topology: Relationships
A topological space is a set of points equipped with
a structure known as a topology that establishes which
Topological Properties
pointsinthesetare“close”toeachother.Thetopology
• Connectedness • Continuity gives spatial structure to an otherwise unstructured
→ Relationships set. Formally, a topology is defined as a collection of
open sets. An open set is a collection of points in a
Geometric Properties
spatial region that excludes points on the boundary.
• Distance • Angle
By grouping points into open sets, we can discuss
→ Measurements
‘neighborhoods’ of points and ‘paths’ between points,
Algebraic Properties giving us a way to formalize concepts like continuity
• Symmetry • Invariance (can you travel from one location to another without
→ Transformations teleporting?) and connectedness (are two locations in
the same neighborhood or region?).
Given the generality of topological structures, topo-
logical spaces can be quite exotic. Within this paper,
II. ELEMENTSOFNON-EUCLIDEANGEOMETRY
we do not consider continuous topological spaces, and
Wefirstprovideanaccessibleandconciseintroduction we explicitly restrict the term topology to discrete
totheessentialmathematicalconcepts.Forreadability, topological structures, such as graphs, cellular com-
we define the concepts primarily linguistically here, plexes, and hypergraphs. We consider these spaces to
and refer the reader to the works Bronstein et al. be a generalization of the discretized Euclidean space.
(2021), Cohen et al. (2021), Guigui et al. (2023), Indeed,Euclideanspacediscretizesintoaregulargrid,
Hajij et al. (2023a) for their precise mathematical whilegraphs,cellularcomplexesandhypergraphsrelax
definitions. the regularity assumption and allow points to be con-3
nected with more complex relationships as illustrated them simpler to model than more general, arbitrary
in Figure manifolds.
C. Algebra: Transformations
B. Geometry: Measurements
A group is a set of transformations, such as 3D rota-
A manifold is a continuous space that locally “re-
tions,thatcanbecomposed.Groupscanbediscreteor
sembles” (is homeomorphic to) Euclidean space in
continuous.ALiegroupisacontinuousgroup,defined
the neighborhood of every point. In other words, it is
as a smooth manifold equipped with a compatible
locally linear, and it does not intersect itself. Though
group structure such that the composition of elements
locally it resembles flat space, its global shape may
on the manifold obeys the group axioms. An example
exhibit curvature. Without additional structure, the
ofaLiegroupisthesetofspecialorthogonalmatrices
manifold can be seen as a soft and elastic surface.
inR3×3 undermatrixmultiplication,whichdefinesthe
To give it more rigid, definite form, a manifold can
group of 3-dimensional rotations, SO(3). Each matrix
be equipped with a notion of distance between points,
is an element of the group and defines rotation at
that freezes how far each point is from each other,
a certain angle. Lie groups are extensively used in
and therefore freezes the overall shape of the mani-
physics where they describe symmetries in physical
fold.
systems.
There are two main approaches to defining a distance
Agroupoftransformations,suchasSO(3)—thegroup
on the manifold, which turn it into either (i) a metric
of3Drotations—mayactonanothermanifoldtotrans-
space, where an extrinsic distance may be chosen
formitselements.Agroupactionmapsanelementin
independently of the manifold’s properties, or into (ii)
a manifold to new location, determined by the group
a length space (also called path-metric spaces), where
element that transforms them. For example, a group
the distance between two points on the manifold is
action on a Euclidean space can translate, rotate and
defined intrinsically by the infimum of the length of
reflect its elements, as illustrated in Figure 3. In this
curves connecting the points. Within this paper, we
paper,weusethetermalgebratodenotethatweequip
focusonlengthspaces.Apowerfulwayofestablishing
a space with a group action.
a notion of distance for this approach is to consider
a Riemannian manifold, that is: a smooth manifold
III. STRUCTUREINDATA
equipped with a positive definite inner product on the
The mathematics of topology, geometry and algebra
tangentspaceateverypointonthemanifold.Thisfam-
provide a conceptual framework to categorize the na-
ily of inner products defines the Riemannian metric,
ture of data found in machine learning. We generally
which defines the lengths of tangent vectors at a given
encountertwotypesofdata:eitherdataascoordinates
point on the manifold and the angle between them.
in space—for example the coordinate of the position
Ultimately, the Riemannian metric induces a distance
of an object in a 2D space; or data as signals over a
alongacurveofthemanifoldthroughintegrationofits
space—for example, an image is a 3D (RGB) signal
tangent vector. A geodesic generalizes the Euclidean
defined over a 2D space. In each case, the space can
concept of a “straight line” to curved manifolds. A
either be a Euclidean space or it can be equipped with
Riemannian geodesic is a curve that traces the locally
topological,geometricandalgebraicstructuressuchas
shortest path between two points.
the ones introduced in Section II.
There exist different flavors of Riemannian manifolds,
Understanding the structure of this space provides
such as the spheres, hyperbolic spaces, and tori. We
essential insights into the nature of the data. These
consider these spaces to be generalizations of the
insights are, in turn, crucial for selecting the machine
continuous Euclidean space. Indeed, Euclidean spaces
learning model that will be most suitable to extract
are globally flat, while spheres, hyperbolic spaces and
knowledge from this data. In this section, we provide
tori can exhibit curvature —as illustrated in Figure 2.
agraphicaltaxonomy,showninFigure4,tocategorize
It is no coincidence that the manifolds most com-
the structures of data based on the mathematics of
monly studied in the literature—the Euclidean space,
topology, geometry and algebra.
the hypersphere, and the hyperbolic space—are the
three manifolds of constant curvature. The Euclidean
A. Data as Coordinates in Space
space has curvature 0, the sphere (and hyperspheres)
have curvature 1, while the hyperbolic spaces have We start by categorizing the mathematical structures
curvature -1. The uniformity of their curvature makes ofdata,whendatapointsarecoordinatesinspace—as4
6780+0$054
!"#$%&’() ,-(+. /’$$"$(- 34+’-5-(+. ,’)’-($
*+(#’ #01+$’2
Fig.1:BeyondEuclid:DiscreteTopologicalStructures.Left:Euclideanspacediscretizedintoaregulargrid.Right:Discretetopological
spaces that go beyond the classical discretized Euclidean space. Graphs, Cellular Complexes, Hypergraphs relax the assumption of the
regular grid and allow points to be connected with more complex relationships. The arrow +topology indicates the addition of a non-
Euclidean,discretetopologicalstructure.AdaptedfromPapillonetal.(2023).
,-.’/0’123
!"#$%&’() 63+’27/$%#
*+(#’ 4+5’2’ *+(#’ 9/2"* 8’)’2($
Fig. 2: Beyond Euclid: Continuous Geometric Structures. Left: Euclidean space. Right: Riemannian manifolds that go beyond the
classical Euclidean space. Spheres, hyperbolic spaces, and tori relax the assumption of flatness of the Euclidean space and can exhibit
positiveornegativecurvature.Thearrow+geometryindicatestheadditionofanon-Euclidean,continuousgeometricstructure.
21($4’5-(
!"#$%&’() ,-()*$(.%/)12
,-()*$(.%/) 0/.(.%/) 0’3’#.%/)
*+(#’ 0/.(.%/)
Fig. 3: Beyond Euclid: Algebraic Transformations. Left: Euclidean space. Right: Group transformations that act on the elements of
an Euclidean space: 2D translation from the group R2, 2D rotation from the group SO(2), 2D reflection from the group {1,−1}, and
a combination of translation and rotation from the Special Euclidean group SE(2). The arrow +algebra indicates the addition of the
non-Euclideanalgebraicstructuredefiningagroupaction.
shown in the top two rows of Figure 4, and described thegeographiccoordinatesofastormevent,
in detail below. We denote with x a data point within i.e., its location on the surface of the earth
a dataset x ,..x ,...,x , and drop its subscript i for represented as a sphere.
1 i N
conciseness of notations.
Card C3: Point in topological space: Card C3 (pur-
ple) considers a data point that resides in
Card C1: Point in Euclidean space: This category
a topological space Ω, here corresponding
of data points forms the basis of conven-
to a node in a graph. An example in this
tional machine learning and deep learning
category would be a data point representing
approaches. Card C1 (white) in Figure 4
a person in the graph of a social network,
shows an example of a point in Euclidean
space Rn, which represents the dimensions where edges represent social relationships.
of a flower from the Iris dataset (Fisher,
The next three data categories add a group action to
1936), where n represents the number of
the data spaces mentioned above.
features studied.
Card C4: Point in Euclidean space equipped with
Card C2: Point inmanifold:Addinganon-Euclidean group action: Card C4 (blue) displays a
geometry to the coordinate space, card C2 datapointinaEuclideanspacethathasbeen
(orange) illustrates a data point on a mani- equippedwithagroupaction.Groupactions
fold M, where the manifold is the sphere. enableustomodelandpreservesymmetries
For example, this data point can represent in the data. For example, a group action on5
the flower dimensions could be the change with values representing its latitude and longitude. In
of units, from centimeters to millimeters. In both cases, the mathematical nature of the data point
thiscase,thegroupofinterestisthegroupof is unchanged: it still represents a point on a manifold,
scalings R∗. The action of this group does independently of the array used to process it in the
+
not change the information contained in the computer.
data (the flower will still have the same size
in the real world), it only changes the way
B. Data as Signals
we encode that information.
In many applications, data points are not coordinates
Card C5: Point in manifold equipped with group in space but rather functions defined over a space,
action: Card C5 adds the notion of group typically assigning a vector in Rm′ to every point in
action to manifold. An example of such the space Rm. Formally, we can write such a function
a group action could be a change of co- as x : Rm → Rm′ where the input space (here, Rm)
ordinate systems on the sphere, changing is called the domain and the output space (here, Rm′)
the origin of longitudes. In this case, the is called the codomain. We refer to data of this type
group of interest would be the group of 3D as a signal. Elements of the codomain are called the
rotations SO(3). Again, the action of this values of the signal x.
grouprepresentssymmetriesinthedata:the
Color images provide a clear example of this. The do-
information content is unchanged (a storm
mainisaboundedregionofR2:forexample,limitedto
will still happen at the same geographical
therange[0,1920]onthehorizontalaxisand[0,1080]
location in the real world) but the way we
on the vertical axis such that each point in the domain
encode that information has changed.
represents a pixel location. Each pixel location is
Card C6: Point in topological space equipped with assigned a vector in the codomain R3, which specifies
group action: Card C6 equips a topologi- the intensity on each RGB color channel.
cal space with the notion of group action.
Figure 4 introduces several types of data points as
An example of this is a graph of people
functions from a domain to a codomain, together with
equipped with an action that can change the
real-world data examples. Formalizing a signal in this
way indexing is done on the dataset. The
way gives us the flexibility to represent more general
order in which we index people does not
classes of signals using non-Euclidean structures. The
change their social relationships, only the
domain or the codomain (or both) may be any one of
way we represent this data in the computer.
the non-Euclidean spaces introduced in the previous
The group of interest in this case in the
section. We present the different options in the four
group of permutations.
bottomrowsofFigure4anddetailthembelow.
These categories describe the mathematical structure Card S1: Euclidean signal on Euclidean domain:
of the data spaces encountered in machine learning. This represents one of the most common
However,evenwhendatapointsbelongtospaceswith type of data in classical deep learning, func-
topological, geometric, or algebraic structures, their tions as x : Rm → Rm′, going from points
computational representations typically take the form indomain Rm tofeatures incodomain Rm′.
ofarrays.Thesedatapointsmaythusappearasvectors Agray-scaleimageprovidesaclearexample
in a computer’s memory, but this is merely a con- of this. The domain is a bounded region of
venience for processing and storage. The underlying R2 representing pixel locations. Each pixel
mathematicalstructureispreservedthroughconstraints locationisassignedavectorincodomainR,
imposed on the values of these vectors. which specifies the intensity in gray scale.
For example, a data point lying on a sphere might be Card S2: Manifold-valued signal on Euclidean do-
represented as a 3D coordinate vector, but constraints main: This represents signals that can be
on the norm of this vector encode the properties that formalized as a function x : Rm (cid:55)→ M′,
define the mathematical space. We note that a data which assigns an element of a manifold M′
point can be represented by arrays of different sizes. to every point in the Euclidean domain Rm.
For example, the data point on the sphere can be Anexampleofthisoccursindiffusiontensor
represented with vector of size 3 with values encoding imaging, in which 3D images of the brain
its Cartesian coordinates, or with a vector of size 2 are taken. Attached to each voxel location6
&N &= &A
Q’*+#$*+$ 2$#’.’1’04 Q’*+#$*+$ 2$0,’3,#(4
Q’*+#$*+$3"+*@’1)
#’.’1’0*/"1$%."/, 68/1*),"+$%."/,
Q,(%’+$*+$ K,"%8(,3,+#% G,’/’’()*+"#,%$
%’/*"1$+,#9’(R ’@$"$S’9,( ’@$"$%#’(3
2$"10,5(" 2$"10,5(" 2$"10,5("
&H &J &F
678*.$#’.’1’0*/"1 2$#’.’1’04 678*.$68/1*),"+$ 2$0,’3,#(4 678*.$3"+*@’1)
%."/,$9*#:$ %."/,$9*#:$
9*#:$0(’8.$"/#*’+
0(’8.$"/#*’+ 0(’8.$"/#*’+
&:"+0,$’@$3,"%8(,3,+#$ &:"+0,$’@$%.:,(*/"1$
&:"+0,$’@$+’),$*+),T*+0
8+*#% /’’()*+"#,$%4%#,3
-J -A -N
K"+*@’1)L<"18,) 2$#’.’1’04 K"+*@’1)L<"18,) 2$0,’3,#(4
K"+*@’1)L<"18,)
’+$#’.’1’0*/"1 ’+$68/1*),"+
’+$3"+*@’1)
)’3"*+ )’3"*+
;’#"#*’+$.,( !*M8%*’+$#,+%’( O*($#("P/
C’*+# *3"0, /’<"(*"+/,
2$0,’3,#(4 2$0,’3,#(4 2$0,’3,#(4
-F -= -H
68/1*),"+$’+$ 68/1*),"+$’+$
2$#’.’1’04 2$0,’3,#(4 68/1*),"+$’+
#’.’1’0*/"1 68/1*),"+
3"+*@’1)
)’3"*+ )’3"*+
&’’()*+"#,
G("4%/"1,$*3"0, I,3.,("#8(,%
.,($C’*+#
2$"10,5(" 2$"10,5(" 2$"10,5("
-B -D -E
678*.$#’.’1’0*/"1 678*.$68/1*),"+
)’3"*+$w*#: 2$#’.’1’04 )’3"*+$9*#: 2$0,’3,#(4 678*.$3"+*@’1)
9*#:$0(’8.$"/#*’+
0(’8.$"/#*’+ 0(’8.$"/#*’+
&:"+0,$C’*+#$*+)*/,% ;’#"#,$#:,$*3"0, ;’#"#,$#,3.,("#8(,%
2$"10,5(" 2$"10,5(" 2$"10,5("
-== 678*.$#’.’1’0*/"1 -=? -=A
678*.$68/1*),"+ 678*.$3"+*@’1)
)’3"*+$"+)
)’3"*+$"+) "+)$68/1*),"+$
68/1*),"+$ 2$#’.’1’04 2$0,’3,#(4
/’)’3"*+$9*#: /’)’3"*+$9*#:
/’)’3"*+
0(’8.$"/#*’+ 0(’8.$"/#*’+
9*#:$0(’8.$"/#*’+
;’#"#,$.’%,$"+)
;’#"#,$<,/#’($>,1) ;’#"#,$<,/#’($>,1)
<,1’/*#4$<,/#’(%
Fig.4:Geometric,Topological,andAlgebraicStructuresinData.Wecategorizedataintotwotypes:dataascoordinatesinamathematical
space(top2rows)ordataassignals,i.e.,asafunctionoveramathematicalspacewithvaluesinanotherspace(bottomfourrows).Each
cardillustratesthestructureofdataandpresentsareal-worldexample.Thearrows+ topology,+ geometryand+ algebrabetween
cardsindicatetheadditionofnon-Euclideantopological,geometricandalgebraicstructuresrespectively.Notations:Rm:Euclideanspace
ofdimensionm,M:Manifold,Ω:Topologicalspace;x:dataaspointorassignal.
,/".-$+*$%,#"+*)(’’&$%"$"#"!
%1"+0*-$%"$"#"!7
in domain R3 is a 3×3 covariance matrix Thenextcategoriesofdatastructureaddagroupaction
that describes how water molecules diffuse in either the domain or the codomain (or both) of a
in the brain—i.e., an element from the man- given signal x.
ifold of symmetric positive definite matrices
M′ =SPD(3) (Pennec et al., 2006).
Card S7: Euclidean signal on topological domain
equipped with domain action: The signal
here is x : Ω (cid:55)→ Rm′, where Ω is equipped
Card S3: Manifold-valued signal on manifold do-
with a group action. An example of appli-
main: In this category, both the domain and
cation here is the pose of the human body,
the codomain are manifolds M and M′,
where the domain is a undirected graph
respectively, i.e., x : M (cid:55)→ M′. Air traffic
representing the body joints and the group
covariance data is a use case where the
action on the domain is the group of permu-
domainisthesphereM =S2(earthsurface)
tations. Permutations can re-index the order
andthevaluesare2×S2 SPDmatrices,i.e.,
of the body joints by changing the indexing
elements of the manifold codomain M′ =
of the nodes of the graph representing the
SPD(2).Thesematricescanencodecovari-
joints.
ance matrices representing representing dif-
ferentlevelsoflocalcomplexity(LeBrigant Card S8: Euclidean signal on manifold domain
and Puechmorel, 2018). equipped with domain action: Here, the
signal is x:M (cid:55)→Rm′, where the manifold
Card S4: Manifold-valued signal on topological do- domain M is equipped with a group action.
main: Here, the signal inputs live in a topo- Usingtheearthsurfacetemperatureexample
logical domain Ω and the signal values live fromCardS6,wecanapplyarotationonthe
on a manifold M′, such that x : Ω (cid:55)→ M′. domainbychangingtheoriginofthespheri-
A representative example for this setting is calcoordinatesforinstance.Inthatcase,the
humanpose.Eachjointinthebodyisrepre- actiongroupisthegroupofrotationsSO(3)
sentedbyit3Dorientation,i.e.,byarotation acting on M =S2.
matrix on the special orthogonal group of
3D rotations M′ = SO(3). The domain, Card S9: Euclidean signal on Euclidean domain
equipped with domain and codomain ac-
representing the indexing of the body joints,
tions: This represents signals such as x :
canbeaencodedasanon-directedgraphΩ.
Rm → Rm′, where both domain Rm and
Card S5: Euclidean signal on topological domain: codomain Rm′ are equipped with group ac-
This represents a signal with a topological tions. The illustration in the cards shows a
domain Ω and values in a Euclidean space vector field in a domain R2 with values as
Rm′, that is: x : Ω (cid:55)→ Rm′. Reusing the vectors in R2. In this example, the actions
example of encoding the human pose, each on both the domain and the codomain are
body joint can instead be represented by its from the group of rotations SO(2), which
3D location in space, i.e., with a value in applies the same rotation to each point in
Rm′ = R3 defined over the non directed the domain and for each independent vector
graph domain Ω. in the vector field.
Card S10: Euclidean signal on topological domain
Card S6: Euclidean signal on manifold domain:
equipped with domain and codomain ac-
This represents the signal inputs on a man- tions: Here, the function is x : Ω (cid:55)→ Rm′,
ifold M and the signal values in Euclidean where both Ω and Rm′ are equipped with
space Rm′, i.e., x:M (cid:55)→Rm′. An example
group actions. The permutation group acts
ofthisisadatapointrepresentingasnapshot
on the domain by changing the joint indices
of the earth at a given time showing the
in the graph Ω and the 3D rotation group
distributionoftemperaturesacrosstheglobe:
SO(3) acts on the values representing ve-
the earth surface temperatures locations live
locity vectors at each joint.
on the sphere manifold M = S2 and the
temperature themselves are in Rm′ = R Card S11: Euclidean signal on manifold domain
(image and dataset credits: Atmo, Inc.). equipped with domain and codomain ac-8
tions: Here x : M (cid:55)→ Rm′, where both as the relationships between its atoms, but also as
M and Rm′ are equipped with group ac- the positions of its atoms, or as the distance between
tions. We present an example of a vector two atoms. In other words, there would be a feature
field defined over the sphere S2 with vector associated to each atom representing the 3D location
values in Rm′. In this example, applying of the atom. Hence, the molecule would be, in fact,
the action of a 3D rotation rotates both the represented as a signal from a graph Ω to R3, and not
sphericalcoordinatesofthevectorsandtheir just as a graph: it will fall in the category described
actual vector directions. This can be useful by Card S5.
for changes of coordinates.
Similarly, processing organ shapes such as heart sur-
As in the previous subsection, we emphasize the dif- faces requires knowing the 3D coordinates of each
ference between the mathematical representation of point on the surface of the heart. Each heart would
data and the computational representation of data. be, in fact, represented as a signal from a manifold
Until now, we described data as signals represented as M to R3 and not just as a manifold. Consequently,
functionsxoveradomain.However,mostofthetime, these examples fall into the category of data as sig-
the functions x are discretized in the computer. For nals.
example, the domain of a function representing a 2D
color image as the signal x : R2 (cid:55)→ R3 is discretized
intoagridwithp2 pixels,whereeachpixelhasavalue Inthisreview,wewillusethetwomainclassesofdata
inR3.Thedatapointxisthereforerepresented,inthe representations—as coordinates, and as signals—to
computer, by an array of length p2∗3. characterizenon-Euclideanstructure inmachinelearn-
ing and deep learning models.
A recent line of research—the literature of operators
and neural operators (Kovachki et al., 2023)—offers a
different approach by treating each data point x as a
continuous function without discretization.
Remark: Opportunities: The cases of manifold-valued
signalsonmanifolddomain,topological-valuedsignals
IV. SURVEY:NON-EUCLIDEANMACHINE
on topological domains, or topological-valued signals
LEARNING
on manifold domains are not included in the table,
since they have rarely been considered in the ma-
chine learning and deep learning literatures. These
classes represents an interesting avenue for future Wenowreviewalargeanddisparatebodyofliterature
research. of non-Euclidean generalizations of algorithms clas-
sically defined for data residing in Euclidean spaces.
The generalization of machine learning methods to
Remark: Data as Spaces: Beyond data as coordinates
non-Euclidean data relies on the generalization of
and data as signals, a third class of data types can be
theirmathematicalfoundations—probabilityandstatis-
considered: data as spaces. In this class, a data point
tics—to non-Euclidean spaces. In most cases, this
mayinsteadrepresentanentirespace:thedata pointx
requiresnon-trivialalgorithmicinnovations.Suchgen-
is a manifold or topological space itself. For example,
eralizations comprise the bulk of the work reviewed
consider a dataset of molecules. Each molecule x can
here.
be represented as a distinct graph that captures the
molecularstructure,withnodesrepresentingatomsand
edges representing bonds. The dataset is comprised of
However, we note that there are two “simple” ap-
the set of different graphs. Alternatively, the dataset
proaches to generalizing Euclidean models that do
may consist of 3D surface scans of the human body,
not requires significant algorithmic innovation. They
e.g.,adatasetofheartsurfaces—eachdatapointxisa
do not work for all algorithms, and they have limi-
distinct manifold. Most of the time, however, there is
tations. Yet, when possible, they have the benefit of
additional information associated with each data point
implementational simplicity. We briefly describe two
x, so that x is not only a space.
classes of such approaches here—what we call “plug-
For example, a molecule would not only be described in” methods and tangent space methods.9
space. Once the data are mapped to Euclidean space,
Non-Euclidean Probability and Statistics. traditional Euclidean machine learning can be applied.
In a non-Euclidean space, many essential Thisapproachtypicallyachievesbetterresultsthanap-
mathematical concepts must be modified to plying Euclidean methods directly to the original non-
respect the inherent structure of the space. Euclidean data. However, in many cases—particularly
Consider, for example, a dataset consisting formanifoldswithgreatercurvature—itinducesbiases
of points that lie on the surface of the in the results due to errors in the local Euclidean
sphere—for example, the coordinates of dif- approximation of the manifold. Nonetheless, the ap-
ferent cities around the globe. The Euclidean proach is relatively simple, and can be worthwhile for
mean of the points is a value that lies off- datalyingonrelativelyflatmanifolds,oronmanifolds
manifold—a point lying somewhere “inside” thatareflatatthescaleofthespreadofthedata.
the sphere but not on the sphere. To find the
Both the plug-in and tangent space methods have the
centroid of these points on the manifold, we
advantage of implementational simplicity. However,
must instead use the Fre´chet mean, which
many machine learning algorithms require more than
is defined as the point that minimizes the
just the specification of distances and means, which
sumofsquaredgeodesicdistancestoallother
limits the applicability of the plug-in method. Ad-
points in the dataset. By using geodesics, the
ditionally, in many cases, the biases induced by the
Fre´chet mean is constrained to the manifold,
tangent space projection are intolerable for the appli-
and results in a natural generalization of the
cation, which limits its scope. Consequently, in many
notion of a mean to non-Euclidean space.
scenarios,itisnecessarytoexplicitlyconstrainaspects
The field of Geometric Statistics defines
of the algorithms to the manifolds of interest. This
such non-Euclidean generalizations. We refer
comprises the bulk of the work of Non-Euclidean Ma-
the reader to Pennec (2006) for theoretical
chineLearning,whichwecoverinthissection.Wefirst
foundations on manifolds, to Guigui et al.
cover regression methods, followed by dimensionality
(2023) for a comprehensive introduction to
reduction methods. In the following section, we will
these foundations and to Pennec et al. (2019)
cover Non-Euclidean Deep Learning methods, which
for real-world applications.
may be used for regression, dimensionality reduction,
or classification.
“Plug-In” Methods: The most straightforward way to We note that the Non-Euclidean Machine Learning
generalizemachinelearningmethodstonon-Euclidean methods reviewed in this paper assume that cer-
spaces is to simply replace the definitions of addition, tain topological, algebraic, or geometric structures are
subtraction, distances, and means employed in the Eu- known a priori to be present in the data or learning
clidean method with their non-Euclidean counterparts. problem. These methods thus require pre-specifying
Forexample,thek-nearest-neighborsalgorithmcanbe these structures. Sometimes, however, the underly-
naturallydefinedforarbitrarynon-Euclideanmanifolds ing structure is unknown. A class of methods aims
by replacing Euclidean distance with geodesics. Simi- to discover and characterize unknown non-Euclidean
larly, k-means can be generalized using geodesic dis- structure in data. This class includes topological data
tances and the Fre´chet mean. Any kernel method that analysis, certain manifold learning approaches that
makes use of geodesic distance in the kernel function learnparametersofalatentmanifold,suchasinmetric
fallintothiscategoryaswell.Manypopularimplemen- learning, and algebraic methods for discovering latent
tations of machine learning algorithms—for example group structure, also known as group learning. Such
in the scikit-learn package—permit the user methods can be used to identify the non-Euclidean
specification of a distance function, thus facilitating structures present in data so that the appropriate non-
easy generalization to non-Euclidean spaces. Euclidean machine learning method can be specified
and applied. These methods are beyond the scope of
Tangent Space Methods: An alternative approach is this review, as they are used “prior to” non-Euclidean
particularly convenient for non-Euclidean spaces that machine learning.
are manifolds. We call it the tangent space method.
It consists in projecting the data from the manifold
A. Regression
into the tangent space of a particular point on the
manifold, using the so-called exponential map. Impor- This subsection reviews regression on manifolds. We
tantly, the tangent space can be seen as a Euclidean firstintroduceourtaxonomydefinedbythemathemati-10
calstructureofboththedataspacesandtheregression data, typically employing so-called frequentist statisti-
model. Then, we review the literature on this topic in cal principles without incorporating prior distributions
Figure 5 with details in the text. on parameters.
In what follows, we review regression models ac-
Taxonomy: In machine learning, the problem of re-
cording to this geometric taxonomy, row by row in
gression can be defined as learning a function f going
Figure 5. For each category of regression models,
from an input space X to an output space Y. Figure 5
weshowcaseoneemblematic,category-defining,paper
organizes regression models into a taxonomy that is
that reflects the transition from traditional Euclidean
first based on the geometric properties of input and
analysis to the manifold paradigm. We consider the
output spaces—see first two columns in Figure 5.
following papers to be out of scope: 1) Papers that
Here, we differentiate conventional Euclidean spaces
generalizeaclassofcurves(e.g.,splines)onmanifolds,
from more complex manifold spaces, setting the stage
but do not leverage this generalization to perform
for a detailed exploration of five key configurations:
regression, 2) Papers that perform interpolation on
Euclidean to Euclidean, one-dimensional Euclidean to
manifolds, but not regression, 3) Papers whose regres-
Manifold, Euclidean to Manifold, Manifold to Eu-
sion method has been developed for only one type
clidean, and Manifold to Manifold.
of manifold (e.g., only for spheres). Additionally, the
Each configuration is then distinguished by its re- methods reviewed in this section exclusively apply to
gression model—see third column in Figure 5. Lin- one of the data types presented in Section III: regres-
ear methods assume a relationship between the input sion inputs x and outputs y are data as coordinates in
variable x ∈ X and output variable y ∈ Y that can a space, where the space is either a Euclidean space
be represented by a linear function as y = Ax+b, or a manifold.
where A is a matrix of coefficients, and b is a vector
of constants—hence constraining the y’s to belong to 1) Euclidean Input, Euclidean Output:Wereview
a linear space characterized by the parameters A,b. classical regression models that have been gen-
We note that linear regressions are not appropriate eralized to manifolds, in the first row of Fig-
on manifolds since the addition operation, a linear ure 5. Initiated with linear regression by Leg-
operation, is not well-defined on a manifold, a non- endre and Gauss (Legendre, 1805), this cate-
linear space. Consequently, linear methods become gory has expanded to include nonlinear para-
geodesic methods on manifolds on rows 2 and 3 of metric models, particularly polynomial models
Figure 5, where the relationship between x and y introduced by Gergonne (Gergonne, 1815). The
can be represented by a geodesic characterized by inclusionofnon-parametricmethodsismarkedby
a set of parameters analogous to A,b above. Next, the Nadaraya-Watson kernel methods (Nadaraya,
non-linear parametric and non-geodesic parametric 1964), further local linear models (Fan, 1993),
methods involve relationships described by a fixed set and Breiman’s development of random forests
of parameters but in a more complex form, such as (Breiman, 2001). In Bayesian analysis, the linear
polynomials or other non-linear equations y = f (x) and polynomial approaches are respectively rep-
θ
where θ represents the parameters. Non-parametric resented by Bayesian Multilinear (Box and Tiao,
approaches, on the other hand, do not assume a para- 1968)andPolynomialBayesianmodels(Halpern,
metric form for the relationship between x and y, 1973), with the Gaussian Process (Williams and
providing flexibility to model relationships that are Rasmussen, 1995) illustrating the non-parametric
directly derived from data. The term nonparametric Bayesian perspective.
doesnotnecessarilymeanthatsuchmodelscompletely
2) One-dimensional Euclidean Input, Manifold
lack parameters but that the number and nature of
Output: The earlier generalizations of classical
the parameters are flexible and not fixed in advance,
regressionmodelstomanifoldsinvolvegeneraliz-
contrarily to parametric approaches.
ing the output space. Many of these models con-
Further, these methods are categorized as Bayesian or sideraone-dimensionalinputx,andareshownin
non-Bayesian—represented by a fuzzy line or a line the second row of Figure 5. Geodesic regression
in Figure 5. Bayesian methods integrate prior proba- extends linear models into manifolds from one-
bilisticdistributionswithobserveddata,facilitatingthe dimensional Euclidean inputs (Fletcher, 2011).
updating of knowledge about the parameters in light We note a difference of 206 years between the
of new evidence. This approach contrasts with non- establishmentoflinearregressiontoitsgeometric
Bayesianmethods,whichrelyexclusivelyonobserved counterpart. Polynomial regressions on manifolds11
(cid:5)(cid:30)(cid:25)(cid:6)(cid:30)(cid:20)(cid:28)(cid:26)ƒ(cid:18)–(cid:20)(cid:28)(cid:21)ƒ(cid:20)(cid:21)(cid:28)(cid:30)(cid:27)(cid:18)(cid:26)(cid:24)(cid:18)(cid:31)(cid:30)(cid:29)(cid:28)(cid:30)(cid:27)(cid:27)(cid:26)(cid:25)(cid:24)
(cid:31)(cid:30)(cid:29)(cid:28)(cid:30)(cid:27)(cid:27)(cid:26)(cid:25)(cid:24)(cid:18)(cid:17)(cid:25)(cid:16)(cid:30)(cid:15)
(cid:31)(cid:30)(cid:29)(cid:28)(cid:30)(cid:27)(cid:27)(cid:26)(cid:25)(cid:24) (cid:31)(cid:30)(cid:29)(cid:28)(cid:30)(cid:27)(cid:27)(cid:26)(cid:25)(cid:24)
(cid:23)(cid:24)(cid:22)(cid:21)(cid:20) (cid:19)(cid:21)(cid:20)(cid:22)(cid:21)(cid:20)
‹(cid:26)(cid:24)(cid:30)(cid:14)(cid:28) (cid:12)(cid:14)(cid:28)(cid:14)(cid:6)(cid:30)(cid:20)(cid:28)(cid:26)ƒ (cid:2)(cid:25)(cid:24)(cid:22)(cid:14)(cid:28)(cid:14)(cid:6)(cid:30)(cid:20)(cid:28)(cid:26)ƒ
(cid:12)(cid:25)(cid:15)(cid:7)(cid:24)(cid:25)(cid:6)(cid:26)(cid:14)(cid:15) (cid:3)(cid:30)(cid:28)(cid:24)(cid:30)(cid:15)
(cid:17)(cid:21)(cid:15)(cid:20)(cid:26)(cid:15)(cid:26)(cid:24)(cid:30)(cid:14)(cid:28) (cid:13)(cid:5)(cid:30)(cid:28)(cid:29)(cid:25)(cid:24)(cid:24)(cid:30)(cid:11)(cid:10)(cid:11)(cid:4)(cid:8) (cid:13)(cid:2)(cid:14)(cid:16)(cid:14)(cid:28)(cid:14)(cid:7)(cid:1)(cid:127)(cid:14)(cid:20)(cid:27)(cid:25)(cid:24)(cid:129)(cid:141)(cid:8)
(cid:13)(cid:12)(cid:30)(cid:14)(cid:28)(cid:27)(cid:25)(cid:24)(cid:11)(cid:10)(cid:9)(cid:10)(cid:8) (cid:143)ˆ™(cid:26)(cid:30)(cid:28) ‹(cid:25)ƒ(cid:14)(cid:15)(cid:18)‹(cid:26)(cid:24)(cid:30)(cid:14)(cid:28)(cid:18)(cid:13)‡(cid:14)(cid:24)(cid:9)‚(cid:8)
(cid:11) (cid:13)Ž(cid:14)(cid:27)(cid:20)(cid:30)(cid:15)“(cid:14)(cid:21)(cid:11)(cid:9)(cid:10)(cid:4)(cid:8) (cid:31)(cid:14)(cid:24)(cid:16)(cid:25)(cid:6)(cid:18)‡(cid:25)(cid:28)(cid:30)(cid:27)(cid:20)(cid:27)(cid:18)
(cid:13)(cid:143)(cid:28)(cid:30)(cid:26)(cid:6)(cid:14)(cid:24)„(cid:11)(cid:8)
(cid:143)(cid:14)(cid:7)(cid:30)(cid:27)(cid:26)(cid:14)(cid:24)(cid:18) (cid:143)(cid:14)(cid:7)(cid:30)(cid:27)(cid:26)(cid:14)(cid:24)(cid:18) (cid:5)(cid:14)(cid:21)(cid:27)(cid:27)(cid:26)(cid:14)(cid:24)(cid:18)(cid:12)(cid:28)(cid:25)ƒ(cid:30)(cid:27)(cid:27)
(cid:17)(cid:21)(cid:15)(cid:20)(cid:26)(cid:15)(cid:26)(cid:24)(cid:30)(cid:14)(cid:28) (cid:12)(cid:25)(cid:15)(cid:7)(cid:24)(cid:25)(cid:6)(cid:26)(cid:14)(cid:15)
(cid:13)(cid:31)(cid:14)(cid:27)(cid:6)(cid:21)(cid:27)(cid:27)(cid:30)(cid:24)„‚(cid:8)
(cid:13)(cid:143)(cid:25)(cid:144)(cid:157)(cid:18) (cid:26)(cid:14)(cid:25)(cid:18)(cid:129)(cid:10)(cid:8) (cid:13) (cid:14)(cid:15)(cid:22)(cid:30)(cid:28)(cid:24)€‚(cid:8)
(cid:5)(cid:30)(cid:25)(cid:16)(cid:30)(cid:27)(cid:26)ƒ (cid:12)(cid:14)(cid:28)(cid:14)(cid:6)(cid:30)(cid:20)(cid:28)(cid:26)ƒ (cid:2)(cid:25)(cid:24)(cid:22)(cid:14)(cid:28)(cid:14)(cid:6)(cid:30)(cid:20)(cid:28)(cid:26)ƒ
(cid:17)(cid:14)(cid:24)(cid:26)…(cid:25)(cid:15)(cid:16)(cid:18)
(cid:12)(cid:25)(cid:15)(cid:7)(cid:24)(cid:25)(cid:6)(cid:26)(cid:14)(cid:15)(cid:18) (cid:3)(cid:30)(cid:28)(cid:24)(cid:30)(cid:15)(cid:18)(cid:13)š(cid:14)‘(cid:26)(cid:27)(cid:11)„(cid:8)
(cid:5)(cid:30)(cid:25)(cid:16)(cid:30)(cid:27)(cid:26)ƒ (cid:13) (cid:26)(cid:24)Š(cid:15)(cid:30)(cid:11)(cid:141)(cid:8) ‹(cid:25)ƒ(cid:14)(cid:15)(cid:18)(cid:5)(cid:30)(cid:25)(cid:16)(cid:30)(cid:27)(cid:26)ƒ(cid:18)
(cid:13)‡(cid:15)(cid:30)(cid:20)ƒ‰(cid:30)(cid:28)(cid:11)(cid:11)(cid:8)
• (cid:17)(cid:14)(cid:24)(cid:26)…(cid:25)(cid:15)(cid:16)(cid:18)(cid:143)(cid:30)™(cid:26)(cid:30)(cid:28)(cid:18) (cid:13)–ƒ‰(cid:25)(cid:20)™••(cid:8)
–(cid:22)(cid:15)(cid:26)(cid:24)(cid:30)(cid:18)(cid:13) (cid:14)(cid:24)(cid:26)Š•„(cid:8)
(cid:19)(cid:24)(cid:30)(cid:1)(cid:16)(cid:26)(cid:6)—
(cid:17)(cid:14)(cid:24)(cid:26)…(cid:25)(cid:15)(cid:16)
(cid:143)(cid:14)(cid:7)(cid:30)(cid:27)(cid:26)(cid:14)(cid:24)
(cid:143)(cid:14)(cid:7)(cid:30)(cid:27)(cid:26)(cid:14)(cid:24) (cid:143)(cid:28)(cid:25)˜(cid:24)(cid:26)(cid:14)(cid:24) (cid:5)(cid:30)(cid:25)(cid:16)(cid:30)(cid:27)(cid:26)ƒ
(cid:12)(cid:25)(cid:15)(cid:7)(cid:24)(cid:25)(cid:6)(cid:26)(cid:14)(cid:15) (cid:13)(cid:127)(cid:14)(cid:24)(cid:29)(cid:11)(cid:4)(cid:8)
(cid:13)”‰(cid:14)(cid:24)(cid:29)•„(cid:8)
(cid:13)(cid:17)(cid:21)(cid:28)(cid:14)(cid:15)(cid:26)(cid:16)‰(cid:14)(cid:28)(cid:14)(cid:24)(cid:11)€(cid:8)
(cid:17)(cid:14)(cid:24)(cid:26)…(cid:25)(cid:15)(cid:16)(cid:18)(cid:31)(cid:14)(cid:24)(cid:16)(cid:25)(cid:6)(cid:18)‡(cid:25)(cid:28)(cid:30)(cid:27)(cid:20)
Ž(cid:25)(cid:24)‘(cid:25)(cid:15)(cid:21)(cid:20)(cid:26)(cid:25)(cid:24)
‡(cid:28)ˆƒ‰(cid:30)(cid:20)(cid:18) (cid:13) (cid:27)(cid:14)(cid:29)Š(cid:28)(cid:14)(cid:27)(cid:25)(cid:21)(cid:15)(cid:26)(cid:27)(cid:11)(cid:10)(cid:8)
(cid:13)(cid:12)(cid:30)(cid:20)(cid:30)(cid:28)(cid:27)(cid:30)(cid:24)(cid:11)€(cid:8) (cid:31)(cid:30)(cid:29)(cid:28)(cid:30)(cid:27)(cid:27)(cid:26)(cid:25)(cid:24) ‹(cid:25)ƒ(cid:14)(cid:15)(cid:18)‡(cid:28)ˆƒ‰(cid:30)(cid:20)(cid:18)
(cid:13)(cid:12)(cid:30)(cid:24)(cid:24)(cid:30)ƒ„(cid:129)(cid:8) (cid:13)(cid:12)(cid:30)(cid:20)(cid:30)(cid:28)(cid:27)(cid:30)(cid:24)(cid:11)€(cid:8)
‹(cid:25)ƒ(cid:14)(cid:15)(cid:18)Œ(cid:144)(cid:20)(cid:28)(cid:26)(cid:24)(cid:27)(cid:26)ƒ(cid:18)(cid:13)‹(cid:26)(cid:24)(cid:11)€(cid:8)
‚
(cid:17)(cid:14)(cid:24)(cid:26)…(cid:25)(cid:15)(cid:16)(cid:18)(cid:5)(cid:14)(cid:21)(cid:27)(cid:27)(cid:26)(cid:14)(cid:24)(cid:18)
(cid:12)(cid:28)(cid:25)ƒ(cid:30)(cid:27)(cid:27)(cid:18)(cid:13)†(cid:14)(cid:24)(cid:29)(cid:11)(cid:129)(cid:8)
(cid:19)(cid:22)(cid:22)(cid:25)(cid:28)(cid:20)(cid:21)(cid:24)(cid:26)(cid:20)(cid:7) (cid:127)(cid:28)(cid:14)(cid:22)(cid:22)(cid:30)(cid:16)
(cid:5)(cid:14)(cid:21)(cid:27)(cid:27)(cid:26)(cid:14)(cid:24)(cid:18)(cid:12)(cid:28)(cid:25)ƒ(cid:30)(cid:27)(cid:27)
(cid:13)(cid:17)(cid:14)(cid:15)(cid:15)(cid:14)(cid:27)(cid:20)(cid:25)(cid:11)(cid:10)(cid:8)
(cid:12)(cid:14)(cid:28)(cid:14)(cid:6)(cid:30)(cid:20)(cid:28)(cid:26)ƒ (cid:2)(cid:25)(cid:24)(cid:22)(cid:14)(cid:28)(cid:14)(cid:6)(cid:30)(cid:20)(cid:28)(cid:26)ƒ
(cid:141)
Ž(cid:26)(cid:28)ƒ(cid:21)(cid:15)(cid:14)(cid:28)(cid:18) (cid:17)(cid:14)(cid:24)(cid:26)…(cid:25)(cid:15)(cid:16)
(cid:13)Ž‰(cid:30)(cid:28)(cid:24)(cid:25)‘(cid:11)„(cid:8) (cid:3)(cid:30)(cid:28)(cid:24)(cid:30)(cid:15)(cid:18)
(cid:13)’(cid:25)‰(cid:24)(cid:27)(cid:25)(cid:24)€(cid:10)(cid:8) (cid:13)(cid:12)(cid:30)(cid:15)(cid:15)(cid:30)(cid:20)(cid:26)(cid:30)(cid:28)„(cid:4)(cid:8)
(cid:17)(cid:14)(cid:24)(cid:26)…(cid:25)(cid:15)(cid:16)(cid:18)(cid:31)(cid:30)(cid:29)(cid:21)(cid:15)(cid:14)(cid:28)(cid:18)(cid:27)(cid:22)(cid:15)(cid:26)(cid:24)(cid:30)(cid:27)(cid:18)
(cid:4)
(cid:31)(cid:30)(cid:29)(cid:28)(cid:30)(cid:27)(cid:27)(cid:26)(cid:25)(cid:24)(cid:18)(cid:14)…(cid:20)(cid:30)(cid:28)(cid:18) (cid:13)–(cid:20)(cid:30)(cid:26)(cid:24)Š(cid:30)„(cid:10)(cid:8)
(cid:20)(cid:14)(cid:24)(cid:29)(cid:30)(cid:24)(cid:20)(cid:18)(cid:12)Ž› (cid:17)(cid:14)(cid:24)(cid:26)…(cid:25)(cid:15)(cid:16)(cid:18)(cid:3)(cid:30)(cid:28)(cid:24)(cid:30)(cid:15)(cid:18)
(cid:13)(cid:5)(cid:21)(cid:25)•„(cid:8) (cid:13)(cid:143)(cid:14)(cid:24)(cid:30)(cid:28)“(cid:30)(cid:30)(cid:11)(cid:4)(cid:8)
(cid:13)”‰(cid:14)(cid:24)(cid:29)•„•„(cid:8)
Fig.5:GeometricStructuresinRegressioncategorizedaccordingtothegeometryoftheinputandoutputdataspaces(firsttwocolumns)
and the geometry of the regression model (last column). Yellow boxes correspond to Euclidean space, while orange corresponds to non-
Euclideanspace.PartiallyEuclideancasesarelightorange.Eachpictogramrepresentsthekindofparametrizationused:linear(geodesic),
nonlinear(nongeodesic)parametric,ornonparametric,withorwithoutBayesianpriors.
(cid:24)(cid:25)(cid:2)
(cid:24)(cid:25)(cid:2)
(cid:24)(cid:25)(cid:2)
(cid:24)(cid:25)(cid:2)
(cid:24)(cid:25)(cid:2)
(cid:24)(cid:14)(cid:26)(cid:27)(cid:30)(cid:7)(cid:14)(cid:143)
(cid:24)(cid:14)(cid:26)(cid:27)(cid:30)(cid:7)(cid:14)(cid:143)
(cid:24)(cid:14)(cid:26)(cid:27)(cid:30)(cid:7)(cid:14)(cid:143)
(cid:24)(cid:14)(cid:26)(cid:27)(cid:30)(cid:7)(cid:14)(cid:143)
(cid:24)(cid:14)(cid:26)(cid:27)(cid:30)(cid:7)(cid:14)(cid:143)
(cid:24)(cid:14)(cid:26)(cid:27)(cid:30)(cid:7)(cid:14)(cid:143)
(cid:24)(cid:14)(cid:26)(cid:27)(cid:30)(cid:7)(cid:14)(cid:143)
(cid:24)(cid:14)(cid:26)(cid:27)(cid:30)(cid:7)(cid:14)(cid:143)12
(Hinkle et al., 2012a) and Bezier-splines fitting years later.
on manifolds (Hanik et al., 2020) generalize their
4) Manifold Input, Euclidean Output: We now
Euclidean counterparts to manifolds in the out-
reviewgeometricgeneralizationsofEuclideanre-
put space, 197 years and 35 years later respec-
gression models that have received less attention:
tively.Non-parametricmethodswithoutputvalues
models in which the regression input space is
on manifolds include Fre´chet-casted Nadaraya-
a manifold. We start with methods for which
Watsonkernelregression(Davisetal.,2010),and
the output space is a Euclidean space, in the
localgeodesicregression(Scho¨tz,2022)—thelat-
fourth row of Figure 5. Johnson and Wehrly
terbeingthecounterpartoflocallinearregression
defined a regression model for an angular (one-
to manifolds. In terms of Bayesian methods, the
dimensional) variable with a Euclidean scalar
geodesic regression model was made Bayesian in
(one-dimensional) output variable (Johnson and
(Zhang et al., 2020), while manifold polynomial
Wehrly, 1978). Chernov’s circular regression out-
regression turned Bayesian in Muralidharan et al.
putstohigher-dimensionalEuclideanspaces,typ-
(2017).Lastly,onenon-geodesic,non-parametric,
icallytwo-orthree-dimensional(Chernov,2010).
Bayesian model belongs to this category: ker-
Pelletier’s non-parametric regression approach
nel regressions by Devito and Wang’s Brownian
standsoutforestimatingfunctionsfrommanifold
motion model (Wang, 2015) which generalizes
inputstoEuclideanoutputs(Pelletier,2006).Both
Nadaraya-Watson’s approach 51 years later.
methods are non-Bayesian. In terms of Bayesian
methods, we find the Bayesian circular-linear re-
3) Euclidean Input, Manifold Output: Next, we
gression method (Gill and Hangartner, 2010), but
review regression models with outputs on a man-
no Bayesian nonparametric methods—indicating
ifold, for which the input is not restricted to be
an opportunity for further exploration.
one-dimensional,inthethirdrowofFigure5.The
Fre´chet regression (Petersen and Muller, 2016) 5) Manifold Input, Manifold Output: The fifth
generalizes the geodesic regression to higher- row of Figure 5 presents the most general case
dimensional Euclidean inputs. We also find sev- of regression, from a geometric perspective. In
eral nongeodesic parametric regression models. thesemodels,boththeregressioninputandoutput
One of the earliest works in this category deals spaces are manifolds. This category includes a
withtheparametricregressionofregularizedman- methodthatfirsttransformsbothinputandoutput
ifold valued functions (Pennec et al., 2006). A data from manifolds to Euclidean spaces, and
key idea is to rephrase convolutions as weighted thenapplyanauxiliaryclassicalregressionmodel
Fre´chet means of manifold variables, which be- on Euclidean spaces (Guo et al., 2019). In the
come the parameters of the implicit function. nonparametric methods, we find Steinke’s regular
Also in the nongeodesic parametric regression splines methods (Steinke et al., 2008) and Baner-
category, we find the semi-parametric intrinsic jee’s manifold kernel regression (Banerjee et al.,
regression model by (Shi et al., 2009), or the 2015) generalizing in 2015 both the classical
stochastic development regression by Ku¨hnel and kernelregressionfrom1964anditsgeneralization
Sommer (2017). Non-parametric methods with to manifold output space from 2010. At the time
manifold-valuedoutputsincludethemanifoldran- of the review, there is no method in this category
dom forests (Tsagkrasoulis and Montana, 2018) that adopts the Bayesian point of view.
that generalize their Euclidean counterpart 15
years later, the local Fre´chet regression (Petersen
B. Dimensionality Reduction
and Muller, 2016), another multi-dimensional
generalization of the local linear regression, and This subsection reviews dimensionality reduction
thelocalextrinsicregressionby(Linetal.,2017). methods, which include: manifold learning methods,
In terms of Bayesian approaches, we observe a whereamanifoldoflower-dimensionislearnedwithin
lackofmethodswithingeodesicandnongeodesic a Euclidean space of higher dimension; submanifold
parametric models. However, we find Bayesian learning methods, where a submanifold of lower-
non-parametric methods, such as Manifold Gaus- dimension is learned within a manifold of higher
sian processes (Yang and Dunson, 2016) and dimension; and encoding methods, where data in a
Mallasto’s wrapped Gaussian processes (Mallasto high-dimension Euclidean or manifold is mapped to
and Feragen, 2018) which generalizes Williams a lower-dimensional Euclidean or manifold via an
and Rasmussen’s Gaussian processes 9 and 13 encoder. We first introduce our taxonomy defined by13
the mathematical structure of both the spaces and the Additionally,wehighlightinthepictogramsifthedata
dimensionality reduction model. Then, we review the is assumed to come from a generative model. A gen-
literature on this topic in Figure 6 with details in the erative model explains how data points are generated
text. from latents using probability distributions. When a
generative model is used for a given dimensionality
Taxonomy: We define the problem of dimensionality reduction approach, it is denoted by a shaded yellow
reduction as transforming high-dimensional data from or orange dot. When there is no generative model,
their data space X into a latent space Y of lower we only use a non-shaded yellow or orange dot. For
dimension,i.e.,dim(Y)<dim(X).Figure6organizes example, principal component analysis (PCA) has a
dimensionalityreductionmethodsintoataxonomyfirst decoder, but no generative model; while probabilistic
basedonthegeometricpropertiesofthedataandlatent PCA uses a decoder with a generative model.
spaces—see the first two columns. It differentiates
Second, approaches are organized depending on
between conventional Euclidean spaces and the more
whether they leverage an encoder E, where E is a
complex manifold spaces, setting the stage for the
function mapping data x’s to latents y’s: E(x) = y.
following four key configurations: Euclidean Data to
Indeed, while every dimensionality reduction methods
EuclideanLatents,ManifoldDatatoOne-Dimensional
convertshigh-dimensionaldataintolower-dimensional
EuclideanLatents,ManifoldDatatoEuclideanLatents,
latents, only some methods do so through an explicit
Euclidean Data to Manifold Latents, and Manifold
encoder function E. Others might only compute the
Data to Manifold Latents. For each configuration, the
latent y corresponding to a data point x through the
lower dimensional latent space Y is schematically i i
result of an optimization min Cost(x ). The presence
represented as a space of dimension 1: a line for Eu- y i
of an encoder is represented by a black arrow and
clidean spaces, and a circle for manifolds. The higher-
the legend E, and the arrow is a dotted arrow is the
dimensionaldataspaceX isschematicallyrepresented
encoder is non-parametric. The use of optimization is
as a space of dimension 2: a plane for Euclidean
represented by the ! and no black arrow.
spaces,and asphereformanifolds. Yet,weemphasize
that we review all dimensionality reduction methods Third,approachesareorganizeddependingonwhether
here; not only the methods going from dimension 2 to theycomputeanuncertaintyonthelatentsyassociated
dimension 1. with the data points x. Uncertainty on the latents is
represented by the posterior distribution p(y|x). If the
Each configuration is then distinguished by the ap- method provides a posterior distribution—or at least
proach to dimensionality reduction—see third column an approximation of it—we use a shaded blue dot.
inFigure6.First,dimensionalityreductionapproaches If not, we use a non-shaded blue dot. The two cases
are organized depending on whether they leverage of without and with posterior form the subrows of
a decoder, and if so, of what type: linear (resp. a given configuration in the table. We mark with an
geodesic), nonlinear (resp. nongeodesic) parametric, S the cases where the posterior distribution p(y|x)
non-parametric—represented by full line, full curved is not computed analytically, but provided through a
line, and dashed curved line in the pictograms of sampling approach.
Figure6.Whileeverydimensionalityreductionmethod
convertshigh-dimensionaldataintolower-dimensional Fourth, approaches are categorized based on how they
latents,onlysomemethodsintroduceadecoderD that compute uncertainty on the parameters of the decoder,
converts low-dimensional latents y’s back into high- i.e.,onwhethertheyareBayesianornot.Forexample,
dimensional data x’s: x = D(y). When a decoder traditional PCA does not incorporate uncertainty, but
D exists, the latent space Y can be embedded into BayesianPCAdoes.Thisdistinctionrepresentsthelast
the data space X via D(Y) ⊂ X. The yellow and subrow of each configuration, illustrated by a shaded
orange curves in the pictograms represent that em- orange line.
bedding.Wefurtherdistinguishtwosubcases:whether
We now survey the various categories of dimensional-
the embedded space D(Y) is linear (geodesic, in
ity reduction row by row in Figure 6.
the manifold case), or nonlinear (or non-geodesic).
The presence of a decoder is represented by a black 1) Euclidean Data, Euclidean Latents: We de-
arrow and the legend D; the black arrow is a dotted scribe approaches from this configuration subcol-
arrowifthedecoderisnon-parametric.Dimensionality umn by subcolumn. Principal Component Anal-
reduction methods that do not leverage any decoder ysis (PCA) (Pearson, 1901) learns a linear sub-
are reviewed independently at the bottom of Figure 6. space, while Probabilistic PCA (PPCA) (Tipping14
(cid:14)(cid:26)(cid:23)(cid:27)(cid:26)(cid:29)(cid:13)(cid:28)(cid:16)(cid:20)(cid:12)(cid:29)(cid:13)(cid:17)(cid:16)(cid:29)(cid:17)(cid:13)(cid:26)(cid:24)(cid:20)(cid:28)(cid:25)(cid:20)(cid:31)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:28)(cid:23)(cid:25)(cid:30)(cid:22)(cid:28)(cid:29)(cid:21)(cid:20)(cid:19)(cid:26)(cid:18)(cid:17)(cid:16)(cid:29)(cid:28)(cid:23)(cid:25)
(cid:4) (cid:4)(cid:25)(cid:16)(cid:23)(cid:18)(cid:26)(cid:13) (cid:31)(cid:31)(cid:26)(cid:16)(cid:23)(cid:18)(cid:26)(cid:13) ‘ ™(cid:9)(cid:29)(cid:28)(cid:27)(cid:28)›(cid:30)(cid:29)(cid:28)(cid:23)(cid:25) (cid:12) (cid:12)(cid:30)(cid:27)(cid:9)(cid:22)(cid:28)(cid:25)† (cid:25)(cid:23)(cid:20)(cid:9)(cid:23)(cid:24)(cid:29)(cid:26)(cid:13)(cid:28)(cid:23)(cid:13) ˜š(cid:20)(cid:9)(cid:23)(cid:24)(cid:29)(cid:26)(cid:13)(cid:28)(cid:23)(cid:13) •(cid:30)(cid:21)(cid:26)(cid:24)(cid:28)(cid:30)(cid:25)
(cid:31)(cid:30)(cid:29)(cid:30) (cid:11)(cid:30)(cid:29)(cid:26)(cid:25)(cid:29) (cid:31)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:28)(cid:23)(cid:25)(cid:30)(cid:22)(cid:28)(cid:29)(cid:21)(cid:20)(cid:19)(cid:26)(cid:18)(cid:17)(cid:16)(cid:29)(cid:28)(cid:23)(cid:25)(cid:20)(cid:15)(cid:23)(cid:18)(cid:26)(cid:22)
(cid:11)(cid:28)(cid:25)(cid:26)(cid:30)(cid:13) (cid:30)(cid:13)(cid:30)(cid:27)(cid:26)(cid:29)(cid:13)(cid:28)(cid:16) Œ(cid:23)(cid:25)(cid:9)(cid:30)(cid:13)(cid:30)(cid:27)(cid:26)(cid:29)(cid:13)(cid:28)(cid:16)
(cid:4) (cid:4) ‘
‰(cid:5) (cid:5)(cid:4) ‰
(cid:3) (cid:26)(cid:30)(cid:13)(cid:24)(cid:23)(cid:25)ƒ(cid:1)(cid:129) (cid:3)(cid:19)(cid:17)(cid:27)(cid:26)(cid:22)(cid:8)(cid:30)(cid:13)(cid:29)(cid:127)‹(cid:129) (cid:3)(cid:10)(cid:30)(cid:24)(cid:29)(cid:28)(cid:26)(cid:127)(cid:157)(cid:129)
(cid:31) (cid:31) (cid:31)
(cid:1) (cid:4) (cid:4) ‘ (cid:11)(cid:11)(cid:4)(cid:20)(cid:3)(cid:19)(cid:23)˜(cid:26)(cid:28)(cid:24)ƒƒ(cid:129)
(cid:13)(cid:23)„(cid:30)„(cid:28)(cid:22)(cid:28)(cid:24)(cid:29)(cid:28)(cid:16) (cid:14) (cid:11)(cid:6)(cid:15)
‰(cid:5) (cid:6)(cid:5)(cid:4) (cid:3)(cid:11)(cid:30)˜(cid:13)(cid:26)(cid:25)(cid:16)(cid:26)ƒ‡(cid:129)
(cid:3)(cid:143)(cid:28)(cid:9)(cid:9)(cid:28)(cid:25)†(cid:157)’(cid:129) (cid:3)“(cid:28)(cid:25)†(cid:27)(cid:30)(cid:1)”(cid:129) (cid:13)(cid:23)„(cid:30)„(cid:28)(cid:22)(cid:28)(cid:24)(cid:29)(cid:28)(cid:16)(cid:20)
(cid:31) (cid:31) (cid:31) ‰(cid:20)(cid:3)‰(cid:8)(cid:30)(cid:25)†ƒ(cid:1)(cid:129)
(cid:4) (cid:4) ‘
•(cid:30)(cid:21)(cid:26)(cid:24)(cid:28)(cid:30)(cid:25) •(cid:30)(cid:21)(cid:26)(cid:24)(cid:28)(cid:30)(cid:25)
‰(cid:5) (cid:141)(cid:17)(cid:22)(cid:22)(cid:7)(cid:6)(cid:5)(cid:4) (cid:14) (cid:20)(cid:11)(cid:6)(cid:15)
(cid:3)•(cid:28)(cid:24)(cid:8)(cid:23)(cid:9)(cid:157)(cid:127)(cid:129) (cid:3)“(cid:28)(cid:25)†(cid:27)(cid:30)(cid:1)”(cid:129) (cid:3)(cid:11)(cid:30)˜(cid:13)(cid:26)(cid:25)(cid:16)(cid:26)ƒ‡(cid:129)
(cid:31) (cid:31) (cid:31)
(cid:14)(cid:26)(cid:23)(cid:18)(cid:26)(cid:24)(cid:28)(cid:16) (cid:30)(cid:13)(cid:30)(cid:27)(cid:26)(cid:29)(cid:13)(cid:28)(cid:16) Œ(cid:23)(cid:25)(cid:9)(cid:30)(cid:13)(cid:30)(cid:27)(cid:26)(cid:29)(cid:13)(cid:28)(cid:16)
(cid:4) ‘ (cid:13)(cid:28)(cid:25)(cid:16)(cid:28)(cid:9)(cid:30)(cid:22)(cid:20)(cid:141)(cid:22)(cid:23)˜(cid:24)
(cid:3) (cid:30)(cid:25)(cid:30)(cid:13)(cid:26)(cid:29)(cid:23)(cid:24)(cid:1)”(cid:129)
(cid:12)(cid:9)(cid:26)(cid:16)(cid:28)(cid:30)(cid:22)(cid:20)(cid:16)(cid:30)(cid:24)(cid:26) (cid:12)(cid:27)(cid:23)(cid:23)(cid:29)(cid:8) (cid:31) (cid:19)(cid:28)(cid:26)(cid:27)(cid:30)(cid:25)(cid:25)(cid:28)(cid:30)(cid:25)(cid:20) ‰
‚ ™(cid:25)(cid:26)(cid:7)(cid:18)(cid:28)(cid:27)š (cid:9)(cid:30)(cid:29)(cid:8)(cid:24) (cid:3)(cid:10)(cid:30)(cid:17)„(cid:26)(cid:13)†(cid:1)‹(cid:129)
(cid:31) (cid:3)(cid:15)(cid:30)(cid:16)(cid:8)(cid:30)(cid:18)(cid:23)ƒ‹(cid:129) (cid:13)(cid:23)„(cid:30)„(cid:28)(cid:22)(cid:28)(cid:24)(cid:29)(cid:28)(cid:16)(cid:20) ‰
(cid:3)“(cid:30)(cid:25)†‚”(cid:129)
(cid:4) (cid:143)(cid:30)(cid:25)†(cid:26)(cid:25)(cid:29)(cid:20) ‰(cid:5)(cid:20) (cid:4) ‘
(cid:3)(cid:141)(cid:22)(cid:26)(cid:29)(cid:16)(cid:8)(cid:26)(cid:13)ƒ”(cid:129) (cid:19)(cid:28)(cid:26)(cid:27)(cid:30)(cid:25)(cid:25)(cid:28)(cid:30)(cid:25)(cid:20)
(cid:14) ‰(cid:5)(cid:20) (cid:19)(cid:28)(cid:26)(cid:27)(cid:30)(cid:25)(cid:25)(cid:28)(cid:30)(cid:25)(cid:20) (cid:11)(cid:11)(cid:4)
(cid:3)(cid:10)(cid:17)(cid:16)(cid:144)(cid:26)(cid:27)(cid:30)(cid:25)(cid:1)ƒ(cid:129) (cid:5)(cid:4) (cid:3)(cid:15)(cid:30)(cid:28)†(cid:25)(cid:30)(cid:25)(cid:29)‚‡(cid:129)
(cid:31) (cid:14)(cid:5) (cid:31) (cid:3)(cid:15)(cid:28)(cid:23)(cid:22)(cid:30)(cid:25)(cid:26)‚ƒ(cid:129) (cid:31)
(cid:3)(cid:12)(cid:23)(cid:27)(cid:27)(cid:26)(cid:13)(cid:1)ƒ(cid:129)
(cid:4)
‡ (cid:12) (cid:13)(cid:23)„ (cid:30) (cid:14)„ (cid:5)(cid:28)(cid:22)(cid:28)(cid:24)(cid:29)(cid:28)(cid:16) (cid:19)(cid:28)(cid:26) (cid:3)(cid:15)(cid:27) (cid:28)(cid:30) (cid:23)(cid:25) (cid:22)(cid:30)(cid:25) (cid:25)(cid:28)(cid:30) (cid:26)(cid:25) ‚(cid:20) ƒ(cid:6) (cid:129)(cid:5)(cid:4)
(cid:3)…(cid:8)(cid:30)(cid:25)†(cid:1)‡(cid:129)
(cid:31) (cid:31)
(cid:4) (cid:4)
•(cid:30)(cid:21)(cid:26)(cid:24)(cid:28)(cid:30)(cid:25) (cid:19)(cid:28)(cid:26)(cid:27)(cid:30)(cid:25)(cid:25)(cid:28)(cid:30)(cid:25)(cid:20)
(cid:14)(cid:5) (cid:141)(cid:17)(cid:22)(cid:22)(cid:7)(cid:6)(cid:5)(cid:4)
(cid:3)…(cid:8)(cid:30)(cid:25)†(cid:1)”(cid:129) (cid:3)(cid:15)(cid:28)(cid:23)(cid:22)(cid:30)(cid:25)(cid:26)‚ƒ(cid:129)
(cid:31) (cid:31)
(cid:14)(cid:26)(cid:23)(cid:18)(cid:26)(cid:24)(cid:28)(cid:16) (cid:30)(cid:13)(cid:30)(cid:27)(cid:26)(cid:29)(cid:13)(cid:28)(cid:16) Œ(cid:23)(cid:25)(cid:9)(cid:30)(cid:13)(cid:30)(cid:27)(cid:26)(cid:29)(cid:13)(cid:28)(cid:16)
(cid:4) (cid:10)(cid:21)(cid:9)(cid:26)(cid:13)(cid:24)(cid:9)(cid:8)(cid:26)(cid:13)(cid:26)(cid:7)(cid:6)(cid:5)(cid:4) ‘
” (cid:3)(cid:31)(cid:30)(cid:2)(cid:28)(cid:18)(cid:24)(cid:23)(cid:25)(cid:1)(cid:127)(cid:129)
(cid:15)(cid:30)(cid:25)(cid:28) (cid:23)(cid:22)(cid:18)(cid:20)
(cid:11)(cid:28)(cid:26)(cid:6)(cid:5)(cid:4)(cid:20)
(cid:14) (cid:11)(cid:6)(cid:15)
(cid:3)(cid:141)(cid:30)(cid:22)(cid:23)(cid:13)(cid:24)(cid:28)(cid:1)(cid:127)(cid:129)
(cid:3)€(cid:26)(cid:25)(cid:24)(cid:26)(cid:25)‚ƒ(cid:129)
(cid:143)(cid:23)(cid:13)(cid:23)(cid:28)(cid:18)(cid:30)(cid:22)(cid:20)(cid:6)(cid:5)(cid:4)
(cid:31) (cid:3)(cid:15)(cid:28)(cid:144)(cid:17)(cid:22)(cid:24)(cid:144)(cid:28)(cid:1)(cid:157)(cid:129) (cid:31)
— Œ(cid:23)(cid:20)(cid:18)(cid:26)(cid:16)(cid:23)(cid:18)(cid:26)(cid:13)
(cid:1)
Ž
‘ (cid:3)(cid:11)(cid:26)Š (cid:22)(cid:30)(cid:15) (cid:25)(cid:5) (cid:18) (cid:1)(cid:127)(cid:129) (cid:4) ‡ ‘• (cid:3)(cid:24) (cid:30) (cid:17) (cid:26)(cid:13) „(cid:21) (cid:25)(cid:24)(cid:16) (cid:25)(cid:9)(cid:26) (cid:26)(cid:30)(cid:25) (cid:16)(cid:16)(cid:29) (cid:1)(cid:26)(cid:13) (cid:127)(cid:28) (cid:24)(cid:16) (cid:20) (cid:129)(cid:20) ”
Ž
‘ (cid:4)(cid:27)
(cid:3)
Œ„(cid:23) (cid:28)(cid:16)(cid:28) (cid:26)(cid:25) (cid:144)(cid:18) (cid:26)(cid:16) (cid:18)(cid:30) (cid:22)(cid:1)(cid:28)(cid:13) (cid:25) ’(cid:26) † (cid:129)(cid:24) ‘ (cid:24)(cid:17)
(cid:3)
(cid:5)„(cid:13) (cid:144)(cid:7)(cid:28) „(cid:25) (cid:8)(cid:17)(cid:16) (cid:23)(cid:28) (cid:25) –(cid:9) ‚(cid:18)(cid:30) ƒ(cid:22)(cid:22) (cid:26) (cid:129)(cid:24)
(cid:29)(cid:12)Œ(cid:4) ˆ(cid:24)(cid:23)(cid:27)(cid:30)(cid:9) ˆ(cid:141)•(cid:31) (cid:19)(cid:28)(cid:26)(cid:7)(cid:12)Œ(cid:4)(cid:20)
(cid:3)(cid:6)(cid:30)(cid:25)(cid:31)(cid:26)(cid:13)(cid:15)(cid:30)(cid:30)(cid:29)(cid:26)(cid:25)ƒ‚(cid:129) (cid:3)(cid:143)(cid:26)(cid:25)(cid:26)(cid:25)„(cid:30)(cid:17)(cid:27)ƒƒ(cid:129) (cid:3)(cid:12)(cid:23)(cid:27)(cid:27)(cid:26)(cid:13)(cid:1)‡(cid:129) (cid:3)•(cid:26)(cid:13)†(cid:24)(cid:24)(cid:23)(cid:25)‚‚(cid:129)
Fig.6:GeometricStructuresinDimensionalityReductioncategorizedaccordingtothegeometryofthedataandlatentspaces(firsttwo
columns)andofthedimensionalityreductionmodel(lastcolumn).YellowboxescorrespondtoEuclideanspace,whileorangecorresponds
to non-Euclidean space. Partially Euclidean cases are light orange. Each model is further classified by use and type of encoder/decoder,
as well as whether it computes a posterior on the latents, and whether it is Bayesian. (P)PC: (Probabilistic) Principal Curves; GP LVM:
GaussianProcessLatentVariableModel;PGA:PrincipalGeodesicAnalysis;GPCA:GeodesicPCA.15
and Bishop, 1999) achieves the same goal within developed methods for manifold data, which take
aprobabilisticframeworkrelyingonalatentvari- intoaccountthegeometricstructure;seenextrow
ablegenerativemodel,whichprovidesaposterior in the Table, described in the next paragraph.
distributiononthelatents.BayesianPCA(Bishop,
1998)additionallylearnsaprobabilitydistribution 2) Manifold Data, One-Dimensional Euclidean
on the parameters of the models, representing the Latents: Fitting a geodesic to manifold data
parameters of the linear subspace (e.g., slope and points has been performed using least-squares
intercept). These methods are restricted in the or other likelihood criteria in many domains,
type of subspace that can be fitted to the data: in particular as a first step of the extension of
only linear subspaces. To lift this restriction, we PCA to manifolds (see next paragraph). In the
also find numerous methods that learn nonlinear nongeodesic parametric decoder category, a nat-
manifolds from Euclidean data. The whole field ural extension with one-dimensional latents is to
of manifold learning fits in this case, and can be define approximating splines using higher order
subdivided into further subcategories depending polynomials that are then fitted to a set of points
onhowthelearnedmanifoldisbeingrepresented: onaRiemannianmanifold(Machadoetal.,2010,
by a nonlinear parametric decoder, by a nonpara- Machado and Leite, 2006) or on a Lie group
metric decoder, or without any decoder at all. (Gay-Balmaz et al., 2012). In the nongeodesic,
Here, we only introduce one category-defining nonparametric decoder category, principal flows
methodforeachofthesubcategoriesweconsider. (Panaretos et al., 2014) and Riemannian principal
curves (Hauberg, 2016) generalize traditional Eu-
In the nonlinear parametric decoder category, clideanprincipalcurvestoRiemannianmanifolds,
we introduce autoencoders. Autoencoders (AEs) 25yearslater.TheprobabilisticRiemannianprin-
(Rumelhart et al., 1986) learn a nonlinear sub- cipal curves (Kang and Oh, 2024) further intro-
space of a Euclidean space, while variational au- duceaprobabilisticframeworkrelyingonalatent
toencoders (VAES) (Kingma and Welling, 2014) variable generative model, hence generalizing the
achieve the same goal with a probabilistic frame- Euclidean probabilistic principal curves, 23 years
work relying on a latent variable generative later.
model.TheFull-VAEmodelproposedin(Kingma
and Welling, 2014) additionally learns a poste- 3) ManifoldData,EuclideanLatents:Wedescribe
rior on the parameters of the model, i.e., the approaches subcolumn by subcolumn. We note
parameters of the decoder. The methods in these that approaches from this configuration can be
category all leverage an encoder. In the nonpara- applied to the previous configuration, i.e., to
metric decoder category, we introduce principal one-dimensional latent spaces as well. In the
curves (Hastie and Stuetzle, 1989), which fit a geodesic decoder category, Principal Geodesic
nonlinearmanifoldtothedata,butdonotleverage Analysis (PGA) (Fletcher et al., 2004, Sommer
a posterior on the latents. In this same category, etal.,2014),tangentPGA(tPGA)(Fletcheretal.,
wealsointroduceLocalLinearEmbedding(LLE) 2004),andGeodesicPrincipalComponentAnaly-
(Roweis and Saul, 2000) and Gaussian Process sis (GPCA) (Huckemann et al., 2010). learn vari-
Latent Variable Models (GP LVM) (Lawrence, ants of geodesic subspaces, generalizing the con-
2003),whichallprovideposteriorsonthelatents. cept of a linear subspace to manifolds. As such,
Aprobabilisticapproachtoprincipalcurves(PPS) these methods represent different generalizations
developed in Chang and Ghosh (2001) also falls of PCA to manifolds. Probabilistic PGA (Zhang
under this category. Finally, we introduce the andFletcher,2013)achievesthesamegoal,while
BayesianGPLVM,whichadditionallyprovidesa adding a latent variable model generating data on
posterioronthemodel’sparameters.Wenotethat a manifold, and hence generalizes probabilistic
the methods of this category do not leverage any PCA, 16 years later. Similarly, Bayesian PGA
encoder,andinsteadcomputethelatentassociated (Zhang and Fletcher, 2013) generalizes Bayesian
to a given data point by solving an optimization PCA by including the posterior distribution of
problem. the parameters defining the submanifolds, i.e.,
the base point and tangent vectors defining the
These techniques are based on vector space op- principal (geodesic) components. However, these
erations that make them unsuitable for data on methods are restricted in the type of submanifold
manifolds. As a consequence, researchers have that can be fitted to the data, that is: geodesic16
subspaces at a point. 5) No Decoder. Few dimensionality reduction mod-
els avoid using a decoder. For this reason, we
briefly survey these models across all geomet-
The restriction to globally defined subspaces ric data and latent structures. In the Euclidean
based on geodesics can be considered both a data with Euclidean latent case (corresponding
strength and a weakness. While it protects from to row 1), we find t-distributed stochastic neigh-
theproblemofoverfittingwithasubmanifoldthat bor embedding (tSNE) (van der Maaten and
is too flexible, it also prevents the method from Hinton, 2008), Uniform Manifold Approximation
capturing possibly nonlinear effects. With current and Projection (UMAP) (McInnes et al., 2018)
dataset sizes exploding (even within biomedical and Isomap (Tenenbaum et al., 2000). These ap-
imaging datasets which have been historically proaches learn lower-dimensional representations
much smaller), the investigation of flexible sub- of data but do not provide a latent variable
manifold learning techniques may become in- generative model, nor a parameterization of the
creasingly important. recovered subspace. In the manifold data with
Euclidean latent case (corresponding to row 3), a
In the nongeodesic parametric decoder category,
flexiblegeneralizationoflinearsubspacestoman-
we consider autoencoder approaches on mani-
ifolds is given by barycentric subspaces (Pennec,
folds. Variational autoencoders have been gener-
2018)wherethesubmanifoldisdefinedimplicitly
alized to manifold data in (Miolane and Holmes,
throughgeodesicstoseveralreferencepoints.One
2020), a methodology that can be applied to
approach of interest in this line of work is to
both AEs and Full-VAEs on manifolds. This is
provide sequences of nested spaces of increasing
the only method that learns a multidimensional
dimensions that better and better approximate the
nongeodesic submanifold parameterized with a
data, a notion conceptualized by (Damon and
latentvariablemodel.Lastly,wefeatureamethod
Marron, 2013) with sequences of nested rela-
that leverages a nonparametric decoder: the Rie-
tionsofpossiblynon-geodesicsubspaces.Iterated
mannian LLE (Maignant et al., 2023), which
Frame Bundle Development (IFBD) (Sommer,
generalizesitsEuclideancounterpart,theLLE,23
2013) is another optimization method iteratively
yearslater.Wenotetheabsenceofworksperform-
building principal coordinates along new direc-
ing dimensionality reduction via a nonparametric
tions. In the Euclidean data with manifold latent
decoder in a generative model nor in a Bayesian
case (corresponding to row 4), we first find the
framework. This represents a possible avenue for
Poincare´ embeddings Nickel and Kiela (2017)
research.
that embeds data points in X into a hyperbolic
4) Euclidean Data, Manifold Latents. Wedescribe latent space Y, which is useful for representing
approaches from this configuration subcolumn by data with hierarchical structure. We note that this
subcolumn. We first find approaches that belong approach does not leverage any encoder function
to the VAE framework, where the latent space is E, and instead solves an optimization problem
a manifold, even though the data belong to a Eu- to find the latent variable y that corresponds to
i
clideanspace.Forexample,thehypersphereVAE each data point x , represented on the Figure 6
i
by Davidson et al. (2018) proposes a hyperspher- by the pictogram “!”. Second, the Riemannian
ical latent space, Falorsi et al. (2018) propose a SNE (Bergsson and Hauberg, 2022) generalizes
Lie group latent space, and Mikulski and Duda traditional Euclidean SNE (van der Maaten and
(2019)proposeatoroidallatentspace.Allofthese Hinton, 2008) but 14 years later. Finally, the last
approaches provide an (approximate, amortized) decoder-free model, called principal subbundles
posterior on the latent variables, represented as a (Akhøj et al., 2023), is the only dimensionality
probability distribution on the manifold of inter- reduction model designed for manifold data and
est.Insomesense,theseapproachesrepresentthe manifold latents. Indeed, we note that there are
counterpartofMiolaneandHolmes(2020)which no decoder-based approaches for dimensionality
considers a manifold data space and a Euclidean reduction for this case. The principal subbundles
latent space. Next, we find nonparametric de- method does not leverage either an encoder or
coder approaches, such as the manifold Gaussian decoder,butinsteadsolvesoptimizationproblems
Process Latent Variable Model (GPLVM) (Jensen to find the low-dimensional latent variable y as-
i
et al., 2020) which generalizes the GPLVM from sociatedwitheachdatapointx .Theauthorsalso
i
the Euclidean case (Lawrence, 2003).17
propose a decoding approach that uses optimiza- Taxonomy: Figure 7 organizes deep learning methods
tionratherthananexplicitdecodertofindthedata into a taxonomy based on the mathematical properties
point x that decodes a given latent variable y . of the input and output of neural network layers (first
i i
Overall, the sparsity of decoder-free dimension- two columns), as well as on the properties of the layer
ality reduction models leaves open opportunities model (third column). The rows show different types
for many choices of geometric data/latents. of layers that have been published in the literature,
organizing them by use of geometry, algebra (group
This concludes our review and categorization of non-
actions), and topology. The figure graphically contex-
Euclideangeneralizationsofmachinelearningmethods
tualizes existing methods and identifies potential areas
forregressionanddimensionalityreduction.Whilesev-
for innovation. We survey the field row by row in
eral deep neural network models—such as variational
Figure 7.
autoencoders—appear in our dimensionality reduction
Geometry in Neural Network Layers: Here, we cate-
taxonomy, we have saved a more complete treatment
gorize neural network layers that consider their inputs
of deep learning for the next section. Deep neural
and outputs as coordinates in space, where the spaces
networks stand out from more traditional machine
can be equipped with geometric structures.
learningmethodsinthattheyareflexiblecompositions
of functions that progressively transform data between 1) Euclidean input, Euclidean output: The
spaces. In our consideration of dimensionality reduc-
category-defining layer for this configuration
tion methods, we abstracted away from the transfor-
is the Perceptron layer (Rosenblatt, 1958),
mationsperformedbyindividualneuralnetworklayers
commonly used as a component in a deep neural
to consider only the structure of the input and latent
network comprised of several identical layers:
spaces(whichmaybemanylayersdeepinamodel).In
the celebrated Multi-Layer Perceptron (MLP).
thenextsection,weexplicitlyconsidertheinput-output
structure of individual neural network layers, and the 2) Euclidean input, manifold output: The
ways in which topology, geometry, and algebra have Perceptron-Exp layer from Miolane and Holmes
been incorporated into these layers. (2020) generalizes the Perceptron layer to layer
outputs on manifolds. Here, Exp denotes the
Riemannian exponential map, which is applied
V. SURVEY:NON-EUCLIDEANDEEPLEARNING to the result of the Perceptron. Indeed, Exp is an
operationthatmapstangentvectorstopointsona
Wenowreviewnon-Euclideanstructuresindeeplearn- manifold, i.e., that maps an input in an Euclidean
ing, particularly focusing on how geometry, topology, space to an output on the manifold. Only the
and algebra can enrich the structure of a given layer Perceptron component of this layer has learnable
within a deep neural network. A neural network layer weights; the manifold needs to be known a priori
isafunctionf :X →Y,andthuscanbeanalyzedand in order to specifyand implement the appropriate
categorized in terms of the mathematical structure of exponential map. Additionally, in general, there
the input space X and output space Y. We first cover is no analytical expression for the Exp map,
neuralnetworklayerswithoutanattentionmechanism, which needs to be computed numerically. To
followed by layers with attention. We then compile a avoid this computational cost, this layer is best
list of datasets and benchmarks used in the literature implemented for manifolds whose Exp enjoys an
to compare deep learning methods with geometric, analytical expression.
topological, or algebraic properties.
3) Manifold input, Euclidean output: The Log-
Perceptron layer from (Davidson et al., 2018)
generalizes the Perceptron layer to layer inputs
A. Neural Network Layers Without Attention
on manifolds. Here, Log denotes the Riemannian
We categorize layers from neural networks according logarithm map, which is applied just before the
to their mathematical structure. A given paper can Perceptron.Indeed,Logisanoperationthatmaps
appear several times in this subsection (and the next), pointsonmanifoldstotangentvectors;thatis,itis
if this paper introduces several novel layers. Neural theinverseoftheExp.Thelayermapsaninputon
networks using the attention mechanism are out of amanifoldtoanoutputinEuclideanspacewhere
scope for this subsection and will be presented in the the Perceptron with its learnable weights can be
next subsection. applied. This layer can thus be thought of as the
inverseofthePerceptron-ExplayerfromMiolane18
‚(cid:24)(cid:12)(cid:24)(cid:21)(cid:24)(cid:28)€•(cid:16)(cid:31)(cid:27)(cid:24)(cid:2)(cid:27)(cid:20)(cid:13)€•(cid:16)(cid:30)(cid:23)(cid:15)(cid:16)ƒ(cid:21)(cid:28)(cid:27)(cid:1)(cid:13)(cid:30)(cid:16)(cid:19)(cid:23)(cid:16)(cid:157)(cid:27)(cid:29)(cid:13)(cid:30)(cid:21)(cid:16)(cid:157)(cid:27)(cid:20)(cid:18)(cid:24)(cid:13) (cid:16) (cid:30)€(cid:27)(cid:13)(cid:11)
(cid:30)€(cid:27)(cid:13)(cid:16)‹(cid:23)(cid:12)(cid:29)(cid:20) (cid:30)€(cid:27)(cid:13)(cid:16)Š(cid:29)(cid:20)(cid:12)(cid:29)(cid:20) (cid:30)€(cid:27)(cid:13)(cid:16)(cid:3)(cid:24)(cid:15)(cid:27)(cid:21) (cid:30)€(cid:27)(cid:13)(cid:16)‹(cid:23)(cid:12)(cid:29)(cid:20) (cid:30)€(cid:27)(cid:13)(cid:16)Š(cid:29)(cid:20)(cid:12)(cid:29)(cid:20) (cid:30)€(cid:27)(cid:13)(cid:16)(cid:3)(cid:24)(cid:15)(cid:27)(cid:21)
‘(cid:27)(cid:28)(cid:29)(cid:21)(cid:30)(cid:13)
(cid:9) (cid:9)(cid:141) (cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)(cid:22)(cid:24)(cid:21)(cid:29)(cid:20)(cid:19)(cid:24)(cid:23)
(cid:10)(cid:25)(cid:24)(cid:17)(cid:27)(cid:23)(cid:9)(cid:8)(cid:30)(cid:7)
…(cid:27)(cid:13)(cid:14)(cid:27)(cid:12)(cid:20)(cid:13)(cid:24)(cid:23)
(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)(cid:22)(cid:24)(cid:21)(cid:29)(cid:20)(cid:19)(cid:24)(cid:23)
(cid:18)(cid:19)(cid:20)(cid:17)(cid:16)(cid:19)(cid:23)(cid:15)(cid:29)(cid:14)(cid:27)(cid:15)
(cid:141) …(cid:27)(cid:13)(cid:14)(cid:27)(cid:12)(cid:20)(cid:13)(cid:24)(cid:23)(cid:26)(cid:143)˜(cid:12)(cid:16)(cid:5)(cid:13)(cid:24)(cid:2) (cid:9)’ (cid:13)(cid:27)(cid:12)(cid:13)(cid:27)(cid:11)(cid:27)(cid:23)(cid:20)(cid:30)(cid:20)(cid:19)(cid:24)(cid:23)
‘(cid:19)(cid:27)(cid:2)(cid:30)(cid:23)(cid:23)(cid:19)(cid:30)(cid:23)(cid:16)–ƒ(cid:143) (cid:10)(cid:25)(cid:24)(cid:17)(cid:27)(cid:23)(cid:9)(cid:8)(cid:30)(cid:7)
(cid:10)(cid:3)(cid:19)(cid:24)(cid:21)(cid:30)(cid:23)(cid:27)(cid:141)„(cid:7)
…(cid:24)(cid:19)(cid:23)(cid:20)(cid:157)(cid:27)(cid:20)††
’ (cid:24)(cid:28)(cid:26)…(cid:27)(cid:13)(cid:14)(cid:27)(cid:12)(cid:20)(cid:13)(cid:24)(cid:23)(cid:16)(cid:5)(cid:13)(cid:24)(cid:2) (cid:9)Ž (cid:10)‡(cid:19)(cid:9)(cid:4)(cid:7)
(cid:127)€(cid:12)(cid:27)(cid:13)(cid:11)(cid:12)(cid:17)(cid:27)(cid:13)(cid:27)(cid:16)–ƒ(cid:143)
(cid:10)—(cid:30)(cid:22)(cid:19)(cid:15)(cid:11)(cid:24)(cid:23)(cid:9)‰(cid:7)
‚(cid:27)(cid:23)(cid:11)(cid:24)(cid:13)(cid:16)ˆ(cid:19)(cid:27)(cid:21)(cid:15)(cid:16)(cid:157)(cid:27)(cid:20)(cid:18)(cid:24)(cid:13) (cid:11)(cid:16)
(cid:10)‚(cid:17)(cid:24)(cid:2)(cid:30)(cid:11)(cid:9)‰(cid:7)
Ž Œ(cid:19)(cid:3) (cid:10)(cid:30) (cid:127)(cid:12) (cid:29)(cid:16) (cid:30)(cid:21)(cid:30) (cid:23)€ (cid:28)(cid:27) (cid:9)(cid:13)(cid:16) ”(cid:5)(cid:13) (cid:7)(cid:24)(cid:2) (cid:9)“ …Š(cid:157)‹‚ƒ
(cid:3)(cid:30)(cid:23)(cid:19)(cid:5)(cid:24)(cid:21)(cid:15)(cid:157)(cid:27)(cid:20)(cid:16)(cid:21)(cid:30)€(cid:27)(cid:13)(cid:16)(cid:5)(cid:13)(cid:24)(cid:2) (cid:10)Œ(cid:27) (cid:27)(cid:13)(cid:11)(cid:141)Ž(cid:7)
(cid:10)(cid:25)(cid:17)(cid:30) (cid:13)(cid:30)(cid:1)(cid:24)(cid:13)(cid:20)€(cid:141)(cid:141)(cid:7)
(cid:31)(cid:13)(cid:30)(cid:12)(cid:17)(cid:16)(cid:14)(cid:24)(cid:23)(cid:22)(cid:24)(cid:21)(cid:29)(cid:20)(cid:19)(cid:24)(cid:23)
(cid:10)(cid:6)(cid:19)(cid:12)(cid:5)(cid:9)(cid:4)(cid:7)
“ (cid:143)(cid:144)(cid:29)(cid:19)(cid:22)(cid:30)(cid:13)(cid:19)(cid:30)(cid:23)(cid:20)(cid:26)…(cid:27)(cid:13)(cid:14)(cid:27)(cid:12)(cid:20)(cid:13)(cid:24)(cid:23) (cid:9)” (cid:3)(cid:27)(cid:11)(cid:11)(cid:30)(cid:28)(cid:27)(cid:16)(cid:12)(cid:30)(cid:11)(cid:11)(cid:19)(cid:23)(cid:28)
(cid:5)(cid:13)(cid:24)(cid:2)(cid:16)(cid:10)ˆ(cid:19)(cid:23)š(cid:19)(cid:141)(cid:9)(cid:7)
(cid:19)(cid:23)(cid:27)(cid:30)(cid:13)(cid:16)(cid:21)(cid:30)€(cid:27)(cid:13)(cid:16)(cid:5)(cid:13)(cid:24)(cid:2)(cid:16)(cid:31)ƒ‚(cid:13) (cid:10)(cid:31)(cid:19)(cid:21)(cid:2)(cid:27)(cid:13)(cid:9)(cid:4)(cid:7)
(cid:10)Œ(cid:13)(cid:27)(cid:17)(cid:2)(cid:27)(cid:13)(cid:141)’(cid:7)
(cid:25)(cid:24)(cid:23)(cid:22)(cid:24)(cid:21)(cid:29)(cid:20)(cid:19)(cid:24)(cid:23)(cid:30)(cid:21)(cid:16)
(cid:10) (cid:27)(cid:14)(cid:29)(cid:23)(cid:7)
(cid:143)›(cid:23)ž(cid:26)(cid:143)(cid:31)(cid:157)(cid:157)(cid:16)
” ‘(cid:27)(cid:28)(cid:29)(cid:21)(cid:30)(cid:13)(cid:16)™(cid:20)(cid:27)(cid:27)(cid:13)(cid:30)(cid:1)(cid:21)(cid:27)(cid:16) (cid:9)(cid:4)
(cid:10)™(cid:30)(cid:20)(cid:24)(cid:13)(cid:13)(cid:30)(cid:11)(cid:141)„(cid:141)(cid:9)(cid:7)
(cid:143)(cid:29)(cid:14)(cid:21)(cid:19)(cid:15)(cid:27)(cid:30)(cid:23)(cid:16)
(cid:10)(cid:25)(cid:24)(cid:17)(cid:27)(cid:23)(cid:9)(cid:4)(cid:7)
™(cid:19)(cid:2)(cid:12)(cid:21)(cid:19)(cid:14)(cid:19)(cid:30)(cid:21)(cid:16)(cid:30)(cid:23)(cid:15)(cid:16)(cid:14)(cid:27)(cid:21)(cid:21)(cid:29)(cid:21)(cid:30)(cid:13)
(cid:14)(cid:24)(cid:23)(cid:22)(cid:24)(cid:21)(cid:29)(cid:20)(cid:19)(cid:24)(cid:23)
™(cid:20)(cid:27)(cid:27)(cid:13)(cid:30)(cid:1)(cid:21)(cid:27)(cid:16)(cid:25)(cid:24)(cid:23)(cid:22)(cid:24)(cid:21)(cid:29)(cid:20)(cid:19)(cid:24)(cid:23)(cid:30)(cid:21) (cid:10)(cid:143)(cid:1)(cid:21)(cid:19)(cid:141)„•(cid:16)‘(cid:24)(cid:15)(cid:15)(cid:27)(cid:23)(cid:1)(cid:27)(cid:13)(cid:13)€(cid:141)(cid:141)(cid:7) (cid:4) (cid:9)‰
(cid:10)(cid:25)(cid:24)(cid:17)(cid:27)(cid:23)(cid:9)(cid:4)(cid:7) ™(cid:19)(cid:2)(cid:12)(cid:21)(cid:19)(cid:14)(cid:19)(cid:30)(cid:21)(cid:16)(cid:30)(cid:23)(cid:15)(cid:16)(cid:14)(cid:27)(cid:21)(cid:21)(cid:29)(cid:21)(cid:30)(cid:13)
(cid:2)(cid:27)(cid:11)(cid:11)(cid:30)(cid:28)(cid:27)(cid:26)(cid:12)(cid:30)(cid:11)(cid:11)(cid:19)(cid:23)(cid:28)
(cid:10)Œ(cid:24)(cid:15)(cid:23)(cid:30)(cid:13)(cid:141)(cid:9)•(cid:16)(cid:127)(cid:30)(cid:129)(cid:19)(cid:129)(cid:141)„(cid:7)
‘(cid:27)(cid:28)(cid:29)(cid:21)(cid:30)(cid:13)(cid:16) (cid:143)›(cid:23)ž(cid:26)(cid:143)(cid:3)…™(cid:157)(cid:16)
(cid:31)(cid:13)(cid:24)(cid:29)(cid:12)(cid:26)(cid:25)(cid:24)(cid:23)(cid:22)(cid:24)(cid:21)(cid:29)(cid:20)(cid:19)(cid:24)(cid:23) (cid:10)(cid:143)(cid:19)(cid:129) (cid:27)(cid:21)(cid:1)(cid:24)(cid:24)(cid:2)(cid:141)„(cid:141)’(cid:7)
‰ ›œ(cid:13)(cid:11)(cid:20)(cid:16)(cid:21)(cid:30)€(cid:27)(cid:13)ž(cid:16) (cid:9)(cid:8) (cid:25)(cid:21)(cid:19)Ÿ(cid:24)(cid:13)(cid:15)(cid:26)(cid:143)(cid:3)…™(cid:157)
(cid:10)(cid:25)(cid:24)(cid:17)(cid:27)(cid:23)(cid:9)”(cid:7) (cid:10) (cid:19)(cid:29)(cid:141)„(cid:141)Ž(cid:7)
‘(cid:27)(cid:28)(cid:29)(cid:21)(cid:30)(cid:13)(cid:16) (cid:127)€(cid:12)(cid:27)(cid:13)(cid:28)(cid:13)(cid:30)(cid:12)(cid:17)(cid:16)(cid:14)(cid:24)(cid:23)(cid:22)(cid:24)(cid:21)(cid:29)(cid:20)(cid:19)(cid:24)(cid:23)
(cid:8) (cid:31)(cid:13)(cid:24)(cid:29)(cid:12)(cid:16)(cid:25)(cid:24)(cid:23)(cid:22)(cid:24)(cid:21)(cid:29)(cid:20)(cid:19)(cid:24)(cid:23) (cid:141)„ (cid:10)ƒ(cid:13)€(cid:30)(cid:141)„(cid:7)
›(cid:11)(cid:27)(cid:14)(cid:24)(cid:23)(cid:15)(cid:16)(cid:21)(cid:30)€(cid:27)(cid:13)ž(cid:16) (cid:3)(cid:27)(cid:11)(cid:11)(cid:30)(cid:28)(cid:27)(cid:16)(cid:12)(cid:30)(cid:11)(cid:11)(cid:19)(cid:23)(cid:28)
(cid:10)(cid:25)(cid:24)(cid:17)(cid:27)(cid:23)(cid:9)”(cid:7) (cid:10)(cid:127)(cid:27)€(cid:15)(cid:30)(cid:13)(cid:19)(cid:141)(cid:141)(cid:7)
(cid:25)(cid:24)(cid:2)(cid:1)(cid:19)(cid:23)(cid:30)(cid:20)(cid:24)(cid:13)(cid:19)(cid:30)(cid:21)(cid:16)
(cid:31)(cid:27)(cid:23)(cid:27)(cid:13)(cid:30)(cid:21)(cid:16)™(cid:20)(cid:27)(cid:27)(cid:13)(cid:30)(cid:1)(cid:21)(cid:27)
(cid:14)(cid:24)(cid:23)(cid:22)(cid:24)(cid:21)(cid:29)(cid:20)(cid:19)(cid:24)(cid:23)
(cid:18)(cid:19)(cid:20)(cid:17)(cid:16)(cid:144)(cid:29)(cid:24)(cid:20)(cid:19)(cid:27)(cid:23)(cid:20)
(cid:9)„ (cid:13)(cid:27)(cid:12)(cid:13)(cid:27)(cid:11)(cid:27)(cid:23)(cid:20)(cid:30)(cid:20)(cid:19)(cid:24)(cid:23) (cid:141)(cid:9) (cid:10)(cid:127)(cid:30)(cid:129)(cid:19)(cid:129)(cid:141)(cid:141)(cid:7)
(cid:3)(cid:27)(cid:11)(cid:11)(cid:30)(cid:28)(cid:27)(cid:16)(cid:12)(cid:30)(cid:11)(cid:11)(cid:19)(cid:23)(cid:28)
(cid:10)(cid:25)(cid:24)(cid:17)(cid:27)(cid:23)(cid:9)(cid:4)(cid:7)
(cid:10)(cid:127)(cid:30)(cid:129)(cid:19)(cid:129)(cid:141)(cid:141)(cid:7)
(cid:31)(cid:13)(cid:24)(cid:29)(cid:12)(cid:16)(cid:25)(cid:24)(cid:23)(cid:22)(cid:24)(cid:21)(cid:29)(cid:20)(cid:19)(cid:24)(cid:23)
(cid:9)(cid:9) (cid:24)(cid:23)(cid:16)(cid:17)(cid:24)(cid:2)(cid:24)(cid:28)(cid:27)(cid:23)(cid:27)(cid:24)(cid:29)(cid:11) (cid:143)›(cid:23)ž(cid:26)(cid:143)(cid:144)(cid:29)(cid:19)(cid:22)(cid:30)(cid:13)(cid:19)(cid:30)(cid:23)(cid:20)(cid:16)‚(cid:157)(cid:157)
(cid:11)(cid:12)(cid:30)(cid:14)(cid:27)(cid:11)(cid:16) (cid:141)(cid:141) (cid:10)Œ(cid:30)(cid:20)(cid:20)(cid:19)(cid:21)(cid:24)(cid:13)(cid:24)(cid:141)Ž(cid:7)
(cid:10)(cid:25)(cid:24)(cid:17)(cid:27)(cid:23)(cid:9)(cid:8)(cid:1)(cid:7)
Fig. 7: Topology, Geometry, and Algebra in Neural Network Layers organized according to the mathematical properties of the layer
inputs x and outputs y (first two columns) and of the layer model (third column). The first five rows show inputs and outputs that are
datarepresentedascoordinatesinaspace,typically,aEuclideanspaceRn oramanifoldM.Thenextrowsshowinputsandoutputsthat
aresignals,i.e.,functionsfromadomaintoacodomain.Theblackcurvedarrowsrepresentagroupactiononaspace,suchasasignal’s
domain or codomain. Notations: Rn: Euclidean space; M: Manifold; Gp: group, Gp/H Homogeneous manifold for the group Gp; P:
pointset;G:graph;Ω:topologicalspace.
(cid:11)(cid:13)(cid:27)€(cid:30)
(cid:16)
(cid:13)(cid:24)(cid:18)(cid:20)(cid:27)(cid:157)(cid:16)(cid:21)(cid:30)(cid:13)(cid:29)(cid:27)(cid:157)(cid:16)(cid:23)(cid:19)(cid:16)€(cid:13)(cid:20)(cid:27)(cid:2)(cid:24)(cid:27)(cid:31)
(cid:11)(cid:13)(cid:27)€(cid:30)
(cid:16)
(cid:13)(cid:24)(cid:18)(cid:20)(cid:27)(cid:157)(cid:16)(cid:21)(cid:30)(cid:13)(cid:29)(cid:27)(cid:157)(cid:16)(cid:23)(cid:19)(cid:16)(cid:27)(cid:14)(cid:23)(cid:30)(cid:19)(cid:13)(cid:30)(cid:22)(cid:19)(cid:29)(cid:144)(cid:143)
(cid:11)(cid:13)(cid:27)€(cid:30)
(cid:16)
(cid:13)(cid:24)(cid:18)(cid:20)(cid:27)(cid:157)(cid:16)(cid:21)(cid:30)(cid:13)(cid:29)(cid:27)(cid:157)(cid:16)(cid:23)(cid:19)(cid:16)€(cid:28)(cid:24)(cid:21)(cid:24)(cid:12)(cid:24)‚19
and Holmes (2020). Just as for the Perceptron- et al., 2021) and the linear layer in the Geomet-
Exp, the manifold needs to be known for this ric Algebra Transformer (GATr) (Brehmer et al.,
layer to be implemented, and manifolds whose 2023) illustrate this configuration. They represent
Logenjoysananalyticalexpressionarepreferred. the only equivariant layers processing inputs and
outputs that are coordinates in space, as opposed
4) Manifold input, manifold output: This config-
to signals over a space. The work by (Finzi
uration is first illustrated with the Bimap and
et al., 2021) allows us to build an Equivariant-
SPDNetlayersfrom(HuangandVanGool,2016),
Perceptron layer given any matrix group action
where SPD refer to symmetric positive definite
on the input and output spaces.
matrices. This layer is indeed restricted to the
manifoldsofSPDmatrices.Moregenerally,Man- 6) Input and output: Euclidean signal on Eu-
ifoldNet (Chakraborty et al., 2022) builds on the clidean domain with group action. While the
reformulationofconvolutionsasweightedFre´chet previous row focused on data as coordinates in
means of Pennec et al. (2006) to propose a layer space, this row considers data as signals. Indeed,
whose inputs and outputs are both coordinates on both inputs and outputs are defined by function
a manifold. In this layer, a weighted mean of the x : Rn (cid:55)→ Rn′ and y : Rm (cid:55)→ Rm′. For
inputsiscomputed,wheretheweightsarelearned example, the input and output could be images,
with classical back propagation. Since convolu- orfeaturemaps,definedoverthedomainsRn and
tions are equivalent to computing weighted sums, Rm respectively. The classic convolutional layer
this layer has also been termed the manifold- (LeCun et al., 1998) is a layer with translation
valued data convolution. We note that, in general, equivariance: a translation of the input image
there is no analytical expression for the weighted x yields a translated feature map y as output.
Fre´chet mean, which needs to be obtained by Regularsteerableconvolutionallayers(Cohenand
optimization. To avoid this computational cost, Welling, 2017) generalize this approach to more
approximationsusingtangentmeansarealsocon- complicated cases of equivariances, using groups
sidered. beyond the group of translations. In this row, the
group action is only on the domain of the input
AlgebrainNeuralNetworkLayers: Here,weconsider
and output signals: this is the meaning of the ad-
neural network layers that fall into the category of
jective“regular”inregularsteerableconvolutions.
equivariant deep learning, which leverages the con-
cepts of symmetries and group actions. We refer the
7) Inputandoutput:Euclideansignalwithgroup
reader to Cohen et al. (2021) and Weiler et al. (2024)
actiononEuclideandomainwithgroupaction.
forextensivetreatmentsofthefoundationsofthisfield.
Similar to the previous row, both inputs and
Acoreideaunderlyingthislineofworkistobuildthe outputsaresignalsoveraspace:x,y :Rn (cid:55)→Rn′.
symmetries natural to a data domain (e.g. translational
By contrast to the previous row, however, both
androtationalsymmetriesforobjectclassification)into
the domain and the codomain of each signal
the structure of the model. This facilitates weight
are equipped with a group action. The Steerable
sharing across different transformations so that the
Convolutionallayer(CohenandWelling,2017)il-
same convolutional filter can be used to detect a given
lustratesthisconfiguration.Comparedtotheother
feature in an image in, for example, all orientations.
layer from the same paper presented above, the
To accomplish this, the input and output spaces of
group action can affect the domain and codomain
these layers are equipped with group actions, and
simultaneously.
the equations defining the layer are written to be
compatible with these actions. Intuitively, if a layer 8) Input: Euclidean signal on Euclidean domain
is equivariant to a group action, this means that, if withgroupaction;output:Euclideansignalon
the layer produces an output y for a given input x, manifold with group action. This row presents
then it should also produce a transformed y for any layers whose input can be described as a sig-
transformed x—where transformed here means “acted nal x : Rn (cid:55)→ Rn′, typically an image, and
upon by the same group element.” Here, we highlight whose output is a more complicated signal of the
key examples of different equivariant layers that fall form y : G (cid:55)→ Rn′, i.e., defined over a Lie
p
into each category of our taxonomy. group G —where we recall that a Lie group is
p
a manifold that is also a group. The first layer
5) Inputandoutput:Euclideanwithgroupaction. of the Group Convolutional Network by (Cohen
The Equivariant-Perceptron layer from (Finzi and Welling, 2016) illustrates this configuration.20
In this network, the second layer necessarily in- actiononmanifold.Thisrowgeneralizesthepre-
puts the output of the first layer, i.e., inputs a viousrow,whereanactioncantransformelements
signal defined over a Lie group. Consequently, of the codomains of the signals x,y : M (cid:55)→ Rn.
the second (and following) layers of this neural An example of this category is the general gauge
network are presented in the next row. convolutional layer with induced representation
(Cohen et al., 2019a).
9) Input and output: Euclidean signal on mani-
fold with group action. Here the manifold do- TopologyinNeuralNetworkLayers: Here,wecatego-
mains of both the input and output signals are rizeneuralnetworklayersthatconsidertheirinputsand
equippedwithagroupaction.Thesecondlayerof outputs as signals over a topological domain. We refer
the Group Convolutional Network by (Cohen and the reader to the survey by Papillon et al. (2023) for a
Welling,2016)showcasesthisexample,wherethe comprehensive overview of the neural network layers
manifold is in fact a Lie group, and the group within this category, and we focus only on selected
action is the group composition. illustrative examples. These layers are all equivariant
to the permutation of the elements of their topological
10) Input and output: Euclidean signal on ho-
domains: permutation of the points in a set, of the
mogeneous manifold with group action. This
nodes in a graph, etc. As such, all these layers are,
row presents layers processing inputs and out-
at a minimum, equipped with group action on their
puts represented as signals of the form x,y :
domain.
G /H (cid:55)→ Rn. Here, the domain G /H de-
p p
fines a so-called homogeneous manifold, that is: 14) Input and output: Euclidean signal on a set,
a manifold equipped with a group action that withdomaingroupaction.LayersinPointNet++
is such that, for every pair of points on the (Qietal.,2017)processpointclouds,whichhave
manifold, there exists a group element that can signaloftheformx,y :P (cid:55)→RnwhereP isaset.
transform one point onto the other via the action. In this work, the codomain is restricted to the 2D
The General Steerable Convolutional Layer with or 3D Euclidean spaces R2, R3 as it is designed
the so-called quotient representation (Cohen and to process point clouds in 2D or 3D. Thanks to
Welling, 2017) falls in this category. its domain group action, the model is equivariant
to a permutation in P. However, PointNet++
11) Inputandoutput:Euclideansignalwithgroup
is only approximately invariant to 3D rotations
action on homogeneous manifold with group and translations operating in the codomain R3.
action. Similar to the previous row, signals are
Namely,itleveragesapreprocessingstepthatfirst
represented by functions x,y : G /H (cid:55)→ Rn,
p aligns point clouds in position and orientation.
i.e., with an homogeneous manifold domain with
group action. By contrast to the previous row, 15) Input and output: Euclidean signal on a set,
however, there is also a group action on the with domain and codomain group actions.
Euclidean codomains of both the input and the Here, the input and output signals are point
output signals. The group convolutional layer for clouds, just like the row above. Layers in Tensor
homogeneous spaces by (Cohen et al., 2019b) Field Networks (TFN) (Thomas et al., 2018), and
falls into this category, as well as the General PONITA (Bekkers et al., 2024) process signals in
Steerable layer. thiscategory.LikePointNet++,TFNandPONITA
process2Dand3Dpointcloudsandareequivari-
12) Input and output: Euclidean signal on mani-
ant to the permutation of their points in P. TFN
fold.Inthisrow,theinputandoutputsignalshave
and PONITA are additionally equivariant to 3D
manifolddomains,andEuclideancodomains.Sig- rotations and translations in R3.
nals are thus represented as x,y : M (cid:55)→ Rn.
The channel-wise gauge convolutional layer by 16) Input and output: Euclidean signal on graph,
(Cohen et al., 2019a) processes this type of sig- with domain group action. Layers in this row
nals. Here, the concept of gauge equivariance process signals of the form x,y : G (cid:55)→ Rn,
replaces the group equivariance. Gauge equivari- i.e., defined over a graph G. This category is
ance describes the idea of being agnostic to the exemplified by Graph Convolutional layer (Kipf
orientation of a local coordinate systems on the and Welling, 2017) and Message Passing layers
manifold of interest M. (Gilmer et al., 2017).
13) Inputandoutput:Euclideansignalwithgroup 17) Input and output: Euclidean signal on graph,21
with domain and codomain group action. Lay- row, the Euclidean codomain is equipped with a
ersinthisrowaresimilartolayersintheprevious group action. The E(n)-equivariant topological
row, except that they add a group action on neural networks from (Battiloro et al., 2024) is
the codomain of the signals. We find, for exam- an example of this configuration. The layers of
ple, the E(n-Equivariant Graph Neural Networks this network can process geometric features in
(EGNN) by (Satorras et al., 2021). the codomain Rn, such as velocities or positions
associated with the elements of the topological
18) Input and output: Euclidean signal on cellular
domain Ω.
complex, with domain group action. This row
considers signals defined over a cellular complex
Ω, i.e., functions of the form x,y : Ω (cid:55)→ Rn. B. Neural Network Layers with Attention
Examples include simplicial convolutional (Ebli We now review mathematical structures in neural net-
et al., 2020), cellular convolutional (Roddenberry work layers that leverage the attention mechanism.
et al., 2022), simplicial message passing (Bodnar In particular, we focus on how geometry, topology,
et al., 2021), and cellular message passing (Hajij and algebra can enrich the structure of the attention
et al., 2020) layers. coefficients and the attention layers.
19) Input and output: Euclidean signal on cellular
Taxonomy: Figure 8 organizes attention coefficients
complex, with domain and codomain group
and layers into a taxonomy based on the mathemat-
actions. Layers in this row are similar to layers
ical properties of their inputs and outputs. First, the
in the previous row, except that they add a group
attention coefficient α is computed from a query q
actiononthecodomainofthesignals.Wefind,for
and a key k. Hence, we examine the structure of the
example, the E(n)-Equivariant Message Passing
key k and the query q inputs, represented as signals
SimplicialNetworks(EMPSN)(Eijkelboometal.,
overadomain.Forexample,thetraditionaltransformer
2023)andtheCliffordgroupEquivariantMessage
considers keys and queries as functions over a one-
PassingSimplicialNetworks(EMPSN)(Liuetal., dimensional domain R representing time. The output
2024) which both introduce group actions on the
attention coefficient α is represented as a signal over
codomain, but for different groups.
the product domain: in the transformer example, it is
a function over the product domain R×R. Second,
20) Input and output: Euclidean signal on hyper-
the attention layer transforms input values v to output
graph, with domain group action. This row
turns to signals x,y : Ω (cid:55)→ Rn, where Ω now values v′ through the attention coefficients α. Hence,
we look at the mathematical properties of v and v′,
denotes a hypergraph, which is another type of
both represented as signals over a domain. In the
topological space. This configuration is exempli-
classical transformers, they are signals over the time
fied by Hypergraph Convolutional layer (Arya
domain R.
et al., 2020) and Hypergraph Message Passing
layer (Heydari and Livi, 2022). InFigure8,wehighlightthepropertiesofthedomains
and codomains of the key k, query q, coefficient
21) Input and output: Euclidean signal on com-
α, input value v and output value v′ signals. This
binatorial complex, with domain group ac-
graphical taxonomy helps readers understand where
tion. This row considers signals with the most
current methods fit within our framework and identify
general type of topological domain, the com-
potential areas for innovation. We survey the field row
binatorial complex Ω, i.e., signals of the form
x,y : Ω (cid:55)→ Rn. Examples of neural network by row in Figure 8.
layers that process this type of signals are the Geometry in Attention Mechanisms.:
Combinatorial Convolutions (Hajij et al., 2023b)
1) Euclidean signal on Euclidean domain. This
and Combinatorial Message Passing (Hajij et al.,
row illustrates the classical transformer (Vaswani
2023b).
et al., 2017), with key, query, and input values
22) Input and output: Euclidean signal on combi- representedassignalsk,q,v :R(cid:55)→Rn.Attention
natorial complex, with domain and codomain coefficients are α : R2 (cid:55)→ R, while the layer
group action. Similar to the previous row, this output values are v′ : R (cid:55)→ Rn. This row
lastconfigurationprocesssignalsdefinedoncom- also illustrates the setting for the Vision Trans-
binatorial complex domain Ω with Euclidean former (Dosovitskiy et al., 2021), which divides
codomain.However,bycontrastwiththeprevious animageintoasequenceofimagepatches:hence,22
(cid:17)(cid:18)(cid:23)(cid:18)(cid:16)(cid:18)(cid:15)(cid:29)(cid:14)(cid:26)(cid:13)(cid:28)(cid:18)(cid:12)(cid:28)(cid:21)(cid:27)(cid:29)(cid:14)(cid:26)(cid:30)(cid:24)(cid:11)(cid:26)(cid:10)(cid:16)(cid:15)(cid:28)(cid:9)(cid:27)(cid:30)(cid:26)(cid:19)(cid:24)(cid:26)(cid:10)(cid:21)(cid:21)(cid:28)(cid:24)(cid:21)(cid:19)(cid:18)(cid:24)(cid:26)(cid:8)(cid:28)(cid:7)(cid:6)(cid:30)(cid:24)(cid:19)(cid:5)(cid:12)(cid:5)
(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)(cid:22)(cid:21) (cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:20)(cid:22)(cid:21)(cid:23)(cid:22)(cid:21) (cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:20)(cid:23)(cid:28)(cid:27)(cid:30)(cid:21)(cid:19)(cid:18)(cid:24) (cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)(cid:22)(cid:21) (cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:20)(cid:22)(cid:21)(cid:23)(cid:22)(cid:21) (cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:20)(cid:23)(cid:28)(cid:27)(cid:30)(cid:21)(cid:19)(cid:18)(cid:24)
(cid:17)(cid:27)(cid:30)(cid:24)(cid:5)(cid:129)(cid:18)(cid:27)(cid:12)(cid:28)(cid:27)
(cid:141) (cid:30)(cid:5)™(cid:30)(cid:24)(cid:19)„” (cid:13)(cid:28)(cid:18)(cid:11)(cid:28)(cid:5)(cid:19)(cid:7)
„ ” (cid:141)(cid:31)(cid:19)(cid:144)(cid:144)
(cid:19)(cid:5)(cid:19)(cid:18)(cid:24)(cid:26)(cid:17)(cid:27)(cid:30)(cid:24)(cid:5)(cid:129)(cid:18)(cid:27)(cid:12)(cid:28)(cid:27)
(cid:141)€(cid:18)(cid:5)(cid:18)‚(cid:19)(cid:21)(cid:5)ƒ(cid:19)(cid:29)(cid:144)„
(cid:8)(cid:22)(cid:16)(cid:21)(cid:19)(cid:127)(cid:8)(cid:30)(cid:24)(cid:19)(cid:129)(cid:18)(cid:16)(cid:11) (cid:13)(cid:10)(cid:17)
(cid:144) (cid:10)(cid:21)(cid:21)(cid:28)(cid:24)(cid:21)(cid:19)(cid:18)(cid:24) • (cid:141) (cid:28)(cid:16)(cid:19)čƒ(cid:18)‚(cid:19)ć„•
(cid:141)€(cid:19)(cid:12)(cid:19)(cid:21)(cid:27)(cid:19)(cid:18)(cid:5)(cid:144)†
(cid:4)(cid:3)(cid:2)(cid:24)(cid:1)(cid:127)(cid:4)(cid:21)(cid:28)(cid:28)(cid:27)(cid:30)(cid:9)(cid:16)(cid:28)
(cid:17)(cid:27)(cid:30)(cid:24)(cid:5)(cid:129)(cid:18)(cid:27)(cid:12)(cid:28)(cid:27)
(cid:141)(cid:143)(cid:22)(cid:24)(cid:11)(cid:22)(cid:144)(cid:157) (cid:4)(cid:3)(cid:2)†(cid:1)(cid:26)(cid:17)(cid:27)(cid:30)(cid:24)(cid:5)(cid:129)(cid:18)(cid:27)(cid:12)(cid:28)(cid:27)
† ˆ (cid:141)˜(cid:22)(cid:7)(cid:6)(cid:5)(cid:144)–
(cid:13)(cid:28)(cid:18)(cid:12)(cid:28)(cid:21)(cid:27)(cid:19)(cid:7)(cid:26)(cid:10)(cid:16)(cid:15)(cid:28)(cid:9)(cid:27)(cid:30)
(cid:17)(cid:27)(cid:30)(cid:24)(cid:5)(cid:129)(cid:18)(cid:27)(cid:12)(cid:28)(cid:27)
(cid:141)…(cid:27)(cid:28)(cid:6)(cid:12)(cid:28)(cid:27)(cid:144)†
Š(cid:10)Œ(cid:26)
(cid:141)(cid:13)(cid:19)(cid:22)(cid:5)(cid:21)(cid:19)(cid:144)(cid:144)
(cid:31)(cid:19)(cid:28)(cid:17)(cid:27)(cid:30)(cid:24)(cid:5)(cid:129)(cid:18)(cid:27)(cid:12)(cid:28)(cid:27) (cid:4)(cid:13)(cid:10)(cid:17)
(cid:157) (cid:141)‡(cid:22)(cid:21)(cid:7)(cid:6)(cid:19)(cid:24)(cid:5)(cid:18)(cid:24)(cid:144)„ „– (cid:141)(cid:31)(cid:28)(cid:28)(cid:144)(cid:144)
—‡(cid:28)(cid:27)(cid:28)
Š(cid:28)(cid:16)(cid:16)(cid:22)(cid:16)(cid:30)(cid:27)(cid:26)(cid:17)(cid:27)(cid:30)(cid:24)(cid:5)(cid:129)(cid:18)(cid:27)(cid:12)(cid:28)(cid:27)
(cid:141)…(cid:30)(cid:16)(cid:16)(cid:28)(cid:5)(cid:21)(cid:28)(cid:27)(cid:144)(cid:157)
€‡(cid:13)ŒŒ
(cid:13)(cid:30)(cid:22)(cid:15)(cid:28)(cid:26)(cid:3)‹(cid:22)(cid:19)‚(cid:30)(cid:27)(cid:19)(cid:30)(cid:24)(cid:21) (cid:141)Ž(cid:19)(cid:30)(cid:24)(cid:15)„ˆ
’ (cid:17)(cid:27)(cid:30)(cid:24)(cid:5)(cid:129)(cid:18)(cid:27)(cid:12)(cid:28)(cid:27)(cid:26) „„
—‡(cid:28)(cid:27)(cid:28) (cid:141)‡(cid:28)(cid:144)„ ‡(cid:29)(cid:23)(cid:28)(cid:27)(cid:13)(cid:10)(cid:17)
(cid:141)€(cid:19)(cid:24)(cid:15)(cid:144)–
(cid:4)(cid:28)(cid:21)(cid:26)(cid:17)(cid:27)(cid:30)(cid:24)(cid:5)(cid:129)(cid:18)(cid:27)(cid:12)(cid:28)(cid:27)
(cid:26)(cid:141)(cid:31)(cid:28)(cid:28)„ˆ
“ „(cid:144) ‡(cid:20)(cid:10)Œ(cid:26)(cid:141)‡(cid:30)‘(cid:19)‘(cid:144)(cid:144)
‰(cid:18)(cid:19)(cid:24)(cid:21)(cid:26)Š(cid:16)(cid:18)(cid:22)(cid:11)
(cid:17)(cid:27)(cid:30)(cid:24)(cid:5)(cid:129)(cid:18)(cid:27)(cid:12)(cid:28)(cid:27)
(cid:141)(cid:13)(cid:22)(cid:18)(cid:144)„
Fig.8:Topology,Geometry,andAlgebrainAttentionMechanismscategorizedaccordingtothemathematicalpropertiesoftheattention
coefficients(firstsubrowofeachrow)andoftheattentionlayer(secondsubrowofeachrow).Thekeykandthequeryqaretheinputsto
theattentioncoefficientα;thevaluevistheinputtotheattentionlayer,andtheoutputvaluev′istheweightedresultofthatlayer.Inputs
andoutputsarerepresentedassignals,i.e.,asfunctionsfromadomaintoacodomain.Theblackcurvedarrowsrepresenttheactionofa
grouponasignal’sdomainorcodomain.Notations:Rn:Euclideanspace;M:Manifold;P:pointset;G:graph;Ω:topologicalspace.
(cid:5)(cid:12)(cid:5)(cid:19)(cid:24)(cid:30)(cid:6)(cid:7)(cid:28)(cid:8)(cid:26)(cid:24)(cid:18)(cid:19)(cid:21)(cid:24)(cid:28)(cid:21)(cid:21)(cid:10)(cid:26)(cid:24)(cid:19)(cid:26)(cid:29)(cid:27)(cid:21)(cid:28)(cid:12)(cid:18)(cid:28)(cid:13)
(cid:5)(cid:12)(cid:5)(cid:19)(cid:24)(cid:30)(cid:6)(cid:7)(cid:28)(cid:8)(cid:26)(cid:24)(cid:18)(cid:19)(cid:21)(cid:24)(cid:28)(cid:21)(cid:21)(cid:10)(cid:26)(cid:24)(cid:19)(cid:26)(cid:28)(cid:7)(cid:24)(cid:30)(cid:19)(cid:27)(cid:30)‚(cid:19)(cid:22)‹(cid:3)
(cid:5)(cid:12)(cid:5)(cid:19)(cid:24)(cid:30)(cid:6)(cid:7)(cid:28)(cid:8)(cid:26)(cid:24)(cid:18)(cid:19)(cid:21)(cid:24)(cid:28)(cid:21)(cid:21)(cid:10)(cid:26)(cid:24)(cid:19)(cid:26)(cid:29)(cid:15)(cid:18)(cid:16)(cid:18)(cid:23)(cid:18)(cid:17)23
patchesinRn overR.Wehighlightthedifference ity, we consider this space as a Euclidean space
between the mathematical representation of these Rn with algebraic structure. The attention coef-
signals, and their computational representation ficients are group invariant, while the attention
during an actual implementation of the trans- layerisgroupequivariant,fortheEuclideangroup
former. Mathematically, we represent the signals’ E(3) of translations, rotations and reflections in
domain as the continuous real line R. Compu- 3Dspace.Thisconfigurationisalsoillustratedby
tationally, however, this real line is discretized the Steerable Transformer (Kundu and Kondor,
into T steps and associated T discrete tokens 2024) which processes Euclidean codomains Rn
(for either words or image patches). Yet, the with equivariance to the special Euclidean group
representationasthereallineisusefultounifythe SE(n), for application to image processing and
original transformer with the more complicated machine learning tasks.
layers introduced next.
4) Euclidean signal on manifold domain for all
2) Manifold signal on Euclidean domain for keys inputs / outputs, with domain group action.
and queries. Compared to the classical trans- Similar to the previous row, this row also intro-
former of the previous row, this row introduces duces geometric structure in the keys, queries,
geometry in the codomains of the keys and query values. In contrast to the previous row, however,
signals, which write k,q : R (cid:55)→ M. Meanwhile, thislayerbringsgeometryintothedomainsofthe
the attention coefficients α, the input and output
signals:k,q,v,v′whereastheabovelayerbrought
values v,v′ have the same structure as in the geometry in their codomains. Consequently, for
classic transformer. The multimanifold attention this row, the attention coefficients α:M×M (cid:55)→
mechanisms by (Konstantinidis et al., 2023) il- R have the product manifold M × M as their
lustratesthisconfiguration.Specifically,thiswork domains=. This configuration is illustrated in the
considers the classical attention coefficient α as Lie Transformer (Hutchinson et al., 2020), where
the computation of a Euclidean distance between the manifolds of interest are Lie groups and their
key and query. Accordingly, their proposed geo- subgroups. We note that the data processed by
metricattentioncoefficientreplacestheEuclidean this architecture does not have to belong to a Lie
distance by a Riemannian geodesic distance be- group; only to be acted upon by a Lie group. A
tween key and query, which are interpreted as lifting layer is introduced to convert the raw data
elements of a manifold: the manifold of SPD into Lie group elements, which are then handled
(symmetric positive definite) matrices, the Grass- bytheLieTransformer.Theattentionlayeristhen
mannmanifold,orboth—hencetheterm“multi”- equivariant.
manifold. We note that the input data to this
5) Euclidean signals on manifold domains for
transformer architecture is still Euclidean, since
all inputs / outputs. This row is similar to the
this attention mechanism is proposed for images
previous row, using manifold domains for keys,
invisiontransformers.However,thewaythisdata
queries,values,andattentioncoefficients.Bycon-
isprocessedbythetransformer’sinternallayersis
trastwiththepreviousrow,thegroupequivariance
non-Euclidean.
is replaced by the concept of gauge equivariance
Equivariance in Attention Mechanisms.: and invariance. The Gauge invariant transformer
in He et al. (2021) presents such configuration:
3) Euclidean signal on Euclidean domains for the attention coefficients are gauge invariant, and
keys, queries and values, with group action on theattentionlayerisgaugeequivariant.Thiswork
codomain.Thisrowillustratesanattentionmech- exclusivelyfocusesontwo-dimensionalmanifolds
anism that includes additional algebraic structure M embedded in 3D Euclidean space.
in both keys, queries, input and output values.
Topology in Attention Mechanisms.:
The Geometric Algebra Transformer represents
thisconfigurationBrehmeretal.(2023).Itslayers 6) Euclidean signal on set domain, with domain
were designed to process “geometric data” de- group action: This row introduces topological
finedasscalars,vectors,lines,planes,objectsand structureonthedomainsofthesignalsk,q,v,v′ :
theirtransformations(e.g.,rotations)in3Dspace. P (cid:55)→ Rn, through the set of points P. The Set
Suchgeometricdataisencodedintomultivectors, Transformer (ST) (Lee et al., 2019) and the Point
whichareelementsoftheprojectivegeometrical- Cloud Transformer (PCT) (Guo et al., 2021) are
gebraalsocalledtheCliffordalgebra.Forsimplic- two examples of this configuration. They enjoy24
equivariance to the group of permutations, which domain group action. Similar to the previous
acts by permuting the indexing of the points in row, this configuration uses topological structure
the set (resp., in the point cloud). on the domains of the signals: here, leverag-
ing hypergraphs. Examples of this configuration
7) Manifold signals on set domains, with do-
include Dynamic Hypergraph Neural Networks
main group action. This row includes geometric
(DHGNN) (Jiang et al., 2019) and Hypergraph
structure on top of the topological structure of
Attention Networks (Hyper GAT) (Ding et al.,
the previous row, using manifold codomains for
2020).
keys k, queries q and Euclidean signals for v
and v′, all on set domains. The geodesic trans- 12) Euclidean signals on combinatorial complex
former (Li et al., 2022b) provides an architecture domain, with domain group action. This final
that processes this configuration. Similar to the rowusesacombinatorialcomplexdomainforthe
Multi-Manifold Attention of row 2, the attention signalsdefininginputsoftheattentioncoefficients
coefficient of the geodesic transformer is com- and attention layers. A Higher Order Attention
putedusingageodesicdistancebetweenkeysand Network(HOAN)architectureisarecentexample
queries:eitheragraph-basedgeodesicdistance,or of this last category (Hajij et al., 2023b).
a Riemannian geodesic distance on the so-called
oblique manifold.
C. Benchmarks
8) Euclidean signals on graph domain, with do-
We now turn to a brief review of the benchmarks that
main group action. This row is similar to the
havebeenconsideredinthenon-Euclideandeeplearn-
previous row, but without a group action on the
ing literature, compiling results from a broad sample
signalscodomains.ThisisillustratedintheGraph
ofneuralnetworkswithtopological,geometric,andal-
Attention Transformer (GAT) (Velicˇkovic´ et al.,
gebraiclayersinTableI,andhighlightingthediversity
2018).
of tasks and datasets used in the literature.
9) Euclidean signals on graph domains, with do-
Tasks and Datasets: We first observe that a wide
main and codomain group actions. This row
also uses a graph domain G such that k,q,v,v′ : varietyoftaskandbenchmarkdatasetshavebeenused
G (cid:55)→ Rn. An example of this configuration in the literature, with little overlap between models. In
other words, it is rare that two different models have
is the SE(3)-Transformer, where the codomain
of these signals is additionally restricted to R3, been benchmarked on the same dataset. This is not
surprising,sincedifferentmodelsusedifferentgeomet-
equipped with an action of the group of trans-
ric, topological, and algebraic structures and different
lations and rotations in 3D SE(3) (Fuchs et al.,
structures are well suited for different tasks.
2020). This transformer was proposed to process
3D point clouds, and provides SE(3)-invariant There are, however several benchmarks that appear
attention coefficients and SE(3)-equivariant at- across models: MNIST and CIFAR for image clas-
tention layer. sification, and Cora, Citeseer, and Pubmed for graph
classification. Many geometrical models are tested by
10) Euclidean signals on cellular complex domain,
examininghowwelltheymodeldynamicalorphysical
with domain group action. Starting on this row,
systems.Theseresultsarenoteasilycomparableacross
more complicated topological structure is intro-
models, as the tasks are often customized for each
duced on the domains Ω of the signals. We refer
paper.
the readerto thesurvey (Papillonet al., 2023)for
a detailed exposition of the attention mechanisms
Number of parameters: A key benefit of building
using topology. This row presents architectures
mathematical structure into neural networks is that it
that specifically use a cellular complex or a sim-
constrains the hypothesis search space. If the structure
plicialcomplexasthedomain.CellAttentionNet-
is well matched to the problem, the model should
works (CAN) (Giusti et al., 2023) and Simplicial
require fewer parameters and fewer computations.
Graph Attention Network (SGAT) (Lee et al.,
Many papers mention this, but only a few report the
2022) illustrate this type of attention mechanism,
number of parameters (see right column of Table I).
while the Cellular Transformer (Ballester et al.,
As parameter and data efficiency are frequently cited
2024) additionally include positional encodings.
asadvantagesofbuildingstructureintoneuralnetwork
11) Euclidean signals on hypergraph domain, with models,weencourageauthorstomoreregularlyreport25
TABLEI:ApplicationsandBenchmarkofNeuralNetworkswithGeometric,TopologicalandAlgebraicStructures.
Weorganizemodelsaccordingtowhetheritusesattentionandtheirgeometric,topologicalandalgebraicstructure,withtheabbreviations:
M:manifold,Gp:group,S:set,G:graph,Ω:topologicaldomain,A:algebra.Modelsarealsoorganizedbasedonwhichtasktheyperform,
and onwhich benchmarkdatasets. We includeaccuracies forbenchmarks thattwo or moremodels use,converting testerror to accuracy
whenneeded,alongwithstandarderrorifreported.Modelparametersarelistedifthepaperreportsthem.N.R.meansNotReported.
Model Structure Task Benchmarkdatasets #Params
RiemannianVAE M DimensionReduction HumanConnectomeProject(HCP) N.R.
Miolane19
S-VAE/VGAE M Latentrepresentationforimage MNIST(93.4±0.2*),Cora(94.1±0.3), N.R.
Davidson18 classificationandlinkprediction Citeseer(95.2±0.2),Pubmed(96.0±0.1)
SPDNet M Visualclassification(emotion, AFEW,HDM05andPaSC N.R.
Huang,VanGool16 action,face)
EMLPFinzi21 Gp Dynamicalmodeling Doublependulum N.R.
LeNet-5LeCun98 Gp Imageclassification MNIST(99.2±0.1) N.R.
SteerableCNN Gp Imageclassification CIFAR(10:76.3;10+:96.4;100+:81.2) 4.4M
Cohen,Welling17 9.1M
G-CNN Gp Imageclassification RotatedMNIST,CIFAR(10:93.5;10+: 2.6M
Cohen,Welling16 95.1)
G-CNNCohen19a Gp Climate,pointcloudsegmentation ClimateSegmentation,Stanford2D-3D-S N.R.
E(n)-EGNN Gp Molecularpropertyprediction, QM9,N-body,Graphautoencoder N.R.
Satorras2021 dynamicalmodeling
PONITABekkers2024 Gp Molecularpropertypredictionand rMD17,QM9,N-body N.R.
generation,dynamicalmodeling
PointNet++Qi17 S Image,3D,sceneclassification MNIST(99.49),ModelNet40(91.9), 1.7M
SHREC15,ScanNet
Tensorfieldnetwork S 3D-point-cloudprediction QM9 N.R.
Thomas18
GCNKipf,Welling17 G Linkprediction Cora,Citeseer,Pubmed,NELL N.R.
enn-s2sGilmer17 G Molecularpropertyprediction QM9 N.R.
SNNEbli20 Ω Coauthorshipprediction SemanticScholarOpenResearchCorpus N.R.
MPSNBodnar21 Ω Trajectory,graphclassification TUDataset N.R.
CXNHajij20 Ω - - N.R.
HMPNNHeydari22 Ω Citationnodeclassification Cora(92.2) N.R.
CCNNHajij23 Ω Imagesegmentation,image,mesh, HumanBody,COSEG,SHREC11 N.R.
graphclassification
E(n)-EMPSN Ω Molecularpropertyprediction, QM9,N-body 200K
Eijkelboom2023 dynamicalmodeling
Clifford-EMPSN Ω Poseestimation,dynamical CMUMoCap,MD17 200K
Liu2024 modeling
E(n)EquivariantTNN Ω Molecularproperty,airpollution QM9,AirPollutionDownscaling 1.5M
Battiloro2024 prediction
TransformerVaswani17 - Machinetranslation WMT2014 N.R.
MMAViT M Imageclassification,segmentation CIFAR(10:94.7,100+:77.5), 3.9M
Konstantinidis23 T-ImageNet,ImageNet,ADE20K
GATrBrehmer23 A Dynamicalmodeling N-body,arterystress,diffusionrobotics 4.0M
SteerableTransformer Gp Point-cloud,Imageclassification RotatedMNIST(99.03),ModelNet10 0.9M
Kundu2024 (90.4)
LieTransformer Gp Regression,dynamics QM9,ODEspringsimulation 0.9M
Hutchinson20
GETHe21 M Shapeclassification,segmentation SHREC07,HumanBodySegmentation 0.15M
SetTransformerLee19 S Maxvalueregression,clustering Omniglot,CIFAR(100:0.92±0.01 N.R.
PCTGuo21 S Point-cloudclassification, ModelNet40(93.2),ShapeNet(86.4), 1.4M
regression,segmentation S3DIS
GSALi22 S Objectclassification,segmentation ModelNet40(93.3),ScanObjectNN, 18.5M
ShapeNet(85.9)
SE(3)-Transformer G Dynamics,classification, N-body,ScanObjectNN,QM9 N.R.
Fuchs20 regression
GATVelicˇkovic´18 G Linkprediction Cora,Citeseer,Pubmed,PPI N.R.
CANGiusti23 Ω Graphclassification TUDataset N.R.
CellularTransformer Ω Graphclassifical,Graphregression GCB,Zinc,OgbgMolhiv N.R.
Ballester24
SGATLee22 Ω Nodeclassification DBLP2,ACM,IMDB N.R.
DHGNNJiang19 Ω Link,sentimentprediction Cora(82.5),Microblog 0.13M
HyperGATDing20 Ω Textclassification 20NG,R8,R52,Ohsumed,MR N.R.
noitnettAtuohtiW
noitnettAhtiW26
parameter counts and computational cost in their pa- opportunity for the application of non-Euclidean ML
pers along with performance metrics. methods. Here, we briefly highlight key developments
in selected application areas. We refer the reader to
Bronstein et al. (2021), Gaudelet et al. (2021), Li
D. Summary et al. (2022a), Rajpurkar et al. (2022), Wang et al.
(2023),Wuetal.(2022,2021)formorecomprehensive
This concludes our review on non-Euclidean in deep
discussions of applications.
neural network layers. While a great diversity of
structures, layer types, and mechanisms have been
proposed, we hope that our illustrated taxonomy may
Chemistry and Drug Development
aid researchers in understanding their differences and
similarities, and in identifying opportunities for inno- Within computational chemistry, molecular analysis
vationandapplication.Wenowturntoabriefsummary and design are traditionally lengthy and expensive
of the existing software libraries for non-Euclidean processes, sometimes requiring decades of develop-
machine learning made available online, and end with ment time, and billions of dollars of investment and
an overview of the domains in which non-Euclidean infrastructure.Moleculardatainherentlyhasgeometric
ML methods have been applied. and graphical structure, well suited for non-Euclidean
approaches (Atz et al., 2021).
Graph neural networks have become a workhorse
for molecular analysis, treating molecules as graphs
VI. NON-EUCLIDEANSOFTWARE with atoms as nodes and bonds as edges (Bronstein
Inthissection,wehighlightpubliclyavailablesoftware et al., 2021, Gilmer et al., 2017). Progress in this
libraries that make the methods of this field compu- field has largely involved the construction of message-
tationally accessible, organized in Table II. Here, we passing neural networks with favorable properties,
limit our discussion to libraries whose commit history such as equivariance to a growing family of group
suggests continued development and have a following transformations, novel forms of weight sharing, more
indicated by at least 50 Github Stars. expressive primitives, and more efficient formulations
for parameterization and computation (Batzner et al.,
Table II highlight libraries core to the field. As shown
2022,Bekkersetal.,2024,Satorrasetal.,2022,Schu¨tt
by the number of stars and actively developed reposi-
et al., 2017, Thomas et al., 2018). Deep networks
tories, packages for topological methods are the most
with geometric structure have also been used directly
well developed, including important engineering foun-
for drug screening to discover new antibiotics (Stokes
dations such as CUDA and C++ accelerated network
et al., 2020).
primitives, and large collections of model implemen-
tations that continue to be maintained. The library Recently, deep equivariant generative modeling has
ecosystem for geometric learning methods is quickly emergedasapowerfulframeworkformoleculesynthe-
growing in interest and contributors, extending the sis. Prior work by (Gebauer et al., 2020, Simm et al.,
packages beyond optimizers over specific manifolds 2020, Simonovsky and Komodakis, 2018) establishes
to more general differential geometry tools. While the the importance of leveraging geometric properties for
packages for algebra in machine learning are the most the synthesis of molecules. (Hoogeboom et al., 2022)
nascent, there have been exciting new developments introduces equivariant denoising diffusion models for
within the past few years in making more specialized molecule generation by directly generating 3D atomic
packagesforacceleratinggroupconvolutionsandother coordinates, demonstrating improved quality and effi-
algebraic operations as the need for more specialized ciency.Thiswasrecentlyextendedby(Xuetal.,2023)
applications have emerged. toperformequivariantdiffusionoveramolecularlatent
space, and by (Vignac et al., 2023) which achieves
much higher stability for generated molecules on the
GEOM-DRUGS dataset. Another line of work gener-
ates molecular invariants such as angles and distances,
VII. APPLICATIONSOFNON-EUCLIDEAN
which then are used to produce coordinates (Luo
GEOMETRY
and Ji, 2022). Recent work has also demonstrated
Many problems in science and engineering are in- the importance of equivariances and invariances for
trinsically non-Euclidean and thus provide an exciting molecular conformer generation (Reidenbach and Kr-27
ishnapriyan, 2023, Xu et al., 2022). Quantitiesofinterestincludeshape,composition,orin-
ternal state. Thus, geometric and topological structure
Structural Biology and Protein Engineering
play an important role in their analysis.
Proteinsare1Dsequencesofaminoacidsthatfoldinto
Many machine learning problems in medical imaging
a 3D structure. The function of a protein is defined
require reasoning about 3D structures, including their
by its structure, so the prediction of protein structure
shape, their variations throughout a population, and
fromtheoriginal1Dsequenceiscrucialtotheanalysis
changes throughout time. As tissue states are non-
of their uses and interactions with other biomolecules.
Euclidean, their statistics and evolution require a ge-
AlphaFold 2, which incorporates a graph-structured
ometric treatment (Pennec et al., 2019). Variations
backbone, equivariant attention, and many geometric
in organ shapes lie on low-dimensional manifolds,
priors about molecules (Jumper et al., 2021), has
andgeometry-awaredimensionality-reductionmethods
emerged as a landmark paper for predicting protein
suchastangentPCA(Boisvertetal.,2008)orPrinciple
structure. Such protein structure predictions are com-
Geodesic Analysis (PGA) enable meaningful repre-
monlyutilizedforapplicationsinbiologyandmedicine
sentations for downstream tasks (Fletcher and Joshi,
(Yang et al., 2023). Prediction of the interactions of
2007, Fletcher et al., 2004, Hinkle et al., 2012a).
proteins with other molecules is also a challenging
Geometric methods have been applied the analysis of
problem well represented by graphs and equivariant
the effects of aging in the corpus collosum with MRI
methods (Anand and Achim, 2022, Ingraham et al.,
scans(Hinkleetal.,2012b),tobrainconnectomicsdata
2023, 2019).
inDiffusionTensorImaging(Pennecetal.,2006),and
Computer Vision to the segmentation of 3D anatomical structures from
CTand MRIscans inthe lateralcerebral ventricle,the
Computer vision entails the inference of properties of
kidney parenchyma and pelvis, and the hippocampus
the visual world from images or other measurements
(Pizer et al., 2003).
suchasLIDAR.Therearemanysub-tasksincomputer
vision such as object recognition, semantic segmen- Medical image registration has historically steered
tation, image and video generation, depth estimation, the need for the development of statistics on Lie
etc. Historically, network primitives that capture the groupsfromthefinite-dimensionalgroupSE(3)(Pen-
topological structure and symmetries of images have nec, 2006, Pennec and Thirion, 1997) to the infinite-
dominated vision benchmarks, including CNNs, GC- dimensional group of diffeomorphisms (Trouve´, 1998,
NNs, and Vision transformers (Cohen and Welling, Younes, 2010). Unfortunately, any attempt to rely on
2016,Dosovitskiyetal.,2021,Krizhevskyetal.,2012, Riemannian geometric structures for machine learning
LeCun et al., 1998). on Lie groups is generally ill-posed. Specifically, clas-
sical Riemannian metrics on Lie groups, called left
Recently, one of the most successful applications
or right invariant, generally fail to be invariant by all
of non-Euclidean deep learning has been that of
the group operations, notably inversion. This triggered
graph neural networks, implemented on data native
thesearchforalternativegeometricstructures(Miolane
to or lifted to point-clouds. Pioneering works such
and Pennec, 2015, Pennec and Arsigny, 2012) and
asPointnet++andPointTransformerintroducedgraph-
the replacement of the (non bi-invariant) Riemannian
structured deep networks as breakthrough methods
metricbythecanonicalbi-invariantsymmetricCartan-
in 3D semantic segmentation and object detection at
Shouten connection Pennec and Lorenzi (2020). In
whole-roomscales(Engeletal.,2020,Qietal.,2017).
infinite dimensions, this grounds the parametrization
Another promising computational primitive, Slot At-
of diffeomorphisms by the flow of Stationary Velocity
tention, introduces a novel messaging-passing strat-
Fields (SVFs). This geometrically grounded statistical
egy to perform unsupervised object discovery using
framework on Lie groups is now used in numerous
permutation invariant slots, which incorporates spatial
non-linear medical image registration algorithms (Ar-
symmetries using slot-centric reference frames (Biza
signy et al., 2006, Lorenzi and Pennec, 2013) and to
et al., 2023, Locatello et al., 2020).
model shape evolution such as the brain changes in
Biomedical Imaging Alzheimer’sdisease(Hadj-Hamouetal.,2016,Lorenzi
et al., 2015, Sivera et al., 2019).
Biomedical imaging involves inferring the structure of
biological tissues from measurements of their physical In medical image segmentation, estimation errors on
properties, typically in the form of electromagnetic the order of a few pixels can lead to significant mis-
fields, acoustic waves, and other physical phenomena. interpretations. The domain has recently seen benefits28
Geometry
Packages Domains CoreFeatures Stars
GeomStats2019 Manifolds,LieGroups,FiberBundles, Manifoldoperations,Algorithms,Statistics,Optimizers 1.2k
Shape Spaces, Information Manifolds,
Graphs
GeoOpt2020 Manifolds Layers, Manifold operations, Stochastic optimizers for 812
deeplearning
PyManOpt2016 Manifolds,LieGroups Manifoldoperations,Optimizers 734
PyRiemann2023 SPDMatricies MachineLearning,DataAnalysisforbiosignals 606
Topology
Packages Domains CoreFeatures Stars
PytorchGeometric2019 Graphs BaselineModels,Layers,FastBasicGraphOperations, 20.6k
Datasets,Dataloaders
NetworkXHagbergetal. Graphs,Digraphs,Multigraphs Data structures, Graph generators, Graph Algorithms, 14.5k
(2008) NetworkAnalysisMeasures
DGL2019 Graphs BaselineModels,Layers,FastBasicGraphOperations, 13.2k
Datasets, Dataloaders, Framework-agnostic (PyTorch,
Tensorflow,etc.areswappable)
DIG2021 Graphs Baselinemodels,Datasets,EvaluationMetrics 1.8k
AutoGL2021 Graphs Neural Architecture Search, Hyper-Parameter Tuning, 1.1k
Ensembles
HyperNetX2023 Hypergraphs MachineLearningAlgorithms,Analysis,Visualization 502
DHG2022 Graphs, hypergraphs, bipartite graphs, Models, Basic Operations, Dataloaders, Visualization, 566
hypergraphs,directedhypergraphs,... AutoML,Metrics,Graphgenerators
TopoModelX2024 Graphs, colored hypergraphs, com- BaselineModels,Layers,Higher-ordermessagepassing 205
plexes
TopoNetX2024 Graphs, colored hypergraphs, com- TopographyGenerators,Computingtopologicalproper- 168
plexes ties,Arbitrarycellattributes
TopoEmbedX2024 Graphs, colored hypergraphs, com- Representationlearning,embeddings 72
plexes
TopoBenchmarkX2024 Graphs,hypergraphs,complexes Benchmarks,lifting,dataloaders,losses,trainingframe- 54
work
XGI2023 Hypergraphs, directed hypergraphs, Graph generators, metrics, algorithms, dataloaders, vi- 172
symplicalcomplexes sualization
Algebra
Packages Domains CoreFeatures Stars
E3NN2022 E(3)EquivariantFeatureFields GroupConvolutions,SteerableGroupConvolutions 891
ESCNN&E2CNN2022 E(n)EquivariantFeatureFields,Graphs GroupConvolutions,SteerableGroupConvolutions 5841
NequIP2022 E(3)EquivarianceonGraphs GroupConvolutions,SteerableGroupConvolutions 538
EMLP2021 Matrix Groups, Tensors, Irreducible ProgrammaticgenerationofequivariantMLPsforarbi- 249
Representations, Induced Representa- trarymatrixgroupsinJAX
tions
PyQuaternion Quaternions Quaternion operations, rotation representation conver- 336
sions,differentiation,integration
TABLEII:SoftwarePackagesforMachineLearningwithTopology,Geometry,andAlgebra.Weorganizepackagesaccordingtothe
mathematical,non-Euclideanstructurestheyfocuson.29
fromtheadditionoftopologicalstructure.Examplesof et al., 2023) proposes an architecture for embedding
topological constraints that improve consistency and geometric data into a Geometric Algebra representa-
accuracy include preserving membrane connectivity tion to be processed by an equivariant Transformer
for cell images and incorporating anatomically correct network, and demonstrates its efficacy on mesh inter-
relative position of organs (Clough et al., 2022, Gupta action estimation and n-body simulation. Astrophysics
et al., 2022, Hu et al., 2019, 2021). data is also well suited for application of equivariant
networks. Some examples include the classification of
Cryo-EM data provide multiple 2D image projections
radio galaxies using group-equivariant CNNs (Scaife
of a 3D biomolecule from different angles. The task
andPorter,2021),optimalcosmologicalanalysisusing
of inferring the original 3D structure of a biomolecule
equivariant normalizing flows (Dai and Seljak, 2022),
benefits greatly from the incorporation of topological
and cosmic microwave background radiation analysis
and geometric structure (Donnat et al., 2022, Miolane
using spherical equivariant CNNs (McEwen et al.,
et al., 2020).
2022).
Histologyimageshavenocanonicalorientation,mean- Other Applications
ingeachorientationofaparticularcellisequallylikely
There are many other interesting applications of non-
to appear. Recent works have explored equivariant
Euclidean methods to domains that do not neatly
and steerable CNNs to leverage these symmetries in
fall into these categories, such as GNNs for arrival
the data (Adnan et al., 2020, Bekkers et al., 2018,
time forecasting (Derrow-Pinion et al., 2021), weather
Grahametal.,2020,Lafargeetal.,2020,Veelingetal.,
forecasting(Keisler,2022,Lametal.,2023),materials
2018)
science (Reiser et al., 2022), patient electronic health
Recommender Systems and Social Networks records (Li et al., 2022a), and many more. As a bur-
geoning young field, the application of non-Euclidean
Recommendersystemswereoneofthefirstsuccessful
machine learning to novel domains is constantly ad-
applications of graph representation learning, where
vancing. We expect (and hope) that the highlights
the proximity of a particular node to other nodes can
presented here will represent only a small fraction of
be learned through graph convolutions to represent
what is to come.
similarity. This has clear economic utility for recom-
mending advertisements, products, music, and content
VIII. CONCLUSION
basedontheuser’sdataandpreferences.PinSagefrom
Pinterest was one of the first to demonstrate their As the availability of richly structured, non-Euclidean
potentialatacommercialscale(Yingetal.,2018).See data grows across application domains, there is an
(Wuetal.,2022)foracomprehensivereviewongraph increasing need for machine learning methods that
neural networks in recommender systems. can fully leverage the underlying geometry, topology,
and symmetries to extract insights. Driven by this
Physics need,anewparadigmofnon-Euclideanmachinelearn-
ing is emerging that generalizes classical techniques
Physics data naturally has many symmetries and often
to curved manifolds, topological spaces, and group-
takes the form of relations between unordered sets, an
structured data. This paradigm shift echoes the non-
ideal setting for topological and equivariant methods.
Euclidean revolution in mathematics in the 19th cen-
Dynamics between particles or nodes in a mesh can
tury,which radicallyexpanded ournotion ofgeometry
be effectively computed using learned graph message
and catalyzed significant advancements across the nat-
passing for various types of physics data (Pfaff et al.,
ural sciences.
2021, Sanchez-Gonzalez et al., 2020). Equivariant
transformer and graph neural network architectures In this review, we have provided an accessible
have been successfully applied to data analysis for overview of this emerging field, unifying disparate
the Large Hadron Collider and other simulations such threads in the literature into a common organizational
as gravity for the n-body problem (Brandstetter et al., framework. Our illustrated taxonomy contextualizes,
2022,Fuchsetal.,2020).Topologicalmethodsarewell classifies, and differentiates existing approaches and
suited to process the hundreds of petabytes of highly illuminates gaps that present opportunities for inno-
relationaldataproducedbyexperimentsfromtheLarge vation. We hope this serves as an invitation for both
HadronColliderandhavedemonstratedtheirutilityfor theoreticians and practitioners to further explore the
the next stage of fundamental discoveries in particle potential of non-Euclidean geometry, topology, and
physics (DeZoort et al., 2023). Recent work (Brehmer algebra to reshape modern machine learning, just as30
they reshaped our fundamental understanding of space Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger,
over a century ago. Jonathan P Mailoa, Mordechai Kornbluth, Nicola Molinari,
TessESmidt,andBorisKozinsky.E(3)-equivariantgraphneural
By empowering models with the tools to respect the networks for data-efficient and accurate interatomic potentials.
Naturecommunications,13(1):2453,2022.
underlying structure of data, non-Euclidean machine
ErikJBekkers,MaximeWLafarge,MitkoVeta,KoenAJEppenhof,
learning greatly expands the frontiers of the types of Josien PW Pluim, and Remco Duits. Roto-translation covariant
learning problems that can be tackled. With a growing convolutionalnetworksformedicalimageanalysis,2018.
Erik J Bekkers, Sharvaree Vadgama, Rob D Hesselink, Putri A
theoretical understanding and the design of architec-
van der Linden, and David W Romero. Fast, expressive
tures that fully leverage the mathematical formalisms se(n) equivariant networks through weight-sharing in position-
of geometry, topology and algebra, non-Euclidean orientationspace,2024.
Andri Bergsson and Søren Hauberg. Visualizing riemannian data
approaches hold immense potential to transform the
withrie-sne. arXivpreprintarXiv:2203.09253,2022.
broadermachinelearninglandscapeanditsapplication Charles M. Bishop. Bayesian pca. In Neural
toengineeringproblemsandthenaturalsciencesinthe Information Processing Systems, 1998. URL
https://api.semanticscholar.org/CorpusID:43329106.
coming years.
Ondrej Biza, Sjoerd van Steenkiste, Mehdi S. M. Sajjadi,
GamaleldinF.Elsayed,AravindhMahendran,andThomasKipf.
Invariant slot attention: Object discovery with slot-centric refer-
REFERENCES
enceframes,2023.
Mohammed Adnan, Shivam Kalra, and Hamid R. Tizhoosh. Rep- Cristian Bodnar. Topological Deep Learning: Graphs, Complexes,
resentationlearningofhistopathologyimagesusinggraphneural Sheaves. PhDthesis,2023.
networks,2020. Cristian Bodnar, Fabrizio Frasca, Yuguang Wang, Nina Otter,
Morten Akhøj, James Benn, Erlend Grong, Stefan Sommer, Guido F Montufar, Pietro Lio´, and Michael Bronstein. We-
and Xavier Pennec. Principal subbundles for dimension re- isfeiler and lehman go topological: Message passing simpli-
duction. working paper or preprint, July 2023. URL cial networks. In Marina Meila and Tong Zhang, editors,
Proceedings of the 38th International Conference on Machine
https://inria.hal.science/hal-04156036.
Namrata Anand and Tudor Achim. Protein structure and sequence Learning, volume 139 of Proceedings of Machine Learning
generationwithequivariantdenoisingdiffusionprobabilisticmod- Research, pages 1026–1037. PMLR, 18–24 Jul 2021. URL
els,2022. https://proceedings.mlr.press/v139/bodnar21a.html.
VincentArsigny,OlivierCommowick,XavierPennec,andNicholas Jonathan Boisvert, Farida Cheriet, Xavier Pennec, Hubert Labelle,
Ayache. A Log-Euclidean Framework for Statistics on Diffeo- andNicholasAyache.GeometricVariabilityoftheScolioticSpine
morphisms. In Proc. of the 9th International Conference on usingStatisticsonArticulatedShapeModels. IEEETransactions
Medical Image Computing and Computer Assisted Intervention on Medical Imaging, 27(4):557–568, 2008. doi: 10.1109/TMI.
(MICCAI’06),PartI,LNCS,pages924–931,October2006. doi: 2007.911474.
10.1007/11866565 113. G. E. P. Box and G. C. Tiao. A bayesian approach to
DevanshuArya,RichardOlij,DeepakK.Gupta,AhmedElGazzar, some outlier problems. Biometrika, 55(1):119–129, 1968.
Guido van Wingen, Marcel Worring, and Rajat Mani Thomas. ISSN 1464-3510. doi: 10.1093/biomet/55.1.119. URL
Fusing structural and functional mris using graph convolutional http://dx.doi.org/10.1093/biomet/55.1.119.
networksforautismclassification.InTalArbel,IsmailBenAyed, Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J
Marleen de Bruijne, Maxime Descoteaux, Herve Lombaert, and Bekkers, and Max Welling. Geometric and physical quantities
ChristopherPal,editors,ProceedingsoftheThirdConferenceon improvee(3)equivariantmessagepassing,2022.
Medical Imaging with Deep Learning, volume 121 of Proceed- Johann Brehmer, Pim De Haan, So¨nke Behrends, and Taco Co-
ingsofMachineLearningResearch,pages44–61.PMLR,06–08 hen. Geometric algebra transformer. In Thirty-seventh Con-
Jul2020. URLhttps://proceedings.mlr.press/v121/arya20a.html. ference on Neural Information Processing Systems, 2023. URL
KennethAtz,FrancescaGrisoni,andGisbertSchneider. Geometric https://openreview.net/forum?id=M7r2CO4tJC.
deeplearningonmolecularrepresentations,2021. Leo Breiman. Machine Learning, 45(1):5–32, 2001. ISSN
Rube´nBallester,PabloHerna´ndez-Garc´ıa,MathildePapillon,Clau- 0885-6125. doi: 10.1023/a:1010933404324. URL
dio Battiloro, Nina Miolane, Tolga Birdal, Carles Casacu- http://dx.doi.org/10.1023/A:1010933404324.
berta, Sergio Escalera, and Mustafa Hajij. Attending to
MichaelMBronstein,JoanBruna,TacoCohen,andPetarVelicˇkovic´.
topological spaces: The cellular transformer. arXiv preprint Geometric deep learning: Grids, groups, graphs, geodesics, and
arXiv:2405.14094,2024. gauges. arXivpreprintarXiv:2104.13478,2021.
MonamiBanerjee,RudrasisChakraborty,EdwardOfori,DavidVail- GabrieleCesa,LeonLang,andMauriceWeiler. Aprogramtobuild
lancourt,andBabaC.Vemuri. NonlinearRegressiononRieman- E(N)-equivariantsteerableCNNs.InInternationalConferenceon
nian Manifolds and Its Applications to Neuro-Image Analysis, LearningRepresentations(ICLR),2022.
page 719–727. Springer International Publishing, 2015. ISBN Rudrasis Chakraborty, Jose Bouza, Jonathan H. Manton, and
9783319245539. doi: 10.1007/978-3-319-24553-9 88. URL Baba C. Vemuri. ManifoldNet: A Deep Neural Net-
http://dx.doi.org/10.1007/978-3-319-24553-9 88. work for Manifold-Valued Data With Applications. IEEE
AlexandreBarachant,QuentinBarthe´lemy,Jean-Re´miKing,Alexan-
Transactions on Pattern Analysis and Machine Intelligence,
dre Gramfort, Sylvain Chevallier, Pedro L. C. Rodrigues, 44(2):799–810, February 2022. ISSN 0162-8828, 2160-
Emanuele Olivetti, Vladislav Goncharenko, Gabriel Wag- 9292, 1939-3539. doi: 10.1109/TPAMI.2020.3003846. URL
ner vom Berg, Ghiles Reguig, Arthur Lebeurrier, Erik https://ieeexplore.ieee.org/document/9122448/.
Bja¨reholt, Maria Sayu Yamamoto, Pierre Clisson, and Marie- KuiYuChangandJoydeepGhosh.Aunifiedmodelforprobabilistic
Constance Corsi. pyriemann/pyriemann: v0.5, 2023. URL principal surfaces. IEEE Transactions on Pattern Analysis and
https://doi.org/10.5281/zenodo.8059038. MachineIntelligence,23(1):22–41, 2001. ISSN 01628828. doi:
Claudio Battiloro, Ege Karaismailog˘lu, Mauricio Tec, George 10.1109/34.899944.
Dasoulas, Michelle Audirac, and Francesca Dominici. E Nikolai Chernov. Circular and Linear Regression: Fitting Cir-
(n) equivariant topological neural networks. arXiv preprint cles and Lines by Least Squares. CRC Press, June 2010.
arXiv:2405.15429,2024. ISBN 9780429151415. doi: 10.1201/ebk1439835906. URL31
http://dx.doi.org/10.1201/EBK1439835906. plicial neural networks. In TDA & Beyond, 2020. URL
James R. Clough, Nicholas Byrne, Ilkay Oksuz, Veronika A. Zim- https://openreview.net/forum?id=nPCt39DVIfk.
mer, Julia A. Schnabel, and Andrew P. King. A topological Floor Eijkelboom, Rob Hesselink, and Erik J Bekkers. E (n)
loss function for deep-learning based image segmentation using equivariantmessagepassingsimplicialnetworks.InInternational
persistent homology. IEEE Transactions on Pattern Analysis Conference on Machine Learning, pages 9071–9081. PMLR,
and Machine Intelligence, 44(12):8766–8778, December 2022. 2023.
ISSN 1939-3539. doi: 10.1109/tpami.2020.3013679. URL Nico Engel, Vasileios Belagiannis, and Klaus C. J. Dietmayer.
http://dx.doi.org/10.1109/TPAMI.2020.3013679. Pointtransformer. IEEEAccess,9:134826–134840,2020. URL
Taco Cohen and Max Welling. Group equivariant convolu- https://api.semanticscholar.org/CorpusID:226227046.
tional networks. In Maria Florina Balcan and Kilian Q. Luca Falorsi, Pim de Haan, Tim R. Davidson, Nicola De Cao,
Weinberger, editors, Proceedings of The 33rd International MauriceWeiler,PatrickForre´,andTacoS.Cohen. Explorations
Conference on Machine Learning, volume 48 of Proceed- in Homeomorphic Variational Auto-Encoding. 2018. URL
ings of Machine Learning Research, pages 2990–2999, New http://arxiv.org/abs/1807.04689.
York, New York, USA, 20–22 Jun 2016. PMLR. URL Jianqing Fan. Local linear regression smoothers and their min-
https://proceedings.mlr.press/v48/cohenc16.html. imax efficiencies. The Annals of Statistics, 21(1), March
TacoCohen,MauriceWeiler,BerkayKicanaoglu,andMaxWelling. 1993. ISSN 0090-5364. doi: 10.1214/aos/1176349022. URL
Gauge equivariant convolutional networks and the icosahedral http://dx.doi.org/10.1214/aos/1176349022.
cnn. In International conference on Machine learning, pages MatthiasFeyandJanE.Lenssen.Fastgraphrepresentationlearning
1321–1330.PMLR,2019a. with PyTorch Geometric. In ICLR Workshop on Representation
TacoCohenetal. Equivariantconvolutionalnetworks. PhDthesis, LearningonGraphsandManifolds,2019.
TacoCohen,2021. MarcFinzi,MaxWelling,andAndrewGordonGordonWilson. A
Taco S. Cohen and Max Welling. Steerable CNNs. In Inter- practical method for constructing equivariant multilayer percep-
national Conference on Learning Representations, 2017. URL trons for arbitrary matrix groups. In Marina Meila and Tong
https://openreview.net/forum?id=rJQKYt5ll. Zhang,editors,Proceedingsofthe38thInternationalConference
TacoSCohen,MarioGeiger,andMauriceWeiler. Ageneraltheory on Machine Learning, volume 139 of Proceedings of Machine
ofequivariantcnnsonhomogeneousspaces. Advancesinneural Learning Research, pages 3318–3328. PMLR, 18–24 Jul 2021.
informationprocessingsystems,32,2019b. URLhttps://proceedings.mlr.press/v139/finzi21a.html.
Biwei Dai and Urosˇ Seljak. Translation and rotation equivariant R. A. Fisher. The use of multiple measurements in taxonomic
normalizingflow(trenf)foroptimalcosmologicalanalysis.2022. problems. AnnalsofEugenics,7(7):179–188,1936.
URLhttps://doi.org/10.1093/mnras/stac2010. P. Thomas Fletcher and Sarang Joshi. Riemannian geometry
James Damon and J. S. Marron. Backwards Principal Component for the statistical analysis of diffusion tensor data. Signal
AnalysisandPrincipalNestedRelations.JournalofMathematical Processing, 87(2):250–262, 2007. ISSN 0165-1684.
Imaging and Vision, 50(1-2):107–114, October 2013. ISSN doi: https://doi.org/10.1016/j.sigpro.2005.12.018. URL
0924-9907, 1573-7683. doi: 10.1007/s10851-013-0463-2. URL https://www.sciencedirect.com/science/article/pii/S0165168406001691.
http://link.springer.com/article/10.1007/s10851-013-0463-2. PThomasFletcher,ConglinLu,StephenMPizer,andSarangJoshi.
TimR.Davidson,LucaFalorsi,NicolaDeCao,ThomasKipf,and Principalgeodesicanalysisforthestudyofnonlinearstatisticsof
Jakub M. Tomczak. Hyperspherical variational auto-encoders. shape. IEEE transactions on medical imaging, 23(8):995–1005,
34th Conference on Uncertainty in Artificial Intelligence 2018, 2004.
UAI2018,2:856–865,2018. Thomas Fletcher. Geodesic regression on riemannian manifolds.
Brad C. Davis, P. Thomas Fletcher, Elizabeth Bullitt, and Sarang In Proceedings of the Third International Workshop on Mathe-
Joshi. Population shape regression from random design data. maticalFoundationsofComputationalAnatomy-Geometricaland
International Journal of Computer Vision, 90(2):255–266, jul Statistical Methods for Modelling Biological Shape Variability,
2010. ISSN1573-1405. doi:10.1007/s11263-010-0367-1. URL pages75–86,2011.
http://dx.doi.org/10.1007/s11263-010-0367-1. Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling.
Austin Derrow-Pinion, Jennifer She, David Wong, Oliver Lange, Se(3)-transformers: 3d roto-translation equivariant attention
ToddHester,LuisPerez,MarcNunkesser,SeongjaeLee,Xueying networks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F.
Guo, Brett Wiltshire, Peter W. Battaglia, Vishal Gupta, Ang Balcan, and H. Lin, editors, Advances in Neural Information
Li, Zhongwen Xu, Alvaro Sanchez-Gonzalez, Yujia Li, and Processing Systems, volume 33, pages 1970–1981. Curran
Petar Velickovic. Eta prediction with graph neural networks in Associates,Inc.,2020.URLhttps://proceedings.neurips.cc/paper
google maps. In Proceedings of the 30th ACM International files/paper/2020/file/15231a7ce4ba789d13b722cc5c955834-Paper.pdf.
Conference on Information & Knowledge Management, CIKM Yue Gao, Yifan Feng, Shuyi Ji, and Rongrong Ji. Hgnn: General
’21.ACM,October2021. doi:10.1145/3459637.3481916. URL hypergraph neural networks. IEEE Transactions on Pattern
http://dx.doi.org/10.1145/3459637.3481916. AnalysisandMachineIntelligence,2022.
GageDeZoort,PeterWBattaglia,CatherineBiscarat,andJean-Roch Thomas Gaudelet, Ben Day, Arian R Jamasb, Jyothish Soman,
Vlimant. Graph neural networks at the large hadron collider. CristianRegep,GertrudeLiu,JeremyBRHayter,RichardVick-
NatureReviewsPhysics,5(5):281–303,2023. ers,CharlesRoberts,JianTang,DavidRoblin,TomLBlundell,
Kaize Ding, Jianling Wang, Jundong Li, Dingcheng Li, and Michael M Bronstein, and Jake P Taylor-King. Utilizing graph
Huan Liu. Be more with less: Hypergraph attention networks machinelearningwithindrugdiscoveryanddevelopment. 2021.
for inductive text classification. In Conference on Empiri- URLhttps://doi.org/10.1093/bib/bbab159.
cal Methods in Natural Language Processing, 2020. URL F Gay-Balmaz, DD Holm, DM Meier, TS Ratiu, and F-X
https://api.semanticscholar.org/CorpusID:226226607. Vialard. Invariant Higher-Order Variational Problems.
ClaireDonnat,AxelLevy,FredericPoitevin,EllenZhong,andNina Communications in Mathematical Physics, 309:413–
Miolane. Deepgenerativemodelingforvolumereconstructionin 458, 2012. doi: 10.1007/s00220-011-1313-y. URL
cryo-electronmicroscopy,2022. http://dx.doi.org/10.1007/s00220-011-1313-y.
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeis- Niklas W. A. Gebauer, Michael Gastegger, and Kristof T. Schu¨tt.
senborn,XiaohuaZhai,ThomasUnterthiner,MostafaDehghani, Symmetry-adapted generation of 3d point sets for the targeted
MatthiasMinderer,GeorgHeigold,SylvainGelly,JakobUszko- discoveryofmolecules,2020.
reit, and Neil Houlsby. An image is worth 16x16 words: Mario Geiger, Tess Smidt, Alby M., Benjamin Kurt Miller,
Transformersforimagerecognitionatscale,2021. WouterBoomsma,BradleyDice,KostiantynLapchevskyi,Mau-
Stefania Ebli, Michae¨l Defferrard, and Gard Spreemann. Sim- rice Weiler, Michał Tyszkiewicz, Simon Batzner, Dylan Madis-32
etti, Martin Uhrin, Jes Frellsen, Nuri Jung, Sophia Sanborn, olane, Aldo Guzma´n-Sa´enz, Karthikeyan Natesan Ramamurthy,
Mingjian Wen, Josh Rackers, Marcel Rød, and Michael Bai- Tolga Birdal, Tamal K. Dey, Soham Mukherjee, Shreyas N.
ley. Euclidean neural networks: e3nn, April 2022. URL Samaga,NealLivesay,RobinWalters,PaulRosen,andMichaelT.
https://doi.org/10.5281/zenodo.6459381. Schaub. Topological deep learning: Going beyond graph data,
J.D. Gergonne. Application de la me´thode des moindres carre´s a` 2023b.
l’interpolation des suites. Annales des Mathe´matiques Pures et Mustafa Hajij, Mathilde Papillon, Florian Frantzen, Jens Ager-
Applique´es,6:242–252,1815. berg, Ibrahem AlJabea, Ruben Ballester, Claudio Battiloro,
Jeff Gill and Dominik Hangartner. Circular data in political sci- Guillermo Berna´rdez, Tolga Birdal, Aiden Brent, Peter Chin,
ence and how to handle it. Political Analysis, 18(3):316–336, Sergio Escalera, Simone Fiorellino, Odin Hoff Gardaa, Gu-
2010. ISSN 1476-4989. doi: 10.1093/pan/mpq009. URL rusankar Gopalakrishnan, Devendra Govil, Josef Hoppe, Ma-
http://dx.doi.org/10.1093/pan/mpq009. neelReddyKarri,JudeKhouja,ManuelLecha,NealLivesay,Jan
JustinGilmer,SamuelS.Schoenholz,PatrickF.Riley,OriolVinyals, Meißner,SohamMukherjee,AlexanderNikitin,TheodorePapa-
and George E. Dahl. Neural message passing for quantum markou, Jaro Pr´ılepok, Karthikeyan Natesan Ramamurthy, Paul
chemistry. In Doina Precup and Yee Whye Teh, editors, Rosen, Aldo Guzma´n-Sa´enz, Alessandro Salatiello, Shreyas N.
Proceedings of the 34th International Conference on Machine Samaga, Simone Scardapane, Michael T. Schaub, Luca Sco-
Learning, volume 70 of Proceedings of Machine Learning Re- fano, Indro Spinelli, Lev Telyatnikov, Quang Truong, Robin
search, pages 1263–1272. PMLR, 06–11 Aug 2017. URL Walters, Maosheng Yang, Olga Zaghen, Ghada Zamzmi, Ali
https://proceedings.mlr.press/v70/gilmer17a.html. Zia, and Nina Miolane. Topox: A suite of python packages
Lorenzo Giusti, Claudio Battiloro, Lucia Testa, Paolo for machine learning on topological domains, 2024. URL
Di Lorenzo, Stefania Sardellitti, and Sergio Barbarossa. https://arxiv.org/abs/2402.02441.
Cell attention networks. In 2023 International Joint Elkan F. Halpern. Polynomial regression from a
Conference on Neural Networks (IJCNN). IEEE, June bayesian approach. Journal of the American Statistical
2023. doi: 10.1109/ijcnn54540.2023.10191530. URL Association, 68(341):137–143, March 1973. ISSN 1537-
http://dx.doi.org/10.1109/IJCNN54540.2023.10191530. 274X. doi: 10.1080/01621459.1973.10481352. URL
SimonGraham,DavidEpstein,andNasirRajpoot. Densesteerable http://dx.doi.org/10.1080/01621459.1973.10481352.
filtercnnsforexploitingrotationalsymmetryinhistologyimages, MartinHanik,Hans-ChristianHege,AnjaHennemuth,andChristoph
2020. vonTycowicz.Nonlinearregressiononmanifoldsforshapeanal-
Chaoyu Guan, Ziwei Zhang, Haoyang Li, Heng Chang, Zeyang ysisusingintrinsicbe´ziersplines.InInternationalConferenceon
Zhang, Yijian Qin, Jiyan Jiang, Xin Wang, and Wenwu Zhu. Medical Image Computing and Computer-Assisted Intervention,
AutoGL:Alibraryforautomatedgraphlearning. InICLR2021 2020. URLhttps://api.semanticscholar.org/CorpusID:220486926.
WorkshoponGeometricalandTopologicalRepresentationLearn- TrevorHastieandWernerStuetzle. PrincipalCurves. Journalofthe
ing,2021. URLhttps://openreview.net/forum?id=0yHwpLeInDn. AmericanStatisticalAssociation,84(406):502–516,1989.
NicolasGuigui,NinaMiolane,XavierPennec,etal. Introductionto SørenHauberg. PrincipalCurvesonRiemannianManifolds. IEEE
riemanniangeometryandgeometricstatistics:frombasictheory Transactions on Pattern Analysis and Machine Intelligence, 38
toimplementationwithgeomstats. FoundationsandTrends®in (9):1915–1921, 2016. ISSN 01628828. doi: 10.1109/TPAMI.
MachineLearning,16(3):329–493,2023. 2015.2496166.
Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Lingshen He, Yiming Dong, Yisen Wang, Dacheng Tao, and
Ralph R. Martin, and Shi-Min Hu. Pct: Point cloud trans- Zhouchen Lin. Gauge equivariant transformer. In M. Ranzato,
former. ComputationalVisualMedia,7(2):187–199,April2021. A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman
ISSN 2096-0662. doi: 10.1007/s41095-021-0229-5. URL Vaughan, editors, Advances in Neural Information Processing
http://dx.doi.org/10.1007/s41095-021-0229-5. Systems, volume 34, pages 27331–27343. Curran Associates,
Mengmeng Guo, Jingyong Su, Li Sun, and Guofeng Cao. Sta- Inc., 2021. URL https://proceedings.neurips.cc/paper
tistical regression analysis of functional and shape data. Jour- files/paper/2021/file/e57c6b956a6521b28495f2886ca0977a-Paper.pdf.
nal of Applied Statistics, 47(1):28–44, September 2019. ISSN SajjadHeydariandLorenzoLivi. Messagepassingneuralnetworks
1360-0532. doi: 10.1080/02664763.2019.1669541. URL for hypergraphs. In Elias Pimenidis, Plamen Angelov, Chrisina
http://dx.doi.org/10.1080/02664763.2019.1669541. Jayne, Antonios Papaleonidas, and Mehmet Aydin, editors, Ar-
Saumya Gupta, Xiaoling Hu, James Kaan, Michael Jin, Mutshipay tificial NeuralNetworks and MachineLearning – ICANN2022,
Mpoy, Katherine Chung, Gagandeep Singh, Mary Saltz, Tahsin pages583–592,Cham,2022.SpringerNatureSwitzerland.
Kurc,JoelSaltz,ApostolosTassiopoulos,PrateekPrasanna,and JacobHinkle,PrasannaMuralidharan,P.Fletcher,andSarangJoshi.
Chao Chen. Learning topological interactions for multi-class Polynomialregressiononriemannianmanifolds.7574,102012a.
medicalimagesegmentation,2022. doi:10.1007/978-3-642-33712-3 1.
Mehdi Hadj-Hamou, Marco Lorenzi, Nicholas Ayache, and Xavier Jacob Hinkle, Prasanna Muralidharan, P Thomas Fletcher, and
Pennec. Longitudinal Analysis of Image Time Series with SarangJoshi.Polynomialregressiononriemannianmanifolds.In
Diffeomorphic Deformations: A Computational Framework Europeanconferenceoncomputervision,pages1–14.Springer,
BasedonStationaryVelocityFields. FrontiersinNeuroscience, 2012b.
10,2016.ISSN1662-453X.doi:10.3389/fnins.2016.00236.URL Emiel Hoogeboom, Victor Garcia Satorras, Cle´ment Vignac, and
https://www.frontiersin.org/articles/10.3389/fnins.2016.00236/full. Max Welling. Equivariant diffusion for molecule generation in
AricHagberg,PieterJSwart,andDanielASchult. Exploringnet- 3d,2022.
workstructure,dynamics,andfunctionusingnetworkx.Technical XiaolingHu,LiFuxin,DimitrisSamaras,andChaoChen.Topology-
report, Los Alamos National Laboratory (LANL), Los Alamos, preservingdeepimagesegmentation,2019.
NM(UnitedStates),2008. Xiaoling Hu, Yusu Wang, Li Fuxin, Dimitris Samaras, and Chao
Mustafa Hajij, Kyle Istvan, and Ghada Zamzmi. Cell com- Chen.Topology-awaresegmentationusingdiscretemorsetheory,
plex neural networks. In TDA & Beyond, 2020. URL 2021.
https://openreview.net/forum?id=6Tq18ySFpGU. Zhiwu Huang and Luc Van Gool. A Riemannian Network
Mustafa Hajij, Ghada Zamzmi, Theodore Papamarkou, Nina Mi- for SPD Matrix Learning. pages 2036–2042, 2016. URL
olane, Aldo Guzma´n-Sa´enz, Karthikeyan Natesan Ramamurthy, http://arxiv.org/abs/1608.04233.
TolgaBirdal,TamalKDey,SohamMukherjee,ShreyasNSam- Stephan Huckemann, Thomas Hotz, and Axel Munk. Intrinsic
aga,etal. Topologicaldeeplearning:Goingbeyondgraphdata. shapeanalysis:GeodesicPCAforriemannianmanifoldsmodulo
2023a. isometric lie group actions. Statistica Sinica, 20(1):1–58, 2010.
Mustafa Hajij, Ghada Zamzmi, Theodore Papamarkou, Nina Mi- ISSN10170405.33
Michael J. Hutchinson, Charline Le Lan, Sheheryar Zaidi, Em- Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter
ilien Dupont, Yee Whye Teh, and Hyunjik Kim. Lietrans- Wirnsberger,MeireFortunato,FerranAlet,SumanRavuri,Timo
former: Equivariant self-attention for lie groups. In In- Ewalds, Zach Eaton-Rosen, Weihua Hu, Alexander Merose,
ternational Conference on Machine Learning, 2020. URL Stephan Hoyer, George Holland, Oriol Vinyals, Jacklynn Stott,
https://api.semanticscholar.org/CorpusID:229340145. AlexanderPritzel,ShakirMohamed,andPeterBattaglia. Graph-
J.B. Ingraham, M. Baranov, and Z. et al. Costello. Illuminating cast:Learningskillfulmedium-rangeglobalweatherforecasting,
proteinspacewithaprogrammablegenerativemodel.2023.URL 2023.
https://doi.org/10.1038/s41586-023-06728-8. Nicholas W. Landry, Maxime Lucas, Iacopo Iacopini, Giovanni
John Ingraham, Vikas Garg, Regina Barzilay, and Tommi Petri, Alice Schwarze, Alice Patania, and Leo Torres. XGI:
Jaakkola. Generative models for graph-based protein A Python package for higher-order interaction networks. Jour-
design. 2019. URL https://papers.nips.cc/paper nal of Open Source Software, 8(85):5162, May 2023. doi:
files/paper/2019/hash/f3a4ff4839c56a5f460c88cce3666a2b-Abstract.htm1l.0.21105/joss.05162. URLhttps://doi.org/10.21105/joss.05162.
Kristopher T. Jensen, Ta-Chu Kao, Marco Tripodi, and Guillaume Neil Lawrence. Gaussian process latent variable models for
Hennequin.Manifoldgplvmsfordiscoveringnon-euclideanlatent visualisation of high dimensional data. In S. Thrun,
structureinneuraldata. InProceedingsofthe34thInternational L. Saul, and B. Scho¨lkopf, editors, Advances in Neural
Conference on Neural Information Processing Systems, NIPS Information Processing Systems, volume 16. MIT
’20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN Press, 2003. URL https://proceedings.neurips.cc/paper
9781713829546. files/paper/2003/file/9657c1fffd38824e5ab0472e022e577e-Paper.pdf.
Jianwen Jiang, Yuxuan Wei, Yifan Feng, Jingxuan Cao, and Yue Alice Le Brigant and Ste´phane Puechmorel. Optimal Riemannian
Gao.Dynamichypergraphneuralnetworks.InProceedingsofthe quantization with an application to air traffic analysis. arXiv
Twenty-EighthInternationalJointConferenceonArtificialIntel- preprintarXiv:1806.07605,2018.
ligence,IJCAI-2019.InternationalJointConferencesonArtificial Yann LeCun, Leon Botton, Yoshua Bengio, and Patrick Haffner.
IntelligenceOrganization,August2019.doi:10.24963/ijcai.2019/ Gradient-Based Learning Applied to Document Recognition.
366. URLhttp://dx.doi.org/10.24963/ijcai.2019/366. Proc.oftheIEEE,86(11):2278–2324,1998.
Richard A. Johnson and Thomas E. Wehrly. Some angular- Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin
linear distributions and related regression models. Journal of Choi, and Yee Whye Teh. Set transformer: A framework
the American Statistical Association, 73:602–606, 1978. URL for attention-based permutation-invariant neural networks. In
https://api.semanticscholar.org/CorpusID:122640093. Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Pro-
J. Jumper, R. Evans, and A. et al. Pritzel. Highly accurate pro- ceedings of the 36th International Conference on Machine
tein structure prediction with alphafold. Nature, 2021. URL Learning, volume 97 of Proceedings of Machine Learning Re-
https://doi.org/10.1038/s41586-021-03819-2. search, pages 3744–3753. PMLR, 09–15 Jun 2019. URL
SeungwooKangandHee-SeokOh.ProbabilisticPrincipalCurveson https://proceedings.mlr.press/v97/lee19d.html.
Riemannian Manifolds. IEEE Transactions on Pattern Analysis SeeLee,FengJi,andWeePengTay.Sgat:Simplicialgraphattention
and Machine Intelligence, 46(7):4843–4849, July 2024. ISSN network. pages 3167–3175, 07 2022. doi: 10.24963/ijcai.2022/
0162-8828, 2160-9292, 1939-3539. doi: 10.1109/TPAMI.2024. 440.
3357801. URLhttps://ieeexplore.ieee.org/document/10413614/. Adrien-MarieLegendre. Nouvellesme´thodespourlade´termination
Ryan Keisler. Forecasting global weather with graph neural net- desorbitesdescome`tes. 1805.
works,2022. M.M. Li, K. Huang, and M. Zitnik. Graph representation learning
Diederik P. Kingma and Max Welling. Auto-Encoding Variational inbiomedicineandhealthcare. NatureBiomedicine,2022a. URL
Bayes. In Proceedings of the 2nd International Conference on https://doi.org/10.1038/s41551-022-00942-x.
LearningRepresentations(ICLR),2014. Zhengyu Li, XUAN TANG, Zihao Xu, Xihao Wang, Hui
Thomas N. Kipf and Max Welling. Semi-supervised clas- Yu, Mingsong Chen, and xian wei. Geodesic self-
sification with graph convolutional networks. In Interna- attention for 3d point clouds. In S. Koyejo, S. Mohamed,
tional Conference on Learning Representations, 2017. URL A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors,
https://openreview.net/forum?id=SJU4ayYgl. Advances in Neural Information Processing Systems,
Max Kochurov, Rasul Karimov, and Serge Kozlukov. Geoopt: volume 35, pages 6190–6203. Curran Associates,
Riemannianoptimizationinpytorch,2020. Inc., 2022b. URL https://proceedings.neurips.cc/paper
Dimitrios Konstantinidis, Ilias Papastratis, Kosmas Dimitropou- files/paper/2022/file/28e4ee96c94e31b2d040b4521d2b299e-Paper-Conference.pdf.
los, and Petros Daras. Multi-manifold attention for vi- LizhenLin,BrianSt.Thomas,HongtuZhu,andDavidB.Dunson.
sion transformers. IEEE Access, 11:123433–123444, 2023. Extrinsic local regression on manifold-valued data. Journal of
ISSN 2169-3536. doi: 10.1109/access.2023.3329952. URL the American Statistical Association, 112(519):1261–1273, May
http://dx.doi.org/10.1109/ACCESS.2023.3329952. 2017. ISSN 1537-274X. doi: 10.1080/01621459.2016.1208615.
Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzade- URLhttp://dx.doi.org/10.1080/01621459.2016.1208615.
nesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima CongLiu,DavidRuhe,FloorEijkelboom,andPatrickForre´.Clifford
Anandkumar. Neuraloperator:Learningmapsbetweenfunction group equivariant simplicial message passing networks. arXiv
spaces with applications to pdes. Journal of Machine Learning preprintarXiv:2402.10011,2024.
Research,24(89):1–97,2023. Meng Liu, Youzhi Luo, Limei Wang, Yaochen Xie, Haonan Yuan,
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Im- Shurui Gui, Zhao Xu, Haiyang Yu, Jingtun Zhang, Yi Liu,
agenet classification with deep convolutional neural networks. Keqiang Yan, Bora Oztekin, Haoran Liu, Xuan Zhang, Cong
Communications of the ACM, 60:84 – 90, 2012. URL Fu, and Shuiwang Ji. Dig: A turnkey library for diving into
https://api.semanticscholar.org/CorpusID:195908774. graphdeeplearningresearch.ArXiv,abs/2103.12608,2021.URL
Line Ku¨hnel and Stefan Sommer. Stochastic development https://api.semanticscholar.org/CorpusID:232320529.
regression on non-linear manifolds. In Information Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Ar-
Processing in Medical Imaging, 2017. URL avindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey
https://api.semanticscholar.org/CorpusID:122942. Dosovitskiy,andThomasKipf. Object-centriclearningwithslot
SoumyabrataKunduandRisiKondor.Steerabletransformers.arXiv attention,2020.
preprintarXiv:2405.15932,2024. Marco Lorenzi and Xavier Pennec. Geodesics, Parallel Transport
Maxime W. Lafarge, Erik J. Bekkers, Josien P. W. Pluim, Remco & One-parameter Subgroups for Diffeomorphic Image Regis-
Duits,andMitkoVeta.Roto-translationequivariantconvolutional tration. International Journal of Computer Vision, 105(2):111–
networks:Applicationtohistopathologyimageanalysis,2020. 127, November 2013. doi: 10.1007/s11263-012-0598-4. URL34
https://hal.inria.fr/hal-00813835. Publisher:SpringerVerlag. Karl Pearson. Liii. on lines and planes of closest fit to systems of
Marco Lorenzi, Xavier Pennec, Giovanni B. Frisoni, and Nicholas pointsinspace. TheLondon,Edinburgh,andDublinphilosoph-
Ayache. Disentangling normal aging from Alzheimer’s icalmagazineandjournalofscience,2(11):559–572,1901.
disease in structural magnetic resonance images. Neu- Bruno Pelletier. Non-parametric regression estimation
robiology of Aging, 36:S42–S52, January 2015. ISSN on closed riemannian manifolds. Journal of
01974580. doi: 10.1016/j.neurobiolaging.2014.07.046. URL Nonparametric Statistics, 18:57 – 67, 2006. URL
https://linkinghub.elsevier.com/retrieve/pii/S0197458014005594. https://api.semanticscholar.org/CorpusID:17937994.
Youzhi Luo and Shuiwang Ji. An autoregressive flow model for X. Pennec, S. Sommer, and T. Fletcher. Riemannian Geo-
3d molecular geometry generation from scratch. In Interna- metric Statistics in Medical Image Analysis. Elsevier Sci-
tional Conference on Learning Representations, 2022. URL ence & Technology, 2019. ISBN 978-0-12-814725-2. URL
https://api.semanticscholar.org/CorpusID:251647192. https://books.google.com/books?id=k8qsDwAAQBAJ.
L. Machado, F. Silva Leite, and K. Krakowski. Higher- XavierPennec. IntrinsicStatisticsonRiemannianManifolds:Basic
order smoothing splines versus least squares problems on Tools for Geometric Measurements. Journal of Mathematical
Riemannian manifolds. Journal of Dynamical and Con- ImagingandVision,25(1):127–154,2006. ISSN0924-9907.
trol Systems, 16(1):121–148, January 2010. ISSN 1079- XavierPennec. Barycentricsubspaceanalysisonmanifolds. Annals
2724, 1573-8698. doi: 10.1007/s10883-010-9080-1. URL of Statistics, 46(6A):2711–2746, 2018. ISSN 00905364. doi:
http://link.springer.com/10.1007/s10883-010-9080-1. 10.1214/17-AOS1636.
LuısMachadoandFSilvaLeite. FittingSmoothPathsonRieman- XavierPennecandVincentArsigny. ExponentialBarycentersofthe
nianManifolds.nternationalJournalofAppliedMathematicsand CanonicalCartanConnectionandInvariantMeansonLieGroups.
Statistics,4,2006. InFredericBarbaresco,AmitMishra,andFrankNielsen,editors,
Elodie Maignant, Alain Trouve´, and Xavier Pennec. Riemannian MatrixInformationGeometry,pages123–168.Springer,52012.
Locally Linear Embedding with Application to Kendall Shape ISBN 978-3-642-30231-2 (Print) / 978-3-642-30232-9 (Online).
Spaces, page 12–20. Springer Nature Switzerland, 2023. ISBN doi:10.1007/978-3-642-30232-9.
9783031382710. doi: 10.1007/978-3-031-38271-0 2. URL XavierPennecandMarcoLorenzi. 5-BeyondRiemanniangeom-
http://dx.doi.org/10.1007/978-3-031-38271-0 2. etry: The affine connection setting for transformation groups.
Anton Mallasto and Aasa Feragen. Application de la methode des In Xavier Pennec, Stefan Sommer, and Tom Fletcher, editors,
moindrequarresal’interpolationdessuites.InAnnalesdesMath Riemannian Geometric Statistics in Medical Image Analysis,
Pures,pages5580–5588,2018. pages 169–229. Academic Press, January 2020. ISBN 978-0-
Jason D. McEwen, Christopher G. R. Wallis, and Augustine N. 12-814725-2. doi:10.1016/B978-0-12-814725-2.00012-1. URL
Mavor-Parker. Scattering networks on the sphere for scalable http://www.sciencedirect.com/science/article/pii/B9780128147252000121.
androtationallyequivariantsphericalcnns,2022. Xavier Pennec and Jean-Philippe Thirion. A Framework for
LelandMcInnes,JohnHealy,NathanielSaul,andLukasGroßberger. Uncertainty and Validation of 3D Registration Methods based
Umap:Uniformmanifoldapproximationandprojection. Journal on Points and Frames. Int. Journal of Computer Vision, 25(3):
of Open Source Software, 3(29):861, 2018. doi: 10.21105/joss. 203–229,December1997.doi:10.1023/A:1007976002485.URL
00861. URLhttps://doi.org/10.21105/joss.00861. http://www.springerlink.com/openurl.asp?genre=article&issn=0920-5691&volume=25&issue=3&spage=203.
MaciejMikulskiandJaroslawDuda. Toroidalautoencoder,2019. XavierPennec,PierreFillard,andNicholasAyache. ARiemannian
N.MiolaneandX.Pennec. Computingbi-invariantpseudo-metrics Framework for Tensor Computing. International Journal of
onliegroupsforconsistentstatistics.Entropy,17(4),2015.ISSN ComputerVision,66(1):41–66,12006.
10994300. doi:10.3390/e17041850. Alexander Petersen and Hans-Georg Muller. Fre´chet
N.Miolane,A.LeBrigant,B.Hou,C.Donnat,M.Jorda,J.Mathe, regression for random objects with euclidean pre-
X. Pennec, and S. Holmes. Geomstats: a python module for dictors. The Annals of Statistics, 2016. URL
computations and statistics on manifolds. Submitted to JMLR, https://api.semanticscholar.org/CorpusID:13666043.
2019. URLhttps://github.com/geomstats/geomstats. Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Pe-
NinaMiolaneandSusanHolmes. Learningweightedsubmanifolds ter W. Battaglia. Learning mesh-based simulation with graph
withvariationalautoencodersandriemannianvariationalautoen- networks,2021.
coders.InProceedingsoftheIEEE/CVFConferenceonComputer StephenM.Pizer,P.ThomasFletcher,SarangJoshi,AndrewThall,
VisionandPatternRecognition,pages14503–14511,2020. Jiahui Z. Chen, Yael Fridman, Daniel S. Fritsch, Grant Gash,
Nina Miolane, Frederic Poitevin, Yee-Ting Li, and Susan Holmes. JamesM.Glotzer,MarkR.Jiroutek,ChiwuLu,KeithE.Muller,
Estimation of orientation and camera parameters from cryo- George Tracton, Paul Yushkevich, and Edward L. Chaney. De-
electron microscopy images with variational autoencoders and formable m-reps for 3d medical image segmentation. Interna-
generativeadversarialnetworks.InProceedingsoftheIEEE/CVF tional Journal of Computer Vision, 55(2-3):85–106, November
ConferenceonComputerVisionandPatternRecognition(CVPR) 2003. doi:10.1023/a:1026313132218.
Workshops,June2020. BrendaPraggastis,SinanG.Aksoy,DustinArendt,MarkBonicillo,
PrasannaMuralidharan,JacobHinkle,andP.Fletcher.Amapestima- CliffAJoslyn,EmiliePurvine,MadelynRShapiro,andJiYoung
tionalgorithmforbayesianpolynomialregressiononriemannian Yun. Hypernetx: A python package for modeling complex
manifolds. pages 215–219, 09 2017. doi: 10.1109/ICIP.2017. networkdataashypergraphs.ArXiv,abs/2310.11626,2023.URL
8296274. https://api.semanticscholar.org/CorpusID:264288882.
E. A. Nadaraya. On estimating regression. Theory of Probability PyT-Team. Topobenchmarkx,2024.
anditsApplications,9:141–142,1964. Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J
Maximillian Nickel and Douwe Kiela. Poincare´ embeddings for Guibas. Pointnet++: Deep hierarchical feature learning
learning hierarchical representations. Advances in neural infor- on point sets in a metric space. In I. Guyon, U. Von
mationprocessingsystems,30,2017. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
Victor M. Panaretos, Tung Pham, and Zhigang Yao. Principal and R. Garnett, editors, Advances in Neural Information
flows. JournaloftheAmericanStatisticalAssociation,109(505): Processing Systems, volume 30. Curran Associates,
424–436, 2014. ISSN 1537274X. doi: 10.1080/01621459.2013. Inc., 2017. URL https://proceedings.neurips.cc/paper
849199. files/paper/2017/file/d8bf84be3800d12f74d8b05e9b89836f-Paper.pdf.
Mathilde Papillon, Sophia Sanborn, Mustafa Hajij, and Nina Mi- P. Rajpurkar, E. Chen, and O. et al. Banerjee. Ai in
olane. Architectures of topological deep learning: A survey on health and medicine. Nature Medicine, 2022. URL
topological neural networks. arXiv preprint arXiv:2304.10031, https://doi.org/10.1038/s41591-021-01614-0.
2023. Danny Reidenbach and Aditi S. Krishnapriyan. Coarsenconf:35
Equivariant coarsening with aggregated attention for molecular 10.1111/j.1467-8659.2008.01141.x.
conformergeneration,2023. Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin,
P. Reiser, M. Neubert, and A. et al. Eberhard. Graph neural Andres Cubillos-Ruiz, Nina M. Donghia, Craig R. MacNair,
networks for materials science and chemistry. 2022. URL Shawn French, Lindsey A. Carfrae, Zohar Bloom-Ackermann,
https://doi.org/10.1038/s43246-022-00315-6. Victoria M. Tran, Anush Chiappino-Pepe, Ahmed H. Badran,
T. Mitchell Roddenberry, Michael T. Schaub, and Mustafa Ha- IanW.Andrews,EmmaJ.Chory,GeorgeChurch,EricD.Brown,
jij. Signal processing on cell complexes. In ICASSP 2022 T. Jaakkola, Regina Barzilay, and James J Collins. A deep
- 2022 IEEE International Conference on Acoustics, Speech learningapproachtoantibioticdiscovery.Cell,180:688–702.e13,
and Signal Processing (ICASSP), pages 8852–8856, 2022. doi: 2020. URLhttps://api.semanticscholar.org/CorpusID:261621991.
10.1109/ICASSP43922.2022.9747233. JoshuaB.Tenenbaum,VindeSilva,andJohnC.Langford.AGlobal
FrankRosenblatt.Theperceptron:aprobabilisticmodelforinforma- Geometric Framework for Nonlinear Dimensionality Reduction.
tionstorageandorganizationinthebrain. Psychologicalreview, Science,290(5500):2319,2000.
65(6):386,1958. NathanielThomas,TessSmidt,StevenKearnes,LusannYang,LiLi,
Sam T. Roweis and Lawrence K. Saul. Nonlinear KaiKohlhoff,andPatrickRiley.Tensorfieldnetworks:Rotation-
Dimensionality Reduction by Locally Linear Em- andtranslation-equivariantneuralnetworksfor3Dpointclouds.
bedding. Science, 290(22):2323–2326, 2000. URL 2018. URLhttp://arxiv.org/abs/1802.08219.
http://www.robots.ox.ac.uk/∼az/lectures/ml/lle.pdf. MichaelETippingandChristopherMBishop.Probabilisticprincipal
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. component analysis. Journal of the Royal Statistical Society:
Williams. Learning representations by back- SeriesB(StatisticalMethodology),61(3):611–622,1999.
propagating errors. Nature, 323:533–536, 1986. URL James Townsend, Niklas Koep, and Sebastian Weichwald. Py-
https://api.semanticscholar.org/CorpusID:205001834. manopt: A python toolbox for optimization on manifolds using
Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex automaticdifferentiation. J.Mach.Learn.Res.,17:137:1–137:5,
Ying,JureLeskovec,andPeterW.Battaglia.Learningtosimulate 2016. URLhttps://api.semanticscholar.org/CorpusID:29194288.
complexphysicswithgraphnetworks,2020. Alain Trouve´. Diffeomorphisms Groups and Pattern Matching in
VıctorGarciaSatorras,EmielHoogeboom,andMaxWelling. E(n) Image Analysis. International Journal of Computer Vision, 28
equivariant graph neural networks. In International conference (3):213–221,1998. ISSN1573-1405.
onmachinelearning,pages9323–9332.PMLR,2021. Dimosthenis Tsagkrasoulis and Giovanni Montana. Random
VictorGarciaSatorras,EmielHoogeboom,andMaxWelling. E(n) forest regression for manifold-valued responses. Pattern
equivariantgraphneuralnetworks,2022. Recognition Letters, 101:6–13, January 2018. ISSN
Anna M M Scaife and Fiona Porter. Fanaroff–riley classification 0167-8655. doi: 10.1016/j.patrec.2017.11.008. URL
of radio galaxies using group-equivariant convolutional neural http://dx.doi.org/10.1016/j.patrec.2017.11.008.
networks.MonthlyNoticesoftheRoyalAstronomicalSociety,503 LaurensvanderMaatenandGeoffreyHinton.VisualizingDatausing
(2):2369–2379,February2021. ISSN1365-2966. doi:10.1093/ {t-SNE}. JournalofMachineLearningResearch,9:2579–2605,
mnras/stab530. URLhttp://dx.doi.org/10.1093/mnras/stab530. 2008.
Christof Scho¨tz. Nonparametric regression in nonstandard spaces. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
Electronic Journal of Statistics, 16(2):4679 – 4741, 2022. doi: Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia
10.1214/22-EJS2056. URLhttps://doi.org/10.1214/22-EJS2056. Polosukhin. Attention is all you need. In I. Guyon,
Kristof T. Schu¨tt, Pieter-Jan Kindermans, Huziel E. Sauceda, Ste- U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus,
fan Chmiela, Alexandre Tkatchenko, and Klaus-Robert Mu¨ller. S. Vishwanathan, and R. Garnett, editors, Advances in
Schnet: A continuous-filter convolutional neural network for Neural Information Processing Systems, volume 30. Curran
modelingquantuminteractions,2017. Associates,Inc.,2017.URLhttps://proceedings.neurips.cc/paper
Xiaoyan Shi, Martin Styner, Jeffrey Lieberman, Joseph G. files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
Ibrahim, Weili Lin, and Hongtu Zhu. Intrinsic BastiaanS.Veeling,JasperLinmans,JimWinkens,TacoCohen,and
Regression Models for Manifold-Valued Data, page Max Welling. Rotation equivariant cnns for digital pathology,
192–199. Springer Berlin Heidelberg, 2009. ISBN 2018.
9783642042713. doi: 10.1007/978-3-642-04271-3 24. URL Petar Velicˇkovic´, Guillem Cucurull, Arantxa Casanova, Adriana
http://dx.doi.org/10.1007/978-3-642-04271-3 24. Romero, Pietro Lio`, and Yoshua Bengio. Graph Attention Net-
GregorN.C.Simm,RobertPinsler,Ga´borCsa´nyi,andJose´Miguel works. International Conference on Learning Representations,
Herna´ndez-Lobato.Symmetry-awareactor-criticfor3dmolecular 2018. URLhttps://openreview.net/forum?id=rJXMpikCZ.
design,2020. ClementVignac,NaghamOsman,LauraToni,andPascalFrossard.
Martin Simonovsky and Nikos Komodakis. Graphvae: Towards Midi: Mixed graph and 3d denoising diffusion for molecule
generationofsmallgraphsusingvariationalautoencoders,2018. generation,2023.
Raphae¨l Sivera, Herve´ Delingette, Marco Lorenzi, Xavier Pennec, Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin
andNicholasAyache. Amodelofbrainmorphologicalchanges Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van
related to aging and Alzheimer’s disease from cross-sectional Katwyk,AndreeaDeac,AnimaAnandkumar,KarianneJ.Bergen,
assessments. NeuroImage,198:255–270,September2019. ISSN Carla P. Gomes, Shirley Ho, Pushmeet Kohli, Joan Lasenby,
10538119. doi: 10.1016/j.neuroimage.2019.05.040. URL JureLeskovec,Tie-YanLiu,ArjunK.Manrai,DeboraS.Marks,
https://linkinghub.elsevier.com/retrieve/pii/S105381191930432X. Bharath Ramsundar, Le Song, Jimeng Sun, Jian Tang, Petar
Stefan Sommer. Horizontal Dimensionality Reduction and Velickovic, Max Welling, Linfeng Zhang, Connor W. Coley,
Iterated Frame Bundle Development. In Frank Nielsen Yoshua Bengio, and Marinka Zitnik. Scientific discovery in the
and Fre´de´ric Barbaresco, editors, Geometric Science of age of artificial intelligence. Nature, 620:47–60, 2023. URL
Information, pages 76–83. Springer Berlin Heidelberg, https://api.semanticscholar.org/CorpusID:260384616.
2013. ISBN 978-3-642-40019-3 978-3-642-40020-9. URL Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang
http://link.springer.com/chapter/10.1007/978-3-642-40020-9 7. Song,JinjingZhou,ChaoMa,LingfanYu,YuGai,TianjunXiao,
Stefan Sommer, Franc¸ois Lauze, and Mads Nielsen. Optimization TongHe,GeorgeKarypis,JinyangLi,andZhengZhang. Deep
overgeodesicsforexactprincipalgeodesicanalysis.Advancesin graph library: A graph-centric, highly-performant package for
ComputationalMathematics,40(2):283–313,2014. graphneuralnetworks. arXivpreprintarXiv:1909.01315,2019.
FlorianSteinke,MatthiasHein,JanPeters,andBernhardScho¨lkopf. Minmin Wang. Height and diameter of brownian tree.
Manifold-valuedthin-platesplineswithapplicationsincomputer Electronic Communications in Probability, 20(none), January
graphics. Comput. Graph. Forum, 27:437–448, 04 2008. doi: 2015. ISSN 1083-589X. doi: 10.1214/ecp.v20-4193. URL36
http://dx.doi.org/10.1214/ECP.v20-4193.
Maurice Weiler et al. Equivariant and coordinate independent
convolutionalnetworks:Agaugefieldtheoryofneuralnetworks.
2024.
Christopher Williams and Carl Rasmussen. Gaussian
processes for regression. In D. Touretzky, M.C.
Mozer, and M. Hasselmo, editors, Advances in Neural
Information Processing Systems, volume 8. MIT
Press, 1995. URL https://proceedings.neurips.cc/paper
files/paper/1995/file/7cce53cf90577442771720a370c3c723-Paper.pdf.
ShiwenWu,FeiSun,WentaoZhang,XuXie,andBinCui. Graph
neural networks in recommender systems: A survey. ACM
Comput.Surv.,55(5),dec2022. ISSN0360-0300. doi:10.1145/
3535101. URLhttps://doi.org/10.1145/3535101.
ZonghanWu,ShiruiPan,FengwenChen,GuodongLong,Chengqi
Zhang,andPhilipS.Yu.Acomprehensivesurveyongraphneural
networks. IEEETransactionsonNeuralNetworksandLearning
Systems,32(1):4–24,2021. doi:10.1109/TNNLS.2020.2978386.
MinkaiXu,LantaoYu,YangSong,ChenceShi,StefanoErmon,and
Jian Tang. Geodiff: a geometric diffusion model for molecular
conformationgeneration,2022.
MinkaiXu,AlexanderPowers,RonDror,StefanoErmon,andJure
Leskovec. Geometric latent diffusion models for 3d molecule
generation,2023.
Yun Yang and David B. Dunson. Bayesian manifold regression.
The Annals of Statistics, 44(2):876 – 905, 2016. doi: 10.1214/
15-AOS1390. URLhttps://doi.org/10.1214/15-AOS1390.
Z. Yang, X. Zeng, and Y. et al. Zhao. Alphafold2 and its ap-
plications in the fields of biology and medicine. 2023. URL
https://doi.org/10.1038/s41392-023-01381-z.
Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai,
William L. Hamilton, and Jure Leskovec. Graph convolu-
tional neural networks for web-scale recommender systems.
In Proceedings of the 24th ACM SIGKDD International Con-
ference on Knowledge Discovery & Data Mining, KDD ’18.
ACM, July 2018. doi: 10.1145/3219819.3219890. URL
http://dx.doi.org/10.1145/3219819.3219890.
Laurent Younes. Shapes and diffeomorphisms. Number v. 171 in
Appliedmathematicalsciences.Springer,Heidelberg;NewYork,
2010. ISBN978-3-642-12054-1. OCLC:ocn632088149.
Liangliang Zhang, Yushu Shi, Robert R. Jenq, Kim-Anh Do, and
Christine B. Peterson. Bayesian compositional regression with
structured priors for microbiome feature selection. Biometrics,
77(3):824–838,July2020. ISSN1541-0420. doi:10.1111/biom.
13335. URLhttp://dx.doi.org/10.1111/biom.13335.
Miaomiao Zhang and P. Thomas Fletcher. Probabilistic principal
geodesic analysis. Advances in Neural Information Processing
Systems,pages1–9,2013. ISSN10495258.