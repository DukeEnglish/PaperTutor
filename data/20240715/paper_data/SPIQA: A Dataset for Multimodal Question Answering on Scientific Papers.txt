SPIQA: A Dataset for Multimodal
Question Answering on Scientific Papers
ShramanPramanick⋆1,2⋄ RamaChellappa2 SubhashiniVenugopalan⋆1†
1GoogleResearch 2JohnsHopkinsUniversity
https://huggingface.co/datasets/google/spiqa
https://github.com/google/spiqa
Abstract
Seekinganswerstoquestionswithinlongscientificresearcharticlesisacrucial
area of study that aids readers in quickly addressing their inquiries. However,
existingquestion-answering(QA)datasetsbasedonscientificpapersarelimitedin
scaleandfocussolelyontextualcontent. Toaddressthislimitation,weintroduce
SPIQA (Scientific Paper Image Question Answering), the first large-scale QA
dataset specifically designed to interpret complex figures and tables within the
contextofscientificresearcharticlesacrossvariousdomainsofcomputerscience.
Leveragingthebreadthofexpertiseandabilityofmultimodallargelanguagemod-
els(MLLMs)tounderstandfigures,weemployautomaticandmanualcurationto
createthedataset. Wecraftaninformation-seekingtaskinvolvingmultipleimages
thatcoverawidevarietyofplots,charts,tables,schematicdiagrams,andresult
visualizations. SPIQA comprises 270K questions divided into training, valida-
tion, and three different evaluation splits. Through extensive experiments with
12prominentfoundationalmodels,weevaluatetheabilityofcurrentmultimodal
systemstocomprehendthenuancedaspectsofresearcharticles. Additionally,we
proposeaChain-of-Thought(CoT)evaluationstrategywithin-contextretrieval
thatallowsfine-grained,step-by-stepassessmentandimprovesmodelperformance.
Wefurtherexploretheupperboundsofperformanceenhancementwithadditional
textualinformation,highlightingitspromisingpotentialforfutureresearchandthe
dataset’simpactonrevolutionizinghowweinteractwithscientificliterature.
1 Introduction
Surfacingpertinentinformationwithinthecontextofacademicresearcharticlesisanessentialareaof
study,asitempowersstudentsandresearcherstoefficientlyaddresstheirquerieswhicharenaturally
triggered when reading a scientific paper. However, existing question-answering (QA) datasets
anchoredonacademicarticlesarelimitedintermsofscale[70,20,29,14,34]. Thislimitationarises
duetothecomplexityandcostassociatedwithcuratingquestions,asunderstandingsucharticles
demandsdomain-specificexpertise,adetailedunderstandingofthetopic,andasignificantamountof
time. Additionally,priorQAdatasetsinthisdomainonlyanalyzetheabstracts,conclusion[57,20]
andthetextualparagraphs[56,54,66,29,14,63,34]ofscientificarticles,overlookingthewealth
ofinformationpresentedinmeticulouslycraftedfiguresandtables,andhence,failtoleverageand
analyzetherich,multidimensionaldataembeddedinthesevisualelements,whicharecrucialfora
comprehensiveunderstandingoftheresearchpresented.
NumerousdatasetshavebeencuratedtoevaluatetheQAabilitiesofLargeLanguageModels(LLMs)
onvariousdocuments,includingfactualdocuments[80,60,21,81,31,12,65,67],bookchapters
[27,28,49],newsarticles[69,35]andmore. However,understandingscientificpapersposesunique
challengesasthesystemsmustcomprehendunderlyingtheoreticalimplicationswithdomain-specific
terminologiesandverifyclaimswithexperimentalresultsandvisualizations. Therearealsoseveral
datasetsthatfocusonthecomprehensionofstandalonesciencediagrams[26,23,17],mathematical
⋆equaltechnicalcontribution,⋄workdoneasastudentresearcheratGoogleResearch.
†Correspondingauthor(vsubhashini@google.com)
Preprint.Underreview.
4202
luJ
21
]LC.sc[
1v31490.7042:viXraFigure1: IllustrationoftheSPIQAtasks. Givenaquestionanchoredinfiguresfromaresearch
paper,weevaluatethecapabilitiesofmultimodalLLMsincomprehendingandintegratinginformation
acrossmultiplefigures,tablesandpapertext.
figures [43, 82], charts [5, 46, 47], plots [48], and tables [58, 71, 9, 8, 11, 25, 50]. However,
simultaneouslyreasoningoverallfiguresandassociatedtextinascientificarticlenecessitatesboth
multimodalandlong-contextcapabilities.
In contemporary research, figures and tables with associated captions are crucial to understand
themotivationandcontributionsofawork. Comprehendingsuchmultimodalcontentpresentsa
significantreal-worldchallenge. Inthiswork,weintroduceSPIQA(ScientificPaperImageQuestion
Answering),thefirstlarge-scaleQAdatasetspecificallydesignedtointerpretcomplexfiguresand
tablesinthecontextofscientificresearcharticlesacrossvariousdomainsofcomputerscience. We
also develop three well-designed tasks using the SPIQA dataset to assess growing long-context
capabilitiesofMLLMs: (1)DirectQAwithfiguresandtables,wherethesystemsrequiretoanswer
questionsafterseeingallfiguresandtablesfromascientificpaper,(2)DirectQAwithfullpaper,
wherethesystemsanalyzethewholepapertextalongwithfiguresandtablestoanswerquestionsand
(3)CoTQA,achainofthought(CoT)retrievalbasedQAwheresystemsneedtofirstidentifyhelpful
figuresandthenanswerthequestion,allowingtoevaluatethemodelsforfine-grainedreasoningand
groundingcapability. WecollectthePDFsandsourceTeXsof26Kpapersfromvariousdomainsof
computersciencepublishedintop-tieracademicconferencesandgenerate270Kquestion-answer-
rationaletripletsfocusingonthepapers’figuresandtables.Wefurtherfilterandaugmenttwoexisting
scientificQAdatasets,QASA[34]andQASPER[14],toidentifyquestionsrequiringreasoningover
figures,tables,andtextualparagraphs. Overall,SPIQAcontainsonetraining,onevalidation,and
threeevaluationssplitwithvaryingdifficultylevels,allowingustofine-tuneandassessthecapabilities
oflargemultimodalsystemstounderstandcomplexscientificresearchpapers. Anillustrationofthe
threeproposedtaskswithexamplepromptsispresentedinFigure1.
WeconductextensiveexperimentsontheSPIQAdatasetsevaluatingthecomprehensionabilities
of several closed large multimodal models, and state-of-the-art open-source models, including
Gemini[61,68],GPT4[52,1],Claude-3[2],LLaVA1.5[38],InstructBLIP[13],XGen-MM[62],
InternLM-XC[15], SPHINX-v2[16]andCogVLM[75]. Wefurtherfine-tuneInstructBLIPand
LLaVA1.5ontheSPIQAtrainingsetandobservesignificantimprovementcomparedtozero-shot
evaluation, indicating potential pathways for designing specialized systems for scientific QA in
thefuture. InadditiontoreportingperformancesontraditionalQAmetrics,weintroduceanovel
LLM-basedevaluationmetric,LLMLogScore(L3Score),whichincorporatestheconfidenceofLLMs
forassessingtheequivalenceofanswerswiththeground-truthsbasedonthelog-likelihoodtoken
probabilities. WedemonstratetheeffectivenessofL3Scoreforautomatedfree-formQAevaluation
overexistingLLM-basedscores[45,86,33]whichusesensitiveratingscales.
In summary, our contributions are: (i) We curate SPIQA, the first large-scale QA dataset specif-
icallydesignedtointerpretcomplexfiguresandtablesinthecontextofscientificresearchpapers
acrossvariouscomputersciencedomains. (ii)Wedevelopthreewell-designedtasksfordirectand
retrieval-basedCoTquestionansweringtoaccessbaselinesystems’step-by-stepfine-grainedreason-
ingcapabilities. (iii)WeproposeLLMLogScore(L3Score),anovelLLM-basedevaluationmetric
forfree-formQA.L3ScoreincorporatestheconfidenceofLLMstoevaluatethequalityofcandidate
answersusinglog-likelihoodtokenprobabilities. (iv)Weperformextensiveexperimentstodemon-
2stratethevalueofSPIQAandLLMLogScoretoassessmultimodalandlong-contextcapabilitiesof
severalclosedandopen-sourcedMLLMs.
2 RelatedWorks
Question-answeringonlongdocumentsisachallengingreal-worldtaskthathasattractedincreasing
attention in recent years, following the success of the long-context reasoning ability of LLMs
[39, 10, 68, 61, 52, 2, 36]. Though there exist numerous general-domain document QA datasets
[80,60,21,81,31,12,65,67,74,59,79,84,73,69,35],understandingscientificpapersrequires
domain-specificexpertiseandreasoningcapability,andhenceposesamoresignificantchallenge.
DatasetsforQAonScientificPapers. Intheearlydays,cloze-styleacademicpaperQAdatasets
[56,54,77,66,57]wereautomaticallyconstructedbyextractingentitiesandrelationsandmatching
themwithstructureknowledgeresources. Thequestionsinsuchdatasetsfollowapre-definedformat;
hence,theyareunsuitableforreal-worldusagewherethereaderasksdetailedopen-endedquestions
[31]. Toovercometheseissues,PubMedQA[20],BioAsq[29]andQASPER[14]constructcorpora
of1K,3.2K,and5Khuman-writtenquestions,respectively. However,theannotatorsinthesedatasets
onlyreadtheabstractswhenwritingthequestions,andhence,mostquestionsaresimpleandcan
beanswerableinyes/noformatorwithshortextractivespans. ArgSciChat[63]proposesadataset
ofargumentivedialoguesbetweenscientistson20NLPpapers. Closertoourwork, QASA[34]
generates1798free-formadvancedquestionsonAI/MLpaperswheretheannotatorsreadthewhole
paper. However, QASA questions are answerable just from the text paragraphs, neglecting the
structuredinformationpresentintermsoffiguresandtables.
DatasetsforQAonScientificDiagrams. Solvingmathematicalproblemsinavisualcontexthas
emerged as a complex reasoning task for MLLMs. Prior attempts, such as GeoQA [7], UniGeo
[6],andGeometry3K[44],haveexclusivelyfocusedonsolvinggeometry-orientedquestions. Ina
differentlineofresearch,datasetslikeDVQA[22],LEAF-QA[5],LEAFQA++[64],FigureQA[23],
PlotQA[48],andChartQA[46]havebeenconstructedtosolveplotandchart-orientedquestions.
Additionally,therearedatasetsforQApurelyontabulardata,includingWTQ[58],TableQA[71],
SQA[19],HiTab[11],AIT-QA[25],FetaQA[50],MultiTabQA[53]. Morerecently,MathVista[43],
andMathVerse[82]haveintegrateddifferentscientificdiagramstodevelopbenchmarkswithawider
varietyoftasks. However,allexistingdatasetsfocusonaskingquestionsaboutstandalonefiguresor
tables. SPIQAbridgesthisgapbyproposingthefirstscientificQAbenchmarkthatincludesquestions
thatrequiresimultaneousreasoningoverfigures,tables,andtextualparagraphs,allowingamore
integratedunderstandingofscientificdocuments.
3 SPIQADatasetandTasks
3.1 CollectionGuidelinesandTaskFormulation
Existing scientific QA benchmarks [56, 66, 54, 20, 29, 14, 63, 34] primarily focus only on text
fromthemainbodyofthepaper,overlookingthewealthofinformationpresentedasfiguresand
tables. Further,curatingQAanchoredinresearcharticlesrequiresdomainexpertiseandadetailed
understandingofthepaper,makingannotationsexpensiveandresultinginsmaller-scaledata. Our
dataset,SPIQA,bridgesthisgapbysystematicallycuratingalarge-scaleQAbenchmarkfocusingon
everyaspectofscientificresearchdocuments-maintext,figures,tables,andtheircaptions,andthus
pushingAIsystemstowardsarobustunderstandingofresearcharticles.Moreover,weannotatewhich
figuresandtableshelpansweraquestiontoevaluatethegroundingandCoTreasoningcapabilitiesof
largemultimodalsystems.
While collecting and annotating the SPIQA benchmark, we adhere to the following collection
guidelines: (i) We identify 19 different top-tier academic research conferences covering a wide
varietyofcomputersciencedomainswherethepapersarelicensedpermissively. (ii)Wecollect27K
PDFsandcorrespondingTeXsourcesofresearchpaperspresentedinthoseconferencesbetween
2018-2023. Usingpeer-reviewedarticleshelpsustomaintainthequalityofSPIQA.TheTeXsources
providehigh-resolutionfiguresandtableTeXs,whichcannotdirectlybeextractedfromthePDFs.
(iii)Wecuratequestionsofvaryinglevelsofdifficultybasedonthecollectedpapers,requiringrobust
long-contextunderstandingofdifferentkindsoffiguresandtablesassociatedwiththeircaptionsand
themainbodyofthepaper. (iv)Lastly,weidentifysubsetsofquestionsintwoexistingdatasets,
QASA[34]andQASPER[14],whichrequireunderstandingoffiguresandtableswiththemaintext,
3Statistics Numbers
T Po ut ba ll isp ha ep der bs
etween
2012 85, -85 29
023
9%ICLR
T To ot ta al lt fia gb ule res
s
1 11 57 2, ,7 40 87
7 IC5
MI %J LCAI
A 4I 6/M %L
Neu 1r 6I %PS
Figuresubcategories
-Schematics 45396
-Plotsandcharts 72327 1A 1C %L 1N 9L %P Vision WACV
- -
T
M A
AMoV O
v
vaat
e
ei t
a
xxs
r
rh
l
ii
a
au e
m
mg
g
ga r
e
e
el s uui
n
m
mz
q
aea nurt
q
a
sa
eio
n
wut
sen
s
te
eid
ws
s
o
rtQ
nei lo
erA
ln
nele
gs
nl ne
tg
hgn thtg hth
22
1
176
1
38
2
406
9
31
.
.16
9
54
30 91
8
63
4 EMN 7L %P
ICASSP
9%
O 1t 8h
K%
D
Ders
W
ebConfSIG1 IR7% 9%CVPRI 5C %CV
Table 1: Statistics of the Figure2: Sourceofthecollectedpapersanddistributionoffigures
collected research papers andtablesperpaper. Wecollectpapersfrom19top-tierconferences
andgeneratedquestions. incomputersciencepublishedbetween2018and2023.
andtreatthemastwoadditionalevaluationsplitsofSPIQA.Weformulatethreenoveltasksforthe
comprehensiveevaluationofvariousQAsystemsonSPIQA:
• DirectQAwithFiguresandTables: Inthistasksetup,weprovideallfiguresandtableswiththeir
captionsfromapaper. Thesystemsarethenrequiredtoanswerquestionsthatnecessitatereasoning
withoneormorefiguresandtables.
• DirectQAwithFullPaper: Here,weprovidetheentirepaper,includingthemaintext,figures,
andtableswithcaptions,andaskthesystemstoanswerquestions. Whilethepapertexthelpsby
providingadditionalinformation,thesystemsrequirestronglong-contextunderstandingcapability
toreasonoverfulltext.
• CoTQA:Toassessthestep-by-stepreasoningabilitiesofMLLMs,CoTQArequiressystemsfirst
toidentifyrelevantfiguresandtablesandthenanswerthequestion. Forsimplicity,CoTQAdoes
notinvolvefullpapertext. Detailedpromptsforeachtaskareprovidedinthesupplement.
3.2 SPIQA:DataCollection,QuestionGenerationandFiltering
CollectionofPaperPDFsandTeXSources. Webeginbycollectingalargecorpusofscientific
researchpapersacrossalldomainsofcomputerscience,focusingonopen-sourcepublications. We
identified19top-tierconferences,asdetailedinFigure2,andgatheredthepaperPDFspublished
atthesevenuesbetween2018and2023. Wethenfilteredthiscollectiontoincludepaperswhose
TeXsourcescouldbedownloadedusingthepythonarXivAPI1.Thisprocessresultedin25,859
peer-reviewedpaperswithcorrespondingTeXsources,whichprovidehigh-resolutionfigures,table
TeXs,andthemainbodyofthearticles. WeadditionallyusePDFFigures2.02tocropfiguresfrom
the paper to account for figures that are missed when processing the TeX sources. Overall, we
gather152,487figures,117,707tables,andtheircaptionsfrom25,859paperswithcorresponding
maintext. Itisworthnotingthatduetoourstructuredstep-by-steppapercollectionstrategy, we
willhavetheopportunitytoexpandSPIQAinthefuturewithpaperspublishedinlateryearsandin
otheropen-sourcedresearchfields. Table1showsadetailedclassificationofthecollectedfiguresin
granularsubcategories.
AutomaticQuestionGenerationandFiltering. Aftergatheringthemaintext,figures,andtables
fromresearchpapers,thenextstepistogeneratehigh-qualityquestion-answerpairsthatcoverall
aspectsofthearticles. Manuallyannotatingqualityquestionsrequiresdomainexpertiseandadeep
understandingoftheresearchpapers,resultinginexistinghuman-annotateddatasetsbeingsmall-scale
orsolelybasedonabstracts[70,20,29,14,34]. Tobridgethisgap,weautomaticallygenerateQAs
byleveragingrecentadvancesinpowerfulmulti-modallargelanguagemodels. Wefirstconducteda
pilotstudybyselecting30papersfromvariousdomainsandexperimentingwithmultiplemodels.
Inourapproach,wepresentedonefigureortabletothemodel,alongwithpassagesreferencingthe
figure. Wethenaskedthemodelstogenerateaquestion,answer,andrationalethatrequiresaholistic
understandingofthefigureortableinthecontextofthepaper. Wemanuallyverifiedthequalityof
1pythonarXivAPI:https://github.com/lukasschwab/arxiv.py
2PDFFigures2.0:https://github.com/allenai/pdffigures2
4
14% AAAI#Figures Table2: Split Statistics of SPIQA.
Split #Papers #Ques. #Tables
Sche. Plots&Charts Vis. Others. Train, val, test-A contains LLM-
Train 25,459 262,524 44,008 70,041 27,297 6,450 114,728 generatedQAs;test-Bandtest-Chave
Val 200 2,085 360 582 173 55 915
human-writtenQAs.Wereportthenum-
test-A 118 666 154 301 131 95 434
test-B 65 228 147 156 133 17 341 beroftablesandfiguresineachsplit.
test-C 314 493 415 404 26 66 1332
thegeneratedQAsbyeachmodelusingthefollowingcriterion: (i)Answeringthequestionwould
requireacompleteunderstandingofthefigureanditsimportanceinthepaper. (ii)Thegenerated
answeriscorrectandtothepoint. (iii)Thequestionisneithertootrivialnortoospecifictothefigure
ortable. OurpilotstudyresultedinGemini1.5ProbeingthebestmodelforgeneratingtheQAs,and
wegenerated270,194QAsover25,859papersusingGemini. Inthesupplementarysection,weshow
ourfinalQAgenerationpromptandexamplesofQAsgeneratedbydifferentmodels.
After generating QAs, we divide the dataset into three splits: 200 papers for evaluation, 200 for
validation, and the remaining 25,459 for training, ensuring each split consists of papers from all
domains. Despitetheimpressivequestionquality, weperformadditionalmanualfilteringonthe
evaluationsettoexcludeanyinconsequentialandincorrectQAs. Inadditiontothethreecriteriaof
thepilotstudy,thefilteringguidelinecontainsthefollowingstandards: (iv)Iftwoormorequestions
fromapaperaresimilar,keepone. (v)Ifthequestionisentirelybasedonthepassage,discardthe
QA.(vi)Iftheanswerisnotclear,e.g.,theanswersays‘Itishardtoanswerthequestionbasedon
thegiveninformation‘or‘Theanswerisnotevidentfromthegivenpassage‘,discardtheQA.(vii)
IftheQAincludesphraseslike‘Basedonthepassage,’modifyitbecauseweshowallfiguresand
tablestothemodelatonceduringevaluation. Theuserinterface(UI)anddetailedguidelinesfor
filteringwithexamplesarepresentedinthesupplementary. Sinceonlyaround11%ofquestionsare
discardedduringfilteringtheevaluationset,wedonotperformadditionalfilteringonthetrainand
validationsplits. Afterfiltering,thefinalevaluationsetcontains666questions,andwerefertothis
splitastest-A.
AdditionalEvaluationSetswithHumanWrittenQAs. Sincethequestionsandanswersintest-A
areautomaticallygeneratedbyGemini,todeveloparobustandcompleteQAbenchmarkonscientific
articles,wecuratetwoadditionaltestsetswithhuman-writtenQAsbyutilizingtwoexistingdatasets,
QASA[34]andQASPER[14]. However,thehumanannotatorsforthesedatasetsmainlyconsidered
thepapertext. Weaimtoidentifythequestionsinthesedatasetsthatrequirereasoningwithfiguresor
tablesforcomprehensiveanswers. Foreveryquestion,bothQASAandQASPERprovideevidence
sentencesfromthepaper,whicharecrucialforanswering. Weparsedtheseevidencesentencesto
lookforphraseslike‘AccordingtoFigurex,’‘AsshowninTabley,’‘Figurezpresents,’andseparate
suchquestions,andtreatthecorrespondingfigureandtablesasevidence. WecollectthePDFsforthe
papersinQASAandQASPERfromarXiv,andextractthefiguresusingPDFFigures2.0. Weconduct
additionalmanualfilteringtoverifythattheidentifiedevidencefigureshelpanswerthequestions.
Thisprocessresultedin228questionsfromQASAand493questionsfromQASPER,wherefigures
ortablesareessentialforanswering. WerefertothesesetsgeneratedfromQASAandQASPERas
test-Bandtest-C,respectively. TheUIforfilteringisshowninthesupplementary.
3.3 DatasetAnalysis
Splits&Statistics. Themainstatisticsofthecollectedpapers,generatedquestions,anddatasplits
arepresentedinTables1and2. SPIQAencompasses25,859computersciencepaperspublished
intop-tierconferencesbetween2018-2023,containing152,487figuresand117,707tables. Figure
2showsthedistributionoffiguresandtablesineverypaper. Wegenerateatotalof270,194QAs
focusingonthefiguresandtablesalongwiththemaintextofthepapers. Thequestionsandanswers,
on average, contain 12.98 and 14.56 words, respectively. Notably, we observe high variances in
theirlengths-20.47forquestionsand243.29foranswers-indicatingadiverserangeofpatternsin
SPIQA.Thetraining,validation,andtest-Asplitsincludepapersfromeverysourceconferenceand
ensurethatquestionsfromthesamepaperremaininthesamesplit. Thetest-Bandtest-Csplitsare
generatedfromQASA[34]andQASPER[14]andcontainhuman-writtenQAs. Examplesfromthe
dataset,illustratedinFigure4,highlighttwodifferentquestionscenteredondifferenttypesoffigures.
Additionally,weprovideacomprehensivecomparisonofSPIQAwithexistingscientificQAdatasets
inthesupplementarymaterials.
Granularity. Wedividethecollectedfiguresinfourbroadcategories-schematicdiagrams,plots&
charts,visualizationsandothers. Table2presentsnumberoffiguresineachcategoryacrossallsplits.
Table5reportsperformanceofbaselinemodelsonalltypesoffigureandtablesfromtest-A.
5SPIQAtest-A SPIQAtest-B SPIQAtest-C
Method
M R-L C B-F1 L3S M R-L C B-F1 L3S M R-L C B-F1 L3S
Zero-shotClosed-WeightMLLMs
GeminiProVision[68] 22.9 38.3 124.6 64.87 43.85 9.9 19.0 29.1 54.83 31.84 11.6 19.4 47.8 48.95 31.98
Gemini1.5Flash[61] 25.4 38.8 110.9 65.84 54.20 11.5 19.4 24.4 56.32 36.04 14.4 18.1 45.5 48.79 36.67
Gemini1.5Pro[61] 23.4 35.5 87.1 64.36 53.49 10.8 19.3 26.8 56.62 43.27 12.6 16.8 40.2 47.51 36.72
Claude3(Opus)[2] 25.0 41.5 120.2 65.84 61.26 12.7 19.2 17.0 57.03 49.54 15.5 29.7 92.6 52.35 43.88
GPT-4Vision[1] 23.1 37.7 113.8 64.01 56.67 12.2 18.8 23.7 55.09 43.62 15.2 22.9 75.5 51.02 40.85
GPT-4o[52] 25.5 42.2 133.7 66.14 64.00 10.7 18.9 31.8 53.73 46.22 15.6 31.3 98.4 53.57 46.68
Zero-shotOpen-WeightMLLMs
SPHINX-v2[16] 2.6 3.2 13.4 6.25 3.34 0.1 0.3 0.4 2.08 1.65 1.0 3.3 11.0 8.03 3.32
InstructBLIP-7B[13] 9.5 18.9 62.6 47.70 7.50 3.5 9.5 16.3 39.62 7.07 2.8 15.5 36.6 48.45 8.79
LLaVA-1.5-7B[38] 22.6 34.7 117.8 61.61 13.86 7.7 15.5 16.8 47.21 9.63 7.0 15.1 26.7 45.55 9.53
XGen-MM[62] 17.3 30.6 127.0 58.41 13.74 4.4 8.0 11.1 35.49 8.18 4.2 17.4 46.4 45.25 10.66
InternLM-XC[15] 22.2 29.2 73.7 53.57 18.28 8.1 12.9 16.8 36.00 12.47 8.5 11.4 20.5 34.58 11.84
CogVLM[75] 20.4 27.9 59.2 51.24 16.89 7.9 16.0 26.2 43.93 9.60 9.7 13.9 24.4 42.90 12.52
Fine-tunedMLLMs
InstructBLIP-7B[13] 17.8 32.5 110.0 62.10 43.90 8.8 17.2 28.6 52.79 31.82 10.1 22.8 69.8 50.22 33.48
∆InstructBLIP-7BFT-ZS 8.3↑ 13.6↑ 47.4↑ 14.40↑ 36.40↑ 5.3↑ 7.7↑ 12.3↑ 13.17↑ 24.75↑ 7.3↑ 7.3↑ 33.2↑ 1.77↑ 24.69↑
LLaVA-1.5-7B[38] 23.8 36.0 121.2 63.74 45.45 11.0 18.4 29.5 53.13 33.50 10.5 24.1 69.6 50.15 32.40
∆LLaVA-1.5-7BFT-ZS 1.2↑ 1.3↑ 3.4↑ 2.13↑ 31.59↑ 3.3↑ 3.1↑ 12.7↑ 5.92↑ 23.87↑ 3.5↑ 9.0↑ 42.9↑ 4.60↑ 22.87↑
Table3: Performanceofzero-shotandfine-tunedsystemsondirectQAwithfiguresandtables.
GPT-4oachievesthestate-of-the-artresultsontest-Aandtest-C,whileClaude-3performssimilarly
wellasGPT-4oontest-B.M:METEOR,R-L:ROUGE-L,C:CIDEr,B-F1: BERTScoreF1andL3S:
L3Score. ∆showsimprovementsafterfine-tuning.
4 LLMLogScore(L3Score): AnImprovedMetricforFree-formQA
Evaluatingfree-formQAischallengingbecausetheanswersareoftendescriptiveandlackaprede-
finedformat. CurrentLLMsgeneratevariedanddetailedresponsesthatmayappeardifferentbut
are still accurate, which traditional QA evaluation metrics such as BLEU [55] and ROUGE [37]
oftenfailtocapture. InspiredbytheabilityofLLMstoevaluatenaturallanguagegeneration(NLG)
[85,40,24],threerecentapproaches,LAVE[45],LIMA[86],andPrometheus-Vision[33],utilize
thein-contextcapabilityofinstruction-tunedLLMstoratecandidateanswerson3,6,and5-point
scales,respectively. However,thesemetricsarehighlysensitivetothechosenscalerangeanddonot
considertheLLM’sconfidenceintheprovidedratings.
Wealleviatethenecessityofapredefinedscalerangeanddetailedinterpretationofeverypointinthe
scalebyproposingLLMLogScore(L3Score),whichdirectlyusesthelog-likelihoodprobabilities
generatedbyanLLMforevaluatingtheanswers. WeuseGPT-4o[52]asourLLM,showthemodel
thecandidateandtheground-truthanswers,andaskifthesemanticmeaningofthecandidateandthe
ground-trutharesimilarinthecontextofthequestion. TheLLMisexpectedtoansweryesornoina
singleword. WeshowtheexactpromptusedforcalculatingL3Scoreinthesupplementary. Next,
insteadofconsideringthefinalresponsebytheLLM,welookintothetop-53logprobabilitiesand
correspondingtokens,andwedefinetheL3Scoremetricasfollows:
exp(l )
L3Score=softmax(x) = yes (1)
yes exp(l )+exp(l )
yes no
exp()istheexponentialandsoftmax()isthesoftmaxoperation,l andl arethelogprobability
yes no
forthetokens‘yes’and‘no’andxrepresentsthevector[l ,l ]. EssentiallyL3Scorerenormalizes
yes no
theprobabilitiesoftokens‘yes’and‘no’tosumto1. However,duetothecaveatthatweuseaclosed
model,onlythetop-5logprobabilitiesareavailabletous,hencewemayneedtoapproximatethelog
probabilityifoneorbothtokensaremissingfromthetop-5. Wedoitasfollows
1. Ifneitherl orl isinthetop-5,L3Score=0.
yes no
2. Ifonlyoneofl orl ispresent,thenweapproximatethel forthecomplementarymissing
yes no xc
token(denotedx ),byconsideringtheminimum(min)ofthetokenwiththelowestprobability
c
(p )inthetop-5vs. logoftheprobabilitymassremaining(p )excludingthetop-5.
low rem
Ifthetop-5logprobabilitiesindescendingorderarel i∈{0,1,2,3,4}. Letx representthetoken
xi k
(either‘yes’or‘no’)thatispresentinthetop-5,withitscorrespondinglogprobabilitydenotedasl .
xk
3Wechoosen=5fortop-nlogprobabilitiesasthisisthehighestvalueofnreturnedbytheOpenAIAPI.
6SPIQAtest-A SPIQAtest-B SPIQAtest-C
Method Ret. QA Ret. QA Ret. QA
Acc. M R-L C B-F1 L3S Acc. M R-L C B-F1 L3S Acc. M R-L C B-F1 L3S
Gemini1.5Flash[61] − 25.4 38.8 110.9 65.84 54.20 − 11.5 19.4 24.4 56.32 36.04 − 14.4 18.1 45.5 48.79 36.67
w/FullPaper − 27.1 41.5 125.2 69.20 58.12 − 13.7 24.1 53.4 59.95 37.42 − 14.8 18.9 52.0 49.77 37.25
w/CoT 86.18 26.0 39.6 120.5 68.11 56.57 57.45 10.9 19.5 26.5 57.75 35.75 69.37 13.1 18.5 47.8 49.36 34.28
Gemini1.5Pro[61] − 23.4 35.5 87.1 64.36 53.49 − 10.8 19.3 26.8 56.62 43.27 − 12.6 16.8 40.2 47.51 36.72
w/FullPaper − 27.0 40.4 116.8 69.05 61.80 − 13.5 22.3 34.8 59.18 47.51 − 13.2 17.5 54.3 49.25 39.48
w/CoT 85.88 25.6 38.7 99.5 67.15 64.68 62.28 11.2 18.6 27.0 56.46 47.38 70.79 14.6 18.7 55.7 49.79 41.12
GPT-4Vision[1] − 23.1 37.7 113.8 64.01 56.67 − 12.2 18.8 23.7 55.09 43.62 − 15.2 22.9 75.5 51.02 40.85
w/FullPaper − 27.0 39.5 128.7 67.24 62.45 − 14.8 22.1 28.3 58.33 46.63 − 15.6 23.8 94.8 52.46 42.28
w/CoT 83.25 25.6 38.8 94.6 66.92 63.37 60.45 11.6 20.0 27.4 57.82 45.35 66.73 14.5 19.4 56.5 50.43 43.83
GPT-4o[52] − 25.5 42.2 133.7 66.14 64.00 − 10.7 18.9 31.8 53.73 46.22 − 15.6 31.3 98.4 53.57 46.68
w/FullPaper − 27.4 45.2 137.5 68.75 65.89 − 14.6 24.1 55.0 59.39 47.61 − 16.3 34.0 107.5 54.15 48.10
w/CoT 85.58 27.2 43.6 131.0 69.34 66.09 63.63 10.8 20.0 35.8 57.75 46.52 70.38 15.7 22.8 64.9 52.52 48.62
Table4: PerformanceondirectQAwithfullpaperandCoTQA.Bothtaskshelptoimprove
theperformanceofGeminiandGPT4modelsoverdirectQAwithfiguresandtables. Acc: top-1
accuracy,M:METEOR,R-L:ROUGE-L,C:CIDEr,B-F1: BERTScoreF1andL3S:L3Score.
4
(cid:88)
l =log(min(p ,p ))where p =exp(l ), p =1− exp(l )
xc low rem low x4 rem xi
i=0

L3Score=softmax(x) yes = 1ex −p(l
se
x
ox
k
fp
)
t+( mlx
e ak x
xp)
( (l xxc ) n) o = exp(lxex kp )+(lx exc p) (lxc) i if fx xk k = =y ne os (2)
Figure4showsQAevaluationswhereL3ScoreismoreappropriatethanROUGEandBERTScore.
WeprovideadditionalexamplescomparingL3ScorewithotherLLM-basedmetricsinsupplementary.
5 Experiments
5.1 ExperimentalSeutp
Weevaluatesixstate-of-the-artclosed-sourceandsixopen-sourcemodelsonthetestsetsofSPIQA
forthreedifferenttasksetups: directQAwithfiguresandtables,directQAwithfullpaper,andCoT
QA.Forthelong-contextmodelslikeGemini[68,61],GPT[1,52]andClaude3[2],weinputall
theimagestogetherandaskthemodeltoanswerquestions. Sincemostopen-weightmodelscan
onlytakeoneimageinasinglequery,weaskthequestionandshoweveryimageonebyoneina
multi-turnsetup,andthenthemodelanswers. Weusefull-resolutionimagesformodelswithhigh
contextlengthand224pximagesforlowcontextlength. Forevaluatingthefree-formanswers,we
report five different metrics for comprehensive analysis - METEOR [4], CIDEr [72], ROUGE-L
[37], BERTScore F1 [83] and the proposed L3Score. For the CoT QA task, we also report the
top-1accuracyforretrievingthehelpfulimagestoanswerthequestion. Wefurtherelaborateonthe
importanceofcomprehendingthecaptionswiththefiguresandtablesandreportgranularresultson
differentfiguresubcategories.
Fine-tuningDetails. Wefine-tunetwoopen-sourcedmodels, InstructBLIP[13]andLLaVA1.5
[38] on SPIQA training set with simple QA prompts, where every training sample contains one
referenceimage,correspondingquestionandanswer. Weinitializebothmodelsfromthepublicly
availablecheckpoints4andtrainthemfortwoepochswithLoRA[18]ofrank32. WeuseAdamW
[42]optimizer,acosinescheduler[41]withalinearwarmupforthefirst3%steps,apeaklearning
rateof1e-5,andabatchsize32. Eachtrainingjobtakesabout4hourson8A6000GPUs. Wedonot
usethemaintextfrompapersduringtraining,asInstructBLIPandLLaVAhavealowcontextlength
of2048. Weresizetheimagesto224pxfortraining. ThetrainedmodelsareevaluatedfordirectQA
withfiguresandtables.
5.2 MainResults
Wehighlightthehighestscoresamongclosedandopenmodelsineverytablewithredandblue,
respectively,andindicatetheperformanceimprovementsbyfine-tunedmodelswith∆.
DirectQAw/FiguresandTables. Table3reportstheperformanceofallopen,closed,andfine-
tunedmodelsfordirectQAwithfiguresandtables. Wealsousethecaptionswiththeimagesin
4InstructBLIPandLLaVA1.5
7(a)Resultsontest-A. (b)Resultsontest-B. (c)Resultsontest-C.
Figure3: AblationontheimportanceofcaptionsintheQAtask. AllGeminiandGPTvariants
sufferwhencaptionsareomitted. AllnumbersarefordirectQAwithfiguresandtables.
Figures
Tables Overall
Method Schematics Plots&charts Visualizations Otherfigures
C B-F1 L3S C B-F1 L3S C B-F1 L3S C B-F1 L3S C B-F1 L3S C B-F1 L3S
Zero-shotClosed-WeightMLLMs
GeminiProVision[68] 130.7 65.12 39.95 111.2 66.79 45.28 127.2 63.47 45.63 119.1 65.15 51.74 113.4 65.79 47.95 124.6 64.87 43.85
Gemini1.5Flash[61] 128.1 66.79 47.71 86.9 67.50 60.72 101.0 62.81 54.02 112.1 66.91 63.47 96.7 67.28 62.16 110.9 65.84 54.20
Gemini1.5Pro[61] 88.9 65.77 55.53 60.8 64.67 55.80 101.6 62.48 48.34 80.4 63.63 57.83 70.4 64.22 56.28 87.1 64.36 53.49
GPT-4Vision[1] 132.6 64.64 56.68 44.3 63.99 60.14 133.3 63.19 53.74 93.0 62.52 58.55 69.2 63.92 60.93 113.8 64.01 56.67
GPT-4o[52] 130.7 65.90 65.18 114.9 68.20 71.38 153.6 65.35 56.73 96.6 64.81 56.07 117.9 67.20 67.57 133.7 66.14 64.00
Zero-shotOpen-WeightMLLMs
InstructBLIP-7B[13] 30.6 43.59 3.68 50.9 54.48 8.28 92.2 47.11 8.92 131.1 55.63 14.81 76.7 53.44 10.26 62.6 47.70 7.50
LLaVA-1.5-7B[38] 104.9 58.00 8.90 86.4 67.59 18.09 159.2 61.02 15.68 123.5 65.87 18.03 95.4 66.53 18.11 117.8 61.61 13.86
XGen-MM[62] 79.0 55.74 7.42 99.5 58.99 17.35 200.6 60.06 17.36 157.7 64.61 19.91 122.3 60.37 18.70 127.0 58.41 13.74
InternLM-XC[15] 93.1 54.40 13.09 66.4 60.49 25.79 38.5 44.25 17.48 98.3 61.5 24.98 78.9 60.66 25.75 73.7 53.57 18.28
CogVLM[75] 79.5 52.10 13.89 50.9 51.80 19.47 47.3 50.25 18.22 32.3 49.84 20.89 43.4 51.03 20.30 59.2 51.24 16.89
Table5:Performaceontablesandfiguresubcategories. AllnumbersarefordirectQAwithfigures
andtablesontest-A.C:CIDEr,B-F1: BERTScoreF1andL3S:L3Score.
thissetup. Amongtheopen-sourcedsystems,InternLM-XCandCogVLMperformslightlybetter
thanothers. InternLM-XCachieves18.28and12.47L3Scoreontest-Aandtest-B,whichare∼5
pointssuperiortoInstructBLIP,SPHINX,LLaVA,andXGen-MM.CogVLMperformsbetterthan
InternLM-XC on test-C, yielding a L3Score of 12.52. Since these models are solely trained on
naturalimages,inmostcases,theyareunabletounderstandcomplexfiguresandtablesandgenerate
randomanswers. Amongtheclosedmodels,GPT-4operformsconsistentlywellontest-Aandtest-C,
producingstate-of-the-artresultsonallmetrics. Claude-3alsodemonstratesstrongperformanceand
achievesthehighestL3Scorescoreof49.54ontest-B,whichis3.32pointshigherthanGPT-4o. We
alsoobservethevariabilityoftraditionalQAmetricsacrossdifferentmodels. Forexample,ontest-C,
Gemini1.5Proachieves1.0pointhigherMETEORscorethanGeminiProVisionbutalsoattains2.6
and7.6pointslowerROUGEandCIDErscores. Incontrast,ourproposedL3Scoremetricconsiders
thesemanticmeaningofanswersinsteadofmeretokenmatchingandpersistentlygenerateshigher
scoresforbetteranswers.
The fine-tuned InstructBLIP and LLaVA 1.5 obtain a massive improvement of 28 and 26 point
L3Scoreonaverageoverthreetestsetscomparedtocorrespondingzero-shotmodels. Thesefine-
tunedmodelsperformalmostequallywellasGeminiProVision,apowerfullong-contextclosed
system. Suchresultssignifytheeffectivenessofourproposedlarge-scaletrainingcorpus,whichwill
pavethewayfordevelopingdedicatedQAsystemsforscientificpapersinthefuture.
DirectQAw/FullPaper. Inthistask,weprovidethefulltextofthepaper,includingfigures,tables,
and captions, and ask the models to answer a question directly. While the complete text aids in
comprehendingthearticle,italsonecessitateslong-contextreasoningcapabilities,asscientificpapers
aretypically8-10pageslong. AsshowninTable4,powerfulmultimodalmodelssuchasGemini1.5,
GPT-4o,andGPT-4Visionexhibitsignificantperformanceimprovementswhenusingthefullpaper.
Forexample,Gemini1.5ProachievesL3Scoreimprovementsof8.31,2.24,and2.76pointswiththe
fulltextcomparedtousingonlyfiguresandtables. Thisdemonstratesthepotentialofleveragingthe
fulltext,whichwemakeavailablewithSPIQA,todevelopmoreadvancedmodelsinthefuture.
CoTQA.Table4presentstheperformanceoftheGeminiandGPTmodelsontheCoTQAtask.
In this task, the system first retrieves reference images and then generates the answer. The CoT
promptguidesthemodelthroughstep-by-stepreasoning, whichoftenresultsinbetterresponses.
Forinstance,GPT-4Visionshowsanincreaseof6.70,1.73,and2.98inL3ScorewhenusingCoT
promptscomparedtodirectQA.SimilarimprovementsareobservedinmostcasesfortheGemini
models. Forsimplicity,weperformCoTQAusingonlyfiguresandtables.
8Figure4: Examplequestions,ground-truthanswers,andresponsesbydifferentbaselinemodels.
BothQAsbelongtotestA.Metricscoloredingreendenotecorrectevaluations,whilethoseinred
indicateincorrectscoring. R-L:ROUGE-L,BERT:BERTScore,L3S:L3Score.
5.3 AblationStudy
FigureandTableCaptions.Inscientificpapers,figuresandtablesareoftenaccompaniedbydetailed
andinformativecaptions. Figure3showsthatallGeminiandGPTvariantsexperiencesignificant
performancedropswhencaptionsareexcluded. Forinstance,thebest-performingGPT-4omodel
suffers a 2-point L3Score decrease on test-A when captions are omitted, which underscores the
importanceofcaptionsinprovidingcontextandenhancingcomprehensioninQAtasks.
PerformanceonTables. Table5showstheperformanceofbaselinemodelsontablesandfigures
intest-A.Open-sourcemodelsstrugglewithtables,scoringlowerthantheiroverallperformance.
Amongclosed-sourcemodels,Gemini1.5Pro,GPT-4Vision,andGPT-4operformwell. Notably,
GPT-4oachievesa65.18L3Scoreontables,outperformingGPT-4Vision,thesecondbestby8.50
points.
PerformanceonFigureSubcategories. Baselinemodelsperformwellonschematicdiagramsbut
strugglewithplots,charts,andvisualizations. Forexample,GPT-4oscores71.38onschematicsbut
only56.73and56.07onplots&charts,andvisualizations,respectively. Suchresultsindicatethe
needforimprovedsystemstocomprehendscientificdiagramsrequiringmathematicalreasoning.
5.4 QualitativeResultsandErrorAnalysis
Figure4illustratestwoexamplequestions,ground-truthanswers,andoutputsfromvariousbaseline
models. For the plot-based question, both InstructBLIP and Gemini 1.5 Flash produce correct
responses. However,onlyourproposedL3Scoreevaluatesthemcorrectlyinbothcases. Notably,
InstructBLIP’sresponse(‘Ours’)correctlyanswersthequestiondespitenotmatchingtheground-
truth. TraditionalmetricslikeROUGE-LandBERTScorefailtoevaluatesuchresponses,scoring
themas0. Fortheschematic-basedquestion,theground-truthislonganddescriptive. GPT-4Vision
generatesasignificantlycorrectresponse,andL3Scoreaccuratelyevaluatesitwithascoreof96.9.
Weobservethatthebest-performingGPT-4ostruggleswithcomplexplots,charts,andtables. Such
errorcasesaredetailedinthesupplementarymaterials.
6 Conclusion
WeintroduceSPIQA(ScientificPaperImageQuestionAnswering),thefirstlarge-scaleQAdataset
specificallydesignedtointerpretcomplexfiguresandtableswithinthecontextofscientificpapers.
Additionally, we propose LLMLogScore, an improved metric for free-form QA that accurately
analyzesthesemanticcontextofcandidateanswersrelativetothegroundtruth. Throughextensive
experimentswith12prominentfoundationalmodels,weevaluatetheirabilitytocomprehendthe
nuancedaspectsofresearcharticles. Furthermore, fine-tuningtwoopen-sourcesystems, LLaVA
and InstructBLIP, on the SPIQA training set results in significant improvements over zero-shot
evaluations,indicatingpromisingavenuesfordesigningspecializedsystemsforscientificQAinthe
future. OurworklaysthefoundationfordevelopingadvancedQAsystemsthateffectivelyunderstand
andanalyzescientificdocuments,drivingfurtherresearchinthiscriticalarea.
LimitationsandSocietalImpact. WeacknowledgethatSPIQAisdesignedforresearchpurposes
andshouldnotberegardedasacomprehensivedataset,asitisrestrictedtocomputersciencepapers.
Modelstrainedonour datasetmayexhibitbiases towardsspecific topicswithinthisdomainand
maynotperformwellonotherscientificliterature. ExtendingSPIQAtoencompassotherscientific
domainsremainsafutureprospect.
9References
[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technicalreport. arXiv
preprintarXiv:2303.08774,2023.
[2] Anthropic. Theclaude3modelfamily:Opus,sonnet,haiku. 2024.
[3] AlanRAronsonandFrançois-MichelLang. Anoverviewofmetamap:historicalperspectiveandrecent
advances. JournaloftheAmericanMedicalInformaticsAssociation,17(3):229–236,2010.
[4] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved
correlationwithhumanjudgments. InACLWorkshoponIntrinsicandExtrinsicEvaluationMeasuresfor
MachineTranslationand/orSummarization,pages65–72,2005.
[5] Ritwick Chaudhry, Sumit Shekhar, Utkarsh Gupta, Pranav Maneriker, Prann Bansal, and Ajay Joshi.
Leaf-qa:Locate,encode&attendforfigurequestionanswering. InWACV,pages3512–3521,2020.
[6] JiaqiChen,TongLi,JinghuiQin,PanLu,LiangLin,ChongyuChen,andXiaodanLiang.Unigeo:Unifying
geometrylogicalreasoningviareformulatingmathematicalexpression. InEMNLP,pages3313–3323,
2022.
[7] JiaqiChen,JianhengTang,JinghuiQin,XiaodanLiang,LingboLiu,EricXing,andLiangLin. Geoqa:A
geometricquestionansweringbenchmarktowardsmultimodalnumericalreasoning. InFindingsofACL,
pages513–523,2021.
[8] WenhuChen,Ming-WeiChang,EvaSchlinger,WilliamYangWang,andWilliamWCohen.Openquestion
answeringovertablesandtext. InICLR,2020.
[9] WenhuChen,HongminWang,JianshuChen,YunkaiZhang,HongWang,ShiyangLi,XiyouZhou,and
WilliamYangWang. Tabfact:Alarge-scaledatasetfortable-basedfactverification. InICLR,2019.
[10] YukangChen,ShengjuQian,HaotianTang,XinLai,ZhijianLiu,SongHan,andJiayaJia. Longlora:
Efficientfine-tuningoflong-contextlargelanguagemodels. arXivpreprintarXiv:2309.12307,2023.
[11] ZhoujunCheng,HaoyuDong,ZhiruoWang,RanJia,JiaqiGuo,YanGao,ShiHan,Jian-GuangLou,
andDongmeiZhang. Hitab: Ahierarchicaltabledatasetforquestionansweringandnaturallanguage
generation. InACL,pages1094–1110,2022.
[12] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke
Zettlemoyer. Quac:Questionansweringincontext. InEMNLP,pages2174–2184,2018.
[13] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuatTiong,JunqiZhao,WeishengWang,Boyang
Li,PascaleNFung,andStevenHoi. Instructblip:Towardsgeneral-purposevision-languagemodelswith
instructiontuning. NeurIPS,2023.
[14] PradeepDasigi, KyleLo, IzBeltagy, ArmanCohan, NoahASmith, andMattGardner. Adatasetof
information-seekingquestionsandanswersanchoredinresearchpapers. InNAACL,pages4599–4610,
2021.
[15] XiaoyiDong,PanZhang,YuhangZang,YuhangCao,BinWang,LinkeOuyang,XilinWei,Songyang
Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering free-form text-image
compositionandcomprehensioninvision-languagelargemodel. arXivpreprintarXiv:2401.16420,2024.
[16] PengGao,RenruiZhang,ChrisLiu,LongtianQiu,SiyuanHuang,WeifengLin,ShitianZhao,ShijieGeng,
ZiyiLin,PengJin,etal. Sphinx-x:Scalingdataandparametersforafamilyofmulti-modallargelanguage
models. arXivpreprintarXiv:2402.05935,2024.
[17] Ting-YaoHsu,CLeeGiles,andTing-HaoHuang. Scicap:Generatingcaptionsforscientificfigures. In
FindingsofEMNLP,pages3258–3264,2021.
[18] EdwardJHu,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,WeizhuChen,etal.
Lora:Low-rankadaptationoflargelanguagemodels. InICLR,2021.
[19] MohitIyyer,Wen-tauYih,andMing-WeiChang. Search-basedneuralstructuredlearningforsequential
questionanswering. InACL,pages1821–1831,2017.
[20] QiaoJin,BhuwanDhingra,ZhengpingLiu,WilliamCohen,andXinghuaLu. Pubmedqa:Adatasetfor
biomedicalresearchquestionanswering. InEMNLP,pages2567–2577,2019.
10[21] MandarJoshi, EunsolChoi, DanielSWeld, andLukeZettlemoyer. Triviaqa: Alargescaledistantly
supervisedchallengedatasetforreadingcomprehension. InACL,pages1601–1611,2017.
[22] KushalKafle,BrianPrice,ScottCohen,andChristopherKanan. Dvqa:Understandingdatavisualizations
viaquestionanswering. InCVPR,pages5648–5656,2018.
[23] SamiraEbrahimiKahou,VincentMichalski,AdamAtkinson,ÁkosKádár,AdamTrischler,andYoshua
Bengio. Figureqa: Anannotatedfiguredatasetforvisualreasoning. arXivpreprintarXiv:1710.07300,
2017.
[24] EhsanKamalloo,NouhaDziri,CharlesClarke,andDavoodRafiei. Evaluatingopen-domainquestion
answeringintheeraoflargelanguagemodels. InACL,pages5591–5606,2023.
[25] YannisKatsis,SaneemChemmengath,VishwajeetKumar,SamarthBharadwaj,MustafaCanim,Michael
Glass,AlfioGliozzo,FeifeiPan,JaydeepSen,KarthikSankaranarayanan,etal.Ait-qa:Questionanswering
datasetovercomplextablesintheairlineindustry. InNAACL,pages305–314,2022.
[26] AniruddhaKembhavi,MikeSalvato,EricKolve,MinjoonSeo,HannanehHajishirzi,andAliFarhadi. A
diagramisworthadozenimages. InECCV,pages235–251.Springer,2016.
[27] AniruddhaKembhavi,MinjoonSeo,DustinSchwenk,JonghyunChoi,AliFarhadi,andHannanehHa-
jishirzi. Are you smarter than a sixth grader? textbook question answering for multimodal machine
comprehension. InCVPR,pages4999–5007,2017.
[28] TomášKocˇisky`,JonathanSchwarz,PhilBlunsom,ChrisDyer,KarlMoritzHermann,GáborMelis,and
EdwardGrefenstette. Thenarrativeqareadingcomprehensionchallenge. TACL,6:317–328,2018.
[29] MartinKrallinger,AnastasiaKrithara,AnastasiosNentidis,GeorgiosPaliouras,andMartaVillegas.Bioasq
atclef2020:Large-scalebiomedicalsemanticindexingandquestionanswering.InAdvancesinInformation
Retrieval:42ndEuropeanConferenceonIRResearch,ECIR2020,Lisbon,Portugal,April14–17,2020,
Proceedings,PartII42,pages550–556.Springer,2020.
[30] AnastasiaKrithara,AnastasiosNentidis,KonstantinosBougiatiotis,andGeorgiosPaliouras. Bioasq-qa:A
manuallycuratedcorpusforbiomedicalquestionanswering. ScientificData,10(1):170,2023.
[31] TomKwiatkowski,JennimariaPalomaki,OliviaRedfield,MichaelCollins,AnkurParikh,ChrisAlberti,
DanielleEpstein,IlliaPolosukhin,JacobDevlin,KentonLee,etal. Naturalquestions:abenchmarkfor
questionansweringresearch. TACL,7:453–466,2019.
[32] RobertLeaman, RezartaIslamajDog˘an, andZhiyongLu. Dnorm: diseasenamenormalizationwith
pairwiselearningtorank. Bioinformatics,29(22):2909–2917,2013.
[33] SeongyunLee,SeungoneKim,SueHyunPark,GeewookKim,andMinjoonSeo. Prometheus-vision:
Vision-languagemodelasajudgeforfine-grainedevaluation. arXivpreprintarXiv:2401.06591,2024.
[34] YoonjooLee,KyungjaeLee,SunghyunPark,DasolHwang,JaehyeonKim,Hong-inLee,andMoontae
Lee. Qasa: advancedquestionansweringonscientificarticles. InICML,pages19036–19052.PMLR,
2023.
[35] AdamDLelkes,VinhQTran,andCongYu.Quiz-stylequestiongenerationfornewsstories.InProceedings
oftheWebConference2021,pages2501–2511,2021.
[36] JiaqiLi,MengmengWang,ZilongZheng,andMuhanZhang. Canlong-contextlargelanguagemodels
understandlongcontexts? InICLR,2024.
[37] Chin-YewLin.Rouge:Apackageforautomaticevaluationofsummaries.InTextSummarizationBranches
Out,pages74–81,2004.
[38] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee. Improvedbaselineswithvisualinstruction
tuning. InNeurIPSWorkshoponInstructionTuningandInstructionFollowing,2023.
[39] NelsonFLiu,KevinLin,JohnHewitt,AshwinParanjape,MicheleBevilacqua,FabioPetroni,andPercy
Liang. Lostinthemiddle:Howlanguagemodelsuselongcontexts. TACL,12:157–173,2024.
[40] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg
evaluationusinggpt-4withbetterhumanalignment. InEMNLP,pages2511–2522,2023.
[41] IlyaLoshchilovandFrankHutter. Sgdr:Stochasticgradientdescentwithwarmrestarts. InICLR,2016.
11[42] IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. InICLR,2018.
[43] PanLu,HritikBansal,TonyXia,JiachengLiu,ChunyuanLi,HannanehHajishirzi,HaoCheng,Kai-Wei
Chang,MichelGalley,andJianfengGao. Mathvista:Evaluatingmathematicalreasoningoffoundation
modelsinvisualcontexts. InICLR,2023.
[44] PanLu,RanGong,ShibiaoJiang,LiangQiu,SiyuanHuang,XiaodanLiang,andSong-chunZhu.Inter-gps:
Interpretablegeometryproblemsolvingwithformallanguageandsymbolicreasoning. InACL,pages
6774–6786,2021.
[45] OscarMañas,BennoKrojer,andAishwaryaAgrawal. Improvingautomaticvqaevaluationusinglarge
languagemodels. InAAAI,volume38,pages4171–4179,2024.
[46] AhmedMasry,XuanLongDo,JiaQingTan,ShafiqJoty,andEnamulHoque. Chartqa:Abenchmarkfor
questionansweringaboutchartswithvisualandlogicalreasoning. InFindingsofACL,pages2263–2279,
2022.
[47] AhmedMasry,ParsaKavehzadeh,XuanLongDo,EnamulHoque,andShafiqJoty. UniChart:Auniversal
vision-language pretrained model for chart comprehension and reasoning. In Houda Bouamor, Juan
Pino,andKalikaBali,editors,EMNLP,pages14662–14684,Singapore,December2023.Associationfor
ComputationalLinguistics.
[48] NiteshMethani,PrithaGanguly,MiteshMKhapra,andPratyushKumar. Plotqa:Reasoningoverscientific
plots. InWACV,pages1527–1536,2020.
[49] TodorMihaylov,PeterClark,TusharKhot,andAshishSabharwal. Canasuitofarmorconductelectricity?
anewdatasetforopenbookquestionanswering. InEMNLP,pages2381–2391,2018.
[50] LinyongNan,ChiachunHsieh,ZimingMao,XiVictoriaLin,NehaVerma,RuiZhang,WojciechKrys´-
cin´ski,HaileySchoelkopf,RileyKong,XiangruTang,etal. Fetaqa:Free-formtablequestionanswering.
TACL,10:35–49,2022.
[51] OpenAI. Gpt-3.5turbo,2023.
[52] OpenAI. Gpt-4orelease,2024.
[53] VaishaliPal,AndrewYates,EvangelosKanoulas,andMaartendeRijke. Multitabqa:Generatingtabular
answersformulti-tablequestionanswering. InACL,2023.
[54] AnusriPampari,PreethiRaghavan,JenniferLiang,andJianPeng. Emrqa:Alargecorpusforquestion
answeringonelectronicmedicalrecords. InEMNLP,2018.
[55] KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu. Bleu:amethodforautomaticevaluation
ofmachinetranslation. InACL,pages311–318,2002.
[56] DimitrisPappas,IonAndroutsopoulos,andHarrisPapageorgiou. Bioread:Anewdatasetforbiomedical
readingcomprehension. InProceedingsoftheEleventhInternationalConferenceonLanguageResources
andEvaluation(LREC2018),2018.
[57] DimitrisPappas,PetrosStavropoulos,IonAndroutsopoulos,andRyanMcDonald. Biomrc: Adataset
forbiomedicalmachinereadingcomprehension. InProceedingsofthe19thSIGBioMedWorkshopon
BiomedicalLanguageProcessing,pages140–149,2020.
[58] PanupongPasupatandPercyLiang. Compositionalsemanticparsingonsemi-structuredtables. InACL,
pages1470–1480,2015.
[59] BhawnaPiryani,JamshidMozafari,andAdamJatowt. Chroniclingamericaqa: Alarge-scalequestion
answeringdatasetbasedonhistoricalamericannewspaperpages. InSIGIR,2024.
[60] PranavRajpurkar,JianZhang,KonstantinLopyrev,andPercyLiang. Squad: 100,000+questionsfor
machinecomprehensionoftext. InEMNLP,pages2383–2392,2016.
[61] MachelReid, NikolaySavinov, DenisTeplyashin, DmitryLepikhin, TimothyLillicrap, Jean-baptiste
Alayrac,RaduSoricut,AngelikiLazaridou,OrhanFirat,JulianSchrittwieser,etal. Gemini1.5:Unlocking
multimodalunderstandingacrossmillionsoftokensofcontext. arXivpreprintarXiv:2403.05530,2024.
[62] SalesforceAIResearch. xgen-mm-phi3-mini-instructmodelcard,May2024.
[63] FedericoRuggeri,MohsenMesgar,andIrynaGurevych.Adatasetofargumentativedialoguesonscientific
papers. InACL,pages7684–7699,2023.
12[64] HriturajSinghandSumitShekhar. Stl-cqa:Structure-basedtransformerswithlocalizationandencoding
forchartquestionanswering. InEMNLP,pages3275–3284,2020.
[65] HaitianSun,WilliamCohen,andRuslanSalakhutdinov.Conditionalqa:Acomplexreadingcomprehension
datasetwithconditionalanswers. InACL,pages3627–3637,2022.
[66] Simon Šuster and Walter Daelemans. Clicr: a dataset of clinical case reports for machine reading
comprehension. InNAACL,pages1551–1563,2018.
[67] Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito.
Slidevqa: Adatasetfordocumentvisualquestionansweringonmultipleimages. InAAAI,volume37,
pages13636–13645,2023.
[68] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,Radu
Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable
multimodalmodels. arXivpreprintarXiv:2312.11805,2023.
[69] Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and
KaheerSuleman. Newsqa:Amachinecomprehensiondataset. InProceedingsofthe2ndWorkshopon
RepresentationLearningforNLP,pages191–200,2017.
[70] GeorgeTsatsaronis, GeorgiosBalikas, ProdromosMalakasiotis, IoannisPartalas, MatthiasZschunke,
MichaelRAlvers,DirkWeissenborn,AnastasiaKrithara,SergiosPetridis,DimitrisPolychronopoulos,etal.
Anoverviewofthebioasqlarge-scalebiomedicalsemanticindexingandquestionansweringcompetition.
BMCbioinformatics,16:1–28,2015.
[71] SvitlanaVakulenkoandVadimSavenkov. Tableqa:Questionansweringontabulardata. arXivpreprint
arXiv:1705.06504,2017.
[72] RamakrishnaVedantam,CLawrenceZitnick,andDeviParikh. Cider:Consensus-basedimagedescription
evaluation. InCVPR,pages4566–4575,2015.
[73] CunxiangWang,RuoxiNing,BoqiPan,TonghuiWu,QipengGuo,ChengDeng,GuangshengBao,Qian
Wang,andYueZhang. Novelqa:Abenchmarkforlong-rangenovelquestionanswering. arXivpreprint
arXiv:2403.12766,2024.
[74] JiexinWang,AdamJatowt,andMasatoshiYoshikawa. Archivalqa:alarge-scalebenchmarkdatasetfor
open-domainquestionansweringoverhistoricalnewscollections. InSIGIR,pages3025–3035,2022.
[75] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang,
LeiZhao,XixuanSong,etal. Cogvlm: Visualexpertforpretrainedlanguagemodels. arXivpreprint
arXiv:2311.03079,2023.
[76] Chih-Hsuan Wei, Bethany R Harris, Donghui Li, Tanya Z Berardini, Eva Huala, Hung-Yu Kao, and
ZhiyongLu. Acceleratingliteraturecurationwithtext-miningtools: acasestudyofusingpubtatorto
curategenesinpubmedabstracts. Database,2012:bas041,2012.
[77] JohannesWelbl,PontusStenetorp,andSebastianRiedel. Constructingdatasetsformulti-hopreading
comprehensionacrossdocuments. TACL,6:287–302,2018.
[78] JohannesWelbl,PontusStenetorp,andSebastianRiedel. Constructingdatasetsformulti-hopreading
comprehensionacrossdocuments. TACL,6:287–302,2018.
[79] YingXu,DakuoWang,MoYu,DanielRitchie,BingshengYao,TongshuangWu,ZhengZhang,Toby
Jia-JunLi,NoraBradford,BrandaSun,TranBaoHoang,YisiSang,YufangHou,XiaojuanMa,DiyiYang,
NanyunPeng,ZhouYu,andMarkWarschauer. Fantasticquestionsandwheretofindthem:FairytaleQA–
anauthenticdatasetfornarrativecomprehension. ACL,2022.
[80] YiYang,Wen-tauYih,andChristopherMeek. Wikiqa: Achallengedatasetforopen-domainquestion
answering. InEMNLP,pages2013–2018,2015.
[81] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and
ChristopherDManning. Hotpotqa:Adatasetfordiverse,explainablemulti-hopquestionanswering. In
EMNLP,pages2369–2380,2018.
[82] RenruiZhang,DongzhiJiang,YichiZhang,HaokunLin,ZiyuGuo,PengshuoQiu,AojunZhou,PanLu,
Kai-WeiChang,PengGao,etal. Mathverse:Doesyourmulti-modalllmtrulyseethediagramsinvisual
mathproblems? arXivpreprintarXiv:2403.14624,2024.
13[83] TianyiZhang,VarshaKishore,FelixWu,KilianQWeinberger,andYoavArtzi. Bertscore:Evaluatingtext
generationwithbert. InICLR,2019.
[84] SanqiangZhao,SeokhwanKim,YangLiu,RobinsonPiramuthu,andDilekHakkani-Tür. Storyqa:Story
groundedquestionansweringdataset. InAAAI2023WorkshoponKnowledgeAugmentedMethodsfor
NLP,2023.
[85] LianminZheng,Wei-LinChiang,YingSheng,SiyuanZhuang,ZhanghaoWu,YonghaoZhuang,ZiLin,
ZhuohanLi, DachengLi, EricXing, etal. Judgingllm-as-a-judgewithmt-benchandchatbotarena.
NeurIPS,36,2023.
[86] ChuntingZhou,PengfeiLiu,PuxinXu,SrinivasanIyer,JiaoSun,YuningMao,XuezheMa,AviaEfrat,
PingYu,LiliYu,etal. Lima:Lessismoreforalignment. NeurIPS,36,2023.
14A DatasetandCodeRelease
FollowingNeurIPSDatasetandBenchmarkTrackguidelines,wepubliclyreleasetheSPIQAdataset
on Hugging Face: https://huggingface.co/datasets/google/spiqa. Our evaluation and
metriccomputationscripts,alongwithresponsesfromallclosed-andopen-sourcemodels,areac-
cessibleintheGitHubrepository: https://github.com/google/spiqa. SPIQAwillbeproperly
maintainedandwillpubliclyremainavailableundertheCreativeCommonsAttributionLicense(CC
BY4.0). TheevaluationcodeandlibraryforL3ScoreareavailableintheGithubrepositoryunder
Apache2.0license. WeincludethedatasetcardandREADMEfortheresources.
B AdditionalAblationStudy
L3ScoreonSPIQAtest-B
Method
GeminiPro[68] GPT-3.5T[51] GPT-4o[52]
GeminiProVision[68] 47.53 28.14 33.10
Gemini1.5Flash[61] 54.55 32.76 36.04
Gemini1.5Pro[61] 56.78 35.15 43.27
GPT-4Vision[1] 65.83 38.30 43.62
GPT-4o[52] 68.61 39.58 46.22
InstructBLIP-7B[13] 18.82 7.26 7.07
LLaVA-1.5-7B[38] 20.69 10.71 9.63
XGen-MM[62] 15.55 7.46 8.18
InternLM-XC[15] 20.51 14.09 12.47
CogVLM[75] 18.44 10.01 9.60
Table B.1: Computation of L3Score using different Figure B.1: Ablation on the input im-
LLMs. WhiletheabsolutevaluesofL3Scorevaryde- ageresolution. L3Scoreimproveswith
pendingonthechoiceofLLM,therelativechangesin increasingresolutionofimages(figures
betweendifferentmodels remainconsistent. Allnum- and tables). All numbers are for direct
bersarefordirectQAwithfiguresandtablesontest-B. QAwithfiguresandtablesontest-B.
L3ScorewithDifferentLLMs. TableB.1comparestheL3ScoreacrossvariousLLMs. Fig.F.1
showsthepromptweusedtocomputetheL3Scoretoevaluateresponsesfrommodels. Weobserve
thatwhiletheabsoluteL3ScorevaluevariesdependingonthechosenLLM,therelativechanges
inthescoresremainconsistent. Forinstance,whentheL3ScoreiscomputedusingGPT-3.5Turbo,
GPT-4oscores11.44pointshigherthanGeminiProVision. UsingGPT-4otocomputetheL3Score,
thedifferenceincreasesto13.12points. However,GPT-4oachievesapproximately40%betterscores
inbothcasesthanGeminiProVision. DifferentLLMshavevaryingvocabularies,whichcancause
slightdifferencesintheirlog-likelihoodprobabilities. Notably,L3Scorecanbecomputedwithany
LLMthatprovideslogprobabilitiesfordifferentoutputtokens. Werecommendusersconsistently
usethesameLLMacrossallevaluationstoensureproperscorecomparison. ExceptforTableB.1,
wealwaysuseGPT-4owhencalculatingL3Score,whichiscurrentlyoneofthemostpowerfuland
reasonablyaffordablepubliclyavailableLLMs.
ImageResolution. FigureB.1demonstratestheperformanceimprovementsofdifferentGeminiand
GPT-4systemswhenusinghigherimageresolutions. Generally,higherresolutionimagesleadto
increasedL3Scores. However,largerinputimagesresultinmoreinputtokens,makingtheLLM
callmoreexpensive. Inourmainexperiments,wealwaysusefull-resolutionimagesfortheclosed
modelsand224pximagesfortheopenmodels.
C SPIQAvs. ExistingScientificQADatasets
Manuallygeneratingfree-form,open-endedquestionsandanswersonscientificarticlesisademand-
ing process in terms of cost and time, requiring expert annotators with detailed domain-specific
knowledge. ManyexistingscientificQAbenchmarkstypicallyfollowcloze-stylequestiongeneration
tobypasssuchcostlyannotationprocedures. Thisapproachremovesanamedentityfromasingle
sentence,andthetaskistoguessthemissingentityafterreadingtheprecedingpassage. Forexample,
BioRead[56]usesthefulltextofunlabeledbiomedicalarticlesfromPubMed5andutilizesMetamap
[3]toannotatebiomedicalentitiesandgeneratecloze-stylequestions. BioReadextractssequences
of 21 sentences from the articles, using the first 20 sentences as a passage and the last sentence
as a question. BioMRC [57] improves and cleans the BioRead corpus by avoiding cross-section
5PubMed:https://www.ncbi.nlm.nih.gov/pmc/
15Free-form Question NumAbstracts/ Questionsbasedon
Dataset NumQA PaperSource Domain
Questions? Generation Papers FullText Figs&Tabs
BioAsq[70,29] ✓ Humanexperts 3.2K − PubMed Biomedical ✗ ✗
BioRead[56] ✗ Cloze-style 16.4M 3.4Mpapers PubMed Biomedical ✓ ✗
BioMRC[57] ✗ Cloze-style 812K 25Mabstracts Pubtator Biomedical ✗ ✗
emrQA[54] ✗ Cloze-style 455K 2.4Kclinicalnotes i2b2datasets EMRs ✓ ✗
MedHop[78] ✗ Cloze-style 2.5K 24Mabstracts Medline2016 MolecularBiology ✗ ✗
PubMedQA[20] ✓ Humanexperts 1K 120Kabstracts PubMed Biomedical ✗ ✗
BioASQ-QA[30] ✓ Humanexperts 4.7K − PubMed Biomedical ✗ ✗
CliCR[66] ✗ Cloze-style 105K 12Kclinicalreports BMJCaseReports Medical ✗ ✗
ArgSciChat[63] ✓ HumanExperts 41dialogues 20papers arXiv NLP ✓ ✗
QASPER[14] ✓ Humanexperts 5K 1.5Kpapers S2ORC NLP ✗ ✗
QASA[34] ✓ Humanexperts 1.8K 112papers S2ORC AI/ML ✓ ✗
SPIQA(Ours) ✓ LLMs+Humanexperts 270K 25.5Kpapers arXiv ComputerScience(all) ✓ ✓
TableC.1: ComparisonofSPIQAwithexistingscientificquestionansweringdatasets. SPIQA
isthefirstlarge-scalefree-formQAcorpusthatfocusesontheholisticaspectsofscientificpapers,
including figures, tables, and full paper text. A dash (−) indicates that the number of papers or
abstractsusedinthefinaldatasetisnotreported.
extractionandexcludingtextfromreferences,captions,andtables.BioMRCuses25millionabstracts
fromPubtator6[76]andDNORM’s[32]biomedicalentityannotationstogenerate812Kcloze-form
questions. Inasimilarline,emrQA[54]developsthefirstpatient-specificelectronicmedicalrecords
(EMR)QAdatasettohelpphysiciansquicklyfindinformationfromclinicalnotescollectedfromthe
i2b2dataset7. MedHop[78]constructsacorpusfordetectingDrug-DrugInteractions(DDIs)from
multiplesourcedocumentsusing24millionpaperabstractscollectedfromtheMedline2016release8.
Similarly,CliCR[66]creates105Kgap-fillingqueriesbasedon12Kclinicalcasereportscollected
fromBMJCaseReports9. However,thesecloze-formdatasetsfailtoreflectreal-worldscenarios
wherethereadersoftenhaveopen-endedquestionswhenreadingalongscientificresearchpaper.
BioAsq[3,29]isoneofthefirstscientificQAbenchmarksthatemployedhumanexpertstoannotate
free-formquestionsbasedonabstractsofbiomedicalarticlescollectedfromthePubMedcorpus.Since
BioAsqprimarilycontainssimplefactualquestions,PubMedQA[20]providesamorecomprehensive
biomedicalQAbenchmarkthatrequiresdetailedreasoningoverarticleabstracts. QASPER[14]
introduces the first QA dataset outside the biomedical field by collecting 1,585 natural language
processing (NLP) papers from the S2ORC corpus10 and generating 5,049 open-ended questions.
GraduatestudentsandfreelancerswithNLPexpertisewererecruitedasannotatorsandwereprovided
withthetitlesandabstractsoftheresearchpapers. Theywereinstructedtowritequestionsthatcould
notbeansweredfromthetitleandabstractbutwereexpectedtobeaddressedsomewhereinthepaper.
Subsequently,expertsreviewedtheannotatedqueriesandtheentirepapertoprovidetheanswers.
Duetothistwo-stepannotationprocedure,manyquestionsinQASPERremainunanswerablefrom
thepaper. Additionally,mostanswerablequestionsinQASPERcanberespondedtowithabinary
yes/noanswerorashortextractivespan,makingthequestionsrelativelyeasyandnotrequiringa
deepunderstandingoftheentirescientificpaper. ArgSciChat[63]proposesthefirstdatasetwith
argumentative dialogues based on NLP papers, consisting of multi-turn conversations between
scientists. However,ArgSciChatcontainsonly41dialoguesover20scientificpapers.
Closesttoourwork,QASA[34]contains1,798open-endedanddetailedquestionson112AI/ML
papers,whereannotatorsreadthefullpapertextwhenwritingthequestions. Theanswerstothese
questionsaremulti-facetedandlong-form,writtenbyAI/MLexpertsortheactualauthorsofthe
correspondingpapers. Duetotheexpensiveannotationscheme,despitemaintaininghighquality,
theQASAbenchmarkissmall-scaleandthequestionsdonotrequireunderstandingcomplexfigures
anddiagrams. OurproposedSPIQAdatasetdiffersfromexistingscientificQAbenchmarksinthree
key aspects: (i) We introduce the first large-scale, free-form, open-ended scientific QA dataset
coveringalldomainsofcomputerscience. (ii)ThequestionsandanswersinSPIQArequireaholistic
understandingofcomplexfiguresandtables,alongwiththefulltextualcontentofthepapers. (iii)In
additiontothedirectQAsetup,weproposeanovelChain-of-Thought(CoT)QAparadigm,where
modelsfirstidentifyhelpfulfiguresandtables,andthengeneratetheanswer. Thisstep-by-stepQA
pipelinehelpsevaluatethefine-grainedreasoningcapabilitiesofthebaselinesystems. TableC.1
providesanextensivecomparisonofSPIQAwithallavailablescientificQAbenchmarks.
6Pubtator:https://www.ncbi.nlm.nih.gov/research/pubtator3/
7i2b2datasets:https://www.i2b2.org/NLP/DataSets/
8Medline:https://www.nlm.nih.gov/medline/medline_home.html
9BMJCaseReport:http://casereports.bmj.com/
10S2ORC:https://github.com/allenai/s2orc
16FigureD.1: SourceofCollectedPapers. SPIQAcomprisesatotalof25,859paperspublishedin19
differenttop-tierconferencesbetween2018and2023,coveringvariousdomainsofcomputerscience.
FigureD.2: Distributionofnumberofwordsinquestions,answersandrationalesinSPIQA.We
observealargevarietyinthelengthofQAs,whichindicatesadiverserangeofpatternsinSPIQA.
D AdditionalDatasetAnalysis
As described in Section 3.2, SPIQA consists of 25,859 papers published in 19 different top-tier
conferences between 2018 and 2023, covering various domains of computer science. Figure 2
categorizesthesourceconferencesintofourbroadgroups: (i)AI/ML:Thiscategorycontributes46%
ofSPIQA,withconferencessuchasNeurIPS,ICLR,ICML,AAAI,andIJCAI.(ii)Naturallanguage
processing(NLP):ConferenceslikeACLandEMNLPmakeup19%ofthedataset. (iii)Computer
visionandcomputergraphics: ThiscategoryincludesCVPR,ICCV,ECCV,WACV,andSIGGRAPH,
contributing17%ofthepapers. (iv)Othercomputersciencedomains: Theseincludeinformation
retrieval(SIGIR,CIKM),databases(ICDE),networking(WebConf,NSDI),datamining(KDD),and
audioandsignalprocessing(ICASSP),collectivelycoveringtheremaining18%ofSPIQA.Figure
D.1illustratesthenumberofpapersfromeachconferenceandeachyearbetween2018and2023.
Aftercollectingthepapers,wegenerated270,194question-answer-rationaletripletsusingGemini
1.5Pro,focusingonthefigures,tables,andtextofthescientificarticles. Theaveragelengthsofthe
questions,answers,andrationalesare12.98,14.56,and37.42words,respectively. Wealsoobserved
highvariances: 20.47forquestions, 243.29foranswers, and468.91forrationales. Asshownin
FigureD.2,approximately36.62%ofanswerscontain5wordsorfewer,56.70%containbetween6
and40words,andtheremaininganswerscontainmorethan40words. Thisdistributiondemonstrates
thepresenceofbothdirectanddescriptiveorexplanatoryQAsinSPIQA.Thenumberofwordsin
questionsandrationalesfollowsalong-tailnormaldistribution. Althoughwedonotusetherationales
inourexperiments,wearereleasingthemforfutureresearch.
17SPIQAtest-A SPIQAtest-B SPIQAtest-C
Method
B@1 B@2 B@3 B@4 B@1 B@2 B@3 B@4 B@1 B@2 B@3 B@4
Zero-shotClosed-WeightMLLMs
GeminiProVision[68] 36.3 27.8 22.7 18.8 23.1 12.2 7.3 4.7 15.7 9.0 6.1 4.5
Gemini1.5Flash[61] 35.4 27.3 22.2 18.4 26.8 13.9 8.1 5.1 16.9 10.8 8.0 6.4
Gemini1.5Pro[61] 35.9 26.7 21.1 17.0 24.2 12.6 7.4 4.6 15.5 9.2 6.4 4.9
Claude3(Opus)[2] 37.2 30.2 23.7 18.7 21.9 11.6 6.9 4.4 16.8 10.6 7.9 6.5
GPT-4Vision[1] 28.7 21.3 16.9 13.8 22.0 11.3 6.7 4.5 15.1 9.4 6.9 5.5
GPT-4o[52] 40.5 31.4 25.5 21.2 23.3 12.6 7.9 5.3 20.8 13.1 9.6 7.6
Zero-shotOpen-WeightMLLMs
InstructBLIP-7B[13] 15.2 11.9 10.0 8.5 1.6 0.9 0.6 0.5 5.4 2.5 1.5 0.9
LLaVA-1.5-7B[38] 35.1 27.6 23.1 19.7 16.9 8.8 5.3 3.6 12.8 6.0 3.4 2.1
XGen-MM[62] 31.1 24.9 21.1 18.2 2.8 1.4 0.9 0.6 8.0 4.6 3.3 2.5
InternLM-XC[15] 35.4 27.5 22.6 19.1 15.6 8.3 5.3 3.7 16.2 9.1 6.2 4.6
CogVLM[75] 33.9 26.5 21.8 18.4 15.8 8.4 5.3 3.8 15.8 8.7 5.8 4.2
Fine-tunedMLLMs
InstructBLIP-7B[13] 34.1 26.0 22.3 18.5 15.7 9.5 5.8 4.2 13.1 8.3 6.3 4.4
∆ 18.9↑ 14.1↑ 12.3↑ 10.0↑ 14.1↑ 8.6↑ 5.2↑ 3.7↑ 7.7↑ 5.8↑ 4.8↑ 3.5↑
InstructBLIP-7BFT-ZS
LLaVA-1.5-7B[38] 38.0 29.6 24.7 21.0 22.6 11.2 7.5 5.1 16.9 10.1 7.0 4.6
∆ 2.9↑ 2.0↑ 1.6↑ 1.3↑ 5.7↑ 2.4↑ 2.2↑ 1.5↑ 4.1↑ 4.1↑ 3.6↑ 2.5↑
LLaVA-1.5-7BFT-ZS
TableE.1:Performanceofzero-shotandfine-tunedsystemsondirectQAwithfiguresandtables
intermsofBLEUscores. B@1: BLEU@1,B@2: BLEU@2,B@3: BLEU@3,B@4:BLEU@4.
Wehighlightthehighestscoresamongclosedandopenmodelsineverytablewithredandblue,
respectively. ∆showsimprovementsafterfine-tuning.
SPIQAtest-A SPIQAtest-B SPIQAtest-C
Method Ret. QA Ret. QA Ret. QA
Acc. B@1 B@2 B@3 B@4 Acc. B@1 B@2 B@3 B@4 Acc. B@1 B@2 B@3 B@4
Gemini1.5Flash[61] − 35.4 27.3 22.2 18.4 − 26.8 13.9 8.1 5.1 − 16.9 10.8 8.0 6.4
w/FullPaper − 37.5 29.0 23.7 19.9 − 30.3 17.7 11.9 8.6 − 17.5 11.2 8.7 6.7
w/CoT 86.18 37.1 28.5 23.2 19.2 57.45 26.0 13.5 7.8 4.8 69.37 15.4 9.6 7.0 5.5
Gemini1.5Pro[61] − 35.9 26.7 21.1 17.0 − 24.2 12.6 7.4 4.6 − 15.5 9.2 6.4 4.9
w/FullPaper − 38.0 28.2 23.0 18.3 − 27.5 15.3 9.6 6.5 − 16.3 9.7 7.3 5.5
w/CoT 85.88 37.1 27.8 22.1 17.9 62.28 24.3 12.3 7.1 4.4 70.79 16.8 10.9 8.0 6.4
GPT-4Vision[1] − 28.7 21.3 16.9 13.8 − 22.0 11.3 6.7 4.5 − 15.1 9.4 6.9 5.5
w/FullPaper − 33.1 25.4 19.0 16.3 − 25.3 14.4 9.3 6.5 − 15.6 9.8 7.5 6.0
w/CoT 83.25 34.3 26.0 20.9 17.2 60.45 26.8 14.0 8.5 5.7 66.73 15.0 9.2 6.7 5.2
GPT-4o[52] − 40.5 31.4 25.5 21.2 − 23.3 12.6 7.9 5.3 − 20.8 13.1 9.6 7.6
w/FullPaper − 41.3 32.1 26.2 22.0 − 31.7 19.4 13.8 10.7 − 21.6 14.1 10.4 7.9
w/CoT 85.58 40.9 31.8 26.1 21.9 63.63 24.1 13.1 8.2 5.5 70.38 18.9 11.9 8.7 6.9
TableE.2: PerformanceondirectQAwithfullpaperandCoTQAintermsofBLEUscores.
BothtaskshelptoimprovetheperformanceofGeminiandGPT4modelsoverdirectQAwithfigures
andtables. B@1: BLEU@1,B@2: BLEU@2,B@3: BLEU@3,B@4:BLEU@4. Wehighlightthe
highestscoresineverytestsplitwithred.
E AdditionalQuantitativeResults
TablesE.1andE.2reporttheresultsforthreedifferenttasks: directQAwithfiguresandtables,direct
QAwiththefullpaper,andCoTQA,usingvariousBLEUmetrics. SimilartoTables3and4ofthe
mainpaper,GPT-4oachievesstate-of-the-artscoresontest-Aandtest-C.Gemini1.5Flashperforms
particularlywellontest-B,achievinga26.8BLEU@1score,whichismorethan2pointshigherthan
anyothermodel. Open-sourcemodelsgenerallyunderperformcomparedtoclosed-sourcemodels,
primarilybecausetheyaretrainedonnaturalimages.
After fine-tuning on the training set, both InstructBLIP-7B and LLaVA-1.5-7B show significant
performanceimprovements. InstructBLIP-7BachievesanaverageBLEU@1improvementof13.56
pointsacrossthethreedatasets,whileLLaVA-1.5-7Bgainsanaverageof4.22pointsBLEU@1score.
Fine-tuningwithscientificdiagramsenhancesthesemodels’abilitytocomprehendthequestions,
highlightingthepotentialimportanceofourtrainingsetforbuildingpowerful,specializedsystems
forscientificQAinthefuture.
18CoTpromptsandfull-textinputenhancetheQAperformanceofvariousGeminiandGPT-4systems.
Ontest-A,GPT-4owiththefullpaperachievesa41.3BLEU@1score,whichis0.8pointshigher
thanitsdirectQAperformance. TheGPT-4Visionmodelsgainanimpressive5.6pointsinBLEU@1
scorewhenusingCoTcomparedtodirectQA.Similartrendsareobservedintheothertwotestsplits.
Thestep-by-stepfine-grainedCoTreasoningandlong-contextunderstandingabilityofGeminiand
GPT-4modelswiththefullpapercontributetotheseimprovedresults.
F PromptforL3ScoreComputation
Fig.F.1showsthepromptusedforcomputingtheproposedLLMLogScore(L3Score)metricbased
on the log-likelihood of the models responses to binary yes, no questions. We use it to measure
similarityofthemodelpredictedanswerstoagivengroundtruthanswer.
You are given a question, ground-truth answer, and a candidate
answer.
Question: <question>
Ground-truth answer: <GT>
Candidate answer: <answer>
Is the semantic meaning of the ground-truth and candidate answers
similar? Answer in one word - Yes or No.
FigureF.1: PromptUsedforComputingL3Score. Weprovidetheground-truthandcandidate
answersandasktheLLMtodetermineiftheirsemanticmeaningsarepreservedinthecontextofthe
question.
G DetailedAnnotationGuidelinesandUserInterfaces
ThegoaloftheSPIQAtestsetistoassisttheevaluationofmultimodalmodelsonrobustunderstanding
ofresearcharticles. WeprompttheLLM(Gemini1.5pro)togeneratequestionsbasedonagiven
image. ThepromptweuseisshowninFigureG.1and G.2. Aftergeneratingquestionsonallpapers
(≈26kpapers,≈270kimages),wesubset200papersastestsetandfiltertoretainhigherquality
questionsmorepertinenttotheresearcharticle. Inthefilteringprocess,weannotatewhichfigures
andtableshelpansweraquestiontoevaluatethegroundingandCoTreasoningcapabilitiesoflarge
multimodalsystems. TheUIusedforannotationisshowninFig.G.3.
Wemanually verified thequality of thegenerated question andanswer pairsusing the following
criterion:
1. Answering the question would require a complete understanding of the figure and its
importanceinthepaper.
2. Thegeneratedansweriscorrectandtothepoint.
3. The question is neither too trivial nor too specific to the figure or table (e.g., avoiding
questionslike‘WhatdoesthebluelineinFigure1represent?’ or‘Howmanyrowsarethere
inTable2?’ forbeingtrivial)
4. Iftwoormorequestionsfromapaperaresimilar,keepone.
5. Ifthequestionisentirelybasedonthepassagei.e. cannotbeansweredfromtheimage,
discardthequestion-answerpair.
6. Iftheanswerisnotclear,e.g.,theanswersays‘Itishardtoanswerthequestionbasedon
thegiveninformation‘or‘Theanswerisnotevidentfromthegivenpassage‘,discardthe
question-answerpair.
7. Ifthequestion-answerpairincludesphraseslike‘Basedonthepassage,’modifyitbecause
weshowallfiguresandtablestothemodelatonceduringevaluation.
Weinitiallyemployedcrowdworkersatacostof$22perhourforfilteringthequestions. However
after a pilot evaluation of 150 questions which were annotated by two different sets of 3 crowd
workersandtheauthors,wefoundthatthecrowdworkerslackeddomainexpertisenecessarytograsp
thenuancesinthequestions. ExampleofthepilotUIwiththequestionandresponsefromacrowd
workerisshowninFig.G.4. ThefilteringofthefinalSPIQAtestAsetwasdonebytheauthors.
19You are a professor. Generate one question based on the image
and caption to test if a student can interpret and understand the
image well.
Also classify the figure as "plot", "schematic", "photograph(s)",
"table" or "others".
Image:
{{ Image }}
Caption: {{ caption }} \
The passage where the figure is referenced is provided below.\
PASSAGE: {{ passage }} \
Construct your questions and corresponding answers. Use this
format. \
Question: <question that tests understanding of the image.> \
Answer: <Answer to the question based on the passage.> \
Explanation: <How the figure helps answer the question.> \
Figure_type: <"type of figure" where type of figure is one of \
["plot", "schematic", "photograph(s)", "table", "other"]>
FigureG.1: Promptusedforgeneratingquestionsbasedonfiguresinthepaper. Weprovidethe
firstpassagereferencingthefigureasadditionalcontexttothemodel.
You are a professor. Generate one question based on the image
and caption to test if a student can interpret and understand the
image well.
Also classify the figure as "plot", "schematic", "photograph(s)",
"table" or "others".
Image:
{{ Image }}
Caption: {{ caption }}
Construct your questions and corresponding answers. Use this
format.
Question: <question that tests understanding of the image.>
Answer: <Answer to the question based on the passage.>
Explanation: <How the figure helps answer the question.>
Figure_type: <"type of figure" where type of figure is one of
["plot", "schematic", "photograph(s)", "table", "other"]>
Figure G.2: Prompt used for generating questions based on figures in the paper. We do not
includethepassagereferencingthefigureinthepromptwhenweareunabletoextractthefigure
from the tex source (we use pdffigures to extract the image and do not have the mapping to the
correspondingpassage).
H QualitativeexamplesofthetaskanddatainSPIQA
Fig.H.1, H.2andH.3showexamplesoftheSPIQACoTQAtask,requiringtheanalysisofmultiple-
imageswhenansweringquestionsbasedonascientificpaper. IntheSPIQAdataset,thereareon
average10.32images(figuresandtables)perpaper. IntheCoTQAtask,givenaquestionandall
thefiguresandtables,theAIsystemneedstoidentifywhichimageismosthelpfulinansweringthe
questionandthenprovideananswertothequestion.
20Figure G.3: UI used for filtering questions for the SPIQA test-A. Given the image, question,
answerandmodel’sexplanation,weasktheannotatorifthequestioncanbeansweredfromthefigure
andwhetherthequestionshouldbekeptordiscarded. Wealsoaskifthequestionoranswershould
bemodifiedandprovidetextboxesfortheannotatortoincludethemodifiedquestionandanswer.
21FigureG.4:Pilotfilteringshowsthatcrowdworkerslackexpertiseforthetask.Examplequestion
andresponsewithintheUIfromthepilotsetoffilteringonSPIQAtest-B.Initialtrialsusingcrowd
workers demonstrated that the task of filtering and identifying pertinent questions also requires
expertiseinthedomain.
22FigureH.1: ExampledemonstratingtheSPIQACoTQAtask. Gvenaquestionandallthefigures
andtables,theAIsystemneedstoidentifywhichimageismosthelpfulinansweringthequestion
andthenprovideananswertothequestion. R-L:ROUGE-L,BERT:BERTScore,L3S:L3Score.FigureH.2: ExampledemonstratingtheSPIQACoTQAtask. Givenaquestionandallthefigures
andtables,theAIsystemneedstoidentifywhichimageismosthelpfulinansweringthequestion
andthenprovideananswertothequestion. R-L:ROUGE-L,BERT:BERTScore,L3S:L3Score.
24FigureH.3: ExampledemonstratingtheSPIQACoTQAtask. Givenaquestionandallthefigures
andtables,theAIsystemneedstoidentifywhichimageismosthelpfulinansweringthequestion
andthenprovideananswertothequestion. R-L:ROUGE-L,BERT:BERTScore,L3S:L3Score.
25I ErrorAnalysis
Fig.I.1showsexampleswhereallthemodelsretrievethecorrectfigure(tableorimage)thathelps
answer the question. However, in many cases the models do not correctly answer the question.
Weobservethatinthecaseoftables(representedasimages),modelshavedifficultyparsingand
comprehendingtheinformationandmakingerrors. Thishighlightsroomforfurtherimprovements
foradvancedsystemsintermsofcomprehendingtablecontentrepresentedasfigures.
Figure I.1: Error Analysis. Examples of questions and answers where some models respond
incorrectly. Modelsstruggletofullyparseandunderstandtheinformationincomplextables. The
L3Scorecorrectlyindicatesthatthemodelresponsesdonotcapturetheexpectedanswer. Metrics
coloredingreendenotecorrectevaluations, whilethoseinredindicateincorrectscoring. BERT:
BERTScore,L3S:L3Score.
26