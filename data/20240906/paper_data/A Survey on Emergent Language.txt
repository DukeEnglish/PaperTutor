A Survey on Emergent Language
Jannik Peters 1*, Constantin Waubert de Puiseau 1, Hasan Tercan 1,
Arya Gopikrishnan 2†, Gustavo Adolpho Lucas De Carvalho 3†, Christian Bitter 1,
Tobias Meisen 1
1University of Wuppertal, Institute of Technologies and Management of the Digital
Transformation, Rainer-Gruenter-Str. 21, Wuppertal, 42119, NRW, Germany.
2Drexel University, College of Engineering, 3141 Chestnut St, Philadelphia, 19104, PA, USA.
3University of Southern California, Department of Computer Science, 941 Bloom Walk, Los
Angeles, 90089, CA, USA.
*Corresponding author(s). E-mail(s): jpeters@uni-wuppertal.de;
Contributing authors: waubert@uni-wuppertal.de; tercan@uni-wuppertal.de;
ag3974@drexel.edu; lucasdec@usc.edu; bitter@uni-wuppertal.de; meisen@uni-wuppertal.de;
†Work done during and after a DAAD RISE internship at Institute of Technologies and
Management of Digital Transformation.
Abstract
Thefieldofemergentlanguagerepresentsanovelareaofresearchwithinthedomainofartificialintelligence,
particularly within the context of multi-agent reinforcement learning. Although the concept of studying
languageemergenceisnotnew,earlyapproacheswereprimarilyconcernedwithexplaininghumanlanguage
formation, with little consideration given to its potential utility for artificial agents. In contrast, studies
basedonreinforcementlearningaimtodevelopcommunicativecapabilitiesinagentsthatarecomparableto
or even superior to human language. Thus, they extend beyond the learned statistical representations that
are common in natural language processing research. This gives rise to a number of fundamental questions,
fromtheprerequisitesforlanguageemergencetothecriteriaformeasuringitssuccess.Thispaperaddresses
these questions by providing a comprehensive review of 181 scientific publications on emergent language in
artificial intelligence. Its objective is to serve as a reference for researchers interested in or proficient in the
field.Consequently,themaincontributionsarethedefinitionandoverviewoftheprevailingterminology,the
analysis of existing evaluation methods and metrics, and the description of the identified research gaps.
Keywords:emergentlanguage,emergentcommunication,artificialintelligence,reinforcementlearning,multi-agent
1 Introduction
Communicationbetweenindividualentitiesisbasedonconventionsandrulesthatemergefromthenecessityor
advantage of coordination. Accordingly, Lewis [1] formalized settings that facilitate the emergence of language
as“coordinationproblems” [1]andintroducedasimplesignalinggame.Thisgame,inwhichaspeakerdescribes
an object and a listener confronted with multiple options has to identify the indicated one, extensively shaped
the field of emergent language (EL) research in computer science. Early works examined narrowly defined
questions regarding the characteristics of emergent communication (EC) via hand-crafted simulations [2–12].
Theseapproachesmostlyutilizedsupervisedlearningmethodsandnon-situatedsettings,limitingthemintheir
abilitytoexaminetheoriginsanddevelopmentofcomplexlinguisticfeatures[2].However,ELresearchrecently
experiencedanupsurge[13–20]withafocusonmulti-agentreinforcementlearning(MARL)approaches[21–32]
to enable the examination of more complex features.
One fundamental goal of EL research from the MARL perspective is to have agents autonomously develop
a communication form that allows not only agent-to-agent but also agent-to-human communication in natural
1
4202
peS
4
]AM.sc[
1v54620.9042:viXralanguage(NL)stylefashion[2,16,24,29,33,34].Therefore,reinforcementlearning(RL)methodsareattractive
from two points of view. First, successful communication settings might lead to agents that are “more flexible
and useful in everyday life” [35]. Furthermore, they may provide insights into the evolution of NL itself [36]. EL
is the methodological attempt to enable agents to not only statistically understand and use NL, like natural
language processing (NLP) models that learn on text alone [37, 38], but rather to design, acquire, develop, and
learn their own language [39, 40]. The autonomy and independent active experience of RL learning settings is a
crucialdifferencetothedata-drivenapproachesinthefieldofNLP[41–43]anditslargelanguagemodels(LLMs).
AccordingtoBrowningandLecun,“weshouldnotconfusetheshallowunderstandingLLMspossessforthedeep
understanding humans acquire” [40] through their experiences in life. In EL settings, the agents experience the
benefits of communication through goal-oriented tasks [44] just like it happens naturally [1] and therefore have
theopportunitytodevelopadeeperunderstandingoftheworld[33,45].Hence,advancesinELresearchenable
novel applications of multi-agent systems and a considerably advanced form of human-centric AI [35].
In the current state of EL research, numerous different methods and metrics are already established but
they are complex to structure and important issues remain regarding the analysis and comparison of achieved
results [29, 35, 46]. Therefore, we see a need for a taxonomy to prevent misunderstandings and incorrect use of
establishedmetrics.Inthispaper,weaddresstheseissuesbyprovidingacomprehensiveoverviewofpublications
inELresearchandbyintroducingataxonomyfordiscreteELthatencompasseskeyconceptsandterminologies
of this field. Additionally, we present established and recent metrics for discrete EL categorized according to
the taxonomy and discuss their utility. Our goal is to provide a clear and concise description that researchers
can use as a shared resource for guidance. Finally, we create a summary of EL research that highlights its
achievements and provides an outlook on future research directions. We base our work on a comprehensive and
systematicliteraturesearchwithreproduciblesearchtermsonwell-knowndatabases.WefollowthePRISMA[47]
specifications and show a corresponding flow diagram in Figure B1 in Appendix B. The literature search and
review process as well as its results are described in detail in Section 4. All identified work has been reviewed
and categorized according to an extensive list of specific characteristics, e.g. regarding communication setting,
game composition, environment configuration, language design, language metrics, and more.
PrevioussurveysofELincomputersciencefocusedonlyonasubgroupofcharacteristicsorspecificpartsof
thisresearcharea.Someoftheseearliersurveysfocusonspecificlearningsettings[44,48,49],onmethodological
summaries and criticism [29, 39, 50–54], or provide a more general overview [24, 35, 36, 46, 55–57]. The most
similar ones to our work are [35] and [57]. [35] gives an introduction and overview of the EL field before 2021,
however, it is mostly a summary of previous work and does not provide a taxonomy or review of existing
metrics in the field as we do. [57] focuses on common characteristics in EC research and the development of
emergent human-machine communication strategies. They discuss distinctions and connections of EC research
to linguistics, cognitive science, computer science, and sociology, while we focus on emergent language and its
analysis. We describe and discuss all relevant surveys in more detail in Section 3.
Based on this preliminary work, the current state of research on EL misses an overarching review and a
comprehensive compilation and alignment of proposed quantification and comparability methods. Accordingly,
the key contributions of the present survey are:
• A taxonomy of the EL field, in particular regarding the properties of discrete EL, see Section 5.
• An analysis of quantification approaches and metrics, including their categorization, see Section 6.
• A summary of open questions and an outlook on potential future work, see Section 7.
In addition, we introduce the fundamental concepts of NL and EC that underlie our survey in Section 2.
As mentioned, we provide a detailed summary of related surveys in Section 3. Section 4 describes our study
methodology, including the keywords and terms of our systematic literature search. Finally, Section 8 offers a
concluding discussion and final remarks.
2 Background
To contextualize the presented taxonomy and analysis, this section summarizes the key concepts of communi-
cation and linguistics and provides an overview of EL research.
2.1 Communication
Communication at its very basis is the transfer or exchange of signals, which can be interpreted to form
some information. These signals include both intended, such as deliberate utterances, and unintended, such
as uncontrolled bodily reactions, and include both explicit and implicit parts [58]. According to Watzlawik’s
“Interactional View” [59], “one cannot not communicate”. In this regard, communication is ubiquitous and
2Fig. 1 The different forms of communication. They are divided by type of recipients and purpose. Intrapersonal communication
encompassesself-centeredcommunicationlikeinternalvocalization.Theremainingformsofcommunicationaredirectedexternallyand
areutilizedtotransmitinformationtoindividuals,intheinterpersonal setting,orgroupsofaddressees.Ingroup communication the
participantsusuallyhaveacommongoal,whereaspublic communication focusesonthegeneraltransferofinformationtoagroupof
interestedbutnotnecessarilygoal-alignedentities.Finally,mass communication isusedtodescribeanyformofcommunicationthat
is directed towards a general audience and focuses availability, for example, through the use of various media, including the internet.
Adaptedfrom[64].
necessary, occurring through various channels and modes [40, 60–63]. Depending on the specific channel and
purpose, communication can be roughly divided into the five forms depicted in Figure 1.
In the context of EL, two of these forms are actively studied (see Section 5), namely interpersonal com-
munication and group communication. Interpersonal communication is communication between entities that
mutually influence each other, and its general setting is depicted in Figure 2. This form of communication is
based on individual entities, each within its perceivable environment. Although these environments are agent-
specific, they overlap and allow communication through a common channel. In addition, there may be noise in
this process that affects the perception of the environment or the communication itself. Group communication,
on the other hand, differs only in the number of entities involved and the communication goal. Usually, group
communication is more formal and focuses on a common goal or group task while interpersonal communication
has a social character and might only relate to a goal or task of one of the participants. Accordingly, the group
communication setting canbe foundinmost population-basedEL research. Intrapersonal communication (e.g.,
internal vocalization), public communication (e.g., lectures), and mass communication (e.g., blog entries) are
not currently examined in the EL literature.
Generally, communication can be seen as a utility to coordinate with others, as motivated by linguistic and
computer science research alike [65–68]. Conversely, the necessity for collaboration within a collective may be a
fundamentalprecursortotheevolutionandsustainedfunctionalityofexplicitcommunication[4,52].Thistheory
leads to an essential differentiation regarding context-dependent communication. Meaningful communication
might emerge in a cooperative but not in a fully competitive or manipulative setting. However, a partially
competitive setting might be vital for the emergence of resilient and comprehensive communication, e.g. to
enable the detection and use of lies [34]. Accordingly, the level of cooperation is a defining element of the
communication setting in EL research, as outlined in Section 5.1.
NLisonemajoraccomplishmentofhumanitythatisutilizedinallformsofcommunication(seeSection2.2).
Itisatoolthatallowsustoencodeverycomplexinformationwithinadiscreteandhumanlymanageableamount
ofutterances.AlotofartificialintelligenceresearchaimstodevelopNLmodels,withapplicationsrangingfrom
translationtocoherentfull-textgenerationbasedonsingle-wordinput[43,70–72].However,atheorywithmany
supporters from the AI community [40, 65, 73, 74] states that current “intelligently designed statistical models
trained on large static datasets [...] do not produce an understanding of language that can lead to productive
cooperation with humans” [18]. Correspondingly, the recently evolved field of EL research in AI aims to enable
agents to utilize intended communication in the same way humans use it to increase cooperation, performance,
andgeneralizationand,inthelongrun,enabledirectmeaningfulcommunicationbetweenhumansandartificial
systems [18, 34]. In line with this, multiple explicit forms of EC in artificial intelligence research have been
investigatedasshowninSection5.4.2.Incontrast,workfocusingonimplicitcommunication,liketheinformation
content of spatial positioning of agents in a multi-agent setting [75], is not part of the present survey.
2.2 Natural Language
NLisaprimeexampleofaversatileandcomprehensiveformofcommunicationdesignedtoconveymeaning[76].
TheflexibilityofNLallows humanstobeexactbutalsodeliberatelyambiguousintheircommunication[77].It
is a vital feature that distinguishes us from other species and gives us a great advantage in terms of knowledge
storage, sharing, and acquisition [77]. However, the origin and evolution of language is still a mystery [78, 79].
Inthefieldoflinguistics,manyconflictingtheorieshavebeenintroducedsofar[79–81],rangingfrombehavioral
to biological explanations. Additionally, accompanying research in the field of computer science has a long
history [31] with a comparable range of theories. Even though there is still a debate around this topic, it is
3Noise
Communicator A Communicator B
sends, receives, Channel(s) sends, receives,
assigns meaning assigns meaning
A's environment B's environment
Fig. 2 Interpersonal communication. Actors are communicator A and B, each depicted by orange circles. They are each situated
in their individual environment, depicted by the blue and green ellipses. At the overlap point the read arrow indicates the available
communicationchannel.Thepotentialenvironmentalnoise,influencingthecommunication,isrepresentedbygreyarrowsgoingthrough
theentireimage.Adaptedfrom[69].
commonly agreed upon that a very intricate evolutionary process was involved [77, 79]. This evolution most
likely took place in two different areas simultaneously, biologically and linguistically. On the biological side, the
human brain most likely developed specific areas and functionalities specifically for more complex language-
basedcommunication,thatarestudiedinthescientificfieldofneurolinguistics[82].Onthelinguisticsside,this
evolution can be seen in language development itself, which is a constantly ongoing process [77]. Similarly, EL
is concerned with the research of suitable model structures for the processing of language, while concurrently
developing and evaluating language.
Whiletheexactoriginoflanguageishighlydebatable,theactualcommunicationprocessviaNLisgenerally
easier to conceptualize. For example, it can be modeled by the semiotic cycle depicted in Figure 3 [44, 83].
This depiction applies to multiple expressive channels, e.g. speech and writing. It assumes at least two involved
parties, a speaker and a listener. The speaker produces an utterance based on the meaning to be conveyed.
Thismeaningresultsfromthecombinedconceptualizationofthespeaker’sgoalandmodeloftheworld.Onthe
other hand, the listener receives the utterance and comprehends it to derive a meaning, which is not a direct
copy of the initial one by the speaker but it still refers to the shared world. The interpretation of the meaning,
which the listener’s world model informs, leads to some action by the listener. At the center of this process are
the shared world and the respective world models of speaker and listener that function as grounding for the
information exchange via language. Further, both linguistic level components of production and comprehension
allow the respective agent to participate in the language process.
The semiotic cycle puts the utterance as an externalized information carrier into focus. While the other
componentsareinternalizedandthusdifficulttodefineandmeasure,theutteranceitselfisexternalandavailable
for analysis. Fundamentally, this specific utterance is based on the underlying communication process and
specifically, the language used. Accordingly, most research papers investigate characteristics of the utilized
languagetoanalyzethecommunicationpossibilitiesandcapabilitiesofusers.Tothisend,linguisticssubdivides
the language structure into six major levels [84–86], as illustrated in Figure 2.2. This structure was originally
developed for spoken language, as indicated by the terms ‘phonetics’ and ‘phonology’ derived from the Greek
word ‘phon’ meaning ‘sound’. However, the levels are also applicable to written language in the context of EL.
Therefore, the following description will address both spoken and written language within this framework.
The phonetics level includes the entire bandwidth of the chosen, often continuous, language channel. For
example,itcomprisesthefullrangeofpossiblespeechsoundsavailabletohumans.Consequently,itisfundamen-
tal for the general transfer range and describes it without any limitation. At the phonology level are the atomic
building blocks of the spoken or written language, defined as phonemes or graphemes. A phoneme or grapheme
enables the creation of meaning as well as the necessary distinction at the lowest level of language. However, in
aNLwithanalphabeticwritingsystem,phonemesandgraphemes,whichinthiscasecorrespondtoletters,are
often not a direct match and are only roughly related. Nevertheless, these individual units comprise the set of
used elements from the continuous channel range for a specific language. These are used and combined at the
morphology level to create and assign meaning by making words, in linguistics called lexemes. In this context,
4Speaker Listener
sensori-
world
motor
level
goal world model world model action
conceptualization interpretation
conceptual
level
meaning meaning
production comprehension
linguistic
level
utterance
Fig. 3 The semiotic cycle. This framework of a language-based exchange between two separate entities, called speaker and listener,
categorizestheprocessintothreelevels.Thesensori-motor level encompassessensorandworld-orientedcomponents,theconceptual
level includesinternalandintangiblepartsliketheindividualworldmodelandconceptualizationcapabilities,andthelinguistic level
consists of the production and comprehension of the linguistic exchange, which is the externalized connection between speaker and
listener.Adaptedfrom[44,83].
Fig. 4 Majorlevelsoflinguisticstructure.Thisconceptualstructuredepictstheelementsofalanguageasconcentricringsthatbuild
upon each other. From the inner circle, phonetics, through phonology, morphology, syntax, and semantics, to pragmatics. Adapted
from[84].
5word-forming rules and underlying structures are of interest. Utilizing these meaningful building blocks, sen-
tences can be realized at the syntax level. This level only concerns the structure of sentences and in particular,
their assembly rules and the word categories that are used. The meaning of these sentences is relevant at the
next level, semantics. At this level, the literal meaning of language constructions is of interest while the final
level, pragmatics, focuses on how context contributes to the meaning. Accordingly, it analyzes how language is
usedininteractionsandtherelationshipbetweentheinvolvedparties.Overall,thepresentedlevelsarenotonly
important to describe language functionally and structurally but also to distinguish language characteristics
and metrics. Thus, we use them to organize parts of the taxonomy in Section 5 and the metrics in Section 6.
2.3 Emergent Language
EL refers to a form of communication that develops among artificial agents through interaction, without being
explicitly pre-programmed. Thus, it is a bottom-up approach, arising from the agents’ need to cooperate and
solve tasks within a given environment [57]. This process involves the agents creating, adapting, and refining
linguisticstructuresandmeaningstoenhancetheirabilitytoexchangeinformationeffectivelyandefficiently[48].
EL research aims to understand the principles and mechanisms underlying this spontaneous development of
communication. It explores how linguistic elements such as syntax [87], semantics [88, 89], and pragmatics [29]
can arise from the interaction of artificial agents and how these elements contribute to the agents’ performance
and cooperation.
A NL-like communication form would make artificial agents and computer systems, in general, more acces-
sible, simpler to comprehend, and altogether more powerful [24, 34, 35]. EL research originally focused on the
question of language origin [3]. Recently, this focus shifted to the more functional aspect of EL, focusing on
how to enable agent systems to benefit from a mechanism that helped humanity thrive and how to achieve
communication capabilities as close as possible to NL [35]. Today, EL within computer science is about self-
learned [4], reusable [45], teachable [90, 91], interpretable [14], and powerful [18] communication protocols. In
thelongrun,ELaimstoenablemachinestocommunicatewitheachotherandwithhumansinamoreseamless
and extendable manner [35, 92].
Accordingly, various research questions and areas were derived. For example, recent papers have addressed
issues around the nature of the setting, which can be semi-cooperative [34, 93], include adversaries [22, 32],
havemessage-influencingnoise[94],orincorporatesocialstructures[95,96].Moreover,someareconcernedwith
the challenge of grounding EL, e.g. using representation learning as basis [97], combining supervised learning
and self-play [98], or utilizing EL agents as the basis for NL finetuning approaches [92]. Others tackle the
direct emergence of language with NL characteristics, e.g. looking at internal and external pressures [99–103],
evaluating factors to enforce semantic conveyance [52], looking at compositionality [104], generalization [25], or
expressivity[105],orquestioningtheimportanceofcharacteristicslikecompositionality[106]andtheconnection
between compositionality and generalization [107].
Based on these examples and the introduced goals and approaches, the difference in comparison to NLP
research becomes apparent. Current approaches in NLP, namely LLMs, learn language imitation via statistics,
but they do not capture the functional aspects and the purpose of communication itself [18, 40]. In contrast,
ELuseslanguagenotasthesoleobjectivebutasameanstoachievesomethingwithmeaning[23].Accordingly,
agents have to learn their own EL to enable functionality beyond simple statistical reproduction. Specifi-
cally, agents should learn communication by necessity or benefits [99] and they need a setting that rewards or
encourages communication, e.g., an at least partially cooperative setting [34].
WhiletheELconceptsoundssimple,itcomeswithmanychallenges.Encouragingcommunicationalonecan
lead to simple gibberish that helps with task completion but does not represent the intended natural language
characteristics[108].Providingtherightincentivesforlanguagedevelopmentisthereforecrucial.Inaddition,it
is important to examine how agents use communication and the opportunity to send and receive information,
raisingthequestionofhowtomeasuresuccessfulcommunication[29].Themeasurabilityoflanguageproperties
such as syntax, semantics, and pragmatics is also important for assessing the emergence of desirable language
properties[107].Thefollowingsectionsexplorethesechallengesandrelatedconstructsandapproachesindetail.
3 Related Surveys
As briefly mentioned in Section 1, our literature review identified 19 publications that we classified as surveys.
We adopted a broad definition of what constitutes a survey, categorizing any publication as a survey if it either
explicitly described itself as such or provided a particularly comprehensive and structured review of previous
research. These publications conduct similar investigations on EL research but with different scopes. We focus
on discrete language emergence, associated taxonomy, characteristics, metrics, and research gaps. In contrast,
6Table 1 PreviouslypublishedsurveysonEL,organizedaccordingtotheirprimaryfocus.
Settings van Eecke and Beuls [44], Lipowska and Lipowski [48], Denamganaï and Walker [49]
Methods Korbak et al. [50], LaCroix [51], Lemon [39], Lowe et al. [29], Mihai and Hare [52],
Galke and Raviv [53], Vanneste et al. [54]
General Hernandez-Leal et al. [46], Brandizzi and Iocchi [24], Moulin-Frier and Oudeyer [55],
Galke et al. [36], Fernando et al. [56], Suglia et al. [109], Zhu et al. [110],
Lazaridou and Baroni [35], Brandizzi [57]
in our review of the existing survey work, three distinct interpretive directions emerge, which we categorize as
summarized in Table 1: Surveys that focus on the learning settings [44, 48, 49], surveys that summarize and
review utilized methods [29, 39, 50–54], and surveys that provide a general discussion or overview of the EL
field [24, 35, 36, 46, 55–57]. The following section briefly summarizes these surveys within these categories.
Settings
Surveysareclassifiedwithinthesettings categorywhentheprimaryfocusisonthegenerallearningproblem,the
environment,andthedesignofthelanguagelearningsetting.vanEeckeandBeuls[44]providedacomprehensive
overview of the language game paradigm and lined out common ideas in MARL research. They categorized
distinct types of experiments in this paradigm and identified properties that should be considered in MARL
research, e.g. symmetric agents taking all roles or fully autonomous behavior. Our survey, on the other hand,
addresses approaches beyond the language game paradigm with increased detail (see Section 7). Similarly,
Lipowska and Lipowski [48] reported on the state of the art in EL based on MARL, focusing on the language
game. Their paper discussed the explainability of protolanguages developed in the surveyed work as well as
sociocultural approaches, such as migration or teachability, with an emphasis on the naming game and simple
one-word communication. While these aspects are part of our analysis, our review goes further by placing
them in a common context. Denamganaï and Walker [49] reviewed literature related to referential games to
generate a nomenclature, leading to the development of the ReferentialGym framework. Their paper included
somewell-knownELmetrics,suchaspositivesignalingandpositivelistening[29],primarilyaimingtointroduce
ReferentialGym as a comprehensive research framework. Although referential games are part of our analysis,
they represent only a small part of our survey. Additionally, our work discusses multiple metrics implemented
in their framework.
Methods
The methods category contains surveys that primarily address learning methods and methods of evaluation.
Korbak et al. [50] discussed existing compositionality metrics and highlighted different types of composition-
ality, which are not fully addressed in the current literature. The authors argued that most EL research and
metrics emphasize the communication aspect of learned representations, like symbol sets and simple concate-
nation. At the same time, NL are “non-trivially compositional” [50] and require an analysis of the semantic
perspective. Hence, they introduced a metric called tree reconstruction error. Our discussion of compositional-
ity (see Section 5.4.5) is shorter, but we refer interested readers to [50] as we also include the proposed metric
(see Section 6.4.2).
LaCroix [51] argued that there is an overemphasis on compositionality in EL research, noting that no
evolutionaryprecursorcouldbeidentifiedsofarthatsupportsthisfocus.Theauthorsuggestedthatapproaches
shouldratherfocusonreflexivity-theabilitytotakeadvantageofpreviouslyevolvedcommunicativedispositions
toshapefuturedispositions.However,reflexivitymetricshavenotbeenestablishedyet,sotheyarenotincluded
in our survey.
Lemon [39] reviewed language grounding, specifically the combination of symbolic grounding and conversa-
tional grounding. Symbolic grounding, as further lined out in Section 5.4.5, describes simple symbol-to-concept
connections,whileconversationalgroundingenablesagentstoadapttheirlanguageandlearnnewconcepts[39].
The author argues for better data collections to resolve disagreements and clarify ambiguities. As no metrics
were proposed, we do not delve further into this topic in our survey.
Loweetal.[29]focusedonlanguageutilityratherthansemanticcharacteristics,reviewingmetricsrelatedto
the usefulness of emergent protocols. The authors also proposed metrics such as positive signaling and positive
listening, arguing for causal relationships over reward-based metrics when investigating EC. This work is one
of the main inspirations for our Section 5.4.6 focusing on language utility.
MihaiandHare[52]motivatedtheexplorationoffactorsthatconveysemanticsratherthanlow-levelhashes
ofanenvironmentortask.Intheirreview,theycriticizedthelackofinsightsintothesemanticsofemergedlan-
guagesandhighlightedtheimportanceofdisentanglingandmeasuringsemanticsasafutureresearchdirection.
7As they do not propose additional metrics, we include their work here as a general reference without discussing
it more extensively.
In their review of language pressures and biases employed in EL research, Galke and Raviv [53] sought to
resolve mismatches between neural agent EL and human NL. They identify four pressures and discuss their
utilityaswellastheirinclusionintotheELprocess,eitherbecausetheyareinherenttotheobjectiveorthrough
inductive biases. These pressures facilitate the generation of NL phenomena in EL that are also part of this
survey. However, our focus is on the measurability of these phenomena and characteristics, rather than on the
training biases that potentially give rise to them.
Vannesteetal.[54]analyzedandcompareddiscretizationmethodsforcommunicationlearningwithMARL.
They concluded that methods like discretize regular unit (DRU), straight through DRU, and straight through
Gumbel-Softmax are suitable for general use but emphasized that the best method depends on the specific
environment. Discretization methods are essential for discrete EL learning, but they are not the focus of our
survey, so we refer interested readers to [54] for further details.
General
Surveys in this general category do not fit into the other categories as they either have a focus that does not fit
the settings or methods classification or are providing a general overview of the field. Hernandez-Leal et al. [46]
provided an extensive overview of the field of multi-agent deep reinforcement learning in general, proposing
four categories to group recent work: emergent behavior, learning communication, learning cooperation, and
agentsmodelingagents.Inthesegmentonlearningcommunication,theauthorsintroducedmultipleapproaches
through short paper summaries without going into too much detail or critical discussion. Overall, the paper
is a compelling source in regard to the historical development of the field and to get an overview of recent
developments in multi-agent learning in general. Additionally, the authors provided a conclusive list of lessons
learned, practical challenges, and open questions. Our survey includes the work mentioned in the learning
communication part of [46] but also several additional sources, and more importantly, we examine and review
these with a different focus. Nevertheless, we explicitly recommend [46] as an extensive survey of the state of
multi-agent deep reinforcement learning in general.
Brandizzi and Iocchi [24] advocated for a general human-in-the-loop concept within EC research to empha-
size the interaction between humans and artificial intelligence (AI). The authors argued that, so far, human
interaction is extremelyunderrepresentedin thefield.Tosubstantiatethis claim, theycompared common char-
acteristicsofELresearchandthemodelingofaspectsofhuman-humaninteractions,focusingonacategorization
of interaction types and the theory of mind approach. Accordingly, they elaborated in depth on the possible
interaction and communication settings, e.g. types of cooperation and competitiveness, but did not provide a
comprehensive categorization of existing papers in the field as we do.
Moulin-Frier and Oudeyer [55] provided a short review of MARL research to put it into perspective with
historical linguistic research and theories. To do so, they summarized established theories on the formation
of language and formulate future challenges for MARL, e.g. decentralized learning, plausible constraints, and
intrinsic motivation. The authors have provided an interesting and inspiring non-technical view, but they do
not discuss appropriate metrics, nor do they provide a detailed explanation of the characteristics of emergent
language.
Galke et al. [36] gave a survey of EC approaches using RL, by examining 15 papers in detail. Additionally,
the authors focused on the perceived mismatch between EL and human NL. The compositionality of NL was
used as an exemplary feature to accentuate the shortcomings of ELs so far. The authors concluded that key
cognitiveandcommunicativeconstraints,whichessentiallyformNL,arestillmissinginthesimulationsutilized
forELlearning,e.g.memoryconstraintsandrolealternation.Thesefindingsarealsodiscussedinourworkand
put into a wider context of publications.
Fernando et al. [56] also provided a brief review of language-based EC approaches. The shortcomings of
thosewereusedtomotivateadrawing-basedcommunicationapproachandaccompanyingcommunicationgame
variants. While this work exhibits an interesting proposal it does not provide enough implementation details
and is thematically outside of the focus group of our survey. Nevertheless, we also report on and discuss
drawing-based communication approaches.
Sugliaetal.[109]providedacategorizationandanalysisofvisuallygroundedlanguagegames,datasets,and
models.Thefocusoftheirsurveyisthemultimodalgroundingapproachassociatedwithvisuallanguagegames.
Visually grounded language games are categorized into discriminative, generative, and interactive tasks. EC is
categorized as interactive and thus part of “the most relevant class of language games to study the problem of
grounded language learning” [109] according to the authors.
8Table 2 Literaturedatabasesandsearchqueriesusedforthepresentsurveyandthenumberofresultsobtainedforeach.
Source Query Results
ScienceDirect “emergentlanguage” 5
Subjectareas:ComputerScience
IEEEXplore “emergentlanguage” 9
ACMDigitalLibrary All:“emergentcommunication” 60
All:“emergentlanguage” 19
WebOfScience TS=(“emerg* communication” or “emerg* language”) NOT TS=emergency NOT TS=5G 16
NOTTS=wireless
Refinedby:WEBOFSCIENCECATEGORIES:(COMPUTERSCIENCEARTIFICIAL
INTELLIGENCE)Timespan:Allyears.
arXiv all=“emergentcommunication” 207
all=“emergentlanguage” 66
all=EmergentMulti-AgentCommunication 208
SemanticScholar multiagentemergentlanguage 23
FilteredbytopicComputerScience
References - 23
Zhu et al. [110] derived nine dimensions to structure EC works. These dimensions include controlled goals,
communication constraints, communicatee type, communication policy, communicated messages, message com-
bination, inner integration, learning methods, and training schemes. Their work focused on learning tasks with
communication rather than the EL itself and is thus a recommended complement to our work.
Lazaridou and Baroni [35], as mentioned earlier, is similar to our work. The paper included a concise intro-
duction and overview of the EL field, featuring the different types of communication, language understanding,
language characteristics, and settings. However, it was mostly a summary of previous work and does not focus
onthemetricsandquantificationofELasmuchaswedo(seeSection6).Additionally,weprovideanextensive
taxonomy to provide a structured overview of the concepts and wording in the field (see Section 5).
Brandizzi [57] is also similar to our work. This survey reviewed EC literature to establish common charac-
teristics within the field, resulting in four categories: game environment, learning paradigm, interaction types,
and theory of mind. It tries to draw parallels to fields like linguistics, cognitive science, computer science, and
sociology to derive open challenges for emergent human-machine communication. It also discussed some of the
metrics we include in this survey. Even though it included a linguistics view, it did not use established frame-
works from this field to derive a comprehensive taxonomy like we do (see Section 5). Additionally, it is based
on the reviewof 73 publications which were foundvia cross-referencing and Connected Paperswhile weinclude
181 publications identified in a systematic literature search.
4 Study Methodology
Theliteraturesearchthatresultedinthebodyofworksurveyedinthispaperwasconductedonthe17th ofJune
2024. The used libraries and databases are: ScienceDirect, IEEE Xplore, ACM Digital Library, WebOfScience,
arXiv,andSemanticScholar.SemanticScholarisaspecialcase,duetothenatureoftheprovidedsearchmachine
that does not allow complex queries and filtering like the others. Consequently, we hand-picked suitable papers
from the first 50 entries of the search result list. A PRISMA [47] flow diagram of the publication selection
process is provided in Figure B1 in Appendix B. Additionally, the individual queries and results of all services
are summarized in Table 2. The queries delivered 613 hits in total which resulted in 516 unique papers. A
first quick read of these papers led to 23 additional papers, referenced by some of the originally found work.
Accordingly, the literature review started with a corpus consisting of 539 individual papers.
Of the 539 papers, 327 were sorted out due to the substantial divergence from the searched topic, often
focusing on domains like 5G, networking, and radio. Of the remaining 212 papers, 181 directly address the
field of interest, while 31 are only partially relevant. Papers were deemed partially relevant if they mentioned
the surveyed topic but primarily focused on different areas such as datasets, language theory, simulation, or
unrelated case studies. In conclusion, this survey mainly reviews 181 papers that directly discuss or contribute
to the topic of EL in computer science.
Figure 5 presents the distribution of the 181 relevant publications over the years, categorized by publication
type. The topic of EL has maintained a steady presence in conference publications, peaking in 2020. The
subsequent decline in total publications may be attributed to the absence of recent topic-specific workshops.
Additionally, the surge in interest in LLM technologies might have diverted attention from EL research. It is
also worth noting that some recent studies may not have been openly published at the time of our literature
search. We therefore expect the publication count to increase by 2024.
9Fig. 5 Numberofpublicationsperyearandbytype.Thenumberofpublicationsperyearisprovidedintheleftmostcolumn,andthe
distributionofdifferentpublicationtypesisshownintheremainingcolumns.
5 Taxonomy of Emergent Language
In the course of our comprehensive literature review, we identified recurrent instances of taxonomic inconsis-
tencies due to missing standardization [111] and “ill-adapted metrics” [25]. Particular concern arises from the
discrepancy between the concepts intended for measurement and their corresponding metrics, or the absence
of such metrics [45, 50, 112–114]. This section is dedicated to the formulation of a systematic taxonomy aimed
at enhancing comparability and mitigating confusion within the field. This taxonomy forms the basis for the
following sections and is designed to ensure consistent representation throughout the survey. It is created with
the hope that it will serve as a cornerstone for future research, promoting the use of standardized terminology,
particularly in the domain of language characteristics.
The taxonomy first describes the main factors influencing the EL, before categorizing the language char-
acteristics. These influencing factors have a significant impact on the investigative possibilities of EL research
and are therefore of particular importance when analyzing EL. Thus, the taxonomy introduces a classifica-
tion system for the communication setting (Section 5.1) and communication games (Section 5.2) that agents
encounter during language emergence. The communication setting encompasses factors such as the number of
agents and the type of communication available to them. The communication game involves the environmental
configurationandcrucialfactorsinfluencingchallengesandthecomplexityofmulti-tasklearning.Furthermore,
a short discussion on the concept of language priors is provided in Section 5.3, considering that the presence
of a prior significantly influences the characteristics of the emerging language [14, 98]. We conclude this section
with a comprehensive overview of the concepts and characteristics examined within EL research (Section 5.4).
The taxonomy adheres to the six major linguistic structural levels introduced in Section 2.2 and illustrated in
Figure 2.2.
5.1 Communication Setting
In the literature, several communication settings are represented. One distinguishing factor is the number of
agents involved. We derived three classes - the single agent, dual agent, and population setting. While the
single agent setting is rare, the other two are well represented in the examined literature, as shown in Table 3.
A single agent is typically used to train human-machine interfaces [115] or fine-tune existing models [37]. In
contrast, dual-agent settings are more common and often involve a pair of speaker-listener agents, with one
agent designated as the speaker and the other as the listener exclusively [14]. The population setting involves
larger groups of agents in the language emergence process. This requires more computational resources but
also enables more possibilities for regularization [21] and language evolution [102]. Accordingly, the population
setting offers more opportunities to actively shape the process [21, 44, 116].
10Table 3 Classificationofthecommunicationsettingsintheliteraturereviewed.
n
Agents Cooper Sat
yio
m
met Rr
ey
cipie
Pnts
aper
(cid:160) (cid:4) p ◎ 37, 88, 117
✓ ◎ 115, 118, 119
' (cid:4) p ◎ 14, 15, 16, 25, 30, 31, 33, 34, 45, 65, 87, 89, 91, 92, 94, 99, 100, 102, 104, 105, 106, 107,
108, 112, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135,
136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,
154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171,
172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185
✓ ◎ 19, 97, 98, 113, 116, 118, 119, 158, 164, 182, 186, 187, 188, 189, 190, 191, 192, 193, 194,
195, 196
7 p ◎ 34
✓ ◎ 194
(cid:21) p ◎ 34
† (cid:4) p ◎ 25, 26, 30, 31, 91, 101, 102, 103, 150, 164, 165, 180, 197, 198, 199, 200, 201, 202
— 30, 31, 91, 102, 203, 204
✓ ◎ 21, 22, 90, 95, 96, 97, 116, 164, 187, 192, 205, 206, 207, 208, 209, 210, 211, 212, 213,
214, 215
— 18, 20, 23, 27, 28, 97, 187, 192, 208, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222,
223, 224, 225
7 p — 32, 203, 226
✓ ◎ 17, 22, 93
Agents:(cid:160)Single,'Dual,†Population
Cooperation:(cid:4)Cooperative,7Semi-cooperative,(cid:21)Competitive
Symmetry:pNo,✓Yes
Recipients:◎Target,—Broadcast
Anadditionalfactorthatshapesthecommunicationsettingisthetypeofcooperationinherentinthesetup.
Determining the level of cooperation or competition feasible within the setting is a fundamental decision and
closelyrelatedtothechoiceofthelanguagegame.Wederivedthreeoptions-thecooperative,semi-cooperative,
and competitive type. In the literature reviewed, the majority of studies adopted a fully cooperative setting
approach, where agents fully share their rewards and lack individual components. The emphasis on strongly
cooperative settings is justified given that AI agents utilize a common language to coordinate and will not
learn to communicate if they dominate without communication [34]. Only a few publications explore semi-
cooperative settings that incorporate individual rewards alongside shared rewards, introducing the challenge
of balancing tasks and rewards [29, 93]. A semi-cooperative setup can be compared to a simplified social
scenario with overarching societal objectives, while also encompassing additional individual interests and goals.
In contrast, investigations of fully competitive settings are rare, with only one work in which agents compete
for rewards without a common goal [34]. This scarcity likely arises from the fact that such settings inherently
favor deceptive language as the only advantageous strategy, making its emergence improbable without any
cooperative element. [34].
The third important factor in communications settings is symmetry. Agents should treat messages similarly
to regular observations; otherwise, they risk devolving into mere directives [132]. Building on this premise, the
symmetry is important for promoting robust language emergence, as opposed to languages that consist pri-
marily of directives. An illustrative example of asymmetric settings is the commonly used, and aforementioned,
speaker-listener paradigm [14, 50, 154]. Languages developed in such settings are severely limited compared
to NL, lacking the capacity for diverse discourse or even basic information exchange beyond directives [132].
Contrary to promoting informed choices by the listener, the speaker-listener approach emphasizes obedience
to commands. Conversely, a symmetric setting facilitates bi-directional communication, thereby allowing for
11more comprehensive language development [95, 188]. For instance, symmetry may result from agents being
randomly assigned roles within the interaction [188]. Additionally, symmetry can emerge from tasks that are
inherently balanced, such as negotiations between equal partners where both parties have equivalent roles and
objectives [17].
At the population level, another important consideration is the choice of recipients, i.e., between targeted
and broadcast communication. While broadcast communication facilitates broader information dissemination
across the agent group, targeted communication promotes the development of social group dynamics and reg-
ularization [208, 212]. For example, targeted communication strategies can be learned through mechanisms
such as attention [208], and agents can develop minimized communication strategies that optimize group
performance [215].
Table3providesasummaryofthesesettingsandtheirvariations.Thesettingcategoriespresentedandtheir
implementation are not inherently tied to the language itself but are crucial in determining the likelihood of
meaningfullanguage emergenceandin shapingthefeatures andexperimentalpossibilities.Theseinitial choices
dictatetheoptionsforthelanguagedevelopmentprocess,theopportunitiesforregularization[21,116],andthe
requirements regarding computational resources.
5.2 Language Games
Distinct communication settings are implemented through different communication games. In this section, we
provide an overview of the games used in EL literature. Specifically, we focus on a subset of these games known
as language games, that emphasize explicit communication via a predefined language channel. The literature
identifiesseveralcategoriesoflanguagegames,suchasreferentialgames,reconstructiongames,question-answer
games,grid-worldgames,amongothers.Ourreviewindicatesthatthesecategoriesrepresentthemostcommonly
used game types. To give a comprehensive view, Table 4 lists the publications that focus on these game types.
In the following, we offer a concise overview of each category to provide a clearer understanding of their
characteristics.
Referential Game:Generally,areferentialgame,alsocalledsignalinggame,consistsoftwoagents,asenderand
a receiver [14]. The objective of this game is for the receiver to correctly identify a particular sample from a
set,whichmayincludedistractors,solelybasedonthemessagereceivedfromthesender.Thissetcanconsistof
images [14, 16, 135], object feature vectors [126], texts [126], or even graphs [172]. To accomplish this selection
task, the sender must first encode a message that contains information about the correct sample. In game
design, a fundamental decision arises regarding whether the sender should only view the correct sample or also
some distractors that may differ from those presented to the receiver [14]. Another design decision concerns
the receiver’s side, specifically the number of distractors and whether to provide the original sample shown to
the sender or only a similar one for selection [146]. However, only the encoded message is transmitted to the
receiver, who then selects an item from their given collection.
Reconstruction Game: The reconstruction game is similar to the referential game, but with a key difference:
the receiver does not have a collection to choose from. Instead, the receiver must construct a sample based
on the message from the sender, aiming to replicate the original sample shown to the sender as closely as
possible [103, 107]. Consequently, this game setup resembles an autoencoder-based approach, with a latent
space tailored to mimic or facilitate language [148]. Therefore, the key distinction between reconstruction and
referential games, often used interchangeably in early literature, lies in the collection’s presence (referential) or
absence (reconstruction) for the receiver to select from [141].
Question-AnswerGame:Thequestion-answergameisavariantofthereferentialgame,butwithoutstrictadher-
ence to previously established rules. It operates as a multi-round referential game, allowing for iterative and
bilateral communication [188]. Unlike referential and reconstruction games, the question-answer game explic-
itly incorporates provisions for multiple rounds with follow-up or clarifying queries from the receiver [19, 21].
Question-answer games have introduced intriguing inquiries and avenues for exploring the symmetry of EL,
although they are not as widely adopted [19].
Grid World Game:Gridworldgamesuseasimplified2Denvironmenttomodelvariousscenarioslikewarehouse
path planning [22], movement of objects [191], traffic junctions [212, 217], or mazes [20, 100]. They offer design
flexibility,allowingagentstobepartoftheenvironmentoractasexternalsupervisors.Designchoicesalsoinclude
environment complexity and the extent of agents’ observations. Although common in the literature surveyed,
implementationsofgridworldgamesvarywidelyintheirdesignchoicesandarethusaveryheterogeneousgroup.
Continuous World Game: Continuous environments add complexity to the learning process [215, 222]. In EL
approaches, the learning landscape involves multi-task settings where one task is tackled directly within the
environment while another involves language formation. Playing continuous world games, whether in two or
12Table 4 Overviewofthedistributionofgametypesinthereviewedliterature.
Type Paper
Referential 14, 15, 16, 25, 26, 31, 32, 33, 34, 37, 45, 48, 49, 50, 52, 56, 57, 65, 87, 89, 91, 92, 95,
96, 98, 99, 101, 102, 105, 108, 112, 116, 123, 124, 125, 126, 127, 130, 131, 133, 134, 135,
136, 137, 138, 140, 141, 142, 143, 144, 146, 147, 148, 151, 153, 154, 156, 157, 159, 160,
161, 163, 164, 166, 167, 168, 170, 172, 173, 174, 175, 176, 177, 179, 181, 185, 192, 197,
198, 200, 201, 206, 207, 210, 211, 216
Reconstruction 48,49,56,57,94,98,103,104,106,107,108,120,121,123,128,129,139,141,143,148,
149, 150, 152, 165, 169, 183, 186, 189, 196, 204, 205, 214
Question-answer 19, 21, 45, 57, 90, 93, 113, 155, 180, 188
Grid World 18, 20, 22, 27, 28, 30, 33, 54, 57, 88, 97, 100, 117, 118, 119, 122, 123, 132, 134, 140, 145,
147, 162, 176, 178, 184, 191, 195, 199, 208, 212, 213, 215, 216, 217, 218, 219, 220, 223,
225, 226, 227, 228, 229, 230
Continuous World 57, 158, 193, 209, 213, 215, 220, 222, 224, 225
Other 17, 20, 23, 28, 29, 30, 54, 126, 144, 182, 190, 194, 203, 220, 221, 224, 228
three dimensions, presents challenges and adds a greater sense of realism and intricacy. These environments
have the potential to make it more feasible to deploy EL agents in real-world scenarios compared to discrete
environments [193].
Other: The literature on EL also covers various other game types besides those mentioned earlier, such as
matrix communication games [29, 30], social deduction games [23, 221], or lever games [20, 220]. These game
types contribute to the creation of new language emergence settings, often designed to target specific aspects
or characteristics of language development. They are valuable tools to explore and understand the complexities
of EL in different contexts.
In summary, although many language games have been developed, comparing different games can be complex
and understanding the nuances of each game can prove challenging. A promising direction would be for the
research community to collectively agree on a standardized subset of these games as benchmarks. By focusing
on a representative set of games from different categories, researchers could systematically explore different
settings, ensuring that new approaches are rigorously tested and their results are directly comparable across
studies. This would accelerate the maturation of the field of EL research, foster collaboration, and enable the
community to better identify and address key challenges.
5.3 Language Prior
EL research occasionally utilizes a concept known as a language prior to incorporate structures from human
NLsintotheemerginglanguage.Alanguagepriorisusedtoimposespecificlinguisticstructuresontheemerging
language, making it easier to align with human NL and improve interpretability and performance. This prior
can be implemented through supervised learning [14, 19, 98], also known as injection, or through divergence
estimation [15]. An overview of prior usage in the literature surveyed is given in Table A2 in Appendix A.
Giventhiscontext,researchonELcanbedividedintotwomainareas.Thefirstareafocusesonindependent
situated learning and does not use priors, so that communication and language emerge spontaneously [18].
The second area explores imitation learning-based approaches, which aim to replicate NL behavior in artificial
agents using priors [88]. However, it is important to note that these approaches differ from LLMs because
language acquisition in EL is generally task-oriented. In academic literature, the independent situated learning
environment is often referred to as the evolution-based approach, while the imitation learning-related approach
is commonly known as the acquisition-based approach. The term evolution implies starting from scratch, while
acquisition involves learning an existing language [154]. The terminology and different approaches are depicted
in Figure 6.
In addition, the concepts of community and generational learning are closely related [21, 101, 116]. In these
methods, language emerges through iterative learning across and within agent sub-groups called communities.
Generational learning additionally involves older generations of agents training younger ones using previously
developed communication as a foundation [90, 102]. Language transfer across groups or generations can be
interpreted as an iterative prior. However, this method remains a fully evolutionary approach in the absence of
a deliberately designed prior.
13Fig. 6 Language evolution and the different language prior options for acquisition approaches. The evolution of language and the
guidedacquisitionarecontraryapproaches.Languageevolutionisbasedonasituatedlearningenvironmentandtheindependentdesign
ofanappropriatelanguagescheme.Acquisition,ontheotherhand,isbasedontheexistenceofapriorandthecombinationofsituated
andimitationlearning.Theprior,generallyaNL,iseitherintroducedintothelearningprocessviainjectionordivergenceestimation.
Injection is a direct supervision approach that uses examples of prior usage to inform and train the language learner. Divergence
estimation isanindirectsupervisionmethodwhichutilizesadistributionrepresentationofthepriorandlearnerlanguage.Thegoalis
tolimitthedivergenceofthesedistributions.
5.4 Language Characteristics
As discussed in Section 2.2, language is a complex, multifaceted system [77, 79]. Therefore, it is essential to
establish a comprehensive taxonomy of its properties to provide a unified framework for EL research. This
taxonomywillnotonlyfacilitatetheunambiguouscategorizationofmetricsusedinELstudies(cf.Section6)but
will also enhance the comparability and comprehensibility of approaches and results within the field. As shown
previously in Figure 2.2, NL can be divided hierarchically into distinct characteristics [84–86]. The following
sections provide a categorization of the reviewed publications along these characteristics, occasionally breaking
them down into smaller sub-characteristics if relevant.
5.4.1 Phonetics
The phonetics of a language inherently represents its medium, delineating the constraints of the specific com-
munication channel [3, 86]. These media or channels can be either discrete or continuous; for example, an
audio channel is continuous, while a symbolic channel is typically discrete. Regardless of the type, they lay
the foundation for the nature of communication. However, for EL research the discrete case is of particular
importance, as it closely mirrors NL as we understand it [35]. Although humans use a continuous phonetic
mediumforcommunication,somedegreeofdiscretizationisessentialtoestablishacommongroundforefficient
communication [85].
Table 5 provides an overview of the reviewed papers, categorized according to the continuous or discrete
approach. Notably, some papers explore both approaches, providing valuable insights for researchers interested
in the basic aspects of phonetics research in EL.
5.4.2 Phonology
Phonology encompasses the actively used vocabulary and determines the part of the medium that is utilized
for communication. We identified five different types of vocabulary actively researched, however, some of them
areraretofindintheliterature.Table6summarizestheresultsofoursurveyregardingvocabularytypesinEL
research.OnecommonlyusedphonologicaltypeinELisabinaryencoding,whileanevenmoreprominenttype
14Table 5 Overviewoftheuseofchanneltypesinthereviewedliterature.
Type Paper
Discrete 14, 15, 16, 17, 18, 19, 21, 23, 25, 26, 29, 30, 31, 32, 33, 37, 45, 48, 50, 52, 54, 65, 87, 88, 90,
91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 112, 113, 115, 116,
117,118,120,121,123,124,125,127,128,129,130,131,132,133,134,135,136,137,138,140,
141,142,143,144,145,146,147,148,149,150,151,152,153,154,156,157,158,159,161,162,
163,164,166,167,168,169,170,171,172,173,174,176,177,178,179,180,181,183,185,186,
188,189,190,191,192,195,196,198,199,200,201,203,204,206,210,211,215,216,217,218,
221, 226, 227, 229
Continuous 20,22,28,56,89,122,155,160,165,193,197,202,205,208,209,212,213,214,222,223,224,225
Both 27, 34, 35, 57, 126, 139, 175, 182, 184, 207, 219, 220
Table 6 Overviewoftheuseofvocabularytypesinthereviewedliterature.
Vocabulary Paper
Binary 14, 16, 23, 27, 30, 31, 48, 54, 57, 94, 95, 97, 104, 116, 122, 123, 124, 125, 126, 130, 132, 137,
141, 149, 174, 176, 185, 189, 192, 195, 196, 201, 204, 215, 216, 217, 218, 219, 220, 221, 229
Token 15, 17, 18, 19, 20, 22, 23, 25, 28, 29, 32, 33, 34, 45, 48, 50, 52, 57, 65, 87, 90, 91, 92, 93, 96, 98,
99, 100, 101, 102, 103, 105, 106, 107, 108, 112, 113, 115, 117, 118, 120, 121, 126, 127, 128, 129,
131,133,134,135,136,139,140,141,142,143,144,146,147,148,150,151,152,153,154,156,
157,158,159,161,162,163,164,165,166,167,168,169,170,171,172,173,175,177,178,179,
181,182,183,184,188,190,191,193,197,198,199,200,202,203,205,206,207,208,209,210,
211, 212, 213, 219, 220, 222, 223, 224, 225, 226, 227
NL 15, 19, 21, 26, 37, 57, 88, 138, 145, 180
Sound 57, 139, 186
Picture 56, 57, 89, 155, 160
is a token-based vocabulary. However, these two phonological classes are not always distinct, as a token-based
vocabulary often builds upon a binary encoded representation [97].
Theotherthreetypes,whicharedistinctfromthetwomostprominent,arerarelymentionedintheliterature
reviewed. One of these types involves using NL vocabulary, such as all the words from an English dictionary.
While this approach enforces the NL resemblance of the EL, it also drastically limits the emergence and asso-
ciated benefits [19]. Essentially, this phonological preset strips the agents of the possibility to shape phonology
and morphology. The other two vocabulary types being referred to are sound and graphics. The former enables
agents to produce and process sound [186], while the latter focuses on enabling agents to draw and analyze
graphical representations [89, 160]. Both mediums present challenges in ensuring discretization, which may be
the reason why they are not as extensively researched in EL.
5.4.3 Morphology
Morphology governs the rules for constructing words and sentences, meaning the overall ability to combine
individual elements, also called tokens, into words and to combine those words into sentences [86]. This is
particularly relevant in the field of EL due to the prominent division of existing work based on morphological
setup and options. The most significant differentiation is between the use of a fixed or flexible message length.
Table 7 demonstrates that much of the existing work employs fixed message lengths, despite this setup not
being comparable to NL [15]. For instance, NL users, such as humans, have the ability to adjust the length
of their message to fit their intention, which may vary depending on the audience, medium, or communicative
goal. When communicating with colleagues, they may use shorter sentences to be efficient, while more detailed
explanations may be used when conversing with friends.
Accordingly, this characteristic can be measured using metrics that assess word formation and vocabulary.
Based on the metrics found in the literature, distinct features of language morphology can be quantified.
Specifically, this refers to the compression of language and the presence of redundancy or ambiguity.
Compression
Compression [90], also known as combinatoriality [210], refers to the ability of a communication system to
combineasmallnumberofbasicelementstocreateavastrangeofwordsthatcancarrymeaning.Thisfeatureof
15Table 7 Overviewofthecharacteristicsofmessagelengthintheliteraturereviewed.
Length Paper
Fixed 14,16,17,20,22,23,25,27,28,29,30,31,32,34,54,87,89,90,91,93,94,95,96,97,98,100,102,
103,104,105,106,107,113,116,120,121,122,123,125,129,130,131,132,136,137,140,141,142,
143,144,145,150,151,152,153,155,157,158,160,162,165,166,168,170,171,172,173,174,175,
176,178,180,181,182,184,185,186,188,189,190,191,193,195,196,198,199,200,201,202,204,
205,206,207,208,209,211,212,213,216,217,218,219,220,221,222,223,224,225,226,227,229
Variable 15, 18, 19, 21, 26, 33, 37, 45, 50, 52, 65, 88, 92, 99, 101, 108, 112, 115, 117, 118, 124, 127, 128,
133, 134, 135, 138, 139, 146, 147, 148, 149, 154, 156, 159, 161, 163, 164, 167, 169, 177, 179, 183,
197, 203, 215
Both 126, 192
discretecommunicationiscrucialinproducingcomprehensiveandflexiblecommunicationwithlimitedresources,
and is an essential characteristic of NL. We assume that using compressed language is generally favorable for
language learners as it reduces the burden of learning [90].
Redundancy or Ambiguity
In NL, words and phrases can have redundant or ambiguous meanings. Redundancy occurs when multiple
wordsconveythesamemeaning, whileambiguityarises fromalimitedvocabulary[14,99].The additionofthis
characteristicinthemorphologysubsectionratherthanthesemanticssubsectionmaybecontroversial.Weargue
that any metric measuring redundancy or ambiguity provides more useful information about the morphology,
encompassing the form and size of the vocabulary, than it does about the semantic range and capabilities of
the language. However, to quantify redundancy or ambiguity, we must establish semantic meaning first.
5.4.4 Syntax
Thesyntaxofalanguageestablishesthegrammaticalrulesthatgovernsentenceformation.Consequently,syntax
plays a central role in establishing a functional correspondence between emerged language and NL [231]. This
specific characteristic of a discrete language is underrepresented in current EL literature. However, we found
twoexamplesinthebodyofliteraturediscussingsyntaxinEL.Uedaetal.[87]introducedamethodtoexamine
the syntactic structure of an EL using categorial grammar induction (CGI), which is based on the induction of
categorial grammars from sentence-meaning pairs. This method is straightforward in simple referential games.
Additionally, van der Wal et al. [179] introduced unsupervised grammar induction (UGI) techniques for syntax
analysis in EL research. We discuss the methods they use to measure and analyze syntax in an EL briefly in
Section 6.3.
5.4.5 Semantics
Semantics is concerned with the literal meaning of language constructs and is a dominant topic in current EL
research, as shown in Table A3 in Appendix A. EL studies often focus on establishing useful and meaningful
communication between agents, making semantics a central feature [35]. It serves as a crucial tool for distin-
guishing actual information exchange from mere noise utterances [154, 188]. Given the complexity of capturing
the meaning of literal language in a single metric, several features have been introduced to measure the seman-
tics of EL. In particular, these features include grounding, compositionality, consistency, and generalization, as
shown in Figure 7. Table 8 provides an overview of the literature addressing the individual semantic features in
EL.
Grounding
A language is considered grounded when it is deeply intertwined with the environment, for example, when it is
tightly bound to environmental concepts and objects [135, 170, 232]. Grounding is essential for the interoper-
abilityofindividualsandisparticularlyimportantinNLcommunication,wheremeaningfulinteractionrequires
shared understanding [33, 36, 97]. While in theory, an EL can establish a unique form of grounding using self-
emergedconceptsdistinctfromthoseinNL,derivingausefulmetricforsuchascenarioproveschallenging.This
difficulty arises from the need to compare ELs to existing and comprehensible grounding principles typically
found in NLs [49, 97, 113].
16Table 8 Semanticfeaturesdiscussedinthereviewedliterature.
Feature Paper
Grounding 14, 15, 16, 17, 18, 19, 21, 26, 45, 57, 89, 91, 92, 99, 113, 130, 132, 134, 137, 138, 139, 151,
155, 157, 160, 161, 171, 178, 184, 193, 202, 217, 220
Compositionality 15, 18, 21, 25, 35, 45, 49, 50, 57, 65, 87, 90, 91, 92, 99, 101, 102, 103, 104, 106, 107, 108,
113, 120, 121, 124, 127, 133, 135, 139, 140, 141, 142, 143, 145, 146, 150, 152, 153, 154,
163, 165, 166, 168, 169, 170, 172, 173, 177, 181, 183, 184, 199, 210
Consistency 18, 21, 28, 29, 32, 33, 50, 57, 65, 89, 93, 95, 99, 101, 103, 108, 120, 122, 136, 137, 145,
146, 150, 153, 157, 159, 162, 163, 171, 178, 181, 188, 211, 216, 217, 226
Generalization 14, 25, 32, 50, 57, 65, 88, 89, 92, 99, 102, 103, 105, 107, 108, 113, 121, 125, 133, 135, 139,
140,141,143,150,153,154,162,163,168,169,171,172,175,180,181,183,197,209,214
Fig. 7 SemanticsfeaturesoflanguageaddressedinELresearch:grounding,compositionality,consistency,andgeneralization.
Compositionality
When a language exhibits compositionality, its components can be rearranged or replaced by conceptually
equivalent words without changing the overall meaning [106, 107]. Compositionality facilitates the construction
of higher-level concepts, using conceptual foundations to enable efficient language expression [45, 50, 121]. For
example, NLs partition concepts such as objects and their attributes to allow compositional constructions [18,
141]. As a result, we can describe variations of a single object using different words from the same semantic
concept, such as ‘blue towel and ‘red towel for the object towel and the semantic concept of color. Similarly,
we can attribute specific properties to different objects using the same phrase, as in ‘green towel and ‘green
car. Ultimately, compositionality is beneficial for the learning process [36, 107] and promotes efficient and rich
language use, even in systems with limited memory capacity [18, 93, 104].
Consistency
Merely having grounded words in a language does not necessarily guarantee its semantic quality. In addition,
consistency is essential for a language to convey meaningful and practical information effectively [36, 233]. If
the words within a language lack consistency in their literal meanings, they will not facilitate effective commu-
nication.Therefore,evenifalanguageissemanticallygroundedandcompositional,itsutilityiscompromisedif
the words exhibit inconsistent literal meanings [113]. While words can change their general meaning to fit the
context, their literal meaning should remain consistent to keep their usefulness [33].
Generalization
Generalization serves as a cornerstone of NL, allowing humans to communicate about topics ranging from
simple to complex, broad to specific, and known to unknown, all with a relatively limited vocabulary [25, 121].
A language that excels at generalization enables its users to navigate different levels of complexity, facilitating
hierarchical descriptions of concepts and relationships [108]. Consequently, generalization and compositionality
are closely related, as they both contribute to the flexibility and expressiveness of language [36, 102, 154]. This
ability to generalize not only enriches communication but also underscores the adaptability and robustness of
human language.
5.4.6 Pragmatics
The final dimension of EL research is pragmatics. This field of study encompasses metrics that examine how
language is employed in context, particularly in interactions, and how it conveys information [86, 146, 234]. By
17Table 9 Pragmaticfeaturesdiscussedinthereviewedliterature.
Feature Paper
Predictability 95
Efficiency 100
Positive signaling 17, 29, 34, 35, 57, 93, 94, 95, 110, 132, 167, 172, 173, 186, 188, 193, 216, 220, 226
Positive listening 29, 34, 35, 57, 93, 94, 95, 97, 110, 132, 162, 172, 173, 186, 188, 193, 216, 220, 226
Symmetry 90, 95
Fig. 8 PragmaticsfeaturesoflanguageinELresearch.Theseillustrationsareintendedtopromoteanintuitiveunderstandingofthe
categoriespredictability,efficiency,positive signaling,positive listening,andsymmetry.
examining the pragmatics of the linguistic structure, we can ascertain whether EL is itself useful and utilized
effectively. While this assessment may be feasible based on rewards in a standard RL setting, integrating
communicationintosuchenvironmentsincreasesthecomplexity.Thisisbecausemostsetupsdonotseparatethe
agent’senvironmentinteractionfromitscommunicationcapabilities,therebyexpandingthenetwork’scapacity,
and making it difficult to attribute an increase in reward directly to EL [35].
As outlined in Table 9 and depicted in Figure 8, five distinct features have been identified for which metrics
havebeenproposed:predictability,efficiency,positivesignaling,positivelistening,andsymmetry.Thesefeatures
are essential for assessing the constructive impact and utilization of EL. Understanding how agents employ
language is crucial in evaluating its effectiveness and overall benefit.
Predictability
Predictability is concerned with the assessment of the complexity of the context, including the action space
within the environment. When actions exhibit less diversity, it becomes more feasible to coordinate without
communication[95].Forinstance,inasimplegrid-basedenvironmentwhereagentshaveonlytwopossibleactions
—movingleftorright—agentscanoftenachievetheirobjectiveswithouttheneedforcommunication.Insuch
a scenario, the limited action space reduces the necessity for EL, as agents can predict each other’s movements
based on past behavior or simple rules. However, in a more complex environment where agents have multiple
actions, such as navigating a maze with numerous paths and obstacles, the need for effective communication
increases. Here, EL can significantly enhance coordination by allowing agents to share information about their
positions, plans, or discoveries, thus improving their overall performance in navigating the maze. Therefore, it
is essential to compare the diversity of signaling and context attributes to evaluate the potential benefit of EL.
Efficiency
Efficiency is a critical aspect considered whenever communication entails a cost. This is particularly true in
the context of modeling the emergence of NL and the broader objective of employing EL for human-computer
interaction (HCI). In EL settings, the achievement of concise communication is contingent upon the presence
18of an opportunity cost [100]. Without such a cost, there is no incentive to communicate concisely, making
EL ineffective as an intermediary for HCI. When communication is accompanied by a cost the necessity for
efficiency in communication becomes paramount. In such scenarios, the objective is to minimize the cost while
maximizing the effectiveness of communication within a given task.
Positive Signaling
The concept of positive signaling is concerned with the degree of alignment between the observations of the
messageproducerandtheircommunicationoutput[29].Theobjectiveistoguaranteethetransmissionofuseful
information, or at the very least, information that the speaker can discern through observation [167]. This
assessment operates on the premise that all communication should be relevant to something observable by the
speaker.
Positive Listening
Positive listening focuses on the role of the message receiver, seeking to quantify the usefulness and application
of incoming information [29]. It seeks to quantify the impact and correspondence between the received message
and subsequent actions taken. This evaluation operates under the assumption that the agent engages with the
message only when it significantly influences the decision-making process, for example, in the form of chosen
actions [29, 188].
Symmetry
SymmetryinELisdefinedastheconsistencyinlanguageusageamongparticipatingagents[90,95].Thisconcept
applies to MARL settings where agents can assume multiple roles, such as message producer and message
receiver.SymmetryplaysacrucialroleinachievingconvergenceonasharedandalignedEL.Forinstance,ifan
agentemployslanguagedifferentlydependingonwhetheritissendingorreceivingmessagessothatwordshave
varying meanings based on the assigned role the EL setting is considered asymmetric. In such instances, rather
than learning a collectively grounded language, agents develop individual protocols specific to their respective
roles [95].
5.5 Summary of the Taxonomy
Our proposed taxonomy systematically categorizes the key features of EL systems, including communication
settings, language games, language priors, and language characteristics. The latter is particularly detailed,
with sub-characteristics and their features aligned with the major levels of linguistic structure, as previously
illustrated in Figure 2.2. This comprehensive taxonomy enables a standardized comparison of approaches in
the EL literature, highlighting the opportunities and properties associated with individual options and topics
in EL research. Specifically, by applying this taxonomy, especially in terms of language characteristics, we can
uncover the capabilities and potentials of various EL approaches. This facilitates a more detailed, comparable,
and insightful analysis of EL.
6 Metrics
This section provides a comprehensive categorization and review of existing metrics used in EL research. The
section is organized along the same categorization used in Section 5.4. Note that the categories of phonetics
and phonology are excluded from this discussion, as these aspects are predetermined settings in the current EL
literature and thus not yet targeted by metrics.
We begin by introducing the notational system used for all metrics to ensure consistency and facilitate ease
ofuse.Wethendescribethemetricswithineachcategory,detailingtheindividualmetricandadaptingittoour
notation. For each metric, we provide references to both original sources and additional literature, if available,
to enable further exploration beyond the scope of this work. Figure 9 provides a visual summary of the existing
metrics and their correspondence to the language characteristics. An extended version including all references
for the individual metrics is provided in Figure B2 in Appendix B
6.1 Notation
GiventhecomplexityandvariabilitywithintheELfield,itiscrucialtoestablishaunifiedandcoherentnotation
system. In this section we present a standardized mathematical notation designed to be consistent across the
various aspects of EL research, thereby facilitating clearer communication and comparison of results within the
community.Thisapproachalignswithourbroadergoalofadvancingthefieldthroughacommontaxonomythat
19SVD
Perplexity
Message
A Wc ot ri dve
s
Dis nt ei sn sct-
Average
Message
Length Redundancy
or
Ambiguity Syntax
Tree
Compression
Distinct CGI
Appear-
ances
Purity
Morphology Syntax Represen-
Causal Divergence tational
Influence Similarity
Message of Analysis
Effect Commu-
nication
Instan- Topo-
taneous graphic
Coordi- Grounding Simi-
nation larity Positional
Disen-
tangle-
Positive ment
Listening Emergent
S C tp o ee n na s ck i yse -r Language SB Dyma isg b eno o -f ls
SP igo nsi at li iv ne
g
Metrics Semantics C tio om nap lo its yi- ta mn eg nl te-
Pragmatics Tree
Recon-
struct
Error
General-
ization
Efficiency Mutual Conflict
Infor- Count
Easeand mation
Transfer
Learning
Sparsity Consistency
Pre- Symmetry
dictability Zero
Shot
Evalu- Correlation
ation
Inter- Similarity
Agent
Diver- Coherence
Behavioral W Ai gt eh nin t- gence Entropy
Divergence Diver-
gence
Fig.9 Graphpresentingavisualrepresentationofthemetricsidentifiedinthesurveyedliterature,sortedbylanguagecharacteristics.
Eachnodecontainsalinktothecorrespondingsectionthatdescribesthemetricindetail,allowingforconvenientnavigation.
:Languagecharacteristics(innernodes)
:Individualmetrics(outermostnodes)
supports the development of measurable and interpretable ELs. Throughout this section we focus on finite and
discretelanguages,althoughsomeofthedefinitionsandmetricsdiscussedherearealsoapplicabletocontinuous
languages. These languages offer a more straightforward mapping to NLs, making them particularly relevant to
the study of EL systems.
6.1.1 Definition
In alignment with the semiotic cycle introduced in Section 2.2, our notation is organized into three intercon-
nected spaces: setting, meaning, and language. The setting space encompasses the typical elements of RL,
providing the foundational environment in which agents operate. The meaning space incorporates a represen-
tation learning endeavor, whereby sensory input is integrated with decision-relevant information to generate a
coherent internal representation. Finally, the language space encompasses both the production and comprehen-
sionofdiscretemessages,encapsulatingthecommunicationprocess.Thesecomponents,illustratedinFigure10,
will be introduced and explored in detail in the following paragraphs.
Setting
The overall setting, consisting of the environment, actions, goals, and other typical RL elements, is denoted by
Ω. Let ξ denote the set of all entities in the system, with an individual entity represented as ξ ∈ξ. Each entity
i
20Fig.10 Theproposednotationisintegratedwithinthesemioticcycleframework(cf.Figure3).ThelanguageL,alongwithitsassoci-
atedmappingfunctionsLprod(production)andLcomp(comprehension),formsthecoreofELresearch.Theselinguisticcomponents,
however,areunderpinnedbytherepresentationslearnedinthemeaningspaceΦ,whichplayacrucialroleinguidingandshapingeffec-
tive communication. All elements in the sender or receiver domain are entity-specific, but we only index duplicate symbols to keep it
concise.Adaptedfrom[44,83].
can assume specific roles, such as the sender (S) or receiver (R) in a communication scenario. An entity can
assume several roles over the course of the entire communication scenario. However, for an individual message
exchange, an entity assumes one specific role. We represent the role of an individual entity i by ξ ∈ξ , where
i,j i
j specifies the role (e.g., j =S or j =R).
EntitiesinteractwiththeirenvironmentE throughactions,denotedasa,whichbelongtothesetofpossible
actions A, such that a ∈ A. The action taken by a specific entity ξ is represented as a . The state of the
i ξi
environment at any given time is denoted by s, which is an element of the state space S, so that s ∈ S.
As the system progresses over time, denoted by discrete points in time [0,...,t], the sequence of states and
actions forms a trajectory τ, generally expressed as τ = (cid:8) s0,a0,...,st,at(cid:9). It is important to note that the
entities described here do not necessarily correspond to autonomous agents in the traditional sense; they could
also represent ground truth models, human participants, or abstract constructs that lack the direct interaction
capabilities typically associated with agents. Despite this distinction, for the sake of clarity and consistency, we
will refer to these entities as agents in the following sections.
GiventheimportanceofpartialobservabilityinELresearch[18,49,158,214],itisessentialtoconsiderthat
agents only have access to their own observations, denoted as o , which are derived from the underlying state
ξ
s. An individual observation o is an element of the collection of observations of an agent O , which is a subset
ξ ξ
of the observation space O, so that o ∈ O ⊆ O. In our framework, an observation o effectively replaces the
ξ ξ ξ
‘world model’ component from the traditional semiotic cycle, highlighting the localized and subjective nature
of an agent’s perception in partially observable environments.
Referential games (cf. Table 4) are frequently employed in EL literature. They often operate on individual,
static samples that are drawn from a corresponding dataset or distribution. In doing so, they differ from
traditional RL setups that emphasize sequential decision-making and environmental interactions over time. In
suchcases,ratherthanspeakingofastatesoranobservationo,weusethetermsamplek,whichisanelement
of the collection of all samples K, so that k ∈K. The specific nature of a sample depends on the environment;
forexample,inanimage-basedsender-receivergame,thesamplewouldbeanimage.Eachsampleisrepresented
by its feature vector f, which belongs to the feature space F, so that f ∈F. The feature vector corresponding
to a specific sample k is denoted by f .
k
21In EL settings, the communicative goal g of an agent may differ from the (reinforcement) learning task
goal.Inaddition,dependingonthegame,thesenderandreceivermayhavedistinctgoals.Theseareimportant
factors to consider when evaluating the communicative behavior.
Meaning
In our notation, the meaning space, denoted by Φ, serves as the critical intermediary between the setting space
and the language space. The meaning space represents the semantic connections derived from the provided
information. Each element within this space, represented by a specific meaning vector φ ∈ Φ, captures the
essence of concepts or objects as understood by the agent. These meaning vectors are critical to the processes
of language comprehension and production, as well as to the processes of conceptualization and interpretation,
that allow an agent to effectively use inputs and generate outputs in the setting space (cf. Figure 10).
The representation mappings Ψ within the meaning space are agent-specific and referred to as Ψ and
con
Ψ , given in Equation 1. These mappings enable the transition between an arbitrary space χ, such as sensory
int
inputs or raw data, and the meaning space, where the data acquires semantic meaning. Ψ refers to the
con
conceptualization process that transforms raw, uninterpreted data into meaningful representations within Φ.
Conversely,Ψ denotestheinterpretationprocessthattranslatesthesemeaningvectorsbackintothearbitrary
int
space that can represent any external or internal stimuli. These mappings are critical to the agent’s ability to
bothunderstanditsenvironmentandcommunicateeffectivelywithinitthroughlanguagethatisbothgrounded
in and reflective of the underlying reality with which the agents interact.
(cid:40)
Ψ :χ→Φ
Ψ= con (1)
Ψ :Φ→χ
int
Language
In our proposed framework, a message m belongs to the message space M, such that m ∈ M. Each message
encapsulatessemanticandpragmaticcontent,servingasavehicleformeaningfulcommunicationbetweenagents.
A message is composed of individual words w, which are elements of a finite collection W, commonly referred
to as vocabulary, lexicon, or dictionary. In this context, each word is considered a semantic unit that carries
(intrinsic) meaning. At the lowest level, a word is composed of characters or symbols υ ∈ Υ. These atomic
characters, while essential for constructing words, do not independently carry semantic meaning. Instead, they
function as elements of a finite set Υ from which any number of meaningful words can be composed.
Building on the formalization from [104], we describe the message space M of an agent ξ, which represents
ξ
the agent’s language capabilities from a compositional standpoint. The message space M ⊆M is composed of
ξ
a set of messages or strings m , each constructed from words within W , as shown in Equation 2. Further, each
ξ ξ
w ∈m is composed of a set of characters υ ∈Υ ⊆Υ utilized by the agent, given by Equation 3.
ξ ξ ξ ξ
m ⊆M
ξ ξ (2)
={w |w ∈W ⊆W ∧|w |≥0}
ξ ξ ξ ξ
w ⊆W
ξ ξ (3)
={υ |υ ∈Υ ⊆Υ∧|υ |≥0}
ξ ξ ξ ξ
AlanguageLencompassesasetofmappingfunctionsthatfacilitatethetransformationbetweenthemessage
space M and other arbitrary spaces χ. These mappings are agent-specific and enable both the production of
messages, denoted as L , and the comprehension of messages, denoted as L . This framework aligns with
prod comp
the linguistic level description of the semiotic cycle presented in Figure 3. Within this context, we formally
define a language L in Equation 4.
(cid:40)
L :χ→M
L= prod (4)
L :M →χ
comp
Theseemergingmappingfunctionsarenotnecessarilyinjective,meaningthatdistinctinputsfromthespace
χ could potentially be mapped to an identical message within M [102, 105]. Conversely, distinct messages
within M could also be mapped to the same value in χ. While this non-injectivity adds a layer of complexity
to the expressiveness of the language, it also introduces a degree of flexibility that can be advantageous in
certain communication scenarios. For example, it allows for synonymy (where different messages convey the
same meaning), which can provide redundancy and flexibility in communication, and homonymy (where the
same message may have multiple interpretations depending on context), which can facilitate more nuanced and
context-dependent communication. These natural phenomena, thoughchallenging,are well-documented in NLs
22andareofparticularinterestinthedesignandevaluationofartificialcommunicationsystems[86,235].However,
managing these complexities effectively is crucial, as unchecked non-injectivity could lead to ambiguities that
complicate communication rather than simplifying it.
6.1.2 Important Notes
The notation presented here is designed to be comprehensible and thorough; however, it may not be directly
applicableinallcasestoexistingworks,astheseemploydifferentwordings.alotofexistingworkusestheterm
‘word’,whichinournotationdescribeselementcarryingsemanticmeaning,and‘symbols’,whichinournotation
serveasfundamentalbuildingblockswithoutinherentsemanticmeaning,interchangeably[19,35,120,136,172].
Furthermore, a considerable proportion of existing literature utilizes a multitude of different definitions for
concepts such as ‘meaning space’ [114, 124, 167], ‘ground-truth oracle’ [120, 134], and other pivotal elements.
In our endeavor to establish a unified framework, we have occasionally adopted terminology that differs from
that used by the original authors. While this may initially lead to some confusion, we intend to mitigate this
by providing transparent and detailed descriptions. Our objective is a consistent application of these concepts
across the field of EL research, thereby promoting coherence between different studies. The following sections
attemptto alignexistingresearchandmetrics withtheproposedframework.Whilethis alignmenthas required
some linguistic adjustments to existing terminology and procedures, it is important to note that no substantive
changes have been made to the underlying methodologies.
6.2 Morphology
Morphological metrics aim to evaluate the structure and formation of words within a language, as well as the
richness and diversity of its vocabulary. The identified metrics focus on aspects such as language compression,
redundancy, and ambiguity. The morphology of a language significantly influences the complexity of language
based tasks [236]. Therefore, the evaluation of morphological features is a crucial component for understanding
and evaluating the effectiveness of ELs.
6.2.1 Compression
The concept of compression within a language refers to its ability to efficiently combine and reuse a limited
set of characters to generate a large collection of words or meanings [90, 210]. Several metrics can be used to
quantify compression in ELs. A straightforward approach for these metrics is to use statistical measures, as
shown in the following paragraphs. These metrics provide insight into the efficiency of the language, indicating
howwellitminimizesredundancywhilemaximizingexpressiveness.Efficientcompressionisakeyindicatorofa
communicationsystem,especiallyinscenarioswhereresources(suchasmemoryorbandwidth)areconstrained.
Distinct Appearances
The metric of distinct appearances (DA) was proposed by Loreto et al. [210]. It is formalized in Equation 5
and designed to quantify the capacity of a communication system to name a diverse set of objects or categories
using its available symbols [210]. Specifically, this metric evaluates how frequently characters υ ∈Υ are reused
across different words or names w within the lexicon W. By examining the set W , which includes all words
υ
containing a given character υ, we can assess the system’s flexibility in recombining basic units to generate a
broad spectrum of expressions.
A high DA value, approaching 1, indicates that the characters are highly versatile and reused extensively
across different words, thereby reflecting a flexible communication system. Conversely, a low DA value suggests
limited reuse of characters, which may imply constraints in the system’s expressiveness or a less efficient use
of its symbolic resources. This metric provides insights into how efficiently a system can balance the trade-off
between a compact character set and the richness of its vocabulary.
(cid:80)
(|W |−1)
DA= υ∈Υ υ with W ={w |υ ∈w∧w ∈W } (5)
(|W|−1)|Υ| υ
Average Message Length
Another way to assess the degree of compression achieved by agents in their communication is to analyze the
averagemessagelength[65,99,128,149,155,179].Thismetric,whichappearsforthefirsttimeinChoietal.[65],
captures the typical length of generated messages and provides insight into the efficiency of the EL in terms
of information density [99]. By tracking the average number of words in the messages, we can quantify how
23effectively the agents compress their language. This metric is computed at the word level, meaning each word
within a message is counted. The average message length |m| for a set of messages M is calculated as follows:
1 (cid:88) (cid:88)
|m|= |m| with |m|= 1 (6)
|M|
m∈M w∈m
Active Words
Theactivewordsmetric,introducedbyLazaridouetal.[14],complementstheaveragemessagelengthbyquan-
tifying the diversity of word usage within the vocabulary [99]. Specifically, this metric measures the variety and
utilizationofdistinctwordsinacommunicationsystem.Ahighnumberofactivewordsindicatesadiversevocab-
ulary,reflectingamorecomplexorredundantEL.Conversely,alowernumbersuggeststhatthecommunication
system relies on a limited set of words, which may indicate a more efficient and compressed language with less
synonyms[18].Thismetriciswidelyusedintheliterature[14,18,32,99,101,104,123,126,128,130,137,173].
Mathematically, the active word value AW for an agent ξ can be defined as the size of the collection of words
i
activelyusedbytheagentW ,asgiveninEquation7.Inmulti-agentsetups,thismetriccanbeaveragedacross
ξi
all agents to provide a collective measure of vocabulary diversity within the joint system.
AW(ξ )=|W | with W ⊂W (7)
i ξi ξi
6.2.2 Redundancy or Ambiguity
Redundancy in language occurs when multiple words are associated with the same meaning, providing alter-
native expressions for the same concept. Conversely, ambiguity occurs when a single word is associated with
multiple meanings, creating the potential for different interpretations depending on the context. Both redun-
dancy and ambiguity are characteristic features of NLs, reflecting the complexity and flexibility inherent in
human communication [99, 118].
Perplexity
Perplexity,introducedbyHavrylovandTitov[15],measureshowoftenawordwasusedinamessagetodescribe
the same object [65, 99]. “A lower perplexity shows that the same [words] are consistently used to describe the
same objects.” [99]. Mathematically, P (w|φ) represents the probability or score of a word for a specific concept
or meaning, e.g., derived from an affine transformation of the sender’s hidden state [99] or from a ground truth
label [65]. Thus, perplexity, given in Equation 8, quantifies the predictability of word usage, with lower values
reflecting a less redundant communication system. It is usually calculated based on a sampled set of meanings
Φ for which the word probability can be generated.
test
(cid:32) (cid:33)
(cid:88)
Ppl=exp − [P (w|φ)·log(P (w|φ))] ∀φ∈Φ ⊆Φ (8)
test
w∈W
Singular Value Decomposition
Another approach to quantitatively assess the redundancy of the vocabulary used in a communication system
is outlined by Lazaridou et al. [14]. This method involves constructing a matrix where the rows correspond to
distinct meanings, the columns represent individual words, and the matrix entries indicate the frequency with
which each word is used for a given meaning. The rows are thus constructed based on a predefined ground
truth classification. By applying Singular Value Decomposition (SVD) to this matrix, we can examine the
dimensionality of the underlying communication strategy. If the communication system relies on a limited set
of highly synonymous words, we would expect the SVD to reveal a low-dimensional structure. Conversely, a
higher-dimensional decomposition would indicate a more diverse use of vocabulary, reflecting a potentially less
synonymous and more redundant language.
Message Distinctness
Message distinctness evaluates thelinguistic representation of distinctfeatures and thusaims to quantifyambi-
guity [65, 99, 154, 211]. The metric, first suggested in Lazaridou et al. [154] and Choi et al. [65], quantifies
the diversity of messages generated by the agent by assessing how well it differentiates between various inputs.
Specifically, message distinctness MD is calculated as the ratio of the number of unique messages generated
within a batch (cf. 9) to the batch size (cf. 10). A higher message distinctness indicates less ambiguity of the
language.
24M ={m |m ∈M ∧m ̸=m ∀ m ∈M ,i̸=j} (9)
unique i i test i j j test
|M |
MD= unique (10)
|M |
test
6.3 Syntax
Despite the significance of structural properties in ELs, particularly regarding their syntax and its relation to
semantics,researchinthisarearemainslimited[231].Recurrentsyntacticalpatternsarecentraltotherobustness
and versatility of NLs [49]. Exploring these properties within the context of EL could provide valuable insights
into their development and alignment with NL.
Syntax Tree
Van der Wal et al. [179] introduced unsupervised grammar induction (UGI) techniques for syntax analysis in
EL research, describing a two-stage approach to deriving grammar and syntax. The first phase involves the
induction of unlabeled constituent tree structures, explained below, and the labeling of these structures. The
second phase extracts a probabilistic context-free grammar (PCFG) from the labeled data. Two methods were
compared for constituency structure induction: the Common Cover Link (CCL), a pre-neural statistical parser
that makes assumptions about NL such as the Zipfian distribution, and the Deep Inside-Outside Recursive
Auto-encoder(DIORA),aneuralparser.Forthelabelingprocess,VanderWaletal.[179]usedBayesianModel
Merging (BMM), to consolidate probabilistic models to label the induced syntax trees.
Insyntaxtrees,thestructureofthelanguageisrepresentedinahierarchicalmanner,wherenodesrepresent
grammatical constructs (such as sentences, phrases, and words) and edges represent the rules or relationships
thatconnecttheseconstructs.Analysisofthesetreeshelpstounderstandhowwellgrammarinductionmethods
match the true syntactic nature of ELs. There are several metrics associated with syntax trees that are used
to measure the complexity of the grammar [179]. First, tree depth measures the maximum distance from the
root of the tree to its deepest leaf. Tree depth reflects the hierarchical complexity of the grammar. Shallow
trees indicate a simpler grammar, while deeper trees suggest a more complex syntactic structure. Second, the
number of unique preterminal groups is a metric that counts the different sets of preterminals (intermediate
symbols) that appear to the right of production rules in a grammar. A larger number of unique preterminal
groups indicates a richer and more diverse syntactic organization, suggesting that the grammar can generate a
greater variety of structures.
Categorical Grammar Induction
Uedaetal.[87]proposedanovelapproachforanalyzingthesyntacticstructureofELsusingCategorialGrammar
Induction (CGI). This technique focuses on deriving categorial grammars from message-meaning pairs, making
it particularly well-suited for simple referential or signaling games.
Inthismethod,derivationtreesareconstructedusinglexicalentriesandapplicationrules,mappingmessages
to atomic syntactical representations. Given that multiple derivations might exist for a single message, “the
most likely derivation [is selected] using a log-linear model” [87]. CGI is particularly valuable for assessing the
syntactic structure of an EL using the generated trees.
6.4 Semantics
Capturing the semantic properties of ELs is inherently complex, making it difficult to encapsulate nuances in a
singlemetric.Toaddressthis,severalkeyfeatureshavebeenintroduced,includinggrounding,compositionality,
consistency, and generalization. These are important because agents can develop representations that are well
aligned with task performance but fail to capture the underlying conceptual properties [16]. Thus, an EL
might enable successful task completion without truly encoding semantic meaning. Therefore, evaluating these
semantic features is essential to evaluate the value and validity of the EL.
6.4.1 Grounding
Groundingisessentialforthedevelopmentofmeaningandforsystematicgeneralizationtonovelcombinationsof
concepts[109].Itformsthebasisofhuman-agentcommunication[33],andwithoutpropergrounding,meaningful
communicationcannotbeeffectivelylearned[97].However,ingeneraldialogsettings,groundingdoesnotemerge
naturally without specific regularization techniques [113]. The grounding problem, which concerns how words
acquire semantic meaning, is central to this challenge [170]. Thus, grounding metrics are vital as they largely
25define the usability of a language. However, a significant limitation of these metrics is their reliance on some
form of oracle or a NL-grounded precursor [15, 26, 120, 193].
Divergence
Havrylov and Titov [15] proposed a weak form of grounding. Weak grounding means that the same word
can correspond to completely different concepts in the induced EL and NL. They used the Kullback-Leibler
divergence D (cf.Equation11)ofanELandaNLdistributiontoensurethatthe statisticalproperties ofEL
KL
messagesresemblethoseofNL.Theyintroducedthisapproachasanindirectsupervisionmeasureduringtraining
butitcanalsoserveasametricforevaluatingthealignmentbetweenELandNL.Foragivensamplek andthe
messagem producedbythesender,thegroundingdivergenceG calculationisshowninEquation12.Since
ξS Div
thetrueNLdistributionP (m )isinaccessible,alanguagemodelistrainedtoapproximatethisdistribution.
NL ξS
TheKLdivergenceyieldsavalueintherange[0,∞),withlowervaluesindicatingacloserresemblancebetween
the generated messages and NL.
(cid:18) (cid:19)
(cid:88) P(x)
D (P||Q)= P(x)log (11)
KL Q(x)
x
G =D (P (m |k)∥P (m )) (12)
Div KL ξS NL ξS
Purity
Purity, proposed by Lazaridou et al. [14], is a metric used to assess the alignment between predefined semantic
categoriesandthoseobservedinanEL.Itmeasurestheeffectivenessofacommunicationsysteminconsistently
mapping signals or words to specific concepts [184]. Thus, purity quantifies the extent to which the clustering
of words reflects meaningful and coherent categories, as determined by ground-truth labels. To assess purity,
we first form clusters by grouping samples based on the most frequently activated words to describe them. The
quality of these clusters is then evaluated using the purity metric, which calculates the proportion of labels in
each cluster that match the majority category of that cluster. A higher purity score indicates that the sender is
producing words that are semantically aligned with predefined categories, as opposed to arbitrary or agnostic
symbol usage, as demonstrated in [14]. However, this metric requires the existence of predefined ground-truth
labels, limiting its applicability in scenarios where such labels are unavailable or ambiguous.
Formally, given a set of clusters {C } where each cluster of samples C has a corresponding majority
k k
ground-truth label c , the purity of a cluster C is defined as:
k k
|{w |w ∈C ∧w =c }|
purity(C )= c c k c k (13)
k |{w |w ∈C }|
k
Here, {w |w ∈C } is the collection of all words used to describe the samples in the cluster and {w |w ∈
k c c
C ∧w =c } is the collection of words within the cluster that fit the majority label of that cluster. The purity
k c k
metric ranges from 0 to 1, where a value of 1 indicates perfect alignment with the ground-truth categories.
Representational Similarity Analysis
RepresentationalSimilarityAnalysis(RSA)emergedinthefieldofneuroscienceandwasproposedbyKriegesko-
rte et al. [237]. It has since been adapted for the evaluation of the similarity of neural representations across
differentmodalities,includingcomputationalmodelsandbrainactivitypatterns.Thistechniquehasbeeneffec-
tivelyappliedinELresearch[16,99,202],wherethefocusshiftsfromanalyzingneuralactivitytoexploringthe
structural relationships between different embedding spaces. For example, RSA has been employed to compare
the similarity of embedding space structures between input, sender, and receiver in a referential game [16, 99].
By calculating pairwise cosine similarities within these spaces and then computing the Spearman correlation
between the resulting similarity vectors, we can calculate an RSA score that measures the global agreement
between these spaces, independent of their dimensionality. The agreement of an agent’s embedding space with
the input embedding space as such provides an intuitive measure of the grounding of the EL.
This approach offers the advantage of being applicable to heterogeneous agents and arbitrary input spaces.
In our framework, this corresponds to any ground truth structured embedding e(o ) of an agent’s observation
ξ
o and its internal meaning representation φ . Nevertheless, a significant limitation is the necessity for an
ξ ξ
embedding, which provides a structured description of the observation oriented towards a ground truth, for
example, based on a NL model. Furthermore, RSA is not directly applicable to the language itself, particularly
for discrete languages. Instead, it operates at the level of earlier meaning representations. Despite this, RSA
provides valuable insights into whether the EL can be grounded by evaluating the grounding of the meaning
space.
26The methodology of [16] utilizes a collection K of samples, comprising k observations, images, or feature
vectors, to compute representational similarities between input and meaning space. First, we generate input
or ground truth embeddings e =e(o ) using an appropriate model and generate the corresponding internal
GT ξ
representations φ from the appropriate architecture part of agent ξ. Next, we compute pairwise similarities
ξ
within each embedding space, denoted as S for the ground truth embeddings and S for the agent represen-
e φ
tations, typically using cosine similarity S as defined in Equation 14. This yields a similarity vector of size
cos
N ·(N −1) for each embedding space. The vectors are converted into rank vectors R(S ) and R(S ). Finally,
e φ
we calculate the Spearman rank correlation ρ [238] between the ranked similarity vectors, using the covariance
cov and standard deviation σ, to assess the alignment between the input and agent representation spaces (cf.
Equation 15). The correlation coefficient ρ takes on values between −1 and 1. A high absolute value of this
coefficient indicates a strong alignment between the two variables.
a·b
S =S (e ,φ ) ∀i,j ∈k, i̸=j with S (a,b)= (14)
e cos i i cos ||a||·||b||
cov(R(S ),R(S ))
ρ= e φ (15)
σ σ
R(Se) R(Sφ)
6.4.2 Compositionality
In EL research, achieving compositionality often requires deliberate guidance, as it does not naturally arise
without specific interventions [104]. For instance, training models on diverse tasks and varying environmental
configurationscanfacilitatethedevelopmentofcompositionalstructures.Thisoccursasatomicconcepts,learned
insimplercontexts,arerecombinedinmorecomplexscenarios[18].Whenalanguageistrulycompositional,its
components can be systematically rearranged or substituted with conceptually equivalent components without
altering the overall meaning [106, 107].
The formalization of compositionality can be framed using the comprehension L or production L
comp prod
functionthatmapexpressionsfromalanguageLtoaspaceofmeaningsΦorviceversa[124].Forexample,the
functionL :L→Φreflects“allthethingsthatthelanguagecandenote” [124].Alanguageiscompositional
comp
if these functions act as a homomorphism, e.g., there exist binary operators ◦ on L and × on Φ such that
comp
for any expression composed of two constituents m and m in L, the following condition holds:
1 2
L (m ◦m )=L (m )×L (m ) (16)
comp 1 2 comp 1 comp 2
Topographic similarity
Topographic similarity (topsim), originally proposed by Brighton and Kirby [239] and first applied to EL by
Lazaridou et al. [154], is a metric designed to quantify the structural alignment between the internal represen-
tations of meanings and the corresponding generated messages in a communication system. Unlike RSA (cf.
Section 6.4.1), which compares the meaning space against a ground truth, topsim focuses on the internal align-
ment within an agent’s meaning and message spaces. “The intuition behind this measure is that semantically
similar objects should have similar messages” [154]. It has become a widely used metric in the study of EL, as
depicted in Figure B2 in Appendix B).
To compute topsim, we start by sampling k meaning representations denoted by φ, typically embedded
featurevectors,fromthemeaningspaceΦ.Letϕ={φ ,...,φ }denotethecollectionofthesesamples,withφ∈
1 k
Φ.Usingthesender’spolicyπM,wegeneratecorrespondingmessagesm =πM(φ )foreachsampleφ ∈ϕ.We
then compute distances withinξSthe meaning and language spaces using si uitabξ lSe dii stance functions fori language
∆ and meaning ∆ space. The choice of distance function ∆ depends on the nature of the spaces involved.
L Φ
For discrete communication, typical choices include Hamming [240] or Levenshtein [241] distance, whereas for
continuous spaces, cosine or Euclidean distance are often used [50]. Finally, we compute the Spearman rank
correlation ρ [238] using the ranked distances to get the topsim value of the language:
cov(R(∆ (m )),R(∆ (φ )))
ρ= L i Φ i ∀φ ∈ϕ (17)
σ σ i
R(∆L(mi)) R(∆Φ(φi))
Positional Disentanglement
Positional Disentanglement (posdis) was introduced by Chaabouni et al. [107] as a metric to evaluate the
extent to which words in specific positions within a message uniquely correspond to particular attributes of
the input. This metric operates on an order-dependent strategy, which is normalized by the message length
and calculated as the ratio of mutual information to entropy. The underlying assumption is that the language
27leverages positional information to disambiguate words, such that “each position of the message should only be
informative about a single attribute” [107]. Thus, “posdis assumes a message whose length equals the number
of attributes in the input object, and where each message token, in a specific position, represents a single
attribute” [166]. This order-dependence is a characteristic feature of NL structures and is essential for the
emergence of sophisticated syntactic patterns [107].
The metric begins by identifying each word w at position p in a message m, where f represents the feature
p
vectorofthegroundtruth.ThemutualinformationI(w ,f )betweenw andaspecificfeaturef iscalculatedto
p i p i
determine how informative the position p is about the attribute f (cf. Equation 18). The two most informative
i
features f1 and f2 are then identified based on the mutual information value (cf. Equation 19). To quantify
i i
positional disentanglement, the mutual information difference between the two most informative features is
normalizedbytheentropyH(w )ofthewordatpositionp,asdefinedinEquation20andEquation21.Finally,
p
the overall posdis value for a language is calculated by averaging the posdis scores across all positions in the
messages within the dataset. For messages of varying lengths, the posdis score is normalized by the average
message length |m|, as given in Equation 22.
(cid:18) (cid:19)
I(w ,f )=
(cid:88) (cid:88)
P (w ,f )log
P (w p,f i)
(18)
p i p i P (w )P(f ))
p i
wp∈mfi∈f
f1 =argmaxI(w ,f ) and f2 = argmax I(w ,f ) (19)
i p i i p i
fi∈f fi∈f∧fi̸=f i1
(cid:88)
H(w )=− P (w )log(P (w )) (20)
p p p
wp∈m
I(cid:0)
w
,f1(cid:1) −I(cid:0)
w
,f2(cid:1)
posdis = p i p i (21)
p H(w )
p
1 (cid:88) 1 (cid:88)
posdis= posdis with |m|= |m| (22)
|m| p |M|
p m∈M
Bag of Symbols Disentanglement
Bag of Symbols Disentanglement (bosdis) is a metric introduced by Chaabouni et al. [107] to assess the degree
towhichwordsinalanguageunambiguouslycorrespondtodifferentinputelements,regardlessoftheirposition
withinamessage.Whilepositionaldisentanglement(posdis)reliesontheassumptionthatpositionalinformation
iscrucialfordisambiguatingwords(cf.Section6.4.2),bosdisrelaxesthisassumptionandcapturestheintuition
behind a permutation-invariant language. In such a language, the order of words is irrelevant, and only the
frequency of words carries meaning [107]. The metric normalizes the mutual information between symbols and
input features by the entropy summed over the entire vocabulary. This approach maintains the requirement
that each symbol uniquely refers to a distinct meaning, but shifts the focus to symbol counts as the primary
informative element.
(cid:18) (cid:19)
I(w,f )=
(cid:88) (cid:88)
P (w,f )log
P (w,f i)
(23)
i i P (w)P (f )
i
w∈mfi∈f
f1 =argmaxI(w,f ) and f2 = argmax I(w,f ) (24)
i i i i
fi∈f fi∈f∧fi̸=f i1
(cid:88)
H(w)=− P (w)log(P (w)) (25)
w∈m
I(cid:0) w,f1(cid:1) −I(cid:0) w,f2(cid:1)
bosdis = i i (26)
w H(w)
1 (cid:88)
bosdis= bosdis (27)
|W| w
w∈W
28Tree Reconstruct Error
Tree Reconstruct Error (TRE) assumes prior knowledge of the compositional structure within the input data,
enablingtheconstructionoftree-structuredderivations[120].AsdefinedbyAndreas[120],alanguageisconsid-
eredcompositionalifitfunctionsasahomomorphismfrominputstotheirrepresentations.Thecompositionality
ofalanguageshouldbeevaluatedbyidentifyingrepresentationsthatallowanexplicitlycompositionallanguage
to closely approximate the true underlying structure [120]. One metric for this assessment is TRE, which quan-
tifies the discrepancy between a compositional approximation and the actual structure, using a composition
function and a distance metric. A TRE value of zero indicates perfect reproduction of compositionality.
The compositional nature of a sender’s language is affirmed if there exists an assignment of representations
to predefined primitives (e.g., categories, concepts, or words) such that for each input, the composition of
primitive representations according to the oracle’s derivation precisely reproduces the sender’s prediction [120].
TREspecificallymeasurestheaccuracywithwhichagivencommunicationprotocolcanbereconstructedwhile
adhering to the compositional structure of the derivation or embedding of the input e∈E [50].
OneofthekeyadvantagesoftheTREframeworkisitsflexibilityacrossdifferentsettings,whetherdiscreteor
continuous. It allows for various choices of compositionality functions, distance metrics, and other parameters.
However, this flexibility comes with challenges, including the requirement for an oracle-provided ground truth
and the necessity of pre-trained continuous embeddings.
It is defined in a way that allows the choice of the distance metric δ and the compositionality function ◦
to be determined by the evaluator [120]. When the exact form of the compositionality function is not known a
priori, it is common to define ◦ with free parameters, as suggested by Andreas [120], treating these parameters
as part of the learned model and optimizing them jointly with the other parameters η. However, care must be
taken when learning the compositional function to avoid degenerate solutions [120].
Given a data sample k from the dataset K (k ∈ K) and a corresponding message m from the set of all
possible messages M (m ∈ M), TRE requires a distance function δ and learnable parameters η. Additionally,
it employs a compositionality function ◦ and pre-trained embeddings of ground truth, denoted by e∈E, which
can be obtained using models like word2vec.
The functions involved in the TRE calculation are as follows:
• Pre-trained ground truth oracle (e.g., word2vec): E :K →E
• Learned language speaker: ξ :K →M
S
• Learnable approximation function for TRE: f(cid:101)η :E →M
In the discrete message setting, which is the focus here, a discrete distance metric such as L is typically
1
chosen, along with a compositional function ◦ defined by a weighted linear combination [50, 120]:
m ◦m =Am +Bm with η ={A,B} (28)
1 2 1 2
To compute the TRE, an optimized approximation function f(cid:101)η is required. This function must satisfy two
key properties: embedding consistency, meaning that the learned parameters η are specific to an embedding,
and compositionality, which ensures that the function behaves according to:
f(cid:101)η(e i)=η
i
and f(cid:101)η(⟨e i,e j⟩)=f(cid:101)η(e i)◦f(cid:101)η(e j)
The optimization process involves minimizing the distance between the output of the learned language
speaker ξ S(k i) and the approximation function f(cid:101)η(e i), based on the ground truth:
(cid:88) (cid:16) (cid:17)
η∗ =argmin δ ξ S(k i),f(cid:101)η(e i) with E(k i)=e
i
(29)
η
i
With the optimized parameters η∗, TRE can be calculated at two levels: the datum level, which assesses
individual instances:
(cid:16) (cid:17)
TRE(k i)=δ ξ S(k i),f(cid:101)η∗(e i) with E(k i)=e
i
(30)
and the dataset level, which measures the overall communication performance across the dataset:
1 (cid:88)
TRE(K)= TRE(k) (31)
|K|
k∈K
Conflict Count
Conflict count, introduced by Kuciński et al. [152], is designed to quantify the extent to which the assignment
offeaturestowordsinalanguagedeviatesfromtheword’sprincipalmeaning.Thismetricisparticularlyuseful
29in scenarios where the language employs synonyms, as it accounts for the possibility of multiple words referring
to the same concept.
Theconflictcountmetricoperatesundertheassumptionthatthenumberofconceptsorfeaturesf givenin
i
afeaturevectorf ofasamplek inthecollectionofsamplesK isequaltothemessagelength|m|,andthatthere
existsaone-to-onemappingbetweenaconceptf ∈f andawordw ∈W.Themetriccountshowfrequentlythis
i
one-to-one mapping is violated, with a value of 0 indicating no conflicts and, therefore, high compositionality.
An advantage of this metric is its ability to accommodate redundancy in the language. However, it also has
limitations, such as the assumption that the number of features or attributes equals the message length, i.e.,
|f| = |m|. Additionally, because conflict count assumes the number of concepts in a derivation to be equal to
the message length, it becomes undefined for languages or protocols that violate this assumption, such as those
involving negation or context-sensitive constructions presented in [50].
The primary objective of conflict count is to quantify the number of times the mapping from a word w to
its principal meaning φ is violated. This requires the assumption that a mapping α exists from the position p
w
of word w in message m to an individual feature in feature vector f, such that:
α={1,...,|m|}→{1,...,|f|} (32)
In this framework, the meaning of a word, denoted by φ , is determined by both the word w itself and its
w
positionpwithinthemessage.Thismeaningcorrespondstoaspecificinstancej ofaparticularfeatureiwithin
the feature vector f, such that f =φ(w,p).
i,j
Theprocessofcalculatingtheconflictcountbeginsbyidentifyingtheprincipalmeaningofeachword-position
pair:
φ(w,p:α)=argmaxcount(w,p,f :α) ∀f ∈f (33)
i,j i,j
fi,j
using the count function:
(cid:88)
count(w,p,f :α)= |{w |w ∈m(k)∧pos (w)=p∧f ∈k}| (34)
i,j m i,j
k∈K
wherem(k)isthemessageproducedforsamplek andpos (w)computesthepositionofwordw inmessagem.
m
Finally, the conflict count value conf is determined by finding the mapping α that minimizes the score:
(cid:88)
conf =argmin score(w,p:α) (35)
α
w,p
where the score function is defined as:
(cid:88)
score(w,p:α)= count(w,p,f :α) (36)
i,j
fi,j̸=φ(w,p)
6.4.3 Consistency
Foralanguagetobeeffective,themeaningofeachwordmustbeconsistentacrossdifferentcontexts.Inconsistent
word meanings can render a language practically useless, even if the language is semantically grounded and
exhibitscompositionalproperties[113].Indialoguesettings,particularlyintheabsenceofexplicitregularization
mechanisms, words often fail to maintain consistent groundings across different instances, leading to ambiguity
andreducedcommunicativeeffectiveness[113].Thus,itiscrucialtocarefullymonitorthislanguagecharacteristic
in EL settings.
Mutual Information
Consistencyinlanguagecanbequantitativelyassessedbyexaminingthemutualinformationbetweenmessages
andtheircorrespondinginputfeatures.Ideally,aconsistentlanguagewillexhibitahighdegreeofoverlapbetween
messages and features, leading to a high mutual information value, indicating strong correspondence [136].
Formally, mutual information between two random variables, say X and Y, with joint distribution P
(X,Y)
and marginal distributions P and P , is defined as the Kullback–Leibler divergence D (see Equation 11)
X Y KL
between the joint distribution and the product of the marginals:
I(X;Y)=D (cid:0) P ∥P ⊗P (cid:1) (37)
KL (X,Y) X Y
30In the context of discrete communication, where both messages and sample features are represented as
discretevariables,themutualinformationbetweenthesetofmessagesM andthesetoffeaturesF iscomputed
using a double summation over all possible message-feature pairs:
I(M;F)=
(cid:88) (cid:88)
P
(m,f)log(cid:18) P (M,F)(m,f) (cid:19)
(38)
(M,F) P (m)P (f)
M F
m∈Mf∈F
where P (m,f) is the joint probability of message m and feature f, and P (m) and P (f) are the
(M,F) M F
marginal probabilities of m and f, respectively.
Correlation
Various studies employ different statistical techniques to measure consistency using correlations [99, 101, 159,
162,170,181].Forexample,consistencywithinalanguagesystemcanbequantifiedbyanalyzingthevariability
of words produced for a given sample k. Specifically, given the set of all words representing k, a heatmap is
generatedusingthemeanofthisset.ThesharpnessoftheheatmapisthenquantifiedbycomputingtheVariance
of the Laplacian (VoL). The average consistency score is obtained by dividing the VoL of the heatmap by the
count of all samples considered, as introduced by Verma and Dhar [181].
Additionally,Muletal.[162]exploredthecorrelationbetweenmessagesandactionsaswellasbetweenmes-
sages and salient properties of the environment. The analysis reveals correlations by examining the conditional
probability distribution of actions given the messages produced by a pretrained or fine-tuned receiver. This dis-
tribution,denotedas P (a|m),wasvisualized usingbin barplots tohighlight theprominentcorrelations [162].
Similarly, the relationship between input and messages is analyzed by examining the conditional distribution of
a pretrained sender’s messages given the observational input, represented as P (m|o) [162].
Coherence
Coherence is often assessed through context independence, a metric initially proposed by Bogin et al. [33].
Context independence examines whether words within a language maintain consistent semantics across varying
contexts. However, context independence may be considered restrictive, particularly in languages where syn-
onyms are prevalent [29, 150]. The context independence metric aims to measure the alignment between words
w ∈ W and features f ∈ F of the input samples by analyzing their probabilistic associations. Specifically,
P (w |f) denotes the probability that a word w is used when a feature f is present, while P (f |w) represents
the probability that a feature f appears when a word w is used. For each feature f, we identify the word w
f
most frequently associated with it by maximizing P (f |w):
w :=argmaxP (f |w) (39)
f
w
ThecontextindependenceorcoherencemetricCIisthencomputedastheaverageproductoftheseprobabilities
across all features:
1 (cid:88)
CI(w ,f)= P (w |f)P (f |w ) (40)
f |F| f f
f∈F
Thismetricrangesfrom0to1,with1indicatingperfectalignment,meaningthateachwordretainsitsmeaning
consistently across different contexts and is thus used coherently.
Entropy
Entropy metrics are instrumental in analyzing the variability and predictability within linguistic systems. The
most fundamental use of entropy involves marginal probabilities, which capture the variability in the number
of words in a language [93, 99]. More advanced applications of entropy focus on sender language entropy, which
examinestheconditionalentropyofmessagesgivenfeaturesandviceversa[32,103].Specifically,lowconditional
entropyH(M |F)indicatesthatauniquemessageisusedforaspecificfeature,whereashighH(M |F)reflects
the generation of synonyms for the same feature [103].
Recent approaches further extend this analysis by combining conditional entropies [108, 163]. For example,
H(M |F) quantifies the uncertainty remaining about messages after knowing the concepts, while H(F |M)
measures the uncertainty about concepts given the messages. A negative correlation between these measures
and agent performance is expected [163]. However, a notable limitation of these entropy-based methods is that
they focus on complete messages rather than individual words, which can limit the evaluation of more complex
languages.
31For example, Ohmer et al. [163] provide the following comprehensive evaluation approach. First, the
conditionalentropyofmessagesgivenfeaturesH(M |F),seeEquation41,andH(F |M)arecalculated.Addi-
tionally, the marginal entropies are calculated using Equation 42, where X represents either messages M or
features F.
(cid:18) (cid:19)
(cid:88) (cid:88) P (f,m)
H(M |F)=− P (f,m)log (41)
P (f)
m∈Mf∈F
(cid:88)
H(X)=− P (x)log(P (x)) (42)
x∈X
Using these entropies, consistency, see Equation 43, measures how much uncertainty about the message is
reduced when the feature is known, with lower values indicating more consistent message usage. effectiveness,
on the other hand, see Equation 44, evaluates the reduction in uncertainty about the feature when the message
is known, with lower values reflecting more unique messages for individual features.
H(M |F)
consistency(F,M)=1− (43)
H(M)
H(F |M)
effectiveness(F,M)=1− (44)
H(F)
Finally, the normalized mutual information NI provides a combined score:
H(M)−H(M |F)
NI(F,M)= (45)
0.5·(H(F)+H(M))
A high NI score indicates a strong predictive relationship between messages and features, reflecting high
consistency.
Similarity
The Jaccard similarity coefficient is a another metric for evaluating the consistency of language usage among
agents [65, 101]. It quantifies the similarity between two sets by comparing the size of their intersection to
the size of their union [101]. To measure language consistency, the Jaccard similarity is computed by sampling
messages for each input and averaging the similarity scores across the population [101]. This approach reflects
howconsistentlywordsareusedacrossdifferentmessages.Specifically,JaccardsimilarityJ(M ,M )isdefined
ξi ξj
in Equation 46, where M and M represent sets of messages generated by different agents based on the same
ξi ξj
input. The similarity ranges from 0 to 1, with 1 indicating complete overlap and thus perfect similarity.
Inpractice,Jaccardsimilarityhelpstoassessthecoherenceoflanguagesemergingfromagent-basedsystems.
Forinstance,inreferentialgameexperiments,highperplexity(cf.Section6.2.2)andlowJaccardsimilarityhave
beenobserved,suggestingthatagentsassignuniquebutincoherentstringstoobjecttypestogainanadvantage
in the game without producing a consistent language [65]. However, Jaccard similarity is only applicable to
scenarioswheremultipleagentsgeneratemessagesaboutthesamesetofobjects.Thus,itsapplicationislimited
to cases where the goal is to compare the overlap of message sets between agents attempting to convey similar
meanings.
|M ∩M | |M ∩M |
J(M ,M )= ξi ξj = ξi ξj (46)
ξi ξj |M ∪M | |M |+|M |−|M ∩M |
ξi ξj ξi ξj ξi ξj
6.4.4 Generalization
A language’s ability to generalize is crucial for describing objects and concepts at different levels of complexity,
allowing for effective clustering and hierarchical representation. Generalization in ELs reflects their ability to
extend beyond specific training instances to novel situations. “If the emergent languages can be generalised,
we then could say that these languages do capture the structure of meaning spaces” [141]. Research shows
that languages capable of generalization tend to emerge only when the input is sufficiently varied [107]. In
contrast, a large dictionary size often indicates a lack of generalization [93]. Human languages have evolved
under the pressure of a highly complex environment, fostering their generalization capabilities [107]. However,
deep learning models often exploit dataset-specific regularities rather than developing systematic solutions [45].
To address this, much research is being done on the systematic generalization abilities of ELs.
32Zero Shot Evaluation
Zero-shot evaluation, which assesses the ability of an agent to generalize to novel stimuli [65, 154], has become
a standard metric in the study of EL as illustrated in Figure B2 in Appendix B. This evaluation is critical
to understand the generalization capabilities of an agent. Zero-shot evaluation can be done in two different
scenarios, one with unseen input and the other with an unseen partner.
In the unseen input scenario, models are tested on a zero-shot test set consisting of samples with feature
combinations not encountered during training. Performance, such as accuracy, is reported for these unseen
samples [99, 107, 113, 150, 154]. Different methods for constructing novel inputs include exposing models to
objects that resemble training data but have unseen properties or entirely novel combinations of features [154].
Moreover, a more drastic approach may involve moving to entirely new input scenarios, such as testing the
ability of agents to generalize across different game types [108].
The unseen partner scenario, also known as cross-play or zero-shot coordination, evaluates models by pair-
ing agents that did not communicate during training. Again, performance is measured, typically in terms of
accuracy [197, 242].
However, these approaches also have drawbacks. The unseen input scenario requires a ground truth oracle
to withhold feature combinations, which is necessary to accurately define novel combinations. Meanwhile, the
unseenpartnersetupcanintroduceinefficienciesbyrequiringadditionalresourcestotrainnovelcommunication
partners for testing.
Ease and Transfer Learning
EaseandTransferLearning(ETL),asproposedbyChaabounietal.[25],evaluateshoweasilynewlistenerscan
adapttoanELondistincttasks.ETLextendstheconceptofease-of-teaching[91]byassessinghoweffectivelya
deterministiclanguage,developedbyafixedsetofspeakers,canbetransferredtonewlistenerswhoaretrained
on tasks different from the original one for which the language was optimized [25]. This metric not only gauges
the language’s generality but also its transferability across tasks [25].
TomeasureETL,afterconvergence,afixednumberofspeakersproduceadeterministiclanguagebyselecting
symbolsusinganargmaxoperationovertheirdistributions.Thislanguageisthenusedtotrainnewlyinitialized
listeners on a new task. The training curve is tracked to observe how quickly and accurately the listeners learn
the task, which may involve more challenging objectives than the former training tasks [25, 140].
6.5 Pragmatics
Pragmatics is a critical aspect of language that examines how context influences meaning [185]. It goes beyond
the literal interpretation of words and requires the listener to infer the speaker’s intentions, beliefs, and mental
states, an ability known as Theory of Mind (ToM) [185]. In human interactions, this contextual reasoning is
essentialforpredictingandunderstandingbehavior.InthecontextofEL,pragmaticsfocusesonhoweffectively
agents use the communication ability in their environment. Empirical studies have shown that agents may
initiallyfailtousecommunicationmeaningfully,but,oncetheydocommunicate,theycanreachalocallyoptimal
solutiontothecommunicationproblem[216].Thus,evaluatingthepragmaticsofELisessentialtodetermining
its utility and effectiveness in real-world applications.
6.5.1 Predictability
Predictability evaluates the complexity of an environment and its effect on the need for communication. Thus,
it is a central metric for the probability of emergence and the use of EL. In simple environments with limited
actions, agents can often coordinate without communication [95].
Behavioral Divergence
Behavioral divergence, introduced by Dubova et al. [95], posits that less diversity in actions or messages cor-
relates with more predictable behavior, potentially reducing the need for communication. To quantify this,
we calculate Behavioral Action Predictability BAP and Behavioral Message Predictability BMP. Both use the
Jensen-ShannonDivergence(JSD)(seeEquation47)whichitselfusestheKullback-LeiblerDivergenceD (cf.
KL
Equation 11).
1 1 P +Q
D (P ∥Q)= D (P ∥M)+ D (Q∥M) with M = (47)
JS 2 KL 2 KL 2
BAP(seeEquation48)andBMP(seeEquation49)bothuseauniformdistributionQforcomparison.BAP
further uses the distribution of actions by the agent P (a ) while BMP uses the distribution of messages by the
ξ
33agent P (m ). Based on that, these metrics provide a robust measure of how predictable agent behaviors and
ξ
messagesare,withhighervaluesindicatinglesspredictabilityandgreaterneedforbeneficialcommunication[95].
BAP=D (P (a )∥Q) (48)
JS ξ
BMP=D (P (m )∥Q) (49)
JS ξ
6.5.2 Efficiency
In EL settings, efficient communication arises only when there is an opportunity cost [100]. Without such a
cost, there is no drive towards brevity, which limits the effectiveness and efficiency of EL in HCI.
Sparsity
Sparsity, as proposed by Kalinowska et al.[100], measures the extent to which agents minimize their commu-
nication during task execution. This metric requires only the collection of messages exchanged per episode for
computation. However, its applicability is limited to scenarios where communication is not strictly necessary
for task completion, i.e., agents have the option to send no messages at all or to send messages that contain
no meaningful information. A sparsity value of 0 indicates that an agent can solve the task using only a single
messagethroughoutanepisode,reflectingahighlyefficientcommunicationstrategy.Conversely,highersparsity
values indicate more frequent or verbose communication, which may indicate inefficiencies in the EL.
Communication sparsity ComSpar is mathematically defined as:
1 (cid:88)
ComSpar= · −log(|{m|m∈M ∧m̸=0}|) (50)
n ep,i
ep
Mep,i∈M
In this equation, M represents the set of all messages exchanged during episode i, and n is the total
ep,i ep
number of episodes observed. The collection {m|m∈M ∧m̸=0} consists of all messages m of episode i
ep,i
that are non-zero and thus contributing.
6.5.3 Positive Signaling
Positive signaling evaluates the alignment between an agent’s observations and its communication output [29].
The goal is to ensure that the outgoing transmitted information is both relevant and observable by the
agent [167].
Speaker Consistency
Speaker Consistency (SC), introduced by Jaques et al. [226], measures how effectively an agent’s messages
reflectitsstateortrajectory,therebyensuringthecommunicationismeaningful.Thisisquantifiedusingmutual
information. For an agent ξ , the trajectory τt represents the sequence of states and actions up to time step t.
i ξi
The message produced at time t is denoted by mt . The mutual information I(mt ,τt ) between the message
and trajectory is calculated as:
ξi ξi ξi
I(mt ,τt )=H(mt )−H(mt |τt )
ξi ξi ξi ξi ξi
(cid:88)
=− P (m)logP (m)
ξi ξi
m∈Mξi (51)
 
(cid:88)
+E τ ξt
i
 P ξi(m|τ ξt i)logP ξi(m|τ ξt i)
m∈Mξi
Here, H(mt ) is the entropy of the message distribution, H(mt |τt ) is the conditional entropy given the
ξi ξi ξi
trajectory, P (m) as marginal distribution of message m over all trajectories, and P (m|τt ) as conditional
ξi ξi ξi
distribution of message m given the trajectory τt . This way, the mutual information value reflects how much
information the message carries about the agent’ξ sitrajectory.
Lowe et al. [29] built on this concept and provided the following formula for Speaker Consistency (SC):
(cid:88) (cid:88) P (a,m)
SC= P (a,m)log (52)
P (a)P (m)
a∈Am∈M
In this equation, P (a,m) is the joint probability of action a and message m, calculated empirically by
averaging their co-occurrences across episodes. In general, SC is a valuable metric for evaluating whether the
EL is both informative and aligned with the behavioral patterns of the sender.
346.5.4 Positive Listening
Positive listening evaluates the effectiveness of how a message receiver utilizes and applies incoming informa-
tion [29]. However, agents should not simply process messages similarly to other observations to avoid treating
themasmeredirectives[132].Nevertheless,themetricspresentedinthissectionfocusonevaluatingthereceiver’s
ability to effectively integrate and use the information received, rather than evaluating the receiver’s ability to
do more than just follow instructions.
Instantaneous Coordination
Instantaneous Coordination (IC), also referred to as listener consistency [29], was introduced by Jaques
et al. [226] as a metric to evaluate how effectively an agent’s message influences another agent’s subsequent
action. IC is computed similarly to Speaker Consistency (cf. Section 6.5.3), but differs in that it measures the
mutual information between one agent’s message and the other agent’s next action, averaged over episodes.
This metric directly captures the receiver’s immediate reaction to an incoming message, making it a measure
ofpositivelistening.However,itprimarilycapturessituationswherethereceiver’sactionisdirectlychangedby
the sender’s message, without considering the broader context or long-term dependencies [29]. Accordingly, “IC
can miss many positive listening relationships” [29].
Jaquesetal.[226]proposedtwospecificmeasuresforIC:Onethatquantifiesthemutualinformationbetween
the sender’s message and the receiver’s next action (see Equation 53), and another one that measures the
mutual information between the sender’s current action and the receiver’s next action (see Equation 54). These
measures are calculated by averaging over all trajectory steps and taking the maximum value between any two
agents, focusing on short-term dependencies between consecutive timesteps.
IC =I(mt;at+1) (53)
mξS→aξR k j
IC =I(at;at+1) (54)
aξS→aξR k j
A unified equation for IC is provided by Lowe et al. [29]:
(cid:16) (cid:17)
P at+1,mt
SC=
(cid:88) (cid:88)
P
(cid:16)
at+1,mt
(cid:17)
log
ξR ξS
(55)
ξR ξS
P
(cid:16) at+1(cid:17)
P
(cid:16)
mt
(cid:17)
mt ξS∈MξSat ξ+ R1∈AξR ξR ξS
(cid:16) (cid:17)
Here, P at+1,mt is the empirical joint probability of the sender’s message and the receiver’s subsequent
ξR ξS
action, averaged over episodes within each epoch.
Message Effect
The Message Effect (ME) metric, introduced by Bouchacourt and Baroni [188], quantifies the influence of a
message sent by one agent on the subsequent actions and messages of another agent. This metric explicitly
considers bidirectional communication, so in the following we use generic agents ξ and ξ instead of sender
A B
and receiver. A notable challenge of this metric is the requirement for counterfactual analysis.
Given an agent ξ at timestep t sending a message mt , we define zt+1 as the combination of the
action and message prA oduced by agent ξ at the following timξAestep. Accordinξ gBly, the conditional distribution
B
(cid:16) (cid:17)
P zt+1 |mt represents the response of ξ to the message from ξ . To account for counterfactuals, which
ξB ξA B A
encodewhatmighthavehappenedhadξ sentadifferentmessagemt ,wedefinethecounterfactualdistribution
(cid:16) (cid:17)
A (cid:101)ξA
P(cid:101) zt+1 (see Equation 56).
ξB
The ME is then measured by the Kullback-Leibler divergence between the actual response and the counter-
factualresponse(seeEquation57).Thecomputationinvolvessamplingzt+1,k fromtheconditionaldistribution
ξ (cid:16)B (cid:17)
for the actual message and sampling counterfactuals m (cid:101)t
ξA
to estimate P(cid:101) z ξt+ B1,k (see Equation 58). The final
ME is calculated as the average KL divergence over the collection of samples K (see Equation 59).
P(cid:101)(cid:16) z ξt+ B1(cid:17) = (cid:88) P (cid:16) z ξt+ B1 |m (cid:101)t ξA(cid:17) P(cid:101)(cid:0) m (cid:101)t ξA(cid:1) (56)
m(cid:101)t
ξA
(cid:16) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17)
MEt
ξA→ξB
=D
KL
P z ξt+ B1 |mt
ξA
∥P(cid:101) z ξt+ B1 (57)
35J
P(cid:101)(cid:16) z ξt+ B1,k(cid:17) =(cid:88) P (cid:16) z ξt+ B1,k |m (cid:101)t ξA(cid:17) P(cid:101)(cid:0) m (cid:101)t ξA(cid:1) (58)
j=1
(cid:16) (cid:17)
P zt+1,k |mt
MEt =
1 (cid:88)
log
ξB ξA
(59)
ξA→ξB |K|
k∈K
P(cid:101)(cid:16) z ξt+ B1,k(cid:17)
Causal Influence of Communication
The Causal Influence of Communication (CIC) metric, introduced independently by Jaques et al. [226] and
Lowe et al. [29], provides a direct measure of positive listening by quantifying the causal effect that one agent’s
message has on another agent’s behavior. Traditional methods of evaluating communication often fall short, as
simply testing for a decrease in reward after removing the communication channel does not adequately capture
the utility of communication [29].
CIC is computed using the mutual information between an agent’s message and the subsequent action
of the receiving agent. Unlike Instantaneous Coordination (cf. Section 6.5.4), CIC considers the probabilities
P (a,m) = π (a|m)π (m) that represent changes in the action distribution of the receiver ξ when the
ξR ξS R
message m from the sender ξ is altered. These probabilities are normalized within each game to accurately
S
reflect the influence of messages on actions within the same context [29].
For multi-time-step causal influence, the CIC metric is defined as the difference between the entropy of the
receiver’s actions with and without communication:
CIC(τ )=H(at |τ )−H(at |τ+M) (60)
ξR ξR ξR ξR ξR
Here, τ denotes the standard trajectory of the receiver, comprising state-action pairs, while τ+M includes
thecommuξ nR
icatedmessages.TheCICisestimatedbylearninganapproximatepolicyfunctionπ(·|τ
ξR
).Formore
ξR
detailsonthemultistepversion,refertoEcclesetal.[216],andforthesingle-stepversion,seeJaquesetal.[226].
6.5.5 Symmetry
Symmetry in EL refers to consistent language use across agents in settings, where agents alternate between
roles such as message sender and receiver [90, 95]. Thus, symmetry ensures convergence to a common language
rather than distinct dialects [95].
Inter-Agent Divergence
Inter-Agent Divergence (IAD), introduced by Dubova et al. [95, 243], quantifies the similarity in how different
agents map messages to actions. Let a denote the action of agent ξ . The first step involves computing the
ξi i
marginal action distributions for each agent given a message m, represented as P(a |m).
ξi
P (a |m) ∀ξ ∈ξ∧m∈M (61)
ξi i
Thedivergencebetweentwoagents,ξ andξ ,basedontheirresponsestothesamemessage,isthencalculated
i j
using the Jensen-Shannon Divergence (JSD) as follows:
(cid:0) (cid:0) (cid:1)(cid:1)
D (ξ ,ξ ,m)=D P (a |m)∥P a |m
JS i j JS ξi ξj
1(cid:2) (cid:0) (cid:0) (cid:1) (cid:1)(cid:3)
= 2 D KL(P (a ξi |m)∥M)+D KL P a ξj |m ∥M (62)
(cid:0) (cid:1)
P (a |m)+P a |m
where M = ξi ξj
2
Finally, the overall IAD is computed by averaging these divergences across all possible agent pairs (ξ ,ξ )∈
i j
ξ and messages m∈M:
comb
1 1 (cid:88) (cid:88)
IAD= D (ξ ,ξ ,m) (63)
|ξ ||M| JS i j
comb
(ξi,ξj)∈ξcombm∈M
While IAD effectively captures the consistency of inter-agent communication, it may have limitations when
applied to more complex languages where message-level comparisons become difficult.
36Within-Agent Divergence
Within-Agent Divergence (WAD), proposed by Dubova et al. [95, 243], measures the consistency of an agent’s
communicationbehaviorwhenitchangesroles,suchasfromsendertoreceiver.Thismetriccapturestheinternal
symmetry in an agent’s behavior and is crucial in complex systems where agents can assume different roles
within the same environment. To compute WAD, we again first consider the action distribution P (a |m) for
ξi
each agent ξ over a set of messages m ∈ M. This distribution reflects how an agent’s actions are conditioned
i
on receiving or sending a specific message.
P (a |m) ∀ξ ∈ξ∧m∈M (64)
ξi i
Given this, the Jensen-Shannon Divergence (JSD) is used to assess the divergence between an agent’s
behavior when acting as a sender ξ versus as a receiver ξ :
i,S i,R
(cid:0) (cid:0) (cid:1) (cid:0) (cid:1)(cid:1)
D (ξ ,ξ ,m)=D P a |m ∥P a |m
JS i,S i,R JS ξi,S ξi,R
1(cid:2) (cid:0) (cid:0) (cid:1) (cid:1) (cid:0) (cid:0) (cid:1) (cid:1)(cid:3)
= 2 D KL P a ξi,S |m ∥Q +D KL P a ξi,R |m ∥Q (65)
(cid:0) (cid:1) (cid:0) (cid:1)
P a |m +P a |m
with Q= ξi,S ξi,R
2
Finally, the overall WAD is computed by averaging this divergence across all agents ξ ∈ ξ based on the
i
WAD for individual agents and their messages m∈M :
ξi
1 (cid:88) 1 (cid:88)
WAD= D (ξ ,ξ ,m) (66)
|ξ| |M | JS i,S i,R
ξi∈ξ
ξi
m∈Mξi
6.6 Summary of the Metrics
While some EL features are quantifiable by multiple metrics and have been investigated in multiple studies,
others remain underexplored, as illustrated in Figure B2 in Appendix B. Metrics such as topographic similarity
and zero shot evaluation, both of which assess semantic properties, are well established and widely utilized
acrossmultiplestudies.Incontrast,metricsrelatedtopragmatics,suchasspeakerconsistency andinstantaneous
coordination, are fairly well established but are less frequently used. Morphology metrics, particularly active
words andaverage message length,aremorecommonlyused,whereassyntaxremainsaperipheralconcern,with
onlytwoisolatedmetricsproposedandnotadoptedinsubsequentresearch.Thisimbalanceindicatesthatwhile
semantic metrics dominate EL research, morphology and pragmatics receive moderate attention, and syntax is
mostly neglected.
Furthermore, the optimality of these metrics is not straightforward. Rather than being simply minimized or
maximized, their ideal values are likely to lie at a nuanced balance point that varies depending on the specific
ELsystemandapplication.Thisuncertaintyleavesthecriticalquestionofwhatconstitutesa‘good’ELsystem
largely unanswered. Addressing this gap will require a deeper exploration of underrepresented metrics and a
more refined understanding of how to evaluate EL systems holistically.
7 Future Work
Inthissection,weoutlinepotentialfuturedirectionsfortheresearchfieldofEL,basedonourvisionoutlinedin
Section 7.1. We present major research opportunities, organized along key research dimensions, in Section 7.2.
Alongwithfutureresearchdirections,wehavesummarizedalistofopensourcecoderepositoriesinTableA1
inAppendixAthatcanserveasconvenientstartingpointsforexperimentingwiththesedirections,forexample,
comprehensive frameworks such as the EGG toolkit [148] and BabyAI [117] are included.
7.1 Vision
Our vision for EL research is grounded in a functional perspective, aiming to achieve significant breakthroughs
in human-agent interaction [24, 26, 29, 33, 35, 162, 242]. This means developing communication systems that
enable HCI at the human level, addressing the purpose, cost, and value of communication with intuitive and
effectiveinterfaces[27,36,57,134,175,219].AkeygoalistoensurethatELsaregroundedinreal-worldcontexts,
allowing agents to understand and interact with human-like comprehension and vice versa [19, 21, 39, 244].
Thisincludescreatinghierarchical,compositionalconceptualizationcapabilitiesthatallowagentstodiscussand
understand novel concepts in a structured, human-relevant manner [25, 119, 168, 173]. In addition, exploring
thepotentialforAIexplainabilitythroughcommunicationisanexcitingarea[21,113,175].Finally,inthelong
37term, creation and creativity through EL comparable to human capabilities would be a milestone. This would
allow agents to truly communicate on a human level and enhance their ability to perceive and adapt to their
environment through the use of language [88].
7.2 Dimensions and Opportunities
The development, evaluation, and application of EL in communication systems can be analyzed along several
criticaldimensions.Weidentifiedeightkeydimensionsthat,tothebestofourknowledge,representtheprimary
areas of focus in EL research.
1. Evaluation Metrics
Evaluation metrics are essential for rigorously assessing the characteristics and effectiveness of ELs. As detailed
inourtaxonomy(cf.Section5.4),wehaveidentifiedkeycharacteristicsandtheirassociatedmetrics.Whilesome
ELfeaturesarequantifiablethroughmultiplemetricsandhavebeenexaminedinmultiplestudies,othersremain
underexplored, as illustrated in Figure B2 in Appendix B. We emphasize the need to develop comprehensive
and quantitative metrics that accurately capture these features, which are critical to determining the practical
utility of ELs. Previous studies have similarly highlighted this need [25, 29, 45, 65, 94, 112, 165, 220, 227].
In addition, further research is needed to systematically investigate existing metrics, especially with respect
to their sensitivity to variations in settings, algorithms, and agent architectures [29, 49]. It is imperative that
these metrics be subjected to more rigorous investigation to ensure that they enable meaningful quantitative
comparisons and support well-founded conclusions about the capabilities and utility of ELs. Thus, we endorse
more comprehensive studies, more edge case testing and, in particular, more analysis of actual human-agent
interaction. We see this as a critical priority for advancing the field.
2. Emergent Language and Natural Language Alignment
ThisdimensionaddressestheconvergenceanddivergencebetweenELandNL.Akeyapproachtothischallenge,
discussed in Section 5.3, involves leveraging language priors to guide this alignment. Achieving robust EL-
NL alignment is essential for advancing human-agent interaction. Thus, future research should explore the
integration of NL-centered metrics and regularization techniques to enhance this alignment [14, 92].
However, this alignment presents a fundamental dilemma. On the one hand, agents need the autonomy to
develop languages organically, tailored to their specific interactions and requirements. On the other hand, to
facilitate seamless human-agent communication, these ELs must closely resemble NLs, which imposes signifi-
cant constraints on their development. This tension creates what we call the Evolution-Acquisition Dilemma,
wheretheevolutionaryprocessfostersintrinsicallymotivatedlanguageemergence,whiletheacquisitionprocess
necessitates alignment with NL. Balancing these competing needs is a critical challenge for future research in
this area.
3. Representation Learning
EL can be viewed as a complex representation learning task, focusing on how agents encode, interpret, and
construct internal representations of observations and linguistic data. While representation learning is a well-
established area in artificial intelligence research, its application in the context of EL remains underexplored.
This dimension is central to the analysis of meaning and language space as outlined in our framework, which is
based on the semiotic cycle (cf. Figure 10). Advancing this dimension requires advanced latent space analyses
toelucidatetherelationshipsbetweenELs,underlyingworldmodels,andNLstructures.Inaddition,evaluating
theimpactofdiscreteversuscontinuousrepresentationsiscriticaltorefiningourunderstandingofELdynamics.
Futureresearchdirectionsincludedevelopingmethodologiestoensurethatagentrepresentationsmoreaccurately
reflecttheinputtheyreceive[16],exploringefficientrepresentationof(multimodal)information[110],conducting
in-depth analyses to uncover and mitigate influencing factors and biases in learned representations [45], and
assessing the efficacy of these representations for downstream tasks [96].
4. Agent Design
Agent design is a critical aspect in EL research, directly influencing the linguistic capabilities and adaptability
of artificial agents. Prominent research directions include the investigation of advanced neural network archi-
tectures tailored for EL [25, 156, 172], the creation of architectures optimized for heterogeneous and dynamic
agent populations, and the refinement of structures that enhance language emergence and linguistic proper-
ties[96,153].Inaddition,modulardesignsratherthanmonolithiconespotentiallyofferadvantagesbyseparating
language processing from other task-specific computations. Addressing these design challenges is critical to
advancing both EL research and broader artificial intelligence goals.
385. Setting Design
The environment in which agents operate is central to shaping the EL, encompassing interaction rules, agent
goals, and communication dynamics (cf. Table 3). This dimension is integral to the setting space outlined in
our framework (cf. Figure 10). Important future research directions include scaling up experimental settings
to include larger and more complex tasks [14, 25, 116, 132, 188, 216] with a focus on realistic perceptually
grounded game environments [95, 188]. In addition, the study of the impact of populations as such [165] and
the use of heterogeneous agent populations [103] are crucial areas of research. While some benchmarks have
been established and utilized [117, 148], there remains a significant need for the development and widespread
dissemination of comprehensive benchmarks in area of research.
6. Communication Design
The design of the communication channel in EL systems is critical, focusing on how agents exchange and struc-
ture information through the channels available to them. This aspect is directly related to the phonetics and
phonology components outlined in our taxonomy (cf. Section 5.4.1 and Section 5.4.2). For discrete ELs, it is
essentialtoestablishchannelsthatsupportword-basedcommunication,withconsiderationssuchasvocabulary
size and variable message length being fundamental to enabling effective and scalable human-agent interaction.
Future research directions in this area include the exploration of topology-aware variable communication chan-
nels,theintegrationofheterogeneouschannelswithinmulti-agentsystems,andtheevolutionofcommunication
channels over time. Moreover, the incorporation of multimodal communication channels could provide more
realistic and contextually rich stimuli, which may significantly enhance the sophistication and applicability of
ELs in NL-oriented human-agent coordination [25].
7. Learning Strategies
Learningstrategiesfocusonhowagentsacquire,adapt,andrefinetheirlinguisticcapabilitiesovertime,including
the development of language rules and adaptation through interactions with other agents. While MARL serves
as the foundational framework, there is significant potential to enhance the learning process through strategic
design choices. Future research directions include the exploration of advanced regularization techniques [92,
226], the adoption of tailored optimization strategies [25], and the integration of supervised or self-supervised
learning objectives using appropriate loss designs [15, 137]. Additionally, the application of meta-learning [141],
decentralized learning approaches [55], and curriculum learning methodologies [55] offer promising avenues for
optimizing the EL learning process.
8. Human-Agent Interaction
ThefinaldimensionfocusesontheinterpretabilityofELsbyhumansandthedegreetowhichhumanscanshape
their development. This aspect is critical for creating human-agent interaction systems where communication
is intuitive and effective [110]. To advance this dimension, future research should prioritize the integration of
human-in-the-loop feedback mechanisms to ensure that ELs are not only practical, but also comprehensible to
human users [18, 24]. This will improve the usability and adoption of these systems in real-world applications.
Keyresearchdirectionsincludedesigningexperimentsthatcreateincentivesforagentstodevelopcommunication
strategies more closely aligned with human language [16]. Additionally, exploring the resilience of communi-
cation protocols to deception through training with competing agents can lead to more robust and realistic
interactions [34]. Exploring adaptive communication strategies to optimize the sparsity and clarity of messages
based on individual or group needs within human-agent teams is another promising direction [219].
8 Limitations and Discussion
In this section, we critically evaluate the limitations of our survey and identify areas for future improvement.
Throughourreview,weaimedtodevelopadetailedtaxonomyforthefieldofEL,focusingonitskeyproperties
(cf. Section 5), and to analyze as well as categorize quantification approaches and metrics (cf. Section 6). In
addition, we curated a summary of open questions and suggestions for future research (cf. Section 7). Despite
considerableeffortstoestablishaviabletaxonomyandframeworkinthemostsystematicandunbiasedmanner,
there are several potential limitations to our research approach and methodology.
First, while we have provided an extensive overview of 181 scientific publications in EL research, it is
important to acknowledge that our search process, despite being thorough, may have overlooked significant
contributions. Consequently, we do not claim completeness. However, we are very confident that our review
represents a fair and well-balanced reflection of the existing body of work and the current state of the art.
Second, our review includes sources that are not peer-reviewed, such as preprints from arXiv, to ensure
that our work captures the most recent developments and diverse perspectives, including those that might be
39controversial. While we have carefully examined each paper included in this review, we cannot guarantee that
every detail in non-peer-reviewed papers is entirely accurate. Consequently, we focused on concepts, findings,
and metrics that are supported by multiple studies.
Third, we have introduced a taxonomy and a comprehensive metrics categorization for EL research, a field
that is still in its early stages. This effort comes with inherent challenges, and while we have addressed many
of these, it is important to note that our proposed framework does not represent a consensus within the wider
research community. We are transparent about this limitation and encourage further discussion and validation.
Fourth, in order to maintain focus and conciseness, we have deliberately excluded ideas that lack associated
metrics. As a result, some conceptual ideas from the reviewed research literature that are difficult to quantify
in this early stage may not be fully explored in this survey.
Finally, we have incorporated several existing metrics into our proposed framework. While many of these
metricsarewellestablishedinthefield,weacknowledgethatamorerigorousandcriticalexperimentalevaluation
of these metrics would be beneficial. We strongly recommend that future research conduct such evaluations to
further refine and validate the tools and methods used in EL research.
9 Conclusion
In this paper, we present a comprehensive taxonomy of emergent language (EL), an overview of applicable
metrics, and a summary of open challenges and potential research directions. Additionally, we provide a list of
opensourcecoderepositoriesofthefieldinTableA1inAppendixA.Ouroverallgoalistocreateastandardised
yet dynamic framework that not only facilitates progress in this area of research, but also stimulates further
interest and exploration.
Section 2 introduces the foundational linguistic concepts that underpin our taxonomy. Section 5 offers a
comprehensive taxonomy of EL based on the review of 181 scientific publications. Section 6 presents a unified
categorizationandnotationforvariousmetrics,depictedinFigure9,ensuringconsistencyandclarity.Section7
provides a summary of current achievements and outlines research opportunities.
By providing a structured overview and systematic categorization of linguistic concepts relevant to EL we
have created a common ground for research and discussion. The detailed presentation of metrics and their
unified notation ensures readability and usability, making it easier for researchers to navigate related topics
and identify potential research opportunities and blind spots of future publications and the research field as a
whole. This survey provides a valuable perspective on the development and analysis of EL, serving as both a
guide and a resource for advancing this area of study.
EL is a fascinating and promising way to achieve grounded and goal-oriented communication among agents
andbetweenhumansandagents.Despiteitssignificantprogressinrecentyears,thefieldfacesmanyopenques-
tionsandrequiresfurtherevaluationmethodsandmetrics.Criticalquestionsremainaboutthemeasurabilityof
linguistic features, the validity of proposed metrics, their utility, and their necessity. Aligning EL with natural
language processing (NLP) for human-computer interaction (HCI) presents additional opportunities and chal-
lenges.Weencouragecontinuedcontributionsandinterdisciplinaryresearchtoaddresstheseissuesandadvance
the field.
Declarations
• Funding:WeacknowledgethefundingoftheinternshipsofAryaGopikrishnanandGustavoAdolphoLucas
De Carvalho by the German Academic Exchange Service (DAAD) project ‘RISE Germany’.
• Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use): Not
applicable.
• Ethics approval and consent to participate: Not applicable.
• Consent for publication: Not applicable.
• Data availability: Not applicable.
• Materials availability: Not applicable.
• Code availability: Not applicable.
• Author contribution: J.P., H.T. and T.M. had the idea for the article. J.P. performed the literature search
and data analysis. The first draft of the manuscript was written by J.P. with the continuous support of
C.W.d.P. and H.T. The first draft of the metrics section was written by A.G. All authors commented on
earlier versions of the manuscript and critically revised the final manuscript.
40Appendix A Additional Tables
TableA1: List of code repositories for the literature reviewed.
Paper Link
[21] https://github.com/agakshat/visualdialog-pytorch
[120] https://github.com/jacobandreas/tre
[121] https://github.com/facebookresearch/EGG/tree/main/egg/zoo/compo_vs_generalization_ood
[187] https://github.com/arski/LEW
[22] https://github.com/proroklab/adversarial_comms
[33] https://github.com/benbogin/emergence-communication-cco/
[122] https://github.com/brendon-boldt/filex-emergent-language
[123] https://github.com/brendon-boldt/filex-emergent-language
[16] https://github.com/DianeBouchacourt/SignalingGame
[188] https://github.com/facebookresearch/fruit-tools-game
[23] https://github.com/nicofirst1/rl_werewolf
[128] https://github.com/facebookresearch/EGG/blob/master/egg/zoo/channel/README.md
[118] https://github.com/facebookresearch/brica
[107] https://github.com/facebookresearch/EGG/blob/master/egg/zoo/compo_vs_generalization/RE
ADME.md
[25] https://github.com/deepmind/emergent_communication_at_scale
[117] https://github.com/mila-iqia/babyai/tree/master
[130] https://github.com/AriChow/EL
[90] https://github.com/mcogswell/evolang
[88] https://github.com/flowersteam/Imagine
[94] https://github.com/DylanCope/zero-shot-comm
[101] https://github.com/gautierdag/cultural-evolution-engine
[19] https://github.com/batra-mlp-lab/visdial-rl
[49] https://github.com/Near32/ReferentialGym
[133] https://github.com/Near32/ReferentialGym/tree/master/zoo/referential-games%2Bst-gs
[134] https://github.com/Near32/Regym/tree/develop-ETHER/benchmark/ETHER
[135] https://github.com/Near32/ReferentialGym/tree/develop/zoo/referential-games%2Bcompositional
ity%2Bdisentanglement
[137] https://github.com/facebookresearch/EGG/tree/main/egg/zoo/emcom_as_ssl
[138] https://github.com/CLMBRs/communication-translation
[95] https://github.com/blinodelka/Multiagent-Communication-Learning-in-Networks
[189] https://github.com/nyu-dl/MultimodalGame
[96] https://github.com/jacopotagliabue/On-the-plurality-of-graphs
[228] https://github.com/alshedivat/lola
[141] https://github.com/Shawn-Guo-CN/EmergentNumerals
[143] https://github.com/Shawn-Guo-CN/GameBias-EmeCom2020
[105] https://github.com/uoe-agents/Expressivity-of-Emergent-Languages
[145] https://github.com/SonuDixit/gComm
Continued on next page
41TableA1: List of code repositories for the literature reviewed. (Continued)
Paper Link
[218] https://github.com/Meta-optimization/emergent_communication_in_agents
[146] https://fringsoo.github.io/pragmatic_in2_emergent_papersite/
[148] https://github.com/facebookresearch/EGG
[149] https://github.com/facebookresearch/EGG/tree/master/egg/zoo/language_bottleneck
[106] https://github.com/facebookresearch/EGG/tree/master/egg/zoo/compositional_efficiency
[150] https://github.com/tomekkorbak/compositional-communication-via-template-transfer
[50] https://github.com/tomekkorbak/measuring-non-trivial-compositionality
[113] https://github.com/batra-mlp-lab/lang-emerge
[192] https://github.com/facebookresearch/translagent
[155] https://github.com/MediaBrain-SJTU/ECISQA
[156] https://github.com/cambridgeltl/ECNMT
[93] https://github.com/pliang279/Competitive-Emergent-Communication
[97] https://github.com/ToruOwO/marl-ae-comm
[221] https://github.com/olipinski/rl_werewolf
[157] https://anonymous.4open.science/r/TPG-916B
[29] https://github.com/facebookresearch/measuring-emergent-comm
[98] https://github.com/backpropper/s2p
[160] https://github.com/Ddaniela13/LearningToDraw
[108] https://github.com/jayelm/emergent-generalization
[34] https://github.com/mnoukhov/emergent-compete
[163] https://github.com/XeniaOhmer/hierarchical_reference_game
[164] https://github.com/XeniaOhmer/language_perception_communication_games
[193] https://github.com/saimwani/CoMON
[166] https://github.com/asappresearch/compositional-inductive-bias
[167] https://github.com/evaportelance/emergent-shape-bias
[102] https://github.com/Joshua-Ren/Neural_Iterated_Learning
[104] https://github.com/backpropper/cbc-emecom
[169] https://github.com/MathieuRita/Population
[194] https://github.com/wilrop/communication_monfg
[195] https://github.com/Homagn/MultiAgentRL
[212] https://github.com/david-simoes-93/A3C3
[213] https://github.com/david-simoes-93/A3C3
[174] https://github.com/shanest/function-words-context
[37] https://github.com/CLMBRs/communication-translation
[20] https://github.com/facebookarchive/CommNet
[177] https://github.com/mynlp/emecom_SignalingGame_as_betaVAE
[178] https://github.com/thomasaunger/babyai_sr
[179] https://github.com/i-machine-think/emergent_grammar_induction
[215] https://github.com/TonghanWang/NDQ
[224] https://github.com/jimmyyhwu/spatial-intention-maps
Continued on next page
42TableA1: List of code repositories for the literature reviewed. (Continued)
Paper Link
[183] https://github.com/wildphoton/Compositional-Generalization
[92] https://github.com/ysymyth/ec-nl
[230] https://github.com/geek-ai/Magent
Table A2 Overviewoflanguagepriorusageinthereviewedliterature.
Language prior Paper
No - Evolution 16, 18, 20, 22, 23, 25, 27, 28, 29, 30, 31, 32, 33, 34, 45, 50, 52, 54, 65, 87, 90, 91, 92, 93,
94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 107, 108, 112, 113, 116, 120, 121, 122, 123,
124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 140, 141, 142, 143, 144,
146, 147, 148, 149, 150, 151, 152, 153, 154, 156, 157, 158, 159, 160, 161, 162, 163, 164,
165, 167, 168, 169, 170, 171, 172, 173, 174, 176, 178, 179, 182, 183, 184, 186, 188, 189,
190, 193, 195, 196, 197, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212,
213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 225, 226, 227, 229
Yes - Acquistion 21, 26, 37, 88, 89, 98, 106, 115, 117, 118, 119, 134, 138, 145, 155, 166, 177, 180, 181, 191,
192, 200, 223, 224
Both 14, 15, 17, 19, 139, 175, 185
Table A3 Overviewoflanguagecharacteristicsdiscussedinthereviewedliterature.
Characteristic Paper
Morphology 14, 15, 18, 32, 49, 57, 65, 87, 91, 99, 101, 103, 104, 123, 126, 127, 128, 130, 135, 137, 149,
154, 155, 163, 173, 179, 199, 210, 211
Syntax 87, 179
Semantics 14, 15, 16, 17, 18, 19, 21, 25, 26, 28, 29, 32, 33, 35, 45, 49, 50, 57, 65, 87, 88, 89, 90, 91, 92,
93, 95, 99, 101, 102, 103, 104, 105, 106, 107, 108, 113, 120, 121, 122, 124, 125, 127, 130, 132,
133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 150, 151, 152, 153, 154, 155,
157, 159, 160, 161, 162, 163, 165, 166, 168, 169, 170, 171, 172, 173, 175, 177, 178, 180, 181,
183, 184, 188, 193, 197, 199, 202, 209, 210, 211, 214, 216, 217, 220, 226
Pragmatics 17, 29, 34, 35, 57, 90, 93, 94, 95, 97, 100, 110, 132, 162, 167, 172, 173, 186, 188, 193, 216,
220, 226
43Appendix B Additional Figures
Fig. B1 PRISMA2020flowdiagramfornewsystematicreviewsforthepresentsurvey.Adaptedfrom[47].
4414,163
15,65,99
65,99,154,211
14,18,32,99,101,104,
SVD
123,126,128,130,137,173 Perplexity
Message
A Wc ot ri dve
s
Dis nt ei sn sct-
179
65,99,128,149,155,179 MA Lv eee nsr s ga a tg g he e Redundancy 87
Ambo ir guity Sy Tn reta ex
16,57,99,202
210 AD pis pti en ac rt - Compression CGI 14,184
ances 15 25,35,45,49,50,57,87,91,
92,99,101,102,103,107,
29,35,97,132,162,216
Purity 108,120,121,127,133,135,
Causal Morphology Syntax Divergence R te ap tir oe nse an l- 140,141,142,143,145,150,
57,188 M Ee ffss ea cg te I C nn iofl cm au o te f m in ouc n-e S Aim nail la yr si it sy 1 15 62 8, ,1 15 63 9, ,1 15 74 2, ,1 16 73 7, ,1 16 85 3, ,1 16 86 4,
29,35,57,93, Instan- Topo-
taneous graphic
95,216,226 C no ao tir od ni-
Positive
Grounding lS ai rm iti y- P to
D a
ms
i
ni
s
et gei nlo
n
ten
-
-al 15 30 5, ,5 17 5, 31 ,0 17 6, 31 ,2 11 6,
6
Listening Emergent
29,35,57,93,
S C tp o ee n na s ck i yse -r
SP igo nsi at li iv ne
g
L Man eg tu ria cg se
Semantics C tio om nap lo its yi-
S tB Dy
a
mma
i ns
eg gb
e
nlno o te-f l -s 15 30 5, ,5 17 6, 31 ,0 17 6, 6121,
95,167,216,226 Pragmatics RT ecre oe n- 50,120,166
struct
Error
General-
Efficiency ization Mutual Conflict 50,152,153
Infor- Count
Easeand mation
Transfer
Learning
Sparsity Pre- Symmetry Consistency 32,57,93,95,99,108,120,
100 dictability 25,140 uESZ avhe ta ir o oo lt - n Correlation 136,145,157,163,216,217
AIn gt ee nr- t Similarity 99,101,159,162,171,181
B De ivh ea rv gi eo nra cel W DAi igt veh eni rn t --
D geiv ne cr e-
Entropy
Coherence
gence 95 65,101 29,50,99,150,
95 95 25,32,50,57,65,88,89,92, 153,178,211,226
99,102,103,105,107,108,113, 29,32,57,93,99,
121,125,133,135,139,140, 103,108,122,163
141,150,153,154,163,168,
172,175,180,183,197,209,214
Fig.B2 Graphpresentingavisualrepresentationofthemetricsidentifiedinthesurveyedliterature,sortedbylanguagecharacteristics.
Eachnodecontainsalinktothecorrespondingsectionthatdescribesthemetricindetailandalistofreferencesforeachmetricisgiven
ateachnode.
:Languagecharacteristics(innernodes)
:Individualmetrics(outermostnodes)
45References
[1] Lewis,D.K.: Convention:APhilosophicalStudy,1stedn.Harvard Univ.Press, Cambridge,Mass. (1969)
[2] Wagner, K., Reggia, J.A., Uriagereka, J., Wilkinson, G.S.: Progress in the simulation of emergent com-
munication and language. Adaptive Behavior 11(1), 37–69 (2003) https://doi.org/10.1177/105971230301
11003
[3] Steels, L.: The synthetic modeling of language origins. Evolution of Communication 1(1), 1–34 (1997)
https://doi.org/10.1075/eoc.1.1.02ste
[4] Nowak,M.A.,Krakauer,D.C.:Theevolutionoflanguage.ProceedingsoftheNationalAcademyofSciences
96(14), 8028–8033 (1999) https://doi.org/10.1073/pnas.96.14.8028
[5] Kirby, S.: Natural language from artificial life. Artificial Life 8(2), 185–215 (2002) https://doi.org/10.116
2/106454602320184248
[6] Cangelosi, A., Parisi, D.: Simulating the Evolution of Language. Springer London, London (2002). https:
//doi.org/10.1007/978-1-4471-0663-0
[7] Christiansen, M.H., Kirby, S.: Language Evolution. Oxford University Press, Oxford, England (2003).
https://doi.org/fxgmbk
[8] Batali,J.:Computationalsimulationsoftheemergenceofgrammar.In:Hurford,J.,Knight,C.,Studdert-
Kennedy, M. (eds.) Approaches to the Evolution of Language, pp. 405–426. Cambridge University Press,
Cambridge, UK (1998)
[9] Oliphant, M., Batali, J.: Learning and the emergence of coordinated communication. Center for research
on language newsletter 11(1), 1–46 (1997)
[10] Steels, L.: A self-organizing spatial vocabulary. Artificial Life 2(3), 319–332 (1995) https://doi.org/10.1
162/artl.1995.2.3.319
[11] Skyrms, B.: Signals, evolution and the explanatory power of transient information. Philosophy of Science
69(3), 407–428 (2002) https://doi.org/10.1086/342451
[12] Smith,K.,Kirby,S.,Brighton,H.:Iteratedlearning:Aframeworkfortheemergenceoflanguage.Artificial
Life 9(4), 371–386 (2003) https://doi.org/10.1162/106454603322694825
[13] Foerster, J.N., Assael, Y.M., Freitas, N.d., Whiteson, S.: Learning to communicate with deep multi-agent
reinforcementlearning.In:Lee,D.D.,Luxburg,U.,Garnett,R.,Sugiyama,M.,Guyon,I.(eds.)Advances
inNeuralInformationProcessingSystems29,pp.2145–2153.CurranAssociatesInc,RedHook,NY,USA
(2017). https://proceedings.neurips.cc/paper/2016/file/c7635bfd99248a2cdef8249ef7bfbef4-Paper.pdf
[14] Lazaridou, A., Peysakhovich, A., Baroni, M.: Multi-agent cooperation and the emergence of (natural)
language. In: OpenReview.net (ed.) 5th International Conference on Learning Representations (2017).
https://openreview.net/forum?id=Hk8N3Sclg
[15] Havrylov, S., Titov, I.: Emergence of language with multi-agent games: Learning to communicate with
sequences of symbols. In: Luxburg, U., Guyon, I., Bengio, S., Wallach, H., Fergus, R., Vishwanathan,
S.V.N.,Garnett,R.(eds.)AdvancesinNeuralInformationProcessingSystems30,pp.2146–2156.Curran
Associates Inc, Red Hook, NY, USA (2017). https://proceedings.neurips.cc/paper/2017/file/70222949cc
0db89ab32c9969754d4758-Paper.pdf
[16] Bouchacourt, D., Baroni, M.: How agents see things: On visual representations in an emergent language
game.In:AssociationforComputationalLinguistics(ed.)Proceedingsofthe2018ConferenceonEmpirical
Methods in Natural Language Processing, pp. 981–985 (2018). https://doi.org/10.18653/v1/D18-1119 .
https://aclanthology.org/D18-1119
[17] Cao, K., Lazaridou, A., Lanctot, M., Leibo, J.Z., Tuyls, K., Clark, S.: Emergent communication
throughnegotiation.In:OpenReview.net(ed.)6thInternationalConferenceonLearningRepresentations:
46Conference Track Proceedings (2018). https://openreview.net/forum?id=Hk6WhagRW
[18] Mordatch, I., Abbeel, P.: Emergence of grounded compositional language in multi-agent populations.
In: Association for the Advancement of Artificial Intelligence (ed.) Proceedings of the Thirty-Second
AAAIConferenceonArtificialIntelligenceandThirtiethInnovativeApplicationsofArtificialIntelligence
Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence, pp. 1495–
1502. AAAI Press, Palo Alto, California, USA (2018). http://arxiv.org/pdf/1703.04908v2
[19] Das, A., Kottur, S., Moura, J.M.F., Lee, S., Batra, D.: Learning cooperative visual dialog agents with
deep reinforcement learning. In: 2017 IEEE International Conference on Computer Vision (ICCV), pp.
2970–2979. IEEE, Piscataway, NJ, USA (2017). https://doi.org/10.1109/ICCV.2017.321 . http://arxiv.
org/pdf/1703.06585v2
[20] Sukhbaatar, S., Szlam, A., Fergus, R.: Learning multiagent communication with backpropagation. In:
Lee, D.D., Luxburg, U., Garnett, R., Sugiyama, M., Guyon, I. (eds.) Advances in Neural Information
Processing Systems 29, pp. 2252–2260. Curran Associates Inc, Red Hook, NY, USA (2017). https://proc
eedings.neurips.cc/paper/2016/file/55b1927fdafef39c48e5b73b5d61ea60-Paper.pdf
[21] Agarwal, A., Gurumurthy, S., Sharma, V., Lewis, M., Sycara, K.: Community regularization of visually-
grounded dialog. In: International Foundation for Autonomous Agents and Multiagent Systems (ed.)
Proceedingsofthe18thInternationalConferenceonAutonomousAgentsandMultiAgentSystems.ACM
DigitalLibrary,pp.1042–1050.InternationalFoundationforAutonomousAgentsandMultiagentSystems,
Richland, SC (2019). https://arxiv.org/abs/1808.04359
[22] Blumenkamp, J., Prorok, A.: The emergence of adversarial communication in multi-agent reinforcement
learning. In: PMLR (ed.) 4th Conference on Robot Learning. Proceedings of Machine Learning Research,
pp. 1394–1414 (2020). https://proceedings.mlr.press/v155/blumenkamp21a.html
[23] Brandizzi,N.,Grossi,D.,Iocchi,L.:Rlupus:Cooperationthroughemergentcommunicationinthewerewolf
social deduction game. In: 13th Adaptive and Learning Agents Workshop (2021). http://arxiv.org/pdf/
2106.05018v2
[24] Brandizzi, N., Iocchi, L.: Emergent communication in human-machine games. In: 5th Workshop on
Emergent Communication (2022). https://openreview.net/forum?id=rqLgeQWCXZ9
[25] Chaabouni, R., Strub, F., Altché, F., Tarassov, E., Tallec, C., Davoodi, E., Mathewson, K.W., Tieleman,
O.,Lazaridou,A.,Piot,B.:Emergentcommunicationatscale.In:OpenReview.net(ed.)10thInternational
Conference on Learning Representations (2022). https://openreview.net/forum?id=AUGBfDIV9rL
[26] Gupta, A.,Lanctot, M., Lazaridou,A.: Dynamic population-basedmeta-learning for multi-agent commu-
nication with natural language. In: M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, J. Wortman
Vaughan(eds.)AdvancesinNeuralInformationProcessingSystems34(NeurIPS2021).CurranAssociates,
Inc, Red Hook, NY, USA (2021). https://openreview.net/forum?id=NFurmj-rIWe
[27] Karten,S.,Agrawal,S.,Tucker,M.,Hughes,D.,Lewis,M.,Shah,J.,Sycara,K.:TheEnforcers:Consistent
Sparse-Discrete Methods for Constraining Informative Emergent Communication (2022). http://arxiv.or
g/pdf/2201.07452v1
[28] Lo, Y.L., Sengupta, B., Lo Long, Y.: Learning to ground decentralized multi-agent communication with
contrastive learning. In: 5th Workshop on Emergent Communication (2022). https://openreview.net/for
um?id=rLceWXWCmZc
[29] Lowe, R., Foerster, J., Boureau, Y.-L., Pineau, J., Dauphin, Y.: On the pitfalls of measuring emergent
communication. In: International Foundation for Autonomous Agents and Multiagent Systems (ed.) Pro-
ceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems. ACM
Digital Library, pp. 693–701. International Foundation for Autonomous Agents and Multiagent Systems,
Richland, SC (2019). http://arxiv.org/pdf/1903.05168v1
[30] Vanneste, S., Vanneste, A., Mets, K., Schepper, T.D., Anwar, A., Mercelis, S., Latré, S., Hellinckx, P.:
Learning to communicate using counterfactual reasoning. In: 14th Workshop on Adaptive and Learning
47Agents (2022). http://arxiv.org/pdf/2006.07200v4
[31] Verma,S.:Towardssampleefficientlearnersinpopulationbasedreferentialgamesthroughactionadvising:
Extended abstract. In: Proceedings of the 20th International Conference on Autonomous Agents and
MultiAgent Systems. AAMAS ’21. International Foundation for Autonomous Agents and Multiagent
Systems, Richland, SC (2021). https://www.ifaamas.org/Proceedings/aamas2021/pdfs/p1689.pdf
[32] Yu,D.,Mu,J.,Goodman,N.:Emergentcovertsignalinginadversarialreferencegames.In:5thWorkshop
on Emergent Communication (2022). https://openreview.net/forum?id=H-eMQbR7Z5
[33] Bogin, B., Geva, M., Berant, J.: Emergence of communication in an interactive world with consistent
speakers. In: 2nd Workshop on Emergent Communication (2018). http://arxiv.org/pdf/1809.00549v2
[34] Noukhovitch,M.,LaCroix,T.,Lazaridou,A.,Courville,A.:Emergentcommunicationundercompetition.
In: Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems.
AAMAS ’21. International Foundation for Autonomous Agents and Multiagent Systems, Richland, SC
(2021). http://arxiv.org/pdf/2101.10276v1
[35] Lazaridou, A., Baroni, M.: Emergent Multi-Agent Communication in the Deep Learning Era (2020).
http://arxiv.org/pdf/2006.02419v2
[36] Galke, L., Ram, Y., Raviv, L.: Emergent communication for understanding human language evolution:
What’s missing? In: 5th Workshop on Emergent Communication (2022). https://doi.org/paper . http:
//arxiv.org/pdf/2204.10590v1//https://openreview.net/forum?id=rqUGZQ-0XZ5
[37] Steinert-Threlkeld, S., Zhou, X., Liu, Z., Downey, C.M.: Emergent communication fine-tuning (ec-ft) for
pretrained language models. In: 5th Workshop on Emergent Communication (2022). https://openreview
.net/forum?id=SUqrM7WR7W5
[38] Bender,E.M.,Koller,A.:Climbingtowardsnlu:Onmeaning,form,andunderstandingintheageofdata.
In: Jurafsky, D., Chai, J., Schluter, N., Tetreault, J. (eds.) Proceedings of the 58th Annual Meeting of
theAssociationforComputationalLinguistics,pp.5185–5198.AssociationforComputationalLinguistics,
Stroudsburg, PA, USA (2020). https://doi.org/10.18653/v1/2020.acl-main.463
[39] Lemon,O.:Conversationalgroundinginemergentcommunication-dataanddivergence.In:5thWorkshop
on Emergent Communication (2022). https://openreview.net/forum?id=BbG-m-0Xbq
[40] Browning, J., Lecun, Y.: AI And The Limits Of Language: An artificial intelligence system trained on
words and sentences alone will never approximate human understanding., Online (23.08.2022). https:
//www.noemamag.com/ai-and-the-limits-of-language/
[41] Manning,C.D.,Schütze,H.:FoundationsofStatisticalNaturalLanguageProcessing,8.[print.]edn.MIT
Press, Cambridge, Mass. (2005)
[42] Qiu,X.,Sun,T.,Xu,Y.,Shao,Y.,Dai,N.,Huang,X.:Pre-trainedmodelsfornaturallanguageprocessing:
A survey. Science China Technological Sciences 63(10), 1872–1897 (2020) https://doi.org/10.1007/s114
31-020-1647-3
[43] Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R.,
Funtowicz,M.,Davison,J.,Shleifer,S.,Platen,P.,Ma,C.,Jernite,Y.,Plu,J.,Xu,C.,LeScao,T.,Gugger,
S., Drame, M., Lhoest, Q., Rush, A.: Transformers: State-of-the-art natural language processing. In: Liu,
Q., Schlangen, D. (eds.) Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing: System Demonstrations, pp. 38–45. Association for Computational Linguistics, Stroudsburg,
PA, USA (2020). https://doi.org/10.18653/v1/2020.emnlp-demos.6
[44] vanEecke,P.,Beuls,K.:Re-conceptualisingthelanguagegameparadigmintheframeworkofmulti-agent
reinforcement learning. In: Association for the Advancement of Artificial (ed.) COMARL AAAI 2020-
2021 - Challenges and Opportunities for Multi-Agent Reinforcement Learning, AAAI Spring Symposium
Series, Palo Alto, California / Virtual, United States (2021). http://arxiv.org/pdf/2004.04722v1
48[45] Keresztury, B., Bruni, E.: Compositional properties of emergent languages in deep learning (2020). http:
//arxiv.org/pdf/2001.08618v1
[46] Hernandez-Leal, P., Kartal, B., Taylor, M.E.: A survey and critique of multiagent deep reinforcement
learning. Autonomous Agents and Multi-Agent Systems 33(6), 750–797 (2019) https://doi.org/10.1007/
s10458-019-09421-1 1810.05587
[47] Page, M.J., McKenzie, J.E., Bossuyt, P.M., Boutron, I., Hoffmann, T.C., Mulrow, C.D., Shamseer, L.,
Tetzlaff, J.M., Akl, E.A., Brennan, S.E., Chou, R., Glanville, J., Grimshaw, J.M., Hróbjartsson, A., Lalu,
M.M.,Li,T.,Loder,E.W.,Mayo-Wilson,E.,McDonald,S.,McGuinness,L.A.,Stewart,L.A.,Thomas,J.,
Tricco, A.C., Welch, V.A., Whiting, P., Moher, D.: The prisma 2020 statement: an updated guideline for
reportingsystematicreviews.BMJ(Clinicalresearched.)372,71(2021)https://doi.org/10.1136/bmj.n71
[48] Lipowska, D., Lipowski, A.: Emergence and evolution of language in multi-agent systems. Lingua 272,
103331 (2022) https://doi.org/10.1016/j.lingua.2022.103331
[49] Denamganaï, K., Walker, J.A.: Referentialgym: A nomenclature and framework for language emergence
& grounding in (visual) referential games. In: 4th Workshop on Emergent Communication (2020). http:
//arxiv.org/pdf/2012.09486v1
[50] Korbak, T., Zubek, J., Rączaszek-Leonardi, J.: Measuring non-trivial compositionality in emergent com-
munication. In: 4th Workshop on Emergent Communication (2020). http://arxiv.org/pdf/2010.15058
v2
[51] LaCroix, T.: Biology and compositionality: Empirical considerations for emergent-communication pro-
tocols. In: 3rd Workshop on Emergent Communication (2019). http://arxiv.org/pdf/1911.11668
v2
[52] Mihai, D., Hare, J.: The emergence of visual semantics through communication games (2021). http:
//arxiv.org/pdf/2101.10253v1
[53] Galke, L., Raviv, L.: Emergent communication and learning pressures in language models: a language
evolution perspective. http://arxiv.org/pdf/2403.14427v1
[54] Vanneste, A., Vanneste, S., Mets, K., Schepper, T.D., Mercelis, S., Latré, S., Hellinckx, P.: An analysis
of discretization methods for communication learning with multi-agent reinforcement learning. In: 14th
Workshop on Adaptive and Learning Agents (2022). http://arxiv.org/pdf/2204.05669v1
[55] Moulin-Frier, C., Oudeyer, P.-Y.: Multi-agent reinforcement learning as a computational tool for lan-
guage evolution research: Historical context and future challenges. In: Association for the Advancement
of Artificial (ed.) COMARL AAAI 2020-2021 - Challenges and Opportunities for Multi-Agent Reinforce-
ment Learning, AAAI Spring Symposium Series, Palo Alto, California / Virtual, United States (2021).
http://arxiv.org/pdf/2002.08878v2
[56] Fernando, C., Zenkova, D., Nikolov, S., Osindero, S.: From Language Games to Drawing Games (2020).
http://arxiv.org/pdf/2010.02820v2
[57] Brandizzi,N.:Towardmorehuman-likeaicommunication:Areviewofemergentcommunicationresearch.
IEEE Access 11, 142317–142340 (2023) https://doi.org/10.1109/ACCESS.2023.3339656 2308.02541
[58] Carston, R.: The explicit/implicit distinction in pragmatics and the limits of explicit communication.
International Review of Pragmatics 1(1), 35–62 (2009) https://doi.org/10.1163/187731009X455839
[59] Watzlawick, P., Bavelas, J.B., Jackson, D.D.: Pragmatics of Human Communication: A Study of
Interactional Patterns, Pathologies, and Paradoxes. Norton, New York (1967)
[60] Andersen, P.A.: When one cannot not communicate: A challenge to motley’s traditional communication
postulates. Communication Studies 42(4), 309–325 (1991) https://doi.org/10.1080/10510979109368346
[61] Antos, G., Ventola, E., Weber, T.: Handbook of Interpersonal Communication. De Gruyter Mouton,
Berlin, Germany (2008). https://doi.org/10.1515/9783110211399
49[62] Witt, P.: Communication and Learning. De Gruyter Mouton, Berlin, Germany (2016). https://doi.org/
10.1515/9781501502446
[63] Hartley, P.: Interpersonal Communication, 1. publ edn. Routledge, London, England (1993)
[64] Jones, R.G.: Communication in the Real World, Version 2.0 edn. Flat World Knowledge, Irvington, NY
(2018). https://catalog.flatworldknowledge.com/catalog/editions/jones_2-communication-in-the-real-w
orld-2-0
[65] Choi, E., Lazaridou, A., Freitas, N.d.: Compositional obverter communication learning from raw visual
input. In: OpenReview.net (ed.) 6th International Conference on Learning Representations: Conference
Track Proceedings (2018). http://arxiv.org/pdf/1804.02341v1
[66] Austin,J.L.:HowtodoThingswithWords:TheWilliamJamesLecturesDeliveredatHarvardUniversity
in 1955, 2. ed. edn. Clarendon Press, Oxford (1975)
[67] Clark, H.H.: Using Language, 1. publ edn. Cambridge Univ. Press, Cambridge (1996)
[68] Wittgenstein, L.: Philosophical Investigations, 3nd ed., repr edn. Blackwell, Cambridge, Mass. (1989)
[69] Adler, R.B.: Interplay: The Process of Interpersonal Communication, 3rd canadian ed. edn. Oxford
University Press, Don Mills, Ont. (2012)
[70] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P.,
Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A.,
Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark,
J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are few-shot
learners. In: Neural Information Processing Systems Foundation (ed.) Advances in Neural Information
ProcessingSystems33.Advancesinneuralinformationprocessingsystems,vol.33,pp.1877–1901.Curran
Associates Inc, Red Hook, NY, USA (2020). https://proceedings.neurips.cc/paper/2020/file/1457c0d6bf
cb4967418bfb8ac142f64a-Paper.pdf
[71] Lauriola,I.,Lavelli,A.,Aiolli,F.:Anintroductiontodeeplearninginnaturallanguageprocessing:Models,
techniques, and tools. NEUROCOMPUTING 470, 443–456 (2022) https://doi.org/10.1016/j.neucom.2
021.05.103
[72] Khurana,D.,Koli,A.,Khatter,K.,Singh,S.:Naturallanguageprocessing:stateoftheart,currenttrends
and challenges. Multimedia tools and applications, 1–32 (2022) https://doi.org/10.1007/s11042-022-134
28-4
[73] Lazaridou, A., Kuncoro, A., Gribovskaya, E., Agrawal, D., Liska, A., Terzi, T., Gimenez, M., Masson
d\textquotesingle Autume, C., Kocisky, T., Ruder, S., Yogatama, D., Cao, K., Young, S., Blunsom, P.:
Mind the gap: Assessing temporal generalization in neural language models. In: M. Ranzato, A. Beygelz-
imer, Y. Dauphin, P.S. Liang, J. Wortman Vaughan (eds.) Advances in Neural Information Processing
Systems34(NeurIPS2021),vol.34,pp.29348–29363.CurranAssociates,Inc,RedHook,NY,USA(2021).
https://proceedings.neurips.cc/paper/2021/file/f5bf0ba0a17ef18f9607774722f5698c-Paper.pdf
[74] Merrill, W., Goldberg, Y., Schwartz, R., Smith, N.A.: Provable limitations of acquiring meaning from
ungrounded form: What will future language models understand? Transactions of the Association for
Computational Linguistics 9, 1047–1060 (2021) https://doi.org/10/jqph
[75] Grupen, N.A., Lee, D.D., Selman, B.: Multi-agent curricula and emergent implicit signaling. In: Proceed-
ingsofthe21stInternationalConferenceonAutonomousAgentsandMultiagentSystems(AAMAS2022)
(2022). http://arxiv.org/pdf/2106.11156v3
[76] Dor, D., Knight, C., Lewis, J.: The Social Origins of Language, 1. ed. edn. Oxford linguistics, vol. 19.
Oxford Univ. Press, Oxford (2014). https://doi.org/10.1093/acprof:oso/9780199665327.001.0001
[77] Pinker, S., Bloom, P.: Natural language and natural selection. Behavioral and Brain Sciences 13(4),
707–727 (1990) https://doi.org/10.1017/S0140525X00081061
50[78] Hauser, M.D., Yang, C., Berwick, R.C., Tattersall, I., Ryan, M.J., Watumull, J., Chomsky, N., Lewontin,
R.C.: The mystery of language evolution. Frontiers in psychology 5, 401 (2014) https://doi.org/10.3389/
fpsyg.2014.00401
[79] Hock, H.H., Joseph, B.D.: Language History, Language Change, and Language Relationship. De Gruyter
Mouton, Berlin, Germany (2019). https://doi.org/10.1515/9783110613285
[80] Chomsky, N.: Knowledge of Language: Its Nature, Origin, and Use. Convergence. Praeger, New York
(1986). http://www.loc.gov/catdir/enhancements/fy1511/85012234-b.html
[81] Ney, J.W.: Knowledge of language: Its nature, origin, and use. Language Sciences 11(4), 409–423 (1989)
https://doi.org/10.1016/0388-0001(89)90029-6
[82] Locke, J.L.: A theory of neurolinguistic development. Brain and language 58(2), 265–326 (1997) https:
//doi.org/10.1006/brln.1997.1791
[83] Bleys, J.: Language strategies for the domain of colour. Language Science Press. https://doi.org/10.171
69/langsci.b51.104
[84] Thomas,J.J.,Cook,K.A.(eds.):IlluminatingthePath:TheResearchandDevelopmentAgendaforVisual
Analytics. IEEE Computer Soc, Los Alamitos, Calif. (2005)
[85] Chandler, D.: Semiotics: The Basics. Routledge, London, England (2007). https://doi.org/10.4324/9780
203014936
[86] Brinton, L.J., Brinton, D.M.: The Linguistic Structure of Modern English. John Benjamins Publishing
Company, Amsterdam (2010). https://doi.org/10.1075/z.156
[87] Ueda, R., Ishii, T., Washio, K., Miyao, Y.: Categorial grammar induction as a compositionality measure
for emergent languages in signaling games. In: 5th Workshop on Emergent Communication (2022). https:
//openreview.net/forum?id=Sbgb7b0Q-5
[88] Colas,C.,Karch,T.,Lair,N.,Dussoux,J.-M.,Moulin-Frier,C.,Dominey,P.F.,Oudeyer,P.-Y.:Language
as a cognitive tool to imagine goals in curiosity-driven exploration. In: Neural Information Processing
Systems Foundation (ed.) Advances in Neural Information Processing Systems 33. Advances in neural
information processing systems. Curran Associates Inc, Red Hook, NY, USA (2020). http://arxiv.org/pd
f/2002.09253v4
[89] Qiu,S.,Xie,S.,Fan,L.,Gao,T.,Zhu,S.-C.,Zhu,Y.:Emergentgraphicalconventionsinavisualcommuni-
cationgame.In:NeuralInformationProcessingSystemsFoundation(ed.)AdvancesinNeuralInformation
Processing Systems 35. Advances in neural information processing systems. Curran Associates Inc, Red
Hook, NY, USA (2022). http://arxiv.org/pdf/2111.14210v2
[90] Cogswell, M., Lu, J., Lee, S., Parikh, D., Batra, D.: Emergence of Compositional Language with Deep
Generational Transmission (2019). http://arxiv.org/pdf/1904.09067v2
[91] Li, F., Bowling, M.: Ease-of-teaching and language structure from emergent communication. In: Neural
InformationProcessingSystemsFoundation(ed.)AdvancesinNeuralInformationProcessingSystems32.
Advances in neural information processing systems. Curran Associates Inc, Red Hook, NY, USA (2019).
https://papers.nips.cc/paper/2019/file/b0cf188d74589db9b23d5d277238a929-Paper.pdf
[92] Yao, S., Yu, M., Zhang, Y., Narasimhan, K., Tenenbaum, J.B., Gan, C.: Linking emergent and natu-
ral languages via corpus transfer. In: OpenReview.net (ed.) 10th International Conference on Learning
Representations (2022). https://openreview.net/forum?id=49A1Y6tRhaq
[93] Liang, P.P., Chen, J., Salakhutdinov, R., Morency, L.-P., Kottur, S.: On emergent communication in
competitive multi-agent teams. In: Proceedings of the 19th International Conference on Autonomous
Agents and MultiAgent Systems. AAMAS ’20, pp. 735–743. International Foundation for Autonomous
Agents and Multiagent Systems, Richland, SC (2020). http://arxiv.org/pdf/2003.01848v2
[94] Cope, D., Schoots, N.: Learning to communicate with strangers via channel randomisation methods. In:
514th Workshop on Emergent Communication (2020). http://arxiv.org/pdf/2104.09557v1
[95] Dubova, M., Moskvichev, A., Goldstone, R.: Reinforcement communication learning in different social
network structures. In: 1st Workshop on Language in Reinforcement Learning (2020). http://arxiv.org/
pdf/2007.09820v1
[96] Fitzgerald, N., Tagliabue, J.: On the plurality of graphs. In: NETREASON @ ECAI 2020 (2020). http:
//arxiv.org/pdf/2008.00920v1
[97] Lin, T., Huh, M., Stauffer, C., Lim, S.-N., Isola, P.: Learning to ground multi-agent communication with
autoencoders. In: Neural Information Processing Systems Foundation (ed.) Advances in Neural Informa-
tionProcessingSystems34.Advancesinneuralinformationprocessingsystems,pp.15230–15242.Curran
Associates Inc, Red Hook, NY, USA (2021). https://papers.nips.cc/paper/2021/file/80fee67c8a4c4989bf
8a580b4bbb0cd2-Paper.pdf
[98] Lowe, R., Gupta, A., Foerster, J., Kiela, D., Pineau, J.: On the interaction between supervision and self-
play in emergent communication. In: OpenReview.net (ed.) 8th International Conference on Learning
Representations (2020). http://arxiv.org/pdf/2002.01093v2
[99] Luna, D.R., Ponti, E.M., Hupkes, D., Bruni, E.: Internal and external pressures on language emergence:
least effort, object constancy and frequency. In: Webber, B., Cohn, T., He, Y., Liu, Y. (eds.) Proceedings
ofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP).Associationfor
Computational Linguistics, Stroudsburg, PA, USA (2020). http://arxiv.org/pdf/2004.03868v3
[100] Kalinowska, A., Davoodi, E., Strub, F., Mathewson, K., Murphey, T., Pilarski, P.: Situated commu-
nication: A solution to over-communication between artificial agents. In: 5th Workshop on Emergent
Communication (2022). https://openreview.net/forum?id=HLqzzQWA7Z9
[101] Dagan,G.,Hupkes,D.,Bruni,E.:Co-evolutionoflanguageandagentsinreferentialgames.In:Association
for Computational Linguistics (ed.) Proceedings of the 16th Conference of the European Chapter of the
Association for Computational Linguistics: Main Volume (2021). http://arxiv.org/pdf/2001.03361v3
[102] Ren, Y., Guo, S., Labeau, M., Cohen, S.B., Kirby, S.: Compositional languages emerge in a neural iter-
atedlearning model. In:OpenReview.net(ed.) 8thInternational ConferenceonLearning Representations
(2020). http://arxiv.org/pdf/2002.01365v2
[103] Rita,M.,Strub,F.,Grill,J.-B.,Pietquin,O.,Dupoux,E.:Ontheroleofpopulationheterogeneityinemer-
gentcommunication.In:OpenReview.net(ed.)10thInternationalConferenceonLearningRepresentations
(2022). http://arxiv.org/pdf/2204.12982v1
[104] Resnick, C., Gupta, A., Foerster, J., Dai, A.M., Cho, K.: Capacity, bandwidth, and compositionality in
emergentlanguage learning.In:Proceedingsofthe 19th InternationalConferenceon AutonomousAgents
and MultiAgent Systems. AAMAS ’20, pp. 1125–1133. International Foundation for Autonomous Agents
and Multiagent Systems, Richland, SC (2020). http://arxiv.org/pdf/1910.11424v3
[105] Guo,S.,Ren,Y.,Mathewson,K.,Kirby,S.,Albrecht,S.V.,Smith,K.:Expressivityofemergentlanguage
is a trade-off between contextual complexity and unpredictability. In: OpenReview.net (ed.) 10th Inter-
national Conference on Learning Representations (2022). https://openreview.net/forum?id=WxuE_J
WxjkW
[106] Kharitonov, E., Baroni, M.: Emergent language generalization and acquisition speed are not tied to com-
positionality.In:ProceedingsoftheThirdBlackboxNLPWorkshoponAnalyzingandInterpretingNeural
Networks for NLP, pp. 11–15 (2020). http://arxiv.org/pdf/2004.03420v2
[107] Chaabouni, R., Kharitonov, E., Bouchacourt, D., Dupoux, E., Baroni, M.: Compositionality and gen-
eralization in emergent languages. In: Association for Computational Linguistics (ed.) Proceedings of
the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4427–4442 (2020).
http://arxiv.org/pdf/2004.09124v1
[108] Mu, J., Goodman, N.: Emergent communication of generalizations. In: Neural Information Processing
52Systems Foundation (ed.) Advances in Neural Information Processing Systems 34. Advances in neural
information processing systems, pp. 17994–18007. Curran Associates Inc, Red Hook, NY, USA (2021).
https://papers.nips.cc/paper/2021/file/9597353e41e6957b5e7aa79214fcb256-Paper.pdf
[109] Suglia, A., Konstas, I., Lemon, O.: Visually grounded language learning: A review of language games,
datasets, tasks, and models. Journal of Artificial Intelligence Research 79, 173–239 (2024) https://doi.or
g/10.1613/jair.1.15185
[110] Zhu,C.,Dastani,M.,Wang,S.:Asurveyofmulti-agentdeepreinforcementlearningwithcommunication.
In: Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems.
AAMAS ’24, pp. 2845–2847. International Foundation for Autonomous Agents and Multiagent Systems,
Richland, SC (2024). https://doi.org/10.1007/s10458-023-09633-6
[111] Boldt, B., Mortensen, D.: Recommendations for Systematic Research on Emergent Language (2022).
http://arxiv.org/pdf/2206.11302v1
[112] Chen, R., Guo, S.: Emergent Semantic Communications for Mobile Augmented Reality: Basic Ideas and
Opportunities. http://arxiv.org/pdf/2308.07342v1
[113] Kottur, S., Moura, J., Lee, S., Batra, D.: Natural language does not emerge ‘naturally’ in multi-agent
dialog.In:Palmer,M.,Hwa,R.,Riedel,S.(eds.)Proceedingsofthe2017ConferenceonEmpiricalMethods
in Natural Language Processing, pp. 2962–2967. Association for Computational Linguistics, Stroudsburg,
PA, USA (2017). https://doi.org/10.18653/v1/D17-1321
[114] Perkins, H.: TexRel: a Green Family of Datasets for Emergent Communications on Relations. http://arxi
v.org/pdf/2105.12804v1
[115] Buck, C., Bulian, J., Ciaramita, M., Gajewski, W., Gesmundo, A., Houlsby, N., Wang, W.: Analyzing
language learned by an active question answering agent. In: 1st Workshop on Emergent Communication
(2017). http://arxiv.org/pdf/1801.07537v1
[116] Harding Graesser, L., Cho, K., Kiela, D.: Emergent linguistic phenomena in multi-agent communication
games. In: Inui, K., Jiang, J., Ng, V., Wan, X. (eds.) Proceedings of the 2019 Conference on Empirical
MethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguage
Processing (EMNLP-IJCNLP), pp. 3698–3708. Association for Computational Linguistics, Stroudsburg,
PA, USA (2019). https://doi.org/10.18653/v1/D19-1384
[117] Chevalier-Boisvert, M., Bahdanau, D., Lahlou, S., Willems, L., Saharia, C., Nguyen, T.H., Bengio, Y.:
Babyai:Aplatformtostudythesampleefficiencyofgroundedlanguagelearning.In:OpenReview.net(ed.)
7th International Conference on Learning Representations (2019). http://arxiv.org/pdf/1810.08272v4
[118] Chaabouni, R., Kharitonov, E., Lazaric, A., Dupoux, E., Baroni, M.: Word-order biases in deep-agent
emergent communication. In: Association for Computational Linguistics (ed.) Proceedings of the 57th
Annual Meeting of the Association for Computational Linguistics, pp. 5166–5175 (2019). http://arxiv.or
g/pdf/1905.12330v3
[119] Woodward, M., Finn, C., Hausman, K.: Learning to interactively learn and assist. In: Proceedings of the
34th AAAI Conference on Artificial Intelligence (AAAI-20), pp. 2535–2543 (2020). https://doi.org/10.1
609/aaai.v34i03.5636 . http://arxiv.org/pdf/1906.10187v3
[120] Andreas, J.: Measuring compositionality in representation learning. In: OpenReview.net (ed.) 7th Inter-
national Conference on Learning Representations (2019). https://openreview.net/forum?id=HJz05o0q
K7
[121] Auersperger, M., Pecina, P.: Defending compositionality in emergent languages. In: Ippolito, D., Li, L.H.,
Pacheco,M.L.,Chen,D.,Xue,N.(eds.)Proceedingsofthe2022ConferenceoftheNorthAmericanChap-
ter of the Association for Computational Linguistics: Human Language Technologies: Student Research
Workshop,pp.285–291.AssociationforComputationalLinguistics,Hybrid:Seattle,Washington+Online
(2022). https://doi.org/10.18653/v1/2022.naacl-srw.35 . http://arxiv.org/pdf/2206.04751v1
53[122] Boldt, B., Mortensen, D.: Modeling emergent lexicon formation with a self-reinforcing stochastic process.
In: 5th Workshop on Emergent Communication (2022). https://openreview.net/forum?id=BdbVlXbR
Xbq
[123] Boldt, B., Mortensen, D.: Mathematically Modeling the Lexicon Entropy of Emergent Language. http:
//arxiv.org/pdf/2211.15783v2
[124] Bosc, T.: Varying meaning complexity to explain and measure compositionality. In: 5th Workshop on
Emergent Communication (2022). https://openreview.net/forum?id=BnGzfmZ07bq
[125] Bullard, K., Kiela, D., Pineau, J., Foerster, J.: Quasi-Equivalence Discovery for Zero-Shot Emergent
Communication (2021). http://arxiv.org/pdf/2103.08067v1
[126] Carmeli, B., Meir, R., Belinkov, Y.: Emergent quantized communication. In: Williams, B.K., Chen, Y.,
Neville, J. (eds.) Thirty-Seventh AAAI Conference on Artificial Intelligence [and] Thirty-Fifth Confer-
ence on Innovative Applications of Artificial Intelligence [and] Thirteenth Symposium on Educational
Advances in Artificial Intelligence, February 714, 2023, Washington DC, USA. Proceedings of the AAAI
Conference on Artificial Intelligence, vol. 37, pp. 11533–11541. Association for the Advancement of
Artificial Intelligence = AAAI, Palo Alto (2023). https://doi.org/10.1609/aaai.v37i10.26363 .
http://arxiv.org/pdf/2211.02412v2
[127] Carmeli, B., Belinkov, Y., Meir, R.: Concept-Best-Matching: Evaluating Compositionality in Emergent
Communication. http://arxiv.org/pdf/2403.14705v1
[128] Chaabouni, R., Kharitonov, E., Dupoux, E., Baroni, M.: Anti-efficient encoding in emergent commu-
nication. In: Neural Information Processing Systems Foundation (ed.) Advances in Neural Information
Processing Systems 32. Advances in neural information processing systems. Curran Associates Inc, Red
Hook, NY, USA (2019). http://arxiv.org/pdf/1905.12561v4
[129] Chowdhury, A., Santamaria-Pang, A., Kubricht, J.R., Qiu, J., Tu, P.: Symbolic Semantic Segmentation
and Interpretation of COVID-19 Lung Infections in Chest CT volumes based on Emergent Languages
(2020). http://arxiv.org/pdf/2008.09866v1
[130] Chowdhury, A., Santamaria-Pang, A., Kubricht, J.R., Tu, P.: Emergent symbolic language based deep
medicalimageclassification.In:2021IEEE18thInternationalSymposiumonBiomedicalImaging(ISBI),
pp. 689–692. IEEE, Piscataway, NJ, USA (2021). https://doi.org/10.1109/ISBI48211.2021.9434073 .
http://arxiv.org/pdf/2008.09860v1
[131] Chowdhury,A.,Kubricht,J.R.,Sood,A.,Tu,P.,Santamaria-Pang,A.:Escell:Emergentsymboliccellular
language. In: 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI), pp. 1604–1607.
IEEE, Piscataway, NJ, USA (2020). https://doi.org/10.1109/ISBI45749.2020.9098343
[132] Cowen-Rivers, A.I., Naradowsky, J.: Emergent communication with world models. In: 3rd Workshop on
Emergent Communication (2019). http://arxiv.org/pdf/2002.09604v1
[133] Denamganaï, K., Walker, J.A.: On (emergent) systematic generalisation and compositionality in visual
referential games with straight-through gumbel-softmax estimator. In: 4th Workshop on Emergent
Communication (2020). http://arxiv.org/pdf/2012.10776v1
[134] Denamganaï, K., Hernandez, D., Vardal, O., Missaoui, S., Walker, J.A.: ETHER: Aligning Emergent
Communication for Hindsight Experience Replay. http://arxiv.org/pdf/2307.15494v2
[135] Denamganaï, K., Missaoui, S., Walker, J.A.: Visual Referential Games Further the Emergence of
Disentangled Representations. http://arxiv.org/pdf/2304.14511v1
[136] Dessì, R., Bouchacourt, D., Crepaldi, D., Baroni, M.: Focus on what’s informative and ignore what’s not:
Communication strategies in a referential game. In: 3rd Workshop on Emergent Communication (2019).
http://arxiv.org/pdf/1911.01892v1
[137] Dessì, R., Kharitonov, E., Baroni, M.: Interpretable agent communication from scratch (with a generic
54visual processor emerging on the side). In: Neural Information Processing Systems Foundation (ed.)
Advances in Neural Information Processing Systems 34. Advances in neural information processing sys-
tems, pp. 26937–26949. Curran Associates Inc, Red Hook, NY, USA (2021). https://papers.nips.cc/pap
er/2021/file/e250c59336b505ed411d455abaa30b4d-Paper.pdf
[138] Downey,C.M.,Zhou,X.,Liu,Z.,Steinert-Threlkeld,S.:Learningtotranslatebylearningtocommunicate.
In: Ataman, D. (ed.) Proceedings of the 3rd Workshop on Multi-lingual Representation Learning (MRL),
pp. 218–238. Association for Computational Linguistics, Singapore (2023). https://aclanthology.org/202
3.mrl-1.17
[139] Eloff, K., Pretorius, A., Räsänen, O., Engelbrecht, H.A., Kamper, H.: Towards Learning to Speak and
Hear Through Multi-Agent Communication over a Continuous Acoustic Channel (2021). http://arxiv.or
g/pdf/2111.02827v2
[140] Feng, Y., An, B., Lu, Z.: Learning multi-object positional relationships via emergent communication.
Proceedings of the AAAI Conference on Artificial Intelligence 38(16), 17371–17379 (2024) https://doi.or
g/10.1609/aaai.v38i16.29685 2302.08084v1
[141] Guo, S.: Emergence of numeric concepts in multi-agent autonomous communication. Master thesis,
University of Edinburgh, Edinburgh, Scotland (2019-11-04). http://arxiv.org/pdf/1911.01098v1
[142] Guo, S., Ren, Y., Havrylov, S., Frank, S., Titov, I., Smith, K.: The emergence of compositional languages
for numeric concepts through iterated learning in neural agents. In: Proceedings of the 13th International
Conference on Evolution in Language (EvoLang XIII) (2020). http://arxiv.org/pdf/1910.05291v1
[143] Guo, S., Ren, Y., Słowik, A., Mathewson, K.: Inductive bias and language expressivity in emergent com-
munication. In: 4th Workshop on Emergent Communication (2020). http://arxiv.org/pdf/2012.02875
v1
[144] Hagiwara, Y., Furukawa, K., Taniguchi, A., Taniguchi, T.: Multiagent multimodal categorization for
symbol emergence: emergent communication via interpersonal cross-modal inference. Advanced Robotics
36(5-6), 239–260 (2022) https://doi.org/10.1080/01691864.2022.2029721 2109.07194v1
[145] Hazra, R., Dixit, S., Sen, S.: Infinite use of finite means: Zero-Shot Generalization using Compositional
Emergent Protocols (2020). http://arxiv.org/pdf/2012.05011v2
[146] Kang, Y., Wang, T., Melo, G.d.: Incorporating pragmatic reasoning communication into emergent lan-
guage. In: Neural Information Processing Systems Foundation (ed.) Advances in Neural Information
Processing Systems 33. Advances in neural information processing systems. Curran Associates Inc, Red
Hook, NY, USA (2020). http://arxiv.org/pdf/2006.04109v2
[147] Karten, S., Kailas, S., Li, H., Sycara, K.: On the Role of Emergent Communication for Social Learning in
Multi-Agent Reinforcement Learning. http://arxiv.org/pdf/2302.14276v1
[148] Kharitonov, E., Chaabouni, R., Bouchacourt, D., Baroni, M.: Egg: a toolkit for research on emergence of
languageingames.In:AssociationforComputationalLinguistics(ed.)Proceedingsofthe2019Conference
on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on
NaturalLanguageProcessing(EMNLP-IJCNLP):SystemDemonstrations,HongKong,China,pp.55–60
(2019). https://doi.org/10.18653/v1/D19-3010 . http://arxiv.org/pdf/1907.00852v2
[149] Kharitonov,E.,Chaabouni,R.,Bouchacourt,D.,Baroni,M.:Entropyminimizationinemergentlanguages.
In: Proceedings of the 37th International Conference on Machine Learning (ICML 2020) (2020). http:
//arxiv.org/pdf/1905.13687v3
[150] Korbak, T., Zubek, J., Kuciński, Ł., Miłoś, P., Rączaszek-Leonardi, J.: Developmentally motivated
emergence of compositional communication via template transfer. In: 3rd Workshop on Emergent
Communication (2019). http://arxiv.org/pdf/1910.06079v1
[151] Kubricht, J.R., Yang, Z., Qiu, J., Tu, P.H.: Grounded Language Acquisition From Object and Action
Imagery. http://arxiv.org/pdf/2309.06335v1
55[152] Kuciński, Ł., Kołodziej, P., Miłoś, P.: Emergence of compositional language in communication through
noisy channel. In: 1st Workshop on Language in Reinforcement Learning (2020). https://openreview.net
/forum?id=ZbXlSL_xwtA
[153] Kuciński, Ł., Korbak, T., Kołodziej, P., Miłoś, P.: Catalytic role of noise and necessity of inductive biases
intheemergenceofcompositionalcommunication.In:NeuralInformationProcessingSystemsFoundation
(ed.) Advances in Neural Information Processing Systems 34. Advances in neural information processing
systems, pp. 23075–23088. Curran Associates Inc, Red Hook, NY, USA (2021). https://papers.nips.cc/p
aper/2021/file/c2839bed26321da8b466c80a032e4714-Paper.pdf
[154] Lazaridou, A., Hermann, K.M., Tuyls, K., Clark, S.: Emergence of linguistic communication from refer-
ential games with symbolic and pixel input. In: OpenReview.net (ed.) 6th International Conference on
Learning Representations: Conference Track Proceedings (2018). http://arxiv.org/pdf/1804.03984v1
[155] Lei,Z.,Zhang,Y.,Xiong,Y.,Chen,S.:Emergentcommunicationininteractivesketchquestionanswering.
In: Neural Information Processing Systems Foundation (ed.) Advances in Neural Information Processing
Systems 36. Advances in neural information processing systems (2023). http://arxiv.org/pdf/2310.15597
v1
[156] Li, Y., Ponti, E.M., Vulić, I., Korhonen, A.: Emergent communication pretraining for few-shot machine
translation. In: International Committee on Computational Linguistics (ed.) Proceedings of the 28th
International Conference on Computational Linguistics, Barcelona, Spain (2020). http://arxiv.org/pdf/
2011.00890v1
[157] Lipinski, O., Sobey, A.J., Cerutti, F., Norman, T.J.: Speaking Your Language: Spatial Relationships in
Interpretable Emergent Communication. http://arxiv.org/pdf/2406.07277v1
[158] Lobos-Tsunekawa, K., Srinivasan, A., Spranger, M.: MA-Dreamer: Coordination and communication
through shared imagination (2022). http://arxiv.org/pdf/2204.04687v1
[159] Mihai, D., Hare, J.: Avoiding hashing and encouraging visual semantics in referential emergent language
games. In: 3rd Workshop on Emergent Communication (2019). http://arxiv.org/pdf/1911.05546v1
[160] Mihai,D.,Hare,J.:Learningtodraw:Emergentcommunicationthroughsketching.In:NeuralInformation
Processing Systems Foundation (ed.) Advances in Neural Information Processing Systems 34. Advances
in neural information processing systems, pp. 7153–7166. Curran Associates Inc, Red Hook, NY, USA
(2021). https://papers.nips.cc/paper/2021/file/39d0a8908fbe6c18039ea8227f827023-Paper.pdf
[161] Mu,Y.,Yao,S.,Ding,M.,Luo,P.,Gan,C.:Ec2:Emergentcommunicationforembodiedcontrol.In:2023
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6704–6714. IEEE,
Piscataway, NJ, USA (2023). https://doi.org/10.1109/CVPR52729.2023.00648
[162] Mul, M., Bouchacourt, D., Bruni, E.: Mastering emergent language: learning to guide in simulated
navigation (2019). http://arxiv.org/pdf/1908.05135v1
[163] Ohmer, X., Duda, M., Bruni, E.: Emergence of hierarchical reference systems in multi-agent communica-
tion.In:Calzolari,N.,Huang,C.-R.,Kim,H.,Pustejovsky,J.,Wanner,L.,Choi,K.-S.,Ryu,P.-M.,Chen,
H.-H., Donatelli, L., Ji, H., Kurohashi, S., Paggio, P., Xue, N., Kim, S., Hahm, Y., He, Z., Lee, T.K.,
Santus,E.,Bond,F.,Na,S.-H.(eds.)Proceedingsofthe29thInternationalConferenceonComputational
Linguistics. International Committee on Computational Linguistics, Gyeongju, Republic of Korea (2022).
http://arxiv.org/pdf/2203.13176v2
[164] Ohmer, X., Marino, M., Franke, M., König, P.: Mutual influence between language and perception in
multi-agent communication games. PLoS computational biology 18(10), 1010658 (2022) https://doi.org/
10.1371/journal.pcbi.1010658
[165] Ossenkopf, M., Luck, K.S., Mathewson, K.W.: Which language evolves between heterogeneous agents? -
communicating movement instructions with widely different time scopes. In: 5th Workshop on Emergent
Communication (2022). https://openreview.net/forum?id=BnfgM7-0mW5
56[166] Perkins, H.: Neural networks can understand compositional functions that humans do not, in the context
of emergent communication (2021). http://arxiv.org/pdf/2103.04180v1
[167] Portelance, E., Frank, M.C., Jurafsky, D., Sordoni, A., Laroche, R.: The emergence of the shape bias
resultsfromcommunicativeefficiency.In:Bisazza,A.,Abend,O.(eds.)Proceedingsofthe25thConference
onComputationalNaturalLanguageLearning.AssociationforComputationalLinguistics,Online(2021).
http://arxiv.org/pdf/2109.06232v2
[168] Ri, R., Ueda, R., Naradowsky, J.: Emergent communication with attention. In: Goldwater, M., Anggoro,
F.K., Hayes, B.K., Ong, D.C. (eds.) Proceedings of the Annual Meeting of the Cognitive Science Society,
(2023). http://arxiv.org/pdf/2305.10920v1
[169] Rita,M.,Tallec,C.,Michel,P.,Grill,J.-B.,Pietquin,O.,Dupoux,E.,Strub,F.:Emergentcommunication:
Generalization and overfitting in lewis games. In: Neural Information Processing Systems Foundation
(ed.) Advances in Neural Information Processing Systems 35. Advances in neural information processing
systems. Curran Associates Inc, Red Hook, NY, USA (2022). http://arxiv.org/pdf/2209.15342v2
[170] Santamaria-Pang, A., Kubricht, J.R., Devaraj, C., Chowdhury, A., Tu, P.: Towards semantic action anal-
ysis via emergent language. In: 2019 IEEE International Conference on Artificial Intelligence and Virtual
Reality (AIVR), pp. 224–2244. IEEE, Piscataway, NJ, USA (2019). https://doi.org/10.1109/AIVR46125.
2019.00047
[171] Santamaria-Pang, A., Kubricht, J.R., Chowdhury, A., Bhushan, C., Tu, P.: Towards emergent language
symbolicsemanticsegmentationandmodelinterpretability.In:Martel,A.L.,Abolmaesumi,P.,Stoyanov,
D., Mateus, D., Zuluaga, M.A., Zhou, S.K. (eds.) Medical Image Computing and Computer Assisted
Intervention – MICCAI 2020. Image Processing, Computer Vision, Pattern Recognition, and Graphics.
Springer International Publishing and Imprint: Springer, Cham (2020). http://arxiv.org/pdf/2007.09448
v2
[172] Słowik, A., Gupta, A., Hamilton, W.L., Jamnik, M., Holden, S.B., Pal, C.: Exploring structural induc-
tive biases in emergent communication. In: Cognitive Science Society (ed.) 43rd Annual Meeting of
the Cognitive Science Society (CogSci 2021). Curran Associates Inc, Red Hook, NY, USA (2021).
http://arxiv.org/pdf/2002.01335v1
[173] Słowik, A., Gupta, A., Hamilton, W.L., Jamnik, M., Holden, S.B.: Towards graph representation learn-
ing in emergent communication. In: Association for the Advancement of Artificial (ed.) Workshop on
Reinforcement Learning in Games @ AAAI-20 (2020). http://arxiv.org/pdf/2001.09063v2
[174] Steinert-Threlkeld,S.:Payingattentiontofunctionwords.In:2ndWorkshoponEmergentCommunication
(2018). http://arxiv.org/pdf/1909.11060v1
[175] Tucker, M., Li, H., Agrawal, S., Hughes, D., Sycara, K., Lewis, M., Shah, J.: Emergent discrete com-
munication in semantic spaces. In: Neural Information Processing Systems Foundation (ed.) Advances in
Neural Information Processing Systems 34. Advances in neural information processing systems. Curran
Associates Inc, Red Hook, NY, USA (2021). http://arxiv.org/pdf/2108.01828v3
[176] Tucker, M., Shah, J., Levy, R., Zaslavsky, N.: Towards human-agent communication via the information
bottleneck principle. In: Robotics Science and Systems (ed.) Social Intelligence in Humans and Robots
(2022). http://arxiv.org/pdf/2207.00088v1
[177] Ueda, R., Taniguchi, T.: Lewis’s signaling game as beta-vae for natural word lengths and segments. In:
OpenReview.net (ed.) 12th International Conference on Learning Representations (2024). http://arxiv.or
g/pdf/2311.04453v2
[178] Unger, T.A., Bruni, E.: Generalizing Emergent Communication (2020). http://arxiv.org/pdf/2001.01772
v3
[179] van der Wal, O., Boer, S., Bruni, E., Hupkes, D.: The grammar of emergent languages. In: Webber, B.,
Cohn, T., He, Y., Liu, Y. (eds.) Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pp. 3339–3359. Association for Computational Linguistics, Stroudsburg,
57PA, USA (2020). http://arxiv.org/pdf/2010.02069v2
[180] Vani, A., Schwarzer, M., Lu, Y., Dhekane, E., Courville, A.: Iterated learning for emergent systematicity
in vqa. In: OpenReview.net (ed.) 9th International Conference on Learning Representations (2021). http:
//arxiv.org/pdf/2105.01119v1
[181] Verma, S., Dhar, J.: Emergence of writing systems through multi-agent cooperation. In: Proceedings of
the 34th AAAI Conference on Artificial Intelligence (AAAI-20), pp. 13941–13942 (2020). https://doi.or
g/10.1609/aaai.v34i10.7243 . http://arxiv.org/pdf/1910.00741v1
[182] Villanger,J.I.F.,Bojesen,T.A.:Aninductivebiasforemergentcommunicationinacontinuoussetting.In:
Lutchyn,T.,RamírezRivera,A.,Ricaud,B.(eds.)Proceedingsofthe5thNorthernLightsDeepLearning
Conference (NLDL). Proceedings of Machine Learning Research, vol. 233, pp. 235–243. PMLR, Online
(2024). https://proceedings.mlr.press/v233/villanger24a.html
[183] Xu,Z.,Niethammer,M.,Raffel,C.:Compositionalgeneralizationinunsupervisedcompositionalrepresen-
tation learning: A study on disentanglement and emergent language. In: Neural Information Processing
Systems Foundation (ed.) Advances in Neural Information Processing Systems 35. Advances in neural
information processing systems. Curran Associates Inc, Red Hook, NY, USA (2022). http://arxiv.org/pd
f/2210.00482v2
[184] Yu,H.,Shen,W.,Huang,L.,Yuan,C.:Manipulatingmulti-agentnavigationtaskviaemergentcommuni-
cations.In:2023IEEE9thInternationalConferenceonCloudComputingandIntelligentSystems(CCIS),
pp. 351–355. IEEE, Piscataway, NJ, USA (2023). https://doi.org/10.1109/CCIS59572.2023.10262852
[185] Yuan, L., Fu, Z., Shen, J., Xu, L., Shen, J., Zhu, S.-C.: Emergence of pragmatics from referential game
between theory of mind agents. In: 3rd Workshop on Emergent Communication (2019). http://arxiv.or
g/pdf/2001.07752v2
[186] Ampatzis, C., Tuci, E., Trianni, V., Dorigo, M.: Evolution of signaling in a multi-robot system: Catego-
rization and communication. Adaptive Behavior 16(1), 5–26 (2008) https://doi.org/10.1177/1059712307
087282
[187] Bachwerk,M.,Vogel,C.:Establishinglinguisticconventionsintask-orientedprimevaldialogue.In:Espos-
ito, A., Vinciarelli, A., Vicsi, K., Pelachaud, C., Nijholt, A. (eds.) Analysis of Verbal and Nonverbal
Communication and Enactment. The Processing Issues. Lecture Notes in Computer Science, vol. 6800,
pp. 48–55. Springer Berlin Heidelberg, Berlin, Heidelberg (2011). https://doi.org/10.1007/978-3-642-257
75-9_4
[188] Bouchacourt, D., Baroni, M.: Miss tools and mr fruit: Emergent communication in agents learning about
object affordances. In: Association for Computational Linguistics (ed.) Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics, pp. 3909–3918 (2019). https://doi.org/10.186
53/v1/P19-1380 . http://arxiv.org/pdf/1905.11871v1
[189] Evtimova, K., Drozdov, A., Kiela, D., Cho, K.: Emergent communication in a multi-modal, multi-step
referential game. In: OpenReview.net (ed.) 6th International Conference on Learning Representations:
Conference Track Proceedings (2018). http://arxiv.org/pdf/1705.10369v4
[190] Hagiwara, Y., Kobayashi, H., Taniguchi, A., Taniguchi, T.: Symbol emergence as an interpersonal multi-
modalcategorization.FrontiersinRoboticsandAI6,134(2019)https://doi.org/10.3389/frobt.2019.00134
1905.13443v1
[191] Kolb,B.,Lang,L.,Bartsch,H.,Gansekoele,A.,Koopmanschap,R.,Romor,L.,Speck,D.,Mul,M.,Bruni,
E.: Learning to request guidance in emergent language. In: Association for Computational Linguistics
(ed.)ProceedingsoftheBeyondVisionandLANguage:inTEgratingReal-worldkNowledge(LANTERN),
Hong Kong, China, pp. 41–50 (2019). https://doi.org/10.18653/v1/D19-6407 . http://arxiv.org/pdf/19
12.05525v1
[192] Lee, J., Cho, K., Weston, J., Kiela, D.: Emergent translation in multi-agent communication. In: OpenRe-
view.net (ed.) 6th International Conference on Learning Representations: Conference Track Proceedings
58(2018). http://arxiv.org/pdf/1710.06922v2
[193] Patel, S., Wani, S., Jain, U., Schwing, A., Lazebnik, S., Savva, M., Chang, A.X.: Interpretation of emer-
gent communication in heterogeneous collaborative embodied agents. In: 2021 IEEE/CVF International
ConferenceonComputerVision.IEEE,Piscataway,NJ,USA(2021).http://arxiv.org/pdf/2110.05769v1
[194] Röpke,W.,Roijers,D.M.,Nowé,A.,Rădulescu,R.:Preferencecommunicationinmulti-objectivenormal-
form games. Neural Computing and Applications (2022) https://doi.org/10.1007/s00521-022-07533-6
2111.09191v1
[195] Saha, H., Venkataraman, V., Speranzon, A., Sarkar, S.: A perspective on multi-agent communication for
informationfusion.In:NeuralInformationProcessingSystemsFoundation(ed.)3rdWorkshoponVisually
Grounded Interaction and Language (2019). http://arxiv.org/pdf/1911.03743v1
[196] Yuan,L.,Fu,Z.,Zhou,L.,Yang,K.,Zhu,S.-C.:Emergenceoftheoryofmindcollaborationinmultiagent
systems. In: 3rd Workshop on Emergent Communication (2019). http://arxiv.org/pdf/2110.00121v1
[197] Bullard, K., Meier, F., Kiela, D., Pineau, J., Foerster, J.: Exploring Zero-Shot Emergent Communication
in Embodied Multi-Agent Populations (2020). http://arxiv.org/pdf/2010.15896v2
[198] Fitzgerald, N.: To populate is to regulate. In: 3rd Workshop on Emergent Communication (2019). http:
//arxiv.org/pdf/1911.04362v1
[199] Kajić, I., Aygün, E., Precup, D.: Learning to cooperate: Emergent communication in multi-agent naviga-
tion. In: Cognitive Science Society (ed.) 42nd Annual Meeting of the Cognitive Science Society (CogSci
2020). Curran Associates Inc, Red Hook, NY, USA (2020). http://arxiv.org/pdf/2004.01097v2
[200] Nevens, J., van Eecke, P., Beuls, K.: A practical guide to studying emergent communication through
grounded language games. In: Society for the Study of Artificial Intelligence and the Simulation of
Behaviour (AISB) Annual Convention 2019 (2019). http://arxiv.org/pdf/2004.09218v1
[201] Sirota, J.,Bulitko,V., Brown, M.R.G., Hernandez, S.P.: Evolving recurrent neural networks for emergent
communication. In: Proceedings of the Genetic and Evolutionary Computation Conference Companion.
GECCO ’19, pp. 189–190. Association for Computing Machinery, New York, NY, USA (2019). https:
//doi.org/10.1145/3319619.3321957
[202] Tieleman, O., Lazaridou, A., Mourad, S., Blundell, C., Precup, D.: Shaping representations through
communication: community size effect in artificial learning systems. In: Neural Information Processing
Systems Foundation (ed.) 3rd Workshop on Visually Grounded Interaction and Language (2019). https:
//doi.org/2019 . http://arxiv.org/pdf/1912.06208v1
[203] Gupta, S., Dukkipati, A.: Winning an election: On emergent strategic communication in multi-agent
networks. In: Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent
Systems. AAMAS ’20, pp. 1861–1863. International Foundation for Autonomous Agents and Multiagent
Systems, Richland, SC (2020). http://arxiv.org/pdf/1902.06897v2
[204] Thomas, J.D., Santos-Rodríguez, R., Piechocki, R., Anca, M.: Multi-lingual agents through multi-headed
neural networks. In: Neural Information Processing Systems Foundation (ed.) Advances in Neural Infor-
mationProcessingSystems34.Advancesinneuralinformationprocessingsystems.CurranAssociatesInc,
Red Hook, NY, USA (2021). http://arxiv.org/pdf/2111.11129v1
[205] Baronchelli,A.,Dall’Asta,L.,Barrat,A.,Loreto,V.:Strategiesforfastconvergenceinsemioticdynamics.
In: Rocha, L.M. (ed.) Artificial Life X. A Bradford book, pp. 480–485. MIT Press, Cambridge, MA, USA
(2006). https://openaccess.city.ac.uk/id/eprint/2673/
[206] Botoko Ekila, J.: Emergence of linguistic conventions in multi-agent systems through situated commu-
nicative interactions. In: Proceedings of the 23rd International Conference on Autonomous Agents and
Multiagent Systems. AAMAS ’24, pp. 2725–2727. International Foundation for Autonomous Agents and
Multiagent Systems, Richland, SC (2024). https://doi.org/10.5555/3635637.3663267
59[207] Botoko Ekila, J., Nevens, J., Verheyen, L., Beuls, K., van Eecke, P.: Decentralised emergence of robust
and adaptive linguistic conventions in populations of autonomous agents grounded in continuous worlds.
In: Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems.
AAMAS ’24, pp. 2168–2170. International Foundation for Autonomous Agents and Multiagent Systems,
Richland, SC (2024). https://arxiv.org/abs/2401.08461
[208] Das, A., Gervet, T., Romoff, J., Batra, D., Parikh, D., Rabbat, M., Pineau, J.: Tarmac: Targeted multi-
agent communication. In: Proceedings of the 36th International Conference on Machine Learning (ICML
2019) (2019). http://arxiv.org/pdf/1810.11187v2
[209] Hildreth, D., Guy, S.J.: Coordinating multi-agent navigation by learning communication. Proc. ACM
Comput. Graph. Interact. Tech. 2(2) (2019) https://doi.org/10.1145/3340261
[210] Loreto, V., Gravino, P., Servedio, V.D.P., Tria, F.: On the emergence of syntactic structures: quantifying
and modelling duality of patterning. Topics in cognitive science 8(2), 469–480 (2016) https://doi.org/10
.1111/tops.12193 1602.03661v1
[211] Lorkiewicz, W., Kowalczyk, R., Katarzyniak, R., Vo, Q.B.: On topic selection strategies in multi-agent
naming game. In: The 10th International Conference on Autonomous Agents and Multiagent Systems -
Volume 2. AAMAS ’11, pp. 499–506. International Foundation for Autonomous Agents and Multiagent
Systems, Richland, SC (2011). https://doi.org/10.5555/2031678.2031688
[212] Simoes, D., Lau, N., Reis, L.P.: Multi-agent actor centralized-critic with communication. NEUROCOM-
PUTING 390, 40–56 (2020) https://doi.org/10.1016/j.neucom.2020.01.079
[213] Simoes, D., Lau, N., Reis, L.P.: Exploring communication protocols and centralized critics in multi-
agentdeeplearning.INTEGRATEDCOMPUTER-AIDEDENGINEERING27(4),333–351(2020)https:
//doi.org/10.3233/ICA-200631
[214] Taylor,J.,Nisioti,E.,Moulin-Frier,C.:Sociallysupervisedrepresentationlearning:theroleofsubjectivity
in learning efficient representations. In: Proceedings of the 21st International Conference on Autonomous
Agents and Multiagent Systems (AAMAS 2022) (2022). http://arxiv.org/pdf/2109.09390v3
[215] Wang, T., Wang, J., Zheng, C., Zhang, C.: Learning nearly decomposable value functions via communi-
cationminimization.In:OpenReview.net(ed.)8thInternationalConferenceonLearningRepresentations
(2020). http://arxiv.org/pdf/1910.05366v2
[216] Eccles, T., Bachrach, Y., Lever, G., Lazaridou, A., Graepel, T.: Biases for emergent communication in
multi-agentreinforcementlearning.In:NeuralInformationProcessingSystemsFoundation(ed.)Advances
inNeuralInformationProcessingSystems32.Advancesinneuralinformationprocessingsystems.Curran
Associates Inc, Red Hook, NY, USA (2019). http://arxiv.org/pdf/1912.05676v1
[217] Gupta, S., Hazra, R., Dukkipati, A.: Networked multi-agent reinforcement learning with emergent com-
munication.In:Proceedingsofthe19thInternationalConferenceonAutonomousAgentsandMultiAgent
Systems. AAMAS ’20, pp. 1858–1860. International Foundation for Autonomous Agents and Multiagent
Systems, Richland, SC (2020). http://arxiv.org/pdf/2004.02780v2
[218] Jimenez Romero, C., Yegenoglu, A., Pérez Martín, A., Diaz-Pier, S., Morrison, A.: Emergent commu-
nication enhances foraging behavior in evolved swarms controlled by spiking neural networks. Swarm
Intelligence (2023) https://doi.org/10.1007/s11721-023-00231-6
[219] Karten, S., Tucker, M., Li, H., Kailas, S., Lewis, M., Sycara, K.: Interpretable learned emergent commu-
nication for human–agent teams. IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL
SYSTEMS 15(4), 1801–1811 (2023) https://doi.org/10.1109/TCDS.2023.3236599
[220] Li, S., Zhou, Y., Allen, R., Kochenderfer, M.J.: Learning emergent discrete message communication
for cooperative reinforcement learning. In: 2022 International Conference on Robotics and Automation
(ICRA), pp. 5511–5517. IEEE, Piscataway, NJ, USA (2022). https://doi.org/10.1109/ICRA46639.2022.9
812285 . http://arxiv.org/pdf/2102.12550v1
60[221] Lipinski,O.,Sobey,A.J.,Cerutti,F.,Norman,T.J.:Emergentpasswordsignallinginthegameofwerewolf.
In: 5th Workshop on Emergent Communication (2022). https://openreview.net/forum?id=B4xM-Qb0
mbq
[222] Pesce, E., Montana, G.: Improving coordination in small-scale multi-agent deep reinforcement learning
throughmemory-drivencommunication.MachineLearning 109(9-10),1727–1747(2020)https://doi.org/
10.1007/s10994-019-05864-5
[223] Resnick, C., Kulikov, I., Cho, K., Weston, J.: Vehicle communication strategies for simulated highway
driving. In: 1st Workshop on Emergent Communication (2017). http://arxiv.org/pdf/1804.07178v2
[224] Wu, J., Sun, X., Zeng, A., Song, S., Rusinkiewicz, S., Funkhouser, T.: Spatial intention maps for multi-
agentmobilemanipulation.In:IEEEInternationalConferenceonRoboticsandAutomation(2021).http:
//arxiv.org/pdf/2103.12710v1
[225] Yuan,L.,Chen,F.,Zhang,Z.,Yu,Y.:Communication-robustmulti-agentlearningbyadaptableauxiliary
multi-agent adversary generation. Frontiers of Computer Science 18(6) (2024) https://doi.org/10.1007/
s11704-023-2733-5
[226] Jaques,N.,Lazaridou,A.,Hughes,E.,Gulcehre,C.,Ortega,P.A.,Strouse,D.J.,Leibo,J.Z.,Freitas,N.d.:
Socialinfluenceasintrinsicmotivationformulti-agentdeepreinforcementlearning.In:Proceedingsofthe
36th International Conference on Machine Learning (ICML 2019) (2019). http://arxiv.org/pdf/1810.086
47v4
[227] Abdel-Aziz, M.K., Elbamby, M.S., Samarakoon, S., Bennis, M.: Cooperative multi-agent learning for
navigation via structured state abstraction. IEEE Transactions on Communications, 1 (2024) https:
//doi.org/10.1109/TCOMM.2024.3365520 2306.11336v2
[228] Foerster,J.,Chen,R.Y.,Al-Shedivat,M.,Whiteson,S.,Abbeel,P.,Mordatch,I.:Learningwithopponent-
learning awareness. In: Proceedings of the 17th International Conference on Autonomous Agents and
MultiAgent Systems. AAMAS ’18, pp. 122–130. International Foundation for Autonomous Agents and
Multiagent Systems, Richland, SC (2018). https://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/foer
steraamas18.pdf
[229] Nakamura, T., Taniguchi, A., Taniguchi, T.: Control as Probabilistic Inference as an Emergent Com-
munication Mechanism in Multi-Agent Reinforcement Learning. http://arxiv.org/pdf/2307.05004
v1
[230] Zheng, L., Yang, J., Cai, H., Zhang, W., Wang, J., Yu, Y.: Magent: A many-agent reinforcement learning
platform for artificial collective intelligence. In: Association for the Advancement of Artificial Intelligence
(ed.)ProceedingsoftheThirty-SecondAAAIConferenceonArtificialIntelligenceandThirtiethInnovative
Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances
in Artificial Intelligence. AAAI Press, Palo Alto, California, USA (2018). http://arxiv.org/pdf/1712.006
00v1
[231] Lee, J., Cho, K., Kiela, D.: Countering language drift via visual grounding. In: Inui, K., Jiang, J., Ng, V.,
Wan,X.(eds.)Proceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessing
andthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pp.4385–
4395. Association for Computational Linguistics, Stroudsburg, PA, USA (2019). http://arxiv.org/pdf/19
09.04499v1
[232] Clark, H.H.: Using Language. Cambridge University Press, Cambridge, UK (2012). https://doi.org/10.1
017/CBO9780511620539
[233] HOCKETT, C.F.: The origin of speech. Scientific American 203, 89–96 (1960)
[234] Bard, N., Foerster, J.N., Chandar, S., Burch, N., Lanctot, M., Song, H.F., Parisotto, E., Dumoulin, V.,
Moitra, S., Hughes, E., Dunning, I., Mourad, S., Larochelle, H., Bellemare, M.G., Bowling, M.: The
hanabi challenge: A new frontier for ai research. Artificial Intelligence 280(1–2), 103216 (2020) https:
//doi.org/10.1016/j.artint.2019.103216 1902.00506v2
61[235] Lipowska, D., Lipowski, A.: Emergence of linguistic conventions in multi-agent reinforcement learn-
ing. PloS one 13(11), 0208095 (2018) https://doi.org/10.1371/journal.pone.0208095 1811.07208 //
1811.07208v1
[236] Park,H.H.,Zhang,K.J.,Haley,C.,Steimel,K.,Liu,H.,Schwartz,L.:Morphologymatters:Amultilingual
language modeling analysis. Transactions of the Association for Computational Linguistics 9, 261–276
(2021) https://doi.org/10.1162/tacl_a_00365
[237] Kriegeskorte, N., Mur, M., Bandettini, P.: Representational similarity analysis - connecting the branches
of systems neuroscience. Frontiers in systems neuroscience 2, 4 (2008) https://doi.org/10.3389/neuro.06
.004.2008
[238] Spearman, C.: The proof and measurement of association between two things. The American Journal of
Psychology 15(1), 72 (1904) https://doi.org/10.2307/1412159
[239] Brighton, H., Kirby, S.: Understanding linguistic evolution by visualizing the emergence of topographic
mappings. Artificial Life 12(2), 229–242 (2006) https://doi.org/10.1162/artl.2006.12.2.229
[240] Hamming,R.W.:Errordetectinganderrorcorrectingcodes.BellSystemTechnicalJournal29(2),147–160
(1950) https://doi.org/10.1002/j.1538-7305.1950.tb00463.x
[241] Levenshtein, V.I.: Binary codes capable of correcting deletions, insertions and reversals. Soviet Physics
Doklady 10, 707 (1966)
[242] Hu, H., Lerer, A., Peysakhovich, A., Foerster, J.: "Other-Play" for Zero-Shot Coordination. http://arxiv.
org/pdf/2003.02979v2
[243] Dubova,M.,Moskvichev,A.(eds.):EffectsofSupervision,PopulationSize,andSelf-PlayonMulti-Agent
Reinforcement Learning to Communicate. Artificial Life Conference Proceedings, vol. ALIFE 2020: The
2020 Conference on Artificial Life (2020). https://doi.org/10.1162/isal_a_00328
[244] Hermann, K.M., Hill, F., Green, S., Wang, F., Faulkner, R., Soyer, H., Szepesvari, D., Czarnecki, W.M.,
Jaderberg,M.,Teplyashin,D.,Wainwright,M.,Apps,C.,Hassabis,D.,Blunsom,P.:GroundedLanguage
Learning in a Simulated 3D World. http://arxiv.org/pdf/1706.06551v2
62