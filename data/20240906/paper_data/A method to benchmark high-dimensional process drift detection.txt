A METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT
DETECTION
EDGAR WOLF AND TOBIAS WINDISCH
University of Applied Sciences Kempten, Germany
Abstract. Process curves are multi-variate finite time series data coming from manufac-
turing processes. This paper studies machine learning methods for drifts of process curves.
A theoretic framework to synthetically generate process curves in a controlled way is in-
troduced in order to benchmark machine learning algorithms for process drift detection. A
evaluation score, called the temporal area under the curve, is introduced, which allows to
quantify how well machine learning models unveil curves belonging to drift segments. Fi-
nally,abenchmarkstudycomparingpopularmachinelearningapproachesonsyntheticdata
generated with the introduced framework shown.
1. Introduction
Manufacturing lines are typically decomposed into a sequential arrangement of individual
steps, called processes, each utilizing one or more manufacturing techniques, such as casting,
forming, separating, joining, or coating, to develop the component in accordance with its
final specifications. Modern and encompassing sensor technology allows to monitor process
behavior precisely, typically by measuring key performance indicators, like force, pressure, or
temperature, over time. In modern IoT-enabled manufacturing lines, persisting these process
curves has become the general case, allowing to monitor not only the process behavior when
working on a single component, but also globally over multiple sequential executions [32].
Malign events like anomalous component batches, tool wear, or wrong calibrations can affect
the line performance badly. Such events proceed subtle and often lead to a slow deformation
oftheresultingprocesscurveiterationbyiteration. Detectingsuchprocess drifts,thatistime
periods where machine behavior changes, is key to keep unplanned downtimes and costly bad
parts at bay [19].
Specifically in high-volume production settings, where processes yield curves with multiple
variables at a small rate and with short cycle times between subsequent executions, detecting
process drifts is challenging due to the vast amount of data that is generated in short time.
Thus, these settings have been an ideal application for machine learning methods [9, 15, 19,
30, 31, 2, 21]. Process drifts should not confused with concept or data drifts, where the
goal is typically to analyse the declining performance of a trained machine learning model
when new data starts to differ from the train data [22]. Thus, methods to detect concept
drifts often have access to train data and trained machine learning models that can be used.
E-mail address: edgar.wolf@hs-kempten.de, tobias.windisch@hs-kempten.de.
1
4202
peS
5
]LM.tats[
1v96630.9042:viXra2 A METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT DETECTION
In our setting, however, we are interested in machine learning models that detect the drift
itself. Generally speaking, process curve datasets are datasets holding finitely many multi-
variatefinitetimeseries. Althoughprocesscurvesaretimeseriesbydefinition, manyanalysis
techniques to analyse time series data do not apply because many methods, like ARMA [7]
methods, assume infinite time series, often with strong assumptions on their stationarity.
When working with high-dimensional data, multivariate functional data analysis [16, 24]
is the statistical workhorse. Deep learning techniques, particularly dimensionality reduction
methods like autoencoder [25, 14], become increasingly popular [20]. A popular approach
in practice is to learn an low-dimensional embedding of the high-dimensional input and
then analyse the learned latent variables with classic machine learning models. There exist
some architectures that also can directly consume temporal context, like the index of the
process iteration, as an auxiliary variable to learn causal meaningful latent variables, like the
iVAE [12] or the independent component analysis [11]. In [13], a derivate of an variational
autoencoderhasbeendevelopedwhichparticularlydealswithprocessdrifts. In[31],Bayesian
autoencoders where used to detect drifts in industrial environments. Moreover, more general
neural network based systems, like for casting [15] or milling [9] have been developed to
analyse process curves. Often, also expert knowledge can be utilized, like by extracting
deterministic features which then are further analyzed by machine learning methods, like
in [2] for process curve analysis of wire bounds. Once the dimensionality has been reduced,
either by hand-crafted feature extraction of representation learning, classic methods for drift
detection apply, like a sliding KS-test [23]. There are also methods that work in high-
dimensional settings for tabular data, like hellinger-distance based techniques [5].
A common challenge when applying machine learning methods on applications from man-
ufacturing is data imbalance as interesting events like scrap parts or drifts are rare. In [27], a
deepgenerativemodelwasusedtosyntheticallygenerateprocesscurvesforcastingprocesses,
like to increase observations from the minority class.
Y(x)
12
10 15
8 10
Y
6 5
4 0
5
2
0 200
2 0 1 2 x 3 4 5 0 1 2
x
3
4 5 0
50 Pr1 oc0 es0
s1 e5 x0 ecutions
Fig. 1. Samples from a process curve (left) as well as a sequence of curve
samples (right).
Overall, the analysis of process curves with machine learning techniques has been studied
in depth for lots of applications from manufacturing. However, some inherent challenges
remain. First, whether a model that works well in one application also does in another is by
YA METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT DETECTION 3
far not guaranteed. Thus, releasing data to the public and allowing to benchmark multiple
models on the same data is important to make machine learning research more transparent
for process curve analysis. Most papers mentioned above, no data has been released to the
public, making it impossible to test other architectures. Often, this is due to privacy issues
and fear of leaking information to competitors. Public real-world datasets to study process
drifts are rare. For the special situation of milling, two process curve datasets [1, 28] exist
that hold ground truth quantifying the health of the tool used. A comprehensive overview
of degradation data sets for tasks with for tasks within prognostics and health management
can be found in [18]. For causal discovery, a few approaches exist that realistically generated
synthetic data, like for time-series [4] and tabular production data [8], however, the data
generated is often identically and independently distributed (iid) and hence inappropriate
for drift detection. The second challenge is how models should be evaluated. In most drift
detection settings, data is imbalanced, meaning that there are far less drifts than normal
data points. In [13], the false alarm rate and accuracy have been used to evaluate the model
performance. A commonly used metric to measure the statistical performance of a binary
classifier, like an anomaly detector, is the area under the ROC curve - short AUC. However,
the usage of the AUC is typically only applicable in settings where data is assumed to be iid,
which is not the case in drift detection.
In this work, we address the issue of benchmarking machine learning models for process
drift detection. We present a simple, flexible and effective theoretic framework to generate
synthetic process curves including drifts with a validated ground truth (Section 2 and Sec-
tion 3). Its possible to feed data of real process curves making the data realistic. Moreover,
we construct a metric called temporal area under the curve (TAUC) in Section 4, which
aims to take the temporal context of a detection into account. Finally, we present a short
benchmarks study in Section 5 as a proof of concept for the effectiveness of our score in
measuring the predictive power of drift detectors. Our work is based on preliminary results
of the first author [29]. In this work, we provide additionally insights into the introduced
score, introduce a variant called soft TAUC and compare it in depth with existing scores.
Moreover, we substantially generalize the data synthetization framework, for instance by
allowing higher-order derivatives, and we generate more sophisticated datasets for the bench-
markstudy. Wealsoreleasethecodethathelpsgeneratingprocesscurvestobenchmarkdrift
detectors, which is freely available under https://github.com/edgarWolf/driftbench. Its
optimization back-end is implemented in Jax [3] allowing a fast GPU-based curve generation.
2. Statistical framework to analyse process drifts
A process curve is a finite time-series C = (Y(x)) with Y(x) Rc and I R a finite
x∈I
set, where Y : R Rc represent physical properties of the process t∈ o be measur⊂ ed and x an
independent varia→ ble, often the time (see Remark 2.1 for concrete examples). We call c N
∈
the dimension of the process curve. Whenever a manufacturing process finishes its work on a
component, a process curve is yielded. Thus, when the same process is executed on multiple
parts sequentially, a sequence C ,...,C is obtained where each C arises under slightly
1 T t
different physical conditions, i.e., C = (Y (x)) . The different conditions can be due to
t t x∈It
different properties of the components or due to degradation and wareout effects of tools4 A METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT DETECTION
within the machine running the process. Also, the elements in I can vary from execution to
t
execution, for instance due to different offsets.
Remark 2.1. In staking processes Y is a measured force and x the walked path of the press.
In pneumatic test stations, Y might be a pressure where x might be time. In bolt fastening
processes, Y holds torque and angle and x is the time [21].
As new components are assembled each time the process is executed, the effect of a part
only affects a given curve, whereas wareout of the tool, which is used in every process, affects
all curves. Let C ,...,C be process curves from T-many executions and Y ,...,Y the
1 T 1 T
corresponding physical relations. We write [T] for the set 1,...,T . In our approach, we
assume that there exists a function f : Rk R Rc as well{ as a func} tion w : [T] Rk, such
× → →
that for all t [T] and x I :
t
∈ ∈
f(w(t),x+ϵ )+ϵ = Y (x)
x y t
where the function f is a proxy for the physics underneath the process and ϵ and ϵ de-
x y
note white noise relating to measurement inaccuracies that is independent of t. The vector
w(t) represents environmental properties of the t-th execution, and some of its coordinates
correspond to component properties, some to properties of the machine.
Assuming no tool degradation but only component variance, we could assume that w(t)
is sampled in each process from a fixed but unknown distribution on Rk, like w(t)
µ,σ
∼ N
with fixed µ Rk and σ Rk×k for all t [T]. Tool degradation, however, affects the curve
∈ ∈ ∈
substantially, often by letting certain support points of the curve move. For instance, in a
staking process, the position and value of the maximal force, i.e., where the first derivative is
zero, starts shifting (see Figure 1). Often, these support points can be formulated in terms
of derivatives of f. More formally, let ∂i be the (i)-th derivative of f according to the second
x
argument and let for t [T], xi(t),yi(t) Rni be such support points of the t-curve C t, i.e.:
∈ ∈
(2.1) ∂if(w(t),xi(t)) = yi(t)
x j j
for all j [n ]. Particularly, the properties w(t) deform in a way such that the derivatives of
i
∈
f(w(t), ) satisfies all support points. As mentioned before, if underlying physical properties
·
of the process or components change, these support points start to change their position.
More formally, at the t-th process execution, the support points are sampled according to a
distribution, whose statistical parameter depend on t, for instance, for a particular support
point j [n ] of yi, the following temporal change could happen, i.e. yi(t) , with
∈ i j ∼ Nµi(t),σ

b, if t < t
 0

µi(t) = d t−t0 +b, if t t t
 · t1−t0 0 ≤ ≤ 1
d, if t < t
1
with b = d. That is, the mean of the support point drifts linearly from b to d between t and
0
̸
t (see also Figure 1).
1A METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT DETECTION 5
3. Data generation
In this section, we explain the synthetization method of process curves. Here, we neither
focus on how the w(t) behave in latent space, nor on how f is formulated exactly. Instead,
our key idea is to model the behavior of support points over time and seek for parameters
w(t) using non-linear optimization satisfying the support point conditions in (2.1). That
way, we obtain a generic and controllable representation of the process curves that is capable
of modelling physical transformations over time by letting the physically motivated support
points drift over time. Without restricting generality and to keep notation simple, we will
assume for the remainder that c = 1. The multivariate case is a straight-forward application
of our approach by modeling each variable in Y individually (see Remark 3.1). For this, let
f : Rk R R be an l-times differentiable map and let for each i [l], xi Rni and yi Rni
× → ∈ ∈ ∈
be two n -dimensional vectors. We want to find w∗ such that
i
(3.1) ∂if(w∗,xi) yi j [n ] i [l].
x j j i
≈ ∀ ∈ ∀ ∈
Particularly, xi and yi can be considered as support points surpassed by the graph of the
map f(w∗, ) : R R (see Figure 2). Assuming that f l+2(Rk R), i.e. f is l+2-times
· → ∈ C ×
f(w, )
12 12
10 10
8 8
6 6
4 4
2 2
0 0
2 2
0 1 2 3 4 5 0 1 2 3 4 5
x
Fig. 2. Visualization of the data synthetization given a function f(w,x) =
(cid:80)5 w xi. Left figure shows f(w, ) solved for concrete xi,yi (red points).
i=0 i · ·
Right figure shows sequence f(w , ),...,f(w , ) where gaussian noise was
1 100
· ·
added on on coordinate in y1(t) (corresponding coordinate x1(t) is marked
with a dashed line).
differentiable, and given xi Rni and yi Rni for each i [l], we can solve (3.1) using
∈ ∈ ∈
second-order quasi-Newton methods [6, Chapter 3] for the objective function
(cid:88)l (cid:88)ni
(3.2) argmin D ∂if(w,xi) yi 2
i x j j
w∈Rk
i=1 j=1
·∥ − ∥
where D ,...,D are constants to account for the different value ranges of the functions ∂if.
1 l x
Now, let xi(1),...,xi(T) Rni and yi(1),...,yi(T) Rni be sequences of T-many n i-
∈ ∈
dimensional vectors respectively. Solving the optimization problem (3.2) for the each t [T],
∈
Y6 A METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT DETECTION
i.e. for the x1(t),y1(t),...,xl(t),yl(t), we get a sequence of parameters w ,...,w Rk and
1 T
a sequence of maps f(w , ),...,f(w , ) from R to R. ∈
1 T
Fixing now a finite and· not necessari· ly equidistant set of points of interests I R and set-
ting C = f(w ,I) R|I|, we obtain a sequence of process curves C ,...,C . In A⊂ ppendix A,
i i 1 T
∈
we showcase in depth an example where f is a polynomial.
Remark 3.1 (Multivariate data). Of course, our theoretic framework extends naturally to
multi-variate time series data, where each signal has either its own or multiple signals share
same latent information.
Remark 3.2 (Feeding real-data). Clearly, the support points x1(t),y1(t),...,xl(t),yl(t) can
come from a real process curve dataset. Choosing a universal function approximator f like a
feed-forward neural network [10] or a Kolmogorov-Arnold network [17].
4. The temporal area under the curve
In this section, we construct a score to measure the predictive power of machine learning
models for process drift detection. In order to do so, we first formalize what we understand
as a process drift and which assumptions we require. Let C ,...,C be a sequence of process
1 T
curves and let [T] be the set of curve indices belonging to drifts. Our first assump-
D ⊂
tion is that drifts, different than point anomalies, appear sequentially and can be uniquely
decomposed into disjoint segments:
Definition 4.1 (Drift segments). Let [T]. Then a series of subsets ,..., is a
1 k
D ⊂ D D ⊂ D
partition of drift segments if there exists 1 l < h < l < h < ...,< l < h T such
1 1 2 2 k k
≤ ≤
that for all i, we have = [l ,h ] and = k .
Di i i
D
∪i=1Di
Thedriftsegmentscanbeconsideredasapartitionofthesmallestconsecutivedriftswhich
cannot decomposed any further into smaller segments. Now, assume we also have the output
s RT of a detector where each coordinate s quantifies how likely the curve C of the i-
t t
∈
t-h process execution belongs to a drift, that is, the higher s the more likely the detector
t
classifies t . By choosing a threshold τ R, we can construct a set
∈ D ∈
ˆ(s,τ) := t [T] : s τ
t
D { ∈ ≥ }
which serves as a possible candidate for . Clearly, if τ τ , then ˆ(s,τ ) ˆ(s,τ ). Its
1 2 1 2
D ≥ D ⊆ D
also straight-forward to see that for every τ, the set ˆ(s,τ) decomposes uniquely into drift
D
segments ˆ ,..., ˆ as defined in Definition 4.1 and that the length and number of these
1 l
D D
atomicsegmentsdependsonτ. Now, toquantifythepredictivepowerofthedetectoryielding
s, one needs to quantify how close ˆ(s, ) is to when τ varies. There are many established
D · D
set-theoretic measurements that are widely used in practice to quantify the distance between
|A∩B|
two finite and binary sets A and B, like the Jaccard index , the Hamming distance
|A∪B|
|A∩B|
A B + B A ,ortheOverlapcoefficient justtonameafew. Mostscores,however,
| \ | | \ | min(|A|,|B|)
have as a build-in assumption that the elements of the set are iid and hence the temporal
context is largely ignored making them unsuitable for process drift detection. Moreover, for
mostdetectorswehavetoselectadiscriminationthresholdτ,makingevaluationcumbersome
asitrequirestotunethethresholdonaseparateheld-outdataset. Moreover,inmostpracticalA METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT DETECTION 7
scenarios, is only a small subset and thus the evaluation metric has to consider highly
D
imbalanced szenarios as well.
1 2
D D D
Dˆ
1
Dˆ
2
Dˆ
3
Dˆ(s,τ)
t
Fig. 3. Temporal arrangements of true and predicted drift segments as input
for Algorithm 1.
Clearly, detectorsarerequiredwherealltruedriftsegments areoverlappedbypredicted
i
drift segments. For this, let T := j [l] : ˆ = . ClD early, T T = if and
i i j i i+1 i
{ ∈ D ∩D ̸ ∅} ∩ ̸ ∅ D
both intersect with a predicted drift segment. Now, the set := ˆ which is the
Di+1 Ti ∪j∈TiDj
unionofallpredictivesegmentsintersectingwith servesasacandidatefor . Tomeasure
i i
D D
how well is covered - or overlapped - by we define the soft overlap score inspired by the
i i
D T
Overlap coefficient as follows:
i
(4.1) sOLS( ,s,τ) := |T |
i
D max( ) min( )+1
i i i i
T ∪D − T ∪D
Obviously, an sOLS of 1 is best possible and this is reached if and only if = . It is easy
i i
T D
to see that for fixed , the enlargement of beyond the boundaries of improves the
i i i
D T D
overlap score, as increases and one of either max( ) or min(T ) increases as
i i i i i
|T | T ∪D − ∪D
well. A special case is if is completely covered by , i.e. , then it follows that is
i i i i i
D T D ⊆ T T
an interval as well and thus sOLS( ,s,τ) = 1. When enlarges, then the number of false
i i
D T
positives, i.e. the time points t contained in some ˆ and in the complement := [T] of
i
D D \D
the ground truth , enlarges as well. Thus, the predictive power of a detector is shown in
D
the overlap score as well as the created false positive rate
ˆ(s,τ)
FPR( ,s,τ) := |D ∩D|.
D
|D|
into account. To also take false negatives into account, the enumerator in (4.2) could be
changed as follows, yielding our final definition of the overlap score:
i i
(4.2) OLS( ,s,τ) := |T ∩D | .
i
D max( ) min( )+1
i i i i
T ∪D − T ∪D
Algorithm 1 illustrates in detail how the Overlap score OLS( ,s,τ) can be computed algo-
D
rithmically.
Ourscoreconsidersboth,theOLSaswellastheFPR,whichmutuallyinfluenceeachother.
In the computation of the AUC, any threshold τ from [min (s ),max (s )] yields a pair of
t t t t
false positive rate FPR( ,s,τ) and true positive rate TPR( ,s,τ) which can be drawn as a
D D
curve in the space where FPR is on the x-axis and TPR on the y-axis. Similarly, we define
the temporal area under the curve, or just TAUC, as the area under the FPR-OLS curve8 A METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT DETECTION
Algorithm 1 Overlap score OLS( ,s,τ)
D
Input: [T], s RT,τ R
D ⊂ ∈ ∈
Output: Overlap score
1: 1,..., k find drift segments of
2:
Dˆ 1,...,Dˆ
l
←
find drift segments of
Dˆ(s,τ)
3:
D
s 0
DRk← D
← ∈
4: for i [k] do
∈
5: T i j [l] : ˆ j i = ▷ All predicted drift segments overlapping with i
← { ∈ D ∩D ̸ ∅} D
6: Ti
←
∪j∈TiDˆ j ▷ Union of all segments intersecting with Di
7: s i ←
max(Ti∪Di|T )−i∩ mD ini|
(Ti∪Di)+1 ▷ fraction of overlap
8: end for
9: return k1 (cid:80)k i=1s i
while the discrimination threshold τ varies. We refer to the soft TAUC, or just sTAUC, to
the area under the FPR-sOLS curve (see Figure 4).
Note that the integral of the curve can be computed using two different methods, the
step rule and the trapezoidal rule and depending on which method is used, the value of the
score may differs. We showcase this behavior in detail for trivial detectors in Appendix C. In
Appendix B, we investigate in several synthetic cases in depth the differences and similarities
between sTAUC, TAUC, and AUC.
1.0
0.8
0.8
0.6
0.4 0.6 sTAUC(0.74)
TAUC(0.06)
0.2 0.4 AUC(0.58)
0.0 0.2
−0.2
0.0
0 200 400 600 800 1000 0.0 0.2 0.4 0.6 0.8 1.0
FPR
Fig. 4. The TPR, sOLS, and OLS when the FPR varies for the synthetic
prediction on the left.
Example 4.2. Consider the situation shown in Figure 3. There, we have two true drift
segments and , and three segments ˆ , ˆ and ˆ as drift segments of some detector
1 2 1 2 3
output
ˆ(D
s,τ).
CD
learly, T = 1,2
whereD
T
=D
3
asD
only overlaps with . To unveil
1 2 3 2
D { } { } D D
, the detector needs to separate drift segments, leading to false negatives and positives and
1
tD hus a relatively low OLS. On the other hand, as ˆ , we have = ˆ .
3 2 2 3
D ⊂ D T D
SLO/SLOs/RPTA METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT DETECTION 9
5. Experiments
Next,webenchmarkexistingalgorithmsondatageneratedwithourframeworkdriftbench
and reporting the TAUC1. The goal of the benchmark is to provide a proof of concept for our
score and data generation method, not to be very comprehensive on the model side. Thus,
based on our literature research in Section 1 we have hand-selected a small set of typically
used model patterns drift detectors used in practice consists of (see Section 5.1).
The basic evaluation loop follows a typical situation from manufacturing, where process
engineers have to identify time periods within a larger curve datasets where the process
has drifted. Thus, all models consume as input a process curve dataset C ,...,C and do
1 T
not have access to the ground truth , which is the set of curves belonging to a drift (see
D
Section 5.3). Afterwards, each model predicts for each curve C from this dataset, a score
t
s on which then the TAUC, sTAUC, and AUC are computed. To account for robustness,
t
we generate each dataset of a predefined specification multiple times with different random
seeds each, leading to slightly different datasets of roughly same complexity.
5.1. Algorithms. The algorithms used can be decomposed into multiple steps (see also
Figure 5), but not all algorithms use all steps. First, there are may some features extracted
from each curve. Afterwards, a sliding window collects and may aggregates these such that
a score is computed.
C1 e1 a1 s1
C2 e2 a2 s2
... ... ... ...
Ct exF te ra at cu tr ioe
n
et W ain gd gro ew gi an tg ioa nnd at comS pc uo tr ae
tion
st
... ... ... ...
CT eT aT sT
Fig. 5. A high-level overview of the elementary tasks of the detectors used.
5.1.1. Feature extraction. In this steps, we use autoencoders [25] to compute a k dimensional
representatione Rk foreachhigh-dimensionalprocesscurveC withksmall. Theindention
t t
∈
behind is to estimate an inverse of the unknown function f and to recover information about
the support points used. Moreover, we also apply deterministic aggregations over the x-
information of each curve C ,
t
5.1.2. Windowing and aggregation. In this step, the algorithms may aggregate the data from
the previous step using a fixed window of size m that is applied in a rolling fashion along the
1All datasets and algorithms used are available in the repository of driftbench.10 A METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT DETECTION
process iterations. One aggregation we use is to first compute for each coordinate j [k] of
e Rk with t m the rolling mean: ∈
t
∈ ≥
t
1 (cid:88)
(5.1) a = e .
t,j i,j
m
i=t−m+1
These values can then further be statistically aggregated, like by taking the maximum a :=
t
max a : j [k] .
t,j
{ ∈ }
5.1.3. Score computing. Goal of this step is to compute a threshold which correlates with
the ground truth, that is, the larger the higher the possibility of a drift. Here, we may
also aggregate previous features in a rolling fashion. The simplest aggregation we use is to
compute the euclidean distance of subsequent elements s = a a which is just the
t t t−1 2
∥ − ∥
absolute difference if a and a are scalars. If a is a scalar, we also can compute the rolling
t t−1 t
standard deviation, again over a window of size m, like this:
(cid:118)
(cid:117) (cid:117) 1 (cid:88)t (cid:32) (cid:32) 1 (cid:88)t (cid:33)(cid:33)2
(cid:117)
s = a a .
t (cid:116) j i
m 1 − m
− j=t−m+1 i=t−m+1
Another approach follows a probabilistic path by testing if a set of subsequent datapoints
a ,...,a come from the same distribution as a given reference set. In our study,
t−m+1 t
{ }
we use a windowed version [23] of the popular Kolmogorov-Smirnov test [26], often called
KSWIN, which makes no assumption of the underlying data distribution. However, this can
only be applied when a is a scalar. More particularly, we define two window sizes, m for the
t r
referencedataandm fortheobservation. Thewindowsareoffsetbyconstantδ > 0. Wethen
o
invoke the KS-test and receive a p-value p , which is small if the datasets come from different
t
distributions. Thus, one way to derive a final score is to compute s = log(1+ 1). We also
t pt
evaluate algorithms that derive their score based on a similarity search within a ,...,a .
1 t
{ }
Here, weuseclusteringalgorithms, likethepopulark-meansalgorithm, andusetheeuclidean
distancetothecomputedclustercenterofa ass . Anotherwayistofitaprobabilitydensity
t t
function on s , like a mixture of Gaussian distributions, and to set s as the log likelihood of
t t
a within this model.
t
5.2. Algorithm Overview. Here is a short summary of the algorithms used in our bench-
mark study:
RollingMeanDifference(m ) First, the rolling mean over a window of size m is
r r
•
computed over all values for the respective curves in the window. Afterwards, the
maximum value for each curve is taken and the absolute difference between two
consecutive maximum values is computed.
RollingMeanStandardDeviation(m ) First, the rolling mean over a window of size
r
•
m is computed over all values for the respective curves in the window. We also
r
choose the maximum value of these computed values per curve. Then, we compute
the standard deviation using the same window for this one-dimensional input.A METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT DETECTION 11
SlidingKSWIN(m ,m ,δ): We compute the mean value for each curve and apply a
r o
•
sliding KS-test on this aggregated data. We use two windows of size m and m
r o
where the windows are offset by δ.
Cluster(n ): A cluster algorithm performed on the raw curves using n clusters
c c
•
where score is distance to closest cluster center.
AE(k)-mean-KS(m ,m ,δ): First, an autoencoder is applied extracting computing
r o
•
k many latent dimensions. Afterwards, the mean across all k latent dimensions is
computed. Finally, a sliding KS-test is applied with two windows of sizes m and m ,
r o
where the windows are offset by δ.
5.3. Datasets. We benchmark the algorithms listed in 5.1 on three different datasets (see
Figure6)createdwithourframeworkdriftbench, alldesignedtocomprisedifferentinherent
challenges. To generate dataset-1 and dataset-2, we used
dataset-1 dataset-2 dataset-3
8 8 10
6 6 8
4 4 6
2Y 2Y 4Y
0 0 2
−2 −2 0
−4
10000 30000 10000
8000 25000 8000
0.0 0.5 1.0 1.5 x2.0 2.5 3.0 3.5 4.0 0 200040Pr0oc0esse6 x0ec0u0tions 0.0 0.5 1.0 1.5 x2.0 2.5 3.0 3.5 4.0 050001000P0ro1 c5 es0s0e0xe2 c0 ut0io0n0s 0.0 0.5 1.0 1.5 x2.0 2.5 3.0 3.5 4.0 0 200040Pr0oc0esse6 x0ec0u0tions
8 8 10.0
6 6 7.5
4 4 5.0
2 2 2.5
0 0 0.0
−2 0 1 2 3 4 −20 1 2 3 4 0 1 2 3 4 5
x x x
Fig. 6. The datasets used in our benchmark study. The true drift segments
are marked in green. Lower figures show selected curves, whose color encodes
the process iteration t [T] - blue marks smaller t values, red larger ones.
∈
7
(cid:88)
f(w,x) = w xi
i
·
i=0
as function. The dataset dataset-1 consists of T = 10.000 curves, each called on I] = 100
|
equidistant values between [0,4]. On the other hand, dataset-2 consists of T = 30.000
curves each having I = 400 values. Both datasets have drifts that concern a movement of
| |
the global maximum together with drifts where only information of first order changes over
time. In the generation process of dataset-3, we used
f(w,x) = w x sin(π x w )+w x
0 1 2
· · · − ·
and generated T = 10.000 many curves, each having I = 100 datapoints. It only holds a
| |
single drift, wherethe globalminimumat driftsconsistentlyovera smallperiodof time along
Y Y Y12 A METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT DETECTION
the x-axis. In all datasets, the relative number of curves belonging to a drift is very small:
roughly 2 percent in dataset-1, 0.1 percent in dataset-2 and 1 percent in dataset-3.
5.4. Results. The result of our benchmark study is shown in Figure 7. We again want
to mention that our benchmark study is not to show whether one model is superior over
another. Instead, its purpose is to highlight how well the TAUC and our synthetization
system allows to explore their performance. Surprisingly, the RandomGuessDetector reached
the highest AUC score on dataset-3, where it ranges on all three datasets among the last
ranks in the TAUC score. The respective predictions of the best detectors are plotted in
Figure 9. Moreover, cluster based systems reach good AUC scores, but are not competitive
when using the TAUC as quantification measurement (see also Figure 8). Unsurprisingly,
dataset1 dataset2 dataset3
AutoencoderDetector(lr=0.0001,numepochs=10)
AutoencoderDetector(lr=0.0001,numepochs=100)
AutoencoderDetector(lr=0.001,numepochs=50)
ClusterDetector(method=gaussianmixture,ncenters=10)
ClusterDetector(method=gaussianmixture,ncenters=5)
ClusterDetector(method=kmeans,ncenters=10)
ClusterDetector(method=kmeans,ncenters=5)
RandomGuessDetector
RollingMeanDifferenceDetector(windowsize=20)
RollingMeanDifferenceDetector(windowsize=40)
RollingMeanStandardDeviationDetector
SlidingKSWINDetector
0.0 0.2 0.4 0.25 0.50 0.75 0.25 0.50 0.75
TAUC sTAUC AUC
Fig. 7. Benchmark results on dataset-1, dataset-2, and dataset-3
the performance of autoencoders-based systems depends heavily on their hyperparameters,
leading to large variations in the AUC score.
1
0
1 0 2000 4000 6000 8000 10000
0
1 0 2000 4000 6000 8000 10000
0
1 0 2000 4000 6000 8000 10000
0
1 0 2000 4000 6000 8000 10000
0
0 2000 4000 6000 8000 10000
Fig. 8. Performance of cluster based systems on dataset-3, ranging among
the models with highest AUC, but lowest TAUC.
Acknowledgements. ThisworkissupportedbytheHightechAgendaBavaria. Theauthors
aregratefultoMatthiasBurkhardt,FabianHueber,KaiMu¨ller,andUlrichG¨ohnerforhelpful
discussions.A METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT DETECTION 13
DetectorwithhighestTAUCscore(RollingMeanStandardDeviationDetector)
1.0
0.5
0.0
0 2000 4000 6000 8000 10000
DetectorwithhighestAUCscore(RandomGuessDetector)
1.0
0.5
0.0
0 2000 4000 6000 8000 10000
Fig. 9. Comparison of the prediction s RT of the best detectors on
∈
dataset-3.
Data availability. The generated data and implemented algorithms are implemented in a
pythonpackagedriftbenchwhichisfreelyavailableunderhttps://github.com/edgarWolf/
driftbench.
Conflictofinterests. Theauthorsprovidenoconflictofinterestassociatedwiththecontent
of this article.
References
[1] A. Agogino and K. Goebel. Milling data set, BEST Lab, UC Berkeley, NASA Prognostics Data Repos-
itory, NASA Ames Research Center, 2007. URL: https://data.nasa.gov/Raw-Data/Milling-Wear/
vjv9-9f3x/about_data.
[2] Anwar Al Assadi, David Holtz, Frank Na¨gele, Christof Nitsche, Werner Kraus, and Marco F. Hu-
ber. Machine learning based screw drive state detection for unfastening screw connections. 65:19–
32. URL: https://www.sciencedirect.com/science/article/pii/S0278612522001248, doi:10.1016/
j.jmsy.2022.07.013.
[3] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclau-
rin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX:
composabletransformationsofPython+NumPyprograms,2018.URL:http://github.com/google/jax.
[4] YuxiaoCheng,ZiqianWang,TingxiongXiao,QinZhong,JinliSuo,andKunlunHe.Causaltime: Realisti-
callygeneratedtime-seriesforbenchmarkingofcausaldiscovery.InTheTwelfthInternationalConference
on Learning Representations, 2024. URL: https://openreview.net/forum?id=iad1yyyGme.
[5] Gregory Ditzler and Robi Polikar. Hellinger distance based drift detection for nonstationary environ-
ments.In2011IEEESymposiumonComputationalIntelligenceinDynamicandUncertainEnvironments
(CIDUE), pages 41–48, 2011. doi:10.1109/CIDUE.2011.5948491.
[6] Roger Fletcher. Practical Methods of Optimization. John Wiley & Sons, New York, NY, USA, second
edition, 1987.
[7] John Gurland. Hypothesis testing in time series analysis., 1954. doi:10.2307/2281054.
[8] KonstantinGo¨bler,TobiasWindisch,TimPychynski,MathiasDrton,SteffenSonntag,andMartinRoth.
causalAssembly: Generating realistic production data for benchmarking causal discovery. In Francesco
LocatelloandVanessaDidelez,editors,3rd Conference on Causal Learning and Rasoning,volume236of
Proceedings of Machine Learning Research, pages 609–642, 2024.
[9] DanielFrankHesserandBerndMarkert.Toolwearmonitoringofaretrofittedcncmillingmachineusing
artificialneuralnetworks.ManufacturingLetters,19:1–4,2019.URL:https://www.sciencedirect.com/
science/article/pii/S2213846318301524, doi:https://doi.org/10.1016/j.mfglet.2018.11.001.14 A METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT DETECTION
[10] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal
approximators.Neural Networks, 2(5):359–366, 1989.URL:https://www.sciencedirect.com/science/
article/pii/0893608089900208, doi:https://doi.org/10.1016/0893-6080(89)90020-8.
[11] Aapo Hyvarinen, Hiroaki Sasaki, and Richard E. Turner. Nonlinear ICA Using Auxiliary Variables and
Generalized Contrastive Learning. URL: http://arxiv.org/abs/1805.08651, arXiv:1805.08651, doi:
10.48550/arXiv.1805.08651.
[12] IlyesKhemakhem,DiederikKingma,RicardoMonti,andAapoHyvarinen.VariationalAutoencodersand
NonlinearICA:AUnifyingFramework.InProceedings of the Twenty Third International Conference on
Artificial Intelligence and Statistics, pages 2207–2217. PMLR. URL: https://proceedings.mlr.press/
v108/khemakhem20a.html.
[13] Youngju Kim, Hoyeop Lee, and Chang Ouk Kim. A variational autoencoder for a semiconductor fault
detection model robust to process drift due to incomplete maintenance. Journal of Intelligent Manufac-
turing, 34(2):529–540, 2023. doi:10.1007/s10845-021-01810-2.
[14] DiederikP.KingmaandMaxWelling.Auto-EncodingVariationalBayes.In2ndInternationalConference
onLearningRepresentations, ICLR2014, Banff, AB,Canada, April14-16, 2014, ConferenceTrackPro-
ceedings, 2014. arXiv:http://arxiv.org/abs/1312.6114v10, doi:https://doi.org/10.48550/arXiv.
1312.6114.
[15] JeongsuLee,YoungChulLee,andJeongTaeKim.Migrationfromthetraditionaltothesmartfactoryin
thedie-castingindustry: Novelprocessdataacquisitionandfaultdetectionbasedonartificialneuralnet-
work.JournalofMaterialsProcessingTechnology,290:116972,2021.URL:https://www.sciencedirect.
com/science/article/pii/S0924013620303939, doi:https://doi.org/10.1016/j.jmatprotec.2020.
116972.
[16] Yehua Li, Yumou Qiu, and Yuhang Xu. From multivariate to functional data analysis: Funda-
mentals, recent developments, and emerging areas. Journal of Multivariate Analysis, 188:104806,
2022.50thAnniversaryJubileeEdition.URL:https://www.sciencedirect.com/science/article/pii/
S0047259X21000841, doi:https://doi.org/10.1016/j.jmva.2021.104806.
[17] Ziming Liu, Yixuan Wang, Sachin Vaidya, Fabian Ruehle, James Halverson, Marin Soljaˇci´c, Thomas Y
Hou, and Max Tegmark. Kan: Kolmogorov-arnold networks. arXiv preprint arXiv:2404.19756, 2024.
[18] FabianMauthe,ChristopherBraun,JulianRaible,PeterZeiler,andMarcoF.Huber.Overviewofpublicly
available degradation data sets for tasks within prognostics and health management, 2024. arXiv:2403.
13694.
[19] Andreas Mayr, Dominik Kißkalt, Moritz Meiners, Benjamin Lutz, Franziska Scha¨fer, Reinhardt Seidel,
AndreasSelmaier,JonathanFuchs,MaximilianMetzner,AndreasBlank,andJo¨rgFranke.Machinelearn-
inginproduction–potentials,challengesandexemplaryapplications.ProcediaCIRP,86:49–54,2019.7th
CIRP Global Web Conference – Towards shifted production value stream patterns through inference of
data,models,andtechnology(CIRPe2019).URL:https://www.sciencedirect.com/science/article/
pii/S2212827120300445, doi:https://doi.org/10.1016/j.procir.2020.01.035.
[20] Moritz Meiners, Marlene Kuhn, and Jo¨rg Franke. Manufacturing process curve monitoring with deep
learning. Manufacturing Letters, 30:15–18, 2021. URL: https://www.sciencedirect.com/science/
article/pii/S2213846321000742, doi:https://doi.org/10.1016/j.mfglet.2021.09.006.
[21] Moritz Meiners, Andreas Mayr, and Jo¨rg Franke. Process curve analysis with machine learning on the
exampleofscrewfasteningandpress-inprocesses.97:166–171.URL:https://www.sciencedirect.com/
science/article/pii/S2212827120314414, doi:10.1016/j.procir.2020.05.220.
[22] Yuri Thomas P. Nunes and Luiz Affonso Guedes. Concept drift detection based on typicality and eccen-
tricity. IEEE Access, 12:13795–13808, 2024. doi:10.1109/ACCESS.2024.3355959.
[23] Christoph Raab, Moritz Heusinger, and Frank-Michael Schleif. Reactive soft prototype computing for
concept drift streams. Neurocomputing, 416:340–351, 2020. URL: https://www.sciencedirect.com/
science/article/pii/S0925231220305063, doi:https://doi.org/10.1016/j.neucom.2019.11.111.
[24] Saptarshi Roy, Raymond K. W. Wong, and Yang Ni. Directed cyclic graph for causal discovery from
multivariatefunctionaldata.InA.Oh,T.Naumann,A.Globerson,K.Saenko,M.Hardt,andS.Levine,A METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT DETECTION 15
editors, Advances in Neural Information Processing Systems, volume 36, pages 42762–42774. Cur-
ran Associates, Inc., 2023. URL: https://proceedings.neurips.cc/paper_files/paper/2023/file/
854a9ab0f323b841955e70ca383b27d1-Paper-Conference.pdf.
[25] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning Internal Representations by
Error Propagation, pages 318–362. 1987.
[26] RichardSimardandPierreL’Ecuyer.Computingthetwo-sidedkolmogorov-smirnovdistribution.Journal
of Statistical Software, 39(11):1–18, 2011. URL: https://www.jstatsoft.org/index.php/jss/article/
view/v039i11, doi:10.18637/jss.v039.i11.
[27] JiyoungSong,YoungChulLee,andJeongsuLee.Deepgenerativemodelwithtimeseries-imageencoding
formanufacturingfaultdetectionindiecastingprocess.JournalofIntelligentManufacturing,34(7):3001–
3014, 2023. doi:10.1007/s10845-022-01981-6.
[28] Mohamed-Ali Tnani, Michael Feil, and Klaus Diepold. Smart data collection system for brownfield
cnc milling machines: A new benchmark dataset for data-driven machine monitoring. Procedia CIRP,
107:131–136,2022.Leadingmanufacturingsystemstransformation–Proceedingsofthe55thCIRPCon-
ferenceonManufacturingSystems2022.URL:https://www.sciencedirect.com/science/article/pii/
S2212827122002384, doi:https://doi.org/10.1016/j.procir.2022.04.022.
[29] Edgar Wolf. Anomalieerkennung in hoch-dimensionalen Sensordaten. Master’s thesis, University of Ap-
plied Sciences, Kempten, April 2024. doi:https://doi.org/10.60785/opus-2338.
[30] Pinku Yadav, Vibhutesh Kumar Singh, Thomas Joffre, Olivier Rigo, Corinne Arvieu, Emilie Le Guen,
and Eric Lacoste. Inline drift detection using monitoring systems and machine learning in selective laser
melting. Advanced Engineering Materials, 22(12):2000660, 2020. URL: https://onlinelibrary.wiley.
com/doi/abs/10.1002/adem.202000660, arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/
adem.202000660, doi:https://doi.org/10.1002/adem.202000660.
[31] Bang Xiang Yong, Yasmin Fathy, and Alexandra Brintrup. Bayesian autoencoders for drift detection in
industrial environments. In 2020 IEEE International Workshop on Metrology for Industry 4.0 and IoT,
pages 627–631, 2020. doi:10.1109/MetroInd4.0IoT48571.2020.9138306.
[32] Ray Y. Zhong, Xun Xu, Eberhard Klotz, and Stephen T. Newman. Intelligent manufacturing in the
contextofindustry4.0: Areview.Engineering,3(5):616–630,2017.URL:https://www.sciencedirect.
com/science/article/pii/S2095809917307130, doi:https://doi.org/10.1016/J.ENG.2017.05.015.16 A METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT DETECTION
Appendix A. Data generation with polynomials
In this section, we demonstrate the data generation method introduced in Section 3 along
an example involving a polynomial f : R6 R R of degree five, i.e.
× →
5
(cid:88)
f(w,x) = w xi
i
·
i=0
with w R6. We simulate 2000 process executions and thus sample 2000 process curves. The
∈
shape of each curve is defined by its support points. We are only interested in its curvature
in I = [0,4]. First, we want to add a condition onto the begin and end of the interval, namely
that f(w,0) = 4 and f(w,4) = 5. Moreover, we would like to have a global maximum at
x = 2, which means the first order derivative
4
(cid:88)
∂1f(w,2) = i w 2i−1
x i
· ·
i=1
should be zero and its second order derivate
3
(cid:88)
∂2f(w,2) = i (i 1) w 2i−2
x i
· − · ·
i=1
should be smaller than zero. Here, we want it to be 1. Finally, we want to the curve to be
−
concave at around x = 1. All in all, these conditions result into the following equations,
−
some of them are visualized in Figure 10:
∂0f(w,2) = 7 ∂1f(w,2) = 0 ∂2f(w,2) = 1
x x x
−
∂0f(w,0) = 4 ∂0f(w,4) = 5 ∂2f(w,1) = 1
x x x
−
Then, we let the data drift at some particular features. We simulate a scenario, where the
peak at x0 and x1 moves from the x-position 2 to 3 during the process executions t = 1000
1 0
until t = 1300. Thus, we let x0 and x1 drift from 2 to 3, resulting in a change of position
1 0
of the peak. We let the corresponding y-values y0 = 7 and y1 = 0 unchanged. Now, we can
1 0
solve each of the 2000 optimization problems, which results in 2000 sets of coefficients for
each process curve, such that the conditions are satisfied. By evaluating f with the retrieved
coefficients in our region of interest [0,4], we get 2000 synthesized process curves with a drift
present at our defined drift segment from t = 1000 until t = 1300.A METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT DETECTION 17
7.0 concave 7.0 7.0
6.5 6.5 6.5
6.0 6.0 6.0
5.5 5.5 5.5
5.0 5.0 5.0
4.5 4.5 4.5
4.0 4.0 concave 4.0 concave
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
t = 1000 t = 1250 t = 1300
Fig. 10. Visualization of some process curves in the example dataset. The
red dots indicate support points with first order information given. The green
line visualizes the slope at the green dot, encoded by the condition for the
first derivative. The purple dashed line indicates the curvature at the corre-
sponding x-value, encoded by the condition for the second derivative. From
t = 1000 to t = 1300, the x-value of the maximum moves from 2 to 3.
4 7.0 t=1000
t=1300
t=1150
6.5 x01 7.0
3 6.5
x00(t) 6.0 6.0
x01(t) 5.5Y
2 x t02 =(t) 1000 5.5 5.0
t=1150 4.5
t=1300 5.0 4.0
1 17502000
0 0 250 500 7 P50 rocess10 ex0 e0 cutio1 n2 s50 1500 1750 2000 44 .. 05 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.0 0.5 1.0 1.5 x2.0 2.5 3.0 3.5 4.0 0250500750Pr1 o0 ce0s0sex1 ec2 u5 ti0ons1500
Fig. 11. Visualization of the drift applied on x0 in this example, with respec-
1
tive curves. The left figure shows how the x0 values change over time. Only
i
x0 changes, as by our drift definition from the process executions t = 1000
1
until t = 1300 linearly from 2 to 3, the others remain unchanged. The middle
figure shows the respective curves, color-coded to the dots in the left figure.
Appendix B. TAUC vs AUC
In this section we explore in depth the similarities and differences of the TAUC introduced
in Section 4 and the established AUC. This is done along synthetic predictions.18 A METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT DETECTION
1.0
0.8
0.8
0.6
0.4 0.6 sTAUC(0.74)
TAUC(0.06)
0.2 0.4 AUC(0.58)
0.0 0.2
−0.2
0.0
0 200 400 600 800 1000 0.0 0.2 0.4 0.6 0.8 1.0
FPR
Fig. 12. Predictionofadetectorthatlagsbehindthegroundtruth(left)and
its curves underneath the TAUC and AUC (right).
B.1. Lagged prediction. The first example we look at is a typical scenario that appears
if window-based approaches are used, namely that the prediction lags a bit behind of the
true window, but still the detector overlaps a significant proportion of the drift segment (see
Figure 12. Other than the TPR, the sOLS rewards these predictors and thus the sTAUC
shows a larger value than the AUC.
B.2. Change point detection. Another typical scenario is that a detector shows signifi-
cantly large values at the start and end of the true drift segment, but sag in between (see
Figure 13). This could appear when using methods based on detecting change points. In
principal, thedetectorcorrectlyidentifiesthetemporalcontextofthedriftsegment, although
showing lower scores while the curves drift. Such predictions also score higher values in the
sTAUC than the AUC.
1.0
0.8
0.6 0.8
0.4 0.6 sTAUC(0.85)
TAUC(0.08)
0.2 0.4 AUC(0.76)
0.0
0.2
−0.2
0.0
0 200 400 600 800 1000 0.0 0.2 0.4 0.6 0.8 1.0
FPR
Fig. 13. Prediction of a detector that shows high scores at the boundary of
the true drift segment only (left) and its curves underneath the TAUC and
AUC (right).
B.3. Varyinglengthandpositionofpredictedsegments. AsituationwherethesTAUC
coincides with the AUC mostly is in when only one true and predicted drift segment exist
(see Figure 14). In cases where the center of the predicted segment coincides with the center
of the true segment, the AUC and sTAUC match almost exactly when the length of the
predicted segment is varied (see left graphic in Figure 15). If the predicted segment has fixed
SLO/SLOs/RPT
SLO/SLOs/RPTA METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT DETECTION 19
length that equals the length of the true segment and the position of its center is varied from
50 to 350, AUC and sTAUC coincide mostly, but the sTAUC shows a faster rise when the
predicted segment overlaps with the true segment due to the effects explained in Section B.1.
0.4
0.3
0.2
0.1
0.0
−0.1
0 50 100 150 200 250 300 350 400
Fig. 14. Situationwithsinglepredictedsegment(reddashed)andsingletrue
segment (green area).
1.0 TAUC
0.8 sTAUC
0.8 AUC
TAUC 0.6
0.6 sTAUC
AUC 0.4
0.4
0.2
0.2
25 50 75 100 125 150 175 50 100 150 200 250 300
Lengthofpredictedsegment Centerofpredictedsegment
Fig. 15. Behavior of sTAUC, TAUC, and AUC when length and position of
predicted segment varies.
Appendix C. TAUC for trivial detector
To get a better understanding of the TAUC, we showcase the behavior on trivial detectors
based on the structure of the ground truth. Suppose two pair of points (FPR ,OLS ) and
i i
(FPR ,OLS ) of the constructed curve. Then the two methods for computing the TAUC
i+1 i+1
are the following:
Trapezoidal rule:
•
Construct the curve by linearly interpolating OLS and OLS in between FPR
i i+1 i
and FPR and then calculate the area under the curve by using the trapezoidal
i+1
integration rule.
Step rule:
•
ConstructthecurvebyfillingthevaluesinbetweenFPR andFPR withaconstant
i i+1
value of OLS and then calculate the area under the curve by using the step rule.
i
For example, take the trivial detector that always predicts a drift, called AlwaysGuesser.
Then we receive the two points (0,0) and (1, P) as the only two points of the curve, where
k
erocS erocS20 A METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT DETECTION
P denotes the portion of drifts in y and k denotes the number of drift segments in y. In case
of the step function, the computed score will always be 0, since the constructed curve only
contains one step from [0,1) with a OLS-value of 0, and only reaches a OLS-value of P when
k
reaching a FPR of 1 on the x-axis. Hence, the area under this constructed curve is always
0. When using the trapezoidal rule, we linearly interpolate the two obtained trivial points of
the curve, thus constructing a line from (0,0) to (1, P). The TAUC is then given by the area
K
under this line, which is equal to P . Now suppose a detector which never indicates a drift,
2k
called NeverGuesser. Then we receive (0,0) as our only point, which does not construct a
curve and thus does not have an area under it. Hence, the TAUC for this trivial detection is
0 in both cases.
TAUC using step rule =0.26 TAUC using trapezoidal integration rule =0.25
0.5 0.5
0.4 0.4
0.3 0.3
0.2 0.2
0.1 0.1
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
FPR FPR
Fig. 16. Visualization of a concrete curve used to calculate the TAUC with
its TAUC-score. Left figure shows the constructed curve when using the step
rule, while the right figure shows the curve when calculating the TAUC using
the trapezoidal integration rule.
In order to investigate how the TAUC behaves with an increasing number of segments k
in y, we simulate such inputs with a trivial detection and compute the resulting values for
the TAUC. We choose an input length of n = 1000. When using the step rule, the TAUC is
always 0 as expected, since the only step always retains its area under the curve of 0. But
when looking at the obtained TAUC values when using the trapezoidal integration rule, we
can clearly see the TAUC decreasing when k increases.
SLO SLOA METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT DETECTION 21
Step rule Trapezoidal integration rule
k= 1 k= 1
k= 2 k= 2
0.30 k= 3 0.30 k= 3
k= 4 k= 4
0.25 k= 5 0.25 k= 5
k= 6 k= 6
k= 7 k= 7
0.20 k= 8 0.20 k= 8
k= 9 k= 9
0.15 k= 10 0.15 k= 10
0.10 0.10
0.05 0.05
0.00 0.00
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
FPR FPR
Fig.17. VisualizationofthebehaviouroftheconstructedcurvefortheTAUC
on increasing number of segments k. The left figure shows that the TAUC for
the computation with the step rule always remains 0. The right figure shows
that the area under the line decreases with increasing k, resulting in a lower
TAUC value in case of the trapezoidal integration rule.
This decreasing behaviour can be approximated by 1 , since the TAUC for a trivial detec-
2k
tion with k segments in case of the trapezoidal rule can be computed with P and 0 < P 1.
2k ≤
Thus, the limit of the TAUC computed with the trapezoidal integration rule with increasing
k follows as:
P
lim = 0
k→∞ 2k
0.5 TAUC scores
1
2k
0.4
0.3
0.2
0.1
0.0
0 100 200 300 400 500
k
Fig. 18. Visualization of the TAUC with the trapezoidal integration rule,
when increasing k, alongside an approximation 1 . The TAUC gets closer to
2k
0 with increasing k.
SLO
CUAT
SLO