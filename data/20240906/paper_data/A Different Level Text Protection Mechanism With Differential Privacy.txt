A Different Level Text Protection Mechanism With Differential
Privacy
Qingwen Fu
September 6, 2024
1 Abstract
A lot of work has been done to address privacy is-
suesLyuetal.(2020);Aniletal.(2021);Dupuyetal.
Withthewidespreadapplicationofdifferentialpri- (2022);Lietal.(2021)totrainlanguagemodelsusing
vacy in text protection, however, the current text differentialprivacy(DP)Dworketal.(2006),whichis
cleaningmechanismbasedonmetriclocaldifferential considered the standard for privacy-preserving com-
privacy (MLDP) is not applicable to non-metric se- puting. These methods protect the data source by
mantic similarity measurement, and cannot achieve adding noise to the gradient or training data. How-
a good trade-off between privacy and practicality. ever,theyrequireserviceproviderstocollectrawdata
And currently when we perform differential privacy for LM training, which may still cause privacy leak-
on long text data, all text data will be perturbed. age.
Thismethodofperturbingalltextsmayberelatively Inordertofundamentallysolvetheprivacyleakage
effectivefordownstreamtasksonsomedatasets,but problem, data needs to be fundamentally protected.
if applied to long text data, it may have a great im- Typically, these privacy mechanisms Feyisetan et al.
pact on the overall meaning of the text. Therefore, (2019, 2020); Yue et al. (2021) work by replacing the
in this article, we propose to use the weights of dif- originaltokensintheoriginaldocumentwithnewto-
ferent words in the pre-trained model to assign dif- kens extracted from the output token set. To gener-
ferent weight parameters to words of different im- ateacleanedtextdocument. Specifically,theyadopt
portance. Perform differential perturbations. In ad- metric local differential privacy (MLDP, also known
dition to conducting inference attacks, we also use as dχ-privacy) to provide privacy and practicality
largemodelstoperformprivacyandvaliditytestson guarantees. MLDPChatzikokolakis et al. (2013) in-
our perturbed data. herits the idea of DP and ensures that the output of
anyadjacentinputtokensisindistinguishabletopro-
tect the original tokens from being inferred. On the
2 Introduction other hand, MLDP preserves the utility of the puri-
fied text by assigning higher sampling probabilities
In many natural language processing (NLP) appli- to tokens that are semantically closer to the original
cations, input text often contains sensitive informa- tokens. In these mechanisms, any metric distance
tion that can infer the identity of a specific person (such as Euclidean distance) can be used to measure
Jegorova et al. (2022). In addition, legal restrictions the semantic similarity between tokens.
such as CCPA and GDPR may further restrict the In the paper Chen et al. (2022)”, an MLDP-based
sharing of sensitive text data. This makes it difficult concept is proposed to assign a smaller custom out-
forNLPserviceproviderstocollecttrainingdataun- put set to each input token to achieve token-level
less the privacy concerns of data owners (including privacy protection. This method is an improvement
individuals and institutions) are properly addressed. on the santextYue et al. (2021) method, which in-
1
4202
peS
5
]LC.sc[
1v70730.9042:viXracreases the text perturbation rate without reducing Raghunathan (2020) proposed a classification for re-
the privacy protection effect by limiting the size of covering sensitive attributes or parts of original text
the output set. The custom parameter K can be ad- from text embeddings output by popular LMs with-
justed to determine the output set size of each input out relying on the structure or pattern of the input
token to achieve different utility-privacy trade-offs, text. Carlini et al. (2021) demonstrated a black-box
and an improved CusText+ mechanism is proposed attack against GPT-2, capable of extracting verba-
to skip stop words when sampling to achieve higher tim text of the training data. These studies show
utility. This analysis does improve the perturbation that privacy attacks on LMs are realistic and dam-
efficiencyofwordsinthetexttoacertainextent,but aging, so it is crucial to develop defenses with strict
accordingtoallpreviousstudies,theytreateveryto- safeguards.
ken that appears in the text equally, which actually Secondly, in terms of Differential Privacy (DP)
perturbs all tokens in the text equally, which may and its application in NLP, DP has become the de
not cause much performance impact on datasets for facto standard for statistical analysis. For example,
specific tasks. However, it will cause loss of mean- some research attempts to inject high-dimensional
ingincommonlongtexts,especiallyinsomemedical DP noise into text representations Feyisetan et al.
datasets or long text novels. If all words are treated (2019, 2020); Xu et al. (2020) but these methods fail
equally and deemed equally important, and are per- to achieve a good balance between privacy and util-
turbed to the same extent, this will greatly affect ity, mainly because of the “dimensionality Curse”.
the effectiveness of the text data and lose some of Anotherapproachistolearnprivatetextrepresenta-
the information we need. Therefore, we propose a tions through adversarial training Xie et al. (2017);
method based on a pre-trained BERT model. Using Coavoux et al. (2018), where the adversary model is
the BERT pre-trained model, the attention weights trained to infer sensitive information together with
ofalltokensinthesampleareextracted,andthenthe themastermodel,whilethemastermodelistrained
weightsofthemulti-headmulti-layerTransformerare to maximize the adversary’s loss and minimize the
averaged and regularized. This regularized weight is main learning objective.
usedtosymbolicallyrepresenttheimportanceofeach Third, the application of local differential privacy
wordinthesample. Accordingtothisimportancepa- (LDP) also plays an important role in NLP. LDP al-
rameter,wordsofdifferentimportanceareselectively lowsdataownerstosanitizedatalocallybeforesend-
perturbed. This can reduce the damage to the effec- ing it to the server. This means data owners can
tiveness of the text to a certain extent. We tested it share information without revealing the content of
ontwopublicdatasets,SST-2andQNLI,andproved their original data. In NLP applications, LDP is
theeffectivenessofourmethodofextractingwordsof particularly valuable because it can collect and an-
different importance. alyze text data while protecting user privacy. For
example, the LDP mechanism can be used to gener-
ate sanitized text datasets that can be used to train
3 Related Work
machine learning models without exposing personal
information. The challenge of LDP is to achieve pri-
Whendiscussingprivacyrisksandprotectionmea- vacy protection while maintaining data practicality,
sures in natural language processing (NLP), we can especially when dealing with text data with complex
see three main research directions: research on pri- structure and high-dimensional features.
vacyattacksondeeplearningmodels,differentialpri- Tosumup,theNLPfieldfacesmultiplechallenges
vacy(DP)anditsapplicationinNLP,andtheappli- when dealing with privacy protection issues. On the
cation of local differential privacy (LDP). one hand, effective defense strategies need to be de-
First, privacy attacks against deep learning mod- velopedagainstprivacyattacksonLMs;ontheother
els, especially language models (LMs), have become hand, differential privacy and local differential pri-
an important research area. For example, Song and vacy provide a series of solutions to protect the pri-
2vacy of text data. These studies not only help im- the original token from its adjacent tokens. In prac-
prove the privacy protection capabilities of existing tice, we may standardize the scoring function u, nor-
technologies,butalsoprovideimportantguidancefor malizing its sensitivity ∆u to a fixed value (e.g., 1),
futureprivacyprotectionresearchinthefieldofNLP. so that the selection probability for each output to-
keny foraninputtokenxissolelyrelatedtou(x,y),
considering that ϵ and ∆u are predetermined, and a
4 Preliminaries
largeru(x,y)resultsinahighersamplingprobability.
In an NLP task, we assume each document D =
BeforewedelvedeeperintoourCusTexttechnique,
⟨R ⟩m contains m records, and each record R =
let’s first briefly review some fundamental concepts, i i=1
⟨t ⟩n contains n tokens. We define the task of text
including ϵ-differential privacy and the exponential j j=1
sanitization as follows: Given an input document D
mechanism.
containing sensitive information, a set of all possible
Definition 1 (ϵ-differential privacy) Given a pri-
input tokens X, a set of all possible output tokens
vacy parameter ϵ ≥ 0, for all adjacent input pairs
Y, and a differential privacy mechanism M (e.g., the
x,x′ ∈ X, and for every possible output y ∈ Y, a
EMusedinthiswork),itappliesthemechanismM to
randomized mechanism M satisfies ϵ-differential pri-
each input token t ∈D, replacing it with an output
vacy if it adheres to the following condition: j
token t′ ∈Y if t ∈X. All tokens after replacement
j j
Pr[M(x)=y] form the sanitized document, i.e., D′ = ⟨R′⟩m and
≤eϵ i i=1
Pr[M(x′)=y] R′ =⟨t′⟩n .
j j=1
FollowingpreviousstudiesXuetal.(2020);Feyise-
In this definition, a smaller ϵ indicates a higher level
tanetal.(2019);Yueetal.(2021);Chenetal.(2022);
of privacy protection. Theoretically, ϵ-DP ensures
Qu et al. (2021), we still adopt a semi-honest threat
that even adversaries with infinite computing power
model in the context of local differential privacy. In
cannot distinguish between the probability distribu-
this model, the data owner only submits sanitized
tions of two adjacent inputs, as their probabilities of
documents to the service provider. However, a ma-
producingthesameoutputy arecloselymatched. In
licious service provider may try to extract sensitive
the context of Natural Language Processing (NLP),
information from the received data. We assume that
any pair of input tokens that produce the same out-
the adversary can only obtain the sanitized text and
put set Y are considered adjacent. This paper con-
all algorithms and mechanisms are public and trans-
tinues to use this definition for adjacent inputs.
parent. In addition, we also assume that the adver-
Definition 2 (Exponential Mechanism).
sary has unlimited computing power.
Given a scoring function u : X ×Y → R, the expo-
nential mechanism M(X,u,Y) achieves ϵ-differential
privacybyrandomlyselectinganoutputtokeny ∈Y 5 Method
to perturb the input token x∈X with a probability
proportional to
Our privacy perturbation method is based on the
ϵ·u(x,y)
e 2∆u CusText mechanism. The difference is that we use
theBERTpre-trainedmodeltoassignweightstodif-
Here,u(x,y)representsthescoreoftheoutputtoken
ferent words in the same example. Then we average
y for the input token x. Additionally, the sensitivity
the weights of multiple heads and layers. We remove
of u, denoted as ∆u, for the exponential mechanism
the weights of CLS and septoken and regularize the
(EM) is defined by
weights of other words. Use this weight value to rep-
∆u:=max max |u(x,y)−u(x′,y)| resent the importance of different words. Then we
y∈Y x,x′∈X
combine the CusText mechanism to perform differ-
According to the second definition, lower sensitiv- ent degrees of perturbation for our words of different
ity makes it statistically more difficult to distinguish importance.
3”CusText”isatailoredtextsanitizationframework reliant on the fmap, selects an output tag for each
designed to safeguard privacy by substituting every input tag. This selection is governed by an exponen-
tokenwithinatext. Itcomprisestwoprimarycompo- tial mechanism, and it requires a carefully designed
nents: firstly, a semantic correlation-based mapping scoringfunctionutomaintainabalancebetweenutil-
function, fmap, which identifies the appropriate out- ity and privacy. The function ensures that the rela-
put set for each input token; secondly, a sampling tionship between each input and output tag pair is
function, fsample, that selects new tokens from this capped, with pairs that are semantically closer re-
output set using an exponential mechanism. ceiving higher scores.
Unlike traditional SANTEXT methods, CusText ScoringFunction,custextisbasedonthesamesim-
enhances the relevance of the output tokens to the ilarity function used in mapping schemes,,e.g., Eu-
originaltokensbycustomizingtheoutputsetforeach clidean distance or cosine similarity based on token-
input token, thus improving the utility of the model. vector,representations Mikolov (2013); Pennington
The development of the mapping function involves et al. (2014).,In general, all similarity measures can
picking tokens from the input set, identifying those be divided into two categories,,negative and posi-
that are semantically closest, and creating a map- tive,,according to the correlation between the score
ping. This mapping is then refined by progressively and semantic proximity.,For example, Euclidean dis-
removing the tokens that have been mapped until tance and cosine similarity are negative,and posi-
a complete mapping is achieved or there are insuffi- tive correlation measures, respectively, because the
cient tokens left to continue. This strategy ensures smaller the Euclidean distance,and the larger the co-
that every input token is paired with at least one sine value between two vectors,,means that the se-
neighboringtoken, preservingtheeffectivenessofthe mantic proximity of their corresponding tokens is
privacy measures. higher.,Next, we will design scoring functions for
these two types of similarity,measurements.
The following is our own perturbation method
Algorithm 1 CusText Mapping Mechanism
based on words of different importance. Our method
1: Input: Customization parameter K, input set
mainly uses the pre-extracted words of different im-
X, output set Y =X, similarity measure d
portanceasoursensitivewordlist,andthenusesthe
2: Output: Mapping Function f map
custext method to perturb these sensitive word lists.
3: while |X|≥K do
Aggressivemechanism. Whenweselecttheimpor-
4: Pick an arbitrary token x from X
tant vocabulary list, if we adopt an aggressive mech-
5: Initialize an output set Y′ ={x} for x
anism, we can perturb all the words in the sensitive
6: for all y ∈Y \{x} do
vocabularylistwithoutdifference, butthismayhave
7: Compute the similarity d(x,y) of x and y
agreaterimpactontheoriginalsemanticsofthetext,
8: end for
because the same noun or the same verb will be per-
9: Add the top-(K −1) tokens that are seman-
turbedintodifferentwords,whichwillcausethetext
tically closest to x to Y′ based on d(.,.)
semantics to be incoherent. The result for short text
10: for all x′ ∈Y′ do
may be less than the effect on long text.
11: Assign the output set of x′ as Y′
Conservativemechanism. Whenthesamesensitive
12: end for
word appears multiple times in a sample, we give it
13: Update X ←X\Y′ and Y ←Y \Y′
the same perturbation result. This is a conservative
14: end while
mechanismandmaybeeasiertoattack. Butitispos-
15: Perform Lines 2–9 for the remaining tokens in X
sible to give the same nouns the same perturbation
and Y with customization parameter K′ =|X|
inthecontentoflongtexts. Inthisway,therelation-
16: return f map
shipbetweenwordssuchassubjectandpredicatecan
be better preserved, and its semantic structure can
Samplingfunction: Thefsamplefunction, whichis be preserved. It is possible to protect sensitive infor-
4Algorithm 2 Different levels of protection mecha- training samples and 1,800 test samples. The
nism evaluation metric is accuracy.
1: Input: Original document D = (R i)m i=1, sam-
pling function f , different level sensitive • QNLI: A dataset for sentence pair classifica-
sample
word list S tionwith105,000trainingsamplesand5,200test
2: Output: different level sanitized document D′ samples. Accuracyisalsousedastheevaluation
3: Initialize the sanitized document D′ =∅ metric here.
4: for all record R∈D do
5: Initialize the sanitized record R′ =∅ In our approach, for both the SST-2 and QNLI
6: for all token x∈R do datasets, we first identify the most and least impor-
7: if token is used and x∈S then tant words, quantified as the top and bottom 10%,
8: x′ ←f sample(x) and append to R′ 20%, 30%, 40%, 50%, and 60% based on the atten-
9: else tion scores. These words are considered as the sensi-
10: Append x to R′ tive words that need to be perturbed. We record the
11: end if number of words actually perturbed during training
12: end for and compare it under similar total perturbation con-
13: Add R′ to D′ ditions to gauge the effectiveness of our method. We
14: end for use the vocabulary from CounterFitting in GloVe,
15: return D′ and apply both Euclidean distance and cosine simi-
larity as measures for comparing GloVe vectors. The
sensitive word list is derived from the probabilities
mationwhilestillhavingbettersemanticinformation associated with different words in the pre-trained
andtextinformation. Theabovetwomechanismscan model. For each downstream task, we set the max-
be used to process different categories of text data, imum sequence length to 128 and limit the training
andcanbefreelyselectedasneeded. Combinedwith to 3 epochs. On both SST-2 and QNLI datasets, the
ourselectionmechanismforwordsofdifferentdegrees batch size is set to 64. We use bert-base-uncased
of importance, the text can be protected more flexi- as the pre-trained model with an increased learning
bly to better achieve a balance between privacy and rate of 2×10−5. The experiments are conducted on
utility. an A100 GPU.
The second part of our experimental analysis fo-
cuses on demonstrating the effectiveness of our ap-
6 Experiment
proach. In this phase, we perturb words of vary-
ing degrees of importance—specifically, 5%, 10%,
. Experimental Setup,
and 20% of the words determined by our quantifier.
5.1 Experimental Setup Following Feyisetan et al.
We then evaluate both the privacy and effectiveness
(2020); Yue et al. (2021) We selected two datasets
of the perturbed datasets using several established
from the GLUE benchmark Wang (2018) in our ex-
mechanisms.
periments,bothofwhichcontainInourexperimental
section, we aim to demonstrate the efficacy of using
• Evaluation Mechanisms: We apply various
attentionmechanismparameterstorepresenttheim-
metricstoassesstheprivacylevelsandtheutility
portanceofdifferentwordswithinasample. Thissec-
of the datasets after perturbation.
tionisdividedintotwoparts,eachutilizingthepublic
datasets SST-2 and QNLI to validate our method.
• Data Perturbation: Wemethodically perturb
Datasets Description:
thewordsidentifiedashavinghigh,medium,and
• SST-2: A widely-used movie review dataset low importance to measure the impact on the
for sentiment classification, consisting of 67,000 dataset’s utility and privacy.
5• Analysis of Important Words: This method
also allows us to count and calculate the dis-
tribution of words based on their importance.
We identify and examine some relatively high-
importance words, observe the categories they
belong to, and analyze their patterns.
This structured evaluation helps in understanding
howdifferentlevelsofperturbationaffecttheprivacy-
security balance and the overall effectiveness of the
sensitive data we intend to protect.
Figure 3: Only Disturb Test Data for QNLI
6.1 Experiment Result
Below are some of my experimental results when ϵ
equals to 3.
Figure 4: Both Disturb Train and Test Data for
QNLIs
Figure 1: Only Disturb Test Data for SST2
Figure 2: Both Disturb Train and Test Data for
SST2
6Only Test Top N Accuracy Conservative Aggressive
Train/Test
SST3 Dataset Data Strategy Strategy
Accuracy
Accuracy
Top 10 0.901 0.88
Top 20 0.8864 0.8656
Top 50 0.840 0.780
Top 30 0.8756 0.8458
Top 40 0.860 0.820
Top 40 0.8623 0.8186
Top 30 0.870 0.850
Top 50 0.8428 0.801
Top 20 0.883 0.863 Top 60 0.821 0.79
Top 10 0.892 0.881
Table 3: Accuracy comparison between conservative
Bottom 50 0.865 0.820
and aggressive strategies
Bottom 40 0.874 0.840
Bottom 30 0.882 0.865
6.2 result analysis
Bottom 20 0.891 0.876
Bottom 10 0.901 0.887
Fortheperturbationofdatawithdifferentimpor-
No Disturbance 0.905 - tance, we conducted experiments on the SST-2 and
Full Disturbance 0.72 - QNLIdatasets. Forfaircomparisoninthefuture,we
chose Glove as the token embedding and controlled
Table1: AccuracyresultsfortheQNLIdatasetunder other variables to be the same. Table 1 shows the
different conditions. results of perturbing words of different importance
on the SST-2 dataset while keeping the training set
unchanged. For the same test set, words of differ-
ent importance are perturbed while keeping ϵ = 3
unchanged. As can be seen from the figure, when
thetestsetdataisperturbedwithbasicallythesame
Only Test
Train/Test amount of data, the result of perturbing more im-
QNLI Dataset Data
Accuracy portant words is worse than that of perturbing less
Accuracy
important words, which also proves that our vocab-
Top 60 0.8331 0.79 ulary extraction method is correct. Figure 2 shows
Bottom 60 0.8451 0.8204 the results of perturbing the training data and test
Top 50 0.8382 0.801 data at the same time. The results show that when
Bottom 50 0.8635 0.8412 perturbing the same number of words of different
Top 40 0.8416 0.8186 importance, perturbing more important words has a
Bottom 40 0.8802 0.8568 greater impact on the results, which also proves that
Top 30 0.8633 0.8458 our extraction strategy is correct. When we make a
Bottom 30 0.8834 0.8684 horizontal comparison, we find that when we use the
Top 20 0.8777 0.8656 perturbed training set for training, the matching ef-
Bottom 20 0.8904 0.8777 fectwiththetestsetisbetter,whichalsoreflectsthe
Top 10 0.8975 0.88 effectiveness of our method for words of different im-
Bottom 10 0.899 0.8864 portance to a certain extent. When we observe the
No Disturbance 0.9096 - results of the QNLI dataset, we can also draw the
Full Disturbance 0.7133 - aboveconclusions. Therefore,ourTransformer-based
extractionmethodiseffective. Whenweperformdif-
Table2: AccuracyresultsfortheQNLIdatasetunder ferential privacy on the text, we can selectively per-
different conditions. turb words of different importance. Of course, this
7method can also be used as a screening mechanism reconstruction. This method may be more suitable
to help us narrow the search scope of keywords and for identifying important words.
privacy words. Combined with other named entity
recognition and LLM reasoning methods, it can help
7 Conclusion and limitation
us find more effective keywords faster. This will be a
general method.
Conservative method, when we adopt a conserva- Conclusion: Thismethodprovesthatwecanreflect
tive strategy, that is, when we keep the same pertur- the importance of different words in different sen-
bationresultsforthesamewordsinthesamesample, tences through multiple layers of Transformers and
theresultsareasfollows. Herewemainlyanalyzethe the attention weights between them, but more sup-
resultsoftheguardstrategyforthetop10,20,30,40, plementaryexperimentsareneeded. Moreover,when
and 50 emphasized words of qnli (long text length, weapplythismethodtolongtextdata,ouraccuracy
with a greater possibility of the same vocabulary). will be biased due to the limitation of the maximum
It can be observed that when we consider adopting lengthofTransformerandthelongtextlength. This
the guard strategy, the results of the experiment will requires us to combine some other models and find a
be significantly improved. Therefore, we can use the way to obtain longer length data at the same time.
guard strategy in the relatively long policy text pro- We need to do more work on this basis to improve
tection process, which can better maintain semantic its performance. We can do more experiments and
coherence. research on this basis in combination with LLM.
Tokenreasoningattacksandqueryattacksarecar-
riedoutontheperturbedtexttotesttheeffectiveness
8 Future work
of our extraction of data of different importance and
its relevance to privacy. Using a pre-trained BERT
With the recent emergence and development of
model can help infer the possibility of recovering the
LLM, I think we can combine the large oracle model
originaltextfromthepurifiedtext. Byreplacingeach
with the discovery of sensitive data. Combined with
token in the purified text with the ”[MASK]” token
prompts, LLM can identify important and sensitive
andinputtingitintotheBERTmodel,wecangetthe
information in the text. And we can combine LLM
model’spredictedoutputfor”[MASK]”,whichisthe
withthismethodtofiltersensitiveinformationinthe
inferredoriginaltoken. Ifthepredictedoutputisthe
textexceptforspecificcategories,becausesomeother
same as the token of the original input, we consider
informationinthetextthatisnotclassifiedmayalso
the attack attempt to be successful. By calculating
contain some critical sensitive information. This is a
the success rate of all such attacks (rmask), we can
direction worth exploring.
measuretheprivacyprotectionofthetext,whichis1-
rmask. Becauseouralgorithmisbasedonthecustext
algorithm and has not been modified to the original References
algorithm, its effect is the same as custext.
ImportancevocabularyanalysisWhenweusechat- Anil, R., Ghazi, B., Gupta, V., Kumar, R., and Ma-
gpt4 to analyze the words of different importance we nurangsi, P. (2021). Large-scale differentially pri-
extracted, we find that the more important words vate bert. arXiv preprint arXiv:2108.01624.
areoftenthosenouns, pronouns, punctuationmarks,
etc. This is the same as the more important words Carlini, N., Tramer, F., Wallace, E., Jagielski, M.,
in a sentence we understand. However, when we use Herbert-Voss, A., Lee, K., Roberts, A., Brown, T.,
GPT4 and our more important words to reconstruct Song, D., Erlingsson, U., et al. (2021). Extracting
the zero-shot sentence, the reconstructed sentence is training data from large language models. In 30th
very different from our original sentence. Therefore, USENIX Security Symposium (USENIX Security
our method does not perform well under unguided 21), pages 2633–2650.
8Chatzikokolakis, K., Andr´es, M. E., Bordenabe, Li, X., Tramer, F., Liang, P., and Hashimoto, T.
N. E., and Palamidessi, C. (2013). Broadening the (2021). Large language models can be strong
scope of differential privacy using metrics. In Pri- differentially private learners. arXiv preprint
vacy Enhancing Technologies: 13th International arXiv:2110.05679.
Symposium, PETS 2013, Bloomington, IN, USA,
Lyu, L., He, X., and Li, Y. (2020). Differentially
July 10-12, 2013. Proceedings 13, pages 82–102.
private representation for nlp: Formal guarantee
Springer.
and an empirical study on privacy and fairness.
Chen, H., Mo, F., Wang, Y., Chen, C., Nie, J.-Y., arXiv preprint arXiv:2010.01285.
Wang, C., and Cui, J. (2022). A customized text
Mikolov, T. (2013). Efficient estimation of word
sanitization mechanism with differential privacy.
representations in vector space. arXiv preprint
arXiv preprint arXiv:2207.01193.
arXiv:1301.3781.
Coavoux, M., Narayan, S., and Cohen, S. B. (2018). Pennington, J., Socher, R., and Manning, C. D.
Privacy-preserving neural representations of text. (2014). Glove: Global vectors for word represen-
arXiv preprint arXiv:1808.09408. tation. In Proceedings of the 2014 conference on
empirical methods in natural language processing
Dupuy, C., Arava, R., Gupta, R., and Rumshisky,
(EMNLP), pages 1532–1543.
A.(2022). Anefficientdp-sgdmechanismforlarge
scale nlu models. In ICASSP 2022-2022 IEEE In- Qu, C., Kong, W., Yang, L., Zhang, M., Bendersky,
ternational Conference on Acoustics, Speech and M., and Najork, M. (2021). Natural language un-
Signal Processing (ICASSP), pages 4118–4122. derstanding with privacy-preserving bert. In Pro-
IEEE. ceedingsofthe30thACMInternationalConference
on Information & Knowledge Management, pages
Dwork, C., McSherry, F., Nissim, K., and Smith, A.
1488–1497.
(2006). Calibrating noise to sensitivity in private
data analysis. In Theory of Cryptography: Third Song, C. and Raghunathan, A. (2020). Information
Theory of Cryptography Conference, TCC 2006, leakage in embedding models. In Proceedings of
New York, NY, USA, March 4-7, 2006. Proceed- the 2020 ACM SIGSAC conference on computer
ings 3, pages 265–284. Springer. and communications security, pages 377–390.
Feyisetan, O., Balle, B., Drake, T., and Diethe, Wang,A.(2018). Glue: Amulti-taskbenchmarkand
T. (2020). Privacy-and utility-preserving textual analysisplatformfornaturallanguageunderstand-
analysis via calibrated multivariate perturbations. ing. arXiv preprint arXiv:1804.07461.
InProceedings of the 13th international conference
Xie, Q., Dai, Z., Du, Y., Hovy, E., and Neubig, G.
on web search and data mining, pages 178–186.
(2017).Controllableinvariancethroughadversarial
feature learning. Advances in neural information
Feyisetan, O., Diethe, T., and Drake, T. (2019).
processing systems, 30.
Leveraginghierarchicalrepresentationsforpreserv-
ing privacy and utility in text. In 2019 IEEE In- Xu, Z., Aggarwal, A., Feyisetan, O., and Teissier,
ternational Conference on Data Mining (ICDM), N. (2020). A differentially private text perturba-
pages 210–219. IEEE. tion method using a regularized mahalanobis met-
ric. arXiv preprint arXiv:2010.11947.
Jegorova, M., Kaul, C., Mayor, C., O’Neil, A. Q.,
Weir, A., Murray-Smith, R., and Tsaftaris, S. A. Yue, X., Du, M., Wang, T., Li, Y., Sun, H., and
(2022). Survey: Leakage and privacy at inference Chow,S.S.(2021). Differentialprivacyfortextan-
time. IEEE Transactions on Pattern Analysis and alyticsvianaturaltextsanitization. arXivpreprint
Machine Intelligence, 45(7):9090–9108. arXiv:2106.01221.
9