[
    {
        "title": "Image captioning for Brazilian Portuguese using GRIT model",
        "authors": "Rafael Silva de AlencarWilliam Alberto Cruz CastañedaMarcellus Amadeus",
        "links": "http://arxiv.org/abs/2402.05106v1",
        "entry_id": "http://arxiv.org/abs/2402.05106v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05106v1",
        "summary": "This work presents the early development of a model of image captioning for\nthe Brazilian Portuguese language. We used the GRIT (Grid - and Region-based\nImage captioning Transformer) model to accomplish this work. GRIT is a\nTransformer-only neural architecture that effectively utilizes two visual\nfeatures to generate better captions. The GRIT method emerged as a proposal to\nbe a more efficient way to generate image captioning. In this work, we adapt\nthe GRIT model to be trained in a Brazilian Portuguese dataset to have an image\ncaptioning method for the Brazilian Portuguese Language.",
        "updated": "2024-02-07 18:57:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05106v1"
    },
    {
        "title": "Language-Based Augmentation to Address Shortcut Learning in Object Goal Navigation",
        "authors": "Dennis HoftijzerGertjan BurghoutsLuuk Spreeuwers",
        "links": "http://arxiv.org/abs/2402.05090v1",
        "entry_id": "http://arxiv.org/abs/2402.05090v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05090v1",
        "summary": "Deep Reinforcement Learning (DRL) has shown great potential in enabling\nrobots to find certain objects (e.g., `find a fridge') in environments like\nhomes or schools. This task is known as Object-Goal Navigation (ObjectNav). DRL\nmethods are predominantly trained and evaluated using environment simulators.\nAlthough DRL has shown impressive results, the simulators may be biased or\nlimited. This creates a risk of shortcut learning, i.e., learning a policy\ntailored to specific visual details of training environments. We aim to deepen\nour understanding of shortcut learning in ObjectNav, its implications and\npropose a solution. We design an experiment for inserting a shortcut bias in\nthe appearance of training environments. As a proof-of-concept, we associate\nroom types to specific wall colors (e.g., bedrooms with green walls), and\nobserve poor generalization of a state-of-the-art (SOTA) ObjectNav method to\nenvironments where this is not the case (e.g., bedrooms with blue walls). We\nfind that shortcut learning is the root cause: the agent learns to navigate to\ntarget objects, by simply searching for the associated wall color of the target\nobject's room. To solve this, we propose Language-Based (L-B) augmentation. Our\nkey insight is that we can leverage the multimodal feature space of a\nVision-Language Model (VLM) to augment visual representations directly at the\nfeature-level, requiring no changes to the simulator, and only an addition of\none layer to the model. Where the SOTA ObjectNav method's success rate drops\n69%, our proposal has only a drop of 23%.",
        "updated": "2024-02-07 18:44:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05090v1"
    },
    {
        "title": "Mamba-UNet: UNet-Like Pure Visual Mamba for Medical Image Segmentation",
        "authors": "Ziyang WangJian-Qing ZhengYichi ZhangGe CuiLei Li",
        "links": "http://arxiv.org/abs/2402.05079v1",
        "entry_id": "http://arxiv.org/abs/2402.05079v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05079v1",
        "summary": "In recent advancements in medical image analysis, Convolutional Neural\nNetworks (CNN) and Vision Transformers (ViT) have set significant benchmarks.\nWhile the former excels in capturing local features through its convolution\noperations, the latter achieves remarkable global context understanding by\nleveraging self-attention mechanisms. However, both architectures exhibit\nlimitations in efficiently modeling long-range dependencies within medical\nimages, which is a critical aspect for precise segmentation. Inspired by the\nMamba architecture, known for its proficiency in handling long sequences and\nglobal contextual information with enhanced computational efficiency as a State\nSpace Model (SSM), we propose Mamba-UNet, a novel architecture that synergizes\nthe U-Net in medical image segmentation with Mamba's capability. Mamba-UNet\nadopts a pure Visual Mamba (VMamba)-based encoder-decoder structure, infused\nwith skip connections to preserve spatial information across different scales\nof the network. This design facilitates a comprehensive feature learning\nprocess, capturing intricate details and broader semantic contexts within\nmedical images. We introduce a novel integration mechanism within the VMamba\nblocks to ensure seamless connectivity and information flow between the encoder\nand decoder paths, enhancing the segmentation performance. We conducted\nexperiments on publicly available MRI cardiac multi-structures segmentation\ndataset. The results show that Mamba-UNet outperforms UNet, Swin-UNet in\nmedical image segmentation under the same hyper-parameter setting. The source\ncode and baseline implementations are available.",
        "updated": "2024-02-07 18:33:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05079v1"
    },
    {
        "title": "LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation",
        "authors": "Jiaxiang TangZhaoxi ChenXiaokang ChenTengfei WangGang ZengZiwei Liu",
        "links": "http://arxiv.org/abs/2402.05054v1",
        "entry_id": "http://arxiv.org/abs/2402.05054v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05054v1",
        "summary": "3D content creation has achieved significant progress in terms of both\nquality and speed. Although current feed-forward models can produce 3D objects\nin seconds, their resolution is constrained by the intensive computation\nrequired during training. In this paper, we introduce Large Multi-View Gaussian\nModel (LGM), a novel framework designed to generate high-resolution 3D models\nfrom text prompts or single-view images. Our key insights are two-fold: 1) 3D\nRepresentation: We propose multi-view Gaussian features as an efficient yet\npowerful representation, which can then be fused together for differentiable\nrendering. 2) 3D Backbone: We present an asymmetric U-Net as a high-throughput\nbackbone operating on multi-view images, which can be produced from text or\nsingle-view image input by leveraging multi-view diffusion models. Extensive\nexperiments demonstrate the high fidelity and efficiency of our approach.\nNotably, we maintain the fast speed to generate 3D objects within 5 seconds\nwhile boosting the training resolution to 512, thereby achieving\nhigh-resolution 3D content generation.",
        "updated": "2024-02-07 17:57:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05054v1"
    },
    {
        "title": "Efficient Multi-Resolution Fusion for Remote Sensing Data with Label Uncertainty",
        "authors": "Hersh VakhariaXiaoxiao Du",
        "links": "http://dx.doi.org/10.1109/IGARSS52108.2023.10282851",
        "entry_id": "http://arxiv.org/abs/2402.05045v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05045v1",
        "summary": "Multi-modal sensor data fusion takes advantage of complementary or\nreinforcing information from each sensor and can boost overall performance in\napplications such as scene classification and target detection. This paper\npresents a new method for fusing multi-modal and multi-resolution remote sensor\ndata without requiring pixel-level training labels, which can be difficult to\nobtain. Previously, we developed a Multiple Instance Multi-Resolution Fusion\n(MIMRF) framework that addresses label uncertainty for fusion, but it can be\nslow to train due to the large search space for the fuzzy measures used to\nintegrate sensor data sources. We propose a new method based on binary fuzzy\nmeasures, which reduces the search space and significantly improves the\nefficiency of the MIMRF framework. We present experimental results on synthetic\ndata and a real-world remote sensing detection task and show that the proposed\nMIMRF-BFM algorithm can effectively and efficiently perform multi-resolution\nfusion given remote sensing data with uncertainty.",
        "updated": "2024-02-07 17:34:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05045v1"
    }
]