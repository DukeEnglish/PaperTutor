[
    {
        "title": "ARCollab: Towards Multi-User Interactive Cardiovascular Surgical Planning in Mobile Augmented Reality",
        "authors": "Pratham MehtaHarsha KaranthHaoyang YangTimothy SlesnickFawwaz ShawDuen Horng Chau",
        "links": "http://arxiv.org/abs/2402.05075v1",
        "entry_id": "http://arxiv.org/abs/2402.05075v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05075v1",
        "summary": "Surgical planning for congenital heart diseases requires a collaborative\napproach, traditionally involving the 3D-printing of physical heart models for\ninspection by surgeons and cardiologists. Recent advancements in mobile\naugmented reality (AR) technologies have offered a promising alternative, noted\nfor their ease-of-use and portability. Despite this progress, there remains a\ngap in research exploring the use of multi-user mobile AR environments for\nfacilitating collaborative cardiovascular surgical planning. We are developing\nARCollab, an iOS AR application designed to allow multiple surgeons and\ncardiologists to interact with patient-specific 3D heart models in a shared\nenvironment. ARCollab allows surgeons and cardiologists to import heart models,\nperform gestures to manipulate the heart, and collaborate with other users\nwithout having to produce a physical heart model. We are excited by the\npotential for ARCollab to make long-term real-world impact, thanks to the\nubiquity of iOS devices that will allow for ARCollab's easy distribution,\ndeployment and adoption.",
        "updated": "2024-02-07 18:29:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05075v1"
    },
    {
        "title": "When the Body Became Data: Historical Data Cultures and Anatomical Illustration",
        "authors": "Michael CorrellLaura A. Garrison",
        "links": "http://arxiv.org/abs/2402.05014v1",
        "entry_id": "http://arxiv.org/abs/2402.05014v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05014v1",
        "summary": "With changing attitudes around knowledge, medicine, art, and technology, the\nhuman body has become a source of information and, ultimately, shareable and\nanalyzable data. Centuries of illustrations and visualizations of the body\noccur within particular historical, social, and political contexts. These\ncontexts are enmeshed in different so-called data cultures: ways that data,\nknowledge, and information are conceptualized and collected, structured and\nshared. In this work, we explore how information about the body was collected\nas well as the circulation, impact, and persuasive force of the resulting\nimages. We show how mindfulness of data cultural influences remain crucial for\ntoday's designers, researchers, and consumers of visualizations. We conclude\nwith a call for the field to reflect on how visualizations are not timeless and\ncontextless mirrors on objective data, but as much a product of our time and\nplace as the visualizations of the past.",
        "updated": "2024-02-07 16:32:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05014v1"
    },
    {
        "title": "Exploring the Opportunity of Augmented Reality (AR) in Supporting Older Adults Explore and Learn Smartphone Applications",
        "authors": "Xiaofu JinWai TongXiaoying WeiXian WangEmily KuangXiaoyu MoHuamin QuMingming Fan",
        "links": "http://arxiv.org/abs/2402.04991v1",
        "entry_id": "http://arxiv.org/abs/2402.04991v1",
        "pdf_url": "http://arxiv.org/pdf/2402.04991v1",
        "summary": "The global aging trend compels older adults to navigate the evolving digital\nlandscape, presenting a substantial challenge in mastering smartphone\napplications. While Augmented Reality (AR) holds promise for enhancing learning\nand user experience, its role in aiding older adults' smartphone app\nexploration remains insufficiently explored. Therefore, we conducted a\ntwo-phase study: (1) a workshop with 18 older adults to identify app\nexploration challenges and potential AR interventions, and (2) tech-probe\nparticipatory design sessions with 15 participants to co-create AR support\ntools. Our research highlights AR's effectiveness in reducing physical and\ncognitive strain among older adults during app exploration, especially during\nmulti-app usage and the trial-and-error learning process. We also examined\ntheir interactional experiences with AR, yielding design considerations on\ntailoring AR tools for smartphone app exploration. Ultimately, our study\nunveils the prospective landscape of AR in supporting the older demographic,\nboth presently and in future scenarios.",
        "updated": "2024-02-07 16:09:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04991v1"
    },
    {
        "title": "ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming Learning for Children Aged 6-12",
        "authors": "Liuqing ChenShuhong XiaoYunnong ChenRuoyu WuYaxuan SongLingyun Sun",
        "links": "http://dx.doi.org/10.1145/3613904.3642229",
        "entry_id": "http://arxiv.org/abs/2402.04975v1",
        "pdf_url": "http://arxiv.org/pdf/2402.04975v1",
        "summary": "As Computational Thinking (CT) continues to permeate younger age groups in\nK-12 education, established CT platforms such as Scratch face challenges in\ncatering to these younger learners, particularly those in the elementary school\n(ages 6-12). Through formative investigation with Scratch experts, we uncover\nthree key obstacles to children's autonomous Scratch learning: artist's block\nin project planning, bounded creativity in asset creation, and inadequate\ncoding guidance during implementation. To address these barriers, we introduce\nChatScratch, an AI-augmented system to facilitate autonomous programming\nlearning for young children. ChatScratch employs structured interactive\nstoryboards and visual cues to overcome artist's block, integrates digital\ndrawing and advanced image generation technologies to elevate creativity, and\nleverages Scratch-specialized Large Language Models (LLMs) for professional\ncoding guidance. Our study shows that, compared to Scratch, ChatScratch\nefficiently fosters autonomous programming learning, and contributes to the\ncreation of high-quality, personally meaningful Scratch projects for children.",
        "updated": "2024-02-07 15:55:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04975v1"
    },
    {
        "title": "Chatbots in Knowledge-Intensive Contexts: Comparing Intent and LLM-Based Systems",
        "authors": "Samuel Kernan FreireChaofan WangEvangelos Niforatos",
        "links": "http://arxiv.org/abs/2402.04955v1",
        "entry_id": "http://arxiv.org/abs/2402.04955v1",
        "pdf_url": "http://arxiv.org/pdf/2402.04955v1",
        "summary": "Cognitive assistants (CA) are chatbots that provide context-aware support to\nhuman workers in knowledge-intensive tasks. Traditionally, cognitive assistants\nrespond in specific ways to predefined user intents and conversation patterns.\nHowever, this rigidness does not handle the diversity of natural language well.\nRecent advances in natural language processing (NLP), powering large language\nmodels (LLM) such as GPT-4, Llama2, and Gemini, could enable CAs to converse in\na more flexible, human-like manner. However, the additional degrees of freedom\nmay have unforeseen consequences, especially in knowledge-intensive contexts\nwhere accuracy is crucial. As a preliminary step to assessing the potential of\nusing LLMs in these contexts, we conducted a user study comparing an LLM-based\nCA to an intent-based system regarding interaction efficiency, user experience,\nworkload, and usability. This revealed that LLM-based CAs exhibited better user\nexperience, task completion rate, usability, and perceived performance than\nintent-based systems, suggesting that switching NLP techniques should be\ninvestigated further.",
        "updated": "2024-02-07 15:39:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.04955v1"
    }
]