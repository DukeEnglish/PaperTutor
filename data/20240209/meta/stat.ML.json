[
    {
        "title": "Tighter Generalisation Bounds via Interpolation",
        "authors": "Paul ViallardMaxime HaddoucheUmut ŞimşekliBenjamin Guedj",
        "links": "http://arxiv.org/abs/2402.05101v1",
        "entry_id": "http://arxiv.org/abs/2402.05101v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05101v1",
        "summary": "This paper contains a recipe for deriving new PAC-Bayes generalisation bounds\nbased on the $(f, \\Gamma)$-divergence, and, in addition, presents PAC-Bayes\ngeneralisation bounds where we interpolate between a series of probability\ndivergences (including but not limited to KL, Wasserstein, and total\nvariation), making the best out of many worlds depending on the posterior\ndistributions properties. We explore the tightness of these bounds and connect\nthem to earlier results from statistical learning, which are specific cases. We\nalso instantiate our bounds as training objectives, yielding non-trivial\nguarantees and practical performances.",
        "updated": "2024-02-07 18:55:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05101v1"
    },
    {
        "title": "On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling",
        "authors": "Marcin SenderaMinsu KimSarthak MittalPablo LemosLuca ScimecaJarrid Rector-BrooksAlexandre AdamYoshua BengioNikolay Malkin",
        "links": "http://arxiv.org/abs/2402.05098v1",
        "entry_id": "http://arxiv.org/abs/2402.05098v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05098v1",
        "summary": "We study the problem of training diffusion models to sample from a\ndistribution with a given unnormalized density or energy function. We benchmark\nseveral diffusion-structured inference methods, including simulation-based\nvariational approaches and off-policy methods (continuous generative flow\nnetworks). Our results shed light on the relative advantages of existing\nalgorithms while bringing into question some claims from past work. We also\npropose a novel exploration strategy for off-policy methods, based on local\nsearch in the target space with the use of a replay buffer, and show that it\nimproves the quality of samples on a variety of target distributions. Our code\nfor the sampling methods and benchmarks studied is made public at\nhttps://github.com/GFNOrg/gfn-diffusion as a base for future work on diffusion\nmodels for amortized inference.",
        "updated": "2024-02-07 18:51:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05098v1"
    },
    {
        "title": "Extending the Reach of First-Order Algorithms for Nonconvex Min-Max Problems with Cohypomonotonicity",
        "authors": "Ahmet AlacaogluDonghwan KimStephen J. Wright",
        "links": "http://arxiv.org/abs/2402.05071v1",
        "entry_id": "http://arxiv.org/abs/2402.05071v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05071v1",
        "summary": "We focus on constrained, $L$-smooth, nonconvex-nonconcave min-max problems\neither satisfying $\\rho$-cohypomonotonicity or admitting a solution to the\n$\\rho$-weakly Minty Variational Inequality (MVI), where larger values of the\nparameter $\\rho>0$ correspond to a greater degree of nonconvexity. These\nproblem classes include examples in two player reinforcement learning,\ninteraction dominant min-max problems, and certain synthetic test problems on\nwhich classical min-max algorithms fail. It has been conjectured that\nfirst-order methods can tolerate value of $\\rho$ no larger than $\\frac{1}{L}$,\nbut existing results in the literature have stagnated at the tighter\nrequirement $\\rho < \\frac{1}{2L}$. With a simple argument, we obtain optimal or\nbest-known complexity guarantees with cohypomonotonicity or weak MVI conditions\nfor $\\rho < \\frac{1}{L}$. The algorithms we analyze are inexact variants of\nHalpern and Krasnosel'ski\\u{\\i}-Mann (KM) iterations. We also provide\nalgorithms and complexity guarantees in the stochastic case with the same range\non $\\rho$. Our main insight for the improvements in the convergence analyses is\nto harness the recently proposed \"conic nonexpansiveness\" property of\noperators. As byproducts, we provide a refined analysis for inexact Halpern\niteration and propose a stochastic KM iteration with a multilevel Monte Carlo\nestimator.",
        "updated": "2024-02-07 18:22:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05071v1"
    },
    {
        "title": "Causal Representation Learning from Multiple Distributions: A General Setting",
        "authors": "Kun ZhangShaoan XieIgnavier NgYujia Zheng",
        "links": "http://arxiv.org/abs/2402.05052v1",
        "entry_id": "http://arxiv.org/abs/2402.05052v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05052v1",
        "summary": "In many problems, the measured variables (e.g., image pixels) are just\nmathematical functions of the hidden causal variables (e.g., the underlying\nconcepts or objects). For the purpose of making predictions in changing\nenvironments or making proper changes to the system, it is helpful to recover\nthe hidden causal variables $Z_i$ and their causal relations represented by\ngraph $\\mathcal{G}_Z$. This problem has recently been known as causal\nrepresentation learning. This paper is concerned with a general, completely\nnonparametric setting of causal representation learning from multiple\ndistributions (arising from heterogeneous data or nonstationary time series),\nwithout assuming hard interventions behind distribution changes. We aim to\ndevelop general solutions in this fundamental case; as a by product, this helps\nsee the unique benefit offered by other assumptions such as parametric causal\nmodels or hard interventions. We show that under the sparsity constraint on the\nrecovered graph over the latent variables and suitable sufficient change\nconditions on the causal influences, interestingly, one can recover the\nmoralized graph of the underlying directed acyclic graph, and the recovered\nlatent variables and their relations are related to the underlying causal model\nin a specific, nontrivial way. In some cases, each latent variable can even be\nrecovered up to component-wise transformations. Experimental results verify our\ntheoretical claims.",
        "updated": "2024-02-07 17:51:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05052v1"
    },
    {
        "title": "Compression of Structured Data with Autoencoders: Provable Benefit of Nonlinearities and Depth",
        "authors": "Kevin KöglerAlexander ShevchenkoHamed HassaniMarco Mondelli",
        "links": "http://arxiv.org/abs/2402.05013v1",
        "entry_id": "http://arxiv.org/abs/2402.05013v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05013v1",
        "summary": "Autoencoders are a prominent model in many empirical branches of machine\nlearning and lossy data compression. However, basic theoretical questions\nremain unanswered even in a shallow two-layer setting. In particular, to what\ndegree does a shallow autoencoder capture the structure of the underlying data\ndistribution? For the prototypical case of the 1-bit compression of sparse\nGaussian data, we prove that gradient descent converges to a solution that\ncompletely disregards the sparse structure of the input. Namely, the\nperformance of the algorithm is the same as if it was compressing a Gaussian\nsource - with no sparsity. For general data distributions, we give evidence of\na phase transition phenomenon in the shape of the gradient descent minimizer,\nas a function of the data sparsity: below the critical sparsity level, the\nminimizer is a rotation taken uniformly at random (just like in the compression\nof non-sparse data); above the critical sparsity, the minimizer is the identity\n(up to a permutation). Finally, by exploiting a connection with approximate\nmessage passing algorithms, we show how to improve upon Gaussian performance\nfor the compression of sparse data: adding a denoising function to a shallow\narchitecture already reduces the loss provably, and a suitable multi-layer\ndecoder leads to a further improvement. We validate our findings on image\ndatasets, such as CIFAR-10 and MNIST.",
        "updated": "2024-02-07 16:32:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05013v1"
    }
]