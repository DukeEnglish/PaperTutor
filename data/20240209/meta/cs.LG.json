[
    {
        "title": "Opening the AI black box: program synthesis via mechanistic interpretability",
        "authors": "Eric J. MichaudIsaac LiaoVedang LadZiming LiuAnish MudideChloe LoughridgeZifan Carl GuoTara Rezaei KheirkhahMateja VukelićMax Tegmark",
        "links": "http://arxiv.org/abs/2402.05110v1",
        "entry_id": "http://arxiv.org/abs/2402.05110v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05110v1",
        "summary": "We present MIPS, a novel method for program synthesis based on automated\nmechanistic interpretability of neural networks trained to perform the desired\ntask, auto-distilling the learned algorithm into Python code. We test MIPS on a\nbenchmark of 62 algorithmic tasks that can be learned by an RNN and find it\nhighly complementary to GPT-4: MIPS solves 32 of them, including 13 that are\nnot solved by GPT-4 (which also solves 30). MIPS uses an integer autoencoder to\nconvert the RNN into a finite state machine, then applies Boolean or integer\nsymbolic regression to capture the learned algorithm. As opposed to large\nlanguage models, this program synthesis technique makes no use of (and is\ntherefore not limited by) human training data such as algorithms and code from\nGitHub. We discuss opportunities and challenges for scaling up this approach to\nmake machine-learned models more interpretable and trustworthy.",
        "updated": "2024-02-07 18:59:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05110v1"
    },
    {
        "title": "Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding",
        "authors": "Zachary AnknerRishab ParthasarathyAniruddha NrusimhaChristopher RinardJonathan Ragan-KelleyWilliam Brandon",
        "links": "http://arxiv.org/abs/2402.05109v1",
        "entry_id": "http://arxiv.org/abs/2402.05109v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05109v1",
        "summary": "To combat the memory bandwidth-bound nature of autoregressive LLM inference,\nprevious research has proposed the speculative decoding framework. To perform\nspeculative decoding, a small draft model proposes candidate continuations of\nthe input sequence, that are then verified in parallel by the base model. One\nway to specify the draft model, as used in the recent Medusa decoding\nframework, is as a collection of light-weight heads, called draft heads, that\noperate on the base model's hidden states. To date, all existing draft heads\nhave been sequentially independent, meaning that they speculate tokens in the\ncandidate continuation independently of any preceding tokens in the candidate\ncontinuation. In this work, we propose Hydra heads, a sequentially dependent,\ndrop-in replacement for standard draft heads that significantly improves\nspeculation accuracy. Decoding with Hydra heads improves throughput compared to\nMedusa decoding with standard draft heads. We further explore the design space\nof Hydra head training objectives and architectures, and propose a\ncarefully-tuned Hydra head recipe, which we call Hydra++, that improves\ndecoding throughput by 1.31x and 2.71x compared to Medusa decoding and\nautoregressive decoding, respectively. Overall, Hydra heads are a simple\nintervention on standard draft heads that significantly improve the end-to-end\nspeed of draft head based speculative decoding.",
        "updated": "2024-02-07 18:58:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05109v1"
    },
    {
        "title": "Tighter Generalisation Bounds via Interpolation",
        "authors": "Paul ViallardMaxime HaddoucheUmut ŞimşekliBenjamin Guedj",
        "links": "http://arxiv.org/abs/2402.05101v1",
        "entry_id": "http://arxiv.org/abs/2402.05101v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05101v1",
        "summary": "This paper contains a recipe for deriving new PAC-Bayes generalisation bounds\nbased on the $(f, \\Gamma)$-divergence, and, in addition, presents PAC-Bayes\ngeneralisation bounds where we interpolate between a series of probability\ndivergences (including but not limited to KL, Wasserstein, and total\nvariation), making the best out of many worlds depending on the posterior\ndistributions properties. We explore the tightness of these bounds and connect\nthem to earlier results from statistical learning, which are specific cases. We\nalso instantiate our bounds as training objectives, yielding non-trivial\nguarantees and practical performances.",
        "updated": "2024-02-07 18:55:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05101v1"
    },
    {
        "title": "Hydragen: High-Throughput LLM Inference with Shared Prefixes",
        "authors": "Jordan JuravskyBradley BrownRyan EhrlichDaniel Y. FuChristopher RéAzalia Mirhoseini",
        "links": "http://arxiv.org/abs/2402.05099v1",
        "entry_id": "http://arxiv.org/abs/2402.05099v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05099v1",
        "summary": "Transformer-based large language models (LLMs) are now deployed to hundreds\nof millions of users. LLM inference is commonly performed on batches of\nsequences that share a prefix, such as few-shot examples or a chatbot system\nprompt. Decoding in this large-batch setting can be bottlenecked by the\nattention operation, which reads large key-value (KV) caches from memory and\ncomputes inefficient matrix-vector products for every sequence in the batch. In\nthis work, we introduce Hydragen, a hardware-aware exact implementation of\nattention with shared prefixes. Hydragen computes attention over the shared\nprefix and unique suffixes separately. This decomposition enables efficient\nprefix attention by batching queries together across sequences, reducing\nredundant memory reads and enabling the use of hardware-friendly matrix\nmultiplications. Our method can improve end-to-end LLM throughput by up to 32x\nagainst competitive baselines, with speedup growing with the batch size and\nshared prefix length. Hydragen also enables the use of very long shared\ncontexts: with a high batch size, increasing the prefix length from 1K to 16K\ntokens decreases Hydragen throughput by less than 15%, while the throughput of\nbaselines drops by over 90%. Hydragen generalizes beyond simple prefix-suffix\ndecomposition and can be applied to tree-based prompt sharing patterns,\nallowing us to further reduce inference time on competitive programming\nproblems by 55%.",
        "updated": "2024-02-07 18:53:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05099v1"
    },
    {
        "title": "On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling",
        "authors": "Marcin SenderaMinsu KimSarthak MittalPablo LemosLuca ScimecaJarrid Rector-BrooksAlexandre AdamYoshua BengioNikolay Malkin",
        "links": "http://arxiv.org/abs/2402.05098v1",
        "entry_id": "http://arxiv.org/abs/2402.05098v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05098v1",
        "summary": "We study the problem of training diffusion models to sample from a\ndistribution with a given unnormalized density or energy function. We benchmark\nseveral diffusion-structured inference methods, including simulation-based\nvariational approaches and off-policy methods (continuous generative flow\nnetworks). Our results shed light on the relative advantages of existing\nalgorithms while bringing into question some claims from past work. We also\npropose a novel exploration strategy for off-policy methods, based on local\nsearch in the target space with the use of a replay buffer, and show that it\nimproves the quality of samples on a variety of target distributions. Our code\nfor the sampling methods and benchmarks studied is made public at\nhttps://github.com/GFNOrg/gfn-diffusion as a base for future work on diffusion\nmodels for amortized inference.",
        "updated": "2024-02-07 18:51:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05098v1"
    }
]