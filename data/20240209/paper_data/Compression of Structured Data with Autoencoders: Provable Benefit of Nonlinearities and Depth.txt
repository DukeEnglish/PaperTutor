Compression of Structured Data with Autoencoders:
Provable Benefit of Nonlinearities and Depth
KevinKo¨gler*1 AlexanderShevchenko*1 HamedHassani2 MarcoMondelli1
Abstract success,anactiveareaofresearchisaimedattheoretically
analyzingtheperformanceofautoencoderstounderstand
Autoencodersareaprominentmodelinmanyem-
thequalityanddynamicsofrepresentationlearningwhen
pirical branches of machine learning and lossy
thesearchitecturesaretrainedwithgradientmethods.
data compression. However, basic theoretical
questionsremainunansweredeveninashallow Formally,considertheencodingofx∈Rdgivenby
two-layer setting. In particular, to what degree
z =σ(Bx), B ∈Rn×d, z ∈Rn, (1)
doesashallowautoencodercapturethestructure
oftheunderlyingdatadistribution? Forthepro- wherethenon-linearactivationσ(·)isappliedcomponent-
totypicalcaseofthe1-bitcompressionofsparse wise. Theratior = n/disreferredtoasthecompression
Gaussiandata,weprovethatgradientdescentcon- rate. Forashallow(two-layer)autoencoder,thedecoding
vergestoasolutionthatcompletelydisregardsthe consistsofasinglelineartransformationA∈Rd×n:
sparsestructureoftheinput. Namely,theperfor-
xˆ (x)=Az =Aσ(Bx). (2)
mance of the algorithm is the same as if it was Θ
compressingaGaussiansource–withnosparsity.
TheoptimalsetofparametersΘ={A,B}minimizesthe
Forgeneraldatadistributions,wegiveevidence
mean-squarederror(MSE)
ofaphasetransitionphenomenonintheshapeof
R(Θ):=d−1E(cid:2)
∥x−xˆ
(x)∥2(cid:3)
, (3) thegradientdescentminimizer,asafunctionof Θ 2
thedatasparsity: belowthecriticalsparsitylevel,
where the expectation is taken over the data distribution
theminimizerisarotationtakenuniformlyatran-
x. The model described in (2) is a natural extension of
dom(justlikeinthecompressionofnon-sparse
linearautoencoders(σ(x)=α·xforsomeα̸=0),which
data); above the critical sparsity, the minimizer
werethoroughlystudiedoverthepastyears(Kuninetal.,
istheidentity(uptoapermutation). Finally,by
2019;Gideletal.,2019;Baoetal.,2020). Inaneffortto
exploiting a connection with approximate mes-
gobeyondthelinearsetting,anumberofrecentworkshave
sagepassingalgorithms,weshowhowtoimprove
consideredthenon-linearmodel(2). Specifically,Refinetti
uponGaussianperformanceforthecompression
&Goldt(2022);Nguyen(2021)studythetrainingdynamics
ofsparsedata: addingadenoisingfunctiontoa
underspecificscalingregimesoftheinputdimensiondand
shallowarchitecturealreadyreducesthelossprov-
the number of neurons n, which lead to either vanishing
ably,andasuitablemulti-layerdecoderleadstoa
ordivergingcompressionrates. Shevchenkoetal.(2023)
furtherimprovement. Wevalidateourfindingson
focusontheproportionalregimeinwhichdandngrowat
imagedatasets,suchasCIFAR-10andMNIST.
thesamespeed,buttheiranalysisreliesheavilyonGaussian
dataassumptions. IncontrastwithGaussiandatathatlacks
anyparticularstructure,realdataoftenexhibitsrichstruc-
1.Introduction
turalproperties. Forinstance,imagesareinherentlysparse,
Autoencoders have achieved remarkable performance in andthispropertyhasbeenexploitedbyvariouscompres-
manymachinelearningareas,suchasgenerativemodeling sionschemessuchasjpeg. Inthisview,itisparamountto
(Kingma&Welling,2014),inverseproblems(Pengetal., gobeyondtheanalysisofunstructuredGaussiandataand
2020)anddatacompression(Balle´etal.,2017;Theisetal., addressthefollowingfundamentalquestions:
2017;Agustssonetal.,2017). Motivatedbythispractical Doesgradientdescenttrainingofthetwo-layerautoencoder
(2)capturethestructureinthedata? Howdoesincreasing
*Equal contribution 1ISTA, Klosterneuburg, Austria
2Department of Electrical and Systems Engineering, Uni- theexpressivityofthedecoderimpacttheperformance?
versity of Pennsylvania, USA. Correspondence to: Kevin
Ko¨gler <kevin.koegler@ist.ac.at>, Alexander Shevchenko To address these questions, we consider the compression
<alex.shevchenko@ist.ac.at>. ofstructureddataviathenon-linearautoencoder(1)with
1
4202
beF
7
]GL.sc[
1v31050.2042:viXraCompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
σ ≡ sign(1-bitcompressedsensing,(Boufounos&Bara- performance. This is in sharp contrast with the compres-
niuk,2008))andshowhowthedatastructureiscapturedby sionofunstructured,Gaussiandatawhere,asdiscussedin
thearchitectureofthedecoder. Letusexplainthechoiceof Section6of(Shevchenkoetal.,2023),multipledecoding
σ ≡sign. Apartfromtheconnectiontoclassicalinforma- layersdonothelp.
tionandcodingtheory(Cover&Thomas,2006),itsscalein-
variancepreventsthemodelfromenteringthelinearregime.
2.Relatedwork
Namely,ifσ(·)hasawell-definednon-vanishingderivative
atzero,bypickinganencodingmatrixBs.t.∥B∥ ≪1, Theoreticalresultsforautoencoders. Thepracticalsuc-
op
onecanlinearizethemodel,i.e.,xˆ (x) ≈ σ′(0)·ABx, cess of autoencoders has spurred a flurry of theoretical
Θ
which results in PCA-like behaviour (Refinetti & Goldt, research, startedwiththeanalysisoflinearautoencoders:
2022). Thus,signisanaturalcandidatetotacklethenon- Kunin et al. (2019) indicate a PCA-like behaviour of the
linearsettingofinterestinapplicationsand,infact,hard- minimizersoftheL -regularizedloss;Baoetal.(2020)pro-
2
thresholdingactivationsarecommoninlarge-scalemodels videevidencethattheconvergencetotheminimizerisslow
(VanDenOordetal.,2017). duetoill-conditioning,whichworsensasthedimensionof
thelatentspaceincreases;Oftadehetal.(2020)studythe
Ourmaincontributionscanbesummarizedasfollows:
geometry of the loss landscape; Gidel et al. (2019) quan-
• Theorem4.1provesthatthelineardecoderin(2)may tifythetime-stepsofthetrainingdynamicsatwhichdeep
beunabletoexploitthesparsityinthedata: whenx linearnetworksrecoverfeaturesofincreasingcomplexity.
hasaBernoulli-Gaussian(or“sparseGaussian”)distri- More recently, the focus has shifted towards non-linear
bution,boththegradientdescentsolutionandtheMSE autoencoders. Refinetti & Goldt (2022) characterize the
coincide withthose obtained for the compressionof trainingdynamicsviaasystemofODEswhenthecompres-
purelyGaussiandata(withnosparsity). sionraterisvanishing. Nguyen(2021)takesamean-field
view that requiresa polynomialgrowthof thenumber of
• GoingbeyondGaussiandata,wegiveevidenceofthe
neuronsnintheinputdimensiond,whichresultsinadi-
emergence of a phase transition in the structure of
verging compression rate. Cui & Zdeborova´ (2023) use
theoptimalmatricesA,Bin(2),asthesparsitylevel
toolsfromstatisticalphysicstopredicttheMSEofdenois-
p ∈ (0,1)varies: Proposition4.2locatesthecritical
ing a Gaussian mixture via a two-layer autoencoder with
valueofpsuchthattheminimizerstopsbeingarandom
askipconnection. Shevchenkoetal.(2023)considerthe
rotation(asforpurelyGaussiandata),anditbecomes
compressionofGaussiandatawithatwo-layerautoencoder
theidentity(upapermutation);numericalsimulations
whenthecompressionraterisfixedandshowthatgradient
forgradientdescentcorroboratethisphenomenology
descentmethodsachieveaminimizeroftheMSE.
anddisplaya“staircase”behaviorofthelossfunction.
. Incrementallearningandstaircasesinthetrainingdy-
• WhileforthecompressionofsparseGaussiandatathe namics. Phenomenasimilartothestaircasebehaviorof
lineardecoderin(2)doesnotcapturethesparsity,we the loss function that we exhibit in Figure 2 have drawn
showinSection5thatincreasingtheexpressivityofthe significantattention. Forparitylearning,thelineofworks
decoderimprovesuponGaussianperformance. First, (Abbe et al., 2021; 2022; 2023a) shows that parities are
wepost-processtheoutputof(2),i.e.,weconsider recoveredinasequentialfashionwithincreasingcomplex-
ity. Asimilarbehaviourisobservedintransformerswith
xˆ (x)=f(Az)=f(Asign(Bx)), (4)
Θ diagonalweightmatricesatsmallinitialization(Abbeetal.,
wheref isappliedcomponent-wise,andweprovethat 2023b): gradientdescentprogressivelylearnsasolutionof
asuitablef leadstoasmallerMSE.Inotherwords, increasing rank. For a single index model, Berthier et al.
addinganonlinearitytothelineardecoderin(2)prov- (2023)showaseparationoftime-scalesatwhichthetrain-
ably helps. Finally, we further improve the perfor- ingdynamicsfollowsanalternatingpatternofplateausand
mancebyincreasingthedepthandusingamulti-layer rapiddecreasesintheloss. Evidenceofincrementallearn-
decoder. Ouranalysisleveragesaconnectionbetween ingindeeplinearnetworksisprovidedbyBerthier(2023);
multi-layer autoencoders and the iterates of the RI- Pesme & Flammarion (2023); Simon et al. (2023); Jacot
GAMPalgorithmproposedbyVenkataramananetal. et al. (2021); Milanesi et al. (2021). The recent work by
(2022),whichmaybeofindependentinterest. Sze´kelyetal.(2023)showsthatthecumulantsofthedata
distributionarelearntsequentially,revealingasamplecom-
Experimentsonsyntethicdataconfirmourfindings,andsim-
plexitygapbetweenneuralnetworksandrandomfeatures.
ilarphenomenaaredisplayedwhenrunninggradientdescent
tocompressCIFAR-10/MNISTimages. Takentogether,our ApproximateMessagePassing(AMP). AMPrefersto
resultsshowthat,forthecompressionofstructureddata,a afamilyofiterativealgorithmsdevelopedforavarietyof
moreexpressivedecodingarchitectureprovablyimproves statisticalinferenceproblems(Fengetal.,2022).Suchprob-
2CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
lemsincludetherecoveryofasignalxfromobservations trajectoryofthealgorithmisthesameasthatobtainedfrom
z oftheformin(1),namely,aGeneralizedLinearModel the compression of non-sparse data, i.e., x ∼ SG (1) ≡
d
(McCullagh&Nelder,1989),whentheencodermatrixB N(0,I). As a consequence, the minimizer has a weight-
isGaussian(Rangan,2011;Mondelli&Venkataramanan, tiedorthogonalstructure(BB⊤ = I,A ∝ B⊤),andthe
2022)orrotationally-invariant(Ranganetal.,2019;Schniter MSEatconvergenceisgivenbyR asdefinedin(5).
Gauss
et al., 2016; Ma & Ping, 2017; Takeuchi, 2019). Of par-
Wenowgointothedetails. Sincetheoptimizationobjec-
ticularinterestforourworkistheRI-GAMPalgorithmby
tiveisconvexinA,weconsiderthefollowingalternating
Venkataramanan et al. (2022). In fact, RI-GAMP enjoys
minimizationversionofRiemanniangradientdescent:
a computational graph structure that can be mapped to a
suitableneuralnetwork,anditapproachestheinformation- A(t+1)=argminR(A,B(t)),
theoreticallyoptimalMSE.TheoptimalMSEwascomputed A (6)
(cid:0) (cid:0) (cid:1)(cid:1)
viathereplicamethodbyTakedaetal.(2006);Tulinoetal. B(t+1):=proj B(t)−η ∇ B(t)+G(t) .
(2013),andthesepredictionswererigorouslyconfirmedfor
In fact, due to the convexity in A of the MSE R(·,·) in
thehigh-temperatureregimebyLietal.(2023).
(3),wecancomputeinclosedformargmin R(A,B(t)).
A
Here, Riemannian refers to the space of matrices with
3.Preliminaries
unit-norm rows, ∇ is a shorthand for the gradient
B(t)
Notation. Weuseplainsymbolsa,bforscalars,boldsym- ∇ B(t)R(A(t),B(t)), and proj normalizes the rows of a
bols a,b for vectors, and capitalized bold symbols A,B matrixtohaveunitnorm. Theprojectionstep(and,hence,
formatrices. Givenavectora,itsℓ -normis∥a∥ . Given the Riemannian nature of the optimization) is due to the
2 2
amatrixA,itsoperatornormis∥A∥ . Wedenoteaunidi- scale-invarianceofsign,anditensuresnumericalstability.
op
mensionalGaussiandistributionwithmeanµandvariance ThetermG(t)correspondstoGaussiannoiseofarbitrarily
σ2 byN(µ,σ2). Weusetheshorthandxˆ forxˆ . Unless smallvariance,whichactsasa(probabilistic)smoothingfor
Θ
specifiedotherwise,functionareappliedcomponent-wise thediscontinuityofsignat0and,therefore,impliesthatthe
to vector/matrix-valued inputs. We denote by C,c > 0 gradientiswell-definedalongthetrajectoryofthealgorithm.
universalconstants,whichareindependentofn,d. (NotethatG(t)isnotneededinexperiments,asweusea
straight-throughestimator,seeAppendixC.1).
Data distribution and MSE. For p ∈ (0,1], a sparse
Theorem4.1(Gradientdescentdoesnotcapturethespar-
Gaussian distribution SG (p) is equal to N(0,1/p) with
1 sity). Considerthegradientdescentalgorithmin(6)with
probabilitypandis0otherwise. Thescalingofthevariance
oftheGaussiancomponentensuresaunitsecondmoment
x ∼ SG d(p) and (G(t))
i,j
∼ N(0,σ2), where d−γg ≤
σ ≤ C/d for some fixed 1 < γ < ∞. Initialize the al-
forallp.Weusethenotationx∼SG (p)todenoteavector g
d gorithm with B(0) equal to a row-normalized Gaussian,
withi.i.d.componentsdistributedaccordingtoSG (p). De-
1 i.e., B′ (0) ∼ N(0,1/d), B(0) = proj(B′(0)), and let
creasingpmakesx∼SG (p)moresparse: forp=1one i,j
d B(0) = US(0)V⊤ be its SVD. Let the step size η be
recovers the isotropic Gaussian, i.e., SG (1) ≡ N(0,I), √
d
Θ(1/ d). Then, foranyfixedr < 1andT ∈ (0,∞),
whilep=0impliesthatx=0. max
withprobabilityatleast1−Cd−3/2,thefollowingholds
Shevchenko et al. (2023) consider Gaussian data x ∼ forallt≤T /η
max
SG (1)andthetwo-layerautoencoderwithlineardecoder
d B(t)=US(t)V⊤+R(t),
in(2). Theiranalysisshowsthat,foracompressionrater ≤
1,theMSEobtainedbyminimizing(3)overΘ={A,B}
(cid:13) (cid:13)S(t)S(t)⊤−I(cid:13)
(cid:13) op ≤Cexp(−cηt), (7)
isgivenby 2
R :=1− ·r. (5) lim sup ∥R(t)∥ =0,
Gauss π d→∞t∈[0,Tmax/η] op
Thesetofminimizers(A,B)hasaweight-tiedorthogonal
whereC,careuniversalconstantsdependingonlyonp,r
structure,i.e.,BB⊤ =IandA∝B⊤,andgradient-based
andT . Moreover,wehavethat,almostsurely,
optimizationschemesreachaglobalminimum. max
lim lim R(A(t),B(t))=R , (8)
Gauss
t→∞d→∞
4.Limitationsofalineardecodinglayer
lim sup ∥B(t)−B (t)∥ =0, (9)
Gauss op
Ourmaintechnicalresultisthatatwo-layerautoencoder
d→∞t∈[0,Tmax/η]
with a single linear decoding layer does not capture the whereR isdefinedin(5)andB (t)isobtained
Gauss Gauss
sparsestructureofthedata. Specifically,weconsiderthe byrunning(6)withx∼N(0,I).
autoencoderin(2)withGaussiandatax∼SG (p)trained
d
via gradient descent. We show that, when n,d are both Inwords,(7)givesaprecisecharacterizationofthegradient
large (holding the compression rate r = n/d fixed), the descenttrajectory: throughoutthedynamics,theeigenbasis
3CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
ofB(t)doesnotchangesignificantly(i.e.,itremainsclose where U,V come from the SVD of B(0), S(t) is a di-
to that of B(0)) and, as t grows, all the singular values agonalmatrixcontainingthesingularvaluesofB(t),and
of B(t) approach 1. As a consequence, (8) gives that, at S˜(t)=G(S(t))foradeterministicfunctionG. Thisshows
convergence, theMSEachievedby(6)withx ∼ SG (p) that,uptotheleadingorderintheapproximation,thesin-
d
approachesR ,whichcorrespondstothecompression gularvectorsofB(t)arefixedalongthegradienttrajectory.
Gauss
ofstandardGaussiandatax∼N(0,I). Infact,astronger Crucially,thefunctionGdoesnotdependonthesparsity
resultholds: (9)givesthatthewholetrajectoryof(6)for pofthedata. Thus,foranyp∈(0,1),thegradientupdate
x∼SG (p)isthesameasthatobtainedforx∼N(0,I). forthemaskedobjective(10)isclosetotheupdateforthe
d
sameobjectivewithoutthemasking(i.e.,correspondingto
Proof sketch. The sparse Gaussian distribution can be thecompressionofGaussiandatawithp=1).
seen as the component-wise product between a standard
Finally,LemmaA.19derivesana-prioriGro¨nwall-typees-
Gaussian vector and a mask m ∈ {0,1}d with i.i.d.
timate,whichbootstrapstheboundstothewholegradient
Bernoulli(p) entries. The key idea is to approximate the
descent trajectory (6) and concludesthe proof. The com-
randomnessinthemaskmwithitsaverage,whichheuris-
pleteargumentisdeferredtoAppendixA.
ticallycorrespondstohavingagainGaussiandata. Thisis
doneformallybypushingthemaskintothenetworkparam- BeyondGaussiandata: Phasetransitions,staircasesin
eters,andthenusinghigh-dimensionalconcentrationtools the learning dynamics, and image data. For general
toboundthedeviationfromtheaverage. distributionsofthedatax,weempiricallyobservethatthe
minimizersofthemodelin(2)foundbystochasticgradient
Wenowgointothedetails. Asastartingpoint,LemmaA.1
descent(SGD)either(i)coincidewiththoseobtainedfor
showsthat,uptoanerrorexponentiallysmallind,instead
standardGaussiandata,or(ii)areequivalentto(suitablysub-
oftheMSEin(3)wecanconsidertheobjective
E (cid:20) Tr(cid:104) A⊤A·arcsin(Bˆ Bˆ⊤ )(cid:105) − √2 ·Tr(cid:104) ABˆ (cid:105)(cid:21) . s oa fm thp ele nd e) up roe nrm s,u tt ha et sio en ts wo of ct ah ne di id de an tt ei sty c. aU np beto ea xpp re er sm seu dta at sio :n
m m m p m
(10) xˆ Haar(x)=α Haar·B⊤sign(Bx), (12)
Here,B mdenotesamaskedversionofB,i.e.,thecolumns (cid:20)
I
(cid:21)
of B are set to zero according to the Bernoulli mask m, xˆ Id(x)=α Id· 0 n sign([I n,0 n×(d−n)]x),
andBˆ isobtainedbynormalizingtherowsofB ,i.e., (d−n)×n
m m
(Bˆ ) =(B ) /∥(B ) ∥ . where B is obtained by subsampling a Haar matrix (i.e.,
m i,: m i,: m i,: 2
a matrix taken uniformly from the group of rotations),
Next, we provide a number of concentration bounds for
0 isa(d−n)×nmatrixofzeros,and(α ,α )
quantitiestowhichtheBernoullimaskmisapplied. We (d−n)×n Haar Id
arescalarcoefficients. Thelossesofthesetwocandidates
startwithrandomvectors(LemmaA.7),randommatrices
canbeexpressedinaclosedformasderivedbelow.
(Lemmas A.8 and A.9), and quantities that appear when
optimizingtheobjective(10)viagradientdescent(Lemma Proposition4.2(Candidatecomparison). Letr ≤1andlet
A.11). Wenotethatboththelargestentryandtheoperator xhavei.i.d.componentswithzeromeanandunitvariance.
normoftheerrormatrixhavetobecontrolled. Then,we Then, we have that, almost surely, the MSE of xˆ Haar(·)
takecareoftherownormalizationinthedefinitionofBˆ m. coincideswiththeGaussianperformanceR Gaussin(5),i.e.,
Todoso,LemmaA.12isageneralresultshowingthat min lim 1 ·E (cid:2) ∥xˆ (x)−x∥2(cid:3) =1−2 ·r. (13)
E F
(cid:16)
Bˆ
(cid:17)
≈E F
(cid:18) √1
B
(cid:19)
, (11)
αHaar∈Rd→∞d x Haar 2 π
m m m p m Furthermore,wehavethat,foranyd,
foraclassofsufficientlyregularmatrix-valuedfunctions min 1 ·E (cid:2) ∥xˆ (x)−x∥2(cid:3) =1−r·(E|x |)2, (14)
F. Inwords,(11)givesthat,onaverageoverm,therow αId∈R d x Id 2 1
normalization can be replaced by the multiplication with
√
1/ p. ThisresultisinstantiatedinLemmaA.13forthree wherex 1isthefirstcomponentofx.Thisimpliesthatxˆ Id(·)
choicesofF usefulfortheanalysisofgradientdescent. issuperiortoxˆ Haar(·)intermsofMSEwhenever
(cid:112)
Armedwiththesetechnicaltools,weareabletoremovethe E|x 1|> 2/π =E g∼N(0,1)|g|. (15)
effectofthemaskingfromthegradientdescentdynamics.
First,LemmaA.14focusesontheoptimizationofthematrix The MSE of xˆ (·) in (14) is obtained via a direct calcu-
Id
A, which has a closed form due to the convexity of the lation. ToevaluatetheMSEofxˆ (·)in(13),werelate
Haar
objective(10)inA. Next,LemmasA.15andA.16estimate thisestimatortothefirstiterateoftheRI-GAMPalgorithm
thegradient∇ B(t)as proposedbyVenkataramananetal.(2022). Then,thehigh
(cid:13) (cid:13) log10(d) dimensional limit of ∥B⊤sign(Bx)−x∥2 follows from
(cid:13)∇ −US˜(t)V⊤(cid:13) ≤C(T )· √ , 2
(cid:13) B(t) (cid:13) max thestateevolutionanalysisofRI-GAMP.Asimilarstrategy
op d
4CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
0.40
0.35
0.30
0.25
0.20
0.15
0.10
SGD
0.05 Theoretical prediction
Gaussian performance
0.00 pcrit=2/
0.0 0.2 0.4 0.6 0.8 1.0
p
Figure1.CompressionofsparseRademacherdataviathetwo-layerautoencoderin(2).Wesetd=200andr=1.Left.MSEachieved
bySGDatconvergence,asafunctionofthesparsitylevelp.Theempiricalvalues(dots)matchourtheoreticalprediction(blueline):for
p<p ,thelossisequaltothevalueobtainedforGaussiandata,i.e.,R =1−2r/π;forp≥p ,thelossissmaller,anditis
crit Gauss crit
equalto1−r·(E|x |)2 =1−r·p.Center.EncodermatrixBatconvergenceofSGDwhenp=0.3<p :thematrixisarandom
1 crit
rotation.Right.EncodermatrixBatconvergenceofSGDwhenp=0.7≥p :thenegativesigninpartoftheentriesofBiscancelled
crit
bythecorrespondingsignintheentriesofA;hence,Bisequivalenttoapermutationoftheidentity.
SGD 1.0 Gaussian performance
Gaussian performance: 1 2/ SGD
1.2 Global minimum: 1 p 0.9
1.0 0.8
0.7
0.8
0.6
0.6
0.5
0.4 0.4
0.0 0.2 0.4 0.6 0.8 1.0
0.2 r
0 1 2 3 4 5 6 7 Figure3.CompressionofmaskedandwhitenedCIFAR-10images
Iteration 1e7
of the class “dog” via the two-layer autoencoder in (2). First,
Figure2.Compression of sparse Rademacher data via the two-
thedataiswhitenedsothatithasidentitycovariance(asinthe
layerautoencoderin(2). Wesetd = 200,r = 1andp = 0.8.
settingofTheorem4.1).Then,thedataismaskedbysettingeach
TheMSEisplottedasafunctionofthenumberofiterationsand,
pixelindependentlyto0withprobabilityp = 0.7. Anexample
asp>p ,itdisplaysastaircasebehavior.
crit
ofanoriginalimageisonthetopright, andthecorresponding
maskedandwhitenedimageisonthebottomright.TheSGDloss
atconvergence(dots)matchesthesolidline,whichcorresponds
willbeusedalsoinSection5toanalyzedifferentdecoding
tothepredictionin(5)forthecompressionofstandardGaussian
architectures. ThecompleteproofisinAppendixB.1.
data(withnosparsity).
Asmentionedabove,ournumericalresultsleadustocon-
andFigure1showsaphasetransitioninthestructureofthe
jecturethatSGDrecoverseitherofthecandidatesin(12),
minimizersfoundbySGD:
dependingonwhichachievesasmallerloss. Specifically,if
condition(15)ismet,theSGDpredictorconvergestoxˆ (·) • Forp < p crit, SGDconvergestoasolutions.t.B is
Id
andimprovesupontheGaussianlossR ;otherwise,it auniformrotation(centralheatmap)andtheMSEis
Gauss
convergestoxˆ (·)anditsMSEisequaltoR . closetoR Gauss =1−2r/π,see(13).
Haar Gauss
ForsparseGaussiandata,condition(15)isneversatisfied, • For p > p , SGD converges to a solution s.t. B
crit
asE |x |=(cid:112) 2p/π ≤(cid:112) 2/π. Infact,asproved is equivalent to a permutation of the identity (right
inThx e1 o∼ rS eG m1( 4p .) 1,t1 heSGDsolutionapproachesxˆ (·)and heatmap)andtheMSEiscloseto1−r·(E|x |)2 =
Haar 1
itsMSEmatchesR . 1−r·p,see(14). Inbothcases,A∝B⊤.
Gauss
ForsparseRademacherdata1,condition(15)reducesto If there is an improvement upon R (i.e., p > p ),
Gauss crit
the SGD dynamics exhibits a staircase behavior. This
p>p :=2/π ≈0.64,
crit phenomenon is displayed in Figure 2, which plots the
√
1Eachi.i.d.componentisequalto0w.p.1−pandto±1/ p error as a function of the number of SGD iterations for
w.p.p/2,whichensuresaunitsecondmomentforallp∈[0,1]. p = 0.8 > p crit: first, the MSE rapidly converges to
5
ESM
ESM
ESMCompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
1.0
2.00 d 1 ||BB I||2F
0.9
T Sh Ge Doretical prediction
1.75 d 1 ||BA I||2F Gaussian performance
1.50 0.8
1.25 0.7
1.00 0.6
0.75 0.5
0.50
0.4
0.25
0.3
0.00
0 250 500 750 1000 1250 1500 1750 2000 0.2 0.4 0.6 0.8 1.0
Iteration r
Figure4.CompressionofsparseGaussiandataviatheautoencoderin(4),wheref hastheformin(16)anditsparameters(α ,α ,α )
1 2 3
areoptimizedviaSGD.Wesetd=100andp=0.4.Left.DistancebetweenBˆBˆ⊤ ,BˆAˆandtheidentity,asafunctionofthenumber
ofiterations, whereBˆ, Aˆ denotetherow-normalizedversionsofB, A. ∥BˆBˆ⊤ −I∥ and∥BˆAˆ−I∥ decreaseandtendto0,
F F
meaningthat(uptoarescalingoftherows)BAandBB⊤approachtheidentity.Here,wetaker=1.Right.MSEachievedbySGD
atconvergence,asafunctionofthecompressionrater. Theempiricalvalues(dots)matchthecharacterizationofProposition5.1for
f =f∗in(19)(blueline),andtheyoutperformtheMSE(5)obtainedbycompressingstandardGaussiandata(orangedashedline).
R ; then, there is a plateau; finally, the global mini- 5.1.Provablebenefitofnonlinearities
Gauss
mum1−r·pisreached.
First, we apply a nonlinearity at the output of the linear
Wealsoremarkthat,aspapproachesp crit,thetimeneeded decodinglayer,asin(4). Specifically,wetake
by SGD to escape the plateau increases. A possible ex-
f(x)=α x+α tanh(α x), (16)
planationisthat,aspdecreases,thenoiseduetomasking 1 2 3
increases,whichincreasesthevarianceofthegradient. This and run SGD on the weight matrices (A,B) and on the
makesitharderforBtofindadirectiontowardsapermuta- trainableparameters(α ,α ,α )inf. Figure4showsthat,
1 2 3
tionoftheidentity(i.e.,theglobalminimum). Additional atconvergence,theminimizershavethesameweight-tied
evidenceofboththephasetransitionandthestaircasebehav- orthogonalstructureasobtainedforGaussiandata(BB⊤ =
iorofSGDisinAppendixC.2,whereFigure8considers I,A∝B⊤),seetheleftplot. However,insharpcontrast
Rademacher data and Figures 9-10 data coming from a with Gaussian data, the loss is smaller than R , see
Gauss
sparsemixtureofGaussians. thebluedotsontherightplotandcomparethemwiththe
orange dashed curve. This empirical evidence motivates
TheproofstrategyofTheorem4.1couldbeusefultotrack
ustoanalyzetheperformanceofautoencodersoftheform
SGDuntilitreachestheplateau. However,characterizing
(4),whereBisobtainedbysubsamplingaHaarmatrixof
thetime-scaleneededtoescapetheplateaulikelyrequires
appropriatedimensionsandA=B⊤.
newtools,anditprovidesanexcitingresearchdirection.
Proposition5.1(MSEcharacterization). Letr ≤1andx
Finally, Figure 3 shows that our theory predicts well the
have i.i.d. components with zero mean and unit variance.
behaviorofthecompressionofCIFAR-10imagesviathe
Considertheautoencoderxˆ(x)in(4),whereBisobtained
two-layerautoencoderin(2). Weletx betheempiricaldis-
1 bysubsamplingaHaarmatrix,A=B⊤,andf isaLips-
tributionoftheimagepixelsafterwhiteningandmasking2,
chitzfunction. Then,wehavethat,almostsurely,
andweverifythatcondition(15)doesnothold. Then,as
expected,theautoencoderisunabletocapturethestructure 1
lim ·E ∥x−xˆ(x)∥2 =E |x −f(µx +σg)|2,
comingfrommaskingpartofthepixels,andthelossatthe d→∞d x 2 x1,g 1 1 2
endofSGDtrainingequalsR . Similarresultsholdfor (17)
Gauss
MNIST,seeFigure11inAppendixC.3. wherex 1 isthefirstentryofx,g ∼ N(0,1)andindepen-
dentofx ,andtheparameters(µ,σ)aregivenby
1
5.Provablebenefitofnonlinearitiesanddepth (cid:114) 2 (cid:18) 2(cid:19)
µ=r , σ2 =r 1−r· >0. (18)
π π
Inthissection,weprovethatmoreexpressivedecodersthan
the linear one in (2) capture the sparsity of the data and,
Proposition5.1isageneralizationofProposition4.2,which
therefore,improveupontheGaussianlossR .
Gauss corresponds to taking a linear f. The idea is to relate
2Thewhiteningmakesthedatahaveisotropiccovariance,as f(B⊤sign(Bx))tothefirstiterateofasuitableRI-GAMP
requiredbyourtheory;themaskingmakesthedatasparse. algorithm,sothatthecharacterizationin(17)followsfrom
stateevolution. ThedetailsareinAppendixB.2.
6
eulaV
evitcejbO ESMCompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
ArmedwithProposition5.1,onecanreadilyestablishthe
functionf thatminimizestheMSEforlarged. Thisinfact
SGD
correspondstothef thatminimizestheRHSof(17),i.e., 0.8 1 Theop r ( eP tr ico ap lo ps rit ei don ic t5 io.2 n)
Gaussian performance
f∗(y)=E[x |µx +σg =y], (19) pcrit 0.67
1 1 0.6
aslongasthelatterisLipschitz(sothatProposition5.1can
0.4
beapplied). Sufficientconditionsforf∗tobeLipschitzare
thateither(i)x hasalog-concavedensity,or(ii)thereexist 0.2
1
independent random variables u ,v s.t. u is Gaussian,
0 0 0
0.0
v is compactly supported and x is equal in distribution
0 1 0.2 0.4 0.6 0.8 1.0
p
to u + v , see Lemma 3.8 of (Feng et al., 2022). The
0 0 Figure5.CompressionofsparseRademacherdataviatheautoen-
expressionoff∗ fordistributionsofx 1 consideredinthe coderin(4). Wesetd=200andr =1. TheMSEachievedby
experiments(sparseGaussian,Laplace,andRademacher)is SGDatconvergenceisplottedasafunctionofthesparsitylevelp.
derivedinAppendixB.4. Theempiricalvalues(bluedots)matchourtheoreticalprediction
(blueline).Forp<p˜ ,theMSEisgivenbyProposition5.1for
The blue curve in the right plot of Figure 4 evaluates the crit
BsampledfromtheHaardistribution;forp≥p˜ ,theMSEis
RHSof(17)fortheoptimalf = f∗,whenx ∼ SG (p). crit
1 1 givenbyProposition5.2forBequaltotheidentity.
Twoobservationsareinorder:
Figure5displaysthephasetransitionforthecompression
1. The blue curve matches the blue dots, obtained by
ofsparseRademacherdata:
optimizing via SGD the matrices A,B and f in the
parametricform(16). ThismeansthattheSGDperfor- • For p < p˜ ≈ 0.67, SGD converges to a solution
crit
manceisaccuratelytrackedbypluggingtheoptimal with MSE given by the RHS of (17) with f = f∗.
function(19)intothepredictionofProposition5.1. Furthermore,Bisauniformrotation(seethecentral
heatmapinFigure15ofAppendixC.6).
2. The blue curve improves upon the Gaussian loss
R
Gauss
(orangedashedline). Thismeansthat,while • Forp>p˜ crit,SGDconvergestoasolutionwithMSE
thetwo-layerautoencoderin(2)isstuckattheMSEin givenbytheRHSof(20). Furthermore, B isequiv-
orange(asprovedbyTheorem4.1),byincorporating alent to a permutation of the identity (see the right
anonlinearity,theautoencoderin(4)doesbetter. In heatmapinFigure15ofAppendixC.6).
fact,asshowninFigure13inAppendixC.5,theMSE
Bycomparingthebluedots/curvewiththeorangedashed
achieved by the autoencoder in (4) with the optimal
lineinFigure5,wealsoconcludethat,forallp,theMSE
choiceoff (namely,theRHSof(17)withf =f∗)is
oftheautoencoderin(4)improvesupontheGaussianper-
strictlylowerthanR foranyp∈(0,1).
Gauss formanceR . Thisisincontrastwiththebehaviorof
Gauss
theautoencoderin(2)whichremainsstuckatR for
Gauss
BeyondGaussiandata: Phasetransitions,staircasesin
p<2/π(seeFigure1),anditdemonstratesthebenefitof
thelearningdynamics,andimagedata. Forgeneraldata
addingthenonlinearityf.
xwithi.i.d.zero-meanunit-variancecomponents,theau-
toencoderin(4)displaysabehaviorsimilartothatdescribed Forp>p˜ crit,thelearningdynamicsexhibitsagainastair-
inSection4fortheautoencoderin(2): theSGDminimizers casebehaviorinwhichtheMSEfirstgetsstuckatthevalue
of the weight matrices A,B either exhibit a weight-tied givenbytheRHSof(17)withf = f∗, andthenreaches
orthogonalstructure(BB⊤ =I,A∝B⊤),orcomefrom theoptimalvalueof1−r·(E|x 1|)2. Thisisreportedfor
permutationsoftheidentity. Thisleadstoaphasetransition p=0.9>p˜ crit ≈0.67inFigure16ofAppendixC.6.
inthestructureoftheminimizer(andintheMSEexpres-
Finally,Figure6showsthatthekeyfeaturesweunveiledfor
sion),asthesparsitypvaries. Toquantifythecriticalvalue
theautoencoderin(4)arestillpresentwhencompressing
ofpatwhichtheminimizerchanges,onecancomparethe
sparseCIFAR-10data. Theempiricaldistributionoftheim-
MSEwhenB issubsampled(i)fromaHaarmatrix, and
agepixelsafterwhiteningiswellapproximatedbyaLaplace
(ii)fromtheidentity. Theformerisreadilyobtainedfrom
randomvariable(seeFigure12inAppendixC.4),thuswe
Proposition5.1wheref isgivenby(19),andthelatteris
denotebyx thecorrespondingsparseLaplacedistribution
1
givenbytheresultbelow,whichisprovedinAppendixB.3.
(see (108) in Appendix B.4 for a formal definition). The
Proposition 5.2. Let x have i.i.d. components with zero encodermatrixBisobtainedbysubsamplingaHaarmatrix,
mean,unitvarianceandasymmetricdistribution(i.e.,the and it is fixed; the decoder matrix A and the parameters
lawofx 1isthesameasthatof−x 1). Definexˆ Id(x)asin (α 1,α 2,α 3)inthedefinition(16)off areobtainedviaSGD
(12),andfixr ≤1. Then,wehavethat,foranyd, training. Twoobservationsareinorder:
min1 ·E (cid:2) ∥f(xˆ (x))−x∥2(cid:3) =1−r·(E|x |)2. (20)
f d x Id 2 1 1. Theautoencoderin(4)capturesthesparsity: theMSE
7
ESMCompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
1.0
0.9 Laplace prediction (p=1) Gaussian performance
Sparse Laplace prediction (p=0.4) 0.9 Prediction, decoder + non-linearity
0.8 SGD, CIFAR (p=1) Bayes-optimal
0.7 S SG GD D, , s mp aa sr ks ee d L a Cp IFla Ac Re (( pp == 00 .. 44 )) 0.8 S SG GD D, , l din ee coa dr ed re +co nd oe nr -linearity
SGD, deep decoder
0.7
0.6
0.6
0.5
0.5
0.4
0.4
0.3
0.3
0.2 0.4 0.6 0.8 1.0
r
0.2 0.4 0.6 0.8 1.0
Figure6.CompressionofmaskedandwhitenedCIFAR-10images r
oftheclass“dog”viatheautoencoderin(4). WeplottheMSE Figure7.CompressionofsparseGaussiandatax∼SG d(p)for
as a function of the compression rate r. Dots are obtained by p = 0.3 and d = 500. We plot the MSE as a function of the
training the decoder matrix A and the parameters (α ,α ,α ) compression rate r for various autoencoder architectures. The
1 2 3
viaSGDonmasked(p = 0.4,green)ororiginal(p = 1,blue) architecturein(22)(orangedots)outperformstheautoencoders
CIFAR-10images. Continuouslinesrefertothepredictionsof in(2)(greendots)andin(4)(bluedots),anditapproachesthe
Proposition5.1fortheoptimalf = f∗ in(19),wherex hasa Bayes-optimalMSE(orangeline).
1
Laplacedistribution(p=1,blue)orasparseLaplacedistribution
(p = 0.4,orange). Thesecurvesmatchwellthecorresponding (21). More generally, xˆt is obtained by multiplications
valuesobtainedviaSGD.Orangedotsareobtainedbytrainingthe
withB,B⊤,linearcombinationsofpreviousiterates,and
matricesA,Bandtheparameters(α ,α ,α )viaSGDwhenx component-wise applications of Lipschitz functions. As
1 2 3
hasi.i.d.sparseLaplaceentrieswithp=0.4. such, it can be expressed via a multi-layer decoder with
residualconnections. Thenumericalresultsin(Venkatara-
achievedonsparsedata(p=0.4,greendots)islower
manan et al., 2022) show that taking f ,g as posterior
t t
thantheMSEonnon-sparsedata(p=1,bluedots).
means (as in (19)) leads to Bayes-optimal performance,
2. Forbothvaluesofp,theSGDperformancematches havingfixedtheencodermatrixB. Thus,thisprovidesa
theRHSof(17)(continuouslines)withf = f∗. As proof-of-conceptoftheoptimalityofmulti-layerdecoders.
expected, this MSE is smaller than 1−r·(E|x |)2,
1 Infact,Figure7showsthatanarchitecturewiththreede-
and it coincides with that obtained for compressing
coding layers is already near-optimal when x ∼ SG (p).
d
syntheticdatawithi.i.d.Laplaceentries(orangedots). Thedecoderoutputisxˆ2 computedas(seealsotheblock
diagraminFigure17inAppendixC.7)
5.2.Provablebenefitofdepth
zˆ1 =sign(Bx), x1 =W zˆ1, xˆ1 =f (x1),
1 1
We conclude by showing that the MSE can be further re-
zˆ2 =g (V xˆ1⊕ zˆ1), (22)
duced by considering a multi-layer decoder. Our design 1 1 1
ofthedecodingarchitectureisinspiredbytheRI-GAMP x2 =xˆ1⊕ W zˆ2, xˆ2 =f (x1⊕ x2).
2 2 2 3
algorithm(Venkataramananetal.,2022),whichiteratively
estimatesxfromanobservationoftheformσ(Bx)via
Here,f (·),f (·),g (·)aretrainableparametricfunctionsof
1 2 1
t−1
xt =B⊤zˆt−(cid:88) β xˆi, xˆt =f (x1,··· ,xt), (21) theformin(16)and,fori∈{1,2,3},a⊕ ib=β ia+γ ib,
t,i t where{β ,γ }arealsotrained. Theplotdemonstratesthe
i i
i=1
benefitofemployingmoreexpressivedecoders:
t
(cid:88)
zt =Bxˆt− α zˆi, zˆt+1 =g (z1,··· ,zt,zˆ1).
t,i t 1. ThegreendotsareobtainedviaSGDtrainingofthe
i=1 autoencoderin(2)and,asprovedinTheorem4.1,they
Here,f t,g tareLipschitzandappliedcomponent-wise,and matchtheGaussianperformanceR Gauss.
theinitializationiszˆ1 =sign(Bx). Thecoefficients{β }
t,i 2. The blue dots are obtained via SGD training of the
and{α }arechosensothat,undersuitableassumptionson
t,i autoencoder in (4) and they match the prediction of
B,3theempiricaldistributionoftheiteratesistrackedviaa
Proposition5.1withf =f∗in(19).
low-dimensionalrecursion,knownasstateevolution. This
inturnallowstoevaluatetheMSElim 1∥x−xˆt∥2. 3. Theorangedotsareobtainedbyusingthedecoderin
d→∞ d 2
(22) where W = W = B⊤, V = B are sub-
1 2 1
The results of Proposition 4.2 and 5.1 follow from relat-
sampledHaarmatricesandtheparametersinthefunc-
ing the autoencoders in (2)-(4) to RI-GAMP iterates in
tionsf ,f ,g ,{⊕ }3 aretrainedviaSGD.Similar
1 2 1 i i=1
3Bhastobebi-rotationallyinvariantinlaw,namely,thematri- resultsareobtainedbytrainingalsoW 1,W 2,V 1,al-
cesappearinginitsSVDaresampledfromtheHaardistribution. thoughatthecostofaslowerconvergence.
8
ESM
ESMCompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
Insummary,thearchitecturein(22)improvesuponthose Berthier, R. Incremental learning in diagonal linear net-
in(2)-(4),anditapproachestheorangecurvewhichgives works. JournalofMachineLearningResearch,2023.
theBayes-optimalMSEachievablebyfixingarotationally
Berthier,R.,Montanari,A.,andZhou,K. Learningtime-
invariantencodermatrixB (Maetal.,2021). Additional
scales in two-layers neural networks. arXiv preprint
detailsaredeferredtoAppendixC.7.
arXiv:2303.00055,2023.
We also note that considering a deep fully-connected de-
coderinplaceofthearchitecturein(22)doesnotimprove Boufounos, P. T. and Baraniuk, R. G. 1-bit compressive
upontheautoencoderin(4). Infact,whilesufficientlywide sensing. Conference on Information Sciences and Sys-
anddeepmodelshavehighexpressivity, theirSGDtrain- tems,2008.
ingisnotoriouslydifficult,duetoe.g.vanishing/exploding
Cover,T.M.andThomas,J.A. ElementsofInformation
gradients(Glorot&Bengio,2010;Heetal.,2016).
Theory. 2006.
Acknowledgements Cui,H.andZdeborova´,L.High-dimensionalasymptoticsof
denoisingautoencoders. AdvancesinNeuralInformation
KevinKo¨gler,AlexanderShevchenkoandMarcoMondelli ProcessingSystems,2023.
are supported by the 2019 Lopez-Loreta Prize. Hamed
HassaniacknowledgesthesupportbytheNSFCIFaward Dytso,A.,Poor,H.V.,andShitz,S.S. Ageneralderivative
(1910056)andtheNSFInstituteforCOREEmergingMeth- identityfortheconditionalmeanestimatoringaussian
odsinDataScience(EnCORE). noiseandsomeapplications. IEEEInternationalSympo-
siumonInformationTheory,2020.
References
Feng,O.Y.,Venkataramanan,R.,Rush,C.,Samworth,R.J.,
etal.Aunifyingtutorialonapproximatemessagepassing.
Abbe,E.,Boix-Adsera,E.,Brennan,M.S.,Bresler,G.,and
FoundationsandTrends®inMachineLearning,2022.
Nagaraj, D. The staircase property: How hierarchical
structurecanguidedeeplearning. AdvancesinNeural
Gidel, G., Bach, F., andLacoste-Julien, S. Implicitregu-
InformationProcessingSystems,2021.
larizationofdiscretegradientdynamicsinlinearneural
networks. AdvancesinNeuralInformationProcessing
Abbe,E.,Adsera,E.B.,andMisiakiewicz,T. Themerged-
Systems,2019.
staircaseproperty: anecessaryandnearlysufficientcon-
ditionforSGDlearningofsparsefunctionsontwo-layer Glorot,X.andBengio,Y. Understandingthedifficultyof
neuralnetworks. ConferenceonLearningTheory,2022. trainingdeepfeedforwardneuralnetworks. International
ConferenceonArtificialIntelligenceandStatistics,2010.
Abbe,E.,Adsera,E.B.,andMisiakiewicz,T. SGDlearning
onneuralnetworks:leapcomplexityandsaddle-to-saddle He, K., Zhang, X., Ren, S., and Sun, J. Deep residual
dynamics. ConferenceonLearningTheory,2023a. learning for image recognition. IEEE Conference on
ComputerVisionandPatternRecognition,2016.
Abbe, E., Bengio, S., Boix-Adsera`, E., Littwin, E., and
Susskind,J.M. Transformerslearnthroughgradualrank Jacot, A., Ged, F., S¸ims¸ek, B., Hongler, C., and Gabriel,
increase. Advances in Neural Information Processing F. Saddle-to-saddle dynamics in deep linear networks:
Systems,2023b. Small initialization training, symmetry, and sparsity.
arXivpreprintarXiv:2106.15933,2021.
Agustsson, E., Mentzer, F., Tschannen, M., Cavigelli, L.,
Kingma,D.P.andWelling,M. Auto-encodingvariational
Timofte, R., Benini, L., and Gool, L. V. Soft-to-hard
bayes. InternationalConferenceonLearningRepresen-
vectorquantizationforend-to-endlearningcompressible
tations,2014.
representations.AdvancesinNeuralInformationProcess-
ingSystems,2017.
Kunin,D.,Bloom,J.,Goeva,A.,andSeed,C. Lossland-
scapesofregularizedlinearautoencoders. International
Balle´, J., Laparra, V., and Simoncelli, E. P. End-to-end
ConferenceonMachineLearning,2019.
optimizedimagecompression. InternationalConference
onLearningRepresentations,2017. Li,Y.,Fan,Z.,Sen,S.,andWu,Y.Randomlinearestimation
withrotationally-invariantdesigns: Asymptoticsathigh
Bao,X.,Lucas,J.,Sachdeva,S.,andGrosse,R.B. Regu-
temperature. IEEETransactionsonInformationTheory,
larizedlinearautoencodersrecovertheprincipalcompo-
2023.
nents,eventually. AdvancesinInformationProcessing
Systems,2020. Ma,J.andPing,L. OrthogonalAMP. IEEEAccess,2017.
9CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
Ma,J.,Xu,J.,andMaleki,A. Analysisofsensingspectral Sze´kely,E.,Bardone,L.,Gerace,F.,andGoldt,S. Learning
forsignalrecoveryunderageneralizedlinearmodel. Ad- fromhigher-orderstatistics,efficiently: hypothesistests,
vancesinNeuralInformationProcessingSystems,2021. random features, and neural networks. arXiv preprint
arXiv:2312.14922,2023.
McCullagh,P.andNelder,J.A. Generalizedlinearmodels.
MonographsonStatisticsandAppliedProbability.1989. Takeda,K.,Uda,S.,andKabashima,Y. AnalysisofCDMA
systems that are characterized by eigenvalue spectrum.
Milanesi,P.,Kadri,H.,Ayache,S.,andArtie`res,T. Implicit
EurophysicsLetters,2006.
regularizationindeeptensorfactorization. IEEEInterna-
tionalJointConferenceonNeuralNetworks,2021. Takeuchi, K. Rigorous dynamics of expectation-
propagation-basedsignalrecoveryfromunitarilyinvari-
Mondelli,M.andVenkataramanan,R. Approximatemes-
ant measurements. IEEE Transactions on Information
sagepassingwithspectralinitializationforgeneralized
Theory,2019.
linearmodels. JournalofStatisticalMechanics: Theory
andExperiment,2022. Theis,L.,Shi,W.,Cunningham,A.,andHusza´r,F. Lossy
imagecompressionwithcompressiveautoencoders.Inter-
Nguyen,P.-M. Analysisoffeaturelearninginweight-tied
nationalConferenceonLearningRepresentations,2017.
autoencoders via the mean field lens. arXiv preprint
arXiv:2102.08373,2021. Tulino,A.M.,Caire,G.,Verdu´,S.,andShamai,S. Support
recovery with sparsely sampled free random matrices.
Oftadeh,R.,Shen,J.,Wang,Z.,andShell,D. Eliminating
IEEETransactionsonInformationTheory,2013.
the invariance on the loss landscape of linear autoen-
coders. InternationalConferenceonMachineLearning, VanDenOord,A.,Vinyals,O.,etal. Neuraldiscreterep-
2020. resentation learning. Advances in Neural Information
ProcessingSystems,2017.
Peng,P.,Jalali,S.,andYuan,X. Solvinginverseproblems
via auto-encoders. IEEE Journal on Selected Areas in Venkataramanan,R.,Ko¨gler,K.,andMondelli,M. Estima-
InformationTheory,2020. tioninrotationallyinvariantgeneralizedlinearmodelsvia
approximatemessagepassing. InternationalConference
Pesme, S. and Flammarion, N. Saddle-to-saddle dy-
onMachineLearning,2022.
namics in diagonal linear networks. arXiv preprint
arXiv:2304.00488,2023. Vershynin,R. High-dimensionalprobability: Anintroduc-
tionwithapplicationsindatascience. 2018.
Rangan,S.Generalizedapproximatemessagepassingfores-
timationwithrandomlinearmixing. IEEEInternational Visick, G. A quantitative version of the observation that
SymposiumonInformationTheory,2011. the hadamard product is a principal submatrix of the
kroneckerproduct. LinearAlgebraandItsApplications,
Rangan,S.,Schniter,P.,andFletcher,A.K. Vectorapproxi-
2000.
matemessagepassing.IEEETransactionsonInformation
Theory,2019. Winkelbauer, A. Moments and absolute moments of the
normal distribution. arXiv preprint arXiv:1209.4340,
Refinetti,M.andGoldt,S. Thedynamicsofrepresentation
2012.
learning in shallow, non-linear autoencoders. Interna-
tionalConferenceonMachineLearning,2022. Yin, P., Lyu, J., Zhang, S., Osher, S., Qi, Y., and Xin,
J. Understanding straight-through estimator in train-
Schniter,P.,Rangan,S.,andFletcher,A.K. Vectorapproxi-
ing activation quantized neural nets. arXiv preprint
matemessagepassingforthegeneralizedlinearmodel.In
arXiv:1903.05662,2019.
AsilomarConferenceonSignals,SystemsandComputers,
2016.
Shevchenko, A., Ko¨gler, K., Hassani, H., and Mondelli,
M. Fundamentallimitsoftwo-layerautoencoders,and
achieving them with gradient methods. International
ConferenceonMachineLearning,2023.
Simon,J.B.,Knutins,M.,Ziyin,L.,Geisz,D.,Fetterman,
A. J., and Albrecht, J. On the stepwise nature of self-
supervisedlearning. arXivpreprintarXiv:2303.15438,
2023.
10CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
A.ProofofTheorem4.1
A.1.Additionalnotation
GiventwomatricesM andM ofthesameshape,theirelement-wiseSchurproductisM ◦M andtheℓ-thelement-wise
1 2 1 2
powerisM◦ℓ. Thesamenotationisadoptedfortheelement-wiseproductofvectors,i.e.,v◦u. Byconvention,ifB is
1 i,:
avectorofzeroes,itsnormalizationBˆ isalsoavectorofzeroes. Wefixtheevaluationofsign(·)attheorigintobea
i,:
Rademacherrandomvariable,i.e.,sign(0)takesvaluesintheset{−1,1}withequalprobability. Notethatthisisatechnical
assumptionwithnobearingontheproofoftheresult.
ForamatrixB,wedenoteitsi-throwbyb =B ,theexceptionbeingthatbyconventiona denotesthek-thcolumn
i i,: k
ofA. B isthemaskedversionofamatrixB,wheremaskingisdefinedasb¯ = b ◦mandmhasi.i.d.Bernoulli(p)
m i i
components. Forconvenienceofnotation,wedefineB¯ =B onlyforthematrixB. Byconvention,maskinghaspriority
m
overtransposing,i.e.,B⊤ =(B )⊤. ForB(andonlyB),wedefineBˆ =Bˆ =DˆB¯ ,whereDˆ isadiagonalmatrix
m m m m
withentriesDˆ
i,i
=1/(cid:13) (cid:13)b¯ i(cid:13) (cid:13),asthemaskedandre-normalizedversionofB. Wedefine∥B∥
max
:=max i,j|B i,j|.
Weusethefollowingconventionfortheconstants. Allconstantsareindependentofdincludingthosethataredependenton
thequantitiesp,r,f(x)=arcsin(x),α=f(1)−1andthedependenceonthesequantitieswillbesuppressedmostofthe
time. UppercaseconstantslikeC,C ,C shouldbethoughtofasbeingmuchlargerthan1,whereaslowercaseconstants
X R
shouldbethoughtofasbeingsmallerthan1.
Foravector,thenorm∥·∥withoutsubscriptalwaysreferstothe2-norm∥·∥ . Unlessstatedotherwise,weconsiderthespace
2
ofmatricesM tobeendowedwith∥·∥ . ForamatrixR,wedenotebyO(R)amatrixofthesamedimensionsasR
n×d op
with∥O(R)∥ ≤ C∥R∥ . ThisisawaytoextendthebigOnotationtomatrices. Similarly,wewillusethenotation
op op
O whichfunctionsasOexceptthat∥·∥ isreplacedby∥·∥ . Wewilloftenusethatn=O(d),sincer = n isfixed.
max op max d
A.2.Outline
Thestartofouranalysisisthefollowinglemma.
LemmaA.1. LetR(·,·)betheMSEdefinedin(3),withx∼SG (p). AssumethatallentriesofBarenotzero. Then,up
d
toamultiplicativescalingandanadditiveconstant,R(A,B)isgivenby
E (cid:20) Tr(cid:104) A⊤Af(BˆBˆ⊤ )(cid:105) − √2 Tr(cid:104) ABˆ(cid:105)(cid:21) +O(cid:16) (1−p)d∥A∥2 (cid:17) , (23)
m̸=0 p op
wheref =arcsinisappliedcomponent-wiseandthesecondtermontheRHSisindependentofB.
Proof. Foranym ̸= 0wecanfixmandapplyLemma4.1in(Shevchenkoetal.,2023). ThesecondtermontheRHS
correspondstom=0.
Wenowbrieflyelaborateonsometechnicaldetails. First,byconvention,allexpectationsovermareunderstoodtobeover
m̸=0. Second,asthelasttermontheRHSin(23)doesnotdependonB,itsufficestotakethegradientoftheobjective
(cid:16) (cid:17)
withoutit. Lastly,thetermO (1−p)d∥A∥2 hasanegligibleeffectwhenrunningthegradientdescentalgorithmin(6).
op
Infact,aby-productofouranalysisisthatAhasboundednormthroughoutthetrainingtrajectory,seeLemmaA.14. Hence,
(cid:16) (cid:17)
thequantityO (1−p)d∥A∥2 isexponentiallysmallindand,therefore,itcanbeincorporatedintheerroroforder
op
Cpoly( √log(d)) beingtrackedduringtherecursion.
d
Asaresult,wecanconsidertheobjective
E (cid:20) Tr(cid:104) A⊤Af(BˆBˆ⊤ )(cid:105) − √2 Tr(cid:104) ABˆ(cid:105)(cid:21) , (24)
m p
wherem ∈ {0,1}d denotesamaskwithi.i.d.Bernoulli(p)entries,bˆ = m◦b /∥m◦b ∥ and(1−p)isthesparsity.
i i i 2
Thus,theRiemanniangradientdescentalgorithm(6)appliedtotheobjective(24)canberewrittenas
1 (cid:16) (cid:17)−1
A(t)= √ E Bˆ(t)⊤ E f(Bˆ(t)Bˆ(t)⊤) ,
p m m (25)
B′(t):=B(t)−η(cid:0) ∇ +G(t)(cid:1) ,B(t+1):=proj(B′(t)),
B(t)
11CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
where A(t) is the optimal matrix for a fixed B(t), ∇ is defined below in (29) and (G(t)) ∼ N(0,σ2) with
B(t) i,j
d−γg ≤σ ≤C/dforsomefixed1<γ
g
<∞.
ThegoalofthisAppendixistoshowthefollowingtheorem.
Theorem A.2. Consider the gradient descent (25) with x ∼ SG (p). Initialize the algorithm with B(0) equal to a
d
row-normalizedGaussian,i.e.,B′ (0)∼N(0,1/d),B(0)=proj(B′(0)),andletB(0)=US(0)V⊤ beitsSVD.Let
√ i,j
the step size η be Θ(1/ d). Then, for any fixed r < 1 and T ∈ (0,∞), with probability at least 1−Cd−3/2, the
max
followingholdsforallt≤T /η
max
B(t)=US(t)V⊤+R(t),
(cid:13) (cid:13)S(t)S(t)⊤−I(cid:13)
(cid:13) op ≤Cexp(−cηt), (26)
lim sup ∥R(t)∥ =0,
op
d→∞t∈[0,Tmax/η]
whereC,careuniversalconstantsdependingonlyonp,randT . Moreover,wehavethat,almostsurely,
max
lim lim R(A(t),B(t))=R ,
Gauss
t→∞d→∞
(27)
lim sup ∥B(t)−B (t)∥ =0,
Gauss op
d→∞t∈[0,Tmax/η]
whereR isdefinedin(5)andB (t)isobtainedbyrunning(25)withx∼N(0,I).
Gauss Gauss
Letusprovideahigh-leveloverviewoftheproofstrategy. Usinghigh-dimensionalprobabilitytools,wewillshowthatwith
highprobability
B(t)=X(t)+R(t),
log(d)
∥X(t)∥ ≤C , ∥X(t)∥ ≤C √ , X(t)=US(t)V, withU,V Haar,
op X max X d
logαR(d) (28)
∥R(t)∥ ≤C √ ,
op R d
A(t)=X(t)⊤(cid:0) X(t)X(t)⊤+αI(cid:1)−1
+O(cid:18)
C10log
√10(d)(cid:19)
+O(R),
X
d
whereα=f(1)−1=arcsin(1)−1. Thisimpliesthatthegradientin(29)concentratestotheGaussianone,namely,to
thegradientobtainedforx∼N(0,I). Then,ana-prioriGro¨nwall-typeinequalitywillextendtheseboundsforalltimes
t ∈ [0,T ]. ItisessentialthattheconstantsC ,C in(28)canbechosentoonlydependonT ,asotherwisethe
max X R max
gradientdynamicscoulddivergeinfinitetime. Thus,itiscrucialthatinallourlemmaswekeeptrackoftheseconstants
explicitlyandthatintheerrorestimatestheydonotdependoneachother.Whiletheanalysisisquitetechnical,thehigh-level
ideaissimple: ifeachtermthatdependsonmwerereplacedbyitsmean,thenwewouldimmediatelyrecovertheGaussian
casep=1whichwasstudiedin(Shevchenkoetal.,2023). Byshowingthateachofthetermsconcentrateswellenough,we
canmakethisintuitionrigorous. Themaintechnicaldifficultyliesincontrollingtheadditionalerrorterms,whichrequiresa
morenuancedapproachcomparedtotheGaussiancaseconsideredin(Shevchenkoetal.,2023).
Therestofthisappendixisstructuredasfollows. SectionA.3containsacollectionofauxiliaryresultsthataresimple
applicationsofstandardresults. InSectionA.4,wedevelopourhigh-dimensionalconcentrationtools. InSectionA.5,we
usethesetoolstoshowthatA,Bˆ,∇ allconcentrate. FinallyinSectionA.6,wecombinetheseapproximationswithan
B
a-prioriGro¨nwallboundinLemmaA.19,whichallowsustoboundthedifferencebetweenthegradienttrajectoryandthat
obtainedwithGaussiandata.
A.3.Auxiliaryresults
Astraightforwardcomputationgives:
LemmaA.3(Gradientformulas). Thederivativeof (24)w.r.t. Bisgivenby
 
∞
(∇ B)
k,:
=E m−2√1 pm◦Jˆ ka k+2(cid:88) ℓc2 ℓ(cid:88) ⟨a k,a j⟩⟨bˆ k,bˆ j⟩ℓ−1Jˆ kbˆ j, (29)
ℓ=1 j̸=k
12CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
whereJˆ = 1
(cid:16)
I−bˆ
bˆ⊤(cid:17)
,a =A andc2aretheTaylorcoefficientsofarcsin(x).
k ∥b¯ k∥ k k k :,k ℓ
Wewillmakeextensiveuseofthefollowinglinearalgebraresults.
LemmaA.4(Linearalgebraresults). Thefollowingresultshold:
1.
(cid:13) (cid:13)B¯(cid:13)
(cid:13) ≤∥B∥ .
op op
√ (cid:13) (cid:13) √
2. ForanyM ∈Rn×d,wehave∥M∥ ≤ nmax ∥M ∥. Inparticular,(cid:13)Bˆ(cid:13) ≤ n.
op k k,: (cid:13) (cid:13)
op
3. ForsquarematricesM ,M ∈Rn×n,
1 2
√
∥M ◦M ∥ ≤ n∥M ∥ ∥M ∥ . (30)
1 2 op 1 op 2 max
4.
ForanysquarematrixM,wehave(cid:13) (cid:13)(11⊤−I)◦M(cid:13)
(cid:13) ≤2∥M∥ .
op op
5. ForanysquarematrixM ∈Rn×n,wehave∥M∥ ≤n∥M∥ .
op max
Proof. 1. Thisfollowsdirectlyfromthevariationalcharacterizationoftheoperatornorm,i.e.,
(cid:13) (cid:13)B¯(cid:13)
(cid:13) = sup
(cid:13) (cid:13)B¯u(cid:13)
(cid:13)= sup ∥B(m◦u)∥≤ sup ∥Bu∥,
op
∥u∥≤1 ∥u∥≤1 ∥u∥≤1
wherethelaststepfollowsfrom∥m◦u∥≤∥u∥.
2. For∥v∥=1,wehave
(cid:118) (cid:118)
∥Mv∥=(cid:117)
(cid:117)
(cid:116)(cid:88)n
⟨M k,:,v⟩2
≤(cid:117)
(cid:117)
(cid:116)(cid:88)n
∥M k,:∥2
≤(cid:114)
nmax∥M k,:∥2
=√
nmax∥M k,:∥.
k k
k=1 k=1
3. Foraunitvectore ,wehave
i
∥M ◦M e ∥≤∥M ∥ ∥M e ∥≤∥M ∥ ∥M ∥ .
1 2 i 2 max 1 i 2 max 1 op
Forageneralvectorv,wecanusethetriangleinequalitytoobtain
(cid:88) (cid:88)
∥M ◦M v∥≤ |v |∥M ∥ ∥M e ∥≤∥M ∥ ∥M ∥ |v |.
1 2 i 2 max 1 i 2 max 1 op i
i i
Byusing
(cid:88) √
|v |≤ n∥v∥,
i
i
weobtainthedesiredbound.
4. Notethat(11⊤−I)◦M =M −Diag(M),so
(cid:13) (cid:13)(11⊤−I)◦M(cid:13)
(cid:13) ≤∥M∥ +∥Diag(M)∥ ≤2∥M∥ ,
op op op op
wherewehaveusedthat
∥Diag(M)∥ =max|M |≤∥M∥ .
op i,i op
i
√
5. Notethat∥M ∥≤ n∥M∥ . Thus,theresultfollowsfromthepoint2. above.
k,: max
13CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
LemmaA.5. Denotebyc2theℓ-thTaylorcoefficientofthefunctionarcsin(x). Then,forx∈[0,1),ℓ ∈N ,wehave
ℓ 0 +
∞
(cid:88) 1
ℓc2xℓ−1 ≤C(ℓ )xℓ0−1√ . (31)
ℓ 0 1−x
ℓ=ℓ0
Proof. Recallthat
∞
d 1 (cid:88)
arcsin(x)= √ = ℓc2xℓ−1,
dx 1−x ℓ
ℓ=1
with
(2k)!
c =0, c2 = .
2k 2k+1 4k(k!)2(2k+1)
ByStirling’sapproximationwehave
(cid:18) (cid:19)
1
c2 =Θ ,
2k+1 k3
2
whichimpliesthat,foroddℓ,ℓ >0
0
ℓc2
ℓ
≤Cℓ(ℓ+ℓ 0−1)23
≤C(ℓ ).
(ℓ+ℓ 0−1)c2
ℓ+ℓ0−1
(ℓ+ℓ 0−1)ℓ23 0
Thuswehave
∞ ∞ ∞ ∞
(cid:88) (cid:88) (cid:88) (cid:88) 1
ℓc2xℓ−1 =xℓ0−1 ℓc2xℓ−ℓ0 =xℓ0−1 (ℓ+ℓ −1)c2 xℓ−1 ≤xℓ0−1 C(ℓ )ℓc2xℓ−1 =C(ℓ )xℓ0−1√ ,
ℓ ℓ 0 ℓ+ℓ0−1 0 ℓ 0 1−x
ℓ=ℓ0 ℓ=ℓ0 ℓ=1 ℓ=1
whichfinishestheproof.
LemmaA.6. AssumethatB =X+Rwith∥X∥
op
≤C X,∥X∥
max
≤C Xlo √g( dd),∥R∥
op
≤C Rlog√α (R d( )d). Then,forlarge
enoughd,wehave
log(d)
∥b ∥2 ≤CC2 √ . (32)
i 4 X d
Proof. ByHo¨lder,wehave
1 1 1 1
∥r i∥
4
≤∥r i∥ 22 ∥r i∥ ∞2 ≤∥R∥ o2 p∥R∥ o2
p
=∥R∥ op,
and
1 1 1 1
∥x i∥
4
≤∥x i∥ 22 ∥x i∥ ∞2 ≤∥X∥ o2 p∥X∥ m2 ax,
so
log(d) (logαR(d))2
∥b ∥2 ≤(∥x ∥ +∥r ∥ )2 ≤C(∥x ∥2+∥r ∥2)≤CC2 √ +CC2 ,
i 4 i 4 i 4 i 4 i 4 X d R d
whichimplies(32).
LemmaA.7(ConcentrationofDˆ). Forb∈Rd,∥b∥=1andm∼Bern(p)i.i.d.,wehave
(cid:32) (cid:33)
(cid:16)(cid:12) (cid:12) (cid:17) λ2
P (cid:12)∥m◦b∥2−p∥b∥2(cid:12)>λ ≤Cexp −c , (33)
(cid:12) 2 2(cid:12) ∥b∥4
4
whichimplies
(cid:32) (cid:33)
√ λ2
P(|∥m◦b∥ − p∥b∥ |>λ)≤Cexp −c . (34)
2 2 ∥b∥4
4
14CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
Proof. Equation(33)isanimmediateconsequenceofHoeffding’sinequalityappliedtotherandomvariables(m −p)b2
i i
(cf.Theorem2.6.2in(Vershynin,2018)).
(cid:12) √ (cid:12)
Toobtain(34)wenotethat(cid:12)∥m◦b∥ − p∥b∥ (cid:12)>λimplies
2 2
(cid:12) (cid:12) √ √ √
(cid:12)∥m◦b∥2−p∥b∥2(cid:12)=|∥m◦b∥ − p∥b∥ ||∥m◦b∥ + p∥b∥ |> p∥b∥ λ.
(cid:12) 2 2(cid:12) 2 2 2 2 2
Sincebyassumption∥b∥=1,theaboveimplies(34).
LemmaA.8. LetU ∈Rn×nbeaHaarmatrixandM ∈Rn×nanindependentrandommatrixwith∥M∥ ≤C . Then,
op M
forY :=UMU⊤,wehave
(cid:18)(cid:13) (cid:13) (cid:19) (cid:18) (cid:19)
P (cid:13) (cid:13)Y − 1 Tr[Y]I(cid:13) (cid:13) >λ ≤Cd2exp − c dλ2 . (35)
(cid:13) n (cid:13) C2
max M
IfinsteadwehaveM ∈Rn×dwith n ≤C andY :=UM,then
d
(cid:18) (cid:19)
c
P(∥Y∥ >λ)≤Cd2exp − dλ2 . (36)
max C2
M
Proof. WefirstfixM,andnotethatsinceM andU areindependent,thedistributionofU doesnotchangeifwecondition
onM. Forbothinequalities,forfixedM,themap(SO ,∥·∥ )→(M ,∥·∥ ),U →Y isLipschitzasitisabounded
n F n×d op
(bi-)linear form on a bounded set. The composition with the projection on the (i,j)-th component of a matrix is also
Lipschitz,sowecanapplyTheorem5.2.7in(Vershynin,2018)toobtainthatY issubgaussianinU withsubgaussian
i,j
norm C√CM. Formallythismeansthat
d
(cid:18) (cid:19)
c
P(|Y −EY |>λ|M)≤Cexp − dλ2 .
U
i,j i,j C2
M
SincetheRHSisindependentofM (i.e.,itonlydependsonC )wehave
M
(cid:18) (cid:19)
P(|Y −EY |>λ)= P P (|Y −EY |>λ|M)
i,j i,j i,j i,j
M U,V
(cid:18) (cid:18) (cid:19)(cid:19)
c
≤ P Cexp − λ2
M C2 d
M
(cid:18) (cid:19)
c
=Cexp − λ2 .
C2 d
M
(cid:104) (cid:105)
Now,(35)followsbynotingthatEUMU⊤ = 1Tr UMU⊤ I andusingasimpleunionboundover(i,j). Theproofof
n
(36)isthesame,withtheonlydifferencebeingEY =0.
i,j
Lemma A.9. Let U ∈ Rn×n,V ∈ Rd×d be Haar matrices, and S ,S⊤ ∈ Rn×d be deterministic diagonal matrices.
1 2
DefineM¯ = S (V⊤) (V⊤)⊤S ,Y¯ = UM¯ U⊤,Y = pUS S U⊤, C := ∥S ∥ ∥S ∥ . Then, foranyγ > 0
1 m m 2 1 2 M 1 op 2 op
andd>d (γ),wehavewithprobabilityatleast1−C/d2(inU,V)
0
mP(cid:18)(cid:13) (cid:13) (cid:13) (cid:13)Y¯ − n1 Tr[Y]I(cid:13) (cid:13) (cid:13)
(cid:13)
>CC Mlog √3 d(d)(cid:12) (cid:12) (cid:12) (cid:12)U,V(cid:19) ≤C d1 γ. (37)
max
Proof. Inthefirststepwewillshowthatwithprobabilityatleast1−C/d2inU,V
mP(cid:18)(cid:13) (cid:13) (cid:13) (cid:13)Y¯ − n1 Tr(cid:2) Y¯(cid:3) I(cid:13) (cid:13) (cid:13)
(cid:13)
>C Mlog √3 d(d)(cid:12) (cid:12) (cid:12) (cid:12)U,V(cid:19) ≤C d1 γ. (38)
max
15CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
Thekeyobservationisthat
(cid:18) (cid:18)(cid:13) (cid:13) (cid:12) (cid:19) (cid:19) (cid:18)(cid:13) (cid:13) (cid:12) (cid:19)
UP
,V
mP (cid:13) (cid:13) (cid:13)Y¯ − n1 Tr(cid:2) Y¯(cid:3) I(cid:13) (cid:13)
(cid:13)
>C Mλ(cid:12) (cid:12) (cid:12)U,V >α ≤ α1 E U,V mP (cid:13) (cid:13) (cid:13)Y¯ − n1 Tr(cid:2) Y¯(cid:3) I(cid:13) (cid:13)
(cid:13)
>C Mλ(cid:12) (cid:12) (cid:12)U,V
max max
(cid:18)(cid:13) (cid:13) (cid:19)
= α1 P (cid:13) (cid:13) (cid:13)Y¯ − n1 Tr(cid:2) Y¯(cid:3) I(cid:13) (cid:13)
(cid:13)
>C Mλ
max
(cid:18) (cid:19)
1 c
≤C d2exp − dC2 λ2
α C2 M
M
=C1 d2exp(cid:0) −cdλ2(cid:1)
,
α
where the first passage follows from Markov’s inequality and the last inequality is due to Lemma A.8 and C =
M
∥S 1∥ op∥S 2∥
op
≥(cid:13) (cid:13)M¯ (cid:13) (cid:13) op.
Bychoosingα= 1 andλ= log √3(d),weobtainthat,withprobabilityatleast1−C/d2inU,V,
dγ d
mP(cid:18)(cid:13) (cid:13) (cid:13) (cid:13)Y¯ − n1 Tr(cid:2) Y¯(cid:3) I(cid:13) (cid:13) (cid:13)
(cid:13)
>C Mlog √3 d(d)(cid:12) (cid:12) (cid:12) (cid:12)U,V(cid:19) ≤ d1 γ. (39)
max
Now,inthesecondstep,wewillshowthat,withprobabilityatleast1−C/d2overV,
mP(cid:18)(cid:13) (cid:13)
(cid:13)
(cid:13)n1 Tr(cid:2) Y¯(cid:3)
I−
n1 Tr[Y]I(cid:13) (cid:13)
(cid:13)
(cid:13)
>C
Mlog √3 d(d)(cid:12) (cid:12)
(cid:12)
(cid:12)U,V(cid:19)
≤
d1
γ. (40)
max
First,notethatbyLemmaA.8withprobabilityatleast1−C/d2(inV)wehavethat∥V∥
max
≤Clo √g( dd),so∥V :,i∥4
4
≤
Clog4(d).
ByLemmaA.7,wehavethat
d
P(cid:18)(cid:12) (cid:12) (cid:12) (cid:12)(cid:16) (V⊤) m(V⊤)⊤ m(cid:17) i,i−p(cid:12) (cid:12) (cid:12) (cid:12)>λ(cid:19) ≤Cexp(cid:18) −c lod gλ 4(2 d)(cid:19) .
Choosingλ= log √3(d) weobtainforlarged
d
P(cid:18)(cid:12) (cid:12) (cid:12) (cid:12)(cid:16) (V⊤) m(V⊤)⊤ m(cid:17) i,i−p(cid:12) (cid:12) (cid:12) (cid:12)> log √3 d(d)(cid:19) ≤C dγ1 +1.
Byasimpleunionboundweobtain
(cid:18)(cid:13) (cid:16) (cid:17) (cid:13) log3(d)(cid:19) 1
P (cid:13)Diag (V⊤) (V⊤)⊤ −pI(cid:13) > √ ≤C .
(cid:13) m m (cid:13) op d dγ
NotethatsinceS ,S arediagonalwehave
1 2
(cid:104) (cid:105) (cid:104) (cid:16) (cid:17) (cid:105)
Tr S (V⊤) (V⊤)⊤S =Tr S Diag (V⊤) (V⊤)⊤ S ,
1 m m 2 1 m m 2
(cid:13) (cid:16) (cid:17) (cid:13)
so(cid:13) (cid:13)Diag (V⊤) m(V⊤)⊤
m
−pI(cid:13)
(cid:13)
op
≤ log √3 d(d) implies
(cid:12) (cid:104) (cid:105) (cid:12) (cid:12) (cid:104) (cid:16) (cid:16) (cid:17) (cid:17) (cid:105)(cid:12)
(cid:12)Tr S (V⊤) (V⊤)⊤S −pTr[S S ](cid:12)=(cid:12)Tr S Diag (V⊤) (V⊤)⊤ −pI S (cid:12)
(cid:12) 1 m m 2 1 2 (cid:12) (cid:12) 1 m m 2 (cid:12)
(cid:13) (cid:16) (cid:16) (cid:17) (cid:17) (cid:13)
≤n(cid:13)S Diag (V⊤) (V⊤)⊤ −pI S (cid:13)
(cid:13) 1 m m 2(cid:13)
op
log3(d)
≤C n √ .
M
d
16CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
Thus,
P(cid:18)(cid:12) (cid:12) (cid:12) (cid:12)n1 Tr(cid:104) S 1(V⊤) m(V⊤)⊤ mS 2(cid:105) −p n1 Tr[S 1S 2](cid:12) (cid:12) (cid:12) (cid:12)>C Mlog √3 d(d)(cid:19) ≤C d1 γ.
Notingthat,bydefinition,Tr(cid:2) Y¯(cid:3) =Tr(cid:104) S (V⊤) (V⊤)⊤S (cid:105) andTr[Y]=pTr[S S ],weobtain(40).
1 m m 2 1 2
Finally,combining(39)and(40)finishestheproof.
LemmaA.10. LetB ∈Rn×dbeanarbitrarymatrix,G∈Rn×dwithG ∼N(0,σ2),andassumen=O(d). Then,for
ij
anyδ >0,wehave
(cid:18) (cid:19)
δ
P min|B +G |≤δ ≤Cd2 .
G i,j ij ij σ
Proof. Bythescaleinvarianceoftheproblem,wemayassumethatσ =1. Letgbeastandardnormalvariableandb∈R.
Then,
P(|b+g|≤δ)=P(g ∈[b−δ,b+δ])≤Cδ,
wherethesecondstepholdssincethepdfofgisboundedbyauniversalconstant. Theresultofthelemmanowfollowsbya
simpleunionboundoverall(i,j)(andusingn=O(d)).
A.4.Concentrationtools
In this section, we provide the matrix concentration results needed for the proof. We recall that we use the shorthand
notationB¯ =B mandBˆ =Bˆ
m
=DˆB¯ m,Dˆ
i,i
=1/(cid:13) (cid:13)b¯ i(cid:13) (cid:13)onlyforthematrixB. Here,themaskingB mwasdefinedas
(B ) =B m .
m i,j i,j j
(cid:16) (cid:17)
LemmaA.11. LetB =X +R=USV⊤+R,A=X⊤(XX⊤+αI)−1+O(R)+O C7 log √10(d) ,whereU,V
X d
(cid:104) (cid:105)
areHaarmatrices,∥R∥ =o(1),S isadiagonalmatrixs.t.∥S∥ ≤C , 1Tr SS⊤ =1,andα>0fixed. Then,for
op op X n
anyγ >0,d>d (γ),withprobabilityatleast1−C/d2inU,V andatleast1−C/dγ inm,
0
(cid:16) (cid:17)
1. ∥B∥
max
≤C Xlo √g( dd) +O ∥R∥
op
.
(cid:18)(cid:16) (cid:17)−2 (cid:19) (cid:20)(cid:16) (cid:17)−2 (cid:21) (cid:16) (cid:17)
2. Diag BB⊤+αI BB⊤ = 1Tr BB⊤+αI BB⊤ I+O lo √g(d) .
n d
3. (cid:13) (cid:13) (cid:13)A⊤A− 1Tr(cid:20)(cid:16) XX⊤+αI(cid:17)−2 XX⊤(cid:21) I(cid:13) (cid:13) (cid:13) ≤C7 log √10(d) +O(cid:16) ∥R∥ (cid:17) .
(cid:13) n (cid:13) X d op
max
(cid:104) (cid:105) (cid:16) (cid:17)
4. p1B¯B¯⊤ − n1Tr BB⊤ I =O max(CC X2 log √3 d(d))+O C X∥R∥
op
.
(cid:16) (cid:17) (cid:16) (cid:17)
5. Diag p1B¯A = n1Tr[BA]I+O C X8 log √10 d(d) +O(C XR).
6. Diag(cid:16) 1A⊤AB¯B¯⊤(cid:17) = 1Tr(cid:104) A⊤ABB⊤(cid:105) I+O(cid:16) C9 log √10(d)(cid:17) +O(cid:0) C2R(cid:1) .
p n X d X
(cid:16) (cid:17)
Proof. The O(R),O ∥R∥ terms are always extracted by using that the LHS is a continuous function in R (w.r.t.
op
∥·∥ ). Wecarrythisoutexplicitlyforthefirstitemandskipthedetailsfortheotheritems.
op
1. ByLemmaA.8,withprobabilityatleast1−C/dγ,
(cid:13) (cid:13) (cid:13) (cid:13) log(d) log(d) (cid:16) (cid:17)
∥B∥ =(cid:13)USV⊤+R(cid:13) ≤(cid:13)USV⊤(cid:13) +∥R∥ ≤C √ +∥R∥ =C √ +O ∥R∥ ,
max (cid:13) (cid:13) max (cid:13) (cid:13) max max X d op X d op
wherewehaveused(36)withλ=C Xlo √g(d) inthethirdstep.
d
17CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
(cid:13) (cid:13)
2. Thisfollowsfrom(35)withλ= lo √g(d) bynotingthatforanymatrixBwehave(cid:13) (cid:13)(cid:16) BB⊤+αI(cid:17)−2 BB⊤(cid:13) (cid:13) ≤ 1.
d (cid:13) (cid:13) α
op
(cid:13) (cid:13)
3. As in the previous item, we have (cid:13) (cid:13)(cid:16) XX⊤+αI(cid:17)−2 XX⊤(cid:13) (cid:13) ≤ 1 so the result follows again from (35) with
(cid:13) (cid:13) α
op
λ= lo √g(d).
d
4. First note that by 1. in Lemma A.4 we have
(cid:13) (cid:13)B¯(cid:13)
(cid:13)
op
≤ ∥B∥
op
≤ CC X, where we have used the assumption
(cid:16) (cid:17)
O ∥R∥ =o(1). Thus,wehave
op
(cid:16) (cid:17) (cid:16) (cid:17)⊤
B¯B¯⊤ =US V⊤ V⊤ SU⊤+O(C R),
X
m m
sotheresultfollowsfromLemmaA.9.
(cid:13) (cid:13)
5. Notethat(cid:13) (cid:13) (cid:13)X⊤(cid:16) XX⊤+αI(cid:17)−2(cid:13) (cid:13)
(cid:13)
≤ 2√1
α
andbyLemmaA.4wehave(cid:13) (cid:13)B¯(cid:13) (cid:13)
op
≤CC X. Thus,wehave
op
(cid:16) (cid:17) (cid:16) (cid:17)⊤
B¯A=US V⊤ V⊤ S˜U⊤+O(C R),
X
m m
whereS˜ isadiagonalmatrix,sotheresultfollowsfromLemmaA.9.
(cid:13) (cid:13)
6. Byusingagainthat(cid:13) (cid:13) (cid:13)X⊤(cid:16) XX⊤+αI(cid:17)−2(cid:13) (cid:13)
(cid:13)
≤ 2√1
α
and(cid:13) (cid:13)B¯(cid:13) (cid:13)
op
≤CC X,wehave
op
A⊤AB¯B¯⊤ =US˜2 S(cid:16) V⊤(cid:17) (cid:16) V⊤(cid:17)⊤ SU⊤+O(cid:0) C2R(cid:1)
,
X
m m
whereS˜ isadiagonalmatrix,sotheresultfollowsfromLemmaA.9.
Lemma A.12 (Master concentration for Bˆ). Consider a fixed B = X +R with unit norm rows and ∥X∥ ≤ C ,
op X
∥X∥
max
≤ C Xlo √g( dd),∥R∥
op
≤ C Rlogα √R d(d). Let2C X√1
p
> C
b
> (1+c)C X√1
p
> 0,forsomesmallconstantc > 0.
LetF : B√ (0)∪(Ω∩B (0)) ⊂ M → M ,forarbitraryn˜,d˜. AssumethatΩiss.t. withprobabilityatleast
d Cb n×d n˜×d˜
1−Cd−kF/2−1/2 inmwehaveforDˆb = min{ 1 , Cb }thatDˆb B¯,√1 B¯ ∈ Ω. FurtherassumethatF satisfiesthe
i,i ∥b¯ i∥ CX p
followingproperties:
1. ∥F(M)∥
op
≤C Fdk 2F foreveryM =Bˆ;
2. ∥F(M)∥ ≤C foreveryM ∈Ω∩B (0);
op F Cb
3. F isLipschitzwithconstantC onΩ∩B (0)(w.r.t. ∥·∥ onbothspaces).
F′ Cb op
Then,forlargeenoughd>d (C ,C ,C ,C ),
0 F F′ b R
(cid:13) (cid:13) (cid:13) (cid:13)E mF(Bˆ)−E m1 {DˆbB¯,√1 pB¯∈Ω}F(√1 pB¯)(cid:13) (cid:13) (cid:13)
(cid:13)
op
≤CC F′C X3 log √3 d(d) , (41)
wherecruciallytheRHSisindependentofC .
R
18CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
Proof. Define1 Ω¯ :=1 {DˆbB¯,√1 B¯∈Ω}and1 Ω¯c :=1−1 Ω¯. Wewillactuallyshowtheslightlystrongerstatement
p
E m(cid:13) (cid:13) (cid:13) (cid:13)F(Bˆ)−1 Ω¯F (cid:18) √1 pB¯(cid:19)(cid:13) (cid:13) (cid:13)
(cid:13)
≤CC F′C X3 log √3 d(d) , (42)
op
whichimmediatelyimplies(41). First,byasimpletriangleestimatewehave
E m(cid:13) (cid:13) (cid:13) (cid:13)F(Bˆ)−1 Ω¯F (cid:18) √1 pB¯(cid:19)(cid:13) (cid:13) (cid:13) (cid:13) ≤E m(cid:13) (cid:13) (cid:13) (cid:13)1 Ω¯ (cid:18) F(Bˆ)−F (cid:18) √1 pB¯(cid:19)(cid:19)(cid:13) (cid:13) (cid:13) (cid:13) +E m(cid:13) (cid:13) (cid:13)1 Ω¯cF(Bˆ)(cid:13) (cid:13) (cid:13) op.
op op
ByourassumptionsonΩandassumption1. wehave
(cid:13) (cid:13) 1
E m(cid:13) (cid:13)1 Ω¯cF(Bˆ)(cid:13)
(cid:13)
≤CC FdkF/2d−kF/2−1/2 =2C F√ ,
op d
sow.l.o.g. wemayassumethat1 Ω¯ ≡1.
WenowshowthatwecantruncateDˆ toDˆb byapplyingthetruncationfunctionmin{x, Cb }toeachentry. Notethat
bydefinitionofC b wehave 1 √+ pc ≤ CC Xb ≤ √2 p. By2. inLemmaA.4,wehavethat(cid:13) (cid:13) (cid:13)Bˆ(cid:13) (cid:13) (cid:13) opC ,X(cid:13) (cid:13) (cid:13)Dˆb B¯(cid:13) (cid:13) (cid:13)
op
≤ √ n. Thus,by
assumption,weobtain
E (cid:13) (cid:13)F(Bˆ)−F(Dˆb B¯)(cid:13) (cid:13) ≤C dkF/2P(cid:18)(cid:13) (cid:13)Dˆ(cid:13) (cid:13) > C b (cid:19) . (43)
m(cid:13) (cid:13) op F (cid:13) (cid:13) op C X
Wealsohavethetrivialbound
(cid:18)
log(d)
logαR(d)(cid:19)4
1
∥b ∥4 ≤d C √ +C √ ≤ √ .
i 4 X d R d d
Byasimpleunionbound,itfollowsfromLemmaA.7that,forlargeenoughd,
(cid:18)(cid:13) (cid:13) C (cid:19) (cid:32) (cid:18) C √ (cid:19)2√ (cid:33)
P (cid:13)Dˆ(cid:13) > b ≤Cd·exp −c X − p d
(cid:13) (cid:13) op C X C b
1
≤C· ,
d1+kF/2
wherewehaveusedthat 1 ≥ Cb implies(cid:13) (cid:13)b¯(cid:13) (cid:13)≤ CX ≤(1−c)√ p(here,ccanindeedbetreatedasauniversalconstant
∥b¯∥ CX Cb
byassumption). Togetherwith(43),wehave
E (cid:13) (cid:13)F(Bˆ)−F(Dˆb B¯)(cid:13) (cid:13) ≤C 1 .
m(cid:13) (cid:13)
op
Fd
Wenowneedtobound
(cid:13) (cid:18) (cid:19)(cid:13)
E m(cid:13) (cid:13) (cid:13)F(Dˆb B¯)−F √1 pB¯ (cid:13) (cid:13)
(cid:13)
.
op
Sinceby3. intheassumptions
(cid:13) (cid:18) (cid:19)(cid:13) (cid:13) (cid:13)
E m(cid:13) (cid:13) (cid:13)F(Dˆb B¯)−F √1 pB¯ (cid:13) (cid:13)
(cid:13)
≤C F′E m(cid:13) (cid:13) (cid:13)Dˆb B¯ − √1 pB¯(cid:13) (cid:13)
(cid:13)
,
op op
wewillonlyneedtoshowconcentrationforDˆb
.
RecallthatDˆb i,i = min{(cid:13) (cid:13)b¯ i(cid:13) (cid:13)− 21 ,C b/C X}. Wenowshowthat(cid:12) (cid:12) (cid:12)Dˆb i,i− √1 p(cid:12) (cid:12) (cid:12) > λimplies(cid:12) (cid:12)(cid:13) (cid:13)b¯ i(cid:13) (cid:13) 2−√ p(cid:12) (cid:12) > cλ. Todoso,
wedistinguishtwocases. If(cid:13) (cid:13)b¯ i(cid:13) (cid:13)≥ C CX
b
,wehave
(cid:12) (cid:12) (cid:12) (cid:12)Dˆb i,i− √1 p(cid:12) (cid:12) (cid:12) (cid:12)=(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)(cid:13) (cid:13) (cid:13) (cid:13)b¯ i b¯(cid:13) (cid:13) i(cid:13) (cid:13)− √√ pp(cid:12) (cid:12) (cid:12) (cid:12) (cid:12).
19CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
Thus,(cid:12) (cid:12) (cid:12)Dˆb i,i− √1 p(cid:12) (cid:12) (cid:12)>λimplies(cid:12) (cid:12)(cid:13) (cid:13)b¯ i(cid:13) (cid:13) 2−√ p(cid:12) (cid:12)>(cid:13) (cid:13)b¯ i(cid:13) (cid:13)√ pλ≥√ pC CX
b
λ=cλ.
Next,if(cid:13) (cid:13)b¯ i(cid:13) (cid:13)≤ C CX
b
<√ p,then(cid:12) (cid:12) (cid:12)Dˆb i,i− √1 p(cid:12) (cid:12) (cid:12)= CC Xb − √1
p
sonecessarilyλ≤ CC Xb − √1 p. Butthen
(cid:12) (cid:12)
(cid:12) (cid:12)(cid:13) (cid:13)b¯(cid:13) (cid:13) −√ p(cid:12) (cid:12)≥(cid:12) (cid:12)C X −√ p(cid:12) (cid:12)≥cλ,
2 (cid:12)C (cid:12)
b
wherethelaststepisjustthepreviouscasefor(cid:13) (cid:13)b¯ i(cid:13) (cid:13)= C CX
b
.
This completes the proof that (cid:12) (cid:12) (cid:12)Dˆb i,i− √1 p(cid:12) (cid:12) (cid:12) > λ implies (cid:12) (cid:12)(cid:13) (cid:13)b¯(cid:13) (cid:13) 2−√ p(cid:12) (cid:12) > cλ. Now, by Lemma A.6 we have ∥b i∥2 4 ≤
CC2 lo √g(d). So,wecanuseLemmaA.7andaunionboundtoobtain
X d
P(cid:32)(cid:13) (cid:13) (cid:13)Dˆb
−
√1 I(cid:13) (cid:13)
(cid:13)
>λ(cid:33) ≤Cd·exp(cid:18)
−c
dλ2 (cid:19)
.
(cid:13) p (cid:13) C4 log(d)2
op X
Forλ = C2 log √3(d) andlargeenoughd,wecanboundtheRHSbyd−2. By1. inLemmaA.4,wehavethat,forlarged,
X d
(cid:13) (cid:13)B¯(cid:13) (cid:13) op ≤∥B∥ op ≤C X +C Rlog √3 d(d) ≤2C X. NotealsothatDˆb B¯ − √1 pB¯ =(cid:16) Dˆb − √1 pI(cid:17) B¯. Hence,
P(cid:32)(cid:13)
(cid:13) (cid:13)Dˆb B¯ − √1
B¯(cid:13)
(cid:13) (cid:13) >2C3 log
√3(d)(cid:33) ≤P(cid:32)(cid:13)
(cid:13) (cid:13)Dˆb − √1
I(cid:13)
(cid:13) (cid:13) >C2 log
√3(d)(cid:33)
≤ 1 .
(cid:13) p (cid:13) X d (cid:13) p (cid:13) X d d2
op op
Thus,byfixingλ=2C3 log √3(d) andusingthatDˆb isbounded,weconclude
X d
(cid:13) (cid:13) (cid:32)(cid:13) (cid:13) (cid:33) (cid:13) (cid:13) (cid:32)(cid:13) (cid:13) (cid:33)
E m(cid:13) (cid:13) (cid:13)Dˆb B¯ − √1 pB¯(cid:13) (cid:13)
(cid:13)
≤λP (cid:13) (cid:13) (cid:13)Dˆb B¯ − √1 pB¯(cid:13) (cid:13)
(cid:13)
≤λ +m max(cid:13) (cid:13) (cid:13)Dˆb B¯ − √1 pB¯(cid:13) (cid:13)
(cid:13)
P (cid:13) (cid:13) (cid:13)Dˆb B¯ − √1 pB¯(cid:13) (cid:13)
(cid:13)
>λ
op op op op
(cid:32)(cid:13) (cid:13) (cid:33)
≤λ+CC XP (cid:13) (cid:13) (cid:13)Dˆb B¯ − √1 pB¯(cid:13) (cid:13)
(cid:13)
>λ
op
log3(d) 1
≤CC3 √ +CC
X
d
Xd2
log3(d)
≤CC3 √ .
X
d
LemmaA.13(Explicitapproximations). AssumethatP (cid:0) B¯ ∈Ω(cid:1) ≥1−C 1 ,where
m d2
log3(d) logαR(d)
Ω={M|(11⊤−I)◦(MM⊤)=Y +Z,∥Y∥ ≤CC2 √ ,∥Z∥ ≤CC C √ }
max X d op X R d
(cid:13) (cid:13) logαR(d))
⊂{M|(cid:13)(11⊤−I)◦MM⊤(cid:13) ≤C(C2 +C C ) √ }.
(cid:13) (cid:13) X X R
max d
Then, with probability at least 1−C 1 in U,V, the following functions satisfy the assumption of Lemma A.12 (with
d2
C ,C independentofd)
F F′
1. F(B)=−A⊤+ √1 A⊤AB+ 1Diag(BA)B− √1 Diag(A⊤ABB⊤)B,
p p p
2. F(B)=(cid:80) c2(11⊤−I)◦(BB⊤)◦ℓ,(cid:80) c2 <∞,
ℓ≥3 ℓ ℓ≥3 ℓ
(cid:16) (cid:16) (cid:17)(cid:17)
3. F(B)=(cid:80) ℓc2 (A⊤A)◦(11⊤−I)◦(BB⊤)◦(ℓ−1)−Diag A⊤A(11⊤−I)◦(BB⊤)◦ℓ B,
ℓ≥3 ℓ
(cid:80) c2 <∞,
ℓ≥3 ℓ
20CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
wherefor3. weneedtoadditionallyassumethattheconclusionofLemmaA.14holds. (Notethatthisisnotanissueasthe
proofofLemmaA.14usesonly2. inthecurrentlemma).
Proof. Firstnotethat,forfixedC b,C X,p,byscalingtheconstantinthedefinitionofΩby p1 CC 2b2 ,wemayw.l.o.g.assume
(cid:16) (cid:17) X
thatP DbB¯,√1 B¯ ∈Ω ≥1−C 1 .
p d2
Wenowshowtheclaimforeachofthefunctionsseparately.
1. Thefirstfunctionisthesumofofmulti-linearfunctionsandthusispolynomiallyboundedandLipschitzonbounded
sets. SincewehaveadimensionindependentboundonA,theboundsaredimension-independentaswell.
2. Wewillshowcondition1.,2. and3. inLemmaA.12separately.
For1.,notethatsince(cid:12) (cid:12)(BˆBˆ⊤
)
(cid:12)
(cid:12) ≤ 1wehave
(cid:12) i,j(cid:12)
(cid:12) (cid:12) (cid:13) (cid:13)
(cid:12)F(Bˆ) (cid:12)≤(cid:80) c2 =C <∞. Thusfrom5. inLemmaA.4,weobtain(cid:13)F(Bˆ)(cid:13) ≤Cdasdesired.
(cid:12) i,j(cid:12) ℓ ℓ (cid:13) (cid:13)
op
Nextweshowcondition2. WehavethefollowingestimateforM ∈Ωandℓ≥3
(cid:13) (cid:13) (cid:13) (cid:13)
ℓ(cid:13) (cid:13)(11⊤−I)◦(cid:16) MM⊤(cid:17)◦ℓ(cid:13)
(cid:13)
≤nℓ(cid:13) (cid:13)(11⊤−I)◦(cid:16) MM⊤(cid:17)◦ℓ(cid:13)
(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
op max
(cid:18) logαR(d)(cid:19)ℓ
≤nℓ C(C2 +C C ) √
M M R
d (44)
(logαR(d))3
≤C(C2 +C C )3 √
M M R
d
1
≤C ,
d1
4
wherethefirststepfollowsfrom5.inLemmaA.4. SincetheaboveboundisindependentofℓitalsoholdsforF since
(cid:80) c2 <∞. Thisgivesusthedesiredboundfor2.
ℓ≥3 ℓ
Toshow3. wewriteF(M)=(cid:80) c2Fℓ(F (M))whereF (M):=(11⊤−I)◦(MM⊤)andFℓ(Q)=Q◦ℓ. We
willshowthatF 1,F 2ℓ
areLipschitℓ z.ℓ By2 4.1 inLemmaA.4,1 wehavethat(cid:13) (cid:13)(11⊤−I)◦Q(cid:13)
(cid:13)
op
≤
2∥2
Q∥ op. Thus,for
B ,B ∈B (0),
1 2 Cb
(cid:13) (cid:13)
∥F (B )−F (B )∥ =(cid:13)(11⊤−I)◦(B B⊤−B B⊤)(cid:13)
1 1 1 2 op (cid:13) 1 1 2 2 (cid:13)
op
(cid:13) (cid:13)
≤2(cid:13)B B⊤−B B⊤(cid:13)
(cid:13) 1 1 2 2(cid:13)
op
(cid:13) (cid:13)
=2(cid:13)B (B⊤−B⊤)+(B −B )B⊤(cid:13)
(cid:13) 1 1 2 1 2 2(cid:13)
op
≤4C ∥B −B ∥ .
b 1 2 op
Hence,F isLipschitzwithconstant4C .
1 b
For F 2ℓ, we will show that (cid:13) (cid:13)DF 2ℓ(Q)(cid:13) (cid:13) ≤ Cd− 41 if Q = (11⊤ −I)◦(MM⊤),M ∈ Ω∩B Cb(0). Note that
∥Q∥
max
≤C(C M2 +C MC R)logα √R d(d) and∥Q∥
op
≤2C b2. Furthermore,F 2ℓisasymmetricℓ-linearfunction,sothe
derivativeinthedirectionZ isgivenbyDFℓ(Q)Z =ℓZ◦Q◦ℓ−1. From3. inLemmaA.4wehave
2
(cid:13) (cid:13) √ (cid:18) logαR(d)(cid:19)ℓ−1
(cid:13)ℓZ◦Q◦ℓ−1(cid:13) ≤ℓ n∥Z∥ C(C2 +C C ) √
(cid:13) (cid:13) op op M M R d
(logαR(d))2
≤C(C2 +C C )2∥Z∥ √ (45)
M M R op d
1
≤C∥Z∥ ,
op d1
4
21CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
so(cid:13) (cid:13)DF 2ℓ(F 1(B))(cid:13) (cid:13)≤Cd−1 4. Now,notethatsince
{Q|(cid:13) (cid:13)(11⊤−I)◦Q(cid:13) (cid:13)
max
≤C(C M2 +C MC R)log √αR d(d) ,Diag(Q)=0}
isconvex,thelinesegmentbetweenanytwopointsliesintheset,soaboundonthederivativeimpliesthattheQ◦ℓis
Lipschitzwiththesameconstant. MultiplyingthetwoLipschitzconstantsofF ,Fℓweobtainthattheircompositionis
1 2
Lipschitzwithconstant4CC bd− 41.
Sincenoneoftheboundsdependsonℓ,thisimmediatelyimpliesthatF(M) = (cid:80) c2Fℓ(F (M))isLipschitzas
ℓ ℓ 2 1
well,uptoanadditionalconstant(cid:80) c2.
ℓ≥3 ℓ
3. Againwewillfirstshowthatcondition1. holdsforB =Bˆ. Firstnotethatwecanwrite(asinLemmaA.15below)
∞
(F(Bˆ)) =(cid:88) ℓc2(cid:88) ⟨a ,a ⟩⟨bˆ ,bˆ ⟩ℓ−1Jˆ′ bˆ ,
k,: ℓ k j k j k j
ℓ=3 j̸=k
whereJˆ′ =I−bˆ bˆ⊤ . Observethat
k k k
(cid:13) (cid:13)Jˆ′ bˆ (cid:13) (cid:13)2 =(cid:13) (cid:13)bˆ −bˆ ⟨bˆ ,bˆ ⟩(cid:13) (cid:13)2 =1−⟨bˆ ,bˆ ⟩2 ≤2(cid:16) 1−(cid:12) (cid:12)⟨bˆ ,bˆ ⟩(cid:12) (cid:12)(cid:17) . (46)
(cid:13) k j(cid:13) (cid:13) j k k j (cid:13) k j (cid:12) k j (cid:12)
Thus,usingLemmaA.5,wehave
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)(cid:88)∞ ℓc2 ℓ(cid:88) ⟨a k,a j⟩⟨bˆ k,bˆ j⟩ℓ−1Jˆ′ kbˆ j(cid:13) (cid:13) (cid:13) (cid:13)≤C(cid:13) (cid:13) (cid:13)(11⊤−I)◦A⊤A(cid:13) (cid:13) (cid:13) max(cid:88)∞ ℓc2 ℓ(cid:88)(cid:12) (cid:12) (cid:12)⟨bˆ k,bˆ j⟩ℓ−1(cid:12) (cid:12) (cid:12)(cid:13) (cid:13) (cid:13)Jˆ′ kbˆ j(cid:13) (cid:13) (cid:13)
(cid:13)ℓ=3 j̸=k (cid:13) ℓ=3 j̸=k
≤C(cid:13) (cid:13)(11⊤−I)◦A⊤A(cid:13) (cid:13) (cid:13) (cid:13)BˆBˆ⊤(cid:13) (cid:13)2 (cid:88) 1 (cid:13) (cid:13)Jˆ′ bˆ (cid:13) (cid:13)
(cid:13) (cid:13) max(cid:13) (cid:13) max j̸=k (cid:114) 1−(cid:12) (cid:12)⟨bˆ ,bˆ ⟩(cid:12) (cid:12)(cid:13) k j(cid:13)
(cid:12) k j (cid:12)
≤Cn(cid:13) (cid:13)(11⊤−I)◦A⊤A(cid:13)
(cid:13)
(cid:13) (cid:13)BˆBˆ⊤(cid:13) (cid:13)2
(cid:13) (cid:13) (cid:13) (cid:13)
max max
≤Cd,
wherewehaveused(46)inthethirdstep. Using2. inLemmaA.4,theaboveimplies
(cid:13) (cid:13)
(cid:13) (cid:13)F(Bˆ)(cid:13)
(cid:13)
≤CC Rd23.
op
Thisshowsthatcondition1. inLemmaA.12holdsforBˆ.
Toshowtherestoftheconditions,wemaynowassumethatB ∈Ω∩B (0). Notethat,ifweshowthatF isLipschitz
Cb
onthisset,condition2. holdssince
∥F(B)∥ =∥F(B)−F(0)∥ ≤C′ ∥B∥ , (47)
op op F op
wherewehaveusedF(0)=0. Thusweonlyneedtoshowthethirdcondition.
Similarlytothepreviouscasein3.,wedefineF(M)=F (F (M))M whereF (M):=(11⊤−I)◦(MM⊤)and
2 1 1
(cid:88) (cid:16) (cid:16) (cid:17)(cid:17)
F (Q)= ℓc2 (A⊤A)◦Q◦(ℓ−1)−Diag A⊤AQ◦ℓ .
2 ℓ
ℓ≥3
NotethatitisenoughtoshowthatF (F (M))isLipschitz,asthenby(47)andM ∈ B (0),F istheproductof
2 1 Cb
twoboundedLipschitzfunctions,andthusLipschitz. Asforthepreviousfunction,wehavethatF isLipschitzwith
1
constant4C . Wewillnowderiveauniformboundforallℓ≥3. Define
b
(cid:16) (cid:16) (cid:17)(cid:17)
Fℓ(Q):=ℓc2 (A⊤A)◦Q◦(ℓ−1)−Diag A⊤AQ◦ℓ .
2 ℓ
22CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
Asinthepreviouscase,sinceQ◦ℓisasymmetricℓ-linearfunction,wehave
(cid:16) (cid:16) (cid:17)(cid:17)
DFℓ(Q)Z =ℓc2 (ℓ−1)(A⊤A)◦Q◦(ℓ−2)◦Z−ℓDiag A⊤AQ◦(ℓ−1)◦Z .
2 ℓ
RecallweassumetheconclusionofLemmaA.14tohold,so
(cid:16) (cid:17)−1 (cid:18) log10(d)(cid:19) (cid:18) log(d)(cid:19) (cid:18) logαR(d)(cid:19)
A=X⊤ XX⊤+αI +O(R)+O C7 √ =O √ +O C7C √ , (48)
X max X R
d d d
wherewehaveusedLemmaA.8inthesecondstep. Similarly,wehave
(cid:18) log(d)(cid:19) (cid:18) logαR(d)(cid:19)
A⊤A=O √ +O C7C √ . (49)
max X R
d d
Thus,byusingthat∥R◦S∥ ≤∥R∥ ∥S∥ foranysquarematricesR,S (seeTheorem1in(Visick,2000)),we
op op op
obtain
(cid:13) (cid:13) (cid:13)(A⊤A)◦Q◦(ℓ−2)◦Z(cid:13) (cid:13) (cid:13) op ≤(cid:13) (cid:13) (cid:13) (cid:13)O max(cid:18) lo √g( dd)(cid:19) ◦Q◦(ℓ−2)◦Z(cid:13) (cid:13) (cid:13) (cid:13) op+O(cid:18) C X7C Rlog √αR d(d)(cid:19)(cid:13) (cid:13) (cid:13)Q◦(ℓ−2)◦Z(cid:13) (cid:13) (cid:13) op,
Usingthesameestimateasin(45),3. inLemmaA.4andrecallingthatℓ≥3,
(cid:13) (cid:13)DF 2ℓ(Q)Z(cid:13) (cid:13)
op
≤Cℓ2c2 ℓ(cid:18)√ n∥Z∥
op
lo √g( dd) ∥Q∥ mℓ− a2 x+C X7C R√ n∥Z∥
op
log √αR d(d) ∥Q∥ℓ m− a2 x+√ n∥Z∥ op∥Q∥ℓ m− a1 x(cid:19)
√ (cid:32)(cid:18) logαR(d)(cid:19)(cid:18) logαR(d)(cid:19)ℓ−2 (cid:18) logαR(d)(cid:19)ℓ−1(cid:33)
≤Cℓ2 d C7C √ C(C2 +C C ) √ + C(C2 +C C ) √ ∥Z∥
X R d X X R d X X R d op
≤Cd−ℓ− 42
∥Z∥ .
op
(50)
Since{Q|∥Q∥
max
≤C(C M2 +C MC R)log √3 d(d),Diag(Q)=0}isconvex,thelinesegmentbetweenanytwopoints
liesintheset,soaboundonthederivativeimpliesthattheFℓisLipschitzwiththesameconstant. AsF =(cid:80) Fℓ,
2 2 ℓ≥3 2
wehavethatF 2isLipschitzwithconstant(cid:80) ℓ≥3Cd−ℓ− 42 ≤Cd−1 4. FinallythecompositionF(B)=F 2(F 1(B))is
LipschitzwithconstantCC bd− 41,socondition3. holds,whichconcludestheproof.
A.5.Concentrationofthegradient
Lemma A.14 (Error analysis of A). Assume that B = X + R with unit norm rows, X = USV⊤,U,V Haar,
∥X∥
op
≤C X,∥X∥
max
≤C Xlo √g( dd),∥R∥
op
≤C Rlogα √R d(d). Then,ford>d 0(C R)withprobabilityatleast1−C d1
2
(in
U,V)wehave
A=
√1
E
Bˆ⊤(cid:16)
E
f(cid:16) BˆBˆ⊤(cid:17)(cid:17)−1 =B⊤(cid:16) BB⊤+αI(cid:17)−1 +O(cid:18)
C7
log √10(d)(cid:19)
p m m X d
(51)
(cid:16) (cid:17)−1 (cid:18) log10(d)(cid:19)
=X⊤ XX⊤+αI +O(R)+O C7 √ .
X
d
Proof. ByastraightforwardapplicationofLemmaA.12,wehave
1 1
(cid:18) log3(d)(cid:19)
√ E Bˆ =E B¯ +O C3 √
p m mp X d
(cid:18) log3(d)(cid:19)
=B+O C3 √ . (52)
X
d
23CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
Next, we will estimate E
f(cid:16) BˆBˆ⊤(cid:17)
. Recall that f(x) = (cid:80) c2xℓ. As f(1) < ∞, we can define α = (cid:80) c2. As
m ℓ ℓ ℓ≥3 ℓ
c =1,wehave
1
f(BˆBˆ⊤ ):=BˆBˆ⊤ +(cid:88) c2(cid:16) BˆBˆ⊤(cid:17)◦ℓ =BˆBˆ⊤ +αI+(cid:88) c2(cid:16) BˆBˆ⊤ −I(cid:17)◦ℓ
.
ℓ ℓ
ℓ≥3 ℓ≥3
Let Ω = {M|(11⊤ − I) ◦ (MM⊤) = Y + Z,∥Y∥
max
≤ CC X2 log √3 d(d),∥Z∥
op
≤ CC XC Rlogα √R d(d)} ⊂
(cid:13) (cid:13)
{M|(cid:13) (cid:13)(11⊤−I)◦MM⊤(cid:13)
(cid:13)
max
≤ C(C X2 +C XC R)logα √R d(d))}, thenbyusing4. inlemmaA.11withγ = 2wehave
P (cid:0) B¯ ∈Ω(cid:1) ≥1−C 1 (withprobabilityatleast1−C/d2inU,V). BynotingthatBˆBˆ⊤ satisfiestheassumptionsof
m d2
LemmaA.12andusing2. inLemmaA.13wehave
BˆBˆ⊤ +(cid:88) c2 ℓ(cid:16) BˆBˆ⊤(cid:17)◦ℓ = p1 B¯B¯⊤ +αI+1 B¯∈Ω(cid:88) c2 ℓ(11⊤−I)◦(cid:18) p1 B¯B¯⊤(cid:19)◦ℓ +O(cid:18) CC X3 log √3 d(d)(cid:19) . (53)
ℓ≥3 ℓ≥3
Bylinearity,wehaveE 1B¯B¯⊤ =BB⊤. Wewillnowshowthat
mp
E
m(cid:88)
1 B¯∈Ωc2
ℓ(11⊤−I)◦(cid:18) p1 B¯B¯⊤(cid:19)◦ℓ =O(cid:18)
C X6
log √10 d(d)(cid:19)
. (54)
ℓ≥3
Fornow,let(11⊤−I)◦B¯B¯⊤ =Y +Z,∥Y∥
max
≤CC X2 log √3 d(d),∥Z∥
op
≤CC XC Rlogα √R d(d),asinthedefinitionofΩ
above. BythedefinitionofΩ,wehavethat,forB¯ ∈Ωandℓ≥3,
(cid:18)
1
(11⊤−I)◦B¯B¯⊤(cid:19)◦ℓ
= 1 Y◦ℓ+ 1 ℓY◦(ℓ−1)◦(cid:0) Z+eℓO(cid:0) Z2(cid:1)(cid:1) . (55)
p pℓ pℓ
Thus,by3. inLemmaA.4,wehavethatforℓ≥3
1 (cid:13) (cid:13) 1 √ (cid:18) log3(d)(cid:19)ℓ−1 log10(d)
(cid:13)Y◦ℓ(cid:13) ≤C dC2 C2 √ ≤CC6 √
pℓ (cid:13) (cid:13) op pℓ X X d X d
and
1 eℓℓ(cid:13) (cid:13)Y◦(ℓ−1)◦(cid:0) Z+eℓO(cid:0) Z2(cid:1)(cid:1)(cid:13) (cid:13) ≤C 1 eℓℓ√ dC C log √αR(d)(cid:18) C2 log √3(d)(cid:19)ℓ−1 ≤d−3/4.
pℓ (cid:13) (cid:13) op pℓ X R d X d
Thus,wecanfurtherestimate(55)by
(cid:13) (cid:13)
(cid:13) (cid:18) 1 (cid:19)◦ℓ(cid:13) log10(d)
E m(cid:13)
(cid:13)
(cid:13)1 B¯∈Ω(11⊤−I)◦ pB¯B¯⊤ (cid:13)
(cid:13)
(cid:13)
≤CC X6 √
d
. (56)
op
Sincetheboundisindependentofℓ,thisshows(54).
Combining(53)and(54)wenowhave
E f(BˆBˆ⊤
)=BB⊤+αI+O(cid:18)
C6 log
√10(d)(cid:19)
. (57)
m X
d
From(57)italsoimmediatelyfollowsthat
(cid:16)
E
f(cid:16) BˆBˆ⊤(cid:17)(cid:17)−1 =(cid:16) BB⊤+αI(cid:17)−1 +O(cid:18)
C6
log √10(d)(cid:19)
, (58)
m X
d
sinceforanypsdmatrixX >αI,themapfrom(M ,∥·∥ )→(M ,∥·∥ ),R→(X+R)−1islocallycontinuously
n,n op n,n op
differentiableat0. Combining(52)and(58)yieldsthefirstequalityin(51). Toseethesecondequalityin(51),itsufficesto
(cid:16) (cid:17)
usethefactthatthefunctionB →B⊤ BB⊤+αI isLipschitzonboundedsetsw.r.t∥·∥ .
op
24CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
LemmaA.15(Gradientconcentration,Part1). AssumethatB =X+Rwithunitnormrows,X =USV⊤,U,V Haar,
∥X∥
op
≤ C X, ∥X∥
max
≤ CC Xlo √g( dd), ∥R∥
op
≤ C Rlogα √R d(d),∥A∥
op
≤ C. Furtherassumethatmin i,j|B i,j| ≥ δ >
d−γδ forsomeγ
δ
>0. Then,ford>d 0(C X,C R,γ δ)withprobabilityatleast1−C d1
2
inU,V,thegradientof (24)w.r.t.
Bcanbewrittenas
(cid:88)∞ (cid:18) log2(d)(cid:19)
∇ =E ∇1 + ℓc2E ∇ℓ +O C7 √ , (59)
B m Bˆ ℓ m Bˆ X
d
ℓ=3
where
1 (∇1 ) =−a + 1 ⟨a ,bˆ ⟩bˆ + √1 (cid:88) ⟨a ,a ⟩Jˆ′ bˆ ,
2 Bˆ k,: k p k k k p k j k j
j (60)
1 ∇1 =−A⊤+ √1 A⊤ABˆ + 1 Diag(BˆA)Bˆ − √1 Diag(A⊤ABˆBˆ⊤ )Bˆ,
2 Bˆ p p p
1 (∇ℓ ) = √1 (cid:88) ⟨a ,a ⟩⟨bˆ ,bˆ ⟩ℓ−1Jˆ′ bˆ ,
2 Bˆ k,: p k j k j k j
j (61)
1 ∇ℓ = √1 (A⊤A)◦(BˆBˆ⊤ −I)◦(ℓ−1)Bˆ − √1 Diag(A⊤A(BˆBˆ⊤ −I)◦ℓ)Bˆ,
2 Bˆ p p
and
Jˆ′ := √1 (cid:16) I−bˆ bˆ⊤(cid:17) .
k p k k
Proof. Recallfrom(29)thatthegradientisgivenby
∞
(∇ B) k,: =E m−2√1 pm◦Jˆ ka k+2(cid:88) ℓc2 ℓ(cid:88) ⟨a k,a j⟩⟨bˆ k,bˆ j⟩ℓ−1Jˆ kbˆ j, (62)
ℓ=1 j̸=k
whereJˆ = 1
(cid:16)
I−bˆ
bˆ⊤(cid:17)
.
k ∥b¯ k∥ k k
WewillapproximateJˆ byJˆ′ . Thiswillmakethegradienthavethesamefunctionalform(forfixedm)asintheGaussian
k k
case. ThisfollowsfromthefactthatthegradientinsidetheexpectationisthesameasthegradientoftheGaussianobjective
(86)in(Shevchenkoetal.,2023)evaluatedatB =B¯. WedenotethenewgradientwithJˆ replacedbyJˆ′ as∇′ . We
k k B
proceedbydecomposingtheerror∥(∇ ) −(∇′ ) ∥intomultiplepartsandanalysingthemindividually.
B k,: B k,:
First,weneedtodecomposetheerror. CombiningLemmaA.7andLemmaA.6,wehave
P(cid:18) (cid:12) (cid:12)(cid:13) (cid:13)b¯ k(cid:13) (cid:13)−√ p(cid:12) (cid:12)>C X2 log √2(d)(cid:19) = P(cid:18)(cid:12) (cid:12) (cid:12)(cid:13) (cid:13)b¯ k(cid:13) (cid:13)2 −p(cid:12) (cid:12) (cid:12)>C X2((cid:13) (cid:13)b¯ k(cid:13) (cid:13)+√ p)−1log √2(d)(cid:19)
m d m d
≤ mP(cid:18)(cid:12) (cid:12) (cid:12)(cid:13) (cid:13)b¯ k(cid:13) (cid:13)2 −p(cid:12) (cid:12) (cid:12)>C X2 1+1 √ plog √2 d(d)(cid:19) (63)
1
≤C .
dγ
DenotingbyAtheeventthat(cid:12) (cid:12)(cid:13) (cid:13)b¯ k(cid:13) (cid:13)−√ p(cid:12) (cid:12)>C X2 log √2 d(d) jointlyforallk,wehave
E (∇ ) −(∇′ ) =E 1 ((∇ ) −(∇′ ) )+E 1 (∇ ) −(∇′ )
m B k,: B k,: m A B k,: B k,: m Ac B k,: B k,:
∞
=E 1 ((∇ ) −(∇′ ) )+E −2√1 m◦ϵk Jˆ′ a +2(cid:88) ℓc2(cid:88) ⟨a ,a ⟩⟨bˆ ,bˆ ⟩ℓ−1ϵk Jˆ′ bˆ
m A B k,: B k,: m p m k k ℓ k j k j m k j
ℓ=1 j̸=k
=:(∇1 ) +(∇2 ) ,
err k,: err k,:
(64)
25CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
where(cid:12) (cid:12)ϵk (cid:12) (cid:12)≤C2 log √2(d) and∇1 ,∇2 arethematricescorrespondingtothefirstandsecondexpectation,respectively.
m X d err err
Usingthisnotation,provingthelemmaisequivalenttoshowingthat
(cid:18) log2(d)(cid:19)
∇1 +∇2 =O C7 √ . (65)
err err X
d
(cid:13) (cid:13)
Wewillstartwithbounding(cid:13)(∇1 err) k,:(cid:13) op. Bythedefinitionof(∇ B) k,:−(∇′ B) k,:),wehavethefollowingsimplebound:
(cid:13) (cid:13)
(cid:13) ∞ (cid:13)
E m∥1 A((∇ B) k,:−(∇′ B) k,:)∥≤C d1
γ
m max(cid:13) (cid:13) (cid:13)−2√1 pm◦(Jˆ k−Jˆ′ k)a k+2(cid:88) ℓc2 ℓ(cid:88) ⟨a k,a j⟩⟨bˆ k,bˆ j⟩ℓ−1(Jˆ k−Jˆ′ k)bˆ j(cid:13) (cid:13) (cid:13).
(cid:13) l=1 j̸=k (cid:13)
(66)
Notethat
(cid:13) (cid:13) (cid:13)Jˆ k−Jˆ′ k(cid:13) (cid:13) (cid:13)
op
=(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)(cid:32) (cid:13) (cid:13)b¯1 k(cid:13)
(cid:13)
− √1 p(cid:33) (cid:16) I−bˆ kbˆ⊤ k(cid:17)(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)
≤(cid:18)√ δp +1(cid:19)(cid:13) (cid:13) (cid:13)Jˆ′ k(cid:13) (cid:13) (cid:13) op. (67)
op
(cid:13) (cid:13) (cid:13) (cid:13)
Furthermore,sincebydefinition(cid:13)bˆ (cid:13)=(cid:13)bˆ (cid:13)=1,
(cid:13) j(cid:13) (cid:13) k(cid:13)
p(cid:13) (cid:13)Jˆ′ bˆ (cid:13) (cid:13)2 =(cid:13) (cid:13)bˆ −bˆ ⟨bˆ ,bˆ ⟩(cid:13) (cid:13)2 =1−⟨bˆ ,bˆ ⟩2 ≤2(cid:16) 1−(cid:12) (cid:12)⟨bˆ ,bˆ ⟩(cid:12) (cid:12)(cid:17) . (68)
(cid:13) k j(cid:13) (cid:13) j k k j (cid:13) k j (cid:12) k j (cid:12)
Weclearlyhave
√
(cid:13) (cid:13)m◦(Jˆ −Jˆ′ )a (cid:13) (cid:13)≤(cid:18) p +1(cid:19)(cid:13) (cid:13)Jˆ′(cid:13) (cid:13) ∥a ∥≤C(1+ 1 ), (69)
(cid:13) k k k(cid:13) δ (cid:13) k(cid:13) op k δ
wherewehaveused(67)andthefactthatmaskingreducesthenorm.
ByLemmaA.5,wehave
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)(cid:88)∞ ℓc2 ℓ(cid:88) ⟨a k,a j⟩⟨bˆ k,bˆ j⟩ℓ−1(Jˆ k−Jˆ′ k)bˆ j(cid:13) (cid:13) (cid:13) (cid:13)≤C(1+ 1 δ)(cid:13) (cid:13) (cid:13)(11⊤−I)◦A⊤A(cid:13) (cid:13) (cid:13) max(cid:88)∞ ℓc2 ℓ(cid:88)(cid:12) (cid:12) (cid:12)⟨bˆ k,bˆ j⟩ℓ−1(cid:12) (cid:12) (cid:12)(cid:13) (cid:13) (cid:13)Jˆ′ kbˆ j(cid:13) (cid:13) (cid:13)
(cid:13)ℓ=1 j̸=k (cid:13) ℓ=1 j̸=k
≤C(1+ 1 )∥A∥2 (cid:88) 1 (cid:13) (cid:13)Jˆ′ bˆ (cid:13) (cid:13)
δ op (cid:114) (cid:12) (cid:12)(cid:13) k j(cid:13)
j̸=k 1−(cid:12)⟨bˆ ,bˆ ⟩(cid:12)
(cid:12) k j (cid:12)
1
≤C(1+ )d,
δ
(70)
wherethelaststepfollowsfrom(68). Nowcombining(69)and(70)wecanboundtheRHSof(66)by
(cid:13) (cid:13)
(cid:13) ∞ (cid:13)
C d1
γ
m max(cid:13) (cid:13) (cid:13)−2√1 pm◦(Jˆ k−Jˆ′ k)a k+2(cid:88) ℓc2 ℓ(cid:88) ⟨a k,a j⟩⟨bˆ k,bˆ j⟩l−1(Jˆ k−Jˆ′ k)bˆ j(cid:13) (cid:13) (cid:13)≤CC X2(1+ 1 δ)d−(γ−1). (71)
(cid:13) l=1 j̸=k (cid:13)
Fromthisand2. inLemmaA.4,itfollowsthat
(cid:13) (cid:13)∇1 (cid:13) (cid:13) ≤C(1+ 1 )d−(γ−1)√ d, (72)
err op δ
Nowbychoosingγ =3+γ δ theRHSisofoforderthanO(cid:0) d−3/2(cid:1) ,whichfinishesbounding(cid:13) (cid:13)∇1 err(cid:13) (cid:13) op.
For(cid:13) (cid:13)∇2 err(cid:13) (cid:13) op weneedamorenuancedapproach. Wewillbreakthisterminthreedifferentparts,∇2 err = −√2 pM 1 +
2M +2M ,in(73),(75),(77)below. Firstweconsider
2 3
(M ) :=m◦ϵk Jˆ′ a =ϵk m◦(a −bˆ ⟨bˆ ,a ⟩), (73)
1 k,: m k k m k k k k
26CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
sodefining(D ) :=ϵk canwrite
ϵ k,k m
(cid:16) (cid:17)
M =D (A −Diag BˆA Bˆ ).
1 ϵ m m
By1. inLemmaA.4,wecanbound
(cid:13) (cid:13)2 log2(d)(cid:18) (cid:13) (cid:13)2 (cid:19)
∥M ∥ ≤∥D ∥ (∥A∥ +(cid:13)Bˆ(cid:13) ∥A∥ )≤CC2 √ 1+(cid:13)Bˆ(cid:13) .
1 op ϵ op op (cid:13) (cid:13) op op X d (cid:13) (cid:13) op
ByLemmaA.12,wehave
E m(cid:13) (cid:13) (cid:13)Bˆ(cid:13) (cid:13) (cid:13)2
op
≤ p1(cid:13) (cid:13)B¯(cid:13) (cid:13)2 op+CC X3 log √3 d(d) ≤CC X2 +CC X3 log √3 d(d) ,
whichgivesus
log2(d)
E ∥M ∥ ≤CC7 √ . (74)
m 1 op X d
Next,weconsidertheterm
(M ) :=ϵk (cid:88) ⟨a ,a ⟩Jˆ′ bˆ +3c2⟨a ,a ⟩⟨bˆ ,bˆ ⟩2Jˆ′ bˆ , (75)
2 k,: m k j k j 3 k j k j k j
j
whichwecanwriteas
M =D (cid:16) A⊤ABˆ −Diag(cid:16) A⊤ABˆBˆ⊤(cid:17) Bˆ +3c2(A⊤A)◦(BˆBˆ⊤ −I)◦2Bˆ −3c2Diag(A⊤A(BˆBˆ⊤ −I)◦3)Bˆ(cid:17) .
2 ϵ 3 3
OnecanverifythattheRHSsatisfiestheassumptionofLemmaA.12. Hence,thesamereasoningasforM givesthat
1
log2(d)
E ∥M ∥ ≤CC7 √ . (76)
m 2 op X d
Lastly,define
∞
(M ) =ϵk (cid:88) ℓc2(cid:88) ⟨a ,a ⟩⟨bˆ ,bˆ ⟩ℓ−1Jˆ′ bˆ . (77)
3 k,: m ℓ k j k j k j
ℓ=5 j
UsingLemmaA.5,wehave
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)ϵk m(cid:88)∞ ℓc2 ℓ(cid:88) ⟨a k,a j⟩⟨bˆ k,bˆ j⟩ℓ−1Jˆ′ kbˆ j(cid:13) (cid:13) (cid:13) (cid:13)≤CC X2 log √2 d(d)(cid:13) (cid:13) (cid:13)(11⊤−I)◦A⊤A(cid:13) (cid:13) (cid:13) max(cid:88)(cid:88)∞ ℓc2 ℓ(cid:12) (cid:12) (cid:12)⟨bˆ k,bˆ j⟩ℓ−1(cid:12) (cid:12) (cid:12)(cid:13) (cid:13) (cid:13)Jˆ′ kbˆ j(cid:13) (cid:13) (cid:13)
(cid:13) ℓ=5 j̸=k (cid:13) j̸=kℓ=5
≤CC2 log √2(d)(cid:13) (cid:13)(11⊤−I)◦A⊤A(cid:13) (cid:13) (cid:88) ⟨bˆ k,bˆ j⟩4 (cid:13) (cid:13)Jˆ′ bˆ (cid:13) (cid:13)
X d (cid:13) (cid:13) max j̸=k (cid:114) 1−(cid:12) (cid:12)⟨bˆ ,bˆ ⟩(cid:12) (cid:12)(cid:13) k j(cid:13)
(cid:12) k j (cid:12)
≤CC2
log √2(d) d(cid:13) (cid:13)(11⊤−I)◦A⊤A(cid:13)
(cid:13)
(cid:13) (cid:13)(11⊤−I)◦BˆBˆ⊤(cid:13) (cid:13)4
.
X (cid:13) (cid:13) (cid:13) (cid:13)
d max max
(78)
Notethat
(cid:13) (cid:13)(11⊤−I)◦BˆBˆ⊤(cid:13) (cid:13)4 =(cid:13)
(cid:13)(11⊤−I)◦DˆB¯B¯⊤
Dˆ(cid:13) (cid:13)4 ≤min{(cid:13) (cid:13)Dˆ(cid:13) (cid:13)8 (cid:13) (cid:13)(11⊤−I)◦B¯B¯⊤(cid:13) (cid:13)4
,1}.
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
max max op max
Thus,byusing4. inLemmaA.11,withprobabilityatleast1−C 1 inU,V,wehave
d2
E
(cid:13) (cid:13)(11⊤−I)◦BˆBˆ⊤(cid:13) (cid:13)4 ≤P(cid:18)(cid:13) (cid:13)Dˆ(cid:13)
(cid:13) >
√2 (cid:19) +(cid:18) √2 (cid:19)8(cid:18)
CC2
log √3(d) +O(cid:16)
C ∥R∥
(cid:17)(cid:19)4
m(cid:13) (cid:13) max (cid:13) (cid:13) op p p X d X op
≤Cd−γ +C(cid:18) C2 log √3(d) +C C log √3(d)(cid:19)4 (79)
X X R
d d
1
≤C .
d3
2
27CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
Next,notethatundertheassumptionsofthecurrentlemmawecanapplybothLemmaA.14and3. inLemmaA.11toobtain
(cid:13) (cid:13) log10(d) logαR(d)
(cid:13)(11⊤−I)◦A⊤A(cid:13) ≤C7 √ +C √ . (80)
(cid:13) (cid:13) X R
max d d
Combining(78),(79),(80),weobtain
log(d)4 1 1 1
E ∥(M ) ∥2 ≤CC4 d2(C7 log10(d)+C logαR(d))2 ≤C .
m 3 k,: X d X R dd3 d5
2
Thisnowgivesus
(cid:115) (cid:115)
E ∥M ∥ ≤E (cid:88) ∥(M ) ∥2 ≤ (cid:88) E ∥(M ) ∥2 ≤C 1 , (81)
m 3 op m 3 k,: op m 3 k,: op d3
4
k k
wherewehaveusedJensen’sinequalityinthesecondstep. Finallycombining(72),(74),(76),(81)wecanconclude.
LemmaA.16(Gradientconcentration,Part2). AssumewehaveB =X+Rwithunitnormrows,X =USV⊤,U,V
(cid:104) (cid:105)
Haar, Tr SS⊤ = n,∥X∥
op
≤ C X, ∥X∥
max
≤ CC Xlo √g( dd), ∥R∥
op
≤ C Rlogα √R d(d). Further assume that
min i,j|B i,j| ≥ δ > d−γδ for some γ
δ
> 0. Then, for d > d 0(C X,C R,γ δ) with probability at least 1 − C d1
2
in
U,V
1 (cid:16) (cid:17)−2 1 (cid:20)(cid:16) (cid:17)−2 (cid:21) (cid:18) log10(d)(cid:19)
∇ =−α BB⊤+αI B+α Tr BB⊤+αI BB⊤ B+O C10 √ (82)
2 B n X d
(cid:16) (cid:17)−2 1 (cid:20)(cid:16) (cid:17)−2 (cid:21) (cid:18) log10(d)(cid:19)
=−α XX⊤+αI X+α Tr XX⊤+αI XX⊤ X+O(R)+O C10 √ , (83)
n X d
where∇ wasdefinedin(29).
B
(cid:16) (cid:17)
Proof. ByLemmaA.15,wemayassumethat,uptoanerroroforderO C7 log √2(d) ,thegradientisgivenby(59),(60)
X d
and(61).
Wewillstartbyanalysingthefirstpartofthegradientin(60)whichwerestatehereforconvenience:
1 ∇1 =−A⊤+ √1 A⊤ABˆ + 1 Diag(BˆA)Bˆ − √1 Diag(A⊤ABˆBˆ⊤ )Bˆ. (84)
2 Bˆ p p p
ByLemma(A.14),wehavewithprobabilityatleast1−C 1 inU,V
d2
(cid:16) (cid:17)−1 (cid:18) log10(d)(cid:19)
A=B⊤ BB⊤+αI +O C7 √
X
d
(cid:16) (cid:17)−1 (cid:18) log10(d)(cid:19)
=X⊤ XX⊤+αI +O C7 √ +O(R),
X
d
wheretheexpectationovermhasnotbeentakenyet. Using1. inLemmaA.13,weseethattheRHSin(84)satisfiesthe
assumptionsofLemmaA.12(notingthatΩistheentirespacefor1.),sowehave
1 1 1 1 1 1
(cid:18) log10(d)(cid:19)
E ∇1 =E −A⊤+ A⊤AB¯ + Diag( B¯A)B¯ − Diag(A⊤A B¯B¯⊤ )B¯ +O C10 √ .
m2 Bˆ m p p p p p X d
WenowestimateE 1∇1 . Weclearlyhave
m2 Bˆ
1
E −A⊤+ A⊤AB¯ =−A⊤+A⊤AB.
m p
28CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
Forthethirdtermwehaveby5. inLemmaA.11that,withprobabilityatleast1−C 1 inm,
d2
1
(cid:18) log10(d)(cid:19)
Diag( B¯A)=βI+O C8 √ +O(C R),
p X d X
whereβ = 1Tr[BA],whichimpliesthat
n
E 1 Diag(1 B¯A)B¯
=βB+O(cid:18)
C9 log
√10(d)(cid:19)
+O(cid:0) C2R(cid:1) .
mp p X d X
Byexactlythesameargument,wecanuse6. inLemmaA.11andobtain
E Diag(A⊤A1 B¯B¯⊤ )B¯
=β˜B+O(cid:18)
C10log
√10(d)(cid:19)
+O(cid:0) C3R(cid:1) ,
m p X d X
(cid:104) (cid:105)
whereβ˜= 1Tr A⊤ABB⊤ .
n
Intotalwehave
E 1 ∇1
=−A⊤+A⊤AB+βB−β˜B+O(cid:18)
C10log
√10(d)(cid:19)
+O(cid:0) C3R(cid:1)
m2 Bˆ X
d
X
=(cid:16) −A⊤+A⊤AX+βX−β˜X(cid:17) +O(cid:18) C10log √10(d)(cid:19) +O(cid:0) C3R(cid:1)
.
X X
d
Plugging in A =
B⊤(cid:16) BB⊤+αI(cid:17)−1
+
O(cid:16)
C7 log
√10(d)(cid:17)
in the second term and A =
X⊤(cid:16) XX⊤+αI(cid:17)−1
+
X d
(cid:16) (cid:17)
O C7 log √10(d) +O(R)inthethirdterm,weobtaintheleadingordertermsfor(82).
X d
(cid:104) (cid:105)
To see that this also implies (83) note that β = 1Tr[BA] = 1Tr[XA] + O(R) and β˜ = 1Tr A⊤ABB⊤ =
n n n
(cid:104) (cid:105)
1Tr A⊤AXX⊤ +O(C R).
n X
Itremainstoshowthatthehigherordertermsaresmall.Herewewillnotneedtodistinguishbetweenthetwoapproximations
ofA. Theremainingpartofthegradientin(61)isgivenby
(cid:88)
c2ℓ∇ℓ ,
ℓ Bˆ
ℓ≥3
where
1 ∇ℓ = √1 (A⊤A)◦(BˆBˆ⊤ −I)◦(ℓ−1)Bˆ − √1 Diag(A⊤A(BˆBˆ⊤ −I)◦ℓ)Bˆ. (85)
2 Bˆ p p
LetΩ = {M|(11⊤−I)◦(MM⊤) = Y +Z,∥Y∥
max
≤ CC X2 log √3 d(d),∥Z∥
op
≤ CC XC Rlogα √R d(d)}. Then,by4. in
LemmaA.11,P(cid:0) B¯ ∈Ω(cid:1) ≥1−C 1 . Thus,byLemmaA.13,wehave
d2
1(cid:88) 1 (cid:88)
2 c2 ℓℓ∇ℓ Bˆ = √ p c2 ℓℓ1 {B¯∈Ω}
ℓ≥3 ℓ≥3
·(cid:32) (A⊤A)◦(cid:18)
(cid:0) 11⊤−I(cid:1) ◦ 1
B¯B¯⊤(cid:19)◦(ℓ−1) −Diag(cid:32) A⊤A(cid:18)
(cid:0) 11⊤−I(cid:1) ◦ 1
B¯B¯⊤(cid:19)◦ℓ(cid:33)(cid:33)
√1 B¯
+O(cid:18)
C3 log
√3(d)(cid:19)
.
p p p X d
(86)
Wewillnowindividuallyboundthedifferentterms. Inthefollowingwealwaysassumeℓ≥3. Wefirstanalysetheterm
(A⊤A)◦(cid:18)
(cid:0) 11⊤−I(cid:1)
◦
1
B¯B¯⊤(cid:19)◦(ℓ−1)
.
p
29CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
Wehadpreviouslyderivedthefollowingin(55)
(cid:18)
1
(11⊤−I)◦B¯B¯⊤(cid:19)◦ℓ−1
= 1 Y◦(ℓ−1)+ 1 ℓY◦(ℓ−2)◦(cid:0) Z+eℓO(cid:0) Z2(cid:1)(cid:1) . (87)
p pℓ−1 pℓ−1
Thus,asin3. ofLemmaA.13weobtainfromLemmaA.14
(cid:18) log(d)(cid:19) (cid:18) logαR(d)(cid:19)
A⊤A=O √ +O C7C √ ,
max X R
d d
so
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:18) 1 (cid:19)◦(ℓ−1)(cid:13) (cid:13) (cid:18) log(d)(cid:19) (cid:18) 1 (cid:19)◦(ℓ−1)(cid:13)
(cid:13)(A⊤A)◦ (11⊤−I)◦B¯B¯⊤ (cid:13) ≤(cid:13)O √ ◦ (11⊤−I)◦B¯B¯⊤ (cid:13)
(cid:13) (cid:13) p (cid:13) (cid:13) (cid:13) (cid:13) max d p (cid:13) (cid:13)
op op
(88)
(cid:13) (cid:13)
+(cid:13) (cid:13)O(cid:18)
C7C
log √αR(d)(cid:19) ◦(cid:18) 1 (11⊤−I)◦B¯B¯⊤(cid:19)◦(ℓ−1)(cid:13)
(cid:13) .
(cid:13) (cid:13) X R d p (cid:13) (cid:13)
op
Pluggingin(87)andusing3. and5. inLemmaA.4weobtain
(cid:13) (cid:13)
(cid:13) (cid:18) log(d)(cid:19) (cid:18) 1 (cid:19)◦(ℓ−1)(cid:13) ℓ (cid:18) log(d)(cid:13) (cid:13) √ log(d)(cid:13) (cid:13) (cid:19)
ℓ(cid:13)O √ ◦ (11⊤−I)◦B¯B¯⊤ (cid:13) ≤C n √ (cid:13)Y◦(ℓ−1)(cid:13) +ℓeℓ n∥Z∥ √ (cid:13)Y◦(ℓ−2)(cid:13)
(cid:13) (cid:13) max d p (cid:13) (cid:13) pℓ−1 d (cid:13) (cid:13) max op d (cid:13) (cid:13) max
op
(cid:32) (cid:33)
ℓ log1+3(ℓ−1)(d) logαR(d)log1+3(ℓ−2)(d)
≤C C2(ℓ−1) +CC1+2(ℓ−2)C eℓℓ
pℓ−1 X d(ℓ−2)/2 X R d(ℓ−1)/2
log7(d)
≤CC4 √ .
X
d
(89)
Similarly,wehave
(cid:13) (cid:13)
ℓ(cid:13) (cid:13)O(cid:18)
C7C
log √αR(d)(cid:19) ◦(cid:18) 1 (11⊤−I)◦B¯B¯⊤(cid:19)◦(ℓ−1)(cid:13)
(cid:13)
(cid:13) (cid:13) X R d p (cid:13) (cid:13)
op
ℓ (cid:18)√ logαR(d)(cid:13) (cid:13) √ logαR(d)(cid:13) (cid:13) (cid:19)
≤C nC7C √ (cid:13)Y◦(ℓ−1)(cid:13) +ℓ nC7C √ (cid:13)Z◦Y◦(ℓ−2)(cid:13)
pℓ−1 X R d (cid:13) (cid:13) max X R d (cid:13) (cid:13) max
(90)
(cid:32) (cid:33)
ℓ logαR(d)log3(ℓ−1)(d) (logαR(d))2log3(ℓ−1)(d)
≤C C7+2(ℓ−1)C +CC7+2(ℓ−1)C ℓ
pℓ−1 X R d(ℓ−1)/2 X R d(ℓ−1)/2
1
≤ √ .
d
Nextwehave
(cid:13) (cid:13) (cid:13)Diag(cid:32) A⊤A(cid:18) (cid:0) 11⊤−I(cid:1) ◦ 1 B¯B¯⊤(cid:19)◦ℓ(cid:33)(cid:13) (cid:13) (cid:13) ≤C(cid:13) (cid:13) (cid:13)(cid:18) (cid:0) 11⊤−I(cid:1) ◦ 1 B¯B¯⊤(cid:19)◦ℓ(cid:13) (cid:13) (cid:13) .
(cid:13) p (cid:13) (cid:13) p (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
op op
Nowexactlyasintheproofof(56)weobtain
(cid:13) (cid:13)
Cℓ(cid:13) (cid:13)(cid:18) (cid:0) 11⊤−I(cid:1) ◦ 1 B¯B¯⊤(cid:19)◦ℓ(cid:13) (cid:13) ≤CC6 log √10(d) . (91)
(cid:13) (cid:13) p (cid:13) (cid:13) X d
op
(Notethatwhenwritingouttheproof,theℓfactoristriviallyabsorbedforℓ≥4.)
30CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
Finally,wecancombine(89),(90),(91)toobtainthattheRHSof(86)isoforderO(C7 log √10(d)),wherewegetanextra
X d
C fromboundingtheoperatornormofB¯. Thus,usingthat(cid:80) c2 <∞,weconclude
X ℓ≥3 ℓ
(cid:88)
(cid:18) log10(d)(cid:19)
c2ℓ∇ℓ =O C7 √ ,
ℓ Bˆ X
d
ℓ≥3
whichfinishestheproof.
A.6.GD-analysisandreductiontoGaussian
Tosimplifythenotationwewillpushthetimedependenceinthesubscript,i.e. B =B(t).
t
TheoremA.17(Gaussianrecursion). Iftheentries(B′) ∼N(0,1)arei.i.d.,B =proj(B′)and
0 i,j d 0 0
(cid:16) (cid:17)−2 (cid:18)(cid:16) (cid:17)−2 (cid:19)
∇ B⊤ =−α B B⊤+αI B B⊤+αDiag B B⊤+αI B B⊤ B B⊤+E˜ , (92)
B t t t t t t t t t t t t
B B⊤ =I+Z +E , (93)
t t t t
withZ =U(Λ −I)U⊤,U aHaarmatrixand
t t
(cid:13) (cid:13)E˜t(cid:13)
(cid:13) ≤C
(cid:18) poly E√(log(d))
·∥Z ∥1/2+∥E ∥2 +∥E ∥ ∥Z
∥1/2(cid:19)
.
(cid:13) (cid:13) op E d t op t op t op t op
ConsidertheGD-minalgorithmin(25)withoutnoise(G =0forallt)andontheGaussianobjective(i.e.,p=1). Picka
√ t
learningrateη =C/ d. Then,withprobabilityatleast1−Cexp(−cd),wehavethat,jointlyforallt≥0,
poly (log(d))
∥E ∥ ≤C e−cηt. E√ ,
t op E d (94)
∥Z ∥ ≤C e−cηt.
t op Z
Proof. TheclaimfollowsfromtheanalysisinAppendixEof(Shevchenkoetal.,2023). First,notethathereE˜ andE
t t
respectivelycorrespondtoE andX in(90)in(Shevchenkoetal.,2023).Then,theassumptionsofourtheoremcorrespond
t t
totheconclusionofLemmaE.4andLemmaE.5. TheprojectionstepishandledinLemmaE.6andtherecursionisanalysed
inLemmaE.7.
Lemma A.18 (Reduction to Gaussian recursion). Fix t
max
= T max/η,T
max
∈ (0,∞), let (B′ 0)
i,j
∼ N(0,√1 d) i.i.d.,
B =proj(B′)andassumethatfort≤t wehave
0 0 max
(cid:16) (cid:17)−2 (cid:18)(cid:16) (cid:17)−2 (cid:19) (cid:18) logαR(d)(cid:19)
∇ B⊤+G =−α B B⊤+αI B B⊤+αDiag B B⊤+αI B B⊤ B B⊤+O C (T ) √ .
B t t t t t t t t t t t t R max
d
(95)
√
ConsidertheGD-minalgorithmin(25)foranyp∈(0,1). Pickalearningrateη =C/ d. Then,withprobabilityatleast
1−Cexp(−cd),wehavethat,jointlyforallt≥0,(93)holdswith
poly (log(d))
∥E ∥ ≤C e−cηt. E√ ,
t op E d (96)
∥Z ∥ ≤C e−cηt,
t op Z
wherecruciallyC ,poly ,C areindependentofd,andC isindependentofT .
E E Z Z max
Proof. TheclaimfollowsfromTheoremA.17aftershowingthat
logαR(d) (cid:18) poly (log(d)) (cid:19)
C (T ) √ ≤C E√ ·∥Z ∥1/2+∥E ∥2 +∥E ∥ ∥Z ∥1/2 , (97)
R max d E d t op t op t op t op
31CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
where∥E ∥ ,∥Z ∥ satisfy(94). Now,(97)triviallyholdsif∥Z ∥ ≥c (T )>0,wherec (T )isindependent
t op t op t op Z max Z max
ofd.
Itremainstoshowthelowerboundon∥Z ∥ . Thiscanbereadilyseenbyanalyzingthedeterministicrecursionofthe
t op
spectrumofZ asinLemmaG.3in(Shevchenkoetal.,2023). First,forsufficientlylarged,ηgetsarbitrarilysmall,hence
wecanapproximatesuchdiscreterecursionwithitscontinouousanalogue. Next,welinearizethecontinuousevolution
sinceZ issmall(otherwisewealreadyhavethedesiredlowerbound). Sincethecoefficientofthelinearizationisstrictly
negative(and,hence,boundedawayfrom0),wereadilyhavethat∥Z ∥ cannotreach0infinitetime.
t op
Fortechnicalreasons,weneedthefollowinglemmathatshowsthatthespectrumofB aprioricannotgrowfasterthan
exponentiallyintheeffectivetimeofthedynamics. Theproofisanon-tightanalogoftheanalysisdoneinLemmaE.7and
G.3in(Shevchenkoetal.,2023)forBinsteadofBB⊤.
LemmaA.19(SpectrumevolutionofB). ConsidertheGD-minalgorithmin(25)foranyp∈(0,1). Pickalearningrate
√
η =C/ d. Underthegradientapproximationgivenin(83)withC (t):=exp(Cηt)∥B ∥ ,wehavethat,fort≤t
X 0 op max
andd>d (C (t )),
0 X max
B =X +R ,
t t t
whereX hasthesamesingularvectorsasB ,
t 0
log10(d)
∥X ∥ ≤C (t), and ∥R ∥ ≤CC7(t )exp(CT ) √ ,
t op X t op X max max d
withprobabilityatleast1−C(ηt ) 1 .
max d3/2
Proof. Considertherecursionwherethegradientisgivenbelow:
1 (cid:16) (cid:17)−2 1 (cid:20)(cid:16) (cid:17)−2 (cid:21)
∇ :=−α XX⊤+αI X+α Tr XX⊤+αI XX⊤ X. (98)
2 B n
Itisevidentthatthisrecursiononlyupdatesthesingularvaluessi ofB astheRHShasthesamesingularvectorsasB.
Furthermore,theupdateequationforthesiisgivenby
(cid:32) si 1 (cid:88)n (si)2 (cid:33)
si =si−η −α +si t .
t+1 t (α+(si)2)2 tn (α+(si)2)2
t i=1 t
Notethat
(cid:12) (cid:12)si −si(cid:12) (cid:12)≤Cη(cid:12) (cid:12)si(cid:12) (cid:12).
t+1 t t
Thus,lettingb =∥B ∥ ,theaboveimpliesthat
t t op
b ≤(1+Cη)b , (99)
t+1 t
whichbymonotonicitygivesthatb ≤(1+Cη)tb . Hence,iftherecursionofthegradientwasactuallygivenby(98),the
t 0
claimwouldimmediatelyfollow.
Now,therecursionofthegradientisgivenby(83). Thus,todealwiththeerror,wecanfollowthestrategyofLemmaE.7in
(Shevchenkoetal.,2023). Inparticular,denotingr
t
:=∥R t∥ op,ϵ
d
:=C X10log √10 d(d),theevolutionoftheerrorisgivenby
r ≤(1+C η)r +C ϵ .
t+1 1 t 2 d
Bymonotonicity,thisrecursionisupperboundedbythesolutionof
r =(1+C η)r +C ϵ .
t+1 1 t 2 d
32CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
Sincetherecursionisinitializedwithr =0,wecanunrollitas
0
t
(cid:88)
r =C (1+C η)iηϵ
t+1 2 1 d
i=1
t
(cid:88)
≤C exp(C ηi)ηϵ
2 1 d
i=1
t
(cid:88)
=C exp(C ηt) exp(−C η(t−i))ηϵ
2 1 1 d
i=1
1
≤C exp(C ηt) ηϵ ,
2 1 1−exp(−C η) d
1
wherewehaveused1+x≤exp(x). Forsmallenoughη,wehave 1 ≤ 1 . Hence,
1−exp(−C1η) C1η
C
r ≤ 2exp(C ηt)ϵ , (100)
t+1 C 1 d
1
whichgivesthat∥R t∥
op
≤C X10C C2 1exp(C 1ηt)log √10 d(d),asrequired. Hence,by(83),b tisupperboundedbythesolutionto
therecursion
b =(1+C η)b +C ηexp(C ηt)ϵ .
t+1 1 t 2 3 d
(cid:104) (cid:105)
AsTr BB⊤ =n,wehavethatb ≥1. Thus,
t
b ≤(1+(C +C exp(C ηt)ϵ )η)b ≤(1+(C +C exp(C T )ϵ )η)b .
t+1 1 2 3 d t 1 2 3 max d t
TakingasufficientlylargedgivesthatC exp(C T )ϵ ≤C ,whichleadsto
2 3 max d 1
b ≤(1+2C η)b .
t+1 1 t
Usingagainmonotonicityand1+x≤exp(x),weconcludethatC :=∥B ∥ ≤exp(2C ηt)∥B ∥ .
X tmax op 1 0 op
Thisprovestheclaimofthelemmaforagradientrecursiongivenexactlyby(83). WenotethattheGD-minalgorithmin
(25)hastwoadditionalsteps: (i)addingnoiseG ateachstep,and(ii)theprojectionsstep,whichnormalizestherowsof
t
B afterthegradientupdate.
t
√
Asfor(i),letGbeann×dmatrixwithi.i.d.N(0,σ2)entries. Then,byTheorem4.4.5in(Vershynin,2018)(witht= d),
√ √ √
wehavethat∥G∥ ≤ Cσ( n+ d) ≤ Cσ dwithprobabilityatleast1−C 1 . Recallthatin(25)weassumethat
op d2
σ ≤C1. Hence,theadditionalerrorfromthenoiseisofhigherorderthanalltheothererrortermsandcanbeneglected. By
d
aunionboundoverT /ηsteps,theaboveboundholdsforalltimestepswithprobabilityatleast1−C 1 .
max d3/2
Asfor(ii),astraightforwardanalysisshowsthat(cid:13) (cid:13)proj(B′)−B′(cid:13) (cid:13) op ≤Cη2∥∇ B +G∥2 op,whichisalsoofhigherorder.
WeskipthedetailshereandrefertoLemmaE.6in(Shevchenkoetal.,2023). Thisconcludestheproof.
WearenowreadytogivetheproofofTheoremA.2bycombiningthepreviousresultsandcarryingoutaninductionover
thetimesteps.
ProofofTheoremA.2. Wefixp ∈ (0,1)andt = T /η,T ∈ (0,∞). Wewanttoshowthattheassumptionsof
max max max
LemmaA.18aresatisfied,astheconclusionofLemmaA.18isprecisely(26).
ByLemmaA.19,wehavethat,withprobabilityatleast1−C(T ) 1 ,forallt≤t ,C (t):=exp(Cηt)∥B ∥ ,
max d3/2 max X 0 op
andC R(t) = CC X10(t max)exp(Cηt)logα √R d(d).Furthermore,bychoosingδ = d−(4+γg),wecanapplyLemmaA.10for
eachstepsothat,withprobabilityatleast1−C d31 /2,min i,j(cid:12) (cid:12)B′ ij(cid:12) (cid:12) ≥ 2δ. Notethattheprojectionstepdoesnotchange
thescaleofanyentrybymorethanafactorthatconvergesto1asdgrowslarge(seeLemmaE.6in(Shevchenkoetal.,
2023)fordetails),soinparticularmin i,j(cid:12) (cid:12)B′ ij(cid:12) (cid:12) ≥ 2δ impliesmin i,j|B ij| ≥ δ. Thisgivesthat,withprobabilityatleast
1−C(T ) 1 ,forallt≤t ,theassumptionsofLemmaA.16aresatisfied,hence(82)and(83)hold.
max d3/2 max
33CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
By2. inLemmaA.11,ateachstepwithprobability1−C 1 ,wehavethat
d2
(cid:18)(cid:16) (cid:17)−2 (cid:19) 1 (cid:20)(cid:16) (cid:17)−2 (cid:21) log(d)
Diag BB⊤+αI BB⊤ = Tr BB⊤+αI BB⊤ I+Cexp(CT )∥B ∥ √ ,
n max 0 op d
sothisholdsjointlyforallt≤t withprobabilityatleast1−C 1 Combiningthiswith(82),weconcludethat,with
max 3
d2
probabilityatleast1−C(T ) 1 ,theassumptionsoflemmaA.18hold,whichimmediatelyimplies
max 3
d2
lim sup ∥R ∥=0
t
d→∞t∈[0,tmax]
and
(cid:13) (cid:13)
(cid:13)S S⊤−I(cid:13) ≤Cexp(−cT ).
(cid:13) t t (cid:13) max
op
Thisproves(26).
Toprove(27),wenotethatthecombinationof(51),(52)and(57)gives
E (cid:20) Tr(cid:104) A⊤Af(BˆBˆ⊤ )(cid:105) − √2 Tr(cid:104) ABˆ(cid:105)(cid:21) =Tr(cid:104) A⊤Af(BB⊤)(cid:105) −2Tr(cid:104) AB⊤(cid:105) +O(cid:18) C(T )dpoly( √log(d))(cid:19) .
m p max d
(101)
Since(24)and(3)differonlybyaconstantandafactor1/d,theaboveimpliesthat,foranyp∈(0,1),(3)isclosetothe
GaussianobjectiveuptoanerrorC(T max)poly( √log(d)). ThefactthattheevolutionofBmatchestheGaussiancaseisalso
d
clear,sincethegradientapproximationinLemmaA.16coincideswiththeGaussianrecursioninTheoremA.17.
B.MSEcharacterizations
B.1.ProofofProposition4.2
Denotebyx1thefirstiterateoftheRI-GAMPalgorithm(Venkataramananetal.,2022),asin(21). Then,bytakingσtobe
thesign,onecanreadilyverifythat
x1 =B⊤sign(Bx).
Note that B is bi-rotationally invariant in law and, as x has i.i.d. components, its empirical distribution converges in
Wasserstein-2distancetoarandomvariablewhoselawisthatofthefirstcomponentofx,denotedbyx . Therefore,the
1
assumptionsofTheorem3.1in(Venkataramananetal.,2022)aresatisfied. Hence,foranyψpseudo-Lipschitzoforder2,4
wehavethat,almostsurely,
d
1(cid:88)
lim ψ((x1) ,(x) )=E[ψ(µx +σg,x )],
d→∞d i i 1 1
i=1
whereg ∼N(0,1)isindependentofx andthestateevolutionparameters(µ,σ)forr ≤1canbecomputedas
1
(cid:114) (cid:114) (cid:18) (cid:19) (cid:18) (cid:19)
2κ 2 2 2
µ=r· 2 =r· , σ2 =r· κ +κ · =r· 1−r· , (102)
π π 2 4 πκ π
2
thatisequation(11)in(Venkataramananetal.,2022). Here,{κ 2k} k∈Ndenotetherectangularfreecumulantsoftheconstant
randomvariableequalto1(sinceallthesingularvaluesofBareequalto1byassumption).Notingthatψ(x,y)=(x−α·y)2
ispseudo-Lipschitzoforder2,wegetthat,almostsurely,
1
lim ·∥x−α·B⊤sign(B⊤x)∥2 =E [|x −α(µx +σg)|2],
d→∞d 2 x1,g 1 1 2
whichimpliesthat
1
lim ·E ∥x−α·B⊤sign(B⊤x)∥2 =E [|x −α(µx +σg)|2].
d→∞d x 2 x1,g 1 1 2
4Werecallthatψ:R2 →Rispseudo-Lipschitzoforder2if,foralla,b∈R2,|ψ(a)−ψ(b)|≤L∥a−b∥ (1+∥a∥ +∥b∥ )for
2 2 2
someconstantL>0.
34CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
ByexpandingtheRHSofthelastequationandusingthatx hasunitsecondmomentbyassumption,weget
1
E [|x −α(µx +σg)|2]=(1−αµ)2·E[x2]+α2σ2·E[g2]=(1−αµ)2+α2σ2
x1,g 1 1 2 1
(cid:114)
2
=1−2αµ+α2(µ2+σ2)=1−2α·r +α2r.
π
Thus,byminimizingoverα,wehave
2
minE [|x−α(µx+σg)|2]=1− ·r,
α x1,g 2 π
whichconcludestheproofof(13).
Toprove(14),adirectcalculationgives
d1 ·E x(cid:13) (cid:13) (cid:13) (cid:13)x−α·(cid:20)
0
(d−I nn )×n(cid:21) sign([I n,0 n×(d−n)]x)(cid:13) (cid:13) (cid:13) (cid:13)2
2
=1−r+r·E[(x 1−αsign(x 1))2]
=1−r+r·(E[x2]−2α·E[|x |]+α2·E[sign2(x )])
1 1 1
=1−r+r·(1−2α·E[|x |]+α2)
1
=1+r·(α2−2α·E[|x |]).
1
TheRHSisminimizedbyα=E[|x |],whichgives
1
m αin d1 ·E x(cid:13) (cid:13) (cid:13) (cid:13)x−α·(cid:20)
0
(d−I nn )×n(cid:21) sign([I n,0 n×(d−n)]x)(cid:13) (cid:13) (cid:13) (cid:13)2
2
=1−r·(E[|x 1|])2,
andtheproofiscomplete.
B.2.ProofofProposition5.1
Letxˆ1beaniterateoftheRI-GAMPalgorithm(Venkataramananetal.,2022),asin(21). Then,bytakingσtobethesign
andf =f,onecanreadilyverifythat
t
xˆ1 =f(B⊤sign(Bx)),
whichisexactlytheformoftheautoencoderin(4)thatwewishtoanalyze. Thus,asf isLipschitz,theassumptionsof
Theorem3.1in(Venkataramananetal.,2022)aresatisfiedand,followingthesamepassagesasintheproofofProposition
4.2,wehave
1
lim ·E ∥x−f(B⊤sign(Bx))∥2 =E [|x −f(µx +σg)|2], (103)
d→∞d x 2 x1,g 1 1 2
wherex isthefirstentryofx,g ∼ N(0,1)isindependentofx ,and(µ,σ)aregivenby(102)(whichcoincideswith
1 1
(18)). Thisconcludestheproof.
B.3.ProofofProposition5.2
Adirectcalculationgives
d1 ·E x(cid:13) (cid:13) (cid:13) (cid:13)x−f(cid:18)(cid:20)
0
(d−I nn )×n(cid:21) sign([I n,0 n×(d−n)]x)(cid:19)(cid:13) (cid:13) (cid:13) (cid:13)2
2
=(1−r)·E(cid:2) (x 1−f(0))2(cid:3) +r·E(cid:2) (x 1−f(sign(x 1)))2(cid:3) ,
(104)
wherex isthefirstentryofx. Thefirsttermin(104)isminimizedwhenf(0)=E[x]=0. Hence,weobtainthat,atthe
1
optimum,
(1−r)·E(cid:2)
(x
−f(0))2(cid:3)
=1−r,
1
asE[x2]=1. Asforthesecondtermin(104),werewrite
E(cid:2) (x −f(sign(x )))2(cid:3) =µ ({0})·1 ·(f(1)2+f(−1)2)+E[1 (x −f(1))2]+E[1 (x −f(−1))2], (105)
1 1 x1 2 x1>0 1 x1<0 1
35CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
whereµ standsforthemeasurethatcorrespondstothedistributionofx ,andweusethatsign(0)isaRademacherrandom
x1 1
variablebyconvention. Asthedistributionofx isthesameasthatof−x ,(105)isminimizedbytakingf(1)=−f(−1).
1 1
Thus,wehavethat
min(105)=minE[(x −u·sign(x ))2].
1 1
f u∈R
TheRHSofthislastexpressioncanbefurtherrewrittenas
minE[(x −u·sign(x ))2]=E[x2]+min(cid:8) u2−2u·E|x |(cid:9) =1−(E|x |)2,
u∈R 1 1 1 u∈R 1 1
whichconcludestheproof.
B.4.Computationoff∗
SparseGaussian. UsingBayesrule,theconditionalexpectationcanbeexpressedas
E [x·P(µx+σg =y|x)] E [x·P(µx+σg =y|x)]
E[x|µx+σg =y]= x = x . (106)
E [P(µx+σg =y|x)] P(µx+σg =y)
x
Given that x ∼ SG (p), with probability p we have that µx+σg ∼ N(0,µ2/p+σ2) as x ∼ N(0,1/p), and with
1
probability(1−p)wehavethatx=0,and,hence,µx+σg =σg ∼N(0,σ2). Combininggives
√
p (cid:18) py2 (cid:19) 1 (cid:18) y2 (cid:19)
P(µx+σg =y)=p· ·exp − +(1−p)· √ ·exp − .
(cid:112) 2π(µ2+pσ2) 2(µ2+pσ2) 2πσ2 2σ2
Notethatduetosparsity,wehavethat
E [x·P(µx+σg =y|x)]=p·E [x·P(µx+σg =y|x)], (107)
x x∼N(0,1/p)
and,inthiscase,weconcludethat
µx+σg|x∼N(µx,σ2).
Thus, the RHS of (107) is a Gaussian integral, which is straight-forward to calculate by “completing a square”. The
computationgives
(cid:114) p (cid:18) py2 (cid:19) 1
E [x·P(µx+σg =y|x)]= ·µy·exp − · .
x∼N(0,1/p) 2π 2(µ2+pσ2) (µ2+pσ2)3/2
Notethat, whenp = 1, i.e., xisanisotropicGaussianvector, f∗ isjustarescalingbyaconstantfactor, i.e., f∗(y) =
const(µ,σ)·y.
SparseLaplace. ThesparseLaplacedistributionwithsparsitylevel(1−p)hasthefollowinglaw
(cid:114)
p (cid:16) (cid:112) (cid:17)
(1−p)·δ +p· ·exp − 2p·|x| , (108)
0 2
whereδ standsforthedeltadistributioncenteredat0. Thescalingfordifferentpischosentoensureaunitsecondmoment.
0
First,wederivetheexpressionfortheconditionalexpectationforp=1. Forp̸=1weelaboratelaterhowasimplechange
ofvariablesallowstoobtainclosed-formexpressionsofthecorrespondingexpectationsviathecasep=1. Forp=1,the
denominatorin(106)isequivalentto
(cid:90) 1 (cid:90) (cid:16) √ (cid:17) (cid:18) (y−µx)2(cid:19)
p(x)p(µx+σg =y|x)dx= √ exp − 2·|x| exp − dx. (109)
R 4πσ2 R 2σ2
Byconsideringtwocases,i.e.,x<0andx≥0,forthelimitsofintegrationandforeachofthem“completingasquare”,
weobtain
(cid:90) (cid:16) √ (cid:17) (cid:18) (y−µx)2(cid:19) (cid:32) (cid:32)√ 2µy−2σ2(cid:33)(cid:33) (cid:32) σ2−√ 2µy(cid:33) (cid:114) π σ
exp − 2·x exp − dx= 1+erf ·exp · · ,
2σ2 2µσ µ2 2 µ
R
+
(cid:90) (cid:16)√ (cid:17) (cid:18) (y−µx)2(cid:19) (cid:32)√ 2µy+2σ2(cid:33) (cid:32) σ2+√ 2µy(cid:33) (cid:114) π σ
exp 2·x exp − dx=erfc ·exp · · ,
2σ2 2µσ µ2 2 µ
R
−
36CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
whereerf(·)standsfortheGaussianerrorfunction,anderfc(·)foritscomplement. Forthecaseofp̸=1,wegetthatthe
RHSof(109)becomes
1 (cid:18) y2 (cid:19) (cid:114) p (cid:90) (cid:16) (cid:112) (cid:17) (cid:18) (y−µx)2(cid:19)
(1−p)· √ ·exp − +p· · exp − 2p·|x| exp − dx.
2πσ2 2σ2 4πσ2 R 2σ2
The change in normalization constant of the second term is then trivial. For the integral itself, consider the change of
√
variablesx˜=x· p:
(cid:90) (cid:16) (cid:112) (cid:17) (cid:18) (y−µx)2(cid:19) 1 (cid:90) (cid:16) √ (cid:17) (cid:32) (y− √µ p ·x˜)2(cid:33)
exp − 2p·|x| exp − dx= √ · exp − 2·|x˜| exp − dx˜
2σ2 p 2σ2
R R
1 (cid:90) (cid:16) √ (cid:17) (cid:18) (y−µ˜·x˜)2(cid:19)
= √ · exp − 2·|x˜| exp − dx˜,
p 2σ2
R
√
whichisexactlythepreviousintegralin(109)butwithµ˜ =µ/ pandanadditionalscalingfactorinfront.
Considerthenumeratorof(106)forp=1. Forthiscase,thecomputationreducestoevaluating:
(cid:90) 1 (cid:90) (cid:16) √ (cid:17) (cid:18) (y−µx)2(cid:19)
x·p(x)p(µx+σg =y|x)dx= √ x·exp − 2·|x| exp − dx. (110)
R 4πσ2 R 2σ2
Reducingtocasesagainand“completingasquare”gives
(cid:90) (cid:16) √ (cid:17) (cid:18) (y−µx)2(cid:19)
x·exp − 2·x exp − dx
2σ2
R
+
=exp(cid:18)
−
y2 (cid:19) · σ2
+
√ πσ·(√ 2µy−2σ2)·e(µy 2− µ√ 2σ2σ 22)2 ·(cid:16) 1+erf(cid:16) √y
2σ
− σ µ(cid:17)(cid:17)
,
2σ2 µ2 2µ3 
(cid:90) (cid:16)√ (cid:17) (cid:18) (y−µx)2(cid:19)
x·exp 2·x exp − dx
2σ2
R
−
=exp(cid:18)
−
y2 (cid:19) · −σ2
+
√ πσ·(√ 2µy+2σ2)·e(µy 2+ µ√ 22 σσ 22)2 ·erfc(cid:16) √y
2σ
+ σ µ(cid:17)
.
2σ2  µ2 2µ3 
Thederivationforthecasep̸=1canbeobtainedanalogously,bynotingthat(110)inthiscaseiswrittenas
(cid:114) p (cid:90) (cid:16) (cid:112) (cid:17) (cid:18) (y−µx)2(cid:19)
p· · x·exp − 2p·|x| exp − dx.
4πσ2 2σ2
R
SparseRademacher. ThesparseRademacherdistributionwithsparsitylevel(1−p)hasthefollowinglaw
p (cid:0) (cid:1)
(1−p)·δ + · δ √ +δ √ .
0 2 1/ p −1/ p
Thedenominatorin(106)reducesto
√ √
1 (cid:18) y2 (cid:19) p 1 (cid:20) (cid:18) (y−µ/ p)2(cid:19) (cid:18) (y+µ/ p)2(cid:19)(cid:21)
(1−p)· √ ·exp − + · √ · exp − +exp − .
2πσ2 2σ2 2 2πσ2 2σ2 2σ2
Moreover,itiseasytoseethattheenumeratorof(106)reducesto
√ √ √
p (cid:20) (cid:18) (y−µ/ p)2(cid:19) (cid:18) (y+µ/ p)2(cid:19)(cid:21)
· exp − −exp − .
2 2σ2 2σ2
37CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
C.Experimentaldetailsandadditionalnumericalresults
C.1.Numericalsetup
ActivationfunctionandreparameterizationoftheweightmatrixB. Sincethesignactivationhasderivativezero
almosteverywhere,itisnotdirectlysuitedforgradient-basedoptimization. ToovercomethisissueforSGDtrainingofthe
modelsdescribedinthemainbody,weusea“straight-through”(seeforexample(Yinetal.,2019))approximationofit.
Indetails,duringtheforwardpasstheactivationofthenetworkσ(·)istreatedasasignactivation. However,duringthe
backwardpass(gradientcomputation)thederivativesarecomputedasifinsteadofσ(·)itsrelaxedversionisused,namely,
thetemperedhyperbolictangent:
(cid:16)x(cid:17)
σ (x)=tanh .
τ τ
Wealsonotethatsuchapproximationispointwiseconsistentexceptzero:
lim =σ(x), ∀x∈R\{0}.
τ→0
Fortheexperimentswefixthetemperatureτ tothevalueof0.1. Refiningtheapproximationfurther,i.e.,makingτ smaller,
doesnotaffecttheendresult,butitmakesnumericsabitlessstableduetotheincreasedvarianceofthederivative.
To ensure consistency of the “straight-though” approximation, we enforce the condition B ∈ Sd−1 via a simple
i,:
differentiablereparameterization. LetB ∈Rn×dbetrainablenetworkparameters,then
B
Bˆ = i,: .
i,: ∥B ∥
i,: 2
Itshouldbenotedthatitisnotclearwhetherthisconstraintisnecessary,sinceduringtheforwardpassweusedirectlyσ(·),
whichisagnostictotherowscalingofB.
Augmentationandwhitening. ForthenaturalimageexperimentsinFigures3,6and11,weusedataaugmentationto
bringtheamountofimagesperclasstotheinitialdatasetscale. Thisstepiscrucialtosimulatetheminimizationofthe
populationriskandnottheempiricalone,whenthenumberofsamplesperclassisinsufficient. Weaugmenteachimage15
timesforCIFAR-10dataand10timesforMNISTdata. Wenotethatthedescribedamountofaugmentationissufficient:
increasingitfurtherdoesnotchangetheresultsofthenumericalexperimentsandonlyincreasescomputationalcost.
Thewhiteningprocedurecorrespondstothematrixmultiplicationofeachimagebytheinversesquarerootoftheempirical
covarianceofthedata. Thisisdonetoensurethatthedataisisotropic(tobeclosertothei.i.d.dataassumptionneededfor
thetheoreticalanalysis). Moreformally,letX ∈Rnsamples×dbetheaugmenteddatathatiscentered,i.e.,thedatameanis
subtracted. Itsempiricalcovarianceisthengivenby
Σˆ =
1 ·ns (cid:88)amples
X X⊤.
n −1 i,: i,:
samples
i=1
Inthisview,thewhiteneddataXˆ ∈Rnsamples×disobtainedfromtheinitialdataX asfollows
Xˆ =Σˆ− 21 X ,
i,: i,:
whereX definesthei-thdatasample.
i,:
C.2.Phasetransitionandstaircaseinthelearningdynamicsfortheautoencoderin(2)
First,weprovideanadditionalnumericalsimulationsimilartotheoneinFigure2forthecaseofnon-sparseRademacherdata,
i.e.,p=1. Sincecondition(15)holds,weexpecttheminimizertobeapermutationoftheidentity,andthecorresponding
SGDdynamicstoexperienceastaircasebehaviour,asdiscussedinSection4. Namely,theSGDalgorithmfirstfindsa
randomrotationthatachievesGaussianperformance(indicatedbytheorangedashedline). Next,itsearchesadirection
towardsasparsesolutiongivenbyapermutationoftheidentity,andthecorrespondinglossremainsattheplateau. Finally,
thecorrectdirectionisfound,andSGDquicklyconvergestotheoptimalsolution.
38CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
SGD
1.2 Gaussian performance: 1 2/
Global minimum: 1 p
1.0
0.8
0.6
0.4
0.2
0.0
0 1 2 3 4 5 6 7
Iteration 1e7
Figure8.CompressionofRademacherdata(p = 1)viatheautoencoderin(2). Wesetd = 200andr = 1. TheMSEisplottedasa
functionofthenumberofiterations,anditdisplaysastaircasebehavior.
Next,weconsiderthecompressionofxwithi.i.d.componentsdistributedaccordingtothefollowingsparsemixtureof
Gaussians:
(cid:18) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19)
1 1−p 1 1−p
x ∼p· ·N 1, + ·N −1, +(1−p)·δ .
i 2 p 2 p 0
ItiseasytoverifythatE[x2]=1. Inordertocomputethetransitionpointweneedtoaccessthefirstabsolutemomentofx ,
i i
i.e.,E|x |. Usingtheresultin(Winkelbauer,2012),weareabletoclaimthat
i
(cid:114) (cid:18) (cid:19)
2 1 1 1
E |x|=σ ·Φ − , ,− , (111)
x∼N(±1,σ2) π 2 2 2σ2
whereΦ(a,b,c)standsforKummer’sconfluenthypergeometricfunction:
(cid:88)∞ an cn
Φ(a,b,c)= · ,
bn n!
n=1
withxndenotingtherisingfactorial,i.e.,
xn =z·(z+1)·····(z+n−1), n∈N .
0
Weusescipy.special.hyp1f1toevaluatenumericallyΦ(cid:0) −1,1,− 1 (cid:1) ,whereσ2 =(1−p)/p. Likewise,tofind
2 2 2σ2
(cid:113)
p atwhichE|x |= 2 werelyonnumerics. TheresultsarepresentedinFigure9.
crit i π
WeremarkthatthefirstabsolutemomentcanalwaysbeestimatedviaMonte-Carlosamplingifafunctionalexpressionsuch
as(111)isoutofreach. Wealsonotethatthebehaviourofthepredictedcurveafterthetransitionpointp canbearbitrary.
crit
Inparticular,itisnotalwayslinearlikeinthecaseofsparseRademacherdatainFigure1. Forinstance,inthecaseofthe
sparseGaussianmixtureofFigure9,theshapeisclearlyofnon-linearnature.
39
ESMCompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
0.35
0.30
0.25
0.20
0.15 SGD
Theoretical prediction
Gaussian performance
0.10 pcrit 0.79
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
p
Figure9.CompressionofdatawhosedistributionisgivenbyasparsemixtureofGaussiansviatheautoencoderin(2).Wesetd=100
andr = 1. Left. MSEachievedbySGDatconvergence,asafunctionofthesparsitylevelp. Theempiricalvalues(dots)matchour
theoreticalprediction(blueline):forp<p ,thelossisequaltothevalueobtainedforGaussiandata,i.e.,1−2r/π;forp>p ,the
crit crit
lossissmaller,anditisequalto1−r·(E|x |)2.Center.EncodermatrixBatconvergenceofSGDwhenp=0.6<p :thematrixis
1 crit
arandomrotation.Right.EncodermatrixBatconvergenceofSGDwhenp=0.9≥p .ThenegativesigninpartoftheentriesofB
crit
iscancelledbythecorrespondingsignintheentriesofA.Hence,Bisequivalenttoapermutationoftheidentity.
1.4
SGD
Gaussian performance: 1 2/
1.2
Global minimum: 1 ( |x1|)2
1.0
0.8
0.6
0.4
0.2
0 1 2 3 4
Iteration 1e7
Figure10.CompressionofdatawhosedistributionisgivenbyasparsemixtureofGaussiansviatheautoencoderin(2).Wesetd=100,
r=1,andp=0.9.TheMSEisplottedasafunctionofthenumberofiterationsand,asp>p ,itdisplaysastaircasebehavior.
crit
InFigure10,weprovideanexperimentsimilartothatofFigure2,butforthecompressionofasparsemixtureofGaussians
withp=0.9atr =1. WecanclearlyseethatFigure10againindicatestheemergentstaircasebehaviouroftheSGDloss
forp>p .
crit
C.3.MNISTexperiment
Inthissubsection,weprovideadditionalnumericalevidencecomplementingtheresultspresentedinFigure3. Namely,we
provideasimilarevaluationonBernoulli-maskedwhitenedMNISTdata. AsfortheexperimentinFigure3,thesparsity
levelpissetto0.7.
Notethattheeigen-decompositionofthecovarianceofMNISTdatahaszeroeigenvalues. Inthiscase,weneedtoapplythe
lowerboundfrom(Shevchenkoetal.,2023)thataccountsforadegeneratespectrum. Thecorrespondingresultisstatedin
Theorem5.2of(Shevchenkoetal.,2023). Inparticular,thenumberofzeroeigenvaluesn isequalto179,whichmeans
0
thatatthevalueofthecompressionratergivenby
d−n 282−179
r = 0 = ≈0.77
d 282
40
ESM
ESMCompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
Gaussian performance
0.7 SGD
0.6
0.5
0.4
0.3
0.2 0.4 0.6 0.8 1.0
r
Figure11.CompressionofmaskedandwhitenedMNISTimagesthatcorrespondtodigit“zero”viathetwo-layerautoencoderin(2).
First,thedataiswhitenedsothatithasidentitycovariance(asinthesettingofTheorem4.1).Then,thedataismaskedbysettingeach
pixelindependentlyto0withprobabilityp=0.7.Anexampleofanoriginalimageisonthetopright,andthecorrespondingmaskedand
whitenedimageisonthebottomright.TheSGDlossatconvergence(dots)matchesthesolidline,whichcorrespondstothepredictionin
(.5)forthecompressionofstandardGaussiandata(withnosparsity).
thederivativeofthelowerboundexperiencesajump-likebehavior,asdescribedin(Shevchenkoetal.,2023).
C.4.CIFAR-10: Laplaceapproximationofpixeldistribution
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
10 5 0 5 10
Figure12.EmpiricaldistributionofwhitenedCIFAR-10imagepixels(bluehistogram),anditsapproximationviaaLaplacedistribution
withunitsecondmoment(orangecurve).
Figure12demonstratesthequalityoftheLaplaceapproximationforwhitenedCIFAR-10images. Namely,wenotethat
theempiricaldistributionoftheimagepixelsafterwhiteningiswellapproximatedbyaLaplacerandomvariablewithunit
secondmoment.
C.5.ProvablebenefitofnonlinearitiesforthecompressionofsparseGaussiandata
Figure13considersthecompressionofsparseGaussiandata,anditshowsthattheMSEachievedbytheautoencoderin
(4)withtheoptimalchoiceoff (namely,theRHSof(17)withf =f∗)isstrictlylowerthantheMSE(5)achievedbythe
autoencoderin(2),foranysparsitylevelp∈(0,1). TheconditionalexpectationE[x |µx +σg](cf.thedefinitionoff∗in
1 1
(19))iscomputednumericallyviaaMonte-Carloapproximation.
41
ESMCompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
0.35
0.30
0.25
0.20
0.15
Prediction, decoder + non-linearity
Gaussian performance
0.2 0.4 0.6 0.8 1.0
p
Figure13.CompressionofsparseGaussiandata.Wesetr=1.ThesolidbluelinecorrespondstotheMSEin(17)withf =f∗(defined
in(19)),fordifferentvaluesofp;thedashedorangelinecorrespondstotheGaussianperformancein(5),whichisachievedbythe
autoencoderin(2).
C.6.Phasetransitionandstaircaseinthelearningdynamicsfortheautoencoderin(4)
ForsparseRademacherdata,theoptimalf∗givenby(19)iscomputedexplicitlyinAppendixB.4andplottedinFigure14.
Wenotethatfunctionsoftheformin(16)areunabletoapproximatef∗well. Thus,intheexperimentsweuseadifferent
parametricfunctionforf givenbythefollowingmixtureofhyperbolictangents:
f(x)=1 ·(γ ·tanh(ε ·x−α )+β )+1 ·(γ ·tanh(ε ·x−α )+β ). (112)
x≥0 1 1 1 1 x<0 2 2 2 2
2
1
0
1
2
4 2 0 2 4
Figure14. Optimalf∗in(19)whenx isasparseRademacherrandomvariable.Wesetr=1andp=0.2.
1
Thenumericalevaluationoftheautoencoderin(4)withf oftheformin(112)forthecompressionofsparseRademacher
dataisprovidedinFigure15. Wesetr =1andd=200. ThesolidbluelinecorrespondstothepredictionofProposition
5.1,obtainedforrandomHaarB;thesolidorangelinecorrespondstothepredictionofProposition5.2,obtainedforB
equaltotheidentity. ThebluedotscorrespondtotheperformanceofSGD,andtheyexhibitthetransitioninthelearntB
fromarandomHaarmatrix(p<p )toapermutationoftheidentity(p>p ). Thecriticalvaluep isobtainedfrom
crit crit crit
theintersectionbetweenthebluecurveandtheorangecurve. Forallvaluesofp,theautoencoderin(4)outperformsthe
GaussianMSE(5)(greendashedline)and,hence,itisabletoexploitthestructureinthedata.
Forp>p ,thestaircasebehavioroftheSGDtrainingdynamicsispresentedinFigure16.
crit
42
ESMCompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
SGD
1 p (Proposition 5.2)
0.8 Theoretical prediction
Gaussian performance
pcrit 0.67
0.6
0.4
0.2
0.0
0.2 0.4 0.6 0.8 1.0
p
Figure15.CompressionofsparseRademacherdataviatheautoencoderin(4)withf oftheformin(112).Wesetd=200andr=1.
Left. MSEachievedbySGDatconvergence,asafunctionofthesparsitylevelp. Theempiricalvalues(dots)matchourtheoretical
prediction(blueline).Forp<p ,thelossisgivenbyProposition5.1forBsampledfromtheHaardistribution;forp≥p ,theloss
crit crit
isgivenbyProposition5.2forBequaltotheidentity. Center. EncodermatrixBatconvergenceofSGDwhenp=0.3<p : the
crit
matrixisarandomrotation.Right.EncodermatrixBatconvergenceofSGDwhenp=0.7≥p .Thenegativesigninpartofthe
crit
entriesofBiscancelledbythecorrespondingsignintheentriesofA.Hence,Bisequivalenttoapermutationoftheidentity.
1.4
SGD
Haar encoder (Proposition 5.1)
1.2 Global minimum: 1-p
1.0
0.8
0.6
0.4
0.2
0 20000 40000 60000 80000 100000 120000
Iteration
Figure16.CompressionofsparseRademacherdataviatheautoencoderin(4).Wesetd=200,r=1,andp=0.9.TheMSEisplotted
asafunctionofthenumberofiterationsand,asp>p ,itdisplaysastaircasebehavior.
crit
C.7.Discussiononmulti-layerdecoder
First,letuselaborateonsomedesignpointsforthenetworkin(22). Themergingoperations⊕ and⊕ playtheroleofthe
2 3
correctionterms−(cid:80)t−1β xˆiand−(cid:80)t α zˆiintheRI-GAMPiteratesin(21). Furthermore,thecompositionof⊕
i=1 t,i i=1 t,i 3
andf (·)inxˆ approximatestakingtheposteriormeanin(21). Wenotethatthenetwork(22)canbegeneralizedtoemulate
2 2
moreRI-GAMPiterations,atthecostofadditionallayersandskipconnections(inducedbythemergingoperations⊕ ).
k
Intherestofthisappendix,wediscusshowtoobtaintheorangecurveintherightplotofFigure7,whichcorrespondstothe
Bayes-optimalMSEwhenBissampledfromtheHaardistribution. ThisoptimalMSEisachievedbythefixedpointofthe
VAMPalgorithmproposedin(Ranganetal.,2019). Thus,weimplementthestateevolutionrecursionfrom(Ranganetal.,
2019),inordertoevaluatethefixedpoint.
Asthespecificsettingconsideredhere(x∼SG (p),BaHaarmatrix,andageneralizedlinearmodelwithsignactivation)
d
isnotconsideredin(Ranganetal.,2019),weprovideexplicitexpressionsfortherecursionleadingtothedesiredMSE.
43
ESM
ESMCompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
σ(Bx)
×W
1 x1
f 1(⋅)
x̂1 ⊕ x2 ⊕
f 2(⋅)
x̂2
2 3
×V
1
×W
2
z1̂ ⊕
1
z2̂
g 1(⋅)
Figure17. Blockdiagramofthedecoderin(22).
Firststateevolutionfunction-E (γ ). Westartwiththestateevolutionfunctionthatisequaltothefollowingexpected
1 1
valueofthederivativeoftheconditionalexpectation
(cid:20) (cid:21)
∂
E (γ )=E E[X|R =X+P] , X ∼SG (p), P ∼N(0,γ−1). (113)
1 1 R1 ∂R 1 1 1
1
Forcompleteness,wenotethatthequantity
∂
E[X|R =X+P]
∂R 1
1
isinfacttheconditionalvarianceVar[X|R =X+P]uptoascaling(Dytsoetal.,2020),whichisrelatedtotheoptimal
1
MSE.
Modulothescalings,thecomputationofE[X|R =X+P]issimilartothecomputationperformedinSectionB.4. For
1
brevity,wejuststatethefinalresult:
p· √R1 ·exp(cid:16) − pR 12 (cid:17) · 1
E[X|R =X+P]= 2πp−1 2(pγ 1−1+1) (pγ 1−1+1)3/2 := E(R 1) . (114)
1 p· √ 1 ·exp(cid:16) − pR 12 (cid:17) +(1−p)· √ 1 ·exp(cid:16) − R 12 (cid:17) p(R 1)
2π(p−1+γ 1−1) 2(pγ 1−1+1) 2πγ 1−1 2γ 1−1
TakingthepartialderivativeinR andsubstitutingin(113)yields:
1
(cid:90) ∂ (cid:90) E′(R )p(R )−E(R )p′(R )
E (γ )=γ−1 E[X|R =X+P]·p(R )dR =γ−1 1 1 1 1 p(R )dR
1 1 1 ∂R 1 1 1 1 p2(R ) 1 1
R 1 R 1
(115)
(cid:90) (cid:18) ∂ (cid:19)
=γ−1 E′(R )−E(R )· logp(R ) dR .
1 1 1 ∂R 1 1
R 1
Wecanreadilyverifythat
(cid:12)+ext
(cid:90) (cid:12)
E′(R )dR = lim E(R )(cid:12) =0.
1 1 1 (cid:12)
R ext→∞ (cid:12)
−ext
Anintegrationbypartsfortheremainingtermin(115)gives:
(cid:12)+ext
(cid:12) (cid:90) (cid:90)
E (γ )=γ−1 lim E(R )logp(R )(cid:12) −γ−1 E′(R )logp(R )dR =−γ−1 E′(R )logp(R )dR . (116)
1 1 1 1 1 (cid:12) 1 1 1 1 1 1 1 1
ext→∞ (cid:12) R R
−ext
TheRHSof(116)isthenevaluatedvianumericalintegration. Forcompleteness,thederivativeE′(R )hasthefollowing
1
form:
1 (cid:18) pR2 (cid:19) 1
E′(R )=p· ·exp − 1 ·
1 (cid:112) 2πp−1 2(pγ−1+1) (pγ−1+1)3/2
1 1
R2 (cid:18) pR2 (cid:19) 1
−p2· 1 ·exp − 1 · .
(cid:112) 2πp−1 2(pγ−1+1) (pγ−1+1)5/2
1 1
44CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
Secondstateevolutionfunction-E (τ ,γ ). ThisfunctionisdefinedintermsofspectrumofB⊤B ∈Rd×d. Namely,
2 2 2
forr ≤1,thedistributionoftheeigenvaluesofB⊤Bobeysthefollowinglaw
ρ =r·δ +(1−r)·δ .
S 1 0
ThestateevolutionfunctionE (τ ,γ )isthendefinedasfollows
2 2 2
(cid:20) (cid:21)
1 1 1
E (τ ,γ ):=E =r· +(1−r)· .
2 2 2 S∼ρS τ ·S2+γ τ +γ γ
2 2 2 2 2
Thirdstateevolutionfunction-B (τ ,γ ). Thecomputationissimilartothecaseofthesecondstateevolutionfunction.
2 2 2
Namely,thethirdstateevolutionfunctionisdefinedasfollows:
1 (cid:20) τ S2 (cid:21) 1 τ τ
B (τ ,γ )= ·E 2 = ·r· 2 = 2 .
2 2 2 r S∼ρS τ S2+γ r τ +γ τ +γ
2 2 2 2 2 2
Fourthstateevolutionfunction-B (τ ). ThelaststateevolutionfunctionisdefinedsimilarlytoE (γ ),namely
1 1 1 1
(cid:20) (cid:21)
∂
B (τ )=E E[Z|P ,Y] . (117)
1 1 P1,Y ∂P 1
1
Here,Z ∼ N(0,1)hasvarianceone(sincethespectrumofB hasunitvariance),Y = sign(Z)andP = b·Z +a·G,
1
whereG∼N(0,1)isindependentofZ and
(cid:112)
b=1−τ−1, a= b·(1−b).
1
Theouterexpectationin(117)isestimatedviaMonte-Carlo. Wenowcomputetheconditionalexpectation. Firstnotethat
thefollowingdecomposition(dependingonthesignofY)holds:
E[Z|P ,Y]=E[Z′|P′], (118)
1 1
whereZ′ =1 ·Z andP′ =b·Z′+a·G. UsingBayesformula,wegetthat
ZY≥0 1
(cid:16) (cid:17) (cid:16) (cid:17)
(cid:82) Zexp −Z2 exp −(P1−bZ)2 dZ
E[Z′|P′]= ZY≥0 2 2a2 . (119)
1 (cid:82) exp(cid:0) −Z2(cid:1) exp(cid:16) −(P1−bZ)2(cid:17)
dZ
ZY≥0 2 2a2
Completingthesquareintheexponentsgives
Z2a2+(P −bZ)2 bZ2−2bZP +P2 (Z−P )2 P2
1 = 1 1 = 1 + 1 ,
2a2 2b(1−b) 2(1−b) 2b
whichaftersubstitutionin(119)resultsin
(cid:16) (cid:17)
(cid:82)
Zexp
−(Z−P1)2
dZ
E[Z′|P′]= ZY≥0 2τ 1−1 . (120)
1 (cid:82) exp(cid:16) −(Z−P1)2(cid:17)
dZ
ZY≥0 2τ−1
1
Notethatthedenominatorof(120)iseasytoaccessviathestandardGaussianCDFΨ(·)asfollows
1 (cid:90) (cid:18) (Z−P )2(cid:19) (cid:34) (cid:32) P (cid:33)(cid:35) (cid:32) P (cid:33)
exp − 1 dZ =1 · 1−Ψ − 1 +1 ·Ψ − 1
(cid:113) 2πτ 1−1 ZY≥0 2τ 1−1 Y≥0 τ 1−1/2 Y<0 τ 1−1/2
(121)
(cid:32) (cid:33)
YP
=Ψ 1 ,
τ−1/2
1
45CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
whereforthelastequalityweusethatΨ(x)=1−Ψ(−x)andY ∈{−1,+1}. Forthenumeratorof(120),weget
1 (cid:90) (cid:18) (Z−P )2(cid:19)
1 ·Zexp − 1 dZ. (122)
(cid:113) 2πτ−1 YZ≥0 2τ 1−1
1
LetusdenotethePDFofN(µ,σ2)byρ , andusetheshorthandρ(·)forρ (·). Notethatρ (0) = σ−1ρ(x/σ).
µ,σ2 0,1 x,σ2
Then,byStein’sidentity,wehave
(cid:32) (cid:33)
P
E[1 ·(Z−P )]=τ−1·E[Y ·δ (Z)]=Yτ−1·ρ (0)=Yτ−1/2·ρ 1 ,
YZ≥0 1 1 0 1 P1,τ 1−1 1 τ−1/2
1
astheweakderivativeof1 iswell-definedandequaltoY ·δ (Z). Notingthatsimilarlyto(121)
YZ≥0 0
(cid:32) (cid:33)
YP
E[1 ·P ]=P ·Ψ 1 ,
YZ≥0 1 1 τ−1/2
1
weconcludethat
(cid:32) (cid:33) (cid:32) (cid:33)
YP P
(122)=P ·Ψ 1 +Yτ−1/2·ρ 1 . (123)
1 τ−1/2 1 τ−1/2
1 1
Combiningtheresultsgives
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
E[Z′|P′]=
P 1·Ψ τY 1−P 1/1
2
+Yτ 1−1/2·ρ
τ
1−P 11
/2 =P +Yτ−1/2·
ρ
τ
1−P 11
/2 . (124)
1 1 (cid:18) (cid:19) 1 1 (cid:18) (cid:19)
Ψ YP1 Ψ YP1
τ−1/2 τ−1/2
1 1
ItnowremainstotakethederivativeinP . Wegetthat
1
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)2
√
YP 1 τ 1·ρ τ−P 11 /2 ·Ψ τY −P 1/1 2 +ρ τ−P 11 /2
B (τ )=1− 1 1 1 , (125)
1 1 (cid:18) (cid:19)2
Ψ YP1
τ−1/2
1
whereweusedthatY2 =1andthat ∂ Ψ(x)=ρ(x).
∂x
Stateevolutionrecursion. Atthispoint,wearereadytopresentthestateevolutionrecursion,whichreads
1−E (γ )
γ =γ · 1 1,k ,
2,k 1,k E (γ )
1 1,k
1−B (τ )
τ =τ · 1 1,k ,
2,k 1,k B (τ )
1 1,k
(126)
1−E (τ ,γ ) r·τ
γ =γ · 2 2,k 2,k =γ · 2,k ,
1,k+1 2,k E (τ ,γ ) 2,k (1−r)·τ +γ
2 2,k 2,k 2,k 2,k
1−B (τ ,γ )
τ =τ · 2 2,k 2,k =γ .
1,k+1 2,k B (τ ,γ ) 2,k
2 2,k 2,k
Theinitializationγ andτ canbesettoasmallstrictlypositivenumber. Fortheexperiments,wechoosethevalueof
1,0 1,0
10−6.
MSEfromthestateevolutionparameterγ . TheMSEafterkstepsoftherecursioncanbeaccessedviathefunction
1,k+1
previously computed in (114). Namely, let x ∼ SG (p) and r = x+p, where p has i.i.d. entries with distribution
d 1
N(0,γ−1 ). Define
1,k+1
g(r )=E[x|r =x+p].
1 1
46CompressionofStructuredDatawithAutoencoders:ProvableBenefitofNonlinearitiesandDepth
Bythetowerpropertyoftheconditionalexpectation,weclaimthatthefollowingholds
E[E[X|Y]·X]=E[E[E[X|Y]·X|Y]]=E(cid:2) (E[X|Y])2(cid:3)
,
whereweusethatE[X|Y]ismeasurablew.r.t. Y. Thus,wehavethat
E⟨g(r ),x⟩=d·E(cid:2) (g(r ) )2(cid:3) ,
1 1 1
whereg(r ) denotesthefirstentryofthevectorg(r ). Finally,thedesiredMSEafterkstepsoftherecursionisequalto
1 1 1
d−1·E∥x−g(r )∥2 =1−E(cid:2) (g(r ) )2(cid:3) . (127)
1 2 1 1
Weevaluate(127)forklargeenough,sothattheMSEhasconverged. FortheexperimentinFigure7,weusek =15,asfor
k ≥15theMSEvaluein(127)isstable.
47