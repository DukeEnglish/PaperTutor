Towards Generalizability of Multi-Agent Reinforcement Learning
in Graphs with Recurrent Message Passing
JannisWeil ZhenghuaBao
TechnicalUniversityofDarmstadt TechnicalUniversityofDarmstadt
Darmstadt,Germany Darmstadt,Germany
jannis.weil@tu-darmstadt.de zhenghua.bao@stud.tu-darmstadt.de
OsamaAbboud TobiasMeuser
HuaweiTechnologies TechnicalUniversityofDarmstadt
Munich,Germany Darmstadt,Germany
osama.abboud@huawei.com tobias.meuser@tu-darmstadt.de
ABSTRACT havelimitedaccesstolocalinformationduringexecution.Decen-
Graph-basedenvironmentsposeuniquechallengestomulti-agent tralizedapproachesaremorereactivethancentralizedapproaches,
reinforcementlearning.Indecentralizedapproaches,agentsop- asagentscandirectlyreacttolocalchanges.However,havingonly
eratewithinagivengraphandmakedecisionsbasedonpartial localinformationmayleadtosuboptimaldecisions.Acommonway
oroutdatedobservations.Thesizeoftheobservedneighborhood tocounteractthisistoexpandtheobservationsofeachagentby
limitsthegeneralizabilitytodifferentgraphsandaffectsthereac- informationfromtheirdirectneighborhood[30].Includingmore
tivityofagents,thequalityoftheselectedactions,andthecom- nodesintheobservedneighborhoodimprovesdecisionmaking[5]
municationoverhead.Thisworkfocusesongeneralizabilityand butincreasesthecommunicationoverhead.
resolvesthetrade-offinobservedneighborhoodsizewithacontin- Recentworksshowthatgraphneuralnetworks[34]andneural
uousinformationflowinthewholegraph.Weproposearecurrent messagepassing[11]arewellsuitedforapplicationsingraph-based
message-passingmodelthatiterateswiththeenvironment’ssteps environments,especiallybecausetheycangeneralizetounseen
andallowsnodestocreateaglobalrepresentationofthegraph graphs[33].However,theapproachesoftenassumeacentralized
byexchangingmessageswiththeirneighbors.Agentsreceivethe view[1],explicitcoordinationacrossallagents[2],ortheavailabil-
resultinglearnedgraphobservationsbasedontheirlocationinthe ityoflabeleddatainordertoapplysupervisedlearning[10].
graph.Ourapproachcanbeusedinadecentralizedmanneratrun- Ourgoalistoresolvethetrade-offinlimitedobservedneigh-
timeandincombinationwithareinforcementlearningalgorithm borhoodsinthecontextofdecentralizedmulti-agentsystemsand
ofchoice.Weevaluateourmethodacross1000diversegraphsin graph-basedenvironments.Toachievethis,weproposearecurrent
thecontextofroutingincommunicationnetworksandfindthatit approachinwhichnodesexchangelocalinformationviamessage
enablesagentstogeneralizeandadapttochangesinthegraph. passingtoimprovetheirunderstandingoftheglobalstate.Nodes
refinetheirlocalstatesovertheenvironment’ssteps,allowinginfor-
KEYWORDS mationtoiterativelytravelthroughthewholegraph.Basedonthese
nodestates,agentsreceivelocation-dependentgraphobservations.
Multi-Agent Reinforcement Learning; Graph Neural Networks;
Our approach provides a novel foundation for learning com-
CommunicationNetworks
municationsystemsinmulti-agentreinforcementlearningandis
ACMReferenceFormat: jointlytrainedinanend-to-endfashion.Ourcontributionsare:
JannisWeil,ZhenghuaBao,OsamaAbboud,andTobiasMeuser.2024.To-
• Weproposetodecouplelearninggraph-specificrepresenta-
wardsGeneralizabilityofMulti-AgentReinforcementLearninginGraphs
tionsandcontrolbyseparatingnodeandagentobservations.
withRecurrentMessagePassing.InProc.ofthe23rdInternationalConference
• Tothebestofourknowledge,wearethefirsttoaddressthe
onAutonomousAgentsandMultiagentSystems(AAMAS2024),Auckland,
NewZealand,May6–10,2024,IFAAMAS,11pages. problemoflimitedobservedneighborhoodsingraph-based
environmentswithrecurrentgraphneuralnetworks.
1 INTRODUCTION • Weshowthatthelearnedgraphobservationsenablegener-
alizationover1000diversegraphsinaroutingenvironment,
Thecapabilityofanadaptivesystemdependsonthequalityofits
achievingsimilarthroughputasagentsthatspecializeon
input.Ideally,ithasaccesstothestateandmakesfullyinformedde-
singlegraphswhencombinedwithactionmasking.
cisionsatalltimes.Researchinmulti-agentreinforcementlearning
• We show that our approach enables agents to adapt to a
rangesfromcentralizedtodecentralizedapproaches[22].Ourfocus
changeinthegraphontheflywithoutretraining.
liesondecentralizedsystemsingraph-basedenvironments.While
Our code is available at https://github.com/jw3il/graph-marl.
thecompletestatemaybeleveragedduringtraining,thesesystems
Theremainderofthispaperisstructuredasfollows.Webeginwith
Proc.ofthe23rdInternationalConferenceonAutonomousAgentsandMultiagentSystems theproblemstatementinSec.2andthenintroduceourapproach
(AAMAS2024),N.Alechina,V.Dignum,M.Dastani,J.S.Sichman(eds.),May6–10,2024, inSec.3.WedescribeourevaluationsetupinSec.4,theresultsare
Auckland,NewZealand.©2024InternationalFoundationforAutonomousAgents
presentedanddiscussedinSec.5.ThefollowingSec.6providesan
andMultiagentSystems(www.ifaamas.org).ThisworkislicencedundertheCreative
CommonsAttribution4.0International(CC-BY4.0)licence. overviewofrelatedworkandSec.7concludesthepaper.
4202
beF
7
]AM.sc[
1v72050.2042:viXra2 REINFORCEMENTLEARNINGINGRAPHS 3 LEARNEDGRAPHOBSERVATIONS
Reinforcementlearningingraph-basedenvironmentshasgained Weconsiderenvironmentsthatarebasedonagraphandproposeto
muchpopularityinmanyapplicationdomains[28],includingcom- decouplebothcomponents,i.e.agentsdonothavetokeeptrackof
municationnetworks[22].Weconsidermulti-agentenvironments thewholegraphstateandcanbuilduponalower-levelmechanism
that build upon a graph𝐺 (cid:17) (𝑉,𝐸) ∈ G, representing a com- thataggregatesgraphinformation.Theinformationflowisillus-
municationnetworkofnodes𝑉 connectedviaundirectededges tratedinFig.1andwillbeexplainedinthefollowingsubsections.
𝐸 ⊆𝑉×𝑉.Weassumethattheagents𝐼arepartofapartiallyobserv-
ablestochasticgame[16]thatrequiresthemtoconsiderthegraph, encode node observations send state to neighbors
i.e.state𝑠 𝑡 ∈𝑆atstep𝑡 contains𝐺 andmayaugmentitwithstate into state and aggregate received states
informationthatcharacterizesthenetwork.Examplesincludeedge 1 2
delaysandcomputeresourcesofnodes.Eachagent𝑖 ∈𝐼 receives update local state to
partialobservations𝑜 𝑡𝑖 ∼𝑂𝑖(·|𝑠 𝑡)andselectsanaction𝑎𝑖
𝑡
∈𝐴using with received states
itspolicy𝑎𝑖
𝑡
∼𝜋𝑖(·|𝑜 𝑡𝑖).Theagent’sgoalistomaximizeitsexpected 3
d anis dco tu imnt eed hore rit zu orn n𝑇E ,(cid:2)(cid:205) w𝑇 𝑡 h′ e= r𝑡 e𝛾𝑡 th′− e𝑡𝑅 in𝑡𝑖 d′(cid:3) ivw idit uh ald ris ec wou arn dt sfa 𝑅c 𝑡𝑖to =r𝛾 𝑅𝑖∈
(𝑠
𝑡[0 ,𝑎,1 𝑡]
) local agent
a o cf rft
e
e s atr te ep ls
o
r 2 ce ap a lne gt di
r
t a3io p:n hs
arebasedonthejointactionofallagents𝑎 𝑡=(𝑎𝑖 𝑡)𝑖∈𝐼. observation 4 observation
Howagentsobservethegraphandthenetwork’sstateisusually for agent
notdiscussedindepthbyrelatedworks.Acentralizedviewallows
Node
forthebestdecisionmakingbutdoesnotscaletobiggergraphs.
Adecentralizedviewtradesoffreactivityandamountofavailable inter-agent Agent
informationwiththesizeoftheobservedneighborhood.Authors Action communication
usually decide for one of these views and their approaches are
therefore,bydesign,limitedtocertainproblemsorgraphstructures.
Consideradecentralizedapproachwhereanagentislocatedon Figure1:Ourgraphobservationmechanismiterativelydis-
anodeandobservesits𝑛-hopneighborhood.Dependingonthe tributesnodestatesviamessagepassing.Agentsinthegraph
environment,theassumptionabouttheobservableneighborhood receivelocalgraphobservationsfordecisionmaking.
iscriticalwithrespecttogeneralizability.Let’simaginetheagent
issupposedtofindtheshortestpathtosomedestinationnodein 3.1 RecurrentMessagePassing
thegraphthatis𝑛+1hopsaway.Howcanitidentifytheshortest
Thecoreideaofourgraphobservationsisleveragingthemessage-
pathtoadestinationnodethat’snotincludedinitsobservation?
passing framework of graph neural networks [13] to distribute
Therearetwostraight-forwardsolutions:
localinformationinthenetwork.Relatedapproachesareusually
(1) Specializationonasinglegraph.Agentsexplorethewhole used in a centralized manner with a global view [3]. However,
graphduringtraining.Ifthegraphisfixedandnodesare recurrentaggregationfunctions[23,37]spreadmultiplemessage
uniquelyidentifiablebasedontheagent’sobservation,agents passingiterationsovertimeandenabledecentralization[10].We
canspecializeandfindtheoptimalpathtoanynode. introduceasecondrecurrentloopbacktotheinputofthegraph
(2) Expansionoftheobservationspacetoincludethemissing neuralnetworkandlabelthisapproachrecurrentmessagepassing.
information,e.g.to(𝑛+1)-hopneighborhoods. Weassumethateachnode𝑣 ∈𝑉 receiveslocalnodeobservations
Withspecialization,thelearnedsolutionwillnotgeneralizeto
𝑚𝑣 ∼𝑀𝑣(· |𝑠)basedonanunknownsystemstate𝑠 ∈𝑆.Instead
othergraphs.Ifthetargetgraphdoesnotchange,thiswouldbea ofdirectlyusingthisasaninputofagraphneuralnetwork,each
sufficientbuthighlyinflexiblesolution.Asrealnetworkshavedi- node𝑣 ∈𝑉 embedsitslocalobservationintoitscurrentnodestate
verseunderlyinggraphsandareusuallydynamic,manyresearchers
ℎ𝑣
withanarbitrarydifferentiablefunctionencode:
consideronlinetrainingonthetargetgraph.However,asreinforce- ℎ𝑣 (cid:17)encode(ℎ𝑣,𝑚 𝑣). (1)
0
ment learning requires agents to explore and make suboptimal Thenodestateℎ𝑣
isinitializedtozerointhefirststepandwill
decisions,onlinetrainingfromscratchmightbeunacceptablein
serveasarecurrentloopbetweensubsequentenvironmentsteps.
practice.Incontrast,expandingtheobservationspaceallowsagents
Thisallowsnodestoconsiderpreviouslyaggregatedinformation
togeneralize.However,theobservationrangetoconsidergreatly
when processing observations, similar to auto-regressive graph
dependsontheconcreteproblem.Forexample,forrouting,global
models[31]thatusepredictionsofpreviousstepsastheirinput.
knowledgeisnecessaryifanynodecouldbethedestination.But Atiteration𝑘 ≥ 0,eachnodesendstheirstateℎ𝑣 toalldirect
thentheapproachisnotdecentralizedanymoreandwillnotscale. 𝑘
neighbors𝑤 ∈𝑁(𝑣) (cid:17){𝑤 | (𝑣,𝑤) ∈𝐸}.Eachneighborgenerates
Ourideaistoaddressthisissuebyexpandingtheobservation
anewstatebyfirstaggregatingincomingnodestatesandthen
spacewithlearnedgraphobservationsthatleveragerecurrentmes-
𝑘
updatingitsstateusingarbitrarydifferentiablefunctionsaggregate
sagepassing.Agentsarestillreactiveanddon’thavetogatherin-
𝑘
andupdate .Onemessagepassingiterationisdefinedas:
formationaboutthewholegraphbeforemakingadecision.While
initialdecisionsmaybesuboptimal,thequalityofthelearnedgraph 𝑀 𝑘𝑣 (cid:17)aggregate𝑘 ((ℎ 𝑘𝑤 )𝑤∈𝑁(𝑣)) (2)
observationsshouldincreaseovertime.Ideally,theyconvergetoa
ℎ𝑣 (cid:17)update𝑘 (ℎ𝑣,𝑀𝑣 ). (3)
globalviewandallowagentstomakeoptimaldecisions. 𝑘+1 𝑘 𝑘
tnemnorivnE
hparG
stnegAAlgorithm1DistributedNodeStateUpdate
(1) (2) (3)
Input: Node𝑣 withneighbors𝑁(𝑣),state𝑠,previousnodestate
ℎ𝑣 ,nodeobservation𝑚𝑣
Output:
Updatednodestateℎ𝑣 andintermediatestates𝐻𝑣
𝐾
1:
ℎ𝑣 ←encode(ℎ𝑣,𝑚𝑣) ⊲Encodenodeobservation
0
2: for𝑘 ←0to𝐾−1do ⊲Updatewithmessagepassing
3:
Sendℎ 𝑘𝑣 toallneighbors𝑤 ∈𝑁(𝑣)
4:
Receiveℎ 𝑘𝑤 fromallneighbors𝑤 ∈𝑁(𝑣)
5:
𝑀 𝑘𝑣 ←aggregate𝑘((ℎ 𝑘𝑤)𝑤∈𝑁(𝑣))
6:
ℎ 𝑘𝑣 +1←update𝑘 (ℎ 𝑘𝑣,𝑀 𝑘𝑣)
7:
𝐻𝑣 ←(ℎ 𝑘𝑣)𝑘 ∥(ℎ 𝑘𝑤)𝑤∈𝑁(𝑣),𝑘 ⊲Getallintermediatestates
8:
returnℎ 𝐾𝑣,𝐻𝑣 Figure 2: Our recurrent message passing model leverages
LSTMcellstoencodethenodeobservationandupdatethe
nodestate.Thehiddenstatesofneighbornodesareaggre-
gatedviasummation,cellstatesremainlocaltoeachnode.
Alg.1showspseudocodeforrecurrentmessagepassingthatis
executedbyallnodes𝑣 ∈𝑉.Thereare𝐾 ∈N iterationsbetween
oftheLSTMmodulesarenotseparated.Instead,asinglepairof
stepsintheenvironment,i.e.equations(2)and(3)arerepeatedly
applieduntil𝑘+1=𝐾.Thefinalaggregateℎ𝑣
andallintermediate
hiddenandcellstatesispassedonbetweenthemodules.
𝐾
nodestates𝐻𝑣 receivedandcalculatedby𝑣 canthenbeusedby
3.3 IntegrationinDeepRL
agents,aswewilldetaillater.Inthenextenvironmentstep,weset
ℎ𝑣 =ℎ𝑣 andrepeatthealgorithm.Thenumberofiterationscanbe Graphobservationsarecompatiblewithalldeepreinforcement
𝐾
adjustedtofittherequirementsofthelearningtask.Anincreased learningapproachesthatallowforbackpropagationthroughthe
numberofiterationsperstepcausesinformationtotraversethe policy’sinput,i.e.theobservationspace.Tothebestofourknowl-
networkfasterbutalsoincreasesthecommunicationoverhead. edge,mostalgorithmsdonothaveanylimitationsinthatregard,
Thisnodestateupdateisperformedateachstepintheenvi- asthepolicyisusuallybasedonadifferentiablefunction.
ronment.Weassumethateachagent𝑖 ∈ 𝐼 isassignedtoexactly InordertointegrateourmethodwithdeepRLalgorithms,the
onenode𝑣𝑖 ∈𝑉 ateachstep.Inadditiontoitsobservationinthe nodestateupdatehastobeperformedateachenvironmentstep
environment,agent𝑖 ∈𝐼 thenreceivesalocalgraphobservation duringinferenceandtraining,resultinginanexpandedobservation
𝜓𝑖 ofthenodeitisassignedtobasedontheinformation𝐻𝑣𝑖 this spacefortheagents.Theintegrationatinferencetimecanbedone
nodereceivedinthisstepviaadifferentiablereadoutfunctionΨ: withasimpleenvironmentwrapper.Dependingontheconsidered
algorithm,theintegrationintothetrainingloopwillrequireaddi-
𝜓𝑖 (cid:17)Ψ(𝐻𝑣𝑖 ). (4) tionaleffort.Forexample,algorithmsbasedonQ-learningbootstrap
thevaluetargetusingtheobservationsoffuturestates.Inorder
Inthesimplestcase,Ψcouldbetheidentityfunctionofthelatest
tocomputethecorrespondinggraphobservations,wetherefore
nodestateΨ(𝐻𝑣𝑖 ) (cid:17)ℎ 𝐾𝑣𝑖 ineachiteration,i.e.agentsreceivethe havetocomputeorsamplenodestatesforthesefuturesteps.Ad-
currentstateofthenodetowhichtheyareassigned.Accesstointer- ditionally,asnodestatesareupdatedovermultipleenvironment
mediatenodestatesisnecessarytoallowforskipconnectionsand steps,wehavetounrollthenodestateupdateoverasequenceof
aggregationmechanismslikejumpingknowledgenetworks[41]. stepsandapplybackpropagationthroughtimeinordertolearn
stableupdatefunctions.Thisisalreadyincludedinalgorithmsthat
3.2 ModelArchitecture considerstatefulagentswithrecurrentmodels[20],butwillrequire
Basedonthedesignfromtheprevioussection,weproposeasim- adjustmentsforotherreinforcementlearningalgorithms.
plerecurrentmessagepassingarchitecture.Theencodefunction
3.4 ExemplaryIntegrationinDeepQ-Learning
isrepresentedbyafullyconnectednetworktoembedthenode
observationandanLSTM[17]toupdatethepreviousnodestate Inthissection,weexemplarydescribehowtointegrateourmethod
withthenewembedding.Theaggregatefunctionisthesumofall intoindependentDQN[27]withparametersharingacrossagents.
neighbors’hiddenstates,butcouldbeanygraphconvolutionfrom OurapproachissummarizedinAlg.2,noteworthychangestothe
relatedwork.Finally,updateismodeledbyanotherLSTM.Weshare originalalgorithmarehighlightedinlightgray.Themaindifference
parametersforalliterations,i.e.∀𝑘.update𝑘 =update.Weprovide liesintheintroductionofnodestatesℎ 𝑡 thatareupdatedinparallel
anoverviewofthearchitecturewithFig.2.AnLSTMcelltakesan totheenvironmentstepsbasedonthenodeobservations𝑚 𝑡 and
inputtensorandapairofhiddenandcellstatetensors(ℎ,𝑐)and thepreviousnodestates.Fornotationalsimplicity,wedenotere-
yieldsnewhiddenandcellstates.Inourarchitecture,thehidden
currentmessagepassingcombinedwiththereadoutfunctionΨas
stateisexchangedwithneighbornodesduringaggregation,the adifferentiablefunction𝑈(ℎ 𝑡,𝑚 𝑡,𝑠 𝑡;𝜃 𝑈)parameterizedby𝜃 𝑈 (see
cellstateremainslocaltothenode.Theinnerloopfromupdateto line9).Itreturnsthenextnodestateofallnodesℎ 𝑡+1 (cid:17) (ℎ 𝑡𝑣 +1)𝑣∈𝑉
aggregatedepictsiterationswithinastep,theouterlooprepresents andthegraphobservationsofallagents𝜓 𝑡 (cid:17) (𝜓 𝑡𝑖)𝑖∈𝐼 basedon
theforwardingofstatesbetweenenvironmentsteps.Thestates thenodestatesofallnodesℎ 𝑡,allnodeobservations𝑚 𝑡 andthe
CF
A
MTSL
B
MTSLstate𝑠 𝑡.Theonlyinformationrequiredfrom𝑠 𝑡 arethegraphand 4.1 ModelsandTrainingAlgorithms
themappingofagentstonodes.Thegraphobservation𝜓 𝑡𝑖 ofagent Our design consist of two parts, a model that generates graph
𝑖dependsonitspositioninthegraphandisconcatenatedwiththe
observationsandareinforcementlearningagent.
agent’sobservation𝑜 𝑡𝑖 receivedfromtheenvironment(seeline12).
𝑄ˆ and𝑈ˆ denotethattherespectivegradientcalculationisdisabled. GraphObservations. Thecoreofthegraphobservationsisthe
messagepassingframework,asdescribedinSec.3.1.Anygraph
Duringtraining,wesampleasequenceoftransitionsfromthere-
neuralnetworkcanbeusedtogeneratesuchgraphobservations.
playmemoryandperformbackpropagationthroughtimeanalogous
WeuseourproposedarchitecturefromSec.3.2andconsiderthree
tothestoredstatemethodfromrecurrentexperiencereplay[20].
Theinitialnodestateℎ′ 𝑗 isloadedfromthereplaymemoryand baselinegraphneuralnetworkarchitecturesfromrelatedworkwith
0 implementationsbyPyTorchGeometric[9]andPyTorchGeometric
subsequentnodestatesinthesequencearerecomputed(seelines18
Temporal[32].Twoarchitecturesarefeed-forwardgraphneural
and19).TheQ-learningtargetrequiresgraphobservationsforthe
networkswithoutrecurrency.GraphSAGE[14]isaGNNwithmul-
nextstep,whichwecomputetemporarily.Wethenaggregatethe
tiplegraphconvolutionallayersthatuseindividualparameters.
squarederroroverallstepsinthesequence(seeline22)andper-
formgradientdescentwithrespecttotheagent’sparameters𝜃
𝑄
A-DGN[12]aimstoimprovelearninglong-rangedependencies
andtheparametersofthemessagepassingmodule𝜃 𝑈. withanaddeddiffusiontermandperformsmultipleiterationswith
thesameparameters.Asarecurrentbaseline,GCRN-LSTM[37]
combinesanLSTMwithChebyshevspectralgraphconvolutions[7].
Algorithm2IndependentDQNwithLearnedGraphObservations
Whileourarchitectureusesasinglesumtoaggregatehiddenstates,
1: Initializereplaymemory𝐷 GCRN-LSTMutilizes8Chebyshevconvolutionstoaggregateinter-
2: Initializeaction-valuefunction𝑄withweights𝜃 𝑄 mediatecomputationsofanLSTMcell.
3:
Initializetargetweights𝜃ˆ
𝑄
WedefinethereadoutfunctionΨofaggregatednodestateup-
4: Initializenodestateupdatefunction𝑈 withweights𝜃 𝑈
dateinformation𝐻𝑣 (seeSec.3.1)tographobservations𝜓𝑖
asa
5: forepisode←0... do concatenationofthecurrentnodestateℎ 𝐾𝑣 andthelastnodestates
6: ℎ 0←0 ⊲Initializenodestates ℎ 𝐾𝑤 −1 thatthisnodereceivedfromitsneighbors.Thisservesas
7: Obtain𝑠 0,𝑜 0, and𝑚 0byresettingtheenvironment askipconnectionoverthelastiteration.Notethatnoadditional
8: for𝑡 ←0to𝑇 −1do messageexchangeisnecessaryforthisskipconnection.Weapply
9: ℎ 𝑡+1,𝜓 𝑡 ←𝑈ˆ(ℎ 𝑡,𝑚 𝑡,𝑠 𝑡;𝜃 𝑈) ⊲Nodestateupdate thesamereadoutfunctiontoallgraphobservationmethods.
10: for𝑖 ∈𝐼 do Agents. We consider independent DQN [27], recurrent DQN
11:
Selectrandomaction𝑎𝑖
𝑡
withprobability𝜖
(DQNR)[20],CommNet[38]andDGN1[19].Webuildupontheim-
12: otherwiseselectaction plementationofDGN2andreimplementtheremainingapproaches.
𝑎𝑖
𝑡
=argmax𝑎𝑄ˆ(𝑜 𝑡𝑖 ∥𝜓 𝑡𝑖,𝑎;𝜃 𝑄)
Allvariantssharethesametrainingsetupbutdifferintheagent’sar-
13: Performenvironmentstepwithactions𝑎 𝑡 andget chitecture.DQNisafeed-forwardnetworkwithfully-connectedlay-
reward𝑟 𝑡,state𝑠 𝑡+1,obs𝑜 𝑡+1,nodeobs𝑚 𝑡+1 ersthatistrainedwithaQ-learningloss.DQNRaddsanLSTM[17]
14: Store(ℎ 𝑡,ℎ 𝑡+1,𝑚 𝑡,𝑚 𝑡+1,𝑠 𝑡,𝑠 𝑡+1,𝑜 𝑡,𝑎 𝑡,𝑟 𝑡,𝑜 𝑡+1)in𝐷 layer and is trained on sequences. Both approaches do not fea-
15: Initializeloss𝐿←0 tureanyinformationexchangebetweenagents,theirpoliciesare
16: forbatchsequenceindicesin𝐷j← 𝑗 0to𝑗 0+(𝐽 −1)do completelyseparatedduringexecution.CommNetextendsDQNR
17: if 𝑗 = 𝑗 0then withtwocommunicationroundswhereagentsexchangetheirhid-
18: ℎ′ 𝑗 ←ℎ 𝑗 ⊲Loadnodestatefromreplaymemory denstatesbeforeselectinganaction.DGNextendsDQNwithtwo
19:
ℎ′ 𝑗+1,𝜓 𝑗′ ←𝑈(ℎ′ 𝑗,𝑚 𝑗,𝑠 𝑗;𝜃 𝑈)⊲Trainnodestateupdate c lao rm izm atu ion nic ta et ri mon tr oou thn eds lou ss si .n Wg is te hl if n-a ott ne ent ci oo mn m[3 u9 n] ia cn ad tioa ndd rs oa unre dg ou f-
20: ℎ′ 𝑗′ +2,𝜓 𝑗′′ +1←𝑈ˆ(ℎ′ 𝑗+1,𝑚 𝑗+1,𝑠 𝑗+1;𝜃 𝑈) ⊲Targetinput CommNetandDGN,agentscommunicatewithotheragentsthat
21: 𝑦 𝑗 ←𝑟 𝑗 +Z𝑗𝛾max𝑎𝑄ˆ(𝑜 𝑗+1∥𝜓 𝑗′′ +1,𝑎;𝜃ˆ 𝑄) resideonthesamenodeoronanodeintheirdirectneighborhood.
(cid:40)
withZ𝑖
𝑗
= 0 ifagent𝑖isdoneatstep𝑗+1 4.2 GraphGenerationandOverview
1 otherwise
Weextendthegraphgenerationusedintheroutingenvironment
22: 𝐿←𝐿+(𝑦 𝑗 −𝑄(𝑜 𝑗 ∥𝜓 𝑗′,𝑎 𝑗;𝜃 𝑄))2 fromJiangetal.[19].Itplaces𝐿nodesrandomlyona2Dplane
23: Performgradientdescenton𝐿withrespect andthenconnectsclosenodeswithedgesuntilallnodesreach
toparameters𝜃 𝑄 and𝜃 𝑈 degree𝐷.Havingafixednodedegreeisnotamandatoryconstraint
24: Updatetargetweights𝜃ˆ 𝑄 forourapproach,butresultsinadiscreteactionspaceoffixed
sizethatsimplifiesreinforcementlearning.Technically,thiscanbe
extendedtographswithnodesofvariabledegrees,e.g.viaaction
4 EXPERIMENTSETUP masking[36].Thedelayofanedgeinstepsisdeterminedbyalinear
functionofthedistancebetweentheconnectednodes,roundedto
Weevaluateourapproachindiversegraphsbasedonarouting
thenextinteger.Disconnectedgraphsarefilteredout.
environment.Thefollowingsectionsdescribeconsideredmodels,
algorithmsandgraphswithgreaterdetail.Thenwebrieflydescribe 1NottobeconfusedwiththegraphneuralnetworkA-DGN.
theroutingenvironmentandasimplifiedsupervisedlearningtask. 2https://github.com/PKU-RL/DGN/,includingthePyTorchversion.𝐺 𝐴 (0.02,0.19, 0.11) 𝐺 𝐵 (0.00,0.53,0.23) 𝐺 𝐶 (0.00,0.70,0.16)
0.7 0.7 0.70.7 0.7
4.0
11
11
3 21 1 2 3 3 21
21
2
122 2 2 2 1 000 ... 2 456 2 1
1 1
2
112121121 222 22
1 1
000 2... 456
4
2
411 1111 12 11 11 2111 1111 000 ... 456 000 ... 456 233 ... 505 00 .. 56
2.0 0.4
2 2 0.3 2 0.3 0.30.3
5 2 222 2 12 2 2 2 1 00 .. 12 11 2 12 2 2 221 5 2615 2 2 3001 .. 22 2 212 21 22 31 22 2 221 2 00 .. 12 00 .. 12 011 ... 505
2 3 4 5
00 .. 23
0.0 0.0 0.00.0 Throughputwithoutbandwidthlimitation
(a)Exemplarytestgraphs𝐺 𝐴,𝐺 𝐵,and𝐺 𝐶 withincreasingmaximumbetweennesscentrality. (b)Meanthroughputon100episodesfor
Thesuffixindicatesthe(min,max,mean)betweennesscentralityintherespectivegraph. alltestgraphs.Eachdotrepresentsagraph.
Figure3:Overviewoftheconsideredgraphswith(a)threeexemplarygraphsfromthetestsetand(b)themeanthroughputof
shortestpathsroutingwithandwithoutbandwidthlimitationinall1000testgraphs.
Wegenerate1000distinctgraphsfortestingwith𝐿 (cid:17)20and𝐷 (cid:17) ofsize𝑔istransmittedviaaselectededgeifthecumulativesizeofall
3.Themeandiameteris7.21±1.42hopsand12.84±2.72steps.The packetsthatarecurrentlytransmittedviathisedgeissmallerthan
meanall-pairsshortestpaths(APSP)lengthsare3.26±1.92hopsand 1−𝑔.Thepacketthentraversestheedgeaccordingtothenumber
5.7±3.6steps.ThemaximumAPSPlengthsequalthemaxdiameters ofstepsinitstransmissiondelay.Otherwise,thepacketisforcedto
of12hopsand23steps.Thebetweennesscentralityin [0, 1] of stayatitscurrentpositionandreceivesapenaltyof−0.2.Anagent’s
anodereflectstheproportionofshortestpathsbetweenanytwo localobservationsincludeitscurrentposition,itsdestinationand
nodesinthegraphthatcontainthisnode.Inrouting,highvalues packetsize.Foreachoutgoingedgeoftheircurrentnode,itobserves
indicatepotentialbottlenecksinthegraph.Weshowexemplary thedelay,thecumulativesizeofpacketsonthatedgeandthere-
graphswithincreasingmaximumbetweennesscentralityinFig.3a, spectiveneighbor’snodeid.Anodeobservesitsownid,thenumber
where the nodes are repositioned to provide a better overview. andsizeofpacketsthatresideonthenode,andlocalinformation
Graph𝐺 𝐴 with a low maximum betweenness centrality is well aboutoutgoingedges.Allnodeidsaregivenasone-hotencodings.
balanced.Graph𝐺 𝐵hasahighmeanbetweennesscentralitydueto Thethroughputreferstothenumberofpacketsperstepthatar-
itsline-likestructure.Graph𝐺 𝐶 hasahighmaximumbetweenness riveattheirdestination.Delaydescribesthelengthoftheirepisodes.
centralitybecauseofthebottlenecknodeinthecenter. Notethatthedelayshouldneverbeconsideredonitsown.Forex-
Apartfromthenodeconnectivityandpotentialbottlenecks,the ample,agentsthatonlyroutetodestinationsintheir1-hopneigh-
diameterofthegraphsandthedistributionoftheshortestpathsare borhoodwouldachievelowdelaysbutalsoalowthroughput.
expectedtoinfluenceourapproach.Inthegraphneuralnetwork Asabaseline,weconsiderheuristicagentswithaglobalview
architectureconsideredinthispaper,messagesonlytraversethe thatalwayschoosetheshortestpathswithrespecttotheedgede-
graphthroughitsedges.Thenumberofiterationsforinformation lays.Fig.3bshowsthethroughputwithandwithoutbandwidth
fromnode𝑣 1tobeforwardedtonode𝑣 2equalsthelengthofthe limitationswhenusingthisheuristicfor100episodesinalltest
shortestpathbetweenthesenodes.Theminimumnumberofitera- graphs.Eachdotiscoloredaccordingtothemaximumbetweenness
tionsrequiredtocollectinformationfromallnodesisthereforethe centralityoftherespectivegraph.Wecanseethatbandwidthlimi-
diameterofthegraph.Whilethemaximumdiameteris12hops,we tationscauseasignificantdropinthroughputandthatgraphswith
foundthatover99%oftheshortestpathsinalltestgraphshaveat highmaximumbetweennesstendtoresultinlowerthroughput.
most8hops.Furtherdetailsareprovidedintheappendix.
4.4 ShortestPathsRegressionTask
4.3 RoutingEnvironment
Theroutingenvironmentrequiresagentstolearnpathsfromsource
WeextendtheroutingenvironmentfromJiangetal.[19]andfix todestinationnodes.Toquicklyevaluatetheefficacyofdifferent
abugthatcausedpacketstoskipedges.Atalltimes,thereare𝑁 graphneuralnetworkarchitectures,wedesignamulti-targetre-
packetsofrandomsizesin[0,1)thathavetoberoutedfromrandom gressionproblemasasimplificationoftheroutingenvironment.
sourcetodestinationnodesinagivengraph.Wefocusongeneral- Weexpectthattheperformanceofdifferentarchitecturesinthis
izabilityacrossgraphsanduse𝑁 (cid:17)20packetsinourexperiments. taskwillindicatetheirsuitabilityfortheroutingenvironment.The
Eachpacketisanagentthatreceivesarewardof10whenitreaches training dataset contains node observations for 100000 graphs
itsdestination.Onanode,agentsselectoneof1+𝐷discreteactions generatedbyresettingtheroutingenvironment.Weexclude1000
thatcorrespondtowaitingandchoosinganoutgoingedge.Each ofthesegraphsforvalidation.Thetargetsforeachnodearethe
edgehasatransmissiondelaygiveninsteps.Weconsidertwoenvi- shortestpathlengthstoallothernodes.Forthetestdataset,weuse
ronmentmodes.Thefirstmodehasnorestrictionsandpacketsare thenodeobservationsandtargetsofthe1000graphsfromSec.4.2.
alwaystransmittedviatheirselectededges.Inthesecondmode,we Thelossisthemeansquarederrorbetweenthepredictedandreal
takepacketsizesandlimitededgecapacitiesintoaccount.Apacket distancesforeachsourceanddestinationnode.
ytilartnecssenneewteB ytilartnecssenneewteB ytilartnecssenneewteB ytilartnecssenneewteB
noitatimilhtdiwdnabhtiwtuphguorhT
ytilartnecssenneewtebxaM5 RESULTS Table1:Resultsfortheshortestpathsregressionproblem.𝐾
denotesthenumberofmessagepassingiterationsand𝐽 the
Wefirstpresentindependentresultsforourtwocorecomponents,
unrolldepthforrecurrentapproaches.ShownistheMSEon
thegraphneuralnetworkarchitectures(seeSec.5.1)andagents
alltesttopologiesafter𝑡 forwardstepswith𝐾 iterations.All
trainedintheroutingenvironmentwithsinglegraphs(seeSec.5.2).
resultsareaveragedoverthreeseeds.
Section5.3combinesbothcomponentsandprovidestheresultsfor
generalizedrouting,followedbyadiscussioninSec.5.4.
WeusetheAdamWoptimizer[25]forallexperiments.Details Architecture 𝐾 𝐽 MSEatForwardStep𝑡
regardingthehyperparametersareprovidedintheappendix. 1 2 4 8 16 32
8 - 1.16(allseeds:1.14,1.22,1.13)
GraphSAGE
16 - 3.57(allseeds:4.18,3.46,3.06)
5.1 ShortestPathsRegression
8 - 1.50(allseeds:1.49,1.56,1.46)
Wefirstevaluatetheconsideredgraphneuralnetworkarchitectures A-DGN
16 - 1.18(allseeds:1.16,1.20,1.18)
intheshortestpathsregressiontask(seeSec.4.4).Wetraineach
1 8 4.98 2.98 1.12 0.60 1.61 4.27
architecturewiththreeseedsfor50000iterationsofbatchsize32.
1 16 5.03 3.09 1.28 0.60 0.53 1.08
BasedontheobservationsregardingtheAPSPdistributionfrom GCRN-LSTM
2 8 3.17 1.18 0.47 0.38 0.50 0.94
Sec.4.2,𝐾 =8messagepassingiterationsallowtopassinformation
4 8 3.18 1.22 0.49 0.40 0.51 1.08
betweenover99%ofallnodepairsinthetestgraphs.Therefore,we
hypothesizethat𝐾 =8shouldleadtogoodperformanceonthetest 1 8 4.98 2.91 0.94 0.43 0.99 3.75
1 16 5.02 2.98 1.02 0.39 0.37 0.46
graphsforthenon-recurrentmodels.Theresultsfordifferentmes- Ours
2 8 3.02 1.07 0.39 0.35 0.57 1.78
sagepassingiterations𝐾 andunrolldepths𝐽 areshowninTab.1. 4 8 1.26 0.48 0.34 0.34 0.42 0.81
In GraphSAGE,𝐾 refers to the number of graph convolutional
layers.ForGCRN-LSTM,wesetthefiltersizeoftheChebyshev
convolutionsto𝐾+1.Bothresultin𝐾 messagepassingiterations.
GraphSAGE(K=8)
Allapproacheslearntoapproximatetheshortestpathlengthsand A-DGN(K=8)
10
achieveameansquarederror(MSE)ofaroundorbelow1forat GCRN-LSTM(K=1,J=8)
leastoneconfiguration.InthecaseofGraphSAGE,increasingthe Ours(K=1,J=8)
numberoflayersto16leadstounstabletrainingandahightest
5
lossinthistask.Asthenon-recurrentarchitecturesarestateless,
theyyieldthesameresultsateachforwardstep𝑡.
Fortherecurrentapproaches,wewanttousealownumberof 0
iterationsperstep(i.e.𝐾 =1)toreducethecommunicationover- 0 10000 20000 30000 40000 50000
Trainingiteration
head.Weevaluatethemwithdifferentunrolldepths𝐽 andhigher
valuesof𝐾 forcomparison.Asexpected,therecurrentapproaches
performpoorlyinthefirstforwardstepwith𝑡 = 1.Theyrefine Figure4:ValidationlossoftheselectedGNNarchitectures
intheshortestpathsproblemduringtraining.Theshaded
theirhiddenstatesinsubsequentstepsandapproximatelyreach
theirminimumlossesattheunrolldepth𝐽 thatwasusedduring areashowsthestandarddeviationoverthreemodels.
training.Afterwards,wecanseethattheirlossesincreaseagain.
5.2 RoutinginSingleGraphs
Increasingtheunrolldepthimproveslong-termstability,butleads
toincreasedtrainingtime.Ahighernumberofiterations𝐾 pre- Beforeweevaluateourmethodonmultiplegraphs,wetrainagents
dominantlyleadstoimprovedpredictions,atthecostofincreasing withoutgraphobservationsintheroutingenvironmentusingsingle
thecommunicationoverheadperstep. graphs.Theagentsaretrainedfor250000totalstepswith24000
Forourexperimentsincombinationwithreinforcementlearn- iterationsofbatchsize32.Episodesaretruncatedafter300steps.
ing,weselect𝐾 =8forthenon-recurrentmodelsand𝐾 =1, 𝐽 =8 InTab.2,weshowtheresultsfortheoutliergraphfromFig.3bat
fortherecurrentmodels.Thisisacompromisebetweenperfor- around(4.5, 2.0).Thetophalfshowstheresultswithoutbandwidth
mance,stabilityandcommunicationoverhead.Fig.4showsthe limitations,thebottomhalfwithbandwidthlimitations.Without
validationlossoftheselectedapproachesduringtraining.There- limitations,allmethodslearntheoptimalshortestpaths.Thisisnot
currentarchitecturesconvergefastertoalowlossvaluethanthe surprising,asthegraphisstaticandagentscanlocatethemselves
non-recurrentones.Thiscouldpossiblybecausedbybettergra- inthegraphusingthenodeidtheyreceiveintheirobservation.
dients,aswecomputeseparatelossesforeachmessagepassing Theagentsspecializeonthegraph.Withbandwidthlimitations,the
iterationwhenunrollingthenetwork.Bothrecurrentapproaches throughputdrasticallydecreasesandthedelayincreases.Wecan
achievesimilarvalidationlosses,althoughourarchitectureissim- seethatinthisgraph,alllearningapproachesareabletooutperform
plerandexchangeslessinformationduringtheforwardsteps.The theshortestpathsolutionintermsofmeanrewardandthroughput.
highstandarddeviationofGCRN-LSTMinthebeginningiscaused Except for DQN, the delay of arrived packets is slightly higher.
byoneofthethreeruns,wherethevalidationlossdoesnotde- Surprisingly,theeffectofcommunicationisverysmall.Usingthe
creaseinitially.Inthereinforcementlearningsetting,weexpect sametrainingsetupforallagentarchitectures,wecannotreproduce
recurrentexperiencereplaywithstoredstatestofurtherimprove theresultsfromJiangetal.[19]inthisparticulargraphandfind
thelong-termstabilityoftherecurrentapproaches. thattheperformanceofDQNisveryclosetoDGN.
ssolnoitadilaVTable 2: Results for routing in a selected graph, averaged Table4:Resultsforroutingin1000testgraphsfor300steps
over1000episodesand3models.Thelearningapproaches usingDQNwithgraphobservationsprovidedbythelisted
outperformtheshortestpathsheuristic. methods.Anasterisk(*)indicatesactionmaskingattesttime.
Mode Agent Metrics Mode Method Metrics
Reward Delay Throughput Reward Delay Throughput
ShortestPath 2.26±0.0 4.39±0.0 4.52±0.0 ShortestPath 1.77±0.00 5.59±0.00 3.54±0.00
DQN 2.26±0.0 4.39±0.0 4.52±0.0 GraphSAGE 0.02±0.00 4.49±0.35 0.04±0.00
DQNR 2.26±0.0 4.39±0.0 4.52±0.0 A-DGN 1.15±0.02 5.72±0.02 2.29±0.04
CommNet 2.26±0.0 4.39±0.0 4.52±0.0 GCRN-LSTM 1.55±0.01 5.60±0.01 3.09±0.01
DGN 2.26±0.0 4.39±0.0 4.52±0.0 Ours 1.56±0.03 5.57±0.02 3.12±0.07
ShortestPath 0.88±0.0 7.06±0.01 1.98±0.0 Ours* 1.74±0.00 5.65±0.00 3.49±0.00
DQN 1.05±0.00 6.81±0.08 2.15±0.00 ShortestPath 1.05±0.00 7.93±0.00 2.26±0.00
DQNR 1.09±0.00 7.20±0.06 2.22±0.01 GraphSAGE 0.02±0.02 14.58±6.85 0.08±0.05
CommNet 1.11±0.00 7.22±0.03 2.26±0.01 A-DGN 0.30±0.14 10.85±1.34 0.67±0.28
DGN 1.10±0.00 7.23±0.13 2.25±0.01 GCRN-LSTM 0.99±0.01 8.37±0.02 2.03±0.01
Ours 1.02±0.01 8.26±0.02 2.10±0.02
Table3:Througputforroutingagentsindividuallytrainedon Ours* 1.10±0.00 7.59±0.01 2.38±0.01
thegraphsfromFig.3awithvaryingbetweennesscentrality.
Shownaretheresultsforthebandwidthlimitationmode. DQN+GraphSAGE DQN+GCRN-LSTM
DQN+A-DGN DQN+Ours
Agent Throughput Nolimitation Withbandwidthlimitation
Graph𝐺 𝐴 Graph𝐺 𝐵 Graph𝐺 𝐶 1.5 1.00
ShortestPath 3.20±0.00 1.53±0.00 1.02±0.00 1.0 0.75
DQN 3.28±0.01 1.43±0.02 0.98±0.01 0.50
DQNR 3.28±0.00 1.48±0.01 0.99±0.01 0.5 0.25
CommNet 3.31±0.00 1.53±0.00 1.01±0.01 0.00
0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 2.0 2.5
DGN 3.29±0.00 1.43±0.05 1.00±0.00 Environmentsteps 106 Environmentsteps 106
× ×
Figure5:RewardofDQNwithgraphobservationsduring
Whilewedidnottrainagentsforall1000testgraphs,wehave trainingwithout(left)andwith(right)bandwidthlimitations.
madesimilarobservationsfortheothergraphsweinvestigated. Theshadedareashowsthestandarddeviationover3models.
Tab.3showsthethroughputinthethreegraphsfromFig.3a,aver-
agedover1000episodesand3models.Weomittheresultswithout TheresultsareshowninTab.4andtherewardduringtraining
limitations,asallapproachesmatchshortestpathswithamean isshowninFig.5.Thehighpositiverewardandthroughputof
throughputof4.05ingraph𝐺 𝐴,2.38ingraph𝐺 𝐵and2.98ingraph
GCRN-LSTMandourproposedarchitectureshowthatgraphob-
𝐺 𝐶. In the limited bandwidth setting, we again find no notable servationsindeedenableagentstogeneralizeoverdifferentgraphs.
differencebetweenDQNandDGNinthesegraphsandnoticethat Ourmethodachievescomparableresultswhilehavingalowercom-
thelearningapproachesoutperformshortestpathsonlyforgraph municationoverhead.However,wefindthattheresultsareworse
𝐺 𝐴.Thethroughputachievedbythelearningapproachesisapprox- thanforagentsthatspecializeonsinglegraphs(seeSec.5.2).Forthe
imatelyonparwithshortestpathsforgraph𝐺 𝐵 and𝐺 𝐶. non-recurrentapproaches,A-DGNlearnsgraphobservationsbut
convergesmuchslowerthantherecurrentapproaches.GraphSAGE
5.3 GeneralizedRouting
failstolearn,eveninexperimentswithbatchsize256andjumping
Thissectionpresentstheresultsforourlearnedgraphobserva- knowledgenetworks[41]thatarenotshownhere.Consideringthe
tions.Weexpectthemtoenableagentstogeneralizeoverdifferent resultsofSec.5.1,itisunclearwhythenon-recurrentapproaches
graphs.Asallagentarchitecturesachievesimilarresultsforsingle perform poorly in this setting. We hypothesize that the targets
graphs,weselectDQNastheunderlyingagentarchitecturedue providedbybackpropagationthroughtimefacilitatelearning,but
toitssimplicity.Insteadofonlyreceivingobservationsfromthe furtherexperimentsindifferentenvironmentswouldberequiredto
environment,agentsnowalsoreceivegraphobservationsfrom verifythis.Allmethodshavealowerthroughputthantheshortest
nodestheyarelocatedat.Wetraingraphobservationsandagents pathsheuristicwithoutadditionalmodifications.
end-to-endwithreinforcementlearningonrandomlygenerated Uponcloserinspectionofthebehaviorofamodeltrainedwith
graphsfor2.5milliontotalsteps,240000iterationsandbatchsize ourarchitecture,wenoticethataround12%ofthe1000testepisodes
32.Episodesaretruncatedafter50stepstoincreasethenumberof containpacketsthatneverarriveattheirdestinationwithin300
generatedgraphs.Theresultingmodelsareevaluatedonour1000 steps.Thisiscausedbyroutingloops,acommonissuethatcanbe
testgraphsand300episodestepsforcomparabilitywithSec.5.2. addressedwithpost-processingofthelearnedpolicy[18].When
htdiwdnab
noitatimilon
noitatimil
draweR
htdiwdnab
noitatimilon
noitatimil
draweR6 RELATEDWORK
3 Reinforcementlearningforgraph-basedenvironmentsexempli-
2 fied by routing has been investigated since the introduction of
Baselines
Q-learning[4].Recentworkshaveshowntoimproveoverprevious
1 SP(stepwise)
SP(static) algorithmsinvariousdomainsandnetworkconditions[22,28].
DQN(best) Manyoftheseapproachesassumecentralizedcontrolwitha
DQN+GraphObs. globalview[1,6,18,21,35].Thisnotonlylimitstheirscalability,
10−1 Peaksatstep52 GCRN-LSTM(best) butalsotheirreactivity.Decentralizedapproaches[5,36]aremore
withvalue0.06 Ours(best)
reactive,buttheirpartialobservabilitymaydegradeperformance.
10−2
0 50 100 150 200 250 300
Learningdirectlyinthetargetnetworkallowsagentstospecial-
Episodesteps ize[35].However,thisischallenginginpracticebecausesubopti-
malactionscanresultinunacceptablereal-worldcosts.Specialized
Figure6:Throughputovertimeandnodestatedifferencesof agentscanalsogetstuckinlocaloptima,requiringretrainingfrom
selectedmodelsingraph𝐺 𝐴fromFig.3aaveragedover100 scratchifthegraphorthenetworkconditionschange[3].
episodes.Thedelayofasingleedgeisincreasedfrom2to10 Ideally,onewouldpre-trainagentstoperformwellinallgraphs
atstep50.Theshadedareashowsthestandarddeviation. andnetworkconditionsandoptionallyfine-tunethemonline.Graph
neuralnetworkshaveshowntoenablegeneralizationinrouting
repeatingtheexperimentwithdifferentseeds,weobserverouting
scenarios[8,33],asopposedtotraditionalmodelswithfixedin-
loopsindifferentgraphs.Weinvestigateanactionmaskingmecha-
putdimensionsthatspecializeonindividualgraphs[21].Tothe
nismthatstoresthepathofapacketandmasksactionsthatlead
bestofourknowledge,relatedworkswithgraphneuralnetworks
toalreadyvisitednodes.Iftherearenolegalactions,thepacketis
aremainlyrestrictedtocentralizedapproachesandagent-to-agent
droppedandanewpacketspawnsatarandomlocation.
communication[19,29].Anoteworthyexceptionistheworkby
Actionmaskingresultsinthroughputimprovementsthatmatch
GeyerandCarle[10],whoproposeadistributedmessagepassing
ourexpectationsfromthefixedtopologysetting,asshowninTab.4.
schemetolearnroutinginasupervisedsetting.Withourwork,we
However,itintroduces0.01and0.1droppedpacketsperstepfor
addressthegapofgeneralizabilityovergraphsinthecontextof
routingwithoutandwithbandwidthlimitations,respectively.
multi-agentreinforcementlearningwithdecentralizedexecution.
5.4 AdaptationandLimitations
7 CONCLUSION
Inthissection,weinvestigatehowagentsreacttoanovelsituation
Inthispaper,weinvestigatepartialobservabilityingraphenviron-
anddiscussthelimitationsofourwork.
mentsinthecontextofdeepmulti-agentreinforcementlearning.
Weexemplaryincreasethedelayofabottleneckedgeingraph
Weproposetodecouplethelearningofgraphrepresentationsand
𝐺 𝐴from2to10atstep50andevaluateitseffecton100episodesof
decisionmaking.Nodesexchangeandupdatetheirhiddenstatesin
300stepswithbandwidthlimitations.Fig.6showsthethroughput
adecentralizedmannertoimprovetheirviewofthegraph.Agents
ofdifferentapproachesinthisscenario,combinedwiththestepwise
thenreceivegraphobservationsbasedontheirlocationinthegraph.
meanabsolutedifferenceofnodestatevaluesfromtherecurrent
Weevaluateourgraphobservationsbasedonfourgraphneural
approaches.Staticshortestpaths(SP)ignoresthedelaychange,
networkarchitecturesacross1000diversegraphsinthecontextofa
stepwise SP considers it. For each learning approach, we show
routingenvironment.Ourresultsindicatethatrecurrentgraphneu-
theresultsofthebestmodel.Therecurrentapproachesquickly
ralnetworkscanbetrainedend-to-endwithreinforcementlearning
convergetosmallnodestatedifferencesatthebeginningandreact
andsparserewards.Weshowthattheresultinggraphobservations
tothechangeatstep50,althoughchangingedgedelayswerenever
donotonlyallowagentstogeneralizeoverdifferentgraphs,but
encounteredduringtraining.Beforestep50,DQNperformsslightly
alsotoadapttochangesinthegraphwithoutretraining.Depend-
betterthanourapproach.Afterthechange,allthreeDQNmodels
ingontheenvironment,however,havingnoconstraintsonthe
failtoadaptanddisplayapoorthroughput,whileoneoutofthree
exchangedmessagesandresultingpoliciescanleadtodeteriorated
modelswithourapproachisabletooutperformstepwiseshortest
behaviorcomparedtoagentsthatspecializeonasinglegraph.This
paths.Followingresearchcouldconsiderdynamicgraphchanges
isreflectedbyroutingloopsintheroutingenvironment,whichwe
duringtrainingandexploreadaptivityinmoredetail.
showcanbealleviatedwithactionmasking.
Theseimprovementsingeneralizabilityandadaptivitycomeat
Ourcontributionsopenupmultipledirectionsforfutureresearch.
thecostofexchangingmessageswithallneighbornodesateach
Toreducethecommunicationoverhead,aselectivemessageex-
step.Fig.6showsthatthereiscomparativelylittlechangeinthe
changebetweennodescanbestudied.Graphobservationsfurther
nodestatesafterconvergence,suggestingareducedneedforcom-
provideanewapplicationwithdistinctrequirementsforgraphneu-
munication.Whileweshowthatasinglemessagepassingiteration
ralnetworks.Forexample,asynchronousmessagepassingwithout
perstepsufficestolearngraphobservationsforgeneralizedrouting,
aneedforsynchronizationbetweennodeswouldincreasethere-
futureworkcouldinvestigatethefurtherreductionofcommuni-
activityinadistributedsetting.Finally,theeffectofincorporating
cationoverhead.Weseegreatpotentialforsynergieswithrecent
graphobservationsindifferentenvironmentsisworthinvestigating,
worksintheareaofagent-to-agentcommunication,whereagents
especiallywhencooperationbetweenagentsisrequired.
decidewhentosendmessages[15,24,40]toselectedrecipients
insteadofbroadcastingthemtoallotheragents[26].
tuphguorhT
ecnereffidetatsedoNACKNOWLEDGMENTS
[21] GyungminKim,YohanKim,andHyukLim.2022.DeepReinforcementLearning-
BasedRoutingonSoftware-DefinedNetworks. IEEEAccess10(2022),18121–
ThisworkhasbeenfundedbytheFederalMinistryofEducation
18133.
andResearchofGermany(BMBF)throughSoftwareCampusGrant [22] TianxuLi,KunZhu,NguyenCongLuong,DusitNiyato,QihuiWu,YangZhang,
01IS17050(AC3Net)andtheproject“Open6GHub”(grantnumber: andBingChen.2022.ApplicationsofMulti-AgentReinforcementLearningin
FutureInternet:AComprehensiveSurvey. IEEECommunicationsSurveys&
16KISK014).Ithasbeenco-fundedbytheGermanResearchFounda- Tutorials24,2(2022),1240–1279.
tion(DFG)intheCollaborativeResearchCenter(CRC)1053MAKI. [23] YujiaLi,DanielTarlow,MarcBrockschmidt,andRichardS.Zemel.2016.Gated
GraphSequenceNeuralNetworks.InProceedingsofthe4thInternationalConfer-
TheauthorsthankAmirkasraAminiforthevaluablediscussions.
enceonLearningRepresentations(ICLR).
[24] Yen-ChengLiu,JunjiaoTian,NathanielGlaser,andZsoltKira.2020.When2com:
REFERENCES Multi-AgentPerceptionviaCommunicationGraphGrouping.In2020IEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR).4105–4114.
[1] PaulAlmasan,JoséSuárez-Varela,KrzysztofRusek,PereBarlet-Ros,andAlbert [25] IlyaLoshchilovandFrankHutter.2019.DecoupledWeightDecayRegularization.
Cabellos-Aparicio.2022.Deepreinforcementlearningmeetsgraphneuralnet- In7thInternationalConferenceonLearningRepresentations(ICLR).
works:Exploringaroutingoptimizationusecase.ComputerCommunications [26] ZiyuanMa,YudongLuo,andJiaPan.2022.LearningSelectiveCommunication
196(2022),184–194. forMulti-AgentPathFinding.IEEERoboticsandAutomationLetters7,2(2022),
[2] GuillermoBernárdez,JoséSuárez-Varela,AlbertLópez,BoWu,ShihanXiao, 1455–1462.
XiangleCheng,PereBarlet-Ros,andAlbertCabellos-Aparicio.2021. IsMa- [27] VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AndreiA.Rusu,JoelVeness,
chineLearningReadyforTrafficEngineeringOptimization?.In2021IEEE29th MarcG.Bellemare,AlexGraves,MartinA.Riedmiller,AndreasFidjeland,Georg
InternationalConferenceonNetworkProtocols(ICNP).1–11. Ostrovski,StigPetersen,CharlesBeattie,AmirSadik,IoannisAntonoglou,Helen
[3] SaiShreyasBhavanasi,LorenzoPappone,andFlavioEsposito.2023.DealingWith King,DharshanKumaran,DaanWierstra,ShaneLegg,andDemisHassabis.2015.
Changes:ResilientRoutingviaGraphNeuralNetworksandMulti-AgentDeep Human-levelcontrolthroughdeepreinforcementlearning. Nature518,7540
ReinforcementLearning.IEEETransactionsonNetworkandServiceManagement (2015),529–533.
20,3(2023),2283–2294. [28] MingshuoNie,DongmingChen,andDongqiWang.2023.ReinforcementLearn-
[4] JustinBoyanandMichaelLittman.1993.PacketRoutinginDynamicallyChang- ingonGraphs:ASurvey.IEEETransactionsonEmergingTopicsinComputational
ingNetworks:AReinforcementLearningApproach.InAdvancesinNeuralInfor- Intelligence7,4(2023),1065–1082.
mationProcessingSystems,Vol.6.Morgan-Kaufmann,671–678. [29] YaruNiu,RohanPaleja,andMatthewGombolay.2021. Multi-AgentGraph-
[5] FlorianBrandherm,JulienGedeon,OsamaAbboud,andMaxMühlhäuser.2022. AttentionCommunicationandTeaming.InProceedingsofthe20thInternational
BigMEC:ScalableServiceMigrationforMobileEdgeComputing.InIEEE/ACM ConferenceonAutonomousAgentsandMultiAgentSystems(AAMAS’21).IFAA-
7thSymposiumonEdgeComputing(SEC).136–148. MAS,964–973.
[6] DanielaM.Casas-Velasco,OscarMauricioCaicedoRendon,andNelsonL.S.da [30] MilenaRadenkovicandVuSanHaHuynh.2020.CognitiveCachingattheEdges
Fonseca.2021.IntelligentRoutingBasedonReinforcementLearningforSoftware- forMobileSocialCommunityNetworks:AMulti-AgentDeepReinforcement
DefinedNetworking.IEEETransactionsonNetworkandServiceManagement18, LearningApproach.IEEEAccess8(2020),179561–179574.
1(2021),870–881. [31] DavisRempe,JonahPhilion,LeonidasJ.Guibas,SanjaFidler,andOrLitany.2022.
[7] MichaëlDefferrard,XavierBresson,andPierreVandergheynst.2016. Convo- GeneratingUsefulAccident-ProneDrivingScenariosviaaLearnedTrafficPrior.
lutionalNeuralNetworksonGraphswithFastLocalizedSpectralFiltering.In InProceedingsofthe2022IEEE/CVFConferenceonComputerVisionandPattern
AdvancesinNeuralInformationProcessingSystems,Vol.29.3844–3852. Recognition(CVPR).IEEE,17284–17294.
[8] MiquelFerriol-Galmés,JordiPaillisse,JoséSuárez-Varela,KrzysztofRusek,Shi- [32] BenedekRozemberczki,PaulScherer,YixuanHe,GeorgePanagopoulos,Alexan-
hanXiao,XiangShi,XiangleCheng,PereBarlet-Ros,andAlbertCabellos- derRiedel,MariaAstefanoaei,OliverKiss,FerencBeres,GuzmanLopez,Nicolas
Aparicio.2023.RouteNet-Fermi:NetworkModelingWithGraphNeuralNetworks. Collignon,andRikSarkar.2021.PyTorchGeometricTemporal:Spatiotemporal
IEEE/ACMTransactionsonNetworking31,6(2023),3080–3095. SignalProcessingwithNeuralMachineLearningModels.InProc.ofthe30thACM
[9] MatthiasFeyandJanE.Lenssen.2019.FastGraphRepresentationLearningwith InternationalConferenceonInformationandKnowledgeManagement.4564–4573.
PyTorchGeometric.InICLRWorkshoponRepresentationLearningonGraphsand [33] KrzysztofRusek,JoséSuárez-Varela,PaulAlmasan,PereBarlet-Ros,andAlbert
Manifolds. Cabellos-Aparicio.2020. RouteNet:LeveragingGraphNeuralNetworksfor
[10] FabienGeyerandGeorgCarle.2018. LearningandGeneratingDistributed NetworkModelingandOptimizationinSDN.IEEEJournalonSelectedAreasin
RoutingProtocolsUsingGraph-BasedDeepLearning.InProceedingsofthe2018 Communications38,10(2020),2260–2270.
WorkshoponBigDataAnalyticsandMachineLearningforDataCommunication [34] FrancoScarselli,MarcoGori,AhChungTsoi,MarkusHagenbuchner,andGabriele
Networks(Big-DAMA’18).AssociationforComputingMachinery,40–45. Monfardini.2009.TheGraphNeuralNetworkModel.IEEETransactionsonNeural
[11] JustinGilmer,SamuelS.Schoenholz,PatrickF.Riley,OriolVinyals,andGeorgeE. Networks20,1(2009),61–80.
Dahl.2017.NeuralMessagePassingforQuantumChemistry.InProceedingsof [35] StefanSchneider,RaminKhalili,AdnanManzoor,HaydarQarawlus,RafaelSchel-
the34thInternationalConferenceonMachineLearning(ICML).PMLR,1263–1272. lenberg,HolgerKarl,andArturHecker.2021. Self-LearningMulti-Objective
[12] AlessioGravina,DavideBacciu,andClaudioGallicchio.2023.Anti-Symmetric ServiceCoordinationUsingDeepReinforcementLearning.IEEETransactionson
DGN:astablearchitectureforDeepGraphNetworks.InThe11thInternational NetworkandServiceManagement18,3(2021),3829–3842.
ConferenceonLearningRepresentations(ICLR). [36] StefanSchneider,HaydarQarawlus,andHolgerKarl.2021.DistributedOnline
[13] WilliamL.Hamilton.2020.GraphRepresentationLearning.SynthesisLectures ServiceCoordinationUsingDeepReinforcementLearning.In2021IEEE41st
onArtificialIntelligenceandMachineLearning14,3(2020),1–159. InternationalConferenceonDistributedComputingSystems(ICDCS).539–549.
[14] WilliamL.Hamilton,ZhitaoYing,andJureLeskovec.2017.InductiveRepresen- [37] YoungjooSeo,MichaëlDefferrard,PierreVandergheynst,andXavierBresson.
tationLearningonLargeGraphs.InAdvancesinNeuralInformationProcessing 2018.StructuredSequenceModelingwithGraphConvolutionalRecurrentNet-
Systems,Vol.30.1024–1034. works.InNeuralInformationProcessing-25thInternationalConference(ICONIP)
[15] ShuaiHan,MehdiDastani,andShihanWang.2023.Model-BasedSparseCom- (LectureNotesinComputerScience,Vol.11301).Springer,362–373.
municationinMulti-AgentReinforcementLearning.InProceedingsofthe2023 [38] SainbayarSukhbaatar,ArthurSzlam,andRobFergus.2016. LearningMultia-
InternationalConferenceonAutonomousAgentsandMultiagentSystems(AAMAS gentCommunicationwithBackpropagation.InAdvancesinNeuralInformation
’23).IFAAMAS,439–447. ProcessingSystems,Vol.29.2244–2252.
[16] EricA.Hansen,DanielS.Bernstein,andShlomoZilberstein.2004. Dynamic [39] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,
ProgrammingforPartiallyObservableStochasticGames.InProceedingsofthe AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin.2017. AttentionisAll
19thNationalConferenceonArtificialIntelligence.AAAIPress,709–715. youNeed.InAdvancesinNeuralInformationProcessingSystems30.5998–6008.
[17] SeppHochreiterandJürgenSchmidhuber.1997. LongShort-TermMemory. [40] XuefengWang,XinranLi,JiaweiShao,andJunZhang.2023.AC2C:Adaptively
NeuralComputation9,8(1997),1735–1780. ControlledTwo-HopCommunicationforMulti-AgentReinforcementLearning.
[18] OliverHopeandEikoYoneki.2021.GDDR:GNN-basedData-DrivenRouting.In InProceedingsofthe2023InternationalConferenceonAutonomousAgentsand
Proceedingsofthe2021IEEE41stInternationalConferenceonDistributedComputing MultiagentSystems(AAMAS’23).IFAAMAS,427–435.
Systems(ICDCS).517–527. [41] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi
[19] JiechuanJiang,ChenDun,TiejunHuang,andZongqingLu.2020.GraphConvolu- Kawarabayashi,andStefanieJegelka.2018.RepresentationLearningonGraphs
tionalReinforcementLearning.InProceedingsofthe8thInternationalConference withJumpingKnowledgeNetworks.InProceedingsofthe35thInternational
onLearningRepresentations(ICLR). ConferenceonMachineLearning(ICML),Vol.80.PMLR,5453–5462.
[20] StevenKapturowski,GeorgOstrovski,JohnQuan,RémiMunos,andWillDabney.
2019.RecurrentExperienceReplayinDistributedReinforcementLearning.In
Proceedingsofthe7thInternationalConferenceonLearningRepresentations(ICLR).APPENDIX DGN. Agentsperformtworoundsofmessageexchangeusing
self-attention.Weusehyperparametersbasedontheofficialimple-
A ARCHITECTURE mentationofDGN,i.e.2attentionlayerswith8attentionheads,
Thissectionprovidesfurtherdetailsregardingthegraphneural andkeyandvaluesize16.However,weusein-andoutputsize256
networkandagentarchitecturesusedinthiswork. insteadof128forconsistency.Theoutputsoftheattentionlayers
areconcatenatedwiththeobservationencodingandprojectedto
A.1 GraphNeuralNetworks actionsusingalinearlayerofsizes(3·256,𝐷+1).
Node observations are encoded using a fully connected neural
B EXPERIMENTDETAILS
networkwith(𝑑 𝑚, 512, 256,𝑑 ℎ)hiddenunits,followedbyLeaky
ReLU activation functions. The input dimension𝑑 𝑚 is the size Tab.5providesanoverviewoftheparametersusedduringtraining
ofthenodeobservations.Theoutputdimension𝑑 ℎ isthehidden andtesting.Thelearningrateissetto0.001intheshortestpaths
dimensionoftherespectivegraphneuralnetworks.Weset𝑑 ℎ =128 regressiontask(seeSec.5.1).Forourevaluations,weusethemodels
forallexperiments.Allgraphneuralnetworksusethesamenetwork from the last training iteration. The following sections provide
toencodenodeobservations. furtherdetailsregardingourimplementationofAlg.2.
GraphSAGE. Weusethedefaultconfigurationprovidedbythe B.1 DeepQ-Learning
implementationofGraphSAGEinPyTorchGeometric[9]with𝑑 ℎ =
AtthebeginningofAlg.2,weinitializetheparametersofthetarget
128and𝐾 layers.ItusestheReLUactivationfunctionbetween
action-valuenetworkas𝜃ˆ 𝑄 ←𝜃 𝑄.Asintheimplementationof
graphconvolutionallayers.ExperimentswithLeakyReLUforim-
provedconsistencyresultedininstabilitiesduringtraining,espe- DGN,3thetargetparameters𝜃ˆ 𝑄 arethensmoothlyupdatedwith
ciallyforahighernumberoflayers.Wealsoexperimentedwith 𝜃ˆ 𝑄′ ←𝜏𝜃 𝑄 +(1−𝜏)𝜃ˆ 𝑄 ineachiteration.DGNfurtheraugments
JumpingKnowledgeNetworks[41].Whiletheyallowedforanim- theregularDQNloss (𝑦 𝑗 −𝑄(𝑜 𝑗,𝑎 𝑗;𝜃 𝑄))2 witharegularization
provedconvergencespeedinthesupervisedsetting,theydidnot term(𝑄ˆ(𝑜 𝑗,𝑎;𝜃 𝑄)−𝑄(𝑜 𝑗,𝑎;𝜃 𝑄))2forallotheractions𝑎≠𝑎 𝑗.
improvelearningofgraphobservations.
B.2 GraphObservations
A-DGN. Weusethedefaultconfigurationprovidedbytheim-
plementation of AntiSymmetricConv in PyTorch Geometric [9] Duringexecution,weimplementgraphobservationsasanenviron-
with𝑑 ℎ =128and𝐾 iterations.Itusesthetanhactivationfunction mentwrapper.Duringtraining,asdescribedinSec.3.4,wesample
betweenmessagepassingiterations. aminibatchofasequenceofstepstoperformbackpropagation
throughtime.Inourimplementation,thesesequencesareallowed
GCRN-LSTM. Weusethedefaultconfigurationprovidedbythe
tocrossepisodeboundaries.Whenreachingtheendofanepisode,
wim itp hle 𝑑m ℎe =nt 1a 2ti 8on ano df fiGC lto en rv sL izS eT 𝐾Mi +n 1P ,y rT eo sr uc lh tinG geo inm 𝐾etr mic eT se sam gp eo pr aa sl s[ i3 n2 g] weresetthenodestatesℎ′ 𝑗 tozero.Thisisdoneindependentlyfor
eachsequenceintheminibatch.
iterations.Thearchitectureusesinternalhiddenandcellstatesof
size𝑑 ℎ foreachnode. Table5:Parametersusedfortrainingandtesting.
Ours. WeusetwoLSTMcellsthatsharesinglehiddenandcell
statesofsize𝑑 ℎ =128foreachnode. Parameter EnvironmentMode
SingleGraph Generalized
A.2 AgentArchitectures
Optimizer AdamW[25]
Allagentarchitecturesuseafullyconnectedneuralnetworkwith
Learningrate 0.001
(𝑑 𝑜, 512, 256) hidden units, followed by Leaky ReLU activation
TotalSteps 250000 2500000
functionstoencodetheagent’sobservation.Theinputdimension
Stepsbeforetraining 10000 100000
𝑑 𝑜 isdeterminedbythesizeoftheagent’sobservations.Notethat
Replaymemorysize 200000steps
graphobservationsleadtoanextendedobservationspace.
Stepsbetweeniterations 10
DQN. DQNusesasinglelinearlayerofinputandoutputsizes Episodelength 300steps 50steps
(256,𝐷+1)tomaptheobservationencodingtothediscreteaction Minibatchsize 32
spacewith𝐷+1actions.Forourexperiments,weset𝐷 =3. Targetnetworkupdate𝜏 0.01
Discountfactor𝛾 0.9
DQNR. Theoutputoftheobservationencoderisfollowedby
Initialexploration𝜖 1.0
anLSTMcellwithhiddenandcellstateofsize256.Theactionis
𝜖-decay(per100steps) 0.996 0.999
predictedwithafullyconnectedlinearlayerwithinputandoutput
Minimumexploration𝜖 0.01
size(256,𝐷+1),usingthehiddenstateasinput.
Testexploration𝜖 0.0
CommNet. ThearchitectureofCommNetbuildsuponDQNRand
Testepisodes 1000
addstworoundsofmessageexchangeusingtheupdatedhidden
Testepisodelength 300steps
stateoftheLSTM.Oneroundofmessageexchangeisthesumofthe
agent’shiddenstateandthemeanofallneighbors’hiddenstates,
excludingtheagent’sownhiddenstate. 3https://github.com/PKU-RL/DGN/,includingthePyTorchversion.C TESTGRAPHS Table6:Metricsforthetestgraphs.Shownaretheminimum,
maximum,mean,andstandarddeviationofeachmetric.The
Whilethetraininggraphsarerandomlygeneratedontheflyand
firstthreemetricshavethesamevaluesforallgraphs.
thereforedependontheusedtrainingseed,thetestgraphsarefixed
andindependentofthetrainingseed.Thissectionsupplements
Metric Min Max Mean Std
Sec.4.2andprovidesfurtherdetailsregardingthetestgraphs.
Order(#nodes) 20 0
C.1 Metrics Nodedegree(#incidentedges) 3 0
Size(total#edges) 30 0
Tab.6liststhemetricsweusedtocomparethegraphs,including Diameter(hops) 5.00 12.00 7.21 1.42
detailsregardingthebetweennesscentralitybrieflymentionedin Diameter(delays) 8.00 23.00 12.84 2.72
thepaper.Consideringthegraphmaxbetweennesscentrality,the APSP(hops) 0.00 12.00 3.26 1.92
standarddeviationof0.1andthehighdifferencebetweenthemin- APSP(delays) 0.00 23.00 5.70 3.60
imumvalueof0.19andthemaximumvalueof0.7suggestthat Nodebetweennesscentrality 0.00 0.70 0.15 0.12
thetestsetcontainsdiversegraphswithandwithoutbottleneck Graphmaxbetweennesscentrality 0.19 0.70 0.39 0.10
nodes.Wethinkthatitisimportantforfutureworkingraph-based Graphmeanbetweennesscentrality 0.11 0.23 0.15 0.02
environmentstoprovidesimilarmetricsfortheirtestgraphsto
improvecomparability.Onecouldalsodrawinspirationfromcom-
municationnetworksandclassifythegraphsaccordingtotheir 1.0
structure.Whenconsideringdynamicgraphsinfuturework,itwill
beessentialtofurtherquantifythedynamicityofthegraphs.
0.8
C.2 ShortestPathsDistribution
0.6
Fig.7showsthecumulativedistributionoftheall-pairsshortest
paths(APSP)lengthsofthetestgraphsinhops.Whilethelongest
0.4
shortestpathtakes12hops,over99%ofallshortestpathstakeat
most8hops.Wethereforeconsidered8tobeasuitablecandidate
forthenumberofmessagepassingiterations𝐾ofthenon-recurrent 0.2
approachesandunrolldepth𝐽oftherecurrentapproaches.Ahigher
numbermayleadtoimprovedperformanceintheory,atthecost 0.0
ofahighercomputationaloverheadduringtrainingandahigher 0 1 2 3 4 5 6 7 8 9 10 11 12
Numberofhops
communicationoverheadduringexecution.Ourexperimentsshow
thattheeffectofusingmoremessagepassingiterationsdependson
theusedarchitectureandisnotnecessarilypositive(seeSec.5.1). Figure7:CumulativedistributionofAPSPlengthsonthetest
graphs.Theerrorbarsindicatethestandarddeviation.
shtaptsetrohsderevocfonoitroporP