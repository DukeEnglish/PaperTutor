How BERT Speaks Shakespearean English?
Evaluating Historical Bias in Contextual Language Models
MiriamCuscito
UniversitàdegliStudidiCassinoedelLazioMeridionale
miriam.cuscito@unicas.it
AlfioFerrara and MartinRuskov
UniversitàdegliStudidiMilano
{firstname}.{lastname}@unimi.it
Abstract In this paper, we explore the idea of analysing
the bias by focusing on the major syntactic, se-
Inthispaper,weexploretheideaofanalysing
mantic,andgrammaticaldifferencesbetweentwo
thehistoricalbiasofcontextuallanguagemod-
varieties of the English language: Early Modern
els based on BERT by measuring their ade-
quacy with respect to Early Modern (EME) (EME)andModern(ME).Moreprecisely,wepro-
andModern(ME)English. Inourpreliminary poseamethodandameasureofadequacytotest
experiments,weperformfill-in-the-blanktests the adherence of CLMs to the natural language
with 60 masked sentences (20 EME-specific, variety of interest. In particular, we assess the
20ME-specificand20generic)andthreedif-
level of diachronic bias of three CLMs: Bert-
ferent models (i.e., BERT Base, MacBERTh,
Base-Uncased(BERTBase)(Devlinetal.,2019);
EnglishHLM)1.Wethenratethemodelpre-
MacBERTh(ManjavacasandFonteyn,2022);and
dictions according to a 5-point bipolar scale
BertBaseHistoricEnglish(EnglishHLM).Inour
betweenthetwolanguagevarietiesandderive
aweightedscoretomeasuretheadequacyof preliminaryexperiments,weperformtestswith60
each model to EME and ME varieties of En- masked questions in which the models have the
glish. task to predict the masked word in the sentence.
Wethenratetheproposedresponsesaccordingto
1 Introduction
a5-pointbipolarscalebetweenthetwolanguage
Contextuallanguagemodels(CLMs)aredeepneu- variants and derive a weighted score from the re-
ral language models which create contextualised sponseprobabilitiesandtheirrespectivescoreson
wordrepresentations,inthesensethattherepresen- thescale.
tationforeachworddependsontheentirecontext Theseresults,althoughpreliminary,mightsug-
inwhichitisused. Thatistosay,wordrepresen- gestamethodapplicableinthedigitalhumanities
tationsareafunctionoftheentireinputsentence. whenCLMsareemployedfortheanalysisofhis-
Suchmodelsareusuallypre-trainedonlargetex- toricalcorpora.
tualcorporaanddesignedtohavehighpredictive
2 RelatedWork
capabilities. This makes them closely tied to the
domainsonwhichtheyweretrainedanddependent
Ifitistruethatlanguageshapesculturewhileitis
ontheinfrastructureuponwhichtheyarebased.
shapedbyit(Boroditsky,2011),languagemodels
The presence of various biases in CLMs has
in general – and CLMs in particular – constitute
beenextensivelystudied,typicallywiththeaimof
a still partially covered mirror of this dual rela-
proposingeffectivemitigationstrategies(deVassi-
tionship. Not only can a CLM be tested based
monManelaetal.,2021;AhnandOh,2021;Moza-
on its level of representativeness of the language
fari et al., 2020). However, there are instances
to determine its reliability, but also it can tell us
where the bias in certain CLMs is not necessar-
aboutlinguistic,social,andhistoricalphenomena
ily negative. This is particularly true when the
that concern the culture tied to that specific lan-
bias manifested in the language reflects its socio-
guage. Inotherwords,aCLMcouldbeavaluable
temporalcontext. Thisbiascouldbeadvantageous
tool towards the expansion of the broader social
fortasksthatdemandthissocio-temporalstaging
knowledgeofagivenculture,rightfullybecoming
(Corfield,2022;Diaz-Faesetal.,2023).
part of the basic tools of Cultural Analytics dis-
1 cussedbyManovich(Manovich,2020). According
https://huggingface.co/bert-base-uncased,
https://huggingface.co/emanjavacas/MacBERTh,
toBruner’s(Bruner,1984)pragmatic-culturalper-
https://huggingface.co/dbmdz/bert-base-historic-english-cased
4202
beF
7
]LC.sc[
1v43050.2042:viXraspective,learningalanguagealsomeanslearning valence score. This function indicates the period
theculturalpatternsassociatedwithit. Similarly, fromwhichthemaskedsentenceistypical.
analysing the language in its various realisations Then,wecalculateatoken-in-sentencetemporal
wouldmeanhavingtheopportunitytovisualisethe valencescoreσ : V×S → T,indicatingthescore
underlyingculturalpatterns. ofatokensubstitutingthesentencemask.
Moreover,CLMscanbehighlybeneficialalso Thementionedtemporalvalencescoresareas-
forphilological(vanLit,2019),pragmatic(Ruskov, signed arbitrarily according to the research hy-
2023),critical(Diaz-Faesetal.,2023),andliterary potheses. Taking this study as an example, the
work(Piperet al.,2023). However, theeffective- criterionusedtodetermineeachscorewasthede-
ness of these models depends on their ability to gree of alignment of certain sentences or tokens
adapttolanguagespecificityinitshistoricaldimen- withaspecifichistoricalperiodonaphilological-
sion. Thisistypicallyachievedbytrainingmodels linguistic basis. Scholars wishing to delve into
onhistoricaltextcorpora. However,thedifficulty languagestudyusingthismethodologicalapproach
of accessing large historical documentary collec- canselectivelychoosethescoretoassigntotheir
tionsmeansthatthemodelsavailablearestillfew testsetbasedontheirspecificresearchneeds. The
and requires verifying whether they adapt effec- versatilityoftheproposedmethodologyisevident
tivelytothehistoricallinguisticcontext. initsadaptabilitytoadiversearrayoffieldsofin-
BERT is a foundational contextual language terest. Thisflexibilityenablesresearcherstoseam-
model (CLM) which to date is the most widely lessly integrate personalized metrics, ensuring a
adopted(MontanelliandPeriti,2023). Anumber tailoredapproachtoanalysiswithoutundermining
of studies have explored different forms of bias theinherentconsistencyoftheresults.
in BERT (de Vassimon Manela et al., 2021; Ahn Asanexampleoftemporalvalencescore,given
andOh,2021;Mozafarietal.,2020). ThreeBERT- EME(EarlyModernEnglish)asthefarthestperiod
basedCLMsareofparticularinterestforourstudy: (i.e.,−1 ∈ T)andME(ModernEnglish)asclosest
(i)Bert-Base-Uncased(Devlinetal.,2019),created (i.e., 1 ∈ T), if we consider the sentence s 1 =
fromacorpusoftextsfromWikipediaandBook- “Whywilt[MASK]beoffendedbythat?” wehave
Corpus and a model of contemporary language, ρ(s 1) = −1ass 1 isarepresentativesentencefor
whichweuseasacontrolconditioninourexper- EME, and σ(“thou”,s 1) = −1, because in this
iment;(ii)MacBERTh(ManjavacasandFonteyn, context“thou”isindicativeforEME.Ontheother
2022),pre-trainedontextsfrom1500to1950;and hand, σ(“not”,s 1) = 0, because “not” is neutral
(iii)Bert-Base-Historic-English,pre-trainedoncon- regardingthetwolanguagevarieties.
temporarytextsandfine-tunedonhistoricaltexts Given a model m, for the masked token
fromthe19thcenturytothepresent. in each sentence (s ∈ S), we have the
set of {w ,w ,...,w } ⊂ V of n words
1 2 n
3 Method predicted by m for s, that are associated
with the vector of corresponding probabilities
To evaluate the adequacy of CLMs on a test set, p = (p(w ),p(w ),...,p(w ))T.
m 1 2 n
wedefineatemporalvalencetaskconsistingofa Forthisset,weexploitthescorefunctionσinor-
collection of test sentences, each with a masked
dertodefineatoken-in-sentencetemporalvalence
token (i.e., word). This is a typical fill-in-the- scorevectorx formgiventhesentences,thatis
m
blank task, where the models are required to pre- x = (σ(w ,s),σ(w ,s),...,σ(w ,s))T.
m 1 2 n
dict the masked token. Formally, we consider
This allows us to define the bias of a model
the following three sets: (i) we denote with S
regardingthesentenceasaweightedscore:
the set of all test sentences, (ii) with V we de-
note a set of vocabulary words, and (iii) with β(m,s) = xT p
m m
T = {−1,−0.5,0,0.5,1} ⊂ R, we denote a
5-pointbipolartemporalvalencescale,where−1 Then,weproceedtodefinethedomainadequacy
represents the farthest historical period and 1 the ofamodelwithrespecttoasentencesas
closesttotoday.
1
Withtheabovenotation,foreachofthemasked δ(m,s) = 1− | ρ(s)−β(m,s) |
2
sentences(denotedass ∈ S),wedefineafunction
ρ : S → T representing the sentence temporal based on the difference between the sentencetemporal valence score ρ(s) and the model bias Theelementstobemaskedwereselectedbased
β(m,s). ontheirbelongingtospecificwordclassesknown
An example of three sentences from different tohavesufferedmoreexposuretothediachronic
periods is provided in Tables 1, 2 and 3, which variationoftheEnglishlanguage: pronouns,verbs,
showthecorrespondingvaluesforρ,p,σ,β(m,s) adverbs,adjectives,andnouns. Ofthe60sentences
andδ(m,s). providedinAppendix,20areselectedtobesugges-
tivefortheEMEvarietyofEnglish,further20–as
BERTBase MacBERTh EnglishHLM
suggestiveforME,andfinal20aregeneric. Once
token p σ token p σ token p σ
thou 0.712 -1.0 thou 0.987 -1.0 not 0.639 0.0 thetestsetwascomplete,atemporalvalencescore
you 0.101 0.0 not 0.008 0.0 thou 0.303 -1.0 wasassignedtoeachsentence(seeρinSection3)
i 0.085 0.0 you 0.004 0.0 never 0.031 0.0
she 0.055 0.0 ye 0.001 -1.0 [UNK] 0.022 0.0 basedontheirlevelofchronologicalmarkedness.
he 0.048 0.0 he 0.000 0.0 ever 0.005 0.0
ThetestsetwasadministeredtothethreeCLMs,
β -0.712 β -0.988 β -0.303
δ 0.856 δ 0.994 δ 0.652 andthesuggestedwordswiththeirprobabilitywere
collected. The resultant vocabulary was marked
Table1: Scoresofthemodelsfor“Whywillt[MASK]
independently from the models that provided it
beoffendedbythat?” (temporalvalenceρ=−1)
bysettingthetoken-in-sentencetemporalvalence
score(i.e. σ)toeachword,basedonanestimation
BERTBase MacBERTh EnglishHLM ofproximityofthetoken’smeaningtoacertainlin-
token p σ token p σ token p σ
here 0.924 0.0 hither 0.740 -1.0 here 0.691 0.0 guisticvarietyinthecontextinwhichitappeared.
back 0.066 0.5 down 0.170 0.0 back 0.194 0.5 Notably, during this phase, our decision was to
there 0.004 -0.5 thus 0.045 -0.5 again 0.051 0.0
forth 0.003 -0.5 in 0.025 0.0 in 0.034 0.0 workonasentencelevel(contextually)ratherthan
out 0.003 1 again 0.020 0.0 hither 0.031 -1.0
onasetlevel(globally). Themethodprovedhighly
β 0.032 β -0.762 β 0.066
δ 0.984 δ 0.619 δ 0.967 effective in avoiding the risk of semantic flatten-
ing,giventhatalmosteverywordhasshownsome
Table2:Scoresfor“Haveyoucome[MASK]totorment
levelofcontextualsemanticspecificityiftakencon-
usbeforethetime?” (ρ=0)
textually rather than globally. An example is the
pronounyouin“fareyouwell,sir”,whichisglob-
BERTBase MacBERTh EnglishHLM ally neutral and yet acquires a strong diachronic
token p σ token p σ token p σ
orientation 0.720 1.0 ##ists 0.493 -0.5 to 0.319 0.0 valueifevaluatedinitscontext,inwhichitappears
misconduct 0.112 1.0 offenders 0.165 1.0 must 0.294 0.0
minorities 0.067 1.0 characters 0.130 0.5 may 0.187 0.0 tobeutmostarchaic.
partners 0.052 1.0 drunkards 0.117 0.0 would 0.104 0.0
harassment 0.048 1.0 delinquents 0.095 0.5 should 0.096 0.0 Once β and δ were calculated, we proceeded
β 1.000 β 0.031 β 0.000
withtheanalysisofthedataandthecollectionof
δ 1.000 δ 0.516 δ 0.500
results. The distribution of the bias score β and
Table3: Scoresfor“Shouldmenwhoareknownsexual
thedomainadequacyscoreδ forthesentencesin
[MASK]begivenaplatform?” (ρ=1)
the three groups (i.e., EME, Neutral, and ME) is
showninFigures1and2,respectively.
4 Evaluation
Figure 1 shows that for all three test sets,
We test our metrics with three BERT-based lin- MacBERTh is most aligned with EME, whereas
guistic models we consider relevant for the vari- BERTBaseisalwaysmostalignedasME.Histori-
etiesoftheEnglishlanguageofinterest: (i)Bert- calBERTshowsatendencytowardsamoreneutral
Base-Uncased,(ii)MacBERTh,and(iii)Bert-Base- languagethantheothertwomodelsinmarkedsen-
Historic-English. Inaccordancewiththeobjectives tences,whilstsurprisinglyitalignstoMEinneutral
of this study, the choice of models reflects a spe- sentences.
cific interest in language; therefore, they can be Figure2showsthatMacBERThhasbestdomain
replaced to best fit any other specific interest in adequacy for EME, and BERT Base has best do-
diachroniclanguageanalysis. Forthetestweused mainadequacyforME.Inthecaseoftheneutral
60word-maskedsentences,specificallycreatedfor testset,domainadequacyisnolessinformative. Al-
thisstudy. Tocreatethetestset,wereliedondif- thoughthesentencesdonotinherentlycarrytheir
ferent types of written language: contemporary expectations regarding language, models appear
standard,journalisticlanguage,socialmedianon- consistently well-suited to a neutral context, and
standard,andEarlyModernlanguage. noneofthempushesforstrongspecificityoftheirEME sentences Neutral sentences ME sentences
1.0 1.0 1.0
0.5 0.5 0.5
0.0 0.0 0.0
0.5 0.5 0.5
1.0 1.0 1.0
BERT Base MacBERTh English HLM BERT Base MacBERTh English HLM BERT Base MacBERTh English HLM
Model Model Model
Figure1: Distributionofbiasβ(m,s)ofthethreemodelswithrespecttothethreetestsets.
EME sentences Neutral sentences ME sentences
1.2 1.2 1.2
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
BERT Base MacBERTh English HLM BERT Base MacBERTh English HLM BERT Base MacBERTh English HLM
Model Model Model
Figure2: Distributionofthedomainadequacyδ(m,s)ofthethreemodelswithrespecttothethreetestsets.
correspondingtraineddomain. Ineffectthisleaves whencompletingtemporality-neutralsentences.
thesentencesclosetotheiroriginalneutrality ThenotionthatLMscanserveasawindowinto
Thispreliminarystudyprovidesanillustrationof thehistoryofapopulationisnotnew,butthereis
thenatureandfunctioningoftheCLMspredictive a growing interest in exploring the relationships
behaviour. Thepresenceorabsenceofmarkedness betweenthesemodelsandthesociolinguisticand
in the sentences enables all three CLMs to select socioculturalcontexts. Itisequallyimperativeto
thetypeofelementwhichbestfitstheco-text. So, establish a procedural framework to address the
while for diachronically marked sentences, mod- lack of evaluative methods for these models, as
els without training in that domain attempted to previouslyhintedatinthistext.
suggestprobablesolutions,sometimesresultingin Within this evaluation, we created a dedicated
a form of linguistically inconsistent mimicry, in test set for each model under scrutiny, drawing
unmarked sentences, the models perform excep- upon approaches used for evaluation of bias in
tionallywell,andlinguisticinaccuraciesarerare. CLMs. In creating our test sets, we built our
sentences both on logical-semantic and logical-
5 Conclusion syntactic tasks. Future work could try to create
a test set for model interrogation that is culture-
Bothnotionsofbias(β)anddomainadequacy(δ) oriented, delving into socio-culturally significant
provideimportantinsightsofthenatureofthemod- elements such as customs, historical events, and
els. The first, β, indicates a tendency in terms of attitudestowardssocialgroups–elementsrecog-
temporal valency. In other words, the interpreta- nisedasbelongingtosocialknowledge.
tion of its value should be considered within the Thisstudyaimsnotonlytoproposeamethodol-
contextofthespecificdichotomyoflanguageva- ogyforassessinglanguagemodelsbutalsotoput
rieties. Ontheotherhand,δ reflectstheadequacy forthhypothesesforexpandingtheavailabletools
foranindividuallanguagevariety. Itsuccessfully tohumanitiesscholarsinterestedinstudyingcom-
capturesmodeltendencieswhencompletinghistor- plex socio-cultural phenomena with an approach
icallypredeterminedsentences. However, itfalls whichbeginsbyinterpretingtextualcluesandin-
short in capturing the preferred language variety ferringtheirconnectionstoreality.
)s,m(
)s,m(
)s,m(
)s,m(
)s,m(
)s,m(References Andrew Piper, Hao Xu, and Eric D. Kolaczyk. 2023.
ModelingNarrativeRevelation. InProceedingsof
JaimeenAhnandAliceOh.2021. Mitigatinglanguage-
theComputationalHumanitiesResearchConference
dependentethnicbiasinBERT. InProceedingsofthe
2023,volume3558ofCEURWorkshopProceedings,
2021ConferenceonEmpiricalMethodsinNatural
pages500–511,Paris,France.CEUR.
Language Processing, pages 533–549, Online and
Punta Cana, Dominican Republic. Association for MartinRuskov.2023. WhoandHow: UsingSentence-
ComputationalLinguistics. Level NLP to Evaluate Idea Completeness. In Ar-
tificial Intelligence in Education. Posters and Late
LeraBoroditsky.2011. HowLanguageShapesThought. BreakingResults,WorkshopsandTutorials,Industry
ScientificAmerican,304(2):62–65. andInnovationTracks,Practitioners,DoctoralCon-
sortiumandBlueSky,CommunicationsinComputer
Jerome Bruner. 1984. Pragmatics of Language and
and Information Science, pages 284–289, Cham.
LanguageofPragmatics. SocialResearch,51(4):969–
SpringerNatureSwitzerland.
984.
L.W.C.vanLit.2019. AmongDigitizedManuscripts.
PenelopeJ.Corfield.2022. Fleetinggesturesandchang- Philology, Codicology, Paleography in a Digital
ingstylesofgreeting:researchingdailylifeinBritish World. Brill,Leiden,TheNetherlands.
townsinthelongeighteenthcentury. UrbanHistory,
49(3):555–567. A TestSet
DanieldeVassimonManela,DavidErrington,Thomas
Fortransparencyandreproducibilitypurposes,the
Fisher,BorisvanBreugel,andPasqualeMinervini.
following anonymous link contains the complete
2021. Stereotypeandskew: Quantifyinggenderbias
in pre-trained and fine-tuned language models. In test set with the corresponding values produced
Proceedingsofthe16thConferenceoftheEuropean duringevaluation:
Chapter of the Association for Computational Lin- https://tinyurl.com/bert-shakespearean
guistics: Main Volume, pages 2232–2242, Online.
AssociationforComputationalLinguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deepbidirectionaltransformersforlanguageunder-
standing. InProceedingsofthe2019Conferenceof
theNorthAmericanChapteroftheAssociationfor
ComputationalLinguistics: HumanLanguageTech-
nologies,Volume1(LongandShortPapers),pages
4171–4186,Minneapolis,Minnesota.Associationfor
ComputationalLinguistics.
AlbaMorollonDiaz-Faes,CarlaMurteira,andMartin
Ruskov. 2023. Explicit references to social values
infairytales: Acomparisonbetweenthreeeuropean
cultures. InTheJoint3rdInternationalConference
onNaturalLanguageProcessingforDigitalHumani-
ties&8thInternationalWorkshoponComputational
LinguisticsforUralicLanguages,Tokyo,Japan.As-
sociationforComputationalLinguistics.
EnriqueManjavacasandLaurenFonteyn.2022. Adapt-
ingvs.Pre-trainingLanguageModelsforHistorical
Languages. JournalofDataMining&DigitalHu-
manities,NLP4DH.
Lev Manovich. 2020. Cultural analytics. The MIT
Press,Cambridge,Massachusetts.
Stefano Montanelli and Francesco Periti. 2023. A
SurveyonContextualisedSemanticShiftDetection.
ArXiv:2304.01666.
MarziehMozafari,RezaFarahbakhsh,andNoëlCrespi.
2020. Hatespeechdetectionandracialbiasmitiga-
tioninsocialmediabasedonBERTmodel. PLOS
ONE,15(8):e0237861.