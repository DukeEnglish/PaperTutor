Tighter Generalisation Bounds via Interpolation
Paul Viallard1∗ Maxime Haddouche2,3 Umut Şimşekli4 Benjamin Guedj2
1Univ Rennes, Inria, CNRS IRISA - UMR 6074, F35000 Rennes, France
2Inria, University College London
3Université de Lille
4Inria, CNRS, Ecole Normale Supérieure, PSL Research University, Paris, France
Abstract on the population risk as R (h) Rˆ (h) + ∆ (h).
D ≤ S S
Hence, a generalpattern for developing generalisation
This paper contains a recipe for deriving
bounds has been based on deriving inequalities of the
new PAC-Bayes generalisationbounds based
following form: with probability at least 1 δ,
on the (f,Γ)-divergence, and, in addition, −
presents PAC-Bayes generalisation bounds
Complexity+ln1
whereweinterpolatebetweenaseriesofprob- ∆ (h) δ.
ability divergences (including but not lim- S ≤s m
itedtoKL,Wasserstein,andtotalvariation), The Complexity term exhibits a facet of the in-
making the best out of many worlds depend-
trinsic complexity of the learning problem such as a
ing on the posterior distributions properties.
certain notion of richness of the predictor class .
Weexplorethetightnessoftheseboundsand H
This encompasses VC dimension (Vapnik, 2000) and
connect them to earlier results from statisti-
Rademacher complexity (Bartlett & Mendelson, 2001;
callearning,whicharespecificcases. Wealso
2002)). However,theVCdimension,forinstance,hap-
instantiateourbounds astrainingobjectives,
pens being too large when huge classes of predictors
yielding non-trivial guarantees and practical
are considered, such as deep neural networks. Then,
performances.
exploitingtheBayesianparadigmoflearningposterior
knowledge from data and prior modelling of the envi-
ronment allows exploiting tools from information the-
1. Introduction ory (Cover & Thomas, 2001) such as mutual informa-
tion as complexity measure (Neal, 2012).
Understanding the generalisation ability of learning
Beyond learning with the Bayesian paradigm de-
algorithms is a fundamental question in machine
scribed above, PAC-Bayes emerged as a rela-
learning(Vapnik & Chervonenkis,1974;Vapnik,2000)
tively recent branch of learning theory (see Guedj,
which has become even more challenging with the
2019; Alquier, 2024; Hellström et al., 2023) and
emergenceofdeeplearning. Typically,ageneralisation
has known a significant increase of interest in
bound is stated with respectto a learning problem de-
the past two decades, providing non-vacuous gen-
scribed by a tuple ( , ,ℓ) consisting of a hypothesis
H Z eralisation guarantees for neural networks, along-
(or predictor) space equipped with a distance d ,
a data space ,
andH
a loss function ℓ :
HR. side novel learning algorithms (see Dziugaite & Roy,
Z H×Z → 2017; Pérez-Ortiz et al., 2021b;c;a, among others).
It aims to bound the population risk of a given hy-
Apart from neural networks, this framework allows
pothesis h, defined as R (h) = E z [ℓ(h,z)] (where
tackling various learning settings such as reinforce-
D ∼D
denotes the unknown data distribution over ).
D Z ment learning (Fard & Pineau, 2010), online learn-
As istypicallynotknowninpractice,ahypothesish ing (Haddouche & Guedj, 2022), multi-armed bandits
D
isusuallybuiltby(approximately)minimisingtheem- (Seldin et al., 2011; 2012; Sakhi et al., 2023), meta-
pirical risk, given by Rˆ (h) = 1 m ℓ(h,z ), where learning (Amit & Meir, 2018; Farid & Majumdar,
= z m is aS datasetm of mi=1 data pi oints, in- 2021;Rothfuss et al., 2021;2022;Ding et al.,2021) or
S depen{ dei nt∈ aZ nd}i i= d1 entically distribuP ted (i.i.d.) from . adversariallyrobust learning (Viallard et al., 2021) to
D
We define the generalisation gap of a hypothesis h as name but a few.
∆ (h):= R (h) Rˆ (h). Ifwe canobtainanupper-
The major part of PAC-Bayes bounds use the
S | D − S |
boundon ∆ (h), weimmediately getanupper-bound
Kullback-Leibler (KL) divergence between a data-
S
∗Theworkwasdonewhentheauthorwasaffiliatedwith dependent “posterior” distribution and a “prior”
Inria Paris. distribution (to be precised in Section 2) as a com-
1
4202
beF
7
]LM.tats[
1v10150.2042:viXraplexity measure (McAllester, 1999; 2003; Catoni, novellearningalgorithminvolvinga combinationof f-
2007; Germain et al., 2009; Tolstikhin & Seldin, 2013; divergencesandIPMsasregulariser,yielding,inmany
Kuzborskij & Szepesvári, 2019; Rivasplata et al., cases, better results than considering those complexi-
2020; Haddouche et al., 2021; Haddouche & Guedj, ties separately.
2023b), and it has been shown recently that it is
Outline Section 2 gather background for both PAC-
possible to obtain PAC-Bayesian bounds for any
Bayes learning and (f,Γ)-divergences. Those two no-
f-divergence (including KL) (Alquier & Guedj, 2018;
tions intricates in Section 3 where generic templates
Ohnishi & Honorio, 2021; Picard-Weibel & Guedj,
are provided. New PAC-Bayes bounds interpolating
2022). Such developments allow using various com-
f-divergencesand IPMs are then derived in Section 4.
plexities to understand generalisation as there is no
Section 5 exploits previous results to connect PAC-
apparent reason to prioritise KL.
BayeswithRademachercomplexitiesandheavy-tailed
However, a major drawback of f-divergences is that
SGD. Finally, Section 6 gathers experiments. Supple-
they are irrelevant to understand the generalisation
mentary background and results are gathered in Ap-
ability of deterministic predictors. Aiming at alleviat-
pendices AandB.Appendix C gathersthe postponed
ing this problem, anotherline ofresearch(Amit et al.,
proofs and Appendix D concludes this paper with ad-
2022; Haddouche & Guedj, 2023b; Viallard et al.,
ditional insights on experiments.
2023b) developed PAC-Bayesian generalisation
bounds based on integral probability metrics (IPMs),
2. Notation and Background
including the well known 1-Wassersteindistance.
Knowingwhichcomplexitymustbe choseninpractice
PAC-Bayes framework. PAC-Bayesian bounds fo-
is challenging. For instance, KL-based PAC-Bayes
cus on a randomised setting where the hypothesis is
bounds are expected to go to zero for large sample
drawn from a posterior distribution ρ ( ), where
size (then to explain generalisation), but they cannot ∈ P H
( ) denotes the set of probability distributions de-
deal with deterministic predictors (KL is infinite in P H
fined on . This posterior is designed from the train-
this case), which is inconsistent, e.g., with practi- H
ing set and a prior distribution π ( ). A clas-
cal optimisation of deep nets. On the other hand S ∈ P H
sical PAC-Bayesian result is Maurer (2004, Theorem
Wasserstein-based bounds deal with deterministic
5)(theso-calledMcAllesterbound),whichstatesthat,
predictors, but do not possess an explicit convergence
for ℓ [0,1], with probability at least 1 δ, for any
rate with respect to the number of data points m in ∈ −
posterior distribution ρ ( ),
most cases. We tackle this question in this work and ∈M H
propose elements of answer in the form of a unifying
KL(ρ π)+ln2√m
framework intricating PAC-Bayes with the recent E ∆ (h) k δ , (1)
notion of (f,Γ)-divergences (Birrell et al., 2022). h ρ S ≤s 2m
∼ h i
Contributions. We derive two generic recipes ex- whereπ ( )is any data-freedistributionand KL
∈M H
ploiting (f,Γ)-divergences, acting as a prelude to denotes the Kullback-Leibler divergence.
generalisation bounds. Using these recipes, we fur-
Background for (f;Γ) divergences. We denote
ther elaborate novel generalisation bounds interpo-
by ( ) (resp. ( )) the set of measurable
b
lating various f-divergences and IPMs. In many (respM . bH ounded meaM suraH ble) functions from to R.
cases, the derived bounds are tractable, contrast- H
(f,Γ)-divergences (Definition 2.2) recently emerged in
ing with many results in Ohnishi & Honorio, 2021;
Birrell et al.(2022)ascomplexitymeasuresinterpolat-
Picard-Weibel & Guedj, 2022. In particular, we fo-
ing f-divergences and Γ-IPMs (Definition 2.1). They
cus on a result interpolating the KL divergence and
requiretwoelementarybuildingblocks: aconvexfunc-
the Wassersteindistance,connectingwitha largepart tionf I R whereI R is anintervaldetermining
of PAC-Bayes literature. Our results are also general ∈ → ⊆
the f-divergence and a set Γ ( ).
enoughto unveilrathersurprising links between PAC- ⊆M H
Definition 2.1. For any distribution ρ,π ( )2,
Bayes and Rademacher complexity and to obtain ef- ∈ P H
the Γ-IPM between π and ρ is defined as WΓ(ρ,π):=
fortlessgeneralisationboundsforheavy-tailedstochas-
sup E ϕ(h) E ϕ(h) . Also, if ρ π,
tic differential equations. All of our results show that thenϕ ∈tΓ he{ fh -∼ diρ vergen−
ce
bg e∼ tπ
ween
π}
and ρ is D (ρ
≪
π):=
combiningPAC-Bayeslearningwith(f,Γ)-divergences f k
provideaunifyingframeworkforgeneralisation,which E h ∼πf dd πρ(g) , otherwise D f(ρ kπ):=+ ∞.
result in tighter bounds. We finally conduct experi- (cid:16) (cid:17)
ments exhibiting the benefits of interpolating (f,Γ)- Both f-divergences and Γ-IPMs have been broadly
divergences and IPMs. More precisely, we propose a used in PAC-Bayes as those complexity measures pos-
sess useful variational formulations allowing to trans-
2fer the generalisation ability of the posterior distri- 3. Elementary Steps Towards
bution of interest to a prior one, usually verifying Generalisation
better properties (e.g., not depending on ). A fa-
mous example stands for the KL divergeS nce (take We propose two new results (Theorems 3.1 and 3.2)
f: x xlnx) which satisfies the ’change of mea- involving (f,Γ)-divergences. Those bounds act as
sure f7→ ormula’ (Csiszár, 1975; Donsker & Varadhan, general templates to recover generalisation bounds
1976), leading to various PAC-Bayes bounds as throughout this work when particularising the func-
the McAllester (McAllester, 2003) and Catoni ones tion f and the set Γ. We start with Theorem 3.1, our
(Catoni,2007). Suchavariationalformulationismore most general statement.
generallyattainedforanyf-divergence(Nguyen et al., Theorem 3.1. Let φ Γ, δ [0,1] and π ( ).
2010; Broniatowski& Keziou, 2006, see Birrell et al., With probability at leaS st∈ 1 δ o∈ ver m,∈ weP haH ve
− S ∼ D
2022, Proposition 50 for a proof) and leads to gen- for all ρ ( )
∈P H
eralisation bounds for hostile data (Alquier & Guedj,
1
2018). The variational formulation is recalled: E φ (h) DΓ(ρ π)+ln +ln E exp Λπ(φ ) .
h ρ S ≤ f k δ m f S
D (ρ π):= sup E ϕ(h) Λπ(ϕ) (2) ∼ hS∼D (cid:0) (cid:1)i
f k ϕ ∈Mb( H)(cid:26)h ∼ρ − f (cid:27) As the exponential moment of Theorem 3.1 may be
where Λπ f(ϕ) := inf c ∈R {c+E h ∼πf ∗(ϕ(h) −c) }, and ah ta erd tht io sc ao sn sutr mol p, tw ioe np ar top thos ee cT osh teo or fe tm he3. b2 ouw nh di ec dh da il ffle ev ri --
f is the Legendre transform of f. Note that, for the
∗
KL divergence, Λπ(ϕ)=lnE eϕ(h). ence property (see e.g. Boucheron et al., 2013).
f h π
Thus, a variational formulat∼ ion is at the core of Theorem 3.2. Let φ Γ,δ [0,1] and π
the interplay between f-divergences and generalisa- ( ). Assume there eS xis∈ ts a func∈ tion gπ such tha∈ t
tion, and this conclusion also holds for Γ-IPMs by P forH any ,π, Λπ(φ ) Bπ(φ ). Assume tShe bounded-
definition (even if the supremum is taken on a re- differencS e propef rtyS on≤ Bπ, i.eS .,
stricted subset Γ ( ). Indeed, PAC-Bayesian
generalisation
bou⊂ ndsM inb vH
olving IPMs have been pro-
i 1,...,m , sup Bπ(φ ) Bπ(φ ′) c i,
posed (Amit et al., 2022), with a particular focus
∀ ∈{ } S∈Zm,z i∈Z| S − Si |≤
onWassersteindistances(Haddouche & Guedj,2023b; where is the learning sample where z in is re-
Viallard et al., 2023b). placedS bi y′ z . Then, we have, with probabi ilityS at least
′i
1 δ over m, we have for all ρ ( )
Despite this similarity, f-divergencesand Γ-IPM have − S ∼D ∈P H
been mainly thought separately until the recent work
of Birrell et al. (2022), which proposed the unifying E φ (h) DΓ(ρ π)+ E Bπ(φ )+
ln1
δ
m
c2.
notion of (f,Γ)-divergences detailed in Definition 2.2. h ρ S ≤ f k m S v 2 i
∼ S∼D u i=1
u X
Definition 2.2 ((f,Γ)-divergence). For any Γ t
⊂
( ), we define for any ρ ( ) and π ( )
Mb H ∈ P H ∈ P H Analysis of Theorems 3.1 and 3.2. InPAC-Bayes,
the (f,Γ)-divergence (with f (a,b)) defined by
∈F φ is often a function of the generalisation gap, e.g.,
φS = λ∆ or λ∆2 with λ > 0. Our two results
D fΓ(ρ kπ):= ϕsu ∈p Γ(cid:26)gE ∼ρϕ(g) −Λπ f(ϕ) (cid:27). φhoS ld hs aswi tt ohS bth ee ba os us nSu dm edpti (o sn atiφ
sS fied∈
eΓ .g.
.,
I fn orp ca lr at si sc iu fil ca ar -,
S
tion tasks). Such an assumption has positive conse-
(f,Γ)-divergences consider the variationalformulation
quencesontightness: fromEquation(3),weknowthat
(2) of f-divergences restrained to the set Γ of the
DΓ(ρ π) min D (ρ π),WΓ(ρ,π) ,thusourbounds
associated IPM. They satisfy an important varia- f k ≤ f k
aretighterthanexistingresultsinvolvingseparatelyei-
tional formula (namely the infimal convolution for- (cid:0) (cid:1)
ther f-divergences or Γ-IPMs: interpolating complex-
mula) (Birrell et al., 2022, Theorem 8-1.), recalled in
ities leads to tightness. Theorem 3.1 involves an ex-
(3).
ponential moment on the Legendre transform Λπ(φ ),
f
D fΓ(ρ kπ)
≤η
in (f
)
WΓ(ρ,η)+D f(η kπ) . (3) whichisofinterestinSection4.1,whenconsideringKS L
∈P H divergence. TogobeyondKL,weexploitTheorem3.2,
(cid:8) (cid:9)
Equation(3)justifiesdescribingthe(f,Γ)-divergences whichusestheboundeddifferenceassumptiontorelax
as ’interpolations’ between f-divergences and Γ-IPMs the dependency on Λπ(φ ). We show the benefits of
f
S
as the former is controlled by an optimal combination this relaxation in Section 4 to obtain generalisation
ofthelasttwo. WeshowinSection3thatEquation(3) boundsholdingwithvariousf-divergencessuchasthe
paves the way towards generalisationbounds. Hellinger distance or the Reverse KL.
34. Generalisation Bounds with Various Theorem 4.1 of Alquier et al., 2016), of the Catoni’s
Complexity Measures fast rate bound McAllester (2013, Theorem 2) and
ofthesupermartingaleboundsofHaddouche & Guedj
WeparticularisetheresultsofSection3toobtainnovel (2023a); Viallard et al. (2023b) involving simultane-
PAC-Bayes bounds involving various pairs of diver- ously a Wasserstein distance and a KL divergence.
gence and IPM. We focus on the important partic- This comes at the cost of bounded Lipschitz losses
ular case of Section 4.1, which shows that a gener- while unbounded subgaussian losses are allowed in
alisation bound interpolating KL and Wasserstein is Alquier et al. (2016, Theorem 4.1), even heavy-tailed
reachable and tightens existing results. Section 4.2 ones in Haddouche & Guedj (2023a); Viallard et al.
providesPAC-Bayes bounds involvinga wide rangeof (2023b).
divergences alongside a generic IPM.
On the value of the Lipschitz constant. If ℓ is
1-Lipschitz and bounded by 1, then ∆2 is at least 4-
4.1. A Fundamental Example: a PAC-Bayes S
Lipschitz. This deterministic constant may be insuffi-
Bound Interpolating KL Divergence and
cientastheKLdivergenceisalwaysattenuatedbythe
Wasserstein
impact of dataset size m. Amit et al. (2022) showed
W ofh ben ounf K deL d(x) L-= Lipx scl hn i( tx z) fuan nd ctiΓ ons=
,
tL hi ep nL b ti hs eth ase sos ce it
-
t Hh aa dt d, of uo cr hea &fin Git ue edH
j
, (2∆ 022 S3bi )s em x8 tL enln d( e2 d|H t|/ hδ i) s-L reip susc lthit tz o.
ated (f,Γ)-divergenceinterpolates between the KL di- anycompact ofRd atthecostofanexplicitdepen-
|H|
vergence and Wasserstein distance in the sense that dency on the dimension. In order to obtain sharper
the optimal combination of those complexities upper Lipschitz constants for general without an explicit
H
bounds the (f,Γ): impact of the dimension, we develop in Theorem 6.1
a novel high probability Lipschitz constant based on
DΓ LipL
b (ρ π) inf LW (ρ,η)+KL(η π) . (4) an empirical surrogate of the Rademacher complex-
fKL k ≤η ∈P( H){ 1 k } ity. This new result, while independent of the (f,Γ)-
Equation(4)comesimmediatelyfromEquation(3)us- divergence, is of interest for the practical optimisa-
Lin Lg ipt 1hat LD Lf iK pL 1.= WK eL noa wnd arW eaL bi lp eL b to≤ pL arW ti1 cua las riL si ep TL b h=
e-
Wtio an sso ef rsT teh ie nor de im sta4 n. c1 ea ws .i rt .td .im thi eni Ksh Les dt ih vee ri gm enp ca ec .tof the
b ⊆
orem 3.1.
4.2. PAC-Bayes Bounds Beyond KL
Theorem 4.1. Assume that ℓ [0,1]. Assume that,
∈ Divergence
for any δ (0,1), with probability 1 δ , h ∆2(h)
′ ′
is L(m,δ ′)-∈ Lipschitz. Thus, for any d− ata-free→ prioSr π, The general results of Section 3 go beyond KL di-
with probability at least 1 δ over m, we have vergence. We show in Theorem 4.2 that tractable
− S ∼ D
for all ρ ( ), any η ( ), PAC-Bayes bounds are reachable through (f,Γ)-
∈P H ∈P H
divergences for the reverse KL (KL) defined as
E ∆ (h) KL(ρ π) := E ln(dρ(g)) and the squared
h ∼ρ S ≤ Hellink
ger H2(ρ
π− ):=g ∼Eπ
g
πd (π
dρ/dπ(g) 1)2.
KL(η π)+ln4√m Ourresults,took urknowl∼ edge,arethefi− rstPAC-Bayes
sL(m,δ/2)W 1(ρ,η)+ k
2m
δ .
generalisationbounds
involvp
ing those complexities.
Theorem 4.2. For ∆ Γ. With probability at least
Comparison with literature. To our knowledge, 1 δ over m, wS e∈ have for all ρ ( ) and
− S ∼ D ∈ P H
Theorem 4.1 is the first PAC-Bayes generalisation η ( ):
∈P H
bounds involving simultaneously a KL divergence Reverse-KL bound:
alongside a Wasserstein distance. Theorem 4.1 shows
that considering bounded Lipschitz losses leads to a E ∆ (h) 2WΓ(ρ,η)+2KL(η π)
h ρ S ≤ k
boundtighterthantheMcAllesterbound(McAllester, ∼
1 1 1
2003) involving a KL term and also Amit et al. (2022,
+ +ln 1+ 2mln . (5)
Theorem 11) involving a Wasserstein distance. Our rm (cid:18) m (cid:19)r δ
additionaltightnesscomesfromconsideringand(f,Γ)- Hellinger bound:
divergence and the infimal convolution formula (4).
Note also that the proof technique behind Theo- E ∆ (h) 2WΓ(ρ,η)+2H2(η π)
h ρ S ≤ k
rem 4.1 allow us to similarly improve on various ∼
PAC-Bayes bounds. Indeed, we provide in Ap- 1 2 1
+ + 2mln . (6)
pendix B.1 variants of Catoni’s bound (Catoni, 2007, m m+1 δ
r r
4TV bound: (Bartlett & Mendelson, 2001, Definition 2). To do
so, we first consider (7) derived in Theorem 4.2 with
E ∆ (h) WΓ(ρ,η)+TV(η,π) the Wasserstein distance as IPM. However, instead
h ρ S ≤
∼ of considering the Euclidean distance, we use the set
1 ln1 of Lipschitz functions w.r.t. the Kronecker distance
+
4m
+ s2mδ. (7) dKron(h,h ′) := 1 h=h′. Then, the Wasserstein boils
r downto the TV dis6 tance(Lindvall,1992;Gibbs & Su,
2002) and we need to estimate the Lipschitz constant.
Analysis of the bounds. To our knowledge, we
propose the first tractable PAC-Bayes bounds in- Lipschitz constant based on the Rademacher
volving the reverse KL or squared Hellinger. Our complexity. Surprisingly, the Rademacher complex-
novelty lies in the use of the bounded difference ity emerges from the estimation of the Lipschitz con-
property in Theorem 3.2, which alleviates the ex- stant w.r.t. the Kroneckerdistance, this fact is stated
ponential moment of Theorem 3.1. This why the in Theorem 5.1 below.
workofOhnishi & Honorio(2021)onlyproposesPAC-
Theorem 5.1. For any hypothesis set , for any L-
Bayesianbounds forRényior χ2 divergencesandthat Lipschitz loss ℓ : R (by considH ering dKron),
Picard-Weibel & Guedj (2022) is forced to consider with probability atH le× astZ 1→ δ over m
rawexponentialmoments. Onthecontrary,webypass − S ∼D
this constraintand show that reverseKL and squared h ∆ (h) is L(m,δ)= 4R( )+L 2ln δ2 -Lipschitz.
Hellingerenjoyempiricalgeneralisationboundsinvolv- 7→ S H m
(cid:18) q (cid:19)
ing IPMs. Note that here, both f-divergences and Γ-
IPMs are not explicitly attenuated by m. This issue
Put into words, the Lipschitz constant L(m,δ) of
has already been unveiled in Viallard et al. (2023b)
the gap ∆ is upper bounded by the Rademacher
(only for Wasserstein distances), and they showed S
complexity. This also confirms the intuition that we
that this limitation couldbe attenuatedthroughdata-
could hope for beneficial statistical effects for the
dependent priors (see their ’Role of data-dependent
value of this constant, since we are dealing with a
priors’paragraphonpage5),the sameremarkapplies
generalisationgap.
hereasourboundsencompassWassersteinPAC-Bayes
bounds.
Retrieving classical Rademacher-based general-
isation bound from PAC-Bayes bounds. Using
5. Novel Connections in Statistical
Theorem5.1,weproposearigorouslinkbetweenPAC-
Learning
Bayes and Rademacher complexity in Corollary 5.2.
Combining (f,Γ)-divergences with PAC-Bayes learn- Corollary 5.2. Assume that ℓ [0,1]. With prob-
∈
ingallowsus(i)totightenexistingPAC-Bayesbounds ability at least 1 δ over m, we have for all
− S ∼ D
by interpolating and (ii) exhibit new links within gen- ρ ( ),
∈P H
eralisation theory. More precisely, we bridge the gap
between PAC-Bayes and the Rademacher complex-
E ∆ (h) 4R( )+L 2ln δ4 TV(ρ,η)
ity, showing that a Rademacher-based generalisation h ρ S ≤ H m
boundisretrievablefromTheorem3.2. Wealsoexploit ∼ (cid:18) q (cid:19)
theflexibilityofTheorem4.1tobuildabridgebetween 1 ln2
+TV(η π)+ + δ, (8)
generalisation and optimisation, through a novel gen- k 4m s2m
r
eralisationboundforheavy-tailedStochasticGradient
Descent (SGD). and in particular, for all h ,
∈H
5.1. A Rigorous Link Between PAC-Bayesian 2ln4 1 ln2
∆ (h) 4R( )+ δ + + δ. (9)
and Rademacher-based Bounds
S ≤ H s m r4m s2m
We connect the PAC-Bayes framework with an-
other subfield of statistical learning in an unex- In other words, Equation (9) shows that it is pos-
pected way. Indeed, we show that, from a PAC- sible to obtain a Rademacher-based generalisation
Bayes bound, it is possible to derive a general- bound from a PAC-Bayes one. This connection is, to
isation bound involving the Rademacher complex- the best of our knowledge, novel. However, a simi-
εity R( EH) a: r=
e
E S∼ 1D ,+m 1E ε ∼ raE nm dom m1 Rm i a= d1 eε miℓ a( ch h, ez ri) va, rw iah be lr ee
s
l ba er enlin sk hob we ntw ineen AmP iA tC et-B aa l.ye (s 20a 2n 2d
,
CV oC rod lli am ryen 8s )i .on Nh oa tes
i
∼ {− } (cid:0) P (cid:1)
5that our result directly implies a link with VC di- On the other hand, when α =2, the process becomes
mension, as Rademacher complexity involves a VC- the Brownian motion, hence having Gaussian tails.
dimension based generalisation bound (Mohri et al.,
UndercertainconditionsonFˆ ,theprocessdefinedby
2012, Chapter 3). Moreover, it has already been S
(10) is ergodic with an invariant distribution ρ corre-
shown that Rademacher-based bound implies PAC- α
sponding to the asymptotic distribution of h Wang
Bayes results. Indeed, Kakade et al. (2008) derived t
(2016); Xie & Zhang (2020). Hence, it has been of in-
a PAC-Bayes bound, starting from Rademacher com-
terest to understand the generalisation error of this
plexity,andlaterYang et al.(2019)managedtoobtain
process at stationarity, i.e., ∆ (h ) when t . In
fast-ratePAC-Bayesbounds fromshiftedRademacher S t → ∞
other words, we set ρ to be the invariant distribution
processes. In this context, Equation (9) draws addi-
of (10), and we would like to estimate ∆ (h) when
tional insights on the interplays between PAC-Bayes, S
h ρ.
Rademacher complexity, and VC dimension. ∼
Attacking this problem with existing PAC-Bayes ap-
Equation (8) goes even further and fully exploit the proaches would require directly estimating a notion
flexibility of (f,Γ)-divergences (and the two TV dis- of distance between π and ρ , such as the KL diver-
α
tances) by interpolating between two different types gence or the 1-Wasserstein distance. Unfortunately,
of generalisation bounds. More precisely, if η = ρ, we directly upper-bounding such quantities is a highly
recover a PAC-Bayes bound, meaning we aim to ex- non-trivial task since ρ does not admit an analyt-
plaingeneralisationthroughtheBayesianparadigmof ical form except for α = 2. Hence, for obtain-
a meaningful prior, available before training. On the ing generalisation bounds for these processes, alterna-
other hand, taking η = π and crudely bounding the tive approaches have been developed, mainly based
TV by 1 gives Equation (9) which fully focuses on the on algorithmic stability Raj et al. (2023a;b): and
structure of the predictor class through Rademacher fractal geometry Şimşekli et al. (2020); Dupuis et al.
complexity, ignoring prior knowledge. Then, the take- (2023); Hodgkinson et al. (2022); Lim et al. (2022);
home message is that, to understand generalisation, Dupuis & Viallard (2023).
theinformationprovidedbybothpriorknowledgeand
In this section, we will show that, thanks to Theo-
the structure of the predictor class can be simultane-
rem 4.1, we can easily provide a generalisation bound
ously exploited.
for the distribution ρ by reducing the task into two
sub-tasks that readily have solutions in the literature.
5.2. Generalisation Bounds for Heavy-tailed
More precisely, to invoke Theorem 4.1, we will design
SDEs
anintermediatedistributionη,suchthatbothW (ρ,η)
1
It has been recently shown that SGD might exhibit a and KL(η π) can be estimated by existing tools.
k
heavy-tailed behavior when a large step-size is chosen For designing such η, we consider the classical over-
Şimşekli et al. (2019); Gürbüzbalaban et al. (2021); damped Langevin SDE Roberts & Tweedie (1996):
Hodgkinson & Mahoney (2021); Pavasovic et al.
(2023) and several works have illustrated that such dh˜ = d Fˆ (h˜ )dt+σdB , h˜ , (11)
t h t t 0
heavy tails can have a direct impact on the gener- − ∇ S ∈H
alisation properties of SGD Şimşekli et al. (2020);
where B denotes the standard Brownian motion. We
Barsbey et al. (2021). t
then choose η as the invariant measure of this SDE.
One fruitful approach for analysing the heavy-tailed
Thanks to this choice for η, we can now appeal to
behavior in SGD has been based on modelling the
existing results for invoking Theorem 4.1: (i) very re-
SGD recursion by using a heavy-tailed continuous-
cently,Deng et al.(2023)provedupper-boundsonthe
time dynamics that is expressed by the following
Wasserstein-1 distance between ρ and η, (ii) whereas
stochastic differential equation (SDE) Şimşekli et al.
the KL-divergence between η and appropriate priors
(2020); Raj et al. (2023a;b); Dupuis & Viallard
π has also been investigated in PAC-Bayesian frame-
(2023):
works, see e.g., Mou et al. (2018). These observations
dh = d Fˆ (h )dt+σdLα, h , (10) are formalised in the next corollary.
t − ∇h S t t 0 ∈H
where = Rd, Fˆ (h) = Rˆ (h)+λ h 2 is the regu- Corollary 5.3. Let π be a data-free prior and δ
larisedH empirical rS isk with sS ome λ >k 0k , σ R and (0,1). Assume ℓ [0,1]. Assume that, for an∈ y
L deα t finis eda r io nta Ati po pn ea nll dy ixsy Am ,m wet hr ii cc hα i- sst aab rl ae nL dé ov m∈ y p pr+ ro oc ce es ss s, Lδ ′ (m∈ ,δ(0 ), -L1) ip, sw chit ith z.p Ar∈ o sb sa ub mili ety th1 at− δ Fˆ′, h
is
→ quas∆ i-2 S sm(h o) oti hs
′
∇ S
whom parameter α (0,2) characterises the heavy- and regular ((H1) and (H2) in Appendix C.7). Thus,
∈
tailed behaviour: the smaller α, the heavier the tail. for any α (1,2), with probability at least 1 δ over
0
∈ −
6m, we have, for any α [α ,2], where ( )and ( )arethesetsofproba-
0 ρ η
S ∼D ∈ C ⊆P H C ⊆P H
bilitydistributionsforρandπ (tobeprecisedfurther).
E ∆ (h) Thanks to this optimisation problem, we are able to
h ∼ρα S ≤ find a posterior distribution ρ and intermediate dis-
KL(η π)+ln4√m tribution η that minimise (approximately) the bound.
sL(m,δ/2)C α0f(α,d)+ k
2m
δ , Hence,thisalgorithmisexpectedtoexploitsimultane-
ously the strengths of KL and Wasserstein.
However, to compute the bound, it remains (i) to ap-
where f(α,d)=dln(d)(2 −α)ln(1/2 −α) and C
α0
>0.
proximate L(m,δ/2) and (ii) to determine the sets
ρ
C
This result provides a novel high-probability generali- and η on which we can compute the Wasserstein dis-
C
sation bound for heavy-tailed SDEs with minimal ef- tance and the KL divergence. To do so, we perform
fort. Note that (2 α)log(1/(2 α)) α 1, this experimentsonmodelshw parametrisedbyavec-
factorthenattenuat− esthevalueof− C ,a≤ sitsc− alespro- tor w Rd. This allows u∈ s H to consider two types of
α0 ∈
p Ho er rt ei ,o tn ha elly Kt Lo (η1/ ,α π0 )− t1 e, rmacc co ar ndi bn eg ut po pD ere -n bg ouet nda el. d( b2 y02 u3 s) -. p ceo ns tt ee rr eio dr od nist tr hi ebu wti eo ign hs: tsa wD air na dc adis Gtr ai ub su st ii ao nn dρ is= tribδ uw -
ing Mou et al. (2018) and is attenuated by m. Fur- tion ρ = (w,σ2I d), where σ is the standard devia-
thermore, while implicit, the impact of the dimension tionandIN d is the identity matrixinRd ×d. The metric
is also attenuated by m through L(m,δ) and Theo- d (hw,hw′)= w w ′ becomes the Euclidean one.
H k − k
rems 5.1 and 6.1.
Computing the Lipschitz constant L(m,δ). Un-
Furthermore, notice that Corollary 5.3 is strictly bet- fortunately, Theorem 5.1 is suited only for the Kro-
ter than considering a single Wasserstein distance in necker distance and is not estimable because the
Theorem4.1(i.e., W (ρ,π))andapplyingthetriangle Rademacher complexity involves the expectation on
1
inequality so that we would need to estimate W (ρ,η) m and ε Em. Therefore, in the next theo-
1 S ∼ D ∼
and W (η,π). Indeed, by doing so, we would lose the rem, we prove an additional bound on the Lipschitz
1
convergence rate of 1/m, which attenuates the KL di- constant (for any metric d ) of the gap that we can
H
vergence,andhence givesarbitrarilymoreimportance estimate.
to the prior. Theorem 6.1. For any hypothesis set , for any L-
Lipschitz loss ℓ : R, with probaH bility at least
6. Experimental Study 1 δ over H m× anZ d ε→ Em
− S ∼D ∼
In the following section, we first presentin Section 6.1 h ∆ (h) is L(m,δ)= 2Rε ( )+3L 2ln4 δ -Lipschitz,
a learning algorithm to minimise the bound of Theo- 7→ S (cid:18) S H q m (cid:19)
rem 4.1 while Section 6.2 introduces the setting and
the results. where R Sε( H):=sup h 6=h′
∈H
m1 m i=1ε i[ℓ(h′ d,z Hi () h− ,ℓ h( ′h ),z i)] .
Theorem 6.1 paves the way tPo the approximation of
6.1. A Novel Learning Algorithm
the Lipschitz constant by mini-batch stochastic opti-
Many PAC-Bayesian bounds, including those of Sec- misation. Indeed,ateachiteration,wehavetosample
tion 4, have the great advantage to be fully empirical. a mini-batch and update the vector w and w
′
B ⊆ S
Thus, minimising such bounds yields theory-driven by maximizing
learning algorithms. More precisely, by rearranging
the terms in our results, we can upper-bound the (ex-
|B1
|
z
i∈Bε i[ℓ(hw′ k,z wi) −−wℓ( ′h kw,z i)],
pected) population risk R (h) by the (expected) em-
pirical risk Rˆ (h) alongsidD e the term containing the where ε i has bePen previously drawn. Moreover, The-
orem 6.1 allows going beyond Amit et al. (2022, The-
S
complexitymeasuresofinterest. Inthissection,wefo-
orem 12). Compared to their theorem, Theorem 6.1
cus on the minimisation of the bound in Theorem 4.1;
does not require the hypothesis set to be finite,
the considered optimisation problem is H
while still being a computable constant. Moreover,
our theorem allows us to bypass more assumptions
min E Rˆ (h) about the learning problem, as is done in Theorem
ρ ∈Cρ, η ∈Cη(h ∼ρ S 14 of Amit et al. (2022) for linear regression.
4√m Computing a tractable Wasserstein and KL di-
KL(η π)+ln
+ rL(m,δ/2)W 1(ρ,η)+ k
2m
δ ), (12) v tae nr cg een Wce (. ρ,I ηn )o ar nd der tt ho ec Kom Lp du it ve ert gh ee ncW ea Kss Le (r ηste πin ),d wis e-
1
k
7have to define the set of probability distributions (w.r.t. h[1],...,h[ ]). We set α = 25 for the linear
ρ
C |Y|
and . Note that restricting the sets ( ) and models andα=250forthe neuralnetworks. The con-
η ρ
C C ⊂P H
( ) provides an upper-bound on the infimal fidence parameter is fixed to δ =0.05.
η
C ⊂ P H
convolution formula, recalled in Equation (3). We re- Datasets. Weconsider3datasets(Yeast,Phishing,
strictthepriordistributionπandtheintermediatedis- Mushrooms) from the UCI repository (Dua & Graff,
tribution η to be two Gaussian distributions, i.e., we 2017), MNIST (LeCun, 1998) and FashionMNIST
have π = (w ,I σ2) and η = (w ,I σ2), where (Xiao et al., 2017). Moreover, for MNIST and Fash-
w and wN areπ thd e π means whileN σ aη ndd ση are the ionMNIST, we keep the original training and test
π η π η
S
standarddeviationsoftheGaussians. Thankstothese set , while for the UCI datasets, we perform a
two definitions, the KL divergence is given by 50%T /50% split. We denote by Rˆ (h), the test risk
T
ofh(computedonthetestset )toestimatethepop-
KL(η π)=1 σ η2 d d+ 1 w w 2+dln σ π2 . ulation risk R (h). T
k 2 "σ π2 − σ π2k η − π k2 (cid:18)σ η2 (cid:19)# OptimisationD . To solve all our optimisation
problems, we use the COCOB-Backprop optimiser
The value of the Wasserstein distance depends on the
(Orabona & Tommasi, 2017); its parameteris fixed to
posterior distribution we are considering. When the
10. Moreover,foreachoptimisation,weoptimise(with
posterior ρ is a Dirac, we have
a batch size of 256) for at least 10000 iterations (and
finish the epoch when the number of iterations is at-
W 1(δw,η) ≤W 1(δw,δw η)+W 1(δw η,η)
tained).
= kw −w η k2+E ε ∼N(0,Idσ η2) kε k Models. We instantiate the algorithm described in
= w w +σ √2Γ((d+1)/2) , Equation(12)forlinearclassifiersandneuralnetworks.
k − η k2 η Γ(d/2) We first define a linear model by hw(x) := Wx+b,
where the weight matrix and the bias are respectively
where Γ( ·) is the gamma function. The inequality fol- defined by W R |Y|×n and b R |Y|; we denote by
lowsfromthetriangleinequality,andthe lastequality w = vec( W,b∈ ) the vectorisat∈ ion of all the param-
follows by the mean of the Chi distribution. Further- { }
eters. We know from Viallard et al. (2023b, Lemma
more, when ρ is a Gaussian distribution, we first use
8) that the loss is √2α-Lipschitz w.r.t. the parame-
Jensen’sinequality to obtainW 1(ρ,η) ≤W 2(ρ,η) and ters w. The initialisation of W and b is done with
then use the closed form solution of the Wasserstein
zeros. For neural networks, we define the model by
distance (of order 2) between two Gaussian distribu- hw(x) := WhK( h1(x))+b, which is composed of
tions. ···
K layers h1(),...,hK(). Similarly to linear mod-
Learning η. To simplify the optimisation of η, we els, W R |Y· |×N and · b RN are the weight ma-
∈ ∈
define its mean by w = λw+(1 λ)w and σ2 = trix and the bias of the last layer. Moreover, the
λσ2+(1 λ)σ2 forthη e Gaussiancas− e,wheπ reλ [0η ,1] i-th layer hi, composed of N nodes, is defined by
i is na thle ea Drn i− re ad cp ca aπ r sa em
;
het ee nr c. eN
,
io tte ist lh ea at rnσ eη2 dis dun ro it ngre ts ht∈ r ea oin pe td
i-
h ani( dx) th:=
e
P bir ao sj(L beaky( RW Nix a+ reb i i) t) s, ww eh ie gr he tW mi
a∈
triR xN a× nN
d
i
misation. bias respectively; ∈ Leaky : RN RN is the Leaky
→
ReLU applied element-wise and Proj project the vec-
6.2. Experiments1 tor Leaky(W ix+b i) in the unit ℓ 2-ball. The weights
w=vec( W,W ,...,W ,b,b ,...,b )representthe
K 1 K 1
In this section, we propose to evaluate our algo- { }
vectorisation of all parameters of the network. Note
rithm on linear classifiers and neural networks on var-
that Viallard et al. (2023b, Lemma 9) show the loss
ious datasets. To do so, we follow the setting of is Lipschitz w.r.t. the parameters w. However, the
Viallard et al. (2023b).
Lipschitz constant is not explicit, thus, we provide it
Setting. We stand in the classification setting where
inLemmaD.1. FollowingViallard et al.(2023b),each
the data space is = , with = x
parameterofthematricesis(i)initialisedwithaGaus-
Rn x 1 the inZ put spX ac× eY and =X 1,...{ , ∈
2 siandistributioncenteredonzeroandwithastandard
|k k ≤ } Y { |Y|}
the label space. Hence, our goal is to learn a model
deviation of 0.04 and (ii) clipped between 0.08 and
hw : R
|Y|
that outputs scores given x ;
+0.08. Each parameterin the biases is
initia−
lised with
we denX ote→ by hw(x)[y ′] R the score for each∈ labX el
a zero except the parameter in b , which is set to 0.1.
∈ 1
y . Based on this score, we define our α-Lipschitz
′ Lastly, we further set N =600 and K =1.
loss ℓ(h,(x,y)) = 1 max(0,1 α(h[y] h[y ]))
y′=y − − ′ Empirical findings. We show in Table 1 the results
|Y| 6
of the bound’s minimisation; for more details, we re-
1WeintroducemoredPetailsandadditionalexperiments
fer the reader to Appendix D. As we can remark, the
in AppendixD.
8Table 1: Results of the bound minimisation of Theorem 4.1 (see Equation (12)) and the ones associated with
Amit et al. (2022) and Maurer (2004). “Test” is the test risk Rˆ (hw), “Bnd” represents the bound value, “Wass”
T
represents the upper bound of the Wasserstein distance multiplied by the Lipschitz constant, and “KL” is the
KL divergence divided by 2m.
Theorem4.1 Amitetal.(2022) Theorem4.1 Amitetal.(2022)
Test Bnd Wass. KL Test Bnd Wass. Test Bnd Wass. KL Test Bnd Wass.
FashionMNIST 0.1150.317 0.017 0.025 0.361 1.040 0.465 0.162 0.683 0.135 0.139 0.884 3.537 7.041
MNIST 0.0770.294 0.018 0.027 0.304 1.078 0.583 0.111 0.673 0.152 0.158 0.776 2.414 2.680
Mushrooms 0.0260.190 0.009 0.015 0.498 0.614 0.012 0.082 0.645 0.167 0.148 0.487 3.832 11.247
Phishing 0.0850.225 0.005 0.013 0.497 0.569 0.004 0.123 0.642 0.132 0.136 0.438 1.903 2.095
Yeast 0.3530.566 0.014 0.017 0.504 1.203 0.482 0.335 0.707 0.061 0.059 0.383 2.469 4.318
(a) Linear models hw with ρ=δw (b) Neural networks hw with ρ=δw
Theorem4.1 Maurer(2004) Theorem4.1 Maurer(2004)
Test Bnd Wass. KL Test Bnd KL Test Bnd Wass. KL Test Bnd KL
FashionMNIST 0.1400.364 0.000 0.019 0.140 0.359 0.019 0.663 1.132 0.001 0.143 0.647 1.115 0.147
MNIST 0.0970.338 0.000 0.021 0.096 0.333 0.021 0.610 1.105 0.001 0.160 0.623 1.105 0.154
Mushrooms 0.0170.265 0.000 0.022 0.035 0.258 0.016 0.384 0.935 0.001 0.209 0.376 0.934 0.220
Phishing 0.0940.302 0.000 0.013 0.094 0.297 0.013 0.287 0.972 0.001 0.348 0.243 0.813 0.232
Yeast 0.3720.644 0.000 0.016 0.372 0.638 0.016 0.648 2.295 0.003 2.342 0.652 2.335 2.472
(c) Linear models hw with ρ= (w,σ2I d) (d)Neuralnetworkshw withρ= (w,σ2I d)
N N
behaviourofourTheorem4.1’sbounddifferswhenwe Broader Impact Statement
consider a Dirac or a Gaussian posterior distribution
Thispaperexploresanovelstrategytounderstandand
ρ. For instance, in the Dirac case, our bounds and
analyse theoretical properties of machine learning al-
test risks are lower than the one of Amit et al. (2022).
gorithms, and notably generalisation. As such, we do
This is due to the fact that the minimisation of our
not anticipate any immediate or longer-term negative
bound allows us to learn η that minimises better the
societal impact – however, we believe a better theo-
KLdivergenceandtheWassersteindistance. However,
retical understanding of machine learning ultimately
in the Gaussian case, the bound values and the test
contributes to a more virtuous use and deployment of
risksaresimilar,illustratingthatwhenρ,π,andη are
AI systems.
Gaussians,interpolating between the Wassersteinand
the KL does not bring any advantage. Indeed, our
algorithm puts almost no weight on the Wasserstein Acknowledgements
here and the value of the KL divergence is then simi-
Paul Viallard and Umut Şimşekli are partially
lar with the algorithm based on the bound of Maurer
supported by the French program “Investissements
(2004). Hence, in this case, Theorem 4.1’s bound be-
d’avenir” ANR-19-P3IA-0001 (PRAIRIE 3IA Insti-
comes the same as the one of Maurer (2004) since
tute). Umut Şimşekli is also supported by the Eu-
ρ=η. Inanycase,resultsshowthatinterpolatingKL
ropean Research Council Starting Grant DYNASTY
and Wasserstein brings the best-of-both worlds with
– 101039676. Benjamin Guedj acknowledges partial
increased performance w.r.t. Wasserstein method for
support from the French Research Agency through
Diracs and equivalent performance w.r.t. KL method
the programme “France 2030” and PEPR IA on grant
for the Gaussian case.
SHARP ANR-23-PEIA-0008.
7. Conclusion
References
We derived novel PAC-Bayes bounds based on (f,Γ)-
Alquier, P. User-friendly Introduction to PAC-Bayes
divergences. We show its theoretical interest for
Bounds. Foundations and Trends® in Machine
analysing the generalisation of SGD and retrieving
Learning, 2024.
Rademacher-based bounds. We also provide non-
trivialgeneralisationboundsfortwomodelsandbring
tighter bounds in the Dirac case. Alquier, P. and Guedj, B. Simpler PAC-Bayesian
bounds for hostile data. Machine Learning, 2018.
9Alquier,P., Ridgway,J.,andChopin,N. Onthe prop- Chugg, B., Wang, H., and Ramdas, A. A uni-
erties of variational approximations of Gibbs poste- fied recipe for deriving (time-uniform) PAC-Bayes
riors. Journal of Machine Learning Research, 2016. bounds. arXiv, abs/2302.03421,2023.
Ambroladze, A., Parrado-Hernández, E., and Shawe- Cover,T.M.andThomas,J.A. Elements of Informa-
Taylor,J. TighterPAC-BayesBounds. In Advances tion Theory. Wiley, 2001.
in Neural Information Processing Systems (NIPS),
Csiszár, I. I-Divergence Geometry of Probability Dis-
2006.
tributions and Minimization Problems. The Annals
of Probability, 1975.
Amit, R. and Meir, R. Meta-Learning by Adjust-
ing Priors Based on Extended PAC-Bayes Theory. Deng, C., Schilling, R. L., and Xu, L. Wasserstein-1
In International Conference on Machine Learning distance between SDEs driven by Brownianmotion
(ICML), 2018. and stable processes, 2023.
Amit, R., Epstein, B., Moran, S., and Meir, R. In- Ding, N., Chen, X., Levinboim, T., Goodman, S., and
tegral Probability Metrics PAC-Bayes Bounds. In Soricut, R. Bridgingthe GapBetweenPractice and
Advances in Neural Information Processing Systems PAC-Bayes Theory in Few-Shot Meta-Learning. In
(NeurIPS), 2022. Advances in Neural Information Processing Systems
(NeurIPS), 2021.
Barsbey,M.,Sefidgaran,M.,Erdogdu,M.A.,Richard,
Donsker, M. D. and Varadhan, S. R. S. Asymptotic
G., and Şimşekli, U. Heavy Tails in SGD and Com-
evaluation of certain Markov process expectations
pressibility of Overparametrized Neural Networks.
for large time—III. Communications on Pure and
In Advances in Neural Information Processing Sys-
Applied Mathematics, 1976.
tems (NeurIPS), 2021.
Dua, D. and Graff, C. UCI Machine Learning Reposi-
Bartlett,P.andMendelson,S. RademacherandGaus-
tory, 2017.
sian Complexities: Risk Bounds and Structural Re-
sults. In Conference on Computational Learning Dupuis, B.andViallard,P. FromMutualInformation
Theory (COLT), 2001. toExpectedDynamics: NewGeneralizationBounds
for Heavy-Tailed SGD. arXiv, abs/2312.00427,
Bartlett,P.andMendelson,S. RademacherandGaus- 2023.
sian Complexities: Risk Bounds and Structural Re-
sults. Journal of Machine Learning Research, 2002. Dupuis, B., Deligiannidis, G., and Şimşekli, U. Gen-
eralization bounds using data-dependent fractal di-
Bégin, L., Germain, P., Laviolette, F., and Roy, J. mensions. In International Conference on Machine
PAC-Bayesian Bounds based on the Rényi Diver- Learning (ICML), 2023.
gence. In International Conference on Artificial In-
Dziugaite, G. K.andRoy, D. Computing Nonvacuous
telligence and Statistics (AISTATS), 2016.
GeneralizationBoundsforDeep(Stochastic)Neural
Networks with Many More Parameters than Train-
Birrell, J., Dupuis, P., Katsoulakis, M., Pantazis, Y.,
ingData. InConference onUncertaintyinArtificial
and Rey-Bellet, L. (f,Γ)-Divergences: Interpolat-
Intelligence (UAI), 2017.
ing between f-Divergences and Integral Probability
Metrics. Journal of Machine Learning Research, 23,
Dziugaite, G. K., Hsu, K., Gharbieh, W., Arpino, G.,
2022.
and Roy, D. On the role of data in PAC-Bayes
bounds. In International Conference on Artificial
Boucheron,S.,Lugosi,G.,andMassart,P. Concentra-
Intelligence and Statistics (AISTATS), 2021.
tion Inequalities - A Nonasymptotic Theory of Inde-
pendence. Oxford University Press, 2013. Fard, M. M. and Pineau, J. PAC-Bayesian Model Se-
lection for Reinforcement Learning. In Advances
Broniatowski, M. and Keziou, A. Minimization of di- in Neural Information Processing Systems (NIPS),
vergences on sets of signed measures. Studia Scien- 2010.
tiarum Mathematicarum Hungarica, 2006.
Farid, A. and Majumdar, A. Generalization Bounds
Catoni, O. PAC-Bayesian supervised classification: for Meta-Learningvia PAC-BayesandUniformSta-
the thermodynamics of statistical learning. Institute bility.InAdvancesinNeuralInformationProcessing
of Mathematical Statistics, 2007. Systems (NeurIPS), 2021.
10Germain, P., Lacasse, A., Laviolette, F., and Marc- LeCun,Y.TheMNISTdatabaseofhandwrittendigits,
hand,M. PAC-Bayesianlearningoflinearclassifiers. 1998.
In International Conference on Machine Learning
(ICML), 2009. Lim, S. H., Wan, Y., and Şimşekli, U. Chaotic regu-
larization and heavy-tailed limits for deterministic
Gibbs, A. L. and Su, F. E. On Choosing and Bound- gradient descent. Advances in Neural Information
ingProbabilityMetrics.InternationalStatisticalRe- Processing Systems (NeurIPS), 2022.
view, 2002.
Lindvall, T. Lectures on the coupling method. Wi-
Guedj, B. A Primer on PAC-Bayesian Learning. In ley seriesinprobabilityandmathematicalstatistics.
Proceedings of the second congress of the French Wiley, 1992.
Mathematical Society, 2019.
Maurer, A. A note on the PAC-Bayesian theorem.
Gürbüzbalaban, M., Şimşekli, U., and Zhu, L. The
arXiv, cs/0411099,2004.
heavy-tail phenomenon in SGD. In International
Conference on Machine Learning (ICML), 2021. McAllester, D. Some PAC-Bayesian Theorems. Ma-
chine Learning, 1999.
Haddouche, M. and Guedj, B. Online PAC-Bayes
Learning. In Advances in Neural Information Pro- McAllester, D. Pac-bayesian stochastic model selec-
cessing Systems (NeurIPS), 2022. tion. Machine Learning, 2003.
Haddouche, M. and Guedj, B. PAC-Bayes Generali-
McAllester, D. A PAC-Bayesian Tutorial with A
sation Bounds for Heavy-Tailed Losses through Su-
Dropout Bound. arXiv, abs/1307.21181,2013.
permartingales. Transactions on Machine Learning
Research, 2023a. Mohri,M.,Rostamizadeh,A.,andTalwalkar,A.Foun-
dationsofMachineLearning.Adaptivecomputation
Haddouche,M.andGuedj,B. WassersteinPAC-Bayes
and machine learning. MIT Press, 2012.
Learning: ABridgeBetweenGeneralisationandOp-
timisation. arXiv, abs/2304.07048,2023b.
Mou, W., Wang, L., Zhai, X., and Zheng, K. General-
izationboundsofsgldfornon-convexlearning: Two
Haddouche,M.,Guedj,B.,Rivasplata,O.,andShawe-
theoretical viewpoints. In Conference on Learning
Taylor, J. PAC-Bayes Unleashed: Generalisation
Theory, 2018.
Bounds with Unbounded Losses. Entropy, 23, 2021.
Neal, R. M. Bayesian learning for neural networks.
Hellström, F., Durisi, G., Guedj, B., and Raginsky,
Springer Science & Business Media, 2012.
M. Generalization bounds: Perspectives from in-
formation theory and PAC-Bayes. arXiv preprint
Nguyen,X.,Wainwright,M.,andJordan,M. Estimat-
arXiv:2309.04381, 2023.
ingDivergenceFunctionalsandtheLikelihoodRatio
by Convex Risk Minimization. IEEE Transactions
Hodgkinson,L.andMahoney,M. Multiplicative noise
on Information Theory, 2010.
and heavytails in stochastic optimization. In Inter-
national Conference on Machine Learning (ICML),
Ohnishi, Y. and Honorio, J. Novel Change of Mea-
2021.
sureInequalitieswithApplicationstoPAC-Bayesian
Hodgkinson, L., Şimşekli, U., Khanna, R., and Ma- Bounds and Monte Carlo Estimation. In Inter-
honey, M. Generalization bounds using lower tail national Conference on Artificial Intelligence and
exponentsinstochasticoptimizers. InInternational Statistics (AISTATS), 2021.
Conference on Machine Learning (ICML), 2022.
Orabona,F.andTommasi,T.TrainingDeepNetworks
Kakade, S. M., Sridharan, K., and Tewari, A. On without Learning Rates Through Coin Betting. In
the Complexity of Linear Prediction: Risk Bounds, Advances in Neural Information Processing Systems
Margin Bounds, and Regularization. In Advances (NIPS), 2017.
in Neural Information Processing Systems (NIPS),
2008. Parrado-Hernández, E., Ambroladze, A., Shawe-
Taylor,J.,andSun,S. PAC-bayesboundswithdata
Kuzborskij, I. and Szepesvári, C. Efron-Stein PAC- dependent priors. Journal of Machine Learning Re-
BayesianInequalities. arXiv, abs/1909.01931,2019. search, 2012.
11Pavasovic, K. L., Durmus, A., and Şimşekli, U. Seldin, Y., Laviolette, F., Shawe-Taylor,J., Peters, J.,
Approximate Heavy Tails in Offline (Multi-Pass) andAuer,P. PAC-BayesianAnalysisofMartingales
StochasticGradientDescent. InAdvances in Neural and Multiarmed Bandits. arXiv, abs/1105.2416,
Information Processing Systems (NeurIPS), 2023. 2011.
Pérez-Ortiz, M., Rivasplata, O., Guedj, B., Gleeson, Seldin, Y., Laviolette, F., Cesa-Bianchi, N., Shawe-
M.,Zhang,J.,Shawe-Taylor,J.,Bober,M.,andKit- Taylor, J., and Auer, P. PAC-Bayesian Inequalities
tler, J. Learning PAC-Bayes priors for probabilistic forMartingales. IEEE Transactions on Information
neural networks. 2021a. Theory, 2012.
Pérez-Ortiz, M., Rivasplata, O., Parrado-Hernandez, Şimşekli, U., Sagun, L., and Gürbüzbalaban, M. A
E., Guedj, B., and Shawe-Taylor, J. Progress in Tail-Index Analysis of Stochastic Gradient Noise in
Self-Certified Neural Networks. In NeurIPS 2021 DeepNeuralNetworks. InInternational Conference
Workshop on Bayesian Deep Learning, 2021b. on Machine Learning (ICML), 2019.
Pérez-Ortiz,M.,Rivasplata,O.,Shawe-Taylor,J.,and Şimşekli, U., Sener, O., Deligiannidis, G., and Er-
Szepesvári, C. Tighter Risk Certificates for Neural dogdu, M. A. Hausdorff Dimension, Heavy Tails,
Networks. Journal of Machine Learning Research, and Generalization in Neural Networks. In Ad-
22, 2021c. vances in Neural Information Processing Systems
(NeurIPS), 2020.
Picard-Weibel, A. and Guedj, B. On change of
measure inequalities for f-divergences. arXiv, Tolstikhin,I.O.andSeldin, Y. PAC-Bayes-Empirical-
abs/2202.05568,2022. Bernstein Inequality. In Advances in Neural Infor-
mation Processing Systems (NeurIPS), 2013.
Raj, A., Barsbey, M., Gürbüzbalaban, M., Zhu, L.,
Vapnik, V. and Chervonenkis, A. Theory of pattern
and Şimşekli, U. Algorithmic stability of heavy-
recognition, 1974.
tailed stochastic gradient descent on least squares.
In International Conference on Algorithmic Learn-
Vapnik,V.N. The Nature of Statistical Learning The-
ing Theory (ALT), 2023a.
ory, Second Edition. Statistics for Engineering and
Information Science. Springer, 2000.
Raj, A., Barsbey, M., Gürbüzbalaban, M., Zhu,
L., and Şimşekli, U. Algorithmic Stability of
Viallard, P., Vidot, G., Habrard, A., and Morvant, E.
Heavy-TailedStochastic Gradient Descent on Least
A PAC-Bayes Analysis of Adversarial Robustness.
Squares. In International Conference on Algorith-
In Advances in Neural Information Processing Sys-
mic Learning Theory (ALT), 2023b.
tems (NeurIPS), 2021.
Rivasplata, O., Kuzborskij, I., Szepesvári, C., and Viallard, P., Germain, P., Habrard, A., and Morvant,
Shawe-Taylor, J. PAC-Bayes analysis beyond the E. A generalframeworkfor the practicaldisintegra-
usualbounds. Advances in Neural Information Pro- tion of PAC-Bayesian bounds. Machine Learning,
cessing Systems (NeurIPS), 2020. 2023a.
Roberts, G. O. and Tweedie, R. L. Exponential con- Viallard, P., Haddouche, M., Şimşekli, U., and Guedj,
vergenceofLangevindistributionsandtheirdiscrete B. Learning via Wasserstein-Based High Probabil-
approximations. Bernoulli, 1996. ity Generalisation Bounds. In Advances in Neural
Information Processing Systems (NeurIPS), 2023b.
Rothfuss, J., Fortuin, V., Josifoski, M., and Krause,
A. PACOH:Bayes-optimalmeta-learningwithPAC- Wang,J. Lp-Wassersteindistanceforstochasticdiffer-
guarantees.InInternationalConferenceonMachine entialequationsdrivenbyLévyprocesses.Bernoulli,
Learning (ICML), 2021. 2016.
Rothfuss, J., Josifoski, M., Fortuin, V., and Krause, Xiao, H., Rasul, K., and Vollgraf, R. Fashion-MNIST:
A. PAC-Bayesian Meta-Learning: From Theory to a Novel Image Dataset for Benchmarking Machine
Practice. arXiv, abs/2211.07206,2022. Learning Algorithms, 2017.
Sakhi, O., Alquier, P., and Chopin, N. PAC- Xie,L.andZhang,X. Ergodicityofstochasticdifferen-
Bayesian Offline Contextual Bandits With Guaran- tial equations with jumps and singular coefficients.
tees.InInternationalConferenceonMachineLearn- Annales de l’Institut Henri Poincaré, Probabilités et
ing (ICML), 2023. Statistiques, 2020.
12Yang,J.,Sun,S.,andRoy,D.M.Fast-ratePAC-Bayes
GeneralizationBoundsviaShiftedRademacherPro-
cesses. In Advances in Neural Information Process-
ing Systems (NeurIPS), 2019.
13The appendix is organisedas follows:
1. We provide supplementary backgroundon α-stable Lévy processes in Appendix A ;
2. Appendix B introduces additionaltheoreticalresults. Moreprecisely,we introduce additionalgenericgener-
alisationbounds in Appendix B.1 andwe provideanother applicationof Theorem 4.1 for heavy-tailedSGD
in Appendix B.2 ;
3. Appendix C contains all the postponed proofs ;
4. Additional details on the experiments are gathered in Appendix D.
A. Supplementary Background on α-stable Lévy Processes
We first define α-stable Lévy processes properly, re-using the definition of Şimşekli et al. (2019).
DefinitionA.1(Symmetricα-stableLévyprocess). Asymmetricα-stableLévyprocessindimensiond,denoted
as Lα), is constituted of independent components (meaning each component of Lα is an independent α-stable
t t
Lévy motion in R). For the scalar case, it is defined as follows for α (0,2] :
∈
• Lα =0 almost surely.
0
• For t <t < <t , the increments (Lα Lα ) are independent (for i=1,...,N).
0 1 ··· N ti − ti−1
• Thedifference(Lα Lα)andLα followthedistribution α (t s)1/α fors<t. Here, α distributions
are defined
throut gh− ths
eir
charat −cts
eristic function via X
S αS −E[exp(iωX)]=exp( σS σωS
α).
∼S S(cid:0)⇔ (cid:1) − | |
• Lα is continuous in probability (it has stochastically continuous sample paths), i.e., for all δ >0 and s 0,
wt e have p( Lα Lα >δ) 0 as t s. ≥
| t − s| → →
Moreover,when α=2, we know that Lα coincides with a scaled version of Brownian motion, √2 B .
t t
B. Supplementary Results
B.1. Improving on existing PAC-Bayes bounds using (f,Γ)-divergences
This section introduces the claims made below Theorem 4.1: it is possible to improve on many bounds in the
literature atthe costof a Lipschitz bounded assumptionby using the same prooftechnique as Theorem4.1. We
state and prove those results.
Theorem B.1. Assume that ℓ [0,1]. Assume that, for any δ (0,1), with probability 1 δ , the loss ℓ(,z) is
′ ′
L(m,δ)-Lipschitz for all z .∈ Thus, for any data-free prior π,∈ any λ > 0, with probabili− ty at least 1 δ over
∈Z −
m, we have for all ρ ( ), any η ( ),
S ∼D ∈P H ∈P H
(i) – Catoni’s bound:
KL(η,π)+log 2 λ
E R (h) R (h) 2L(m,δ/2)W(ρ,η)+ δ + ;
h ∼ρ D − S ≤ λ (cid:0) (cid:1) 2m
(ii) – Supermartingale bound:
KL(η,π)+log 2 λ
E R (h) R (h) (2+λ)L(m,δ/2)W(ρ,η)+ δ + E E [ℓ2(h,z)];
h ∼ρ D − S ≤ λ (cid:0) (cid:1) 2 h ∼ρz ∼D
(iii) – Catoni’s bound with fast rate:
1 KL(η,π)+log 2
E R (h) E Rˆ (h)+(2+λ)L(m,δ/2)W(ρ,η)+ δ .
h ∼ρ D ≤ 1 − λ 2 h ∼ρ S λm (cid:0) (cid:1)!
14Under our assumptions, Catoni’s bound is an improvement of Alquier et al. (2016, Theorem 4.1), the super-
martingale bounds improves Haddouche & Guedj (2023a); Viallard et al. (2023b), and the Catoni’s fast rate
bound improves on McAllester (2013, Theorem 2).
Proof. We first start from Equation (14) in Theorem 4.1’s proof alongside with the infimal convolution formula
(4). Assume that φ Lipα(m,δ) with probability at least 1 δ/2. Then, with probability at least 1 δ, for any
S ∈ b − −
ρ,η we have
2
E [φ (h)] α(m,δ)W (ρ,η)+KL(η π)+ln +ln E E eφS(h) . (13)
1
h ρ S ≤ k δ h π m
∼ (cid:20) ∼ S∼D (cid:21)
Note that we use that π was data-free to swap the integrals in the last term.
Catoni’s bound. Weuse(13)withφ =λ(R Rˆ ),andα(m,δ)=λL(m,δ/2),thenwehave,withprobability
S D− S
at least 1 δ, for any ρ,η,
−
KL(η π)+ln2 1
E R Rˆ (h) L(m,δ/2)W 1(ρ,η)+ k δ+ ln E E eλ∆S(h) .
h ρ D− S ≤ λ λ h π m
∼ h i (cid:20) ∼ S∼D (cid:21)
As is i.i.d., applying Hoeffding’slemma m times onthe randomvariablesR (h) ℓ(h,z ) [ 1,1],givesthat
D i
S − ∈ −
for any h, the inequality E S∼DmeλR D−Rˆ S(h)
≤
2λ m2 . This concludes the proof.
Supermartingale bound. We use Equation (13) with φ (h) = mλR (h) Rˆ (h) mλ2 E z [ℓ(h,z)2], and
S D − S − 2 ∼D
α(m,δ)=2m λ+ λ 22 L(m,δ/2). Indeed,becauseℓisL
1
:=L(m,δ/2)withprobabilityatleast1 −δ/2,R D−Rˆ
S
is 2L 1-Lipschit(cid:16)z and h(cid:17) E z [ℓ(h,z)2] is 2L 1 Lipschitz. Then, applying (13) and dividing by mλ gives, with
→ ∼D
probability at least 1 δ, for any ρ,η,
−
KL(η π)+ln2 1 λ
E R (h) Rˆ (h) (2+λ)L(m,δ/2)W(ρ,η)+ k δ+ ln E E eφS(h) + E E [ℓ2(h,z)].
h ρ D − S ≤ mλ mλ h π m 2 h ρz
∼ h i (cid:20) ∼ S∼D (cid:21) ∼ ∼D
Then, Chugg et al. (2023) proved in their corollary 4.8 that E
h
π[eφS(h)] is a supermartingale with respect to
the data z i,i 1 and an adapted filtration. In particular, beca∼ use ℓ is non-negative, E mE
h
πeφS(h) 1.
≥ S∼D ∼ ≤
This concludes the proof.
Catoni’sfastrate. Westartfromthesupermartingaleboundandnoticethat,becausethelossliesin[0;1],ℓ2
≤
ℓ. Upper-bounding the last term of the supermartingale bound and re-organising the terms conclude the proof.
B.2. Additional result for heavy-tailed SGD: going beyond continuous processes.
In a similar spirit to Section 5.2, we show that it is possible to provide sound generalisation bounds for heavy-
tailed SGD when discrete modelisation is involved instead of continuous SDEs.
Modelling heavy-tailed SGD beyond continuous processes. Corollary 5.3 gives quantitative results on
the generalisationability ofthe asymptotic heavy-taileddistribution ρ andthus fills a gapbetween PAC-Bayes
α
learningandcontinuousapproximationsofSGD.However,ρ remainsmainly theoretical,asitisthe continuous
α
approximation of a discrete optimisation process. Thus, in order to get closer to practical optimisation, we
modelise the heavy-tailed behaviour of SGD by a multivariate p-Student distribution with parameters µ,Σ.
Recall that a multivariate p-Student variable y over Rd with parameters µ,Σ (namely Stud (µ,Σ)) and can be
p
writtenasy =µ+x p/uwherex ∼N(0Rd,Σ)andu ∼χ2
p
andx,uareindependent. Theheavy-tailedbehaviour
of y is determined by p; note that y has a mean (equal to µ) only if p>1 and a covarianceequal to p/p 2Σ only
p −
if p>2. As p goesto infinity, the studentdistribution convergesto (µ,Σ). Sucha processcanthenbe seenas
N
a discrete approximation of an α stable Levy process with the rescaling p= α/2 α. For practical instantiations,
−
we can assume, for instance, that µ is the averaged predictor over a few runs of SGD and Σ := σI is a fixed
d
level of noise.
FollowingtheideaofCorollary5.3’sproof,weexploitTheorem4.1inordertoboundthegeneralisationabilityof
multivariate p-Student distribution. The resulting upper bound lies in Theorem B.2 and is tractable in practice.
15Theorem B.2. Let σ > 0,µ Rd and π = (µ ,σI ). Assume ℓ [0,1] and that, for any δ (0,1), with
0 0 d ′
∈ N ∈ ∈
probability 1 δ , h ∆2(h) is L(m,δ )-Lipschitz. Thus, with probability at least 1 δ over m, we have,
′ ′
for any multi− variate→ studeSnt Stud (µ,σ2Id), with p>1,µ Rd, − S ∼D
p
∈
µ µ 2 ln4√m
E ∆ (h) L(m,δ/2)σf(p,d)+ k − 0 k + δ ,
h ∼ρα S ≤s 2σm 2m
where f(p,d):=√dE p 1 with u χ2.
u − ∼ p
(cid:2)(cid:12)p (cid:12)(cid:3)
Proof. We start from T(cid:12)heorem(cid:12)4.1. We have with probability at least 1 δ, for any η,ρ:
−
KL(η π)+ln4√m
hE ρ∆ S(h) ≤sL(m,δ/2)W 1(ρ,η)+ k
2m
δ .
∼
We recall that π = (µ ,σ2I ), we then pick η = (µ,σI ) and ρ = Stud (µ,σI ). We then know that
0 d d p d
N N
KL(η,π) = kµ − σµ0k2 . The only thing left to control is W 1(ρ,η). To do so, we exploit the following definition of
the Wasserstein distance coming from optimal transport:
W (ρ,η)= inf E ( Y Y ),
1 1 2
γ ∈Γ(ρ,η)(Y1,Y2) ∼γ k − k
where Γ(ρ,η) is the set of all distributions over such that the marginal distribution of (Y ,Y ) γ are
1 2
respectively ρ and η. We then consider the
coupH lin×
g
γH
such that Y,Y γ (Y ,Y ) = µ+σZ
p,µ∼
+σZ
2 ∼ ↔ 1 2 U
with Z (0,I ),U χ2 and U,Z are mutually independent. Then, γ Γ(ρ,η) and we have
∼N d ∼ p ∈ (cid:0) p (cid:1)
W (ρ,η) E ( Y Y )
1 1 2
≤(Y1,Y2) ∼γ k − k
p
= E E µ+σZ µ σZ
Z ∼N(0,Id)U ∼χ2
p(cid:13)
rU − −
(cid:13)
(cid:13) (cid:13)
(cid:13) p (cid:13)
=σ E E(cid:13) Z 1 (cid:13)
Z ∼N(0,Id)U ∼χ2 pk k (cid:12)rU −
(cid:12)
(cid:12) (cid:12)
p (cid:12) (cid:12)
σ√d E Z (cid:12) 1 , (cid:12)
≤ U χ2k k U −
∼ p (cid:12)r (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
the last line holding thanks to the independence of Z an(cid:12)d U and (cid:12)because E [ Z ] √d. Plugging this
Z ∼N(0,Id)
k k ≤
in the bound above concludes the proof.
C. Postponed Proofs
C.1. Proof of Theorem 3.1
Theorem 3.1. Let φ Γ, δ [0,1] and π ( ). With probability at least 1 δ over m, we have for
S ∈ ∈ ∈P H − S ∼D
all ρ ( )
∈P H
1
E φ (h) DΓ(ρ π)+ln +ln E exp Λπ(φ ) .
h ρ S ≤ f k δ m f S
∼ hS∼D (cid:0) (cid:1)i
Proof. From Definition 2.2, we can deduce that we have for all ρ ( )
∈P H
E φ (h) DΓ(ρ π)+Λπ(φ ).
h ρ S ≤ f k f S
∼
Moreover,we have
Λπ(φ )=ln exp(Λπ(φ ))
f f
S S
(cid:2) (cid:3)
16andsinceexp(Λπ(φ ))>0,wecanapplyMarkov’sinequalitytohavewithprobabilityatleast1 δover m
f S − S ∼D
1
exp(Λπ(φ )) E exp(Λπ(φ ))
f S ≤ δ m f S
S∼D
1
ln exp(Λπ(φ )) ln +ln E exp Λπ(φ ) .
⇐⇒ f S ≤ δ m f S
(cid:2) (cid:3) hS∼D (cid:0) (cid:1)i
C.2. Proof of Theorem 3.2
Theorem 3.2. Let φ Γ,δ [0,1] and π ( ). Assume there exists a function gπ such that for any ,π,
Λπ(φ ) Bπ(φ ). AsS su∈ me the∈ bounded-diffe∈ reP nceH property on Bπ, i.e., S S
f S ≤ S
i 1,...,m , sup Bπ(φ ) Bπ(φ ′) c i,
∀ ∈{ } S∈Zm,z i∈Z| S − Si |≤
where is the learning sample where z in is replaced by z . Then, we have, with probability at least 1 δ
Si′ i
S
′i
−
over m, we have for all ρ ( )
S ∼D ∈P H
ln1 m
E φ (h) DΓ(ρ π)+ E Bπ(φ )+ δ c2.
h ρ S ≤ f k m S v 2 i
∼ S∼D u i=1
u X
t
Proof. From Definition 2.2 and the upper bound on Λπ, we can deduce that we have for all ρ ( )
f ∈P H
E φ (h) DΓ(ρ π)+Bπ(φ ).
h ρ S ≤ f k S
∼
The bounded-difference property alongside McDiarmid’s inequality gives, with probability at least 1 δ,
−
Λπ(φ ) E Bπ(φ )+ ln(1 δ) m c2.
f S ≤ m S 2 i=1 i
S∼D q
P
Combining those two equations yields the desired result.
C.3. Proof of Theorem 4.1
Theorem 4.1. Assume that ℓ [0,1]. Assume that, for any δ (0,1), with probability 1 δ , h ∆2(h) is
′ ′
L(m,δ )-Lipschitz. Thus, for an∈ y data-free prior π, with probabili∈ ty at least 1 δ over − m, we→ haveSfor all
′
− S ∼D
ρ ( ), any η ( ),
∈P H ∈P H
KL(η π)+ln4√m
hE ρ∆ S(h) ≤sL(m,δ/2)W 1(ρ,η)+ k
2m
δ .
∼
Proof. The proof is split into three steps.
Step 1: finding the f-divergence. It is known that the f-divergence with f :x xlnx is
7→
dρ dρ dρ dρ
D (ρ π):= E f (g) = E (g)ln (g) = E ln (g) :=KL(ρ π).
f
k g π dπ g π dπ dπ g ρ dπ k
∼ (cid:18) (cid:19) ∼ (cid:18) (cid:19) ∼ (cid:18) (cid:19)
Step 2: finding the closed-form solution of Λπ(ϕ). It is known for the KL divergence. However, for the
f
sake of completeness, we provide a proof. Recall that with f :x xlnx, we have
7→
f (y):=sup yx f(x) =sup g(x,y) , where g(x,y):=yx xlnx.
∗
x
R{ − }
x
R{ } −
∈ ∈
17In order to find the supremum (which is attained), we can find the derivative of the function g, and we have
∂g
(x,y)=y lnx 1.
∂x − −
Since the Legendre transform is convex, we can set the derivative to 0 to obtain
∂g
(x,y)=0 y lnx 1=0 x=ey 1.
−
∂x ⇐⇒ − − ⇐⇒
Hence, we can deduce that we have
f (y)=g(ey 1,y)=yey 1 ey 1lney 1 =yey 1 yey 1+ey 1 =ey 1.
∗ − − − − − − − −
− −
Now, the goal is to find the value c associated with Λπ f(ϕ) := inf c ∈R {c+E g ∼πf ∗(ϕ(g) −c)
}
=
inf c ∈R c+E g ∼πeϕ(g) −c −1 . To do so, we set c = lnE g ∼πeϕ(g)
−
c ′ with c ′
∈
R, and we show that Λπ f(ϕ)
is optimal with c =1. We have
′
(cid:8) (cid:9)
c+ E f∗(ϕ(g) c)=ln E eϕ(g) c′+ E eϕ(g) −lnE g∼πeϕ(g)+c′ −1 =ln E eϕ(g) c′+ec′ −1 :=g′(c′)
g π − g π − g π g π −
∼ ∼ ∼ ∼
Moreover,the derivative g (c) with respect to c is given by
′ ′ ′
∂g
′ (c)=ec′ 1 1.
′ −
∂c −
′
Then, notice thatthis derivative on( ,1[ andnon-negativeon [1,+ ),thus the minimumofg is reachedfor
′
−∞ ∞
c =1. We then deduce that
′
Λπ(ϕ)=ln E eφ(g).
f
g π
∼
Step 3: deriving the PAC-Bayesian bound. We use Theorem 3.1 with f = f and Γ = Lipα(m,δ) with
KL b
α(m,δ)=2mL(m,δ/2). Then, for any φ Lipα(m,δ), we have, with probability at least 1 δ/2,
S ∈ b −
2
E φ (h) DΓ(ρ π)+ln +ln E exp Λπ(φ )
h ρ S ≤ f k δ m f S
∼ 2 hS∼D (cid:0) (cid:1)i
=DΓ(ρ π)+ln +ln E E eφS(h) . (14)
f k δ mh π
(cid:20)S∼D ∼ (cid:21)
Let φ (h) = 2m∆2. Note that, thanks to our assumption, we know that with probability at least 1 δ/2,
∆2 isS L(m,δ/2)-LipSschitz, then φ is α(m,δ)-Lipschitz. We then take a union bound, apply Equation (4)− , and
reSarrange the terms to obtain, wiS th probability at least 1 δ:
−
1 2
E ∆ (h)2 inf α(m,δ)W (ρ,η)+KL(η π) +ln +ln E E e2m∆S(h)2 .
1
h ∼ρ S ≤ 2m (cid:20)η ∈M1( H){ k } δ (cid:20)S∼Dmh ∼π
(cid:21)(cid:21)
(cid:2) (cid:3)
From Jensen’s inequality, we have (E R (h) R (h))2 E ∆ (h)2 and we can deduce that for any
h ρ h ρ
∼ | D − S | ≤ ∼ S
η ( ),
1
∈M H (cid:2) (cid:3)
1 2
E [∆ (h)] α(m,δ)W 1(ρ,η)+KL(η π)+ln +ln E E e2m∆S(h)2 .
|h ρ S |≤s2m k δ mh π
∼ (cid:20) (cid:20)S∼D ∼ (cid:21)(cid:21)
From Pinsker’s inequality and Maurer (2004), we have
ln E E e2m∆S(h)2 ln E E emkl(R D(h) kR S(h)) ln 2√m ,
mh π ≤ mh π ≤
(cid:20)S∼D ∼ (cid:21) (cid:20)S∼D ∼ (cid:21)
(cid:0) (cid:1)
where kl(a b)=alna +(1 a)ln1 a.
k b − 1−b
−
Re-organising the terms alongisde with α(m,δ)=2mL(m,δ/2) concludes the proof.
18C.4. Proof of Theorem 4.2
We split the proof in half, one for each equation. We restate below Theorem 4.2 for completeness.
Theorem 4.2. For ∆ Γ. With probability at least 1 δ over m, we have for all ρ ( ) and
S ∈ − S ∼ D ∈ P H
η ( ):
∈P H
Reverse-KL bound:
1 1 1
E ∆ (h) 2WΓ(ρ,η)+2KL(η π)+ +ln 1+ 2mln . (5)
h ∼ρ S ≤ k rm (cid:18) m (cid:19)r δ
Hellinger bound:
1 2 1
E ∆ (h) 2WΓ(ρ,η)+2H2(η π)+ + 2mln . (6)
h ∼ρ S ≤ k rm m+1 r δ
TV bound:
1 ln1
E ∆ (h) WΓ(ρ,η)+TV(η,π)+ + δ. (7)
h ∼ρ S ≤ r4m s2m
C.4.1. Proof of Equation (5)
Proof. The proof is split into three steps.
Step 1: finding the f-divergence. It is known that the f-divergence with f :x lnx is
7→−
dρ dρ
D (ρ π):= E f (g) = E ln (g) :=KL(ρ π).
f
k g π dπ −g π dπ k
∼ (cid:18) (cid:19) ∼ (cid:18) (cid:19)
Step 2: finding an upper bound Bπ(ϕ) of Λπ(ϕ). Recall that with f :x lnx, we have
f 7→−
f (y):=sup yx f(x) =sup g(x,y) , where g(x,y):=yx+lnx.
∗
x
R{ − }
x
R{ }
∈ ∈
In order to find the supremum (which is attained), we can find the derivative of the function g, and we have
∂g 1
(x,y)=y+ .
∂x x
Since the Legendre transform is convex, we can set the derivative to 0 to obtain
∂g 1 1
(x,y)=0 y+ =0 x= .
∂x ⇐⇒ x ⇐⇒ −y
Hence, we can deduce that we have
1 1 1 1
f (y)=g ,y = y +ln =ln 1.
∗
−y − y −y −y −
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
Now, we upper-bound
Λπ f(ϕ):= cin Rf c+ gE πf∗(ϕ(g) −c)
∈ (cid:26) ∼ (cid:27)
1
= inf c+ E ln 1
c R g π −ϕ(g) c −
∈ (cid:26) ∼ (cid:18) − (cid:19) (cid:27)
1
E 1+ln 1
≤g π −ϕ(g) 1 −
∼ (cid:20) (cid:18) − (cid:19) (cid:21)
= E [ ln(1 ϕ(g))]:=Bπ(ϕ).
g π − −
∼
19Step 3: deriving the PAC-Bayesian bound. We use Theorem3.2 with Bπ(ϕ)=E [ ln(1 ϕ(g))] and
g π
φ (h)= 1 R (h) R (h) to obtainthe followinginequalityholding withprobabilityatl∼ east− 1 δ o− ver m
S 2| D − S | − S ∼D
1 ln1 m
E R (h) R (h) 2WΓ(ρ,η)+2KL(η π)+2 E E ln 1 R (h) R (h) +2 δ c2.
h ρ| D − S |≤ k mh π − −2| D − S | v 2 i
∼ S∼D ∼ (cid:20) (cid:18) (cid:19)(cid:21) u i=1
u X
t (15)
Thefinalstepistofindtheupperboundc foreachi 1,...,m . Todoso,wefirstupper-boundthedifference
i
∈{ }
Bπ(φ ) Bπ(φ ′) for all i 1,...,m . We first have
| S − Si | ∈{ }
Bπ(φ ) Bπ(φ ′) = E [ ln(1 φ (h))] E ln 1 φ ′(h)
S − Si h π − − S −h π − − Si
(cid:12) ∼ ∼ (cid:12)
(cid:12) (cid:12) (cid:12) (cid:2) (cid:0) (cid:1)(cid:3)(cid:12)
(cid:12) (cid:12) =(cid:12) (cid:12) E ln 1 φ ′(h) ln(1 φ (h)) (cid:12) (cid:12)
h π − Si − − S
(cid:12) ∼ (cid:12)
=(cid:12)
(cid:12) (cid:12) E
(cid:2)
ln
(cid:0)1 −φ Si′(h)(cid:1)
.
(cid:3)(cid:12)
(cid:12) (cid:12)
h π 1 φ (h)
(cid:12) ∼ (cid:18) − S (cid:19)(cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
Now, in order to upper-bound E
h
π(cid:12) ln(1 φ ′(h)) ln(cid:12) (1 φ (h)), the goal is to upper-bound
E
h
πln(1 φ ′(h)) ln(1 φ (h)) and|E h∼ πln(1− φS (i h)) − ln(1 φ− ′(hS )). |
∼ − Si − − S ∼ − S − − Si
Then, since we have max(φ S(h) −21 m,0) ≤φ Si′(h) ≤min(φ S(h)+ 21 m, 21), we have
E ln
1 −φ Si′(h)
E ln
1 −max(φ S(h) −21 m,0)
max ln
1 −max( 21x −21 m,0)
.
h π 1 φ (h) ≤h π 1 φ (h) ≤x [0,1] 1 1x
∼ (cid:18) − S (cid:19) ∼ (cid:18) − S (cid:19) ∈ (cid:18) −2 (cid:19)
For x [0, 1], we have
∈ m
1 1
max ln =ln
x [0,1] 1 1x 1 1
∈ m (cid:18) −2 (cid:19) (cid:18) −2m(cid:19)
For x [1,1], we have
∈ m
1 1x+ 1 1 1
max ln −2 2m max ln 1+ =ln 1+
x ∈[ m1,1] (cid:18) 1 −1 2x (cid:19)≤x ∈[ m1,1] 2m 1 −1 2x ! (cid:18) m (cid:19)
(cid:2) (cid:3)
Since, for m 1, we have ln 1 ln 1+ 1 , we can deduce that
≥ 1 −21 m ≤ m
(cid:16) (cid:17) (cid:0) (cid:1)
E ln
1 −φ Si′(h)
ln 1+
1
. (16)
h π 1 φ (h) ≤ m
∼ (cid:18) − S (cid:19) (cid:18) (cid:19)
Moreover,we have
1 φ (h) 1 φ (h) 1 1x
E ln − S E ln − S max ln −2
h ∼π 1 −φ Si′(h) !≤h ∼π (cid:18)1 −min(φ S(h)+ 21 m, 21) (cid:19)≤x ∈[0,1] (cid:18)1 −min(1 2x+ 21 m, 21)
(cid:19)
For x [0,1 1], we have
∈ − m
1 1x 1 1 + 1 1 + 1 1
max ln −2 =ln −2 2m =ln 2 2m =ln 1+
x [0,1 1] 1 1x 1 1 1 + 1 1 1 m
∈ −m (cid:18) −2 −2m(cid:19) (cid:18) −2 2m−2m(cid:19) (cid:18) 2 (cid:19) (cid:18) (cid:19)
20For x [1 1,1], we have
∈ − m
1 1x 1
max ln −2 = max ln(2 x)=ln 1+ .
x [1 1,1] 1 1 x [1 1,1] − m
∈ −m (cid:18) −2 (cid:19) ∈ −m (cid:18) (cid:19)
Hence, we can deduce that
1 φ (h) 1
E ln − S ln 1+ . (17)
h ∼π 1 −φ Si′(h) !≤
(cid:18)
m
(cid:19)
By combining Equations (16) and (17) we have for all i 1,...,m
∈{ }
E ln
1 −φ Si′(h)
ln 1+
1
:=c i.
h π 1 φ (h) ≤ m
(cid:12) ∼ (cid:18) − S (cid:19)(cid:12) (cid:18) (cid:19)
(cid:12) (cid:12)
Then, by substituting in Equation(cid:12)(15), we have (cid:12)
(cid:12) (cid:12)
E R (h) R (h) 2WΓ(ρ,η)+2KL(η π)
h ρ| D − S |≤ k
∼
1 1 m 1
+2 E E ln 1 R (h) R (h) +2ln 1+ ln .
S∼Dmh ∼π (cid:20)− (cid:18) −2| D − S | (cid:19)(cid:21) (cid:18) m (cid:19)r 2 δ
Using the fact that for all x [0,1], we have ln(1 1x) x. Hence, we can deduce that
∈ − − 2 ≤
E R (h) R (h) 2WΓ(ρ,η)+2KL(η π)
h ρ| D − S |≤ k
∼
1 m 1
+2 E E R (h) R (h) +2ln 1+ ln .
S∼Dmh ∼π| D − S |
(cid:18)
m (cid:19)r2 δ
From Fubini’s theorem and Hölder’s inequality, we have
E E R (h) R (h) = E E R (h) R (h) E E (R (h) R (h))2.
mh π| D − S | h π m| D − S |≤ h π m D − S
S∼D ∼ ∼ S∼D ∼ S∼D
q
Thanks to Bégin et al. (2016), we have
1
E E (R (h) R (h))2 ,
h ∼π S∼Dm D − S ≤ r4m
q
which allows us to obtain the desired result.
C.4.2. Proof of Equation (6)
Proof. The proof is split into three steps.
Step 1: finding the f-divergence. It is known that the f-divergence with f :x (√x 1)2 is
7→ −
2
dρ dρ
D (ρ π):= E f (g) = E (g) 1 :=H2(ρ π).
f
k g ∼π (cid:18)dπ (cid:19) g ∼π rdπ − ! k
Step 2: finding an upper bound Bπ(ϕ) of Λπ(ϕ). Recall that with f :x (√x 1)2, we have
f 7→ −
f (y):=sup yx f(x) =sup g(x,y) , where g(x,y):=yx+(√x 1)2.
∗
x
R{ − }
x
R{ } −
∈ ∈
In order to find the supremum (which is attained), we can find the derivative of the function g, and we have
∂g 1
(x,y)=y 1+ .
∂x − √x
21Since the Legendre transform is convex, we can set the derivative to 0 to obtain
2
∂g 1 1
(x,y)=0 y 1+ =0 x= .
∂x ⇐⇒ − √x ⇐⇒ 1 y
(cid:18) − (cid:19)
Hence, we can deduce that we have
2 2 2
1 1 1 y
f (y)=g ,y =y 1 = .
∗
(cid:18)1 −y (cid:19) ! (cid:18)1 −y (cid:19) − (cid:18)1 −y − (cid:19) 1 −y
Now, we upper-bound
Λπ(ϕ):= inf c+ E f (ϕ(g) c)
f c R g π ∗ −
∈ (cid:26) ∼ (cid:27)
ϕ(g) c
= inf c+ E −
c R g π 1 ϕ(g)+c
∈ (cid:26) ∼ − (cid:27)
ϕ(g)
E :=Bπ(ϕ).
≤g π 1 ϕ(g)
∼ −
Step 3: deriving the PAC-Bayesian bound. We use Theorem 3.2 with the bound Bπ(ϕ) = E ϕ(g)
and the function φ (h) = 1 R (h) R (h) to obtain the following inequality holding with probabilig t∼ yπ at1 − leϕ a(g s) t
1 δ over
mS 2| D − S |
− S ∼D
E R (h) R (h) 2WΓ(ρ,η)+2H2(η π)
h ρ| D − S |≤ k
∼
1 R (h) R (h) ln1 m
+2 E E 2| D − S | +2 δ c2. (18)
mh π 1 1 R (h) R (h) v 2 i
S∼D ∼ − 2| D − S | u u Xi=1
t
Thefinalstepistofindtheupperboundc foreachi 1,...,m . Todoso,wefirstupper-boundthedifference
i
∈{ }
Bπ(φ ) Bπ(φ ′) for all i 1,...,m . We first have
| S − Si | ∈{ }
Bπ(φ ) Bπ(φ ′) = E
φ S(h)
E
φ Si′(h)
.
S − Si (cid:12)h π 1 φ (h) −h π 1 φ ′(h)(cid:12)
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ∼ − S ∼ − Si (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12)
Then, since we have max(φ S(h) −21 m,0) ≤φ Si′(h) ≤min(φ S(h)+ 21 m, 21), we have
E
φ S(h) φ Si′(h)
E
φ S(h) max(φ S(h) −21 m,0)
h ∼π"1 −φ S(h) − 1 −φ Si′(h) #≤h ∼π (cid:20)1 −φ S(h) − 1 −max(φ S(h) −21 m,0)
(cid:21)
1x max(1x 1 ,0)
max 2 2 −2m .
≤x [0,1] 1 1x − 1 max(1x 1 ,0)
∈ (cid:20) − 2 − 2 −2m (cid:21)
For x [0, 1], we have
∈ m
1x max(1x 1 ,0) 1x
max 2 2 −2m = max 2
x [0,1] 1 1x − 1 max(1x 1 ,0) x [0,1] 1 1x
∈ m (cid:20) − 2 − 2 −2m (cid:21) ∈ m (cid:20) − 2 (cid:21)
1
= 2m
1 1
− 2m
1
= .
2m 1
−
22For x [1,1], we have
∈ m
1x max(1x 1 ,0) 1x 1x 1
max 2 2 −2m = max 2 2 −2m
x [1,1] 1 1x − 1 max(1x 1 ,0) x [1,1] 1 1x − 1 1x+ 1
∈ m (cid:20) − 2 − 2 −2m (cid:21) ∈ m (cid:20) − 2 − 2 2m(cid:21)
1 1 1
= 2−2m
2 − 1+ 1
(cid:20) 2 2m(cid:21)
3 m
= −
2m+2
1
.
≤ 2m 1
−
Hence, we can deduce that
E
φ S(h) φ Si′(h) 1
. (19)
h π"1 φ (h) − 1 φ ′(h) #≤ 2m 1
∼ − S − Si −
Moreover,we have
E
φ Si′(h) φ S(h)
E
min(φ S(h)+ 21 m, 21) φ S(h)
h ∼π"1 −φ Si′(h) − 1 −φ S(h) #≤h ∼π (cid:20)1 −min(φ S(h)+ 21 m, 21) − 1 −φ S(h)
(cid:21)
min(1x+ 1 ,1) 1x
max 2 2m 2 2 .
≤x [0,1] 1 min(1x+ 1 ,1) − 1 1x
∈ (cid:20) − 2 2m 2 − 2 (cid:21)
For x [0,1 1], we have
∈ − m
1x+ 1 1x 1 1 1
max 2 2m 2 = 2−2m
x [0,1 1] 1 1x 1 − 1 1x 2 − 1+ 1
∈ −m (cid:20) − 2 −2m − 2 (cid:21) (cid:20) 2 2m(cid:21)
3 m
= −
2m+2
1
.
≤ 2m 1
−
For x [1 1,1], we have
∈ − m
1 1x 1 1 1
max 2 2 = 2 2 − 2m
x [1 1,1] 1 1 − 1 1x 1 1 − 1 1 + 1
∈ −m (cid:20) − 2 − 2 (cid:21) (cid:20) − 2 − 2 2m(cid:21)
1 1
= 1 2 − 2m
− 1 + 1
(cid:20) 2 2m(cid:21)
2
.
≤ m+1
Hence, we can deduce that
E
φ Si′(h) φ S(h) 2
. (20)
h π"1 φ ′(h) − 1 φ (h) #≤ m+1
∼ − Si − S
By combining Equations (19) and (20) we have for all i 1,...,m
∈{ }
Bπ(φ ) Bπ(φ ′) = E
φ S(h)
E
φ Si′(h) 2
:=c i.
S − Si (cid:12)h π 1 φ (h) −h π 1 φ ′(h)(cid:12)≤ m+1
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ∼ − S ∼ − Si (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12)
23Then, by substituting in Equation (18), we have
E R (h) R (h) 2WΓ(ρ,η)+2H2(η π)
h ρ| D − S |≤ k
∼
1 R (h) R (h) 2 m 1
+2 E E 2| D − S | +2 ln .
S∼Dmh ∼π 1
−
21 |R D(h) −R S(h)
|
m+1 r2 δ
Using the fact that for all x [0,1], we have x 2x. Hence, we can deduce that
∈ 2 1 x ≤
−
E R (h) R (h) 2WΓ(ρ,η)+2H2(η π)
h ρ| D − S |≤ k
∼
2 m 1
+4 E E R (h) R (h) +2 ln .
S∼Dmh ∼π| D − S | m+1 r2 δ
From Fubini’s theorem and Hölder’s inequality, we have
E E R (h) R (h) = E E R (h) R (h) E E (R (h) R (h))2.
mh π| D − S | h π m| D − S |≤ h π m D − S
S∼D ∼ ∼ S∼D ∼ S∼D
q
Thanks to Bégin et al. (2016), we have
1
E E (R (h) R (h))2 ,
h ∼π S∼Dm D − S ≤ r4m
q
which allows us to obtain the desired result.
C.4.3. Proof of Equation (7)
Proof. The proof is split into three steps.
Step 1: finding the f-divergence. It is known that the f-divergence with f :x 1 x 1 is
7→ 2| − |
dρ 1 dρ
D (ρ π):= E f (g) = E (g) 1 :=TV(ρ π).
f
k g π dπ g π 2 dπ − k
∼ (cid:18) (cid:19) ∼ (cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
Step 2: finding an upper bound Bπ(ϕ) of Λπ(ϕ). Recall(cid:12) that with(cid:12) f :x 1 x 1, we have
f 7→ 2| − |
f (y):= sup yx f(x) = sup g(x,y) , where g(x,y):=yx 1 x 1.
∗ { − } { } − 2| − |
x [0,1] x [0,1]
∈ ∈
First of all, remark that for any x [0,1], we have
∈
g(x,y)=yx 1 x 1
− 2| − |
=yx 1(x 1)
− 2 −
=x(y+ 1) 1.
2 − 2
Hence, we can deduce that
y if y 0
f ∗(y)= sup {g(x,y) }= sup x(y+ 21)
−
21 =
1
other≥
wise.
x ∈[0,1] x ∈[0,1]
(cid:8) (cid:9)
(cid:26) −2
Now, we upper-bound
Λπ f(ϕ):= cin Rf c+ gE πf∗(ϕ(g) −c)
∈ (cid:26) ∼ (cid:27)
E ϕ(g):=Bπ(ϕ).
≤g π
∼
24Step 3: deriving the PAC-Bayesian bound. We use Theorem 3.2 with the bound Bπ(ϕ)=E ϕ(g) and
g π
∼
the function φ (h) = R (h) R (h) to obtain the following inequality holding with probability at least 1 δ
over m S | D − S | −
S ∼D
ln1 m
E R (h) R (h) WΓ(ρ,η)+TV(η,π)+ E E R (h) R (h) + δ c2. (21)
h ρ| D − S |≤ mh π| D − S | v 2 i
∼ S∼D ∼ u i=1
u X
t
Thefinalstepistofindtheupperboundc foreachi 1,...,m . Todoso,wefirstupper-boundthedifference
i
∈{ }
Bπ(φ ) Bπ(φ ′) for all i 1,...,m . We first have
| S − Si | ∈{ }
Bπ(φ ) Bπ(φ ′) = E φ (h) E φ ′(h) .
S − Si h π S −h π Si
(cid:12) ∼ ∼ (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12)
Then, for φ S(h)= |R D(h) −R S(h) |, we have max(φ S(h) −m1,0) ≤φ Si′(h) ≤min(φ S(h)+ m1,1) and
hE
π
φ S(h) −φ Si′(h) ≤hE
π
φ S(h) −max(φ S(h) −m1,0)
∼ ∼
(cid:2) (cid:3) max(cid:2) x max(x 1,0) , (cid:3)
≤x [0,1] − −m
∈
and hE
π
φ Si′(h) −φ S(h) ≤hE
π
m(cid:2) in(φ S(h)+ m1,1) −(cid:3) φ S(h)
∼ ∼
(cid:2) (cid:3) max(cid:2) min(x+1,1) x . (cid:3)
≤x [0,1] m −
∈
(cid:2) (cid:3)
Moreover,we have
1 1
max x max(x 1,0) = , max x max(x 1,0) =x x+ 1 = ,
x [0,1] − −m m x [1,1] − −m − m m
∈ m ∈ m
(cid:2) (cid:3) 1 (cid:2) (cid:3) 1
max min(x+1,1) x = , and max min(x+1,1) x =1 x= .
x [0,1 1] m − m x [1 1,1] m − − m
∈ −m ∈ −m
(cid:2) (cid:3) (cid:2) (cid:3)
Hence, we can deduce that
1
c
i
:= Bπ(φ ) Bπ(φ ′) = E φ (h) E φ ′(h)
S − Si h π S −h π Si ≤ m
(cid:12) ∼ ∼ (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
and we have (cid:12) (cid:12)
ln1
E R (h) R (h) WΓ(ρ,η)+TV(η,π)+ E E R (h) R (h) + δ.
h ρ| D − S |≤ mh π| D − S | s2m
∼ S∼D ∼
From Fubini’s theorem and Hölder’s inequality, we have
E E R (h) R (h) = E E R (h) R (h) E E (R (h) R (h))2.
mh π| D − S | h π m| D − S |≤ h π m D − S
S∼D ∼ ∼ S∼D ∼ S∼D
q
Thanks to Bégin et al. (2016), we have
1
E E (R (h) R (h))2 ,
h ∼π S∼Dm D − S ≤ r4m
q
which allows us to obtain the desired result.
25C.5. Proof of Theorems 5.1 and 6.1
We define for conciseness, we define = (h,h) h =h . We first prove Lemmas C.1 and C.2 to further
′ ′
H { ∈H| 6 }
prove Theorems 5.1 and 6.1.
Lemma C.1. For any hypothesis set (with the metric d ), for any learning sample m, for any function
φ : R, for any metric d: H R, we have H S ∈Z
S H→ H×H→
φ (h) φ (h) φ (h) φ (h)
′ ′
sup | S − S | = sup S − S .
d (h,h) d (h,h)
(h,h′) ′ (h,h′) ′
∈H H ∈H H
Proof. Forthesakeofreadability,wedenoteby := (h,h) φ (h) φ (h) 0 and := (h,h)
0 ′ ′ 0 ′
H≥ { ∈H| S − S ≥ } H≤ { ∈
φ (h) φ (h) 0 . Then, we have
′
H| S − S ≤ }
φ (h) φ (h) φ (h) φ (h) φ (h) φ (h)
′ ′ ′
sup S − S =max sup S − S , sup S − S
d (h,h) d (h,h) d (h,h)
(h,h′)
∈H H
′ (h,h′)
∈H≤0 H
′ (h,h′)
∈H≥0 H
′ !
φ (h) φ (h)
′
= sup S − S .
d (h,h)
(h,h′)
∈H≥0 H
′
Let A= {|φS d(h H) (− hφ ,hS ′( )h′) | |(h,h ′)
∈H}
and A≥0 = {φS d(h H) (− hφ ,hS ′( )h′) |(h,h ′) ∈H≥0 }, then, note that since A≥0
⊆A
wehaveforalla ∈A≥0 ⇒a ∈A. Moreover,foralla= |φS d(h H) (− hφ ,hS ′( )h′) | ∈A,wehaveeithera= φS d(h H) (− hφ ,hS ′( )h′) ∈A≥0
(if φ S(h) −φ S(h ′) ≥0) or a= φS d(h H′ () h− ,φ hS ′)(h) ∈A≥0 (if φ S(h) −φ S(h ′) ≤0) by definition of A≥0. Hence, we have
a a , and so = .
0 0
∈A⇒ ∈A≥ A A≥
Since we have = , we can deduce that
0
A≥ A
φ (h) φ (h) φ (h) φ (h)
′ ′
sup S − S =sup 0 =sup = sup | S − S |.
(h,h′)
∈H≥0
d H(h,h ′) A≥ A
(h,h′)
∈H
d H(h,h ′)
Lemma C.2. For any hypothesis set (with any metric d ), for any L-Lipschitz loss ℓ : R, with
probability at least 1 δ over m H H H×Z →
− S ∼D
h ∆ (h) is L(m,δ)= 2R( )+L 2ln δ2 -Lipschitz,
7→ S H m
(cid:18) q (cid:19)
where R( H):=E ε ∼EmE S∼Dmsup (h,h′)
∈H
m1 m i=1ε i[ℓ(h′ d,z Hi () h− ,ℓ h( ′h ),z i)]
P
Proof. From Lemma C.1, we have
φ (h) φ (h) φ (h) φ (h)
′ ′
sup | S − S | = sup S − S .
d (h,h) d (h,h)
(h,h′) ′ (h,h′) ′
∈H H ∈H H
Hence, the goal is to upper-bound the following quantity with high-probability over m
S ∼D
φ (h) φ (h)
′
sup S − S , (22)
d (h,h)
(h,h′) ′
∈H H
26where φ (h)=R (h) R (h). First of all, remark that we have
S D − S
sup
φ S(h) −φ S(h ′)
sup
φ Si′(h) −φ Si′(h ′)
sup
φ S(h) −φ S(h ′) −φ Si′(h)+φ Si′(h ′)
d (h,h) − d (h,h) ≤ d (h,h)
(h,h′) ′ (h,h′) ′ (h,h′) ′
∈H H ∈H H ∈H H
1 ℓ(h,z ) ℓ(h,z ) ℓ(h,z )+ℓ(h,z )
= sup
i
−
′ i
−
′i ′ ′i
m d (h,h)
(h,h′) ′
∈H H
1 ℓ(h,z ) ℓ(h,z ) + ℓ(h,z ) ℓ(h,z )
sup |
i
−
′ i
| |
′i
−
′ ′i
|
≤ m d (h,h)
(h,h′) ′
∈H H
2L
.
≤ m
Note that by upper-bounding in the same way, we have
sup
φ Si′(h) −φ Si′(h ′)
sup
φ S(h) −φ S(h ′) 2L
.
d (h,h) − d (h,h) ≤ m
(h,h′) ′ (h,h′) ′
∈H H ∈H H
We canthusdeduce that(22)hasthe bounded-differenceproperty. Hence,wecanapply McDiarmid’sinequality
to have
φ (h) φ (h) R (h) R (h) R (h)+R (h)
E sup S − S ′ = E sup D − S − D ′ S ′
m d (h,h) m d (h,h)
S∼D (h,h′) ∈H H ′ S∼D (h,h′) ∈H H ′
R (h) R (h) R (h)+R (h)
E E sup S′ − S − S′ ′ S ′ .
≤ m ′ d (h,h)
S∼D S ∼D(h,h′) ∈H H ′
Hence, from the symmetrization lemma, we have
R (h) R (h) R (h)+R (h)
E E sup S′ − S − S′ ′ S ′
m ′ d (h,h)
S∼D S ∼D(h,h′) ∈H H ′
m
1 1
= E E sup [ℓ(h,z ) ℓ(h,z ) ℓ(h,z )+ℓ(h,z )]
m ′ d (h,h)m
′i
−
i
−
′ ′i ′ i
S∼D S ∼D(h,h′) ∈H H ′ Xi=1
m
1 1
= E E E sup ε [ℓ(h,z ) ℓ(h,z ) ℓ(h,z )+ℓ(h,z )]
ε ∼Em S∼Dm S′
∼D(h,h′)
∈Hd H(h,h ′)m
Xi=1
i ′i
−
i
−
′ ′i ′ i
1 m [ℓ(h,z ) ℓ(h,z )] 1 m [ℓ(h,z ) ℓ(h,z )]
E E sup ε ′ i − i + E E sup ε ′i − ′ ′i
i i
≤ε ∼Em S∼Dm
(h,h′)
∈Hm
Xi=1
d H(h,h ′) ε ∼Em S′
∼D(h,h′)
∈Hm
Xi=1
d H(h,h ′)
2 m [ℓ(h,z ) ℓ(h,z )]
= E E sup ε ′ i − i
i
ε ∼Em S∼Dm
(h,h′)
∈Hm
Xi=1
d H(h,h ′)
We have with probability at least 1 δ over m
− S ∼D
φ (h) φ (h) φ (h) φ (h) ln1 4L2
sup S − S ′ E sup S − S ′ + δm
d (h,h) ≤ m d (h,h) s 2 m2
(h,h′) ∈H H ′ S∼D (h,h′) ∈H H ′
φ (h) φ (h) 2ln1
= E sup S − S ′ +L δ
m d (h,h) s m
S∼D (h,h′) ∈H H ′
2 m [ℓ(h,z ) ℓ(h,z )] 2ln1
E E sup ε ′ i − i +L δ. (23)
i
≤ε ∼Em S∼Dm
(h,h′)
∈Hm
Xi=1
d H(h,h ′) s m
27Finally, we apply the same proof with φ (h)=R (h) R (h) to obtain
S S − D
φ (h) φ (h) 2 m [ℓ(h,z ) ℓ(h,z )] 2ln1
sup S − S ′ E E sup ε i ′ i − i +L δ. (24)
d (h,h) ≤ε Em m m d (h,h) s m
(h,h′) ∈H H ′ ∼ S∼D (h,h′) ∈H Xi=1 H ′
Hence, by merging Equations (23) and (24) with a union bound, we obtain the desired result.
We are now able to prove Theorem 5.1.
Theorem 5.1. For any hypothesis set , for any L-Lipschitz loss ℓ : R (by considering dKron), with
H H×Z →
probability at least 1 δ over m
− S ∼D
h ∆ (h) is L(m,δ)= 4R( )+L 2ln2 δ -Lipschitz.
7→ S H m
(cid:18) q (cid:19)
Proof. The proof boils down to upper-bound the term R( ). Indeed, we have
H
m
1
R( )= E E sup ε [ℓ(h,z ) ℓ(h,z )]
i ′ i i
H ε Em m m −
∼ S∼D (h,h′) i=1
∈H X
m
1
E E sup ε [ℓ(h,z ) ℓ(h,z )]
i ′ i i
≤ε ∼Em S∼Dm (h,h′) ∈H2 m
Xi=1
−
m m
1 1
= E E sup ε ℓ(h,z )+ E E sup ( ε )ℓ(h,z )
i ′ i i i
ε ∼Em S∼Dmh′ ∈H2 m
Xi=1
ε ∼Em S∼Dmh ∈Hm
Xi=1
−
m
1
=2 E E sup ε iℓ(h′,z i)
ε ∼Em S∼Dmh′ ∈H2 m
Xi=1
:=2R( ).
H
Theorem 6.1. For any hypothesis set , for any L-Lipschitz loss ℓ: R, with probability at least 1 δ
over m and ε Em H H×Z → −
S ∼D ∼
h ∆ (h) is L(m,δ)=
2Rε
( )+3L
2ln4
δ -Lipschitz,
7→ S S H m
(cid:18) q (cid:19)
where R Sε( H):=sup
h 6=h′
∈H
m1 m i=1ε i[ℓ(h′ d,z Hi () h− ,ℓ h( ′h ),z i)] .
P
Proof. First of all, let ε be the vector ε that differs only from its i-th element. Now, remark that we have
′i
R Sε ( H) −R Sε′ i i′( H)
≤
(h,s hu ′)p ∈Hm1 (cid:20)ε i[ℓ(h ′, dz Hi) (h− ,hℓ( ′h ),z i)] −ε′i[ℓ(h ′, dz H′i) (h− ,hℓ( ′h ),z ′i)]
(cid:21)≤
2 mL
and similarly, we have
R Sε′ i i′( H) −R Sε ( H)
≤
(h,s hu ′)p ∈Hm1 (cid:20)ε i[ℓ(h ′, dz H′i) (h− ,hℓ( ′h ),z ′i)] −ε ′i[ℓ(h ′, dz Hi) (h− ,hℓ( ′)h,z i)]
(cid:21)≤
2 mL .
Hence, we can deduce that
Rε
( ) has the bounded-difference property. Hence, we can apply McDiarmid’s
inequality (with δ/2 instead ofSδ)H to obtain with probability at least 1 δ/2 over m and ε Em
− S ∼D ∼
ln2
2R( )=2 E E Rε ( ) 2Rε ( )+2L δ. (25)
H ε Em m S H ≤ S H s m
∼ S∼D
28From Lemma C.2 (with δ/2 instead of δ), we have
φ (h) φ (h) 2ln4
sup | S − S ′ | 2R( )+L δ. (26)
d (h,h) ≤ H s m
(h,h′) ′
∈H H
By combining (25) and (26), we have
φ (h) φ (h) 2ln4 2ln2
sup | S − S ′ | 2Rε ( )+L δ +2L δ
(h,h′) d (h,h ′) ≤ S H s m s m
∈H H
2ln4 2ln4
2Rε ( )+L δ +2L δ,
≤ S H s m s m
which leads to the desired result.
C.6. Proof of Corollary 5.2
To prove Corollary5.2, we first prove the following corollary.
Corollary C.3. Assume that ℓ [0,1]. Assume that, for any δ (0,1), with probability 1 δ , h ∆ (h) is
′ ′
∈ ∈ − → S
L(m,δ)-Lipschitz w.r.t. the Kronecker distance. Thus, for any data-free prior π, with probability at least 1 δ
−
over m, we have for all ρ ( ), any η ( ),
S ∼D ∈P H ∈P H
1 ln1
E ∆ (h) L(m,δ)TV(ρ,η)+TV(η π)+ + δ.
h ∼ρ S ≤ k r4m s2m
Proof. We apply Theorem4.2, inparticular,Equation(7) andwe use the fact thatWΓ(ρ,η)=L(m,δ)TV(η,π).
We are now able to prove Corollary 5.2.
Corollary C.4. Assume that ℓ [0,1]. With probability at least 1 δ over m, we have for all ρ ( ),
∈ − S ∼D ∈P H
E ∆ (h) 4R( )+L 2ln δ4 TV(ρ,η)+TV(η π)+ 1 + ln δ2 ,
h ∼ρ S ≤
(cid:18)
H
q
m
(cid:19)
k r4m s2m
and in particular, for all h ,
∈H
2ln4 1 ln2
∆ (h) 4R( )+ δ + + δ.
S ≤ H s m r4m s2m
Proof. We combine Corollary C.3 and Theorem 5.1 to obtain the first inequality. The second inequality is
obtained by setting η =π and upper-bounding TV(ρ,η) 1.
≤
C.7. Proofs for heavy-tailed SGD
We first state properly the assumption (H1), (H2) extracted from Raj et al. (2023b).
• (H1) (Quasi-smoothness)Thereexistconstantsθ >0andK 0suchthat x y, Fˆ (x) Fˆ (y)
0
θ x y 2 K for all x,y ; ≥ h − ∇ S −∇ S i≥
0
k − k − ∈H
• (H2) (Regularity) Rename b= Fˆ . Assume b to be 3(Rd,R). There exist constants θ ,θ ,θ 0 such
1 2 3
∇ S C ≥
that
b(x) θ v , v,x Rd,
v 1
k∇ k≤ k k ∈
b(x) θ v v , v ,v ,x Rd,
k∇v1∇v2
k≤
2
k
1
kk
2
k
1 2
∈
b(x) θ v v v , v ,v ,v ,x Rd,
k∇v1∇v2∇v3
k≤
3
k
1
kk
2
kk
3
k
1 2 3
∈
29where the directional derivatives are defined as
b(x+ǫv ) b(x)
1
b(x):= lim −
∇v1
ǫ 0 ǫ
→
b(x+ǫv ) b(x)
f(x):= lim
∇v1 2 −∇v1
∇v2∇v1
ǫ 0 ǫ
→
and
b(x+ǫv ) b(x)
b(x):= lim
∇v2∇v1 3 −∇v2∇v1
.
∇v3∇v2∇v1
ǫ 0 ǫ
→
We are now ready to prove Corollary5.3.
Corollary C.5. Let π be a data-free prior and δ (0,1). Assume ℓ [0,1]. Assume that, for any δ (0,1),
′
with probability 1 δ , h ∆2(h) is L(m,δ )-Lips∈ chitz. Assume that∈ Fˆ is quasi-smooth and regular∈ ((H1)
′ ′
and (H2) in Appe− ndix C.→ 7). TShus, for any α (1,2), with probability∇ at lS east 1 δ over m, we have, for
0
∈ − S ∼D
any α [α ,2],
0
∈
KL(η π)+ln4√m
h
∼E ρα∆ S(h) ≤sL(m,δ/2)C α0f(α,d)+ k
2m
δ ,
where f(α,d)=dln(d)(2 −α)ln(1/2 −α) and C
α0
>0.
Proof. We start from Theorem 4.1 with the Gaussian prior π. We have, with probability at least 1 δ, for any
−
distributions η,ρ,
KL(η π)+ln4√m
hE ρ∆ S(h) ≤sL(m,δ/2)W 1(ρ,η)+ k
2m
δ .
∼
We take η = ρ ,ρ = ρ . We then know that under (H1),(H2), Deng et al. (2023, Theorem 1) gives, for any
2 α
α>α ,
0
W (ρ ,ρ ) C f(α,d).
1 α 2
≤
α0
Plugging this into the previous bound concludes the proof.
D. About The Experiments
D.1. Lipschitzness for neural networks
Lemma D.1. Consider the neural networks and the setting of Section 6.2 (with = x Rn x 1 ), then,
2
X { ∈ |k k ≤ }
the loss is α 2(K+2)-Lipschitz w.r.t. the parameters on the subspace where all the weight matrices have their
Frobenius norm bounded by 1 (no constraint on biases).
p
Proof. First of all, since the loss is α-Lipschitz w.r.t. the outputs, we have
ℓ(hw,z) ℓ(hw′,z) α hw(x) hw′(x)
| − |≤ k − k
=α (WhK(x)+b) (W hK (x)+b)
w ′ w′ ′
k − k
=α WhK(x)+W hK (x)+W hK(x) W hK(x)+b b
w ′ w′ ′ w ′ w ′
k − − k
α (W W ′)hK w(x) + W ′(hK w(x) hK w′(x)) + b b
′
.
≤ k − k k − k k − k
(cid:16) (cid:17)
30Moreover,note that we have
(W W ′)hK w(x) W W
′ F
hK w(x) W W
′
F,
k − k≤k − k k k≤k − k
and W (hK(x) hK (x)) W hK(x) hK (x) hK(x) hK (x)
′ w w′ ′ F w w′ w w′
k − k≤k k k − k≤k − k
since we have hL(x) 1 and W 1. Hence, we can deduce that
w ′ F
k k≤ k k ≤
ℓ(hw,z) ℓ(hw′,z) α W W
′ F
+ hK w(x) hK w′(x) + b b
′
.
| − |≤ k − k k − k k − k
(cid:16) (cid:17)
Moreover,for any i 1,...,L (with h0(x)=x), we have
∈{ }
khi w(x) −hi w′(x) k= kProj(Leaky(W ihi w−1(x)+b i)) −Proj(Leaky(W i′hi w−′1(x)+b′i))
k
≤kLeaky(W ihi w−1(x)+b i) −Leaky(W i′hi w−′1(x)+b ′i)
k
≤k(W ihi w−1(x)+b i) −(W i′hi w−′1(x)+b ′i)
k
≤
k(W
i
−W i′)hi w−1(x) k+ kW i′(hi w−1(x) −hi w−′1(x)) k+ kb
i
−b
′ik
(cid:16) (cid:17)
≤
kW
i
−W
i′ kF
+ khi w−1(x) −hi w−′1(x) k+ kb
i
−b
′ik
. (27)
(cid:16) (cid:17)
Hence, by applying Equation (27) for any i 1,...,K we can deduce that
∈{ }
K
|ℓ(hw,z) −ℓ(hw′,z) |≤α kW −W
′
k+ kb −b
′
k+ ( kW
i
−W
i′
k+ kb
i
−b ′ik) !. (28)
i=1
X
Now,the goalis to upper-bound W W + b b + K ( W W + b b )by w w multipliedby
k − ′ k k − ′ k i=1 k i − i′ k k i − ′ik k − ′ k
a constant. To do so, we can use the fact that for any real numbers a,b, we have (a+b)2 2(a2+b2) in order
P ≤
to gather the norms. If K is odd, we can apply the property in a divide-and-conquer manner to obtain
L
kW −W′ k+ kb −b′ k+ ( kW i −W i′ k+ kb i −b′ik)
≤
2log 2(2(K+1)) kw −w ′
k
Xi=1 p
= 2(K+1) w w ′
k − k
p2(K+2) w w ′ (29)
≤ k − k
p
and when L is even, we also have
L
kW −W
′
k+ kb −b
′
k+ ( kW
i
−W
i′
k+ kb
i
−b ′ik)
≤
2log 2(2(K+2)) kw −w
′
k
Xi=1 p
= 2(K+2) w w . (30)
′
k − k
p
Hence, we can deduce from Equations (28) to (30) that
ℓ(hw,z) ℓ(hw′,z) α 2(K+2) w w ′ ,
| − |≤ k − k
p
which concludes the proof.
D.2. Additional Insights on the Experiments
We present, in this section, additional information concerning the experimental setting. Moreover, we present
complementary experiments with Equations (5) to (7); the experiments are presented from Tables 2 to 10.
About additional experiments with data-dependent priors. In the PAC-Bayesian framework, using
data-dependent priors to tighten the bound has been a popular strategy (see e.g., Ambroladze et al., 2006;
Parrado-Hernándezet al., 2012; Dziugaite et al., 2021; Pérez-Ortiz et al., 2021c; Viallard et al., 2023a). Hence,
31we provide additional experiments where the prior vector w is learned with a portion of the original training
π
set. More precisely, the original training set is split into two sets and ; the set is used when we consider
′
data-free priors while aims to produce a good vector w . To dS o so, tS he weight vS ector w by performing an
′ π π
S
empirical risk minimisation (while keeping the same parameters for the optimiser). The vector is selected by
early stopping with the set (with the empirical risk as metric). Since we select the weights by early stopping,
S
we have to perform a union bound to obtain a bound holding for all the intermediate vectors. Hence, imagine
that we have T weight vectors (corresponding to T epochs), we have to replace ln1 by lnT to obtain a bound
δ δ
holding for the T weight vectors with probability at least 1 δ. Moreover, note that when we estimate the
−
Lipschitz constant, we still use the original training set (to obtain a better Lipschitz constant).
Learning the prior variance σ2. Inordertolearnthepriorvariance,weperformaunionboundsimilartothe
π
one ofDziugaite & Roy (2017). To do so,for eachboundthat uses a Gaussianprior (w ,σ2I ), we consider a
N π π d
boundthatholdswithprobabilityatleast1 6 wherethevarianceisdefinedbyσ2 =cexp( j/b). Thevalue
−π2j2 π −
c corresponds to an upper bound of the variance, and b corresponds to a level of precision; we set c = 1.1 and
b=100. Byperformingtheunionboundforallj N,weobtainaboundholdingforalldiscretisedvariancewith
∈
probabilityatleast1 −δ. Bydoingso,theconfidencetermln(1/δ)isreplacedby2ln(bln(c/σ π2))+ln(π2/6δ). Note
that during the optimization, we do not constrain the variance, but we discretise it only during the evaluation
of the bound.
Estimating the empirical risk for Gaussian posteriors. In order to evaluate the (expected) empirical
risk E R (h), we perform a Monte Carlo sampling. However, to have a bound that remains valid, we use
h ρ
Hoeffdin∼ g’sS inequality to obtain with probability at least 1 δ over h ,...,h ρT
1 T
− ∼
1 T 2ln1
E Rˆ (h) Rˆ (h )+ δ.
t
h ρ S ≤ T S s T
∼ t=1
X
In the experiments, we set T =1000.
About the union bounds in the bounds. Depending onthe boundwe consider,we haveto performaunion
bound to take into accountthe Lipschitz constantand the samplingfor the estimation ofthe empiricalrisk. For
instance,inthe Diraccase,we onlyhaveto considerthe Lipschitz constant. Hence, toperformthe unionbound,
the bound holds with probability at least 1 δ/2 as well as the Lipschitz constant L(m,δ/2). For the Gaussian
−
case, when we have ρ =η (i.e., with W (ρ,η) =0), we perform a union bound with Hoeffding’s bound holding
1
with probabilityat least 1 δ/2 as well as the bound. However,when we consider the Wassersteindistance, the
−
originalbound, the Lipschitz constant,andHoeffding’sbound holdindividually with probabilityatleast1 δ/3
−
before applying the union bound.
Value of the Wasserstein distance. The value of the Wasserstein distance is given by the following formula.
• Wasserstein distance (Gaussians posterior and prior):
W (ρ,η) W (ρ,η)= w w 2+[d(σ σ )]2
1 2 η η ρ
≤ k − k −
q
• Wasserstein distance (Dirac posterior, Gaussian prior):
Γ((d+1)/2)
W (ρ,η) w w +σ √2
1 η 2 η
≤k − k Γ(d/2)
Value of the f-divergences. The different f-divergences between two Gaussian distributions are defined as
follows.
• KL divergence:
1 σ2 1 σ2
KL(η π)= η d d+ w w 2+dln π
k 2 "σ π2 − σ π2k η − π k2 (cid:18)σ η2 (cid:19)#
32Table 2: Results of the bound minimisation of Theorem 4.1 (see Equation (12)) and the ones associated with
Amit et al. (2022) and Maurer (2004) when the prior is learned with 25% of the original training set. “Test” is
the test risk Rˆ (hw), “Bnd” represents the bound value, “Wass” represents the upper bound of the Wasserstein
T
distance multiplied by the Lipschitz constant, and “KL” is the KL divergence divided by 2m.
Theorem4.1 Amitetal.(2022) Theorem4.1 Amitetal.(2022)
Test Bnd Wass. KL Test Bnd Wass. Test Bnd Wass. KL Test Bnd Wass.
FashionMNIST 0.0630.088 0.000 0.000 0.064 0.289 0.052 0.111 0.248 0.014 0.004 0.884 3.537 7.041
MNIST 0.0380.068 0.000 0.000 0.038 0.199 0.025 0.088 0.238 0.016 0.004 0.776 2.414 2.680
Mushrooms 0.0040.077 0.000 0.000 0.004 0.073 0.002 0.002 0.177 0.019 0.005 0.487 3.832 11.247
Phishing 0.0700.130 0.000 0.000 0.070 0.129 0.002 0.068 0.211 0.014 0.002 0.438 1.903 2.095
Yeast 0.1880.363 0.000 0.000 0.188 0.393 0.025 0.329 0.614 0.034 0.009 0.383 2.469 4.318
(a) Linear models hw with ρ=δw (b) Neural networks hw with ρ=δw
Theorem4.1 Maurer(2004) Theorem4.1 Maurer(2004)
Test Bnd Wass. KL Test Bnd KL Test Bnd Wass. KL Test Bnd KL
FashionMNIST 0.1190.603 0.000 0.122 0.068 0.171 0.000 0.770 5.288 0.000 14.706 0.479 0.737 0.023
MNIST 0.0790.531 0.000 0.098 0.042 0.153 0.000 0.825 5.057 0.000 12.867 0.551 0.937 0.065
Mushrooms 0.0050.296 0.000 0.028 0.005 0.292 0.028 0.456 6.313 0.000 24.986 0.340 0.711 0.058
Phishing 0.0760.226 0.000 0.000 0.076 0.396 0.042 0.386 4.546 0.000 12.394 0.239 0.593 0.049
Yeast 0.1900.454 0.000 0.000 0.220 0.883 0.235 0.586 7.151 0.000 31.416 0.412 0.758 0.026
(c) Linear models hw with ρ= (w,σ2I d) (d)Neuralnetworkshw withρ= (w,σ2I d)
N N
• (Squared) Hellinger distance:
d d w w 2
H2(η π)=1 exp ln(σ2)+ln(σ2) ln(1(σ2+σ2)) k − π k
k − 4 π − 2 2 π − 4(σ2+σ2)
(cid:18) π (cid:19)
(cid:0) (cid:1)
• Reverse KL divergence:
1 σ2 1 σ2
KL(η π)=KL(π η)= πd d+ w w 2+dln η
k k 2 "σ η2 − σ η2k π − η k2 σ π2 !#
33Table3: Results ofthe boundminimisationforlinearmodelsofthe differentbounds whenthe priorisinitialized
with the vector of zeros and for the Dirac posterior distribution ρ = δw. “Test” is the test risk Rˆ (hw), “Bnd”
T
represents the bound value, “Wass” represents the upper bound of the Wasserstein distance multiplied by the
Lipschitz constant, and “KL” is the KL divergence divided by 2m.
Test Bnd Wass.
Test Bnd Wass.
FashionMNIST 0.442 0.819 0.371
FashionMNIST 0.361 1.040 0.465 MNIST 0.480 0.872 0.375
MNIST 0.304 1.078 0.583 Mushrooms 0.494 0.533 0.009
Mushrooms 0.498 0.614 0.012 Phishing 0.497 0.527 0.005
Phishing 0.497 0.569 0.004 Yeast 0.892 0.972 0.012
Yeast 0.504 1.203 0.482
Equation (7) with π =η
Theorem 4.1 with π =η
(Similar to Theorem 6 of Viallard et al.,
(Amit et al., 2022)
2023b)
Test Bnd Wass. KL Test Bnd Wass. KL
FashionMNIST 0.059 2.077 0.001 0.000 FashionMNIST 0.894 1.037 0.038 0.000
MNIST 0.033 2.056 0.001 0.000 MNIST 0.896 1.017 0.034 0.000
Mushrooms 0.000 2.098 0.000 0.000 Mushrooms 0.498 0.728 0.012 0.000
Phishing 0.068 2.145 0.000 0.000 Phishing 0.499 0.688 0.007 0.000
Yeast 0.346 2.566 0.001 0.001 Yeast 0.896 1.398 0.012 0.000
Equation (6) Equation (5)
Test Bnd Wass. KL
FashionMNIST 0.115 0.317 0.017 0.025
MNIST 0.077 0.294 0.018 0.027
Mushrooms 0.026 0.190 0.009 0.015
Phishing 0.085 0.225 0.005 0.013
Yeast 0.353 0.566 0.014 0.017
Theorem 4.1
34Table4: Resultsoftheboundminimisationforlinearmodelsofthedifferentboundswhenthepriorweightvector
is learnedwith 25%ofthe originaltraining setandforthe Diracposteriordistributionρ=δw. “Test” is the test
risk Rˆ (hw), “Bnd” represents the bound value, “Wass” represents the upper bound of the Wasserstein distance
T
multiplied by the Lipschitz constant, and “KL” is the KL divergence divided by 2m.
Test Bnd Wass.
Test Bnd Wass.
FashionMNIST 0.065 0.080 0.008
FashionMNIST 0.064 0.289 0.052 MNIST 0.038 0.063 0.010
MNIST 0.038 0.199 0.025 Mushrooms 0.004 0.061 0.004
Mushrooms 0.004 0.073 0.002 Phishing 0.070 0.115 0.003
Phishing 0.070 0.129 0.002 Yeast 0.188 0.337 0.016
Yeast 0.188 0.393 0.025
Equation (7) with π =η
Theorem 4.1 with π =η
(Similar to Theorem 6 of Viallard et al.,
(Amit et al., 2022)
2023b)
Test Bnd Wass. KL Test Bnd Wass. KL
FashionMNIST 0.058 2.083 0.001 0.000 FashionMNIST 0.065 0.192 0.028 0.000
MNIST 0.033 2.062 0.001 0.000 MNIST 0.038 0.246 0.065 0.000
Mushrooms 0.000 2.137 0.000 0.000 Mushrooms 0.004 0.313 0.014 0.000
Phishing 0.068 2.178 0.000 0.000 Phishing 0.070 0.320 0.007 0.000
Yeast 0.185 2.514 0.001 0.001 Yeast 0.188 0.896 0.018 0.000
Equation (6) Equation (5)
Test Bnd Wass. KL
FashionMNIST 0.063 0.088 0.000 0.000
MNIST 0.038 0.068 0.000 0.000
Mushrooms 0.004 0.077 0.000 0.000
Phishing 0.070 0.130 0.000 0.000
Yeast 0.188 0.363 0.000 0.000
Theorem 4.1
35Table 5: Results of the bound minimisation for neural network models of the different bounds when the prior is
initializedwiththevectorofzerosandfortheDiracposteriordistributionρ=δw. “Test” isthetestriskRˆ (hw),
T
“Bnd” represents the bound value, “Wass” represents the upper bound of the Wasserstein distance multiplied by
the Lipschitz constant, and “KL” is the KL divergence divided by 2m.
Test Bnd Wass.
Test Bnd Wass.
FashionMNIST 0.893 1.637 0.736
FashionMNIST 0.884 3.537 7.041 MNIST 0.806 1.583 0.769
MNIST 0.776 2.414 2.680 Mushrooms 0.490 1.276 0.763
Mushrooms 0.487 3.832 11.247 Phishing 0.436 1.458 0.979
Phishing 0.438 1.903 2.095 Yeast 0.431 3.089 2.586
Yeast 0.383 2.469 4.318
Equation (7) with π =η
Theorem 4.1 with π =η
(Similar to Theorem 6 of Viallard et al.,
(Amit et al., 2022)
2023b)
Test Bnd Wass. KL Test Bnd Wass. KL
FashionMNIST 0.095 2.237 0.060 0.000 FashionMNIST 0.891 3.785 1.421 0.000
MNIST 0.057 2.204 0.060 0.000 MNIST 0.836 3.323 1.217 0.000
Mushrooms 0.000 2.278 0.089 0.000 Mushrooms 0.493 6.007 2.501 0.000
Phishing 0.047 2.232 0.060 0.000 Phishing 0.444 8.790 3.390 0.000
Yeast 0.346 2.751 0.088 0.001 Yeast 0.435 9.357 3.312 0.001
Equation (6) Equation (5)
Test Bnd Wass. KL
FashionMNIST 0.162 0.683 0.135 0.139
MNIST 0.111 0.673 0.152 0.158
Mushrooms 0.082 0.645 0.167 0.148
Phishing 0.123 0.642 0.132 0.136
Yeast 0.335 0.707 0.061 0.059
Theorem 4.1
36Table 6: Results of the bound minimisation for neural network models of the different bounds when the prior
weight vector is learned with 25% of the original training set and for the Dirac posterior distribution ρ = δw.
“Test” is the test risk Rˆ (hw), “Bnd” represents the bound value, “Wass” represents the upper bound of the
T
Wasserstein distance multiplied by the Lipschitz constant, and “KL” is the KL divergence divided by 2m.
Test Bnd Wass.
Test Bnd Wass.
FashionMNIST 0.113 0.213 0.090
FashionMNIST 0.125 0.640 0.267 MNIST 0.091 0.190 0.082
MNIST 0.089 0.286 0.037 Mushrooms 0.003 0.437 0.380
Mushrooms 0.002 0.277 0.073 Phishing 0.068 0.345 0.234
Phishing 0.068 0.965 0.805 Yeast 0.325 1.291 0.824
Yeast 0.328 2.138 3.217
Equation (7) with π =η
Theorem 4.1 with π =η
(Similar to Theorem 6 of Viallard et al.,
(Amit et al., 2022)
2023b)
Test Bnd Wass. KL Test Bnd Wass. KL
FashionMNIST 0.427 2.580 0.060 0.000 FashionMNIST 0.112 6.877 3.350 0.000
MNIST 0.210 2.372 0.060 0.000 MNIST 0.090 5.453 2.770 -0.000
Mushrooms 0.000 2.315 0.088 0.000 Mushrooms 0.002 2.269 0.962 0.000
Phishing 0.055 2.276 0.060 0.000 Phishing 0.069 1.816 0.725 0.000
Yeast 0.345 2.856 0.087 0.001 Yeast 0.325 3.433 1.116 0.000
Equation (6) Equation (5)
Test Bnd Wass. KL
FashionMNIST 0.111 0.248 0.014 0.006
MNIST 0.088 0.238 0.016 0.005
Mushrooms 0.002 0.177 0.019 0.007
Phishing 0.068 0.211 0.014 0.003
Yeast 0.329 0.614 0.034 0.012
Theorem 4.1
37Table7: Results ofthe boundminimisationforlinearmodelsofthe differentbounds whenthe priorisinitialized
with the vector of zeros and for the Gaussian posterior distribution ρ = (w,σ2I ). “Test” is the test risk
d
N
Rˆ (hw), “Bnd” represents the bound value, “Wass” represents the upper bound of the Wasserstein distance
T
multiplied by the Lipschitz constant, and “KL” is the KL divergence divided by 2m.
Test Bnd Wass.
Test Bnd Wass.
FashionMNIST 0.515 8.684 8.067
FashionMNIST 0.256 6.153 33.787 MNIST 0.510 8.761 8.142
MNIST 0.190 8.651 69.962 Mushrooms 0.460 3.161 2.559
Mushrooms 0.368 0.937 0.228 Phishing 0.485 1.895 1.275
Phishing 0.498 0.709 0.012 Yeast 0.820 3.811 2.779
Yeast 0.635 1.347 0.370
Equation (7) with π =η
Theorem 4.1 with π =η
(Similar to Theorem 6 of Viallard et al.,
(Amit et al., 2022)
2023b)
Test Bnd KL Test Bnd KL
FashionMNIST 0.059 2.150 0.000 FashionMNIST 0.843 1.006 0.000
MNIST 0.033 2.129 0.000 MNIST 0.841 0.997 0.000
Mushrooms 0.000 2.147 0.000 Mushrooms 0.481 0.846 0.000
Phishing 0.069 2.201 0.000 Phishing 0.491 0.768 0.000
Yeast 0.167 2.389 0.001 Yeast 0.726 1.309 0.000
Equation (6) with ρ=η Equation (5) with ρ=η
Test Bnd KL Test Bnd Wass. KL
FashionMNIST 0.140 0.359 0.019 FashionMNIST 0.217 2.415 0.045 0.000
MNIST 0.096 0.333 0.021 MNIST 0.128 2.338 0.045 0.000
Mushrooms 0.035 0.258 0.016 Mushrooms 0.476 0.713 0.001 0.000
Phishing 0.094 0.297 0.013 Phishing 0.498 0.684 0.000 0.000
Yeast 0.372 0.638 0.016 Yeast 0.688 1.094 0.000 0.000
Theorem 4.1 with ρ=η Equation (6)
Test Bnd Wass. KL Test Bnd Wass. KL
FashionMNIST 0.784 0.965 0.011 0.000 FashionMNIST 0.140 0.364 0.000 0.019
MNIST 0.783 0.960 0.011 0.000 MNIST 0.097 0.338 0.000 0.021
Mushrooms 0.490 0.814 0.002 0.000 Mushrooms 0.017 0.265 0.000 0.022
Phishing 0.499 0.765 0.000 0.000 Phishing 0.094 0.302 0.000 0.013
Yeast 0.726 1.312 0.000 0.000 Yeast 0.372 0.644 0.000 0.016
Equation (5) Theorem 4.1
38Table8: Resultsoftheboundminimisationforlinearmodelsofthedifferentboundswhenthepriorweightvector
is learned with 25% of the originaltraining set, i.e., and for the Gaussianposterior distribution ρ= (w,σ2I ).
d
N
“Test” is the test risk Rˆ (hw), “Bnd” represents the bound value, “Wass” represents the upper bound of the
T
Wasserstein distance multiplied by the Lipschitz constant, and “KL” is the KL divergence divided by 2m.
Test Bnd Wass.
Test Bnd Wass.
FashionMNIST 0.8351.588 0.647
FashionMNIST 0.5406.33832.621 MNIST 0.8381.575 0.632
MNIST 0.4586.79838.968 Mushrooms 0.4010.633 0.080
Mushrooms 0.0603.57011.686 Phishing 0.4650.669 0.061
Phishing 0.0710.534 0.136 Yeast 0.8581.193 0.094
Yeast 0.1991.460 1.330
Equation (7) with π =η
Theorem 4.1 with π =η
(Similar to Theorem 6 of Viallard et al.,
(Amit et al., 2022)
2023b)
Test Bnd KL Test Bnd KL
FashionMNIST 0.0582.1570.000 FashionMNIST 0.7820.9660.000
MNIST 0.0342.1360.000 MNIST 0.7850.9630.000
Mushrooms 0.0002.1930.000 Mushrooms 0.0620.7490.000
Phishing 0.0692.2380.000 Phishing 0.1510.4790.000
Yeast 0.1852.5350.001 Yeast 0.2831.0810.000
Equation (6) with ρ=η Equation (5) with ρ=η
Test Bnd KL Test Bnd Wass. KL
FashionMNIST 0.0680.1710.000 FashionMNIST 0.2292.349 0.005 0.000
MNIST 0.0420.1530.000 MNIST 0.1312.262 0.005 0.000
Mushrooms 0.0050.2920.038 Mushrooms 0.0002.216 0.000 0.000
Phishing 0.0760.3960.056 Phishing 0.0772.269 0.000 0.000
Yeast 0.2200.8830.312 Yeast 0.2850.737 0.000 0.000
Theorem 4.1 with ρ=η Equation (6)
Test Bnd Wass. KL Test Bnd Wass. KL
FashionMNIST 0.8321.106 0.003 0.000 FashionMNIST 0.1190.603 0.000 0.163
MNIST 0.8341.085 0.003 0.000 MNIST 0.0790.531 0.000 0.131
Mushrooms 0.0550.419 0.000 0.000 Mushrooms 0.0050.296 0.000 0.037
Phishing 0.1300.454 0.000 0.000 Phishing 0.0760.226 0.000 0.000
Yeast 0.2781.042 0.000 0.000 Yeast 0.1900.454 0.000 0.000
Equation (5) Theorem 4.1
39Table 9: Results of the bound minimisation for neural network models of the different bounds when the prior is
initialized with the vectorofzerosand for the Gaussianposteriordistribution ρ= (w,σ2I ). “Test” is the test
d
N
risk Rˆ (hw), “Bnd” represents the bound value, “Wass” represents the upper bound of the Wasserstein distance
T
multiplied by the Lipschitz constant, and “KL” is the KL divergence divided by 2m.
Test Bnd Wass.
Test Bnd Wass.
FashionMNIST 0.855 390.291 389.331
FashionMNIST 0.844101.59710133.044 MNIST 0.8561153.9561152.995
MNIST 0.846 40.370 1554.983 Mushrooms 0.509 425.913 425.253
Mushrooms 0.489134.80418016.426 Phishing 0.4642371.8502371.239
Phishing 0.488 51.590 2602.070 Yeast 0.736 968.250 967.294
Yeast 0.726 69.722 4747.634
Equation (7) with π =η
Theorem 4.1 with π =η
(Similar to Theorem 6 of Viallard et al.,
(Amit et al., 2022)
2023b)
Test Bnd KL Test Bnd KL
FashionMNIST 0.1472.2460.000 FashionMNIST 0.88853.5740.000
MNIST 0.0872.1930.000 MNIST 0.88753.4350.000
Mushrooms 0.0002.1470.000 Mushrooms 0.50232.7200.002
Phishing 0.0702.2030.000 Phishing 0.49328.9210.001
Yeast 0.3352.5660.001 Yeast 0.77224.3890.008
Equation (6) with ρ=η Equation (5) with ρ=η
Test Bnd KL Test Bnd Wass. KL
FashionMNIST 0.6471.1150.147 FashionMNIST 0.89547.29622.1570.000
MNIST 0.6231.1050.154 MNIST 0.89516.096 6.591 0.000
Mushrooms 0.3760.9340.220 Mushrooms 0.50112.524 5.236 0.000
Phishing 0.2430.8130.232 Phishing 0.500 9.806 3.891 0.000
Yeast 0.6522.3352.472 Yeast 0.748 6.761 1.932 0.001
Theorem 4.1 with ρ=η Equation (6)
Test Bnd Wass. KL Test Bnd Wass. KL
FashionMNIST 0.89159.62014.9660.000 FashionMNIST 0.6631.132 0.001 0.143
MNIST 0.89159.62114.9660.000 MNIST 0.6101.105 0.001 0.160
Mushrooms 0.50114.762 2.610 0.001 Mushrooms 0.3840.935 0.001 0.209
Phishing 0.49911.764 1.196 0.000 Phishing 0.2870.972 0.001 0.348
Yeast 0.76816.625 1.772 0.004 Yeast 0.6482.295 0.003 2.342
Equation (5) Theorem 4.1
40Table 10: Results of the bound minimisation for neural network models of the different bounds when the prior
weight vector is learned with 25% of the original training set, i.e., and for the Gaussian posterior distribution
ρ= (w,σ2I d). “Test” is the test risk Rˆ (hw), “Bnd” represents the bound value, “Wass” represents the upper
N T
bound of the Wasserstein distance multiplied by the Lipschitz constant, and “KL” is the KL divergence divided
by 2m.
Test Bnd Wass.
Test Bnd Wass.
FashionMNIST 0.898411.972410.968
FashionMNIST 0.89824.128 535.465 MNIST 0.898412.984411.980
MNIST 0.89839.2811466.337 Mushrooms 0.500228.537227.884
Mushrooms 0.50019.780 368.224 Phishing 0.500124.881124.238
Phishing 0.500 7.916 53.671 Yeast 0.890101.518100.387
Yeast 0.76020.853 399.991
Equation (7) with π =η
Theorem 4.1 with π =η
(Similar to Theorem 6 of Viallard et al.,
(Amit et al., 2022)
2023b)
Test Bnd KL Test Bnd KL
FashionMNIST 0.1382.2460.000 FashionMNIST 0.897 1.121 0.000
MNIST 0.1882.3050.000 MNIST 0.897 1.136 0.000
Mushrooms 0.0042.1970.000 Mushrooms 0.500 0.949 0.000
Phishing 0.0652.2320.000 Phishing 0.500 1.191 0.000
Yeast 0.3322.6930.001 Yeast 0.80718.9570.008
Equation (6) with ρ=η Equation (5) with ρ=η
Test Bnd KL Test Bnd Wass. KL
FashionMNIST 0.4790.7370.030 FashionMNIST 0.8985.087 1.998 0.000
MNIST 0.5510.9370.087 MNIST 0.8985.085 1.998 0.000
Mushrooms 0.3400.7110.077 Mushrooms 0.5002.963 1.108 0.000
Phishing 0.2390.5930.065 Phishing 0.5001.647 0.458 0.000
Yeast 0.4120.7580.035 Yeast 0.7544.557 1.032 0.001
Theorem 4.1 with ρ=η Equation (6)
Test Bnd Wass. KL Test Bnd Wass. KL
FashionMNIST 0.8985.113 1.997 0.000 FashionMNIST 0.7705.288 0.000 19.609
MNIST 0.8986.115 2.496 0.000 MNIST 0.8255.057 0.000 17.155
Mushrooms 0.5002.527 0.830 0.000 Mushrooms 0.4566.313 0.000 33.309
Phishing 0.5001.460 0.305 0.000 Phishing 0.3864.546 0.000 16.523
Yeast 0.7678.521 0.967 0.002 Yeast 0.5867.151 0.000 41.850
Equation (5) Theorem 4.1
41