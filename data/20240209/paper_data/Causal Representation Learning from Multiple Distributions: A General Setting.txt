Causal Representation Learning from Multiple Distributions: A General Setting
KunZhang12 ShaoanXie1 IgnavierNg1 YujiaZheng1
Abstract Moreover,thereareusuallychangesonthecausalmecha-
nismsinreal-world,suchastheheterogeneousornonsta-
Inmanyproblems,themeasuredvariables(e.g.,
tionary data. Identifying the hidden causal variables and
imagepixels)arejustmathematicalfunctionsof
theirstructurestogetherwiththechangeofthecausalmech-
thehiddencausalvariables(e.g.,theunderlying
anism is in pressing need to understand the complicated
concepts or objects). For the purpose of mak-
real-worldcausalprocess. Thishasbeenrecentlyknownas
ingpredictionsinchangingenvironmentsormak-
causalrepresentationlearning(Scho¨lkopfetal.,2021).
ing proper changes to the system, it is helpful
to recover the hidden causal variables Z i and It is worth noting that identifying only the hidden causal
their causal relations represented by graph G Z. variables but not the structure among them, is already a
Thisproblemhasrecentlybeenknownascausal considerable challenge. In the i.i.d. case, different latent
representationlearning. Thispaperisconcerned representationscanexplainthesameobservationsequally
withageneral,completelynonparametricsetting well,whilenotallofthemareconsistentwiththetruecausal
of causal representation learning from multiple process. For instance, nonlinear independent component
distributions (arising from heterogeneous data analysis(ICA),whereasetofobservedvariablesX isrep-
ornonstationarytimeseries),withoutassuming resentedasamixtureofindependentlatentvariablesZ,i.e,
hard interventions behind distribution changes. X =g(Z),isknowntobeunidentifiablewithoutadditional
Weaimtodevelopgeneralsolutionsinthisfun- assumptions(Comon,1994). Whilebeingastrictlyeasier
damental case; as a by product, this helps see tasksincetherearenorelationsamonghiddenvariables,the
theuniquebenefitofferedbyotherassumptions identifiabilityofnonlinearICAoftenreliesonconditions
suchasparametriccausalmodelsorhardinterven- ondistributionalassumptions(non-i.i.d. data)(Hyva¨rinen&
tions. Weshowthatunderthesparsityconstraint Morioka,2016;2017;Hyva¨rinenetal.,2019;Khemakhem
ontherecoveredgraphoverthelatentvariables et al., 2020a; Sorrenson et al., 2020; Lachapelle et al.,
andsuitablesufficientchangeconditionsonthe 2022;Ha¨lva¨ &Hyva¨rinen,2020;Ha¨lva¨ etal.,2021; Yao
causalinfluences,interestingly,onecanrecover et al., 2022) or specific functional constraints (Comon,
the moralized graph of the underlying directed 1994;Hyva¨rinen&Pajunen,1999;Taleb&Jutten,1999;
acyclicgraph,andtherecoveredlatentvariables Buchholzetal.,2022;Zhengetal.,2022).
and their relations are related to the underlying
To generalize beyond the independent hidden variables
causal model in a specific, nontrivial way. In
and achieve causal representation learning (recovering
somecases,eachlatentvariablecanevenbere-
the latent variables and their causal structure), recent
covered up to component-wise transformations.
advances either introduce additional experiments in the
Experimentalresultsverifyourtheoreticalclaims.
forms of interventional or counterfactual data, or place
more restrictive parametric or graphical assumptions on
1.Introduction
the latent causal model. For observational data, various
Causalrepresentationlearningholdsparamountsignificance graphical conditions have been proposed together with
acrossnumerousfields,offeringinsightsintointricaterela- parametricassumptionssuchaslinearity(Silvaetal.,2006;
tionshipswithindatasets. Mosttraditionalmethodologies Caietal.,2019;Xieetal.,2020;2022;Adamsetal.,2021;
(e.g., causal discovery) assume the observation of causal Huang et al., 2022) and discreteness (Kivva et al., 2021).
variables. Thisassumption,howeverreasonable,fallsshort For interventional data, single-node interventions have
incomplexscenariosinvolvingindirectmeasurements,such beenconsideredtogetherwithparametricassumptions(e.g.,
as electronic signals, image pixels, and linguistic tokens. linearity)onthemixingfunction(Varicietal.,2023;Ahuja
et al., 2023; Buchholz et al., 2022) or also on the latent
1CarnegieMellonUniversity2MohamedbinZayedUniversity causal model (Squires et al., 2023). The nonparametric
ofArtificialIntelligence.
settingsforboththemixingfunctionandcausalmodelhave
beenexploredby(Brehmeretal.,2022;vonKu¨gelgenetal.,
Preprint.
1
4202
beF
7
]GL.sc[
1v25050.2042:viXraCausalRepresentationLearningfromMultipleDistributions:AGeneralSetting
2023; Jiang & Aragam, 2023) together with additional
θ θ θ θ θ
assumptionsoncounterfactualviews(Brehmeretal.,2022), 1 2 3 4 5
distinct paired interventions (von Ku¨gelgen et al., 2023), Z
3
andgraphicalconditions(Jiang&Aragam,2023).
Z Z Z Z
1 2 4 5
Despitetheexcitingdevelopmentsinthefield,onefunda-
mentalquestionpertinenttocausalrepresentationlearning
frommultipledistributionsremainsunanswered–inthemost
generalsituation,withoutassumingparametricmodelson
X
thedata-generatingprocessortheexistenceofhardinterven-
tionsinthedata,whatinformationofthelatentvariablesand Figure 1: The generating process for each hidden causal
thelatentstructurecanberecovered? Thispaperattemptsto variableZ ichanges,governedbyalatentfactorθ i. Theob-
provideananswertoit,which,surprisingly,showsthateach servationsX aregeneratedbyX =g(Z)withanonlinear
latentvariablecanberecovereduptoclearlydefinedinde- mixingfunctiong.
terminacies. Itsuggestswhatwecanachieveinthegeneral
caseandfurthermore,whatuniquecontributionthetypical
assumptionsthatarecurrentlymadeincausalrepresentation wherePA(Z )denotestheparentsofvariableZ , ϵ ’sare
i i i
learningfrommultipledistributionsmaketowardscomplete exogenous noise variables that are mutually independent,
identifiabilityofthelatentvariables(uptocomponent-wise and θ denotes the latent (changing) factor (or effective
i
transformations). Thismaymakeitpossibletofigureout parameters) associated with each model. Here, the data
whatminimalassumptionsareneededtoachievecomplete generatingprocessofeachhiddenvariableZ maychange,
i
identifiability,givenpartialknowledgeofthesystem. e.g., across domains or over time, governed by the corre-
spondinglatentfactorθ ;itiscommonplacetoencounter
Contributions. Concretely,asourcontributions,weshow i
suchchangesincausalmechanismsinpractice(arisingfrom
that under the sparsity constraint on the recovered graph
heterogeneousdataornonstationarytimeseries). Inaddi-
over the latent variables and suitable sufficient change
tion, interventional data can be seen as a special type of
conditionsonthecausalinfluences,interestingly,onecanre-
change,whichqualitativelyrestructurethecausalrelations.
coverthemoralizedgraphoftheunderlyingdirectedacyclic
Astheirnamessuggest,weassumethattheobservationsX
graph (Thm. 3.1), and the recovered latent variables and
areobserved,whilethehiddencausalvariablesZ andlatent
theirrelationsarerelatedtotheunderlyingcausalmodelin
factorsθ =(θ ,...,θ )areunobserved.
aspecific,nontrivialway(Thm. 3.4)–eachlatentvariables 1 n
isrecoveredasafunctionofitselfanditsso-calledintimate Let P and P be the distributions of X and Z, respec-
X Z
neighbors in the Markov network implied by the true tively,andtheircorrespondingprobabilitydensityfunctions
causalstructureoverthelatentvariables. Dependingonthe bep (X;θ)andp (Z;θ),respectively. Tolightentheno-
X Z
propertiesofthetruecausalstructureoverlatentvariables, tation,wedropthesubscriptinthedensitywhenthecontext
thesetofintimateneighborsmightevenbeempty,inwhich isclear. ThelatentSEMinEq. (1)inducesacausalgraph
caseeachlatentvariablecanberecovereduptoaninvertible G withvertices{Z }n andedgesZ →Z ifandonlyif
Z i i=1 j i
transformation(Remark1). Lastly,weshowhowtherecov- Z ∈PA(Z ). WeassumethatG isacyclic,i.e.,adirected
j i Z
eredmoralizedgraphrelatestotheunderlyingcausalgraph acyclicgraph(DAG).Thisimpliesthatthedistributionof
under new relaxations of faithfulness assumption (Thm. variables Z satisfy the Markov property w.r.t. DAG G
Z
3.5). Simulationstudiesverifiedourtheoreticalfindings. (Pearl,2000),i.e.,p(Z;θ) = (cid:81)n p(Z |PA(Z );θ ). We
i=1 i i i
provide an example of the data generating process in Eq.
2.ProblemSetting
(1) and its corresponding latent DAG G in Figure 1. In
Z
particular,giventheobservationsX arisingfrommultiple
LetX =(X ,...,X )beand-dimensionalrandomvector
1 d
distributions(governedbythelatentfactorsθ),ourgoalisto
thatrepresentstheobservations. Weassumethattheyare
recoverthehiddencausalvariablesZ =g−1(X)andtheir
generatedbynhiddencausalvariablesZ =(Z ,...,Z )
1 n
via a nonlinear injective mixing function g : Rn → Rd causalrelationsuptosurprisinglyminorindeterminacies.
(d≥n),whichisalsoaC2diffeomorphism. Furthermore,
thevariablesZ ’sareassumedtofollowastructuralequation 3.LearningCausalRepresentationswith
i
model(SEM)(Pearl,2000). Puttingthemtogether,thedata SparsityConstraints
generatingprocesscanbewrittenas
Inthissection,weprovidetheoreticalresultstoshowhow
X =g(Z) , Z i =f i(PA(Z i),ϵ i;θ i),i=1,...,n. one is able to recover the underlying hidden causal vari-
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ablesandtheircausalrelationsuptocertainindetermina-
Nonlinearmixing LatentSEM
(1) cies. Specifically,weshowthatunderthesparsityconstraint
2
gCausalRepresentationLearningfromMultipleDistributions:AGeneralSetting
on the recovered graph over the latent variables and suit- overtruehiddencausalvariablesZandtheMarkovnetwork
ablesufficientchangeconditionsonthecausalinfluences, M over the estimated hidden variables Zˆ. This result
Zˆ
therecoveredlatentvariablesarerelatedtotheunderlying servesasthebackboneofourfurtheranalysisinthissection.
hiddencausalvariablesinaspecific,nontrivialway. Such Denoteby⊕thevectorconcatenationsymbol.
theoreticalresultsserveasthefoundationofouralgorithm
Theorem 3.1. Let the observations be sampled from the
describedinSection4.
datageneratingprocessinEq. (1),andM betheMarkov
Z
Tostartwith,weestimateamodel(gˆ,fˆ,p )whichassumes networkoverZ. Supposethatthefollowingassumptions
Zˆ
thesamedatageneratingprocessasinEq. (1)andmatches hold:
thetruedistributionofX indifferentdomains:
• A1(Smoothandpositivedensity): Theprobabilityden-
p X(X′;θ′)=p Xˆ(X′;θ′), ∀X′,θ′. (2) sityfunctionoflatentcausalvariablesissmoothand
positive,i.e. p issmoothandp >0overRn.
Z Z
whereXandXˆ aregeneratedfromthetruemodel(g,f,p )
Z
andtheestimatedmodel(gˆ,fˆ,p ),respectively. • A2 (Sufficient changes): For any Z ∈ Rn, there
Zˆ
exist 2n + |M | + 1 values of θ, i.e., θ(u) with
Z
AkeyingredientinourtheoreticalanalysisistheMarkov
u=0,...,2n+|M |,suchthatthevectorsw(Z,u)−
Z
network that represents conditional dependencies among
w(z,0)withu=1,...,2n+|M |arelinearlyinde-
Z
randomvariablesinagraphicalmannerviaanundirected
pendent,wherevectorw(Z,u)isdefinedas
graph. Let M be the Markov network over variables
Z
Z, specifically, with vertices {Z i}n i=1 and edges (i,j) ∈ (cid:18) ∂logp(Z;θ(u)) ∂logp(Z;θ(u))
E(M ) if and only if Z ⊥⊥ Z | Z .1 Also, we w(Z,u)= ,..., ,
Z i j [n]\{i,j} ∂Z ∂Z
1 n
denote by |M | the number of undirected edges in the
Z ∂2logp(Z;θ(u)) ∂2logp(Z;θ(u))(cid:19)
Markovnetwork. InSection3.1,apartfromshowinghow
,...,
toestimatetheunderlyinghiddencausalvariablesuptocer- ∂Z 12 ∂Z n2
tainindeterminacies,wealsoshowthatsuchlatentMarkov (cid:18) ∂2logp(Z;θ(u))(cid:19)
⊕ .
networkM canberecovereduptotrivialindeterminacies
Z ∂Z ∂Z
(i.e.,relabelingofthehiddenvariables). Toachieveso,we
i j (i,j)∈E(MZ)
make use of the following property (assuming that p is
Z
twicedifferentiable): Supposethatwelearn(gˆ,fˆ,p Zˆ)toachieveEq. (2). Then,
foreverypairofestimatedhiddenvariablesZˆ andZˆ that
k l
∂2logp(Z) arenotadjacentintheMarkovnetworkM overZˆ,we
Z
i
⊥⊥Z
j
|Z
[n]\{i,j}
⇐⇒
∂Z ∂Z
=0.
havethefollowingstatements:
Zˆ
i j
Suchaconnectionbetweenpairwiseconditionalindepen-
(a) EachtruehiddencausalvariableZ isafunctionofat
denceandcrossderivativesofthedensityfunctionhasbeen i
mostoneofZˆ andZˆ.
noted by Lin (1997). With the recovered latent Markov k l
networkstructure,weprovideresultsinSection3.2toshow
(b) ForeachpairoftruehiddencausalvariablesZ and
i
howitrelatestothetruelatentcausalDAGG ,byexploiting
Z Z thatareadjacentintheMarkovnetworkM over
j Z
aspecifictypeoffaithfulnessassumptionthatisconsider- Z,atmostoneofthemisafunctionofZˆ orZˆ.
k l
ablyweakerthanthestandardfaithfulnessassumptionused
intheliteratureofcausaldiscovery(Spirtesetal.,2001).
TheproofisprovidedinAppx. A.Hereisthebasicidea.
Leth′ := ∂Zi, andh′′ = ∂2Zi . Undertheassump-
3.1.RecoveringHiddenCausalVariablesandLatent i,l ∂Zˆ
l
i,kl ∂Zˆ k∂Zˆ
l
MarkovNetwork tionsA1andA2, onecanfinallyshowthatthefollowing
constraintshold:
We show how one benefits from multiple distributions to
recover the hidden causal variables and the true Markov h′ h′ =0, (3)
i,l i,k
networkstructureamongthemuptominorindeterminacies,
h′ h′ =0, (4)
bymakinguseofsparsityconstraintandsufficientchange j,l i,k
conditionsonthecausalmechanisms. h′ i′ ,kl =0. (5)
Westartwiththefollowingresultthatprovidesinformation Eq. (3)indicatesthatZ isafunctionofatmostoneofZˆ
about the relationship between the Markov network M Z andZˆ,whileEq. (4)imi pliesthatgiventhatZ andZ ark e
l i j
{Z
i1 }W
n
i=e 1u \se {Z[n i,] Zto j}d .enote {1,...,n} and Z
[n]\{i,j}
to denote a fud nja cc tie on nt oin fM
Zˆ
kar ok ro Zv
ˆ
ln .etworkM Z,atmostoneofthemisa
3CausalRepresentationLearningfromMultipleDistributions:AGeneralSetting
Itisworthnotingthattherequirementofasufficientnum- termedintimateneighborset,playsanimportantrole:
ber of environments has been commonly adopted in the
literature (e.g., see (Hyva¨rinen et al., 2023) for a recent Ψ :={Z |j ̸=i,butZ isadjacenttoZ and
Zi j j i
survey),suchasvisualdisentanglement(Khemakhemetal., allotherneighborsofZ inM }.
i Z
2020b),domainadaptation(Kongetal.,2022),videoanaly-
sis(Yaoetal.,2021),andimage-to-imagetranslation(Xie For example, according to the Markov network implied
etal.,2023). Also,wedonotspecifyexactlyhowtolearn by G in Figure 1, Ψ = {Z }, Ψ = Φ, where Φ
Z Z1 2 Z2
(gˆ,fˆ,p )toachieveEq. (2), andleavethedooropenfor denotes the empty set, Ψ = {Z ,Z }, Ψ = Φ, and
Zˆ Z3 2 4 Z4
differentapproachestobeused,suchasnormalizingflowor Ψ ={Z }.Asanotherexample,accordingtotheMarkov
Z5 4
variationalapproaches. Forexample,weadoptavariational network in Figure 2(b), which is implied by the DAG in
approachinSection4. Figure2(a),wehaveΨ =Φforalli=1,2,...,6.
Zi
Theaboveresultshedslightonhoweachpairoftheesti- Theorem3.4(IdentifiabilityofHiddenCausalVariables).
mated latent variables Zˆ and Zˆ that are not adjacent in Lettheobservationsbesampledfromthedatagenerating
k l
MarkovnetworkM Zˆ relatetothetruehiddencausalvari- processinEq. (1),andM Z betheMarkovnetworkover
ablesZ. Withoutanyconstraintontheestimatingprocess,a Z. LetN Zi bethesetofneighborsofvariableZ i inM Z.
trivialsolutionwouldbeacompletegraphoverZˆ. Toavoid SupposethatAssumptionsA1andA2fromTheorem1hold.
it,weenforcethesparsityoftheMarkovnetworkoverZˆ. Suppose also that we learn (gˆ,fˆ,p Zˆ) to achieve Eq. (2)
withtheminimalnumberofedgesofMarkovnetworkM
Zˆ
Infact,theMarkovnetworkoftheunderlyingDAGG Z can overZˆ. Then,thereexistsapermutationπoftheestimated
be recovered, as shown in the following theorem, with a hiddenvariables,denotedasZˆ ,suchthateachZˆ isa
π π(i)
proofprovidedinAppx. B.
functionof(asubsetof)thevariablesin{Z }∪Ψ .
i Zi
Theorem3.2(IdentifiabilityofLatentMarkovNetwork).
Lettheobservationsbesampledfromthedatagenerating The proof is given in Appx. D. It is worth noting that in
processinEq. (1),andM betheMarkovnetworkover manycases,theaboveresultalreadyenablesustorecover
Z
Z. SupposethatAssumptionsA1andA2fromTheorem1 someofthehiddenvariablesuptoacomponent-wisetrans-
hold. Supposealsothatwelearn(gˆ,fˆ,p )toachieveEq. formation.
Zˆ
(2)withtheminimalnumberofedgesofMarkovnetwork Remark1. Nomatterhowmanyneighborseachhidden
M ZˆoverZˆ.Then,theMarkovnetworkM Zˆoverestimated causalvariableZ ihas,aslongaseachofitsneighborsis
hiddenvariablesZˆisisomorphictothetruelatentMarkov
notadjacenttoatleastoneotherneighborintheMarkov
networkM . networkM ,thenZ canberecovereduptoacomponent-
Z Z i
wisetransformation.
SincetraditionalnonlinearICAalwayshasavalidsolution
(toproducenonlinearindependentcomponents)(Hyva¨rinen Even if the above case does not hold, Theorem 3.4 still
etal.,1999),onemaywonderwhetheritispossibletofind showshowtheestimatedhiddenvariablesrelatetotheun-
nonlinearcomponentsasfunctionsofX thatareindepen- derlyingcausalvariablesinaspecific,nontrivialway. Two
dent in each domain, as produced by recent methods for examplesareprovidedbelow.
nonlinearICAwithsurrogates(Hyva¨rinenetal.,2019). As
Example1. FirstconsidertheMarkovnetworkM corre-
Z
acorollaryoftheabovetheorem,weshowthattheanswer
spondingtotheDAGG overZ inFigure1. ByTheorem
Z i
isno–theredonotexistnonlinearcomponentsthatareinde-
3.4andsuitablepermutationofestimatedhiddenvariables
pendentacrossdomains. Zˆ,wehave: (a)Zˆ isafunctionofZ andpossiblyZ ,
π(1) 1 2
Corollary3.3(ImpossibilityofFindingIndependentCom- (b)Zˆ isafunctionofZ ,(c)Zˆ isafunctionofZ
π(2) 2 π(3) 3
ponents). Let the observations be sampled from the data andpossiblyZ ,Z ,(d)Zˆ isafunctionofZ ,and(d)
generatingprocessinEq. (1),andM betheMarkovnet- 2 4 π(4) 4
Z Zˆ isafunctionofZ andpossiblyZ . Inthisexample,
workoverZ. SupposethatAssumptionsA1andA2from π(5) 5 4
the hidden causal variables Z and Z can be recovered
Theorem1hold,andthatM isnotanemptygraph. Sup- 2 4
Z
posealsothatwelearn(gˆ,fˆ,p )withthecomponentsofZˆ uptocomponent-wisetransformation,whilevariablesZ 1,
Zˆ
Z , and Z can be identified up to mixtures with certain
beingindependentineachdomain. Then,(gˆ,fˆ,p )cannot 3 5
Zˆ neighborsintheMarkovnetwork.
achieveEq. (2).
Example 2. One may think that generally speaking, the
ApartfromrecoveringthetrueMarkovnetworkM ,we morecomplexG ,themoreindeterminacieswehaveinthe
Z Z
show that the sparsity constraint on the Markov network estimatedlatentvariables(inthesensethateachestimated
structureover Zˆ alsoallowsustorecovertheunderlying latentvariablereceivescontributionsfrommorelatentvari-
hiddencausalvariablesZ uptospecific,relativelyminor ables). Infact,thismaynotbethecase. Inexample2,the
indeterminacies. In the result, the following variable set, underlyinglatentcausalgraphG isgiveninFigure2(a),
Z
4CausalRepresentationLearningfromMultipleDistributions:AGeneralSetting
Z Z Z Z both of them are strictly weaker than the faithfulness as-
5 6 5 6
sumption,sincethecombinationofAdjacency-faithfulness
andOrientation-faithfulnessisweakerthanthefaithfulness
Z Z Z Z Z Z Z Z
1 2 3 4 1 2 3 4
assumption(Zhang&Spirtes,2008).
(a)G ,theDAGovertruelatent (b)ThecorrespondingMarkov Interestingly, not only they are weaker variants of
Z
variablesZ . networkM . faithfulness, but we also prove that they are actually
i Z
necessaryandsufficientconditions, thus the weakest pos-
Figure2: Illustrativeexample2. sibleones,tobridgeconditionalindependencerelationsand
causalstructures. Specifically,weshowthattherecovered
whichinvolvesmorevariablesandmoreedgesandwhose
Markovnetworkisexactlythemoralizedgraphofthetrue
MarkovnetworkisshowninFigure2(b). Interestingly,for
causalDAGifandonlyiftheproposedvariantsoffaithful-
allvariablesZ , Ψ = Φ. Asaconsequence, allZ are
i Zi i ness hold. The proofs of Lemma 1 and Theorem 3.5 are
recovereduptoonlycomponent-wisetransformations.
showninAppx. E.
Permutationofestimatedlatentvariables. Theorems
Lemma1. GivenalatentcausalgraphG anddistribution
3.2 and 3.4 involve certain permutation of the estimated Z
hiddenvariablesZˆ. Suchanindeterminacyiscommonin P Z withitsMarkovNetworkM Z,underMarkovassump-
tion,theundirectedgraphdefinedbyM isasubgraphof
theliteratureofcausaldiscoveryandrepresentationlearn- Z
themoralizedgraphofthetruecausalDAGG.
ingtasksinvolvinglatentvariables. Inourcase,sincethe
functionh:=gˆ−1◦gwhereZˆ =h(Z)isinvertible,there
existsapermutationofthelatentvariablessuchthatthecor-
Theorem3.5. GivenacausalDAGG anddistributionP
respondingJacobianmatrixJ hasnonzerodiagonalentries Z Z
h withitsMarkovNetworkM ,underMarkovassumption,
(see Lemma 2 in Appx. B); such a permutation is what Z
theundirectedgraphdefinedbyM isthemoralizedgraph
Theorems3.2and3.4referto. Z
ofthetruecausalDAGG ifandonlyiftheSAFandSUCF
Z
assumptionsaresatisfied.
3.2.FromLatentMarkovNetworktoLatentCausal
DAG
Itisworthnotingthattheconnectionbetweenconditional
Now we have identified the Markov network up to an
independencerelationsandcausalstructureshasbeende-
isomorphism, which characterizes conditional indepen-
veloped by (Loh & Bu¨hlmann, 2014; Ng et al., 2021) in
dencerelationsinthedistribution. Tobuildtheconnection
thelinearcasebyleveragingthepropertiesoftheinverse
between Markov network or conditional independence
covariancematrix;ourresultsherefocusonthenonparamet-
relations and causal structures, prior theory relies on
riccaseandthusbeingabletoservetheconsideredgeneral
the Markov and faithfulness assumptions. However, in
settingsforidentifiability. Alsonotethatthenecessaryand
real-worldscenarios,thefaithfulnessassumptioncouldbe
sufficient assumptions may also be of independent inter-
violatedduetovariousreasonsincludingpathcancellations
estforothercausaldiscoverytasksexploringconditional
(Zhang&Spirtes,2008;Uhleretal.,2013).
independencerelationsinthenonparametriccase.
Sinceourgoalistogeneralizetheidentifiabilitytheoryas
muchaspossibletofitpracticalapplications,weintroduce
tworelaxationsofthefaithfulnessassumptions:
Discussiononadditionalassumptions. Weinvestigated
Assumption 1 (Single adjacency-faithfulness (SAF)). how the sparsity constraint on the recovered graph over
Given a DAG G and distribution P over the variable latentvariablesandsufficientchangeconditionsoncausal
Z Z
setZ,iftwovariablesZ andZ areadjacentinG ,then influencescanbeusedtorecoverthelatentvariablesand
i j Z
Z ⊥̸⊥Z |Z . causal graph up to certain indeterminacies. We may be
i j [n]\{i,k}
Assumption 2 (Single unshielded-collider-faithfulness abletoleveragepossibleparametricconstraintsonthedata
(SUCF) (Ng et al., 2021)). Given a latent causal graph generatingprocess(orotherwaystoconstraintheinvolved
G anddistributionP overthevariablesetZ,letZ → functions) or specific types of interventions (Varici et al.,
Z Z i
Z ← Z be any unshielded collider in G , then Z ⊥̸⊥ 2023; Ahuja et al., 2023; Buchholz et al., 2022; Squires
j k Z i
Z |Z . et al., 2023; Brehmer et al., 2022; von Ku¨gelgen et al.,
k [n]\{i,k}
2023;Brehmeretal.,2022;vonKu¨gelgenetal.,2023). For
We propose SAF as a relaxtion of the Adjacency- instance,ifweknowthatthechangeshappentothelinear
faithfulness(Ramseyetal.,2012). TheSUCFassumptionis causal mechanisms with Gaussian noises, this constraint
firstintroducedbyNgetal.(2021),whichisstrictlyweaker canreadilyhelpreducethesearchspaceandimprovethe
thanOrientation-faithfulness(Ramseyetal.,2012). Thus, identifiability.
5CausalRepresentationLearningfromMultipleDistributions:AGeneralSetting
4.ChangeEncodingNetworkfor thedomainlabelutogenerateparametersofnormalizing
RepresentationLearning flowandapplytheflowtransformationonZˆ toturnitinto
i
ϵˆ. Specifically,wehave
i
Thanks to the identifiability result, we now present two
different practical implementations to recover the latent ϵˆ i,logdet i =Flow(Zˆ i;NN({Aˆ i,jZˆ j}i j− =1 1,u)), (7)
variablesandtheircausalrelationsfromobservationsfrom
wherelogdet isthelogdeterminantoftheconditionalflow
multipledomains. Webuildourmethodonthevariational transformatioi nonZˆ .
autoencoder(VAE)frameworkandcanbeeasilyextended i
toothermodels,suchasnormalizingflows. Tocomputethepriordistribution,wemakeanassumption
onthenoisetermϵthatitfollowsanindependentpriordis-
Welearnadeeplatentgenerativemodel(decoder)p(X|Z)
tribution p(ϵ), such as a standard isotropic Gaussian or a
andavariationalapproximation(encoder)q(Z|X,θ)ofits
Laplacian. Then according to the change of variable for-
trueposteriorp(Z|X,θ)sincethetrueposteriorisusually
mula,thepriordistributionofthedependentlatentscanbe
intractable. To learn the model, we minimize the lower
writtenas
boundofthelog-likelihoodas
n
(cid:90)
logp(Zˆ|u)=(cid:88)
(logp(ϵˆ)+logdet ). (8)
i i
logp(X|u,θ)=log p(X|Z,u)p(Z|u)dZ (6)
i=1
(cid:90) q(Z|X,u)(cid:90) Intuitively, to minimize the KL divergence loss between
=log p(X|Z,u,θ)p(Z|u)dZ
q(Z|X,u) p(Z|u)andq(Z|X,u),thenetworkhastolearnthecorrect
structureandtheunderlyinglatentvariables;otherwise,it
≥−KL(q(Z|X,u)||p(Z|u))+E [logp(X|Z)]
q canbedifficulttotransformthedependentlatentvariables
=−L
ELBO
Zˆtoafactorizedpriordistribution,e.g.,N(0,I).
Fortheposteriorq(Z|X,u),weassumethatitisamultivari-
4.2.ParametricImplementationofthePrior
ateGaussianoraLaplaciandistribution, wherethemean
Distribution
andvariancearegeneratedbytheneuralnetworkencoder.
Asforq(X|Z),weassumethatitisamultivariateGaussian We can make parametric assumption on the latent causal
andthemeanistheoutputofthedecoderandthevariance processandfacilitatethelearningoftruecausalstructure
isapre-definedvalue(weuse0.01). and components. Here, we consider the linear SEM and
morecomplexSEMscanbegeneralized. Specifically,we
In practice, we can parameterize p(X|Z) as the decoder
assumethatthetruegenerationprocessofthelatentZ is
which takes as input the latent representation Z and
linearandonlyconsistsofscalingandshiftingmechanisms:
q(Z|X,u)asanencoderwhichoutputsthemeanandscale
oftheposteriordistribution. Anessentialdifferencefrom Z =A(C(u)Z)+S(u)ϵ+B(u), (9)
VAE(Kingma&Welling,2013)andiVAE(Khemakhem
etal.,2020a)isthatourmethodallowthecomponentsofZ
whereA∈[0,1]n×nisacausaladjacencymatrixwhichcan
tobecausallydependentandweareabletolearnthecom- bepermutedtobestrictlylower-triangular,C(u) ∈ Rn×n
ponentsandcausalrelationships. Andthekeyistheprior andS(u) ∈ Rn×1 areunderlyingdomain-specificscaling
distributionP(Z|u). Nowwepresenttwodifferentimple- matrixandvectorfordomainu,respectively,B(u) ∈Rn×1
mentationstocapturethechangeswithaproperlydefined istheunderlyingdomain-specificshiftvector,andϵisthe
priordistribution. independentnoise.
To estimate the latent variables Z, the causal structure
4.1.NonparametricImplementationofthePrior
A,andcapturethechangesacrossdomains,weintroduce
Distribution the learnable scaling Cˆ ∈ Rn×n,Sˆ ∈ Rn×1and bias pa-
TorecovertherelationshipsandlatentvariablesZ,webuild rameters Bˆ ∈ Rn×1 and pre-define a causal ordering as
Zˆ ,Zˆ ,...,Zˆ . Thenwehavethematrixformas
thenormalizingflowtomimictheinverseofthelatentSEM 1 2 n
Z i = f i(PA(Z i),ϵ i)inEq. (1). Wefirstassumeacausal ϵˆ=(Zˆ−Bˆ(u)−AˆCˆ(u)Zˆ)/Sˆ(u). (10)
orderingasZˆ ,...,Zˆ . Then,foreachcomponentZˆ ,we
1 n i
considerthepreviouscomponents{Zˆ ,...,Zˆ }aspoten- Given a prior distribution of the noise term p(ϵˆ), and ac-
1 i−1
tialparentsofZˆ andwecanselectthetrueparentswiththe cording to the change of variable rule, we have the prior
i
adjacencymatrixAˆ,whereAˆ denotesthatcomponentZˆ distributionforZˆinparametriccaseas
i,j j
contributesinthegenerationofZˆ . IfAˆ = 0,itmeans
i i,j n
thatZˆ willnotcontributetothegenerationofZˆ . Then, logp(Zˆ|u)=(cid:88) (logp(ϵˆ)−log|Sˆ(u)|), (11)
j i i i
weusetheselectedparents{Aˆ i,1Zˆ 1,...,Aˆ i,i−1Zˆ i−1}and i=1
6CausalRepresentationLearningfromMultipleDistributions:AGeneralSetting
sincethedeterminantofthestrictlowertriangularmatrixCˆ under most cases, our model learns a strong one-to-one
is0. correspondencefromtheestimatedcomponentsandthetrue
components. Forinstance,thefirstcolumninFig. 3show
4.3.FullObjective thatZˆ 1isstronglycorrelatedwiththetruecomponentsZ 1
whileitisnearlyindependentfromthetrueZ .
2
After we have properly defined the needed distributions
p(X|Z),p(Z|X,u),p(Z|u),wecantrainourmodeltomin- FromtheestimatedAˆ, wefindthatourmethodisableto
imize the loss function L . However, without any recoverthetruecausalstruccture. Forinstance,ontheY
ELBO
furtherconstraint,thepowerfulnetworkmaychoosetouse structurewithZ 1 →Z 3 ←Z 2andZ 3 →Z 4,ourestimated
thefullyconnectedcausalgraphduringtraining. Inother modelonlykeepthecomponentsAˆ 1,3,Aˆ 2,3,Aˆ 3,4nonzero
words,alllower-triangularelementsoftheestimatedgraph with the proposed sparsity regularization. The estimated
Aˆ is non-zero, which implies that each component Zˆ is causalgraphisconsistentwiththetrueY-structurecausal
i
causedbyallpreviousi−1components. Toexcludesuch graph. Wecanalsoseethatthelatentcausalstructureisalso
unwantedsolutionsandencouragethemodeltolearnthe
recoveredfromFig.4and3.WeobservethatthelearnedZˆ
1
truecausalstructureamongcomponentsofZ,weapplythe isstronglycorrelatedwiththetrueZ 2 andisindependent
ℓ 1regularizationonAˆ,i.e., fromthetrueZ 1,butcorrelatedwiththeZˆ 3andZˆ 4. These
results aligns well with the true causal graph since Z is
2
L =∥Aˆ∥ . (12) independentfromZ whileisthecauseofZ andZ .
sparsity 1 1 3 4
Theexperimentssupportourtheoreticalresultthatthecom-
Itisworthnotingthatthesparsityregularizationtermabove
ponentsandstructureareidentifiableuptocertainindeter-
isanapproximationofthesparsityconstraintontheedges
minacies. AsfortheresultsinFig. 3,weobservethatour
of the estimated Markov network specified in Thms. 3.2
non-parametricmethodisstillabletorecoverthetruelatent
and3.4,sinceitisnotstraightforwardtoimposethelatter
variableswithLaplacenoise.
constraint in a differentiable end-to-end training process.
Finally,thefulltrainingobjectiveis
5.RelatedWork
L =L +L . (13)
full ELBO sparsity
Causalrepresentationlearningaimstounearthcausallatent
Afterthemodelconverges,theoutputoftheencoderZˆisour variables and their relations from observed data. Despite
recoveredlatentsfromtheobservationsinmultipledomains its significance, the identifiability of the hidden generat-
andtherevealedcausalstructureisinAˆwhichencapsulates ingprocessisknowntobeimpossiblewithoutadditional
thecausalrelationshipsacrossthecomponents. constraints,especiallywithonlyobservationaldata. Inthe
linear, non-Gaussian case, Silva et al. (2006) recover the
Markovequivalenceclass,providedthateachobservedvari-
4.4.Simulations
able has a unique latent causal parent; Xie et al. (2020);
Toverifyourtheoryandtheproposedimplementations,we Caietal.(2019)estimatethelatentvariablesandtheirrela-
runexperimentsonthesimulateddatabecausetheground tionsassumingatleasttwicemeasuredvariablesaslatent
truthcausaladjacencymatrixandthelatentvariablesacross ones, which has been further extended to learn the latent
domainsareavailableforsimulateddata. Consequently,we hierarchicalstructure(Xieetal.,2022). Moreover,Adams
considerfollowingcommoncausalstructures(i)Y-structure et al. (2021) provide theoretical results on the graphical
with4variables,Z 1 →Z 3 ←Z 2,Z 3 →Z 4 and(ii)chain conditionsforidentification. Inthelinear,Gaussiancase,
structure Z 1 → Z 2 → Z 3 → Z 4. The noises are modu- Huangetal.(2022)leveragerankdeficiencyoftheobserved
latedwithscalingrandomsampledfromUnif[0.5,2]and sub-covariance matrix to estimate the latent hierarchical
shifts are sampled from Unif[−2,2]. The scaling on the structure. Inthediscretecase,Kivvaetal.(2021)identify
Z are also randomly sampled from Unif[0.5,2]. In other thehidden causalgraphupto Markov equivalence byas-
words, the changes are modular. After generating Z, we sumingamixturemodelwheretheobservedchildrensets
feedthelatentvariablesintoMLPwithorthogonalweights ofanypairoflatentvariablesaredifferent.
andLeakyReLUactivationsforinvertibility. Specifically,
Giventhechallengeofidentifiabilityonpurelyobservational
wesampleorthogonalmatrixastheweightsoftheMLPlay-
data, a different line of research leverage experiments by
ers. SinceorthogonalmatrixandLeakyReLUareinvertible,
assumingtheaccessibilityofvarioustypesofinterventional
theMLPfunctionisalsoinvertible.
data. Basedonthesingle-nodeperfectintervention,Squires
We present the results in Fig. 3 and 4. Each sub-figure etal.(2023)leveragesingle-nodeinterventionsfortheiden-
consist of 4 × 4 panels and penal on i-th row and j − tifiabilityoflinearcausalmodelandlinearmixingfunction;
thcolumndenotestherelationshipbetweentheestimated (Varicietal.,2023)incorporatefornonlinearcausalmodel
component Zˆ with the true latent Z . We can see that
i j
7CausalRepresentationLearningfromMultipleDistributions:AGeneralSetting
Figure3: Recoveredlatentvariablesv.s. thetruelatentvariableswithNon-ParametricApproach. (a)Y-structurewith
Laplace noise. (b) Y-structure with Gaussian noise. (c) Chain structure with Laplace noise. (d) Chain structure with
Gaussiannoise. Ineachsub-figure,i-throwandj-thcolumndepcitstherelationshipbetweentheestimatedZˆ andthetrue
i
componentsZ .
j
Figure4: Recoveredlatentvariablesv.s. thetruelatentvariableswithLinearParameterizationApproach. TheX-axis
denotesthecomponentsoftruelatentvariablesZ andtheY-axisrepresentthecomponentsofestimatedlatentvariables
Zˆ. (a)Y-structurewithLaplacenoise. (b)Y-structurewithGaussiannoise. (c)ChainstructurewithLaplacenoise. (d)
ChainstructurewithGaussiannoise.
andlinearmixingfunction;(Varicietal.,2023;Buchholz vationaldata(e.g.,multi-domaindata,nonstationarytime
etal.,2023;Jiang&Aragam,2023)providetheidentifia- series)andavoidadditionalexperimentsforinterventions.
bilityofthenonparametriccausalmodelandlinearmixing
function;(Ahujaetal.,2023)furthergeneralizetheresultto
6.ConclusionandDiscussions
nonparametriccausalmodelandpolynomialmixingfunc-
tionswithadditionalconstraintsonthelatentsupport;and We establish a set of new identifiability results to reveal
(Brehmeretal.,2022;vonKu¨gelgenetal.,2023;Jiang& hiddencausalvariablesandlatentstructuresinthegeneral
Aragam,2023)explorethenonparametricsettingsforboth nonparametric settings. Specifically, with sparsity regu-
the causal model and mixing function. In addition to the larization during estimation and sufficient changes in the
single-nodeperfectinterventions,Brehmeretal.(2022)in- causalinfluences,wedemonstratethattherevealedhidden
troducedcounterfactualpre-andpost-interventionviews; variablesandstructuresarerelatedtotheunderlyingcausal
vonKu¨gelgenetal.(2023)assumetwodistinct,pairedinter- model in a specific, nontrivial way. In contrast to recent
ventionspernodeformultivariatecausalmodels;andJiang worksontherecoveryofhiddencausalvariablesandstruc-
&Aragam(2023)placesspecificstructuralrestrictionson tures,ourresultsrelyonpurelyobservationaldatawithout
thelatentcausalgraph. graphicalorparametricconstraints. Ourresultsofferinsight
intounveilingthelatentcausalprocessinoneofthemost
Ourstudyliesinthelineofleveragingonlyobservational
universalsettings.Experimentsinvarioussettingshavebeen
data,andprovidesidentifiabilityresultsinthegeneralnon-
conductedtovalidatethetheory. Asfuturework,wewillex-
parametricsettingsonboththelatentcausalmodelandmix-
plorethescenariowhereonlyasubsetofthecausalrelations
ingfunction. Unlikepriorworkswithobservationaldata,
change,whichcouldbeachallengeaswellasachance,and
wedonothaveanyparametricassumptionsorgraphicalre-
showuptowhatextenttheunderlyingcausalvariablescan
strictions;Comparedtothoserelyingoninterventionaldata,
berecoveredwithpotentiallyweakerassumptions.
ourresultsnaturallybenefitfromtheheterogeneityofobser-
8CausalRepresentationLearningfromMultipleDistributions:AGeneralSetting
Acknowledgements Hyva¨rinen,A.andMorioka,H. Unsupervisedfeatureex-
ThisprojectispartiallysupportedbyNSFGrant2229881, tractionbytime-contrastivelearningandnonlinearICA.
the National Institutes of Health (NIH) under Contract AdvancesinNeuralInformationProcessingSystems,29:
R01HL159805,andgrantsfromAppleInc.,KDDIResearch 3765–3773,2016.
Inc.,QurisAI,andInfiniteBrainTechnology.
Hyva¨rinen, A. and Morioka, H. Nonlinear ICA of tem-
porally dependent stationary sources. In International
References Conference on Artificial Intelligence and Statistics, pp.
460–469.PMLR,2017.
Adams, J., Hansen, N., and Zhang, K. Identification of
partiallyobservedlinearcausalmodels: Graphicalcon- Hyva¨rinen,A.andPajunen,P. Nonlinearindependentcom-
ditions for the non-gaussian and heterogeneous cases. ponentanalysis:Existenceanduniquenessresults.Neural
AdvancesinNeuralInformationProcessingSystems,34: networks,12(3):429–439,1999.
22822–22833,2021.
Hyva¨rinen,A.,Cristescu,R.,andOja,E. Afastalgorithm
Ahuja, K., Mahajan, D., Wang, Y., and Bengio, Y. Inter- for estimating overcomplete ICA bases for image win-
ventionalcausalrepresentationlearning. InInternational dows. InProc.Int.JointConf.onNeuralNetworks,pp.
ConferenceonMachineLearning,pp.372–407.PMLR, 894–899,Washington,D.C.,1999.
2023.
Hyva¨rinen,A.,Sasaki,H.,andTurner,R. NonlinearICAus-
ingauxiliaryvariablesandgeneralizedcontrastivelearn-
Brehmer,J.,DeHaan,P.,Lippe,P.,andCohen,T.S.Weakly
supervised causal representation learning. Advances ing. InInternationalConferenceonArtificialIntelligence
in Neural Information Processing Systems, 35:38319–
andStatistics,pp.859–868.PMLR,2019.
38331,2022.
Hyva¨rinen, A., Khemakhem, I., and Morioka, H. Non-
linear independent component analysis for principled
Buchholz, S., Besserve, M., and Scho¨lkopf, B. Function
disentanglement in unsupervised deep learning.
classesforidentifiablenonlinearindependentcomponent
Patterns, 4(10):100844, 2023. ISSN 2666-3899.
analysis. arXivpreprintarXiv:2208.06406,2022.
doi: https://doi.org/10.1016/j.patter.2023.100844.
URL https://www.sciencedirect.com/
Buchholz, S., Rajendran, G., Rosenfeld, E., Aragam, B.,
science/article/pii/S2666389923002234.
Scho¨lkopf,B.,andRavikumar,P. Learninglinearcausal
representationsfrominterventionsundergeneralnonlin- Jiang, Y. and Aragam, B. Learning nonparametric latent
earmixing. arXivpreprintarXiv:2306.02235,2023. causalgraphswithunknowninterventions.arXivpreprint
arXiv:2306.02899,2023.
Cai,R.,Xie,F.,Glymour,C.,Hao,Z.,andZhang,K. Triad
constraintsforlearningcausalstructureoflatentvariables. Khemakhem,I.,Kingma,D.,Monti,R.,andHyva¨rinen,A.
Advancesinneuralinformationprocessingsystems,32, VariationalautoencodersandnonlinearICA:Aunifying
2019. framework. In International Conference on Artificial
IntelligenceandStatistics,pp.2207–2217.PMLR,2020a.
Comon,P. Independentcomponentanalysis–anewcon-
cept? SignalProcessing,36:287–314,1994. Khemakhem, I., Monti, R., Kingma, D., and Hyvarinen,
A. Ice-beem: Identifiableconditionalenergy-baseddeep
Ha¨lva¨,H.andHyva¨rinen,A.HiddenmarkovnonlinearICA: modelsbasedonnonlinearica. AdvancesinNeuralInfor-
Unsupervisedlearningfromnonstationarytimeseries. In mationProcessingSystems,33:12768–12778,2020b.
ConferenceonUncertaintyinArtificialIntelligence,pp.
Kingma,D.P.andWelling,M. Auto-encodingvariational
939–948.PMLR,2020.
bayes. arXivpreprintarXiv:1312.6114,2013.
Ha¨lva¨,H.,LeCorff,S.,Lehe´ricy,L.,So,J.,Zhu,Y.,Gassiat,
Kivva, B., Rajendran, G., Ravikumar, P., andAragam, B.
E.,andHyva¨rinen,A. Disentanglingidentifiablefeatures Learning latent causal graphs via mixture oracles. Ad-
fromnoisydatawithstructurednonlinearICA. Advances vances in Neural Information Processing Systems, 34:
inNeuralInformationProcessingSystems,34,2021.
18087–18101,2021.
Huang,B.,Low,C.J.H.,Xie,F.,Glymour,C.,andZhang, Kong,L.,Xie,S.,Yao,W.,Zheng,Y.,Chen,G.,Stojanov,
K. Latent hierarchical causal structure discovery with P.,Akinwande,V.,andZhang,K.Partialdisentanglement
rankconstraints. AdvancesinNeuralInformationPro- fordomainadaptation. InInternationalConferenceon
cessingSystems,35:5549–5561,2022. MachineLearning,pp.11455–11472.PMLR,2022.
9CausalRepresentationLearningfromMultipleDistributions:AGeneralSetting
Lachapelle,S.,Lo´pez,P.R.,Sharma,Y.,Everett,K.,Priol, von Ku¨gelgen, J., Besserve, M., Liang, W., Gresele, L.,
R.L.,Lacoste,A.,andLacoste-Julien,S. Disentangle- Kekic´,A.,Bareinboim,E.,Blei,D.M.,andScho¨lkopf,
mentviamechanismsparsityregularization: Anewprin- B. Nonparametric identifiability of causal represen-
ciplefornonlinearICA. ConferenceonCausalLearning tations from unknown interventions. arXiv preprint
andReasoning,2022. arXiv:2306.00542,2023.
Lin,J. Factorizingmultivariatefunctionclasses. Advances Xie, F., Cai, R., Huang, B., Glymour, C., Hao, Z., and
inneuralinformationprocessingsystems,10,1997. Zhang,K. Generalizedindependentnoiseconditionfor
estimatinglatentvariablecausalgraphs. InAdvancesin
Loh,P.-L.andBu¨hlmann,P. High-dimensionallearningof NeuralInformationProcessingSystems,2020.
linearcausalnetworksviainversecovarianceestimation.
TheJournalofMachineLearningResearch,15(1):3065– Xie,F.,Huang,B.,Chen,Z.,He,Y.,Geng,Z.,andZhang,
3105,2014. K. Identificationoflinearnon-gaussianlatenthierarchi-
calstructure. InInternationalConferenceonMachine
Ng,I.,Zheng,Y.,Zhang,J.,andZhang,K. Reliablecausal Learning,pp.24370–24387.PMLR,2022.
discoverywithimprovedexactsearchandweakerassump-
tions. In Advances in Neural Information Processing Xie,S.,Zhang,Z.,Lin,Z.,Hinz,T.,andZhang,K. Smart-
Systems,2021. Brush: Text and shape guided object inpainting with
diffusionmodel. InProceedingsoftheIEEE/CVFCon-
Pearl, J. Causality: Models, Reasoning, and Inference. ferenceonComputerVisionandPatternRecognition,pp.
CambridgeUniversityPress,Cambridge,2000. 620–629,2023.
Ramsey, J., Zhang, J., and Spirtes, P. L. Adjacency- Yao,W.,Sun,Y.,Ho,A.,Sun,C.,andZhang,K. Learning
faithfulness and conservative causal inference. arXiv temporallycausallatentprocessesfromgeneraltemporal
preprintarXiv:1206.6843,2012. data. arXivpreprintarXiv:2110.05428,2021.
Scho¨lkopf, B., Locatello, F., Bauer, S., Ke, N. R., Kalch- Yao,W.,Chen,G.,andZhang,K. Temporallydisentangled
brenner,N.,Goyal,A.,andBengio,Y. Towardscausal representationlearning. InAdvancesinNeuralInforma-
representationlearning. ProceedingsoftheIEEE,109(5): tionProcessingSystems,2022.
612–634,2021.
Zhang, J.andSpirtes, P. Detectionofunfaithfulnessand
Silva,R.,Scheines,R.,Glymour,C.,andSpirtes,P. Learn- robustcausalinference. MindsandMachines,18:239–
ingthestructureoflinearlatentvariablemodels. Journal 271,2008.
ofMachineLearningResearch,7:191–246,2006.
Zheng, Y., Ng, I., and Zhang, K. On the identifiability
Sorrenson,P.,Rother,C.,andKo¨the,U. Disentanglement ofnonlinearICA:Sparsityandbeyond. InAdvancesin
bynonlinearICAwithgeneralincompressible-flownet- NeuralInformationProcessingSystems,2022.
works(GIN). arXivpreprintarXiv:2001.04872,2020.
Spirtes,P.,Glymour,C.,andScheines,R. Causation,Pre-
diction, andSearch. MITPress, Cambridge, MA,2nd
edition,2001.
Squires,C.,Seigal,A.,Bhate,S.S.,andUhler,C. Linear
causaldisentanglementviainterventions.InInternational
ConferenceonMachineLearning,2023.
Taleb,A.andJutten,C. Sourceseparationinpost-nonlinear
mixtures. IEEETransactionsonsignalProcessing, 47
(10):2807–2820,1999.
Uhler,C.,Raskutti,G.,Bu¨hlmann,P.,andYu,B. Geometry
ofthefaithfulnessassumptionincausalinference. The
AnnalsofStatistics,pp.436–463,2013.
Varici,B.,Acarturk,E.,Shanmugam,K.,Kumar,A.,and
Tajer,A. Score-basedcausalrepresentationlearningwith
interventions. arXivpreprintarXiv:2301.08230,2023.
10CausalRepresentationLearningfromMultipleDistributions:AGeneralSetting
A.ProofofTheorem3.1
Theorem3.1. LettheobservationsbesampledfromthedatageneratingprocessinEq. (1),andM betheMarkovnetwork
Z
overZ. Supposethatthefollowingassumptionshold:
• A1(Smoothandpositivedensity): Theprobabilitydensityfunctionoflatentcausalvariablesissmoothandpositive,i.e.
p issmoothandp >0overRn.
Z Z
• A2(Sufficientchanges): ForanyZ ∈Rn,thereexist2n+|M |+1valuesofθ,i.e.,θ(u)withu=0,...,2n+|M |,
Z Z
suchthatthevectorsw(Z,u)−w(z,0)withu=1,...,2n+|M |arelinearlyindependent,wherevectorw(Z,u)
Z
isdefinedas
(cid:18) ∂logp(Z;θ(u)) ∂logp(Z;θ(u))
w(Z,u)= ,..., ,
∂Z ∂Z
1 n
∂2logp(Z;θ(u)) ∂2logp(Z;θ(u))(cid:19)
,...,
∂Z2 ∂Z2
1 n
(cid:18) ∂2logp(Z;θ(u))(cid:19)
⊕ .
∂Z ∂Z
i j (i,j)∈E(MZ)
Supposethatwelearn(gˆ,fˆ,p )toachieveEq. (2). Then,foreverypairofestimatedhiddenvariablesZˆ andZˆ thatare
Zˆ k l
notadjacentintheMarkovnetworkM overZˆ,wehavethefollowingstatements:
Zˆ
(a) EachtruehiddencausalvariableZ isafunctionofatmostoneofZˆ andZˆ.
i k l
(b) ForeachpairoftruehiddencausalvariablesZ andZ thatareadjacentintheMarkovnetworkM overZ,atmost
i j Z
oneofthemisafunctionofZˆ orZˆ.
k l
Proof. SincehinZˆ =h(Z)isinvertible,wehave
p(Zˆ;θˆ)=p(Z;θ)/|J |,
h
logp(Zˆ;θˆ)=logp(Z;θ)−log|J |. (14)
h
SupposeZˆ andZˆ areconditionallyindependentgivenZˆ i.e.,theyarenotadjacentintheMarkovnetworkoverZˆ.
k l [n]\{k,l}
Foreachθ,byLin(1997),wehave
∂2logp(Zˆ;θˆ)
=0.
∂Zˆ ∂Zˆ
k l
Toseewhatitimplies,wefindthefirst-orderderivative:
∂logp(Zˆ;θ) =(cid:88)∂logp(Z;θ) ∂Z
i −
∂log|J q|
.
∂Zˆ
k i
∂Z
i
∂Zˆ
k
∂Zˆ
k
Letη(θ) = logp(Z;θ),η′(θ) = ∂logp(Z;θ),η′′(θ) = ∂2logp(Z;θ),h′ = ∂Zi,andh′′ = ∂2Zi . Wethenderivethe
i ∂Zi ij ∂Zi∂Zj i,l ∂Zˆ
l
i,kl ∂Zˆ k∂Zˆ
l
second-orderderivative:
0=(cid:88)(cid:88)∂2logp(Z;θ)∂Z
j
∂Z
i
+(cid:88)∂logp(Z;θ) ∂2Z
i −
∂2log|J q|
.
j i
∂Z i∂Z
j
∂Zˆ
l
∂Zˆ
k i
∂Z
i
∂Zˆ k∂Zˆ
l
∂Zˆ k∂Zˆ
l
=(cid:88)∂2logp(Z;θ)∂Z i ∂Z i +(cid:88) (cid:88) ∂2logp(Z;θ)∂Z j ∂Z i
i
∂Z i2 ∂Zˆ
l
∂Zˆ
k j (j,i)∈E(MZ)
∂Z i∂Z
j
∂Zˆ
l
∂Zˆ
k
+(cid:88)∂logp(Z;θ) ∂2Z
i −
∂2log|J q|
i
∂Z
i
∂Zˆ k∂Zˆ
l
∂Zˆ k∂Zˆ
l
=(cid:88)
η′′(θ)h′ h′
+(cid:88) (cid:88)
η′′(θ)h′ h′
+(cid:88)
η′(θ)h′′ −
∂2log|J q|
. (15)
ii i,l i,k ij j,l i,k i i,kl ∂Zˆ ∂Zˆ
i j (j,i)∈E(MZ) i k l
11CausalRepresentationLearningfromMultipleDistributions:AGeneralSetting
HerewedenotebyE(M )thesetofedgesintheMarkovnetworkoverZ andwealreadymakeuseofthefactthatifZ
Z i
andZ arenotadjacentintheMarkovnetwork,then
∂2logp(Z;θ)
=0.
j ∂Zi∂Zj
ByAssumptionA2,considerthe2n+|M |+1valuesofθ,i.e.,θ(u)withu=0,...,2n+|M |,suchthatEq. (15)hold.
Z Z
Then,wehave2n+|M |+1suchequations. Subtractingeachequationcorrespondingtoθ(u),u=1,...,2n+|M |
Z Z
withtheequationcorrespondingtoθ(0)resulsin2n+|M |equations:
Z
(cid:88) (cid:88) (cid:88)
0= (η′′(θ(u))−η′′(θ(0)))h′ h′ + (η′′(θ(u))−η′′(θ(0)))h′ h′
ii ii i,l i,k ij ij j,l i,k
i j (j,i)∈E(MZ)
(cid:88)
+ (η′(θ(u))−η′(θ(0)))h′′ ,
i i i,kl
i
whereu = 1,...,2n+|M |. ByAssumptionA2,thevectorsformedbycollectingthecorrespondingcoefficientsare
Z
linearlyindependent. Therefore,foranyiandanyj suchthat(j,i)∈E(M ),wehave
Z
h′ h′ =0, (16)
i,l i,k
h′ h′ =0, (17)
j,l i,k
h′′ =0. (18)
i,kl
Eq. (16)indicatesthatZ isafunctionofatmostoneofZˆ andZˆ,whileEq. (17)impliesthatgiventhatZ andZ are
i k l i j
adjacentinMarkovnetworkM ,atmostoneofthemisafunctionofZˆ orZˆ.
Z k l
B.ProofofTheorem3.2
First,weintroducethefollowinglemma,whichwillbeusedintheproof.
Lemma2. ForanyinvertiblematrixA,thereexistsapermutationofitsrowsuchthatthediagonalentriesoftheresulting
matrixarenonzero.
Proof. Supposebycontradictionthatthereexistsatleastazerodiagonalentryforeveryrowpermutation. ByLeibniz
formula,wehave
(cid:32) n (cid:33)
(cid:88) (cid:89)
det(A)= sgn(σ) a ,
σ(i),i
σ∈Sn i=1
whereS denotesthesetofn-permutations. Sincethereexistsazerodiagonalentryforeverypermutation,wehave
n
n
(cid:89)
a =0, ∀σ ∈S ,
σ(i),i n
i=1
which implies det(A) = 0 and that matrix A is not invertible. This is a contradiciton with the assumption that A is
invertible.
Theorem3.2(IdentifiabilityofLatentMarkovNetwork). Lettheobservationsbesampledfromthedatageneratingprocess
inEq. (1),andM betheMarkovnetworkoverZ. SupposethatAssumptionsA1andA2fromTheorem1hold. Suppose
Z
alsothatwelearn(gˆ,fˆ,p )toachieveEq. (2)withtheminimalnumberofedgesofMarkovnetworkM overZˆ. Then,
Zˆ Zˆ
theMarkovnetworkM overestimatedhiddenvariablesZˆisisomorphictothetruelatentMarkovnetworkM .
Zˆ Z
Proof. BasedonLemma2,thereexistsapermutationπoftheestimatedhiddenvariables,denotedasZˆ ,suchthat
π
∂Z
i ̸=0, i=1,...,n. (19)
∂Zˆ
π(i)
Suppose that Z and Z are adjacent in the Markov network M over Z, but Z˜ and Z˜ are not adjacent in the
i j Z π(i) π(i)
MarkovnetworkM overZˆ. ByTheorem3.1,atmostoneofZ andZ isafunctionofZ˜ andZ˜ . Thisisclearlya
Zˆ i j π(i) π(i)
contradictionwithEq. (19).
12CausalRepresentationLearningfromMultipleDistributions:AGeneralSetting
Therefore,wehaveshownbycontradictionthat,ifZ andZ areadjacentintheMarkovnetworkM overZ,thenZ˜
i j Z π(i)
andZ˜ areadjacentintheMarkovnetworkM overvariablesZˆ . Thatis,alledgesinM mustbepresentinMarkov
π(i) Zˆ
π
π Z
networkM overvariablesZˆ . Also,notethatthetruemodel(g,f,p )isclearlyoneofthesolutionsthatachievesEq.
Zˆ
π
π Z
(2). Thus,undersparsityconstraintontheedgesofM ,weconcludethattheMarkovnetworkM overZˆ mustbe
Zˆ Zˆ
π
π
identicaltotheMarkovnetworkM overZ,
Z
C.ProofofCorollary3.3
Supposebycontradictionthat(gˆ,fˆ,p )achievesEq. (2). Byassumption,thecomponentsofZˆareindependentineach
Zˆ
domain,indicatingthattheMarkovnetworkM isanemptygraph. BysimilarreasoningintheproofofTheorem3.2,all
Zˆ
edgesinthetrueMarkovnetworkM mustbepresentinM (uptoisomorphism). SinceM isanemptygraph,this
Z Zˆ Zˆ
impliesthatM isalsoanemptygraph,whichiscontradictorywiththeassumptionthatM isnotanemptygraph.
Z Z
D.ProofofTheorem3.4
We first state the following lemma that is used to prove Statement (b) of Theorem 3.4. The proof is a straightforward
consequenceofCayley–Hamiltontheoremandisomittedhere.
Lemma3. LetAbeann×ninvertiblematrix. Then,itcanbeexpressedasalinearcombinationofthepowersofA,i.e.,
n−1
(cid:88)
A−1 = c Ak
k
k=0
forsomeappropriatechoiceofcoefficientsc ,c ,...,c .
0 1 n−1
NowconsidertheMarkovnetworkM overhiddencausalvariablesZ. LetN bethesetofneighborsofZ inM . We
Z Zi i Z
providethefollowingresultthatrelatesamatrixtoitsinverse,giventhatsuchmatrixsatisfiescertainpropertydefinedby
M .
Z
Proposition1. ConsideraMarkovnetworkM overZ. LetN bethesetofneighborsofZ inM ,andAbeann×n
Z Zi i Z
invertiblematrix. Foreachi̸=j whereZ isnotadjacenttosomenodesin{Z }∪N ,supposeA =0. Then,wehave
j i Zi ij
A−1 =0.
ij
Proof. ByLemma3,A−1canbeexpressedaslinearcombinationofthepowersofA. Therefore,itsufficestoprovethat,
eachmatrixpowerAk satisfiesthefollowingproperty: Ak =0foreachi̸=j whereZ isnotadjacenttosomenodesin
ij j
{Z }∪N . Weproceedwithmathematicalinductiononk. Bydefinition,thepropertyholdsinthebasecasewherek =1.
i Zi
NowsupposethatthepropertyholdsforAk. WeprovebycontradictionthatthepropertyholdsforAk+1. Supposeby
contradictionthatAk+1 ̸=0forsomei̸=j whereZ isnotadjacenttosomenodesin{Z }∪N . Thisimpliesthatone
ij j i Zi
ofthefollowingcasesholds:
• Case(a): Z isnotadjacenttoZ inM .
j i GZ
• Case(b): ThereexistsZ ∈N \{Z }suchthatZ andZ arenotadjacentinM .
l Zi j j l GZ
Since Ak+1 = (cid:80)n AkA , the assumption Ak+1 ̸= 0 implies that there must exist m such that Ak A ̸= 0, i.e.,
ij r=0 ir rj ij im mj
Ak ̸=0andA ̸=0. SincebothAk andAsatisfytheproperty,thisindicates(i)Z isadjacenttoZ andallnodesin
im mj m i
N \{Z },and(ii)Z isadjacenttoZ andallnodesinN \{Z }. Weconsiderthefollowingcases:
Zi m j m Zm j
• Caseofm=l: By(ii),Z isadjacenttoZ ,whichcontradictsCase(b)above. Also,weknowthatZ isadjacenttoZ
j l l i
by(i),whichindicatesthatZ isadjacenttoZ ,contradictingCase(a)above.
i j
• Caseofm̸=l: By(i)and(ii),Z isadjacenttoZ andZ isadjacenttoZ ,implyingthatZ andZ areadjacent,
m i j m i j
whichiscontradictorywithCase(a)above. Furthermore,sinceZ isaneighborofZ ,weknowthatZ andZ are
l i m l
adjacentby(i). Also,by(ii),Z isadjacenttoZ ,whichcontradictsCase(b)above.
j l
13CausalRepresentationLearningfromMultipleDistributions:AGeneralSetting
Ineitherofthecasesabove,thereisacontradiction.
Wearenowreadytogivethefollowingresult.
Theorem3.4(IdentifiabilityofHiddenCausalVariables). Lettheobservationsbesampledfromthedatageneratingprocess
inEq. (1),andM betheMarkovnetworkoverZ. LetN bethesetofneighborsofvariableZ inM . Supposethat
Z Zi i Z
AssumptionsA1andA2fromTheorem1hold. Supposealsothatwelearn(gˆ,fˆ,p )toachieveEq. (2)withtheminimal
Zˆ
numberofedgesofMarkovnetworkM overZˆ. Then,thereexistsapermutationπ oftheestimatedhiddenvariables,
Zˆ
denotedasZˆ ,suchthateachZˆ isafunctionof(asubsetof)thevariablesin{Z }∪Ψ .
π π(i) i Zi
Proof. Wefirstproveasimplercase:thereexistsapermutationπoftheestimatedhiddenvariables,denotedasZˆ ,suchthat
π
Z isafunctionofZˆ andasubsetofthevariablesin{Zˆ |Z isadjacenttoZ andallotherneighborsofZ inM }.
i π(i) π(j) j i i Z
ByTheorem3.2anditsproof,thereexistsapermutationπoftheestimatedvariables,denotedasZˆ ,suchthattheMarkov
π
networkM overZˆ isidenticaltoM ,andthat
Zˆ
π
π Z
∂Z
i ̸=0, i=1,...,n.
∂Zˆ
π(i)
Clearly,eachvariableZ isafunctionofZˆ .
i π(i)
WefirstshowthatifZ isnotadjacenttoZ inM ,thenZ cannotbeafunctionofZˆ . SinceZ andZ arenotadjacent
j i Z i π(j) i j
inM ,weknowthatZˆ andZˆ arenotadjacentinM . ByTheorem3.1,Z isafunctionofatmostoneofZˆ
Z π(i) π(j) Zˆ
π
i π(i)
andZˆ ,whichimpliesthatZ cannotbeafunctionofZˆ ,becausewehaveshownthatZ isafunctionofZˆ .
π(j) i π(j) i π(i)
Torefinefurther,nowsupposethatZ isadjacenttoZ ,butnotadjacenttosomeZ ∈N \{Z }. SinceM andM
j i k Zi j Z Zˆ
π
areidentical,Zˆ isalsonotadjacenttoZˆ inM . SinceZ andZ areadjacentinM ,byTheorem3.1,atmost
π(j) π(k) Zˆ
π
i k Z
oneofthemisafunctionofZˆ orZˆ . ThisimpliesthatZ cannotbeafunctionofZˆ ,becausewehaveshownthat
π(j) π(k) i π(j)
Z isafunctionofZˆ .
k π(k)
Therefore,wehavejustshownthatZ isafunctionofatmostthevariablesin
i
Zˆ ∪{Zˆ |Z isadjacenttoZ andallotherneighborsofZ inM }.
π(i) π(j) j i i Z
Nowforeachk ̸=iwhereZ isnotadjacenttosomenodesin{Z }∪N ,wehave
i k Zk
∂Z
k =0.
∂Zˆ
π(i)
ByProposition1,wehave
(cid:18)
∂Z
(cid:19)−1
=0,
∂Zˆ
π ki
which,byinversefunctiontheorem,implies
∂Zˆ (cid:18)
∂Z
(cid:19)−1
π(i)
= =0.
∂Z
k
∂Zˆ
π ki
ThisimpliesthatZˆ cannotbeafunctionofZ .
π(i) k
E.ProofofLemma1andTheorem3.5
Lemma1. GivenalatentcausalgraphG anddistributionP withitsMarkovNetworkM ,underMarkovassumption,
Z Z Z
theundirectedgraphdefinedbyM isasubgraphofthemoralizedgraphofthetruecausalDAGG.
Z
14CausalRepresentationLearningfromMultipleDistributions:AGeneralSetting
Proof. LetZ andZ ,j ̸=kbetwovariablesthatiandj arenotadjacentinthemoralizedgraphofG . Thenitsufficesto
j k Z
showthat(j,k)∈/ E(M )and(k,j)∈/ E(M ). BecausetheyarenotadjacentinthemoralizedgraphofG ,theymust
Z Z Z
notbeadjacentinG andmustnotshareacommonchildinG . Thus,j andkared-separatedconditioningonV \{j,k},
Z Z
whichimpliestheconditionalindependenceZ ⊥⊥Z |Z\{Z ,Z }basedontheMarkovassumptionon{G ,P }. Then
j k j k Z Z
wehave(j,k)∈/ E(M )and(k,j)∈/ E(M ).
Z Z
Theorem3.5. GivenacausalDAGG anddistributionP withitsMarkovNetworkM , underMarkovassumption,
Z Z Z
theundirectedgraphdefinedbyM isthemoralizedgraphofthetruecausalDAGG ifandonlyiftheSAFandSUCF
Z Z
assumptionsaresatisfied.
Proof. Weprovebothdirectionsasfollows.
Sufficientcondition. Weproveitbycontradiction. Supposethatthestructuredefinedbysupp(M )isnotequivalentto
Z
themoralizedgraphofG . Then,accordingtoLem. 1,thereexistsapairofvariablesZ andZ ,j ̸=kthatiandj are
Z j k
adjacentinthemoralizedgraphbut(j,k)∈/ E(M )and(k,j)∈/ E(M ). Thus,wehaveZ ⊥⊥Z |Z\{Z ,Z }. Then
Z Z j k j k
weconsiderthefollowingtwocases:
• IfvariablesZ andZ correspondtoapairofneighborsinG ,thentheyareadjacent. Togetherwiththeconditional
j k Z
independencerelationZ ⊥⊥Z |Z\{Z ,Z },thisimpliesthattheSAFassumptionisviolated.
j k j k
• IfvariablesZ andZ correspondtoapairofnon-adjacentspousesinG . Thentheyhaveanunshieldedcollider,
j k Z
indicatingthattheSUCFassumptionisviolated.
Necessarycondition. Weproveitbycontradiction. SupposeSUCForSAFisviolated,wehavethefollowingtwocases:
• Suppose SUCF is violated, i.e., there exists an unshielded collider j → i ← k in the DAG G such that Z ⊥⊥
Z j
Z |Z\{Z ,Z }. Thisconditionalindependencerelationindicatesthat(j,k)∈/ E(M )and(k,j)∈/ E(M ). Sincej
k j k Z Z
andkarespouses,thereexistsanedgebetweentheminthemoralizedgraphofG ,butisnotcontainedinthestructure
Z
definedbyM ,showingthattheyarenotthesame.
Z
• Or,supposeSAFisviolated,i.e.,thereexistsapairofneighborsjandkintheDAGG suchthatZ ⊥⊥Z |Z\{Z ,Z }.
Z j k j k
Thisconditionalindependencerelationindicatesthat(j,k) ∈/ E(M )and(k,j) ∈/ E(M ). Becausej andk are
Z Z
adjacentinG ,clearlytheyarealsoadjacentinthemoralizedgraphofG . However,theedgebetweenthemisnot
Z Z
containedinthestructuredefinedbyM ,showingthattheyarenotthesame.
Z
Thus,whenSUCForSAFisviolated,thestructuredefinedbyM isthemoralizedgraphofthetrueDAGG .
Z Z
15