IMAGE CAPTIONING FOR BRAZILIAN PORTUGUESE USING GRIT
MODEL
RafaelSilvadeAlencar WilliamAlbertoCruzCastañeda MarcellusAmadeus
AlanaAIResearch AlanaAIResearch AlanaAIResearch
SãoPaulo,Brazil SãoPaulo,Brazil SãoPaulo,Brazil
rafael.silva@alana.ai william.cruz@alana.ai marcellus@alana.ai
ABSTRACT
ThisworkpresentstheearlydevelopmentofamodelofimagecaptioningfortheBrazilianPortuguese
language. WeusedtheGRIT(Grid-andRegion-basedImagecaptioningTransformer)modelto
accomplishthiswork. GRITisaTransformer-onlyneuralarchitecturethateffectivelyutilizestwo
visualfeaturestogeneratebettercaptions. TheGRITmethodemergedasaproposaltobeamore
efficientwaytogenerateimagecaptioning. Inthiswork,weadapttheGRITmodeltobetrained
inaBrazilianPortuguesedatasettohaveanimagecaptioningmethodfortheBrazilianPortuguese
Language.
Keywords ImageCaptioning·BrazilianPortugueseCaptioning·GridFeatures·RegionFeatures
1 Introduction
ImagecaptioningisanewtrendintheMachineLearning(ML)fieldheavilystudiedinthepastfewyears. Thegoalis
togenerateasemanticallyunderstandabletextgivenanimage. Severalstudieshaveuniqueapproachestosolvingthis
problemwithTransformersasafundamentalbasis. ExiststwoprimarymethodsfortheImaginecaptioningproblem:
grid features and region features. Grid features are local image features extracted at the regular grid points, often
obtaineddirectlyfromahigherlayerfeaturemap(s)ofCNNs/ViTs. Regionfeaturesareasetoflocalimagefeaturesof
theregions(i.e.,boundingboxes)detectedbyanobjectdetector[1]. Inthisstudy,wereintegratedthetwomethodsto
buildabettermodelforimagecaptioning. Withtheintegrationofthesetwomethods,themodelwillprovideabetter
representationofinputimagessincetheyarecomplementarytoeachother[1].
ThesecomponentsformaTransformer-onlyneuralarchitecture,dubbedGRIT(Grid-andRegion-basedImagecaptioning
Transformer). Theexperimentalresultsfromthestudy[1]showthatGRIThasestablishedanewstate-of-the-arton
standardimagecaptioning. Inthiswork,weusedaBrazilianPortuguese-languagetranslatedversionoftheCOCO
dataset[2]. Besidesthedataset,themodelalsorequiresavocabularyfile,whichisasetofwordspresentinthecorpus
ofthedataset. TheexperimentwasconductedwiththeCOCOdatasetinBrazilianPortuguesetoobtainamodelthat
comprehendsotherlanguagethanEnglish.
2 Method
TheGRIT(Grid-andRegion-basedImagecaptioningTransformer)consistsoftwoparts,oneforextractingthedual
visualfeaturesfromaninputimageandtheotherforgeneratingacaptionsentencefromtheextractedfeatures[1]. The
modelarchitectureisdescribedbelowinFigure1.
4202
beF
7
]VC.sc[
1v60150.2042:viXraFigure1: GRITmodelarchitecture[1]
2.1 ExtractionofVisualFeaturesfromImages
InthefirststepintheGRITmodel,aSwinTransformerappliesinitialvisualfeaturesfromtheinputimage. Thisfirst
stepismentionedinthepaperasBackboneNetworkforExtractingInitialFeatures[1]. Thesecondstepisgenerating
RegionFeatures. Itemploysatransformer-baseddecoderframeworkcalledDETR,insteadofCNN-baseddetectors,
suchasFasterR-CNNusedbySOTAimagecaptioningmodels[1]. Afterward,theGridFeatureNetworkreceivesthe
lastoneofthemulti-scalefeaturemapsfromtheSwinTransformerbackbone,i.e.,V
Lb
∈Rd×dLb,whereM =H/64
×W/64[1].
2.2 CaptionGenerationUsingDualVisualFeatures
Thecaptiongeneratorreceivesthetwotypesofvisualfeatures,theregionfeaturesandthegridfeatures,asinputs. It
generatesacaptionsentenceinanautoregressivemanner. Thatis,receivingthesequenceofpredictedwords(rigorously
theirembeddings)attimet−1. Itpredictsthenextwordattimet. Weemploythesinusoidalpositionalembeddingof
thetimestep;weaddittothewordembeddingtoobtaintheinputxt ∈Rdatt[1]. Thecaptiongeneratorconsistsofa
0
stackofL identicallayers. Theinitiallayerreceivesthesequenceofpredictedwordsandtheoutputfromthelastlayer
c
isinputtoalinearlayerwhoseoutputdimensionequalsthevocabularysizetopredictthenextword[1].
3 ExperimentalSetup
In this section, we describe the experimental setup used for training the Image-To-Text (ITT) GRIT model[1] for
theBrazilianPortugueselanguage. Italsoinformedthehardwareandsoftwareinfrastructure,specificsettings,the
benchmarkused,metrics,anddataset. Theexperimentwasconductedentirelywiththesamestepstakenintheoriginal
GRITmodelinEnglish,whicharetheextractionofvisualfeaturesfromimagesandCaptionGenerationusingdual
visualfeatures. Itwasnotrequiredtomakestructuralchangesintheoriginalmodeltoproducesimilarresultsforthe
BrazilianPortugueseversion.
3.1 HardwareandSoftwares
ImplementingtheoriginalITTGRITmodelrequirestwomachineswithaGPUA10080GB.Theexperimentappliedone
epochcomparedwiththeoriginalGRIT[1],whichhastenepochs,andthewholetrainingprocesstookapproximately
sevenhoursandthirtyminutestocomplete. AlltheconfigurationfortheexperimentwasinaLinuxUbuntuOperating
Systemversion20.0.
3.2 Datasets
ShowingthesimilaritiesanddifferencesbetweentheEnglishGRITandBrazilianPortugueseGRITmodels,wedescribe
thedatasetsused.
2EnglishImageCaptioning TheEnglishGRITmodelusestheCOCOdatasetfromtheoriginalexperiment. The
datasetcontains123287images. Eachimagehasfivedifferentannotationcaptions. Theexperimentuses113287images
inthetrainingprocessand5000forvalidationandtesting.
PortugueseImageCaptioning TheBrazilianPortugueseGRITmodelusesatranslatedversionoftheCOCOdataset.
This dataset contains the same images as the original COCO (123,287). Each image has five different annotation
captionstranslatedintotheBrazilianPortugueselanguage. Theexperimentuses113,287imagesinthetrainingprocess
and5,000forvalidationandtesting.
Animportantthingtopointoutisthatthetrainingofthedatasetalsoinvolvestheuseofavocabulary,whichisessentially
alistoftokenizedwordspresentinthecorpusofthedataset. Thedetailsofhowitworksinthetrainingprocessareyet
tobeknown;thework[1]doesnotshowdetailsofimplementationlefttoprogrammers,theinterpretationofthescripts,
andthestudyofhowthevocabularyisinvolvedintheprocess.
ITTEnglishGRITmodelexperimentsimplementavocabularycontaining10201words. Inourattempttotraininthe
translatedversionoftheGRITmodel,wetooktheoriginalvocabularyfileandperformedaliteraltranslationofthefile.
So,inthisfirstattempttoproduceanITTBrazilianPortuguesemodel,weuseavocabulary,whichisaliteraltranslation
oftheoriginal.
3.3 EvaluationMetrics
ToevaluatethequalityofthemodelitwasemployedthemetricsBLEU@N[3],METEOR[4],ROUGE-L[5],CIDEr[6].
BLEU(BiLingualEvaluationUnderstudy)isanautomaticevaluationmetricofmachine-translatedtext. TheBLEU
scoreisanumberbetweenzeroandonethatcomparesthesimilarityofmachine-translatedtexttoasetofhigh-quality
referencetranslations. Thevalue0meansthatthemachinetranslationoutputdoesnotmatchthereferencetranslation
(lowquality). Value1meansperfectmatchwithreferencetranslations(highquality)[7]. Thevaluesintable1areona
scalebetween0e1,where0means0%and1means100%.
Table1: PossibleoutcomesoftheBLEUmetric[7].
BLEUScore Interpretation
<10 Practicallyuseless
10-19 Difficulttounderstandthemeaning
20-29 Themeaningisclear,butthereareseriousgrammaticalerrors
30-40 Canbeunderstoodasgoodtranslations
40-50 High-qualitytranslations
50-60 Veryhighquality,adequateandfluenttranslations
>60 Ingeneral,higherthanhumanquality
METEOR, is an automatic metric for machine translation evaluation based on a generalized concept of unigram
matchingbetweenmachine-producedtranslationandhuman-producedreferencetranslations. Unigramscanbematched
basedontheirsurfaceforms,stemmedforms,andmeanings. Onceallgeneralizedunigrammatchesbetweenthetwo
stringshavebeenfound, METEORcomputesascoreforthismatchingusingacombinationofunigram-precision,
unigram-recall,andameasureoffragmentationdesignedtocapturehowwell-orderedthematchedwordsinthemachine
translationareaboutthereference[4]. METEORgetsanRcorrelationvalueof0.347withhumanevaluationonthe
Arabicdataand0.331ontheChinesedata.
CIDEr A Consensus-based Image Description Evaluation metric for evaluating the quality of generated textual
descriptions of images. The CIDEr metric measures the similarity between a generated caption and the reference
captions. Basedontheconceptofconsensus: theideathatgoodcaptionsshouldnotonlybesimilartothereference
captionsintermsofwordchoiceandgrammarbutalsointermsofmeaningandcontent[6].
ROUGE,orRecall-OrientedUnderstudyforGistingEvaluation,isasetofmetricsandasoftwarepackageforevaluating
automaticsummarizationandmachinetranslationsoftwareinnaturallanguageprocessing. Themetricscomparean
automaticallyproducedsummaryortranslationagainstareferenceorasetofreferences(human-produced)summary
ortranslation. ROUGEmetricsrangebetween0and1,withhigherscoresindicatinghighersimilaritybetweenthe
automaticallyproducedsummaryandthereference[5].
34 Results
Thissectionshowsthemodelresultsafterobtainingthecheckpointsfromthetrainingsteps. Withthecheckpoint,we
implementascriptthatgeneratesthecaptionsinBrazilianPortugueseforagivenimage. Themodelgeneratescaptions
thatarestillmissingabettersemanticquality. LegiblyinBrazilianPortugueseandrelatestotheimageassociatedwith
it. Theexperimentinitializeswiththesamedatasettingsastheoriginalexperiment. Inthefirstinstance,thegoalwasto
testthereproducibilityofthemodelandproduceafirstattemptataBrazilianPortugueseversionofthemodel. Itwas
possibletoinitiateandfinishthetrainingprocessgivenoneepoch. Throughtheexamplesshownbelow,wecanobserve
theresultingprocessoftheexperiment. Weshowcaptionsgeneratedbyanimageasinput. Intheimages2,3and4
wehaveafairlyaccuratedescriptionthroughthecaption,however,therearesomeissueswithliteraltranslationand
semantics.
Figure2: "umcarroEstacionadoemolado Figure 3: "um cão Isso É sentado em o
deumrua" Voltardeumcarro"
Figure4: "umhomemJogartênisemumtênistribunal"
Table2showsthemetricsresultsoftheBrazilianPortugueseversionoftheGRITmodel. Comparedwiththeoriginal
EnglishGRITmodel,theresultsaredifferentbutnotextremelyfar,whichmeansthatevenrunningthemodelwithone
epochachievessimilarresults.
Table2: BrazilianPortugueseGRITmodelresultmetrics
Metrics Portuguesemodel Englishmodel[1]
BLEU 0.758 0.842
METEOR 0.268 0.306
ROUGE 0.557 0.607
CIDEr 1.100 1.442
5 Conclusion
ThemainobjectiveofthisworkistoproposeaTransformer-basedarchitectureforimagecaptioningnamedGRITfor
BrazilianPortuguese. Theideaistointegratetheregionfeaturesandthegridfeaturesextractedfromaninputimageto
4extractrichervisualinformationfrominputimages. Theexperimentalresultsvalidatedourapproach,showingthat
GRIToutperformsallpublishedmethodsbyalargemarginininferenceaccuracyandspeed. InourversionoftheITT
GRITmodel,weusedadatasetinBrazilianPortuguese,whichisatranslationfromtheoriginalCOCOdataset. The
experimentusesthesamestepstakenintheoriginalGRITmodel.
Asafuturework,themaingoalistoproduceanotherprototypeoftheITTGRITmodelfortheBrazilianPortuguese
languagewithoutavocabularyfile. Asareference,weareworkingonadifferentbranchoftheGRITmodel(vicap
branch)[1]. Thisbranchisspecificallyforfine-tuningotherdatasetsbesidestheCOCO.
Acknowledgments
WeacknowledgethesupportoftheNationalCouncilforScientificandTechnologicalDevelopment(CNPq),andAlana
AIforfundingtheresearch.
Funding
ThisresearchissupportedbytheNationalCouncilforScientificandTechnologicalDevelopment(CNPq/MCTI/SEMPI
Number021/2021RHAE-384217/2023-0),MinistryofScience,TechnologyandInnovation,andAlanaAI.
Contributions
RAconductedtheexperiments. WCandMSprovidedoversightfortheresearch. Allauthorscontributedtothepaper’s
composition,withMSandWCcontributingtothestudy’sconception. Thefinalmanuscriptwasreviewedandapproved
byallauthors.
Interests
Theauthorsdeclarethattheyhavenocompetinginterests.
Materials
Thedatasetsgeneratedand/oranalyzedduringthecurrentstudyareavailableonrequest.
References
[1] Van-Quang Nguyen, Masanori Suganuma, and Takayuki Okatani. Grit: Faster and better image captioning
transformerusingdualvisualfeatures. InComputerVision–ECCV2022: 17thEuropeanConference,TelAviv,
Israel,October23–27,2022,Proceedings,PartXXXVI,pages167–184.Springer,2022.
[2] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and
CLawrenceZitnick. Microsoftcoco:Commonobjectsincontext. InComputerVision–ECCV2014:13thEuropean
Conference,Zurich,Switzerland,September6-12,2014,Proceedings,PartV13,pages740–755.Springer,2014.
[3] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of
machinetranslation. InProceedingsofthe40thannualmeetingoftheAssociationforComputationalLinguistics,
pages311–318,2002.
[4] SatanjeevBanerjeeandAlonLavie. Meteor: Anautomaticmetricformtevaluationwithimprovedcorrelationwith
humanjudgments. InProceedingsoftheaclworkshoponintrinsicandextrinsicevaluationmeasuresformachine
translationand/orsummarization,pages65–72,2005.
[5] Chin-YewLin. Rouge: Apackageforautomaticevaluationofsummaries. InTextsummarizationbranchesout,
pages74–81,2004.
[6] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description
evaluation. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages4566–4575,
2015.
[7] GoogleGoogle. Comoavaliarmodelos | documentaçãodoautomltranslation | googlecloud.
5