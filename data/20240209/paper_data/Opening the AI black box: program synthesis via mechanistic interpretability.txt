Opening the AI black box:
program synthesis via mechanistic interpretability
EricJ.Michaud*12 IsaacLiao*1 VedangLad*1 ZimingLiu*12 AnishMudide1 ChloeLoughridge3
ZifanCarlGuo1 TaraRezaeiKheirkhah1 MatejaVukelic´1 MaxTegmark12
Abstract interpretabilitycanbefullyautomated(Tegmark&Omo-
hundro,2023).
We present MIPS, a novel method for program
synthesisbasedonautomatedmechanisticinter- Thegoalofthepresentpaperistotakeamodestfirststepin
pretabilityofneuralnetworkstrainedtoperform thisdirectionbypresentingandtestingMIPS(Mechanistic-
thedesiredtask,auto-distillingthelearnedalgo- Interpretability-based Program Synthesis), a fully auto-
rithmintoPythoncode. WetestMIPSonabench- mated method that can distill simple learned algorithms
markof62algorithmictasksthatcanbelearned from neural networks into Python code. The rest of this
byanRNNandfindithighlycomplementaryto paperisorganizedasfollows. Afterreviewingpriorworkin
GPT-4: MIPS solves 32 of them, including 13 SectionII,wepresentourmethodinSectionIII,testitona
thatarenotsolvedbyGPT-4(whichalsosolves benchmarkinSectionIVandsummarizeourconclusionsin
30). MIPSusesanintegerautoencodertoconvert SectionV.
theRNNintoafinitestatemachine,thenapplies
Booleanorintegersymbolicregressiontocapture
2.RelatedWork
thelearnedalgorithm. Asopposedtolargelan-
guagemodels,thisprogramsynthesistechnique ProgramsynthesisisavenerablefielddatingbacktoAlonzo
makes no use of (and is therefore not limited Church in 1957; Zhou & Ding (2023) and Odena et al.
by)humantrainingdatasuchasalgorithmsand (2020)providerecentreviewsofthefield. Largelanguage
codefromGitHub. Wediscussopportunitiesand Models(LLMs)havebecomeincreasinglygoodatwriting
challengesforscalingupthisapproachtomake codebasedonverbalproblemdescriptionsorauto-complete.
machine-learnedmodelsmoreinterpretableand Weinsteadstudythecommonalternativeproblemsetting
trustworthy. known as “programming by example” (PBE), where the
desiredprogramisspecifiedbygivingexamplesofinput-
outputpairs(Wuetal.,2023). Theaforementionedpapers
1.Introduction reviewawidevarietyofprogramsynthesismethods,many
ofwhichinvolvesomeformofsearchoveraspaceofpos-
Machine-learned algorithms now outperform traditional
sibleprograms. LLMsthatsynthesizecodedirectlyhave
human-discoveredalgorithmsonmanytasks,fromtransla-
recentlybecomequitecompetitivewithsuchsearch-based
tiontogeneral-purposereasoning.Theselearnedalgorithms
approaches(Sobaniaetal.,2022). Ourworkprovidesan
tendtobeblack-boxneuralnetworks,andwetypicallylack
alternativesearch-freeapproachwheretheprogramlearning
afullunderstandingofhowtheywork. Thishasmotivated
happensduringneuralnetworktrainingratherthanexecu-
thegrowingfieldofmechanisticinterpretability,aimingto
tion.
assessandimprovetheirtrustworthiness. Majorprogress
has been made in interpreting and understanding smaller Ourworkbuildsontherecentprogressinmechanisticinter-
models, but this work has involved human effort, which pretability(MI)ofneuralnetworks(Olahetal.,2020;Cam-
raisesquestionsaboutwhetheritcanscaletolargermod- marataetal.,2020;Wangetal.,2022;Olssonetal.,2022).
els. Thismakesittimelytoinvestigatewhethermechanistic MuchMIworkhastriedtounderstandhowneuralnetworks
representvarioustypesofinformation,e.g.,geographicin-
*Equalcontribution,alphabeticalorder 1MassachusettsInsti- formation (Goh et al., 2021; Gurnee & Tegmark, 2023),
tuteofTechnology, Cambridge, MA,USA2InstituteforArtifi-
truth(Burnsetal.,2022;Marks&Tegmark,2023)andthe
cialIntelligenceandFundamentalInteractions3HarvardUniver-
stateofboardgames(McGrathetal.,2022;Toshniwaletal.,
sity,Cambridge,MA,USA.Correspondenceto: MaxTegmark
<tegmark@mit.edu>. 2022;Lietal.,2022). AnothermajorMIthrusthasbeento
understandhowneuralnetworksperformalgorithmictasks,
Preprint.Underreview.
1
4202
beF
7
]GL.sc[
1v01150.2042:viXraProgramsynthesisviamechanisticinterpretability
e.g.,modulararithmetic(Nandaetal.,2023;Liuetal.,2022; usearecurrentneuralnetwork(RNN)ofthegeneralform
Zhongetal.,2023;Quirkeetal.,2023),greater-than(Hanna
h = f(h ,x ), (1)
etal.,2023),andgreatest-common-divisor(Charton,2023). i i−1 i
y = g(h ), (2)
i i
WhereasLindneretal.(2023)automaticallyconverttradi-
thatmapsinputvectorsx intooutputvectorsy viahidden
tionalcodeintoaneuralnetwork,weaimtodotheopposite. i i
statesh . TheRNNisdefinedbythetwofunctionsf and
OtherrecenteffortstoautomateMIincludeidentifyinga i
g,whichareimplementedasfeed-forwardneuralnetworks
sparse subgraph of the network whose units are causally
(MLPs)toallowmoremodelexpressivitythanforavanilla
relevanttoabehaviorofinterest(Conmyetal.,2023;Syed
RNN.Thetechniquesdescribedbelowcanalsobeapplied
etal.,2023)andusingLLMstolabelinternalcomponentsof
tomoregeneralneuralnetworkarchitectures.
neuralnetworks,forinstanceneurons(Billsetal.,2023)and
featuresdiscoveredbysparseautoencoders(Cunningham Step2attemptstoautomaticallysimplifythelearnedneu-
etal.,2023;Brickenetal.,2023). ral network without reducing its accuracy. Steps 3 and 4
automaticallydistillthissimplifiedlearnedalgorithminto
3.MIPS,ourprogramsynthesisalgorithm Pythoncode. Whenthetrainingdataisdiscrete(consisting
ofsaytexttokens,integers,orpixelcolors),theneuralnet-
workwillbeafinitestatemachine: theactivationvectors
foreachofitsneuronlayersdefinefinitesetsandtheentire
working of the network can be defined by look-up tables
specifyingtheupdaterulesforeachlayer.ForourRNN,this
meansthatthespaceofhiddenstateshisdiscrete,sothat
thefunctionsf andgcanbedefinedbylookuptables. As
wewillseebelow,thenumberofhiddenstatesthatMIPS
needstokeeptrackofcanoftenbegreatlyreducedbyclus-
teringthem,correspondingtolearnedrepresentations. After
this,thegeometryoftheclustercentersinthehiddenspace
oftenrevealsthattheyformeitheranincompletemultidi-
mensionallatticewhosepointsrepresentintegertuples,or
a set whose cardinality is a power of two, whose points
representBooleantuples. Inbothofthesecases,theafore-
mentionedlookuptablessimplyspecifyintegerorBoolean
functions,whichMIPSattemptstodiscoverviasymbolic
regression. Belowwepresentanintegerautoencoderand
a Boolean autoencoder to discover such integer/Boolean
representationsfromarbitrarypointsets.
We will now describe each of the three steps of MIPS in
greaterdetail.
Figure1.Thepipelineofourprogramsynthesismethod. MIPS
3.1.AutoMLoptimizingforsimplicity
reliesondiscoveringintegerrepresentationsandbitrepresentations
ofhiddenstates,whichenableregressionmethodstofigureoutthe WewishtofindthesimplestRNNthatcanlearnourtask,to
exactsymbolicrelationsbetweeninput,hidden,andoutputstates. facilitatesubsequentdiscoveryofthealgorithmthatithas
learned. WethereforeimplementanAutoML-styleneural
AssummarizedinFigure1,MIPSinvolvesthefollowing
architecturesearchthattriestominimizenetworksizewhile
keysteps.
achievingperfecttestaccuracy. Thissearchspaceisdefined
byavectorpoffivemainarchitecturehyperparameters: the
1. Neuralnetworktraining
fiveintegersp=(n,w ,d ,w ,d )correspondingtothe
f f g g
2. Neuralnetworksimplification dimensionality of hidden state h, the width and depth of
thef-network,andthewidthanddepthoftheg-network,
3. Finitestatemachineextraction respectively. Boththef-andg-networkshavealinearfinal
layerandReLUactivationfunctionsforallpreviouslayers.
4. Symbolicregression
Thehiddenstateh isinitializedtozero.
0
Step 1 is to train a black-box neural network to learn an To define the parameter search space, we define ranges
algorithmthatperformsthedesiredtask. Inthispaper,we for each parameter. For all tasks, we use n ∈
2Programsynthesisviamechanisticinterpretability
{1,2,...,128}, w ∈ {1,2,...,256}, d ∈ {1,2,3}, f(h,x) = Wh+Vx+b, then the symmetry transfor-
f f
w ∈{1,2,...,256}andd ∈{1,2,3},sothetotalsearch mation
g g
spaceconsistsof128×256×3×256×3=75,497,472
W′ ≡ AWA−1, V′ = AV, U′ = UA−1, h′ ≡ Ah,
hyperparametervectorsp . Weorderthissearchspaceby
i b′ =AbkeepstheRNNinthesameform:
imposingastrictorderingontheimportanceofminimizing
eachhyperparameter–lowerd g isstrictlymoreimportant h′ i = Ah i =AWA−1Ah i−1+AVx i+Ab
thanlowerd ,whichisstrictlymoreimportantthanlower
f = W′−1h′ +V′x +b′, (3)
n,whichisstrictlymoreimportantthanlowerw ,which i−1 i
g y = G(Uh +c)=G(UA−1h′ +c)
isstrictlymoreimportantthanlowerw f. Weaimtofind i i i
thehyperparametervector(integer5-tuple)p inthesearch = G(U′h′ +c). (4)
i i
spacewhichhaslowestrankiunderthisordering.
Wethinkofneuralnetworksasnails,whichcanbehitby
Wesearchthespaceinthefollowingsimplemanner.Wefirst
variousauto-normalizationhammers. Eachhammerisan
startatindexi=65,536,whichcorrespondstoparameters
algorithmthatappliestransformationstotheweightstore-
(1,1,2,1,1). Foreachparametertuple,wetrainnetworks
move degrees of freedom caused by extra symmetries or
using5differentseeds. Weusethelossfunctionℓ(x,y)=
1log[1+(x−y)2],findingthatitledtomorestabletraining cleans the neural network up in some other way. In this
2 section, we describe five normalizers we use to simplify
than using vanilla MSE loss. We train for either 10,000
ourtrainednetworks,termed“Whitening”,“Jordannormal
or 20,000 steps, depending on the task, using the Adam
form”,“Toeplitz”,“De-bias”,and“Quantization”.Forevery
optimizer,alearningrateof10−3,andbatchsize4096. The
neuralnetwork,wealwaysapplythissequenceofnormal-
testaccuracyisevaluatedwithabatchof65536samples. If
izers in that specific order, for consistency. We describe
nonetworksachieve100%testaccuracy(onanytestbatch),
thembelowandprovideadditionaldetailsabouttheminthe
weincreaseiby21/4. Weproceedinthismanneruntilwe
AppendixD.
findanetworkwhereoneoftheseedsachievesperfecttest
accuracyoruntilthefullrangeisexhausted. Ifwefinda
1. Whitening: Justaswenormalizeinputdatatousefor
workingnetworkonthisupwardssweep,wethenperform
trainingneuralnetworks,wewouldlikeactivationsin
abinarysearchusingtheintervalhalvingmethod,starting
thehiddenstatespaceh tobenormalized. Toensure
fromthesuccessfuli,tofindthelowestiwhereatleastone i
normalization in all directions, we feed the training
seedachievesperfecttestaccuracy.
datasetintotheRNN,collectallthehiddenstates,com-
pute the uncentered covariance matrix C, and then
3.2.Auto-simplification
applyawhiteningtransformh(cid:55)→C−1/2htothehid-
After finding a minimal neural network architecture that denstatespacesothatitsnewcovariancebecomesthe
cansolveatask,theresultingneuralnetworkweightstyp- identitymatrix. Thisoperationexistspurelytoprovide
icallyseemrandomandun-interpretable. Thisisbecause betternumericalstabilitytothenextstep.
there exist symmetry transformations of the weights that
2. Jordannormalform: Whenthefunctiongisaffine,
leave the overall input-output behavior of the neural net-
wecanapplytheaforementionedsymmetrytransfor-
workunchanged. Therandominitializationofthenetwork
mation to try to diagonalize W, so that none of the
hasthereforecausedrandomsymmetrytransformationsto
hiddenstatedimensionsinteractwithoneanother. Un-
beappliedtotheweights. Inotherwords,thelearnednet-
fortunately, notallmatricesW canbediagonalized,
work belongs to an equivalence class of neural networks
soweuseageneralizedalternative: theJordannormal
withidenticalbehaviorandperformance,correspondingto
form,whichallowselementsofthesuperdiagonalto
a submanifold of the parameter space. We exploit these
beeitherzeroorone. Toeliminatecomplexnumbers,
symmetrytransformationstosimplifytheneuralnetwork
wealsoapply2×2unitarytransformationstoeigen-
intoanormalform,whichinasenseisthesimplestmember
vectorscorrespondingtoconjugatepairsofcomplex
ofitsequivalenceclass. Conversionofobjectsintoanor-
eigenvaluesafterward. Theaforementionedwhitening
mal/standardformisacommonconceptinmathematicsand
isnowruined,butithelpedmaketheJordannormal
physics(forexample,conjunctivenormalform,wavefunc-
formcalculationmorenumericallystable.
tionnormalization,reducedrowechelonform,andgauge
fixing). 3. Toeplitz: Once W isin Jordannormalform, we di-
videitupintoJordanblocksandapplyupper-triangular
Two of our simplification strategies below exploit a sym-
Toeplitztransformationstothedimensionsbelonging
metry of the RNN hidden state space h. We can always
toeachJordanblock. Thereisnowanadditionalsym-
write the MLP g in the form g(h) = G(Uh + c) for
metry,correspondingtomultiplyingeachJordanblock
some function G. So if f is affine, i.e., of the form
byanuppertriangularToeplitzmatrix,andweexploit
3Programsynthesisviamechanisticinterpretability
Bit representation (binary_addition) Integer lattice (sum_last2) Sincethetasksinourbenchmarkinvolvebitsandintegers,
100 1.75 95 which are already discrete, the only non-discrete parts in
1.50 a recurrent neural network are its hidden representations.
90
1.25
1.00 85 Here we show two cases when hidden states can be dis-
0.75 80 cretized: theyare(1)abitrepresentationor(2)a(typically
0.50 75 incomplete)integerlattice. Generalizingtothemixedcase
0.25
70
0.6 0.4 0.2 0.0 0.2 0.4 0.6 50 45 40 35 30 25 20 ofbitsandintegersisstraightforward. Figure2showsall
First hidden dimension First hidden dimension
Figure2.Thesehiddenstructurescanbeturnedintodiscreterep- hiddenstateactivationvectorsh iforallstepswithalltrain-
resentations.Left:thehiddenstatesforthebitstringadditiontask ing examples for two of our tasks. The left panel shows
areseentoformfourclusters,correspondingto2bits:theoutput thatthe104pointsh form22 =4tightclusters,whichwe
i
bitandthecarrybit. Right: thehiddenstatesfortheSum Last2 interpretasrepresenting2bits. Therightpanelrevealsthat
taskareseentoformclustersona2Dlatticecorrespondingtotwo thepointsh formanincomplete2Dlatticethatweinterpret
i
integers. assecretlyrepresentingapairofintegers.
Bitrepresentations
theToeplitzmatrixthatmaximallysimplifiestheafore-
Thehiddenstatesforthe2bitsinFigure2areseentoform
mentionedV-matrix.
aparallelogram. Moregenerally,wefindthathiddenstates
encode b bits as 2b clusters, which in some cases form b-
4. De-bias: SometimesWisnotfullrank,andbhasa
dimensionalparallelogramsandinothercaseslookmore
component in the direction of the nullspace. In this
random. Ouralgorithmtriesall(2b)!possibleassignments
case, thecomponentcanberemoved, andthebiasc
of the 2b clusters to bitstrings of length b and selects the
canbeadjustedtocompensate.
assignmentthatminimizesthelengthoftheresultingPython
5. Quantization: Afterapplyingallthepreviousnormal- program.
izers, many of the weights may have become close
Integerlattice
tointegers,butnotexactlyduetomachineprecision
andtrainingimperfections. Sometimes,dependingon AsseeninFigure2,thelearnedrepresentationofaninteger
thetask,alloftheweightscanbecomeintegers. We latticetendstobebothnon-square(deformedbyarandom
thereforeroundanyweightsthatarewithinϵ ≡ 0.01 affinetransformation)andsparse(sincenotallintegertu-
ofanintegertothatinteger. plets occur during training). We thus face the following
problem: given(possiblysparse)samplesofpointsh from
i
an n-dimensional lattice, how can we reconstruct the in-
3.3.Booleanandintegerautoencoders
teger lattice in the sense that we figure out which integer
As mentioned, our goal is to convert a trained recurrent tuple each lattice point represents? We call the solution
neural network (RNN) into a maximally simple (Python) an integer autoencoder since it compresses any point set
program that produces equivalent input-output behavior. intoasetofintegertuplesfromwhichtheoriginalpoints
ThismeansthatiftheRNNhas100%accuracyforagiven canbeatleastapproximatelyrecoveredash =Ak +b,
i i
dataset, soshouldtheprogram, withtheaddedbenefitof whereAisamatrixandbisavectorthatdefinestheaffine
beingmoreinterpretable,precise,andverifiable. transformationandasetofintegervectorsk .
i
Oncetrained/written,thegreatestdifferencebetweenaneu- In the Appendix A, we present a solution that we call
ral network and a program implementing the same finite the GCD lattice finder. For the special case n = 1, its
state machine is that the former is fuzzy and continuous, core idea is to compute the greatest common denomina-
whilethelatterispreciseanddiscrete. Toconvertaneural tor of pairwise separations: for example, for the points
network to a program, some discretization (“defuzzifica- {1.7,3.2,6.2,7.7...},allpointseparationsaredivisibleby
tion”)processisneededtoextractpreciseinformationfrom A=1.5,fromwhichoneinfersthatb=0.2andthelattice
seeminglynoisyrepresentations. Fortunately,mechanistic can be rewritten as 1.5×{1,2,4,5}+0.2. For multidimen-
interpretability research has shown that neural networks sional lattices, our algorithm uses the GCD of ratios of
tendtolearnmeaningful,structuredknowledgerepresenta- generalizedcellvolumestoinferthedirectionsandlengths
tionsforalgorithmictasks(Liuetal.,2022;Nandaetal., ofthelatticevectorsthatformthecolumnsofA.
2023). Previousinterpretabilityeffortstypicallyinvolved
ForthespecialcasewheretheMLPdefiningthefunction
case-by-casemanualinspection,andonlygainedalgorith-
f isaffineorcanbeaccuratelyapproximatedasaffine,we
micunderstandingatthelevelofpseudocodeatbest. We
useasimplermethodwetermtheLinearlatticefinder,also
tacklethismoreambitiousquestion: canwecreateanau-
describedinAppendixB.Heretheideaistoexploitthatthe
tomatedmethodthatdistillsthelearnedrepresentationand
latticeissimplyanaffinetransformationofaregularinteger
associatedalgorithmsintoanequivalent(Python)program?
4
noisnemid
neddih
dnoceS
noisnemid
neddih
dnoceSProgramsynthesisviamechanisticinterpretability
lattice (the input data), so we can simply “read off” the 10or20intoanewintegerlist. Werefertointegerswhose
desiredlatticebasisvectorsfromthisaffinetransformation. range is limited to {0,1} as bits. We generated this task
listmanually,attemptingtoproduceacollectionofdiverse
Symbolicregression
tasksthatwouldinprinciplebesolvablebyanRNN.We
Oncethehiddenstatesh havebeensuccessfullymappedto alsofocusedontaskswhoseknownalgorithmsinvolvedma-
i
Booleanorintegertuplesasdescribedabove,thefunctions jority,minimum,maximum,andabsolutevaluefunctionsbe-
f andgthatspecifythelearnedRNNcanbere-expressed causewebelievedtheywouldbemoreeasilylearnablethan
aslookuptables,showingtheirBoolean/integeroutputtuple othernonlinearalgorithmsduetoourchoiceoftheReLU
foreachBoolean/integerinputtuple.Allthatremainsisnow activation for our RNNs. The benchmark training data
symbolicregression,i.e.,discoveringthesimplestpossible andprojectcodeisavailableathttps://github.com/
symbolicformulaethatdefinef andg. ejmichaud/neural-verification. Thetasksare
describedinTable1,withadditionaldetailsinAppendixE.
Boolean regression: In the case where a function maps
bits to a bit, our algorithm determines the following set SincethefocusofourpaperisnotonwhetherRNNscan
ofcorrectBooleanformulaeandthenreturnstheshortest learnalgorithms,butonwhetherlearnedalgorithmscanbe
one. The first candidate formula is the function written auto-extractedintoPython,wediscardedfromourbench-
in disjunctive normal form, which is always possible. If markanygeneratedtasksonwhichourRNN-trainingfailed
theBooleanfunctionissymmetric,i.e.,invariantunderall toachieve100%accuracy.
permutationsofitsarguments,thenwealsowriteitasan
OurbenchmarkcannevershowthatMIPSoutperformsany
integerfunctionofitsbitsum.
largelanguagemodel(LLM).BecauseLLMsaretypically
Integerregression: Inthecasewhenafunctionmapsinte- trainedonGitHub,manyLLMscanproducePythoncode
gerstoaninteger,wetrythefollowingtwomethods: forcomplicatedprogrammingtasksthatfalloutsideofthe
classwestudy. Instead,thequestionthatourMIPS-LLM
1. Ifthefunctionislinear,thenweperformsimplelinear comparisonaddressesiswhetherMIPScomplementsLLMs
regression,roundtheresultingcoefficientstointegers, bybeingabletosolvesometaskswhereanLLMfails.
andsimplify,e.g.,multiplicationsby0and1.
4.2.Evaluation
2. Otherwise, we use the brute-force symbolic solver
from AI Feynman (Udrescu et al., 2020), including For both our method and GPT-4 Turbo, a task is consid-
the 6 unary operations {>,<,∼,H,D,A} and 4 bi- ered solved if and only if a Python program is produced
naryoperations{+,−,∗,%}whosemeaningsareex- thatsolvesthetaskwith100%accuracy. GPT-4Turbois
plainedinAppendixC,thenconvertthesimplestdis- promptedusingthe“chain-of-thought”approachdescribed
coveredformulaintoPythonformat. belowandillustratedinFigure5.
Foragiventask, theLLMreceivestwolistsoflength10
Oncesymbolicformulashavebeenseparatelydiscoveredfor
sourcedfromtherespectiveRNNtrainingset. Themodel
eachcomponentofthevector-valuedfunctionsf andg,we
is instructed to generate a formula that transforms the el-
insertthemintoatemplatePythonprogramthatimplements
ements of list “x” (features) into the elements of list ‘y’
thebasicloopoverinputsthatareinherentinanRNN.We
(labels). Subsequently,themodelisinstructedtotranslate
presentexamplesofourauto-generatedprogramsinFigures
this formula into Python code. The model is specifically
3and4andinAppendixF.
askedtouseelementsoftheaforementionedlistsasatest
caseandprint“Success”or“Failure”ifthegeneratedfunc-
4.Results tion achieves full accuracy on the test case. An external
programextractsafencedmarkdowncodeblockfromthe
WewillnowtesttheprogramsynthesisabilitiesofourMIPS
output, which is saved to a separate file and executed to
algorithm on a benchmark of algorithmic tasks specified
determineifitsuccessfullycompletesthetask. Toimprove
bynumericalexamples. Forcomparison,wetrythesame
thechanceofsuccess,thisGPT-4Turbopromptingprocess
benchmarkonGPT-4Turbo,whichiscurrently(asofJan-
isrepeatedthreetimes,requiringonlyatleastoneofthem
uary2024)describedbyOpenAIastheirlatestgeneration
tosucceed. WerunGPTusingdefaulttemperaturesettings.
model,witha128kcontextwindowandmorecapablethan
theoriginalGPT-4.
4.1.Benchmark
Ourbenchmarkconsistsofthe62algorithmictaskslisted
inTable1. Theyeachmaponeortwointegerlistsoflength
5Programsynthesisviamechanisticinterpretability
Table1. Benchmarkresults.Fortaskswithnote“seetext”,pleaserefertoAppendixE
Task# Input Element TaskDescription TaskName Solved by Solved by
Strings Type GPT-4? MIPS?
1 2 bit Binaryadditionoftwobitstrings Binary Addition 0 1
2 2 int Ternaryadditionoftwodigitstrings Base 3 Addition 0 0
3 2 int Base4additionoftwodigitstrings Base 4 Addition 0 0
4 2 int Base5additionoftwodigitstrings Base 5 Addition 0 0
5 2 int Base6additionoftwodigitstrings Base 6 Addition 1 0
6 2 int Base7additionoftwodigitstrings Base 7 Addition 0 0
7 2 bit BitwiseXOR Bitwise Xor 1 1
8 2 bit BitwiseOR Bitwise Or 1 1
9 2 bit BitwiseAND Bitwise And 1 1
10 1 bit BitwiseNOT Bitwise Not 1 1
11 1 bit Parityoflast2bits Parity Last2 1 1
12 1 bit Parityoflast3bits Parity Last3 0 1
13 1 bit Parityoflast4bits Parity Last4 0 0
14 1 bit Parityofallbitsseensofar Parity All 0 1
15 1 bit Parityofnumberofzerosseensofar Parity Zeros 0 1
16 1 int Cumulativenumberofevennumbers Evens Counter 0 0
17 1 int Cumulativesum Sum All 1 1
18 1 int Sumoflast2numbers Sum Last2 0 1
19 1 int Sumoflast3numbers Sum Last3 0 1
20 1 int Sumoflast4numbers Sum Last4 1 1
21 1 int Sumoflast5numbers Sum Last5 1 1
22 1 int sumoflast6numbers Sum Last6 1 1
23 1 int Sumoflast7numbers Sum Last7 1 1
24 1 int Currentnumber Current Number 1 1
25 1 int Number1stepback Prev1 1 1
26 1 int Number2stepsback Prev2 1 1
27 1 int Number3stepsback Prev3 1 1
28 1 int Number4stepsback Prev4 1 1
29 1 int Number5stepsback Prev5 1 1
30 1 int 1iflasttwonumbersareequal Previous Equals Current 0 1
31 1 int current−previous Diff Last2 0 1
32 1 int |current−previous| Abs Diff 0 1
33 1 int |current| Abs Current 1 1
34 1 int |current|−|previous| Diff Abs Values 1 0
35 1 int Minimumofnumbersseensofar Min Seen 1 0
36 1 int Maximumofintegersseensofar Max Seen 1 0
37 1 int integerin0-1withhighestfrequency Majority 0 1 1 0
38 1 int Integerin0-2withhighestfrequency Majority 0 2 0 0
39 1 int Integerin0-3withhighestfrequency Majority 0 3 0 0
40 1 int 1ifeven,otherwise0 Evens Detector 1 0
41 1 int 1ifperfectsquare,otherwise0 Perfect Square Detector 0 0
42 1 bit 1ifbitstringseensofarisapalindrome Bit Palindrome 1 0
43 1 bit 1ifparenthesesbalancedsofar,else0 Balanced Parenthesis 0 0
44 1 bit Numberofbitsseensofarmod2 Parity Bits Mod2 1 0
45 1 bit 1iflast3bitsalternate Alternating Last3 0 0
46 1 bit 1iflast4bitsalternate Alternating Last4 1 0
47 1 bit bitshifttoright(sameasprev1) Bit Shift Right 1 1
48 2 bit Cumulativedotproductofbitsmod2 Bit Dot Prod Mod2 0 1
49 1 bit Binarydivisionby3(seetext) Div 3 1 0
50 1 bit Binarydivisionby5(seetext) Div 5 0 0
51 1 bit Binarydivisionby7(seetext) Div 7 0 0
52 1 int Cumulativeadditionmodulo3 Add Mod 3 1 1
53 1 int Cumulativeadditionmodulo4 Add Mod 4 0 0
54 1 int Cumulativeadditionmodulo5 Add Mod 5 0 0
55 1 int Cumulativeadditionmodulo6 Add Mod 6 0 0
56 1 int Cumulativeadditionmodulo7 Add Mod 7 0 0
57 1 int Cumulativeadditionmodulo8 Add Mod 8 0 0
58 1 int 1Ddithering,4-bitto1-bit(seetext) Dithering 1 0
59 1 int Newton’sof-freebody(integerinput) Newton Freebody 0 1
60 1 int Newton’slawofgravity(seetext) Newton Gravity 0 1
61 1 int Newton’slaww.spring(seetext) Newton Spring 0 1
62 2 int Newton’slaww.magneticfield(seetext) Newton Magnetic 0 0
Totalsolved 30 32
6Programsynthesisviamechanisticinterpretability
1 1 def f(s):
2 def f(s,t): 2 a = 198;b = -11;c = -3;d = 483;e = 0;
3 a = 0;b = 0; 3 ys = []
4 ys = [] 4 for i in range(20):
5 for i in range(10): 5 x = s[i]
6 c = s[i]; d = t[i]; 6 next_a = -b+c+190
7 next_a = b ˆ c ˆ d 7 next_b = b-c-d-e+x+480
8 next_b = b+c+d>1 8 next_c = b-e+8
9 a = next_a;b = next_b; 9 next_d = -b+e-x+472
10 y = a 10 next_e = a+b-e-187
11 ys.append(y) 11 a = next_a;b = next_b;c = next_c;d
12 return ys = next_d;e = next_e;
12 y = -d+483
13 ys.append(y)
Figure3.Thegeneratedprogramfortheadditionoftwobinary 14 return ys
numbersrepresentedasbitsequences.NotethatMIPSrediscovers
the“rippleadder”,wherethevariablebaboveisthecarrybit. 1 def f(s):
2 a = 0;b = 0;c = 0;d = 0;e = 0;
3 ys = []
4.3.Performance 4 for i in range(20):
5 x = s[i]
AsseeninTable1,MIPSishighlycomplementarytoGPT-4
6 next_a = +x
Turbo: MIPSsolves32ofourtasks,including13thatare 7 next_b = a
notsolvedbyChatGPT-4(whichsolves30). 8 next_c = b
9 next_d = c
The AutoML process of Section 3.1 discovers networks 10 next_e = d
ofvaryingtask-dependentshapeandsize. Table2shows 11 a = next_a;b = next_b;c = next_c;d
theparameterspdiscoveredforeachtask. Acrossour62 = next_d;e = next_e;
tasks,16taskscouldbesolvedbyanetworkwithhidden 12 y = a+b+c+d+e
13 ys.append(y)
dimensionn = 1,andthelargestnrequiredwas81. For
14 return ys
manytasks,therewasaninterpretablemeaningtotheshape
of the smallest network we discovered. For instance, on
Figure4.ComparisonofcodegeneratedfromanRNNtrainedon
tasks where the output is the element occurring k steps
Sum Last5,with(top)andwithout(top)normalizers.Thewhiten-
earlier in the list, we found n = k+1, since the current
ingnormalizerprovidednumericalstabilitytotheJordannormal
elementandthepreviouskelementsmustbestoredforlater
formnormalizer,whichitselfsimplifiedtherecurrentportionof
recall.
theprogram.TheToeplitzandde-biasingnormalizersjointlyspar-
sifiedtheoccurrencesofxintheprogram,andthenumberofterms
WefoundtwomainfailuremodesforMIPS:
requiredtocomputey. Thequantizationnormalizerenabledall
variablestoberepresentedasintegers.
1. Noiseandnon-linearity. Thelatentspaceisstillclose
to being a finite state machine, but the non-linearity
and/ornoisepresentinanRNNissodominantthatthe
integerautoencoderfails,e.g.,forDiff Abs Values. Hu-
”ripple-carry adder” algorithm. The normalizers signifi-
manscanstareatthelookuptableandregressthesym-
cantlysimplifiedsomeoftheresultingprograms,asillus-
bolicfunctionwiththeirbrains,butsincethelookup
tratedinFig.4,andsometimesmadethedifferencebetween
table is not perfect, i.e., has the wrong integer in a
MIPS failing and succeeding. We found that applying a
fewexamples,MIPSfailstosymbolicallyregressthe
smallL1weightregularizationsometimesfacilitatedinte-
function. Thiscanprobablybemitigatedbylearning
gerautoencodingbyaxis-aligningthelattice.
andgeneralizingfromatrainingsubsetwithasmaller
dynamicrange.
5.Conclusions
2. Continuouscomputation.AkeyassumptionofMIPSis
thatRNNsarefinite-statemachines. However,RNNs WehavepresentedMIPS,anovelmethodforprogramsyn-
canalsousecontinuousvariablestorepresentinforma- thesisbasedonautomatedmechanisticinterpretabilityof
tion—theMajority 0 Xtasksfailforthisreason. This neuralnetworkstrainedtoperformthedesiredtask,auto-
can probably be mitigated by identifying and imple- distillingthelearnedalgorithmintoPythoncode.Itsessence
mentingfloating-pointvariables. istofirsttrainarecurrentneuralnetworktolearnaclever
finitestatemachinethatperformsthetask,andthenauto-
Figure 3 shows an example of a MIPS rediscovering the maticallyfigureouthowthismachineworks.
7Programsynthesisviamechanisticinterpretability
ConversationStart 5.2.Outlook
User:”Each Ourworkismerelyamodestfirstattemptatmechanistic-
rowinthetable
interpretability-basedprogramsynthesis,andtherearemany
belowcontains
twolists...giveme obvious generalizations worth trying in future work. For
aformulafor...”
example:
GPT:[Response]
1. Improvements in training and integer autoencoding
User:”Please (since many of our failed examples failed only just
writeaPython
programto...” barely)
GPT:[Response] 2. GeneralizationfromRNNstootherarchitecturessuch
astransformers
ExtractedCodeBlock
3. Generalization from bits and integers to more gen-
eralextractabledatatypessuchasfloating-pointnum-
bersandvariousdiscretemathematicalstructuresand
SuccessorFailure? knowledgerepresentations
4. Scalingtotasksrequiringmuchlargerneuralnetworks
Success Failure
5. Automated formal verification of synthesized pro-
Figure5.WecompareMIPSagainstprogramsynthesiswiththe
grams (we perform such verification with Dafny in
largelanguagemodelGPT-4Turbo,promptedwitha“chain-of-
Appendix F.1 to show that our MIPS-learned ripple
thought”approach. Itbeginswiththeuserprovidingatask,fol-
lowedbythemodel’sresponse,andculminatesinassessingthe addercorrectlyaddsanybinarynumbers,notmerely
successorfailureofthegeneratedPythoncodebasedonitsaccu- thoseinthetestset,butsuchmanualworkshouldide-
racyinprocessingtheprovidedlists. allybefullyautomated)
LLM-basedcodingco-pilotsarealreadyhighlyusefulfor
programsynthesistasksbasedonverbalproblemdescrip-
tions or auto-complete, and will only get better. MIPS
insteadtacklesprogramsynthesisbasedontestcasesalone.
5.1.Findings Thismakesitanalogoustosymbolicregression(Udrescu
etal.,2020;Cranmer,2023),whichhasalreadyprovenuse-
WefoundMIPShighlycomplementarytoLLM-basedpro-
fulforvariousscienceandengineeringapplications(Cran-
gramsynthesiswithGPT-4Turbo,witheachapproachsolv-
meretal.,2020;Maetal.,2022)whereonewishestoap-
ingmanytasksthatstumpedtheother.WhereasLLM-based
proximatedatarelationshipswithsymbolicformulae. The
methodshavetheadvantageofdrawinguponavastcorpus
MIPSframeworkgeneralizessymbolicregressionfromfeed-
ofhumantrainingdata,MIPShastheadvantageofdiscover-
forwardformulaetoprogramswithloops,whichareinprin-
ingalgorithmsfromscratchwithouthumanhints,withthe
cipleTuringcomplete. Ifthisapproachcanbescaledup,it
potentialtodiscoverentirelynewalgorithms. Asopposed
mayenablepromisingopportunitiesformakingmachine-
to genetic programming approaches, MIPS leverages the
learnedalgorithmsmoreinterpretable,verifiable,andtrust-
powerofdeeplearningbyexploitinggradientinformation.
worthy.
Programsynthesisaside,ourresultsshedfurtherlighton
mechanisticinterpretability,specificallyonhowneuralnet- BroaderImpact
worksrepresentbitsandintegers. Wefoundthatnintegers
tendtogetencodedlinearlyinndimensions,butgenerically Becausemachine-learnedalgorithmsnowoutperformtra-
innon-orthogonaldirectionswithanadditiveoffset. This ditionalhuman-discoveredalgorithmsonmanytasks,there
is presumably because there are many more such messy are incentives to deploy them even without a full under-
encodingsthansimpleones,andthemessinesscanbeeas- standingofhowtheyworkandofwhethertheyarebiased,
ily(linearly)decoded. Wesawthatnbitssometimesget unsafe,orotherwiseproblematic. Theaspirationalbroader
encodedasann-dimensionalparallelogram,butnotalways impactmotivatingthispaperistohelpautomatetheprocess
–––possiblybecauselineardecodabilityislesshelpfulwhen ofmakingAIsystemsmoretransparent,robust,andtrust-
thesubsequentbitoperationstobeperformedarenonlinear worthy,withtheultimategoalofdevelopingprovablysafe
anyway. AIsystems(Tegmark&Omohundro,2023).
8Programsynthesisviamechanisticinterpretability
Acknowledgements Cunningham, H., Ewart, A., Riggs, L., Huben, R., and
Sharkey, L. Sparse autoencoders find highly inter-
We thank Wes Gurnee, James Liu, and Armaun Sanayei
pretable features in language models. arXiv preprint
for helpful conversations and suggestions. This work is
arXiv:2309.08600,2023.
supportedbyErikOtto,JaanTallinn,theRothbergFamily
Fund for Cognitive Science, the NSF Graduate Research Goh,G.,†,N.C.,†,C.V.,Carter,S.,Petrov,M.,Schubert,
Fellowship(GrantNo. 2141064),andIAIFIthroughNSF L.,Radford,A.,andOlah,C. Multimodalneuronsinarti-
grantPHY-2019786. ficialneuralnetworks. Distill,2021. doi:10.23915/distill.
00030. https://distill.pub/2021/multimodal-neurons.
References
Gu, A. and Dao, T. Mamba: Linear-time sequence
Bills, S., Cammarata, N., Mossing, D., Tillman, H., modeling with selective state spaces. arXiv preprint
Gao, L., Goh, G., Sutskever, I., Leike, J., Wu, arXiv:2312.00752,2023.
J., and Saunders, W. Language models can
Gurnee,W.andTegmark,M. Languagemodelsrepresent
explain neurons in language models. https:
spaceandtime. arXivpreprintarXiv:2310.02207,2023.
//openaipublic.blob.core.windows.net/
neuron-explainer/paper/index.html, Hanna, M., Liu, O., and Variengien, A. How does gpt-2
2023. computegreater-than?: Interpretingmathematical abil-
ities in a pre-trained language model. arXiv preprint
Bricken,T.,Templeton,A.,Batson,J.,Chen,B.,Jermyn,A., arXiv:2305.00586,2023.
Conerly,T.,Turner,N.,Anil,C.,Denison,C.,Askell,A.,
Lasenby,R.,Wu,Y.,Kravec,S.,Schiefer,N.,Maxwell, Li, K., Hopkins, A. K., Bau, D., Vie´gas, F., Pfister, H.,
T.,Joseph,N.,Hatfield-Dodds,Z.,Tamkin,A.,Nguyen, and Wattenberg, M. Emergent world representations:
K., McLean, B., Burke, J. E., Hume, T., Carter, S., Exploringasequencemodeltrainedonasynthetictask.
Henighan,T.,andOlah,C. Towardsmonosemanticity: arXivpreprintarXiv:2210.13382,2022.
Decomposinglanguagemodelswithdictionarylearning.
Lindner, D., Krama´r, J., Farquhar, S., Rahtz, M., Mc-
TransformerCircuitsThread,2023. https://transformer-
Grath,T.,andMikulik,V. Tracr: Compiledtransform-
circuits.pub/2023/monosemantic-features/index.html.
ers as a laboratory for interpretability. arXiv preprint
arXiv:2301.05062,2023.
Burns,C.,Ye,H.,Klein,D.,andSteinhardt,J. Discovering
latentknowledgeinlanguagemodelswithoutsupervision. Liu, Z., Kitouni, O., Nolte, N., Michaud, E. J., Tegmark,
arXivpreprintarXiv:2212.03827,2022. M.,andWilliams,M. Towardsunderstandinggrokking:
An effective theory of representation learning. In Oh,
Cammarata, N., Goh, G., Carter, S., Schubert, L., A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
Petrov, M., and Olah, C. Curve detectors. Advances in Neural Information Processing Systems,
Distill, 2020. doi: 10.23915/distill.00024.003. 2022. URLhttps://openreview.net/forum?
https://distill.pub/2020/circuits/curve-detectors. id=6at6rB3IZm.
Charton, F. Can transformers learn the greatest common Ma,H.,Narayanaswamy,A.,Riley,P.,andLi,L. Evolving
divisor? arXivpreprintarXiv:2308.15594,2023. symbolicdensityfunctionals. ScienceAdvances,8(36):
eabq0279,2022.
Conmy,A.,Mavor-Parker,A.N.,Lynch,A.,Heimersheim,
Marks, S. and Tegmark, M. The geometry of truth:
S., andGarriga-Alonso, A. Towardsautomatedcircuit
Emergent linear structure in large language model
discoveryformechanisticinterpretability. arXivpreprint
representations of true/false datasets. arXiv preprint
arXiv:2304.14997,2023.
arXiv:2310.06824,2023.
Cranmer, M. Interpretable machine learning for science
McGrath, T., Kapishnikov, A., Tomasˇev, N., Pearce, A.,
with pysr and symbolicregression. jl. arXiv preprint
Wattenberg,M.,Hassabis,D.,Kim,B.,Paquet,U.,and
arXiv:2305.01582,2023.
Kramnik,V.Acquisitionofchessknowledgeinalphazero.
ProceedingsoftheNationalAcademyofSciences,119
Cranmer,M.,SanchezGonzalez,A.,Battaglia,P.,Xu,R.,
(47):e2206625119,2022.
Cranmer,K.,Spergel,D.,andHo,S. Discoveringsym-
bolicmodelsfromdeeplearningwith inductivebiases. Nanda, N., Chan, L., Liberum, T., Smith, J., and Stein-
AdvancesinNeuralInformationProcessingSystems,33: hardt,J. Progressmeasuresforgrokkingviamechanistic
17429–17442,2020. interpretability. arXivpreprintarXiv:2301.05217,2023.
9Programsynthesisviamechanisticinterpretability
Odena, A., Shi, K., Bieber, D., Singh, R., Sutton, C., Zhou,B.andDing,G. Surveyofintelligentprogramsyn-
and Dai, H. Bustle: Bottom-up program synthesis thesis techniques. In International Conference on Al-
through learning-guided exploration. arXiv preprint gorithms,HighPerformanceComputing,andArtificial
arXiv:2007.14381,2020. Intelligence(AHPCAI2023),volume12941,pp.1122–
1136.SPIE,2023.
Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov,
M.,andCarter,S. Zoomin: Anintroductiontocircuits.
Distill,5(3):e00024–001,2020.
Olsson,C.,Elhage,N.,Nanda,N.,Joseph,N.,DasSarma,
N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen,
A.,Conerly,T.,Drain,D.,Ganguli,D.,Hatfield-Dodds,
Z.,Hernandez,D.,Johnston,S.,Jones,A.,Kernion,J.,
Lovitt,L.,Ndousse,K.,Amodei,D.,Brown,T.,Clark,J.,
Kaplan,J.,McCandlish,S.,andOlah,C.In-contextlearn-
ingandinductionheads. TransformerCircuitsThread,
2022. https://transformer-circuits.pub/2022/in-context-
learning-and-induction-heads/index.html.
Quirke, P. et al. Understanding addition in transformers.
arXivpreprintarXiv:2310.13121,2023.
Sobania, D., Briesch, M., and Rothlauf, F. Choose your
programmingcopilot: Acomparisonoftheprogramsyn-
thesisperformanceofgithubcopilotandgeneticprogram-
ming. In Proceedings of the genetic and evolutionary
computationconference,pp.1019–1027,2022.
Syed,A.,Rager,C.,andConmy,A. Attributionpatching
outperformsautomatedcircuitdiscovery. arXivpreprint
arXiv:2310.10348,2023.
Tegmark, M. and Omohundro, S. Provably safe sys-
tems: theonlypathtocontrollableagi. arXivpreprint
arXiv:2309.01933,2023.
Toshniwal, S., Wiseman, S., Livescu, K., andGimpel, K.
Chessasatestbedforlanguagemodelstatetracking. In
ProceedingsoftheAAAIConferenceonArtificialIntelli-
gence,volume36,pp.11385–11393,2022.
Udrescu, S.-M., Tan, A., Feng, J., Neto, O., Wu, T., and
Tegmark,M.Aifeynman2.0:Pareto-optimalsymbolicre-
gressionexploitinggraphmodularity.AdvancesinNeural
InformationProcessingSystems,33:4860–4871,2020.
Wang, K., Variengien, A., Conmy, A., Shlegeris, B., and
Steinhardt, J. Interpretability in the wild: a circuit for
indirectobjectidentificationingpt-2small.arXivpreprint
arXiv:2211.00593,2022.
Wu,J.,Wei,L.,Jiang,Y.,Cheung,S.-C.,Ren,L.,andXu,C.
Programmingbyexamplemadeeasy. ACMTransactions
onSoftwareEngineeringandMethodology,33(1):1–36,
2023.
Zhong,Z.,Liu,Z.,Tegmark,M.,andAndreas,J. Theclock
andthepizza: Twostoriesinmechanisticexplanationof
neuralnetworks. arXivpreprintarXiv:2306.17844,2023.
10Programsynthesisviamechanisticinterpretability
A.Latticefindingusinggeneralizedgreatestcommondivisor(GCD)
Ourmethodoftenencounterscaseswherehiddenstatessecretlyformanaffinetransformationofanintegerlattice. However,
not all lattice points are observed in training samples, so our goal is to recover the hidden integer lattice from sparse
observations.
A.1.Problemformulation
SupposewehaveasetoflatticepointsinRD spannedbyDindependentbasisvectors,b (i=1,2,··· ,D). Eachlattice
i
pointj hastheposition
D
(cid:88)
x = a b +c, (5)
j ji i
i=1
wherecisaglobaltranslationvector,andthecoefficientsa areintegers
ji
Ourproblem: givenN suchdatapoints(x ,x ,··· ,x ),howcanwerecovertheintegercoefficientsa foreachpoint
1 2 N ji
datapointaswellasb andc?
i
Notethatevenwhenthewholelatticeisgiven, therearestilldegreesoffreedomforthesolution. Forexample, {c (cid:55)→
c+b ,a (cid:55)→a −1}remainsasolution,and{b
→(cid:80)D
Λ b }remainsasolutionifΛisanintegermatrixwhose
i ji ji i j=1 ij j
determinantis±1. Sooursuccesscriterionis: (1)a areintegers;(2)thediscoveredbasesandthetruebaseshavethesame
ji
determinant(thevolumeofaunitcell). Onceasetofbasesisfound,wecansimplifythembyminimizingtheirtotalnorms
overvalidtransformations(Λ∈ZD×D,det(Λ)=±1).
A.2.RegularGCD
Asareminder, givenalistofnnumbers{y ,y ,··· ,y }, acommondivisordisanumbersuchthatforalli, yi isan
1 2 n d
integer. Allcommondivisorsaretheset{d|y /d∈Z,andthegreatestcommondivisor(GCD)isthelargestnumberinthis
i
set. Because
GCD(y ,··· ,y )=GCD(y ,GCD(y ,GCD(y ,...))), (6)
1 n 1 2 3
itwithoutlossofgeneralitysufficestoconsiderthecasen=2. AcommonalgorithmtocomputeGCDoftwonumberisthe
so-calledEuclideanalgorithm. Westartwithtwonumbersr ,r andr >r ,whichisstep0. Forthekthstep,weperform
0 1 0 1
division-with-remaindertofindthequotientq andtheremainderr sothatr =q r +r with|r |>|r |1. The
k k k−2 k k−1 k k−1 k
algorithmwilleventuallyproduceazeroremainderr =0,andtheothernon-zeroremainderr isthegreatestcommon
N N−1
divisor. Forexample,GCD(55,45)=5,because
55=1×45+10,
45=4×10+5, (7)
10=2×5+0.
A.3.GeneralizedGCDinDdimensions
Givenalistofnvectors{y ,y ,··· ,y }wherey ∈RD,andassumingthatthesevectorsareinthelatticedescribedby
1 2 n i
Eq.(5),wecanwithoutlossofgeneralitysetc=0,sincewecanalwaysredefinetheorigin. InDdimensions,theprimitive
ofinterestistheD-dimensionalparallelogram: alinesegmentforD = 1(onebasisvector),aparallelogramforD = 2
(twobasisvectors),parallelepipedforD =3(threebasisvectors),etc.
OnecanconstructaD-dimensionalparallelogrambyconstructingitsbasisvectorsasalinearintegercombinationofy ,i.e.,
j
n
(cid:88)
q = m y ,m ∈Z,i=1,2,··· ,D. (8)
i ij j ij
j=1
ThegoalofD-dimensionalGCDistofinda“minimal”parallelogram,suchthatitsvolume(whichisdet(q ,q ,··· ,q ))
1 2 D
1Weareconsideringageneralcasewherer andr maybenegative.Otherwiser canalwaysbepositivenumbers,hencenoneedto
0 1 k
usetheabsolutefunction.
11Programsynthesisviamechanisticinterpretability
Figure6.Bothredandbluebasisformaminimalparallelogram(intermsofcellvolume),butonecanfurthersimplifyredtoblueby
linearcombination(simplicityinthesenseofsmallℓ norm).
2
isGCDofvolumesofotherpossibleparallelograms. Oncetheminimalparallelogramisfound2,wecanalsodetermineb
i
inEq.(5),sinceb areexactlyq ! Tofindtheminimalparallelogram,weneedtwosteps: (1)figureouttheunitvolume;(2)
i i
figureoutq (i=1,2,···)whosevolumeistheunitvolume.
i
Step 1: Compute unit volume V . We first define representative parallelograms as one where all i = 1,2,··· ,D,
0
m ≡(m ,m ,··· ,m )areone-hotvectors,i.e.,withonlyoneelementbeing1and0otherwise. Itiseasytoshowthat
i i1 i2 iD
thevolumeofanyparallelogramisalinearintegercombinationofvolumesofrepresentativeparallelograms,soWLOGwe
canfocusonrepresentativeparallelograms. Wecomputethevolumesofallrepresentativeparallelograms,whichgivesa
volumearray. Sincevolumesarejustscalars,wecangettheunitvolumeV bycallingtheregularGCDofthevolumearray.
0
Step2: Findaminimalparallelogram(whosevolumeistheunitvolumecomputedinstep1). Recallthatinregular
GCD,wearedealingwithtwonumbers(scalars). Toleveragethisinthevectorcase,weneedtocreatescalarsoutofvectors,
andmakesurethatthevectorssharethesamelinearstructureasthescalarssothatwecanextenddivision-and-remainder
tovectors. Anaturalscalarisvolume. NowconsidertwoparallelogramsP1andP2, whichshareD−1basisvectors
(y ,...,y ),butlastbasisvectorisdifferent: y forP1andy forP2. DenotetheirvolumeasV andV :
3 D+1 1 2 1 2
V =det(y ,y ,y ,...,y )
1 1 3 4 D
(9)
V =det(y ,y ,y ,...,y )
2 2 3 4 D
Since
aV +bV =det(ay +by ,y ,y ,...,y ), (10)
1 2 1 2 3 4 D
whichshowsthat(V ,V )and(y ,y )sharethesamelinearstructure. Wecansimplyapplydivision-and-remaindertoV
1 2 1 2 1
andV asinregularGCD:
2
V′,V′ =GCD(V ,V ), (11)
1 2 1 2
whosequotientsinalliterationsaresavedandtransferredtoy andy :
1 2
y′,y′ =GCD with predefined quotients(y ,y ). (12)
1 2 1 2
IfV =V (whichistheconditionforminimalparallelogram),thealgorithmterminatesandreturns(y′,y ,y ,··· ,y ).
1 0 1 3 4 D
IfV >V ,weneedtorepeatstep2withthenewvectorlist{y′,y ,··· ,y }.
1 0 1 3 N
Whycanweremovey′ fornextiteration? NotethatalthougheventuallyV′ >0andV′ =0,typicallyy ̸=0. However,
2 1 2 2
since
0=V′ =det(y′,y ,y ,··· ,y ), (13)
2 2 3 4 D
thismeansy′ isalinearcombinationof(y ,··· ,y ),hencecanberemovedfromthevectorlist.
2 3 D
Step3: Simplificationofbasisvectors. Wewanttofurthersimplifybasisvectors. Forexample,thebasisvectorsobtained
instep2mayhavelargenorms. ForexampleD =2,thestandardintegerlatticehasb =(1,0)andb =(0,1),butthey
1 2
2Therecouldbemanyminimalparallelograms,butfindingoneissufficient.
12Programsynthesisviamechanisticinterpretability
areinfinitelymanypossibilitiesafterstep2,aslongaspt−sq =±1forb =(p,q)andb =(s,t),e.g.,b =(3,5)and
1 2 1
b =(4,7).
2
Tominimizeℓ norms,wechooseabasisandproject-and-subtractforotherbases.Notethat:(1)againweareonlyallowedto
2
subtractintegertimesofthechosenbasis;(2)thevolumeoftheparallelogramdoesnotchangesincetheproject-and-subtract
matrixhasdeterminant1(supposeb (i=2,3,··· ,D)areprojectedtob andsubtractedbymultiplesofb . p represents
i 1 1 ∗
projectionintegers):
 
1 p p ··· p
2→1 3→1 D→1
0 1 0 ··· 0 
 
0 0 1 ··· 0  (14)
. . . . 
. . . . 
. . . . 
0 0 0 ··· 1
Wedothisiteratively,untilnonormcanbecomeshorterviaproject-and-subtract. PleaseseeFigure6foranillustrationof
howsimplificationworksfora2Dexample.
Computationoverheadisactuallysurprisinglysmall. Intypicalcases,weonlyneedtocallO(1)timesofGCD.
DealingwithnoiseUsuallytheintegerlatticeinthehiddenspaceisnotperfect,i.e.,vulnerabletonoise. Howdoweextract
integerlatticesinarobustwayinthepresenceofnoise? NotethattheterminatingconditionfortheGCDalgorithmis
whentheremainderisexactlyzero-werelaxthisconditiontothattheabsolutevalueoftheremaindertobesmallerthana
thresholdϵ . AnotherissueregardingnoiseisthatnoisecanbeaccumulatedintheGCDiterations,sowehopethatGCD
gcd
canconvergeinafewsteps. Toachievethis,weselecthiddenstatesinasmallregionwithdatafractionp%ofthewhole
data. Sincebothϵ andpdependsondataandneuralnetworktrainingwhichwedonotknowapriori,wechoosetogrid
gcd
sweepϵ ∈[10−3,1]andp∈(0.1,100);foreach(ϵ ,p),weobtainanintegerlatticeandcomputeitsdescriptionlength.
gcd gcd
Weselectthe(ϵ ,p)whichgivesthelatticewiththesmallestdescriptionlength. Thedescriptionlengthincludestwoparts:
gcd
integerdescriptionsofhiddenstateslog(1+|Z|2),andresidualofreconstructionlog(1+(AZ+b−X)2)withϵ =10−4.
ϵdl dl
B.Linearlatticefinder
AlthoughourRNNcanrepresentgeneralnonlinearfunctions,inthespecialcasewhentheRNNactuallyperformslinear
functions,programsynthesiscanbemucheasier. SoifthehiddenMLPislinear,wewouldexpectthehiddenstatestobean
integerlattice,becauseinputsareintegerlatticesandthemappingsarelinear. EffectivelythehiddenMLPworksasalinear
function: h(t) =W h(t−1)+W x(t)(weneglectedthebiastermsinceitisnotrelevanttofindingbasisvectorsofalattice).
h i
Supposewehaveinputseriesx(1),x(2),...,x(t),thenh(t)is
t
(cid:88)
h(t) = Wt−jW x , (15)
h i j
j=1
Sincex themselvesareintegerlattices,wecouldtheninterpretthefollowingasbasisvectors:
j
Wt−jW ,j =1,2,··· ,t, (16)
h i
whicharenotnecessarilyindependent. Forexample,forthetaskofsummingupthelasttwonumbers,W W andW are
h i i
non-zerovectorsandareindependent,whileothersWnW ≈0,n≥2. ThenW W andW arethetwobasisvectorsfor
h i h i i
thelattice. Ingeneral,wemeasurethenormofallthecandidatebasisvectors,andselectthefirstkvectorswithhighest
norms,whichareexactlybasisvectorsofthehiddenlattice.
C.Symbolicregression
Theformulationofsymbolicregressionisthatonehasdatapair(x ,y ),i=1,2,...,N withN datasamples. Thegoalis
i i
tofindasymbolicformulaf suchthaty =f(x ). Afunctionisexpressedinreversepolishnotation(RPN),forexample,
i i
|a|−cisexpressedasaAc-whereAstandsfortheabsolutevaluefunction. Wehavethreetypesofvariables:
13Programsynthesisviamechanisticinterpretability
• type-0operator. Weincludeinputvariablesandconstants.
• type-1operator(takesinonetype-0toproduceonetype-0). Weincludeoperations{>,<,∼,H,D,A}. >means+1;
<means−1;∼meansnegatingthenumber;Disdiracdeltawhichoutputs1onlywhentakingin0;Aistheabsolute
valuefunction;
• type-2operator(takesintwotype-0toproduceontype-0). Weincludeoperations{+,∗,−,%}. +meansadditionof
twonumbers;∗meansmultiplicationoftwonumbers;−meanssubtractionoftwonumbers;%istheremainderofone
numbermoduletheother.
Thereareonlycertaintypesoftemplates(astringofnumbersconsistingof0,1,2)thataresyntacticallycorrect. Forexample,
002iscorrectwhile02isincorrect. Weiterateoverallthetemplatesnotlongerthan6symbols,andforeachtemplate,we
tryallthevariablecombinations. Eachvariablecombinationcorrespondstoasymbolicequationf,forwhichwecancheck
whetherf(x )=y for100datapoints. Ifsuccess,weterminatethebruteforceprogramandreturnthesuccessfulformula.
i i
Ifbruteforcesearchdoesnotfindanycorrectsymbolicformulawithincomputebudget,wewillsimplyreturntheformulaa,
tomakesurethattheprogramcanstillbesynthesizedbutsimplyfailtomakecorrectpredictions.
D.NeuralNetworkNormalizationAlgorithms
Itiswellknownthatneuralnetworksexhibitalargeamountofsymmetry. Thatis,therearemanytransformationsthatcan
beappliedtonetworkswithoutaffectingthemapy =f(x)thattheycompute. Aclassicexampleistopermutetheneurons
withinlayers.
Inthissection,wedescribeasuiteofnormalizersthatweusetotransformournetworksintoastandardform,suchthatthe
algorithmsthattheylearnareeasiertointerpret. Wecallourfivenormalizers“Whitening”,“Jordannormalform(JNF)”,
“Toeplitz”,“De-bias”,and“Quantization”.
Themainsymmetrywhichwefocusonisalineartransformationofthehiddenspaceh(cid:55)→Ah,whichrequiresthefollowing
changestof andg:
f(h,x)=Wh+Vx+b =⇒f(h,x)=AWA−1h+AVx+Ab
g(h)=G(Uh+c) =⇒f(h)=G(UA−1h+c)
andisimplementedbychangingtheweights:
W =⇒AWA−1
V =⇒AV
b =⇒Ab
U =⇒UA−1
Forthissymmetry,wecanapplyanarbitraryinvertiblesimilaritytransformationAtoW,whichisthecoreideaunderlying
ournormalizers,threeofwhichhavetheirownuniquewaysofconstructingA,aswedescribeinthesectionsbelow. Most
importantly, one of our normalizers exploits A to convert the hidden-to-hidden transformation W into Jordan normal
form, inthecasewheref islinear. Recentworkhasshownthatlargerecurrentnetworkswithlinearhidden-to-hidden
transformations,suchasstatespacemodels(Gu&Dao,2023)canperformjustaswellastransformer-basedmodelsin
languagemodelingonalargescale. Amainadvantageofusinglinearhidden-to-hiddentransformationsisthepossibilityof
expressingthehiddenspaceinit’seigenbasis. Thiscausesthehidden-to-hiddentransformationtobecomediagonal,sothat
itcanbecomputedmoreefficiently. Inpractice,modernstatespacemodelsassumediagonality,andgofurthertoassume
theelementsonthediagonalarereal;theyfixthearchitecturetobethiswayduringtraining.
Bydoingthis,weignorethepossibilityoflinearhidden-to-hiddentransformationsthatcannotbetransformedintoareal
diagonalmatrixviadiagonalization. Suchexamplesincluderotationmatrices(whoseeigenvaluesmaybecomplex),and
shiftmatrices(whoseeigenvaluesaredegenerateandwhoseeigenvectorsareduplicated). Amoregeneralformthanthe
diagonalformistheJordannormalform,whichconsistsofJordanblocksalongthediagonal,eachofwhichhastheform
λI+SforaneigenvalueλandtheshiftmatrixSwithonesonthesuperdiagonalandzeroselsewhere. Thediagonalization
14Programsynthesisviamechanisticinterpretability
isaspecialcaseofJordannormalform,andallmatricescanbetransformedtoJordannormalform. Asimpletransformation
canalsobeappliedtoJordannormalformsthatcontainpairsofcomplexgeneralizedeigenvectors,toconvertthemintoreal
matrices.
Fornonlinearhidden-to-hiddentransformations,wecomputeWasthoughthenonlinearitieshavebeenremoved.
D.1.WhiteningTransformation
Similartonormalizingthemeansandvariancesofadatasetbeforefeedingitintoamachinelearningmodel,agoodfirst
preprocessingstepistonormalizethedistributionofhiddenstates. Wethereforechoosetoapplyawhiteningtransformation
tothehiddenspace. Tocomputethetransformation,wecomputethecovariancematrixofhiddenactivationsacrossthe
dataset,andusethesingularvaluedecomposition(SVD)ofthiscovariancematrixtofindtheclosesttransformationtothe
identitythatwillbringthiscovariancematrixtotheidentity. Weignoreanydirectionswithcovariancelessthanϵ=0.1,
which cause more instability when normalized. We then post-apply this transformation to the last linear layer of the
hidden-to-hiddentransformationanditsbiases,andpre-applyitsinversetothefirstlayersofthehidden-to-hiddenand
hidden-to-outputtransformations. Thisleavesthenetbehaviorofthenetworkunchanged. Othertransformationswhichwe
useinothernormalizersoperateinasimilarmanner,bypost-applyingandpre-applyingatransformationanditsinverse
transformationtothefirstandlastlayersthatinteractwiththehiddenspace.
D.2.JordanNormalFormTransformation
Critically,thehidden-to-hiddentransformationswhichwewouldliketoconvertintoJordannormalformareimperfect
because they are learned. Eigenvectors belonging to each Jordan block must be identical, whereas this will only be
approximatelytrueofthelearnedtransformation.
TheJordannormalformofamatrixisunstable;consideramatrix
(cid:18) (cid:19)
0 1
W=
δ 0
which,whenδ ̸=0,canbetransformedintoJordannormalformby:
√
(cid:18) (cid:19) (cid:18) (cid:19)(cid:18) (cid:19)(cid:18) (cid:19)−1
0 1 1 1 δ 0 1 1
= √ √ √ √ √ (17)
δ 0 δ − δ 0 − δ δ − δ
(cid:124) (cid:123)(cid:122) (cid:125)
T
butwhenδ =0,istransformedintoJordannormalformby:
(cid:18) (cid:19) (cid:18) (cid:19)(cid:18) (cid:19)(cid:18) (cid:19)−1
0 1 1 0 0 1 1 0
= (18)
0 0 0 1 0 0 0 1
(cid:124) (cid:123)(cid:122) (cid:125)
T
Aswecansee,allofthematricesinthedecompositionareunstablenearδ =0,sotheissueoferrorthresholdingisnotonly
numerical,butismathematicalinnatureaswell.
WewouldliketoconstructanalgorithmwhichcomputestheJordannormalformwithanerrorthreshold|δ| < ϵ = 0.7
withinwhichthealgorithmwillpickthetransformationTfromEquation(18)insteadoffromEquation(17).
Ouralgorithmfirstcomputestheeigenvaluesλ ,andtheniterativelysolvesforthegeneralizedeigenvectorswhichliein
i
ker((W−λI)k)forincreasingk. Theapproximationoccurswheneverwecomputethekernel(ofunknowndimension)ofa
matrixX;wetaketheSVDofXandtreatanysingularvectorsaspartofthenullspaceiftheirsingularvaluesarelowerthan
thethresholdϵ,callingtheresultϵ-ker(X).
SpacesarealwaysstoredintheformofarectangularmatrixFoforthonormalvectors,andtheirdimensionisalwaysthe
widthofthematrix. Webuildprojectionsusingproj(F) = FFH,whereFH denotestheconjugatetransposeofF. We
computekernelsker(X)ofknowndimensionofmatricesXbytakingtheSVDX=V SVH andtakingthelastsingular
1 2
vectorsinVH. WecomputecolumnspacesofprojectorsofknowndimensionbytakingthetopsingularvectorsoftheSVD.
2
Thestepsinouralgorithmareasfollows:
15Programsynthesisviamechanisticinterpretability
1. Solvefortheeigenvaluesλ ofW,andcheckthateigenvaluesthatarewithinϵofeachotherformgroups,ie. that
i
|λ −λ |≤ϵand|λ −λ |≤ϵalwaysimplies|λ −λ |≤ϵ. Computethemeaneigenvalueforeverygroup.
i j j k k i
2. Solve for the approximate kernels of W − λI for each mean eigenvalue λ. We will denote this operation by
ϵ-ker(W−λI). Werepresentthesekernelsbystoringthesingularvectorswhosesingularvaluesarelowerthanϵ.
Also,constructa“correctedmatrix”ofW−λIforeveryλbytakingtheSVD,discardingthelowsingularvalues,and
multiplyingthepruneddecompositionbacktogetheragain.
3. SolveforsuccessivespacesF ofgeneralizedeigenvectorsatincreasingdepthskalongthesetofJordanchainswith
k
eigenvalueλ,forallλ. Inotherwords,findchainsofmutuallyorthogonalvectorswhicharemappedtozeroafter
exactlykapplicationsofthemapW−λI. WefirstsolveforF =ker(W−λI). Thenfork >0,wefirstsolvefor
0
J =ϵ-ker((I−proj(F ))(W−λI))anddeducethenumberofchainswhichreachdepthkfromthedimensionof
k k−1
J ,andthensolveforF =col(proj(J )−proj(F )).
k k k 0
4. PerformaconsistencychecktoverifythatthedimensionsofF alwaysstaythesameordecreasewithk. Gothrough
k
thespacesF inreverseorder,andwheneverthedimensionofF decreases,figureoutwhichdirection(s)arenot
k k
mappedtobyapplyingW−λItoF . DothisbybuildingaprojectorJfrommappingvectorsrepresentingF
k+1 k+1
throughW−λI,andtakingcol(proj(F )−J). SolvefortheJordanchainbyrepeatedlyapplyingproj(F )(W −λI)
k i i
foristartingfromk−1andgoingallthewaydowntozero.
5. ConcatenatealltheJordanchainstogethertoformthetransformationmatrixT.
ThetransformationTconsistsofgeneralizedeigenvectorswhichneednotbecompletelyrealbutmayalsoincludepairsof
generalizedeigenvectorsthatarecomplexconjugatesofeachother. Sincewedonotwanttheweightsofournormalized
networktobecomplex,wealsoapplyaunitarytransformationwhichchangesanypairofcomplexgeneralizedeigenvectors
intoapairofrealvectors,andtheresultingblockofWintoamultipleofarotationmatrix. Asanexample,forareal2by2
matrixWwithcomplexeigenvectors,wehave
(cid:18) (cid:19)
a+bi 0
W=T T−1
0 a−bi
(cid:18) (cid:19) (cid:18) (cid:19)
a −b 1 1 i
=TT′ (TT′)−1, T′ = √
b a 2 1 −i
D.3.ToeplitzTransformation
OnceWisinJordannormalform,eachJordanblockisanuppertriangularToeplitzmatrix. Upper-triangularToeplitz
matrices,includingJordanblocks,willalwayscommutewitheachother,becausetheyareallpolynomialsoftheshiftmatrix
(whichhasonesonthesuperdiagonalandzeroselsewhere,)andthereforethesetransformationswillleaveWunchanged,
butwillstillaffectV. WesplitVupintopartsoperatedonbyeachJordanblock,andusetheseToeplitztransformationsto
reducethemostnumericallystablecolumnsofeachblockofVtoone-hotvectors. Thenumericallystabilityofacolumn
vectorisdeterminedbytheabsolutevalueofthebottomelementofthatcolumnvector,sinceit’sinversewillbecomethe
degenerateeigenvaluesoftheresultingToeplitzmatrix. Ifnocolumnhasanumericallystabilityaboveϵ=0.0001,wepick
theidentitymatrixforourToeplitztransformation.
D.4.De-biasingTransformation
Oftentimes,Wisnotfullrank,andhasanontrivialnullspace. Thebiasbwillhavesomecomponentinthedirectionofthis
nullspace,andeliminatingthiscomponentonlyaffectsthebehavioroftheoutputnetworkg,andtheperturabtioncannot
carryontotheremainderofthesequenceviaf. Therefore,weeliminateanysuchcomponent,andcompensateaccordingly
bymodifyingthebiasinthefirstaffinelayerofg. WeidentifythenullspacesbytakinganSVDandidentifyingcomponents
whosesingularvalueislessthanϵ=0.1.
D.5.QuantizationTransformation
AfterapplyingalloftheprevioustransformationstotheRNN,itiscommonformanyoftheweightstobecomecloseto
zeroorsomeothersmallinteger. Treatingthisasasignthatthenetworkisattemptingtoimplementdiscreteoperations
usingintegers,wesnapanyweightsandbiasesthatarewithinathresholdϵ=0.01ofaninteger,tothatinteger. Forcertain
simpletasks,sometimesthisallowstheentirenetworktobecomequantized.
16Programsynthesisviamechanisticinterpretability
E.Supplementarytrainingdatadetails
Herewepresentadditionaldetailsonthebenchmarktasksmarked”seetext”inTable1:
• Div 3/5/7: Thisisalongdivisiontaskforbinarynumbers. Theinputisabinarynumber,andtheoutputisthatbinary
numberdividedby3,5,or7,respectively. Theremainderisdiscarded. Forexample,wehave1000011/11=0010110
(67/3=22). Themostsignificantbitsoccurfirstinthesequence.
• Dithering: Thisisabasicimagecolorquantizationtask,for1Dimages. Wemap4-bitimagesto1-bitimagessuch
thatthecumulativesumofpixelbrightnessesofboththeoriginalandditheredimagesremainsascloseaspossible.
• Newton Gravity: ThisisaneulerforwardpropagationtechniquewhichfollowstheequationF = input−1,v (cid:55)→
v+F,x(cid:55)→x+v.
• Newton Spring: ThisisaneulerforwardpropagationtechniquewhichfollowstheequationF = input−x,v (cid:55)→
v+F,x(cid:55)→x+v.
• Newton Magnetic:ThisisaneulerforwardpropagationtechniquewhichfollowstheequationF =input −v ,F =
x 1 y y
input +v ,v(cid:55)→v+F,x(cid:55)→x+v.
2 x
F.Generatedprograms
ThissectionincludesallsuccessfullygeneratedPythonprograms.
Binary-Addition Bitwise-Or
1 1
2 def f(s,t): 2 def f(s,t):
3 a = 0;b = 0; 3 a = 0;
4 ys = [] 4 ys = []
5 for i in range(10): 5 for i in range(10):
6 c = s[i]; d = t[i]; 6 b = s[i]; c = t[i];
7 next_a = b ˆ c ˆ d 7 next_a = b+c>0
8 next_b = b+c+d>1 8 a = next_a;
9 a = next_a;b = next_b; 9 y = a
10 y = a 10 ys.append(y)
11 ys.append(y) 11 return ys
12 return ys
Bitwise-And
Bitwise-Xor
1
1 2 def f(s,t):
2 def f(s,t): 3 a = 0;b = 1;
3 a = 0; 4 ys = []
4 ys = [] 5 for i in range(10):
5 for i in range(10): 6 c = s[i]; d = t[i];
6 b = s[i]; c = t[i]; 7 next_a = (not a and not b and c and
7 next_a = b ˆ c d) or (not a and b and not c and d) or
8 a = next_a; (not a and b and c and not d) or (not
9 y = a a and b and c and d) or (a and not b
10 ys.append(y) and c and d) or (a and b and c and d)
11 return ys 8 next_b = c+d==0 or c+d==2
9 a = next_a;b = next_b;
10 y = a+b>1
11 ys.append(y)
12 return ys
17Programsynthesisviamechanisticinterpretability
Bitwise-Not Parity-Zeros
1 1
2 def f(s): 2 def f(s):
3 a = 1; 3 a = 0;
4 ys = [] 4 ys = []
5 for i in range(10): 5 for i in range(10):
6 x = s[i] 6 b = s[i]
7 next_a = x 7 next_a = a+b==0 or a+b==2
8 a = next_a; 8 a = next_a;
9 y = -a+1 9 y = a
10 ys.append(y) 10 ys.append(y)
11 return ys 11 return ys
Parity-Last2 Sum-All
1 1
2 def f(s): 2 def f(s):
3 a = 0;b = 0; 3 a = 884;
4 ys = [] 4 ys = []
5 for i in range(10): 5 for i in range(10):
6 c = s[i] 6 x = s[i]
7 next_a = c 7 next_a = a-x
8 next_b = a ˆ c 8 a = next_a;
9 a = next_a;b = next_b; 9 y = -a+884
10 y = b 10 ys.append(y)
11 ys.append(y) 11 return ys
12 return ys
Sum-Last2
Parity-Last3
1
1 2 def f(s):
2 def f(s): 3 a = 0;b = 99;
3 a = 0;b = 0;c = 0; 4 ys = []
4 ys = [] 5 for i in range(10):
5 for i in range(10): 6 x = s[i]
6 d = s[i] 7 next_a = -b+x+99
7 next_a = d 8 next_b = -x+99
8 next_b = c 9 a = next_a;b = next_b;
9 next_c = a 10 y = a
10 a = next_a;b = next_b;c = next_c; 11 ys.append(y)
11 y = a ˆ b ˆ c 12 return ys
12 ys.append(y)
13 return ys Sum-Last3
Parity-All 1
2 def f(s):
1 3 a = 0;b = 198;c = 0;
2 def f(s): 4 ys = []
3 a = 0; 5 for i in range(10):
4 ys = [] 6 x = s[i]
5 for i in range(10): 7 next_a = x
6 b = s[i] 8 next_b = -a-x+198
7 next_a = a ˆ b 9 next_c = -b+198
8 a = next_a; 10 a = next_a;b = next_b;c = next_c;
9 y = a 11 y = a+c
10 ys.append(y) 12 ys.append(y)
11 return ys 13 return ys
18Programsynthesisviamechanisticinterpretability
Sum-Last4 Sum-Last7
1 1
2 def f(s): 2 def f(s):
3 a = 0;b = 99;c = 0;d = 99; 3 a = 297;b = 198;c = 0;d = 99;e = 0;f =
4 ys = [] -15;g = 0;
5 for i in range(10): 4 ys = []
6 x = s[i] 5 for i in range(20):
7 next_a = c 6 x = s[i]
8 next_b = -x+99 7 next_a = -a+d-f+g+480
9 next_c = -b-d+198 8 next_b = a-d
10 next_d = b 9 next_c = d+e-99
11 a = next_a;b = next_b;c = next_c;d 10 next_d = -c+99
= next_d; 11 next_e = -b+198
12 y = a-b-d+198 12 next_f = -c+f+x
13 ys.append(y) 13 next_g = x
14 return ys 14 a = next_a;b = next_b;c = next_c;d
= next_d;e = next_e;f = next_f;g =
Sum-Last5 next_g;
15 y = -d+f+114
1 16 ys.append(y)
2 def f(s): 17 return ys
3 a = 198;b = -10;c = -2;d = 482;e = 1;
4 ys = [] Current-Number
5 for i in range(20):
6 x = s[i] 1
7 next_a = -b+c+190 2 def f(s):
8 next_b = b-c-d-e+x+480 3 a = 99;
9 next_c = b-e+8 4 ys = []
10 next_d = -b+e-x+472 5 for i in range(10):
11 next_e = a+b-e-187 6 x = s[i]
12 a = next_a;b = next_b;c = next_c;d 7 next_a = -x+99
= next_d;e = next_e; 8 a = next_a;
13 y = -d+483 9 y = -a+99
14 ys.append(y) 10 ys.append(y)
15 return ys 11 return ys
Sum-Last6 Prev1
1 1
2 def f(s): 2 def f(s):
3 a = 0;b = 295;c = 99;d = 0;e = 297;f = 3 a = 0;b = 99;
99; 4 ys = []
4 ys = [] 5 for i in range(10):
5 for i in range(20): 6 x = s[i]
6 x = s[i] 7 next_a = -b+99
7 next_a = -b+295 8 next_b = -x+99
8 next_b = b-c+f 9 a = next_a;b = next_b;
9 next_c = b-c+d-97 10 y = a
10 next_d = -f+99 11 ys.append(y)
11 next_e = -a+297 12 return ys
12 next_f = -x+99
13 a = next_a;b = next_b;c = next_c;d
= next_d;e = next_e;f = next_f;
14 y = -b+c-e-f+592
15 ys.append(y)
16 return ys
19Programsynthesisviamechanisticinterpretability
Prev2 Prev5
1 1
2 def f(s): 2 def f(s):
3 a = 99;b = 0;c = 0; 3 a = 0;b = 0;c = 99;d = 99;e = 99;f =
4 ys = [] 99;
5 for i in range(10): 4 ys = []
6 x = s[i] 5 for i in range(20):
7 next_a = -x+99 6 x = s[i]
8 next_b = -a+99 7 next_a = -c+99
9 next_c = b 8 next_b = -d+99
10 a = next_a;b = next_b;c = next_c; 9 next_c = -b+99
11 y = c 10 next_d = e
12 ys.append(y) 11 next_e = f
13 return ys 12 next_f = -x+99
13 a = next_a;b = next_b;c = next_c;d
Prev3 = next_d;e = next_e;f = next_f;
14 y = a
1 15 ys.append(y)
2 def f(s): 16 return ys
3 a = 0;b = 0;c = 99;d = 99;
4 ys = [] Previous-Equals-Current
5 for i in range(10):
6 x = s[i] 1
7 next_a = b 2 def f(s):
8 next_b = -c+99 3 a = 0;b = 0;
9 next_c = d 4 ys = []
10 next_d = -x+99 5 for i in range(10):
11 a = next_a;b = next_b;c = next_c;d 6 c = s[i]
= next_d; 7 next_a = delta(c-b)
12 y = a 8 next_b = c
13 ys.append(y) 9 a = next_a;b = next_b;
14 return ys 10 y = a
11 ys.append(y)
Prev4 12 return ys
1 Diff-Last2
2 def f(s):
3 a = 0;b = 99;c = 0;d = 99;e = 0; 1
4 ys = [] 2 def f(s):
5 for i in range(10): 3 a = 199;b = 100;
6 x = s[i] 4 ys = []
7 next_a = c 5 for i in range(10):
8 next_b = -a+99 6 x = s[i]
9 next_c = -d+99 7 next_a = -a-b+x+498
10 next_d = -e+99 8 next_b = a+b-199
11 next_e = x 9 a = next_a;b = next_b;
12 a = next_a;b = next_b;c = next_c;d 10 y = a-199
= next_d;e = next_e; 11 ys.append(y)
13 y = -b+99 12 return ys
14 ys.append(y)
15 return ys Abs-Diff
1
2 def f(s):
3 a = 100;b = 100;
4 ys = []
5 for i in range(10):
6 c = s[i]
7 next_a = b
8 next_b = c+100
9 a = next_a;b = next_b;
10 y = abs(b-a)
11 ys.append(y)
12 return ys
20Programsynthesisviamechanisticinterpretability
Abs-Current Newton-Freebody
1 1
2 def f(s): 2 def f(s):
3 a = 0; 3 a = 82;b = 393;
4 ys = [] 4 ys = []
5 for i in range(10): 5 for i in range(10):
6 b = s[i] 6 x = s[i]
7 next_a = abs(b) 7 next_a = a-x
8 a = next_a; 8 next_b = -a+b+82
9 y = a 9 a = next_a;b = next_b;
10 ys.append(y) 10 y = -a+b-311
11 return ys 11 ys.append(y)
12 return ys
Bit-Shift-Right
Newton-Gravity
1
2 def f(s): 1
3 a = 0;b = 1; 2 def f(s):
4 ys = [] 3 a = 72;b = 513;
5 for i in range(10): 4 ys = []
6 x = s[i] 5 for i in range(10):
7 next_a = -b+1 6 x = s[i]
8 next_b = -x+1 7 next_a = a-x+1
9 a = next_a;b = next_b; 8 next_b = -a+b+x+71
10 y = a 9 a = next_a;b = next_b;
11 ys.append(y) 10 y = b-513
12 return ys 11 ys.append(y)
12 return ys
Bit-Dot-Prod-Mod2
Newton-Spring
1
2 def f(s,t): 1
3 a = 0; 2 def f(s):
4 ys = [] 3 a = 64;b = 57;
5 for i in range(10): 4 ys = []
6 b = s[i]; c = t[i]; 5 for i in range(10):
7 next_a = (not a and b and c) or (a 6 x = s[i]
and not b and not c) or (a and not b 7 next_a = a+b-x-57
and c) or (a and b and not c) 8 next_b = -a+121
8 a = next_a; 9 a = next_a;b = next_b;
9 y = a 10 y = -a+64
10 ys.append(y) 11 ys.append(y)
11 return ys 12 return ys
Add-Mod-3
1
2 def f(s):
3 a = 0;
4 ys = []
5 for i in range(10):
6 b = s[i]
7 next_a = (b+a)%3
8 a = next_a;
9 y = a
10 ys.append(y)
11 return ys
F.1.FormalVerification
The Dafny programming language is designed so that programs can be formally verified for correctness. The desired
behaviorofaprogramcanbeexplicitlyspecifiedviapreconditions,postconditions,andinvariants,whichareverifiedvia
automatedtheoremproving. ThesecapabilitiesmakeDafnyusefulinfieldswherecorrectnessandsafetyarecrucial.
21Programsynthesisviamechanisticinterpretability
WeleverageDafny’srobustverificationcapabilitiestoprovethecorrectnessofthebitadditionPythonprogramsynthesizedby
MIPS.ThebitadditionPythonprogramwasfirstconvertedtoDafny,thenannotatedwithspecificassertions,preconditions,
andpostconditionsthatdefinedtheexpectedbehaviorofthecode. Eachannotationinthecodewasthenformallyverifiedby
Dafny,ensuringthatunderallpossiblevalidinputs,thecode’soutputwouldbeconsistentwiththeexpectedbehavior. On
line79,weshowthatthealgorithmfoundbyMIPSisindeedequivalenttoperformingbitadditionwithlength10bitvectors
inDafny.
Dafny-Code
1
2 function ArrayToBv10(arr: array<bool>): bv10 // Converts boolean array to bitvector
3 reads arr
4 requires arr.Length == 10
5 {
6 ArrayToBv10Helper(arr, arr.Length - 1)
7 }
8
9 function ArrayToBv10Helper(arr: array<bool>, index: nat): bv10
10 reads arr
11 requires arr.Length == 10
12 requires 0 <= index < arr.Length
13 decreases index
14 ensures forall i :: 0 <= i < index ==> ((ArrayToBv10Helper(arr, i) >> i) & 1) == (if arr
[i] then 1 else 0)
15 {
16 if index == 0 then
17 (if arr[0] then 1 else 0) as bv10
18 else
19 var bit: bv10 := if arr[index] then 1 as bv10 else 0 as bv10;
20 (bit << index) + ArrayToBv10Helper(arr, index - 1)
21 }
22
23 method ArrayToSequence(arr: array<bool>) returns (res: seq<bool>) // Converts boolean
array to boolean sequence
24 ensures |res| == arr.Length
25 ensures forall k :: 0 <= k < arr.Length ==> res[k] == arr[k]
26 {
27 res := [];
28 var i := 0;
29 while i < arr.Length
30 invariant 0 <= i <= arr.Length
31 invariant |res| == i
32 invariant forall k :: 0 <= k < i ==> res[k] == arr[k]
33 {
34 res := res + [arr[i]];
35 i := i + 1;
36 }
37 }
38
39 function isBitSet(x: bv10, bitIndex: nat): bool
40 requires bitIndex < 10
41 ensures isBitSet(x, bitIndex) <==> (x & (1 << bitIndex)) != 0
42 {
43 (x & (1 << bitIndex)) != 0
44 }
45
46 function Bv10ToSeq(x: bv10): seq<bool> // Converts bitvector to boolean sequence
47 ensures |Bv10ToSeq(x)| == 10
48 ensures forall i: nat :: 0 <= i < 10 ==> Bv10ToSeq(x)[i] == isBitSet(x, i)
49 {
50 [isBitSet(x, 0), isBitSet(x, 1), isBitSet(x, 2), isBitSet(x, 3),
51 isBitSet(x, 4), isBitSet(x, 5), isBitSet(x, 6), isBitSet(x, 7),
52 isBitSet(x, 8), isBitSet(x, 9)]
53 }
22Programsynthesisviamechanisticinterpretability
54
55 function BoolToInt(a: bool): int {
56 if a then 1 else 0
57 }
58
59 function XOR(a: bool, b: bool): bool {
60 (a || b) && !(a && b)
61 }
62
63 function BitAddition(s: array<bool>, t: array<bool>): seq<bool> // Performs traditional
bit addition
64 reads s
65 reads t
66 requires s.Length == 10 && t.Length == 10
67 {
68 var a: bv10 := ArrayToBv10(s);
69 var b: bv10 := ArrayToBv10(t);
70 var c: bv10 := a + b;
71
72 Bv10ToSeq(c)
73 }
74
75 method f(s: array<bool>, t: array<bool>) returns (sresult: seq<bool>) // Generated program
for bit addition
76 requires s.Length == 10 && t.Length == 10
77 ensures |sresult| == 10
78 ensures forall i :: 0 <= i && i < |sresult| ==> sresult[i] == ((s[i] != t[i]) != (i > 0
&& ((s[i-1] || t[i-1]) && !(sresult[i-1] && (s[i-1] != t[i-1])))))
79 ensures BitAddition(s, t) == sresult // Verification of correctness
80 {
81 var a: bool := false;
82 var b: bool := false;
83 var result: array<bool> := new bool[10];
84 var i: int := 0;
85
86 while i < result.Length
87 invariant 0 <= i <= result.Length
88 invariant forall j :: 0 <= j < i ==> result[j] == false
89 {
90 result[i] := false;
91 i := i + 1;
92 }
93
94 i := 0;
95
96 assert forall j :: 0 <= j < result.Length ==> result[j] == false;
97
98 while i < result.Length
99 invariant 0 <= i <= result.Length
100 invariant b == (i > 0 && ((s[i-1] || t[i-1]) && !(result[i-1] && (s[i-1] != t[i-1]))))
101 invariant forall j :: 0 <= j < i ==> result[j] == ((s[j] != t[j]) != (j > 0 && ((s[j
-1] || t[j-1]) && !(result[j-1] && (s[j-1] != t[j-1])))))
102 {
103 assert b == (i > 0 && ((s[i-1] || t[i-1]) && !(result[i-1] && (s[i-1] != t[i-1]))));
104
105 result[i] := XOR(b, XOR(s[i], t[i]));
106 b := BoolToInt(b) + BoolToInt(s[i]) + BoolToInt(t[i]) > 1;
107 assert b == ((s[i] || t[i]) && !(result[i] && (s[i] != t[i])));
108
109 i := i + 1;
110 }
111
112 sresult := ArrayToSequence(result);
113 }
23Programsynthesisviamechanisticinterpretability
Table2. AutoMLarchitecturesearchresults.Allnetworksachieved100%accuracyonatleastonetestbatch.
Task# TaskName n w d w d TrainLoss TestLoss
f f g g
1 Binary Addition 2 1 1 4 2 0 0
2 Base 3 Addition 2 1 1 5 2 0 0
3 Base 4 Addition 2 1 1 5 2 0 0
4 Base 5 Addition 2 1 1 5 2 0 0
5 Base 6 Addition 2 1 1 6 2 2.45e-09 2.53e-09
6 Base 7 Addition 2 1 1 10 2 2.32e-06 2.31e-06
7 Bitwise Xor 1 1 1 2 2 0 0
8 Bitwise Or 1 1 1 1 1 3.03e-02 3.03e-02
9 Bitwise And 1 1 1 1 1 3.03e-02 3.03e-02
10 Bitwise Not 1 1 1 1 1 0 0
11 Parity Last2 1 1 1 229 2 1.68e-02 1.69e-02
12 Parity Last3 2 1 1 5 2 1.62e-04 1.64e-04
13 Parity Last4 3 1 1 29 2 3.07e-07 2.99e-07
14 Parity All 1 1 1 2 2 0 0
15 Parity Zeros 1 1 1 2 2 0 0
16 Evens Counter 4 1 1 73 3 8.89e-05 8.88e-05
17 Sum All 1 1 1 1 1 6.09e-08 6.13e-08
18 Sum Last2 2 1 1 1 1 0 0
19 Sum Last3 3 1 1 1 1 6.34e-07 6.35e-07
20 Sum Last4 4 1 1 1 1 2.10e-04 2.11e-04
21 Sum Last5 5 1 1 1 1 8.86e-03 8.87e-03
22 Sum Last6 6 1 1 1 1 1.82e-02 1.81e-02
23 Sum Last7 7 1 1 1 1 3.03e-02 3.01e-02
24 Current Number 1 1 1 1 1 0 0
25 Prev1 2 1 1 1 1 0 0
26 Prev2 3 1 1 1 1 0 0
27 Prev3 4 1 1 1 1 0 0
28 Prev4 5 1 1 1 1 2.04e-07 2.05e-07
29 Prev5 6 1 1 1 1 6.00e-05 5.96e-05
30 Previous Equals Current 2 1 1 5 2 6.72e-05 6.61e-05
31 Diff Last2 2 1 1 1 1 0 0
32 Abs Diff 2 2 2 1 1 1.84e-07 1.84e-07
33 Abs Current 1 1 1 2 2 4.51e-08 5.71e-08
34 Diff Abs Values 2 1 1 4 2 3.15e-06 2.96e-06
35 Min Seen 1 1 1 2 2 0 0
36 Max Seen 1 1 1 2 2 1.46e-12 0
37 Majority 0 1 1 1 1 63 2 4.03e-03 4.05e-03
38 Majority 0 2 4 1 1 98 2 1.64e-04 1.71e-04
39 Majority 0 3 21 1 1 132 3 6.94e-05 6.86e-05
40 Evens Detector 5 1 1 163 2 8.18e-04 8.32e-04
41 Perfect Square Detector 48 1 1 100 2 1.92e-03 1.97e-03
42 Bit Palindrome 18 1 1 86 2 3.81e-05 3.69e-05
43 Balanced Parenthesis 1 1 1 16 2 7.44e-03 7.10e-03
44 Parity Bits Mod2 1 1 1 1 1 0 0
45 Alternating Last3 2 1 1 3 2 1.85e-02 1.87e-02
46 Alternating Last4 2 1 1 3 2 8.24e-06 8.09e-06
47 Bit Shift Right 2 1 1 1 1 0 0
48 Bit Dot Prod Mod2 1 1 1 3 2 0 0
49 Div 3 2 1 1 59 2 6.40e-03 6.43e-03
50 Div 5 4 1 1 76 2 1.50e-04 1.55e-04
51 Div 7 4 1 1 103 2 6.65e-04 6.63e-04
52 Add Mod 3 1 1 1 149 2 1.02e-03 1.04e-03
53 Add Mod 4 2 1 1 33 2 1.53e-04 1.44e-04
54 Add Mod 5 3 1 1 43 2 1.02e-03 1.03e-03
55 Add Mod 6 4 1 1 108 2 6.14e-04 6.12e-04
56 Add Mod 7 4 1 1 199 2 3.96e-04 4.07e-04
57 Add Mod 8 67 1 1 134 2 8.53e-04 8.34e-04
58 Dithering 81 1 1 166 2 7.72e-04 7.75e-04
59 Newton Freebody 2 1 1 1 1 2.61e-07 2.62e-07
60 Newton Gravity 2 1 1 1 1 1.81e-07 1.87e-07
61 Newton Spring 2 1 1 1 1 0 0
62 Newton Magnetic 4 1 1 1 1 8.59e-05 8.60e-05
24