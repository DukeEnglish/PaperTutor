Language-Based Augmentation to Address Shortcut
Learning in Object-Goal Navigation
1st Dennis Hoftijzer 2nd Gertjan Burghouts 3rd Luuk Spreeuwers
University of Twente TNO University of Twente
Enschede, The Netherlands Den Haag, The Netherlands Enschede, The Netherlands
dennishoftijzer@gmail.com gertjan.burghouts@tno.nl l.j.spreeuwers@utwente.nl
Abstract—Deep Reinforcement Learning (DRL) has shown
great potential in enabling robots to find certain objects (e.g., Instruction: “Find a sofa”
‘find a fridge’) in environments like homes or schools. This task
is known as Object-Goal Navigation (ObjectNav). DRL methods Training environment Testing environment
are predominantly trained and evaluated using environment
Sofa room is blue Sofa room is green
simulators. Although DRL has shown impressive results, the
simulators may be biased or limited. This creates a risk of
shortcut learning, i.e., learning a policy tailored to specific
visual details of training environments. We aim to deepen our v a
N
understandingofshortcutlearninginObjectNav,itsimplications tc
and propose a solution. We design an experiment for inserting e
jb
a shortcut bias in the appearance of training environments. O
As a proof-of-concept, we associate room types to specific wall A
T
colors (e.g., bedrooms with green walls), and observe poor O
S
generalization of a state-of-the-art (SOTA) ObjectNav method
to environments where this is not the case (e.g., bedrooms with
blue walls). We find that shortcut learning is the root cause: the
Navigates to sofa room Skips sofa room
agent learns to navigate to target objects, by simply searching
for the associated wall color of the target object’s room. To
solve this, we propose Language-Based (L-B) augmentation. Our
key insight is that we can leverage the multimodal feature
n
space of a Vision-Language Model (VLM) to augment visual o
representationsdirectlyatthefeature-level,requiringnochanges
ita
tothesimulator,andonlyanadditionofonelayertothemodel.
tn
e
m
Where the SOTA ObjectNav method’s success rate drops 69%,
g
our proposal has only a drop of 23%. Code is available at u
a
https://github.com/Dennishoftijzer/L-B Augmentation B
-
L
Index Terms—Vision-based Navigation, Deep Reinforcement
Learning, Vision-Language
Navigates to sofa room Navigates to sofa room
I. INTRODUCTION
Fig. 1. We propose Language-Based (L-B) augmentation to generalize
Humans can easily find certain objects (e.g. ‘find a fridge’)
better to scenes with different wall colors. In this example, we interchange
in complex environments we have not seen before, such as thewallcolorofthebedroomandlivingroom,causingtheSOTAobjectNav
a friend’s house. We effortlessly avoid any obstacles but method [1] to look for the sofa in the blue bedroom (wrong). With our
augmentations,thisiscountered.
also reason about the unseen environment to decide which
room to explore next e.g., ‘where is a fridge most likely
located?’.Theembodied-AI(E-AI)communityhasmadegreat
strides by learning embodied agents (or ‘virtual robot’) such due to data collection artifacts, limitations in rendering, or
skills using Deep Reinforcement Learning (DRL) in a task simply unintended biases the simulator designer is not aware
called Object-Goal Navigation (ObjectNav) [2]. Despite good of. For instance, all kitchens in training environments might
progress,DRLreliesongatheringexperienceovermillions(or have a tiled floor. Consequently, training in E-AI simulators
billions) of iterations, making it impossible to learn in real- creates a profound risk of shortcut learning [7]: learning a
worldenvironments.Therefore,researchhasbeendrawntoE- simple, non-essential policy, tailored to specific details of the
AIsimulators[3]–[6],whichallowforeasilytrainingagentsin simulated environment, rather than learning any semantic rea-
various simulated 3D indoor environments. However, subtle, soning or task-related skills. Efficient object-goal navigation
but detrimental, dataset biases might arise in E-AI simulators involves learning useful semantic priors such as object-room
4202
beF
7
]OR.sc[
1v09050.2042:viXrarelations (e.g., a fridge is in the kitchen), however can easily reachability and object localization. They set new SOTA
lead to unintended shortcuts (e.g., fridge is located near a resultsonseveralvisualnavigationtasks,includingObjectNav,
tiled floor), which fail to generalize to environments where and show promising results for generalizing to an open-
the shortcuts are no longer valid. world setting i.e., navigating to target objects not seen during
In this work, we deepen our understanding of shortcut training. Moreover, as generating the robot trajectories and
learninginObjectNav,itsimplicationsandproposeasolution. paired language annotations in the real world might be costly,
First, we introduce an out-of-distribution (o.o.d.) generaliza- further works have proposed utilizing VLMs out-of-the-box
tiontest.Weinsertadatasetbiasintheappearanceoftraining with maps to enable zero-shot navigation i.e., without super-
environments, which offers the agent a shortcut pathway for vision of DRL reward signals or human demonstrations [16]–
finding a given target object. As a proof-of-concept of such [18]. We adopt the architecture of EmbCLIP as a baseline,
a shortcut bias, we associate each room type to a unique givenitsstrongperformanceonavarietyofsettings.However,
wall color i.e., kitchens have red walls, bedrooms have green our focus is specifically on addressing shortcut bias in E-
walls and so forth. Using our setup, we are able to evaluate AI simulators, which might impede generalization of DRL
o.o.d. generalization of a state-of-the-art (SOTA) ObjectNav methods to novel environments.
method [1] to environment where we change wall colors (e.g.
B. Embodied AI simulators and scene datasets
kitchens have blue walls). As a result, we find that (1) only
changing wall colors degrades performance significantly, and Many E-AI simulators [3]–[6] have been developed, along
(2) shortcut learning is the root cause. The agent learns to with several (near) photo-realistic scene datasets [10]–[12],
navigate towards target objects by simply searching for the [20].Scenescanbeeitherreconstructedfrom3Dscansofreal-
wall color associated with the target object’s room. world houses e.g., Matterport [11], or synthetically composed
Secondly, we aim to increase domain generalization. Do- from artist created 3D assets e.g., RoboTHOR [5]. Both
mainrandomizationmethodse.g.,randomizingtextures,colors methodsareextremelycostlytocollect.Reconstructingscenes
and shapes of objects or environments are commonly used to from 3D scans involves stitching images from specialized
transfer policies in DRL [8]–[10]. However, these methods cameras whilst manually composing synthetic scenes involves
specifically require changes to the simulator, which might be carefully configuring lighting, object placement and textures.
inflexible or difficult to modify e.g., high-fidelity simulators ProcTHOR [10] recognizes this fact and instead uses a pro-
withtrainingdatareconstructedfromreal-world3Dscans[11], cedural generation process to generate 10,000 scenes (dubbed
[12]. While more sophisticated methods for partially editing ProcTHOR-10k). In this work, we leverage the ProcTHOR-
individual frames during training exist (e.g., text-to-image 10k scene dataset. Due to the procedural generation process,
models [13], [14]), they are slow, computationally expensive wecanfullycustomizethesescenesbyalteringtheappearance
and error-prone. Instead, we take a different approach and of individual objects and room surfaces (walls, floors and
proposeLanguage-Based(L-B)augmentation(seeFig.1).We ceilings).Forinstance,aredsofacanbereplacedwithablack
augment directly at feature-level, without editing individual one. This customization and our proposed interventions on
frames or any changes to the simulator. ProcTHOR-10k, allow for inserting a shortcut bias in the ap-
We build upon promising results from [1], where visual pearance of training scenes and evaluate o.o.d. generalization.
representations within the agent’s architecture are based on Although ProcTHOR enables E-AI to scale, this does not
a Vision-Language Model (VLM). RGB observations are en- imply shortcut biases completely disappear. Recent works
codedusingaContrastiveLanguageImagePretraining(CLIP) show that even Large-Language Models (LLMs), pre-trained
[15]visualbackbone.CLIPjointlytrainsanimageandtexten- on text amounting to billions of words, suffer from shortcut
coder,suchthatbothproducesimilarrepresentationsforvisual learning, largely due to collection artifacts in training data
conceptsinimagesortheirnamesinnaturallanguage.Ourkey [7],[21].Moreover,shortcutbiasmightbedifficulttoobserve
insight is that we can augment agent’s visual representations for humans e.g., superficial statistics in training data such as
at feature-level, by describing variations of the dataset bias textures of specific frequencies in image classification tasks
in natural language. By an elegant modification to the SOTA [22]. Consequently, in E-AI simulators, shortcut bias might
architecture [1], with only one additional layer, we generalize arise inadvertently. For instance, as ProcTHOR is generated
bettertoenvironmentswithdifferentwallcolorsinObjectNav. procedurally, some smaller objects (e.g. a pen) are always
placedonlargerobjects(e.g.adeskinthebedroom).Anagent
II. RELATEDWORK
might learn a bias for navigating to a target object only when
A. Vision-Language for visual navigation
it is placed on this larger object and not when the object is
Several recent works have proposed utilizing pre-trained placed independently (e.g. on the floor in the living room).
features of Vision-Language Models (VLMs), pre-trained on We employ a simple wall color bias as a proof-of-concept for
internet-scale data, for several visual navigation tasks [1], such unintended shortcut biases.
[16]–[19]. In [1], authors explore the effectiveness of learning
C. Shortcut learning
a navigation policy based on CLIP embeddings [15]. With
their method, EmbCLIP [1], they show that CLIP’s visual Shortcut learning is emerging as a key impediment in the
representations encode useful navigation primitives such as generalization ability of deep neural networks (DNNs) [7].Shortcuts are decision rules, often learned by DNNs, which B. Evaluation metrics
aid performance on a particular dataset but do not match
FollowingstandardObjectNavprocedure[2],wereporttwo
with human-intended ones. Accordingly, they typically fail
primary evaluation metrics: the average success rate over all
whentestedinonlyslightlydifferentconditions.Priorworkin N evaluation episodes (Success) and Success weighted by
shortcut learning is predominantly concerned with supervised
normalized inverse Path Length (SPL) [2], a measure for
learning [22]–[24]. Similar to our work, [23] designs an path efficiency. SPL is bounded by [0,1], where 1 is optimal
experimental setup to observe whether DNNs prefer to adopt
performancei.e.,theagenttooktheshortestpathpossibleinall
color, shape or size shortcuts, and find DNNs naturally prefer N evaluation episodes. Note that, SPL is a stringent measure.
certain shortcuts. In contrast, we study the shortcut learning
AchievinganSPLof1isinfeasible(evenforhumans),without
phenomenon in the context of DRL.
knowing the target object location a priori. Additionally, we
A common implication of shortcut learning in DRL is reporttwomoreevaluationmetrics:DistanceToTarget(DTT)
observed when transferring policies from simulation to the and the episode length. DTT is the remaining shortest path
real-world [7], [8], [25]. Most policies trained in simulation length to visibly see the target object.
generalize poorly to the real-world due to agents adapting to
IV. O.O.D.TEST:INTERVENTIONSONPROCTHOR-10K
specific visual details of the simulator. Prior works cope with
this so-called ‘reality gap’ by domain randomization methods A. Interventions on ProcTHOR-10k
i.e., randomizing appearances in training environments [8], Inordertoevaluateo.o.d.generalization,weneedtoguaran-
[9]. Similarly, ProcTHOR [10] allows for randomizing e.g., tee only to measure performance degradation due to changing
texturesandcolorsofwalls,ceilings,floorsandobjects.While wall colors, without other aspects (object appearances, scene
ProcTHORshowsincrediblypowerfulresults,suchaugmenta- layout,etc.)influencingourevaluation.Therefore,wepropose
tionsmightnotbeavailableforallsimulators,andmoreoften some interventions on ProcTHOR-10k. First, we start by
thannot,difficulttoapplypost-hoc.Contrary,ourmethodcan selecting a more uniform subset of scenes and targets. We
readilybeappliedpost-hocasitrequiresnochangestotraining select only houses with 3 rooms, which all consist of 3 room
data or the simulator. We propose augmentations where we types: kitchen, bedroom and living room. For each room
use targeted randomization of specific unintended biases, in type, we select 3 target object categories (9 total) which are
our case, wall color. Although a simple wall color bias might semantically related (e.g. fridge in kitchen). Section VII-A
be addressed using conventional domain randomization, these (Appendix) shows an overview of all target objects selected.
methods are inconvenient considering more intricate biases Next, we restrict ourselves to scenes which contain exactly
(e.g., a pen is always on a desk). In contrast, our method one instance of each target object category in the associated
utilizes free-form natural language, which allows for easily targetroome.g.,everyhousecontains1bed,positionedinthe
adaptingtodifferentbiases.Vision-LanguageModels(VLMs) bedroom. We ensure this restriction by (1) selecting scenes
e.g., CLIP [15], allows us to augment at feature-level based which contain at least one instance of each target object in
on prior knowledge of the environment without any changes the associated room type and (2) manually removing any
to training data. double (or more) instances of target objects. Secondly, we
set identical appearances for all object categories (including
doors) e.g., all chairs appear exactly alike, and identical
III. OBJECTNAVPRELIMINARIES
appearances of room surfaces for each room type e.g., all
kitchens have identically colored walls, floors and ceilings.
A. ObjectNav definition Wesetobjectappearancesidenticalbyassigningone3Dasset
from the ProcTHOR library to each object type. We set all
In ObjectNav [2], agents are initialized at a random pose
room surfaces identical by setting the same materials from
in an unseen environment and given a label specifying the
the ProcTHOR library. Lastly, we remove windows and wall
target object category (e.g. ‘Bed’). The goal for the agent is
decoration. These interventions limit the house variations to
to navigate to an instance of the target object within a certain
just wall colors, which is the only aspect influencing the
timebudget(T =500).Ateachtimestept,theagentreceives
performance.
an image from an RGB forward-facing camera and can take
oneof6possibleactions:moveahead,rotateleft,rotateright, B. Evaluating o.o.d. generalization
look up, look down and done. We do not utilize any depth Weaimtoassesstheagent’sgeneralizationacrossincreasing
sensor readings. Also, we simulate actuation noise to better numbers of changing wall colors (illustrated in Fig. 2). The
resemble actuation in the real-world. A full description of the training set consists of visually identical houses, where living
discrete action space is shown in Table I (Appendix). rooms have blue walls, kitchens have red walls and bedrooms
Anepisodeisconsideredsuccessfulif(1)theagentexecutes have green walls. For our test sets, we use houses with a
the done action; (2) The target object is within a certain different layout such that agents cannot simply memorize
distancethreshold,typicallyd =1m;and(3)thetargetobject object locations, and permute wall colors. The 0-room test
t
isconsideredvisiblei.e.,withinthecamera’sfieldofviewand set serves as a reference. To solely evaluate generalization to
not fully obstructed. different wall colors, we use the same layouts in each testTrain Test set Test set Test set Test set
set 0-room 1-room 2-room 3-room
Specific wall color for each room type
No wall color change 1 wall color change 2 wall color changes 3 wall color changes
Target room = Kitchen
Disjoint Change Change Change
set of Target Non-target Non-target
scenes room room room
Identical visual appearance
Target room
Deceptive!
Fig.2. Setup for our o.o.d. generalization test.Inthisexample,thetargetroomisthekitchen(redwallsintestset0-room).Wechangethetargetroom
first(testset1-room)andincrementallychangemorerooms(testset2/3-room).Thebottomrowshowstwoexamplesofdeceptivechanges,wherethewall
colorassociatedwiththetargetroom(redwallcolor)ismovedtoadifferentroomtype.Thetoprowonlyshowsnondeceptivechanges.
set and compare performance to the reference 0-room test set. (CLIP ). CLIP learns to associate text strings with their
v
First, we change the wall color of the target object’s room visual concepts in images. Our key insight is that we can
as this is the simplest deviation (1 wall color change). For representdomainspecificknowledge,regardingthechangesin
instance,ifthetargetobjectisafridge,westartbyalteringthe environmentappearances,usingnaturallanguage.Byencoding
wallcolorofthekitchenfromredtoe.g.,green,whereasifthe text descriptions of variations of the dataset bias (e.g. ‘a blue
targetobjectisabed,westartwithchangingthewallcolorof wall’), using CLIP’s text encoder (CLIP ), we vary visual
T
thebedroomfromgreentoe.g.,blue.Next,wechangeanother representationswithoutactuallyhavingseenimagescontaining
room’s wall color (2 wall color changes). Finally, we change these variations (e.g. an image of a blue wall). This allows
the wall colors of all three rooms (3 wall color changes). We us to augment directly at feature-level. For encoding the text
change to all possible permutations (e.g. red kitchen to blue descriptionsweusethedefaultprompttemplaterecommended
and green wall colors in test set 1-room) with repetition i.e., by [15]: ‘a photo of a {label}’. We insert descriptions of
multiple room types can have the same wall color. variations of the dataset bias (e.g. ‘red wall’). We obtain
Wedifferentiatewallcolorchangesofnon-targetrooms(test our augmented embeddings IIILLLBBB by computing differences
ttt
set 2- and 3-room) into two types: deceptive vs nondeceptive. between n encoded text descriptions of variations of the
We expect that moving the learned color i.e, the wall color dataset bias TTT , and adding to visual representation III :
111,,,.........,,,nnn ttt
associated with the target room, to a non-target room will
have a high impact, because the agent may look in the latter,
wrongroom.Werefertothiswallcolorchangeas‘deceptive’. III tttLLLBBB =III ttt+α·∆(TTT), (1)
   
Examples of deceptive changes are shown in the bottom row ∆∆∆ TTT −TTT
111 111 222
(test set 2- and 3-room). Instead, when none of the rooms  ∆∆∆ 222  TTT 111−TTT 333
hasthelearnedcolor,weexpectlessperformancedegradation, ∆(TTT)=  . .  =  . .   (2)
 .   . 
because the agent is not misled. We refer to such a wall color
∆∆∆ TTT −TTT
change as ‘nondeceptive’ (top row). nnn(((nnn−−−111))) nnn 111
where, α controls the degree of augmentation and ∆ com-
V. METHOD:LANGUAGE-BASEDAUGMENTATION
putes differences of all permutations of length 2 of text de-
We increase domain generalization by augmenting the scriptionsTTT.Byrandomlysamplinganaugmentedembedding
agent’s visual representations at feature-level, such that these fromIIILLLBBB at each time step, we aim to provide the RL model
ttt
are more invariant to changing environments. We implement (RNN) with an embedding which resembles the same room
this by adding one layer on top of EmbCLIP [1]. In Emb- type (e.g., a living room in Fig. 3), but with a different
CLIP, the agent’s visual representations are based on a VLM wall color (e.g., red and blue instead of green in Fig. 3).
(CLIP). We leverage the vision-language representations for We empirically find α = 50 to work well by tuning for
feature-level augmentations, without the need to modify the our specific dataset and shortcut bias. We standardize features
simulator.Ouraugmentationsarebasedontextualdescriptions beforefeedingintotheRNNtoensurestabilityduringtraining
of variations of the dataset bias that we want the agent (assomefeaturesmightdominatethelossduetolargenorms).
to learn and generalize. We call this Language-Based (L-B) In our case, we insert three (n = 3) text descriptions of
augmentation (Fig. 3). In EmbCLIP, at each time step t, a variationsofthedatasetbias:‘bluewall’,‘redwall’and‘green
visual representation or image embedding III is obtained by wall’. This results in 6 augmented embeddings III +∆∆∆
ttt ttt nnn(((nnn−−−111)))
encoding RGB observations using CLIP’s [15] visual encoder per image embedding III .
tttLanguage-based augmentation Testing
via Vision-Language Space environments
Text embeddings
TTT TTT ---TTT
111 111 222
(cid:862) (cid:898)(cid:4) (cid:18)(cid:381)(cid:3)(cid:393) (cid:367)(cid:381)(cid:346) (cid:396)(cid:381) (cid:899)(cid:410) (cid:3)(cid:381) (cid:449)(cid:3)(cid:381) (cid:258)(cid:296) (cid:367)(cid:367)(cid:3) (cid:863)(cid:258)(cid:3) (cid:18)(cid:62)(cid:47)(cid:87) (cid:100) TTT ......222
...
ΔΔ TTT 111 ...--- ......TTT 333
I+Δ Resembles
t 1
Describe n variations TTT nnn ......
of dataset bias TTT –––TTT αα I t+Δ 2
nnn 111
Living room with
...
red walls
Image embedding
(cid:18)(cid:62)(cid:47)(cid:87) (cid:115) I t ++ ... Resembles
I+Δ
t n(n-1)
RGB Living room with
observation green walls Random sampling
Living room with
blue walls
(cid:100)(cid:396)(cid:258)(cid:349)(cid:374)(cid:349)(cid:374)(cid:336) (cid:4)(cid:272)(cid:410)(cid:349)(cid:381)(cid:374) (cid:90)(cid:62)(cid:3)(cid:373)(cid:381)(cid:282)(cid:286)(cid:367)
(cid:286)(cid:374)(cid:448)(cid:349)(cid:396)(cid:381)(cid:374)(cid:373)(cid:286)(cid:374)(cid:410) (cid:896)(cid:862)(cid:68)(cid:381)(cid:448)(cid:286)(cid:3)(cid:296)(cid:381)(cid:396)(cid:449)(cid:258)(cid:396)(cid:282)(cid:863)(cid:853)(cid:3)(cid:862)(cid:90)(cid:381)(cid:410)(cid:258)(cid:410)(cid:286)(cid:3)(cid:367)(cid:286)(cid:296)(cid:410)(cid:863)(cid:897) (cid:894)(cid:90)(cid:69)(cid:69)(cid:895)
Fig.3. Language-Based(L-B)augmentationviaathefeaturespaceofavision-languagespace.Ourkeyinsightisthatwecanaugmentagent’svisual
representations (It) using differences (∆) between encoded text descriptions of variations of the dataset bias (T1,...,n). The augmented embedding of an
image‘Alivingroomwithgreenwalls’resemblesa‘livingroomwithredorbluewalls’.TheRLmodel(RNN)isnotabletouseashortcutstrategyevenif
duringtraininglivingroomsalwayshavegreenwalls.
VI. EXPERIMENTS setup [1], [10], the embodied agent approximately matches a
Weperformtwoexperiments,whereweaimto:(1)evaluate LoCoBot with a 90° horizontal camera field of view, a 0.25m
generalization of EmbCLIP [1] to scenes where we change step size and a 30° turn angle.
wall colors and study to what extend shortcuts influence 3) Reward setting: At each time step t, the reward r t is:
the generalization ability; and (2) validate L-B augmentation
r =max(0,min∆ −∆ )+r +r (3)
t 0:t−1 t slack succ
can increase domain generalization by integrating within the
architecture of EmbCLIP. wheremin∆ 0:t−1 istheminimalpathlengthfromtheagentto
thetargetobjectthattheagenthaspreviouslyobservedduring
A. Experimental setup
the episode, ∆ is the current path length, r = −0.01
t slack
1) ObjectNav dataset details: We train agents on 20 visu- is the slack penalty and r = 10 is a large reward for
succ
ally identical but biased scenes, generated using our proposed successful episodes. The reward is shaped to optimize path
interventions (Section IV). During training, we randomly efficiency and, therefore, SPL.
sample 1 of 9 target objects. We analyze o.o.d. performance 4) Implementation details: We use the Allenact framework
on a set of 5 disjoint scene layouts. We run 1080 evaluation [26] and render frames at 224×224 resolution. To parallelize
episodes per test set to evenly distribute episodes over the 5 training,weuseDD-PPO[27]with40environmentinstances.
layouts, possible wall color permutations and target objects. After each rollout, the model is updated using 4 epochs of
Section VII-B (Appendix) details the distribution. PPO [28] in a single global batch size of 7680 frames. We
2) Agent architecture and configuration: We use the Ob- perform validation every 200,000 frames and report results of
jectNav EmbCLIP architecture [1], which has two different the checkpoint with highest SPL. Additional hyperparemeters
variations: a closed-world architecture, which assumes known are shown in Section VII-C (Appendix).
target objects, and an open-world variant. Both encode RGB
B. Impact of Shortcut Learning
egocentric views using a frozen CLIP image encoder with
a ResNet-50 backbone. However, the closed-world variant How well does a SOTA ObjectNav method generalize to
obtains a goal-conditioned embedding before feeding into an scenes with different wall colors and to what extent do short-
RNN, which involves removing the final layers from CLIP, cuts affect the o.o.d. generalization ability? We hypothesise
whereas the open-world variant feeds the image embedding the performance drop coheres with the number of wall color
directly. For our o.o.d. generalization test, we adopt Emb- changes. Moreover, we posit a deceptive change will lead to
CLIP’s closed-world architecture given its better performance more performance degradation than a nondeceptive change as
inaclosed-worldsetting.WeintegrateourL-Baugmentations the agent will be misled to search for the target object in the
in the open-world variant, as this architecture feeds the CLIP wrong room. Fig. 4 shows the performance. We report the
image embedding directly into an RNN, which allows for mean over all episodes, the mean over episodes with decep-
substituting this image embedding with a L-B augmented em- tive changes and the mean over episodes with nondeceptive
bedding,usingrandomsampling.FollowingtypicalObjectNav changes.WeobservethatchangingthewallcolorsofonlytheNo. of wall color changes No. of wall color changes No. of wall color changes No. of wall color changes
Fig. 4. Degradation for o.o.d. cases. Performance of EmbCLIP [1] to scenes with different wall colors. When only changing the wall color of the target
object’sroom(1wallcolorchange),wealreadyobservealargedecreaseinperformanceinallmetrics.
episodelengthsfor deceptivechanges.We conjecturethatdue
Instruction: “Find a bed”
to deceptive changes, the agent directly navigates towards the
Starting point A Starting point B
learnedcolorofthetargetroom,whichisnowplacedinanon-
targetroom,withoutexploringanyotherrooms.Theagentwill
erroneously search this non-target room, but can not find the
target object, and terminates the episode. In contrast, an agent
will explore the entire scene when wall colors have only been
changed nondeceptively, leading to longer episodes. We show
No wall color change a qualitative example of this behaviour in Fig. 5, where the
agent has learned to look for a room with green walls instead
ofabedroom.Theagentisdeceivedbythegreenlivingroom,
and terminates the episode when it sees the sofa. This leads
to a much shorter episode than in the nondeceptive example,
where the agent explores large parts of the scene. Evidently,
the agent has learned a shortcut strategy, it navigates towards
Nondeceptive a particular wall color instead of the right target room.
C. Benefit of Proposed L-B Augmentations
Do our L-B augmentations make the agent’s model more
robust to shortcuts, i.e., more domain invariant against biased
wallcolorandroomtype?WeintegrateourL-Baugmentation
method within the EmbCLIP architecture as a single extra
Deceptive layer, as detailed in Section V. Note that training time is only
marginallylonger:from88to90GPU-hours(both30Msteps).
Fig. 6 shows a comparison of EmbCLIP with and without our
Fig.5. ErrorsandshortcutsbytheSOTAObjectNavmethod.Weshow L-Baugmentations.EmbCLIP’sperformancealreadydegrades
exampletrajectoriesfrom2differentstartingposition(leftvsrightcolumn).
significantly after changing wall colors of the target room (1
Notice how nondeceptive episodes (middle) are much longer than deceptive
episodes(bottom),whilstbothareunsuccessful.Alsonotetheabsolutelack wall color change). We observe 69% relative drop in success
ofsearchinthebedroomwhenchangingwallcolorsdeceptively. (45% → 14%) and 82% drop in SPL (0.22 → 0.04). In
contrast, our method shows improved domain generalization.
When changing wall colors of the target room (1 wall color
target room already leads to a large decrease in performance. change), our method incurs only a 23% relative drop in
On average (blue mean bar), we observe a 67% relative drop success (39% → 30%) and 29% drop in SPL (0.17 → 0.12).
in SPL (0.39 → 0.13) and 56% in success rate (68% → 30%) We observe less performance degradation with an increasing
going from 0 wall color changes to 1 wall color change, with numberofwallcolorchangesthanEmbCLIP.Inthequalitative
even lower mean performance for more wall color changes. example of Fig. 1, the agent is now able to find the sofa even
Indeed, we find that EmbCLIP generalizes poorly to scenes thoughitisnotinalivingroomwithbluewalls(training).The
with different wall colors when using limited training data. agent finds the sofa successfully in a living room with green
Regarding deceptive vs nondeceptive changes, the Success, walls(notseenduringtraining).Theseresultsdemonstratethat
SPL and DTT metrics indicate that deceptive changes indeed ourL-BaugmentationsareaninterestingdirectiontomakeRL
deteriorate performance most, even more than multiple non- agents more robust to biases, by only adding one additional
deceptive changes. Interestingly, however, we observe shorter layer to the agent’s model.Fig.6. Extending EmbCLIP [1] with our L-B augmentations.Performanceono.o.d.casesforEmbCLIP[1]vsEmbCLIPwithourL-Baugmentations.
UsingourL-Baugmentations,EmbCLIPgeneralizesbettertosceneswithdifferentwallcolors.
VII. CONCLUSIONANDLIMITATIONS [2] D. Batra, A. Gokaslan, A. Kembhavi, O. Maksymets, R. Mottaghi,
M. Savva, A. Toshev, and E. Wijmans, “Objectnav revisited: On
We evaluated how well a SOTA method for ObjectNav evaluation of embodied agents navigating to objects,” CoRR, vol.
abs/2006.13171,2020.
generalizes to scenes with different wall colors, and studied
[3] E.Kolve, R.Mottaghi, D.Gordon,Y. Zhu,A. Gupta,and A.Farhadi,
to what extent shortcut learning influences this o.o.d. gener- “AI2-THOR:aninteractive3denvironmentforvisualAI,”CoRR,vol.
alization. We found that, when deliberately limiting training abs/1712.05474,2017.
data, only changing wall colors in testing scenes decreases [4] M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain,
J.Straub,J.Liu,V.Koltun,J.Malik,D.Parikh,andD.Batra,“Habitat:
performancesignificantly,withtherootcausebeingthedecep-
Aplatformforembodiedairesearch,”inProceedingsoftheIEEE/CVF
tive wall color changes. We proposed Language-Based (L-B) InternationalConferenceonComputerVision(ICCV),October2019.
augmentation to mitigate shortcut learning. By encoding text [5] M.Deitke,W.Han,A.Herrasti,A.Kembhavi,E.Kolve,R.Mottaghi,
J. Salvador, D. Schwenk, E. VanderBilt, M. Wallingford, L. Weihs,
descriptions of variations of the dataset bias, and leveraging
M. Yatskar, and A. Farhadi, “Robothor: An open simulation-to-real
the multimodal embedding space of CLIP, we were able to embodied ai platform,” in Proceedings of the IEEE/CVF Conference
augment agent’s visual representations directly at feature- onComputerVisionandPatternRecognition(CVPR),June2020.
level. Finally, experiments showed that our L-B augmentation [6] B. Shen, F. Xia, C. Li, R. Mart´ın-Mart´ın, L. Fan, G. Wang, C. Pe´rez-
D’Arpino,S.Buch,S.Srivastava,L.Tchapmi,M.Tchapmi,K.Vainio,
method is able to improve domain generalization to scenes
J. Wong, L. Fei-Fei, and S. Savarese, “igibson 1.0: A simulation
with different wall colors in ObjectNav. When changing the environment for interactive tasks in large realistic scenes,” in 2021
target object’s room, our method incurs a 23% relative drop IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS),2021,pp.7520–7527.
in success rate whilst the SOTA ObjectNav method’s success
[7] R. Geirhos, J.-H. Jacobsen, C. Michaelis, R. Zemel, W. Brendel,
rate drops 69%. M. Bethge, and F. A. Wichmann, “Shortcut learning in deep neural
To demonstrate the usefulness of our approach, we consid- networks,” Nature Machine Intelligence, vol. 2, no. 11, pp. 665–673,
2020.
ered a simple case of shortcut bias e.g., wall color. Although
[8] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel,
our agents showed improved domain generalization, such “Domainrandomizationfortransferringdeepneuralnetworksfromsim-
simple cases may well be accommodated using conventional ulationtotherealworld,”in2017IEEE/RSJInternationalConference
onIntelligentRobotsandSystems(IROS),2017,pp.23–30.
domainrandomizationmethodsinsimulatorswhichareeasily
[9] F.SadeghiandS.Levine,“(cad)$ˆ2$rl:Realsingle-imageflightwithout
modifiable. In future work, we hope to explore how to use
asinglerealimage,”CoRR,vol.abs/1611.04201,2016.
natural language for augmentations addressing more intricate [10] M.Deitke,E.VanderBilt,A.Herrasti,L.Weihs,K.Ehsani,J.Salvador,
biases. For instance, bias at the object-level. Some objects W.Han,E.Kolve,A.Kembhavi,andR.Mottaghi,“ProcTHOR:Large-
scaleembodiedAIusingproceduralgeneration,”inAdvancesinNeural
might usually occur in combination with other objects (e.g.,
Information Processing Systems, A. H. Oh, A. Agarwal, D. Belgrave,
pillow on a bed). Agents could learn a bias for navigating andK.Cho,Eds.,2022.
towardsthebedroominsteadofalsoexploringthelivingroom [11] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niebner, M. Savva,
(e.g., pillow on a sofa). Moreover, as efficient ObjectNav S. Song, A. Zeng, and Y. Zhang, “Matterport3d: Learning from rgb-d
datainindoorenvironments,”in2017InternationalConferenceon3D
requiresagentstoleverageusefulsemanticpriorsabouttheen-
Vision(3DV),2017,pp.667–676.
vironment(e.g.,object-roomrelations),itwouldbeinteresting [12] S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets,
to see how to use natural language to guide the exploration A. Clegg, J. M. Turner, E. Undersander, W. Galuba, A. Westbury,
A.X.Chang,M.Savva,Y.Zhao,andD.Batra,“Habitat-matterport3d
of agents in more unusual situations, where such priors are
dataset (HM3d): 1000 large-scale 3d environments for embodied AI,”
disadvantageous (e.g. pillow in the kitchen). in Thirty-fifth Conference on Neural Information Processing Systems
Datasets and Benchmarks Track (Round 2), 2021. [Online]. Available:
https://openreview.net/forum?id=-v4OuqNs5P
REFERENCES
[13] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, “Hierar-
chical text-conditional image generation with clip latents,” ArXiv, vol.
[1] A.Khandelwal,L.Weihs,R.Mottaghi,andA.Kembhavi,“Simplebut abs/2204.06125,2022.
effective: Clip embeddings for embodied ai,” in Proceedings of the [14] A.Ramesh,M.Pavlov,G.Goh,S.Gray,C.Voss,A.Radford,M.Chen,
IEEE/CVF Conference on Computer Vision and Pattern Recognition and I. Sutskever, “Zero-shot text-to-image generation,” in Proceedings
(CVPR),June2022,pp.14829–14838. of the 38th International Conference on Machine Learning, ser. Pro-ceedingsofMachineLearningResearch,M.MeilaandT.Zhang,Eds., TABLEI
vol.139. PMLR,18–24Jul2021,pp.8821–8831. ACTIONSPACEDESCRIPTION.WEUSEA6-ACTIONDISCRETEACTION
[15] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, SPACE.
G.Sastry,A.Askell,P.Mishkin,J.Clark,G.Krueger,andI.Sutskever,
“Learning transferable visual models from natural language supervi- Action Description
sion,”inProceedingsofthe38thInternationalConferenceonMachine
Movestheagentforward(ifpossible)bysampling
Learning, ser. Proceedings of Machine Learning Research, M. Meila MOVEAHEAD fromN(µ=0.25m,σ=0.005m).
andT.Zhang,Eds.,vol.139. PMLR,18–24Jul2021,pp.8748–8763.
[16] S. Y. Gadre, M. Wortsman, G. Ilharco, L. Schmidt, and S. Song,
ROTATELEFT Rotatestheagentleftorrightbysamplingfrom
“Cliponwheels:Zero-shotobjectnavigationasobjectlocalizationand ROTATERIGHT N(µ=30°,σ=0.5°)
exploration,”arXivpreprintarXiv:2203.10421,2022.
[17] D.Shah,B.Osin´ski,S.Levineetal.,“Lm-nav:Roboticnavigationwith
LOOKUP Tiltthecameraoftheagentupwardor
largepre-trainedmodelsoflanguage,vision,andaction,”in6thAnnual
LOOKDOWN downwardby30°
ConferenceonRobotLearning,2022.
[18] C.Huang,O.Mees,A.Zeng,andW.Burgard,“Visuallanguagemaps
DONE Specialactionoftheagenttoterminatetheepisode.
forrobotnavigation,”arXivpreprintarXiv:2210.05714,2022.
[19] A. Majumdar, G. Aggarwal, B. Devnani, J. Hoffman, and D. Batra,
“Zson: Zero-shot object-goal navigation using multimodal goal
TABLEII
embeddings,” 2022. [Online]. Available: https://arxiv.org/abs/2206.
TARGETOBJECTSSELECTEDFOREACHROOMTYPE.
12403
[20] F.Xia,A.R.Zamir,Z.He,A.Sax,J.Malik,andS.Savarese,“Gibson
env:Real-worldperceptionforembodiedagents,”inProceedingsofthe Roomtype Targetobjectcategory
IEEEConferenceonComputerVisionandPatternRecognition(CVPR), Fridge
June2018. Kitchen Kettle
[21] M.Du,F.He,N.Zou,D.Tao,andX.Hu,“Shortcutlearningoflarge Apple
languagemodelsinnaturallanguageunderstanding,”2023. Sofa
[22] S. Wang, R. Veldhuis, C. Brune, and N. Strisciuglio, “What do neural Livingroom Television
networkslearninimageclassification?afrequencyshortcutperspective,” Newspaper
inProceedingsoftheIEEE/CVFInternationalConferenceonComputer Bed
Vision(ICCV),October2023,pp.1433–1442. Bedroom Dresser
[23] L. Scimeca, S. J. Oh, S. Chun, M. Poli, and S. Yun, “Which shortcut Alarmclock
cueswilldnnschoose?Astudyfromtheparameter-spaceperspective,”
CoRR,vol.abs/2110.03095,2021.
[24] R. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A. Wichmann,
wallcolorpermutationispossible(nowallcolorchangesw.r.t.
and W. Brendel, “Imagenet-trained cnns are biased towards texture;
increasing shape bias improves accuracy and robustness,” CoRR, vol. training set). Hence, we run 216 episodes per unique scene.
abs/1811.12231,2018. For the 1-room test set, we can change the target room to 2
[25] D.Gordon,A.Kadian,D.Parikh,J.Hoffman,andD.Batra,“Splitnet:
different wall colors e.g., bedroom from green (train) to blue
Sim2sim and task2task transfer for embodied visual navigation,” in
Proceedings of the IEEE/CVF International Conference on Computer or red. In this case, we distribute the 1080 episodes over 10
Vision(ICCV),October2019. unique scenes (5 layouts and 2 wall color permutations). We
[26] L. Weihs, J. Salvador, K. Kotar, U. Jain, K. Zeng, R. Mottaghi, and
do the same for the 2-room and 3-room test set. Next, we
A. Kembhavi, “Allenact: A framework for embodied AI research,”
CoRR,vol.abs/2008.12760,2020. evenly distribute over the 9 target objects. For instance, for
[27] E.Wijmans,A.Kadian,A.Morcos,S.Lee,I.Essa,D.Parikh,M.Savva, the 0-room test set, we distribute the 216 episodes over the 9
and D. Batra, “Decentralized distributed PPO: solving pointgoal navi-
target objects (24 per target object).
gation,”CoRR,vol.abs/1911.00357,2019.
[28] J.Schulman,P.Moritz,S.Levine,M.I.Jordan,andP.Abbeel,“High-
C. Additional training details
dimensionalcontinuouscontrolusinggeneralizedadvantageestimation,”
in 4th International Conference on Learning Representations, ICLR
Table III details the hyperparameters we set for all of our
2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Pro-
ceedings,Y.BengioandY.LeCun,Eds.,2016. training runs. We use DD-PPO [27] and Generalized Advan-
tage Estimation (GAE) [28], parameterized by λ=0.95.
APPENDIX
A. Target object selection
We select 3 target objects per room type. The selection TABLEIII
is based on the object’s overall frequency of occurrence in TRAININGHYPERPARAMETERS.
ProcTHOR-10kandiftheyhaveaclearsemanticrelationwith
Hyperparameter Value
one of the room types. We select different sized objects for
No.ofGPUs 2
each room type. Table II shows an overview. No.environmentsperGPU 20
Rolloutlength 192
B. Distribution evaluation episodes No.mini-batchesperrollout 1
PPOepochs 4
We evenly distribute 1080 evaluation episodes over the
Discountfactor(γ) 0.99
9 target objects, 5 scene layouts and possible wall color GAE[28]parameter(λ) 0.95
permutations for each test set. As there are more possible Valuelosscoefficient 0.5
Entropylosscoefficient 0.01
permutationsforincreasingnumberofwallcolorchanges,the
PPOclipparameter(ϵ) 0.1
number of episodes per unique scene (combination of layout Gradientclipnorm 0.5
and wall color permutation) decreases. For example, for the Optimizer Adam
Learningrate 3e-4
0-room test set, only 5 unique scenes are possible as only 1