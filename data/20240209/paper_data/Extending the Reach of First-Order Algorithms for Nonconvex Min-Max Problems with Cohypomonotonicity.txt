Extending the Reach of First-Order Algorithms for Nonconvex
Min-Max Problems with Cohypomonotonicity
Ahmet Alacaoglu∗ Donghwan Kim† Stephen J. Wright‡
Abstract
We focus on constrained, L-smooth, nonconvex-nonconcave min-max problems either satisfying ρ-
cohypomonotonicityoradmittingasolutiontotheρ-weaklyMintyVariationalInequality(MVI),where
larger values of the parameter ρ > 0 correspond to a greater degree of nonconvexity. These problem
classes include examples in two player reinforcement learning, interaction dominant min-max problems,
and certain synthetic test problems on which classical min-max algorithms fail. It has been conjectured
that first-order methods can tolerate value of ρ no larger than 1, but existing results in the literature
L
have stagnated at the tighter requirement ρ < 1 . With a simple argument, we obtain optimal or
2L best-known complexity guarantees with cohypomonotonicity or weak MVI conditions for ρ < 1. The
L
algorithms we analyze are inexact variants of Halpern and Krasnosel’ski˘ı-Mann (KM) iterations. We
alsoprovidealgorithmsandcomplexityguaranteesinthestochasticcasewiththesamerangeonρ. Our
maininsightfortheimprovementsintheconvergenceanalysesistoharnesstherecentlyproposedconic
nonexpansiveness propertyofoperators. Asbyproducts,weprovidearefinedanalysisforinexactHalpern
iteration and propose a stochastic KM iteration with a multilevel Monte Carlo estimator.
1 Introduction
We consider the problem
minmaxf(u,v), (1.1)
u∈U v∈V
whereU ⊆Rm,V ⊆Rn areclosedconvexsetsadmittingefficientprojectionoperatorsandf: Rm×Rn →R
is a function such that ∇ f(u,v) and ∇ f(u,v) are Lipschitz continuous. The general setting where f(u,v)
u v
is allowed to be nonconvex-nonconcave is extremely relevant in machine learning (ML), with applications in
generative adversarial networks (GANs) [Goodfellow et al., 2014] and adversarial ML [Madry et al., 2018].
Yet, at the same time, such problems are extremely challenging to solve, with documented hardness results,
see e.g., Daskalakis et al. [2021]. As a result, an extensive literature has arisen about special cases of the
nonconvex-nonconcaveproblem(1.1)forwhichalgorithmswithgoodconvergenceandcomplexityproperties
can be derived [Diakonikolas et al., 2021, Bauschke et al., 2021, Lee and Kim, 2021, Pethick et al., 2022,
2023a,b, Gorbunov et al., 2023, B¨ohm, 2022, Cai et al., 2022, Cai and Zheng, 2022, Hajizadeh et al., 2023,
Kohlenbach, 2022, Anonymous, 2024a,b, Grimmer et al., 2023, Tran-Dinh and Luo, 2023].
To describe these special cases of (1.1), we state the following nonmonotone inclusion problem, which
generalizes (1.1):
Find x⋆ ∈Rd such that 0∈F(x⋆)+G(x⋆), (1.2)
whereF: Rd →Rd isL-LipschitzandG: Rd ⇒Rd ismaximallymonotone. Mappingthisproblemtofinding
stationary points of (1.1) is standard by setting
x=(cid:0)u(cid:1)
,
F(x)=(cid:0)∇uf(u,v)(cid:1)
and
G(x)=(cid:0)∂ιU(cid:1)
, where ι is
v −∇vf(u,v) ∂ιV U
the indicator function for set U. The nonmonotonicity in problem (1.2) is due to nonconvex-nonconcavity
of problem (1.1).
∗WisconsinInstituteforDiscovery,UniversityofWisconsin–Madison. alacaoglu@wisc.edu
†DepartmentofMathematicalSciences,KAIST.donghwankim@kaist.ac.kr
‡DepartmentofComputerSciences,UniversityofWisconsin–Madison. swright@cs.wisc.edu
1
4202
beF
7
]CO.htam[
1v17050.2042:viXraThe main additional assumption we make is that F +G is ρ-cohypomonotone. Recalling the standard
definition gra(F +G)={(x,u)∈Rd×Rd: u∈(F +G)(x)}, ρ-cohypomonotonicity is defined as
⟨u−v,x−y⟩≥−ρ∥u−v∥2
(1.3)
∀(x,u)∈gra(F +G) and ∀(y,v)∈gra(F +G),
for ρ> 0, see [Bauschke et al., 2021, Def. 2.4]. When (1.3) holds only for y = x⋆, it is also called the weak
MVI condition or ρ-star-cohypomonotonicity, due to [Diakonikolas et al., 2021]. For ρ > 0, the weak MVI
condition requires the existence of a solution x⋆ to the ρ-weakly MVI:
⟨u,x−x⋆⟩≥−ρ∥u∥2 ∀(x,u)∈gra(F +G). (1.4)
For standard monotone operators (or convex-concave instances of (1.1)), the inner product in (1.3) is lower
bounded by 0. The assumption (1.3) allows the right-hand side to be negative, allowing nonmonotonicity
of F +G or nonconvex-nonconcavity of f(u,v), while the limit of nonmonotonicity is determined by ρ > 0.
These two assumptions, cohypomonotonicity or weak MVI, are required in the extensive literature cited at
the end of the first paragraph.
In this paper, we extend the range of ρ, doubling the upper limit of 1 considered in the previous works,
2L
thus allowing a wider range of nonconvex problems of the form (1.1) to be solved by first-order algorithms,
while ensuring optimal or best-known complexity guarantees.
Motivation. Cohypomonotonicity and weak MVI conditions, defined in (1.3) and (1.4), allowed
progress to be made in understanding the behavior of first-order algorithms for structured nonconvex-
nonconcave problems, in a wide variety of works cited at the end of first paragraph. On the one hand,
these assumptions are not as general as one might desire: They have not been shown to hold for problems
arisingingenerativeoradversarialML.Ontheotherhand, theyhavebeenproventoholdforotherrelevant
problems in ML.
Examples where cohypomonotonicity holds include the interaction dominant min-max problems (Exam-
ple 1.2) and some stylized worst-case nonconvex-nonconcave instances [Hsieh et al., 2021, Pethick et al.,
2023b] (see also [Bauschke et al., 2021, Sections 5, 6]). The relaxed assumption of having a weak MVI
solution is implied by star (and quasi-strong) monotonicity [Loizou et al., 2021] or existence of a solution
to MVI [Dang and Lan, 2015], the latter being relevant in the context of policy gradient algorithms for
reinforcement learning (RL) [Lan, 2023]. Weak MVI condition is satisfied for the following RL problem.
Example 1.1. von Neumann’s ratio game: This is a simple two player stochastic game [Neumann, 1945,
Daskalakis et al., 2020, Diakonikolas et al., 2021]. Using the standard definition of the simplex ∆d = {x ∈
Rd: ,x≥0,(cid:80)d x =1}, the problem is
i=1 i
⟨x,Ry⟩
min max ,
x∈∆my∈∆n ⟨x,Sy⟩
where R ∈ Rm×n, S ∈ Rm×n and ⟨x,Sy⟩ > 0 ∀(x,y) ∈ ∆m ×∆n. As described in Diakonikolas et al.
+
[2021], it is easy to construct instances of this problem where it satisfies ρ-weakly MVI condition, but not
cohypomonotonicity. ♦
Example 1.2. Interaction dominant min-max problems [Grimmer et al., 2023]: We say that f in (1.1) is
α(r)-interaction dominant if it satisfies for all z =(cid:0)u(cid:1) ∈Rn+m that
v
∇2 f(z)+∇2 f(z)(r−1Id−∇2 f(z))−1∇2 f(z)⪰α(r)Id,
xx xy yy yx
−∇2 f(z)+∇2 f(z)(r−1Id+∇2 f(z))−1∇2 f(z)⪰α(r)Id.
yy yx xx xy
Interaction is captured by the second terms on the left-hand side of each condition. The problem is called
(nonnegative) interaction dominant if these terms dominate the smallest eigenvalue of ∇2 f and largest
xx
eigenvalue of ∇2 f, i.e., α(r) ≥ 0. This is equivalent to the r-cohypomonotonicity of F [Hajizadeh et al.,
yy
2023, Prop. 1]. ♦
2Upper bound Oracle
Assumption Reference Constraints
of ρ complexity
cohypomonotone Cai and Zheng [2022] 1 ✓ O(ε−1)
60L
Cai et al. [2022], Pethick et al. [2023b]
Lee and Kim [2021], Tran-Dinh [2023] 1 ✓ O(ε−1)
2L
Gorbunov et al. [2023]
Cai et al. [2024] 0.7 × O(cid:101)(ε−1)
L
Theorem 2.1 1 ✓ O(cid:101)(ε−1)
L
weak MVI Diakonikolas et al. [2021]‡ 1 × O(ε−2)
8L
B¨ohm [2022]‡ 1 × O(ε−2)
2L
Cai and Zheng [2022] √1 ✓ O(ε−2)
12 3L
Anonymous [2024a]‡ 1 ✓ O(cid:101)(ε−2)
3L
Pethick et al. [2022] 1 ✓ O(ε−2)
2L
Anonymous [2024b] 0.63 × O(ε−2)
L
Theorem 3.1 1 ✓ O(cid:101)(ε−2)
L
Table 1: Comparisonoffirst-orderalgorithmsfordeterministicproblems. Complexityreferstothenumberoforacle
calls to get dist(0,(F +G)(x)) ≤ ε. See also Remark 2.3. ‡These works defined weak MVI as ⟨F(x),x−x⋆⟩ ≥
−γ∥F(x)∥2, i.e., γ =2ρ.
2
The limit for the parameter ρ in (1.3) and (1.4) for which convergence can be proved in most algorithms
seemstohavestagnatedatρ< 1 . TwoexceptionsexistforaspecialcaseofoursettingwhenG≡0,which
2L
corresponds in view of (1.1) to an unconstrained problem. First is the recent work [Anonymous, 2024b]
that claimed to improve the limit of ρ for weak MVI to ≈ 0.63 with a rather complicated analysis. The
L
rate obtained is also suboptimal under cohypomonotonicity. This work conjectured (but did not prove) 1
L
as the maximum limit for ρ and also did not provide any algorithm achieving this. For an unconstrained
cohypomonotone problem, [Cai et al., 2024, Corollary 4.5] also showed possibility of obtaining guarantees
with ρ< √1 ≈ 0.7. Relevant citations and discussions appear in Table 1 and Appendix D.
2L L
First-orderoracles. Asstandardintheoperatorsplittingliterature(seee.g.,BauschkeandCombettes
[2017]), a first-order oracle call for (1.2) consists of one evaluation of F and one resolvent of G (see (1.5)).
In the context of the min-max problem (1.1), this requires computation of gradients ∇ f(u,v),∇ f(u,v)
u v
together with projections on sets U,V. (All works in Table 1 have the same oracle access.)
Contributions. We show how to increase the range of the cohypomonotonicity parameter to ρ < 1
L
whilemaintainingfirst-orderoraclecomplexityO(cid:101)(ε−1)forfindingapointxsuchthatdist(0,(F+G)(x))≤ε.
Suchacomplexityisoptimal(uptoalogfactor)evenformonotoneproblems[YoonandRyu,2021,Section3].
With weak MVI and the improved range of ρ < 1, we show complexity O(cid:101)(ε−2) for dist(0,(F +G)(x)) ≤ ε
L
which is the best-known (up to a log factor) under this assumption. Table 1 summarizes known results on
complexity and the upper bound of ρ.
Thanks to the modularity of our approach, we show some corollaries. First, we provide complexity
guarantees for stochastic versions of our problems where the operator F is accessed via unbiased oracles
F(·,ξ) (that is, E [F(x,ξ)] = F(x)). We also discuss how to improve the best-known ρ-independent and
ξ
ρ-agnostic complexitybounds. Onatechnicalside,wetightentheanalysisofHalperniterationwithinexact
resolvent computations, an ingredient that is critical for the stochastic extension. Similarly, to obtain the
best-knowncomplexityforstochasticproblemsunderweakMVI,weincorporatethemultilevelMonteCarlo
estimator to KM iteration to control the bias in subproblem solutions.
31.1 Preliminaries
Notation. We denote the ℓ norm as ∥·∥. Given G: Rd ⇒ Rd, we use standard definitions graG =
2
{(x,u) ∈ Rd ×Rd: u ∈ G(x)} and dist(0,G(x)) = min ∥u∥. Domain of an operator is defined as
u∈G(x)
domG = {x ∈ Rd: G(x) ̸= ∅}. The operator G is maximally monotone (resp. cohypomonotone or hy-
pomonotone)ifitsgraphisnotstrictlycontainedinthegraphofanyothermonotone(resp. cohypomonotone
or hypomonotone) operator.
An operator F: Rd ⇒ Rd, given (x,u) ∈ graF and (y,v) ∈ graF, is (i) γ-strongly monotone if ⟨u−
v,x−y⟩ ≥ γ∥x−y∥2 with γ > 0 and monotone if the inequality holds with γ = 0; (ii) ρ-hypomonotone if
⟨u−v,x−y⟩ ≥ −ρ∥x−y∥2 with ρ > 0. An operator F: Rd → Rd is (iii) L-Lipschitz if ∥F(x)−F(y)∥ ≤
L∥x−y∥; (iv) nonexpansive if F is 1-Lipschitz; (v) γ-cocoercive if ⟨F(x)−F(y),x−y⟩≥γ∥F(x)−F(y)∥2
with γ >0. We refer to star variants of these properties (e.g., star-cocoercive) when they are required only
at (y,v)=(x⋆,0) where 0∈F(x⋆).
The resolvent of an operator F: Rd ⇒Rd is defined as
J =(Id+F)−1. (1.5)
F
The resolvent generalizes the well-known proximal operator that has been ubiquitous in optimization and
ML, where F is typically the subdifferential of a regularizer function, e.g., ℓ norm. Favorable properties
1
of the resolvent are well-known when F is monotone [Bauschke and Combettes, 2017]. Meanwhile, in our
nonmonotone case, immense care must be taken in utilizing this object, as it might even be undefined. A
comprehensive reference for the properties of resolvent of a nonmonotone operator is [Bauschke et al., 2021].
We review and explain the results relevant to our work in the sequel.
The algorithms we analyze are based on the classical Halpern [Halpern, 1967] and Krasnosel’ski˘ı-Mann
(KM) [Krasnosel’skii, 1955, Mann, 1953] iterations. Given an operator T: Rd → Rd, Halpern iteration is
defined as
x =β x +(1−β )T(x ), (1.6)
k+1 k 0 k k
for a decreasing sequence {β } ∈ (0,1) and initial point x . The KM iteration, with a fixed β ∈ (0,1), is
k 0
defined as
x =βx +(1−β)T(x ). (1.7)
k+1 k k
Conic nonexpansiveness. The key to relaxing the range of ρ parameter for both assumptions is to
harness the algorithmic consequences of conic nonexpansiveness, the notion introduced by the influential
work of Bauschke et al. [2021] that also inspired our developments. We say that T: Rd →Rd is α-conically
nonexpansive with α>0 when there exists a nonexpansive operator N: Rd →Rd such that T =(1−α)Id+
αN,see[Bauschkeetal.,2021,Def. 3.1]. ThisequivalentlymeansthataparticularcombinationofIdandT
is nonexpansive: ∥((1−α−1)Id+α−1T)(x−y)∥≤∥x−y∥2. An important characterization of this property
given in [Bauschke et al., 2021, Cor. 3.5(iii)] is that T is α-conically nonexpansive if and only if Id−T is
1 -cocoercive. We also consider the star variants (in the sense defined in the Notation paragraph) of these
2α
properties and characterizations, which are detailed in Appendix B.1.1.
Assumption 1. The operator F: Rd → Rd is L-Lipschitz and G: Rd ⇒ Rd is maximally monotone. The
solution set for the problem (1.2) is nonempty.
Assumption 1 is standard, see Facchinei and Pang [2003], and is required throughout the text. Mono-
tonicity is not assumed for F. Lipschitzness of F corresponds to smoothness of f in context of (1.1) and
maximal monotonicity of G is satisfied when we have constraint sets given in (1.1) but also when we have
convex regularizers that can be added on (1.1) (e.g., ∥·∥ ).
1
Assumption 2. The operator F +G is maximally ρ-cohypomonotone (see (1.3) for the definition).
Assumption 2 is abundant in the recent literature for nonconvex-nonconcave optimization [Lee and Kim,
2021, Bauschke et al., 2021, Cai et al., 2022, Cai and Zheng, 2022, Gorbunov et al., 2023, Pethick et al.,
2023b]. An instance is provided in Example 1.2 with further pointers to related problems are given in
Section 1. Assumption 2 is required only for the results in Sections 2 and 4.1.
Assumption 3. There exists a ρ-weakly MVI solution to the problem (1.2) (see (1.4) for the definition).
4Assumption 3 is weaker than Assumption 2 as it is only required on the ray to a solution, see also
Example 1.1. Assumption 3, used in Sections 3 and 4.2, is also widespread in the recent literature for
nonconvex-nonconcave optimization [Diakonikolas et al., 2021, Pethick et al., 2022, 2023a, Cai et al., 2022,
Anonymous, 2024a,b, B¨ohm, 2022].
2 Algorithm and Analysis under Cohypomonotonicity
Algorithm 1 Inexact Halpern iteration for problems with cohypomonotonicity
Input: Parameters β = 1 ,η,L,ρ, α = 1− ρ, K ≥ 1, initial iterate x ∈ Rd, subroutine FBF given in
k k+2 η 0
Algorithm 2
for k =0,1,2,...,K−1 do
(cid:108) √ (cid:109)
J(cid:101)η(F+G)(x k)=FBF(x k,T,G,Id+ηF −x k,1+ηL) where T = 4( 11 −+ ηη LL)log(98 k+2log(k+2))
x
k+1
=β kx 0+(1−β k)((1−α)x k+αJ(cid:101)η(F+G)(x k))
end for
Algorithm 2 FBF(z ,T,A,B,L ) from [Tseng, 2000]
0 B
Input: Parameter τ = 1 , initial iterate z ∈Rd
2LB 0
for t=0,1,2,...,T −1 do
z =J (z −τB(z ))
t+1/2 τA t t
z =z +τB(z )−τB(z )
t+1 t+1/2 t t+1/2
end for
2.1 Algorithm Construction and Analysis Ideas
Recallthedefinitionsofresolvent(1.5)andcohypomonotonicity(1.3). Wesketchthealgorithmicconstruction
and analysis ideas which will be expanded on in Section 2.2.
(I) We know that Halpern iteration in (1.6) with β = 1 has optimal rate when Id−T is cocoercive,
k k+2
see [Sabach and Shtern, 2017, Lieder, 2021, Kim, 2021]. That is, one gets η−1∥x −J (x )∥≤ε
k η(F+G) k
with O(ε−1) evaluations of J .
η(F+G)
(II) WhenF+Gismaximallyρ-cohypomonotone(perAssumption2),weknowfromBauschkeetal.[2021]
(with precise pointers in App. A.1) that J is 1 -conically nonexpansive where α = 1− ρ, its
η(F+G) 2α η
domain is Rd and it is single-valued when ρ < 1. Consequently, Id−J is α-cocoercive. Then,
η η(F+G)
one can use the result in (I).
We next see a high level discussion on the approximate computation of J .
η(F+G)
(III) Since F is L-Lipschitz, we have that F is L-hypomonotone by Cauchy-Schwarz inequality, i.e.,
⟨F(x)−F(y),x−y⟩≥−L∥x−y∥2.
Hence, Id+ηF is (1−ηL)-strongly monotone.
By definition, we have x⋆ =J (x )=(Id+η(F +G))−1(x ). Existence and uniqueness of x⋆ is
k η(F+G) k k k
guaranteed by (II) when ρ<η. By definition, x⋆ is the solution of the problem
k
0∈(Id+η(F +G))(x⋆)−x .
k k
Hence,computationoftheresolventisastronglymonotoneinclusionproblemwhereId+ηF is(1−ηL)-
strongly monotone and (ηL+1)-Lipschitz, and G is maximally monotone. In view of (1.1) this also
corresponds to a strongly convex-strongly concave problem, also known as the proximal operator of f
with a center point x over constraint sets U,V.
k
5(IV) Any optimal variational inequality (or monotone inclusion) algorithm, such as the forward-backward-
(cid:16) (cid:17)
forward (FBF) [Tseng, 2000], gives xˆ
k
with ∥xˆ
k
−J η(F+G)(x k)∥2 ≤ ε2
k
with complexity O(cid:101) 11 −+η ηL
L
.
In summary, our requirements are ρ < 1 for ensuring well-definedness of the resolvent, as per (II), and
η
1−ηL>0 for ensuring strong monotonicity for efficient approximation of the resolvent, as per (III). Hence,
we need ρ<η < 1, leading to the claimed improved range on ρ.
L
Item (II) refers to the resolvent of η(F +G), which cannot be evaluated exactly in general with stan-
dard first-order oracles. We approximate J , which leads to the inexact Halpern iteration, similar to
η(F+G)
Diakonikolas et al. [2021], Cai et al. [2024]. Note that in the context of problem (1.1), approximating the
resolvent corresponds to computing approximation of proximal operator for function f which is a strongly
convex-strongly concave min-max problem.
In the next section, by extending the arguments in [Diakonikolas, 2020, Lemma 12] and [Cai et al., 2024,
Lemma C.3] to accommodate conic nonexpansiveness, we show that η−1∥x −J (x )∥≤ε, where the
k η(F+G) k
(cid:16) (cid:17)
number of (outer) Halpern iterations is O
∥x0−x⋆∥
, when we approximate the resolvent to an accuracy of
(η−ρ)ε
poly(cid:0)1(cid:1)
. To achieve this, we can run a subsolver as per (IV), with
O(cid:101)(cid:16) 1+ηL(cid:17)
calls to evaluations of F and
k 1−ηL
resolvents of G. By combining the complexities at outer and inner levels, we obtain the optimal first-order
complexity under ρ< 1.
L
Discussion. From the construction (I)-(IV), we see that the ingredients of our approach are based on
known results. This raises the question: what insight makes it possible to go beyond the ρ < 1 barrier?
2L
Thekeyisconic nonexpansiveness, theinfluentialnotionintroducedbyBauschkeetal.[2021]. Inparticular,
previous results on first-order complexity for nonmonotone problems (including Pethick et al. [2023b] who
utilized a similar algorithmic construction based on KM as ours in Section 3) used nonexpansiveness of the
resolvent,whichasksforthestringentrequirementρ< 1 . Thisyields 1-cocoercivityofId−J ,which
2L 2 η(F+G)
allows Halpern or KM iteration to be analyzed in a standard way.
Our main starting insight is that, from the viewpoint of the analysis of Halpern iteration (or KM in
Section 3), we need only cocoercivity of Id−J , not necessarily with the constant 1. In particular,
η(F+G) 2
withconicnonexpansiveness,whichrelaxesnonexpansiveness,westillobtainthatId−J iscocoercive,
η(F+G)
just with a constant (other than 1) that now depends on ρ, that is, 1− ρ. Hence, as long as we stay in the
2 η
range ρ<η, Halpern iteration can be analyzed for ρ<η < 1, at essentially no cost.
L
2.2 Analysis
We now analyze the construction described in the previous section, given as Algorithm 1. We start with the
main result, see Appendix A.4 for its proof.
Theorem 2.1. Let Assumptions 1 and 2 hold. Let η < 1 in Algorithm 1 and suppose ρ < η. For any
L
k =1,...,K, we have that (x ) from Algorithm 1 satisfies
k
1 16∥x −x⋆∥2
∥x −J (x )∥2 ≤ 0 .
η2 k η(F+G) k (η−ρ)2(k+1)2
Thenumberoffirst-orderoraclesusedatiterationk isupperboundedby2T whereT isdefinedinAlgorithm1.
Corollary 2.2. Under the setting of Theorem 2.1, for any ε > 0, we have η−1∥(Id−J )(x )∥ ≤ ε,
η(F+G) K
(cid:108) (cid:109)
for K ≤
4∥x0−x∗∥
and first-order oracle complexity
(η−ρ)ε
(cid:18)
(1+ηL)∥x
−x⋆∥(cid:19)
O(cid:101) 0 .
ε(η−ρ)(1−ηL)
Remark2.3. Thedefinitionofx⋆givesthat(Id−J )(x⋆)=0and(Id−J )(x )isindeedthefixed
η(F+G) η(F+G) k
pointresidual,whichisastandardwaytomeasureoptimalityforfixedpointiterations,seee.g.,[RyuandYin,
2022, Section 2.4.2]. Based on Cor. 2.2, it is straightforward to produce xout with dist(0,(F +G)(xout))≤ε
asclaimedinTable1,withnochangeintheworst-casecomplexity. ThisisclearwhenG≡0. Inthegeneral
case, see [Cai et al., 2024, Lem. C.4].
6Remark 2.4. The constant in our complexity deteriorates as ρ gets close to η which is the same case as
most of the works included in Table 1. It is straightforward to make our bound ρ-independent in view of
Pethick et al. [2023b] by simply expressing ρ as a fraction of η, e.g. assume ρ < 9η. Then, at the expense
10
(cid:16) (cid:17)
of a constant multiple of 10, we have the complexity O(cid:101) (1+ηL)∥x0−x⋆∥ , valid for the range ρ < 9 . In
εη(1−ηL) 10L
comparison, the ρ-independent complexity result in Pethick et al. [2023b] had O(cid:101)(ε−2) for ρ< 1 . A similar
2L
reasoning by slightly restricting the range of ρ can also make the algorithms agnostic to the knowledge of ρ.
Outline of the analysis. We follow the steps sketched in Section 2.1. First, we compute the required
number of outer iterations by using the tools mentioned in (I), (II). Second, we analyze the inner loop
(Algorithm 2) as mentioned in (IV). Finally we piece together these ingredients.
2.2.1 Outer-Loop Complexity
We now analyze Halpern iteration with inexactness in the resolvent computation. See Appendix A.2 for the
proof.
Lemma2.5. LetAssumptions1and2hold. Supposethattheiterates(x )ofAlgorithm1satisfy∥J (x )−
k η(F+G) i
J(cid:101)η(F+G)(x i)∥≤ε
i
for some ε
i
>0 and ρ<η. Then, we have for any K ≥1 that
K(K+1) K+1 K (cid:88)−1(cid:18) (k+1)(k+2)ε2 (cid:19)
∥x −J (x )∥2− ∥x⋆−x ∥2 ≤ k +(k+1)∥R(x )∥ε ,
4 K η(F+G) K Kα2 0 2 k k
k=0
where ∥x −x⋆∥≤∥x −x⋆∥+ α (cid:80)k−1(i+1)ε and R=Id−J .
k 0 k+1 i=0 i η(F+G)
In (2.1) below, we define appropriate values for ε , and show that the number of inner iterations T
i
selected for FBF in Algorithm 1 suffices to achieve the inexactness level ε .
i
This analysis extends Diakonikolas [2020], who studied monotone inclusions, in two aspects. First, we
analyze the rate under conic nonexpansiveness which is the relevant property when the parameter ρ lies
in the range [ 1 , 1). Second, and more importantly, we conduct a tighter error analysis that allows the
2L L
inexactness on the error in resolvent computation (ε k) to be O(cid:101)(k−3/2) instead of the tolerance O(cid:101)(k−3) used
in [Diakonikolas, 2020, Yoon and Ryu, 2022, Cai et al., 2024]. Even though it is not immediately obvious,
this is because the bottleneck term on the bound in Lemma 2.5 is
(cid:80)K−1(k+1)(k+2)ε2
which sums to a
k=0 k
log with ε
k
= O(cid:101)(k−3/2). This tightening becomes important in the stochastic case in Section 4, where the
inner loop does not have an exponential convergence rate. The improvement derives from using a slightly
smaller step size, which helps avoid the main source of looseness in the previous analysis. We discuss this
further following (A.9). See Remark A.3 for a discussion from the viewpoint of nonexpansive operators.
2.2.2 Inner-Loop Complexity
The seminal FBF algorithm of Tseng [2000] is optimal for solving the resolvent subproblem, which is a
strongly monotone inclusion. We provide the derivation of the precise constants appearing in the statement
in Appendix A.3.
Theorem2.6. (See[Tseng,2000,Theorem3.4])LetB beµ-stronglymonotonewithµ>0andL -Lipschitz;
B
A be maximally monotone, and z⋆ = (A+B)−1(0) ̸= ∅. For any ζ > 0, after running Algorithm 2 with
(cid:108) (cid:109)
initial point z 0 for T = 4L µB log∥z0− ζz⋆∥ iterations and τ = 2L1 B, we get
∥z −z⋆∥≤ζ,
T
where the number of calls to evaluations of B and resolvents of A is upper bounded by 2T.
2.2.3 Total complexity
Section 2.1 already shows the key steps in our analysis, but we combine the preliminary results above into
a proof sketch here, to highlight the simplicity of our approach. Full proof is given in Appendix A.4.
7Proof sketch of Theorem 2.1. Denote R=Id−J for brevity. Suppose that ε in Lemma 2.5 satisfies
η(F+G) k
γ∥R(x )∥ 1
ε = √ k , with γ = . (2.1)
k k+2log(k+2) 98
We justify this supposition further below. Then we have by Lemma 2.5 (after multiplying both sides by α)
that
√
αK(K+1) K+1 K (cid:88)−1 (cid:18) αγ2(k+1) αγ k+2(cid:19)
∥R(x )∥2− ∥x −x⋆∥2 ≤ ∥R(x )∥2 + .
4 K Kα 0 k 2log2(k+2) log(k+2)
k=0
We can show by induction from this bound that
4∥x −x⋆∥
∥R(x )∥≤ 0 ∀k ≥0.
k α(k+1)
We see that for K ≤⌈4∥x0−x⋆∥⌉, we are guaranteed to have η−1∥R(x )∥≤ε.
ηαε K
We now calculate the number of inner iterations to reach the accuracy ε (see (2.1)). At iteration k, as
k
per the setup in Theorem 2.6, we set
A≡ηG, B(·)≡(Id+ηF)(·)−x k, z
0
≡x k, z
N
≡J(cid:101)η(F+G)(x k), z⋆ ≡J η(F+G)(x k), ζ ≡ε k.
hence z −z⋆ =(Id−J )(x )=R(x ). B is L ≡(1+ηL)-Lipschitz and (1−ηL)-strongly monotone
0 η(F+g) k k B
due to Fact A.1(iv). Existence of z⋆ is guaranteed by Fact A.1(i).
By matching these definitions with Algorithm 1, we see by invoking Theorem 2.6 that the number of
inner iterations used at step k to obtain ∥J η(F+G)(x k)−J(cid:101)η(F+G)(x k)∥≤ε
k
is
(cid:24) (cid:25)
4(1+ηL) ∥R(x )∥
T ≡ log k ,
1−ηL ε
k
by the settings of z , z⋆, R(x ), and ζ above, along with ε defined in (2.1). This value is precisely T used
0 k k
in Algorithm 1, which justifies our application of Lemma 2.5. By combining the bounds on inner and outer
iterations, we conclude.
3 Algorithm and Analysis under weak MVI
3.1 Algorithm Construction and Analysis Ideas
Algorithm 3 Inexact KM iteration for problems with weak MVI
Input: Parameters η,L,ρ, α=1−ρ, K >0, initial iterate x ∈Rd, subroutine FBF given in Algorithm 2
η 0
for k =0,1,2,...,K−1 do
(cid:108) (cid:109)
J(cid:101)η(F+G)(x k)=FBF(x k,T,G,Id+ηF,1+ηL), where T = 4( 11 −+ ηη LL)log(8(k+1)log2(k+2))
x
k+1
=(1−α)x k+αJ(cid:101)η(F+G)(x k)
end for
Weturntotheweak MVI condition ofAssumption3,which(asmentionedinSection1.1)isweakerthan
cohypomonotonicity. Thebest-knowncomplexityunderthisassumptionisO(ε−2): thelowerpartofTable1
outlines existing results. Our aim is to obtain O(cid:101)(ε−2) complexity for the extended range ρ< 1. The steps
L
of our construction are as follows.
(i) WeknowthatKMiteration(1.7),whenId−T isstar-cocoercive,getsη−1∥x −J (x )∥≤εwith
k η(F+G) k
O(ε−2) evaluations of J [Groetsch, 1972].
η(F+G)
8(ii) We learn from Bauschke et al. [2021] that J has domain Rd and is single-valued when F is L-
η(F+G)
Lipschitzandη < 1. LemmaB.2givesthatJ is 1 -conicallystar-nonexpansive,withα=1−ρ,
L η(F+G) 2α η
leading to Id−J being α-star-cocoercive.
η(F+G)
Thus, we require ρ<η. Then, as per (ii), KM applied to Id−J requires O(ε−2) evaluations of
η(F+G)
J to find x such that η−1∥x−J (x)∥≤ε.
η(F+G) η(F+G)
(iii) Since F is Lipschitz and G is maximally monotone, we can estimate J as before (via (III) and
η(F+G)
(IV) of Section 2), with a linear rate of convergence when η < 1. The existence of a solution to the
L
subproblem is guaranteed by item (ii). The inner iterations introduce a logarithmic factor into the
total complexity. As a result, the range for ρ is again ρ<η < 1.
L
Even with inexactness, Alg. 3 is classical; see [Facchinei and Pang, 2003, Theorem 12.3.7], Combettes
[2001] and Combettes and Pennanen [2002]. We analyze this scheme for problems with weak MVI solutions
and characterize the first-order oracle complexity. Pethick et al. [2023b] recently analyzed a similar scheme
under cohypomonotonicity, by using star-nonexpansiveness of the resulting operator.1 Our main difference
regarding the results in this section is that we harness the milder property of star-conic nonexpansiveness
to improve the range of ρ (see also Bartz et al. [2022] for a similar idea by using exact resolvent). We also
approximatetheresolventdifferentlybyviewingitasastronglymonotoneproblemandapplyinganoptimal
algorithm for this problem. FBF can be replaced with other optimal algorithms like [Malitsky and Tam,
2020], showing the modularity of our approach.
The key insight for extending the upper bound of ρ to 1 is similar to that of Section 2. The difference
L
is that the analysis of Halpern iteration requires conic nonexpansiveness between any pair of points in the
space, making it unsuitable with weak MVI. By contrast, the KM iteration can be analyzed with conic
nonexpansiveness restricted on a ray to the solution, a property that is a consequence of weak MVI. Star-
conic nonexpansiveness, while not defined explicitly in Bauschke et al. [2021], directly follows by adapting
the corresponding results therein by using ρ-weak MVI condition instead of cohypomonotonicity; see App.
B.1.1.
3.2 Analysis
Similar to Section 2, we start with the main complexity result, under weak MVI. Its proof appears in
Appendix B.4.
Theorem 3.1. Let Assumptions 1 and 3 hold. Let η < 1 in Algorithm 3 and suppose ρ < η. For any
L
K ≥1, we have
1 K (cid:88)−1 1
∥x −J (x )∥2 ≤
11∥x 0−x⋆∥2
.
K η2 k η(F+G) k (η−ρ)2K
k=0
Thenumberoffirst-orderoraclesusedatiterationk isupperboundedby2T whereT isdefinedinAlgorithm3.
Corollary 3.2. Under the setting of Theorem 3.1, for any ε>0, we have for some xout ∈{x ,...,x }
0 K−1
(cid:108) (cid:109)
that η−1∥(Id−J )(xout)∥≤ε for K ≤ 11∥x0−x∗∥2 with first-order oracle complexity
η(F+G) (η−ρ)2ε2
(cid:32) (cid:33)
(1+ηL)∥x −x⋆∥2
O(cid:101) 0 .
ε2(η−ρ)2(1−ηL)
See Remark 2.3 for details to convert this result to produce a point with dist(0,(F +G)(xout))≤ε as in
Table 1.
Remark 3.3. This result is for the best iterate, that is, xout = argmin ∥(Id−J )(x)∥,
x∈{x0,...,xk−1} η(F+G)
consistent with existing results for weak MVI, see Diakonikolas et al. [2021], Pethick et al. [2022], Cai and
Zheng [2022].
1ThisworkclaimedthatsomeoftheirresultsextendtoaccommodateweakMVIconditionaswell.
9Remark 3.4. Note that xout as defined in Remark 3.3 is not computable since we do not have ac-
cess to J (x ). For the unconstrained case, i.e., G ≡ 0, we can show the result with xout =
η(F+G) k
argmin ∥Fx∥2, which is computable. For the constrained problem (1.1), we can handle this
x∈{x0,...,xK−1}
issue by slightly changing how J(cid:101)η(F+G) is calculated and requiring the knowledge of the target accuracy ε,
withnochangeintheorderofcomplexitybounds. WepresentAlg. 3initscurrentformsothatitisanytime,
not requiring the target accuracy as an input. The details for making xout computable are in App. B.5. We
can also present this result as an expected bound for a randomly selected xout, like [Diakonikolas et al., 2021,
Thm. 3.2(ii)].
Outer-loop complexity. We now analyze the iteration complexity of the outer loop; see App. B.2
for a proof which is a modification of Combettes [2001] and Bartz et al. [2022] to accommodate star-conic
nonexpansiveness and inexact resolvent computations.
Lemma3.5. LetAssumptions1and3hold. Supposethattheiterates(x )ofAlgorithm3satisfy∥J (x )−
k η(F+G) k
J(cid:101)η(F+G)(x k)∥≤ε
k
for some ε
k
>0 and ρ<η. Then, we have for K ≥1 that
K (cid:88)−1 2η2 K (cid:88)−1 4η K (cid:88)−1
∥(Id−J )(x )∥2− ∥x −x⋆∥2 ≤6 ε2 + ∥x −x⋆∥ε ,
η(F+G) k (η−ρ)2 0 k η−ρ k k
k=0 k=0 k=0
where ∥x −x⋆∥≤∥x −x⋆∥+(1−ρ/η)ε .
k k−1 k−1
Total Complexity. The sketch of the proof of Theorem 3.1 follows Section 2.2.3 closely. We use
Lemma 3.5 instead of Lemma 2.5. The definition of ε is slightly different, as can be noticed by the number
k
of inner iterations T in Algorithm 3. However, with the same argument in Section 2.2.3, we can show that
T as in Algorithm 3 is sufficient to obtain ε .
k
4 Algorithms and Analyses with Stochasticity
In this case, F in (1.2) is accessed via unbiased oracles. Let Ξ denote the underlying distribution that we
can sample from.
Assumption 4. The stochastic first-order oracle (SFO) F : Rd →Rd satisfies
ξ
F(x)=E [F (x)] and E ∥F (x)−F(x)∥2 ≤σ2.
ξ∼Ξ ξ ξ∼Ξ ξ
In view of (1.1), this corresponds to using stochastic gradients F(x) =
(cid:0)∇ufξ(u,v)(cid:1)
. Table 2, with
−∇vfξ(u,v)
comparisons for stochastic problems, is in Appendix C.
4.1 Cohypomonotone Case
In this case, Algorithm 1 will call FBF with stochastic oracles F(cid:101)(x t) := F ξt(x t) for ξ
t
∼ Ξ to approximate
J(cid:101)η(F+G):
(cid:16) (cid:17)
J(cid:101)η(F+G)(x k)=FBF x k,T,G,Id+ηF(cid:101),1+ηL , (4.1)
where T =⌈1734(k+2)3log2(k+2)(1−ηL)−2⌉.
Corollary 4.1. Let Assumptions 1, 2 and 4 hold. Let η < 1 in Alg. 1, ρ<η and use (4.1) for computing
L
J(cid:101)η(F+G) (see Alg. 4). For any ε>0, we have η−1E∥(Id−J η(F+G))(x K)∥≤ε for the last iterate, with SFO
complexity O(cid:101)(ε−4).
The proof, provided in App. C.2.1 is the stochastic adaptation of Section 2. Our tighter analysis for
the level of inexactness (which is highlighted after Lemma 2.5) is the main reason we could get the O(cid:101)(ε−4)
complexity. The inexactness level required in the existing analyses in Diakonikolas [2020], Cai et al. [2024]
would instead result in O(cid:101)(ε−7) complexity.
10Remark 4.2. The previous last iterate result for constrained, cohypomonotone, stochastic problems by
[Pethick et al., 2023b, Corollary E.3(ii)] was O(cid:101)(ε−16). This result also required increasing batch sizes in
the inner loop and ρ < 1 . For unconstrained problems, Chen and Luo [2022] showed an improved O(cid:101)(ε−2)
2L
expected complexity for ρ < 1 with some drawbacks described in App. D. It is an open question to get a
2L
similar complexity improvement in our setup.
Remark 4.3. Pethicketal.[2023a]hascomplexityO(cid:101)(ε−4)foraconstrainedproblemwithweakMVI.How-
ever, this work additionally assumed mean-square (MS)-Lipschitzness: E ∥F (x)−F (y)∥2 ≤L2∥x−y∥2
ξ∼Ξ ξ ξ
andadditionaloracleaccesstoquerytheoperatorforthesameseedfortwodifferentpoints: F (x ),F (x ).
ξ k ξ k−1
Thesetwoassumptionsdefineafundamentallydifferenttemplate. Fornonconvexminimization,forexample,
lower bounds improve with these assumptions compared to our standard stochastic approximation setting
in Assumption 4, see Arjevani et al. [2023]. Moreover, the additional assumption might not hold even for
trivial problems: F (x)=x2, F (x)=−x2 where F =F +F is clearly Lipschitz but not MS-Lipschitz.
1 2 1 2
4.2 Weak MVI Case
WenextmodifyAlgorithm3forthestochasticcase. Themainobservationfromtheanalysis(seeLemmaC.8)
is that bounding the bias ∥E[J(cid:101)η(F+G)(x k)]−J η(F+G)(x k)∥ with square root of variance E∥J(cid:101)η(F+G)(x k)−
J η(F+G)(x k)∥2 by Jensen’s inequality is too loose and would give complexity O(cid:101)(ε−6), like [Pethick et al.,
2023b, Cor. E.3(i)].
A natural candidate for a careful bias analysis is the multilevel Monte Carlo (MLMC) technique which
helps control the bias-variance tradeoff [Giles, 2008, Blanchet and Glynn, 2015, Asi et al., 2021, Hu et al.,
2021]. The high level idea is that stochastic KM iteration, in our setting would give O(ε−4) complexity
if we had unbiased samples of J (see, e.g., Bravo and Cominetti [2024]). Obtaining such unbiased
η(F+G)
samples is highly non-trivial since J is an optimization problem. Fortunately, MLMC is a way to get
η(F+G)
an estimator with bias O(ε) and variance O(cid:101)(1) by making, in expectation, O(cid:101)(1) calls to the oracle defined
in Assumption 4. MLMC is used in Asi et al. [2021] for the related proximal point algorithm.
Estimator. Given T ≥1, M ≥1, set for m=1,...,M,
(cid:40)
y0+2I(yI −yI−1) if I ≤T,
J(cid:101) η(m (F) +G)(x k)=
y0, otherwise,
(4.2)
where I ∼Geom(1/2)
and yi =FBF(x k,2i,G,Id+ηF(cid:101),1+ηL) ∀i≥0.
Given M independent draws of this estimator, we define J(cid:101)η(F+G)(x k) = M1 (cid:80)M m=1J(cid:101) η(m (F) +G)(x k). To show
that the scheme is implementable we give the (non-optimized) values of M,T. This is to illustrate that they
are agnostic to unknown quantities {∥x −x⋆∥2,σ2}, unlike some MLMC methods [Chen and Luo, 2022].
0
Proof is in App. C.3.1.
Corollary 4.4. Let Assumptions 1, 3 and 4 hold. In Algorithm 3, set η < 1, α ← √ α , suppose
L k+2log(k+3)
that ρ < η and use (4.2) for computing J(cid:101)η(F+G) (see Algorithm 6) with T ≡ ⌈ min{96(1− αkηL)− ,2
1
}⌉ and M ≡
120α(k+1) 120
⌈672×120(log 2T)⌉. Foranyε>0,wehavethatη−1E∥(Id−J )(xout)∥≤ε,with expectedSFOcomplexity
(1−ηL)2 η(F+G)
O(cid:101)(ε−4) where xout is selected uniformly at random from {x 0,...,x K−1}.
This result is an alternative to Pethick et al. [2023a] which required additional assumptions as explained
in Remark 4.3. In our setting under Assumption 4, the only O(ε−4) complexity was known in the special
case of unconstrained problems (G≡0), due to Diakonikolas et al. [2021] (also obtained in Choudhury et al.
[2023] for a different algorithm). Because of the use of MLMC, our complexity result is expected number of
stochastic oracle calls and hence the four results mentioned in this paragraph complement each other. See
also Table 2.
MLMC is used in conditional/compositional stochastic minimization [Hu et al., 2021], distributionally
robustoptimization[Levyetal.,2020],andstochasticminimizationwithnon-i.i.d. data[DorfmanandLevy,
2022]. Our development of the KM iteration with MLMC can provide the potential to extend some of these
results to stochastic min-max setting.
11A Proofs for Section 2
A.1 Preliminary Results
We start with the properties of the resolvent of a cohypomonotone operator and the properties of the
subproblem for approximating this resolvent. These important points are also sketched in Section 2.1. We
present this preliminary result here for the ease of reference throughout the proofs. Most of the conclusions
follow from the results of Bauschke et al. [2021]. Note that ρ-cohypomonotone in our notation is −ρ-
comonotone in the notation of Bauschke et al. [2021]. See also [Bauschke et al., 2021, Remark 2.5] for these
two conventions.
Fact A.1. Let Assumptions 1 and 2 hold and let η >0. Then, we have
(i) The operator J is single-valued and domJ =Rd when ρ<η.
η(F+G) η(F+G)
(cid:16) (cid:17)
(ii) The operator J is 1 -conically nonexpansive and Id−J is 1− ρ -cocoercive when
η(F+G) 2(1−ρ) η(FG) η
η
ρ<η.
(iii) For any x¯∈Rd, computing J (x¯) is equivalent to solving the problem:
η(F+G)
Find x∈Rd such that 0∈(Id+η(F +G))(x)−x¯. (A.1)
The problem (A.1) has a unique solution when ρ<η.
(iv) The operator Id+ηF is (1+ηL)-Lipschitz and (1−ηL)-strongly monotone when η < 1.
L
Proof. (i) By Assumption 2 and the definition of cohypomonotonicity in (1.3), we have that η(F +G) is
maximally ρ-cohypomonotone. Then for ρ <1, [Bauschke et al., 2021, Corollary 2.14] gives the result.
η η
(ii) Since η(F +G) is maximally ρ-cohypomonotone, [Bauschke et al., 2021, Prop. 3.11(ii)] gives 1 -
η 2(1−ρ)
η
conicnonexpansiveness. CocoercivityofId−J thenfollowsfrom[Bauschkeetal.,2021,Corollary
η(F+G)
3.5(iii)].
(iii) Let us denote x¯⋆ =J (x¯) and use the definition of a resolvent to obtain
η(F+G)
x¯⋆ =J (x¯)=(Id+η(F +G))−1(x¯) ⇐⇒ x¯⋆+η(F +G)(x¯⋆)∋x¯,
η(F+G)
where the existence of x¯⋆ is guaranteed by (i). Rearranging the inclusion gives (A.1). Uniqueness of
the solution is due to (i).
(iv) By Lipschitzness of F and Cauchy-Schwarz inequality, we have
⟨ηF(x)−ηF(y),x−y⟩≥−η∥F(x)−F(y)∥∥x−y∥≥−ηL∥x−y∥2.
As a result, we have that Id+ηF is (1−ηL)-strongly monotone. We also have by triangle inequality
that
∥(Id+ηF)(x)−(Id+ηF)y∥≤∥x−y∥+η∥F(x)−F(y)∥≤(1+ηL)∥x−y∥,
completing the proof.
A.2 Complexity of the Outer loop
Bounding the norm of the iterates.
LemmaA.2. LetAssumptions1and2hold. Supposethattheiterates(x )ofAlgorithm1satisfy∥J (x )−
k η(F+G) k
J(cid:101)η(F+G)(x k)∥≤ε
k
for some ε
k
>0 and ρ<η. Then, we have for k ≥0 that
(cid:18) (cid:19) k
ρ 1 (cid:88)
∥x −x⋆∥≤∥x −x⋆∥+ 1− (i+1)ε .
k+1 0 η k+2 i
i=0
12Proof. Recall the following notation from Algorithm 1:
ρ η−ρ
α=1− = .
η η
Then, by Fact A.1(ii), we know that J is 1 -conically nonexpansive. This means that we can write
η(F+G) 2α
J =(1− 1 )Id+ 1 N for a nonexpansive operator N.
η(F+G) 2α 2α
Adding and subtracting α(1−β )J (x ) in the definition of x in Algorithm 1 and rearranging
k η(F+G) k k+1
gives
(cid:16) (cid:17)
x
k+1
=β kx 0+(1−β k) (1−α)x k+αJ(cid:101)η(F+G)(x k)
(cid:0) (cid:1) (cid:16) (cid:17)
=β kx 0+(1−β k) (1−α)x k+αJ η(F+G)(x k) +α(1−β k) J(cid:101)η(F+G)(x k)−J η(F+G)(x k)
1−β 1−β (cid:16) (cid:17)
=β kx 0+
2
kx k+
2
kN(x k)+α(1−β k) J(cid:101)η(F+G)(x k)−J η(F+G)(x k) ,
where the last step is because J = 2α−1Id+ 1 N for a nonexpansive operator N.
η(F+G) 2α 2α
We now use triangle inequality, nonexpansiveness of N, the definition of ε , and the last equality to
k
obtain
1−β 1−β
∥x −x⋆∥≤β ∥x −x⋆∥+ k∥x −x⋆∥+ k∥N(x )−x⋆∥
k+1 k 0 2 k 2 k
+α(1−β k)∥J(cid:101)η(F+G)(x k)−J η(F+G)(x k)∥
≤β ∥x −x⋆∥+(1−β )∥x −x⋆∥+α(1−β )ε , (A.2)
k 0 k k k k
where the inequality used that Nx⋆ =x⋆ since N =2αJ +(1−2α)Id and that J (x⋆)=x⋆ by
η(F+G) η(F+G)
the definition of x⋆ and Fact A.1(i).
The result of the lemma now follows by induction after using the definition β = 1 . In particular, the
k k+2
assertion is true for k =0 by inspection. Assume the assertion holds for k =K−1, then (A.2) gives
1 K+1 α(K+1)
∥x −x⋆∥≤ ∥x −x⋆∥+ ∥x −x⋆∥+ ε
K+1 K+2 0 K+2 K K+2 K
(cid:32) K−1 (cid:33)
1 K+1 α (cid:88) α(K+1)
≤ ∥x −x⋆∥+ ∥x −x⋆∥+ (i+1)ε + ε
K+2 0 K+2 0 K+1 i K+2 K
i=0
K
α (cid:88)
=∥x −x⋆∥+ (i+1)ε ,
0 K+2 i
i=0
which completes the induction. The result follows after using α=1− ρ.
η
Iteration complexity
Lemma2.5. LetAssumptions1and2hold. Supposethattheiterates(x )ofAlgorithm1satisfy∥J (x )−
k η(F+G) k
J(cid:101)η(F+G)(x k)∥≤ε
k
for some ε
k
>0 and ρ<η. Then, we have for any K ≥1 that
K−1
αK(K+1) K+1 (cid:88) (cid:16)α (cid:17)
∥(Id−J )(x )∥2− ∥x⋆−x ∥2 ≤ (k+1)(k+2)ε2 +α(k+1)∥R(x )∥ε ,
4 η(F+G) K Kα 0 2 k k k
k=0
where α=1− ρ, as defined in Algorithm 1.
η
Remark A.3. Halpern iteration with a nonexpansive operator N is
x =β x +(1−β )N(x ).
k+1 k 0 k k
13Without using smaller step size for the operator, as we propose (corresponding to strict inexactness require-
ments, such as Diakonikolas [2020], Cai et al. [2024]), we have N = (1−2α)Id+2αJ that is nonexpansive
(since (Id−J) is α-cocoercive):
∥x−2α(Id−J)x−(y−2α(Id−J)y)∥2
=∥x−y∥2−4α⟨(Id−J)x−(Id−J)y,x−y⟩+4α2∥(Id−J)(x−y)∥2
≤∥x−y∥2,
where the inequality is by α-cocoercivity of Id−J.
ThiscanbeviewedasaCayley(orreflection)operatorofafirmlynonexpansiveoperatorN′ =(1−α)Id+
αJ, since N =2N′−Id. Our choice to make the algorithm more robust (to work with a relaxed inexactness
requirement), is setting a smaller step size (named the halved-step Halpern iteration) which corresponds to
x =β x +(1−β )N′(x ).
k+1 k 0 k k
With our choice, the operator N′ is firmly-nonexpansive, which is defined as
∥N′x−N′y∥≤∥x−y∥−∥(Id−N′)x−(Id−N′)y∥,
instead of N which is just nonexpansive. We believe this helps us make the algorithm more robust to
inexactness in the resolvent computation.
(cid:16) (cid:17)
Proof of Lemma 2.5. ByFactA.1(ii),wehavethatId−J is 1− ρ cocoercive. Recallthedefinition
η(F+G) η
of α from Algorithm 1 and introduce a new notation for Id−J as:
η(F+G)
ρ
α=1− and R=Id−J .
η η(F+G)
With these notations, we use α-cocoercivity of R:
⟨R(x )−R(x ),x −x ⟩≥α∥R(x )−R(x )∥2. (A.3)
k+1 k k+1 k k+1 k
By rearranging the update rule of x in Algorithm 1, we have for k ≥0 that
k+1
x
k+1
=β kx 0+(1−β k)x k−α(1−β k)(Id−J(cid:101)η(F+G))(x k)
=β kx 0+(1−β k)x k−α(1−β k)R(x k)+α(1−β k)(J(cid:101)η(F+G)−J η(F+G))(x k), (A.4)
where we added and subtracted α(1−β )J (x ) and used the definition R=Id−J .
k η(F+G) k η(F+G)
We now use a step that is common in the rate analysis of Halpern-type methods, which can be seen for
example in Diakonikolas [2020] or Yoon and Ryu [2021]. In particular, from (A.4), we obtain two identical
representations for x −x :
k+1 k
x k+1−x
k
=β k(x 0−x k)−α(1−β k)R(x k)+α(1−β k)(J(cid:101)η(F+G)−J η(F+G))(x k), (A.5a)
β
x k+1−x
k
= 1−k
β
(x 0−x k+1)−αR(x k)+α(J(cid:101)η(F+G)−J η(F+G))(x k), (A.5b)
k
where the second representation follows from subtracting β x from both sides of (A.4) and rearranging.
k k+1
With these at hand, we develop the left-hand side of (A.3). First, by using (A.5b), we have that
β
⟨R(x ),x −x ⟩= k ⟨R(x ),x −x ⟩−α⟨R(x ),R(x )⟩
k+1 k+1 k 1−β k+1 0 k+1 k+1 k
k
+α⟨R(x k+1),(J(cid:101)η(F+G)−J η(F+G))(x k)⟩
= β k ⟨R(x ),x −x ⟩− α(cid:0) ∥R(x )∥2+∥R(x )∥2−∥R(x )−R(x )∥2(cid:1)
1−β k+1 0 k+1 2 k+1 k k+1 k
k
+α⟨R(x k+1),(J(cid:101)η(F+G)−J η(F+G))(x k)⟩, (A.6)
14where the last step used the expansion ∥a−b∥2 =∥a∥2−2⟨a,b⟩+∥b∥2.
Second, by using (A.5a), we have that
−⟨R(x ),x −x ⟩=−β ⟨R(x ),x −x ⟩+α(1−β )∥R(x )∥2
k k+1 k k k 0 k k k
−α(1−β k)⟨R(x k),(J(cid:101)η(F+G)−J η(F+G))(x k)⟩. (A.7)
After using (A.6) and (A.7) on (A.3) and rearranging, we obtain
α β
∥R(x )∥2+ k ⟨R(x ),x −x ⟩
2 k+1 1−β k+1 k+1 0
k
α
≤ (1−2β )∥R(x )∥2+β ⟨R(x ),x −x ⟩
2 k k k k k 0
α
+α⟨R(x k+1)−(1−β k)R(x k),(J(cid:101)η(F+G)−J η(F+G))(x k)⟩− 2∥R(x k+1)−R(x k)∥2. (A.8)
For the third term on the right-hand side of (A.8), we apply Cauchy-Schwarz, triangle and Young’s inequal-
ities along with the definition of ε to obtain
k
α⟨R(x k+1)−(1−β k)R(x k),(J(cid:101)η(F+g)−J η(F+G))(x k)⟩
≤α∥R(x )−(1−β )R(x )∥ε
k+1 k k k
≤α(∥R(x )−R(x )∥+β ∥R(x )∥)ε
k+1 k k k k
=α∥R(x )−R(x )∥ε +αβ ∥R(x )∥ε
k+1 k k k k k
α α
≤ ∥R(x )−R(x )∥2+ ε2 +αβ ∥R(x )∥ε . (A.9)
2 k+1 k 2 k k k k
ThisisthemainpointofdeparturefromthestandardanalyseswherethisinequalityisboundedbyO(∥x −x⋆∥ε ),
k k
cf. [Diakonikolas, 2020, display equation after (14)]. We instead use the last term in (A.8) (which we ob-
tainedbyusingasmallerstepsize)tocancelthecorrespondingerrortermin(A.9). Weusethislastestimate
in (A.8) and get
α β α
∥R(x )∥2+ k ⟨R(x ),x −x ⟩≤ (1−2β )∥R(x )∥2+β ⟨R(x ),x −x ⟩
2 k+1 1−β k+1 k+1 0 2 k k k k k 0
k
α
+ ε2 +αβ ∥R(x )∥ε . (A.10)
2 k k k k
Noting the identities
1 k+1 β 1 k
β = =⇒ 1−β = , k = , 1−2β = ,
k k+2 k k+2 1−β k+1 k k+2
k
on(A.10) we obtain
α 1 α k 1
∥R(x )∥2+ ⟨R(x ),x −x ⟩≤ ∥R(x )∥2+ ⟨R(x ),x −x ⟩
2 k+1 k+1 k+1 k+1 0 2 k+2 k k+2 k k 0
α α
+ ε2 + ∥R(x )∥ε ,
2 k k+2 k k
which holds for k ≥0. Multiplying both sides by (k+1)(k+2) gives
α(k+1)(k+2)
∥R(x )∥2+(k+2)⟨R(x ),x −x ⟩
2 k+1 k+1 k+1 0
αk(k+1)
≤ ∥R(x )∥2+(k+1)⟨R(x ),x −x ⟩
2 k k k 0
α
+ (k+1)(k+2)ε2 +α(k+1)∥R(x )∥ε .
2 k k k
15We sum the inequality for k =0,1,...,K−1 to get
αK(K+1)
∥R(x )∥2+(K+1)⟨R(x ),x −x ⟩
2 K K K 0
K−1
(cid:88) (cid:16)α (cid:17)
≤ (k+1)(k+2)ε2 +α(k+1)∥R(x )∥ε . (A.11)
2 k k k
k=0
By the standard estimation for the inner product on this left-hand side (using (i) monotonicity of R, which
is implied by α-cocoercivity of R with α>0; (ii) definition of x⋆ as R(x⋆)=(Id−J )(x⋆)=0 which
η(F+G)
uses Fact A.1(i); (iii) Young’s inequality), we derive
(K+1)⟨R(x ),x −x ⟩=(K+1)⟨R(x ),x⋆−x ⟩+(K+1)⟨R(x ),x −x⋆⟩
K K 0 K 0 K K
≥(K+1)⟨R(x ),x⋆−x ⟩+(K+1)⟨R(x⋆),x −x⋆⟩
K 0 K
=(K+1)⟨R(x ),x⋆−x ⟩
K 0
αK(K+1) K+1
≥− ∥R(x )∥2− ∥x⋆−x ∥2.
4 K Kα 0
We use this lower bound on (A.11) to conclude.
A.3 Complexity of the Inner Loop
Theorem 2.6. (See e.g., [Tseng, 2000, Theorem 3.4]) Let B be µ-strongly monotone with µ > 0 and L -
B
Lipschitz; A be maximally monotone, and z⋆ =(A+B)−1(0)̸=∅. For any ζ >0, after running Algorithm 2
(cid:108) (cid:109)
with initial point z 0 for T = 4L µB log∥z0− ζz⋆∥ iterations and τ = 2L1 B, we get
∥z −z⋆∥≤ζ,
T
(cid:108) (cid:109)
with the number of calls to evaluations of B and resolvents of A is upper bounded by 2 4LB log∥z0−z⋆∥ .
µ ζ
Proof. Weonlyderivethenumberofiterationsforeaseofreferencewhichfollowstriviallyfrom[Tseng,2000,
Theorem 3.4]. In particular, in the notation of [Tseng, 2000, Theorem 3.4(c)], we select θ = 1, α= 1 and
2 2LB
assume without loss of generality that µ ≤ 1 to obtain
LB 2
(cid:18) (cid:19)
µ
∥z −z⋆∥2 ≤ 1− ∥z −z⋆∥2,
t+1 2L t
B
which after unrolling gives that
(cid:18)
µ
(cid:19)T
∥z −z⋆∥2 ≤ 1− ∥z −z⋆∥2.
T 2L 0
B
(cid:108) (cid:109)
Standard manipulations give that after T = 4L µB log∥z0− ζz⋆∥ iterations, we have ∥z T −z⋆∥2 ≤ζ2.
A.4 Total complexity
Theorem 2.1. Let Assumptions 1 and 2 hold. Let η < 1 in Algorithm 1 and ρ<η. For any k =1,...,K,
L
we have that (x ) from Algorithm 1 satisfies
k
1 16∥x −x⋆∥2
∥x −J (x )∥2 ≤ 0 .
η2 k η(F+G) k (η−ρ)2(k+1)2
The number of first-order oracles used at iteration k of Algorithm 1 is upper-bounded by
(cid:24) 4(1+ηL) √ (cid:25)
log(98 k+2log(k+2)) .
1−ηL
16Proof of Theorem 2.1. We recall the notations
ρ
α=1− and R=Id−J
η η(F+G)
and start from the result of Lemma 2.5 which states for K ≥1 that
K−1
αK(K+1) K+1 (cid:88) (cid:16)α (cid:17)
∥R(x )∥2 ≤ ∥x⋆−x ∥2+ (k+1)(k+2)ε2 +α(k+1)∥R(x )∥ε .
4 K Kα 0 2 k k k
k=0
Let us set
γ∥R(x )∥
ε = √ k (A.12)
k
k+2log(k+2)
and note that we will not evaluate ε but we will show that for a computable number of inner iterations,
k
this error criterion will be proven to be satisfied.
We substitute the definition of ε to the previous inequality and get
k
√
αK(K+1)
∥R(x )∥2 ≤
K+1
∥x
−x⋆∥2+K (cid:88)−1(cid:18) αγ2(k+1)∥R(x k)∥2
+
αγ k+2∥R(x k)∥2(cid:19)
. (A.13)
4 K Kα 0 2log2(k+2) log(k+2)
k=0
We now show by induction that
4∥x −x⋆∥
∥R(x )∥≤ 0 ∀k ≥1. (A.14)
k α(k+1)
Note that α−1-Lipschitzness of R and R(x⋆) = 0 gives ∥R(x )∥ ≤ 1∥x −x⋆∥. For k = 1, we have by
0 α 0
α−1-Lipschitzness of R, R(x⋆)=0 and Lemma A.2 that
1 1 (cid:18) γ∥x −x⋆∥(cid:19) 2∥x −x⋆∥
∥R(x )∥≤ ∥x −x⋆∥≤ ∥x −x⋆∥+ √0 < 0 , (A.15)
1 α 1 α 0 2 2log2 α
for γ = 1 , which establishes the base case of induction. Now we assume (A.14) holds for all k ≤ K −1.
98
Then, we use (A.13) for K ≥2 (where we also use K+1 ≤2):
K
√
αK(K+1)
∥R(x )∥2 ≤
2
∥x
−x⋆∥2+K (cid:88)−1(cid:18) αγ2(k+1)∥R(x k)∥2
+
αγ k+2∥R(x k)∥2(cid:19)
4 K α 0 2log2(k+2) log(k+2)
k=0
√
≤
2
∥x
−x⋆∥2+K (cid:88)−1(cid:18) 16γ2∥x 0−x⋆∥2
+
16γ k+2∥x 0−x⋆∥2(cid:19)
.
α 0 α(k+1)log2(k+2) α(k+1)2log(k+2)
k=0
Since we have that
√
K−1 K−1
(cid:88) 16 (cid:88) 16 k+2
<55 and <49,
(k+1)log2(k+2) (k+1)2log(k+2)
k=0 k=0
the value γ = 1 results in
98
αK(K+1) 2.6
∥R(x )∥2 ≤ ∥x −x⋆∥2.
4 K α 0
A direct implication of this inequality is that
10.4
∥R(x )∥2 ≤ ∥x −x⋆∥2
K α2K(K+1) 0
15.6
≤ ∥x −x⋆∥2,
α2(K+1)2 0
where we used 1 ≤ 1.5 which holds when K ≥2. This completes the induction.
K(K+1) (K+1)2
17WenextseethatwithT setasinAlgorithm1, wegettheinexactnesslevelspecifiedbyε andtheoracle
k
complexity of each iteration is as claimed in the statement.
At iteration k, to apply the result in Theorem 2.6, we identify the following settings stemming from
Algorithm 1
A≡ηG, B(·)≡(Id+ηF)(·)−x , z ≡x , z⋆ ≡J (x ), ζ ≡ε
k 0 k η(F+G) k k
=⇒ z −z⋆ =(Id−J )(x )=R(x )
0 η(F+g) k k
hence B is (1 + ηL)-Lipschitz and (1 − ηL)-strongly monotone due to Fact A.1(iv). Existence of z⋆ is
guaranteed by Fact A.1(iii).
We now see that by the setting of
(cid:24) 4(1+ηL) √ (cid:25) (cid:24) 4(1+ηL) ∥R(x )∥(cid:25)
T = log(98 k+2log(k+2)) = log k ,
1−ηL 1−ηL ε
k
Theorem 2.6 gives us that
∥J(cid:101)η(F+G)(x k)−J η(F+G)(x k)∥≤ε
k
as claimed.
Since each iteration of Algorithm 2 uses 2 evaluations of F and 1 resolvent for G, the first-order oracle
complexity is 2T and the result follows.
We now continue with the proof of Corollary 2.2 which follows trivially from Theorem 2.1.
(cid:108) (cid:109)
Proof of Corollary 2.2. ByTheorem2.1, wehavethatafteratmost
4∥x0−x⋆∥
iterations, i.e., foraK such
(η−ρ)ε
that
(cid:24)
4∥x
−x⋆∥(cid:25)
K ≤ 0 , (A.16)
(η−ρ)ε
we are guaranteed to have
η−1∥(Id−J )(x )∥≤ε.
η(F+G) K
Total number of first-oracle calls during the run of the algorithm then be calculated as
(cid:88)K (cid:24) 4(1+ηL) √ (cid:25) (cid:18) 4(1+ηL) √ (cid:19)
log(98 k+2log(k+2)) ≤K· log(98 K+2log(K+2))+1 .
1−ηL 1−ηL
k=1
We conclude after using (A.16).
B Proofs for Section 3
B.1 Preliminary results
WenowderivesimilarpropertiestoFactA.1butwithAssumption3. Theseproofsareslightlymoreinvolved
than Fact A.1 to accommodate the weaker assumption.
Forexample,forthewell-definednessoftheresolventinFactA.1,wecoulddirectlyusethecorresponding
result from Bauschke et al. [2021] since these results are shown with cohypomonotonicity. However, show-
ing this with only star-cohypomonotonicity (or equivalently weak MVI condition) requires a couple more
additional tools from the operator splitting literature to show, for example, that η(F +G) is maximally
ηL-hypomonotone which is required to utilize existence results from Bauschke et al. [2021] in our setting.
We start with the definition of star-conic nonexpansiveness that will be used in Fact B.1. Recall that an
operator N is star-nonexpansive when ∥Nx−x⋆∥≤∥x−x⋆∥ where x⋆ is a fixed point of N.
Definition 1. T: Rd → Rd is α-conically star-nonexpansive if there exists a star-nonexpansive operator
N: Rd →Rd such that T =(1−α)Id+αN.
18This is a direct relaxation of conic nonexpansiveness in [Bauschke et al., 2021, Definition 3.1]. In Ap-
pendix B.1.1, we show the star-conic nonexpansiveness (and related properties) of the resolvent of a star-
cohypomonotone operator in view of Assumption 3, by invoking the corresponding arguments of Bauschke
et al. [2021] restricted to a point in the domain and a fixed point of the resolvent. Then we show star-
cocoercivity of Id−J which facilitates the analysis of KM iteration.
η(F+G)
Fact B.1. Let Assumptions 1 and 3 hold. Then, we have
(i) The operator J is single-valued and domJ =Rd when η < 1.
η(F+G) η(F+G) L
(cid:16) (cid:17)
(ii) The operator J is 1 -star-conically nonexpansive and Id−J is 1− ρ -star-cocoercive
η(F+G) 2(1−ρ) η(FG) η
η
when ρ<η.
(iii) For any x¯∈Rd, computing J (x¯) is equivalent to solving the problem
η(F+G)
Find x∈Rd such that 0∈(Id+η(F +G))(x)−x¯. (B.1)
The problem (B.1) has a unique solution when η < 1.
L
(iv) The operator Id+ηF is (1+ηL)-Lipschitz and (1−ηL)-strongly monotone.
Proof.
(i) Since F +G has a ρ-weak MVI solution under Assumption 3, we have that η(F +G) has ρ/η-weak
MVI solution, i.e., by simple change of variables, we have for some η >0
⟨ηu,x−x∗⟩≥ηρ∥u∥2 where u∈(F +G)(x)
ρ
⇐⇒⟨v,x−x∗⟩≥ ∥v∥2 where v ∈η(F +G)(x).
η
Additionally, when F is L-Lipschitz, it is maximally L-hypomonotone (see e.g., [Giselsson and Moursi,
2021, Lemma 2.12]) and ηF is maximally ηL-hypomonotone since
⟨F(x)−F(y),x−y⟩≥−∥F(x)−F(y)∥∥x−y∥≥−L∥x−y∥2
=⇒η⟨F(x)−F(y),x−y⟩≥−ηL∥x−y∥2.
By [Dao and Phan, 2019, Lemma 3.2(ii)], we know that ηF + Id is maximally (1 − ηL)-(strongly)
monotone. Then, using this and maximal monotonicity of G, we have by [Bauschke and Combettes,
2017, Corollary 25.5] that Id+η(F +G) is maximally (1−ηL)-(strongly) monotone. Invoking [Dao
and Phan, 2019, Lemma 3.2(ii)] again gives us that η(F +G) is maximally ηL-hypomonotone.
We can then use [Bauschke et al., 2021, Lemma 2.8] to obtain that (η(F +G))−1 is maximally ηL-
cohypomonotone. This can be combined with [Bauschke et al., 2021, Corollary 2.14] to get the result
when ηL<1.
(ii) As shown in the proof of (i), we have that η(F +G) is ρ-star-cohypomonotone (i.e., a solution exists
η
to the ρ-weakly MVI). Lemma B.2 then gives us that J is 1 -conically star-nonexpansive
η η(F+G) 2(1−ρ)
(cid:16) (cid:17) η
and as a result Id−J is 1− ρ star-cocoercive by Corollary B.3.
η(F+G) η
(iii) The proof is the same as Fact A.1(iii) where the only difference is that now we ensure the existence of
J with (i). Uniqueness of the solution is apparent from combining (i) and (iii).
η(F+G)
(iv) The proof is the same as Fact A.1(iv).
19B.1.1 Properties of Conic Star-Nonexpansiveness
Thissectionparticularizesthenotionandpropertiesoftheα-conicnonexpansivenessinBauschkeetal.[2021]
to their star variants. The aim is to show that the properties extend to their star-variants when we use
weak MVI condition instead of cohypomonotonicity. This sections implicitly assumes that J for operator
A
A: Rd ⇒ Rd is well-defined, which will be detailed later. We say that an operator N is star-nonexpansive
when ∥Nx−x⋆∥≤∥x−x⋆∥ where x⋆ is a fixed point of N.
Lemma B.2. (See [Bauschke et al., 2021, Lemma 3.4]) Consider T: Rd →Rd and let T =(1−α)Id+αN.
Then, N is star-nonexpansive if and only if we have, for all x∈Rd,
2α⟨Tx−x⋆,(Id−T)x⟩≥(1−2α)∥(Id−T)x∥2,
or equivalently
(cid:13)(cid:18) (cid:19) (cid:13)
(cid:13) (cid:13) 1− 1 x+ 1 Tx−x⋆(cid:13) (cid:13)≤∥x−x⋆∥. (B.2)
(cid:13) α α (cid:13)
Proof. Using α2∥a∥2−∥(α−1)a+b∥2 =2α⟨b,a−b⟩−(1−2α)∥a−b∥2 (see [Bauschke et al., 2021, Lemma
3.3]) with a=x−x⋆ and b=Tx−x⋆, we have
0≤ 2α⟨Tx−x⋆,(Id−T)x⟩−(1−2α)∥(Id−T)x∥2
= α2∥x−x⋆∥2−∥(α−1)(x−x⋆)+Tx−x ∥2
∗
= α2∥x−x⋆∥2−∥(α−1)(x−x⋆)+(1−α)(x−x⋆)+α(Nx−x⋆)∥2
= α2(∥x−x⋆∥2−∥Nx−x⋆∥2),
which gives the assertion. Last claim follows by substituting N = 1T + (cid:0) 1− 1(cid:1) Id in the definition of
α α
star-nonexpansiveness for N.
CorollaryB.3. (See[Bauschkeetal.,2021,Corollary3.5(iii)])T: Rd →Rd isα-conicallystar-nonexpansive
if and only if Id−T is 1 -star-cocoercive.
2α
Proof. We use Lemma B.2:
(cid:18) (cid:19)
1 1
⟨Tx−x⋆,(Id−T)x⟩≥ −1 ∥(Id−T)x∥2 ⇔ ⟨x−x⋆,(Id−T)x⟩≥ ∥(Id−T)x∥2,
2α 2α
which is simply adding to both sides ∥(Id−T)(x)∥2.
Proposition B.4. (See [Bauschke et al., 2021, Proposition 3.6(i)]) Let A = T−1−Id and set N = 1T −
α
1−αId, i.e., T =J =(Id+A)−1 =(1−α)Id+αN. Then, T is α-conically star-nonexpansive if and only
ifα A is (cid:0) 1− 1 (cid:1) -staA r-cohypomonotone, i.e.,
2α
(cid:18) (cid:19)
1
⟨x−x⋆,Ax⟩≥− 1− ∥Ax∥2.
2α
Proof. We see the two directions:
“⇒” Let (x,u) ∈ graA. Then by definition of A = T−1 −Id and manipulations, it follows that (x,u) =
(T(x+u),(Id−T)(x+u)). By Lemma B.2 invoked with x←x+u, we have
2α⟨T(x+u)−x⋆,(Id−T)(x+u)⟩≥(1−2α)∥(Id−T)(x+u)∥2
⇔ 2α⟨x−x⋆,u⟩≥(1−2α)∥u∥2,
where the last step substituted (x,u)=(T(x+u),(Id−T)(x+u)).
“⇐” Since (Tx,(Id − T)x) ∈ graA, we have by star-cohypomonotonicity that ⟨Tx−x⋆,(Id−T)x⟩ ≥
(cid:0) 1 −1(cid:1) ∥(Id−T)x∥2. In view of Lemma B.2, we deduce conic star-nonexpansiveness.
2α
20B.2 Complexity of the Outer Loop
Bounding the norm of iterates. Just like Appendix A, we start with the bound of the norms of the
iterates.
LemmaB.5. LetAssumptions1and3hold. Supposethattheiterates(x )ofAlgorithm3satisfy∥J (x )−
k η(F+G) k
J(cid:101)η(F+G)(x k)∥≤ε
k
for some ε
k
>0 and ρ<η. Then, we have for k ≥0 that
(cid:18) (cid:19)
ρ
∥x −x⋆∥≤∥x −x⋆∥+ 1− ε .
k+1 k η k
Proof. From Fact B.1(ii), we know that J is 1 -conically star-nonexpansive. Then, by property
η(F+G) 2(1−ρ)
η (cid:16) (cid:17)
(B.2)derivedinLemmaB.2,sinceJ isalso 1 -conicallystar-nonexpansivedueto2 1− ρ ≥1−ρ
η(F+G) 1−ρ η η
η
(see also Corollary B.3), we have
(cid:13) (cid:18) (cid:19) (cid:13)
(cid:13) (cid:13) (cid:13)ηρ x k+ 1− ηρ J η(F+G)(x k)−x⋆(cid:13) (cid:13) (cid:13)≤∥x k−x⋆∥. (B.3)
By the definition of x in Algorithm 3, the definition of ε and triangle inequality, we have for k ≥0 that
k+1 k
(cid:13) (cid:18) (cid:19) (cid:13) (cid:18) (cid:19)
∥x
k+1−x⋆∥≤(cid:13)
(cid:13)
(cid:13)ηρ
x k+ 1−
ηρ
J η(F+G)(x
k)−x⋆(cid:13)
(cid:13) (cid:13)+ 1−
ηρ
∥J η(F+G)(x k)−J(cid:101)η(F+G)(x k)∥
(cid:13) (cid:18) (cid:19) (cid:13) (cid:18) (cid:19)
≤(cid:13)
(cid:13)
(cid:13)ηρ
x k+ 1−
ηρ
J η(F+G)(x
k)−x⋆(cid:13)
(cid:13) (cid:13)+ 1−
ηρ
ε k.
Combining with (B.3) gives the result.
Iteration complexity. Equippedwiththisresult, weproceedtoderivingtheiterationcomplexityofthe
outer loop.
Lemma3.5. LetAssumptions1and3hold. Supposethattheiterates(x )ofAlgorithm3satisfy∥J (x )−
k η(F+G) k
J(cid:101)η(F+G)(x k)∥≤ε
k
for some ε
k
>0 and ρ<η. Then, we have for K ≥1 that
K−1 K−1 K−1
(cid:88) 2 (cid:88) 4 (cid:88)
∥(Id−J )(x )∥2 ≤ ∥x −x⋆∥2+6 ε2 + ∥x −x⋆∥ε , (B.4)
η(F+G) i (cid:16) (cid:17)2 0 k 1− ρ k k
k=0 1− ρ k=0 η k=0
η
where
(cid:18) (cid:19)
ρ
∥x −x⋆∥≤∥x −x⋆∥+ 1− ε .
k k−1 η k−1
(cid:16) (cid:17)
Proof. From Fact B.1(ii), we have that Id−J is 1− ρ -star cocoercive. Let us recall our running
η(F+G) η
notations:
ρ
α=1− η, R=Id−J η(F+G), R(cid:101)=Id−J(cid:101)η(F+G).
As a result, we have the following equivalent representation of x (see the definition in Algorithm 3):
k+1
(cid:18) ρ(cid:19)(cid:16) (cid:17)
x
k+1
=x k− 1−
η
Id−J(cid:101)η(F+G) (x k)
=x k−αR(cid:101)(x k). (B.5)
Then, by α-star-cocoercivity of R, we have
⟨R(x ),x −x⋆⟩≥α∥R(x )∥2. (B.6)
k k k
21A simple decomposition gives
⟨R(x k),x k−x⋆⟩=⟨R(cid:101)(x k),x k−x⋆⟩+⟨R(x k)−R(cid:101)(x k),x k−x⋆⟩. (B.7)
We estimate the first term on the right-hand side of (B.7) as
1
⟨R(cid:101)(x k),x k−x⋆⟩= α⟨x k−x k+1,x k−x⋆⟩
= 1 (cid:0) ∥x −x ∥2+∥x −x⋆∥2−∥x −x⋆∥2(cid:1)
2α k k+1 k k+1
≤ 21 α(cid:0) ∥x k−x⋆∥2−∥x k+1−x⋆∥2(cid:1) + 3 4α ∥R(x k)∥2+ 3 2α ∥R(cid:101)(x k)−R(x k)∥2
≤ 1 (cid:0) ∥x −x⋆∥2−∥x −x⋆∥2(cid:1) + 3α ∥R(x )∥2+ 3αε2 k, (B.8)
2α k k+1 4 k 2
where we used the definition of x from (B.5) in the first step, standard expansion ∥a−b∥2 = ∥a∥2 −
k+1
2⟨a,b⟩+∥b∥2 for the second step, the definition of x from (B.5) and Young’s inequality in the third step,
k+1
and the definitions of R k,R(cid:101)k,ε
k
in the last step.
For the second term on the right-hand side of (B.7), we have by Cauchy-Schwarz inequality and the
definition of R(cid:101) and ε
k
that
⟨R(x k)−R(cid:101)(x k),x k−x⋆⟩≤∥R(x k)−R(cid:101)(x k)∥∥x k−x⋆∥
≤∥x −x⋆∥ε . (B.9)
k k
We combine (B.8) and (B.9) in (B.7), plug in the result to (B.6) and rearrange to obtain
α ∥R(x )∥2 ≤ 1 (cid:0) ∥x −x⋆∥2−∥x −x⋆∥2(cid:1) + 3αε2 k +∥x −x⋆∥ε .
4 k 2α k k+1 2 k k
Theresultfollowsbymultiplyingbothsidesby4/α,summingfork =0,1,...,K−1,andusingthedefinition
of α. The bound on ∥x −x⋆∥2 follows by Lemma B.5.
k
B.3 Complexity of the Inner Loop
Inamodularfashion,wewillusepreciselythesamealgorithmfortheinnerloop,i.e.,theForward-Backward-
Forward(FBF)algorithmofTseng[2000]liketheSectionA.3. Hencethecomplexityoftheinnerloopisthe
same as Theorem 2.6. As we see in the next section, the accuracy required by J(cid:101)η(F+G) is slightly different
leading to the number of inner loop iterations T in Algorithm 3 to be slightly different than Algorithm 1.
B.4 Total Complexity
Theorem 3.1. Let Assumptions 1 and 3 hold. Let η < 1 in Algorithm 3 and suppose that ρ<η. For any
L
K >1, we have that
1 K (cid:88)−1 1
∥x −J (x )∥2 ≤
11∥x 0−x⋆∥2
.
K η2 k η(F+G) k (η−ρ)2K
k=0
The number of first-order oracles used at iteration k of Algorithm 3 is upper bounded by
(cid:24) (cid:25)
4(1+ηL)
log(8(k+2)log2(k+2)) .
1−ηL
Remark B.6. It is straightforward to convert this to a last-iterate result if we additionally assume cohy-
pomonotonicityasinPethicketal.[2023b], butwerefrainfromdoingsosincethemainpointofthissection
is to relax cohypomonotonicity.
22Proof of Theorem 3.1. Recall the notations α=1− ρ and R=Id−J . Let us set
η η(F+G)
1
ε = ∥x −J (x )∥ (B.10)
k 8(k+1)log2(k+2) k η(F+G) k
andnote, justasintheproofofTheorem2.1, thatwewillnotevaluatethevalueofε butwewillshowthat
k
forthenumberofiterationsthatFBFrunsateachKMiterationinAlgorithm3, theerrorcriteriondictated
by ε is satisfied.
k
By using the definition of ε in Lemma B.5 gives
k
α
∥x −x⋆∥≤∥x −x⋆∥+ ∥x −J (x )∥. (B.11)
k+1 k 8(k+1)log2(k+2) k η(F+G) k
We note that α = 1− ρ ≤ 1 and since R = Id−J is α-star cocoercive (see, e.g., Corollary B.3), we
η η(F+G)
have that R is α−1-star Lipschitz and hence by (Id−J )(x⋆)=0 we have
η(F+G)
∥(Id−J )(x )∥=∥(Id−J )(x )−(Id−J )(x⋆)∥≤α−1∥x −x⋆∥. (B.12)
η(F+G) k η(F+G) k η(F+G) k
Consequently, (B.11) becomes, after summing for k =0,1,...,K−1 that
K−1
(cid:88) 1
∥x −x⋆∥≤∥x −x⋆∥+ ∥x −x⋆∥.
K 0 8(i+1)log2(i+2) i
i=0
We can show, by induction, that
∥x −x⋆∥≤2∥x −x⋆∥ ∀k ≥0, (B.13)
k 0
because (cid:80)∞ 1 <4.
i=0 (i+1)log2(i+2)
We use (B.13) in the result of Lemma 3.5 to obtain (also noting the definitions of α and R)
K−1 K−1 K−1
(cid:88) 2 (cid:88) 4 (cid:88)
∥R(x )∥2 ≤ ∥x −x⋆∥2+ 6ε2 + 2∥x −x⋆∥ε . (B.14)
k α2 0 k α 0 k
k=0 k=0 k=0
By using (B.13) and (B.12) in (B.10) we also know the following upper bound on ε :
k
∥x −x⋆∥
ε ≤ 0 .
k 4α(k+1)log2(k+2)
With this, (B.14) becomes
K (cid:88)−1
∥R(x )∥2 ≤
2
∥x
−x⋆∥2+K (cid:88)−1 3∥x 0−x⋆∥2 +K (cid:88)−1 2∥x 0−x⋆∥2
k α2 0 8α2(k+1)2log4(k+2) α2(k+1)log2(k+2)
k=0 k=0 k=0
11
< ∥x −x⋆∥2, (B.15)
α2 0
since(cid:80)K−1 3 <2and(cid:80)K−1 2 <7. Thisestablishesthefirstpartoftheassertion.
k=0 8(k+1)2log4(k+2) k=0 (k+1)log2(k+1)
Wenext seethat, with T set asinAlgorithm 1, weget theinexactness levelspecified by ε andweverify
k
that the oracle complexity of each iteration is as claimed in the statement.
Forthesecond partofthe result, weproceedsimilarto theproofofTheorem 2.1. Namely, atiteration k,
weapplytheresultinTheorem2.6. Forthis,letusidentifythefollowingfromthedefinitionsinAlgorithm3
A≡ηG, B(·)≡(Id+ηF)(·)−x , z ≡x , z⋆ ≡J (x ), ζ ≡ε
k 0 k η(F+G) k k
=⇒ z −z⋆ =(Id−J )(x )=R(x ).
0 η(F+g) k k
Asbefore,wehavethatBis(1+ηL)-Lipschitzand(1−ηL)-stronglymonotoneduetoFactB.1(iv). Existence
of z⋆ is guaranteed by Fact B.1(iii) since η < 1.
L
23We now see that by the setting of T from Algorithm 3 and definition of ε , we have
k
(cid:24) (cid:25) (cid:24) (cid:25)
4(1+ηL) 4(1+ηL) ∥R(x )∥
T = log(8(k+1)log2(k+2)) = log k .
1−ηL 1−ηL ε
k
With this value, Theorem 2.6 gives us
∥J(cid:101)η(F+G)(x k)−J η(F+G)(x k)∥≤ε
k
as claimed.
Since each iteration of Algorithm 2 uses 2 evaluations of F and 1 resolvent for G, the first-order oracle
complexity is 2T and the result follows.
We continue with the proof of Corollary 3.2 which follows trivially from Theorem 3.1.
Proof of Corollary 3.2. Based on Theorem 3.1, we have that after K iterations where
(cid:24)
11∥x
−x⋆∥2(cid:25)
K ≤ 0 (B.16)
η2α2ε2
we are guaranteed to obtain
K−1
1 (cid:88)
min η−1∥R(x )∥≤ η−1∥R(x )∥≤ε
0≤k≤K−1 k K k
k=0
Total number of first-oracle calls during the run of the algorithm then be calculated as
K (cid:24) (cid:25) (cid:18) (cid:19)
(cid:88) 4(1+ηL) 4(1+ηL)
log(8(k+2)log2(k+2)) ≤K· log(8(K+2)log2(K+2))+1 .
1−ηL 1−ηL
k=1
We conclude after using (B.16).
B.5 Additional Results
Let us re-emphasize the strategy in the previous proof: we set a value for ε and then we show that when
k
we run the inner algorithm FBF for a certain computable number of iterations, the criterion enforced on
J(cid:101)η(F+G) byε
k
issatisfied. However,thisnumberofinneriterationsisworst-case. Anotheralternative,which
could be more useful in practice is to set ε to a computable value and monitor the progress of the inner
k
algorithm and break when ε is attained. One sidenote is that this is attainable in the deterministic case
k
considered in this section, howeverit cannot be done in the stochastic case since the convergence guarantees
are generally given in expectation.
This described strategy can be made rigorous with the only change being in the constants in our deter-
ministic case. We now see this in the next proposition.
Corollary B.7. Let Assumptions 1 and 3 hold and let G=∂ι for a convex closed set C. Let η < 1 and
C L
ρ < η in Algorithm 1 with ∥J(cid:101)η(F+G)(x k)−J η(F+G)(x k)∥ ≤ klog2c
(k+2)
for any c > 0 and use [Diakonikolas,
2020, Algorithm 4] to obtain such J(cid:101)η(F+G)(x k) at iteration k. Then, we have that
K−1
1 (cid:88)
η−1∥(Id−J )(x )∥≤ε,
K η(F+G) k
k=0
(cid:16) (cid:17)
with the number of calls to evaluation of F and resolvent of G is bounded by O˜ (1+ηL)((1+c)∥x0−x⋆∥2+c2) .
ε2(η−ρ)2(1−ηL)
24Remark B.8. Notethat[Diakonikolas,2020,Algorithm4]hasabuilt-instoppingcriteriontoterminatethe
algorithm when the required accuracy is achieved. The value for ε defined in this corollary is computable
k
since it only depends on k and a user-defined constant c. This is an alternative to FBF we used in the main
textwhereweuseacomputablenumberofiterationstoruntheinneralgorithmratherthanusingastopping
criterion as [Diakonikolas, 2020, Algorithm 4]. On the one hand, in practice, a stopping criterion can be
moredesirablesincetheworst-casenumberofiterationscanbepessimistic. Ontheotherhand, thestrategy
ofusingastoppingcriterionisinherentlymorecomplicatedinthestochasticcasewhereasusingaworst-case
computable number of inner iteration is still easily implementable. This is why we considered the latter
setting throughout the paper. However, this corollary is still included for the former strategy.
Proof of Corollary B.7. We obtain the result for modifying the proof of Theorem 3.1. We set
c
ε = ,
k (k+1)log2(k+2)
for any c>0.
By using this on Lemma B.5 and summing the result for k =0,1,...,K−1 we obtain
K−1
(cid:88) c
∥x −x⋆∥≤∥x −x⋆∥+α
k 0 (k+1)log2(k+2)
k=0
≤∥x −x⋆∥+4αc,
0
since (cid:80)K−1 1 <4. We use this bound on the result of Lemma 3.5 to obtain
k=0 (k+1)log2(k+2)
K (cid:88)−1
∥R(x )∥2 ≤
2
∥x
−x⋆∥2+K (cid:88)−1 6c2
+
4 K (cid:88)−1 c(∥x 0−x⋆∥+4αc)
k α2 0 (k+1)2log4(k+2) α (k+1)log2(k+2)
k=0 k=0 k=0
(cid:18) (cid:19)
2 16c
≤ + ∥x −x⋆∥2+30c2+64c2,
α2 α 0
which gives the result after dividing by η2 and noting that [Diakonikolas, 2020, Lemma 17] gives complexity
(cid:16) (cid:17)
O(cid:101) 11 −+η ηL
L
for obtaining such a J(cid:101)η(F+G)(x k).
We continue with the result mentioned in Remark 3.4.
Corollary B.9. Let Assumptions 1 and 3 hold. Let η < 1 and ρ<η.
L
(i) Let G≡0 and consider Algorithm 3. Then we have that min ∥F(x )∥≤2ε with the first-order
0≤k≤K−1 k
oracle calls bounded by
(cid:32) (cid:33)
(1+ηL)∥x −x⋆∥2
O(cid:101) 0 .
ε2(η−ρ)2(1−ηL)
(ii) Let G ≡ ∂ι for a convex closed set C ⊆ Rd. Given ε > 0, consider Algorithm 3 with the up-
C
date J(cid:101)η(F+G)(x k) replaced with [Diakonikolas, 2020, Algorithm 4] with error criterion ∥J(cid:101)η(F+G)(x k)−
J η(F+G)(x k)∥≤ (k+1)lη oε g2 2(k+3). Then, for xout =argmin x∈{x0,...,xk−1}∥x−J(cid:101)η(F+G)(x)∥, we have that
η−1∥(Id−J )(xout)∥≤2ε+3ε2 with the first-order oracle calls bounded by
η(F+G)
(cid:32) (cid:33)
(1+ηL)∥x −x⋆∥2
O(cid:101) 0 . (B.17)
ε2(η−ρ)2(1−ηL)
SeealsoRemark2.3fordetailsonhowwecanusethisresulttofurtherobtainaguaranteelikedist(0,(F+
G)(xout))≤ε.
25Proof of Corollary B.9. (i) In this case, we start from the final steps of the proof of Theorem 3.1 (see
(B.15)) which, after using R=Id−J , gives us that
η(F+G)
K−1
1 (cid:88)
η−2∥x −J (x )∥2 ≤ε2, (B.18)
K k ηF k
k=0
with the prescribed complexity bound given in Theorem 3.1. Let us define x¯ =J (x ).
k ηF k
On the one hand, we use the definition of resolvent to obtain
x¯ =J (x ) ⇐⇒ x¯ +ηF(x¯ )=x ⇐⇒ x −x¯ =ηF(x¯ ),
k ηF k k k k k k k
which, in view of (B.18), means that we have
K−1
1 (cid:88)
∥F(x¯ )∥2 ≤ε2. (B.19)
K k
k=0
On the other hand, we know by Young’s inequality and Lipschitzness of F that
K−1 K−1 K−1
1 (cid:88) 1 (cid:88) 1 (cid:88)
∥F(x )∥2 ≤ 2∥F(x¯ )∥2+ 2∥F(x )−F(x¯ )∥2
K k K k K k k
k=0 k=0 k=0
K−1 K−1
1 (cid:88) 1 (cid:88)
≤ 2∥F(x¯ )∥2+ 2L2∥x −x¯ ∥2
K k K k k
k=0 k=0
≤(2+2η2L2)ε2
<4ε2,
where we used (B.18) and (B.19).
(ii) A slight modification of the proof of Corollary B.7 by using ε =
ηε2
gives us that
k (k+1)log2(k+3)
K−1
1 (cid:88)
η−2∥(Id−J )(x )∥2 ≤ε2 (B.20)
K η(F+G) k
k=0
with the complexity bound (B.17). This is because [Diakonikolas, 2020, Lemma 17] showed that [Di-
akonikolas,2020,Algorithm4]outputsaJ(cid:101)η(F+g)(x k)satisfyingtherequirementsetbyε
k
= (k+1)lη oε g2 2(k+3),
with the same worst-case complexity as Theorem 2.6. The difference is that [Diakonikolas, 2020, Algo-
rithm 4] has a computable stopping criterion (instead of the maximum number of iterations Algorithm
2 takes) where we can check if ε =
ηε2
accuracy is achieved and break the loop.
k (k+1)log2(k+3)
Since we have the pointwise bound ∥J η(F+G)(x k)−J(cid:101)η(F+G)(x k)∥2 ≤η2ε4, we derive from (B.20) that
K−1
1 (cid:88)
K
η−2∥(Id−J(cid:101)η(F+G))(x k)∥2 ≤2(ε2+ε4).
k=0
Hence, for xout defined in the statement, we get
η−2∥(Id−J(cid:101)η(F+G))(xout)∥2 ≤2(ε2+ε4). (B.21)
Then, by using the pointwise bound again, we know that
η−1∥(Id−J η(F+G))(xout)∥≤η−1∥(Id−J(cid:101)η(F+G))(xout)∥+η−1∥(J η(F+G)−J(cid:101)η(F+G))(xout)∥
(cid:112)
≤ε2+ 2(ε4+ε2)<2ε+3ε2,
which uses (B.21) and the implication of the error criterion ∥J η(F+G)(x k)−J(cid:101)η(F+G)(x k)∥≤ηε2, com-
pleting the proof.
26C Proofs for Section 4
Notation. We use the following definitions for conditional expectations: For expectation conditioned
on the filtration generated by the randomness of x ,...,x , we use E [·] while analyzing Algorithm 4 and
k 1 k
Algorithm 6. In the notation of Algorithm 5, we similarly use E [·] for the expectation conditioned on
t+1/2
the filtration generated by the randomness of z ,z ,...,z ,z . Unif denotes the uniform distribution
t+1/2 t 1 1/2
and Geom denotes the geometric distribution.
Table 2 summarizes the existing works for stochastic min-max problems satisfying cohypomonotonicity
or weak MVI conditions.
C.1 Analysis of the inner loop for stochastic problems
ThemainchangeforalgorithmsinthestochasticcaseiscomputingtheresolventapproximationJ(cid:101)η(F+G)(x k).
We now need to invoke FBF with unbiased oracles for F, see for example (4.1). For ease of reference, we
specifythealgorithmbelow. NotethatAlgorithm4ispreciselyAlgorithm1when(4.1)isusedforestimating
the resolvent and Algorithm 5 is precisely Algorithm 2 when unbiased oracle B(cid:101) is inputted rather than full
operator B. Algorithm 5 is a stochastic version of FBF, which is analyzed in the monotone case by B¨ohm
et al. [2022].
Algorithm 4 Stochastic Inexact Halpern iteration for problems with cohypomonotonicity
Input: Parameters β = 1 ,η,L,ρ, α = 1− ρ, K > 0, initial iterate x ∈ Rd, subroutine FBF given in
k k+2 η 0
Algorithm 5
for k =0,1,2,...,K−1 do
(cid:16) (cid:17) (cid:108) (cid:109)
J(cid:101)η(F+G)(x k)=FBF x k,T,G,Id+ηF(cid:101),1+ηL , where T = 1734(k (+ 12 −)3 ηLlo )g 22(k+2)
x
k+1
=β kx 0+(1−β k)((1−α)x k+αJ(cid:101)η(F+G)(x k))
end for
Algorithm 5 FBF(z 0,T,A,B(cid:101),L B) from [Tseng, 2000] – Stochastic
Input: Parameter τ = 2 , initial iterate z ∈Rd
t (t+1)µ+6LB 0
for t=0,1,2,...,T −1 do
z =J (z −τ B (z ))
t+1/2 τtA t t ξt t
z =z +τ B (z )−τ B (z )
t+1 t+1/2 t ξt t t ξt+1/2 t+1/2
end for
More particularly, we solve the following stochastic strongly monotone inclusion problem:
Find x⋆ ∈Rd such that 0∈(A+B)(x⋆), where B =E [B ].
ξ∼Ξ ξ
Similar results to next theorem appeared in Hsieh et al. [2019], Kotsalis et al. [2022]. We provide a
proof for being complete and precise since we could not find a particular reference for stochastic FBF with
strong monotonicity and explicit constants. It is also worth noting that we do not focus on optimizing the
non-dominant terms. A tight bound for all the terms can be found in Kotsalis et al. [2022].
Theorem C.1. Let z⋆ =(A+B)−1(0)̸=∅, the operator B be L -Lipschitz and µ-strongly monotone with
B
µ>0, A be maximally monotone. Let B(cid:101)(·,ξ) satisfy E ξ∼Ξ[B(cid:101)(·,ξ)]=B(·) and E ξ∼Ξ∥B(cid:101)(x,ξ)−B(x)∥2 ≤σ2.
Then, we have that the last iterate of Algorithm 5 after running for T iterations, when initialized with z ,
0
and step size τ = 2 satisfies the bound
t (t+1)µ+6LB
6L /µ∥z −z⋆∥2+48σ2/µ2
E∥z −z⋆∥2 ≤ B 0 .
T T +6L
B
Each iteration of the algorithm uses two evaluations of B(cid:101) and one resolvent of A
27Batch
Assumption Reference Limit of ρ Constraints Oracle† Complexity
size
weak MVI Diakonikolas et al. [2021] √1 × O(ε−2) Single O(ε−4)
4 2L
Choudhury et al. [2023] 1 × O(ε−2) Single O(ε−4)
2L
Bo¨hm [2022] 1 × O(ε−2) Single O(ε−6)
2L
Pethick et al. [2023a] 1 ✓ 1 Multiple O(ε−4)
2L
Theorem C.11 1 ✓ O(cid:101)(1) Single O(cid:101)(ε−4)
L
cohypomonotone Pethick et al. [2023b] 1 ✓ k2 Single O(cid:101)(ε−6) (best)‡
2L
Pethick et al. [2023b] 1 ✓ k3 Single O(cid:101)(ε−16) (last)
2L
Chen and Luo [2022]∗ 1 × O(cid:101)(1) Single O(cid:101)(ε−2)
2L
Corollary C.5∗ 1 ✓ 1 Single O(cid:101)(ε−4)
L
Table 2: Comparison of first order algorithms for stochastic problems. Complexity refers to the number of oracle
calls to get the fixed point residual E∥(Id−J )(xout)∥≤ε. See also Remark 2.3. †Oracle access refers to the
η(F+G)
number of operator evaluations algorithm makes with one random seed. For example, ”Single” refers to algorithms
that only access one sample per seed, i.e., only F (x ), ”Multiple” is for algorithms that access multiple samples
ξt t
per seed, i.e., F (x ) and F (x ). Algorithms with ”Multiple” access also make the additional assumption that
ξt t ξt t−1
E ∥F (x)−F (y)∥2 ≤L2∥x−y∥2 which is stronger than mere Lipschitzness of F. ‡(best) refers to best iterate in
ξ∼Ξ ξ ξ
viewofRemark3.3;(last)referstoalastiterateconvergencerate. ∗Theseworkshavecomplexityasexpected number
of oracle calls due to the use of MLMC estimator. See also Appendix D.1 for derivations of the complexities when
they are not written explicitly in the existing works.
Proof. Note that the definition of z implies τ A(z ) ∋ z −z −τ B (z ). The definition of z⋆
t+1/2 t t+1/2 t t+1/2 t ξt t
implies τ A(z⋆)∋−τ B(z⋆) By using this with monotonicity of A, we get
t t
⟨z −z +τ B (z )−τ B(z⋆),z⋆−z ⟩≥0.
t+1/2 t t ξt t t t+1/2
By the definition of z , we then have
t+1
⟨z −z +τ B (z )−τ B(z⋆),z⋆−z ⟩≥0. (C.1)
t+1 t t ξt+1/2 t+1/2 t t+1/2
By taking expectation conditioned on z and also using strong monotonicity of B, we also have
t+1/2
E ⟨τ B (z )−τ B(z⋆),z −z⋆⟩=⟨τ B(z )−τ B(z⋆),z −z⋆⟩
t+1/2 t ξt+1/2 t+1/2 t t+1/2 t t+1/2 t t+1/2
≥µτ ∥z⋆−z ∥2
t t+1/2
µτ
≥ t∥z⋆−z ∥2−µτ ∥z −z ∥2
2 t+1 t t+1 t+1/2
µτ 1
≥ t∥z⋆−z ∥2− ∥z −z ∥2, (C.2)
2 t+1 3 t+1 t+1/2
where the third step is by Young’s inequality and last step is by the definition of τ , i.e., τ µ= 2µ ≤
t t (t+1)µ+6L
2µ ≤ 1 since µ≤L.
6L We3
have,bytheelementaryidentities⟨a,b⟩=
1(cid:0) ∥a∥2+∥b∥2−∥a−b∥2(cid:1)
=
1(cid:0) −∥a∥2−∥b∥2+∥a+b∥2(cid:1)
,
2 2
that
⟨z −z ,z⋆−z ⟩=⟨z −z ,z⋆−z ⟩+⟨z −z ,z −z ⟩
t+1 t t+1/2 t+1 t t+1 t+1 t t+1 t+1/2
= 1(cid:0) ∥z −z⋆∥2−∥z −z⋆∥2−∥z −z ∥2+∥z −z ∥2(cid:1) . (C.3)
2 t t+1 t t+1/2 t+1 t+1/2
Using (C.2) and (C.3) on (C.1) after taking total expectation, using tower rule and dividing both sides by
τ gives
t
(cid:18) (cid:19)
1 µ 1 5 1
+ E∥z⋆−z ∥2 ≤ E∥z⋆−z ∥2+ E∥z −z ∥2− E∥z −z ∥2. (C.4)
2τ 2 t+1 2τ t 6τ t+1 t+1/2 2τ t t+1/2
t t t t
28Definition of z in Algorithm 2 gives
t+1
5 5τ2
∥z −z ∥= t ∥B (z )−B (z )∥2
6 t+1 t+1/2 6 ξt t ξt+1/2 t+1/2
≤ 5τ t2 (cid:0) ∥B (z )−B(z )∥2+∥B(z )−B(z )∥2+∥B(z )−B (z )∥2(cid:1)
2 ξt t t t t+1/2 t+1/2 ξt+1/2 t+1/2
5τ2L2
≤5τ2σ2+ t B∥z −z ∥2.
t 2 t t+1/2
With this, we get in place of (C.4) that
(cid:18) 1 µ(cid:19) 1 1 (cid:18) 5τ2L2 1(cid:19)
+ E∥z⋆−z ∥2 ≤ E∥z⋆−z ∥2+ t B − E∥z −z ∥2+5τ σ2. (C.5)
2τ 2 t+1 2τ t τ 2 2 t t+1/2 t
t t t
The definition of τ = 2 has two consequences:
t (t+1)µ+6LB
1 µ 6L +(t+3)µ
+ = B
2τ 2 4
t
and
2 1 1
τ = ≤ =⇒τ2 ≤ ⇐⇒ 5τ2L2 ≤1.
t (t+1)µ+6L 3L t 5L2 t B
B B B
(cid:16) (cid:17)−1
As a result, we obtain, after multiplying both sides of (C.5) by 1 + µ = 4 that
2τt 2 6LB+(t+3)µ
(cid:18) (t+1)µ+6L(cid:19) 40σ2
E∥z⋆−z ∥2 ≤ ∥z⋆−z ∥2+ . (C.6)
t+1 (t+3)µ+6L t (6L+(t+1)µ)(6L+(t+3)µ)
We next show by induction that
6L/µ∥z −z⋆∥2+48σ2/µ2
E∥z⋆−z ∥2 ≤ 0 ∀t≥0.
t t+6L/µ
For brevity, let us denote κ=6L/µ.
Thebasecaset=0holdsbyinspection. Nextweassumetheassertionholdsfort=T andconsider(C.6)
to deduce
E∥z⋆−z ∥2
T+1
T +1+κ6L/µ∥z −z⋆∥2+48σ2/µ2 40σ2/µ2
≤ 0 +
T +3+κ T +κ (T +1+κ)(T +3+κ)
(cid:18) (cid:19)
≤
(T +1+κ)
+
1 (cid:0)
6L/µ∥z
−z⋆∥2+48σ2/µ2(cid:1)
.
(T +3+κ)(T +κ) 1.2(T +1+κ)(T +3+κ) 0
As a result, the inductive step will be implied by
(cid:18) (T +1+κ)2 (T +κ) (cid:19) 1
+ ≤ ,
(T +1+κ)(T +3+κ)(T +κ) 1.2(T +1+κ)(T +3+κ)(T +κ) T +1+κ
which, after letting α=T +κ, is equivalent to
(cid:18) 1.2(α+1)2 α (cid:19)
+ ≤1.2 ⇐⇒ 1.2(α+1)2 ≤1.2α2+2.6α ⇐⇒ 1.2≤0.2α ⇐⇒ 6≤α.
(α+3)α (α+3)α
This holds because α=T +κ=T +6L/µ≥6 since L/µ>1. This completes the induction.
29C.2 Stochastic Problem with Cohypomonotonicity
We have a stochastic version of Lemma A.2 proof of which is almost equivalent.
LemmaC.2. LetAssumptions1and2hold. Forthesequence(x )generatedbyAlgorithm4withE ∥J (x )−
k k η(F+G) k
J(cid:101)η(F+G)(x k)∥2 ≤ε2 k, we have for k ≥0 that
k
α (cid:88)
E∥x −x⋆∥≤∥x −x⋆∥+ (i+1)E[ε ].
k+1 0 k+2 i
i=0
Proof. The proof follows the same steps as Lemma A.2 after taking expectation on (A.2) and using Jensen’s
inequality since
(cid:114)
(cid:104) (cid:105) (cid:104) (cid:105)
E
k
∥J(cid:101)η(F+G)(x k)−J η(F+G)(x k)∥ ≤ E
k
∥J(cid:101)η(F+G)(x k)−J η(F+G)(x k)∥2 ≤ε k.
Hence the result follows by tower rule and the same induction as the proof of Lemma A.2.
LemmaC.3. LetAssumptions1and2hold. ConsiderAlgorithm4withE k∥J(cid:101)η(F+G)(x k)−J η(F+G)(x k)∥2 ≤
ε2. Then, we have for any γ >0 and K ≥1 that
k
αK(K+1)
E∥R(x )∥2 ≤
K+1
∥x⋆−x
∥2+K (cid:88)−1(cid:18) α(γ+1)
(k+1)(k+2)E[ε2]+
γαE∥R(x k)∥2(cid:19)
.
4 K Kα 0 2γ k 2
k=0
Proof. We follow the proof of Lemma 2.5 until (A.8) and then we take expectation to obtain
α β
E∥R(x )∥2+ k E⟨R(x ),x −x ⟩
2 k+1 1−β k+1 k+1 0
k
α
≤ (1−2β )E∥R(x )∥2+β E⟨R(x ),x −x ⟩
2 k k k k k 0
α
+αE⟨R(x k+1)−(1−β k)R(x k),(J(cid:101)η(F+G)−J η(F+G))(x k)⟩− 2E∥R(x k+1)−R(x k)∥2. (C.7)
Wethenconsider(A.9)aftertakingexpectationandusingCauchy-Schwarz,triangleandYoung’sinequalities
to obtain
αE⟨R(x k+1)−(1−β k)R(x k),(J(cid:101)η(F+g)−J η(F+G))(x k)⟩
(cid:104) (cid:105)
≤αE (∥R(x k+1)−R(x k)∥+β k∥R(x k)∥)∥J(cid:101)η(F+G)(x k)+J η(F+G)(x k)∥
α γαβ2 α(cid:18) 1(cid:19)
≤ 2E∥R(x k+1)−R(x k)∥2+
2
kE∥R(x k)∥2+
2
1+
γ
E∥J(cid:101)η(F+G)(x k)−J η(F+G)(x k)∥2
α γαβ2 α(cid:18) 1(cid:19)
≤ E∥R(x )−R(x )∥2+ kE∥R(x )∥2+ 1+ E[ε2], (C.8)
2 k+1 k 2 k 2 γ k
where the last step also used the tower rule along with the definition of ε .
k
We then use the same arguments as those after (A.10) to get the result.
C.2.1 Proof for Corollary 4.1
Corollary 4.1 is essentially the summary of the results proven below.
Theorem C.4. Let Assumptions 1, 2, and 4 hold. Let η < 1 in Algorithm 4 and ρ < η. For any K ≥ 1,
L
we have that
1 36(∥x −x⋆∥2+σ2)
E∥x −J (x )∥2 ≤ 0 .
η2 K η(F+G) K (η−ρ)2K2
The number of first-order oracles used at iteration k of Algorithm 1 is upper bounded by
(cid:24) 1734(k+2)3log2(k+2)(cid:25)
2 . (C.9)
(1−ηL)2
30Corollary C.5. Let Assumptions 1 and 2 hold. Let η < 1 in Algorithm 4 and ρ < η. For any ε > 0, we
have that E(cid:2) η−1∥x −J (x )∥(cid:3) ≤ε with stochastic firL st-order oracle complexity
k η(F+G) k
(cid:18) ∥x −x⋆∥4+σ4 (cid:19)
O(cid:101) 0
(η−ρ)4(1−ηL)2ε4
Proof. This corollary immediately follows from Theorem C.4 by combining the number of outer iterations
and the number of stochastic first-order oracle calls for each outer iteration.
Remark C.6. Thecomplexityinthepreviouscorollaryhasthesamedependenceon∥x −x⋆∥,σ asPethick
0
et al. [2023a], Bravo and Cominetti [2024]. As we see in the next remark, the dependence on (η−ρ) can
be improved by using the knowledge of the target accuracy ε and the variance upper bound σ2 as done in
Diakonikolas et al. [2021], Kim [2021], Chen and Luo [2022].
Remark C.7. By using parameters depending on target accuracy ε and noise variance σ2, we can improve
the complexity to
(cid:18) ∥x −x⋆∥2σ2 (cid:19)
O(cid:101) 0
(η−ρ)2(1−ηL)2ε4
Proof of Theorem C.4. Let us set
γ2(α2∥R(x )∥2+8σ2)
ε2 = k (C.10)
k α2(k+2)3log2(k+2)
and plug this in to the result of Lemma C.3 to obtain
αK(K+1)
E∥R(x )∥2
4 K
≤
K+1
∥x
−x⋆∥2+K (cid:88)−1 E(cid:18) (γ2+γ)(α2∥R(x k)∥2+8σ2)
+
γα∥R(x k)∥2(cid:19)
Kα 0 2α(k+2)log2(k+2) 2
k=0
=
K+1
∥x
−x⋆∥2+K (cid:88)−1(cid:18) 4(γ2+γ)σ2
+
α(γ2+γ)E∥R(x k)∥2
+
γαE∥R(x k)∥2(cid:19)
Kα 0 α(k+2)log2(k+2) 2(k+2)log2(k+2) 2
k=0
<
K+1
∥x −x⋆∥2+
12(γ2+γ)σ2 +K (cid:88)−1(cid:18) α(γ2+γ)E∥R(x k)∥2
+
γαE∥R(x k)∥2(cid:19)
, (C.11)
Kα 0 α 2(k+2)log2(k+2) 2
k=0
since (cid:80)K−1 1 <3.
k=0 (k+2)log2(k+2)
We now show by induction that
36(∥x −x⋆∥2+σ2)
E∥R(x )∥2 ≤ 0
k α2(k+1)2
The base case for the induction with K = 0,1 hold the same way as the proof of Theorem 2.1 where the
only change is we use Lemma C.2 and the definition of ε in (C.10), see also (A.15).
k
Let us consider (C.11) for K ≥ 2 and assume the assertion holds for k ≤ K −1. We then have, by also
noting K+1 ≤2 that
K
αK(K+1)
E∥R(x )∥2
4 K
≤
2
∥x −x⋆∥2+
12(γ2+γ)σ2 +K (cid:88)−1(cid:18) 18(γ2+γ)(∥x 0−x⋆∥2+σ2)
+
18γ(∥x 0−x⋆∥2+σ2)(cid:19)
.
α 0 α α(k+2)(k+1)2log2(k+2) α(k+1)2
k=0
By using (cid:80)∞ 18 <30 and (cid:80)∞ 18 <21.
k=0 (k+1)2 k=0 (k+2)(k+1)2log2(k+2)
With γ = 1 , we have that
17
αK(K+1) 6(∥x −x⋆∥2+σ2)
E∥R(x )∥2 ≤ 0 .
4 K α
31We use 1 ≤ 1.5 which holds for K ≥2 to complete the induction.
K(K+1) (K+1)2
To see the number of first-order oracles, we use the result for stochastic FBF in Theorem C.1. For our
subproblem at iteration k, this result gives
(cid:16) (cid:17)
6 1+ηL∥x −J (x )∥2+ 8σ2
E k(cid:104) ∥J(cid:101)η(F+G)(x k)−J η(F+G)(x k)∥2(cid:105) ≤ 1−ηL k η(F+ TG) k (1−ηL)2
6 (cid:0) ∥x −J (x )∥2+8σ2(cid:1)
(1−ηL)2 k η(F+G) k
≤ .
T
Recall that (C.10), with γ = 1 and R=Id−J , requires
17 η(F+G)
(cid:104) (cid:105) (α2∥(Id−J )(x )∥2+8σ2)
E
k
∥J(cid:101)η(F+G)(x k)−J η(F+G)(x k)∥2 ≤ 289α2(kη +(F 2+ )G 3) log2k
(k+2)
Noting that 1 >1, a sufficient condition to attain this requirement is
α2
(1−6
ηL)2
(cid:0) ∥x k−J η(F+G)(x k)∥2+8σ2(cid:1) ∥x k−J η(F+G)(x k)∥2+8σ2
≤ ,
T 289(k+2)3log2(k+2)
verifying the required number of iterations T as given in Algorithm 4 to be sufficient for the inexactness
criterion. Since each iteration of FBF takes 2 stochastic operator evaluations F(cid:101) and one resolvent of G, we
have the result.
C.3 Stochastic Problem with weak MVI condition
As motivated in Section 4.2, we use the multilevel Monte Carlo (MLMC) estimator [Giles, 2008, Blanchet
and Glynn, 2015, Asi et al., 2021, Hu et al., 2021]. In Section 4.2, we only sketched the main changes in
Algorithm 3 because of space limitations. We start by explicitly writing down the algorithm with MLMC
estimator.
Algorithm 6 Inexact KM iteration for problems with weak MVI
Input: Parameters η,L,ρ, α = 1− ηρ, α k = √ k+2lα og(k+3) K > 0, initial iterate x 0 ∈ Rd, subroutine
MLMC-FBF given in Algorithm 7
for k =0,1,2,...,K−1 do
T ←⌈ 96(1−ηL)−2 ⌉ and M ←⌈672×120(log 2T)⌉
min{ αk , 1 } (1−ηL)2
120α(k+1) 120 (cid:16) (cid:17)
J(cid:101) η(m (F) +G)(x k)=MLMC-FBF x k,T,Id+ηF(cid:101),G,1+ηL independently for each m=1,...,M
J(cid:101)η(F+G)(x k)= M1 (cid:80)M i=1J(cid:101) η(i () F+G)(x k)
x
k+1
=(1−α k)x k+α kJ(cid:101)η(F+G)(x k)
end for
Algorithm 7 MLMC-FBF(z ,T,A,B,L )
0 B
Input: Initial iterate z ∈Rd, subsolver FBF from Algorithm 5
0
Define yi =FBF(z 0,2i,B(cid:101),A,L B) for any i≥0. Draw I ∼Geom(1/2)
Output: yout =y0+2I(yI −yI−1) if 2I ≤T, otherwise yout =y0.
We start by modifying the proof of Lemma 3.5 for the stochastic problem, which is the most important
for getting the final complexity.
32Lemma C.8. Let Assumptions 1 and 3 hold. Suppose that the iterates generated by Algorithm 6 satisfy
E k∥J(cid:101)η(F+G)(x k)−J η(F+G)(x k)∥2 ≤ε2
k,v
and ∥E k[J(cid:101)η(F+G)(x k)]−J η(F+G)(x k)∥≤ε k,b. Then, we have that
K−1 K−1 K−1
α (cid:88) 1 3 (cid:88) (cid:88)
α E∥(Id−J )(x )∥2 ≤ ∥x −x⋆∥2+ α2E[ε2 ]+ α E[∥x −x⋆∥ε ].
4 k η(F+G) k 2 0 2 k k,v k k k,b
k=0 k=0 k=0
Proof. WeproceedmostlyastheproofofLemma3.5apartfromminorchangesduetothestochasticsetting
such as iteration-dependent step sizes.
(cid:16) (cid:17)
From Fact B.1(ii), we have that Id−J is 1− ρ -star cocoercive. Recall our running notations:
η(F+G) η
ρ
α=1− η, R=Id−J η(F+G), R(cid:101)=Id−J(cid:101)η(F+G).
As a result, we have the following equivalent representation of x (see the definition in Algorithm 6):
k+1
x
k+1
=x k−α kR(cid:101)(x k). (C.12)
By α-star-cocoercivity of R=Id−J , we have
η(F+G)
⟨R(x ),x −x⋆⟩≥α∥R(x )∥2. (C.13)
k k k
By a simple decomposition, we write
⟨R(x k),x k−x⋆⟩=⟨R(cid:101)(x k),x k−x⋆⟩+⟨R(x k)−R(cid:101)(x k),x k−x⋆⟩. (C.14)
For the expectation of the first term on the right-hand side of (C.14), we derive that (cf. (B.8))
1
E⟨R(cid:101)(x k),x k−x⋆⟩=
α
E⟨x k−x k+1,x k−x⋆⟩
k
= 1 E(cid:0) ∥x −x ∥2+∥x −x⋆∥2−∥x −x⋆∥2(cid:1)
2α k k+1 k k+1
k
≤ 2α1 E(cid:0) ∥x k−x⋆∥2−∥x k+1−x⋆∥2(cid:1) + 3α 4kE∥R(x k)∥2+ 3α 2kE∥R(cid:101)(x k)−R(x k)∥2
k
≤ 1 E(cid:0) ∥x −x⋆∥2−∥x −x⋆∥2(cid:1) + 3α E∥R(x )∥2+ 3α kE[ε2 k,v] , (C.15)
2α k k+1 4 k 2
k
where we used the definition of x from (C.12) in the first step, standard expansion ∥a−b∥2 = ∥a∥2 −
k+1
2⟨a,b⟩+∥b∥2 forthesecondstep,thedefinitionofx from(C.12)andYoung’sinequalityinthethirdstep,
k+1
the definition of ε with tower rule and α ≤α in the last step.
k,v k
For the second term on the right-hand side of (C.14), we have, by Cauchy-Schwarz inequality and the
definition of R(cid:101) and ε k,b, that
E⟨R(x k)−R(cid:101)(x k),x k−x⋆⟩=E[E k⟨R(x k)−R(cid:101)(x k),x k−x⋆⟩]
=E⟨E k[R(x k)−R(cid:101)(x k)],x k−x⋆⟩
(cid:104) (cid:105)
≤E ∥R(x k)−E k[R(cid:101)(x k)]∥∥x k−x⋆∥
≤E[∥x −x⋆∥ε ], (C.16)
k k,b
wherethefirststepisbytowerruleandthesecondstepisbyx −x⋆ beingmeasurableundertheconditioning
k
of E .
k
We combine (C.15) and (C.16) in (C.14), plug in the result to (C.13) and rearrange to obtain
α E∥R(x )∥2 ≤ 1 E(cid:0) ∥x −x⋆∥2−∥x −x⋆∥2(cid:1) + 3α kE[ε2 k,v] +E[∥x −x⋆∥ε ].
4 k 2α k k+1 2 k k,b
k
We conclude after multiplying both sides by α and summing for k =0,1,...,K−1.
k
33ThenextlemmaconsidersthebiasandvarianceoftheMLMCestimatorandfollowsthesamearguments
as [Asi et al., 2021, Proposition 1]. The only change is that we use the algorithm FBF (see Algorithm 5 for
a stochastic version) as the subsolver and we consider a strongly monotone inclusion problem rather than
minimization. These do not alter the estimations significantly as can be seen in the proof.
Lemma C.9. Under the same setting as Theorem C.1 and T ≥ 2, for the output of Algorithm 7, we have
that
12L/µ∥z −z⋆∥2+96σ2/µ2
∥E[yout]−z⋆∥2 ≤ 0
T
E∥yout−z⋆∥2 ≤14(6L/µ∥z −z⋆∥2+48σ2/µ2)log (T)
0 2
where the expected number of calls to F(cid:101) is O(log T).
2
Proof. We argue as [Asi et al., 2021, Property 1]. The only difference is that we call Theorem C.1 which is
our main solver for the strongly monotone problem.
Let us denote i = max{i ≥ 0: 2i ≤ T}. For a given event E, consider also the following notation for
T
the characteristic function: 1 =1 if E is true and 1 =0 if E is false.
E E
Then, we have by the definition of yout in Algorithm 7 that
E[yout]=E[y0]+E[1 ·2I(yI −yI−1)]
{2I≤T}
(cid:88)iT
=E[y0]+ Pr(I =i)2iE[yi−yi−1]
i=1
=E[y0]+E[yiT −y0]
=E[yiT]. (C.17)
By the definition of i T, we have that 2iT ≥ T
2
and hence, by Jensen’s inequality and Theorem C.1, we have
∥E[yiT]−z⋆∥2 ≤E∥yiT −z⋆∥2
12L∥z −z⋆∥2+96σ2/µ
≤ 0 ,
Tµ
which is the claimed bound on the bias due to (C.17).
We continue with estimating the variance of yout. First, Young’s inequality gives that
E∥yout−z⋆∥2 ≤2E∥yout−y0∥2+2E∥y0−z⋆∥2. (C.18)
We estimate the first term on the right-hand side:
(cid:88)iT
E∥yout−y0∥2 = Pr(I =i)E∥2i(yi−yi−1)∥2
i=1
(cid:88)iT
= 2iE∥yi−yi−1∥2
i=1
≤(cid:88)iT
2i+1(cid:0)E∥yi−z⋆∥2+E∥yi−1−z⋆∥2(cid:1)
, (C.19)
i=1
where the last step is by Young’s inequality.
By the definitions of yi,yi−1 and Theorem C.1, we have that
6L∥z −z⋆∥2+48σ2/µ
E∥yi−z⋆∥2 ≤ 0 ,
2iµ
6L∥z −z⋆∥2+48σ2/µ
E∥yi−1−z⋆∥2 ≤ 0 .
2i−1µ
34This gives, in view of (C.19), that
6(6L∥z −z⋆∥2+48σ2/µ)
E∥yout−y0∥2 ≤ 0 i .
µ T
The second term on the right-hand side of (C.18) is estimated the same way by using Theorem C.1:
6L∥z −z⋆∥2+48σ2/µ
E∥y0−z⋆∥2 ≤ 0 .
µ
Combining the last two estimates in (C.18) gives the claimed bound on the variance after using i ≤log T.
T 2
The expected number of calls to B(cid:101) is calculated as
(cid:88)iT
2+2 P(I =i)(2i+2i−1)=O(1+i )=O(1+log T),
T 2
i=1
since each iteration of stochastic FBF uses 2 unbiased samples of F. This completes the proof.
Let us consider M ≥1 independent draws of MLMC-FBF and denote for i-th draw:
(cid:16) (cid:17)
J(cid:101) η(i () F+G)(x k)=MLMC-FBF x k,T,G,Id+ηF(cid:101),1+ηL .
Then, we define
M
J(cid:101)η(F+G)(x k)= M1 (cid:88) J(cid:101) η(i () F+G)(x k). (C.20)
i=1
This will help us get a better control on the variance as [Asi et al., 2021, Theorem 1].
Corollary C.10. For J(cid:101)η(F+G)(x k) as defined in (C.20) and under the setting of Theorem C.1, we have the
bias and variance bounds given as
∥E k[J(cid:101)η(F+G)(x k)]−J η(F+G)(x k)∥2 ≤b2 k(∥(Id+J η(F+G))(x k)∥2+σ2),
E k∥J(cid:101)η(F+G)(x k)−J η(F+G)(x k)∥2 ≤v2(∥(Id+J η(F+G))(x k)∥2+σ2),
where
(cid:38) max{12L/µ,96/µ2}(cid:39) (cid:24)
2log T
max{84L/µ,672/µ2}(cid:25)
T = , and M = 2 .
min{b2,v2} v2
k 2
Each iteration makes in expectation O(logT ·M) calls to stochastic first-order oracle.
Proof. This proof follows the arguments in [Asi et al., 2021, Theorem 1]. The difference is that we set the
values of T,M independent of ∥R(x )∥2 and σ2, to make T,M computable, which results in these terms
k
appearing in the bias and variance upper bounds.
We first note that E k[J(cid:101)η(F+G)(x k)]=E k[J(cid:101) η(1 (F) +G)(x k)]. We next have by direct expansion that
1
E k∥J(cid:101)η(F+G)(x k)−J η(F+G)(x k)∥2 = ME k∥J(cid:101) η(1 (F) +G)(x k)−J η(F+G)(x k)∥2
(cid:18) (cid:19)
1
+ 1−
M
∥E k[J(cid:101) η(1 (F) +G)(x k)]−J η(F+G)(x k)∥2,
since J(cid:101)(i) are independent draws of the same estimator.
η(F+G)
By applying the identity E∥X −EX∥2 = E∥X∥2 −∥EX∥2 with X = J(cid:101) η(1 (F) +G)(x k)−J η(F+G)(x k), we
obtain
1
E k∥J(cid:101)η(F+G)(x k)−J η(F+G)(x k)∥2 = ME k∥J(cid:101) η(1 (F) +G)(x k)−E k[J(cid:101) η(1 (F) +G)(x k)]∥2
+∥E k[J(cid:101) η(1 (F) +G)(x k)]−J η(F+G)(x k)∥2. (C.21)
35On the one hand, the fact E∥X−EX∥2 ≤E∥X∥2 gives
E k∥J(cid:101) η(1 (F) +G)(x k)−E k[J(cid:101) η(1 (F) +G)(x k)]∥2 ≤E k∥J(cid:101) η(1 (F) +G)(x k)−J η(F+G)(x k)∥2. (C.22)
On the other hand, the bounds in Lemma C.9 gives, after substituting z =x and z⋆ =J (x ) that
0 k η(F+G) k
12L/µ∥(Id+J )(x )∥2+96σ2/µ2
∥E k[J(cid:101) η(1 (F) +G)(x k)]−J η(F+G)(x k)∥2 ≤ η(F+ TG) k , (C.23a)
E k∥J(cid:101) η(1 (F) +G)(x k)−J η(F+G)(x k)∥2 ≤(cid:0) 84L/µ∥(Id+J η(F+G))(x k)∥2+672σ2/µ2(cid:1) log 2T. (C.23b)
Using E k[J(cid:101)η(F+G)(x k)]=E k[J(cid:101) η(1 (F) +G)(x k)] gives the bias bound after using the definition of T and (C.23a)
Plugging in (C.23b) and (C.22) in (C.21) gives the variance bound after substituting the values of T and
M.
C.3.1 Proof for Corollary 4.4
Corollary 4.4 is essentially the summary of the results proven below.
Let us remark the recent work [Bravo and Cominetti, 2024, Corollary 5.4] that studied stochastic KM
iterationfornonexpansiveoperatorsassumesaccesstoanunbiasedoracleandgetthecomplexityO(cid:101)(ε−4). As
mentioned in Section 4.2, this corresponds to requiring unbiased samples of J in our setting, which is
η(F+G)
difficultduetothedefinitionoftheresolvent. Wegetthesamecomplexityuptologarithmicfactorswithout
access to unbiased samples of J , which we go around by using the MLMC technique. We also do not
η(F+G)
require nonexpansiveness from J and work with star-conic nonexpansiveness.
η(F+G)
Theorem C.11. Let Assumptions 1, 3, and 4 hold. Consider Algorithm 6 with η < 1 and ρ < η. Then,
L
we have for K ≥1 that
64(∥x −x⋆∥2+α2σ2)log(K+3)
E [E∥(Id−J )(xout)∥2]≤ 0 √ ,
xout∼Unif{x0,...,xK−1} η(F+G)
α2 K
where α = 1− ρ. Each iteration makes, in expectation, O(log2(k +2)) calls to stochastic oracle B(cid:101) and
η
resolvent of A. Hence to obtain E∥(Id−J )(xout)∥ ≤ ε, we have the expected stochastic first-order
η(F+G)
complexity O(cid:101)(ε−4).
The main reason for the length of the following proof is the lack of boundedness of (x ). In particular,
k
proving this theorem is rather straightforward when we assume a bounded domain. We have to handle
the complications without this assumption. There are also additional difficulties that arise because we are
making sure that the inputs to MLMC-FBF will not involve unknown quantities such as ∥x −x⋆∥ or σ to run
0
the algorithm. These are, for example, used in Chen and Luo [2022] for setting the parameters. Because of
this reason, the bounds for ε and ε involve ∥(Id+J )(x )∥ and σ2.
k,v k,b η(F+G) k
The main reason for the difficulty here is ∥(Id+J )(x )∥2, since we do not have a uniform bound
η(F+G) k
on this quantity, unlike σ2 and this term appears in many terms. We will carry these terms coming from
the MLMC bounds to get a recursion involving the sum of ∥(Id+J )(x )∥2 for different ranges on
η(F+G) k
both sides. We then go around the issue of lacking of a bound on (x ) by using an inductive argument on
k
(cid:80)K ∥(Id+J )(x )∥2.
k=0 η(F+G) k
Proof of Theorem C.11. Recall our running notations:
ρ
α=1− η, R=Id−J η(F+G), R(cid:101)=Id−J(cid:101)η(F+G).
We start by following the proof of Lemma B.5. By α-star-cocoercivity of Id−J and α ≥ α
η(F+G) k
(which gives that J is 1 -star-conic nonexpansive), we can use property (B.2) derived in Lemma B.2
η(F+G) αk
to obtain
(cid:13) (cid:13)(1−α k)x k+α kJ η(F+G)(x k)−x⋆(cid:13) (cid:13)≤∥x k−x⋆∥
36and by the definition of x , we get
k+1
∥x k+1−x⋆∥≤∥(1−α k)x k+α kJ η(F+G)(x k)−x⋆∥+α k∥J(cid:101)η(F+G)(x k)−J η(F+G)(x k)∥
≤∥x k−x⋆∥+α k∥J(cid:101)η(F+G)(x k)−J η(F+G)(x k)∥. (C.24)
Summing the inequality for 0,...,k−1 gives
k−1
(cid:88)
∥x k−x⋆∥≤∥x 0−x⋆∥+ α i∥J(cid:101)η(F+G)(x i)−J η(F+G)(x i)∥
i=0
k−1
(cid:88)
=⇒ E∥x k−x⋆∥2 ≤2E∥x 0−x⋆∥2+2k α i2E∥J(cid:101)η(F+G)(x i)−J η(F+G)(x i)∥2, (C.25)
i=0
where we first squared both sides, used Young’s inequality and then took expectation.
We continue by restating the result of Lemma C.8 after applying Young’s inequality on the last term to
obtain
K−1 K−1
α (cid:88) 1 3 (cid:88)
α E∥(Id−J )(x )∥2 ≤ ∥x −x⋆∥2+ α2E[ε2 ]
4 k η(F+G) k 2 0 2 k k,v
k=0 k=0
K (cid:88)−1(cid:18) α2 (k+1)α2 (cid:19)
+ k E∥x −x⋆∥2+ E[ε2 ] . (C.26)
2α2(k+1) k 2 k,b
k=0
We now estimate the second and third terms on the right-hand side. By using Corollary C.10 and the
√
definition of R(x k), α k = √ k+2lα og(k+2) < √ 2α log3 and using v2 = 61 0 ≤ 2 2l 4og3, we obtain
K−1 K−1
3 (cid:88) α2E[ε2 ]≤ 3 (cid:88) α2v2(cid:0)E∥R(x )∥2+σ2(cid:1)
2 k k,v 2 k k
k=0 k=0
K−2
≤ αα K−1(E∥R(x )∥2+σ2)+ 3 (cid:88) α2v2(cid:0)E∥R(x )∥2+σ2(cid:1) . (C.27)
16 K−1 2 k k
k=0
We continue with the first part of the third term on the right-hand side of (C.26) and bound it using (C.25):
K (cid:88)−1 α2 1 K (cid:88)−1 α2
k E∥x −x⋆∥2 ≤ ∥x −x⋆∥2+ k E∥x −x⋆∥2
2α2(k+1) k 2 0 2α2(k+1) k
k=0 k=1
1 K (cid:88)−1 α2 (cid:32) k (cid:88)−1 (cid:33)
≤ ∥x −x⋆∥2+ k 2∥x −x⋆∥2+2k α2E[ε2 ] . (C.28)
2 0 2α2(k+1) 0 i i,v
k=1 i=0
We focus on the last term here to get
K (cid:88)−1 α2 k (cid:88)−1 1 K (cid:88)−2 K (cid:88)−1 k
k ·2k α2E[ε2 ]= α2α2E[ε2 ]
2α2(k+1) i i,v α2 k+1 k i i,v
k=1 i=0 i=0 k=i+1
(cid:32)K−1 (cid:33)K−2
1 (cid:88) (cid:88)
≤ α2 α2E[ε2 ]
α2 k i i,v
k=0 i=0
(cid:32)K−1 (cid:33)K−2
≤ 1 (cid:88) α2 (cid:88) α2v2(cid:0)E∥R(x )∥2+σ2(cid:1) ,
α2 k i i
k=0 i=0
where the last step used Corollary C.10.
37Plugging in back to (C.28) gives
K (cid:88)−1 α2 (cid:32) 1 K (cid:88)−1 α2 (cid:33)
k E∥x −x⋆∥2 ≤ + k ∥x −x⋆∥2
2α2(k+1) k 2 α2(k+1) 0
k=0 k=1
+(cid:32)K (cid:88)−1
α
k2(cid:33)K (cid:88)−2
α2v2(cid:0)E∥R(x )∥2+σ2(cid:1)
. (C.29)
α2 i i
k=0 i=0
Wefinallyestimatethesecondpartofthethirdtermontheright-handsideof (C.26)byusingCorollaryC.10:
K−1 K−1
(cid:88) k+1 (cid:88)
α2 E[ε2 ]≤α2 (k+1)b2E[∥R(x )∥2+σ2].
2 k,b k k
k=0 k=0
We use the setting b2 = αk and b2 < αK−1 to obtain
k 120α(k+1) K−1 16αK
K−1 K−2
α2 (cid:88) k+1 E[ε2 ]≤ αα K−1(E∥R(x )∥2+σ2)+α2 (cid:88) (k+1)b2E[∥R(x )∥2+σ2]. (C.30)
2 k,b 16 K−1 k k
k=0 k=0
We collect (C.27), (C.29), and (C.30) in (C.26) to get
αK (cid:88)−1
α E∥R(x )∥2
≤(cid:32) 1+K (cid:88)−1 α k2 (cid:33)
∥x −x⋆∥2+
αα
K−1σ2
8 k k α2(k+1) 0 8
k=0 k=1
+(cid:32)
3
+K (cid:88)−1
α
k2(cid:33)K (cid:88)−2
α2v2(cid:0)E∥R(x )∥2+σ2(cid:1)
2 α2 k k
k=0 k=0
K−2
(cid:88)
+α2 (k+1)b2E[∥R(x )∥2+σ2]. (C.31)
k k
k=0
We now show by induction that
K−1
α (cid:88) α E∥R(x )∥2 ≤C(cid:0) ∥x −x⋆∥2+α2σ2(cid:1) ∀K ≥1, (C.32)
k k 0
k=0
for some C to be determined. Let us set
α
α = √ ,
k
k+2log(k+3)
which gives
K (cid:88)−1 α2 K (cid:88)−1 α2
k <3, k <0.25, α ≤α ∀k ≥0.
α2 α2(k+1) k
k=0 k=0
With these estimations and α<1, (C.31) becomes
K−1
α (cid:88)
α E∥R(x )∥2 ≤1.25∥x −x⋆∥2+α2σ2
8 k k 0
k=0
K−2
(cid:88)
+4.5 v2(αα E∥R(x )∥2+α2σ2)
k k
k=0
K (cid:88)−2 (k+1)b2
+α kE[αα ∥R(x )∥2+α2σ2]. (C.33)
α k k
k
k=0
38Let us set
α 1
C =32, b2 = k , v2 =
k 120α(k+1) 60
and use the inductive assumption α(cid:80)K−2α E∥R(x )∥2 ≤32(∥x −x⋆∥2+α2σ2) in (C.33) to obtain
k=0 k k 0
K−1
α (cid:88)
α E∥R(x )∥2 ≤4(∥x −x⋆∥2+α2σ2),
8 k k 0
k=0
which verifies α(cid:80)K−1α E∥R(x )∥2 ≤32(∥x −x⋆∥2+α2σ2).
k=0 k k 0
For the base case, we use α 0 = √ 2α log3 < 1 and α−1-star-Lipschitzness of R = Id−J η(F+G) to get
αα ∥R(x )∥2 ≤∥x −x⋆∥2. This establishes the base case and completes the induction.
0 0 0
Finally, in view of Corollary C.10, and definitions of b ,v, each iteration makes expected number of calls
k
O(log2(k+1)). By using α k ≥ α K = √ K+2lα og(K+3) in (C.32) with C = 32 and multiplying both sides by
√
1 and using K+2 ≤ √2 which is true for K ≥1, we get the claimed rate result. By using the expected
KαK K K
cost of each iteration, we also get the final expected stochastic first-order complexity result.
D Additional Remarks on Related Work
There exist a line of works that attempted to construct local estimation of Lipschitz constants to offer an
improved range for ρ depending on the curvature [Pethick et al., 2022, Alacaoglu et al., 2023]. However,
these results cannot bring global improvements in the worst-case range of ρ where the limit for ρ is still
1 . This is because it is easy to construct examples where the local Lipschitz constants are the same as the
2L
global Lipschitz constant.
TheworkHajizadehetal.[2023]getslinearrateofconvergenceforinteractiondominantproblemswhich
is shown to be closely related to cohypomonoconity, see Example 1.2. One important difference is that
cohypomonotonicity is equivalent to α interaction dominance with α ≥ 0 whereas Hajizadeh et al. [2023]
requires α > 0 for linear convergence. This is an important difference because we know that cohypomono-
tonicity relaxes monotonicity and that that even monotonicity is not sufficient for linear convergence. Even
for monotone problems O(ε−1) is the optimal first-order oracle complexity (see, e.g., Yoon and Ryu [2021])
and hence it is also optimal with cohypomonotonicity.
In the literature for fixed point iterations, several works considered inexact Halpern or KM iterations
without characterizing explicit first-order complexity results, see for example Leu¸stean and Pinto [2021],
Bartz et al. [2022], Kohlenbach [2022], Combettes and Pennanen [2004]. In particular, Bartz et al. [2022]
used conic nonexpansiveness to analyze KM iteration. The dependence of the range of ρ on L arises when
we start characterizing the first-order complexity. This is the reason these works have not been included in
comparisons in Table 1.
Forthestochasticcohypomonotoneproblems,thebestcomplexityresulttoourknowledgeisduetoChen
and Luo [2022]. This paper can obtain the optimal complexity O(cid:101)(ε−2) with cohypomonotone stochastic
problems with a 6-loop algorithm using many carefully designed regularization techniques, extending the
work of Allen-Zhu [2018] that focused on minimization. Some disadvantages of this approach compared to
ours: (i) the bound for cohypomonotonicity is ρ≤ 1 ; (ii) the algorithm needs estimates of variance upper
2L
boundσ2 and,moreimportantly,∥x0−x⋆∥2; (iii) theresultisonlygivenforunconstrainedproblems,which
also makes it difficult to assume a bounded domain since there is no guarantee a priori for the iterates to
stay bounded for an unconstrained problem. Given that the 6-loop algorithm and analysis of Chen and Luo
[2022] is rather complicated, it is not clear to us if their arguments generalize to constraints or if the other
drawbacks can be alleviated.
The work Tran-Dinh and Luo [2023] focused on problems with ρ-weakly MVI solutions for ρ < 1 and
8L
derived O(ε−2) for a randomized coordinate algorithm. Due to randomization, the complexity result in
this work holds for the expectation of the optimality measure. Because of the coordinatewise updates, the
problem focused in this work is deterministic, similar to the setup in Section 3.
39D.1 Clarifications about Table 2
Sincethecomplexityresultshavenotbeenwrittenexplicitlyinsomeofthereferences, weprovidedetailson
how we computed the complexities that we report for the existing works.
Choudhury et al. [2023]: We use Theorem 4.5 in this corresponding paper to see that squared operator
norm is upper bounded by O(K−1). To make the operator norm smaller than ε, the order of K is ε−2. The
batch-size has order K and hence the total number of oracle calls is O(K2)=O(ε−4).
B¨ohmetal.[2022]: WeuseTheorem3.3inthiscorrespondingpaper. Thepaperstatedthattomakethe
squared operator norm smaller than ε, number of iterations is O(ε−2) and the batch size is O(ε−3). This
givescomplexityO(ε−3)formakingthesquared operatornormsmallerthanε. Hence, tomaketheoperator
norm smaller than ε, the complexity is O(ε−6).
[Pethick et al., 2023b]: (i) For “best rate” result, we use Corollary E.3(i) in this corresponding paper.
ThedominanttermintheboundforthesquaredresidualisO(K−1). Hencetomakethenormoftheresidual
smallerthanε(equivalently, thesquarednormsmallerthanε−2), oneneedsK tobeoftheorderε−2. Then,
thesquared variance isassumed todecrease attheorder ofk2 which requiresthebatchsize atiteration k to
be k2. Then the complexity is upper bounded by (cid:80)K τk2 = O(cid:101)(K3) = O(cid:101)(ε−6), (ii) for the “last iterate”,
k=1
we use the Corollary E.3(ii) given in the paper to see that the dominant term in the bound of the squared
(cid:16) (cid:17)
residual is O √1 . To make the squared residual smaller than ε2, this means K is of the order ε−4. The
K
squaredvarianceisassumedtodecreaseattheratek3 whichrequiresabatchsizeofk3 atiterationk. Then,
with the same calculation as before, the complexity of stochastic first-order oracles to make the residual less
than ε is (cid:80)K τk3 =O(cid:101)(K4)=O(cid:101)(ε−16).
k=1
Acknowledgments
This work was supported in part by the NSF grant 2023239, the NSF grant 2224213, the AFOSR award
FA9550-21-1-0084, National Research Foundation of Korea (NRF) grant funded by the Korea government
(MSIT)(No. 2019R1A5A1028324,2022R1C1C1003940),andtheSamsungScience&TechnologyFoundation
grant (No. SSTF-BA2101-02).
References
A. Alacaoglu, A. B¨ohm, and Y. Malitsky. Beyond the golden ratio for variational inequality algorithms.
Journal of Machine Learning Research, 24(172):1–33, 2023.
Z. Allen-Zhu. How to make the gradients small stochastically: Even faster convex and nonconvex sgd.
Advances in Neural Information Processing Systems, 31, 2018.
Anonymous. Semi-anchored gradient methods for nonconvex-nonconcave minimax problems, 2024a. URL
https://openreview.net/forum?id=rmLTwKGiSP.
Anonymous. Weaker MVI condition: Extragradient methods with multi-step exploration. In The Twelfth
International Conference on Learning Representations, 2024b. URL https://openreview.net/forum?
id=RNGUbTYSjk.
Y. Arjevani, Y. Carmon, J. C. Duchi, D. J. Foster, N. Srebro, and B. Woodworth. Lower bounds for
non-convex stochastic optimization. Mathematical Programming, 199(1-2):165–214, 2023.
H. Asi, Y. Carmon, A. Jambulapati, Y. Jin, and A. Sidford. Stochastic bias-reduced gradient methods.
Advances in Neural Information Processing Systems, 34:10810–10822, 2021.
S. Bartz, M. N. Dao, and H. M. Phan. Conical averagedness and convergence analysis of fixed point
algorithms. Journal of Global Optimization, 82(2):351–373, 2022.
H.H.BauschkeandP.L.Combettes. Convexanalysisandmonotoneoperatortheoryinhilbertspaces. CMS
Books in Mathematics, 2017.
40H.H.Bauschke,W.M.Moursi,andX.Wang. Generalizedmonotoneoperatorsandtheiraveragedresolvents.
Mathematical Programming, 189:55–74, 2021.
J. H. Blanchet and P. W. Glynn. Unbiased monte carlo for optimization and functions of expectations via
multi-level randomization. In 2015 Winter Simulation Conference (WSC), pages 3656–3667. IEEE, 2015.
A. B¨ohm. Solving nonconvex-nonconcave min-max problems exhibiting weak minty solutions. Transactions
on Machine Learning Research, 2022.
M.BravoandR.Cominetti. Stochasticfixed-pointiterationsfornonexpansivemaps: Convergenceanderror
bounds. SIAM Journal on Control and Optimization, 62(1):191–219, 2024.
A. B¨ohm, M. Sedlmayer, E. R. Csetnek, and R. I. Bot. Two steps at a time—taking gan training in stride
with tseng’s method. SIAM Journal on Mathematics of Data Science, 4(2):750–771, 2022.
X. Cai, A. Alacaoglu, and J. Diakonikolas. Variance reduced halpern iteration for finite-sum monotone
inclusions. In The Twelfth International Conference on Learning Representations, 2024.
Y. Cai and W. Zheng. Accelerated single-call methods for constrained min-max optimization. In The
Eleventh International Conference on Learning Representations, 2022.
Y. Cai, A. Oikonomou, and W. Zheng. Accelerated algorithms for monotone inclusions and constrained
nonconvex-nonconcave min-max optimization. arXiv preprint arXiv:2206.05248, 2022.
L.ChenandL.Luo. Near-optimalalgorithmsformakingthegradientsmallinstochasticminimaxoptimiza-
tion. arXiv preprint arXiv:2208.05925, 2022.
S. Choudhury, E. Gorbunov, and N. Loizou. Single-call stochastic extragradient methods for structured
non-monotonevariationalinequalities: Improvedanalysisunderweakerconditions. InAdvances in Neural
Information Processing Systems, 2023.
P. L. Combettes. Quasi-fej´erian analysis of some optimization algorithms. In Studies in Computational
Mathematics, volume 8, pages 115–152. Elsevier, 2001.
P. L. Combettes and T. Pennanen. Generalized mann iterates for constructing fixed points in hilbert spaces.
Journal of Mathematical Analysis and Applications, 275(2):521–536, 2002.
P. L. Combettes and T. Pennanen. Proximal methods for cohypomonotone operators. SIAM journal on
control and optimization, 43(2):731–742, 2004.
C.D.DangandG.Lan.Ontheconvergencepropertiesofnon-euclideanextragradientmethodsforvariational
inequalities with generalized monotone operators. Computational Optimization and applications, 60:277–
310, 2015.
M. N. Dao and H. M. Phan. Adaptive douglas–rachford splitting algorithm for the sum of two operators.
SIAM Journal on Optimization, 29(4):2697–2724, 2019.
C. Daskalakis, D. J. Foster, and N. Golowich. Independent policy gradient methods for competitive rein-
forcement learning. Advances in neural information processing systems, 33:5527–5540, 2020.
C. Daskalakis, S. Skoulakis, and M. Zampetakis. The complexity of constrained min-max optimization. In
Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 1466–1478,
2021.
J. Diakonikolas. Halpern iteration for near-optimal and parameter-free monotone inclusion and strong
solutions to variational inequalities. In Conference on Learning Theory, pages 1428–1451. PMLR, 2020.
J. Diakonikolas, C. Daskalakis, and M. I. Jordan. Efficient methods for structured nonconvex-nonconcave
min-max optimization. In International Conference on Artificial Intelligence and Statistics, pages 2746–
2754. PMLR, 2021.
41R. Dorfman and K. Y. Levy. Adapting to mixing time in stochastic optimization with markovian data. In
International Conference on Machine Learning, pages 5429–5446. PMLR, 2022.
F. Facchinei and J.-S. Pang. Finite-dimensional variational inequalities and complementarity problems.
Springer, 2003.
M. B. Giles. Multilevel monte carlo path simulation. Operations research, 56(3):607–617, 2008.
P. Giselsson and W. M. Moursi. On compositions of special cases of lipschitz continuous operators. Fixed
Point Theory and Algorithms for Sciences and Engineering, 2021(1):1–38, 2021.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio.
Generative adversarial nets. Advances in neural information processing systems, 27, 2014.
E. Gorbunov, A. Taylor, S. Horv´ath, and G. Gidel. Convergence of proximal point and extragradient-
based methods beyond monotonicity: the case of negative comonotonicity. In International Conference
on Machine Learning, pages 11614–11641. PMLR, 2023.
B.Grimmer,H.Lu,P.Worah,andV.Mirrokni. Thelandscapeoftheproximalpointmethodfornonconvex–
nonconcave minimax optimization. Mathematical Programming, 201(1-2):373–407, 2023.
C. Groetsch. A note on segmenting mann iterates. Journal of Mathematical Analysis and Applications, 40
(2):369–372, 1972.
S. Hajizadeh, H. Lu, and B. Grimmer. On the linear convergence of extragradient methods for nonconvex–
nonconcave minimax problems. INFORMS Journal on Optimization, 2023.
B. Halpern. Fixed points of nonexpanding maps. Bulletin of the American Mathematical Society, 73(6):
957–961, 1967.
Y.-G. Hsieh, F. Iutzeler, J. Malick, and P. Mertikopoulos. On the convergence of single-call stochastic
extra-gradient methods. Advances in Neural Information Processing Systems, 32, 2019.
Y.-P.Hsieh,P.Mertikopoulos,andV.Cevher. Thelimitsofmin-maxoptimizationalgorithms: Convergence
to spurious non-critical sets. In International Conference on Machine Learning, pages 4337–4348. PMLR,
2021.
Y. Hu, X. Chen, and N. He. On the bias-variance-cost tradeoff of stochastic optimization. Advances in
Neural Information Processing Systems, 34:22119–22131, 2021.
D.Kim. Acceleratedproximalpointmethodformaximallymonotoneoperators. MathematicalProgramming,
190(1-2):57–87, 2021.
U. Kohlenbach. On the proximal point algorithm and its halpern-type variant for generalized monotone
operators in hilbert space. Optimization Letters, 16(2):611–621, 2022.
G.Kotsalis,G.Lan,andT.Li.Simpleandoptimalmethodsforstochasticvariationalinequalities,i: operator
extrapolation. SIAM Journal on Optimization, 32(3):2041–2073, 2022.
M. A. Krasnosel’skii. Two remarks on the method of successive approximations. Uspekhi matematicheskikh
nauk, 10(1):123–127, 1955.
G.Lan. Policymirrordescentforreinforcementlearning: Linearconvergence,newsamplingcomplexity,and
generalized problem classes. Mathematical programming, 198(1):1059–1106, 2023.
S. Lee and D. Kim. Fast extra gradient methods for smooth structured nonconvex-nonconcave minimax
problems. Advances in Neural Information Processing Systems, 34:22588–22600, 2021.
L. Leu¸stean and P. Pinto. Quantitative results on a halpern-type proximal point algorithm. Computational
Optimization and Applications, 79(1):101–125, 2021.
42D. Levy, Y. Carmon, J. C. Duchi, and A. Sidford. Large-scale methods for distributionally robust optimiza-
tion. Advances in Neural Information Processing Systems, 33:8847–8860, 2020.
F. Lieder. On the convergence rate of the halpern-iteration. Optimization letters, 15(2):405–418, 2021.
N. Loizou, H. Berard, G. Gidel, I. Mitliagkas, and S. Lacoste-Julien. Stochastic gradient descent-ascent and
consensus optimization for smooth games: Convergence analysis under expected co-coercivity. Advances
in Neural Information Processing Systems, 34:19095–19108, 2021.
A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to
adversarial attacks. In International Conference on Learning Representations, 2018.
Y. Malitsky and M. K. Tam. A forward-backward splitting method for monotone inclusions without cocoer-
civity. SIAM Journal on Optimization, 30(2):1451–1472, 2020.
W. R. Mann. Mean value methods in iteration. Proceedings of the American Mathematical Society, 4(3):
506–510, 1953.
J.v.Neumann. Amodelofgeneraleconomicequilibrium. The Review of Economic Studies, 13(1):1–9, 1945.
T. Pethick, P. Patrinos, O. Fercoq, V. Cevher, and P. Latafat. Escaping limit cycles: Global convergence
for constrained nonconvex-nonconcave minimax problems. In International Conference on Learning Rep-
resentations, 2022.
T. Pethick, O. Fercoq, P. Latafat, P. Patrinos, and V. Cevher. Solving stochastic weak minty variational
inequalitieswithoutincreasingbatchsize. InInternationalConferenceonLearningRepresentations,2023a.
T. Pethick, W. Xie, and V. Cevher. Stable nonconvex-nonconcave training via linear interpolation. In
Thirty-seventh Conference on Neural Information Processing Systems, 2023b.
E. K. Ryu and W. Yin. Large-scale convex optimization: algorithms & analyses via monotone operators.
Cambridge University Press, 2022.
S. Sabach and S. Shtern. A first order method for solving convex bilevel optimization problems. SIAM
Journal on Optimization, 27(2):640–660, 2017.
Q. Tran-Dinh. Sublinear convergence rates of extragradient-type methods: A survey on classical and recent
developments. arXiv preprint arXiv:2303.17192, 2023.
Q. Tran-Dinh and Y. Luo. Randomized block-coordinate optimistic gradient algorithms for root-finding
problems. arXiv preprint arXiv:2301.03113, 2023.
P. Tseng. A modified forward-backward splitting method for maximal monotone mappings. SIAM Journal
on Control and Optimization, 38(2):431–446, 2000.
T. Yoon and E. K. Ryu. Accelerated algorithms for smooth convex-concave minimax problems with o (1/kˆ
2) rate on squared gradient norm. In International Conference on Machine Learning, pages 12098–12109.
PMLR, 2021.
T. Yoon and E. K. Ryu. Accelerated minimax algorithms flock together. arXiv preprint arXiv:2205.11093,
2022.
43