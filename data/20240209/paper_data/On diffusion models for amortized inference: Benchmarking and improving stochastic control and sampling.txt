On diffusion models for amortized inference:
Benchmarking and improving stochastic control and sampling
MarcinSendera123 MinsuKim124 SarthakMittal12 PabloLemos12567 LucaScimeca12
JarridRector-Brooks127 AlexandreAdam125 YoshuaBengio128 NikolayMalkin12
Abstract chainMonteCarlo(MCMC)–suchasMetropolis-adjusted
Langevin(MALA;Grenander&Miller,1994;Roberts&
We study the problem of training diffusion
Tweedie,1996;Roberts&Rosenthal,1998)andHamilto-
models to sample from a distribution with a
nianMC(HMC;Duaneetal.,1987;Hoffmanetal.,2014)
given unnormalized density or energy function.
–maybeslowtomixbetweenmodesandhaveahighcost
We benchmark several diffusion-structured
persample. WhilevariantssuchassequentialMC(SMC;
inference methods, including simulation-based
Halton, 1962; Chopin, 2002; Del Moral et al., 2006) and
variational approaches and off-policy methods
nested sampling (Skilling, 2006; Buchner, 2021; Lemos
(continuous generative flow networks). Our
etal.,2023)havebettermodecoverage,theircostmaygrow
results shed light on the relative advantages of
prohibitivelywiththedimensionalityoftheproblem. This
existingalgorithmswhilebringingintoquestion
motivates the use of amortized variational inference, i.e.,
someclaimsfrompastwork. Wealsoproposea
fittingparametricmodelsthatsamplethetargetdistribution.
novelexplorationstrategyforoff-policymethods,
basedonlocalsearchinthetargetspacewiththe Diffusionmodels,continuous-timestochasticprocessesthat
useofareplaybuffer,andshowthatitimproves graduallyevolveasimpledistributiontoacomplextarget,
the quality of samples on a variety of target arepowerfuldensityestimatorswithprovenmode-mixing
distributions. Ourcodeforthesamplingmethods properties (De Bortoli, 2022); as such, they have been
and benchmarks studied is made public at widely used in the setting of generative models learned
github.com/GFNOrg/gfn-diffusion fromdata(Sohl-Dicksteinetal.,2015;Songetal.,2021b;
asabaseforfutureworkondiffusionmodelsfor Hoetal.,2020;Nichol&Dhariwal,2021;Rombachetal.,
amortizedinference. 2021). However,theproblemoftrainingdiffusionmodels
tosamplefromadistributionwithagivenblack-boxdensity
orenergyfunctionhasattractedlessattention. Recentwork
1.Introduction hasdrawnconnectionsbetweendiffusion(learningthede-
noisingprocess)andstochasticcontrol(learningtheFo¨llmer
Approximating and sampling from complex multivariate drift),leadingtoapproachessuchasthepathintegralsam-
distributionsisafundamentalprobleminprobabilisticdeep pler(PIS;Zhang&Chen,2022),denoisingdiffusionsam-
learning(e.g.,Herna´ndez-Lobato&Adams,2015;Izmailov
pler(DDS;Vargasetal.,2023),andtime-reverseddiffusion
et al., 2021; Harrison et al., 2024) and in scientific appli- sampler(DIS;Berneretal.,2022);suchapproacheswere
cations(Albergoetal.,2019;Noe´ etal.,2019;Jingetal., recently unified by Richter et al. (2023). Another line of
2022;Adametal.,2022;Holdijketal.,2023). Theprob- work(Lahlouetal.,2023;Zhangetal.,2024)isbasedon
lem of drawing samples from a distribution given only continuousgenerativeflownetworks(GFlowNets),which
an unnormalized probability density or energy is partic- aredeepreinforcementlearningalgorithmsadaptedtovaria-
ularly challenging in high-dimensional spaces and when tionalinferencethatofferstableoff-policytrainingandthus
thedistributionofinteresthasmanyseparatedmodes(Ban- flexibleexploration(Malkinetal.,2023).
deira et al., 2022). Sampling methods based on Markov
Unfortunately, the advances in sampling methods have
1Mila – Que´bec AI Institute 2Universite´ de Montre´al not been accompanied by comprehensive benchmarking:
3Jagiellonian University 4KAIST 5Ciela Institute 6Center the reproducibility of the results is often unclear, with
for Computational Astrophysics, Flatiron Institute
the works differing in the choice of model architectures,
7Dreamfold 8CIFAR. Correspondence to: Marcin Sendera
using unstated hyperparameters, and even disagreeing in
<marcin.sendera@{mila.quebec,gmail.com}>,NikolayMalkin
<nikolay.malkin@mila.quebec>. their definitions of the same target densities.1 The first
Preprint.Copyright2024bytheauthor(s).
1Forexample,Vargasetal.(2023)notethatZhang&Chen
1
4202
beF
7
]GL.sc[
1v89050.2042:viXraOndiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
main contribution of this paper is a unified library for Chen, 2022) leverage stochastic optimal control for sam-
diffusion-structured amortized inference methods. The pling from unnormalized densities, albeit still struggling
library has a focus on off-policy methods (continuous withscalabilityinhigh-dimensionalspaces.
GFlowNets)butalsoincludessimulation-basedvariational
Finally,GFlowNets,originallydefinedinthediscretecase
objectives such as PIS. Using this codebase, we are able
byBengioetal.(2021;2023),viewhierarchicalsampling
tobenchmarkmethodsfrompastworkundercomparable
(i.e.,stepwisegeneration)asasequentialdecision-making
conditionsandconfirmclaimsaboutexplorationstrategies
processandrepresentasynthesisofreinforcementlearning
anddesirableinductivebiases,whilecallingintoquestion
andvariationalinferenceapproaches(Malkinetal.,2023;
other claims on the robustness and efficiency of credit
Zimmermannetal.,2023;Tiapkinetal.,2023),expanding
assignment. Ourlibraryalsoincludesseveralnewmodeling
from specific scientific domains (e.g., Jain et al., 2022;
and training techniques, and we provide preliminary
Atanackovic et al., 2023; Zhu et al., 2023) to amortized
evidenceoftheirutilityinpossiblefuturework(§5.3).
inference over a broader array of latent structures (e.g.,
Oursecondmaincontributionisastudyofmethodsfor van Krieken et al., 2023; Hu et al., 2024). Their ability
improvingcreditassignment–thepropagationoflearning to efficiently navigate trajectory spaces via off-policy
signalsfromthetargetdensitytotheparametersofearlier explorationhasbeencrucial,yettheyencounterchallenges
samplingsteps–indiffusion-structuredsamplers(§4).First, in training dynamics, such as credit assignment and
our results (§5.2) suggest that the technique of utilizing exploration efficiency (Malkin et al., 2022; Madan et al.,
partialtrajectoryinformation(Madanetal.,2022;Panetal., 2022; Pan et al., 2023; Rector-Brooks et al., 2023; Shen
2023),asdoneinthediffusionsettingbyZhangetal.(2024), et al., 2023; Kim et al., 2023; Jang et al., 2024). These
offerslittlebenefit,andahighertrainingcost,overon-policy challenges have repercussions in the scalability of these
(Zhang&Chen,2022)oroff-policy(Lahlouetal.,2023) methodsinmorecomplexscenarios,achallengethispaper
trajectory-based optimization. Second, we examine the addressesinthecontinuouscase.
utilityofagradient-basedvariantwhichparametrizesthe
denoisingdistributionasacorrectiontoaLangevinprocess
3.Setting: Diffusion-structuredsampling
(Zhang&Chen,2022). Weshowthatthisinductivebiasis
alsobeneficialintheoff-policy(GFlowNet)settingdespite Let E : R𝑑 → R be a differentiable energy function and
higher computational cost. Finally, motivated by recent define 𝑅(x) = exp(−E(x)), the reward or unnormalized
approachesinthediscretestatespacesetting,weproposean target density. Assuming the integral 𝑍 := ∫ 𝑅(x)𝑑x
R𝑑
efficientexplorationtechniquebasedonlocalsearchinthe exists,E definesaBoltzmanndensity 𝑝 (x) = 𝑅(x) on
target 𝑍
targetspacewiththeuseofareplaybuffer,whichimproves R𝑑
. We are interested in the problems of sampling from
samplequalityacrossvarioustargetdistributions. 𝑝 and approximating the partition function 𝑍 given
target
accessonlytoE andpossiblytoitsgradient∇E.
2.Priorwork
Wedescribetwocloselyrelatedperspectivesonthisprob-
Amortizedvariationalinference(VI)approachesutilizea lem: vianeuralSDEsandstochasticcontrol(§3.1)andvia
parametric model 𝑞 𝜃 to approximate a given target den- continuousgenerativeflownetworks(§3.2).
sity 𝑝 throughstochasticoptimization(Hoffmanetal.,
target
2013; Ranganath et al., 2014; Agrawal & Domke, 2021). 3.1.Euler-Maruyamahierarchicalsamplers
Notably,explicitdensitymodelslikeautoregressivemod-
GenerativemodelingwithSDEs. Diffusionmodelsas-
els and normalizing flows have been extensively utilized
sumeacontinuous-timegenerativeprocessgivenbyaneu-
indensityestimation(Rezende&Mohamed,2015;Dinh
ralstochasticdifferentialequation(SDE;Tzen&Raginsky,
etal.,2017;Wuetal.,2020;Gaoetal.,2020;Nicolietal.,
2019a;Øksendal,2003;Sa¨rkka¨ &Solin,2019):
2020). However,thesemodelsimposestructuralconstraints,
therebylimitingtheirexpressivepower(Cornishetal.,2020; 𝑑x𝑡 =𝑢(x𝑡,𝑡;𝜃)𝑑𝑡+𝑔(x𝑡,𝑡;𝜃)𝑑w𝑡, (1)
Grathwohletal.,2019;Zhang&Chen,2021). wherex followsafixedtractabledistribution𝜇 (suchasa
0 0
Theadoptionofdiffusionprocessesingenerativemodels Gaussianorapointmass). Theinitialdistribution 𝜇 0 and
hasstimulatedarenewedinterestinhierarchicalmodelsas the stochastic dynamics specified by (1) induce marginal
densityestimatorsandpavedthewayformoresophisticated densities 𝑝 𝑡 onR𝑑 foreach𝑡 > 0. Thefunctions𝑢 and𝑔
sampling methodologies (Vincent, 2011; Ho et al., 2020; havelearnableparametersthatwewishtooptimize,using
Tzen&Raginsky,2019b). ApproacheslikePIS(Zhang& someobjective,soastomaketheterminaldensity 𝑝 1close
to 𝑝 . Samplescanbedrawnfrom 𝑝 bysamplingx ∼
target 1 0
(2022)usesadifferentvarianceofthefirstcomponentintheFunnel 𝜇 andsimulatingtheSDE(1)totime𝑡 =1.
density, and the reference partition function value for the log- 0
GaussianCoxprocessbenchmarkalsovariesbetweenauthors. TheSDEdriving𝜇 to 𝑝 isnotunique. However,ifone
0 target
2Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
fixesareverse-timeSDE,ornoisingprocess, thatpushes asΔ𝑡 → 0(i.e., incrementsareinfinitesimallyGaussian),
𝑝 at𝑡 = 1to 𝜇 at𝑡 = 0,thenitsreverse,theforward an application of the central limit theorem that is key to
target 0
SDE(1),isuniquelydeterminedundermildconditionsand stochasticcalculus(Øksendal,2003).
is called the denoising process. For usual choices of the
noisingprocess,therearestochasticregressionobjectives SDElearningashierarchicalvariationalinference. The
for learning the drift 𝑢 of the denoising process given problem of learning the parameters 𝜃 of the forward pro-
samplesfrom 𝑝 target,andthediffusionrate𝑔isavailablein cesssoastoenforce(6)isoneofhierarchicalvariational
closedform(Hoetal.,2020;Songetal.,2021b). inference. Thebackwardprocesstransformsx intox viaa
1 0
sequenceoflatentvariablesx 1−Δ𝑡,...,x 0,andtheforward
Time discretization. In practice, the integration of the processaimstomatchtheposteriordistributionoverthese
SDE (1) is approximated by a discrete-time scheme, the variablesandthustoapproximatelyenforce(6).
simplest of which is Euler-Maruyama integration. The
Inthesettingofdiffusionmodelslearnedfromdata,where
process (1) is replaced by a discrete-time Markov chain
onehassamplesfrom 𝑝 ,onecanoptimizetheforward
x 0 → x Δ𝑡 → x 2Δ𝑡 → ··· → x 1,whereΔ𝑡 = 𝑇1 isthetime process by minimizingta tr hge et KL divergence 𝐷 (𝑝 ·
incrementandand𝑇 isthenumberofsteps: KL target
𝑝 𝐵∥𝜇 0·𝑝 𝐹)betweenthedistributionovertrajectoriesgiven
x ∼ 𝜇 , (2) bythereverseprocessandthat givenby theforwardpro-
0 0
√ cess. Thisisequivalenttothetypicaltrainingofdiffusion
x𝑡+Δ𝑡 =x𝑡 +𝑢(x𝑡,𝑡;𝜃)Δ𝑡+𝑔(x𝑡,𝑡;𝜃) Δ𝑡z𝑡 z𝑡 ∼N(0,I𝑑).
models, which optimizes a variational bound on the data
log-likelihood(seeSongetal.,2021a).
The density of the transition kernel from x𝑡 to x𝑡+Δ𝑡 can
explicitlybewrittenas However,inthesettingofanintractabledensity𝑝 ,unbi-
target
asedestimatorsofthisdivergencearenotavailable. Instead,
𝑝 𝐹(x𝑡+Δ𝑡 | x𝑡) =N(x𝑡+Δ𝑡;x𝑡+𝑢(x𝑡,𝑡;𝜃)Δ𝑡,𝑔(x𝑡,𝑡;𝜃)Δ𝑡I𝑑), onecanoptimizethereverseKL:3
(3)
where 𝑝 𝐹 denotesthetransitiondensityofthediscretized 𝐷 KL(𝜇 0· 𝑝 𝐹∥𝑝 target· 𝑝 𝐵) = (7)
f tro ar jw eca tr od riS eD ssE t. arT tih ni gsd ae tn xs 0i :tydefinesajointdistributionover ∫ log 𝑝 ta𝜇 rg0 et( (𝑥 𝑥0 1) )𝑝 𝑝𝐹 𝐵( (x xΔ 0𝑡 ,, .. .. .. ,, xx 11 −Δ| 𝑡x 0 |) x 1)𝜇 0(x 0)𝑑x Δ𝑡 ... 𝑑x 1.
𝑇−1
(cid:214)
𝑝 𝐹(x Δ𝑡,...,x 1 | x 0) = 𝑝 𝐹(x (𝑖+1)Δ𝑡 | x𝑖Δ𝑡). (4) Various estimators of this objective are available. For in-
stance,thepathintegralsamplerobjective(PIS;Zhang&
𝑖=0
Chen,2022)usesthereparametrizationtricktoexpress(7)
Similarly, adiscrete-timereverseprocessx 1 → x 1−Δ𝑡 → asanexpectationovernoisevariablesz𝑡 thatparticipatein
x 1−2Δ𝑡 →···→x 0withtransitiondensities 𝑝 𝐵(x𝑡−Δ𝑡 | x𝑡) thehierarchicalsamplingofx Δ𝑡,...,x 1,yieldinganunbi-
definesajointdistribution2via
asedgradientestimator,butonethatrequiresbackpropaga-
tionintothesimulationoftheforwardprocess. Therelated
𝑇
(cid:214)
𝑝 𝐵(x 0,...,x 1−Δ𝑡 | x 1) = 𝑝 𝐵(x (𝑖−1)Δ𝑡 | x𝑖Δ𝑡). (5) denoisingdiffusionsampler(DDS;Vargasetal.,2023)ap-
pliesthesameprincipleinadifferentintegrationscheme.
𝑡=1
If the forward and backward processes (starting from 𝜇
0 3.2.Euler-MaruyamasamplersasGFlowNets
and 𝑝 , respectively) are reverses of each other, then
target
theydefinethesamedistributionovertrajectories,i.e.,for Continuousgenerativeflownetworks(Lahlouetal.,2023)
allx 0 →x Δ𝑡 →···→x 1, express the problem of enforcing (6) as a reinforcement
learningtask. Inthissection,wesummarizethisinterpreta-
𝜇 0(x 0)𝑝 𝐹(x Δ𝑡,...,x 1|x 0) = 𝑝 target(x 1)𝑝 𝐵(x 0,...,x 1−Δ𝑡|x 1). tion,itsconnectiontoneuralSDEs,theassociatedlearning
(6) objectives,andtheirrelativeadvantagesanddisadvantages.
Inparticular,themarginaldensitiesofx undertheforward
1
andbackwardprocessesarethenequalto 𝑝 ,andthefor- Theconnectionbetweengenerativeflownetworksanddif-
target
fusion models was first made informally by Malkin et al.
wardprocesscanbeusedtosamplethetargetdistribution.
(2023) in the distribution-matching setting and by Zhang
Inpractice,(6)canbeenforcedonlyapproximately,since etal.(2023a)inthemaximum-likelihoodsetting. Continu-
thereverseofaprocesswithGaussianincrementsis,ingen- ousGFlowNets,andtheirconnectiontosamplingviaEuler-
eral,notitselfGaussian. However,thediscrepancyvanishes
3Tobeprecise,thefractionin(7)shouldbeunderstoodasa
2Inthecasethat𝜇 isapointmass,weassumethedistribution Radon-Nikodymderivative,whichmakessensewhether𝜇 isa
0 0
x
0
|xΔ𝑡 toalsobeapointmass,whichhasdensity𝑝 𝐵(x
0
|xΔ𝑡)= pointmassoracontinuousdistributionandgeneralizestocontinu-
1withrespecttothemeasure𝜇 . oustime(Berneretal.,2022;Richteretal.,2023).
0
3Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
Maruyamaintegration,werefirstsetonasolidtheoretical ThelearningproblemsolvedbyGFlowNetsistofindthe
foundationbyLahlouetal.(2023). parameters𝜃 ofapolicy 𝑝 𝐹 whoseterminatingdensity 𝑝⊤ 𝐹
isequalto 𝑝 ,i.e.,
target
Stateandactionspace. Toformulatesamplingasase-
𝑅(x )
quential decision-making problem, one must define the 𝑝⊤(x ;𝜃) = 1 ∀x ∈R𝑑. (9)
𝐹 1 𝑍 1
spaces of states and actions. In the case of sampling by
𝑇-stepEuler-Maruyamaintegration,assuming𝜇 isapoint However, because the integral (8) is intractable and 𝑍 is
0
massat0,thestatespaceis unknown, auxiliary objects must be introduced into opti-
mizationobjectivestoenforce(9),asdiscussedbelow.
S = {(0,0)∪(cid:8) (x,𝑡) :x∈R𝑑,𝑡 ∈ {Δ𝑡,2Δ𝑡,...,1}(cid:9),
Notably,ifthepolicyisaGaussianwithmeanandvariance
withthepoint(x,𝑡)representingthatthesamplingagentis given by neural networks taking x𝑡 and 𝑡 as input, then
atpositionxattime𝑡.
learningthepolicyamountstolearningthedrift𝑢(x𝑡,𝑡;𝜃)
anddiffusion𝑔(x𝑡,𝑡;𝜃) ofaSDE(1),i.e.,fittinganeural
Samplingbeginswiththeinitialstatex 0 := (0,0),proceeds SDE.Thelearningproblemin§3.1isthusthesameas
throughasequenceofstates(x Δ𝑡,Δ𝑡),(x 2Δ𝑡,2Δ𝑡),...,and thatoffittingaGFlowNetwithGaussianpolicies.
ends at a state (x ,1); states (x,𝑡) with 𝑡 = 1 are called
1
terminalstatesandtheircollectionisdenotedX. Fromnow Backwardpolicyandtrajectorybalance. Abackward
on,wewilloftenwritex𝑡 inplaceofthestate(x𝑡,𝑡)when policy is a collection of conditional probability densities
the time 𝑡 is clear from context. The sequence of states 𝑝 𝐵(x𝑡−Δ𝑡 | x𝑡;𝜓),representingaprobabilitydensityoftran-
x 0 →x Δ𝑡 →···→x 1iscalledacompletetrajectory. sitioningfromx𝑡 toanancestorstatex𝑡−Δ𝑡. Thebackward
Theactionsfromanonterminalstate (x𝑡,𝑡) correspondto policy induces a distribution over complete trajectories 𝜏
thepossiblenextstates (x𝑡+Δ𝑡,𝑡+Δ𝑡) thatcanbereached conditionedontheirterminalstate(cf.(5)):
from (x𝑡,𝑡) by a single step of the Euler-Maruyama 𝑇
(cid:214)
integrator.4 𝑝 𝐵(𝜏 | x 1;𝜓;𝜓) = 𝑝 𝐵(x (𝑖−1)Δ𝑡 | x𝑖Δ𝑡;𝜓),
𝑖=1
Forwardpolicyandlearningproblem. A(forward)pol- whereexceptionally 𝑝 𝐵(x
0
| x Δ𝑡) =1as𝜇 0isapointmass.
icyisacollectionofcontinuousdistributionsoverthesuc-
cessorstates–statesreachablebyasingleaction–ofevery Generalizingaresultinthediscrete-spacesetting(Malkin
nonterminalstate(x,𝑡).Inourcontext,thisamountstoacol- et al., 2022), Lahlou et al. (2023) show that 𝑝 𝐹 samples
lectionofconditionalprobabilitydensities𝑝 𝐹(x𝑡+Δ𝑡 | x𝑡;𝜃), from the target distribution (i.e., satisfies (9)) if and only
representingtheprobabilitydensityoftransitioningfrom ifthereexistsabackwardpolicy 𝑝 𝐵 andascalar 𝑍 𝜃 such
x𝑡 tox𝑡+Δ𝑡. GFlowNettrainingoptimizestheparameters𝜃, thatthetrajectorybalanceconditionsarefulfilledforevery
whichmaybetheweightsofaneuralnetworkspecifyinga completetrajectory𝜏 = (x 0 →x Δ𝑡 →···→x 1):
densityoverx𝑡+Δ𝑡 conditionedonx Δ𝑡.
𝑍 𝜃𝑝 𝐹(𝜏;𝜃) = 𝑝 target(x 1)𝑝 𝐵(𝜏 | x 1;𝜓). (10)
Apolicy𝑝 𝐹inducesadistributionovercompletetrajectories
𝜏 = (x 0 →x Δ𝑡 →···→x 1)via I fuf nth ce tis oe nc 𝑍on =di ∫tio 𝑅n (s xh )o 𝑑ld x.,t Th he en t𝑍 ra𝜃 jee cq tu oa ryls bt ah le antr cu ee op ba jr et cit ti io vn
e
x
𝑇−1 foratrajectory𝜏 isthesquaredlog-ratioofthetwosides
(cid:214)
𝑝 𝐹(𝜏;𝜃) = 𝑝 𝐹(x (𝑖+1)Δ𝑡 | x𝑖Δ𝑡;𝜃). of(10),thatis:
𝑖=0
Inparticular,wegetamarginaldensityoverterminalstates: L TB(𝜏;𝜃,𝜓) = (cid:18) log
𝑝
target(𝑍 x𝜃 1𝑝 )𝑝𝐹 𝐵(𝜏 (𝜏;𝜃 |)
x
1;𝜓)(cid:19)2 . (11)
∫
𝑝⊤ 𝐹(x 1;𝜃)= 𝑝 𝐹(x 0 →x Δ𝑡 →···→x 1;𝜃)𝑑x Δ𝑡...𝑑x 1−Δ𝑡. One can thus achieve (9) by minimizing to zero the loss
L (𝜏;𝜃,𝜓)withrespecttotheparameters𝜃 and𝜓,where
TB
(8) thetrajectories𝜏usedfortrainingaresampledfromsome
4Formally,themathematicalfoundationsinLahlouetal.(2023) trainingpolicy𝜋(𝜏). Whileitispossibletooptimize(11)
requireustodefinereferencemeasuresonthestateandaction with respect to the parameters of both the forward and
spaceswithrespecttowhichdensitiesmaybedefined.Here,we backwardpolicies,insomelearningproblems,onefixesthe
aredealingwithEuclideanspacesandalwaysassumetheLebesgue backwardpolicyandonlyoptimizestheparametersof 𝑝 𝐹
measure,soreadersneednotburdenthemselveswithmeasurethe-
ory.Wenote,however,thatthisflexibilityallowseasygeneraliza-
andtheestimateofthepartitionfunction𝑍 𝜃. Forexample,
tiontosamplingonmorecomplexspaces,suchasanyRiemannian formostexperimentsin§5,wefixthebackwardpolicyto
manifolds,whereothermethodsdonotdirectlyapply. adiscretizedBrownianbridge,followingpastwork.
4Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
Off-policy optimization. Unlike the KL objective (7), forward-lookingrewardshapingschemeproposedbyPan
whosegradientinvolvesanexpectationoverthedistribution etal.(2023). Ithasalsobeentestedinthecontinuouscase,
of trajectories under the forward process, (11) can be op- butourexperimentalresultssuggestthatitofferslittleben-
timizedoff-policy,i.e.,usingtrajectoriessampledfroman efitovertheTBobjectiveinthediffusionsetting(see§4.1).
arbitarydistribution𝜋.BecauseminimizingL (𝜏;𝜃,𝜓)to
TB It is also worth noting the off-policy VarGrad estimator
0forall𝜏inthesupportof𝜋willachieve(9),𝜋canbetaken
(Richteretal.,2020),originallystatedforhierarchicalvari-
beanydistributionwithfullsupport,soastopromotedis-
ationalmodelsandrediscoveredforGFlowNetsbyZhang
coveryofmodesofthetargetdistribution. Variouschoices
et al. (2023b). Like TB, VarGrad can be optimized over
motivatedbyreinforcementlearningtechniqueshavebeen
trajectoriesdrawnoff-policy. Ratherthanenforcing(10)for
proposed,includingnoisyexplorationortempering(Bengio
everytrajectory,VarGradoptimizestheempiricalvariance
etal.,2021),replaybuffers(Deleuetal.,2022),Thompson
(overaminibatch)ofthelog-ratioofthetwosidesof(10).
sampling(Rector-Brooksetal.,2023),andbackwardtraces
AsnotedbyMalkinetal.(2023),thisisequivalenttomini-
from terminal states obtained by MCMC (Lemos et al.,
2023). Inthecontinuouscase,Malkinetal.(2023);Lahlou mizingL TB firstwithrespecttolog𝑍 𝜃 tooptimalityover
thebatch,thenwithrespecttotheparametersof 𝑝 𝐹.
etal.(2023)proposedtosimplyaddasmallconstanttothe
policyvariancewhensamplingtrajectoriesfortraining. Off-
policyoptimizationisakeyadvantageofGFlowNetsover 4.CreditassignmentincontinuousGFlowNets
variationalmethodssuchasPIS,whichrequireon-policy
Themainchallengesintrainingoff-policysamplingmodels
optimization(Malkinetal.,2023).
areexplorationefficiency(discoveryofhigh-rewardstates)
However,whenL happenstobeoptimizedonpolicy,i.e., and credit assignment (propagation of reward signals to
TB
usingtrajectoriessampledfromthepolicy 𝑝 𝐹 itself,weget the actions that led to them). We describe several new
anunbiasedestimatorofthegradientoftheKLdivergence and existing methods for addressing these challenges in
withrespectto 𝑝 𝐹’sparametersuptoaconstant(Malkin the context of diffusion-structured GFlowNets. These
etal.,2023;Zimmermannetal.,2023),thatis: techniqueswillbeempiricallystudiedandcomparedin§5.
E 𝜏∼𝑝𝐹(𝜏) [∇𝜃L TB(𝜏;𝜃,𝜓)] = 4.1.Partialenergiesandsubtrajectory-basedlearning
=2∇𝜃𝐷 KL(𝑝 𝐹(𝜏;𝜃)∥𝑝 target(x 1)𝑝 𝐵(𝜏 | x 1;𝜓)).
Zhang et al. (2024) studied the continuous GFlowNet
Thisgradientestimatortendstohavehighervariancethan learning problem introduced by Lahlou et al. (2023), but
thereparametrization-basedestimatorof(7)usedbyPIS. replacedtheTBlearningobjectivewiththeSubTBobjec-
Ontheotherhand,itisstillunbiased,doesnotrequireback- tive. Inaddition, forthestateflowfunction, aninductive
propagationthroughthesimulationoftheforwardprocess, biasresemblingthegeometricdensityinterpolationinMa´te´
and can be used to optimize the parameters of both the &Fleuret(2023)wasused:
forwardandbackwardpolicies.
log 𝑓(x𝑡;𝜃) = (1−𝑡)log𝑝r 𝑡ef(x𝑡)+𝑡log𝑅(x𝑡)+NN(x𝑡,𝑡;𝜃),
Otherobjectives. Thetrajectorybalanceobjective(11) (13)
is not the only possible objective that can be used to en- whereNNisaneuralnetworkand 𝑝r 𝑡ef(x𝑡) =N(x𝑡;0,𝜎2𝑡)
force(9). Anotablegeneralizationissubtrajectorybalance isthemarginaldensityofaBrownianmotionwithrate𝜎at
(SubTB;Madanetal.,2022), whichinvolvesmodelinga x𝑡. Theuseofthetargetdensitylog𝑅(x𝑡) =−E(x𝑡)inthe
scalarstateflow 𝑓(x𝑡;𝜃)fassociatedwitheachstatex𝑡 –in- stateflowfunctionwashypothesizedtoprovideaneffective
tendedtomodelthemarginaldensityoftheforwardprocess signal driving the sampler to high-density states at early
atx𝑡 –andenforcingsubtrajectorybalanceconditionsfor stepsinthetrajectory. Suchaninductivebiasonthestate
allpartialtrajectoriesx𝑚Δ𝑡 →x (𝑚+1)Δ𝑡 →···→x𝑛Δ𝑡: flowwascalledforward-looking(FL)byPanetal.(2023),
andwewillrefertothismethodasFL-SubTBin§5.
𝑛−1
(cid:214)
𝑓(x𝑚;𝜃) 𝑝 𝐹(x (𝑖+1)Δ𝑡 | x𝑖Δ𝑡;𝜃) =
4.2.Langevindynamicsinductivebias
𝑖=𝑚
(cid:214)𝑛 Zhang & Chen (2022) proposed an inductive bias on
= 𝑓(x𝑛;𝜃) 𝑝 𝐵(x (𝑖−1)Δ𝑡 | x𝑖Δ𝑡;𝜓), (12) the architecture of the drift of the neural SDE 𝑢(x𝑡,𝑡;𝜃)
𝑖=𝑚+1
(in GFlowNet terms, the mean of the Gaussian density
where for terminal states 𝑓(x ) = 𝑅(x ). This approach 𝑝 𝐹(x𝑡+Δ𝑡 | x𝑡;𝜃)) that resembles a Langevin process on
1 1
thetargetdistribution. Onewrites
hassomecomputationaloverheadassociatedwithtraining
the state flow, but has been shown to be effective in
discrete-spacesettings,especiallywhencombinedwiththe 𝑢(x𝑡,𝑡;𝜃) =NN 1(x𝑡,𝑡;𝜃)+NN 2(𝑡;𝜃)∇E(x𝑡), (14)
5Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
whereNN andNN areneuralnetworksoutputtingavector Trainingwithlocalsearch. Totrainsamplerswiththe
1 2
andascalar,respectively.Thesecondtermin(14)isascaled aidofthebuffer,wedrawasamplexfromD (uniformly
LS
gradientofthetargetenergy–thedriftofaLangevinSDE orusingaprioritizationscheme,§D),sampleatrajectory𝜏
–andthefirsttermisalearnedcorrection. Thisinductive leadingtoxfromthebackwardprocess,andmakeagradient
bias,whichwenametheLangevinparametrization(LP), updateontheobjective(e.g.,TB)associatedwith𝜏.
wasshowntoimprovetheefficiencyofPIS.Wewillstudy
When training with local search guidance, we alternate
forthefirsttimeitseffectoncontinuousGFlowNetsin§5.
twosteps,inspiredbyLemosetal.(2023),whoalternate
Theinductivebias(14)placedonpoliciesrepresentsadiffer- trainingonforwardtrajectoriesandbackwardtrajectories
entwayofincorporatingtherewardsignalatintermediate initializedatafixedsetofMCMCsamples. StepAinvolves
steps in the trajectory and can steer the sampler towards training with on-policy or exploratory forward sampling
low-energyregions. Itcontrastswith(13)inthatitprovides while Step B uses samples drawn from the local search
thegradientoftheenergydirectlytothepolicy,ratherthan bufferdescribedabove. Thisallowsthesamplertoexplore
justusingtheenergytoprovidealearningsignaltopolicies bothdiversifiedsamples(StepA)andlow-energysamples
viatheparametrizationofthelog-stateflow(13). (StepB).See§Dfordetailedpseudocodeofadaptive-step
parallelMALAandlocalsearch-guidedGFlowNettraining.
Considerationsofthecontinuous-timelimitleadustocon-
jecture that the Langevin parametrization (14) with NN
1
independentofx𝑡 isequivalenttotheforward-lookingflow 5.Experiments
(13) in the limit of small time increments Δ𝑡 → 0, i.e.,
Weconductcomprehensivebenchmarksofvariousdiffusion-
theyinducethesameasymptoticsofthediscrepancyinthe
structured samplers, encompassing both GFlowNet sam-
SubTBconstraints(12)overshortpartialtrajectories. Such
plersandmethodssuchasPIS.FortheGFlowNetsamplers,
theoreticalanalysiscanbethesubjectoffuturework.
we investigate a range of techniques, including different
explorationstrategiesandlossfunctions. Additionally,we
4.3.Efficientexplorationwithlocalsearch
examinetheefficacyoftheLangevinparametrizationand
TheFLandLPinductivebiasesbothinducecomputational thenewlyproposedlocalsearchwithbuffer.
overhead:eitherintheevaluationandoptimizationofastate
floworintheneedtoevaluatetheenergygradientatevery 5.1.Tasksandbaselines
stepofsampling. Wepresentanalternativetechniquethat
Weexploretwotypesoftasksinourstudy: firstly,sampling
does not induce additional computation cost per training
fromenergydistributions–a2-dimensionalGaussianmix-
trajectoryoverthatofregularTBtrainingofGFlowNets.
turemodelwith25modes(25GMM),the10-dimensional
Toenhancethequalityofsamplesduringtraining,wepro- Funnel,andthe32-dimensionalManywelldistribution–
pose the incorporation of local search methodologies to and,secondly,conditionalsamplingfromthelatentposte-
steertheexplorationprocessofGFlowNets. Whilepromis- riorofavariationalautoencoder(VAE; Kingma&Welling
inglocalsearchstrategies(Zhangetal.,2022;Kimetal., (2014);Rezendeetal.(2014)). Thisallowsustoinvestigate
2024)andreplaybuffermethods(e.g.Deleuetal.,2022) both unconditional and conditional generative modeling
forGFlowNetsindiscretespaceshavebeenproposed,our techniques. Alltargetdensitiesaredescribedin§B.
methodology leverages the parallel Metropolis-adjusted
Weevaluatethreealgorithmcategories:
Langevinalgorithm(MALA)forcontinuousdomains.
(1) Traditional sampling methods. Weconsider a stan-
Indetail,weinitiallysample 𝑀 candidatesfromthesam-
pler: {x(1),...,x(𝑀)} ∼ 𝑝⊤(·). Subsequently,wedeploy dard Sequential Monte Carlo (SMC) implementation
𝐹 andastate-of-the-artnestedsamplingmethod(GGNS,
parallelLangevindynamicssamplingacross𝑀 chains,with
theinitialstatesoftheMarkovchainbeing{x(1),...,x(𝑀)}. Lemosetal.(2023)).
(2) Simulation-driven variational approaches. Our
Thisprocessinvolvesexecuting𝐾 transitions,duringwhich
benchmarks include DIS (Berner et al., 2022), DDS
certaincandidatesmayberejectedbasedontheMetropolis-
(Vargasetal.,2023),andPIS(Zhang&Chen,2022).
Hastingsacceptancerule. Afterthe𝐾 burn-intransi-
burn-in (3) Diffusion-basedGFlowNetsamplers. Ourevaluation
tions, the accepted samples are stored in the local search
focuses on TB-based training and the enhancements
bufferD .WeoccasionallyupdatethebufferusingMALA
LS describedin§4: theVarGradestimator(VarGrad),off-
stepsandreplaysamplesfromittominimizethecomputa-
policyexploration(Expl.), Langevinparametrization
tionaldemandsofiterativelocalsearch. MALAstepsare
(LP),andlocalsearch(LS).Additionally,weassessthe
farmoreparallelizablethansamplertrainingandneedtobe
FL-SubTB-basedcontinuousGFlowNetasstudiedby
madeonlyrarely,sotheoverheadoflocalsearchissmall.
Zhangetal.(2024)foracomprehensivecomparison.
6Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
Table1.Log-partitionfunctionestimationerrorsand2-Wassersteindistancesforunconditionalmodelingtasks(meanandstandard
deviationover5runs).Thefourgroupsofmodelsare:MCMC-basedsamplers,simulation-drivenvariationalmethods,baselineGFlowNet
methodswithdifferentlearningobjectives,andmethodsaugmentedwiththeLangevinparametrizationandlocalsearchtechniques.
Energy→ 25GMM(𝑑=2) Funnel(𝑑=10) Manywell(𝑑=32)
Algorithm↓Metric→ Δlog𝑍 Δlog𝑍RW W2 Δlog𝑍 Δlog𝑍RW W2 Δlog𝑍 Δlog𝑍RW W2
2 2 2
SMC 0.569±0.010 0.86±0.10 0.561±0.801 50.3±18.9 14.99±1.078 8.28±0.32
GGNS(Lemosetal.,2023) 0.016±0.042 1.19±0.17 0.033±0.173 25.6±4.75 0.292±0.454 6.51±0.32
DIS(Berneretal.,2022) 1.125±0.056 0.986±0.011 4.71±0.06 0.839±0.169 0.093±0.038 20.7±2.1 N/A N/A N/A
DDS(Vargasetal.,2023) 1.760±0.08 0.746±0.389 7.184±0.044 0.424±0.049 0.206±0.033 29.3±9.5 N/A N/A N/A
PIS(Zhang&Chen,2022) 1.769±0.104 1.274±0.218 6.37±0.65 0.534±0.008 0.262±0.008 22.0±4.0 3.85±0.03 2.69±0.04 6.15±0.02
+LP 1.799±0.051 0.225±0.583 7.16±0.11 0.587±0.012 0.285±0.044 22.1±4.0 13.19±0.82 0.07±0.85 6.55±0.34
TB(Lahlouetal.,2023) 1.176±0.109 1.071±0.112 4.83±0.45 0.690±0.018 0.239±0.192 22.4±4.0 4.01±0.04 2.67±0.02 6.14±0.02
TB+Expl.(Lahlouetal.,2023) 0.560±0.302 0.422±0.320 3.61±1.41 0.749±0.015 0.226±0.138 21.3±4.0 4.01±0.05 2.68±0.06 6.15±0.02
VarGrad+Expl. 0.615±0.241 0.487±0.250 3.89±0.85 0.642±0.010 0.250±0.112 22.1±4.0 4.01±0.05 2.69±0.06 6.15±0.02
FL-SubTB(Zhangetal.,2024) 1.150±0.054 1.043±0.054 4.74±0.23 0.541±0.010 0.227±0.106 22.1±4.0 4.05±0.05 2.71±0.02 6.15±0.01
TB+Expl.+LS(ours) 0.171±0.013 0.004±0.011 1.25±0.18 0.653±0.025 0.285±0.099 21.9±4.0 4.57±2.13 0.19±0.29 5.66±0.05
TB+Expl.+LP(ours) 0.206±0.018 0.011±0.010 1.29±0.07 0.666±0.615 0.051±0.616 22.3±3.9 7.46±1.74 1.06±1.11 5.73±0.31
TB+Expl.+LP+LS(ours) 0.190±0.013 0.007±0.011 1.31±0.07 0.768±0.052 0.264±0.063 21.8±3.9 4.68±0.49 0.07±0.17 5.33±0.03
Highlight:meannotdistinguishablefromminimumincolumnwith𝑝<0.05assumingGaussian-distributederrors(Welchunpaired𝑡-test).
0.0
0.5
1.0
1.5
Constant exploration
2.0 Decaying exploration
Ground truth
Figure1.Two-dimensionalprojectionsofManywellsamplesfrom 2.5
modelstrainedbydifferentalgorithms.Ourproposedreplaybuffer 0 0.1 0.2 0.3 0.4 0.5
Exploration rate
withlocalsearchiscapableofpreventingmodecollapse.
Figure2.EffectofexplorationvarianceonmodelstrainedwithTB
onthe25GMMenergy.Explorationpromotesmodediscovery,but
For(2)and(3),weemployaconsistentneuralarchitecture shouldbedecayedovertimetooptimallyallocatethemodeling
powertohigh-likelihoodtrajectories.
acrossmethods,benchmarkingthemwithinourcomprehen-
sivelibrary(detailsin§C).
Benchmarkingmetrics. Toevaluatediffusion-basedsam-
Learningproblemandfixedbackwardprocess. Inour plers,weusetwometricsfrompastwork(Zhang&Chen,
main experiments, we borrow the modeling setting from 2022;Lahlouetal.,2023),whichwerestateinournotation.
Zhang&Chen(2022). WeaimtolearnaGaussianforward Givenanyforwardpolicy 𝑝 𝐹,wehaveavariationallower
policy 𝑝 𝐹 thatsamplesfromthetargetdistributionin𝑇 = boundonthelog-partitionfunctionlog𝑍 =∫ R𝑑 𝑅(x)𝑑x:
1
2
p0
0
ro0
2
c2s e;t se
L
sp as
ih
s( loΔ fiu𝑡
xe
e=
t
d0
a
tl. o.0 ,1
2
a) 0. d2J i3u
s;
cs Zt rea
h
ts
a
izni en
g
dp ea
t
Bs
a
rt
l
o.w
,
w2o n0r ik
2 a4
n( )Z
,
bh
t
rha in
e
dg
gb
ea&
c
wkC
w
ith hae rn
d
a, log∫
R𝑑
𝑅(x)𝑑x=logE 𝜏=(···→x1)∼𝑝𝐹(𝜏)
(cid:20)𝑅(x
𝑝1
𝐹) (𝑝 𝜏𝐵( |𝜏
x
1| )x 1)(cid:21)
noiserate𝜎thatdependsonthedomain;explicitly,
≥ E 𝜏=(···→x1)∼𝑝𝐹(𝜏)
(cid:20)
log
𝑅(x
𝑝1
𝐹) (𝑝 𝜏𝐵( |𝜏
x
1| )x 1)(cid:21)
.
(cid:18) 𝑡−Δ𝑡 𝑡−Δ𝑡 (cid:19)
𝑝 𝐵(x𝑡−Δ𝑡 | x𝑡)=N x𝑡−Δ𝑡;
𝑡
x𝑡,
𝑡
𝜎2Δ𝑡I𝑑 , (15) Weusea𝐾-sampleMonteCarloestimateofthisexpectation,
log𝑍ˆ,asametric,whichalwaysequalsthetruelog𝑍 if 𝑝 𝐹
and 𝑝 𝐵 jointlysatisfy(10)andthus 𝑝 𝐹 samplesfromthe
understoodtobeapointmassat0when𝑡 = Δ𝑡. Tokeep targetdistribution. (Inourexperiments,𝐾 =2000.)
thelearningproblemconsistentwithpastwork,wefixthe
varianceoftheforwardpolicy 𝑝 𝐹 to𝜎2. Thissimplification We also employ an importance-weighted variant, which
isjustifiedincontinuoustime,whentheforwardandreverse emphasizesmodecoverageoveraccuratelocalmodeling:
wSD illE ps rh oa vv ie deth ee vs ida em ne cedi tf hfu atsi lo en arr nat ie n. gH tho ew fe ov re wr, ai rn d§ p5 o.3 li, cyw ’e
s log𝑍ˆRW
:=log∑︁𝐾 (cid:34)
log
𝑅(x 1(𝑖))𝑝 𝐵(𝜏(𝑖) | x 1(𝑖))(cid:35)
,
varianceisquitebeneficialforshortertrajectories. 𝑖=1 𝑝 𝐹(𝜏(𝑖) | x 1(𝑖))
7
ZgolOndiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
baselines,non-GFlowNetbaselines,andnon-amortizedsam-
Table2.Log-likelihoodestimatesonatestsetforapretrainedVAE
plingmethodsinmosttasksandmetrics. Thisadvantageis
decoderonMNIST.Thelatentbeingsampledis20-dimensional.
attributedtoefficientexplorationandtheabilitytoreplay
TheVAE’strainingELBO(Gaussianencoder)was≈−101.
pastlow-energyregions,thuspreventingmodecollapsedur-
Algorithm↓Metric→ log𝑍ˆ log𝑍ˆRW
ingtraining(Fig.1). FurtherdetailsonLSenhancements
GGNS(Lemosetal.,2023) −82.406±0.882
arediscussedin§Dwithablationstudiesin§D.2.
PIS(Zhang&Chen,2022) −102.54±0.437 −47.753±2.821
+LP −99.890±0.373 −47.326±0.777 Incorporating Langevin parametrization (LP) into TB re-
TB(Lahlouetal.,2023) −162.73±35.55 −61.407±17.83 sultsinnotableperformanceimprovements(despite2-3×
VarGrad −102.54±0.934 −46.502±1.018 slowertimeperiteration),indicatingthattheobservations
TB+Expl.(Lahlouetal.,2023) −148.04±4.046 −49.967±5.683 ofZhang&Chen(2022)transfertooff-policyalgorithms.
FL-SubTB(Zhangetal.,2024) −202.78±144.9 −67.501±38.08 ComparedtoFL-SubTB,whichaimsforenhancedcredit
TB+Expl.+LS(ours) −245.78±13.80 −55.378±9.125 assignment through partial energy, LP achieves superior
TB+Expl.+LP(ours) −112.45±0.671 −48.827±1.787 creditassignmentleveraginggradientinformation,akinto
TB+Expl.+LP+LS(ours) −117.26±2.502 −49.157±2.051
partial energy in a continuous-time domain. LP is either
VarGrad+Expl.(ours) −103.39±0.691 −47.318±1.981
VarGrad+Expl.+LS(ours) −105.40±0.882 −48.235±0.891 superiororcompetitiveacrossmosttasksandmetrics.
VarGrad+Expl.+LP(ours) −99.472±0.259 −46.574±0.736
VarGrad+Expl.+LP+LS(ours) −99.783±0.312 −46.245±0.543
Conditional sampling. For the VAE task, we observe
thattheperformanceofthebaselineGFlowNet-basedsam-
where𝜏(1),...,𝜏(𝐾) aretrajectoriessampledfrom 𝑝 𝐹 and plersisgenerallyworsethanthatofthesimulation-based
leading to terminal states
x(1),...,x(𝐾)
. The estimator
PIS(Table2). WhileLPandLSimprovetheperformance
1 1 ofTB,theydonotclosethegapinlikelihoodestimation;
log𝑍ˆRW is also a lower bound on log𝑍 and approaches
however, with the VarGrad objective, the performance is
it as 𝐾 → ∞ (Burda et al., 2016). In the unconditional
competitive with or superior to PIS. We hypothesize that
modelingbenchmarks,wecomparebothestimatorstothe
this discrepancy is due to the difficulty of fitting the con-
truelog-partitionfunction,whichisknownanalytically.
ditionallog-partitionfunctionestimator,whichisrequired
Inaddition,wecomputeasample-basedmetric,thesquared fortheTBobjectivebutnotforVarGrad,whichonlylearns
2-WassersteindistanceW2betweensetsof𝐾samplesfrom thepolicy. (InFig.C.1weshowdecodedsamplesencoded
2
thetruedistributionandgeneratedbyatrainedsampler. usingthebest-performingdiffusionencoder.)
5.2.Results 5.3.ExtensionstogeneralSDElearningproblems
Unconditional sampling. We report the metrics for all Ourimplementationofdiffusion-structuredgenerativeflow
algorithmsandenergiesinTable1. networksincludesseveraladditionalfeaturesthatdiverge
fromthemodelingassumptionsmadeinpastworkinthe
WeobservethatTB’sperformanceisgenerallymodestwith-
field. Notably,itfeaturestheabilityto:
out additional exploration and credit assignment mecha-
nisms,exceptontheFunneltask,wherevariationsinper- • optimizethebackward(noising)process,notonlythe
formance across methods are negligible. This confirms denoisingprocess;
hypotheses from past work about the importance of off- • learn the forward process’s diffusion rate 𝑔(x𝑡,𝑡;𝜃),
policyexploration(Malkinetal.,2023;Lahlouetal.,2023) notonlythemean𝑢(x𝑡,𝑡;𝜃);
andtheimportanceofimprovedcreditassignment(Zhang • assumeavaryingnoisescheduleforthebackwardpro-
etal.,2024). Ontheotherhand,ourresultsdonotshowa cess, making it possible to train models with standard
consistentandsignificantimprovementoftheFL-SubTB noisingSDEsusedfordiffusionmodelsforimages.
objectiveusedbyZhangetal.(2024)overTB.Replacing
Theseextensionswillallowotherstobuildonourimplemen-
TBwiththeVarGradobjectiveyieldssimilarresults.
tationandapplyittoproblemssuchasfinetuningdiffusion
Thesimpleoff-policyexplorationmethodofaddingvariance modelstrainedonimageswithaGFlowNetobjective.
tothepolicynotablyenhancesperformanceonthe25GMM
Asnotedin§5.1,inthemainexperimentswefixedthedif-
task. We investigate this phenomenon in more detail in
fusionrateofthelearnedforwardprocess,anassumption
Fig.2,findingthatexplorationthatslowlydecreasesover
inheritedfromallpastworkandjustifiedinthecontinuous-
thecourseoftrainingisthebeststrategy.
timelimit. However,weperformanexperimenttoshowthe
Ontheotherhand,ourlocalsearch-guidedexplorationwith importanceofextensionssuchaslearningtheforwardvari-
a replay buffer (LS) leads to a substantial improvement anceindiscretetime. Fig.3showsthesamplesofmodels
inperformance,surpassingorcompetingwithGFlowNet onthe25GMMenergyfollowingtheexperimentalsetupof
8Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
0
1
2
3
4
Fixed variance
5
Learned variance
6 Ground truth
1 2 4 8 16 32 64 128
Number of discretization steps
Figure3.Left:Distributionofx 0,x 0.1,...,x 1learnedby10-stepsamplerswithfixed(top)andlearned(middle)forwardpolicyvariance
onthe25GMMenergy. Thelaststepofsamplingthefixed-variancemodeladdsGaussiannoiseofavarianceclosetothatofthe
componentsofthetargetdistribution,preventingthethesamplerfromsharplycapturingthemodes.Thelastrowshowsthepolicyvariance
learnedasafunctionofx𝑡 atvarioustimesteps𝑡(whiteishighvariance,blueislow),showingthatlessnoiseisaddedaroundthepeaks
near𝑡 =1.Thetwomodels’log-partitionfunctionestimatesare−1.67and−0.62,respectively.Right:Forvaryingnumberofsteps𝑇,we
plotthelog𝑍ˆ obtainedbymodelswithfixedandlearnedvariance.Learningpolicyvariancesgivessimilarsamplerswithfewersteps.
Lemosetal.(2023). Weseethatwhentheforwardpolicy’s helpfuldiscussions.
varianceislearned,themodelcanbettercapturethedetails
The authors acknowledge funding from UNIQUE, CI-
ofthetargetdistributions, choosingalowvarianceinthe
FAR,NSERC,Intel,RecursionPharmaceuticalsandSam-
vicinityofthepeakstoavoid‘blurring’themthroughthe
sung. Theresearchwasenabledinpartbycomputational
noiseaddedinthelaststepofsampling.
resources provided by the Digital Research Alliance of
Theabilitytomodeldistributionsaccuratelyinfewersteps Canada(https://alliancecan.ca),Mila(https:
isimportantforcomputationalefficiency. Futureworkcan //mila.quebec),andNVIDIA.
consider further extensions that improve performance in
coarsetimediscretizations,suchasnon-Gaussiantransition
Impactstatement
densities, whose utility in diffusion models trained from
datahasbeendemonstrated(Xiaoetal.,2022). Thisworkstudiesamortizedvariationalinferenceovercon-
tinuous variables, a problem of independent interest in
6.Conclusion Bayesian machine learning but also widely applicable in
thesciences. Weenvisionourworkasabuildingblockfor
We have presented a study of diffusion-structured sam- futureresearchinthisfield,settingnecessarycomparative
plers for amortized inference over continuous variables. standards amongst different methodologies and enabling
Our results suggest promising techniques for improving fairbenchmarking. Weultimatelyhopethatouralgorithms
the mode coverage and efficiency of these models. Fu- willbeusedresponsiblyandhelptoadvancescientificun-
tureworkonapplicationscanconsiderinferenceofhigh- derstandingoftheworld.
dimensionalparametersofdynamicalsystemsandinverse
problems. In probabilistic machine learning, extensions
References
ofthisworkshouldstudyintegrationofouramortizedse-
quentialsamplersasvariationalposteriorsinanexpectation- Adam, A., Coogan, A., Malkin, N., Legin, R., Perreault-
maximization loopfor traininglatent variable models, as Levasseur, L., Hezaveh, Y., and Bengio, Y. Posterior
wasrecentlydonefordiscretecompositionallatentsbyHu samplesofsourcegalaxiesinstronggravitationallenses
et al. (2023), and for sampling Bayesian posteriors over withscore-basedpriors.arXivpreprintarXiv:2211.03812,
high-dimensionalmodelparameters. Themostimportantdi- 2022.
rectionoftheoreticalworkisunderstandingthecontinuous-
timelimit(𝑇 →∞)ofallthealgorithmswehavestudied. Agrawal, A. and Domke, J. Amortized variational infer-
enceforsimplehierarchicalmodels. NeuralInformation
ProcessingSystems(NeurIPS),2021.
Acknowledgments
WethankCheng-HaoLiuforassistancewithmethodsfrom Albergo, M. S., Kanwar, G., and Shanahan, P. E. Flow-
priorwork,aswellasJuliusBerner,V´ıctorElvira,Lorenz basedgenerativemodelsforMarkovchainMonteCarlo
Richter, Alexander Tong, and Siddarth Venkatraman for inlatticefieldtheory. PhysicalReviewD,100(3):034515,
2019.
9
ZgolOndiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
Atanackovic,L.,Tong,A.,Wang,B.,Lee,L.J.,Bengio,Y., Gao, C., Isaacson, J., and Krause, C. i-flow: High-
andHartford,J. DynGFN:Towardsbayesianinference dimensionalintegrationandsamplingwithnormalizing
of gene regulatory networks with GFlowNets. Neural flows. MachineLearning: ScienceandTechnology,1(4):
InformationProcessingSystems(NeurIPS),2023. 045023,2020.
Bandeira,A.S.,Maillard,A.,Nickl,R.,andWang,S. On Grathwohl,W.,Chen,R.T.,Bettencourt,J.,Sutskever,I.,
freeenergybarriersinGaussianpriorsandfailureofcold andDuvenaud,D. FFJORD:Free-formcontinuousdy-
startMCMCforhigh-dimensionalunimodaldistributions. namicsforscalablereversiblegenerativemodels. Interna-
Philosophicaltransactions.SeriesA,Mathematical,phys- tionalConferenceonLearningRepresentations(ICLR),
ical,andengineeringsciences,381,2022. 2019.
Bengio,E.,Jain,M.,Korablyov,M.,Precup,D.,andBen-
Grenander,U.andMiller,M.I. Representationsofknowl-
gio,Y. Flownetworkbasedgenerativemodelsfornon-
edgeincomplexsystems. JournaloftheRoyalStatisti-
iterativediversecandidategeneration.NeuralInformation
calSociety: SeriesB(Methodological),56(4):549–581,
ProcessingSystems(NeurIPS),2021.
1994.
Bengio,Y.,Lahlou,S.,Deleu,T.,Hu,E.J.,Tiwari,M.,and
Halton, J. H. Sequential Monte Carlo. In Mathematical
Bengio,E. GFlowNetfoundations. JournalofMachine
ProceedingsoftheCambridgePhilosophicalSociety,vol-
LearningResearch,24(210):1–55,2023.
ume58,pp.57–78.CambridgeUniversityPress,1962.
Berner, J., Richter, L., and Ullrich, K. An optimal con-
trolperspectiveondiffusion-basedgenerativemodeling. Harrison,J.,Willes,J.,andSnoek,J. VariationalBayesian
arXivpreprintarXiv:2211.01364,2022. lastlayers. InternationalConferenceonLearningRepre-
sentations(ICLR),2024.
Buchner, J. Nested sampling methods. arXiv preprint
arXiv:2101.09675,2021. Herna´ndez-Lobato,J.M.andAdams,R. Probabilisticback-
propagationforscalablelearningofBayesianneuralnet-
Burda, Y., Grosse, R. B., and Salakhutdinov, R. Impor-
works. InternationalConferenceonMachineLearning
tanceweightedautoencoders. InternationalConference
(ICML),2015.
onLearningRepresentations(ICLR),2016.
Ho,J.,Jain,A.,andAbbeel,P. Denoisingdiffusionproba-
Chopin, N. A sequential particle filter method for static
bilisticmodels. NeuralInformationProcessingSystems
models. Biometrika,89(3):539–552,2002.
(NeurIPS),2020.
Cornish, R., Caterini, A., Deligiannidis, G., and Doucet,
A. Relaxing bijectivity constraints with continuously Hoffman,M.,Sountsov,P.,Dillon,J.V.,Langmore,I.,Tran,
indexednormalisingflows. InternationalConferenceon D., and Vasudevan, S. NeuTra-lizing bad geometry in
MachineLearning(ICML),2020. HamiltonianMonteCarlousingneuraltransport. arXiv
preprintarXiv:1903.03704,2019.
DeBortoli,V. Convergenceofdenoisingdiffusionmodels
underthemanifoldhypothesis. TransactionsonMachine Hoffman,M.D.,Blei,D.M.,Wang,C.,andPaisley,J.W.
LearningResearch(TMLR),2022. Stochastic variational inference. Journal of Machine
LearningResearch(JMLR),14:1303–1347,2013.
DelMoral,P.,Doucet,A.,andJasra,A. SequentialMonte
Carlosamplers. JournaloftheRoyalStatisticalSociety Hoffman,M.D.,Gelman,A.,etal.TheNo-U-Turnsampler:
SeriesB:StatisticalMethodology,68(3):411–436,2006. adaptively setting path lengths in Hamiltonian Monte
Carlo. JournalofMachineLearningResearch(JMLR),
Deleu, T., Go´is, A., Emezue, C., Rankawat, M., Lacoste-
15(1):1593–1623,2014.
Julien,S.,Bauer,S.,andBengio,Y. Bayesianstructure
learningwithgenerativeflownetworks. Uncertaintyin
Holdijk, L., Du, Y., Hooft, F., Jaini, P., Ensing, B., and
ArtificialIntelligence(UAI),2022.
Welling,M. Stochasticoptimalcontrolforcollectivevari-
Dinh,L.,Sohl-Dickstein,J.,andBengio,S. Densityesti- ablefreesamplingofmoleculartransitionpaths. Neural
mation using Real NVP. International Conference on InformationProcessingSystems(NeurIPS),2023.
LearningRepresentations(ICLR),2017.
Hu, E. J., Malkin, N., Jain, M., Everett, K., Graikos, A.,
Duane,S.,Kennedy,A.,Pendleton,B.J.,andRoweth,D. and Bengio, Y. GFlowNet-EM for learning composi-
HybridMonteCarlo. PhysicsLettersB,195(2):216–222, tionallatentvariablemodels. InternationalConference
1987. onMachineLearning(ICML),2023.
10Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
Hu,E.J.,Jain,M.,Elmoznino,E.,Kaddar,Y.,Lajoie,G., Malkin, N., Lahlou, S., Deleu, T., Ji, X., Hu, E., Everett,
Bengio,Y.,andMalkin,N. Amortizingintractableinfer- K., Zhang, D., and Bengio, Y. GFlowNets and varia-
enceinlargelanguagemodels. InternationalConference tionalinference. InternationalConferenceonLearning
onLearningRepresentations(ICLR),2024. Representations(ICLR),2023.
Izmailov,P.,Vikram,S.,Hoffman,M.D.,andWilson,A.G. Ma´te´,B.andFleuret,F. Learninginterpolationsbetween
WhatareBayesianneuralnetworkposteriorsreallylike? Boltzmanndensities. TransactionsonMachineLearning
InternationalConferenceonMachineLearning(ICML), Research(TMLR),2023.
2021.
Nichol,A.andDhariwal,P. Improveddenoisingdiffusion
Jain,M.,Bengio,E.,Hernandez-Garcia,A.,Rector-Brooks, probabili1sticmodels. InternationalConferenceonMa-
J., Dossou, B.F., Ekbote, C.A., Fu, J., Zhang, T., Kil- chineLearning(ICML),2021.
gour,M.,Zhang,D.,etal. Biologicalsequencedesign
with gflownets. International Conference on Machine Nicoli, K. A., Nakajima, S., Strodthoff, N., Samek, W.,
Learning(ICML),2022. Mu¨ller,K.-R.,andKessel,P. Asymptoticallyunbiased
estimationofphysicalobservableswithneuralsamplers.
Jang,H.,Kim,M.,andAhn,S. Learningenergydecompo-
PhysicalReviewE,101(2):023304,2020.
sitionsforpartialinferenceofGFlowNets. International
ConferenceonLearningRepresentations(ICLR),2024. Noe´,F.,Olsson,S.,Ko¨hler,J.,andWu,H.Boltzmanngener-
ators:Samplingequilibriumstatesofmany-bodysystems
Jing,B.,Corso,G.,Chang,J.,Barzilay,R.,andJaakkola,T.
withdeeplearning. Science,365(6457):eaaw1147,2019.
Torsionaldiffusionformolecularconformergeneration.
NeuralInformationProcessingSystems(NeurIPS),2022. Øksendal,B. StochasticDifferentialEquations: AnIntro-
ductionwithApplications. Springer,2003.
Kim, M., Ko, J., Zhang, D., Pan, L., Yun, T., Kim, W.,
Park, J., and Bengio, Y. Learning to scale logits for
Pan, L., Malkin, N., Zhang, D., and Bengio, Y. Better
temperature-conditional GFlowNets. arXiv preprint
trainingofGFlowNetswithlocalcreditandincomplete
arXiv:2310.02823,2023.
trajectories. InternationalConferenceonMachineLearn-
ing(ICML),2023.
Kim,M.,Yun,T.,Bengio,E.,Zhang,D.,Bengio,Y.,Ahn,
S.,andPark,J. LocalsearchGFlowNets. International
Pillai, N. S., Stuart, A. M., and Thie´ry, A. H. Optimal
ConferenceonLearningRepresentations(ICLR),2024.
scalinganddiffusionlimitsforthelangevinalgorithmin
Kingma,D.P.andWelling,M. Auto-encodingvariational highdimensions. TheAnnalsofAppliedProbability,22
Bayes. InternationalConferenceonLearningRepresen- (6),December2012.
tations(ICLR),2014.
Ranganath, R., Gerrish, S., and Blei, D. Black box vari-
Lahlou, S., Deleu, T., Lemos, P., Zhang, D., Volokhova, ational inference. Artificial Intelligence and Statistics
A., Herna´ndez-Garcıa, A., Ezzine, L. N., Bengio, Y., (AISTATS),2014.
andMalkin,N. Atheoryofcontinuousgenerativeflow
Rector-Brooks,J.,Madan,K.,Jain,M.,Korablyov,M.,Liu,
networks.InternationalConferenceonMachineLearning
C.-H.,Chandar,S.,Malkin,N.,andBengio,Y.Thompson
(ICML),2023.
samplingforimprovedexplorationinGFlowNets. arXiv
Lemos,P.,Malkin,N.,Handley,W.,Bengio,Y.,Hezaveh,Y., preprintarXiv:2306.17693,2023.
andPerreault-Levasseur,L. Improvinggradient-guided
nestedsamplingforposteriorinference. arXivpreprint Rezende,D.andMohamed,S. Variationalinferencewith
arXiv:2312.03911,2023. normalizingflows. InternationalConferenceonMachine
Learning(ICML),2015.
Madan,K.,Rector-Brooks,J.,Korablyov,M.,Bengio,E.,
Jain,M.,Nica,A.,Bosc,T.,Bengio,Y.,andMalkin,N. Rezende, D. J., Mohamed, S., and Wierstra, D. Stochas-
LearningGFlowNetsfrompartialepisodesforimproved ticbackpropagationandapproximateinferenceindeep
convergenceandstability. InternationalConferenceon generativemodels. InternationalConferenceonMachine
MachineLearning(ICML),2022. Learning(ICML),2014.
Malkin, N., Jain, M., Bengio, E., Sun, C., and Bengio, Richter, L., Boustati, A., Nu¨sken, N., Ruiz, F. J. R., and
Y. Trajectory balance: Improved credit assignment O¨merDenizAkyildiz. VarGrad: Alow-variancegradient
in gflownets. Neural Information Processing Systems estimatorforvariationalinference. NeuralInformation
(NeurIPS),2022. ProcessingSystems(NeurIPS),2020.
11Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
Richter,L.,Berner,J.,andLiu,G.-H. Improvedsampling Tzen,B.andRaginsky,M. Theoreticalguaranteesforsam-
vialearneddiffusions. arXivpreprintarXiv:2307.01198, plingandinferenceingenerativemodelswithlatentdif-
2023. fusions. ConferenceonLearningTheory(CoLT),2019b.
Roberts,G.O.andRosenthal,J.S. Optimalscalingofdis- van Krieken, E., Thanapalasingam, T., Tomczak, J., van
creteapproximationstolangevindiffusions. Journalof Harmelen,F.,andtenTeije,A. A-NeSI:Ascalableap-
theRoyalStatisticalSociety:SeriesB(StatisticalMethod- proximatemethodforprobabilisticneurosymbolicinfer-
ology),60(1):255–268,1998. ence. NeuralInformationProcessingSystems(NeurIPS),
2023.
Roberts,G.O.andTweedie,R.L. Exponentialconvergence
ofLangevindistributionsandtheirdiscreteapproxima- Vargas,F.,Grathwohl,W.,andDoucet,A. Denoisingdif-
tions. Bernoulli,pp.341–363,1996. fusionsamplers. InternationalConferenceonLearning
Representations(ICLR),2023.
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
Ommer,B. High-resolutionimagesynthesiswithlatent Vincent,P. Aconnectionbetweenscorematchingandde-
diffusionmodels. ConferenceonComputerVisionand noisingautoencoders. Neuralcomputation,23(7):1661–
PatternRecognition(CVPR),2021. 1674,2011.
Sa¨rkka¨, S. and Solin, A. Applied stochastic differential Wu, H., Ko¨hler, J., and Noe´, F. Stochastic normalizing
equations. CambridgeUniversityPress,2019. flows. NeuralInformationProcessingSystems(NeurIPS),
2020.
Shen,M.W.,Bengio,E.,Hajiramezanali,E.,Loukas,A.,
Cho,K.,andBiancalani,T. Towardsunderstandingand Xiao, Z., Kreis, K., and Vahdat, A. Tackling the genera-
improvingGFlowNettraining. InternationalConference tive learning trilemma with denoising diffusion GANs.
onMachineLearning(ICML),2023. InternationalConferenceonLeraningRepresentations
(ICLR),2022.
Skilling,J. NestedsamplingforgeneralBayesiancompu-
tation. Bayesian Analysis, 1(4):833 – 859, 2006. doi: Zhang,D.,Malkin,N.,Liu,Z.,Volokhova,A.,Courville,
10.1214/06-BA127. URL https://doi.org/10. A., and Bengio, Y. Generative flow networks for dis-
1214/06-BA127. creteprobabilisticmodeling. InternationalConference
onMachineLearning(ICML),2022.
Sohl-Dickstein,J.,Weiss,E.A.,Maheswaranathan,N.,and
Ganguli,S. Deepunsupervisedlearningusingnonequi- Zhang, D., Chen, R. T. Q., Malkin, N., and Bengio, Y.
libriumthermodynamics. InternationalConferenceon UnifyinggenerativemodelswithGFlowNetsandbeyond.
MachineLearning(ICML),2015. arXivpreprintarXiv:2209.02606,2023a.
Song,Y.,Durkan,C.,Murray,I.,andErmon,S. Maximum Zhang,D.,Rainone,C.,Peschl,M.,andBondesan,R. Ro-
likelihoodtrainingofscore-baseddiffusionmodels. Neu- bustschedulingwithGFlowNets. InternationalConfer-
ralInformationProcessingSystems(NeurIPS),2021a. enceonLearningRepresentations(ICLR),2023b.
Song,Y.,Sohl-Dickstein,J.,Kingma,D.P.,Kumar,A.,Er- Zhang,D.,Chen,R.T.Q.,Liu,C.-H.,Courville,A.,and
mon,S.,andPoole,B. Score-basedgenerativemodeling Bengio,Y. Diffusiongenerativeflowsamplers: Improv-
throughstochasticdifferentialequations. International inglearningsignalsthroughpartialtrajectoryoptimiza-
ConferenceonLearningRepresentations(ICLR),2021b. tion. InternationalConferenceonLearningRepresenta-
tions(ICLR),2024.
Tiapkin,D.,Morozov,N.,Naumov,A.,andVetrov,D. Gen-
erativeflownetworksasentropy-regularizedRL. arXiv Zhang,Q.andChen,Y. Diffusionnormalizingflow. Neural
preprintarXiv:2310.12934,2023. InformationProcessingSystems(NeurIPS),2021.
Tripp, A., Daxberger, E., and Herna´ndez-Lobato, J. M. Zhang,Q.andChen,Y. Pathintegralsampler: astochastic
Sample-efficientoptimizationinthelatentspaceofdeep controlapproachforsampling. InternationalConference
generativemodelsviaweightedretraining. NeuralInfor- onLearningRepresentations(ICLR),2022.
mationProcessingSystems(NeurIPS),2020.
Zhu, Y., Wu, J., Hu, C., Yan, J., Hsieh, C.-Y., Hou, T.,
Tzen, B.andRaginsky, M. Neuralstochasticdifferential andWu, J. Sample-efficientmulti-objectivemolecular
equations: DeeplatentGaussianmodelsinthediffusion optimizationwithGFlowNets. NeuralInformationPro-
limit. arXivpreprintarXiv:1905.09883,2019a. cessingSystems(NeurIPS),2023.
12Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
Zimmermann, H., Lindsten, F., van de Meent, J.-W., and
Naesseth, C. A. A variational perspective on genera-
tiveflownetworks. TransactionsonMachineLearning
Research(TMLR),2023.
13Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
A.Codeandhyperparameters
Code is available https://github.com/GFNOrg/gfn-diffusion and will continue to be maintained and ex-
tended.
Below are commands to reproduce some of the results on Manywell and VAE with PIS and GFlowNet models as an
example,showingthehyperparameters:
PIS:
--mode_fwd pis --lr_policy 1e-3
PIS+Langevin:
--mode_fwd pis --lr_policy 1e-3 --langevin
GFlowNetTB:
python train.py
--t_scale 1. --energy many_well --pis_architectures --zero_init --clipping
--mode_fwd tb --lr_policy 1e-3 --lr_flow 1e-1
GFlowNetTB+Expl.:
python train.py
--t_scale 1. --energy many_well --pis_architectures --zero_init --clipping
--mode_fwd tb --lr_policy 1e-3 --lr_flow 1e-1
--exploratory --exploration_wd --exploration_factor 0.2
GFlowNetVarGrad+Expl.:
python train.py
--t_scale 1. --energy many_well --pis_architectures --zero_init --clipping
--mode_fwd tb-avg --lr_policy 1e-3 --lr_flow 1e-1
--exploratory --exploration_wd --exploration_factor 0.2
GFlowNetFL-SubTB:
python train.py
--t_scale 1. --energy many_well --pis_architectures --zero_init --clipping
--mode_fwd subtb --lr_policy 1e-3 --lr_flow 1e-2
--partial_energy --conditional_flow_model
GFlowNetTB+Expl. +LS:
python train.py
--t_scale 1. --energy many_well --pis_architectures --zero_init --clipping
--mode_fwd tb --lr_policy 1e-3 --lr_back 1e-3 --lr_flow 1e-1
--exploratory --exploration_wd --exploration_factor 0.1
--both_ways --local_search
--buffer_size 600000 --prioritized rank --rank_weight 0.01
--ld_step 0.1 --ld_schedule --target_acceptance_rate 0.574
GFlowNetTB+Expl. +LP:
python train.py
--t_scale 1. --energy many_well --pis_architectures --zero_init --clipping
--mode_fwd tb --lr_policy 1e-3 --lr_flow 1e-1
--exploratory --exploration_wd --exploration_factor 0.2
--langevin --epochs 10000
14Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
GFlowNetTB+Expl. +LS(VAE):
python train.py
--energy vae --pis_architectures --zero_init --clipping
--mode_fwd cond-tb-avg --mode_bwd cond-tb-avg --repeats 5
--lr_policy 1e-3 --lr_flow 1e-1 --lr_back 1e-3
--exploratory --exploration_wd --exploration_factor 0.1
--both_ways --local_search
--max_iter_ls 500 --burn_in 200
--buffer_size 90000 --prioritized rank --rank_weight 0.01
--ld_step 0.001 --ld_schedule --target_acceptance_rate 0.574
GFlowNetTB+Expl. +LP+LS(VAE):
python train.py
--energy vae --pis_architectures --zero_init --clipping
--mode_fwd cond-tb-avg --mode_bwd cond-tb-avg --repeats 5
--lr_policy 1e-3 --lr_flow 1e-1
--lgv_clip 1e2 --gfn_clip 1e4 --epochs 10000
--exploratory --exploration_wd --exploration_factor 0.1
--both_ways --local_search
--lr_back 1e-3 --max_iter_ls 500 --burn_in 200
--buffer_size 90000 --prioritized rank --rank_weight 0.01
--langevin
--ld_step 0.001 --ld_schedule --target_acceptance_rate 0.574
15Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
B.Targetdensities
Gaussian Mixture Model with 25 modes (25GMM). The model, termed as 25GMM, consists of a two-dimensional
Gaussianmixturemodelwith25distinctmodes. Eachmodeexhibitsanidenticalvarianceof0.3. Thecentersofthesemodes
arestrategicallypositionedonagridformedbytheCartesianproduct{−10,−5,0,5,10}×{−10,−5,0,5,10},effectively
distributingthemacrossthecoordinatespace.
Funnel(Hoffmanetal.,2019). Thefunnelrepresentsaclassicalbenchmarkinsamplingtechniques,characterizedbya
ten-dimensionaldistributiondefinedasfollows: Thefirstdimension,𝑥 ,followsanormaldistributionwithmean0and
0
variance 9, denotedas 𝑥 ∼ N(0,9). Conditionalon𝑥 , theremainingdimensions, 𝑥 , aredistributedaccordingtoa
0 0 1:9
multivariatenormaldistributionwithmeanvector0andacovariancematrixexp(𝑥 )I,whereIistheidentitymatrix. Thisis
0
succinctlyrepresentedas𝑥 | 𝑥 ∼N (0,exp(𝑥 )I).
1:9 0 0
Manywell(Noe´ etal.,2019). Themanywellischaracterizedbya32-dimensionaldistribution,whichisconstructedasthe
productof16identicaltwo-dimensionaldoublewelldistributions. Eachofthesetwo-dimensionalcomponentsisdefinedby
apotentialfunction,𝜇(𝑥 ,𝑥 ),expressedas𝜇(𝑥 ,𝑥 ) =exp(cid:0) −𝑥4+6𝑥2+0.5𝑥 −0.5𝑥2(cid:1) .
1 2 1 2 1 1 1 2
VAE(Kingma&Welling,2014). Thistaskinvolvessamplingfroma20-dimensionallatentposterior 𝑝(𝑧|𝑥) ∝ 𝑝(𝑧)𝑝(𝑥|𝑧),
where 𝑝(𝑧)isafixedpriorand 𝑝(𝑥|𝑧)isapretrainedVAEdecoder,usingaconditionalsampler𝑞(𝑧|𝑥)dependentoninput
data(image)𝑥.
C.Experimentdetails
Samplingenergies. Inthissection,wedetailthehyperparametersusedforourexperiments. Animportantparameteris
thediffusioncoefficientoftheforwardpolicy,whichisdenotedby𝜎andalsousedinthedefinitionofthefixedbackward
process. Thebasediffusionrate𝜎issetto5for25GMMand1forFunnelandManywell,consistentwithpastwork.
Forallourexperiments,weusedalearningrateof10−3. Additionally,weusedahigherlearningrateforlearningtheflow
parameterization,whichissetas10−1whenusingtheTBlossand10−2withtheSubTBloss. Thesesettingswerefoundto
beconsistentlystable(unlikethosewithhigherlearningrates)andconvergewithintheallottednumberofsteps(unlike
thosewithlowerlearningrates).
For the SubTB loss, we experimented with the settings of 10× lower learning rates for both flow and policy models
communicatedbytheauthorsofZhangetal.(2024),butfoundtheresultstobeinferiorbothusingtheirpublishedcode(and
otherunstatedhyperparameterscommunicatedbytheauthors)andusingourreimplementation.
Formodelswithexploration,weuseanexplorationfactorof0.2(thatis,noisewithavarianceof0.2isaddedtothepolicy
whensamplingtrajectoriesfortraining),whichdecayslinearlyoverthefirsthalfoftraining,consistentwithLahlouetal.
(2023).
Wetrainallourmodelsfor25,000iterationsexceptthoseusingLangevindynamics,whicharetrainedfor10,000iterations.
Thisresultsinapproximatelyequalcomputationtimeowingtotheoverheadfromcomputationofthescoreateachsampling
step.
WeusethesameneuralnetworkarchitecturefortheGFlowNetasoneofourbaselines(Zhang&Chen,2022). Similarto
Zhang&Chen(2022),wealsouseaninitializationschemewithlast-layerweightssetto0atthestartoftraining. Since
theSubTBrequirestheflowfunctiontobeconditionedonthecurrentstatex𝑡 andtime𝑡,wefollowZhangetal.(2024)
andparametrizetheflowmodelwiththesamearchitectureastheLangevinscalingmodelNN inZhang&Chen(2022).
2
Additionally,weperformclippingontheoutputofthenetworkaswellasthescoreobtainedfromtheenergyfunction,
typicallysettingtheclippingparameterofLangevinscalingmodelto102andpolicynetworkto104,similarlytoVargas
etal.(2023):
(cid:16) (cid:17)
𝑓 𝜃(𝑘,𝑥)=clip NN 1(𝑘,𝑥;𝜃)+NN 2(𝑘;𝜃) ⊙clip(cid:0) ∇ln𝜋(𝑥),−102,102(cid:1),−104,104 . (16)
Allmodelsweretrainedwithabatchsizeof300.
VAEexperiment. IntheVAEexperiment, weusedastandardVAEmodelpretrainedfor100epochsontheMNIST
dataset. Theencoder𝑞(𝑧|𝑥) containsaninputlinearlayer(784neurons)followedbyhiddenlinearlayer(400neurons),
16Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
ReLUactivationfunction,andtwolinearheads(20neuronseach)whoseoutputswerereparametrizedtobemeansand
scalesofmultivariateNormaldistribution. Thedecoderconsistsof20-dimensionalinput,onehiddenlayer(400neurons),
followedbytheReLUactivation,and784-dimensionaloutput. Theoutputisprocessedbythesigmoidfunctiontobescaled
properlyinto [0,1].
Thegoalistosampleconditionallyon𝑥thelatent𝑧fromtheunnormalizeddensity 𝑝(𝑧,𝑥) = 𝑝(𝑧)𝑝(𝑥 | 𝑧)(where 𝑝(𝑧)is
thepriorand 𝑝(𝑥|𝑧)isthelikelihoodcomputedfromthedecoder),whichisproportionaltotheposterior 𝑝(𝑧 | 𝑥). Wereuse
themodelarchitecturesfromtheunconditionalsamplingexperiments,butalsoprovide𝑥asaninputtothefirstlayerofthe
modelsexpressingthepolicydrift(aswellastheflow,forFL-SubTB)andaddonehiddenlayertoprocesshigh-dimensional
conditions. FormodelstrainedwithTB,log𝑍 𝜃 alsobecomesaMLPtaking𝑥asinput.
TheVarGradandLStechniquesrequireadaptationsintheconditionalsetting. ForLS,buffers(D andD )muststore
buffer LS
theassociatedconditions𝑥togetherwiththesamples𝑧andthecorrespondingunnormalizeddensity𝑅(𝑧;𝑥),i.e.,atupleof
(𝑥,𝑧,𝑅(𝑧;𝑥)). ForVarGrad,becausethepartitionfunctiondependsontheconditioninginformation𝑥,itisnecessaryto
computevarianceovermanytrajectoriessharingthesamecondition. Wechoosetosample10trajectoriesforeachcondition
occurringinaminibatchandcomputetheVarGradlossforeachsuchsetof10trajectories.
TheVAEmodelwastrainedontheentireMNISTtrainingsetandneverupdatedonthetestpartofMNIST.Inorderto
evaluatesamplers(withrespecttothevariationallowerbound)onauniquesetofexamples,wechosethefirst100elements
ofMNISTtestdata. AllofthesamplersweretrainedhavingaccesstotheMNISTtrainingdataandthefrozenVAEdecoder.
Forafaircomparison,samplersutilizingtheLPweretrainedfor10,000,whereastheremainingfor25,000iterations. In
eachiteration,abatchof300examplesfromMNISTwasgivenasconditions.
(a)Conditioningdata(MNISTtestset) (b)VarGrad+Expl.+LPsamplesdecoded (c)VAEreconstruction
FigureC.1.Oursampler(VarGrad+Expl.+LP)isconditionedbyasubsetofnever-seendatacomingfromthegroundtruthdistribution
(left).TheconditionalsampleswerethendecodedbythethefixedVAE(middle).Forthecomparison,weshowthereconstructionofthe
realdatabyVAE(right).Weobservedthatthedecodedsamplesarevisuallyverysimilartothereconstructionsmakingthesetwopictures
almostindistinguishable.Both,decodedsamplesandreconstruction,aremoreblurrythanthegroundtruthdata,whichiscausedbya
limitedcapacityoftheVAE’slatentspace.
17Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
D.Localsearch-guidedGFlowNet
Prioritizedsamplingscheme. Wecanuseuniformorprioritizedsamplingtodrawsamplesfromthebufferfortraining.
Wefoundprioritizedsamplingtoworkslightlybetterinourexperiments(seeablationstudyin§D.2),althoughthechoice
shouldbeinvestigatedmorethoroughlyinfuturework.
Weuserank-basedprioritization(Trippetal.,2020),whichfollowsaprobabilisticapproachdefinedas:
𝑝(x;D ) ∝ (cid:0)𝑘|D |+rank (x)(cid:1)−1, (17)
buffer buffer D buffer
whererank (x)representstherelativerankofasample𝑥basedonarankingfunction𝑅(x)(inourcase,theunnormalized
D
buffer
targetdensityatsamplex). Theparameter𝑘 isahyperparameterforprioritization,wherealowervalueof𝑘 assignsahigher
probabilitytosampleswithhigherranks,therebyintroducingamoregreedyselectionapproach. Weset𝑘 =0.01forevery
task. Giventhatthesamplingisproportionaltothesizeof D ,weimposeaconstraintonthemaximumsizeofthe
buffer
buffer: |D | =600,000withfirst-infirstout(FIFO)datastructureforeverytask,exceptweuse|D | =90,000for
buffer buffer
VAEtask. Seethealgorithmbelowforadetailedpseudocode.
Algorithm1GFlowNetTrainingwithLocalsearch
1: Initializepolicyparameters𝜃for𝑃 𝐹,andemptybuffersD buffer,D
LS
2: for𝑖=1,2,...,𝐼do
3: if𝑖%2==0then
4: Sample𝑀trajectories{𝜏 1,...,𝜏 𝑀}∼𝑃 𝐹(·|𝜖-greedy)
5: UpdateD ←D ∪{𝑥|𝜏→𝑥}
buffer buffer
6: Minimize𝐿(𝜏;𝜃)using{𝜏 1,...,𝜏 𝑀}toupdate𝑃 𝐹
7: else
8: if𝑖%100==0then
9: Sample{𝑥 1,...,𝑥 𝑀}∼D
buffer
10: D LS←LocalSearch({𝑥 1,...,𝑥 𝑀};D LS)
11: endif
12: Sample{𝑥′,...,𝑥′ }∼ 𝑝 (··· ;D )
1 𝑀 buffer LS
13: Sample{𝜏 1′,...,𝜏 𝑀′ }∼𝑃 𝐵(···|𝑥′)
14: Minimize𝐿(𝜏′;𝜃)using{𝜏 1′,...,𝜏 𝑀′ }toupdate𝑃 𝐹
15: endif
16: endfor
Weusethenumberoftotaliterations𝐼 =25,000foreverytaskasdefault. NoteaslocalsearchisperformedtoupdateD
LS
occasionallythatper100iterations,thenumberoflocalsearchupdatesisdone25,000/100=250.
18Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
D.1.Localsearchalgorithm
This section describes a detailed algorithm for local search, which provides an updated buffer D , which contains
LS
low-energysamples.
Dynamicadjustmentofstepsize𝜂. ToenhancelocalsearchusingparallelMALA,wedynamicallyselecttheLangevin
stepsize(𝜂),whichgovernstheMHacceptancerate. Ourobjectiveistoattainanaverageacceptancerateof0.574,whichis
theoreticallyoptimalforhigh-dimensionalMALA’sefficiency(Pillaietal.,2012). Whiletheusercancustomizethetarget
acceptancerate,theadaptiveapproacheliminatestheneedformanualtuning.
Computationalcostoflocalsearch. Thecomputationalcostoflocalsearchisnotsignificant. Localsearchforiteration
of𝐾 =200requires6.04seconds(averagedwithfivetrialsinManywell),whereweonlyoccasionally(every100iterations)
updateD withMALA.ThespeedisevaluatedusingthecomputationalresourcesoftheIntelXeonScalableGold6338
LS
CPU(2.00GHz)andtheNVIDIARTX4090GPU.
Algorithm2Localsearch(ParallelMALA)
input Initialstates {𝑥(0),...,𝑥(0) }, currentbuffer D , totalsteps 𝐾, burninsteps 𝐾 , initialstepsize𝜂 , amplifyingfactor
1 𝑀 LS burn-in 0
𝑓 ,dampingfactor 𝑓 ,unnormalizedtargetdensity𝑅
increase decrease
output UpdatedbufferD
LS
Initializeacceptancecounter𝑎=0
Set𝜂←𝜂
0
for𝑘 =1:𝐾do
Initializestepacceptancecount𝑎 𝑘 =0
for𝑚=1:𝑀do
Sample𝜎∼N(0,𝐼)
Propose𝑥 𝑚∗ ←𝑥 𝑚(𝑘−1) +𝜂∇log𝑅(𝑥 𝑚(𝑘−1) )+√︁ 2𝜂𝜎
Computeacceptanceratio𝑟
←min(cid:32)
1,
𝑅(𝑥 𝑚∗ )exp(cid:16) − 41 𝜂∥𝑥 𝑚(𝑘−1)−𝑥 𝑚∗ −𝜂∇log𝑅(𝑥 𝑚∗ )∥2(cid:17) (cid:33)
𝑅(𝑥 𝑚(𝑘−1))exp(cid:16) − 41 𝜂∥𝑥 𝑚∗ −𝑥 𝑚(𝑘−1)−𝜂∇log𝑅(𝑥 𝑚(𝑘−1))∥2(cid:17)
Withprobability𝑟,accepttheproposal:𝑥 𝑚(𝑘) ←𝑥 𝑚∗ andincrement𝑎 𝑘 ←𝑎 𝑘+1
if𝑘 >𝐾 then
burn-in
Updatebuffer:D LS←D LS∪{𝑥 𝑚∗ }
endif
endfor
Computestepacceptancerate𝛼
𝑘
=𝑎 𝑘/𝑀
if𝛼 𝑘 >𝛼 targetthen
𝜂←𝜂× 𝑓
increase
elseif𝛼 𝑘 <𝛼 targetthen
𝜂←𝜂× 𝑓
decrease
endif
endfor
Weadoptdefaultparameters: 𝑓 =1.1, 𝑓 =0.9,𝜂 =0.01,𝐾 =200,𝐾 =100,and𝛼 =0.574forthree
increase decrease 0 burn-in target
unconditionaltasks. ForconditionaltasksofVAE,wegivemoreiterationsoflocalsearch: 𝐾 =500,𝐾 =200.
burn-in
Itisnoteworthythatbyadjustingtheinversetemperature 𝛽into 𝑅𝛽 duringthecomputationoftheMetropolis-Hastings
acceptanceratio𝑟,wecanfacilitateagreedierlocalsearchstrategyaimedatexploringsampleswithlowerenergy(i.e.,
higherdensity 𝑝 ). Thisapproachprovesadvantageousfornavigatinghigh-dimensionalandsteeplandscapes,whichare
target
typicallychallengingforlocatinglow-energysamples. Forunconditionaltasks,weset 𝛽=1.
InthecontextoftheVAEtask(Table2),weutilizetwoGFlowNetlossfunctions: TBandVarGrad. Forlocalsearchwithin
TB,weset 𝛽 = 1,whileforVarGrad,weemploy 𝛽 = 5. AsillustratedinTable2,employingalocalsearchwith 𝛽 = 1
failstoenhancetheperformanceoftheTBmethod. Conversely,alocalsearchwith 𝛽=5resultsinimprovementsatthe
log𝑍ˆRW metricovertheVarGrad+Expl. +LP,eventhoughtheperformanceofVarGrad+Expl. +LPsurpassesthatof
TBsubstantially. Thisunderscorestheimportanceofselectinganappropriate 𝛽value,whichiscriticalforoptimizingthe
exploration-exploitationbalancedependingonthetargetobjectives.
19Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
D.2.Ablationstudyforlocalsearch-guidedGFlowNets
Increasingcapacityofbuffer. Thecapacityofthereplaybufferinfluencesthedurationforwhichitretainspastexperi-
ences,enablingittoreplaytheseexperiencestothepolicy.Thismechanismhelpsinpreventingmodecollapseduringtraining.
TableD.1demonstratesthatenhancingthebuffer’scapacityleadstoimprovedsamplingquality. Furthermore,Figure1illus-
tratesthatincreasingthebuffer’scapacity—therebyencouragingthemodeltorecallpastlow-energyexperiences—enhances
itsmode-seekingcapability.
TableD.1.ComparisonofthesamplingqualityofeachsamplertrainedwithvaryingreplaybuffercapacitiesinManywell.Fiveindependent
runshavebeenconducted,withboththemeanandstandarddeviationreported.
BufferCapacity↓Metric→ Δlog𝑍 Δlog𝑍RW W2
2
30,000 4.41±0.10 2.73±0.15 6.17±0.02
60,000 4.06±0.05 2.38±0.38 6.14±0.04
600,000 4.57±2.13 0.19±0.29 5.66±0.05
(a)Capacity30,000 (b)Capacity60,000 (c)Capacity600,000
FigureD.1.Illustrationofeachsamplertrainedwithvaryingcapacitiesofreplaybuffers,depicting2,000samples.Asthecapacityofthe
bufferincreases,thenumberofmodescapturedbythesampleralsoincreases.
Benefitofprioritization. Rank-prioritizedsamplinggivesfasterconvergencecomparedwithnoprioritization(uniform
sampling),asshowninFig.D.2a.
Dynamic adjustment of 𝜂 vs. fixed 𝜂 = 0.01. As shown in Fig. D.2b, dynamic adjustment to target acceptance rate
𝛼 =0.574givesbetterperformancesthanfixedLangevinstepsizeof𝜂showcasingtheeffectivenessofthedynamic
target
adjustment.
20Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
165.00
165 No scheudling
164.75 Scheduling
160
164.50
155
164.25 150
164.00
145
163.75
140
163.50
135
No prioritization 163.25
130 Prioritization
163.00
0 5000 10000 15000 20000 25000 0 5000 10000 15000 20000 25000
Plot Plot
(a)Benefitofprioritizationinreplaybuffersampling.. (b)Benefitofscheduling𝜂dynamically.
FigureD.2.Ablationstudyforprioritizedreplaybufferandstepsize𝜂schedulingoflocalsearch.Meanandstandarddeviationareplotted
basedonfiveindependentruns.
21
WRZgol WRZgol