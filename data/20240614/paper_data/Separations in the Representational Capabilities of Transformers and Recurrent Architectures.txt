Separations in the Representational Capabilities of
Transformers and Recurrent Architectures
SatwikBhattamishra1† MichaelHahn2 PhilBlunsom1,3 VarunKanade1†
1UniversityofOxford 2SaarlandUniversity 3Cohere
Abstract
Transformerarchitectureshavebeenwidelyadoptedinfoundationmodels. Dueto
theirhighinferencecosts,thereisrenewedinterestinexploringthepotentialof
efficientrecurrentarchitectures(RNNs).Inthispaper,weanalyzethedifferencesin
therepresentationalcapabilitiesofTransformersandRNNsacrossseveraltasksof
practicalrelevance,includingindexlookup,nearestneighbor,recognizingbounded
Dycklanguages,andstringequality. Forthetasksconsidered,ourresultsshow
separationsbasedonthesizeofthemodelrequiredfordifferentarchitectures. For
example,weshowthataone-layerTransformeroflogarithmicwidthcanperform
indexlookup,whereasanRNNrequiresahiddenstateoflinearsize. Conversely,
whileconstant-sizeRNNscanrecognizeboundedDycklanguages,weshowthat
one-layerTransformersrequirealinearsizeforthistask. Furthermore,weshow
thattwo-layerTransformersoflogarithmicsizecanperformdecisiontaskssuchas
stringequalityordisjointness,whereasbothone-layerTransformersandrecurrent
modelsrequirelinearsizeforthesetasks. Wealsoshowthatalog-sizetwo-layer
Transformercanimplementthenearestneighboralgorithminitsforwardpass;on
theotherhandrecurrentmodelsrequirelinearsize. Ourconstructionsarebased
ontheexistenceofN nearlyorthogonalvectorsinO(logN)dimensionalspace
andourlowerboundsarebasedonreductionsfromcommunicationcomplexity
problems.Wesupplementourtheoreticalresultswithexperimentsthathighlightthe
differencesintheperformanceofthesearchitecturesonpractical-sizesequences.
1 Introduction
Transformers[59]arethego-toarchitectureforbuildingLLMs[11],butrecently,therehasbeen
significantinterestinrevivingrecurrentarchitecturestobuildLLMsforpracticaltasks[21,42,45]to
circumventthequadraticcomplexityofinferenceforTransformers. WhileTransformersprocessall
tokensinparallel,maintainingN vectors,andareinsomesensestateless,recurrentmodelsprimarily
storeinformationinafixed-sizehiddenstatethattheycanupdateduringthecourseofthecomputation.
Thisraisesafundamentalquestionthatweexplore: Aretheretasksthatarecomputationallyeasier
foronearchitecturetorepresentbutsignificantlymoredifficultfortheotherone?
EversinceTransformerssupplantedLSTMs,therehasbeensignificantinterestinunderstandingthe
differencesinthecomputationalcapabilitiesofthemodelsboththeoreticallyandempirically. Onthe
onehand,evenasTransformershavebeenwidelyadoptedforbuildingLLMs,severalstudiesfound
thattheystruggledatmodelingvariousformallanguages,particularlythosethatrequiredmodular
counting or state-tracking [5, 16, 38]. At the same time, despite substantial effort, it has proven
hard to match the performance of Transformer-based LLMs at scale with recurrent architectures
[15,45,21];inparticular,ithasbeenobservedthatLLMsbasedonrecurrentmodelsstruggleon
associativerecallorextraction-relatedtasks[15,2]. Morerecently,Aroraetal.[2]andBhattamishra
†CorrespondingAuthors:satwik.bmishra,varun.kanade@cs.ox.ac.uk
Preprint.
4202
nuJ
31
]GL.sc[
1v74390.6042:viXraetal.[8]demonstratedthatTransformersarebetterthanattention-freemodelsatsynthetictasksbased
onassociativerecallandmoregeneralformssuchasimplementingnearestneighbors. Basedon
theseobservations,itisnaturaltowonderifsuchtasksaretheoreticallyeasierforTransformersto
representincomparisontorecurrentmodels.
OurContributions. WeshowdifferencesintherepresentationalcapabilitiesofTransformersand
recurrentmodelsacrossseveralnaturaltasksofreal-worldrelevance. Inthispaper,weshowstrong
separationresults: whenataskiseasyweshowthatitcanbeexpressedbyonearchitecturewith
sizepoly-logarithmicintheinputlengthN;ontheotherhandweshowtheothertypeofarchitecture
requiresthesizetobelinearinN. Wedescribethetasksstudiedandourkeyresultsbelow.
(i)IndexLookup. Givenasequenceofsymbolss ,...,s followedbyapositionp∈[N],amodel
1 N
hastooutputthesymbolinthep-thpositions (Figure1a). Ourfirstresultshowsthatone-layer
p
Transformerswithsizepoly-logarithmicinN canexpressthistask(Theorem1)whereasanyrecurrent
modelmusthavewidthΩ(N)toperformthistask(Theorem3).
(ii)BoundedDycks. Dycklanguageswithboundeddepthrequireamodeltorecognizewhethera
stringofparenthesesiswell-balanced(Figure1b). Theseformallanguageshavereceivedagreatdeal
ofattentioninthestudyofneuralsequencemodelsbecausetheycapturehierarchicaldependencies
thatoccurinnaturallanguages[17,25,23,6]. Theyarealsocentraltoformallanguagetheoryasall
context-freelanguagescanbeexpressedintermsofDycklanguages[14]. Incontrasttotheresults
forindexlookup,weshowthatone-layerTransformersmusthaveasizethatgrowslinearlyininput
lengthtorepresentthistask(Theorem5),whereaspriorworksfoundthatconstant-sizerecurrent
modelscanexpressthistask[25,6].
(iii)StringEquality. ThistaskisformalizedbytheEqualityfunctionEQ(x,y)=I[x=y]where
x,yaretwostringsoflengthN;itmodelsthenaturaltaskofmatchingtwodocuments. Weshowthat
log-sizedtwo-layerTransformerscanrepresentthefunctionEQ(Theorem6),whereasbothone-layer
Transformers(Theorem12)andrecurrentmodels(Theorem11)musthaveasizethatgrowslinearly
withthesequencelength. TheseresultsextendtoabroaderclassofBooleanfunctions.
(iv)NearestNeighborandAssociativeRecall. Inthistask,amodelisprovidedwithasequenceof
inputsandlabelsx ,y ,...,x ,y followedbyaqueryinputx . Themodelhastodetermine
1 1 k−1 k−1 k
the label y of the query input by applying the nearest neighbor algorithm in its forward pass
k
(Figure 1c). This task subsumes various associative recall tasks considered in earlier works (cf.
AppendixG)andwasintroducedtounderstandin-contextlearning. Bhattamishraetal.[8]found
empiricaldifferencesintheperformanceofTransformersandattention-freearchitectures,though
theoreticalunderstandingislacking. Weshowthatatwo-layerTransformerwithsizelogarithmicin
theinputlengthcanperformthistask(Theorem7),whereasrecurrentmodelsrequireasizelinearin
thelength(Theorem8).
WealsoempiricallyinvestigatedtheperformanceofTransformersandstandardrecurrentmodels
[26],includingrecentlyproposedstate-spacemodels[22,21]onthesetasks. Theobservedbehavior
isalongthelinesindicatedbyourtheoreticalresults.
OurTechniques. ForconstructingTransformersthatperformthetasks,oneofthekeyrequirements
is the ability to precisely attend to specific positions. In order to do this with low-dimensional
embeddings,ourconstructionsmakeuseofnearlyorthogonalvectorsobtainedusingtheJohnson-
Lindenstrausslemma[29];furthermorethesecanbegeneratedefficientlyinlogarithmicspacewhich
allowsthesizeofthemodelstobepoly-logarithmicintheinputlength(cf. AppendixB.3).
For our lower bounds, we appeal to results from communication complexity. We show how to
obtain protocols with communication complexity bounded by the size of the models. Together
withestablished(andnew)communicationcomplexitylowerbounds,weobtainlowerboundson
themodelsize. Forourlowerboundforone-layerTransformers,wederivealowerboundonthe
communicationcomplexityforboundedDycklanguages(Lemma1).
1.1 RelatedWork
ExpressivityofSequenceModels. Theexpressivepowerofneuralsequencemodelshasbeenan
activeareaofresearch,targetingbothRNNsandTransformers[e.g.23,52,40,55,37]. Withinfinite
precisionorunboundedresources,Transformersareuniversalfunctionapproximators[67]andTuring
complete[47,7]. Inboundedprecisionsettings,theyrelatetoBooleancircuits[39,24]andlogical
2formalisms[12]. Importantly,suchstudiesusuallyconsiderasymptoticexpressivityofasinglemodel
inthelimitofunboundedlylonginputs. Ourstudyprovidesamorefine-grainedandrealisticpicture
byaccountingforthesizeofthenetwork,anditsscalingwiththeinput. Theclosesttoourworkis
Sanfordetal.[51],whofirstusedcommunicationcomplexitytoprovelowerboundsforTransformers
andRNNsforabstracttaskssuchasasparseaveragingtaskandpair/tripledetectiontasks. Ourwork
extendsthatlineofworktoshowseparationsonnaturaltasksofpracticalandreal-worldrelevance.
FormallanguagesandAlgorithmictasks. Astrandofworkhassoughttounderstandsequence
modelsviaempiricalanalysisonformallanguagesandalgorithmictasks[5,57,68].Numerousworks
haveexaminedtheabilityofrecurrentmodels[58,54,66,6,25,62]andTransformers[17,65,62]to
modelDycklanguages. Morerecently,significanteffort[19,60,4]hasbeendevotedtoinvestigating
howTransformerscanlearntoimplementlearningalgorithmsintheirforwardpasstounderstand
thein-contextlearningphenomenon. Bhattamishraetal.[8]empiricallyobservedthatattention-free
architecturesstruggletoimplementthenearestneighboralgorithmincomparisontoTransformers.
Ourresulttakesasteptowardunderstandingthisphenomenonbyshowingthatnearestneighborscan
beimplementedbysmall-sizedTransformersbutnotbyrecurrentarchitectures.
2 Definitions
Weconsidertwotypesofmodels: Transformers[59]andrecurrentmodels. Weuserecurrentmodels
torefermoregenerallytononlinearRNNssuchasLSTMs[26],state-spacemodels[22,21]aswell
asvariantsoflinearTransformer[31,56]whichcanprocessinputsinarecurrentmanner[31].
For some finite alphabet Σ, a sequence model is given a sequence in ΣN and depending on the
taskoutputseither{0,1}orasequenceofoutputs. Eachs ∈ Σfromasequences ···s ∈ ΣN
i 1 N
is mapped to a vector in Rd via an embedding map ϕ : Σ → Rd. For Transformers, the input
embeddingfunctionfurthertakesthepositioniasinput,alongwiths . EachlayerforTransformers
i
andrecurrentmodelsmapsinputsfromRN×d →RN×d. AmodelM hasfixedprecisionpifallthe
parametersofthemodelaswellasthevaluesintheintermediatevectorscanbeimplementedwith
p-bitprecisionnumbers(cf. AppendixB.2).
Transformers. Each layer of a Transformer has an attention block followed by an MLP
block. The attention block takes as input X ∈ RN×d and applies the operation Att(X) =
softmax(XW⊤W X⊤)XW⊤whereW ,W ,W ∈Rm×d.Forsimplicity,wewilluseQ(x )
Q K V Q K V i
(and likewise K(x ) and V(x )) to denote W x . The width of the Transformer is max(m,d),
i i Q i
wherem×distheshapeoftheprojectionmatricesW ,W . ForanymatrixA ∈ RN×M,the
Q K
exp(A )
softmaxoperatorisappliedrow-wiseasfollowssoftmax(A) = i,j .Multi-head
i,j (cid:80)M
exp(A )
k=1 i,k
attention with H heads is defined as M-Att (X) = [Att (X),...,Att (X)]W where each
H 1 H O
Att (X)hasitsownsetofparameters. ThematrixW ∈RmH×dprojectstheconcatenatedvector
i O
toavectorofdimensiond. ForaninputX ∈ RN×d,theoutputofalayerofTransformerwillbe
ψ(M-Att(X))∈RN×dwhereψ :Rd →Rdisafeedforwardnetwork. WeuseTFL todenote
m,p,H
theclassofallTransformersoperatingoverp-bitprecisionnumberswithwidthm,H heads,andat
mostLlayers.
Recurrent Models. A general recurrent neural network (RNN) takes as input the sequence
x ,...,x where x ∈ Rd and produces an output sequence y ,...,y ; in this paper we will
1 N i 1 N
mostlyconsiderthecasewheny ∈{0,1}. AnRNNwithahiddenstateofsizemoverp-bitnumbers
i
can be defined as follows. The hidden state is an mp-bit memory h ∈ {0,1}mp and for some
i
h ∈{0,1}mp,theRNNcomputesh =g (x ,h )andy =f (h )fort=1,...,N,andg
0 t (t) t t−1 t (t) t (t)
andf arearbitraryfunctions. Sincethetransitionfunctionisallowedtobearbitrary,thisdefinition
(t)
capturesthegeneralfamilyofrecurrentorstate-spacearchitecturesincludingLSTMs,state-space
models,andlinearTransformers,eachofwhichdiffersinthewaythetransitionfunctionisdefined.
Forarecurrentmodel,wesaythattherepresentationsizeofahiddenstateismp,anditswidthism,
i.e. thehiddenstateconsistsofmunitseachofwhichusesp-bits. Throughoutthepaper,wewilluse
recurrentmodelsorRNNstorefertothegeneralfamilyofrecurrentarchitecturesmentionedabove.
Bytherepresentationsizeofamodel,wewillrefertothetotalnumberofbitsrequiredtorepresent
themodelincludingalltheparametersandembeddings. ForTransformers,thisisΘ(mdpH). Fora
recurrentmodel,therepresentationsizeisatleastmp.
3j = argmax( xx iTx xk)
i i k
Nearest Neighbor
x 1 y 1 x 2 y 2 x j y j x k ?
Prediction
(a)IndexLookupTask (b)Dyck-2withdepth≤2 (c)NearestNeighborTask
Figure1: Illustrationofafewkeytasksconsideredinourwork.
3 IndexLookupTask
TaskDescription. WeintroduceasimpletaskcalledIndexLookup(IdxL).Inthistaskamodel
receivesasequenceoftokenss ,...,s (possiblywithrepetitions)followedbyanindexpwhere
1 N
p∈[N]andthegoalofthemodelistooutputthetokens . Herethesymbolss belongtoafinite
p i
alphabetΣ.
Thissimpleandnaturaltaskhelpsillustratethekeytoolsandbuildingblocksweusetoobtainother
moregeneralresultsinthispaper:Ontheonehand,weshowhowaone-layerTransformerwithwidth
O(logN)canperformthistask;ontheotherhand,weusecommunicationcomplexityargumentsto
showthatanytypeofrecurrentorstate-spacemodelperformingthistaskneedsahiddenstatewith
representationsizeΩ(N).
Ourfirstresultshowsthat,foranylengthN ∈N,thereisa1-layerTransformerwithwidthO(logN)
that performs the Index Lookup task for all input sequences of length at most N. Naïvely one
couldconstructsuchatransformerbyusingone-hotencodingsaspositionalembeddings,asthey
are orthogonal and would allow to attend to the desired index. However, this would require the
embeddingdimension,andhencethewidthofthemodel,tobeΩ(N). Keytoourconstructionsofa
widthO(logN)Transformer,bothhereandinothersections,isaresult(Lemma2intheAppendix)
whichstatesthat,ink =O(logN/γ2)dimensionalspacewecanfindN nearlyorthogonalvectors.
WeusesuchvectorsinourconstructionoftheTransformerstoallowittoattendalmostexactlyover
desiredpositions.
Theorem1. ForallN ∈N,thereisa1-layerTransformerwithwidthm=O(logN)andprecision
p=O(logN)whichperformstheindexlookuptaskforallinputsequencesoflengthsuptoN.
ProofSketch. Foraninputsequence(s ,...,s ,p),theTransformerusestheembeddingsofthe
1 N
positiontokenpandthepositionalembeddingsofthefirstN inputstoattendovers ,sothatthe
p
feedforwardnetworkcanextractthelabelfromtheoutputoftheattentionblock. Ourkeyideaisto
usetheN almostorthogonalvectorsprovidedbyLemma2,bothaspositionalembeddingsandalso
asawaytoembedthenumbers{1,...,N},anyofwhichcanbeusedastheindexp. Formally,let
T(1),...,T(N)beN vectorsofdimensionk =O(logN)suchthat⟨T(i),T(j)⟩≤1/4fori̸=j
and⟨T(i),T(j)⟩≥3/4fori=j.
FormaldetailsoftheconstructionareinAppendixC.2;weprovideasketch. Theembeddingofeach
inputtokenisofsizelog|Σ|+2kwherelog|Σ|+kentriesareusedforthetokenembeddingsand
thelastkentriesareusedforthepositionalembeddings. Thequeryandkeymatricesaredesignedso
thatthequeryvectorQ(p)=η[T(p)],thekeyvectorsK(x )=[T(i)]andK(p)=[0 ]. Thevalue
i k
vectorssimplycontainthetokenembeddingsV(x ) = [ρ(s )],whereρ : Σ → {0,1}|Σ| issome
i i
binaryencodingofΣ. Withsuchqueryandkeyvectors,thedotproductsinattention,⟨Q(p),K(x )⟩,
i
are≥ 3η/4ifi = p,and≤ η/4otherwise. Thedotproductofthequeryvectorwithitselfwillbe
⟨Q(p),K(p)⟩=0. Wechooseη >0toscalethedotproductstoamplifythedifferencebetweenthe
highandlowdotproducts. Thuswehave,
exp(Q(p)⊤K(x )) exp(3η)
softmax(Q(p)⊤K(X))= exp(Q(p)⊤K(x p))+(cid:80) j̸=pexp p(Q(p)⊤K(x j)) ≥ exp(3 4η)+N4 exp(η 4)
whichisatleast 3 forsomeη = Θ(logN);thetotalattentionweightovertheremainingtokens
4
is at most < 1. Recall that the value vectors contain the binary encodings of the input symbols,
4
V(x ) = [ρ(s )]. The attention-weighted average value vector aligns closely with ρ(s ) as 3/4
i i p
4weightisonit. ItisthenstraightforwardtodesignaReLU-FFNthatcanactasathresholdfunction
toretrieveρ(s )fromit,whichleadstothedesiredoutput.
p
WeuseresultsfromcommunicationcomplexitytoshowthatRNNsrequireessentiallyΩ(N)widthto
solvethisandseveralotherproblems. Incommunicationcomplexity,therearetwoparties,typically
calledAliceandBob,eachofwhomhaspartofthe(discrete)input,andtheirgoalistocomputea
functionofthecombinedinputusingaslittlecommunicationaspossible. Ourfirstkeyinsighthereis
thattheoutputofanRNNcanbecomputedwithaboundedamountofcommunicationwhenAlice
hasaprefixoftheinputandBobhastheremainingpart. Theresultingprotocolwillbeone-way
(AlicetoBob)andone-round. Wefirststateamoregeneralresultandthendiscussimplicationsfor
theIdxLproblem.
Theorem2. IfanRNNwithahiddenstateofrepresentationsizempcomputesanyfunction
f : ΣN → {0,1},thenforanyK < N,ifAlicehasaccesstos ,...,s andBobhasaccessto
1 K
s ,...,s ,thenthereexistsaone-waycommunicationprotocolwithmpbitsfromAlicetoBob,
K+1 N
bywhichBobcancomputetheoutputofthefunctionf(s ...s ).
1 N
Proof. AssumethatbothAliceandBobhaveaccesstotheRNNthatrepresentsthefunctionf. Alice
canprovidethesequences ,...,s totherecurrentmodelanditerativelyupdatethehiddenstate
1 K
fromtheinitialstateh toobtaintheKthhiddenstateh . Alicecanthensendthehiddenstateto
0 K
Bobwhichrequiresmpbits. Bobcanthenupdatethehiddenstateusings ,...,s toobtainh ,
K+1 N N
fromwhichhecanobtaintheoutputoftheRNN.NotethatAliceandBobcancomputetheoutput
usingone-waycommunicationofmpbits.
ProblemssimilartoIndexLookuparewell-studiedincommunicationcomplexity;specifically,the
INDEXproblem(SeeAppendixB.1)hasaone-waycommunicationcomplexityofΩ(N)(Fact3).
WededucealowerboundonthesizeofthehiddenstateofRNNsbyshowingthatanyRNNthat
canrepresenttheIndexLookuptaskcanalsocomputetheINDEXproblemandsincethatimpliesthe
existenceofaone-waycommunicationprotocolwithmpbits(Theorem2),itfollowsthatthewidth
ofthehiddenstatemmustbeΩ(N/p)(cf. AppendixC.1).
Theorem 3. Any recurrent model with a hidden state of width m using p-bits of precision that
computestheIndexLookuptaskforallsequencesoflengthN musthavem≥N/p.
Discussion. TheaboveresultstheoreticallyformalizeintuitivedifferencesbetweenthewayTrans-
formersandrecurrentmodelsprocesssequences. SinceTransformershaveaccesstoN inputvectors
duringtheircomputation,asmall-sizedattentionblockcanattendoverthedesiredinputvectorto
makethecorrectprediction. Ontheotherhand,anyrecurrentmodel—evenwitharbitrarypositional
embeddings—muststorealltherequiredinformationinitshiddenstate,whichlowerboundsthesize
ofsuchmodelstocomputetherightoutput. Theseintuitionsaremaderigorousbyshowing(i)how
soft-attentioncandolookupusingthealmostorthogonalvectors,and(ii)small-widthRNNsyield
ashortone-waycommunicationprotocol. Theselowerboundsalsoapplytocausalformsoflinear
attentionarchitectureswheresoftmaxisremovedandattentionweightsbecomedotproducts[31].
Atfirstglance,itmightseemunfairtocompareTransformersandRNNswiththesamenumberof
parameters: TransformershaveaccesstoN inputvectors,whereasRNNshaveafixed-sizehidden
state. Butnotethat,inpractice,empiricalresearchonlanguagemodelstypicallycomparesmodels
of the same size e.g., a 7B Transformer vs a 7B state-space model. Hence, it is natural to ask if
Transformersofaparticularsizecanexpresssomethingthatrecurrentmodelscannot.
4 LowerBoundsforRNNsand1-layerTransformers
Whereas Section 3 established a case where one-layer Transformers can be more powerful than
RNNs,wenextexhibitanexampleoftheoppositephenomenon. Here,thekeytoolwillagainbea
communicationcomplexityargument,butthistimeitappliestoone-layerTransformers: Weestablish
acommunicationprotocolbywhichAliceandBobcancomputetheoutputofaone-layerTransformer
byexchanginganumberofbitsthatisboundedbytherepresentationsizeoftheTransformerandan
overheadthatislogarithmicintheinputlength. Thekeypropertyhereisthatthisprotocolworks
notjustwhenAliceandBobhaveaccesstoaprefixandsuffixofastring,butinsteadworksforan
arbitrarypartitioningoftheinputstring(proofisinAppendixD):
5Theorem4. Consideraone-layerTransformerf ∈ TF1 operatingoverinputsoflengthN.
m,p,H
Consider any disjoint subsets S ∪S = {1,...,N}, S ∩S = ∅. Assume Alice has access
A B A B
to s for i ∈ S , and Bob has access to s for i ∈ S . Then Alice and Bob can communicate
i A i B
3m(p+logN)H bitstocomputetheoutputf(s ...s ).
1 N
TheproofideaisthatAliceandBobfirstcomputetheirpartsofthenumeratoranddenominatorof
thesoftmaxandexchangethesetocomputetheoverallattentionoutput. Anaiveimplementationof
thisidearunsintotheissuethattheexponentiationoflogitsmayexceedtheboundsofp-bitprecision;
wecircumventthisbyfirstcommunicatingthemaximumlogitandsubtractingitfromeachlogit,
keepingtheexponentialsboundedwithoutalteringtheresultingattentionweights. Theorem4isa
slightlymoregeneralandformalversionofaresultinSanfordetal.[51,Theorem. 7].
4.1 SeparationonBoundedHierarchicalLanguages
Wenowusethecommunicationprotocolforone-layerTransformerstoestablishaseparationbetween
theseandRNNsonboundedDycklanguages. Dycklanguagesareofcentralimportanceinformal
languagetheory,asanycontext-freelanguagecanbeexpressedintermsofDycklanguages[14].
Duetotheboundednessofhumanmemory[41],naturallanguagetendstohavemoreboundedlevels
ofembedding[30,10]. Thishasmotivatedthestudyofbounded-depthDycklanguagesasplausible
simplemodelsofthehierarchicalstructureunderlyinglanguage[25,65,6,62].
Task. Formally,Dyck-(n,k)(cf. AppendixE)isthelanguageofwell-matchedstringsoverntypes
ofparenthesispairs( ,) ,( ,) ,...,( ,) ,whereanyprefixhasatmostkopeningparenthesesnot
1 1 2 2 n n
yetclosed. Forinstance,thestring‘([])()’hasamaximumdepth2correspondingtotheprefix‘([’.
Dyck-(n,k)canberecognizedwithaccesstoaboundedstackthatneverholdsmorethankelements.
Infact,eachDyck-(n,k)isaregularlanguageandisacceptedbyafiniteautomaton.
WeshowthatthereisalinearcommunicationcomplexitylowerboundforDyck-(n,k),alreadyat
n=k =2. However,unlikethecommunicationboundweusedinTheorem3,AliceandBobnow
haveaccessnottotwohalvesoftheinput;rather,AliceandBobhaveaccesstotheevenandodd
positionsinthestring,respectively. Intuitively,insuchasituation,Aliceneedstoknowalmostallof
thebitsavailabletoBobinordertodecidewhetheragivenstringiswell-bracketed–andviceversa.
Moreformally,theyneedtoexchangeatleastN −1bitstodecidemembershipinDyck-(2,2):
Lemma1. SupposeAliceandBobhavethesymbolsintheoddandevenindicesofastrings∈ΣN
respectively. Toeachcomputewhethers∈Dyck-(2,2),theymustexchangeatleastN −1bits.
TheproofofLemma1isinAppendixEandisbasedonfoolingsetswhichisastandardtechnique
toprovelowerboundsoncommunicationcomplexity. CombiningLemmas4and1entailsalower
boundonthewidthofaTransformerforcomputingDyck-(2,2):
Theorem5. Consideraone-layerTransformerf ∈ TF1 decidingmembershipinDyck-(2,2).
m,p,H
ThenmH ≥ N−1 .
3(p+logN)
Thisresultestablishesasecondseparationbetweenone-layerTransformersandRNNs,butnowin
theotherdirection: Bounded-depthDycklanguagesareregular,andpreviousworkhasshownthat
constantwidthRNNscanrecognizethemwithstandardactivationfunctions,Sigmoid[25]andReLU
[6]. Wefurthernotethattwo-layerTransformerscanmodelbounded-depthDycklanguages[65].
4.2 LowerBoundsonBooleanFunctions
Therearesomenotabledifferencesbetweenthetypesofcommunicationcomplexitylowerboundsfor
one-layerTransformers(Theorem4)andforRNNs(Theorem2). RNNscomputingf yieldaone-way
protocolforcontiguouspartitionsoftheinput;thusshowingaonewaycommunicationlowerbound
forsuchpartitionsissufficienttoobtainlowerboundsonthesizeofRNNs. Transformerscomputing
f yieldamulti-wayprotocolthatworkforarbitrarypartitionsoftheinput;thusshowingalower
boundforanypartitionissufficienttoestablishlowerboundsonthesizeoftheTransformer. For
Dyck-(2,2),contiguouspartitionsarenotahardcase,andinfact,communicating≤2openbrackets
fromthefirsthalfissufficient. ThisiswhythelowerboundofLemma1doesnotapplytoRNNs.
LowerBounds. Despitethedifferencesdiscussedabove,thereareseveralBooleanfunctions,for
whichwecanestablishthatwhenwidthsarebounded,neitherone-layerTransformersnorRNNs
6cancomputethem. TheEqualityfunctionEQ : {0,1}N → {0,1}isaBooleanfunctiondefined
asEQ(x)=I[(x ,...,x )=(x ,...,x )]. ArelatedproblemisDisjointness: giventwo
1 N/2 N/2+1 N
vectorsx,y ∈ {0,1}N/2,thefunctionDISJ(x,y) = maxx iy
i
= I[xTy > 0]. Boththefunctions
EqualityandDisjointnessareknowntohavecommunicationcomplexityΩ(N)(seeAppendixB.1)
andTheorems4and2implythatbothone-layerTransformersandRNNsmusthavewidthΩ(N)
torepresentthem. Inthenextsection,weshowthattheselowerboundsdonotapplytotwo-layer
Transformers. Additionally,itisworthnotingthatthefunctionsEQandDISJcanalsobeexpressed
intheformof2-CNFswithO(N)terms. Hence,amoregeneralconsequenceofthelimitationsof
RNNs(andone-layerTransformers)isthatwithwidtho(N),theycannotcomputecertainfunctions
intheclassofuniformAC0.3 ItisinterestingsincetheclassofuniformAC0isconsideredoneofthe
simplestclassesofBooleancircuitsandeventheexpressivepowerofhard-attentionTransformers
hasbeenshowntobewithinthisclass[24]. InAppendixF.1,weprovideanalternateproofofthe
lowerboundforRNNscomputingEqualitybasedontheirrelationtoDFAs.
5 RepresentationalCapabilitiesof2-layerTransformers
InSection4,weshowedthatsingle-layerTransformersandrecurrentmodelsmusthavesizelinear
intheinputlengthtoexpressnaturalBooleanfunctionssuchasEQ. Inthissection,weshowthat
two-layertransformersovercometheselimitationsbyefficientlyexpressingsuchBooleanfunctions
andmoregeneralformsofassociativerecalltasks,suchassimulatingthenearestneighboralgorithm.
5.1 RepresentingBooleanFunctions
Westartbyshowingthattwo-layerTransformersofpoly-logarithmicsizecanexpresstheEquality
function(proofisinAppendixF.2). TheinputdomainneednotnecessarilybetheBooleanvectors
{0,1}N;rather,theconstructionworksforsequencesoveranyfinitealphabetΣ.
Theorem6. ForanyN ∈ N, thereexistsa2-layerTransformerf ∈ TF2 wherewidthm =
m,p,2
O(logN)andprecisionp=O(logN)suchthatf(x)=EQ(x)forallx∈{0,1}N.
The construction is based on tools developed in Section 3. The broad idea is as follows. In the
firstlayer,ateachpositioni > N/2,anattentionheadattendstopositioni−N/2andcopiesthe
inputx . Afeedforwardnetworkthencheckswhethertheretrievedvalueisequaltox . The
i−N/2 i
secondlayersimplyusesuniformattentionovertheoutputsofthepreviouslayertocheckifthereis
amismatchatanyposition. Importantly,weshowthattheabovestrategycanbeimplementedwitha
representationsizeO((logN)3).
Generalizingthisresult, wefindthattwo-layerTransformerswithlogarithmicwidthcanexpress
amoregeneralclassofBooleanfunctions: thresholdsofk-sparsefeatures,aclassincludingfunc-
tions such as Equality and Disjointness. Since such functions cannot be expressed by one-layer
Transformersandrecurrentmodelswithwidtho(N),theseresultsimplyaseparationonEquality
andDisjointness: thesefunctionscanbeexpressedbysmall-sizedtwo-layerTransformerswhereas
one-layerTransformersandrecurrentmodelsmustgrowlinearlywithinputlengthtorepresentthem.
5.2 ImplementingtheNearestNeighborsAlgorithm
Thegoalofthenearestneighbortask(NSTNB)istoanalyzewhetherasequencemodelingarchitecture
can implement the well-known nearest neighbor algorithm to make predictions. Our description
followscloselytotheoneusedbyBhattamishraetal.[8]fortheirexperiments.
NearestNeighbors. IntheNSTNBtask,amodelisprovidedwithasequenceofvectorsandlabels
(x ,y ,...,x ,y ,x ) where N/2 < k ≤ N, the input unit vectors x ∈ Rd and labels
1 1 k−1 k−1 k i
y ∈ {0,1}. For each x where k > N/2, the output is the label corresponding to the nearest
i k
neighborin(x ,...,x ),thatis,ifj =argmax x⊤x orj =argmin ∥x −x ∥ ,
1 k−1 i∈[k−1] k i i∈[k−1] k i 2
thentheoutputforx isthelabely . Sinceweareworkingwithunitvectors,maximizingtheinner
k j
productisequivalenttominimizingtheℓ distance. Ifthesecondhalfofthesequencex ,...,x
2 N+1 N
2
isapermutationofthefirsthalfx ,...,x thenthetaskreducestotheMulti-QueryAssociative
1 N
2
Recall(MQAR)task[2](c.f. AppendixG).
3TheclassAC0containspolynomialsizeAND/ORcircuitswithunboundedfan-inandconstantdepth.
7Assumptions. We will make two assumptions about the problem. The first assumption is that
all input vectors are of unit norm, i.e., ∥x∥ = 1 and the second is the existence of a margin
2
between the dot product with the nearest neighbor and the dot product with other input vectors,
i.e. there exists γ ≥ N−c for some universal constant c, such that for any N/2 < k ≤ N, if
j∗ =argmax x⊤x ,thenx⊤x ≥x⊤x +γ foranyi̸=j∗.
i∈[k−1] k i k j∗ k i
Thefollowingisoneofourmainresultswhichstatesthattwo-layerTransformersoflogarithmic
sizecanimplementthenearestneighboralgorithmintheirforwardpassandasacorollarycanalso
performassociativerecalltaskslikeMQAR(ProofsinAppendixG.2).
Theorem 7. For any N ∈ N, there exists a 2-layer Transformer f ∈ TF2 with width
NN m,p,2
m = O(logN) and precision p = O(logN) such that f computes the nearest-neighbor task
NN
allsequencesoflengthatmostN satisfyingtheassumptionsabove.
The broad idea of the construction is to identify the nearest neighbor input x and retrieve the
j∗
positionofthecorrespondinglabely inthefirstlayer. Thesecondlayerthenusesthispositional
j∗
informationtoretrievethedesiredlabel. Thereareafewchallengestoimplementingthisstrategy
whichweaddressinourconstruction. First,notethatforinputvectorsx ,...,x ,naivelyusing
1 k
themwithdot-productattentionwillresultinthequeryinputx havingmaximumdotproductand
k
hencemaximumattentionweightoveritself. Second,thedotproductwithsomelabelvectorsy s
i
couldbehigherthanthedotproductwiththenearestneighborx . Third,thepositionalinformation
j∗
must be retrieved using soft-attention in a way that it can be used in the next layer to obtain the
desiredlabel. Ourintuitive,thoughsomewhatinvolved,constructiondealswiththeseissuestoensure
thatatwo-layerTransformerwithO((logN)3)totalsizeimplementsthenearestneighboralgorithm.
Theorem8. Anyrecurrentmodelwithahiddenstateofwidthmwithp-bitsofprecisionthatcan
performthenearestneighbortaskforallinputsoflengthN musthavem≥N/2p.
ThelowerboundforrecurrentmodelsfollowsviaareductionfromtheDisjointnessproblem.
Discussion. Priorworks[8,2]haveempiricallydemonstratedthatTransformer-basedLLMscan
exhibitmechanismssuchasnearestneighborsandMQAR. Further,onsyntheticsetups,theyhave
observedthatrecurrentmodelsstruggletoperformthesetaskscomparedtoTransformers. Ourresults
takeasteptowardsunderstandingthedifferencesintheperformancebetweenthetwoarchitectures.
6 EmpiricalAnalysis
WhilewefocusonthedifferencesintherepresentationalcapabilitiesofTransformersandrecurrent
models,itisnaturaltoexamineifdifferencesofasimilarnatureariseintheirempiricalperformance.
Onethingtokeepinmindisthatpositiveresultsregardingexpressivenesspresentedinearliersections
donotimplythatmodelscanlearnsuchtasks. Withregardtonegativeresults,theydoimplythat
whenthesequencelengthismuchlargerthanthesizeofthehiddenstateorwidthofthemodel,then
themodelwillbeincapableofrepresentingthetaskandconsequentlyfailtolearnthetask. However,
evenforone-layerrecurrentmodelswithhiddenstatesofsize128with64bitsofprecision,ourlower
boundappliesatlengthsover8k.
Inthissection,weinvestigatetheperformanceofTransformersandrecurrentmodelsontaskssuchas
IndexLookupandrecognizingboundedDycklanguagesonsequencesofsmalllengths(< 1000).
Ourexperimentsaredesignedtoanswerthefollowingquestions: (1)Areone-layerTransformers
betterthanlargerrecurrentmodelsontheIndexLookuptask? (2)Arerecurrentmodelsandtwo-
layerTransformersbetterthanone-layerTransformersatrecognizingtheDyck-(2, 2)language?
Importantly, as our results concern the scaling of the model size with the input length, we are
specificallyinterestedinthebehaviorofdifferentmodelsacrossinputlengths.
WealsoexploretheperformanceofmodelsonstringequalityinAppendixH.2. TaskslikeNSTNB
andMQARhavealreadybeenanalyzedempiricallyinpriorworks[8,2]sowedonotincludethem.
SetupandTrainingdetails. Wetrainthemodelswithcross-entropylossusingtheAdamoptimizer
[32]. Themodelsaretrainedforupto250kstepswhereateachstepwesampleafreshbatchof64
trainingexamples–resultingin≈16millionexamplesover250ksteps. Themodelsareevaluatedon
5000examplesforeachtask. Foreachmodel,wetunethevarioushyperparameters,notablyacross
learningrates∈ {1e-2,5e-3,...,1e-6}tofindthebest-performingmodel. Thedetailsofthedata
generationmethod,hyperparameters,andimplementationdetailscanbefoundinAppendixH.
8Index Lookup: Dyck-(2, 2):
Index Lookup Task Validation Curves at length 200 Validation Curves at length 400
100
LSTM-(3, 256) 100.0 49.8 18.2 9.4 5.8 1.0 TF-(1, 64) 1.0
LSTM-(3, 256) LinTF-(6, 256) 100.0 71.6 20.6 9.4 4.1 80 0.8 Mamba-(6, 256) 0.8 RetNet-(6, 256)
RetNet-(6, 256) 100.0 78.5 27.0 14.2 5.2 60 0.6 DSS-(6, 256) LinTF-(6, 256) 0.6
DSS-(6, 256) 100.0 75.8 39.6 10.7 5.3 40 0.4
0.4 TF-(2, 64)
Mamba-(6, 256) 100.0 99.3 90.2 28.9 12.6 0.2 LSTM-(1, 64)
20 Mamba-(1, 64)
TF-(1, 64) 100.0 100.0 100.0 100.0 99.9 0.2 TF-(1, 256)
0.0
20 50 100 200 400 0 50000 100000 150000 200000 250000 0 50000 100000 150000 200000 250000
Input Length # Iterations # Iterations
Figure2: PerformanceofmodelsontheIndexLookupandboundedDycktask. LabelssuchasTF-(1,
64)denoteTransformerswith1layerand64widths. SeeSection6formoredetails.
IndexLookupTask. Wecomparetheperformanceofone-layerTransformerswithfivedifferent
recurrent models – LSTMs [26], state space models such as DSS [22] and Mamba [21], linear
Transformers[31],anditsvariantRetNet[56]. Weexploretheperformanceacrossvariouslengths
∈{20,50,100,200,400}. Weevaluaterelativelysmall-sizedTransformerswithwidths64against
recurrentmodelswithupto6layersandwidthsorhiddenstatesizeof256. Thesizeofthealphabet
intheexperimentsis|Σ|=64. Figure2(left)depictstheperformanceofallmodelsacrossvarious
lengthsandFigure2(middle)depictsthevalidationcurvesduringtrainingonexamplesoflength
200. Asdepictedbythefigures,whileone-layerTransformerswithwidth64achievenear-perfect
accuracywithinafewthousandsteps,theperformanceofrelativelylargerrecurrentorstate-space
modelsdegradesonlengthsover100andtheyfailtolearnevenwith10×trainingiterations. We
exploretheinfluenceofwidthontheperformanceofMambainAppendixH.1.
Bounded Dycks. For Dyck-2 with depth at most 2, our separation results apply to one-layer
TransformersandnonlinearrecurrentmodelssuchasLSTMsbutnottolinearRNNssuchasstate-
space models and linear Transformers. Hence, we are primarily interested in the difference in
performance between one-layer Transformers and LSTMs. In our experiments, we compare the
performanceofone-layerTransformerswithrelativelysmallerrecurrentmodelssuchasLSTMsand
two-layerTransformers. WealsoincludeMambaforreference. WeconsiderLSTMsandMamba
withhiddenstatesizesof64andsimilarly,two-layerTransformerswithwidth64. Weevaluatea
one-layerTransformerwithawidthof256acrosslengths∈{20,...,400}mostofwhicharesmaller
thanthewidthofthemodel. Weobservethatone-layerTransformersachievenear-perfectaccuracy
uptolengths100butstruggleonhigherlengths. Incontrast,small-sizedrecurrentmodelsaswell
astwo-layerTransformerscanachievenear-perfectaccuracyforlengthsupto400. Figure2(right)
depictsthevalidationcurveofthemodelsonexamplesoflength400.
7 DiscussionandFinalRemarks
Basedonpriortheoreticalresults,itisknownthat,whilerecurrentmodelscanexpressanyregular
language[34,33],Transformerswithlogarithmicprecisioncanonlyexpresslanguagesintheclass
ofuniformconstantdepththresholdcircuits(TC0)[40]. Theseresultsindicatethat—understandard
conjectures—Transformersareunabletorepresentcertainstate-trackingtasksthatrecurrentmodels
canrepresent. Withsuchresults,itmightappearthatTransformersarelessexpressivethanrecurrent
models–potentiallyatoddswiththepersistentpracticalsuccessofTransformer-basedLLMs. Our
findings, however, show that when the model size is constrained relative to the sequence length,
a variety of tasks relevant to practice can be represented by small-sized Transformers but not by
recurrentmodels. Ourresultssuggestthattheattentionmechanismdoesleadtoexpressivenessthat
cannotbereplicatedbyrecurrentarchitecturesevenwitharbitrarytransitionfunctions.
Limitations. Agenerallimitationofthisline ofworkisthatpositiveexpressivityresultsdonot
implythattheproblemsunderconsiderationarelearnable. Additionally,whilelowerboundsforan
architectureimplydifficultyinlearning,whenusingdoubleprecisiontheseresultsonlyapplytovery
longsequencesinpractice. Ourresults(andprobablytechniques)donotimplyanylimitationson
two-layerTransformers;thisisleftasanopenquestion. Wenotethatcommunicationcomplexity-
basedtechniquesakintoTheorem4cannotexistfortwo-layerTransformers(cf. AppendixF.4).
Hence,webelievethatothertoolswillbeneededtoprovelowerboundsfortwo-layerTransformers.
9
erutcetihcrA )%(
ycaruccA
)%( ycaruccA
noitadilaV
)%( ycaruccA
noitadilaVReferences
[1] D.Achlioptas. Database-friendlyrandomprojections: Johnson-lindenstrausswithbinarycoins.
JournalofcomputerandSystemSciences,66(4):671–687,2003.
[2] S.Arora,S.Eyuboglu,A.Timalsina,I.Johnson,M.Poli,J.Zou,A.Rudra,andC.Re. Zoology:
Measuringandimprovingrecallinefficientlanguagemodels. InTheTwelfthInternational
ConferenceonLearningRepresentations,2024. URLhttps://openreview.net/forum?
id=LY3ukUANko.
[3] J.Ba,G.E.Hinton,V.Mnih,J.Z.Leibo,andC.Ionescu. Usingfastweightstoattendtothe
recentpast. Advancesinneuralinformationprocessingsystems,29,2016.
[4] Y.Bai,F.Chen,H.Wang,C.Xiong,andS.Mei. Transformersasstatisticians: Provablein-
contextlearningwithin-contextalgorithmselection. Advancesinneuralinformationprocessing
systems,36,2024.
[5] S.Bhattamishra,K.Ahuja,andN.Goyal. OntheAbilityandLimitationsofTransformersto
RecognizeFormalLanguages. InProceedingsofthe2020ConferenceonEmpiricalMethodsin
NaturalLanguageProcessing(EMNLP),pages7096–7116,Online,Nov.2020.Association
for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.576. URL https://
aclanthology.org/2020.emnlp-main.576.
[6] S.Bhattamishra,K.Ahuja,andN.Goyal. Onthepracticalabilityofrecurrentneuralnetworks
torecognizehierarchicallanguages. InProceedingsofthe28thInternationalConferenceon
ComputationalLinguistics,pages1481–1494,Barcelona,Spain(Online),Dec.2020.Interna-
tionalCommitteeonComputationalLinguistics. doi: 10.18653/v1/2020.coling-main.129. URL
https://aclanthology.org/2020.coling-main.129.
[7] S.Bhattamishra,A.Patel,andN.Goyal. Onthecomputationalpoweroftransformersandits
implicationsinsequencemodeling. InR.FernándezandT.Linzen,editors,Proceedingsof
the24thConferenceonComputationalNaturalLanguageLearning,pages455–475,Online,
Nov.2020.AssociationforComputationalLinguistics. doi: 10.18653/v1/2020.conll-1.37. URL
https://aclanthology.org/2020.conll-1.37.
[8] S.Bhattamishra,A.Patel,P.Blunsom,andV.Kanade. Understandingin-contextlearningin
transformersandLLMsbylearningtolearndiscretefunctions. InTheTwelfthInternational
ConferenceonLearningRepresentations,2024. URLhttps://openreview.net/forum?
id=ekeyCgeRfC.
[9] P.Blanchard,D.J.Higham,andN.J.Higham. Accuratelycomputingthelog-sum-expand
softmaxfunctions. IMAJournalofNumericalAnalysis,41(4):2311–2330,2021.
[10] D.Blasi,R.Cotterell,L.Wolf-Sonkin,S.Stoll,B.Bickel,andM.Baroni. Onthedistribution
ofdeepclausalembeddings: Alargecross-linguisticstudy. InProceedingsofthe57thAnnual
MeetingoftheAssociationforComputationalLinguistics,pages3938–3943,2019.
[11] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
P.Shyam,G.Sastry,A.Askell,S.Agarwal,A.Herbert-Voss,G.Krueger,T.Henighan,R.Child,
A.Ramesh,D.Ziegler,J.Wu,C.Winter,C.Hesse,M.Chen,E.Sigler,M.Litwin,S.Gray,
B.Chess,J.Clark,C.Berner,S.McCandlish,A.Radford,I.Sutskever,andD.Amodei. Lan-
guagemodelsarefew-shotlearners. InH.Larochelle,M.Ranzato,R.Hadsell,M.Balcan,and
H.Lin,editors,AdvancesinNeuralInformationProcessingSystems,volume33,pages1877–
1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/
2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.
[12] D.Chiang,P.Cholak,andA.Pillay. Tighterboundsontheexpressivityoftransformerencoders.
InInternationalConferenceonMachineLearning,pages5544–5562.PMLR,2023.
[13] N.Chomsky. Syntacticstructures. Mouton,TheHague,1957.
[14] N.ChomskyandM.P.Schützenberger. Thealgebraictheoryofcontext-freelanguages. In
StudiesinLogicandtheFoundationsofMathematics, volume35, pages118–161.Elsevier,
1963.
[15] S.De,S.L.Smith,A.Fernando,A.Botev,G.Cristian-Muraru,A.Gu,R.Haroun,L.Berrada,
Y.Chen,S.Srinivasan,etal. Griffin: Mixinggatedlinearrecurrenceswithlocalattentionfor
efficientlanguagemodels. arXivpreprintarXiv:2402.19427,2024.
10[16] G. Deletang, A. Ruoss, J. Grau-Moya, T. Genewein, L. K. Wenliang, E. Catt, C. Cundy,
M.Hutter,S.Legg,J.Veness,andP.A.Ortega. Neuralnetworksandthechomskyhierarchy.
InTheEleventhInternationalConferenceonLearningRepresentations,2023. URLhttps:
//openreview.net/forum?id=WbxHAzkeQcn.
[17] J. Ebrahimi, D. Gelda, and W. Zhang. How can self-attention networks recognize Dyck-n
languages? InT.Cohn,Y.He,andY.Liu,editors,FindingsoftheAssociationforComputational
Linguistics:EMNLP2020,pages4301–4306,Online,Nov.2020.AssociationforComputational
Linguistics. doi:10.18653/v1/2020.findings-emnlp.384. URLhttps://aclanthology.org/
2020.findings-emnlp.384.
[18] M.B.Everaert,M.A.Huybregts,N.Chomsky,R.C.Berwick,andJ.J.Bolhuis. Structures,
notstrings: linguisticsaspartofthecognitivesciences. Trendsincognitivesciences,19(12):
729–743,2015.
[19] S.Garg,D.Tsipras,P.S.Liang,andG.Valiant. Whatcantransformerslearnin-context? a
casestudyofsimplefunctionclasses. AdvancesinNeuralInformationProcessingSystems,35:
30583–30598,2022.
[20] F. A. Gers and E. Schmidhuber. LSTM recurrent networks learn simple context-free and
context-sensitivelanguages. IEEETransactionsonNeuralNetworks,12(6):1333–1340,2001.
[21] A.GuandT.Dao. Mamba: Linear-timesequencemodelingwithselectivestatespaces. arXiv
preprintarXiv:2312.00752,2023.
[22] A.Gupta,A.Gu,andJ.Berant. Diagonalstatespacesareaseffectiveasstructuredstatespaces.
AdvancesinNeuralInformationProcessingSystems,35:22982–22994,2022.
[23] M.Hahn. Theoreticallimitationsofself-attentioninneuralsequencemodels. Transactionsof
theAssociationforComputationalLinguistics,8:156–171,2020.
[24] Y.Hao,D.Angluin,andR.Frank. Formallanguagerecognitionbyhardattentiontransformers:
Perspectives from circuit complexity. Transactions of the Association for Computational
Linguistics,10:800–810,2022.
[25] J.Hewitt,M.Hahn,S.Ganguli,P.Liang,andC.D.Manning. RNNscangeneratebounded
hierarchical languages with optimal memory. In B. Webber, T. Cohn, Y. He, and Y. Liu,
editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing(EMNLP),pages1978–2010,Online,Nov.2020.AssociationforComputational
Linguistics. doi: 10.18653/v1/2020.emnlp-main.156. URLhttps://aclanthology.org/
2020.emnlp-main.156.
[26] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780,1997.
[27] T. S. Jayram, R. Kumar, and D. Sivakumar. The one-way communication complexity of
hammingdistance. TheoryofComputing,4(1):129–135,2008.
[28] S.Jelassi,D.Brandfonbrener,S.M.Kakade,andE.Malach. Repeatafterme: Transformersare
betterthanstatespacemodelsatcopying. arXivpreprintarXiv:2402.01032,2024.
[29] W. B. Johnson and J. Lindenstrauss. Extensions of lipschitz mappings into hilbert space.
Contemporary mathematics, 26:189–206, 1984. URL https://api.semanticscholar.
org/CorpusID:117819162.
[30] F.Karlsson. Constraintsonmultiplecenter-embeddingofclauses. JournalofLinguistics,43(2):
365–392,2007.
[31] A.Katharopoulos,A.Vyas,N.Pappas,andF.Fleuret.Transformersarernns:Fastautoregressive
transformers with linear attention. In International conference on machine learning, pages
5156–5165.PMLR,2020.
[32] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980,2014.
[33] J.F.KolenandS.C.Kremer. Afieldguidetodynamicalrecurrentnetworks. JohnWiley&
Sons,2001.
[34] S. A. Korsky and R. C. Berwick. On the computational power of rnns. arXiv preprint
arXiv:1906.06349,2019.
[35] E.KushilevitzandN.Nisan. CommunicationComplexity. CambridgeUniversityPress,1996.
11[36] D.Lindner,J.Kramár,S.Farquhar,M.Rahtz,T.McGrath,andV.Mikulik. Tracr: Compiled
transformersasalaboratoryforinterpretability. AdvancesinNeuralInformationProcessing
Systems,36,2024.
[37] B.Liu,J.T.Ash,S.Goel,A.Krishnamurthy,andC.Zhang. Transformerslearnshortcutsto
automata. InTheEleventhInternationalConferenceonLearningRepresentations,2023. URL
https://openreview.net/forum?id=De4FYqjFueZ.
[38] B.Liu, J.Ash, S.Goel, A.Krishnamurthy, andC.Zhang. Exposingattentionglitcheswith
flip-floplanguagemodeling. AdvancesinNeuralInformationProcessingSystems,36,2024.
[39] W.MerrillandA.Sabharwal.Theparallelismtradeoff:Limitationsoflog-precisiontransformers.
TransactionsoftheAssociationforComputationalLinguistics,11:531–545,2023.
[40] W.MerrillandA.Sabharwal. Alogicforexpressinglog-precisiontransformers. Advancesin
NeuralInformationProcessingSystems,36,2024.
[41] G.A.MillerandN.Chomsky. Finitarymodelsoflanguageusers. InR.D.Luce,R.R.Bush,
andE.Galanter,editors,HandbookofMathematicalPsychology,pages419–492.JohnWiley,
1963.
[42] A.Orvieto,S.L.Smith,A.Gu,A.Fernando,C.Gulcehre,R.Pascanu,andS.De. Resurrecting
recurrentneuralnetworksforlongsequences.InInternationalConferenceonMachineLearning,
pages26670–26698.PMLR,2023.
[43] R.Pascanu,T.Mikolov,andY.Bengio. Onthedifficultyoftrainingrecurrentneuralnetworks.
InInternationalconferenceonmachinelearning,pages1310–1318.Pmlr,2013.
[44] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
N.Gimelshein,L.Antiga,etal. Pytorch: Animperativestyle,high-performancedeeplearning
library. Advancesinneuralinformationprocessingsystems,32,2019.
[45] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao, X. Cheng, M. Chung,
M.Grella,K.K.GV,etal. Rwkv: Reinventingrnnsforthetransformerera. arXivpreprint
arXiv:2305.13048,2023.
[46] B.Peng,S.Narayanan,andC.Papadimitriou. Onlimitationsofthetransformerarchitecture,
2024.
[47] J.Pérez,J.Marinkovic´,andP.Barceló. Ontheturingcompletenessofmodernneuralnetwork
architectures. InInternationalConferenceonLearningRepresentations,2019. URLhttps:
//openreview.net/forum?id=HyGBdo0qFm.
[48] A. Rao and A. Yehudayoff. Communication Complexity: and Applications. Cambridge
UniversityPress,2020.
[49] J.-F.Raymond,P.Tesson,andD.Thérien. Analgebraicapproachtocommunicationcomplexity.
InAutomata,LanguagesandProgramming: 25thInternationalColloquium,ICALP’98Aalborg,
Denmark,July13–17,1998Proceedings25,pages29–40.Springer,1998.
[50] P.Rodriguez. Simplerecurrentnetworkslearncontext-freeandcontext-sensitivelanguagesby
counting. Neuralcomputation,13(9):2093–2118,2001.
[51] C.Sanford,D.Hsu,andM.Telgarsky.Representationalstrengthsandlimitationsoftransformers.
InThirty-seventhConferenceonNeuralInformationProcessingSystems,2023. URLhttps:
//openreview.net/forum?id=36DxONZ9bA.
[52] C.Sanford,D.Hsu,andM.Telgarsky. Transformers,parallelcomputation,andlogarithmic
depth. arXivpreprintarXiv:2402.09268,2024.
[53] D. Sivakumar. Algorithmic derandomization via complexity theory. In Proceedings of the
thiry-fourthannualACMsymposiumonTheoryofcomputing,pages619–626,2002.
[54] N.Skachkova,T.A.Trost,andD.Klakow. Closingbracketswithrecurrentneuralnetworks. In
Proceedingsofthe2018EMNLPWorkshopBlackboxNLP:AnalyzingandInterpretingNeural
NetworksforNLP,pages232–239,2018.
[55] L. Strobl, W. Merrill, G. Weiss, D. Chiang, and D. Angluin. What formal languages can
transformersexpress? asurvey. TransactionsoftheAssociationforComputationalLinguistics,
12:543–561,2024.
[56] Y.Sun,L.Dong,S.Huang,S.Ma,Y.Xia,J.Xue,J.Wang,andF.Wei. Retentivenetwork: A
successortotransformerforlargelanguagemodels. arXivpreprintarXiv:2307.08621,2023.
12[57] M.Suzgun,Y.Belinkov,S.Shieber,andS.Gehrmann. LSTMnetworkscanperformdynamic
counting. InProceedingsoftheWorkshoponDeepLearningandFormalLanguages: Building
Bridges,pages44–54,Florence,Aug.2019.AssociationforComputationalLinguistics. doi:
10.18653/v1/W19-3905. URLhttps://www.aclweb.org/anthology/W19-3905.
[58] M.Suzgun,S.Gehrmann,Y.Belinkov,andS.M.Shieber. Memory-augmentedrecurrentneural
networkscanlearngeneralizeddycklanguages. arXivpreprintarXiv:1911.03329,2019.
[59] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and
I.Polosukhin. Attentionisallyouneed. InAdvancesinneuralinformationprocessingsystems,
pages5998–6008,2017.
[60] J.VonOswald,E.Niklasson,E.Randazzo,J.Sacramento,A.Mordvintsev,A.Zhmoginov,and
M.Vladymyrov.Transformerslearnin-contextbygradientdescent.InInternationalConference
onMachineLearning,pages35151–35174.PMLR,2023.
[61] G.Weiss,Y.Goldberg,andE.Yahav. Thinkingliketransformers. InInternationalConference
onMachineLearning,pages11080–11090.PMLR,2021.
[62] K.Wen,Y.Li,B.Liu,andA.Risteski. Transformersareuninterpretablewithmyopicmethods:
acasestudywithboundeddyckgrammars.AdvancesinNeuralInformationProcessingSystems,
36,2024.
[63] T.Wolf,L.Debut,V.Sanh,J.Chaumond,C.Delangue,A.Moi,P.Cistac,T.Rault,R.Louf,
M.Funtowicz,etal. Transformers: State-of-the-artnaturallanguageprocessing. InProceed-
ings of the 2020 conference on empirical methods in natural language processing: system
demonstrations,pages38–45,2020.
[64] A.C.-C.Yao. Somecomplexityquestionsrelatedtodistributivecomputing(preliminaryreport).
InProceedingsoftheeleventhannualACMsymposiumonTheoryofcomputing,pages209–213,
1979.
[65] S.Yao,B.Peng,C.Papadimitriou,andK.Narasimhan. Self-attentionnetworkscanprocess
boundedhierarchicallanguages. InC.Zong,F.Xia,W.Li,andR.Navigli,editors,Proceedings
of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th
International Joint Conference on Natural Language Processing (Volume 1: Long Papers),
pages 3770–3785, Online, Aug. 2021. Association for Computational Linguistics. doi: 10.
18653/v1/2021.acl-long.292. URLhttps://aclanthology.org/2021.acl-long.292.
[66] X.Yu,N.T.Vu,andJ.Kuhn. Learningthedycklanguagewithattention-basedseq2seqmodels.
InProceedingsofthe2019ACLWorkshopBlackboxNLP:AnalyzingandInterpretingNeural
NetworksforNLP,pages138–146,2019.
[67] C. Yun, S. Bhojanapalli, A. S. Rawat, S. Reddi, and S. Kumar. Are transformers universal
approximatorsofsequence-to-sequencefunctions? InInternationalConferenceonLearning
Representations,2020. URLhttps://openreview.net/forum?id=ByxRM0Ntvr.
[68] H.Zhou,A.Bradley,E.Littwin,N.Razin,O.Saremi,J.Susskind,S.Bengio,andP.Nakkiran.
What algorithms can transformers learn? a study in length generalization. arXiv preprint
arXiv:2310.16028,2023.
13Contents
1 Introduction 1
1.1 RelatedWork . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
2 Definitions 3
3 IndexLookupTask 4
4 LowerBoundsforRNNsand1-layerTransformers 5
4.1 SeparationonBoundedHierarchicalLanguages . . . . . . . . . . . . . . . . . . . 6
4.2 LowerBoundsonBooleanFunctions. . . . . . . . . . . . . . . . . . . . . . . . . 6
5 RepresentationalCapabilitiesof2-layerTransformers 7
5.1 RepresentingBooleanFunctions . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
5.2 ImplementingtheNearestNeighborsAlgorithm . . . . . . . . . . . . . . . . . . . 7
6 EmpiricalAnalysis 8
7 DiscussionandFinalRemarks 9
A Clarifications 14
B Preliminaries 15
B.1 CommunicationComplexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
B.2 FinitePrecisionImplementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
B.3 TechnicalTools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
C IndexLookupTask 17
C.1 RecurrentmodelsmustbewidetoperformIndexLookup . . . . . . . . . . . . . . 17
C.2 1-layerTransformerwithsmallwidthcanperformIndexLookup . . . . . . . . . . 18
D LowerBoundsfor1-layerTransformers 19
E DyckwithBoundedDepths 21
F TransformersandBooleanfunctions 23
F.1 LowerBounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
F.2 TransformerConstructionforEquality . . . . . . . . . . . . . . . . . . . . . . . . 25
F.3 RepresentingmoregeneralclassofBooleanfunctions . . . . . . . . . . . . . . . . 27
F.4 DifficultyofDerivingCommunication-basedLowerBoundsfor2-layerTransformers 28
G NearestNeighborsandAssociativeRecall 29
G.1 LowerBoundsforRecurrentModels . . . . . . . . . . . . . . . . . . . . . . . . . 30
G.2 TransformerConstructionforNearestNeighbor . . . . . . . . . . . . . . . . . . . 30
H EmpiricalAnalysis: AdditionalDetailsandExperiments 34
H.1 AdditionalExperimentsandDataGeneration . . . . . . . . . . . . . . . . . . . . 34
H.2 StringEqualityTask. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
A Clarifications
(1)Whatdotheresultspresentedinthepaperimplyaboutthelearnabilityofthetasksconsidered?
Thelowerboundsonrecurrentmodelsandone-layerTransformershavenegativeimplicationsfor
learnabilitybutthepositiveresultsdonothaveanynontrivialimplications. Thelowerboundson
thesizeofrecurrentmodelsandone-layerTransformersimplythatunlessthewidthofthemodels
growslinearlywithrespecttotheinputlength,theycannotrepresentandconsequentlycannotlearn
suchtasks. Note,however,thateventhoughtheinputlengthneednotbeunboundedlylongforour
lowerboundstoapply,theystillneedtobesufficientlylargeN ≳manddonotapplyatthelengths
14consideredinourexperiments. TheresultsindicatingthatTransformerscanexpresstaskslikeIndex
lookup,stringequalityornearestneighborsdonotimplythattheycanlearnthosetasksinpractice.
TheexperimentsinSections6andHaswellasfrompriorworks[8]seemtoindicatethatsmall-sized
Transformersperformreasonablywellonthesetasks.
(2)FortheupperboundsonthetotalsizeofTransformerstoexpressfunctions,doesitincludethe
inputandpositionalembeddingsorjusttheTransformermodelwithattentionandfeedforwardblock?
Yes,whenwesaythatTransformerswithtotalsizeO(f(N))orpoly-logarithmicinN canexpressa
certaintask,itincludestheinputandpositionalembeddings. Thetotalrepresentationsizeindicates
thetotalnumberofbitsrequiredtorepresentalltheparametersofthemodel. Ourresultsimplythata
TransformerwithtotalsizeO((logN)3)canrepresenttaskssuchasnearestneighbors,Equality,etc,
whereasthesizeofanyrecurrentmodelmustbeΩ(N). ForourconstructionsofTransformers,we
ensurethatthepositionalembeddingscanbegeneratedinlogspace(discussedinAppendixB)and
neednotbestoredasaN ×dmatrixfortherequiredcomputations. Lastly,itisworthnotingthatall
ourlowerboundsforrecurrentmodelsapplyeveniftheyhavearbitrarypositionalembeddingsstored
inN ×dmatrix,andhencethesizeofrecurrentmodelsisΩ(N)excludingtheembeddings.
(3)FortheconstructionswithTransformers,whycan’tthoseresultsfollowfromsomeframeworks
likeRASP?
RASP[61,36]isaprogramminglanguageaimingtoabstractcomputationsthattransformerscan
compute. While one might be able to construct RASP programs for some of the tasks we have
considered,suchasIndexLookup,suchconstructionswouldnotentailresultssimilartoours,because
RASPsubstantiallyabstractsfromimplementationalaspects. Forinstance,RASPallowsMLPsto
computearbitraryfunctions,andattentionisnotcomputedfromdotproductsofkeysandqueries.
Itisnotclearifgeneral-purposetranslationsfromRASPtorealistictransformerswouldbeableto
recoverourefficientbounds,e.g.,logarithmicsize.
(4)Dothenegativeresultsinexperimentsimplythatthosearchitecturescannotlearnthosetasks?
Thedifferencesintheperformanceofmodelsapplymoretotherateoflearningthanabinaryform
ofsuccess/failure. Forinstance,intheIndexLookuptask,onecanseethatinFigure2,one-layer
Transformersachievenear-perfectaccuracyinafewthousandstepswhereasrecurrentmodelsfailto
achievehighaccuracyevenafterbeingtrainedfor25×steps. Itcanstillbetruethatifthemodelsare
trainedformuchlongeroraremuchbigger,theymightachievehighaccuracy. SeeFigure3which
depictstheperformanceofMambaacrossvarioussizesandlengths. FortaskslikeIndexlookup,
weobservethatTransformersachievenear-perfectaccuracyacrossseverallearningrateswhereas
recurrentarchitecturesfailonallthelearningrateswetried. Asimilarthingistrueforothertasks
suchasDyck-(2,2)whereone-layerTransformersseemtolearnatamuchslowerratecomparedto
LSTMsandtwo-layerTransformersevenforthelengthswheretheydoachievenear-perfectaccuracy.
(5)DothelowerboundsforTransformersandRNNsapplywithadditionalcomponentssuchaslayer
norm,residualconnections,positionalembeddings,etc?
Yes,forourlowerboundsforone-layerTransformersandRecurrentmodels,theystillapplyifthe
modelshaveadditionalcomponentsincludingarbitrarypositionalembeddings,layernorms,etc. We
onlyassumethattheattentionmechanismoperatesoverfiniteprecisionandthehiddenstateofRNN
isinfiniteprecision. Theresultsdonotmakeanyassumptionsaboutthecomputationallimitsofthe
remainingcomponents. Theyapplyevenwhentheoutputoftheattentionblockorthehiddenstateof
anRNNisprocessedbyanarbitraryfunction.
B Preliminaries
B.1 CommunicationComplexity
Our lower bounds for recurrent models and 1-layer Transformers are based on communication-
complexity bounds. We assume some familiarity with communication complexity (see Rao and
Yehudayoff[48],KushilevitzandNisan[35]foranintroduction). Wewillprimarilyfocusonthe
two-partysettingwherethecommunicationcomplexityofafunctionindicatesthenumberofbitstwo
partiesmustexchangeinordertocomputetheoutputofafunction. IfAliceandBobwanttocompute
afunctionf :{0,1}N →{0,1}whereAlicehasthebitsintheindicesI ⊂[N]andBobhasthebits
inindices[N]\I,thecommunicationcomplexityoff overthatpartitionistheminimumnumber
15ofbitstheymustexchangetocomputetheoutputforallinputsx ∈ {0,1}N. AliceandBobare
allowedunboundedcomputationalpower. IfAliceandBobmustexchangeatleastkbitstocompute
afunctionf(x)∀x ∈ {0,1}n overanypartitionoftheinputthenwesaythatthecommunication
complexityC(f)≥k. Iftheyuseacommunicationprotocolwhereonlyonepartyisallowedtosend
bitstotheotherparty,thenitiscalledone-waycommunication,andthecommunicationcomplexity
ofthefunctioninthatsettingisreferredtoasone-waycommunicationcomplexity.
Forourresults,wewillusethefollowingthreewell-knownfactsaboutthecommunicationcomplexity
oftheDisjointness,Equality,andtheINDEXproblem.
Disjointness. The disjointness function takes two sets A,B ⊆ [N] as input and returns 0 if
the two sets are disjoint and returns 1 otherwise. This can also be seen as a function DISJ :
{0,1}N×{0,1}N →{0,1}overtwoBooleaninputssuchthatDISJ(a,b)=maxa ib
i
=I[aTb>0].
IfAlicehastheinputvectora∈{0,1}N andBobhasthevectorb∈{0,1}N,thenthecommunication
complexityofDISJindicatestheminimumnumberofbitsthatAliceandBobwillhavetoexchangeto
determinetheoutputoftheDISJfunction. Thefollowingisawell-knownfactaboutthedisjointness
problem,
Fact1. [Disjointness[64]]IfAliceandBobhavetwoinputsa,b∈{0,1}N,thenanydeterministic
communicationprotocolusedbythemtocompute DISJ(a,b) = maxa ib
i
mustexchangeatleast
N-bits. Moreover,therandomizedcommunicationcomplexityoftheDISJisΩ(N).
Fact2. [Equality[48,Ch.1]]IfAliceandBobhavetwoinputsa,b∈{0,1}N,thenanydeterministic
communicationprotocolusedbythemtocomputeEQ(a,b)=I[a=b]mustexchangeatleastN-bits.
Fact3. [INDEX[27]]IfAliceandBobhavetwoinputsa∈{0,1}N andb∈[N]respectively,then
anydeterministiccommunicationprotocolusedbyAlicetosendbitstoBobmustrequireN bitsfor
BobtocomputeINDEX(a,b)=a b. Moreover,theone-wayrandomizedcommunicationcomplexityof
theINDEXisΩ(N).
One thing to note about the Equality problem is that although the deterministic communication
complexityoftheEQproblemisΩ(N),therandomizedcommunicationcomplexityisO(logN).
For the Disjointness and INDEX problems, the randomized communication complexity is Ω(N)
aswell. Inotherwords,evenifthetwopartiesareallowedtocomputetheoutputofthefunction
correctlywithhighprobability(say> 2/3), eventhenthenumberofbitstheymustexchangeis
Ω(N).
B.2 FinitePrecisionImplementation
Inthiswork,weareinterestedintheexpressivenessoffiniteprecisionmodels. Forourconstructions
withTransformers,wewillworkwithp-bitnumberswherep=Θ(logN)whereN isthemaximum
lengthoftheinputstring.
In particular, for some sufficiently large constant K > 0, we will work with numbers between
c
−NKc to NKc with a step size of ∆ = N1 Kc. If Q
p
is the set of all such numbers then Q
p
=
{−NKc,−NKc +∆,...,0,∆,2∆,...,NKc}. Hence, the size of the set is |Q p| = N2Kc =⇒
p=2K logN =Θ(logN). Foranyrealnumberz ∈R,inthefiniteprecisionimplementation,zis
c
roundeddowntothenearestzˆsuchthatzˆNKc ∈Z. Ifz >NKc,thenzisroundeddowntoNKc. In
ourconstructionswithTransformers,alltheparametersandintermediatevaluesfollowtheabove
implementation.
B.3 TechnicalTools
For our constructions of Transformers, we will use the following result about high dimensional
vectors. Thestatementessentiallysaysthatatahighdimensionk thenumberofvectorsthatare
almostorthogonaltoeachotherisexponentialinkeventhoughthenumberoforthogonalvectors
canbeatmostk.
Lemma2. ForanyN,thereexistsN k-dimensionalvectorsT ,...,T wherek = O( 1 logN)
1 N γ2
andeachentryofthevectorsisin{−√1 ,√1 }suchthat
k k
(cid:40)
≥1−γ ifi=j,
⟨T ,T ⟩
i j ≤γ otherwise.
16It is quite straightforward to see why this is true. It follows from a simple application of the
probabilisticmethod. Suppose,onesamplesN k-dimensionalvectorsX ,...,X independentlyat
1 N
randomsuchthateachentryofeachvectorisdrawnuniformlyfrom{−√1 ,√1 }. Hence,eachvector
k k
X
i
∈{−√1 ,√1 }k andtheexpectationofthedotproductofanytwovectorsE[⟨X i,X j⟩]=0for
k k
alli̸=j. Sincethedotproductsofanytwovectors⟨X ,X ⟩isaboundedrandomvariable,wecan
i j
applyHoeffding’sinequalitytoget
(cid:32) (cid:33)
−kγ2
P[|⟨X ,X ⟩|≥γ] ≤ 2exp .
i j 4
(cid:16) (cid:17)
TakingaunionboundoveratmostN2pairsofdotproductsandsetting2N2exp −kγ2 <1/2,we
4
getthatfork = 8 log2N,theprobabilityofalldotproducts⟨X ,X ⟩beinglessthanγ atthesame
γ2 i j
timeisatleast1/2. Sincetheprobabilityoftheeventisnonzero,itfollowsthatthestatementinthe
lemmaistrue.
In our construction, we will use such vectors as positional embedding vectors. While Lemma 2
impliestheexistenceofvectors,storingN suchvectorswillrequireΘ(N)space. Wewouldliketo
generatei-thvectorT whennecessaryinpolynomialtimeusinglogspacewithoutstoringallofthem
i
together. Toachievethat,weuseaderandomizationoftheJohnson-Lindenstrauss(JL)Lemma[29]
thatcanoutputthei-thvectorinlog-spaceandpolynomialtime[53].
WeuseaslightlymodifiedversionoftheJLlemmawhichpreservesinnerproductsoverunitvectors.
Lemma3. [Innerproductpreservation[1]]Letϵ∈(0,1/2)andletQ⊂Sd−1 beasetofN unit
normvectorsofdimensiond. Fork = c2logN,thereexistsalinearmapT(x)= √1 Axwhereeach
ϵ2 k
entryofA:Rd →Rk isin{−1,1}suchthatforallu,v ∈Q,
|⟨u,v⟩−⟨T(u),T(v)⟩|≤ϵ
The above result has interesting implications which we will use in our constructions. Let
x ,...,x ∈RN beN unitnormvectorsthatformthebasisofRN whichimplies⟨x ,x ⟩=1if
1 N i j
i=j andis0otherwise. ThenthereexistsamapT suchthatthevectorsT(x ),...,T(x )∈Rk
1 N
havedotproducts⟨T(x ),T(x )⟩=1±ϵifi=j andis0±ϵotherwise.
i j
WewilluseJLtransformationsofthestandardbasisvectorse ,...,e whereT(1),...,T(N)will
1 N
refertok =O(logN)dimensionalvectorssuchthattheirinnerproductis≈1withthemselvesand
is≈0. Intuitively,wecanusesuchvectorstogetaTransformertoattendtouniquepositions.
Corollary 8.1. For any N, there exists N k-dimensional vectors T(1),...,T(N) where k =
O(logN)andeachentryofthevectorsisin{−√1 ,√1 }suchthat
k k
(cid:40)
≥3/4 ifi=j,
⟨T(i),T(j)⟩
≤1/4 otherwise.
C IndexLookupTask
Index Lookup. The index lookup task (IdxL) is a multi-class classification task where a model
receivesasequenceoftokensx ,...,x followedbyapositiontokenpwherep∈[N]andthegoal
1 N
ofthemodelistooutputthetokenx atpositionp. Herethesymbolsx belongtoavocabularyΣ,a
p i
finitesetofsymbols. Thesequencex=(x ,...,x )canhaverepetitions. Moreprecisely,wesay
1 N
amodelcomputesthefunctionIdxL : Σ ×[N] → Σifforallinputs(x,p),themodeloutputs
≤N
IdxL(x,p).
C.1 RecurrentmodelsmustbewidetoperformIndexLookup
Theorem 3. Any recurrent model with a hidden state of width m using p-bits of precision that
computestheIndexLookuptaskforallsequencesoflengthN musthavem≥N/p.
17Proof. The proof follows naturally via a reduction from the INDEX problem in communication
complexity. AssumethevocabularysizefortheIdxLisatleast2,pickanytwosymbolsfromthe
vocabulary,andmapthemto0and1. SupposeAlicehasasequencea ∈ {0,1}N andBobhasan
indexi∈[N]. BothofthemhaveaccesstoarecurrentmodelasdescribedinSection2,whichcan
performtheIdxLtaskperfectly. Alicecanthenprovidethesequenceatotherecurrentmodelusing
anytwosymbolsinthevocabularyΣanditerativelyupdatethehiddenstatetoobtaintheNthhidden
stateh . AlicecanthensendthehiddenstatetoBobwhichrequiresmpbits. Bobcanthenprovide
N
thepositiontokenbasedontheindexiandcomputetheoutputtofigureoutwhethera is0or1.
i
NotethatAliceandBobcancomputetheoutputusingone-waycommunicationofmpbitsandhence
basedonFact3,itmustbethecasethatmp≥N.
Associative Recall. A similar argument can be used to show that any recurrent model that can
correctlyperformthesinglequeryassociativerecalltask[3]musthaveawidthorhiddenstateof
sizeatleastΩ(N/p). Intheassociativerecalltask,amodelispresentedwithasequenceofsymbols
andlabelsx ,y ,...,x ,y followedbyaquerysymbolx whichisoneofthesymbolspresented
1 1 N N q
earlier (x ,...,x ). The goal of a model is to output the label y corresponding to the symbol
1 N i
x =x wherei=1,...,N. Inthistask,thesymbols(x ,...,x )mustbedistinct.
q i 1 N
Proposition 9. Any recurrent model with a hidden state of size m over p-bits of precision that
computestheassociativerecalltaskforallsequencesoflengthN musthavem≥N/p.
A similar reduction from the INDEX problem can be used to show a lower bound for recurrent
models. BothAliceandBobhavethedescriptionoftherecurrentmodelandbothofthemcanhavea
predeterminedprotocolforthesequenceofsymbolsx ,...,x . Alicecanusetherecurrentmodel
1 N
tocomputethehiddenstateh byprovidingitwithinputsx ,a ,...,x ,a wherethebitsin
2N 1 1 N N
a{0,1}N areprovidedaslabels. AlicecansendthehiddenstateusingmpbitsandBobcanprovide
thesymbolx correspondingtothequeryindexiandcomputetheoutput. Hence,thesizeofthe
i
hiddenstateoftheRNNmustbeatmostN/p.
C.2 1-layerTransformerwithsmallwidthcanperformIndexLookup
WhileanyformofrecurrentmodelmusthaveawidthorhiddenstateofsizeΩ(N)toperformthe
indexlookuptask,weshowthata1-layerTransformerwithwidthO(logN)canperformtheindex
lookuptask.
Theorem1. ForallN ∈N,thereisa1-layerTransformerwithwidthm=O(logN)andprecision
p=O(logN)whichperformstheindexlookuptaskforallinputsequencesoflengthsuptoN.
Proof. Foraninputsequence(s ,...,s ,p),theTransformerusestheembeddingsoftheposition
1 N
tokenpandthepositionalembeddingsofthefirstN inputstoattendovers ,sothatthefeedforward
p
network can extract the label from the output of the attention block. Our key idea is to use the
N almostorthogonalvectorsprovidedbyLemma2,bothaspositionalembeddingsandalsoasa
way to embed the numbers {1,...,N}, any of which can be used as the index p. Formally, let
T(1),...,T(N)beN vectorsofdimensionk =O(logN)suchthat⟨T(i),T(j)⟩≤1/4fori̸=j
and⟨T(i),T(j)⟩≥3/4fori=j.
Recallthat,intheindexlookuptask,thealphabetV = Σ∪[N]consistsofsymbolss fromaset
i
Σ and the index tokens [N] = {1,...,N}. The embeddings of each input token will be of size
log|Σ|+2kwherelog|Σ|+kindicesarereservedforthetokenembeddingsandthelastkindices
areusedforthepositionalembeddings. SupposeweuseabinaryencodingforsymbolsinΣ,and
ρ:Σ→{0,1}log|Σ|issuchthatρ(s)isthebinaryencodingofs∈Σ. Thetokenembeddingofeach
symbols ∈Σisρ(s )oflengthlog|Σ|followedbykzeros. Forinputssequences ,...,s ,the
j j 1 N
embeddingvectorswillbeoftheformx =[ρ(s ),0 ,T(i)].
i i k
Theembeddingforanyindextokenp∈[N]containslog|Σ|zerosfollowedbythevectorT(p). In
otherwords,theembeddingfortheindextokenwillbeidenticaltothepositionalembeddingsused
inthefirstN tokens. Theembeddingvectorcorrespondingtotheindextokenpwillbeoftheform
p=[0 ,T(p),0 ].
log|Σ| k
18Theoutputofthe1-layerTransformeriscomputedbyapplyingattentionovertheinputsequence
withthequeryvectorcorrespondingtothelasttoken,followedbyanapplicationofthefeedforward
network over the output of the attention block. We can define the matrices W = η[O;I;O],
Q
W = [O,O,I], W = [I,O,O], whereη > 0isaparameterthatwillbespecifiedlater, Iisa
K V
squareidentitymatrixandOisazero-matrixoftheappropriateshape. Withthesedefinitionswe
get that the query vector Q(p) = η[T(p)] contains the middle part of the input embedding, the
key vectors K(x ) = [T(i)] contain the last part of the input embedding and the value vectors
i
V(x )=[ρ(s )]containthefirstpartoftheinputembedding.
i i
Withthesequeryandkeyvectors,thedotproductsinattentionsatisfy:
(cid:40)
≥3η/4 ifi=p,
⟨Q(p),K(x )⟩ .
i ≤η/4 ifi̸=p
Additionally,thedotproductofthequeryvectorwithitselfwillbe⟨Q(p),K(p)⟩=0.
Toretrievetherequiredtokenwiththesoftmaxoperator,consider
exp(⟨Q(p),K(x )⟩)
softmax(Q(p),K(X)⊤) = p
p exp(⟨Q(p),K(x )⟩)+(cid:80) exp(⟨Q(p),K(x )⟩)
p j̸=p j
exp(3η)
≥ 4
exp(3η)+Nexp(η)
4 4
whichis> 3 foranyη > 2log(3N). Thatis,forη > 2log(3N),theattentionweightoverinput
4
tokenx withaquerytokenpwillbegreaterthan3/4,andhencethetotalattentionweightoverthe
p
remainingtokenswillbelessthan1/4.
Recallthatthevaluevectorscontainthebinaryencodingsoftheinputsymbols,V(x ) = [ρ(s )].
i i
Let q ,...,q be the probabilities assigned tothe N tokens by the attention operator. Note that
1 N
(cid:80)
q ≥3/4. Letz¯= q ρ(s ). Notethatifthej-thbitofρ(s )is1,thenz¯ ≥3/4andotherwise,
p i i i p j
z¯ ≤ 1/4. From there a ReLU-FFN can transform the vector z¯to the vector ρ(s ) which is the
j p
desiredoutput.
Discussion. Whileweshowedthataone-layerTransformerwithO(logN)widthcanrepresentthe
index lookup task, it is unclear whether a one-layer Transformer with a small width can express
theassociativerecalltask. Theconstructionabovecannotbeadaptedinastraightforwardmanner
toshowthatone-layerTransformerscanrepresenttheassociativerecalltask. Fromtheresultsin
SectionGonnearestneighbors,itfollowsthattwo-layerTransformerswithlogarithmicwidthcan
express the associative recall task. At the same time, the lower bound techniques for one-layer
TransformerspresentedinSectionDdonotdirectlyapplytotheassociativerecalltaskandhence
itisnotstraightforwardtoprovethatTransformersmusthavetwolayersinordertoperformthe
associativerecalltask.
The index lookup task serves a few purposes in our work. The result that it is in some sense
easierforTransformersanddifficultforrecurrentmodelsmaynotbeverysurprisingbasedonthe
intuitiveunderstandingofthearchitectures. Ourresultshelptheoreticallyformalizetheintuitionsthat
Transformerscanuseattentiontoarbitrarilyretrievetokenswhereasrecurrentmodelsmustcompress
theinformationintheirhiddenstates. Secondly,thetaskhelpsusintroducethetechniquesthatwill
beusedfortheconstructionsandlowerboundstoobtainmoregeneralresultsinthelatersections.
Lastly, it serves as a simple task that separates one-layer Transformer and recurrent models. As
describedabove,itisnotstraightforwardtoshowthattheassociativerecalltaskcanorcannotbe
expressedbyone-layerTransformerswithsmallwidthbutwiththeindexlookuptask,wehavethat
one-layerTransformerscanrepresentthemefficiently.
D LowerBoundsfor1-layerTransformers
AsdescribedinSectionB.2,andinkeepingwithreal-worldimplementations,theoutputsofinterme-
diatecomputationsareroundedtop-bitprecision. Furtherinkeepingwithrealimplementations,and
toavoidoverflowinexponentiation,softmaxisimplementedbyfirstsubtractingthemaximumlogit,
19as
exp(A −max A )
softmax(A) = i,j l i,l .
i,j (cid:80)M
exp(A −max A )
k=1 i,k l i,l
Thisisapopularapproachtoimplementingsoftmax[9];itensuresthattheexponentiatedintermediate
resultsarein[0,1],avoidingpossibleoverflowwhenexponentiatinginfiniteprecision.
Theorem4. Consideraone-layerTransformerf ∈ TF1 operatingoverinputsoflengthN.
m,p,H
Consider any disjoint subsets S ∪S = {1,...,N}, S ∩S = ∅. Assume Alice has access
A B A B
to s for i ∈ S , and Bob has access to s for i ∈ S . Then Alice and Bob can communicate
i A i B
3m(p+logN)H bitstocomputetheoutputf(s ...s ).
1 N
WenotethatconceptuallyrelatedargumentswereusedinSanfordetal.[51,Theorem7]andPeng
etal.[46,proofofTheorem1]). Ourapproachheregeneralizesbystatingthisforarbitrarypartitions
overtheinput.
Proof. Withoutlossofgenerality,assumethatN ∈S ,i.e.,Alicehasaccesstox .
A N
In the first step, Alice sends x to Bob using dp bits of communication. Then, Alice and Bob
N
computeforeachheadtheattentionlogitsforeachpositionwithintheirrespectivesets:
A :=⟨Q(x ),K(x )⟩ (1)
N,i N i
Thesenumbersareroundedtopbitsofprecision.
Inthesecondstepoftheprotocol,AliceandBobexchange2pbitstodetermineM :=max A ,
i N,i
andcompute
A(cid:98)N,i =A N,i−M (2)
fortheirrespectivepositionsi∈S A,S B. Notethat,asA(cid:98)N,i ≤0,exp(A(cid:98)N,i)∈(0,1],sothereare
nooverflowissuesarisingfromthep-bitrepresentation. Thentheycanindividuallycompute
(cid:88)
Z
A
:= exp(A(cid:98)N,i) (Alice)
i∈SA
(cid:88)
Z
B
:= exp(A(cid:98)N,i) (Bob)
i∈SB
eachwithpbitsofprecision; bothnumbersarein[0,N], andatleastoneofthemis≥ 1. Asall
intermediatecomputationsareroundedtopbits,exp(A(cid:98)N,i)isin[0,1]androundedtopbits,Z A,Z
B
canberepresentedwithp+logN bits.
Inthethirdstepoftheprotocol,theyexchange2(p+logN)bitstoexchangethese. Theythenboth
haveaccessto
N
(cid:88)
Z := exp(A(cid:98)N,j)=Z A+Z
B
(3)
j=1
Here,wenotethatthedefinitionoffiniteprecisionarithmeticinAppendixB.2makesadditionof
nonnegativenumbersassociative;hence,theoutcomehereisindependentofthepartitionS ,S
A B
andagreeswiththeresultwhendirectlysummingtheexponentiatedlogits.4
The result Z is in [1,N] as max iexp(A(cid:98)N,i) = 1. This permits Alice and Bob to compute the
attentionscoresa :
(cid:98)i
a :=softmax(A) =
exp(A(cid:98)Ni)
(4)
(cid:98)i N,i (cid:80)N
j=1exp(A(cid:98)Nj)
eachwithpbitsofprecision.
4Asimilarprotocolstillworksinotherbounded-precisionschemeswhereadditionisnotassociative;asall
exponentiatedlogitsarein[0,1],onecanuseextendedprecisionwithp+logN bitstoperformthesummation
exactlyandthenroundbacktopbits.
20Thentheybotheachcompute:
(cid:88)
U := softmax(A) V(x ) (Alice)
A N,i i
i∈SA
(cid:88)
U := softmax(A) V(x ) (Bob)
B N,i i
i∈SB
(cid:80)
withpbitsofprecision. AseachentryAtt(A) ,x haspbitsofprecision,and Att(A) =1,
N,i i i N,i
thesumscanbeexactlyrepresentedat2pbitsofprecision.
Inthefourthstepoftheprotocol,theyexchangethese,whichamountsto4mpbitsofcommunication.
Thentheycompute
N
(cid:88)
softmax(A) V(x )=U +U (5)
N,i i A B
i=1
androundittopbitsofprecision. Fromthis,theycanobtaintheresult.
In total, the four steps took ≤ 3m(p+logN) bits of communication. Performing this protocol
separatelyforeveryheadleadsto≤3m(p+logN)H bitsofcommunication.
E DyckwithBoundedDepths
It is generally agreed that processing and comprehending natural language requires processing
hierarchical structures [e.g. 13, 18]. The fundamental computational problem here consists of
matchingandrelatingmaterialthatmatcheshierarchically,evenifitappearsatagreatlineardistance.
ThisproblemisformalizedbythefamilyofDycklanguages[14]: languagesofwell-matchedwords
overoneormoretypesofparentheses. Theselanguagesformalizeproblemsthatcanbesolvedwith
accesstoastack,whereopeningparenthesesarepushedandclosingparenthesesarepopped. Beyond
afundamentalmodelofhierarchicalstructure,theyareofcentralimportanceinformallanguage
theory,asanycontext-freelanguagecanbeexpressedintermsofDycklanguages[14]. Perhapsdue
totheboundednessofhumanmemory[41],naturallanguagetendstohavemoreboundedlevelsof
embedding[30,10]. Thishasmotivatedthestudyofbounded-depthDycklanguagesasplausible
simple models of the hierarchical structure underlying language, with substantial interest in the
abilitiesofneuralarchitecturestomodelthem[25,65,6].
WewillprimarilyfocusonDyck-2languageswithdepthatmostk,denotedasDyck-(2,k). The
Dyck-2languagecontainstwotypesofbrackets,suchasroundbrackets‘(’,‘)’andsquarebrackets
‘[’,‘]’. TheDyck-(2,2)languagewithdepthatmost2containswell-balancedparenthesiswherefor
anyprefix,thenumberofunbalancedparenthesescanbeatmost2. Forinstance,thestring‘([])[]’
hasadepthatmost2whereasthestring‘([[]])’hasadepthof3. Wesayamodelrecognizesa
languageifitcancorrectlyclassifywhetherornotastringbelongstothelanguage. Wewillprimarily
focusonstringswithmaximumlengthN andstudyhowthesizeofamodeldependsonthat.
Our main result in this section is that any 1-layer Transformer that can recognize Dyck-2 with
boundeddepthsmusthaveawidththatgrowslinearlywiththeinputlengthN. Toshowthat,wewill
firstshowthatthecommunicationcomplexityoftheDyck-2languagewithdepthatmost2isatleast
N −1. Thelowerboundonthewidthof1-layerTransformerswillfollowfromthelowerboundon
thecommunicationcomplexityofDyck-(2,2).
Problem. Let Σ = {‘(′,‘)′,‘[′,‘]′} be the vocabulary of a language Dyck-(2,k). Let Σn denote
the set of all strings of length exactly n and Σ≤n denote the set of all strings of lengths up to n.
ThecommunicationproblembetweenAliceandBobisdefinedasfollows. AssumethelengthN
is even for this problem. For a string x ∈ ΣN, Alice has the symbols in the odd indices of the
stringandBobhasthesymbolsintheevenindicesofthestring. Theyhavetocomputethefunction
f : ΣN/2×ΣN/2 → {0,1}whichoutputs1ifthestringxisinthelanguageDyck-(2,2)and
Dyck
outputs0otherwise.
Foranyfunction,thefoolingsetisdefinedinthefollowingway,
Definition 1. [Fooling set] A fooling set for any function f : ΣN/2 ×ΣN/2 → {0,1} is a set
S ⊆ΣN/2×ΣN/2andavalueb∈{0,1}suchthat,
21• Forevery(x,y)∈S,f(x,y)=b.
• Foreverytwodistinctpairs(x ,y ),(x ,y )∈S,eitherf(x ,y )̸=borf(x ,y )̸=b.
1 1 2 2 1 2 2 1
Thefollowingisawell-knownfactincommunicationcomplexity.
Fact4. Foranyfunctionf,ifthereexistsafoolingsetofsize|S|,thenthecommunicationcomplexity
C(f)≥log |S|.
2
Lemma1. SupposeAliceandBobhavethesymbolsintheoddandevenindicesofastrings∈ΣN
respectively. Toeachcomputewhethers∈Dyck-(2,2),theymustexchangeatleastN −1bits.
Proof. Theprooffollowsfromthefactthatthereexistsafoolingsetofsize2N−1forthelanguage
Dyck-(2,2)withstringsoflengthN. ThefoolingsetSisconstructedwithallstringsins=(x,y)∈
ΣN suchthatf (s)=1. EachstringsinS satisfies:
Dyck
s=(x,y) where x∈ΣN/2,y ∈ΣN/2, andf (x,y)=1.
odd even Dyck
Here,x=(x ,x ,...,x )andy =(y ,y ,...,y )representthesequencesofsymbolsatodd
1 2 N/2 1 2 N/2
andevenindices,respectively.
Constructingthefoolingset. Notethat,ifxisthestringofsymbolsintheoddindicesofastring
s ∈ Dyck-(2,2), then the string of symbols y in the even indices such that f (x,y) = 1 is
Dyck
unique. Supposeoneisprovidedwiththestringx=(x ,...,x ),thenonecandeterministically
1 N/2
determinethesymbolsintheevenindicesy =(y ,...,y )inthefollowingway. Iteratethrough
1 N/2
thesymbolsinx,startingwithx whichmustbeanopenbracket. Aftertheopenbracketx ,ifthere
1 1
areoneormoreclosingbracketsx ,...,x beforeencounteringanotheropenbracketx ,then
2 K K+1
thesymbolsy ,...,y canbeconstructeddeterministicallyusingthefollowingmapping,
1 K

‘ ‘( [’
’
i if fx
xj+1
= =‘ ‘) ]’ ’f fo or rj
j
< <K K,
,
y = j+1
j ‘)’ ifx =‘(’forj =K,
‘]’ ifx1
=‘[’forj =K.
1
Inotherwords,thesymbolsy ,...,y willbetheopenbracketscorrespondingtotheclosing
1 K−1
brackets x ,...,x . After the symbol x , whenever you encounter another open bracket x
2 K 1 K+1
whereK >0,thenthesymboly willbetheclosingbracketcorrespondingtothesymbolx . Once
K 1
youhavematchedthefirstopenbracketsymbolx withaclosingbracket,youareboundtoencounter
1
anotheropenbracket. Followthesameprocessuntiltheendofthestringandonecanobtainthe
stringy,suchthat(x,y)∈Dyck-(2,2).
Hence, if x and y are the symbols in the odd and even indices of a string s ∈ Dyck-(2,2), then
placinganyotherstringy′ ̸=yintheevenindicesleadstoastringwhichisnotinDyck-(2,2). Our
foolingsetS containsallstringsoflengthN inthelanguageDyck-(2,2). Hence,byconstruction,
wehavethatf (s)=f (x,y)=1foralls=(x,y)∈S and
Dyck Dyck
f (x ,y )=f (x ,y )=0 forall (x ,y )̸=(x ,y )∈S.
Dyck 1 2 Dyck 2 1 1 1 2 2
Thus,suchasetS isafoolingsetbyDefinition1.
Sizeofthefoolingset. OnecanshowthatthetotalnumberofstringsoflengthN inthelanguage
Dyck-(2,2) is exactly 2N−1 with some elementary combinatorics. First, see that the number of
Dyck-2sequencesofdepthatmost1andlengthN isexactly2N/2. Thisisbecausethestringis
madeupofblocksof‘()’and‘[]’,andhencetherearetwochoicesforeveryblock. Then,considera
blockofDyck-2stringoflengthk(wherekiseven)whichstartsandendswithanopenandclosing
bracketrespectively. Moreover,theDyck-2stringhasadepthexactly2,e.g. ‘[()[]()]’. Notethat,
thetotalnumberofsuchstringsoflengthkisexactly2k/2 sincetherearetwochoicesforthefirst
bracketandthereisaDyck-2stringofdepth1andlengthk−2insideit. Forsimplicity,wewillcall
suchstringsasbelongingtothelanguageDyckb-(2,2)whichisasubsetofthelanguageDyck-(2,
2). Insimpleterms,stringsoflengthminthesubsetDyckb-(2,2)haveanoveralldepth2andhave
adepth1stringoflengthm−2betweenthefirstsymbol(openbracket)andthelastsymbol(closing
bracket);forinstance‘[()[]()()]’.
22ThetotalnumberofDyck-2stringsofdepthatmost2andlengthexactlyN canbecomputedas
follows. Partition the indices into k contiguous blocks where each partition is of an even length.
Supposetheindicesbeginwith1,thenthepointsofpartitioncouldbebetween2and3,4and5,and
soon. Forinstance,avalidpartitionoftheindices[1,2,...,8]into3blocksis[1,2],[3,...,6],and
[7,8]. ThetotalnumberofpartitionpointsisN/2−1. Foranysuchpartition,ifthelengthofablock
is2thenthatblockcanonlyhaveDyck-2stringsofdepth1andiftheblockisoflength≥4,then
considerallpossibilitiesofDyck-(2,2)stringsstartingandendingwithopenandclosingbrackets
respectivelyorinotherwords,stringsinDyckb-(2,2)describedearlier. Ifthenumberofpartitions
isN/2−1,thenallpossiblestringshavedepthatmost1andifthenumberofpartitionsis0,then
theentirestringisinDyckb-(2,2).
Seethat,nomatterhowyoupartitiontheinputs,thenumberofpossiblestringsateachblockoflength
mis2m/2. FurtherifP ={p ,...,p }denotesthesetoflengthsofeachblockinthepartition,then
1 k
k
thetotalnumberofstringswithsuchapartitionis(cid:89) 2pi/2 =2(cid:80)k i=1pi/2 =2N/2. Inotherwords,
i=1
regardlessofhowtheindicesarepartitionedifeachblockofsize> 2isrequiredtobeastringin
Dyckb-(2,2),thenthetotalnumberofpossiblestringsis2N/2.
ThetotalnumberofDyck-2stringsofdepthatmost2canbecomputedbyconsideringpartitionsof
allsizesk =0,...,N/2−1andallpossiblepartitionsforagivensize. Withthis,wegetthetotal
numberofvalidstringsinDyck-(2,2)oflengthN tobe
N (cid:88)2−1(cid:18)N −1(cid:19)
2 2N/2 =2N−1
k
k=0
Hencefromfact4,itfollowsthatthecommunicationcomplexityofDyck-(2,2)isatleastN −1.
Whilewehavegivenaself-containedproofbasedonfoolingsets,analternativeproofofLemma1
couldproceedusingvarietiesoffinitemonoids,byprovingthatthesyntacticmonoidofDyck-(2,2)
isnotinthevarietyDA,andthenapplyingtheresultofRaymondetal.[49].
Using Lemma 1 and Theorem 4, it follows that any 1-layer Transformer that can recognize
Dyck-(2,2)musthaveawidththatgrowslinearlywiththeinputlength.
Theorem10. Consideraone-layertransformerf ∈ TF1 decidingmembershipinDyck-(2,2).
d,p,H
ThendH ≥ N−1 .
3(p+logN)
Supposeweallowthetransitionfunctioninarecurrentmodeltobeanyarbitraryfunctionasdefined
inSection2. Inthatcase,itnaturallyfollowsthatsuchanRNNcanrepresentanyDFAwithkstates
withahiddenstateofsizelogk. AnyboundeddycklanguageDyck-(n,k)canberepresentedby
aDFAwithO(2k)stateswhichisindependentoftheinputlengthN. Inparticular,thelanguage
Dyck-(2,2)consideredherecanberepresentedbyaDFAwithjust7states. Hence,itfollowsthat
arecurrentmodelofconstantsizecanrepresenttheDyck-(2,2)languageforarbitrarylengthsin
afiniteprecisionsetting. Additionally,priorworkshavedescribedconstructionsofsuchrecurrent
modelswithpracticalactivationfunctionssuchasSigmoid[25]andReLU[6]. Thus,thebounded
DycklanguageDyck-(2,2)providesusaseparationbetweenone-layerTransformerandrecurrent
modelssinceconstant-sizedRNNscanrepresentthemwhereasone-layerTransformersmusthaveat
leastlinearwidthtorepresentthem.
F TransformersandBooleanfunctions
Communicationprotocolsandlowerbounds. Therearesomenotabledifferencesbetweenthe
typesofcommunicationcomplexitylowerboundswehaveforone-layerTransformers(Theorem4)
and for RNNs (Theorem 2). If an RNN can compute a function f then Theorem 2 implies that
thereexistsaone-waycommunicationprotocolwithmpbitsoveranycontiguouspartitionsofthe
input. Ontheotherhand, ifaone-layerTransformercancomputeafunctionf, thenTheorem4
impliestheexistenceofacommunicationprotocolwithO(mpHlogN)bitsoveranypartitionofthe
23input—butnotone-way. Hence,thetypesoflowerboundsthatapplytothesetwoarchitecturesare
different. Foranyfunctionf suchastheINDEXproblem,iftheone-waycommunicationcomplexity
islowerboundedbyΩ(N)oversomecontiguouspartitionsthenitimpliesthatforRNNs,thewidth
m = Ω(N/p). However since the two-way communication complexity of such problems might
belower,thoselowerboundsneednotapplytoone-layerTransformers. Forinstance,theINDEX
problemcanbesolvedwithlogN bitsofcommunicationiftwo-waycommunicationisallowedorin
otherwords,BobisallowedtosendbitstoAlice.Conversely,toprovelowerboundsforTransformers
computingacertainfunctionf,provingacommunicationcomplexitylowerboundforf overany
partitionofinputssuffices. Foraparticularpartitioningofinputs,weshowedalowerboundonthe
communicationcomplexityofboundedDycklanguages. Whilethatresultimpliesalowerbound
onone-layerTransformers,thepartitioningisnotcontiguousandhencedoesnotapplytorecurrent
models. ForDyck-(2,2),contiguouspartitionsarenotahardcase,andinfact,communicating≤2
openbracketsfromthefirsthalfissufficient. ThisiswhythelowerboundofLemma1doesnot
applytoRNNs. Despitethesedifferences,foralargeclassofBooleanfunctionsincludingfunctions
suchasEqualityandDisjointness,thelowerboundsapplytobothofthesearchitectures.
Equality. Forconvenience,wediscussthecomplementoftheEqualityfunctionINEQ=1−EQ(x)
which is the inequality function. It does not influence any of our results or the constructions for
Transformers but it helps make the intuitions for the construction clearer. The function INEQ :
{0,1}N →{0,1}isaBooleanfunctiondefinedas,
INEQ(x)=I[(x ,...,x )̸=(x ,...x )]
1 N/2 N/2+1 N
Thiscanalsoberepresentedasa2-DNFwithN/2terms,
INEQ(x)=(x ∧¬x )∨(¬x ∧x )∨...∨(x ∧¬x )∨(¬x ∧x )
1 N/2+1 1 N/2+1 N/2 N N/2 N
F.1 LowerBounds
WeshowthatforanyRNNthatcomputestheINEQfunctionover{0,1}N withahiddenstateof
sizemandpbitsofprecision,itisnecessarythatmp=Ω(N). Inotherwords,thesizeofthehidden
stateoftheRNNgrowslinearlywiththeinputlengthN. Similartootherresults,thiswillbebased
oncommunicationcomplexitybutwealsoprovideanalternatewaytoprovethislowerboundbased
onthestatecomplexityofDFAsthatcomputetheINEQfunction.
TheresultstatesthatifanyRNNwithahiddenstateofsizemoverp-bitprecisioncansimulateaDFA
withnstates,thentherepresentationsizeofthehiddenstatempmustbeatleastlogn. Thereisone
caveatrelatedtothisDFA-basedresultwhichdoesnotapplytothecommunicationcomplexity-based
lowerbounds. FortheDFA-basedresult,whilethetransitionfunctionisallowedtobeanarbitrary
function, it has to be of the form g(x ,h ),i.e., the functions cannot be different based on the
t t−1
timestept. Itisuncleariftheresultstillapplieswhenthefunctionisallowedtobedifferentbased
onthetimestep. However,sincethetransitionfunctionisallowedtobeanyarbitraryfunction,it
stillcapturesalltherecurrentandstate-spacearchitecturesusedinpractice. Additionally,thisfairly
simpletechniquecouldbeusedtoprovelowerboundsforRNNsbasedonlowerboundsinformal
languagetheory.
Lemma4. LetAbeaDFAwithnstates. LetRbeanRNNsuchthatforallx∈Σ∗,A(x)=R(x).
ThenthesizeofthehiddenstateoftheRNN:mp≥logn.
Proof. Suppose an RNN R exists such that the dimension of its hidden state m < logn. It is
straightforwardtoseethatwecanconstructanotherDFAA′fromtheRNNRsuchthattheDFAA′
acceptsthesamelanguageasAbutithas2mp <nstates.
Createastateforeachvectorh∈{0,1}mp andassignthestatecorrespondingtothevectorh as
0
thestartstate. Foreachstateqcorrespondingtovectorh,makeqafinalstateiff(h)=1. Foreach
h ∈ {0,1}mp andeachsymbolx ∈ Σ,computeh′ = g(x,h). Addatransitionbetweenthestate
correspondingtohandh′usingthesymbolx. Hence,wegettheentireDFAwiththetransitionmap
andthesetofstartandfinalstatesthatacceptthesamelanguage.
SincewearegiventhattheautomataAistheminimum-sizedDFAthatacceptsthelanguage,the
automataA′ havingfewerstatesandacceptingthesamelanguageisacontradiction. Hence,such
anRNNcannotexistandthedimensionofthehiddenstatevectormoftheRNNmustbeatleast
logn.
24Theorem11. Anyrecurrentmodelwithahiddenstateofsizemoverp-bitsofprecisionthatcomputes
theINEQ(x)forallx∈{0,1}N musthavemp≥N/2.
Proof. Weshowtwowaystoprovetheabovestatement.Thefirstproofisbasedonthecommunication
complexityoftheEqualityproblem.
Proof 1. Suppose Alice and Bob have the input vectors a,b ∈ {0,1}N/2 respectively and have
access to an RNN that computes the INEQ function over {0,1}N. Alice can first use the RNN
startingwithinputa andthevectorh anditerativelyupdatethehiddenstateuntiltheinputa .
1 0 N/2
Alicecanthensendthevectorh toBobwhocanprovideb ,...,b asinputsandcompute
N/2 1 N/2
y =INEQ(a·b)=1−EQ(a,b). Sincesendingthehiddenstatevectorh requiresmpbits,it
N/2
mustbethatmp≥N/2duetoTheorem2.
Proof2. ThesecondproofusesthefollowingrelationbetweenrecurrentmodelsandaDFA.
Let A be the minimum-sized DFA that computes the INEQ function over {0,1}N. When
INEQ
wesayaDFAcomputesINEQfunctionover{0,1}N,wemeanthatA (x) = INEQ(x)for
INEQ
allx∈{0,1}N andA (x)canbedefinedarbitrarilyforx∈/ {0,1}N toobtaintheDFAwith
INEQ
minimumnumberofstates.
Notethat,anyDFAthatagreeswithINEQoverallxin{0,1}N musthaveatleast2N/2states. This
followsfromthefactthatforanytwodistinctx ,x ∈ {0,1}N/2 thereisadistinguishingsuffix
1 2
s∈{0,1}N/2suchthatINEQ(x ·s)̸=INEQ(x ·s). Hence,anyDFAthatagreeswithINEQon
1 2
allinputsin{0,1}N musthaveatleast2N/2statesevenifitisdefinedarbitrarilyoverotherinputs
x∈/ {0,1}N. FromLemma4,itfollowsthatmp≥N/2.
Theorem 12. Any one-layer Transformer with a width m and H heads operating over p-bits of
precisionthatcomputestheINEQ(x)forallx∈{0,1}N musthavempH =Ω(N).
ThestatementimmediatelyfollowsfromTheorem4andFact2.
F.2 TransformerConstructionforEquality
Wenowshowhowalog-sized2-layerTransformeroperatingoverlog-precisionnumberscancompute
theINEQfunctionoverallBooleaninputs.
Theorem6. ForanyN ∈ N, thereexistsa2-layerTransformerf ∈ TF2 wherewidthm =
m,p,2
O(logN)andprecisionp=O(logN)suchthatf(x)=EQ(x)forallx∈{0,1}N.
Wefirstdescribethebroadideabehindtheconstruction. Wewillconsidertheinputdomaintobe
{−1,1}N andourgoalistocomputeINEQ(x)forallx∈{−1,1}N. Thiscanalsobeformulated
as,
INEQ(x)=(x ⊕x )∨(x ⊕x )∨...∨(x ⊕x )
1 N+1 2 N+2 N N
2 2 2
Letf(1) denotethefirstlayerofourconstructionf thatcomputesINEQandletf(1)(x) denote
ineq ineq ineq i
theithoutputvectorofthefirstlayer. Ourconstructionwillbesuchthatfori>N/2,ontheithinput
theattentionmechanismatthefirstlayerwillretrievethe(i−N/2)thinputusingtoolsdescribed
inSectionB.3. WiththeMLP,thefirstlayerf(1)(x) willcomputex ⊕x foreachi > N/2
ineq i i i−N
2
wherex ⊕x =0ifx =x andis1otherwise. ThesecondlayerwillthentakeanORover
i i−N i i−N
2 2
allthosevalueswhichwillresultincomputingINEQ(x).
Proof. Let x = (x ,...,x ) denote our input vector. The input to the Transformer model will
1 N
includethepositionsaswellx˜=((x ,1),...,(x ,N)). Letx ∈Rddenotetheembeddingofthe
1 N i
ithinputx˜ whered = 2+2k. Here,k = O(logN)isthedimensionofvectorsT(1),...,T(N)
i
describedinCorollary8.1. Theinputembeddingswillcontainfourpartsandwillbeofthefollowing
form
(cid:40)
[x ,0,T(i),T(i),1] ifi≤N/2,
x = i
i [x ,0,T(i),T(i− N),1] otherwise.
i 2
25ThequeryvectorsQ(x )=x W ∈willbethelastpartoftheembeddings,i.e.,Q(x )=[T(i),1]
i i Q i
fori≤N/2andisT(i−N/2)fori>N/2. Similarly,thekeyvectorsK(x )=[T(i),−1/2]for
i
alli. Thevaluevectorwillbead-dimensionalvectorV(x )=[0,x ,0 ,0 ]. Thequery,key,and
i i k k
valuetransformationscanbeimplementedwithblockmatricescontainingzeroandidentitymatrices
similartotheonedescribedinSectionC.2.
Byconstruction,thedotproductsA =⟨Q(x ),K(x )⟩=⟨T(i−N/2),T(j)⟩−1/2willbesuch
i,j i j
thatfori>N/2,
(cid:40)
≥1/4 ifj =i− N,
A =⟨Q(x ),K(x )⟩ 2
i,j i j ≤−1/4 otherwise.
For i < N/2, the dot products A will be greater than 1/4 if i = j and will be less than
i,j
−1/4 for i ̸= j. If this was a Transformer with the hard-attention mechanism, then, note that
Att(X) = [0,x ,0 ,0 ] for i ≤ N/2 and Att(X) = [0,x ,0 ,0 ] for i > N/2. With
i i k k i i−N/2 k k
residual connections or another attention-head, the output of the attention block will include the
originalinputaswellwhichwillleadto
(cid:40)
[x ,x ,T(i),T(i),1] fori≤N/2,
Att(X) +x = i i
i i [x ,x ,T(i),T(i),1] fori>N/2.
i i−N/2
ThenasimpleReLUFFNcancomputetheXORofthefirsttwovaluesoftheoutputvectorfrom
the attention block. Hence, by construction, the output vector from the first layer f(1)(x) =
ineq i
[x ⊕x ,...]fori>N/2and[0,...]fori≤N/2.
i i−N/2
SecondlayercomputingOR.ThesecondlayeroftheTransformerwillcomputetheORoverthe
firstcoordinateoftheinputvectorwhichisquitestraightforward. LetthequeryvectorQ(x(1))=0.
i
ThevaluevectorswillbeoftheformV(x(1))=[(x ⊕x ),0,...,0]fori>N/2andtheywill
i i i−N/2
bezerovectorsconstructionfori≤N/2. Then,regardlessofthekeys,thedotproductswillbe0for
eachposition,andhence
N
Att(X)(2) = 1 (cid:88) (x ⊕x ).
N N i i−N/2
i=N/2
IfAtt(X)(2) ≥ 1,thentheFFNcanoutput1anditcanoutput0otherwise.
N N
Softmaxattention. Wenowdescribehowtoretrievethex valueswithsoftmaxattentionin
i−N/2
thefirstlayerofthemodel. TheapproachisslightlydifferentfromtheoneusedinSectionC.2and
makesuseoffiniteprecisionrounding.
Consideranotherconstructionthatisidenticaltotheconstructiondescribedabovewiththeexception
that Q˜(x ) = ηQ(x ) = η[T(i),1]. We show that for large enough η and i > N/2, the weight
i i
softmax(A) willbesocloseto1forj =i−N/2thatitwillberoundedto1. Similarly,itwillbe
i,j
roundedto0forj ̸=i−N/2. Fori≤N/2,theweightwillberoundedto1fori=j andwillbe
roundedto0otherwise.
Recall that the finite precision implementation is parameterized by a large constant K (c.f. Ap-
c
pendixB.2). Fori>N/2andj =i−N/2,thereexistsanηsuchthat,
(cid:12) (cid:12)
(cid:12) exp(η⟨Q(x ),K(x )⟩) (cid:12) 1
(cid:12)1− i j (cid:12)≤ .
(cid:12) (cid:12) (cid:80)N k=1exp(η⟨Q(x i),K(x k)⟩)(cid:12) (cid:12) 2NKc
Forsuchanη,theweightsoftmax(A) willberoundedto1.
i,j
exp(η⟨Q(x ),K(x )⟩) exp(1η) 1
1≥ i j ≥ 4 ≥1−
(cid:80)N exp(η⟨Q(x ),K(x )⟩) exp(1η)+(N −1)exp(−1η) 2NKc
k=1 i k 4 4
1 1 1
=⇒ 2NKcexp( η)≥(2NKc −1)(exp( η)+(N −1)exp(− η))
4 4 4
26η η
=⇒ exp( )≥(N −1)(2NKc −1)exp(− )
4 4
(cid:16) (cid:17)
=⇒ η ≥2log (N −1)(2NKc −1)
Thus,forη =logN +K log2N,thesoftmaxattentionweightatj =i−N/2willberoundedto1.
c
Similarly,onemayverifythatifη ≥K log2N,thentheweightsoftmax(A) forj ̸=i−N/2will
c i,j
belessthan1/2NKc andhencewillberoundedto0.Hence,forη ≥logN+K clog2N,thesoftmax
attentionwillbehavelikehardattention,andasdescribedearliertheTransformerwillcomputethe
INEQfunction. Thiscompletestheproof.
Thescalingintheattentionmechanisminthelastpartcanalsobeimplementedinalmostexactlythe
samewayastheconstructionforTheorem1. ImplementingitthatwaythenrequirestheReLUFFN
toactasathresholdfunction. Thescalingdescribedabovemakesuseofthefiniteprecisionsettingto
amplifythedotproductstothepointthatitactsashardattention.
F.3 RepresentingmoregeneralclassofBooleanfunctions
We now describe how the construction for Equality in Section F.2 can be extended to a class of
Booleanfunctionsnamely,thresholdsofatmostN k-SPARSEfeatures. Bythresholdfunctions,we
meanfunctionsoftheformTh :{0,1}n →{0,1}whereforx∈{0,1}n,thefunctionisdefinedas
b
Th (x) =
I[(cid:80)n
x −b > 0]. Thefunctionisparameterizedbyaconstantb. Forb = n−1,the
b i=1 i
functioneffectivelyistheANDoverallbits. Similarly,forb = n/2,thefunctionoutputs1ifthe
majorityoftheinputbitsare1andoutputs0otherwise.
Ak-SPARSE functionissimplyafunctionwhoseoutputonanyinputx ∈ {0,1}n dependsonat
mostk indicesoftheinput. Moreformally,afunctionf : {0,1}n → {0,1}isk-SPARSE ifthere
existindices1≤i <i <...<i ≤nandafunctiong :{0,1}k →{0,1},suchthatforevery
1 2 k
x ∈ {0,1}n,f(x 1,x 2,...,x n) = g(x i1,x i2,...,x ik). Ak-SPARSE functioncanbeanyBoolean
function(AND,OR,XOR,etc)withtheconstraintthatitcandependonatmostk =O(1)bitsof
input.
WenowdefinetheclassofthresholdofatmostN k-SPARSEfeaturesdenotedasTHRESk,N.Letg
I
be
ak-SPARSEfunctiondependingonI ⊂[N]indiceswhere|I|≤k. Afunctionf :{0,1}N →{0,1}
isintheclass THRESk,N ifitisoftheformf(x) = Th b(g I1(x),g I2(x),...,g IN(x))whereeach
I ⊂[N]and|I |≤kforallj ∈[N]. Furtherg (x)=0forallx∈{0,1}N ifthesetI =∅.
j j Ij j
ThefollowingresultstatesthatforanyfunctionhintheclassofthresholdofatmostN k-SPARSE
features,thereexistsatwo-layerTransformerwithlogarithmicwidththatcanexpressthefunctionh.
Theorem13. ForanyN ∈Nandanyfunctionh∈ THRESk,N,thereexistsa2-layerTransformer
f ∈ TF2 withwidthm = O(logN),H = k heads,andprecisionp = O(logN)suchthat
TF m,p,k
f (x)=h(x)forallx∈{0,1}N.
TF
Proof. TheresultfollowsfromastraightforwardextensionoftheconstructioninTheorem6. Wewill
againremapthedomainto{−1,1}N.Forthisproblem,wewillalsoprependtheinputx∈{−1,1}N
withanadditionalbeginningofsequencetoken[BOS]. Hence,theTransformerwillreceiveN +1
tokensx ,x ,...,x .
0 1 N
For any function h ∈ THRESk,N, the first layer of the Transformer will compute the k-SPARSE
features,g (x),g (x),...,g (x). ThesecondlayerwillthencomputethethresholdfunctionTh
I1 I2 IN b
overthek-SPARSEfeatures.
ComputingThreshold. Thesecondlayerisquitetrivialtoconstruct. Letx(1),x(0),...,x(1) be
1 1 N
theoutputvectorsofthefirstlayerandinputstothesecondlayer. Supposetheithoutputvector
contains the ith k-SPARSE feature, x( i1) = [g I1(x),...] for i = 1,...,N and x( 01) = 0. If the
query transformation is a null matrix, Q(x(1)) = 0, and the value transformation is such that
N
V(x(1))=[g (x)],itfollowsthattheoutputoftheattentionblockwillbe 1 (cid:80)N g (x). The
i I1 N+1 i=1 Ii
27ReLU-basedfeedforwardnetworkcanthenbeusedtosubtractwithaconstant b andimplementa
N+1
thresholdfunctiontocomputethedesiredoutput.
Computingk-SPARSEfeatures. Inthefirstlayer,wecomputethek-SPARSEfeaturesbymaking
useofthealmostorthogonalpositionalvectorsT(0),T(1),...,T(N)ofsizer =O(logN). From
theconstructionforequality,itshouldbeclearthatusingoneattentionhead,onecanattendtoany
desiredpositionandretrieveasinglebit. Extendingthat,usingkheads,wecanretrievekbitsfromk
differentpositions. Tocomputeg (x),wewillhaveinputembeddingsofsize1+(k+1)r. Forthe
Ii
ithinputx ,thefirstcoordinatewillcontaintheinputx ∈{−1,1}andwillcontain0forthe[BOS]
i i
tokenx . ThenextrindiceswillcontainthepositionalvectorT(i)correspondingtothepositioni.
0
Theremainingkrindicescanbedividedintokblockseachofwhichwillcontainapositionalvector.
IfthesetI containsindicesI1,...,Ik,thenthekblockswillcontainvectorsT(I1),...,T(Ik). If
i i i i i
thesetI haslessthankindices,thenthelastk−|I |blockswillhavethepositionalvectorsT(0).
i i
Seethatiftheinputembeddingsaredesignedasdescribedabovethenonecanobtaininputbitsx
Ii
intheoutputoftheattentionblockattheithtoken. Thevaluevectorswillbeofkdimensionsuch
that V (x ) = x at the coordinate h and is 0 everywhere else. The key transformation for each
h i i
headcanbeK(x )=[T(i)]containingthevectorcorrespondingtothetoken’sposition. Thequery
i
transformationisdistinctforeachhead,andforthejthhead,thequeryvectorcontainsjthblock
fromthelastkblockscontainingthepositionalvectorsforeachindexinthesetI . Ifthequeryand
i
keytransformationsaredesignedinsuchaway,thenusingtheargumentsdescribedinTheorem6,it
canbeseenthattheoutputoftheattentionblockattheithpositionwillbeakdimensionalvector
containing[x ,...,x ]. IfthesetI hasasizelessthank,thentheoutputvectorwillbefollowed
I1 Ik i
i i
byzerosafter|I |coordinatesandwillbeazerovectorifI =∅. Finally,thefeedforwardnetwork
i i
canthencomputethefunctiong (x)ateverypositionwhichcanbedonebyanetworkofconstant
Ii
sizebecausek =O(1)doesnotdependonN. Thesecondlayercanthencomputethethresholdover
thek-SPARSEfeaturesasdescribedearlierwhichleadstothedesiredoutput.
Discussion. Theclassofthresholdsofk-SPARSEfeaturescontainscertainfunctionsofinterestsuch
as Disjointness and Equality as well as more general classes such as k-DNFs and k-CNFs with
atmostN termsorclauses. Asdescribedearlier,the(In)Equalityfunctioncanbealsodefinedas
INEQ(x) = (x
1
⊕x N+1)∨...∨(x
N
⊕x N). Seethatitcontains N
2
2-SPARSE featureswhere
2 2
thesetofindicesI = {i,i+ N}fori = 1,...,N/2andthefeaturefunctiong(a,b) = a⊕bfor
i 2
a,b∈{0,1}. Similarly,theDisjointnessfunctioncanberepresentedasa2-CNF.Thecomplement
oftheDisjointnessfunctioncanbedescribedmoresimplyas
(x ∧x )∨(x ∧x )∨...∨(x ∧x )
1 N+1 2 N+2 N N
2 2 2
whichisa2-DNFthatoutputs0ifthefirstandsecondhalfoftheinputx∈{0,1}N aredisjointand
outputs1otherwise. Thus,itfollowsfromTheorem13thattwo-layerTransformerswithlogarithmic
widthcanrepresentfunctionssuchasDisjointnessaswellask-DNFsandk-CNFswithatmostN
termsorclauses.
F.4 DifficultyofDerivingCommunication-basedLowerBoundsfor2-layerTransformers
Our lower bounds both for RNNs and for one-layer transformers are based on communication
complexityarguments. Here,weprovideevidencethatothertechniquesmaybeneededtoestablish
lowerboundsfortwo-layertransformersbyshowingthat,inacertainsense,noshortcommunication
protocol of the same kind as Theorem 4 can exist for Alice and Bob to obtain the output of any
two-layertransformer. Westartfromthefollowinglemma:
Lemma5. AssumeN iseven. ForeverypartitionS ,S of{1,...,N}where|S |=|S |,there
A B A B
is a 2-layer transformer f ∈ TF2 with width m = O(logN) with precision p = O(logN)
d,p,2
withf(x) ∈ {0,1}foreachx ∈ {0,1}N, suchthatAliceandBob, havingaccesstox andx
A B
respectively,needtoexchange≥ N bitstocomputef(x).
2
Proof. For any partitioning S ,S , consider the task of determining whether x and x are
A B SA SB
identical. Thatis,define(forx∈{0,1}N):
(cid:40)
1 ifx ̸=x
f (x)= SA SB (6)
SA,SB 0 else
28If Alice and Bob have access to x and x , respectively, they need to exchange ≥ N bits
SA SB 2
to compute f(x). Now by Theorem 6, there is a transformer f ∈ TF2 that computes
ineq m,p,2
f ,wherem = O(logN)andp = O(logN). Nowrenumberingthepositional
[1,...,n/2],[n/2+1,...,n]
encodingsresultsinatransformercomputingf .
SA,SB
Corollary13.1. IfthereisanycommunicationprotocolbywhichAliceandBobcancomputethe
outputforanytwo-layerTransformer,foranypartition,withO(mpHg(n))bits,thenitmustbethe
(cid:16) (cid:17)
casethatg(n)=Ω N .
(logN)2
Proof. Supposethereisacommunicationprotocolsuchthatforanytwo-layerTransformerandover
anypartition,theprotocolcancomputetheoutputoftheTransformerwith
(cid:18) (cid:19)
N
o mpH (7)
(logN)2
bits. ButbyLemma5,foreachpartition,thereexistsatwo-layerTransformerwith
mpH =O((logN)2) (8)
suchthatAliceandBobneedtoexchange≥N/2bitstocomputeitsoutput.Weobtainacontradiction.
G NearestNeighborsandAssociativeRecall
RecallfromSection5.2thatintheNSTNBtask,amodelisprovidedwithasequenceofvectorsand
labels(x ,y ,...,x ,y ,x )whereN/2<k ≤N andthegoalofthemodelistopredictthe
1 1 k−1 k−1 k
labelcorrespondingtothenearestneighborofx foreachk = N +1,...,N. Wefirstshowthat
k 2
anyrecurrentmodelthatperformsthistaskmusthaveawidthorhiddenstateofsizeΩ(N).
p
RelationtoAssociativeRecall. Thenearestneighbortaskiscloselyrelatedtothesingleandmulti-
queryassociativerecall(MQAR)taskintroducedinBaetal.[3]andAroraetal.[2]. Intheassociative
recalltask,amodelreceivesasequences ,y ,...,s ,y ,s wherethesymbolss belongto
1 1 k−1 k−1 k i
analphabet|Σ|. Assumethesymbolsthes ,...,s tobedistinctandthelabelsy ∈{0,1}for
1 k−1 i
simplicity. Thequeryinputs isarepetitionofoneoftheprecedinginputs. Thegoalofthemodelis
k
topredictthelabelofthequeryinputs byfindingtheexactmatchfromthecontextandproducing
k
the corresponding label. A model that can compute the nearest neighbor algorithm can perform
theseassociativerecalltasksbyembeddingthesequenceofsymbolss ,y ,...,s ,y ,s asa
1 1 k−1 k−1 k
sequenceofvectors(x ,y ,...,x ,y ,x ).
1 1 k−1 k−1 k
Ifamodelreceivesasequence(x ,y ,...,x ,y ,x )wherethevectorsx ,x ,...,x are
1 1 k−1 k−1 k 1 2 k−1
distinctandthequeryvectorx isarepetitionthenthetaskofapplyingnearestneighbortopredictthe
k
labelforthequeryvectorreducestothesinglequeryassociativerecalltask. Implementingthenearest
neighboralgorithmisequivalenttofindingtheexactmatchforthequeryvectorx andproducingthe
k
labely correspondingtothatinput. Inthecase,wherethemodelsarerequiredtopredictiteratively
k
forallx
k
fork = N
2
+1,...,N,thetaskbecomesequivalenttoMQAR.
The MQAR taskis asimplifiedversionor asubsetof thenearestneighbor taskwhere the model
is first provided with the prompt (x ,y ,...,x ,y ) and the subsequent N/2 input vectors
1 1 N/2 N/2
are a permutation of the first N/2 vectors. In other words, for N/2 < k ≤ N and a sequence
(x ,y ,...,x ,y ,x ),themodelhastofindtheexactmatchofx inthefirstN/2vectors
1 1 k−1 k−1 k k
andoutputthecorrespondinglabel. Itisstraightforwardtoseethatanymodelthatcanperformthe
nearestneighbortaskcanalsoperformtheMQARtask. Assumethesizeofthealphabet|Σ|=Θ(N)
Theembeddingofeachsymbols
i
∈ Σcanbeadistinctvectorfromthehypercube{−√1 ,√1 }d
d d
where d = ⌈log|Σ|⌉ = O(logN). The embeddings can be thought of as the normalized binary
encodingsofeachsymbol.Thus,amodelreceivesasequenceofvectors(x ,y ,...,x ,y ,x )
1 1 k−1 k−1 k
correspondingtoasequenceofsymbolss ,y ,...,s ,y ,s . Sincethequeryvectorsx for
1 1 k−1 k−1 k k
k > N/2arerepetitions, thereisanexactmatchforeachoftheminx ,...,x . Supposethe
1 N/2
exactmatchforaqueryx =x ,thentheirdotproducts⟨x ,x ⟩=1andthedotproductwith
k j∗ k j∗
29everyothervector⟨x ,x ⟩≤1− 1 . Sincethemarginbetweenthedotproductwiththenearest
k i logN
neighborandothervectorssatisfy 1 =O(logN)andalltheinputembeddingshaveunitnorms,the
γ
problemsatisfiestheassumptionsofthenearestneighbortask.
G.1 LowerBoundsforRecurrentModels
Theorem8. Anyrecurrentmodelwithahiddenstateofwidthmwithp-bitsofprecisionthatcan
performthenearestneighbortaskforallinputsoflengthN musthavem≥N/2p.
Proof. Theproofisviaareductionfromthedisjointnessproblem. Weshowthatthelowerbound
istrueevenfortherestrictedproblemofmulti-queryassociativerecall(MQAR). Recallthatinthe
MQARtaskthesequenceofqueryinputsx N+1,...,x
N
isapermutationofthesequenceoflabelled
vectorsx ,...,x . 2
1 N/2
IfthereexistsarecurrentmodelRthatcansolvetheMQARtask,thenweshowthatAliceandBob
canuseittofollowacommunicationprotocolandcomputetheDISJfunction.
Thecommunicationprotocolisasfollows. AliceandBobhavetwoBooleanvectorsa,b∈{0,1}N/2
respectively. Both of them know the description of the recurrent model R. Alice and Bob have
decided on a set of N/2 vectors v ,...,v in Sd−1 that they will use in the communication
1 N/2
protocol. ChoosinganyN/2distinctvectorsfromthehypercube{−√1 ,√1 }dwillsufficeandalso
d d
satisfytheassumptionsmentionedinSection5.2.
Toreiterate,bothAliceandBobhavethemodelparameters/descriptionandhavedecidedonaset
ofN/2unitvectorsasapartoftheircommunicationprotocol. Alicethenusestherecurrentmodel
Randprovidesthesequenceofpairs(v ,a )inanyarbitraryorder. Alicethensendsthehidden
i i
statevectorh toBob. BobthenusesthemodelRanditerativelyprovidesv vectorsalongwith
N i
thegeneratedoutputinanyorder. Bobknowsthattheoutputproducedbytherecurrentmodelfor
the vector v corresponds to the value a . Hence, Bob obtains the entire vector a and computes
i i
DISJ(a,b).
SinceAlicesentthehiddenstatevector,whichrequiresmpbits,andtheycouldcomputeDISJ(a,b)
over{0,1}N/2byexchangingmpbits,byFact1wehavethatmp≥N/2.
G.2 TransformerConstructionforNearestNeighbor
Wenowshowhowalog-sized2-layerTransformeroperatingoverlog-precisionnumberscancompute
theNSTNBfunctionoverallinputsthatsatisfytheconstraintsdescribedinSection5.2. Recallthe
assumptionsthat
• Norm. Allvectorsx haveunitnorms.
i
• Margin. For any N/2 < k ≤ N, and let j∗ = argmax xTx , then xTx ≥
i∈[k−1] k i k j∗
xTx +γ foranyi̸=j∗.
k i
Theorem 7. For any N ∈ N, there exists a 2-layer Transformer f ∈ TF2 with width
NN m,p,2
m = O(logN) and precision p = O(logN) such that f computes the nearest-neighbor task
NN
allsequencesoflengthatmostN satisfyingtheassumptionsabove.
ForN/2 < k ≤ N,themodelwillbeprovidedthesequence(x ,y ,...,x ,y ,x )where
1 1 k−1 k−1 k
x ∈ Rd′ vectorscontaintheinputpointsandy scontainthelabels. Forclarity, wewillreferto
i i
the embedding of all inputs as z ∈ Rd where z = ϕ(x ,2i−1) is the embedding of input
i 2i−1 i
pointsfori = 1,...,k. Similarly,thevectorsz = ϕ(y ,2i)willcontaintheembeddingforthe
2i i
correspondinglabels. Hence,technicallythesequenceofinputvectorstotheTransformermodelwill
be(z ,z ,...,z ).
1 2 2k−1
OverviewofIdea. Theexplicitconstructionisabittediousbutthekeyideasarestraightforwardto
understand. Theconstructionitselfdoesnotrelyoninputsequencesoffixedlengths,unliketheone
fortheEqualityproblem.
Recall that for an input sequence (z ,z ,...,z ), the odd indices (2i−1) correspond to the
1 2 2k−1
inputsx andtheevenindices(2i)containthelabelinformationy . Thefinalinputz contains
i i 2k−1
30thequeryinputx andthegoalofthemodelistofindthenearestneighborinputx andoutputthe
k j∗
labelcorrespondingtothaty .
j∗
Intuitively, atwo-layerTransformercandothatinthefollowingway: thefirstlayercanfindthe
nearestneighborx andretrievethepositionofthelabely . Thesecondlayercanthenattendover
j∗ j∗
thatpositionandproducethedesiredlabel.
Challenges. Thereareafewchallengestoexecutingthisstrategywhichourconstructionwilladdress.
Iftheinputembeddingswereidenticaltotheinputvectors,thenx wouldhavemaximumdotproduct
k
with itself and not with its nearest neighbor x . Second, if you remove that, even then the dot
j∗
productwithsomelabelvectorcouldbelargerthanthedotproductwiththenearestneighbor(which
couldbecloseto−1). Lastly,supposebydesignyouhavethemaximumdotproductwiththedesired
input,youstillneedtoretrievetherequiredinformationinausefulwaysinceweareworkingwith
softmaxattentionwhichwillalsoputweightoverotherinputs.
Key ideas. One can think of the vectors T(1),...,T(N) as some form of positional or address
vectorsofdimensionO(logN). Allvectorsz correspondingtoinputsx willhavetheaddress
2i−1 i
vectors of their next position T(2i). Along with that, all the vectors z will also have their own
i
address/positionvectorsT(i). Ifinthefirstlayer,themodelcanattendoverthevectorz witha
2j∗−1
highattentionweight,thenitwillbeabletoretrievetherequiredaddressvectorT(2j∗)whichitcan
thenuseinthesecondlayertoretrievetherequiredlabel.
Theinputembeddingsaredesignedinsuchawaythatthemaximumdotproductwillbewiththe
desiredinputvectorx ormorespecificallyz . Theembeddingsaresuchthatthedotproducts
j∗ 2j∗−1
betweenthequeryvectorofinputz andthekeyvectorsofallotherinputvectorsz willbeof
2k−1 i
thefollowingform,
⟨Q(z ),K(z )⟩=⟨x ,x ⟩−c ⟨T(2k−1),T(2i−1)⟩ fori=1,...,k.
2k−1 2i−1 k i 1
Seethatfori̸=k,⟨Q(z ),K(z )⟩≈⟨x ,x ⟩whereasfori=k,⟨Q(z ),K(z )⟩<
2k−1 2i−1 k i 2k−1 2i−1
−2ormuchsmallerbasedontheconstantc . Hence,attentionweightwillbesmalleronthequery
1
inputitselfcomparedtootherinputs.
Secondly,fori=1,...,k−1,thedotproductswiththelabelvectorswillbeofthefollowingform,
⟨Q(z ),K(z )⟩=⟨x ,0⟩−c ⟨T(2k−1),T(2i−1)⟩−c ≈−c fori=1,...,k.
2k−1 2i k 1 2 2
whichwillbesmalldependingontheconstantc . Hence,thedotproductwillbemaximumwith
2
inputwiththenearestneighborx withamarginΩ(γ).
j∗
ToretrieveanduseT(2j∗)inthenextlayer,wedonotneedtohard-attendonz . Sincethe
2j∗−1
addressvectorsareoftheformT(i)∈{−√1 ,√1 }k,weonlyretrievethecorrespondingsignvectors
√ k k
T (i)= kT(i)∈{−1,1}k. Sincetheattentionweightwillbemaximumonz withamargin,
s 2j∗−1
wecanscalethequeryvectorstoincreasetheweighttoasufficientvalueandthenuseReLUasa
formofthresholdtoobtainT (2j∗). Thesecondlayeristhenstraightforwardandwillretrievethe
s
desiredlabel.
Proof. WewillnowdescribetheexplicitconstructionforTransformerstocomputenearestneighbors.
InputEmbeddings. Theembeddingvectorsaredefinedinthefollowingway,
ϕ(x ,2i−1)=z =[x ,0,1,T(2i−1),T(2i),0 ] (9)
i 2i−1 i k
ϕ(y ,2i)=z =[0 ,y ,−2,T(2i),T(2i),0 ].
i 2i d′ i k
Here,thevectorsT(i)∈{−√1 ,√1 }k areJLtransformationsasdescribedinSectionB.3andhence
k k
k =O(logN). Thedimensionoftheembeddingvectorsd=3k+d′+2=O(logN). Thevectors
T(1),...,T(2N)aresuchthat,
(cid:40)
1±γ/100 ifi=j,
⟨T(i),T(j)⟩=
0±γ/100 otherwise.
31Query,keyandvaluevectors. Thequeryandkeytransformationsinthefirstlayersaredesignedin
thefollowingway,
Q(z )=[x ,1,−10T(2k−1)]
2k−1 k
K(z )=[x ,3,T(2i−1)] fori=1,...,k,
2i−1 i
K(z )=[0 ,−2.3,T(2i)] fori=1,...,k.
2i d
Thevaluevectorswillonlyhavethe5thpart(T(2i))inthelastslotandallothervalueswillbe0. Let
√
T (i)= kT(i)∈{−1,1}k,thatis,T (i)containsthesignsofthevectorT(i). Thevaluevector
s s
willbeoftheformV(z )=[0 ,0,0,0 ,0 ,2T (2i)]andV(z )=[0 ,0,0,0 ,0 ,2T (2i)]
2i−1 d′ k k s 2i d′ k k s
for all i = 1,...,N. The goal of the attention in the first layer is to retrieve the vector T (2j∗)
s
correspondingtothelabelofthenearestneighbour.
Innerproducts. Thedesignofsuchqueryandkeytransformationensuresthatforaninputquery
x , the dot product is maximum with its nearest neighbour x and not with itself or any of the
k j∗
embeddingsofthelabelsy s.
i
TheinnerproductsA =⟨Q(z ),K(z )⟩havethefollowingform,
2k−1,2i−1 2k−1 2i−1
⟨Q(z ),K(z )⟩=⟨x ,x ⟩+2−10⟨T(2k−1),T(2i−1)⟩
2k−1 2i−1 k i
(cid:40)
⟨x ,x ⟩+3±γ/10 ifi̸=k,
= k i
⟨x ,x ⟩+3−10±γ/10 ifi=k.
k i
(cid:40)
≥1 ifi̸=k,
=⇒ ⟨Q(z ),K(z )⟩
2k−1 2i−1 ≤−5 ifi=k.
Similarly,theinnerproductwiththelabelvectorsz sislessthan0foralliaswell,
2i
⟨Q(z ),K(z )⟩=0−6±γ/10≤−5
2k−1 2i
Tosummarize,thequeryandkeyvectorsaresuchthatforthequeryinputz k−1,thedotproduct
2
with itself ⟨Q(z ),K(z )⟩ ≤ −5 and the dot product with all label vectors containing y
2k−1 2k−1 i
is⟨Q(z ),K(z )⟩ ≤ −5. Theremainingdotproductsarewiththevectorsofinterestwhich
2k−1 2i
includethenearestneighbourinputpoint,
⟨Q(z ),K(z )⟩=⟨x ,x ⟩±γ/10 fori=1,...,k−1.
2k−1 2i−1 k i
Suppose j∗ = argmax xTx is the index for the input point which has the minimum L2
i∈[k−1] k i
distanceormaximuminnerproductwiththequeryvectorx . Thenwehavethat,
k
⟨Q(z ),K(z )⟩−⟨Q(z ),K(z )⟩≥γ−γ/5 (10)
2k−1 2j∗−1 2k−1 2i−1
foralli̸=j∗. Thisindicatesthemaximuminnerproductwillbewithitsnearestneighborandall
otherinnerproductswillhaveamarginofatleastτ = 4γ.
5
IfwehaveaTransformerwithahard-attentionmechanism,thenitwillattendonlytotheinputwhich
isthenearestneighborofthequeryvectorx . However,withsoftmaxattention,itisnon-trivialto
k
onlyattendoverasingleinput.
Theembeddingofallinputvectorsx containsavectorT(2i)(seeEq. 9)whichservesasakey
i
vectortoretrievethelabelfollowingthatinputvector. Notethat,weonlyneedtoretrievethesignsof
thevectorT(2i)sincethevectorsareoftheform{−√1 ,√1 }k. Wecanthenuseittoretrievethe
k k
labelofthecorrespondinginputinthenextlayer.
Retrieving with softmax. We show using softmax attention and a ReLU FFN we can retrieve
√
therequiredT(2j∗). Inparticular, wewillobtainT (2j∗) = kT(2j∗) ∈ {−1,1}k. Asshown
s
earlier, if j∗ = argmax xTx , then the maximum dot product ⟨Q(z ),K(z )⟩ ≥
i∈[k−1] k i 2k−1 2j∗−1
⟨Q(z ),K(z )⟩+τ isgreaterbyamarginτ = 4γ. Thevaluevectorcorrespondingtoz ,
2k−1 2i−1 5 j∗
i.e.,V(z )=[0,...,0,2T (2j∗)]hasthekeyvectorwhichwillallowthenextlayertoretrievethe
j∗ s
requiredlabel.
32The basic idea is that even if at least 9/10 of the attention weight is on V(z ) and essentially
j∗
2T (2j∗)thenthatsufficestopreservethesignsusingaReLUFFN.
s
Let σ(a) be a function such that σ(a) = 1 for a > 1, σ(a) = −1 for a < −1, and σ(a) = a
for−1 ≤ a ≤ 1. Seethatσ(a)caneasilybeimplementedbyaReLUFFNsinceitisessentially
ReLU(x+1)−ReLU(x−1)−1.
Withoutlossofgenerality,let’ssay2T (2j∗) = +2. Iftheattentionweightonitis9/10andthe
s 1
remaining1/10weightisdistributedamongtherestoftheinputs,thenintheworstcasethatvalue
willbe 9 2− 1 2 ≥ 1. Hence,applyingσ(·)overthevaluewillresultin+1. Thesamegoesfor
10 10
thecasewhen2T (2j∗) =−2andforotherindicesof2T (2j∗). Thus,itsufficestoshowthatthe
s 1 s
attentionweightoverV(z )canbegreaterthan9/10sinceitimpliesthatwiththeReLUFFN,
2j∗−1
theoutputforz inthefirstlayerwillcontainT (2j∗).
2k−1 s
Consideranotherconstructionwhichisidenticaltotheconstructiondescribedabovewiththeex-
ceptionthatQ˜(z )=ηQ(z ). Weshowthatforlargeenoughη,theweightsoftmax(A)
i i 2k−1,2j∗−1
will be greater than 9/10 and the remaining weights combined will be less than 1/10. Let
β = ⟨Q˜(z ),K(z )⟩ and Z ∈ RN×d contain vectors (z ,...,z ). Then, for any
2k−1 2j∗−1 1 2k−1
r ∈N,
softmax(Q˜(z )K(Z)T)
2k−1 2j∗−1
exp(η⟨Q(z ),K(z )⟩)
= 2k−1 2j∗−1
(cid:80)
exp(η⟨Q(z ),K(z )⟩)+ exp(η⟨Q(z ),K(z )⟩)
2k−1 2j∗−1 p̸=2j∗−1 2k−1 p
exp(ηβ) exp(ηβ) r−1
≥ ≥ ≥
exp(ηβ)+(2k−1)exp(η(β−τ)) exp(ηβ)+2Nexp(η(β−τ)) r
=⇒ exp(ηβ)≥(r−1)(2N)exp(η(β−τ))
1
=⇒ η ≥ log(r−1)2N.
τ
Hence,forr =10,ifη = 5 log18N thentheattentionweightonthe2j∗−1thinputvectorwillbe
4γ
greaterthan9/10. Similarly,onecanverifythatforη = 5 log18N,theattentionweightfortherest
4γ
oftheinputscombinedisatmost1/10.
Outputofthefirstlayer. Letz(1) denotetheoutputofthefirstlayerontheithinputvector. By
i
construction,withMLPandresidualconnectionaftertheattentionblock,theoutputofthefirstlayer
issuchthat,
z(1) =[x ,0,1,T(2k−1),T(2k),T (2j∗)]
2k−1 k s
z(1) =[x ,0,1,T(2i−1),T(2i),...] fori=1,...,k−1
2i−1 i
z(1) =[0 ,y ,−2,T(2i),T(2i),...] fori=1,...,k−1.
2i d′ i
SecondLayer. Sincewehavetheaddress/keyvectorT (2j∗)forthetargetlabelastheinputinthe
s
firstlayer,itisstraightforwardtoapplyattentionandσ(·)withReLUFFNtoproducethedesired
label.
SeethatifthequeryvectorQ(z(1) )=[1T (2j∗)],andthekeyvectorscontainthe4thpartofthe
2k−1 k s
inputvector,thatis,K(z(1))=[T(i)],thenthedotproductwillbegreaterthan1−γ/100withthe
i
desiredinputat2j∗andwillbelessthanγ/100fortherestoftheinputs. Thevaluevectorswillbe
assignedthesecondpartoftheinputV(z(1)) = [0 ,y ,0,...,0]andwillV(z(1) ) = [0,...,0]
2i d′ i 2i−1
foralli=1,...,k. Usingthetechniquesusedforthefirstlayer,itisstraightforwardtoseethata
scalingofthequeryvectorandapplyingσ(·)totheoutputwillproducethedesiredlabely .
j∗
33Index Lookup Task Index Lookup: Validation Curves
1.0
Mamba-(2, 32) 97.7 34.3 28.4 18.8 9.8 3.6 Mamba-(2, 512)
80 0.8 M Ma am mb ba a- -( (2 2, , 6 14 0) 24)
Mamba-(2, 64) 99.6 78.9 47.7 34.6 13.5 6.0 Mamba-(2, 32)
60 0.6 Mamba-(2, 256)
Mamba-(2, 256) 99.7 98.7 87.5 52.8 30.4 11.5
0.4
40
Mamba-(2, 512) 99.6 99.6 98.2 70.5 29.8 9.8
0.2
20
Mamba-(2, 1024) 99.8 99.4 97.8 82.1 34.1 12.6
0.0
20 50 75 100 200 400 0 50000 100000 150000 200000 250000
Input Length # Iterations
Figure3: PerformanceofMambaontheIndexLookuptaskacrossvariouslengthsandwidths. See
SectionH.1formoredetails.
H EmpiricalAnalysis: AdditionalDetailsandExperiments
Inthissection,wediscussthedetailsofimplementationanddatagenerationforexperimentsdescribed
inSection6. Further,wediscusssomeadditionalexperimentsonthestringequalitytask.
Implementationandhyperparameters. Allofourimplementationsforexperimentsarebasedon
PyTorch[44]. ForourexperimentswithTransformers,weusetheHuggingfaceTransformerslibrary
[63]withtheGPT-2backboneaswellasourowncustomimplementation. WeusePyTorch’sstandard
orin-builtimplementationforexperimentswithLSTMs. Forstate-spacemodelslikeMamba[21]
andDiagonalstate-spacemodels[22],weusetheofficialimplementationprovidedbytheauthorsfor
ourexperiments. ForRetNet[56]andLinearTransformers,weuseourowncustomimplementation.
Foreachtask,wetunethemodelsacrossseveralhyperparametersandreporttheresultsbasedonthe
best-performingmodel. Weusegridsearchtotunethemodelsforeachtask. Foreacharchitecture
excluding LSTMs, we tuned across depths ∈ {1,2,4,6} and widths {64,128,256,512}. Since
LSTMswithlargedepthsarehardtotrain[43],weonlyconsiderLSTMswithdepthsupto3. For
TransformersandlinearTransformervariants,wetunetheheadsacross{4,8}. Forallmodels,we
tunethelearningrateacross{0.01,0.005,0.001,0.0005,0.0001,0.00005,0.000001}. Weprimarily
useTransformerswithabsoluteencodingsfortheresultpresentedinthemainpaper. Wetrainthe
modelsforupto250kstepsunlesstheyachieve(almost)perfectvalidationaccuracyearlier. After
training,weevaluatethemodelsonafreshsetof5000examples.
Note, however, that the focus of our paper is slightly different and for some problems, we are
interestedinthebehaviorofsmall-sized(widthordepth)modelsofonearchitectureandarelatively
largermodelforanotherarchitecture. Forinstance,intheindexlookuptask,weareconcernedwith
howwellone-layerTransformerswithsmallwidthsfareagainstrelativelylargerrecurrentmodels.
Additionally,ourfocusisonthecomparisonofdifferentarchitecturesoffixedsizesacrosslengths
andnotprimarilyonhowwellmodelsofthescaleusedinpracticeperformthesetasksonmuch
largerlengths. Hence,wedonotconsidermodelswithmuchlargerdepthorwidth.
Compute. All our experiments were conducted using 8 NVIDIA Tesla V100 GPUs each with
16GBmemoryand16NVIDIAGTX1080TiGPUseachwith12GBmemory. Eachrunfor500k
stepscouldtakebetween1hourand16hoursdependingonthelengthoftheinputsandthesize
of the models. Some runs are much shorter if the model achieves high accuracy quite early (e.g.
onlengths20). Whileeachindividualrundoesnottakeasignificantamountoftime, tuningthe
models,particularlyforthetaskswheretheyfailtolearnrequiresseveralrunsandconsequentlya
muchlongerduration. Weestimatethatalltherunsacrosshyperparameters,architectures,andinput
lengthstook≤1200GPUhoursintotalontheGPUsmentionedabove.
H.1 AdditionalExperimentsandDataGeneration
IndexLookupTask. TheinputsequencesarecomprisedofsymbolsinΣandapositionaltokenin
[N]. Inourexperiments,thesizeofthesetofsymbolsis|Σ|=64andN variesfrom20to400. To
createeachexample,wefirstsamplealengthk uniformlybetween10andN (20-400)andthen
sampleN symbolsindependentlyfromΣuniformlyatrandom. Lastly,wesampleapositionbetween
34
ledoM
)%(
ycaruccA
)%(
ycaruccA
noitadilaV1andkuniformlyatrandomandappendittothesequencetoproducealabeledexamplefortraining
orevaluation. Duringevaluation,themodelistestedonsequencesoflengthexactlyN.
BoundedDycks. FortheDyck-(2,2)task,wegeneratetheexamplesinsuchawaythatwith0.5
probability,thegeneratedstringiswell-balancedwithdepthatmost2,andwith0.5probabilityit
doesnotbelongtothelanguageandhaslabel0. ThegeneratedstringsareoflengthexactlyN in
bothcases. Togeneratepositiveexamplesweusethefollowingstrategy: weiterateoverN −2steps
andforeachstep,wecheckwhetherthecurrentdepthofthestackis<2. Ifthedepthofthestackis
2,thesequencecontinueswiththeclosingbracketcorrespondingtotheopenbracketinthestack. If
thedepthis<2,thesequenceiseithercontinuedbychoosingoneoftheopenbracketsuniformlyif
thestackisemptyorbychoosinguniformlybetweenopenbracketsandtheclosingbracketifthe
stackisnon-empty. AfterN −2steps,thegeneratedstringcouldbeawell-balancedstringoflength
N −2inwhichcaseweaddadepth1stringoflength2attheendwhichresultsinawell-balanced
stringoflengthN. Otherwise,thestackcouldbenonemptyweaddtheremainingclosingbrackets
whichalsoleadstoastringoflengthN.
To generate negative examples, we first sample a positive example and then corrupt some of the
symbolsinthestring. ForstringsoflengthN,wefirstsampleanumberk =1,...,N/10uniformly
atrandomandthenpickkdifferentindicesuniformlyatrandom. Foreachofthosepositions,with
probability1/2,weswapthetypesofbrackets,e.g. round‘(’tosquare‘[’,andwithprobability1/2
weswitchopenbracketstoclosingbrackets(orviceversa). Thereisaverysmallprobabilitythat
afterthecorruptiontheresultingstringwillstillbeavalidwell-balancedstringofdepthatmost2,
in which case we redo thecorruption again. The probability of theevent is too low to affect the
efficiencyofthegenerationprocess.
AdditionalExperimentwithMamba. Weexplorehowthesizeofthehiddenstateinfluencesthe
performance of a Mamba model across various lengths on the Index Lookup task. We evaluate
two-layermodelsofdifferentwidths{32,64,256,512,1024}acrossvariouslengthsrangingfrom
20to400. WefindacleartrendwheretheperformanceofMambamodelsincreasesmonotonically
withtheincreaseinthewidthofthemodel(seeFigure3). Whiletheperformancedoesexactlyscale
linearlywithwidthitisstillsomewhatinterestingthatthetrendexists. Wedidasimilarexperiment
withLSTMbutdidnotobservesuchatrendandforlengthsabove100theperformanceremainedat
chancelevelevenwhenthewidthwasincreased. Note,however,thatevenwithawidthof1024,the
performanceofMambaisstillmuchworsethanaone-layerTransformerwithawidthof64.
H.2 StringEqualityTask
Weexploreafewdifferentstrategiestoevaluatetheperformanceofmodelsonthestringequalitytask.
Intheseexperiments,thegoalofthemodelsistodeterminewhetherthefirsthalfoftheinputstring
andthesecondhalfofthestringareequal. Thefirsttwoexperimentsareinthestandardclassification
settingbutdifferinthewaythenegativeexamplesarecreated. Thethirdexperimentisinthenext
characterpredictionwhichiscommonlyusedinpriorworkstotestmodelsonformallanguages. For
theequalitytask,wefindthatitisnotstraightforwardtogeneratenegativeexamplesinawaythatthe
problemcannotbesolvedusingshortcuts. Hence,weincludethenextcharacterpredictionsetting
whichdoesnotrequirethegenerationofnegativeexamples.
Summaryofresults. TheresultsontheStringEqualitytaskarerelativelymorenuancedthanthe
resultsforindexlookupandDyck-2. Onastandardbinaryclassificationsetup,onemustmakea
designchoiceregardingthegenerationofnegativeexamplesthatcouldinfluencethedifficultyof
thetask. Wefirstconductexperimentsintheclassificationsetupbygeneratingnegativeexamples
usingtwodifferentstrategies. Ononetask,wefindthatwhilerecurrentmodelslikeLSTMsstruggle
beyondacertainlength,state-spacemodelslikeDSSandMambaareabletomatchTransformer’s
performance. Inthesecondtask,wefindthatallmodelsareabletoachievenear-perfectaccuracy.
Wenotethatbothclassificationtaskscanbesolvedbyusingshortcutsbasedonthewaythenegative
examplesarecreated. Hence,weexploreanotherstrategycalledthenextcharacterpredictionsetting
inspiredbypriorworks[57,20]onempiricalanalysisonformallanguages. Wenotethatthetaskin
thatsettingbecomesalmostidenticaltothecopyingtaskwhereamodelobservesasequenceand
thenhastoproducethe samesequence. Inthatsetting, we observethatatasmallwidthlike64,
state-spacemodelsfailtoperformthetaskaccuratelyatcertainlengthswhereasTransformerswith
thesamewidthsucceed. Formodelswithlargerwidths,wefindthatDSSandTransformerssucceed
atperformingthetasksforthelengthsconsideredinourexperiments.
35Eq-random Task Eq-ncp Task
100 100
LSTM-(2, 256) 100.0 99.8 99.7 51.0 49.7 LSTM-(2, 64) 99.4 99.6 0.0 0.0 0.0
90 80
LinTF-(2, 256) 100.0 99.9 99.5 51.2 50.7 LinTF-(2, 64) 99.9 99.4 0.0 0.0 0.0
RetNet-(2, 256) 100.0 100.0 98.7 50.9 49.8 80 RetNet-(2, 64) 99.8 99.9 0.2 0.0 0.0 60
DSS-(2, 256) 100.0 99.9 100.0 99.9 99.9 70 DSS-(2, 64) 100.0 99.9 92.5 0.1 0.0 40
Mamba-(2, 256) 100.0 99.3 99.8 99.6 99.5 Mamba-(2, 64) 99.9 99.3 89.8 0.0 0.0
60 20
TF-(2, 256) 100.0 100.0 100.0 99.8 99.9 TF-(2, 64) 100.0 99.9 99.8 99.9 99.8
50 0
20 50 100 200 400 100 200 400 600 800
Input Length Input Length
Figure4: PerformanceofarchitecturesontheEqualitytask. SeeSectionH.2formoredetails.
Setup. Inallourexperimentswithstringequality,wehaveavocabularyΣwhichcontainsonetoken
thatisreservedasaseparatortoken‘[SEP]’andisplacedbetweenthefirsthalfandthesecondhalf
oftheinputstring. Foreachtask,whilecreatinganexamplewefirstpickthelengthk uniformly
atrandomfrom N, N +2, N +4,...N. Thechoiceoflengthsisduetotherequirementthatthe
10 10 10
lengthofthestringsmustbeeven. Duringevaluation,wetestthemodelsonstringsoflengthexactly
N.
(i) Eq-random. The first experiment is pretty straightforward and contains binary strings, i.e.,
Σ={0,1,[SEP]}. Withprobability1/2,wecreateapositiveexamplebyfirstsamplingastringof
lengthk/2uniformlyfollowedbytheseparatortokenandthenthesamestringagain.Byconstruction,
thecreatedexamplehasapositivelabel. Withprobability1/2,wecreatethesecondhalfofthestring
bysamplinganotherstringofthesamelengthwhereeachsymbolissampleduniformlyatrandom.
We assign the label based on whether or not it is equal to the first half but with high probability
(1− 1 ),theexamplehasanegativelabel.
2k/2
(ii)Eq-oneInthesecondexperiment,wehaveavocabularywith1024andwecreatethepositive
exampleinthesamewayasearlierintheEq-onesettingbysamplingstringsuniformlyatrandom.
Togenerateanegativeexample,wesetthesecondhalftobeequaltothefirsthalfandthenchange
thesymbolatonlyoneofthepositions. Inotherwords,thesecondhalfmatchesthefirsthalfatall
positionsexceptone.
Thetrainingdetailsandhyperparametersarealmostidenticaltotheexperimentsdescribedearlier.
Results. OnboththeEq-randomtaskandEq-onetaskwefindthatTransformersachievenear-perfect
accuracy on lengths up to 400. Recurrent models like LSTMs and linear Transformers struggle
atlengthsbeyond100ontheEq-randomtask. Ontheotherhand,wefindthatrecentlyproposed
state-spacemodelslikeDSSandMambaareabletomatchTransformers’performanceonbothtasks
(SeeFigure4left). OntheEq-onetask,wefindthatforlengthsupto400,allmodelsareableto
achievenear-perfectaccuracy.
Remark. Onethingtonoteisthatbothoftheexperimentalsetupsdescribedabovecanbesolved
using some form of shortcuts. In the first case, it is straightforward to see that the first half and
thesecondhalfwilldifferatabouthalfthepositionsonaverage. Arecurrentalgorithmdoesnot
necessarilyhavetostorethefirstN/2elementstocomputetheoutputcorrectlywithhighprobability.
Even if it stores the first few elements, then with high probability it can determine the label of
the string correctly. For the second case (Eq-one), even if it might seem difficult at first look, a
recurrentmodelcansolveitperfectlybyjustmaintainingadictionaryofthecountsofeachsymbol
inthevocabulary. Sincethefirsthalfandsecondhalfdifferatexactlyoneposition,thenumberof
occurrencesofatleastonesymbolwillbedifferentinthetwohalves.
Toruleoutsuchphenomena,weadoptthenextcharacterpredictionsetting[20,50,57,17].
(iii)Eq-ncp. Inthenextcharactersetting(NCP),amodelisrequiredtopredictthenextsetofvalid
continuationsforeveryprefixofagivenstring. Itcanbeseenasamulti-labelclassificationproblem
foreveryprefixofagivenstring. Thepredictionforaparticularinputstringisconsideredcorrectif
thepredictionforeveryprefixiscorrect.
InthecontextofthestringequalitytaskwithlengthN,forthefirstN/2symbols,allsymbolsare
validcontinuations,andhencethepredictionsforthefirsthalfofthestringaretrivial. Afterobserving
36
erutcetihcrA )%(
ycaruccA
erutcetihcrA )%(
ycaruccAtheseparatortoken [SEP],onlyoneofthe|Σ|symbolsisallowedateveryprefixuntiltheendof
thestring. Wenotethatthepredictionproblembecomesequivalenttocopying[28]asequenceof
symbols. For our experiments the vocabulary size |Σ| = 1024. We explore sequences of higher
lengthsupto800. Inthissetting,wefindthatwhenthemodelsizesarerestricted,i.e.,thewidth
ofthemodelsis64, state-spacemodelssuchasDSSandMambastruggletoperformbetterthan
chance-levelaccuracyforlengthsover400. Incontrast,Transformersareabletoachievenear-perfect
accuracy(SeeFigure4). However,unlikethecaseofIndexLookup,wefindthattheDSSarchitecture
inparticularisabletosolvethetaskforlengthsupto800withlargerwidthsintheNCPsetting.
Discussion. Wediscussafewtakeawaysfromourexperiments. FortheIndexLookuptask, our
resultsindicatethatevensmall-sizedone-layerTransformerscanlearnalotmoreefficientlythan
recurrent models of much larger sizes. The experiments with bounded Dycks are primarily for
one-layerTransformersandindicatethattheylearnatamuchslowerratethanrecurrentmodelslike
LSTMsandeventwo-layerTransformers. Onthestringequalitytask,thedifferenceinperformance
betweenTransformersandrecurrentmodelsisnotasstarkastheIndexLookuptask,particularly
withstate-spacemodelssuchasDSS.However,unlikeTransformers,theyseemtostruggleonlong
sequencesintheNCPsettingwhenthewidthsofmodelsaresmall.
37