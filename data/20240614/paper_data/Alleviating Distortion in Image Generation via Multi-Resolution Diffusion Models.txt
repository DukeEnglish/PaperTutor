Alleviating Distortion in Image Generation via
Multi-Resolution Diffusion Models
QihaoLiu1,2*,ZhanpengZeng1,3*,JuHe1,2*,QihangYu1,XiaohuiShen1,Liang-ChiehChen1
1ByteDance 2JohnsHopkinsUniversity 3UniversityofWisconsin-Madison
*equalcontribution
https://qihao067.github.io/projects/DiMR
Figure1: (Top)Randomlysampled512×512imagesgeneratedbytheproposedDiMR.(Bottom)
Randomsamplesofthelowvisualfidelity256×256imagesgeneratedbyDiMRandDiT[40]. To
detectlowvisualfidelityimagesforbothmodels,aclassifier-basedrejectionmodelisemployed(with
thesamerejectionrate). DiMRgeneratesimageswithhigherfidelityandlessdistortionthanDiT.
Abstract
Thispaperpresentsinnovativeenhancementstodiffusionmodelsbyintegrating
anovelmulti-resolutionnetworkandtime-dependentlayernormalization. Diffu-
sionmodelshavegainedprominencefortheireffectivenessinhigh-fidelityimage
generation. WhileconventionalapproachesrelyonconvolutionalU-Netarchitec-
tures,recentTransformer-baseddesignshavedemonstratedsuperiorperformance
andscalability. However, Transformerarchitectures, whichtokenizeinputdata
(via“patchification”),faceatrade-offbetweenvisualfidelityandcomputational
complexity due to the quadratic nature of self-attention operations concerning
tokenlength. Whilelargerpatchsizesenableattentioncomputationefficiency,they
struggle to capture fine-grained visual details, leading to image distortions. To
addressthischallenge,weproposeaugmentingtheDiffusionmodelwiththeMulti-
Resolutionnetwork(DiMR),aframeworkthatrefinesfeaturesacrossmultipleres-
4202
nuJ
31
]VC.sc[
1v61490.6042:viXra
selpmas
215
x
215
detarucnU
selpmas
ytiledif-wol
detarucnU
)sruO(
RMiD
)sruO(
RMiD
TiDolutions,progressivelyenhancingdetailfromlowtohighresolution. Additionally,
weintroduceTime-DependentLayerNormalization(TD-LN),aparameter-efficient
approachthatincorporatestime-dependentparametersintolayernormalizationto
injecttimeinformationandachievesuperiorperformance. Ourmethod’sefficacy
isdemonstratedontheclass-conditionalImageNetgenerationbenchmark,where
DiMR-XLvariantsoutperformpriordiffusionmodels,settingnewstate-of-the-art
FIDscoresof1.70onImageNet256×256and2.89onImageNet512×512.
1 Introduction
Diffusionandscore-basedgenerativemodels[23,52,54,19,53]havedemonstratedpromisingresults
for high-fidelity image generation [8, 37, 43, 45, 47]. These models generate images through an
iterativeprocessofgraduallydenoisingGaussianrandomnoisetocreaterealisticsamplesCentral
tothisprocessisaneuralnetwork,taskedwithdenoisingtheinputsthroughameansquarederror
lossfunction. Traditionally,U-Netarchitectures[46](enhancedwithresidualblocks[15]andself-
attentionblocks[58]atlowerresolution)havebeenprevalent. However,recentadvancementshave
introducedTransformer-baseddesigns[58,9],offeringsuperiorperformanceandscalability.
In practice, Transformer-based architectures face the challenge of balancing visual fidelity and
computationalcomplexity,primarilystemmingfromtheself-attentionoperationandthepatchification
process employed for downsampling inputs [9] (i.e., a smaller patch size results in better visual
fidelity at the cost of a longer token length and thus more computational complexity by the self-
attentionoperation). Thequadraticcomplexityinherentinself-attentionconcerningtokenlength
necessitates larger patch sizes to facilitate more efficient attention computations. However, the
adoptionoflargepatchsizesinevitablycompromisesthemodel’scapacitytocapturefinervisual
details,resultinginimagedistortion(i.e.,lowvisualfidelity). ThisdilemmapromptsDiT[40]to
conductasystematicstudyontheimpactofpatchsizeonimagedistortion,asdepictedinFig.7of
theirpaper. Consequently,theysettledonapatchsizeof2fortheirfinaldesign. Similarly,U-ViT[3]
optedforapatchsizeof2forinputsizesof256×256andapatchsizeof4for512×512images,
effectivelybalancingthetokenlengthfordifferentimagesizes. Despitethesemeticulousadjustments,
thegeneratedresultsstillexhibitdiscernibleimagedistortion,asillustratedinFig.1.
OnesimplisticsolutiontomitigateimagedistortioninTransformer-basedarchitecturesisadoptinga
patchsizeof1,butthissignificantlyincreasescomputationalcomplexity. Instead,inspiredbythe
successofimagecascade[20,47]whichgenerateimagesatincreasingresolutions,weproposea
featurecascadeapproachthatprogressivelyupsampleslower-resolutionfeaturestohigherresolutions,
alleviating distortion in image generation. In this study, we present DiMR, which enhances the
DiffusionmodelwithaMulti-Resolutionnetwork. DiMRtacklesthechallengeofbalancingvisual
detail capture and computational complexity through improvements in the denoising backbone
architecture. We employ a multi-resolution network design that comprises multiple branches to
progressively refine features from low to high resolution, preserving intricate details within the
inputdata. Specifically, thefirstbranchhandlingthelowestresolutionincorporatesTransformer
blocks [58], leveraging the superior performance and scalability observed in prior works [3, 40],
whiletheremainingbranchesutilizeConvNeXtblocks[34],whichareefficientforhighresolution
features. Thenetworkprocessesinputsprogressivelyfromthelowestresolution, withadditional
featuresfromtheprecedingresolution. Thelastbranchrefinesfeaturesatthesamespatialresolution
astheinput,effectivelymitigatingimagedistortionarisingfromthepatchification.
Additionally,weobservethatexistingtimeconditioningmechanisms[41,25,8],suchasadaptive
layernormalization(adaLN)[40],areparameter-intensive. Incontrast,weproposeamoreefficient
approach,Time-DependentLayerNormalization(TD-LN),thatintegratestime-dependentparameters
directlyintolayernormalization[2],achievingsuperiorperformancewithfewerparameters.
Todemonstrateitseffectiveness,weevaluateDiMRontheclass-conditionalImageNetgeneration
benchmark[7]. OnImageNet64×64,DiMR-M(133Mparameters)andDiMR-L(284M),without
classifier-free guidance [18], achieve FID scores of 3.65 and 2.21, respectively, outperforming
the Transformer-based U-ViT-M/4 and U-ViT-L/4 by 2.20 and 2.05 FID. On ImageNet 256 ×
256, DiMR-XL (505M) achieves FID scores of 4.50 and 1.70, without and with classifier-free
guidance,respectively. OnImageNet512×512,DiMR-XL(525M)achievesFIDscoresof7.93
and2.89,withoutandwithclassifier-freeguidance,respectively. Theseresultsdemonstratesuperior
2performance compared to all previous methods, despite having similar or smaller model sizes,
establishinganewstate-of-the-artperformance. Insummary,ourmaincontributionsareasfollows:
1. We develop effective strategies for integrating multi-resolution networks into diffusion
models, introducing the novel feature cascade approach that captures visual details and
reducesimagedistortionsinhigh-fidelityimagegeneration.
2. WeproposeTD-LN,asimpleyeteffectiveparameter-efficientmethodthatexplicitlyencodes
crucialtemporalinformationintothediffusionmodelforenhancedperformance.
3. WeintroduceDiMR,anovelarchitecturethatenhancesdiffusionmodelswiththeproposed
multi-resolutionnetworkandtheTD-LN.DiMRdemonstratessuperiorperformanceonthe
class-conditionalImageNetgenerationbenchmarkcomparedtoexistingmethods.
2 RelatedWork
Diffusionmodels. Diffusion[52,19]andscore-basedgenerativemodels[23,54],centeredarounda
denoisingnetworktrainedtoprogressivelyproducedenoisedvariantsoftheinputdata. Theyhave
drivensignificantadvancesacrossvariousdomains[33,28,57,55,60,39,59],particularlyexcelling
in high-fidelity image generation tasks [37, 43, 45, 47]. Key advancements in diffusion models
include the improvements in sampling methodologies [19, 53, 26] and the adoption of classifier-
free guidance [18]. Latent Diffusion Models (LDMs) [45, 40, 42, 61] address the challenges of
high-resolutionimagegenerationbyconductingdiffusioninthelower-resolutionlatentspaceviaa
pre-trainedautoencoder[27]. Inthisstudy,ourfocusliesondesigningthedenoisingnetworkwithin
diffusionmodelsandexaminingitsapplicabilityacrossbothpixeldiffusionmodelsandLDMs.
Architecturefordiffusionmodels. EarlydiffusionmodelsemployedconvolutionalU-Netarchi-
tectures[46]asthedenoisingnetwork,whichweresubsequentlystrengthenedthroughexplorations
of either computing attention [8, 38] or performing diffusion directly at multiple scales [20, 13].
Recently,Transformer-basedarchitectures[3,40,14]alongwithotherexplorations[62,56]have
emergedaspromisingalternatives,showcasingsuperiorperformanceandscalability. Specifically,
forTransformer-basedarchitectures,U-ViT[3]treatsallinputs,includingtime,condition,andnoisy
imagepatches,astokensandemployslong-skipconnectionsbetweenshallowanddeeptransformer
layersinspiredbyU-Net. Similarly,DiT[40]leveragesVisionTransformers(ViTs)[9]tosystemati-
callyexplorethedesignspaceundertheLatentDiffusionModels(LDMs)framework,demonstrating
favorable properties such as scalability, robustness, and efficiency. In this study, we introduce
the Multi-Resolution Network as a new denoising architecture for diffusion models, featuring a
multi-branchdesignwhereeachbranchisdedicatedtoprocessingaspecificresolution.
Timeconditioningmechanisms. Followingthewidespreadusageofadaptivenormalization[41]in
GANs[4,25],diffusionmodelssimilarlyexploreadaptivegroupnormalization(AdaGN)[8]and
adaptivelayernormalization(AdaLN)[40]toencodethetimeinformation. Thesemethodssharethe
similarityinrequiringcomputingalinearprojectionofthetimestep,whichsignificantlyincreasesthe
parameterofthemodel. Recently,U-ViT[3]introducesanewstrategytosimplytreattimeasatoken
andprocesswithTransformerblocks. Eventhougheffective,itisnotfeasibletotreattimeasinput
forotherblocks(e.g.,ConvNeXtblocks[34]). Inthisstudy,weintroduceTime-DependentLayer
Normalization(TD-LN),aparameter-efficientapproachthatexplicitlyencodestemporalinformation
byincorporatingtime-dependentparametersintolayernormalization[2].
3 Preliminary
Diffusionmodels[52,19]arecharacterizedbyaforwardprocessthatgraduallyinjectsnoisesto
destroydatax ∼q(x ),andareverseprocessthatinvertstheforwardprocesscorruptions.Formally,
0 0
thenoiseinjectionprocessisformulatedasaMarkovchain:
T
(cid:89)
q(x |x )= q(x |x ),
1:T 0 t t−1
t=1
where x for t ∈ [1 : T] is a family of random variables obtained by progressively injecting
t √
Gaussian noise into the data x , and q(x |x ) = N(x | α x ,β I) represents the noise
0 t t−1 t t t−1 t
3: Predicted noise at different scales
Transformer Block ConvNeXt Block
+ Conv 3x3 Conv 3x3 Conv 3x3 +
MLP
Transformer Block ConvNeXt Block ConvNeXt Block MLP
C C C
TD-LN
Transformer Block ConvNeXt Block ConvNeXt Block
TD-LN
+
Multi-Head Transformer Block ConvNeXt Block ConvNeXt Block
Attention …
C0 1 2 L U + U +
Depthwise
TD-LN Conv 4x4, Stride 4 Conv 2x2, Stride 2 Conv 1x1, Stride 1 Conv
C: Class Token : Position Embedding : Noised input
TD-LN : Time-Dependent Layer Norm + : Add C: Concat U: Upsample
Figure 2: Model overview. We propose DiMR that enhances Diffusion models with a Multi-
ResolutionNetwork. Inthefigure,wepresenttheMulti-ResolutionNetworkwiththreebranches.
Thefirstbranchprocessesthelowestresolution(4timessmallerthantheinputsize)usingpowerful
Transformerblocks,whiletheothertwobrancheshandlehigherresolutions(2timessmallerthanthe
inputsizeandthesamesizeastheinput,respectively)usingeffectiveConvNeXtblocks. Thenetwork
employsafeaturecascadeframework,progressivelyupsamplinglower-resolutionfeaturestohigher
resolutionstoreducedistortioninimagegeneration. TheTransformerandConvNeXtblocksare
furtherenhancedbytheproposedTime-DependentLayerNormalization(TD-LN),detailedinFig.4.
injectionschedulesuchthatα +β =1. Inthereverseprocess,aGaussianmodelp(x |x )=
t t t−1 t
N(x |µ (x ),σ2I)islearnedtoapproximatethegroundtruthreversetransitionq(x |x ). This
t−1 t t t t−1 t
stepisequivalenttopredictingthedenoisedvariantoftheinputx ,andthusthelearningobjective
t
canbefurthersimplifiedtopredictingthenoiseϵ viaanoisepredictionnetwork(withparameters
t
θ),i.e.,ϵ (x ): minE ∥ϵ −ϵ (x )∥2. Theconditioninformationccanbeincorporatedinto
θ t
θ
t,x0,ϵt t θ t 2
thelearningobjectivewhenthediffusionprocessisguidedbytheclasscondition[8],i.e.,ϵ (x ,c):
θ t
minE ∥ϵ −ϵ (x ,c)∥2. Traditionally,learningthisobjectivereliesontheU-Net[46],with
θ
t,x0,c,ϵt t θ t 2
theconditioncencodedintotheU-Netthroughvariousmethods[3,8,40,45].
4 Method
Inthissection,webeginbyintroducingtheproposedMulti-ResolutionNetwork(Sec.4.1),which
progressively refines features from low to high resolution. Next, we detail the proposed Time-
DependentLayerNormalization(Sec.4.2). Wethendiscussseveralmicro-leveldesignenhancements
(Sec.4.3). Finally,wepresenttheDiMRmodelvariants,scaledfordifferentmodelsizes(Sec.4.4).
4.1 Multi-ResolutionNetwork
Motivation. There is a trade-off between generation quality and computational complexity as
depictedintheablationstudyinFig.7ofDiT[40]. TheircarefulstudyrevealedthatTransformer-
baseddiffusionmodelswithsmallerpatchsizesoperateathigherfeatureresolutionsandproduce
bettergenerationqualitybutincurhighercomputationalcostsduetotheincreasedinputsize.
We conjecture that the distortion in U-ViT [3] and DiT [40] arises from their oversimplified up-
samplingmodule,wherelower-resolutionfeaturemapsareupsampleddirectlytothetargetsizeof
thegeneratedimagesviaasimplelinearlayer(forincreasingchannels)andpixelshufflingupsam-
pling[51]. Inspiredbyimagecascade[20,47]—amethodforgeneratinghigh-resolutionimages
byusingmultiplecascadeddiffusionmodelstoproduceimagesofprogressivelyincreasingresolu-
tion—weproposefeaturecascade,whichprogressivelyupsamplelower-resolutionfeaturestohigher
resolutionstoalleviatedistortioninimagegeneration. Thefeaturecascadeisimplementedthrough
theproposedMulti-ResolutionNetwork,deployedasthedenoisingnetworkindiffusionmodels.
4Figure3: PrincipalComponentAnalysis(PCA)oflearnedscaleandshiftparametersinadaLN-
Zero[40]. WeconductPCAonthelearnedscale(γ ,γ )andshift(β ,β )parametersobtained
1 2 1 2
fromaparameter-heavyMLPinadaLN-Zerousingapre-trainedDiT-XL/2[40]model. Thevertical
axisrepresentstheexplainedvarianceratioofthecorrespondingPrincipalComponents(PCs). Our
observationsrevealthatthelearnedparameterscanbelargelyexplainedbytwoprincipalcomponents,
suggestingthepotentialtoapproximatethembyasimplerfunction.
Overviewofmulti-branchdesign. TheproposedMulti-ResolutionNetworkcomprisesRbranches,
whereeachbranchisdedicatedtoprocessaspecificresolution. Forther-thbranch(r ∈{1,··· ,R}),
theinputfeaturesareprocessedbyaconvolutionwithakernelsizeof2R−r×2R−r andastrideof
2R−r,whicheffectivelypatchifiestheinputfordifferentresolutions. Thefirstbranch(i.e.,r =1)
downsamplestheinputfeaturesbyafactorof2R−1,andsubsequentlyhandlesthelowestresolution
featuresviatheTransformerblocks[58],whichenjoysthesuperiorperformanceandscalabilityof
self-attentionoperations[3,40]. Forhigherresolutionfeatures,theremainingbranchesutilizethe
ConvNeXtblocks[34],whichleveragestheefficiencyoflargekerneldepthwise-convolutionopera-
tions[22,49]. Intermediatefeaturesfromthepreviousbranch(lastblock’soutput)areupsampled
andaddedwiththeinputsforthecurrentbranch. FollowingU-ViT[3],allbranchesemploythelong
skipconnectionsandanadditional3×3convolutionintheend. Thefinalbranch(i.e.,r =R)refines
featuresatthesamespatialresolutionastheinput.
Designdetails. Forr ∈{1,··· ,R},wedefinether-thbranchasafunctionf asfollows:
θ,r
ϵ (x ,c,r),y =f (x ,y ,t,c), (1)
θ t r θ,r t r−1
where the function f , parameterized by θ and r, takes as input the input features x and the
θ,r t
featuresfrompreviousresolutiony (alsotimetandconditionc). Theoutputsoff containthe
r−1 θ,r
intermediatefeaturesy (lastblock’soutput,beforethefinal3×3convolution)andthepredicted
r
noiseϵ (x ,c,r)fortheresolutionspecifictor-thbranch.
θ t
Toprocesstheinputs, thefunctionf firstpatchifiestheinputfeaturesx , andaddsitwiththe
θ,r t
upsampledfeaturesy fromthepreviousresolutionr−1.Theresultingfeaturesarethenprocessed
r−1
byeitherastackofTransformerblocks(whenr =1)orConvNeXtblocks(whenr ̸=1)withanother
3×3convolutionaddedintheend. Formally,wehave:
f (x ,y ,t,c)=Conv (g (Patchify(x )+Upsample(y ),t,c)), (2)
θ,r t r−1 3×3 θ,r t r−1
whereConv is3×3convolution,Patchifyispatchificationinstantiatedviaaconvolutionwith
3×3
a kernel size of 2R−r ×2R−r and a stride of 2R−r, Upsample is the pixel shuffling upsampling
operation [51], and g is a stack of Transformer blocks or ConvNeXt blocks, depending on r,
θ,r
augmentedwiththelongskipconnections[3]. Forthefirstbranch(i.e.,r = 1),y issettozero.
0
Thenoisepredictionϵ (x ,c,R)atthelastbranch(i.e.,r = R)isusedfortheiterativediffusion
θ t
process. WeillustratetheproposedMulti-ResolutionNetworkwiththreebranchesinFig.2. Note
thattheinputfeaturescanbeeitherrawimagepixelsorlatentfeaturesafterVAE[27],wherethe
latentfeaturesfacilitateefficienthigh-resolutionimagegeneration[45].
4.2 Time-DependentLayerNormalization
Motivation. Timeconditioningplaysacrucialroleinthediffusionprocess. WhiletheConvNeXt
blocksintheMulti-ResolutionNetworkefficientlyprocesshigh-resolutionfeatures,theyalsopresent
anewchallenge: HowdoweinjecttimeinformationintoConvNeXtblocks? Toaddressthis, we
carry out a systematic ablation study (details in Tab. 2), starting with the U-ViT architecture [3],
whichencodestimeinformationviaanin-contextconditioningmechanism,Time-Token(i.e.,treating
timeasinputtokentoTransformer). UnlikeTransformerblocks,however,itisnotfeasibletoadd
atimetokendirectlytoConvNeXtblocks,whichcanonlyprocess2Dfeatures. Asanalternative,
weexploredtheadaptivenormalizationmechanism,particularlytheadaptivelayernormalization
5AdaLN-Zero[40]. Interestingly,wefoundAdaLN-ZerotobemoreeffectivethanTime-Tokenon
theImageNet64×64benchmark,contradictingU-ViT’sfindingsonCIFAR-10[29](SeeFig.2(b)
in[3]). However,AdaLN-Zerosignificantlyincreasesmodelparameters(from130.9Mto202.4M)
duetotheMulti-LayerPerceptron(MLP)usedtoadaptivelylearnthescaleandshiftparameters.
TounderstandhowtimeinformationisutilizedinadaLN-
Zero, we conducted Principal Component Analysis
+ +
(PCA) on the learned scale (γ ,γ ) and shift (β ,β )
Scale 1 2 1 2
MLP parametersfromaparameter-heavyMLPinadaLN-Zero
MLP
Scale, Shift TD-LN using a pre-trained DiT-XL/2 [40] model, as shown
Layer Norm Layer Norm in Fig. 3. Intriguingly, we observed that the learned
parameters can be largely explained by two principal
+ +
Scale Multi-Head components, suggesting that a parameter-heavy MLP
Multi-Head Attention mightbeunnecessaryandthatasimplerfunctioncould
Attention
Scale, Shift TD-LN suffice. Toaddresstheincreaseinparameters,weintro-
Layer Norm MLP Layer Norm duceTime-DependentLayerNormalization(TD-LN),
astraightforwardandlightweightmethodtoinjecttime
Inp Tu rt a T no ske fons rmeT ri m Be locC kl ass Class TI rn ap nut s T fo ok re mns er Block Time intolayernormalization. Wedetailthedesignsbelow.
w/ adaLN-Zero w/ TD-LN
adaLN design. Building on layer normalization [2],
Figure 4: Time conditioning mecha-
adaLN additionally learns the scale parameter γ and
nisms. (Left) adaLN-Zero [40] learns 1
shift parameter β via an MLP from the sum of the
scale and shift parameters (γ , β , α , 1
i i i embeddingvectorsoftimetandclassconditionc. For-
i={1,2})usingparameter-heavyMLPs.
mally,giventheinputx(ignoringthedependencyont
(Right) The proposed Time-Dependent
forsimplicity),wehave:
LayerNormalization(TD-LN)formulates
theLNstatisticsasfunctionsoftime(γ(t), γ 1,β 1 =MLP(Embed(t)+Embed(c)), (3)
β(t)),makingitparameter-efficient. z =γ ·LN(x,γ,β)+β , (4)
1 1
whereγ andβ scaleandshifttheoutputfromthelayernormalizationLN,thefunctionEmbed
1 1
generatestheembeddingvectorsfortimetandclassconditionc,andzistheoutput. TheLNhas
itsownlearnableaffinetransformparametersγ andβ. adaLN-Zero[40]introducesanotherscale
parameterα ,obtainedfromthesameMLPforzeroinitializationofaresidualblock[12]. Wenote
1
thatDiTemploystwosetsof(γ ,β ,α )and(γ ,β ,α )inaTransformerblock,asshowninFig.4.
1 1 1 2 2 2
TD-LNdesign. Incontrast,ourproposedmethod,Time-DependentLayerNormalization(TD-LN),
directlyincorporatestimetintolayernormalizationbyformulatingLN’slearnableaffinetransform
parametersγ andβ asfunctionsoft. Motivatedbytheobservationthatthelearnedparametersof
adaLN-Zerocanbelargelyexplainedbytwoprincipalcomponents,weproposetomodelthisthrough
thelinearinterpolationoftwolearnableparametersp andp . Formally,
1 2
s(t)=Sigmoid(w·t+b), (5)
γ(t)=s(t)·p +(1−s(t))·p , (6)
1 2
wheres(t)isatransformationoftimet,wandbarethelearnableweightandbias,andSigmoidisthe
sigmoidactivationfunction. Theotheraffinetransformparameter,β(t),isformulatedsimilarlywith
anothertwoparametersp andp . Consequently,theproposedTD-LNisrepresentedasfollows:
3 4
z =LN(x,γ(t),β(t)). (7)
UnlikeadaLN,whichlearnsadditionalre-scalingγ andre-centeringβ variables,TD-LNdirectly
1 1
incorporates the time-dependent γ(t) and β(t) into layer normalization, eliminating the need for
aparameter-heavyMLP.Furthermore,TD-LNisaversatilemechanism,enablingtheinjectionof
time information into both Transformer blocks and ConvNeXt blocks. In DiMR, we replace all
layernormalizationswiththeproposedTD-LN,andtreattheclassconditioncasinputtokenforthe
Transformerblocks(ConvNeXtblocksdonottakeclassconditiontoken).
4.3 Micro-LevelDesign
Inadditiontothemajorarchitecturalmodificationsdiscussedearlier,wealsoexploreseveralmicro-
leveldesignchangestoenhancemodelperformance.
Multi-scaleloss. TheproposedMulti-ResolutionNetworkcomprisesRbranches,eachdedicatedto
processingfeaturesataspecificresolution,naturallyproducingmulti-scaleoutputs. Toleveragethis,
6weexploretrainingthenetworkwithamulti-scalelossL ,whichisaweightedsumofmean
multi
squarederrorlossateachresolution. Formally,themulti-scalelossisdefinedasfollows:
R
(cid:88)
L = α ·E ∥Downsample(ϵ ,r)−ϵ (x ,c,r)∥2, (8)
multi r t,x0,c,ϵt t θ t 2
r=1
whereα isthelossweightforther-thbranch,andDownsample(ϵ ,r)downsamplesthetargetnoise
r t
ϵ byafactorof2R−r usingaveragepooling(theR-thbranch,containingnodownsampling,isour
t
finaloutput). Wesetα =1/(2R−r×2R−r),motivatedbythepriorwork[21]whichfoundthatthe
r
signaltonoiseratioincreasesbyafactorofk2whenthenoisedinputisaverage-pooledwithak×k
kernel. Intuitively,ourtargetoutput(theR-thbranch)hasalossweightα =1,andthelossweights
R
fortheintermediateoutputsarescaleddownquadraticallybasedonthedownsamplingfactor.
Gatedlinearunit. IntheproposedMulti-ResolutionNetwork,bothTransformerandConvNeXt
blocksincludeanMLPblock,consistingoftwolineartransformationswithGeLUactivation[16]in
between. WealsoexplorereplacingthefirstlinearlayerwithGeGLU[50],anenhancedversionof
theGatedLinearUnit(GLU)[6]thathas2×expansionrate.
4.4 DiMRModelVariants
We now introduce the DiMR model variants, scaled appropriately for different model sizes. We
present three sizes: DiMR-M (medium, 133M parameters), DiMR-L (large, 284M parameters),
and DiMR-XL (extra-large, around 500M parameters). Three hyperparameters—R (number of
branches),N (numberoflayersperbranch),andD(hiddensizeperbranch)—defineeachDiMR
variant. Specifically, R determines the number of branches in the multi-resolution network. We
append2Ror3Rtothemodelnametoindicatewhethertwoorthreebranchesareused. Thenumber
oflayersN inthemulti-resolutionnetworkisrepresentedasatupleofRnumbers,wherether-th
numberspecifiesthenumberoflayersinther-thbranch. Similarly,thehiddensizeDisalsoatuple
ofRnumbers. ThemodelvariantsdetailsarepresentedinTab.3inSec.BintheAppendix.
5 ExperimentalResults
5.1 ExperimentalSetup
Datasets. Weconsiderclass-conditionalimagegenerationtasksat64×64,256×256,and512×512
resolutionsonImageNet-1K[7]. Forimagesat64×64,wetrainDiMRonpixelspace. Forimages
at256×256and512×512,followingthebaselines[3,40],weutilizeanoff-the-shelfpre-trained
variationalautoencoder[27]fromStableDiffusion[45]toextractthelatentrepresentationssizedat
32×32and64×64,respectively. ThenwetrainourDiMRtomodeltheselatentrepresentations.
Evaluation. Wemeasurethemodel’sperformanceusingFréchetInceptionDistance(FID)[17]. We
reportFIDon50Kgeneratedsamplestomeasuretheimagequality(i.e.,FID-50K).Toensurefair
comparisons,wefollowthesameevaluationsuiteasthebaseliness[8,40]tocomputetheFIDscores.
WealsoreportInceptionScore[48]andPrecision/Recall[30]inSec.Dassecondarymetrics.
Implementationdetails. ForafaircomparisonwithDiT[40],wetrainthe256×256and512×512
models for 1M iterations, and also report results for 500K iterations to compare with U-ViT [3].
We train the 64 × 64 models for 300K iterations, following the U-ViT protocol. Our training
hyperparameters are almost entirely retained from U-ViT [3]. We did not tune learning rates,
decay/warm-upschedules,Adamβ /β values,orweightdecays. Furtherdetailsonhyperparameters
1 2
andconfigurationsareprovidedinSec.CintheAppendix.
5.2 State-of-the-ArtDiffusionModels
WecompareDiMRwithstate-of-the-artdiffusionmodelsonImageNet256×256and512×512in
Tab.1,andprovidemorecomparisonswithothertypesofgenerativemodelsinTab.6andTab.7in
theAppendix. ResultsonImageNet64×64arereportedinTab.5intheAppendix. Morerandom
samplesofthegeneratedimagesarealsopresentedinFig.7toFig.18intheAppendix.
ImageNet256×256. FromTab.1a,weobservethatourDiMRoutperformsallpreviousdiffusion-
basedmodelsandachievesastate-of-the-artFID-50Kscoreof1.70. Specifically, withacompa-
7Table1: Class-conditionalimagegenerationonImageNet256×256andImageNet512×512.
Wereporttrainingepochs,numberofparameters(#Params),GFLOPs,andFID-50Kwithandwithout
Classifier-FreeGuidance(CFG).Bestresultsaremarkedinbold. OurDiMRachievesstate-of-the-art
results,andevensurpassesmostrecentlarge-scalemodelsonImageNet256×256(markedingray).
(a) ImageNet256×256 (b) ImageNet512×512
Model Epoch #Params. Gflops FID(w/oCFG)↓ FID↓ Model Epoch #Params. Gflops FID(w/oCFG)↓ FID↓
ADM-U[8] 396 608M 742 7.49 3.94 ADM[8] - 422M - 23.24 7.72
LDM-4 [45] 166 400M 104 10.56 3.60 ADM-U 1081 731M 2813 9.96 3.85
U-ViT-H/2 [3] 400 501M 133 6.58 2.29 U-ViT-L/4 [3] 400 287M 77 - 4.67
DiT-XL/2[40] 1399 675M 119 9.62 2.27 U-ViT-H/4 [3] 400 501M 133 15.70 4.05
Large-DiT-3B[1] 340 3.0B - - 2.10 DiT-XL/2[40] 599 675M 525 12.03 3.04
DiMR-XL/2R(Ours) 400 505M 160 4.87 1.77 DiMR-XL/3R(Ours) 400 525M 206 8.56 3.23
DiMR-XL/2R(Ours) 800 505M 160 4.50 1.70 DiMR-XL/3R(Ours) 800 525M 206 7.93 2.89
U-ViT
FID=2.29
Distortion rate
= 63.5%
DiT
FID=2.27
Distortion rate
= 71.0%
DiMR (Ours)
FID=1.77
Distortion rate
= 29.2%
Figure5: DiMRalleviatesdistortionsandimprovesvisualfidelity. Inthisfigure,werandomly
visualizethedetectedlow-fidelityimages,identifiedbyapretrainedclassifier,whicharegenerated
bythebestmodelsfromthebaselinesandourDiMR.ThefirstcolumnreportsboththeirFID-50K
scoresandtheproportionofdistortedimagesbasedonhumanevaluation. DiMRdemonstratesbetter
generationperformanceandlowerdistortionratesthanthebaselines.
rablemodelsizeandequalorfewertrainingepochs,ourmodelsurpassespreviousstate-of-the-art
transformer-baseddiffusionmodels,includingU-ViT[3](1.77vs.2.29withClassifier-FreeGuid-
ance[18](CFG)and4.87vs.6.58withoutCFG)andDiT[40](1.70vs.2.27withCFGand4.50
vs. 9.62 without CFG). Moreover, with only 505M parameters, our model even outperforms the
mostrecentlarge-scalediffusionmodel(Large-DiT[1],3.0B)byasignificantmargin(1.77vs.2.10),
indicatingthestrongcapabilitiesoftheproposedDiMR.
ImageNet512×512. OurDiMRoutperformsallpreviousdiffusion-basedmodelsonImageNet
512×512andachievesastate-of-the-artFID-50Kscoreof2.89asshowninTab.1b. Itisworth
notingthat,althoughbothGflopsandmodelsizesarecriticalforimprovingperformance,asdiscussed
intheDiTpaper[40],westilloutperformitwithonly39.2%oftheGFLOPsand77.8%ofthemodel
size,improvingtheFID-50Kfrom3.04to2.89.
5.3 AlleviatingDistortion
Transformer-basedarchitecturesencounterthechallengeofbalancingvisualfidelitywithcompu-
tational complexity. Despite adopting a small patch size of 2, current models still struggle with
distortions. To illustrate the effectiveness of DiMR in alleviating these distortions, we adopt a
classifier-basedrejectionmodelfollowingpreviouswork[44]. However,wedivergefromprevious
approachesbysolelyusingtherejectionmodeltoanalyzedistortedimages,ratherthanfilteringout
badimagesandcomputingmetricsonlyonselected‘good’images. Itisimportanttonotethatall
metricsinourpaperarecomputedwithoutusingtherejectionmodeltoensurefaircomparisons.
Specifically, we randomly generate 80K images for each model and utilize a pretrained Vision
Transformerclassifier[9]toidentifylow-fidelityimagesbasedonthepredictedprobabilities. Images
8Table2:Ablationstudy. Beginningwiththebaseline,weverifytheeffectivenessofeachcomponent.
Model AdaLN-Zero[40] TD-LN Multi-branch GLU[50] Multi-scaleLoss FID(↓) #Params.
1 Baseline(U-ViT-M/4[3]) 5.85 130.9M
2 ✓ 5.44 202.4M
3 ✓ ✓ 7.91 217.9M
4 ✓ ✓ 5.21 154.0M
5 ✓ ✓ ✓ 4.86 132.9M
6 DiMR-M/3R(Ours) ✓ ✓ ✓ ✓ 3.65 132.9M
withaprobabilitybelowathresholdof0.2areconsideredlow-fidelityorpotentiallydistorted. Fig.5
showsrandomsamplesoflow-fidelityimagesdetectedbytheclassifier. However,wefindthatnotall
detectedimagesaredistorted;manyareclassifiedwithlowprobabilityduetoclassifiererrors. To
accuratelyidentifydistortedimagesamongthosedetectedbytheclassifier,weconductuserstudies
wherehumanevaluatorsmanuallyassesstheimages. Imagesgeneratedbyallthreemethodsare
mergedandpresented,alongwiththeircorrespondingclasslabels,tohumanevaluators,whoare
instructedtodeterminewhethereachimageisdistorted(i.e., identifylow-fidelityimages). Each
imageisevaluatedbyfivedifferenthumanevaluators. Weconsidertheproportionofdistortedimages
generatedbydifferentmodels,i.e.distortionrate. Wecomputethreedistortionrates,oneforeach
model, fromeachevaluatorbasedontheimagestheyevaluate. Thefinaldistortionrateforeach
modelisobtainedbyaveragingtheratesfromallevaluators. AsreportedinFig.5,weobservethat
evenamongthoselow-fidelityimages,only29.2%oftheimagesgeneratedbyDiMRaredistorted,
whilepreviousmethodsyieldmuchhigherdistortionratesof63.5%and71.0%.
5.4 AblationStudies
We conduct ablation experiments on ImageNet 64×64, step by step starting from the baseline
U-ViT-M/4[3],toverifytheeffectivenessoftheproposeddesigns,aspresentedinTab.2.
AdaLN-Zero vs.TD-LN. Sincethe timetoken usedinU-ViT cannotbe adoptedforConvNeXt
blocks,wefirstapplyAdaLN-Zero[40]totheoriginalU-ViTandourmulti-branchnetwork. As
observedinrow2ofTab.2,AdaLN-ZeroslightlyimprovestheperformanceofU-ViTfrom5.85to
5.44. However,itdoesnotperformwellonConvNeXtblocksandthusdecreasestheperformance
from5.44to7.91(row3). Additionally,AdaLN-Zerosignificantlyincreasesthemodelsizefrom
130.9Mto202.4M.Incontrast,ourTD-LNismoreflexibleandparameter-efficient: itefficiently
providestimeinformationtobothTransformerblocksandConvNeXtblocks,improvingtheFID-50K
scorefrom7.91to5.21(row4),andalsoreducesthemodelsizefrom217.9Mto154.0M.
GLUfurtherreducesmodelsize. InTab.2,row5showstheimprovementofGLUcomparedwith
thevanillaMLPblock. WeobservethatusingGLUslightlyimprovestheperformancefrom5.21to
4.86andfurtherreducesthemodelsizefrom154.0Mto132.9M.
Multi-scale loss is critical for multi-resolution network. Training a multi-resolution network
presentsadditionalchallengesandcanresultinsub-optimalresults. InTab.2,row6illustratesthat
ourmulti-scalelosssignificantlyenhancestheperformance,achievingaFID-50Kscoreof3.65.
Multi-branchdesignimprovesvisualfidelityandalleviatesdistortionsinimagegenerations.
Finally,comparingthemulti-branchdesigninrow6(incorporatingTD-LN,GLU,andmulti-scaleloss
tofacilitatetraining)withthebaselineinrow1revealsasignificantimprovementinFID-50K,from
5.85to3.65,withjusta1.5%increaseinmodelsize(130.9Mto132.9M).Additionally,fromFig.5,
it’sevidentthatthemulti-branchdesigngeneratesimageswithhigherfidelityandlessdistortion.
6 Conclusion
Inthiswork,weintroduceDiMR,whichenhancesdiffusionmodelsthroughtheMulti-Resolution
Network,progressivelyrefiningfeaturesfromlowtohighresolutionsandeffectivelyreducingimage
distortion. Additionally,DiMRincorporatestheproposedparameter-efficientTime-DependentLayer
Normalization(TD-LN),furtherimprovingimagegenerationquality. TheeffectivenessofDiMRhas
beendemonstratedonthepopularclass-conditionalImageNetgenerationbenchmark,outperforming
prior methods and setting new state-of-the-art performance on diffusion-style generative models.
WehopethatDiMRwillinspirefuturedesignsofbothdenoisingnetworksandtimeconditioning
mechanisms,pavingthewayforevenmoreadvancedimagegenerationmodels.
9Acknowledgement: WethankXueqingDengandPengWangfortheirvaluablediscussionduring
Zhanpeng’sinternship.
Appendix
Intheappendix,weprovideadditionalinformationaslistedbelow:
• Sec.Aprovidesthedatasetinformationandlicenses.
• Sec.BprovidestheDiMRmodelvariants,scaledappropriatelyfordifferentmodelsizes.
• Sec.CprovidestheimplementationdetailsofDiMR.
• Sec.Dprovidesmorecomparisonwithothermethodsforclass-conditionalimagegeneration
onImageNet64×64,ImageNet256×256,andImageNet512×512.
• Sec.EprovidesdetailedintroductionandmoreresultsofthePrincipalComponentAnalysis
(PCA)onthescaleandshiftparameterslearnedinadaLN-Zero.
• Sec.FprovidesmoregeneratedimagesamplesbyDiMR.
• Sec.Gdiscussesthelimitationsofourmethod.
• Sec.Hdiscussesthepositivesocietalimpactsofourmethod.
• Sec.Idiscussesthepotentialriskofourmethodandsafeguardsthatwillbeputinplacefor
responsiblereleaseofourmodels.
A DatasetsInformationandLicenses
ImageNet: TheImageNet[7]dataset,containing1,281,167trainingand50,000validationimages
from1,000differentclasses,isastandardbenchmarkforimageclassificationandclass-conditional
imagegeneration. Forthetaskofclass-conditionalimagegeneration,theimagesaretypicallyresized
toaspecifiedsize,e.g.,64×64,256×256,or512×512.
License: CustomLicense,non-commercial. https://image-net.org/accessagreement
Datasetwebsite: https://image-net.org/
B DiMRModelVariants
WeintroducetheDiMRmodelvariants,scaledappropriatelyfordifferentmodelsizes. Wepresent
three sizes: DiMR-M (medium, 132.7M parameters), DiMR-L (large, 284.0M parameters), and
DiMR-XL(extra-large,around500Mparameters). Threehyperparameters—R(numberofbranches),
N (number of layers per branch), and D (hidden size per branch)—define each DiMR variant.
Specifically,Rdeterminesthenumberofbranchesinthemulti-resolutionnetwork. Weappend2R
or3Rtothemodelnametoindicatewhethertwoorthreebranchesareused. Thenumberoflayers
N inthemulti-resolutionnetworkisrepresentedasatupleofRnumbers,wherether-thnumber
specifiesthenumberoflayersinther-thbranch. Similarly,thehiddensizeDisalsoatupleofR
numbers. Wefollowastraightforwardscalingrule: mostlayersarestackedinthefirstbranch,which
isprocessedbyTransformerblocks,whiletheremainingbranchesuseonlyhalfthenumberoflayers
ofthefirstbranch. Additionally,whentheresolutionisdoubled,thehiddensizeisreducedbyafactor
oftwo. ThemodelvariantsareillustratedinTab.3.
Table3: DiMRfamily. ThespecificconfigurationofaDiMRvariantisdeterminedbythehyperpa-
rametersR(numberofbranches),N (numberoflayersperbranch),andD(hiddensizeperbranch).
model inputsize latentsize #branchesR #layersN hiddensizeD #params
DiMR-M/3R 64×64 - 3 (15,8,8) (768,384,192) 133M
DiMR-L/3R 64×64 - 3 (33,17,17) (768,384,192) 284M
DiMR-XL/2R 256×256 32×32 2 (39,20) (960,480) 505M
DiMR-XL/3R 512×512 64×64 3 (39,20,20) (960,480,240) 525M
10Table 4: Experimental setup of DiMR. Experimental settings for all DiMR variants, including
modelarchitectures,traininghyperparameters,trainingcosts,andsamplerinformation.
Model DiMR-M/3R DiMR-L/3R DiMR-XL/2R DiMR-XL/3R
Parameters 133M 284M 505M 525M
ImageResolution 64×64 64×64 256×256 512×512
Latentspace (cid:37) (cid:37) (cid:33) (cid:33)
Latentshape - - 32×32×4 64×64×4
Imagedecoder - - sd-vae-ft-ema sd-vae-ft-ema
NumberofbranchesR 3 3 2 3
Blocksin1stbranch Transformer Transformer Transformer Transformer
#Layers 15 33 39 39
#Dimensions 768 768 960 960
#Heads 12 12 16 16
Resolution 16×16 16×16 16×16 16×16
LossCoeffs. 1/16 1/16 1/4 1/16
Blocksin2ndbranch ConvNeXt ConvNeXt ConvNeXt ConvNeXt
#Layers 8 17 20 20
#Dimensions 384 384 480 480
Kernelsize 7×7 7×7 7×7 7×7
Resolution 32×32 32×32 32×32 32×32
LossCoeffs. 1/4 1/4 1 1/4
Blocksin3rdbranch ConvNeXt ConvNeXt - ConvNeXt
#Layers 8 17 - 20
#Dimensions 192 192 - 240
Kernelsize 7×7 7×7 - 7×7
Resolution 64×64 64×64 - 64×64
LossCoeffs. 1 1 - 1
Batchsize 1024 1024 1024 1024
Trainingiterations 300K 300K 1M 1M
Warm-upsteps 5K 5K 5K 5K
Optimizer AdamW AdamW AdamW AdamW
learningrate 3×10−4 3×10−4 2×10−4 2×10−4
Weightdecay 0.03 0.03 0.03 0.03
Betas (0.99,0.99) (0.99,0.99) (0.99,0.99) (0.99,0.99)
Trainingdevices 8A100 16A100 16A100 16A100
Trainingtime 62hours 80hours 172hours 288hours
Sampler DPM-Solver DPM-Solver DPM-Solver DPM-Solver
Samplingsteps 50 50 250 250
C ImplementationDetails
WeusetheAdamWoptimizer[35]withaconstantlearningrateof2×10−4formostexperiments,
exceptforthe64×64modelswhereweuse3×10−4. Wesettheweightdecayto0.03andthe
betasto(0.99,0.99)forallexperiments. Abatchsizeof1024isusedforallarchitectures. Fora
faircomparisonwithDiT[40],wetrainthe256×256and512×512modelsfor1Miterations,and
wealsoreportresultsfor500KiterationstocomparewithU-ViT[3]. Wetrainthe64×64models
for300Kiterations,followingtheU-ViTprotocol. Allexperimentsuse5Kstepsforwarm-up. We
presentthedetailedexperimentalsetupforallDiMRvariantsinTab.4.
11Table5: Class-conditionalimagegenerationonImageNet64×64(w/oclassifier-freeguidance).
MetricsincludeFréchetInceptionDistance(FID),InceptionScore(IS),Precision,andRecall,where
“↓”or“↑”indicatewhetherlowerorhighervaluesarebetter,respectively. “Type”: thetypeofthe
generativemodel. “Epoch”: thenumberofepochstrainedonImageNet[7]. “#Params”: thenumber
ofparametersinthemodel. “#Gflops”: thecomputationalcost. “Diff.”: Diffusionmodels.
Model Type Epoch #Params. Gflops FID(↓) IS(↑) Precision(↑) Recall(↑)
U-ViT-M/4[3] Diff. 240 131M 35 5.85 33.71 0.69 0.61
U-ViT-L/4[3] Diff. 240 287M 77 4.26 40.66 0.71 0.62
DiMR-M/3R(Ours) Diff. 240 133M 54 3.65 42.41 0.74 0.59
DiMR-L/3R(Ours) Diff. 240 284M 111 2.21 55.73 0.75 0.60
Table6: Class-conditionalimagegenerationonImageNet256×256(withclassifier-freeguid-
ance). MetricsincludeFréchetInceptionDistance(FID),InceptionScore(IS),Precision,andRecall,
where“↓”or“↑”indicatewhetherlowerorhighervaluesarebetter,respectively. Wereportresultsof
GAN-basedmodels(GAN),BERT-stylemasked-predictionmodels(Mask.),autoregressivemodels
(AR),visualautoregressivemodels(VAR),anddiffusionbasedmodels(Diff.).“Type”:thetypeofthe
generativemodel. “Epoch”: thenumberofepochstrainedonImageNet[7]. “#Params”: thenumber
ofparametersinthemodel. “#Gflops”: thecomputationalcost. “-re”: themodelsutilizerejection
sampling. “Mask. +Diff.": themodelsusingmasked-predictiontoimprovediffusionmodels.
Model Type Epoch #Params. Gflops FID(↓) IS(↑) Precision(↑) Recall(↑)
BigGAN[4] GAN - 112M - 6.95 224.5 0.89 0.38
GigaGAN[24] GAN - 569M - 3.45 225.5 0.84 0.61
MaskGIT[5] Mask. 300 227M - 6.18 182.1 0.80 0.51
MaskGIT-re[5] Mask. 300 227M 300 4.02 355.6 - -
RCG[32] Mask. 200 502M - 3.49 215.5 - -
MDT-G[11] Mask.+Diff. 1299 700M 121 1.79 283.0 0.81 0.61
VQGAN[10] AR 100 1.4B - 15.78 74.3 - -
VQGAN-re[10] AR 100 1.4B - 5.20 280.3 - -
ViTVQ[63] AR 100 1.7B - 4.17 175.1 - -
ViTVQ-re[63] AR 100 1.7B - 3.04 227.4 - -
RQTran[31] AR 50 3.8B - 7.55 134.0 - -
RQTran-re[31] AR 50 3.8B - 3.80 323.7 - -
VAR-d16[56] VAR 200 310M - 3.60 257.5 0.85 0.48
VAR-d20[56] VAR 250 600M - 2.95 306.1 0.84 0.53
VAR-d24[56] VAR 350 1.0B - 2.33 320.1 0.82 0.57
VAR-d30[56] VAR 350 2.0B - 1.97 334.7 0.81 0.61
VAR-d30-re[56] VAR 350 2.0B - 1.80 356.4 0.83 0.57
ADM-G[8] Diff. 396 554M - 4.59 186.7 0.82 0.52
ADM-G,ADM-U[8] Diff. 208 608M 742 3.94 215.8 0.83 0.53
CDM[20] Diff. 2158 - - 4.88 158.7 - -
LDM-4[45] Diff. 166 400M - 3.60 247.7 - -
DiT-L/2[40] Diff. 1399 458M 81 5.02 167.2 0.75 0.57
DiT-XL/2[40] Diff. 1399 675M 119 2.27 278.2 0.83 0.57
U-ViT-L/2[3] Diff. 240 287M 77 3.40 219.9 0.83 0.52
U-ViT-H/2[3] Diff. 400 501M 133 2.29 263.9 0.82 0.57
DIFFUSSM-XL[62] Diff. 515 673M 280 2.28 259.1 0.86 0.56
SiT-XL[36] Diff. 1399 675M 119 2.06 270.3 0.82 0.59
DiMR-XL/2R(Ours) Diff. 400 505M 160 1.77 285.7 0.79 0.62
DiMR-XL/2R(Ours) Diff. 800 505M 160 1.70 289.0 0.79 0.63
D AdditionalExperimentalResults
WepresentthefullresultsoftheproposedDiMRcomparedtoothermethodsonImageNet[7]in
termsofFréchetInceptionDistance(FID)[17],InceptionScore(IS)[48],andPrecision/Recall[30].
Thecomparisonsaremadeonclass-conditionalimagegenerationwithoutclassifier-freeguidanceon
ImageNet64×64inTab.5,andwithclassifier-freeguidance[18]onImageNet256×256inTab.6
andImageNet512×512inTab.7.
ImageNet64×64. WefollowtheexactexperimentalsetupofU-ViT[3]forclass-conditionalimage
generationonImageNet64×64withoutclassifier-freeguidancetoverifytheeffectivenessofour
proposedbackbone. Therefore, wefocussolelyoncomparingagainstU-ViTonthisbenchmark.
AsshowninTab.5,whenbotharetrainedfor240epochs,theproposedDiMR-M/3Rwith133M
parametersachievesanFIDof3.65andanISof42.41,improvinguponthecounterpartU-ViT-M/4
12Table7: Class-conditionalimagegenerationonImageNet512×512(withclassifier-freeguid-
ance). MetricsincludeFréchetInceptionDistance(FID),InceptionScore(IS),Precision,andRecall,
where“↓”or“↑”indicatewhetherlowerorhighervaluesarebetter,respectively. Wereportresultsof
GAN-basedmodels(GAN),BERT-stylemasked-predictionmodels(Mask.),autoregressivemodels
(AR),visualautoregressivemodels(VAR),anddiffusionbasedmodels(Diff.). “Type”: thetypeof
thegenerativemodel. “Epoch”: thenumberofepochstrainedonImageNet[7]. “#Params”: the
numberofparametersinthemodel. “#Gflops”: thecomputationalcost.
Model Type Epoch #Params. Gflops FID(↓) IS(↑) Precision(↑) Recall(↑)
BigGAN[4] GAN - 158M - 8.43 177.9 0.88 0.29
MaskGIT[5] Mask. 300 227M - 7.32 156.0 0.78 0.50
MaskGIT-re[5] Mask. 300 227M - 4.46 342.0 - -
VAR-d36-s[56] VAR 350 2.35B - 2.63 303.2 - -
ADM-G[8] Diff. - 422M - 7.72 172.7 0.87 0.42
ADM-G,ADM-U[8] Diff. 1081 731M 2813 3.85 221.7 0.84 0.53
DiT-XL/2[40] Diff. 599 675M 525 3.04 240.8 0.84 0.54
U-ViT-L/4[3] Diff. 400 287M 77 4.67 213.3 0.87 0.45
U-ViT-H/4[3] Diff. 400 501M 133 4.05 263.8 0.84 0.48
DIFFUSSM-XL[62] Diff. 236 673M 1066 3.41 255.0 0.85 0.49
DiMR-XL/3R(Ours) Diff. 400 525M 206 3.23 285.1 0.82 0.54
DiMR-XL/3R(Ours) Diff. 800 525M 206 2.89 289.8 0.83 0.55
with131Mparametersby2.20inFIDand8.70inIS.Forthelargermodel,DiMR-L/3Rwith284M
parametersoutperformsU-ViT-L/4with287Mparametersby2.05inFIDand15.07inIS.These
consistentandsignificantimprovementsdemonstratethecapabilityoftheproposedMulti-Resolution
NetworkandTD-LNinenhancingdiffusionmodelstogeneratehigh-fidelityimages.
ImageNet 256×256. We compare DiMR with state-of-the-art generative models on ImageNet
256×256 with classifier-free guidance in Tab. 6. Compared to U-ViT [3] in a fair setting, our
DiMR-XL/2Rwith505Mparameters,trainedfor400epochs,significantlyoutperformsU-ViT-H/2
with501Mparameters,alsotrainedfor400epochs,by0.52inFIDand21.8inIS.Incomparison
withtherecentlypopulardiffusionmodelDiT[40],DiMR-XL/2Rtrainedfor800epochsconsistently
outperformsDiT-L/2with458Mparametersby3.32inFIDand121.8inIS.DiMR-XL/2Reven
surpassesthelargervariantDiT-XL/2with675Mparametersby0.57inFIDand11.8inIS.Notably,
DiTmodelsrequiretrainingfor1399epochs,whileDiMR-XL/2Rachievessuperiorperformance
withonly800epochs. Asaresult,DiMR-XL/2Rsetsanewstate-of-the-artonImageNet256×256
imagegenerationwithanFIDof1.70andanISof289.0.
ImageNet 512×512. We compare DiMR with state-of-the-art generative models on ImageNet
512×512withclassifier-freeguidanceinTab.7. Underthesametrainingsettingfor400epochs,
ourDiMR-XL/3Rwith525MparametersoutperformsU-ViT-H/4with501Mparametersby0.82
inFIDand21.3inIS.ComparedtoDiT-XL/2with675Mparameters,DiMR-XL/3Rshowsslight
improvementsof0.15inFIDand49.0inIS.Consequently,DiMR-XL/3Rsetsthestate-of-the-art
performanceonImageNet512×512imagegenerationwhenusingdiffusion-basedgenerativemodels.
E AdditionalPCAofLearnedScaleandShiftParametersinadaLN-Zero
WeconductPCAonthelearnedscale(γ ,γ )andshift(β ,β )parametersobtainedfromaparameter-
1 2 1 2
heavyMLPinadaLN-Zerousingapre-trainedDiT-XL/2[40]modelwith28layersindepth. To
conducttheanalysis,weutilizethepre-trainedDiT-XL/2togenerateimagesandcollectedthescale
andshiftparameters(tensors)producedbytheMLPatdifferentlayersalongthesamplingsteps. PCA
isthenperformedonthecollectedtensorsateachlayerseparately. TheresultsarepresentedinFig.6,
whereeachrowofthefiguredisplaystheanalysisresultatdifferentdepths,fromtoptobottom: 7,
14,21,28. TheverticalaxisrepresentstheexplainedvarianceratioofthecorrespondingPrincipal
Components(PCs). Asobserved,inmostcases,themostimportantprincipalcomponentcanexplain
mostofthevariance,whilestartingfromthe3rdprincipalcomponent,itusuallyonlyaccountsfor
less than 5% of the variance. Our observations reveal that the learned parameters, regardless of
whetherproducedbyanMLPatashallowerlayeroradeeperlayer,canbelargelyexplainedbytwo
principalcomponents,suggestingthepotentialtoapproximatethembyasimplerfunction,TD-LN,
wherethelinearinterpolationoftwolearnableparametersislearnedasintroducedinSec.4.2.
13Figure6: PrincipalComponentAnalysis(PCA)oflearnedscaleandshiftparametersinadaLN-
Zero[40]. WeconductPCAonthelearnedscale(γ ,γ )andshift(β ,β )parametersobtainedfrom
1 2 1 2
aparameter-heavyMLPinadaLN-Zerousingapre-trainedDiT-XL/2[40]modelwith28layersin
depth. Eachrowofthefigurepresentstheanalysisresultatdifferentdepths,fromtoptobottom: 7,
14,21,28. TheverticalaxisrepresentstheexplainedvarianceratioofthecorrespondingPrincipal
Components(PCs). Ourobservationsrevealthatthelearnedparameterscanbelargelyexplainedby
twoprincipalcomponents,suggestingthepotentialtoapproximatethembyasimplerfunction.
F ModelSamples
Wepresentsamplesfromourlargestvariant,DiMR-XL/3R,at512×512resolutiontrainedfor800
epochs. Fig.7-18displayuncuratedsamplesfromthemodelacrossarangeofinputclasslabelswith
classifier-freeguidance. Itisworthnotingthatourgeneratedimagesamplesexhibithigh-qualityand
minimalimagedistortions.
G Limitations
TheproposedDiMRhasafewremaininglimitations. First,itfocusesonclass-conditionalimage
generation, rather than full text-to-image generation approaches. Additionally, although DiMR
demonstrates great scalability in our experiments, the exploration of DiMR variants stops at the
DiMR-XL/3Rmodelwith524.8Mparametersduetoconstraintsoncomputationalresources. In
contrast,afewrecentmethodshavescaledimagediffusionmodelstobillionsofparameters. We
leaveitasfutureworktofurtherexplorethescalinglawofourproposedDiMRtoenhanceitsimage
generationcapabilities.
14Figure7: Uncurated512×512DiMRsamples. Classlabel=‘arcticwolf’(270)
Figure8: Uncurated512×512DiMRsamples. Classlabel=‘volcano’(980)
Figure9: Uncurated512×512DiMRsamples. Classlabel=‘husky’(250)
15Figure10: Uncurated512×512DiMRsamples. Classlabel=‘sulphur-crestedcockatoo’(89)
Figure11: Uncurated512×512DiMRsamples. Classlabel=‘cliffdrop-off’(972)
Figure12: Uncurated512×512DiMRsamples. Classlabel=‘balloon’(417)
16Figure13: Uncurated512×512DiMRsamples. Classlabel=‘lion’(291)
Figure14: Uncurated512×512DiMRsamples. Classlabel=‘otter’(360)
Figure15: Uncurated512×512DiMRsamples. Classlabel=‘redpanda’(387)
17Figure16: Uncurated512×512DiMRsamples. Classlabel=‘panda’(388)
Figure17: Uncurated512×512DiMRsamples. Classlabel=‘coralreef’(973)
Figure18: Uncurated512×512DiMRsamples. Classlabel=‘macaw’(88)
18H BroaderImpacts
The proposed DiMR has the potential to facilitate numerous fields through its advanced image
generationcapabilities. Intherealmofcreativeindustries,DiMRcanenhancetheefficiencyand
creativity of artists and designers by generating high-fidelity images with fewer distortions. The
high-quality generated images can also contribute to research on synthetic datasets by creating
realisticimages,aidinginreducingtheannotationsrequiredfortrainingvisionmodels. However,
withtheseadvancementscomeethicalconsiderations,suchastheriskofgeneratingdeepfakesor
othermaliciouscontent. Itisthuscrucialtoimplementsafeguardstominimizepotentialharms.
I SafetyConcernsandSafeguards
GiventhepowerfulcapabilitiesofDiMR,itisessentialtoimplementrobustsafeguardstoaddress
potentialsafetyandethicalconcerns.Oneprimaryconcernisthemisuseofgeneratedcontent,suchas
thecreationofdeepfakes,whichcanleadtomisinformationandprivacyviolations.Tomitigatethis,it
isimportanttoestablishstrictaccesscontrolsandusagepoliciestopreventthemisuseofthesemodels
whenreleased. Transparencyinthetrainingdataandmodelarchitectureisalsocriticaltoensure
accountabilityandtoidentifypotentialbiasesthatcouldleadtoharmfuloutputs. Byprioritizing
thesesafeguards,wecanensuretheresponsibleuseofDiMRwhileminimizingpotentialrisks.
19References
[1] Alpha-VLLM. Large-dit-imagenet. https://github.com/Alpha-VLLM/LLaMA2-Accessory/tree/
f7fe19834b23e38f333403b91bb0330afe19f79e/Large-DiT-ImageNet,2024.
[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450,2016.
[3] FanBao,ShenNie,KaiwenXue,YueCao,ChongxuanLi,HangSu,andJunZhu. Allareworthwords:A
vitbackbonefordiffusionmodels. InCVPR,2023.
[4] AndrewBrock,JeffDonahue,andKarenSimonyan. Largescalegantrainingforhighfidelitynatural
imagesynthesis. arXivpreprintarXiv:1809.11096,2018.
[5] HuiwenChang,HanZhang,LuJiang,CeLiu,andWilliamTFreeman.Maskgit:Maskedgenerativeimage
transformer. InCVPR,2022.
[6] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated
convolutionalnetworks. InICML,2017.
[7] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.Imagenet:Alarge-scalehierarchical
imagedatabase. InCVPR,2009.
[8] PrafullaDhariwalandAlexanderNichol. Diffusionmodelsbeatgansonimagesynthesis. NeurIPS,2021.
[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,etal.Animageisworth
16x16words:Transformersforimagerecognitionatscale. InICLR,2021.
[10] PatrickEsser,RobinRombach,andBjornOmmer.Tamingtransformersforhigh-resolutionimagesynthesis.
InCVPR,2021.
[11] ShanghuaGao,PanZhou,Ming-MingCheng,andShuichengYan. Maskeddiffusiontransformerisa
strongimagesynthesizer. InICCV,2023.
[12] PriyaGoyal,PiotrDollár,RossGirshick,PieterNoordhuis,LukaszWesolowski,AapoKyrola,Andrew
Tulloch,YangqingJia,andKaimingHe.Accurate,largeminibatchsgd:Trainingimagenetin1hour.arXiv
preprintarXiv:1706.02677,2017.
[13] JiataoGu,ShuangfeiZhai,YizheZhang,JoshuaMSusskind,andNavdeepJaitly. Matryoshkadiffusion
models. InICLR,2023.
[14] Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and Arash Vahdat. Diffit: Diffusion vision
transformersforimagegeneration. arXivpreprintarXiv:2312.02139,2023.
[15] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecognition.
InCVPR,2016.
[16] DanHendrycksandKevinGimpel. Gaussianerrorlinearunits(gelus). arXivpreprintarXiv:1606.08415,
2016.
[17] MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,andSeppHochreiter. Gans
trainedbyatwotime-scaleupdateruleconvergetoalocalnashequilibrium. NeurIPS,2017.
[18] JonathanHoandTimSalimans. Classifier-freediffusionguidance. arXivpreprintarXiv:2207.12598,
2022.
[19] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. NeurIPS,2020.
[20] JonathanHo,ChitwanSaharia,WilliamChan,DavidJFleet,MohammadNorouzi,andTimSalimans.
Cascadeddiffusionmodelsforhighfidelityimagegeneration. JMLR,23(47):1–33,2022.
[21] EmielHoogeboom,JonathanHeek,andTimSalimans. simplediffusion:End-to-enddiffusionforhigh
resolutionimages. InICML,2023.
[22] AndrewGHoward,MenglongZhu,BoChen,DmitryKalenichenko,WeijunWang,TobiasWeyand,Marco
Andreetto,andHartwigAdam. Mobilenets: Efficientconvolutionalneuralnetworksformobilevision
applications. arXivpreprintarXiv:1704.04861,2017.
20[23] AapoHyvärinenandPeterDayan. Estimationofnon-normalizedstatisticalmodelsbyscorematching.
JMLR,6(4),2005.
[24] MingukKang,Jun-YanZhu,RichardZhang,JaesikPark,EliShechtman,SylvainParis,andTaesungPark.
Scalingupgansfortext-to-imagesynthesis. InCVPR,2023.
[25] TeroKarras,SamuliLaine,andTimoAila. Astyle-basedgeneratorarchitectureforgenerativeadversarial
networks. InCVPR,2019.
[26] TeroKarras,MiikaAittala,TimoAila,andSamuliLaine. Elucidatingthedesignspaceofdiffusion-based
generativemodels. NeurIPS,2022.
[27] DiederikPKingmaandMaxWelling. Auto-encodingvariationalbayes. arXivpreprintarXiv:1312.6114,
2013.
[28] ZhifengKong,WeiPing,JiajiHuang,KexinZhao,andBryanCatanzaro. Diffwave:Aversatilediffusion
modelforaudiosynthesis. arXivpreprintarXiv:2009.09761,2020.
[29] AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiplelayersoffeaturesfromtinyimages. 2009.
[30] TuomasKynkäänniemi,TeroKarras,SamuliLaine,JaakkoLehtinen,andTimoAila. Improvedprecision
andrecallmetricforassessinggenerativemodels. NeurIPS,2019.
[31] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image
generationusingresidualquantization. InCVPR,2022.
[32] TianhongLi,DinaKatabi,andKaimingHe. Self-conditionedimagegenerationviageneratingrepresenta-
tions. arXivpreprintarXiv:2312.03701,2023.
[33] XiangLi,JohnThickstun,IshaanGulrajani,PercySLiang,andTatsunoriBHashimoto. Diffusion-lm
improvescontrollabletextgeneration. NeurIPS,2022.
[34] ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeichtenhofer,TrevorDarrell,andSainingXie. A
convnetforthe2020s. InCVPR,2022.
[35] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101,2017.
[36] NanyeMa,MarkGoldstein,MichaelSAlbergo,NicholasMBoffi,EricVanden-Eijnden,andSainingXie.
Sit:Exploringflowanddiffusion-basedgenerativemodelswithscalableinterpolanttransformers. arXiv
preprintarXiv:2401.08740,2024.
[37] AlexNichol,PrafullaDhariwal,AdityaRamesh,PranavShyam,PamelaMishkin,BobMcGrew,Ilya
Sutskever,andMarkChen. Glide:Towardsphotorealisticimagegenerationandeditingwithtext-guided
diffusionmodels. arXivpreprintarXiv:2112.10741,2021.
[38] AlexanderQuinnNicholandPrafullaDhariwal. Improveddenoisingdiffusionprobabilisticmodels. In
ICML,2021.
[39] WeiliNie,BrandonGuo,YujiaHuang,ChaoweiXiao,ArashVahdat,andAnimaAnandkumar. Diffusion
modelsforadversarialpurification. arXivpreprintarXiv:2205.07460,2022.
[40] WilliamPeeblesandSainingXie. Scalablediffusionmodelswithtransformers. InICCV,2023.
[41] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual
reasoningwithageneralconditioninglayer. InAAAI,2018.
[42] DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann,TimDockhorn,JonasMüller,JoePenna,
andRobinRombach. Sdxl:Improvinglatentdiffusionmodelsforhigh-resolutionimagesynthesis. arXiv
preprintarXiv:2307.01952,2023.
[43] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen.Hierarchicaltext-conditional
imagegenerationwithcliplatents. arXivpreprintarXiv:2204.06125,2022.
[44] AliRazavi,AaronVandenOord,andOriolVinyals.Generatingdiversehigh-fidelityimageswithvq-vae-2.
NeurIPS,2019.
[45] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-resolution
imagesynthesiswithlatentdiffusionmodels. InCVPR,2022.
21[46] OlafRonneberger,PhilippFischer,andThomasBrox. U-net: Convolutionalnetworksforbiomedical
imagesegmentation. InMICCAI,2015.
[47] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar
Ghasemipour,RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,etal. Photorealistictext-to-
imagediffusionmodelswithdeeplanguageunderstanding. NeurIPS,2022.
[48] TimSalimans,IanGoodfellow,WojciechZaremba,VickiCheung,AlecRadford,andXiChen. Improved
techniquesfortraininggans. NeurIPS,2016.
[49] MarkSandler,AndrewHoward,MenglongZhu,AndreyZhmoginov,andLiang-ChiehChen. Mobilenetv2:
Invertedresidualsandlinearbottlenecks. InCVPR,2018.
[50] NoamShazeer. Gluvariantsimprovetransformer. arXivpreprintarXiv:2002.05202,2020.
[51] Wenzhe Shi, Jose Caballero, Ferenc Huszár, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel
Rueckert,andZehanWang. Real-timesingleimageandvideosuper-resolutionusinganefficientsub-pixel
convolutionalneuralnetwork. InCVPR,2016.
[52] JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,andSuryaGanguli.Deepunsupervisedlearning
usingnonequilibriumthermodynamics. InICML,2015.
[53] JiamingSong,ChenlinMeng,andStefanoErmon. Denoisingdiffusionimplicitmodels. arXivpreprint
arXiv:2010.02502,2020.
[54] YangSongandStefanoErmon. Generativemodelingbyestimatinggradientsofthedatadistribution.
NeurIPS,2019.
[55] YusukeTashiro,JiamingSong,YangSong,andStefanoErmon. Csdi:Conditionalscore-baseddiffusion
modelsforprobabilistictimeseriesimputation. NeurIPS,2021.
[56] KeyuTian,YiJiang,ZehuanYuan,BingyuePeng,andLiweiWang. Visualautoregressivemodeling:
Scalableimagegenerationvianext-scaleprediction. arXivpreprintarXiv:2404.02905,2024.
[57] ArashVahdat,FrancisWilliams,ZanGojcic,OrLitany,SanjaFidler,KarstenKreis,etal. Lion:Latent
pointdiffusionmodelsfor3dshapegeneration. NeurIPS,2022.
[58] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. NeurIPS,2017.
[59] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-
vocabularypanopticsegmentationwithtext-to-imagediffusionmodels. InCVPR,2023.
[60] MinkaiXu,LantaoYu,YangSong,ChenceShi,StefanoErmon,andJianTang. Geodiff: Ageometric
diffusionmodelformolecularconformationgeneration. arXivpreprintarXiv:2203.02923,2022.
[61] ZeyueXue,GuangluSong,QiushanGuo,BoxiaoLiu,ZhuofanZong,YuLiu,andPingLuo. Raphael:
Text-to-imagegenerationvialargemixtureofdiffusionpaths. NeurIPS,2024.
[62] JingNathanYan,JiataoGu,andAlexanderMRush. Diffusionmodelswithoutattention. arXivpreprint
arXiv:2311.18257,2023.
[63] JiahuiYu,XinLi,JingYuKoh,HanZhang,RuomingPang,JamesQin,AlexanderKu,YuanzhongXu,
JasonBaldridge,andYonghuiWu. Vector-quantizedimagemodelingwithimprovedvqgan. arXivpreprint
arXiv:2110.04627,2021.
22