1
Scene Graph Generation in Large-Size VHR
Satellite Imagery: A Large-Scale Dataset and A
Context-Aware Approach
Yansheng Li, Senior Member, IEEE, Linlin Wang, Tingzhu Wang, Xue Yang, Junwei Luo, Qi Wang, Senior
Member, IEEE,Youming Deng, Wenbin Wang, Xian Sun, Senior Member, IEEE, Haifeng
Li,Member,IEEE,BoDang,YongjunZhang,Member,IEEE,YiYu,andJunchiYan,SeniorMember,IEEE
Abstract—Scenegraphgeneration(SGG)insatelliteimagery(SAI)benefitspromotingintelligentunderstandingofgeospatialscenarios
fromperceptiontocognition.InSAI,objectsexhibitgreatvariationsinscalesandaspectratios,andthereexistrichrelationshipsbetween
objects(evenbetweenspatiallydisjointobjects),whichmakesitnecessarytoholisticallyconductSGGinlarge-sizevery-high-resolution
(VHR)SAI.However,thelackofSGGdatasetswithlarge-sizeVHRSAIhasconstrainedtheadvancementofSGGinSAI.Duetothe
complexity of large-size VHR SAI, mining triplets <subject, relationship, object> in large-size VHR SAI heavily relies on long-range
contextualreasoning.Consequently,SGGmodelsdesignedforsmall-sizenaturalimageryarenotdirectlyapplicabletolarge-sizeVHR
SAI.Toaddressthescarcityofdatasets,thispaperconstructsalarge-scaledatasetforSGGinlarge-sizeVHRSAIwithimagesizes
rangingfrom512×768to27,860×31,096pixels,namedRSG,encompassingover210,000objectsandmorethan400,000triplets.To
realizeSGGinlarge-sizeVHRSAI,weproposeacontext-awarecascadecognition(CAC)frameworktounderstandSAIatthreelevels:
object detection (OBD), pair pruning and relationship prediction. As a fundamental prerequisite for SGG in large-size SAI, a holistic
multi-classobjectdetectionnetwork(HOD-Net)thatcanflexiblyintegratemulti-scalecontextsisproposed.Withtheconsiderationthat
thereexistahugeamountofobjectpairsinlarge-sizeSAIbutonlyaminorityofobjectpairscontainmeaningfulrelationships,wedesign
apairproposalgeneration(PPG)networkviaadversarialreconstructiontoselecthigh-valuepairs.Furthermore,arelationshipprediction
networkwithcontext-awaremessaging(RPCM)isproposedtopredicttherelationshiptypesofthesepairs.Topromotethedevelopment
ofSGGinlarge-sizeVHRSAI,thispaperreleasesaSAI-orientedSGGtoolkitwithabout30OBDmethodsand10SGGmethods,and
developsabenchmarkbasedonRSGwhereourHOD-NetandRPCMsignificantlyoutperformthestate-of-the-artmethodsinbothOBD
andSGGtasks.TheRSGdatasetandSAI-orientedtoolkitwillbemadepubliclyavailableathttps://linlin-dev.github.io/project/RSG.
IndexTerms—Scenegraphgeneration(SGG)benchmark,large-sizesatelliteimagery,objectdetection,relationshipprediction.
✦
1 INTRODUCTION
SCENE graph generation (SGG) [1], [2], [3] in satellite
imagery (SAI) aims to detect objects and predict rela-
tionships between objects, where scene graph is a high-
level structured representation of SAI via extensive triplets
Yansheng Li, Linlin Wang, Tingzhu Wang, Junwei Luo, Bo Dang
and Yongjun Zhang are with School of Remote Sensing and Informa-
tion Engineering, Wuhan University, Wuhan 430079, China (e-mail: yan-
sheng.li@whu.edu.cn; wangll@whu.edu.cn; tingzhu.wang@whu.edu.cn; luo-
junwei@whu.edu.cn;bodang@whu.edu.cn;zhangyj@whu.edu.cn).
XueYangiswithOpenGVLab,ShanghaiAILaboratory,Shanghai200030,
China(e-mail:yangxue@pjlab.org.cn).
Qi Wang is with School of Artificial Intelligence, Optics and Electronics
(iOPEN), Northwestern Polytechnical University, Xi’an 710072, China (e-
mail:crabwq@gmail.com).
YoumingDengiswithDepartmentofComputerScience,CornellUniver-
sity,Ithaca14853,UnitedStates(e-mail:ymdeng@cs.cornell.edu).
Wenbin Wang is with College of Computer and Information Technology,
Yichang 443002, China Three Gorges University, China (e-mail: wangwen-
bin@ctgu.edu.cn).
Xian Sun is with Aerospace Information Research Institute, Chinese
AcademyofSciences,Beijing100190,China(e-mail:sunxian@mail.ie.ac.cn).
HaifengLiiswithSchoolofGeosciencesandInfo-Physics,CentralSouth
University,Changsha410083,China(e-mail:lihaifeng@csu.edu.cn).
Yi Yu is with School of Automation, Southeast University, Nanjing Fig. 1. Illustration of SGG in large-size VHR SAI. (a) and (c) show
210096,China(e-mail:yuyi@seu.edu.cn) OBDandSGGresultsinlarge-sizeVHRSAI,respectively.In(d),black
Junchi Yan is with School of Artificial Intelligence & Department of arrowsdenotesemanticrelationshipswhosepredictiononlydependson
Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong isolatedpairs,butredarrowsdenotesemanticrelationshipsthatshould
University,Shanghai200240,China(e-mail:yanjunchi@sjtu.edu.cn). beinferredwiththeaidofcontexts.
4202
nuJ
31
]VC.sc[
1v01490.6042:viXra2
<subject, relationship, object>. Compared with traditional object. (iii) High-value relationships considering contex-
perceptualunderstandingtasksinSAI,suchasobjectdetec- tual reasoning. Unlike natural imagery, the relationships
tion[4],[5],[6],sceneclassification[7],[8],[9]andsemantic between objects in SAI are rotationally invariant, i.e., they
segmentation [10], [11], [12], SGG in SAI works towards donotchangewiththerotationoftheimagery.Inaddition,
cognitively mining the high-value knowledge contained in manymeaningfulrelationshipsbetweenobjectsareheavily
SAI (e.g., in Fig. 1, <ship1, away from, dock1> can be context-dependent, and the same types of object pairs in
inferredbythespatiallayoutbetweenship1anddock1and SAI may correspond to different categories under different
the wake trailing of ship1). SGG in SAI benefits cognitive contexts.
interpretation tasks in SAI, such as image retrieval [13], To address the dataset scarcity problem, we construct
image captioning [14] and image visual question answer- RSG, a large-scale dataset with more than 210,000 ob-
ing [15], and further serves various applications, such as jects and over 400,000 triplets for SGG in large-size VHR
traffic planning [16], [17], circuit layout [18] and military SAI. In this dataset, (i) SAI with a spatial resolution of
situationawareness[19]. 0.15mto1miscollected,covering11categoriesofcomplex
As illustrated in Fig. 1(a) and Fig. 1(b), large-size VHR geospatial scenarios associated closely with human activi-
SAI allows for the retention of detailed information on ties worldwide (e.g., airports, ports, nuclear power stations
small objects without compromising the integrity on large and dams). (ii) Under the guidance of experts in SAI, all
objects, which provides mandatory support for predicting objects are classified into 48 fine-grained categories and
rich relationships between multi-scale objects. However, to precisely annotated with oriented bounding boxes (OBB),
thebestofourknowledge,thereisasignificantlackofboth and all relationships are annotated in accordance with 8
datasets and approaches for SGG in large-size VHR SAI. major categories including 58 fine-grained categories. (iii)
Therefore, there is an urgent need to explore datasets and All object pairs and their contained relationships are one-
approaches around SGG in large-size VHR SAI to pursue to-many annotated, and all relationship annotations are
thecognitiveunderstandingofSAI. absolute (unaffected by imagery rotation). In conclusion,
As shown in Table 1, existing datasets for the SGG task RSG hassignificant advantages overexisting OBDdatasets
in SAI [20], [21] face limitations concerning the number andSGGdatasetsinSAI.Tothebestofourknowledge,RSG
of objects, the number of triplets, and the image size. isthefirstlarge-scaledatasetforSGGinlarge-sizeVHRSAI.
Moreover, they tend to provide horizontal bounding box To advance the research on the cognitive understand-
(HBB)annotationswithtoomuchbackgrounddisturbance, ing of SAI, this paper points out a new challenging and
which does not provide accurate information for recogniz- meaningfultopic:SGGinlarge-sizeVHRSAI.Considering
ing relationships between man-made objects with distinct thecharacteristicsoflarge-sizeVHRSAI,potentialsolutions
oriented boundaries in SAI. For the object detection (OBD) shouldovercomethreemainproblems:(i)OBD.Multi-class
task in SAI, while datasets for OBD in SAI [5], [22], [23] OBD in large-size VHR SAI presents significant challenges,
are continually improving in object annotation granularity primarily due to the holistic detection for multi-class ob-
as well as annotation volume, they lack relationship an- jectswithdiverseobjectscales(e.g.,samecategory:bridges,
notations. Additionally, they also necessitate considerable docks; different categories: ”airplanes and aprons”, ”ships
improvements in terms of image size, object diversity, and and docks”), extreme aspect ratios (e.g., bridges, runways
scene complexity, all of which are crucial for SGG in SAI. and gravitydams), and arbitrary orientations (e.g., trucks,
Given these constraints, it appears unrealistic to train a ships, airplanes, bridges, runways). These factors make it
robustSGGmodelusingtheselimiteddatasets.Specifically, challengingtoholisticallydetecttheseobjectswhilepreserv-
some special characteristics should be considered while ingthecompletedetailsnecessaryforaccuraterelationship
annotating the triplets <subject, relationship, object> for prediction. (ii) Pair pruning. Large-size VHR SAI usually
SGG in SAI: (i) Large-size VHR SAI spanning diverse contains a large number of objects and a plethora of object
scenarios.InSAI,objectsexhibitlargevariations,andthere pairs (the number of which grows exponentially as the
exist rich relationships between objects in various complex number of objects), which inevitably results in excessive
scenarios, even between those not spatially intersecting. computation for exhaustive methods based on all object
Large-size VHR SAI retains the detailed information of pairs. Thus, it is necessary to prune object pairs without
small objects while simultaneously preserving the integrity losinghigh-valuepairs.Notably,thetripletsareusuallypar-
of large objects, which makes the predictions of relation- tially annotated in reality (i.e., many non-annotated object
ships between objects at different scales/ranges feasible. pairs still carry meaningful relationships), which suggests
(ii) Fine-grained objects with precise annotations. Given that some approaches treating pair sparsity as a binary
the top-down perspective inherent in SAI, objects such as classification problem (i.e., annotated pairs are treated as
ships, trucks and runways often appear in arbitrary orien- positive samples and non-annotated pairs are treated as
tations. Therefore, precise object annotations are necessary negativesamples)arenotreasonable.(iii)Relationshippre-
for accurate object localization and the exclusion of irrel- diction.Differentfromsmall-sizenaturalimagery,thereare
evant background elements. Notably, fine-grained object a large number of object pairs containing rich knowledge,
categories are also indispensable for practical applications which are even spatially disjoint in large-size SAI. A few
of SGG in SAI. For instance, when investigating parking relationships in large-size SAI can be directly predicted by
violations<car,incorrectlyparkedon,truckparking>,itis isolated pairs, while most high-value relationships need to
crucialtodistinguishnotonlybetweenfine-grainedvehicle beinferredwiththehelpofcontext.AsillustratedinFig.1(c)
categories (truck/car) for subject, but also between fine- and Fig. 1(d), only simple triplets (e.g., <crane1, over,
grained parking categories (truckparking/carparking) for dock1>) in large-size SAI can be predicted when relying3
TABLE1
ComparisonoftheRSGdatasetwithotherdatasetsforOBDandSGGtasksinSAI(onlyfordatasetssourcedfromsatelliteplatformsand
coveringGSD<1m).Thecomparisonincludesimagesize,objectannotationtype,groundsamplingdistance(GSD),objectinformation(the
numberofinstances,thenumberofclasses,andthenumberofobjectsperimage(OPI)),relationshipinformation(thenumberoftriplets,the
numberofclasses,andthenumberofrelationshipsperimage(RPI)),source,andpublic.
Object Relationship
Annotation
Tasktype Dataset Imagesize GSD Num.of Num.of Num.of Num.of Num.of Num.of Venue Public
type
instances classes OPI triplets classes RPI
NWPUVHR-10[24] ∼1,000 HBB 0.08∼2 3,775 10 4.8 - - - TGRS’16 ✓
DOTA-v1.0[4] 800∼13,000 OBB 0.5 188,282 15 67.1 - - - CVPR’18 ✓
HRRSD[25] 152∼10,569 HBB 0.15∼1.2 55,740 13 2.6 - - - TGRS’19 ✓
OBD DIOR[22],[26] 800 OBB 0.5∼30 190,288 20 8.1 - - - ISPRS’20 ✓
DOTA-v2.0[23] 800∼20,000 OBB 0.5 1,973,658 18 159.2 - - - TPAMI’21 ✓
FAIR1M[5] 1,000∼10,000 OBB 0.3∼0.8 1,020,000 37 66.8 - - - ISPRS’22 ✓
GLH-Bridge[27] 2,048∼16,384 OBB 0.3∼1 59,737 1 10.0 - - - TPAMI’24 ✓
RSSGD[21] 224 HBB - - 39 - - 13 - ISPRS’21 ×
SGG GRTRD[20] 600 HBB 0.5 19,904 12 6.2 18,602 33 5.8 GRSL’21 ×
RSG(Ours) 512∼31,096 OBB 0.15∼1.0 219,120 48 172.1 400,795 58 314.8 - ✓
onlyonisolatedpairs.However,morecomplextripletscan SAI. The toolkit unifies the SGG process in natural
beinferredwhenthecontextofthepairsisintroduced(e.g., imageryandSAI,offeringflexibilityandeaseofuse.
<ship2,parallellydockedat,dock1>and<ship5,parallelly Ourtoolkitwillprovidevaluablecontributionstothe
dockedat,dock2>canbeinferredas<ship2,dockingatthe developmentoftheSGGcommunity.
differentdockwith,ship5>). • Rigorous experiments on the RSG dataset, employ-
Facing the aforementioned problems, we propose a ingstate-of-the-artalgorithms,underscoretheindis-
context-aware cascade cognition (CAC) framework to un- pensability of this dataset and the efficacy of the
derstand SAI at three levels: OBD, pair pruning and rela- algorithms. The experimental results showcased in
tionshipprediction.First,aholisticmulti-classobjectdetec- this paper serve as a benchmark for future research
tion network (HOD-Net) incorporating multi-scale context inthisdirection.
is adopted to detect multi-scale objects in large-size VHR
The rest of this paper is organized as follows. Section 2
SAI. Then, to reduce the complexity of predicting pairs
reviewstherelatedwork.Section3introducestheproposed
withoutlosinghigh-valuepairs,apairproposalgeneration
RSGdataset.Section4introducestheproposedCACframe-
(PPG)networkviaadversarialreconstructionisdesignedto
work for SGG in large-size VHR SAI. Section 5 reports the
obtain contextual interaction pairs containing rich knowl-
experiments and provides a discussion of the experimental
edge. Finally, considering the dependence of relationship
results.Section6givestheconclusion.
prediction in large-size SAI on the contexts of object pairs,
a relationship prediction network with context-aware mes-
saging(RPCM)isproposedtocomprehensivelypredictthe 2 RELATED WORK
relationshiptypes.
In this section, we provide a concise review of the most
In summary, the RSG dataset and CAC framework pro-
pertinent studies in the field, encompassing datasets and
posed in this paper aim to establish a benchmark for SGG
methodsforSGGinbothnaturalimageryandSAI.
in large-size VHR SAI. With the help of this benchmark,
more innovative algorithms can be developed to facilitate
2.1 SceneGraphGenerationDatasets
thecognitiveunderstandingofSAI.Themaincontributions
ofthisworkare: Scene graph has attracted extensive attention from many
• We propose RSG, the first large-scale dataset for researchers as a powerful tool for the understanding and
SGG in large-size VHR SAI. Containing more than reasoning analysis of images [14], [15], [28], [29], [30]. Vari-
210,000objectsandover400,000tripletsacross1,273 ousdatasetshavebeenproposedforSGGinrecentyears.
complexscenariosglobally,theRSGdatasetbenefits
infacilitatingthedevelopmentofSAI-orientedSGG. 2.1.1 Scene Graph Generation Datasets in Natural Im-
• To address SGG in large-size VHR SAI, the CAC agery
framework is constructed where HOD-Net is pro- VisualPhrases[31]isanearlydatasetforvisualrelationship
posedforholisticdetectionofmulti-classmulti-scale detection,whichisatasksimilartoSGG,andithas8object
objects in large-size SAI, and then the PPG network categoriesand13relationshipcategories.RW-SGD[13]may
and RPCM network are designed to retain high- be the first dataset for SGG, is constructed by gathering
value pairs as well as to predict relationship types 5,000 images from YFCC100m [32] and Microsoft COCO
ofthesepairs. datasets[33],containing93,832objectinstancesand112,707
• We release a versatile toolkit, encompassing about relationship instances. VRD [34] is constructed for the task
30 OBD methods and 10 SGG methods, designed of visual relationship detection, which has 100 object cate-
to accommodate OBD and SGG in large-size VHR gories from 5,000 images and contains 37,993 relationship4
Fig.2.ThegeographicaldistributionmapofthesampledimagesfromtheproposedRSGdataset.
instances. Visual Genome (VG) [35] is a large-scale scene very sparse, and many object pairs containing high-value
graphdataset,whichconsistsof108,077imageswithtensof relationshipsarecorrupted,whichmakesitdifficulttosup-
thousands of object and relationship categories. Later, VG porthigh-levelcognitiveunderstandingofSAI.Inaddition,
variants [36], [37] are gradually introduced by researchers theobjectsintheabovedatasetareannotatedbyHBBwith
to address its key shortcomings. Specifically, VG-150 [36] toomuchbackgrounddisturbance,whichdoesnotprovide
retains only the most common 150 object categories and 50 accurate information for recognizing relationships between
predicatecategoriestoreflectamorerealisticscenario.VrR- regularman-madeobjectswithdistinctorientedboundaries
VG [37] argues that many predicates in VG-150 could be in large-size VHR SAI. To meet the need for cognitive
easilyestimatedbystatistics,andthenre-filterstheoriginal understanding of SAI, a large-scale SGG dataset named
VGcategories.However,therearestillcasesofredundancy RSG for large-size VHR SAI is constructed, which contains
and ambiguity of predicates in VrR-VG. Similar shortcom- multiple complex scenarios and meaningful relationships.
ingsarealsofoundinthepopulardatasetsGQA[38],Open In Table 1, we give the statistics of popular OBD datasets
Images V4 and V6 [39]. In addition, the co-occurrence of andSGGdatasetsinSAI.Comparedtosmall-scaledatasets
multiple relationships between subject-object pairs is com- fortheSGGtaskinSAI,suchasRSSGDdatasetandGRTRD
monintherealworld,butmostpreviousdatasetstreatedge dataset, our RSG outperforms them by at least an order of
predictionassingle-labelcategorization. magnitude in both the number of objects and relationships
onall/singleimages.ForOBDdatasetsinSAI[23],[24],[25]
2.1.2 SceneGraphGenerationDatasetsinSAI lacking relationship annotations, our RSG dataset provides
effective improvements in both object category diversity
Comparedtothenaturalimageryfield,thedevelopmentof
and scene complexity. The RSG dataset, a large-scale and
SGG in SAI field has been relatively slow. SGG datasets in
comprehensive SGG dataset, supports both the OBD task
SAI [20], [21], [40], [41] with detailed annotation and con-
and the SGG task based on the HBB-based and OBB-based
sideringthecharacteristicsofSAIarerare.GRTRD[20]may
detectors in large-size VHR SAI, serving as an invaluable
be the first SGG dataset in SAI, which focuses on 12 kinds
resourceforcognitiveunderstandingofSAI.
of objects and contains 19,904 object instances and 18,602
relationship instances. RSSGD [21] is a SAI-oriented SGG
dataset constructed based on the remote sensing caption
dataset RSICD [42], which has about 39 object categories
and16relationshipcategories.Unfortunately,theimagesize 2.2 SceneGraphGenerationMethods
of GRTRD dataset is 600×600 pixels, and the image size of
RSSGD dataset is only 224×224 pixels. Since the large-size In the following sections, we discuss existing methods for
VHR SAI scenarios are cropped into small image blocks, the SGG task and the challenges faced by the SGG task in
the objects and relationships contained in each image are large-sizeVHRSAI.5
Fig.3.Statisticsandvisualizationofobjects(a)andrelationships(b)fromtheRSGdataset.Therelationshipsarecolor-codedtoshowparking
statu,spatialtopologyandfunctionaldescription,movementstatus,distancewarning,circuitlayout,constructionstatusandemissionstatus.Some
typicalobjectsandinterestingtripletsarevisualized.
2.2.1 SceneGraphGenerationMethodsinNaturalImagery [52], [53], [54] have consecutively explored the messaging
mechanisms of objects and relationships. Most recently,
ExistingSGGmethodsfornaturalimageryhavebeendom- HetSGG[47]attemptedtocapturetherelationshipsbetween
inated by the two-stage process consisting of OBD and objects in more detail by proposing a heterogeneous graph
relationshipprediction betweenobjects. Giventhe needfor and proved its effectiveness. However, the above methods
compatibility between OBD models and relationship pre- of exhaustive enumeration based on all pairs tend to result
diction models within the SGG framework, the majority of in excessive computation, which makes it difficult to run
existing SGG models, in alignment with the literature [43], under common computational resources (e.g., 24 GB of
adopt Faster R-CNN as the standard for the OBD task. To memory in a single GPU) for large-size VHR SAI scenarios
generatehigh-qualityscenegraphsfromimages,aseriesof withalargenumberofobjectsandmoreobjectpairs.
workshavebeenexploredinseveraldirections,suchastask-
specific [36], [44], [45], [46], [47] and dataset-specific [43], 2.2.2 SceneGraphGenerationMethodsinSAI
[48], [49]. Notably, aggregating contextual information be- Limited by a few semantic understanding datasets in SAI,
tween objects has been shown to be effective for SGG. Xu there are only a few approaches related to SGG in SAI.
et al. [36] first combined contextual information to refine ShiandZou[55]proposedaframeworkforSAIcaptioning
the features of objects and relationships by using GRU [50] by exploring whether machines can automatically generate
to pass messages between graphs. Based on the discovery human-like linguistic descriptions of SAI, and this work
that there are some inherent relationship patterns in sub- provides an initial exploration of high-level understanding
graphs, Motifs [44] constructed a new global context com- of SAI. Chen et al. [20] proposed a geospatial relation-
puting mechanism. A series of context-based studies [51], ship triplet representation dataset (GRTRD) based on the6
1.802% apron adjacent 1.084%
0 0. .0 32 14 8% % ba bs ae sb ka el tl b_ ad li la _m coo un rd t approach 0.905%
0.381% runway around 0.133%
0.087% soccerball_field
5.138% taxiway away from 0.509%
0 0. .1 38 94 8% % tenn ti es r_ mco inu art l connect 4.321%
converge 1.686%
9.210% airplane co-storage with 3.029%
directlyconnected to 0.283%
directlytransmitelectricity to 0.107%
5.898% boat dockedalongsidewith 1.467%
dockingatthedifferent dock with 0.549%
dockingatthesamebreakwaterwith 0.027%
dockingatthesamedockwith 2.474%
42.430% car drive off 0.117%
drive toward 0.131%
1.452% dock driving alongside with 0.300%
0.025% gas_station drivinginthedifferentlane with 1.886%
0.366% intersection drivingintheoppositedirectionwith 0.516%
0.052% roundabout driving in the same direction with 3.812%
drivinginthesamelanewith 1.992%
3.901% ship exhaustto 0.035%
inthedifferent parking with 0.748%
0.017% ship_lock inthesameparkingwith 5.284%
0.023% toll_gate incorrectlyparkedon 0.108%
indirectlyconnectedto 0.050%
2.388% truck indirectlytransmitelectricity to 0.044%
intersect 2.709%
0.073% ground_track_field isolatedly docked at 0.110%
0.010% stadium isolatedly parked on 0.228%
03
00
..
..
08
02
38
37
63
17
%%
%%
boar gd
b
ri
f
arn
l
ve
og
a
io_
tk
d
yb
w
__r ddi ad aatg
e
mmre n
n
no
o
ot
t
t
c
d
po
o
a-
c
rs
k
kto
e
er
d
da
a
ag
l
le
o
o
n
nw
g
git
s
sh
i id de
e
w wi it th
h
04
0
..
.
62
3
94
7
50
1
%%
%
0.073% vapor not run along 0.033%
6.892% tank not working on 0.021%
2.760% lattice_tower over 6.573%
0.152% substation parallelly docked at 3.350%
0.379% bridge parallelly parked on 9.386%
0.131% chimney parked alongside with 12.341%
0.173% genset parkinginthedifferent apron with 11.790%
parkinginthesameapronwith 9.085%
2.994% car_parking
0.253% truck_parking
pass across 0.367%
0.371% tower_crane pass through 0.009%
0.095% unfinished_building pass under 0.004%
0 3. .0 11 62 9%
%
arch_ crd aa nm
e
randomly docked at 0.295%
1.742% goods_yard randomly parked on 0.380%
1 0. .1 03 25 3% % cement_concrete_s pto ar ve eh mou ense t run along 0.207%
0 0. .1 01 54 9% % cooling_ st mow oker e r r ru u un n nn n ni i in n ng g g a aa l ll o oo n nn g gg t tt h hh e ee d sd aii ff mff ee err ee tann xtt ir t wau axn yiw w wa a iy y t hw wi it th h 0 0 0. . .0 2 00 1 54 5 6% % %
00 .. 00 77 12 %% containmec no ta _l_ vy esa sr ed l slightly emit 0.030%
0.592% wind_mill supply to 0.089%
0.293% engineering_vehicle through 0.426%
0.041% foundation_pit
violently emit 0.034%
within danger distance of 0.752%
within different line of 0.435%
within safe distance of 2.493%
within same line of 1.389%
working on 0.287%
Fig.4.Interactionmappingbetweenobjectsandrelationships.Thereareeightcolorsinthefigure,whichrepresentthetypesofeightrelationships:
parkingstatus(inpurple),spatialtopology(inblue)andfunctionaldescription(inorange),movementstatus(inred),distancewarning(ingrey),
circuitlayout(ingold),constructionstatus(ingreen)andemissionstatus(inbrown).Thevaluesoneithersideindicatetheproportionofeachobject
categoryandrelationshipcategory,respectively.
characteristics of high-resolution SAI and then adoptedthe Therefore,itisnecessarytodevelopanovelSGGmethod
”object-relationship”message-passingmechanismtorealize thatcaneffectivelyselectpairscontainingrichknowledgeas
thepredictionofgeographicobjectsandgeospatialrelation- wellascorrectlypredictrelationshiptypesofthesepairsin
ships. Li et al. [21] proposed a multi-scale semantic fusion large-sizeVHRSAI.
network (MSFN) for SGG in SAI based on their dataset
RSSGD, which integrates global information through a
graph convolution network to improve the accuracy of 3 DETAILS OF THE RSG DATASET
SGG.However,theabovemethodsaremainlydesignedfor
3.1 ImageCollectionandAnnotation
small-size SAI with HBB and do not take into account the
characteristics of large-size VHR SAI, such as the extreme 3.1.1 ImageCollection
aspect ratio of the objects, the great scale variation among
To meet the needs of practical applications, images in the
the objects, and the random orientation of the objects. Fur-
RSGdatasetarecollectedfromGoogleEarth,withaspatial
thermore,duetothelimitationofthedataset,thesemethods
resolutionrangingfrom0.15mto1m.Consideringthevalue
do not have the ability to predict relationships between
ofcognitiveunderstandingoflarge-sizeVHRSAIscenarios
objectsseparatedatlong-rangeinlarge-sizeVHRSAI.
inpracticalapplicationscloselyrelatedtohumanactivities,7
such as transportation, energy, and life, we collected sce-
narios from more than 1,200 airports, ports, wind power
stations, nuclear power stations, thermal power stations,
construction sites, sports, service areas, toll stations, traffic
bridges,anddamsfromallovertheworld.Unlikeobjectsin
natural imagery, objects in SAI are oriented in a variety of
directions.Therefore,HBB-basedannotationcannotprovide
accurate spatial information for oriented objects. To wrap
objects more accurately and develop more suitable algo-
rithmsfororientedobjectsinSAI,allobjectinstancesinthe
RSGdatasetareannotatedwithOBB.Samplesofannotated
instancesintheRSGdatasetcanbeseeninFig.2.
3.1.2 AnnotationCriteria Fig.5.Examplesofintra-classvariationsandinter-classsimilaritiesin
relationshipontheRSGdataset.
The construction of the SGG dataset for large-size VHR
SAI mainly contains two tasks: object annotation and re-
lationship annotation. Referring to the category system of withrichinterpretationexperienceinSAI,andeachimageis
mainstream OBD datasets in SAI, and combining with the annotatedby2professionalsandcheckedby1professional
actualrequirementsoftheSGGtaskandsomedownstream at the same time. During the quality-checking stage, cross-
tasks such as visual question answer and image caption, samplingandrevisionswereperformedbymainauthors.
we carry out detailed interpretation and value analysis of Specifically, we use RoLabelImg4 to manually gener-
the object categories appearing in the above scenarios, and ate fine-grained oriented labeled boxes for the objects.
finally choose 48 objects categories as shown in Fig. 3(a). Each object is represented by four clockwise-aligned an-
Specifically, we select to annotate ”smoke” and ”vapor” gular coordinates (x 1,y 1,x 2,y 2,x 3,y 3,x 4,y 4) and category
becausetheycanbeusedasabasisforjudgingtheworking labels, and each triplet is represented by <subject, rela-
statusofpowerplants.Inaddition,thecollectionandanno- tionship, object>. For a more meaningful annotation, all
tation of objects like ”containmentvessel”, ”latticetower”, pre-annotated pairs (both long-range and short-range) are
”substation” and ”gravitydam” are expected to play an selectedbasedontheinteractiongraphinFig.4duringthe
importantroleinSAI-basedelectricityresearch[18]. relationshipannotationprocess.
For the relationship description, unlike the ”human-eye
view” perspective of natural imagery, the ”birds-eye view”
3.2 DataCharacteristicsandAnalysis
perspective in SAI makes the semantic relationships be-
3.2.1 DataCharacteristics
tweenobjectsabsolute,sowediscardtheorientationwords
suchas”east”,”south”,”west”,and”north”.Moreover,we Compared with the existing OBD dataset in SAI, the RSG
have learned a lesson from the ambiguous labeling of SGG dataset has unique properties on task diversity, category
datasets in natural imagery and removed relationship de- diversity, and scale diversity. Sparse high-value pairs, high
scriptionswhosesemanticsareobviouslyambiguous.After intra-class variation and inter-class similarity are also im-
carefulscreening,thesemanticrelationshipsaredefinedinto portantcharacteristicsoftheRSGdataset.
8 major categories: ”distance warning”, ”spatial topology”, Task diversity. RSG supports both the OBD task and
”functional description”, ”circuit layout”, ”movement sta- SGG task in large-size VHR SAI. After processing the
tus”,”emissionstatus”,”constructionstatus”and”parking annotated images, it can further satisfy the various task
status”,whichcontain58subcategoriesasinFig.3(b). requirementssuchasOBDbasedonHBB/OBBinlarge-size
Inaddition,Fig.4givestheinteractionmappingbetween VHRSAI,SGGbasedonHBB/OBBinlarge-sizeVHRSAI,
objects and relationships. There are eight colors in the fig- andsoon.
ure,representingtheeightmajorcategoriesofrelationships Category diversity. RSG has various scene categories,
consistent with Fig. 3(b). It can be seen that almost ev- various object categories, and various relationship cate-
ery relationship category is associated with multiple object gories. Specifically, it contains 11 categories of VHR SAI
categories, which reflects the complex interactions between scenariosovertheworld,48categoriesofimportantobjects,
objectsinlarge-sizeVHRSAI. and 58 categories of high-value relationships, which will
bringnewopportunitiesforcognitiveunderstandingofSAI.
3.1.3 AnnotationManagement Scale diversity. In the RSG dataset, there are many
RSG dataset has a standardized dataset construction typical objects with extreme aspect ratios (e.g., runways,
pipeline: pre-annotation and rules refinement stage, large- bridges, gravitydams), and there are large scale variations
scaledetailedannotationstage,andquality-checkingstage. betweenobjectsofthesamecategory(e.g.,airplanes,ships)
Inthepre-annotationstage,6expertsintheSAIfieldarefirst and different categories (e.g., ”boats and docks”, ”cars and
invited to formulate annotation rules, and then the main carparkings”) that contain rich knowledge, which will
authorspre-annotatetheimageswith11typesofscenarios, bring great challenges to the OBD and SGG tasks in large-
whichareexaminedandevaluatedbytheseexperts,thereby sizeVHRSAI.
forming a detailed object and relationship annotation doc- Sparsehigh-valuepairs.Thereareanenormousnumber
ument. During the large-scale annotation stage, the main of objects in a large-size SAI. If relationship prediction is
authors provide comprehensive training to 9 professionals performedforallobjectpairs,morethanN(N −1)triplets8
Fig. 6. The overall architecture of the CAC framework. First, multi-scale objects in large-size VHR SAI are holistically detected by HOD-Net.
Then,contextualinteractionpairscontainingrichknowledgeareselectedbythePPGnetworkviaadversarialreconstruction.Finally,high-value
relationshipsbetweensubjectandobjectarepredictedbytheRPCMnetwork.
willappearforN objectsinthecaseofone-to-manyrelation- pruningandrelationshipprediction.Consideringthedrastic
ships (e.g., in the RSG dataset where the number of objects changesinthescalesandaspectratiosofobjectsinlarge-size
inoneimageisupto6,800,thepossibletripletswillbemore VHR SAI, a HOD-Net that flexibly integrates multi-scale
than40,000,000),whichwillcausememoryoverflowunder contextsisproposedfortheholisticdetectionofmulti-class
commoncomputationalresources.Infact,notallobjectpairs objects. Facing the problem of memory overflow caused
are of interest to us, and only a few of them contain high- by pair redundancy in large-size VHR SAI under common
valuerelationships.Therefore,itwillbeanewchallengeto computational resources, we propose the PPG network via
selecthigh-valuepairsfromthenumerouspairsfortheSGG adversarial reconstruction to obtain object pair ranking
taskinlarge-sizeVHRSAI. scores, which can efficiently sift out contextual interaction
High intra-class variation and inter-class similarity. pairscontainingrichknowledgeundertheconditionofno-
Intra-class variation of relationships is mainly due to the negative samples. Considering the dependence of relation-
differentialappearanceanddiversesubject-objectpairs.The ship prediction in large-size SAI on the contexts of object
inter-classsimilarityofrelationshipsarisesfromsimilarap- pairs, the RPCM network for predicting relationship types
pearancerepresentations,butmanifestsitselfdifferentlyfor of these pairs is proposed, which enhances the cognitive
differentrelationshipcategories.AsshowninFig.5,forthe ability of the model by fusing the contexts of objects and
objectpairsofthesametype,therelationshiplabelsbetween relationshipsintriplets.BasedonthethreelevelsintheCAC
themarenon-unique(e.g.,<ship,awayfrom,dock>,<ship, framework,thedefinitionoftheSGGtaskinlarge-sizeVHR
approach,dock>),andfortheobjectpairsofdifferenttypes, SAIispresentedhere.Specifically,viewingtheobjectentities
theirrelationshiplabelsmaybethesame(e.g.,<ship,away as nodes of the scene graph, and the relationships between
from,dock>,<truck,awayfrom,tollgate>). the entities as directed edges connecting different entities,
theSGGtaskinlarge-sizeVHRSAIcanbedescribedas:
3.2.2 DataStatisticsandVisualization
P(G SA |I)=P(E |I)P(E S,E O |E,I)P(R|E S,E O,I), (1)
Thestatisticalresultsofobjectandrelationshipannotations
from the RSG dataset are shown in Fig. 3(a) and Fig. 3(b), where I denotes large-size SAI, G SA denotes the scene
respectively. It can be seen that there is an obvious long- graph in large-size SAI, E is object entities, and R is the
tail effect for both object and relationship categories in relationshipsbetweensubjectsE S andobjectsE O.
the RSG dataset, which is also consistent with reality. The
highest-frequency objects are common small objects such
4.1 OrientedObjectDetectioninLarge-sizeVHRSAI
as ”cars”, ”airplanes”, ”tanks” and ”boats”. The highest-
frequency relationships are mainly related to the parking OBD in large-size SAI is an indispensable basis for SGG in
states, such as ”parked alongside with”, ”parking in the large-sizeSAI,whichdirectlyaffectsthepredictionaccuracy
differentapronwith”,”parallellyparkedon”and”parking of the triplets. Compared with small-size or low-resolution
inthesameapronwith”. imagery, large-size VHR SAI can not only accommodate
morecompletelargeobjects(e.g.,runways,docks),butalso
greatlyretainimportantdetailsofsmallobjects(suchasthe
4 THE PROPOSED METHOD
head and tail of a car, the head and tail of an airplane),
Fig.6presentstheoverallarchitectureoftheproposedCAC which can help to mine more high-value knowledge in
framework, which mainly contains three parts: OBD, pair SAI. In processing large-size VHR SAI, mainstream deep9
learning-basedOBDmethods,whethersupervised[56],[57], pairsasnegativesamples.Toaddressthisproblem,thispa-
[58], [59] or weakly supervised [60], [61], [62], [63], adopt a perproposesapairproposalgeneration(PPG)networkvia
cropping strategy [4], [64], which results in large objects, adversarialreconstruction,whichcanrealizetherankingof
such as runways, being cut off. In addition to the cropping tripletsaccordingtotheirscorebasedontheircontainment
strategy, some OBD methods tackle the original large-size of high-value knowledge without negative samples, and
imagerywithafixed-windowdown-samplingstrategy[65], effectivelyaddressestheissueofselectingmeaningfulpairs
[66],[67],resultinginthelossofmassivedetailinformation with contextual associations in the case of only positive
of the image. Recently, HBD-Net for single-class bridge samplesfortheSGGtaskinlarge-sizeVHRSAI.
detection in large-size VHR SAI is proposed [27], and the To enable the model to autonomously learn the re-
SDFF architecture in HBD-Net effectively solves the large construction ability for high-value pairs without negative
scale variations in single-class bridge detection by fusing samples,thePPGnetworkisdividedintotwo-stageadver-
multi-scalecontextsintothedynamicimagepyramid(DIP) sarial training with pair encoder-decoder (PED), because
ofthelarge-sizeimage.However,thissingle-classOBDonly the auto-encoder can achieve stability and reconstruct the
considers the intra-class variations of the objects, which is inputs well during adversarial training. The PPG network
not applicable in the multi-class OBD task with both intra- contains two encoder networks: encoder1 (E 1), and en-
classandinter-classvariations. coder2(E 2)aswellastwodecodernetworks:decoder1(D 1),
To address the aforementioned problem, this paper ex- and decoder (D 2), which constitute the two auto-encoders
tendsHBD-Nettotheholisticmulti-classOBDinlarge-size PED 1andPED 2,asshowninFig.6.TwoPEDaretrained
VHR SAI, named HOD-Net. In HOD-Net, the total loss of to reconstruct the input feature X, which represents the
the HBB-based and OBB-based detectors in large-size VHR unionofspatialandsemanticfeaturesforobjectpairs.
SAIisdefinedasfollows. Similartothegamingstrategyingenerativeadversarial
L O = (cid:88)M ( Γ1 m (cid:88) Lc ils+ Γm1 + (cid:88) w jregLr jeg), (2) n adet vw ero sr ak ri( aG lmAN an) n[ e6 r9 ,] i, nP wE hD ich1 Pan Ed DP 1E trD ies2 ta ore det cr ea ii vn eed PEin Da 2n ,
m=1 i∈∆m j∈∆m+ while PED 2 tries to distinguish whether the data is the
L H = (cid:88)M ( Γ1 m (cid:88) Lc ils+ Γm1 + (cid:88) Lr jeg), (3) i sn tap gu et , X theor feX atu1 rerec Xon is str su qc ute ed ezb ey d P inE toD t1 h. eD lau tr ein ng t st pra ai cn ein Zg
m=1 i∈∆m j∈∆m+ by E 1, and then reconstructed by D 1, which can be rep-
where m means the layer of the DIP, m ∈ [1,M]. ∆m resented as D 1(E 1(X)). PED 2 is trained by D 2(E 2(X1))
and
∆m+
are the set of all samples and the set of positive
to distinguish whether the data is the original input X
samples, respectively. Γm and Γm+ indicate the total num- or from PED 1, during which PED 1 maintains learning
ber of all samples and positive samples in the m-th layer, to deceive PED 2. Overall, PED 1 and PED 2 similar to
respectively.
wreg
denotes the regression weight obtained
minimal-maximalgames:
j
using the SSRW strategy [27]. Regression loss
Lr jeg
is set ζ = min max G∥X−PED 2(PED 1(X))∥ 2, (4)
withreferencetothesmoothedL1lossdefinedin[68]. PED1PED2
To enable the model to adaptively focus on different ThetrainingobjectiveofPED 1istominimizetherecon-
object categories in different layer, a hierarchical adap-
structionerror(cid:13)
(cid:13)X i−X
i1(cid:13)
(cid:13) 2tolearnthelatentrepresentation
tive weighted strategy is designed in classification loss ofthedata,andthetrainingobjectiveofPED 2 istoreduce
Lc ils = −t clog(eΨm c /(cid:80)C c=1eΨm c ). Specifically, t c represents thereconstructionerror(cid:13) (cid:13)X i−X i2(cid:13) (cid:13) 2 toaminimum.Asthe
the one-hot vector, and Ψm = wm ⊙ Φm denotes the iterations n are constantly changing, the loss function is
wei
class-weighted score. wm = [wm,...,wm] is the learnable denotedas:
1 C
w the eig nh et t, wa on rd k.Φ Cm is= the[ϕ nm 1 um,.. b., eϕ rm C of] od be jn eo ctte cs att eh ge oc rio en sfi
,
ad ne dnc ⊙e o isf
L PED =
n1 (cid:13) (cid:13)X−X1(cid:13)
(cid:13) 2+(1−
n1 )(cid:13) (cid:13)X−X2(cid:13)
(cid:13) 2, (5)
theelement-wiseproduct.
In the inference stage, n in Eq. (5) is set to 2, and then
4.2 Pair Proposal Generation via Adversarial Recon- the top k 1 proposal pairs are selected as input indices of
subject-objectpairsforrelationshipprediction.
struction
Given N objects in the image, the number of subject-object
4.3 Relationship Prediction with Context-Aware Mes-
pairsisN(N −1),whichexhibitsexponentialgrowthasN
saging
increases.Thereareahugeamountofsubject-objectpairsin
alarge-sizeVHRSAI,andthepairredundancyproblemwill Forlarge-sizeVHRSAI,anytwoobjectscanoftenbedirectly
cause the model to be ineffective under common computa- associated through significant relationships or indirectly
tionalresourcesduetomemoryoverflow.Someresearchers associated with other medium objects, so the introduction
regard pair pruning as a binary classification problem for of context-aware messaging from objects and relationships
theSGGtaskinnaturalimagery,i.e.,annotatedobjectpairs is essential to enhance the cognitive ability of the model.
areregardedaspositivesamples,andnon-annotatedobject In addition, numerous possible subject-object pairs present
pairs are regarded as negative samples. It is worth noting different visual appearances, resulting in large intra-class
that it is difficult to exhaust the annotation of triplets for variation among the same relationships, which pose criti-
large-size VHR SAI, and most of the non-annotated object cal challenges for SGG in large-size VHR SAI. This paper
pairs still have high-value semantic relationships, so it is proposes a relationship prediction network with context-
unreasonable to directly regard the non-annotated object aware messaging (RPCM), which mainly consists of two10
components:aprogressivebi-contextaugmentationcompo-
nent and a prototype-guided relationship learning compo-
nent. The former uses context messaging from nodes and
edges to augment the local representations of objects and
relationships.Meanwhile,thelattercontinuouslyoptimizes
and updates the semantic prototype under instance-level
and prototype-level constraints. Ultimately, accurate rela-
tionship prediction is achieved by matching the predicted
relationship representation with the prototype representa-
tioninthesamesemanticspace.
4.3.1 ProgressiveBi-contextAugmentationNetwork
In the sparse graph formulated based on the proposed
PPG network, the semantic representations of both objects
and relationships are influenced by neighboring objects
Fig.7.IllustrationoftheproposedPBAmodule.Thisillustrationshows
and relationships. However, the interplay between object- the details of messaging and fusion for one progressive context aug-
object, object-relationship, and relationship-relationship in- mentationofentity-relationship.
teractionsmayexhibitsubstantialdiversityindifferentcon-
texts.Attentiongraphnetwork[70],whichleadsthemodel
where σ(·) denotes the sigmoid activation function [71].
to focus on reliable context information by assigning the
αee,αrs and αro represent the importance of messaging
correspondingweightstodifferentregionsofanimage,has
fromentitytoentity,relationshiptosubject,andrelationship
shown its effectiveness for SGG in natural imagery [47],
to object, respectively. By learning to adjust α [70], each
[52]. Nonetheless, due to the complexity of large-size VHR
iteration leads to a change in attention and affects subse-
SAI, conventional SGG models often exhibit deficiencies in
quent iters. wee,wrs and wro denote the weight matrix of
relationship context inference capabilities when processing
the directed messaging from entity to entity, relationship
SAI.Moreover,althoughtheintegrationofglobalcontextin-
to subject, and relationship to object, respectively. Fe and
formationassistsinenhancedrelationshiprecognition,more i
Frareentityrepresentationandrelationshiprepresentation,
fine-grained discriminative features are primarily extracted i
respectively.
from the triplets themselves. An overabundance of global
Similarly,differenttypesofmessaging(i.e.,relationship-
context information may induce significant confusion. In
relationship,subject-relationship,object-relationship)onre-
our work, a novel progressive bi-context augmentation
lationshipsareusedtolearntheglobalcontextualinforma-
(PBA) component is proposed. As shown in Fig. 7, PBA
tionofrelationships:
mainly consists of entity-relationship progressive global
context-aware messaging and global-local feature fusion. F(cid:98)r =σ(wrrFrαrr+wsrFeαsr+worFeαor), (7)
i+1 i i i
The progressive global context-aware messaging module
for entity-relationship adaptively learns global information Global-LocalFeatureFusion.Introducingreliableglobal
fromneighboringentitiesandrelationshipsusingaprogres- information helps to better recognize relationships, but
sive strategy. The global-local feature fusion module fuses morefine-graineddiscriminativefeaturesmainlycomefrom
thecollectedglobalandlocalfeaturestoobtainaugmented local.Therefore,weadoptaglobal-localfeaturefusionstrat-
features.Afterseveralitersofnodesandedgesinthesparse egytocapturereliableglobalinformationandmaintainthe
graph, the dual optimization and updating of entity and local more discriminative features. Specifically, the context
relationshiprepresentationsareaccomplished. augmentedfeatureofentityFe canbeobtainedbyfusing
i+1
Progressive Global Context-aware Messaging.Consid- thecontext-awarefeatureofentityF(cid:98)e fromglobalandthe
i+1
ering that it is easy to confuse when introducing too much entityfeatureFe fromlocal.Similarly,thecontextaugmen-
0
unnecessary context information, we adopt a progressive tation feature of relationship Fr can be obtained. After
i+1
training strategy to solve this problem. In this strategy, all L iters, the predicted relationship representations Rel with
entities and relationships first collect reliable information context-awareaugmentationareupdated.Itisworthnoting
fromtheirdirectneighboringnodesandedgesviatheatten- that the predicted relationship representations in 4.3.1 are
tion graph network, and then learn their indirectly related optimized and updated simultaneously with the prototype
contexts with the help of neighboring nodes and edges, so representationsin4.3.2.
that each entity and relationship can learn more reliable
contextualinformationwithoutintroducingextranoise. 4.3.2 Prototype-guidedRelationshipLearning
Sparsescenegraphsconstructedbasedontheproposed PrototypelearningallowsSGGmodelstofocusonallcate-
PPG network can provide multiple types of messaging goriesequallyduringtraining,andiseffectiveinalleviating
indexesforentitiesandrelationships.Fori+1iters,different the category imbalance problem of datasets [46]. However,
types of messaging (i.e., entity-entity, relationship-subject, previous linear classifier-based prediction methods often
and relationship-object) are performed on entities to learn lack the ability to model data at different semantic lev-
context-awareinformationofentities: els and suffered from a lag in representational capabili-
ties (e.g., <ship, away from, dock> and <ship, approach,
F(cid:98) ie +1 =σ(weeF ieαee+wrsF irαrs+wroF irαro), (6) dock> correspond to different relationship labels, but it is11
clear that <ship, away from, dock> and <ship, approach, 5 EXPERIMENTS AND ANALYSIS
dock> are closer than <truck, away from, gasstation> in
In this subsection, we first introduce the sub-tasks related
the visual representation). We propose a prototype-guided
to the OBD and the SGG tasks, and then describe the
relationship learning method that utilizes category labels
evaluation metrics and our implementation details for the
as prompts to help generate learnable relationship proto-
differenttasks.Finally,extensiveevaluationofourproposed
types, and prototype representations updated dynamically
methods for the OBD and SGG tasks are performed on the
byinstance-levelconstraintsandprototype-levelconstraints
RSGdataset.
to help the SGG model enhance the discriminability be-
tweenprototypes.
Prototype Construction. To construct more discrimina- 5.1 Benchmark
tive prototype representations, word embeddings (Glove) Task Settings.FortheOBDtask,weestablishtwotypesof
ofdifferentcategorylabels[label 1,label 2,...,label C]areuti-
OBD benchmarks: HBB-based and OBB-based detectors in
lized to construct initial class-specific semantic prototypes large-size VHR SAI. For the SGG task, we evaluate SGG in
T=[t 1,t 2,...,t C].Afterobtainingtheinitialprototyperep-
large-size VHR SAI following three conventional sub-tasks
resentations,theyarefedintoalearnableprototypeencoder ofSGGinnaturalimagery.
Enc p, during which the prototype representations Pro = PredicateClassification(PredCls).Giventheobjectcate-
Enc p(T) are allowed to be updated and matched with gories and their bounding boxes, it is necessary to predict
the predicted relationship representations Rel in the same pairs containing rich knowledge and relationship types of
semantic space by r = MAP(Rel) and p = MAP(Rro), thesepairsinlarge-sizeVHRSAI.Thissub-taskistoverify
whereMAP(·)istwo-layerMLPs.
theperformanceofthemodelinpairpruningandrelation-
Instance-wise Matching Loss. To enable the model to shippredictionwithoutotherinterferingfactors.
learn more fine-grained category variation, the matching Scene Graph Classification (SGCls). Given only the
constraints between samples and prototypes are injected bounding boxes of objects, it is necessary to recognize the
intothelearningobjectiveduringinstance-levelmatchingto categories of both subjects and objects, and thus predict
achievepositivesamplesclosertotheprototypesandnega- high-valuepairsaswellasrelationshiptypesofthesepairs
tivesamplesfurtherawayfromtheprototypes.Specifically, inlarge-sizeVHRSAI.
weobtaininstance-levellossL IC andL ID basedoncosine Scene Graph Generation (SGGens). Given a large-size
distanceandeuclideandistance,respectively: VHRSAI,itisdirecttopredicthigh-valuetriplets<subject,
exp(<r,p>/τ) relationship, object>. These triplets encompass the bound-
L IC =−log (cid:80)C
exp(<r,p
>/τ), (8) ingboxesandcategoriesofbothsubjectsandobjects,aswell
i=0 i asthecategoriesoftheirrelationships.
L ID =max(0,q+−q−+γ 1), (9) Evaluation Metrics. For the OBD task, mean average
where q− = (cid:13) (cid:13)rNeg−p (cid:13) (cid:13)2 ,q+ = ∥rtrue−p ∥2 . rtrue de- precision (mAP) is adopted as the evaluation metric. For
i 2 i 2 the SGG task, different from the one-to-one relationship
notes positive samples with label annotations, and rNeg is
prediction in natural imagery, the relationship prediction
thenegativesamplessampledinorderofdistancewiththe
for the SGG task in large-size VHR SAI is one-to-many.
exclusionofpositivesamples.
Based on the common evaluation metrics recall@K (R@K)
Prototypical Matching Loss. The purpose of prototype
and mean recall@K (mR@K) of the SGG task in natural
contrast learning is to make samples of the same category
imagery [43], we propose multi-label recall@K (MR@K)
tightly clustered around the corresponding prototype to
andmeanmulti-labelrecall@K(mMR@K)asthemulti-label
formtightclusters,withclusterscorrespondingtodifferent
evaluationmetricsforSGGinlarge-sizeVHRSAI.
prototypes pulling away from each other. To achieve this
To evaluate the performance of the SGG model more
goal,weutilizethesemanticstructurecapturedbyrelation-
comprehensively, the harmonized mean (HMR@K) of both
ship prototypes to realize prototype contrast learning loss
MR@K and mMR@K is taken as the comprehensive eval-
L PC andlossL PD.
uation metric for the SGG task in large-size VHR SAI.
(cid:13) (cid:13)
L PC =(cid:13) (cid:13)p·pT(cid:13) (cid:13) , (10) For all tasks, the prediction can be considered as a true
2,1
positive (TP) result only when it matches the ground-truth
L PD =max(0,−N sk elect(∥p i−p j∥2 2)+γ 2), (11) triplet <subject, relationship, object> and has at least 0.5
whereN sk elect(D M)denotestheselectionofthetopksmall- IoU between the bounding boxes of subject-object and the
estvaluesinthedistancematrixD M. bounding boxes of ground-truth. As for the K, instead of
In the training stage, the overall loss function L of our the setting (K= 50/100), which is commonly used by the
SGG task in natural imagery, we adopt the setting (K=
RPCMisdenotedas:
1000/1500). This is because, in large-size VHR SAI, the
L=L IC +L ID+L PC +L PD, (12) average number of triplets per image far exceeds that in
naturalimagery.
Intheinferencestage,theprototypetypewiththehigh-
estcosinesimilaritys iwillbeconsideredastheresultofthe
relationshipprediction: 5.2 Implementationdetails
R class =argmax(s i |s i =<r,p i >/τ), (13) Object Detector. Taking the number of instances and the
i typeofscenariosasthebasisofdivision,wesplitthedataset
whererandparebothregularizationoperations. into train set (60%), val set (20%), and test set (20%). The12
TABLE2
Baselineresults(%)ofHBB-basedandOBB-baseddetectorsonRSGtestset.b bdenotesboarding bridge,l tdenoteslattice tower,s lis
ship lock,andg drepresentsgravity dam.Allexperimentsarebasedonthestandard‘1x’(12epochs)trainingschedule.†meansthat
Swin-L[72]isusedandothersindicatesResNet50[73]asthebackbone.Underlineindicatesthebasedetectorforsubsequentlines.
OBD Detectors Venue ship boat truck car airplane crane b b tank l t bridge runway s l dock g d ... mAP
FasterR-CNN[74] NeurIPS’15 30.7 7.2 26.2 38.1 54.3 23.6 24.7 21.1 21.9 5.3 0.0 0.0 4.2 0.0 ... 32.2
RetinaNet[75] ICCV’17 26.1 4.6 25.6 35.5 52.6 20.1 21.0 18.4 21.9 3.3 0.0 0.0 2.8 0.0 ... 24.6
CascadeR-CNN[76] TPAMI’19 32.0 7.8 29.3 39.1 55.0 26.4 25.6 20.9 22.1 6.0 0.0 0.0 5.6 0.0 ... 32.4
FCOS[77] TPAMI’20 20.0 2.3 23.7 34.6 48.0 14.8 17.4 17.6 11.5 1.2 0.0 0.0 2.0 0.0 ... 20.8
HBB
TOOD[78] ICCV’21 29.3 5.5 30.8 38.4 53.6 23.5 24.8 19.8 20.6 4.0 0.0 3.3 3.8 1.2 ... 30.1
GCL[67] KBS’23 30.6 7.2 26.6 38.1 54.3 23.5 24.5 21.1 21.8 5.4 6.6 0.0 5.0 2.2 ... 34.4
HOD-Net(Ours) - 32.5 7.6 28.5 37.5 53.3 25.5 24.5 21.8 22.8 9.7 9.3 0.9 12.3 37.7 ... 45.2
HOD-Net†(Ours) - 35.4 11.7 31.9 36.6 53.0 32.7 26.6 20.8 23.1 16.6 20.9 4.7 15.4 36.3 ... 53.2
DeformableDETR[79] ICLR’21 18.1 5.5 15.8 42.5 85.1 12.6 32.7 51.1 21.2 0.5 0.0 0.0 2.1 0.0 ... 17.1
ARS-DETR[80] TGRS’24 44.6 16.4 36.1 60.8 88.6 40.5 59.0 54.7 41.6 8.2 0.0 0.0 7.8 0.0 ... 28.1
RetinaNet[75] ICCV’17 39.3 14.3 36.7 65.3 88.5 42.9 43.1 50.8 44.8 2.1 0.0 0.0 5.2 0.0 ... 21.8
ATSS[81] CVPR’20 38.4 15.5 31.4 67.6 89.0 35.6 39.2 52.6 43.2 5.1 0.0 0.0 5.1 0.0 ... 20.4
KLD[82] NeuIPS’21 43.1 12.7 39.1 65.7 88.7 47.2 50.0 51.2 48.0 3.9 0.0 0.0 12.7 0.0 ... 25.0
GWD[83] ICML’23 42.5 14.5 38.4 65.8 89.1 52.2 54.0 51.0 49.2 4.4 0.0 0.0 12.6 0.0 ... 25.3
KFIoU[84] ICLR’23 41.6 14.1 39.5 67.1 89.4 47.7 54.6 51.4 47.0 8.3 0.0 0.0 9.2 4.5 ... 25.5
R3Det[85] AAAI’21 44.0 15.3 45.3 68.6 89.6 52.5 43.5 53.6 47.5 4.3 0.0 0.0 11.2 0.0 ... 23.7
S2A-Net[86] TGRS’21 46.6 13.9 45.1 69.3 89.7 52.2 52.4 57.9 48.9 13.5 0.0 0.0 11.7 0.0 ... 27.3
RepPoints[87] ICCV’19 26.3 5.6 27.1 62.5 89.0 40.3 35.3 54.2 38.6 9.1 0.0 0.0 4.9 0.0 ... 19.7
CFA[88] CVPR’21 45.6 15.1 44.3 66.8 88.8 57.3 48.6 56.1 46.4 2.9 0.0 0.0 13.7 0.0 ... 25.1
OrientedRepPoints[89] CVPR’22 48.0 16.9 43.8 67.9 89.3 59.0 49.7 55.1 46.5 9.1 0.0 0.0 11.4 0.0 ... 27.0
OBB SASM[90] AAAI’22 40.6 10.4 42.6 68.1 89.0 56.3 41.8 51.0 44.1 14.5 0.0 0.0 14.3 0.0 ... 28.2
FCOS[91] ICCV’19 43.3 12.7 44.9 68.2 89.5 56.9 53.6 52.9 44.4 12.7 0.0 0.0 12.9 0.0 ... 28.1
CSL[92] ECCV’20 43.8 12.6 45.5 67.3 89.1 55.5 52.1 49.9 44.3 10.3 0.0 0.0 8.9 0.0 ... 27.4
PSC[93] TPAMI’24 44.6 18.0 44.3 66.2 89.6 56.4 55.4 53.1 44.8 16.1 0.0 0.0 13.9 0.0 ... 30.5
H2RBox-v2[61] NeurIPS’23 43.7 12.2 44.0 67.7 89.3 50.7 51.4 4.2 45.4 15.7 0.0 0.0 12.3 0.0 ... 26.2
FasterR-CNN[74] NeurIPS’15 50.4 20.9 48.1 68.0 89.6 20.9 54.4 55.0 48.6 18.8 0.0 0.0 14.0 0.0 ... 32.6
GlidingVertex[94] TPAMI’20 50.6 23.8 38.6 68.1 89.4 56.9 54.7 54.2 45.9 17.8 0.0 0.0 14.9 0.0 ... 30.7
RoITransformer[57] CVPR’19 56.6 26.2 50.3 69.1 89.9 59.7 63.4 57.3 51.0 15.8 0.0 0.0 13.6 0.0 ... 35.7
ReDet[95] CVPR’21 60.1 25.5 50.0 69.9 90.1 68.0 63.7 60.1 54.4 25.2 0.0 0.0 18.6 0.0 ... 39.1
OrientedR-CNN[96] ICCV’21 57.6 26.6 51.6 67.8 89.5 65.0 62.8 54.1 49.1 21.2 0.0 0.0 14.8 0.1 ... 33.2
GCL[67] KBS’23 57.1 26.6 51.5 67.8 89.4 64.2 62.2 54.1 49.1 21.1 0.0 0.0 15.1 0.1 ... 33.7
LSKNet-S[97] CVPR’23 59.0 25.5 53.2 70.9 89.7 65.5 62.4 53.6 55.0 24.7 0.0 0.0 19.5 11.4 ... 37.8
HOD-Net(Ours) - 56.1 19.5 48.4 64.2 89.6 63.9 50.5 57.4 55.7 20.5 12.6 30.2 23.0 66.8 ... 45.6
HOD-Net†(Ours) - 65.2 32.6 53.4 66.4 89.7 69.3 63.1 59.3 57.6 38.7 35.7 46.9 29.4 78.6 ... 55.9
TABLE3
Results(%)ofdifferenttrainingstrategiesonRSGtestset.b bdenotesboarding bridge,l tdenoteslattice tower,s lisship lock,andg d
representsgravity dam.Swin-L[72]isadoptedasthebackbone.
OBD Detectors ship boat truck car airplane crane b b tank l t bridge runway s l dock g d ... mAP
FasterR-CNN[74](Resizing) 7.5 0.0 12.6 1.3 1.8 3.4 0.3 0.6 0.4 6.9 29.5 4.7 10.1 41.6 ... 33.7
HBB FasterR-CNN[74](Cropping) 35.6 9.5 29.8 37.1 53.1 31.6 27.2 20.9 22.3 14.0 0.0 0.0 10.8 10.2 ... 42.6
HOD-Net†(Ours) 35.4 11.7 31.9 36.6 53.0 32.7 26.6 20.8 23.1 16.6 20.9 4.7 15.4 36.3 ... 53.2
OrientedR-CNN[96](Resizing) 22.4 0.3 32.4 8.8 15.6 11.9 4.5 9.1 4.5 14.5 0.8 3.0 23.0 68.4 ... 30.9
OBB OrientedR-CNN[96](Cropping) 64.8 31.2 54.9 69.7 89.6 69.1 63.0 57.7 56.7 34.3 0.0 0.0 22.2 4.5 ... 41.9
HOD-Net†(Ours) 65.2 32.6 53.4 66.4 89.7 69.3 63.1 59.3 57.6 38.7 35.7 46.9 29.4 78.6 ... 55.9
detectors are trained on RSG train set using AdamW [98] initial learningrate are4 and1×10−3 for thePredCls and
asandefaultoptimizer,andtheperformanceisreportedon SGCls tasks, and 2 and 1 × 10−3 for the SGDet task, re-
RSG test set. All the listed models are trained on NVIDIA spectively.FortheSGDettask,objectdetectionisperformed
RTX3090 GPUs. We set the batch size to 2 and the initial using a Per-Class NMS [43], [44] with 0.5 IoU. Considering
learningrateto1×10−4.Themeanaverageprecision(mAP) thecomputationalresourceconsumptioncausedbynumer-
iscalculatedfollowingPASCALVOC07[99]. ous invalid object pairs in large-size VHR SAI, we use the
SceneGraphGeneration.Ontopofthefrozendetectors, proposed PPG network in the overall framework, which
we train all SGG models on RSG train set using SGD as an enablestheSGGtaskinlarge-sizeVHRSAItoberealizedin
optimizerandevaluatethemonRSGtestset.Batchsizeand commoncomputationalconditions(e.g.,onesingleGPU).13
Fig.8.ThevisualizationresultsofOBBandHBBdetectionontheRSGdatasetusingtheHOD-NetandcomparisonOBDmethods.
5.3 ResultsandAnalysisforOBD in the OBB detection task. Notably, for objects with ex-
Comparison with Baselines. To explore the proper- treme aspect ratios (e.g., runways, shiplocks), our HOD-
ties of RSG and provide guidelines for future OBD Net outperforms other methods in OBB detection by a
task in large-size VHR SAI, we conduct a com- greatmarginof(34.9%/31.2%).Aboveexperimentsindicate
prehensive evaluation of about 30 OBD methods that our HOD-Net can provide more comprehensive and
and analyze the results. Table 2 provides a com- accurate object representation for SGG in large-size VHR
prehensive benchmark on RSG, including single/two- SAI.
stage, CNN/Transformer, anchor-based/anchor-free and Qualitative Results and Visualization. In Fig. 8, we
supervised/weakly-supervisedmethods.WeadoptMMDe- further give the visualizations of different strategies on
tection1 [100] and MMRotate2 [101] as the toolkits and all HBB-based detectors and OBB-based detectors. It can be
experiments are based on the standard ‘1x’ (12 epochs) seen that many large objects (e.g., docks, aprons) cannot be
training schedule. It can be observed that our HOD-Net detectedholisticallyduetotheintegritybeingdestroyedin
significantly outperforms other methods under the same thedetectionbasedoncroppingstrategy,andalotofsmall
backbone (ResNet50 [73]) for both HBB-based and OBB- objects(e.g.,airplanes,boats,cranes,boardingbridges)can-
based detectors. To provide more accurate OBD results for notbedetectedcorrectlyduetothelossofinformationinthe
the SGG task in large-size VHR SAI, we evaluated three detection based on resizing strategy. The proposed HOD-
OBDstrategiesonHBB-basedandOBB-baseddetectors(see Netshowssuperiorityinthedetectionresultsofmulti-scale
Table 3): resizing, cropping, and our HOD-Net. It can be objectsandachievesalmostthesameresultsastheannota-
seen that our HOD-Net achieves 53.2% and 55.9% mAP tion for objects with extreme aspect ratios (e.g., runways).
on the HBB-based and OBB-based detectors, respectively The above results show that the proposed HOD-Net can
(using a 0.5 IoU threshold). Specifically, under the same provide mandatory support for the SGG task in large-size
backbone(Swin-L[72]),itachievesa10.6%/19.5%improve- VHRSAI.
ment in mAP compared to the baseline Faster R-CNN
detector(cropping/resizing)intheHBBdetectiontask,and 5.4 ResultsandAnalysisforSGG
a 12.9%/23.9% improvement in mAP metric compared to
Comparison with the State-of-the-art Methods.Asshown
the baseline Oriented R-CNN detector (cropping/resizing)
in section 5.3, HOD-Net exhibits superior performance for
1.https://github.com/Zhuzi24/RSG-MMDetection the OBD task, which is a prerequisite for achieving ac-
2.https://github.com/yangxue0827/RSG-MMRotate curate relationship prediction in the SGG task. For a fair14
TABLE4
Baselineresults(%)ofSGGonRSGtestset.OurRPCMoutperformspreviousmethodsinallmetricsonbothHBB-andOBB-baseddetectors,
especiallyonthemorechallengingSGClsandSGDettasks.Specifically,ourmodeloutperformspreviousstate-of-the-artmodelsbyanabsolute
5.17%/4.76%onHMR@1500/2000fortheSGClstaskand3.80%/3.80%onHMR@1500/2000fortheSGDettaskontheOBB-baseddetector.
OBD Relationship PredCls SGCls SGDet
Venue
Type Prediction MR@1500/2000 mMR@1500/2000 HMR@1500/2000 MR@1500/2000 mMR@1500/2000 HMR@1500/2000 MR@1500/2000 mMR@1500/2000 HMR@1500/2000
IMP[36] CVPR’17 49.90/51.62 18.51/19.40 27.00/28.20 42.48/43.63 16.46/17.02 23.73/24.49 14.98/15.03 5.11/5.13 7.62/7.65
Motif[44] CVPR’18 61.81/63.78 31.33/32.66 41.58/43.20 45.13/46.19 21.46/22.57 29.09/30.32 18.20/18.32 7.69/7.80 10.81/10.94
HBB HETSGG[47] AAAI’23 63.83/65.26 28.70/29.72 39.60/40.84 30.03/31.08 10.50/11.05 15.56/16.30 18.11/18.18 5.91/5.99 8.91/9.01
PE-Net[46] CVPR’23 63.21/65.11 35.94/37.10 45.82/47.27 38.84/40.05 19.46/20.20 25.93/26.86 16.49/16.66 7.30/7.38 10.12/10.23
RPCM(Ours) - 64.27/65.67 38.73/39.70 48.33/49.48 46.28/47.55 25.32/26.12 32.73/33.72 22.77/22.85 10.43/10.63 14.31/14.51
IMP[36] CVPR’17 51.52/53.19 21.47/22.31 30.31/31.43 46.08/47.50 18.57/19.38 26.47/27.53 18.13/19.03 5.60/5.99 8.56/9.11
Motif[44] CVPR’18 62.02/64.00 29.40/30.64 39.89/41.44 50.44/52.29 24.21/25.36 32.72/34.16 20.63/21.60 7.78/8.22 11.30/11.91
OBB HETSGG[47] AAAI’23 63.81/65.37 29.74/30.98 40.57/42.04 27.64/28.96 12.18/12.78 16.91/17.73 19.45/20.32 5.44/5.73 8.50/8.94
PE-Net[46] CVPR’23 62.87/64.98 36.99/38.29 46.58/48.19 41.79/43.45 20.64/21.60 27.63/28.86 21.26/22.50 8.75/9.30 12.40/13.16
RPCM(Ours) - 64.23/65.86 41.24/42.30 50.23/51.51 51.29/52.72 30.04/30.85 37.89/38.92 27.23/28.50 11.53/12.07 16.20/16.96
Fig. 9. Qualitative comparisons between proposed RPCM and other methods on different scenarios. Blue edges represent the ground-truth
relationships correctly predicted, purple edges indicate ground-truth relationships that failed to be detected, and green edges are reasonable
relationshipspredictedbythemodelbutnotannotatedintheground-truth.
comparison, all experiments utilized the same toolkit3 and MR@1500/2000, mMR@1500/2000, and HMR@1500/2000
employedHOD-NetasthedetectorfortheSGGtask.Weuse as evaluation metrics of the SGG task. In Table 4, we
conductacomprehensiveevaluationof10experimentsand
3.https://github.com/Zhuzi24/SGG-ToolKit15
TABLE5
ComparisonofthePPGandotherpairpruningmethodsonRSGtestset.WereportMR@1500/2000,mMR@1500/2000,and
HMR@1500/2000(%)onPredCls,SGCls,andSGDettasks.
PredCls SGCls SGDet
Models
MR@1500/2000 mMR@1500/2000 HMR@1500/2000 MR@1500/2000 mMR@1500/2000 HMR@1500/2000 MR@1500/2000 mMR@1500/2000 HMR@1500/2000
Randomization+RPCM 51.49/52.62 31.96/32.46 39.44/40.15 43.12/43.54 26.02/26.28 32.46/32.78 16.31/16.57 6.66/6.76 9.46/9.60
ABBS[41]+RPCM 53.32/53.92 33.45/33.75 41.11/41.51 44.04/44.58 26.16/26.41 32.82/33.17 17.02/17.49 7.02/7.68 9.94/10.67
PPG+RPCM(Ours) 64.23/65.86 41.24/42.30 50.23/51.51 51.29/52.72 30.04/30.85 37.89/38.92 27.23/28.50 11.53/12.07 16.20/16.96
Fig.10.Thevisualizationresultsofpairscorematrix.Blueedgesrepresenttheground-truthpairscorrectlypredicted,purpleedgesindicateground-
truthpairsthatfailedtobedetected,andgreenedgesarereasonablepairspredictedbythemodelbutnotannotatedintheground-truth.
validated the effectiveness of RPCM by comparing it with 5.5 AblationStudyonSGG
four state-of-the-art SGG methods on both HBB-based and
In this section, we first explore the effect of PPG network,
OBB-baseddetectors.AsshowninTable4,forthethreesub-
and then analyze the impact of PBA iters on SGG per-
tasksofSGG,SGGbasedontheOBB-baseddetectorshows
formance. Further ablation experiments are conducted on
better results overall, especially for the more challenging
the RSG dataset to demonstrate the necessity of each key
SGCls and SGDet tasks. For the SGCls task based on the componentinourproposedRPCM(i.e.,objectcontextaug-
OBB-baseddetector,ourRPCMachieves37.89%/38.92%on
mentation(OCA),relationshipcontextaugmentation(RCA),
HMR@1500/2000, outperforming the SOTA method by a
andprototypematching).
largemarginof5.17%/4.76%.FortheSGDettask,ourRPCM
also outperforms the previous method with a 3.80%/3.80% 5.5.1 EffectofPPGNetwork
improvementonHMR@1500/2000.
Table 5 gives the quantitative comparison results of the
Qualitative Results and Visualization. The SGG visu- proposed PPG network with other pair pruning methods
alization results of different models on the RSG dataset on the RSG dataset when the sorted top 10,000 pairs are
are shown in Fig. 9. It can be seen that HETSGG has selected. It can be seen that the ABBS method based on
a limited ability to learn discriminative details(e.g., HET- statistics has only a weak improvement over the random-
SGGfailstodiscriminatespecificrelationships<ship,away ized method in all metrics, which shows that the semantic
from/approach,dock>bythewaketrailingoftheship).PE- and spatial combination types of the object pairs in large-
Net does not have the ability to constrain relationship pre- size VHR SAI are so complex that it is difficult to obtain
dictionbyrelyingoncontextualreasoning(e.g.,inferringthe combination patterns of objects by statistics. The score ma-
unreasonabletriplet<airplane0,parkinginthesameapron trix, as depicted in the first row of Fig. 10, becomes sparse
with, airplane2> via triplet <airplane2, parallelly parked after the PPG network learning, effectively retaining the
on, apron2> and triplet <airplane0, parallelly parked on, annotatedpairs.Thishighlightstherobustlearningcapacity
apron1>). In summary, our RPCM network can effectively of the proposed PPG network when dealing with pairs
guide the model in learning the discriminative details of containing rich knowledge. The local visualization shows
different relationships, demonstrating a strong capacity for that the PPG network can retain the annotated pairs with
contextlearningandinference. high scores, as well as some pairs that are non-annotated16
Fig.11.VisualizationofdifferentPBAitersL.Blueedgesrepresenttheground-truthrelationshipscorrectlypredicted,purpleedgesindicateground-
truth relationships that failed to be detected, and green edges are reasonable relationships predicted by the model but not annotated in the
ground-truth.
TABLE6 TABLE7
ComparisonwithdifferentPBAitersLonMR@1500/2000, AblationstudyoneachcomponentoftheRPCM.OCAandRCA
mMR@1500/2000,andHMR@1500/2000. denoteobjectcontextaugmentationandrelationshipcontext
augmentation,respectively.Classifierindicatesthemethod(prototype
matching(PM)/linearclassification)forrelationshipprediction.
PredCls
ItersL
MR@1500/2000 mMR@1500/2000 HMR@1500/2000
Component PredCls
L=1 63.56/65.44 39.12/40.54 48.43/50.06
OCA RCA Classifier MR@1500/2000 mMR@1500/2000 HMR@1500/2000
L=2 64.14/65.79 40.10/41.18 49.35/50.65
× × PM 63.10/65.28 36.53/38.05 46.27/48.08
L=3 63.90/65.54 40.79/41.88 49.79/51.10
✓ × PM 63.11/65.09 37.86/39.07 47.87/48.83
L=4 64.23/65.86 41.24/42.30 50.23/51.51
✓ ✓ Linear 66.55/68.18 34.58/35.80 45.51/46.95
L=5 62.14/64.09 39.92/41.09 48.61/50.08
✓ ✓ PM 64.23/65.86 41.24/42.30 50.23/51.51
but may contain high-value relationships, this is because
the annotation of triplets in the real annotation is hardly due to insufficient contextual information when L is set
exhaustive, so achieving the same sparsity as ground-truth to 1. As L increases, the introductions of more contextual
isnotnecessary. information cause the wrong relationships to be gradually
corrected. When L is set to 4, the relationships between
5.5.2 EffectofPBAIters latticetowers at different distances are predicted with bet-
ter results. It is worth noting that the previous incorrect
In Table 6, we compare the results on PredCls for different
relationship prediction recurred when L is 5, which may
numbers of PBA iters L (L is set to 1, 2, 3, 4, 5). We find
becausedbyexcessivecontextualinterference.Overall,the
that the number of iters has little effect on the results and
introductionsofpropercontextshelprelationshipinference
our model is robust to the number of iters. Notably, the
andexcessivecontextbringsdisruption,consistentwiththe
HMR@K keeps improving with the increase in iters when
quantitativemetricsinTable6.
iters are less than 4. This suggests that proper contextual
informationcaneffectivelyimprovethemodelperformance,
5.5.3 EffectofDifferentComponents
but excessive introduction of contextual information may
causedisturbance.Inthispaper,thenumberofitersforPBA To investigate the effectiveness of the components of the
issetto4. RPCM, we perform component ablation experiments by
Toexploretheintrinsicimpactofcontextintroductionon gradually removing them from the RPCM on the PredCls
themodel,Fig.11givesexamplesofvisualizationsobtained task, as shown in Table 7. Not surprisingly, the experi-
from different PBA iters. It can be seen that latticetowers mental results show that all three components, object con-
thatarefarapart(e.g.,”latticetower11andlatticetower28”, text augmentation, relationship context augmentation, and
”latticetower15 and latticetower26”, and ”latticetower16 prototype matching, are important for the performance of
andlatticetower26”)haveincorrectrelationshippredictions SGG.Specifically,prototypematchingenablesthemodelto17
capture the discriminative features of different relationship [5] X. Sun, P. Wang, Z. Yan, F. Xu, R. Wang, W. Diao, J. Chen,
categories in the semantic space, which in turn effectively J. Li, Y. Feng, T. Xu et al., “Fair1m: A benchmark dataset for
fine-grainedobjectrecognitioninhigh-resolutionremotesensing
distinguishes different categories with strong similarities.
imagery,” ISPRS Journal of Photogrammetry and Remote Sensing,
The object augmentation and relationship augmentation vol.184,pp.116–130,2022.
modules assist the model in learning contextual reasoning [6] X.Yang,J.Yan,W.Liao,X.Yang,J.Tang,andT.He,“Scrdet++:
constraintsforrelationshipprediction,andthevisualization Detectingsmall,cluttered and rotated objects viainstance-level
featuredenoisingandrotationlosssmoothing,”IEEETransactions
inFig.9alsoreflectsitwell.
on Pattern Analysis and Machine Intelligence, vol. 45, no. 2, pp.
2384–2399,2022.
[7] G. Cheng, J. Han, and X. Lu, “Remote sensing image scene
6 CONCLUSION classification:Benchmarkandstateoftheart,”Proceedingsofthe
IEEE,vol.105,no.10,pp.1865–1883,2017.
In this paper, we propose a large-scale dataset named RSG
[8] K.Nogueira,O.A.Penatti,andJ.A.DosSantos,“Towardsbetter
for SGG in large-size VHR SAI. The RSG dataset covers exploiting convolutional neural networks for remote sensing
1,273complexscenariosworldwide,withimagesizesrang- scene classification,” Pattern Recognition, vol. 61, pp. 539–556,
2017.
ingfrom512×768to27,860×31,096pixels,includingmore
[9] H. Zhu, M. Ma, W. Ma, L. Jiao, S. Hong, J. Shen, and B. Hou,
than 210,000 objects (with both OBB and HBB annotations)
“Aspatial-channelprogressivefusionresnetforremotesensing
and more than 400,000 triplet annotations. The large im- classification,”InformationFusion,vol.70,pp.72–87,2021.
age size, the extensive sample volume, and the diversity [10] M.D.HossainandD.Chen,“Segmentationforobject-basedim-
ageanalysis(obia):Areviewofalgorithmsandchallengesfrom
of object scale and relationship semantics make RSG a
remotesensingperspective,”ISPRSJournalofPhotogrammetryand
valuable dataset, providing indispensable data support for
RemoteSensing,vol.150,pp.115–134,2019.
advancing a new challenging and meaningful task: SGG [11] M.Wurm,T.Stark,X.X.Zhu,M.Weigand,andH.Taubenbo¨ck,
in large-size VHR SAI. Furthermore, we propose the CAC “Semanticsegmentationofslumsinsatelliteimagesusingtrans-
fer learning on fully convolutional neural networks,” ISPRS
framework,asolutiontailoredfortheSGGtaskinlarge-size
JournalofPhotogrammetryandRemoteSensing,vol.150,pp.59–69,
VHRSAI,toachieveacascadingunderstandingofSAIfrom
2019.
three levels: object detection, pair pruning and relationship [12] Y.Li,T.Shi,Y.Zhang,W.Chen,Z.Wang,andH.Li,“Learning
prediction. To facilitate the development of SGG in large deep semantic segmentation network under multiple weakly-
supervised constraints for cross-domain remote sensing image
VHR SAI, this paper releases a SAI-oriented toolkit con-
semantic segmentation,” ISPRS Journal of Photogrammetry and
taining various SGG methods and develops a benchmark RemoteSensing,vol.175,pp.20–33,2021.
inwhichtheeffectivenessoftheproposedCACframework [13] J. Johnson, R. Krishna, M. Stark, L.-J. Li, D. Shamma, M. Bern-
is empirically validated. In future work, we will continue stein, and L. Fei-Fei, “Image retrieval using scene graphs,” in
Proceedings of the IEEE/CVF Conference on Computer Vision and
to enrich the RSG dataset in terms of its sample volume,
PatternRecognition,2015,pp.3668–3678.
object categories, and relationship categories. Additionally,
[14] X. Yang, K. Tang, H. Zhang, and J. Cai, “Auto-encoding scene
we will strive to explore methods that can simultaneously graphs for image captioning,” in Proceedings of the IEEE/CVF
improve the accuracy of OBD and SGG in large-size VHR Conference on Computer Vision and Pattern Recognition, 2019, pp.
10685–10694.
SAI, thereby maximizing the potential of SGG in various
[15] L. Li, Z. Gan, Y. Cheng, and J. Liu, “Relation-aware graph at-
downstream tasks of SAI, such as image retrieval, image
tentionnetworkforvisualquestionanswering,”inProceedingsof
captioningandvisualquestionanswering. theIEEE/CVFInternationalConferenceonComputerVision,October
2019.
[16] P. Reinartz, M. Lachaise, E. Schmeer, T. Krauss, and H. Runge,
ACKNOWLEDGMENT “Traffic monitoring with serial images from airborne cameras,”
ISPRSJournalofPhotogrammetryandRemoteSensing,vol.61,no.
Wesincerelythanktheanonymouseditorsandreviewersfor 3-4,pp.149–158,2006.
their insightful comments and suggestions. The numerical [17] M.Smigaj,C.R.Hackney,P.K.Diem,N.T.Ngoc,D.DuBui,S.E.
Darby,J.Leylandetal.,“Monitoringriverinetrafficfromspace:
calculations in this paper have been done on the super-
Theuntappedpotentialofremotesensingformeasuringhuman
computingsystemintheSupercomputingCenterofWuhan footprintoninlandwaterways,”ScienceoftheTotalEnvironment,
University. vol.860,p.160363,2023.
[18] L. Matikainen, M. Lehtoma¨ki, E. Ahokas, J. Hyyppa¨, M. Kar-
jalainen, A. Jaakkola, A. Kukko, and T. Heinonen, “Remote
sensingmethodsforpowerlinecorridorsurveys,”ISPRSJournal
REFERENCES
ofPhotogrammetryandRemotesensing,vol.119,pp.10–31,2016.
[19] H. Baek and J. Lim, “Design of future uav-relay tactical data
[1] X. Chang, P. Ren, P. Xu, Z. Li, X. Chen, and A. Hauptmann,
link for reliable uav control and situational awareness,” IEEE
“A comprehensive survey of scene graphs: Generation and
CommunicationsMagazine,vol.56,no.10,pp.144–150,2018.
application,” IEEE Transactions on Pattern Analysis and Machine
Intelligence,vol.45,no.1,pp.1–26,2021. [20] J. Chen, X. Zhou, Y. Zhang, G. Sun, M. Deng, and H. Li,
[2] Y. Cong, M. Y. Yang, and B. Rosenhahn, “Reltr: Relation trans- “Message-passing-driventripletrepresentationforgeo-objectre-
formerforscenegraphgeneration,”IEEETransactionsonPattern lational inference in hrsi,” IEEE Geoscience and Remote Sensing
AnalysisandMachineIntelligence,vol.45,no.9,pp.11169–11183, Letters,vol.19,pp.1–5,2021.
2023. [21] P.Li,D.Zhang,A.Wulamu,X.Liu,andP.Chen,“Semanticrela-
[3] S. Sun, S. Zhi, Q. Liao, J. Heikkila¨, and L. Liu, “Unbiased tionmodelanddatasetforremotesensingsceneunderstanding,”
scene graph generation via two-stage causal modeling,” IEEE ISPRSInternationalJournalofGeo-Information,vol.10,no.7,p.488,
Transactions on Pattern Analysis and Machine Intelligence, vol. 45, 2021.
no.10,pp.12562–12580,2023. [22] K.Li,G.Wan,G.Cheng,L.Meng,andJ.Han,“Objectdetectionin
[4] G.-S.Xia,X.Bai,J.Ding,Z.Zhu,S.Belongie,J.Luo,M.Datcu, opticalremotesensingimages:Asurveyandanewbenchmark,”
M. Pelillo, and L. Zhang, “Dota: A large-scale dataset for ob- ISPRSJournalofPhotogrammetryandRemoteSensing,vol.159,pp.
ject detection in aerial images,” in Proceedings of the IEEE/CVF 296–307,2020.
Conference on Computer Vision and Pattern Recognition, 2018, pp. [23] J.Ding,N.Xue,G.-S.Xia,X.Bai,W.Yang,M.Y.Yang,S.Belongie,
3974–3983. J.Luo,M.Datcu,M.Pelilloetal.,“Objectdetectioninaerialim-18
ages:Alarge-scalebenchmarkandchallenges,”IEEETransactions [43] K. Tang, Y. Niu, J. Huang, J. Shi, and H. Zhang, “Unbiased
onPatternAnalysisandMachineIntelligence,2021. scenegraphgenerationfrombiasedtraining,”inProceedingsofthe
[24] G. Cheng, P. Zhou, and J. Han, “Learning rotation-invariant IEEE/CVFConferenceonComputerVisionandPatternRecognition,
convolutionalneuralnetworksforobjectdetectioninvhroptical 2020,pp.3716–3725.
remotesensingimages,”IEEETransactionsonGeoscienceandRe- [44] R.Zellers,M.Yatskar,S.Thomson,andY.Choi,“Neuralmotifs:
moteSensing,vol.54,no.12,pp.7405–7415,2016. Scene graph parsing with global context,” in Proceedings of the
[25] Y.Zhang,Y.Yuan,Y.Feng,andX.Lu,“Hierarchicalandrobust IEEEConferenceonComputerVisionandPatternRecognition,2018,
convolutional neural network for very high-resolution remote pp.5831–5840.
sensing object detection,” IEEE Transactions on Geoscience and [45] K. Tang, H. Zhang, B. Wu, W. Luo, and W. Liu, “Learning to
RemoteSensing,vol.57,no.8,pp.5535–5548,2019. composedynamictreestructuresforvisualcontexts,”inProceed-
[26] G. Cheng, J. Wang, K. Li, X. Xie, C. Lang, Y. Yao, and J. Han, ings of the IEEE/CVF Conference on Computer Vision and Pattern
“Anchor-free oriented proposal generator for object detection,” Recognition,2019,pp.6619–6628.
IEEETransactionsonGeoscienceandRemoteSensing,2022. [46] C.Zheng,X.Lyu,L.Gao,B.Dai,andJ.Song,“Prototype-based
[27] Y. Li, J. Luo, Y. Zhang, Y. Tan, J.-G. Yu, and S. Bai, “Learning embeddingnetworkforscenegraphgeneration,”inProceedings
toholisticallydetectbridgesfromlarge-sizevhrremotesensing oftheIEEE/CVFConferenceonComputerVisionandPatternRecog-
imagery,”IEEETransactionsonPatternAnalysisandMachineIntel- nition,2023,pp.22783–22792.
ligence,vol.44,no.11,pp.7778–7796,2024. [47] K.Yoon,K.Kim,J.Moon,andC.Park,“Unbiasedheterogeneous
[28] S. Schuster, R. Krishna, A. Chang, L. Fei-Fei, and C. D. Man- scene graph generation with relation-aware message passing
ning,“Generatingsemanticallyprecisescenegraphsfromtextual neuralnetwork,”inProceedingsoftheAAAIConferenceonArtificial
descriptionsforimprovedimageretrieval,”inProceedingsofthe Intelligence,vol.37,no.3,2023,pp.3285–3294.
FourthWorkshoponVisionandLanguage,2015,pp.70–80. [48] C.Chen,Y.Zhan,B.Yu,L.Liu,Y.Luo,andB.Du,“Resistance
[29] D.-J.Kim,J.Choi,T.-H.Oh,andI.S.Kweon,“Denserelational trainingusingpriorbias:towardunbiasedscenegraphgenera-
captioning: Triple-stream networks for relationship-based cap- tion,”inProceedingsoftheAAAIConferenceonArtificialIntelligence,
tioning,” in Proceedings of the IEEE/CVF Conference on Computer vol.36,no.1,2022,pp.212–220.
VisionandPatternRecognition,2019. [49] Y.Deng,Y.Li,Y.Zhang,X.Xiang,J.Wang,J.Chen,andJ.Ma,
[30] S. Chen, Q. Jin, P. Wang, and Q. Wu, “Say as you wish: Fine- “Hierarchicalmemorylearningforfine-grainedscenegraphgen-
grainedcontrolofimagecaptiongenerationwithabstractscene eration,” in Proceedings of the European Conference on Computer
graphs,” in Proceedings of the IEEE/CVF Conference on Computer Vision,2022,pp.266–283.
VisionandPatternRecognition,2020,pp.9962–9971. [50] K. Cho, B. Van Merrie¨nboer, C. Gulcehre, D. Bahdanau,
[31] M. A. Sadeghi and A. Farhadi, “Recognition using visual F.Bougares,H.Schwenk,andY.Bengio,“Learningphraserep-
phrases,” in Proceedings of the IEEE/CVF Conference on Computer resentations using rnn encoder-decoder for statistical machine
VisionandPatternRecognition,2011,pp.1745–1752. translation,”arXivpreprintarXiv:1406.1078,2014.
[32] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, [51] J.Yang,J.Lu,S.Lee,D.Batra,andD.Parikh,“Graphr-cnnfor
D.Poland,D.Borth,andL.-J.Li,“Thenewdataandnewchal- scenegraphgeneration,”inProceedingsoftheEuropeanConference
lenges inmultimedia research,” arXiv preprint arXiv:1503.01817, onComputerVision,2018,pp.670–685.
vol.1,no.8,2015. [52] X.Lin,C.Ding,J.Zeng,andD.Tao,“Gps-net:Graphproperty
[33] T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ramanan, sensingnetworkforscenegraphgeneration,”inProceedingsofthe
P. Dolla´r, and C. L. Zitnick, “Microsoft coco: Common objects IEEE/CVFConferenceonComputerVisionandPatternRecognition,
incontext,”inProceedingsoftheEuropeanConferenceonComputer 2020,pp.3746–3753.
Vision. Springer,2014,pp.740–755. [53] M. Suhail, A. Mittal, B. Siddiquie, C. Broaddus, J. Eledath,
[34] C. Lu, R. Krishna, M. Bernstein, and L. Fei-Fei, “Visual rela- G.Medioni,andL.Sigal,“Energy-basedlearningforscenegraph
tionship detection with language priors,” in Proceedings of the generation,”inProceedingsoftheIEEE/CVFConferenceonComputer
EuropeanConferenceonComputerVision,2016,pp.852–869. VisionandPatternRecognition,2021,pp.13936–13945.
[35] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, [54] R.Li,S.Zhang,B.Wan,andX.He,“Bipartitegraphnetworkwith
S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma et al., “Visual adaptivemessagepassingforunbiasedscenegraphgeneration,”
genome: Connecting language and vision using crowdsourced inProceedingsoftheIEEE/CVFConferenceonComputerVisionand
denseimageannotations,”InternationalJournalofComputerVision, PatternRecognition,2021,pp.11109–11119.
vol.123,pp.32–73,2017. [55] Z.ShiandZ.Zou,“Canamachinegeneratehumanlikelanguage
[36] D.Xu,Y.Zhu,C.B.Choy,andL.Fei-Fei,“Scenegraphgeneration descriptions for a remote sensing image?” IEEE Transactions on
by iterative message passing,” in Proceedings of the IEEE/CVF GeoscienceandRemoteSensing,vol.55,no.6,pp.3623–3634,2017.
Conference on Computer Vision and Pattern Recognition, 2017, pp. [56] X. Yang, H. Sun, K. Fu, J. Yang, X. Sun, M. Yan, and Z. Guo,
5410–5419. “Automaticshipdetectioninremotesensingimagesfromgoogle
[37] Y.Liang,Y.Bai,W.Zhang,X.Qian,L.Zhu,andT.Mei,“Vrr-vg: earth of complex scenes based on multiscale rotation dense
Refocusingvisually-relevantrelationships,”inProceedingsofthe featurepyramidnetworks,”Remotesensing,vol.10,no.1,p.132,
IEEE/CVF International Conference on Computer Vision, 2019, pp. 2018.
10403–10412. [57] J. Ding, N. Xue, Y. Long, G.-S. Xia, and Q. Lu, “Learning roi
[38] D.A.HudsonandC.D.Manning,“Gqa:Anewdatasetforreal- transformer for oriented object detection in aerial images,” in
worldvisualreasoningandcompositionalquestionanswering,” Proceedings of the IEEE/CVF conference on computer vision and
inProceedingsoftheIEEE/CVFConferenceonComputerVisionand patternrecognition,2019,pp.2849–2858.
PatternRecognition,2019,pp.6700–6709. [58] X.Yang,J.Yang,J.Yan,Y.Zhang,T.Zhang,Z.Guo,X.Sun,and
[39] A.Kuznetsova,H.Rom,N.Alldrin,J.Uijlings,I.Krasin,J.Pont- K.Fu,“Scrdet:Towardsmorerobustdetectionforsmall,cluttered
Tuset, S. Kamali, S. Popov, M. Malloci, A. Kolesnikov et al., androtatedobjects,”inProceedingsoftheIEEE/CVFinternational
“Theopenimagesdatasetv4:Unifiedimageclassification,object conferenceoncomputervision,2019,pp.8232–8241.
detection,andvisualrelationshipdetectionatscale,”International [59] L. Hou, K. Lu, X. Yang, Y. Li, and J. Xue, “G-rep: Gaussian
JournalofComputerVision,vol.128,no.7,pp.1956–1981,2020. representation for arbitrary-oriented object detection,” Remote
[40] Z.Lin,F.Zhu,Y.Kong,Q.Wang,andJ.Wang,“Srsgands2sg: Sensing,vol.15,no.3,p.757,2023.
a model and a dataset for scene graph generation of remote [60] X.Yang,G.Zhang,W.Li,X.Wang,Y.Zhou,andJ.Yan,“H2rbox:
sensingimagesfromsegmentationresults,”IEEETransactionson Horizontal box annotation is all you need for oriented object
GeoscienceandRemoteSensing,vol.60,pp.1–11,2022. detection,” International Conference on Learning Representations,
[41] Y. Li, K. Li, Y. Zhang, L. Wang, and D. Zhang, “Aug: A new 2023.
datasetandanefficientmodelforaerialimageurbanscenegraph [61] Y. Yu, X. Yang, Q. Li, Y. Zhou, G. Zhang, F. Da, and J. Yan,
generation,”arXivpreprintarXiv:2404.07788,2024. “H2rbox-v2: Incorporating symmetry for boosting horizontal
[42] X.Lu,B.Wang,X.Zheng,andX.Li,“Exploringmodelsanddata boxsupervisedorientedobjectdetection,”inAdvancesinNeural
forremotesensingimagecaptiongeneration,”IEEETransactions InformationProcessingSystems,2023.
on Geoscience and Remote Sensing, vol. 56, no. 4, pp. 2183–2195, [62] J. Luo, X. Yang, Y. Yu, Q. Li, J. Yan, and Y. Li, “Pointobb:
2017. Learningorientedobjectdetectionviasinglepointsupervision,”19
inIEEE/CVFConferenceonComputerVisionandPatternRecognition, [84] X.Yang,Y.Zhou,G.Zhang,J.Yang,W.Wang,J.Yan,X.Zhang,
2024. and Q. Tian, “The kfiou loss for rotated object detection,” in
[63] Y.Yu,X.Yang,Q.Li,F.Da,J.Dai,Y.Qiao,andJ.Yan,“Point2rbox: InternationalConferenceonLearningRepresentations,2023.
Combine knowledge from synthetic visual patterns for end-to- [85] X. Yang, J. Yan, Z. Feng, and T. He, “R3det: Refined single-
endorientedobjectdetectionwithsinglepointsupervision,”in stage detector with feature refinement for rotating object,” in
IEEE/CVFConferenceonComputerVisionandPatternRecognition, ProceedingsoftheAAAIConferenceonArtificialIntelligence,vol.35,
2024. no.4,2021,pp.3163–3171.
[64] F.C.Akyon,S.O.Altinuc,andA.Temizel,“Slicingaidedhyper [86] J. Han, J. Ding, J. Li, and G.-S. Xia, “Align deep features for
inferenceandfine-tuningforsmallobjectdetection,”inProceed- oriented object detection,” IEEE transactions on geoscience and
ingsoftheIEEEInternationalConferenceonImageProcessing. IEEE, remotesensing,vol.60,pp.1–11,2021.
2022,pp.966–970. [87] Z. Yang, S. Liu, H. Hu, L. Wang, and S. Lin, “Reppoints: Point
[65] S. Deng, S. Li, K. Xie, W. Song, X. Liao, A. Hao, and H. Qin, set representation for object detection,” in Proceedings of the
“Aglobal-localself-adaptivenetworkfordrone-viewobjectde- IEEE/CVF international conference on computer vision, 2019, pp.
tection,”IEEETransactionsonImageProcessing,vol.30,pp.1556– 9657–9666.
1569,2020. [88] Z. Guo, C. Liu, X. Zhang, J. Jiao, X. Ji, and Q. Ye, “Beyond
bounding-box:Convex-hullfeatureadaptationfororientedand
[66] Z.-Z. Wu, X.-F. Wang, L. Zou, L.-X. Xu, X.-L. Li, and T. Weise,
denselypackedobjectdetection,”inProceedingsoftheIEEE/CVF
“Hierarchical object detection for very high-resolution satellite
conference on Computer Vision and Pattern Recognition, 2021, pp.
images,”AppliedSoftComputing,vol.113,p.107885,2021.
8792–8801.
[67] X. Chen, C. Wang, Z. Li, M. Liu, Q. Li, H. Qi, D. Ma, Z. Li,
[89] W.Li,Y.Chen,K.Hu,andJ.Zhu,“Orientedreppointsforaerial
and Y. Wang, “Coupled global–local object detection for large
object detection,” in Proceedings of the IEEE/CVF conference on
vhraerialimages,”Knowledge-BasedSystems,vol.260,p.110097,
computervisionandpatternrecognition,2022,pp.1829–1838.
2023.
[90] L.Hou,K.Lu,J.Xue,andY.Li,“Shape-adaptiveselectionand
[68] R.Girshick,“Fastr-cnn,”inProceedingsoftheIEEE/CVFInterna-
measurementfororientedobjectdetection,”inProceedingsofthe
tionalConferenceonComputerVision,2015,pp.1440–1448.
AAAIConferenceonArtificialIntelligence,vol.36,no.1,2022,pp.
[69] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-
923–932.
Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative ad-
[91] Z.Tian,C.Shen,H.Chen,andT.He,“Fcos:Fullyconvolutional
versarialnets,”AdvancesinNeuralInformationProcessingSystems,
one-stageobjectdetection,”inProceedingsoftheIEEEInternational
vol.27,2014.
ConferenceonComputerVision,2019,pp.9627–9636.
[70] P. Casanova, A. R. P. Lio, and Y. Bengio, “Graph attention net- [92] X. Yang and J. Yan, “Arbitrary-oriented object detection with
works,”ICLR.PetarVelickovicGuillemCucurullArantxaCasanova circularsmoothlabel,”inEuropeanConferenceonComputerVision,
AdrianaRomeroPietroLio`andYoshuaBengio,2018. 2020,pp.677–694.
[71] V. Nair and G. E. Hinton, “Rectified linear units improve re- [93] Y.YuandF.Da,“Onboundarydiscontinuityinangleregression
strictedboltzmannmachines,”inProceedingsofthe27thInterna- based arbitrary oriented object detection,” IEEE Transactions on
tionalConferenceonMachineLearning,2010,pp.807–814. PatternAnalysisandMachineIntelligence,2024.
[72] Z.Liu,Y.Lin,Y.Cao,H.Hu,Y.Wei,Z.Zhang,S.Lin,andB.Guo, [94] Y. Xu, M. Fu, Q. Wang, Y. Wang, K. Chen, G.-S. Xia, and
“Swintransformer:Hierarchicalvisiontransformerusingshifted X.Bai,“Glidingvertexonthehorizontalboundingboxformulti-
windows,”inProceedingsoftheIEEE/CVFInternationalConference orientedobjectdetection,”IEEEtransactionsonpatternanalysisand
onComputerVision,2021,pp.10012–10022. machineintelligence,vol.43,no.4,pp.1452–1459,2020.
[73] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning [95] J. Han, J. Ding, N. Xue, and G.-S. Xia, “Redet: A rotation-
for image recognition,” in Proceedings of the IEEE Conference on equivariantdetectorforaerialobjectdetection,”inProceedingsof
ComputerVisionandPatternRecognition,2016,pp.770–778. theIEEE/CVFconferenceoncomputervisionandpatternrecognition,
[74] S.Ren,K.He,R.Girshick,andJ.Sun,“Fasterr-cnn:Towardsreal- 2021,pp.2786–2795.
timeobjectdetectionwithregionproposalnetworks,”Advances [96] X. Xie, G. Cheng, J. Wang, X. Yao, and J. Han, “Oriented r-cnn
inNeuralInformationProcessingSystems,vol.28,2015. forobjectdetection,”inProceedingsoftheIEEE/CVFInternational
[75] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dolla´r, “Focal ConferenceonComputerVision,2021,pp.3520–3529.
loss for dense object detection,” in Proceedings of the IEEE/CVF [97] Y.Li,Q.Hou,Z.Zheng,M.-M.Cheng,J.Yang,andX.Li,“Large
InternationalConferenceonComputerVision,2017,pp.2980–2988. selectivekernelnetworkforremotesensingobjectdetection,”in
[76] Z.CaiandN.Vasconcelos,“Cascader-cnn:Highqualityobject Proceedings of the IEEE/CVF International Conference on Computer
detectionandinstancesegmentation,”IEEEtransactionsonpattern Vision,2023,pp.16794–16805.
analysisandmachineintelligence,vol.43,no.5,pp.1483–1498,2019. [98] I.LoshchilovandF.Hutter,“Decoupledweightdecayregulariza-
tion,”inInternationalConferenceonLearningRepresentations,2018.
[77] Z.Tian,C.Shen,H.Chen,andT.He,“Fcos:Asimpleandstrong
[99] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and
anchor-freeobjectdetector,”IEEETransactionsonPatternAnalysis
A.Zisserman,“Thepascalvisualobjectclasses(voc)challenge,”
andMachineIntelligence,vol.44,no.4,pp.1922–1933,2020.
InternationalJournalofComputerVision,vol.88,pp.303–338,2010.
[78] C. Feng, Y. Zhong, Y. Gao, M. R. Scott, and W. Huang, “Tood:
[100] K.Chen,J.Wang,J.Pang,Y.Cao,Y.Xiong,X.Li,S.Sun,W.Feng,
Task-alignedone-stageobjectdetection,”in2021IEEE/CVFInter-
Z.Liu,J.Xuetal.,“Mmdetection:Openmmlabdetectiontoolbox
nationalConferenceonComputerVision(ICCV). IEEEComputer
andbenchmark,”arXivpreprintarXiv:1906.07155,2019.
Society,2021,pp.3490–3499.
[101] Y. Zhou, X. Yang, G. Zhang, J. Wang, Y. Liu, L. Hou, X. Jiang,
[79] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable
X. Liu, J. Yan, C. Lyu, W. Zhang, and K. Chen, “Mmrotate: A
detr:Deformabletransformersforend-to-endobjectdetection,”
rotatedobjectdetectionbenchmarkusingpytorch,”inProceedings
inInternationalConferenceonLearningRepresentations,2020.
of the 30th ACM International Conference on Multimedia, 2022, p.
[80] Y. Zeng, Y. Chen, X. Yang, Q. Li, and J. Yan, “Ars-detr: Aspect
7331–7334.
ratio-sensitive detection transformer for aerial oriented object
detection,” IEEE Transactions on Geoscience and Remote Sensing,
vol.62,pp.1–15,2024.
[81] S.Zhang,C.Chi,Y.Yao,Z.Lei,andS.Z.Li,“Bridgingthegap
between anchor-based and anchor-free detection via adaptive
trainingsampleselection,”inProceedingsoftheIEEE/CVFconfer-
enceoncomputervisionandpatternrecognition,2020,pp.9759–9768.
[82] X. Yang, X. Yang, J. Yang, Q. Ming, W. Wang, Q. Tian, and
J.Yan,“Learninghigh-precisionboundingboxforrotatedobject
detection via kullback-leibler divergence,” Advances in Neural
InformationProcessingSystems,vol.34,pp.18381–18394,2021.
[83] X. Yang, J. Yan, Q. Ming, W. Wang, X. Zhang, and Q. Tian,
“Rethinking rotated object detection with gaussian wasserstein
distance loss,” in International Conference on Machine Learning.
PMLR,2021,pp.11830–11841.