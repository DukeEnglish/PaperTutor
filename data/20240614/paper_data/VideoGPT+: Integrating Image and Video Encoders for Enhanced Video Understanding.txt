VideoGPT+ : Integrating Image and Video
Encoders for Enhanced Video Understanding
MuhammadMaaz1,HanoonaRasheed1,SalmanKhan1,2,FahadShahbazKhan1,3
1MohamedbinZayedUniversityofAI,UAE
2AustralianNationalUniversity,Australia 3LinköpingUniversity,Sweden
Abstract
Buildingontheadvancesoflanguagemodels,LargeMultimodalModels(LMMs)
have contributed significant improvements in video understanding. While the
currentvideoLMMsutilizeadvancedLargeLanguageModels(LLMs),theyrely
oneitherimageorvideoencoderstoprocessvisualinputs,eachofwhichhasits
ownlimitations. Imageencodersexcelatcapturingrichspatialdetailsfromframe
sequencesbutlackexplicittemporalcontext,whichcanbeimportantinvideoswith
intricateactionsequences. Ontheotherhand,videoencodersprovidetemporal
contextbutareoftenlimitedbycomputationalconstraintsthatleadtoprocessing
onlysparseframesatlowerresolutions,resultinginreducedcontextualandspatial
understanding. Tothisend,weintroduce VideoGPT+,whichcombinesthecom-
plementarybenefitsoftheimageencoder(fordetailedspatialunderstanding)and
thevideoencoder(forglobaltemporalcontextmodeling). Themodelprocesses
videosbydividingthemintosmallersegmentsandappliesanadaptivepooling
strategyonfeaturesextractedbybothimageandvideoencoders. Ourarchitecture
showcasesimprovedperformanceacrossmultiplevideobenchmarks,including
VCGBench,MVBenchandZero-shotquestion-answering. Further,wedevelop
112Kvideo-instructionsetusinganovelsemi-automaticannotationpipelinewhich
furtherimprovesthemodelperformance. Additionally,tocomprehensivelyeval-
uate video LMMs, we present VCGBench-Diverse, covering 18 broad video
categoriessuchaslifestyle,sports,science,gaming,andsurveillancevideos. This
benchmarkwith4,354question-answerpairsevaluatesthegeneralizationofex-
istingLMMsondensevideocaptioning,spatialandtemporalunderstanding,and
complexreasoning,ensuringcomprehensiveassessmentacrossdiversevideotypes
anddynamics. Code: https://github.com/mbzuai-oryx/VideoGPT-plus.
1 Introduction
Existing methods for video understanding often rely solely on either image encoders or video
encoders [1, 2, 3]. Most works focus on image encoders, which encode multiple frames and
eitherfusetheinformationorconcatenatetheembeddingsbeforepassingthemtotheLLM.When
fusingtheinformation,spatialortemporalpoolingistypicallyused[1]. Spatialpoolinghasshown
minimaleffectivenessincapturingvideoinformation,whereastemporalpoolingretainssomespatial
informationbutlacksexplicittemporalcontext.Ontheotherhand,concatenatingembeddingswithout
pooling[2,3,4]canrapidlyincreasecomputationalcomplexityduetotheextendedcontextlength
requiredbytheLLM,limitingthenumberofframesthatcanbeprocessed. Whilethisapproach
providesbetterspatialrepresentation,theoverallcontextisstilllimitedtofewframes. Thelimited
contextresultsinapoorunderstandingofthevideo, especiallyifauniformsamplingstrategyis
employed,asitonlycapturessmallsegmentsofthevideo,missingimportanttemporaldynamics.
TechnicalReport.
4202
nuJ
31
]VC.sc[
1v81490.6042:viXraInordertoaddressthesechallenges,wepropose
VideoGPT+whicheffectivelycombinesthemer-
itsofbothimageandvideoencoders(seeFig.2).
Byleveraginganimageencoderforrichspatial
details and a video encoder for global tempo-
ralcontext,ourmodelachievesimprovedvideo
understanding. Tomodelfinegrainedtemporal
dynamics in VideoGPT+ , we use a segment-
wise sampling strategy. Unlike uniform sam-
plingusedinexistingvideoLMMs[1],which
maymissimportanttemporaldynamics,ourap-
proachdividesthevideointosmallersegments
and applies segment-wise sampling. This en-
suresthatthemodelcapturesrepresentativein-
formationfromdifferentsegmentsofthevideo,
enablingamorecomprehensiveunderstanding. Figure1: Performancecomparisonof VideoGPT+
withvariousSoTAmodelsacrossmultiplevideobench-
To facilitate the integration of image and
marks. VideoGPT+ demonstrates better perfor-
videofeatures, VideoGPT+introducesavisual mance compared to various models [5, 2, 6, 1] on
adaptermodulethatcombinestheircomplimen- video conversation benchmarks: VCGBench [1] and
tarybenefits. Thismoduleperformsprojection MVBench [5], Zero-shot video question answering:
and pooling operations, mapping both image MSVD-QA,MSRVTT-QA,ActivityNet-QA.Wealso
and video features to a common space while evaluateon VCGBench-Diversethatcovers18broad
reducingcomputationalcomplexity. Byalign- videocategories(acrossdensecaptioning,spatialunder-
standing,andreasoning).
ingthefeaturesinthismanner, themodelcan
effectivelyutilizethecombinedspatialandtemporalinformationforimprovedvideounderstanding.
Wedemonstratetheeffectivenessof VideoGPT+acrossmultiplevideo-conversationbenchmarks,
includingVCGBench[1],MVBench[7],andZero-shotquestion-answering[1],whereitoutperforms
previousSoTAapproaches(seeFig.1).Further,wedevelop VCG+112Kusinganovelsemi-automatic
annotationpipeline(seeFig.3),whichprovidesdensevideocaptionsalongwithspatialunderstanding
andreasoning-basedquestion-answer(QA)pairs,furtherenhancingthemodel’sperformance. We
alsopropose VCGBench-Diverse,extendingVCGBench[1]byincludingvideosfrom18different
domainstoextensivelyevaluatethevideo-basedconversationmodelsindiversedomains(seeFig.4).
Ourworkhasthreemaincontributions:
• Wepresent VideoGPT+,thefirstvideo-conversationmodelthatbenefitsfromadual-encoding
schemebasedonbothimageandvideofeatures. Thesecomplimentarysetsoffeaturesofferrich
spatiotemporaldetailsforimprovedvideounderstanding(Sec.3).
• Addressing the limitations of existing VideoInstruct100K dataset [1], we develop VCG+112K
withanovelsemi-automaticannotationpipeline,offeringdensevideocaptionsalongwithspatial
understandingandreasoning-basedQApairs,furtherimprovingthemodelperformance(Sec.4).
• Recognizing the lack of diverse benchmarks for video-conversation task, we propose
VCGBench-Diverse, which provides 4,354 human annotated QA pairs across 18 video cate-
goriestoextensivelyevaluatetheperformanceofavideo-conversationmodel(Sec.5).
2 RelatedWorks
Buildingonadvancesinlanguagemodels,LLMsofferaflexibleinterfaceforvariousmultimodal
applications. Earlyeffortsinimage-basedconversationmodelssuchasBLIP-2[8],MiniGPT-4[9]
andLLaVA[10,11]projectimagefeaturesintothelanguagespacethroughalearnablemoduleand
performinstructiontuningforvisualconversationscapabilities. Othereffortsextendthesemodelsto
visualgroundingtasks[12,13,14],exploringthepotentialofLLMsincomplexvisiontasks.
VideoConversationModels: InitialworkslikeVideo-ChatGPT[1]andVideo-LLaMA[15]extend
image-basedmodelstothevideodomainbyintroducingcomponentstoencodetemporalfeatures,
whereframe-levelvisualfeaturesarefedtotheLLM.However,thisiscomputationallyexpensive
andquicklyfillsitscontextwindow. Toaddressthisissue,Video-ChatGPT[1]employsspatialand
temporalpooling. LLaMA-Vid[16]proposesrepresentingasingleimagewithtwotokens,context
andcontent. IG-VLM[17]treatsavideoasagridofimages,whileLITA[18]employsslow-fast
2Figure2: Overviewof VideoGPT+. VideoGPT+isalargemultimodalmodelforvideounderstanding.Ituses
adual-encoderdesignthatcombinesthecomplementarystrengthsofanimageencoderandavideoencoder.The
imageencodercapturesdetailedspatialfeatures,whilethevideoencodercapturestemporaldynamicsacross
multipleframes.Toretainfine-grainedtemporaldetailswhileensuringefficiency,weusesegment-wiseframe
samplinginsteadofrandomsparsesampling.Bothsetsoffeaturesarethenprojectedintoaunifiedspacethrough
Vision-Language(V-L)projectionlayersandtheresultingtokensarepooledandconcatenatedbeforebeing
processedbyaLargeLanguageModeltogeneratecomprehensiveresponsestovideo-basedquestions.Symbols
indicatesfrozencomponents, indicatestrainablecomponents,andthe indicatesLoRA-training.
tokenpoolingtoreducethenumberofvisualfeatures. Chat-UniVi[2]usesclusteringinbothspatial
andtemporaldimensionstomergetokens, andVideoChat[5]usesQ-Former[8]tolearnafixed
numberofqueriesbycross-attendingtothevisualfeatures. MobileVLM[19,20]utilizealightweight
CNNtoreducethespatialdimensions. Othernotablemethodsinclude[21,6,22,23,24].
Alternatively, methods such as VideoChat2 [7] use pretrained video encoders. Although video
encodersprovidetemporalcontext,theyarelimitedbycomputationalconstraints,operatingwith
limited frames at lower resolutions, restricting temporal context and spatial understanding. Our
VideoGPT+modeladdressestheseissuesbyusingsegment-wisesamplingandeffectivelycombining
themeritsofimageandvideoencoderstocapturerichspatialandtemporaldetails(seeFig.2).
VideoInstructionTuningDatasets: VideoChat[5]buildsavideo-instructiontuningdatasetcon-
sisting of 7K instructions using videos from WebVid-10M [25]. Video-ChatGPT [1] introduces
a semi-automatic annotation pipeline to generate VideoInstruct100K using videos from Activi-
tyNet[26]. VideoChat2[7]combinesmultipleexistingimageandvideodatasetstodevelopa1.9M
jointimage-videoinstructiontuningdataset. Inourexperiments, weuseVideoInstruct100Kand
asubsetofthedatasetfromVideoChat2. Additionally,addressingthelimitationsoftheVideoIn-
struct100Kdataset[1],wedevelop VCG+112Kthroughanovelsemi-automaticannotationpipeline,
which provides dense video captions along with 112K QA pairs targeting reasoning, spatial and
temporalunderstanding,whichfurtherimprovesmodel’sunderstandingofvideocontent(seeFig.3).
VideoConversationBenchmarks: Video-ChatGPT[1]introducesVCGBenchandZero-shotQA
benchmarks, where VCGBench includes 500 videos with 3000 QA pairs, evaluated using GPT-
3.5 across various metrics. Despite its comprehensive evaluation, it only contains videos from
the ActivityNet dataset. The Zero-shot evaluation covers MSVD-QA [27], MSR-VTT-QA [27],
TGIF-QA [28], and ActivityNet-QA [26]. MVBench [7] consists of 4K QA pairs evaluating 20
temporal tasks, though it mostly includes short videos averaging 5-40 seconds. Considering the
limitationofexistingbenchmarks,whichoftenlackfocusongeneralizationanddiversity,wepropose
VCGBench-Diverse,featuring4,354QApairsfrom877videosacross18domains(seeFig.4).
3 Method
Foreffectivevideounderstanding, combiningdetailedspatialinformationwithexplicittemporal
contextiscrucial. Toachievethis,wepropose VideoGPT+,whichfeaturesadualencoderdesign
thatleveragesthecomplementarystrengthsofanimageencoderandavideoencoder.
3OverallArchitecture: Theoverallarchitectureconsistsof(i)segment-wisesampling,(ii)dualvisual
encoder,(iii)vision-languageadaptersthatprojectvisionfeaturestothelanguagedomainand(iv)
alargelanguagemodel. Framesselectedthroughasegment-wisesamplingstrategyareencoded
throughadualencoderconsistingofanimageandavideoencoder. Bothsetsoffeaturesareprojected
tolanguagespaceusingvision-language(V-L)adapters,andtheresultingtokensarepooledthrough
adaptivetokenpoolingandconcatenatedbeforebeingfedtotheLLM(seeFig.2).
Segment-wise Sampling: To extract fine-grained temporal cues, we use a segment-wise frame
samplingstrategy. GivenaninputvideoV ∈ RT×H×W×C,wedivideitintoK segments,where
eachsegmentconsistsofn= T frames. Thus,thevideocanberepresentedasV=[V ]K . Each
K k k=1
segmentV ∈Rn×H×W×C canbedescribedasasequenceofframes,X ,whereV =[X ]n .
k i k i,j j=1
Thevideosegmentsaredownsampledtoalowerresolutionofn×h×w×cforvideoencoding.
Comparedtoauniformsampling,segment-wisesamplingbetteralignswithourdualencoderdesign.
Videoencodersoftenfacecomputationalconstraints,limitingthemtoprocessingonlysparseframes.
Uniformsamplingincreasestheself-attentioncomputationcomplexityasitrequiresattendingto
featuresofallframes. Additionally,videoencodersaretypicallytrainedwithsparseframes,and
providingmoreframescanhindertheirabilitytoaccuratelycapturetemporalinformation. Incontrast,
thesegment-wisesamplingstrategydividesthevideointosmaller,manageablesegments,enabling
thevideoencodertoefficientlycapturerichtemporalcueswithineachsegment.
DualVisionEncoder: Ourdesignleveragesthecomplementarystrengthsofanimageencoderthat
capturesdetailedspatialfeaturesandavideoencoderthatprovidesexplicittemporalcontext. The
imageencoderg,processesT frames,g(X)∈RT×Hg×Wg×Dg,producinglocalfeaturesthatprovide
frame-levelcontext. Meanwhile,thevideoencoderh,operatesonlow-resolutionvideosegmentsV ,
k
yieldingglobalfeaturesthatprovidesegment-wisecontext,h(V k)∈Rn×hh×wh×Dh.
Theprimarygoalof VideoGPT+istoleveragethecapabilitiesofapre-trainedLLMalongsidevisual
modalitiesfrombothapre-trainedimageencoderandapre-trainedvideoencoder. Specifically,we
utilizethepre-trainedCLIPmodel,ViT-L/14(336×336)[29]astheimageencoder,andInternVideo-
v2(224×224)[30]asthevideoencoder. Thesemodelsareselectedfortheirrobustperformance
and their ability to complement each other in capturing both spatial and temporal information.
Bothencodersarepre-trainedonlarge-scaledatasetsinamultimodalsettingusingcontrastiveloss,
facilitatingtheirintegrationwithinourarchitecture.
VisualAdapter:Theoutputembeddingsfromthesecondlastlayerofbothimageandvideoencoders
arepassedthroughseparateV-Lprojectionlayers, W andW , respectively. TheseMulti-Layer
g h
perceptrons(MLPs)projectthevisualfeaturesintothelanguagespace. Theprojectionlayersare
trainable,whilethevisualencodersremainfrozen,preservingtherich,pre-trainedrepresentations.
Theprojectedembeddingsarereshapedbackintotheirgridformsandsubjectedtoa2×2adaptive
tokenpooling,whichoperatesonthespatialdimensionsofthelocalandglobalfeatures. Thispooling
reducesthetokenlengthbyafactorof4,therebyallowingtofitinlargervisualcontextwithinthesame
LLMcontextwindow. ThepooledembeddingsfromthelocalfeaturesformEimg ∈RT×hg×wg×Dt,
whilethepooledembeddingsfromtheglobalfeaturesofeachsegmentformEvid ∈Rn×hh×wh×Dt.
LargeLanguageModel: WeobtainthefinalrepresentationbyconcatenatingtheembeddingsEimg
with K segment-wise embeddings Evid, such that we have detailed spatial representation across
allsegmentsfollowedbytheirglobaltemporalcontext. Wethenconcatenatethetextembeddings
Etext ∈RL×Dt oftheusertextquerywiththevisualembeddings,
E=[Eimg,Evid,...,Evid,Etext]. (1)
1 K
This integration ensures that the LLM receives a sequence of embeddings that include detailed
spatialfeaturesfromtheimageencoderandcomprehensivetemporalcontextfromthevideoencoder,
allowing for robust video understanding. The LLM is fine-tuned using LoRA [31] in an auto-
regressivemannerwithanext-tokenpredictionloss. RefertoFig.2fordetailedillustration.
4 Dataset
Video-ChatGPT [1] introduces the VideoInstruct100K dataset, which employs a semi-automatic
annotation pipeline to generate 75K instruction-tuning QA pairs. To address the limitations of
thisannotationprocess,wepresent VCG+112Kdatasetdevelopedthroughanimprovedannotation
4Figure3: Illustrationofthesemi-automaticannotationprocessin VCG+112K.Thefigureshowshowwe
useground-truthvideocaptionsandframe-leveldescriptionstogenerateadetailedvideodescription.GPT-4is
usedtoremoveirrelevantandconflictingnoisyinformationintheframe-leveldescriptionstoproduceahigh-
qualityvideodescription.Thesemi-automaticannotationprocessintegratesspatial,temporalandevent,and
reasoningdetailsintothebriefinformationwestartwith.Thisdensevideodescriptionisthenusedtogenerate
instruction-tuningQApairsusingGPT-3.5.WeprovidedetailedpromptsusedinbothstagesinAppendix11
(seeFigs.8and9). WealsocomparethevideodescriptionintheVideoInstruct100K[1]datasettoshowthe
improvementinqualityachievedbyournewannotationpipeline.
pipeline. Ourapproachimprovestheaccuracyandqualityofinstructiontuningpairsbyimproving
keyframeextraction,leveragingSoTAlargemultimodalmodels(LMMs)fordetaileddescriptions,
andrefiningtheinstructiongenerationstrategy.
Keyframe Extraction: VideoInstruct100K uses a fixed number of video keyframes, regardless
of video length or dynamics, to generate frame-level dense captions. This often results in both
insufficientandredundantinformation. Weaddressthisbyfirstextractingscenesfromvideos[32],
andthenselectingonekeyframe/scene. Consequently,weobtaindetailedinformationforvideoswith
richcontentandreduceredundancyforvideoswithlesscontent. Itprovidesbettervisualcontextby
extractingmorestablekeyframes,thusofferingamoreaccuratevideorepresentation.
Frame-Level Descriptions: After extracting keyframes, we use a SoTA image LMM, LLaVA-
v1.6[33],togeneratedensedescriptionsforeachkeyframe. Thesedescriptionsencompasscompre-
hensivevisualdetails,includingspatialattributes,scenecontext,andobjectcharacteristics,which
areoftenabsentinconcisegroundtruthcaptions. Whilegroundtruthcaptionsareprecise,theylack
thegranularitytocaptureintricatevisualandspatialinformation. Toaddressthis,weaugmentthem
captionswithdetailedbutnoisyinformationfromtheframe-leveldescriptions,thusenhancingthe
qualityandaccuracyofthesubsequentvideodescriptions.
DetailedVideoDescriptions: VideoInstruct100K[1]promptsGPT-3.5directlywithframe-level
descriptionsandconcisegroundtruthcaptionstogenerateQApairs,imposingasignificantcognitive
loadonthemodeltoverifyframe-leveldescriptionswiththegroundtruth. Weimprovethisprocess
byfirstcreatingacoherentanddetailedvideodescription. WepromptGPT-4tointegratethedetailed
frame-leveldescriptions withthe groundtruth captionsby comparing informationand removing
anyinconsistencies. Theresultingdetaileddescriptionsincludeatimelineofevents,actions,object
attributes,andscenesettings,providingathoroughrepresentationofthevideocontent.Thisstructured
inputsimplifiesthetaskforLLM,therebyenhancingthegeneratedQApairsquality.
ImprovedInstructionTuningData:Usingthegroundtruthcaptionsanddetailedvideodescriptions,
we generate two types of high-quality QA pairs using GPT-3.5: descriptive and concise. For
descriptiveinstructionpairs, wefocusonthreecategories: (i)densecaptioning, whichprovides
descriptions of the video covering the entire sequence of events and visual details; (ii) detailed
temporalinformation,whichaddressesthesequenceofeventsandtheirdependencytolearntemporal
relationships;and(iii)genericquestionanswering,whichinvolvesin-depthquestionsaboutdifferent
actions,theirconsequences,andotherdetailedaspectsofthevideo. Forconciseinstructionpairs,
we target (i) spatial reasoning, focusing on understanding and describing spatial details such as
scenesettings,numberofobjects,attire,andlocations;(ii)reasoningofevents,coveringthecausal
5Figure 4: Illustrationof VCGBench-Diversevideoconversationalbenchmark. VCGBench-Diverse
comprehensivebenchmarkisdesignedtoevaluatevideoLMMsacross18broadvideocategories.With4,354QA
pairs, VCGBench-Diversetestsgeneralizationondensevideocaptioning,spatialandtemporalunderstanding,
andcomplexreasoning.Itcoversfivevideo-capturingmethods,ensuringdiversityandrobustgeneralizationand
sixreasoningcomplexities,assessingvariousanalyticalandcomprehensionskills.
relationships between events; and (iii) short temporal questions, addressingspecific moments or
sequences,suchaswhathappenedatthebeginningorend.
5 ProposedBenchmark
Recognizing the limited diversity in existing video conversation benchmarks, we introduce
VCGBench-DiversetocomprehensivelyevaluatethegeneralizationabilityofvideoLMMs. While
VCG-Bench [1] provides an extensive evaluation protocol, it is limited to videos from the Ac-
tivityNet200 [26] dataset. Our benchmark comprises a total of 877 videos, 18 broad video cate-
gories and 4,354 QA pairs, ensuring a robust evaluation framework. The detailed breakdown of
VCGBench-Diverse is illustrated in Fig. 4, showcasing the distribution of videos across content
domains,videocapturingmethods,andreasoningcomplexities.
We collect videos from 18 distinct domains, including lifestyle, how-to, science and technology,
news,travel,entertainment,film,sports,comedy,activism,gaming,education,surveillance,pets,
cooking,music,automobile,andtraffic(seeFig.4). Thesecategoriesencompassabroadspectrumof
real-worldscenarios,ensuringthatmodelsareevaluatedonadiversesetofchallenges. Inadditionto
contentdiversity, VCGBench-Diverseincludesavarietyofvideocapturemethods,whichensuresa
comprehensiveassessmentofrobustnesstodifferentfilmingtechniques,cameramovements,quality
levelsandlighting. Thebenchmarkcoversfivevideocapturemethodsincludingstaticandcontrolled
settings,dynamicandunpredictablesettings,fixedcameraperspectives,professionalandhigh-quality
videos,anduncontrolledandvariablequality. Further,thebenchmarkevaluatesmodelsacrosssix
reasoningcomplexities,includingsequentialunderstanding,complexactionandpredictivereasoning,
contextualandworldknowledgereasoning,causalreasoning,narrativeandemotionalreasoning,and
analyticalandcriticalreasoning,whichiscrucialforunderstandingdiversevideocontent.
Thevideosin VCGBench-DiversearesourcedfromHDVILA[34], MPII[35], YouCook2[36],
UCFCrime[37],andSTUDTraffic[38]. Thevideodurationsrangefrom29secto471sec,with
anaverageof217sec. Humanannotatorsaretaskedwithwritingdetaileddescriptionsbasedon
theirunderstandingofbothaudioandvisualelementsofthevideos. Thiscomprehensiveannotation
processinvolvesasetofannotatorswhoareprovidedwithaninitialsetoftenvideoseach. These
annotationsundergoameta-reviewstagewherefeedbackisprovided,andnecessarycorrectionsare
madetomeettherequiredstandards. Followingthis, annotatorsreceiveadditionalbatches, with
randomsamplesbeingselectedforqualitychecksbythemeta-reviewer. Thefinalhumanannotations
areutilizedtogenerateQApairsusingGPT-3.5,basedonpromptsdetailedinFig.10.
FollowingVCG-Bench[1],theevaluationiscomputedoverfivedifferentaspects: (i)correctness
ofinformation(ii)detailorientation(iii)contextualunderstanding(iv)temporalunderstandingand
(v)consistency. Additionally, VCGBench-Diverseprovidesabreakdownofperformanceacross
threekeyaspects: (i)densevideocaptioning, whichassessestheabilitytogeneratedetailedand
accuratedescriptionsofthevideocontent,(ii)spatialunderstanding,whichevaluatesthecapability
6tounderstandanddescribethespatialrelationshipsandsettingswithinthevideo,and(iii)reasoning,
whichteststheadeptnessininferringandexplainingcausalrelationshipsandactionswithinthevideo.
6 Experiments
Weperformquantitativeevaluationof VideoGPT+onfourstandardbenchmarks: i)VCGBench[1],
ii) VCGBench-Diverse,iii)MVBench[7]andiv)Zero-shotQA.
ImplementationDetails: WeuseCLIP-L/14[29]asourimageencoder,InternVideo-v2[30]stage-2
1B model as our video encoder in conjunction with Phi-3-Mini-3.8B [39] based LLM with 4K
context window in our experiments. The image encoder operates at 336×336, while the video
encoderoperatesat224×224resolution. Ourtrainingconsistsoftwopretrainingstagesandone
instruction-tuningstage. Inthepretrainingstage,wetrainwithonlytheimageencoderandonlythe
videoencoderontheCC-595Kdataset[40],withonlythevisualadaptersbeinglearnedwhilethe
restofthemodeliskeptfrozen. Duringtheinstruction-tuningstage,weuseLoRA[41]withr =64
forLLM,whilevisualadaptersarefullytrainedandvisionencodersarekeptfrozen. TheLRissetto
1e−3duringpretrainingand2e−4duringinstructiontuning.
ForexperimentsonVCGBench, VCGBench-DiverseandZero-shotQA,wesample16framesfrom
videos,whileforMVBenchwhichconsistsofrelativelyshortervideos,wesample8frames. Wekeep
thesamesamplingstrategyduringinference. ForVCGBenchand VCGBench-Diverse,themodelis
trainedonVideoInstruct100K[1], VCG+112K,conversationandcaptiondatafromVideoChat[5]and
VQAdatasetfromWebVid[25],thatcombinestoapproximately260Ksingleturnconversations. For
MVBench,themodelistrainedonKinetics-710[42],Something-Something-v2[43],conversations
fromVideoChat[5],CLEVRER[44],VQAdatasetfromWebVid[25]andNExT-QA[45]datasets,
which combines to approximately 330K single turn conversations. We run all trainings for one
epoch. Followingpreviousapproaches[1,2,3],weemployGPT-3.5-Turbo-0613forVCGBenchand
Zero-shotQAevaluation. However,forourproposed VCGBench-Diverse,weemploythelatest
GPT-3.5-Turbo-0125forevaluation.
VCGBench: The benchmark con-
Method CI DO CU TU CO Avg.
sistsofapproximately3000QApairs
generatedusing500human-annotated Video-ChatGPT[1] 2.40 2.52 2.62 1.98 2.37 2.38
videos from ActivityNet [26]. The BT-Adapter[21] 2.68 2.69 3.27 2.34 2.46 2.69
VTimeLLM[24] 2.78 3.10 3.40 2.49 2.47 2.85
benchmarkevaluatestheresponseson
Chat-UniVi[2] 2.89 2.91 3.46 2.89 2.81 2.99
fivedifferentaspects:i)Correctnessof LLAMA-VID[16] 2.96 3.00 3.53 2.46 2.51 2.89
Information (CI), which assesses the Video-LLaVA[6] 2.84 2.86 3.44 2.46 2.57 2.81
correctnessoftheresponsetoensure VideoChat2[7] 3.02 2.88 3.51 2.66 2.81 2.98
it aligns with the video contents, ii) IG-VLM[17] 3.11 2.78 3.51 2.44 3.29 3.03
VideoGPT+(ours) 3.27 3.18 3.74 2.83 3.39 3.28
DetailOrientation(DO),whichevalu-
atesthedepthoftheresponse,iii)Con-
Table1: Performanceof VideoGPT+onVCGBench[1].All
textualUnderstanding(CU),whichas-
models use 16 frames except Video-ChatGPT and Chat-UniVi
sessesiftheresponsealignswiththe whichuse100and64framesrespectively.
overallcontextofthevideo,iv)Tempo-
ralUnderstanding(TU),whichassessesthemodel’sabilitytoidentifytemporalsequencesaccurately,
andv)Consistency(CO),whichevaluatestheconsistencyinthemodelresponsetosimilarquestions.
Table1comparesourmodelwithpreviousSoTAapproaches. VideoGPT+achievesanaveragescore
of3.28surpassingpreviousbestmethodbyamarginof0.25(5%).
VCGBench-Diverse: Weprovideaquantitativecomparisonof VideoGPT+againstpreviousSoTA
approacheson VCGBench-Diverse,whichcontains4,354QApairsfrom877videos. Following[1],
weevaluatetheCorrectnessofInformation(CI),DetailOrientation(DO),ContextualUnderstanding
(CU),TemporalUnderstanding(TU),andConsistency(CO).Additionally,weprovideresultsfor
densecaptioning,spatialunderstanding,andvisualreasoningabilities. Theresultsarepresentedin
Table2. VideoGPT+achievesanaveragescoreof2.47surpassingallpreviousmethods. Further,
VideoGPT+achievesascoreof1.38,2.80,and3.63ondensecaptioning,spatialunderstanding,and
visualreasoning,respectively. Notably, VideoGPT+achievesimprovementsinspatialandtemporal
understanding,surpassingpreviousbestmodelsby0.37(7.4%)and0.23(4.6%),respectively. Thisis
attributedtothedualencoderarchitecture,wherethehigh-resolutionimageencoderenhancesspatial
understandingandthevideoencoderimprovestemporalaccuracy.
7Method CI DO CU TU CO Avg. Caption Spatial Reasoning
Video-ChatGPT(ACL2024)[1] 2.07 2.42 2.46 1.39 2.06 2.08 0.89 2.25 3.60
BT-Adapter(CVPR2024)[21] 2.20 2.62 2.59 1.29 2.27 2.19 1.03 2.35 3.62
VTimeLLM(CVPR2024)[24] 2.16 2.41 2.48 1.46 2.35 2.17 1.13 2.29 3.45
Chat-UniVi(CVPR2024)[2] 2.29 2.56 2.66 1.56 2.36 2.29 1.33 2.36 3.59
VideoChat2(CVPR2024)[7] 2.13 2.42 2.51 1.66 2.27 2.20 1.26 2.43 3.13
VideoGPT+(ours) 2.46 2.73 2.81 1.78 2.59 2.47 1.38 2.80 3.63
Table 2: Performanceof VideoGPT+onVCGBench-Diverse. Allmodelsuse16framesexceptVideo-
ChatGPTandChat-UniVi,whichuse100and64frames,respectively.Thegoodperformanceofourmodelon
VCGBench-Diverseshowsitsgeneralizationtodiversescenarios.
Model AS AP AA FA UA OE OI OS MD AL ST AC MC MA SC FP CO EN ER CI Avg.
Random 25.025.033.325.025.033.325.033.325.025.025.033.325.033.333.325.033.325.020.030.9 27.3
GPT-4V[46] 55.563.572.046.573.518.559.029.512.040.583.539.012.022.545.047.552.031.059.011.0 43.5
Otter-V[47] 23.023.027.527.029.553.028.033.024.523.527.526.028.518.038.522.022.023.519.019.5 26.8
mPLUG-Owl-V[48]22.028.034.029.029.040.527.031.527.023.029.031.527.040.044.024.031.026.020.529.5 29.7
Video-ChatGPT[1] 23.526.062.022.526.554.028.040.023.020.031.030.525.539.548.529.033.029.526.035.5 32.7
VideoLLaMA[15] 27.525.551.029.039.048.040.538.022.522.543.034.022.532.545.532.540.030.021.037.0 34.1
VideoChat[5] 33.526.556.033.540.553.040.530.025.527.048.535.020.542.546.026.541.023.523.536.0 35.5
VideoChat2[7] 66.047.583.549.560.058.071.542.523.023.088.539.042.058.544.049.036.535.040.565.5 51.1
VideoGPT+(ours) 69.060.083.048.566.585.575.536.044.034.089.539.571.090.545.053.050.029.544.060.0 58.7
Table3: Performanceof VideoGPT+onMVBench.Following[7],weevaluateon20tasksincludingAS:
ActionSequence,AP: ActionPrediction,AA: ActionAntonym,FA: Fine-grainedAction,UA: Unexpected
Action,OE:ObjectExistence,OI:ObjectInteraction,OS:ObjectShuffle,MD:MovingDirection,AL:Action
Localization,ST:SceneTransition,AC:ActionCount,MC:MovingCount,MA:MovingAttribute,SC:State
Change,FP:Fine-grainedPose,CO:CharacterOrder,EN:EgocentricNavigation,ER:EpisodicReasoningand
CI:CounterfactualInference.
MVBench: Weevaluate VideoGPT+onMVBench[7], whichprovides4,000QApairsfrom11
videodatasetscoveringabroadspectrumofscenes,rangingfromfirst-persontothird-personand
from indoor to outdoor environments. The tasks are categorized into 20 fine-grained temporal
understandingtasks. TheresultspresentedinTable3compare VideoGPT+withpreviousmethods,
indicatinganoverallimprovementof7.6%comparedtothepreviousbest,VideoChat2. Specifically,
VideoGPT+ achieves SoTA results in 14 out of 20 tasks and comes second in 4 out of 20 tasks,
obtaininganaveragescoreof58.7%acrossthe20tasks. Additionally, VideoGPT+showssignificant
improvementsintheActionPrediction(+12.5%),ObjectExistence(OE)(+27.5%),MovingDirection
(MD)(+17%),MovingCount(MC)(+29%)andMovingAttributes(MA)(+32%)indicatingtherich
spatialinformationandtemporalcontextachievedbyourmodel.
Zero-shotQuestion-Answering: Weprovideaquantitativecomparisonofourmethodonthezero-
shotQAtaskacrossfouropen-endedQAdatasets,includingMSVD-QA[27],MSRVTT-QA[27],
TGIF-QA[28],andActivityNet-QA[26]. ResultspresentedinTable4show VideoGPT+achieves
superiorperformancecomparedtopreviousmethods,indicatingitsabilitytoadapteffectivelyto
unseenvideosandgenerateaccuratecontextuallyrelevantresponsesinchallengingsettings.
Model MSVD-QA MSRVTT-QA TGIF-QA ActivityNet-QA
Accuracy Score Accuracy Score Accuracy Score Accuracy Score
FrozenBiLM[49] 32.2 – 16.8 – 41.0 – 24.7 –
VideoChat[5] 56.3 2.8 45.0 2.5 34.4 2.3 26.5 2.2
LLaMAAdapter[50] 54.9 3.1 43.8 2.7 - - 34.2 2.7
Video-LLaMA[15] 51.6 2.5 29.6 1.8 - - 12.4 1.1
Video-ChatGPT[1] 64.9 3.3 49.3 2.8 51.4 3.0 35.2 2.8
ChatUniVi[2] 65.0 3.6 54.6 3.1 60.3 3.4 45.8 3.2
LLaMA-VID[16] 70.0 3.7 58.9 3.3 – – 47.5 3.3
Video-LLaVA[6] 70.7 3.9 59.2 3.5 70.0 4.0 45.3 3.3
VideChat2[7] 70.0 3.9 54.1 3.3 – – 49.1 3.3
VideoGPT+(ours) 72.4 3.9 60.6 3.6 74.6 4.1 50.6 3.6
Table4: Performanceof VideoGPT+onZero-shotQA.Weevaluateaccuracyandscoreonfourcommonly
useddatasets.Allthemodelsareevaluatedinzero-shotsettingwherenoneofthevideoswereincludedinthe
trainingset. VideoGPT+achievesgoodresultsonalldatasets.
8VisionEncoderType ImagePooling VideoPooling VCG+112K
Image Video Dual CNN 4×4 2×2 Time Space × ✓
Correctness(CI) 3.14 3.22 3.27 3.24 3.24 3.27 3.21 3.27 3.20 3.27
Detail(DO) 3.09 3.10 3.18 3.13 3.18 3.18 3.13 3.18 3.08 3.18
Context(CU) 3.68 3.70 3.74 3.70 3.73 3.74 3.70 3.74 3.66 3.74
Temporal(TU) 2.69 2.70 2.83 2.74 2.73 2.83 2.72 2.83 2.66 2.83
Consistency(CO) 3.26 3.31 3.39 3.41 3.39 3.39 3.36 3.39 3.28 3.39
Average 3.17 3.20 3.28 3.25 3.25 3.28 3.23 3.28 3.17 3.28
Table 5: AblationonVisionEncodertype,Imagefeaturepooling,Videofeaturepooling,andVCG+
112K.Weevaluate VideoGPT+trainingusingonlyanimageencoder,videoencoder,andbothimageandvideo
encoders(dualencoder)onVCGBench.Usingadualencoderprovidesrichsemanticandtemporalcues.
VisionEncoderType: Weablateourdualvisualencoderdesignin VideoGPT+inonVCGBench
withresultspresentedinTable5. Weconductthreeexperiments: usingonlytheimageencoder,only
thevideoencoder,andbothencoders. Theimageencoderaloneachievesascoreof3.17,whilethe
videoencoderaloneachievesabetterscoreof3.20,indicatingthebenefitsofvideo-basedpretraining.
Thedualencoderdesign, combiningbothspatialandtemporalinformation, achievesthehighest
scoreof3.28,demonstratingenhancedperformanceinvideo-conversationtasks.
Pooling Strategy: We ablate different pooling strategies for the image and video encoders in
Table5. Theimageencoderoutputsa24×24featuremapfroma336×336input. Wecompare
twodownsamplingmethods: alearnablelightweightCNN(LDPv2from[20])andanon-learnable
adaptiveaveragepoolingwitha2×2kernel. Resultsindicatethatadaptivepoolingperformsbetter
thanCNN.A4×4adaptivepoolingwasalsotestedbutshowedinferiorperformance.
Similarly,weablatethepoolingchoiceforthevideoencoder,whichtakesaninputofsizeT ×224×
224×C andoutputsafeaturemapofT ×16×16×d. Wecomparetwopoolingstrategies: time
poolingacrossthetemporaldimensiontoreducethefeaturemapto1×16×16×d, andspace
poolingacrossthespatialdimensionwitha2×2kernel. Table5showsthatspacepoolingeffectively
preservestemporalinformationandyieldsbetterresults.
VCG+112K:Todemonstratetheeffective-
VCGBench
ness of VCG+112K, we train VideoGPT+ LLM Avg.
with and without it. As shown in Table 5, CI DO CU TU CO
VCG+112Kimprovesperformance,particu- Phi3-Mini-3.8B 3.27 3.18 3.74 2.83 3.39 3.28
larlyindetailorientation(DO)andtemporal Vicuna-7B 3.22 3.14 3.69 2.65 3.46 3.23
Vicuna-13B 3.30 3.20 3.75 2.77 3.48 3.30
understanding(TU).Thisimprovementcan
LLaMA3-8B 3.29 3.21 3.73 2.86 3.38 3.29
beattributedtoournovelsemi-automatican-
notationpipelineandtheenhancedinstruc- Table6: AblationonLLMtype. Wetrainandevaluate
tion tuning data, which focuses on gener- VideoGPT+withdifferentLLMs,includingvicuna[51]and
atingbothdetailedandconciseinstruction LLaMA3[52],whichfurtherimprovesaccuracy.
pairs. RefertoFig.3forqualitativevisualizationofthedata.
LLM Type: We train VideoGPT+ with different LLMs including Vicuna 7B and 13B [51] and
LLaMA-38B[52]andshowsresultsinTable6. WeobserveslightimprovementsinVCGBench
scoreswhentrainingusingbetterLLMs,includingVicuna13BandLLaMA-38Bmodels.
7 Conclusion
In this work, we introduce VideoGPT+, a novel video conversation model that leverages the
complementary benefits of image and video encoders to achieve enhanced video understanding.
VideoGPT+demonstratesbetterperformanceacrossmultiplevideobenchmarks,owingtoitsdual-
encoder design, lightweight visual adapters that map image/video features to a common space
andasegment-wisesamplingstrategythatretainsfine-grainedtemporalinformation. Wealsode-
velop VCG+112K,a112Kvideo-instructionsetusingaresource-efficientsemi-automatedannotation
pipelinethatdeliversfurthergains. Lastly,wepropose VCGBench-Diverse,adiversebenchmark
covering18videocategories,tocomprehensivelyevaluatevideoLMMs. Despitereportedimprove-
ments, video LMMs still find challenges in precise action localization, understanding very long
videos,andnavigatinglongpaths;areaswheremajorimprovementscanunlocknewapplications.
9Figure5: Qualitativecomparisonof VideoGPT+withVideoChat2.Our VideoGPT+demonstratessuperior
temporalunderstandingbycorrectlyidentifyingmultipleeventsinthevideo,effectivereasoningingeneratinga
creativeadvertisement,andaccuratespatialunderstandingbyidentifyingtheSPFvalueandbrandnameofthe
sunscreen.
8 QualitativeResults
Weprovideaqualitativecomparisonofour VideoGPT+withthepreviousstate-of-the-artapproach,
VideoChat2[7],inFig.5. Theexampleshowsanadvertisementvideoforsunscreen,wheremultiple
scenechangesarepresent. Thevideostartswithaclose-upviewofthesunscreen,followedbya
womanapplyingsunscreenonherhand,thenapplyingsunscreennearabeach. Thewomanisthen
seenapplyingsunscreenonherarms,andfinally,thevideoshowsthekeyingredientsofthesunscreen
andendswiththecoverofthesunscreen.
AsshowninFig.5,our VideoGPT+correctlyidentifiestheeventspresentinthevideoandprovides
adetailedandaccuratedescription. Ontheotherhand,VideoChat2strugglestoaccuratelycaptureall
theevents. Further,ourmodelgeneratesanadvertisementposthighlightingoneoftheuniquefeatures
of the sunscreen shown in the video, namely that it functions as both sunscreen and moisturizer.
Lastly,our VideoGPT+correctlyidentifiestheSPFvalueandbrandnameofthesunscreen,while
VideoChat2strugglestocorrectlyidentifythebrandname. WepresentfurthercomparisoninFig.7.
109 AdditionalImplementationDetails
Inthissection,weprovideadditionalimplementationdetailsregardingourtrainingsetupandcompute
requirements. All of our experiments are conducted using 8xA100 40GB GPUs. The training
forVCGBenchexperimentstakesaround12hourstocomplete,whilethetrainingforMVBench
experimentsfinishesinaround10hours. WeusethemodeltrainedfortheVCGBenchtasktoevaluate
on VCGBench-Diverse and zero-shot question-answering benchmarks. All of our training and
evaluationcodes,pretrainedmodelsanddatasetwillbepubliclyreleased.
10 AdditionalAblations
Featureconcatenationstrategy: Wecon-
ductanablationstudytodeterminetheopti-
VCGBench
malorderinwhichimageandvideofeatures Feature Avg.
should be input to the LLM. Specifically, Concatenation CI DO CU TU CO
we perform two experiments. In the first Interleaved 3.25 3.17 3.72 2.78 3.39 3.26
experiment, image and video features are Sequential 3.27 3.18 3.74 2.83 3.39 3.28
extractedforeachvideosegmentandcon-
Table 7: Ablation on Feature Concatenation Strategy.
catenatedinaninterleavedmannerbefore
Performance comparison between interleaved and sequen-
sendingasinputtotheLLM.Forexample,
tialfeatureconcatenationstrategies.Thesequentialfeature
thevideoisdividedintosegmentsofequal
concatenationperformsbetter.
size,andthentheimageandvideofeatures
fromeachsegmentareconcatenatedandinputtotheLLM.Inthesecondexperiment,wefirstplace
alltheimagefeaturesfollowedbyallthevideofeatures. TheresultsshowninTable7,indicatethat
thesequentialdesign,wheretheimagefeaturesareplacedfirstfollowedbythevideofeatures,yields
betterperformance. Thiscanbejustifiedbythefactthatweusedifferentvisualadaptersforimage
andvideofeatures,sointerleavingthefeaturesfrombothmodalitiescancreatealargerdistribution
shift,hinderingthelearningprocess.
11 GPTPrompts
Inthissection,weprovidetheGPTpromptsusedforthefollowingtasks: (i)Densevideodescription
generationfor VCG+112K,(ii)Question-answergenerationfor VCG+112Kand(iii)Question-answer
generationfor VCGBench-Diverse.
Dense Video Description Generation for VCG+ 112K: To generate dense video captions, we
provideGPT-4withaconcisegroundtruthcaptionofthevideoanddetailedframe-levelcaptions
of the key-frames generated from LLaVA-v1.6 [33]. GPT-4 is then prompted to combine this
informationintoadetailedcaptionfortheentirevideo. AsillustratedinFig.8,thepromptincludes
clearinstructionstoeliminateanyconflictinginformation,ensuringanaccurateanddetailedcaption.
Question-answergenerationforVCG+112K:Aftergeneratingdetailedvideodescriptionsusing
GPT-4, we use GPT-3.5 to create question-answer pairs for instruction tuning. Fig. 9 shows the
prompttogeneratedetailedsummaryquestion-answerpairusingthegroundtruthcaptionandthe
densedescriptionofthevideo.
Question-AnswerGenerationfor VCGBench-Diverse: Weprovidepromptsusedtogenerate
comprehensivequestion-answerpairsfor VCGBench-Diverse. AsillustratedinFig.10,theques-
tions are generated in three categories: temporal, spatial, and reasoning. Similar prompts are
usedtogenerateconsistencyandsummaryquestions,offeringanextensiveevaluationprotocolfor
VCGBench-Diverse.
11Figure6: Qualitativecomparisonfrom VCGBench-Diverseof VideoGPT+.Weshowqualitativecompari-
sonof VideoGPT+withVideoChat2andproprietymodelsGPT-4VandGemini-1.5-Pro-Vfromthreedifferent
categoriesincludingtraffic,educationandsurveillancefrom VCGBench-Diverse.
12Figure7: Qualitativecomparisonfrom VCGBench-Diverseof VideoGPT+.Weshowqualitativecompari-
sonof VideoGPT+withVideoChat2andproprietymodelsGPT-4VandGemini-1.5-Pro-Vfromthreedifferent
categoriesincludingsports,newsandautomobilesvideosfrom VCGBench-Diverse.
13Figure8: PromptforDenseVideoCaptionsGenerationforVCG+112K.WeuseGPT-4togeneratedetailed
videocaptionsusingconcisegroundtruthandframe-leveldetailedcaptions.
Figure9: PromptforQuestion-answergenerationforVCG+112K.WeuseGPT-3.5togeneratequestion-
answerpairsforinstructiontuningusingtheconcisevideogroundtruthsanddetailedvideodescriptions.
14Figure10: PromptforQuestion-AnswerGenerationfor VCGBench-Diverse.WeuseGPT-3.5togenerate
temporal,spatial,andreasoningquestion-answerpairs.
15References
[1] MuhammadMaaz,HanoonaRasheed,SalmanKhan,andFahadShahbazKhan. Video-chatgpt:Towards
detailedvideounderstandingvialargevisionandlanguagemodels. InAssociationforComputational
Linguistics,2024.
[2] PengJin,RyuichiTakanobu,CaiwanZhang,XiaochunCao,andLiYuan. Chat-univi: Unifiedvisual
representationempowerslargelanguagemodelswithimageandvideounderstanding. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,2024.
[3] RuyangLiu,ChenLi,HaoranTang,YixiaoGe,YingShan,andGeLi. St-llm:Largelanguagemodelsare
effectivetemporallearners. arXivpreprintarXiv:2404.00308,2024.
[4] YuanhanZhang, BoLi, haotianLiu, YongjaeLee, LiangkeGui, DiFu, JiashiFeng, ZiweiLiu, and
ChunyuanLi. Llava-next:Astrongzero-shotvideounderstandingmodel,April2024.
[5] KunchangLi,YinanHe,YiWang,YizhuoLi,WenhaiWang,PingLuo,YaliWang,LiminWang,and
YuQiao. Videochat:Chat-centricvideounderstanding. arXivpreprintarXiv:2305.06355,2023.
[6] BinLin,BinZhu,YangYe,MunanNing,PengJin,andLiYuan. Video-llava: Learningunitedvisual
representationbyalignmentbeforeprojection. arXivpreprintarXiv:2311.10122,2023.
[7] KunchangLi,YaliWang,YinanHe,YizhuoLi,YiWang,YiLiu,ZunWang,JilanXu,GuoChen,Ping
Luo,etal. Mvbench:Acomprehensivemulti-modalvideounderstandingbenchmark. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,2024.
[8] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. BLIP-2: bootstrappinglanguage-imagepre-
trainingwithfrozenimageencodersandlargelanguagemodels. InInternationalConferenceonMachine
Learning,2023.
[9] DeyaoZhu,JunChen,XiaoqianShen,XiangLi,andMohamedElhoseiny. Minigpt-4:Enhancingvision-
languageunderstandingwithadvancedlargelanguagemodels. InInternationalConferenceonLearning
Representations,2024.
[10] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. InAdvancesin
NeuralInformationProcessingSystems,2023.
[11] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee. Improvedbaselineswithvisualinstruction
tuning. arXiv:2310.03744,2023.
[12] ZhiliangPeng,WenhuiWang,LiDong,YaruHao,ShaohanHuang,ShumingMa,andFuruWei.Kosmos-2:
Groundingmultimodallargelanguagemodelstotheworld. ArXiv,abs/2306,2023.
[13] HanoonaRasheed,MuhammadMaaz,SahalShaji,AbdelrahmanShaker,SalmanKhan,HishamCholakkal,
Rao M. Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad S. Khan. Glamm: Pixel grounding large
multimodalmodel. TheIEEE/CVFConferenceonComputerVisionandPatternRecognition,2024.
[14] HaoxuanYou,HaotianZhang,ZheGan,XianzhiDu,BowenZhang,ZiruiWang,LiangliangCao,Shih-Fu
Chang,andYinfeiYang. Ferret:Referandgroundanythinganywhereatanygranularity. arXivpreprint
arXiv:2310.07704,2023.
[15] HangZhang,XinLi,andLidongBing. Video-llama:Aninstruction-tunedaudio-visuallanguagemodel
forvideounderstanding. arXivpreprintarXiv:2306.02858,2023.
[16] YanweiLi,ChengyaoWang,andJiayaJia. Llama-vid: Animageisworth2tokensinlargelanguage
models. arXivpreprintarXiv:2311.17043,2023.
[17] WonkyunKim,ChanginChoi,WonseokLee,andWonjongRhee. Animagegridcanbeworthavideo:
Zero-shotvideoquestionansweringusingavlm. arXivpreprintarXiv:2403.18406,2024.
[18] De-AnHuang,ShijiaLiao,SubhashreeRadhakrishnan,HongxuYin,PavloMolchanov,ZhidingYu,and
JanKautz. Lita:Languageinstructedtemporal-localizationassistant. arXivpreprintarXiv:2403.19046,
2024.
[19] XiangxiangChu,LimengQiao,XinyangLin,ShuangXu,YangYang,YimingHu,FeiWei,XinyuZhang,
BoZhang,XiaolinWei,etal. Mobilevlm:Afast,reproducibleandstrongvisionlanguageassistantfor
mobiledevices. arXivpreprintarXiv:2312.16886,2023.
[20] XiangxiangChu,LimengQiao,XinyuZhang,ShuangXu,FeiWei,YangYang,XiaofeiSun,YimingHu,
XinyangLin,BoZhang,etal. Mobilevlmv2: Fasterandstrongerbaselineforvisionlanguagemodel.
arXivpreprintarXiv:2402.03766,2024.
[21] RuyangLiu,ChenLi,YixiaoGe,YingShan,ThomasHLi,andGeLi. Oneforall:Videoconversation
isfeasiblewithoutvideoinstructiontuning. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,2024.
[22] Shehan Munasinghe, Rusiru Thushara, Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan,
MubarakShah,andFahadKhan. Pg-video-llava:Pixelgroundinglargevideo-languagemodels. ArXiv
2311.13435,2023.
16[23] EnxinSong,WenhaoChai,GuanhongWang,YuchengZhang,HaoyangZhou,FeiyangWu,XunGuo,
TianYe,YanLu,Jenq-NengHwang,etal. Moviechat:Fromdensetokentosparsememoryforlongvideo
understanding. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
2024.
[24] BinHuang,XinWang,HongChen,ZihanSong,andWenwuZhu. Vtimellm:Empowerllmtograspvideo
moments. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
2024.
[25] MaxBain,ArshaNagrani,GülVarol,andAndrewZisserman. Frozenintime:Ajointvideoandimage
encoderforend-to-endretrieval. InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision,2021.
[26] BernardGhanemFabianCabaHeilbron,VictorEscorciaandJuanCarlosNiebles. Activitynet:Alarge-
scalevideobenchmarkforhumanactivityunderstanding. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,2015.
[27] DejingXu,ZhouZhao,JunXiao,FeiWu,HanwangZhang,XiangnanHe,andYuetingZhuang. Video
questionansweringviagraduallyrefinedattentionoverappearanceandmotion. InACMInternational
ConferenceonMultimedia,2017.
[28] YunseokJang,YaleSong,ChrisDongjooKim,YoungjaeYu,YoungjinKim,andGunheeKim. Video
QuestionAnsweringwithSpatio-TemporalReasoning. InternationalJournalofComputerVision,2019.
[29] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InInternationalConferenceonMachineLearning,2021.
[30] YiWang,KunchangLi,XinhaoLi,JiashuoYu,YinanHe,GuoChen,BaoqiPei,RongkunZheng,Jilan
Xu,ZunWang,etal. Internvideo2:Scalingvideofoundationmodelsformultimodalvideounderstanding.
arXivpreprintarXiv:2403.15377,2024.
[31] EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,and
WeizhuChen. Lora:Low-rankadaptationoflargelanguagemodels. arXivpreprintarXiv:2106.09685,
2021.
[32] Brandon Castellano. Pyscenedetect: Automated video scene detection. https://github.com/
Breakthrough/PySceneDetect,2022.
[33] HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,ShengShen,andYongJaeLee. Llava-next:
Improvedreasoning,ocr,andworldknowledge,January2024.
[34] HongweiXue,TiankaiHang,YanhongZeng,YuchongSun,BeiLiu,HuanYang,JianlongFu,andBaining
Guo. Advancinghigh-resolutionvideo-languagerepresentationwithlarge-scalevideotranscriptions. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,2022.
[35] MykhayloAndriluka,LeonidPishchulin,PeterGehler,andBerntSchiele. 2dhumanposeestimation:New
benchmarkandstateoftheartanalysis. InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,June2014.
[36] LuoweiZhou, ChenliangXu, andJasonCorso. Towardsautomaticlearningofproceduresfromweb
instructionalvideos. InAAAI,2018.
[37] WaqasSultani,ChenChen,andMubarakShah. Real-worldanomalydetectioninsurveillancevideos. In
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages6479–6488,2018.
[38] LiXu,HeHuang,andJunLiu. Sutd-trafficqa:Aquestionansweringbenchmarkandanefficientnetwork
forvideoreasoningovertrafficevents. InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pages9878–9888,2021.
[39] MarahAbdin,SamAdeJacobs,AmmarAhmadAwan,JyotiAneja,AhmedAwadallah,HanyAwadalla,
NguyenBach,AmitBahree,ArashBakhtiari,HarkiratBehl,etal. Phi-3technicalreport:Ahighlycapable
languagemodellocallyonyourphone. arXivpreprintarXiv:2404.14219,2024.
[40] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee. Improvedbaselineswithvisualinstruction
tuning. arXivpreprintarXiv:2310.03744,2023.
[41] EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,and
WeizhuChen. LoRA:Low-rankadaptationoflargelanguagemodels. InInternationalConferenceon
MachineLearning,2022.
[42] WillKay,JoaoCarreira,KarenSimonyan,BrianZhang,ChloeHillier,SudheendraVijayanarasimhan,
FabioViola,TimGreen,TrevorBack,PaulNatsev,etal. Thekineticshumanactionvideodataset. arXiv
preprintarXiv:1705.06950,2017.
17[43] RaghavGoyal, SamiraEbrahimiKahou, VincentMichalski, JoannaMaterzynska, SusanneWestphal,
HeunaKim,ValentinHaenel,IngoFruend,PeterYianilos,MoritzMueller-Freitag,etal. The"something
something" video database for learning and evaluating visual common sense. In Proceedings of the
IEEE/CVFInternationalConferenceonComputerVision,2017.
[44] KexinYi,ChuangGan,YunzhuLi,PushmeetKohli,JiajunWu,AntonioTorralba,andJoshuaBTenenbaum.
Clevrer:Collisioneventsforvideorepresentationandreasoning. arXivpreprintarXiv:1910.01442,2019.
[45] JunbinXiao,XindiShang,AngelaYao,andTat-SengChua. Next-qa:Nextphaseofquestion-answeringto
explainingtemporalactions. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pages9777–9786,2021.
[46] OpenAI. Gpt-4v(ision)systemcard. https://api.semanticscholar.org/CorpusID:263218031,
2023.
[47] BoLi,YuanhanZhang,LiangyuChen,JinghaoWang,JingkangYang,andZiweiLiu.Otter:Amulti-modal
modelwithin-contextinstructiontuning. arXivpreprintarXiv:2305.03726,2023.
[48] QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan,YiyangZhou,JunyangWang,AnwenHu,
Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with
multimodality. arXivpreprintarXiv:2304.14178,2023.
[49] AntoineYang,AntoineMiech,JosefSivic,IvanLaptev,andCordeliaSchmid. Zero-shotvideoquestion
answering via frozen bidirectional language models. In Advances in Neural Information Processing
Systems,2022.
[50] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu,
HongshengLi, andYuQiao. Llama-adapter: Efficientfine-tuningoflanguagemodelswithzero-init
attention. InInternationalConferenceonLearningRepresentations,2024.
[51] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,Siyuan
Zhuang,YonghaoZhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing. Vicuna: Anopen-source
chatbotimpressinggpt-4with90%*chatgptquality.https://lmsys.org/blog/2023-03-30-vicuna,
2023.
[52] MetaAI. Llama3. https://llama.meta.com/llama3,2024.
18