Learning conditional distributions on continuous spaces
Cyril B´en´ezet ∗† Ziteng Cheng ∗‡ Sebastian Jaimungal ∗§
Abstract
We investigate sample-based learning of conditional distributions on multi-dimensional unit boxes,
allowing for different dimensions of the feature and target spaces. Our approach involves clustering
data near varying query points in the feature space to create empirical measures in the target space.
We employ two distinct clustering schemes: one based on a fixed-radius ball and the other on nearest
neighbors. Weestablishupperboundsfortheconvergenceratesofbothmethodsand,fromthesebounds,
deduce optimal configurations for the radius and the number of neighbors. We propose to incorporate
the nearest neighbors method into neural network training, as our empirical analysis indicates it has
betterperformanceinpractice. Forefficiency,ourtrainingprocessutilizesapproximatenearestneighbors
search with random binary space partitioning. Additionally, we employ the Sinkhorn algorithm and
a sparsity-enforced transport plan. Our empirical findings demonstrate that, with a suitably designed
structure, the neural network has the ability to adapt to a suitable level of Lipschitz continuity locally.
For reproducibility, our code is available at https://github.com/zcheng-a/LCD_kNN.
1 Introduction
Learning the conditional distribution is a crucial aspect of many decision-making scenarios. While this
learning task is generally challenging, it presents unique complexities when explored in a continuous space
setting. Below, we present a classic example (cf. [BHW92, PP16]) that highlights this core challenge.
For simplicity, we suppose the following model
Y = 1X+ 1U,
2 2
where the feature variable X and the noise U are independent Uniform([0,1]), and Y is the target variable.
Upon collecting a finite number of independent samples D = {(X ,Y )}M , we aim to estimate the
m m m=1
conditional distribution of Y given X. Throughout, we treat this conditional distribution as a measure-
valued function of x, denoted by P . A naive approach is to first form an empirical joint measure
x
M
ψˆ:= 1 (cid:88) δ ,
M (Xm,Ym)
m=1
where δ stands for the Dirac meaasure, and then use the conditional distribution induced from ψˆ as an
estimator. As the marginal distribution of X is continuous, with probability 1 (as P(X =X )=0 for all
m m′
m̸=m′), we have that1
(cid:40)
δ , x=X for some m,
P(cid:98)x =
Ym m
Uniform([0,1]), otherwise.
∗Orderedalphabetically.
†LaboratoiredeMath´ematiquesetMod´elisationd’E´vry(LaMME),Universit´ed’E´vry-Val-d’Essonne,ENSIIE,UMRCNRS
8071,IBGBI23BoulevarddeFrance,91037E´vryCedex,France. Email: cyril.benezet@ensiie.fr.
‡DepartmentofStatisticalSciences,UniversityofToronto,Canada. Email: ziteng.cheng@utoronto.ca.
§Department of Statistical Sciences, University of Toronto, Canada. Email: sebastian.jaimungal@utoronto.ca. http://
sebastian.statistics.utoronto.ca/
1Inaccordancetothemodel,wesettheconditionaldistributiontoUniform([0,1])atpointswhereitisnotwell-defined.
1
4202
nuJ
31
]LM.tats[
1v57390.6042:viXraRegardless of the sample size M, P(cid:98)x fails to approximate the true conditional distribution,
P
=Uniform(cid:0)
[x,x+
1](cid:1)
, x∈[0,1].
x 2
Despite the well-known convergence of the (joint) empirical measure to the true distribution [Dud69,
FG15], theresultingconditionaldistributionoftenfailstoprovideanaccurateapproximationofthetruedis-
tribution. Thisdiscrepancycouldbeduetothefactthatcalculatingconditionaldistributionisaninherently
unbounded operation. As a remedy, clustering is a widely employed technique. Specifically, given a query
point x in the feature space, we identify samples where X is close to x and use the corresponding Y ’s to
m m
estimate P . Two prominent methods within the clustering approach are the kernel method and the nearest
x
neighborsmethod2. Roughlyspeaking,thekernelmethodreliesprimarilyonproximitytothequerypointfor
selecting X ’s, while the nearest neighbors method focuses on the rank of proximity. Notably, discretizing
m
the feature space (also known as quantization), a straightforward yet often effective strategy, can be seen as
a variant of the kernel method with static query points and flat kernels.
Theproblemofestimatingconditionaldistributionscanbeaddressedwithinthenon-parametricregression
framework,byemployingclusteringorresortingtonon-parametricleastsquares,amongothers. Alternatively,
it is feasible to estimate the conditional density function directly: a widely-used method involves estimating
the joint and marginal density functions using kernel smoothing and then calculating their ratio. This
method shares similarities with the clustering heuristics mentioned earlier. For a more detailed review of
these approaches, we refer to Section 1.2.
This work draws inspiration from recent advancements in estimating discrete-time stochastic processes
usingconditionaldensityfunctionestimation[PP16]andquantizationmethods[BBBW22,AH23]. Anotable
featureoftheseworksistheiruseoftheWassersteindistancetocalculatelocalerrors: thedifferencebetween
the true and estimated conditional distributions at a query point x. One could average these local errors
acrossdifferentvaluesofx’stogaugetheglobalerror. EmployingWassersteindistancesnaturallyframesthe
study within the context of weak convergence, thereby enabling discussions in a relatively general setting,
although this approach may yield somewhat weaker results in terms of the mode of convergence. Moreover,
utilizing a specific distance rather than the general notion of weak convergence enables a more tangible
analysis of the convergence rates and fluctuations. We would like to point out that the advancements
made in [BBBW22, AH23], as well as our analysis in this paper, relies on recent developments concerning
the Wasserstein convergence rate of empirical measures under i.i.d. sampling from a static distribution (cf.
[FG15]).
1.1 Main contributions
First, we introduce some notations to better illustrate the estimators that we study. Let X and Y be multi-
dimensional unit cubes, with potentially different dimensions, for feature and target spaces. For any integer
M ≥1, any D={(x ,y )}M ∈(X×Y)M, and any Borel set A⊂X, we define a probability measure on
m m m=1
Y by
(cid:16) (cid:17)−1
 (cid:80)M 1 (x ) (cid:80)M 1 (x )δ , (cid:80)M 1 (x )>0,
µˆD := m=1 A m m=1 A m ym m=1 A m (1)
A
λY, otherwise,
whereλY istheLebesguemeasureonYand,fory ∈Y,δ
y
isaDiracmeasurewithatomaty. Ingeneral,one
could consider weighting δ ’s (cf. [GKKW02, Section 5], [BD15, Chapter 5]), which may offer additional
ym
benefits in specific applications. As such adjustments are unlikely to affect the convergence rate, however,
we use uniform weighting for simplicity.
With (random) data D ={(X ,Y )}M , we aim to estimate the conditional distribution of Y given X.
m m m=1
Weviewthisconditionaldistributionasameasure-valuedfunctionP :X→P(Y)anduseasubscriptforthe
2Theseshouldnotbeconfusedwithsimilarlynamedmethodsusedindensityfunctionestimation.
2input argument and write P . Consider a clustering scheme3 given by the map AD :X→2X. We investigate
x
estimators of the form x(cid:55)→µˆD . We use P(cid:98)A to denote said estimator and suppress D from the notation
AD(x)
for convenience. In later sections, we consider two kinds of maps AD (i) a ball with fixed radius centered at
x, called an r-box and (ii) the k nearest neighbors of x, called k-nearest-neighbor estimator. Wee Definitions
5 and 9 for more details.
One of our main contribution pertains to analyzing the error
(cid:90) (cid:16) (cid:17)
W P x,P(cid:98) xA ν(dx), (2)
X
whereW isthe1-Wassersteindistance(cf. [Vil08,ParticularCase6.2])andν ∈P(X)isarbitraryandprovides
versatility to the evaluation criterion. A canonical choice for ν is the Lebesgue measure on X, denoted by
λX. This is particularly relevant in control settings where X represents the state-action space and accurate
approximationsacrossvariousstateandactionscenariosarecrucialformakinginformeddecisions. Theform
oferroraboveisalsofoundationalinstochasticprocessestimationundertheadaptedWassersteindistance(cf.
[BBBW22,Lemma3.1]),makingthetechniqueswedeveloppotentiallyrelevantinothercontexts. Underthe
assumption that P is Lipschitz continuous (Assumption 2) and standard assumptions on the data collection
process (Assumption 3), we analyze the convergence rate and fluctuation by bounding the following two
quantities
(cid:20)(cid:90) (cid:16) (cid:17) (cid:21) (cid:20)(cid:90) (cid:16) (cid:17) (cid:21)
E W P x,P(cid:98) xA ν(dx) and Var W P x,P(cid:98) xA ν(dx) .
X X
Moreover, by analyzing the above quantities, we gain insights into the optimal choice of the clustering
mapping A. For the detail statements of these results, we refer to Theorems 7, 10, 8, and 11. We also refer
to Section 2.4 for related comments.
To illustrate another aspect of our contribution, we note by design x(cid:55)→P(cid:98)A is piece-wise constant. This
x
characteristic introduces limitations. Notably, it renders the analysis of performance at the worst-case x
elusive. Contrastingly, by building a Lipschitz-continuous parametric estimator P˜Θ from the raw estimator
P(cid:98)A, in Proposition 13 we demonstrate that an upper bound on the aforementioned expectation allows us to
deriveaworst-caseperformanceguarantee. GuidedbyProposition13,weexploreanovelapproachoftraining
a neural network for estimation, by using P(cid:98)A as training data and incorporating suitably imposed Lipschitz
continuity. To be comprehensive, we include in Section 1.2 a review of studies on Lipschitz continuity in
neural networks.
In Section 3.1, we define P˜θ as a neural network that approximates P, where θ represents the network
parameters. We train P˜θ with the objective:
N
argmin(cid:88) W(cid:16) P(cid:98)A ,P˜θ (cid:17) ,
θ
X˜
n
X˜
n
n=1
where(X˜ )N isasetofrandomlyselectedquerypoints. Forimplementationpurposes,weusethek-nearest-
n n=1
neighborestimatorintheplaceofP(cid:98)A (seeDefinition9). Tomitigatethecomputationalcostsstemmingfrom
thenearestneighborssearch,weemploythetechniqueofApproximateNearestNeighborSearchwithRandom
BinarySpacePartitioning(ANN-RBSP),asdiscussedinSection3.1.1. InSection3.1.2,wecomputeW using
theSinkhornalgorithm,incorporatingnormalizationandenforcingsparsityforimprovedaccuracy. Toimpose
asuitableleveloflocalLipschitzcontinuityonP˜θ,inSection3.1.3,weemployaneuralnetworkwithaspecific
architecture and train the networks using a tailored procedure. The key component of this architecture is
the convex potential layer introduced in [MDAA22]. In contrast to most extant literature that imposes
Lipschitz continuity on neural networks, our approach does not utilize specific constraint or regularization of
the objective function, but relies on certain self-adjusting mechanism embedded in the training.
3Ingeneral,theclusteringschememayrequireinformationon(x1,...,xM). Forexample,clusteringthek-nearest-neighbor
nearaquerypointxrequirestoknowallxm’s.
3InSection3.2,weevaluatetheperformanceofthetrainedP˜θ,denotedbyP˜Θ,usingthreesetsofsynthetic
datain1Dand3Dspaces. OurfindingsindicatethatP˜Θ generallyoutperformsP(cid:98)A,eventhoughitisinitially
trained to match P(cid:98)A. This superior performance persists even when comparing P˜Θ to different P(cid:98)A using
various k values, without retraining P˜Θ. Furthermore, despite using the same training parameters, P˜Θ
consistently demonstrates the ability to adapt to a satisfactory level of local Lipschitz continuity across all
cases. Moreover, inoneofthetestcases, weconsiderakernelthatexhibitsajumpdiscontinuity, andwefind
that P˜Θ handles this jump case well despite Lipschitz continuity does not hold.
Lastly,weprovidefurthermotivationofourapproachbyhighlightingsomepotentialapplicationsforP˜Θ.
The first application is in model-based policy gradient method in reinforcement learning. We anticipate that
the enforced Lipschitz continuity allows us to directly apply the policy gradient update via compositions of
P˜Θ and cost function for more effective optimality searching. The second application of P˜Θ is in addressing
optimisation in risk-averse Markov decision processes, where dynamic programming requires knowledge be-
yond the conditional expectation of the risk-to-go (cf. [CTMP15, HH17, CJC23, CJ23]). The study of these
applications is left for further research.
1.2 Related works
In this section, we will first review the clustering approach in estimating conditional distributions, and then
proceed to review recent studies on Lipschitz continuity in neural networks.
1.2.1 Estimating conditional distributions via clustering
The problem of estimating conditional distributions is frequently framed as non-parametric regression prob-
lems for real-valued functions. For instance, when dY =1, estimate the conditional α-quantile of Y given X.
Therefore, we begin by reviewing some of the works in non-parametric regression.
Thekernelmethodinnon-parametricregressiontracesitsoriginsbacktotheNadaraya-Watsonestimator
[Nad64, Wat64], if not earlier. Subsequent improvements have been introduced, such as integral smoothing
[GM79] (also known as the Gasser-Mu¨ller estimator), local fitting with polynomials instead of constants
[Fan92], and adaptive kernels [HWY99]. Another significant area of discussion is the choice of kernel band-
width, as detailed in works like [HM85, GG04, KSS14]. Regarding convergence rates, analyses under various
settings can be found in [Sto82, HH90, KKW09, KKW10], with [Sto82] being particularly relevant to our
study for comparative purposes. According to [Sto82], if the target function is Lipschitz continuous, with
i.i.d. sampling and that the sampling distribution in the feature space has a uniformly positive density, then
the optimal rate of the ∥ · ∥ -distance between the regression function and the estimator is of the order
1
M− dX1 +2. For a more comprehensive review of non-parametric regression using kernel methods, we refer to
the books [GKKW02, FV06, Was06] and references therein.
Non-parametricregressionusingnearestneighborsmethodsoriginatedfromclassificationproblems[FH51].
Early developments in this field can be found in [Mac81, Dev82, BG90]. For a comprehensive introduction
to nearest neighbors methods, we refer to [GKKW02]. More recent reference [BD15] offers further detailed
exploration of the topic. The nearest neighbor method can be viewed as a variant of the kernel method
thatadjuststhebandwidthbasedonthenumberoflocaldatapoints—apropertythathasgainedsignificant
traction. Recently, the application of the nearest neighbor method has expanded into various less standard
settings, including handling missing data [RLK+21], reinforcement learning [SX10, GOR24], and time series
forecasting [MFPR17]. For recent advancements in convergence analysis beyond the classical setting, see
[ZL19, PSCW20, RK22, DFG+24].
Although the review above mostly focuses on clustering approach, other effective approaches exist, such
as non-parametric least square, or more broadly, conditional elicitability (e.g., [GKKW02, MFSS17, Wai19,
CJC23]). Non-parametric least square directly fits the data using a restricted class of functions. At first
glance, this approach appears distinct from clustering. However, they share some similarities in their heuris-
tics: the rigidity of the fitting function, due to imposed restrictions, allows data points near the query point
4to affect the estimation, thereby implicitly incorporating elements of clustering.
Apart from non-parametric regression, conditional density function estimation is another significant
method for estimating conditional distributions. One approach is based on estimating joint and marginal
density functions, and then using the ratio of these two to produce an estimator for the conditional density
function. Akeytechniqueusedinthisapproachiskernelsmoothing. Employingastatickernelforsmoothing
resultsinaconditionaldensityestimatorthatsharessimilarclusteringheuristicstothosefoundinthekernel
method of non-parametric regression. For a comprehensive overview of conditional density estimation, we
refertoreferencebooks[Sco15,Sim96]. Forcompleteness,wealsoreferto[MFSS17,Section5.1]foraperspec-
tive on static density function estimation from the standpoint of reproducing kernel Hilbert space. Further
discussions on estimation using adaptive kernels can be found in, for example, [BH01, Lac07, BLR16, ZT23].
Despite extensive research in non-parametric regression and conditional density function estimation, in-
vestigationsfromtheperspectiveofweakconvergencehavebeenrelativelylimited,onlygainingmoretraction
in the past decade. Below, we highlight a few recent studies conducted in the context of estimating discrete-
time stochastic processes under adapted Wasserstein distance, as the essence of these studies are relevant to
our evaluation criterion (2). [PP16] explores the problem asymptotically, employing tools from conditional
density function estimation with kernel smoothing. Subsequently, [BBBW22] investigates a similar problem
with a hypercube as state space, employing the quantization method. Their approach removes the need to
work with density functions. They calculate the convergence rate, by leveraging recent developments in the
Wasserstein convergence rate of empirical measures [FG15]. Moreover, a sub-Gaussian concentration with
parameter M−1 is established. The aforementioned results are later extended to Rd in [AH23], where a
non-uniform grid is used to mitigate assumptions on moment conditions. Most recently, [Hou24] examines
smoothed variations of the estimators proposed in [BBBW22, AH23]. Other developments on estimators
constructed from smoothed quantization can be found in [vK24].
Lastly,regardingthemachinelearningtechniquesusedinestimatingconditionaldistributions,conditional
generative models are particularly relevant. For reference, see [MO14, PPM17, VSP+17, FJGZ20]. These
models have achieved numerous successes in image generation and natural language processing. We suspect
that,duetotherelativelydiscrete(albeitmassive)featurespacesintheseapplications,clusteringisimplicitly
integrated into the training procedure. In continuous spaces, under suitable setting, clustering may also
become an embedded part of the training procedure. For example, implementations in [LAO20, VPC24,
HHT24] do not explicitly involve clustering and use training objectives that do not specifically address the
issues highlighted in the motivating example at the beginning of the introduction. Their effectiveness could
possiblybeattributedtocertainregularizationembeddedwithintheneuralnetworkandtrainingprocedures.
Nevertheless, research done in continuous spaces that explicitly uses clustering approaches when training
conditional generative models holds merit. Such works are relatively scarce. For an example of this limited
body of research, we refer to [XA22], where the conditional density function estimator from [PP16] is used
to train an adversarial generative network for stochastic process generation.
1.2.2 Lipschitz continuity in neural networks
Recently, there has been increasing interest in understanding and enforcing Lipschitz continuity in neural
networks. The primary motivation is to provide a certifiable guarantee for classification tasks performed by
neural networks: it is crucial that minor perturbations in the input object have a limited impact on the
classification outcome.
OnestrategyinvolvesboundingtheLipschitzconstantofaneuralnetwork,whichcanthenbeincorporated
into the training process. For refined upper bounds on the (global) Lipschitz constant, see, for example,
[BFT17,VS18,TSS18,FRH+19,XLR+22,FERC24]. Forlocalbounds,wereferto[JD21,BDR21,SWZ+22]
andthereferencestherein. Wealsoreferto[ZJHW22]forastudyoftheLipschitzpropertyfromtheviewpoint
of boolean functions.
Alternatively, designing neural network architectures that inherently ensure desirable Lipschitz constants
is another viable strategy. Works in this direction include [MDAA22, SSF22, WM23, AHD+23]. Notably,
5the layer introduced in [MDAA22] belongs to the category of residual connection [HZSS16].
Below, we review several approaches that enforce Lipschitz constants during neural network training.
[TSS18, LWJ+22] explore training with a regularized objective function that includes upper bounds on
the network’s Lipschitz constant. [GFPC21] frame the training problem into constrained optimization and
train with projected gradients descent. Given the specific structure of the refined bound established in
[FRH+19], [PKB+22] combines training with semi-definite programming. They develop a version with a
regularized objective function and another that enforces the Lipschitz constant exactly. [FERC24] also
investigates training with a regularized objective but considers Lipschitz constants along certain directions.
[HZS+21]devisesatrainingprocedurethatremovescomponentsfromtheweightmatricestoachievesmaller
local Lipschitz constants. [TK21] initially imposes orthogonality on the weight matrices, and subsequently
enforces a desirable Lipschitz constant based on that orthogonality. Ensuring desirable Lipschitz constants
with tailored architectures, [SSF22, WM23] train the networks directly. Although the architecture proposed
in [MDAA22] theoretically ensures the Lipschitz constant, it requires knowledge of the spectral norm of the
weight matrices, which does not admit explicit expression in general. Their training approach combines
power iteration for spectral norm approximation with the regularization methods used in [TSS18].
Finally, we note that due to their specific application scenarios, these implementations concern relatively
stringent robustness requirements and thus necessitate more specific regularization or constraints. In our
setting, it is generally desirable for the neural network to automatically adapt to a suitable level of Lipschitz
continuity based on the data, while also avoiding excessive oscillations from over-fitting. The literature
directlyaddressingthisperspectiveislimited(especiallyinthesettingofconditionaldistributionestimation).
We refer to [BLZ+21, BZLX23, CRK19] for discussions that could be relevant.
1.3 Organization of the paper
Our main theoretical results are presented in Section 2. Section 3 is dedicated to the training of P˜Θ. We
will outline the key components of our training algorithm and demonstrate its performance on three sets of
syntheticdata. WewillprovethetheoreticalresultsinSection4. Furtherimplementationdetailsandablation
analysisareprovidedinSection5. InSection6,wediscusstheweaknessesandpotentialimprovementsofour
implementation. Appendix A and B respectively contain additional plots and a table that summarizes the
configuration of our implementation. Additionally, Appendix C includes a rougher version of the fluctuation
results.
Notations and terminologies
Throughout, we adopt the following set of notations and terminologies.
◦ On any normed space (E,∥·∥), for all x ∈ E and γ > 0, B(x,γ) denotes the closed ball of radius γ
around x, namely B(x,γ)={x′ ∈E|∥x−x′∥≤γ}.
◦ For any measurable space (E,E), P(E) denotes the set of probability distributions on (E,E). For all
x∈E, δ ∈P(E) denotes the Dirac mass at x.
x
◦ We endow normed spaces (E,∥ · ∥) with their Borel sigma-algebra B(E), and W denotes the 1-
Wasserstein distance on P(E).
◦ On X = [0,1]d, we denote by λX the Lebesgue measure. We say a measure ν ∈ P(X) is dominated by
Lebesgue measure with a constant C >0 if ν(A)≤CλX(A) for all A∈B([0,1]d).
◦ The symbol ∼ denotes equivalence in the sense of big O notation, indicating that each side dominates
the other up to a multiplication of some positive absolute constant. More precisely, a ∼ b means
n n
6there are finite constants c,C >0 such that
ca ≤b ≤Ca , n∈N.
n n n
Similarly, ≲ implies that one side is of a lesser or equal, in the sense of big O notation, compared to
the other.
2 Theoretical results
InSection2.1,wefirstformallysetuptheproblemandintroducesometechnicalassumption. Wethenstudy
in Section 2.2 and 2.3 the convergence and fluctuation of two versions of P(cid:98)A, namely, the r-box estimator
and the k-nearest-neighbor estimator. Related comments are organized in Section 2.4. Moreover, in Section
2.5, we provide a theoretical motivation for the use of P˜Θ, the Lipschitz-continuous parametric estimator
trained from P(cid:98)A.
2.1 Setup
For dX,dY ≥ 1 two integers, we consider X := [0,1]dX and Y := [0,1]dY, endowed with their respective
sup-norm ∥·∥ .
∞
Remark 1. The sup-norm is chosen for simplicity of the theoretical analysis only: as all norms on Rn are
equivalent(foranygenericn≥1),ourresultsarevalid,uptodifferentmultiplicativeconstants,foranyother
choice of norm.
We aim to estimate an unknown probabilistic kernel
P :X→P(Y)
x(cid:55)→P (dy).
x
To this end, given an integer-valued sampled size M ≥ 1, we consider a set of (random) data points
D := {(X ,Y )}M associated to P. We also define the set of projections of the data points onto the
m m m=1
feature space as DX :={X m}M m=1.
Throughout this section, we work under the following technical assumptions.
Assumption 2 (Lipschitz continuity of kernel). There exists L≥0 such that, for all (x,x′)∈X2,
W(P ,P )≤L∥x−x′∥ .
x x′ ∞
Assumption 3. The following is true:
(i) D is i.i.d.with probability distribution ψ := ξ⊗P, where ξ ∈ P(X) and where ξ⊗P ∈ P(X×Y) is
(uniquely, by Caratheodory extension theorem) defined by
(cid:90)
(ξ⊗P)(A×B):= 1 (x)P (B)ξ(dx), A∈B(X),B ∈B(Y).
A x
X
(ii) There exists c∈(0,1] such that, for all A∈B(X), ξ(A)≥cλX(A).
These assumptions allow us to analyze convergence and gain insights into the optimal clustering hyper-
parameters without delving into excessive technical details. Assumption 2 is mainly used for determining
the convergence rate. If the convergence rate is not of concern, it is possible to establish asymptotic results
with less assumptions. We refer to [Dev82, BBBW22] for relevant results. The conditions placed on ξ in
Assumption 3 are fairly standard, though less stringent alternatives are available. For instance, Assumption
3 (i) can be weakened by considering suitable dependence [HH90] or ergodicity in the context of stochastic
7processes[RS18]. Assumption3(ii), impliesthereismassalmosteverywhereandisalignedwiththemotiva-
tion from control settings discussed in the introduction. Assumptions 2 and 3 are not exceedingly stringent
and provides a number of insights into the estimation problem. More general settings are left for further
research.
TheestimatorsdiscussedinsubsequentsectionsareoftheformP(cid:98)A, asintroducedrightafter(1), fortwo
specific choices of clustering schemes A constructed with the data D.
Remark 4. In the following study, we assert all the measurability needed for P(cid:98)A to be well-defined. These
measurability can be verified using standard measure-theoretic tools listed in, for example, [AB06, Section 4
and 15].
2.2 Results on r-box estimator
The first estimator, which we term the r-box estimator, is defined as follows.
Definition 5. Choose r, a real number, s.t. 0<r < 1. The r-box estimator for P is defined by
2
Pˆr :X→P(Y)
x(cid:55)→Pˆr :=µˆD ,
x Br(x)
where, for all x∈X, Br(x):=B(βr(x),r) and βr(x):=r∨x∧(1−r), where r∨· and ·∧(1−r) are applied
entry-wise.
Remark 6. The set Br(x) is defined such that it is a ball of radius around x whenever x is at least r away
from the boundary ∂X (in all of its components), otherwise, we move the point x in whichever components
are within r from ∂X to be a distance r away from ∂X. Consequently, for all 0 < r < 1 and for all x ∈ X,
2
Br(x) has a bona fide radius of r, as the center βr(x) is smaller or equal to r away from ∂X.
For the r-box estimator, we have the following convergence results. The theorem below discusses the
convergence rate of the average Wasserstein distance between the unknown kernel evaluated at any point
and its estimator, when the radius r is chosen optimally with respect to the data sample M. Section 4.2 is
dedicated to its proof.
Theorem 7. Under Assumptions 2 and 3, choose r as follows
(cid:40) M− dX1 +2, dY =1,2
r ∼
M− dX+1 dY, dY ≥3.
Then, there is a constant C >0 (which depends only on dX,dY,L,c), such that, for all probability distribution
ν ∈P(X), we have

(cid:20)(cid:90) (cid:16) (cid:17) (cid:21) (cid:104) (cid:16) (cid:17)(cid:105)
M− dX1 +2, dY =1,
E W P x,Pˆ xr ν(dx) ≤supE W P x,Pˆr ≤C× M− dX1 +2 ln(M), dY =2, (3)
X x∈X M− dX+1
dY, dY ≥3.
Next, we bound the associated variance whose proof is postponed to Section 4.3.
Theorem 8. Under Assumptions 3, consider r ∈ (0,1]. Let ν ∈ P(X) be dominated by λX with a constant
2
C >0. Then,
(cid:20)(cid:90) (cid:16) (cid:17) (cid:21) 4dX+1C2
Var W P ,Pˆr dν(x) ≤ .
x x c2(M +1)
X
82.3 Results on k-nearest-neighbor estimator
Here, we focus in the second estimator – the k-nearest-neighbor estimator, defined as follows.
Definition 9. Let k ≥1 an integer. The k-nearest-neighbor estimator for P is defined by
Pˇk :X→P(Y)
x(cid:55)→Pˇk :=µˆD ,
x Nk,DX(x)
where,foranyintegerM ≥1andanyDX ∈XM,Nk,DX(x)contains(exactly)k pointsofDX whichareclosest
to x, namely
Nk,DX(x):=(cid:110) x′ ∈DX(cid:12) (cid:12)∥x−x′∥
∞
is among the k-smallest of (∥x−x′∥ ∞) x′∈DX(cid:111) ,
Here, in case of a tie when choosing the k-th smallest, we break the tie randomly with uniform probability.
We have the following analogs of the convergence results (Theorems 7 and 8) for the k-nearest-neighbor
estimator. The proofs are postponed to Section 4.4 and Section 4.5, respectively.
Theorem 10. Under Assumptions 2 and 3, and choosing k as
(cid:40) 2
MdX+2, dY =1,2,
k ∼
dY
MdX+dY, dY ≥3,
there is a constant C > 0 (which depends only on dX,dY,L,c), such that, for all probability distribution
ν ∈P(X), we have

(cid:20)(cid:90) (cid:21)
M− dX1 +2, dY =1,
E W(cid:0) P x,Pˇ xk(cid:1) ν(dx) ≤supE(cid:2) W(cid:0) P x,Pˇ xk(cid:1)(cid:3) ≤C× M− dX1 +2 lnM, dY =2, (4)
X x∈X M− dX+1
dY, dY ≥3.
Theorem 11. Under Assumptions 3, for any ν ∈P(X), we have
Var(cid:20)(cid:90)
W(cid:0)
P
,Pˇk(cid:1)
ν(dx)(cid:21)
≤
1
. (5)
x x k
X
Moreover, if ν is dominated by λX with a constant C >0, then
(cid:20)(cid:90) (cid:21)
Var
W(cid:0)
P
,Pˇk(cid:1)
ν(dx)
x x
X
22dX+1C2 M(cid:32)(cid:32) (cid:114)
2dXln(M) k
(cid:33)2 √
2π
(cid:32) (cid:114)
2dXln(M) k
(cid:33)
4
(cid:33)
≤ 8 + + √ 8 + + .
c2k2 M −1 M −1 M −1 M −1 M −1 M −1
With k chosen as in Theorem 10, this reduces to

Var(cid:20)(cid:90)
W(cid:0)
P
,Pˇk(cid:1)
ν(dx)(cid:21) ≲M−2 d( X2∨ +d dY Y)
ln(M), 2∨dY ≤dX,
x x
X M−1, 2∨dY >dX.
2.4 Comments on the convergence rate
This sections gathers several comments on the convergence results we have developed in Section 2.2 and 2.3.
92.4.1 On the convergence rate
We first comment on the expectations in Theorem 7 and 10.
Sharpness of the bounds. Currently, we cannot establish the sharpness of the convergence rates in
Theorems 7 and 10. We can, however, compare our results to established results in similar settings. For
dY =1,wemaycompareittotheoptimalrateofnon-parametricregressionofaLipschitzcontinuousfunction.
It is shown in [Sto82] that the optimal rate is M− dX1 +2, the same as in Theorems 7 and 10 when dY =1. For
dY ≥ 3, as noted in [BBBW22], we may compare to the Wasserstein convergence rate of empirical measure
in the estimation of a static distribution on RdX+dY. We refer to [FG15] for the optimal rate, which coincides
with those in Theorems 7 and 10.
Error components. We discuss the composition of our upper bound on the expected average error by
dissecting the proof of Theorem 7 and 10. In the proofs, we decompose the expected average errors into two
components: approximation error and estimation error. The approximation error occurs when treating P
x′
as equal to P when x′ is close to the query point x, leading to an error of size L∥x−x′∥ . The estimation
x ∞
error is associated with the Wasserstein error of empirical measure under i.i.d. sampling (see (21)). From
Definitions 5 and 9, the r-box estimator effectively manages the approximation error but struggles with
controlling the estimation error, whereas the k-nearest-neighbor estimator exhibits the opposite behavior.
Explicit bounds. We primarily focus on analyzing the convergence rates of the r-box and k-nearest-
neighbor estimators as M → ∞. Therefore, within the proofs of these results, we track only the rates (and
ignore various constant coefficients). If more explicit bounds are preferred, intermediate results such as (23),
or (27) could be good starting points for computing them.
2.4.2 On the fluctuation
We next discuss the variances studied in Theorems 8 and 11. In Appendix C, we also include results derived
from the Azuma-Hoeffding inequality (e.g., [Wai19, Corollary 2.20]), though they provide rougher rates.
Condition that ν is dominated by λX. In Theorems 8 and 11, we assume that the ν is dominated
by λX. This assumption is somewhat necessary. To illustrate, let us examine the non-parametric regression
problem under a comparable scenario. We consider a fixed query point. In this context, the central limit
theorem for k-nearest-neighbor estimator is well-established, and the normalizing rate is k− 21 (cf. [BD15,
Theorem 14.2]). This suggests that the rate in (5) is sharp. For the r-box estimator, we believe that a
supporting example can be constructed where ν is highly concentrated. On the other hand, we conjecture
that if ξ ∼ ν, the variance could potentially attain the order of M−1. For a pertinent result, we direct the
reader to [BBBW22, Theorem 1.7].
Sharpness of the bounds. RegardingthevarianceinTheorem8,itisupperboundedbythecommonly
observed order of M−1. We believe that this rate is sharp, though we do not have a proof at this time. As
for Theorem 11, the variance is subject to a rougher rate when 2∨dY ≤ dX. We, however, conjecture that
this variance attains the order of M−1 as long as ν is dominated by λX.
2.5 Towards implementation with neural networks
In light of recent practices in machine learning, during the learning of P, we may combine the r-box method
or k-nearest-neighbor method into the training of certain parameterized model. To this end we let
P˜ :T×X→P(Y)
(θ,x)(cid:55)→P˜θ
x
beaparameterizedmodel(e.g.,aneuralnetwork),whereTistheparameterspaceandθ ∈Tistheparameter
to be optimized over. Given an integer N ≥ 1, we may train P˜θ on a set of query points Q = (X˜ )N
n n=1
satisfying the assumption below.
10Assumption 12. The query points Q = {(X˜ )}N are i.i.d. with uniform distribution over X, and are
n n=1
independent of the data points D ={(X ,Y )}M .
m m m=1
We propose the training objectives below
N N
argmin 1 (cid:88) W(cid:16) Pˆr ,P˜θ (cid:17) or argmin 1 (cid:88) W(cid:16) Pˇk ,P˜θ (cid:17) , (6)
θ∈T N X˜ n X˜ n θ∈T N X˜ n X˜ n
n=1 n=1
that is, minimize the mean of 1-Wasserstein errors between the parametrized model and the empirical r-box
(or k-nearest-neighbour) approximation of the conditional distribution at the location of the random query
points.
The following proposition together with Theorem 7 or Theorem 10 justifies using the objectives in (6). It
is valid for any estimator for P that satisfies the bounds in (3) or (4). Moreover, due to Lipschitz continuity
conditions in the proposition, the proposition provides insights into the worst-case performance guarantee.
We also refer to [AHS23] for a worst-case performance guarantee for conditional generative models, which is
contingent upon Lipschitz continuity. In contrast, similar guarantees for the r-box and k-nearest-neighbor
estimators are more elusive due to their inherently piece-wise constant nature. We refer to Section 4.6 for
the proof.
Proposition 13. Suppose Assumptions 2, 3, and 12 hold. Let P of P be an estimator constructed using the
data points D only. Consider a training procedure that produces a (random) Θ=Θ(D,Q) satisfying
W(P˜Θ,P˜Θ)
sup x x′ ≤LΘ (7)
x,x′∈X
∥x−x′∥
∞
for some (random) LΘ >0. Then,
E(cid:20)(cid:90)
XW(P x,P˜
xΘ)dx(cid:21) ≤E(cid:34) (L+LΘ)W(cid:32)
λX,
N1
(cid:88)N
δ
X˜
n(cid:33)(cid:35)
n=1
(8)
+E(cid:20)(cid:90)
W(P ,P
)dx(cid:21) +E(cid:34) 1 (cid:88)N W(cid:16)
P ,P˜Θ
(cid:17)(cid:35)
.
X x x N X˜ n X˜ n
n=1
Moreover, with probability 1,
supW(cid:16) P x,P˜ xΘ(cid:17) ≤(dX+1)dX1 +1(L+LΘ)dXd +X 1 (cid:18)(cid:90) W(P x,P˜ xΘ)dx(cid:19) dX1 +1 . (9)
x∈X X
Remark 14. Assuming LΘ ≤L for some (deterministic) L>0, by (9) and Jensen’s inequality, we have
E(cid:20) supW(cid:16) P x,P˜ xΘ(cid:17)(cid:21) ≤(dX+1)dX1 +1(L+L)dXd +X 1E(cid:20)(cid:90) W(P x,P˜ xΘ)dx(cid:21) dX1 +1 .
x∈X X
This together with (8) provides a worst-case performance guarantee for P˜Θ.
Remark 15. Proposition 13 along with Remark 14 provides insights into the worst-case performance guaran-
(cid:104) (cid:105)
tees,butmoreanalysisisneeded. Specifically,understandingthemagnitudeofLΘandE 1 (cid:80)N W(P ,P˜Θ )
N n=1 X˜ n X˜ n
requires deeper knowledge of the training processes for P˜Θ, which are currently not well understood in the
extant literature. Alternatively, in the hypothetical case where P˜Θ = P, LΘ would match L as specified in
(cid:104) (cid:16) (cid:17)(cid:105)
Assumption 2, and E 1 (cid:80)N W P ,P˜Θ would obey Theorem 7 or 10. However, practical applica-
N n=1 X˜ n X˜ n
tionsmustalsoconsidertheuniversalapproximationcapabilityofP˜θ. Furtherdiscussiononthistopiccanbe
found in [Kra23, AKP24], although, to the best of our knowledge, recent universal approximation theorems
in this subject do not yet concern continuity constraints.
113 Implementation with neural networks
Let X and Y be equipped with ∥·∥ . Following the discussion in Section 2.5, we let P˜θ : X → P(Y) be
1
parameterized by a neural network and develop an algorithm that trains P˜θ based on k-nearest-neighbor
estimator. The k-nearest-neighbor estimator Pˇk is preferred as Pˇk consistently outputs k atoms. This
x
regularity greatly facilities implementation. For instance, it enables the use of 3D tensors during Sinkhorn
iterations to enhance execution speed (see Section 3.1.2 later). We refer also to the sparsity part of Section
5.2 for another component that necessitates the aforementioned regularity of Pˇk. These components would
notbefeasiblewithther-boxestimatorPˆr,asPˆr producesanundeterminednumberofatoms. Furthermore,
x
there is a concern that in some realizations, Pˆr at certain x may contain too few data points, potentially
x
leading P˜Θ to exhibit unrealistic concentration.
x
We next provide some motivation for this implementation. For clarity, we refer to the r-box estimator
and the k-nearest-neighbor estimator as raw estimators. Additionally, we refer to P˜Θ, once trained, as the
neural estimator. While raw estimators are adequate for estimating P on their own, they are piece-wise
constantinxbydesign. Ontheotherhand,aneuralestimatoriscontinuousinx. Thiscontinuityprovidesa
performanceguaranteeinsupW distance,asoutlinedinProposition13andthefollowingremark. Moreover,
theneuralestimatorinherentlypossessesgradientinformation. Asdiscussedintheintroduction, thisfeature
renders the neural estimators useful in downstream contexts where gradient information is important, e.g.,
when performing model-based reinforcement learning.
WeconstructP˜θ suchthatitmapsx∈XtoatomsinYwithequalprobabilities. Fortherelateduniversal
approximation theorems, we refer to [Kra23, AKP24]. We represent these atoms with a vector with N
atom
e imnt pr li ee ms ed ne tn ao tt ie od n,b wy ey sθ e( tx) N= (y 1θ =(x k), .. T.. o,y bNθ
ea pto rm
e( cx is) e) ,∈ weY cN oa nto sm tr, uw cthe P˜re
θ
N sua ct hom th∈ atN is chosen by the user. In our
atom
P˜θ =
1 N (cid:88)atom
δ , x∈N. (10)
x N
atom
y jθ(x)
j=1
This is known as the Lagrangian discretization (see [PC19, Section 9]). In Algorithm 1, we present a high
level description of our implementation of training P˜θ based on the raw k-nearest-neighbor estimator.
Algorithm 1 Deep learning conditional distribution in conjunction with k-NN estimator
Input: data{(Xm,Ym)}M
m=1
valuedinRdX×RdY,neuralestimatorP˜θ representedbyyθ(x)aselaboratedin(10),parameters
suchask,Natoms,N batch∈N +,andlearningrateη
θ
Output: trainedparameterΘfortheneuralestimator
1: repeat
2: forn=1,...,N do
batch
3: generateaquerypointX˜ n∼Uniform(X)
4: findthek nearestneighborsofX˜ n fromdata(Xm)M m=1 andcollectaccordingly(Y˜ n,i)k i=1
5: endfor
6: computewithSinkhornalgorithm(Yisequippedwith∥·∥1)
Nb(cid:88)atch  1(cid:88)k 1 N (cid:88)atom 
L[θ]:=
n=1
W
k
i=1δ Y˜n,i,
Natom
j=1
δ yjθ(X˜n) (11)
7: updateθ←θ−η ∇ L[θ]
θ θ
8: untilConvergence
9: return Θ=θ
3.1 Overview of key components
In this section, we outline the three key components of our implementation. Each of these components
addresses a specific issue:
12Figure 1: An instance of RBSP in [0,1]2.
The 2D unit box is partitioned into 16 rectangles based on 500 samples from Uniform([0,1]). Note that the overlap
between the bounding rectangles is intentionally maintained. Each partitioning is performed along an axis selected
at random, dividing the samples within the pre-partitioned rectangle according to a random ratio drawn from
Uniform([0.45,0.55]). The edge ratio for mandatory bisecting along the longest edge is 5. If this ratio is exceeded,
partitioning along the longest edge is enforced. The black dots represent samples within the respective rectangle.
◦ Managing the computational cost arising from the nearest neighbors search.
◦ Implementing gradient descent after computing W.
◦ Selecting an appropriate Lipschitz constant for the neural estimator, preferably at a local level.
Further details and ablation analysis on these three components can be found in Section 5.
3.1.1 ApproximateNearestNeighborsSearchwithRandomBinarySpacePartitioning(ANNS-
RBSP)
Given a query point, performing an exact search for its k-nearest-neighbor requires O(M) operations. While
a single search is not overly demanding, executing multiple searches as outlined in Algorithm 1 can result
in significant computational time, even when leveraging GPU-accelerated parallel computing. To address
this, we use ANNS-RBSP as a more cost-effective alternative. Prior to searching, we sort (X )M along
m m=1
each axis and record the order of indices. During the search, the data is divided into smaller subsets by
repeatedly applying bisection on these sorted indices, with a random bisecting ratio, on a randomly chosen
axis. Furthermore, weapplyarestrictionthatmandatesbisectionalongthelongestedgeofarectanglewhen
the edge ratio exceeds certain value (a hyper-parameter of the model). We record the bounding rectangle
for each subset created through this partitioning process. Once partitioning is complete, we generate a small
batch of query points within each rectangle and identify the k nearest neighbors for each query point within
that same rectangle. For a visual representation of ANNS-BSP, we refer to Figure 1. Leveraging the sorted
indices, we can reapply this partitioning method during every training episode without much computational
cost. We refer to Section 5.1 for additional details. There are similar ideas in the extant literature (cf.
[HAYSZ11, RS19, LZS+20]). Given the substantial differences in our setting, however, we conduct further
empirical analysis in Section 5.1 to showcase the advantage of our approach against exact search.
133.1.2 Computing W for gradient descent
Thefollowingdiscussionpertainstothecomputationof(11),withthesubsequentgradientdescentinconsid-
eration. For simplicity, let us focus on the summand and reduce the problem to the following minimization.
Let (y˜ ,...,y˜ )∈Yk be fixed, we aim to find
1 k
 
k n
1 (cid:88) 1 (cid:88)
ar yg ∈m YninW
k
δ y˜i,
n
δ yj. (12)
i=1 j=1
The criterion in (12) is convex as W is convex in both arguments (cf. [Vil08, Theorem 4.8]). To solve (12),
asisstandard, wecastitintoadiscreteoptimaltransportproblem. Todoso, firstintroducethe(k×n)-cost
matrixC , whereC :=∥y˜ −y ∥ . Asthecriterionin(12)hasuniformweightsontheatoms, wenextaim
y y,ij i j 1
to solve the problem
 
 (cid:88) 
argmin φ (T):= T C (13)
y ij y,ij
T∈[0,1]k×n 
(i,j)∈{1,...,k}×{1,...,n}
n k
(cid:88) 1 (cid:88) 1
subject to T = , i=1,...,k and T = , j =1,...,n.
ij k ij n
j=1 i=1
Let T∗ be an optimal transport plan that solves (13) for y fixed. Taking derivative of y (cid:55)→φ (·) yields
y y
∂ yjφ y(T)(cid:12) (cid:12)
T=T∗
= (cid:88) T∗
y,ij
∂ yj∥y˜ i−y j∥ 1, j =1,...,n. (14)
y
i∈{1,...,k}
This gradient is in general not the gradient corresponding to (12), as T∗ depends on y, while (14) excludes
y
suchdependence. Nevertheless,itisstillviabletoupdateyusingthegradientdescentthatemploysthepartial
gradient specified in (14). To justify this update rule, first consider y′ ∈Y satisfying φ (T∗)≤φ (T∗), then
y′ y y y
observe that
   
k n k n
1 (cid:88) 1 (cid:88) 1 (cid:88) 1 (cid:88)
W
k
δ y˜i,
n
δ
y
j′≤φ y′(T∗ y)≤φ y(T∗ y)=W
k
δ y˜i,
n
δ yj.
i=1 j=1 i=1 j=1
This inequality is strict if φ (T∗) < φ (T∗). We refer to [PC19, Section 9.1] and the reference therein for
y′ y y y
related discussions.
TheSinkhornalgorithm,whichaddsanentropyregularization,isawidely-usedalgorithmforapproximat-
ing the solution to (13). Specifically, here, it is an iterative scheme that approximately solves the following
regularized problem, subject to the constraints in (13),
 
 (cid:88) (cid:88) 
argmin Tϵ C +ϵ Tϵ (logT −1) , (15)
ij ij ij ij
Tϵ∈[0,1]k×n 
i,j∈{1,...,k}×{i,...,n} i,j∈{1,...,k}×{i,...,n}
whereϵ>0isahyper-parameter, andshouldnotbeconfusedwiththeεusedelsewhere. WerefertoSection
5.2 for further details. We also refer to [PC19, Section 4] and the reference therein for convergence analysis
of the Sinkhorn algorithm. It is well known that the regularization term in (15) is related to the entropy of
a discrete random variable. Larger values of ϵ encourages the regularized optimal transport plan to be more
diffusive. That is, for larger values of ϵ, the mass from each y is distributed more evenly across all y˜’s.
j i
Performing gradient descent along the direction in (14) tends to pull y ’s towards the median of the y˜’s, as
j i
we are equipping Y with the norm ∥·∥ . Conversely, small values of ϵ often leads to instability, resulting in
1
NaN loss/gradient. To help with these issues, we implement the Sinkhorm algorithm after normalizing the
costmatrix. Additionally, weusealargeϵ(e.g., 1)inthefirstfewtrainingepisodes, thenswitchtoasmaller
ϵ(e.g.,0.1)inlaterepisodes. Furthermore,weimposesparsityonthetransportplanbymanuallysettingthe
smaller entries of the transport plan to 0. The specific detailed configurations and related ablation analysis
are provided in Section 5.2 and Appendix B.
143.1.3 Network structure that induces locally adaptive Lipschitz continuity
As previously discussed, it is desirable for the neural estimator to exhibit certain Lipschitz continuity. In
practice, however, determining an appropriate Lipschitz constant for training the neural estiamtor P˜θ is
challenging, largely because understanding the true Lipschitz continuity of P (if it exists) is very challeng-
ing. Additionally, the estimate provided in Proposition 13 is probabilistic. Fortunately, a specific network
structureallowstheneuralestimator,whenproperlytrained,toexhibitlocallyadaptiveLipschitzcontinuity.
Subsequently,weprovideahigh-leveloverviewofthisnetworkstructure. Furtherdetailedconfigurationsand
ablation analysis are presented in Section 5.3 and Appendix B.
Consider a fully connected feed-forward neural network with equal width hidden layers and layer-wise
residual connection [HZSS16]. Let N denote the width of the hidden layers. For activation, we use
neuron
Exponential Linear Unit (ELU) function [CUH16], denoted by σ. For hidden layers, we employ the convex
potential layer introduced in [MDAA22],
x =x −∥W∥−1WTσ(Wx +b). (16)
out in 2 in
By [MDAA22, Proposition 3], the convex potential layer is 1-Lipschitz continuous in ∥·∥ sense. For the
2
input layer, with a slight abuse of notation, we use
x =N−1 diag(|W|−1∧1)σ(Wx +b), (17)
out neuron 1 in
where |W| computes the absolute sum of each row of the weight matrix to form a vector of size N ,
1 neuron
the reciprocal and ·∧1 are applied entry-wise, and diag produces a diagonal square matrix based on the
input vector. In short, the normalization in (17) is only applied to the rows of W with ℓ -norm exceeding 1.
1
Consequently, the input layer is 1-Lipschitz continuous in ∥·∥ sense. A similar treatment is used for the
1
output layer but without activation,
x =Ld−1diag(|W|−1∧1)(Wx +b). (18)
out Y 1 in
where L > 0 is a hyper-parameter. The output represents atoms on Y with uniform weight, therefore, no
N−1 is required here.
atom
Thespectralnorm∥W∥ in(16),however,doesnot,ingeneral,haveanexplicitexpression. Followingthe
2
implementationin[MDAA22],weapproximateeach∥W∥ withpoweriteration. Poweriterationsareapplied
2
to all hidden layers simultaneously during training. To control the pace of iterations, we combine them with
momentum-based updating. We refer to Algorithm 2 for the detailed implementation. Our implementation
differs from that in [MDAA22], as the authors of [MDAA22] control the frequency of updates but not the
momentum. In a similar manner, for input and output layers, instead of calculating the row-wise ℓ -norm
1
explicitly, we update them with the same momentum used in the hidden layers. Our numerical experiments
consistently show that a small momentum value of τ =10−3 effectively maintains adaptive continuity while
maintaining a satisfactory accuracy. The impact of L in (18) and τ in Algorithm 2 is discussed in Section
5.3.
During training, due to the nature of our updating schemes, the normalizing constants do not achieve
the values required for the layers to be 1-Lipschitz continuous. We hypothesize that this phenomenon leads
to a balance that ultimately contributes to adaptive continuity: on one hand, the weights W stretch to fit
(or overfit) the data, while on the other, normalization through iterative methods prevents the network from
excessive oscillation. As shown in Section 5.3.2 and 5.3.3, the L value in (18) and the momentum τ in
Algorithm 2 affect the performance significantly. For completeness, we also experiment with replacing (16)
by fully connected feedforward layers similar to (17), with or without batch normalization [IS15] after affine
transformation. This alternative, however, failed to produce satisfactory results.
3.2 Experiments with synthetic data
We consider data simulated from three different models. The first two have dX = dY = 1, while the third
has dX = dY = 3. Here we no longer restrict Y to be the unit box, however, we still consider X to be a
15Algorithm 2 Power iteration with momentum for updating ∥W∥ estimate, applied to all convex potential
2
layers simultaneously at every epoch during training
Input: weightmatrixW∈Rd×d ofaconvexpotentiallayer,previousestimatehˆ∈Randauxiliaryvectoruˆ∈Rd,momentum
τ ∈(0,1)
Output: updatedhˆ anduˆ,inparticular,hˆ willbeusedasasubstituteof∥W∥2 in(16)
1: v←Wuˆ/∥Wuˆ∥2
2: u←WTv/∥WTv∥2 power iteration
3:
h←2/(cid:0)(cid:80) i(Wu·v)i(cid:1)2
4: hˆ←τhˆ+(1−τ)h momentum-
5: uˆ←τuˆ+(1−τ)u based updating
6: return hˆ,uˆ
dX-dimensional unit box (not necessarily centered at the origin).
In Model 1 and 2, X ∼ Uniform([0,1]). Model 1 is a mixture of two independent Gaussian random
variables with mean and variance depending on x,
(cid:16) (cid:0) (cid:1) (cid:12) (cid:12) (cid:17)
Y =ξ 0.1 1+cos(2πX) +0.12(cid:12)1−cos(2πX)(cid:12)Z+0.5 ,
where Z ∼Normal(0,1) and ξ is a Rademacher random variable independent of Z. For Model 2, we have
Y =0.51 (X)+0.5U,
[0,1)
where U ∼ Uniform([0,1]). The conditional distribution in Model 2 is intentionally designed to be discon-
tinuous in the feature space. This choice was made to evaluate performance in the absence of the Lipschitz
continuity stipulated in Assumption 2. Model 3 is also a mixture of two independent Gaussian random
variables,constructedbyconsideringX ∼Uniform([−1,1]3)andtreatingX asacolumnvector(i.e.,X take
2 2
values in R3×1),
(cid:16) (cid:17) (cid:16) (cid:17)
Y =ζ cos(AX)+0.1cos(Σ )W +(1−ζ) cos(A′X)+0.1cos(Σ′ )W′ .
X X
Above, the cos functions act on vector/matrix entrywise, A ∈ R3×3, and Σ also takes value in R3×3. Each
x
elementofΣ isdefinedasv xforsomev ∈R1×3. TheentriesofAandv aredrawnfromstandardnormal
x ij ij ij
in advance and remain fixed throughout the experiment. The matrices A′ and Σ′ are similarly constructed.
x
Furthermore, W and W′ are independent three-dimensional standard normal r.v.s, while ζ represents the
toss of a fair coin, independent of X, W, and W′.
For the purpose of comparison, two different network structures are examined. The first, termed LipNet,
is illustrated in Section 3.1. The second, termed StdNet, is a fully connected feedforward network with
layer-wise residual connections [HZSS16], ReLU activation, and batch normalization immediately following
each affine transformation, without specifically targeting Lipschitz continuity. With a hyper-parameter k
for the k-nearest-neighbor estimator, which we specify later, each network contains 5 hidden layers with 2k
neurons. These networks are trained using the Adam optimizer [KB17] with a learning rate of 10−3. For
StdNet in Model 1 and 2, the learning rate is set to 0.01, as it leads to better performance. Other than the
learning rates, StdNet and LipNet are trained with identical hyper-parameters across all models. We refer
to Appendix B for a summary of hyper-parameters involved.
We generate 104 samples for Models 1 and 2. Given the convergence rate specified in Theorem 10, we
notethatthesamplesizeareconsideredrelativelysmall. Forthesetwomodels,wechosek =100andutilized
neural networks P˜θ that output atoms of size N =k. The choice of k is determined by a rule of thumb.
atom
Inparticular,ourconsiderationsincludethemagnitudeofk suggestedbyTheorem10andthecomputational
costs associated with the Sinkhorn iterations discussed in Section 3.1.2. The results under Model 1 and 2
are plotted in Figure 2, 3 and 4. Figure 2 provides a perspective on joint distributions, while Figure 3 and 4
focus on conditional CDFs across different x values.
16(a) Model 1, StdNet (b) Model 1, LipNet (c) Model 2, StdNet (d) Model 2, LipNet
Figure 2: Various estimators under Model 1 and 2, joint distributions.
The first row presents the data. Neural networks with different structures are trained on the same set of data for
comparison. Thesecondrowshowsscatterplotsofatomsatvariousxvalues. Thethirdrowillustratestheevolution
of 20 atoms as x varies. The final row presents the average of the derivative of each atom with respect to x, with a
notable difference in the y axis scale.
17
ataD
derettacs
smota
llA
smota
02
fo.jarT
.red.sba.evAFigure 3: Various estimators under Model 1 and 2, conditional CDFs.
We compare the conditional CDFs at various values of x, derived from StdNet, LipNet, the raw k-nearest estimator
with k=100 (also used in the training of StdNet and LipNet), and the ground truth. The first row pertains to data
set 1. The second row pertains to data set 2. Subfigure titles display the values of x.
Figure 4: Errors at different x’s of various estimators under Model 1 and 2.
We compute the W-distance between estimators and the true conditional distribution at different x’s. StdNet and
LipNet are trained based on the raw k-nearest-neighbor estimator with k=100.
18
1
ledoM
2
ledoM
1
ledoM
2
ledoMFigure 2 suggets that both StdNet and LipNet adequately recover the joint distribution. The LipNet’s
accuracyis,however,notablysuperiorandproducessmoothmovementsofatoms(asseeninthethirdrowof
Figure 2). Although further fine-tuning may provide slight improvements in StdNet’s performance, StdNet
will still not achieve the level of accuracy and smoothness observed in LipNet. The average absolute value
of derivative of each atom (fourth row of Figure 2), makes it evident that LipNet demonstrates a capacity
of automatically adapting to a suitable level of Lipschitz continuity locally. In particular, in Model 2, the
atoms of LipNet respond promptly to jumps while remaining relatively stationary around values of x where
the kernel is constant. We emphasize that LipNet is trained using the same hyper-parameters across Models
1, 2, and 3.
Figure 3 shows the estimated conditional distribution at different values of x. Figure 3 indicates that
the raw k-nearest-neighbor estimator deviates frequently from the actual CDFs. This deviation of the raw
k-nearest-neighbor estimator is expected, as it attempts to estimate an unknown CDF with only k = 100
samplesgivenanx. Conversely,theneuralestimator,especiallytheLipNet,appearstoofferextracorrections
even if they are trained based on the raw k-nearest-neighbor estimator. This could be attributed to neural
estimators implicitly leveraging information beyond the immediate neighborhood.
Figure4comparestheW-distancebetweeneachestimatorandthetrueconditionaldistributionatvarious
values of x, using the following formula (see [PC19, Remark 2.28]),
(cid:90)
(cid:12) (cid:12)
W(F,G)= (cid:12)F(r)−G(r)(cid:12)dr, (19)
R
where F and G are CDFs. This quantity can be accurately approximated with trapezoidal rule. In Model
1, the neural estimator generally outperforms the raw estimator with k = 100 across most values of x,
even though the raw estimator is used for training the neural estimators. Furthermore, LipNet continues to
outperformrawestimatorswithlargervaluesofk –eventhoughLipNetistrainedwitharawestimatorwith
k = 100. In Model 2, LipNet continues to demonstrate a superior performance, except when compared to
the raw estimator with k =1,000 at x distant from 0.5, the reason is that, here, the conditional distribution
is piece-wise constant in x, which enhances the performance of the raw estimator at larger k values.
The aforementioned findings indicate superior performance by LipNet. We, however, recognize that
improvements are not always guaranteed, as demonstrated in Figures 3 and 4.
For Model 3, we generate 106 samples and select k =300. We train both neural estimators using Adam
optimizerwithalearningrateof10−3. HyperparameterssuchasLin(18)andτ inAlgorithm2areconsistent
with those used for Models 1 and 2. We refer to Appendix B for the detailed configuration.
In Figure 5, we visualize the outcomes in Model 3: the conditional CDFs at an arbitrarily chosen x
are projected onto various vectors. We observe that the neural estimators considerably outperform the
raw k-nearest-neighbor estimator, likely owing due to their implicit use of global information outside of the
immediate neighbors during training. For further comparisons, we present additional figures in Appendix A:
Figures 14, 15 and 16 feature the exact same neural estimators as shown in Figure 5, but with the raw k-
nearest-neighbor estimators employing different k values, k =1,000, 3,000, 10,000. Raw k-nearest-neighbor
estimatorswithk =1,000, 3,000aresuperiortothatwithk =300,whileatk =10,000,theaccuracybegins
to decline. Upon comparison, the neural estimator trained with k = 300 consistently outperforms the raw
k-nearest-neighbor estimators for all values of k.
Foramorecomprehensivecomparison,werandomlyselect10,000querypoints. Foreachquerypoint,we
randomlygenerateavectorinR3,normalizedunder∥·∥ ,andprojecttheatomsproducedbytheestimators
1
onto said vector. With the same vector, we also compute the corresponding true CDFs of the projected Y
given the query point. We then approximately compute the W-distance between the projected distributions
via (19). The resulting histograms are shown in Figure 6, which suggests that LipNet performs best. The
rationale for employing this projection approach, rather than directly computing the W-distance between
discrete and continuous distributions over R3, is due to the higher cost and lower accuracy of the latter
approach (see also the discussion in Section 5.2). While this projection approach provides a cost-effective
19Figure 5: Various estimators under Model 3, projections of conditional CDFs.
We compare the projected conditional CDFs at x=(0.12,−0.33,0.1). The estimations are obtained from StdNet,
LipNet, the raw estimator with k=300 (also used in training StdNet and LipNet), and the ground truth. Subfigure
titles display the vectors used for projection. Note the difference in the x axis scale.
Raw k=1,000 Raw k=3,000 StdNet LipNet
Figure 6: Histogram of 10,000 projected Wasserstein-1 errors.
Each histogram consists of 20 uniformly positioned bins between 0 to 0.1. The errors of different estimators
are computed with the same set of query points and projection vectors. Errors larger than 0.1 will be
placed in the right-most bins. Note StdNet and LipNet are trained with k =300.
20Figure 7: LipNet under Model 3, projected trajectories of 20 atoms.
We illustrate the projected trajectories of 20 atoms by evaluating LipNet at 100 evenly allocated points along the
straight line that intersects the origin and x=(0.12,−0.33,0.1), situated within [0,1]3. The x-axis denotes the
specific points along the line, consistent across all subfigures. Subfigure titles display the vectors used for projection.
Note the difference in the y axis scale.
21alternative for performance evaluation, it may not fully capture the differences between the estimations and
ground truth.
Lastly,todemonstratehowatoms,intheneuralestimator,moveasxvaries,Figure7showstheprojected
trajectories along a randomly selected straight line through the origin. The movement of atoms in LipNet
is smooth, consistent with previous observations. Interestingly, the movement of atoms in StdNet isn’t
excessively oscillatory either, although its continuity is slightly rougher compared to LipNet. The reader
may execute the Jupyter notebook on our github repisitory https://github.com/zcheng-a/LCD_kNN to explore the
projected conditional CDFs and atoms’ trajectories for different x values.
4 Proofs
4.1 Auxiliary notations and lemmas
In this section, we will introduce a few technical results that will be used in the subsequent proofs.
We first define
(cid:90) (cid:32) 1 (cid:88)m (cid:33) (cid:79)m
R(m):=sup W P , δ P (dy ), m∈N. (20)
x∈X Ym x m yℓ x ℓ
ℓ=1 ℓ=1
We stipulate that R(0)=1. By [FG15], we have

⌢
m− 21, dY =1,
R(m)≤C × m− 21 ln(m), dY =2, (21)
m− d1
Y, dY ≥3,
⌢
for some constant C > 0 depending only on dY. For comprehension, we also point to [Klo20, Fou23] for
results that are potentially useful in analyzing explicit constant, though it is out of the scope of this paper.
The lemma below pertains to the so-called approximation error, which arises when treating data points
Y with X around an query point as though they are generated from the conditional distribution at the
j j
query point.
Lemma 16. Under Assumption 2, for any integer J ≥1 and x,x ,...,x ∈XJ+1, we have
1 J
(cid:12)     (cid:12)
(cid:12) (cid:12)(cid:90) 1 (cid:88)J (cid:79)J (cid:90) 1 (cid:88)J (cid:79)J (cid:12) (cid:12) L(cid:88)J
(cid:12) (cid:12)
(cid:12)
YJ W J j=1δ yj,P x j=1P xj(dy j)− YJ W J j=1δ yj,P x j=1P x(dy j)(cid:12) (cid:12) (cid:12)≤ J j=1∥x j −x∥ ∞.
Proof. For x,x ,...,x ∈XJ+1, note that
1 J
(cid:12) (cid:12) (cid:12)(cid:90)  1 (cid:88)J  (cid:79)J (cid:90) (cid:32) 1 (cid:88)J (cid:33) (cid:79)J (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12)
YJ W J j=1δ yj,P x j=1P xj(dy j)− YJ W J k=1δ yj,P x j=1P x(dy j)(cid:12) (cid:12)
(cid:12)
(cid:12)     (cid:12)
(cid:88)J (cid:12) (cid:12)(cid:90) 1 (cid:88)J (cid:79)ℓ−1 (cid:79)J (cid:90) 1 (cid:88)J (cid:79)ℓ (cid:79)J (cid:12) (cid:12)
≤ ℓ=1(cid:12)
(cid:12)
(cid:12)
YJ
W
J
j=1δ yj,P x j=1P x(dy j)⊗ j=ℓP xj(dy j)−
YJ
W
J
j=1δ yj,P x j=1P x(dy j)⊗ j=ℓ+1P xj(dy j)(cid:12)
(cid:12)
(cid:12),
where for the sake of neatness, at ℓ=1,J, we set
0 J J J J J
(cid:79) (cid:79) (cid:79) (cid:79) (cid:79) (cid:79)
P (dy )⊗ P (dy )= P (dy ) and P (dy )⊗ P (dy )= P (dy ).
x j xj j xj j x j xj j xj j
j=1 j=1 j=1 j=1 j=J+1 j=1
22Regardingtheℓ-thsummand,invokingFubini-Tonelitheoremtointegratey firstthencombiningtheintegrals
ℓ
on outer layers using linearity, we obtain
(cid:12)     (cid:12)
(cid:12) (cid:12)(cid:90) 1 (cid:88)J (cid:79)ℓ−1 (cid:79)J (cid:90) 1 (cid:88)J (cid:79)ℓ (cid:79)J (cid:12) (cid:12)
(cid:12)
(cid:12)
(cid:12)
YJ
W
J
j=1δ yj,P x j=1P x(dy j)⊗ j=ℓP xj(dy j)−
YJ
W
J
j=1δ yj,P x j=1P j(dy j)⊗ j=ℓ+1P xj(dy j)(cid:12)
(cid:12)
(cid:12)
(cid:12)   (cid:12)
(cid:12) (cid:12)(cid:90) (cid:90) 1 (cid:88)J (cid:79)ℓ−1 (cid:79)J (cid:12) (cid:12)
=(cid:12)
(cid:12)
(cid:12)
YJ−1
YW
J
j=1δ yj,P x(P x−P xℓ)(dy ℓ) j=1dP x(y j)⊗ j=ℓ+1P xj(dy j)(cid:12)
(cid:12)
(cid:12)
(cid:12)   (cid:12)
(cid:12) (cid:12)(cid:90) 1 (cid:88)J (cid:12) (cid:12) 1 L
≤ (yj)j̸=su ℓ∈p YJ−1(cid:12) (cid:12)
(cid:12)
YW J j=1δ yj,P x(P xℓ −P x)(dy ℓ)(cid:12) (cid:12) (cid:12)≤ JW(P xℓ,P x)≤ J∥x ℓ−x∥ ∞,
where in the second last inequality we have invoked Kantorovich-Rubinstein duality (cf. [Vil08, Particular
(cid:16) (cid:17)
case 5.16]) and the fact that, for all (y ) ∈YJ−1, the map y (cid:55)→W 1 (cid:80)J δ ,P is 1-Lipschitz, and
j j̸=ℓ ℓ J j=1 yj x J
where in the last equality, we have used Assumption 2.
Wewillbeusingthelemmabelow,whichregardsthestochasticdominancebetweentwobinomialrandom
variables.
Lemma 17. Let n∈N and 0≤p<p′ ≤1. Then, Binomial(n,p′) stochastically dominates Binomial(n,p).
i.i.d.
Proof. Let U ,...,U ∼ Uniform[0,1] and define
1 n
n n
(cid:88) (cid:88)
H := 1 (U ), H′ := 1 (U ).
[0,p] i [0,p′] i
i=1 i=1
Clearly, H ∼ Binomial(n,p) and H′ ∼ Binomial(n,p′). Moreover, we have H ≤ H′, and thus P(H > r) ≤
P(H′ >r), which completes the proof.
4.2 Proof of Theorem 7
The proof of Theorem 7 relies the technical lemma below that we state and prove now.
Lemma 18. Let p∈[0,1] a real number, and let M ≥1 and d≥1 two integers. We then have
M (cid:18) (cid:19)
(cid:88) M pm(1−p)M−mm− d1 ≤((M +1)p)− d1 +((M +1)p)−1.
m
m=1
Proof. We compute
M (cid:18) (cid:19) M (cid:18) (cid:19)
(cid:88) M pm(1−p)M−mm− d1 = 1 (cid:88) M +1 pm+1(1−p)M−m(m+1)m− d1
m (M +1)p m+1
m=1 m=1
M+1(cid:18) (cid:19)
= 1 (cid:88) M +1 pm(1−p)M+1−mm(m−1)− d1
(M +1)p m
m=2
M+1(cid:18) (cid:19)
= 1 (cid:88) M +1 pm(1−p)M+1−m(m−1)1− d1
(M +1)p m
m=2
M (cid:18) (cid:19)
+ 1 (cid:88) M +1 pm(1−p)M+1−m(m−1)− d1,
(M +1)p m
m=2
23whereweusedthatm=m−1+1inthelastequality. Then,usingthat(m−1)1− d1 ≤m1− d1 and(m−1)− d1 ≤1
for all m≥2, we continue to obtain
M (cid:18) (cid:19) M+1(cid:18) (cid:19)
(cid:88) M pm(1−p)M−mm− d1 ≤ 1 (cid:88) M +1 pm(1−p)M+1−mm1− d1
m (M +1)p m
m=1 m=2
M (cid:18) (cid:19)
1 (cid:88) M +1
+ pm(1−p)M+1−m
(M +1)p m
m=2
M+1(cid:18) (cid:19)
≤ 1 (cid:88) M +1 pm(1−p)M+1−mm1− d1 + 1 ,
(M +1)p m (M +1)p
m=0
where the second term in the last equality are derived from the binomial formula. Finally, introducing a
random variable V with binomial distribution B(M + 1,p), and using Jensen inequality for the concave
function R+ ∋x(cid:55)→x1− d1 ∈R+, we obtain
(cid:88)M (cid:18) M(cid:19)
pm(1−p)M−mm− d1 ≤
1 E(cid:104)
V1−
d1(cid:105)
+
1
m (M +1)p (M +1)p
m=1
≤((M +1)p)1− d1
+
1
=((M +1)p)− d1 +((M +1)p)−1,
(M +1)p (M +1)p
which conclude the proof.
We are now ready to prove Theorem 7.
Proof of Theorem 7. For ν ∈P(X), we obviously have
(cid:20)(cid:90) (cid:21) (cid:104) (cid:105)
E W(P ,Pˆr)ν(dx) ≤supE W(P ,Pˆr) ,
x x x x
X x∈X
we then focus on proving the right hand side inequality in Theorem 7. To this end, we fix x ∈ X and, to
alleviate the notations, we let B := Br(x) as introduced in Definition 5. Let N := (cid:80)M 1 (X ). By
B m=1 B m
Definition 5 and Assumption 3 (i), we have
M
E(cid:104)
W(P
,Pˆr)(cid:105) =E(cid:2)
W(P
,µˆD)(cid:3)
=
(cid:88) E(cid:2)1
W(P
,µˆD)(cid:3)
x x x B NB=m x B
m=0
M (cid:18) (cid:19) (cid:34) (cid:32) m (cid:33)(cid:35)
(cid:88) M 1 (cid:88)
=
m
E 1 X1,...,Xm∈B1 Xm+1,...,XM̸∈BW
m
δ Yl,P
x
+P[X 1,...,X
M
̸∈B]W(λY,P x)
m=1 l=1
M (cid:18) (cid:19) (cid:34) (cid:32) m (cid:33)(cid:35)
(cid:88) M 1 (cid:88)
≤ P[X ,...,X ̸∈B]E 1 W δ ,P +ξ(Bc)MR(0)
m m+1 M X1,...,Xm∈B m Yl x
m=1 l=1
(cid:88)M (cid:18) M(cid:19) (cid:90) (cid:32) 1 (cid:88)m (cid:33) (cid:79)m
= µ(Bc)M−m W δ ,P ψ(dx dy )+ξ(Bc)MR(0). (22)
m m yl x ℓ ℓ
m=1
(B×Y)m
l=1 ℓ=1
To compute the integral terms, observe that, for fixed m≥1, by definition of R(m) in (20), Lemma 16 and
Remark 6,
(cid:90) (cid:32) 1 (cid:88)m (cid:33) (cid:79)m (cid:90) (cid:90) (cid:32) 1 (cid:88)m (cid:33) (cid:79)m (cid:79)m
W δ ,P ψ(dx dy )= W δ ,P P (dy ) ξ(dx )
m yl x ℓ ℓ m yl x xl l ℓ
(B×Y)m
l=1 ℓ=1
Bm Ym
l=1 l=1 ℓ=1
(cid:90) (cid:32) (cid:90) (cid:32) 1 (cid:88)m (cid:33) (cid:79)m L (cid:88)m (cid:33) (cid:79)m
≤ W δ ,P P (dy )+ ∥x −x∥ ξ(dx )
m yl x x ℓ m ℓ ∞ ℓ
Bm Ym
l=1 ℓ=1 ℓ=1 ℓ=1
(cid:90) m
(cid:79)
≤ (R(m)+2Lr) ξ(dx )=(R(m)+2Lr)ξ(B)m.
ℓ
Bm
ℓ=1
24This together with (22) implies that, for any x∈X,
E(cid:104)
W(P
,Pˆr)(cid:105)
≤
(cid:88)M (cid:18) M(cid:19)
ξ(Bc)M−mξ(B)m(R(m)+2Lr)+ξ(Bc)MR(0)
x x m
m=1
M (cid:18) (cid:19)
(cid:88) M
≤2Lr+ ξ(Bc)M−mξ(B)mR(m)+ξ(Bc)R(0) (23)
m
m=1
Theremainderoftheproofissplitintothreecases. Inordertoproceed,wewillputtogether(21),Lemma
18, and (23). Below we only keep track of the rate.
• For dY =1, we have
(cid:104) (cid:105)
E W(P x,Pˆ xr) ≤2Lr+(ξ(B)(M +1))−1 2 +(ξ(B)(M +1))−1+(1−ξ(B))M
≤2Lr+(cid:0) c(2r)dX(M +1)(cid:1)−1 2 +(cid:0) c(2r)dX(M +1)(cid:1)−1 +e−cMrdX.
Controlling the dominating term(s) by setting r ∼r−d 2XM−1 2, we yield
(cid:104) (cid:105)
r ∼M− dX1 +2 and E W(P x,Pˆ xr) ≲M− dX1 +2.
• For dY =2, we have
(cid:104) (cid:105)
E W(P x,Pˆ xr) ≤2Lr+ln(M)(ξ(B)(M +1))−1 2 +(ξ(B)(M +1))−1+(1−ξ(B))M
≤2Lr+ln(M)(cid:0) c(2r)dX(M +1)(cid:1)− 21 +(cid:0) c(2r)dX(M +1)(cid:1)−1 +e−cMrdX.
Since r ∼ln(M)r−d 2XM−1 2 may not have a closed-form solution, we simply follow the case of dY =1 to
yield
(cid:104) (cid:105)
r ∼M− dX1 +2 and E W(P x,Pˆ xr) ≲M− dX1 +2 lnM.
• For dY ≥3, we have
(cid:104) (cid:105)
E W(P x,Pˆ xr) ≤2Lr+(ξ(B)(M +1))− d1 Y +(ξ(B)(M +1))−1+(1−ξ(B))M
≤2Lr+(cid:0) c(2r)dX(M +1)(cid:1)− d1 Y +(cid:0) c(2r)dX(M +1)(cid:1)−1 +e−cMrdX.
By setting r ∼r−d dX YM− d1 Y, we yield
(cid:104) (cid:105)
r ∼M− dX+1 dY and E W(P x,Pˆ xr) ≲M− dX+1 dY.
The proof is complete.
4.3 Proof of Theorem 8
Proof of Theorem 8. We will proceed by using Efron-Stein inequality. Let (X′,Y′) be an independent copy
1 1
of(X ,Y ),anddefineD′ :={(X′,Y′),(X ,Y ),...,(X ,Y )}. InviewofAssumption3(i),bythetriangle
1 1 1 1 2 2 M M
inequality of W, it is sufficient to investigate
1
(cid:34)(cid:18)(cid:90)
(cid:16) (cid:17)
(cid:19)2(cid:35)
ME W µˆD ,µˆD′ dν(x) .
2 Br(x) Br(x)
X
25Notice that, by definitions (1),
(cid:110) (cid:111) (cid:110) (cid:111) (cid:110) (cid:111)
µˆD ̸=µˆD′ ⊆ X ∈Br(x) ∪ X′ ∈Br(x) .
Br(x) Br(x) 1 1
(cid:110) (cid:111)
Additionally, by definitions (1) again, on the event that µˆD ̸=µˆD′ , we have
Br(x) Br(x)
(cid:32) M (cid:33)−1
W(cid:16) µˆD ,µˆD′ (cid:17) ≤ 1+(cid:88) 1 (X ) .
Br(x) Br(x) Br(x) ℓ
ℓ=2
The above together with the condition that ν is dominated by λX implies that
(cid:34)(cid:18)(cid:90)
(cid:16) (cid:17)
(cid:19)2(cid:35) (cid:32)
(cid:90) (cid:16) (cid:17)
(cid:33)2
E W µˆD Br(x),µˆD Br′
(x)
ν(dx) ≤C2E  W µˆD Br(x),µˆD Br′
(x)
λX(dx) 
X B(X1,2r)∪B(X 1′,2r)

(cid:90) (cid:32) M (cid:33)−1
2
≤C2E  1+(cid:88) 1 Br(x)(X ℓ) λX(dx)  
B(X1,2r)∪B(X 1′,2r) ℓ=2

(cid:90) (cid:32) M (cid:33)−1
2
≤4C2E  1+(cid:88) 1 Br(x)(X ℓ) λX(dx)  
B(X1,2r) ℓ=2
=4C2E
 λX(B(X
1,2r))2 (cid:90) B(X1,2r)(cid:32) 1+(cid:88) ℓM
=21 Br(x)(X
ℓ)(cid:33)−1 λX(BλX (( Xd 1x ,) 2r)) 2


≤4C2 (4r)2dXE E (cid:90) B(X1,2r)(cid:32) 1+(cid:88) ℓM =21 Br(x)(X ℓ)(cid:33)−2 λX(BλX (( Xd 1x ,) 2r))(cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)X 1  , (24)
where we have used Jensen’s inequality and tower property in the last line. In view of Assumption 3 (i),
expanding the inner conditional expectation into an integral with respect to regular conditional distribution
(cf. [Bog07, Section 10]) then invoking Fubini-Tonelli theorem, we yield
E (cid:90) B(X1,2r)(cid:32) 1+(cid:88) ℓM
=21 Br(x)(X
ℓ)(cid:33)−2 λX(BλX (( Xd 1x ,) 2r))(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)X
1

=(cid:90) (cid:90) (cid:32) 1+(cid:88)M
1 (x
)(cid:33)−2 (cid:79)M
ξ(dx )
λX(dx)
. (25)
B(X1,2r) XM−1 ℓ=2
Br(x) ℓ
ℓ=2
ℓ λX(B(X 1,2r))
For the inner integral in (25), by Assumption 3 (ii), we have
(cid:90) (cid:32) M (cid:33)−2 M
(cid:88) (cid:79)
1+ 1 (x ) ξ(dx )
Br(x) ℓ ℓ
XM−1
ℓ=m+1 ℓ=2
=M (cid:88)−1(cid:18) M −1(cid:19) ξ(cid:0) Br(x)(cid:1)ℓ(cid:16) 1−ξr(cid:0) Br(x)(cid:1)(cid:17)M−1−ℓ (1+ℓ)−2
ℓ
ℓ=0
=
1 M (cid:88)−1(cid:18) M +1(cid:19) ξ(cid:0) Br(x)(cid:1)ℓ+2(cid:16) 1−ξ(cid:0) Br(x)(cid:1)(cid:17)M−1−ℓℓ+2
M(M +1)ξ(cid:0) Br(x)(cid:1)2 ℓ+2 ℓ+1
ℓ=0
≤
2 M (cid:88)+1(cid:18) M +1(cid:19) ξ(cid:0) Br(x)(cid:1)ℓ(cid:16) 1−ξ(cid:0) Br(x)(cid:1)(cid:17)M+1−ℓ
M(M +1)ξ(cid:0) Br(x)(cid:1)2 ℓ
ℓ=2
2
≤ .
(cid:0) (cid:1)2
M(M +1)ξ Br(x)
26This together with (24), (25) and Assumption 3 (ii) implies
(cid:34)(cid:18)(cid:90)
(cid:16) (cid:17)
(cid:19)2(cid:35) 22dXC2
E W µˆD ,µˆD′ dν(x) ≤8 .
Br(x) Br(x) c2M(M +1)
X
Invoking Efron-Stein inequality, we conclude the proof.
4.4 Proof of Theorem 10
In order to prove Theorem 10, we first establish a few technical lemmas. The following lemma is a first step
toward finding the average rate of k-nearest neigbhor method.
Lemma 19. Suppose Assumption 2 and 3. Let R be defined in Section 4.1. Then, for any x∈X, we have
k
E(cid:2) W(cid:0)
P
,Pˇk(cid:1)(cid:3)
≤R(k)+
L (cid:88) E(cid:104) Z(m)(cid:105)
,
x x k x
m=1
where Zx ,m=1,...,M are the order statistics of (∥X −x∥ )M in ascending order.
(m) m ∞ m=1
Proof. We fix x∈X for the rest of the proof. By Assumption 3, we have
(cid:34) (cid:32) k (cid:33)(cid:35)
E(cid:2) W(cid:0) P ,Pˇk(cid:1)(cid:3) =M!E 1 W P , 1 (cid:88) δ
x x ∥X1−x∥∞≤∥X2−x∥∞≤···≤∥XM−x∥∞ x k Yℓ
ℓ=1
(cid:90) (cid:32) 1 (cid:88)k (cid:33) (cid:79)M
=M! 1 W P , δ ψ(dx dy )
∥x1−x∥∞≤∥x2−x∥∞≤···≤∥xM−x∥∞ x k yℓ ℓ ℓ
(X×Y)M
ℓ=1 ℓ=1
(cid:90) (cid:90) (cid:32) 1 (cid:88)k (cid:33) (cid:79)k (cid:79)M
=M! 1 W P , δ P (dy ) ξ(dx ).
∥x1−x∥∞≤∥x2−x∥∞≤···≤∥xM−x∥∞ x k yℓ xℓ ℓ ℓ
XM Yk
ℓ=1 ℓ=1 j=1
In view of Lemma 16, replacing P above with P , we have
xℓ x
E(cid:2) W(cid:0)
P
,Pˇk(cid:1)(cid:3)
x x
(cid:90) (cid:90) (cid:32) 1 (cid:88)k (cid:33) (cid:79)k (cid:79)M
≤M! 1 W P , δ P (dy ) ξ(dx )
∥x1−x∥∞≤∥x2−x∥∞≤···≤∥xM−x∥∞ x k yℓ x ℓ ℓ
XM Yk
ℓ=1 ℓ=1 j=1
L(cid:88)k (cid:90) (cid:79)M
+
k
M! 1 ∥x1−x∥∞≤∥x2−x∥∞≤···≤∥xM−x∥∞dX(x ℓ,x) ξ(dx ℓ)
XM
ℓ=1 j=1
(cid:90) (cid:32) 1 (cid:88)k (cid:33) (cid:79)k L(cid:88)k (cid:90) (cid:79)M
= W
k
δ yl,P
x
P x(dy ℓ)+
k
M! 1 ∥x1−x∥∞≤∥x2−x∥∞≤···≤∥xM−x∥∞dX(x ℓ,x) ξ(dx ℓ).
Yk XM
l=1 ℓ=1 ℓ=1 j=1
In view of R defined above (21) and Zx defined in the statement of this lemma, we conclude the proof.
(m)
(cid:104) (cid:105)
The next lemma provides an upper bound to (cid:80)k E Z(m) listed in Lemma 19.
m=1 x
Lemma 20. Let Zx be defined as in Lemma 19. Under Assumption 3, for any x∈X, we have
(m)
(cid:88)k E(cid:104)
Zx
(cid:105)
≤
2 M! (cid:88)k m (cid:88)−1Γ(j+ d1 X)
.
m=1
(m) cd1 XdXΓ(M + d1
X
+1)
m=1 j=0
j!
27Proof. For any x∈X, we compute, since Zx ∈[0,1],
(m)
(cid:104) (cid:105) (cid:90) 1 (cid:104) (cid:105) (cid:90) 1(cid:16) (cid:104) (cid:105)(cid:17)
E Zx = P Zx ≥r dr = 1−P Zx <r dr,
(m) (m) (m)
0 0
(cid:110) (cid:111)
and we observe that Zx <r = {N(x,r)≥m} with N(x,r) := ♯{1≤m≤M|∥X −x∥<r}. We
(m) m
hence have
(cid:104) (cid:105) (cid:90) 1
E Zx = (1−P[N(x,r)≥m])dr.
(m)
0
Since N(x,r) ∼ Binomial(M,ξ(B(x,r))) and ξ(B(x,r)) ≥ cλX(B(x,r)) ≥ crdX by Assumption 3 (ii), we
2dX
obtain that P[N(x,r)≥m] ≥ P[N′(x,r)≥m] with N′(x,r) ∼ Binomial(M,crdX) due to Lemma 17. This
2dX
implies
(cid:104) (cid:105) (cid:90) 1 (cid:90) 1
E Zx ≤ (1−P[N′(x,r)≥m])dr = P[N′(x,r)<m]dr
(m)
0 0
=m (cid:88)−1(cid:18) M(cid:19)(cid:90) 1(cid:18) crdX(cid:19)j(cid:18) 1−crdX(cid:19)M−j
dr = 2
m (cid:88)−1(cid:18) M(cid:19)(cid:90) 2dc
X rd1 X+j−1(1−r)M−jdr
j=0
j
0
2dX 2dX cd1
XdX j=0
j
0
≤
2 m (cid:88)−1 Γ(M +1) Γ( d1
X
+j)Γ(M −j+1)
cd1 XdX
j=0
Γ(j+1)Γ(M −j+1) Γ( d1
X
+M +1)
=
2M! m (cid:88)−1Γ( d1
X
+j)
, (26)
cd1 XdXΓ( d1
X
+M +1)
j=0
j!
and the proof is over.
We are now in position to prove Theorem 10.
Proof of Theorem 10. By combining Lemma 19 and Lemma 20, noting that the upper bound is constant in
x, we have
supE(cid:2) W(cid:0)
P ,µˆ
(cid:1)(cid:3)
≤R(k)+
L 2M! (cid:88)k m (cid:88)−1Γ(j+ d1 X)
. (27)
x∈X x Nk(x) k cd1 XdXΓ(M + d1
X
+1)
m=1 j=0
j!
Below we only investigate the rate of the right hand side of (27) as M → ∞, and do not keep track of the
constant. We first analyze the second term in the right hand side of (27). By Gautschi’s inequality [Mer08,
Eqs (10.6) and (12.2)], we have
Γ(j+ 1 ) Γ(j+ 1 )
dX = dX ≤jd1 X−1, j ∈{0}∪N.
j! Γ(j+1)
Thus,
(cid:88)k m (cid:88)−1Γ(j+ d1 X)
≤
(cid:88)k m (cid:88)−1
jd1 X−1 ≲
(cid:88)k
md1 X ≲k1+ d1 X.
j!
m=1 j=0 m=1 j=0 m=1
By Gautschi’s inequality again, we have
M! = Γ(M +1) ≤M− d1 X.
Γ(M + 1 +1) Γ(M + 1 +1)
dX dX
The above implies
supE(cid:2) W(cid:0)
P x,µˆ
Nk(x)(cid:1)(cid:3)≲R(k)+M− d1 Xkd1
X.
x∈X
We will split the remainder of the proof into three cases.
28• For dY =1, by letting k−1 2 ∼M− d1 Xkd1 X, we yield
k
∼MdX2
+2 and
supE(cid:2) W(cid:0)
P x,µˆ
Nk(x)(cid:1)(cid:3)≲M− dX1
+2
x∈X
• For dY =2, since the explicit solution of k− 21 lnk ∼M− d1 Xkd1 X is elusive, we simply follow the configu-
ration derived in the case of dY =1 and yield
k
∼MdX2
+2 and
supE(cid:2) W(cid:0)
P x,µˆ
Nk(x)(cid:1)(cid:3)≲M− dX1
+2 lnM.
x∈X
• For dY ≥3, by letting k− d1 Y ∼M− d1 Xkd1 X, we yield
k
∼MdXd +Y
dY and
supE(cid:2) W(cid:0)
P x,µˆ
Nk(x)(cid:1)(cid:3)≲M− dX+1
dY.
x∈X
The proof is complete.
4.5 Proof of Theorem 11
Proof of Theorem 11. We will proceed by using Efron-Stein inequality. Let (X′,Y′) be an independent copy
1 1
of(X ,Y ),anddefineD′ :={(X′,Y′),(X ,Y ),...,(X ,Y )}. InviewofAssumption3(i),bythetriangle
1 1 1 1 2 2 M M
inequality of W, it is sufficient to investigate
1
(cid:34)(cid:18)(cid:90)
(cid:16) (cid:17)
(cid:19)2(cid:35)
ME W µˆD ,µˆD′ (x) ν(dx) .
2
X
Nk,DX(x) Nk,DX′
(cid:16) (cid:17)
Note that for W µˆD ,µˆD′ (x) to be positive, the event A ∪A′ is necessary, where
Nk,DX(x) Nk,DX′ x x
A :=(cid:8) X ∈Nk,DX(x)(cid:9) and A′ :=(cid:8) X′ ∈Nk,DX(x)(cid:9) .
x 1 x 1
Moreover,
(cid:16) (cid:17) 1
W µˆD ,µˆD′ (x) ≤ .
Nk,DX(x) Nk,DX′ k
It follows that
(cid:34)(cid:18)(cid:90)
(cid:16) (cid:17)
(cid:19)2(cid:35)
1
(cid:34)(cid:18)(cid:90) (cid:19)2(cid:35)
E W µˆD ,µˆD′ (x) ν(dx) ≤ E 1 ν(dx) (28)
X Nk,DX(x) Nk,DX′ k2 X Ax∪A′ x
1 (cid:20)(cid:90) (cid:21) 2 (cid:90)
≤ E 1 ν(dx) ≤ P[A ]ν(dx).
k2 X Ax∪A′ x k2 X x
where the second inequality is due to the fact that the integral value always fall into in [0,1], and we have
used Fubini-Tonelli theorem and the subadditivity of probability in the third inequality. Regarding P[A ],
x
by the symmetry stemming from Assumption 3 (i) and the random tie-breaking rule in Definition 9, we have
(cid:18)
M
−1(cid:19)(cid:18) M(cid:19)−1
k
P[A ]= = .
x k−1 k M
Consequently,
(cid:34)(cid:18)(cid:90)
(cid:16) (cid:17)
(cid:19)2(cid:35)
2
ME W µˆD ,µˆD′ (x) ν(dx) ≤ .
X
Nk,DX(x) Nk,DX′ k
29Invoking Efron-Stein inequality, we conclude the proof of (5).
We now assume additionally that ν ≤CλX to prove the second statement. Following from (28), by using
the positivity and subadditivity of indicator functions as well as AM–GM inequality, we have
(cid:34)(cid:18)(cid:90)
(cid:16) (cid:17)
(cid:19)2(cid:35)
4
(cid:34)(cid:18)(cid:90) (cid:19)2(cid:35) 4C2 (cid:34)(cid:18)(cid:90) (cid:19)2(cid:35)
E XW µˆD Nk,DX(x),µˆD N′ k,DX′(x) ν(dx) ≤ k2E X1 Axν(dx) ≤
k2
E X1 AxλX(dx)
4C2 (cid:90) (cid:34)(cid:18)(cid:90) (cid:19)2 (cid:35)
≤
k2
P 1 AxλX(dx) >δ dδ,
[0,1] X
whereinthesecondinequalitywehaveusedtheconditionthatν isdominatedbyλX, andinthelastonethe
alternative expression of expectation for positive random variables. Let Cubeι be the set of cubes within X
X
with edge length ι. Since ν is dominated by λX, with probability 1 we have
(cid:110) (cid:111)
A = at most (k−1) of X ,ℓ=2,...,M, falls into B∥X1−x∥∞ ,
x ℓ x
A′
x
=(cid:110)
at most (k−1) of X ℓ,ℓ=2,...,M, falls into B x∥X
1′−x∥∞(cid:111)
.
It follows that
(cid:40) M (cid:41) (cid:26)(cid:90) (cid:27)
(cid:88)
1 (X )>k, ∀B ∈Cubeι ⊆ 1 λ(dx)≤(2ι)dX .
B m X Ax
X
m=2
By combining the above and setting δ =(2ι)2dX, we yield
E(cid:34)(cid:18)(cid:90) W(cid:16)
µˆD ,µˆD′
(x)(cid:17) ν(dx)(cid:19)2(cid:35)
≤
4C2 (cid:90) P(cid:34) 1 (cid:88)M
1 (X )≤
k
, ∀B
∈Cube21δd1 X(cid:35)
dδ
X
Nk,DX(x) Nk,DX′ k2
[0,1]
M −1
m=2
B m M −1 X
(29)
Inordertoproceed,westateandproveausefultechnicallemmausingtheRademachercomplexitytechnique
(cf. [Wai19, Section 4]). Below we let CubeX be the set of cubes inside X with edge lengths within [0,1].
Lemma 21. Let X ,...,X be introduced in Assumption 3 (i). For ε≥0,
2 M
(cid:34) M (cid:114) (cid:35) (cid:18) (cid:19)
P
M1
−1
(cid:88)
1 B(X m)≤cλX(B)−8
2d MXln −(M
1
)
−ε, ∀B ∈CubeX ≤exp
−M 2−1
ε2 .
m=2
Proof. Let xM = (xM,...,xM) ∈ XM−1. To utilize the machinery of Rademacher complexity, we will
2 M
upper bound the cardinality of the set {1 B(xM) : B ∈ CubeX}, where 1
B
applies entry-wise. More pre-
cisely, 1 B(xM) = (1 B(xM
2
),...,1 B(xM M)). To start with, we first note that for d = 1,...,dX, the projected
(xM ,...,xM ) at most separates axis-d into M intervals. Additionally, each element in {1 (xM) : B ∈
2,d M,d B
CubeX} corresponds to selecting two intervals (one for starting and one for ending of the cube) on each axis.
Therefore, the cardinality is at most M2dX, i.e., CubeX has polynomial discrimination 2dX. It follows from
[Wai19, Lemma 4.14 and Theorem 4.10] that, for any ε≥0,
P(cid:34)
sup
(cid:12) (cid:12)
(cid:12)
1 (cid:88)M
1 (X
)−ξ(B)(cid:12) (cid:12) (cid:12)≥8(cid:114) 2dXln(M) +ε(cid:35) ≤exp(cid:18) −M −1 ε2(cid:19)
.
(cid:12)M −1 B m (cid:12) M −1 2
B∈CubeX(cid:12)
m=2
(cid:12)
Finally, in view of Assumption 3 (ii), we conclude the proof of Lemma 21.
In view of (29) and Lemma 21, for δ ∈[0,1], we consider ε≥0 such that
k cδ1 2 (cid:114) 2dXln(M)
= −8 −ε.
M −1 2dX M −1
30(cid:18) (cid:113) (cid:19)2
Note that this is feasible only if 4dX 8 2dXln(M) + k ≤1.4 It follows that
c2 M−1 M−1
P(cid:34) 1 (cid:88)M 1 (X )≤ k , ∀B ∈Cube1 2δ2d1 X(cid:35)
M −1 B m M −1 X
m=2
 (cid:34) (cid:18) (cid:113) (cid:19)2(cid:35)
1, δ ∈ 0,4 cd 2X 8 2dX Mln −( 1M) + Mk
−1
,
≤ exp(cid:32)
−M
2−1(cid:18)
c 2δ
dX1
2
−8(cid:113)
2dX Mln −( 1M) − Mk
−1(cid:19)2(cid:33)
, δ
∈(cid:32)
2d cX
(cid:18) 8(cid:113)
2dX Mln −( 1M) + Mk
−1(cid:19)2 ,1(cid:35)
.
The above together with (29) implies
1
(cid:34)(cid:18)(cid:90)
(cid:16) (cid:17)
(cid:19)2(cid:35)
ME W µˆD ,µˆD′ (x) ν(dx)
2
X
Nk,DX(x) Nk,DX′
2C2 M(cid:32) 4dX (cid:32) (cid:114) 2dXln(M) k (cid:33)2
≤ 8 +
k2 c2 M −1 M −1
(cid:90) 1  M −1(cid:32) cη (cid:114) 2dXln(M) k (cid:33)2 (cid:33)
+
2dX(cid:18) 8(cid:113) 2dXln(M)+ k
(cid:19)exp−
2 2dX
−8
M −1
−
M −1
2ηdη ,
c M−1 M−1
where we have performed a change of variable η = δ21 in the last line. Relating to exponential and normal
density functions, we calculate the integral to obtain
1
(cid:34)(cid:18)(cid:90)
(cid:16) (cid:17)
(cid:19)2(cid:35)
ME W µˆD ,µˆD′ (x) ν(dx)
2
X
Nk,DX(x) Nk,DX′
2C2
M
4dX(cid:32)(cid:32) (cid:114)
2dXln(M) k
(cid:33)2 √
2π
(cid:32) (cid:114)
2dXln(M) k
(cid:33)
4
(cid:33)
≤ 8 + + √ 8 + + ,
k2 c2 M −1 M −1 M −1 M −1 M −1 M −1
(cid:32)(cid:18)√ (cid:19)2 (cid:18)√ (cid:19) (cid:33)
wherewenotetherighthandsideisofO ln(M) + √1 + 1 ln(M) + √1 + 1 . InvokingEfron-
k M k k M k2
Stein inequality, we conclude the proof.
4.6 Proof of Proposition 13
Proof of Proposition 13. By triangle inequality,
(cid:20)(cid:90) (cid:21)
E W(P ,P˜Θ)dx
x x
X
≤E(cid:34) (cid:90)
XW(P x,P
xΘ)(cid:32)
λX−
N1 (cid:88)N
δ
X˜
n(cid:33) (dx)(cid:35) +E(cid:34) N1 (cid:88)N
W(P
X˜
n,P
X˜
n)(cid:35) +E(cid:34) N1 (cid:88)N
W(P X(cid:101)n,P˜ XΘ
˜
n)(cid:35)
.
n=1 n=1 n=1
Then, by Assumption 2 and (7),
(cid:34) (cid:90) (cid:32) 1 (cid:88)N (cid:33) (cid:35) (cid:34) (cid:32) 1 (cid:88)N (cid:33)(cid:35)
E XW(P x,P xΘ) λX−
N
δ
X˜
n
(dx) ≤E (L+LΘ)W λX,
N
δ
X˜
n
.
n=1 n=1
4WedonotincludethisconditioninthestatementofTheorem11,astheboundpresentedremainsvalid,albeitvacuous,if
thisconditionisnotmet.
31In view of Assumption 12, we have
E(cid:34) 1 (cid:88)N
W(P ,P
)(cid:35)
=
1 (cid:88)N E(cid:104) E(cid:104)
W(P ,P
)(cid:12)
(cid:12)X˜
(cid:105)(cid:105) =E(cid:20)(cid:90)
W(P ,P
)dx(cid:21)
.
N X˜ n X˜ n N X˜ n X˜ n (cid:12) n X x x
n=1 n=1
Combining the above, we prove the first statement.
As for the second statement, consider Q,Q′ : X → P(Y) that are Lipschitz-continuous with constants
L,L′. Suppose that
W(Q ,Q′ )=supW(Q ,Q′)=δ
x∗ x∗ x x
x∈X
for some δ >0 and x∗ ∈X. This supremum is indeed attainable because X is compact that x(cid:55)→W(Q ,Q′)
x x
is continuous. Consequently, by triangle inequality and the Lipschitz-continuity, we have
(cid:16) (cid:17) (cid:16) (cid:17)
W(Q ,Q′)≥ W(Q ,Q′ )−W(Q′,Q′ ) ∨0≥ W(Q ,Q′ )−W(Q ,Q )−W(Q′ ,Q′) ∨0
x x x x∗ x x∗ x∗ x∗ x∗ x x∗ x
(cid:16) (cid:17)
≥ δ−(L+L′)∥x−x∗∥ ∨0, x∈X.
∞
We may then lower bound (cid:82) W(Q ,Q′)dx with the volume of the cone on right hand side above (note the
X x x
worst case is when x∗ =(0,0)),
(cid:90) (cid:90) δ(cid:18) δ−z (cid:19)dX δdX+1
W(Q ,Q′)dx≥ dz = .
X
x x
0
L+L′ (dX+1)(L+L′)dX
It follows that
(cid:18)(cid:90) (cid:19) 1
supW(Q x,Q′ x)≤(dX+1)dX1 +1(L+L′)dXd +X 1 W(Q x,Q′ x)dx dX+1 ,
x∈X X
which completes the proof.
5 Implementation details and ablation analysis
Inthissection,wewillprovidefurtherimplementationdetailsandconductablationanalysisofthecomponents
highlighted in Section 3.1.
5.1 Comparing ANNS-RBSP to exact NNS
Algorithm 3 outlines a single slice of RBSP, which divides an array of x’s into two arrays of a random
ratio along a random axis. Throughout the training, we execute RBSP 5 times during each training epoch,
yielding 25 = 32 parts. Within each part, we then select a small batch of 8 query points, locating the k
nearest neighbors for each query point within the same part. In Table 1, we compare the execution times of
exact NNS and ANNS-RBSP. ANNS-RBSP offers considerable time savings for M = 106, while exact NNS
is more efficient for M =105 or fewer.
It’simportanttonotethatANNS-RBSPmayintroduceadditionalerrorsbyinaccuratelyincludingpoints
that are not within the k nearest neighbors. As elucidated in the proof of Theorem 10, the magnitude of
thisinducederrorcanbeunderstoodbycomparingtheexcessivedistanceincurredtothatofexactNNS.For
simplicity, we investigate the difference below
 
∆:= N
b1
atch
N (cid:88)batch

k1 (cid:88)k (cid:13)
(cid:13) (cid:13)Xˇ i′ j −X˜
i(cid:13)
(cid:13) (cid:13) 1−
k1 (cid:88)k (cid:13)
(cid:13) (cid:13)Xˇ ij −X˜
i(cid:13)
(cid:13) (cid:13) 1,
i=1 j=1 j=1
32Algorithm 3 Single slice of random binary space partitioning
Input: dataDX=(xi)M i=1⊂[0,1]dX,arraysofindexesS d,d=1,...,dX oflengthM withthej-thentryindicatingtheposition
ofthej-thsmallestvalueinthed-thdimensionofDX,abooleanarrayBoflengthM withthei-thentryindicatingwhether
xi isinvolvedinthecurrentslicing,arectangleRthatboundsxi’sinvolvedinthecurrentslicing,i.e.,RcorrespondstoB,
aparameterr ∈(1,∞)foravoidingthinrectangles,aninterval[p,p]∈(0,1)forrandombisectingratio
edge
Output: twobooleanarraysB,B′ oflengthM indicatingthebisecteddata,twoboundingrectanglesR,R′ thatcorrespondto
B,B′
1: Randomlypickadimensiond
2: if TheedgeratioofRexceedsr then
edge
3: Replacedwiththatcorrespondstothelongestedge
4: endif
5: RearrangeBaccordingtoS byB˜ ←B[S ]
d d
6: PickouttheindexesfromS involvedinANNSbyS˜ ←S [B˜]
d d d
7: Generatep∼Uniform([p,p])androundpintop˜sothatp˜len(S˜ )isaninteger
d
8: BisectS˜ intwoarrayswithlengthp˜len(S˜ )and(1−p˜)len(S˜ ),denotedbyS˜ andS˜′
d d d d d
9: FormnewboundingrectanglesR,R′ usingS˜ d,S˜′ d,DX andtheoriginalR(mayenforcesomeoverlaphere)
10: InitializetwobooleanarraysB,B′ withlengthM andallentriesbeingFalse
11: B[S˜ ]←True,B′[S˜′]←True
d d
12: return B,B′,R,R′
Table 1 Execution times of two NNS methods.
M =104 M =105 M =106
dX =1 (0.03,2.2) (0.5,2.5) (9.4,2.7)
dX =3 (0.04,2.2) (0.6,2.7) (12.8,2.8)
dX =10 (0.07,2.2) (0.8,2.7) (26.8,3.4)
This table compares the execution times for 100 runs of exact NNS versus ANNS-RBSP, both utilizing parallel
computing, facilitated by PyTorch, with an NVIDIA L4 GPU. Each iteration (approximately) finds the 300 nearest
neighbors from M samples for all of 256 randomly generated query points. The values within each parenthesis
denote the seconds consumed by both methods, with the first number corresponding to exact NNS. For faster
processing, exact NNS employs a 3D tensor, except in the case of M =106,dX =10, where memory limitations
necessitate using a for-loop over individual query points. ANNS-RBSP regenerates a new partition each run. The
table does not include the time required to sort the data along all dimensions, which takes about 0.2 seconds in the
worst case and is not repeatedly executed.
where X˜ ’s are query points, and Xˇ ,Xˇ′ are the k-nearest-neighbor identified by exact NNS and ANNS-
i ij ij
RBSP, respectively. In our experiments, we evaluated scenarios with dX = 3,10 and k = 300. Regarding
the data, we generated M = 104,105,106 samples from Uniform([0,1]dX). Once the data set is generated,
we fixed the data and conducted 100 simulations of ∆, each with N = 256 query points. This process
batch
was repeated 10 times, each with a separately generated data. The results are illustrated in Figure 8. It
is expected that ∆ will approach 0 as the sample size M tends to infinity. The convergence rate is likely
influenced by factors such as dX, k, and N batch. Further analysis of the convergence of ANNS-RBSP will be
conducted in future studies.
5.2 An implementation of the Sinkhorn algorithm
In this section, we will detail our implementation of the Sinkhorn algorithm and highlight a few novel
treatments that seem to enhance the training of the neural estimator. While the mechanisms are not yet
fully understood, they constitute important improvement in the accuracy of the neural estimator.
Let us first recall the iterative procedure involved in the Sinkhorn algorithm. We follow the setup in
Section 3.1.2. In particular, the row indexes of the cost matrix stand for atoms in the empirical measures,
while the column indexes stand for atoms produced by the neural estimator. We set N = k and let
atom
u(0),v(0) be column vectors of size k with all entries being k−1. We will suppress the dependence on y from
33(a) dX =3 (b) dX =10
Figure 8: Empirical CDFs of ∆.
We compare the empirical CDFs of ∆. Each line corresponding to a independently generated set of data. Each plot
includes 10 empirical CDFs. Note the difference in the x axis scale.
34
selpmas501
selpmas601the notation. Upon setting
(cid:18) (cid:19)
C
Kϵ :=exp −
ϵ
with entry-wise exponential, the Sinkhorn algorithm performs repeatedly
u(0) v(0)
u(ℓ+1) = and v(ℓ+1) = , (30)
Kϵv(ℓ) (Kϵ)⊤u(ℓ+1)
where the division is also calculated entry-wise. After a certain number of iterations, denoted as N , we
iter
obtain an approximate optimal transport plan for problem (15):
Tϵ =diag(u(Niter))Kϵdiag(v(Niter)).
Let us set ϵ = 1 momentarily. Note that if the entries of C are excessively large, K effectively becomes a
zero matrix, which impedes the computations in (30). This issue may occur at the initiation of the neural
estimator or during training, possibly due to the use of stochastic gradient descent. To tackle this issue, we
employ a rule-of-thump normalization that
(cid:18) (cid:19)
C
K˜ϵ :=exp − with c˜:=minmaxC , (31)
c˜ϵ i j ij
and use K˜ϵ instead of Kϵ in (30). Regarding the selection of ϵ and the number of iterations, we currently
lack a method for adaptively determining these values. Instead, we adjust them manually based on training
episodes. This manual adjustment works well for all models discussed in this paper. For more information,
please see Appendix B.
As alluded in Section 3.1.2, we enforce sparsity on the transport plan to improve the performance of the
neural estimator. Let T˜ϵ be the output of the Sinkhorn algorithm. We construct Tˆϵ and Tˇϵ by setting the
row-wise and column-wise maximum of T˜ϵ to k−1, respectively, and setting the remaining entries to 0. We
then use
Tϵ =γTˆϵ+(1−γ)Tˇϵ, (32)
where γ ∈ [0,1] is a hyper-parameter, in gradient descent (14). We observe that Tˆϵ relates each atom in
the empirical measure to a single corresponding atom from the neural estimator, and Tˇϵ does the same in
reverse. The optimal choice of γ remains an open question, though we have set γ =0.5 in all three models.
Next, we explore the impact of enforcing sparsity and varying the choices of γ. Figure 9 compares
the performance in Model 1 under different sparsity parameters. When no sparsity is enforced, the neural
estimator tend to handles singularities more adeptly, but may overlooks points located on the periphery of
the empirical joint distribution, potentially resulting in overly concentrated atoms from the neural estimator
(see around x = 0.1,0.9). Compare Figure 4 and 10 for the extra error due to the lack of enforced sparsity.
This phenomenon is more noticeable in Model 3. We refer to panel (2,3) of Figure 17 in Appendix A for
an example. Moreover, Figure 18, which is obtained without enforced sparsity, indicates a downgrade in
accuracy when compared to Figure 6.
ϵ
Finally, it is not recommended to use T at the early stages of training, as our empirical experiments
suggest this could deteriorates performance. In training, we start by not enforcing sparsity and then begin
to enforce it in later episodes. We refer to Appendix B for further details of the training configuration.
5.3 More on LipNet
We will investigates the impact of various hyper-parameters on the performance of LipNet. The LipNets
presented in this section are trained with the same hyper-parameters as in Section 3.2 (see also Appendix
B), expect for those specified otherwise.
35(a) No sparsity enforced (b) γ =0 (c) γ =0.5 (d) γ =1
Figure 9: LipNet under Model 1 with different sparsity enforcement.
Figure 10: Errors at different x’s of various estimators under Model 1, LipNet is trained without enforced
sparsity.
We compute the W-distance between estimators and the true conditional distribution at different x’s. The setting is
similar to Figure 4, but LipNet is trained without enforcing sparsity on the transport plan. The errors of LipNet at
around x=0.1,0.9 are slightly higher than those in Figure 4.
365.3.1 Activation function
Switching the activation function from ELU to Rectified Linear Unit (ReLU) appears to retain the adaptive
continuity property. In Figure 11, we illustrate the joint distribution and the average absolute derivatives
of all atoms of LipNet with ReLU activation. The outcomes are on par with those achieved using ELU
activation as shown in Figure 2.
(a) Model 1, all atoms (b) Model 2, ave.abs.der. (c) Model 2, all atoms (d) Model 2, ave.abs.der.
Figure 11: LipNet under Model 1 with ReLU activation.
5.3.2 Value of L in (18)
Note that the LipNets discussed in Section 3.2 were trained with L = 0.1. If the normalizing constants in
LipNet are exactly computed, L reflects the Lipschitz constant of LipNet, upto the discrepancy in the choice
of norms in different layers. The effect of L in our implementation, however, is rather obscure. Figure 12
showcases the performance of LipNets across various L values in Model 1. The comparison in Model 2 is
presented in Figure 19 in Appendix A. The best choice of L appears to depend on the ground truth model.
For Model 3, we compared the performance of L = 0.1 and L = 1 and observed no significant differences.
Generally, we prefer a smaller L; however, smaller values of L tend to exhibit greater sensitivity to other
training parameters. For instance, in Model 3, with L = 0.1, starting enforcing sparsity too soon leads to
significantly poorer performance, while the impact on the outcomes for L=1 is much less noticeable.
(a) L=0.01 (b) L=0.03 (c) L=1 (d) L=3
Figure 12: LipNet under Model 1 with various L’s.
37
derettacs
smota
llA
smota
02
fo.jarT5.3.3 Momentum τ in Algorithm 2
In our training of LipNet, we use τ = 10−3. Figure 13 demonstrates the impact of various τ values on
neural estimator’s performance in Model 1. It is clear that the performance declines with a τ that is too
large. While we initially speculated that a smaller τ might cause atoms to exhibit more erratic movements
as x changes, observations contradict this hypothesis. We now believe that a suitable τ value helps prevent
neurons from stagnating in the plateau region of the ELU activation function. This is supported by the
outcomes observed with τ = 10−6, where atom movements are overly simplistic. Additional comparisons in
Model 2 are presented in Figure 20.
(a) τ =10−1 (b) τ =10−2 (c) τ =10−5 (d) τ =10−6
Figure 13: LipNet under Model 1 with various τ’s.
Despiteconsideringasapotentialimprovementtheinclusionofbatchnormalizationintheconvexpoten-
tiallayer(16),rightaftertheaffinetransformation,alongwithacorrespondingoffsetinthepositionof∥W∥ ,
2
our experiments with both ELU and ReLU activations, using the default batch normalization momentum of
0.1, resulted in reduced performance. Lowering said batch normalization momentum often leads to a NaN
network.
6 Weakness and potential improvement
Inthissection,weprovidesomediscussionontheweaknessandpossibleimprovementofourimplementation
in Section 3.
Extra correction. In more realistic scenarios, the true conditional distribution is often unknown or
intractable. Insuchcases,itisunclearwhetheraneuralestimatoroffersextracorrectionoverrawestimators.
A potential solution to this issue is to train StdNet and LipNet simultaneously. If StdNet and LipNet align
more closely with each other than with the raw estimator involved in their training, it is possible that the
neural estimators are providing extra corrections.
Hyper-parameters for Sinkhorn algorithm. OurimplementationoftheSinkhornalgorithminvolves
several hyper-parameters: (i) k in Definition 9; (ii) N in (10); (iii) ϵ in (31); (iv) γ in (32); and (v)
atom
additional hyper-parameters listed in Section B. The impact of these hyper-parameters is not yet fully un-
derstood. Additionally, an adaptive ϵ that balances the accuracy and stability of the Sinkhorn iteration is
desirable. Furthermore,asillustratedinSection5.2,enforcingsparsityonthetransportplangenerallyyields
38
derettacs
smota
llA
smota
02
fo.jarTbetter approximations at x where the conditional distribution is more diffusive, but may performs worse
where the conditional distribution exhibits atoms. This observation motivates further investigation into a
sparsity policy that adjusts according to the indications from the raw estimator.
Adaptive continuity. The impact of hyper-parameters in LipNet also warrants further investigation.
In addition, despite the results presented in this study, more evidence is needed to understand how LipNet
and its variations perform under various conditions.
Scalability. While the implementation produces satisfactory results when M and k are relatively small
(recall that we set N = k), our further experiments indicate a scalability bottleneck. For example, in
atom
Model 1, significantly increasing M and k does not necessarily improve the performance of neural estimators
in a comparable manner. To address this issue, we could experiment with varying the ratios between N
atoms
and k, rather than setting them equal, in hopes of reducing the strain on the Sinkhorn algorithm. We note
that varying the ratio between N and k requires adjusting the enforced sparsity accordingly. Another
atoms
issuerelatestothedimensionsofXandY. InviewofthecurseofdimensionalityinTheorem10,ourmethod
is inherently suited for low-dimensional settings. Fortunately, in many practical scenarios, the data exhibits
low-dimensional structures, such as: (i) the sampling distribution of X concentrating on a low-dimensional
manifold; and (ii) the mapping x (cid:55)→ P exhibiting low-dimensional dependence. For (i), we might resort to
x
dimension reduction techniques, although an extension of the results in Section 2 has yet to be established.
For (ii), a data-driven method that effectively leverages the low-dimensional dependence is of significant
interest.
Conditional generative models. Utilizing a conditional generative model could potentially lead to
further improvements. One advantage of conditional generative models is the ease of incorporating various
training objectives. For instance, it can easily adapt to the training objectives in (6) to accommodate
multiple different hyper-parameters simultaneously. We may also incorporate the joint empirical measure in
the training process. This flexibility also allows for the integration of specific tail conditions as needed.
Lastly, we would like to point out an issue observed in our preliminary experiments when utilizing a
na¨ıveconditionalgenerativemodel: itmayassignexcessiveprobabilitymasstotheblankregionbetweentwo
distinct clusters (for example, in Model 1 around (x,y)=(0.1,0.5)). This possibly stems from the inherent
continuity of neural networks. One possible solution is to consider using a mixture of multiple conditional
generative models.
39A Additional plots
Figure 14: Various estimators under Model 3, projections of conditional CDFs, k =1000 for k-NN
estimator.
This follows the setting of Figure 5, expect that we set k=1000 for the k-NN estimator plotted here. The neural
estimator is trained under the same setting as that in Figure 5 Plot titles display the vectors used for projection.
Note the difference in the x axis scale.
40Figure 15: Various estimators under Model 3, projections of conditional CDFs, k =3000 for k-NN
estimator.
This follows the setting of Figure 5, expect that we set k=3000 for the k-NN estimator plotted here. The neural
estimator is trained under the same setting as that in Figure 5 Plot titles display the vectors used for projection.
Note the difference in the x axis scale.
41Figure 16: Various estimators under Model 3, projections of conditional CDFs, k =104 for k-NN
estimator.
This follows the setting of Figure 5, expect that we set k=104 for the k-NN estimator plotted here. The neural
estimator is trained under the same setting as that in Figure 5 Plot titles display the vectors used for projection.
Note the difference in the x axis scale.
42Figure 17: Various estimators under Model 3, projections of conditional CDFs, both StdNet and Lipnet
are trained without enforced sparsity.
ThisfollowsthesettingofFigure5, expectthatwedonotenforcesparsityonthetransportplanduringthetraining
of the neural estimator. Plot titles display the vectors used for projection. Note the difference in the x axis scale.
Raw k=1000 Raw k=3,000 StdNet LipNet
Figure 18: Histogram of 10,000 projected Wasserstein-1 errors, no enforced sparsity on transport plan.
This follows the setting of Figure 6, expect that we do not enforce sparsity on the transport plan during the
training of the neural estimator. Histograms for raw estimators remain the same. The histogram consists of
20 uniformly positioned bins between 0 to 0.1. The errors of different estimators are computed with the
same set of query points and projection vectors. Errors larger than 0.1 will be placed in the right-most bins.
43(a) L=0.01 (b) L=0.03 (c) L=1 (d) L=3
Figure 19: LipNet under Model 2 with various L’s.
(a) τ =10−1 (b) τ =10−2 (c) τ =10−5 (d) τ =10−6
Figure 20: LipNet under Model 2 with various τ’s.
44
derettacs
smota
llA
smota
02
fo.jarT
derettacs
smota
llA
smota
02
fo.jarTB Configuration of network components and training param-
eters
The table below summarizes the configuration of the neural network and the training procedure. It
applies to both StdNet and LipNet in all models.
Network Component
Configuration Note
/Training parameters
Samplesize 1e4forModel1&2,1e6forModel3
k 100forModel1&2,300forModel3 SeeDefinition9
Layer-wiseresidualconnection[HZSS16],
StdNet: batchnormalization[IS15]after
Networkstucture affinetransformation
Layer-wiseresidualconnection[HZSS16]
LipNet:
withconvexpotentiallayer[MDAA22]
Inputdimension dX
Outputdimension dY×N
atom
N atom=k,see(10)
Numberofhiddenlayers 5
Numberofneurons
2k k asinDefinition9
eachhiddenlayer
StdNet: ReLU
Activationfunction SeeSection5.3.1
LipNet: ELU
L 0.1 Introducedin(18)
τ 1e-3 SeeAlgorithm2
Learningrateis0.01
Optimizer Adam[KB17]withlearningrate10−3
forStdNetinModel1&2
100forModel1&2
Batchsize
256forModel3
Numberofepisodes 5e3forModel1&2,1e4forModel3
RBSPsetting 25 partition,8querypointseachpart SeeSection3.1.1
IntroducedinSection3.1.1
Randombisectingratio ∼Uniform([0.45,0.55])
SeealsoAlgorithm3
Ratioformandatoryslicing IntroducedinSection3.1.1
5
alongthelongestedge Seealsor edge inAlgorithm3
5,ifepoch≤500
NumberofSinkhorniterations
10,ifepoch>500
1,ifepoch≤100
ϵ 0.1,ifepoch∈[100,500] Introducedin(31)
0.05,ifepoch>500
Off,ifepoch≤500
Enforcedsparsity SeeSection5.2
On,ifepoch>500
γ 0.5 Introducedin(32)
C Another set of results on fluctuation
C.1 On r-box estimator
Theorem 22. Under Assumptions 2 and 3, and choosing r as in Theorem 7, let ν ∈P(X) be dominated by
λX with constant C >0. Then, there is a constant C >0 (which depends only on dX,c, C and the constants
45involved in r), such that, for any ε≥0, we have
(cid:20)(cid:90) (cid:16) (cid:17) (cid:20)(cid:90) (cid:16) (cid:17) (cid:21) (cid:21)
 exp(cid:16) −CMdX2 +2ε2(cid:17)
, dY =1,2,
P W P ,Pˆr dν(x)≥E W P ,Pˆr dν(x) +ε ≤ (cid:18) (cid:19)
x x x x dX
X X exp −CMdX+dYε2 , dY ≥3.
Proof. Let ν ∈P(X) as in the statement of the Theorem. We define
(cid:90)
Z := W(P ,Pˆr)dν(x),
x x
X
and introduce the following discrete time filtration: F := {∅,Ω} and F := σ((cid:83)m σ(X ,Y )) for m =
0 m i=1 i i
1,...,M. We consider the Doob’s martingale Z :=E[Z|F ], m=1,...,M. Note that Z =Z. We will
m m M
apply Azuma-Hoeffding inequality (cf. [Wai19, Corollary 2.20]) to complete the proof.
Let us define
Dm :={(X ,Y ),...,(X ,Y ),(x ,y ),...,(x ,y )}, m=1,...,M, (33)
1 1 m m m+1 m+1 M M
D0 := {(x ,y )}M , and DM := D. Note that, for all m < M, we have, by Assumptions 3 (i), conditional
ℓ ℓ ℓ=0
Fubini-Tonelli theorem, and independent lemma,
Z
=(cid:90) (cid:90) W(cid:16)
P
,µˆDm
(cid:17) (cid:79)M
ψ(dx dy )ν(dx).
m x Br(x) ℓ ℓ
X (X×Y)M−m
ℓ=m+1
This together with the linearity of integral, the fact that ψ is a probability, and the triangular inequality of
W implies that for m=1,...,M,
|Z −Z
|≤(cid:90) (cid:90) W(cid:16)
µˆDm
,µˆDm−1(cid:17)(cid:79)M
ψ(dx dy )ν(dx). (34)
m m−1 Br(x) Br(x) ℓ ℓ
X (X×Y)M−m+1
ℓ=m
Notice that, by definitions (1) and (33),
(cid:110) (cid:111) (cid:110) (cid:111) (cid:110) (cid:111)
µˆDm ̸=µˆDm−1 ⊆ X ∈Br(x) ∪ x ∈Br(x) . (35)
Br(x) Br(x) m m
(cid:110) (cid:111)
Additionally, by definitions (1) and (33) again, on the event that µˆDm ̸=µˆDm−1 , we have
Br(x) Br(x)
(cid:32) m−1 M (cid:33)−1 (cid:32) M (cid:33)−1
W(cid:16) µˆDm ,µˆDm−1(cid:17) ≤ 1+ (cid:88) 1 (X )+ (cid:88) 1 (x ) ≤ 1+ (cid:88) 1 (x ) . (36)
Br(x) Br(x) Br(x) ℓ Br(x) ℓ Br(x) ℓ
ℓ=1 ℓ=m+1 ℓ=m+1
Combining (34),(35), (36), and Fubini-Tonelli theorem, we get
(cid:90) (cid:90) (cid:90) (cid:32) M (cid:33)−1 M
(cid:88) (cid:79)
|Z −Z |≤ 1+ 1 (x ) ξ(dx )ν(dx)ξ(dx )
m m−1 Br(x) ℓ ℓ m
X B(Xm,2r)∪B(xm,2r) XM+1−m ℓ=m+1 ℓ=m+1
(cid:90) (cid:90) (cid:32) M (cid:33)−1 M
(cid:88) (cid:79)
≤ sup 1+ 1 (x ) ξ(dx )ν(dx) (37)
Br(x) ℓ ℓ
xm∈X B(Xm,2r)∪B(xm,2r) XM+1−m ℓ=m+1 ℓ=m+1
where the 2r in the domain of the integral stems from the usage of βr in the definition of Br (see Definition
465). Now, for fixed x,x ∈X, we have
m
(cid:90) (cid:32) M (cid:33)−1 M
(cid:88) (cid:79)
1+ 1 (x ) ξ(dx )
Br(x) ℓ ℓ
XM−m+1
ℓ=m+1 ℓ=m+1
=M (cid:88)−m(cid:18) M −m(cid:19) ξ(cid:0) Br(x)(cid:1)ℓ(cid:16) 1−ξr(cid:0) Br(x)(cid:1)(cid:17)M−m−ℓ (1+ℓ)−1
ℓ
ℓ=0
=
1 M (cid:88)−m(cid:18) M −m+1(cid:19) ξ(cid:0) Br(x)(cid:1)ℓ+1(cid:16) 1−ξ(cid:0) Br(x)(cid:1)(cid:17)M−m−ℓ
(cid:0) (cid:1)
(M −m+1)ξ Br(x) ℓ+1
ℓ=0
=
1 M− (cid:88)m+1(cid:18) M −m+1(cid:19) ξ(cid:0) Br(x)(cid:1)ℓ(cid:16) 1−ξ(cid:0) Br(x)(cid:1)(cid:17)M−m+1−ℓ
(cid:0) (cid:1)
(M −m+1)ξ Br(x) ℓ
ℓ=1
1−(cid:16) 1−ξ(cid:0) Br(x)(cid:1)(cid:17)M−m+1
=
≤1∧(cid:0)
(M
−m+1)ξ(cid:0) Br(x)(cid:1)(cid:1)−1 ≤1∧(cid:0)
(M
−m+1)c(2r)dX(cid:1)−1
,
(cid:0) (cid:1)
(M −m+1)ξ Br(x)
where we have used Assumption 3 (ii) in the last inequality. Recall C introduced in Theorem 22. In view of
(37), we have
(cid:90)
|Z −Z |≤ sup
1∧(cid:0)
(M
−m+1)c(2r)dX(cid:1)−1
ν(dx)
m m−1
xm∈X B(Xm,2r)∪B(xm,2r)
≤2C(4r)dX(cid:16) 1∧(cid:0)
(M
−m+1)c(2r)dX(cid:1)−1(cid:17) =(cid:0) C22dX+1rdX(cid:1)
∧
C2dX+1
:=C .
c(M −m+1) m
By Azuma-Hoeffding inequality (cf. [Wai19, Corollary 2.20]), one obtains
(cid:32) (cid:33)
2ε2
P(Z−E[Z]≥ε)≤exp − .
(cid:80)M C2
m=1 m
Tocompletetheproof, wesubstituteintheconfigurationofTheorem7. Sinceweonlyaimtoinvestigatethe
rate of (cid:80)M C2 as M →∞, we simply set
m=1 m
r =M− dX1 +d with d:=2∨dY.
It follows that
dX
M M (cid:90) ∞ (cid:90) MdX+d (cid:90) ∞
(cid:88) C m2 ∼ (cid:88) M− d2 Xd +X d ∧m−2 ≲ M− d2 Xd +X d ∧z−2dz ∼ M− d2 Xd +X d dz+
dX
z−2dz ∼M− dXd +X d,
m=1 m=1 1 1 MdX+d
which completes the proof.
C.2 On k-nearest-neighbor estimator
Theorem 23. Under Assumptions 2 and 3, and the choice of k as in Theorem 10, there is a constant C >0
(which depends only on c and the constants involved in k), such that, for any ν ∈P(X) and ε≥0, we have
(cid:20)(cid:90) (cid:20)(cid:90) (cid:21) (cid:21)
 exp(cid:16) −CMdX2 +2ε2(cid:17)
, dY =1,2,
P W(cid:0) P ,Pˇk(cid:1) ν(dx)≥E W(cid:0) P ,Pˇk(cid:1) ν(dx) +ε ≤ (cid:18) (cid:19)
x x x x dY
X X exp −CMdX+dYε2 , dY ≥3.
47Proof of Theorem 23. For notational convenience, we will write µˆD for µˆD . Clearly, with D = D,
Nk(x) Nk,D(x)
we recover µˆD =µˆD =Pˇk. In what follows, we let
Nk(x) Nk,D(x) x
(cid:90)
Z :=
W(cid:0)
P
,Pˇk(cid:1)
ν(dx).
x x
X
We also define F :={∅,Ω} and F :=σ((cid:83)m σ(X ,Y )) for m=1,...,M. The proof relies on an applica-
0 m i=1 i i
tion of Azuma-Hoeffding inequality (cf. [Wai19, Corollary 2.20]) to the Doob’s martingale {E[Z|F ]}M .
m m=0
In order to proceed, we introduce a few more notations:
x:=(x ,...,x ), X :=(X ,...,X ),
1 M 1 M
Xm :=(X ,...,X ,x ,...,x ),
1 m m+1 M
Dm :={(X ,Y ),...,(X ,Y ),(x ,y ),...,(x ,y )},
1 1 m m m+1 m+1 M M
ηk,x := the k-th smallest of {∥x −x∥ }M .
x m ∞ m=1
By independence lemma, we have
E(cid:2) Z(cid:12) (cid:12)F m(cid:3) =(cid:90) (cid:90) W(cid:16) P x,µˆD Nm k(x)(cid:17) ν(dx) (cid:79)M ψ(dx ℓdy ℓ)
(X×Y)M−m X
ℓ=m+1
(cid:90) (cid:90) (cid:32) 1 (cid:32) (cid:88)m (cid:88)M (cid:33)(cid:33) (cid:79)M
=
(X×Y)M−m
XW P x,
k
i=11 ∥Xi−x∥∞≤ηxk,Xmδ
Yi
+ ℓ=m+11 ∥xℓ−x∥∞≤ηxk,Xmδ
yℓ
ν(dx) ℓ=m+1ψ(dx ℓdy ℓ),
where we note that (X×Y)M−m and (cid:78)M ψ(dx dy ) in the right hand side can be replaced by (X×
ℓ=m+1 ℓ ℓ
Y)M−m+1and(cid:78)M
ψ(dx dy )astheintegrandisconstantinx andψisaprobabilitymeasure. Therefore,
ℓ=m ℓ ℓ m
by Fubini’s theorem and triangle inequality for W, we have
(cid:12) (cid:12)E(cid:2) Z(cid:12) (cid:12)F m(cid:3) −E(cid:2) Z(cid:12) (cid:12)F m−1(cid:3)(cid:12)
(cid:12)
(cid:90) (cid:90) (cid:32) 1 (cid:32) (cid:88)m (cid:88)M (cid:33)
≤
X
(X×Y)M−m+W
1
k
i=11 ∥Xi−x∥∞≤ηxk,Xmδ Yi+ ℓ=m+11 ∥xℓ−x∥∞≤ηxk,Xmδ
yℓ
,
(cid:32)m−1 M (cid:33)(cid:33) M
1 (cid:88) (cid:88) (cid:79)
1 δ + 1 δ ψ(dx dy )dx.(38)
k ∥Xi−x∥∞≤ηxk,Xm−1 Yi ∥xℓ−x∥∞≤ηxk,Xm−1 yℓ ℓ ℓ
i=1 ℓ=m ℓ=m
Above, the only difference between the two measures inside W is the m-th summand. Due to the definition
of W and the boundedness of X, the transport cost induced by altering the m-th summand is at most k−1.
It follows that
(cid:12) (cid:12)E(cid:2) Z(cid:12) (cid:12)F m(cid:3) −E(cid:2) Z(cid:12) (cid:12)F m−1(cid:3)(cid:12) (cid:12)≤ k1 , m=1,...,M. (39)
Below we further refine the upper bound of the absolute difference in the left hand side of (38) when
m=1,...,M −k. For the integrand in the right hand side of (38) to be positive, it is necessary that
1 ∥Xm−x∥∞≤ηxk,Xm +1 ∥xm−x∥∞≤ηxk,Xm−1 ≥1.
This, together with the tie breaking rule stipulated in Definition 9, further implies that
1 +1 ≥1,
Am Am
1 2
where
(cid:110) (cid:111)
Am := at most (k−1) of x ,ℓ=m+1,...,M −m, falls into B∥Xm−x∥∞ ,
1 ℓ x
(cid:110) (cid:111)
Am := at most (k−1) of x ,ℓ=m+1,...,M −m, falls into B∥xm−x∥∞ .
2 ℓ x
48Combining the above with the reasoning leading to (39), we yield
(cid:12) (cid:12)E(cid:2) Z(cid:12) (cid:12)F m(cid:3) −E(cid:2) Z(cid:12) (cid:12)F m−1(cid:3)(cid:12)
(cid:12)
1 (cid:32) (cid:90) (cid:90) (cid:79)M (cid:90) (cid:90) (cid:79)M (cid:33)
≤ 1 ξ(dx )ν(dx)+ 1 ξ(dx )ν(dx)
k X (X×Y)M−m Am 1
ℓ=m+1
ℓ X (X×Y)M−m+1 Am 2
ℓ=m
ℓ
Above, we have replaced ψ in (38) by ξ because Am and Am no longer depend on y ,ℓ=m+1,...,M. The
1 2 ℓ
analogue applies to the domain of integral as well. We continue to have
(cid:12) (cid:12)E(cid:2) Z(cid:12) (cid:12)F m(cid:3) −E(cid:2) Z(cid:12) (cid:12)F m−1(cid:3)(cid:12)
(cid:12)
1 (cid:32) (cid:90) (cid:90) (cid:79)M (cid:90) (cid:90) (cid:79)M (cid:33) 1
≤ 1 ξ(dx )ν(dx)+ 1 ξ(dx )ν(dx) =: (Im+Im()4.0)
k X (X×Y)M−m Am 1
ℓ=m+1
ℓ X (X×Y)M−m+1 Am 2
ℓ=m
ℓ k 1 2
Regarding I1 defined in (40), note that by Assumption 3,
m
(cid:90)
(X×Y)M−m1
Am
1
ℓ=(cid:79)M
m+1ξ(dx
ℓ)=P(cid:104)
at most (k−1) of Xˇ 1,...,Xˇ
M−m
falls into B
x∥x′−x∥∞(cid:105)(cid:12)
(cid:12)
(cid:12)
x′=Xm,
where Xˇ 1,...,Xˇ
M−m
i. ∼i.d. ξ. Below we define a CDF G(r):=crd,r ∈[0,c− d1]. By Assumption 3 (ii), for any
x,x′ ∈X, we have
(cid:90) (cid:90) (cid:90)
1 ξ(dxˇ)≥c 1 dxˇ≥c 1 dxˇ=G(∥x′−x∥ ),
X
xˇ∈Bx∥x′−x∥∞
X
xˇ∈Bx∥x′−x∥∞
X
xˇ∈B 0∥x′−x∥∞ ∞
where we have used the fact that ∥x′−x∥
∞
≤1≤c− d1 in the last equality. It follows from Lemma 17 that
(cid:90) 1 (cid:79)M ξ(dx )≤k (cid:88)−1(cid:18) M −m(cid:19) G(∥X −x∥ )j(cid:0) 1−G(∥X −x∥ )(cid:1)M−m−j ,
(X×Y)M−m Am 1
ℓ=m+1
ℓ
j=0
j m ∞ m ∞
and thus, by letting U ∼Uniform(X),
Im ≤(cid:90) k (cid:88)−1(cid:18) M −m(cid:19) G(∥X −x∥ )j(cid:0) 1−G(∥X −x∥ )(cid:1)M−m−j dx
1 j m ∞ m ∞
X
j=0
 
=E k (cid:88) j=− 01(cid:18) M −
j
m(cid:19) G(∥x′−U∥ ∞)j(cid:0) 1−G(∥x′−U∥ ∞)(cid:1)M−m−j (cid:12) (cid:12)
(cid:12)
x′=Xm,
where we note that the upper bounded no longer involves ν. For x′ ∈X, it is obvious that
P(cid:2) ∥x′−U∥ ≤r(cid:3) ≥P(cid:2) ∥U∥ ≤r(cid:3) , r ∈R,
∞ ∞
i.e., ∥U∥ stochastically dominates ∥x′ −U∥ . Note additionally that, by Lemma 17 again, below is a
∞ ∞
non-decreasing function,
k−1(cid:18) (cid:19)
r
(cid:55)→(cid:88) M −m G(r)j(cid:0) 1−G(r)(cid:1)M−m−j
.
j
j=0
Consequently,
 
k−1(cid:18) (cid:19)
I 1m ≤E (cid:88) M −
j
m G(∥U∥ ∞)j(cid:0) 1−G(∥U∥ ∞)(cid:1)M−m−j dx.
j=0
49Since ∥U∥
∞
has CDF r (cid:55)→rdX,r ∈[0,1] and G(r)=crd,r ∈[0,c− d1], we continue to obtain
k (cid:88)−1(cid:18) M −m(cid:19)(cid:90) 1 k (cid:88)−1 (M −m)! (cid:90) 1
Im ≤ crdXj(1−crdX)M−m−jdrdX ≤c−1 rj(1−r)M−m−jdr.
1 j j!(M −m−j)!
j=0 r=0 j=0 0
With a similar calculation as in (26), which involves beta distribution and gamma function, we arrive at
k (cid:88)−1 (M −m)! j!(M −m−j)! c−1k
Im ≤c ≤ . (41)
1 j!(M −m−j)! (M −m+1)! M −m
j=0
Regarding Im defined in (40), we first let Xˇ ,Xˇ ,...,Xˇ i. ∼i.d. ξ. Then, note that
2 0 1 M−m
(cid:90)
(X×Y)M−m1
Am
2
ℓ=(cid:79)M
m+1ξ(dx
ℓ)≤P(cid:104)
at most (k−1) of Xˇ 1,...,Xˇ
M−m
falls into B x∥Xˇ
0−x∥∞(cid:105)
(cid:18)
M
−m−1(cid:19)(cid:18)
M
−m(cid:19)−1
k
≤ = ,
k−1 k M −m
wheretheinequalityinthesecondlineisduetothesymmetrystemmingfromAssumption3(i),andthefact
that congestion along with the tie-breaking rule specified in Definition 9 may potentially rules out certain
permutations. Consequently,
k
Im ≤ . (42)
2 M −m
Putting together (39), (40), (41), and (42), we yield
(cid:12) (cid:12)E(cid:2) Z(cid:12) (cid:12)F m(cid:3) −E(cid:2) Z(cid:12) (cid:12)F m−1(cid:3)(cid:12) (cid:12)≤C
m
:= C( Mc− −1+ m1) ∧ k1 , m=1,...,M.
By Azuma-Hoeffding inequality (cf. [Wai19, Corollary 2.20]),
(cid:20)(cid:90) (cid:16) (cid:17) (cid:20)(cid:90) (cid:16) (cid:17) (cid:21) (cid:21) (cid:32) ε2 (cid:33)
P W P ,µˆD ν(dx)−E W P ,µˆD ν(dx) ≥ε ≤exp − , ε≥0.
X x Nk(x) X x Nk(x) 2(cid:80)M C2
m=1 m
To complete the proof, we substitute in the configuration of Theorem 10. Below we only investigate the rate
of (cid:80)M C2 as M →∞, and do not keep track of the constant. For simplicity, we set
m=1 m
d
k =k ∼MdX+d with d:=2∨dY
It follows that
d
(cid:88)M C m2 ∼⌊M− (cid:88)MdX+d⌋
(M
−1
m)2
+ MdX 2d + dd ∼(cid:90) ∞
d
r1
2
dr+ 1
d
∼M− dXd +d,
m=1 m=1
MdX+d MdX+d MdX+d
which completes the proof.
References
[AB06] C. D. Aliprantis and K. C. Border. Infinite Dimensional Analysis: A Hitchhiker’s Guide.
Springer-Verlag Berlin Heidelberg, 2006.
[AH23] B. Acciaio and S. Hou. Convergence of adapted empirical measures on Rd. arXiv:2211.10162,
2023.
50[AHD+23] A.Araujo,A.J.Havens,B.Delattre,A.Allauzen,andB.Hu. Aunifiedalgebraicperspectiveon
lipschitz neural networks. The Eleventh International Conference on Learning Representations,
2023.
[AHS23] F. Altekru¨ger, P. Hagemann, and G. Steidl. Conditional generative models are provably robust:
Pointwiseguaranteesforbayesianinverseproblems. TransactionsonMachineLearningResearch,
2023.
[AKP24] B. Acciaio, A. Kratsios, and G. Pammer. Designing universal causal deep learning models: The
geometric (hyper)transformer. Mathematical Finance, 34:671–735, 2024.
[BBBW22] J. Backhoff, D. Bartl, M. Beiglb¨ock, and J. Wiesel. Estimating processes in adapted wasserstein
distance. Annals of Applied Probability, 32:529–550, 2022.
[BD15] G. Biau and L. Devroye. Lectures on the Nearest Neighbor Method. Springer, 2015.
[BDR21] A. Bhowmick, M. D’Souza, and G. S. Raghavan. Lipbab: Computing exact lipschitz constant of
relu networks. International Conference on Artificial Neural Networks, 2021.
[BFT17] P.L.Bartlett,D.J.Foster,andM.J.Telgarsky. Spectrally-normalizedmarginboundsforneural
networks. Advances in Neural Information Processing Systems, 30, 2017.
[BG90] P. K. Bhattacharya and A. K. Gangopadhyay. Kernel and nearest-neighbor estimation of a
conditional quantile. The Annals of Statistics, 18:1400–1415, 1990.
[BH01] D. M. Bashtannyk and R. J. Hyndman. Bandwidth selection for kernel conditional density
estimation. Computational Statistics & Data Analysis, 36:279–298, 2001.
[BHW92] J. Booth, P. Hall, and A. Wood. Bootstrap estimation of conditional distributions. The Annals
of Statistics, 20:1594–1610, 1992.
[BLR16] K. Bertin, C. Lacour, and V. Rivoirard. Adaptive pointwise estimation of conditional density
function. Ann. Inst. H. Poincar´e Probab. Statist., 52:939–980, 2016.
[BLZ+21] T. Bai, J. Luo, Jun Zhao, B. Wen, and Qian Wang. Recent advances in adversarial training for
adversarial robustness. Proceedings of the Thirtieth International Joint Conference on Artificial
Intelligence, 2021.
[Bog07] V. I. Bogachev. Measure Theory Volume II. Springer-Verlag Berlin Heidelberg, 2007.
[BZLX23] P. Bountakas, A. Zarras, A. Lekidis, and C. Xenakis. Defense strategies for adversarial machine
learning: A survey. Computer Science Review, 49, 2023.
[CJ23] Z. Cheng and S. Jaimungal. Distributional dynamic risk measures in markov decision processes.
arXiv:2203.09612, 2023.
[CJC23] AnthonyCoache,SebastianJaimungal,andA´lvaroCartea. Conditionallyelicitabledynamicrisk
measuresfordeepreinforcementlearning. SIAM Journal on Financial Mathematics,14(4):1249–
1289, 2023.
[CRK19] J. Cohen, E. Rosenfeld, and Z. Kolter. Certified adversarial robustness via randomized smooth-
ing. Proceedings of the 36th International Conference on Machine Learning, 97, 2019.
[CTMP15] Y. Chow, A. Tamar, S. Mannor, and M. Pavone. Risk-sensitive and robust decision-making: a
cvar optimization approach. Advances in Neural Processing Systems, 2015.
51[CUH16] D. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accurate deep network learning by
exponential linear units (elus). arXiv:1511.07289, 2016.
[Dev82] L.Devroye. Necessaryandsufficientconditionsforthepointwiseconvergenceofnearestneighbor
regression function estimates. Probability Theory and Related Fields, 61:467–481, 1982.
[DFG+24] E.Demirkayaa,Y.Fan,L.Gao,J.Lv,P.Vossler,andJ.Wang. Optimalnonparametricinference
with two-scale distributional nearest neighbors. Journal of the American Statistical Association,
199:297–307, 2024.
[Dud69] R. M. Dudley. The speed of mean glivenko-cantelli convergence. The Annals of Mathematical
Statistics, 40(1):40–50, 1969.
[Fan92] J. Fan. Design-adaptive nonparametric regression. Journal of the American Statistical Associa-
tion, 87:998–1004, 1992.
[FERC24] M. Fazlyab, T. Entesari, A. Roy, and R. Chellappa. Certified robustness via dynamic margin
maximizationandimprovedlipschitzregularization. Advances in Neural Information Processing
Systems, 36, 2024.
[FG15] Nicolas Fournier and Arnaud Guillin. On the rate of convergence in wasserstein distance of the
empirical measure. Probability Theory and Related Fields, 162:707–738, 2015.
[FH51] E. Fix and J. L. Hodges. Discriminatory analysis; nonparametric discrimination, consistency
properties. USAF SAM Series in Statistics, Project No. 21-49-004, 1951.
[FJGZ20] E. Fetaya, J., W. Grathwohl, and R. Zemel. Understanding the limitations of conditional gener-
ative models. International Conference on Learning Representations, 2020.
[Fou23] N. Fournier. Convergence of the empirical measure in expected wasserstein distance: non-
asymptotic explicit bounds in Rd. ESAIM: Probability and Statistics, 27:749–775, 2023.
[FRH+19] M.Fazlyab, A.Robey, H.Hassani, M.Morari, andG.Pappas. Efficientandaccurateestimation
of lipschitz constants for deep neural networks. Advances in Neural Information Processing
Systems, 32, 2019.
[FV06] F.FerratyandP.Vieu.NonparametricFunctionalDataAnalysis: TheoryandPractics.Springer,
2006.
[GFPC21] H.Gouk,E.Frank,B.Pfahringer,andM.J.Cree. Regularisationofneuralnetworksbyenforcing
lipschitz continuity. Machine Learning, 110:393–416, 2021.
[GG04] I.GijbelsandA.Goderniaux. Bandwidthselectionforchangepointestimationinnonparametric
regression. Technometrics, 46:76–86, 2004.
[GKKW02] L. Gy¨orfi, M. Kohler, A. Krzy´zak, and H. Walk. A Distribution-Free Theory of Nonparametric
Regression. Springer, 2002.
[GM79] T. Gasser and H. Mu¨ller. Kernel estimation of regression functions. Smoothing Technique for
Curve Estimation, pages 23–63, 1979.
[GOR24] M.Giegrich,R.Oomen,andC.Reisinger.K-nearest-neighborresamplingforoff-policyevaluation
in stochastic control. arXiv:2306.04836, 2024.
[HAYSZ11] K. Hajebi, Y. Abbasi-Yadkori, H. Shahbazi, and H. Zhang. Fast approximate nearest-neighbor
search with k-nearest neighbor graph. Proceedings of the Twenty-Second International Joint
Conference on Artificial Intelligence, 2011.
52[HH90] P. Hall and J. D. Hart. Nonparametric regression with long-range dependence. Stochastic Pro-
cesses and their Applications, 36:339–351, 1990.
[HH17] W. Huang and W. B. Haskell. Risk-aware q-learning for markov decision processes. IEEE 56th
Annual Conference on Decision and Control, 2017.
[HHT24] B. Hosseini, A. W. Hsu, and A. Taghvaei. Conditional optimal transport on function spaces.
arXiv:2311.05672, 2024.
[HM85] W. Hardle and J. S. Marron. Optimal bandwidth selection in nonparametric regression function
estimation. The Annals of Statistics, 13:1465–1481, 1985.
[Hou24] S. Hou. Convergence of the adapted smoothed empirical measures. arXiv:2401.14883, 2024.
[HWY99] P. Hall, R. C. L. Wolff, and Q. Yao. Methods for estimating a conditional distribution function.
Journal of the American Statistical Association, 94:154–163, 1999.
[HZS+21] Y. Huang, H. Zhang, Y. Shi, J. Z. Kolter, and A. Anandkumar. Training certifiably robust
neuralnetworkswithefficientlocallipschitzbounds. Advances in Neural Information Processing
Systems, 34, 2021.
[HZSS16] K. He, X. Zhang, S., and J. Sun. Deep residual learning for image recognition. Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.
[IS15] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. Proceedings of the 32nd International Conference on Machine Learning,
pages 448–456, 2015.
[JD21] M. Jordan and A. G. Dimakis. Exactly computing the local lipschitz constant of relu networks.
Advances in Neural Information Processing Systems, 33, 2021.
[KB17] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv:1412.6980, 2017.
[KKW09] M. Kohler, A. Krzyz˙ak, and H. Walk. Optimal global rates of convergence for nonparametric
regression with unbounded data. Journal of Statistical Planning and Inference, 193:1286–1296,
2009.
[KKW10] M. Kohler, A. Krzyz˙ak, and H. Walk. Uniform convergence rate for nonparametric regrssion
and principle component analysis with functional/longtitudeinal data. The Annals of Statistics,
38:3321–3351, 2010.
[Klo20] Benoˆıt R. Kloeckner. Empirical measures: regularity is a counter-curse to dimensionality.
ESAIM: Probability and Statistics, 24:408–434, 2020.
[Kra23] A.Kratsios. Universalregularconditionaldistributionsviaprobabilistictransformers. Construc-
tive Approximation, 57:1145–1212, 2023.
[KSS14] M. K¨ohler, A. Schindler, and S. Sperlich. A review and comparison of bandwidth selection
methods for kernel regression. International Statistical Review, 82:243–274, 2014.
[Lac07] C.Lacour. Adaptivepointwiseestimationofconditionaldensityfunction. Ann.Inst.H.Poincar´e
Probab. Statist., 43:571–597, 2007.
[LAO20] Y. Li, S. Akbar, and J. Oliva. Acflow: Flow models for arbitrary conditional likelihoods. Pro-
ceedings of the 37th International Conference on Machine Learning, 119:5831–5841, 2020.
53[LWJ+22] H.D.Liu,F.Williams,A.Jacobson,S.Fidler,andO.Litany. Learningsmoothneuralfunctions
via lipschitz regularization. SIGGRAPH ’22: Special Interest Group on Computer Graphics and
Interactive Techniques Conference, 2022.
[LZS+20] Wen Li, Ying Zhang, Yifang Sun, Wei Wang, Mingjie Li, Wenjie Zhang, and Xuemin Lin.
Approximate nearest neighbor search on high dimensional data — experiments, analyses, and
improvement. IEEE Transactions on Knowledge and Data Engineering, 32:1475–1488, 2020.
[Mac81] Y. P. Mack. Local properties of k-nn regression estimates. SIAM Journal on Algebraic Discrete
Methods, 2:311–323, 1981.
[MDAA22] L. Meunier, B. J. Delattre, A. Araujo, and A. Allauzen. A dynamical system perspective for
lipschitz neural networks. International Conference on Machine Learning, 165, 2022.
[Mer08] Milan Merkle. Inequalities for the gamma function via convexity. Advances in Inequalities for
Special Functions, pages 81–100, 2008.
[MFPR17] F. Mart´ınez, M. P. Fr´ıas, M. D. P´erez, and A. J. Rivera. A methodology for applying k-nearest
neighbor to time series forecasting. Artificial Intelligence Review, 52:2019–2037, 2017.
[MFSS17] K. Muandet, K. Fukumizu, B. Sriperumbudur, and B. Sch¨olkopf. Kernel mean embedding of
distributions: A review and beyond. Foundations and Trends in Machine Learning, 10:1–144,
2017.
[MO14] M. Mirza and S. Osindero. Conditional generative adversarial nets. arXiv:1411.1784, 2014.
[Nad64] E. A. Nadaraya. On estimating regression. Theory of Probability and Its Applications, 1964.
[PC19] GabrielPeyr´eandMarcoCuturi. Computational Optimal Transport: With Applications to Data
Science. Foundations and Trends in Machine Learning, 2019.
[PKB+22] P. Pauli, A. Koch, J. Berberich, P. K¨ohler, and F. Allg¨ower. Training robust neural networks
using lipschitz bounds. IEEE Control Systems Letters, 6:121–126, 2022.
[PP16] G.Ch.PflugandA.Pichler. Fromempiricalobservationstotreemodelsforstochasticoptimiza-
tion: Convergence properties. SIAM Journal on Optimization, 26:1715–1740, 2016.
[PPM17] G.Papamakarios, T.Pavlakou, andIainMurray. Maskedautoregressiveflowfordensityestima-
tion. Advances in Neural Information Processing Systems, 30, 2017.
[PSCW20] O.H.M.Padilla,J.Sharpnack,Y.Chen,andD.M.Witten. Adaptivenonparametricregression
with the k-nearest neighbour fused lasso. Technometrics, 107:293–310, 2020.
[RK22] J. J. Ryu and Y. Kim. Minimax regression via adaptive nearest neighbor. arXiv:2202.02464,
pages 1447–1451, 2022.
[RLK+21] M. Rachdi, A. Laksaci, Z. Kaid, A. Benchiha, and F. A. Al-Awadhi. k-nearest neighbors local
linear regression for functional and missing data at random. Statistica Neerlandica, 75, 2021.
[RS18] D. Rudolf and N. Schweizer. Perturbation theory for markov chains via wasserstein distance.
Bernoulli, 24:2610–2639, 2018.
[RS19] P. Ram and K. Sinha. Revisiting kd-tree for nearest neighbor search. KDD’19: Proceedings of
the25thACMSIGKDDInternationalConferenceonKnowledgeDiscovery&DataMining,page
1378–1388, 2019.
[Sco15] D. Scott. Multivariate Density Estimation: Theory, Practics, and Visualization. Wiley, 2015.
54[Sim96] J. S. Simonoff. Smoothing Methods in Statistics. Springer, 1996.
[SSF22] S. Singla, S. Singla, and S. Feizi. Improved deterministic l2 robustness on cifar-10 and cifar-100.
The Tenth International Conference on Learning Representations, 2022.
[Sto82] C. J. Stone. Optimal global rates of convergence for nonparametric regression. The Annals of
Statistics, 10:1040–1053, 1982.
[SWZ+22] Z.Shi,Y.Wang,H.Zhang,J.Z.Kolter,andCho-JuiHsieh. Efficientlycomputinglocallipschitz
constantsofneuralnetworksviaboundpropagation. Advances in Neural Information Processing
Systems, 35, 2022.
[SX10] D. Shah and Q. Xie. Q-learning with nearest neighbors. Advances in Neural Information Pro-
cessing Systems, 31, 2010.
[TK21] A. Trockman and J. Z. Kolter. Orthogonalizing convolutional layers with the cayley transform.
International Conference on Learning Representations, 36, 2021.
[TSS18] Y. Tsuzuku, I. Sato, and M. Sugiyama. Lipschitz-margin training: Scalable certification of
perturbation invariance for deep neural networks. Advances in Neural Information Processing
Systems, 31, 2018.
[Vil08] C´edricVillani. Optimaltransport: Oldandnew,volume338. SpringerScience&BusinessMedia,
2008.
[vK24] M. Sˇm´ıd and V. Kozm´ık. Approximation of multistage stochastic programming problems by
smoothed quantization. Review of Managerial Science, 2024.
[VPC24] M. Vuleti´c, F. Prenzel, and M. Cucuringu. Fin-gan: forecasting and classifying financial time
series via generative adversarial networks. Quantitative Finance, 24, 2024.
[VS18] A. Virmaux and K. Scaman. Lipschitz regularity of deep neural networks: analysis and efficient
estimation. Advances in Neural Information Processing Systems, 31, 2018.
[VSP+17] A.Vaswani, N.Shazeer, N.Parmar, J.Uszkoreit, L.Jones, A.N.Gomez, L.Kaiser, andI.Polo-
sukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30,
2017.
[Wai19] M. J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge university press, 2019.
[Was06] L. Wasserman. All of Nonparametric Statistics. Springer, 2006.
[Wat64] G. S. Watson. Smooth regression analysis. Sankhya: The Indian Journal of Statistics, Series A,
26:359–372, 1964.
[WM23] R. Wang and I. Manchester. Direct parameterization of lipschitz-bounded deep networks. Pro-
ceedings of the 40th International Conference on Machine Learning, 202, 2023.
[XA22] T.XuandB.Acciaio. Conditionalcot-ganforvideopredictionwithkernelsmoothing. NeurIPS
2022 Workshop on Robustness in Sequence Modeling, 2022.
[XLR+22] A. Xue, L. Lindemann, A. Robey, H. Hassani, G. J. Pappas, and R. Alur. Chordal sparsity for
lipschitz constant estimation of deep neural networks. 2022 IEEE 61st Conference on Decision
and Control, 6:3389–3396, 2022.
55[ZJHW22] B. Zhang, D. Jiang, D. He, and L. Wang. Rethinking lipschitz neural networks and certified
robustness: Abooleanfunctionperspective.AdvancesinNeuralInformationProcessingSystems,
35, 2022.
[ZL19] P.ZhaoandL.Lai. Minimaxregressionviaadaptivenearestneighbor. 2019 IEEE International
Symposium on Information Theory, pages 1447–1451, 2019.
[ZT23] W. Zhao and E. G. Tabak. Adaptive kernel conditional density estimation. 2023.
56