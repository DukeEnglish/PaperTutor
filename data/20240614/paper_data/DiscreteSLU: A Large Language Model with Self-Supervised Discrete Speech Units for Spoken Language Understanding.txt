DiscreteSLU: A Large Language Model with Self-Supervised
Discrete Speech Units for Spoken Language Understanding
SuwonShon1,KwangyounKim1,Yi-TeHsu1,PrashantSridhar1,ShinjiWatanabe2,KarenLivescu3
1ASAPP,USA 2CarnegieMellonUniversity,USA 3ToyotaTechnologicalInstituteatChicago,USA
sshon@asapp.com
Abstract “mid-atlantic”
The integration of pre-trained text-based large language mod- Large Language Model
els(LLM)withspeechinputhasenabledinstruction-following
capabilitiesfordiversespeechtasks. Thisintegrationrequires Concat
the use of a speech encoder, a speech adapter, and an LLM,
Text
trainedondiversetasks. Weproposetheuseofdiscretespeech embedder Speech Adapter
units(DSU),ratherthancontinuous-valuedspeechencoderout-
puts, that are converted to the LLM token embedding space
Text tokenizer
using the speech adapter. We generate DSU using a self-
supervisedspeechencoderfollowedbyk-meansclustering.The Below is an instruction that describes a task, paired with an input that provides
proposed model shows robust performance on speech inputs further context. Write a response that appropriately completes the request.
###Instruction: "Which regions have temperate climates?
fromseen/unseendomainsandinstruction-followingcapability
###Input: 20 32 44 21 295 185 ... 998 29 2 44 842
in spoken question answering. We also explore various types ###Output:
of DSU extracted from different layers of the self-supervised Prompt generated using SLUE-SQA5 sample
speechencoder,aswellasMelfrequencyCepstralCoefficients
(MFCC).OurfindingssuggestthattheASRtaskanddatasets Feature k-means De- Subword
arenotcrucialininstruction-tuningforspokenquestionanswer- extraction clustering duplication model
ingtasks.
Figure1:Modelarchitectureoverview
Index Terms: large language models, discrete speech units,
self-supervisedlearning,spokenlanguageunderstanding featuressuchasMFCCstosignificantlyreducethecomputation
load.Thekeycontributionsofthisworkareasfollows:
1. Introduction
• WecompareASR-trainedspeechencodersandDSU-based
speech input, with quantitative analyses on seen/unseen
Recent work integrating pre-trained text-based large language
speechdomainsandzero-shottasks.
models (LLMs) with speech input has enabled instruction-
followingcapabilitiesfordiversespeechprocessingtasks[1–5]. • WeinvestigatevarioustypesofDSUsasinputstolargelan-
TofeedspeechintoanLLM,additionalmodulesconsistingof guagemodels,rangingfromdeeptoshallowlayersandeven
a speech encoder and speech adapter are typically used. The MFCCs.
speech encoder converts audio into a speech embedding se- • Wepresentareproduciblemethodforbuildingourmodels,
quence. Then, the speech adapter maps the speech embed- usingpubliclyavailabledatasetswithoutanyadditionaldata
dingsequenceintothetexttokenembeddingspaceoftheLLM. mining methods, text-to-speech data generation, or LLM-
These previous studies typically use a speech encoder trained baseddatageneration.
forautomaticspeechrecognition(ASR)usinghumantranscrip-
tions. Thelengthofthespeechencoderoutputisthenreduced
2. Relatedwork
viaoneofseveralcompressionapproaches. Finally,thespeech
adapterandLLMarefine-tunedonaspeechdatasetwithappro- Therehasbeengrowinginterestin,andanumberofinnovative
priateinstructions,whilethespeechencoderistypicallyfrozen. approachesfor,usingLLMstoenabletraditionalspeechtasks
However, using self-supervised learning (SSL) speech suchasASRandSLUvianaturallanguagepromptsorinstruc-
models [6,7], which can exploit a much larger set of speech tions. Themostcommonapproachistouseaspeechencoder
data, may generate better speech representations than using trainedforASRusingeitheraconnectionisttemporalclassifica-
single-task supervised encoders. Moreover, a recent study on tion(CTC)[1,4,11–13]oranattention-basedencoder-decoder
discreteunits[8]withlengthreduction[9,10]showspromising (AED)[2,3,5,14]approach. ForCTC,lengthcompressionis
performanceacrossspeechtasks,comparedtousingcontinuous achievedbyremovingembeddingframescorrespondingtothe
SSLrepresentations,andmaybeagoodcompromisebetween blanktokenorbyaveragingembeddingsthatbelongtoconsecu-
performanceandefficiency.Webelievethatthisapproachcould tivesame-labeltokens[1,12,15].Down-samplingwithstriding
alsobebeneficialforfeedingspeechintoanLLM. canalsobeappliedinbothapproaches[2,4,11,13,14,16].
This paper studies discrete speech units (DSU) combined Theseapproacheshaveshowncompetitivespeechtaskper-
withaspeechadaptertoconvertDSUintotheembeddingspace formancewithinstructionfollowingcapabilityandevenzero-
ofLLMtokens. TheDSUcanbeextractedusinganylayerof shot or 1-shot capability [1,2,17,18]. However, some stud-
anSSLspeechmodel,andalsocanbeextractedusingacoustic ies train and evaluate on fixed tasks such as ASR or speech-
4202
nuJ
31
]LC.sc[
1v54390.6042:viXraTable1:Instructionsandoutputsforourspeechtasks. ous studies [6,8]. First, the speech input X is mapped to a
robusthigh-dimensionalspeechrepresentationsequenceH =
SQA Intruction:{question}
[h ,h ,...,h ]usingapre-trainedself-supervisedmodelsuch
Output:{answer} 0 1 T
asWavLM[7]orHuBERT[6]. Wethenusek-meanscluster-
ASR Instruction:Generatetranscriptionofthegiven ingtoconverttheembeddingsequenceH toak-meanscluster
speechinput
indexsequenceZ.Theresultingclusterindex(DSU)sequence
Output:{normalizedtranscrition}
canbeuseddirectlyasinputtoanLLMorcanbefurthercom-
SA Instruction:Classifythegivenspeechintoone pressed by length reduction methods like de-duplication and
ofpositive,neutralandnegativesentiments
subword modeling. De-duplication involves converting sub-
Output:{positive,neutral,negative}
sequences of consecutive repeated cluster indices to a single
NER: Instruction:Findnamedentityinthespeech. unique index. A subword model can then be trained on the
Output:setof{phrase(typeofnamedentity)} de-duplicatedclusterindexsequencestoconvertZ toameta-
S2TT: Instruction:Translatetheinputto{language} cluster index sequence Z(cid:101) = [z 0,z 1,...,z T(cid:101)] where T(cid:101) < T is
thefinalreducedlengthinframes.Theoveralllengthreduction
to-texttranslation(S2TT)[11–14,16,17];thesemodelsdonot ratio T(cid:101)/T depends on the subword size and typically ranges
followinstructionsandperformonlythelimitedtaskstheyhave between30%and60%.
seenduringtraining.Furthermore,mostofthepriorworklacks
out-of-domain evaluation or has not been thoroughly bench- 3.2. Intruction-tuningwithaspeechadapter
marked.In this work, we extensively investigate the out-of- Forinstructiontuning,wecombinetheDSUsequenceZ(cid:101)witha
domaincapabilityoftheproposedmethod. textinstructionandthedesiredoutputasshowninFigure1.The
Some prior work uses training datasets that are generated speechadapterconvertstheDSUportionofthepromptintoa
automatically via LLM-assisted text label generation, using continuousembeddingsequencethatalignswiththetexttoken
modelssuchasGPT3.5[1,3]orLLama-2[4],andaudiogen- embeddingspaceoftheLLM.Thetextportionoftheprompt
erationviaTTS[2,4]. Theevaluationisalsooftendoneusing istokenizedintosubwords, usingtheLLM’stokenizer, andis
thesamedatagenerationprocess[1,4],whichmaynotreflect mappedtoacontinuousembeddingsequenceusingtheLLM’s
the real-world performance of the model on the task. In this embedding lookup table. The two continuous embedding se-
study, weuseenitrelyopen-sourcedatasetsfortraining, with- quences,onefromtheDSUsequenceandonefromthetext,are
outanyTTS-generatedaudioorLLM-generatedlabels,andthe concatenatedinthesameorderasintheoriginalprompt.Using
evaluationisdonewithhumanlabels. thisconcatenatedinputsequence,alongwiththedesiredoutput
While many studies investigate discrete units for text, instruction-tuning consists of updating the parameters of
speech [10,19,20], the most relevant prior approach to ours thespeechadapterandLLM.FortheLLMparameters,weuse
is AudioPaLM [21]. Rather than using a speech adapter, parameter-efficientfine-tuning,specificallyLow-RankAdapta-
AudioPaLMfeedsdiscreteaudiotokensintothemodeldirectly tion(LoRA)[31].
to generate audio/text. However, the tasks in this prior work
3.3. Instruction-tuningwithadiversedataset
are limited to ASR and translation; the model’s ability to
Adiverseinstruction-tuningdatasetisnecessarytoenablethe
follow instructions or perform other SLU tasks has not yet
instruction-followingcapabilityoftheLLM.Onecommonway
been evaluated. For extracting the discrete audio tokens,
to build diverse (instruction, input, output) triplets is through
AudioPaLMconsideredmultiplespeechrepresentationmodels
LLM-assisted label generation, such as generating translation
suchasw2v-BERT[22]andUSM[23].However,thestudydid
labels given a monolingual text dataset. Speech input can
not consider the effect of extracting embeddings and discrete
be generated using a TTS system. However, such automatic
units from different layers in the same model. As shown in
datasetgenerationisaformofweaksupervision,dependsonthe
prior work [24], different speech attributes are encoded in
LLMandTTSperformance,andrequirespotentiallyhighinfer-
different model layers, so the choice of layer may impact
encecostsforthelargegenerationmodels. Inthisstudy,wedo
considerably both downstream performance and computa-
notuseanyLLM-assistedlabelorTTS-generatedspeech.Allof
tional load. VoxtLM [25] also considers DSUs as input to a
ourtrainingdataisbasedonSLUE[28,30],Tedlium3[27],and
language model for multitask learning, but using task tokens
Fleurs[29]toprovideASR,spokenquestionanswering(SQA),
rather than instruction-following. Finally, SpeechGPT [26]
sentiment analysis (SA), and named entity recognition (NER)
also uses DSUs as input to an LLM, but the evaluation is
examplesfortraining. FortheSQAtask,Weusetheprovided
qualitative via examples; there is no quantitative performance
questionastheinstruction. Forothertasks,weusethesimple
evaluation. In contrast to all these studies, we quantitatively
instructionsgiveninTable1. WealsoincludeS2TTasatask
evaluate instruction-following ability for spoken language
onlyforevaluation;thistaskisunseenintraining.
understanding, explore DSUs extracted from different layers,
and also evaluate DSUs extracted using MFCCs, which may
4. Experiments
significantly reduce both training effort and computational
load.
4.1. Modelandtrainingsetup
3. Proposedapproach To generate DSUs, we extract speech representations using
WavLM and then cluster using k-means. We choose layer 21
3.1. Generatingspeechdiscreteunits followingapreviousstudy[8]thatconsideredtheCCAsimilar-
Speech discretization is the process of transforming an audio itieswithwordlabels[24].Fork-meansclusteringandsubword
waveform X = [x ,...,x ], where x ∈ R, into a sequence modeling,following[8]weuseK = 1000and2000subword
1 L l
ofDSUZ = [z ,...,z ],whereT ≤ Landz ∈ {1,...,K}. tokens. Forcomparison,wereproduceCTC-basedrepresenta-
0 T t
Our tokenization approach is based on k-means clustering of tionandcompressionapproaches[1],includingbothblankre-
pre-trained self-supervised representations, similarly to previ- movalandframeaveraging.Table2:ASRtasktrainedsystemevaluation.AandBaretraditionalASRsystems.C,D,E,andFusecompressedCTCembeddingsas
inputtothespeechadapter+LLM.G,HandJareoursystemsthatuseDSUsasinputtothespeechadapter+LLM.
Systemtype WER BLEU-1
SystemID
Tedlium3 SLUE-VP SLUE-VC Fleurs[29] SLUE-SQA5[30]
(dev/test)[27] (test)[28] (test)[28] (testenus) (dev/test/v-test)
A Fbank-CTC-AEDa TraditionalASR 7.3/ 6.4 - - - -
B DSU(WavLM/21)-CTC-AEDb TraditionalASR 9.0/ 8.9 - - - -
C COSMIC-ASR-7B[1] CTC-LLM - /13.1 - - - -
D Fbank-CTC(blank-removed)a CTC-LLM 21.1/16.8 25.0 32.4 37.6 1.2/1.1/1.5
E Fbank-CTC(averaged)a baseline CTC-LLM 10.1/ 8.4 16.8 24.5 27.1 1.4/1.4/1.9
F DSU(WavLM/21)-CTC(averaged)b CTC-LLM 11.2/ 9.0 12.7 22.3 14.4 1.5/1.5/1.9
G DSU(WavLM/21) DSU-LLM 6.6/ 6.4 12.0 17.2 13.0 1.5/1.5/2.0
H +dedup DSU-LLM 8.1/ 6.4 11.8 17.3 12.6 1.5/1.5/2.0
J +dedup+subword DSU-LLM 7.7/ 7.1 11.2 17.4 13.2 1.5/1.5/1.9
actcmodelusedhere:https://huggingface.co/espnet/dongweitedlium3asre-branchformerexternallm
bctcmodelusedhere:https://huggingface.co/espnet/kohei0209ted3asr2ebranchformer1rawwavlmlarge21km1000bperm2000bpets500sp
Table 3: ASR + SLU task trained system evaluation. Note: the verified test set (v-test) for SLUE-SQA5 is a human-validated test
set[30].∗:humantranscriptionfine-tunedtext-basedmodelasanupperbound.
WER BLEU1
SystemID Tedlium3 SLUE-VP SLUE-VC Fleurs SLUE-SQA5
Trainingdata
(test) (test) (test) (testenus) (dev/test/v-test)
E1 CTC Tedlium3+SLUE-SQA5 8.5 17.5 24.9 26.7 62.1/56.4/50.5
(averaged)
+SLUE-VC{ASR,SA}
E2 baseline 9.7 18.6 33.5 32.8 57.9/52.4/46.6
+SLUE-VP{ASR,NER}
G1 Tedlium3+SLUE-SQA5 7.9 12.1 17.9 13.9 62.7/56.8/48.2
DSU
(WavLM/21) +SLUE-VC{ASR,SA}
G2 6.8 11.2 15.3 13.0 62.1/55.8/49.9
+SLUE-VP{ASR,NER}
J1 Tedlium3+SLUE-SQA5 8.5 14.1 19.9 17.5 60.7/55.3/48.3
DSU
(WavLM/21) +SLUE-VC{ASR,SA}
J2 8.6 13.0 18.4 15.3 60.1/54.8/47.6
+dedup+subword +SLUE-VP{ASR,NER}
K1∗ Mistral-7B-v0.1 SLUE-{SQA5,VCSA,VPNER} - - - - 83.2/78.8/77.2
Table4:Zero-shotS2TTevaluationonFleurstestset 4.2. Taskanddataspecifications
BLEU We use various datasets for different tasks. For ASR, we use
SystemID
En→Fr En→De En→Es theTedlium3[27],SLUE-VoxCeleb(SLUE-VC)[28],SLUE-
VoxPopuli(SLUE-VP)[28],andFleurs[29]datasets.ForSQA,
Mistral-7B-
K2 19.01 12.08 16.18 we use SLUE-SQA5 [30]. We also use SLUE-VC for SA,
Instruct-v0.1
SLUE-VP for NER, and Fleurs for S2TT. Note that not all
E1 CTC(averaged) 0.11 0.09 0.10 datasetsareusedfortraininginallexperiments,dependingon
E2 6.71 3.09 3.23
baseline theexperimentsetup. Forexample, theFleursdatasetisused
J1 DSU(WavLM/21)+ 0.13 0.13 0.12 onlyforevaluationinallexperiments.
J2 dedup+subword 8.52 3.01 5.27 Tovalidatemodelperformanceindiversesettings,wegrad-
uallyaddtasksstartingfromASR-only,thenSQA,andthenSA
andNERplusadditionalASRdata.ForASR-onlytraining,we
usetheTedlium3devsetasavalidationset. Forallotherex-
For the speech adapter, an embedding layer first converts
periments,weusetheSLUE-SQA5devsetasavalidationset.
DSUto512-dimensionalembeddings. Therestoftheadapter
Ourevaluationmetricsareworderrorrate(WER)forASRand
structure follows [1], with two 2D convolution layers with 2
BLEUscoreforS2TT.ForSQA,weuseBLEUwithamaxi-
stride,4transformerlayers,andalinearlayer. Thelinearlayer
mumn-gramorderof1(i.e.,weuseBLEU-1)sincethemajor-
converts the 512-dimensional transformer output to 4096 di-
ityofanswerscontain1-2words. NotethatourSLUE-SQA5
mensions,tomatchthetokenembeddingdimensionalityofthe
task is slightly different from the original task in SLUE [30]
LLM.FortheLLMcomponent,weinitializethemodelweights
sincewemodifytheoutputforgeneralQAasshowninTable1.
withtheMistral-7B-v0.1pre-trainedmodel[32].WeuseLoRA
fine-tuningwitharankof8andα =16onallfourprojection
4.3. Results
layersinthetransformer.Thespeechadapterhas18Mtrainable
parameters,andtheLoRAadapterhas7Mparameters. 4.3.1. ASR-onlytask
Our first experiment focuses on the ASR task alone by train-
We conduct experiments on 8 A6000 GPUs, using an ingthemodelusingtheTedlium3trainingset. Theresultsare
AdamWoptimizerwithminibatchsizeof1280for15epochs showninTable2. Similarlytopreviouswork[33], lengthre-
and0.005learningrate. Thevalidationlossisevaluatedevery ductiondoesnotalwaysgivethebestresult,butisefficientwith
200steps. alengthreductionofabout-50%onTedlium3.SystemsAandTable5:ComparisonofDSUsextractedfromshallowlayersandMFCCs.*notethatE3isnotDSU-basedapproach
WER BLEU1
SystemID
Tedlium3 SLUE-VP SLUE-VC Fleurs SLUE-SQA5
Trainingdata
(test) (test) (test) (testenus) (dev/test/v-test)
J2 WavLM/21 8.6 13.0 18.4 15.3 60.1/54.8/47.6
J3 WavLM/15 Tedlium3 12.7 19.1 29.3 25.8 61.7/56.8/50.9
J4 WavLM/10 +SLUE-SQA5 18.4 32.3 43.8 37.7 58.8/54.2/46.4
J5 WavLM/5 +SLUE-VC{ASR,SA} 40.7 55.6 76.8 79.1 55.3/51.5/44.9
J6 WavLM/1 +SLUE-VP{ASR,NER} 64.5 92.4 107.1 147.3 51.7/47.7/41.9
L1 MFCC 127.9 113.3 113.3 162.2 51.4/46.3/40.8
E3 CTC(averaged)* SLUE-SQA5 147.2 105.6 159.0 126.7 53.1/49.7/42.3
J7 WavLM/21 +SLUE-VCSA 151.6 164.8 126.6 197.9 54.2/49.3/43.2
L2 MFCC +SLUE-VPNER 99.0 98.9 98.2 96.6 53.2/50.0/43.3
BareconventionalASRsystemstrainedasjointCTC-attention ontheFleursdatasetoranytranslationtask(E1vs.E2andJ1
models [34] from scratch. Note that system A also uses a vs.J2). Asan“oracle”reference,SystemK2usesatextLLM
transformer-basedLMwhendecoding. ThebaselinesystemE instruction-tunedbyfeedinghumantranscriptionsandthesame
isour(approximately)reproducedversionofsystemC[1];we S2TTtextinstructioninTable1. Thisresultsuggeststhatwe
assumethattheWERgapbetweenCandEareduetodifferent couldexpectanevenmoregeneralmodelifweaddmoretasks
layertypes(4transformerblocksvs.12E-Branchformer[35]). anddatasetsintrainingtheDSU-basedmodel.
OurDSU+LLMapproach(G,H,J)demonstratessignificantly
betterperformanceonallASRtestsets.NotethatWavLMuses 4.3.3. ComparisonofDSUtypes
theVoxPopulidatasetinpre-training,soitisapartiallyseendo- Table5showstheresultsofvaryingthelayerfrom15to1for
mainfortheDSUmodel,butanunseendomainforthespeech extractingDSU.WealsoconsiderusingMFCCsinsteadofpre-
adapterandLLM. trainedWavLM.Whentheembeddingisextractedfromashal-
BothSLUE-VCandFleursarecompletelyunseendomains low layer, the ASR performance declines significantly. How-
forWavLMandthespeechadapter.However,ourmodelshows ever, the BLEU score for the SQA task remains in a similar
significantlybetterperformanceonthesedomainsthantheCTC range. MFCC input causes only about 13% degradation in
compressionbaseline. ThisindicatesthatalthoughtheLLMis BLEUcomparedtoWavLM/21(J2vs.L1). TrainingonASR
robust across text domains, the performance suffers when the dataisbeneficialfortheSQAtaskforDSU(J2vs. J7),butnot
inputspeechisout-of-domainfortheencoder. DSUmayserve forMFCC(L1andL2). Webelievethemodelsimplycannot
asamoregeneral-purposeinput,whichabstractsawaycertain generatethetranscriptiongiventheASRinstruction,butunder-
domaindetails,andsoarebeneficialforunseenspeechdomains stands the content enough to answer the given question using
intherealworld.TheextremelylowBLEU-1scoresonSLUE- the SQA data. In addition, neither the DSU-based approach
SQA5showthatthemodelisunabletofollowtheinstructions (J7)northespeechencoderusingCTC(E3)showsanystrength
foranewtask. ontheSQAtaskcomparedtoMFCC(L2).Thissuggeststhata
betteralternativeaudioquantization/tokenizationcouldfillthe
4.3.2. AddingSLUtasks
gapbetweenMFCCandWavLM/21foruniversalspeechtasks.
NextweusesystemsE,G,JfromTable2, andtrainthemby Overall, thisisaveryintriguingresultsinceitistypicallyas-
adding more tasks. We train from scratch when adding more sumedthatthelanguagemodelfirsttranscribesspeech,thenun-
data. TheresultsareshowninTable3. WefirstaddtheSQA derstandsit,andfinallygeneratestheanswer. Wewillconduct
task (E1, G1, J1) and then add SA and NER with more ASR furtherstudiestodeterminewhetherthemodeldoesnotneedto
data(E2,G2,J2). transcribethespeechinordertoanswer, orinsteadthemodel
Note that the SLUE-VC and SLUE-VP datasets are rel- just leverages pre-trained knowledge to answer, regardless of
atively small (<15h each) and are intended for low-resource theaudiodocument.
conditions. At the same time, the tasks are also much sim-
pler than SQA: SA is a classification task, and NER is a de- 5. Conclusion
tection+classificationtask. WhenweaddthesesmallSLUand Our study introduce the use of DSU to prompt LLMs with
ASRdatasetsintraining,weobserveaperformancedegradation speech, and has found it effective in both seen and unseen
intheCTCcompression-basedsystemforbothASRandSQA speechdomains,evenonazero-shottask.Wedemonstratethat
tasks(E1vs.E2inTable3). Thisindicatesthatusefulcontext ASRisnotnecessaryforreasoning(SQA)ifthespeechrepre-
informationmaybediscardedintheCTC-compressedsequence sentation is close to the raw signal. While our approach uses
sinceitisoptimizedforASR.Incontrast, theDSU-basedap- smaller-scaledatasetsthanthoseinotherwork[2]anddoesnot
proachshowsbetterorsimilarperformancewhenaddingmore relyonlarge-scalelabelgenerationwithateacherLLM[1,3,4],
datasets (G1 vs. G2, and J1 vs. J2). For NER and SA tasks, it is still able to train the LLM to follow instructions. In ad-
wefindthatthemodelisnotabletogenerateoutputsthatare dition, all data and labels are publicly available, so this study
well-formedenoughtoevaluate.Weassumethisisbecausethe shouldbeeasilyreproducible.However,wenotethatwedidnot
trainingsetsaretoosmallandthegenerativemodelisweakfor validateopen-endedSQAperformance,whichweleavetofu-
classificationtasks. turework.Furthermore,thetrainingprocessisinherently2-step
Aninterestingfinding, showninTable4, isthatthesmall since the k-means clustering part is not differentiable. Future
amount of SLU training data appears to have unlocked the workcouldincludeusinganeuralaudiocodecorquantization
zero-shotcapabilityofthemodelforspeech-to-texttranslation methodthatenablesend-to-endtrainingwithdiscreteunits.
(S2TT).Themodelisabletofollowthetranslationinstruction
and generate text in the target language without being trained6. References
[18] Y.Gong, A.H.Liu, H.Luo, L.Karlinsky, andJ.Glass, “Joint
audio and speech understanding,” in IEEE Auto-matic Speech
[1] J.Pan,J.Wu,Y.Gaur,S.Sivasankaran,Z.Chen,S.Liu,andJ.Li,
RecognitionandUnderstandingWorkshop(ASRU),2023.
“Cosmic: Dataefficientinstruction-tuningforspeechin-context
learning,”arXivpreprintarXiv:2311.02248,2023. [19] J.-C. Chou, C.-M. Chien, W.-N. Hsu, K. Livescu, A. Babu,
A. Conneau, A. Baevski, and M. Auli, “Toward joint lan-
[2] M.Wang,W.Han,I.Shafran,Z.Wu,C.-C.Chiu,Y.Cao,N.Chen,
guage modeling for speech units and text,” arXiv preprint
Y.Zhang,H.Soltau,P.K.Rubensteinetal.,“Slm:Bridgethethin
arXiv:2310.08715,2023.
gapbetweenspeechandtextfoundationmodels,”inIEEEAuto-
maticSpeechRecognitionandUnderstandingWorkshop(ASRU), [20] T. A. Nguyen, B. Muller, B. Yu, M. R. Costa-Jussa, M. El-
2023,pp.1–8. bayad, S. Popuri, P.-A. Duquenne, R. Algayres, R. Mavlyutov,
I.Gatetal.,“Spirit-lm:Interleavedspokenandwrittenlanguage
[3] C.Tang,W.Yu,G.Sun,X.Chen,T.Tan,W.Li,L.Lu,Z.Ma,and
model,”arXivpreprintarXiv:2402.05755,2024.
C.Zhang,“Salmonn: Towardsgenerichearingabilitiesforlarge
languagemodels,”arXivpreprintarXiv:2310.13289,2023. [21] P.K.Rubenstein,C.Asawaroengchai,D.D.Nguyen,A.Bapna,
Z. Borsos, F. d. C. Quitry, P. Chen, D. E. Badawy, W. Han,
[4] Y.Fathullah,C.Wu,E.Lakomkin,J.Jia,Y.Shangguan,J.Ma-
E.Kharitonovetal.,“Audiopalm: Alargelanguagemodelthat
hadeokar, O. Kalinli, C. Fuegen, and M. Seltzer, “Towards
canspeakandlisten,”arXivpreprintarXiv:2306.12925,2023.
general-purposespeechabilitiesforlargelanguagemodelsusing
unpaireddata,”arXivpreprintarXiv:2311.06753,2023. [22] Y.-A.Chung,Y.Zhang,W.Han,C.-C.Chiu,J.Qin,R.Pang,and
Y.Wu,“W2v-bert: Combiningcontrastivelearningandmasked
[5] Y.Chu,J.Xu,X.Zhou,Q.Yang,S.Zhang,Z.Yan,C.Zhou,and
language modeling for self-supervised speech pre-training,” in
J. Zhou, “Qwen-audio: Advancing universal audio understand-
IEEEAuto-maticSpeechRecognitionandUnderstandingWork-
ingviaunifiedlarge-scaleaudio-languagemodels,”arXivpreprint
shop(ASRU).
arXiv:2311.07919,2023.
[23] Y.Zhang,W.Han,J.Qin,Y.Wang,A.Bapna,Z.Chen,N.Chen,
[6] W.-N.Hsu,B.Bolte,Y.-H.H.Tsai,K.Lakhotia,R.Salakhutdi-
B.Li,V.Axelrod,G.Wangetal.,“Googleusm: Scalingauto-
nov,andA.Mohamed,“HuBERT:Self-SupervisedSpeechRepre-
maticspeechrecognitionbeyond100languages,”arXivpreprint
sentationLearningbyMaskedPredictionofHiddenUnits,”arXiv
arXiv:2303.01037,2023.
preprintarXiv:2106.07447,2021.
[24] A.Pasad,J.-C.Chou,andK.Livescu,“Layer-wiseanalysisofa
[7] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li,
self-supervisedspeechrepresentationmodel,”inIEEEAuto-matic
N.Kanda,T.Yoshioka,X.Xiaoetal.,“Wavlm:Large-scaleself-
SpeechRecognitionandUnderstandingWorkshop(ASRU),2021,
supervised pre-training for full stack speech processing,” IEEE
pp.914–921.
JournalofSelectedTopicsinSignalProcessing, vol.16, no.6,
[25] S.Maiti,Y.Peng,S.Choi,J.-w.Jung,X.Chang,andS.Watanabe,
pp.1505–1518,2022.
“Voxtlm: unifieddecoder-onlymodelsforconsolidatingspeech
[8] X.Chang,B.Yan,K.Choi,J.Jung,Y.Lu,S.Maiti,R.Sharma,
recognition/synthesisandspeech/textcontinuationtasks,” arXiv
J.Shi,J.Tian,S.Watanabeetal.,“Exploringspeechrecognition,
preprintarXiv:2309.07937,2023.
translation,andunderstandingwithdiscretespeechunits:Acom-
parativestudy,”arXivpreprintarXiv:2309.15800,2023. [26] D. Zhang, S. Li, X. Zhang, J. Zhan, P. Wang, Y. Zhou, and
X. Qiu, “Speechgpt: Empowering large language models with
[9] F. Wu, K. Kim, S. Watanabe, K. J. Han, R. McDonald, K. Q.
intrinsic cross-modal conversational abilities,” arXiv preprint
Weinberger, and Y. Artzi, “Wav2seq: Pre-training speech-to-
arXiv:2305.11000,2023.
textencoder-decodermodelsusingpseudolanguages,” inIEEE
ICASSP,2023,pp.1–5. [27] F. Hernandez, V. Nguyen, S. Ghannay, N. Tomashenko, and
Y. Este`ve, “TED-LIUM 3: Twice as Much Data and Corpus
[10] K. Lakhotia, E. Kharitonov, W.-N. Hsu, Y. Adi, A. Polyak,
RepartitionforExperimentsonSpeakerAdaptation,”inLecture
B. Bolte, T.-A. Nguyen, J. Copet, A. Baevski, A. Mohamed
NotesinComputerScience,2018.
et al., “On generative spoken language modeling from raw au-
dio,”TransactionsoftheAssociationforComputationalLinguis- [28] S.Shon, A.Pasad, F.Wu, P.Brusco, Y.Artzi, K.Livescu, and
tics,vol.9,pp.1336–1354,2021. K.J.Han,“Slue:Newbenchmarktasksforspokenlanguageun-
derstandingevaluationonnaturalspeech,”inIEEEICASSP,2022,
[11] E.Lakomkin,C.Wu,Y.Fathullah,O.Kalinli,M.L.Seltzer,and
pp.7927–7931.
C.Fuegen,“End-to-endspeechrecognitioncontextualizationwith
largelanguagemodels,”arXivpreprintarXiv:2309.10917,2023. [29] A.Conneau,M.Ma,S.Khanuja,Y.Zhang,V.Axelrod,S.Dalmia,
J. Riesa, C. Rivera, and A. Bapna, “Fleurs: Few-shot learning
[12] J.Wu,Y.Gaur,Z.Chen,L.Zhou,Y.Zhu,T.Wang,J.Li,S.Liu,
evaluationofuniversalrepresentationsofspeech,”inIEEESpoken
B.Ren,L.Liuetal.,“Ondecoder-onlyarchitectureforspeech-
LanguageTechnologyWorkshop(SLT),2022,pp.798–805.
to-textandlargelanguagemodelintegration,”inIEEEAuto-matic
[30] S.Shon, S.Arora, C.-J.Lin, A.Pasad, F.Wu, R.Sharma, W.-
SpeechRecognitionandUnderstandingWorkshop(ASRU),2023.
L.Wu,H.-Y.Lee,K.Livescu,andS.Watanabe,“Sluephase-2:A
[13] Y.Fathullah,C.Wu,E.Lakomkin,J.Jia,Y.Shangguan,K.Li,
benchmarksuiteofdiversespokenlanguageunderstandingtasks,”
J.Guo,W.Xiong,J.Mahadeokar,O.Kalinlietal.,“Prompting
arXivpreprintarXiv:2212.10525,2022.
largelanguagemodelswithspeechrecognitionabilities,” arXiv
[31] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang,
preprintarXiv:2307.11795,2023.
L.Wang,andW.Chen,“Lora:Low-rankadaptationoflargelan-
[14] W.Yu,C.Tang,G.Sun,X.Chen,T.Tan,W.Li,L.Lu,Z.Ma,and
guagemodels,”arXivpreprintarXiv:2106.09685,2021.
C.Zhang,“Connectingspeechencoderandlargelanguagemodel
[32] A.Q.Jiang,A.Sablayrolles,A.Mensch,C.Bamford,D.S.Chap-
forasr,”arXivpreprintarXiv:2309.13963,2023.
lot,D.d.l.Casas,F.Bressand,G.Lengyel,G.Lample,L.Saulnier
[15] E. Tsunoo, H. Futami, Y. Kashiwagi, S. Arora, and S. Watan-
etal.,“Mistral7b,”arXivpreprintarXiv:2310.06825,2023.
abe, “Decoder-only architecture for speech recognition with
[33] X. Chang, B. Yan, Y. Fujita, T. Maekaku, and S. Watanabe,
ctc prompts and text data augmentation,” arXiv preprint
“Exploration of efficient end-to-end asr using discretized input
arXiv:2309.08876,2023.
fromself-supervisedlearning,”arXivpreprintarXiv:2305.18108,
[16] Y. Hono, K. Mitsuda, T. Zhao, K. Mitsui, T. Wakatsuki, and
2023.
K. Sawada, “An integration of pre-trained speech and lan-
[34] S. Kim, T. Hori, and S. Watanabe, “Joint CTC-attention based
guagemodelsforend-to-endspeechrecognition,”arXivpreprint
end-to-endspeechrecognitionusingmulti-tasklearning,”inIEEE
arXiv:2312.03668,2023.
ICASSP,2017.
[17] Z.Chen,H.Huang,A.Andrusenko,O.Hrinchuk,K.C.Puvvada,
[35] K. Kim, F. Wu, Y. Peng, J. Pan, P. Sridhar, K. J. Han, and
J. Li, S. Ghosh, J. Balam, and B. Ginsburg, “SALM: Speech-
S. Watanabe, “E-branchformer: Branchformer with enhanced
augmented language model with in-context learning for speech
mergingforspeechrecognition,”inIEEESpokenLanguageTech-
recognition and translation,” arXiv preprint arXiv:2310.09424,
nologyWorkshop(SLT),2022,pp.84–91.
2023.