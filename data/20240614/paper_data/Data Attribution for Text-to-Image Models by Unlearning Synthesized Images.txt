Data Attribution for Text-to-Image Models
by Unlearning Synthesized Images
Sheng-YuWang1 AaronHertzmann2 AlexeiA.Efros3 Jun-YanZhu1 RichardZhang2
1CarnegieMellonUniversity 2AdobeResearch 3UCBerkeley
Abstract
The goal of data attribution for text-to-image models is to identify the training
imagesthatmostinfluencethegenerationofanewimage.Wecandefine‚Äúinfluence‚Äù
by saying that, for a given output, if a model is retrained from scratch without
thatoutput‚Äôsmostinfluentialimages,themodelshouldthenfailtogeneratethat
outputimage. Unfortunately,directlysearchingfortheseinfluentialimagesiscom-
putationallyinfeasible,sinceitwouldrequirerepeatedlyretrainingfromscratch.
We propose a new approach that efficiently identifies highly-influential images.
Specifically,wesimulateunlearningthesynthesizedimage,proposingamethod
toincreasethetraininglossontheoutputimage,withoutcatastrophicforgetting
ofother,unrelatedconcepts. Then,wefindtrainingimagesthatareforgottenby
proxy,identifyingoneswithsignificantlossdeviationsaftertheunlearningprocess,
andlabeltheseasinfluential. Weevaluateourmethodwithacomputationallyin-
tensivebut‚Äúgold-standard‚Äùretrainingfromscratchanddemonstrateourmethod‚Äôs
advantagesoverpreviousmethods.
1 Introduction
Dataattributionfortext-to-imagegenerationaimstoidentifywhichtrainingimages‚Äúinfluenced‚Äùa
givenoutput. Theblack-boxnatureofstate-of-the-artimagegenerationmodels[1,2,3,4,5,6,7,8],
togetherwiththeenormousdatasetsrequired[9],makesitextremelychallengingtounderstandthe
contributions of individual training images. Although generative models can, at times, replicate
trainingdata[10,11],theytypicallycreatesamplesdistinctfromanyspecifictrainingimage.
We believe that a counterfactual definition of ‚Äúinfluence‚Äù best matches the intuitive goal of attri-
bution[12,13]. Specifically, wesaythatacollectionoftrainingimagesisinfluentialforagiven
outputimageif: removingthoseimagesfromthetrainingsetandthenretrainingfromscratchmakes
themodelunabletogeneratethesynthesizedimage. Unfortunately,directlysearchingforthemost
influentialimagesaccordingtothisdefinitioniscomputationallyinfeasiblesinceitwouldrequire
traininganexponentiallylargenumberofnewmodelsfromscratch.
Hence, practical influence estimation requires effective approximations. For example, many ap-
proachesreplaceretrainingwithaclosed-formapproximation,computedseparatelyforeachtraining
image[12,14,15,13,16]. Fortext-to-imageattribution,thesemethodsareoutperformedbysimple
matchingofoff-the-shelfimagefeatures[17]. Wangetal.[18]usecustomizationtostudytheeffect
oftrainingamodeltowardsanexemplar,butfindlimitedgeneralizationtothegenerallarge-scale
training case. We aim for a tractable method that accurately predicts influence according to the
counterfactualdefinition.
We propose an approach to influence prediction with two key ideas (Figure 1). First, we can
approximateremovingatrainingimagefromamodelbyanoptimizationthatwecallunlearning.
Unlearningincreasesthetraininglossofthetargetimagewhileprotectingunrelatedconcepts;wecan
thencomputetraininglossfortheoriginalsynthesizedimage. However,directlyapplyingthisidea
Preprint.Underreview.
4202
nuJ
31
]VC.sc[
1v80490.6042:viXraTraining Dataset Pretrained Synthesized ùëßÃÇ
ùúÉ
(a) Attribution by
Unlearning
Unlearning
Assess influence ùúÉ!"ÃÇ
(by loss increase)
Remove Top-K Unlearned
Influential Images
Failed
Counterfactual
Resynthesis
(b) Counterfactual Retrain
ùúÉ!$
Evaluation "ÃÇ
Figure1: (a)Ouralgorithm: Weproposeanewdataattributionmethodusingmachineunlearning.
BymodifyingthepretrainedmodelŒ∏tounlearnthesynthesizedresultzÀÜ,themodelalsoforgetsthe
influentialtrainingimagescrucialforgeneratingthatspecificresult. (b)Evaluation: Wevalidateour
methodthroughcounterfactualevaluation,whereweretrainthemodelwithoutthetopK influential
imagesidentifiedbyourmethod. Whentheseinfluentialimagesareremovedfromthedataset,the
modelfailstogeneratethesynthesizedimage.
wouldrequireunlearningseparatelyforeachtrainingimage. Oursecondmainideaistoreversethe
roles: weunlearnthesynthesizedimage,andthenevaluatewhichtrainingimagesarerepresented
worse by the new model. This requires only one unlearning optimization, rather than a separate
unlearningforeachtrainingimage.
Themethodologyforunlearningisimportant.Unlearningasynthesizedimagebynaivelymaximizing
its loss leads to catastrophic forgetting [19], where the model fails to generate other unrelated
concepts as well. Inspired by work on unlearning data for classifiers [20, 21], we mitigate this
issuebyregularizinggradientdirectionsusingFisherinformationtoretainpretrainedinformation.
Additionally,wefindthatupdatingonlythekeyandvaluemappingsinthecross-attentionlayers
improvesattributionperformance. Weshowhow‚Äúinfluencefunctions‚Äù[13,15]canbeunderstoodas
approximationstounlearninginAppendixA,buttheyarelimitedbytheirclosed-formnature.
We perform a rigorous counterfactual validation: removing a predicted set of influential images
fromthetrainingset,retrainingfromscratch,andthencheckingthatthesynthesizedimageisno
longerrepresented. WeuseMSCOCO[22](‚àº100kimages),whichallowsforretrainingmodels
withinareasonablecomputebudget. Wealsotestonapublicly-availableattributionbenchmark[18]
usingcustomizedtext-to-imagemodels[23]. Ourexperimentsshowthatouralgorithmoutperforms
priorworkonbothbenchmarks,demonstratingthatunlearningsynthesizedimagesisaneffective
waytoattributetrainingimages. Ourcodeisavailableat: https://peterwang512.github.io/
AttributeByUnlearning.
Insummary,ourcontributionsare:
‚Ä¢ Weproposeanovelmethodfordataattributionfortext-to-imagemodels,unlearningthesynthe-
sizedimageandidentifyingwhichtrainingimagesareforgotten.
‚Ä¢ Wefindandablatethecomponentsformakingunlearningefficientandeffective,employingFisher
informationandtuningacriticalsetofweights.
‚Ä¢ Werigorouslyshowthatourmethodiscounterfactualpredictivebyomittinginfluentialimages,
retraining,andcheckingthatthesynthesizedimagecannotberegenerated. Alongwiththeexisting
CustomizedModelbenchmark,weshowourmethodidentifiesinfluentialimagesmoreeffectively
thanrecentbaselinesbasedoncustomizationandinfluencefunctions.
22 RelatedWork
Attribution. Influencefunctions[15,13]approximatehowtheobjectivefunctionofatestdatapoint
wouldchangeafterperturbingatrainingdatapoint. Onemaythenpredictattributionaccordingtothe
trainingpointsthatcanproducethelargestchanges. KohandLiang[13]proposedusinginfluence
functions to understand model behavior in deep discriminative models. The influence function
requirescalculatingaHessianofthemodelparameters,forwhichvariousefficientalgorithmshave
been proposed, such as inverse hessian-vector products [13], Arnoldi iteration [24], Kronecker
factorization[25,26,27],Gauss-Newtonapproximation[16],andnearestneighborsearch[28].
Other methods explore different approaches. Inspired by the game-theory concept of Shapley
value[29],severalmethodstrainmodelsonsubsetsoftrainingdataandestimatetheinfluenceof
a training point by comparing the models that had that training point in their data to those that
didnot[30,31,32,33]. Pruthietal.[34]estimateinfluencebytrackingtrain-testimagegradient
similarityoverthecourseofmodeltraining.
Recentmethodsperformattributionfordiffusion-basedimagegeneration. Wangetal.[18]proposed
attribution by model customization [35, 23], where a pretrained model is influenced by tuning
towardsanexemplarconcept. SeveralworksadaptTRAK[16],aninfluencefunction-basedmethod,
todiffusionmodels,extendingitbyattributingatspecificdenoisingtimesteps[12],orbyimproving
gradient estimation and using Tikhonov regularization [14]. Unlike these methods, our method
performs attribution by directly unlearning a synthesized image and tracking the effect on each
trainingimage. Ourmethodoutperformsexistingmethodsinattributingbothcustomizedmodelsand
text-to-imagemodels.
Machineunlearning. Machineunlearningseekstoefficiently‚Äúremove‚Äùspecifictrainingdatapoints
from a model. Recent studies have explored concept erasure for text-to-image diffusion models,
specifiedbyatextrequest[36,37,38,39,40],whereasweremoveindividualimages.Whileforgetting
maybeachievedusingmultiplemodelstrainedwithsubsetsofthedatasetbeforehand[41,42,43],
doingsoisprohibitivelyexpensiveforlarge-scalegenerativemodels.
Instead,ourapproachfollowsunlearningmethodsthatupdatemodelweightsdirectly[20,44,45,46,
21]. ThemajorityofpriormethodsusetheFisherinformationmatrix(FIM)toapproximateretraining
without forgetting other training points [20, 45, 47, 21, 48, 49]. In particular, we are inspired by
theworksfromGuoetal.[20]andTannoetal.[21],whichdrawaconnectionbetweenFIM-based
machineunlearningmethodsandinfluencefunctions. Weshowthatunlearningcanbeefficiently
appliedtotheattributionproblem,by‚Äúunlearning‚Äùoutputimagesinsteadoftrainingdata.
Replicationdetection. Shenetal.[50]identifyrepeatedpictorialelementsinarthistory. Somepalli
etal.[11]andCarlinietal.[10]investigatetext-to-imagesynthesisofperceptually-exactcopiesof
trainingimages. Unliketheseworks,ourworkfocusesondataattributionformoregeneralsynthesis
settingsbeyondreplication.
3 ProblemSettingandEvaluation
Our goal is to attribute a generated image to its training data. We represent the training data as
D ={(x ,c )}N ,wherex‚ààX denotesimagesandcrepresentstheconditioningtext. Alearning
i i i=1
algorithmA:D ‚ÜíŒ∏yieldsparametersofagenerativemodel;forinstance,Œ∏ =A(D)isamodel
trainedonD. Wefocusondiffusionmodelsthatgenerateanimagefromanoisemapœµ‚àºN(0,I).
A generated image from text c is represented as ÀÜx = G (œµ,c). To simplify notation, we write a
Œ∏
text-imagetupleasasingleentity. AsynthesizedpairisdenotedaszÀÜ=(ÀÜx,c),andatrainingpairis
denotedasz =(x ,c )‚àºD. WedenotethelossofanimagexconditionedoncasL(z,Œ∏).
i i i
Next,wedescribethe‚Äúgold-standard‚Äùevaluationmethodthatweusetodefineandevaluateinfluence.
Section4describesourmethodforpredictinginfluentialimages.
Counterfactual evaluation. A reliable data attribution algorithm should accurately reflect a
counterfactualprediction.Thatis,ifanalgorithmcanidentifyasetoftrulyinfluentialtrainingimages,
thenamodeltrainedwithout thoseimageswouldbeincapableofgeneratingorrepresentingthat
image. AsnotedbyIlyasetal.[33]andParketal.[16],counterfactualpredictioniscomputationally
intensivetovalidate. Assuch,theseworksintroducetheLineardatamodeling(LDS)scoreasan
3efficientproxy,butwiththeassumptionthatdataattributionmethodsareadditive,whichdoesnot
holdforfeaturematchingmethodsandourmethod.
Inourwork, weinvestsubstantialcomputationalresourcestothe‚Äúgoldstandard‚Äùcounterfactual
evaluationwithinourresourcelimits. Thatis,weuseanattributionalgorithmtoidentifyacritical
setofK images,denotedasDK ‚äÇD. Wethentrainagenerativemodelwithoutthoseimagesfrom
zÀÜ
scratch,persynthesizedsampleandperattributionmethod. Despitethecomputationalcost,this
allowsustoprovidethecommunitywithadirectevaluationofcounterfactualprediction,without
relyingonalayerofapproximations. Weformalizeourevaluationschemeasfollows.
Trainingacounterfactualmodel. Forevaluation,anattributionalgorithmisgivenabudgetofK
imagesforattributingasynthesizedsamplezÀÜ,denotedasDK. Wethentrainaleave-K-outmodel
zÀÜ
Œ∏‚àíK fromscratchusingD‚àíK =D\DK,thedatasetwiththeK attributedimagesremoved:
zÀÜ zÀÜ zÀÜ
Œ∏‚àíK =A(D‚àíK), (1)
zÀÜ zÀÜ
Evaluatingthemodel. Wethencomparethis‚Äúleave-K-out‚ÄùmodelagainstŒ∏ =A(D),themodel
0
trainedwiththeentiredataset,andassesshowmuchitlosesitscapabilitytorepresentzÀÜintermsof
boththelosschange‚àÜL(zÀÜ,Œ∏)andthecapabilitytogeneratethesamesample‚àÜG (œµ,c).
Œ∏
First,iftheleave-K-outmodelistrainedwithoutthetopinfluentialimages,itshouldreconstruct
syntheticimagezÀÜmorepoorly,resultinginahigher‚àÜL(zÀÜ,Œ∏):
‚àÜL(zÀÜ,Œ∏)=L(zÀÜ,Œ∏‚àíK)‚àíL(zÀÜ,Œ∏ ). (2)
zÀÜ 0
Second, if properly selected, the leave-K-out model should no longer be able to generate xÀÜ =
G (œµ,c). Fordiffusionmodels,inparticular,wecanrelyonthe‚Äúseedconsistency‚Äùproperty[12,51,
Œ∏
52]. Georgievetal.[12]findthatimagesgeneratedbytwoindependentlytraineddiffusionmodels
fromthesamerandomnoiseœµhavelittlevariations.Theyleveragethispropertytoevaluateattribution
via ‚àÜG (œµ,c), the difference of generated images between Œ∏ and Œ∏‚àíK. An effective attribution
Œ∏ 0 zÀÜ
algorithmshouldleadtoaleave-K-outmodelgeneratingimagesthatdeviatemorefromtheoriginal
images,resultinginalarger‚àÜG (œµ,c)value:
Œ∏
(cid:0) (cid:1)
‚àÜG (œµ,c)=d G (œµ,c),G (œµ,c) , (3)
Œ∏ Œ∏0 Œ∏‚àíK
ÀÜz
where d can be any distance function, such as L2 or CLIP [53]. Georgiev et al. [12] also adopt
‚àÜG (œµ,c)forevaluation. Whileevaluatinglossincreasesandseedconsistencyisspecifictodiffusion
Œ∏
models,theoverarchingideaofretrainingandevaluatingifasynthesizedimageisstillinthemodel
appliesacrossgenerativemodels.
4 AttributionbyUnlearning
Inthis section, weintroduce ourapproach toattribute trainingimages fora text-to-imagemodel
Œ∏ =A(D),trainedondatasetD.Ourgoalistofindthehighlyinfluentialimagesonagivensynthetic
0
imagezÀÜindatasetD.
Formally,wedefineadataattributionalgorithmœÑ,whichgivenaccesstothetrainingdata,model,
andlearningalgorithm,producesasetofscores,denotedbyœÑ(zÀÜ,D,Œ∏,A) ‚àà RN. ThehighestK
influencingimagesarethenselectedaccordingtotheirscores. DuetothetrainingsetsizesDused
intext-to-imagemodels,wesimplifytheproblembyindividuallyestimatingtheinfluenceofeach
trainingpoint: œÑ(zÀÜ,D,Œ∏,A)=[œÑ(zÀÜ,z ),œÑ(zÀÜ,z ),...,œÑ(zÀÜ,z )],whereœÑ representsthefactorized
1 2 N
attributionfunctionthatestimatesinfluencebasedonthesynthesizedsampleandatrainingpoint.
AlthoughœÑ canaccessalltrainingdata,parameters,andthelearningalgorithm,weomitthemfor
notationalsimplicity.
Given infinite compute, one can find critical sets of influential images by training a model from
everypossiblesubsetofK imagesfromscratchandsearchingforonethat‚Äúforgets‚Äùthesynthesized
imagethemost. However,thesearchspaceiscombinatoricandinfeasibleinpractice. Toaddress
this problem, we have made two modifications. First, rather than training from scratch, we use
4model unlearning‚Äîefficiently tuning a pretrained model to remove a data point. Second, rather
thansearchingthroughthecombinatoricspaceoftrainingpoints,weinsteadapplyunlearningtothe
synthesizedimageandthenassesshoweffectivelyeachtrainingimageisalsoforgottenasaresult.
Asaheuristictomeasurethedegreeofremoval,wetrackthetraininglosschangesforeachtraining
imageafterunlearning,andwefindthiseffectivefordataattribution.
Unlearningthesynthesizedimage. AnaiveapproachtounlearnasynthesizedimagezÀÜistosolely
maximizeitslossL(zÀÜ,Œ∏). However,onlyoptimizingforthisleadstocatastrophicforgetting[19],
wherethemodelcannolongerrepresentotherconcepts.
Instead,weproposetoretaintheinformationfromtheoriginaldatasetwhile‚Äúremoving‚Äùthesyn-
thesizedimage,asthoughithadbeenpartoftraining. Givenmodeltrainedontheoriginaldataset
Œ∏ =A(D)andasynthesizedimagezÀÜ,wecomputeanewmodelŒ∏ =A(D\zÀÜ),withzÀÜremoved.
0 ‚àízÀÜ
Hereusethesetremovalnotation\tospecifya‚Äúnegative‚Äùdatapointinthedataset. Concretely,we
solveforthefollowingobjectivefunction,usingelasticweightconsolidation(EWC)loss[19]asan
approximation:
(cid:88)
LzÀÜ (Œ∏)=‚àíL(zÀÜ,Œ∏)+ L(z,Œ∏)
unlearn
z‚ààD (4)
N
‚âà‚àíL(zÀÜ,Œ∏)+ (Œ∏‚àíŒ∏ )TF(Œ∏‚àíŒ∏ ),
2 0 0
whereF istheFisherinformationmatrix,whichisapproximatedasadiagonalformforcomputational
efficiency. F approximatesthetrainingdatalosstothesecond-order[54],atechniquewidelyusedin
continuallearning[19,55]. ThisenablesustosolveforthenewmodelparametersŒ∏ efficiently,by
‚àízÀÜ
initializingfromthepretrainedmodelŒ∏ . WeoptimizethislosswithNewtonupdates:
0
Œ±
Œ∏ ‚ÜêŒ∏+ F‚àí1‚àáL(zÀÜ,Œ∏), (5)
N
where Œ± controls the step size, F‚àí1 is the inverse of the Fisher information matrix. In practice,
Newtonupdatesallowustoachieveeffectiveattributionwithfewiterationsand,insomecases,as
fewasonestep. WedenotetheunlearnedmodelasŒ∏ . WeprovidedetailsoftheEWClossand
‚àízÀÜ
NewtonupdateinAppendixA.
Attributionusingtheunlearnedmodel. AfterweobtaintheunlearnedmodelŒ∏ ,wedefineour
‚àízÀÜ
attributionfunctionœÑ bytrackingthetraininglosschangesforeachtrainingsamplez:
œÑ(zÀÜ,z)=L(z,Œ∏ )‚àíL(z,Œ∏ ). (6)
‚àízÀÜ 0
ThevalueœÑ(zÀÜ,z)isexpectedtobeclosetozeroformostunrelatedtrainingimagessincetheEWC
lossusedinobtainingŒ∏ actsasaregularizationtopreservetheoriginaltrainingdataset. Ahigher
‚àízÀÜ
valueofœÑ(zÀÜ,z)indicatesthatunlearnedmodelŒ∏ nolongerrepresentsthetrainingsamplez.
‚àízÀÜ
Relation with influence functions. Our method draws a parallel with the influence function,
which aims to estimate the loss change of zÀÜ by removing a training point z. However, training
leave-one-outmodelsoneverytrainingpointisgenerallyinfeasible. Instead,theinfluencefunction
reliesonaheavierapproximationtoestimatetheeffectofperturbingasingletrainingpoint,rather
thanactuallyforgettingthetrainingsamples. Incontrast,ourapproachonlyrequiresrunningthe
unlearningalgorithmonceforagivensynthesizedimagequery. Thisallowsustouseamoremild
approximationandobtainamodelthatforgetsthesynthesizedsample. Guoetal.[20]andTannoet
al.[21]exploreasimilarformulationforunlearningtrainingimagesanddrawaconnectionbetween
theirunlearningalgorithmsandinfluencefunction. Ourapproachaimstounlearnthesynthesized
imageinstead,whichconnectstoinfluencefunctioninasimilarfashion. Wediscussourmethod‚Äôs
connectiontoinfluencefunctioninmoredetailinAppendixA.3.
Optimizingasubsetofweights. Tofurtherregularizetheunlearningprocess,weoptimizeasmall
subsetofweights,specificallyWk andWv,thekeyandvalueprojectionmatricesincross-attention
layers[56,57]. Intext-to-imagemodels,cross-attentionfacilitatestext-to-imagebinding,whereWk
identifieswhichfeaturesmatcheachtexttoken,whileWv determineshowtomodifythefeatures
forthematchedpatches. WefindthatperformingunlearningWk andWv iseffectiveforattribution.
Priorworksalsoselectthesamesetofparameterstoimprovefine-tuning[23]andunlearning[36].
5Generated Sample
‚ÄúA bus traveling
on a freeway
next to other
traffic.‚Äù
Generated Sample
‚ÄúA man in a
blue coat skiing
through a
snowy field.‚Äù
Figure2: AttributionresultsonMSCOCOmodels. Weshowgeneratedsamplesusedasaquery
ontheleft,withtrainingimagesbeingidentifiedbydifferentmethodsontheright. Qualitatively,our
methodretrievesimageswithmoresimilarvisualattributes. Notably,ourmethodbettermatchesthe
posesofthebuses(consideringrandomflipsduringtraining)andtheposesandenumerationofskiers.
Implementationdetails. Weconductourstudiesontext-conditionedlatentdiffusionmodels[2].
SincediffusionmodelsaretypicallytrainedwithT =1000steps,evaluatingthelossforalltimesteps
iscostly. Therefore,wespeedupcomputationbycalculatingthelossL(z,Œ∏)withstridedtimesteps;
wefindthatusingastrideof50or100leadstogoodattributionperformance. Forcalculatingthe
losschange‚àÜL(zÀÜ,Œ∏)duringevaluation,wetakeafinerstrideof5stepstoensureamoreaccurate
estimationoftheDDPMloss. Additionaldetailsofourmethod,includinghyperparameterchoices,
areprovidedinAppendixB.
5 Experiments
Wevalidateourmethodintwoways. Thefirstisareliable,‚Äúgold-standard‚Äù,butintensive‚Äìretraining
a model from scratch without influential images identified by the algorithm. In Section 5.1, we
performthisevaluationonamedium-sizeddatasetof100kMSCOCOimages[22]. Secondly,in
Section5.2,weevaluateourmethodontheCustomizedModelBenchmark[18],whichmeasures
attributionthroughcustomizationonStableDiffusionmodels[2]. Thistestshowwellourmethod
canapplytolarge-scaletext-to-imagemodels.
5.1 Leave-K-outcounterfactualevaluation
Evaluation protocol. We select latent diffusion models [2] trained on MSCOCO [22], as its
moderatesize(118,287images)allowsforrepeatedleave-K-outretraining. Specifically,weusethe
pre-trainedmodelevaluatedinGeorgievetal.[12]. AsoutlinedinSection3,foreachsynthesized
imagezÀÜ,wemeasuretheleave-K-outmodel‚Äôs(1)losschange‚àÜL(zÀÜ,Œ∏)and(2)deviationofgener-
ation‚àÜG (œµ,c). Thedeviationismeasuredbymeansquareerror(MSE)andCLIPsimilarity[53].
Œ∏
Wecollect110synthesizedimagesfromthepre-trainedmodelforevaluation, withdifferenttext
promptssourcedfromtheMSCOCOvalidationset. Weevaluate‚àÜL(zÀÜ,Œ∏)and‚àÜG (œµ,c)forall
Œ∏
synthesizedimagesandreportmeanandstandarderror.
Wecompareourmethodwithseveralbaselines:
‚Ä¢ Imagesimilarity: pixelspace,CLIPimagefeatures[53],DINO[17],andDINOv2[58]
‚Ä¢ Textsimilarity: CLIPtextfeatures
6
sruO
ONID
KARTyenruoJ
sruO
ONID
KARTyenruoJLoss Change vs. K Images Removed
0.03
random CLIP TRAK
Pixel CLIP (AbC) JourneyTRAK
0.02
CLIP Text DINO Ours
DINOv2 DINO (AbC)
0.01
0.00
500 1000 4000 10000 Full dataset
K images removed
Equivalent K Random Images Removed
Full dataset
100000 k=500
k=1000
75000 k=4000
50000
25000
0
Pixel CLIP Text DINOv2 CLIP CLIP (AbC) DINO DINO (AbC) TRAK JourneyTRAK Ours
Figure3: Evaluatinglosschangesinleave-K-outmodels. GivenasynthesizedimagezÀÜ,wetrain
leave-K-out models for each of the attribution methods and track ‚àÜL(zÀÜ,Œ∏), the increase in loss
change. (Top)WeplotDDPMlosschangeversusK. Thelosschangebecomesmoresignificantwith
alargerK value,andourmethod(brown)yieldsthelargestlosschangeforeveryK,indicatingour
methodmoresuccessfullyidentifiestheinfluentialimages. Thepurplecurverepresentsmodelswhere
randomimagesareremoved,servingasareference. Weshowerrorbarsrepresenting1standarderror
inthisplot(whicharesmallandnegligible). (Bottom)Foreachmethod,weshowtheequivalent
numberofrandomimagesneededtoachievealosschange.
‚Ä¢ AttributionbyCustomization[18](AbC):fine-tunedimagefeaturestrainedontheCustomized
Modelbenchmark,denotedasCLIP(AbC)andDINO(AbC)
‚Ä¢ Influencefunction: BothTRAKandJourneyTRAK[12]areinfluencefunction-basedmethods
thatmatchthelossgradientsoftrainingandsynthesizedimages, usingrandomprojectionfor
efficiency. Both methods run the influence function on multiple models trained on the same
dataset (20 in this test) and average the scores. The main difference is in the diffusion loss
calculation: TRAKrandomlysamplesandaveragesthelossovertimesteps,whileJourneyTRAK
calculatesitonlyatt=400forsynthesizedimagesduringcounterfactualevaluation.
‚Ä¢ Random: WetrainmodelswithK randomimagesremoved,using10modelspervalueofK,
sweepingthroughK =500,1000,2000,3000,4000,andthen5000tothefulldatasetsizeby
incrementsof2000.
For attribution methods, we use K = 500,1000, and 4000, representing approximately 0.42%,
0.85%, and 3.4% of the MSCOCO dataset, respectively. Densely sweeping K is generally not
feasibleforattributionmethods,aseachqueriedsynthesizedimagerequiresretrainingadifferent
setofK images. Toprovidebetterintuition,wereportthenumberofrandomimagesthatneedto
beremovedfromthedatasettoachievethesameforgettingeffectasremovingK imagesfroman
attributionmethod.
Visualcomparisonofattributedimages. InFigure2,wefindthatourmethod,alongwithother
baselines,canattributesynthesizedimagestovisuallysimilartrainingimages. However,ourmethod
moreconsistentlyattributesimageswiththesamefine-grainedattributes,suchasobjectlocation,
pose,andcounts.WeprovidemoreresultsinAppendixC.1.Next,weproceedwiththecounterfactual
analysis,wherewetestwhethertheseattributedimagesaretrulyinfluential.
Trackinglosschangesinleave-K-outmodels. First,wereportthechangeinDDPMlossforleave-
K-outmodelsinFigure3. Matchinginplainpixelortextfeaturespaceyieldsweakperformance,
while deep image features, particularly DINO, perform better. Interestingly, DINO outperforms
influencefunctionmethodsatK = 4000,despitenotbeingtrainedspecificallyfortheattribution
task. Fine-tuningimagefeatureswiththeCustomizedModelbenchmark,suchasCLIP(AbC),shows
7
egnahC
ssoL
MPDD
devomeR
segamI
modnaR
KK=500 K=1000 K=4000 K=500 K=1000 K=4000
Generated Sample Generated Sample
‚ÄúA bus traveling ‚ÄúA man in a
on a freeway blue coat skiing
next to other through a
traffic.‚Äù snowy field.‚Äù
Figure4: Leave-K-outanalysisforMSCOCOmodels. Wecompareimagesacrossourmethodand
baselinesgeneratedbyleave-K-outmodels,usingdifferentK values,allunderthesamerandom
noiseandtextprompt.Asignificantdeviationinregenerationindicatesthatcritical,influentialimages
wereidentifiedbytheattributionalgorithm. Whilebaselinesregeneratesimilarimagestotheoriginal,
our method generates ones that deviate significantly, even with as few as 500 influential images
removed(‚àº0.42%ofthedataset).
‚ÄúA motorcycle and Cropped
Attributed training images
a stop sign.‚Äù Query
Figure 5: Spatially-localized attribution. Given a synthesized image (left), we crop regions
containing specific objects using GroundingDINO [59]. We attribute each object separately by
onlyrunningforgettingonthepixelswithinthecroppedregion. Ourmethodcanattributedifferent
synthesizedregionstodifferenttrainingimages.
someimprovement. However,ingeneral,theimprovementislimited,indicatingthattransferring
fromattributingcustomizedmodelstogeneralmodelsremainschallenging[18].
Amonginfluencefunctions,TRAKsignificantlyoutperformsJourneyTRAK.Wehypothesizethat
thisisbecauseJourneyTRAKcollectsgradientsonlyfordenoisinglossattimestept=400,making
itlesseffectiveforidentifyinginfluentialimagesthataffecttheDDPMtraininglossdifferentnoise
levels. OurmethodconsistentlyperformsbestacrossallK values,outperformingbothinfluence
functions and feature-matching methods. For example, removing 500 images (just 0.42% of the
dataset)usingourmethodisequivalenttoremoving57.5krandomimages(48.6%ofthedataset)vs.
51.6kimages(43.6%)and‚àº44.3kimages(37.5%)imagesforthebest-performingbaselines,TRAK
andDINO.
Deviationofgeneratedoutputinleave-K-outmodels. Figure4showsthedeviationingenerated
outputsforleave-K-outmodels,whereallimagesaregeneratedusingthesamenoiseinputandtext
prompt. Ourmethodyieldsthelargestdeviationwithasmallbudget. DINOisthesecondstrongest
methodinthisevaluation,whereitalsostartssignificantlyalteringthegeneratedoutputwithhigher
K. Incontrast,influencefunctions,includingTRAKandJourneyTRAK,performsubparinthistest,
whereasJourneyTRAKoutperformsTRAKinthiscontext. Wereportquantitativeresults,including
MSEandCLIPsimilaritiesofimagesbetweenthepre-trainedandleave-K-outmodels,alongwith
ablationstudiesandmorequalitativeresults,inAppendixC.1.
Spatially-localizedattribution. Whileourformulationiswrittenforwholeimages,wecanrun
attributiononspecificregionswithlittlemodification. WedemonstratethisinFigure5onagenerated
image of a motorcycle and stop sign, using bounding boxes identified by GroundingDINO [59].
Foreachdetectedobject,werunourunlearning(usingthesameprompt)onthatspecificobjectby
optimizingtheobjectiveonlywithintheboundingbox. Bydoingso,weattributedifferenttraining
imagesforthestopsignandmotorcycle.
8
sruO
ONID
KARTyenruoJ
sruO
ONID
KARTyenruoJ1.0 1.0
Ours Ours
CLIP (AbC) CLIP (AbC)
0.8 0.8
DINO (AbC) DINO (AbC)
CLIP CLIP
0.6 DINO 0.6 DINO
0.4 0.4
0.2 0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Object-Centric Models (Recall@10) Object-Centric Models (mAP)
Figure6: CustomizedModelbenchmark[18]. WereportRecall@10(left)andmAP(right)and
showperformanceonartist-stylemodels(y-axis)vs. object-centricmodels(x-axis). Onobject-style
models,ourmethodperformsonparwithAbCfeatures,whichweredirectlytunedonthebenchmark,
whilesignificantlyoutperformingthemonartist-stylemodels. Weplot1standarderroronbothaxes.
Object Centric Artistic Style
Generated Sample Generated Sample
‚ÄúAn ink drawing of ‚ÄúA picture of tree
V* turtle‚Äù in the style of
V* art‚Äù
Figure7: QualitativeexamplesontheCustomizedModelbenchmark. Theredboxesindicate
groundtruthexemplarimagesusedforcustomizingthemodel. Bothourmethodandbaselinessuc-
cessfullyidentifytheexemplarimagesonobject-centricmodels(left),whileourmethodoutperforms
thebaselineswithartisticstylemodels(right).
5.2 CustomizedModelBenchmark
Wangetal.[18]focusonaspecializedformofattribution: attributingcustomizedmodelstrainedon
anindividualorafewexemplarimages. Thisapproachprovidesgroundtruthattributionsincethe
imagesgeneratedbycustomizedmodelsarecomputationallyinfluencedbyexemplarimages. While
thisevaluationhaslimitedgeneralizationtoattributionperformancewithlargertrainingsets,itisthe
onlytractableevaluationforattributinglarge-scaletext-to-imagemodelstodate.
Evaluationprotocol. SincetheCustomizedModelBenchmarkhasgroundtruth,theproblemis
evaluatedasaretrievaltask. WereportRecall@KandmAP,measuringthesuccessofretrieving
theexemplarimagesamongstasetincluding100KLAIONimages. WecomparewithWangetal.‚Äôs
feature-matchingapproachthatfinetunesontheCustomizedModeldataset, referredtoasDINO
(AbC)andCLIP(AbC).Forourevaluation,weselectedasubsetofthedatasetcomprising20models:
10object-centricand10artist-stylemodels. Weselect20synthesizedimageswithdifferentprompts
foreachmodel,resultingin400synthesizedimagequeries.
ComparingwithAbCfeatures. WereportRecall@10andmAPinFigure6. Ourmethodperforms
onparwithbaselineswhentestingonobject-centricmodels,whilesignificantlyoutperformingthem
onartist-stylemodels. AlthoughCLIP(AbC)andDINO(AbC)arefine-tunedforthisattribution
task,thefeaturematchingapproachcansometimesconfusewhethertoattributeasynthesizedimage
tostyle-relatedorobject-relatedimages. Incontrast, ourmethod, whichhasaccesstothemodel
itself,tracesinfluentialtrainingimagesmoreeffectively. InFigure7,weshowaqualitativeexample.
WhileDINO(AbC)andCLIP(AbC)canretrievevisuallyorsemanticallysimilarimages,ourmethod
successfullyidentifiestheexemplarsinbothcases. WeincludeablationstudiesinAppendixC.2.
6 Discussion,BroaderImpacts,andLimitations
Generative models have entered the public consciousness, spawning companies and ecosystems
thataredeeplyimpactingthecreativeindustry. Thetechnologyraiseshigh-stakesethicalandlegal
questions surrounding the authorship of generated content [60, 61, 62, 63]. Data attribution is a
critical piece of understanding the behavior of generative models, with potential applications in
9
)01@llaceR(
sledoM
elytS-tsitrA
sruO
)CbA(
ONID
)CbA(
PILC
)PAm(
sledoM
elytS-tsitrA
sruO
)CbA(ONID
)CbA(PILCinforming a compensation model for rewarding contributors for training data. In addition, data
attributioncanjoinotherworks[64,65,66,67]asasetoftoolsthatallowenduserstointerprethow
andwhyamodelbehaves,enablingamoretrustworthyenvironmentformachinelearningmodels.
Ourworkproposesamethodfordataattributionfortext-to-imagemodels,leveragingmodelunlearn-
ing. Weprovideacounterfactualvalidation,verifyingthatremovingtheidentifiedinfluentialimages
indeeddestroysthetargetimage. Whileourmethodempiricallydemonstratesthatunlearningcan
be effectively used, work remains to make this practical. Though our model unlearns efficiently,
estimatingthereconstructionlossonthetrainingsetremainsabottleneck,asseveralforwardpasses
arerequiredoneachtrainingestimate. Whileourevaluationshowedthatunlearningisusefulfor
attribution,directevaluationofunlearningalgorithmsforlargegenerativemodelsremainsanopen
researchchallenge. Furthermore,tofindacriticalsetofimages,ourmethodandbaselinesassign
influencescorestoindividualimagesandsortthem.However,groupsofimagesmayhaveinteractions
thatarenotcapturedinsuchasystem. Furthermore,ourmethodandbaselinesexploreattributionof
thewholeimage,whilefinerattributiononindividualaspectsoftheimage,suchasstyle,structure,or
individualsegments,areoffurtherinterest.
Acknowledgements. We thank Kristian Georgiev for answering all of our inquiries regarding
JourneyTRAKimplementationandevaluation,andprovidingustheirmodelsandanearlierversion
ofJourneyTRAKcode. WethankNupurKumari,KangleDeng,GraceSuforfeedbackonthedraft.
ThisworkispartlysupportedbythePackardFellowship,JPMCFacultyResearchAward,andNSF
IIS-2239076.
References
[1] AlexNichol,PrafullaDhariwal,AdityaRamesh,PranavShyam,PamelaMishkin,BobMcGrew,Ilya
Sutskever,andMarkChen. Glide:Towardsphotorealisticimagegenerationandeditingwithtext-guided
diffusionmodels. arXivpreprintarXiv:2112.10741,2021.
[2] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBj√∂rnOmmer. High-resolution
image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern
Recognition(CVPR),2022.
[3] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen.Hierarchicaltext-conditional
imagegenerationwithcliplatents. arXivpreprintarXiv:2204.06125,2022.
[4] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,
AlexanderKu,YinfeiYang,BurcuKaragolAyan,BenHutchinson,WeiHan,ZaranaParekh,XinLi,Han
Zhang,JasonBaldridge,andYonghuiWu. Scalingautoregressivemodelsforcontent-richtext-to-image
generation. arXivpreprintarXiv:2206.10789,2022.
[5] ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyDenton,SeyedKamyarSeyed
Ghasemipour,BurcuKaragolAyan,SSaraMahdavi,RaphaGontijoLopes,TimSalimans,HoJonathan,
DavidJFleet,andMohammadNorouzi. Photorealistictext-to-imagediffusionmodelswithdeeplanguage
understanding. arXivpreprintarXiv:2205.11487,2022.
[6] OranGafni,AdamPolyak,OronAshual,ShellySheynin,DeviParikh,andYanivTaigman. Make-a-scene:
Scene-basedtext-to-imagegenerationwithhumanpriors. arXivpreprintarXiv:2203.13131,2022.
[7] MingukKang,Jun-YanZhu,RichardZhang,JaesikPark,EliShechtman,SylvainParis,andTaesung
Park. Scalingupgansfortext-to-imagesynthesis. InIEEEConferenceonComputerVisionandPattern
Recognition(CVPR),2023.
[8] HuiwenChang,HanZhang,JarredBarber,AaronMaschinot,JoseLezama,LuJiang,Ming-HsuanYang,
KevinPatrickMurphy,WilliamT.Freeman,MichaelRubinstein,YuanzhenLi,andDilipKrishnan. Muse:
Text-to-imagegenerationviamaskedgenerativetransformers. InInternationalConferenceonMachine
Learning(ICML),2023.
[9] ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeGordon,RossWightman,MehdiCherti,
TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,etal. Laion-5b:Anopenlarge-scale
datasetfortrainingnextgenerationimage-textmodels. arXivpreprintarXiv:2210.08402,2022.
[10] NicholasCarlini,JamieHayes,MiladNasr,MatthewJagielski,VikashSehwag,FlorianTram√®r,Borja
Balle,DaphneIppolito,andEricWallace. Extractingtrainingdatafromdiffusionmodels. arXivpreprint
arXiv:2301.13188,2023.
10[11] GowthamiSomepalli,VasuSingla,MicahGoldblum,JonasGeiping,andTomGoldstein. Diffusionartor
digitalforgery?investigatingdatareplicationindiffusionmodels. arXivpreprintarXiv:2212.03860,2022.
[12] KristianGeorgiev,JoshuaVendrow,HadiSalman,SungMinPark,andAleksanderMadry. Thejourney,
notthedestination:Howdataguidesdiffusionmodels. arXivpreprintarXiv:2312.06205,2023.
[13] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In
Internationalconferenceonmachinelearning,pages1885‚Äì1894.PMLR,2017.
[14] XiaosenZheng,TianyuPang,ChaoDu,JingJiang,andMinLin. Intriguingpropertiesofdataattribution
ondiffusionmodels. InInternationalConferenceonLearningRepresentations(ICLR),2024.
[15] FrankR.Hampel. Theinfluencecurveanditsroleinrobustestimation. JournaloftheAmericanStatistical
Association,1974.
[16] Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. Trak:
Attributingmodelbehavioratscale. arXivpreprintarXiv:2303.14186,2023.
[17] MathildeCaron,HugoTouvron,IshanMisra,Herv√©J√©gou,JulienMairal,PiotrBojanowski,andArmand
Joulin. Emergingpropertiesinself-supervisedvisiontransformers. InIEEEInternationalConferenceon
ComputerVision(ICCV),2021.
[18] Sheng-Yu Wang, Alexei A. Efros, Jun-Yan Zhu, and Richard Zhang. Evaluating data attribution for
text-to-imagemodels. InIEEEInternationalConferenceonComputerVision(ICCV),2023.
[19] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, KieranMilan, JohnQuan, TiagoRamalho, AgnieszkaGrabska-Barwinska, etal. Overcoming
catastrophicforgettinginneuralnetworks. ProceedingsoftheNationalAcademyofSciences(PNAS),
114(13):3521‚Äì3526,2017.
[20] ChuanGuo,TomGoldstein,AwniHannun,andLaurensVanDerMaaten. Certifieddataremovalfrom
machinelearningmodels. InInternationalConferenceonMachineLearning(ICML),2020.
[21] RyutaroTanno,MelanieFPradier,AdityaNori,andYingzhenLi. Repairingneuralnetworksbyleaving
therightpastbehind. InConferenceonNeuralInformationProcessingSystems(NeurIPS),2022.
[22] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,PiotrDoll√°r,
and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on
ComputerVision(ECCV),2014.
[23] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept
customizationoftext-to-imagediffusion.InIEEEConferenceonComputerVisionandPatternRecognition
(CVPR),2023.
[24] AndreaSchioppa,PolinaZablotskaia,DavidVilar,andArtemSokolov. Scalingupinfluencefunctions. In
ProceedingsoftheAAAIConferenceonArtificialIntelligence,2022.
[25] JamesMartensandRogerGrosse. Optimizingneuralnetworkswithkronecker-factoredapproximate
curvature. InInternationalConferenceonMachineLearning(ICML),2015.
[26] ThomasGeorge,C√©sarLaurent,XavierBouthillier,NicolasBallas,andPascalVincent. Fastapproxi-
matenaturalgradientdescentinakroneckerfactoredeigenbasis. InConferenceonNeuralInformation
ProcessingSystems(NeurIPS),2018.
[27] RogerGrosse,JuhanBae,CemAnil,NelsonElhage,AlexTamkin,AmirhosseinTajdini,BenoitSteiner,
DustinLi,EsinDurmus,EthanPerez,etal. Studyinglargelanguagemodelgeneralizationwithinfluence
functions. arXivpreprintarXiv:2308.03296,2023.
[28] Han Guo, Nazneen Fatema Rajani, Peter Hase, Mohit Bansal, and Caiming Xiong. Fastif: Scalable
influencefunctionsforefficientmodelinterpretationanddebugging. arXivpreprintarXiv:2012.15781,
2020.
[29] LloydSShapley. Avalueforn-persongames. PrincetonUniversityPressPrinceton,1953.
[30] AmirataGhorbaniandJamesZou. Datashapley: Equitablevaluationofdataformachinelearning. In
InternationalConferenceonMachineLearning,pages2242‚Äì2251.PMLR,2019.
11[31] Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nick Hynes, Nezihe Merve G√ºrel, Bo Li,
CeZhang,DawnSong,andCostasJSpanos. Towardsefficientdatavaluationbasedontheshapleyvalue.
InThe22ndInternationalConferenceonArtificialIntelligenceandStatistics,pages1167‚Äì1176.PMLR,
2019.
[32] VitalyFeldmanandChiyuanZhang. Whatneuralnetworksmemorizeandwhy:Discoveringthelongtail
viainfluenceestimation. ConferenceonNeuralInformationProcessingSystems(NeurIPS),2020.
[33] AndrewIlyas,SungMinPark,LoganEngstrom,GuillaumeLeclerc,andAleksanderMadry. Datamodels:
Predictingpredictionsfromtrainingdata. InInternationalConferenceonMachineLearning(ICML),2022.
[34] GarimaPruthi,FrederickLiu,SatyenKale,andMukundSundararajan. Estimatingtrainingdatainfluence
bytracinggradientdescent. ConferenceonNeuralInformationProcessingSystems(NeurIPS),33:19920‚Äì
19930,2020.
[35] NatanielRuiz,YuanzhenLi,VarunJampani,YaelPritch,MichaelRubinstein,andKfirAberman. Dream-
booth: Fine tuning text-to-image diffusion models for subject-driven generation. In arXiv preprint
arxiv:2208.12242,2022.
[36] NupurKumari,BingliangZhang,Sheng-YuWang,EliShechtman,RichardZhang,andJun-YanZhu.
Ablatingconceptsintext-to-imagediffusionmodels. InIEEEInternationalConferenceonComputer
Vision(ICCV),2023.
[37] RohitGandikota,JoannaMaterzynska,JadenFiotto-Kaufman,andDavidBau. Erasingconceptsfrom
diffusionmodels. InIEEEInternationalConferenceonComputerVision(ICCV),2023.
[38] RohitGandikota,HadasOrgad,YonatanBelinkov,JoannaMaterzyn¬¥ska,andDavidBau. Unifiedconcept
editingindiffusionmodels. InIEEE/CVFWinterConferenceonApplicationsofComputerVision(WACV),
2024.
[39] Chi-PinHuang,Kai-PoChang,Chung-TingTsai,Yung-HsuanLai,andYu-ChiangFrankWang. Re-
celer:Reliableconcepterasingoftext-to-imagediffusionmodelsvialightweighterasers. arXivpreprint
arXiv:2311.17717,2023.
[40] EricZhang,KaiWang,XingqianXu,ZhangyangWang,andHumphreyShi. Forget-me-not:Learningto
forgetintext-to-imagediffusionmodels. arXivpreprintarXiv:2303.17591,2023.
[41] LucasBourtoule,VarunChandrasekaran,ChristopherAChoquette-Choo,HengruiJia,AdelinTravers,
BaiwuZhang, DavidLie, andNicolasPapernot. Machineunlearning. In2021IEEESymposiumon
SecurityandPrivacy(SP),2021.
[42] LauraGraves,VineelNagisetty,andVijayGanesh.Amnesiacmachinelearning.InConferenceonArtificial
Intelligence(AAAI),2021.
[43] AyushKTarun,VikramSChundawat,MurariMandal,andMohanKankanhalli.Fastyeteffectivemachine
unlearning. IEEETransactionsonNeuralNetworksandLearningSystems,2023.
[44] QuocPhongNguyen,BryanKianHsiangLow,andPatrickJaillet. Variationalbayesianunlearning. In
ConferenceonNeuralInformationProcessingSystems(NeurIPS),2020.
[45] AdityaGolatkar,AlessandroAchille,andStefanoSoatto. Eternalsunshineofthespotlessnet:Selective
forgettingindeepnetworks. InIEEEConferenceonComputerVisionandPatternRecognition(CVPR),
2020.
[46] AdityaGolatkar,AlessandroAchille,AvinashRavichandran,MarziaPolito,andStefanoSoatto. Mixed-
privacyforgettingindeepnetworks. InIEEEConferenceonComputerVisionandPatternRecognition
(CVPR),2021.
[47] AyushSekhari,JayadevAcharya,GautamKamath,andAnandaTheerthaSuresh. Rememberwhatyou
wanttoforget: Algorithmsformachineunlearning. InConferenceonNeuralInformationProcessing
Systems(NeurIPS),2021.
[48] YongjingZhang,ZhaoboLu,FengZhang,HaoWang,andShaojingLi. Machineunlearningbyreversing
thecontinuallearning. AppliedSciences,2023.
[49] JackFoster,StefanSchoepf,andAlexandraBrintrup. Fastmachineunlearningwithoutretrainingthrough
selectivesynapticdampening. InConferenceonArtificialIntelligence(AAAI),2024.
12[50] XiShen,AlexeiAEfros,andMathieuAubry. Discoveringvisualpatternsinartcollectionswithspatially-
consistentfeaturelearning. InIEEEConferenceonComputerVisionandPatternRecognition(CVPR),
pages9278‚Äì9287,2019.
[51] XuanSu,JiamingSong,ChenlinMeng,andStefanoErmon. Dualdiffusionimplicitbridgesforimage-to-
imagetranslation. InInternationalConferenceonLearningRepresentations(ICLR),2023.
[52] ValentinKhrulkov,GlebRyzhakov,AndreiChertkov,andIvanOseledets. Understandingddpmlatent
codesthroughoptimaltransport. InInternationalConferenceonLearningRepresentations(ICLR),2023.
[53] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger,andIlyaSutskever. Learning
transferablevisualmodelsfromnaturallanguagesupervision. InInternationalConferenceonMachine
Learning(ICML),2021.
[54] RazvanPascanuandYoshuaBengio. Revisitingnaturalgradientfordeepnetworks. InInternational
ConferenceonLearningRepresentations(ICLR),2014.
[55] YijunLi,RichardZhang,JingwanLu,andEliShechtman. Few-shotimagegenerationwithelasticweight
consolidation. InConferenceonNeuralInformationProcessingSystems(NeurIPS),2020.
[56] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointlylearning
toalignandtranslate. InInternationalConferenceonLearningRepresentations(ICLR),2015.
[57] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Lukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. InConferenceonNeuralInformationProcessing
Systems(NeurIPS),2017.
[58] MaximeOquab,Timoth√©eDarcet,Th√©oMoutakanni,HuyVo,MarcSzafraniec,VasilKhalidov,Pierre
Fernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal. Dinov2:Learningrobustvisual
featureswithoutsupervision. arXivpreprintarXiv:2304.07193,2023.
[59] ShilongLiu,ZhaoyangZeng,TianheRen,FengLi,HaoZhang,JieYang,ChunyuanLi,JianweiYang,
HangSu,JunZhu,etal. Groundingdino:Marryingdinowithgroundedpre-trainingforopen-setobject
detection. arXivpreprintarXiv:2303.05499,2023.
[60] KatherineLee,AFederCooper,andJamesGrimmelmann. Talkin‚Äùboutaigeneration:Copyrightandthe
generative-aisupplychain. arXivpreprintarXiv:2309.08133,2023.
[61] NivaElkin-Koren,UriHacohen,RoiLivni,andShayMoran. Cancopyrightbereducedtoprivacy? arXiv
preprintarXiv:2305.14822,2023.
[62] UriHacohen,AdiHaviv,ShaharSarfaty,BruriaFriedman,NivaElkin-Koren,RoiLivni,andAmitH
Bermano. Notallsimilaritiesarecreatedequal:Leveragingdata-drivenbiasestoinformgenaicopyright
disputes. arXivpreprintarXiv:2403.17691,2024.
[63] ZivEpstein,AaronHertzmann,HanyFarid,JessicaFjeld,MorganR.Frank,MatthewGroh,LauraHerman,
NeilLeach,RobertMahari,Alex‚ÄúSandy‚ÄùPentland,OlgaRussakovsky,HopeSchroeder,andAmySmith.
Artandthescienceofgenerativeai. Science,380(6650):1110‚Äì1111,2023.
[64] DavidBau,Jun-YanZhu,HendrikStrobelt,ZhouBolei,JoshuaB.Tenenbaum,WilliamT.Freeman,and
AntonioTorralba. Gandissection: Visualizingandunderstandinggenerativeadversarialnetworks. In
InternationalConferenceonLearningRepresentations(ICLR),2019.
[65] KevinMeng,DavidBau,AlexAndonian,andYonatanBelinkov. Locatingandeditingfactualknowledge
ingpt. arXivpreprintarXiv:2202.05262,2022.
[66] AlexFoote,NeelNanda,EsbenKran,IoannisKonstas,ShayCohen,andFazlBarez. Neurontograph:
Interpretinglanguagemodelneuronsatscale. arXivpreprintarXiv:2305.19911,2023.
[67] AmilDravid,YossiGandelsman,AlexeiAEfros,andAssafShocher.Rosettaneurons:Miningthecommon
unitsinamodelzoo. InIEEEConferenceonComputerVisionandPatternRecognition(CVPR),2023.
[68] James Martens. New insights and perspectives on the natural gradient method. Journal of Machine
LearningResearch,21(146):1‚Äì76,2020.
[69] DanRuta,SaeidMotiian,BaldoFaieta,ZheLin,HailinJin,AlexFilipkowski,AndrewGilbert,andJohn
Collomosse. Aladin:Alllayeradaptiveinstancenormalizationforfine-grainedstylesimilarity. InIEEE
InternationalConferenceonComputerVision(ICCV),2021.
[70] MarkHarden. Markharden‚Äôsartchive. https://www.artchive.com/,1998.
13A DerivationsforUnlearning
InSection4,wedescribeourunlearningmethodanditsrelationshiptoinfluencefunctions. Here,
weprovidemoredetailedderivations. LetŒ∏ bethepretrainedmodeltrainedondatasetDandloss
0
(cid:80)
L(z,Œ∏). N is the size of the dataset. Our goal is to obtain a model Œ∏ that unlearns the
z‚ààD ‚àízÀÜ
synthesizedimagezÀÜ.
A.1 EWCLoss
We summarize EWC Loss [19], which is the second order Taylor approximation of the data
(cid:80)
loss L(z,Œ∏) around Œ∏ . We denote the Hessian of the loss as H , where [H ] =
z‚ààD 0 Œ∏0 Œ∏0 ij
1 ‚àÇ2 (cid:80) L(z,Œ∏)| . WedenotetheremaindertermasR(Œ∏).
N ‚àÇŒ∏i‚àÇŒ∏j z‚ààD Œ∏=Œ∏0
(cid:88) (cid:88) (cid:88) N
L(z,Œ∏)= L(z,Œ∏ )+ ‚àáL(z,Œ∏)| (Œ∏‚àíŒ∏ )+ (Œ∏‚àíŒ∏ )TH (Œ∏‚àíŒ∏ )+R(Œ∏)
0 Œ∏=Œ∏0 0 2 0 Œ∏0 0
z‚ààD z‚ààD z‚ààD
(cid:88) N
‚âà L(z,Œ∏ )+ (Œ∏‚àíŒ∏ )TH (Œ∏‚àíŒ∏ ).
0 2 0 Œ∏0 0
z‚ààD
(7)
WeassumethatpretrainedŒ∏ isnearthelocalminimumoftheloss,resultinginanear-zerogradient
0
(cid:80)
‚àáL(z,Œ∏ )| . We drop out higher order remainder term R(Œ∏) in our approximation. If
z‚ààD 0 Œ∏=Œ∏0
themodelistrainedwithanegativelog-likelihoodloss,theHessianH isequivalenttotheFisher
Œ∏0
informationF [27],leadingto:
(cid:88) (cid:88) N
L(z,Œ∏)‚âà L(z,Œ∏ 0)+
2
(Œ∏‚àíŒ∏ 0)TF(Œ∏‚àíŒ∏ 0). (8)
z‚ààD z‚ààD
Wenotethatwefocusondiffusionmodels,whicharetrainedonalowerboundofthelog-likelihood.
Inthiscontext, theFisherinformationcanbeviewedastheGauss-Newtonapproximationofthe
Hessian[68]. Theformulationsatisfiesthepurposeofapproximatingthetraininglossonthedataset
andservesasaneffectiveregularizeronourunlearningobjective.
DiagonalapproximationofFisher. SinceFisherinformationisthecovarianceofthelog-likelihood
gradient,itsdiagonalapproximationisequivalenttotakingthesquareofthegradientsandaveraging
themacrossthetrainingset. ThisdiagonalapproximationisadoptedbyEWCloss[19,55]. Inthe
contextofdiffusionmodels,theFisherinformationisestimatedbyaveragingacrosstrainingdata,
randomnoise,andtimesteps.
A.2 Updatingstepforunlearning
WethenderivetheNewtonupdateofourunlearningobjectiveinEquation5. Below,werepeatour
unlearningobjectiveinEquation4:
N
LzÀÜ (Œ∏)‚âà‚àíL(zÀÜ,Œ∏)+ (Œ∏‚àíŒ∏ )TF(Œ∏‚àíŒ∏ ). (9)
unlearn 2 0 0
TheNewtonstepisasecond-orderupdateoftheformbelow,whereŒ±controlsthestepsize:
(cid:104) (cid:105)‚àí1
Œ∏ ‚ÜêŒ∏‚àíŒ± HÀÜz (Œ∏) ‚àáLzÀÜ (Œ∏), (10)
unlearn unlearn
whereHÀÜz (Œ∏)istheHessianofLzÀÜ (Œ∏). Nowwederive‚àáLzÀÜ (Œ∏):
unlearn unlearn unlearn
‚àáLzÀÜ (Œ∏)‚âà‚àí‚àáL(zÀÜ,Œ∏)+N ¬∑F(Œ∏‚àíŒ∏ )
unlearn 0 (11)
‚âà‚àí‚àáL(zÀÜ,Œ∏),
14whereweassumeŒ∏isclosetoŒ∏ ,sothetermN ¬∑F(Œ∏‚àíŒ∏ )canbeomitted. Empirically,wealso
0 0
triedunlearningwiththistermadded,butobservedlittlechangeinperformance. Then,wederive
HÀÜz (Œ∏)asfollows:
unlearn
HÀÜz (Œ∏)‚âà‚àíH (Œ∏)+N ¬∑F
unlearn ÀÜz (12)
‚âàN ¬∑F,
whereH (Œ∏)istheHessianofL(zÀÜ,Œ∏). Weassumethemagnitude(inForbeniusnorm)ofH (Œ∏)is
ÀÜz ÀÜz
bounded,andwithalargedatasetsizeN,wecanapproximatetheHessianHÀÜz (Œ∏)asN ¬∑F only.
unlearn
IncorporatingEquation11and12intoEquation10,weobtainourNewtonupdateinEquation5:
Œ±
Œ∏ ‚ÜêŒ∏+ F‚àí1‚àáL(zÀÜ,Œ∏). (13)
N
A.3 Connectiontoinfluencefunctions
Wenotethataspecialcaseofourformulation,runningourupdatesteponce,withasmallstepsize,is
closetotheformulationofinfluencefunctions. Thedifferenceismainlyonthelinearapproximation
errorofthelossonthetrainingpoint. StartingwithpretrainedmodelŒ∏ andtakinganinfinitesimal
0
stepŒ≥:
Œ∏ =Œ∏ +Œ≥F‚àí1‚àáL(zÀÜ,Œ∏). (14)
‚àízÀÜ 0
WhenweevaluatethelossofthetrainingpointzusingtheunlearnedmodelŒ∏ ,wecanwritethe
‚àízÀÜ
lossinalinearizedformaroundŒ∏ ,aswetakingasmallstep:
0
L(z,Œ∏ )‚âàL(z,Œ∏ )+‚àáL(z,Œ∏ )(Œ∏ ‚àíŒ∏ )
‚àízÀÜ 0 0 ‚àízÀÜ 0
(15)
=L(z,Œ∏ )+Œ≥‚àáL(z,Œ∏ )F‚àí1‚àáL(zÀÜ,Œ∏).
0 0
Now,weplugEquation15intoourattributionfunctioninEquation6:
œÑ(zÀÜ,z)=L(z,Œ∏ )‚àíL(z,Œ∏ )
‚àízÀÜ 0
(16)
‚âàŒ≥‚àáL(z,Œ∏ )F‚àí1‚àáL(zÀÜ,Œ∏).
0
Inthisspecialcase,ourmethodisequivalenttoinfluencefunction‚àáL(z,Œ∏ )F‚àí1‚àáL(zÀÜ,Œ∏),after
0
approximations. Practically,thedifferencebetweenourmethodandinfluencefunctionsisthatweare
takinglarger,sometimesmultiplesteps(ratherthanasingle,infinitesimalstep),andareexplicitly
evaluatingtheloss(ratherthanwithalinear,closed-formapproximation).
B ImplementationDetails
B.1 MSCOCOModels
Modelstrainedfromscratch. WeselectoursourcemodelforattributionfromGeorgievetal.[12],
whichisalatentdiffusionmodelwheretheCLIPtextencoderandVAEareexactlytheonesused
inStableDiffusionv2,butwithasmallerU-Net. ToretraineachMSCOCOmodelforleave-K-out
evaluation,wefollowthesametrainingrecipeasthesourcemodel,whereeachmodelistrainedwith
200epochs,alearningrateof10‚àí4,andabatchsizeof128. WeusetheCOCO2017trainingsplitas
ourtrainingset.
Unlearning. TounlearnasynthesizedsampleinMSCOCOmodels,wefindthatrunningwith1
stepalreadyyieldsgoodattributionperformance. WeperformNewtonunlearningupdateswithstep
sizesof0.01andupdateonlycross-attentionKV(Wk,Wv). Wefindthatupdatingcross-attention
KVyieldsthebestperformance,andwelaterprovideablationstudiesontheoptimalsubsetoflayers
toupdate. Wesamplegradients591,435timestoestimatethediagonalFisherinformation,equivalent
to5epochsofMSCOCOtrainingset.
15B.2 CustomizedModelBenchmark
Modelcollection. AsdescribedinSection5.2,weselectedasubsetofthedataset[18]comprising
of20models: 10object-centricand10artist-stylemodels. Forallobject-centricmodels,weselect
models with distinct categories. For artist-style models, we select 5 models trained from BAM-
FG[69]exemplarsand5modelstrainedfromArtchive[70]exemplars. Tospeedupcomputation,
we calculate Fisher information on Stable Diffusion v1.4, the base model of all the customized
models,overtheselectedsubsetofLAIONimages. WethenapplythesameFisherinformationtoall
customizedmodels.
Unlearning. Wefindthatrunning100unlearningstepsyieldsamuchbetterperformancethan
runningwith1stepforthistask. Moreover,updatingonlycross-attentionKVyieldsasignificant
boostinperformanceinthistestcase. InAppendixC.2,weshowanablationstudyonthesedesign
choices. Wesamplegradients1,000,000timestoestimatethediagonalFisherinformation,wherethe
gradientsarecalculatedfromthe100kLaionsubsetusingStableDiffusionv1.4.
B.3 Baselines.
Pixelspace. FollowingJourneyTRAK‚Äôsimplementation[12],weflattenthepixelintensitiesand
usecosinesimilarityforattribution.
CLIPimageandtextfeatures. WeusetheofficialViT-B/32modelforimageandtextfeatures.
DINO. WeusetheofficialViT-B/16modelforimagefeatures.
DINOv2. WeusetheofficialViT-L14modelwithregistersforimagefeatures.
CLIP(AbC)andDINO(AbC). Weusetheofficialmodelstrainedonthecombinationofobject-
centricandstyle-centriccustomizedimages. CLIP(AbC)andDINO(AbC)areselectedbecausethey
arethebest-performingchoicesoffeatures.
TRAKandJourneyTRAK. WeadopttheofficialimplementationofTRAKandJourneyTRAKand
usearandomprojectiondimensionof4096,thesameaswhattheyuseforMSCOCOexperiments.
B.4 AdditionalDetails
Horizontalflips. Text-to-imagemodelsinourexperimentsarealltrainedwithhorizontalflips. Asa
result,themodelsareeffectivelyalsotrainedwiththeflippedversionofthedataset.Therefore,werun
anattributionalgorithmforeachtrainingimageonitsoriginalandflippedversionandobtainthefinal
scorebytakingthemaxofthetwo. Forafaircomparison,weadoptthisapproachforallmethods.
Wealsofindthattakingtheaverageinsteadofthemaxempiricallyyieldssimilarperformance.
Computationalresources. WeconductallofourexperimentsonA100GPUs. Ittakesaround16
hourstotrainanMSCOCOmodelfromscratch,20hourstoevaluatealltrainingimageloss,and2
minutestounlearnasynthesizedimagefromapretrainedMSCOCOmodel. Tofinishallexperiments
onMSCOCOmodels,ittakesaround77KGPUhours. ForCustomizedModelBenchmark,ittakes
2hourstounlearnasynthesizedimageand16hourstotrackthetrainingimageloss. Tofinishall
experimentsonthisbenchmark,ittakesaround36KGPUhours.
Licenses. ThesourcemodelfromGeorgievetal.[12](JourneyTRAK)isreleasedundertheMIT
License. TheMSCOCOdatasetisreleasedundertheCreativeCommonsAttribution4.0License.
StableDiffusionv2isreleasedundertheCreativeMLOpenRAIL++-MLicense. TheCLIPmodelis
releasedundertheMITLicense. TheCustomizedModelBenchmarkisreleasedundertheCreative
CommonsAttribution-NonCommercial-ShareAlike4.0InternationalLicense.
C AdditionalAnalysis
C.1 MSCOCOModels
Deviationofgeneratedoutputinleave-K-outmodels. Wereportthequantitativeevaluation
forthedeviationofgeneratedoutputintermsofMSEinFigure8andintermsofCLIPsimilarity
inFigure9. Wefindthatintermsofthismetric,ourmethodstilloutperformsallbaselineswhile
being slightly less performant than K = 4000 when compared with DINO features under CLIP
16MSE error
0.07 random CLIP Text CLIP DINO TRAK Ours
Pixel DINO v2 CLIP (AbC) DINO (AbC) JourneyTRAK
0.06
0.05
0.04
500 1000 4000 10000 Full dataset
K images removed
Equivalent Number of Random Images Removed
Full dataset
100000
75000
50000 k=500
k=1000
25000
k=4000
0
Pixel CLIP Text DINO v2 CLIP CLIP (AbC) DINO DINO (AbC) TRAK JourneyTRAK Ours
Figure8: Deviationofgeneratedoutputinleave-K-outmodels,inmeansquareerror(MSE).
WereportthedeviationofgeneratedoutputinthesamefashionasinFigure3. ThehighertheMSE,
thebettersincemoredeviationindicatesthattheK imagesremovedaremoreinfluential.
CLIP error
0.8
0.7
0.6
0.5 random CLIP Text CLIP DINO TRAK Ours
Pixel DINO v2 CLIP (AbC) DINO (AbC) JourneyTRAK
0.4
500 1000 4000 10000 Full dataset
K images removed
Equivalent Number of Random Images Removed
Full dataset
100000 k=500
k=1000
75000 k=4000
50000
25000
0
Pixel CLIP Text DINO v2 CLIP CLIP (AbC) DINO DINO (AbC) TRAK JourneyTRAK Ours
Figure9: Deviationofgeneratedoutputinleave-K-outmodels,inCLIPsimilarity. Wereport
thedeviationofgeneratedoutputinthesamefashionasinFigure3. AlowerCLIPsimilarityin
leave-K-outmodelsindicatesabetterattributionalgorithm.
17
rorrE
noitcurtsnoceR
ESM
devomeR
segamI
modnaR
K
rorrE
noitcurtsnoceR
PILC
devomeR
segamI
modnaR
Ksimilarity. Interestingly,wefindthatJourneyTRAKperformsbetterthanTRAKinthismetricdespite
significantlyunderperformingregardinglosschanges(Figure3). WenotethatDINO,despitenot
beingdesignedforattribution,clearlyoutperformsbothTRAKandJourneyTRAKinthisevaluation
setup.
Ablationstudies. Weperformthefollowingablationstudiesandselecthyperparametersforbest
performanceineachtestcase:
‚Ä¢ SGD(1step): 1SGDstep,stepsize0.001
‚Ä¢ SGD(10steps): 10SGDsteps,stepsize0.0001
‚Ä¢ Fullweight: 1Newtonsteps,stepsize0.0005
‚Ä¢ Attention: 1Newtonsteps,stepsize0.005
‚Ä¢ Cross-attention: 1Newtonsteps,stepsize0.005
‚Ä¢ Cross-attentionKV:1Newtonsteps,stepsize0.01(Thisisourfinalmethod)
TheSGDstepreferstothebaselineofdirectlymaximizingsynthesizedimagelosswithoutEWCloss
regularization,asdescribedinSection4.
Wealsocomparedifferentsubsetsofweightstooptimizeandreportlosschange,deviationmeasured
by MSE, and deviation measured by CLIP similarity in Figure 12, 13, 14, respectively. We find
that the 4 choices of weight subset selection all lead to effective attribution performance, where
restricting weight updates to cross-attention KV yields the best performance overall. However,
bothconfigurationsforSGDupdatesperformmuchworse,indicatingtheimportanceofregulating
unlearningwithFisherinformation.
Additionalresults. WeprovidemoreattributionresultsinFigure10andmoreresultsonleave-K-
outmodelsinFigure11.
C.2 CustomizedModelBenchmark
Ablationstudies. Weperformthefollowingablationstudiesandselecthyperparametersforbest
performanceineachtestcase:
‚Ä¢ Cross-attentionKV(100steps): 100Newtonsteps,stepsize0.1(denotedasOursinFigure15)
‚Ä¢ Cross-attentionKV(1step): 1Newtonstep,stepsize10
‚Ä¢ Fullweight(100steps): 100Newtonsteps,stepsize5√ó10‚àí5
‚Ä¢ Cross-attentionKV,SGDstep(100steps): 100SGDstep,stepsize0.01
Again,theSGDstepreferstothebaselineofdirectlymaximizingsynthesizedimagelosswithout
EWClossregularization,asdescribedinSection4.
WereporttheresultofourablationstudiesinFigure15. Ourfindingsindicatethatforthistestcase,
selectinga smallsubsetof weights(i.e., cross-attentionKV)combined withmultipleunlearning
steps(100steps)iscrucialforeffectiveattribution. Wehypothesizethatstrongerregularizationis
necessaryforunlearninginlarger-scalemodels,andthatsuchmodelsbenefitmorefromnumerous
smallerunlearningstepsratherthanfewer,largerstepstoachieveabetteroptimization.
Customized models in this benchmark associate the exemplar with a special token V‚àó, which is
alsousedforgeneratingsynthesizedimages. Ourmethodinvolvesforgettingthesynthesizedimage
associated with its text prompt, so by default, we tested it with V‚àó included. Meanwhile, we
also evaluated our method without V‚àó in the prompts. Figure 6 shows that removing V‚àó reduces
performance,butthemethodstillperformswelloverall.
18Generated Sample
‚ÄúA small closed
toilet in a
cramped space.‚Äù
Generated Sample
‚ÄúAzebraallby
itselfinthe
greenforest.‚Äù
Generated Sample
‚ÄúA catlaying
onclothesthat
areina
suitcase.‚Äù
Generated Sample
‚ÄúAtennis player
running to get
to the ball.‚Äù
Figure10: AdditionalattributionresultsforMSCOCOmodels. ThisisanextensionofFigure2
inthemainpaper. Ourmethodidentifiesimageswithsimilarposesandvisualattributionstothe
queryimage. Importantly,inFigure11,weverifythatleavingtheseimagesoutoftrainingcorrupts
theabilityofthemodeltosynthesizethequeriedimages.
19
sruO
ONID
KARTyenruoJ
sruO
ONID
KARTyenruoJ
sruO
ONID
KARTyenruoJ
sruO
ONID
KARTyenruoJK=500 K=1000 K=4000 K=500 K=1000 K=4000
Generated Sample Generated Sample
‚ÄúA small closed ‚ÄúAzebraallby
toilet in a itselfinthe
cramped space.‚Äù greenforest.‚Äù
K=500 K=1000 K=4000 K=500 K=1000 K=4000
Generated Sample Generated Sample
‚ÄúA catlaying ‚ÄúAtennis player
onclothesthat running to get
areina to the ball.‚Äù
suitcase.‚Äù
Figure11: Additionalleave-K-outmodelresultsforMSCOCOmodels. Thisisanextension
ofFigure4inthemainpaper,showingtheresultsfromremovingtop-K influentialimagesfrom
differentalgorithms,retraining,andattemptingtoregenerateasynthesizedsample. Theinfluential
imagesfortheseexamplesareshowninFigure10. Ourmethodconsistentlydestroysthesynthesized
examples,verifyingthatourmethodisidentifyingthecriticalinfluentialimages.
Loss Change vs. K Images Removed
0.03
random Full Cross Attn
SGD (1 step) Attn Cross Attn KV
0.02
SGD (10 step)
0.01
0.00
500 1000 4000 10000 Full dataset
K images removed
Equivalent K Random Images Removed
Full dataset
100000
75000
50000 k=500
k=1000
25000
k=4000
0
SGD (1 step) SGD (10 step) Full Attn Cross Attn Cross Attn KV
Figure12:AblationstudiesforattributingMSCOCOmodels. Wereportthechangeinsynthesized
imagelossinleave-K-outmodelsinthesamefashionasinFigure3inthemainpaper. Wecompare
four different sets of weights to unlearn (full, attention layers, cross-attention layers, and cross-
attentionWk,Wv),andwefindthatcross-attentionWk,Wv outperformsotherconfigurations. We
alsoevaluateanaivevariationwhereweapplySGDtomaximizethesynthesizedimage‚Äôsloss,and
wefindthatupdatingwith1stepsor10stepsbothperformonlyatchance(randombaseline).
20
egnahC
ssoL
MPDD
devomeR
segamI
modnaR
K
sruO
ONID
KARTyenruoJ
sruO
ONID
KARTyenruoJ
sruO
ONID
KARTyenruoJ
sruO
ONID
KARTyenruoJMSE error
0.07 random SGD (1 step) SGD (10 step) Full Attn Cross Attn Cross Attn KV
0.06
0.05
0.04
500 1000 4000 10000 Full dataset
K images removed
Equivalent Number of Random Images Removed
Full dataset
k=500
100000
k=1000
75000 k=4000
50000
25000
0
SGD (1 step) SGD (10 step) Full Attn Cross Attn Cross Attn KV
Figure13:AblationstudiesforattributingMSCOCOmodels.Wereportthedeviationofgenerated
outputinleave-K-outmodelsinmeansquareerror(MSE)inthesamefashionasinFigure8. We
findthatthetrendofeachablationfollowsthatofFigure12.
.
CLIP error
0.8
0.7
0.6
0.5
random SGD (1 step) SGD (10 step) Full Attn Cross Attn Cross Attn KV
0.4
500 1000 4000 10000 Full dataset
K images removed
Equivalent Number of Random Images Removed
Full dataset
100000 k=500
k=1000
75000 k=4000
50000
25000
0
SGD (1 step) SGD (10 step) Full Attn Cross Attn Cross Attn KV
Figure14:AblationstudiesforattributingMSCOCOmodels.Wereportthedeviationofgenerated
outputinleave-K-outmodelsinCLIPsimilarityinthesamefashionasinFigure9. Wefindthatthe
trendofeachablationfollowsthatofFigure12.
1.0 1.0
Ours Ours
Ours (no V*) Ours (no V*)
0.8 0.8
1-step 1-step
Full weight update Full weight update
0.6 SGD 0.6 SGD
0.4 0.4
0.2 0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Object-Centric Models (Recall@10) Object-Centric Models (mAP)
Figure 15: Ablation studies for Customized Model Benchmark. We report evaluation on the
CustomizedModelBenchmarkinthesamefashionasinFigure6. Wefindthattrainingwithmultiple
steps,updatingaselectedsubsetofweights,andregularizingunlearningviaFisherinformationis
crucialtothistask. Additionally,wetestaversionwhereweapplyouralgorithmwithoutthespecial
tokenV‚àó. Whileitreducesperformance,itstillperformswellinoverall.
21
rorrE
noitcurtsnoceR
ESM
devomeR
segamI
modnaR
K
rorrE
noitcurtsnoceR
PILC
devomeR
segamI
modnaR
K
)01@llaceR(
sledoM
elytS-tsitrA
)PAm(
sledoM
elytS-tsitrA