ConsistDreamer: 3D-Consistent 2D Diffusion for High-Fidelity Scene Editing
Jun-KunChen1† SamuelRotaBulo`2 NormanMu¨ller2 LorenzoPorzi2
PeterKontschieder2 Yu-XiongWang1
1UniversityofIllinoisUrbana-Champaign 2Meta
{junkun3,yxw}@illinois.edu
{rotabulo,normanm,porzi,pkontschieder}@meta.com
Figure1.OurConsistDreamerlifts2Ddiffusionwith3Dawarenessandconsistency,achievinghigh-fidelityinstruction-guidedscene
editing with superior sharpness and detailed textures. Left: The three synergistic components within ConsistDreamer that enable 3D
consistency. Right: State-of-the-artperformanceofConsistDreameracrossvariouseditingtasksandscenes,especiallywhenpriorwork
(e.g.,IN2N[8])failsandinchallenginglarge-scaleindoorscenesfromScanNet++[43].Moreresultsareonourprojectpage.
Abstract 1.Introduction
This paper proposes ConsistDreamer – a novel frame- With the emergence of instruction-guided 2D generative
work that lifts 2D diffusion models with 3D awareness models as in [2], it has never been easier to generate or
and3Dconsistency,thusenablinghigh-fidelityinstruction- editimages. Extendingthissuccessto3D,i.e.,instruction-
guided scene editing. To overcome the fundamental limi- guided 3D scene editing, becomes highly desirable for
tation of missing 3D consistency in 2D diffusion models, artists,designers,andthemovieandgameindustries. Nev-
our key insight is to introduce three synergistic strategies ertheless, editing 3D scenes or objects is inherently chal-
that augment the input of the 2D diffusion model to be- lenging. The absence of large-scale, general 3D datasets
come 3D-aware and to explicitly enforce 3D consistency makes it difficult to create a counterpart generative model
during the training process. Specifically, we design sur- similar to [2] that can support arbitrary 3D scenes. There-
rounding views as context-rich input for the 2D diffusion fore,state-of-the-artsolutions[8,25]circumventthischal-
model,andgenerate3D-consistentstructurednoiseinstead lenge by resorting to generalizable 2D diffusion models.
of image-independent noise. Moreover, we introduce self- This approach, known as 2D diffusion distillation, renders
supervised consistency-enforcing training within the per- the scene into multi-view images, applies an instruction-
scene editing procedure. Extensive evaluation shows that conditioneddiffusionmodelin2D,andthendistillstheedit-
ourConsistDreamerachievesstate-of-the-artperformance ing signal back to 3D, such as through a neural radiance
for instruction-guided scene editing across various scenes field(NeRF)[6,8,15].
andeditinginstructions,particularlyincomplicatedlarge- However,afundamentallimitationofthissolutionisthe
scaleindoorscenesfromScanNet++,withsignificantlyim- lack of 3D consistency: a 2D diffusion model, acting in-
provedsharpnessandfine-grainedtextures. Notably,Con- dependentlyacrossviews, islikelytoproduceinconsistent
sistDreamer stands as the first work capable of success- edits, both in color and shape. For example, a person in
fully editing complex (e.g., plaid/checkered) patterns. Our one view might be edited to be wearing a red shirt, while
project page is at immortalco.github.io/ConsistDreamer. appearinginagreenshirtinanotherview. Usingtheseim-
agestotrainaNeRFcanstillproducereasonableedits,but
the model will naturally converge towards an “averaged”
†WorkstartedduringaninternshipatMetaRealityLabsZurich. representationoftheinconsistent2Dsupervision, andlose
1
4202
nuJ
31
]VC.sc[
1v40490.6042:viXramost of its details and sharpness. A commonly observed tatestheprocessofachievingconsistentimagesbytheend.
failure mode is that of regular (e.g., checkered) patterns,
The combination of surrounding views and structured
whichcompletelydisappearoncedistilledto3Dduetomis-
noise provides the 2D diffusion model with 3D consistent
alignmentsacrossviews. Generatingconsistentmulti-view
input, yet it is insufficient. An explicit enforcement of 3D
imagesthusbecomescrucialforachievinghigh-fidelity3D
consistencyisalsorequiredduringthelearningprocess. To
sceneediting.
thisend, weproposeself-supervisedconsistency-enforcing
While largely overlooked in prior work, our investiga-
trainingwithintheper-sceneeditingprocedure(Fig.1).We
tionrevealsthatthesourceofinconsistencyismulti-faceted,
augmentthe2DdiffusionmodelbyaControlNet[47]that
andprimarilyoriginatesfromtheinput. (1)Asthe2Ddif-
introduces 3D positional embedding to make it 3D-aware.
fusion model can only observe a single view at a time, it
Inspiredby[37,42],weperformwarpingandaveragingfor
lacks sufficient context to understand the entire scene and
all sub-views in the edited surrounding view image. This
apply consistent editing. (2) The editing process for each
process yields a surrounding view of 3D consistent sub-
imagestartsfromindependentlygeneratedGaussiannoise,
viewsusedastheself-supervisiontarget.Tofurtherachieve
whichbringschallengestoconsistentimagegeneration. In-
“cross-batch consistency” – consistency between different
tuitively,itisdifficulttogenerateconsistentmulti-viewim-
batchesindifferentgenerations–weperformmultiplegen-
agesbydenoisinginconsistentnoise,andevenforasingle
erations in parallel, and construct consistent target images
view,itmaynotalwaysyieldthesameeditedresult.(3)The
from all sub-views in all generated surrounding view im-
inputtothe2Ddiffusionmodelcontainsno3Dinformation,
ages, so as to supervise all generations collectively. Af-
making it much harder for the model to reason about 3D
ter consistency-enforcing training, the 2D diffusion model
geometryandtoshareinformationacrossdifferentviewsof
is able to generate consistent multi-view images. Conse-
thescene,evenwhenmadeavailabletoit.
quently,atrainedNeRFwillnothavetosmoothoutincon-
Motivated by these observations, we propose Consist- sistencies,butultimatelyconvergetosharpresultspreserv-
Dreamer–anovelframeworktoachieve3Dconsistencyin ingfine-graineddetails.
2D diffusion distillation. ConsistDreamer introduces three
synergistic strategiesthat augmentthe inputofthe 2Ddif- Empowered by such a 3D-consistent 2D diffusion
fusionmodeltobe3D-awareandenforce3Dconsistencyin model, our ConsistDreamer achieves high-fidelity and di-
aself-supervisedmannerduringthetrainingprocess. verseinstruction-guided3Dsceneeditingwithoutanymesh
exportationandrefinementsorabetterscenerepresentation
Toaddressthelimitedcontextissuewithinasingleview,
like Gaussian Splatting [14], as shown in Fig. 1. Com-
our framework involves incorporating context from other
pared with previous work, the editing results of Consist-
views. We capitalize on the observation that 2D diffusion
Dreamer exhibit significantly improved sharpness and de-
modelsinherentlysupport“composedimages,”wheremul-
tail,whilepreservingthediversityintheoriginal2Ddiffu-
tiple sub-images are tiled to form a larger image. Given
sion model’s [2] editing results. Notably, ConsistDreamer
the capability of the self-attention modules in the UNet of
standsasthefirstworkcapableofsuccessfullyeditingcom-
the 2D diffusion model to establish connections between
plex(e.g.,checkered)patterns. Moreover,ConsistDreamer
the same objects across different sub-images, each image
demonstrates superior performance in complicated, high-
can be edited with the context derived from other images.
resolution ScanNet++ [43] scenes – an accomplishment
Therefore, we leverage the composed images to construct
where state-of-the-art methods faced challenges in achiev-
asurroundingview(Fig.1), whereonelarge, centralmain
ingsatisfactoryedits.
view is surrounded by several small reference views. This
approach allows us to edit the main view with the context Our contributions are three-fold. (1) We introduce
fromreferenceviews,andviceversa.Doingsonotonlyen- ConsistDreamer, asimpleyeteffectiveframeworkthaten-
richesthecontextofthesceneintheinput,butalsoenables ables3D-consistentinstruction-guidedsceneeditingbased
thesimultaneouseditingofmultipleviews. on distillation from 2D diffusion models. (2) We propose
Regarding the noise, we introduce 3D-consistent struc- threenovel,synergisticcomponents–structurednoise,sur-
turednoise(Fig.1),withthekeyinsightofgeneratingcon- rounding views, and consistency-enforcing training – that
sistent noise for each view once at the beginning. Specifi- lift 2D diffusion models to generate 3D-consistent images
cally,wegenerateandfixGaussiannoiseonthesurfaceof acrossallgeneratedbatches. Notably, ourworkisthefirst
the scene objects, and then render each view to obtain the thatexplorescross-batchconsistencyanddenoisingconsis-
2D noise used for the image at that view in all subsequent tency in 2D diffusion distillation and attains these through
diffusion generations. This approach aligns with existing manipulatingnoise. (3)Weevaluatearangeofscenesand
3Ddiffusionwork[22]whichalsogeneratesnoisein3Dat editinginstructions,achievingstate-of-the-artperformance
thebeginningofageneration. Ensuringthatthedenoising inboth,scenesconsideredbypreviousworkandmorecom-
procedure starts with consistent noise substantially facili- plicated,large-scaleindoorscenesfromScanNet++.
22.RelatedWork tively generates edited images to update the NeRF dataset
forNeRFfitting,andsupportsmoregeneraleditinginstruc-
NeRF-Based Scene Editing. Neural radiance field
tions such as object-specific editing. ViCA-NeRF [6] pro-
(NeRF) [21] and its variants [1, 3, 7, 19, 33, 35, 38, 44]
poses a different pipeline to first edit key views and then
are widely-used approaches to representing scenes. NeRF
blendkeyviewsandapplyrefinement. Edit-DiffNeRF[45]
leveragesneuralnetworksorotherlearnablearchitecturesto
augmentsthediffusionmodelandfine-tunesitwithaCLIP
learn to reconstruct the 3D geometry of a scene only from
loss to improve the success rate of editing. DreamEdi-
multi-view images and their camera parameters, and sup-
tor [49] utilizes a fine-tuned variant of [29] instead of [2]
portnovelviewsynthesis. WiththedevelopmentofNeRF,
andfocusesonobject-specificediting.
editing a NeRF-represented scene is also deeply studied,
ConsistencyinDistillation-BasedPipelines. Whendis-
covering different types of editing objectives and editing
tillingfrom2Ddiffusiontoperform3Dgenerationoredit-
operation indicators, a.k.a., “user interfaces.” Some meth-
ing, 3D awareness and 3D consistency of the generated
ods [16, 20, 41] support editing the position, color, and/or
images are crucial, as 3D-inconsistent multi-view images
shape of a specific object indicated by users through a
are not a valid descriptor of a scene. However, achiev-
pixel, a text description, or a segment, etc. Another line
ing 3D consistency in 2D diffusion is challenging. Early
of work [4, 24, 39, 46] studies human-guided shape edit-
work[8,13,18,25,48]doesnotalterthediffusionmodel
ing, which allows users to indicate a shape editing oper-
and relies on consistency derived from NeRF, by directly
ation with a cage or point cloud provided by the model.
trainingNeRFwiththeinconsistentmulti-viewimages.The
Thetaskweinvestigateisinstruction-guidedsceneediting,
NeRF will then converge to an averaged or smoothed ver-
whichallowsuserstoindicatetheeditingoperationthrough
sion of the scene, according to its model capability, which
instructions in natural language. The first work in this di-
results in blurred results with few textures and even fails
rection is NeRF-Art [34], which mainly focuses on style
to generate regular patterns like a plaid or checkered pat-
transferinstructions,andusespre-trainedCLIP[26]asthe
tern. Follow-upworkbeginstoimprovetheconsistencyof
stylizationlossfortheinstruction-indicatedstyle. Morere-
thediffusionand/orthepipeline. ViCA-NeRF[6]achieves
centwork[8,13,15,45,49]leveragesdiffusionmodels[10]
consistencybyproposingadifferentpipelinebasedonkey
insteadofCLIPtobenefitfrompowerfuldiffusionmodels
views. ProlificDreamer[36]makesthediffusion3D-aware
andsupportmoregeneralinstructions.
by inputting the camera parameter to the diffusion model
Distillation-Based 3D Scene Generation. Lacking 3D
and applying per-scene fine-tuning. CSD [15], IVID [37],
datasetstotrainpowerful3Ddiffusionmodels, currentso-
and ConsistNet [42] propose a joint distillation procedure
lutions distill the generation signal from a 2D diffusion
formultipleviews, aimingtogenerateoreditmultipleim-
model to exploit its ability in 3D generation. DreamFu-
ages in one batch consistently, through either attention,
sion[25]isthefirstworkinthisdirection,whichproposes
depth-basedwarping,orKullback–Leiblerdivergence.
scoredistillationsampling(SDS)todistillthegradientup-
However,thesemethodsallsharetwomajorconstraints:
date direction (“score”) from 2D diffusion models, and
(1)thenoiseusedforgenerationisnotcontrolled,therefore
supportsinstruction-guidedscenegenerationbydistillinga
asingleviewmayleadtodifferentandinconsistentgener-
pre-traineddiffusionmodel[28].HiFA[48]proposesanan-
ation results with different noises; (2) these methods only
nealing technique and rephrases the distillation formula to
study and enforce the consistency between images within
improvethegenerationresult. Magic3D[18]improvesthe
a single batch. Nevertheless, the full generation or editing
generation results by introducing a coarse-to-fine strategy
procedureforthesceneisacrossmultiplebatches,andthere
and a mesh exportation and refinement method. Prolific-
might be inconsistencies in different batches. Our Con-
Dreamer[36]furtherimprovesthegenerationresultsbyin-
sistDreamer resolves these limitations by proposing novel
troducinganimprovedversionofSDS,namelyvariational
structurednoiseandconsistency-enforcingtraining.
score distillation (VSD), to augment and fine-tune a pre-
traineddiffusionmodelanduseitforgeneration.
3.ConsistDreamer: Methodology
DiffusionDistillation-Based3DSceneEditing. Similar
to[28]forinstruction-guidedgenerationtasks,anotherdif- Our ConsistDreamer is a novel IN2N-like [8] framework
fusion model [2] was proposed for instruction-guided im- applieduponadiffusion-based2Dimageeditingmodel[2].
ageediting,bygeneratingtheeditedimageconditionedon As illustrated in Fig. 2, our pipeline maintains a buffer of
boththeoriginalimageandtheinstruction,whichisthere- edited views for the NeRF to fit, and uses [2] to generate
forecompatiblewithSDS[25]. Instruction3D-to-3D[13] new edited images for random views according to the in-
usesSDSwith[2]tosupportinstruction-guidedstyletrans- struction, the original appearance, and the current NeRF
feron3Dscenes. Instruct-NeRF2NeRF(IN2N)[8]adopts rendering results. Noticing that the NeRF fitting proce-
another way to operate the 2D diffusion model, similar to dureanddiffusiongenerationprocedurearerelativelyinde-
the rephrased version of SDS in HiFA [48], which itera- pendent, we equivalently execute them in parallel. Within
3Figure2. ConsistDreamerframeworkisanIN2N-like[8]pipelinecontainingtwomajorprocedures. (a)IntheNeRFfittingprocedure,
wecontinuouslytrainNeRFwithabufferofeditedviews. (b)Indiffusiongenerationandtraining,weaddour3D-consistentstructured
noisetorenderedmulti-viewimages,andcomposesurroundingviewswiththem,asinputtotheaugmented3D-aware2Ddiffusion[2].
Wethenaddtheeditedimagestothebufferfor(a),andapplyself-supervisedconsistency-enforcingtrainingusingtheconsistency-warped
images.Note:imagesofstructurednoiseareonlyforillustration–theyareactuallyvisuallyindistinguishablefromGaussiannoiseimages.
this framework, we propose (1) structured noise to enable frompre-trained2Ddiffusionmodels.
a3D-consistentdenoisingstep,startingfrom3D-consistent Asalatentdiffusionmodel,[2]actuallyrequiresnoisein
noise at the beginning and ending with 3D-consistent im- latent space, which is (H/8,W/8,4) instead of the image
ages; (2)surroundingviewstoconstructcontext-richcom- shape(H,W,3).Eachelementinthisnoiselatentshouldbe
posedimagesasinputtothe2Ddiffusioninsteadofasingle independently generated from N(0,1). Constructing such
view;and(3)aself-supervisedconsistency-enforcingtrain- 3D-consistentstructurednoiseremainsnon-trivial:weneed
ingmethodviaconsistentwarpinginsurroundingviews,to toplacenoisein3D,projectnoiseinto2Dpixelsatmultiple
achievecross-viewandcross-batchconsistency. scales,andensurecorrespondencebetweendifferentviews.
Additionally, thedistributionofeachimage’snoiseshould
3.1.StructuredNoise
beGaussian,asnoiseinanincorrectordependentdistribu-
2D diffusion models generate a new image from a noisy tion may lead to abnormal generation results (as shown in
image, which is either pure Gaussian noise or a mixture thesupplementarymaterial).
of noise and the original image. Prior works like Dream- To overcome these challenges, we construct a dense
Fusion [25] and IN2N typically sample different Gaussian point cloud of the scene by unprojecting all the pixels in
noise in each iteration. However, varying noise leads to all the views to points, with the depth predicted by NeRF.
highlydifferentgenerationresults(asshowninthesupple- For each point p, we randomly sample a weighted noise
mentarymaterial).Inotherwords,previousmethodscannot c(p) = (x,w), where x ∼ N(0,1) is an independently
evenproduceconsistent(i.e.,identical)imagesforthesame generated Gaussian noise, and w ∼ U(0,1) is its weight.
view in different generations, fundamentally limiting their To generate the noise at one view, we identify the sub-
abilitytogenerateconsistentresults. point cloud that is front-most in this view, and project it
Thisobservationmotivatesustocontrolandmanipulate onto the plane. For multiple points projected to the same
the noise, by introducing 3D-consistent structured noise. pixel, we aggregate them by selecting the weighted noise
Intuitively, while it is difficult to generate, denoise, or re- (x,w) with the maximum w, and form a noise image I of
store3D-consistentimagesfrominconsistentrandomnoise, shape(H,W)consistingofvaluesinx. Aseachxisinde-
thetaskbecomesmoremanageablewhengeneratingconsis- pendentlygeneratedandselected(accordingtow),wehave
tent images from noise that is itself consistent. Therefore, I ∼N(0,1)H×W,i.e.,makingI valid2DGaussiannoise.
instead of using independently generated noise in each it- Giventhateachpixelinthelatentspaceisonlyroughly
eration, wegeneratenoiseonthesurfaceofthesceneonly related to its corresponding 8 × 8 region in the image,
onceduringinitialization,andrenderthenoiseateachview we can generate noise in the latent space by operating at
to obtain the noise used in generating the image for that the downsampled resolution of (H/8)×(W/8). We thus
view. Our strategy aligns with 3D diffusion models like generate different weighted noise {c (p)} for each of the
i
DiffRF[22],whichdirectlygeneratenoisein3Dspace.The four channels of the latent space, and stack the individu-
difference lies in the denoising step: while such work di- ally rendered noise I to construct a Gaussian noise image
i
rectlydenoisesin3D,wedistillthe“3Ddenoisingprocess” of (H/8,W/8,4), which is then used as the noise by the
4diffusionmodel. 3.3.Consistency-EnforcingTraining
The structured noise serves as the foundation for 3D-
Wedesignconsistentper-scenetrainingbasedonstructured
consistent generation. In Sec. 3.3, we introduce a training
noiseandsurroundingviews,enforcing2Ddiffusiontogen-
methodtoensureaconsistentdenoisingprocedurefromthe
erate 3D consistent images through a consistent denoising
beginningtotheend,sothatthedenoisedimagesofdiffer-
procedure.
entviewsateverydenoisingstepisalso3Dconsistent.
Multi-GPU Parallelization Paradigm. Our pipeline in-
volves training both NeRF and 2D diffusion. Observing
3.2.SurroundingViews that training and inferring a diffusion model is consider-
ably more time-consuming than training the NeRF, while
Usingtheoriginalviewasinputisastandardpractice,when
thereareveryfewdependenciesbetweenthem,wepropose
employing2Ddiffusionmodels.Thismethodworkswellin
amulti-GPUparallelizationparadigm. With(n+1)GPUs,
simple360◦ orforward-facingscenesusedbyIN2N[8],as
we dedicate GPU0 to continuously and asynchronously
asingleviewcoversmostobjectsinthescene. However,in
train a NeRF on the buffer of edited images. The remain-
morecomplicatedscenesliketheclutteredroomsinScan-
ing n GPUs are utilized to train the diffusion model and
Net++[43],aviewmayonlycontainacornerorabarewall
generate new edited images added to the buffer for NeRF
in the room. This hinders the diffusion model to generate
training. At each diffusion training iteration, we allocate
plausibleresults,duetothelimitedcontextinasingleview.
a view to each of the n GPUs and train diffusion on them
Intriguingly, our investigation reveals that [2] performs
synchronously. This parallelization eliminates the need to
well on composed images, generating an image composed
explicitly trade off between NeRF and diffusion training,
of style-consistent edited sub-images with the same struc-
leading to a 10× speed-up in training. With multiple dif-
ture(asshowninthesupplementary). Thisobservationin-
fusiongenerationsrunningsynchronously,wecanalsoen-
spiresustoexploitanovelinputformatfordiffusionmodels
forcecross-generationconsistency.
–surroundingviewswithacompositionof onemainview
Augmenting 2D Diffusion with 3D-Informing Control-
andmanyreferenceviews,sothatallviewscollectivelypro-
Net. Intuitively, a 3D-consistent model needs to be 3D-
videcontextualinformation.AsillustratedinFig.2,thekey
aware; otherwise, it lacks the necessary information and
principles in the design of a surrounding view are: (1) the
may solely adapt to the input structured noise, potentially
main view that we focus on in this generation should oc-
leading to overfitting. Therefore, we incorporate an addi-
cupyalarge proportion; and (2)itshouldincludeas many
tionalControlNet[47]adaptorintoour2Ddiffusion,which
reference views as possible at a reasonable size to provide
injects 3D information as a new condition. The 3D infor-
context. Inpractice,weconstructasurroundingvieww.r.t.
mation is obtained by using NeRF to infer the depth and
a specific main view, by surrounding a large image of this
3Dpointforeachpixelintheview. Wethenqueryitsfea-
viewwith4(k−1)smallreferenceimagesofotherviews,
ture in a learnable 3D embedding (implemented as a hash
leaving a margin of arbitrary color. This ensures that the
tablein[23])toacquireapixel-wise3D-awarefeatureim-
main image is roughly (k−2) times larger than the small
age, which serves as the condition for ControlNet. These
images. Herek isahyperparameter. Thereferenceimages
componentsmaketheaugmenteddiffusiontobeawareof,
are randomly selected from all the views or nearby views
learn from, and generate results based on 3D information.
fromthemainview, providingbothaglobalpictureofthe
Additionally, we apply LoRA [11] to further enhance the
sceneandmuchoverlappedcontenttobenefittraining.
capabilityofdiffusion.
We use such surrounding views as input images to Self-Supervised Consistency Loss. Lacking ground
[2], by constructing the surrounding views of the current truth for consistently edited images, we introduce a self-
NeRF’s rendering results, structured noise, and original supervisedmethodtoenforce3Dconsistency. Forasetof
views. Though not directly trained with this image for- generatedmulti-viewimages,weconstructacorresponding
mat,[2]stillsupportsgeneratingeditedimagesinthesame reference set of 3D consistent multi-view images to serve
format, witheachimagecorrespondingtotheeditedresult as a self-supervision target. Inspired by [37, 42], we em-
of the image in the same position. The attention modules ploydepth-basedwarpingwithNeRF-rendereddepthtoes-
in its UNet implicitly connect the same regions in differ- tablish pixel correspondence across views. We design a
entviews,enablingthesmallviewstoprovideextracontext weightedaveragingprocesstoaggregatethesepixelstothe
to the main view. This results in consistently edited styles finalimage,ensuringmulti-viewconsistency(detailinsup-
amongallthesub-imagesinthesurroundingviewimage. plementary).
The surrounding views not only provide a context-rich Specifically,weeditnsurroundingviewssynchronously
input format for 2D diffusion models, but also allow it to onnGPUs,witheachsurroundingviewcontaining(4k−3)
generateeditedresultsfor(4k−3)viewsinonebatch,ben- views,resultinginatotalofV =(4k−3)nviews.Foreach
efitingourconsistency-enforcingtraininginSec.3.3. viewv,wewarptheeditedresultsoftheremainingV −1
5Figure3. ComparisonintheFangzhousceneshowsthatourConsistDreamerproducessignificantlysharpereditingresultswithmore
fine-grained textures and higher consistency with the instruction, e.g., Lord Voldemort with no hair on which all baselines fail. The
instructionsarethebottomtexts,exceptforNArt[34],whichusestheunderlinedtexts.Theimagesofbaselinesaretakenfromtheirpaper.
views to it, and compute their weighted average to obtain facilitatingthegenerationof3Dconsistentresults.
thereferenceviewv′.Wethenre-aggregatereferenceviews Shape Editing. Some instructions, e.g., Make him smile,
{v′} back into surrounding views in the original structure change the shape or geometry of the scene during editing,
for each GPU. These re-assembled surrounding views are whileourstructurednoiseandconsistency-enforcingtrain-
thenusedasthetargetimagestosupervise2Ddiffusion. ing rely on the geometry. To be compatible with shape
To guide 2D diffusion in preserving the original style editing, we design a coarse-to-fine strategy: we first edit
andavoidingsmoothingout,wedefineourconsistencyloss the scene using ConsistDreamer with only the surround-
as the sum of the VGG-based perceptual and stylization ing view and disabling the other two components, i.e., us-
loss [12], instead of a pixel-wise loss, between diffusion’s ing image-independent noise and the original implementa-
outputandthetargetimage.Inadditiontothisprimaryloss, tion of [2]. This allows the scene to converge to a coarse
we propose several regularization losses to prevent mode editedshapeaccordingtotheinstruction. Wethenactivate
collapse and promote 3D awareness (detail in supplemen- structured noise and consistency-enforcing training to re-
tary). With the consistency loss, ConsistDreamer effec- finetheediting. Weperiodicallyadjustthestructurednoise
tively enforces not only cross-view consistency among all with changes in geometry, while preserving the noise val-
viewsineachsurroundingview,butalsocross-generationor ues. Withthisstrategy,ConsistDreameralsoachieveshigh-
cross-batchconsistencyforviewseditedbydifferentGPUs. fidelityshapeediting.
Consistent Denoising Procedure. With our structured
noise, the denoising in 2D diffusion initiates with consis- 4.Experiments
tent noise. This leads to a further goal to make the entire
denoising procedure 3D consistent and thus end with con- EditingTasks. Inoursetting,eacheditingtaskisapairof
sistentimages.Weachievethisbyenforcingalltheviewsin (scene,instruction), indicating which instruction-
theintermediatedenoisingimagestobealso3Dconsistent guidededitingoperationshouldbeappliedonwhichscene.
at each denoising step. Therefore, unlike the conventional The output of the task is another scene, being the edited
diffusion training with single-step denoising, our training sceneundertheinstruction. Thescenesweuseforevalua-
involvesafullmulti-stepdenoisingprocedurewithpassing tioncontaintwoparts:(1)IN2N.ScenesusedbyIN2N[21],
throughgradients. Asitisimpossibletofittheentirecom- includingscenesofhumanfacesorbodies,outdoorscenes,
putationalgraphintotheGPUmemory,weusecheckpoint- and statues; and (2) SN++. Scenes in ScanNet++ [43],
ing[27,40]totradespacewithtime. Doingsoenablescon- which are complicated indoor scenes with free-formed
structingthereferencesetofimageswithwarpingforeach structures and camera trajectories. We also use two types
intermediate denoising step, which is then used to super- ofeditinginstructions:(1)styletransferwhichtransfersthe
visetheintermediatedenoisingimage. Thisprovidesmore style of the scene into the described style, and (2) object-
directsignalsof3Dconsistencyinthetrainingofdiffusion, specific editing which edits a specific object of the scene.
6Weusethesetaskstocompareourapproachwithbaselines,
andconductablationstudyonrepresentativetasks.
NeRF Backbone and Diffusion Model. For a fair com-
parison with previous works [6, 8], we use the Nerfacto
modelinNeRFStudio[32]asourNeRFbackbone,andthe
Variant Components A B C D
pre-trained diffusion model [2] from Hugging Face as our
Full All 27.4±0.4 159.8±2.1 62.7±1.9 132.8±1.2
initialcheckpoint.TheNeRFrepresentationforthesceneis NoStr.Noise −SN 35.0±0.5 234.8±2.4 83.1±2.2 217.8±2.0
trained with NeRFStudio in advance, and then used in our NoTraining −T 34.6±1.1 221.8±2.0 80.4±2.0 201.8±1.9
OnlySur.Views −SN−T 34.0±0.3 262.0±2.7 83.9±2.2 214.4±2.0
pipeline. “IN2N” −SN−SV−T 91.0±1.2 255.8±2.0 90.9±1.5 222.4±2.0
ConsistDreamerVariants. Weinvestigatethefollowing Table1.Ablationstudyonthedistillationfidelityscore(↓)quan-
variantsforourablationstudy(where−SN,−SV,and−T titativelyvalidatestheeffectivenessandcomplementarityofeach
denote removing structured noise, surrounding views, and ofourcomponents.OurfullConsistDreamersignificantlyoutper-
consistency-enforcingtraining,respectively): (1)FullCon- formsallvariantsacrossvariousscenesandtypesofinstructions.
sistDreamer. (2) No structured noise (−SN): use inde- withthetrainingobjectiveofDreamFusion[25],weaimto
pendently generated noise for each view instead of struc- minimizethedistancebetweentwodistributions: thedistri-
tured noise, but still use surrounding views and perform butionofarenderedimageatarandomviewfromtheedited
consistency-enforcing training. (3) No training (−T): use NeRF,andthedistributionofthediffusioneditingresultof
surrounding views and structured noise, but do not aug- animageatarandomviewintheoriginalscene. Following
ment and train [2] and keep using the original checkpoint. this, we define the fidelity metric as the Fre´chet inception
(4)Onlysurroundingviews(−SN−T):onlyusesurround- distance(FID)[9,30]betweentwosets–thesetofimages
ing views, and do not use structured noise or train [2]. rendered by the edited NeRF at all training views, and the
(5) “IN2N” (−SN −SV −T): ours with all the proposed set of edited images generated by the original [2] for all
componentsremoved,whichcanberegardedasanalterna- trainingviews,correspondingtothesetwodistributions. A
tiveversionofIN2N.Notethatconsistency-enforcingtrain- lowerFIDmeansahigherfidelitythattheeditingisapplied
ingrequiressurroundingviewstoproducesufficientedited tothescene.
views in one generation; we cannot remove surrounding Qualitative Results. The qualitative comparison in the
viewsbutstillapplyconsistency-enforcingtrainingon[2]. FangzhouscenefromtheIN2NdatasetisshowninFig.3.
Baselines. We mainly compare our method with two Distilling from the same diffusion model [2], IN2N [8],
baselines: Instruct-NeRF2NeRF (IN2N) [8] and ViCA- ViCA[6],andourConsistDreamerproduceresultsinasim-
NeRF (ViCA) [6], as they are most closely related to our ilarstyle. Asespeciallyshowninthe“VincentVanGogh”
task. We also compare with NeRF-Art (NArt) [34] as an and “Edvard Munch” editing, our ConsistDreamer gener-
early work. Other methods, however, lack publicly avail- ates results containing fine-grained representative textures
able or working code and/or only use a few scenes sup- of Van Gogh and Munch, while the baseline results are
ported by NerfStudio. Therefore, we could only compare blurredandonlycontainsimpleorcoarsetextures.Thisval-
with CSD [15], DreamEditor [49], GE [5], EN2N [31], idatesthatwithourproposedcomponents,ConsistDreamer
and PDS [17] under a few tasks in supplementary, and isabletogenerateconsistentimagesfrom[2]withdetailed
are unable to compare with Edit-DiffNeRF [45] and In- textures, and does not rely on consistency derived from
struct 3D-to-3D [13]. Note that ConsistDreamer solves NeRF, which, unfortunately, smooths out the results. No-
instruction-guided scene editing instead of scene genera- tably,inthe“LordVoldemort”case,ourConsistDreameris
tion,sowedonotcomparewithmodelsforthegeneration the only one that successfully edits the image to resemble
task[25,37,42,48]. thewell-known,distinctiveappearanceofLordVoldemort,
EvaluationMetrics. ObservingthatourConsistDreamer featuringnohairandapeculiarnose. Amongalltheediting
generates significantly sharper editing results, consistent tasks,ourConsistDreamerconsistentlyproduceseditingre-
with previous work [6, 8], we compare ConsistDreamer sultswiththemostdetailedearsandhair/head,anddoesnot
with baselines mainly through qualitative evaluation. For containunnaturalcolorblocks.
the ablation study, the appearance of the scenes edited by Additional qualitative results are shown in Fig. 4, and
our different variants may be visually similar and unable more results and the comparison with baselines on these
to be fairly compared using qualitative results. Therefore, tasksareprovidedinthesupplementaryandonourproject
weproposedistillationfidelityscore(DFS)toevaluatehow page. Overall, ourConsistDreamergeneratessharp, bright
faithful the editing is distilled and applied on NeRF com- editing results in all tasks across various scenes, including
pared with the diffusion’s output [2], rooted in the basic human, indoor, and outdoor scenes. (1) In the Face scene,
setting that we distill from [2] to edit 3D scenes. In this our ConsistDreamer successfully applies the plaid (check-
situation,oureditingabilityisboundedby[2]’s. Consistent ered)jacketediting,acommonfailurecaseinmostprevious
7Figure 4. ConsistDreamer consistently generates high-quality and high-fidelity editing results, featuring detailed, fine-grained textures
acrossvariousscenesandinstructions. Notably,ConsistDreameralsomaintainsthehighdiversityfrom[2],asexemplifiedbythehighly
diversifiedresults(a)(b).Additionalresultsandcomparisonsareprovidedinthesupplementaryandonourprojectpage.
methods,includingIN2N.Also,ourConsistDreamerisable sistDreamer outperforms all the variants with significant
to assign fine-grained marble texture in the marble statue gains in all tasks under DFS, which mainly comes from
editing, a clear mustache in Mustache editing, and clear ourconsistentdenoisingprocedureinSec. 3.3thatrequires
wrinkle and hair in Einstein editing, while IN2N produces allthreemajorcomponentstoachieve. Trainingtowardsa
blurredandover-smoothresultswithpoordetails. Notably, consistentdenoisingprocedureproducesconsiderableextra
ourConsistDreamerminimizesthesideeffectsoftheedit- supervision signals to the augmented [2], making it con-
ing,whileIN2Nunexpectedlyandsignificantlychangesthe verge better towards consistent generation results. We can
skincolorintheMustacheeditingandthewallcolorinthe alsoobservethattheconsistency-enforcingtrainingandthe
Einstein editing. (2) The Tolkien Elf and Fauvism editing use of surrounding views improve the fidelity in most of
tasksintheFangzhousceneshowthatourConsistDreamer the tasks, especially in the complicated large-scale indoor
could preserve most diversity from the original [2], due scenes (C)(D), showing that these components indeed im-
to the use of structured noise sampled for the whole edit- provetheconsistencyingeneration.
ing. With the structured noise, we can focus on the con-
sistency of generation for the given noise, without suffer-
5.Conclusion
ingfromaveragingresultsgeneratedfromdifferentnoises,
whichmaylosediversitybyconvergingtoanaveragestyle ThispaperproposesConsistDreamer,aninstruction-guided
for all noises. (3) Our ConsistDreamer works well in out- scene editing framework that generates 3D consistently
doorscenes,asallthedetailsonthefloor,mountain,plants, editedimagesfrom2Ddiffusionmodels. Empiricalevalua-
andcampsarepreservedintheeditedresults.(4)Incompli- tionshowsthatConsistDreamerproduceseditingresultsof
catedindoorscenesfromtheScanNet++dataset,ourCon- significantlyhigherquality,exhibitingsharper,brighterap-
sistDreamer generates editing results that are easy to rec- pearance with fine-grained textures, across various scenes
ognize as the given style, with fine-grained textures (Van including forward-facing human scenes, outdoor scenes,
Gogh),regularpatterns(Picasso),orspeciallightingcondi- andevenlarge-scaleindoorscenesinScanNet++, whereit
tions(BastionandTransistor).Alltheseresultsvalidatethat succeedsincommonfailurecasesofpreviousmethods. We
ourConsistDreamergenerateshigh-qualityeditingresults. hopethatourworkcanserveasasourceofinspirationfor
AblationStudy. AsshowninTable1,weconducttheab- distillation-based3D/4Deditingandgenerationtasks.
lation study on four representative tasks (A)-(D), covering Acknowledgement. Jun-Kun and Yu-Xiong were supported in part by
instructions of object-specific editing, artistic style trans- NSFGrant2106825andNIFAAward2020-67021-32799,usingNVIDIA
fer, and other style transfer, and scenes of human, indoor, GPUsatNCSADeltathroughallocationsCIS220014andCIS230012from
and outdoor scenes. The results show that our full Con- theACCESSprogram.
8References [17] JuilKoo,ChanhoPark,andMinhyukSung. Posteriordistil-
lationsampling,2023. 7,12
[1] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Pe-
[18] Chen-HsuanLin,JunGao,LumingTang,TowakiTakikawa,
ter Hedman, Ricardo Martin-Brualla, and Pratul P. Srini-
Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,
vasan. Mip-NeRF: A multiscale representation for anti-
Ming-YuLiu,andTsung-YiLin. Magic3D:High-resolution
aliasingneuralradiancefields. InICCV,2021. 3
text-to-3Dcontentcreation. InCVPR,2023. 3,17
[2] Tim Brooks, Aleksander Holynski, and Alexei A. Efros.
[19] LingjieLiu,JiataoGu,KyawZawLin,Tat-SengChua,and
Learning to follow image editing instructions. In CVPR,
ChristianTheobalt. Neuralsparsevoxelfields. InNeurIPS,
2023. 1,2,3,4,5,6,7,8,11,12,13,15,16,17
2020. 3
[3] AnpeiChen,ZexiangXu,FuqiangZhao,XiaoshuaiZhang,
[20] Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard
Fanbo Xiang, Jingyi Yu, and Hao Su. MVSNeRF: Fast
Zhang, Junyan Zhu, and Bryan C. Russell. Editing condi-
generalizableradiancefieldreconstructionfrommulti-view
tionalradiancefields. InICCV,2021. 3
stereo. InICCV,2021. 3
[21] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
[4] Jun-KunChen,JipengLyu,andYu-XiongWang. NeuralEd-
JonathanT.Barron,RaviRamamoorthi,andRenNg.NeRF:
itor: Editing neural radiance fields via manipulating point
Representingscenesasneuralradiancefieldsforviewsyn-
clouds. InCVPR,2023. 3
thesis. InECCV,2020. 3,6
[5] YiwenChen,ZilongChen,ChiZhang,FengWang,Xiaofeng
[22] Norman Mu¨ller, Yawar Siddiqui, Lorenzo Porzi,
Yang, YikaiWang, ZhongangCai, LeiYang, HuapingLiu,
Samuel Rota Bulo, Peter Kontschieder, and Matthias
andGuoshengLin. GaussianEditor: Swiftandcontrollable
Nießner. DiffRF: Rendering-guided 3D radiance field
3deditingwithgaussiansplatting,2023. 7,12
diffusion. InCVPR,2023. 2,4
[6] Jiahua Dong and Yu-Xiong Wang. ViCA-NeRF: View-
[23] ThomasMu¨ller,AlexEvans,ChristophSchied,andAlexan-
consistency-aware 3D editing of neural radiance fields. In
derKeller.Instantneuralgraphicsprimitiveswithamultires-
NeurIPS,2023. 1,3,7,11,17
olution hash encoding. ACM Trans. Graph., 41(4):102:1–
[7] Yuan-ChenGuo,DiKang,LinchaoBao,YuHe,andSong- 102:15,2022. 5
Hai Zhang. NeRFReN: Neural radiance fields with reflec-
[24] Yicong Peng, Yichao Yan, Shengqi Liu, Yuhao Cheng,
tions. InCVPR,2022. 3
Shanyan Guan, Bowen Pan, Guangtao Zhai, and Xiaokang
[8] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Yang. CageNeRF:Cage-basedneuralradiancefieldforgen-
Holynski, and Angjoo Kanazawa. Instruct-NeRF2NeRF: eralized3Ddeformationandanimation. InNeurIPS,2022.
Editing3Dsceneswithinstructions. InICCV,2023. 1, 3, 3
4,5,7,11,12,13,15,17
[25] BenPoole,AjayJain,JonathanT.Barron,andBenMilden-
[9] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, hall.DreamFusion:Text-to-3Dusing2Ddiffusion.InICLR,
Bernhard Nessler, and Sepp Hochreiter. GANs trained by 2023. 1,3,4,7
atwotime-scaleupdateruleconvergetoalocalnashequi-
[26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
librium. InNeurIPS,2017. 7
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
[10] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu- Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
sionprobabilisticmodels. InNeurIPS,2020. 3 Krueger, and Ilya Sutskever. Learning transferable visual
[11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- modelsfromnaturallanguagesupervision. InICML,2021.
Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhuChen. 3,12,13
LoRA: Low-rank adaptation of large language models. In [27] Elvis Rojas, Albert Njoroge Kahira, Esteban Meneses,
ICLR,2022. 5 Leonardo Bautista-Gomez, and Rosa M. Badia. A study
[12] JustinJohnson,AlexandreAlahi,andLiFei-Fei. Perceptual ofcheckpointinginlargescaletrainingofdeepneuralnet-
losses for real-time style transfer and super-resolution. In works. arXivpreprintarXiv:2012.00825,2020. 6
ECCV,2016. 6 [28] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
[13] HiromichiKamata,YuikoSakuma,AkioHayakawa,Masato PatrickEsser,andBjo¨rnOmmer.High-resolutionimagesyn-
Ishii, and Takuya Narihira. Instruct 3D-to-3D: Text in- thesiswithlatentdiffusionmodels. InCVPR,2022. 3,12
struction guided 3D-to-3D conversion. arXiv preprint [29] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
arXiv:2303.15780,2023. 3,7,12,17 MichaelRubinstein,andKfirAberman. Dreambooth: Fine
[14] Bernhard Kerbl, Georgios Kopanas, Thomas Leimku¨hler, tuning text-to-image diffusion models for subject-driven
and George Drettakis. 3d gaussian splatting for real-time generation. InCVPR,2023. 3,11,12
radiancefieldrendering. TOG,2023. 2 [30] Maximilian Seitzer. pytorch-fid: FID Score for PyTorch.
[15] SubinKim,KyungminLee,JuneSukChoi,JongheonJeong, https://github.com/mseitzer/pytorch-fid,
KihyukSohn2,andJinwooShin1. Collaborativescoredis- 2020. Version0.3.0. 7
tillationforconsistentvisualediting. InNeurIPS,2023. 1, [31] Liangchen Song, Liangliang Cao, Jiatao Gu, Yifan
3,7,11 Jiang, Junsong Yuan, and Hao Tang. Efficient-
[16] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitz- NeRF2NeRF:Streamliningtext-driven3deditingwithmul-
mann. DecomposingNeRFforeditingviafeaturefielddis- tiviewcorrespondence-enhanceddiffusionmodels,2023. 7,
tillation. InNeurIPS,2022. 3 12
9[32] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, [47] LvminZhang, AnyiRao, andManeeshAgrawala. Adding
Brent Yi, Justin Kerr, Terrance Wang, Alexander Kristof- conditional control to text-to-image diffusion models. In
fersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, David ICCV,2023. 2,5
McAllister,andAngjooKanazawa. Nerfstudio: Amodular [48] Joseph Zhu and Peiye Zhuang. HiFA: High-fidelity text-
framework for neural radiance field development. In SIG- to-3D with advanced diffusion guidance. arXiv preprint
GRAPH,2023. 7,12 arXiv:2305.18766,2023. 3,7,14
[33] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, [49] Jingyu Zhuang, Chen Wang, Lingjie Liu, Liang Lin, and
Jonathan T. Barron, and Pratul P. Srinivasan. Ref-NeRF: Guanbin Li. DreamEditor: Text-driven 3D scene editing
Structured view-dependent appearance for neural radiance withneuralfields. InSIGGRAPHAsia,2023. 3,7,11
fields. InCVPR,2022. 3
[34] Can Wang, Ruixiang Jiang, Menglei Chai, Mingming He,
DongdongChen,andJingLiao.NeRF-Art:Text-drivenneu-
ralradiancefieldsstylization. IEEETransactionsonVisual-
izationandComputerGraphics,pages1–15,2023. 3,6,7
[35] QianqianWang,ZhichengWang,KyleGenova,PratulSrini-
vasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-
Brualla, Noah Snavely, and Thomas Funkhouser. IBRNet:
Learningmulti-viewimage-basedrendering.InCVPR,2021.
3
[36] ZhengyiWang,ChengLu,YikaiWang,FanBao,Chongxuan
Li, Hang Su, and Jun Zhu. ProlificDreamer: High-fidelity
anddiversetext-to-3Dgenerationwithvariationalscoredis-
tillation. InNeurIPS,2023. 3,17
[37] Jianfeng Xiang, Jiaolong Yang, Binbin Huang, and Xin
Tong. 3D-awareimagegenerationusing2Ddiffusionmod-
els. InICCV,2023. 2,3,5,7
[38] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin
Shu,KalyanSunkavalli,andUlrichNeumann. Point-NeRF:
Point-basedneuralradiancefields. InCVPR,2021. 3
[39] TianhanXuandTatsuyaHarada. Deformingradiancefields
withcages. InECCV,2022. 3
[40] XiangzheXu,HongyuLiu,GuanhongTao,ZhouXuan,and
Xiangyu Zhang. Checkpointing and deterministic training
fordeeplearning.In2022IEEE/ACM1stInternationalCon-
ference on AI Engineering – Software Engineering for AI
(CAIN),2022. 6
[41] Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han
Zhou, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui.
Learningobject-compositionalneuralradiancefieldfored-
itablescenerendering. InICCV,2021. 3
[42] JiayuYang, ZiangCheng, YunfeiDuan, PanJi, andHong-
dong Li. ConsistNet: Enforcing 3Dconsistencyfor multi-
view images diffusion. arXiv preprint arXiv:2310.10343,
2023. 2,3,5,7
[43] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner,
andAngelaDai. ScanNet++: Ahigh-fidelitydatasetof3D
indoorscenes. InICCV,2023. 1,2,5,6,13,14
[44] AlexYu,VickieYe,MatthewTancik,andAngjooKanazawa.
pixelNeRF:Neuralradiancefieldsfromoneorfewimages.
InCVPR,2021. 3
[45] LuYu,WeiXiang,andKangHan. Edit-DiffNeRF:Editing
3Dneuralradiancefieldsusing2Ddiffusionmodel. arXiv
preprintarXiv:2306.09551,2023. 3,7,12
[46] Yu-Jie Yuan, Yang tian Sun, Yu-Kun Lai, Yuewen Ma,
Rongfei Jia, and Lin Gao. NeRF-Editing: Geometry edit-
ingofneuralradiancefields. InCVPR,2022. 3
10Supplementary Material
FigureB.1. QualitativecomparisonswithbaselineCSDonthree
tasksshowthatourConsistDreamerachieveshigh-qualityediting,
outperformingbothIN2NandCSDwithmoresuccessfulediting.
Thisdocumentcontainsadditionalanalysisandextraex-
periments. Thecontentofthisdocumentissummarizedas
below:
A.SupplementaryVideo(SV)
To better visualize our results and compare with base-
lines beyond static 2D images, we provide a supple-
mentary video (SV) on our project page at immor-
talco.github.io/ConsistDreamer. We also include a short
demo in this video, to enhance the understanding of 3D-
consistentstructurednoise. Theoriginalsizeofthevideois Figure B.2. Compared with DreamEditor, our ConsistDreamer
around1.25GB,thereforewehavetocompressittofititin achieves better editing, which not only follows and satisfies the
theuploadsizelimitationof200MBonOpenReview. giveninstructions,butalsopreservesasmuchcontentoftheorig-
inalsceneaspossible. Onthecontrary,DreamEditorcompletely
Inthefollowingsections,weuseSVtorefertothissup-
editstheoriginalpersontoanotherinallthetasks.
plementaryvideo.
B.ComparisonswithAdditionalBaselines
(Graphic),Anime,andSmile.
AsshowninFig. B.1andSV,ourConsistDreamersig-
In the main paper, we compare our ConsistDreamer with
nificantly outperforms IN2N, which fails in the Low-Poly
IN2N [8] and ViCA [6]. In this section, we compare
andAnimetasks,andhasthesideeffectsofaddingbeards
ourConsistDreamerwithotherbaselinesandprovidesome
in the Smile task. Compared with CSD, our editing in
analysis. Thesemethodseitherdonothavepubliclyavail-
the Low-Poly task is more noticeable, with a successfully
ablecode,orevaluateonthesceneswhicharenotsupported
edited hair part. Our edited scene in the Smile task is the
by NeRFStudio. Therefore, we could only compare our
only one among all three to successfully show the teeth
ConsistDreamerunderthetasksusedbythem,withthepro-
whensmiling,whileCSD’sresultcontainsstrangemuscles
videdvisualizationsfromtheirpapersorwebsites.
as if the person is keeping a straight face. In conclusion,
We also provide some comparisons in the video format
our ConsistDreamer achieves more successful editing than
ofthebaselinesinSV.
CSD.
B.1.CSD[15]
B.2.DreamEditor[49]
CSDisamethodfocusingongeneralconsistentgeneration,
DreamEditor is another method focusing on scene editing,
includinglargeimageediting,sceneediting,andscenegen-
but with another diffusion model [29] instead of [2]. As
eration. We compare our ConsistDreamer with CSD un-
NeRFStudiodoesnotsupporttheotherscenes,wecompare
der three tasks shown on the website of CSD1: Low-Poly
our ConsistDreamer with DreamEditor by comparing Fig.
1https://subin-kim-cv.github.io/CSD/ 3inourmainpaperwithFig. 8in[49].
11contrast, we focus on editing more challenging and realis-
tic scenes. In addition, as NeRFStudio and NeRFacto do
notsupportLLFFandNSdatasetswell(morespecifically,
NeRFStudiodoesnotsupporttheLLFFdataset, andNeR-
Factoworkswellinrealscenesbutnotinsyntheticscenes
like NS), we cannot compare with Instruct 3D-to-3D on
these two datasets. Moreover, the code of Instruct 3D-to-
3D is not publicly available. Therefore, we are unable to
comparewithInstruct3D-to-3D.
B.5. Concurrent Works: GE [5], EN2N [31], And
PDS[17]
[5, 17, 31] are three concurrent works. GE [5] and EN2N
[31] achieve 3D editing through the same 2D diffusion
model [2] and have some modifications in the pipeline or
scenerepresentation,whilePDS[17]proposesanotherdis-
tillationformulaandusesDreamBooth[29]forediting.
The comparisons against them are in Fig. B.4. Our
ConsistDreamergenerateshigh-qualityeditingresultswith
FigureB.3.OurConsistDreamerachievesconsistenteditinginthe brighter color and clearer textures, while all these concur-
checkered/plaidpattern(alsovisualizedassmoothvideoinSV), rentworksgenerateblurredtextures,gloomycolors,and/or
whileEdit-DiffNeRFhasobviousinconsistencyintheshapeand unsuccessfulorunreasonableediting.
textureofthecollar.
C.CLIP[26]MetricsInIN2N[8]
Fig. B.2 presents the results in these tasks, along with We provide the quantitative comparison with CLIP [26]
other baselines in Fig. 3 in our main paper. It shows that metricsintroducedinIN2N[8]inTab. C.1. Inallfourabla-
our ConsistDreamer preserves most of the contents in the tionscenes,ourssignificantlyandconsistentlyoutperforms
originalscenewhileediting,e.g.,theshapeoftheheadand IN2Ninbothmetrics.
face,andtheshapeandtypeoftheclothes,minimizingthe
side effects of editing. DreamEditor, however, completely D.ImplementationDetails
editsthepersontoanotherperson,evenintheFauvismtask,
D.1.HyperparametersandSettings
which is supposed to be only style transfer. This demon-
strates that our ConsistDreamer achieves more reasonable In our experiments, we use the multi-GPU pipeline with
editingthanDreamEditor. n = 3 (4 GPUs in total), and surrounding views of k = 5
(1mainviewand12referenceviews).
B.3.Edit-DiffNeRF[45]
Thelearningrateofeachcomponentisshownbelow:
Edit-DiffNeRF is another paper that also claims to suc- • NeRFacto [32]: 5 × 10−3 for field part, and 10−2 for
cessfully complete the checkered/plaid pattern. As they proposalnetworkpart.
didnotprovideanycode,wecompareourConsistDreamer • LoRA-augmented diffusion model [2]: consistent with
with the images provided in their paper. As shown in Fig. theiroriginalimplementation(10−4).
B.3,ourConsistDreamerachievesconsistenteditingamong • Learnable3Dpositionalembedding: 2×10−3.
all three views, while Edit-DiffNeRF’s results are multi- All the views are resized to 3 : 4 or 4 : 3 according
view inconsistent, obviously shown in the collar part. The totheirorientations. Forlandscapeimages(portraitimages
smooth video of our rendering result in SV also shows the use the same setting with a flipped height and width), the
consistency of our ConsistDreamer. These results validate diffusion model takes a surrounding view image input at
that our ConsistDreamer archives significantly better con- 1152×864(also4 : 3),withhorizontalsplittersatheights
sistency in checkered/plaid patterns, while Edit-DiffNeRF 6pix, andverticalsplittersatwidths8pix. Thesizesofthe
failstoachievesuchconsistency. main view and reference views are 688×516 and 224×
168,respectively.Thissettingisconsistentwiththeoriginal
B.4.Instruct3D-to-3D[13]
usage of diffusion models [2, 28] trained at 512×512, as
Instruct3D-to-3Disamethodfocusingonstyletransferof ourmainviewhasaheightclosetoit.
scenes. It uses LLFF and NeRF Synthetic (NS) scenes as Consistent with IN2N [8], both MSE and LPIPS losses
editing tasks instead of the widely-used IN2N dataset. In areusedtotrainNeRF.
12FigureB.4. OurConsistDreameroutperformsallthreeconcurrentworkswithbrightercolor, clearertextures, andbettereditingresults
matchedwiththeeditinginstruction.
Text-ImageDirectionSimilarity(CTIDS)↑ DirectionConsistency(CDC)↑
Method
A B C D A B C D
Ours 0.0259 0.1679 0.1204 0.1268 0.5785 0.1735 0.3077 0.2878
IN2N[8] 0.0099 0.1252 0.1163 0.1055 0.5106 0.1634 0.2900 0.1772
Table C.1. Our ConsistDreamer significantly and consistently outperforms baseline IN2N in CLIP [26] metrics over all four ablation
scenes.
FigureD.1. Ourmulti-GPUtrainingpipelineusesn+1GPUs,onededicatedforNeRFfitting,andtheothersfordiffusiontrainingand
generation.
D.2.ViewpointsAndCameraTrajectory D.3.TrainingSchedule
One standard full training contains 1,600 epochs across
multiplesub-stages. Allthesestagesareexplainedbelow:
During the distillation process, we directly use the view- • InitializationStage(Epoch1 ∼ 200): Traindiffusion[2]
pointsprovidedintheoriginalscenedataset, whichissuf- before NeRF fitting. We perform one diffusion training
ficient to cover the whole scene. In visualization, we use stepinoneepoch.
the provided camera trajectory for IN2N [8] dataset, and – Early Bootstrap (Epoch 1 ∼ 50): Train the LoRA-
manuallyconstructanothercameratrajectoryforasmooth augmented diffusion model to mimic the behavior of
visualizationforScanNet++[43]dataset. theoriginalmodelwiththeaugmentedinputof3Dpo-
13sitional embedding. The weight regularization loss of D.4.StructuredNoiseImplementation
maintaining original behavior (detailed in D.8) is sig-
In the main paper, the structured noise is implemented by
nificantlyhigher. NeRFtraininghasnotstarted.
constructing“adensepointcloudofthescenebyunproject-
– Bootstrap (Epoch 51 ∼ 150): Train the consistency-
ingallthepixelsinalltheviews”,andrendering/projecting
awareness of the LoRA-augmented diffusion model
such a point cloud at a view to generate the structured
while keeping original behavior, at a similar impor-
noise.Directlyimplementingthisliteraldescriptioniscom-
tancewithbalancedweights.
plicatedandinefficient.
– Warming Up (Epoch 151 ∼ 200): Use the standard
Therefore,weuseanequivalentimplementation.
weightstobalancetheconsistencylossandregulariza-
• Insteadofexplicitlygeneratingthisdensepointcloud,we
tion, focusing more on consistency. This epoch gen-
justputtheweightednoisesoneachpixelofallviews.
erates sufficient images for the edited view buffer for
• Fortheviewwequeryforstructurednoise, wewarpthe
NeRFfitting.
noisefromallotherviewstoit. Thisisequivalenttopro-
• Distillation Stage (Epoch 201 ∼ 1600): Train diffusion
jectingthesub-pointcloudgeneratedbyeachviewtothe
while fitting NeRF. In each of 4 epochs, we do 3 diffu-
querying view; therefore, it is equivalent to the original
sion generation steps without training (to fill the edited
design.
viewbuffer), andonlyonediffusiontrainingstep. Here,
With this implementation, explicitly generating, main-
the “noise level” means the mixture rate of the current
taining,andprojectingapointcloudwithbillionsofpoints
NeRF(beingedited)renderedimageandthenoiseasthe
(numberofviews×height×width)isunnecessary,anda
diffusion’sinput: fullnoiselevelmeansusingonlynoise
querycanbecompletedinlessthanonesecond.
for generation (standard generation), while a 30% noise
levelmeanstheinputimageisthemixtureof30%noise D.5.SurroundingViews-ReferenceViewSelection
and70%renderedimage.
We construct the surrounding view with one large main
– FullNoiseGeneration(Epoch201 ∼ 500): Thediffu-
view and several small reference views. The purpose of
sion model is trained and used for generation at a full
the reference views is two-folded: (1) to provide enough
noise level to edit the views sufficiently regardless of
context about the whole scene, and (2) to have enough
NeRF.
overlappedpartsofthemainviewtofacilitateconsistency-
– Pre-Annealing (Epoch 501 ∼ 600): The diffusion
enforcingtraining.Therefore,weselect40%oftheviewsto
model is trained and used for generation with a noise
bearandomviewofthescene,andtherest60%oftheviews
level sampled from [70%,100%]. It edits the views
to be a view with at least 20% overlap of the main view
with a few references to the current NeRF, starting to
(quantifiedbytheareaofmatchedpixelsthroughwarping).
refinethecurrentNeRF.
Theorderoftheviewsisrandomlyshuffled. Weobserved
– Annealing (Epoch 601 ∼ 1500): Following the idea
thatnoneoftheserandomnesseshighlyaltertheeditingre-
of HiFA [48], the range of the noise level linearly an-
sult – after consistency-enforcing training, any choice of
nealsfrom[70%,100%]to[10%,40%].TheNeRFwill
reference views and their order will lead to a consistent
graduallyconvergetoafine-grainededitedversion.
editedresultofthemainview.
– Ending(Epoch1501 ∼ 1600): Thediffusionmodelis
trainedandusedforgenerationwithanoiselevelsam- D.6.Training-PixelWeights
pled from the annealed range [10%,40%], to further
In consistency-enforcing training, we apply warping and
refinetheeditedNeRF.
weightedaveragestocomputethetrainingreferenceviews
{v′},sothatalltheviewsin{v′}are3Dconsistent. Using
Iftheeditingtaskrequireseditingthegeometryorshape
identicalweightsforallpixelswillresultinblurredimages:
ofthescene(“shapeediting”),thedepth-basedwarpingus-
In a scene of a person, one view only contains their face,
ingthedepthoftheoriginalscenewillbeinaccurate.There-
andanotherviewcontainsthewholebody. Warpingthelat-
fore, in the Initialization Stage, we put the original diffu-
ter to the former indicates an upsampling of the face part,
sionmodel’soutputtotheeditedviewbufferforNeRFfit-
which will be blurred. Merging the blurred, warped view
ting, equivalently using IN2N in this stage. In the distilla-
with the former view at the same weight results in blurred
tion stage, the shape of the NeRF will be adjusted to the
overallresults.
edited shape in a short time, and then we will start to use
Weproposeabetterpixel-weightingstrategybasedona
thetraineddiffusionmodel’soutputforNeRFfitting.
furtheranalysisofthissituation. Ifwewarppixelatopixel
b, where a has a larger “scope” and contains more scene
Inourexperiments,mostIN2Nscenesconvergetoafine- objects, then we need to upsample the b part of the view
grained edited scene at 600 ∼ 700 epochs, while Scan- froma,resultinginblurry. Therefore,theweightshouldbe
Net++[43]scenestakearound1000epochs. relatedtothescopeofthepixel. Followingthis,wedefine
14thepixelareatoquantifythisscope. Forapixelpinaview original image, encouraging the utilization of 3D infor-
fromcamerapositiono,thefourverticesofthepixelgrids mation. This regularization loss is also applied on the
correspondtotherays{o+td }4 .WeuseNeRFtopredict UNetineachdenoisingstep.
i i=1
eachoftheirdepth{t }4 ,andcalculatetheircorrespond- • EncourageConsistentEditingStyle. Thediffusionmodel
i i=1
ingpointsP =o+t d .ThepixelareaS(p)ofthispixelis hassomediversityinediting. However,weneedtocon-
i i i
definedastheareaofasquarewithverticesP ,P ,P ,P vergetoonespecificstyleinoneeditingprocedure, oth-
1 2 3 4
inthe3Dspace,whichcanberegardedasanapproximation erwise, the NeRF may use view-dependency to overfit
ofthesurfaceareathepixelrepresents. Asweneedalower different styles at different views. Therefore, in the Pre-
weightforapixelwithalargerscope,i.e.,largerS(p),we Annealingstep(Sec. D.3),weusetheNeRF’srendering
definetheweightas1/S(p),whichsatisfiesallourneeds. result to supervise the diffusion model, to make it con-
vergetothestyleNeRFconvergesto.
D.7.Training-Multi-GPUPipeline
Anillustrationofourmulti-GPUtrainingpipelineisinFig.
D.1. By implementing such a parallelization pipeline by
ourselves, we decouple NeRF training with diffusion gen- D.9.Variant“IN2N”AndIN2N[8]
erationandtraininginthemostasynchronizedway,waiving
thenecessity oftrade-offsbetween NeRFtrainingand dif-
In our ablation study in the main paper, we have a variant
fusion,achievingconsiderablespeedup.
“IN2N”beingourfullConsistDreamerwithallthreemajor
D.8.Training-Regularizations components removed. In this section, we discuss how it
isequivalenttoanimplementationofIN2N,andthemajor
We use the consistency loss as the main loss in the
differencesbetweenthem.
consistency-enforcingtraining. However,thislossonlyen-
forcesseveralequalities(requiredbyconsistency), leading IN2N is a method that (1) gradually generates newly
to trivial results of a pure-color image without regulariza- editedimageswithanoiselevel(detailedinSec. D.3)sam-
tionlosses–thisisalsoreasonableasallpure-colorimages pled from [70%,98%], and (2) uses the newly generated
ofthesamecolorareperfectlyconsistent. Also,thereisno images to fit the NeRF, while the fitting NeRF’s rendering
encouragementorenforcementtousethe3Dinformationin results can affect the following editing (through the input
the 3D positional embedding. To avoid these, we propose of diffusion modelas a mixture with noise). This matches
severalregularizationlosses,asshownbelow: our pre-annealing sub-stage. Therefore, “IN2N” includes
• Maintain Original Behavior. We expect that the trained vanillaIN2Nasasub-procedure. Additionally,“IN2N”has
diffusionmodelwillgenerateimagesthatareverysimilar thefollowingimprovementsbeyondIN2N:
totheoriginalmodelwhenalltheinputs(image, noises,
and 3D positional embeddings) are identical. Therefore, • IN2N only samples noise levels from [70%,98%]. This
weuseMSEandVGGperceptualandstylizationlosses, makesIN2N(1)sometimesunabletosufficientlyeditthe
to regularize both the generated images and the con- sceneduetotheabsenceof100%noiselevelediting(e.g.,
structed referenced images (with gradient, generated by unabletoachieveaLordVoldemorteditingwithnohair
warpingandaveraging)ofthetraineddiffusion,withthe inFig.B.2),and(2)cannotrefinetheeditingresultsbased
originalmodel’soutput. WefurtherexpectthattheUNet onaconvergedstyle,andsometimesevendeviatesfroma
in the trained diffusion model predicts similar noises at convergedstyletoanother,asthenoiselevelisalwaysas
eachdenoisingstepastheoriginalUNet, sowealsouse highas70%. Thevariant“IN2N”startsatafullnoisebe-
thistoregularizeduringeachdenoisingstep. fore the pre-annealing sub-stage, guaranteeing sufficient
• Encourage 3D Information Utilization. The original [2] editing. After the pre-annealing sub-stage, “IN2N” an-
takes the original image of the scene as another part of nealsthenoiselevelrangetorefinetheresults,leadingto
input,usingitasaconditiontogeneratetheeditedimage. amorefine-grainedediting.
Toencourage3Dinformationutilization,wedesignareg- • IN2N adds the newly edited image to the dataset by re-
ularization loss, to enforce the diffusion model without placing a subset of pixels, which may negatively affect
the original image input to generate very similar results the LPIPS/perceptual loss. “IN2N” uses an edited view
totheonewiththeoriginalimageinput(bothwith3Dpo- buffer to fit NeRF containing only full, edited views, on
sitional embedding input). With the lack of the original whichtheperceptuallosscanperformwell.
image, the only way for the diffusion model to perceive
theoriginalviewisthe3Dpositionalembedding. There- Inconclusion,ourvariant,“IN2N,”isanequivalentandim-
fore,thediffusionistrainedtousethe3Dpositionalem- provedimplementationofIN2N.AsshowninSV,“IN2N”
bedding at least for novel view synthesis to recover the generatesnoticeablybetterresultsthanIN2N.
15FigureE.1. Thepre-traineddiffusionmodel[2]worksasexpectedonsurroundingviews,byeditingeachsub-viewintheinstructedway
individuallybutinaconsistentstyle. Notably,asshowninthelastrow,thesurroundingviewenrichesthecontext,makingthediffusion
modelsucceedinviewsthatfailinsingle-viewediting.
FigureE.2.Evenforthesameview,generatingfromdifferentnoisesdoesnotnecessarilyleadtotheconsistenti.e.,thesame,editedresult.
Eachcolumnrepresentsagenerationfromanoisedifferentfromothercolumns.
E.SupportingEvidenceforClaims E.2.DifferentNoisesLeadtoVariedResults
AsshowninFig.E.2,generationfromdifferentnoisesleads
E.1.DiffusionModelsPerformWellwithComposed
to completely different images, which is the fundamental
Images
constraint of all the baselines, which do not control the
As shown in Fig. E.1, the pre-trained diffusion model [2], noise. Even with surrounding views, the diffusion model
thoughnotdirectlytrainedinthispattern,stillworksasex- [2]stillgeneratesimagesinhighlyinconsistentways. The
pectedinsurroundingviews. Itgenerateseditingresultsfor diversityofthediffusionmodelunderdifferentnoisesisde-
each sub-view individually while all of them also share a sirablein2Dgenerationandediting,buthastobecontrolled
similar style, across various scenes, including indoor, out- in3Dgenerationforconsistency.
door,andface-forwardingscenes.
F.AdditionalAblationStudyAnalysis
Notably, as shown in the last row, when editing a view
with little context, directly editing the single view fails.
F.1.‘NoStr. Noise’vs. ‘OnlySur. Views’
Constructingasurroundingviewusingitasthemainview,
however, helpsthediffusionmodel[2]toachievesuccess- Both variants do not have structured noise. Hence, the
fulediting. Thisshowstheeffectsofsurroundingviewsin consistency-enforcing training in ‘No Str. Noise’ forces
achievingsuccessfulandconsistentediting. themodeltogeneratethesameresultfromdifferentnoises,
16whichleadstomodecollapseanddegradestheeditingresult or view-independence, only generate blurred results with-
towards blurred, averaged color. These negative effects of out notable effects, or even overfit to inconsistent editing
trainingin‘NoStr. Noise’leadstosimilarandevenworse withtheview-dependencyofNeRF.
resultsandDFSthan‘OnlySur. Views’withnotraining. EditingCapabilitiesConstrainedby2DDiffusionMod-
els. OurConsistDreamerdistillsfromthediffusionmodel
F.2.‘OnlySur. Views’vs. ‘IN2N’
[2]toeditscenes. Therefore, theeditingability, style, and
Tasks B,C,D are style transfer, specifically well supported diversity of ConsistDreamer are inherently constrained by
by our current 2D diffusion model [2]. Our DFS metric, [2]. Our ConsistDreamer edits a scene in a specific man-
basedonFID,usesafeatureextractorwithmoretolerance nerfollowing[2]. Forexample,inthe“VincentVanGogh”
fordifferentstyletransferresultsinthesameimage.Hence, editinginFig. B.2, ourConsistDreamer, alongwithIN2N
even ‘IN2N’ performs comparably with a slightly lower [8]andViCA[6]whichusethesame[2]forediting,shows
DFS. a side effect that transfers the style of the image to Van
By contrast, task A is a general object-centric editing Gogh’s painting style. Moreover, we cannot support edit-
withdiversifiededitingmanners–differentvalideditingre- ing tasks on which the diffusion model cannot perform.
sultscanhavejacketswithcompletelydifferentcolorsand Despite this common constraint among all the distillation-
styles. There can even be geometric changes in the cloth- based methods, our ConsistDreamer successfully transfers
ing without surrounding views as context to constrain the mostoftheeditingcapabilitiesofthe2Ddiffusionmodelto
editing,leadingtoasignificantlyworseDFSfor‘IN2N.’ 3D,byachievinghigh-qualityandhigh-diversity3Dscene
editing.
G.Discussion 3DUnderstandingandReasoning. ThoughourConsist-
Dreameris3D-informedand3D-awarewiththeadditional
G.1.ExtensiontoSceneGeneration
input of 3D positional embedding – already surpassing all
The proposed ConsistDreamer primarily focuses on the thebaselines–itisunabletoreasonandunderstandthese-
distillation-guided 3D scene editing task. However, the mantics of each part of 3D scenes. Therefore, while our
core contributions – structured noise, surrounding views, ConsistDreamercaneditaviewusingtheknowledgeofthe
andconsistency-enforcingtraining–canalsobeextendedto wholescene’sshape(via3Dpositionalembedding)andap-
the scene generation task. For example, these components pearance (through the surrounding view), it may still en-
can be used in the refinement phase, when the shape of a countermulti-faceissuesorJanusproblems. Specifically,it
sceneisroughlydetermined. Inthisway,thesecomponents doesnotunderstandwhatthecorrectorientationoftheface
could help achieve consistent and high-fidelity generation, is,doesnotknowthatapersoncanonlyhaveoneface,and
refiningtheshapewithslightadjustmentsformoredetailed thuscannotavoidthisproblem.
and precise geometry. Compared with previous methods Shape Editing. Some instructions for editing tasks may
[18, 36], this method can generate scenes with detailed, involve modifying the geometry or shape of a scene, e.g.,
high-fidelitytexturesandshapes,withoutmeshexportation “give him a beard” creates a beard on the face. Like the
orfixinggeometry. baselines, our ConsistDreamer is designed to support sim-
pleshapeeditingtasksthatcanbeachievedbyslightlyand
G.2.Limitations
graduallyalternatingthesurface. Forexample, intheedit-
This section discusses the limitations of ConsistDreamer, ing of “give him a beard,” our pipeline gradually “grows”
which are also the common challenges encountered by ex- the beard’s shape from the face’s surface. Notice that
isting3Dsceneeditingmethods. bothConsistDreamerandthebaselinescannotperformag-
View-Dependent or Specular Effects. Our Consist- gressive and complicated editing (e.g., removing an object
Dreamer pipeline performs consistency-enforcing training whilereconstructingthewholeoccludedpart),ordirection-
by warping and averaging between different views. This relatedediting(e.g., performing“lowerdownherarm”for
procedure enforces that each part of the scene “looks the a scene of a person raising her arm requires a multi-view
same” in different views, i.e., is view-independent, mak- consensusonwhichdirectionthearmismovedto).
ing the edited scene unlikely to show view-dependent or Efficiency. Incontrasttothediffusiontraining-freebase-
specular effects. To preserve the ability to generate view- lines such as [6, 8, 13], our ConsistDreamer needs addi-
dependent effects, our ConsistDreamer has introduced a tional training of a 2D diffusion model. This extends the
regularization loss that trades off between consistency and editingduration,resultingintaking12hourstoeditascene
similarity to original [2] (detailed in Sec. D.8). With this in the IN2N dataset, and up to 24 hours to edit a large-
regularization, our ConsistDreamer could still achieve 3D scale indoor scene in the ScanNet++ dataset. However, as
consistencywhileallowingnaturalview-dependenteffects. atrade-offagainstefficiency,ourConsistDreamerexcelsin
The baselines, though are not trained towards consistency achieving high-fidelity editing, surpassing all the training-
17freebaselines.
G.3.FutureDirections
SupportingSpecularEffects. Onedirectionistosupport
speculareffectsandbetterview-dependency.Thismayneed
animprovedformulationofconsistencyunderspecularre-
flections,ormodelingtheambientenvironment.
3DUnderstandingforSceneEditing. Anotherdirection
is to enable the diffusion model to understand and reason
the semantics of a scene. Introducing a model that gener-
ates3Dsemanticembeddingsforeachpointinthesceneal-
lowsforcombiningthisinformationwiththe3Dpositional
embedding as the input to the diffusion model, potentially
mitigatingJanusproblems.
18