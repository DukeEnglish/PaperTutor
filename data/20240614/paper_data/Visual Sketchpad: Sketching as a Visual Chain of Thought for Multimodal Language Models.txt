Visual SKETCHPAD: Sketching as a Visual Chain of
Thought for Multimodal Language Models
YushiHu∗1,2 WeijiaShi∗1 XingyuFu3 DanRoth3 MariOstendorf1
LukeZettlemoyer1 NoahA.Smith1,2 RanjayKrishna1,2
1UniversityofWashington 2AllenInstituteforAI 3UniversityofPennsylvania
Abstract
Humansdrawtofacilitatereasoning: wedrawauxiliarylineswhensolvingge-
ometryproblems;wemarkandcirclewhenreasoningonmaps;weusesketches
toamplifyourideasandrelieveourlimited-capacityworkingmemory. However,
suchactionsaremissingincurrentmultimodallanguagemodels(LMs). Current
chain-of-thoughtandtool-useparadigmsonlyusetextasintermediatereasoning
steps. Inthiswork,weintroduceSKETCHPAD,aframeworkthatgivesmultimodal
LMsavisualsketchpadandtoolstodrawonthesketchpad. TheLMconducts
planningandreasoningaccordingtothevisualartifactsithasdrawn.Differentfrom
priorwork,whichusestext-to-imagemodelstoenableLMstodraw,SKETCHPAD
enables LMs to draw with lines, boxes, marks, etc., which is closer to human
sketching and better facilitates reasoning. SKETCHPAD can also use specialist
visionmodelsduringthesketchingprocess(e.g.,drawboundingboxeswithobject
detectionmodels,drawmaskswithsegmentationmodels),tofurtherenhancevisual
perceptionandreasoning. Weexperimentonawiderangeofmathtasks(including
geometry,functions,graph,chess)andcomplexvisualreasoningtasks. SKETCH-
PADsubstantiallyimprovesperformanceonalltasksoverstrongbasemodelswith
nosketching,yieldinganaveragegainof12.7%onmathtasks,and8.6%onvision
tasks. GPT-4owithSKETCHPADsetsanewstateoftheartonalltasks,including
V∗Bench(80.3%),BLINKspatialreasoning(83.9%),andvisualcorrespondence
(80.8%).Allcodesanddataareinhttps://visualsketchpad.github.io/.
1 Introduction
Sketching is a fundamental human activity, serving as a versatile tool for communication [11],
ideation[47],andproblem-solving[46]. Unlikewrittenlanguage,sketcheshavetheadvantageof
conveying visuo-spatial ideas directly, for example by using spatial relations on paper to convey
spatialrelationsorothermoreabstractrelationshipsintheworld. Thismayexplaintheirubiquity;
maps[39]andarchitecturalplans[12]havebeenfoundincisedinstone,etchedonleather,impressed
in clay, and drawn on paper in diverse cultures scattered across the world [38]. Sketches are so
fundamentalthatweusethemtoteachschoolchildrenhowtosolvegeometryproblemsbydrawing
supportlines,toaidengineersconveyingprototypes,tosupportarchitectscreatingblueprints,andto
allowscientistslikeustoconveyscientificcontributions(seeFigure1).
As multimodal language models (LMs) [35, 41, 28, 27, 1, 3, 48, 7, 31, 40, 6, 5] have begun to
mature,wenowexpectthemtosolvetasksliketheonesmentionedabove,i.e.,oneswherepeople
draw intermediate sketches to simplify reasoning. Popular benchmarks now include questions
*EqualContribution.Correspondto<YushiHu:yushihu@uw.edu>,<WeijiaShi:swj0419@uw.edu>
Preprint.Underreview.
4202
nuJ
31
]VC.sc[
1v30490.6042:viXraGeometry Math Function
Prove the angles of Is f(x) an convex
△ABC sum to 180° SketchPad + GPT-4o function? SketchPad + GPT-4o
Draw line DE parallel to AC f(x)=0.14 x −5.09 Plot the function
GPT-4o GPT-4o
Start by assuming, ∠4 is congruent to ∠2, and ∠5 is
f t 1o h 8r a 0 t °c .o t n h Wt e er ’a s ld u li m c it i ni s to rn n o, o d t u ce c f ∠o o 1n r ,g m r ∠u a 2e ,n s t t ∠ r 3t a o i a g d∠ h dt3 . ua pn∠ g1 tl, oe ∠ 1= 84 0, 1 ° 8 .a 0n °d . ∠ S5 o I bt u ti s c oc no cn av ve ex ff oo rr xx <> 00 c to wn ov e px o ib ne tc sa u ls ie e l ai bn oe v eb e tt hw ee e gn r aa pn hy .
a new angle, ∠4 …
(a) (b)
Visual Search Spatial Reasoning
According to the Are there cookies
board, what is the SketchPad + GPT-4o stacked on other SketchPad + GPT-4o
type of the shop? cookies?
Use sliding_window to
find the board Use segmentation & marking
2 7 Cookies 2 and 5 seem
5 to overlap. Need to
analyze their depth
GPT-4o I a s nde e 3 a a nbo d a wrd il lin zp oa ot mc h i n2 GPT-4o Use depth estimation
The zoomed-in image Warmer colors
T rh ee a ds si g “n c ei rn e mt oh ne i ai lm age s “h co ow rs e pt oh we e rt e Yx Ot G A”. So No i pn rd oi xc ia mt ie t yc ,l o ss ue gr g esting
matcha,”. it’s a cafe. it’s a yoga studio they are stacked on
cooler-colored ones.
(c) (d)
Figure1: SKETCHPADequipsGPT-4withtheabilitytogenerateintermediatesketchestoreason
over tasks. Given a visual input and query, such as proving the angles of a triangle equal 180°,
SKETCHPADenablesthemodeltodrawauxiliarylineswhichhelpsolvethegeometryproblem. The
examplesarefrom[8,50,43]. Foralltheseexamples,withoutSKETCHPAD,GPT-4ofailstogetthe
correctanswer,whileSKETCHPAD+GPT-4oachievesthecorrectsolution.
aboutgeometry(e.g.,Geometry3K[32])andcomplexmathproblems(e.g.,IsoBench[8]). Inthese
benchmarks,modelsaregivenimagesofdiagramsandaskedquestionsrequiringsymbolicgrounding
andspatialunderstanding,whereintermediatesketcheslikeauxiliarylinescanenhancereasoning.
Evenbenchmarksincomputervisionnowhaveasimilarflavor. Specialistvisionmodelscanbe
viewedassketchingonnaturalimages. Forexample,objectdetectionisplottingboundingboxes
aroundobjects;depthestimationisdrawingcolormapsaccordingtodepth. Therecentlyproposed
BLINKbenchmark[9]wouldbenefitsignificantlyfromsuchintermediatevisualsketches. Similarly,
theV*Benchbenchmark[50]couldfocusreasoningonimagecropstofindanswers. Unfortunately,
currentLMslackascaffoldforusingsketch-basedreasoningwhensolvingtasks.
Inthispaper,weintroduceVisualSKETCHPAD: aframeworkthatprovidesmultimodalLMs
with the tools necessary to generate intermediate sketches to reason over tasks. Inspired by
textualchain-of-thoughtreasoninginLMs[49,60],SKETCHPADpromptstheunderlyingvisualLM
toproducevisualartifactsaspartofachainofmixedtextual,programmatic,andvisualreasoning.
Forexample,toprovethattheanglesoftrianglessumupto180degreesinFigure1(a),SKETCHPAD
enables agents to modify the diagram by introducing a new auxiliary line. This new line, along
withnewannotatedangles,providesthecriticalinformationtosolvethegeometrytask. Similarly,
SKETCHPAD improves models’ spatial reasoning for computer vision. To determine if there are
cookies stacked on top of other cookies in the image (Figure 1b), the model first produces an
intermediatedepthestimate. Byanalyzingthedepthestimate,whichrevealscookiesoverlappingat
differentdepths,themodelisabletocorrectlyanswerthatthecookiesareindeedstacked.
Wedemonstratetheeffectivenessofvisual SKETCHPAD acrossawiderangeofmathematicsand
computervisiontasks. Formath,wetackleproblemsincluding(1)geometry [32],(2)mathematical
functions,(3)graphalgorithms,and(4)strategygames [8]. Forgeometryquestions,SKETCHPAD
enablesmodelstogenerateMatplotlibcodewithauxiliarylinesandvariables,giventhediagram
input and questions(Figure 1a). Notably, even when theinput is solely language-based, such as
2mathematicalfunctions,SKETCHPADenablesmodelstoplotthefunctionsandreasonabouttheir
properties, using only the mathematical function expression as input (Figure 1b). These results
highlighttheabilityofSKETCHPADtoaidreasoning,evenintaskswithpurelylanguage-basedinputs.
Acrossallfourcategoriesofmathematicaltasks,SKETCHPADconsistentlyimprovesthebaseline
GPT-4operformance,yieldinganaveragegainof12.7%. Forcomputervision,wetacklediverse
tasksincluding(1)depth,(2)spatialreasoning,(3)jigsaw,(4)visualcorrespondence,(5)semantic
correspondence,aswellasquestionsfrom(6)theMMVPand(7)theV*Benchbenchmarks[9,43,50].
Forthisdomain,SKETCHPADenablesmodelstogeneratesegmentationmasks,cropimages,draw
boundingboxes,zoomintoimageregions,overlayimages,etc. Similartomath,SKETCHPADshows
consistentimprovementsacrossallseventypesofcomputervisiontasks. Forexample,GPT-4o,
augmentedwithSKETCHPAD,sees14.3%improvementonV*Bench,12.1%,and9.7%improvements
onBLINK’sdepthandsemanticcorrespondencetasks,settinganewstateoftheartsacrossalltasks.
Finally,weanalyzetheeffectivenessofSKETCHPADbycomparingtheplansgeneratedbyourmodel
withhuman-createdplans,showingthattheyarewell-alignedandexhibitsimilarreasoningpatterns.
WehopeSKETCHPADopensupnewresearchopportunitiestowardmorecapableandinterpretable
multimodalintelligence.
2 RelatedWork
SKETCHPADgeneralizesrecentworkonmultimodaltool-useandvisualprompting. Wealsoplace
ourworkwithinthelargersphereexploringLMsasagents.
Visualprogrammingandtool-use. WiththeadvancementofLMs[4,35,41,44,13],researchers
havedemonstratedthepossiblityofdecomposingcomplexvisiontasksintosimplersubstepsthat
can each be solved using vision tools [56, 58, 18, 17]. Among them, the most relevant to us are
Visprog[14]andViperGPT[37]. TheyuseLMstogeneratePythoncode,whichsequenciallyinvokes
specialized vision tools. These methods share a common problem that the multimodal modules
followapre-definedplanoutlinedbytheLM.Bycontrast,SKETCHPADallowsLMstochangetheir
planaccordingtotheintermediatevisualartifacts,yieldingbetterperformanceandrobustnesswhen
solvingcomplexmultimodaltasks.
Visualprompting. Recentworkshowsthatmultimodalmodelscanbeaugmentedbyvisualprompts
addedtonaturalimages[36]. Forexample,SoM[54]showsthataddinglabeledsegmentationmasks
onimagesunleashesGPT-4V’svisualgroundingability. Priorworkalsoreportssimilarfindingsin
3D[25]andRobotics[34]. SKETCHPADisageneralizedframeworkforallthesemethods,allowing
LMstodecidewhatvisualpromptingtouseaspartofthemultimodalreasoningprocess.
LMsasagents. RecentworkhasstartedtotreatLMsasagentsthatcanbothreasonandact[57,
33, 51]. Researchers have applied this idea to software engineering [20, 59, 15], robotics [34],
vision[29,56],andGUInavigation[53,23,52]. SKETCHPADcanalsobeviewedasanagentthat
acceptsmultimodalinputsandoutputs. Onebigdifferenceisthat SKETCHPAD cancreatevisual
artifactstofacilitatereasoning,whilepriorLMagentsonlygeneratetextduringreasoning.
3 Visual SKETCHPAD
WeintroducevisualSKETCHPAD,ageneralframeworkthatenablesmultimodalLMstodrawsketches
asintermediatereasoningstepsandtousethesesketchestofacilitatefurtherreasoning. Figure2
showsexamplesofhowSKETCHPADworks. Givenamultimodalquery,SKETCHPADagentgenerates
asketchingplantoaddressthequery(Thought), andthensynthesizesaprogramtocreatevisual
sketches (Action). By analyzing the resulting sketches (Observation), which serve as a visual
representationofthereasoningprocess,themodelgeneratesafinalresponsetothequery.
Ourframeworkrequiresnofinetuningortraining. MultimodalLMs,outofthebox,canbeprompted
tosketchusingourframework. WegivetheoverviewofourSKETCHPADframeworkin§3.1,and
delvedeepintohowitintegratessketchingintothereasoningprocessin§3.2.
3.1 OverviewofSKETCHPAD
The SKETCHPAD agentsolvestasksbyengaginginaniterativeinteractionprocesswithanenvi-
ronment. Givenamultimodalqueryqthatincludesbothvisualandtextualcomponents,themodel
3Relative depth Visual Search
A
Which point is closer to Tell me the number of that
the camera, A or B? player who is shooting
B
THOUGHT 1: I can use a depth estimation tool that provides a depth THOUGHT 1: I need to detect the players in the image and zoom in on
map with colors indicating distances. the player who is shooting to see the number clearly
ACTION 1: Call depth estimation ACTION 1: Call object detection
depth_map = depth(image) new_img, boxes = detection(image_1, ["player"])
display(depth_map) display(new_img.annotated_image)
THOUGHT 2: The depth map uses colors from purple (farther) to yellow THOUGHT 2: From the image, Player2 seems to be shooting. I need to
(closer). I need to overlay the original image with the depth map zoom in on this player to see the number on their jersey clearly
ACTION 2: Call overlay A ACTION 2: Call overlay
o dv ie sr pl la ay y (= o vo ev re lr al ya )y_images(depth_map, image) B s z dh o io o so m pt e le d ar y = (= z z ob o oo o mx m ee _ ds i )[ n1 _] i mage_by_bbox(image_1, shooter)
THOUGHT 3: It is evident that point B, in the yellow region, is closer to THOUGHT 3: From the zoomed-in image, I can clearly see that the
the camera compared to point A, which is in the purple region number of the player who is shooting is 8
(a) (b)
Geometry Graph Connectivity
[0, 0, 0, 0, 0, 1, 0, 0, 0],
[0, 0, 1, 0, 0, 0, 1, 0, 0],
∠KG Ei Fve =n
1
0∠ 5B °C
.
FJ= in1 d0 ∠5 E°,
I C
Is
0
t h ane dre na
o
p da et h
4
b ine tt hw ee e gn
ra
n po hd ?e ad mj aa tc re in xcy = [ [ [ [0 0 0 1, , ,
,
1 0 0 0, , ,
,
0 0 1 0, , ,
,
0 0 0 0, , ,
,
1 0 0 0, , ,
,
0 0 0 0, , ,
,
0 0 0 1, , ,
,
0 0 0 0, , ,
,
0 0 0 1] ] ] ], , ,
,
[0, 1, 0, 0, 0, 1, 0, 0, 0],
[0, 0, 0, 0, 0, 0, 0, 0, 0],
[0, 0, 0, 0, 0, 1, 0, 0, 0]
THOUGHT 1: I need to draw an auxiliary Line IX parallel to Line BD THOUGHT 1:We can use the networkx library to plot the graph
ACTION 1: Draw the aux. line using Python Code ACTION 1: Call Python Ploting tool
OBSERVATION 1 G = nx.from_numpy_matrix(np.array(adjacency_matrix),
# x y _ _D I I e = =f in p pe o o i ia n nu t tx s s[ [i “ "li I Ia ” "r ] ][ [y 0 1 p ] ]o int X for the line IX c p nr o xe s .a dt = re a_ n wu x (s . Gi s ,n p g r p= i on n sx g ,. _ D l wi a iG y tr o ha u _p t lh ( a) G b ) e ls=True)
x_X = x_I + 100
y_X = y_I + slope_bd * (x_X - x_I) OBSERVATION 1
# Plot the auxiliary line IX in red color
ax.plot([x_I, x_X], [y_I, y_X], ‘red') …
THOUGHT 2: Given ∠BCJ = ∠KEF = 105°, their supplementary angles,
∠KEH = ∠DCJ = 180° - 105° = 75°. Given BD parallel to IX, it follows
that ∠EIX = ∠KEH, and ∠XIC = ∠DCJ, both measuring 75°. Therefore, THOUGHT 2: From the graph, I saw there is a path between node 0 and
∠EIC = ∠EIX + ∠XIC = 150° node 4.
(a) (b)
Figure2: OverviewofSKETCHPAD. Givenamultimodalquery,theSKETCHPADagentgenerates
asketchingplantoaddressthequery(Thought), andthensynthesizesaprogramtocreatevisual
sketches (Action). By analyzing the resulting sketches (Observation), which serve as a visual
representationofthereasoningprocess,themodelgeneratesafinalresponsetothequery.
generatesaseriesofthoughts,actions,andobservationstogathertheinformationneededtoanswer
thequery. Ateachtimestept,themodelperformsthreekeysteps:
Thought: Themodelanalyzesthecurrentcontextc ,whichincludesthequery,previousthoughts,
t
actions,andobservations,togenerateathoughtplanp forthenextaction. Forexample,giventhe
t
queryq–“findthe∠EIC”inFigure2a,themodel’sthoughtplanp istodrawanauxiliarylineIX
1
paralleltoBDservingasavisualsketchtohelpsolvetheproblem.
Action: Basedonthethoughtplan, themodelexecutesanactiona , whichcanmanipulateboth
t
visual and textual content. In the geometry example, to realize the proposed thought of drawing
theauxiliaryline,themodelgeneratesPythoncodetomodifytheoriginalgeometrydiagram. The
generatedcodeisthencompiledandexecuted.
Observation: Basedontheactiona t,SKETCHPAD’senvironmentreturnsanewobservationo t+1,
such as a new diagram with the auxiliary line drawn in the geometry example. The multimodal
contextisthenupdatedtoc =(c ,p ,a ,o ).
t+1 t t t t+1
Themulti-turninteractionprocesscontinuesuntiltimestepT,whenthemodeldeterminesthatithas
gatheredenoughinformationfromthecontextc toanswerthequery. Atthispoint,itgeneratesa
T
specialTerminateactionandprovidestheanswer.
Differentfrompriorwork[57],whereLMsprimarilygenerateandmanipulatetext-basedobservations
andactions,SKETCHPADenablesthemodeltoworkwithmultimodalobservationso
t
andactions
a ,manipulatingbothvisualandtextualcontent. Thisallowsthemodeltoplanandreasonwith
t
thevisualsketchesithasdrawn,enhancingitsproblem-solvingcapabilities.
3.2 SketchingviaCodeGeneration
ThecorecomponentofSKETCHPADissketching,whichenablestheLMtogeneratevisualsketches
bysynthesizingprogramsthatcalldifferentspecialistvisionmodelsorPythonplottingpackages.
ProgramGeneration. SimilartorecentworkslikeViperGPTandVPD[14,37,18],SKETCHPAD
enables LMs to sketch through code generation. The LM is provided, through a prompt, with
a detailed description of the available tools that can generate multimodal content (an example
promptanddescriptioncanbefoundin§B).ThepromptincludesPythonfunctionsignaturesand
docstrings[16]forthesemodules,butdoesnotcontaintheirfullimplementation. TheLMgenerates
Pythoncodeinacodeblock,usingtheprovidedtools,which,whenexecuted,generatesnewimage
andtextoutputs. AspecialdisplayfunctionallowstheLMtovisualizethesketchimageinthenext
observationo .
t+1
4Modulesforsketching. SKETCHPADusesvarioustoolstofacilitatethesketchingprocess,depending
on the task at hand. For mathematical tasks, SKETCHPAD uses common Python packages like
matplotlib and networkx for plotting (see §4). For vision tasks, the LM leverages specialist
visionmodelsduringthesketchingprocess. Thesemodelsincludedetectiontoolsthatdrawbounding
boxesontheimage,aswellassegmentationandmarkingtools(inspiredbySoM[54])thatdraw
colorfulmasksontheimageandusenumberstolabelsegments. Wefindthesespecialistspossess
usefulperceptionskillsforvisualreasoningtasks,andSKETCHPADisaneffectivewaytocombine
themintoamultimodalLM(see§5.1).
4 SketchingtoSolveMathProblems
Inthissection,weexperimentwithSKETCHPADonfourcomplexmathematicaltasks: (1)geometry,
(2) mathematical functions, (3) graph algorithms, and (4) game strategies. We demonstrate that
incorporating sketching capabilities into LMs significantly improves their performance on these
mathematicalproblems,settingnewstate-of-the-artresults(§4.1).
Detailsofourevaluationtasksandthetoolsemployedforvisualreasoningareasfollows:
GeometryProblems. Drawingauxiliarylinesingeometrydiagramsisoftenhelpfulforproblem-
solving. Forexample,inFigure2(a),whenaskedtofind∠EIC,theLMplanstodrawanauxiliary
lineIX paralleltoBD,allowingittousethepropertiesofparallellinestodetermine∠EIC. To
evaluatetheeffectivenessofSKETCHPAD,weusetheproblemsfromtheGeometry3Kdataset[32].
Torealizethelinedrawingprocess,SKETCHPADtakesageometrydiagramanditscorresponding
matplotlibcodeasinput. Themodelthenproposesandmodifiesthecodetogenerateauxiliary
lines,andexecutesthemodifiedcodetovisualizetheupdateddiagramwiththeaddedlines.
Mathematicalfunctions. Understandingmathematicalfunctionsiscrucialforvariousapplications
inscience,engineering,andeconomics. Wefocusontwotasksrelatedtomathematicalfunctions
fromtheIsoBenchdatasets[8]:
• Classifying parity aims to determine whether a function is even, odd, or neither. Even
functionssatisfyf(−x)=f(x)forallx,whileoddfunctionssatisfyf(−x)=−f(x).
• Identifyingconvexity/concavityaimstodeterminewhetherafunctionisconvexorconcave.
Existing LMs can only analyze functions and attempt to prove their properties analytically. 1
However,SKETCHPADenablesthemtovisuallysketchfunctionstosolveproblemsmoreefficiently.
Forinstance,todeterminetheconvexityofthefunctioninFigure1b,SKETCHPADallowsthemodel
toplotthefunctionusingmatplotlib,andvisuallyinspectitsoverallshape.
Graphalgorithms. Manyreal-worldproblems, suchasthoserelatedtocomputernetworksand
transportationsystems,canbeformulatedasgraphproblems. Weevaluate SKETCHPAD onthree
graphproblemsfromIsoBench[8]:
• Graphconnectivitydetermineswhetherthereexistsapathbetweentwoverticesinagraph.
• Maximumflowaimstofindthemaximumamountofflowthatcanbesentthroughanetwork
fromasourcevertextoasinkvertex,subjecttocapacityconstraintsontheedges.
• Graphisomorphismtestswhethertwographsarestructurallyequivalent.
GivenanadjacencymatrixofagraphlikeinFigure2(b),SKETCHPADcandrawtheactualgraph
structure, using using Python’s networkx library, enabling direct visual reasoning about graph
propertiesandrelationships.
Gamestrategies. Chessgamescanberepresentedinvariousformats,includingvisualboardstates
andtextualmovenotations. Givenonlythetextualmovenotations,SKETCHPADcandrawthevisual
representationsofthechessboardtoanalyzepositionsandformulatestrategies. Weevaluatethe
performanceofSKETCHPADonthewinnderidentificationtaskfromtheIsoBenchdatasets[8]that
aimstofindtheoutcomeofachessgame(winforWhite,winforBlack,ordraw)basedonthefinal
1Forhumans,theanalyticalapproachisthecorrectwaytotacklethesemathtasks.However,weobservethat
LMsarenotgoodatanalyticalreasoninginmath.Theymakeerrorswhendeducingf(−x)andderivatives.
5Geometry Graph Math Game
Model Geometry Maxflow Isomorphism Connectivity Convexity Parity WinnerID
PriorLLMswithoutvisualinputs
Gemini-Pro \ 15.6 47.7 50.0 87.9 48.2 8.1
Claude3OPUS \ 56.3 50.0 82.0 93.0 77.6 74.4
Mixtral8x7B[19] \ 8.6 50.0 62.5 69.1 41.7 7.4
LLaMA-2-70B[45] \ 18.0 50.0 50.0 74.2 33.3 12.4
LatestmultimodalLLMs+VisualSketchpad
GPT-4Turbo 37.5 32.8 62.5 66.0 57.0 80.5 50.4
+Sketchpad 45.8 63.3 64.2 95.1 93.1 93.1 74.3
+8.3 +30.5 +1.7 +29.1 +25.4 +12.6 +23.9
GPT-4o 62.5 25.0 50.8 96.1 87.2 84.4 61.1
+Sketchpad 66.7 66.3 65.3 98.1 90.1 88.1 81.2
+4.2 +41.3 +14.5 +2.0 +2.9 +3.7 +20.1
Table1: Accuracyscoresongeometryproblems, graphalgorithms, mathematicalfunctions, and
game. SKETCHPADyieldslargeperformancegainsacrossalltasksandoutperformallbaselines.
boardstate. Tocreatethegraphicalboard, SKETCHPAD usesPython’schesslibrarytodrawthe
boardfromtheForsyth-EdwardsNotation(FEN)ofchess.
4.1 Results
We evaluate the performance of SKETCHPAD on multimodal LMs with API access, including
gpt-4-turbo-2024-04-29 and gpt-4o-2024-05-13. We compare these results to baselines
withouttheVisualSketchpadandothernotableclosed-sourcemodels,suchasClaude3andGemini-
Pro,aswellasopen-sourcemodelslikeMistral[19]andLLaMA-270B[45].
Mainresults. AsshowninTable1,SKETCHPADconsistentlyimprovesbasemodelperformance
acrossalltasks,withanaverageimprovementof18.8%forGPT-4oand13.5%forGPT-4 Turbo.
Inparticular,weobservelargegainsongraphalgorithmssuchasmaximumflowandconnectivity.
For instance, GPT-4o with SKETCHPAD achieves an accuracy of 66.3% on the maximum flow
problem,improvingoverthebasemodelby41.3%. Similarly,SKETCHPADsubstantiallyimproves
theperformanceonmathematicalfunctions,withGPT-4 Turboachievingover90%accuracyand
GPT-4oover88%accuracyonconvexityandparityclassificationtasks. Furthermore,weobserve
notablegains(∼20%)ongamestrategies,demonstratingthatdrawngameboardsdrawncanimprove
reasoningaboutgamestrategies. Overall,theseresultshighlighttheeffectivenessofSKETCHPADin
enhancingthereasoningcapabilitiesofmultimodallanguagemodelsacrossdiversedomains.
5 SketchingtoSolveComputerVisionTasks
Inthissection,weexperimentwithSKETCHPADoncomplexvisualreasoningtasks. Recentwork
(BLINK)[9]findsthatmanycorevisualperceptionabilitiesarestillmissingfromexistingmultimodal
LMs—eventhoughmanycomputervisionspecialistmodelspossesssuchabilities. Also,SoM[54]
showsthatdrawingsegmentationmasksonimagesunleashesthestrongvisualgroundingabilityof
GPT-4V.WegeneralizetheseideaswithSKETCHPAD,allowingLMstousespecialistvisionmodels
to sketch. Details of these modules are in §5.1. SKETCHPAD enhances multimodal LMs’ visual
reasoningabilitiesandestablishesnewSOTAsonall7tasks(§5.2).
Tasks. We experiment with a wide range of complex visual reasoning tasks: (1) V∗Bench [50].
Thisbenchmarkcontainsquestionsaboutsmallitemsinanimage. (2)MMVPbenchmarkfrom
Eyes Wide Shut [43]. This benchmark contains visual questions specially designed to reveal the
visualshortcomingsofCLIP-basedmultimodalLMs. (3)BLINK[9]. Thisbenchmarkcontains
visual perception tasks that are easy for humans, but post significant challenge for multimodal
LMs. Specifically, we experiment with relative depth, spatial reasoning, jigsaw puzzle, visual
correspondence,andsemanticcorrespondencetasks. Moredetailsofeachtaskarein§C.
5.1 VisionSpecialistsasSketchingToolsinSKETCHPAD
LMscanusethefollowingmodulestosketchandmanipulateimages. Wewrapthesemodulesinto
PythonfunctionsthattheLMscancall. Referto§Bforthefunctiondefinitions.
6Relative depth Visual Search
A
Which point is closer to Tell me the number of that
the camera, A or B? player who is shooting
B
THOUGHT 1: I can use a depth estimation tool that provides a depth THOUGHT 1: I need to detect the players in the image and zoom in on
map with colors indicating distances. the player who is shooting to see the number clearly
ACTION 1: Call depth estimation ACTION 1: Call object detection
depth_map = depth(image) new_img, boxes = detection(image_1, ["player"])
display(depth_map) display(new_img.annotated_image)
THOUGHT 2: The depth map uses colors from purple (farther) to yellow THOUGHT 2: From the image, Player2 seems to be shooting. I need to
(closer). I need to overlay the original image with the depth map zoom in on this player to see the number on their jersey clearly
ACTION 2: Call overlay A ACTION 2: Call zoom-in
o dv ie sr pl la ay y (= o vo ev re lr al ya )y_images(depth_map, image) B s z dh o io o so m pt e le d ar y = (= z z ob o oo o mx m ee _ ds i )[ n1 _] i mage_by_bbox(image_1, shooter)
THOUGHT 3: It is evident that point B, in the yellow region, is closer to THOUGHT 3: From the zoomed-in image, I can clearly see that the
the camera compared to point A, which is in the purple region number of the player who is shooting is 8
(a) (b)
Figure3:ExamplesofSKETCHPADappliedtovisiontasks.Thefigureshowsactualoutputsgenerated
Geometry Graph Connectivity
bySKETCHPAD. Bycontrast,thebaselineGPT-4omodelcannotanswerthesequestionscorrectly.
[0, 0, 0, 0, 0, 1, 0, 0, 0],
N imo at ge et sha i∠nKt G Eif tFv he =o n 1er 0∠ 5B ed °C .xe FJ pm = in1 ed0o r∠5 in E° m,
I
s Ct er na tt sio .npurposes, the“A”and“B” Im s
0
t h aa ne drr ek na os p da ei t n h
4
b in( e a tt hw) ee e ga n rr
a
ne po hd ?d e i af df mj ae a tc rr e ie n xcn y t = [ [ [ [ [f0 0 0 0 1, , , , ,r o0 1 0 0 0, , , ,
,
m1 0 0 1 0, , , ,
,
0 0 0 0 0, , , , ,t 0 1 0 0 0h, , , ,
,
e0 0 0 0 0, , , ,
,
1 0 0 0 1a, , , ,
,
c0 0 0 0 0, , , , ,t 0 0 0 0 1u] ] ] ] ], , , , ,a l
[0, 1, 0, 0, 0, 1, 0, 0, 0],
[0, 0, 0, 0, 0, 0, 0, 0, 0],
[0, 0, 0, 0, 0, 1, 0, 0, 0]
D Te Ht Oe Uc Gt Hi To 1n : I. neTedh tios dmrawo adn uaulxeiliatray kLinees IXa pnarailmlel atog Leinea BnDdasimplTeHOteUGxHtTq 1u:Weer cyan( uese.g th.e, n“ectwaotr”kx) libarsaryi tno ppluott .theW greaphrunthe
Grounding-DINO[30]open-vocabularyobjectiondetectionmodelandplotthedetectedbounding
boAC xT eIO sN ( 1 to: g e tD hra ew r th we ia tu hx. l aine n u usi mng bPy eth ron la C bod ee l)ontheimage. ItACaTlIsOoN r1e: t u r nCasll tPhytehonb Poloutinngd tionolgboxcoordinates.
OBSERVATION 1 G = nx.from_numpy_matrix(np.array(adjacency_matrix),
Se# x yg _ _D I Ime f = =i n p pee o o i ina n nu t ttx s si [ [al “ "i I Ita ” "ir ] ]oy [ [ 0 1np ] ]o i .nt T X hfo ir s th me l oin de uIX l etakesanimageasinputanc p ndr o xe s .a drt = ree a_ n wtu x (us . Gi s ,rn p g r pn= i on n ssx g ,. _ aD l wi a inG y tr o ha u _ip t lh ( am) G b ) e la s=g Tre ue)withcolorfulsegmentation
mx ya_ _X Xs k=
=
sx y_ _I Io + +n 1 s0 l0 oi pt e. _bdE *a (c x_h
X
-m x_Ia
)
skalsohasanumberlabel. WeOfBoSElRloVAwTIOtNh e1 implementationofSoM[54]. The
# Plot the auxiliary line IX in red color
unadx.pelortl(y[xi_nI, gx_Xs],e g[y_mI, ey_nX]t,a ‘trieod')n …modelsareSegmentAnything[22]andSemantic-SAM[24].
Depthestimation. Thismoduletakesanimageasinputandreturnsadepthmap. Theunderlying
THOUGHT 2: Given ∠BCJ = ∠KEF = 105°, their supplementary angles,
modelisDepthAnything[55].
∠KEH = ∠DCJ = 180° - 105° = 75°. Given BD parallel to IX, it follows
Vt ih sa ut ∠ aE lIX s= e ∠ aK rEH c, h an vd i∠ aXIC s = li ∠ dD iC nJ g, bo wth i m ne das our wing . 7 T5° h. T ih ser mefo ore d, ulemimTHiOcUsGhHoT w2: Fhroumm thea gnrasphs,e I asarwc thherfeo isr as pmatha bleltwiteeenm nosdeo 0n anadn
∠EIC = ∠EIX + ∠XIC = 150° node 4.
image. Ittakesatextqueryasinputandrunsaslidingwindowovertheimage. Thewindowsizeis
(a) (b)
1/3oftheimagesize,andthestepsizeis2/9oftheimagesize(soanimagewillhave4×4=16
windows). Itreturnsasequenceofimagepatchesinwhichthequeryisdetected.
Otherimagemanipulationmodules. Othermodulesinclude(1)zoom-inandcrop,whichtakesan
imageandaboundingboxasinputandreturnstheimagepatchinsidethebox;(2)Overlayimages,
whichtakestwoimagesandalphavaluesasinput,andreturnstheoverlayedimage.
5.2 Results
We experiment with the same multimodal LMs as in §4 on complex visual reasoning tasks. We
comparetheperformancewithandwithoutSKETCHPAD,aswellasothernotablemultimodalLMs,
includingGemini[41],Claude3[2],andtheopen-sourceLLaVA1.5[26],LLaVA-NeXT[27].
Main results. Table 2 shows the performance of our SKETCHPAD and baselines. SKETCHPAD
consistentlyimprovesbasemodelperformanceacrossalltasks. GPT-4owithSKETCHPADsetsthe
newstate-of-the-artresultsonalltasks. SKETCHPADisparticularlyeffectiveonV∗Bench,yielding
18.5%accuracyimprovementforGPT-4 Turboand14.3%improvementforGPT-4o,surpassing
thepreviousstateoftheartSEAL[50]whichusedavisualsearchmodelspecificallytrainedfor
thistask. OnBLINKtasks,SKETCHPADonaverageyields6.6%absoluteaccuracygainforGPT-4
Turboand9.0%gainforGPT-4o. Interestingly,despitethefactthatallmodulesin SKETCHPAD
workonasingleimage,theLMsalsogetsubstantialimprovementonmulti-imagetasks,including
jigsawpuzzles,visualcorrespondence,andsemanticcorrespondence. Finally,GPT-4o,theLMwith
strongermultimodalabilitythanGPT-4 Turbo,benefitsmorefromSKETCHPAD. Forexample,on
therelativedepthtask,GPT-4ogets12.1%accuracyimprovement,whileGPT-4 Turboonlygets
2.4%,showingthatGPT-4oisbetteratunderstandingthedepthmapSKETCHPADgenerated. Overall,
7Model V∗Bench MMVP Depth Spatial Jigsaw Vis.Corr. Sem.Corr.
PriormultimodalLLMs
LLaVA-1.5-7B[26] 48.7 - 52.4 61.5 11.3 25.6 23.0
LLaVA-1.5-13B[26] - 24.7 53.2 67.8 58.0 29.1 32.4
LLaVA-NeXT-34B[27] - - 67.7 74.8 54.7 30.8 23.7
Claude3OPUS[2] - - 47.6 58.0 32.7 36.6 25.2
Gemini-Pro[41] 48.2 40.7 40.3 74.8 57.3 42.4 26.6
GPT-4V-preview[35] 55.0 38.7 59.7 72.7 70.0 33.7 28.8
Previousstateoftheart 75.4[50] 49.3[10] 67.7[27] 76.2[42] 70.0[35] 42.4[41] 33.1[48]
LatestmultimodalLLMs+VisualSketchpad
GPT-4Turbo 52.5 71.0 66.1 68.5 64.7 48.8 30.9
+Sketchpad 71.0 73.3 68.5 80.4 68.5 52.3 42.4
+18.5 +2.3 +2.4 +11.9 +3.8 +3.5 +11.5
GPT-4o 66.0 85.3 71.8 72.0 64.0 73.3 48.6
+Sketchpad 80.3 86.3 83.9 81.1 70.7 80.8 58.3
+14.3 +1.0 +12.1 +9.1 +6.7 +7.5 +9.7
Table2: Accuracyoncomplexvisualreasoningtasks. SKETCHPADenhancesbothGPT-4Turbo
andGPT-4operformance,establishingnewSOTAperformancelevelsonallthetasks.
ourexperimentsshowthatSKETCHPADisaneffectivewaytoimprovemultimodalLMs’performance
onvisualreasoningtasks.
How many times is each vision specialist used? We count the number of times each vision
specialistisusedineachtask, asshowninFigure4. Herewechoosethefourtasksthatachieve
thelargestimprovement: V∗Bench,relativedepth,spatialreasoning,andsemanticcorrespondence.
Weobservethat(1)theuseofvisionspecialististask-dependent,andthetwoLMsanalyzed
utilizesimilartools. Forexample,forV∗,whichneedstolocatesmallobjects,theLMsmainlyuse
detection,slidingwindowsearch,andzoom-in,similartohowpeoplewouldsearch. Fortherelative
depth task, both models rely on depth estimation. For spatial reasoning, the LMs use detection
andsegmentationtofacilitatevisualreasoning. (2)GPT-4olikestousemoretools. GPT-4ouses
the vision specialists more often than GPT-4 Turbo. Also, the two LMs behave differently for
the semantic correspondence tasks. GPT-4o uses the segmentation module for 40% of the task
instances,whileGPT-4 Turbousesthedetectionmoduleforlessthan20%oftimes,andrarelyuses
thesegmentationmodule. ThisdifferencemayexplaintheperformancegapbetweenthetwoLMs
(58.3%v.s. 42.4%)onthistask.
Model V∗ MMVP Depth Spatial
GPT-4Turbo 52.5 71.0 66.1 68.5
SoM 42.0 60.7 58.9 78.3
SoM+orig. 51.3 74.3 66.9 79.7
Visprog 33.2 16.3 67.8 53.8
Sketchpad 71.0 73.3 68.5 80.4
GPT-4o 66.0 85.3 71.8 72.0
SoM 49.0 70.7 62.9 83.2
SoM+orig. 68.1 84.0 75.0 82.5
Visprog 32.4 17.3 46.8 37.8
Sketchpad 80.3 86.3 83.9 81.1
Table 3: Comparison with other augmenta-
tionframeworksformultimodalLMsonsingle-
Figure4: PercentageoftimesGPT-4oandGPT-
imagetasks. Forfaircomparison, wemodify
4 Turbo use a visual module in SKETCHPAD
when solving V∗Bench, relative depth, spatial theoriginalVisprog[14]frameworkbyreplac-
ingtheLMandVQAcomponentswiththecor-
reasoning,andsemanticcorrespondencetasks.
respondingGPT-4model.
Comparisonwithvisualpromptingandtool-useframeworks. InTable3,wecompareSKETCH-
PADwiththevisualpromptingframeworkSoM[54]andtheLLMtool-useframeworkVisprog[14].
Detailsofthesemethodscanbefoundin§2. Forafaircomparison,wemakethefollowingadap-
tations: (1) we find that prompting LMs with SoM images can hurt performance, likely because
8thevisualpromptsconfusethemodel. Tomakeastrongerbaseline,weprompttheLMwithboth
theoriginalimageandtheSoMimage(fullpromptin§B),whichwereferas“SoM+orig.” (2)
WereplacetheLMandVQAmodulesinVisprogwiththecorrespondingGPT-4model. (3)Since
baselinemethodsaredevelopedonsingle-imagetasks,wecompareSKETCHPADonsuchtasks. From
Table3,wecanseethatSKETCHPADistheonlyframeworkthatyieldsconsistentimprovement
onalltasks. SoMcanboostspatialreasoningability,astheauthorsreported. However,itcanhurtthe
performanceonothertasks,eveninthe“SoM+orig.” setting. Visprogperformsworsethanthebase
LMonallthetasks. Aspriorwork[21,18]suggests,onepossiblereasonisthatthevisionmodules
themselveshaveerrors,andtheerrorpropagateswhenthemodulesarecomposedbyaprogram.
6 AnalysisandDiscussion
Why does SKETCHPAD work? First, vision is a versatile and informational interface that
complementslanguage. Denseinformationlikedepthandsegmentationcannotbedescribedeasily
throughlanguage[9].Inabroaderperspective,humanshavedevelopedmanyvisualizationtechniques
thataredirect,efficient,andinformational. SKETCHPADprovidesLMstheopportunitytousethem.
Second,inSKETCHPAD,multimodalLMscanplanandreasonbasedontheintermediatevisual
artifactstheycreated. Incontrast,inpriormodularvisionwork[14,37,54],multimodalmodules
followapredefinedplanbyeitherhumansorcode. SKETCHPADismuchmoreflexibleandrobust
toerrors. Forexample, supposeobjectdetectionmakesanerror. TheLMcan(inprinciple)find
theerrorbyviewingtheboundingboxes,andchangeitsfollowingplans,butpriormethodscannot.
Third,asdiscussednext,theplansofmultimodalLMsaresimilartohumanplans,andtherefore
likelybenefitfromthefactthattheunderlyingLMshaveseendatawithsimilarreasoningpatterns
duringpretraining.
DoLMshavethesameplansashumans? Weconductahumanstudyonallgeometryproblems
and10problemsoneachvisiontask. Ongeometry,humansdrawthesameauxiliarylineasGPT-4o
80%ofthetime. Onvision,weshow2humansubjectsthefullplanofGPT-4o,whichtheyrateis
validin92.8%ofinstances. Mosterrorsarecausedbyfailuresinthevisionspecialists(e.g.,failto
detectanobject)andmistakesinsimplevisualquestionanswering,ratherthanplanning.
Model Geometry Maxflow Convexity WinnerID
LLaVA-NeXT-13B 11.1 7.8 50.39 5.8
+oracleSketchpad 22.2 10.2 50.0 36.7
LLaVA-NeXT-34B 26.1 0.8 81.6 49.0
+oracleSketchpad 28.3 14.1 87.1 49.4
Table4: Open-sourceLLaVAmodels’performanceonmathtasks. TheoracleSketchpadusesthe
visualartifactgeneratedinthelastactionofGPT-4o+SKETCHPADasinputs.
Experiments on open-source models. Can sketches like diagrams, plots, and auxiliary lines
facilitateexistingopen-sourcemultimodalLMs?Toanswerthisquestion,weconducttheexperiments
in Table 4. We use the visual artifacts generated in the last action of GPT-4o + SKETCHPAD
experimentastheimageinputforopen-sourceLLaVA-NEXTmodels[27]. Wecanseethatthis
oracleSKETCHPADbringsconsistentimprovementtomathtasksandboostsmathematicalreasoning.
7 Conclusion
WepresentVisualSKETCHPAD,aframeworkthatprovidesmultimodalLMswiththetoolsnecessary
togenerateintermediatesketchestoreasonovertasks. Forcomplexmathematicalreasoningtasks,
SKETCHPADyieldslargeperformancegains,byvisualizingauxiliarylines,mathfunctions,graphs,
andgamesduringreasoning. Forvisualreasoningtasks,weaddvisionspecialiststoSKETCHPAD.
TheLMcancallthesespecialistsduringreasoning,observingthevisualizationofthesespecialists’
predictions(e.g.,boundingboxesfromtheobjectdetectionmodel;masksfromthesegmentation
model), and then conduct further planning and reasoning. Experiments show that SKETCHPAD
enhancestheLMs’performanceacrossalltasks,andsetsnewstate-of-the-artresults. Ultimately,
SKETCHPADrepresentsasteptowardendowingLMswithmorehuman-likemultimodalintelligence,
leveraging the complementary strengths of language and vision to tackle increasingly complex
reasoningchallenges.
9Limitationsandfuturedirections. First, SKETCHPAD requiresmorecomputingresourcesthan
directlyoutputtinglanguagetokens. WediscussmoreaboutcomputingcostsinD.Second,thiswork
focusesonexistingoff-the-shelfLMs. FutureworkmayexplorethetrainingsideofSKETCHPAD.
For example, recent multimodal models like Unified-IO 2 [31] and Chameleon [40] are natively
multimodalandcanoutputbothtextandimages. SKETCHPADmayemergeasanewparadigmfor
instructiontuningthesemodels. Finally,SKETCHPADcanbeappliedinmoreareas. Forexample,
forrobotics,wecanapplySKETCHPADtosearchforsmallthingsinacrowdedspace,highlightthe
objectofinterest,andzoomthecameraforabettervieworusedepthestimationtohelpnavigation.
8 Acknowledgement
ThisworkisinpartsupportedbygiftsfromAllenInstituteforArtificialIntelligence,Meta,Apple,
andAmazonScience. WealsogreatlyappreciatethehelpfromZixianMa,Wei-ChiuMa,BenlinLiu,
JieyuZhang,UWNLP,AllenNLP,AI2PRIOR,andUWRAIVNfortheirhelp.
10References
[1] Jean-BaptisteAlayrac, JeffDonahue, PaulineLuc, AntoineMiech, IainBarr, YanaHasson,
KarelLenc,ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisual
languagemodelforfew-shotlearning. AdvancesinNeuralInformationProcessingSystems,
35:23716–23736,2022.
[2] Anthropic. Introducingthenextgenerationofclaude. https://www.anthropic.com/news/
claude-3-family,March2024.
[3] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,Chang
Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding,
localization,textreading,andbeyond,2023.
[4] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. Advancesinneuralinformationprocessingsystems,33:1877–1901,2020.
[5] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu,
Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa
Dehghani,DanielSalz,MarioLucic,MichaelTschannen,ArshaNagrani,HexiangHu,Mandar
Joshi,BoPang,CesleeMontgomery,PaulinaPietrzyk,MarvinRitter,AJPiergiovanni,Matthias
Minderer,FilipPavetic,AustinWaters,GangLi,IbrahimAlabdulmohsin,LucasBeyer,Julien
Amelot,KentonLee,AndreasPeterSteiner,YangLi,DanielKeysers,AnuragArnab,Yuanzhong
Xu,KeranRong,AlexanderKolesnikov,MojtabaSeyedhosseini,AneliaAngelova,Xiaohua
Zhai,NeilHoulsby,andRaduSoricut. Pali-x: Onscalingupamultilingualvisionandlanguage
model,2023.
[6] ZheChen,JiannanWu,WenhaiWang,WeijieSu,GuoChen,SenXing,ZhongMuyan,Qinglong
Zhang,XizhouZhu,LeweiLu,etal. Internvl:Scalingupvisionfoundationmodelsandaligning
forgenericvisual-linguistictasks. arXivpreprintarXiv:2312.14238,2023.
[7] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng
Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose
vision-languagemodelswithinstructiontuning,2023.
[8] DeqingFu∗,GhazalKhalighinejad∗,OllieLiu∗,BhuwanDhingra,DaniYogatama,RobinJia,
andWillieNeiswanger. IsoBench:Benchmarkingmultimodalfoundationmodelsonisomorphic
representations,2024.
[9] XingyuFu,YushiHu,BangzhengLi,YuFeng,HaoyuWang,XudongLin,DanRoth,NoahA
Smith,Wei-ChiuMa,andRanjayKrishna. Blink: Multimodallargelanguagemodelscansee
butnotperceive. arXivpreprintarXiv:2404.12390,2024.
[10] PengGao,RenruiZhang,ChrisLiu,LongtianQiu,SiyuanHuang,WeifengLin,ShitianZhao,
ShijieGeng,ZiyiLin,PengJin,etal. Sphinx-x: Scalingdataandparametersforafamilyof
multi-modallargelanguagemodels. arXivpreprintarXiv:2402.05935,2024.
[11] VinodGoel. Sketchesofthought. MITpress,1995.
[12] GabrielaGoldschmidt. Thedialecticsofsketching. Creativityresearchjournal,4(2):123–143,
1991.
[13] DirkGroeneveld,IzBeltagy,PeteWalsh,AkshitaBhagia,RodneyKinney,OyvindTafjord,
AnanyaHarshJha,HamishIvison,IanMagnusson,YizhongWang,etal. Olmo: Accelerating
thescienceoflanguagemodels. arXivpreprintarXiv:2402.00838,2024.
[14] TanmayGuptaandAniruddhaKembhavi.Visualprogramming:Compositionalvisualreasoning
withouttraining. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages14953–14962,2023.
[15] SiruiHong,MingchenZhuge,JonathanChen,XiawuZheng,YuhengCheng,CeyaoZhang,
JinlinWang,ZiliWang,StevenKaShingYau,ZijuanLin,LiyangZhou,ChenyuRan,Lingfeng
Xiao,ChenglinWu,andJürgenSchmidhuber. Metagpt: Metaprogrammingforamulti-agent
collaborativeframework,2023.
11[16] Cheng-YuHsieh,Si-AnChen,Chun-LiangLi,YasuhisaFujii,AlexanderRatner,Chen-YuLee,
RanjayKrishna,andTomasPfister. Tooldocumentationenableszero-shottool-usagewithlarge
languagemodels. arXivpreprintarXiv:2308.00675,2023.
[17] YushiHu,HangHua,ZhengyuanYang,WeijiaShi,NoahASmith,andJieboLuo. Promptcap:
Prompt-guidedtask-awareimagecaptioning. arXivpreprintarXiv:2211.09699,2022.
[18] Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji Hata, Enming
Luo, Ranjay Krishna, and Ariel Fuxman. Visual program distillation: Distilling tools and
programmaticreasoningintovision-languagemodels. arXivpreprintarXiv:2312.03052,2023.
[19] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,Lucile
Saulnier,LélioRenardLavaud,Marie-AnneLachaux,PierreStock,TevenLeScao,Thibaut
Lavril,ThomasWang,TimothéeLacroix,andWilliamElSayed. Mistral7b,2023.
[20] CarlosEJimenez,JohnYang,AlexanderWettig,ShunyuYao,KexinPei,OfirPress,andKarthik
Narasimhan.Swe-bench:Canlanguagemodelsresolvereal-worldgithubissues? arXivpreprint
arXiv:2310.06770,2023.
[21] ApoorvKhandelwal,ElliePavlick,andChenSun. Analyzingmodularapproachesforvisual
questiondecomposition. arXivpreprintarXiv:2311.06411,2023.
[22] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,
TeteXiao,SpencerWhitehead,AlexanderCBerg,Wan-YenLo,etal. Segmentanything. In
ProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages4015–4026,
2023.
[23] JingYuKoh,RobertLo,LawrenceJang,VikramDuvvur,MingChongLim,Po-YuHuang,
Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena:
Evaluatingmultimodalagentsonrealisticvisualwebtasks. arXivpreprintarXiv:2401.13649,
2024.
[24] FengLi,HaoZhang,PeizeSun,XueyanZou,ShilongLiu,JianweiYang,ChunyuanLi,Lei
Zhang,andJianfengGao. Semantic-sam: Segmentandrecognizeanythingatanygranularity.
arXivpreprintarXiv:2307.04767,2023.
[25] DingningLiu,XiaomengDong,RenruiZhang,XuLuo,PengGao,XiaoshuiHuang,Yongshun
Gong,andZhihuiWang. 3daxiesprompts: Unleashingthe3dspatialtaskcapabilitiesofgpt-4v.
arXivpreprintarXiv:2312.09738,2023.
[26] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual
instructiontuning,2023.
[27] HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,ShengShen,andYongJaeLee.
Llava-next: Improvedreasoning,ocr,andworldknowledge,January2024.
[28] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning,2023.
[29] ShilongLiu,HaoCheng,HaotianLiu,HaoZhang,FengLi,TianheRen,XueyanZou,Jianwei
Yang,HangSu,JunZhu,etal. Llava-plus: Learningtousetoolsforcreatingmultimodalagents.
arXivpreprintarXiv:2311.05437,2023.
[30] ShilongLiu,ZhaoyangZeng,TianheRen,FengLi,HaoZhang,JieYang,ChunyuanLi,Jianwei
Yang,HangSu,JunZhu,etal. Groundingdino: Marryingdinowithgroundedpre-trainingfor
open-setobjectdetection. arXivpreprintarXiv:2303.05499,2023.
[31] JiasenLu,ChristopherClark,SanghoLee,ZichenZhang,SavyaKhosla,RyanMarten,Derek
Hoiem,andAniruddhaKembhavi. Unified-io2: Scalingautoregressivemultimodalmodels
withvision,language,audio,andaction. arXivpreprintarXiv:2312.17172,2023.
[32] PanLu,RanGong,ShibiaoJiang,LiangQiu,SiyuanHuang,XiaodanLiang,andSong-Chun
Zhu. Inter-gps: Interpretablegeometryproblemsolvingwithformallanguageandsymbolic
reasoning. InThe59thAnnualMeetingoftheAssociationforComputationalLinguistics(ACL),
2021.
12[33] Zixian Ma, Weikai Huang, Jieyu Zhang, Tanmay Gupta, and Ranjay Krishna. m&m’s: A
benchmarktoevaluatetool-useformulti-stepmulti-modaltasks.InSyntheticDataforComputer
VisionWorkshop@CVPR2024,2024.
[34] SoroushNasiriany,FeiXia,WenhaoYu,TedXiao,JackyLiang,IshitaDasgupta,AnnieXie,
DannyDriess,AyzaanWahid,ZhuoXu,QuanVuong,TingnanZhang,Tsang-WeiEdwardLee,
Kuang-HueiLee,PengXu,SeanKirmani,YukeZhu,AndyZeng,KarolHausman,Nicolas
Heess,ChelseaFinn,SergeyLevine,andBrianIchter. Pivot: Iterativevisualpromptingelicits
actionableknowledgeforvlms. 2024.
[35] OpenAI. Gpt-4technicalreport,2023.
[36] AleksandarShtedritski,ChristianRupprecht,andAndreaVedaldi. Whatdoesclipknowabouta
redcircle? visualpromptengineeringforvlms. InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,pages11987–11997,2023.
[37] DídacSurís,SachitMenon,andCarlVondrick. Vipergpt: Visualinferenceviapythonexecution
for reasoning. Proceedings of IEEE International Conference on Computer Vision (ICCV),
2023.
[38] HollyATaylorandBarbaraTversky. Descriptionsanddepictionsofenvironments. Memory&
cognition,20:483–496,1992.
[39] HollyATaylorandBarbaraTversky. Spatialmentalmodelsderivedfromsurveyandroute
descriptions. JournalofMemoryandlanguage,31(2):261–292,1992.
[40] ChameleonTeam. Chameleon: Mixed-modalearly-fusionfoundationmodels. 2024.
[41] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,
RaduSoricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighly
capablemultimodalmodels. arXivpreprintarXiv:2312.11805,2023.
[42] InternLM Team. Internlm: A multilingual language model with progressively enhanced
capabilities. https://github.com/InternLM/InternLM,2023.
[43] ShengbangTong,ZhuangLiu,YuexiangZhai,YiMa,YannLeCun,andSainingXie. Eyeswide
shut? exploringthevisualshortcomingsofmultimodalllms. arXivpreprintarXiv:2401.06209,
2024.
[44] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas
Blecher,CristianCantonFerrer,MoyaChen,GuillemCucurull,DavidEsiobu,JudeFernandes,
JeremyFu,WenyinFu,BrianFuller,CynthiaGao,VedanujGoswami,NamanGoyal,Anthony
Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian
Khabsa,IsabelKloumann,ArtemKorenev,PunitSinghKoura,Marie-AnneLachaux,Thibaut
Lavril,JenyaLee,DianaLiskovich,YinghaiLu,YuningMao,XavierMartinet,TodorMihaylov,
PushkarMishra,IgorMolybog,YixinNie,AndrewPoulton,JeremyReizenstein,RashiRungta,
KalyanSaladi,AlanSchelten,RuanSilva,EricMichaelSmith,RanjanSubramanian,Xiao-
qingEllenTan,BinhTang,RossTaylor,AdinaWilliams,JianXiangKuan,PuxinXu,Zheng
Yan,IliyanZarov,YuchenZhang,AngelaFan,MelanieKambadur,SharanNarang,Aurelien
Rodriguez,RobertStojnic,SergeyEdunov,andThomasScialom. Llama2: Openfoundation
andfine-tunedchatmodels,2023.
[45] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas
Blecher,CristianCantonFerrer,MoyaChen,GuillemCucurull,DavidEsiobu,JudeFernandes,
JeremyFu,WenyinFu,BrianFuller,CynthiaGao,VedanujGoswami,NamanGoyal,Anthony
Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian
Khabsa,IsabelKloumann,ArtemKorenev,PunitSinghKoura,Marie-AnneLachaux,Thibaut
Lavril,JenyaLee,DianaLiskovich,YinghaiLu,YuningMao,XavierMartinet,TodorMihaylov,
PushkarMishra,IgorMolybog,YixinNie,AndrewPoulton,JeremyReizenstein,RashiRungta,
KalyanSaladi,AlanSchelten,RuanSilva,EricMichaelSmith,RanjanSubramanian,Xiao-
qingEllenTan,BinhTang,RossTaylor,AdinaWilliams,JianXiangKuan,PuxinXu,Zheng
13Yan,IliyanZarov,YuchenZhang,AngelaFan,MelanieKambadur,SharanNarang,Aurelien
Rodriguez,RobertStojnic,SergeyEdunov,andThomasScialom. Llama2: Openfoundation
andfine-tunedchatmodels,2023.
[46] BarbaraTverskyandMasakiSuwa. Thinkingwithsketches. 2009.
[47] BarbaraTversky,MasakiSuwa,ManeeshAgrawala,JulieHeiser,ChrisStolte,PatHanrahan,
DoantamPhan,JeffKlingner,Marie-PauleDaniel,PaulLee,etal. Sketchesfordesignand
designofsketches. HumanBehaviourinDesign: Individuals,Teams,Tools,pages79–86,2003.
[48] WeihanWang,QingsongLv,WenmengYu,WenyiHong,JiQi,YanWang,JunhuiJi,Zhuoyi
Yang,LeiZhao,XixuanSong,etal. Cogvlm: Visualexpertforpretrainedlanguagemodels.
arXivpreprintarXiv:2311.03079,2023.
[49] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,
DennyZhou, etal. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels.
Advancesinneuralinformationprocessingsystems,35:24824–24837,2022.
[50] PenghaoWuandSainingXie. V*: Guidedvisualsearchasacoremechanisminmultimodal
llms. ArXiv,abs/2312.14135,2023.
[51] QingyunWu,GaganBansal,JieyuZhang,YiranWu,ShaokunZhang,ErkangZhu,BeibinLi,
LiJiang,XiaoyunZhang,andChiWang. Autogen: Enablingnext-genllmapplicationsvia
multi-agentconversationframework. arXivpreprintarXiv:2308.08155,2023.
[52] TianbaoXie,DanyangZhang,JixuanChen,XiaochuanLi,SihengZhao,RuishengCao,TohJing
Hua,ZhoujunCheng,DongchanShin,FangyuLei,etal. Osworld: Benchmarkingmultimodal
agentsforopen-endedtasksinrealcomputerenvironments. arXivpreprintarXiv:2404.07972,
2024.
[53] AnYan,ZhengyuanYang,WanrongZhu,KevinLin,LinjieLi,JianfengWang,JianweiYang,
YiwuZhong,JulianMcAuley,JianfengGao,etal. Gpt-4vinwonderland: Largemultimodal
modelsforzero-shotsmartphoneguinavigation. arXivpreprintarXiv:2311.07562,2023.
[54] JianweiYang,HaoZhang,FengLi,XueyanZou,ChunyuanLi,andJianfengGao. Set-of-mark
promptingunleashesextraordinaryvisualgroundingingpt-4v.arXivpreprintarXiv:2310.11441,
2023.
[55] LiheYang, BingyiKang, ZilongHuang, XiaogangXu, JiashiFeng, andHengshuangZhao.
Depthanything: Unleashingthepoweroflarge-scaleunlabeleddata. InCVPR,2024.
[56] ZhengyuanYang*,LinjieLi*,JianfengWang*,KevinLin*,EhsanAzarnasab*,FaisalAhmed*,
Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for
multimodalreasoningandaction. 2023.
[57] ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,KarthikNarasimhan,andYuanCao.
React: Synergizingreasoningandactinginlanguagemodels. arXivpreprintarXiv:2210.03629,
2022.
[58] AndyZeng,MariaAttarian,BrianIchter,KrzysztofChoromanski,AdrianWong,StefanWelker,
Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, et al. Socratic models:
Composingzero-shotmultimodalreasoningwithlanguage. arXivpreprintarXiv:2204.00598,
2022.
[59] JieyuZhang,RanjayKrishna,AhmedHAwadallah,andChiWang. Ecoassistant: Usingllm
assistantmoreaffordablyandaccurately. arXivpreprintarXiv:2310.03046,2023.
[60] ZhuoshengZhang,AstonZhang,MuLi,HaiZhao,GeorgeKarypis,andAlexSmola. Mul-
timodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923,
2023.
14Appendices
A MoreQualitativeExamples 16
B Prompts 17
C Datasetstatistics 26
D Costs 26
E ImpactStatement 26
15A MoreQualitativeExamples
HereweprovidemoredetailsofhowGPT-4oand SKETCHPAD solvecomplexmathematicaland
visualreasoningtasks.
Math Function Correspondonce
A
Is f(x) even, odd, or neither? f(x)=− 2x8−4x6+122 xx 45 +4x2+11.16 Wh tic oh t hp eo i rn et f eis r ec no cr ere psp oo inn td ?ing REF D
C B
THOUGHT 1:Even functions are symmetric about the y-axis, and odd functions are THOUGHT 1:To find the corresponding point for the reference point in the first image,
symmetric about the origin. I need to plot the function. I will use the segmentation and marking tool to understand the objects relationships
ACTION 1: Call Python Ploting tool ACTION 1: Call segmentation and marking
y p= l- t2 . p* l ox t* (* x5 ,/ y( )2 * x**8 - 4 * x**6 + 12 * x**4 + 4 * x**2 + 11.16) s se eg gm me en nt te ed d1 2, , b bo ox xe es s1 2 = = s se eg gm me en nt t_ _a an nd d_ _m ma ar rk k( (i im ma ag ge e_ _1 2) )
OBSERVATION 1 A
D
REF
C B
THOUGHT 2: The function is symmetric about the origin, indicating f(x) is odd. THOUGHT 2:The reference point in the first image is on the monitor screen,
specifically on the right speaker. In the second image, I need to find the point that
corresponds to the right speaker on the monitor. The segmentation and marking tool
has labeled the objects, and I can see that the monitor in the second image is labeled
(a) as 3. The corresponding point should be on the right side of the monitor, which is B.
(b)
Figure5: QualitativeexamplesofhowGPT-4oandSKETCHPADsolve(a)complexmathematical
and(b)visualreasoningtasks.
16B Prompts
Promptsforcomputervisiontasks. HereweprovidetherunninglogofGPT-4o+SKETCHPAD
tackling a visual search problem from V∗Bench [50]. We use the same prompt template for all
computer vision problems in this work. For visualization purposes, we present the prompts that
containcodesdifferently. Inourimplementation,theyarealltextinputstotheLLM.
SYSTEMPROMPT
YouareahelpfulmultimodalAIassistant.[MOREINSTRUCTIONS...]
Foreachturn,youshouldfirstdoa"THOUGHT",basedontheimagesandtextyousee.Ifyouthink
you get the answer to the intial user request, you can reply with "ANSWER: <your answer>" and
endswith"TERMINATE".
InitialPrompt+Request
1 Hereare sometools that canhelpyou. All are pythoncodes. Theyare in tools.pyandwill beimported for you.
2 Theimageshas their owncoordinate system. Theupper left corner of the imageis the origin (0, 0). All coordinates are normalized, i.e., the
range is [0, 1].
3 Allboundingboxesare in the formatof [x, y, w,h], whichis apython list. x is the horizontal coordinate of the upper−left corner of the box,y
is the vertical coordinate of that corner, wis the boxwidth, andh is the boxheight.
4 Notice that you, asanAI assistant, is notgoodat locating things anddescribe themwith coordinate. Youcanuse tools to generate boundingboxes
.
5 Youare also notgoodat answering questions about small visual details in the image.Youcanuse tools to zoominonthe imageto see the details.
Beloware the tools in tools.py:
6 ‘‘‘python
7 class AnnotatedImage:
8 #Aclass to represent anannotated image. It contains the annotated imageandthe original image.
9
10 def __init__(self, annotated_image:Image.Image,original_image: Image.Image=None):
11 self.annotated_image=annotated_image
12 self.original_image =original_image
13
14 def detection(image, objects):
15 """Object detection usingGroundingDINOmodel.Itreturns the annotated imageandthe boundingboxesof the detected objects.
16 Thetext canbesimplenoun,orsimple phrase (e.g., ’bus’, ’red car’). Cannotbetoohardor the modelwill break.
17 Thedetector is not perfect, it maywronglydetect objects ormisssomeobjects.
18 Also, notice that the boundingboxlabel mightbeout of the imageboundary.
19 Youshoulduse the output asa reference, not asagroundtruth.
20 Whenansweringquestionsabout the image,youshoulddouble−checkthe detected objects.
21
22 Args:
23 image(PIL.Image.Image): the input image
24 objects (List[str]): a list of objects to detect. Eachobject shouldbeasimplenounorasimple phrase. Shouldnotbehardor abstract
concepts like "text" or "number".
25
26 Returns:
27 output_image(AnnotatedImage):the original image, annotated withboundingboxes. Eachboxis labeled with the detected object, andan
index.
28 processed boxes(List): listthe boundingboxesof the detected objects
29
30 Example:
31 image=Image.open("sample_img.jpg")
32 output_image,boxes= detection(image, ["bus"])
33 display(output_image.annotated_image)
34 print(boxes)# [[0.24, 0.21, 0.3, 0.4], [0.6, 0.3, 0.2, 0.3]]
35 #youneedto double−checkthe detected objects. Someobjectsmaybemissedorwronglydetected.
36 """
37
38 def sliding_window_detection(image, objects):
39 """Usethis whenyouare searching for objects in the image,but the objects are not detected bythe object detection model.
40 In that case, the mostcommonreasonis that the object is too smallsuch that both the vision−languagemodelandthe object detection model
fail to detect it.
41 This function tries to detect the object by sliding windowsearch.
42 Withthe help of the detection model, it tries to detect the object in the zoomed−inpatches.
43 Thefunction returns a list of annotated imagesthat maycontain at leas oneof the objects, annotated withboundingboxes.
44 It also returns a list ofa list ofboundingboxesof the detected objects.
45
46 Args:
47 image(PIL.Image.Image): the input image
48 objects (List[str]): a list of objects to detect. Eachobject shouldbeasimplenounorasimple phrase. Shouldnotbehardor abstract
concepts like "text" or "number".
49
50 Returns:
51 possible_patches (List[AnnotatedImage]):a list of annotated zoomed−inimagesthat maycontain the object, annotated withboundingboxes.
52 possible_boxes (List[List[List[Float]]]): Foreachimagein possible_patches, a list ofboundingboxesof the detected objects.
53 Thecoordinates are w.r.t. eachzoomed−inimage.Theorder of the boxesis the sameas the order of the imagesin possible_patches.
54
55 Example:
56 image=Image.open("sample_img.jpg")
1757 possible_patches, possible_boxes =search_object_and_zoom(image,["bird", "sign"])
58 for i, patch in enumerate(possible_patches):
59 print(f"Patch{i}:")
60 display(patch.annotated_image)
61
62 # print the boundingboxesof the detected objects in the first patch
63 print(possible_boxes[0]) # [[0.24, 0.21, 0.3, 0.4], [0.6, 0.3, 0.2, 0.3]]
64 """
65
66 defsegment_and_mark(image,anno_mode:list=[’Mask’,’Mark’]):
67 """Useasegmentationmodeltosegmentthe image,andadd colorful masksonthe segmentedobjects. Eachsegmentis also labeled withanumber.
68 Theannotated imageis returned alongwith the boundingboxesof the segmentedobjects.
69 This tool mayhelpyouto better reason about the relationship betweenobjects, whichcanbe useful for spatial reasoning etc.
70 DONOTusethistool to search or detect an object. It is likely the object is smallandsegmentaitondoesnot help.
71 Segmentationandmarkingcan also be helpful for 3Dandvideo reasoning. Forexample, helping youto seemoreclearly andanalyzes the
relationship between different framesofavideo.
72
73 Args:
74 image(PIL.Image.Image): the input image
75 anno_mode(list, optional): Whatannotation is addedonthe input image.Maskis the colorful masks.Andmarkis the numberlabels.
Defaults to [’Mask’,’Mark’].
76
77 Returns:
78 output_image(AnnotatedImage):the original imageannotated with colorful masksandnumberlabels. Eachmaskis labeled withanumber.The
numberlabel starts at 1.
79 bboxes(List): listthe boundingboxesof the masks.Theorder of the boxesis the sameas the order of the numberlabels.
80
81 Example:
82 Userrequest: I wantto find a seat close to windows,whereshould I sit?
83 Code:
84 ‘‘‘python
85 image=Image.open("sample_img.jpg")
86 output_image,bboxes=segment_and_mark(image)
87 display(output_image.annotated_image)
88 ‘‘‘
89 Modelreply: Youcan sit onthe chair numberedas5, whichis close to the window.
90 User:Givemetheboundingboxof that chair.
91 Code:
92 ‘‘‘python
93 print(bboxes[4]) # [0.24, 0.21, 0.3, 0.4]
94 ‘‘‘
95 Modelreply: Theboundingboxof the chair numberedas5 is [0.24, 0.21, 0.3, 0.4].
96 """
97
98 defdepth(image):
99 """Depthestimation usingDepthAnythingmodel.It returns the depthmapofthe input image.
100 Acolormapis usedto represent the depth. It uses Inferno colormap.Thecloser the object, the warmerthe color.
101 This tool mayhelpyouto better reason about the spatial relationship, like whichobject is closer to the camera.
102
103 Args:
104 image(PIL.Image.Image): the input image
105
106 Returns:
107 output_image(PIL.Image.Image): the depthmapofthe input image
108
109 Example:
110 image=Image.open("sample_img.jpg")
111 output_image=depth(image)
112 display(output_image)
113 """
114
115 defzoom_in_image_by_bbox(image,box,padding=0.05):
116 """Asimplewrapperfunction to crop the imagebasedonthe boundingbox.
117 Whenyouwanttoanswerquestion about visual details in aboundingboxannotated bythe detection tool, youwouldlike to zoominonthe
object using this function.
118
119 Args:
120 image(PIL.Image.Image): the input image
121 box(List[float]): the boundingboxin the formatof [x, y, w,h]
122 padding(float, optional): Thepaddingfor the imagecrop, outside of the boundingbox. Defaults to 0.1. Thezoomfactor cannotbetoo
small. Minimumis0.05
123
124 Returns:
125 cropped_img(PIL.Image.Image): the croppedimage
126
127 Example:
128 image=Image.open("sample_img.jpg")
129 annotated_img, boxes= detection(image,"bus")
130 cropped_img=zoom_in_image_by_bbox(image,boxes[0],padding=0.05)
131 display(cropped_img)
132 """
133
134 defoverlay_images(background_img,overlay_img, alpha=0.3, bounding_box=[0,0, 1, 1]):
135 """
136 Overlayanimageonto another imagewith transparency.
137 This is particularly useful visualizing heatmapwhile preserving someinfofromthe original image.
138 Forexample,youcan overlay asegmentedimageonaheatmapto better understand the spatial relationship betweenobjects.
139 It will also help seeing the labels, circles onthe original imagethat maynotbe visible onthe heatmap.
140
141 Args:
142 background_img_pil(PIL.Image.Image):ThebackgroundimageinPILformat.
143 overlay_img_pil (PIL.Image.Image):Theimageto overlay in PILformat.
18144 alpha (float): Transparencyof the overlay image.
145 bounding_box(List[float]): Theboundingboxof the overlay image.Theformat is [x, y, w,h]. Thecoordinates are normalized to the
backgroundimage. Defaults to [0, 0, 1, 1].
146
147 Returns:
148 PIL.Image.Image:The resulting imageafter overlay, in PILformat.
149 s
150 Example:
151 image=Image.open(’original.jpg’)
152 depth_map=depth(image)
153 overlayed_image=overlay_images(depth_map,image,alpha=0.3)
154 display(overlayed_image)
155 """
156 ‘‘‘
157 #GOAL#:Basedontheabovetools, I wantyouto reason abouthowto solve the #USERREQUEST#andgeneratetheactionsstepbystep (each action
is apython jupyter notebookcodeblock) to solve the request.
158 Youmayneedtouse the tools aboveto process the imagesandmakedecisions basedonthe visual outputs of the previous codeblocks.
159 Yourvisual ability is not perfect, soyoushoulduse these tools to assist youin reasoning about the images.
160
161 #[six in−context exampleshere]
162
163 #USERREQUEST#:
1 Is the red balloon aboveorbelowthe white balloon?Options: (A)Thered balloon is belowthe white balloon. (B)Thered balloon is abovethe
white balloon.
2
3 #USERIMAGEstoredinimage_1asPILimage.
4 Nowpleasegenerate onlyTHOUGHT0andACTION0inRESULT.Ifnoactionneeded,alsoreplywithANSWER:<youranswer>andendswithTERMINATE
intheRESULT:
5
6 #RESULT#:
Thought+Action0
1 THOUGHT0:Todeterminetherelative positions of the redandwhite balloons, I needto detect the balloons in the image. I will use the detection
tool to find the balloons.
2 ACTION0:
3 ‘‘‘python
4 image_1_balloon_detection, image_1_balloon_boxes=detection(image_1,["red balloon", "white balloon"])
5 display(image_1_balloon_detection.annotated_image)
6 ‘‘‘
19Observation0
Executionsuccess. Theoutputisasfollows.
PleasegeneratethenextTHOUGHTandACTION.Ifyoucangettheanswer,pleasealsoreplywith
ANSWER:<youranswer>andendswithTERMINATE.
Thought+Action1
1 THOUGHT1:Thedetectiontooldidnot annotate the balloons in the image. I will use the sliding_window_detection tool to search for the redand
white balloons in the image.
2
3 ACTION1:
4 ‘‘‘python
5 possible_patches, possible_boxes =sliding_window_detection(image_1,["red balloon", "white balloon"])
6 for i, patch in enumerate(possible_patches):
7 print(f"Patch{i+1}:")
8 display(patch.annotated_image)
9 ‘‘‘
Observation1
Executionsuccess.Theoutputisasfollows.Patch1:
20Patch2:
Patch3:
Patch4:
Patch5:
PleasegeneratethenextTHOUGHTandACTION.Ifyoucangettheanswer,pleasealsoreplywith
ANSWER:<youranswer>andendswithTERMINATE.
Thought+Action2
THOUGHT2: FromPatch4and5,Icanseeboththeredandwhiteballoons. Theredballoonis
clearlybelowthewhiteballooninthesepatches.
ACTION2: Noactionneeded.
ANSWER:Theredballoonisbelowthewhiteballoon. Sotheansweris(A).TERMINATE
21Promptsformathtasks. Thepromptsaresimilartothevisiontaskformathtasks,exceptweremove
thecomputervisionspecialists,andaddexamplecodesformathplotting. Besides,theuserqueryis
different. Foreachtask,theuserquerypartofthepromptisasfollows.
PROMPT
Youaregivenareal-valued,scalarfunctionf(x).
YOURTASKistodeterminewhetherf(x)isanevenfunction,anoddfunction,orneither.Definition
ofanoddfunction:Afunctionsuchthat
f(−x)=−f(x)
where the sign is reversed but the absolute value remains the same if the sign of the independent
variableisreversed.Afunctionisneitherevennoroddifitdoesnotsatisfyeithercondition.
Hereistheexpressionoff(x):
−2x5
f(x)=
2x8−4x6+12x4+4x2+11.16
Respond with ‘even’, ‘odd’, ‘neither’ first on whether the function f(x) is even,
odd, or neither, based on the definitions and your observation of the function.
Youcangeneratematplotlibcodetovisualizethefunction.
Ifyoucangettheanswer, pleasereplywithANSWER:<youranswer>, extractthefinalanswerin
FINALANSWER:<finalanswer>andendswithTERMINATEintheRESULT.
Answer:
Figure6:PromptfortheMathParitytask.Wefollowasimilarpromptformatto[8],exceptprompting
themodelstowritethecodetogenerateimages.
PROMPT
Youaregivenareal-valued,scalarfunctionf(x).
YOURTASKistodeterminewhetherf(x)isanconvexfunctionorconcavefunction.Definitionofa
convexfunction:Afunctionsuchthat
∀x,y,0≤t≤1,f(tx+(1−t)y)≤tf(x)+(1−t)f(y)
Definitionofaconcavefunction:Afunctionsuchthat
∀x,y,0≤t≤1,f(tx+(1−t)y)≥tf(x)+(1−t)f(y)
Hereistheexpressionoff(x):
f(x)=7.57−0.08∗Abs(x)
Respond with ‘convex’ or ‘concave’ first on whether the function f (x) is con-
vex or concave, based on the definitions and your observation of the function.
Youcangeneratematplotlibcodetovisualizethefunction.
Ifyoucangettheanswer, pleasereplywithANSWER:<youranswer>, extractthefinalanswerin
FINALANSWER:<finalanswer>andendswithTERMINATEintheRESULT.
Answer:
Figure7: PromptfortheMathConvexitytask. Wefollowthesimilarpromptformatto[8],except
promptingthemodelstowritethecodetogenerateimages.
22PROMPT
Youaregivenanadjacencymatrixofagraphandtwoquerynodes.
YOURTASKistofindifthereisapathbetweenthetwonodes.
Definition of connectivity: In an undirected graph G, two vertices u and v are called connected ifG
containsapathfromutov.Apathinagraphisafinitesequenceofedgeswhichjoinsasequenceof
vertices.Inthequeryexample,thenodesandtheadjacencymatrixarezero-indexed.
QueryExample:
AdjacencyMatrix:
0 0 0 0 0 1 0 0 0
0 0 1 0 0 0 1 0 0
 
0 1 0 0 1 0 0 0 0
 
0 0 0 0 0 0 0 0 0
 
0 0 1 0 0 0 0 0 0
 1 0 0 0 0 0 1 0 1 
0 1 0 0 0 1 0 0 0
 
0 0 0 0 0 0 0 0 0
0 0 0 0 0 1 0 0 0
Querynodesindices(zero-indexed):4and0
Respondwith‘yes’or‘no’firstonwhetherthequerynodesareconnectedornotinthegraph.
Ifthereisapath,firstprovidethepathasasequenceofvertices(nodes),andthenexplainyourrea-
soning. Youcanusenetworkxtodrawthegraph. Ifthereisnopath,explainwhyindetails.Answer
(start with ’yes’ or ’no’): If you can get the answer, please reply with ANSWER: <your answer>,
extractthefinalanswerinFINALANSWER:<finalanswer>andendswithTERMINATEintheRE-
SULT.
Answer:
Figure8: PromptfortheGraphConnectivitytask. Wefollowthesimilarpromptformatto[8],except
promptingthemodelstowritethecodetogenerateimages.
23PROMPT
Youaregivenavisualrepresentationoftwographs,graphGontheleftandgraphHontheright.
YOURTASKistodeterminewhetherthetwographsareisomorphictoeachother.
Definitionofgraphisomorphism: Ingraphtheory,anisomorphismofgraphsGandHisabijectionf
betweenthevertexsetsofGandH,denotedasf :V(G)→V(H).GandHaresaidtobeisomorphic
whenf satisfiesthefollowing:anytwoverticesuandvofGareadjacentinGifandonlyiff(u)and
f(v)areadjacentinH.Thiskindofbijectioniscommonlydescribedas"edge-preservingbijection",
inaccordancewiththegeneralnotionofisomorphismbeingastructure-preservingbijection.
Inthequeryexample,theadjacencymatricesarezero-indexed.
AdjacencyMatrixG:
0 0 0 0 0 1 0 0 0
0 0 1 0 0 0 1 0 0
 
0 1 0 0 1 0 0 0 0
 
0 0 0 0 0 0 0 0 0
 
0 0 1 0 0 0 0 0 0
 1 0 0 0 0 0 1 0 1 
0 1 0 0 0 1 0 0 0
 
0 0 0 0 0 0 0 0 0
0 0 0 0 0 1 0 0 0
AdjacencyMatrixH:
0 0 0 0 0 0 0 0 0
0 0 1 0 0 0 1 0 0
 
1 0 0 0 0 0 0 0 0
 
0 0 0 0 0 0 0 0 0
 
0 0 0 0 0 0 0 0 0
 1 0 0 0 0 0 1 0 0 
0 1 0 1 0 1 0 0 0
 
0 0 0 0 0 0 0 1 0
0 0 0 0 0 1 0 0 0
Respondwith’yes’or’no’firstonwhetherthetwographsareisomorphictoeachother. Youcan
usenetworkxtodrawthegraph. Iftheyareisomorphic,firstprovidethebijectionbetweenthetwo
graphs,andthenexplainyourreasoning. Youcanusenetworkxtodrawthegraph. Iftheyarenot
isomorphic,explainwhyindetail.Answer(startwith’yes’or’no’):Ifyoucangettheanswer,please
replywithANSWER:<youranswer>,extractthefinalanswerinFINALANSWER:<finalanswer>
andendswithTERMINATEintheRESULT.
Answer:
Figure9: PromptfortheGraphIsomorphismtask. Wefollowasimilarpromptformatto[8],except
promptingthemodelstowritethecodetogenerateimages.
24PROMPT
Youaregivenanadjacencymatrixofagraphandtwoquerynodes(onesourcenodeandonesink
node). Thesourcenodeisthenodewheretheflowstartsandthesinknodeisthenodewherethe
flowends.
YOURTASKistosolvethemaxflowproblemgiventheweighteddirectedgraph.
DefinitionofMaxflowproblem:Inthemaxflowproblem,wehaveadirectedgraphwithasourcenode
sandasinknodet,andeachedgehasacapacity(integervalued,coloredingreen)thatrepresents
themaximumamountofflowthatcanbesentthroughit. Thegoalistofindthemaximumamount
offlowthatcanbesentfromstot,whilerespectingthecapacityconstraintsontheedges.
QueryExample:
AdjacencyMatrix:
 
0 1 4
0 0 6
0 0 0
Sourcenode(zero-indexed):0
Sinknode(zero-indexed):2
In the query example, the nodes and the adjacency matrix are zero-indexed.
Youcanusenetworkxtodrawthegraph. If you can get the answer, please reply with AN-
SWER:<youranswer>,extractthefinalanswerinFINALANSWER:<finalanswer>andendswith
TERMINATEintheRESULT.
Answer:
Figure 10: Prompt for Graph Maxflow task. We follow the similar prompt format to [8], except
promptingthemodelstosolvethemaxflowproblem.
PROMPT
GiventhefollowingFENofthechessgame:
1r1q1rk1/1b2b1Qp/4pp1B/pp1nP3/2pPN3/P1P5/1PB3PP/R4RK1b--018
Determinethegame’soutcome. Whowon: WhiteorBlack? Answer(startwith’white’or’black’or
’draw’):
YoucandrawthechessboardusingPythongiventheFENstring. Ifyoucangettheanswer,please
replywithANSWER:<youranswer>,extractthefinalanswerinFINALANSWER:<finalanswer>
andendswithTERMINATEintheRESULT.
Answer:
Figure11: PromptforWinnerIDtask. Wefollowasimilarpromptformatto[8],exceptprompting
themodelstoanalyzethegameoutcome.
25C Datasetstatistics
Table 5 and 6 show the statistics of the datasets we used, including IsoBench [8], BLINK [9],
MMVP[43],andV∗Bench[50].
Dataset size partition representation
MathParity 383 val code
MathConvexity 255 val code
GraphMaxflow 128 val array
GraphConnectivity 128 val array
GraphIsomorphism 128 val array
WinnerID 257 val FEN
Table5: IsoBench[8]datastatistics.
Dataset size partition input
V∗Bench 257 - SingleImage
MMVP 300 - SingleImage
BLINKRelativeDepth 124 val SingleImage
BLINKSpatialRelation 143 val SingleImage
BLINKJigsawPuzzle 150 val MultipleImages
BLINKVisualCorrespondence 172 val MultipleImages
BLINKSemanticCorrespondence 139 val MultipleImage
Table6: Visiontasksdatastatistics.
D Costs
ThecostofrunningeachtaskusingGPT-4oisinTable7.
Dataset tokenspersample GPT-4ocostpersample
MathParity 2994 $0.015
MathConvexity 2211 $0.011
GraphConnectivity 2819 $0.014
GraphIsomorphism 3143 $0.016
V∗Bench 26647 $0.133
MMVP 11870 $0.059
BLINKRelativeDetph 14078 $0.070
BLINKSpatialRelation 12848 $0.064
BLINKJigsawPuzzle 13206 $0.066
BLINKVisualCorrespondence 16988 $0.085
BLINKSemanticCorrespondence 11508 $0.058
Table7: ThecostofrunningSKETCHPADoneachtask.
E ImpactStatement
OurworkproposesSKETCHPAD,aframeworkaimingatadvancingacademicresearchandmeeting
industryneeds. Inabroaderperspective,SKETCHPADproposesanewwaythathumanscaninteract
withLMs,andmakesLMsmoreinterpretablebyelicitingtheirreasoningprocesswithbothlanguage
andsketches. Ontheotherhand,ifmisused,theLMsmaybeusedtogenerateharmfulvisionand
textartifacts. Nevertheless,thisisnotdirectlyrelatedtoourresearch,andmoreresearcherscanbe
involvedtoresearchonthesafetyissueinamultimodalcontext.
26