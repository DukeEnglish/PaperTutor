[
    {
        "title": "An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels",
        "authors": "Duy-Kien NguyenMahmoud AssranUnnat JainMartin R. OswaldCees G. M. SnoekXinlei Chen",
        "links": "http://arxiv.org/abs/2406.09415v1",
        "entry_id": "http://arxiv.org/abs/2406.09415v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09415v1",
        "summary": "This work does not introduce a new method. Instead, we present an interesting\nfinding that questions the necessity of the inductive bias -- locality in\nmodern computer vision architectures. Concretely, we find that vanilla\nTransformers can operate by directly treating each individual pixel as a token\nand achieve highly performant results. This is substantially different from the\npopular design in Vision Transformer, which maintains the inductive bias from\nConvNets towards local neighborhoods (e.g. by treating each 16x16 patch as a\ntoken). We mainly showcase the effectiveness of pixels-as-tokens across three\nwell-studied tasks in computer vision: supervised learning for object\nclassification, self-supervised learning via masked autoencoding, and image\ngeneration with diffusion models. Although directly operating on individual\npixels is less computationally practical, we believe the community must be\naware of this surprising piece of knowledge when devising the next generation\nof neural architectures for computer vision.",
        "updated": "2024-06-13 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09415v1"
    },
    {
        "title": "Rethinking Score Distillation as a Bridge Between Image Distributions",
        "authors": "David McAllisterSongwei GeJia-Bin HuangDavid W. JacobsAlexei A. EfrosAleksander HolynskiAngjoo Kanazawa",
        "links": "http://arxiv.org/abs/2406.09417v1",
        "entry_id": "http://arxiv.org/abs/2406.09417v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09417v1",
        "summary": "Score distillation sampling (SDS) has proven to be an important tool,\nenabling the use of large-scale diffusion priors for tasks operating in\ndata-poor domains. Unfortunately, SDS has a number of characteristic artifacts\nthat limit its usefulness in general-purpose applications. In this paper, we\nmake progress toward understanding the behavior of SDS and its variants by\nviewing them as solving an optimal-cost transport path from a source\ndistribution to a target distribution. Under this new interpretation, these\nmethods seek to transport corrupted images (source) to the natural image\ndistribution (target). We argue that current methods' characteristic artifacts\nare caused by (1) linear approximation of the optimal path and (2) poor\nestimates of the source distribution. We show that calibrating the text\nconditioning of the source distribution can produce high-quality generation and\ntranslation results with little extra overhead. Our method can be easily\napplied across many domains, matching or beating the performance of specialized\nmethods. We demonstrate its utility in text-to-2D, text-based NeRF\noptimization, translating paintings to real images, optical illusion\ngeneration, and 3D sketch-to-real. We compare our method to existing approaches\nfor score distillation sampling and show that it can produce high-frequency\ndetails with realistic colors.",
        "updated": "2024-06-13 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09417v1"
    },
    {
        "title": "Interpreting the Weight Space of Customized Diffusion Models",
        "authors": "Amil DravidYossi GandelsmanKuan-Chieh WangRameen AbdalGordon WetzsteinAlexei A. EfrosKfir Aberman",
        "links": "http://arxiv.org/abs/2406.09413v1",
        "entry_id": "http://arxiv.org/abs/2406.09413v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09413v1",
        "summary": "We investigate the space of weights spanned by a large collection of\ncustomized diffusion models. We populate this space by creating a dataset of\nover 60,000 models, each of which is a base model fine-tuned to insert a\ndifferent person's visual identity. We model the underlying manifold of these\nweights as a subspace, which we term weights2weights. We demonstrate three\nimmediate applications of this space -- sampling, editing, and inversion.\nFirst, as each point in the space corresponds to an identity, sampling a set of\nweights from it results in a model encoding a novel identity. Next, we find\nlinear directions in this space corresponding to semantic edits of the identity\n(e.g., adding a beard). These edits persist in appearance across generated\nsamples. Finally, we show that inverting a single image into this space\nreconstructs a realistic identity, even if the input image is out of\ndistribution (e.g., a painting). Our results indicate that the weight space of\nfine-tuned diffusion models behaves as an interpretable latent space of\nidentities.",
        "updated": "2024-06-13 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09413v1"
    },
    {
        "title": "Explore the Limits of Omni-modal Pretraining at Scale",
        "authors": "Yiyuan ZhangHandong LiJing LiuXiangyu Yue",
        "links": "http://arxiv.org/abs/2406.09412v1",
        "entry_id": "http://arxiv.org/abs/2406.09412v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09412v1",
        "summary": "We propose to build omni-modal intelligence, which is capable of\nunderstanding any modality and learning universal representations. In specific,\nwe propose a scalable pretraining paradigm, named Multimodal Context (MiCo),\nwhich can scale up the numbers of modalities and amount of data, together with\nthe model parameters, in the pretraining process. With MiCo, the pretrained\nmodels show significant emergent abilities in multimodal learning, which are\nevaluated on the following tasks: i) single-modality perception benchmarks of\n10 different modalities, ii) 25 cross-modality understanding tasks of\nretrieval, question-answering, captioning, and iii) 18 multimodal large\nlanguage model benchmarks. Our models establish 37 new records for\nstate-of-the-art performance. We hope that our research could contribute to the\ndevelopment of omni-modal intelligence. Code and Models are at\nhttps://github.com/invictus717/MiCo",
        "updated": "2024-06-13 17:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09412v1"
    },
    {
        "title": "Data Attribution for Text-to-Image Models by Unlearning Synthesized Images",
        "authors": "Sheng-Yu WangAaron HertzmannAlexei A. EfrosJun-Yan ZhuRichard Zhang",
        "links": "http://arxiv.org/abs/2406.09408v1",
        "entry_id": "http://arxiv.org/abs/2406.09408v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09408v1",
        "summary": "The goal of data attribution for text-to-image models is to identify the\ntraining images that most influence the generation of a new image. We can\ndefine \"influence\" by saying that, for a given output, if a model is retrained\nfrom scratch without that output's most influential images, the model should\nthen fail to generate that output image. Unfortunately, directly searching for\nthese influential images is computationally infeasible, since it would require\nrepeatedly retraining from scratch. We propose a new approach that efficiently\nidentifies highly-influential images. Specifically, we simulate unlearning the\nsynthesized image, proposing a method to increase the training loss on the\noutput image, without catastrophic forgetting of other, unrelated concepts.\nThen, we find training images that are forgotten by proxy, identifying ones\nwith significant loss deviations after the unlearning process, and label these\nas influential. We evaluate our method with a computationally intensive but\n\"gold-standard\" retraining from scratch and demonstrate our method's advantages\nover previous methods.",
        "updated": "2024-06-13 17:59:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09408v1"
    }
]