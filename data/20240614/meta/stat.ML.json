[
    {
        "title": "Why Warmup the Learning Rate? Underlying Mechanisms and Improvements",
        "authors": "Dayal Singh KalraMaissam Barkeshli",
        "links": "http://arxiv.org/abs/2406.09405v1",
        "entry_id": "http://arxiv.org/abs/2406.09405v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09405v1",
        "summary": "It is common in deep learning to warm up the learning rate $\\eta$, often by a\nlinear schedule between $\\eta_{\\text{init}} = 0$ and a predetermined target\n$\\eta_{\\text{trgt}}$. In this paper, we show through systematic experiments\nusing SGD and Adam that the overwhelming benefit of warmup arises from allowing\nthe network to tolerate larger $\\eta_{\\text{trgt}}$ by forcing the network to\nmore well-conditioned areas of the loss landscape. The ability to handle larger\n$\\eta_{\\text{trgt}}$ makes hyperparameter tuning more robust while improving\nthe final performance. We uncover different regimes of operation during the\nwarmup period, depending on whether training starts off in a progressive\nsharpening or sharpness reduction phase, which in turn depends on the\ninitialization and parameterization. Using these insights, we show how\n$\\eta_{\\text{init}}$ can be properly chosen by utilizing the loss catapult\nmechanism, which saves on the number of warmup steps, in some cases completely\neliminating the need for warmup. We also suggest an initialization for the\nvariance in Adam which provides benefits similar to warmup.",
        "updated": "2024-06-13 17:59:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09405v1"
    },
    {
        "title": "Oblivious subspace embeddings for compressed Tucker decompositions",
        "authors": "Matthew PietrosanuBei JiangLinglong Kong",
        "links": "http://arxiv.org/abs/2406.09387v1",
        "entry_id": "http://arxiv.org/abs/2406.09387v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09387v1",
        "summary": "Emphasis in the tensor literature on random embeddings (tools for\nlow-distortion dimension reduction) for the canonical polyadic (CP) tensor\ndecomposition has left analogous results for the more expressive Tucker\ndecomposition comparatively lacking. This work establishes general\nJohnson-Lindenstrauss (JL) type guarantees for the estimation of Tucker\ndecompositions when an oblivious random embedding is applied along each mode.\nWhen these embeddings are drawn from a JL-optimal family, the decomposition can\nbe estimated within $\\varepsilon$ relative error under restrictions on the\nembedding dimension that are in line with recent CP results. We implement a\nhigher-order orthogonal iteration (HOOI) decomposition algorithm with random\nembeddings to demonstrate the practical benefits of this approach and its\npotential to improve the accessibility of otherwise prohibitive tensor\nanalyses. On moderately large face image and fMRI neuroimaging datasets,\nempirical results show that substantial dimension reduction is possible with\nminimal increase in reconstruction error relative to traditional HOOI ($\\leq$5%\nlarger error, 50%-60% lower computation time for large models with 50%\ndimension reduction along each mode). Especially for large tensors, our method\noutperforms traditional higher-order singular value decomposition (HOSVD) and\nrecently proposed TensorSketch methods.",
        "updated": "2024-06-13 17:58:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09387v1"
    },
    {
        "title": "Learning conditional distributions on continuous spaces",
        "authors": "Cyril BénézetZiteng ChengSebastian Jaimungal",
        "links": "http://arxiv.org/abs/2406.09375v1",
        "entry_id": "http://arxiv.org/abs/2406.09375v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09375v1",
        "summary": "We investigate sample-based learning of conditional distributions on\nmulti-dimensional unit boxes, allowing for different dimensions of the feature\nand target spaces. Our approach involves clustering data near varying query\npoints in the feature space to create empirical measures in the target space.\nWe employ two distinct clustering schemes: one based on a fixed-radius ball and\nthe other on nearest neighbors. We establish upper bounds for the convergence\nrates of both methods and, from these bounds, deduce optimal configurations for\nthe radius and the number of neighbors. We propose to incorporate the nearest\nneighbors method into neural network training, as our empirical analysis\nindicates it has better performance in practice. For efficiency, our training\nprocess utilizes approximate nearest neighbors search with random binary space\npartitioning. Additionally, we employ the Sinkhorn algorithm and a\nsparsity-enforced transport plan. Our empirical findings demonstrate that, with\na suitably designed structure, the neural network has the ability to adapt to a\nsuitable level of Lipschitz continuity locally. For reproducibility, our code\nis available at \\url{https://github.com/zcheng-a/LCD_kNN}.",
        "updated": "2024-06-13 17:53:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09375v1"
    },
    {
        "title": "Advancing Graph Generation through Beta Diffusion",
        "authors": "Yilin HeXinyang LiuBo ChenMingyuan Zhou",
        "links": "http://arxiv.org/abs/2406.09357v1",
        "entry_id": "http://arxiv.org/abs/2406.09357v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09357v1",
        "summary": "Diffusion models have demonstrated effectiveness in generating natural images\nand have been extended to generate diverse data types, including graphs. This\nnew generation of diffusion-based graph generative models has demonstrated\nsignificant performance improvements over methods that rely on variational\nautoencoders or generative adversarial networks. It's important to recognize,\nhowever, that most of these models employ Gaussian or categorical diffusion\nprocesses, which can struggle with sparse and long-tailed data distributions.\nIn our work, we introduce Graph Beta Diffusion (GBD), a diffusion-based\ngenerative model particularly adept at capturing diverse graph structures. GBD\nutilizes a beta diffusion process, tailored for the sparse and range-bounded\ncharacteristics of graph adjacency matrices. Furthermore, we have developed a\nmodulation technique that enhances the realism of the generated graphs by\nstabilizing the generation of critical graph structures, while preserving\nflexibility elsewhere. The outstanding performance of GBD across three general\ngraph benchmarks and two biochemical graph benchmarks highlights its capability\nto effectively capture the complexities of real-world graph data. The code will\nbe made available at https://github.com/YH-UtMSB/Graph_Beta_Diffusion",
        "updated": "2024-06-13 17:42:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09357v1"
    },
    {
        "title": "Separations in the Representational Capabilities of Transformers and Recurrent Architectures",
        "authors": "Satwik BhattamishraMichael HahnPhil BlunsomVarun Kanade",
        "links": "http://arxiv.org/abs/2406.09347v1",
        "entry_id": "http://arxiv.org/abs/2406.09347v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09347v1",
        "summary": "Transformer architectures have been widely adopted in foundation models. Due\nto their high inference costs, there is renewed interest in exploring the\npotential of efficient recurrent architectures (RNNs). In this paper, we\nanalyze the differences in the representational capabilities of Transformers\nand RNNs across several tasks of practical relevance, including index lookup,\nnearest neighbor, recognizing bounded Dyck languages, and string equality. For\nthe tasks considered, our results show separations based on the size of the\nmodel required for different architectures. For example, we show that a\none-layer Transformer of logarithmic width can perform index lookup, whereas an\nRNN requires a hidden state of linear size. Conversely, while constant-size\nRNNs can recognize bounded Dyck languages, we show that one-layer Transformers\nrequire a linear size for this task. Furthermore, we show that two-layer\nTransformers of logarithmic size can perform decision tasks such as string\nequality or disjointness, whereas both one-layer Transformers and recurrent\nmodels require linear size for these tasks. We also show that a log-size\ntwo-layer Transformer can implement the nearest neighbor algorithm in its\nforward pass; on the other hand recurrent models require linear size. Our\nconstructions are based on the existence of $N$ nearly orthogonal vectors in\n$O(\\log N)$ dimensional space and our lower bounds are based on reductions from\ncommunication complexity problems. We supplement our theoretical results with\nexperiments that highlight the differences in the performance of these\narchitectures on practical-size sequences.",
        "updated": "2024-06-13 17:31:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09347v1"
    }
]