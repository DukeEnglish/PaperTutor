[
    {
        "title": "Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging",
        "authors": "Yuzhe YangYujia LiuXin LiuAvanti GulhaneDomenico MastrodicasaWei WuEdward J WangDushyant W SahaniShwetak Patel",
        "links": "http://arxiv.org/abs/2402.14815v1",
        "entry_id": "http://arxiv.org/abs/2402.14815v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14815v1",
        "summary": "Advances in artificial intelligence (AI) have achieved expert-level\nperformance in medical imaging applications. Notably, self-supervised\nvision-language foundation models can detect a broad spectrum of pathologies\nwithout relying on explicit training annotations. However, it is crucial to\nensure that these AI models do not mirror or amplify human biases, thereby\ndisadvantaging historically marginalized groups such as females or Black\npatients. The manifestation of such biases could systematically delay essential\nmedical care for certain patient subgroups. In this study, we investigate the\nalgorithmic fairness of state-of-the-art vision-language foundation models in\nchest X-ray diagnosis across five globally-sourced datasets. Our findings\nreveal that compared to board-certified radiologists, these foundation models\nconsistently underdiagnose marginalized groups, with even higher rates seen in\nintersectional subgroups, such as Black female patients. Such demographic\nbiases present over a wide range of pathologies and demographic attributes.\nFurther analysis of the model embedding uncovers its significant encoding of\ndemographic information. Deploying AI systems with these biases in medical\nimaging can intensify pre-existing care disparities, posing potential\nchallenges to equitable healthcare access and raising ethical questions about\ntheir clinical application.",
        "updated": "2024-02-22 18:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14815v1"
    },
    {
        "title": "WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition",
        "authors": "Lianghui ZhuJunwei ZhouYan LiuXin HaoWenyu LiuXinggang Wang",
        "links": "http://arxiv.org/abs/2402.14812v1",
        "entry_id": "http://arxiv.org/abs/2402.14812v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14812v1",
        "summary": "Weakly supervised visual recognition using inexact supervision is a critical\nyet challenging learning problem. It significantly reduces human labeling costs\nand traditionally relies on multi-instance learning and pseudo-labeling. This\npaper introduces WeakSAM and solves the weakly-supervised object detection\n(WSOD) and segmentation by utilizing the pre-learned world knowledge contained\nin a vision foundation model, i.e., the Segment Anything Model (SAM). WeakSAM\naddresses two critical limitations in traditional WSOD retraining, i.e., pseudo\nground truth (PGT) incompleteness and noisy PGT instances, through adaptive PGT\ngeneration and Region of Interest (RoI) drop regularization. It also addresses\nthe SAM's problems of requiring prompts and category unawareness for automatic\nobject detection and segmentation. Our results indicate that WeakSAM\nsignificantly surpasses previous state-of-the-art methods in WSOD and WSIS\nbenchmarks with large margins, i.e. average improvements of 7.4% and 8.5%,\nrespectively. The code is available at \\url{https://github.com/hustvl/WeakSAM}.",
        "updated": "2024-02-22 18:59:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14812v1"
    },
    {
        "title": "GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion",
        "authors": "Xueyi LiuLi Yi",
        "links": "http://arxiv.org/abs/2402.14810v1",
        "entry_id": "http://arxiv.org/abs/2402.14810v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14810v1",
        "summary": "In this work, we tackle the challenging problem of denoising hand-object\ninteractions (HOI). Given an erroneous interaction sequence, the objective is\nto refine the incorrect hand trajectory to remove interaction artifacts for a\nperceptually realistic sequence. This challenge involves intricate interaction\nnoise, including unnatural hand poses and incorrect hand-object relations,\nalongside the necessity for robust generalization to new interactions and\ndiverse noise patterns. We tackle those challenges through a novel approach,\nGeneOH Diffusion, incorporating two key designs: an innovative contact-centric\nHOI representation named GeneOH and a new domain-generalizable denoising\nscheme. The contact-centric representation GeneOH informatively parameterizes\nthe HOI process, facilitating enhanced generalization across various HOI\nscenarios. The new denoising scheme consists of a canonical denoising model\ntrained to project noisy data samples from a whitened noise space to a clean\ndata manifold and a \"denoising via diffusion\" strategy which can handle input\ntrajectories with various noise patterns by first diffusing them to align with\nthe whitened noise space and cleaning via the canonical denoiser. Extensive\nexperiments on four benchmarks with significant domain variations demonstrate\nthe superior effectiveness of our method. GeneOH Diffusion also shows promise\nfor various downstream applications. Project website:\nhttps://meowuu7.github.io/GeneOH-Diffusion/.",
        "updated": "2024-02-22 18:59:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14810v1"
    },
    {
        "title": "CriticBench: Benchmarking LLMs for Critique-Correct Reasoning",
        "authors": "Zicheng LinZhibin GouTian LiangRuilin LuoHaowei LiuYujiu Yang",
        "links": "http://arxiv.org/abs/2402.14809v1",
        "entry_id": "http://arxiv.org/abs/2402.14809v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14809v1",
        "summary": "The ability of Large Language Models (LLMs) to critique and refine their\nreasoning is crucial for their application in evaluation, feedback provision,\nand self-improvement. This paper introduces CriticBench, a comprehensive\nbenchmark designed to assess LLMs' abilities to critique and rectify their\nreasoning across a variety of tasks. CriticBench encompasses five reasoning\ndomains: mathematical, commonsense, symbolic, coding, and algorithmic. It\ncompiles 15 datasets and incorporates responses from three LLM families.\nUtilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in\ngeneration, critique, and correction reasoning, i.e., GQC reasoning. Our\nfindings reveal: (1) a linear relationship in GQC capabilities, with\ncritique-focused training markedly enhancing performance; (2) a task-dependent\nvariation in correction effectiveness, with logic-oriented tasks being more\namenable to correction; (3) GQC knowledge inconsistencies that decrease as\nmodel size increases; and (4) an intriguing inter-model critiquing dynamic,\nwhere stronger models are better at critiquing weaker ones, while weaker models\ncan surprisingly surpass stronger ones in their self-critique. We hope these\ninsights into the nuanced critique-correct reasoning of LLMs will foster\nfurther research in LLM critique and self-improvement.",
        "updated": "2024-02-22 18:59:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14809v1"
    },
    {
        "title": "A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health",
        "authors": "Nikhil BehariEdwin ZhangYunfan ZhaoAparna TanejaDheeraj NagarajMilind Tambe",
        "links": "http://arxiv.org/abs/2402.14807v1",
        "entry_id": "http://arxiv.org/abs/2402.14807v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14807v1",
        "summary": "Efforts to reduce maternal mortality rate, a key UN Sustainable Development\ntarget (SDG Target 3.1), rely largely on preventative care programs to spread\ncritical health information to high-risk populations. These programs face two\nimportant challenges: efficiently allocating limited health resources to large\nbeneficiary populations, and adapting to evolving policy priorities. While\nprior works in restless multi-armed bandit (RMAB) demonstrated success in\npublic health allocation tasks, they lack flexibility to adapt to evolving\npolicy priorities. Concurrently, Large Language Models (LLMs) have emerged as\nadept, automated planners in various domains, including robotic control and\nnavigation. In this paper, we propose DLM: a Decision Language Model for RMABs.\nTo enable dynamic fine-tuning of RMAB policies for challenging public health\nsettings using human-language commands, we propose using LLMs as automated\nplanners to (1) interpret human policy preference prompts, (2) propose code\nreward functions for a multi-agent RL environment for RMABs, and (3) iterate on\nthe generated reward using feedback from RMAB simulations to effectively adapt\npolicy outcomes. In collaboration with ARMMAN, an India-based public health\norganization promoting preventative care for pregnant mothers, we conduct a\nsimulation study, showing DLM can dynamically shape policy outcomes using only\nhuman language commands as input.",
        "updated": "2024-02-22 18:58:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14807v1"
    }
]