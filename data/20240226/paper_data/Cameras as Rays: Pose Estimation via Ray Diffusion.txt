PublishedasaconferencepaperatICLR2024
CAMERAS AS RAYS:
POSE ESTIMATION VIA RAY DIFFUSION
JasonY.Zhang,AmyLin ,MoneishKumar,
∗ ∗
Tzu-HsuanYang,DevaRamanan,ShubhamTulsiani
CarnegieMellonUniversity
ABSTRACT
Estimating camera poses is a fundamental task for 3D reconstruction and re-
mains challenging given sparsely sampled views (<10). In contrast to existing
approachesthatpursuetop-downpredictionofglobalparametrizationsofcamera
extrinsics, we propose a distributed representation of camera pose that treats a
camera as a bundle of rays. This representation allows for a tight coupling with
spatialimagefeaturesimprovingposeprecision. Weobservethatthisrepresenta-
tion is naturally suited for set-level transformers and develop a regression-based
approachthatmapsimagepatchestocorrespondingrays. Tocapturetheinherent
uncertaintiesinsparse-viewposeinference,weadaptthisapproachtolearnade-
noisingdiffusionmodelwhichallowsustosampleplausiblemodeswhileimprov-
ing performance. Our proposed methods, both regression- and diffusion-based,
demonstrate state-of-the-art performance on camera pose estimation on CO3D
whilegeneralizingtounseenobjectcategoriesandin-the-wildcaptures.
Images Ray Diffusion Timesteps Recovered
Cameras
Figure1: RecoveringSparse-viewCameraParametersbyDenoisingRays. Top: Givensparselysampled
images,ourapproachlearnstodenoisecamerarays(representedusingPlu¨ckercoordinates). Wethenrecover
cameraintrinsicsandextrinsicsfromthepositionsoftherays. Bottom: Wedemonstratethegeneralizationof
ourapproachforbothseen(teddybear)andunseenobjectcategories(couch,sandwich).
1 INTRODUCTION
We have witnessed rapid recent progress toward the goal of obtaining high-fidelity 3D represen-
tations given only a sparse set of input images (Zhang et al., 2021; Zhou & Tulsiani, 2023; Long
etal.,2022;Cerkezi&Favaro,2023). However,acrucialrequirementistheavailabilityofcamera
poses corresponding to the 2D input images. This is particularly challenging as structure-from-
motionmethodsfailtoreliablyinfercameraposesundersettingswithsparselysampledviews(also
referred to as sparse-view or wide-baseline in prior works). To fill this performance gap, recent
learning-basedapproacheshaveexaminedthetaskofpredictingcamerasgivenasparsesetofinput
images, and investigated regression (Jiang et al., 2022; Sinha et al., 2023), energy-based model-
ing(Zhangetal.,2022;Linetal.,2023)anddenoisingdiffusion(Wangetal.,2023)forinference.
∗denotesequalcontribution.ProjectPage:https://jasonyzhang.com/RayDiffusion.
1
couch 215_22688_47261 tensor([ 18, 171, 107, 98, 178, 183, 5, 146])
teddybear
sandwich 198_21285_41285 tensor([ 20, 40, 80, 90, 100, 120, 125])
4202
beF
22
]VC.sc[
1v71841.2042:viXraPublishedasaconferencepaperatICLR2024
However,whileexploringaplethoraoflearningtechniques,thesemethodshavelargelyside-stepped
acrucialquestion: whatrepresentationofcameraposesshouldlearning-basedmethodspredict?
At first, there may seem to be an obvious answer. After all, every student of projective geometry
is taught that (extrinsic) camera matrices are parameterized with a single rotation and a transla-
tion. Indeed,alltheabovementionedmethodsadaptthisrepresentation(albeitwithvaryingrotation
parametrizations e.g., matrices, quaternions, or angles) for predicting camera poses. However, we
argue that such a parsimonious global pose representation maybe suboptimal for neural learning,
which often benefits from over-parameterized distributed representations. From a geometric per-
spective,classicalbottom-upmethodsbenefitfromlow-levelcorrespondenceacrosspixels/patches,
whilelearning-basedmethodsthatpredictglobalcamerarepresentationsmaynoteasilybenefitfrom
such(implicitorexplicit)associations.
Inthiswork,weproposeanalternatecameraparametrizationthatrecaststhetaskofposeinference
as that of patch-wise ray prediction (Fig. 1). Instead of predicting a global rotation and global
translation for each input image, our model predicts a separate ray passing through each patch in
each input image. We show that this representation is naturally suited for transformer-based set-
to-set inference models that process sets of features extracted from image patches. To recover the
camera extrinsics (R, t) and intrinsics (K) corresponding to a classical perspective camera, we
optimizealeast-squareobjectivegiventhepredictedraybundle. Itisworthnotingthatthepredicted
ray bundle itself can be seen as an encoding of a generic camera as introduced in Grossberg &
Nayar(2001);Schopsetal.(2020),whichcancapturenon-perspectivecamerassuchascatadioptric
imagerswhoseraysmaynotevenintersectatacenterofprojection.
We first illustrate the effectiveness of our distributed ray representation by training a patch-based
transformer with a standard regression loss. We show that this already surpasses the performance
ofstate-of-the-artposepredictionmethodsthattendtobefarmorecomputeheavy(Linetal.,2023;
Sinhaetal.,2023;Wangetal.,2023). However,therearenaturalambiguitiesinthepredictedrays
duetosymmetriesandpartialobservations(Zhangetal.,2022;Wangetal.,2023). Weextendour
regression-basedmethodtoadenoisingdiffusion-basedprobabilisticmodelandfindthatthisfurther
improvestheperformanceandcanrecoverdistinctdistributionmodes.Wedemonstrateourapproach
ontheCO3Ddataset(Reizensteinetal.,2021)wherewesystematicallystudyperformanceacross
seencategoriesaswellasgeneralizationtounseenones. Moreover,wealsoshowthatourapproach
can generalize even to unseen datasets and present qualitative results on in-the-wild self-captures.
Insummary,ourcontributionsareasfollows:
• We recast the task of pose prediction as that of inferring per-patch ray equations as an
alternativetothepredominantapproachofinferringglobalcameraparametrizations.
• We present a simple regression-based approach for inferring this representation given
sparselysampledviewsandshoweventhissimpleapproachsurpassesthestate-of-the-art.
• Weextendthisapproachtocapturethedistributionovercamerasbylearningadenoising
diffusionmodeloverourraycameraparametrization,leadingtofurtherperformancegains.
2 RELATED WORK
2.1 STRUCTURE-FROM-MOTIONANDSLAM
Both Structure-from-motion and SLAM aim to recover camera poses and scene geometry from
a large set of unordered or ordered images. Classic SfM (Snavely et al., 2006) and indirect
SLAM (Mur-Artal et al., 2015; Mur-Artal & Tardo´s, 2017; Campos et al., 2021) methods gener-
ally rely on finding correspondences (Lucas & Kanade, 1981) between feature points (Bay et al.,
2006; Lowe, 2004) in overlapping images, which are then efficiently optimized (Scho¨nberger &
Frahm,2016;Scho¨nbergeretal.,2016)intocoherentposesusingBundleAdjustment(Triggsetal.,
1999). Subsequent works have improved the quality of features (DeTone et al., 2018), correspon-
dences(Shenetal.,2020;Yang&Ramanan,2019;Sarlinetal.,2020),andimprovedthebundlead-
justmentprocessitself(Tang&Tan,2019;Lindenbergeretal.,2021). Onthecontrary,ratherthan
minimize geometric reconstruction errors, indirect SLAM methods (Davison et al., 2007; Schops
etal.,2019)generallyoptimizephotometricerrors.
2t
Image Dirs. Moments
+ε +ε
t t
Images with Associated
Patch-wise Rays Features Noisy Rays Feats. + Denoised
Noisy Rays Rays
Cameras to Rays Rays to Cameras
(0, 0)
Identity Cam.
Camera C Ray Directions R
R Parameterized
Uniform by K, R, T
Image Grid
Predicted
Ray Directions
PublishedasaconferencepaperatICLR2024
(W, H) Solve for Optimal
Unproject Rays from Camera Camera Center Solve for Optimal Rotation
Directions p=d ×m Ray Bundle Rays to Camera RId ae yn Dti it ry e C cta iom n. s Camera
d
" " di
C KR ui
Moments
pi
Predicted
Solve for Optimal Ray Directions
m=p d Camera Center Solve for Optimal Rotation
×
K,R,t
di=R!K−1ui pi= −R!t
Figure2: ConvertingBetweenCameraandRayRepresentations. Werepresentcamerasasacollectionof
6-DPlu¨ckerraysconsistingofdirectionsandmoments. Weconvertthetraditionalrepresentationofcameras
totheraybundlerepresentationbyunprojectingraysfromthecameracentertopixelcoordinates. Weconvert
raysbacktothetraditionalcamerarepresentationbysolvingleast-squaresoptimizationsforthecameracenter,
intrinsicsmatrix,androtationmatrix.SeeSec.3.1formoredetails.
Whilethemethodsdescribedinthissectioncanachieve(sub)pixel-perfectaccuracy, theirreliance
ondenseandlargeimagesetsmakesthemunsuitableforsparse-viewposeestimation.
2.2 POSEESTIMATIONFROMSPARSELYSAMPLEDVIEWS
Estimatingposesfromsparselysampledimages(alsocalledsparse-vieworwide-baselineposees-
timation in prior work) is challenging as methods cannot rely on sufficient (or even any) overlap
between nearby images to rely on correspondences. The most extreme case of estimating sparse-
view poses is recovering the relative pose given 2 images. Recent works have explored how to
effectivelyregressrelativeposes(Balntasetal.,2018;Rockwelletal.,2022;Caietal.,2021)from
wide-baseline views. Other works have explored probabilistic approaches to model uncertainty
whenpredictingrelativepose(Zhangetal.,2022;Chenetal.,2021).
Most related to our approach are methods that can predict poses given multiple images. Rel-
Pose (Zhang et al., 2022) and RelPose++ (Lin et al., 2023) use energy-based models to compose
relative rotations into sets of camera poses. SparsePose (Sinha et al., 2023) learns to iteratively
refinesparsecameraposesgivenaninitialestimatewhileFORGE(Jiangetal.,2022)exploitssyn-
theticdatatolearncameraposes.MostcomparabletousisPoseDiffusion(Wangetal.,2023),which
alsousesadiffusionmodeltodenoisecameraposes. However,PoseDiffusiondenoisesthecamera
parametersdirectlywhereaswedenoisecamerarayswhichwedemonstratetobemoreprecise.
2.3 RAY-BASEDCAMERAPARAMETERIZATIONS
Somepriorworkincalibratinggenericcamerarepresentationshasmadeuseofray-basedrepresen-
tations of cameras, mainly for lenses such as fish-eyed for which the pinhole model is not a good
approximation(Kannala&Brandt,2006). Grossberg&Nayar(2001);Dunneetal.(2010)consider
themostgeneralcameramodel,whereeachpixelprojectionismodeledbyitsray. Evenwithbetter
calibrationalgorithms(Schopsetal.,2020),thelargenumberofparametersinthesecameramodels
makesthemdifficulttocalibrateaccurately. Whiletheseworksalsomakeuseofray-basedcamera
representations,theirfocusisoncalibration(intrinsics)andrequireknowncalibrationpatterns.
3 METHOD
Weaimtorecovercamerasfromasparseimageset I ,...,I . Ratherthanpredictglobalcam-
1 N
{ }
eraparametrizationsdirectlyasdoneinpreviouswork,weproposearay-basedrepresentationthat
can be seamlessly converted to and from the classic representation (Sec. 3.1). We then describe
a regression-based architecture for predicting ray-based cameras in Sec. 3.2. We build on this ar-
chitecture to introduce a probabilistic framework that estimates the rays using diffusion to handle
uncertaintiesandsymmetriesthatarisefromsparselysampledviewsinSec.3.3.
3.1 REPRESENTINGCAMERASWITHRAYS
Distributed Ray Representation. Typically, a camera is parameterized by its extrinsics (rotation
R SO(3), translation t R3) and intrinsics matrix K R3 3. While this parameterization
×
∈ ∈ ∈
3
noisuffiD
remrofsnarTPublishedasaconferencepaperatICLR2024
t
Image Dirs. Moments
+ε +ε
t t
Images with Associated
Patch-wise Rays Features Noisy Rays Feats. + Denoised
Noisy Rays Rays
Figure3:DenoisingRayDiffuserNetwork.Givenanoisyraycorrespondingtoanimagepatch,ourdenoising
raydiffusionmodCelapmreedraicst tsot hReadyesnoisedray.WeconcatenatespatialimageRfaeyast utore Csa(mOqeruaasbetal.,2023)with
noisyrays,representedwith6-dimensionalPlu¨ckercoordinates(Plu¨cker,1828)thatarevisualizedas3-channel
direction(m0, a0p)sand3-channelmomentmaps. Weuseatransformertojointlyprocessallimagepatchesand
Identity Cam.
associatednoisyraystopredicttheori Cg ai mn ea rl a denoisedray Cs. Ray Directions R
Parameterized
R
Uniform by K, R, T
Image Grid
compactlyrelatestherelationshipofworldcoordinatestopixelcoordinatesusingPrecdaicmtederaprojection
Ray Directions
(u = K[R T]x),wehypothesizethatitmaybedifficultforaneuralnetworktodirectlyregress
thislow-dim| ensionalrepres(W en, H ta) tion. Instead,inSsoplviree fdor bOyptigmeanl eralizedcameramodels(Grossberg&
Nayar,2001U;nSpcrohjeocpt Rsaeyts afrlo.,m2 C0a2m0e)rausedforcalibraC ta iom ner ,a w Ce en pte rr oposetoover-parS ao mlve e f to er r O izp etim aa cl aR mota et rio an as
acollectionofrays:
= r ,...,r , (1)
1 m
R { }
whereeachrayr R6 isassociatedwithaknownpixelcoordinateu . Weparameterizeeachray
i i
r 18tD 2rai 8rve )ce :tliionngs in pd =i dr ×ec m∈ tiondRa ∈y BR u3 ndtlheroughanypoiRnatyps to
∈
CR am3eurasingtheP RIld aue y¨n c Dtik it ry
e
eC ctra iomr n.e
s
presentation(Plu¨ckCearm,era
d
" " d i r = Cd,m R6, KR (2)
wh Mer oe mm ents= p ×d ∈p iR3 is the moment
vec⟨
tor,
an⟩ d∈
importantly P, redi is ctea dgnostic to the specific point
ui
ontherayusedtocomputeit. WhendisofuSnoilvtel efonr gOtphti,mtahl enorRmay Doirfecttihones momentmrepresentsthe
distancefrommt=hperdaytotheorigin. Camera Center Solve for Optimal Rotation
×
K,R,t
ConvertingfromCameratoRayBundle.Givedni=aRk!nKo−w1unicamerapain=d −Ra!stetof2Dpixelcoordinates
u ,thedirectionsdcanbecomputedbyunprojectingraysfromthepixelcoordinates,andthe
i m
{ }
momentsmcanbecomputedbytreatingthecameracenterasthepointpsinceallraysintersectat
thecameracenter:
d=R K 1u, m=( R t) d. (3)
⊤ − ⊤
− ×
In practice, we select the points u by uniformly sampling points on a grid across the image
i m
{ }
or image crop, as shown in Fig. 2. This allows us to associate each patch in the image with a
ray passing through the center of the patch, which we will use later to design a patch- and ray-
conditionedarchitecture.
ConvertingfromRayBundletoCamera. Givenacollectionofrays = r associatedwith
i m
R { }
2D pixels u , we show that one can recover the camera extrinsics and intrinsics. We start by
i m
{ }
solvingforthecameracentercbyfindingthe3Dworldcoordinateclosesttotheintersectionofall
raysin :
R (cid:88)
c=argmin p d m 2. (4)
p R3 ∥ × − ∥
∈ d,m
⟨ ⟩∈R
To solve for the rotation R (and intrinsics K) for each camera, we can solve for the optimal ho-
mographymatrixP thattransformsper-pixelraydirectionsfromthepredictedonestothoseofan
‘identity’camera(K =I andR=I):
m
(cid:88)
P =argmin Hd u . (5)
i i
∥ × ∥
∥H ∥=1 i=1
ThematrixP canbecomputedviaDLT(Abdel-Azizetal.,2015)andcanallowrecoveringRusing
RQ-decomposition as K is an upper-triangular matrix and R is orthonormal. Once the camera
rotationRandcameracentercarerecovered,thetranslationtcanbecomputedast= R c.
⊤
−
4
noisuffiD
remrofsnarTPublishedasaconferencepaperatICLR2024
Predicted Cameras
Input Images
Figure4: VisualizingtheDenoisingProcessUsingOurRayDiffuser. Giventhe2imagesofthesuitcase
(BottomRight),wevisualizethedenoisingprocessstartingfromrandomlyinitializedcamerarays.Wevisualize
the noisy rays using the Plu¨cker representation (ray directions and moments) in the bottom row and their
corresponding3Dpositionsinthetoprow. Intherightmostcolumn,werecoverthepredictedcameras(green)
andcomparethemtothegroundtruthcameras(black).
3.2 POSEESTIMATIONVIARAYREGRESSION
WenowdescribeanapproachforpredictingtherayrepresentationoutlinedinSec.3.1forcamera
pose estimation given N images I ,...,I . Given ground truth camera parameters, we can
1 N
{ }
compute the ground truth ray bundles ,..., . As shown in Fig. 2, we compute the rays
1 N
overauniformp pgridovertheimage{ sR uchthatR each} raybundleconsistsofm=p2rays(eq.(1)).
×
Toensureacorrespondencebetweenraysandimagepatches,weuseaspatialimagefeatureextractor
andtreateachpatchfeatureasatoken:
f (I)=f Rp p d. (6)
feat × ×
∈
Tomakeuseofthecropparameters,wealsoconcatenatethepixelcoordinateu(innormalizedde-
vicecoordinateswithrespecttotheuncroppedimage)toeachspatialfeature. Weuseatransformer-
basedarchitecture(Dosovitskiyetal.(2021);Peebles&Xie(2023))thatjointlyprocesseseachof
thep2tokensfromN images,andpredictstheraycorrespondingtoeachpatch:
{Rˆ }N
i=1
=f Regress(cid:16)(cid:8) f i,u i(cid:9)N i=· 1p2(cid:17) . (7)
Wetrainthenetworkbycomputingareconstructionlossonthepredictedcamerarays:
(cid:88)N (cid:13) (cid:13)2
= (cid:13)ˆ (cid:13) . (8)
recon (cid:13) i i(cid:13)
L R −R 2
i=1
3.3 POSEESTIMATIONVIADENOISINGRAYDIFFUSION
Whilethepatchwiseregression-basedarchitecturedescribedinSec.3.2caneffectivelypredictour
distributedray-basedparametrization,thetaskofpredictingposes(intheformofrays)maystillbe
ambiguousgivensparseviews. Tohandleinherentuncertaintyinthepredictions(duetosymmetries
andpartialobservations),weextendthepreviouslydescribedregressionapproachtoinsteadlearna
diffusion-basedprobabilisticmodeloverourdistributedrayrepresentation.
Denoisingdiffusionmodels(Hoetal.,2020)approximateadatalikelihoodfunctionbyinvertinga
noisingprocessthataddstime-dependentGaussiannoisetotheoriginalsamplex :
0
x =√α¯ x +√1 α¯ ϵ, (9)
t t 0 t
−
where ϵ (0,I) and α is a hyper-parameter schedule of noise weights such that x can be
t T
∼ N
approximated as a standard Gaussian distribution. To learn the reverse process, one can train a
denoisingnetworkf topredictthedenoisedsamplex conditionedonx :
θ 0 t
(θ)=E x f (x ,t) 2. (10)
L
t,x0,ϵ
∥
0
−
θ t
∥
5
+
.sriD
yaR
D3
ni
syaR
stnemoMPublishedasaconferencepaperatICLR2024
Images Pose Diffusion RelPose++ Ray Regression (Ours) Ray Diffusion (Ours)
Figure5:QualitativeComparisonBetweenPredictedCameraPoses.Wecomparetheresultsofourregres-
sionanddiffusionapproacheswithPoseDiffusionandRelPose++.Groundtruth(black)cameratrajectoriesare
alignedtothepredicted(colored)cameratrajectoriesbyperformingProcrustesoptimalalignmentonthecam-
eracenters.Thetoptwoexamplesarefromseencategories,andthebottomtwoarefromheldoutcategories.
Images Predicted Cameras Images Predicted Cameras Images Predicted Cameras
Figure6: GeneralizationtoIn-the-wildSelf-captures. Wetestthegeneralizationofourraydiffusionmodel
onavarietyofself-captureddataonobjectsthatarenotinCO3D.
We instantiate this denoising diffusion framework to model the distributions over patchwise rays
conditionedontheinputimages. Wedothisbysimplymodifyingourrayregressionnetworkfrom
Sec.3.2tobeadditionallyconditionedonnoisyrays(concatenatedwithpatchwisefeaturesandpixel
coordinates)andapositionallyencoded(Vaswanietal.,2017)timeembeddingt:
{Rˆ }N
i=1
=f Diffuse(cid:16)(cid:8) (f i,u i,r i,t)(cid:9)N i=· 1p2 ,t(cid:17) , (11)
wherethenoisyraysr canbecomputedas:
i,t
r =√α¯ r +√1 α¯ ϵ. (12)
i,t t i t
−
Conveniently, our time-conditioned ray denoiser can be trained with the same L2 loss func-
tion (eq. (8)) as our ray regressor. We visualize the states of the denoised rays during backward
diffusioninFig.4.
3.4 IMPLEMENTATIONDETAILS
FollowingLinetal.(2023),weplacetheworldoriginatthepointclosesttotheopticalaxesofthe
trainingcameras,whichrepresentsausefulinductivebiasforcenter-facingcamerasetups.Tohandle
6PublishedasaconferencepaperatICLR2024
coordinatesystemambiguity,werotatetheworldcoordinatessuchthatthefirstcameraalwayshas
identityrotationandre-scalethescenesuchthatthefirstcameratranslationhasunitnorm.Following
priorwork(Zhangetal.,2022),wetakesquareimagecropstightlyaroundtheobjectboundingbox
andadjusttheuniformgridofpixelcoordinatesassociatedwiththeraysaccordingly.
We use a pre-trained, frozen DINOv2 (S/14) (Oquab et al., 2023) as our image feature extractor.
WeuseaDiT(Peebles&Xie,2023)with16transformerblocksasthearchitectureforbothf
Regress
(withtalwayssetto100)andf . WetrainourdiffusionmodelwithT=100timesteps. When
Diffusion
training our denoiser, we add noise to the direction and moment representation of rays. The ray
regressionandraydiffusionmodelstakeabout2and4daysrespectivelytotrainon8A6000GPUs.
To predict cameras with our ray denoiser, we use DDPM (Ho et al., 2020) inference with slight
modifications. Empirically, we found that removing the stochastic noise in DDPM inference and
stoppingthebackwarddiffusionprocessearly(andusingthepredictedx astheestimate)yielded
0
improved performance. We hypothesize that this is because while the earlier diffusion steps help
selectamongdistinctplausiblemodes,thelaterstepsyieldsamplesaroundthese—andthismaybe
detrimentaltoaccuracymetricsthatpreferdistributionmodes.
4 EXPERIMENTS
4.1 EXPERIMENTALSETUP
Dataset.OurmethodistrainedandevaluatedusingCO3Dv2(Reizensteinetal.,2021).Thisdataset
containsturntablevideosspanning51categoriesofhouseholdobjects. Eachframeislabeledwith
posesdeterminedbyCOLMAP(Scho¨nbergeretal.,2016;Scho¨nberger&Frahm,2016). Following
Zhangetal.(2022),wetrainon41categoriesandholdouttheremaining10categoriesforevaluating
generalization.
Baselines. Weevaluateourmethodagainstahandfuloflearning-basedandcorrespondence-based
poseestimationworks.
COLMAP(Scho¨nbergeretal.,2016;Scho¨nberger&Frahm,2016). COLMAPisastandarddense
correspondence-based SfM pipeline. We use an implementation (Sarlin et al., 2019) which uses
SuperPointfeatures(DeToneetal.,2018)andSuperGluematching(Sarlinetal.,2020).
RelPose(Zhangetal.,2022). RelPosepredictsrelativerotationsbetweenpairsofcamerasandde-
finesevaluationprocedurestooptimizeoveralearnedscoringfunctionanddeterminejointrotations.
RelPose++ (Lin et al., 2023). RelPose++ builds upon the pairwise scoring network of RelPose to
incorporatemulti-viewreasoningviaatransformerandalsoallowspredictingcameratranslations.
R+TRegression(Linetal.,2023). Totesttheimportanceofmodelinguncertainty,Linetal.(2023)
trainsabaselinethatdirectlyregressesposes. WereportthenumbersfromLinetal.(2023).
PoseDiffusion(Wangetal.,2023). PoseDiffusionreformulatestheposeestimationtaskasdirectly
diffusingcameraextrinsicsandfocallength. Additionally,theyintroduceageometry-guidedsam-
plingerrortoenforceepipolarconstraintsonpredictedposes. WeevaluatePoseDiffusionwithand
withoutthegeometry-guidedsampling.
4.2 METRICS
We evaluate sparse image sets of 2 to 8 images for each test sequence in CO3D. For an N image
evaluation, we randomly sample N images and compute the accuracy of the predicted poses. We
averagetheseaccuraciesover5samplesforeachsequencetoreducestochasticity.
Rotation Accuracy. We first compute the relative rotations between each pair of cameras for both
predictedandgroundtruthposes. Thenwedeterminetheerrorbetweenthegroundtruthandpre-
dictedpairwiserelativerotationsandreporttheproportionoftheseerrorswithin15degrees.
CameraCenterAccuracy.WealignthegroundtruthandpredictedposesinCO3Dusingtheoptimal
similaritytransform(s,R,t). Wecompareourpredictiontothescenescale(thedistancefromthe
scene centroid to the farthest camera, following Sinha et al. (2023)). We report the proportion of
alignedcameracenterswithin10percentofthescenescaletothegroundtruth.
7PublishedasaconferencepaperatICLR2024
#ofImages 2 3 4 5 6 7 8
COLMAP(SP+SG)(Sarlinetal.,2019) 30.7 28.4 26.5 26.8 27.0 28.1 30.6
RelPose(Zhangetal.,2022) 56.0 56.5 57.0 57.2 57.2 57.3 57.2
PoseDiffusionw/oGGS(Wangetal.,2023) 74.5 75.4 75.6 75.7 76.0 76.3 76.5
PoseDiffusion(Wangetal.,2023) 75.7 76.4 76.8 77.4 78.0 78.7 78.8
RelPose++(Linetal.,2023) 81.8 82.8 84.1 84.7 84.9 85.3 85.5
R+TRegression(Linetal.,2023) 49.1 50.7 53.0 54.6 55.7 56.1 56.5
RayRegression(Ours) 88.8 88.7 88.7 89.0 89.4 89.3 89.2
RayDiffusion(Ours) 91.8 92.4 92.6 92.9 93.1 93.3 93.3
COLMAP(SP+SG)(Sarlinetal.,2019) 34.5 31.8 31.0 31.7 32.7 35.0 38.5
RelPose(Zhangetal.,2022) 48.6 47.5 48.1 48.3 48.4 48.4 48.3
PoseDiffusionw/oGGS(Wangetal.,2023) 62.1 62.4 63.0 63.5 64.2 64.2 64.4
PoseDiffusion(Wangetal.,2023) 63.2 64.2 64.2 65.7 66.2 67.0 67.7
RelPose++(Linetal.,2023) 69.8 71.1 71.9 72.8 73.8 74.4 74.9
R+TRegression(Linetal.,2023) 42.7 43.8 46.3 47.7 48.4 48.9 48.9
RayRegression(Ours) 79.0 79.6 80.6 81.4 81.3 81.9 81.9
RayDiffusion(Ours) 83.5 85.6 86.3 86.9 87.2 87.5 88.1
Table1: CameraRotationAccuracyonCO3D(@15◦). Herewereporttheproportionofrelativecamera
rotationsthatarewithin15degreesofthegroundtruth.
#ofImages 2 3 4 5 6 7 8
COLMAP(SP+SG)(Sarlinetal.,2019) 100 34.5 23.8 18.9 15.6 14.5 15.0
PoseDiffusionw/oGGS(Wangetal.,2023) 100 76.5 66.9 62.4 59.4 58.0 56.5
PoseDiffusion(Wangetal.,2023) 100 77.5 69.7 65.9 63.7 62.8 61.9
RelPose++(Linetal.,2023) 100 85.0 78.0 74.2 71.9 70.3 68.8
R+TRegression(Linetal.,2023) 100 58.3 41.6 35.9 32.7 31.0 30.0
RayRegression(Ours) 100 91.7 85.7 82.1 79.8 77.9 76.2
RayDiffusion(Ours) 100 94.2 90.5 87.8 86.2 85.0 84.1
COLMAP(SP+SG)(Sarlinetal.,2019) 100 36.0 25.5 20.0 17.9 17.6 19.1
PoseDiffusionw/oGGS(Wangetal.,2023) 100 62.5 48.8 41.9 39.0 36.5 34.8
PoseDiffusion(Wangetal.,2023) 100 63.6 50.5 45.7 43.0 41.2 39.9
RelPose++(Linetal.,2023) 100 70.6 58.8 53.4 50.4 47.8 46.6
R+TRegression(Linetal.,2023) 100 48.9 32.6 25.9 23.7 22.4 21.3
RayRegression(Ours) 100 83.7 75.6 70.8 67.4 65.3 63.9
RayDiffusion(Ours) 100 87.7 81.1 77.0 74.1 72.4 71.4
Table2: CameraCenterAccuracyonCO3D(@0.1). Herewereporttheproportionofcameracentersthat
arewithin0.1ofthescenescale. Weapplyanoptimalsimilaritytransform(s,R,t)toalignpredictedcamera
centerstogroundtruthcameracenters(hencethealignmentisperfectatN =2butworsenswithmoreimages).
4.3 EVALUATION
WereportthecamerarotationaccuracyinTab.1andcameracenteraccuracyinTab.2evaluatedon
CO3D.WefindthatCOLMAPstrugglesinwide-baselinesettingsduetoinsufficientimageoverlap
tofindcorrespondences.Wefindthatboththeregressionanddiffusionversionsofourmethodsafely
outperformallexistingapproaches,suggestingthatourray-basedrepresentationcaneffectivelyre-
cover precise camera poses in this setup. In particular, our ray regression method significantly
outperformsthebaselinethatregressesextrinsicsRandTdirectly(R+TRegression). Similarly,our
ray diffusion model demonstrates a large improvement over R+T Diffusion (PoseDiffusion with-
out GGS) (Wang et al., 2023), while also outperforming their full method (PoseDiffusion) which
includesgeometry-guidedsampling.
WeshowqualitativeresultscomparingbothourRayRegressionandDiffusionmethodswithPoseD-
iffusion and RelPose++ in Fig. 5. We find that our ray-based representation consistently achieves
finerlocalization. Additionally, raydiffusionachievesslightlybetterperformancethanrayregres-
sion. More importantly, it also allows recovering multiple plausible modes under uncertainty, as
highlightedinFig.7.
8
seirogetaCneeS
seirogetaCneesnU
seirogetaCneeS
.sgetaCneesnUPublishedasaconferencepaperatICLR2024
Images Regression Diffusion (100 Samples)
Figure7: ModelingUncertaintyViaSamplingModes. Sparse-viewcameraposesaresometimesinherently
ambiguousduetosymmetry. ThecapacitytomodelsuchuncertaintyinprobabilisticmodelssuchasourRay
Diffusionmodelisasignificantadvantageoverregression-basedmodelsthatmustcommittoasinglemode.
We thus investigate taking multiple samples from our diffusion model. We visualize the predicted cameras
(colored)ofbothourregression-anddiffusion-basedapproachescomparedtothegroundtruth(black). While
the regression model predicts the green camera incorrectly, we can recover better modes by sampling our
diffusionmodelmultipletimes.
#ofRays Rot@15 CC@0.01
AblatingRayResolution. Weconductanablationstudyto 2×2 52.5 72.5
evaluatehowthenumberofcameraraysaffectsperformance 4×4 70.3 82.6
inTab.3. Wefindthatincreasingthenumberofcamerarays 8×8 76.1 84.8
significantly improves performance. Note that we kept the 16×16 84.0 89.8
parameter count of the transformer constant, but more to-
kens incur a greater computational cost. All other experi- Table3: RayResolutionAblation. We
mentsareconductedwith16 16rays. evaluatevariousnumbersofpatches/rays
× bytrainingacategory-specificmodelfor
Demonstration on Self-captures. Finally, to demonstrate 2 different training categories (hydrant,
thatourapproachgeneralizesbeyondthedistributionofse- wineglass) with N = 3 images. Per-
quencesfromCO3D,weshowqualitativeresultsusingRay formanceacrossthe2categoriesisaver-
Diffusiononavarietyofin-the-wildself-capturesinFig.6. aged. We find that increasing the num-
berofrayssignificantlyimprovesperfor-
mance. However,wefoundthatincreas-
5 DISCUSSION ing the number of rays beyond 16×16
wascomputationallyprohibitive.
Inthiswork,weexploredrepresentingcameraposesusinga
distributedrayrepresentation,andproposedadeterministicregressionandaprobabilisticdiffusion
approachforpredictingtheserays. Whileweexaminedthisrepresentationinthecontextofsparse
views,itcanbeexploredforsingle-viewordensemulti-viewsetups.Inaddition,whileourrepresen-
tationallowsimplicitlyleveragingassociationsbetweenpatches, wedonotenforceanygeometric
consistency(asdoneinclassicalposeestimationpipelines). Itwouldbeinterestingtoexplorejoint
inferenceofourdistributedrayrepresentationandgeometryinfuturework.
Acknowledgements. We would like to thank Shikhar Bahl and Samarth Sinha for their feedback
ondrafts. ThisworkwassupportedinpartbytheNSFGFRP(GrantNo. DGE1745016),aCISCO
gift award, and the Intelligence Advanced Research Projects Activity (IARPA) via Department of
Interior/InteriorBusinessCenter(DOI/IBC)contractnumber140D0423C0074. TheU.S.Govern-
mentisauthorizedtoreproduceanddistributereprintsforGovernmentalpurposesnotwithstanding
any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are
thoseoftheauthorsandshouldnotbeinterpretedasnecessarilyrepresentingtheofficialpoliciesor
endorsements,eitherexpressedorimplied,ofIARPA,DOI/IBC,ortheU.S.Government.
9PublishedasaconferencepaperatICLR2024
REFERENCES
Yousset I Abdel-Aziz, Hauck Michael Karara, and Michael Hauck. Direct linear transformation
fromcomparatorcoordinatesintoobjectspacecoordinatesinclose-rangephotogrammetry. Pho-
togrammetricengineering&remotesensing,81(2):103–107,2015. 4
VassileiosBalntas,ShudaLi,andVictorPrisacariu. RelocNet: ContinuousMetricLearningRelo-
calisationusingNeuralNets. InECCV,2018. 3
HerbertBay,TinneTuytelaars,andLucVanGool. SURF:SpeededUpRobustFeatures. InECCV,
2006. 2
Ruojin Cai, Bharath Hariharan, Noah Snavely, and Hadar Averbuch-Elor. Extreme Rotation Esti-
mationusingDenseCorrelationVolumes. InCVPR,2021. 3
Carlos Campos, Richard Elvira, Juan J. Go´mez, Jose´ M. M. Montiel, and Juan D. Tardo´s. ORB-
SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM.
T-RO,2021. 2
LlukmanCerkeziandPaoloFavaro. Sparse3dreconstructionviaobject-centricraysampling. arXiv
preprintarXiv:2309.03008,2023. 1
KefanChen,NoahSnavely,andAmeeshMakadia.Wide-BaselineRelativeCameraPoseEstimation
withDirectionalLearning. InCVPR,2021. 3
Andrew J Davison, Ian D Reid, Nicholas D Molton, and Olivier Stasse. MonoSLAM: Real-time
SingleCameraSLAM. TPAMI,2007. 2
DanielDeTone,TomaszMalisiewicz,andAndrewRabinovich.SuperPoint:Self-supervisedInterest
PointDetectionandDescription. InCVPR-W,2018. 2,7
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
ImageisWorth16x16Words: TransformersforImageRecognitionatScale. InICLR,2021. 5
Aubrey K Dunne, John Mallon, and Paul F Whelan. Efficient Generic Calibration Method for
GeneralCameraswithSingleCentreofProjection. ComputerVisionandImageUnderstanding,
114(2):220–233,2010. 3
Michael D Grossberg and Shree K Nayar. A general imaging model and a method for finding its
parameters. In Proceedings Eighth IEEE International Conference on Computer Vision. ICCV
2001,volume2,pp.108–115.IEEE,2001. 2,3,4
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. NeurIPS,
2020. 5,7
HanwenJiang, ZhenyuJiang, KristenGrauman, andYukeZhu. Few-ViewObjectReconstruction
withUnknownCategoriesandCameraPoses. ArXiv,2212.04492,2022. 1,3
JuhoKannalaandSamiSBrandt.Agenericcameramodelandcalibrationmethodforconventional,
wide-angle,andfish-eyelenses. TPAMI,28(8):1335–1340,2006. 3
ZhengqiLiandNoahSnavely. MegaDepth: LearningSingle-ViewDepthPredictionfromInternet
Photos. InCVPR,2018. 13,14
AmyLin,JasonYZhang,DevaRamanan,andShubhamTulsiani.RelPose++:Recovering6DPoses
fromSparse-viewObservations. arXivpreprintarXiv:2305.04926,2023. 1,2,3,6,7,8,15,16
Philipp Lindenberger, Paul-Edouard Sarlin, Viktor Larsson, and Marc Pollefeys. Pixel-Perfect
Structure-from-MotionwithFeaturemetricRefinement. InICCV,2021. 2
Xiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and Wenping Wang. SparseNeuS: Fast
GeneralizableNeuralSurfaceReconstructionfromSparseViews. InECCV,2022. 1
DavidGLowe. DistinctiveImageFeaturesfromScale-invariantKeypoints. IJCV,2004. 2
10PublishedasaconferencepaperatICLR2024
BruceDLucasandTakeoKanade. AnIterativeImageRegistrationTechniquewithanApplication
toStereoVision. InIJCAI,1981. 2
Rau´lMur-ArtalandJuanD.Tardo´s.ORB-SLAM2:AnOpen-SourceSLAMSystemforMonocular,
StereoandRGB-DCameras. T-RO,2017. 2
Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D Tardos. ORB-SLAM: A Versatile and
AccurateMonocularSLAMSystem. T-RO,2015. 2
MaximeOquab,Timothe´eDarcet,TheoMoutakanni,HuyV.Vo,MarcSzafraniec,VasilKhalidov,
PierreFernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,RussellHowes,Po-Yao
Huang,HuXu,VasuSharma,Shang-WenLi,WojciechGaluba,MikeRabbat,MidoAssran,Nico-
lasBallas,GabrielSynnaeve,IshanMisra,HerveJegou,JulienMairal,PatrickLabatut,Armand
Joulin, and Piotr Bojanowski. DINOv2: Learning Robust Visual Features without Supervision.
arXivpreprintarXiv:2304.07193,2023. 4,7
WilliamPeeblesandSainingXie. ScalableDiffusionModelswithTransformers. InICCV,2023. 5,
7
JuliusPlu¨cker. Analytisch-geometrischeEntwicklungen,volume2. GDBaedeker,1828. 4
Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and
David Novotny. Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D
CategoryReconstruction. InICCV,2021. 2,7
ChrisRockwell,JustinJohnson,andDavidFFouhey. The8-PointAlgorithmasanInductiveBias
forRelativePosePredictionbyViTs. In3DV,2022. 3
Paul-EdouardSarlin,CesarCadena,RolandSiegwart,andMarcinDymczyk. FromCoarsetoFine:
RobustHierarchicalLocalizationatLargeScale. InCVPR,2019. 7,8,15,16
Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. SuperGlue:
LearningFeatureMatchingwithGraphNeuralNetworks. InCVPR,2020. 2,7
JohannesLutz Scho¨nbergerand Jan-MichaelFrahm. Structure-from-MotionRevisited. In CVPR,
2016. 2,7
Johannes Lutz Scho¨nberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise
ViewSelectionforUnstructuredMulti-ViewStereo. InECCV,2016. 2,7
ThomasSchops,TorstenSattler,andMarcPollefeys. BADSLAM:BundleAdjustedDirectRGB-D
SLAM. InCVPR,2019. 2
ThomasSchops,ViktorLarsson,MarcPollefeys,andTorstenSattler. WhyHaving10,000Parame-
tersinYourCameraModelisBetterThanTwelve. InCVPR,2020. 2,3,4
Xi Shen, Franc¸ois Darmon, Alexei A Efros, and Mathieu Aubry. RANSAC-Flow: Generic Two-
stageImageAlignment. InECCV,2020. 2
Samarth Sinha, Jason Y Zhang, Andrea Tagliasacchi, Igor Gilitschenski, and David B Lindell.
SparsePose: Sparse-View Camera Pose Regression and Refinement. In CVPR, 2023. 1, 2, 3,
7
NoahSnavely,StevenMSeitz,andRichardSzeliski. PhotoTourism: ExploringPhotoCollections
in3D. InSIGGRAPH.ACM,2006. 2
ChengzhouTangandPingTan. BA-Net: DenseBundleAdjustmentNetwork. InICLR,2019. 2
Bill Triggs, Philip F McLauchlan, Richard I Hartley, and Andrew W Fitzgibbon. Bundle
Adjustment—AModernSynthesis. InInternationalworkshoponvisionalgorithms,1999. 2
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
ŁukaszKaiser,andIlliaPolosukhin. AttentionisAllYouNeed. NeurIPS,2017. 6
11PublishedasaconferencepaperatICLR2024
JianyuanWang,ChristianRupprecht,andDavidNovotny. PoseDiffusion: SolvingPoseEstimation
viaDiffusion-aidedBundleAdjustment. InICCV,2023. 1,2,3,7,8,15,16
Gengshan Yang and Deva Ramanan. Volumetric Correspondence Networks for Optical Flow.
NeurIPS,32,2019. 2
JasonY.Zhang,GengshanYang,ShubhamTulsiani,andDevaRamanan.NeRS:NeuralReflectance
SurfacesforSparse-view3DReconstructionintheWild. InNeurIPS,2021. 1
JasonY.Zhang,DevaRamanan,andShubhamTulsiani. RelPose: PredictingProbabilisticRelative
RotationforSingleObjectsintheWild. InECCV,2022. 1,2,3,7,8,15
TinghuiZhou,RichardTucker,JohnFlynn,GrahamFyffe,andNoahSnavely.Stereomagnification:
Learningviewsynthesisusingmultiplaneimages. SIGGRAPH,37,2018. 13,15
ZhizhuoZhouandShubhamTulsiani. SparseFusion: DistillingView-conditionedDiffusionfor3D
Reconstruction. InCVPR,2023. 1
12PublishedasaconferencepaperatICLR2024
A APPENDIX
Inthissection,weincludethefollowing:
• Additional qualitative comparisons on both seen (Fig. 8) and unseen (Fig. 9) object cate-
gories.
• EvaluationsonCO3Datmultiplethresholds(Tabs.10and11).
• EvaluationsonCO3DusingArea-under-Curve(AUC)toaccountforallthresholds(Tabs.2
and8andFig.11).
• Evaluationofinferencetimeofallmethods(Tab.6).
• Benchmarkofmemoryusageofourdiffusionmodel(Tab.7).
• Generalization of our method on up to 43 images (trained on 8 images) in Tab. 5 and to
RealEstate10K(Zhouetal.,2018)(trainedonCO3D).
• QualitativeresultsontrainingonSfM-styledatasets(MegaDepth(Li&Snavely,2018))in
Fig.10.
Images Pose Diffusion RelPose++ Ray Regression (Ours) Ray Diffusion (Ours)
Figure8:AdditionalQualitativeResultsforSeenCategories.
13PublishedasaconferencepaperatICLR2024
Images Pose Diffusion RelPose++ Ray Regression (Ours) Ray Diffusion (Ours)
Figure9:AdditionalQualitativeResultsforUnseenCategories.
Input Images COLMAP (SP+SG) Ray Diffusion (Ours)
Did Not
Converge
Figure 10: Qualitative Results on MegaDepth Dataset (Li & Snavely, 2018). As a proof of concept on
scene-leveldatasets,wetrainourraydiffusionmodelon235oftheSfMreconstructionsfromMegaDepth. At
trainingtime,wenormalizethescenesuchthatthefirstcamerahasidentityrotationandzerotranslationand
rescalethescenesuchthatthestandarddeviationinthegroundtruthraymomentsisconstant.Herewevisualize
predictedcameraposesincolorandgroundtruthcamerasinblackforheld-outscenesfromthedataset. The
colorofthecamera(andcolorofthecircleforgroundtruthcameras)indicatethecorrespondencewiththeinput
image. WecompareourmethodwithCOLMAP(withSuperPoint+SuperGlue)whichishighlyaccuratewhen
itconverges.OurmethodismorerobustbutlessaccuratewhenCOLMAPdoesconverge.
14PublishedasaconferencepaperatICLR2024
#ofImages 2 3 4 5 6 7 8
ConstantRot. 84.0 83.8 83.9 84.0 84.0 84.0 83.9
PoseDiffusion 77.6 77.9 78.4 78.7 78.9 79.3 79.0
RelPose++ 83.8 85.1 85.8 86.4 86.5 86.7 86.8
RayRegression(Ours) 90.8 90.0 89.9 89.7 89.5 89.5 89.5
RayDiffusion(Ours) 90.9 89.9 89.5 89.3 89.1 88.8 88.3
PoseDiffusion 100 77.7 65.9 60.1 55.0 52.2 50.2
RelPose++ 100 71.2 60.6 54.0 49.4 47.1 45.5
RayRegression(Ours) 100 74.4 62.0 56.0 51.3 49.2 47.1
RayDiffusion(Ours) 100 79.7 68.6 62.2 57.8 54.9 52.1
Table 4: Evaluation of Rotation and Camera Center Accuracy on RealEstate10K (Zhou et al., 2018).
Here we report zero-shot generalization of methods trained on CO3D and tested on RealEstate10K without
anyfine-tuning. Wemeasurerotationaccuracyatathresholdof15degreesandcameracenteraccuracyata
thresholdof0.2. Theconstantrotationbaselinealwayspredictsanidentityrotation. Wefindthatthisdataset
hasastrong,forward-facingbias,soevennaivelypredictinganidentityrotationperformswell.
#ofImages 8 15 22 29 36 43
RotationAcc.(SeenCategories) 93.3 93.1 93.3 93.1 93.4 93.4
RotationAcc.(UnseenCategories) 88.1 88.2 89.2 88.7 89.0 88.9
CameraCenterAcc.(SeenCategories) 84.1 78.3 76.5 75.3 74.7 74.2
CameraCenterAcc.(UnseenCategories) 71.4 62.7 61.1 59.3 59.2 58.9
Table5:GeneralizationtoMoreImagesonCO3DusingRayDiffusion. Ourraydiffusionmodelistrained
withbetween2and8images. Atinferencetime,wefindthatwecaneffectivelyrunbackwarddiffusionwith
more images by randomly sampling new mini-batches at each iteration of DDPM (keeping the first image
fixed).
InferenceTime(s)
COLMAP(SP+SG)(Sarlinetal.,2019) 2.06
RelPose(Zhangetal.,2022) 29.5
PoseDiffusionw/oGGS(Wangetal.,2023) 0.304
PoseDiffusion(Wangetal.,2023) 2.83
RelPose++(Linetal.,2023) 4.49
R+TRegression(Linetal.,2023) 0.0300
RayRegression(Ours) 0.133
RayDiffusion(Ours) 11.1
Table6: InferencetimeforN=8Images. AllbenchmarksarecompletedusingasingleNvidiaA6000GPU.
Wecomputethebestof5runs. RayDiffusionandPoseDiffusionbothuseDDPMinferencewith100steps.
RelPoseuses200iterationsofcoordinateascentwhileRelPose++uses50iterations. Unsurprisingly,wefind
that feedforward methods (Ray Regression, R+T Regression) achieve very low latency. The other methods
requirelengthieroptimizationloops.
#ofImages 2 3 4 5 6 7 8
MemoryUsage(MiB) 2877 2903 2913 2965 3007 3013 3095
Table7:MemoryUsageofourRayDiffusionModel.WemeasuretheGPUmemoryusagewhenrunningin-
ferenceusingourRayDiffusionmodelwithvariousnumbersofimages.Wereportthepeakmemoryconsumed
asreportedbynvidia-smi,whichmaynotbeexact. Notethatloadingourmodelintomemoryconsumes
2637MiB,whichisthemajorityofthememoryusage. Weobservesub-quadraticgrowthinmemoryusage,
likelybecausetheDINOfeaturecomputationisheavierthantheraytransformer.
15
.ccA.toR
.ccA.CCPublishedasaconferencepaperatICLR2024
Figure 11: Accuracy For All Thresholds We visualize our unseen category accuracy curves for 8 images.
Note for very fine thresholds of rotation accuracy, between 1 and 5 degrees, COLMAP rises faster than all
other methods. This is due to the precision of COLMAP when it is able to converge to a reasonable set of
poses. However,COLMAPisonlyabletoconvergeinroughly40%ofourtesttimeevaluations. SeeTabs.8
and9forAUCmetricsforallnumbersofimages.
#ofImages 2 3 4 5 6 7 8
COLMAP(SP+SG)(Sarlinetal.,2019) 57.8 56.9 56.8 57.5 58.1 58.9 60.3
PoseDiffusion(Wangetal.,2023) 84.9 84.9 85.3 85.6 85.9 86.2 86.5
RelPose++(Linetal.,2023) 88.7 89.2 89.8 90.3 90.4 90.8 90.7
RayRegression(Ours) 93.6 93.4 93.3 93.5 93.6 93.6 93.5
RayDiffusion(Ours) 94.3 94.4 94.4 94.5 94.6 94.6 94.7
COLMAP(SP+SG)(Sarlinetal.,2019) 59.1 58.8 59.1 60.2 61.2 62.6 64.6
PoseDiffusion(Wangetal.,2023) 76.8 77.0 77.3 77.8 78.3 78.5 79.0
RelPose++(Linetal.,2023) 79.4 81.2 81.7 82.8 83.4 83.6 83.8
RayRegression(Ours) 91.2 91.0 91.5 91.5 91.5 91.7 91.7
RayDiffusion(Ours) 91.7 92.2 92.5 92.9 92.6 92.8 92.8
Table8: RotationAccuracyAUConCO3D.Herewereporttherotationaccuracyacrossallthresholds(0to
180degreesin1degreeincrements)bymeasuringtheAreaUnderCurve(AUC).SeeFig.11foravisualization
ofthecurvesforN=8.
#ofImages 2 3 4 5 6 7 8
COLMAP(SP+SG)(Sarlinetal.,2019) 40.1 34.1 25.9 22.4 20.6 20.5 22.0
PoseDiffusion(Wangetal.,2023) 92.5 85.6 82.3 80.4 79.4 78.9 78.6
RelPose++(Linetal.,2023) 92.5 85.6 82.3 80.4 79.4 78.9 78.6
RayRegression(Ours) 92.5 90.2 88.5 87.6 87.1 86.6 86.2
RayDiffusion(Ours) 92.5 90.7 89.5 88.8 88.3 88.0 87.8
COLMAP(SP+SG)(Sarlinetal.,2019) 41.0 36.6 29.4 25.8 25.0 26.2 28.7
PoseDiffusion(Wangetal.,2023) 92.5 81.3 75.8 73.3 71.4 70.3 69.7
RelPose++(Linetal.,2023) 92.5 83.3 79.0 76.9 75.7 74.8 74.3
RayRegression(Ours) 92.5 88.6 86.3 85.0 84.1 83.6 83.2
RayDiffusion(Ours) 92.5 89.2 87.2 86.2 85.4 84.9 84.6
Table 9: Camera Center Accuracy AUC on CO3D. Here we report the camera center accuracy across all
thresholds(0to1ofscenescalein0.05increments)bymeasuringtheAreaUnderCurve(AUC).SeeFig.11
foravisualizationofthecurvesforN=8.
16
.etaCneeS
.etaCneesnU
.etaCneeS
.etaCneesnUPublishedasaconferencepaperatICLR2024
#ofImages 2 3 4 5 6 7 8
RayRegression@5 39.8 38.5 38.7 38.7 38.7 38.6 38.4
RayRegression@10 75.3 75.4 75.5 76.0 76.2 76.1 76.1
RayRegression@15 88.8 88.7 88.7 89.0 89.4 89.3 89.2
RayRegression@30 96.0 95.8 95.7 95.8 96.0 96.0 95.8
RayDiffusion@5 48.7 48.5 48.6 49.0 49.2 49.2 49.3
RayDiffusion@10 81.9 82.4 83.0 83.4 83.7 84.0 84.2
RayDiffusion@15 91.8 92.4 92.6 92.9 93.1 93.3 93.3
RayDiffusion@30 96.7 96.9 96.9 97.1 97.2 97.2 97.3
RayRegression@5 29.2 29.3 29.9 29.9 30.6 29.9 30.1
RayRegression@10 61.8 63.3 64.8 64.9 65.4 65.7 65.4
RayRegression@15 79.0 79.6 80.6 81.4 81.3 81.9 81.9
RayRegression@30 92.8 92.4 93.0 93.3 93.0 93.4 93.5
RayDiffusion@5 37.7 36.6 37.8 38.2 38.6 38.2 38.6
RayDiffusion@10 68.5 70.3 71.4 72.3 73.2 73.1 73.8
RayDiffusion@15 83.5 85.6 86.3 86.9 87.2 87.5 88.1
RayDiffusion@30 93.8 94.4 94.8 95.4 95.0 95.2 95.2
Table10:CameraRotationAccuracyonCO3Datvaryingthresholds.
#ofImages 2 3 4 5 6 7 8
RayRegression@0.05 100.0 74.3 57.7 49.6 45.0 41.7 39.2
RayRegression@0.1 100.0 91.7 85.7 82.1 79.8 77.9 76.2
RayRegression@0.2 100.0 97.4 95.5 94.3 93.9 93.2 92.7
RayDiffusion@0.05 100.0 80.8 67.8 60.5 56.2 53.5 51.1
RayDiffusion@0.1 100.0 94.2 90.5 87.8 86.2 85.0 84.1
RayDiffusion@0.2 100.0 98.0 96.6 96.0 95.5 95.1 94.8
RayRegression@0.05 100.0 64.7 46.8 38.0 33.4 29.9 28.6
RayRegression@0.1 100.0 83.7 75.6 70.8 67.4 65.3 63.9
RayRegression@0.2 100.0 94.3 90.8 88.6 87.4 86.8 85.9
RayDiffusion@0.05 100.0 70.6 54.7 47.5 42.3 39.0 38.0
RayDiffusion@0.1 100.0 87.7 81.1 77.0 74.1 72.4 71.4
RayDiffusion@0.2 100.0 94.7 92.2 90.9 89.8 89.2 88.5
Table11:CameraCenterAccuracyonCO3Datvaryingthresholds.
17
.etaCneeS
.etaCneesnU
seirogetaCneeS
seirogetaCneesnUPublishedasaconferencepaperatICLR2024
R@15 (Seen) CC@0.1 (Seen)
0.825
0.92
0.800
0.90 0.775
100 80 60 40 20 0 100 80 60 40 20 0
R@15 (Unseen) CC@0.1 (Unseen)
0.88 0.70
0.86
0.65
0.84
0.82
100 80 60 40 20 0 100 80 60 40 20 0
Figure12:EarlyStoppingAblationforBackwardDiffusiononCO3Dwith8Images.Wefindempirically
that stopping the backward diffusion process early yields slightly improved results. Here, we visualize the
accuracyofthepredictedX aftereachiterationofbackwarddiffusion,startingfromcompletenoise(T =100)
0
tothefinalrecoveredraysX .Forallexperiments,weusetheX predictedatT =30.
0 0
18PublishedasaconferencepaperatICLR2024
#ofImages 2 3 4 5 6 7 8
Apple 87.6 83.6 83.6 83.2 84.1 83.6 84.8
Backpack 93.0 94.0 94.7 93.9 94.7 94.6 94.3
Banana 90.2 90.3 91.6 91.3 91.4 91.8 92.0
Baseballbat 95.7 91.0 90.2 91.1 91.9 90.3 90.2
Baseballglove 85.3 86.7 86.7 86.8 85.9 85.9 86.2
Bench 94.8 94.3 93.2 94.2 94.1 94.1 93.4
Bicycle 92.4 92.4 92.0 93.9 93.7 93.5 94.0
Bottle 87.2 84.1 86.5 86.2 84.2 85.0 85.4
Bowl 90.5 92.2 92.4 91.4 91.9 92.5 91.9
Broccoli 73.8 71.9 77.2 76.2 78.1 78.1 77.9
Cake 81.8 83.4 81.6 84.3 83.8 84.0 83.6
Car 90.0 90.9 91.0 91.3 91.5 90.5 90.3
Carrot 86.1 87.6 87.4 88.1 89.1 89.0 88.7
Cellphone 87.6 87.7 88.3 88.1 88.9 89.6 89.3
Chair 97.5 98.2 98.4 98.3 98.4 98.7 98.6
Cup 80.0 77.6 77.1 78.0 78.9 77.3 77.0
Donut 71.2 76.5 74.7 76.1 74.2 75.8 75.4
Hairdryer 90.6 93.2 93.9 94.7 94.7 94.5 94.4
Handbag 85.1 85.1 86.8 87.5 87.6 87.9 87.1
Hydrant 98.0 96.9 95.9 96.7 96.8 96.7 97.5
Keyboard 95.4 95.4 94.2 94.7 94.5 94.7 94.7
Laptop 95.6 96.0 96.1 96.6 96.5 96.5 96.4
Microwave 88.0 86.1 85.3 83.9 85.2 86.1 86.1
Motorcycle 92.4 92.9 93.3 94.0 94.6 94.6 94.1
Mouse 93.5 93.6 94.4 94.0 94.5 94.1 94.5
Orange 75.7 75.4 73.7 73.6 74.9 75.2 74.7
Parkingmeter 86.7 84.4 79.4 80.0 87.3 80.8 78.2
Pizza 92.4 92.1 89.8 92.8 92.8 92.8 94.2
Plant 83.9 84.4 85.4 85.5 84.8 85.4 85.8
Stopsign 86.5 88.0 89.4 87.5 88.1 87.2 87.5
Teddybear 92.0 93.8 94.4 94.4 94.8 95.1 95.3
Toaster 99.2 98.4 98.5 99.2 99.0 98.8 99.0
Toilet 97.2 97.0 95.6 96.5 97.1 96.5 96.5
Toybus 92.3 93.1 91.9 93.2 91.5 92.4 93.0
Toyplane 79.5 80.0 80.7 81.9 82.4 82.9 82.5
Toytrain 90.6 89.2 91.5 91.6 89.9 91.8 90.8
Toytruck 89.4 87.9 88.9 89.2 89.9 90.2 89.7
Tv 100.0 100.0 98.9 97.3 99.1 98.1 98.3
Umbrella 88.4 90.3 89.3 90.1 90.5 90.6 89.9
Vase 85.2 82.7 85.0 84.4 84.6 84.7 85.1
Wineglass 77.5 78.1 78.7 78.0 78.6 78.7 78.1
Ball 61.7 62.8 62.8 62.9 62.3 62.2 62.1
Book 81.7 83.5 84.7 86.0 85.3 86.2 85.1
Couch 86.8 88.0 85.3 87.8 86.9 87.7 88.2
Frisbee 75.2 75.5 76.8 78.0 77.2 77.0 77.8
Hotdog 75.7 72.9 75.7 73.3 73.2 74.6 75.3
Kite 69.2 71.0 72.7 73.8 76.8 77.9 77.7
Remote 88.0 90.4 92.2 93.0 91.9 93.5 92.6
Sandwich 79.5 79.0 78.8 78.0 78.0 78.5 78.0
Skateboard 77.8 77.8 80.7 85.1 85.7 85.5 85.6
Suitcase 94.8 95.6 96.3 96.1 95.7 95.6 95.9
Table12:Per-categoryCameraRotationAccuracyonCO3D(@15◦)forRayRegression.
19
seirogetaCneeS
seirogetaCneesnUPublishedasaconferencepaperatICLR2024
#ofImages 2 3 4 5 6 7 8
Apple 100.0 92.9 87.3 81.4 78.5 74.7 74.1
Backpack 100.0 93.5 90.7 87.7 85.1 82.7 81.4
Banana 100.0 91.0 84.9 79.6 76.2 74.0 74.1
Baseballbat 100.0 90.5 78.2 71.7 68.6 68.8 64.6
Baseballglove 100.0 93.3 86.7 81.3 81.1 77.1 74.7
Bench 100.0 94.7 91.5 89.3 87.5 85.6 84.3
Bicycle 100.0 94.7 92.5 90.6 88.2 85.7 85.2
Bottle 100.0 91.6 86.6 83.8 79.3 78.1 76.4
Bowl 100.0 91.7 89.6 86.5 85.7 84.8 84.3
Broccoli 100.0 89.9 82.3 76.7 73.5 69.9 67.7
Cake 100.0 87.7 82.8 76.4 73.8 71.2 67.9
Car 100.0 92.1 89.9 87.9 87.6 86.4 85.3
Carrot 100.0 90.1 83.0 77.7 75.1 72.1 70.2
Cellphone 100.0 91.2 84.2 76.1 73.7 73.1 70.5
Chair 100.0 97.5 96.9 95.6 94.9 94.1 93.5
Cup 100.0 84.8 75.9 73.0 70.2 65.5 63.4
Donut 100.0 83.7 68.2 65.9 64.7 62.7 62.6
Hairdryer 100.0 95.6 91.3 87.4 85.6 82.7 80.8
Handbag 100.0 90.7 84.6 80.8 76.4 75.4 72.4
Hydrant 100.0 97.9 96.0 94.7 94.1 93.8 92.2
Keyboard 100.0 91.5 83.3 79.8 77.4 74.5 74.0
Laptop 100.0 94.9 88.0 86.7 82.7 80.2 78.8
Microwave 100.0 89.3 79.4 77.4 74.0 72.3 71.3
Motorcycle 100.0 96.8 94.7 92.9 91.8 91.4 90.3
Mouse 100.0 95.6 88.8 85.5 82.2 79.8 75.3
Orange 100.0 85.9 73.5 68.4 63.2 60.9 59.5
Parkingmeter 100.0 82.2 73.3 72.7 73.9 68.1 62.5
Pizza 100.0 92.1 83.3 80.4 78.9 77.7 74.8
Plant 100.0 90.5 84.7 79.5 76.9 74.6 73.2
Stopsign 100.0 90.1 84.9 80.8 77.5 72.2 73.5
Teddybear 100.0 97.0 92.9 90.8 88.2 86.8 85.8
Toaster 100.0 97.3 96.9 95.6 95.5 93.1 93.2
Toilet 100.0 90.3 82.6 80.8 76.6 74.4 72.6
Toybus 100.0 96.7 88.5 88.0 85.0 83.3 82.9
Toyplane 100.0 86.5 77.9 73.0 70.5 69.1 66.9
Toytrain 100.0 90.4 87.3 83.1 79.3 79.3 75.7
Toytruck 100.0 91.5 85.1 82.7 81.4 80.1 77.0
Tv 100.0 95.6 91.7 84.0 83.3 84.8 87.5
Umbrella 100.0 95.2 89.0 85.4 85.4 83.6 82.0
Vase 100.0 89.0 84.0 79.0 76.3 75.9 74.5
Wineglass 100.0 86.0 80.0 74.4 73.5 71.8 69.3
Ball 100.0 79.7 65.0 58.8 53.1 51.0 48.5
Book 100.0 90.5 83.0 79.2 75.1 75.0 69.9
Couch 100.0 79.6 66.9 64.2 60.9 58.1 57.0
Frisbee 100.0 84.0 75.0 70.9 68.7 64.2 64.6
Hotdog 100.0 67.6 63.2 51.1 49.5 48.8 48.0
Kite 100.0 71.8 60.4 57.4 52.8 50.0 50.6
Remote 100.0 92.1 87.7 83.0 80.5 80.3 77.8
Sandwich 100.0 89.0 81.6 76.7 73.0 71.7 71.4
Skateboard 100.0 87.0 80.3 76.4 72.4 66.2 66.0
Suitcase 100.0 95.3 93.3 90.5 88.2 87.3 85.5
Table13:Per-categoryCameraCenterAccuracyonCO3D(@0.1)forRayRegression.
20
seirogetaCneeS
seirogetaCneesnUPublishedasaconferencepaperatICLR2024
#ofImages 2 3 4 5 6 7 8
Apple 92.8 91.3 91.3 92.2 91.8 91.5 91.6
Backpack 94.9 95.5 95.3 95.8 96.2 95.7 96.0
Banana 92.2 93.2 94.4 95.1 95.8 96.0 96.5
Baseballbat 97.1 95.2 94.8 94.6 95.0 95.8 94.4
Baseballglove 89.3 92.0 90.0 90.3 90.8 91.1 90.7
Bench 98.0 98.4 97.5 97.4 97.5 97.8 97.3
Bicycle 94.4 92.5 93.5 94.4 95.1 95.3 96.3
Bottle 91.6 90.8 90.9 91.0 91.3 92.0 92.6
Bowl 91.5 93.2 93.4 93.1 93.8 93.9 93.8
Broccoli 80.0 83.8 85.3 85.6 86.1 86.7 86.9
Cake 91.6 91.1 91.3 91.4 90.9 91.3 90.9
Car 92.6 93.3 92.3 93.2 93.5 93.1 93.3
Carrot 88.7 90.4 92.0 92.5 92.6 92.5 92.2
Cellphone 92.0 92.9 91.9 92.3 92.4 93.0 92.6
Chair 98.9 98.9 98.8 99.3 99.1 99.4 99.4
Cup 84.4 82.5 85.1 83.8 84.1 84.5 84.4
Donut 89.6 86.9 88.5 87.8 88.9 87.9 88.6
Hairdryer 93.5 95.4 96.5 95.9 96.4 97.0 96.9
Handbag 88.3 89.6 91.0 90.9 91.4 91.8 91.2
Hydrant 97.6 98.0 97.7 98.1 99.0 98.8 99.1
Keyboard 94.4 95.3 95.3 95.6 95.4 96.0 96.2
Laptop 97.1 97.0 96.5 96.8 97.1 97.2 97.1
Microwave 89.6 88.3 88.0 87.6 87.6 88.5 88.7
Motorcycle 95.6 97.2 96.7 96.7 96.9 97.1 96.6
Mouse 97.1 96.6 96.9 97.8 97.3 97.5 97.8
Orange 85.4 86.9 84.1 86.4 85.9 85.5 85.7
Parkingmeter 76.7 84.4 90.0 90.0 91.8 93.5 92.4
Pizza 95.2 95.9 93.7 95.1 95.3 95.0 94.7
Plant 91.2 91.6 92.8 93.8 93.4 93.6 94.1
Stopsign 90.2 89.8 89.9 89.5 90.5 89.4 89.8
Teddybear 94.1 96.6 96.9 97.3 97.7 97.6 98.0
Toaster 97.6 98.4 98.5 99.2 99.0 98.8 99.5
Toilet 99.3 97.7 97.1 97.2 97.0 96.8 96.9
Toybus 89.2 91.8 92.1 94.4 91.7 92.7 92.9
Toyplane 84.1 85.3 85.5 86.3 86.5 86.7 85.8
Toytrain 93.1 93.3 92.2 93.3 92.4 93.4 93.1
Toytruck 88.1 90.1 88.6 89.1 89.9 91.0 91.1
Tv 100.0 100.0 100.0 98.0 100.0 100.0 100.0
Umbrella 90.8 92.5 93.1 92.6 92.4 93.3 93.7
Vase 87.1 90.4 90.8 90.8 90.8 90.5 91.3
Wineglass 87.0 85.7 85.6 86.8 87.4 86.8 86.6
Ball 73.6 74.0 74.6 74.8 76.0 74.0 75.6
Book 90.4 90.6 91.6 91.9 92.6 92.5 92.7
Couch 90.8 89.7 89.8 92.1 90.4 90.9 90.3
Frisbee 75.2 78.1 79.1 83.0 82.3 82.7 84.2
Hotdog 70.0 80.0 80.2 78.4 78.9 79.9 81.3
Kite 76.9 77.7 78.7 79.8 81.1 82.5 82.7
Remote 92.0 94.0 95.9 94.8 95.4 95.1 95.5
Sandwich 87.0 87.3 87.0 88.1 88.6 89.3 90.3
Skateboard 83.3 86.7 88.9 88.8 89.5 90.5 90.1
Suitcase 95.6 97.9 97.5 97.6 97.7 98.0 97.9
Table14:Per-categoryCameraRotationAccuracyonCO3D(@15◦)forRayDiffusion.
21
seirogetaCneeS
seirogetaCneesnUPublishedasaconferencepaperatICLR2024
#ofImages 2 3 4 5 6 7 8
Apple 100.0 96.1 92.4 89.5 87.1 85.7 84.5
Backpack 100.0 96.1 93.8 91.6 90.0 87.1 86.9
Banana 100.0 94.1 88.9 84.7 82.5 81.5 81.1
Baseballbat 100.0 92.4 81.8 78.0 77.4 77.6 71.4
Baseballglove 100.0 92.0 85.3 83.7 81.8 79.6 79.0
Bench 100.0 97.3 96.1 93.7 92.5 93.6 93.8
Bicycle 100.0 94.9 94.5 92.8 90.3 89.2 89.7
Bottle 100.0 94.5 93.1 90.1 89.4 89.9 88.2
Bowl 100.0 92.7 90.6 89.3 89.3 88.3 87.5
Broccoli 100.0 95.0 88.8 84.3 81.5 81.1 78.9
Cake 100.0 93.9 90.6 87.7 83.5 83.1 80.6
Car 100.0 94.9 92.5 91.1 91.1 90.5 90.4
Carrot 100.0 93.4 88.0 84.5 82.2 79.6 79.5
Cellphone 100.0 94.4 87.8 83.6 80.5 77.8 75.9
Chair 100.0 98.1 97.2 96.8 96.5 95.9 95.7
Cup 100.0 88.9 83.5 81.5 80.1 77.7 77.9
Donut 100.0 90.4 87.4 85.6 85.2 81.7 80.7
Hairdryer 100.0 97.4 95.6 93.1 91.2 90.4 89.2
Handbag 100.0 93.9 91.2 86.9 85.2 83.9 81.9
Hydrant 100.0 98.7 98.0 97.1 97.7 97.3 96.7
Keyboard 100.0 94.4 86.1 83.6 81.5 80.3 80.1
Laptop 100.0 95.3 88.4 87.0 84.8 83.2 82.3
Microwave 100.0 90.7 82.8 79.4 76.1 76.2 75.4
Motorcycle 100.0 98.3 97.3 95.0 94.9 94.8 94.4
Mouse 100.0 98.2 94.9 91.5 89.7 88.5 85.9
Orange 100.0 92.3 84.9 81.8 77.5 75.3 74.4
Parkingmeter 100.0 87.8 93.3 84.7 88.9 86.2 85.0
Pizza 100.0 92.4 90.7 87.4 84.8 83.3 81.3
Plant 100.0 95.7 92.3 89.7 88.0 86.6 86.3
Stopsign 100.0 93.6 87.2 83.3 80.7 77.1 77.5
Teddybear 100.0 97.9 95.5 94.3 92.8 91.9 91.5
Toaster 100.0 98.5 98.0 98.7 97.9 96.2 97.4
Toilet 100.0 91.7 87.9 83.3 80.8 79.2 77.5
Toybus 100.0 95.1 88.1 89.5 83.6 84.5 85.9
Toyplane 100.0 87.0 80.8 76.8 74.4 73.5 72.2
Toytrain 100.0 92.3 87.0 85.9 82.5 83.8 79.7
Toytruck 100.0 90.4 85.6 80.8 80.5 80.1 78.1
Tv 100.0 100.0 100.0 98.7 97.8 95.2 96.7
Umbrella 100.0 95.3 92.9 90.5 90.5 88.7 88.5
Vase 100.0 95.0 91.5 88.2 87.1 85.9 85.0
Wineglass 100.0 90.6 88.1 84.6 83.2 82.2 81.4
Ball 100.0 85.9 75.1 70.4 67.1 62.7 63.5
Book 100.0 93.7 88.0 85.3 84.2 82.5 82.3
Couch 100.0 83.3 74.9 71.1 65.7 64.5 61.5
Frisbee 100.0 84.3 80.2 75.8 72.5 70.7 70.5
Hotdog 100.0 77.6 67.5 60.0 59.3 59.2 55.5
Kite 100.0 77.2 63.8 58.8 54.2 52.1 53.5
Remote 100.0 94.0 91.9 87.4 85.8 84.3 83.4
Sandwich 100.0 95.0 91.5 88.9 86.4 84.5 84.9
Skateboard 100.0 88.1 84.2 79.3 73.7 72.7 67.9
Suitcase 100.0 97.5 94.3 93.0 91.8 90.9 90.8
Table15:Per-categoryCameraCenterAccuracyonCO3D(@0.1)forRayDiffusion.
22
seirogetaCneeS
seirogetaCneesnU