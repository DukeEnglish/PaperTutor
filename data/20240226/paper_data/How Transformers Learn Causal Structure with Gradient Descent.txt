How Transformers Learn Causal Structure with
Gradient Descent
Eshaan Nichani, Alex Damian, and Jason D. Lee
Princeton University
February 23, 2024
Abstract
The incredible success of transformers on sequence modeling tasks can be largely at-
tributed to the self-attention mechanism, which allows information to be transferred between
different parts of a sequence. Self-attention allows transformers to encode causal structure
whichmakesthemparticularlysuitableforsequencemodeling. However,theprocessbywhich
transformerslearnsuchcausalstructureviagradient-basedtrainingalgorithmsremainspoorly
understood. Tobetterunderstandthisprocess,weintroduceanin-contextlearningtaskthatre-
quireslearninglatentcausalstructure. Weprovethatgradientdescentonasimplifiedtwo-layer
transformer learns to solve this task by encoding the latent causal graph in the first attention
layer. The key insight of our proof is that the gradient of the attention matrix encodes the
mutual information between tokens. As a consequence of the data processing inequality, the
largestentriesofthisgradientcorrespondtoedgesinthelatentcausalgraph. Asaspecialcase,
whenthesequencesaregeneratedfromin-contextMarkovchains,weprovethattransformers
learnaninductionhead[21]. Weconfirmourtheoreticalfindingsbyshowingthattransformers
trainedonourin-contextlearningtaskareabletorecoverawidevarietyofcausalstructures.
1 Introduction
The transformer architecture [26] has revolutionized the field of deep learning, and has led to
state-of-the art performance on tasks spanning language modeling [6], computer vision [10], re-
inforcement learning [7], and the sciences [18]. The basic primitive of a transformer is a self-
attention head, a sequence-to-sequence mapping in which each token in the output is a weighted
linearcombinationof,i.e“attendsto,”theothertokensinthesequence. Priorwork[11]hassought
to understand which specific computational operations, or “circuits,” are implemented by self-
attention layers in trained transformers. However, the process by which such circuits arise when
transformersaretrainedfromscratchviagradient-basedalgorithmsisstillunknown.
One hallmark capability of transformers is in-context learning [6], which is the ability to learn
from information present in the input context without needing to update the model parameters.
1
4202
beF
22
]GL.sc[
1v53741.2042:viXraFor example, given a prompt of input-output pairs, in-context learning is the ability to predict the
output corresponding to a new input. Prior work has shown that this in-context learning ability
relieson the existence of specificcircuits called induction heads[21]. Givena promptof theform
[ ,A,B, ,A], an induction head copies the token which follows the previous occurrence of
··· ···
A,inthiscasebeingB. Thiscanbeimplementedusingtwoattentionlayers: thefirstperformsthe
operation of “copying” the previous token, while the second compares this previous token to the
last token of the context. By copying the previous token, the first attention layer thus implicitly
encodesthecausalstructureofaMarkovchain.
As another example, consider the setting of learning a function class in-context, introduced by
Garg et al. [14]. Each prompt sequence is formed by sampling a new function f from some
function class , and generating the prompt [x ,f(x ),x ,f(x ),...,x ,f(x ),x ] for i.i.d in-
1 1 2 2 n n test
F
puts x ,...,x ,x . The model must learn to estimate f(x ) from the (x ,f(x )) pairs given
1 n test test i i
in-context. This setting has proved a useful testbed to understand which in-context learning al-
gorithms can be implemented by transformers; see Section 1.2 for additional discussion. Such
prompts also possess special causal structure. Conditioned on f, the 2k-th token in the prompt
only depends on the (2k 1)-th token, and is independent of the rest of the sequence. The model
−
mustthuslearntoassociateeachf(x ),atposition2k,withitscorrespondingx ,atposition2k 1.
k k
−
We view this instance as a problem with both a global causal structure, which comes from pairing
x with f(x ), and an in-context transition which comes from the specific f sampled for the
k k
∈ F
sequence.
Transformers are clearly able to model causal structure, yet despite the necessity of doing so for
performingin-contextlearningtasks,westilldonotunderstandhowsuchstructuresarelearnedby
gradientdescentwhentrainingfromscratch. Wethusaskthefollowingquestion:
Howdotransformerslearncausalstructurewithgradientdescent?
1.1 Our Contributions
In this work, we analyze the gradient descent dynamics of an autoregressive two-layer attention-
only transformer, and prove that it recovers latent causal structure. Our specific contributions are
asfollows:
• We introduce a novel family of in-context learning problems, which we call random se-
quences with causal structure (Task 1). The task fixes a latent causal structure, unknown to
the transformer, and samples each sequence from a different distribution which respects the
causalstructure.
• When the latent causal graph is a tree, we prove that gradient descent on a simplified two-
layer transformer solves this task by encoding the causal graph in the first attention layer
in order to perform in-context estimation of the transition distribution (Theorem 1). As a
special case of Theorem 1, we show that when the sequences are generated from in-context
Markovchains,thetransformerlearnsaninductionhead.
• The proof of Theorem 1 relies on showing that the gradient of the first attention layer auto-
matically computes the χ2-mutual information between pairs of tokens. As a result of the
2data processing inequality, the largest entries of this gradient correspond to edges in the la-
tentcausalgraph,andhencethefirstattentionlayerconvergestotheadjacencymatrixofthis
graph.
• When the causal graph is not a tree, we explicitly construct a multi-head transformer which
solves this task by distributing the latent causal graph across many heads. We show empiri-
callythattransformerstrainedbygradientdescentonthistasklearnourconstruction.
1.2 Related Work
In-Context Learning. Brown et al. [6] demonstrated that GPT-3 can perform in-context learn-
ing,whichhasledtomuchsubsequentworkonunderstandinghowsuchin-contextlearningability
arises. Xieetal.[28]presentsaBayesianperspectiveonin-contextlearningbylookingatthelog-
likelihoodofout-of-distributionpromptsequences. Olssonetal.[21]positsthatin-contextlearning
reliesontheemergenceofinductionheads.
Garg et al. [14] shows that transformers can be trained to in-context learn various simple function
classes such as (sparse) linear models or shallow neural networks. Many recent works have thus
sought to understand which in-context learning algorithms can be efficiently expressed by a trans-
former. Akyu¨reketal.[2],Baietal.[3],VonOswaldetal.[27]showthattransformerscanimple-
ment gradient descent in order to learn linear regression in-context, while Fu et al. [13] constructs
transformers which implement higher-order learning algorithms such as Newton’s method. Gian-
nou et al. [15] construct a transformer that can express general-purpose computational operations.
However, these works are solely concerned with the representational capabilities of transformers,
anddonotanswerthequestionofwhethergradientdescentcanlearnsuchconstructions.
Training dynamics of transformers. Prior works have studied the optimization dynamics of a
single attention layer. Tarzanagh et al. [23, 22] show an equivalence between the optimization
dynamics of a single attention layer and a certain SVM problem. Ahn et al. [1] shows that the
globaloptimumofasinglelinearattentionlayertrainedonanin-contextlinearregressionobjective
implements a single step of preconditioned gradient descent. Mahankali et al. [20], Zhang et al.
[29] study the optimization dynamics of a single linear attention layer for performing in-context
linear regression. Huang et al. [16] shows that gradient descent on a single softmax attention
layer learns to solve linear regression in-context when the input data are orthogonal. Jelassi et al.
[17] analyzes the gradient descent dynamics of the position-position block of a single layer vision
transformer, and shows that it converges to a solution encoding spacial structure. Tian et al. [24]
studies the optimization dynamics of a single attention layer for a specific toy dataset, while Tian
etal.[25]showsthatjointlytrainingaself-attentionandMLPlayercorrespondstotheoptimization
dynamics of a certain modified MLP module. Boix-Adsera et al. [5] shows that transformers with
diagonalattentionmatricesdisplayincrementallearningbehavior.
Bietti et al. [4] studies a synthetic ICL task, and shows that an induction head is formed during
training. They demonstrate heuristically that a few gradient steps on a modifed transformer archi-
tecture approximately learns the induction head. Our synthetic task handles more general causal
structure,andrequiresattendingtoallpriorinstancesofthefinaltokenratherthanthemostrecent
one. Furthermore, Bietti et al. [4] requires the alphabet size S to be significantly larger than the
3context length T; we, however, assume that T S, and our main theorem (Theorem 1) is an
≫
end-to-endguaranteeonlearningthecausalstructureandobtainingvanishingpopulationloss.
2 Setup
2.1 Transformer Architecture
Let [S] be a finite alphabet. Transformers are models mapping sequences s := (s ,...,s )
1:T 1 T
∈
[S]T of length T to a length T sequence of vectors z 1,...,z
T
Rdout. A transformer first embeds
∈
the sequence s
1:T
as a matrix X= x 1,x 2,...,x
T
⊤ RT×d, where d is the embedding dimen-
∈
sion. This embedding is parameterized by the token embedding E Rd×S, and the positional
(cid:2) (cid:3) ∈
embeddingP Rd×T:
∈
x = embed(s ;(E,P)) := Ee +Pe fori = 1,...,T. (1)
i 1:T i si i
Transformers consist of two types of layers: attention layers and MLP layers. Throughout, we
focus on decoder-based, attention-only transformers. These are models in which every layer is a
causalself-attentionlayer,definedbelow:
Definition 1 (Causal self-attention head). For a vector v Rk, define the softmax function :
Rk
→
Rk by S(v)
i
:=
(cid:80)k
je =x 1p e( xv pi) (vj). For a matrix A
∈
R∈ d×d, define the operator attn( ·;AS ) :
RT×d RT×d by
→
attn(h;A) := MASK(hAh⊤) h RT×d, (2)
S ∈
whereMASK(M)
i,j
isM
i,j
wheni j and(cid:0) otherwise,(cid:1)andthesoftmaxfunction isapplied
≥ −∞ S
row-wise.
InDefinition1,themaskingoperatorensurestokensonlyattendtoprevioustokensinthesequence,
andthesoftmaxnormalizestheoutputsothateachrowsumsto1. Theamountthattokeniattends
to token j, for j
≤
i, is thus
S
MASK(XAX⊤)
i,j
=
S
X ≤iA⊤x
i
j, where X
≤i ∈
Ri×d is the
submatrixformedbythefirstirowsofX.
(cid:0) (cid:1) (cid:0) (cid:1)
A single attention head is parameterized by the tuple of d d matrices (Q,K,V), referred to as
×
thequery,key,andvaluematrices,andmapsX tothesequenceattn(X;QK⊤)V⊤.
Adecoder-basedtransformeraggregatesmultiplecausalself-attentionheadsovermanylayers:
Definition 2 (Decoder-based transformer). Let L be the depth, m be the number of heads
ℓ ℓ∈[L]
{ }
per layer, and d be the embedding dimension. For ℓ [L], i [m ], let (Q(ℓ),K(ℓ),V(ℓ)) be the
∈ ∈ ℓ i i i
query, key, and value matrices for the ith head in the ℓth layer. Let W
O
Rdout×d be the output
∈
layer and let E Rd×S and P Rd×T be the token and positional embeddings respectively.
∈ ∈
Define the parameter vector θ := (Q(ℓ),K(ℓ),V(ℓ)) E,P,W . A decoder-based
{ i i i }ℓ∈[L],i∈[m ℓ] ∪ { O }
transformerTF
θ
: [S]T RT×dout operatesons
1:T
by
→
h(0) = embed(s ;(E,P))
1:T
m
ℓ
h(ℓ) = h(ℓ−1) + attn
h(ℓ−1);Q(ℓ)K(ℓ)⊤ V(ℓ)⊤
(3)
i i i
(cid:88)i=1 (cid:16) (cid:17)
TF (s ) = h(L)W⊤.
θ 1:T O
4Weremarkthath(ℓ) RT×d forℓ = 0,...,L.
∈
Disentangled Transformer. Prior works on mechanistic interpretability have introduced the
residual stream viewpoint to understand the behavior of trained transformers [11]. The resid-
ual stream exists as a memory and communication channel that various attention heads read and
writeto. Informationintheresidualstreamisstoredinlow-dimensionalsubspacesofintermediate
layers h(ℓ). For a single attention layer attn( ;QK⊤)V⊤, the query and key matrices “read” in-
·
formation from the relevant subspace, and the value matrix “writes” the output to a new subspace
of the residual stream. The weight matrices thus act as associative memories [4], storing various
embeddingswithintheresidualstream.
While this residual stream viewpoint provides intuition for the flow of information through the
forward pass of a transformer, from an interpretability perspective it is difficult to know which
subspaces contain which information. The outputs of each attention layer are added together and
thus their informations may overlap with each other, leading to a memory bottleneck [11, 4].
Friedmanetal.[12]thusconsideratransformermodelinwhichtheresidualstreamisdisentangled,
and the outputs of each attention layer are appended to the residual stream. The dimension of the
residualstreamthusgrowswiththedepth. Weformalizethisasadisentangledtransformer,defined
below:
Definition 3 (Disentangled Transformer). Let L be the depth, and m be the number of
ℓ ℓ∈[L]
{ }
headsperlayer. Definethesetofdimensionsd ,...,d byd = S+T andd = (1+m )d . Let
0 L 0 ℓ ℓ ℓ−1
{Aℓ
i}
be the attention matrices with A( iℓ)
∈
Rd ℓ−1×d ℓ−1, let W
O ∈
Rdout×dL be the output matrix,
andletθ = A(ℓ) W . AdisentangledtransformerTF actsonasequences by:
{ i }ℓ∈[L],i∈[m ℓ] ∪{ O } θ(cid:101) 1:T
(cid:101) (cid:101) (cid:102)
(cid:101) (cid:101) h(0) = X˜ =(cid:102)[x˜ 1,...,x˜ T]⊤ wherex˜ t = [e st,e t(cid:102)]
∈
Rd0
h(ℓ)= h(ℓ−1),attn(h(ℓ−1);A(ℓ)),...,attn(h(ℓ−1);A(ℓ)) (4)
1 m
ℓ
TF (s(cid:104) ) = h(L)W⊤. (cid:105)
θ(cid:101) 1:T O (cid:101) (cid:101)
Weremarkthath(ℓ) (cid:102)RT×d ℓ forℓ = 0,(cid:102)...,L.
∈
In addition to disentangling the residual stream, Definition 3 replaces the query and key matrices
with a single attention matrix A := QK⊤ and absorbs the value matrices into W . By allow-
O
ing d to grow with the depth, this disentangled transformer is actually equivalent to a decoder-
ℓ
basedattention-onlytransformer(cid:101)(seeTheorem3fortheformalstatement). Givent(cid:102)hisequivalence,
throughouttherestofthepaperwestudythedisentangledtransformer.
Finally, we remark that when the target is a vector in Rdout rather than a sequence in RT×dout, it is
customarytousetheembeddingofthelasttoken,i.e. TF (s ) = W h(L) andsimilarlyforTF .
θ 1:T O T θ(cid:101)
2.2 Problem Setup: Random Sequences with Causal Structure (cid:102)
Let = ([T], ) be a directed acyclic graph on [T] = 1,...,T with edge set , which will
G E { } E
representthelatentcausalstructure. Wewillassumethat(j i) onlyifj < i,i.e. eachtoken
→ ∈ E
canonlypointtofuturetokens. Forapositioni [T],wewillusep(i)todenotethesetofparents
∈
5s s s s s s
1 2 3 4 5 6
Figure 1: Random Sequence with Causal Structure: The causal structure is defined by the
graph , denoted by the arrows. In this figure, p(1) = , p(2) = 1 , p(3) = 1 , p(4) = 2 and
G ∅ { } { } { }
p(5) = 3 . Sequences are generated by sampling π P , s µ , s π( s ), s π( s ),
π 1 π 2 1 3 1
{ } ∼ ∼ ∼ ·| ∼ ·|
s π( s ), s π( s ), and finally s Unif([S]). The target y for this sequence is drawn
4 2 5 3 6
∼ ·| ∼ ·| ∼
fromπ( s ).
6
·|
toi,i.e. p(i) := j : (j i) .Welet denotethesetofrootnodes,i.e = i : p(i) = .
{ → ∈ E} R R { ∅}
For most of the paper, we assume that each position has at most one parent, i.e. p(i) 1 for all
| | ≤
i [T]. See Section 6 for the generalization to multiple parents. When p(i) = 1, we overload
∈ | |
notationandusep(i) [T]todenotetheuniqueparentofi.
∈
We will also assume there exists a prior P over irreducible and aperiodic Markov chains π on
π
[S] = 1,...,S . For each π, we will use µ to denote the unique stationary measure of π. Then
π
{ }
eachsequence[s ,...,s ]anditscorrespondingtargetyaregeneratedbythefollowingprocedure:
1 T
Task1(RandomSequencewithCausalStructure).
1. First,drawπ P .
π
∼
2. Fori = 1,...,T 1,samples µ ifp(i) = . Otherwisesamples π( s ).
i π i p(i)
− ∼ ∅ ∼ ·|
3. Draws Unif([S])ands π( s )
T T+1 T
∼ ∼ ·|
4. Returntheinputx = s andthetargety = s .
1:T T+1
We resample s from Unif([S]) to ensure that our prediction for π( s) is good for all s [S], not
T
·| ∈
justthosewhereµ (s) > 0. Becauses Unif([S]),T isarootnodeof ,i.e. T .
π T
∼ G ∈ R
SeeFigure1foranexampleinstanceofthistask.
2.3 Examples
MarkovChainsandInductionHeads. First,considerthecasewherep(i) = i 1. Thesequence
−
s ,...,s is now a Markov chain conditioned on π, with transition matrix π. Task 1 reduces to
1 T−1
theproblemofestimatingtheMarkovchainπ in-context. Thistaskcanbesolvedviaaninduction
head[21]which,whenpresentedwithaprompt = [ ,A,B, ,A,C, ,A],averagesover
P ··· ··· ···
the tokens following the previous occurrences of A, (in this case B and C). Explicitly, the output
ofaninductionheadonthesequences willbetheempiricalestimateforπ( s ):
1:T T
· |
i : (s ,s ) = (s ,s′)
i−1 i T
TF (s ) = |{ }|.
θ 1:T s′
i : s = s
i−1 T
|{ }|
InthelimitasT ,thisconvergestothetruetransitionπ( s ).
T
→ ∞ · |
6Figure 2: The Weights of a Trained Transformer: We plot the weights of a two layer disen-
tangled transformer trained on Task 1 with S = 10 and T = 20 when the causal graph is the
in-context learning graph where p(2i) = 2i 1 for all i > 0. All entries of A(1),A(2),W remain
O
−
small except the three blocks highlighted in red. The highlighted block in A(1) converges to the
adjacency matrix of the causal graph , and the highlighted blocks in A(2),W converge to the
O
G
identitymatrixI .
S
In-Context Learning. Consider the in-context learning setup from Garg et al. [14]. This corre-
spondstothecausalgraphwherep(2k 1) = andp(2k) = 2k 1. Sequencesaregeneratedby
− ∅ −
sampling f : [S] [S] from and using the transition matrix π(s′ s) = 1(s′ = f(s)). To learn
→ F |
this function class in-context, the transformer must learn to associate the (x,y) pairs in positions
2k 1and2k.
−
3 What does the Transformer Learn?
3.1 Experiments
We train a series of two-layer disentangled transformers with one head per layer on Task 1, for
varying latent graphs . The prior P is chosen so that each row of π is sampled i.i.d from the
π
G
Dirichlet distribution with parameter α, i.e π( s) Dir(α 1 ), for varying values of α. We
S
· | ∼ ·
train using gradient descent on the cross entropy loss with initial learning rate 1 and cosine decay
over217 steps.
We observe that the weights of the trained disentangled transformers exhibit consistent structure.
First,alloftheentriesofA(1) remainsmallexcepttheposition-positionblock(redboxunderA(1)
in Figure 2), which converges to the adjacency matrix of the graph . Next, all of the entries of
G
A(2) are small, except the(cid:101)token/token block comparing the h(0) component of the residual stre(cid:101)am
of token i to the attn(h(0),A(1)) component of the residual stream of token j for j i (red
≤
b(cid:101)ox under A(2) in Figure 2). Finally, all of the entries of the output matrix W are small except
O
(cid:101)
(cid:101)
7the token/token block which returns the value of the first component of the output of the second
attentionattn(h(1),A(2))(redboxunderW inFigure2). InFigure6,weobservethatthisweight
O
patternpersistsfordifferentlatentgraphs .
G
In the following section, we explicitly define this construction and describe the corresponding
dynamicsoftheforwardpassinFigure3.
3.2 Construction
InFigure2weobservethattheattentionweightsA(1),A(2) andoutputweightW areoftheform
O
0 0
A(1) = S×S S×T (cid:101) (cid:101) (cid:102)
0 A(1)
T×S
(cid:20) (cid:21)
0 0 A(2) 0
(cid:101) S×S S×T S×T
0 0 0 0
A(2) =  T×S T×T T×S T×T  (5)
0 0 0 0
S×S S×T S×S S×T
 0 0 0 0 
T×S T×T T×S T×T
(cid:101)  
 
W = 0 0 I 0 0
O S×d S×d S S×T S×d
for matrices A(1) RT×T and A(2)(cid:2) RS×S. We now explicitly cons(cid:3)truct the A(1) and A(2) from
(cid:102)
∈ ∈
Figure2thatsolveTask1. Indeed,weshowthatthisconstructionsolvesthetaskbyestimatingthe
empiricaltransitionmatrixπˆ :
s1:T
(j i) : (s ,s ) = (s,s′)
πˆ (s′ s) := |{ → ∈ E j i }|. (6)
s1:T
| (j i) : s = s
j
|{ → ∈ E }|
Construction1. Thereexistsatwo-layerdisentangledtransformerf = TF suchthat
θ(cid:101) θ(cid:101)
f (s ) πˆ (s′ s ). (7)
θ(cid:101) 1:T s′ ≈ s1:T | T (cid:102)
Proof. Set A(1) to be the (scaled) adjacency matrix of , i.e A(1) = β 1(j = p(i)), and A(2) =
G i,j 1
β I , where β ,β . We will now show that the output of the disentangled transformer
2 S 1 2
→ ∞
approximatesπˆ ( s ).
s1:T
· |
T
First Attention. Note that by the construction of A(1), X˜ A(1)X˜⊤ = A(1), which is the scaled
adjacencymatrixof . Ifiisnotarootnode(i.e. i ,p(i) = ),then
G ∈ R ̸ ∅
(cid:101) (cid:101)
(X˜ A(1)X˜⊤)
= 1(j = p(i)) (8)
i,j
S
so i attends to its parent p(i). Therefore, the output of the first attention is the token at position
(cid:101)
p(i),i.e. attn(X˜ ;A(1)) = x˜ . Thetransformerthenappendsx˜ totheresidualstreamoftoken
i p(i) p(i)
i.
When i is a root
no(cid:101)
de (i.e. i , p(i) = ), then for all j,
(X˜ A(1)X˜⊤)
= 0. Therefore after the
ij
∈ R ∅
softmax,iwillattendequallytoallprevioustokens:
(cid:101)
1
(X˜ A(1)X˜⊤)
= forall j i. (9)
i,j
S i ≤
(cid:101)
8Thusthefirstattentionlayeraveragesallofthetokensinthesequence: attn(X˜ ;A(1)) = 1 x˜ .
i i j≤i j
Itthencopiesthisaverageintotheresidualstream.
(cid:80)
(cid:101)
Second Attention. We next show that the Tth token attends to all prior tokens whose parents
tokensareequaltos . Itthenaveragesthemandcopiesthemintotheresidualstream.
T
After the first attention layer, the residual stream is h(1) = [x˜ ,attn(X˜ ;A(1)) ]⊤. The second
j j j
attentionlayercomparestheTthtokenoftheoriginalsequencex˜ totheoutputofthefirstattention
T
atallotherpositions. Explicitly,theattentionpatternisequalto: (cid:101)
h(1)⊤ A(2)h(1) = β x˜⊤ A(2) 0 S×T attn(X˜ ;A(1)) = β 1(s p(i) = s T) i ∈ R
T j 2 · T 0 0 j 2 · 1 1(s = s ) i .
(cid:20) T×S T×T (cid:21) (cid:40) i j≤i j T ∈ R
(10)
(cid:101) (cid:101)
(cid:80)
As β , the softmax converges to a hard max, and so the Tth token attends equally to all
2
→ ∞
tokens i such that s = s . The attention then averages all of these tokens, so the Tth token in
p(i) T
theresidualstreamisequaltoh(2) = x˜ , 1 x˜ ,Z,x˜ where
T T T j≤T j T
(cid:104) (cid:105)
(cid:80)
x˜
Z :=
s p(i)=sT i
(11)
i : s = s
(cid:80) p(i) T
|{ }|
istheaverageofthetokenswhoseparentisequaltos .
T
Output Layer. W reads from the third block in this stream, which we denoted by Z in (11)
O
above. ItthenreturnsthetokenembeddingofZ whichisequalto:
e
f (s ) =
s p(i)=sT si
= πˆ ( s ), (12)
θ(cid:101) 1:T i : s = s s1:T ·| T
(cid:80) p(i) T
|{ }|
asdesired.
SeeFigure3forabreakdownofthisforwardpassthroughthetransformerforaspecificsequence.
3.3 The Reduced Model
Motivated by the sparsity pattern in Figure 2 and Equation (5), we consider training a simplified
two-layer transformer architecture where the sparsity in Equation (5) is fixed, and only A(1) and
A(2) aretrained. ThetransformerTF canberewrittenasthefollowingreducedmodel:
θ(cid:101)
Lemma 1. Let θ = (A(1),A(2)), and let θ = (A(1),A(2),W ) be defined in Equation (5). Let
(cid:102) O
f = TF beatwo-layerdisentangledtransformerparameterizedbyθ. ThenifX = [x ,...,x ]T
θ θ(cid:101) 1 T
wherex = e , (cid:101) (cid:101) (cid:101) (cid:102)
i si
(cid:102)
f (s ) =
X⊤ (MASK(A(1)))XA(2)⊤
x . (13)
θ 1:T T
S S
(cid:16) (cid:17)
9Causal Graph
h(0) = a b a c b a
h First Attention i
a b a c b a
h(1) =
a a b a
" #
∅ ∅
Second Attention
a b a c b a
 a a b a 
h(2) = ∅ ∅
a+b c a+2b
 ∅ ∅ 2 ∅ 3 
 a b a 
 
 ∅ ∅ ∅ 
 Network Output 
TF
θ
= {P[y =a]= 1 3,P[y =b]= 2
3}
f
Figure 3: Understanding the Forward Pass: The solid arrows represent the causal graph
G
defined in Figure 1 and h(0) denotes the unmodified input sequence. The first attention reverses
this causal pattern, as every token attends to its parent (solid arrows). It then appends this parent
token to the residual stream (dashed arrows). In the second attention layer, each token i attends to
all previous tokens j whose parent token p(j) has the same value, i.e. s = s (solid arrows),
i p(j)
and appends the average of these tokens into the residual stream (dashed arrows). Finally, the
transformer returns the third entry in the last column (red box), which is the average of all of the
tokenswhoseparenttokenhasthesamevalueasthelasttoken.
Duetothemaskingoperation,wecanrestrictA(1) tobelowerdiagonal.
Our goal is to analyze the gradient descent dynamics of f under the cross entropy loss. However,
θ
if the token s′ does not appear in s , then f (s ) is 0 and the cross entropy loss is infinite. As
1:T θ 1:T s′
such,weperturbthepredictionsbysomesmallϵ > 0. Theperturbedpopulationlossisthus:
L(θ) = E π(s′ s T)log(f θ(s 1:T) s′+ϵ) . (14)
−π,s1:T | 
s′∈[S]
(cid:88)
 
In the following sections, we will study the gradient descent dynamics of the reduced model (13)
ontheloss(14).
4 Main Results
4.1 Training Algorithm
Ourtrainingalgorithmisstage-wisegradientdescentonthepopulationloss(14)usingthereduced
model (13). The model is initialized at A(1) = 0 ,A(2) = β I for small initialization scale
T×T 0 S×S
β . The first stage is gradient descent on A(1) with learning rate η for τ timesteps. The second
0 1 1
stage is gradient descent on A(2) with learning rate η for τ timesteps. Pseudocode is given in
2 2
Algorithm1.
10Algorithm1TrainingAlgorithm
Input: initsizeβ ;learningratesη ,η ;timesτ ,τ
0 1 2 1 2
InitializeA(1)(0) = 0 ,A(2)(0) = β I
T×T 0 S×S
·
fort = 1,...,τ do
1
A(1)(t) A(1)(t 1) η L(θ(t−1)) ▷Stage1
1 A(1)
← − − ∇
θ(t) = (A(1)(t),A(2)(0))
endfor
fort = τ ,...,τ +τ do
1 1 2
A(2)(t) A(2)(t 1) η L(θ(t−1)) ▷Stage2
2 A(2)
← − − ∇
θ(t) = (A(1)(τ ),A(2)(t))
1
endfor
θˆ θ(τ1+τ2)
←
ˆ
Output: θ.
WerequirethefollowingassumptionsonthepriorP :
π
Assumption 1 (Assumptions on prior P .). There exists γ > 0 such that almost surely over the
π
drawofπ:
• (Transitionlowerbounded): min π(s′ s) > γ/S.
s,s′
|
• (Non-degeneracy of chain): The chain does not immediately mix to the stationary measure
µ inonestep:
π
π( s) µ ( ) 2 γ2/S
s∥ · | − π · ∥2 ≥
• (Symmetry): Foranypermutat(cid:80)ionσ
on[S],σ−1πσ=d
π.
• (Constantmean): Eπ[π] = S11 S1⊤ S.
The final two assumptions imply that the marginal distributions of π(s′ s) are equal for any
|
s′ = s, and likewise for π(s s), and that these distributions have mean 1/S. Additionally, we
̸ |
assumethatanon-vanishingfractionofnodeshaveaparent.
Assumption2. Letr := /T. Thenr 1 γ.
|R| ≤ −
Throughout the proof, we let C denote an absolute constant that depends polynomially on γ−1
γ,S
and S. If A C B, then we write A = O (B) or A ≲ B. For convenience, we also drop
γ,S γ,S γ,S
thedependen≤ ceonγ,S,andwriteO( )or≲.
·
4.2 Main Theorem
The minimum possible value for the (unperturbed) loss is the mean entropy of π, averaged over
thepriorP :
π
1
L∗ := Eπ π(s′ s)logπ(s′ s) . (15)
− S | |
(cid:34) (cid:35)
s,s′
(cid:88)
Wealsodefinetheeffectivesequencelengthasfollows:
11Definition 4 (Effective Sequence Length). Decompose = k where are disjoint trees.
G
i=1Ti Ti
LetL denotethenumberofleavesoftree . Then,T := T .
i Ti eff max(cid:83)k i=1Li
The effective sequence length roughly captures the number of independent samples present in the
sequence s , and is related to the mixing time of the process on . For both the Markov chain
1:T
G
andin-contextlearningexamples,weseethatL = 1andthusT = T.
i eff
Ourmaintheoremisthefollowing:
Theorem1(GuaranteeforAlgorithm1). AssumethattheeffectivesequencelengthsatisfiesT
eff
≥
poly(γ−1,S). There exist ϵ,η ,η ,τ ,τ such that the output of Algorithm 1, θˆ = (Aˆ(1),Aˆ(2)),
1 2 1 2
satisfies
logT 1
L(θˆ ) L∗ ≲ and (Aˆ(1)) 1 O for i . (16)
− Tcγ S i,p(i) ≥ − T ∈ R
eff (cid:18) (cid:19)
forsomeconstantc > 0(independentofγ,S).
Algorithm 1 thus approximately minimizes the loss by encoding the adjacency matrix of in the
first attention layer
Aˆ(1).
Furthermore, we show that the trained model
θˆ
achieves good
preG
diction
ontransitionsπ whichareoutofdistribution:
Theorem2(OODGeneralization). Letπ˜havetransitionlowerboundedasmin π˜(s′ s) γ/S
s,s′
| ≥
ˆ
andletθ bethetrainedmodelfromTheorem1. Lets begeneratedbysteps2-4ofTask1. Then
1:T
withprobabilityatleast0.99overthedrawofs ,
1:T
logT
sup f (s ) π˜(s′ s ) ≲ . (17)
|
θˆ 1:T s′
− |
T
|
Tcγ
s′ eff
We remark that the only assumption needed on π˜ is the lower bound on the transition; it does not
needtobeclosetotypicaldrawfromthepriorP .
π
5 Proof Sketch
5.1 Stage 1: Learning the Causal Graph
The first step of the proof is to show that during the first stage of training, the first attention layer
A(1) learnsthelatentcausalgraph .
G
5.1.1 TheOracleAlgorithm
We begin by describing an efficient algorithm for learning the graph . The goal is to recover the
G
parent node p(i) for each i. The key idea is that as a result of the data generating process, s is
p(i)
thenodewhichmaximizesmutualinformationwiths .
i
Webrieflyrecallthedefinitionofanf-divergence.
12Definition5. Letf beaconvexfunctionwithf(1) = 0. Thef-divergencebetweentwoprobability
distributionsP,Qonstatespace isdefinedas
X
P(x)
D (P Q) := Q(x)f . (18)
f
|| Q(x)
x∈X (cid:18) (cid:19)
(cid:88)
Thef mutualinformationbetweentworandomvariablesY,Z,denotedbyI (Y;Z),is
f
I (Y;Z) := D (P P P ). (19)
f f Y,Z Y Z
|| ⊗
GivenalatentvariableC,theconditionalmutualinformationI (Y;Z C)isdefinedas
f
|
I f(Y;Z C) := EC D f(P (Y,Z)|C P Y|C P Z|C) . (20)
| || ⊗
(cid:2) (cid:3)
Informationmeasuresadmitadataprocessinginequality:
Lemma 2 (Data Processing Inequality). Let I be an information measure, and let W Y Z
f
→ →
beaMarkovchain. ThenI (W;Z) I (Y;Z).
f f
≤
Thedataprocessinginequalitysuggestsanefficientalgorithmforrecovering . Fornon-rootnodes
G
i , s s s form a Markov chain conditioned on π. Therefore by the data processing
j p(i) i
∈ R → →
inequality, p(i) argmax I (s ;s π). Otherwise, if i , s and s are independent given
j<i f i j j i
∈ | ∈ R
π, and thus I (s ;s π) = 0. To recover the graph , one can compute the conditional mutual
f i j
| G
informations I (s ;s π). If I (s ;s π) = 0 for all j < i, then i is a root node. Otherwise,
f i j f i j
| |
p(i) = argmax I (s ;s π). PseudocodeforthisoraclealgorithmisgiveninAlgorithm2.
j<i f i j
|
Algorithm2OracleAlgorithm
E ← ∅
fori = 1,...,T 1:do
−
ifmax I (s ;s π) > 0then
j<i f i j
|
p(i) argmax I (s ,s π).
j<i f i j
← |
(p(i) i) .
E ← E ∪{ → }
endif
endfor
We remark that Algorithm 2 is a special case of the celebrated Chow-Liu algorithm [8], when a
topological ordering of the tree is known a priori: the tree consisting of edges (p(i) i) where
→
p(i) = argmax I (s ;s π) is indeed the max-weight spanning forest when the edge weights
j<i f i j
|
aretheconditionalmutualinformations.
5.1.2 TheGradientDescentDynamics
We next compute the gradient with respect to A(1). Let A(1) Ri denote the ith row of A(1)
i ∈
(restricted to the first i entries, since A(1) is lower triangular). Define J : Rk Rk×k by J(v) =
→
13s
p(i)
I χ2(s i;s j∣π)<I χ2(s i;s p(i)∣π) −∇A i( ,1 j)L(θ)<−∇A i( ,1 p) (i)L(θ)
s s
i j
s j→s p(i)→s
i
is Markov
chain conditioned on π
Figure4:
Bythedataprocessinginequality,A(1) growsfasterthanA(1)
.
i,p(i) i,j
diag(v) vv⊤; we remark that J is the Jacobian of the softmax function , in that (u) =
u
− S ∇ S
J( (u)).
S
ThefollowinglemmacomputesthegradientwithrespecttoA(1);aheuristicderivationisdeferred
toAppendixD.3.
Lemma3.
β
L(θ) = 0 J (A(1)) g +O(T−1/2) , (21)
∇A( i1) −ST S i i eff
(cid:16) (cid:17)(cid:16) (cid:17)
wherethejthentryofg ,g ,is
i i,j
π(s′ s)
g i,j := Eπ µ (s| ′) PX[s j = s,x i = s′] −1. (22)
(cid:34) π (cid:35)
s,s′
(cid:88)
Fornon-rootnodesi ,(s i,s p(i))hasjointdistributionP[s
i
= s′,s
p(i)
= s] = µ π(s)π(s′ s),and
∈ R |
thusg is
i,p(i)
π(s′ s)2µ (s)2
π
g i,p(i) = Eπ µ (| s′)µ (s) −1 . (23)
(cid:34) π π (cid:35)
s,s′
(cid:88)
It turns out that this expression is exactly equal to the χ2-mutual information, I , between s and
χ2 i
s conditionedonπ. Theχ2-divergenceisthef-divergenceobtainedbysettingf(z) = (z 1)2.
p(i)
−
Therefore
g = I (s ;s π). (24)
i,p(i) χ2 i p(i)
|
ByCauchy-Schwarz,wecanalsoupperboundg bythesumoftwoχ2-mutualinformations:
i,j
1 1
g I (s ;s π)+ I (s ;s π). (25)
i,j
≤ 2
χ2 i p(i)
| 2
χ2 i j
|
14Applyingthedataprocessinginequality1,weobtainthatforj = p(i)
̸
g < I (s ;s π) = g . (26)
i,j χ2 i p(i) i,p(i)
|
Therefore g is maximized at j = p(i), and the gradient is aligned with the adjacency matrix of
i,j
thelatentcausalgraph . Infact,thegradientdescentdynamicsmimicAlgorithm2!
G
Maintainingtheinductivehypothesisthatargmax A(1) = p(i),weseebythegradientformulain
j i,j
Lemma3thatargmax L(θ) = p(i). ThusA(1) continuestogrowfasterthantheother
−∇A( i1) j i,p(i)
entriesthroughoutstage(cid:104)1. Thisgrowt(cid:105)hcontinuesuntil (A(1)) 1.
i,p(i)
S ≈
Forrootnodesi ,iisindependentofj. Sinceboths ands havethemarginalµ ,onehas
i j π
∈ R
π(s′ s)
g i,j = Eπ µ (s| ′) µ π(s′)µ π(s) −1 = 0 (27)
(cid:34) π (cid:35)
s,s′
(cid:88)
andthus L(θ) 0. Thereforeattheendofstage1, (A(1)) 1 forallj < i.
∇A( i1) ≈ S i,j ≈ i
Altogether,attheendofstage1,A(1) satisfies
1(j = p(i)) i
(A(1)) ∈ R . (28)
S i,j ≈ 1 i
(cid:40)
i ∈ R
The precise quantitative bound is given in Corollary 1, and requires controlling the various error
termsthroughoutmultiplestepsofgradientdescent.
5.2 Stage 2: Decreasing the Loss
We next show that during the second stage, A(2) grows large in the direction I 11 1⊤. By a
S − S S S
symmetry argument, one can show that L(θ) is proportional to I 11 1⊤. Writing A(2) =
∇A(2) S − S S S
βI + β−β01 1⊤,itsufficestoshowthat L(θ) < 0.
S S S S ∇β
In Lemma 10, we show that L(θ) can be approximated by a quantity which is an f-mutual
β
−∇
informationforsomeconvexf definedintermsofβ. Weshowthatthisquantityisstrictlypositive
(Lemma11)untilβ = Θ(logT ). Thusattheendofstage2,β = Θ(logT ).
eff eff
ToconcludetheproofofTheorem1,wemustshowthatf (X;s) π(s′ s). Indeed,Lemma32
θˆ s′
≈ |
showsthat
f (X;s) π(s′ s) exp( Θ(β)) = T−Θ(1), (29)
| θˆ s′ − | | ≤ − eff
whichimpliesthedesiredboundontheloss.
6 Causal Graphs with Multiple Parents
We next consider a generalization of Task 1. Let be a directed acyclic graph over the vertex set
G
[T +1]. For each node i [T +1], we assume that the set of parent nodes p(i) [i 1] satisfy
∈ ⊂ −
1BytheassumptionsonthepriorP ,thedataprocessinginequalityisindeedstrict.
π
15(a) 3-gramwhereeachpositioniattendstoi 1,i 2. (b) Eachpositioniattendstoi 1and i−1
− − − ⌊ 2 ⌋
(c) 4-gramwhereeachpositioniattendstoi 1,i 2,i 3.
− − −
Figure 5: Multiple Parents: We show three examples of trained transformers on Task 2 with
k = 2,2,3 respectively. The left column shows the adjacency matrix of the causal graphs . To
G
theirright,weplottheattentionpatterns
(A(1))foreachheadiwhereA(1)
istheposition-position
S i i
componentofA(1)
. Weseethateachattentionheadlearnsasinglesetofparentsinthecausalgraph
i
,whichagreeswithConstruction2.
SeeFigure7forplotsofthefullmatricesA(1)
.
G i
(cid:101)
(cid:101)
the property that either p(i) = or p(i) = k. If p(i) = , we write p(i) = p(i) ,...,p(i) ,
1 k
∅ | | ̸ ∅ { }
where p(i) < < p(i) . As before, let = i [T + 1] : p(i) = be the root nodes. We
1 k
··· R { ∈ ∅}
additionallyassumethatT +1 .
̸∈ R
We now consider k-parent transition tensors π: For any a ,...,a [S], π( a ,...,a ) is a
1 k 1 k
∈ ·|
probability distribution over [S]. Let Pk be a prior over such π. Each sequence is now generated
π
asfollows:
Task2(GraphswithMultipleParents).
1. Drawπ Pk.
∼ π
2. For i = 1,...,T + 1, if p(i) = , sample s Unif([S]). Otherwise, sample s
i i
∅ ∼ ∼
π( s ,...,s )
·|
p(i)1 p(i)
k
3. Returntheinputx = s andthetargety = s .
1:T T+1
Example. One example of Task 2 is learning in-context n-grams. In an n-gram model, each
tokenonlydependsonthepriorn 1tokensinthesequence. Thisn-grammodelcanbeobtained
−
by setting k = n 1, letting the root nodes be = [n 1], and choosing the parent sets p(i) =
− R −
i n+1,i n+2,...,i 1 for i n. The conditional density P(s
k+n
s k+1:k+n−1) is then
{ − − − } ≥ |
just the transition π(s s ,...,s ); the goal is to estimate this transition in-context, by
k+n k+1 k+n−1
|
firstlearningthatallsequencessharethesamen-gramcausalstructure.
16Given a sequence s , a good estimate for the transition π is the empirical transition πˆ(s′
1:T
|
a ,...,a ),definedas
1 k
i : s = s′,s = a ,...,s = a
πˆ (s′ a ,...,a ) := { i p(i)1 1 p(i) k k } (30)
s1:T
|
1 k
i : s = a ,...,s = a
(cid:12) { p(i)1 1 p(i) k k } (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
We explicitly construct a two-layer transform(cid:12) er with k heads in the first laye(cid:12)r that approximately
expressesthisempiricaltransition.
Construction2. Thereexistsatwoattentionlayertransformerf withk headssuchthat
θ˜
f (s ) πˆ (s′ s ,...,s ) (31)
θ˜ 1:T s′
≈
s1:T
|
p(T+1)1 p(T+1)
k
Construction2isdeferredtoAppendixB.Atahighlevel,theℓthheadinthefirstlayercopiesp(i)
ℓ
to the residual stream of i, and copies p(T +1) to the residual stream of T; the second attention
ℓ
head compares these tuples of parents, and thus attends to tokens i where s = s for all
p(i) p(T+1)
ℓ ℓ
ℓ [k].
∈
In Figures 5 and 7, we show empirically that transformers trained on Task 2 for varying latent
graphs indeedconvergetosuchaconstruction. Thechallenge,however,inanalyzingthegradient
G
descent dynamics is that there are multiple attention heads each of which attends to a different
parent. The dynamics must thus break the symmetry between the multiple heads. Analyzing the
optimization dynamics of a multi-head transformer for solving Task 2 is thus a very interesting
directionforfuturework.
Acknowledgements
EN acknowledges support from a National Defense Science & Engineering Graduate Fellowship.
ADacknowledgessupportfromaNSFGraduateResearchFellowship. EN,AD,andJDLacknowl-
edgesupportoftheAROunderMURIAwardW911NF-11-1-0304,theSloanResearchFellowship,
NSF CCF 2002272, NSF IIS 2107304, NSF CIF 2212262, ONR Young Investigator Award, and
NSFCAREERAward2144994
17References
[1] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn
to implement preconditioned gradient descent for in-context learning. arXiv preprint
arXiv:2306.00297,2023.
[2] EkinAkyu¨rek,DaleSchuurmans,JacobAndreas,TengyuMa,andDennyZhou. Whatlearn-
ingalgorithmisin-contextlearning? investigationswithlinearmodels. InTheEleventhInter-
national Conference on Learning Representations, 2023. URL https://openreview.
net/forum?id=0g0X4H8yN4I.
[3] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statis-
ticians: Provable in-context learning with in-context algorithm selection. arXiv preprint
arXiv:2306.04637,2023.
[4] Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth
ofatransformer: Amemoryviewpoint. arXivpreprintarXiv:2306.00802,2023.
[5] Enric Boix-Adsera, Etai Littwin, Emmanuel Abbe, Samy Bengio, and Joshua Susskind.
Transformerslearnthroughgradualrankincrease. arXivpreprintarXiv:2306.07042,2023.
[6] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language
models are few-shot learners. Advances in neural information processing systems, 33:1877–
1901,2020.
[7] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter
Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learn-
ing via sequence modeling. Advances in neural information processing systems, 34:15084–
15097,2021.
[8] CKCN Chow and Cong Liu. Approximating discrete probability distributions with depen-
dencetrees. IEEEtransactionsonInformationTheory,14(3):462–467,1968.
[9] Joel E. Cohen, Yoh Iwasa, Gh. Rautu, Mary Beth Ruskai, Eugene Seneta, and Gh. Zbaganu.
Relative entropy under mappings by stochastic matrices. Linear Algebra and its Applica-
tions, 179:211–235, 1993. ISSN 0024-3795. doi: https://doi.org/10.1016/0024-3795(93)
90331-H. URL https://www.sciencedirect.com/science/article/pii/
002437959390331H.
[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv
preprintarXiv:2010.11929,2020.
[11] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,
Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep
Ganguli,ZacHatfield-Dodds,DannyHernandez,AndyJones,JacksonKernion,LianeLovitt,
18Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish,
and Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits
Thread,2021. https://transformer-circuits.pub/2021/framework/index.html.
[12] Dan Friedman, Alexander Wettig, and Danqi Chen. Learning transformer programs. arXiv
preprintarXiv:2306.01128,2023.
[13] Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. Transformers learn higher-order
optimization methods for in-context learning: A study with linear models. arXiv preprint
arXiv:2310.17086,2023.
[14] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers
learn in-context? a case study of simple function classes. Advances in Neural Information
ProcessingSystems,35:30583–30598,2022.
[15] Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dim-
itris Papailiopoulos. Looped transformers as programmable computers. arXiv preprint
arXiv:2301.13196,2023.
[16] Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. arXiv
preprintarXiv:2310.05249,2023.
[17] Samy Jelassi, Michael Sander, and Yuanzhi Li. Vision transformers provably learn spatial
structure. AdvancesinNeuralInformationProcessingSystems,35:37822–37836,2022.
[18] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ron-
neberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zˇ´ıdek, Anna Potapenko, et al.
Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583–589,
2021.
[19] DiederikP.KingmaandJimmyBa. Adam: Amethodforstochasticoptimization,2017.
[20] Arvind Mahankali, Tatsunori B Hashimoto, and Tengyu Ma. One step of gradient descent is
provablytheoptimalin-contextlearnerwithonelayeroflinearself-attention. arXivpreprint
arXiv:2307.03576,2023.
[21] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain,
Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jack-
son Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared
Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. Trans-
formerCircuitsThread,2022. https://transformer-circuits.pub/2022/in-context-learning-and-
induction-heads/index.html.
[22] Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak. Trans-
formersassupportvectormachines. arXivpreprintarXiv:2308.16898,2023.
19[23] Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak. Max-margin
tokenselectioninattentionmechanism. InThirty-seventhConferenceonNeuralInformation
ProcessingSystems,2023.
[24] Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and snap: Under-
standing training dynamics and token composition in 1-layer transformer. arXiv preprint
arXiv:2305.16380,2023.
[25] Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Du. Joma: De-
mystifying multilayer transformers via joint dynamics of mlp and attention. arXiv preprint
arXiv:2310.00535,2023.
[26] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-
tionprocessingsystems,30,2017.
[27] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joa˜o Sacramento, Alexander
Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by
gradient descent. In International Conference on Machine Learning, pages 35151–35174.
PMLR,2023.
[28] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of
in-contextlearningasimplicitbayesianinference. arXivpreprintarXiv:2111.02080,2021.
[29] Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models
in-context. arXivpreprintarXiv:2306.09927,2023.
20A Disentangled Transformer Equivalence
Theorem 3. For any transformer TF with any hidden dimension d, there exists a disentangled
θ
transformer TF with the same depth and number of heads such that TF (s ) = TF (s ) for
θ˜ θ 1:T θ˜ 1:T
any input sequence s [S]T. Likewise, for any disentangled transformer TF , there exists a
1:T
∈
θ˜
transformer T(cid:102)F with the same depth and number of heads and with hidden dimens(cid:102)ion d(L) such
θ
thatTF (s ) = TF (s )foranys [S]T. (cid:102)
θ 1:T θ˜ 1:T 1:T
∈
Proof. Let (cid:102)
θ = (Q(ℓ),K(ℓ),V(ℓ)) E,P,W and θ˜ = A(ℓ) W˜ .
{ i i i ℓ∈[L],i∈[m ℓ] }∪{ O } { i }ℓ∈[L],i∈[m ℓ] ∪{ O }
Note that the reverse direction is clear as any disentangled transformer is also a transformer with
hiddendimensiond :
L
0
I S×T
E =
0
S
∈
RdL×S P =

I
T  ∈
RdL×T W
O
= W˜
O
(cid:20) (dL−S)×S (cid:21) 0
(dL−d)×T
  0
A(ℓ) 0 I 0 i·d ℓ×d ℓ
Q( iℓ) = 0i
0 ∈
RdL,dL K i(ℓ) = 0d ℓ
0 ∈
RdL,dL V i(ℓ) =

I
d ℓ
0
dL×(dL−d ℓ)
.
(cid:20) (cid:21) (cid:20) (cid:21) 0
(dL−(i+1)·d ℓ)×d
ℓ
 
˜
Wewillprovethateverytransformerθcanberepresentedbyadisentangledtransformerθ. Wewill
begin by defining a sequence of matrices Z(ℓ) Rd×d ℓ for ℓ 0,...,L . Let Z(0) := [E,P]
∈ ∈ { } ∈
Rd×d0,andforℓ > 1let
Z(ℓ) := Z(ℓ−1) V 1(ℓ)Z(ℓ−1)
···
V m(ℓ ℓ)Z(ℓ−1)
∈
Rd×d ℓ.
(cid:104) (cid:105)
Thenwedefine
A( iℓ) := (Z(ℓ−1))⊤Q( iℓ)(V i(ℓ))⊤Z(ℓ−1)
∈
Rd ℓ−1×d ℓ−1 and W˜
O
= W OZ(L).
We will prove by induction that for any sequence s , h(ℓ) = h˜(ℓ)(Z(ℓ))⊤ for ℓ = 0,...,L where
1:T
h(ℓ) is the residual stream of TF and h˜(ℓ) is the residual stream of TF . First, when ℓ = 0
{ }
θ
{ }
θ˜
wehavethat
(cid:103)
e
h(0) = Ee +Pe = E P si = Z(0)h˜(0).
i si i e i
i
(cid:20) (cid:21)
(cid:2) (cid:3)
21Next,assumetheresultforℓ 1 0. Then
− ≥
m
ℓ
h(ℓ) = h(ℓ−1) + attn
h(ℓ−1);Q(ℓ)K(ℓ)⊤ V(ℓ)⊤
i i i
(cid:88)i=1 (cid:16) (cid:17)
m
ℓ
= h˜(ℓ−1)(Z(ℓ−1))⊤ + attn h˜(ℓ−1)(Z(ℓ−1))⊤;Q(ℓ)(K(ℓ))⊤ (V(ℓ))⊤
i i i
(cid:88)i=1 (cid:16) (cid:17)
m
ℓ
= h˜(ℓ−1)(Z(ℓ−1))⊤ + attn h˜(ℓ−1);(Z(ℓ−1))⊤Q(ℓ)(K(ℓ))⊤Z(ℓ−1) (Z(ℓ−1))⊤(V(ℓ))⊤
i i i
(cid:88)i=1 (cid:16) (cid:17)
m
ℓ
= h˜(ℓ−1)(Z(ℓ−1))⊤ + attn h˜(ℓ−1);A(ℓ) (V(ℓ)Z(ℓ−1))⊤
i i
(cid:88)i=1 (cid:16) (cid:17)
=
h˜(ℓ)(Z(ℓ))⊤
whichcompletestheinduction. Therefore,
TF (s ) = h(L)W⊤ = h˜(L)(Z(ℓ))⊤W⊤ = h(L)(W Z(ℓ))⊤ = h(L)W˜ ⊤ = TF (s )
θ 1:T O O O O θ 1:T
whichcompletestheproof.
(cid:102)
B Multiple Parents Construction
WenowpresentConstruction2.
Proof. Let X˜ RT×d be the embedding of the sequence. Recall that the ℓth attention block is of
∈
theform
attn(X˜ ;A(1)) := (X˜ A(1)X˜⊤)X˜ RT×d,
ℓ S ℓ ∈
and
(cid:101) (cid:101)
x
i
attn(X˜ ;A(1))
h(1) =  . 1 i  R(k+1)d.
i . . ∈
 attn(X˜ ;A(cid:101)(1)) .

 k i 
 
The ℓth attention head performs two roles. For i < T, it copies p(i) to the residual stream of i.
(cid:101) ℓ
Additionally,itcopiesp(T +1) totheresidualstreamofT.
ℓ
Formally, let
A(1)
follow the same sparsity pattern as the construction in Construction 1, where
ℓ
onlytheposition-positionblockA(1)
isnonzero,andonthisblocklet
ℓ
(cid:101)
1(j = p(i) ) i < T
(A(1)) = β ℓ .
ℓ ij 1 · 1(j = p(T +1) ) i = T
(cid:40) ℓ
22Takingβ ,theoutputofthisattentionblockis
1
→ ∞
x˜ i T
attn(X˜ ;A(1)) = p(i) ℓ ∈ R\{ } .
ℓ i x˜ i = T
(cid:40) p(T+1)
ℓ
(cid:101)
We let A(2)
∈
R(k+1)d×(k+1)d be the block diagonal matrix which compares the attn(X˜ ;A( ℓ1))
i
componentsoftheresidualstreamsofh(1)
toeachotherviatheirtokenembeddings.
i
(cid:101) (cid:101)
Formally,welet
0 0 0 0
d×d d×d d×d d×d
···
0 A(2) 0 0
d×d 1 d×d d×d
 ··· 
A(2) = 0 0 A(2) 0
d×d d×d 2 d×d


. .
.
. .
.
. .
.
· .· ..· . .
.


 
(cid:101) 0 0 0 A(2)
 d×d d×d d×d ··· k 
 
whereeachA(2) Rd×d is
k ∈
I 0
A(2) = β S×S S×T .
k 2 0 0
T×S T×T
(cid:20) (cid:21)
Wethushave,fori T .
∈ R\{ }
k
h(1)⊤ A(2)h(1) = β attn(X˜ ;A(1)) ⊤ A(2)attn(X˜ ;A(1))
i T 2 ℓ i k ℓ T
(cid:88)ℓ=1 (cid:16) (cid:17)
(cid:101) k (cid:101) (cid:101)
= β 1(s = s )
2 p(i) p(T+1)
· ℓ ℓ
ℓ=1
(cid:88)
Taking β , the softmax converges to a uniform distribution over tokens where h(1)⊤ A(2)h(1)
2 → ∞ i T
is maximized. These are the tokens i in which s = s for all ℓ, along with the token T2.
p(i) p(T+1)
ℓ ℓ
Thus
1 +1 s = s , ,s = s
h(1)A(2)h(1) i=T p(i)1 p(T+1)1 ··· p(i) k p(T+1) k .
S (cid:16) T (cid:17)i ≈ 1+ j<T 1 (cid:0) s p(j)1 = s p(T+1)1, ··· ,s p(j) k = s p(T+1(cid:1)) k
(cid:101) (cid:80) (cid:0) (cid:1)
Finally,chooseW tooutputthetokenembeddingofthex blockofh(1)(X) ,sothath(1)(X)W =
O i i O
2Itispossibleforcertainrootnodesatthebeginningofthesequencetobeincluded, butthiswillbeavanishing
fractionoftokensfortypicalsequences
23e . Wethenhavethat
si
f (s ) = 1(s = s′) h(1)A(2)h(1)
θˆ 1:T s′ i ·S T i
(cid:88)i (cid:16) (cid:17)
1(s = s′)+ 1 s(cid:101) = s′,s = s , ,s = s
T i<T i p(i)1 p(T+1)1 ··· p(i) k p(T+1) k
≈ 1+ 1 s = s , ,s = s
(cid:80)j<T (cid:0)p(j)1 p(T+1)1 ··· p(j) k p(T+1) k (cid:1)
1 s = s′,s = s , ,s = s
i i (cid:80)p(i)1 (cid:0) p(T+1)1 ··· p(i) k p(T+1) k (cid:1)
≈ 1 s = s , ,s = s
(cid:80) (cid:0)j p(j)1 p(T+1)1 ··· p(j) k p(T+1) k (cid:1)
= πˆ s′ s ,...,s ,
s1:T(cid:80) |(cid:0)p(T+1)1 p(T+1)
k
(cid:1)
(cid:0) (cid:1)
asdesired.
C Additional Experiments and Details
All single-parent experiments were run with a vocabulary size of S = 10, a sequence length of
T = 20,abatchsizeof1024,α = 0.1,andlearningrateη = 0.3. WeinitializeA(1) = 0,A(2) = 0,
andW = 0.
O
(cid:101) (cid:101)
InFigure6,werepeatFigure2forthein-contextlearninggraphinfig.6(b),inadditiontoversions
whenthegraph comesfromaMarkovchain(fig.6(a))andwhenitisrandomgraph(fig.6(c)).
G
For experiment with multiple parents (Figure 7), we used α = 1 and Adam [19] with η = 0.01
but we initialized A(1),A(2) N(0,σ2) for σ = 0.01. This was necessary to break the symmetry
ij ij ∼
betweentheheads.
(cid:101) (cid:101)
24(a) MarkovChain
(b) In-ContextLearning
(c) RandomCausalGraph
Figure6: A(1) encodesthegraphstructure,fordifferentlatentgraphs.
(cid:101)
25(a) 3-gramwhereeachpositioniattendstoi 1,i 2.
− −
(b) Eachpositioniattendstoi 1and i−1
− ⌊ 2 ⌋
(c) 4-gramwhereeachpositioniattendstoi 1,i 2,i 3
− − −
Figure 7: Multiple Parents: On the left, we plot the causal graph in the setting of Section 6 with
k = 2,2,3respectively. Thefirstrowcorrespondstothe3-gramtaskinwhicheachtokendepends
on the previous 2. In the second row, each token at position i depends on the previous token and
the token at position i−1 . The third row corresponds to 4-gram in which each token depends on
⌊ 2 ⌋
the previous 3 tokens. We train two-layer disentangled transformers on these tasks with k heads
in each layer. On the right, we plot the first layer attention matrices, i.e.
A(1)
. We see that
{ i }i
each attention head learns a single set of parents in the causal graph , which agrees with our
G
Construction2. (cid:101)
26D Analyzing the Dynamics
InthissectionweproveTheorem1.
D.1 Proof of Lemma 1
Proof. Theoutputofthefirstattentionlayeris
attn(X˜ ;A(1)) = (MASK(X˜ A(1)X˜⊤)X˜ = (MASK(A(1)))X˜ .
S S
Next,wehavethat
(cid:101) (cid:101)
h(1)⊤ A(2)h(1)⊤ = x˜⊤ A(2) 0 S×T attn(X˜ ;A(1))⊤
T T 0 0
T×S T×T
(cid:20) (cid:21)
A(2) 0
(cid:101) = x˜⊤ S×T X˜⊤ (MA(cid:101) SK(A(1)))⊤
T 0 0 S
T×S T×T
(cid:20) (cid:21)
= x⊤A(2)X⊤ (MASK(A(1)))⊤.
T S
Thustheoutputofthesecondattentionlayeris
attn(h(1);A(2)) = h(1)⊤ h(1) A(2) ⊤ h(T)
T
S
(cid:18) (cid:19)
(cid:16) (cid:17)
=
h(cid:101)(1)⊤ (MASK(A(1)))(cid:101)XA(2)⊤
x
T
S S
(cid:16) (cid:17)
Finally,theoutputis
TF (s ) = W⊤h(2)
θ(cid:101) 1:T O T
= I 0 0 attn(h(1);A(2))
S S×T S×d T
(cid:102) (cid:102) |
= (cid:2)I 0 0
(cid:3)h(1)⊤ (MASK(A(1)))XA(2)⊤
x
S S×T | S×d S S (cid:101) T
= X(cid:2) ⊤ (MASK((cid:3)A(1)))X(cid:16) A(2)⊤ x , (cid:17)
T
S S
(cid:16) (cid:17)
asdesired.
D.2 Notation
We briefly introduce notation which will be used throughout the rest of the appendix. We let
X RT×S be the token embedding of the sequence s 1:T. Additionally, for a lower triangular
∈
matrix A RT×T, let A
i
Ri denote the first i coordinates of the ith row of A. We overload
∈ ∈
notation so that (A) RT×T is the lower triangular matrix satisfying (A)
i
= (A i); i.e, the
S ∈ S S
softmaxoperationisappliedrow-wisetothefirsticoordinatesofrowi. Finally,wereparameterize
A(2)⊤ withA(2).
Wecanthusrewritethereducedmodelf as
θ
f (s ) = X⊤ (A(1))XA(2)x .
θ 1:T T
S S
(cid:0) (cid:1)
27We let f θ(X;s) denote prediction of a transformer with embedding X RT×S conditioned on
∈
s = s,i.e
T
f (X;s) := X⊤ (A(1))XA(2)e .
θ s
S S
Itiseasytoseethattheperturbedloss(14)canb(cid:0)ewrittenas (cid:1)
1
L(θ) = Eπ,X π(s′ s)log(f θ(X;s) s′ +ϵ) ,
−S  | 
s,s′∈[S]
(cid:88)
 
where we use EX and Es1:T interchangeably to represent expectation over the sequence s 1:T. We
settheperturbationasϵ = T−1/2 .
eff
Fornotationalconvenience,wedefine
v (X;s) := ( (A(1))XA(2)e ),
θ s
S S
sothatf (X;s) = X⊤v (X;s).
θ θ
Letδ s(X)
∈
RT bethevectorwhereδ s(X)
i
= x i,s,andletµˆ X(s) := T1 ⊤ i=1x
i,s
betheempirical
estimate of the frequency of s over the sequence X. We let X
≤i
Ri×S be the embedding of the
∈ (cid:80)
firstitokensinthesequence,andletδ s(X ≤i) Ri betheindicatorofsonthesefirstitokens.
∈
Givenavectorv Rk,theoperatorJ
k
: Rk Rk×k isgivenbyJ k(v) = diag(v) vv⊤. J
k
isthe
∈ → −
Jacobianof : (u) = J ( (u)). Wedropthesubscriptk whenitisclearfromcontext.
u k
S ∇ S S
D.3 Heuristic Derivation of Lemma 3
Duringstage1,themodelcanberewrittenas
f (X;s) = e⊤X⊤ β (A(1))Xe = δ (X)⊤ β (A(1))δ (X) .
θ s′ s′
S
0
S
s s′
S
0
S
s
Whenβ 0,wecanlinearizethe(cid:0)outersoftmaxa(cid:1)s (cid:0) (cid:1)
0
≈
1 1 1
(β z) 1 +β I 1 1⊤ z,
S 0 ≈ T T 0 · T T − T2 T T
(cid:18) (cid:19)
andgetthat
1 β β
f (X;s) δ (X)⊤1 + 0 δ (X)⊤ (A(1))δ (X) 0 δ (X)⊤1 1⊤ (A(1))δ (X)
θ s′ ≈ T s′ T T s′ S s − T2 s′ T · TS s
β
= µˆ (s′)+ 0 δ (X)⊤ (A(1))δ (X) µˆ (s′) 1⊤ (A(1))δ (X) . (32)
X T s′ S s − X · TS s
(cid:0) (cid:1)
First,observethatsinceβ 0,
0
≈
f (X;s) µˆ (s′)
θ s′ X
≈
Next,takingthegradientoftheapproximation(32)withrespecttoA(1)
yields
i
β
f (X;s) 0 J (A(1)) δ (X ) (x µˆ (s′)).
∇A( i1) θ s′ ≈ T S i s ≤i · i,s′ − X
(cid:16) (cid:17)
28Thereforebythechainrule,thepopulationgradientisgivenby
1 π(s′ s)
∇A( i1)L(θ)
≈ −S
Eπ,X
(cid:34)
f
θ(X;s)|
s′
+ϵ∇A( i1)f θ(X;s) s′ (cid:35).
s,s′
(cid:88)
β π(s′ s)
≈
−ST0 J S(A( i1)) ·Eπ,X
µˆ
(|
s′)
(x i,s′ −µˆ X(s′))δ s(X ≤i) .
(cid:34) X (cid:35)
(cid:16) (cid:17)
(cid:88)s,s′
Lettinggˆ idenotethetermafterthepreconditioner,i.egˆ i := Eπ,X s,s′ µπ ˆX(s (′ s|s ′) )(x i,s′ −µˆ X(s′))δ s(X ≤i) ,
wegetthatthejthentryofgˆ i,gˆ i,j,is (cid:104)
(cid:80)
(cid:105)
π(s′ s)
gˆ i,j = Eπ,X µˆ (| s′) (x i,s′ −µˆ X(s′))x j,s
(cid:34) X (cid:35)
s,s′
(cid:88)
π(s′ s)
= Eπ,X µˆ (| s′) x i,s′x j,s − π(s′ | s)x j,s
(cid:34) X (cid:35)
s,s′ s,s′
(cid:88) (cid:88)
π(s′ s)
= Eπ,X µˆ (| s′) x i,s′x j,s −1.
(cid:34) X (cid:35)
s,s′
(cid:88)
Conditioned on π, as the effective length of the sequence X grows large, due to our assumptions
onP thesequencex ,...,x mixes,andthusµˆ (s′) µ (s). Assuch,
π 1 T X π
→
π(s′ s)
gˆ i,j ≈ Eπ µ (s| ′) EX[x i,s′x j,s] −1
(cid:34) π (cid:35)
s,s′
(cid:88)
π(s′ s)
= Eπ µ (s| ′) PX[s j = s,s i = s′] −1
(cid:34) π (cid:35)
s,s′
(cid:88)
= g .
i,j
D.4 Gradient Computations
RecallthatA(1) Ri istheithrowofA(1). Definethepopulationgradientsas
i ∈
G(1)(A(1),A(2)) := L(θ)
i ∇A( i1) θ=(A(1),A(2))
G(2)(A(1),A(2)) := L(θ)(cid:12) .
∇A(2) (cid:12)θ=(A(1),A(2))
(cid:12)
Thefollowinglemmacomputesthepopulationgradients: (cid:12)
Lemma4(Populationgradients).
1 π(s′ s)
G(1)(A(1),A(2)) i = −SJ( S(A( i1))) Eπ,X
f
(X;s)| +ϵδ s′(X)⊤J(v θ(X;s))e i ·X ≤iA(2)e s
θ s′
s,s′ (cid:20) (cid:21)
(cid:88)
1 π(s′ s)
G(2)(A(1),A(2)) =
−S
E
f
(X;s)|
+ϵ
·X⊤ S(A(1))⊤J(v θ(X;s))δ s′(X)e⊤
s
θ s′
s,s′ (cid:20) (cid:21)
(cid:88)
29Proof. ThemodelgradientwithrespecttoA(1) is
i
f (X;s) = X⊤J(v (X;s))e J( (A(1)))X A(2)e
∇A( i1) θ θ i ⊗ S i ≤i s
Thereforethelossgradientisgivenby
1 π(s′ s)
G(1)(A(1),A(2)) i = E | f θ(X;s) s′
−S f (X;s) +ϵ∇
θ s′
s,s′ (cid:20) (cid:21)
(cid:88)
1 π(s′ s)
= −SJ( S(A( i1))) E
f
(X;s)| +ϵδ s′(X)⊤J(v θ(X;s))e i ·X ≤iA(2)e s .
θ s′
s,s′ (cid:20) (cid:21)
(cid:88)
Next,themodelgradientofA(2) is
f (X;s) = X⊤ (A(1))⊤J(v (X;s))δ (X)e⊤.
∇A(2) θ s′ S θ s′ s
Thus
1 π(s′ s)
G(2)(A(1),A(2)) =
−S
E
f
(X;s)|
+ϵ
·X⊤ S(A(1))⊤J(v θ(X;s))δ s′(X)e⊤
s
θ s′
s,s′ (cid:20) (cid:21)
(cid:88)
D.5 Gradient of A(1) (Stage 1)
Weshowthatduringthefirststageoftraining,A(1) convergestotheadjacencymatrixofthegraph
.
G
The first step is to show that a quantity called the “idealized gradient” approximately aligns with
theadjacencymatrixof . Foratransitionmatrixπ,define
G
π(s′ s)
g i,j(π) :=
µ
(s|
′)
·PX[s
i
= s′,s
j
= s] −1,
π
s,s′
(cid:88)
andletg
i,j
:= Eπ[g i,j(π)].
Thefollowinglemmashowsthatthisidealizedgradientismaximizedatj = p(i). Theproofrelies
onthedataprocessinginequalityargument,andisdeferredtoAppendixG.1.
Lemma5(Idealizedgradientisalignedwith ). Ifp(i) = ,then
G ̸ ∅
γ3
g g +
i,p(i) i,j
≥ 2S
forallj [i] p(i). Otherwiseg = 0.
i,j
∈ \
Next, we show that the true gradient with respect to A(1) can indeed be approximated by this
idealizedgradient,andhencetheadjacencymatrixof .
G
30Lemma6(TruegradientofA(1)isalignedwith (Stage1)). LetA(2) = β I. Thereexistconstants
0
G
c ,C suchthat,ifβ c T−3/2,
γ,S γ,S 0 ≤ γ,S eff
• Ifp(i) = ,
∅
G(1)(A(1),A(2)) = J( (A(1)))v
i S i
forv with v C β0 .
∥ ∥∞ ≤ γ,S T√Teff
• Ifp(i) = ,thenforanyj = p(i),
̸ ∅ ̸
C β
G(1)(A(1),A(2)) G(1)(A(1),A(2)) (A(1)) 1 (A(1)) γ,S 0 .
i,p(i) ≤ i,j −S i p(i) −S i p(i) · T
(cid:16) (cid:17)
Proof. First,seethat
X A(2)e = β X e = β δ (X ).
≤i s 0 ≤i s 0 s ≤i
Thus
1 π(s′ s)
G(1)(A(1),A(2)) i = −β 0J( S(A( i1)))
· S
Eπ,X
f
(X;s)| +ϵδ s′(X)⊤J(v θ(X;s))e i ·δ s(X ≤i) .
θ s′
s,s′ (cid:20) (cid:21)
(cid:88)
Letθˆ := (A(1),0),anddefinethequantitiesg∗,gˆ by
i i
π(s′ s)
g i∗ := T Eπ,X
f
(X;s)| +ϵδ s′(X)⊤J(v θ(X;s))e i ·δ s(X ≤i) ,
θ s′
s,s′ (cid:20) (cid:21)
(cid:88)
π(s′ s)
gˆ i := T Eπ,X
f
(X;s)| +ϵδ s′(X)⊤J(v θˆ(X;s))e i ·δ s(X ≤i) .
s,s′ (cid:20)
θˆ s′
(cid:21)
(cid:88)
Weremarkthat
β
G(1)(A(1),A(2)) = 0 J( (A(1)))g∗.
i −ST S i i
Sinceβ c 1 1,byLemma23wehave
0 ≤ γ,S T3/2 ≤
eff
1
gˆ g∗ 6S2ϵ−2β C .
∥ i − i∥∞ ≤ 0 ≤ γ,S √T
eff
Itthussufficestoanalyzegˆ. Notethatv (X;s) = 11 . Therefore
i θˆ T T
1
f (X;s) = 1⊤δ (X) = µˆ (s′).
θˆ s′ T T s′ X
and
1 1 1
δ (X)⊤J(v (X;s))e = δ (X)⊤ I 1 1⊤ e = (x µˆ (s′)).
s′ θˆ i s′ T T − T2 T T i T i,s′ − X
(cid:18) (cid:19)
31Thejthentryofgˆ,gˆ ,isthusequalto
i i,j
π(s′ s)
gˆ i,j = Eπ,X
µˆ
(s′)| +ϵ(x i,s′ −µˆ X(s′))x j,s .
X
s,s′ (cid:20) (cid:21)
(cid:88)
ByLemma24,thisisapproximatelyequaltotheidealizedgradientg :
i,j
1
gˆ g C .
i,j i,j γ,S
| − | ≤ √T
eff
We are now ready to prove the theorem. First, consider the case where p(i) = . By Lemma 5,
∅
g = 0,andthus
i,j
1
g∗ ≲
i,j √T
eff
(cid:12) (cid:12)
SinceG(1)(A(1),A(2)) = β0 J( (A(1)))(cid:12) g∗,t(cid:12) heclaimfollows.
i −ST S i i
Otherwiseifp(i) = ,Lemma5tellsusthat,forallj = p(i),
̸ ∅ ̸
γ3 1 γ3
g∗ g∗ g g + g g∗ + g g∗ +C .
i,j − i,p(i) ≤ i,j − i,p(i) i,j − i,j i,p(i) − i,p(i) ≤ −2S γ,S √T ≤ −4S
eff
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
Next,seethat
β
G(1)(A(1),A(2)) = 0 (A(1)) g∗ (A(1))⊤g∗ (A(1)) .
i,j −ST S i j i,j −S i iS i j
(cid:16) (cid:17)
Thereforeforanyj = p(i),wecanbound
̸
G(1)(A(1),A(2)) G(1)(A(1),A(2))
i,j i,p(i)
−
β
= 0 (A(1)) (A(1)) g∗ (A(1))⊤g∗ + (A(1)) (g∗ g∗ )
ST S i p(i) −S i j i,p(i) −S i i S i j i,p(i) − i,j
β (cid:104)(cid:16) (cid:17)(cid:16) γ3(cid:17) γ3 (cid:105)
0 (A(1)) (A(1)) 1 (A(1)) + (A(1))
≥ ST S i p(i) −S i j −S i p(i) 4S S i j 4S
(cid:20) (cid:21)
(cid:16) (cid:17)(cid:16) (cid:17)
β γ3
(A(1)) 1 (A(1)) 0 ,
≥ S i p(i) −S i p(i) · 4S2T
(cid:16) (cid:17)
asdesired.
We can now analyze the gradient descent dynamics over multiple timesteps. First, we show that
formostrootnodesi ,A(1) movesverylittle.
∈ R i
Lemma7. Leti . Then
∈ R
1 τη β
(A(1)(τ)) ≲ 1 0
S i j − i T T i2
(cid:12) (cid:12) eff ·
(cid:12) (cid:12)
forallj i. (cid:12) (cid:12) (cid:112)
(cid:12) (cid:12)
≤
32Proof. Letr(A(1)) = max A(1) min A(1) . Wehavethat(wherev isthevectorfromLemma6),
i j i,j − j i,j
G(1)(A(1),A(2)) max (A(1)) v ,
i ∞ ≤ j S i j ·∥ ∥∞
(cid:13) (cid:13)
andthus (cid:13) (cid:13)
r(A(1)(t+1)) r(A(1)(t))+2η max (A(1)(t)) v .
i ≤ i 1 j S i j ·∥ ∥∞
Fix ω 1. Assume there exists some t τ such that r(A(1)(t)) > log(1+ω), and let t∗ be the
≤ ≤ i
firstsuchtimet. Wecanalwaysbound
exp r(A(1)(t))
i
max (A(1)(t)) ,
j S i j ≤ (i 1)+(cid:16) exp r(A(1(cid:17))(t))
− i
(cid:16) (cid:17)
andthusfort < t∗,max (A(1)(t)) 1+ω 1+ω. Therefore
j S i j ≤ i+ω ≤ i
log(1+ω) < r(A(1)(t∗)) 2τη v i−1 (1+ω),
i ≤ 1 ∥ ∥∞ ·
Boundinglog(1+ω) ω/2and1+ω 2,wegetthat
≥ ≤
τη β
ω 8τη v i−1 ≲ 1 0 .
≤ 1 ∥ ∥∞ T√T i
eff
·
Additionally,whenr(A(1)(t)) log(1+ω),wehavethebound
i ≤
1 1 1
(1 ω) (A(1)(t)) (1+ω).
i − ≤ 1+(1+ω)(i 1) ≤ S i j ≤ i
−
Therefore
1 ω τη β
(A(1)(τ)) ≲ 1 0 ,
S i j − i ≤ i T√T i2
(cid:12) (cid:12) eff ·
(cid:12) (cid:12)
asdesired. (cid:12) (cid:12)
(cid:12) (cid:12)
Next,weboundthetimeittakesuntil A(1)(t) 1.
S i,p(i) ≈
Lemma8. LetA(2)(0) = β I ,whereβ(cid:0) c (cid:1) 1 .Thereexistsτ ≲ η−1β−1(T2+Tα−1)log(T/α)
0 S 0 ≤ γ,S T3/2 1 1 0
eff
suchthat,foranyt τ ,
1
≥
A(1)(t) 1 α.
S i,p(i) ≥ −
(cid:0) (cid:1)
foralliwithp(i) = .
̸ ∅
33Proof. Byinduction,onehasthatA(1)(t) A(1)(t) throughouttraining. Thus A(1)(t)
i,p(i) ≥ i,j S i,p(i) ≥
1. Additionally,byLemma6,onehasthat A(1)(t) isincreasingint.
T S i,p(i) (cid:0) (cid:1)
Fixi. Define∆(t) = A(1)(t) max (cid:0)A(1)(t)(cid:1). Oneseesthat
i,p(i) j̸=p(i) i,j
−
exp(∆(t))
A(1)(t) .
S i,p(i) ≥ T +exp(∆(t))
(cid:0) (cid:1)
Let τ+(1/2) be the first time t at which A(1)(t) > 1. For t < τ+(1/2) we have 1
S i,p(i) 2 −
A(1)(t) 1,andthusbyLemma6,
S i,p(i) ≥ 2 (cid:0) (cid:1)
(cid:0) (cid:1)
C β
γ,S 0
∆(t+1) ∆(t)+ η .
≥ T2 1
Therefore∆(τ+(1/2)) ≳ β0η1τ+(1/2). Assumethat∆(τ+(1/2)) log(2T). Then
T2 ≥
exp(log(2T)) 2
A(1)(τ+(1/2)) = ,
S i,p(i) ≥ T +exp(log(2T)) 3
(cid:0) (cid:1)
acontradiction. Thus∆(τ+(1/2)) log(2T),soτ+(1/2) ≲ T2η−1β−1log(2T).
1 0
≤
Let τ+(α) be the first time at which (A(1)(τ+(α)) < 1 α. For τ+(1/2) t < τ+(α), we
i,p(i)
S − ≤
thenhave
C β α
γ,S 0
∆(t+1) ∆(t)+ η ,
1
≥ T
andthusifτ+(α) τ+(1/2) ≳ Tα−1β−1log(T/α),
0
−
C β α T
∆(τ+(α)) γ,S 0 η (τ+(α) τ+(1/2)) log
2
≥ T − ≥ α
(cid:18) (cid:19)
Then
exp(log(T/α)) 1
A(1)(τ+(α)) = α 1 α,
S i,p(i) ≥ T +exp(log(T/α)) 1+ 1 ≥ −
α
(cid:0) (cid:1)
acontradiction. Thusτ+(α) τ+(1/2) ≲ Tα−1β−1log(T/α),andsoτ+(α) ≲ T2η−1β−1log(2T)+
0 1 0
Tα−1β−1log(T/α) ≲ η−1β− −1(T2 +Tα−1)log(T/α),asdesired.
0 1 0
Combining the previous two lemmas, the following corollary tells us the value of A(1) after stage
2ofthealgorithm.
Corollary 1 (Ouptut of stage 1). Let β c 1 , and set τ = C η−1β−1T2log(T) for
0 ≤ γ,S T3/2 1 γ,S 1 0
eff
appropriatelychosenconstantsc ,C . Then:
γ,S γ,S
• Ifi ,
∈ R
1 A(1)(τ ) ≲ T−1,
−S 1 i,p(i)
(cid:0) (cid:1)
34• Ifi ,
∈ R
1 T logT
sup (A(1)(τ )) ≲ min 1, .
S i 1 j − i T i2
j∈[i](cid:12) (cid:12) (cid:32) eff · (cid:33)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:112)
Proof. This follows directly from(cid:12) plugging in τ =(cid:12)τ into Lemma 7 and selecting α = Θ(T−1) in
1
Lemma8.
D.6 Gradient of A(2) (Stage 2)
First,weobservethatthepopulationdynamicsofA(2) possessacertainsymmetry:
Lemma9. Foralltime,A(2) = β I +β(I 11 1⊤)forsomescalarβ.
0 S S − S S S
Proof. If A(2) = β I +β 1 1⊤ (all diagonals are equal and all off-diagonals are equal), then by
1 S 2 S S
symmetrythegradientisalsoofthisform. Additionally,seethat
1 π(s′ s)
1⊤ SG(2)(A(1),A(2)) =
−S
Eπ,X
f
(X;s)|
+ϵ
·1⊤ SX⊤ S(A(1))⊤J(v θ(X;s))δ s′(X)e⊤
s
θ s′
s,s′ (cid:20) (cid:21)
(cid:88)
1 π(s′ s)
=
−S
Eπ,X
f
(X;s)|
+ϵ
·1⊤ TS(A(1))⊤J(v θ(X;s))δ s′(X)e⊤
s
θ s′
s,s′ (cid:20) (cid:21)
(cid:88)
1 π(s′ s)
=
−S
Eπ,X
f
(X;s)|
+ϵ
·1⊤ TJ(v θ(X;s))δ s′(X)e⊤
s
θ s′
s,s′ (cid:20) (cid:21)
(cid:88)
= 0,
sinceJ(v (X;s))1 = 0. ThereforeG(2)(A(1),A(2)) = β I 11 1⊤ forsomescalarβ. Since
θ T · S − S S S
weinitializeA(2) = β I,throughouttrainingA(2) isoftheformA(2) = β I +β(I 11 1⊤).
0 (cid:0) (cid:1)0 S S −S S S
Throughouttherestoftheproof,weletβ(t)bethescalarsuchthat
1
A(2)(t) = β(t)I (β(t) β ) 1 1⊤.
S − − 0 S S S
The goal of this section is to show that when A(1) approximates the adjacency matrix of , β(t)
G
willgrowlarge. SincethegradientdescentupdateforA(2) is
A(2)(t+1) = A(2)(t) η G(2)(A(1)(t),A(2)(t)),
2
−
theupdateforβ(t)is
1
β(t+1) = β(t) η Tr G(2)(A(1)(t),A(2)(t)) .
2
− · S 1
−
(cid:0) (cid:1)
Assuch,wedefinethequantity∆ (θ)by
β
1
∆ (θ) := Tr G(2)(A(1),A(2)) .
β
S 1
−
(cid:0) (cid:1)
35Finally,fornotationalconvenience,letA(1) betheT T matrixsuchthat
∗
×
1(j = p(i)) ifi
A(1) = ∈ R .
S ∗ ij (A(1)(τ )) ifi
(cid:40) 1 i,j
S ∈ R
(cid:0) (cid:1)
A(1) encodestheadjacencymatrixof onnodesiwherep(i) = .
∗
G ̸ ∅
Lemma 10 (Stage 2). Let θ = (A(1),A(2)), where A(1) = A(1)(τ ) is the output of stage 1, and
1
A(2) = βI (β β )11 1⊤ forβ 0. Ifβ satisfies
S − − 0 S S S ≥
exp(β) exp(β∗) := C T1/12log−1/6T,
≤ γ,S eff
then
1
1 ∆ (θ) γ8S−6e−2β > 0.
β
≥ − ≥ 4
Proof. NotethatXA(2)e = βXe (β β )11 . Sincetherowsumsof (A(1))are1,
s s − − 0 S T S
β β
(A(1))XA(2)e = β (A(1))Xe − 0 1 ,
s s T
S S − S
andthus
v (X;s) = (β (A(1))Xe ).
θ s
S S
Definez (X;s) = (A(1))Xe . Wehavethat
θ s
S
1
∆ (θ) = Tr G(2)(A(1),A(2))
β
− −S 1
−
1 (cid:2) π(s′(cid:3) s)
= Eπ,X | δ s′(X)⊤J(v θ(X;s)) (A(1))Xe s
S(S 1) f (X;s) +ϵ S
θ s′
− s,s′ (cid:20) (cid:21)
(cid:88)
1 π(s′ s)
= Eπ,X | δ s′(X)⊤J( (βz θ(X;s)))z θ(X;s)
S(S 1) f (X;s) +ϵ S
θ s′
− s,s′ (cid:20) (cid:21)
(cid:88)
1 π(s′ s)
=
S(S 1)
Eπ,X
δ (X)⊤
(βz| (X;s))+ϵδ s′(X)⊤J( S(βz θ(X;s)))z θ(X;s)
s′ θ
− s,s′ (cid:20) S (cid:21)
(cid:88)
Wefirstshowtheupperbound. Wecanwrite
δ (X)⊤J( (βz (X;s)))z (X;s) δ (X) (βz (X;s)) z (X;s)
s′ θ θ s′ i θ i θ i
S ≤ S
i
(cid:88)
δ (X) (βz (X;s))
s′ i θ i
≤ S
i
(cid:88)
= δ (X)⊤ (βz (X;s)),
s′ θ
S
36since0 z (X;s) 1. Therefore
θ i
≤ ≤
1 1
∆ β(θ) Eπ,X [π(s′ s)] = 1.
− ≤ S(S 1) | S 1 ≤
− s,s′ −
(cid:88)
Wenextmovetothelowerbound. Definez˜(X;s) := (A(1))Xe . Wehavethat
∗ s
S
x ifi
p(i),s
z˜(X;s) = ̸∈ R . (33)
i
z (X;s) ifi
(cid:40) θ i
∈ R
First,wewillaimtoreplacez (X;s)withz˜(X;s). Indeed,whenp(i) = ,
θ
̸ ∅
z˜(X;s) z (X;s) = (A(1)) (A(1)) ⊤ δ (X) (A(1)) (A(1)) ≲ T−1,
| i − θ i | S i −S ∗ i s ≤ S i −S ∗ i 1
(cid:12) (cid:12)
(cid:0) (cid:1) (cid:13) (cid:13)
(cid:12) (cid:12)
(cid:13) (cid:13)
byCorollary1. Thus z˜(X;(cid:12)s) z (X;s) ≲ T−1. (cid:12)
∥ − θ ∥∞
Define
δ (X)⊤J( (βz))z
s′
q (z) := S ,
s′
δ (X)⊤ (βz)+ϵ
s′
S
sothat
1
∆ β(θ) = Eπ,X[π(s′ s)q s′(z θ(X;s))].
− S(S 1) |
− s,s′
(cid:88)
ByLemma25,wehavethat
q (z (X;s)) q (z˜(X;s)) ≲ (1+β) z (X;s) z˜(X;s) ≲ (1+β)T−1,
| s′ θ − s′ | ∥ θ − ∥∞
andthus
1
∆ β(θ) Eπ,X[π(s′ s)q s′(z˜(X;s))] ≲ (1+β)T−1.
(cid:12)− − S(S 1) | (cid:12)
(cid:12) − (cid:88)s,s′ (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
Next,plugging(cid:12) inthedefinitionofq s′,weget (cid:12)
1
Eπ,X[π(s′ s)q s′(z˜(X;s))]
S(S 1) |
− s,s′
(cid:88)
1 δ (X)⊤ diag( (βz˜(X;s))) (βz˜(X;s)) (βz˜(X;s))⊤ z˜(X;s)
=
S(S 1)
Eπ,X π(s′
|
s) s′ S
δ
(X)⊤− (S βz˜(X;s))+S
ϵ
− s,s′ (cid:34) (cid:0) s′ S (cid:1) (cid:35)
(cid:88)
1 x (βz˜(X;s)) z˜(X;s)
Eπ,X π(s′ s) i i,s′ S i i (βz˜(X;s)) iz˜(X;s) i .
≥ S(S 1) | · ϵ+ x (βz˜(X;s)) − S
− s,s′ (cid:34) (cid:32) (cid:80) i i,s′ S i i (cid:33)(cid:35)
(cid:88) (cid:88)
(34)
(cid:80)
37Our next goal is to replace the term in the parentheses in (34) with something independent of X,
wheretheconcentrationholdsasT growslarge. Indeed,definethequantitiesE(1)(X),E(2)(X),E(3)(X)
eff s,s′ s,s′ s
by
E(1)(X) := x (βz˜(X;s)) z˜(X;s) (35)
s,s′ i,s′
S
i i
i
(cid:88)
E(2)(X) := x (βz˜(X;s)) (36)
s,s′ i,s′
S
i
i
(cid:88)
E(3)(X) := (βz˜(X;s)) z˜(X;s) , (37)
s S i i
i
(cid:88)
sothat
1 1
E(1)(X)
S(S 1)
Eπ,X[π(s′
|
s)q s′(z˜(X;s))]
≥ S(S 1)
Eπ,X (cid:34)π(s′
|
s)
·
(cid:32)ϵ+s E,s′
(2)(X)
−E s(3)(X) (cid:33)(cid:35).
− s,s′ − s,s′ s,s′
(cid:88) (cid:88)
Letr = |R|. Onecanmaketheapproximation
T
E s( ,1 s) ′(X) (1 r)eβµ π(s)π(s′ s)+reβµπ(s)µ π(s)µ π(s′)
− | (38)
ϵ+E(2)(X) ≈ (1 r)(eβ 1)µ π(s)π(s′ s)+(1 r)µ π(s′)+reβµπ(s)µ π(s′)
s,s′ − − | −
(1 r)eβµ (s)+reβµπ(s)µ (s)
E(3)(X) − π π . (39)
s ≈ (1 r)(eβ 1)µ π(s)+(1 r)+reβµπ(s)
− − −
Thismotivatesdefiningthefollowingidealizedgradient:
1 (1 r)eβπ(s′ s)2 +reβµπ(s)µ (s′)π(s′ s)
π
gˆ(β) :=
S(S 1)
Eπ µ π(s)
· (1 r)(eβ
−
1)µ
π(s)π(s|
′ s)+(1 r)µ
π(s′)+r|
eβµπ(s)µ π(s′)
− (cid:88)s (cid:104) (cid:16)(cid:88)s′ − − | −
(1 r)eβ +reβµπ(s)
−
− (1 r)(eβ 1)µ π(s)+(1 r)+reβµπ(s)
− − − (cid:17)(cid:105)
Indeed, the approximations in (38) and (39) can be made rigorous: by Lemma 30 and Lemma 31,
wehavethat
1 E(1)(X) log1/2T
(cid:12)S(S 1)
Eπ,X (cid:34)π(s′
|
s)
·
(cid:32)ϵ+s E,s′
(2)(X)
−E s(3)(X) (cid:33)(cid:35)−gˆ(β)
(cid:12)
≲ (1+β)
· T1/4
(cid:12) − (cid:88)s,s′ s,s′ (cid:12) eff
(cid:12) (cid:12)
(cid:12) (cid:12)
log1/2T
(cid:12) (cid:12) ≲ eβ .
T1/4
eff
Finally,itsufficestoshowthatgˆ(β) 0. Definethefunctionh s : R Rby
≥ →
(1 r)eβz2 +reβµπ(s)z (1 r)eβ +reβµπ(s)
h (z) = − − .
s (1 r)(eβ 1)µ π(s)z +(1 r)+reβµπ(s)−(1 r)(eβ 1)µ π(s)+(1 r)+reβµπ(s)
− − − − − −
38Simplifyingtheformulaforgˆ(β),weseethatitcanbewrittenintermsofthish :
s
1 π(s′ s)
gˆ(β) = S(S 1) Eπ µ π(s) · µ π(s′)h s µ (s| ′) .
(cid:34) (cid:32) π (cid:33)(cid:35)
− s s′ (cid:18) (cid:19)
(cid:88) (cid:88)
Furthermore, h is convex, and so gˆ(β) is actually a linear combination of h -divergences and is
s s
hence nonnegative. The following lemma relates the h -divergence to the χ2-divergence in order
s
togetaquantitativelowerboundongˆ(β)awayfrom0. TheproofisdeferredtoAppendixG.1.
Lemma11. gˆ(β) 1γ8S−6e−2β > 0.
≥ 2
Toconclude,whenβ β∗,
≤
1 1
∆ β(θ) E[π(s′ s)q(z˜(X;s))] ≲ eβT−1 γ8S−6e−2β
(cid:12)− − S(S 1) | (cid:12) ≤ 8
(cid:12) − (cid:88)s,s′ (cid:12)
(cid:12) (cid:12)
1 (cid:12) E(1)(X) (cid:12) log1/2T 1
(cid:12)S(S 1)
Eπ,X (cid:34)(cid:12)π(s′
|
s)
·
(cid:32)ϵ+s E,s′
(2)(X)
−E s(3)(X) (cid:33)(cid:35)−gˆ(β) (cid:12)(cid:12) ≲ eβ
T1/4 ≤
8γ8S−6e−2β,
(cid:12) − (cid:88)s,s′ s,s′ (cid:12) eff
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12)andthus (cid:12)
1
∆ (θ) γ8S−6e−2β,
β
− ≥ 4
asdesired.
Lemma12(DynamicsofA(2)). LetA(1)(τ )betheoutputofstage1ofAlgorithm1,andletη 1.
1 2
Thereexistsτ ≲ e2β∗β∗η−1 suchthat ≤
2 γ,S 2
1+β∗ β(τ +τ ) β∗.
1 2
≥ ≥
Proof. Ifβ(t) β∗,thenbyLemma10
≤
1 1
β(t+1) β(t)+η γ8S−6e−2β(t) β(t)+η γ8S−6e−2β∗.
2 2
≥ · 4 ≥ · 4
Assumethatβ(τ +t) < β∗ forallt := 4S6γ−8e2β∗β∗η−1. Then
1 2
≤ T
1
β(τ + ) γ8S−6e−2β∗ η = β∗,
1 2
T ≥ 4 T
acontradiction. Thereforeβ(τ +τ ) β∗ forsomeτ ≲ e2β∗β∗η−1. Finally,byLemma10,
1 2 2 2
≥ ≤ T
β(t+1) β(t)+1,andthuslettingτ bethesmallestsuchtimewehave1+β∗ β(τ +τ )
2 1 2
≤ ≥ ≥
β∗.
39D.7 Proof of Theorem 1
ProofofTheorem1. Pickβ c 1 ,andsetτ = C η−1β−1T2log(T)forconstantsc ,C
0 ≤ γ,S T3/2 1 γ,S 1 0 γ,S γ,S
eff
chosenappropriately. ByCorollary1,theoutputofstage1satisfies
1 A(1)(τ ) ≲ T−1.
−S 1 i,p(i)
(cid:0) (cid:1)
fori .
∈ R
Next,byLemma12thereexistsτ = O˜ T1/6η−1 suchthatβ(τ +τ ) β∗.
2 eff 2 1 2 ≥
(cid:16) (cid:17)
ˆ
Itnowsufficestoboundthelossofthepredictorθ. Wehave
1
L(θˆ ) −L∗
≤
Eπ,X
S
π(s′
|
s) ·|log(f θˆ(X;s) s′ +ϵ) −logπ(s′
|
s)
|
(cid:34) (cid:35)
(cid:12) (cid:12) (cid:88)s,s′
(cid:12) (cid:12)
(cid:12) (cid:12) 1
= Eπ
S
π(s′
|
s)EX[ |log(f θˆ(X;s) s′ +ϵ) −logπ(s′
|
s) |]
(cid:34) (cid:35)
s,s′
(cid:88)
ForA,B > 0,onehasthebound
A B
logA logB | − | .
| − | ≤ min(A,B)
Therefore
log(f (X;s) +ϵ) logπ(s′ s)
|
θˆ s′
− | |
1
( f (X;s) π(s′ s) +ϵ)
≤ |
θˆ s′
− | | · min(f (X;s) +ϵ,π(s′ s))
θˆ s′
|
≲ ( f (X;s) π(s′ s) +ϵ) 1 +ϵ−11 ,
| θˆ s′ − | | f θˆ(X;s) s′≥ 4γ S3 f θˆ(X;s) s′< 4γ S3
(cid:16) (cid:17)
andthusbyCauchy
EX |log(f θˆ(X;s) s′ +ϵ) −logπ(s′
|
s)
|
1/2 γ3
≲ EX |f θˆ(X;s) s′ −π(s′
|
s) |2 +ϵ 1+ϵ−1 PX f θˆ(X;s) s′ <
4S
(cid:18) (cid:19)(cid:18) (cid:18) (cid:19)(cid:19)
(cid:16) (cid:17)
1/2
≲ EX |f θˆ(X;s) s′ −π(s′
|
s) |2 +ϵ 1+ϵ−1T e− ff1
(cid:18) (cid:19)
(cid:16) (cid:17)
1/2 (cid:0) (cid:1)
≲ EX |f θˆ(X;s) s′ −π(s′
|
s) |2 +ϵ,
(cid:16) (cid:17)
where the bound PX f θˆ(X;s) s′ < 4γ S3 ≲ T e− ff1 follows from Lemma 32. Altogether, applying
(cid:16) (cid:17)
40Lemma32again,weget
1/2
L(θˆ ) −L∗ ≲ EX |f θˆ(X;s) s′ −π(s′
|
s) |2 +ϵ
(cid:12) (cid:12) (cid:16) eβ∗log1/2T (cid:17)
(cid:12) (cid:12) (cid:12) (cid:12) +e−β∗γ/2 +ϵ
≤ T1/4
eff
γ/2
log1/3T log1/6T
≲ +
T1/6
(cid:32)
T1/12
(cid:33)
eff eff
log2T γ/24
≲ .
T
(cid:18) eff (cid:19)
E Markov Chain Preliminaries
Given a Markov chain π with stationary measure µ , we define the normalized and centered tran-
π
sitionmatrixB
π
RS×S by:
∈
µ (s)
(B ) := π [π(s′ s) µ (s′)].
π s,s′ (cid:115)µ π(s′) | − π
Animmediateconsequenceisthat
µ (s)
(Bk) := π [πk(s′ s) µ (s′)]
π s,s′ (cid:115)µ π(s′) | − π
whichallowsforthedecomposition
µ (s′)
πk(s′ s) = µ (s′)+(Bk) π .
| π π s,s′ (cid:115)µ π(s)
Wealsoobservethat
µ (s)π(s′ s)2 µ (s)π(s′ s)2
B 2 = π | µ (s′)µ (s) = π | 1. (40)
∥ π ∥F µ (s′) − π π µ (s′) −
π π
s,s′ (cid:18) (cid:19) s,s′
(cid:88) (cid:88)
Definition 6 (Spectral Gap). We say that a Markov chain π with stationary measure µ has a
π
spectralgapof1 λ(π)whereλ(π) := B .
π 2
− ∥ ∥
Lemma13. Letmin π(s s′) γ/S. Thenλ(π) 1 γ/S.
s,s′
| ≥ ≤ −
Proof. ByLemma14,wecanwrite
γ
π = 1µ⊤ +(1 γ)Q
S π −
41foranotherstochasticmatrixQ. Onethenseesthatπ⊤Q = π. Therefore
π 1µ⊤ = (1 γ/S)(π 1µ⊤),
− π − − π
so
π 1µ⊤ = (1 γ/S) Q 1µ⊤ 1 γ/S.
∥ −
π∥µπ
− ∥ −
π∥µπ
≤ −
Thereforeλ(π) 1 γ/S.
≤ −
Lemma14. Letmin π(s s′) γ/S. Thenmin µ (s) γ/S.
s,s′ s π
| ≥ ≥
Proof. Sinceµ (s)isstationary,
π
µ (s′) = π(s′ s)µ (s)
π π
|
s
(cid:88)
γ/S µ (s)
π
≥ ·
s
(cid:88)
= γ/S,
asdesired.
Lemma15. Letmin π(s s′) γ/S. Then
s,s′
| ≥
minTV(π( j),π( k)) 1 γ.
j̸=k · | · | ≤ −
Proof. Write
1
TV(π( j),π( k)) = π(s j) π(s k)
· | · | 2 | | − | |
s
(cid:88)
1
= (π(s j)+π(s k) 2min π(s j),π(s k) )
2 | | − { | | }
s
(cid:88)
1 γ.
≤ −
Lemma16. B 2 γ2/S
∥ π ∥F ≥
Proof. Bydefinition
π(s′ s)2µ (s)
B 2 = | π 1,
∥ π ∥F µ (s′) −
π
s,s′
(cid:88)
42andthus
1
B 2 = π(s′ s)2µ (s) µ (s′)2
∥ π ∥F µ (s′) | π − π
π (cid:32) (cid:33)
s′ s
(cid:88) (cid:88)
µ (s)(π(s′ s) µ (s′))2
π π
≥ | −
s,s′
(cid:88)
γ2
µ (s)
π
≥ S
s
(cid:88)
γ2
= .
S
Lemma 17 ([9], Theorem 3.1). Let π be a stochastic matrix such that max π(s′ s) > 0 for all
s
|
s′. Then,foranyf-divergenceD andprobabilityvectorsx,y,
f
D (π x π y) α(π)D (x y),
f f
◦ || ◦ ≤ ||
wherethecontractioncoefficientα(π)isdefinedas
1
α(π) := maxTV(π( j),π( k)) = max π( j) π( k) .
1
j̸=k · | · | 2 j̸=k ∥ · | − · | ∥
F Concentration
Definition 7 (Graph Distance). Let be the directed acyclic graph in Section 2.2. Let G denote
G
the undirected version of . Then we define d(i,j) to be length of the shortest path between i,j in
G
. Ifi,j arenotconnectedinGthend(i,j) := .
G ∞
Definition 8 (Effective Sequence Length). For λ (0,1), we define the effective sequence length
∈
T (λ)by:
eff
T2
T (λ) := .
eff ⊤ λd(i,j)
i,j=1
(cid:80)
ThisformulaforT (λ)iscloselyrelatedtothedefinitionofT (Definition4):
eff eff
Lemma 18. Decompose = k where are disjoint trees. Let L denote the number of
G
i=1Ti Ti i
leavesoftree fori = 1,...,k. Then,
i
T (cid:83)
T(1 λ)
T (λ) − =: (1 λ)T
eff ≥ maxk L − eff
i=1 i
43Proof. NotethatT (λ)−1 naturallydecomposestoasumwithineachtreeasd(i,j) := wheni
eff
∞
andj arenotconnected:
k
1 1
= λd(i,j)
T (λ) T2
eff
(cid:88)l=1 i (cid:88),j∈T l
k
1
= λd(i,j)
T2
(cid:88)l=1 i (cid:88),j∈T l
k
1
= # j : d(i,j) = k λk.
T2 { ∈ Tl }
(cid:88)l=1 (cid:88)i∈T l (cid:88)k≥0
Now note that for a fixed node i, each path from i to j with d(i,j) = k can be lengthened to
a path that reaches a leaf. Furthermore, for each leaf there can be only one such j. Therefore,
# j : d(i,j) = k L .Pluggingthisingives:
l l
{ ∈ T } ≤
k
1 1
L λk
T (λ) ≤ T2 |Tl | l
eff
l=1 k≥0
(cid:88) (cid:88)
k L
=
l=1|Tl
|
l
T2(1 λ)
(cid:80) −
max T
l l
≤ T(1 λ)
−
whichcompletestheproof.
Throughouttheremainderofthissection,theonlyassumptionweplaceonπisthatminπ(s′ s)
γ/S. Definingλ := 1 γ/S,wehavethatλ λ(π)byLemma13,andthusT (λ)−1 ≲ T| −1. ≥
− ≥ eff eff
Lemma19. Foranyπ andanyi,j < T,
PX[x
j
= s,x
i
= s′] µ π(s)µ π(s′) µ π(s)µ π(s′)λ(π)d(i,j).
| − | ≤
(cid:112)
Proof. Letk betheclosestcommonparentofi,j sothatd(k,i)+d(k,j) = d(i,j)andthereexist
directedpathsfromk toiandk toj in . Then,
G
P[s
j
= s,s
i
= s′] µ π(s)µ π(s′)
−
= Cov[x x ]
j,s i,s′
= E[(x
j,s
µ π(s))(x
i,s′
µ π(s′))]
− −
= µ (s )(πd(k,j)(s′ s ) µ (s′))(πd(k,i)(s s ) µ (s))
π k k π k π
| − | −
s ∈[S]
(cid:88)k
µ (s) µ (s′)
= µ (s ) (Bd(k,j)) π (Bd(k,i)) π
π k π s k,s µ (s ) π s k,s′ µ (s )
(cid:32) (cid:115) π k (cid:33)(cid:32) (cid:115) π k (cid:33)
s ∈[S]
(cid:88)k
= µ (s)µ (s′) (Bd(k,j)) (Bd(k,i))
π π π s k,s π s k,s′
s ∈[S]
(cid:112) (cid:88)k
= µ (s)µ (s′)[(Bd(k,j))⊤(Bd(k,i))] .
π π π π s,s′
(cid:112)
44Thereforetakingabsolutevaluesgives:
P[s
j
= s,s
i
= s′] µ π(s)µ π(s′) µ π(s)µ π(s′) B
π
d(k,j)+d(k,i)
| − | ≤ ∥ ∥
(cid:112)µ π(s)µ π(s′)λ(π)d(i,j).
≤
(cid:112)
Lemma20. ForanysubsetI [T 1],define
⊂ −
1
µˆ (s) := x .
XI
I
i,s
| | i∈I
(cid:88)
Then,
µ (s)T2
EX[µˆ XI(s)] = µ π(s) and EX[(µˆ XI(s) −µ π(s))2]
≤ T
π
(λ) I
2.
eff
| |
NotethatLemma20isexcludingthetokenx asitisresampledfromUnif([S]).
T
Proof. The first claim follows from the fact that E[x i,s] = µ π(s) as the sequence X is initialized
fromµ . Then,
π
1
EX[(µˆ XI(s) −µ π(s))2] =
I 2
EX[x i,sx
j,s
−µ π(s)2]
| | i,j∈I
(cid:88)
µ (s)
π λd(i,j)
≤ I 2
| | i,j∈I
(cid:88)
T−1
µ (s)
π λd(i,j)
≤ I 2
| | i,j=1
(cid:88)
µ (s)(T 1)2
π
= −
T (λ) I 2
eff
| |
whichcompletestheproof.
Corollary2.
1
EX[(µˆ X(s) µ π(s))2] ≲ .
− T (λ)
eff
Proof. Onecanwrite
T 1 1
µˆ (s) = − µˆ (s)+ x .
X X T,s
T [T−1] T
Thus
T 1 2 2 1 1
EX[(µˆ X(s) −µ π(s))2] ≤ T− EX µˆ X [T−1](s) −µ π(s) + T2 ≲ T (λ).
(cid:18) (cid:19) (cid:20) (cid:21) eff
(cid:16) (cid:17)
45Lemma21. ForanysubsetI [T 1]suchthatp(i) = foralli I,define
⊂ − ̸ ∅ ∈
1
cˆ (s,s′) := x x .
XI
I
p(i),s i,s
| | i∈I
(cid:88)
Then,
T2
EX[cˆ XI(s,s′)] = µ π(s)π(s′ |s) and EX[(µˆ XI(s) −µ π(s)π(s′ |s))2] ≲
T (λ) I
2.
eff
| |
Proof. ThefirstresultfollowsfromlinearityofexpectationandthefactthattheMarkovprocessis
stationary. Then,
EX[(µˆ XI(s) −µ π(s)π(s′ |s))2]
1
=
I 2
E[x p(i),sx i,s′x p(j),sx j,s′] −µ π(s)2π(s′ |s)2.
| | i,j∈I
(cid:88)
There are three possibilities for the dependency graph of i,j. First, if i = j the expression in
the sum is equal to µ (s)π(s′ s)(1 µ (s)π(s′ s)). Next, if i,j are independent conditioned on
π π
| − |
p(i),p(j),weget
E[x p(i),sx i,s′x p(j),sx j,s′] µ π(s)2π(s′ s)2
− |
= π(s′ s)2(E[x p(i),sx p(j),s] µ π(s)2)
| −
µ (s)π(s′ s)2λd(p(i),p(j)).
π
≤ |
Finally, if i,j are dependent conditioned on p(i),p(j) it means that either there is a directed path
fromitop(j)oradirectpathfromj top(i)inthedirectedgraph . Withoutlossofgenerality,we
G
canassumethatthereisadirectedpathfromj top(i). Thenwehave:
µ (s)π(s′ s)πd(j,p(i))(s s′)π(s′ s) µ (s)2π(s′ s)2
π π
| | | − |
= µ (s)π(s′ s)2 πd(j,p(i))(s s′) µ (s)
π π
| | −
µ (s)µ (s′)π(s′ s)2λd(j,p(i)).
π π (cid:2) (cid:3)
≤ |
Therefore, (cid:112)
EX[(µˆ XI(s) −µ π(s)π(s′ |s))2]
1
≲ λd(i,j)
I 2
| | i,j∈I
(cid:88)
T2
≲ .
T (λ) I 2
eff
| |
Lemma22. ForanysubsetI [T 1]suchthatp(i) = foralli I,
⊂ − ̸ ∅ ∈
2
1 T2
EX
 I
x p(i),s −µ π(s)

≲
T (λ) I
2.
(cid:32) (cid:33) eff
| | i∈I | |
(cid:88)
 
46Proof. Asabove,wewilldirectlycomputethesecondmoment:
1 µ (s)
x x µ (s)2 π λd(p(i),p(j))
I 2 p(i),s p(j),s − π ≤ I 2
| | i,j∈I | | i,j∈I
(cid:88) (cid:88)
µ (s)
π λd(i,j)−2
≤ I 2
| | i,j∈I
(cid:88)
µ (s)
π λd(i,j)
≤ λ2 I 2
| | i,j∈T
(cid:88)
T2µ (s)
π
.
≤ T (λ)λ2 I 2
eff
| |
G Lemmas for Stage 1
G.1 Strong Data Processing Inequality
We briefly recall the definition of the χ2 divergence between two probability distributions on state
space :
X
P(x)2
χ2(P Q) := 1,
|| Q(x) −
x∈X
(cid:88)
alongwiththeχ2 mutualinformationbetweentworandomvariablesY,Z
P(Y = y,Z = z)2
I (Y;Z) = 1
χ2
P(Y = y)P(Z = z) −
y,z∈X
(cid:88)
ProofofLemma5. First we consider the case where i and j are in separate trees. If i = T, then
̸
PX[s
i
= s′,s
j
= s] = µ π(s)µ π(s′),andthus
g (π) = π(s′ s)µ (s) 1 = 0.
i,j π
| −
s,s′
(cid:88)
We note that this subsumes the case where i is a root note, since that necessarily implies that j is
inadifferenttree. Otherwisewheni = T,
1 π(s′ s)µ (s) 1 µ (s′)
π π
g (π) = | 1 = 1 = 0.
i,j S µ (s′) − S µ (s′) −
π π
s,s′ s′
(cid:88) (cid:88)
Next,assumethatiandj areinthesametree. Whenj = p(i),wehave
π(s′ s)
g i,p(i)(π) =
µ
(s|
′)
·PX[s
i
= s′,s
j
= s] −1
π
s,s′
(cid:88)
π(s′ s)2µ (s)
π
= | 1
µ (s′) −
π
s,s′
(cid:88)
= B 2,
∥ π ∥F
47wherethelastequalityis(40).
Ifj = p(i)andj = i,thenbyAM-GM:
̸ ̸
π(s′ s)
g i,j(π) =
µ
(s|
′)
·PX[s
i
= s′,s
j
= s] −1
π
s,s′
(cid:88)
1 µ π(s)π(s′ s)2 1 PX[s
i
= s′,s
j
= s]2
| + 1
≤ 2 µ (s′) 2 µ (s)µ (s′) −
π π π
s,s′ s,s′
(cid:88) (cid:88)
1 1
= B 2 + I (s ;s π).
2∥ π ∥F 2 χ2 i j |
Weseethattheχ2-mutualinformationcanberewrittenas
I χ2(s i;s
j
π) = µ π(s′) χ2(PX[s
j
= s
i
= s′] µ π).
| · · | ||
s′
(cid:88)
Let p(i,j) be the least common ancestor of i and j. Let x be the probability distribution defined
byx = PX s p(i,j) = s i = s′ . Thedistributionπd(j,p(i,j)) xis
· | ◦
(cid:2) (cid:3)
(πd(j,p(i,j)) x)(s) = πd(j,p(i,j))(s s∗) x(s∗)
◦ | ·
s∗
(cid:88)
= PX[s j = s s p(i,j) = s∗] PX s p(i,j) = s∗ s i = s′
| · |
s∗
(cid:88) (cid:2) (cid:3)
= PX[s
j
= s s
i
= s′],
|
wherethelastlineusesthefactthats ands areconditionallyindependentgivenp(i,j).
i j
ApplyingLemma17,wethushave
χ2(PX[s j = s i = s′] µ π) α(π)d(j,p(i,j)) χ2 PX s p(i,j) = s i = s′ µ π .
· | || ≤ · · | ||
(cid:0) (cid:2) (cid:3) (cid:1)
Therefore
I χ2(s i;s j π) α(π)d(j,p(i,j)) µ π(s′) χ2 PX s p(i,j) = s i = s′ µ
| ≤ · · | ||
s′
(cid:88) (cid:0) (cid:2) (cid:3) (cid:1)
= α(π)d(j,p(i,j)) I (s ;s π)
χ2 p(i,j) i
· |
= α(π)d(j,p(i,j)) µ π(s) χ2 PX s i = s p(i,j) = s µ
· · | ||
s
(cid:88) (cid:0) (cid:2) (cid:3) (cid:1)
= α(π)d(j,p(i,j)) µ (s) χ2 πd(i,p(i,j))( s) µ .
π
· · | ||
s
(cid:88) (cid:0) (cid:1)
Sincei > j,d(i,p(i,j)) 1,andthuswecanapplyLemma17toget
≥
χ2 πd(i,p(i,j))( s) µ α(π)d(i,p(i,j))−1 χ2(π( s) µ).
· | || ≤ · · | ||
(cid:0) (cid:1)
48Altogether,
I (s ;s π) α(π)d(j,p(i,j))+d(i,p(i,j))−1 µ (s) χ2(π( s) µ)
χ2 i j π
| ≤ · · | ||
s
(cid:88)
π(s′ s)2µ (s)
= α(π)d(i,j)−1 | π 1
· µ (s′) −
(cid:32) π (cid:33)
s,s′
(cid:88)
= α(π)d(i,j)−1 B 2.
∥ π ∥F
Forj = p(i),d(i,j) 2,so
̸ ≥
1
g (π) α(π)d(i,j)−1 +1 B 2
i,j ≤ 2 ∥ π ∥F
1
(cid:0)
(α(π)+1) B
2(cid:1)
.
≤ 2 ∥ π ∥F
andthus
1 α(π)
g (π) g (π) − B 2.
i,p(i) − i,j ≥ 2 ·∥ π ∥F
ByAssumption1andLemma15,wehave1 α(π) γ and B 2 γ2/S. Therefore
− ≥ ∥ π ∥F ≥
γ3
g (π) g (π) .
i,p(i) i,j
− ≥ 2S
Finally,whenj = i,wehave
g (π) = π(s s) 1.
i,i
| −
s
(cid:88)
Therefore
g
i,i
= E[π(s s)] 1 = 0.
| −
s
(cid:88)
Thereforeg g γ2/S γ3 .
i,p(i) − i,i ≥ ≥ 2S
G.2 Auxiliary Dynamics Lemmas
Lemma23. Letθ = (A(1),β 0I S),θˆ = (A(1),0),forβ
0
≤
1. Defineg i∗,gˆ
i
∈
Ri by
π(s′ s)
g i∗ := T E
f
(X;s)| +ϵδ s′(X)⊤J(v θ(X;s))e i ·δ s(X ≤i) ,
θ s′
s,s′ (cid:20) (cid:21)
(cid:88)
π(s′ s)
gˆ i := T E
f
(X;s)| +ϵδ s′(X)⊤J(v θˆ(X;s))e i ·δ s(X ≤i) .
s,s′ (cid:20)
θˆ s′
(cid:21)
(cid:88)
Then g∗ gˆ 3S2ϵ−2(eβ0 1)
∥ i − i ∥∞ ≤ −
49Proof. Wecanbound
1 1
δ (X)⊤J(v (X;s))e δ (X)⊤J(v (X;s))e
f (X;s) +ϵ
s′ θ i
− f (X;s) +ϵ
s′ θˆ i
(cid:12) θ s′ θˆ s′ (cid:12)
(cid:12) 1 1 (cid:12)
(cid:12) δ (X)⊤J(v (X;s))e (cid:12)
(cid:12)
≤ f (X;s) +ϵ − f (X;s) +ϵ
s′ θˆ i (cid:12)
(cid:12) θ s′ θˆ s′ (cid:12)
(cid:12) 1 (cid:12)(cid:12) (cid:12)
+ (cid:12) δ (X)⊤(J(v (X;(cid:12)s(cid:12))) J(v (X;s)))e . (cid:12)
f(cid:12) (X;s) +ϵ s′ θ (cid:12) − θˆ i
θˆ s′
(cid:12) (cid:12)
(cid:12) (cid:12)
First,seethat
f (X;s) f (X;s) = δ (X)⊤(v (X;s) v (X;s))
|
θ s′
−
θˆ s′
|
s′ θ
−
θˆ
v (X;s) v (X;s) ,
≤ (cid:12) ∥ θ − θˆ ∥1 (cid:12)
(cid:12) (cid:12)
since δ (X) 1. Next,wehave
∥ s′ ∥∞ ≤
v (X;s) = β (A(1))X⊤I e ) = β (A(1))δ (X) ).
θ 0 S s 0 s
S ·S S ·S
(cid:0) (cid:1) (cid:0) (cid:1)
Since (A(1))δ (X)hasentriesin[0,1],wecanboundeachentryofv (X;s)as
s θ
S
1 eβ0
v (X;s) ,
(T 1)eβ0 +1 ≤ θ i ≤ eβ0 +(T 1)
− −
andthus
1 eβ0 1 eβ0 1
v (X;s) v (X;s) = v (X;s) − .
| θ i − θˆ i | θ i − T ≤ eβ0 +(T 1) − T ≤ T
(cid:12) (cid:12) −
(cid:12) (cid:12)
Thus (cid:12) (cid:12)
(cid:12) (cid:12)
f (X;s) f (X;s) eβ0 1.
|
θ s′
−
θˆ s′
| ≤ −
Next,seethat
δ (X)⊤J(v (X;s))e = v (X;s) [x f (X;s) ],
s′ θ i θ i i,s′ θ s′
−
andthus
δ (X)⊤(J(v (X;s)) J(v (X;s)))e
s′ θ
−
θˆ i
v (X;s) v (X;s) x f (X;s) +v (X;s) f (X;s) f (X;s)
(cid:12)
≤ |
θ i
−
θˆ i
||
i,s′
−
θ (cid:12) s′
|
θˆ i
|
θ s′
−
θˆ s′
|
(cid:12) (cid:12)
2(eβ0 1)
− .
≤ T
Altogether,wehavethebound
1 1 3(eβ0 1)
δ (X)⊤J(v (X;s))e δ (X)⊤J(v (X;s))e − .
f (X;s) +ϵ s′ θ i − f (X;s) +ϵ s′ θˆ i ≤ ϵ2T
(cid:12) θ s′ θˆ s′ (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
50Therefore
g∗ gˆ
∥ i − i ∥
1 1
≤
T Eπ,X π(s′
|
s)
f (X;s)
+ϵδ s′(X)⊤J(v θ(X;s))e i
− f (X;s)
+ϵδ s′(X)⊤J(v θˆ(X;s))e i
s,s′ (cid:20) (cid:12) θ s′ θˆ s′ (cid:12)(cid:21)
(cid:88) (cid:12) (cid:12)
(cid:12) 3(eβ0 1) (cid:12)
T E π(s′ s)(cid:12) − (cid:12)
≤ π,X | · ϵ2T
s,s′ (cid:20) (cid:21)
(cid:88)
3Sϵ−2(eβ0 1)
≤ −
6Sϵ−2β ,
0
≤
sinceez 1 2z forz [0,1].
− ≤ ∈
G.3 Concentration
Lemma 24. For any s,s′ and any π with spectral gap 1 λ(π) 1 λ (see Definition 6)
∈ S − ≥ −
and µ (s′) γ/S, there exists a sufficiently large constant C such that if ϵ C T−1/2 and
π ≥ γ,S ≥ γ,S eff
i j,
≥
EX
(x
i,s µ′ ˆ−
(µˆ
sX
′)( +s′) ϵ)x
j,s
−
PX[s
i
µ= (s s′, ′)s
j
= s]
−PX[s j = s] ≲
1
T
.
(cid:12) (cid:20) X (cid:21) (cid:18) π (cid:19)(cid:12) eff
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:112)
Proof. (cid:12) (cid:12)
E π(s,s′) := EX
(x
i,s µ′ ˆ−
(µˆ
sX
′)( +s′) ϵ)x
j,s
−
PX[s
i
µ= (s s′, ′)s
j
= s]
+PX[s j = s]
X π
(cid:20) (cid:21)
x i,s′x
j,s
PX[s
i
= s′,s
j
= s] µˆ X(s′)
= EX µˆ (s′)+ϵ − µ (s′) −EX µˆ (s′)+ϵx j,s +PX[s j = s].
X π X
(cid:20) (cid:21) (cid:20) (cid:21)
E (s,s′)canberewrittenas:
π
x x x x µˆ (s′)
E π(s,s′) = EX
µˆ
i (, ss′ ′)j +,s
ϵ −
µi,s (′ s′j ),s
− µˆ
(X s′)+ϵx j,s +x j,s
X π X
(cid:20) (cid:21)
x x x x ϵx
i,s′ j,s i,s′ j,s j,s
= EX
µˆ (s′)+ϵ − µ (s′)
+
µˆ (s′)+ϵ
X π X
(cid:20) (cid:21)
x x [µ (s′) µˆ (s′) ϵ]+ϵx µ (s′)
i,s′ j,s π X j,s π
= EX
(µˆ
− (s′)+ϵ)µ−
(s′)
X π
(cid:20) (cid:21)
51Note that the inside of the expectation is upper bounded by O(ϵ−1). Therefore by the triangle
inequalitywehave
x x µˆ (s′) µ (s′) +ϵ[x x +µ (s′)x ]
|E π(s,s′)
| ≤
EX i,s′ j,s | X −
(µˆ
π (s′)+|
ϵ)µ
i (, ss′ ′)j,s π j,s
X π
(cid:20) (cid:21)
x x µˆ (s′) µ (s′) +ϵ[x x +µ (s′)x ]
i,s′ j,s X π i,s′ j,s π j,s
= EX | − (µˆ X(s′)+| ϵ)µ π(s′) 1 µˆX(s′)>µπ 2(s′)
(cid:20) (cid:21)
x x µˆ (s′) µ (s′) +ϵ[x x +µ (s′)x ]
i,s′ j,s X π i,s′ j,s π j,s
+EX | − (µˆ X(s′)+| ϵ)µ π(s′) 1 µˆX(s′)≤µπ 2(s′)
(cid:20) (cid:21)
x x µˆ (s′) µ (s′) +ϵ[x x +µ (s′)x ]
≲ i,s′ j,s X π i,s′ j,s π j,s
EX | −
µ
|
(s′)2
π
(cid:20) (cid:21)
µ (s′)
+ϵ−1 PX µˆ X(s′) π
≤ 2
(cid:20) (cid:21)
1
≲ E[(µˆ X(s′) µ π(s′))2]+ϵ+
− ϵT
eff
(cid:112)1
≲ +ϵ,
√T
eff
wherethelastinequalityfollowsfromCorollary2.
H Lemmas for Stage 2
H.1 Idealized Gradient
ProofofLemma11. Recall
(1 r)eβz2 +reβµπ(s)z (1 r)eβ +reβµπ(s)
h (z) = − − .
s (1 r)(eβ 1)µ π(s)z +(1 r)+reβµπ(s)−(1 r)(eβ 1)µ π(s)+(1 r)+reβµπ(s)
− − − − − −
and
1 π(s′ s)
gˆ(β) = S(S 1) Eπ µ π(s) · µ π(s′)h s µ (s| ′) .
(cid:34) (cid:32) π (cid:33)(cid:35)
− s s′ (cid:18) (cid:19)
(cid:88) (cid:88)
Forafunctionh(z) = Az2+Bz,onehas
Cz+D
2D(AD BC)
h′′(z) = − .
(Cz +D)3
52Thusforz [0,Sγ−1],
∈
2 1 r+reβµπ(s) (1 r) eβ(1 r)+reβµπ(s)+β r(eβ 1)µ (s)eβµπ(s)
h′′(z) = − · − · − − − π
s
(cid:0)
(( (cid:1)1 r)(eβ (cid:0)1)µ π(s)z +(1 r)+reβµπ(s))3
(cid:1)
− − −
2(1 r)2eβ
−
≥ ((1 r)(eβ 1)µ π(s)Sγ−1 +(1 r)+reβµπ(s))3
− − −
2(1 r)2eβ
−
≥ (Sγ−1eβ)3
2γ5S−3e−2β.
≥
Thereforeforz [0,Sγ−1],
∈
h (z) h′(1)(z 1)+γ5S−3e−2β (z 1)2.
s ≥ s − · −
Notethat π(s′|s) S. Therefore
µπ(s′) ≤ γ
π(s′ s) π(s′ s) π(s′ s) 2
h | h′(1) | 1 +γ5S−3e−2β | 1
s µ (s′) ≥ s µ (s′) − · µ (s′) −
π π π
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
andthus
π(s′ s) π(s′ s) 2
µ (s′)h | γ5S−3e−2β µ (s′) | 1
π s µ (s′) ≥ π µ (s′) −
(cid:32) π (cid:33) π
s′ (cid:18) (cid:19) s′ (cid:18) (cid:19)
(cid:88) (cid:88)
= γ5S−3e−2βχ2(π( s) µ ).
π
· | ||
Altogether,
1 π(s′ s)
gˆ(β) = S(S 1) Eπ µ π(s) · µ π(s′)h s µ (s| ′)
(cid:34) (cid:32) π (cid:33)(cid:35)
− s s′ (cid:18) (cid:19)
(cid:88) (cid:88)
γ5S−5e−2β Eπ µ π(s)χ2((π( s) µ)
≥ · | ||
(cid:34) (cid:35)
s
(cid:88)
= γ5S−5e−2β Eπ ∥B π ∥2 F
1
= γ8S−6e−2β.(cid:2) (cid:3)
2
H.2 Auxiliary Dynamics Lemmas
Lemma25. Define
δ (X)⊤J( (βz))z
s′
q (z) = S ,
s′
δ (X)⊤ (βz)+ϵ
s′
S
Thensup q (z) 10(1+β).
z∈[0,1]T ∥∇ s′ ∥1 ≤
53Proof. Wehavethat
J(βz)δ (X)+β J( (βz))(δ (X),z) δ (X)⊤J(βz)z βJ(βz)δ (X)
s′ s′ s′ s′
q (z) = ∇ S · .
∇z s′ δ (X)⊤ (βz)+ϵ − (δ (X)⊤ (βz)+ϵ)2
s′ s′
S S
First,byLemma26,
J(βz)δ (X) 2δ (X)⊤ (βz).
s′ 1 s′
∥ ∥ ≤ S
Next,byLemma27,
J( (βz))(δ (X),z) 2 (βz)⊤(δ (X) z)+4 (βz)⊤δ (X) (βz)⊤z 6 (βz)⊤δ (X),
∥∇ S s′ ∥1 ≤ S s′ ⊙ S s′ S ≤ S s′
wherethelastinequalityusesthefactthatz hasentriesin[0,1]. Finally,
δ (X)⊤J(βz)z J(βz)δ (X) J(βz)δ (X) J(βz)δ (X) z 4(δ (X)⊤ (βz))2.
s′ s′ s′ 1 s′ 1 ∞ s′
· ≤ ∥ ∥ ·∥ ∥ ·∥ ∥ ≤ S
(cid:12) (cid:12)
A(cid:12) ltogether, (cid:12)
(2+6β)δ (X)⊤ (βz) 4β(δ (X)⊤ (βz))2
s′ s′
q (z) S + S 2+10β.
∥∇z s′ ∥1 ≤ δ (X)⊤ (βz)+ϵ (δ (X)⊤ (βz)+ϵ)2 ≤
s′ s′
S S
Lemma26. Letubeavectorwithnonnegativeentries. Then J( (v))u 2 (v)⊤u
1
∥ S ∥ ≤ S
Proof.
J( (v))u = (v) (u (v)⊤u) (v) u + (v)⊤u (v) = 2 (v)⊤u.
1 i i i i i
∥ S ∥ S −S ≤ S S · S S
i i i
(cid:88)(cid:12) (cid:12) (cid:88) (cid:88)
(cid:12) (cid:12)
Lemma27. RecallthatJ(s) = diag(s) ss⊤. Then vJ( (v)) Rd×d×d satisfies
− ∇ S ∈
J( (v))(u,w) 2 (v)⊤(u w)+4 (v)⊤u (v)⊤w.
1
∥∇ S ∥ ≤ S ⊙ S S
fornonnegativevectorsu,w.
Proof. Seethat
J( (v))(u,w) = u⊤diag( (v))w (v)⊤u (v)⊤w = (v)⊤(u w) (v)⊤u (v)⊤w.
S S −S S S ⊙ −S S
Takingthegradient,andnotingthat (v) = J(v),weget
v
∇ S
J( (v))(u,w) = J( (v))(u w) (v)⊤w J( (v))u (v)⊤u J( (v))w.
∇ S S ⊙ −S · S −S · S
Sinceu w isalsoanonnegativevector,wegetthat
⊙
J( (v))(u,w) 2 (v)⊤(u w)+4 (v)⊤u (v)⊤w.
1
∥∇ S ∥ ≤ S ⊙ S S
54H.3 Concentration
Lemma28. ForanynonzeroscalarsA ,A ,B ,B ,
1 2 1 2
A A 1 A
1 2 1
B B + A A .
1 2 1 2
B − B ≤ B B ·| − | | − |
(cid:12) 1 2(cid:12) | 2 |(cid:18)(cid:12) 1(cid:12) (cid:19)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
Proof. (cid:12) (cid:12) (cid:12) (cid:12)
A A A A A A
1 2 1 1 1 2
+
B − B ≤ B − B B − B
(cid:12) 1 2(cid:12) (cid:12) 1 2(cid:12) (cid:12) 2 2(cid:12)
(cid:12) (cid:12) (cid:12) 1 (cid:12) 1 (cid:12) 1 (cid:12)
(cid:12) (cid:12) = (cid:12)A (cid:12) (cid:12) + A(cid:12) A
(cid:12) (cid:12) (cid:12) 1 (cid:12) (cid:12) (cid:12)1 2
| | B − B B | − |
(cid:12) 1 2(cid:12) | 2 |
A (cid:12)B B (cid:12) A A
= | 1 |(cid:12) | 1 − 2 |(cid:12)+ | 1 − 2 |
(cid:12) (cid:12)
B B B
1 2 2
| | | |
1 A
1
= B B + A A .
1 2 1 2
B B ·| − | | − |
| 2 |(cid:18)(cid:12) 1(cid:12) (cid:19)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
Forthefollowinglemmas,letθˆ
=
(Aˆ(1),Aˆ(2))betheoutputofAlgorithm1.
Define
1(j = p(i)) p(i) =
A(1) = ̸ ∅ .
S ∗ ij Aˆ(1) p(i) =
(cid:40) i,j ∅
(cid:0) (cid:1)
andletz˜(X;s) := (A(1))Xe .
∗ s
S
Lemma29. Fori ,
∈ R
T2log2T
EX z˜(X;s) i −µˆ X ≤i(s) 2 ≲ min 1, T i2 .
eff
(cid:104) (cid:105) (cid:18) · (cid:19)
(cid:12) (cid:12)
(cid:12) (cid:12)
Proof. ByCorollary1
1 T logT
(A(1)) ≲ .
S ∗ i,j − i T1/2i2
(cid:12) (cid:12) eff
(cid:12) (cid:12)
(cid:12) (cid:12)
Therefore (cid:12) (cid:12)
1
z˜(X;s) µˆ (s) = (A(1)) 1 δ (X )
i − X ≤i S ∗ i − i i · s ≤i
(cid:12)(cid:18) (cid:19) (cid:12)
(cid:12) (cid:12) (cid:12) 1 (cid:12)
(cid:12) (cid:12) (cid:12) (A(1)) 1 (cid:12)
≤ (cid:12) S ∗ i − i i (cid:12)
(cid:13) (cid:13)1
(cid:13)T logT (cid:13)
≲ (cid:13) . (cid:13)
(cid:13) T1/2i (cid:13)
eff
55Finally,
µ (s)T2
EX µˆ X ≤i(s) −µ π(s) 2 ≲ Tπ (λ)i2.
eff
(cid:104) (cid:105)
(cid:12) (cid:12)
Altogether, (cid:12) (cid:12)
T2log2T
EX z˜(X;s) i −µˆ X ≤i(s) 2 ≲ T i2 ,
eff
(cid:104) (cid:105) ·
(cid:12) (cid:12)
andtheconclusionfollowsasz˜(X(cid:12) ;s) i,µˆ
X
(s) [0,(cid:12)1].
≤i ∈
Lemma30. Define
E(3)(X) := (βz˜(X;s)) z˜(X;s)
s S i i
i
(cid:88)
Then
(1 r)eβµ (s)+reβµπ(s)µ (s) 2 logT
EX
(cid:34)
(cid:12)E s(3)(X)
− (1
−r)(−
eβ
−1)µπ
π(s)+(1
−r)+π
reβµπ(s)
(cid:12)
(cid:35)
≲ (1+β2)
· T e1 ff/2
.
(cid:12) (cid:12)
(cid:12) (cid:12)
Proof. Plug(cid:12)gingintheformulaforz˜(X;s)(33),wegetthat (cid:12)
exp(βz˜(X;s) )z˜(X;s)
E(3)(X) = i i i
s exp(βz˜(X;s) )
(cid:80) i i
eβ x + eβz˜(X;s)iz˜(X;s)
= (cid:80) i∈R p(i),s i∈R i
(eβ −(cid:80)1) i∈Rx p(i),s(cid:80)+
R
+ i∈Reβz˜(X;s)i
Wedefinetheerrorterms (cid:80) (cid:12) (cid:12) (cid:80)
(cid:12) (cid:12)
1
(X) := x (1 r)µ (s)
1 p(i),s π
E T − −
(cid:88)i∈R
1
(X) := eβz˜(X;s)iz˜(X;s) reβµπ(s)µ (s)
2 i π
E T −
i∈R
(cid:88)
1
(X) := eβz˜(X;s)i reβµπ(s).
3
E T −
i∈R
(cid:88)
Then
(1 r)eβµ (s)+reβµπ(s)µ (s)+eβ (X)+ (X)
E(3)(X) = − π π E1 E2
s (1 r)(eβ 1)µ π(s)+(1 r)+reβµπ(s) +(eβ 1) 1(X)+ 3(X)
− − − − E E
ThusapplyingLemma28,wegetthat
(1 r)eβµ (s)+reβµπ(s)µ (s)
E(3)(X) − π π
(cid:12)
s − (1 −r)(eβ −1)µ π(s)+(1 −r)+reβµπ(s)
(cid:12)
(cid:12) eβ (X) + (X) + eβ (X) +(cid:12) (X)
(cid:12)
(cid:12)
E(3)(X) E1 E3 E1 (cid:12)
(cid:12)
E2
≤ s · (cid:0)(cid:12)(1 −r)(e (cid:12)β −1)µ π( (cid:1)s)+ (cid:0)(cid:12)(1 −r)+ (cid:12)reβµπ(s)
(cid:1)
≲ (cid:12)(1 r)−1(cid:12) (cid:12) (X) +(cid:12) e−β (X) +e(cid:12)−β (X)(cid:12) ,
(cid:12) (cid:12) 1 2 3
− · |E | |E | |E |
(cid:0) (cid:1)
56since E s(3)(X) 1and(1 r)(eβ 1)µ π(s)+(1 r)+reβµπ(s) (1 r)eβγ.
≤ − − − ≥ −
(cid:12) (cid:12)
First,byLemma22,wehave
(cid:12) (cid:12)
(cid:12) (cid:12)
1
E 1(X)2 ≲ .
E T
eff
(cid:2) (cid:3)
Next,webound :
2
E
1 1
(X) eβz˜(X;s)z˜(X;s) eβµπ(s)µ (s) (1+β)eβ z˜(X;s) µ (s) ,
2 π i π
|E | ≤ T − ≤ T | − |
i∈R i∈R
(cid:88)(cid:12) (cid:12) (cid:88)
(cid:12) (cid:12)
andthusbyLemma29
(1+β)2e2β
EX 2(X)2 E z˜(X;s) i µ π(s) 2
E ≤ T | − |
i∈R
(cid:2) (cid:3) (cid:88)
(1+β)2e2β T2log2T
≲ min 1,
T T i2
i (cid:18) eff · (cid:19)
(cid:88)
(1+β)2e2β T logT T2log2T
=  + 
T T1/2 T
eff
i2

eff i>(cid:88)Tl 1o /g 2T ·

 T eff 
(1+β)2e2β
logT

≲ .
T1/2
eff
Next,webound .
3
E
1 1
(X) eβz˜(X;s)i eβµπ(s) βeβ z˜(X;s) µ (s) ,
3 i π
|E | ≤ T − ≤ T | − |
i∈R i∈R
(cid:88)(cid:12) (cid:12) (cid:88)
(cid:12) (cid:12)
sobyanidenticalcalculationtoasfor ,
2
E
β2e2βlogT
EX E3(X)2 ≲
T1/2
.
eff
(cid:2) (cid:3)
Altogether,
(1 r)eβµ (s)+reβµπ(s)µ (s) 2
E E(3)(X) − π π
(cid:34)
(cid:12)
s − (1 −r)(eβ −1)µ π(s)+(1 −r)+reβµπ(s)
(cid:12)
(cid:35)
(cid:12) (cid:12)
≲(cid:12) (cid:12)(1 −r)−2 E E1(X)2 +e−2β E E2(X)2 +e−2β E E3(X(cid:12) (cid:12))2
logT
(1+β2) (cid:0) (cid:2). (cid:3) (cid:2) (cid:3) (cid:2) (cid:3)(cid:1)
· T1/2
eff
57Lemma31. Define
E(1)(X) := x (βz˜(X;s)) z˜(X;s)
s,s′ i,s′
S
i i
i
(cid:88)
E(2)(X) := x (βz˜(X;s)) ,
s,s′ i,s′
S
i
i
(cid:88)
Then
E s( ,1 s) ′(X) (1 r)eβµ π(s)π(s′ s)+reβµπ(s)µ π(s′)µ π(s) 2
E − |
 (cid:12)E(2)(X)+ϵ − (1 r)(eβ 1)µ π(s)π(s′ s)+(1 r)µ π(s′)+reβµπ(s)µ π(s′) (cid:12) 
(cid:12) s,s′ − − | − (cid:12)
(cid:12) (cid:12)
 ≲(cid:12)
(cid:12) (1+β2)
logT
.
(cid:12)
(cid:12)

· T1/2
eff
Proof. Pluggingintheformulaforz˜(X;s)(33),wehavethat
E s( ,1 s) ′(X)
=
eβ i∈Rx i,s′x
p(i),s
+ i∈Reβz˜(X;s)iz˜(X;s) ix
i,s′
E s( ,2 s) ′(X)+ϵ (eβ −1) i∈Rx (cid:80)i,s′x p(i),s + i∈Rx (cid:80)i,s′ + i∈Reβz˜(X;s)ix i,s′ +ϵ ieβz˜(X;s)i
Wedefinetheerrorterms (cid:80) (cid:80) (cid:80) (cid:80)
1
(X) := x x (1 r)µ (s)π(s′ s)
4 p(i),s i,s′ π
E T − − |
(cid:88)i∈R
1
(X) := x (1 r)µ (s′)
5 i,s′ π
E T − −
(cid:88)i∈R
1
(X) := eβz˜(X;s)iz˜(X;s) x eβµπ(s)µ (s)µ (s′)
6 i i,s′ π π
E T −
i∈R
(cid:88)(cid:0) (cid:1)
1
(X) = eβz˜(X;s)ix eβµπ(s)µ (s′) .
7 i,s′ π
E T −
i∈R
(cid:88)(cid:0) (cid:1)
Then
E(1)(X)
s,s′
=
E(2)(X)+ϵ
s,s′
(1 r)eβµ (s)π(s′ s)+reβµπ(s)µ (s)µ (s′)+eβ (X)+ (X)
π π π 4 6
− | E E .
(1 −r)[(eβ −1)µ π(s)π(s′
|
s)+µ π(s′)]+reβµπ(s)µ π(s′)+(eβ −1) E4(X)+ E5(X)+ E7(X)+ Tϵ ieβz˜(X;s)i
ThereforebyLemma28,
(cid:80)
E s( ,1 s) ′(X) (1 r)eβµ π(s)π(s′ s)+reβµπ(s)µ π(s′)µ π(s)
− |
(cid:12)E(2)(X)+ϵ − (1 r)(eβ 1)µ π(s)π(s′ s)+(1 r)µ π(s′)+reβµπ(s)µ π(s′) (cid:12)
(cid:12) s,s′ − − | − (cid:12)
(cid:12) (cid:12) E(1)(X) eβ (X) + (X) + (X) +eβϵ +eβ (x) + (X(cid:12) (cid:12))
(cid:12)
s,s′ |E4
|
|E5
|
|E7
|
|E4
|
|E6
(cid:12) |
≤ (cid:12) (cid:12)E s( ,2 s) ′(X)+ϵ(cid:12) (cid:12)· ( (cid:0)1 −r)(eβ −1)µ π(s)π(s′
|
s)+(1 −r) (cid:1)µ π(s′)+reβµπ(s)µ π(s′)
≲ (cid:12) (X) +e−β(cid:12) (X) +e−β (X) +e−β (X) +ϵ,
(cid:12) 4 (cid:12) 5 6 7
|(cid:12)E | (cid:12)|E | |E | |E |
58wherethelaststepuses(1 r)(eβ 1)µ (s)π(s′ s)+(1 r)µ (s′)+reβµπ(s)µ (s′) (1 r)eβγ2,
π π π
− − | − ≥ −
E(1)(X)
andthat s,s′ 1.
E(2)(X)+ϵ ≤
(cid:12) s,s′ (cid:12)
(cid:12) (cid:12)
WecanuseLemma21tobound :
(cid:12) (cid:12) 4
(cid:12) (cid:12) E
2
E[ E4(X)2] = R
T2
E[(cˆ
X
R(s,s′) −µ π(s)π(s′
|
s)2]
(cid:12) (cid:12)
(cid:12) (cid:12)2
µ (s′)T2
≲ R π
(cid:12)T2 (cid:12) · T eff(λ) 2
(cid:12) (cid:12) R
1
≲ . (cid:12) (cid:12)
T (λ) (cid:12) (cid:12)
eff
Next,weuseLemma20tobound :
5
E
2
E[ E5(X)2] = R
T2
E[(µˆ
X
R(s′) −µ π(s′))2]
(cid:12) (cid:12)
(cid:12) (cid:12)2
µ (s′)T2
≲ R π
(cid:12)T2 (cid:12) · T eff(λ) 2
(cid:12) (cid:12) R
1
≲ . (cid:12) (cid:12)
T (λ) (cid:12) (cid:12)
eff
Next,webound :
6
E
1 1
(X) x eβz˜(X;s)z˜(X;s) eβµπ(s)µ (s) + eβµπ(s)µ (s) (x µ (s′))
6 i,s′ π π i,s′ π
|E | ≤ T − T (cid:12) − (cid:12)
(cid:88)i∈R
(cid:12) (cid:0) (cid:1)(cid:12)
(cid:12) (cid:88)i∈R (cid:12)
(cid:12) (cid:12)
1 (cid:12) 1 (cid:12) (cid:12) (cid:12)
(1+β)eβ z˜(X;s) µ (s) + eβ (x (cid:12) µ (s′)) . (cid:12)
π i,s′ π
≤ T | − | T (cid:12) − (cid:12)
(cid:88)i∈R (cid:12)(cid:88)i∈R (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
Thefirsttermcanbeboundedequivalentlyastowasd(cid:12)onefor 2,andthus(cid:12)
E
2
1 (1+β)2e2βlogT
E
 (cid:32)T
(1+β)eβ |z˜(X;s) −µ π(s)
|
(cid:33)

≲
T1/2
.
i∈R eff
(cid:88)
 
Inthesecondterm,sincex µ (s′)areindependentandmean0foralli = T,
i,s′ π
− ̸
2
1 e2β
E

(cid:32)Teβ
(cid:12)
(x i,s′ −µ π(s′))
(cid:12)(cid:33)

=
T2
E (x i,s′ −µ π(s′))2
(cid:12)(cid:88)i∈R (cid:12) (cid:88)i∈R (cid:104) (cid:105)
(cid:12) (cid:12)
 (cid:12) (cid:12)  e2β
(cid:12) (cid:12) ≲ .
T
Altogether
(1+β)2e2βlogT
E[ |E6(X) |2] ≲
T1/2
.
eff
59Finally,webound :
7
E
1 1
(X) x eβz˜(X;s) eβµπ(s) + eβµπ(s) (x µ (s′))
7 i,s′ i,s′ π
|E | ≤ T − T (cid:12) − (cid:12)
(cid:88)i∈R
(cid:12) (cid:0) (cid:1)(cid:12)
(cid:12) (cid:88)i∈R (cid:12)
(cid:12) (cid:12)
1 (cid:12) (cid:12)1 (cid:12) (cid:12)
βeβ z˜(X;s) µ (s) + eβ (cid:12) (x µ (s′)) . (cid:12)
π i,s′ π
≤ T | − | T (cid:12) − (cid:12)
(cid:88)i∈R (cid:12)(cid:88)i∈R (cid:12)
(cid:12) (cid:12)
Thusviaanidenticalcalculationas , (cid:12) (cid:12)
E6 (cid:12) (cid:12)
(1+β)2e2βlogT
E[ |E7(X) |2] ≲
T1/2
.
eff
Altogether,
E s( ,1 s) ′(X) (1 r)eβµ π(s)π(s′ s)+reβµπ(s)µ π(s′)µ π(s) 2
E − |
 (cid:12)E(2)(X)+ϵ − (1 r)(eβ 1)µ π(s)π(s′ s)+(1 r)µ π(s′)+reβµπ(s)µ π(s′) (cid:12) 
(cid:12) s,s′ − − | − (cid:12)
(cid:12) (cid:12)
≲ ((cid:12)
(cid:12)1+β2)
logT
.
(cid:12)
(cid:12)

· T1/2
eff
Lemma32.
Letθˆ
=
Aˆ(1),Aˆ(2) betheoutputofAlgorithm1,whereAˆ(2)
= (β +β(τ +τ ))I
0 1 2 S
−
β(τ1+τ2)1 1⊤. Then (cid:16) (cid:17)
S S S
logT
EX |f θˆ(X;s) s′ −π(s′
|
s) |2 ≲ (1+β∗2)
· T1/2
+e−β∗γ.
(cid:104) (cid:105) eff
and
γ3 1
P f θˆ(X;s) s′
≤ 4S2
≲
T
.
eff
(cid:20) (cid:21)
Proof. First, by Lemma 12, 1 + β∗ β(τ + τ ) β∗. For notational convenience, let β =
1 2
≥ ≥
β(τ +τ )Recallthedefinitions
1 2
1(j = p(i)) p(i) =
A(1) = ̸ ∅ .
S ∗ ij Aˆ(1) p(i) =
(cid:40) i,j ∅
(cid:0) (cid:1)
and
x ifi
z˜(X;s) = (A(1))δ (X) = p(i),s ̸∈ R .
S ∗ s z (X;s) ifi
(cid:40) θˆ i
∈ R
ByCorollary1 z (X;s) z˜(X;s) ≲ T−1. Lettingf(z) = δ⊤ (βz),weseethat f(z) =
∥
θˆ
−
∥∞
S
∥∇z ∥1
β J( (βz))δ 2β,andthus
1
∥ S ∥ ≤
f (X;s) δ (X)⊤ (βz˜(X;s)) ≲ βT−1.
θˆ s′
−
s′
S
(cid:12) (cid:12)
(cid:12) 60 (cid:12)Next,wehavethat
x exp(βz˜(X;s) )
δ (X)⊤ (βz˜(X;s)) = i i,s′ i ,
s′
S exp(βz˜(X;s) )
(cid:80) i i
andthus (cid:80)
δ (X)⊤ (βz˜(X;s))
s′
S
=
(eβ −1) i∈Rx p(i),sx
i,s′
+ i∈Rx
i,s′
+ i∈Rx i,s′eβz˜(X;s)i
(e (cid:80)β −1) i∈Rx p(i),s(cid:80)+
R
+ i∈(cid:80)Reβz˜(X;s)i
=
(1 −r)(eβ −1)µ (cid:80)π(s)π(s′
|
s)+
(cid:12)
(cid:12)
(1
(cid:12)
(cid:12)−(cid:80)r)µ π(s′)+rµ π(s′)eβµπ(s) +(eβ −1) E4(X)+ E5(X)+ E7(X)
(1 r)(eβ 1)µ π(s)+(1 r)+reβµπ(s) +(eβ 1) 1(X)+ 3(X)
− − − − E E
ThereforebyLemma28,
(1 r)(eβ 1)µ (s)π(s′ s)+(1 r)µ (s′)+rµ (s′)eβµπ(s)
δ (X)⊤ (βz˜(X;s)) − − π | − π π
(cid:12)
s′
S − (1 −r)(eβ −1)µ π(s)+(1 −r)+reβµπ(s)
(cid:12)
(cid:12) δ (X)⊤ (βz˜(X;s)) eβ (X) + (X) + (X) +eβ (x) + (X) (cid:12)
(cid:12) s′ 4 5 7 1 3 (cid:12)
(cid:12) S · |E | |E | |E | |E | |E | (cid:12)
≤
(cid:12)
(1
(cid:12)
−(cid:0)r)(eβ −1)µ π(s)+(1 −r)+reβ (cid:1)µπ(s)
≲ +(cid:12) (x) +e−β (X)(cid:12) + (X) +e−β (X) +e−β (X) .
1 3 4 5 7
|E | |E | |E | |E | |E |
Next,seethat
(1 r)(eβ 1)µ (s)π(s′ s)+(1 r)µ (s′)+rµ (s′)eβµπ(s)
− − π | − π π π(s′ s)
(cid:12)
(1 −r)(eβ −1)µ π(s)+(1 −r)+reβµπ(s) − |
(cid:12)
(cid:12)
(cid:12)
(1 r)µ π(s′)+rµ π(s′)eβµπ(s) +(1 r)π(s′ s)+reβµπ(s)π(s′ s) (cid:12)
(cid:12)
(cid:12) − − | | (cid:12)
≤
(cid:12)
(1 r)(eβ 1)µ π(s)+(1 r)+reβµπ(s)
(cid:12)
− − −
(cid:12) (cid:12)
eβµπ(s)
≲
(1 r)eβγ
−
≲ eβ(µπ(s)−1)
≲ e−βγ(S−1)
S
≲ e−βγ/2
Altogether,weget
β
f (X;s) π(s′ s) ≲ + (X) + (x) +e−β (X) +e−β (X) +e−β (X) +e−βγ/2,
|
θˆ s′
− | | T
|E4
|
|E1
|
|E3
|
|E5
|
|E7
|
andthus
logT logT
EX |f θˆ(X;s) s′ −π(s′
|
s) |2 ≲ (1+β2)
· T1/2
+e−βγ ≲ (1+β∗2)
· T1/2
+e−β∗γ
(cid:104) (cid:105) eff eff
61Next,weneedtoboundPX f θˆ(X;s)
s′ ≤
4γ S3
2
. Westartbyboundingtheprobabilityδ s′(X)⊤ S(βz˜(X;s))
issmall. Wehavethenaive(cid:104)bound (cid:105)
x exp(βz˜(X;s) )
δ (X)⊤ (βz˜(X;s)) = i i,s′ i
s′
S exp(βz˜(X;s) )
(cid:80) i i
eβ x x
(cid:80)i∈R p(i),s i,s′
≥ eβ T
1 (cid:80) ·
= x x
p(i),s i,s′
T
(cid:88)i∈R
= (1 r)cˆ (s,s′).
X
− R
ByMarkov’sinequalityandLemma21,
γ2 γ2
PX cˆ X R(s,s′) ≤ 2S2 ≤ PX cˆ X R(s,s′) −µ π(s)π(s′ | s) ≥ 2S2
(cid:20) (cid:21) (cid:20) (cid:21)
2S2(cid:12) (cid:12)
≤ γ2 (cid:12) EX c X R(s,s′) −µ π(s)π(s(cid:12)′ | s) 2
(cid:104) (cid:105)
T2 (cid:12) (cid:12)
≲ (cid:12) (cid:12)
2
T
eff
R
1
≲ (cid:12) (cid:12).
(cid:12)T (cid:12)
eff
Therefore
γ3 γ2 1
PX δ s′(X)⊤ S(βz˜(X;s))
≤ 2S2 ≤
PX δ s′(X)⊤ S(βz˜(X;s))
≤
(1 −r)
2S2
≲
T
.
(cid:20) (cid:21) (cid:20) (cid:21) eff
Toconclude,ontheeventthatδ (X)⊤ (βz˜(X;s)) > γ3 ,wehave
s′ S 2S2
γ3
f (X;s) > f (X;s) δ (X)⊤ (βz˜(X;s))
θˆ s′
2S2 −
θˆ s′
−
s′
S
γ3 (cid:12) (cid:12)
(cid:12)O βT−1 (cid:12)
≥ 2S2 −
γ3 (cid:0) (cid:1)
,
≥ 4S2
sinceβ 1+β∗ ≲ T. Altogether,
≤
γ3 1
PX f θˆ(X;s) s′
≤ 4S2
≲
T
.
(cid:20) (cid:21) eff
H.4 Proof of Theorem 2
Proof. ByLemma32,wegetthat
logT
EX (f(X;s) s′ −π(s′ | s))2 ≲ γ,S TΘγ(1)
(cid:104) (cid:105) eff
62ThereforebyMarkov’sinequality,
1
PX (f(X;s) s′ −π(s′
|
s))2
≥
100S2 ·EX (f(X;s) s′ −π(s′
|
s))2
≤
100S2.
(cid:104) (cid:104) (cid:105)(cid:105)
Unionbounding,withprobability0.99wehave
logT
s su ,sp
′
|f(X;s) s′ −π(s′
|
s)
| ≤
100S2 ·EX (f(X;s) s′ −π(s′
|
s))2 ≲ γ,S TΘγ(1),
(cid:104) (cid:105) eff
asdesired.
63