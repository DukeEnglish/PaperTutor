PublishedasaconferencepaperatICLR2024
FINE-TUNING ENHANCES EXISTING MECHANISMS:
A CASE STUDY ON ENTITY TRACKING
NikhilPrakash1∗ TamarRottShaham2 TalHaklay3 YonatanBelinkov3 DavidBau1
1NortheasternUniversity 2MITCSAIL 3Technion–IIT
ABSTRACT
Fine-tuning on generalized tasks such as instruction following, code generation,
andmathematicshasbeenshowntoenhancelanguagemodels’performanceona
rangeoftasks. Nevertheless,explanationsofhowsuchfine-tuninginfluencesthe
internalcomputationsinthesemodelsremainelusive. Westudyhowfine-tuning
affectstheinternalmechanismsimplementedinlanguagemodels.Asacasestudy,
we explore the property of entity tracking, a crucial facet of language compre-
hension, where models fine-tuned on mathematics have substantial performance
gains. Weidentifythemechanismthatenablesentitytrackingandshowthat(i)in
boththeoriginalmodelanditsfine-tunedversionsprimarilythesamecircuitim-
plementsentitytracking. Infact,theentitytrackingcircuitoftheoriginalmodel
on the fine-tuned versions performs better than the full original model. (ii) The
circuitsofallthemodelsimplementroughlythesamefunctionality: Entitytrack-
ingisperformedbytrackingthepositionofthecorrectentityinboththeoriginal
modelanditsfine-tunedversions.(iii)Performanceboostinthefine-tunedmodels
isprimarilyattributedtoitsimprovedabilitytohandletheaugmentedpositional
information. Touncoverthesefindings,weemploy: PatchPatching,DCM,which
automatically detects model components responsible for specific semantics, and
CMAP,anewapproachforpatchingactivationsacrossmodelstorevealimproved
mechanisms. Our findings suggest that fine-tuning enhances, rather than funda-
mentallyalters,themechanisticoperationofthemodel.
1 INTRODUCTION
The capabilities of models fine-tuned on general reasoning tasks have hinted at nontrivial mecha-
nisms underlying task learning. While it has been widely understood that fine-tuning a pretrained
modelonaspecifictaskcanimprovetaskperformanceonthatsametask(Howard&Ruder,2018),
studies of fine-tuning on generalized domains (Gururangan et al., 2020) have suggested that fine-
tuningongenericproblemscanimprovespecifictaskperformanceaswell. Inparticular,fine-tuning
oncodinghasbeenobservedtoleadtoarangeofimprovedcapabilitiesinamodel(Madaanetal.,
2022;Kim&Schuster,2023). Inthispaper,westudythemechanismsunderlyingonespecificca-
pabilitywhichisdramaticallyimprovedbyfine-tuningastandardlargelanguagemodel(LLM)on
the generic task of arithmetic-problem solving: the ability of a model to perform in-context entity
tracking,wherethemodelcaninferpropertiesassociatedwithanentitypreviouslydefinedinthein-
putcontext. Forexample,ifwesay“TheappleisinBoxC,”amodelwilllaterbeabletoinfer“Box
Ccontainstheapple”. Theabilitytotrackandmaintaininformationassociatedwithvariousentities
withinthecontextisfundamentalforcomplexreasoning(Karttunen,1976;Heim,1983;Nieuwland
&VanBerkum,2006;Kampetal.,2010),thusmakingentitytrackinganintriguingcasestudy.
We ask several specific questions about the mechanisms underlying the emergence of improved
entitytrackinginanarithmetic-tunedmodel. First, weask: cantheperformancegapbeexplained
becausethefine-tunedmodelscontainadifferentcircuitforperformingentitytracking? Ordoesit
contain the same entity-tracking circuit as the base model? To answer this question, we explicitly
identify the entity-tracking circuit in the base Llama-7B model, using the path-patching method
fromElhageetal.(2021);Wangetal.(2022),consistingofasparsesetof72attentionheadsinfour
∗Correspondencetoprakash.nik@northeastern.edu
tamarott@mit.edu,tal.ha@campus.technion.ac.il,belinkov@technion.ac.il,d.bau@northeastern.edu
1
4202
beF
22
]LC.sc[
1v11841.2042:viXraPublishedasaconferencepaperatICLR2024
groups,eachgroupactiveataspecifictokenlocation(Fig.1);actinginisolation,thissparsecircuit
can reproduce the entire entity-tracking capability of the base model. Then, without altering the
graph,weaskifexactlythesamesetofcomponentsconstitutestheentity-trackingcircuitinthefine-
tunedmodels. Weobservethattheidenticalcircuitexistsinthefine-tunedmodels,whichalonecan
restoreatleast88%oftheoverallperformanceoftheentirefine-tunedmodel. However, achieving
thefullperformanceofthefine-tunedmodelsrequiresincorporationofadditionalcomponents.
Next, we ask: how does this common circuit work? Can we discern the role of each group of at-
tention heads? To answer these questions, we use Desiderata-based Component Masking (DCM;
Daviesetal.,2023),amethodforautomaticallyidentifyingmodelcomponentsresponsibleforper-
formingaspecificsemanticsubtask.Thatisdonebyspecifyingasetof“desiderata,”eachconsisting
ofpairsofentitytrackingtasks,abasetask,andacarefullydesignedalternationofit.Thealternation
isdoneonaspecificsemanticpartofthetask(e.g.theentityname)withaknowntargetoutput(e.g.
switch the entity property). Using these sets of tasks, we automatically identify groups of model
componentsthathavecausaleffectsthatcorrespondtospecificsemantics. Forexample, wecould
identifywhethercircuitcomponentsaretransportingentitynameinformation(e.g.“BoxC”inthe
previousexample),oritsassociatedproperty(e.g.“containstheapple”),orsomeotherscheme. We
testthesehypothesesandsurprisinglyfindathirdschemethatisused: entitytrackingisperformed
byidentifyingandtransportingthepositionofthequeriedentityinthecontext,withmultiplegroups
ofheadscollaboratingtopassthepositiondownstream. Furthermore,thisschemeandspecificrole
ofeachgroupofheadsremainthesamebetweenmodels,confirmingthatfine-tuningpreservesthe
overallmechanismforperformingtheentitytrackingtask. Themechanisminvarianceisobserved
inbothlow-rankadaptations(LoRA)(Huetal.,2021)andfullyfine-tunedmodels.
Third, we ask: if the mechanism remains the same after fine-tuning, can we attribute the perfor-
mance improvement to a specific step in the mechanism? To study this question, we introduce
cross-modelactivation-patching(CMAP),whichallowsustolocalizethespecificsub-mechanism
being improved by fine-tuning. Cross-model activation patching shows evidence that (i) the inter-
nal representation of both the original model and the fine-tuned models is similar enough so that
patchingcomponentsoftheentity-trackingcircuitfromthefine-tunedmodelstoLlama-7Bleadsto
enhancedperformance.(ii)Infine-tunedmodelstheentitytrackingcircuithasaugmentedpositional
informationforattendingtothecorrectobjectandhencefetchingitsenhancedrepresentation.
Takentogether,ourfindingsindicatethatfine-tuningenhancestheexistingmechanismoftheorig-
inal model rather than causing a fundamental shift. Notably, the entity tracking circuit remains
consistent across both base and fine-tuned models and maintains the same functionality, with the
performance gap mainly attributed to an improved core sub-mechanism. The code, data and fully
fine-tunedmodelcanbeaccessedathttps://finetuning.baulab.info.
2 RELATED WORK
Mechanisticinterpretabilityaimstoelucidateneuralnetworkbehaviorsbycomprehendingtheun-
derlyingalgorithmsimplementedbymodels (Olahetal.,2017;Elhageetal.,2022). Recently,no-
tableprogresshasbeenmadeinidentifyingcircuitsperformingvarioustaskswithinmodels (Nanda
etal.,2023;Wangetal.,2022;Chughtaietal.,2023;Olahetal.,2020;Lieberumetal.,2023),and
in methods enabling circuit discoveries (Davies et al., 2023; Conmy et al., 2024; Wu et al., 2024;
Meng et al., 2022; Chan et al., 2022). We aim to harness mechanistic interpretability to uncover
an explanation for the performance enhancement observed in fine-tuned models. Specifically, our
exploration focuses on whether the performance gap results from varying circuit implementations
ofthesametaskandifnot,weaimtoidentifytheenhancedmechanismwithinthecircuit.
Fine-tuning on generic domains such as code, mathematics, and instructions has been shown to
enhancelanguagemodelsperformance,bothinthecontextofgeneralfine-tuningandwhentailored
for specific tasks (Christiano et al., 2017; Gururangan et al., 2020; Madaan et al., 2022; Ouyang
et al., 2022; Chung et al., 2022; Taori et al., 2023; Chiang et al., 2023; Liu & Low, 2023; Kim &
Schuster, 2023; Zheng et al., 2023; Touvron et al., 2023b; Bommarito II & Katz, 2022). Several
attemptstounderstandtheeffectofsuchfine-tuningonmodeloperationsrevealinterestingcharac-
teristics;instructionfine-tuningcandestroyknowledgeforOODinput(Kumaretal.,2022),shiftthe
model’sweighttoatask-dependedsub-domain(Guetaetal.,2023;Ilharcoetal.,2022),andenhance
existing capabilities rather than introduce new knowledge (Zhou et al., 2023). Fine-tuned models
2PublishedasaconferencepaperatICLR2024
were shown to have a localized set of components that perform the task (Panigrahi et al., 2023),
andmodifiedunderlyingembeddingspacesandattentionpatterns(Kovalevaetal.,2019;Merchant
etal.,2020;Wuetal.,2020;Zhou&Srikumar,2022).Concurrenttoourresearch,(Jainetal.,2023)
delvedintotheimpactoffine-tuningonLLMsfromamechanisticperspective. Althoughtheirmain
finding, suggesting that fine-tuning rarely alters pretrained capabilities, resonates with our result
ofenhancingexistingmechanismsthroughfine-tuning,theirstudyinvolvedcontrolledexperiments
utilizingtransformermodelscreatedusingthetracrlibrary(Lindneretal.,2024). Incontrast, our
experimentsfocusonestablishedLLMssuchasLlama-7Bandtheirfine-tunedvariants,specifically
inthecontextofentitytrackingtasks,whichwebelievebetterrepresentreal-worldlanguagetasks.
Entity tracking is a fundamental cognitive ability that enables AI models to recognize and trace
entities,includingobjects,individuals,orconcepts,withinagivencontext (Karttunen,1976;Heim,
1983; Nieuwland & Van Berkum, 2006; Kamp et al., 2010; Marcus, 2018). In the large language
modelsrealm,modelssuchasGPT-2(Radfordetal.,2019)haveshownsomerelatedabilities,such
as predicting the next moves in board games (Toshniwal et al., 2022; Li et al., 2022). Utilizing a
probing technique, Li et al. (2021) shows that entity state can be recovered from internal activa-
tions in BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020). Lately, Kim & Schuster (2023)
presented a dataset of entity tracking tasks, showing that models fine-tuned on code data perform
entitytrackingmoreaccurately. Weuseentitytrackingasacasestudytoexplorehowfine-tuning
changesthemodel’sfunctionalitytoachieveenhancedperformance. Complimentaryofourwork,
(Feng&Steinhardt,2023)investigatedhowLLMskeeptrackofvariouspropertiesassociatedwith
anentity. TheirfindingsindicatedthatmodelsgeneratebindingIDvectorscorrespondingtoentities
andattributes. WefinditintriguingtofurtherinvestigatetheinteractionbetweenthesebindingID
vectorsandtheentitytrackingcircuitwehaveidentified.
3 EXPERIMENTAL SETUP
ToexploretheinternalmechanismthatenablesentitytrackingweadaptthedatasetpresentedinKim
& Schuster (2023), aimed at evaluating the ability of a language model to track state changes of
discourse entities. The dataset contains English sentences describing different settings of objects
locatedindifferentboxes,withdifferentlabels,andthetaskistodiscoverwhatisinsideaspecific
box. Forexample,whenthemodelispresentedwith“TheappleisinboxF,thecomputerisinBox
Q,thedocumentisinBoxX... BoxFcontainsthe”,itshouldpredictthenexttokenas“apple”(see
additionaltaskexamplesinFig.2andintheAppendixJ).Eachofourtasksinvolves7boxesandno
operations(i.e.contentsoftheboxesarenotaltered),eachboxislabeledwitharandomalphabetic
letter. Forconvenience,weonlyusesingle-tokenobjects. IncontrasttoKim&Schuster(2023),we
reorderthestructureofthecontextsegment(whereeachboxinformationisdefined)suchthatthe
object is mentioned before the box label (“The apple is in box F” instead of “Box F contains the
apple”).Thisistoensurethatthecontextsegmentandthequerysegment(wheretheboxisqueried)
havedifferentstructures,andthemodelneedstoinfertheboxinformationratherthanlocatingthe
longestidenticalcontextsegmentinthetext.
Westudyfourlanguagemodels: LLaMA-7B(Touvronetal.,2023a),andthreefine-tunedversions
of it: Vicuna-7B (Chiang et al., 2023) that was fine-tuned on user-shared conversations collected
fromShareGPT,Goat-7B(Liu&Low,2023), fine-tunedonsyntheticallygeneratedarithmeticex-
pressions using LoRA (Hu et al., 2021), and FLoat-7B (Fine-tuned Llama on arithmetic tasks),
fine-tunedonthesamedataasGoat-7BwithoutLoRA.Allthesemodelsachievehighperformance
ontheentitytrackingtask,asshowninTable1(firstcolumn,evaluationwasdoneover500tasks).
AlthoughGoat-7BandFLoat-7Bwerefine-tunedonarithmetictasks,theirabilitytoperformentity
tracking is significantly improved compared to the base Llama-7B model. This aligns with Kim
&Schuster(2023), whoalsofoundthatmodelstrainedonstructureddataarebetteratperforming
entitytracking. Weseekamechanisticexplanationforthisperformancegap.
4 IS THE SAME CIRCUIT PRESENT AFTER FINE-TUNING?
In this section we ask whether the circuit that enables entity tracking changes across the different
fine-tunedmodels. Entitytrackingmightbesolvedbythesamecircuitinallfourmodels, oreach
modelmayimplementadifferentcircuitinthelightoffine-tuningdata.Toanswerthis,westartwith
3PublishedasaconferencepaperatICLR2024
Group A (L21 H3) Path-Patching A
Attention flow Q
Group B (L14 H27) Attention strength B
Attention heads
in circuit V
Group C (L10 H3) C
V
Group D (L8 H21) D
…
Figure 1: Entity Tracking circuit in Llama-7B (Cir). The circuit is composed of 4 groups of
heads(A,B,C,D)locatedatthelasttoken(A,B),querylabel(C),andpreviousquerylabel(D)token
positions. Eachgroupisillustratedbyaprominentheadinthatgroup.
identifyingtheentitytrackingcircuitinLlama-7B,andthenevaluatethesamecircuitcomponents
inVicuna-7B,Goat-7B,andFLoat-7B.
4.1 CIRCUITDISCOVERYINLLAMA-7B
Theentity-trackingcircuitwillbeasubgraphofthetransformercomputationalgraph, whereeach
nodeisanattentionheadataspecifictokenposition,sothewholecircuitisasetCir ={(a,t)}.For
example,Fig.1illustratestheentitytrackingcircuitinLlama-7Bconsistingoffourgroupsofnodes,
eachrepresentedbyaprominenthead;e.g. GroupAischaracterizedwith(a ,t ). Giventhe
L21H3 last
natureoftheentitytrackingtask,weareprimarilyinterestedinhowandwhatkindsofinformation
aretransportedbetweentokensratherthanhowthatinformationistransformed. Wethereforefocus
ouranalysisontheattentionheadsofthecircuit,andweconsiderallMLPlayerstobeinvolvedin
thecomputationofthefinaloutput.
Toidentifythecomponentsoftheentitytrackingcircuit,weusePathPatching(Wangetal.,2022;
Goldowsky-Dill et al., 2023), using the synthetic box tracking dataset with 300 examples. For
each of the original entity tracking tasks x we define a corresponding noise task x with a
org noise
randomized query, box labels, and objects. Then we evaluate each candidate pair of nodes with
a score defined as follows. We denote p as the probability of the correct token predicted by
org
theoriginalrun, andweletp betheprobabilityassignedtothecorrecttokenwhenpatchinga
patch
specificpathfromonespecificnodetoanotherusingactivationsfromthenoisyrun. Thepatching
scoreforthecandidatepairisdefinedas(p −p )/p . Ateachiterationweaddthepathswith
patch org org
thelowest(mostnegative)scores.
Inthefirststep,weidentifythegroupofheadsthatdirectlyinfluencethefinallogitwiththelowest
patching scores. These attention heads attend mainly to the correct object token: in other words,
theylookdirectlyattheanswer,e.g.,‘apple’thatshouldbepredicted(Fig.1). Werefertothissetof
headsasGroupA.Wetheniterativelyidentifygroupsofheadsthathavehighdirecteffectsoneach
otherusingthepathpatchingscore;thisleadsustothreeadditionalgroupsofattentionheads,(B,C,
andD),activeatthelast,querylabel,andpreviousquerylabeltokenpositions,asshowninFig.1.
WemarkthepathsbetweengroupswitheitherQorVtoindicatewhethertheheadsoftheprevious
groupaffectthequeryorthevaluevectorcalculationofthefollowinggroupcorrespondingly.
Overall,thecircuitCirconsistsoffourgroupsofheads. GroupDatthepreviousquerylabeltoken
collectsinformationofitssegmentandpassesitontotheheadsinGroupCatthequeryboxlabel
positionviaV-composition. TheoutputofGroupCistransportedtothelasttokenresidualstream
via the heads of Group B through V-composition, which is used by the heads of Group A via Q-
composition to attend to the correct object token. The validity of this information flow channel is
further substantiated by the results obtained from the attention knockout technique introduced in
Geva et al. (2023), as demonstrated in Appendix A. Interestingly, this circuit suggests that correct
object information is fetched directly from its token residual stream, instead of getting it from the
query label token residual stream. This result is consistent with the findings of Lieberum et al.
4
elppa C CC CehtPublishedasaconferencepaperatICLR2024
Table1: Entity-trackingcircuitfoundinLlama-7B,evaluatedonLlama-7B,Vicuna-7B,Goat-7B,
andFLoat-7B,withoutanyadjustmentofthecircuitgraph. Thecircuitachieveshighaccuracyand
faithfulnessscoresinallmodels(chanceaccuracyis0.14).
Accuracy
Model Finetuned? Full-Model Circuit RandomCircuit Faithfulness
Llama-7B – 0.66 0.66 0.00 1.00
Vicuna-7B Userconversations 0.67 0.65 0.00 0.97
Goat-7B Arithmetictasks(LoRA) 0.82 0.73 0.01 0.89
FLoat-7B Arithmetictasks(w/oLoRA) 0.82 0.72 0.01 0.88
(2023),reportingthatheadsaffectingfinallogitattendtothecorrectlabel,insteadofcontenttokens,
toidentifythelabelcorrespondingtothealready-determinedcorrectanswer.
4.2 CIRCUITEVALUATION
Althoughpathpatchingranksaheadbasedonitsrelevanceviathepatchingscore,itdoesnotprovide
aclearthresholdforthenumberofheadsthatshouldbeincludedinthecircuit. Inoursetting, we
includeatotalof90headsinthecircuitdiscoveredwithpathpatching(50,10,25,5headsinGroups
A,B,C,Drespectively).However,theremightberedundancyamongtheheadsineachgroup.Hence,
inspired by Wang et al. (2022), we use a minimality criterion to prune the initial circuit. We then
measure the performance of the minimal circuit compared with that of the entire model using the
faithfulnessmetric. WealsoevaluateitwiththecompletenessmetricintheAppendixC.
For both criteria, we define the performance metric F to be the accuracy score averaged over 500
examples. That is, forthemodelM anditscircuit Cir, F(M),F(Cir) representtheaccuracyof
the model and circuit respectively. Specifically, we compute F(Cir) by first mean ablating of all
theheadsinthemodelthatarenotinvolvedinCir.
Minimality. Theminimalitycriterionhelpsidentifyheadsthatdonotsignificantlycontributetothe
circuit performance found with path patching (90 heads in total). For each head, v ∈ Cir, and a
subsetofheadsK,wemeasuretherelativeperformancedifferenceofCirwhentheheadsinK are
knockout,withandwithoutvfromthecircuit. Thatis,wedefinethecontributionofeachheadvto
Ciras(F(Cir\K)−F(Cir\(K∪{v})))/F(Cir\(K∪{v})). Wefilterouttheheadswitha
scorelowerthan1%(e.g.contributelessthan1%totheperformanceofthecircuitintheabsenceof
thefunctionalitydefinedbysubsetK).UnlikeWangetal.(2022),weuseagreedyapproachtoform
thesubsetforeachheadinCir (checkAppendixBformoredetails),andonlyconsiderheadsthat
positivelycontributetothemodelperformance(e.g.contributetoperformingofthetask).Usingthis
criterionweprune20%oftheheadsoftheinitialcircuit,hencereducingthetotalnumberofheads
to72(seeAppendixDforexactdistributionandheadsineachgroup).
Faithfulness. Wenextmeasurehowgoodistheidentifiedcircuitcomparedwiththeentiremodel.
Weusethecriterionoffaithfulness, whichisdefinedasthepercentageofmodelperformancethat
canberecoveredwiththecircuit,i.e.F(Cir)/F(M). AsshowninTable1,Llama-7Bhasafaith-
fulnessscoreof1.0,suggestingidentifiedcircuitcanrecoverentiremodelperformance.
4.3 CIRCUITGENERALIZATIONACROSSFINE-TUNEDMODELS
As described in section 3, fine-tuned models perform the entity tracking task better than the base
Llama-7B. Better performance could be attributed to a superior circuit in the fine-tuned models.
Hence, in this subsection, we ask the question of whether the fine-tuned models use a different or
thesamecircuit,i.e.withexactlythesamegroupofheads,toperformtheentitytrackingtask.
To answer this, we evaluate the circuit identified in Llama-7B, on the fine-tuned models using the
faithfulnesscriterion. Surprisingly,wefindthatfine-tunedmodelshavegoodfaithfulnessscoresfor
the circuit identified in Llama-7B (without any additional optimization or adaptation) as shown in
Table1. Specifically,Vicuna-7Bhasalmostaperfectfaithfulnessscoreof0.97,whileGoat-7Band
FLoat-7B exhibit slightly lower scores of 0.89 and 0.88, respectively. As a baseline, we calculate
5PublishedasaconferencepaperatICLR2024
the average accuracy of 10 random circuits with the same total and per-position number of heads;
random circuits have virtually zero accuracy. This suggests that Vicuna-7B utilizes roughly the
samecircuitasthatofLlama-7Btoperformentitytracking. Whereas,inGoat-7BandFLoat-7Bthe
same circuit is present, but achieving the complete performance of the fine-tuned models requires
theincorporationofadditionalcomponents.
To further investigate the overlap between the circuits of fine-tuned models and the base model,
we identify the entity tracking circuits of the Goat-7B and FLoat-7B models, using the same pro-
cedure as in Section 4.1 (Refer to Appendix E and Appendix F). We found that these circuits are
significantly larger, consisting of 175 attenton heads and approximately forming a superset of the
Llama-7Bcircuit(RefertoAppendixE4andAppendixF4formoredetails). Thisfindingsuggests
thatfine-tuningisinsertingadditionalcomponentstothecircuitrythatperformsentitytracking.
5 IS CIRCUIT FUNCTIONALITY THE SAME AFTER FINE-TUNING?
Whilethesamecircuitisprimarilyresponsibleforperformingentitytrackinginboththebaseand
fine-tuned models, the specific functionality of different parts of the circuit remain unknown. In
other words, to fully comprehend the underlying mechanism through which these models execute
the task, it is crucial to understand the functionalities of the circuit components. There are two
hypothesis pertaining to circuit functionality in base and fine-tuned models: (i) The same circuit
exists in all four models, but the functionalities it implements may vary, accounting for the per-
formance difference. (ii) The circuits of all models implement the same mechanism, but with an
enhancedfunctionalityinfine-tunedmodels. Toinvestigatethesehypotheses,weusetheautomatic
Desiderata-basedComponentMasking(DCM)method,introducedinDaviesetal.(2023),foriden-
tifyinggroupsofmodelcomponentsresponsibleforspecificfunctionalities. First,weuseDCMon
the groups of heads in the minimal circuit of Llama-7B to identify subsets of heads with specific
functionalities,(e.g.movingpositionalinformationorobjectvalues).Then,foreachmodelweapply
activationpatchingonthosesubsetsofheads,toquantifytheirefficacyonvariousfunctionalities.
5.1 DESIDERATA-BASEDCOMPONENTMASKING
TheDCMmethodinvolvesusingdesiderataforidentifyingmodelcomponentsresponsibleforspe-
cific functionality. Each desideratum consists of numerous 3-tuple (original, alternative, target),
where original is an original entity tracking task, alternate is a carefully designed counterfactual
task,andtargetisthedesiredoutput,asshowninFig.2.Ifasetofcomponentsencodesinformation
regarding the desired semantics, then patching activations from the alternate run into the original
runshouldalterthemodeloutputtotarget. RefertoDaviesetal.(2023)formoredetails.
DCMusegradientdescentoptimizationprocedures;Foreachdesideratum,wetrainasparsebinary
maskoverpotentialmodelcomponentstoidentifytheonesthatwhenpatchedfromcounterfactual
tooriginalrunmaximizethetargetvalue.Hence,comparedtobrute-forceactivationpatching,DCM
is much more efficient. More importantly, it overcomes a major drawback of activation patching,
i.e.itcanlocatethesubsetofmodelcomponentsthatworktogethertoproducethefinaloutput.
5.2 CIRCUITFUNCTIONALITYINLLAMA-7B
TountanglethefunctionalityofgroupsofheadsintheLlama-7Bcircuit,wedefinethreedesiderata,
asshowninFig.2: (i)Object desideratum, whichisusedtoidentifymodelcomponentsencoding
the value of correct object, (ii) Label desideratum, used to identify model components encoding
theboxlabelvalueinformation,and(iii)Positiondesideratumwhichcanbeusedtoidentifymodel
componentsencodingthepositionalinformationofthecorrectobject. PleaserefertoFig.2caption
andAppendixGforadditionaldetailsabouteach.
WeapplyDCMtoidentifythesubsetofheadsthatencodethesefunctionalitiesinLlama-7Bcircuit.
For each group of heads, we train three binary masks, one for each desideratum, that identify the
subsetofheadsencodingspecificfunctionality(checkAppendixHformoredetails).Theresultsare
showninTableA2.AllGroupAheadsencodethevalueofcorrectobjectintheiroutput.Whilemost
oftheheadsinGroupB(71.43%)andC(70.0%)encodepositionalinformationofthecorrectobject
intheiroutput.TheheadsofGroupDarenotprofoundlyinvolvedinanyofthethreefunctionalities.
6PublishedasaconferencepaperatICLR2024
Desiderata Alternate Original Target
ThebookisinBoxA,the cup is in Box B,the ThedocumentisinBoxX,thepotisinBoxY,the
(a) Object cup
computerisinBoxC,… Box Bcontainsthe____ crossisinBoxZ,… BoxXcontainsthe____
ThebookisinBoxA,thecupisinBoxB,the ThedocumentisinBoxX,the pot is in Box Y,the
(b) Label pot
computerisinBoxC,… Box Ycontainsthe____ crossisinBoxZ,… BoxXcontainsthe____
ThebookisinBoxA,thecupisinBoxB,the ThedocumentisinBoxX,thepotisinBoxY,the
(c) Position cross
computerisinBoxC,… Box Ccontainsthe____ crossisinBoxZ,… BoxXcontainsthe____
Figure2:Desiderataforidentifyingcircuitfunctionality.Wedefinedifferentdesiderata(setsofan
originalsentenceandacarefullydesignedcounterfactualalternationofitwithaknowntargetoutput)
toevaluatevarioushypothesesregardingthefunctionalityofasubsetofheadswithinthecircuit. (a)
whenpatchingheadsencodinginformationaboutthecorrectobject,theobjectinformationfromthe
alternaterun(e.g.“cup”)isimplantedintotheoriginalrun.(b)headssensitivetothequeryboxlabel
will be affected by the alternate query label (e.g. “Box Y”) causing the output to be the object in
that box. (c) Position encoding heads represent positional information of the correct object token.
Thereforepatchingthese,themodeloutputstheobjectatthelocationofthecorrectobjecttokenin
thealternatesequence(e.g.“cross”whichislocatedatthesamepositionas“computer”.)
We next apply activation patching on this subset of heads, using additional (N = 500) samples
from the three desiderata, and compute the accuracy with respect to the target value. In order to
incorporate randomness in the generated data, we repeated the evaluation ten times with different
samplesofthetestsetandreportthemeanaccuracyandstandarddeviation.Theresultsareshownin
Fig.3,indicatingthatheadsinGroupAareprimarilyresponsibleforfetchingthevalueinformation
ofthecorrectobject. Hence,werefertothissetofheadsasValueFetcher. HeadsinGroupBandC
aremainlyresponsiblefordetectingandtransmittingthepositionalinformationofthecorrectobject
andarethereforereferredtoasPositionDetector andPositionTransmitter. Sincewewereunable
toestablishthefunctionalityofheadsinGroupD,weusedtheirattentionpatterntoannotatethem.
Theseheadsprimarilyattendtotokensintheirownsegment,asshowninFig.1,hencewereferto
themasStructureReaderheads.
Overall,thecircuitgeneratescorrectoutputbyfirstdetectingthepositionalinformationofthecorrect
objectwithPositionDetectorheads,usingtheinformationcollectedbytheStructureReaderheads.
The positional information is transmitted to the Value Fetcher heads, by the Position Transmitter
heads,whichresolvesthisinformationtolocatethecorrectobjectlocationandfetchesitsvalue,to
begeneratedasfinaloutput. Thisindicatesthatthemodelisprimarilyusingpositionalinformation
to keep track of in-context entities. Additionally, we have some early evidence that the model is
encodingpositionalinformationrelativetothecontextsegment;seeAppendixJformoredetails.
5.3 CIRCUITFUNCTIONALITYINFINE-TUNEDMODELS
NowthatwehaveidentifiedthefunctionalityofthegroupofheadsintheLlama-7Bcircuit,wecan
examinewhetherthiscircuit,alsopresentinthefine-tunedmodels,implementsthesameordifferent
functionalitiesacrossdifferentmodels. Toassessthis, weemployactivationpatchingonthesame
subsetofheadsofVicuna-7B,Goat-7B,andFLoat-7Bthatareinvolvedinaspecificfunctionality.
As shown in Fig. 3, the functionality of the subset of heads remains the same across fine-tuned
models. PositionDetectorandPositionTransmitterheadsofVicuna-7BandGoat-7Bachieveper-
formance similar to that of Llama-7B, though they demonstrate enhanced accuracy in FLoat-7B.
TheValueFetcherheadsinfine-tunedmodelsconsistentlyshowanimprovedcapabilitytoretrieve
thecorrectobjectvalue,e.g.Goat-7Bcanachieveaperformanceimprovementof20%comparedto
Llama-7B.Furthermore,wefoundthatbothGoat-7BandFLoat-7Bcircuitsimplementpreciselythe
samefunctionality withineachgroup, asdepictedin Fig.A8and Fig.A9. Thesefindings suggest
that neither additional functionality nor a shift in functionality is introduced in fine-tuned models.
Overall, the results confirm the hypothesis that circuits in fine-tuned models implement the same
functionality with the insight that the Value Fetcher in fine-tuned models has a better ability to
resolvepositionalinformationforfetchingthecorrectobjectvalueinformation.
7PublishedasaconferencepaperatICLR2024
Figure3:CircuitFunctionalityinLlama-7B,Vicuna-7B,Goat-7B,andFLoat-7B.WeuseDCM
touncoverfunctionalityofeachsubgroupofLlama-7Bcircuit.GroupA(pink)ismainlysensitiveto
valuedesideratum,whilegroupsB,C(purple,turquoise)areresponsibleforpositionalinformation.
WefindgroupDinsensitivetoeachofthethreedesideratum. Errorbarsindicatestandarddeviation.
Combining the results from previous experiments indicates that not only the circuit from the base
modelispresentinthefine-tunedmodels,butalsoitsfunctionalityremainsthesame. Further,addi-
tionalcomponentsinfine-tunedmodels’circuitsimplementtheexactsamefunctionality.Hence,we
concludethatfine-tunedmodelsimplementthesamemechanismtoperformentitytrackingtaskas
thebasemodel. However,theincreasedperformanceoffine-tunedmodelssuggeststhatfine-tuning
enhances that existing mechanism. This implies that unraveling the mechanism through which a
fine-tuned model accomplishes a task provides valuable insights into how the same task would be
executedinthebasemodel.Thisinsightisparticularlycrucialfortasksthatthebasemodelstruggles
toperformwell,makingunravelingitsmechanismmorechallenging.
6 WHY DO GOAT-7B AND FLOAT-7B PERFORM BETTER?
Intheprevioussections,weestablishedthatfine-tunedmodelsemploythesamemechanismasthe
basemodeltoperformthe entitytrackingtask, albeitwithadditionalcomponents. In thissection,
weaimtoattributeperformanceimprovementtoaspecificstepinthemechanism.
6.1 CROSS-MODELACTIVATIONPATCHING
In order to be able to attribute the performance improvement to a specific step in the mechanism,
we introduce Cross-Model Activation Patching (CMAP). Unlike naive activation patching, which
involvespatchingactivationsofthesamemodelondifferentinputs,CMAPrequirespatchingacti-
vationsofthesamecomponentsofdifferentmodelsonthesameinput,asshowninFig4.
WeuseCMAPtopatchtheoutputofthesubsetofheadsresponsiblefordominantfunctionalityin
eachgroupofGoat-7BandFLoat-7Bcircuits. Sincewedonotfullyunderstandthefunctionalityof
StructureReaderheads,wepatchalltheheadsinthisgroup. Morespecifically,wepatchtheoutput
of heads in the Goat-7B circuit from the Goat-7B to Llama-7B model, to identify which step in
theGoat-7Bmodelmechanismleadstoperformanceimprovement. Similarly,weperformthesame
patchingprocessforheadsintheFLoat-7Bcircuit.
8PublishedasaconferencepaperatICLR2024
Q K V
Q K V
Q K V
Q K V
!
!"#$"#%
$&#'()*+
Q K V
Q K V
Q K V
Q K V
!
Figure 4: Why do Goat-7B and FLoat-7B perform better? We use CMAP to patch activations
oftheGoat-7BandFLoat-7Bcircuitcomponents,fromGoat-7BandFLoat-7BtoLlama-7Bmodel
respectively,toattributetheperformanceimprovementtoaspecificsub-mechanismusedtoperform
entitytrackingtasks. Wepatchtheoutputofthesubsetofheadsineachgroupthatareinvolvedin
theprimaryfunctionality. WefindthatpatchingValueFetcherheadscansolelyimprovetheperfor-
mance of Llama-7B to that of Goat-7B and FLoat-7B. Additionally, we also observe a significant
performanceboostwhentheoutputofPositionTransmitterheadsispatched.
6.2 RESULTS
AsshowninFig. 4, patchingtheoutputofthePositionTransmitterandValueFetcherheadsfrom
fine-tuned models to Llama-7B improves the performance of Llama-7B beyond its default perfor-
mance (red dashed line). It is interesting to observe that the activations of fine-tuned models are
compatible with base model, even though they could have been using completely different sub-
spacesand/ornormstoencodeinformation. Weobservethemaximalincreaseinperformancewhen
the Value Fetcher heads are patched, recovering the full fine-tuned models’ performance (green
dashedline).Thisindicatesthattheoutputoftheseheadsinfine-tunedmodelsencodesanenhanced
representationofthecorrectobject,corroboratingresultsfromSection5.Additionally,wealsoseea
substantialincreaseinperformancewhentheoutputsofthePositionTransmitterheadsarepatched,
suggestingthatfine-tunedmodelsarealsotransmittingaugmentedpositionalinformation. Wespec-
ulatethattheenhancedencodinginfine-tunedmodelsstemfrombothadditionalcomponentsintheir
circuitandtheimprovedabilitytoencodevitalinformationofsharedcomponentswithLlama-7B.
7 DISCUSSION AND CONCLUSION
In this work, we investigated the effect of fine-tuning on circuit-level mechanisms in LLMs. We
discoveredthatnotonlydoesthecircuitfromthebasemodelpersistinthefine-tunedmodels, but
itsfunctionalityalsoremainsunchanged. Further,thecircuitsinfine-tunedmodels,augmentedwith
additionalcomponents,preciselyemploythesamefunctionality. WehaveintroducedCross-Model
ActivationPatching(CMAP)tocomparemechanismsintwodifferentmodels,revealinghowafine-
tuned model enhances the existing mechanism in a base model to obtain a better performance on
entity tracking. In our work we have studied the interaction between a single task and three fine-
tunedmodels. Understandingwhethersuchmechanisminvarianceistypicalwillrequireexperience
withfurthertasksonmoremodels.Nevertheless,themethodspresentedinthepaperaregenericand
couldbeapplied toavarietyofsettings. Futureworkmay studythetrainingdynamicsduring the
fine-tuningprocess,topinpointexactlywhenandhowthecircuitenhancementoccurs.
9
)(,+$
)(,+$
)(,0//%
)(,0//%
)('&
)('&
)(#&
)(#&
)(."-
)(."-
)(*
)(*
)(1
)(1
)(."-
)(."-
)(*
)(*
)('#&%$#"!
)('#&%$#"!
)(,+$
)(,+$
'&%*#)(
'&%#$#"!PublishedasaconferencepaperatICLR2024
8 ETHICS STATEMENT
Thisworkinvestigatingtheimpactoffine-tuningonlargelanguagemodelssuggeststhatfine-tuning
primarilyenhancesexistingmechanismspresentinthebasemodel. Thishighlightstheimportance
oftrainingsafeandunbiasedbasemodelsthatareopenlyavailable. Ifsuchmodelsaredeveloped
responsibly, then the risks of fine-tuning introducing new biases or dangerous behaviors can be
greatlyreduced. Hence,indicatingthatcarefulstewardshipisrequiredinthefoundationalphasesof
modeldevelopmenttopromotebeneficialapplicationsasthecapabilitiesofAIsystemsadvance.
9 ACKNOWLEDGEMENT
WewouldliketothankOpenPhilanthropyfortheirgeneroussupportthroughanAIAlignmentgrant
(NP,TH,YB,DB).THandYBalsoreceivedsupportfromtheIsraelScienceFoundation(grantNo.
448/20)andanAzrieliFoundationEarlyCareerFacultyFellowship. TRSreceivedpartialsupport
fromtheZuckermanSTEMLeadershipProgramandtheViterbiFellowship. Wewouldalsoliketo
thanktheCenterforAISafety(CAIS)formakingcomputingresourcesavailableforthisresearch.
REFERENCES
Michael Bommarito II and Daniel Martin Katz. Gpt takes the bar exam. arXiv preprint
arXiv:2212.14402,2022.
Lawrence Chan, Adria` Garriga-Alonso, Nicholas Goldowsky-Dill, Ryan Greenblatt,
Jenny Nitishinskaya, Ansh Radhakrishnan, Buck Shlegeris, and Nate Thomas.
Causal scrubbing: A method for rigorously testing interpretability hypothe-
ses. https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/
causal-scrubbing-a-method-for-rigorously-testing, 2022. Accessed:
February14,2023.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
SiyuanZhuang,YonghaoZhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing. Vicuna: An
open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:
//lmsys.org/blog/2023-03-30-vicuna/.
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcementlearningfromhumanpreferences. Advancesinneuralinformationprocessingsys-
tems,30,2017.
BilalChughtai,LawrenceChan,andNeelNanda. Atoymodelofuniversality: Reverseengineering
hownetworkslearngroupoperations. arXivpreprintarXiv:2302.03025,2023.
HyungWonChung,LeHou,ShayneLongpre,BarretZoph,YiTay,WilliamFedus,EricLi,Xuezhi
Wang,MostafaDehghani,SiddharthaBrahma,etal.Scalinginstruction-finetunedlanguagemod-
els. arXivpreprintarXiv:2210.11416,2022.
ArthurConmy, AugustineMavor-Parker, AengusLynch, StefanHeimersheim, andAdria` Garriga-
Alonso.Towardsautomatedcircuitdiscoveryformechanisticinterpretability.AdvancesinNeural
InformationProcessingSystems,36,2024.
Xander Davies, Max Nadeau, Nikhil Prakash, Tamar Rott Shaham, and David Bau. Discovering
variablebindingcircuitrywithdesiderata,2023.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
deepbidirectionaltransformersforlanguageunderstanding. InJillBurstein,ChristyDoran,and
Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of
theAssociationforComputationalLinguistics: HumanLanguageTechnologies,Volume1(Long
and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association for Com-
putationalLinguistics. doi: 10.18653/v1/N19-1423. URLhttps://aclanthology.org/
N19-1423.
10PublishedasaconferencepaperatICLR2024
NElhage,NNanda,COlsson,THenighan,NJoseph,BMann,AAskell,YBai,AChen,TConerly,
etal. Amathematicalframeworkfortransformercircuits. TransformerCircuitsThread,2021.
NelsonElhage,TristanHume,CatherineOlsson,NicholasSchiefer,TomHenighan,ShaunaKravec,
ZacHatfield-Dodds,RobertLasenby,DawnDrain,CarolChen,etal. Toymodelsofsuperposi-
tion. arXivpreprintarXiv:2209.10652,2022.
JiahaiFengandJacobSteinhardt. Howdolanguagemodelsbindentitiesincontext?,2023.
Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. Dissecting recall of factual
associationsinauto-regressivelanguagemodels. InHoudaBouamor,JuanPino,andKalikaBali
(eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Pro-
cessing, pp.12216–12235, Singapore, December2023.AssociationforComputationalLinguis-
tics.doi:10.18653/v1/2023.emnlp-main.751.URLhttps://aclanthology.org/2023.
emnlp-main.751.
Nicholas Goldowsky-Dill, Chris MacLeod, Lucas Sato, and Aryaman Arora. Localizing model
behaviorwithpathpatching,2023.
Almog Gueta, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem Choshen.
Knowledge is a region in weight space for fine-tuned language models. arXiv preprint
arXiv:2302.04863,2023.
Suchin Gururangan, Ana Marasovic´, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,
and Noah A Smith. Don’t stop pretraining: Adapt language models to domains and tasks. In
Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp.
8342–8360,2020.
Irene Heim. File change semantics and the familiarity theory of definiteness. Semantics Critical
ConceptsinLinguistics,pp.108–135,1983.
JeremyHowardandSebastianRuder. Universallanguagemodelfine-tuningfortextclassification.
In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics
(Volume1: LongPapers),pp.328–339,2018.
EdwardJHu,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,WeizhuChen,
et al. Lora: Low-rank adaptation of large language models. In International Conference on
LearningRepresentations,2021.
Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt,
Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. ArXiv,
abs/2212.04089, 2022. URL https://api.semanticscholar.org/CorpusID:
254408495.
SamyakJain,RobertKirk,EkdeepSinghLubana,RobertP.Dick,HidenoriTanaka,EdwardGrefen-
stette,TimRockta¨schel,andDavidScottKrueger. Mechanisticallyanalyzingtheeffectsoffine-
tuningonprocedurallydefinedtasks,2023.
HansKamp,JosefVanGenabith,andUweReyle. Discourserepresentationtheory. InHandbookof
PhilosophicalLogic: Volume15,pp.125–394.Springer,2010.
LauriKarttunen. Discoursereferents. InNotesfromthelinguisticunderground,pp.363–385.Brill,
1976.
Najoung Kim and Sebastian Schuster. Entity tracking in language models. arXiv preprint
arXiv:2305.02363,2023.
Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the dark se-
crets of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-
guage Processing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP),pp.4365–4374, HongKong, China, November2019.AssociationforCom-
putationalLinguistics. doi: 10.18653/v1/D19-1445. URLhttps://aclanthology.org/
D19-1445.
11PublishedasaconferencepaperatICLR2024
Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Fine-
tuning can distort pretrained features and underperform out-of-distribution. arXiv preprint
arXiv:2202.10054,2022.
Belinda Z. Li, Maxwell Nye, and Jacob Andreas. Implicit representations of meaning in neural
languagemodels. InChengqingZong,FeiXia,WenjieLi,andRobertoNavigli(eds.),Proceed-
ings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th
International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp.
1813–1827,Online,August2021.AssociationforComputationalLinguistics. doi: 10.18653/v1/
2021.acl-long.143. URLhttps://aclanthology.org/2021.acl-long.143.
KennethLi,AspenKHopkins,DavidBau,FernandaVie´gas,HanspeterPfister,andMartinWatten-
berg. Emergentworldrepresentations: Exploringasequencemodeltrainedonasynthetictask.
arXivpreprintarXiv:2210.13382,2022.
Tom Lieberum, Matthew Rahtz, Ja´nos Krama´r, Neel Nanda, Geoffrey Irving, Rohin Shah, and
Vladimir Mikulik. Does circuit analysis interpretability scale? evidence from multiple choice
capabilitiesinchinchilla,2023.
David Lindner, Ja´nos Krama´r, Sebastian Farquhar, Matthew Rahtz, Tom McGrath, and Vladimir
Mikulik. Tracr: Compiledtransformersasalaboratoryforinterpretability. AdvancesinNeural
InformationProcessingSystems,36,2024.
TiedongLiuandBryanKianHsiangLow. Goat: Fine-tunedllamaoutperformsgpt-4onarithmetic
tasks,2023.
AmanMadaan, ShuyanZhou, UriAlon, YimingYang, andGrahamNeubig. Languagemodelsof
codearefew-shotcommonsenselearners. InProceedingsofthe2022ConferenceonEmpirical
MethodsinNaturalLanguageProcessing,pp.1384–1403,2022.
Gary Marcus. The deepest problem with deep learning, De-
cember 2018. URL https://medium.com/@GaryMarcus/
the-deepest-problem-with-deep-learning-91c5991f5695.
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual
associationsinGPT. AdvancesinNeuralInformationProcessingSystems,36,2022.
Amil Merchant, Elahe Rahimtoroghi, Ellie Pavlick, and Ian Tenney. What happens to BERT em-
beddingsduringfine-tuning? InAfraAlishahi,YonatanBelinkov,GrzegorzChrupała,Dieuwke
Hupkes, Yuval Pinter, and Hassan Sajjad (eds.), Proceedings of the Third BlackboxNLP Work-
shop on Analyzing and Interpreting Neural Networks for NLP, pp. 33–44, Online, November
2020.AssociationforComputationalLinguistics. doi: 10.18653/v1/2020.blackboxnlp-1.4. URL
https://aclanthology.org/2020.blackboxnlp-1.4.
NeelNanda,LawrenceChan,TomLiberum,JessSmith,andJacobSteinhardt. Progressmeasures
forgrokkingviamechanisticinterpretability. arXivpreprintarXiv:2301.05217,2023.
Mante S Nieuwland and Jos JA Van Berkum. When peanuts fall in love: N400 evidence for the
powerofdiscourse. Journalofcognitiveneuroscience,18(7):1098–1111,2006.
ChrisOlah,AlexanderMordvintsev,andLudwigSchubert. Featurevisualization. Distill,2(11):e7,
2017.
Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter.
Zoomin: Anintroductiontocircuits. Distill,5(3):e00024–001,2020.
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,Chong
Zhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelstofollow
instructions with human feedback. Advances in Neural Information Processing Systems, 35:
27730–27744,2022.
AbhishekPanigrahi,NikunjSaunshi,HaoyuZhao,andSanjeevArora. Task-specificskilllocaliza-
tioninfine-tunedlanguagemodels. arXivpreprintarXiv:2302.06600,2023.
12PublishedasaconferencepaperatICLR2024
AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal.Language
modelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,Yanqi
Zhou,WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-text
transformer. TheJournalofMachineLearningResearch,21(1):5485–5551,2020.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang,andTatsunoriBHashimoto.Stanfordalpaca:Aninstruction-followingllamamodel,2023.
Shubham Toshniwal, Sam Wiseman, Karen Livescu, and Kevin Gimpel. Chess as a testbed for
languagemodelstatetracking. InProceedingsoftheAAAIConferenceonArtificialIntelligence,
volume36,pp.11385–11393,2022.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe´e
Lacroix, Baptiste Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-
mand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation
languagemodels,2023a.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Niko-
layBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfounda-
tionandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023b.
Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer,
and Stuart Shieber. Investigating gender bias in language models using causal mediation
analysis. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 12388–12401. Curran
Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/
paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf.
Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Inter-
pretabilityinthewild: acircuitforindirectobjectidentificationingpt-2small,2022.
John Wu, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass. Sim-
ilarity analysis of contextual word representation models. In Proceedings of the 58th An-
nual Meeting of the Association for Computational Linguistics, pp. 4638–4655, Online, July
2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.422. URL
https://aclanthology.org/2020.acl-main.422.
ZhengxuanWu,AtticusGeiger,ThomasIcard,ChristopherPotts,andNoahGoodman.Interpretabil-
ityatscale:Identifyingcausalmechanismsinalpaca.AdvancesinNeuralInformationProcessing
Systems,36,2024.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and
chatbotarena. arXivpreprintarXiv:2306.05685,2023.
ChuntingZhou,PengfeiLiu,PuxinXu,SriniIyer,JiaoSun,YuningMao,XuezheMa,AviaEfrat,
PingYu,LILIYU,SusanZhang,GargiGhosh,MikeLewis,LukeZettlemoyer,andOmerLevy.
LIMA:Lessismoreforalignment. InThirty-seventhConferenceonNeuralInformationProcess-
ingSystems,2023.
Yichu Zhou and Vivek Srikumar. A closer look at how fine-tuning changes BERT. In Smaranda
Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1046–1061,
Dublin,Ireland,May2022.AssociationforComputationalLinguistics. doi: 10.18653/v1/2022.
acl-long.75. URLhttps://aclanthology.org/2022.acl-long.75.
13PublishedasaconferencepaperatICLR2024
A UNRAVELING CRITICAL INFORMATION FLOW FOR ENTITY TRACKING
TASK THROUGH ATTENTION KNOCKOUT
DrawinginspirationfromtheAttentionKnockouttechniqueproposedinGevaetal.(2023),which
aimstoinvestigatetheflowofcrucialinformationfromthesubjecttokentothelasttokenposition,
we adapted this technique to understand how essential information for entity tracking is conveyed
withinLlama-7B.Inouradaptation,allattentionheadsofaspecificlayerandpositionareobstructed
fromattendingtoheadsinthesamelayeratadifferentposition,therebylimitingtheflowofinfor-
mationbetweenthesepositionsatthedesignatedlayer.
UnliketheapproachinGevaetal.(2023),whereawindowoflayersaroundthespecifiedlayerwas
blocked from attending to a previous position, our method initiates by blocking all layers. Subse-
quently, ateachstep, weprogressivelyunblockthenextpreviouslyblockedlayer. Moreprecisely,
we block attention heads of all layers at a given position from attending to heads in a different
position. Then, in subsequent steps, we systematically unblock each layer, revealing which layer
encodes vital information which when unblocked leads to improved performance of the model in
conductingentitytrackingtasks.
Blocking attention heads at last position
Model Accuracy
Random Position
0.80 Query Box Position
Correct Object Position
0.66
0.60
0.40
0.20
0.00
1 2 3 4 5 6 7 8 9 1011121314151617181920212223242526272829303132
Knockout Starting Layer
FigureA1: CriticalInformationFlowtoLastTokenPosition
InFig.A1,theresultsarepresentedwhenheadsatthelasttokenpositionarepreventedfromattend-
ing to various previous positions. It is observedthat there are twoprimary sources of information
thatheadsatthelasttokenpositionutilize: thequeryboxtokenandthecorrectobjecttokenposi-
tion. Specifically,headsintheinitiallayersfocusonandextractcrucialinformationfromthequery
boxtokenresidualstream,whileheadsinthelaterlayersattendtothecorrectobjecttoken,bringing
in another essential piece of information. As a baseline comparison, we blocked the heads from
attending to a randomly selected previous position, which did not result in a loss of performance.
Additionally,Fig.A2showstheresultswhenheadsatthequeryboxtokenpositionareblockedfrom
attendingtopreviouspositions.Weobservethatheadsintheinitiallayerstransportvitalinformation
fromthepreviousqueryboxtokenposition.
The findings from the attention knockout methods align with the information flow subgraph iden-
tifiedusingpathpatchinginsection.4.1,asshowninFig.1. Inotherwords,headsattheprevious
queryboxpositioncollectinformationabouttheirsegment,whichisthenpassedontothequerybox
token residual stream by the initial layer heads at that position. Subsequently, heads in the initial
layers at the final token position attend to the query box token position to incorporate it into their
14
ycaruccAPublishedasaconferencepaperatICLR2024
Blocking attention heads at query box position
Model Accuracy
Random Position
0.80
Prev Query Box Position
0.66
0.60
0.40
0.20
0.00
1 2 3 4 5 6 7 8 9 1011121314151617181920212223242526272829303132
Knockout Starting Layer
FigureA2: CriticalInformationFlowtoQueryBoxTokenPosition
residualstream. Thisinformationisutilizedbytheheadsinthelaterlayerstoattendtothecorrect
objecttokenpositionandconveyittothefinallogit.
B MINIMALITY
Wehaveutilizedtheminimalitytoidentifyheadsthatdonotcontributesignificantlytothecircuit
performance. Foreachheadv ∈ Cir, weseekasubsetofheadsK ⊆ Cir\{v}, suchthatwhen
headsinK areknockedoutofCir,vcanstillrecovertheperformanceofCirconsiderably. Unlike
in Wang et al. (2022) which primarily defined K as the heads in the same class G ⊆ Cir that v
belongs to, we use a greedy approach to compute the subset. We cannot use the entire class as
K, since some of the classes have a large number of heads, e.g. Value Fetcher Heads, that when
knocked out would result in a colossal decrease in performance, which cannot be recovered via a
singlehead. TodeterminethesubsetK associatedwithaheadv ∈ G,werankalltheotherheads
inGbasedonthedifferenceincircuitperformance. Thisdifferenceiscomputedbycomparingthe
circuit’s performance F(Cir), when only the other head v (where v is any head in G excluding
j j
v) is removed, and when both v and v are removed from the circuit. More specifically, we use
j
{F(Cir\{v })−F(Cir\{v,v })|v ∈G,v ̸=v}torankallheadsinG\vandthenconsider
j j j j
uptotop30%oftheheadstoformthesubsetK.
C EVALUATING LLAMA-7B CIRCUIT WITH COMPLETENESS CRITERION
To evaluate the integrity of the Llama-7B entity tracking circuit, we use an additional criterion;
completeness,whichcomparestheperformanceofthecircuitandthemodelunderknockouts. The
circuit is considered complete if eliminating any subset of its components, denoted as K ⊆ Cir,
resultsinacomparableperformanceimpacttoremovingthesamesubsetK fromtheentiremodel,
asdescribedinWangetal.(2022). Specifically,foreverysubsetK wecalculatetheperformanceof
thefullmodelM intheabsenceofK andcompareittothecircuitperformanceCirintheabsence
ofK,i.e.|F(Cir\K)−F(M\K)|. Wedefinethisastheincompletenessscore. IfthecircuitCir
iscomplete,itshouldhavealowincompletenessscore. SincethereareexponentiallymanyK,itis
computationallyintractabletoevaluatetheincompletenessscoreofeverypossiblesubsetK.Hence,
Wangetal.(2022)proposedafewsamplingmethodstocomputeK andweusethefollowingtwo:
1. Random: Weuniformlysampleagroupofcircuitcomponents.
2. Circuitgroups: WedefineK tobeoneoffourgroupsofthecircuit,i.e.A,B,C,andDas
showninFig.1.
15
ycaruccAPublishedasaconferencepaperatICLR2024
The results in Fig. A3 and Table A1 indicate that the Llama-7B minimal circuit is not perfectly
complete, i.e. there are additional heads in the model that are involved in the entity tracking task.
Higherincompletenessscorecanalsobeattributedtobackupheads,whichbecomeactiveonlywhen
otherrelevantheadsareexcludedduringforwardpropagation,asidentifiedbyWangetal.(2022).
Llama-7B circuit completeness tests
1.0
0.8
0.6
Random set
Group A
0.4
Group B
Group C
0.2
Group D
y = x
0.0
0.0 0.2 0.4 0.6 0.8 1.0
F(C\K)
FigureA3: CompletenessofLlama-7Bcircuit. Weplotthefullmodelaccuracyintheabsenceof
asubgroupvs. thecircuitaccuracyintheabsenceofthesamesubgroup. Inacompletecircuit,this
trendshouldrepresentequality(x = y). Ourcircuitisnotperfectlycompleteandadditionalheads
mightbeincluded.
Table A1: Completeness evaluation of Llama-7B circuit. For the random setting we report the
meanandstandardover10randomsubsetsK.
Accuracy
Full-Model Circuit Completeness
Random 0.11±0.06 0.0±0.01 0.1±0.06
GroupA 0.19 0.0 0.19
GroupB 0.23 0.02 0.21
GroupC 0.04 0.02 0.02
GroupD 0.58 0.46 0.12
D ENUMERATE LLAMA-7B CIRCUIT HEADS IN EACH GROUP
TableA2: GroupsofheadsintheLlama-7Bcircuit.
Group #Heads Functionality Name #DCM
A 40 Value ValueFetcher 40
B 7 Position PositionTransmitter 5
C 20 Position PositionDetector 14
D 5 - StructureReader -
A:ValueFetcher
L15 H13, L21 H3, L24 H5, L20 H14, L18 H8, L29 H7, L18 H3, L15 H18,
L17 H28, L21 H4, L21 H25, L23 H15, L18 H28, L23 H19, L23 H20, L19
H30, L23 H5, L17 H27, L15 H5, L21 H0, L23 H17, L15 H2, L17 H3, L19
H20, L19 H11, L19 H8, L15 H6, L20 H29, L16 H23, L24 H0, L25 H14,
L14 H13, L21 H26, L24 H8, L18 H6, L19 H26, L23 H16, L16 H27, L18
H20, L18 H25.
B:PositionTransmitter
16
)K\M(FPublishedasaconferencepaperatICLR2024
L14 H27, L11 H23, L12 H23, L19 H12, L13 H0, L16 H2, L13 H14
C:PositionDetector
L10 H3, L13 H14, L9 H2, L9 H7, L11 H23, L9 H10, L1 H9, L7 H17, L13
H0, L6 H10, L4 H4, L7 H26, L9 H21, L8 H1, L12 H0, L8 H22, L10 H4,
L11 H7, L7 H9, L10 H15
D:StructureReader
L8 H21, L12 H23, L11 H9, L8 H12, L11 H23
E CIRCUIT DISCOVERY IN GOAT-7B
Tobetterunderstandtheimpactoffine-tuningonunderlyingmechanismforperformingentitytrack-
ingtask, wealsoidentifythecircuitinGoat-7Bmodelresponsibleforperformingthistask, using
thesameprocedureasdescribedinSection4.1.TheGoat-7Bcircuitconsistsoffourgroupsofheads
positionedatthesametokenpositionsandsimilarlayers,connectedthroughthesametypeofcom-
positionasobservedintheLlama-7Bcircuit. Weincludeatotalof200attentionheads(80,30,50,40
headsinGroupsA,B,C,Drespectively)intheidentifiedcircuit. Toeliminateredundantheads, we
employtheminimalitycriterion,mirroringtheapproachusedfortheLlama-7Bcircuit,resultingina
circuitwith175heads.ThedistributionofheadsamonggroupsisdetailedinTableA3.Theminimal
circuitisevaluatedusingcompletenessandfaithfulnessmetricsinthefollowingsubsections.
E1 ENUMERATEGOAT-7BCIRCUITHEADSINEACHGROUP
TableA3: GroupsofheadsintheGoat-7Bcircuit.
Group #Heads Functionality Name #DCM
A 68 Value ValueFetcher 56
B 28 Position PositionTransmitter 15
C 40 Position PositionDetector 18
D 39 - StructureReader -
A:ValueFetcher
L24 H5, L17 H28, L20 H14, L21 H4, L21 H3, L18 H8, L23 H19, L19
H30, L30 H4, L23 H17, L29 H7, L19 H20, L23 H15, L31 H6, L28 H16,
L30 H8, L28 H17, L17 H8, L25 H14, L24 H8, L19 H8, L23 H16, L21
H25, L31 H26, L19 H11, L31 H25, L18 H20, L31 H23, L15 H12, L31 H1,
L23 H27, L19 H26, L21 H19, L20 H0, L19 H1, L17 H27, L20 H29, L20
H7, L17 H5, L18 H21, L31 H14, L15 H2, L18 H6, L16 H23, L15 H31,
L19 H23, L21 H11, L23 H31, L31 H0, L17 H24, L11 H5, L22 H17, L13
H10, L14 H9, L18 H23, L15 H24, L21 H17, L16 H27, L19 H2, L17 H23,
L24 H0, L15 H9, L31 H24, L19 H15, L24 H4, L25 H19, L14 H3, L31 H30
B:PositionTransmitter
L14 H27, L12 H23, L11 H23, L19 H12, L17 H26, L13 H1, L16 H16, L13
H0, L16 H2, L15 H4, L15 H13, L16 H28, L15 H18, L13 H25, L14 H11,
L14 H13, L10 H6, L13 H27, L12 H25, L12 H8, L12 H0, L11 H9, L18 H3,
L18 H28, L14 H10, L11 H26, L11 H24, L13 H3
C:PositionDetector
L10 H3, L13 H14, L6 H10, L11 H23, L11 H24, L9 H7, L1 H9, L9 H10,
L10 H7, L7 H17, L13 H0, L5 H7, L12 H0, L12 H8, L12 H23, L12 H20,
L13 H25, L13 H12, L4 H4, L13 H4, L12 H16, L11 H2, L11 H7, L7 H26,
L10 H4, L4 H27, L9 H21, L8 H1, L11 H28, L12 H30, L8 H12, L9 H30,
L15 H26, L13 H23, L6 H17, L13 H1, L8 H29, L8 H25, L13 H6, L12 H17
D:StructureReader
L8 H21, L11 H9, L12 H23, L11 H23, L12 H13, L9 H14, L9 H21, L10 H6,
L7 H2, L9 H29, L12 H29, L8 H13, L8 H12, L11 H28, L12 H30, L12 H25,
L8 H10, L8 H26, L8 H7, L10 H24, L6 H23, L9 H30, L11 H24, L9 H18,
L9 H12, L12 H0, L8 H19, L11 H11, L8 H9, L11 H20, L11 H26, L9 H6,
L8 H25, L9 H7, L9 H26, L10 H3, L9 H9, L12 H11, L7 H0
17PublishedasaconferencepaperatICLR2024
E2 EVALUATINGGOAT-7BCIRCUITWITHCOMPLETENESSCRITERION
To assess whether the identified Goat-7B circuit encompasses all the components engaged in ex-
ecuting the entity tracking task within the Goat-7B model, we utilize the completeness metric, as
detailedinSectionC.WeusetwosamplingmethodsforcomputingK: 1)Randomsamplingand2)
Circuitgroups.
Fig. A4 and Table A4 presents the completeness score of the Goat-7B circuit, revealing that the
circuitisnearlycomplete,exceptforheadsinGroupA.Thissuggeststhepossibilityofadditional
headscontributingtotheentitytrackingtask. ThehigherincompletenessinGroupAcouldalsobe
attributedtobackupheads,whichbecomeactiveonlywhenotherrelevantheadsareexcludedduring
forwardpropagation,asidentifiedby(Wangetal.,2022).
Table A4: Completeness evaluation of Goat-7B circuit. For the random setting we report the
meanandstandardover20randomsubsetsK.
Accuracy
Full-Model Circuit Completeness
Random 0.13±0.06 0.16±0.11 0.06±0.06
GroupA 0.41 0.21 0.2
GroupB 0.13 0.11 0.02
GroupC 0.06 0.11 0.05
GroupD 0.62 0.63 0.01
Goat-7B circuit completeness tests
1.0
0.8
0.6
Random set
Group A
0.4
Group B
Group C
0.2
Group D
y = x
0.0
0.0 0.2 0.4 0.6 0.8 1.0
F(C\K)
FigureA4: CompletenessofGoat-7Bcircuit. Weplotthefullmodelaccuracyintheabsenceofa
subgroupvs. thecircuitaccuracyintheabsenceofthesamesubgroup. Inacompletecircuit, this
trendshouldrepresentequality(x=y).
E3 EVALUATIONOFGOAT-7BCIRCUITWITHFAITHFULNESSCRITERION
ForamorethoroughevaluationoftheGoat-7BcircuitincomparisontotheentireGoat-7Bmodel,
we apply the faithfulness criterion as defined in Section 4.2. As presented in Table A5, Goat-7B
attainsafaithfulnessscoreof0.99,indicatingthattheidentifiedGoat-7Bcircuitcanrecoveralmost
the entire model performance. We also observe that the Goat-7B circuit has a similar faithfulness
scoreonFLoat-7B,suggestingahighoverlapbetweenthecomponentsthatperformentitytracking
taskinthesemodels.
WealsoobservethattheperformanceoftheGoat-7BcircuitonLlama-7BandVicuna-7Bishigher
than the entire model. Essentially, this implies that when the heads not present in the Goat-7B
circuitaremean-ablatedinLlama-7BandVicuna-7B,theirperformanceshowsimprovement. This
phenomenonalignswithobservationsmadeinVigetal.(2020),i.e.asmallproportionofheadscan
surpassthegenderbiaseffectoftheentiremodel. Ourspeculationrevolvesaroundthepresenceof
18
)K\M(FPublishedasaconferencepaperatICLR2024
TableA5:Entity-trackingcircuitfoundinGoat-7B,evaluatedinLlama-7B,Vicuna-7B,Goat-7B,
andFLoat-7B,withoutanyadjustmentofthecircuitgraph. Thecircuitachieveshighaccuracyand
faithfulnessscoresinallmodels(chanceaccuracyis0.14).
Accuracy
Model Finetuned? Full-Model Circuit RandomCircuit Faithfulness
Llama-7B – 0.66 0.77 0.00 1.17
Vicuna-7B Userconversations 0.67 0.76 0.00 1.13
Goat-7B Arithmetictasks(LoRA) 0.82 0.81 0.01 0.99
FLoat-7B Arithmetictasks(w/oLoRA) 0.82 0.79 0.02 0.96
negativeheads,i.e.,attentionheadsthatworkagainstpredictingthecorrectobject,inthesemodels,
contributingtoadecreaseintheiroverallperformance,asalsoreportedinWangetal.(2022).
E4 COMPARISONOFLLAMA-7BANDGOAT-7BCIRCUITS
We observe that, while the faithfulness scores of the Llama-7B and Goat-7B circuits in their re-
spective models are comparable (i.e., 1.0 and 0.99), there is a substantial disparity in their sizes.
Specifically, the Goat-7B circuit comprises 175 heads, whereas the Llama-7B circuit has only 72
heads. This notable difference in the number of attention heads in the circuits implies that fine-
tuning introduces additional components to the circuitry dedicated to solving the entity tracking
task. Indeed,TableA10andFig.A5suggestthattheGoat-7Bcircuitisapproximatelyasupersetof
theLlama-7Bcircuit. Additionally,mostofthehighlycausalheadsintheLlama-7Bcircuitremain
tobehighlyinfluencialintheGoat-7Bcircuit,suggestingthataminimalalterationinthebasemodel
circuit.
TableA6: IntersectionofAttentionHeadsinLlama-7BandGoat-7BCircuits: Comparisonof
thenumberofheadsineachgroupofLlama-7BandGoat-7Bcircuitsaswellastheirintersection.
HeadGroup Numberofheads Numberofheads Intersection Precision Recall
inLlama-7Bcircuit inGoat-7Bcircuit
A 40 68 27 0.68 0.4
B 7 28 6 0.86 0.21
C 20 40 16 0.8 0.4
D 5 39 5 1.0 0.13
F CIRCUIT DISCOVERY IN FLOAT-7B
InadditiontoexaminingthecircuitsofLlama-7BandGoat-7B,wealsoidentifythecircuitinthe
FLoat-7B model responsible for performing entity tracking tasks. The primary objective of ana-
lyzing the FLoat-7B model and its associated circuit is to establish whether the results generalize
to models fine-tuned without LoRA. Similar to other circuit identification process, we followed
theproceduredescribedinSection4.1,todiscovertheFloat-7Bcircuitwhichalsoconsistsoffour
groupsofheadslocatedatthesametokenpositionsandsimilarlayerstothatinLlama-7BandGoat-
7Bcircuits. Weinitiallyincludedatotalof200attentionheads(80,30,50,40headsinGroupsA,
B,C,D,respectively),whichweresubsequentlyreducedto175headsafterapplyingtheminimality
criterion. The distribution of heads in the minimal circuit is detailed in Section F1. Notably, we
found that the FLoat-7B circuit closely resembles the Goat-7B circuit, suggesting that the impact
offine-tuningwithandwithoutLoRAontheunderlyingmechanismforperformingentitytracking
tasksissimilar. Inthefollowingsubsections,weevaluatetheFLoat-7Bcircuitusingcompleteness
andfaithfulnesscriterion.
19PublishedasaconferencepaperatICLR2024
Causal Impact of Heads in Llama and Goat Circuit
Heads in Goat Circuit
0.3
Heads NOT in Goat Circuit
0.2
0.1
0.1
0.2 Heads in Llama Circuit
Heads NOT in Llama Circuit
0.3
0 25 50 75 100 125 150 175
Heads
FigureA5:CausalImpactofLlamaandGoatCircuitHeads:Thefirstsubplotshowstheattention
heads that are included in the Goat circuit (red), sorted based on their causal impact, i.e. absolute
patchingscore. Forcomparison, thesecondsubplotshowstheattentionheadsthatareincludedin
theLlamacircuit(green)inthesameorderastheGoatcomponentintheuppersubplot(e.g.thefirst
barinbothsubplotsrepresentsthesameheadthatispresentinboththecircuits,hencecoloredred
andgreenrespectively).
TableA7: GroupsofheadsintheFLoat-7Bcircuit.
Group #Heads Functionality Name #DCM
A 68 Value ValueFetcher 60
B 29 Position PositionTransmitter 13
C 40 Position PositionDetector 22
D 38 - StructureReader -
F1 ENUMERATEFLOAT-7BCIRCUITHEADSINEACHGROUP
A:ValueFetcher
L18 H3, L24 H5, L18 H8, L21 H4, L20 H14, L17 H28, L30 H4, L18 H28,
L23 H17, L19 H30, L23 H19, L28 H16, L15 H5, L19 H20, L31 H26, L15
H18, L25 H14, L19 H11, L15 H12, L29 H7, L17 H3, L30 H8, L19 H1,
L31 H6, L16 H23, L24 H0, L23 H27, L31 H29, L20 H29, L15 H22, L23
H16, L15 H6, L18 H23, L17 H24, L15 H15, L12 H16, L23 H31, L23 H30,
L29 H22, L18 H6, L20 H0, L27 H19, L21 H10, L25 H19, L21 H23, L13
H23, L22 H17, L18 H20, L17 H27, L19 H15, L14 H9, L21 H11, L13 H10,
L17 H5, L14 H13, L20 H26, L18 H25, L19 H24, L18 H21, L24 H11, L12
H9, L31 H23, L26 H16, L16 H15, L19 H17, L17 H15, L19 H16, L22 H5
B:PositionTransmitter
L14 H27, L11 H23, L12 H23, L13 H0, L13 H14, L13 H1, L19 H12, L16
H2, L17 H26, L16 H16, L14 H0, L15 H13, L12 H8, L15 H4, L14 H11,
L16 H28, L13 H25, L10 H6, L13 H27, L11 H26, L12 H30, L12 H21, L12
H5, L14 H17, L11 H5, L16 H17, L15 H26, L10 H12, L15 H1
C:PositionDetector
L10 H3, L13 H14, L7 H17, L11 H23, L9 H10, L9 H7, L6 H10, L11 H24,
L13 H0, L4 H4, L7 H3, L12 H9, L10 H4, L8 H1, L9 H21, L12 H23, L11
H2, L12 H8, L10 H7, L12 H0, L10 H21, L9 H15, L13 H4, L13 H12, L12
H5, L7 H9, L11 H7, L8 H29, L12 H20, L15 H26, L12 H17, L9 H1, L12
H30, L11 H28, L5 H5, L13 H1, L10 H18, L13 H26, L10 H6, L11 H19
D:StructureReader
L8 H21, L12 H23, L11 H9, L11 H23, L9 H21, L8 H12, L12 H13, L9 H14,
L8 H11, L10 H6, L11 H28, L9 H30, L12 H29, L6 H17, L9 H12, L8 H7,
L12 H30, L12 H25, L12 H15, L8 H26, L7 H30, L10 H24, L9 H9, L6 H25,
20
erocS
gnihctaP
etulosbAPublishedasaconferencepaperatICLR2024
L8 H13, L11 H24, L5 H4, L9 H26, L12 H11, L10 H12, L9 H6, L11 H11,
L12 H5, L7 H2, L9 H28, L10 H7, L7 H0, L6 H31
F2 EVALUATINGFLOAT-7BCIRCUITWITHCOMPLETENESSCRITERION
We assess the entirety for the FLoat-7B circuit using the completeness criterion, as described in
SectionC.Similartoothercircuits’evaluation,weutilizetwosamplingmethodsforcomputingK:
1)Randomsamplingand2)Circuitgroups.
Fig. A6 and Table A8 presents the completeness score of the FLoat-7B circuit. As with Goat-7B
circuit,FLaot-7Bcircuitisalmostcomplete,exceptfortheheadsinGroupA,suggestingeitherthe
presenceofadditionalheadsthatarefetchingthevalueofcorrectobjectorbackupheads, thatget
activatedwhenotherrelevantheadsareablatedduringforwardpropagation.
Table A8: Completeness evaluation of FLoat-7B circuit. For the random setting we report the
meanandstandardover20randomsubsetsK.
Accuracy
Full-Model Circuit Completeness
Random 0.2±0.07 0.12±0.1 0.09±0.04
GroupA 0.51 0.08 0.43
GroupB 0.15 0.11 0.04
GroupC 0.19 0.07 0.12
GroupD 0.68 0.57 0.11
FLoat-7B circuit completeness tests
1.0
0.8
0.6
Random set
Group A
0.4
Group B
Group C
0.2
Group D
y = x
0.0
0.0 0.2 0.4 0.6 0.8 1.0
F(C\K)
FigureA6: CompletenessofFLoat-7Bcircuit. Weplotthefullmodelaccuracyintheabsenceof
asubgroupvs. thecircuitaccuracyintheabsenceofthesamesubgroup. Inacompletecircuit,this
trendshouldrepresentequality(x=y).
F3 EVALUATINGFLOAT-7BCIRCUITWITHFAITHFULNESSCRITERION
Inadditiontothecompletenessscore,weassesstheFLoat-7Bcircuitusingthefaithfulnesscriterion,
whichevaluatestheperformanceoftheidentifiedcircuitcomparedtotheentiremodel. Weobserve
that the FLoat-7B circuit attains a faithfulness score of 1.0 on the FLoat-7B model, indicating its
capabilitytorecovertheentiremodelperformance. Furthermore,wenotethatitsfaithfulnessonthe
Goat-7B circuit is 0.93, suggesting a high degree of overlap between the circuits of the FLoat-7B
andGoat-7Bmodelsforperformingentitytracking.
21
)K\M(FPublishedasaconferencepaperatICLR2024
TableA9: Entity-trackingcircuitfoundinFLoat-7B,evaluatedinLlama-7B,Vicuna-7B,Goat-
7B,andFLoat-7Bwithoutanyadjustmentofthecircuitgraph. Thecircuitachieveshighaccuracy
andfaithfulnessscoresinallmodels(chanceaccuracyis0.14).
Accuracy
Model Finetuned? Full-Model Circuit RandomCircuit Faithfulness
Llama-7B – 0.66 0.69 0.0 1.05
Vicuna-7B Userconversations 0.67 0.7 0.0 1.04
Goat-7B Arithmetictasks(LoRA) 0.82 0.76 0.01 0.93
FLoat-7B Arithmetictasks(w/oLoRA) 0.82 0.82 0.02 1.0
F4 COMPARISONOFLLAMA-7BANDFLOAT-7BCIRCUITS
Togaininsightsintohowfine-tuningimpactstheunderlyingmechanismforperformingentitytrack-
inginLlama-7B,wecomparetheidentifiedcircuitsofLlama-7BandFLoat-7Bmodels. Similarto
theobservationwiththeGoat-7Bmodel,wenoteasubstantialdifferenceinthesizesoftheLlama-
7B and FLoat-7B circuits (72 and 175, respectively). This difference in circuit sizes suggests that
fine-tuningintroducesadditionalcomponentsdedicatedtothetaskofentitytracking. Thisisfurther
supportedbytheresultspresentedinTableA10andFig.A7,demonstratingthattheFLoat-7Bcircuit
indeed forms a superset of the Llama-7B circuit. Additionally, most of the highly causal heads in
theLlama-7BcircuitremainhighlyinfluentialintheGoat-7Bcircuit,suggestingminimalalteration
inthebasemodelcircuit.
TableA10: IntersectionofAttentionHeadsinLlama-7BandFLoat-7BCircuits: Comparison
ofthenumberofheadsineachgroupofLlama-7BandFLoat-7Bcircuitsaswellastheirintersection.
HeadGroup Numberofheads Numberofheads Intersection Precision Recall
inLlama-7Bcircuit inFLoat-7Bcircuit
A 40 68 27 0.68 0.4
B 7 29 7 1.0 0.24
C 20 40 15 0.75 0.38
D 5 38 5 1.0 0.13
Causal Impact of Heads in Llama and FLoat Circuit
Heads in FLoat Circuit
0.3
Heads NOT in FLoat Circuit
0.2
0.1
0.1
0.2 Heads in Llama Circuit
Heads NOT in Llama Circuit
0.3
0 25 50 75 100 125 150 175
Heads
Figure A7: Causal Impact of Llama and FLoat Circuit Heads: The first subplot shows the at-
tention heads that are included in the FLoat circuit (red), sorted based on their causal impact, i.e.
absolutepatchingscore. Forcomparison,thesecondsubplotshowstheattentionheadsthatarein-
cludedintheLlamacircuit(green)inthesameorderastheFLoatcomponentintheuppersubplot
(e.g.thefirstbarinbothsubplotsrepresentsthesameheadthatispresentinboththecircuits,hence
coloredredandgreenrespectively).
22
erocS
gnihctaP
etulosbAPublishedasaconferencepaperatICLR2024
G DESIDERATA FOR DCM
To identify the functionality of various circuit components, we conceptualized and defined three
desiderata:
(1)Objectdesideratumisusedtoidentifyattentionheadsthatencodethevalueofthecorrectobject
intheiroutput. Consequently,whentheoutputofthesecomponentsispatchedfromthecounterfac-
tualrun(whichcontainsadifferentcorrectobjectvalue,associatedwithacompletelydifferentbox
label)totheoriginalrun,thefinaloutputchangestothevalueofthecorrectobjectofthecounter-
factualexample,asshowninFig.2(a). Thisoccurseventhoughthatobjectwasnotincludedinthe
originalstatement.
(2)Labeldesideratumisusedtoidentifycircuitcomponentsthatareencodingthequeryboxlabel
value. Hence,whentheiroutputispatchedfromthecounterfactualrun(thatqueriedadifferentbox
label)totheoriginalrun,thefinaloutputoftheoriginalrunchangestotheobjectfromtheoriginal
runthatisassociatedwiththequeryboxlabelofthecounterfactualstatement,asshowninFig.2(b).
Notably, this occurs even though that object was not initially associated with the query box in the
originalstatementbutratherwiththequeryboxofthecounterfactualone,anditisnotpresentinthe
counterfactualstatement.
(3)Positiondesideratumisusedtoidentifycircuitcomponentsthatencodethepositionalinforma-
tionofthecorrectobject,i.e. whentheyarepatchedfromcounterfactualruntotheoriginalrun,the
finaloutputoftheoriginalrunchangestotheobjectintheoriginalstatementlocatedatthesamepo-
sitionasthecorrectobjectofthecounterfactualstatement,asshowninFig.2(c). Thishappenseven
thoughthisobjectisnotthecorrectobjectoftheoriginalrunandisnotpresentinthecounterfactual
statement.
Foreachofthethreedesiderata,wetrainabinarymaskoverthemodelcomponentstoidentifythe
circuitcomponentsencodingthecorrespondingvitalinformationtoaccomplishthetask.
H DCM EXPERIMENT DETAILS
Asmentionedinsection5.2,werestrictedthemodelcomponentsearchspacetotheheadsineach
group. More specifically, for each group in the circuit, we identify the subset of heads that are
involvedinthreefunctionalities: 1)encodingobjectvalue,2)boxlabelvalue,and3)correctobject
position. Wesyntheticallygeneratedtraining(N = 1000)andevaldatasets(N = 500),according
tothedesiderata. Totrainabinarymaskconsistingoflearnableparameters(W),weminimizethe
followinglossfunction:
(cid:88)
L=−logit +λ 1−W (1)
target
withλ=0.01. Wetraineditfortwoepochs,withADAMoptimizerandabatchsizeof32.
I CIRCUIT FUNCTIONALITY IN GOAT-7B AND FLOAT-7B
To identify the functionality of the head groups within the Goat-7B and FLoat-7B circuits, we
adopted a procedure similar to the one detailed in section 5. Initially, we employed DCM to lo-
calize the subset of heads within each group encoding various functionalities. Subsequently, we
conductedactivationpatchingonthesesubsetsofheads.
AsdepictedinTableA3aswellasFig.A8forGoat-7Bcircuit,andinTableA7aswellasFig.A9
for FLoat-7B circuit, the functionality of each head group mirrors that of the Llama-7B circuit.
Specifically,GroupAheadspredominantlyencodethevalueofthecorrectobject,whileGroupsB
and C are tasked with encoding the positional information of the correct object. Nevertheless, the
functionalityofheadsinGroupDcontinuestoremainamystery.
J ADDITIONAL DESIDERATA FOR POSITIONAL INFORMATION
Insection5,wefoundthatthemodelsareusingthepositionalinformationofthecorrectobjectfor
entitytracking. However,wecouldnotpreciselycharacterizethepositionalinformation. Therefore,
23PublishedasaconferencepaperatICLR2024
(A) Value fetcher (B) Position transmitter (C) Position detector (D) Structure reader
0.6 Llama-7B
Vicuna-7B
FLoat-7B
0.4 Goat-7B
Chance (position)
0.2
0.0
Label Object Position Label Object Position Label Object Position Label Object Position
FigureA8: ActivationpatchingresultsofthefunctionalityheadsintheGoat-7Bcircuit, identified
usingDCM.Errorbarsindicatestandarddeviation.
(A) Value fetcher (B) Position transmitter (C) Position detector (D) Structure reader
0.6 Llama-7B
Vicuna-7B
FLoat-7B
0.4 Goat-7B
Chance (position)
0.2
0.0
Label Object Position Label Object Position Label Object Position Label Object Position
FigureA9: ActivationpatchingresultsofthefunctionalityheadsintheFloat-7Bcircuit,identified
usingDCM.Errorbarsindicatestandarddeviation.
we devised a few additional positional desiderata, as shown in Tables A12 and A13. Similar to
ouranalysisonpreviouslydefineddesiderata,weappliedactivationpatchingonthesubsetofheads
in the Position Transmitter group that encodes positional information. In other words, we were
interestedinunderstandingwhatkindofpositionalinformationisbeingusedbyValueFetcherheads
tolocatethecorrectobjectinthecontext.
Results are summarized in Table A11. Although redundant text at the start, end, or between the
objectandtheboxdoesnotimpactthepositionalinformationencodedinthoseheads,anadditional
segmentatthestartdoesseemtoaffectit. Further,mentionsofboxeswithlabelsbeforethecorrect
segment(thesegmentcontainingthecorrectobject)alsointerferewiththepositionalinformation.In
additiontorelativepositionalinformation,thesemanticsoftheassociationbetweentheobjectand
theboxarealsoinfluential. Combiningtheseresults,wespeculatethatthemodelisenumeratingthe
associationoftheboxesandtheircorrespondingobjectfromthestarttokenaswellaskeepingtrack
ofthesemanticsoftheassociation. However, moreworkisneededtocompletelycharacterizethe
positionalinformation.
TableA11:PositionaldesideratatocharacterizethepositionalinformationtransportedbythePosi-
tionTransmitterheadstotheValueFetcherheads. PleaserefertoTableA12andA13forexamples.
Positionhyp. Acc. Var.
1 Randomtextatthestart 0.35 0.02
2 Randomtextattheend 0.34 0.01
3 Additionaltokensbetweenobjectandbox 0.38 0.02
4 Additionalsegmentatthestart 0.25 0.02
5 Additionalsegmentattheend 0.31 0.02
6 Additionalboxesbeforecorrectsegment 0.2 0.02
7 Incorrectboxsegment 0.16 0.01
8 Alteredboxobjectorder 0.16 0.01
9 Alteredassociationbtwboxandobject 0.18 0.02
10 Nocommatoseparatesegments 0.38 0.02
11 Additionalcommaaftertheobject 0.36 0.02
24
ycaruccA
ycaruccAPublishedasaconferencepaperatICLR2024
Table A12: Examples of Positional desiderata: For each positional desiderata reported in Table
A11,thistablecontainsspecificexamplesoforiginal,alternate,andtargetdata.
Desiderata Base Source Target
ThedocumentisinBoxX,thepotis There are a bunch of boxes con-
in Box T, themagnetisinBoxA, taining objects, the magnet is in
Box O, the bell is in Box M,
1 the game is in Box E, the bill is in magnet
Box M, the cross is in Box K, the theleafisinBoxW, the cup is in
mapisinBoxD.BoxScontainsthe BoxG,theiceisinBoxJ,themilk
is in Box Z, the wire is in Box H.
BoxWcontainsthe
The document is in Box X, the pot The document is in Box Q, the bus
is in Box T, the magnet is in Box isinBoxF,thecameraisinBoxR,
A, the game is in Box E, the bill theglassisinBoxW,themagazine
2 map
is in Box M, the cross is in Box K, is in Box Z, the coffee is in Box E,
themapisinBoxD. Box A con- thewatchisinBoxC, these are a
tainsthe bunch of boxes containing objects.
BoxCcontainsthe
The document is in Box X, the The pot is in Box U, the flower
pot is in Box T, the magnet is is in Box D, the car is in
in Box A, the game is in Box E, Box K, the disk is in Box C,
3 bill
thebillisinBoxM,thecrossisin thefaniscontainedintheBoxH,
BoxK,themapisinBoxD.BoxA the bill is in Box S, the painting is
containsthe inBoxL.BoxHcontainsthe
The document is in Box X, the TheappleisinBoxO,thedressisin
pot is in Box T, the magnet is BoxN,thebootisinBoxY,thehat
in Box A, the game is in Box E, is in Box L, thebusisinBoxX,
4 game
thebillisinBoxM,thecrossisin thepaintingisinBoxF,thedrugis
BoxK,themapisinBoxD.BoxA inBoxJ,thestringisinBoxD.Box
containsthe Xcontainsthe
The document is in Box X, the pot ThehatisinBoxK,theplaneisin
is in Box T, the magnet is in Box BoxH,thetieisinBoxU,thewireis
A, the game is in Box E, the bill inBoxF,thefileisinBoxR,thenote
5 map
is in Box M, the cross is in Box K, is in Box Y, thetrainisinBoxG,
themapisinBoxD. Box A con- the apple is in Box O. Box G con-
tainsthe tainsthe
The document is in Box X, the pot The hat is in Box K, the plane
isinBoxT,themagnetisinBoxA, is in Box H, the tie is in Box U,
thegameisinBoxE,thebillisin there are three additional boxes,
6 game
Box PP, Box BB and Box AA,
Box M, the cross is in Box K, the
mapisinBoxD.BoxAcontainsthe thewireisinBoxF, the file is in
Box R, the note is in Box Y, the
train is in Box G. Box F contains
the
The document is in Box X, the The magnet is in Box O, the
pot is in Box T, the magnet is bell is in Box M, the leaf is in
in Box A, the game is in Box E, Box W, the cup is in Box G,
7 cross
thebillisinBoxM,thecrossisin theiceisinBoxJ, the milk is in
BoxK,themapisinBoxD.BoxA Box Z, the wire is in Box H. Box J
containsthe containsthe
The document is in Box X, the pot The pot is in Box U, the flower
isinBoxT,themagnetisinBoxA, is in Box D, the car is in Box K,
thegameisinBoxE,thebillisin BoxCcontainsthedisk,thefanis
8 game
Box M, the cross is in Box K, the in Box H, the bill is in Box S, the
mapisinBoxD.BoxAcontainsthe paintingisinBoxL.BoxCcontains
the
25PublishedasaconferencepaperatICLR2024
Table A13: Examples of Positional desiderata: For each positional desiderata reported in Table
A11,thistablecontainsspecificexamplesoforiginal,alternate,andtargetdata.
Desiderata Base Source Target
The document is in Box X, the pot The ticket is in Box N, the book
isinBoxT,themagnetisinBoxA, is in Box J, the gift is in Box W,
the game is in Box E, the bill is in the coat is in Box Y, the rose is in
9 cross
Box M, thecrossisinBoxK, the Box K, thewheelisnotinBoxG,
mapisinBoxD.BoxAcontainsthe thebrickisinBoxV.BoxGcontains
the
The document is in Box X, the ThepotisinBoxUtheflowerisin
pot is in Box T, the magnet is Box D the car is in Box K the disk
in Box A, the game is in Box E, isinBoxC thefanisinBoxH the
10 bill
thebillisinBoxM,thecrossisin billisinBoxSthepaintingisinBox
BoxK,themapisinBoxD.BoxA L.BoxHcontainsthe
containsthe
The document is in Box X, the pot Theclock,isinBoxM,thebomb,is
isinBoxT,themagnetisinBoxA, in Box J, the newspaper, is in Box
thegameisinBoxE,thebillisin G, theletter,isinBoxL, the suit,
11 bill
Box M, the cross is in Box K, the isinBoxY,thecomputer, isinBox
mapisinBoxD.BoxAcontainsthe R,thewheel,isinBoxV.BoxLcon-
tainsthe
26