Rao-Blackwellising Bayesian Causal Inference
ChristianToth1 ChristianKnoll1,2 FranzPernkopf1 RobertPeharz1
1GrazUniversityofTechnology
2LevataGmbH
Abstract
biased results. To address these issues, Toth et al. [2022]
propose to infer a Bayesian posterior over some causal
quantityofinterest,suchasanaveragetreatmenteffect,the
Bayesian causal inference, i.e., inferring a pos- presence of a causal connection, or the full causal graph,
terior over causal models for the use in down- by marginalising over posterior structural causal models
stream causal reasoning tasks, poses a hard com- (SCMs).
putationalinferenceproblemthatislittleexplored
Although this is a conceptually appealing approach sub-
inliterature.Inthiswork,wecombinetechniques
sumingbothBayesiancausaldiscoveryandreasoning,pos-
from order-basedMCMC structure learningwith
teriorinferenceoverentireSCMs isingeneralintractable.
recentadvancesin gradient-basedgraphlearning
Hence, most research focuses on the on the (still hard)
intoaneffectiveBayesiancausalinferenceframe-
sub-problem of causal structure learning. While classical
work.Specifically,wedecomposetheproblemof
MCMC-based approaches to structure learning often de-
inferring the causal structure into (i) inferring a
composeDAGlearninginto(i)learningatopologicalvari-
topologicalorderovervariablesand(ii)inferring
ableorderand(ii)conditionalontheorder,learningthepar-
the parent sets for each variable. When limiting
entsetsforeachvariable[KollerandFriedman,2003,Koiv-
thenumberofparentspervariable,wecanexactly
istoandSood,2004],recentgradient-basedDAGstructure
marginalise over the parent sets in polynomial
learningtechniquesfocusesonsoftacyclicityconstraintsor
time.WefurtheruseGaussianprocessestomodel
usestopologicalordersonlyasavehicleto sampleDAGs.
the unknown causal mechanisms, which also al-
Surprisingly, a combination of these two approaches has
lows their exact marginalisation. This introduces
beenlittleexploredintheliterature.
a Rao-Blackwellization scheme, where all com-
ponents are eliminated from the model, except In this work, we close this gap by combining the best
for the causal order, for which we learn a distri- ofbothworlds:weemploygradient-basedoptimisationto
butionvia gradient-basedoptimisation.Thecom- learnadistributionovercausalorders,and,bylimitingthe
binationofRao-Blackwellizationwithoursequen- number of parents per variable, we can perform exact in-
tial inference procedure for causal orders yields ferenceoverparentsetsinpolynomialtime.Wefurtheruse
state-of-the-art on linear and non-linear addit- Gaussianprocessestomodeltheunknowncausalmechan-
ive noise benchmarks with scale-free and Erdos- isms,whichalsoallowstheirexactmarginalisation.Thisef-
Renyigraphstructures. fectivelyintroducesaRao-Blackwellizationscheme,where
all componentsare eliminated from the model, exceptfor
the causal order. Combining this with a sequential infer-
1 INTRODUCTION enceprocedureovertopolgicalorderyieldsstate-of-the-art
estimation results on linear and non-linear additive noise
In recent work, Toth et al. [2022] pointed out, that the benchmarks with scale-free and Erdos-Renyi graph struc-
classical separation of causal discovery and causal reas- tures.Ourmaincontributionsare:
oninginto distinct,consecutivetasksdoesnotaccountfor
theepistemicuncertaintyaboutthecausalstructureduring • We propose an effective Bayesian causal inference
thecausalreasoningphase.Moreover,Graduetal.[2022] framework by Rao-Blackwellising the hard problem
showthatreusingdataduringthereasoningphasethathas ofBayesiancausalinferenceandshowitspracticalsu-
alreadybeenused forthecausaldiscoverytask mayyield periorityinsimulatedbenchmarks.
Submittedtothe40thConferenceonUncertaintyinArtificialIntelligence (UAI2024).
4202
beF
22
]GL.sc[
1v18741.2042:viXra• Interpreting our estimation procedure from the per- θ L
spective of importance sampling, we analyse the re-
quirementsofthe involvedproposaldistributionover G ψ f
parent sets and causal orders. We argue that these
φ Pa
considerations are likewise relevant to variational
D
inference-basedapproaches.
• We provide a modular Python-based implementation
Figure1:Bayesiannetworkviewofourproposedinference
ofourinferenceframeworktothecommunity.
framework.Weinfertheposteriorparametersθandφofa
parameterisedjointdistributionp(L,Pa|θ,φ)overcausal
orders L and (unconstrained) parent sets Pa. Our model
2 BACKGROUND
factorises the joint into the marginal over causal orders
p(L|θ),andtheconditionaloverunconstrainedparentsets
2.1 STRUCTURALCAUSALMODELS
p(Pa|L,φ). The causal DAG G = ML ⊙ Pa is determ-
inisticallycomputedbyelement-wisemultiplicationofthe
Themathematicaltreatmentofcausalquestionsrequiresa
adjacency mask induced by L, and the unconstrainedpar-
well-posedmodel.Oneofthemostprominentcausalmod-
entsetmatrixPa. Forafixedmaximumcardinalityofthe
elsisthestructuralcausalmodel(SCM)[Pearl,2009]:
parentsets,wemarginaliseoverDAGsbyexhaustivelyenu-
meratingallpossibleparentsets(whichobsoletesinference
Definition 2.1 (SCM). An SCM M over observed endo-
ofφ).Additionally,weinfertheposteriorparametersψof
genousvariablesX ={X ,...,X }andunobservedexo-
1 d
the parameterised joint distribution over mechanisms and
genousvariablesU = {U ,...,U }consistsofstructural
1 d noise variables p(f,U|ψ). In our case, ψ are the hyper-
equations,ormechanisms,
parametersof a set of Gaussian Processes, but they could
X :=f (Pa ,U ), for i∈{1,...,d}, (2.1) also be the parameters of a neural network, etc. Mechan-
i i i i
isms,noisedistributionandgraphtogethergiverisetothe
whichassignthevalueofeachX asadeterministicfunc-
data-generatinglikelihoodp(D|f,ψ,G).
i
tion f ofits directcauses, or causalparents,Pa ⊆ X \
i i
{X }andanexogenousvariableU ,andajointdistribution
i i LetλL: X7→{1,...,d}bethebijectivemappingbetween
p(U)overtheexogenousvariables.
X andindicesinL,i.e., λL(X )=k ⇐⇒ L =X .We
i k i
denotebyQL ∈ {0,1}d×d thepermutationmatrix repres-
Associated with each SCM is a directed acylclic graph enting L, where QL = 1 iff λL(X ) = j. Further, we
(DAG)GinducedbythesetofparentsetsPa = {Pa i}d i=1 denote by ML ∈ {ij 0,1}d×d the adjai cency mask induced
with vertices X and edges X j → X i if and only if byL,whereML =1iffλL(X )<λL(X ).
X ∈ Pa .AnyacyclicSCMtheninducesauniqueobser- ij i j
j i
vational distribution p(X|M) over the endogenous vari-
ablesX,whichisobtainedasthepushforwardmeasureof 3 BAYESIAN CAUSAL INFERENCE VIA
p(U)throughthecausalmechanismsinEq.(2.1). CAUSALORDERS
TheBayesiancausalinferenceproblem[Tothetal.,2022]
2.2 CAUSALORDERS
istoinferaBayesianqueryposterior
A permutationL = hL 1,...,L di of the endogenousvari- p(Y |D)=E [p(Y |M)] (3.1)
ables X = d {L }, where L 6= L for all i 6= j, en- M|D
i=1 i i j
tails a strict total order among L ≺ L ≺ ··· ≺ L by marginalising over posterior SCMs given a set of (ob-
1 2 d
S
the variables. Henceforth, we refer to such a permutation servational)1 dataD = {x i ∼.i.d p(X|M∗)}N collected
n n=1
L as a causal order. A causal order L constrains the pos- fromthetrueunderlyingSCMM∗.ThequeryY couldbe,
sible causal interactionsbetween the causal variables,i.e.,
e.g.,anaveragetreatmenteffectorthefullcausalgraph.In
X i canbea(direct)causeofX j if,andonlyif,X i ≺ X j general, the marginalisationover SCMs is intractable and
inL.
posesthekeycomputationalchallengeforBayesiancausal
WedefineL = hL ,...,L itobethefirstk−1ele- inference. For our inference framework as described in
<k 1 k−1
mentsinLandlet[L ] = k−1{L }.Wewilloftenuse
<k i=1 i 1Whileour frameworkandimplementationcanhandleinter-
vectorsof length d, for example θ = (θ ,...,θ ), whose
1 d ventional data, we do not evaluate this scenario experimentally
S
entriescorrespondtoendogenousvariables.Wewillindex because not all baselines support the use of interventional data,
suchvectorsbyasubsetofXtoformasub-vector,forex-
andtheobservationalcaseisthemoredifficultproblemfromthe
ampleθ [L<k] =(θ i|X i ∈[L <k]). perspectiveofmodelidentifiability.
2Fig.1,anSCMisparameterisedasatripleM=(G,f,ψ) whichisintractableingeneral.Aswefocuson(non-)linear
of the causal graph G, the causal mechanisms f, and the additivenoise modelsin this work,we follow[Toth etal.,
parameters of mechanisms and exogenous noise distribu- 2022]andmodeleachmechanismsviaadistinctGaussian
tions ψ. Thus, an expectation w.r.t. SCMs as in Eq. (3.1) process(GP),assuminghomoscedasticGaussiannoiseand
canbewrittenasanimportanceweightedexpectation(see causalsufficiency.TheadvantageofusingGPsis,thatthe
Appx.A) ingeneralintractablelikelihood
E M|D[Y(M)]= (3.2) p(D i|ψ i,G)=E f|ψi,G[p(D i|f i,ψ i,G)] (3.6)
E
θ,φ,ψ|D
E
L|θ
E
G|L,φ
w·E f|ψ,D[Y(M)]
can be computedin closed form. The same holdstrue for
where (cid:2) (cid:2) (cid:2) (cid:3)(cid:3)(cid:3) thepredictiveposteriorp(Dtest|ψ ,G,D)possiblyneeded
i i
inEq.(3.2).
p(D|ψ,G)·p(ψ|G)
w:= . (3.3)
E L′|θ E G′|L′,φ[p(D|ψ,G′)·p(ψ|G′)] We factorise the GP hyper-parameterprior3 andthe likeli-
hoodoverparentsetsforeachnodei,i.e.,
This formulati(cid:2)on, inspired by Lorch et al. [2021(cid:3)], trans-
forms inference over SCMs into the problem of inferring
p(D|ψ,G)·p(ψ|G)= p(D |ψ ,G)·p(ψ |PaG),
posteriorparametersp(θ,φ,ψ|D)ofagenerativemodel, i i i i
i
aslaidoutinFig.1.Theso-learnedparameteriseddistribu- Y
(3.7)
tion over causal graphs p(G|θ,φ) = E [p(G|L,φ)]
L|θ
appearing in Eq. (3.2) can be interpreted as a proposal allowingforefficientcomputationbycachingintermediate
distribution,2 and the w as defined in Eq. (3.3) as import- results.ToinfertheindividualGPhyper-parametersweper-
ance weight. Thus, to practically estimate the expectation formforaMAP-TypeIIestimationbyperforminggradient-
in Eq. (3.2), we use a simple Monte-Carlo estimator, by ascenton
drawing proposal SCMs from our generative model and
weighting their query value Y(M) according to their im- ∇ ψilogp(ψ i|D,G)= (3.8)
portanceweights. ∇ p(D |ψ ,G)+∇ p(ψ |G).
ψi i i ψi i
Ensuringthatourproposaldistributionsaresufficientlyex-
Forrootnodes,i.e.,nodeswithoutparents,weplaceacon-
pressive,itsufficestoinferamaximumaposteriori(MAP)
jugate normal-inverse-gammaprior on the mean and vari-
estimate of the posterior parameters θ,φ,ψ via gradient-
anceofthatnode,whichalsoallowsforclosed-forminfer-
basedoptimisationbymaximisingthelog-posterior:
ence.
∇logp(θ,φ,ψ|D)= (3.4)
∇logp(θ,φ)+∇logE [p(D,ψ|G)]. 3.1.2 ParentSetInference
Pa,L|θ,φ
A major issue for the training of such a model is the
Themarginalisationw.r.t.parentsetsinEqs.(3.2)and(3.4)
quality of the gradient estimation in the face of the high-
poses a hard combinatorial problem. Although, the num-
dimensional problem space. Likewise, using a Monte-
ber of DAGs consistent with any given (causal) order
CarloestimateofEq.(3.2)maysufferfromhighvariance.
d·(d−1)
To mitigate these issues, we now proposeseveralways to is 2 2 and as such significantly smaller than the
total number of DAGs with d nodes, which grows super-
Rao-Blackwelliseourinferenceprocedure.
exponentially in d (see [OEIS Foundation Inc., 2024]).
Thus, an exhaustive enumeration of DAGs is still infeas-
3.1 RAO-BLACKWELLIZINGTHEINFERENCE ible.
PROCEDURE
Inthiswork,weexploretwodifferentwaysoftacklingthis
problem.First,byrestrictingthenumberofparentspervari-
We deferderivationsand proofsin the contextof this sec-
able,i.e.introducingacardinalityconstraintsontheparent
tiontoAppx.A.
sets, we can tractably compute the expectation w.r.t. par-
3.1.1 MechanismInference entsetsviaexhaustiveenumeration.Second,weproposea
tractable and expressive proposal distribution p(Pa|L,φ)
TocomputetheimportanceweightsinEq.(3.3)wehaveto overunrestrictedparentsetsPaconditionalonacausalor-
computethemarginallog-likelihood derLthatweusetocomputeMonteCarloestimates.
p(D|ψ,G)=E f|ψ,G[p(D|f,ψ,G)·p(f|ψ)], (3.5)
3The parameter set ψi contains the GP’s kernel parameters
2Note,thatthelearneddistributionsarenotper-setheactual andthenoisevariance.Notethat,otherthanthestructuraldepend-
posteriordistribution,althoughtheywouldbeoptimalproposals enceontheparentsets,thepriorparametersψareindependentof
iftheywere. φandθ.
3ExhaustiveEnumerationofParentSets. Byrestricting Algorithm1:SampleParentSets
themaximumsizeofanyadmissibleparentsettosomein-
Input:Logitfunction
tegerK,thenumberofdistinctparentsetsconsistentwith h :Rd×d 7→(RM,RM×d×K,Rd×K×d)
φ
any causal order L is polynomial in K. Although the ex-
Output:ParentsetsPasampledfromp(Pa|L,φ)
haustiveenumerationofallDAGswithrestrictedparentset φ , φ , φ ←h(QL) ⊲Step I
sizeisstillinfeasible,itturnsout,thatgivenacausalorder, mTL ∼M CAM TO (M φ M )N ⊲Step II
theexpectationw.r.t.graphscanbetractablycomputedfor k ∼CAT(φ TLM [m]) ⊲Step III
quantitiesY(G)thatdecomposeovertheparentsets. Pa←{pa ∼MOM MN(Pa |φ [i,k])} ⊲Step IV
i i MN
Proposition 3.1. Let Y(G) = Y (PaG) and w(G) =
i i i
w(PaG)befactorisingovertheparentsets,thenwecan
i i Q
compute score-functionestimator, the gradientin Eq. (3.4) w.r.t. φ
Q
canbeestimatedas
E [w(G)Y(G)]= (3.9)
G|L
∇ logp(θ,φ,ψ|D)= (3.11)
p(pa |L)w (pa )Y (Pa ). φ
i i i i i
Yi XPai ∇ φlogp(φ)+E L|θ E Pa|L,φ[w·∇ φp(Pa|L,φ)] .
(cid:2) (cid:3)
Additionally, withwasdefinedinEq.(3.3).
Whendesigninganyproposaldistribution,notethattheop-
Proposition 3.2. Let Y(G) = Y (PaG) be summing
i i i timalchoicewouldbetheactualtargetdistribution.Inour
and w(G) = w(PaG) be factorising over the parent
i i P case,thiswouldcoincidewiththetrueposterioroverDAGs.
sets,thenwecancompute
Q Hence,weshouldaskourselvesthequestion,whatkindof
structuralpropertiesthetrueDAGposteriormayexhibit,in
E [w(G)Y(G)]= (3.10)
G|L orderto designa proposaldistributionthatcan,atleast in
principle,reasonablyapproximateit.Clearly,thesamecon-

α j(L) · p(pa i|L)w i(pa i)Y i(Pa i), siderationsapplytothedesignofavariationaldistribution
Xi Yj6=i XPai usedinthecontextofvariationalinference.
 
where Weargue,thattheconvenientandcommonlymadechoice
[Zheng et al., 2018, Brouillard et al., 2020, Cundy et al.,
α (L)= p(pa ,L)w (pa ). 2021,Charpentieretal.,2022]ofmodelingthegraphpos-
j i i i
terior as a set of independentBernoulliedge distributions
XPai
may fail even in simple cases. Consider the canonical ex-
ample where the causal chain graph G : X → Y → Z
These propositions generalises the results presented by 1
is not identifiable from observational data. The fork G :
KollerandFriedman[2003],KoivistoandSood[2004]on 2
X ←Y →ZandthereversechainG :X ←Y ←Z are
howtocomputetheposteriorprobabilitiesofedgesorpar- 3
inthesameMarkovequivalenceclass(MEC).Inthelimit
ent sets, which is a special case of Prop. 3.1, to our set-
of infinite data, the graph posterior will collapse onto the
ting of Bayesian causalinferencewhere w will be the im-
MEC, all elements having the same posterior probability.
portanceweightsasin Eq.(3.3).Inpractice,weassumea
However,fortheindependentBernoulliparametrisation,it
uniformpriorp(Pa |L)overparentsetsconsistentwitha
i
is impossible to represent the true DAG posterior. By as-
givencausalorder.
signingeachedgepresentintheMECapositiveprobability,
weidentifytwopathologies.First,themodelfailstorepres-
AnExpressiveDistributionoverParentSets. Asthere-
entadistributionoverdisjointparentsets,e.g.,thetruepar-
striction of the maximumparentset sizes may violate the
entsetsofY maybe{X}or{Z},butnot{X,Z}.Second,
reality of the true underlying causal graph to be inferred,
theparentsetsoftheindividualnodesarecoupledinsofar,
weproposetolearnatractableandexpressiveproposaldis-
astheymustjointlyentailanacyclicgraph,whichisclearly
tribution p(Pa|L,φ) over parent sets which is amenable
violated,asthejointprobabilityoftheedgesX → Y and
to gradient-based optimisation. Importantly, p(Pa|L,φ)
Y → X is non-zero as soon as both independently have
modelsadistributionoverparentsetsthatareunrestricted
non-zeroprobability.
in cardinality, and possibly form a cyclic graph. Combin-
ing the causal order L with such parent sets, we can de- This motivates that any distribution over parent sets resp.
terministicallycomputetheadjacencymatrixofanacyclic DAGsshould(A)be ableto representa rich,multi-modal
graphG=ML·Pabyelement-wisemultiplicationofthe distributionoverparentsetsforeachnode,and(B)thejoint
adjacencymask ML inducedby the causal order,and the distribution over parent sets of all nodes should be able
adjacencymatrixPainducedbytheparentsets.Usingthe to concentrate its support on acyclic graphs. Considering
4these properties, we propose an expressive, yet tractable Algorithm2:SampleCausalOrder
distributionoverDAGsinthefollowing.
Input:Logitfunctiong :Rd×d 7→(Rd,Rd×M×d)
θ
Toprovideproperty(A),weproposetorepresentthedistri- Output:CausalorderLsampledfromp(L|θ)
butionoverparentsetsforeachnodewithamixtureofMul- U ←X ⊲unassigned elements
tinoulli distributions parameterised by φ ∈ Rd×K×d, L←∅ ⊲causal order
MN
i.e., for each node i the k-th component of the mixture k ←1 ⊲current layer index
has corresponding logits φ [i,k] and is a collection of repeat
MN
d independent Bernoulli probabilities, indicating parent- θ ←g(QL<k) ⊲compute logits
CAT
hood of all nodes j w.r.t. the sink node i. To provide L ∼CAT(L |θ ,U) ⊲sample next
k k CAT
the coupling between parent sets as required by property element
(B), we select the k-th Multinoullimixture componentby L←L∪hL i ⊲update causal order
k
sampling from a mixture of categorical distributions with U ←U \L ⊲update unassigned
k
logitsφ ∈RM×d×K,i.e.,them-thcomponentofthis k ←k+1
MOM
mixtureofmixturescontainsforeachsinknodeiasetofK untilU 6=∅
mixtureweights.Weselectthecomponentofthistop-level
mixturebysamplingfromacategoricaldistributionaccord-
ing to mixture weights φ TLM ∈ RM. Finally, to model Inthefollowing,weproposeanexpressive,auto-regressive
thedependenceoftheparentsetsonthecausalorderLin proposaldistribution
p(Pa|L,φ), we computethe logits{φ ,φ ,φ }
MN MOM TLM
using a differentiable function h (·) (e.g., as in our case d
θ
a simple feed-forwardneuralnetwork)takingas inputthe p(L|θ)=p(L 1|θ)· p(L k|L <k,θ) (3.12)
permutationmatrixQLinducedbythecausalorderL.The k=2
Y
expressiveness of the resulting distribution is determined
forcausalorders,thatisamenabletogradient-basedoptim-
bytherichnessofh(·)andthenumberofmixturecompon-
isationandavoidstheshortcomingdescribedabove.Theor-
ents M and K, We summarise the sampling procedurein
deringofnodesnaturallyimpliesasequentialsamplingpro-
Alg.1.
cedureaslistedinAlg.2.Ineachstepofthesamplingpro-
cedure,wesamplefromp(L |L ,θ)anelementfromthe
k <k
set of yet unassigned elements, conditional on all preced-
ingelementsfromacategoricaldistributionp(L |L ,θ).
k <k
Toaccountforthedependenceonprecedingelements,we
compute the logits of the categorical distribution using a
3.1.3 CausalOrderInference differentiable function g (·) (e.g., as in our case a simple
θ
feed-forwardneuralnetwork)takingasinputasuitableen-
coding of the so-far sampled order. We chose to encode
Similarlyto§ 3.1.2,theexpectationw.r.t.causalordersin
these (sub-)ordersusing their induced permutation matrix
Eqs.(3.2)and(3.4)posesahardcombinatorial,asthenum-
QL<k wherewemasktherowscorrespondingtoelements
berofpermutationsofdelementsisd!.Thus,weaimtoper-
L withzeros.Toevaluatep(L |L ,θ),wecompute
formMonteCarloestimationusingaproposaldistribution >=k k <k
thelogitsgiventhemaskedpermutationmatrixQL<k and
for causal orders. Following the considerationsabout pro-
re-normalisethemaccordingtotheelementsin[L ]toget
posaldesignregardingtheparentsetproposal,weillustrate ≥k
the probabilityof elementL having been sampled at the
potentialpitfallsregardingdistributionsovercausalorders. k
k-thpositionintheorder.
ConsideranyMECincludingachaingraph.Consequently,
thereversechaingraphmustalsobeintheMEC,andthus, We again use the score-functionestimator to estimate the
thattheproposalovercausalordersmustbeabletorepres- gradientinEq.(3.4)w.r.t.θas
entbothchainswithhighprobability.
∇ logp(θ,φ,ψ|D)= (3.13)
θ
Notethatasimpleparametrisationasin[Charpentieretal.,
∇ logp(θ)+E E [w]·∇ p(L|θ) .
2022] is not be able to represent the true posterior over θ L|θ Pa|L,φ θ
causalordersinthiscase.Specifically,theysampleorders
withwasdefinedinEq.(3.3(cid:2)). (cid:3)
usingtheGumbelTop-ktrick[Kooletal.,2019]usingonly
d logits to represent a distribution of permutationsover d
elements.Now,tosampleachaingraphandareversechain 3.1.4 Bringingitalltogether:
graph, one element is the rootnode in one case and must Rao-BlackwellisationviaSequentialInference
thus have the highest (perturbed) logit, and the leaf node
intheothercasewhereitmusthavethelowest(perturbed) RecallthatweoptimiseourmodelbyinferringMAPestim-
logit,whichiscontradictory. ates of the parameter posteriorby performinggradientas-
5centonlogp(θ,φ,ψ|D).Thestandardapproachofsimul- Besidessamplingbasedinference,introducingorderstruc-
taneouslyupdatingallparametersinasinglegradientstep ture also leads to exact optimization schemes [Cussens,
ispronetoyieldverynoisygradients: 2010, De Campos and Ji, 2011, Peharz and Pernkopf,
2012].
1. Thegradientw.r.t.φ(parentsetmodel)dependsonψ
After the advent of differentiable DAG structure learning
through the quality of the estimated the importance
methods [Zheng et al., 2018, Yu et al., 2019, Brouillard
weights w in Eq. (3.11). Therefore, a bad estimate
et al., 2020, Lachapelle et al., 2020, Lorch et al., 2021],
of ψ will result in poor estimates of the importance
inferenceviaordersrecentlyresurfacedinthecausalstruc-
weights.
ture learning community, mainly as a vehicle to sample
2. Thegradientw.r.t.θ(causalordermodel)additionally DAGs without the need of utilising soft acyclicity con-
depends on φ via the quality of the sampled parent straints duringoptimisation [Cundyet al., 2021,Charpen-
setsinEq.(3.13).Therefore,abadestimateofφwill
tier et al., 2022, Annadani et al., 2023]. In contrast to
leadtoapoorproposalofparentsetsthatarenotrep- these works,althoughwe use gradient-basedoptimisation
resentativeoftherespectivecausalorder. tolearnadistributionovercausalorders,wemainlyutilise
causalorderstoRao-Blackwelliseourinferenceprocedure
To mitigate these issues, we propose a sequential,
byexhaustiveparentsetenumeration.
coordinate-descent style optimisation procedure.
Whenever we sample a yet unseen parent set, we first
optimise the associated GP hyper-parameters via MAP- 5 EXPERIMENTS
Type II (see § 3.1.1), ensuring high-quality importance
weightestimates.
5.1 SETUP
Whenexhaustivelyenumeratingtheparentsetsispossible,
wegetalow-variancegradientestimateforthecausalorder We briefly summarise our experimental setup in the fol-
model in Eq. (3.13). Alternatively, when using the parent lowing. Additional results and ablations are provided in
setproposaltoapproximatelyinferparentsets,wepropose Appx. C. We provide our code as a supplementarymater-
a two-stage approach. First, we approximate the expecta- ial.
tion E [w] in Eq. (3.13)with a single set of parents
Pa|L,φ
sets, specifically, with the set of maximalparent sets con- Data. We generate ground truth SCMs with d = 20
sistentwiththecausalorder(i.e.,theresultingDAGwould endogenousvariableswith graph structuressampled from
be the adjacency mask induced by L) and optimise the the commonly used Erdös-Rényi (ER) [Erdös and Rényi,
causalordermodelindependentlyofthe parentset model. 1959]andscale-free(SF)[BarabásiandAlbert,1999]mod-
Second,weleaveθfixedandoptimisetheparentsetmodel els (cf. [Lorch et al., 2021, Charpentier et al., 2022, An-
viaEq.(3.11).Weshowtheeffectivenessofthissequential nadani et al., 2023, Toth et al., 2022]). We evaluate our
optimisationprocedureinanablationstudyinAppx.C. method both on linear and on non-linear additive noise
models. For each ground truth SCM, we sample a fixed
setof100training,and200testsamplesfromtheobserva-
4 RELATED WORK
tionaldistribution.Thisemulatesthe setting ofsignificant
uncertaintyrelevanttotheBayesianinferencescenario.Re-
The Bayesian causal inference problem that we focus on
isachetal.[2021]argue,thatthecausalordermaystrongly
inthispaperwasrecentlymotivatedby[Tothetal.,2022],
correlate with increasing marginal variance in simulated
but dates back in the simpler form of Bayesian structure
data, and therefore, benchmarks may be easy to game by
inference[Heckerman,1995,Heckermanetal.,1997].
exploitingthisproperty.Asthismaybeespeciallyrelevant
Utilising(causal)ordersisanwellestablishedapproachin to order-based inference methods, we follow their recom-
the MCMC-based Bayesian structurelearningcommunity. mendation and normalise the training data for each endo-
The combinatorial space of orders is significantly smal- genousvariabletozeromeanandunitvariance.
ler than the combinatorial space of DAGs, which serves
MCMC methods to better explore the posterior space of Metrics. Asiscustom,wereporttheexpectedstructural
DAGs [Koller and Friedman, 2003, Koivisto and Sood, Hammingdistance(ESHD), aswell asthe area underthe
2004,TeyssierandKoller,2012,Niinimäkietal.,2016,Vi- receiver operating characteristic (AUROC) and the area
inikka et al., 2020]. Moreover, the restriction of the max- under the precision recall curve (AUPRC) w.r.t. posterior
imum size of any parent set in the DAG allows for the edgeprediction.FollowingAnnadanietal.[2023],wealso
exhaustive enumeration of all parent sets in polynomial reporttheF1 score,and,to provideadditionalinsightinto
time.However,MCMCinferencecomeswithitsownsetof the methodsstrengthsand weaknesses, we also reportthe
challenges,andnoneoftheseworkscanhandlenon-linear true positive rate (TPR) and true negativerate (TNR) for
mechanismmodels. theedgepredictiontask.Finally,toevaluatethequalityof
6theinferredcausalmechanisms,wereporttheexpectedrel- larisingmechanismasusedbyDDSandBayesDAGwould
ativeroot-mean-squareerror(RRMSE)onthetestset.We improveperformance.
donotreportthelog-likelihoodonthetestdataasisusually
Among the baselines, BayesDAG performs best on most
done, because the evaluated methods implement different
metrics in the majority of cases and represents a strong
noise models and approximate inference schemes, which
baselinemethod.DDSsuffersfromahighESHDandpre-
leads to the (marginal) log-likelihoodsbeing uncalibrated
dictssignificantlymoreedgesinexpectationthanallother
and thus incomparable (cf. [Murphy,2023][Sec. 7.5] and
methods.This is reflectedby a highTPR and a low TNR,
referencestherein).
i.e., DDS predicts very dense graphs. Unfortunately, we
could not find suitable hyper-parametersto regularise the
Baselines. We compare our two modelvariants, i.e., ex- sparsity, and similar behaviourwas reportedby Annadani
haustive enumeration of parent sets with maximum size et al. [2023][App. G, Fig. 4]. We hypothesise that (i) the
K =2(COM-EX-GP)vs.approximategradient-basedpar- posterior over causal orders fails to capture the true pos-
entsetinference(COM-PSM-GP),tothreerecentstructure terior for reasons explained in § 3.1.3, and (ii) the miss-
learning methods. First, we compare against DIBS, pro- ingcouplingbetweencausalordersandedgeprobabilities.
posed by Lorch et al. [2021], which implements differen- DIBS obtains higher TNR than TPR and a rather low ex-
tiable Bayesian graphlearningusinga softacyclicitycon- pectednumberofedgesincomparisonwiththeothermeth-
straint in line with Zheng et al. [2018], Yu et al. [2019]. ods.We conjecturethat,althoughwe didnotuse anexpli-
Second, we compare against DDS, proposed by Charpen- cit sparsity regulariser,the optimisationinvolvingthe soft
tieretal.[2022],whichbuildsuponthepermutation-based acyclicityconstraintactsasanimplicitsparsityregulariser,
approach of Cundy et al. [2021] and utilises differenti- as sparse graphs are more likely to have a lower cyclicity
able permutation sampling and variational inference (VI) scorethandenserones.
toinferaposterioroverDAGs.Third,wecompareagainst
BayesDAG,proposedbyAnnadanietal.[2023],whichutil-
ises a mixtureof MCMC to infer permutationsand mech- 6 DISCUSSION
anismparameters,andVItoinferDAGedgesgiventheper-
mutations.DDSandBayesDAGbothutilisecausalorders Instead of attempting to directly infer a posterior over
forstructurelearning. SCMs, we shift the Bayesian causal inference problem
to inferring high-quality proposal densities amenable to
gradient-based optimisation, which we use to compute
Monte-Carloestimatesviaimportancesampling.Westress
5.2 RESULTS
theimportanceofRao-Blackwellisingtheinferenceproced-
ure and show its effectiveness on simulated benchmarks.
OurresultsinTab.1 show,thatourCOM-EX-GPmethod Specifically,forthecaseof(non-)linearadditivenoisemod-
consistentlyandsubstantiallyoutperformsthebaselineson els,weimproveuponasetofstate-of-the-artbaselinemeth-
allmetricsinallscenarios,exceptforTPR(seediscussion ods. Some limitations and potential improvementsare de-
below).Thisistobeexpectedonscale-freegraphs,where scribedinthefollowing.
the restriction to parent sets of maximum size K = 2
conformswiththestructuralpropertiesoftheground-truth
ModellingMechanisms. Themaindriverofcomplexity
causal graphs. Interestingly, COM-EX-GP also delivers istheexactinferenceusingGPs,whichgrowswithN3 in
strongresultsonSCMswithErdös-Rényigraphstructures,
thenumberofavailabledatapoints.Althoughweusedonly
althoughtheassumptionontheparentsetsizesisclearlyvi-
CPUsforrunningourexperiments,scalableGPUinference
olated.GiventheconsistentlyhighTNRdemonstratesthat
techniquesforGPswereproposedbyGardneretal.[2018],
ourmodelsinferverywellwhichedgesarenotpresentand
Pleiss et al. [2018]. Conceptually, our framework is flex-
givehighscorestosub-graphsofthetruegraph.
ible and modular, allowing to use alternative mechanism
Our second model variant, COM-PSM-GP, achieves com- modelslikenormalisingflowsasinBrouillardetal.[2020],
petitive scores especially in the linear scenario. We con- Pawlowskietal.[2020].Lastly,givenoursequentialinfer-
jecture that, as in the linear case the true causal graph is ence procedure, the training of the mechanisms could be
not identifiable, using an expressive proposal distribution parallelised.
overparentsets (see § 3.1.2)isparamount.In contrast,in
the non-linear case the true graph is identifiable and thus, ParentSetInference. Aseconddriverofcomputational
anexpressiveparentsetproposalmaynotbenecessaryor complexity is the exhaustive enumeration of parent sets.
evendetrimental,asitmaybehardertooptimise.Notably, The number of parent sets with bounded size K grows
exceptfor the Erdös-Rényilinear case, this modelvariant polynomially with O(dK), which may be prohibitive on
hasaratherhighESHDjoinedbyahighexpectednumber largerprobleminstances.Note,however,thatthenecessary
ofedges.Mostlikely,usinganexplicit,tunedsparsityregu- weightscanbepre-computedinparallel.Aninterestingex-
7Table1:Experimentalresultsonsimulated(non-)lineargroundtruthmodelswith20nodesanddifferentDAGstructures.
We reportmeansand95%confidenceintervals(CIs)across20differentgroundtruthmodels.Bestvaluesarebold-faced.
As thetruegraphis notidentifiablein thelinearscenario,thereportedstructurelearningmetricsarecomputedw.r.t.the
skeletonofthetrueDAG.
Model #Edges ↓ESHD ↑AUROC ↑AUPRC ↑TPR ↑TNR ↑F1 ↓RRMSE
DDS 157±7 127±5 0.80±0.03 0.57±0.05 0.90±0.03 0.19±0.04 0.35±0.02 1.04±0.03
DIBS 21±4 39±4 0.69±0.02 0.45±0.03 0.27±0.05 0.93±0.02 0.34±0.05 1.44±0.05
BayesDAG 38±2 39±3 0.74±0.03 0.56±0.05 0.49±0.03 0.87±0.01 0.49±0.03 0.91±0.03
COM-EX-GP(ours) 26±1 22±2 0.82±0.03 0.73±0.03 0.55±0.03 0.97±0.01 0.66±0.03 0.21±0.00
COM-PSM-GP(ours) 89±6 72±4 0.66±0.02 0.54±0.02 0.72±0.04 0.60±0.03 0.43±0.02 1.07±0.06
(a)Erdös-RényiNonlinear.
Model #Edges ↓ESHD ↑AUROC ↑AUPRC ↑TPR ↑TNR ↑F1 ↓RRMSE
DDS 168±5 137±4 0.79±0.03 0.54±0.05 0.93±0.02 0.13±0.03 0.33±0.00 1.01±0.04
DIBS 35±5 42±4 0.71±0.02 0.46±0.04 0.40±0.06 0.87±0.02 0.40±0.05 1.42±0.06
BayesDAG 65±2 48±2 0.82±0.03 0.61±0.04 0.73±0.03 0.75±0.01 0.52±0.02 0.88±0.03
COM-EX-GP(ours) 32±1 13±2 0.93±0.02 0.87±0.04 0.77±0.03 0.97±0.01 0.81±0.03 0.20±0.00
COM-PSM-GP(ours) 88±4 68±4 0.69±0.02 0.55±0.02 0.77±0.03 0.61±0.03 0.45±0.02 1.04±0.04
(b)Scale-freeNonlinear.
Model #Edges ↓ESHD ↑AUROC ↑AUPRC ↑TPR ↑TNR ↑F1 ↓RRMSE
DDS 156±5 126±3 0.68±0.02 0.38±0.03 0.88±0.03 0.19±0.03 0.35±0.02 0.60±0.04
DIBS 39±6 56±6 0.58±0.03 0.37±0.03 0.29±0.03 0.82±0.04 0.29±0.03 0.50±0.07
BayesDAG 38±3 48±5 0.65±0.04 0.41±0.03 0.38±0.04 0.85±0.02 0.38±0.03 0.30±0.03
COM-EX-GP(ours) 31±2 40±4 0.68±0.03 0.44±0.04 0.40±0.03 0.90±0.01 0.44±0.03 0.17±0.00
COM-PSM-GP(ours) 31±2 40±4 0.69±0.03 0.45±0.03 0.38±0.04 0.89±0.01 0.43±0.04 0.17±0.00
(c)Erdös-RényiLinear.
Model #Edges ↓ESHD ↑AUROC ↑AUPRC ↑TPR ↑TNR ↑F1 ↓RRMSE
DDS 163±2 136±1 0.59±0.04 0.29±0.04 0.88±0.01 0.15±0.01 0.32±0.00 0.35±0.04
DIBS 42±4 60±4 0.53±0.03 0.29±0.03 0.25±0.04 0.79±0.02 0.23±0.03 0.22±0.05
BayesDAG 36±3 55±2 0.54±0.03 0.25±0.03 0.23±0.04 0.82±0.01 0.23±0.04 0.20±0.02
COM-EX-GP(ours) 34±1 47±3 0.61±0.03 0.33±0.04 0.32±0.04 0.85±0.01 0.33±0.04 0.16±0.00
COM-PSM-GP(ours) 86±8 87±6 0.52±0.02 0.40±0.02 0.49±0.04 0.56±0.04 0.29±0.01 0.21±0.05
(d)Scale-freeLinear.
tensionofourworkwouldbetousee.g.K=2asapproxima- ImpactStatement. Thispaperpresentsworkwhosegoal
tiontolearnagoodproposaldistributionovercausalorders, is to advance the field of Machine Learning. There are
andthentosubsequentlyinferunrestrictedparentsetswith many potential societal consequences of our work, none
ourproposedparentsetmodel. whichwefeelmustbespecificallyhighlightedhere.
Acknowledgements
Alternative Inference Techniques. We employ the
score-functionestimatortocomputethegradientestimates The computational results presented have in part been
forthecausalorderandparentsetmodels.Althoughthises- achievedusingtheViennaScientificCluster(VSC).
timatorisknowntosufferfromhighvariance,itisunbiased
and we achieve good results using a simple exponential
movingaveragebaseline.Furthermore,itwouldbeinterest- References
ingtoseehowavariationalinferenceapproachwouldfare
whenincorporatingourexpressiveproposaldistributionsas Yashas Annadani, Nick Pawlowski, Joel Jennings, Stefan
variationaldistributions. Bauer, Cheng Zhang,and Wenbo Gong. Bayes{DAG}:
8Gradient-Based Posterior Inference for Causal Discov- D.KollerandN.Friedman. BeingBayesianaboutnetwork
ery.InThirty-seventhConferenceonNeuralInformation structure.ABayesianapproachtostructurediscoveryin
ProcessingSystems,2023. Bayesiannetworks. MachineLearning,2003.
Albert-László Barabási and Réka Albert. Emergence of Wouter Kool, Herke Van Hoof, and Max Welling.
scalinginrandomnetworks. Science,286,1999. Stochastic Beams and Where To Find Them: The
{G}umbel-Top-k Trick for Sampling Sequences
Philippe Brouillard, Sébastien Lachapelle, Alexandre
Without Replacement. In Kamalika Chaudhuri
Lacoste, Simon Lacoste-Julien, and Alexandre Drouin. and Ruslan Salakhutdinov, editors, Proceedings
Differentiable Causal Discovery from Interventional of the 36th International Conference on Machine
Data. InHLarochelle,MRanzato,RHadsell,MFBal- Learning, volume 97. PMLR, 2019. URL ht-
can,andHLin,editors,AdvancesinNeuralInformation
tps://proceedings.mlr.press/v97/kool19a.html.
ProcessingSystems.CurranAssociates,Inc.,2020.
Sébastien Lachapelle, Philippe Brouillard, Tristan Deleu,
Bertrand Charpentier, Simon Kibler, and Stephan Gün-
andSimonLacoste-Julien. Gradient-BasedNeuralDAG
nemann. Differentiable {DAG} Sampling. In Interna-
Learning. InInternationalConferenceonLearningRep-
tionalConferenceonLearningRepresentations,2022.
resentations,2020.
Chris Cundy, Aditya Grover, and Stefano Ermon. BCD
LarsLorch,JonasRothfuss, BernhardSchölkopf,andAn-
Nets: Scalable Variational Approaches for Bayesian
dreas Krause. DiBS: DifferentiableBayesian Structure
CausalDiscovery. InThirty-FifthConferenceonNeural
Learning. Advances in Neural Information Processing
InformationProcessingSystems,2021.
Systems,2021.
JamesCussens. Maximumlikelihoodpedigreereconstruc-
KevinP.Murphy. ProbabilisticMachineLearning:Anin-
tionusingintegerprogramming.InWCB@ICLP,pages
troduction. MITPress,2021.
8–19,2010.
Kevin P. Murphy. Probabilistic Machine Learning: Ad-
CassioPDeCamposandQiangJi.Efficientstructurelearn-
vancedTopics. MITPress,2023.
ingofbayesiannetworksusingconstraints. TheJournal
ofMachineLearningResearch,12:663–689,2011.
Teppo Niinimäki, Pekka Parviainen, and Mikko Koivisto.
StructureDiscoveryinBayesianNetworksbySampling
P.ErdösandA.Rényi. Onrandomgraphsi. Publicationes
Partial Orders. JournalofMachineLearningResearch,
MathematicaeDebrecen,6:290,1959.
2016.
Jacob Gardner, Geoff Pleiss, Kilian Q Weinberger, David
OEIS Foundation Inc. Number of acyclic digraphs (or
Bindel, and Andrew G Wilson. Gpytorch: Blackbox
dags) with n labeled nodes, 2024. Entry A003024 in
matrix-matrix gaussian process inference with gpu ac-
The On-Line Encyclopedia of Integer Sequences, ht-
celeration. In S. Bengio, H. Wallach, H. Larochelle,
tps://oeis.org/A003024.
K. Grauman, N. Cesa-Bianchi, and R. Garnett, edit-
ors,AdvancesinNeuralInformationProcessingSystems.
NickPawlowski,DanielCCastro,andBenGlocker. Deep
CurranAssociates,Inc.,2018.
Structural Causal Models for Tractable Counterfactual
Paula Gradu, Tijana Zrnic, Yixin Wang, and Michael I. Inference. In Advances in Neural Information Pro-
Jordan. Valid Inference after Causal Discovery. cessingSystems,2020.
arXiv:2208.05949,2022.
JudeaPearl. Causality. CambridgeUniversityPress,2009.
DavidHeckerman. Abayesianapproachtolearningcausal ISBN9780511803161.
networks. In Proceedings of Eleventh Conference on
UncertaintyinArtificialIntelligence.MorganKaufmann, RobertPeharzand FranzPernkopf. Exactmaximummar-
1995. gin structure learning of bayesian networks. arXiv pre-
printarXiv:1206.6431,2012.
DavidHeckerman,ChristopherMeek,andGregoryCooper.
A Bayesian Approach to Causal Discovery. Computa- Geoff Pleiss, Jacob Gardner, Kilian Weinberger, and An-
tion,Causation,andDiscovery,1997. drewGordonWilson. Constant-timepredictivedistribu-
tions for Gaussian processes. In Jennifer Dy and An-
Mikko Koivisto and Kismat Sood. Exact Bayesian struc- dreas Krause, editors, Proceedings of the 35th Interna-
turediscoveryinBayesiannetworks.JournalofMachine tionalConferenceonMachineLearning,Proceedingsof
LearningResearch,2004. MachineLearningResearch.PMLR,2018.
9Alexander G. Reisach, Christof Seiler, and Sebastian
Weichwald. BewareoftheSimulatedDAG!CausalDis-
coveryBenchmarksMayBeEasyToGame. InM.Ran-
zato Vaughan,A. Beygelzimer, Y. Dauphin, P.S. Liang,
and J. Wortman, editors, Advances in Neural Informa-
tionProcessingSystems.CurranAssociates,Inc.,2021.
Marc Teyssier and Daphne Koller. Ordering-Based
Search: A Simple and Effective Algorithm for Learn-
ingBayesianNetworks. Proceedingsofthe21stConfer-
enceonUncertaintyinArtificialIntelligence,UAI2005,
2012.
Christian Toth, Lars Lorch, Christian Knoll, Andreas
Krause, Franz Pernkopf,Robert Peharz, and Julius von
Kügelgen. Active Bayesian Causal Inference. In
SKoyejo,SMohamed,AAgarwal,DBelgrave,KCho,
andAOh,editors,AdvancesinNeuralInformationPro-
cessingSystems.CurranAssociates,Inc.,2022.
Jussi Viinikka, Antti Hyttinen, Johan Pensar, and Mikko
Koivisto.TowardsScalableBayesianLearningofCausal
DAGs.InHLarochelle,MRanzato,RHadsell,MFBal-
can,andHLin,editors,AdvancesinNeuralInformation
ProcessingSystems.CurranAssociates,Inc.,2020.
Yue Yu, Jie Chen, Tian Gao, and Mo Yu. DAG-GNN:
DAG Structure Learning with Graph Neural Networks.
In Kamalika Chaudhuri and Ruslan Salakhutdinov, ed-
itors, Proceedingsofthe 36th InternationalConference
onMachineLearning,ProceedingsofMachineLearning
Research.PMLR,2019.
Xun Zheng, Bryon Aragam, Pradeep K Ravikumar, and
Eric P Xing. DAGs with NO TEARS: ContinuousOp-
timization for StructureLearning. In S Bengio,H Wal-
lach, H Larochelle, K Grauman, N Cesa-Bianchi, and
RGarnett,editors,AdvancesinNeuralInformationPro-
cessingSystems31.CurranAssociates,Inc.,2018.
10Rao-Blackwellising Bayesian Causal Inference
(Supplementary Material)
ChristianToth1 ChristianKnoll1,2 FranzPernkopf1 RobertPeharz1
1GrazUniversityofTechnology
2LevataGmbH
Submittedtothe40thConferenceonUncertaintyinArtificialIntelligence (UAI2024).A PROOFSAND DERIVATIONS
A.1 DERIVATIONOFTHEPOSTERIOREXPECTATIONW.R.T.SCMS
Inthefollowing,wederivetheexpectationw.r.t.SCMsinEq.(3.2)andthecorrespondingimportanceweightsinEq.(3.3).
E M|D[Y(M)]=E G,f,ψ|D[Y(M)]
=E
θ,φ,ψ|D
E G,f|θ,φ,ψ,D[Y(M)]
=E θ,φ,ψ|D(cid:2)E
G|θ,φ,ψ,D
E f|ψ,D[(cid:3)Y(M)]
p(ψ,D|G)
=E θ,φ,ψ|D(cid:2)E
G|θ,φ
p(ψ(cid:2) ,D|θ,φ)E f|ψ,(cid:3) D(cid:3) [Y(M)]
(cid:20) (cid:20) (cid:21)(cid:21)
=E
θ,φ,ψ|D
E
L|θ
E
Pa|L,φ
w·E f|ψ,D[Y(M)]
with (cid:2) (cid:2) (cid:2) (cid:3)(cid:3)(cid:3)
p(ψ,D|G)
w:=
p(ψ,D|θ,φ)
p(ψ,D|G)
=
E L′|θ E Pa′|L′,φ[p(ψ,D|G)]
p(D|ψ,G)·p(ψ|G)
= (cid:2) (cid:3) .
E L′|θ E Pa′|L′,φ[p(D|ψ,G)·p(ψ|G)]
(cid:2) (cid:3)
A.2 DERIVATIONOFTHEGRADIENTESTIMATORS
In the following,we derivethe gradientestimatorsin Eqs. (3.4),(3.11)and (3.13).We denoteby ∇ = ∇ to avoid
θ,φ,ψ
clutter.
Thegeneralposteriorgradientreadsasfollows.
p(D|θ,φ,ψ)·p(θ,φ,ψ)
∇logp(θ,φ,ψ|D)=∇log
p(D)
=∇logp(θ,φ,ψ)+∇logp(D|θ,φ,ψ)
=∇logp(θ,φ)+∇logp(D,ψ|θ,φ)
=∇logp(θ,φ)+∇logE [p(D,ψ|L,φ)]
L|θ
=∇logp(θ,φ)+∇logE E p(D,ψ|G=Pa⊙ML)
L|θ Pa|L,φ
(cid:2) (cid:2) (cid:3)(cid:3)
Usingthisasasstartingpointforthepartialgradients,weget
∇ logp(θ,φ,φ|D)=∇ logp(θ,φ)+∇ logE E p(D,ψ|G=Pa⊙ML)
φ φ φ L|θ Pa|L,φ
=∇ φlogp(φ)+∇ φlogE L|θ E (cid:2)Pa|L,φ p((cid:2)D,ψ|G=Pa⊙ML) (cid:3)(cid:3)
=∇ logp(φ)+E
∇(cid:2)φE Pa|L,φ(cid:2)p(D,ψ|G=Pa⊙ML)(cid:3)(cid:3)
φ L|θ "E
L′|θ
E
Pa′|L′,φ(cid:2)
p(D,ψ|G=Pa′⊙ML (cid:3)′)
#
(cid:2) (cid:2) (cid:3)(cid:3)
andusing∇ p(Pa|L,φ)=p(Pa|L,φ)·∇ logp(Pa|L,φ)
φ φ
E p(D,ψ|G=Pa⊙ML)·∇ logp(G|L,φ)
=∇ logp(φ)+E Pa|L,φ φ
φ L|θ
"
E
L′|(cid:2)θ
E
Pa′|L′,φ
p(D,ψ|G=Pa′⊙ML′)
(cid:3)#
=∇ φlogp(φ)+E L|θ[w·∇ φlogp(cid:2)(G|L,φ)](cid:2). (cid:3)(cid:3)
ThederivationforthegradientinEq.(3.13)followsbyanalogy.
12A.3 PROOFSREGARDINGEXHAUSTIVEPARENTSETENUMERATION
ProofofProp.3.1
Proof.
E [w(G)·Y(G)]= p(G|L)·w(G)·Y(G)
G|L
G
X
Sinceweassumethatw(G)andY(G)factoriseovertheparentsets,wehave
= p(PaG|L)·w (PaG)·Y (PaG)
i i i i i
G i
XY
Thesumoverallgraphscanberepresentedassumoverallcombinationsofpossibleparentsetstoget
= ··· p(Pa |L)·w (Pa )·Y (Pa )
i i i i i
XPa1 XPa2 XPad Yi
= p(Pa |L)·w (Pa )·Y (Pa ) p(Pa |L)·w (Pa )·Y (Pa )...
1 1 i 1 i 2 2 i 2 i
XPa1 XPa2
Sinceeachsummationoverparentsetsisindependentoftheothers,wegetthefinalresult
= p(Pa |L)·w (Pa )·Y (Pa )
i i i i i
Yi XPai
ProofofProp.3.2
Proof.
E [w(G)·Y(G)]
G|L
= p(G|L)·w(G)·Y(G)
G
X
Sinceweassumethatw(G)factorisesandY(G)sumsovertheparentsets,wehave
= p(PaG|L)·w (PaG)· Y (PaG)
i i i j j
G i j
XY X
Thesumoverallgraphscanberepresentedassumoverallcombinationsofpossibleparentsetstoget
= ··· p(Pa |L)·w (Pa )· Y (Pa )
i i i j j
XPa1 XPa2 XPad Yi Xj
= ··· · Y (Pa ) p(Pa |L)·w (Pa )
j j i i i
XPa1 XPa2 XPad Xj Yi
d
= ··· Y (Pa ) p(Pa |L)·w (Pa )+ ··· ...
1 1 i i i
XPa1 XPa2 XPad Yi XPa1 XPa2 XPad Xj=2
d
= Y (Pa )·p(Pa |L)·w (Pa ) p(Pa |L)·w (Pa )···+ ··· ...
1 1 1 1 1 2 2 2
XPa1 XPa2 XPa1 XPa2 XPad Xj=2
13Byabbreviatingα (L)= ·p(pa ,L)w (pa )weget
j Pai i i i
P
d
= Y (Pa )·p(Pa |L)·w (Pa )· α (L)+ ··· Y (Pa ) p(Pa |L)·w (Pa )
1 1 1 1 1 j j j i i i
XPa1 j Y6=1 XPa1 XPa2 XPad Xj=2 Yi
Byrepeatingthisprocedurefortheremainingsummandsj,wegetthefinalresult
= α (L) · p(pa |L)·w (pa )·Y (Pa )
 j  i i i i i
Xi Yj6=i XPai
 
14B EXPERIMENTAL SETUP
B.1 SIMULATEDDATA.
We follow the setup of Lorch et al. [2021], Toth et al. [2022] and generate scale-free and Erdös-Rényi graphs with an
expecteddegreeof2.
B.2 EVALUATIONMETRICS
Ourevaluationmetricsforedgeprediction,i.e.,AUROC,AUPRC,TPR,TNRandF1arestandardclassificationmetricsas
describede.g.byMurphy[2021].WecomputedtheRRMSEonheld-outobservationaltestdata
DTEST ={xn i. ∼i.d. p(X|M∗)}NT
n=1
sampledfromthetrueSCMM∗as
1 ||xˆ −x ||
RRMSE=E i i 2 (B.1)
M|D d ||x ||
" i 2 #
i
X
where
x =(xn,xn,...,xn)T
i 1 2 3
denotesthevectoroftestsamplesfornodei,and
xˆ =(xˆn,xˆn,...,xˆn)T
i 1 2 3
denotesthevectorofmodelpredictionsxˆn =f (paG)fornodeigivenaposteriorSCMM=(G,f,ψ).
1 i i
B.3 SIMULATIONPARAMETERS
If not stated otherwise, we use a feed-forward neural network g with a single hidden layer of 30 neurons and ReLu
θ
activation functionfor our autoregressivecausal order model. For our parentset model, we use M = K = 10 mixture
componentsandafeed-forwardneuralnetworkh withasinglehiddenlayerof30neuronsandReLuactivationfunction.
φ
We train GPs in batches of 20 for a maximum of 100 steps and RMSprop optimiser with learning rate 0.02. For the
estimationofgradientsandscores,wesample100causalordersand10parentsetsconditionaloneachsampledorder.We
train the causal order and parent set model parameterswith the Adam optimiser and learning rate 0.01 for a maximum
of 500 iterations and early stopping when improvement stagnates. For the score function estimators, we use a simple
exponentialmovingaveragewithadecayfactorof0.1.Ourprovidedcodecontainsallparametersofoursimulationsetup
intheincludedconfig.pyfile.
B.4 BASELINES
DDS. We use the implementation provided by Charpentier et al. [2022] at ht-
tps://github.com/sharpenb/Differentiable-DAG-Sampling. We needed to adapt the hyper-parameters in order to get
meaningfulresultsandranourexperimentswiththefollowingconfiguration.
# Architecture parameters
’seed_model ’: 123, # Seed to init model. int
’ma_hidden_dims ’: [32, 32, 32], # Output dimension . int
’ma_architecture ’: ’linear ’ , # Output dimension. int
’ma_fast ’: False , # Output dimension. int
’pd_initial_adj ’: ’Learned ’ , # Output dimension. int
’pd_temperature ’: 1.0, # Output dimension. int
’pd_hard ’: True , # Output dimension. int
’pd_order_type ’: ’topk ’ , # Output dimension . int
15’pd_noise_factor ’: 1.0 , # Hidden dimensions. list of ints
# Training parameters
’max_epochs ’: 500, # Maximum number of epochs for training
’patience ’: 150, # Patience for early stopping. int
’frequency ’: 2, # Frequency for early stopping test . int
’batch_size ’: 16, # Batch size . int
’ma_lr ’: 1e−3, # Learning rate . float
’pd_lr ’: 1e−2, # Learning rate . float
’loss ’: ’ELBO’ , # Loss name. string
’regr ’: 1e−1, # Regularization factor in Bayesian loss . float
’prior_p ’: 1e−6 # Regularization factor in Bayesian loss . float
DIBS. As we build our implementation based on the code provided by Toth et al. [2022] at ht-
tps://github.com/chritoth/active-bayesian-causal-inference, we use their implementation of DIBS. This also makes
resultsmorecomparable,astheirimplementationalsousesGaussianProcessestomodelmechanisms,whichisincontrast
totheoriginalimplementationbyLorchetal.[2021]basedonneuralnetworks.Weusetheirstandardparameterswith10
latentparticlesandconstanthyper-parametersα=β =1.
BayesDAG. We use the implementation provided by Annadani et al. [2023] at ht-
tps://github.com/microsoft/Project-BayesDAG. We needed to adapt the sparsity regularisation hyper-parameter in
ordertogetmeaningfulresultsandranourexperimentswiththefollowingconfiguration.
Forlinearmodels,weuse:
"model_hyperparams": {
"num_chains": 1,
"sinkhorn_n_iter ": 3000,
"scale_noise_p": 0.001,
"scale_noise ": 0.001,
"VI_norm": true ,
"input_perm ": false ,
"lambda_sparse": 10,
"sparse_init ": false
},
"training_hyperparams": {
"learning_rate ": 1e−3,
"batch_size ": 512,
"stardardize_data_mean": false ,
"stardardize_data_std ": false ,
"max_epochs": 500
}
Fornon-linearErdös-Rényimodels,weuse:
"model_hyperparams": {
"num_chains": 1,
"sinkhorn_n_iter ": 3000,
"scale_noise_p": 0.001,
"scale_noise ": 0.001,
"VI_norm": true ,
"input_perm ": false ,
"lambda_sparse": 10,
"sparse_init ": false
},
"training_hyperparams": {
"learning_rate ": 1e−3,
16"batch_size ": 512,
"stardardize_data_mean": false ,
"stardardize_data_std ": false ,
"max_epochs": 500
}
Fornon-linearscale-freemodels,weuse:
"model_hyperparams": {
"num_chains": 1,
"sinkhorn_n_iter ": 3000,
"scale_noise_p": 0.001,
"scale_noise ": 0.001,
"VI_norm": true ,
"input_perm ": false ,
"lambda_sparse": 10,
"sparse_init ": false
},
"training_hyperparams": {
"learning_rate ": 1e−3,
"batch_size ": 512,
"stardardize_data_mean": false ,
"stardardize_data_std ": false ,
"max_epochs": 500
}
As the implementation runs a number of MCMC chains and only evaluates the best chain afterwards, we use only one
MCMCchaintoenableafaircomparison,asmultiplechainswouldcorrespondtomultiplerunsoftheothermethods.
17C EXTENDED EXPERIMENTAL RESULTS
WeprovideadditionalresultsandablationstudiesinTab.2.
Influence of the maximum parent set size. We evaluate the influence of the maximum parent set size in range k ∈
{1,2,3} in models COM-EX-Kk-GP. As expected, k = 1 performsworst, as each node can only have one parent. Not
surprisingly, k = 2 performs best on scale-free graphs, whereas k = 3 performs best on Erdös-Rényi graphs, as the
respectivekbetterfitthestucturalpropertiesofthetruegraphs.
Influenceoftheexpressivenessofthecausalordermodel. Wecompareourauto-regressivecausalordermodel,COM-
EX-GP,withpredictedlogitstoasimplebaselineCOM-SIMPLE-EX-GP,wherewehaveg
θ(QL)
=θ,i.e.,aconstantlogit
vectorθ ∈Rd.Surprisingly,thesimplebaselineperformsonlyslightlyworsethanourproposedexpressivemodel.Inour
experimentsweobservethat,apparently,using100causalorderssamplesfortheMonte-Carloestimatesincludesatleast
1 ‘reasonably good’ order, that is then responsible for the good performance,i.e., the simple models producesa have a
winner-takes-allsituationbecauseitsuffersfromweightdegeneracyintheimportanceweights.Incontrast,theimportance
weightsforourexpressiveauto-regressivecausalordermodelareapproximatelyuniform,whichindicatesthatwelearna
meaningfulproposaldistribution.
Influence of sequential vs. concurrent optimisation. We compareour modelCOM-PSM-GP with our sequentialop-
timisationprocedureasproposedin§3.1.4,forthatwealsoreportresultsinthemainresultsinTab.1,tomodelsCOM-
PSM-CONC-GPwherethecausalorderandparentsetmodelsaretrainedsimultaneously,i.e.,ineachoptimisationepoch,
both models perform one gradient update step. Note, that the concurrentoptimisation variant is more truthfulto the ac-
tual optimisation problem in Eq. (3.4), as the sequential optimisation procedure relies on an approximationto optimise
the causal order model. Apparently, the COM-PSM-CONC-GP yields sparser graphs, trading a better TNR for a worse
TPR.However,qualityoftheinferredmechanismsasreflectedbytheRRMSEissignificantlyworsethaninthesequential
optimisationmodel.
18Table2:Ablationstudiesonsimulated(non-)lineargroundtruthmodelswith20nodesanddifferentDAGstructures.We
reportmeansand95%confidenceintervals(CIs)across20differentgroundtruthmodels.Bestvaluesarebold-faced.Asthe
truegraphisnotidentifiableinthelinearscenario,thereportedstructurelearningmetricsarecomputedw.r.t.theskeleton
ofthetrueDAG.
Model #Edges ↓ESHD ↑AUROC ↑AUPRC ↑TPR ↑TNR ↑F1 ↓RRMSE
COM-EX-K1-GP 15±1 27±2 0.76±0.03 0.62±0.03 0.34±0.02 0.99±0.00 0.49±0.02 0.21±0.00
COM-EX-K2-GP 26±1 22±2 0.82±0.03 0.73±0.03 0.55±0.03 0.97±0.01 0.66±0.03 0.21±0.00
COM-EX-K3-GP 34±2 21±2 0.87±0.02 0.77±0.04 0.66±0.04 0.95±0.01 0.71±0.03 0.24±0.00
COM-EX-GP 26±1 22±2 0.82±0.03 0.73±0.03 0.55±0.03 0.97±0.01 0.66±0.03 0.21±0.00
COM-SIMPLE-EX-GP 25±2 23±2 0.84±0.02 0.73±0.03 0.53±0.03 0.97±0.01 0.64±0.03 0.24±0.01
COM-PSM-GP 89±6 72±4 0.66±0.02 0.54±0.02 0.72±0.04 0.60±0.03 0.43±0.02 1.07±0.06
COM-PSM-CONC-GP 52±9 48±6 0.67±0.02 0.53±0.02 0.54±0.05 0.80±0.05 0.47±0.02 1.48±0.05
(a)Erdös-RényiNonlinear.
Model #Edges ↓ESHD ↑AUROC ↑AUPRC ↑TPR ↑TNR ↑F1 ↓RRMSE
COM-EX-K1-GP 18±1 22±1 0.79±0.02 0.69±0.03 0.44±0.02 0.99±0.00 0.59±0.03 0.20±0.00
COM-EX-K2-GP 32±1 13±2 0.93±0.02 0.87±0.04 0.77±0.03 0.97±0.01 0.81±0.03 0.20±0.00
COM-EX-K3-GP 40±1 19±2 0.91±0.02 0.84±0.04 0.79±0.03 0.93±0.01 0.75±0.03 0.24±0.01
COM-EX-GP 32±1 13±2 0.93±0.02 0.87±0.04 0.77±0.03 0.97±0.01 0.81±0.03 0.20±0.00
COM-SIMPLE-EX-GP 31±1 14±2 0.91±0.02 0.85±0.03 0.74±0.04 0.97±0.01 0.79±0.04 0.24±0.01
COM-PSM-GP 88±4 68±4 0.69±0.02 0.55±0.02 0.77±0.03 0.61±0.03 0.45±0.02 1.0±0.04
COM-PSM-CONC-GP 68±12 55±9 0.70±0.02 0.56±0.02 0.67±0.05 0.72±0.07 0.48±0.04 1.41±0.06
(b)Scale-freeNonlinear.
Model #Edges ↓ESHD ↑AUROC ↑AUPRC ↑TPR ↑TNR ↑F1 ↓RRMSE
COM-EX-GP 31±2 40±4 0.68±0.03 0.44±0.04 0.40±0.03 0.90±0.01 0.44±0.03 0.17±0.00
COM-SIMPLE-EX-GP 30±2 43±4 0.66±0.03 0.43±0.05 0.34±0.03 0.89±0.01 0.38±0.04 0.17±0.00
COM-EX-K3-GP 45±2 49±4 0.65±0.03 0.42±0.05 0.46±0.04 0.82±0.02 0.43±0.04 0.17±0.00
(c)Erdös-RényiLinear.
Model #Edges ↓ESHD ↑AUROC ↑AUPRC ↑TPR ↑TNR ↑F1 ↓RRMSE
COM-EX-GP 34±1 47±3 0.61±0.03 0.33±0.04 0.32±0.04 0.85±0.01 0.33±0.04 0.16±0.00
COM-SIMPLE-EX-GP 34±1 48±3 0.61±0.03 0.31±0.04 0.31±0.04 0.85±0.01 0.31±0.04 0.16±0.00
COM-EX-K3-GP 48±1 57±4 0.58±0.04 0.29±0.04 0.38±0.06 0.78±0.01 0.32±0.05 0.16±0.00
(d)Scale-freeLinear.
19