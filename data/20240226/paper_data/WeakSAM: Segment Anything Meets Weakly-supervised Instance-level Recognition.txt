WeakSAM: Segment Anything Meets Weakly-supervised Instance-level
Recognition
LianghuiZhu*1 JunweiZhou*1 YanLiu2 XinHao2 WenyuLiu1 XinggangWang1
Abstract WSOD VOC 2007
test (W2N)
Weakly supervised visual recognition using in-
exact supervision is a critical yet challenging 73.4
learningproblem. Itsignificantlyreduceshuman WSIS COCO 2017 WSOD VOC 2012
test (CIM) test (SoS-WSOD)
labeling costs and traditionally relies on multi-
25.2 69.9
instancelearningandpseudo-labeling. Thispa- 65.4
perintroducesWeakSAMandsolvestheweakly- 17.2 61.9
supervisedobjectdetection(WSOD)andsegmen-
tationbyutilizingthepre-learnedworldknowl- 17.0 16.6
edge contained in a vision foundation model,
55.9
25.0 24.6
i.e.,theSegmentAnythingModel(SAM).Weak-
WSIS COCO 2017
SAM addresses two critical limitations in tra-
val (CIM) WSOD COCO 2014
ditional WSOD retraining, i.e., pseudo ground 63.9 val (SoS-WSOD)
truth (PGT) incompleteness and noisy PGT in-
Previous SOTA
WSIS VOC 2012
stances, through adaptive PGT generation and
WeakSAM
test (CIM)
Region of Interest (RoI) drop regularization. It
alsoaddressestheSAM’sproblemsofrequiring
Figure1.QuantitativecomparisonsbetweenWeakSAMandpre-
promptsandcategoryunawarenessforautomatic
viousSOTAmethodsunderdifferenttasksandbenchmarks.The
object detection and segmentation. Our results
scaleofeachaxisintheradarchartisnormalizedbytheperfor-
indicate that WeakSAM significantly surpasses
manceofthepreviousSOTAmethods(markedinparentheses),
previousstate-of-the-artmethodsinWSODand
andthestrideofeachaxisisthesame.
WSISbenchmarkswithlargemargins,i.e. aver-
ageimprovementsof7.4%and8.5%,respectively. exactsupervision,suchasimage-levellabels. Subsequently,
Codeisavailableathttps://github.com/ thetrainedWSLnetworkisemployedtogeneratepseudo
hustvl/WeakSAM. groundtruth(PGT),whichservesasaformofrefined,al-
beitstillinaccuratesupervision. Finally,thePGTisusedas
inaccuratesupervisiontolaunchWSLretraining. Although
1.Introduction theiterativeWSLprocessachievessignificantprogress,it
is still limited by the lack of external knowledge, which
Weakly-supervised learning (WSL) (Zhou, 2018; Wang
restrictstheperformanceofWSLandhindersitfrommatch-
et al., 2013; Xu et al., 2014) is a crucial component of
ingfully-supervisedlearning(FSL).
machinelearning. Itisparticularlyvaluableintaskswhere
strong supervision is difficult to annotate due to the high Nowadays,foundationmodelsaregainingincreasingatten-
costofdatalabeling(Locatelloetal.,2020;Schroeteretal., tionbecauseoftheirtransferablepre-learnedworldknowl-
2019; Fu et al., 2020). Due to the massive demand for edge, whichcanberegardedaspowerfulexternalknowl-
annotated data in visual perception, WSL is essential in edgeforWSL.Asavisionfoundationmodel,SAM(Kirillov
developingalabel-efficientrecognitionsystem. Inthestan- etal.,2023)achievesoutstandingperformanceininterac-
dardweakly-supervisedvisualperceptionparadigm(Tang tive, class-agnostic segmentation. SAM owes its success
etal.,2018a;Suietal.,2022),trainingcommenceswithin- topromptabletrainingonalarge-scaledataset. However,
therearetwomaindrawbackstoSAM:First,SAMrequires
*Equalcontribution 1SchoolofEIC,HuazhongUniversityof
interactiveoperationsasinput,whichmeansitcannotwork
Science&Technology2AlipayTianQianSecurityLab. Corre-
automaticallywithouthumanprompts. Second,SAMpro-
spondenceto:XinggangWang<xgwang@hust.edu.cn>.
ducesclass-agnosticsegmentsandcannotassignclasslabels.
ThesedrawbacksseverelyrestricttheapplicationofSAM
1
4202
beF
22
]VC.sc[
1v21841.2042:viXraWeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
as a generic visual framework. As a strong complement, significantly surpassing previous SOTA methods as
WSLisgoodatminingclassificationcluesthroughinexact showninFig.1.
supervision,whichcanprovideautomaticpromptsforSAM.
Subsequently, WSL with SAM’s knowledge can further
2.RelatedWork
bringclass-awareperception.
2.1.SegmentAnythingModel
This motivates us to assimilate SAM within the WSL
paradigm.TheWeakSAMframeworkisdesignedtoharness TherecentSegmentAnythingModel(SAM)(Kirillovetal.,
transferableknowledgefromSAM,therebyenrichingthe 2023)drawsgreatattentionfromresearchers. TheSAMis
WSL process. Simultaneously, it offers the capability to trainedonSA-1Bwithover1billionmasks,followingthe
deliver automatic classification clues to SAM. This bidi- model-in-the-loopmanner. Besides,SAMperformssupe-
rectionalenhancementconstructsapromisingfoundation- riorzero-shottransfercapabilitiesandisappliedinmany
model-based weakly-supervised visual perception frame- visual tasks, e.g., FGVP (Yang et al., 2023) incorporates
work. Specifically,inaweakly-supervisedobjectdetection SAM to achieve zero-shot fine-grained visual prompting,
(WSOD) setting, WeakSAM uses classification clues as MedSAM (Ma & Wang, 2023) adapts SAM into a large
SAMpromptstoproduceproposalsautomatically. These scalemedicaldatasettobuildamedicalfoundationmodel,
proposalsarethenusedinWSODtrainingforclass-aware andsomemethods(Sunetal.,2023;Jiang&Yang,2023;
perception. Chen et al., 2023) utilize SAM to deal with the weakly-
supervisedsemanticsegmentationproblem. However,SAM
WithinthescopeoftheWeakSAMframework,ouranalysis
isaninteractivesegmentationmethod,whichheavilyrelies
identifiestwoprevailinglimitationsintheiterativeWSOD
onhumanprompts.
retrainingapproach: theissueofpseudogroundtruth(PGT)
incompletenessandthepresenceofnoisyPGTinstances. Inourapproach,weinnovativelyproposetoautomatically
Theformer,PGTincompleteness,referstothetendencyof promptSAMusingclassificationcluesforextractingregion
WSOD-generatedPGTtoomitsomeobjectsorcategories, proposals. Thismethodresultsinhigh-recallproposalsthat
leading to insufficient training for these categories. The surpasstraditionalmethodslikeSelectiveSearchinterms
latter, noisy PGT instances, pertain to the prevalent pres- ofbothefficiencyandeffectiveness. Thisadvancementrep-
enceofnoisewithinthePGT,whichadverselyimpactsthe resentsasignificantimprovementinthedomainofproposal
retrainingprocess. Toeffectivelymitigatethesechallenges, generationwithintheWSODframework.
weintroducetwokeystrategies: adaptivePGTgeneration
2.2.Weakly-supervisedObjectDetection
toaddressthePGTincompletenessproblem,andRegionof
Interest(RoI)dropregularizationtocounteractthenoisein Weakly-supervisedobjectdetection(WSOD)withimage-
PGTinstances. Moreover,WeakSAM’scapabilityenables level labels (Laptev et al.; Diba et al., 2017; Tang et al.,
the extension in the realm of weakly-supervised instance 2018b; Gao et al., 2018; Wan et al., 2018; Zhang et al.,
segmentation(WSIS).Inthiscontext,SAMisemployedto 2018a;Liuetal.,2019;Lietal.,2019;Arunetal.,2019;
furtherrefineWeakSAM-PGT,enablingthegenerationof Sunetal.,2020;Arunetal.,2020;Jiaetal.,2021;Wanetal.,
pseudoinstancesegmentationlabels. Thisapproachexem- 2019)isimportantforreducingthehumanannotationbur-
plifiesWeakSAMispromisingtobuildaunifiedweakly- den. Thepreviousworks,i.e.,WSDDN(Bilen&Vedaldi,
supervisedinstance-levelrecognitionframework. 2016b)andOICR(Tangetal.,2017),proposedtheMultiple
InstanceLearningandonlinerefinementparadigms. The
Themaincontributionsofthispapercanbesummarizedas
laterworksaimedtoimprovetheWSODperformancefrom
follows: differentperspectives. SuchasWSOD2(Zengetal.,2019)
• Weproposeaweakly-supervisedinstance-levelrecog- introduced bottom-up object evidence, PCL (Tang et al.,
nition framework (WeakSAM), which automatically 2018a) proposed to cluster proposals, MIST (Ren et al.,
prompts SAM by classification clues for proposals. 2020)utilizedaself-trainingalgorithm,etc. Besides,some
TheWeakSAM-proposalsimproveboththeeffective- methods(Tangetal.,2018a;Jieetal.,2017;Lietal.,2016;
nessandefficiencyofWSOD. Suietal.,2022;Zhangetal.,2018b;Huangetal.,2022)also
retrainedafully-supervisedobjectdetectionnetworkwith
• We analyze the weaknesses in traditional WSOD re- generated pseudo ground truth (PGT). However, most of
training,andproposeadaptivePGTgenerationandRoI themusedtheproposalsgeneratedfromlow-levelmethods,
dropregularizationtoaddressthem,respectively. Af- i.e.,SelectiveSearch(Uijlingsetal.,2013),EdgeBox(Zit-
tertheWeakSAM-WSODiscomplete,theproposed nick&Dolla´r,2014),andMCG(Pont-Tusetetal.,2016),
WeakSAMcanbeeasilyappliedtoWSISfurther. whichcontainagreatnumberofredundantproposalsand
bringanoptimizationchallenge.
• The proposed WeakSAM achieves state-of-the-art
(SOTA)resultsontheWSODandWSISbenchmarks, Differentfrompreviousmethods,ourWeakSAM-proposals
2WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
ClassificationCluesGeneration WeakSAMProposalsGen. WSODPipeline WSIS
FineCAM
PlainTransformer Weakly-supervised
Generation SAM
Detector
Adaptive PGT
CrossAttention CoarseCAM FineCAM
K×H×N×N×C N×N×C N×N×C SAM Generation
Dense Spatial PseudoInstanceLabels
Sampling
PeakPoints Extraction
RoIDrop Instance
Retraining Segmentation
PromptPointsGeneration Proposals & Masks
Figure2.AnoverviewoftheproposedWeakSAMframework.WefirstintroduceclassificationcluesandspatialpointsasautomaticSAM
prompts,whichaddresstheproblemofSAMrequiringinteractiveprompts.Next,weusetheWeakSAM-proposalsintheWSODpipeline,
inwhichtheweakly-superviseddetectorperformsclass-awareperceptiontoannotatepseudogroundtruth(PGT).Then,weanalyze
theincompletenessandnoiseproblemexistinginPGTandproposeadaptivePGTgeneration,RoIdropregularizationtoaddressthem,
respectively.Finally,weuseWeakSAM-PGTtopromptSAMforWSISextension.Thesnowflakemarkmeansthemodelisfrozen.
havefewernumbersandhigherrecall,whichreducesthe 3.Method
difficultyoffindingthecorrectproposalsforWSODmeth-
WepresenttheWeakSAMframeworkasshowninFig.2.
ods. ForthekeyproblemofPGTincompletenessandnoisy
Atfirst,WeakSAMautomaticallygeneratespromptsfrom
PGTinstances,weproposeadaptivePGTgenerationand
classificationcluesandspatialsamples. Next,WeakSAM
RegionofInterest(RoI)dropregularizationtoaddressthem,
sendthepromptstoSAMforWeakSAM-proposals. Then,
respectively.
welaunchtheweakly-supervisedobjectdetection(WSOD)
2.3.Weakly-supervisedInstanceSegmentation pipeline,whichisenhancedbyWeakSAM-proposals,adap-
tivepseudogroundtruth(PGT)generation,andRoIdrop
Weakly-supervisedinstancesegmentation(WSIS)aimsto
regularization. Last,weusethePGTboxesgeneratedbythe
achieveinstancesegmentationthroughweaksupervision,
WSODpipelinetolaunchtheweakly-supervisedinstance
suchasbox-levelsupervision(Tianetal.,2021;Wangetal.,
segmentationextension.
2021;Chengetal.,2023;Hsuetal.,2019;Liaoetal.,2019;
Leeetal.,2021;Khorevaetal.,2017;Zhangetal.,2023;
3.1.ClassificationCluesasAutomaticPrompt
Zhuetal.,2023b;Lietal.,2022),andimage-levelsuper-
vision (Ge et al., 2019; Ou et al., 2021; Zhu et al., 2019; Previous WSOD methods face an optimization prob-
Liu et al., 2020; Hwang et al., 2021; Zhang et al., 2021; lem caused by the redundant proposals, e.g., Selective
Huetal.,2020;Hsiehetal.,2023;Laradji12etal.). The Search (Uijlings et al., 2013) and EdgeBox (Zitnick &
WSISwithimage-levelsupervisionischallengingbecauseit Dolla´r,2014), becausetheseproposalsareonlybasedon
lacksaccurateinstancelocations. Someimage-levelWSIS low-levelfeatures. Toaddressthisproblem,weproposeto
methodsuseCAMtoextractcoarseobjectlocations,such transferknowledgeinthefoundationmodel,i.e.,SAM,for
as PRM (Zhou et al., 2018), IAM (Zhu et al., 2019), IR- proposalgeneration. Weuseclassificationcluestoprompt
Net (Ahn et al., 2019), BESTIE (Kim et al., 2022), etc. SAMautomatically,whichalsosolvestheshortcomingof
Someotherimage-levelWSISmethodstrytoincorporate SAMrequiringinteractiveprompts
instancecluesfromextrapriors,suchasFanetal.(Fanetal.,
2018b),LIID(Liuetal.,2020),CIM(Lietal.,2023),etc.
ClassificationCluesGeneration AsshowninFig.2,we
However,theyalwaysneedcomplicatednetworksandlack
extractclassificationcluesfromaclassificationViT.Specif-
high-qualityinstancesegments.
ically,wechoosethepre-trainedweakly-supervisedseman-
DifferentfrompreviousWSISmethods,theproposedWSIS ticsegmentationnetwork, WeakTr(Zhuetal.,2023a), to
extensionusingWeakSAM-PGTandSAM’spredictionis provide classification clues because of its superior local-
concise and effective. The generated pseudo instance la- ization ability. At first, we extract cross-attention maps
belscanfurtherbeappliedtoanyfully-supervisedinstance CA ∈ RK×H×N×N×C from the self-attention maps,
segmentationmethod. whereK isthenumberoftransformerencodinglayers,H
isthenumberofattentionheadsineachlayer,N×N isthe
spatialsizeofthevisualtokens,andC representsthetotal
3WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
numberofclassificationcategories. Then,weobtaincoarse V ,andmaxpoolingoperation. Next,wereshapethe
delete
CAM, CAM ∈ RN×N×C, from the convolutional inputmapsandensurethelasttwodimensionscorrespondto
coarse
CAM head, which takes visual tokens at the final trans- theoriginalimagesizeandtheothersasthefirstdimension.
formerlayerasinputandproducescoarseCAM.Last,we Then,weapplymaxpoolingontheinputmapsM,andsort
useWeakTrtoproducefineCAM,CAM ∈RN×N×C. V andP indescendingorderbasedonV. Last,weremove
fine
points with low activation values or close to high-score
PromptPointsGeneration AsshowninFig.2,weex- points.
tractpromptsfromdensesamplingpoints,cross-attention
maps, and CAMs. At first, the dense sampling requires WeakSAMProposalsGeneration AttheWeakSAMpro-
splittingtheimageintoS×S patchesandtakingthecen- posalgenerationstage,weusethethreekindsofprompts
terpointsasprompts. Notably,thedensesamplingpoints topromptSAMautomatically. Wedirectlyaddsemantic-
provide spatial-aware prompts but lackexplicitreference awarepromptsandspatial-warepromptstothepromptlist,
to objects and semantics, which means increasing the S becausetheyusuallyhaveclearlocalizationtoforeground
usuallyleadstoagreatnumberofinvalidsamplingpoints. objectsandspatialpositions,respectively. Fortheinstance-
Then,wegetpeakpointsfromthecross-attentionmapsas awarepromptsthathavesomeredundancy,weclusterthem
prompts. We observe that these maps do not solely con- tofiltertheduplicatedonesandthenaddthemtotheprompt
centrateonobjectsfromtheircorrespondingcategoriesbut list.Finally,allpromptsinthepromptlistareusedtoprompt
alsogiveattentiontoobjectsfromdifferentcategories. So, SAMforproposals.
wemarkthesepromptsasinstance-awareones. Last, we
extract peak points from coarse CAM and fine CAM as
3.2.WeakSAMWSODPipeline
semantic-awareprompts,whicharemorepreciseandfocus
onareasofforegroundobjects. Tobetterdescribetheproposedweakly-supervisedobject
detection (WSOD) pipeline, we first present the weakly-
Algorithm1PeakPointsExtraction supervised detector training with WeakSAM-proposals.
Require: mapsM(eitherCAorCAM),kernelsizek,activa- Then, we identify the PGT incompleteness problem and
tionthresholdτ introducetheproposedadaptivePGTgenerationtoaddress
Ensure: peak points coordinates list P = [p 0,p 1,...,p n−1], it. Last, we analyze the noise problem existing in the re-
correspondingpeakvalueslistV =[v ,v ,...,v ]
0 1 n−1 trainingphase,andproposeRegionofInterest(RoI)drop
1: M=M.view(-1,N,N)//reshape
regularizationtoalleviatetheeffectofnoise.
2: InitializeP,V asemptylist
3: InitializeMaxpool()operationwithkernelsizek
4: P,V =Maxpool(M)//getcoordinatesandvalues Weakly-supervisedDetectorTraining Aprimarychal-
5: SortV indescendingorderofnumericalvalue,andrearrange
lengeintraditionalWSODmethodsisthelowtrainingef-
P accordingly
ficiency,largelyattributedtotheredundancyofproposals.
6: InitializelistP ,V tomarkpointsfordeletion
delete delete
7: foreachindexifrom0tolength(P)do TraditionalapproachesofteninvolvetheRegionofInterest
8: //skipfurtherchecksforpointsmarkedfordeletion poolinglayerprocessingthousandsofproposalsperimage,
9: ifp iinP deletethen which impairs both effectiveness and efficiency. To ad-
10: Continue
dressthisissue,ourWeakSAM-proposalsadopttransferred
11: endif
knowledge from SAM and classification clues. The pro-
12: //markactivationpointswithlowscore
13: ifv <τ then posedmethodfocusesongeneratingasmallerquantityof
i
14: Appendp i,v itoP delete,V
delete
proposalswhilemaintaininghighrecall,therebyenhancing
15: Continue theoverallefficiencyandefficacyofthedetectionprocessin
16: endif
aWSODcontext.WemainlyapplytheproposedWeakSAM
17: //marklower-scorepointsnearthecurrentpoint
onsomeconvincingWSODmethods,includingOICR(Tang
18: foreachindexj =i+1tolength(P)do
19: if||p −p ||≤k/2then et al., 2017) and MIST (Ren et al., 2020), which receive
j i
20: Appendp j,v j toP delete,V delete significantimprovements. AsshowninTable1,quantitative
21: endif resultsshowthatWeakSAM-enhancedWSODcanannotate
22: endfor
boundingboxesforobjectsmoreprecisely.
23: endfor
24: RemoveallpointsinP andV fromP andV
delete delete
25: returnP,V Adaptive PGT Generation Generating high-quality
pseudo ground truth (PGT) is the key to the WSOD
Specifically, we extract peak points from cross-attention paradigm. Traditional WSOD methods often encounter
mapsandCAMs, asshowninAlgorithm1. Givencross- theissueofPGTincompleteness. Thisoccursbecausethese
attention maps or CAMs as input, we first initialize the methodstypicallyselecttop-scoringproposalsasPGTor
peakpointslistP,peakvalueslistV,deletedlistsP , applyauniformthresholdtofilterproposalsacrossallcate-
delete
4WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
gories. Suchapproachescanleadtotheomissionofobjects
or entire categories, especially when proposals in certain
         
categoriesscorelow. Toaddresstheseproblems, wepro-
poseanadaptivePGTgenerationmethodtonormalizethe          
scoredistributionofproposals,ensuringtheyfallwithina
        
similarrange,asshowninAlgorithm.2.
ForboxlistB ∈ RN×5 andcorrespondingscorelistS ∈         
RN×1, we first select them with a specific classification         
 1 X P E H U  R I  5 R , V
labelandthennormalizethescores. TheN isthenumber
ofpredictedboxes,andtheseconddimensionofB isthe       ( U U R U  5 D W H    
combinationofacategorylabelandfourcoordinatevalues.  
                                           
Next,wekeepboxeswithscoreshigherthanthethreshold  1 R U P D O L ] H G  & O D V V L I L F D W L R Q  / R V V
τ . Pleasenotethatthenormalizationenablesthethreshold
s
toworkforallcategoriesadaptively,sowewouldnotlosea Figure3.Therelationshipbetweenthenormalizedclassification
groundtruthcategoryevenifallboxesinthiscategoryhave loss,thecorrespondingnumberofRoIs,andthecorresponding
low scores. Then, we select the boxes whose main parts errorrate.TheresultsareobtainedfromtrainingtheFaster-RCNN
usingPGTinthepreliminarytrainingstage.
arenotcontainedinsomebiggerboxes. Becausetheboxes
thathavemoreoverlapareoftenlocalcomponentsofsome
Intuitively, we propose a method, named RoI drop regu-
objects. Last,wereturntheboxlistB′asthefinalPGT.
larization, to adaptively drop the RoIs with larger losses.
Notably,theproposedmethodiseasytoimplementandcan
Algorithm2AdaptivePseudoGroundTruthGeneration
furtherhelpthequery-baseddetectorstoalleviatethenoisy
Require: boxes list B of an image, corresponding scores list PGTproblembyitsvariant,querydropregularization. For
S,correspondingclassificationlabelsY,scorethresholdτ s, anchor-basedFSODmethods,e.g.,Faster-RCNN(Renetal.,
overlapthresholdτ
o 2015), we first determine the thresholds τ and τ for
Ensure: pseudogroundtruthboxesB′ cls reg
1: initializeB′asemptylist classificationlossandregressionloss,respectively. Then,
2: foreachy iinY do wecomputethedropsignald ifori−thRoI.
3: //getboxes’indiceswithlabely
4: idx =where(B[:,0]==y ) i (cid:26) 1, lcls ≤τ , andlreg ≤τ
i i d = i cls i reg , (1)
5: S i=S[idx i,:] i 0, others
6: B =B[idx ,:]
i i
7: S inorm= maS x(i S− im )−in m(S ini ()
Si)
//normalizescores wherethel iclsandl ireg representtheclassificationlossand
8: //keepboxeswithhighscore regression loss for each RoI, respectively. When the two
9: idx ={j|s ∈Snorm, s >τ }
keep j i j s losses of a RoI are all below their thresholds, we set its
10: B =B [idx ,:]
11:
Sni ormi =Snok re mep
[idx ,:]
drop signal d
i
as 1. Finally, we integrate the d
i
into the
i i keep
12: //selectboxeswithlessoverlap computationoffinallossL.
13: foreachboxb inB do
j i
14: overlaps={|bj∩bk| |b ∈B , k̸=j}
|bj| k i
(cid:88) (cid:88)
1 15 6:
:
ifa All po pv ee nr dl bap t< oBτ o
′
inoverlapsthen L= d il icls+λ p∗ id il ireg, (2)
j i i
17: endif
18: endfor
19: endfor where p∗ i is 1 if the box is positive, and 0 if the box is
20: returnB′ negative. Theλisabalancingweight.
RoIDropRegularization Arecognizedissueinthere- Forquery-basedFSODmethods,e.g.,DINO(Zhangetal.,
training phase of WSOD is noisy PGT instances. These 2022),sincequeriescanberegardedasdynamicRoIs,we
noisyinstancesresultinPGTactingastheinaccuratesuper- applyquerydropregularizationonthem. Becauseonlya
vision. Alleviatingthisproblemiscriticalforenhancingthe few matched queries need to calculate box loss lbox and
performanceofWSODretraining. Toanalyzethisproblem
IoUlossliou,weonlysetapercentilethresholdbasedon
indepth,wefirstdividetheRoIsintodifferentlossintervals. classification loss lcls. Only when the i−th query’s loss
Then,wemarktheRoIswhosecorrespondingPGTsdonot l icls islessthanthelossatτ%percentile,i.e.,l τcls,willits
haveatleast70%IoUwiththegroundtruthboxesaserror correspondingd ibesetto1.
ones. Last, we present the statistics as shown in Fig. 3,
(cid:26) 1, lcls ≤lcls
whichdemonstratesthattheRoIswithlargerlossesareina d = i τ . (3)
i 0, others
smallamountandhaveahigherrorrate.
5
 V , R 5  I R  U H E P X 1
 H W D 5  U R U U (WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
Table1.ComparisonsoftheWSODperformanceintermsofAPmetricsonthreebenchmarks: PASCALVOC2007,PASCALVOC
2012,andCOCO2014.TheSup.columndenotesthetypeofsupervisionusedfortrainingincludingfullsupervision(F),point-level
labels(P),image-levellabels(I).“*”meanstheresultsrelyonMCG(Pont-Tusetetal.,2016)proposals.“‡”meansthismethodusethe
aheavyRN50-WS-MRRP(Shenetal.,2020)backbone(1.76×parametersthanVGG16and10.10×parametersthanRN50).Wemark
thebestWSODresultsinbold.
VOC07 VOC12 COCO14
Methods Proposal Sup. Retrain
AP50 AP50 AP50:95 AP50 AP75
Fully-supervisedobjectdetectionmethods.
FasterR-CNN(Renetal.,2015) RPN F – 69.9 – 21.2 41.5 –
WSODmethodswithpointsupervision.
P2BNet(Chenetal.,2022) RPN P – 60.2 – 19.4 43.5 –
WSODmethodswithimage-levelsupervision.
C-MIDN (Gaoetal.,2019) SS,MCG – 52.6 50.2 9.6∗ 21.4∗ –
WSOD2 (Zengetal.,2019) SS – 53.6 47.2 10.8 22.7 –
SLV (Chenetal.,2020) SS – 53.5 49.2 – – –
CASD (Huangetal.,2020) SS – 56.8 53.6 12.8 26.4 –
IM-CFB (Yinetal.,2021) SS I – 54.3 49.4 – – –
OD-WSCL (Seoetal.,2022) SS,MCG – 56.4 54.6 13.7∗ 27.7∗ 11.9∗
WSOD-CBL (Yinetal.,2023) SS – 57.4 53.5 13.6 27.6 –
WSOVOD (Linetal.,2024) LO-WSRPN+SAM – 59.1 59.8 18.8 27.1 19.7
WSOVOD‡ LO-WSRPN+SAM – 63.4 62.1 20.5 29.1 21.4
Baselineandours.
OICR(Tangetal.,2017) SS,MCG – 41.2 37.9 8.0∗ 18.9∗ 7.0∗
I
WeakSAM(OICR) WeakSAM – 58.9+17.7 58.4+20.5 19.9+11.9 32.1+13.2 20.6+13.6
Baselineandours.
MIST (Renetal.,2020) SS,MCG – 54.9 52.1 11.4∗ 24.3∗ 9.4∗
I
WeakSAM(MIST) WeakSAM – 67.4+12.5 66.9+14.8 22.9+11.5 35.2+10.9 24.6+15.2
WSODmethodswithimage-levelsupervision.+Retrain
W2F(Zhangetal.,2018b) RPN FasterR-CNN 52.4 47.8 – – –
SoS-WSOD(Suietal.,2022) RPN I FasterR-CNN 64.4 61.9 16.6 32.8 15.2
W2N(Huangetal.,2022) RPN FasterR-CNN 65.4 60.8 15.9 33.3 13.4
Ours.+Retrain
WeakSAM(OICR) RPN FasterR-CNN 65.7 62.9 22.3 36.5 23.0
WeakSAM(MIST) RPN FasterR-CNN 71.8 69.2 23.8 38.5 25.1
I
WeakSAM(OICR) – DINO 66.1 63.7 24.9 36.9 26.8
WeakSAM(MIST) – DINO 73.4 70.2 26.6 39.3 29.0
may have different settings. For WSOD, we use three
datasets,i.e.,PASCALVOC2007(Everinghametal.,2015),
(cid:88)
L Hungarian = d i[l icls+p∗ il ibox+p∗ il iiou]. (4) PASCALVOC2012(Everinghametal.,2015),andCOCO
i 2014(Linetal.,2014). PASCALVOC2007has2501im-
ages for training, 2510 images for evaluation, and 4592
3.3.WeakSAMforWSIS imagesfortesting. PASCALVOC2012contains5717train-
ingimages,5823validationimages,and10991testimages.
Thankstothehigh-qualityWeakSAM-PGT,wecandirectly
COCO2014includesaround80,000imagesfortrainingand
usethemtopromptSAMforprecisesegmentsaspseudo
40,000imagesforvalidation. FollowingpreviousWSOD
instancelabels. FollowingthepracticesintheWeakSAM-
methods, we train WeakSAM on train and val sets and
WSODpipeline,weevaluatethequalityofWeakSAM-PGT
evaluateWeakSAMonthetestsetforPASCALVOC2007
usingR-CNN-basedandquery-basedinstancesegmentation
and2012. ForCOCO2014,weusethetrainsetfortrain-
methods,respectively. Notably,wedonotintroducemore
ing and the val set for evaluating. PASCAL VOC 2007
techniquesintheWeakSAM-WSIS,becausetheWeakSAM
and2012datasetscomprise20objectcategoriesandCOCO
pseudoinstancelabelsareaccurateenough.
2014comprises80ones. Wereporttheaverageprecision
APmetricsforthesebenchmarks. ForWSIS,weusetwo
4.Experiment
datasets,i.e.,PASCALVOC2012,andCOCO2017. The
4.1.ExperimentalSetup PASCAL VOC 2012 dataset includes 10582 images for
training, and 1449 images for evaluation, comprising 20
DatasetsandMetrics WeevaluatetheproposedWeak-
objectcategories. TheCOCO2017datasetincludes115K
SAMonbothweakly-supervisedobjectdetection(WSOD)
trainingimages,5Kvalidationimages,and20Ktestingim-
and weakly-supervised instance segmentation (WSIS)
ages,comprising80objectcategories. Followingprevious
benchmarks. Notably,thesamedatasetsfordifferenttasks
6WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
Table2.ComparisonsoftheWSISperformanceintermsofAPmetricsonPASCALVOC2012.TheSup.columndenotesthetypeof
supervisionusedfortrainingincludingmasksupervision(M),saliencymaps(S),image-levellabels(I),andSAMmodels(A).Wemark
thebestWSISresultsinbold.
VOC12
Methods Backbone Sup. Retrain
AP AP AP AP
25 50 70 75
Fully-supervisedinstancesegmentationmethods.
MaskR-CNN(Heetal.,2017) ResNet-101 M – 76.7 67.9 52.5 44.9
WSISmethodswithimage-levelsupervision.+Retrain
WISE(Laradjietal.,2019) ResNet-50 I MaskR-CNN 49.2 41.7 – 23.7
IRNet(Ahnetal.,2019) ResNet-50 I MaskR-CNN – 46.7 23.5 –
LIID(Liuetal.,2020) ResNet-50 I+S MaskR-CNN – 48.4 – 24.9
Arunetal.(Arunetal.,2020) ResNet-50 I MaskR-CNN 59.7 50.9 30.2 28.5
WS-RCNN(Ouetal.,2021) VGG-16 I MaskR-CNN 62.2 47.3 – 19.8
BESTIE(Kimetal.,2022) HRNet-W48 I MaskR-CNN 61.2 51.0 31.9 26.6
CIM(Lietal.,2023) ResNet-50 I MaskR-CNN 68.7 55.9 37.1 30.9
Ours.
WeakSAM ResNet-50 I+A MaskR-CNN 70.3 59.6 43.1 36.2
WeakSAM ResNet-50 I+A Mask2Former 73.4 64.4 49.7 45.3
Table3.ComparisonsoftheWSISperformanceintermsofAPmetricsonCOCO2017.TheSup.columndenotesthetypeofsupervision
usedfortrainingincludingmasksupervision(M),saliencymaps(S),image-levellabels(I),andSAMmodels(A).Wemarkthebest
WSISresultsinbold.
COCOval2017 COCOtest-dev
Methods Backbone Sup. Retrain
AP AP AP AP AP AP
50:95 50 75 50:95 50 75
Fully-supervisedinstancesegmentationmethods.
MaskR-CNN(Heetal.,2017) ResNet-50 M – 34.4 55.1 36.7 33.6 55.2 35.3
WSISmethodswithimage-levelsupervision.
WS-JDS(Shenetal.,2019) VGG-16 I – 6.1 11.7 5.5 – – –
PDSL(Shenetal.,2021) ResNet18-WS I – 6.3 13.1 5.0 – – –
Fanetal.(Fanetal.,2018a) ResNet-101 I+S MaskR-CNN – – – 13.7 25.5 13.5
LIID(Liuetal.,2020) ResNet-50 I+S MaskR-CNN – – – 16.0 27.1 16.5
BESTIE(Kimetal.,2022) HRNet-W48 I MaskR-CNN 14.3 28.0 13.2 14.4 28.0 13.5
CIM(Lietal.,2023) ResNet-50 I MaskR-CNN 17.0 29.4 17.0 17.2 29.7 17.3
Ours.
WeakSAM ResNet-50 I+A MaskR-CNN 20.6 33.9 22.0 21.0 34.5 22.2
WeakSAM ResNet-50 I+A Mask2Former 25.2 38.4 27.0 25.9 39.9 27.9
methods,wereporttheaverageprecisionAPmetricswith 4.2.ComparisonswithState-of-the-artMethods
differentIntersection-over-Union(IoU)thresholds.
Weakly-supervised object detection We present the
ImplementationDetails ForWeakSAMproposalsgener- quantitative WSOD results in Table. 1. Compared with
ation,weadopttheWeakTr(Zhuetal.,2023a)withDeiT- our WSOD baseline methods, i.e., OICR and MIST, the
S(Touvronetal.,2021)modelforgeneratingclassification proposedWeakSAMachievesover10%improvementson
clues,theSAM(Kirillovetal.,2023)withViT-H(Dosovit- allmetrics. TheresultsofWeakSAM(MIST)surpassall
skiyetal.,2020)modeltogenerateproposals. ForWeak- WSODmethodsonallmetrics,whichdemonstratetheeffec-
SAM WSOD pipeline, we use the WSOD networks, i.e., tivenessofWeakSAM-proposals. ComparedwithWSOD
OICR (Tang et al., 2017), and MIST (Ren et al., 2020), methodsretrainedbypseudogroundtruth(PGT),theWeak-
withtheVGG-16(Hanetal.,2021)backbonetogenerate SAM (MIST) with Faster R-CNN retraining still outper-
pseudogroundtruth(PGT),andFSODnetworks,i.e.,Faster formstheSoS-WSOD(Suietal.,2022)andW2N(Huang
R-CNN(Renetal.,2015)andDINO(Zhangetal.,2022), et al., 2022) on all metrics, and the WeakSAM (MIST)
with the ResNet-50 (He et al., 2016) backbone to retrain. with DINO retraining even has comparable performance
AsfortheWeakSAM-WSIS,weuseSAM-ViT-Htogener- with fully-supervised Faster R-CNN. The retraining re-
atepseudoinstancelabelsandtraintheR-CNN-basedand sultsdemonstratetheeffectivenessoftheproposedWSOD
query-basedmethods,i.e.,MaskR-CNN(Heetal.,2017) pipeline,whichincludestheadaptivePGTgenerationand
and Mask2former (Cheng et al., 2022), respectively. All RoI drop retraining. Compared with concurrent work,
hyper-parameters in Alg. 1 and Alg. 2 are following the WSOVOD(Linetal.,2024),whichalsoincorporatesSAM,
defaultmannersas Zhuetal.(2023a)and Suietal.(2022).
7WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
Table4.AblationstudiesforWeakSAMpromptsonPASCALVOC2007. Weevaluatetheaveragenumberofproposals,recall,and
WSODperformancebyMIST(Renetal.,2020).
Recall
SS DenseSample CAM CAM CrossAttn. Num. AP
fine coarse IoU=0.50 IoU=0.75 IoU=0.90 50
(cid:33) 2001 92.6 57.7 19.2 54.9
(cid:33) 129 79.6 50.7 24.3 45.2
(cid:33) (cid:33) 151 88.9 67.0 37.2 63.3+18.1
(cid:33) (cid:33) (cid:33) 174 90.6 70.1 40.1 65.5+20.3
(cid:33) (cid:33) (cid:33) (cid:33) 213 95.6 75.0 42.1 67.4+22.2
Duetothelimitationofpages,weleavemoreablationstud-
Table5.Ablation studies for adaptive PGT generation and RoI
iesinthesupplementarymaterial,includingefficiencyanal-
dropregularization.WepresenttheresultsonthePASCALVOC
ysis,sensitivityanalysis,qualitativeanalysis,etc.
2007testset.
(a)Ablationstudiesfortheanchor-baseddetector,i.e.,FasterR- ImprovementsofWeakSAMPrompts Tofurtherana-
CNN(Renetal.,2015). lyzetheimprovementsbroughtbytheproposedWeakSAM
prompts, we conduct ablation experiments for different
Top-1PGT AdaptivePGT RoIDrop AP
50 promptsinTable4. Here,weusetheSelectiveSearch(Ui-
(cid:33) 68.4 jlingsetal.,2013)asthebaselinemethod. Whenonlyusing
(cid:33) 70.7+2.3 the densely sampled points, the generated proposals can
(cid:33) (cid:33) 71.8+3.4 achieve 5.1% higher Recall (IoU=0.90), and 9.7% lower
AP forMIST.AfteraddingpeakCAMpointsandpeak
50
(b) Ablation studies for the query-based detector, i.e., cross attention points as prompts, we can achieve higher
DINO(Zhangetal.,2022).
recallandAP throughonly213proposalsonaverage.
50
Top-1PGT AdaptivePGT QueryDrop AP 50 ImprovementsofWSODPipeline Tofurtheranalyzethe
(cid:33) 71.1 improvementsbroughtbytheproposedWeakSAM-WSOD
pipeline,weconductablationexperimentsforadaptivePGT
(cid:33) 72.8+1.7 generation and RoI drop regularization in Table 5. Here,
(cid:33) (cid:33) 73.4+2.3 wesetabaselinethatusesthepredictedboxeswiththetop-
1scoreasPGTandplainFasterR-CNNastheretraining
ourWeakSAM(MIST)alsoachievesbetterperformance.
network. Itshowsthat adaptive PGTgenerationandRoI
drop can both improve the retraining results of Faster R-
Weakly-supervised instance segmentation We first
CNN (Ren et al., 2015) and DINO (Zhang et al., 2022),
presentthequantitativeWSISresultsofthePASCALVOC
respectively.
2012valsetinTable2.TheproposedWeakSAMwithMask
R-CNN retraining achieves the best performance, which
5.Conclusion
demonstratestheWeakSAMcanbenefitWSISeffectively.
Furthermore,thepseudoinstancelabelsgeneratedbyWeak- In this paper, we introduce WeakSAM, a novel frame-
SAMcanalsobeusedbythemodernquery-basedmethods, work utilizing the Segment Anything Model (SAM) for
e.g.,Mask2Former(Chengetal.,2022),whichachievesthe weakly-supervisedinstance-levelrecognition,demonstrat-
bestresults. ingleadingperformanceinWSODandWSISbenchmarks.
Different from the original SAM, which requires interac-
WethenshowthequantitativeWSISresultsonCOCO2017
tionandcannotbeawareofcategories,WeakSAMrepre-
valandtestsets. Onthesemorechallengingbenchmarks,
sentsaninnovativefusionofSAMwithweakly-supervised
WeakSAM with Mask R-CNN retraining achieves better
learning (WSL), overcoming the redundancy problem of
resultsthanCIM(Lietal.,2023). Besides,theWeakSAM
WSODproposals. TofurtheraddressWSODissuessuch
withMask2Formeralsopresentsthebestresults.
as pseudo ground truth (PGT) incompleteness and noisy
PGTinstances, ourapproachincludesadaptivePGTgen-
4.3.AblationStudies
erationandaRegionofInterest(RoI)dropregularization.
Inthissection,wepresenttheablationstudiestoevaluate TheadaptabilityofWeakSAMisfurthershowcasedthrough
the improvements brought by the proposed methods, i.e., itsextensiontoweakly-supervisedinstancesegmentation
WeakSAMprompts,adaptivePGTgeneration,andRoIdrop (WSIS). Our work aims to inspire further research with
retraining. SAMandWSL,contributingsignificantlytothedevelop-
8WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
mentofauniversalframeworkforweakly-supervisedrecog- Diba, A., Sharma, V., Pazandeh, A., Pirsiavash, H., and
nition. VanGool,L. Weaklysupervisedcascadedconvolutional
networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 914–922,
References
2017.
Ahn,J.,Cho,S.,andKwak,S. Weaklysupervisedlearning
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
of instance segmentation with inter-pixel relations. In
D.,Zhai,X.,Unterthiner,T.,Dehghani,M.,Minderer,M.,
ProceedingsoftheIEEE/CVFConferenceonComputer
Heigold,G.,Gelly,S.,etal. Animageisworth16x16
VisionandPatternRecognition(CVPR),June2019.
words:Transformersforimagerecognitionatscale.arXiv
Arun, A., Jawahar, C., and Kumar, M. P. Dissimilarity preprintarXiv:2010.11929,2020.
coefficientbasedweaklysupervisedobjectdetection. In
Everingham, M., Eslami, S. A., Van Gool, L., Williams,
ProceedingsoftheIEEE/CVFConferenceonComputer
C. K., Winn, J., and Zisserman, A. The pascal visual
VisionandPatternRecognition(CVPR),June2019.
objectclasseschallenge: Aretrospective. International
Arun,A.,Jawahar,C.,andKumar,M.P. Weaklysupervised journalofcomputervision,111:98–136,2015.
instancesegmentationbylearningannotationconsistent
Fan, R., Hou, Q., Cheng, M.-M., Yu, G., Martin, R. R.,
instances. InEuropeanConferenceonComputerVision,
andHu,S.-M. Associatinginter-imagesalientinstances
pp.254–270.Springer,2020.
for weakly supervised semantic segmentation. In Pro-
Bilen,H.andVedaldi,A. Weaklysuperviseddeepdetection ceedingsoftheEuropeanconferenceoncomputervision
networks. In Proceedings of the IEEE conference on (ECCV),pp.367–383,2018a.
computervisionandpatternrecognition,pp.2846–2854,
Fan, R., Hou, Q., Cheng, M.-M., Yu, G., Martin, R. R.,
2016a.
andHu,S.-M. Associatinginter-imagesalientinstances
Bilen,H.andVedaldi,A. Weaklysuperviseddeepdetection for weakly supervised semantic segmentation. In Pro-
networks. In Proceedings of the IEEE conference on ceedingsoftheEuropeanconferenceoncomputervision
computervisionandpatternrecognition,pp.2846–2854, (ECCV),pp.367–383,2018b.
2016b.
Fu,D.,Chen,M.,Sala,F.,Hooper,S.,Fatahalian,K.,and
Chen,P.,Yu,X.,Han,X.,Hassan,N.,Wang,K.,Li,J.,Zhao, Re, C. Fast and three-rious: Speeding up weak super-
J.,Shi,H.,Han,Z.,andYe,Q. Point-to-boxnetworkfor visionwithtripletmethods. InIII,H.D.andSingh,A.
accurateobjectdetectionviasinglepointsupervision. In (eds.),Proceedingsofthe37thInternationalConference
European Conference on Computer Vision, pp. 51–67. on Machine Learning, volume 119 of Proceedings of
Springer,2022. MachineLearningResearch,pp.3280–3291.PMLR,13–
18 Jul 2020. URL https://proceedings.mlr.
Chen, T., Mai, Z., Li, R., and Chao, W.-l. Segment any-
press/v119/fu20a.html.
thing model (sam) enhanced pseudo labels for weakly
supervised semantic segmentation. arXiv preprint Gao, M., Li, A., Yu, R., Morariu, V. I., and Davis, L. S.
arXiv:2305.05803,2023. C-wsl: Count-guidedweaklysupervisedlocalization. In
Proceedings of the European conference on computer
Chen,Z.,Fu,Z.,Jiang,R.,Chen,Y.,andHua,X.-S. Slv:
vision(ECCV),pp.152–168,2018.
Spatial likelihood voting for weakly supervised object
detection. InProceedingsoftheIEEE/CVFConference Gao,Y.,Liu,B.,Guo,N.,Ye,X.,Wan,F.,You,H.,andFan,
on Computer Vision and Pattern Recognition (CVPR), D. C-midn: Coupledmultipleinstancedetectionnetwork
June2020. withsegmentationguidanceforweaklysupervisedobject
detection. InProceedingsoftheIEEE/CVFInternational
Cheng,B.,Misra,I.,Schwing,A.G.,Kirillov,A.,andGird-
ConferenceonComputerVision,pp.9834–9843,2019.
har,R. Masked-attentionmasktransformerforuniversal
imagesegmentation. InProceedingsoftheIEEE/CVF Ge,W.,Guo,S.,Huang,W.,andScott,M.R. Label-penet:
conferenceoncomputervisionandpatternrecognition, Sequentiallabelpropagationandenhancementnetworks
pp.1290–1299,2022. for weakly supervised instance segmentation. In Pro-
ceedingsoftheIEEE/CVFInternationalConferenceon
Cheng, T., Wang, X., Chen, S., Zhang, Q., and Liu, W.
ComputerVision,pp.3345–3354,2019.
Boxteacher: Exploring high-quality pseudo labels for
weaklysupervisedinstancesegmentation.InProceedings Han, K., Xiao,A., Wu, E., Guo, J., Xu,C., andWang, Y.
of the IEEE/CVF Conference on Computer Vision and Transformerintransformer. AdvancesinNeuralInforma-
PatternRecognition,pp.3145–3154,2023. tionProcessingSystems,34:15908–15919,2021.
9WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
He,K.,Zhang,X.,Ren,S.,andSun,J. Deepresiduallearn- Khoreva, A., Benenson, R., Hosang, J., Hein, M., and
ingforimagerecognition. InProceedingsoftheIEEE Schiele,B. Simpledoesit: Weaklysupervisedinstance
conferenceoncomputervisionandpatternrecognition, andsemanticsegmentation. InProceedingsoftheIEEE
pp.770–778,2016. ConferenceonComputerVisionandPatternRecognition
(CVPR),July2017.
He,K.,Gkioxari,G.,Dolla´r,P.,andGirshick,R. Maskr-
cnn. InProceedingsoftheIEEEinternationalconference Kim,B.,Yoo,Y.,Rhee,C.E.,andKim,J. Beyondsemantic
oncomputervision,pp.2961–2969,2017. to instance segmentation: Weakly-supervised instance
segmentationviasemanticknowledgetransferandself-
Hsieh,Y.-H.,Chen,G.-S.,Cai,S.-X.,Wei,T.-Y.,Yang,H.- refinement. InProceedingsoftheIEEE/CVFConference
F.,andChen,C.-S. Class-incrementalcontinuallearning onComputerVisionandPatternRecognition(CVPR),pp.
forinstancesegmentationwithimage-levelweaksuper- 4278–4287,June2022.
vision. InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision(ICCV),pp.1250–1261, Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C.,
October2023. Gustafson,L.,Xiao,T.,Whitehead,S.,Berg,A.C.,Lo,
W.-Y., Dolla´r, P., and Girshick, R. Segment anything.
Hsu,C.-C.,Hsu,K.-J.,Tsai,C.-C.,Lin,Y.-Y.,andChuang, arXiv:2304.02643,2023.
Y.-Y. Weakly supervised instance segmentation using
the bounding box tightness prior. Advances in Neural Laptev,I.,Kantorov,V.,Oquab,M.,andCho,M. Context-
InformationProcessingSystems,32,2019. locnet: Context-awaredeepnetworkmodelsforweakly
supervisedlocalization.
Hu, Z., Liu, Z., Li, G., Ye, L., Zhou, L., and Wang, Y.
Weakly supervised instance segmentation using multi- Laradji,I.H.,Vazquez,D.,andSchmidt,M. Wherearethe
stageerasingrefinementandsaliency-guidedproposals masks: Instancesegmentationwithimage-levelsupervi-
ordering. JournalofVisualCommunicationandImage sion. arXivpreprintarXiv:1907.01430,2019.
Representation,73:102957,2020.
Laradji12, I.H., Vazquez, D., Schmidt,M., andElement,
Huang, Z., Zou, Y., Kumar, B., and Huang, D. Compre- A. Where are the masks: Instance segmentation with
hensiveattentionself-distillationforweakly-supervised image-levelsupervision.
objectdetection. Advancesinneuralinformationprocess-
ingsystems,33:16797–16807,2020. Lee, J., Yi, J., Shin, C., and Yoon, S. Bbam: Bounding
boxattributionmapforweaklysupervisedsemanticand
Huang, Z., Bao, Y., Dong, B., Zhou, E., and Zuo, W. instancesegmentation. InProceedingsoftheIEEE/CVF
W2n:switching from weak supervision to noisy super- ConferenceonComputerVisionandPatternRecognition
visionforobjectdetection,2022. (CVPR),pp.2643–2652,June2021.
Hwang,J.,Kim,S.,Son,J.,andHan,B. Weaklysupervised Li, D., Huang, J.-B., Li, Y., Wang, S., and Yang, M.-H.
instancesegmentationbydeepcommunitylearning. In Weaklysupervisedobjectlocalizationwithprogressive
ProceedingsoftheIEEE/CVFWinterConferenceonAp- domainadaptation. InProceedingsoftheIEEEConfer-
plicationsofComputerVision(WACV),pp.1020–1029, ence on Computer Vision and Pattern Recognition, pp.
January2021. 3512–3520,2016.
Jia,Q.,Wei,S.,Ruan,T.,Zhao,Y.,andZhao,Y.Gradingnet: Li, W., Liu, W., Zhu, J., Cui, M., Hua, X.-S., andZhang,
Towardsprovidingreliablesupervisionsforweaklysu- L. Box-supervisedinstancesegmentationwithlevelset
pervisedobjectdetectionbygradingtheboxcandidates. evolution. InEuropeanconferenceoncomputervision,
InProceedingsoftheAAAIConferenceonArtificialIn- pp.1–18.Springer,2022.
telligence,volume35,pp.1682–1690,2021.
Li,X.,Kan,M.,Shan,S.,andChen,X. Weaklysupervised
Jiang,P.-T.andYang,Y.Segmentanythingisagoodpseudo- objectdetectionwithsegmentationcollaboration. InPro-
labelgeneratorforweaklysupervisedsemanticsegmen- ceedingsoftheIEEE/CVFInternationalConferenceon
tation. arXivpreprintarXiv:2305.01275,2023. ComputerVision(ICCV),October2019.
Jie, Z., Wei, Y., Jin, X., Feng, J., and Liu, W. Deep self- Li,Z.,Zeng,Z.,Liang,Y.,andYu,J.-G.Completeinstances
taughtlearningforweaklysupervisedobjectlocalization. miningforweaklysupervisedinstancesegmentation. In
InProceedingsoftheIEEEconferenceoncomputervi- InternationalJointConferenceonArtificialIntelligence,
sionandpatternrecognition,pp.1377–1385,2017. 2023.
10WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
Liao,S.,Sun,Y.,Gao,C.,KP,P.S.,Mu,S.,Shimamura,J., segmentationandobjectproposalgeneration. IEEEtrans-
andSagata,A. Weaklysupervisedinstancesegmentation actionsonpatternanalysisandmachineintelligence,39
usinghybridnetworks. InICASSP2019-2019IEEEIn- (1):128–140,2016.
ternationalConferenceonAcoustics,SpeechandSignal
Processing(ICASSP),pp.1917–1921.IEEE,2019. Ren, S., He, K., Girshick, R., and Sun, J. Faster r-cnn:
Towardsreal-timeobjectdetectionwithregionproposal
Lin, J., Shen, Y., Wang, B., Lin, S., Li, K., and Cao, L.
networks. Advances in neural information processing
Weaklysupervisedopen-vocabularyobjectdetection. In
systems,28,2015.
ProceedingsoftheAAAIConferenceonArtificialIntelli-
gence,2024.
Ren,Z.,Yu,Z.,Yang,X.,Liu,M.-Y.,Lee,Y.J.,Schwing,
A.G.,andKautz,J. Instance-aware,context-focused,and
Lin,T.-Y.,Maire,M.,Belongie,S.,Hays,J.,Perona,P.,Ra-
memory-efficientweaklysupervisedobjectdetection. In
manan,D.,Dolla´r,P.,andZitnick,C.L. Microsoftcoco:
ProceedingsoftheIEEE/CVFconferenceoncomputer
Commonobjectsincontext. InComputerVision–ECCV
visionandpatternrecognition,pp.10598–10607,2020.
2014: 13thEuropeanConference,Zurich,Switzerland,
September6-12,2014,Proceedings,PartV13,pp.740–
Schroeter, J., Sidorov, K., and Marshall, D. Weakly-
755.Springer,2014.
supervised temporal localization via occurrence count
Lin, T.-Y., Goyal, P., Girshick, R., He, K., and Dolla´r, P. learning. InInternationalConferenceonMachineLearn-
Focallossfordenseobjectdetection. InProceedingsof ing,pp.5649–5659.PMLR,2019.
the IEEE international conference on computer vision,
pp.2980–2988,2017. Seo, J., Bae, W., Sutherland, D. J., Noh, J., and Kim, D.
Objectdiscoveryviacontrastivelearningforweaklysu-
Lin,Y.,Chen,M.,Wang,W.,Wu,B.,Li,K.,Lin,B.,Liu, pervised objectdetection. InEuropeanConference on
H., and He, X. Clip is also an efficient segmenter: A ComputerVision,pp.312–329.Springer,2022.
text-drivenapproachforweaklysupervisedsemanticseg-
mentation. InProceedingsoftheIEEE/CVFConference Shen, Y., Ji, R., Wang, Y., Wu, Y., and Cao, L. Cyclic
onComputerVisionandPatternRecognition,pp.15305– guidanceforweaklysupervisedjointdetectionandseg-
15314,2023. mentation. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pp.697–
Liu,B.,Gao,Y.,Guo,N.,Ye,X.,Wan,F.,You,H.,andFan,
707,2019.
D. Utilizingtheinstabilityinweaklysupervisedobject
detection. InCVPRWorkshops,2019.
Shen,Y.,Ji,R.,Wang,Y.,Chen,Z.,Zheng,F.,Huang,F.,
andWu,Y. Enablingdeepresidualnetworksforweakly
Liu, Y., Wu, Y.-H., Wen, P., Shi, Y., Qiu, Y., and Cheng,
supervisedobjectdetection. InComputerVision–ECCV
M.-M. Leveraginginstance-,image-anddataset-levelin-
2020: 16thEuropeanConference,Glasgow,UK,August
formationforweaklysupervisedinstancesegmentation.
23–28, 2020, Proceedings, Part VIII 16, pp. 118–136.
IEEETransactionsonPatternAnalysisandMachineIn-
Springer,2020.
telligence,44(3):1415–1428,2020.
Locatello,F.,Poole,B.,Ra¨tsch,G.,Scho¨lkopf,B.,Bachem, Shen, Y., Cao, L., Chen, Z., Zhang, B., Su, C., Wu, Y.,
O.,andTschannen,M. Weakly-superviseddisentangle- Huang,F.,andJi,R. Paralleldetection-and-segmentation
mentwithoutcompromises. InInternationalConference learningforweaklysupervisedinstancesegmentation. In
onMachineLearning,pp.6348–6359.PMLR,2020. ProceedingsoftheIEEE/CVFInternationalConference
onComputerVision,pp.8198–8208,2021.
Loshchilov,I.andHutter,F. Decoupledweightdecayregu-
larization. arXivpreprintarXiv:1711.05101,2017. Sui,L.,Zhang,C.-L.,andWu,J. Salvageofsupervisionin
weaklysupervisedobjectdetection. InProceedingsofthe
Ma,J.andWang,B. Segmentanythinginmedicalimages.
IEEE/CVFConferenceonComputerVisionandPattern
arXivpreprintarXiv:2304.12306,2023.
Recognition,pp.14227–14236,2022.
Ou,J.-R.,Deng,S.-L.,andYu,J.-G. Ws-rcnn: Learningto
scoreproposalsforweaklysupervisedinstancesegmenta- Sun, G., Wang, W., Dai, J., and Van Gool, L. Mining
tion. Sensors,21(10):3475,2021. cross-imagesemanticsforweaklysupervisedsemantic
segmentation. In Computer Vision–ECCV 2020: 16th
Pont-Tuset,J.,Arbelaez,P.,Barron,J.T.,Marques,F.,and EuropeanConference,Glasgow,UK,August23–28,2020,
Malik,J. Multiscalecombinatorialgroupingforimage Proceedings,PartII16,pp.347–365.Springer,2020.
11WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
Sun,W.,Liu,Z.,Zhang,Y.,Zhong,Y.,andBarnes,N.Anal- Wang,X.,Feng,J.,Hu,B.,Ding,Q.,Ran,L.,Chen,X.,and
ternativetowsss? anempiricalstudyofthesegmentany- Liu, W. Weakly-supervised instance segmentation via
thingmodel(sam)onweakly-supervisedsemanticseg- class-agnosticlearningwithsalientimages. InProceed-
mentationproblems. arXivpreprintarXiv:2305.01586, ingsoftheIEEE/CVFConferenceonComputerVision
2023. andPatternRecognition,pp.10225–10235,2021.
Tang,P.,Wang,X.,Bai,X.,andLiu,W. Multipleinstance Xu,C.,Tao,D.,Xu,C.,andRui,Y. Large-marginweakly
detectionnetworkwithonlineinstanceclassifierrefine- supervised dimensionality reduction. In International
ment.InProceedingsoftheIEEEconferenceoncomputer conference on machine learning, pp. 865–873. PMLR,
visionandpatternrecognition,pp.2843–2851,2017. 2014.
Xu,L.,Ouyang,W.,Bennamoun,M.,Boussaid,F.,andXu,
Tang, P., Wang, X., Bai, S., Shen, W., Bai, X., Liu, W.,
D. Multi-classtokentransformerforweaklysupervised
andYuille,A. Pcl: Proposalclusterlearningforweakly
semanticsegmentation. InProceedingsoftheIEEE/CVF
supervisedobjectdetection.IEEEtransactionsonpattern
ConferenceonComputerVisionandPatternRecognition,
analysisandmachineintelligence,42(1):176–191,2018a.
pp.4310–4319,2022.
Tang, P., Wang, X., Wang, A., Yan, Y., Liu, W., Huang,
Yang,K.,Li,D.,andDou,Y. Towardspreciseend-to-end
J., and Yuille, A. Weakly supervised region proposal
weaklysupervisedobjectdetectionnetwork. InProceed-
network and object detection. In Proceedings of the
ingsoftheIEEE/CVFInternationalConferenceonCom-
European conference on computer vision (ECCV), pp.
puterVision,pp.8372–8381,2019.
352–368,2018b.
Yang, L., Wang, Y., Li, X., Wang, X., and Yang,
Tian,Z.,Shen,C.,Wang,X.,andChen,H. Boxinst: High- J. Fine-grained visual prompting. arXiv preprint
performanceinstancesegmentationwithboxannotations. arXiv:2306.04356,2023.
In Proceedings of the IEEE/CVF Conference on Com-
Yin, Y., Deng, J., Zhou, W., and Li, H. Instance mining
puter Vision and Pattern Recognition, pp. 5443–5452,
with class feature banks for weakly supervised object
2021.
detection. In Proceedings of the AAAI Conference on
Touvron,H.,Cord,M.,Douze,M.,Massa,F.,Sablayrolles, ArtificialIntelligence,volume35,pp.3190–3198,2021.
A.,andJe´gou,H.Trainingdata-efficientimagetransform-
Yin, Y., Deng, J., Zhou, W., Li, L., and Li, H. Cyclic-
ers&distillationthroughattention. InInternationalcon-
bootstraplabelingforweaklysupervisedobjectdetection.
ferenceonmachinelearning,pp.10347–10357.PMLR,
InProceedingsoftheIEEE/CVFInternationalConfer-
2021.
enceonComputerVision,pp.7008–7018,2023.
Uijlings, J. R. R., van de Sande, K. E. A., Gevers, Zeng,Z.,Liu,B.,Fu,J.,Chao,H.,andZhang,L. Wsod2:
T., and Smeulders, A. W. M. Selective search for Learningbottom-upandtop-downobjectnessdistillation
object recognition. International Journal of Com- forweakly-supervisedobjectdetection. InProceedings
puter Vision, 104:154 – 171, 2013. URL https: oftheIEEE/CVFinternationalconferenceoncomputer
//api.semanticscholar.org/CorpusID: vision,pp.8292–8300,2019.
216077384.
Zhang, H., Li, F., Liu, S., Zhang, L., Su, H., Zhu, J., Ni,
Wan,F.,Wei,P.,Jiao,J.,Han,Z.,andYe,Q. Min-entropy L.M.,andShum,H.-Y.Dino:Detrwithimproveddenois-
latentmodelforweaklysupervisedobjectdetection. In inganchorboxesforend-to-endobjectdetection,2022.
ProceedingsoftheIEEEconferenceoncomputervision
Zhang,J.,Su,H.,He,Y.,andZou,W.Weaklysupervisedin-
andpatternrecognition,pp.1297–1306,2018.
stancesegmentationviacategory-awarecenternesslearn-
ingwithlocalizationsupervision. PatternRecognition,
Wan,F.,Liu,C.,Ke,W.,Ji,X.,Jiao,J.,andYe,Q. C-mil:
136:109165,2023.
Continuationmultipleinstancelearningforweaklysuper-
visedobjectdetection. InProceedingsoftheIEEE/CVF Zhang,K.,Yuan,C.,Zhu,Y.,Jiang,Y.,andLuo,L. Weakly
ConferenceonComputerVisionandPatternRecognition,
supervisedinstancesegmentationbyexploringentireob-
pp.2199–2208,2019. jectregions. IEEETransactionsonMultimedia,2021.
Wang, X., Wang, B., Bai, X., Liu, W., and Tu, Z. Max- Zhang,X.,Feng,J.,Xiong,H.,andTian,Q.Zigzaglearning
margin multiple-instance dictionary learning. In Inter- forweaklysupervisedobjectdetection. InProceedings
nationalconferenceonmachinelearning,pp.846–854. oftheIEEEconferenceoncomputervisionandpattern
PMLR,2013. recognition,pp.4262–4270,2018a.
12WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
Zhang,Y.,Bai,Y.,Ding,M.,Li,Y.,andGhanem,B. W2f:
Aweakly-supervisedtofully-supervisedframeworkfor
objectdetection. InProceedingsoftheIEEEconference
oncomputervisionandpatternrecognition,pp.928–936,
2018b.
Zhou,Y.,Zhu,Y.,Ye,Q.,Qiu,Q.,andJiao,J.Weaklysuper-
visedinstancesegmentationusingclasspeakresponse. In
ProceedingsoftheIEEEconferenceoncomputervision
andpatternrecognition,pp.3791–3800,2018.
Zhou, Z.-H. A brief introduction to weakly supervised
learning. Nationalsciencereview,5(1):44–53,2018.
Zhu, L., Li, Y., Fang, J., Liu, Y., Xin, H., Liu, W., and
Wang,X. Weaktr: Exploringplainvisiontransformerfor
weakly-supervisedsemanticsegmentation.arXivpreprint
arXiv:2304.01184,2023a.
Zhu, L., Peng, L., Ding, S., and Liu, Z. An encoder-
decoderframeworkwithdynamicconvolutionforweakly
supervisedinstancesegmentation. IETComputerVision,
2023b.
Zhu,Y.,Zhou,Y.,Xu,H.,Ye,Q.,Doermann,D.,andJiao,J.
Learninginstanceactivationmapsforweaklysupervised
instancesegmentation. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition
(CVPR),June2019.
Zitnick,C.L.andDolla´r,P. Edgeboxes: Locatingobject
proposalsfromedges. InComputerVision–ECCV2014:
13thEuropeanConference,Zurich,Switzerland,Septem-
ber 6-12, 2014, Proceedings, Part V 13, pp. 391–405.
Springer,2014.
13WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
A.MoreDetailsofWeakSAMProposals regression losses of RoIs have different number distribu-
tionscomparedtotheclassificationlosses. However,they
            exhibitsimilarerrorratecurves. Thisobservationfurther
demonstratesthenecessityofRoIdropregularizationwith
           
aregressionthresholdτ .
reg
           
          C.MoreDetailsofQueryDropRegularization
         
BecauseDINO(Zhangetal.,2022)employsFocalloss(Lin
        et al., 2017) as the classification loss, queries associated
                                       
withbackgroundclassestendtohavehigherpredictedprob-
Figure4.Thecosinesimilarityamongthefeaturesofproposals, abilities and lower losses. This results in the inadvertent
i.e.,LeftforSelectiveSearchproposalsandRightforWeakSAM- omissionofmostforegroundcategoryquerieswhendirectly
proposals. For a single image from PASCAL VOC 2007, we droppingqueries. To mitigatethisissue, ourfirst stepin-
randomlysampled200proposalfeaturestocalculatetheirsimilar- volvesnormalizingtheunweightedFocalloss,whichises-
ity. sentiallythebinarycross-entropyloss,forbothforeground
and background queries within each training batch. Nor-
Wefurtheranalyzetheproposalsimilarityinthedifferent
malizing at the batch level broadens the sampling scope
weakly-supervisedobjectdetection(WSOD)proposals,as
from a single image to the size of the batch. In the sec-
showninFig.4. Werandomlysample200proposalfeatures
ondstep,queriesaredroppedbasedontheirlossranking
eachfromSelectiveSearch(Uijlingsetal.,2013)proposals
post-normalization.Thisapproachavoidsmakingthemodel
andWeakSAM-proposals, andthencomputetheircosine
convergeslowlyduetothedroppingofthemostforeground
similarity,respectively. Pleasenotethatallthefeaturesare
queries.
output by the RoI pooling layer. It can be seen that the
featuresfromSelectiveSearchtendtohavehighersimilarity
withotherones. Incontrast,thefeaturesfromWeakSAM- D.EfficiencyComparison
proposalsshowlowersimilarity,whichusuallymeansithas
Tofurtheranalyzetheefficiencyimprovementbroughtby
lessoverlapandredundancy.
the WeakSAM, we present the efficiency comparison be-
tweenSelectiveSearch(Uijlingsetal.,2013)andourWeak-
B.MoreDetailsofRoIDropRegularization
SAMonamachinewith4GPUcards,asshowninTable6.
OurWeakSAMreducesthenumberofproposalsby89.4%,
the proposal generation time by 65.5%, the WSOD net-
worktrainingtimeby43.8%,andtheGPUmemorycostby
    68.2%. Theresultsdemonstratethesignificantefficiency
    
improvementbroughtbytheproposedWeakSAM.
   
        
E.AdditionalQuantitativeResults
   
   
We present the comparison on PASCAL VOC 2007
   
    trainvalsetintermsofCorLoc,asshowninTable7.Itcan
   
 1 X P E H U  R I  5 R , V beseenthattheWeakSAMachievesthe13.9%and14.1%
     ( U U R U  5 D W H     CorLoc improvements on OICR and MIST, respectively.
TheWeakSAM(OICR)outperformstheWSOD-CBL(Yin
   
 
                                            et al., 2023) by 11.1% CorLoc. The results demonstrate
 1 R U P D O L ] H G  5 H J U H V V L R Q  / R V V
thesignificanteffectivenessimprovementbroughtbyour
WeakSAM.
Figure5.Therelationshipbetweenthenormalizedregressionloss,
thecorrespondingnumberofRoIs,andthecorrespondingerror
rate.TheresultsareobtainedfromtrainingtheFaster-RCNN(Ren F.AdditionalAblationStudies
etal.,2015)usingPASCALVOC2007pseudogroundtruth(PGT)
F.1.ImprovementsofClassificationMethods
inthepreliminarytrainingstage.
Tofurtheranalyzetheimpactofmethodsthatgenerateclas-
Wefurtherpresenttherelationshipbetweenthenormalized sification clues, we replaced WeakTr in WeakSAM with
regression loss, the corresponding number of RoIs, and MCTformerandCLIP-ES.AsindicatedinTable8,Weak-
the corresponding error rate in Fig. 5. It shows that the
14
 V , R 5  I R  U H E P X 1
 H W D 5  U R U U (WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
Table6.EfficiencycomparisonbetweenSelectiveSearchandourWeakSAMduringthetrainingonthePASCALVOC2007.‘Num.’is
thenumberofproposals,‘T ’isthetimeconsumptionforgeneratingproposals,‘T ’isthetimeconsumptionfortrainingthe
Proposals WSOD
WSODnetwork,i.e.,MIST(Renetal.,2020),and‘M ’istheGPUmemorycostforeachGPUcard.
WSOD
Num. T T M
Proposals WSOD WSOD
SS(Uijlingsetal.,2013) 2001 11.6hrs 16hrs 17810MiB
Ours 213-89.4% 4hrs-65.5% 9hrs-43.8% 5667MiB-68.2%
Table7. ComparisononPASCALVOC2007trainvalsetintermsofCorLoc(%)withmulti-scaletesting.
Methods Sup. Proposal CorLoc
WSDDN(Bilen&Vedaldi,2016a) EB(Zitnick&Dolla´r,2014) 53.5
Yangetal.(Yangetal.,2019) SS(Uijlingsetal.,2013) 68.0
C-MIL (Wanetal.,2019) SS 65.0
C-MIDN(Gaoetal.,2019) SS 53.5
WSOD2(Zengetal.,2019) SS 69.5
I
CASD(Huangetal.,2020) SS 70.4
OD-WSCL(Seoetal.,2022) SS 69.8
WSOD-CBL(Yinetal.,2023) SS 71.8
WSOVOD(Linetal.,2024) LO-WSRPN+SAM 77.2
WSOVOD‡ LO-WSRPN+SAM 80.1
OICR(Tangetal.,2017) SS 60.6
I
WeakSAM(OICR) WeakSAM 74.5+13.9
MIST(Renetal.,2020) SS 68.8
I
WeakSAM(MIST) WeakSAM 82.9+14.1
SAM(MCTformer)achievesa1.6%higherRecall(IoU=90) mance,whereasdroppingbothtypesofqueriesresultsina
thanWeakSAM(WeakTr). Furthermore,WeakSAM(CLIP- slightdecreaseinperformance. Wemaintaintheviewpoint
ES) records increases of 0.6% and 2.4% in Recall over that dropping more background queries may also lead to
WeakSAM(WeakTr)atIoUthresholdsof75and90,respec- slowerconvergence. Consequently,wechoosetodroponly
tively. TheseresultsdemonstratetheversatilityoftheWeak- Query toachievebetterperformance.
things
SAMproposal-generatingmethodacrossdifferentclassifi-
Wefurtheranalyzetheimpactofclassificationthresholdτ
cationmethods. Pleasenotethatallclassificationmethods
inquerydropregularization,asshowninTable11. Quan-
employed in this study are CAM networks from weakly-
titative results demonstrate that 90 is the best percentile
supervisedsemanticsegmentation(WSSS)methods. Since
classificationthreshold.
thesenetworksaretypicallywell-tunedonspecificdatasets,
such as PASCAL VOC 2012 and COCO 2014, they are
adeptatprovidingrichclassificationclues. G.AdditionalVisualizationResults
Fig.6comparestheSelectiveSearchproposalswiththose
F.2.AblationStudiesforRoIDropRegularization
generatedbyWeakSAM.TheWeakSAM-proposalsexhibit
To further analyze the impact of the regression threshold lessredundancythanSelectiveSearchproposals. Fig.7con-
andclassificationthresholdinRoIdropregularization,we trasts the Top-1 PGT with adaptive PGT, demonstrating
conduct experiments as shown in Table 9. It is observed that adaptive PGT generation captures a greater number
that the best regression threshold τ and classification ofobjects,whichmightbemissedbytheTop-1approach.
reg
threshold τ for RoI drop regularization is 1.0 and 4.0, Additionally, adaptive PGT can be seamlessly integrated
cls
respectively. togeneratepseudoinstancelabels. Fig.8presentstheob-
jectdetectionresultsusingWeakSAM(MIST),showingits
F.3.AblationStudiesforQueryDropRegularization capabilitytoaccuratelycaptureentireobjectswithoutgener-
atingexcessivenoisyboundingboxes. InFig.9,theinstance
Toanalyzetheimpactofdroppingqueriescorresponding segmentationresultsofWeakSAM-Mask2Formerretraining
to foreground categories Query things and background cat- areshowcased. Theresultsindicateeffectivesegmentation
egories Query bkg, we conduct ablations as shown in Ta- ofentireinstanceswithanotablereductioninoverlapping
ble 10. Experimental results indicate that dropping only segments.
theforegroundqueries(Query )leadstothebestperfor-
things
15WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
Table8.AblationstudiesforclassificationmethodsthatgenerateWeakSAMqueriesonPASCALVOC2007trainvalset.Weevaluate
theaveragenumberofproposalsandRecall.WepresenttheresultsofSelectiveSearch(Uijlingsetal.,2013)atthefirstrowasabaseline.
Recall
CLSMethods Num.
IoU=0.50 IoU=0.75 IoU=0.90
None 2001 92.6 57.7 19.2
WeakTr(Zhuetal.,2023a) 213 95.6 75.0 42.1
MCTformer(Xuetal.,2022) 173 93.2 74.8 43.7
CLIP-ES(Linetal.,2023) 205 93.8 75.6 44.5
Figure6. VisualizationoftheproposalsboxesonthePASCALVOC2007trainvalset.
H.MoreImplementationDetails
Table9.Ablationstudyfortheregressionthresholdandclassifica- ForAlgorithm1,wesetthekernelsizekto128andactiva-
tionthresholdinRoIdropregularizationintermsofAP onthe tionthresholdτ to0.9followingdefaultparametersfrom
50
PASCALVOC2007testset. WeakTr(Zhuetal.,2023a). AndforAlgorithm2,wefol-
lowthedefaultmannerssimilartoSoS-WSOD(Suietal.,
(a)Ablationstudyforthere- (b)Ablationstudyfortheclas-
2022),inwhichscorethresholdτ issetto0.3,andoverlap
gressionthreshold. sificationthreshold. s
thresholdτ issetto0.85.
o
τ 0.8 1.0 1.2 τ 3.0 4.0 5.0
reg cls ForFasterR-CNN(Renetal.,2015)retraining,weadopt
AP 71.0 71.8 71.3 AP 71.2 71.8 71.1
50 50 thesametrainingstrategyandhyper-parametersastheFully-
supervisedones. ForDINO(Zhangetal.,2022)retraining,
weusealearningrateof9e-5withtheAdamW(Loshchilov
Table10.Ablation studies for query drop regularization on the &Hutter,2017)optimizer,andamaxepochof14. More-
PASCALVOC2007testset. over,weapplymulti-scaleaugmentationandhorizontalflips
inbothtrainingandtesting.
Baseline Query Query AP
things bkg 50
ForimplementationsofMaskR-CNN(Heetal.,2017)and
(cid:33) 72.8
Mask2Former(Chengetal.,2022),wefollowtheirdefault
(cid:33) 73.4+0.6 hyper-parameters.
(cid:33) (cid:33) 73.3+0.5
Table11.Ablationstudyfortheclassificationthresholdinquery
dropregularizationintermsofAP onthePASCALVOC2007
50
testset.
τ (%) 100 90 80
AP 72.8 73.4 71.8
50
16
egamIlanigirO
SS
MASkaeWWeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
Figure7.VisualizationofthepseudogroundtruthboxesandpseudoinstancelabelsonthePASCALVOC2012trainaugset.
Figure8. Visualizationoftheweakly-supervisedobjectdetectiononthePASCALVOC2007testset.
Figure9. Visualizationoftheweakly-supervisedinstancesegmentationonthePASCALVOC2012valset.
17
TGP
1-poT
TGP
evitpadA
.tsnI
oduesP
hturTdnuorG
TSIM
MASkaeW
hturTdnuorG
MIC
MASkaeW