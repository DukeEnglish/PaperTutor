Extended Fiducial Inference: Toward an Automated Process of
Statistical Inference
Faming Liang∗, Sehwan Kim†, and Yan Sun‡
Abstract
While fiducial inference was widely considered a big blunder by R.A. Fisher, the goal he initially
set–‘inferringtheuncertaintyofmodelparametersonthebasisofobservations’–hasbeencontinually
pursued by many statisticians. To this end, we develop a new statistical inference method called ex-
tended Fiducial inference (EFI). The new method achieves the goal of fiducial inference by leveraging
advanced statistical computing techniques while remaining scalable for big data. EFI involves jointly
imputing random errors realized in observations using stochastic gradient Markov chain Monte Carlo
and estimating the inverse function using a sparse deep neural network (DNN). The consistency of
the sparse DNN estimator ensures that the uncertainty embedded in observations is properly prop-
agated to model parameters through the estimated inverse function, thereby validating downstream
statisticalinference. ComparedtofrequentistandBayesianmethods,EFIofferssignificantadvantages
in parameter estimation and hypothesis testing. Specifically, EFI provides higher fidelity in param-
eter estimation, especially when outliers are present in the observations; and eliminates the need for
theoretical reference distributions in hypothesis testing, thereby automating the statistical inference
process. EFI also provides an innovative framework for semi-supervised learning.
Keywords: Complex Hypothesis Test, Markov chain Monte Carlo, Semi-Supervised Learning,
Sparse deep learning, Uncertainty Quantification
1 Introduction
Statistical inference is a fundamental task in modern data science, which studies how to propagate the
uncertainty embedded in data to model parameters. During the past century, frequentist and Bayesian
methods have evolved as two major frameworks of statistical inference. However, due to some intrinsic
issues (see Section 2), these methods may lack one or more features — such as fidelity, automaticity, and
scalability — necessary for performing statistical inference on complex models in modern data science.
∗Correspondence author: Faming Liang, email: fmliang@purdue.edu, Department of Statistics, Purdue University, West
Lafayette, IN 47907, USA; † Department of Population Medicine, Harvard Medical School/ Harvard Pilgrim Health Care
Institute,Boston,MA02215,USA;‡DepartmentofBiostatistics,Epidemiology,andInformatics,UniversityofPennsylvania,
Philadelphia, PA 19104, USA.
1
4202
luJ
13
]LM.tats[
1v22612.7042:viXraSpecifically, the frequentist methods often estimate model parameters using the maximum likelihood ap-
proach and test hypotheses by comparing a test statistic with a known theoretical reference distribution.
It is well-known that the maximum likelihood estimator (MLE) can be significantly influenced by out-
liers, which reduces the fidelity of parameter estimates. For hypothesis testing, the required theoretical
reference distribution is test statistic-dependent, making statistical inference difficult to automate. Al-
though this issue can be partially mitigated by asymptotic normality, the sample size required to achieve
asymptotic normality can be very large especially in high-dimensional scenarios. For Bayesian methods,
their dependence on prior distributions has been a subject of criticism throughout the history of Bayesian
statistics, often raising concerns about their fidelity.
As a possible way to overcome the drawbacks of frequentist and Bayesian methods, the fiducial
method has been proposed by R.A. Fisher in a series of papers starting from 1930s (see [94] for a review),
which quantifies uncertainty of model parameters by the so-called fiducial distribution. Fisher originally
introduced this method, motivated by the observation that pivotal quantities permit uncertainty quan-
tificationforanunknownparameterinthesamewayasthefrequentistmethod. However, heencountered
difficulties in extending this pivotal quantity-based method to models with multiple parameters. It is
worth noting that for some models, the fiducial distribution is the same as the posterior distribution
derived with Jeffreys’ prior, but Fisher argued that the logic behind the Bayesian method is unacceptable
because the use of prior is unjustifiable [34]. This argument also distinguishes the fiducial method from
objective Bayesian methods, even though non-informative priors are used in the latter.
Fiducial inference was generally regarded as a big blunder by Fisher. However, the goal he initially
set, making inference about unknown parameters on the basis of observations [29], has been continually
pursued by many statisticians. Building on early works in sparse deep learning [49, 83, 84] and adaptive
stochastic gradient Markov chain Monte Carlo (MCMC) [18, 52, 20], this paper develops a new statistical
inference framework called the extended fiducial inference (EFI), which achieves the initial goal of fiducial
inference while possessing necessary features like fidelity, automaticity, and scalability that are essential
for statistical inference in modern data science.
Our contributions in this work are in three folds:
• Development of the EFI framework: We develop a scalable and effective method for conducting
fiducial inference. Our method involves jointly imputing the random errors contained in the data
and estimating the inverse function for the model parameters. It ensures that the uncertainty
embedded in the data is properly propagated to the model parameters through the estimated
inverse function, thereby validating downstream statistical inference. Compared to frequentist and
Bayesian methods, EFI provides higher-fidelity inference, especially in the presence of outliers.
• Innovativestatisticalframeworkforsemi-supervisedlearning: EFIprovidesaninnovativeframework
of statistical inference for missing data problems, especially in scenarios where missing values are
2present in response data, as encountered in semi-supervised learning problems. This innovation
can have profound implications for modern data science, particularly in biomedical research where
obtaining labeled data can be costly.
• Automaticityofstatisticalinference: EFIenablesautomaticstatisticalinferenceforcomplexmodels,
atleastconceptually. Itcanbeasflexibleasfrequentistmethodsinparameterestimation. However,
unlike frequentist methods, it eliminates the requirement for theoretical reference distributions
(including asymptotic normality as a special case) in hypothesis testing. Compared to Bayesian
methods, EFI eliminates the requirement for prior distributions, which can vary depending on the
problem or analyst’s choice, thus enhancing the fidelity of statistical inference.
In summary, with the aid of advanced statistical computing techniques, EFI holds the potential to
significantly advance modern data science. Specifically, it provides higher-fidelity inference, introduces
an innovative statistical framework for semi-supervised learning, and automates statistical inference for
complex models.
The remaining part of the paper is organized as follows. Section 2 distinguishes the concepts of
frequentist, Bayesian and EFI from the perspective of structural inference [30, 31]. Section 3 provides a
theoretical framework for EFI. Section 4 describes an effective algorithm for performing EFI and studies
its theoretical properties. Section 5 presents some numerical examples validating EFI as a statistical
inference method. Section 6 presents applications of EFI on semi-supervised learning. Section 7 presents
applications of EFI for complex hypothesis tests. Section 8 concludes the paper with a brief discussion.
2 Frequentist, Bayesian, and Extended Fiducial Inference
This section elaborates the conceptual difference between frequentist, Bayesian, and EFI methods from
the perspective of structural inference [30, 31]. Consider a regression model:
Y = f(X,Z,θ), (1)
where Y ∈ R and X ∈ Rd represent the response and explanatory variables, respectively; θ ∈ Rp
represents the vector of unknown parameters; and Z ∈ R represents a scaled random error that follows
a known distribution denoted by π (·). Suppose that a random sample of size n has been collected from
0
the model, denoted by {(y ,x ),(y ,x ),...,(y ,x )}, and our goal is to quantify uncertainty of θ based
1 1 2 2 n n
on the collected samples (also known as observations).
In the view of structural inference [30, 31], we can express the observations {(y ,x ),(y ,x ),...,
1 1 2 2
(y ,x )} in the data generating equation as follow:
n n
y = f(x ,z ,θ), i = 1,2,...,n. (2)
i i i
3This system of equations consists of n+p unknowns, namely, {θ,z ,z ,...,z }, while there are only n
1 2 n
equations. Therefore, the values of θ cannot be uniquely determined by the data-generating equation,
which gives the source of uncertainty of the parameters as illustrated by Figure 1. For convenience, we
will refer to z ,z ,...,z as latent variables in the context of data-generating equations, while still calling
1 2 n
them random errors when appropriate.
Figure 1: Illustration for the source of uncertainty of model parameters: the space that θ can take values
becomes smaller and smaller as the sample size increases.
Frequentist Methods The frequentist methods treat θ as fixed unknowns. To solve for θ from the
undetermined system (2), they often impose a constraint on the system such that the latent variables
can be dismissed and θ can be uniquely determined. For example, the maximum likelihood estimation
method works under the constraint that the joint likelihood function of the samples, or equivalently, the
likelihood of {z ,z ,...,z }, is maximized. As an illustration, let’s consider the linear regression model:
1 2 n
y = xTβ+σz , i = 1,2,...,n, (3)
i i i
whereβ ∈ Rp−1 istheregressioncoefficientvector, σ ∈ R+ isapositivescaleparameter, andz ,z ,...,z
1 2 n
arei.i.dstandardGaussianrandomvariables. Forthismodel,themaximumlikelihoodestimationmethod
is to solve for θ := (β,σ) subject to the constraint
n n
(cid:89) (cid:89)
ϕ(z ) = max ϕ(z˜), (4)
i i
i=1
(z˜1,z˜2,...,z˜n)∈Rn
i=1
where ϕ(·) denotes the standard Gaussian density function. As it turns out, this is equivalent to solving
the optimization problem:
max(cid:88)n logϕ(cid:18) y i−xT
i
β(cid:19)
, (5)
(β,σ) σ
i=1
and the resulting estimator is given by
n
1 (cid:88)
βˆ = (XTX )−1XTY , σˆ2 = (y −xTβˆ)2, (6)
n n n n n i i
i=1
4where Y = (y ,...,y )T and X = (x ,x ,...,x )T.
n 1 n n 1 2 n
Anotherexampleoffrequentistmethodsismomentestimation,whichsolvesforθ undertheconstraint
thatthesamplemomentsareequaltothepopulationmoments. Forthemodel(2),themomentconstraint
can be expressed as
n n (cid:90)
(cid:88) (cid:88)
yk = [f(x ,z,θ)]kπ (z)dz, i = 1,2,...,p,
i i 0
i=1 i=1
where the latent variables z ,z ,...,z are dismissed via integration.
1 2 n
Let θˆ denote an estimator of θ. The frequentist method assesses the uncertainty of θ in an uncondi-
tional mode, where the distribution of θˆ is derived based on the preassumed distribution π (z) instead
0
of the random errors z ,z ,...,z realized in the observations. For example, considering the MLE given
1 2 n
in (6), one can derive that βˆ ∼ N(β,σ2(XTX )−1) and nσˆ2 ∼ χ2(n−p+1) based on the preassumed
n n σ2
Gaussian distribution for the random errors. This unconditional mode makes the inference procedure
challenging to automate; in particular, the distribution of θˆ is problem-dependent and generally difficult
to derive. Additionally, the constraints used for the solution of θ might be violated by observations. For
example, when outliers exist, the maximum likelihood constraint might not hold, and the resulting MLE
can significantly differ from the true value of θ. Refer to Section 5.4 for numerical examples.
Bayesian Methods In contrast to frequentist methods, Bayesian methods treat θ as random variables
and circumvent the issue of latent variables by adopting a conditional approach. Specifically, Bayesian
methodsassumethatθ followsapriordistribution,andquantifyuncertaintyofθ basedontheconditional
distribution (also known as the posterior distribution):
(cid:81)n
p(y |x ,θ)π(θ)
π(θ|(y ,x ),(y ,x ),...,(y ,x )) = i=1 i i , (7)
1 1 2 2 n n (cid:82) (cid:81)n
p(y |x ,θ)π(θ)dθ
i=1 i i
where p(y |x ,θ) denotes the likelihood function of y , and π(θ) represents the prior distribution of θ.
i i i
The dependence of the inference on the prior distribution has been subject to criticism throughout the
history of Bayesian statistics, as the prior distribution introduces subjective elements that may affect the
fidelity of statistical inference.
Extended Fiducial Inference Let Z := {z ,z ,...,z } denote the collection of latent variables,
n 1 2 n
and let G(Y ,X ,Z ) denote an inverse function for the solution of θ in the system (2). As a general
n n n
computational procedure, EFI jointly imputes Z and estimates G(Y ,X ,Z ), and then quantifies the
n n n n
uncertainty of θ based the estimated inverse function and the imputed values of Z , where the estimated
n
inverse function serves as an uncertainty propagator from Z to θ. Technically, EFI approximates
n
G(Y ,X ,Z )usingasparsedeepneuralnetwork(DNN)[49,83,84],andemploysanadaptivestochastic
n n n
gradient MCMC algorithm [18, 52] to jointly simulate the values of Z and estimate the parameters of
n
the sparse DNN.
5While treating θ as fixed unknowns, EFI distinguishes itself from frequentist methods by conducting
inference for θ in a conditional mode and sidestepping the imposition of any constraints on the latent
variables. Additionally, unlike Bayesian methods, EFI eliminates the need for placing a prior distribution
on θ. In summary, EFI aims to make statistical inference of θ based solely on observations.
Related Works During the past several decades, there have been quite a few works on statistical
inference with the attempt to achieve the goal of fiducial inference, although some gaps remain. These
works are briefly reviewed in what follows.
Generalized Fiducial Inference (GFI). Like EFI, GFI [34, 35, 36, 54, 64] also attempts to solve the data
generating equation, but employs an acceptance-rejection procedure similar to the approximate Bayesian
computation (ABC) algorithm [3]. As an illustration, let’s consider model (3), for which the acceptance-
rejection procedure consists of the following steps:
(a) (Proposal) Generate Z˜ = (z˜ ,z˜ ,...,z˜ )T from the Gaussian distribution N(0,I ).
n 1 2 n n
(b) (θ-fitting) Find the best fitting parameters θ˜ = argmin ∥Y −X β−σZ˜ ∥, where ∥·∥ denotes
θ n n n
an appropriate norm, and compute the fitted value Y˜ = X β˜ +σ˜Z˜ .
n n n
(c) (Acceptance-rejection) Accept θ˜ if ∥Y −Y˜ ∥ ≤ ϵ for some pre-specified small value ϵ, and reject
n n
otherwise.
Subsequently, statistical inference is made based on the accepted samples of θ˜. However, as n increases,
this procedure can become extremely inefficient due to its decreasing acceptance rate.
As a potential solution to resolving this computational issue, the limiting distribution of accepted θ˜
(as ϵ → 0) was derived in [35, 36, 54]. However, as shown in [36], the limiting distribution depends on
the norm used in the above procedure. Furthermore, for many problems, direct simulation of the limiting
distributionmightbechallenging,asshownin[54],whichinvolvesthecalculationofthedeterminantofan
n×nmatrixateachiteration. Quiterecently, [44]proposedreplacingtheθ-fittingstepoftheacceptance-
rejection procedure with a mapping G˜ : (Y ,X ,Z˜ ) → θ˜ pre-learned using a DNN. However, this
n n n
replacement cannot improve the acceptance rate of θ˜, since Z˜ is still proposed from an independent
n
trial distribution. Other concerns about the replacement include the consistency of the DNN estimator
and its difficulty in dealing with cases where X and Y contain missing data. Compared to GFI, EFI
n n
provides a more feasible computational scheme for conducting fiducial inference, in addition to some
conceptual differences in defining the fiducial distribution as discussed later.
Structural Inference. Fraser [30, 31] introduced the concept of modeling the data as a function of param-
eters and random errors through a structural equation (also known as data generating equation), under
which statistical inference would be conditioned on the realized random errors. The structural infer-
ence approach has successfully addressed some difficulties suffered by the pivotal quantity-based fiducial
6method. In particular, it avoids the issues of improper normalization [80] and non-uniqueness [25, 61].
However, like the Bayesian method, the structural inference method can suffer from the marginalization
paradox[14]thatcancauseinconsistencyofinference. Itisimportanttonotethatthestructuralequation
concept has led to a fruitful framework for statistical inference. Both GFI and EFI are developed based
on it. However, EFI differs significantly from structural inference in its treatment of θ. EFI regards θ as
fixed unknowns, whereas structural inference treats θ as variables. As a result, EFI successfully sidesteps
the marginalization paradox like a frequentist method. In EFI, θ can be determined only in the limit
n → ∞, where the inverse function G(Y ,X ,Z ) derived with finite samples can be understood as a
n n n
stochastic estimator of θ with a random component formed by Z .
n
TheDempster–Shafer theory(seee.g., [16], [77], and[17])andtheinferential model(seee.g., [57], [59],
and [60]) provide interesting frameworks for statistical reasoning with uncertainty. However, they are not
primarily concerned with fiducial inference in the form Fisher conceived. The inferential model method
avoids imposing any constraints on the latent variables Z but instead conducts inference for θ in an
n
unconditional mode, as discussed in [58]. It achieves this by working with a low-dimensional association
model, which is built upon the sufficient or summary statistics for θ and includes only a limited number
of latent variables. Leveraging this association model, it subsequently constructs a confidence set for θ
using the Dempster-Shafer theory by considering a set of plausible random errors pre-constructed for the
association model in an unconditional mode. For many statistical models, it yields the same confidence
set as the maximum likelihood estimation method. To maintain conciseness of this review, we omit
detailed descriptions for them.
3 Extended Fiducial Inference
3.1 Extended Fiducial Distribution
Before introducing the EFI method, we first define the extended fiducial distribution (EFD) as a confi-
dence distribution (CD) estimator [90] of b(θ), where b(·) is a function of interest. Let’s revisit the data
generating equation (2) and begin by making several assumptions.
Assumption 1 There exists an inverse function G : Rn×Rn×d×Rn → Rp:
θ = G(Y ,X ,Z ). (8)
n n n
In this context, “inverse” implies that if (X ,Y ,Z ) satisfies Y = f(X ,Z ,θ) for some θ, then
n n n n n n
G(Y ,X ,Z ) = θ follows. From the perspective of parameter estimation, Assumption 1 implies that
n n n
the parameters are identifiable given the random error-augmented data {Y ,X ,Z }. This is generally
n n n
true when n ≥ p, as in this case the system (2) has no more unknowns than the number of equations
by considering Z as known. For the case p > n, we recommend reducing the dimension of the problem
n
7through an application of a model-free sure independence screening procedure. More discussions on this
issue can be found at the end of the paper.
It is worth noting that the inverse function is not necessarily constructed using all n samples. For
example, it can be simply constructed by solving any p equations in (2) for θ. This raises an issue
about non-uniqueness of G(·). In what follows, we will study how the non-uniqueness of G(·) impacts
the statistical inference for the unknowns Z and θ.
n
For a given inverse function, we define an energy function:
U (z) := U(Y ,X ,z,G(·)).
n n n
To ensure proper inference for the unknowns, the energy function U (·) needs to satisfy certain regularity
n
conditions as outlined in Assumptions 2-4.
Assumption 2 The energy function U (·) is non-negative, min U (z) exists and equals 0, and U (z) =
n z n n
0 if and only if Y = f(X ,z,G(Y ,X ,z)).
n n n n
Let Z denote the zero-energy set
n
Z = (cid:8) z ∈ Rn : U (z) = 0(cid:9) .
n n
Lemma 3.1 If Assumptions 1-2 hold, then the zero-energy set Z is invariant to the choice of G(·).
n
Proof: Suppose that there exist two inverse functions G (·) and G (·). Let Z(1) and Z(2) denote their
1 2 n n
respective zero-energy sets. For any z ∈ Rn, if z ∈ Z(1) , then Y = f(X ,z,G (Y ,X ,z)) holds by
n n n 1 n n
Assumption 2. Let θ˜ = G (Y ,X ,z). Hence, (Y ,X ,z) satisfies the data generating equation (2)
1 n n n n
with the parameter θ˜.
SinceG (·)isalsoaninversefunctionforthedatageneratingequation,wehaveG (Y ,X ,z) = θ˜ by
2 2 n n
(2)
Assumption 1. This implies Y = f(X ,z,G (Y ,X ,z)), and thus z ∈ Z according to Assumption
n n 2 n n n
(1) (2) (2) (1) (1) (2)
2. That is, Z ⊆ Z holds. Vice versa, we can show Z ⊆ Z . Therefore, Z = Z and the
n n n n n n
zero-energy set is invariant to the choice of the inverse function. □
Let p∗(z|Y ,X ) denote the extended fiducial density function of Z on Z . To properly define
n n n n n
p∗(z|Y ,X ), we adopt a limiting way. More precisely, we first define the conditional distribution
n n n
(cid:26) (cid:27)
U (z)
pϵ(z|X ,Y ) ∝ exp − n π⊗n(z), (9)
n n n ϵ 0
whereϵ > 0representsthetemperature,andπ⊗n(z) = π (z )×π (z )×···×π (z )servesasthemarginal
0 0 1 0 2 0 n
distribution in the construction of this conditional distribution. Then, we define p∗(z|Y ,X ) as the
n n n
limit
p∗ = limpϵ. (10)
n n
ϵ↓0
This type of convergence has been studied in [39]. Specifically, the convergence can be studied in two
cases: (a) Π (Z ) > 0 and (b) Π (Z ) = 0, where Π (·) denotes a probability measure on (Rn,R) with
n n n n n
R being the Borel σ-algebra and the corresponding density function given by π⊗n.
0
83.1.1 Case (a): Π (Z ) > 0
n n
For this case, we follow [39] to further assume that U (z) satisfies:
n
Assumption 3 Π (U (z) < a) > 0 for any a > 0.
n n
Then, following Proposition 2.2 of [39], it can be shown that the limiting probability measure of pϵ exists
n
and is uniformly distributed on Z with respect to Π . This is summarized in the following Theorem:
n n
Theorem 3.1 If Assumptions 1-3 hold and Π (Z ) > 0, then p∗(z|X ,Y ) is invariant to the choice
n n n n n
of the inverse function G(·) and the energy function U (·), and it is given by
n
dP∗(z|X ,Y ) 1
n n n = π⊗n(z), z ∈ Z , (11)
dz Π (Z ) 0 n
n n
where P∗ represents the cumulative distribution function (CDF) corresponding to p∗.
n n
The proof of Theorem 3.1 follows Proposition 2.2 of [39] and Lemma 3.1 directly, and it is thus
omitted. An example of this case is the logistic regression as discussed in Section §4.2 of the supplement,
for which the energy function is defined as
n
(cid:88) (cid:16) (cid:17)
U (z) = ρ (z −xTG(Y ,X ,Z ))(2y −1) , (12)
n i i n n n i
i=1
where z ,z ,...,z i ∼id Logistic(0,1) with the CDF given by F(z) = 1/(1+e−z), and ρ(·) is the ReLU
1 2 n
function: ρ(s) = s if s > 0 and 0 otherwise.
3.1.2 Case (b): Π (Z ) = 0
n n
For this case, we assume that Z forms a manifold in Rn with the highest dimension p. Following [39],
n
by the tubular neighborhood theorem [63], we can decompose z ∈ Z as follows:
n
z = m(u ,u ,...,u )+t N(1)+···+t N(n−p), (13)
1 2 p 1 n−p
where m(u ,u ,...,u ) is local coordinates, and N(1),...,N(n − p) are normalized smooth normal
1 2 p
vectors perpendicular to Z . Let t = (t ,t ,...,t )T. In addition to Assumptions 1-3, we assume the
n 1 2 n−p
following conditions hold:
Assumption 4
(i) There exists a > 0 such that {U (z) ≤ a} is compact.
n
(ii) π⊗n(z) is continuous, and U (z) ∈ C3(Rn) is three-time continuously differentiable.
0 n
(iii) Z has finitely many components and each component is a compact smooth manifold with the highest
n
dimension p.
9(iv) π⊗n(z) is not identically zero on the p-dimensional manifold, and det(∂2U(z)) ̸= 0 for z ∈ Z .
0 ∂t2 n
Lemma 3.2 (Theorem 3.1; [39]) If Assumptions 1-4 hold, then the limiting probability measure p∗ con-
n
centrates on the highest dimensional manifold and is given by
dP∗(z|X ,Y ) π⊗n(z)(cid:0) det(∇2U (z))(cid:1)−1/2
n n n (z) = 0 t n , z ∈ Z , (14)
dν (cid:82) π⊗n(z)(cid:0) det(∇2U (z)(cid:1)−1/2 dν n
Zn 0 t n
where ν is the sum of intrinsic measures on the p-dimensional manifold in Z .
n
Lemma 3.2 is a restatement of Theorem 3.1 of [39] and its proof is thus omitted. We note that
the distribution P∗ can also be derived using the co-area formula (see e.g., [24], section 3.2.12; [19],
n
Proposition 2; [54], Theorem 1) under similar conditions.
Given the inverse function G(·), we define the parameter space
Θ = {θ ∈ Rp : θ = G(Y ,X ,z),z ∈ Z },
n n n
which represents the set of all possible values of θ that G(·) takes when z runs over Z . Then for any
n
function b(θ), its EFD associated with the inverse function G(·) can be defined as follows:
Definition 3.1 [EFD of b(θ)] Consider the data generating equation (2) and an inverse function θ =
G(Y ,X ,z). For any function b(θ) of interest, its EFD associated with the inverse function G(·) is
n n
defined as
(cid:90)
µ∗ n(B|Y n,X n) = dP n∗(z|Y n,X n), for any measurable set B ⊂ Θ, (15)
Zn(B)
where Z (B) = {z ∈ Z : b(G(Y ,X ,z)) ∈ B}, and P∗(z|Y ,X ) is given by (14).
n n n n n n n
Essentially, b(G(Y ,X ,Z )) can be considered as an estimator of b(θ), and Eq. (15) represents the
n n n
CD estimator of b(θ) associated with the inverse function G(·). Here we would like to emphasize that
viewing µ∗ as a distribution function of b(θ) (i.e., regarding θ as a variable) is not appropriate, as in this
n
context it will easily lead to a Bayesian approach for jointly simulating of (θ,Z ). The resulting sample
n
pair (θ,Z ) will break the inverse mapping (8) and, in consequence, the uncertainty of Z will not be
n n
properly propagated to θ.
For an effective implementation of EFI, we propose the following importance resampling procedure:
(a) (Manifold sampling) For any given inverse function G˜(·), simulate M samples, denoted by S =
M
{z ,z ,...,z }, from π⊗n(z) subject to the constraint U (z) = 0. This can be done using a
1 2 M 0 n
constrained Monte Carlo algorithm such as constrained Hamiltonian Monte Carlo [10, 72].
(b) (Weighting) Calculate the importance weight ω = (cid:0) det(∇2U (z ))(cid:1)−1/2 for each sample z ∈ S
i t n i i
using an inverse function G(·) of interest.
10(c) (Resampling)DrawmsamplesfromS withoutreplacementaccordingtotheprobabilities: ωi
M (cid:80)M j=1ωj
for i = 1,2,...,M.
(d) (Inference) For b(θ), find the EFD associated with G(·) according to (15) based on the m samples
obtained in step (c).
This procedure involves two inverse functions: G˜(·) and G(·). By Lemma 3.1, any inverse function
G˜(·) can be used in step (a) to generate samples from Z , and this greatly facilitates comparisons
n
of the inference results from different choices of G(·). If G(·) and G˜(·) are chosen to be the same,
p∗(z|X ,Y ) ∝ π⊗n(z)(cid:0) det(∇2U (z))(cid:1)−1/2 can also be directly simulated on Z using a constrained
n n n 0 t n n
Monte Carlo algorithm.
Remark 1 (On the flexibility of EFI) EFI provides a flexible framework of statistical inference. One can
adjust the inverse function G(·) and the energy function U (·) to ensure that the resulting CD estimator
n
of b(θ) satisfies desired properties, such as efficiency, unbiasedness, and robustness. This mirrors the
flexibility of frequentist methods, where different estimators of b(θ) can be designed for different purposes.
However, its conditional inference nature makes EFI even more attractive than frequentist methods, as it
circumvents the need for derivations of theoretical distributions of the estimators.
Lastly, we note that the fiducial distribution defined above conceptually differs from that defined in
GFI [35, 36, 54]. Specifically, GFI interprets the fiducial distribution as the θ-marginal of a distribution
defined on the manifold formed by the data generating equations in the joint space of (θ,Z ) ∈ Rp×Rn,
n
while EFI interprets it as the θ-transformation of a distribution defined on a subset or manifold formed
by the data generating equations in the sample space of Z ∈ Rn. Our definition is consistent with the
n
EFI algorithm developed in this paper.
3.2 EFI for the Models with Additive Noise
The importance resampling procedure proposed in Section 3.1 is general for simulations of p∗, but com-
n
puting the importance weights can be challenging when the sample size n is large. Specifically, it involves
calculating the determinant of an (n−p)×(n−p)-matrix at each iteration. To address this issue, we con-
sidermodelswithadditivenoise,whichrepresentabroadclassofmodelsandhavebeenextensivelystudied
in the context of causal inference (see e.g., [67] and [37]). Additionally, we suggest setting the energy
function as prescribed in Assumption 5-(i), with the L -norm U (z) = ∥Y −f(X ,z,G(Y ,X ,z))∥2
2 n n n n n
as a special case. Consequently, for these models, we show that the importance weight is reduced to a
constant and, therefore, one can simulate from p∗ by directly simulating from π⊗n(z) using a constrained
n 0
Monte Carlo algorithm.
Assumption 5 (i) U (·) is specified in the form: U (z) = h(J(z)) =
(cid:80)n
h(e ) for some function h(·)
n n i=1 i
satisfying ∂h(J) (z) = 0 for any z ∈ Z , where J(z) = Y −f(X ,z,G(Y ,X ,z)) = (e ,e ,...,e )T,
∂J n n n n n 1 2 n
11and e = y − f(x ,z ,θ) for i = 1,2,...,n; and (ii) the model noise is additive; i.e., the function
i i i i
f(X,Z,θ) in model (1) is a linear function of Z.
Theorem 3.2 If Assumptions 1-5 hold, then P∗(z|Y ,X ) given in (14) is invariant to the choices of
n n n
G(·) and U (·). Furthermore, P∗ reduces to a truncated distribution of π⊗n on the manifold Z .
n n 0 n
Proof: Under Assumptions 1-2, the uniqueness of Z has been established in Lemma 3.1. If U (·) is
n n
∂h(J)
specified as in Assumption 5, the condition (z) = 0 on Z implies
∂J n
∇2U (z) = (∇ J(z))T ∇2h(J(z))∇ J(z). (16)
t n t J t
Furthermore, with the aid of Assumption 4-(iv) and the symmetric form of h(·) (with respect to e ’s),
i
∇2h(J(z)) reduces to a diagonal matrix of ςI for some positive constant ς > 0. This ensures the
J n−k
factor ς to be canceled out for the numerator and denominator in (14). Consequently, P∗ is invariant to
n
the choice of U (·).
n
To further establish the invariance of P∗ with respect to the choice of G(·), we consider an inverse
n
function
G(Y ,X ,Z ) = θˆ(z ,z ,...,z ),
n n n 1 2 p
where θˆ(z ,z ,...,z ) is obtained by solving the first p equations in (2). Here we assume the solution
1 2 p
θˆ(z ,z ,...,z ) is unique for the p equations. Let ˜t = (z ,z ,...,z )T, which corresponds to a
1 2 p p+1 p+2 n
transformation of t in (13). Then, it is easy to verify that at any point z ∈ Z , the first p rows of the
n
matrix ∇ J(z) ∈ Rn×(n−p) are all zero, and the remaining (n−p)-rows forms a (n−p)×(n−p)-diagonal
˜t
matrixforwhichthediagonalelementsarenonzeroandexpressedasafunctionof(X ,θ)(i.e.,aconstant
n
functionofz)bytheassumptionthatf(X,Z,θ)isalinearfunctionofZ. Therefore, atanypointz ∈ Z ,
n
∇2U (z) forms a positive-definite constant matrix with rank n−p; and P∗ in (14) reduces to a truncated
t n n
distribution of π⊗n on Z .
0 n
Inthesameway,wecanconstruct(cid:0)n(cid:1)
differentinversefunctions,eachobtainedbychoosingadifferent
p
set of p equations to solve for θ. Therefore, each of them results in a positive definite matrix ∇2U (z)
t n
and the same distribution P∗. For any appropriate linear combination of these inverse functions, which
n
still forms an inverse function, the above result still holds. For the combination case, the desired result
can be established via appropriate matrix operations, as illustrated using a linear regression example in
Section §3 of the supplement.
Finally, we note that for any inverse function, since it solves all n equations, it must also be a solver
for a selected set of p equations. By the uniqueness of the solution for p equations, the inverse function
can be regarded as a linear combination of these
(cid:0)n(cid:1)
basis inverse functions. □
p
Example 1 Consider the linear regression model (3) again. Let θ = (β,σ2). To conduct EFI for θ,
we set G(Y ,X ,z) = θˆ(z ,z ,...,z ) := (βˆT ,σˆ)T, a solver for the first p equations in (2), and set the
n n 1 2 p
12energy function
U (z) = ∥Y −f(X ,z,G(Y ,X ,z))∥2. (17)
n n n n n
Consequently, we have
   
Y −X βˆ −σˆZ 0
1:p 1:p 1:p
J(z) =  , ∇ Z J(z) =  ,
Y −X βˆ −σˆZ (p+1):n σˆI
(p+1):n (p+1):n (p+1):n n−p
where Y , X and Z to denote the response, explanatory, and noise variables of the first p samples
1:p 1:p 1:p
in the dataset, respectively; likewise, Y , X and Z denote the response, explanatory,
(p+1):n (p+1):n (p+1):n
and noise variables of the last n − p samples. At any z ∈ Z , we have θˆ = θ, i.e., θˆ can be treated
n
as constants, and thus ∇2U (z) = 2D˜TD˜ for some matrix D˜ of rank n−p. This yields the result that
t n
p∗(z|X ,Y ) is a truncation of π⊗n on Z .
n n n 0 n
Example 1 (continuation) For simplicity, let’s first consider the case where σ2 is known. In this
scenario, we set
G(Y ,X ,Z ) = (XTX )−1XT(Y −σZ ),
n n n n n n n n
which utilizes all available data. Then the EFD of β, denoted by µ∗(β|Y ,X ,σ2), is given by
n n n
N((XTX )−1XTY ,σ2(XTX )−1) after normalizing p∗(z|Y ,X ) on Z , which coincides with the
n n n n n n n n n n
posterior distribution of β under Jeffery’s prior π(β) ∝ 1. Furthermore, the resulting confidence set for
β is identical to those obtained by the GFI and ordinary least square (OLS) methods.
Next, let’s consider the case where σ2 is unknown. To find the EFD of σ2, we solve the first p−1
equations for β, resulting in the solution:
β˜ = (XT X )−1XT (Y −σZ ),
1:(p−1) 1:(p−1) 1:(p−1) 1:p−1 1:(p−1)
whereY , X andZ denotetheresponse, explanatory, andnoisevariablesofthefirstp−1
1:(p−1) 1:(p−1) 1:(p−1)
samples in the dataset, respectively. By substituting β˜ into each of the remaining n−p+1 equations
and adjusting with the covariance of the Z-terms, we obtain a combined solution for σ2:
(Y −X (XT X )−1XT Y )TΣ−1(Y −X (XT X )−1XT Y )
σ˜2 = p:n p:n 1:(p−1) 1:(p−1) 1:(p−1) 1:p−1 p:n p:n 1:(p−1) 1:(p−1) 1:(p−1) 1:p−1 ,
(Z −X (XT X )−1XT Z )TΣ−1(Z −X (XT X )−1XT Z )
p:n p:n 1:(p−1) 1:(p−1) 1:(p−1) 1:p−1 p:n p:n 1:(p−1) 1:(p−1) 1:(p−1) 1:p−1
A
:= ,
W
where and Y , X and Z denote the response, explanatory, and noise variables of the last n−p+1
p:n p:n p:n
samples in the dataset, respectively; and Σ = I + X (XT X )−1XT , representing
n−p+1 p:n 1:(p−1) 1:(p−1) p:n
the covariance matrix of (Z −X (XT X )−1XT Z ). Here, A forms an unbiased
p:n p:n 1:(p−1) 1:(p−1) 1:(p−1) 1:p−1
estimatorof(n−p+1)σ2,andW followsaχ2-distributionwithadegree-of-freedomofn−p+1. Therefore,
if we set σ˜2 as the inverse function G(·) for σ2, the resulting EFD of σ2 is given by
(cid:18) σ2(cid:19)
1
µ∗(σ2|Y ,X ) = π , (18)
n n n χ− n−2 p+1 A A
13whereπ (u)denotesthedensityfunctionofaninverse-chi-squareddistributionwithadegree-of-freedom
χ−2
k
of k. If we use the mean of µ∗(σ2|Y ,X ) as an estimator of σ2, it can be shown that it has a bias of
n n n
2 σ2. In contrast, the MLE of σ2 has a bias of −p−1σ2. Therefore, the EFD results in a smaller bias
n−p−3 n
than the MLE when n > (p+3)(p−1)/(p−3). Note that, as stated in Remark 1, we can adjust σ˜2 by
the factor n−p−3 to make the mean of the EFD unbiased for σ2, if desired.
n−p−1
Finally, we can obtain the EFD of β by completing the integration:
(cid:90)
µ∗(β|Y ,X ) = µ∗(β|Y ,X ,σ2)µ∗(σ2|Y ,X )dσ2, (19)
n n n n n n n n n
which is a multivariate non-central t-distribution t(µ ,Σ ,ν ) with the parameters given by
β β β
A
µ = (XTX )−1XTY , Σ = (XTX )−1, ν = n−p+1.
β n n n n β n−p+1 n n β
The mean and covariance matrix of the EFD is given by (XTX )−1XTY and A (XTX )−1.
n n n n n−p−1 n n
It is worth noting that our EFD (18) matches the result obtained with OLS. The latter often presents
the result as
YT(I −X (XTX )−1XT)Y
n n n n n n n ∼ χ2 ,
σ2 n−p+1
where the numerator forms an unbiased estimator of (n−p+1)σ2. Similarly, in EFI, A serves as an
unbiased estimator of (n − p + 1)σ2. Also, the EFD (18) can be represented as an inverse-Gamma
distribution IG(α ,β ) with α = n−p+1 and β = A, which is the same as the GFI solution [36] except
g g g 2 g 2
for the expression of A.
Remark 2 The EFD derivation procedure described in Example 1 can be extended to general nonlinear
regression problems with additive noise. Consider the model Y = f(X ,β)+σZ := µ +σZ , where
n n n y n
Z is assumed to follow a known distribution symmetric about 0. First, let’s assume that σ2 is known.
n
Let T(Y ) be the OLS estimator of β, which makes use of all n samples. Let π denote the distribution
n T
of T(Y ), and let µ(i) = η(i)(µ ) denote its ith moment for i = 1,2,...,k. We can regard T(Y −σz)
n T y n
as an inverse function for β. Consequently, by (15), the EFD of β, associated with T(Y −σz), has the
n
ith moment given by η(i)(Y ) for i = 1,2,...,k. Furthermore, EFI shares the same distribution π as
n T
the frequentist method for quantifying the uncertainty of β.
If σ2 is unknown, we can follow the same procedure as described in Example 1 to find the EFD for
σ2. Finally, we can obtain the EFD for β by completing an integration similar to (19).
Theorem 3.2 implies that for an additive noise model, if any inverse function G˜(·) is known, then
p∗(z|X ,Y ) can be directly simulated from π⊗n(z) subject to the constraint U (z) = 0 using a con-
n n n 0 n
strained Monte Carlo algorithm. Furthermore, for b(θ), the empirical EFD associated with a known
inverse function G(·) can be constructed based on the samples simulated from p∗(z|X ,Y ).
n n n
144 Extended Fiducial Inference with a Sparse DNN Inverse Function
As implied by Theorem 3.1, Lemma 3.2, and Theorem 3.2, conducting fiducial inference requires finding
appropriate inverse functions. However, in practice, these inverse functions are typically very difficult to
determine. To address this issue, we propose approximating G˜(·) with a sparse DNN and employing an
adaptive stochastic gradient MCMC algorithm to simultaneously simulate from p∗(z|X ,Y ) and train
n n n
the sparse DNN. Then an empirical EFD of b(θ) associated with G(·) = G˜(·) can be constructed based on
the z-samples simulated from p∗(z|X ,Y ). We call this proposed algorithm the EFI-DNN algorithm.
n n n
4.1 The EFI-DNN Algorithm
Figure 2: Illustration of the EFI network, where the red nodes and links form a DNN (parameterized
by the weights w) to learn, the green node represents latent variables to impute, and the black lines
represent deterministic functions.
The entire structure of the algorithm is depicted by the so-called EFI network, as shown in Figure 2.
Let θˆ := gˆ(y ,x ,z ,w) denote the DNN prediction function parameterized by the weights w in the EFI
i i i i
network, and let
n n
1 (cid:88) 1 (cid:88)
θ¯ := θˆ = gˆ(y ,x ,z ,w), (20)
n i i i i
n n
i=1 i=1
which works as an estimator for the inverse function G(Y ,X ,Z ). Henceforth, we will call θ¯ an
n n n n
EFI-DNN estimator of G(Y ,X ,Z ). The EFI network has two output nodes defined, respectively, by
n n n
e := ∥θˆ −θ¯ ∥2,
i1 i n
(21)
e := d(y ,y˜) := d(y ,x ,z ,θˆ ),
i2 i i i i i i
where y˜ = f(x ,z ,θˆ ), the function f(·) is as defined in (2), and d(·) is a function that measures the
i i i i
difference between y and y˜. With a slight abuse of notation, we rewrite d(y ,y˜) as a function of y , x ,
i i i i i i
15z , and θˆ . For example, for normal linear/nonlinear regression, we define
i i
d(y ,x ,z ,θˆ ) = ∥y −f(x ,z ,θˆ )∥2.
i i i i i i i i
For logistic regression, we define d(y ,x ,z ,θˆ ) via a ReLu function, see Section §4.2 of the supplement.
i i i i
For the EFI network, we consider w as the parameters to estimate, Z as the latent variable (or
n
missing data) to impute, (X ,Y ) as the observed data (or incomplete data), and (X ,Y ,Z ) as the
n n n n n
complete data. Regarding the EFI network, we have a few further remarks.
Remark 3 The DNN in the EFI network is a fully-connected feedforward neural network, which maps
(y ,x ,z ) to θˆ for each i ∈ {1,2,...,n}. Both the depths and widths of the DNN can increase with
i i i i
the sample size n but under a constraint as given in Assumption A9-(ii-1) (in the supplement). To
ensure Assumption 4-(ii), the activation function needs to be continuously differentiable and, therefore,
can be chosen from options like tanh, softplus or sigmoid. In practice, the ReLU activation function can
also be used, as the resulting energy function is non-continuously differentiable at isolated points only.
Consequently, for case (b), we will have (14) holding almost surely, as implied by the proof provided in
[39]; for case (a), (11) still holds as the continuously differentiability condition is not required.
Remark 4 To address the potential overfitting issue in the DNN, we treat w in a Bayesian approach. We
impose a sparse prior on w, as given in (32), based on the sparse deep learning theory in [83]. However,
this Bayesian treatment is optional, as w can still be consistently estimated within the frequentist frame-
work when the training sample size n is sufficiently large. Furthermore, as discussed in Remark 7, the
prior hyperparameters can be entirely determined by the data through cross-validation, aligning the EFI-
DNN algorithm with the principle of fiducial inference. Sparse learning enables the EFI-DNN algorithm
to exhibit robust performance across a wide range of DNNs with different depths and widths, provided
they possess sufficient capacity to approximate desired inverse functions. Regarding the interpretability
of the sparse DNN, we refer to [49] and [83]. In the context of EFI networks, the sparse DNN provides
a parsimonious approximation to the inverse function. If the inverse function is a sparse neural network
function, then its structure can be consistently recovered (up to some loss-invariant transformations).
Remark 5 While the EFI network shares a similar structure with the fiducial autoencoder used in [44],
the DNNs in the two works are trained in different ways. In [44], the DNN is pre-trained using data
simulated from the model with a wide range of parameter values. In the present work, the DNN is trained
concurrently with the imputation of latent variables.
Let π(w) denote the prior density function of w, and let π(Y ,Z |X ,w) denote the conditional
n n n
density function of (Y ,Z ) given (X ,w). The form of π(w) will be detailed later; as discussed in
n n n
Remark 7, π(w) should be chosen such that θ¯ forms a consistent estimator for the inverse mapping
n
16G(Y ,X ,Z ). We propose to estimate w by maximizing the posterior distribution π(w|X ,Y ) ∝
n n n n n
(cid:82)
π(w) π(Y ,Z |X ,w)dZ . This can be done by solving the equation
n n n n
∇ logπ(w|X ,Y ) = 0. (22)
w n n
Further, by the Bayesian version of Fisher’s identity, see Lemma 1 of [79], (22) can be expressed as
(cid:90)
∇ logπ(w|X ,Y ) = ∇ logπ(w|X ,Y ,Z )π(Z |X ,Y ,w)dw = 0. (23)
w n n w n n n n n n
To define π(w|X ,Y ,Z ) and π(Z |X ,Y ,w), we first define a scaled energy function for the distri-
n n n n n n
bution π(Y |X ,Z ,w), up to an additive constant and a multiplicative constant:
n n n
n n
(cid:88) (cid:88)
U˜ (Z ,w;X ,Y ) = η ∥θˆ −θ¯ ∥2+ d(y ,x ,z ,θˆ ), (24)
n n n n i n i i i i
i=1 i=1
where the first term serves as a penalty function enforcing θˆ ’s to converge to the same value, and η > 0
i
is a regularization parameter. This penalty allows us to address possible non-uniqueness of the inverse
functions θˆ = gˆ(y ,x ,z ,w) for i = 1,2,...,n. Let
i i i i
π(Y |X ,Z ,w) = Ce−λU˜ n(Zn,w;Xn,Yn),
n n n
for some constants C > 0 and λ > 0. Then we have the following conditional distributions:
π(w|X ,Y ,Z ) ∝ π(w)e−λU˜ n(Zn,w;Xn,Yn),
n n n
(25)
π(Z |X ,Y ,w) ∝ π⊗n(Z )e−λU˜ n(Zn,w;Xn,Yn),
n n n 0 n
where λ is a tuning parameter resembling the inverse of the temperature in (9), and π⊗n(Z ) is the
0 n
marginal distribution of Z in the space Rn. With respect to the EFI network, we call π(Y |X ,Z ,w),
n n n n
π(w|X ,Y ,Z ), and π(Z |X ,Y ,w) the complete-data likelihood function, the complete-data pos-
n n n n n n
terior distribution, and the missing-data predictive distribution, respectively.
Remark 6 Alternative to (24), we can define the energy function as
n n
(cid:88) (cid:88)
U˜′(Z ,w;X ,Y ) = η ∥θˆ −θ¯ ∥2+ d(y ,x ,z ,θ¯ ). (26)
n n n n i n i i i n
i=1 i=1
Without confusion, we will refer to the EFI-DNN algorithm with the energy functions (24) and (26)
as EFI-a (alternative version) and EFI (default version), respectively, in the remaining of the paper.
Compared to (26), (24) is more regular, where the fitting errors are assumed to be mutually independent
given w. As λ → ∞, EFI-a and EFI are asymptotically equivalent, leading to the same zero-energy set.
With the distributions given in (25), Eq. (23) is now well defined and can be solved using an adaptive
stochasticgradientMCMCalgorithm[18,20,52]. Thealgorithmworksbyiteratingbetweenthefollowing
two steps, where k indexes the iterations:
17(k+1)
(a) (Latentvariablesampling)GenerateZ fromatransitionkernelinducedbyastochasticgradient
n
(k+1)
MCMC algorithm. For example, we can simulate Z using the stochastic gradient Langevin
n
dynamics (SGLD) algorithm [89]:
Z( nk+1) = Z( nk)+ϵ k+1∇(cid:98)znlogπ(Z( nk)|X n,Y n,w(k))+(cid:112) 2τϵ k+1e(k+1), (27)
wheree(k+1) ∼ N(0,I )isastandardGaussianrandomvectorofdimensiond ,ϵ isthelearning
dz z k+1
rate,∇(cid:98)Znlogπ(Z( nk) |X n,Y n,w(k))denotesanunbiasedestimatorof∇ Znlogπ(Z( nk) |X n,Y n,w(k)),
and τ is the temperature that is generally set to 1 in simulations.
(b) (Parameter updating) Update the estimate of w by stochastic gradient descent (SGD):
γ
w(k+1) = w(k)+ k n+1 ∇(cid:98)wlogπ(w(k)|X n,Y n,Z( nk+1)), (28)
whereγ k+1denotesthestepsizeofstochasticapproximation[73],and∇(cid:98)wlogπ(w(k)|X n,Y n,Z( nk+1) )
denotes an unbiased estimator of ∇ logπ(w(k)|X ,Y ,Z(k+1) ).
w n n n
The algorithm is referred to as “adaptive” as the transition kernel in step (a) changes along with the
update of w. Applying the adaptive SGLD algorithm to the EFI network leads to Algorithm 1, where
the parameter updating step is implemented with mini-batches, and a fiducial sample collection step is
added. Note that, given the current estimate of w, the latent variable sampling step can be executed in
parallel for each observation (x ,y ). Therefore, the whole algorithm is scalable with respect to big data.
i i
4.2 Convergence Theory of the EFI-DNN Algorithm
To indicate the dependency of w on the sample size n, we rewrite w as w in this subsection. We note
n
that the theoretical study is conducted under the assumption that the EFI network has been correctly
specified such that there exists a sparse solution w˜∗, at which (X ,Y ,Z∗) can be generated from the
n n n n
EFI network; specifically, Z∗ ∼ π(Z|X ,Y ,w˜∗) holds, where Z∗ represents the values of the latent
n n n n n
variables realized in the observations. The convergence of the EFI-DNN algorithm is studied in a few
steps. First, we show inTheorem 4.1 that∥w(k) −w∗∥ →p 0 ask → ∞, wherew∗ is asolution to(22) and
n n n
p (k)
→ denotes convergence in probability. Second, we show in Theorem 4.2 that Z converges weakly to
n
π(Z |X ,Y ,w∗) in 2-Wasserstein distance as k → ∞. Third, we show in Theorem 4.3 and the followed
n n n n
discussions that with an appropriate choice of the prior distribution π(w ) and as n → ∞ and λ → ∞,
n
gˆ(y ,x ,z ,w∗) constitutes a consistent estimator of θ∗ and, subsequently, the EFI-DNN estimator
i i i n
n
θ¯∗ := 1 (cid:88) gˆ(y ,x ,z ,w∗), (31)
n n i i i n
i=1
constitutes a consistent estimator for the inverse mapping G(Y ,X ,Z ). By summarizing the three
n n n
theorems, we conclude that the EFI-DNN algorithm leads to valid uncertainty quantification for θ.
Finally,weshowthatifθ¯∗ isconsistent,π(Z|X ,Y ,w∗)isreducedtotheextendedfiducialdistribution
n n n n
of Z as defined in Section 3.1.
n
18Algorithm 1: Adaptive SGLD for Extended Fiducial Inference
(i) (Initialization) Initialize the DNN weights w(0) and the latent variable Z(0) . set M as the
n
number of fiducial samples to collect. Let K denote the number iterations to perform in the
burn-in period, and let K+M be the total number of iterations to perform in a run.
for k=1,2,...,K+M do
(ii) (Latent variable sampling) Given w(k), simulate Z(k+1) by the SGLD algorithm:
n
Z(k+1) = Z(k)+ϵ ∇ logπ(Z(k)|Z ,Y ,w(k))+(cid:112) 2τϵ e(k+1), (29)
n n k+1 Zn n n n k+1
where e(k+1) ∼ N(0,I ), ϵ is the learning rate, and τ = 1 is the temperature.
dz k+1
(k) (k)
(iii) (Parameter updating) Draw a minibatch {(y ,x ,z ),...,(y ,x ,z )} and
1 1 1 m m m
update the network weights by the SGD algorithm:
(cid:34) m (cid:35)
w(k+1) = w(k)+γ n (cid:88) ∇ logπ(y |x ,z(k) ,w(k))+∇ logπ(w(k)) , (30)
k+1 m w i i i w
i=1
where γ is the step size, and logπ(y |x ,z(k) ,w(k)) can be appropriately defined according
k+1 i i i
to (24) or (26).
(iv) (Fiducial sample collection) If k+1 > K, calculate θˆ(k+1) = gˆ(y ,x ,z(k+1) ,w(k+1))
i i i i
for each i ∈ {1,2,...,n} and average them to get a fiducial θ¯ -sample as calculated in (20).
n
end
(v) (Statistical Inference) Conducting statistical inference for the model based on the
collected fiducial samples.
4.2.1 Convergence of Algorithm 1
Theorem 4.1 SupposeAssumptionsA1-A5(inthesupplement)hold. Ifwesetthelearningratesequence
{ϵ : k = 1,2,...} and the step size sequence {γ : k = 1,2,...} in the form ϵ = Cϵ and γ = Cγ
k k k cϵ+kα k cγ+kβ
for some constants C > 0, c > 0, C > 0 and c > 0, α,β ∈ (0,1], and β ≤ α ≤ min{1,2β}, then there
ϵ ϵ γ γ
exists a root w∗ ∈ {w : ∇ logπ(w|X ,Y ) = 0} such that
n w n n
E∥w(k)−w∗∥2 ≤ ξγ , k ≥ k ,
n n k 0
for some constant ξ > 0 and iteration number k > 0.
0
Since the adaptive SGLD algorithm can be viewed as a special case of the adaptive pre-conditioned
SGLD algorithm [20], Theorem 4.1 can be proved by following the proof of Theorem A.1 of [20] with
minor modifications. Regarding the convergence rate of the algorithm, [20] provides an explicit form of
ξ. To make the presentation concise, we omit it in the paper.
19Let π∗ = π(Z |X ,Y ,w∗), let T = (cid:80)k−1ϵ , and let µ denote the probability law of Z(k) .
n n n n k i=0 i+1 T k n
Theorem 4.2 establishes convergence of µ in 2-Wasserstein distance.
T
k
Theorem 4.2 Suppose Assumptions A1-A6 (in the supplement) hold, and {ϵ } and {γ } are set as in
k k
Theorem 4.1. Then, for any k ∈ N,
W (µ ,π∗) ≤ (Cˆ δ1/4+C˜ γ1/4 )T +Cˆ e−T k/cLS,
2 T k 0 g 1 1 k 2
for some positive constants Cˆ , Cˆ , and Cˆ , where W (·,·) denotes the 2-Wasserstein distance, c
0 1 2 2 LS
denotes the logarithmic Sobolev constant of π∗, and δ is a coefficient as defined in Assumption A3 and
g
reflects the variation of the stochastic gradient ∇(cid:98)Znlogπ(Z n(k) |X n,Y n,w(k)).
We use the full data in the sampling step such that δ = 0, choose α ∈ (0,1], and choose γ ≺ 1 for
g 1 T4
k
any T , which ensures W (µ ,π∗) → 0 as k → ∞.
k 2 T k
4.2.2 On the Consistency of
θ¯∗
n
Let W
n
⊂ Rdw denote the space of w n, where d
w
denotes the dimension of w n. Let each component of
w be subject to a truncated mixture Gaussian distribution with the density function given by
n
π(w(i)) = ρ f(w(i);0,σ2 )+(1−ρ )f(w(i);0,σ2 ), w(i) ∈ W(i), i = 1,2,...,d , (32)
n n n 1,n n n 0,n n n w
where W(i) ⊂ R denotes the ith component of W , ρ is the mixture proportion, σ < σ , the density
n n n 0,n 1,n
function of each component of the mixture distribution is given by
(cid:90)
f(w;0,σ2) = ϕ(w/σ)/ [ρ ϕ(w/σ )+(1−ρ )ϕ(w/σ )]dw,
n 1,n n 0,n
Wn(i)
andϕ(·)denotesthestandardGaussiandensityfunction. Allcomponentsofw area prioriindependent.
n
In our experience, the weights of DNNs often cluster around a small subset near the origin 0 in the space
Rdw. Therefore, it is reasonable to constrain W
n
to a compact set, as stipulated in Assumption A7.
To establish the consistency of
θ¯∗
, we first define
n
1 1
G(cid:98)(w n|w˜∗ n) :=
n
logπ(Y n,Z∗ n|X n,w n)+
n
logπ(w n), (33)
where Z∗ ∼ π(Z|Y ,X ,w˜∗) as defined previously. Therefore,
n n n n
wˆ∗
n
:= arg max G(cid:98)(w n|w˜∗ n),
wn∈Wn
is also the global maximizer of the log-posterior logπ(w |X ,Y ,Z∗), given the pseudo-complete data.
n n n n
Further, we define
(cid:90)
1 1
G(cid:101)(w n|w˜∗ n) :=
n
logπ(Y n,Z∗ n|X n,w n)dπ(Z∗ n|X n,Y n,w˜∗ n)+
n
logπ(w n)
1(cid:110) (cid:90) π(Z∗|X ,Y ,w˜∗)
= logπ(w |X ,Y )− log n n n n dπ(Z∗|X ,Y ,w˜∗) (34)
n n n n π(Z∗|X ,Y ,w ) n n n n
n n n n
(cid:90) (cid:111)
+ logπ(Z∗|X ,Y ,w˜∗)dπ(Z∗|X ,Y ,w˜∗)+c ,
n n n n n n n n
20(cid:82)
wherec = log π(Y |X ,w )π(w )dw isthelog-normalizingconstantoftheposteriorπ(w |X ,Y ).
Wn n n n n n n n n
In the derivation of (34), X can be ignored for simplicity as it is constant. For simplicity of notation,
n
we let D (w ) = (cid:82) log π(Z∗ n|Xn,Yn,w˜∗ n) dπ(Z∗|X ,Y ,w˜∗) be the Kullback-Leibler divergence between
KL n π(Z∗ n|Xn,Yn,wn) n n n n
π(Z∗|X ,Y ,w˜∗) and π(Z∗|X ,Y ,w ) in what follows.
n n n n n n n n
Let Q∗(w ) = E(logπ(Y,Z|X,w ))+ 1 logπ(w ), where the expectation is taken with respect to the
n n n n
joint distribution of (X,Y,Z). Further, by Assumption A7 and the weak law of large numbers,
1 logπ(w |X ,Y ,Z )−Q∗(w ) →p 0, (35)
n n n n n
n
holds uniformly over the parameter space W . Assumption A8 restricts the shape of Q∗(w ) around
n n
the global maximizer, which cannot be discontinuous or too flat. Given nonidentifiability of the neural
network model, see e.g. [83], we have implicitly assumed that each w is unique up to the loss-invariant
n
transformations,e.g.,reorderingthehiddenneuronsofthesamehiddenlayerandsimultaneouslychanging
the signs of some weights and biases. The same assumption has often been used in theoretical studies of
neural networks, see e.g. [49] and [83].
On the other hand, by Theorem 1 of [47], under some regularity conditions we have
(cid:12) (cid:12)
sup (cid:12) (cid:12)G(cid:98)(w n|w˜∗ n)−G(cid:101)(w n|w˜∗ n)(cid:12)
(cid:12)
→p 0, as n → ∞. (36)
wn∈Wn
Putting (35) and (36) together and assuming that Q∗(w ) satisfies Assumption A8, then we have the
n
following lemma, whose proof is given in the supplement.
Lemma 4.1 Suppose Assumptions A7-A8 (in the supplement) hold, and π(Y ,Z |X ,w ) is continu-
n n n n
ous in w . If wˆ∗ is unique, then w∗ that maximizes π(w |X ,Y ) and minimizes D (w ) is unique
n n n n n n KL n
and, subsequently, ∥wˆ∗ −w∗∥ →p 0 holds as n → ∞.
n n
The uniqueness of wˆ∗, up to some loss-invariant transformations, can be ensured by the consistency
n
of the posterior π(w |Y ,X ,Z ) as established in Theorem 4.3 with an appropriate prior π(w ). The
n n n n n
conditionminimizingD (w )isgenerallyimpliedbyU˜ (Z ,w ;X ,Y ) = 0providedtheconsistency
KL n n n n n n
of θ¯∗ , and the convergence of w∗ to a maximum of π(w |X ,Y ) is generally implied by the Monte
n n n n n
Carlo nature of Algorithm 1. Therefore, by Theorem 4.1, if w(k) converges and U˜ (Z ,w(k) ;X ,Y )
n n n n n n
converges to 0, we would have ∥wˆ∗ −w∗∥ →p 0, provided that the prior has been appropriately chosen
n n
such that the posterior consistency holds and gˆ(y ,x ,z ,wˆ∗) constitutes a consistent estimator of θ∗.
i i i n
Supposeourchoiceofthepriorπ(w )ensuresthattheposteriorconsistencyholdsandgˆ(y ,x ,z ,wˆ∗)
n i i i n
is consistent for θ∗. By Lemma 4.1, gˆ(y ,x ,z ,w∗) would also be consistent for θ∗, provided gˆ(·) is
i i i n
continuous. The posterior consistency and the consistency of gˆ(y ,x ,z ,wˆ∗) can be proved based on the
i i i n
results of [83]. This is summarized in Theorem 4.3, whose proof can be found in the supplement. Note
that working on wˆ∗ is simpler than working on w∗, as the former is based on the complete data.
n n
21Theorem 4.3 Suppose that π(w ) is a truncated mixture Gaussian prior distribution as specified in
n
(32) and Assumptions A1-A10 (in the supplement) hold. Then, under the limit λ → ∞, the posterior
consistency holds for π(w |Y ,X ,Z ) and the inverse mapping estimator gˆ(·) (with either the energy
n n n n
function (24) or (26)) constitutes a consistent estimator for the model parameters, i.e.,
∥gˆ(y,x,z,w∗)−θ∗∥ →p 0, as n → ∞,
n
whereθ∗ denotesthefixedunknownparametervalues, and(y,x,z)denotesagenericelementof(Y ,X ,Z ).
n n n
Following from Theorem 4.3, we immediately have ∥1 (cid:80)n gˆ(y ,x ,z ,w∗)−θ∗∥ →p 0 as n → ∞. As
n i=1 i i i n
a slight relaxation of Assumption 1, we can write (8) as
θ∗ = lim G(Y ,X ,Z ), (37)
n n n
n→∞
where Z is assumed to be known. For example, consider the normal mean model
n
y = θ+z , z ∼ N(0,1), i = 1,2,...,n, (38)
i i i
forwhichG(Y ,Z ) = (cid:80)n (y −z )/n ≡ θ∗ and, therefore, (37)holdstrivially. Bycombiningtheabove
n n i=1 i i
two limits, we have
(cid:13) (cid:13)
n
(cid:13) (cid:13)1 (cid:88) gˆ(y ,x ,z ,w∗)−G(Y ,X ,Z )(cid:13) (cid:13) →p 0, as n → ∞, (39)
(cid:13)n i i i n n n n (cid:13)
(cid:13) (cid:13)
i=1
i.e.,theEFI-DNNestimatorθ¯∗ := 1 (cid:80)n gˆ(y ,x ,z ,w∗)isconsistentfortheinversemappingG(Y ,X ,Z ).
n n i=1 i i i n n n n
Further, bySlutsky’stheorem, theuncertaintyofZ canbepropagatedtoθ viatheEFI-DNNestimator.
n
Therefore, the confidence distribution of θ can be approximated by
M
1 (cid:88)
µ˜ (dθ) = δ (dθ), as M → ∞, (40)
n
M
θ¯∗,k
n
k=1
where δ stands for the Dirac measure at a given point a, θ¯∗,k := 1 (cid:80)n gˆ(x ,y ,z∗,k,w∗), and Z∗,k :=
a n n i=1 i i i n n
(z∗,k,z∗,k,...,z∗,k)fork = 1,2,...,MdenoteMrandomdrawsfromthedistributionπ(Z |X ,Y ,w∗)
1 2 n n n n n
under the limit setting of λ.
In this paper, although we set both the learning rate and step size sequences to decay with iterations,
(k) (k)
for which we particularly set 0.5 < β ≤ α < 1, we can still treat (w ,z ) approximately equally
n n
weighted by Theorem 2 of [79] and some classical results of stochastic approximation MCMC (see e.g.,
Theorem 3.3 of [46]). That is, we can approximate the confidence distribution of θ by
M
1 (cid:88)
µˆ (dθ) = δ (dθ), as M → ∞, (41)
n
M
θ¯k
n
k=1
where θ¯k := 1 (cid:80)n gˆ(x ,y ,zk,w(k) ), Z(k) := (zk,zk,...,zk), and (Z(k) ,w(k) ) denotes the sample and
n n i=1 i i i n n 1 2 n n n
parameter estimate produced by Algorithm 1 at iteration k. Some weighted estimation schemes, see e.g.
[85], also work, but involve extra computation.
22Remark 7 To obtain a consistent EFI-DNN estimator for the inverse mapping, we impose a truncated
mixture Gaussian prior (32) on w . It is worth noting that the hyperparameters of the prior distribution
n
can be entirely determined from the data. Specifically, we can employ cross-validation to determine their
values while constraining their orders to meet Assumption A9-(iv). We refer to [92] for the setup of the
cross-validation procedure which, together with the sparse DNN approximation theory established above,
ensures consistency of the inverse mapping estimator. This consistency property significantly mitigates
the impact of the prior distribution on downstream inference, aligning the EFI-DNN algorithm with the
principle of fiducial inference. When the sample size n is much larger than the dimension of w, we can
treat w in a frequentist way. Mathematically, this is equivalent to setting π(w(k)) ∝ 1 in (30) when
running Algorithm 1.
4.2.3 On the Property of π(z|Y ,X ,w∗)
n n n
We are now to study the property of π(z|Y ,X ,w∗). Consider the energy function defined in (24)
n n n
again. For convenience, we rewrite it as
n n
1 (cid:88) (cid:88)
Uˇ (z) = η∥gˆ(y ,x ,z ,w∗)− gˆ(y ,x ,z ,w∗)∥2+ d(y ,x ,z ,gˆ(y ,x ,z ,w∗)),
n i i i n n i i i n i i i i i i n
i=1 i=1
where we replace θˆ ’s and θ¯ with their DNN expressions. Define
i n
Z = (cid:8) z ∈ Rn : Uˇ (z) = 0(cid:9) . (42)
Uˇ
n
n
Let Π denote a probability measure on (Rn,R), where R is the Borel σ-algebra, and let π⊗n be the
n 0
corresponding density function. Further, we rewrite π(Z |Y ,X ,w∗) as the following:
n n n n
p (z|X ,Y ) ∝ π⊗n(z)e−λUˇ n(z). (43)
n,λ n n 0
A direct application of the theory in [39] to (43) leads to the following lemma, for which Assumptions
2-5 will be be justified in Remark 8.
Lemma 4.2 (Proposition2.2andTheorem3.1of[39])SupposethattheEFInetwork, theenergyfunction
Uˇ (z), the probability measure Π , and the zero-energy set Z satisfy Assumptions 2-4.
n n Uˇ
n
(a) If Π (Z ) > 0, then lim p (z|X ,Y ) is given by
n Uˇ
n
λ→∞ n,λ n n
P∗(z|X ,Y ) 1
n n n = π⊗n(z), z ∈ Z . (44)
dz Π n(Z Uˇ n) 0 Uˇ n
(b) If Π (Z ) = 0, then lim p (z|X ,Y ) is given by
n Uˇ
n
λ→∞ n,λ n n
P∗(z|X ,Y ) π⊗n(z)(cid:0) det∇2Uˇ (z))(z)(cid:1)−1/2
n n n = 0 t n , z ∈ Z , (45)
dν (cid:82) π⊗n(z)(cid:0) det∇2Uˇ (z))(z)(cid:1)−1/2 dν
Uˇ
n
Z Uˇn 0 t n
where ν is the sum of intrinsic measures on the p-dimensional manifold in Z .
Uˇ
n
23Remark 8 The conditions specified in Assumptions 2-4 are readily met by the EFI network. The exis-
tence of the minimum min Uˇ (z) = 0 is asymptotically guaranteed by the consistency of gˆ(y ,x ,z ,w∗).
z n i i i n
In particular, we have Uˇ (Z∗) →p 0 as n → ∞. The condition Π (Z ) > 0 is satisfied for logistic
n n n Uˇ
n
regression as discussed in Section §4.2 of the supplement. While the condition Π (Z ) = 0 is naturally
n Uˇ
n
satisfied for normal linear/nonlinear regression problems, as Z forms a manifold in Rn in this case.
Uˇ
n
In the model (1), if the function f satisfies the continuity condition as required in Assumption 4-(ii), we
can ensure that the EFI network also satisfies it by employing appropriate activation functions, such as
sigmoid, tanh and softplus. The other conditions are standard and generally hold.
Remark 9 Lemma 4.2 implies that the choice of η is not critical for the convergence of the EFI-DNN
algorithm, as long as λ → ∞. Specifically, different choices of η will result in the same zero-energy set
as λ → ∞. In practice, to enhance the convergence of the EFI-DNN estimator to the desired inverse
function, onecansetη toamoderatevaluesuchas2, 5, or10, andsetλtobereasonablylarge. Recallthat
η represents a regularization parameter as defined in (24). An appropriate value of λ can be determined
by gradually increasing it until the resulting confidence intervals of the model parameters cease to shrink.
In summary, we have developeda valid algorithmfor conductingfiducial inference forgeneral statisti-
cal models by leveraging a sparse DNN for the inverse function approximation. The EFI-DNN algorithm
is computationally efficient. When simulating the latent variables, it essentially samples from (9) with
a small value of ϵ rather than directly from the limiting distribution (14). This circumvents the need
to compute the determinant det(∇2U (z)), thereby significantly enhancing computational efficiency. On
t n
the other hand, since the algorithm is designed to sample from the limiting distribution of (9), it can
be applied to models with any type of noise, whether additive or non-additive. Furthermore, thanks to
the universal approximation capability of DNNs, the EFI-DNN algorithm is highly versatile and can be
applied to statistical models of various complexities.
4.3 Some Variants of the EFI-DNN Algorithm
In Algorithm 1, the latent variable sampling step is performed using a SGLD algorithm. This can be
replacedwithanadvancedstochasticgradientMCMCalgorithm, suchasstochasticgradientHamiltonian
Monte Carlo (SGHMC) [12], momentum SGLD [41], or preconditioned SGLD [43]. The convergence of
adaptive SGHMC has been studied in [52], where similar theoretical results to Theorem 4.1 and Theorem
4.2wereachieved. ComparedtoSGLD,SGHMCincludesanextramomentumterm, whichenablesfaster
exploration of the sample space [45].
Other than adaptive SGHMC, we also recommend replacing SGLD with tempering SGLD in Algo-
rithm 1. In this tempering algorithm, the temperature τ in (29) is replaced by a decreasing sequence τ
k
that converges to 1 along with iterations. Such a tempering algorithm is particularly useful for outlier
24detection problems, as illustrated in Section 5.4. With the tempering technique, random errors of large
magnitudes can be easily drawn for some observations, accelerating the convergence of the simulation.
Similar to the tempering technique discussed above, using an increasing sequence of {λ } that con-
k
verges to a target value along with iterations can also improve the convergence of the simulation. As λ
k
increases, the latent variable samples gradually shift toward the set Z . In this setup, Algorithm 1 pos-
Uˇ
n
sesses a dual adaptive mechanism, adapting both the values of λ and w(k). The convergence properties
k
of such an algorithm will be investigated in future work, following a framework similar to [46].
5 Illustrative Examples
5.1 Linear Regression
We begin by considering a linear regression model given by
y = xTθ+σz , i = 1,2,...,n, (46)
i i i
where z ∼ N(0,1), x = (x ,...,x )T, x = 1, x ∼ N(0,1) for k = 1,...,9, σ = 1, and the
i i i,0 i,9 i,0 i,k
regression coefficient θ = (θ ,θ ,...,θ )T = (1,1,1,1,1,0,0,0,0,0)T. For convenience, we refer to
0 1 9
(θ ,θ ,...,θ ) as signal parameters and (θ ,θ ,...,θ ) as noise parameters. We simulated 100 datasets
0 1 4 5 6 9
from this model, each with a sample size of n = 500.
EFI-a and EFI were applied to this example with σ assumed to be known. For EFI, we have also
trieddifferentactivationfunctions, includingReLU,softplus, tanh, andsigmoid. Refertothesupplement
for the settings of the experiment. The numerical results are summarized in Table 1. Figure 3 illustrates
the concept of EFI. The left plot displays a scatter plot of Z versus Zˆ , where Z represents the true
n n n
randomerrorsrealizedfortheobservationsandZˆ representsasetofrandomerrorsimputedbyEFI.The
n
scatter plot highlights the presence of uncertainty in the random errors contained in the data. According
to the theory of EFI, the uncertainty in Z propagates to θ, giving rise to uncertainty in θ. The middle
n
plot is a quantile-quantile (Q-Q) plot for Z and Zˆ , indicating that they follow the same distribution.
n n
The right plot compares the confidence intervals of β produced by EFI and the OLS method. For this
1
dataset, the two methods produced nearly identical confidence intervals for β . This complies with our
1
theoretical result presented in Example 1 of Section 3.1.
For comparison, we have applied OLS and GFI to this example. The OLS method is simple, whose
implementation is available in many statistical packages such as R Studio. There are two ways to im-
plement GFI as described in Section 2. One is to use the acceptance-rejection procedure as described in
Section 2. However, due to its importance sampling nature, this procedure becomes highly inefficient for
the problems with a large value of n. For instance, in this example, we attempted to generate 50,000,000
samples of Z from N(0,I ) for n = 500, but none of them was accepted. The other way involves direct
n n
simulations from the limiting distribution as given in Theorem 1 of [36]. For this example, the limiting
25Figure 3: Results of EFI (with the ReLU activation function) for one dataset simulated from (46) with
n = 500: (left) scatter plot of zˆ (y-axis) versus z (x-axis), (middle) Q-Q plot of zˆ and z , (right)
n n n n
confidence intervals of β produced by EFI and OLS.
1
distribution is given by θ ∼ N(XTX )−1XTY ,σ2(XTX )−1), where X represents the design matrix
n n n n n n n
of (46) and Y = (y ,y ,...,y )T, which is identical to the extended fiducial distribution.
n 1 2 n
Table 1: Statistical inference results for the model (46) with known σ2, where “Coverage” refers to the
averaged coverage rate over 100 datasets and respective parameters, and “CI-width” refers to the average
width of respective confidence intervals.
Signal parameters Noise parameters
Method Activation Coverage rate CI-width Coverage rate CI-width
OLS — 0.95 0.177 0.956 0.177
GFI — 0.95 0.177 0.952 0.177
EFI-a ReLU 0.948 0.176 0.95 0.171
EFI Sigmoid 0.948 0.176 0.956 0.176
EFI Tanh 0.948 0.176 0.956 0.176
EFI Softplus 0.95 0.177 0.95 0.176
EFI ReLU 0.95 0.176 0.95 0.176
Table 1 shows that both versions of EFI work very well for this example. In our experience, EFI-a
often requires a larger value of η to control the variability of θˆ than EFI. Additionally, EFI tends to be
i
more robust to parameter settings than EFI-a, as it directly use the average θ¯ in generating the fitted
n
values y˜’s. Since EFI and EFI-a are asymptotically equivalent, as mentioned in Remark 6, we will only
i
present the results of EFI in the following analysis. Furthermore, Table 1 shows that EFI is robust to the
26choice of the activation functions. Note that each of these activation functions is Lipschitz continuous
(with a Lipschitz constant of 1) and can result in a consistent estimator for the inverse function.
For a comprehensive treatment of the model (46), we applied EFI to the simulated datasets with σ2
assumed to be unknown. The results are summarized in Table 2, which demonstrates the validity of EFI
for performing statistical inference on the model. In this case, we experimented different settings of η
and λ, and EFI proved to be robust to these settings.
Table 2: Statistical inference results for the model (46) with unknown σ2, where “Coverage” refers to the
averaged coverage rate over 100 datasets and respective parameters, and ‘CI-width” refers to the average
width of respective confidence intervals.
Signal parameters Noise parameters Variance (σ2)
Method (η,λ) Coverage CI-width Coverage CI-width Coverage CI-width
OLS — 0.948 0.176 0.948 0.175 0.95 0.252
GFI — 0.952 0.177 0.946 0.176 0.95 0.251
EFI (2,30) 0.95 0.180 0.948 0.178 0.95 0.255
EFI (2,40) 0.952 0.179 0.954 0.179 0.95 0.252
EFI (2,50) 0.95 0.178 0.946 0.177 0.95 0.252
EFI (4,50) 0.954 0.178 0.946 0.175 0.95 0.252
In summary, EFI performs as expected for this example, yielding similar results to OLS and GFI.
This is consistent with our analytic results in Example 1, where we showed that EFI results in the same
theoretical confidence distribution as OLS and GFI for the linear regression model. It is worth noting
that in this particular example, the observations precisely follow the presumed model. In Section 5.4, we
will demonstrate that EFI can outperform likelihood-based methods when this situation is altered.
5.2 Behrens-Fisher problem
Consider two Gaussian distributions N(µ ,σ2) and N(µ ,σ2). Suppose that two independent random
1 1 2 2
samples of sizes n and n are drawn from them, respectively. The structural equations are given by
1 2
y = µ +σ z , i = 1,...,n ,
1i 1 1 1i 1
(47)
y = µ +σ z , i = 1,...,n ,
2i 2 2 2i 2
where z ,z ∼ N(0,1) independently. The Behrens-Fisher problem pertains to the inference for the
i1 i2
difference µ − µ when the ratio σ /σ is unknown. Behrens [4] proposed the first solution to the
1 2 1 2
27problem in the context of testing the hypothesis H : µ = µ versus H : µ ̸= µ , based on the pivot:
0 1 2 1 1 2
(Y¯ −Y¯ )−(µ −µ )
1 2 1 2
T = , (48)
(cid:112)
S2/n +S2/n
1 1 2 2
where Y¯ and S2 denote, respectively, the sample mean and sample variance of population i for i = 1,2.
i i
Fisher [27] pointed out that this solution could be justified using the fiducial theory. Jeffreys [40] showed
that a Bayesian calculation with the prior π(θ) ∝ (σ σ )−1 yields the same confidence interval as the
1 2
fiducial method. From a frequentist perspective, Bartlett [2] noted that inverting Behrens’ test can lead
to a conservative confidence interval for µ −µ , i.e., its coverage probability is greater than the nominal
1 2
level. Later, basedonthe samestatisticT, Welch[88]proposedat-testforwhichtheresultingconfidence
interval for µ −µ has a coverage probability nearly equal to the nominal level. However, Fisher [28]
1 2
criticized Welch’s test for its negatively biased relevant selections, i.e., the coverage rate of its confidence
interval can be lower than the nominal level for some instances. As shown in [53], there are no exact
fixed-level tests based on the complete sufficient statistics for this problem. However, exact solutions
based on other statistics and approximate solutions based on the complete sufficient statistics do exist.
Recently, Martin and Liu [59] applied the inferential model method to this problem, resulting in the same
confidence interval as Hsu-Scheff´e’s [38, 74], but which is known to be conservative [21]. Wang and Jia
[87] developed a non-asymptotic t-test for the problem based on a statistic different from T, but the
efficiency of the test is still unclear.
We applied EFI to this problem by solving the two structural equations in (47) separately: one for
(k) (k)
(µ ,σ ) and the other for (µ ,σ ). Let {µˆ : k = 1,2,...,M} and {µˆ : k = 1,2,...,M} denote,
1 1 2 2 1 2
respectively, the fiducial samples for the population means produced by the two EFI solvers. Then, the
95% confidence interval for µ −µ can be directly constructed by finding the 2.5th and 97.5th percentiles
1 2
(k) (k)
of the samples {µˆ −µˆ : k = 1,2,...,M}. This confidence interval construction method sets EFI
1 2
significantly apart from existing methods, as it doesn’t directly seek the distribution of a test statistic.
This advantage of EFI will be further illustrated in Section 7.
In our first simulations, we set n = n = 50, µ = 1, µ = 0, and varied the values of (σ2,σ2) as
1 2 1 2 1 2
provided in Table 3. The widths and coverage rates of the resulting confidence intervals are reported
in Table 3, where the results were obtained with M = 10,000 and by averaging over 200 independent
datasets. For comparison, we also report the results from the Behrens-Fisher method (available in the R
package ‘asht’ [23]), Welch’s method, Hsu-Scheff´e’s method, and Te-test [87]. The comparison suggests
thatforthisexample,EFItendstobemoreefficientthantheexistingmethods,yieldingshorterconfidence
intervals while maintaining the same level of coverage rates.
(k)
To explain the efficiency of EFI, we present in Figure 4 the Q-Q plots of {µˆ : k = 1,2,...,M}
i
versus {t˜( ik) : k = 1,2,...,M} for i = 1,2. Here, t˜( ik) = y¯
i
− √s ni it∗ ni−1(k), and t∗ ni−1(k) denotes the kth
samplerandomlydrawnfromastudentt-distributionwithn −1degreesoffreedom. Sincethesamplesize
i
n is finite and thus the samples can be viewed as drawn from a tail-truncated distribution, EFI imputes
28Table 3: Statistical inference results for the Behrens-Fisher problem, where “Coverage” refers to the
coverage rate of µ −µ calculated by averaging over 200 datasets, and “CI-width” refers to the average
1 2
width of respective confidence intervals.
(σ2,σ2) = (0.25,1) (σ2,σ2) = (1,1)
1 2 1 2
Method Coverage CI-width std CI Coverage CI-width std CI
n = n = 50
1 2
Behrens-Fisher 0.95 0.634 0.0040 0.955 0.802 0.0043
Welch 0.95 0.630 0.0040 0.95 0.794 0.0042
Hsu-Scheff´e 0.95 0.635 0.0040 0.955 0.804 0.0043
Te-Test 0.94 0.633 0.0045 0.95 0.800 0.0055
EFI 0.95 0.609 0.0058 0.955 0.788 0.0047
n = n = 500
1 2
Behrens-Fisher 0.95 0.196 0.0004 0.95 0.248 0.0004
Welch 0.95 0.196 0.0004 0.95 0.247 0.0004
Hsu-Scheff´e 0.95 0.196 0.0004 0.95 0.248 0.0004
Te-Test 0.95 0.196 0.0005 0.95 0.248 0.0005
EFI 0.95 0.198 0.0006 0.95 0.245 0.0014
(k)
Figure 4: Results of EFI for one dataset simulated from (47) with n = n = 50: (left) Q-Q plot of {µˆ :
1 2 1
k = 1,2,...M} (x-axis) and
{t˜(k)
: k = 1,2,...M} (y-axis); (right) Q-Q plot of
{µˆ(k)
: k = 1,2,...M}
1 2
(x-axis) and
{t˜(k)
: k = 1,2,...M} (y-axis)
2
.
29the latent variables essentially from a tail-truncated distribution, due to its conditional inference nature.
As a result, the Q-Q plots in Figure 4 display a tail-cut phenomenon. Therefore, when the sample size
n is small, the EFI confidence intervals can be shorter than those from unconditional inference methods,
even they have the same coverage rates. However, when the sample size becomes large, this feature of
conditional inference can disappear as illustrated by Table 3 with the results of n = n = 500. We refer
1 2
tothisfeatureasthefinite-sampleeffectforconditionalinference. ItisworthnotingthatsinceEFIsolves
for (µ ,σ ) and (µ ,σ ) separately, the Behrens-Fisher problem essentially becomes a linear regression
1 1 2 2
problem with unknown variances for EFI. Therefore, it is not surprising that the empirical distribution
of µˆ closely matches a location-scale student t-distribution.
i
5.3 Bivariate Normal Distribution
Let y ,y ,...,y , with y = (y ,y )T for i = 1,2,...,n, be independent samples from a bivariate
1 2 n i i,1 i,2
normal distribution with the mean vector and covariance matrix given as follows:
       
µ 1 σ2 ρσ σ 1 0.5
 1  =  ,  1 1 2  =  ,
µ 0 ρσ σ σ2 0.5 1
2 1 2 2
where ρ is the coefficient of correlation between two components of the bivariate normal vector. To
perform EFI, we consider the following decomposition:
y = µ +l z ,
i,1 1 1 i,1
(49)
y = µ +l z +l z ,
i,2 2 2 i,1 3 i,2
where l > 0 and l > 0, and z ’s (for k = 1,2 and i = 1,2,...,n) are i.i.d standard normal random
1 3 i,k
variables. It is easy to derive that σ = l , σ = (cid:112) l2+l2, and ρ = √l2 . Based on this decomposition,
1 1 2 2 3 l2+l2
2 3
we set θ = (µ ,µ ,log(l ),l ,log(l ))T for EFI. The results are presented in Table 4, where we calculated
1 2 1 2 3
the coverage rates and confidence interval widths based on 100 replications of the data set. The sample
size is n = 100 for each dataset.
Inference for the parameters of the bivariate normal distribution has served as a classical example of
fiducial inference. This can be seen in works such as Fisher [26, 29], Segal [76], and Bennett [6]. Their
derivations have yielded the following established results:
√
• The marginal fiducial distribution of either µ is given by n(y¯ − µ )/s ∼ t(n − 2), where
k k i k
y¯ = 1 (cid:80)n y , s = √1 (cid:112)(cid:80)n (y −y¯ )2, and t(n−2) denotes a student-t distribution with
k n i=1 i,k k n−1 i=1 i,k k
the degree of freedom n−2.
• The marginal fiducial distribution of either σ2 is given by (n−1)s2/σ2 ∼ χ2 , where χ2 denotes
k k k n−2 n−2
a chi-squared distribution with the degree of freedom being n−2.
30According to [7], the marginal fiducial distribution of ρ that was derived by Fisher [26] is the
same as its marginal posterior distribution when the parameters are subject to the right-Haar prior
π(µ ,µ ,σ ,σ ,ρ) ∝ σ−2(1−ρ2)−1. More precisely, the marginal fiducial distribution of ρ has a stochas-
1 2 1 2 1
tic representation as
(cid:32) (cid:115) (cid:115) (cid:33)
χ2∗ χ2∗ r
ψ − 1 + n−2√ , where ψ(x) = √ x ,
χ2∗ χ2∗ 1−r2 1+x2
n−1 n−1
r = 1 (cid:80)n (y −y¯ )(y −y¯ )/(s s ) is the sample correlation coefficient, χ2∗, χ2∗ and χ2∗ are
n−1 i=1 i,1 1 i,2 2 1 2 1 n−1 n−2
chi-squared random variables with the indicated degrees of freedom, and all the random variables are
mutually independent.
Table 4: Comparison of the fiducial and EFI for inference of the parameters of the bivariate normal
distribution, where the coverage rate and confidence interval length, given in the parentheses, were
calculated by averaging over 100 datasets of sample size n = 100.
Method µ µ σ σ ρ Average
1 2 1 2
Fiducial 0.96 (0.398) 0.96 (0.399) 0.97 (0.592) 0.96 (0.597) 0.95 (0.295) 0.96
EFI 0.95 (0.394) 0.96 (0.404) 0.97 (0.564) 0.97 (0.555) 0.95 (0.289) 0.96
The comparison suggests that for this example, EFI tends to produce shorter confidence intervals
than the Fiducial method for the scale parameters σ , σ , and ρ, while the two methods tend to yield
1 2
similar results for the location parameters µ and µ . Once again, we attribute the efficiency of EFI in
1 2
this example to the finite-sample effect, similar to the Behrens-Fisher problem.
5.4 Fidelity in Parameter Estimation
The frequentist methods often conduct parameter estimation under the maximum likelihood principle.
As implied by the constraint (4), the MLE can be easily contaminated by outliers. In contrast, as
implied by (24) and (25), EFI essentially estimates θ by maximizing the predictive likelihood function
π(Z n|X n,Y n,θ) ∝ π 0⊗n(Z n)e−λ(cid:80)n i=1d(yi,xi,zi,θ), which balances the fitting errors and the likelihood of
random errors. Compared to the MLE θˆ = argmax π⊗n(Z ), where Z can be expressed as a
MLE θ 0 n n
function of (Y ,X ,θ), the EFI estimator tends to be more robust to outliers and provides higher
n n
fidelity in parameter estimation. However, if the model is correctly specified, no outliers exist, and the
sample size is reasonably large, maximizing π⊗n(Z ) leads to an approximate minimization of the fitting
0 n
error (cid:80)n d(y ,x ,z ,θ). Specifically, when θˆ →p θ∗, Z∗ can be recovered in probability and thus
i=1 i i i MLE n
(cid:80)n
d(y ,x ,z ,θ)
→p
0. In such cases, the two methods will yield similar estimates, refer to Table 2 for
i=1 i i i
an illustrative example of this issue.
To illustrate EFI’s robustness to outliers, we consider the model (3) again. In this new simulation, we
setn = 600andgeneratedrandomerrorsfromamixtureGaussiandistributions: z ,z ,...,z ∼ N(0,1)
1 2 540
31and z ,z ,...,z ∼ N(4,1). The latter cases were considered as outliers, although some of them
541 542 600
might be indistinguishable from the former ones. Figure 5 compares the performances of EFI and OLS
on a simulated dataset. It suggests that EFI only slightly shrank the random errors and led to a more
accurate estimate of σ2 (≈ 1.0) and narrower confidence intervals for β, while the OLS estimate of σ2
(≈ 1.8) was significantly enlarged by outliers and the resulting confidence intervals of β were much wider.
The Bayesian method performs similarly to the maximum likelihood estimation method, as they both
are likelihood-based.
Figure 5: Fidelity of EFI in parameter estimation: (left) scatter plot of residuals: z versus zˆ; (middle
i i
left) scatter plot of ordered residuals: z versus zˆ ; (middle right) EFI and OLS confidence intervals
(i) (i)
for β ; (right) EFI and OLS confidence intervals for σ2.
1
In Section §4.1 of the supplement, we present another example which shows that the EFI estimator
is less prone to overfitting compared to those from the maximum likelihood or ordinary least square
method. This is again attributed to its emphasis on balancing the fitting errors and the likelihood of
random errors.
6 EFI for Semi-Supervised Learning
Asmentionedpreviously, theincorporationofcomputertechnologyintoscienceanddailylifehasenabled
scientists to collect massive volumes of data during the past two decades. However, many of the data
are unlabeled, as acquisition of labeled data for many problems can be expensive. In such situations,
semi-supervised learning (SSL), which is to combine a small amount of labeled data with a large amount
ofunlabeleddatatoenhancethelearningofaclassifier, canbeofgreatpracticalvalue. However, tomake
use of unlabeled data, some assumptions about the distribution of the data are needed [11]. For example,
one often makes i) the smoothness assumption that the points closing to each other are more likely to
share a label, ii) the cluster assumption that the points form some clusters and those in the same cluster
are more likely to share a label (although the data share a label may spread across multiple clusters), or
iii) the manifold assumption that the high-dimensional data lie roughly on a low-dimensional manifold.
The existing SSL methods can be roughly divided into categories such as consistency regularization,
32proxy-label, generative models, and graph-based methods. See [96] and [66] for overviews.
ForabetterexplanationoftheideabehindthecurrentSSLmethods,let’sconsideratextclassification
problem. Let x denote labeled text data, let y denote the labels, and let x denote unlabeled text data.
l l u
[65] modeled the text data using a mixture multinomial distribution as a generative model. By treating
y , the labels of x , as missing data, they derived the incomplete data posterior:
u u
(cid:88)
logπ(θ|y ,x ,x ) = Const+logπ(θ)+ log(p(y = c |θ)p(x |y = c ,θ))
l l u i j i i j
xi∈x
l (50)
(cid:88) (cid:0) (cid:88) (cid:1)
+ log p(c |θ)p(x |c ,θ) ,
j i j
xi∈xu cj∈S
where S denotes the set of classes, θ denotes the set of parameters of the mixture distribution, and
π(θ) denotes the prior of θ. As implied by (50), the key for SSL is to model the text data (x ,x ) for
l u
its class-wise distribution, i.e., p(x |c ,θ). Otherwise, under the conventional regression setting where
i j
(x ,x ) is treated as constants, the last term in (50) will be dropped and the unlabeled data will not be
l u
able to help to improve the estimate of θ.
Incontrast, asindicatedbyFigure2, EFIusesboththetextdataxandlabelsy asinput, andmodels
the distribution of x in an implicit way. Moreover, such an implicit model is general and user friendly
due to the universal approximation power of deep neural networks. Therefore, EFI can be easily adapted
to SSL by treating y as missing data, which will be sampled along with the latent variable Z in step
u n
(ii) of Algorithm 1. To illustrate the potential of EFI in SSL, we consider some classification problems
taken at UCI machine learning repository.
For binary classification, the second term (i.e., fitting error term) in (24) can be replaced by
(cid:88)n l
ρ((u −xTθˆ )(2y
−1))+(cid:88)nu
ρ((umiss−xTθˆ
)(tanh(v jmiss
)), (51)
i i i i j j j τ
i=1 j=1
whereρ(·)isaReLUfunction, n denotesthenumberoflabeleddata, n denotesthenumberofunlabeled
l u
data, u and umiss are latent variables, vmiss is defined through the equation P(ymiss = 1) = 1
i j j j −vmiss/τ
1+e j
for the missed label, and τ is a scale parameter. In simulations, we set τ = 1/50, ensuring the probability
1 is dichotomized to either 1 or 0, and treat {u : i = 1,2,...,n } and {umiss,vmiss : j =
1+e−v imiss/τ i l j j
1,2,...,n } as latent variables to simulate at each iteration. For EFI, (51) can be changed by replacing
u
θˆ ’s and θˆ ’s with θ¯ . For multiclass classification problems, (51) can be slightly modified.
i j n
For each dataset, EFI was run in 5-fold cross-validation, where the labels were removed from 50% of
the training samples. The results are summarized in Table 5, where the results of supervised learning
were obtained with the classical logistic regression. For comparison, the self-training algorithm [93] and
label-propagation algorithm [5, 15], which both belong to the category of proxy-label methods and are
available in the package scikit-learn 1.2.0, were applied to the datasets. In self-training, a model is
first trained on labeled data, this trained model is then used to predict the classification probabilities of
unlabeled data, and predictions with high confidence are added to the training set to retrain the model.
33In label-propagation learning, a graph is first created to connect the training samples, and then the
known labels are propagated through the edges of the graph to unlabeled samples in the training set. A
drawback of these methods is that the model is unable to correct its own mistakes, potentially amplifying
wrongclassificationsorbiasesthroughthetrainingprocess. Thesupervisedlearningmethodsaretolearn
a logistic regression model for each of the datasets.
The comparison shows the superiority of EFI in SSL, which can generally perform much better
than the self-training and label propagation algorithms. For the dataset “Raisin”, EFI even outperforms
supervisedlearning,andwewouldattributethisperformanceofEFItoitsfidelityinparameterestimation.
For these datasets, we have also applied EFI to the full training set and labeled data only. The results
are similar to those from the logistic regression. Refer to the supplement for the detail.
Table 5: Comparison of EFI with supervised learning and semi-supervised learning algorithms for some
classificationproblems,whereµ±serepresentsthemeanpredictionaccuracyofthe5-foldcrossvalidation
runs and the standard deviation of the mean value.
Supervised Learning Semi-Supervised Learning
Dataset size Full Labeled Only Self-training Label-propagation EFI
Divorce 170 98.82±1.05 96.47±1.29 92.94±3.87 96.47±1.29 98.82±1.05
Diabetes 520 89.62±1.29 87.69±1.69 87.31±2.01 85.77±2.01 88.08± 0.64
Breast Cancer 699 96.52±0.66 95.36±0.26 94.39±0.76 95.07±0.52 96.23±0.52
Raisin 900 82.89±1.16 83.78±0.24 58.67±1.35 50.22±0.20 85.56± 0.99
7 EFI for Complex Hypothesis Tests
As the scale and complexity of scientific data grow, there is often an interest in testing more complex
hypotheses. However, within the frequentist framework, it is usually challenging to derive the theoretical
reference distributions for the corresponding test statistics. In contrast, EFI operates in the mode of
conditional inference, circumventing the need for theoretical reference distributions and enabling easy
hypothesis testing based on collected fiducial samples. In this sense, EFI is driving statistical inference
toward an automated process.
To illustrate the automaticity of EFI in hypothesis testing, we consider the following mediation
analysis model [1]:
Y = β T +βM +βTX +ϵ , ϵ ∼ N(0,σ2),
T x Y Y Y
(52)
M = γT +γTX +ϵ , ϵ ∼ N(0,σ2 ),
x M M M
34where Y, T, M and X denote the outcome, treatment, mediator and design matrix, respectively. The
mediator effect can be inferred by testing the hypothesis H : βγ = 0 against H : βγ ̸= 0 with the
0 A
natural test statistic βˆγˆ. As mentioned by [62], this is a challenging inferential task due to the non-
uniform asymptotics of the univariate test statistic. Specifically, the null hypothesis consists of three
cases: (i) β = 0,γ ̸= 0, (ii) β ̸= 0,γ = 0, and (iii) β = γ = 0, while the theoretical reference distribution
of βˆγˆ under case (iii) is different from that under cases (i) and (ii). It is known that traditional statistical
tests such as Sobel’s test [78] and Max-P test [55] are conservative under case (iii). Recently, with a
fine theoretical analysis, [62] derived a test that is minimax optimal with respect to local power over the
alternative parameter space while preserving type-I error.
In contrast, applying EFI to such a composite hypothesis test is straightforward. The mediator effect
can be directly inferred based on the fiducial samples of β and γ, which can be collected along with
iterations of Algorithm 1. We note that the bootstrap method [22] works in a similar way to EFI, which
performs conditional inference for the model parameters and approximates their confidence distributions
in an empirical way. In this paper, we implemented the bootstrap method for the model (52) using the
R package “mediation” [86] under the default setting.
Simulation Studies For illustration, we simulated 100 datasets from the model (52) under each of the
cross settings of n ∈ {500,1000,2000} and (β,γ) ∈ {(0.2,0),(0,0.2),(0,0)}, where X = (X ,X ) consists
1 2
√
of two independent standard Gaussian random variables, σ = 2, σ = 1, β = (0.2,0.4)T, β = 1,
Y M x T
γ = (0.4,0.6)T. The results are summarized in Table 6, which indicates the validity and superiority of
x
EFI in testing complex hypotheses. Compared to the other methods, the type-I errors of EFI are much
closer to the nominal level 0.05, see Figure S2 in the supplement for a graphical view of the results.
Table 6: Type-I errors of the Sobel, MaxP, minimax optimal (mm-opt), bootstrap, and EFI tests for the
mediator effect, where the significance level of each test is α = 0.05.
n = 500 n = 1000 n = 2000
(β,γ) (0.2,0) (0,0.2) (0,0) (0.2,0) (0,0.2) (0,0) (0.2,0) (0,0.2) (0,0)
Sobel 0.01 0.00 0.00 0.05 0.02 0.00 0.04 0.06 0.00
MaxP 0.04 0.03 0.00 0.06 0.05 0.00 0.07 0.07 0.00
mm-opt 0.05 0.04 0.03 0.06 0.05 0.07 0.07 0.07 0.07
Bootstrap 0.06 0.05 0.01 0.04 0.07 0.00 0.13 0.04 0.00
EFI 0.05 0.06 0.04 0.06 0.04 0.04 0.05 0.04 0.05
Further, we simulated datasets for comparison of the powers of these tests, where (β,γ) ∈ {(0.1,0.4),
(−0.1,0.4), (0.2,0.2)} and other parameters were as set in the type-I error experiments. The results are
35summarized in Table 7, see also Figure S3 in the supplement for a graphical view of the results. The
comparison indicates that the EFI test has higher power than the other methods. The superiority of
EFI over the Bootstrap method is particularly encouraging, highlighting the great potential of EFI in
conditional inference and advancing the automation of statistical inference.
Table 7: Powers of the Sobel, MaxP, minimax optimal (mm-opt), bootstrap, and EFI tests for the
mediator effect, where the significance level of each test is α = 0.05. Part of the results of mm-opt are not
available (NA), as the test is inefficient for the alternative hypothesis settings of (β,γ) when the sample
size becomes large.
n=500 n=1000 n=2000
(β,γ) (0.1,0.4) (-0.1,0.4) (0.2,0.2) (0.1,0.4) (-0.1,0.4) (0.2,0.2) (0.1,0.4) (-0.1,0.4) (0.2,0.2)
Sobel 0.29 0.31 0.67 0.65 0.57 0.96 0.78 0.89 1.00
MaxP 0.34 0.37 0.79 0.66 0.59 0.98 0.78 0.89 1.00
mm-opt 0.34 0.37 0.79 NA NA NA NA NA NA
Bootstrap 0.33 0.42 0.52 0.59 0.51 0.93 0.93 0.92 1.00
EFI 0.48 0.64 0.84 0.70 0.74 0.97 0.86 0.95 1.00
Remark 10 This example demonstrates the potential of EFI in hypothesis testing. Due to its conditional
inference nature, EFI eliminates the need for theoretical reference distributions, thereby automating the
process of hypothesis testing. Moreover, compared to frequentist methods, EFI lowers the requirement for
sample size. In particular, under high-dimensional scenarios where the model dimension p grows with
the sample size n, frequentist methods typically require p2/n → 0 for achieving asymptotic normality (see
e.g. [69] and [70]). For EFI, we believe that p/n → 0 is sufficient for achieving valid fiducial inference,
which ensures Assumption (37) holds for many data generation equations. A further theoretical study on
this issue will be reported elsewhere.
8 Discussion
We have developed EFI as a novel and flexible framework for statistical inference, applicable to general
statistical models regardless of the type of noise, whether additive or non-additive. We have also intro-
duced the EFI-DNN algorithm for effective implementation of EFI, which jointly imputes the realized
random errors in observations using stochastic gradient Markov chain Monte Carlo and estimates the
inverse function using a sparse DNN based on all available data. The consistency of the sparse DNN
estimator ensures that the uncertainty embedded in the observations is properly propagated to the model
parameters through the estimated inverse function, thereby validating downstream statistical inference.
36The EFI-DNN algorithm has demonstrated appealing properties in parameter estimation, hypothesis
testing, and semi-supervised learning. Additionally, thanks to the conditional inference nature of EFI
and the universal approximation power of DNNs, the EFI-DNN algorithm holds great potential to au-
tomate statistical inference. Toward this direction, further study on the theoretical properties of the
EFI-DNN inference is of great interest.
The EFI-DNN algorithm is scalable, which can handle very large-scale datasets with the use of adap-
tive stochastic gradient MCMC algorithms. Specifically, its parameter updating step can be accelerated
by the mini-batch strategy; and the latent variable sampling step can be executed separately for each ob-
servation, enabling straightforward implementation in a parallel architecture. Theoretical guarantees for
the convergence of the algorithm have been studied; we established the weak convergence of the imputed
random errors and the consistency of the inverse function estimator.
This paper has considered only the problems where p is either fixed or grows with n slowly enough
to satisfy Assumption A9-(ii). Extending the EFI-DNN algorithm to high-dimensional problems, where
p > n and/or p grows with n at a higher rate, is possible. For instance, if the high-dimensional issue
arises from including an excessively large number of covariates, a model-free sure independence screening
procedure (see e.g.,[91, 13]) can be performed on the data before applying the algorithm. Furthermore, if
oneaimstoexaminetheuncertaintyofaparameterforanindividualcovariate, theMarkovneighborhood
regression (MNR) approach [50, 51, 81] can be applied. This approach decomposes the high-dimensional
inference problem into a sequence of low-dimensional inference problems based on the graphical model
formed by the covariates.
Availability
The code that implements the EFI method can be found at https://github.com/sehwankimstat/EFI.
Acknowledgments
Liang’s research is support in part by the NSF grants DMS-2015498 and DMS-2210819, and the NIH
grants R01-GM126089 and R01-GM152717. The authors thank the editor, associate editor, and three
referees for their constructive comments, which have led to significant improvement of this paper.
37Appendix: Supplement for “Extended Fiducial Inference: Toward
an Automated Process of Statistical Inference”
This supplement is organized as follows. Section §1 provides the proofs for Theorem 4.1 and Theorem
4.2. Section§2providestheproofforTheorem4.3. Section§3providestheproofforExample1ofSection
3.1 of the main text. Section §4 provides more numerical results. Section §5 presents detailed parameter
settings used in the numerical experiments.
§1 Proof of Theorem 4.1 and Theorem 4.2
Notation: For both Theorem 4.1 and Theorem 4.2, the sample size n is fixed. For simplicity of notation,
we will replace the dataset notation (X ,Y ,Z ) by (x ,y ,z ) and further drop the subscripts of x ,
n n n n n n n
y , z , w and W in the remaining part of this section. Additionally, for convenience, we redefine
n n n n
h(w) := ∇ logπ(w|x,y), H(w,z) := ∇ logπ(w|x,y,z), π (z|w) := π(z|x,y,w), and F (z,w) :=
w w D D
logπ (z|w), where D represents a training dataset. Furthermore, with a slight abuse of notation, we
D
use z and w to denote the latent variable sample and parameter estimate obtained at iteration k of
k k
Algorithm 1.
§1.1 Proof of Theorem 4.1
With the simplified notation, the equation (23) of the main text can be rewritten as
(cid:90)
h(w) = H(w,z)π (z|w)dw = 0, (S1)
D
where w ∈ Rdw, z ∈ Rdz, and d
w
and d
z
denote the dimensions of w and z, respectively. The adaptive
SGLD algorithm used for solving equation (S1) can be written in a general form as
(cid:112)
z = z +ϵ g(z ,w ,u )+ 2ϵ e ,
k+1 k k+1 k k D,k k+1 k+1
(S2)
w = w +γ H(w ,z ),
k+1 k k+1 k k+1
where k ∈ N indexes iterations, ϵ ∈ R+ denotes the learning rate, γ ∈ R+ denotes the step size,
k+1 k+1
e ∼ N(0,I ) is a zero mean standard Gaussian random vector, g(z ,w ,u ) : Rdz ×Rdw ×U → Rdz
k dz k k D,k
denotes an unbiased estimator of ∇ F (z ,w ), U = {1,2,...,n} is the index set of the observations in
z D k k
D, and {u : k = 1,2,...} is a sequence of i.i.d random elements of U with probability measure Q . In
D,k D
general, u can be understood as the index set of a mini-batch sample. In the case that the full dataset
D,k
is used at each iteration, we have u = U for all k.
D,k
To prove the convergence of the adaptive SGLD algorithm (S2), we make the following assumptions.
Assumption A1 The step size sequence {γ k} k∈N is a positive decreasing sequence of real numbers such
that
∞
(cid:88)
lim γ = 0, γ = ∞. (S3)
k k
k→∞
k=1
38There exist δ > 0 and a stationary point w∗ such that for any w ∈ W,
⟨w−w∗,h(w)⟩ ≤ −δ∥w−w∗∥2,
and, in addition,
γ γ −γ
k k+1 k
liminf2δ + > 0, (S4)
k→∞ γ k+1 γ k2 +1
where ∥·∥ denotes the default l -norm.
2
Assumption A2 F (w,z) is M-smooth on w and z with M > 0, and (m,b)-dissipative on z for some
D
constants m > 1 and b > 0. In other words, for any z,z′,z′′ ∈ X and w,w′ ∈ W, the following conditions
are satisfied:
(smoothness) ∥∇ F (w,z′)−∇ F (w′,z′′)∥ ≤ M∥z′−z′′∥+M∥w−w′∥, (S5)
z D z D
(dissipativity) ⟨∇ F (w∗,z),z⟩ ≤ b−m∥z∥2, (S6)
z D
where w∗ is a stationary point as defined in Assumption A1.
Let (w∗,z∗) be a minimizer of F (w,z) and w∗ be a stationary point such that ∇ F (w∗,z∗) = 0.
D z D
By (S6), we have ∥z∗∥2 ≤ b . Therefore,
m
∥∇ F (w,z)∥ ≤ ∥∇ F (w∗,z∗)∥+M∥z∗−z∥+M∥w−w∗∥
z D z D
≤ M∥w−w∗∥+M∥z∥+B,
(cid:113)
where B = M b , and
m
∥∇ F (w,z)∥2 ≤ 3M2∥z∥2+3M2∥w−w∗∥2+3B2. (S7)
z D
Assumption A3 LetR = g(w ,z ,u )−∇ F (w ,z ). AssumethatR ’saremutuallyindependent
k k k D,k z D k k k
white noise, and they satisfy the conditions
E(R |F ) = 0, E∥R ∥2 ≤ δ (M2E∥z ∥2+M2E∥w −w∗∥2+B2), (S8)
k k k g k k
where δ and B are positive constants, and F = σ{w ,x ,w ,x ,...,w ,x } denotes a σ-filtration.
g k 1 1 2 2 k k
Assumption A4 There exist positive constants M and B such that
∥H(w,z)∥2 ≤ M2∥w−w∗∥2+M2∥z∥2+B2.
Lemma S1 (Uniform L2 bounds; Lemma A.2 of [20]) Suppose Assumptions A1-A4 hold, and the learn-
ing rate sequence {ϵ : k = 1,2,...} and the step size sequence {γ : k = 1,2,...} are set in the form:
k k
C C
ϵ γ
ϵ = , γ = ,
k c +kα k c +kβ
ϵ γ
for some constants C > 0, c > 0, C > 0, c > 0, α,β ∈ (0,1], and β ≤ α ≤ min{1,2β}. Then there
ϵ ϵ γ γ
exist constants G and G such that E∥z ∥2 ≤ G and E∥w −w∗∥2 ≤ G for all k = 0,1,2,....
z w k z k w
39Assumption A5 (SolutionofPoissonequation)Foranyw ∈ W, z ∈ X, andafunctionV(z) = 1+∥z∥,
there exists a function µ on X that solves the Poisson equation µ (z)−T µ (z) = H(w,z)−h(w),
w w w w
where T denotes a probability transition kernel with T µ (z) = (cid:82) µ (z′)T (z,z′)dz′, such that
w w w X w w
H(w ,z ) = h(w )+µ (z )−T µ (z ), k = 1,2,.... (S9)
k k+1 k w k k+1 w k w k k+1
Moreover, for all w,w′ ∈ W and z ∈ X, ∥µ (z)−µ (z)∥+∥T µ (z)−T µ (z)∥ ≤ ς ∥w−w′∥V(z)
w w′ w w w′ w′ 1
and ∥µ (z)∥+∥T µ (z)∥ ≤ ς V(z) for some constants ς > 0 and ς > 0.
w w w 2 1 2
Lemma S2 [Theorem A.1 of [20]] Suppose Assumptions A1-A5 hold, and the learning rate sequence
{ϵ : k = 1,2,...} and the step size sequence {γ : k = 1,2,...} are chosen as in Lemma S1. Then there
k k
exists a root w∗ ∈ {w : h(w) = 0} such that
E∥w −w∗∥2 ≤ ξγ , k ≥ k , (S10)
k k 0
whereξ andk aresomeconstantsdeterminedbythesequences{ϵ }and{γ }andtheconstants(δ,δ ,M,B,
0 k k g
m,b,ς ,ς ).
1 2
Proof of Theorem 4.1
Proof: [20] proved the result (S10) for the adaptive Langevinized ensemble Kalman filter (LEnKF)
algorithm,whichisequivalenttoanadaptivepre-conditionedSGLDalgorithm. SincetheSGLDalgorithm
(S2) is a special case of the pre-conditioned SGLD algorithm, this theorem can be proved by following
the proof of [20] with minor modifications. We omit the details of the proof. □
Remark S1 Regarding the convergence rate of w , we note that [20] gives an explicit form of ξ. Refer
k
to Theorem A.1 of [20] for the detail.
§1.2 Proof of Theorem 4.2
Let T =
(cid:80)k
ϵ . Let µ = L(z |w ,D) denote the probability law of z at iteration k of Algorithm
k i=1 k D,T k k k k
1, let ν = L(z(T )|w∗,D) denote the probability law of a continuous time diffusion process, and let
D,T k k
π∗ = π (z|w∗).
D
Lemma S3 Suppose the conditions of Lemma S2 hold. Then there exist some constants C > 0 and
0
C > 0 such that
1
D (µ ∥ν ) ≤ (C δ +C γ )T , (S11)
KL D,T k D,T k 0 g 1 1 k
where D |(·∥·) denotes the Kullback-Leibler divergence.
KL
40Proof: Our proof follows the proof of Lemma 7 in [71] closely, but changing from a constant learning
rate sequence to a decaying learning rate sequence. Similar developments can also be found in Appendix
D of [95].
Let T¯(s) = T for T ≤ s < T ,k = 1,...,∞. Conditioned on D and w, {Z } forms a Markov
k k k+1 k
process. Consider the following continuous-time interpolation of this process:
(cid:90) t √ (cid:90) t
Z¯(t) = Z − g(cid:0) Z¯(T¯(s)),w¯(s),u¯ (s)(cid:1) ds+ 2 dB(s), t ≥ 0, (S12)
0 D
0 0
where w¯(s) = w
k
for T
k
≤ s < T k+1, and {B(s)}
s≥0
is the standard Brownian motion in Rdz. Note that,
for each k, Z¯(T ) and Z have the same probability law µ . Moreover, by a result of [33], the process
k k D,T k
Z¯(t) has the same one-time marginals as the Itˆo process
(cid:90) t √ (cid:90) t
Z′(t) = Z − g (Z′(s))ds+ 2 dB(s), (S13)
0 D,s
0 0
where
g (z) := E(cid:2) g(cid:0) Z¯(T¯(s)),w¯(s),u¯ (s)(cid:1) | Z¯(s) = z(cid:3) . (S14)
D,s D
Crucially, Z′(t) is a Markov process, while Z¯(t) is not.
Let Pt := L(Z′(s) : 0 ≤ s ≤ t | D,w¯(s)) and Pt := L(Z(s) : 0 ≤ s ≤ t | D,w∗). The Radon-
Z′ Z
Nikodym derivative of Pt w.r.t. Pt is given by the Girsanov formula
Z Z′
dPt Z (Z′) = exp(cid:26) 1 (cid:90) t (cid:0) ∇F (Z′(s),w∗)−g (Z′(s))(cid:1)∗ dB(s)
dPt 2 D D,s
Z′ 0
(S15)
−1 (cid:90) t (cid:13) (cid:13)∇F D(Z′(s),w∗)−g D,s(Z′(s))(cid:13) (cid:13)2 ds(cid:27) .
4
0
Using (S15) and the martingale property of the Itˆo integral, we have
D (cid:0) Pt ∥Pt (cid:1) =
−(cid:90)
dPt log
dPt
Z
KL Z′ Z Z′ dPt
Z′
= 1 (cid:90) t E(cid:13) (cid:13)∇F D(Z′(s),w∗)−g D,s(Z′(s))(cid:13) (cid:13)2 ds (S16)
4
0
= 1 (cid:90) t E(cid:13) (cid:13)∇F D(Z¯(s),w∗)−g D,s(Z¯(s))(cid:13) (cid:13)2 ds,
4
0
where the last line follows from the fact that L(Z¯(s)) = L(Z′(s)) for each s.
Recall that T = 0 and T =
(cid:80)k
ϵ . Then, by the definition of g , Jensen’s inequality and the
0 k i=1 k D,s
41M-smoothness of F , we have
D
D KL(cid:16) PT Zk ′∥PT Zk(cid:17) = 1
4
(cid:88)k−1(cid:90) Tj+1 E(cid:13) (cid:13)∇F D(Z¯(s),w∗)−g D,s(Z¯(s))(cid:13) (cid:13)2 ds
j=0
Tj
≤ 1 (cid:88)k−1(cid:90) Tj+1 E(cid:13) (cid:13)∇F D(Z¯(s),w∗)−∇F D(Z¯(T¯(s)),w∗)(cid:13) (cid:13)2 ds
2
j=0
Tj
+ 1 (cid:88)k−1(cid:90) Tj+1 E(cid:13) (cid:13)∇F D(Z¯(T¯(s)),w∗)−g(cid:0) Z¯(T¯(s)),w¯(s),u¯ D(s)(cid:1)(cid:13) (cid:13)2 ds (S17)
2
j=0
Tj
M2 (cid:88)k−1(cid:90) Tj+1
≤ E∥Z¯(s)−Z¯(T¯(s))∥2 ds
2
j=0
Tj
+ 1 (cid:88)k−1(cid:90) Tj+1 E(cid:13) (cid:13)∇F D(Z¯(T¯(s)),w∗)−g(cid:0) Z¯(T¯(s)),w¯(s),u¯ D(s)(cid:1)(cid:13) (cid:13)2 ds.
2
j=0
Tj
To estimate the first summation on the right side of (S17), we consider some s ∈ [T ,T ). By (S12),
j j+1
we have
√
Z¯(s)−Z¯(T ) = −(s−T )g(Z ,w ,u )+ 2(B(s)−B(T ))
j j j j D,j j
√ (S18)
= −(s−T )∇F (Z ,w∗)+(s−T )(∇F (Z ,w∗)−g(Z ,w ,u ))+ 2(B(s)−B(T )).
j D j j D j j j D,j j
Therefore, by (S7), Assumption A3, Lemma S1 and Lemma S2, we have
E∥Z¯(s)−Z¯(T )∥2
j
≤ 3ϵ2 E∥∇F (Z ,w∗)∥2+3ϵ2 E∥∇F (Z ,w∗)−g(Z ,w ,u )∥2+6ϵ d
j+1 D j j+1 D j j j D,j j+1 z
≤ 3ϵ2 E∥∇F (Z ,w∗)∥2+6ϵ d
j+1 D j j+1 z
(cid:16) (cid:17) (S19)
+6ϵ2 E∥∇F (Z ,w∗)−∇F (Z ,w )∥2+E∥∇F (Z ,w )−g(Z ,w ,u )∥2
j+1 D j D j j D j j j j D,j
(cid:16) (cid:17)
≤ 9ϵ2 M2E∥Z ∥2+B2 +6d ϵ +6ϵ2 (ξM2γ +δ (M2G +ξM2γ +B2))
j+1 j z j+1 j+1 j g z j
≤ 9ϵ2 (cid:0) M2G +B2(cid:1) +6d ϵ +6ξM2γ ϵ2 +6δ ϵ2 (M2G +ξM2γ +B2).
j+1 z z j+1 j j+1 g j+1 z j
Consequently, we can bound the first summation on the right-hand side of (S17) as follows:
(cid:88)k−1(cid:90) Tj+1
E∥Z¯(s)−Z¯(T¯(s))∥2 ds
j=0
Tj
k−1 k−1 k−1 k−1
≤ 9(cid:0) M2G +B2(cid:1)(cid:88) ϵ3 +6d (cid:88) ϵ2 +6ξM2(cid:88) γ ϵ3 +6δ (cid:88) ϵ3 (M2G +ξM2γ +B2).
z j+1 z j+1 j j+1 g j+1 z j
j=0 j=0 j=0 j=0
(S20)
42Similarly,byLemmaS2,thesecondsummationontheright-handsideof(S17)canbeboundedasfollows:
(cid:88)k−1(cid:90) Tj+1
E(cid:13) (cid:13)∇F D(Z¯(T¯(s)),w∗)−g(cid:0) Z¯(T¯(s)),w¯(s),u¯ D,s(cid:1)(cid:13) (cid:13)2 ds
j=0
Tj
k−1
(cid:88)
= ϵ E∥∇F (Z ,w∗)−g(Z ,w ,u )∥2
j+1 D j j j D,j
j=0
(S21)
k−1
(cid:88) (cid:16) (cid:17)
≤ 2 ϵ E∥∇F (Z ,w∗)−F (Z ,w )∥2+E∥F (Z ,w )−g(Z ,w ,u )∥2
j+1 D j D j j D j j j j D,j
j=0
k−1 k−1
(cid:88) (cid:88)
≤ 2ξM2 γ ϵ +2δ ϵ (M2G +ξM2γ +B2).
j j+1 g j+1 z j
j=0 j=0
Substituting Equations (S20) and (S21) into (S17), we obtain
k−1 k−1 k−1 k−1
D (cid:16) PT k∥PT k(cid:17) ≤ 9 (cid:0) M4G +M2B2(cid:1)(cid:88) ϵ3 +3M2d (cid:88) ϵ2 +3ξM4(cid:88) γ ϵ3 +ξM2(cid:88) γ ϵ
KL Z′ Z 2 z j+1 z j+1 j j+1 j j+1
j=0 j=0 j=0 j=0
k−1
(cid:88)
+δ ϵ (3M2ϵ2 +1)(M2G +ξM2γ +B2).
g j+1 j+1 z j
j=0
(S22)
Since µ = L(Z |D,w ) and ν = L(Z(t)|D,w∗), the data-processing inequality for the
D,w k,T k k k D,w∗,T k
Kullback-Leibler divergence gives
(cid:16) (cid:17)
D (µ ∥ν ) ≤ D PT k∥PT k
KL D,w k,T k D,w∗,T k KL Z′ Z
k−1 k−1 k−1 k−1
≤9 (cid:0) M4G +M2B2(cid:1)(cid:88) ϵ3 +3M2d (cid:88) ϵ2 +3ξM4(cid:88) γ ϵ3 +ξM2(cid:88) γ ϵ
2 z j+1 z j+1 j j+1 j j+1
j=0 j=0 j=0 j=0
(S23)
k−1
(cid:88)
+δ ϵ (3M2ϵ2 +1)(M2G +ξM2γ +B2)
g j+1 j+1 z j
j=0
≤ (C δ +C γ )T ,
0 g 1 1 k
for some constants C > 0 and C > 0. □
0 1
Assumption A6 The probability law µ of the initial hypothesis w has a bounded and strictly positive
0 0
density p with respect to the Lebesgue measure on R , and
0 dz
(cid:90)
κ := log
e∥w∥2
p (w)dw < ∞.
0 0
Rdz
Lemma S4 Suppose Assumption A6 and the conditions of Lemma S2 hold. Then there exist some
constants C˜ > 0 and C˜ > 0 such that
0 1
W2(µ ,ν ) ≤ (C˜ (cid:112) δ +C˜ √ γ )T2,
2 D,T k D,T k 0 g 1 1 k
where W (·,·) denotes 2-Wasserstein distance.
2
43Proof: The proof of Lemma S4 follows that of Proposition 8 of [71] closely. First, we apply Corollary
2.3 of [9] to get the inequality
(cid:18) (cid:113) (cid:19)
W2(µ ,ν ) ≤ CT D (µ ,ν )+ D (µ ,ν ) , (S24)
2 D,T k D,T k k KL D,T k D,T k KL D,T k D,T k
for some constant C, for which we assume both µ and ν have finite second moments. Further,
D,T D,T
k k
by substituting (S11) into (S24), we can complete the proof. □
Lemma S5 Suppose Assumption A6 and the conditions of Lemma S2 hold. Then there exist some
constants Cˆ > 0, Cˆ > 0 and Cˆ such that
0 1 2
W (µ ,π∗) ≤ (Cˆ δ1/4+Cˆ γ1/4 )T +Cˆ e−T k/cLS,
2 D,T k 0 g 1 1 k 2
where c denotes the logarithmic Sobolev constant of π∗ = π (z|w∗).
LS D
The proof of Lemma S5 follows that of Proposition 10 in [71] closely, and it is thus omitted.
§2 Proof of Theorem 4.3
Notation: We use (x,y,z) to denote a generic observation in the dataset (X ,Y ,Z ).
n n n
Assumption A7 The EFI network satisfies the conditions:
(i) The parameter space W (of w ) is convex and compact.
n n
(ii) E(logπ(y,z|x,w ))2 < ∞ for any w ∈ W .
n n n
Assumption A8 For any positive integer n, the following conditions hold:
(i) Q∗(w ) is continuous in w and uniquely maximized at some point wb;
n n n
(ii) for any ϵ > 0, sup Q∗(w ) exists, where B(ϵ) = {w : ∥w − wb∥ < ϵ}, and δ =
w∈Wn\B(ϵ) n n n n
Q∗(wb)−sup Q∗(w ) > 0.
n wn∈Wn\B(ϵ) n
Proof of Lemma 4.1
Proof: Suppose that π(w |X ,Y ) has a different maximizer that minimizes D (w ) as well. Let
n n n KL n
w† denote such a maximizer, which is different from w∗ but maintains Z∗ ∼ π(z|X ,Y ,w†). For w†,
n n n n n n n
similar to (34), we have
(cid:90)
1 1
G(cid:101)(w n|w† n) :=
n
logπ(Y n,Z∗ n|X n,w n)dπ(Z∗ n|X n,Y n,w† n)+
n
logπ(w n)
1(cid:110) (cid:90) π(Z∗|X ,Y ,w†)
= logπ(w |X ,Y )− log n n n n dπ(Z∗|X ,Y ,w†) (S25)
n n n n π(Z∗|X ,Y ,w ) n n n n
n n n n
(cid:90) (cid:111)
+ logπ(Z∗|X ,Y ,w†)dπ(Z∗|X ,Y ,w†)+c ,
n n n n n n n n
44which, by the non-negativeness of the Kullback-Leibler divergence, implies that w† is also the maximizer
n
of G(cid:101)(w n|w† n). By (35), (36), and Assumption A8, we would have
∥wˆ∗ −w†∥ →p 0, as n → ∞,
n n
following the proof of Lemma 2 in [82]. This contradicts with the uniqueness of wˆ∗.
n
Therefore, if wˆ∗ is unique, then w∗ is unique. Subsequently, we have ∥wˆ∗ −w∗∥ →p 0 as n → ∞,
n n n n
which completes the proof. □
We follow [83] to make the following assumption on the DNN model embedded in the EFI network,
for which the random errors Z are assumed to be known. The sparse DNN has H −1 hidden layers,
n n
and each layer consists of L hidden units. Specifically, we use L and L to denote the input and
j 0 Hn
output dimensions, respectively. The weights and biases of the sparse DNN are specified by w , and the
n
structure of the sparse DNN is specified by Λ , a binary vector corresponding to the elements of w .
n n
Assumption A9 (i) The complete data (x,y,z) is bounded by 1 entry-wisely, i.e. (x,y,z) ∈ Ω =
[−1,1]pn, and the density of (x,y,z) is bounded in its support Ω uniformly with respect to n.
(ii) The underlying true sparse DNN (w˜∗,Λ˜∗) satisfies the following conditions:
n n
(ii-1) The network structure satisfies: r H logn+r logL+s logp ≤ C n1−ε, where 0 < ε < 1 is a
n n n n n 0
small constant, r denotes the connectivity of Λ˜∗, L = max L denotes the maximum
n n 1≤j≤Hn−1 j
hidden layer width, and s denotes the input dimension of Λ˜∗.
n n
(ii-2) The network weights are polynomially bounded: ∥w˜∗ n∥
∞
≤ E n, where E
n
= nC1 for some
constant C > 0.
1
(iii) The activation function ψ used in the DNN is Lipschitz continuous with a Lipschitz constant of 1.
(iv) The mixture Gaussian prior (32) satisfies the conditions: ρ
n
= O(1/{K n[nHn(Lp n)]τ′}) for some
constant τ′ > 0, E /{H logn + logL}1/2 ≲ σ ≲ nα′ for some constant α′ > 0, and σ ≲
n n 1,n 0,n
√ √
min(cid:8) 1/{ nK n(n3/2σ 1,0/H n)Hn}, 1/{ nK n(nE n/H n)Hn}(cid:9) , where K
n
= (cid:80)H h=n 1(L
h−1
×L
h
+L h)
denotes the total number of parameters of the fully connected DNN.
(v) For the normal regression case, y = f(x,θ )+σz with z ∼ N(0,1), the function f(x,θ ) is Lipschitz
0 0
continuous with respect to θ ; and for the logistic regression case, log(P(Y = 1)/(1−P(Y = 1))) =
0
µ(x,θ), the logit link function µ(x,θ) is Lipschitz continuous with respect to θ.
Remark S2 If we further assume that the exponent 0 ≤ C < 1 and the connectivity r = O(nζ′) for
1 2 n
some 0 < ζ′ < 1 −C −ε′ and 0 < ε′ < 1 −C −ζ′. Then, based on the proof of Theorem 1 and the
2 1 2 1
followed remark in [48], it is easy to figure out that K is allowed to increase with the sample size n in
n
an exponential rate: K ≺ exp(n2ε′). By the arguments provided in [83], the sparse DNN approximation
n
45under the above assumptions is achievable for quite a few classes of functions, such as bounded α-H¨older
smooth functions [75], piecewise smooth functions with fixed input dimensions [68], and the functions that
can be represented by an affine system [8].
Remark S3 Assumption A9-(i) restricts Ω, the domain of the complete data (x,y,z), to a bounded set
[−1,1]pn. To satisfy this condition, we can add a data transformation/normalization layer to the DNN
model, ensuring that the transformed input values fall within the set Ω. In particular, the transforma-
tion/normalization layer can form an 1-1 mapping and contain no tuning parameters. For example, when
dealing with the standard Gaussian random variable, we can transform it to be uniform over (0,1) via the
probability integral transformation Φ(z), where Φ(·) denotes the CDF of the standard Gaussian random
variable.
For the EFI network, we define
1 1
h (w ) = logπ(Y ,Z∗|X ,w )+ logπ(w ), (S26)
n n n n n n n n n
where Z∗ is the true random errors realized in the data (X ,Y ) and it is thus independent of w .
n n n n
Then the posterior density of w is given by π(w |X ,Y ,Z∗) = enhn(wn) and, for a function
n n n n n (cid:82) enhn(wn)dwn
b(w ), the posterior expectation is given by (cid:82) b(wn)enhn(wn)dwn. Recall that we have defined wˆ∗ =
n (cid:82) enhn(wn)dwn n
argmax π(w |X ,Y ,Z∗), which is also the global maximizer of h (w ). Let B (w ) denote an
wn n n n n n n δ n
Euclidean ball of radius δ centered at w . Let h (w ) denote the d-th order partial derivative
n i1,i2,...,i
d
n
∂dh(wn)
, let H (w ) denote the Hessian matrix of h (w ), let h denote the (i,j)-th component of
∂wni1∂wni2···∂wnid n n n n ij
the Hessian matrix, and let hij denote the (i,j)-component of the inverse of the Hessian matrix. Recall
that Λ˜∗ denotes the set of indicators for the connections of the true sparse DNN, r denotes the size of
n
the true sparse DNN, and K denotes the size of the fully connected DNN.
n
Assumption A10 There exist positive numbers ϵ, M, and n such that for any n > n , the function
0 0
h (w ) in (S26) satisfies the following conditions:
n n
(i) |h (wˆ∗)| < M hold for any w ∈ B (wˆ∗) and any 1 ≤ i ,...,i ≤ K , where 3 ≤ d ≤ 4.
i1,...,i d n n ϵ n 1 d n
(ii) |hij(wˆ∗)| < M if Λ˜∗ = Λ˜∗ = 1 and |hij(wˆ )| = O( 1 ) otherwise, where Λ˜∗ denotes the i-th
n n,i n,j n K2 n,i
n
element of Λ˜ .
n
(iii) det(− 2n πH n(wˆ n))1 2 (cid:82) RKn\B δ(wˆn)en(hn(wn)−hn(wˆn))dw n = O(r nn4 ) = o(1) for any 0 < δ < ϵ.
AssumptionA10-(i)&(iii)aretypicalconditionsforLaplaceapproximation, seee.g., [32]. Assumption
A10-(ii) requires the inverse Hessian to have very small values for the elements corresponding to the false
connections. Refer to [83] for its justification.
46Proof of Theorem 4.3
Proof: As discussed in Section 4, we have the likelihood function for the EFI network as
π(Y |X ,Z ,w) = Ce−λUn(Zn,w;Xn,Yn).
n n n
In the context of this proof, we assume that Z = (z ,z ,...,z )T is known. Additionally, we use
n 1 2 n
d (p,p∗) = t−1((cid:82) p∗(p∗/p)t−1) to denote a divergence measure for two distributions p and p∗. It is easy
t
to see that d converges to the KL-divergence as t ↓ 0.
t
Normal Regression For the normal linear/nonlinear regression, we essentially have the energy func-
tion:
n
(cid:88)
U (Z ,w;X ,Y ) = ∥y −f(x ,θ )−σz ∥2, (S27)
n n n n i i 0 i
i=1
as λ → ∞, where θ := (θ ,log(σ)) = G(x ,y ,z ,w) is a constant function over the observations
0 i i i
{(y ,x ,z ) : i = 1,2,...,n}. Therefore, as λ → ∞, (S27) enforces the output θ of the DNN to sat-
i i i
isfy the relationship:
y = f(x ,θ )+σz , i = 1,2,...,n,
i i 0 i
i.e., y ∼ N(f(x,θ ),σ2). A direct calculation shows that the divergence d (·,·) of two Gaussian distribu-
0 1
tions p(x) := N(f(x,θ ),σ2) and q(x) := N(f(x,θ′),ς2) is given by
0 0
ς2/σ ∥f(x,θ0)−f(x,θ′ 0)∥2
d 1(q,p) = √ e 2ς2−σ2 −1,
2ς2−σ2
provided that 2ς2−σ2 > 0. The divergence d (·,·) is a function of the two factors ∥f(x,θ )−f(x,θ′)∥2
1 0 0
and |log(σ)−log(ς)|. In particular, if both the factors goes to 0, then d (·,·) goes to 0. Therefore, to
1
bound the value of d (·,·), one can bound
1
∥f(x,θ )−f(x,θ′)∥2+|log(σ)−log(ς)|2 = O(∥θ−θ′∥2) = O(∥G(x,y,z,w)−G(x,y,z,w′)∥2),
0 0
providedthatf(x,θ )isLipschitzcontinuouswithrespecttoθ ,whereθ′ = G(x,y,z,w′)andw′ denotes
0 0
the corresponding DNN weights. This result implies that as λ → ∞, the posterior consistency for the
DNN model in the EFI network can be studied as for a conventional normal regression DNN model with
input variables (x,y,z) and the output variable θ, provided that f(x,θ ) is Lipschitz continuous with
0
respect to θ . Therefore, by Theorem 2.1 of [83], the posterior consistency holds for the DNN model
0
under Assumption A9.
47Logistic Regression For logistic regression, the reasoning is similar. As implied by (S38), we essen-
tially have the following probability mass function for a generic observation (x,y,z):
p (y|z,x,θ) ∝ exp{−λρ((z−µ)(2y−1))}, (S28)
λ
where θ = G(x,y,z,w) is a constant function over the observations {(y ,x ,z ) : i = 1,2,...,n}, and
i i i
µ = µ(x,θ) = xTθforthelinearcase. Asλ → ∞,thetwoevents{Z < µ}and{Y = 1}areasymptotically
equivalent, i.e., {Z < µ} ⇐⇒ {Y = 1} with probability 1. Due to the monotonicity of the function
(cid:110) (cid:111)
1 , 1 < 1 ⇐⇒ {Y = 1} with probability 1 as λ → ∞. Furthermore, since Z follows
1+e−z 1+e−Z 1+e−µ(x,θ)
the logistic distribution, 1 is uniform on (0,1). Therefore, as λ → ∞, (S28) enforces the output θ of
1+e−Z
the DNN model to satisfy the following relationship:
µ(x,θ) = log(P(Y = 1)/(1−P(Y = 1))). (S29)
Following the calculation in [49], the divergence d (·,·) (up to a multiplicative constant) of two logistic
1
distributions, with respective logit link functions µ(x,θ) and µ(x,θ′), is given by
∥µ(x,θ)−µ(x,θ′)∥2 = O(∥θ−θ′∥2) = O(∥G(x,y,z,w)−G(x,y,z,w′)∥2),
provided that µ(x,θ) is Lipschitz continuous with respect to θ, where θ′ = G(x,y,z,w′) and w′ denotes
the corresponding DNN weights. This result implies that as λ → ∞, the posterior consistency for the
DNN model in the EFI network can be studied as for a conventional logistic regression DNN model with
inputvariables(x,y,z)andtheoutputvariableθ,providedthatthelogitlinkfunctionµ(x,θ)isLipschitz
continuous with respect to θ. Therefore, by Theorem 2.1 of [83], the posterior consistency holds for the
EFI network under Assumption A9.
Furthermore, by Assumption A7, the parameter space W is compact and convex. Therefore, for any
n
bounded function b(w ), the posterior mean E(b(w )) is a consistent estimator of b(w˜∗) under posterior
n n n
consistency. For the inverse mapping estimator gˆ(x,y,z,w ), by Assumption A7-(i) and Assumption
n
(A9)-(i), it is bounded and
(cid:12) (cid:12) ∂dgˆ(x,y,z,w n) (cid:12) (cid:12)
|gˆ i1,...,i d(x,y,z,w n)| = (cid:12) (cid:12)∂wi1∂wi2···∂wi d(cid:12) (cid:12) < M,
n n n
holds for some constant M, for any 1 ≤ d ≤ 2 and 1 ≤ i ,...,i ≤ K . Then, under Assumption A10 and
1 d n
by Theorem 2.3 of [83], gˆ(x,y,z,wˆ∗) (as an approximator to the posterior mean Eg(x,y,z,w )) forms a
n n
consistent estimator of θ∗.
Finally,byLemma4.1,∥wˆ∗−w∗∥ →p 0holds,whichimpliesgˆ(x,y,z,w∗)isalsoaconsistentestimator
n n n
of θ∗. This completes the proof. □
48§3 Derivation of EFD for a Regression Example
Consider the linear regression model as defined in equation (3), where β ∈ Rp−1. For an illustrative
purpose, we assume that σ2 is known. We set
G(Y ,X ,z) = G˜(Y ,X ,z) = (XTX )−1XT(Y −σz),
n n n n n n n n
and the energy function
U (z) = ∥Y −f(X ,z,G(Y ,X ,z))∥2.
n n n n n
Let R = I −X (XTX )−1XT, which is an idempotent matrix of rank n−p+1. Then
n n n n n n
J(z) =Y −f(X ,z,G(Y ,X ,z))
n n n n
=Y −X G(Y ,X ,z)−σz (S30)
n n n n
=R (Y −σz).
n n
Let (v ,...,v ) be the eigenvectors corresponding to the zero eigenvalues of R , i.e. R v = 0 ∈ Rn
1 p−1 n n i
for i = 1,2,...,p−1. Let (v ,...,v ) be the eigenvectors corresponding to the nonzero eigenvalues of
p n
R . Let V = (v ,...,v ) ∈ Rn×(p−1) and V = (v ,...,v ) ∈ Rn×(n−p+1). Then it is clear that
n 1 1 p−1 2 p n
(cid:26) (cid:27)
1
{z : J(z) = 0} = Y −V u : u ∈ Rp−1 . (S31)
n 1
σ
For any vector z ∈ Rn, we can write down the exact form of the decomposition in (13) as:
1
z = Y −V u−V t, (S32)
n 1 2
σ
where u ∈ Rp−1 and t ∈ Rn−p+1. Then, for U (z) = ∥J(z)∥2, we have
n
∇2U (z) = 2VTRTR V . (S33)
t n 2 n n 2
Note that
rank(∇2U (z)) = rank(R V ) = rank(R (V ,V )) = rank(R ) = n−p+1. (S34)
t n n 2 n 1 2 n
Therefore, det∇2U (z) is a positive constant. Furthermore, for any z ∈ Z , it can be written as
t n n
z = 1Y −V u for some u ∈ Rp−1, and the limiting measure has the form
σ n 1
1 1
p∗(z|X ,Y ) = p∗( Y −V u|X ,Y ) ∝ π⊗n( Y −V u), (S35)
n n n n σ n 1 n n 0 σ n 1
which corresponds to a truncation of π⊗n(·) on the manifold Z . Therefore,
0 n
1
Y −V u ∼ N(0,I ).
n 1 n
σ
For any z ∈ Z , we set
n
1
G˜(Y ,X ,z) = (XTX )−1XT(Y −σ( Y −V u)), (S36)
n n n n n n σ n 1
49and the resulting EFD is given by
µ∗(β|Y ,X ) = N(βˆ,σ2(XTX )−1),
n n n n n
where βˆ = (XTX )−1XTY .
n n n n
§4 More Numerical Results
§4.1 Nonlinear Regression
Nonlinear least squares regression problems are intrinsically hard due to their complex energy land-
scapes, which may contains some saddle points, local minima or pathological curvatures. To test the
performance of EFI on nonlinear regression, we took a benchmark dataset, Gauss2, at NIST Statistical
Reference Datasets (https://www.itl.nist.gov/div898/strd/nls/nls_main.shtml), which consists
of 250 observations. The nonlinear regression function of the example is given by
(cid:26)
(x−β
)2(cid:27) (cid:26)
(x−β
)2(cid:27)
4 7
y = β exp{−β x}+β exp − +β exp − +ϵ := f(x,θ)+ϵ, ϵ ∼ N(0,6.25),
1 2 3 β2 6 β2
5 8
(S37)
where θ = (β ,β ,...,β ) denotes the vector of unknown parameters. The nonlinear regression function
1 2 8
represents two slightly-blended Gaussian density curves on a decaying exponential baseline plus normally
distributed zero-mean noise with known variance 6.25. For this example, the “best-available” OLS solu-
tion has been given as shown in Table S1, which was obtained using 128-bit precision and confirmed by
at least two different algorithms and software packages using analytic derivatives.
The EFI method was applied to this example with the experimental settings given in Section §5.5 of
this supplement. Table S1 compares the parameter estimates and confidence intervals by the OLS and
EFI methods. For OLS, the confidence intervals are constructed by Wald’s method with the estimates’
standard deviations given in the website. For EFI, the parameter estimates are obtained by averaging
the fiducial θ¯-samples collected in the simulation, and the confidence intervals are constructed with 2.5%
and 97.5% quantiles of the fiducial samples. Therefore, the EFI confidence intervals are not necessarily
symmetric about the parameter estimates. The comparison shows that the EFI confidence intervals tend
to be shorter than the OLS confidence intervals. More importantly, since EFI and OLS employ different
objective functions, they actually converge to different solutions. This can be seen from the confidence
intervals of β resulting from the two methods, which have no overlaps.
3
To further explore the difference of the EFI and OLS solutions, we examined their fitting and residual
plots in Figure S1. The right plot indicates that EFI tends to have larger residuals than OLS. A simple
calculation shows that the OLS has a mean-squared-residuals of 4.99, while the EFI has a mean-squared-
residuals of 5.72, which is closer to the ideal value 6.25. This comparison implies that OLS, which simply
minimizes the sum of squared fitting errors, can lead to an overfitting issue even for this reasonably large
50Table S1: Parameter estimates and confidence intervals of the EFI and “best-available” OLS solutions
for the Gauss2 example.
“Best-available” OLS EFI
Parameter Estimate CI-width 95% CI Estimate CI-width 95% CI
β 99.0183 2.1070 (97.9649, 100.0718) 98.8713 1.0243 (98.3466, 99.3710)
1
β 0.0110 0.0005 (0.0107,0.0113) 0.0109 0.0004 (0.0108, 0.0111)
2
β 101.8802 2.3213 (100.7196,103.0409) 99.2748 1.2274 (98.6559, 99.8832)
3
β 107.0310 0.5883 (106.7368,107.3251) 107.0377 0.6427 (106.6962, 107.3389)
4
β 23.5786 0.8897 (23.1338,24.0234) 23.5636 0.8447 (23.1306, 23.9753)
5
β 72.0456 2.4195 (70.8358,73.2553) 72.5515 0.8255 (72.1315, 72.9570)
6
β 153.2701 0.7631 (152.8886,153.6516) 153.2575 0.7788 (152.8596, 153.6383)
7
β 19.5260 1.0355 (19.0082,20.0437) 19.6559 1.0776 (19.1351, 20.2127)
8
dataset. EFI performs better in this regard by striking a balance between fitting errors and the likelihood
of random errors, as discussed in Section 3.4 of the main text. This balance potentially results in a
solution of higher fidelity.
For this example, we have also tried the Bayesian method, which leads to almost the same solution
as OLS, as they essentially employ the same objective function.
FigureS1: ComparisonoftheEFIsolutionwiththe“best-available”OLSsolutionfortheGauss2example:
(left) fitting curves and (right) scatter plot of residuals.
51§4.2 Logistic Regression
TheEFImethodcanbeeasilyextendedtodiscretestatisticalmodelsviaapproximationortransformation.
For example, for the logistic regression, whose response variable y ∈ {0,1} is discrete, making the fitting-
i
error term in (24) and (26) less well defined. To address this issue, we define ReLU function: ρ(δ) = δ if
δ > 0 and 0 otherwise, and then replace the fitting-error term in (24) and (26) by
n
(cid:88) (cid:16) (cid:17)
ρ (z −xTθˆ )(2y −1) , (S38)
i i i i
i=1
where z ,z ,...,z i ∼id Logistic(0,1) with the CDF given by F(z) = 1/(1+e−z). That is, z represents
1 2 n i
the random error realized in the observation (x ,y ). Correspondingly, the fitted value y˜ is defined by
i i i
y˜ = 1 if F(z ) ≤ F(xTθˆ ) and 0 otherwise, where F(z ) ∼ Uniform(0,1) by the probability integral
i i i i i
transformation theorem. It is easy to see that (S38) penalizes the cases {i : y ̸= y˜} and the resulting
i i
energyfunctionsatisfies Assumption3. In particular, wecanhaveΠ (Z ) > 0forthis problem, because
n Uˇ
n
each z can take any value in an interval (−∞,a] or [b,∞) (for some a,b ∈ R) while maintaining the zero
i
total-fitting-error given in (S38).
TableS2: ComparisonofMLEandEFIforinferenceoflogisticregression,wherecoveragerate(confidence
length) is reported for each parameter.
Method θ θ θ θ θ Average
0 1 2 3 4
MLE 0.94 (0.370) 0.96 (0.393) 0.96 (0.389) 0.93 (0.390) 0.96 (0.394) 0.95
EFI 0.95 (0.378) 0.95 (0.390) 0.95 (0.388) 0.94 (0.388) 0.97 (0.393) 0.952
We simulated 100 datasets from a logistic regression consisting of 4 covariates independently drawn
from N(0,1). The true regression coefficients were θ = (θ ,θ ,...,θ ) = (1,1,1,−1,−1), including the
0 1 4
interpret θ . The sample size of each dataset was n = 1000. The numerical results are summarized in
0
Table S2. The comparison with the MLE results indicates the validity of EFI for statistical inference of
logistic regression.
For comparison, we applied GFI to this example by running the R package gfilogisreg [42], but which
did not produce results for this example due to a computational instability issue suffered by the package.
For IM, we refer to [56], where the likelihood function is used for inference of θ and the confidence
intervals are constructed by inverting the Monte Carlo hypothesis tests conducted on a lattice of grid
points in Θ. For example, if we take 50 grid points in each dimension of Θ and simulate 1000 samples at
each grid point, then we need to simulate a total of 3.125×1011 samples. This is time consuming even
for such a 5-dimensional problem.
For multiclass logistic regression, similar to (S38), the fitting-error term in (24) and (26) can be
52defined as
n
(cid:88)(cid:104) (cid:88) (cid:105)
ρ(xTθˆ −xTθˆ )+ρ(z −xTθˆ ) , (S39)
i i,j i i,mi i i i,mi
i=1 j̸=mi
where m denotes the true class of the training sample x , and θˆ denotes the parameter corresponding
i i i,j
to class j for the training sample x .
i
§4.3 Semi-Supervised Learning
Table S3 presents more examples for the semi-supervised learning.
Table S3: EFI results for different datasets, where the labels of 50% training samples were removed in
each run of the 5-fold cross validation.
Dataset n p Full Labeled only Semi
Divorce 170 54 98.824±1.052 97.647±1.289 98.824±1.052
Diabetes 520 16 89.615±1.032 88.462±1.088 88.846±1.668
Breast Cancer 699 9 96.52±0.661 95.942±0.485 96.232±0.518
Raisin 900 6 85.333±0.795 85.333±0.659 85.556±0.994
53§4.4 EFI for Complex Hypothesis Tests
(a) n=500 (b) n=1000 (c) n=2000
EFI EFI EFI
bootstrap bootstrap bootstrap
mm−opt mm−opt mm−opt
MaxP MaxP MaxP
Sobel Sobel Sobel
1.0 1.5 2.0 2.5 3.0 1.0 1.5 2.0 2.5 3.0 1.0 1.5 2.0 2.5 3.0
Figure S2: Graphical representation of Table 6, where ‘1’, ‘2’ and ‘3’ in x-axis represent the experimental
settings (β,γ) = (0.2,0), (β,γ) = (0,0.2) and (β,γ) = (0,0), respectively.
54
rorre
I epyT
51.0
01.0
50.0
00.0
rorre
I epyT
51.0
01.0
50.0
00.0
rorre
I epyT
51.0
01.0
50.0
00.0(a) n=500 (b) n=1000 (c) n=2000
EFI EFI EFI
bootstrap bootstrap bootstrap
mm−opt MaxP MaxP
MaxP Sobel Sobel
Sobel
1.0 1.5 2.0 2.5 3.0 1.0 1.5 2.0 2.5 3.0 1.0 1.5 2.0 2.5 3.0
Figure S3: Graphical representation of Table 7, where ‘1’, ‘2’ and ‘3’ in x-axis represent the experimental
settings (β,γ) = (0.1,0.4), (β,γ) = (−0.1,0.4) and (β,γ) = (0.2,0.2), respectively.
§5 Experimental Setting
To enforce a sparse DNN to be learned for the inverse function g(·), we impose the following mixture
Gaussian prior on each element of w :
n
π(w) ∼ ρN(0,σ2)+(1−ρ)N(0,σ2), (S40)
1 0
where w denotes a generic element of w and, unless stated otherwise, we set ρ = 1e−2, σ = 1e−5
n 0
and σ = 0.02. The elements of w are a priori independent.
1 n
In all experiments of this paper, we use ReLU as the activation function, and set the learning rate
sequence {ϵ } and the step size sequence {γ } in the forms given in Theorem 4.1. Specifically, we set
k k
α = 13/14 and β = 4/7 unless stated otherwise, and set different values of C , c , C and c for different
ϵ ϵ γ γ
experiments as given below.
§5.1 Linear regression
For both cases with known and unknown σ2, we use a DNN with structure 12−300−100−d for inverse
θ
function approximation, where d denotes the dimension of θ.
θ
55
rewop
0.1
8.0
6.0
4.0
2.0
0.0
rewop
0.1
8.0
6.0
4.0
2.0
0.0
rewop
0.1
8.0
6.0
4.0
2.0
0.0Knownσ2 ForEFI-a,wesetη = 100andλ = 10;andforEFI,wesetη = 10andλ = 10. EFI-aandEFI
share the same learning rate and step size sequences with (C ,c ,C ,c ) = (50000,10000,5000,100000).
ϵ ϵ γ γ
We set the burn-in period K = 1000 and the iteration number M = 100,000. The Markov chain is
thinned by a factor of B = 10 in sample collection, i.e., M/B = 10,000 θ¯-samples were collected for
calculation of the coverage rates and CI-widths. For EFI, we employ the same parameter settings for
different activation functions.
Unknown σ2 For EFI, we used SGHMC in latent variable sampling, i.e., we simulate Z(k+1) in the
following formula:
(cid:112)
V(k+1) = (1−ζ)V(k)+ϵ ∇ logπ(Z(k)|X ,Y ,w(k))+ 2ζτϵ e(k+1),
n n k+1 Zn n n n k+1
(S41)
Z(k+1) = Z(k)+V(k+1),
n n n
where τ = 1, 0 < ζ ≤ 1 is the momentum parameter, e(k+1) ∼ N(0,I ), and ϵ is the learning rate. It
d k+1
is worth noting that the algorithm is reduced to SGLD if one sets ζ = 1.
In simulations, we set the decaying parameters α = β = 4/7, and the Markov chain is thinned by
a factor of B as below for M/B = 10,000 samples were used for calculation of the coverage rates and
CI-widths.
• (η = 2,λ = 30) : We set ζ = 0.025 and (C ,c ,C ,c ) = (6500,100000,1700,100000), (K,M) =
ϵ ϵ γ γ
(10000,50000) thinned by B = 5;
• (η = 2,λ = 40) : We set ζ = 0.025 and (C ,c ,C ,c ) = (5600,100000,1400,100000), (K,M) =
ϵ ϵ γ γ
(10000,90000) thinned by B = 9;
• (η = 2,λ = 50) : We set ζ = 0.05 and (C ,c ,C ,c ) = (4000,100000,1000,100000),, (K,M) =
ϵ ϵ γ γ
(10000,200000) thinned by B = 20;
• (η = 4,λ = 50) : We set ζ = 0.005 and (C ,c ,C ,c ) = (1950,80000,490,80000),, (K,M) =
ϵ ϵ γ γ
(10000,120000) thinned by B = 12;
§5.2 Behrens-Fisher problem
We use a DNN with structure 2-20-10-2 and set η = 5 and λ = 20. The burn-in period K = 10000, the
iteration number M = 40000 and 60000 for n = 50 and 500, respectively. The Markov chain is thinned
by a factor of B = 4 and 6 for n = 50 and 500, respectively, in sample collection. This makes that
M/B = 10,000 samples are used in calculation of the coverage rates and CI-widths for each case.
• σ2 = 0.25,σ2 = 1 :(i)forn = 50,wesetζ = 0.01,and(C ,c ,C ,c ) = (2500,100000,2500,100000);
1 2 ϵ ϵ γ γ
(ii) for n = 500, we set ζ = 0.005, and (C ,c ,C ,c ) = (3000,100000,3000,100000);
ϵ ϵ γ γ
56• σ2 = 1, σ2 = 1 : (i) for n = 50, we set ζ = 0.05, and (C ,c ,C ,c ) = (2800,100000,2800,100000);
1 2 ϵ ϵ γ γ
(ii) for n = 500, we set ζ = 0.028, and (C ,c ,C ,c ) = (3100,100000,3100,100000).
ϵ ϵ γ γ
§5.3 Bivariate normal
We used a DNN with structure 4-80-20-5 for inverse function approximation, and we set η = 2 and
λ = 50. We used SGHMC in latent variable sampling as in (S41) . In simulations, we set the momentum
parameter ζ = 0.1, the decaying parameters α = β = 4/7, (C ,c ,C ,c ) = (4500,100000,1100,100000).
ϵ ϵ γ γ
the burn-in period K = 10000, the iteration number M = 50000, and the Markov chain is thinned by a
factorofB = 5insamplecollection, i.e., M/B = 10,000sampleswereusedforcalculationofthecoverage
rates and CI-widths.
§5.4 Fidelity in Parameter Estimation
We used a DNN with structure 12-300-100-11 for inverse function approximation, and set (η,λ) = (2,50).
The tempering SGLD algorithm is used in the latent variable sampling step, where we set the tem-
perature sequence τ = max(100 ∗ (0.9999)t,1). For the learning rate and step size sequences, we set
t
(C ,c ,C ,c ) = (50000000,10000000,50,10000). For sample collections, we set K = 50,000, M =
ϵ ϵ γ γ
150,000, and B = 15.
§5.5 Nonlinear Regression in the Supplement
We used a DNN with structure 3-150-50-8 for inverse function approximation, and we set (η,λ) =
(500,0.2) in order to avoid a local trap of fitting z to y . For the learning rate and step size se-
n n
quences, we set (C ,c ,C ,c ) = (1,10000000,1,100) for iterations t < 50,000, and set (C ,c ,C ,c ) =
ϵ ϵ γ γ ϵ ϵ γ γ
(1000,100000,10,10000) for t ≥ 50,000. For sample collections, we set K = 60,000, M = 150,000 and
B = 15.
§5.6 Logistic regression in the Supplement
For EFI, we set η = 2 and λ = 1000. We used SGHMC (S41) in latent variable sampling. In simulations,
we set the momentum parameter ζ = 0.01, the decaying parameters α = β = 2/7, (C ,c ,C ,c ) =
ϵ ϵ γ γ
(50000,100000,30000,100000). the burn-in period K = 10000, the iteration number M = 50000, and the
Markov chain is thinned by a factor of B = 5 in sample collection, i.e., M/B = 10,000 samples were used
for calculation of the coverage rates and CI-widths.
§5.7 EFI for Semi-Supervised Learning
WeusedaDNNwithstructure(p+2)−90−30−pforinversefunctionapproximation,wherepcorresponds
to the dimension of x for all cases. For EFI on both full label cases, and labeled-data only cases (use
5750% of training data), we set α = β = 2,η = 5,λ = 200,K = 10000,M = 40000,B = 4 with ζ = 0.1,
7
(C ,c ,C ,c ) = (100000,100000,2000,100000). For semi-supervised EFI, the same parameter settings
ϵ ϵ γ γ
have been used with the exceptions given as follows:
• Beast-Cancer: (η,λ) = (5,200);
• Diabetes: (η,λ) = (2,500) and (C ,c ,C ,c ) = (200000,100000,1000,100000);
ϵ ϵ γ γ
• Divorce: (η,λ) = (10/3,300);
• Raisin: (η,λ) = (2,500).
§5.8 EFI for Complex Hypothesis Tests
We used a DNN with structure 7-180-30-9 for inverse function approximation, and set α = β = 4,η =
7
10,λ = 10,K = 10000,M = 50000,B = 5. Inaddition,wevariedthevaluesofotherparametersaccording
to the problem and sample size.
Type-I error For different sample sizes, we set the parameters as follows:
• n = 500. For case 1, we set ζ = 0.1 and (C ,c ,C ,c ) = (290000,100000,4000,100000); for case 2,
ϵ ϵ γ γ
we set ζ = 0.1 and (C ,c ,C ,c ) = (100000,100000,2000,100000); for case 3, we set ζ = 0.1 and
ϵ ϵ γ γ
(C ,c ,C ,c ) = (100000,100000,2000,100000).
ϵ ϵ γ γ
• n = 1000. For case 1, we set ζ = 0.1 and (C ,c ,C ,c ) = (100000,100000,4000,100000); for case
ϵ ϵ γ γ
2, we set ζ = 1 and (C ,c ,C ,c ) = (200000,100000,4000,100000); for case 3, we set ζ = 0.1 and
ϵ ϵ γ γ
(C ,c ,C ,c ) = (2000,100000,1000,100000).
ϵ ϵ γ γ
• n = 2000. For case 1 and case 2, we set ζ = 1 and (C ,c ,C ,c ) = (200000,100000,4000,100000);
ϵ ϵ γ γ
and for case 3, we set ζ = 0.1 and (C ,c ,C ,c ) = (2000,100000,1000,100000).
ϵ ϵ γ γ
Power For all cases, we set the parameters ζ = 0.1 and (C ,c ,C ,c ) = (2000,100000,1000, 100000).
ϵ ϵ γ γ
58References
[1] Baron, R. M. and Kenny, D. A. (1986), “The moderator-mediator variable distinction in social
psychological research: conceptual, strategic, and statistical considerations.” Journal of personality
and social psychology, 51 6, 1173–82.
[2] Bartlett, M. S. (1936), “The information available in small samples,” Mathematical Proceedings of
the Cambridge Philosophical Society, 32, 560 – 566.
[3] Beaumont, M. A., Zhang, W., and Balding, D. J. (2002), “Approximate Bayesian Computation in
Population Genetics,” Genetics, 162, 2025–2035.
[4] Behrens, W. (1929), “Ein Beitrag zur Fehlerberechnung bei wenige Beobachtungen,” Land-
wirtschaftliches Jahresbuch, 68, 807–837.
[5] Bengio, Y., Delalleau, O., and Roux, N. L. (2006), “Label Propagation and Quadratic Criterion,” in
Semi-Supervised Learning, eds. Chapelle, O., Schlk¨opf, B., and Zien, A., MIT Press, chap. 11, pp.
193–216.
[6] Bennett, G. (1969), “On the Fiducial Distribution of the Parameters of the Bivariate Normal Dis-
tribution,” Sankhya, 31, 195–198.
[7] Berger, J. O. (2006), “The case for objective Bayesian analysis,” Bayesian Analysis, 1, 385–402.
[8] Bolcskei,H.,Grohs,P.,Kutyniok,G.,andPetersen,P.(2019),“Optimalapproximationwithsparsely
connected deep neural networks,” SIAM Journal on Mathematics of Data Science, 1, 8–45.
[9] Bolley, F. and Villani, C. (2005), “Weighted Csisz´ar-Kullback-Pinsker inequalities and applications
to transportation inequalities,” Annales de la Facult´e des sciences de Toulouse: Math´ematiques, 14,
331–352.
[10] Brubaker,M.A.,Salzmann,M.,andUrtasun,R.(2012),“AFamilyofMCMCMethodsonImplicitly
Defined Manifolds,” in International Conference on Artificial Intelligence and Statistics.
[11] Chapelle, O., Schlkopf, B., and Zien, A. (2006), Semi-Supervised Learning, Cambridge, Mass.: MIT
Press.
[12] Chen, T., Fox, E., and Guestrin, C. (2014), “Stochastic gradient hamiltonian monte carlo,” in
International conference on machine learning, pp. 1683–1691.
[13] Cui, H., Li, R., and Zhong, W. (2015), “Model-Free Feature Screening for Ultrahigh Dimensional
Discriminant Analysis,” Journal of the American Statistical Association, 110, 630 – 641.
59[14] Dawid, A. P., Stone, M., and Zidek, J. V. (1973), “Marginalization Paradoxes in Bayesian and
Structural Inference,” Journal of the Royal Statistical Society, Series B, 35, 189–233.
[15] Delalleau, O., Bengio, Y., and Roux, N. L. (2004), “Efficient Non-Parametric Function Induction in
Semi-Supervised Learning,” in International Conference on Artificial Intelligence and Statistics.
[16] Dempster, A. P. (1967), “Upper and lower probabilities induced by a multivalued mapping,” Ann.
Math. Statist., 38, 325–339.
[17] — (2008), “The Dempster-Shafer calculus for statisticians,” Int. J. Approx. Reason., 48, 365–377.
[18] Deng, W., Zhang, X., Liang, F., and Lin, G. (2019), “An adaptive empirical Bayesian method for
sparse deep learning,” Advances in neural information processing systems, 32.
[19] Diaconis, P., Holmes, S. P., and Shahshahani, M. (2013), “Sampling From A Manifold,” Advances in
Modern Statistical Theory and Applications: A Festschrift in honor of Morris L. Eaton, 10, 102–125.
[20] Dong, T., Zhang, P., and Liang, F. (2022), “A Stochastic Approximation-Langevinized Ensemble
Kalman Filter Algorithm for State Space Models with Unknown Parameters,” Journal of Computa-
tional and Graphical Statistics, 33, 448–469.
[21] Dudewicz, E. J., Ma, Y., Mai, E. S., and Su, H. (2007), “Exact solutions to the Behrens–Fisher
Problem: Asymptotically optimal and finite sample efficient choice among,” Journal of Statistical
Planning and Inference, 137, 1584–1605.
[22] Efron, B. and Tibshirani, R. (1993), An Introduction to the Bootstrap, Boca Raton, FL: Chapman
& Hall/CRC.
[23] Fay, M. P. (2023), “Package ‘asht’: Applied Statistical Hypothesis Tests,” R Package.
[24] F´ed´erer, H. (1969), Geometric Measure Theory, Berlin: Springer-Verlag.
[25] Fieller, E. C. (1954), “Some Problems in Interval Estimation,” Journal of the Royal Statistical
Society, Series B, 16, 175–185.
[26] Fisher,R.A.(1930),“InverseProbability,”Mathematical Proceedings of the Cambridge Philosophical
Society, 26, 528–535.
[27] — (1935), “The fiducial argument in statistical inference,” Annals of Eugenics, 6, 391–398.
[28] — (1956), “On a Test of Significance in Pearson’s Biometrika Tables (No. 11),” Journal of the royal
statistical society series B-methodological, 18, 56–60.
[29] — (1956), Statistical Methods and Scientific Inference, New York: Hafner Press.
60[30] Fraser, D. A. S. (1966), “Structural probability and a generalization,” Biometrika, 53, 1–9.
[31] — (1968), The Structure of Inference, New York-London-Sydney: John Wiley & Sons.
[32] Geisser, S., Hodges, J., Press, S., and ZeUner, A. (1990), “The validity of posterior expansions based
on Laplace’s method,” Bayesian and likelihood methods in statistics and econometrics, 7, 473.
[33] Gy¨ongy, I. (1986), “Mimicking the one-dimensional marginal distributions of processes having an Itˆo
differential,” Probability theory and related fields, 71, 501–516.
[34] Hannig, J. (2009), “On generalized fiducial inference,” Statistica Sinica, 19, 491–544.
[35] — (2013), “Generalized fiducial inference via discretization,” Statistica Sinica, 23, 489–514.
[36] Hannig, J., Iyer, H., Lai, R. C. S., and Lee, T. C. M. (2016), “Generalized Fiducial Inference: A
Review and New Results,” Journal of the American Statistical Association, 111, 1346–1361.
[37] Hoyer, P. O., Janzing, D., Mooij, J. M., Peters, J., and Scholkopf, B. (2008), “Nonlinear causal
discovery with additive noise models,” in Neural Information Processing Systems.
[38] Hsu, P. L. (1938), “Contributions to the theory of “student’s” t-test as applied to the problem of
two samples,” In Statistical Research Memoirs, 1–24.
[39] Hwang, C.-R. (1980), “Laplace’s Method Revisited: Weak Convergence of Probability Measures,”
Annals of Probability, 8, 1177–1182.
[40] Jeffreys, H. (1961), Theory of Probability, Oxford University Press.
[41] Kim, S., Song, Q., and Liang, F. (2022), “Stochastic Gradient Langevin Dynamics Algorithms with
Adaptive Drifts,” Journal of Statistical Computation and Simulation, 92, 318–336.
[42] Laurent, S. (2021), “gfilogisreg: Generalized Fiducial Inference for the Logistic Regression Model,”
, https://github.com/stla/gfilogisreg.
[43] Li, C., Chen, C., Carlson, D., and Carin, L. (2016), “Preconditioned stochastic gradient Langevin
dynamics for deep neural networks,” in AAAI.
[44] Li, G. and Hannig, J. (2020), “Deep fiducial inference,” Stat, 9, e308.
[45] Li, Z., Zhang, T., and Li, J. Y. (2019), “Stochastic gradient Hamiltonian Monte Carlo with variance
reduction for Bayesian inference,” Machine Learning, 108, 1701–1727.
[46] Liang, F., Cheng, Y., and Lin, G. (2014), “Simulated Stochastic Approximation Annealing for
Global Optimization with a Square-Root Cooling Schedule,” Journal of the American Statistical
Association, 109, 847–863.
61[47] Liang, F., Jia, B., Xue, J., Li, Q., and Luo, Y. (2018), “An imputation–regularized optimization
algorithm for high dimensional missing data problems and beyond,” Journal of the Royal Statistical
Society, Series B, 80, 899–926.
[48] — (2018), “An imputation-regularized optimization algorithm for high-dimensional missing data
problems and beyond,” Journal of the Royal Statistical Society, Series B, 80, 899–926.
[49] Liang, F., Li, Q., and Zhou, L. (2018), “Bayesian Neural Networks for Selection of Drug Sensitive
Genes,” Journal of the American Statistical Association, 113, 955–972.
[50] Liang, F., Xue, J., and Jia, B. (2022), “Markov neighborhood regression for high-dimensional infer-
ence,” Journal of the American Statistical Association, 117, 1200–1214.
[51] Liang, S. and Liang, F. (2022), “A Double Regression Method for Graphical Modeling of High-
dimensional Nonlinear and Non-Gaussian Data,” Satistics and Its Interface, in press.
[52] Liang,S.,Sun,Y.,andLiang,F.(2022),“NonlinearSufficientDimensionReductionwithaStochastic
Neural Network,” NeurIPS 2022.
[53] Linnik, J. (1968), Statistical Problems with Nuise Parameters, Providence, RI: American Mathemat-
ical Society.
[54] Liu,Y.,Hannig,J.,andMurph,A.C.(2022),“AGeometricPerspectiveonBayesianandGeneralized
Fiducial Inference,” arXiv:2210.05462v2.
[55] MacKinnon, D., Lockwood, C., Hoffman, J., West, S., and Sheets, V. (2002), “A comparison of
methods to test the mediation and other intervening variable effects,” Psychological Methods, 8,
1–35.
[56] Martin,R.(2015),“PlausibilityFunctionsandExactFrequentistInference,”JournaloftheAmerican
Statistical Association, 110, 1552 – 1561.
[57] Martin, R. and Liu, C. (2013), “Inferential Models: A Framework for Prior-Free Posterior Proba-
bilistic Inference,” Journal of the American Statistical Association, 108, 301 – 313.
[58] — (2014), “Discussion: Foundations of Statistical Inference, Revisited,” Statistical Science, 29, 247–
251.
[59] — (2015), “Conditional inferential models: combining information for prior-free probabilistic infer-
ence,” Journal of the Royal Statistical Society, Series B, 77, 195–217.
[60] — (2015), Inferential Models: Reasoning with Uncertainty, CRC Press.
62[61] Mauldon, J. G. (1955), “Pivotal Quantities for Wishart’s and Related Distributions, and a Paradox
in Fiducial Theory,” Journal of the Royal Statistical Society, Series B, 17, 79–85.
[62] Miles, C. H. and Chambaz, A. (2021), “Optimal tests of the composite null hypothesis arising in
mediation analysis,” arXiv:2107.07575.
[63] Milnor, J. and Stasheff, J. D. (1974), Characteristic Classes, Princeton University Press.
[64] Murph, A. C., Hannig, J., and Williams, J. P. (2022), “Generalized Fiducial Inference on Differen-
tiable Manifolds,” arXiv:2209.15473v2.
[65] Nigam, K., McCallum, A., and Mitchell, T. M. (2006), “Semi-Supervised Text Classification Using
EM,” in Semi-Supervised Learning, MIT Press, chap. 3, pp. 33–56.
[66] Ouali, Y., Hudelot, C., and Tami, M. (2020), “An Overview of Deep Semi-Supervised Learning,”
ArXiv, abs/2006.05278.
[67] Peters, J., Mooij, J. M., Janzing, D., and Sch¨olkopf, B. (2013), “Causal discovery with continuous
additive noise models,” J. Mach. Learn. Res., 15, 2009–2053.
[68] Petersen, P. and Voigtlaender, F. (2018), “Optimal approximation of piecewise smooth functions
using deep ReLU neural networks,” Neural Networks, 108, 296–330.
[69] Portnoy, S. (1986), “On the central limit theorem in Rp when p → ∞,” Probability Theory and
Related Fields, 73, 571–583.
[70] — (1988), “Asymptotic behavior of likelihood methods for exponential families when the number of
parameters tend to infinity,” Annals of Statistics, 16, 356–366.
[71] Raginsky, M., Rakhlin, A., and Telgarsky, M. (2017), “Non-convex learning via stochastic gradient
langevindynamics: anonasymptoticanalysis,”inConference on Learning Theory,PMLR,pp.1674–
1703.
[72] Reich, S. (1996), “Symplectic integration of constrained Hamiltonian systems by composition meth-
ods,” SIAM Journal on Numerical Analysis, 33, 475–491.
[73] Robbins, H. and Monro, S. (1951), “A stochastic approximation method,” The Annals of Mathe-
matical Statistics, 22, 400–407.
[74] Scheffe, H. (1970), “Practical Solutions of the Behrens-Fisher Problem,” Journal of the American
Statistical Association, 65, 1501–1508.
[75] Schmidt-Hieber, J. (2020), “Nonparametric regression using deep neural networks with ReLU acti-
vation function,” The Annals of Statistics, 48, 1875–1897.
63[76] Segal,I.E.(1938),“Fiducialdistributionofseveralparameterswithapplicationtoanormalsystem,”
Mathematical Proceedings of the Cambridge Philosophical Society, 34, 41 – 47.
[77] Shafer, G. (1976), A Mathematical Theory of Evidence, New Jersey: Princeton University Press.
[78] Sobel, M. E. (1982), “Asymptotic Confidence Intervals for Indirect Effects in Structural Equation
Models,” Sociological Methodology, 13, 290–312.
[79] Song, Q., Sun, Y., Ye, M., and Liang, F. (2020), “Extended Stochastic Gradient MCMC for Large-
Scale Bayesian Variable Selection,” Biometrika, 107, 997–1004.
[80] Stein, C.M.(1959), “AnExampleofWideDiscrepancyBetweenFiducialandConfidenceIntervals,”
Annals of Mathematical Statistics, 30, 877–880.
[81] Sun, L. and Liang, F. (2022), “Markov neighborhood regression for statistical inference of high-
dimensional generalized linear models,” Statistics in Medicine, 41, 4057 – 4078.
[82] Sun, Y. and Liang, F. (2022), “A kernel-expanded stochastic neural network,” Journal of the Royal
Statistical Society Series B, 84, 547–578.
[83] Sun, Y., Song, Q., and Liang, F. (2022), “Consistent Sparse Deep Learning: Theory and Computa-
tion,” Journal of the American Statistical Association, 117, 1981–1995.
[84] Sun, Y., Xiong, W., and Liang, F. (2021), “Sparse Deep Learning: A New Framework Immune to
Local Traps and Miscalibration,” NeurIPS 2021.
[85] Teh, Y. W., Thiery, A. H., and Vollmer, S. J. (2016), “Consistency and Fluctuations For Stochastic
Gradient Langevin Dynamics,” Journal of Machine Learning Research, 17, 1–33.
[86] Tingley, D., Yamamoto, T., Hirose, K., Keele, L. J., and Imai, K. (2014), “mediation: R Package
for Causal Mediation Analysis,” Journal of Statistical Software, 59, 1–38.
[87] Wang, C. and Jia, J. (2022), “Te Test: A New Non-asymptotic T-test for Behrens-Fisher Problems,”
arXiv, arXiv:2210.16473.
[88] Welch, B. (1947), “The generalization of ‘student’s’ problem when several different population vari-
ances are involved,” Biometrika, 34, 28–35.
[89] Welling, M. and Teh, Y. W. (2011), “Bayesian Learning via Stochastic Gradient Langevin Dynam-
ics,” in ICML.
[90] Xie, M. and Singh, K. (2013), “Confidence Distribution, the Frequentist Distribution Estimator of
a Parameter: A Review,” International Statistical Review, 81, 3–39.
64[91] Xue, J. and Liang, F. (2017), “A Robust Model-Free Feature screening Method for Ultrahigh di-
mensional Data,” Journal of Computational and Graphical Statistics, 26, 803–813.
[92] Yang, Y. (2007), “Consistency of Cross Validation for Comparing Regression Procedures,” Annals
of Statistics, 35, 2450–2473.
[93] Yarowsky, D. (1995), “Unsupervised Word Sense Disambiguation Rivaling Supervised Methods,” in
Annual Meeting of the Association for Computational Linguistics.
[94] Zabell, S. L. (1992), “R. A. Fisher and Fiducial Argument,” Statistical Science, 7, 369–387.
[95] Zhang, R., Li, C., Zhang, J., Chen, C., and Wilson, A. G. (2020), “Cyclical Stochastic Gradient
MCMC for Bayesian Deep Learning,” in ICLR.
[96] Zhu,X.(2005),“Semi-SupervisedLearningLiteratureSurvey,”Technical Report #1530, Department
of Computer Sciences, University of Wisconsin-Madison.
65