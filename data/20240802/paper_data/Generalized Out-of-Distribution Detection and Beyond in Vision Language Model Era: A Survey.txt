1
Generalized Out-of-Distribution Detection and
Beyond in Vision Language Model Era: A Survey
Atsuyuki Miyai, Student Member, IEEE, Jingkang Yang, Jingyang Zhang, Yifei Ming, Yueqian Lin,
Qing Yu, Member, IEEE, Go Irie, Member, IEEE, Shafiq Joty, Yixuan Li, Member, IEEE, Hai Li, Fellow, IEEE,
Ziwei Liu, Member, IEEE, Toshihiko Yamasaki, Member, IEEE, Kiyoharu Aizawa, Fellow, IEEE
Abstract—Detectingout-of-distribution(OOD)samplesiscrucialforensuringthesafetyofmachinelearningsystemsandhasshaped
thefieldofOODdetection.Meanwhile,severalotherproblemsarecloselyrelatedtoOODdetection,includinganomalydetection(AD),
noveltydetection(ND),opensetrecognition(OSR),andoutlierdetection(OD).Tounifytheseproblems,ageneralizedOODdetection
frameworkwasproposed,taxonomicallycategorizingthesefiveproblems.However,VisionLanguageModels(VLMs)suchasCLIPhave
significantlychangedtheparadigmandblurredtheboundariesbetweenthesefields,againconfusingresearchers.Inthissurvey,wefirst
presentageneralizedOODdetectionv2,encapsulatingtheevolutionofAD,ND,OSR,OODdetection,andODintheVLMera.Our
frameworkrevealsthat,withsomefieldinactivityandintegration,thedemandingchallengeshavebecomeOODdetectionandAD.In
addition,wealsohighlightthesignificantshiftinthedefinition,problemsettings,andbenchmarks;wethusfeatureacomprehensive
reviewofthemethodologyforOODdetection,includingthediscussionoverotherrelatedtaskstoclarifytheirrelationshiptoOOD
detection.Finally,weexploretheadvancementsintheemergingLargeVisionLanguageModel(LVLM)era,suchasGPT-4V.We
concludethissurveywithopenchallengesandfuturedirections.
IndexTerms—AnomalyDetection,NoveltyDetection,OpenSetRecognition,Out-of-DistributionDetection,OutlierDetection,Vision
LanguageModel,CLIP,LargeVisionLanguageModel,LargeMulti-modalModel
✦
1 INTRODUCTION
AReliable visual recognition system should not only researchinthefieldofOODdetection.
accurately predict known contexts, but also identify While OOD detection primarily focuses on semantic
and reject unknown examples [1], [2], [3], [4]. In critical distribution shift, several other tasks share similar goals
applicationslikeautonomousdriving,thesystemmustalert andmotivations,includingoutlierdetection(OD)[7],[8],[9],
andcedecontroltothedriveruponencounteringunfamiliar [10], anomaly detection (AD) [11], [12], [13], [14], novelty
scenes or objects not seen during training. However, most detection(ND) [15],[16],[17],[18],andopensetrecognition
existingmachinelearningmodelsaretrainedbasedonthe (OSR) [19], [20], [21]. Subtle differences in the specific
closed-world assumption [5], [6], where the test data is definitionsamongthesesub-topicshavecausedconfusion
assumed to be drawn i.i.d. from the same distribution as in the field, leading to similar approaches being proposed
thetrainingdata,knownasin-distribution(ID).Therefore, across them. To address this issue, the generalized OOD
the development of classifiers capable of detecting out-of- detection framework was introduced [22]. The taxonomy
distribution (OOD) samples is a crucial challenge for real- of the generalized OOD detection framework is shown in
worldapplications.Thischallengeispreciselythefocusof Fig.1.ThegeneralizedOODdetectionframeworkclassifies
these tasks as special cases or sub-tasks under a unified
taxonomy. This framework provides clear definitions and
• A.Miyai,Q.Yu,T.Yamasaki,andK.AizawaarewiththeDepartmentof
InformationandCommunicationEngineering,TheUniversityofTokyo, fostersadeeperunderstandingofeachfield.
Japan,1138656.Q.YuisalsowithLYCorporation,Japan,1028282. Inrecentyears,theemergenceofVisionLanguageModels
E-mail: {miyai,yamasaki}@cvm.t.u-tokyo.ac.jp, {yu,aizawa}@hal.t.u-
(VLMs), represented by CLIP [23], has rapidly accelerated
tokyo.ac.jp
• J. Yang and Z. Liu are with S-Lab, School of Computer Science and researchinthefieldofComputerVision.Thishaschanged
Engineering,NanyangTechnologicalUniversity,Singapore,639798. the paradigm of the recognition field, allowing for zero-
E-mail:{jingkang001,ziwei.liu}@ntu.edu.sg
shot[23]orfew-shotlearning[24],[25]invariousdomains.
• J.Zhang,Y.Lin,andHaiLiarewiththeDepartmentofElectricaland
VLMshavesignificantlyinfluencedtheaforementionedfive
ComputerEngineering,DukeUniversity,Durham,NC,UnitedStates,
27708. problems(OD,AD,ND,OSR,andOODdetection),andthe
E-mail:{jz288,yueqian.lin,hai.li}@duke.edu applicationofVLMshasbecomeahighlynotableresearch
• Y.MingandS.JotyarewithSalesforceAIResearch,PaloAlto,CA,United
field[26],[27],[28],[29].However,alongsidethisremarkable
States,94301.S.JotyisalsowithNTU,Singapore,639798(onleave).
E-mail:{yifei.ming,sjoty}@salesforce.com progress,theparadigmshiftwiththeadventoftheVLMshas
• G.IrieiswiththeDepartmentofInformationandComputerTechnology, blurredtheboundariesbetweenthefiveproblems.Dueto
TokyoUniversityofScience,Japan,1258585. thedifficultyofaclearunderstandingofthedistinctionsand
E-mail:goirie@ieee.org
interrelationsbetweenthesetasks,eachcommunitywithin
• Y. Li is with the Department of Computer Sciences, University of
Wisconsin-Madison,Madison,WI,UnitedStates,53706. thefieldsisfacingsignificantchallengesinidentifyingthe
E-mail:sharonli@cs.wisc.edu optimaldirectiontopursueinthisVLMera.
Resources:https://github.com/AtsuMiyai/Awesome-OOD-VLM. Inthissurvey,weintroduceanovelunifiedframework
4202
luJ
13
]VC.sc[
1v49712.7042:viXra2
TABLE1:NumberofVLM-basedpapersintheTopVenues
Covariate Shift Detection Semantic Shift Detection
byJune2024.
Single/Multi-Class Single-Class Multi-Class
Anomaly Detection Task TopVenue
Sensory Semantic CVPR2023×1[27]
Anomaly Anomaly (a)SensoryAD ICLR2024×1[29]
Detection Detection CVPR2024×4[33],[34],[35],[36]
Novelty Detection TMLR2022×1[37](one-class)
(b)SemanticAD/ND CVPR2024×1[35] One-Class Multi-Class
(one/multi-class)
Novelty Novelty
Detection Detection (c)OSR None
NeurIPS2021×1[38]
Open Set
AAAI2022×1[39]
Recognition
NeurIPS2022×1[26]
ICCV2023×1[40]
Out-of-Distribution IJCV2023×1[41]
Detection (d)OODDetection
NeurIPS2023×3[28],[42],[43]
ICLR2024×2[44],[45]
Outlier Detection CVPR2024×2[46],[47]
ICML2024×1[48]
*Exception: In OOD Detection, density-based methods do not require ID classification
(e)OD ICML2024[49]
Fig. 1: Taxonomy of generalized OOD detection frame-
work[22],illustratedbyclassificationtasks.Figureadapted
from[22]. community.
2) A Comprehensive Survey for OOD Detection in
termedgeneralizedOODdetectionv2,whichextendstheprevi- theVLMera:WhilecomprehensivesurveysonOD,
ousgeneralizedOODdetectionframeworkandsummarizes AD, ND, OSR, and OOD detection methodologies
theevolutionofthesefiveproblemsintheVLMera.Tocreate havebeenpublishedinrecentyears[11],[12],[13],
it, we systematically review the use of VLMs across these [14], [20], [22], [50], [51], this survey is the first to
fiveproblemareas,tracingtheirdevelopmentfromthestart comprehensivelyoverviewOODdetectionmethods
to the present, and summarize the evolutionary trajectory specifically in the VLM era. By connecting with
of each problem. Importantly, our framework reveals that otherrelatedtasks,weaimtoprovidereaderswith
aparadigmshifthascausedsomefieldstobecomeinactive a holistic understanding of the developments and
orintegratewithothers,andthedemandingchallengesin interconnectionswithineachproblem,particularly
the VLM era become AD and OOD detection, which is a inthecontextofOODdetection.
remarkablefindingforeachcommunity.Inadditiontothe 3) AnIntroductiontotheEvolutionintheLVLMEra:
inter-field evolution, we elaborate on the important shifts Wefurtherintroducetheevolutionofeachproblem
in the definition of OOD detection as well as the problem in the Large Vision Language Model era. Despite
settings and benchmarks, with the contrast of those for theinfantstageofthesefields,thissurveyoffersan
related tasks. Then, we conduct a thorough review of the in-depth introduction to each problem, aiming to
methodology for OOD detection and related tasks in the facilitatefutureadvancementsinthisarea.
VLMera,intendingtoclarifytheirsimilaritiesanddifferences 4) Future Research Directions: We draw readers’ at-
andinspirefutureresearchinOODdetection. tentiontothefutureworknecessaryforadvancing
Finally,weintroducetheevolutionoftheseproblemsin the field in the VLM and LVLM era. We conclude
theemergingLargeVisionLanguageModel(LVLM)era,such thissurveywithdiscussionsonopenchallengesand
asGPT-4V[30]orLLaVA[31](alsoreferredtoasLargeMulti- opportunitiesforfutureresearch.
modalModelsorMulti-modalLargeLanguageModels[32]).
Thepapercontentisorganizedasfollows.InSec.2,we
Wesummarizethedefinitionofeachevolvingproblem,the
introducethenewversionofgeneralizedOODdetectionby
findingssofar,andfuturechallenges.
summarizing the evolution of the five related fields in the
Tosumup,inthissurveypaper,wemakethreecontribu-
VLM era. We then overview the two key problems (OOD
tionstotheresearchcommunity:
detectionandAD)thathaveevolvedandremainactivein
1) Unified Framework in the VLM era: We system- Sec.3,withadetailedbreakdownofexistingmethodologies
aticallyreviewtheevolutionoffivecloselyrelated beingpresentedinSec.4(CLIP-basedOODdetection)and
topics of OD, AD, ND, OSR, and OOD detection Sec. 5 (CLIP-based AD). In Sec. 6, we introduce early
in the Vision Language Model era and provide advancementsofOODdetectionandADintheLVLMera.
an updated unified framework termed generalized Sec.7andSec.8featurediscussionsonpotentialchallenges
OOD detection v2. Our framework reveals that the andfuturedirections.Finally,weconcludewithSec.9.
paradigm shift has led to some field inactivity or
integration,andthedemandingchallengesareAD
2 GENERALIZED OOD DETECTION V2
andOODdetection.Wehopethattheseobservations
highlight the demanding challenges in the VLM In this section, we introduce a novel unified framework
era and foster collaborative efforts among each termedgeneralizedOODdetectionv2,whichsummarizesthe
evitcudnI
evitcudsnarT
ID
Classification
Not
Required
Required*
No3
(a) Sensory AD (b) Semantic AD / ND (c) OSR (d) OOD Detection (e) OD
Normal one-class multi-class Known Known ID ID
Normal Normal
OOD
CIFAR-10 (horse, frog) ImageNet-1K CIFAR-10 ImageNet-1K
Anomaly
CIFAR-10 (horse) CIFAR-10 OOD OOD
Unknown Unknown
Anomaly Anomaly OOD
SVHN LSUN iNaturalistTexture
CIFAR-10 (other classes) ImageNet-21K
CIFAR-10 (other classes) SVHN LSUN
MNIST, CIFAR-10/100 ImageNet-SSB CIFAR-10/100 ImageNet CIFAR-10/100
MVTec-AD MNIST, CIFAR-10/100, ImageNet-30 TinyImageNet
Inactive Inactive
CLIP-based AD CLIP-based OOD Detection
Normal Normal Hard OOD Common OOD
ID ID Split-1 ID All dog ID
Split-2 ID Half of hunting dog
Anomaly Anomaly Split-3 ID Mixed 151 classes
ImageNet-20 ImageNet-10 Split-4 ID First 100 classes ImageNet-1K
OOD OOD Split-1OODNon-animal OOD
Split-2 OODNon-animal
Split-3OODMixed 164 classes
ImageNet-10 ImageNet-20 Split-4OODLast 900 classes iNaturalist Texture
MVTec-AD VisA ImageNet-20 ImageNet-10 ImageNet-protocol ImageNet
Fig.2:GeneralizedOODdetectionframeworkv2,reflectingtheevolutionofeachproblemintheVLMera.(a)SensoryAD
hasbecomeahighlyactiveandnoteworthyfieldintheVLMera.Intermsofbenchmarks,inadditiontothecommonly
usedMVTec-AD[52],VisA[53],thelargestindustrialanomalydetectiondataset,hasalsobecomeastandardbenchmark
in the field. (b) Semantic AD/ND has become inactive in the VLM era. (c) OSR has been integrated into hard OOD
detection.CLIP-basedhardOODdetectionincorporatesthebenchmarksetupofOSRandcreatesnewbenchmarkssuchas
ImageNet-10/ImageNet-20[26]andImageNet-protocol[47],[54].(d)OODdetectionisahighlyactiveresearchareainthe
VLMera.(e)ODhasbecomeinactiveintheVLMera.
evolutionofthefiverelatedfieldsintheVLMera.Wefirst categorized as shown in Fig. 1: Anomaly detection is cat-
revisitthepreviousgeneralizedOODdetectionframework egorizedintosensoryanomalydetection,whichdealswith
inSec.2.1.Next,weintroducetheevolutionofeachproblem. covariateshift,andsemanticanomalydetection,whichdeals
withsemanticshift.Noveltydetectionfallsunderthesame
2.1 Background:GeneralizedOODDetectionV1 categoryassemanticanomalydetection.Whenaddressinga
multi-classscenariothatnecessitatesIDclassification,both
We first briefly revisit a previous generalized OOD detec-
open-setrecognitionandout-of-distributiondetectionareen-
tion, which encapsulates five related sub-topics: anomaly
compassedwithinthiscategory.Themaindifferencebetween
detection (AD), novelty detection (ND), open set recogni-
OSRandOODdetectionwasthebenchmarksetup[22],[55]
tion(OSR),out-of-distribution(OOD)detection,andoutlier
(Sec.2.2(c)).Outlierdetectionbelongstoadifferentcategory
detection(OD).Thesesub-topicscanbesimilarinthesense
fromtheothertasks,asthisproblemistransductive(i.e.,it
thattheyalldefineacertainin-distribution,withthecommon
hasaccesstoallobservations).
goalofdetectingout-of-distributionsamplesundertheopen-
For the detailed definition of each task, we refer the
world assumption. Previously, subtle differences existed
readerstothepreviousgeneralizedOODdetectionsurvey
amongthesub-topicsintermsofthespecificdefinitionand
paper[22].
propertiesofin-distribution(ID)andOODdata.
Toprovideacleardefinition,ageneralizedOODdetection
2.2 EvolutionofEachProbleminVLMEra
frameworkwasproposed[22].Thetaxonomyforgeneralized
OODdetectionisshowninFig.1.Itisbasedonthefollowing WereviewhoweachproblemhasevolvedintheVLMera.
fourbases:(1)Distributionshifttodetect:Thetaskfocuses Tomakeafairjudgment,wecomprehensivelyinvestigated
ondetectingeithercovariateshift(e.g.,OODsamplesfroma papers that use VLMs from top venues and summarized
differentdomain)orsemanticshift(e.g.,OODsamplesfrom them in Table 1. Our survey revealed that CLIP [23] is
a different semantic). (2) ID data type: The in-distribution predominantly used as the VLM for OOD detection and
(ID) data contains either a single class or multiple classes. othersub-tasks,andotherVLMs[56],[57]arerarelyutilized.
(3) Whether the task requires ID classification: Some tasks Therefore,wefocusonCLIPasthetargetVLMinthissurvey
require classification of the ID data, while others do not. andrefertoOODdetectionusingCLIPasCLIP-basedOOD
(4)Transductivevs.inductivelearning:Transductivetasks detection.Similarly,wewillprefixothertaskswith“CLIP-
requireallobservations(bothIDandOOD),whileinductive based”(e.g.,CLIP-basedAD).AsOODdetectionresearchis
tasks follow the common train-test scheme. According to primarilyfocusedontheimagedomain,weconductasurvey
the above taxonomy, these five problems can be clearly of other tasks within the image domain that are common4
andhavestrongconnectionstoOODdetectionresearch.For indicatingahighinterestfromthecommunity.Additionally,
instance,oursurveydoesnotcovervideodomaintasks[58], asmentionedabove,OSRhasbeenintegratedwithOODde-
[59],[60]duetotheirlimitedconnectiontoOODdetection. tectionasafieldofhardOODdetection[26],[47].Therefore,
(a) Sensory AD → CLIP-based AD Sensory AD has itisexpectedthatOODdetectionwillcontinuetogrowand
continuedtodevelopasacommonproblemsettingforCLIP- developfurther.
based AD, inheriting the challenges of traditional sensory (e) OD → Inactive OD has become less active in the
AD [27], [29], [33], [35], [61], [62], [62], [63], [64], [65]. As VLM era. Previously, OD was used for open-set semi-
showninTable1,thefirstappearanceinatopvenuewasat supervised learning [73], [74], [75], learning with open-set
CVPR2023,andsincethen,atotalofsixpapershavebeen noisylabels[76],andnoveltydiscovery[77],[78],[79],[80],
published in top venues. In addition, there are numerous [81]. The reason for the inactivity is that the use of CLIP
other papers [61], [63], [64], [65], [66]. Moreover, in terms ledtoareductionintrainingcostsandonlyasmallamount
ofbenchmarks,inadditiontothecommonlyusedMVTec- ofdataneedstobecollected,eliminatingtheneedforlarge
AD [52], the largest industrial anomaly detection dataset amountsofunlabeleddataandreducingtheneedtoconsider
VisA[53]hasalsobecomeastandardbenchmarkinthefield. noisy data. However, recently, Liang et al. [49] proposed
Therefore,itisevidentthatsensoryADhasbecomeahighly UnsupervisedUniversalFine-Tuning,anewproblemsetting
activeandnoteworthyfieldintheVLMera. forCLIP-basedODinICML2024.UnsupervisedUniversal
(b) Semantic AD/ND → Inactive Research onsemantic Fine-Tuning assumes a more realistic problem setting for
AD/ND appears to become inactive in the VLM era. As unsupervised tuning of the downstream task with CLIP
showninTable1,thereareonlytwopapers,TMLR2022[37] where some OOD samples are included in the unlabeled
and CVPR 2024 [35]. However, the CVPR 2024 work [35] samples. With this new problem setting, there is still a
aimstobuildageneralistanomalydetectorthatsolvesmany possibilitythatODwillbecomeactiveinthefuture.However,
AD tasks, including sensory AD and semantic AD, and asODisnotcurrentlyanactivearea,weexcludeODfrom
is not primarily focused on semantic AD. The reasons for themaindiscussionofthissurvey.UnsupervisedUniversal
the inactivity include saturation of performance for one- Fine-TuningisdeeplyrelatedtoOODdetectionandwillbe
classsemanticAD/ND,andincompatibilityofmethodswith discussedindetailinSec.4.3.
CLIP for multi-class semantic AD/ND. As for one-class
semanticAD/ND,TMLR[37]exists,buttheperformances 2.3 Discussion
withcommonCIFARandImageNet-30datasetshavealready
Through Sec. 2.2, we found that previously mixed fields
achievedaround99%.Asformulti-classsemanticAD/ND,a
havebeencorrectlyorganizedintheVLMera,andthatthe
commonapproachistotreatIDclassesasasingleclass,but
focusshouldbeonOODdetectionandsensoryAD.These
treatingIDclassesasasingleclassislesscompatiblewith
fields are still developing, with an increasing number of
CLIP’sclass-wisediscriminativecapability.
methodologiesandbenchmarks,andareexpectedtobecome
(c) OSR → CLIP-based OOD Detection We consider moreactiveinthefuture.Noteherethatthisdoesnotmean
that OSR has been integrated into CLIP-based hard OOD that other fields have come to an end. For example, one
detection. According to Table 1, there are no top venue reasonwhyone-classsemanticAD/NDhasnotbeenstudied
publications on OSR research in the VLM era. Originally, is the saturation of performance [37]. If more fine-grained
themaindifferencebetweenOSRandOODdetectionwas andchallengingdatasetscouldbeconstructed,thefieldcould
the benchmark setup [22], [55]. OSR typically divides the bereactive.Weputthisinout-of-scopeforthissurveypaper,
classesintheonedatasetintosomeknown(ID)classesand butthisisanimportantfuturechallenge.
unknown(OOD)classes,asseeninMNIST-4/6[67]CIFAR-
4/6[68],CIFAR-50/50[69],andTinyImageNet-20/180[70].
3 OVERVIEW OF EACH PROBLEM IN VLM ERA
However,inrecentyears,someworksonCLIP-basedOOD
Inadditiontotheaboveinter-fieldevolution,weemphasize
detectionincorporatethebenchmarksetupofOSRandcreate
that the advent of VLMs has significantly changed the
new benchmarks such as ImageNet-10/ImageNet-20 [26]
field of OOD detection itself. In this section, we present
and ImageNet-protocol [47], [54] for hard OOD detection.
anoverviewofCLIP-basedOODdetection,highlightingthe
Therefore,theboundarybetweenOODdetectionandOSR
keychangesintheproblemdefinition,theproblemsetting,
haseffectivelydisappeared,andallresearchintheVLMera
andbenchmarks.Inaddition,wealsopresentanoverview
hasbeenintegratedintoOODdetection.
of CLIP-based AD in the hope that the understanding of
Nevertheless,whilepureOSRresearchisdeclining,some
each field will lead to a deeper understanding of CLIP-
studies have used the term “open-set” in the context of
basedOODdetection.Foritemsthatremainunchangedfrom
domaingeneralization[71].Thesestudiesdeviatefromthe
traditionalproblems,suchasbackground,applications,and
originalscopeofOSRresearchandarerathercloselyaligned
evaluation,wereferthereaderstotheoriginalgeneralized
withthefieldofdomaingeneralization[72].Therefore,within
OODdetectionpaper[22].
ourgeneralizedOODdetectionv2,wedonotclassifythese
studiesasfallingunderOSRresearch.Wewilldiscussthem
in the context of full-spectrum OOD detection, a research 3.1 CLIP-basedOut-of-DistributionDetection
fieldcombininggeneralizationanddetection,inSec.7.2. Definition ThedefinitionofCLIP-basedOODdetection
(d)OODDetection→CLIP-basedOODDetection OOD differssignificantlyfromthatofconventionalOODdetection.
detection is a highly active research area in the VLM era. Conventional OOD detection aims to detect test samples
AsshowninTable1,therearemanypapersintopvenues, drawnfromadistributionthatisdifferentfromthetraining5
distribution.Asanotherdefinition,OODdetectionisdefined Problem Setting CLIP-based AD focuses on solving
as a task to detect test samples that the model cannot or anomalyclassificationandsegmentationinacomputation-
doesnotwanttogeneralize[22].However,forCLIP-based allyefficientway.Anomalyclassification,likeconventional
OODdetection,CLIPhasavastamountofknowledge,sothe AD,isabinaryclassificationtaskthatdistinguishesbetween
OODsampleiscompletelyunrelatedtothedistributionof normality and abnormality. Anomaly segmentation, also
theCLIP’spretrainingdataortheCLIP’sowngeneralization followingconventionalAD,involvessegmentingthelocation
ability.Therefore,traditionaldefinitionscannotadequately ofanomalies.LikeCLIP-basedOODdetection,CLIP-based
describethedefinitionofCLIP-basedOODdetection. ADalsoprimarilyfocusesonazero-shot[27](i.e.,without
Unlike the previous definition, CLIP-based OOD de- utilizingimagesinthetargetdataset)orfew-shot[27](i.e.,
tection is defined as follows [26], [39]: CLIP-based OOD usingonlyafewnormalimagesinthetargetdataset)setting.
detectionaimstodetectsamplesthatdonotbelongtoanyID Eachdetaileddefinitionofzero-shotandfew-shotsettingsis
classtextprovidedbytheuser.Givenapre-trainedmodel, describedlaterinSec.5.Asanothershift,conventionalAD
a classification task of interest is defined by a set of class createdseparatemodelsforeachcategory [87],[88],[89],[90],
labelsY ,whichwerefertoastheIDclasses.Thesemantic [91],[92],[93],whileCLIP-basedADcreatesasingleunified
ID
distributionisrepresentedbythedistributionP(Y ).CLIP- model across multiple categories [27], [29], [35], [61], [63],
ID
basedOODdetectionaimstodetecttestsamplesthatcome whichleadstoamorecomputationallyefficientapproach.
from the distribution with the semantic shift from the ID One key difference from CLIP-based OOD detection is
classes,i.e.,P(Y )̸=P(Y ).Followingthedefinitionof thatCLIP-basedOODdetectiondoesnotinvolvelocalization
ID OOD
thegeneralizedOODdetectionframework[22],idealOOD tasks,whilethesearemainstreaminCLIP-basedAD.This
detectorsshouldkeeptheclassificationperformanceontest willbediscussedindetailinSec.5.4.
samplesfromIDclassspaceY ID,andrejectOODtestsamples Benchmark MostworksonCLIP-basedADtackleindus-
withsemanticsoutsidethesupportofY ID. trialinspection[52],[94],[95].Asforthebenchmarks,besides
Problem Setting CLIP-based OOD detection focuses on thecommonMVTec-ADdataset[52],themorechallenging
solving the image classification task in a computationally dataset VisA [53] has been newly employed [27]. The
efficient way. Unlike traditional OOD detection settings, VisA benchmark includes objects with complex structures
whichprimarilyinvolvetraininganIDclassifierwithwhole suchasprintedcircuitboardsandmultipleinstanceswith
ID data, CLIP-based OOD detection primarily focuses on differentlocationswithinasingleview,makingitoneofthe
a zero-shot [26] (i.e., without utilizing ID images) or few- most challenging datasets currently available in the open
shot[28](i.e.,utilizingonlyafewIDimages)setting.Each datasets.SincethepioneeringworkinCLIP-basedAD(i.e.,
detailed definition of both settings are described later in WinCLIP[27])usedMVTec-ADandVisA,manysubsequent
Sec.4.Thefieldisadvancingtowardsgreatercomputational workshavealsousedthesedatasets[33],[63],[65].
efficiency,requiringminimalornotrainingdata.
Benchmark Most recent works in CLIP-based OOD de- 4 CLIP-BASED OOD DETECTION: METHODOLOGY
tectionusehigh-resolutionandlarge-scaledatasetssuchas
In this section, we introduce the methodologies for CLIP-
ImageNet[26],[28],[46],[47],[48].ThecommonImageNet
based out-of-distribution (OOD) detection. Fig. 3 presents
OODbenchmarkusesImageNetasIDandotherdatasets[82],
thetimelineforrepresentativemethodologiesforCLIP-based
[83],[84],[85]asOOD.However,inthiscommonbenchmark,
OODdetection.Table2presentsrepresentativemethods.We
thesemanticsbetweenIDandOODarefar,whichmayallow
introducemethodsforzero-shotOODdetectioninSec.4.1,
easydistinctionbetweentheIDandOOD.Therefore,recent
few-shot OOD detection in Sec. 4.2, and other research
worksusemorechallengingOODbenchmarkswherethey
directionsinSec.4.3.Foreachmethodology,wecategorize
splitImageNetclassesintoIDandOODcategoriesforhard
thembythetypeoftrainingandwhetheradditionalOOD
OOD detection [26], [47], [86]. The representative datasets
promptswereemployed.
are ImageNet-20 [26], ImageNet-10 [26], and the recently
proposed ImageNet-protocol [54] created by dividing into
4.1 Zero-shotOut-of-DistributionDetection
multiple variations of ID/OOD pairs from ImageNet-1K.
This creation strategy initially focused on OSR but has Zero-shot OOD detection was proposed in 2021 by Fort et
recentlybeenrepurposedforOODdetection.Thesechanges al.[38].Sincethen,agrowingnumberofmethodshavebeen
inthedatasetsshiftOODdetectionclosertotherealworld proposedyearbyyear.
andmakeitamorechallengingandpracticaltask. Definition of Zero-shot OOD Detection In zero-shot
OODdetection,theterm“Zero-shot”referstothenon-use
ofIDimagesduringbothtrainingandinferencephases.For
3.2 CLIP-basedAnomalyDetection
instance,themethodwithadditionaltrainingwithauxiliary
Definition Unlike OOD detection, the definition of datasets(non-useofIDimages)canberegardedasazero-
anomalydetection(AD)hasnotchangedbetweenconven- shotmethod[40].Themethodwiththepre-processingofthe
tional AD and CLIP-based AD. AD is intended for use in IDclasstextscanalsoberegardedasazero-shotmethod[38],
specificcircumstances(industrialinspection),wheresamples [39],[45],[48].
that deviate from predefined normality are considered an
anomaly [11], [22]. Whether a model can generalize is 4.1.1 Training-freeMethods
irrelevant to the definition of “Anomaly”. Therefore, even a.WithOODPrompts CLIP-basedOODdetectionstarted
withtheemergenceofCLIP,thedefinitionhasnotchanged. in this setting. The earliest work is ZeroOE [38]. ZeroOE6
Zero-shot Few-shot
(i) CLIP-based Out-of-Distribution Detection
GL-MCM LoCoOp LSN NegPrompt SeTAR
ZeroOE ZOE MCM
CLIPN NegLabel IDPrompt EOE Dual-Adapter GalLoP
2021 2022 2023 2024 2024.07
(ii) CLIP-based Anomaly Detection
WinCLIP Anomaly
AnoCLIP InCTRL
WinCLIP+ CLIP
FiLo
APRIL-GAN SDP
RWDA PromptAD
APRIL-GAN SDP+
Before 2023 2023 2024 2024.07
Fig.3:TimelineforrepresentativemethodologiesforCLIP-basedout-of-distributiondetectionandCLIP-basedanomaly
detection.Weobservethatanincreasingnumberofmethodshaverecentlybeenproposedforbothtasks,indicatingthe
growingactivityinthesefields.
TABLE2:RepresentativepaperlistforCLIP-basedout-of-distributiondetectionandCLIP-basedanomalydetection.
IDImage OOD
Task TrainingType Methods
Availability Prompts
✓ ZeroOE[38],ZOC[39],NegLabel[45],EOE[48]
§4.1Zero-shot
§4.1.1Training-free
✗ MCM[26],GL-MCM[96],SeTAR[97]
§4CLIP-based §4.1.2AuxiliaryTraining ✓ CLIPN[40],NegPrompt[47]
OODDetection ✗ PEFT-MCM[41],LoCoOp[28],GalLoP[98]
§4.2Few-shot
§4.2.1IDTraining
✓ LSN[44],NegPrompt[47],IDPrompt[46]
§4.2.2Training-free ✗ Dual-Adapter[99]
§5.1.1Training-free ✓ WinCLIP[27],AnoCLIP[61],SDP[62]
§5.1Zero-shot §5.1.2AuxiliaryTraining ✓ APRIL-GAN(zero-shot)[63],RWDA[64],
SDP+[62],AnomalyCLIP[29],FiLo[65]
§5CLIP-based
AnomalyDetection
§5.2.1Training-free ✓ WinCLIP+[27]
§5.2Few-shot §5.2.2IDTraining ✓ PromptAD(one-class)[33]
§5.2.3AuxiliaryTraining+ref. ✓ APRIL-GAN(few-shot)[63],InCTRL[35]
feeds the potential OOD labels to the textual encoder of many methods utilize OOD labels, but the difficulty and
CLIP.However,themethodofusingknownOODlabelsis cost of creating these labels pose challenges. To address
infeasibleforreal-worldapplications.Tosolvethisproblem, these issues, Ming et al. [26] proposed MCM, which uses
ZOC[39]proposedtotrainanOODlabelgeneratorbased only ID labels to detect OOD. MCM is a simple approach
onthevisualencoderofCLIPandusethegeneratedpseudo- that devises softmax scaling to align visual features with
OODlabelsforOODdetection.However,whendealingwith textual concepts for OOD detection. Despite its simplicity,
large-scaledatasetsencompassingamultitudeofIDclasses, MCMhashigheffectivenessandscalability,anditservesas
the label generator may not generate effective candidate a crucial baseline in CLIP-based OOD detection. Building
OOD labels, resulting in poor performance. Building on on the concept of MCM, Miyai et al. [96] proposed GL-
these early works [38], [39], recent works focus on how MCM, which extends MCM by just adding a local MCM
to obtain high-quality OOD labels through either (i) OOD score to enhance the fine-grained detection capability in
label retrieval [45], [100] or (2) OOD label generation [48]. localregions.SeTAR[97]enhancesMCMandGL-MCMby
(i) One of the representative retrieval-based methods is changingthemodel’sweightmatricesusingasimplegreedy
NegLabel [45]. NegLabel selects high-quality OOD labels search algorithm. We consider these methods to be post-
fromextensivecorpusdatabasesbycalculatingthedistance hoc methods for CLIP-based OOD detection in that they
betweenanextractedOODlabelandIDlabel.(ii)Oneofthe directlyemployanIDclassifierforOODdetection.Dueto
representativegeneration-basedmethodsisEOE[48].EOE theirsimplicityandhighscalability,thesepost-hocmethods
utilizes Large Language Models (LLMs) to produce high- canbringfundamentalperformanceimprovementsformany
qualityOODlabels.Bymodifyingthepromptsgiventothe subsequentmethods[28],[44],[47].Therefore,weexpectthat
LLM,EOEcanbegeneralizedtoavarietyoftasks,including thisfieldshouldbedevelopedfurtherinthefuture,reflecting
farandnearOODdetection. thetrajectoryofthefieldbeforeCLIPemerged[101],[102],
[103],[104],[105],[106],[107],[108],[109],[110],[111].
b. Without OOD Prompts In zero-shot OOD detection,7
4.1.2 AuxiliaryTraining-basedMethods promptsandtrainingwiththem,LSNandNegPromptcan
learn suitable negative prompts, enabling more accurate
CLIPN [40] is the only auxiliary training-based method
detectionofOODsamples.ThedifferencebetweenLSNand
forzero-shotOODdetection.CLIPNaimstoempowerthe
NegPrompt lies in their approach to the use of negative
logic of saying “no” within CLIP, and it designs a novel
prompts. LSN prepares unique negative prompts for each
learnable“no”promptandanadditional“no”textencoder
classandlearnssuitablenegativepromptsforeachclass.In
to capture negation semantics within images. To create an
contrast, NegPrompt prepares multiple negative prompts
additionaltextencoder,CLIPNneedstobepre-trainedon
common to all ID classes and trains them to learn generic
the CC-3M dataset [112]. While the extensive pre-training
templatesrepresentingthenegativesemanticsofanygiven
of CLIPN may indeed lead to intensive computations and
classlabels.Also,NegPrompttestedtheperformancesinthe
lowerscalability,oncepre-trained,itperformszero-shotOOD
hardOODdetectionsettingwithImageNet-protocol[54],out-
detectionacrossawiderangeofdomainswithcomparable
performingLoCoOpandCoOp.Alternatively,IDPrompt[46]
performancetofew-shotOODdetectionmethods[28],[44].
takesadifferentapproachbyintroducingID-likeprompts,
In the future, within this field, there is potential for zero-
which are designed for OOD features that are close to the
shotopen-vocabularyOODdetectiontofurtheradvancethe
ID features. It extracts ID-like OOD regions in ID training
realmofzero-shotOODdetection,whichwillbediscussed
images and trains ID-like prompts with these extracted
laterinSec.7.2.
OOD data. In a unique direction, LAPT [114] proposes
an automatic sample collection strategy that retrieves or
4.2 Few-shotOut-of-DistributionDetection
generatestrainingIDimagesonlywithIDclassnames,which
Few-shot OOD detection was concurrently proposed by achieves high performance without image collection and
Miyaietal.[28]andMingetal.[41]inJune2023.Sincethen, annotation costs. LAPT then performs distribution-aware
ithasbecomethemostactiveresearchareainCLIP-based promptlearning,whichdistinguishesbetweenIDclassand
OODdetection. OODclasstokens.LAPTispositionedwithinthecontextof
moreefficientfew-shotOODdetectioninthissurveypaper
DefinitionofFew-shotOODDetection CLIP-basedfew-
sinceitrequiresgeneratingorretrieving“IDimages”forthe
shotOODdetectionaimstodetectOODimagesusingonly
datacollection.
a few labeled ID images. In few-shot OOD detection, the
term“Few-shot”referstotheuseofafewIDimagesduring In the context of few-shot OOD detection, recently,
trainingorinferencephases.Forinstance,themethodwith Li et al. [47] proposed a new problem setting called open-
additionaltrainingwithafewIDimagescanberegardedas vocabularyOOD(OV-OOD)detection.Whilecommonfew-
afew-shotmethod[33].Evenwithouttraining,ifamethod shot OOD detection involves training on images from all
usesafewIDimagesasareference,weregarditasafew-shot ID classes during training, OV-OOD detection involves
method[27].Regardingthenumberofshots,itiscommon training on images from just a small subset of ID classes
toexperimentwith1-shotto16-shot[28],[99],followingthe and performing OOD detection using all ID classes at
closed-setsetting[24]. inference time. Formally, we define a subset of semantic
labelsY ⊂Y ,whereY representsallIDlabels.Based
ID,sub ID ID
4.2.1 IDTraining-basedMethods on this subset of labels, we define a corresponding subset
dataset Dtrain ⊂ Dtrain. During training, only Dtrain is
a.WithoutOODPrompts Few-shotOODdetectionbegan ID,sub ID ID,sub
used. Then, at inference time, all ID classes Y are used,
in this setting. Ming et al. [41] proposed PEFT-MCM for ID
andthegoalistodetectOODfromacombinationofallID
CLIP-basedOODdetection,whichdemonstratestheeffec-
test data Dtest with Y and OOD test data Dtest with
tiveness of combining parameter-efficient tuning methods ID ID OOD
Y . For this setting, existing few-shot OOD detection
(e.g.,promptlearning[24]oradapter[113])andMCM[26]. OOD
methods[28],[47]canbeeasilyappliedbysimplycombining
Concurrently,Miyaietal.[28]proposedLoCoOp,apioneer
therestoftheIDclasses.Inparticular,NegPrompt[47]learns
prompt learning approach for few-shot OOD detection.
generalnegativepromptsthatarenotspecifictothetraining
LoCoOpenhancesCoOp’s[24]OODdetectioncapabilitiesby
ID classes, so it achieves high performance in OV-OOD
performingOODregularizationwithlocalOODfeatures.Lo-
detection.
CoOpisthesimplestpromptlearningmethodandservesasa
crucialbaselineinfew-shotOODdetection.UnlikeLoCoOp,
whichutilizesnon-IDlocalregionsforOODregularization,
GalLoP [98] proposes an approach that utilizes local ID 4.2.2 Training-freeMethods
regionstoenableamorefine-graineddistinctionbetweenID
andOODsamples.GalLoPlearnsadiversesetofpromptsby
Training-free few-shot OOD detection is a novel research
utilizingbothglobalandlocalvisualrepresentations,thereby
field, and only Dual-Adapter [99] falls under this cat-
enhancingthedetectioncapabilities.
egory. Dual-Adapter adopts a prior-based method Tip-
b.WithOODPrompts Similartozero-shotOODdetection, Adapter [113], which leverages both textual and visual
recentworksinfew-shotOODdetectionutilizeadditional features with a cache model and enhances performance
OOD prompts [44], [46], [47]. As representative methods, withouttraining.Toadaptthistofew-shotOODdetection,
LSN[44]andNegPrompt[47]wereproposedconcurrently. Dual-Adapteremploystheconceptofdualcachemodeling
Theystatethatthesimplenegativepromptsadded“not”(e.g., andconstructsPositive-AdapterandNegative-Adapter,and
“notaphotoofa[cls]”)failtocapturethedissimilarityfor identifiesOODsampleswiththepredictiondifferencewith
identifyingOODsamples.Therefore,bypreparingnegative bothadapters.8
4.3 OtherImportantResearchDirections OODdetection.Inzero-shotAD,theterm“Zero-shot”refers
4.3.1 CLIP-basedFull-spectrumOODDetection tothenon-useoftheimagesinthetargetdomainduringboth
trainingandinferencephases.Forinstance,themethodwith
CLIP-based full-spectrum OOD (FS-OOD) detection is a
additionaltrainingwithauxiliarydatasetscanberegarded
crucial challenge [115]. FS-OOD detection was proposed
asazero-shotmethod[29],[62],[63],[64],[65].Themethod
by Yang et al. [116] in 2022 as an important setting that
withthepre-processingofthetargetclasstextscanalsobe
considers both OOD generalization [117], [118] and OOD
regardedasazero-shotmethod[27],[61],[62].
detectionsimultaneously.UnlikestandardOODdetection,
which only focuses on semantic shifts between training
5.1.1 Training-freeMethods
andtestdistributions,FS-OODdetectionfurtherconsiders
With Anomaly Prompts In zero-shot AD, a common
non-semanticcovariateshiftbyincludingcovariate-shifted
approachistoutilizeanomalypromptstodetectanomalies.
ID images. As for the benchmarks, OpenOOD v1.5 [119]
Thishypothesisissupportedbyseveralobservationsfrom
provides two large-scale benchmarks based on ImageNet-
existing work [27]. Firstly, the concepts of normality and
200andImageNet-1K,incorporatingImageNet-C[117]with
anomalies are context-dependent states [125] of an object,
image corruptions, ImageNet-R [118] with style changes,
withlanguageplayingacrucialroleindefiningthesestates.
andImageNet-V2[120]withresamplingbiasasID.Asfor
Secondly, language provides additional insights that help
CLIP-basedmethods,LSA[115]usesabidirectionalprompt
differentiatedefectsfromacceptablevariationsinnormality.
customizationmechanism,whichadjustsdiscriminativeID
The simplest zero-shot AD methods are (i) to perform
andOODboundary.
anomaly classification with CLIP using text prompts for
4.3.2 OtherTaskswithCLIP-basedOODDetection
normalityandanomaliesasclasses(i.e.,“normal [class]”
vs. “anomalous [class]”) and (ii) to calculate the simi-
UnsupervisedUniversalFine-Tuning CLIP-basedOOD
larity to the normal prompt (i.e., “normal [class]”) as
detection is useful for a new task called Unsupervised
thescore.ThesemethodsarecalledCLIP-AC[27].Jeonget
UniversalFine-Tuning(UUFT)[49].UUFTisaproblemof
al. [27] reported that CLIP-AC with both normal and
unsupervised learning for outlier detection (OD). Existing
anomaly prompts outperforms that with only normal text
studiesforunsupervisedlearningassumedthatallunlabeled
prompts, which indicates the importance of the use of
imagesbelongtooneoftheIDclasses[121],[122],[123],but
anomalyprompts.However,theperformancesforthisnaive
they require prior knowledge of exact class names linked
method are not yet satisfactory due to the wide range of
to ground truth labels, which restricts their usefulness in
variationsofanomalies.Tosolvethisissue,Jeongetal.[27]
various real-world situations. For a more realistic setting,
proposed WinCLIP. WinCLIP performs a compositional
UUFTassumesthatOODimagesareincludedintheunla-
ensemble on a large number of pre-defined normal and
beledimages.TodetectOODimagesduringtraining,they
anomalytemplatesandefficientextractionandaggregation
developedMCM[26]andproposedUEO,whichleverages
ofwindow/patch/image-levelfeaturesalignedwiththetext.
sample-level confidence to approximately minimize the
WinCLIPoutperformsCLIP-ACbyalargemargin.Because
conditional entropy of confident instances and maximize
ofitssimplicityandpioneeringwork,WinCLIPhasbecome
themarginalentropyoflessconfidentinstances.
an important baseline for CLIP-based AD. AnoCLIP [61]
Open-worldPromptTuning CLIP-basedOODdetectionis
follows WinCLIP’s approach of using a large number of
usefulforanewtaskcalledOpen-worldPromptTuning[124].
pre-definednormalandanomalytemplatesbutmodifiesthe
Open-world Prompt Tuning is a task that evaluates the
templates to be domain-aware (e.g., industrial photo) and
classification accuracy on a mix of known and novel ID
contrastivestatefornormalandanomaly(e.g.,perfectand
classes while training the model with known classes. To
imperfect).However,itisnoteworthythattheperformanceof
solvethisproblem,Zhouetal.[124]proposedDeCoOpwhich
theensemblestrategiesofpreviousmethodsheavilydepends
incorporatesOODdetectionintotheinferencepipelinesand
on the text descriptions [27], [61]. Also, it is observed that
improves the base-to-new separability, preventing perfor-
moredescriptionsarenotalwaysbetter[62],whichmakes
mancedegradationonnewclasses.
the previous approaches [27], [61] using a naive ensemble
of large templates somewhat uncontrollable and random
5 CLIP-BASED AD: METHODOLOGY in their applications. Therefore, SDP [62] proposes RVS, a
representativevectorselectionparadigm,whichmakesthe
Inthissection,weintroducemethodologiesforCLIP-based
mechanismofextractingrepresentativevectorsfromlarge
anomalydetection(AD)inthehopethatthecontrastwith
templatescontrollable,allowingforamorediverseselection
OOD detection clarifies the similarities and differences
ofrepresentativevectors.
betweeneachtaskandfacilitatesadeeperunderstandingof
CLIP-basedOODdetection.
5.1.2 AuxiliaryTraining-basedMethods
Unlike CLIP-based OOD detection, all auxiliary training-
5.1 Zero-shotAnomalyDetection
based methods in zero-shot AD are open-vocabulary AD
CLIP-basedzero-shotADwasproposedin2023byJeonget methods that are trained with auxiliary AD datasets and
al.[27].AlthoughitstartedabouttwoyearslaterthanOOD tested on unseen target datasets by simply changing the
detection, many methods have been proposed up to the category prompt. Existing methods perform supervised
present. training on the test set of one dataset and perform zero-
Definition of Zero-shot AD The meaning of the term shottestingontheotherdataset[29],[63],[65](e.g.,training
“Zero-shot”forzero-shotADissimilartothatforzero-shot withMVTec-ADandevaluationwithVisA.)9
In recent years, the development of auxiliary training- to generalize to new domains, as they typically require
based methods has received more attention than training- retrainingwiththetargetdatasets.WiththeadventofCLIP,
free zero-shot methods. There are two main reasons why the field of few-shot AD is shifting towards using a few
trainingisnecessaryforAD:(i)Thefirstisthedomaingap targetimagesonlyforinferenceattesttimewithouttraining.
between semantics and anomalies. CLIP is pre-trained to DefinitionofFew-shotAD CLIP-basedfew-shotADaims
understandthesemanticsofimages,sowhenappliedina to detect anomaly images using only a few images in the
zero-shot manner, it captures the semantics of the image. targetdomain.Themeaningoftheterm“Few-shot”issimilar
However, actual anomalies are not semantics, but rather to that of few-shot OOD detection. In few-shot AD, the
representthestateofanobjectandappearonlyinlocalareas term “Few-shot” refers to the use of a few normal images
oftheimage.Therefore,withouttraining,thisdomaingap in the target domain during training or inference phases.
betweensemanticsandanomaliescannotbebridged.(ii)The For instance, the method with additional training with a
second reason is that there are limitations to relying on a few normal images in the target domain can be regarded
largesetofmanuallycraftedanomalyprompts.Thisincurs asafew-shotmethod[28],[41].Evenwithouttraining,ifa
promptcreationcostsandalsomakesitdifficulttorespond methodusesafewnormalimagesinthetargetdomainasa
tounknownanomalies.Therefore,byreplacingtheanomaly reference,weregarditasafew-shotmethod[27].
prompts with learnable parameters, they aim to solve the
highcostsandlimitedadaptabilitytonewanomalies. 5.2.1 Training-freeMethods
Toaddresstheaboveissue(i),APRIL-GAN(alsoknown
The earliest approach in CLIP-based few-shot AD is Win-
as VAND) [63] was proposed. APRIL-GAN tackles the
CLIP+ [27], an improved method of WinCLIP. WinCLIP, a
domain gap between semantics and anomaly by adding
basezero-shotADmethod,cannotidentifycertaindefects
additional linear layers in vision encoders. These linear
thatcanonlybedefinedvisuallyratherthantextually.For
layers project image features at each scale into the text
example, the “Metal-nut” category in MVTecAD has an
space, creating and aggregating anomaly maps at each
anomaly type labeled “flipped upside down,” which can
stage.Similarly,SDP+[99]alsoincorporatesadditionallinear
onlybeidentifiedrelativetoanormalimage.Toaddressthis,
layers into SDP [99] to effectively project image features
WinCLIP+incorporatesafewnormalreferenceimagesinto
into the text feature space, addressing the misalignment
amemorybank[134]andcalculatestheanomalyscorewith
between image and text. To solve both the issue (i) and
thecosinesimilaritybetweenthequeryimageanditsmost
(ii), AnomalyCLIP [29] was proposed. AnomalyCLIP is a
similarimageinthememorybank.
promptlearning-basedmethodsimilartoCoOp.Byreplacing
anomaly prompts with learnable parameters, it eliminates
5.2.2 IDTraining-basedMethods
theneedtopreparealargenumberofmanuallypre-defined
Therearefewtraining-basedmethodsforCLIP-basedfew-
promptssuchasthoseinWinCLIP[27].Furthermore,unlike
shot AD research, although CLIP-based few-shot OOD
CoOpwhichlearnsobjectsemantics,AnomalyCLIPlearns
detectionactivelyexploresthetraining-basedmethods.This
object-agnostictextpromptsthatcapturegenericnormality
is likely because detecting anomalies in unknown classes
and abnormality in an image regardless of its semantics.
ishighlyvaluableinpracticalapplications,andtrainingon
To achieve this, AnomalyCLIP introduces object-agnostic
targetdatamayoversimplifythetaskoffew-shotanomaly
text prompt templates for both normal and anomaly and
detection.Thisoversimplificationconcernarisesbecausethe
performs global and local context optimization. A more
anomalyspaceforknowncategoriesismuchmorelimited
recentapproach,FiLo[65]leveragesLargeLanguageModels
compared to that for OOD detection. Training with the
(LLMs) to generate fine-grained anomaly descriptions for
target data could therefore make the task oversimplified,
eachobjectcategory.Thismethodreplacesgenericabnormal
reducing its difficulty. The only existing work in this area
descriptionswithLLM-generatedspecificanomalycontent
isPromptAD[33],apromptlearningmethodforone-class
for each sample. By adding learnable prompts before the
AD (where the normal class consists of one class). In one-
generated anomaly prompts, FiLo performs global and
class AD, traditional prompt learning methods for multi-
local context optimization, enhancing the ability to detect
class classification (e.g., CoOp [24]) do not work well. To
anomalies. As a unique direction from these methods,
addressthis,PromptADcreatesalargenumberofanomaly
RWDA [64] proposes a data augmentation approach by
promptsbyaddingalearnableanomalysuffixtothenormal
utilizing CLIP’s text embeddings as training data. RWDA
prompt. It then learns to bring the visual features closer
addsrandomlygeneratedwordsintonormalandanomaly
tothenormalpromptandfurtherawayfromtheanomaly
prompts to generate a diverse set of normal and anomaly
prompts,enablingpromptlearningforone-classAD.
training samples and trains a regular feed-forward neural
networkwithdiversetextembeddings.
5.2.3 AuxiliaryTraining-andReference-basedMethods
We explore the methods trained on auxiliary datasets and
5.2 Few-shotAnomalyDetection
utilizethenormalimagesinthetargetdomainasreferences
CLIP-basedfew-shotADwasproposedin2023byJeonget duringinference.AnearlyworkinthiscategoryisAPRIL-
al. [27], concurrently with the development of zero-shot GAN (few-shot) [63], which uses a linear layer trained
AD[27].Traditionalfew-shotADresearchfocusesonmodel- with auxiliary datasets. Similar to WinCLIP+ [27], APRIL-
ingthenormaldistributionfromalimitednumberofnormal GAN (few-shot) utilizes a few ID reference images with a
samplestodetectanomalies[126],[127],[128],[129],[130], memorybank-basedapproach[134].Morerecently,Zhuet
[131], [132], [133]. However, these methods often struggle al.[35]proposedanin-context-learning-basedmethodcalled10
InCTRL.InCTRLtrainsamodeltodiscriminateanomalies utilizeanomalyprompts.Conversely,inOODdetection,as
from normal samples by learning to identify residuals or explainedinSec.3.1,anythingsemanticallydifferentfrom
discrepanciesbetweenqueryimagesandasetoffew-shot theIDclassisconsideredOOD.Thus,utilizingnaivemanual
normalimages(in-contextsampleprompts)fromauxiliary OODpromptsisforbidden(evenifitimprovesbenchmark
data. During inference, InCTRL identifies anomalies by performance). This vastness of the OOD space is the key
measuringthediscrepancybetweenthefeaturesofthequery factor differentiating the methodologies between the two
imageandafewin-contextnormalsamplesfromthetarget fields.
dataset. EssentialFeaturesLearnedforDetectingOOD Thereisa
significantdifferenceinthefeaturesthatneedtobecaptured
5.3 OtherResearchDirection between CLIP-based OOD detection and CLIP-based AD.
In CLIP-based OOD detection, it is ideal to learn a more
5.3.1 AnomalyDetectionwithLocalizationModels
compactIDdecisionboundarythatproduceslowuncertainty
Some works [135], [136] tackle AD using SAM [57] or
fortheIDdata,withhighuncertaintyforOODdata[28].On
DINO [56], foundation models for localization. The repre-
theotherhand,CLIP-basedADaimstolearntheanomalies,
sentative work is SAA and SAA+ [135]. SAA is a simple
insteadofIDdecisionboundaryfordetectinganomalies[29].
baselineapproachandutilizesGrounding-DINO[56]forthe
Thelearnedfeaturesareentirelydifferent,whichishelpful
anomaly region generator and SAM [57] for the anomaly
indevelopingeachtraining-basedmethod.
region refiner. SAA+ is an improved method of SAA and
DifficultyofLocalizationTask CLIP-basedOODdetection
itincorporatesdomainexpertknowledgeandtargetimage
and CLIP-based AD differ significantly in the difficulty
contextintoSAA.GiventhatADnecessitateslocalization,it
of OOD (anomaly) localization tasks. In CLIP-based AD,
isexpectedthatthenumberofworksemployinglocalization
anomalysegmentationisamainstreamtask,oftenperformed
foundationmodelssuchasSAMwillcontinuetoincrease.
alongsideclassificationinmanypapers.However,inCLIP-
basedOODdetection,therehasbeennoresearchonobject-
5.3.2 MedicalAnomalyDetection
level OOD detection/segmentation. Object-level OOD de-
While most works on CLIP-based AD focus on industrial
tection aims to detect OOD objects [139], [140], [141]. The
AD,recentstudieshavebeguntochallengemedicalanomaly
inactivity is related to the size of the OOD space, and the
detection (medical AD) [35], [36], [62], [137], [138]. CLIP-
too-vast space of OOD makes it difficult to identify OOD
basedmedicalADisamorechallengingareathanindustrial
objectswithpromptseffectively.Topavethewayforfuture
ADduetothelargergapbetweendifferentdatamodalities.
development,thefoundationmodelsforlocalizationsuchas
ArepresentativeworkonmedicalADisMVFA[36].MVFAis
SAM[57],whichcansegmentindividualobjects,havethepo-
amethodspecificallytailoredformedicalAD.Itincorporates
tentialtoaddressobject-levelOODdetection/segmentation.
multipleresidualadaptersintotheCLIP’svisualencoderto
Object-levelOODdetection/segmentationusingSAMisa
reducethedomaingap,enablingastepwiseenhancementof
promisingfutureresearchdirection.
visualfeaturesacrossdifferentlevels.Thefutureprogression
of medical AD and industrial AD offers an intriguing 5.4.2 SimilaritybetweenEachMethodology
perspective, exploring whether these fields will develop Each Problem Setting Theexistingproblemsettingsfor
independently or influence each other. However, when CLIP-basedADandCLIP-basedOODdetectionaresimilar.
considering practical applications, it should be noted that Bothprimarilyfocusonzero-shotandfew-shotsettingsand
medicalADfacesthechallengethatanomaliesarenotalways canbecategorizedintotraining-free,auxiliarytraining-based,
describableinthelanguage.Therefore,thedevelopmentof andIDtraining-basedmethods.Byexaminingeachproblem
medicalADmethodsthatdonotuseCLIPisalsoimportant. settingmoreclosely,wecanobserve,forinstance,thatwhile
open-vocabularyADispredominantinCLIP-basedAD,it
5.4 Discussion hasnotbeendeeplyexploredinCLIP-basedOODdetection.
Thisprovidesvaluableinsightsintofuturedirectionsthatare
We discuss the similarities and differences between CLIP-
crucialforOODdetection.
based OOD detection and CLIP-based AD to deepen our
understandingofCLIP-basedOODdetection. History of Approaches The history of the progress of
methodsforCLIP-basedADandCLIP-basedOODdetection
5.4.1 DifferencebetweenEachMethodology issimilar.Forinstance,bothproblemsinitiallystartedwith
naive methods with manual OOD prompts (ZeroOE [38]
DifferingScopesofOOD OODdetectionandADdiffer
for OOD detection, WinCLIP [27] for AD). To address the
significantly in the scope of OOD (anomaly) they cover,
issues with these initial approaches, subsequent methods
whichleadstodifferencesinmethodologies,particularlyin
emergedthatreplacedOODpromptswithlearnableparam-
theuseofOODprompts.AsexplainedinSec.3,sensoryAD
eters (LSN [44] and NegPrompt [47] for OOD detection,
isintendedforspecificusecaseslikeindustrialinspection,
and AnomalyCLIP [29] for AD). Therefore, by carefully
where samples deviating from predefined normality (e.g.,
examiningeachother’sfields,thereispotentialformutual
defective products) are considered anomalies [11], [22]. In
enhancementandinteractioninthefuture.
other words, in sensory AD, the anomaly space is limited
to damaged objects with shared semantics, and anomalies
likeimagesofdogsarenotexpected.Thislimitedanomaly 6 EVOLUTION IN LVLM ERA
space allows even simple prompts to achieve decent per- In this section, we introduce the early advances in OOD
formance.Therefore,asshowninTable2,allADmethods detection and AD in the Large Vision Language Models11
(LVLM)era.WhileprevioussectionsfocusedonVLMssuch 3) IncompatibleVisualQuestionDetection(IVQD):
asCLIP,thissectionshiftsourfocustothemoreemerging IVQD evaluates the model’s capacity to discern
topicof“Large”VLMs.Recentadvancementsincomputer whether a question and image are unrelated or
vision have led to the emergence of LVLMs such as GPT- mismatched.
4V [30] and LLaVA [31]. Although these fields are still in
theirearlystageswithlimitedpapers,thissurveyprovides Benchmark Miyaietal.[143]createdMM-UPDBenchfor
a deep introduction to each problem in the hope that our theUPDchallenge.MM-UPDencompassesMM-AAD,MM-
detailedreviewcanhelpfosterfurtheradvancementsinthis IASD,andMM-IVQDbenchmarksforeachUPDproblem.
area. Each benchmarks are created on the top of MMBench
(dev) [162], which is a systematically designed objective
benchmark for evaluating various abilities of LVLMs. Fol-
6.1 ChangeofEachProblem
lowing the definition of each ability in MMBench (e.g.,
i.SensoryAD→SensoryAD SensoryADhascontinued “Coarse Perception: Image Scene” and “Logic Reasoning:
to develop in the LVLM era [142], [144], [145]. The use FuturePrediction”),MM-UPDevaluatesthetrustworthiness
of LVLMs has made AD applicable in many domains and ofLVLMsfromvariousabilities.
modalities[142]. Although MM-UPD is the main benchmark, the adap-
ii.OODDetection→UnsolvableProblemDetection In tation cost of creating UPD problems is not high, making
ithighlyapplicabletootherbenchmarks.Forinstance,the
the LVLM era, OOD detection has evolved into a new
recentlyproposedMuirBench[164],abenchmarkformulti-
task termed Unsolvable Problem Detection (UPD) [143].
imageunderstanding,hasincorporatedtheconceptofUPD
UPDevaluatestheLVLMs’abilitytorecognizeandabstain
byaddingunsolvableproblems.
fromansweringunexpectedorunsolvableinputquestions,
effectivelyexpandingthescopeofOODdetectionintothe Application UPDhasawiderangeofapplications,from
context of Visual Question Answering (VQA) tasks. This everyday use of LVLMs to robot manipulation. Especially
shifttotheVQAtaskhassignificantlybroadenedtheconcept whenincorporatingLVLMsintosafety-criticaldomainssuch
of OOD detection to a wider range of AI tasks involving asrobotmanipulation[165]andautonomousdriving[166],
LVLMs. there is a risk of significant problems if the LVLM fails
to identify erroneous user questions and makes incorrect
predictions. UPD serves as a task to ensure safety in such
6.2 UnsolvableProblemDetection
safety-criticalscenarios.
6.2.1 SummaryofProblem
Evaluation UPDintroducesnewevaluationmetricsthat
Background Following the recent revolutionary devel- incorporatetheconceptoftheevaluationprotocolsforOOD
opment of LLMs [146], [147], [148], [149], [150], [151], detection,takingintoaccountthepredictiondistributionfor
LVLMs [149], [152], [153], [154], [155], [156], [157], [158], bothstandard(ID)andUPD(OOD)samples.Therationale
[159]havedemonstratedremarkablecapabilitiesindiverse is that ideal LVLMs should not only give correct answers
applications[160],[161],[162],[163].However,asignificant forthestandardproblemsbutneedtowithholdanswering
concernhasarisenregardingthereliabilityofthesemodels, in the UPD scenario where the problem is unsolvable. To
specificallytheirabilitytogenerateaccurateandtrustworthy better reflect the ideal behavior of LVLMs, UPD measures
information.Thesemodelsfrequentlyproduceincorrector several metrics: (i) Standard Accuracy: The accuracy on
misleadinginformation,aphenomenonreferredtoas“hallu- standardproblemswheretheimage,question,andanswer
cination”[32].Amongthevarioushallucinationissues[32], setsareallaligned,andtheground-truthanswerisalways
thechallengeofidentifyingout-of-placequestionsiscrucial present within the provided options. (ii) UPD Accuracy:
for deploying LVLMs in safety-critical applications. This The accuracy of AAD/IASD/IVQD problems. (iii) Dual
challengeextendstheconceptofOODdetectiontotheVQA Accuracy:AccuracyofstandardandUPDpairs.Wecount
tasksforLVLMsandrepresentsaspecificaspectofLVLMs’ success only if the model is correct for both standard and
trustworthiness. UPDproblems.
Definition UnsolvableProblemDetection(UPD)isatask
tomeasurethetrustworthinessofLVLMs,whichisdesigned 6.2.2 Findings
toevaluatemodels’capacitytowithholdanswerswhenfaced Inthefollowing,webrieflysummarizedthefindingsofthe
withunsolvableproblems.TheUPDtaskcanbecategorized UPDchallenge[143].
intothreedistinctproblemtypes:AbsentAnswerDetection
1.MostLVLMsHardlyHesitatetoAnswer.MostLVLMs,
(AAD), Incompatible Answer Set Detection (IASD), and
especiallyopen-sourceLVLMs,havesignificantlylowUPD
IncompatibleVisualQuestionDetection(IVQD).Thedetails
accuracies, which indicates the difficulty of the UPD chal-
ofeachsettingareasfollows:
lenge. For example, LLaVA-1.5 [31] and CogVLM [155],
1) Absent Answer Detection (AAD):AADevaluates whicharestate-of-the-artLVLMs,completelyfailtowithhold
themodel’scapacitytodeterminewhenthecorrect answering.GPT-4Vachieveshigherperformancesthanother
answerisabsentfromtheprovidedoptions. LVLMs due to its safety training process [167]. However,
2) IncompatibleAnswerSetDetection(IASD):IASD thereisstillaperformancegapfromtheupperboundscores.
assessesthemodel’sabilitytodiscernanswerchoices 2.PerformanceTendencyDiffersalotbyEachAbilityin
thatarecompletelyirrelevanttothegivenquestion theBenchmark.TheperformanceofLVLMsdiffersineach
andimage. abilityintheMM-UPDBench.Forinstance,GPT-4Vhasits12
(i) Sensory AD (ii) OOD Detection
Sensory AD in LVLM era Unsolvable Problem Detection
Absent Answer Incompatible Answer Incompatible Visual
Detection (AAD) SetDetection (IASD) Question Detection (IVQD)
What color is What color is What color is
the car? the car? the car?
A. Green A. Right A. Green
B. Red B. Acute B. Red
C. Blue C. Obtuse C. Blue
I choose C. I choose C. I choose C.
LLaVA1.5 LLaVA1.5 LLaVA1.5
None of the None of the Cannot
above! above! answer!
Fig.4:OverviewoftheevolutionofeachproblemintheLargeVisionLanguageModel(LVLM)era.(i)SensoryADhas
becomeahighlyactiveandnoteworthyfieldintheLVLMera[142].(ii)Notably,OODdetectionisevolvingintoanewtask
calledUnsolvableProblemDetection[143]intheLVLMera.Figureadaptedfrom[142]andpartiallyfrom[143].
limitationinattributecomparisonandLLaVA-NeXT-34Bhas MVTec-AD [52]), point cloud anomaly detection (MVTec
itslimitationinobjectlocalization. 3D[168]),medicalimageanomalydetection/localization(e.g.,
3.EffectivePromptStrategiesVaryAcrossDifferentLVLMs ChestX-ray[169],HeadCT[170]),logicalanomalydetection
(e.g.,MVTecLOCO[171]),pedestriananomalydetection(e.g.,
Effective prompt strategies vary across different LVLMs.
UCF-Crime Dataset [172]), traffic anomaly detection (e.g.,
In the original paper, they experimented with an option-
KaggleAccidentDetection[173]),andtimeseriesanomaly
based prompt approach that adds an option of “None of
detection(e.g.,OutlierDetectionDataset[174]).
the above” and an instruction-based approach that adds
aninstruction“Ifalloptionsareincorrect,answerNoneof Evaluation EvaluationinanomalydetectionwithLVLMs
theabove”.Asaresult,theeffectivenessofeachapproach is an open challenge. AnomalyGPT [144] asks LVLMs the
differssignificantlydependingonthetypeofLVLMs.This question“Isthereananomalyinthisimage?”anddetermines
highlights the difficulty of finding an effective prompt anomalyornormalbasedonthesimplerule-basedapproach
strategyforallLVLMs. ofwhethertheresponsecontainsa“yes”or“no”.However,
this rule-based approach is not robust, as a response is
considered correct even if the explanation following “yes”
6.3 AnomalyDetectioninLVLMEra
iscompletelyincorrect.Ontheotherhand,Caoetal.[142]
6.3.1 SummaryofProblem conductedonlyqualitativeevaluationsandleftquantitative
Background Anomalydetectionisacrucialtaskinavari- evaluationsasanopenchallenge.Therefore,theevaluation
etyofdomainsanddatatypes.However,existinganomaly ofanomalydetectionbyLVLMisafuturechallenge.
detectionmodelsareoftendesignedforspecificdomainsor
6.3.2 Findings
modalities [142]. Also, current AD methods only provide
ananomalyscoreforthetestsampleandrequireamanual Caoetal.[142]describedtheobservationsofGPT-4Vinthe
threshold to distinguish between normal and anomalous paper,sowebrieflysummarizedthemhere.
instances for each sample [144]. To facilitate real-world 1.GPT-4VExcelsinZero/One-shotSettingsacrossVarious
applications, developing a system capable of expressing Modalities and Fields. GPT-4V shows proficiency in
anomalies in natural language across various modalities identifyinganomaliesinmulti-modality(e.g.,images,point
anddomainsiscrucialforensuringaccessibilitytoawider clouds, X-rays) and multi-field (e.g., industrial, medical,
rangeofusers. pedestrian, traffic, and time series anomaly detection). In
Definition ThedefinitionofADremainsconsistentwith addition,GPT-4Vdemonstratesstrongperformanceinboth
conventionalandCLIP-basedAD,aimingtoidentifysamples zero-shotandone-shotsettings.
thatdeviatefrompredefinednormality.Thekeydistinction 2.GPT-4VcanUnderstandBothGlobalandFine-grained
lies in the output. While previous methods produced an Anomalies. GPT-4Vcanrecognizebothglobalandlocal
anomaly score requiring a manual threshold, AD with abnormalpatternsorbehaviors,whichindicatestheability
LVLMsaimstorecognizeanddescribeanomaliesusingtext, tounderstandglobalandfine-grainedsemantics.
eliminatingtheneedformanualthresholdsandenhancing
3.GPT-4VcanbeEnhancedwithIncreasingPrompts. By
humaninterpretability.
givingmorecontextandinformation,themodelsignificantly
Benchmark Since the field of AD with LVLMs is in its improvesitsabilitytodetectanomaliesaccurately.
infantstage,therearestillnounifiedbenchmarks.Anoma-
lyGPT [144] focuses on industrial image anomaly detec-
7 POTENTIAL CHALLENGES
tion/localizationandusesthestandardbenchmarksMVTec-
AD[52]andVisA[53].Morerecently,Caoetal.[142]extend This section discusses potential challenges for CLIP-based
thedomainandmodalityanddemonstratetheapplications OODdetectionthatmaybehighlightedbythewidespread
in industrial image anomaly detection/localization (e.g., adoption of our framework. Since similar or ambiguous13
Semantic Shift benchmark setup that randomly selects subset ID classes
Training ID fromallIDclasses[47].
Zero-shotOV-OODDetection Zero-shotOV-OODdetec-
tionsharesthetrainingsettingwithin-classevaluation,using
Test ID encompass
asmallsubsetofIDclasses.However,itdiffersinthetesting
ID Class Subset all ID classes.
phase,whereitexclusivelyutilizesclassesnotincludedin
Test ID Test OOD thetrainingsubset.Thisproblemsettingisconsistentwith
the common open-vocabulary setting [175], [176] and the
open-vocabulary setting in CLIP-AD [29], [63], [65]. Zero-
All ID Classes shotOV-OODdetectioncanbepositionedwithintheareaof
(a) In-class Open-vocabulary OOD Detection zero-shotOODdetection,whichwouldfacilitatetheresearch
Semantic Shift of zero-shot OOD detection. When selecting the subset of
classesfortraining,weshouldensuresemanticdissimilarity
Training ID
between the classes in the evaluation and training sets to
guaranteethevalidityofthezero-shotproblemsetting.For
Test ID exclude
instance,whensplittingImageNetclassesintotrainingand
training ID classes.
ID Class Subset evaluationsubsetsforzero-shotOV-OODdetection,itwould
bepreferabletoconsiderthehierarchicalclassesofImageNet
Test ID Test OOD
toavoidsimilarsemanticsbetweentrainingandevaluation
subsets.
ID Class Subset
7.2 CLIP-basedFull-spectrumOODDetection
(b) Zero-shot Open-vocabulary OOD Detection
CLIP-based full-spectrum OOD detection is a well-
Fig.5:Illustrationofthesettingsforin-classopen-vocabulary
established task with existing research [115]. However,
OOD detection and zero-shot open-vocabulary OOD de-
with the future prevalence of hard OOD detection, which
tection.In-classopen-vocabularyOODdetectionshouldbe
incorporatestheOSRsetup,thedistinctionbetweenthistask
comparedwithfew-shotOODdetectionmethods,whilezero-
and open-set domain generalization (OSDG) [177], [178],
shotopen-vocabularyOODdetectionshouldbediscussedin
[179],[180]maybecomeambiguous.OSDGsharesasimilar
thecontextofzero-shotOODdetection.
motivationofrealizingmodelscapableofbothgeneralization
and detection. The illustration of both problem settings is
shown in Fig. 6 (a)(b). OSDG assumes that only data with
problemsettingscurrentlyexist,wepresentthesechallenges
covariate shifts will be input during testing, and aims to
toavoidfutureconfusionforreaders.
classifyinputsamplesintooneofIDclassesiftheirsemantics
alignwithIDclassesanddetectOODthatexhibitsemantic
shifts.OSDGisaresearchareawithamuchsmallernumber
7.1 Open-vocabularyOODDetection
of works compared to the main topics of this survey (OD,
The existing setting for open-vocabulary OOD (OV-OOD) AD, ND, OSR, and OOD detection), but, in recent years,
detection [47] involves training with images from a small someworkshavetackledOSDGusingCLIP[71],[181],[182].
subsetofIDclassesandevaluatingwithallIDclassnames, To eliminate potential ambiguity, this paper defines a
asexplainedinSec.4.2.1.However,thisproblemdefinition hard full-spectrum OOD (FS-OOD) detection, following
hasseveralissues:(i)TheOODdetectionperformanceonID our generalized OOD detection v2. The illustration of the
classesnotincludedinthetrainingsubsetisunclear,(ii)This problem setting for hard FS-OOD detection is shown in
contradictsthesettingsoftheexistingopen-vocabularyset- Fig. 6 (c). Hard FS-OOD detection expands the scope of
tings,includingCLIP-basedAD(Sec.5.1.2),whichassumeno existingFS-OODdetectionbyutilizingtheconventionalOSR
explicitclassoverlapbetweentrainingandtestIDdata[29], setup.ItintroducesOODsamplesfromdifferentcategories
[63], [65], [175], [176], leading to confusion in the fields. within the training domain and OOD samples that share
Therefore,forclearevaluationandcommonunderstanding the same covariate shift as the target domain but exhibit
in thefield, weredefinethe OV-OOD detection asin-class semantic shifts. This task can be regarded as an extensive
open-vocabularyOODdetection(in-classOV-OODdetection) versionofbothexistingFS-OODdetectionandOSDGtasks.
and zero-shot open-vocabulary OOD detection (zero-shot HardFS-OODdetectionrepresentsadevelopingchallenge
OV-OODdetection).Theillustrationforeachsettingisshown withpromisingpotentialforfutureresearch.
inFig.5.Detailedexplanationsforeachsettingareasfollows:
In-classOV-OODDetection In-classOV-OODdetectionis
8 FUTURE DIRECTIONS
thesameastheoriginalsettingproposedby [47],involving In this section, we discuss the future directions of OOD
trainingwithimagesfromasmallsubsetofIDclassesand detection and UPD. For OOD detection, we explore not
evaluatingonallIDclassnamesincludingtrainingclasses.In- onlyOODdetectionforVLMsbutalsosingle-modalOOD
classOV-OODdetectionshouldbeevaluatedinthefew-shot detection, with a specific focus on emerging challenges
OOD detection setting to assess how well these methods as VLMs evolve. For a discussion of the long-standing
retain their performance when trained with limited class challenges in OOD detection, we can refer the readers to
data. In this setting, it is preferable to employ the existing thepreviousgeneralizedOODdetectionpaper[22].
gniniarT
tseT
gniniarT
tseT14
Semantic Shift (Detection) Semantic Shift (Detection) Semantic Shift (Detection)
Test ID Test OOD Test ID Test OOD
ID in training domain ID in training domain Non exist ID in training domain
ImageNet SSB ImageNet-O ImageNet SSB ImageNet-O ImageNet SSB ImageNet-O
Test ID Test OOD
Covariate-shifted ID Covariate-shifted ID Covariate-shifted ID
ImageNet-R iNaturalist Texture ImageNet-R iNaturalist Texture ImageNet-R iNaturalist Texture
(a) Common Full-spectrum OOD Detection (b) Open-set Domain Generalization (c) Hard Full-spectrum OOD Detection
Fig.6:Illustrationofthesettingsforfull-spectrumOODdetection,open-setdomaingeneralization,andhardfull-spectrum
OODdetection.Hardfull-spectrumOODdetectionisapotentialchallengearisingfromourgeneralizedOODdetectionv2.
8.1 OODDetectinforVisionLanguageModels gapwithclosed-setIDclassifiers.
a. Hard OOD Detection Hard OOD detection will be- d.Training-freeFew-shotOODDetection Theresearch
come increasingly important in the future due to its high directionoftraining-freefew-shotOODdetectionisstillin
practicalityandthechallengingnatureoftheproblem.Hard its infant stage, with only one existing study [99]. Given
OOD detection utilizes the OSR benchmark setup, where theprevalenceoftraining-basedmethodsinfew-shotOOD
someclasseswithinasingledatasetaredesignatedasIDand detection, proposing methods that do not require training
othersasOOD.Inthisfield,notonlysmalldatasetssuchas is crucial. Considering the advancements of training-free
ImageNet-10andImageNet-20[26]butalsodatasetswitha methods in CLIP-based anomaly detection, we anticipate
largernumberofclassessuchasImageNet-protocol[54]have a similar trajectory for CLIP-based OOD detection. Future
beenproposed.Manyexistingstudies,suchasLoCoOp[28] directionsincluderefiningadapter-basedmethodsorleverag-
and LSN [44], primarily use the common ImageNet OOD ingexternalknowledgesuchasretrievalaugmentation[192],
benchmark, so hard OOD detection has not yet been well [193].Addressingtraining-freefew-shotOODdetectionisa
studied.Thisfieldwilldevelopfurtherinthefuture. pivotalsteptowardsrealizingmorecomputationallyefficient
b. Post-hoc Methods To propose post-hoc methods is OODdetectioninthefuture.
important for the fundamental performance improvement
e. Full-spectrum OOD Detection CLIP-based full-
of CLIP-based OOD detection. The methods of directly
spectrumOOD(FS-OOD)detectionisapromisingresearch
employing an ID classifier such as MCM [26] are called
area[115],[116].Inpracticalapplications,thereisastrong
post-hocmethods.PriortoCLIP,variousapproacheswere
motivationtocreatemodelsthatcannotonlydetectseman-
proposed [101],[102],[103],[104],[105],[108],[109],[111],
ticallyshiftedOODinputsbutalsogeneralizetocovariate-
[183],[184].However,CLIP-basedpost-hocmethodsoften
shifteddata[116],[194].WithinCLIP-basedmethods,OOD
underperform methods with additional OOD prompts, so
detectionandgeneralizationareoftendiscussedinseparate
they are not extensively researched in zero-shot OOD de-
contexts[28],[195],resultinginatrade-offbetweendetection
tection. However, we should focus on the scalability of
and generalization performance [98]. Furthermore, there
post-hoc methods. The post-hoc methods [26], [96] can be
existspotentialambiguityinthedistinctionbetweenFS-OOD
easilyappliedtomanysubsequentmethods[28],[44],[47],
detectionandopen-setdomaingeneralization.Toeliminate
[185], bringing fundamental performance improvements.
the potential ambiguity, we have formulated a potential
Furthermore, very recently, post-hoc methods specifically
problemsettingcalledhardFS-OODdetection.Wehopethis
tailoredforpromptlearningmethodshavealsoemerged[86].
surveyinspiresfurtheradvancementsanddevelopmentsfor
Therefore,proposingpost-hocmethodsanddemonstrating
FS-OODdetection.
the improvements not only in zero-shot but also in subse-
quent few-shot settings [28], [185] is crucial, even if they
f. Open-vocabulary OOD Detection Open-vocabulary
underperformmethodsusingOODpromptsinthezero-shot
OOD(OV-OOD)detectionhasahighpracticalpotential,but
setting. This field should continue evolving, mirroring its
itisstillinitsinfantstages[47].Inparticular,asexplainedin
growthbeforetheadventofCLIP.
Sec.7.1,zero-shotOV-OODdetectionisapotentialresearch
c. Bridging the Gap with Closed-set Classifiers. OOD field. We hope that this survey paper will inspire future
detectionensuresthesafetyofIDclassifiers,soitiscrucial effortsinOV-OODdetection.
to bridge the gap between the advancements in existing
closed-set classifiers and OOD detection. Currently, the g.Object-levelOODDetection Object-levelOODdetec-
representative method for few-shot OOD detection is Lo- tion remains an unexplored area. As discussed in Sec. 5.4,
CoOp[28],atextpromptlearningmethodbasedonCoOp. thisisduetothevastnessoftheOODspace,whichmakes
However,inclosed-setsettings,few-shotlearningmethods itdifficulttoidentifyOODobjectsusingtexts.Topavethe
basedontextpromptlearninghavebeenproposedotherthan way for future advancements in object-level OOD detec-
CoOp [186], [187], [188]. Furthermore, text-based prompt tion/segmentation,foundationmodelsforlocalization,such
learningmethodsonlytrainthetextprompts,sotheycannot asSAM[57],offerapromisingsolution.Byintegratingthese
handle differences in image domains. Therefore, adopting models with methods such as MCM [26], we can poten-
methods that can handle image domain differences [189], tiallyachieveobject-levelOODdetectionandsegmentation,
[190],[191]forOODdetectionisessentialforbridgingthe openingupanewfrontierinOODdetectionresearch.
tfihS
etairavoC
)noitazilareneG( tfihS
etairavoC
)noitazilareneG( tfihS
etairavoC
)noitazilareneG(15
8.2 Single-modalOODDetection 9 CONCLUSION
a.LeveragingLargePre-trainedModels Leveraginglarge Inthissurvey,wecomprehensivelyreviewtheevolutionof
thefiveproblemsincludingAD,ND,OSR,OODdetection,
pre-trained models is important for single-modal OOD
and OD in the VLM era, and propose a framework of
detection.NumerousmethodsforOODdetectionconduct
generalized OOD detection v2. Our framework identifies
experimentsusingbackbonestrainedfromscratchanddonot
OOD detection and AD as the primary challenges in the
utilizepre-trainedmodels[22],[101],[104],[108],[109],[119],
[196],[197].Inarecentstudy,Miyaietal.[198]systematically VLMera,whichhighlightsthedemandingchallengesand
fosters collaborative efforts among each community. By
investigated the impact of pre-training on OOD detection
articulatingtheshiftsinthedefinitions,problemsettings,and
from both the perspectives of the types of OOD data
and pre-training algorithms [199], [200]. Dong et al. [201] benchmarks,weencouragesubsequentworkstoaccurately
understandtheirevolvingtargetproblemsintheVLMera.
exploredparameter-efficientlearningforsingle-modalOOD
Bysortingoutthemethodologies,wehopethatreaderscan
detection and proposed DSGF, which leverages both fine-
easily grasp the mainstream methods, identify important
tuned features and original pre-trained features. While
baselines and novel problem settings, and propose future
leveraginglargepre-trainedmodels[202]withlightweight
solutions.BysheddinglightonrecentstudiesintheLVLM
tuningisanactiveareaofresearchinsingle-modalclosed-set
era, we hope that researchers within each community can
classification[191],[203],therehavebeenlimitedstudiesfor
identifypromisingresearchdirectionsinthisemergingera.
single-modal OOD detection, which presents a promising
Byprovidingfuturedirections,wehopethatoursurveywill
avenueforfutureresearch.
clarifythetaskstobetackledbyfutureworksintheVLM
b.Real-worldBenchmarksandEvaluations Considering
and LVLM era, thereby facilitating future advances in the
thefuturedevelopmentofCLIP-basedOODdetection,there
rightdirection.
shouldbeincreasingfocusonexpandingthescopeofbench-
markstoencompassreal-worldscenarioswhereCLIPisless
applicable.Forinstance,recently,Baeketal.[204]introduced
ACKNOWLEDGMENT
ImageNet-ES,consistingofvariationsinenvironmentaland ThisworkwassupportedbyJSTBOOST,JapanGrantNum-
camerasensorfactorsbydirectlycapturing202kimageswith ber JPMJBS2418 and JST JPMJCR22U4. We thank Toyooka
anactualcamerainacontrolledtestbed,whichbridgesthe Mashiro(AYMLabatUTokyo)forhisvaluableassistancein
gap between the common benchmark and the real-world designingthefigures,andKazukiEgashira,YukiImajuku,
scenario. Besides, utilizing datasets such as WILDS [205], TakubonSon,andZaiyingZhao(AYMLabatUTokyo)for
[206],whichconsiderreal-worlddatashifts,ordatasetsfor valuablefeedbackonthepaper.
medicalOODdetection[207],canprovidevaluableinsights,
especially in safety-critical applications like autonomous
REFERENCES
drivingandmedicalimageanalysis.
[1] D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman,
andD.Mane´,“ConcreteproblemsinAIsafety,”arXivpreprint
arXiv:1606.06565,2016. 1
8.3 UnsolvableProblemDetection
[2] S.Mohseni,H.Wang,Z.Yu,C.Xiao,Z.Wang,andJ.Yadawa,
“Practicalmachinelearningsafety:Asurveyandprimer,”arXiv
a.ExploringEffectiveSolutions Itisimportanttopropose
preprintarXiv:2106.04823,2021. 1
effectivesolutionsforUPD.Oneofthepotentialapproaches [3] D.Hendrycks,N.Carlini,J.Schulman,andJ.Steinhardt,“Un-
istoadaptthemethodologiesofOODdetectiontoUPD.For solvedproblemsinMLsafety,”arXivpreprintarXiv:2109.13916,
2021. 1
example, the perplexity of the LVLM’s response could be
[4] D.HendrycksandM.Mazeika,“X-riskanalysisforAIresearch,”
usedasascoretoidentifyunsolvablequeries.Furthermore, arXivpreprintarXiv:2206.05862,2022. 1
proposingmodel-agnosticpost-hocmethodsisalsoimpor- [5] A.Krizhevsky,I.Sutskever,andG.E.Hinton,“Imagenetclassifi-
tant to enhance the reliability of many LVLMs. Therefore, cationwithdeepconvolutionalneuralnetworks,”inNIPS,2012.
1
incorporatingtheconceptsofOODdetectiontechniquesfor
[6] K.He,X.Zhang,S.Ren,andJ.Sun,“Delvingdeepintorectifiers:
UPDisanimportantdirectionforfuturework. Surpassinghuman-levelperformanceonimagenetclassification,”
inICCV,2015. 1
b. Extension to Diverse Benchmarks MM-UPD Bench
[7] C.C.AggarwalandP.S.Yu,“Outlierdetectionforhighdimen-
consists of general QA datasets. However, UPD can be in- sionaldata,”inACMSIGMOD,2001. 1
corporatedintomorediversebenchmarksincludingdomain- [8] V.HodgeandJ.Austin,“Asurveyofoutlierdetectionmethodolo-
gies,”Artificialintelligencereview,2004. 1
specific knowledge for advanced reasoning [163], [208]
[9] I. Ben-Gal, “Outlier detection,” in Data Mining and Knowledge
and multi-image understanding [164]. For example, Muir- DiscoveryHandbook,2005. 1
Bench [164] incorporated the concept of UPD as a metric [10] H. Wang, M. J. Bah, and M. Hammad, “Progress in outlier
forrobustevaluation.IntegratingtheconceptofUPDinto detectiontechniques:Asurvey,”IEEEAccess,vol.7,pp.107964–
108000,2019. 1
benchmarks is essential for evaluating the robustness and
[11] L. Ruff, J. R. Kauffmann, R. A. Vandermeulen, G. Montavon,
trustworthinessofLVLMsintheirtargettasks. W.Samek,M.Kloft,T.G.Dietterich,andK.-R.Mu¨ller,“Aunifying
reviewofdeepandshallowanomalydetection,”Proceedingsofthe
c. Theoretical Understanding of UPD Theoretically un-
IEEE,vol.109,no.5,pp.756–795,2021. 1,2,5,10
derstandingthereasonsbehindthedifficultyofUPDcould [12] G.Pang,C.Shen,L.Cao,andA.V.D.Hengel,“Deeplearningfor
providethecommunitywithvaluableinsights.Theorizing anomalydetection:Areview,”ACMComput.Surv.,vol.54,no.2,
thebehaviorofLVLMsposesasharedchallengeinthefield, 2021. 1,2
[13] S. Bulusu, B. Kailkhura, B. Li, P. K. Varshney, and D. Song,
highlightingtheimportanceofcollaborativeeffortswithin
“Anomalousexampledetectionindeeplearning:Asurvey,”IEEE
thecommunity. Access,vol.8,pp.132330–132347,2020. 1,216
[14] R. Chalapathy and S. Chawla, “Deep learning for anomaly [41] Y. Ming and Y. Li, “How does fine-tuning impact out-of-
detection: A survey,” arXiv preprint arXiv:1901.03407, 2019. 1, distributiondetectionforvision-languagemodels?,”IJCV,vol.132,
2 no.2,pp.596–609,2024. 2,6,7,9
[15] M.A.Pimentel,D.A.Clifton,L.Clifton,andL.Tarassenko,“A [42] W.Tu,W.Deng,andT.Gedeon,“Acloserlookattherobustnessof
reviewofnoveltydetection,”Signalprocessing,vol.99,pp.215–249, contrastivelanguage-imagepre-training(clip),”inNeurIPS,2023.
2014. 1 2
[16] D.Miljkovic´,“Reviewofnoveltydetectionmethods,”inMIPRO, [43] S.Park,J.Mok,D.Jung,S.Lee,andS.Yoon,“Onthepowerfulness
2010. 1 oftextualoutlierexposureforvisualooddetection,”inNeurIPS,
[17] M.MarkouandS.Singh,“Noveltydetection:areview—part1: 2023. 2
statisticalapproaches,”Signalprocessing,vol.83,no.12,pp.2481– [44] J.Nie,Y.Zhang,Z.Fang,T.Liu,B.Han,andX.Tian,“Out-of-
2497,2003. 1 distributiondetectionwithnegativeprompts,”inICLR,2023. 2,6,
[18] M. Markou and S. Singh, “Novelty detection: a review—part 7,10,14
2::neuralnetworkbasedapproaches,”Signalprocessing,vol.83, [45] X.Jiang,F.Liu,Z.Fang,H.Chen,T.Liu,F.Zheng,andB.Han,
no.12,pp.2499–2521,2003. 1 “Negative label guided ood detection with pretrained vision-
[19] T. E. Boult, S. Cruz, A. R. Dhamija, M. Gunther, J. Henrydoss, languagemodels,”inICLR,2024. 2,5,6
andW.J.Scheirer,“Learningandtheunknown:Surveyingsteps [46] Y.Bai,Z.Han,C.Zhang,B.Cao,X.Jiang,andQ.Hu,“Id-like
towardopenworldrecognition,”inAAAI,2019. 1 promptlearningforfew-shotout-of-distributiondetection,”in
[20] C.Geng,S.-j.Huang,andS.Chen,“Recentadvancesinopenset CVPR,2024. 2,5,6,7
recognition:Asurvey,”IEEETPAMI,vol.43,no.10,pp.3614–3631, [47] T.Li,G.Pang,X.Bai,W.Miao,andJ.Zheng,“Learningtransfer-
2020. 1,2 ablenegativepromptsforout-of-distributiondetection,”inCVPR,
[21] A.MahdaviandM.Carvalho,“Asurveyonopensetrecognition,” 2024. 2,3,4,5,6,7,10,13,14
arXivpreprintarXiv:2109.00893,2021. 1 [48] C.Cao,Z.Zhong,Z.Zhou,Y.Liu,T.Liu,andB.Han,“Envisioning
[22] J.Yang,K.Zhou,Y.Li,andZ.Liu,“Generalizedout-of-distribution outlierexposurebylargelanguagemodelsforout-of-distribution
detection:Asurvey,”IJCV,pp.1–28,2024. 1,2,3,4,5,10,13,15 detection,”inICML,2024. 2,5,6
[49] J.Liang,L.Sheng,Z.Wang,R.He,andT.Tan,“Realisticunsuper-
[23] A.Radford,J.W.Kim,C.Hallacy,A.Ramesh,G.Goh,S.Agarwal,
visedCLIPfine-tuningwithuniversalentropyoptimization,”in
G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., “Learning
ICML,2024. 2,4,8
transferablevisualmodelsfromnaturallanguagesupervision,”in
ICML,2021. 1,3 [50] Y.Cao,X.Xu,J.Zhang,Y.Cheng,X.Huang,G.Pang,andW.Shen,
“Asurveyonvisualanomalydetection:Challenge,approach,and
[24] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, “Learning to prompt
prospect,”arXivpreprintarXiv:2401.16402,2024. 2
forvision-languagemodels,”IJCV,vol.130,no.9,pp.2337–2348,
[51] J. Liu, G. Xie, J. Wang, S. Li, C. Wang, F. Zheng, and Y. Jin,
2022. 1,7,9
“Deepindustrialimageanomalydetection:Asurvey,”Machine
[25] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, “Conditional prompt
IntelligenceResearch,vol.21,no.1,pp.104–135,2024. 2
learningforvision-languagemodels,”inCVPR,2022. 1
[52] P.Bergmann,M.Fauser,D.Sattlegger,andC.Steger,“Mvtecad–
[26] Y.Ming,Z.Cai,J.Gu,Y.Sun,W.Li,andY.Li,“Delvingintoout-
acomprehensivereal-worlddatasetforunsupervisedanomaly
of-distributiondetectionwithvision-languagerepresentations,”
detection,”inCVPR,2019. 3,4,5,12
inNeurIPS,2022. 1,2,3,4,5,6,7,8,14
[53] Y.Zou,J.Jeong,L.Pemula,D.Zhang,andO.Dabeer,“Spot-the-
[27] J. Jeong, Y. Zou, T. Kim, D. Zhang, A. Ravichandran, and
differenceself-supervisedpre-trainingforanomalydetectionand
O.Dabeer,“Winclip:Zero-/few-shotanomalyclassificationand
segmentation,”inECCV,2022. 3,4,5,12
segmentation,”inCVPR,2023. 1,2,4,5,6,7,8,9,10
[54] A.Palechor,A.Bhoumik,andM.Gu¨nther,“Large-scaleopen-set
[28] A.Miyai,Q.Yu,G.Irie,andK.Aizawa,“Locoop:Few-shotout-
classificationprotocolsforimagenet,”inWACV,2023.3,4,5,7,14
of-distributiondetectionviapromptlearning,”inNeurIPS,2023.
[55] M.Salehi,H.Mirzaei,D.Hendrycks,Y.Li,M.H.Rohban,and
1,2,5,6,7,9,10,14
M.Sabokrou,“Aunifiedsurveyonanomaly,novelty,open-set,
[29] Q. Zhou, G. Pang, Y. Tian, S. He, and J. Chen, “Anomalyclip:
andoutof-distributiondetection:Solutionsandfuturechallenges,”
Object-agnosticpromptlearningforzero-shotanomalydetection,”
TransactionsonMachineLearningResearch,2022. 3,4
inICLR,2024. 1,2,4,5,6,8,9,10,13
[56] S.Liu,Z.Zeng,T.Ren,F.Li,H.Zhang,J.Yang,C.Li,J.Yang,
[30] C. Lu, C. Qian, G. Zheng, H. Fan, H. Gao, et al., “From gpt-
H. Su, J. Zhu, et al., “Grounding dino: Marrying dino with
4 to gemini and beyond: Assessing the landscape of mllms
groundedpre-trainingforopen-setobjectdetection,”arXivpreprint
ongeneralizability,trustworthinessandcausalitythroughfour
arXiv:2303.05499,2023. 3,10
modalities,”arXivpreprintarXiv:2401.15071,2024. 2,11
[57] A.Kirillov,E.Mintun,N.Ravi,H.Mao,C.Rolland,L.Gustafson,
[31] H.Liu,C.Li,Y.Li,andY.J.Lee,“Improvedbaselineswithvisual
T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al., “Segment
instructiontuning,”inCVPR,2024. 2,11
anything,”inICCV,2023. 3,10,14
[32] Z.Bai,P.Wang,T.Xiao,T.He,Z.Han,Z.Zhang,andM.Z.Shou, [58] H. Du, S. Zhang, B. Xie, G. Nan, J. Zhang, J. Xu, H. Liu,
“Hallucinationofmultimodallargelanguagemodels:Asurvey,” S.Leng,J.Liu,H.Fan,etal.,“Uncoveringwhatwhyandhow:A
arXivpreprintarXiv:2404.18930,2024. 2,11 comprehensivebenchmarkforcausationunderstandingofvideo
[33] X. Li, Z. Zhang, X. Tan, C. Chen, Y. Qu, Y. Xie, and L. Ma, anomaly,”inCVPR,2024. 4
“Promptad:Learningpromptswithonlynormalsamplesforfew- [59] G. Zara, S. Roy, P. Rota, and E. Ricci, “Autolabel: Clip-based
shotanomalydetection,”inCVPR,2024. 2,4,5,6,7,9 frameworkforopen-setvideodomainadaptation,”inCVPR,2023.
[34] C.-H.Ho,K.-C.Peng,andN.Vasconcelos,“Long-tailedanomaly 4
detectionwithlearnableclassnames,”inCVPR,2024. 2 [60] P.Wu,X.Zhou,G.Pang,Y.Sun,J.Liu,P.Wang,andY.Zhang,
[35] J.ZhuandG.Pang,“Towardgeneralistanomalydetectionvia “Open-vocabularyvideoanomalydetection,”inCVPR,2024. 4
in-contextresiduallearningwithfew-shotsampleprompts,”in [61] H.Deng,Z.Zhang,J.Bao,andX.Li,“Anovl:Adaptingvision-
CVPR,2024. 2,4,5,6,9,10 language models for unified zero-shot anomaly localization,”
[36] C. Huang, A. Jiang, J. Feng, Y. Zhang, X. Wang, and Y. Wang, arXivpreprintarXiv:2308.15939,2023. 4,5,6,8
“Adapting visual-language models for generalizable anomaly [62] X.Chen,J.Zhang,G.Tian,H.He,W.Zhang,Y.Wang,C.Wang,
detectioninmedicalimages,”inCVPR,2024. 2,10 Y. Wu, and Y. Liu, “Clip-ad: A language-guided staged dual-
[37] P.Liznerski,L.Ruff,R.A.Vandermeulen,B.J.Franks,K.-R.Mu¨ller, path model for zero-shot anomaly detection,” arXiv preprint
andM.Kloft,“Exposingoutlierexposure:Whatcanbelearned arXiv:2311.00453,2023. 4,6,8,10
fromfew,one,andzerooutlierimages,”TransactionsonMachine [63] X. Chen, Y. Han, and J. Zhang, “A zero-/few-shot anomaly
LearningResearch,2022. 2,4 classification and segmentation method for cvpr 2023 vand
[38] S.Fort,J.Ren,andB.Lakshminarayanan,“Exploringthelimitsof workshopchallengetracks1&2:1stplaceonzero-shotadand4th
out-of-distributiondetection,”inNeurIPS,2021. 2,5,6,10 placeonfew-shotad,”arXivpreprintarXiv:2305.17382,2023. 4,5,
[39] S.Esmaeilpour,B.Liu,E.Robertson,andL.Shu,“Zero-shotout- 6,8,9,13
of-distributiondetectionbasedonthepretrainedmodelclip,”in [64] M.Tamura,“Randomworddataaugmentationwithclipforzero-
AAAI,2022. 2,5,6 shotanomalydetection,”inBMVC,2023. 4,6,8,9
[40] H. Wang, Y. Li, H. Yao, and X. Li, “Clipn for zero-shot ood [65] Z. Gu, B. Zhu, G. Zhu, Y. Chen, H. Li, M. Tang, and J. Wang,
detection:Teachingcliptosayno,”inICCV,2023. 2,5,6,7 “Filo:Zero-shotanomalydetectionbyfine-graineddescriptionand17
high-qualitylocalization,”arXivpreprintarXiv:2404.13671,2024. 4, [93] S.Thulasidasan,S.Thapa,S.Dhaubhadel,G.Chennupati,T.Bhat-
5,6,8,9,13 tacharya,andJ.Bilmes,“Aneffectivebaselineforrobustnessto
[66] Y. Li, A. Goodge, F. Liu, and C.-S. Foo, “Promptad: Zero-shot distributionalshift,”arXivpreprintarXiv:2105.07107,2021. 5
anomalydetectionusingtextprompts,”inWACV,2024. 4 [94] W.-H. Chu and K. M. Kitani, “Neural batch sampling with
[67] L. Deng, “The mnist database of handwritten digit images reinforcementlearningforsemi-supervisedanomalydetection,”
for machine learning research [best of the web],” IEEE Signal inECCV,2020. 5
ProcessingMagazine,vol.29,no.6,pp.141–142,2012. 4 [95] D.J.AthaandM.R.Jahanshahi,“Evaluationofdeeplearning
[68] A. Krizhevsky, G. Hinton, et al., “Learning multiple layers of approachesbasedonconvolutionalneuralnetworksforcorrosion
featuresfromtinyimages,”2009. 4 detection,”StructuralHealthMonitoring,2018. 5
[69] A.Krizhevsky,V.Nair,andG.Hinton,“Cifar-10andcifar-100 [96] A.Miyai,Q.Yu,G.Irie,andK.Aizawa,“Zero-shotin-distribution
datasets,”URl:https://www.cs.toronto.edu/kriz/cifar.html,vol.6, detectioninmulti-objectsettingsusingvision-languagefounda-
no.1,p.1,2009. 4 tionmodels,”arXivpreprintarXiv:2304.04521,2023. 6,14
[97] Y.Li,B.Xiong,G.Chen,andY.Chen,“Setar:Out-of-distribution
[70] A.Torralba,R.Fergus,andW.T.Freeman,“80milliontinyimages:
detectionwithselectivelow-rankapproximation,”arXivpreprint
Alargedatasetfornonparametricobjectandscenerecognition,”
arXiv:2406.12629,2024. 6
IEEETPAMI,vol.30,no.11,pp.1958–1970,2008. 4
[98] M.Lafon,E.Ramzi,C.Rambour,N.Audebert,andN.Thome,
[71] Y.Shu,X.Guo,J.Wu,X.Wang,J.Wang,andM.Long,“Clipood:
“Gallop:Learningglobalandlocalpromptsforvision-language
Generalizingcliptoout-of-distributions,”inICML,2023. 4,13
models,”inECCV,2024. 6,7,14
[72] K. Zhou, Z. Liu, Y. Qiao, T. Xiang, and C. C. Loy, “Domain
[99] X.Chen,Y.Li,andH.Chen,“Dual-adapter:Training-freedual
generalization:Asurvey,”IEEETPAMI,vol.45,no.4,pp.4396–
adaptation for few-shot out-of-distribution detection,” arXiv
4415,2022. 4
preprintarXiv:2405.16146,2024. 6,7,9,14
[73] Q.Yu,D.Ikami,G.Irie,andK.Aizawa,“Multi-taskcurriculum
[100] C. Ding and G. Pang, “Zero-shot out-of-distribution detection
frameworkforopen-setsemi-supervisedlearning,”inECCV,2020.
withoutlierlabelexposure,”inIJCNN,2024. 6
4
[101] D.HendrycksandK.Gimpel,“Abaselinefordetectingmisclas-
[74] K.Saito,D.Kim,andK.Saenko,“Openmatch:Open-setsemi- sifiedandout-of-distributionexamplesinneuralnetworks,”in
supervisedlearningwithopen-setconsistencyregularization,”in ICLR,2017. 6,14,15
NeurIPS,2021. 4
[102] S.Liang,Y.Li,andR.Srikant,“Enhancingthereliabilityofout-of-
[75] K.Cao,M.Brbic,andJ.Leskovec,“Open-worldsemi-supervised distributionimagedetectioninneuralnetworks,”inICLR,2018.
learning,”inICLR,2022. 4 6,14
[76] Y.Wang,W.Liu,X.Ma,J.Bailey,H.Zha,L.Song,andS.-T.Xia, [103] K.Lee,K.Lee,H.Lee,andJ.Shin,“Asimpleunifiedframework
“Iterativelearningwithopen-setnoisylabels,”inCVPR,2018. 4 fordetectingout-of-distributionsamplesandadversarialattacks,”
[77] K. Han, A. Vedaldi, and A. Zisserman, “Learning to discover inNeurIPS,2018. 6,14
novelvisualcategoriesviadeeptransferclustering,”inCVPR, [104] W.Liu,X.Wang,J.D.Owens,andY.Li,“Energy-basedout-of-
2019. 4 distributiondetection,”inNeurIPS,2020. 6,14,15
[78] B.ZhaoandK.Han,“Novelvisualcategorydiscoverywithdual [105] C.S.SastryandS.Oore,“Detectingout-of-distributionexamples
rankingstatisticsandmutualknowledgedistillation,”inNeurIPS, withgrammatrices,”inICML,2020. 6,14
2021. 4 [106] H. Wang, W. Liu, A. Bocchieri, and Y. Li, “Can multi-label
[79] X.Jia,K.Han,Y.Zhu,andB.Green,“Jointrepresentationlearning classificationnetworksknowwhattheydon’tknow?,”inNeurIPS,
andnovelcategorydiscoveryonsingle-andmulti-modaldata,”in 2021. 6
ICCV,2021. 4 [107] J.Zhang,Q.Fu,X.Chen,L.Du,Z.Li,G.Wang,S.Han,D.Zhang,
[80] S. Vaze, K. Han, A. Vedaldi, and A. Zisserman, “Generalized etal.,“Out-of-distributiondetectionbasedonin-distributiondata
categorydiscovery,”inCVPR,2022. 4 patternsmemorizationwithmodernhopfieldenergy,”inICLR,
[81] K.Joseph,S.Paul,G.Aggarwal,S.Biswas,P.Rai,K.Han,andV.N. 2023. 6
Balasubramanian,“Novelclassdiscoverywithoutforgetting,”in [108] Y. Sun and Y. Li, “Dice: Leveraging sparsification for out-of-
ECCV,2022. 4 distributiondetection,”inECCV,2022. 6,14,15
[82] G.VanHorn,O.MacAodha,Y.Song,Y.Cui,C.Sun,A.Shepard, [109] Y.Sun,Y.Ming,X.Zhu,andY.Li,“Out-of-distributiondetection
H. Adam, P. Perona, and S. Belongie, “The inaturalist species withdeepnearestneighbors,”inICML,2022. 6,14,15
classificationanddetectiondataset,”inCVPR,2018. 5 [110] Z.Lin,S.D.Roy,andY.Li,“Mood:Multi-levelout-of-distribution
detection,”inCVPR,2021. 6
[83] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba,
“Places: A 10 million image database for scene recognition,” [111] C.S.SastryandS.Oore,“Detectingout-of-distributionexamples
TPAMI,vol.40,no.6,pp.1452–1464,2017. 5 within-distributionexamplesandgrammatrices,”inNeurIPS-W,
2019. 6,14
[84] J.Xiao,J.Hays,K.A.Ehinger,A.Oliva,andA.Torralba,“Sun
[112] P.Sharma,N.Ding,S.Goodman,andR.Soricut,“Conceptual
database:Large-scalescenerecognitionfromabbeytozoo,”in
captions: A cleaned, hypernymed, image alt-text dataset for
CVPR,2010. 5
automaticimagecaptioning,”inACL,2018. 7
[85] M.Cimpoi,S.Maji,I.Kokkinos,S.Mohamed,andA.Vedaldi,
[113] R.Zhang,W.Zhang,R.Fang,P.Gao,K.Li,J.Dai,Y.Qiao,and
“Describingtexturesinthewild,”inCVPR,2014. 5
H.Li,“Tip-adapter:Training-freeadaptionofclipforfew-shot
[86] M.C.Jung,H.Zhao,J.Dipnall,B.Gabbe,andL.Du,“Enhancing
classification,”inECCV,2022. 7
nearooddetectioninpromptlearning:Maximumgains,minimal
[114] Y. Zhang, W. Zhu, C. He, and L. Zhang, “Lapt: Label-driven
costs,”arXivpreprintarXiv:2405.16091,2024. 5,14
automatedprompttuningforooddetectionwithvision-language
[87] P.Bergmann,M.Fauser,D.Sattlegger,andC.Steger,“Uninformed
models,”inECCV,2024. 7
students:Student-teacheranomalydetectionwithdiscriminative
[115] F. Lu, K. Zhu, K. Zheng, W. Zhai, and Y. Cao, “Likelihood-
latentembeddings,”inCVPR,2020. 5
awaresemanticalignmentforfull-spectrumout-of-distribution
[88] T. Defard, A. Setkov, A. Loesch, and R. Audigier, “Padim: a detection,”arXivpreprintarXiv:2312.01732,2023. 8,13,14
patchdistributionmodelingframeworkforanomalydetection [116] J.Yang,K.Zhou,andZ.Liu,“Full-spectrumout-of-distribution
andlocalization,”inICPR,2021. 5 detection,”IJCV,vol.131,no.10,pp.2607–2622,2023. 8,14
[89] C.-L.Li,K.Sohn,J.Yoon,andT.Pfister,“Cutpaste:Self-supervised [117] D.HendrycksandT.Dietterich,“Benchmarkingneuralnetwork
learningforanomalydetectionandlocalization,”inCVPR,2021. robustnesstocommoncorruptionsandperturbations,”inICLR,
5 2019. 8
[90] P.Liznerski,L.Ruff,R.A.Vandermeulen,B.J.Franks,M.Kloft, [118] D.Hendrycks,S.Basart,N.Mu,S.Kadavath,F.Wang,E.Dorundo,
andK.-R.Mu¨ller,“Explainabledeepone-classclassification,”in R.Desai,T.Zhu,S.Parajuli,M.Guo,etal.,“Themanyfacesofro-
ICLR,2021. 5 bustness:Acriticalanalysisofout-of-distributiongeneralization,”
[91] J. Yi and S. Yoon, “Patch svdd: Patch-level svdd for anomaly inICCV,2021. 8
detectionandsegmentation,”inACCV,2020. 5 [119] J.Zhang,J.Yang,P.Wang,H.Wang,Y.Lin,H.Zhang,Y.Sun,X.Du,
[92] V.Zavrtanik,M.Kristan,andD.Skocˇaj,“Draem-adiscriminatively K.Zhou,W.Zhang,Y.Li,Z.Liu,Y.Chen,andH.Li,“Openood
trainedreconstructionembeddingforsurfaceanomalydetection,” v1.5: Enhanced benchmark for out-of-distribution detection,”
inICCV,2021. 5 arXivpreprintarXiv:2306.09301,2023. 8,1518
[120] B.Recht,R.Roelofs,L.Schmidt,andV.Shankar,“Doimagenet [146] L.Chen,S.Li,J.Yan,H.Wang,K.Gunaratna,V.Yadav,Z.Tang,
classifiersgeneralizetoimagenet?,”inICML,2019. 8 etal.,“Alpagasus:Trainingabetteralpacawithfewerdata,”arXiv
[121] T.Huang,J.Chu,andF.Wei,“Unsupervisedpromptlearningfor preprintarXiv:2307.08701,2023. 11
vision-languagemodels,”arXivpreprintarXiv:2204.03649,2022. 8 [147] W.-L.Chiang,Z.Li,Z.Lin,Y.Sheng,etal.,“Vicuna:Anopen-
[122] M.Shu,W.Nie,D.-A.Huang,Z.Yu,T.Goldstein,A.Anandkumar, sourcechatbotimpressinggpt-4with90%*chatgptquality.”https:
andC.Xiao,“Test-timeprompttuningforzero-shotgeneralization //lmsys.org/blog/2023-03-30-vicuna,2023. Accessed:2024-07-02.
invision-languagemodels,”inNeurIPS,2022. 8 11
[123] K.Tanwisuth,S.Zhang,H.Zheng,P.He,andM.Zhou,“Pouf: [148] Z. Li, P. Xu, F. Liu, and H. Song, “Towards understanding in-
Prompt-orientedunsupervisedfine-tuningforlargepre-trained context learning with contrastive demonstrations and saliency
models,”inICML,2023. 8 maps,”arXivpreprintarXiv:2307.05052,2023. 11
[124] Z. Zhou, M. Yang, J.-X. Shi, L.-Z. Guo, and Y.-F. Li, “Decoop: [149] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
Robust prompt tuning with out-of-distribution detection,” in T.Lacroix,B.Rozie`re,N.Goyal,E.Hambro,F.Azhar,etal.,“Llama:
ICML,2024. 8 Openandefficientfoundationlanguagemodels,”arXivpreprint
[125] P. Isola, J. J. Lim, and E. H. Adelson, “Discovering states and arXiv:2302.13971,2023. 11
transformationsinimagecollections,”inCVPR,2015. 8 [150] J.Wei,J.Wei,Y.Tay,D.Tran,A.Webson,Y.Lu,X.Chen,H.Liu,
[126] N.Belton,M.T.Hagos,A.Lawlor,andK.M.Curran,“Fewsome: D.Huang,D.Zhou,etal.,“Largerlanguagemodelsdoin-context
One-classfewshotanomalydetectionwithsiamesenetworks,”in learningdifferently,”arXivpreprintarXiv:2303.03846,2023. 11
CVPR,2023. 9 [151] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,
[127] C.Huang,H.Guan,A.Jiang,Y.Zhang,M.Spratling,andY.-F. B.Zhang,J.Zhang,Z.Dong,etal.,“Asurveyoflargelanguage
Wang,“Registrationbasedfew-shotanomalydetection,”inECCV, models,”arXivpreprintarXiv:2303.18223,2023. 11
2022. 9 [152] A. Awadalla, I. Gao, J. Gardner, J. Hessel, Y. Hanafy, W. Zhu,
[128] J.Liao,X.Xu,M.C.Nguyen,A.Goodge,andC.S.Foo,“Coft-ad: K.Marathe,Y.Bitton,S.Gadre,S.Sagawa,etal.,“Openflamingo:
Contrastivefine-tuningforfew-shotanomalydetection,”IEEE An open-source framework for training large autoregressive
TIP,vol.33,pp.2090–2103,2024. 9 vision-languagemodels,”arXivpreprintarXiv:2308.01390,2023. 11
[129] E.Schwartz,A.Arbelle,L.Karlinsky,S.Harary,F.Scheidegger, [153] S.Bubeck,V.Chandrasekaran,R.Eldan,J.Gehrke,E.Horvitz,
S.Doveh,andR.Giryes,“Maeday:Maeforfew-andzero-shot E.Kamar,P.Lee,Y.T.Lee,Y.Li,S.Lundberg,etal.,“Sparksof
anomaly-detection,” Computer Vision and Image Understanding, artificialgeneralintelligence:Earlyexperimentswithgpt-4,”arXiv
p.103958,2024. 9 preprintarXiv:2303.12712,2023. 11
[130] S.Sheynin,S.Benaim,andL.Wolf,“Ahierarchicaltransformation- [154] W.Dai,J.Li,D.Li,A.Tiong,J.Zhao,W.Wang,B.Li,P.Fung,and
discriminatinggenerativemodelforfewshotanomalydetection,” S.Hoi,“InstructBLIP:Towardsgeneral-purposevision-language
inICCV,2021. 9 modelswithinstructiontuning,”inNeurIPS,2023. 11
[131] Z.Wang,Y.Zhou,R.Wang,T.-Y.Lin,A.Shah,andS.N.Lim,
[155] W.Wang,Q.Lv,W.Yu,W.Hong,J.Qi,Y.Wang,J.Ji,Z.Yang,
“Few-shotfast-adaptiveanomalydetection,”inNeurIPS,2022. 9
L.Zhao,X.Song,etal.,“Cogvlm:Visualexpertforpretrained
[132] J.-C.Wu,D.-J.Chen,C.-S.Fuh,andT.-L.Liu,“Learningunsuper- languagemodels,”arXivpreprintarXiv:2311.03079,2023. 11
visedmetaformerforanomalydetection,”inICCV,2021. 9
[156] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu,
[133] G.Xie,J.Wang,J.Liu,F.Zheng,andY.Jin,“Pushingthelimits
P. Shi, Y. Shi, et al., “mplug-owl: Modularization empowers
offewshotanomalydetectioninindustryvision:Graphcore,”in
large language models with multimodality,” arXiv preprint
ICLR,2023. 9
arXiv:2304.14178,2023. 11
[134] K.Roth,L.Pemula,J.Zepeda,B.Scho¨lkopf,T.Brox,andP.Gehler,
[157] D.Zhu,J.Chen,X.Shen,X.Li,andM.Elhoseiny,“Minigpt-4:
“Towardstotalrecallinindustrialanomalydetection,”inCVPR,
Enhancingvision-languageunderstandingwithadvancedlarge
2022. 9
languagemodels,”inICLR,2024. 11
[135] Y. Cao, X. Xu, C. Sun, Y. Cheng, Z. Du, L. Gao, and W. Shen,
[158] B. Li, Y. Zhang, L. Chen, J. Wang, J. Yang, and Z. Liu, “Otter:
“Segment any anomaly without training via hybrid prompt
Amulti-modalmodelwithin-contextinstructiontuning,”arXiv
regularization,”arXivpreprintarXiv:2305.10724,2023. 10
preprintarXiv:2305.03726,2023. 11
[136] C.Li,L.Qi,andX.Geng,“Asam-guidedtwo-streamlightweight
[159] J. Lin, H. Yin, W. Ping, Y. Lu, P. Molchanov, A. Tao, H. Mao,
model for anomaly detection,” arXiv preprint arXiv:2402.19145,
J.Kautz,M.Shoeybi,andS.Han,“Vila:Onpre-trainingforvisual
2024. 10
languagemodels,”inCVPR,2024. 11
[137] L. Hua, Y. Luo, Q. Qi, and J. Long, “Medicalclip: Anomaly-
[160] S.Antol,A.Agrawal,J.Lu,M.Mitchell,D.Batra,C.L.Zitnick,
detectiondomaingeneralizationwithasymmetricconstraints,”
andD.Parikh,“Vqa:Visualquestionanswering,”inICCV,2015.
Biomolecules,vol.14,no.5,p.590,2024. 10
11
[138] X.Zhang,M.Xu,D.Qiu,R.Yan,N.Lang,andX.Zhou,“Mediclip:
[161] F.Liu,H.Tan,andC.Tensmeyer,“Documentclip:Linkingfigures
Adaptingclipforfew-shotmedicalimageanomalydetection,”
arXivpreprintarXiv:2405.11315,2024. 10 andmainbodytextinrefloweddocuments,”inICPRAI,2024. 11
[139] X.Du,Z.Wang,M.Cai,andY.Li,“Vos:Learningwhatyoudon’ [162] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan,
tknowbyvirtualoutliersynthesis,”inICLR,2022. 10 J.Wang,C.He,Z. Liu,etal.,“Mmbench:Isyour multi-modal
modelanall-aroundplayer?,”inECCV,2024. 11
[140] X.Du,X.Wang,G.Gozum,andY.Li,“Unknown-awareobject
detection:Learningwhatyoudon’tknowfromvideosinthe [163] X.Yue,Y.Ni,K.Zhang,T.Zheng,R.Liu,G.Zhang,S.Stevens,
wild,”inCVPR,2022. 10 D.Jiang,W.Ren,Y.Sun,etal.,“Mmmu:Amassivemulti-discipline
multimodalunderstandingandreasoningbenchmarkforexpert
[141] X.Du,G.Gozum,Y.Ming,andY.Li,“Siren:Shapingrepresenta-
agi,”inCVPR,2024. 11,15
tionsfordetectingout-of-distributionobjects,”inNeurIPS,2022.
10 [164] F. Wang, X. Fu, J. Y. Huang, Z. Li, Q. Liu, X. Liu, M. D. Ma,
[142] Y. Cao, X. Xu, C. Sun, X. Huang, and W. Shen, “Towards N.Xu,W.Zhou,K.Zhang,etal.,“Muirbench:Acomprehensive
generic anomaly detection and understanding: Large-scale benchmarkforrobustmulti-imageunderstanding,”arXivpreprint
visual-linguistic model (gpt-4v) takes the lead,” arXiv preprint arXiv:2406.09411,2024. 11,15
arXiv:2311.02782,2023. 11,12 [165] J.Liu,Y.Yuan,J.Hao,F.Ni,L.Fu,Y.Chen,andY.Zheng,“En-
[143] A. Miyai, J. Yang, J. Zhang, Y. Ming, Q. Yu, G. Irie, Y. Li, hancingroboticmanipulationwithaifeedbackfrommultimodal
H. Li, Z. Liu, and K. Aizawa, “Unsolvable problem detection: largelanguagemodels,”arXivpreprintarXiv:2402.14245,2024. 11
Evaluating trustworthiness of vision language models,” arXiv [166] Y.Li,W.Zhang,K.Chen,Y.Liu,P.Li,R.Gao,L.Hong,M.Tian,
preprintarXiv:2403.20331,2024. 11,12 X. Zhao, Z. Li, et al., “Automated evaluation of large vision-
[144] Z.Gu,B.Zhu,G.Zhu,Y.Chen,M.Tang,andJ.Wang,“Anoma- language models on self-driving corner cases,” arXiv preprint
lygpt:Detectingindustrialanomaliesusinglargevision-language arXiv:2404.10595,2024. 11
models,”inAAAI,2024. 11,12 [167] OpenAI,“Gpt-4v(ision)systemcard.”https://cdn.openai.com/
[145] Y.Li,H.Wang,S.Yuan,M.Liu,D.Zhao,Y.Guo,C.Xu,G.Shi, papers/GPTV System Card.pdf,2023. 11
and W. Zuo, “Myriad: Large multimodal model by applying [168] P.Bergmann,X.Jin,D.Sattlegger,andC.Steger,“Themvtec3d-ad
visionexpertsforindustrialanomalydetection,”arXivpreprint datasetforunsupervised3danomalydetectionandlocalization,”
arXiv:2310.19070,2023. 11 inVISAPP,2022. 1219
[169] D.S.Kermany,M.Goldbaum,W.Cai,etal.,“Identifyingmedical [197] H.Wang,Z.Li,L.Feng,andW.Zhang,“Vim:Out-of-distribution
diagnosesandtreatablediseasesbyimage-baseddeeplearning,” withvirtual-logitmatching,”inCVPR,2022. 15
Cell,vol.172,no.5,pp.1122–1131,2018. 12 [198] A. Miyai, Q. Yu, G. Irie, and K. Aizawa, “Can pre-trained
[170] K. Felipe, “Head ct - hemorrhage.” https://www.kaggle.com/ networksdetectfamiliarout-of-distributiondata?,”arXivpreprint
datasets/felipekitamura/headct-hemorrhage,2018. 12 arXiv:2310.00847,2023. 15
[171] P.Bergmann,K.Batzner,M.Fauser,D.Sattlegger,andC.Steger, [199] X.Chen,H.Fan,R.Girshick,andK.He,“Improvedbaselineswith
“Beyonddentsandscratches:Logicalconstraintsinunsupervised momentumcontrastivelearning,”arXivpreprintarXiv:2003.04297,
anomalydetectionandlocalization,”IJCV,vol.130,no.4,pp.947– 2020. 15
969,2022. 12 [200] M.Caron,H.Touvron,I.Misra,H.Je´gou,J.Mairal,P.Bojanowski,
[172] W.Sultani,C.Chen,andM.Shah,“Real-worldanomalydetection and A. Joulin, “Emerging properties in self-supervised vision
insurveillancevideos,”inCVPR,2018. 12 transformers,”inICCV,2021. 15
[173] C. Kay, “Accident detection from cctv footage.” [201] J.Dong,Y.Gao,H.Zhou,J.Cen,Y.Yao,S.Yoon,andP.D.Sun,
https://www.kaggle.com/datasets/ckay16/ “Towardsfew-shotout-of-distributiondetection,”arXivpreprint
accident-detection-fromcctv-footage, 2018. Kaggle dataset. arXiv:2311.12076,2023. 15
12 [202] A.Dosovitskiy,L.Beyer,A.Kolesnikov,D.Weissenborn,X.Zhai,
[174] S. E. User, “Simple outlier detection for time series.” T.Unterthiner,M.Dehghani,M.Minderer,G.Heigold,S.Gelly,
https://stats.stackexchange.com/questions/427327/ etal.,“Animageisworth16x16words:Transformersforimage
simple-outlier-detection-for-time-series, 2021. Accessed: recognitionatscale,”inICLR,2021. 15
2024-07-02. 12 [203] Y.Zhang,K.Zhou,andZ.Liu,“Neuralpromptsearch,”arXiv
[175] M.Xu,Z.Zhang,F.Wei,H.Hu,andX.Bai,“Sideadapternetwork preprintarXiv:2206.04673,2022. 15
foropen-vocabularysemanticsegmentation,”inCVPR,2023. 13 [204] E.Baek,K.Park,J.Kim,andH.-S.Kim,“Unexploredfacesofro-
[176] X.Gu,T.-Y.Lin,W.Kuo,andY.Cui,“Open-vocabularyobject bustnessandout-of-distribution:Covariateshiftsinenvironment
detection via vision and language knowledge distillation,” in andsensordomains,”inCVPR,2024. 15
ICLR,2022. 13 [205] P. W. Koh, S. Sagawa, H. Marklund, S. M. Xie, M. Zhang,
[177] Y.Shu,Z.Cao,C.Wang,J.Wang,andM.Long,“Opendomain A. Balsubramani, W. Hu, M. Yasunaga, R. L. Phillips, I. Gao,
generalizationwithdomain-augmentedmeta-learning,”inCVPR, etal.,“Wilds:Abenchmarkofin-the-wilddistributionshifts,”in
2021. 13 ICML,2021. 15
[178] K.Katsumata,I.Kishida,A.Amma,andH.Nakayama,“Open-set [206] L.Cultrera,L.Seidenari,andA.DelBimbo,“Leveragingvisual
domaingeneralizationviametriclearning,”inICIP,2021. 13 attentionforout-of-distributiondetection,”inICCV,2023. 15
[207] Z.Hong,Y.Yue,Y.Chen,H.Lin,Y.Luo,etal.,“Out-of-distribution
[179] R. Zhu and S. Li, “Crossmatch: Cross-classifier consistency
regularizationforopen-setsingledomaingeneralization,”inICLR, detection in medical image analysis: A survey,” arXiv preprint
arXiv:2404.18279,2024. 15
2021. 13
[208] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng,
[180] M.NoguchiandS.Shirakawa,“Simpledomaingeneralization
K.-W. Chang, M. Galley, and J. Gao, “Mathvista: Evaluating
methodsarestrongbaselinesforopendomaingeneralization,”
mathematicalreasoningoffoundationmodelsinvisualcontexts,”
arXivpreprintarXiv:2303.18031,2023. 13
inICLR,2024. 15
[181] Z. Chen, W. Wang, Z. Zhao, F. Su, A. Men, and H. Meng,
“Practicaldg:Perturbationdistillationonvision-languagemodels
forhybriddomaingeneralization,”inCVPR,2024. 13
[182] M.Singha,A.Jha,S.Bose,A.Nair,M.Abdar,andB.Banerjee,
“Unknownprompttheonlylacuna:Unveilingclip’spotentialfor
opendomaingeneralization,”inCVPR,2024. 13
[183] Y.Sun,C.Guo,andY.Li,“React:Out-of-distributiondetection
withrectifiedactivations,”inNeurIPS,2021. 14 Atsuyuki Miyai (Student Member, IEEE) re-
[184] X.Dong,J.Guo,A.Li,W.-T.Ting,C.Liu,andH.Kung,“Neural ceivedhisB.E.degreeinInformationandCom-
meandiscrepancyforefficientout-of-distributiondetection,”in munication Engineering in 2022 and his M.E.
CVPR,2022. 14 degreeinInterdisciplinaryStudiesinInformation
[185] Y.Ming,H.Yin,andY.Li,“Ontheimpactofspuriouscorrelation Sciencein2024fromTheUniversityofTokyo.He
forout-of-distributiondetection,”inAAAI,2022. 14 iscurrentlyaPh.D.studentwiththeDepartment
[186] G. Chen, W. Yao, X. Song, X. Li, Y. Rao, and K. Zhang, “Plot: ofInformationandCommunicationEngineering
Prompt learning with optimal transport for vision-language at The University of Tokyo. His research topic
models,”inICLR,2023. 14 is the safety of vision language models and
[187] A.BulatandG.Tzimiropoulos,“Lasp:Text-to-textoptimization foundationmodels.HeisanIEEEStudentBranch
forlanguage-awaresoftpromptingofvision&languagemodels,” ChairatTheUniversityofTokyo.
inCVPR,2023. 14
[188] Y.Lu,J.Liu,Y.Zhang,Y.Liu,andX.Tian,“Promptdistribution
learning,”inCVPR,2022. 14 Jingkang Yang is currently a PhD student at
[189] N.Houlsby,A.Giurgiu,S.Jastrzebski,B.Morrone,Q.DeLarous- the College of Computing and Data Science
silhe, A. Gesmundo, M. Attariyan, and S. Gelly, “Parameter- (CCDS) at Nanyang Technological University
efficienttransferlearningfornlp,”inICML,2019. 14 (NTU),Singapore,workingintheMMLab@NTU
[190] M.Jia,L.Tang,B.-C.Chen,C.Cardie,S.Belongie,B.Hariharan, under the supervision of Dr. Ziwei Liu. His re-
andS.-N.Lim,“Visualprompttuning,”inECCV,2022. 14 searchinterestsincludevisualgeneralistmodels
[191] E.J.Hu,Y.Shen,P.Wallis,Z.Allen-Zhu,Y.Li,S.Wang,L.Wang, andAIsafetyforfoundationmodels.Hehaspub-
and W. Chen, “Lora: Low-rank adaptation of large language lishedover20papersinrelevantfieldsattop-tier
models,”inICLR,2022. 14,15 conferencesandjournals,includingCVPR,ICCV,
[192] V.Udandarao,A.Gupta,andS.Albanie,“Sus-x:Training-free ECCV,NeurIPS,ICLR,andIJCV.Heservesasa
name-onlytransferofvision-languagemodels,”inICCV,2023. 14 reviewerofCVPR,ICCV,ECCV,NeurIPS,ICLR,
[193] Y. Ming and Y. Li, “Understanding retrieval-augmented task etc.,andisanoutstandingreviewerinICCV2021andCVPR2024.
adaptationforvision-languagemodels,”inICML,2024. 14
[194] H.Bai,G.Canal,X.Du,J.Kwon,R.D.Nowak,andY.Li,“Feed
twobirdswithonescone:Exploitingwilddataforbothout-of- Jingyang Zhang received his B.E. degree in
distributiongeneralizationanddetection,”inICML,2023. 14 ElectronicEngineeringfromTsinghuaUniversity,
[195] M. U. Khattak, S. T. Wasim, M. Naseer, S. Khan, M.-H. Yang, Beijing,China,in2019.Sincethen,hehasbeen
and F. S. Khan, “Self-regulating prompts: Foundational model pursuingthePh.D.degreeatDept.ofElectrical
adaptationwithoutforgetting,”inICCV,2023. 14 and Computer Engineering at Duke University,
[196] C.Leys,O.Klein,Y.Dominicy,andC.Ley,“Detectingmultivariate supervisedbyDr.YiranChenandDr.Hai(Helen)
outliers:Usearobustvariantofthemahalanobisdistance,”Journal Li.Hisresearchspansfromrobustnessofdeep
ofExperimentalSocialPsychology,vol.74,pp.150–156,2018. 15 learning-basedvisionsystemsto(morerecently)
generativeAIandmulti-modalLLMs.20
YifeiMingreceivedhisPh.D.inComputerSci- Hai(Helen)Li(Fellow,IEEE)receivedthePh.D.
encefromtheUniversityofWisconsin-Madison degreefromPurdueUniversityin2004.Dr.Liis
in2024,advisedbyDr.YixuanLi.Heiscurrently currentlytheClareBootheLuceProfessorand
aResearchScientistatSalesforceAIResearch. DepartmentChairoftheElectricalandComputer
Hisresearchinterestsencompassreliablema- EngineeringDepartmentatDukeUniversity.Her
chine learning, vision-language models, large currentresearchinterestsincludeneuromorphic
language models, and responsible foundation circuitsandsystemsforbrain-inspiredcomputing,
models.Hehasaprolificpublicationrecordand machine learning acceleration and trustworthy
serves on the program committees of leading AI,conventionalandemergingmemorydesign
machinelearningconferencesandjournals,in- andarchitecture,andsoftwareandhardwareco-
cludingNeurIPS,ICLR,ICML,EMNLP,andIJCV. design.Dr.LiisarecipientoftheNSFCareer
Award (2012), DARPA Young Faculty Award (2013), TUM-IAS Hans
Yueqian Lin received a B.S. degree in Data FischerFellowshipfromGermany(2017),andELATEFellowship(2020).
SciencefromDukeKunshanUniversityin2024, Shereceived9bestpaperawardsandadditional9bestpapernomina-
graduatingsummacumlaudewithsignaturework tionsfrominternationalconferences.Dr.LiisaDistinguishedLecturerof
distinction. He is currently pursuing a Ph.D. in theIEEECASSociety(2018-2019)andaDistinguishedSpeakerofACM
Electrical and Computer Engineering at Duke (2017-2020).
University under the supervision of Dr. Yiran
ChenandDr.Hai(Helen)Li.Hisresearchfocuses
onrobustnessingenerativemodels.
ZiweiLiu(Member,IEEE)iscurrentlyananyang
assistant professor with Nanyang Technologi-
calUniversity,Singapore.Hisresearchinterests
QingYu(Member,IEEE)receivedhisB.E.de- include computer vision machine learning and
gree in Information and Communication Engi- computergraphics.Hehaspublishedextensively
neeringandtheM.E.degreeinInterdisciplinary ontop-tierconferencesandjournalsinrelevant
Studies in Information Science from The Uni- fields, including CVPR, ICCV, ECCV, NeurlPS,
versityofTokyoin2018and2020,respectively. ICLR,ICML,IEEETransactionsonPatternAnal-
HecompletedhisPh.D.inInformationScience ysisandMachineIntelligence,ACMTransactions
and Technology from The University of Tokyo onGraphicsandNature-MachineIntelligence.
in2023.HeiscurrentlyaResearchScientistat HeistherecipientofICCVYoungResearcher
LY Corporation, Japan. His research interests Award,HKSTPBestPaperAward,CVPRBestPaperAwardCandidate,
includemultimodalrecognitionandgeneration. ICBSFrontiersofScienceAwardandMITTechnologyReviewInnovators
under35AsiaPacific.HeservesasanareachairofCVPR,ICCV,ECCV,
NeurlPSandICLR,aswellasanassociateeditorofInternationalJournal
GoIrie(Member,IEEE)receivedhisB.E.and ofComputerVision.
M.E. in System Engineering from Keio Univer-
sity,Japan,in2004and2006,respectively.He
completedhisPh.D.inInformationScienceand
TechnologyfromTheUniversityofTokyoin2011. ToshihikoYamasaki(Member,IEEE)received
He is currently an Associate Professor at the thePh.D.degreefromTheUniversityofTokyo.
DepartmentofInformationandComputerTech- He is currently a Professor at the Department
nology,TokyoUniversityofScience,Japan.He ofInformationandCommunicationEngineering,
was a research scientist at NTT Corporation, Graduate School of Information Science and
Japan, from 2006 to 2022, and a Visiting Re- Technology,TheUniversityofTokyo.Hewasa
searchScholaratColumbiaUniversityfrom2012 JSPSFellowforResearchAbroadandavisiting
to 2013. His research interests include pattern recognition, machine scientistatCornellUniversityfrom2011to2013.
learningandmediaunderstanding. Hiscurrentresearchinterestsarecomputervi-
sion, multimedia, pattern recognition, machine
ShafiqJotyiscurrentlyaResearchDirectorat learning.Heisapioneerofattractivenesscom-
Salesforce Research (Palo Alto, USA), where puting.
he oversees the NLP group’s efforts in large
languagemodeling(LLM)andgenerativeAI.He
also holds the position of a tenured Associate
Professor(currentlyonleave)intheSchoolof Kiyoharu Aizawa (Fellow, IEEE) received the
ComputerScienceandEngineering(SCSE)at B.E., the M.E., and the Dr.Eng. degrees in
NTU, Singapore. He was a founding manager ElectricalEngineeringallfromtheUniversityof
of the Salesforce Research Asia (Singapore) Tokyo,in1983,1985,1988,respectively.Heis
lab. His research has contributed to over 30+ aprofessorwiththeDepartmentofInformation
patents and 145+ papers in top-tier NLP and and Communication Engineering, and director
ML conferences and journals. He has served as the Program Chair of VR center, University of Tokyo. He was a
ofSIGDIAL-2023,asamemberofthebestpaperawardcommitteesfor visitingassistantprofessorwiththeUniversityof
ICLR-23andNAACL-22,andinthecapacityofa(senior)areachairfor Illinois from 1990 to 1992. His research fields
manyoftheleadingNLPandMLconferences. aremultimedia,imageprocessing,andcomputer
vision,withaparticularinterestininterdisciplinary
Yixuan Li (Member, IEEE) received the PhD andcross-disciplinaryissues.Hereceivedthe1990,1998BestPaper
degreefromCornellUniversity,in2017,advised Awards,the1991AchievementAward,1999ElectronicsSocietyAward
byJohnE.Hopcroft.Sheisanassistantprofessor from IEICE Japan, and the 1998 Fujio Frontier Award, the 2002 and
in the Department of Computer Sciences, the 2009BestPaperAward,and2013AchievementawardfromITEJapan.
UniversityofWisconsin-Madison.Subsequently, HereceivedtheIBMJapanSciencePrizein2002.HeisonEditorial
shewasapostdoctoralscholarinthecomputer BoardofACMTOMM.HeservedastheEditor-in-ChiefofJournalofITE
sciencedepartment,StanfordUniversity.Herre- Japan,anAssociateEditorofIEEETIP,TCSVT,TMM,andMultiMedia.
searchfocusesonthealgorithmicandtheoretical Hehasservedanumberofinternationalanddomesticconferences;he
foundationsoflearninginopenworlds.Shehas wasaGeneralco-ChairofACMMultimedia2012andICMR2018.Heis
servedasAreaChairforICLR,NeurIPS,ICML, aFellowofIEEE,IEICE,ITEandamemberofScienceCouncilofJapan.
andProgramChairforWorkshoponUncertainty
andRobustnessinDeepLearning.SheistherecipientoftheAFOSR
Young Investigator Program (YIP) award, NSF CAREER award, MIT
TechnologyReviewInnovatorUnder35,Forbes30Under30inScience,
andmultiplefacultyresearchawardsfromGoogle,Meta,andAmazon.
HerworksreceivedaNeurIPSOutstandingPaperAward,andanICLR
OutstandingPaperAwardHonorableMention,in2022.