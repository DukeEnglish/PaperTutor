Paying More Attention to Image: A Training-Free
Method for Alleviating Hallucination in LVLMs
Shi Liu1, Kecheng Zheng1,2†, and Wei Chen1†
1 State Key Lab of CAD&CG, Zhejiang University
2 Ant Group
{liushi0927,zkechengzk}@gmail.com
chenvis@zju.edu.cn
Abstract. Existing Large Vision-Language Models (LVLMs) primar-
ily align image features of vision encoder with Large Language Models
(LLMs)toleveragetheirsuperiortextgenerationcapabilities.However,
the scale disparity between vision encoder and language model may led
to LLMs assuming a predominant role in multi-modal comprehension.
This imbalance in LVLMs may result in the instances of hallucinatory.
Concretely,LVLMsmaygenerateconsistentdescriptionswithorwithout
visualinput,indicatingthatcertainoutputsareinfluencedsolelybycon-
text text. We refer to this phenomenon as “text inertia.” To counteract
this issue, we introduce a training-free algorithm to find an equilibrium
pointbetweenimagecomprehensionandlanguageinference.Specifically,
weadaptivelyinvolveadjustingandamplifyingtheattentionweightsas-
signed to image tokens, thereby granting greater prominence to visual
elements. Meanwhile, we subtract the logits of multi-modal inputs from
ones of pure text input, which can help LVLMs be not biased towards
LLMs.Byenhancingimagestokensandreducingthestubbornoutputof
LLM, we can let LVLM pay more attention to images, towards alleviat-
ingtextinertiaandreducingthehallucinationinLVLMs.Ourextensive
experimentsshowsthatthismethodsubstantiallyreducesthefrequency
ofhallucinatoryoutputsinvariousLVLMsintermsofdifferentmetrics.
Project page is available at https://lalbj.github.io/projects/PAI/
Keywords: Vision-Language Models · Visual Dialogs · Hallucination
Mitigation
1 Introduction
Recently,LargeVision-LanguageModels(LVLMs)havemadesignificantstrides,
exhibitingimpressivecapabilitiesacrossamultitudeoftasks[13,17,38,44].How-
ever, these models still struggle with the phenomenon of hallucination. Specif-
ically, there is often a mismatch between the textual content generated by the
model and the actual visual input it receives [25].
Hallucination in LVLMs is often attributed to issues with modality align-
ment, leading to the development of mitigating strategies through alignment
† Corresponding authors.
4202
luJ
13
]VC.sc[
1v17712.7042:viXra2 Liu et al.
<Placeholder>
LLaVAP -1le .5ase de T w f p as o nehc a r r dr e el si k s ob ai i tm pne n , p g sat i eh u s g t ae rhe wr r r oi sf o em e u a tua ona rgt i dg u bnh ee r e gd e a i es an s b n a n y rd je o o e p t dw yt re a e ijr - ni aes cl gcso. o . k n v tT e h e t ehr e eH d uman P i t s w e cm h u n al ere r rja o ar o ra g o yu rs yie u ige n i n nn hf gd gge d e aa a e ats t d h sc ru n e er r bi o d eb wyw s je iata - nrt cc eh tp ko eee e e rv s r ti .e ss m aTr co e n ea n hd d ng e w ee f apo r a pi yern l pe .rk s es Td i ot ane h, n r gt esa i yi tsl o . a T rb eeh e Human P i t s w e cm h u n al ere r rja o ar o ra g o yu rs yie u ige n i n nn hf gd gge d e aa a e ats t d h sc ru n e er r bi o d eb wy ws je iata - nrt cc eh tp ko eee e e rv s r ti .e ss m aTr co e n ea n hd d ng e w ee f apo r a pi yern l pe .rk s es Td i ot ane h, n r gt esa i yi tsl o . a T rb eeh e Human L /wL a PP V Al Ae Ia -s 1e . 5descri I s a o a ub n k nr ne t i dr it wst qah h u ai uoee li e mt r i tii e m hn am a drg nna o a c d g ugg i a se ge ie p n, a h w ,i ta r n em a e i r p asd ef ae roe skwr i tt r ns i ia ea no ngi sl gnl g k t. b . .i tw ln hT Tage ech h a ke ed sr c H os pm e w au na nm n en t s a n
winter scenery. They are carrying skiing or skiing device, possibly a snowy pathway is covered in
a backpack, possibly filled with backpack, possibly filled with pair of skis, as they are following their tracks, indicating they are
supplies for Hthe ai lr
l
j uo cu ir nne ay t.
ions
LLaVA-1.5 supplies
S
fo ar
m
th ee i Hr j aou llr une cy in.
ations
L /wL a PV AA I-1.5 the same path down the path. using skisuitc.
(a) Image description by LLaVA (b) Same hallucinations without (c) Correct description with same (d) Image description by PAI
image history
Fig.1:Wepresentanexaminationofvariousinputsettings,withhallucinationsspecif-
ically highlighted in red. (a) When using LLAVA for image description, it generates
a hallucinated description. (b) Even without image input, when only the historical
response preceding the hallucinated description is input to LLAVA, it reproduces the
same hallucinated description, a phenomenon we refer to as “Text Inertia”. (c) Our
proposedmethod,PAI,effectivelymitigatesthistextinertiaproblemandyieldsaccu-
ratedescriptions.(d)UtilizingPAIforimagedescriptionresultsinasignificantlymore
precise description.
Fig.3: Visualizationoftheaverageatten-
tion ratio of different content. The x-axis
denotes thesequence length ofthe history
tokens. The lengths of the image, instruc-
Fig.2: Percentage of text inertia halluci- tion, BOS, and outlier tokens [11] are all
nationinallhallucination(calculatedwith fixed as they are part of the model input,
500 samples). For specific calculation pro- with 576 tokens for the image, 21 for the
cesses, please refer to the supplementary instruction, and 1 each for the BOS and
material section A. outlier tokens.
training optimization [9,22,33]. However, is hallucination in LVLMs merely a
result of the model’s capacity, and can it only be alleviated through additional
training? We propose a scenario where LVLMs generate a hallucinated object
description. Specifically, even when the image input is removed and only the
generated text preceding the hallucinated object word is retained, the LVLMs
persist in producing the same hallucinated description, as depicted in Fig. 1.
Inordertoempiricallyinvestigatethisbehavior,weconductedtestsonthree
LVLMs within the context of image describing tasks on the COCO dataset.
We identified and conducted a statistical analysis on instances where LVLMsPAI 3
generated identical hallucinated object descriptions, even when the input was
exclusively historical response text without any image. The observation from
Fig. 2 clearly indicates that, even with the application of rigorous identification
settings, the phenomenon continues to represent a substantial proportion.
Wereferredthisphenomenonas“Text Inertia”.Ourhypothesisisthattext
inertia arises due to the current generative paradigms mapping image represen-
tations onto the text representation space as text tokens. In this mechanism,
the LLM becomes the dominant character, and the inference process lacks addi-
tional handling of image tokens, leading to their neglect during the generation
process.Tovalidatethishypothesis,wehaveanalyzedtheattentionvaluesratios
of the LLaVA model during the inference process in Fig. 3. Our findings show
thatdespiteimagetokensoccupyingasignificantproportion,theydonotreceive
substantialattentionunderthecurrentmechanism.Thismultimodalchatresem-
blesmoreofanautomaticcompletionbasedoncontextratherthanacontinuous
attention to the image for completion.
To close this gap, we introduce a method refered to as Pay Attention
to Image (PAI). At a high level, PAI intervenes in the inference process to
make it more image-centric, following the original image perception direction.
To achieve this, we focus on the self-attention heads in the decoder layers of
LVLMs. We enhance the attention weights for image tokens in their original
directions during inference. This allows us to use the updated attention matrix
to calculate the hidden states for the generated token, thereby incorporating
more consideration for image representation during the generation process. To
furthermitigatetextinertia,weconstructtheinputusinginstructiontokensand
historical response tokens, and subtract the model logits of this input from the
logits of original model with image tokens input. This strategy helps to reduce
the influence of language priors during the generation process. Unlike previous
methodsformitigatinghallucinationthatrequireadditionaltrainingorexternal
tools, our approach is training-free. Moreover, we are the first to propose an
inference intervention method for mitigating hallucination in LVLMs.
We then evaluate the response accuracy in the image description task from
long sequence generation perspective by employing the CHAIR metric [31] and
GPT-4V. In addition, we use POPE [21] and MMHal-Bench [34] to more com-
prehensively evaluate the model’s hallucination performance on VQA task. Fur-
thermore, we have constructed single-turn and multi-turn chat evaluations for
POPE. Since our model intervenes in the inference process, it can be used for
any decoding method. Therefore, we conducted experiments on three decoding
methods of the three models. The experimental results proved the effectiveness
of our method in mitigating hallucinations.
2 Related Work
2.1 Large Vision-Language Models
The development of pre-training techniques [5,29] and instruction tuning tech-
niques [28,39] has rapidly advanced LLMs technology, such as LLaMA [35] and4 Liu et al.
Input Construct Input Self Attention Image-Centric
LogitRefine
With Image Image
Neglect E Imxc ai gt ee 𝜸 (𝜸𝒍𝒐 -𝒈 1)𝒊 𝒕 𝒍𝒔 𝒐𝒈𝒚 𝒊𝑿 𝒕𝒔𝒗 (, 𝒚𝑿 |𝑰 𝑿,𝑿 𝑰,𝑯 𝑿𝑯− )
Image 𝑋! Without Image
Instruction𝑋"
LLaMA
history𝑋# Decoder
Fig.4: ArchitectureofourPAI.Toalleviatetextinertia,weadditionallyconstructan
input without image. Throughout the forward inference process, we amplify the focus
on the image token by edit self-attention maps in LLaMA. Ultimately, we subtract
the logits distribution of the language prior during decoding to achieve an accurate
description.
Vicuna[30],furtherleadingtotheprosperityofLVLMstechnology.Earlyworks,
like Flamingo [2] and BLIP-2 [19], have successfully adapted LLMs to visual
tasks,demonstratingnotablegenerativecapabilitiesandin-contextlearningabil-
ities.Recently,thecapabilitiesofLVLMshavefurtheradvancedundertheinflu-
enceofvisualinstructiontuningtechniques[26,27].Usingdifferentprojectorsto
map images to the text domain, thereby endowing language generation models
withimageunderstandingcapabilities,isalsoahotresearchtopic[10,27,41,46].
Additionally, several studies focus on visual language tasks such as grounding
capabilities [8] and reasoning capabilities [17]. However, recent LVLMs still face
the issue of hallucination generation [23].
2.2 Mitigation of LVLMs Hallucination
HallucinationinLVLMsreferstocontradictionsbetweentheimageinputandthe
textual output. Various methods have been proposed to mitigate hallucination.
The most direct reason for hallucination generation is that hallucination arises
from data bias and knowledge gaps between vision and language. Therefore,
better data filtering methods [14,24,43] and higher quality annotated data [3]
are introduced. Simultaneously, these methods also imply the need for more
alignment training [33] or adjustments in the model architecture [9,22]. These
methodscanachievegoodresults,buttheyaretime-consumingandrequirehigh
computational resources.
Apart from addressing the ability of LVLMs itself, hallucination can also be
mitigatedthroughpost-processingmethods.Thismethodusuallyinvolvesusing
anadditionalmoduleorexternaltoolstoedittheresponse.Recentmethodssuch
as LURE [45] utilize additional data to train a state detector and when hallu-
cination issues are detected, content is regenerated by a revisor model. Wood-
pecker[42]introducesanexternalvisualmodeltoinspectentitiesextractedfrom
Tokenizer
LVLMsPAI 5
the response, and then the detection results are handed over to the generation
model to regenerate better answers. These methods also extend the inference
chain and increase inference costs.
Training-free hallucination mitigation methods have so far only been at-
tempted in decoding methods. OPERA [15] discovered an abnormal attention
pattern that accompanies model decoding. It was statistically found that this
pattern often accompanies hallucination descriptions, and thus a detection and
mitigationmethodwasproposedbasedonthispatterntoalleviatethehallucina-
tionfacedbythemodel.VCD[18]introducedthenotionthatvisualuncertainty
increases hallucination descriptions and, based on this discovery, proposed a
contrast decoding method to alleviate hallucination issues.
3 Preliminaries
The architecture of LVLMs typically comprises of three main components: an
image encoder, a projector, and a language decoder. Both the image encoder
andlanguagedecoderareusuallypre-trained.Theimageencoderisemployedto
transformimagesintoimagetokens,whicharesubsequentlymappedtothetext
representationspacebytheprojector.Thisprocessenablestheconcatenationof
image tokens with text tokens that are then fed into the language decoder. The
language decoder subsequently generates corresponding responses based on the
provided instructions.
The existing projectors.Currently,projectorspredominantlyfallintotwo
categories: linear projectors and resamplers. A projector takes N visual features
from the image encoder and transforms them into M visual tokens. The lin-
ear projector employs a multilayer perceptron to transform all visual features,
maintaining a one-to-one transformation which meaning that M equals N. In
contrast, the resampler does notpreserveall visual features but instead samples
visual cues (M, where M < N). For instance, Q-former [19] utilizes M learnable
queries and Bert [12] to extract information from visual features. Given that
the knowledge of images during the generation process solely originates from
the output image tokens of the projectors, our attention is concentrated on the
image tokens post-projection, irrespective of their preceding modeling process.
Autoregressive language decoders. Nearly all LVLMs adopt LLaMA-
familymodelsastheirlanguagedecoders,whichemploytheself-attentionmech-
anism. The visual tokens processed by the projector are concatenated with text
tokensandfedintotheLLaMA,whichcarriesouttheforwarddecodingprocess.
From the perspective of a single attention head in a single layer, each head re-
peatedly performs the following attention operation with the same input shape:
(cid:18)
Q
K⊤(cid:19)
O =A V , A =softmax √h h . (1)
h h h h
d
k
Each attention head h performs an attention operation using its own set
of queries Q
h
∈ Rn×dk, keys K
h
∈ Rn×dk, and values V
h
∈ Rn×dk, where n
represents the sequence length and d represents the hidden dimensions. The
k6 Liu et al.
output O
h
∈ Rn×dk is modeled by multiplying V
h
and the attention weights
A ∈Rn×n,whereeachrowrepresentstheweightsforeachtokenduringfeature
h
mixing.Thisoperationenablesthemodeltofocusondifferentpartsoftheinput
for each head through attention weights, thereby capturing various parts of the
information from the sequence token representations. The final output is the
current generated token vocabulary conditional probability distribution y ∈Rv
based on the input instruction representations X , image representations X
I V
and history generated token representations X , where v is the size of the
H
vocabulary. This process can be formatted as:
y ∼p (y |X ,X ,X ),
model I V H
(2)
∝softmax(logit (y |X ,X ,X )),
model I V H
whichcalculatesthedistributionofonetokenanditeratesfortheentireresponse.
ThesequencegenerationcontinuesuntilanEOS(EndofSentence)tokenispro-
duced, marking the end of the generation and resulting in a complete response.
4 Method
At the core of our method is a solution for image neglect and text inertia,
both of which are fundamentally interconnected. Essentially, as paying more
attention to the image, there is a corresponding reduction in the reliance on
language priors. Intuitively, in a conversation centered around an image, the
model should devote more attention to the image, thereby allowing it to have a
significant impact on the response. As such, we identify the self-attention map
in the token-level generation and augment the image attention in its original
directions. This strategy promotes a more image-centric latent representations.
Additionally,tofurthermitigatetheinfluenceoftextinertia,wedevidethelogits
distribution of pure text input into the model’s output.
4.1 Pay More Attention to Image
Extracting the Self-Attention Matrix. We start from a token-level per-
spective. The response process in LVLMs is fundamentally generated token by
token. Each token is generated based on the input image, instruction, and the
historically generated response. This process is facilitated through a multi-layer
attention decoder architecture. Consequently, this results in a probability dis-
tribution of the vocabulary for the currently generated token. Our goal is to
extract the attention matrix of each attention head at every layer, indicating
the influence of each content during inference.
When generating the k-th token in the sequence, the input representation
for the attention head in the forward process includes the instruction represen-
tation X = [x ,...,x ], image representation X = [x ,...,x ], and
I i1 inI V v1 vnV
the representation of the historically generated response X =[x ,...,x ].
H h1 hnH
Notably, the image representation considered here is the one that has been pro-
cessed by the projector. Essentially, the hidden states of each input layer are
X =concat(X [1:m],X ,X [m+1:n ],X ), where the notation X [1:m]
I V I I H IPAI 7
indicates the first m elements in the instruction representation. Each attention
head assigns different degrees of attention to each element during the current
tokenrepresentationdecodingprocess.Ouraimistoenhancetheattentionpaid
to the image. Therefore, we extract the attention weight values related to the
image token for the current generated token, intervene, and then redistribute
the attention values of each element through softmax.
Excite model in a trustful direction. There have been attempts in some
LLMsworkstomaketheanswersgeneratedbyLLMsmoretrustworthybymeans
of intervention [4,6,16,20,36]. The implemented approach typically involves
intervening with the hidden states. As for defining what constitutes a more
trustworthy direction, it usually requires additional projection and training to
probe this trustful direction. In our case, a response that is more image-based is
considered more trustworthy. Since LVLMs have undergone alignment training,
the original attention values provide a direction based on image content. As
illustratedinFig.4,byamplifyingtheattentionvaluesofimagetokensbasedon
the original attention values, we can enhance the trustworthiness of our results.
Anothernuanceinvolvesouravoidanceofchoosingtheattentionheadusedto
shiftintervention.IntheITImethod[20],itisstatedthatnotallattentionheads
should be subjected to intervention. Therefore, they introduce a trustful score
to rank each head across all layers and select the top-k heads for intervention.
In our case, the less trustworthy heads with lower attention values receive less
intervention. We first extract the attention weights of the image tokens for the
currentgeneratedtokenfromtheattentionweightsA˜ beforesoftmaxopeartion.
Wethenusethehyper-parameterαtocontrolthestepsizeforintervention.From
a single attention head perspective, our method can be expressed as follows:
A˜ =A˜ +α·|A˜ | for j =m+1 to m+n . (3)
n,j n,j n,j V
Themodel’sfinalvocabularyprobabilitydistributionisderivedfromtheprojec-
tionofthehiddenstatesofthelasttokeninthesequence.Therefore,weextract
the attention weights of the last token n on the image tokens by indexing A˜ .
n,j
Followingtheintervention,weusethesoftmaxfunctiontoredistributetheatten-
tionvaluesofeachtokenduringthereassignmentofencodedhiddenstates.This
procedure is repeated for each subsequent token prediction in an autoregressive
manner and is independent of the choice of the decoding algorithm.
Excite more precisely with an attention mode prior. The presence of
the BOS token, an attention sink pattern [40], in a sentence results in higher
attention values during the attention computation process, which may seem
counterintuitive. The BOS token typically signifies the start of a sentence and,
as such, does not carry significant semantic content. However, the generation
of tokens is significantly influenced by this particular token, a similar pattern
thatalsomanifestsitselfinvisualmodels[11].AsmentionedinStreamLLM[40],
the pattern of the attention sink emerges when redundant attention values are
present.
Naturally, one might infer that when the sink pattern appears, we excite the
image token. To further investigate this phenomenon, as depicted in Fig. 5, we8 Liu et al.
(a) LLAVA (b) Minigpt4 (c) Shikra
Fig.5: The BOS token attention ratios of three model. We calculate the attention
weightsoftheBOStokenforeachheadineachlayeranddisplaytheminaheatmap.
find that the sink phenomenon is not overtly evident in the shallow layers. This
is because the shallow layers tend to focus more on encoding semantically rich
information [37]. When the encoding of semantically rich tokens stabilizes, the
attention sink phenomenon arises. Therefore, we build upon the judgement of
intervention timing by calculating the similarity of the hidden states.
4.2 Image-Centric Logit Refine
InFig.1,weobserveapeculiarphenomenonwhereLVLMscontinuetogenerate
identicalhallucinatedtextevenwhentheimageisremovedfromtheinput.This
observation naturally leads us to the concept of using the output distribution
(when no image is in input) as a reference to penalize our initial prediction
distribution. Therefore, we update the distribution of the generated token by:
p =γ·p (y|X ,X ,X )
model model V I H
(4)
−(γ−1)·p (y|X ,X ).
model I H
This equation effectively reduces the predicted probability based on text alone.
The weight γ is used to control the degree of penalty applied to the initial
prediction distribution.
This operation is conceptually similar to LLM-CFG [32]. Essentially, it pro-
vides a guided generation mechanism that allows the model to make informed
choices between outputs based on image content and those based on language
logic.Thisway,themodelcanbetterbalancetheinfluenceofvisualandtextual
information in its outputs, leading to more contextually accurate and relevant
results.
5 Experiments
5.1 Setup
Baselines. We evaluate the effectiveness of our method on three different mod-
els. To better compare the impact of image feature tokens after different projec-
tors on our method, we selected two models that use linear projectors, LLAVAPAI 9
andShikra,aswellasonemodelthatusesresamplers,Minigpt4.Additionally,for
amoreconvincingcomparison,wereportonthreedecodingmethodsforcompar-
ison:greedy,beamsearch,andnucleussample.WealsoselectedtheOPERA[15]
method, which is an improvement on beam search, and the VCD [18] method,
which is an improvement on nucleus sampling, to compare with our results. We
used the default hyperparameters from the open-source versions of these two
methods.
Implementation Details. As different models have different lengths of image
tokens, leading to different degrees of image neglect, to better align with the
image sequence length of the model, we set α = 0.5 for LLAVA, α = 0.6 for
Shikrawithlongimagetokensequencelengthsandα=0.2forresamplermodels
with short image token sequences. As text inertia is independent of the image
token length, we continuously use γ =1.1. Apart from this, in the beam search
tests, the beam number is set to 5 for all methods, and in the nucleus sample
tests, all the common parameters are consistent.
5.2 Benchmark & Evaluation Metrics
CHAIR[31].CaptionHallucinationAssessmentwithImageRelevance(CHAIR)
is a widely used metric in image captioning tasks. CHAIR operates by creating
a set of ground-truth object labels for each image. Any object mentioned in the
caption that does not exist in the label set is considered a hallucinated object.
CHAIR comprises two evaluation dimensions: instance-level and sentence-level,
represented as CHAIR and CHAIR , respectively. These are calculated in the
I S
following manner:
|{hallucinated objects}|
CHAIR = , (5)
I all mentioned objects
|{captions with hallucinated objects}|
CHAIR = . (6)
S all captions
WeconductedexperimentsbasedonthevalidationsetofMSCOCO2014.Given
“Please help me describe the image in detail.” asprompt,wesubsequently
employtheCHAIRmetrictoevaluatethegenerateddescription.Toevaluatehal-
lucinations in long sequence generation, we adopted the same setup as used in
LURE [45] and OPERA [15], setting the max_new_tokens parameter to 512
and randomly sampled 500 instances for evaluation.
POPE [21].ThePolling-basedObjectProbingEvaluation(POPE)isaevalua-
tionmetricdesignedintheVQAparadigm.POPEservesasametricforassessing
object hallucination, evaluating hallucinations by asking LVLMs questions such
as “Is there a <object> in the image?” Here, <object> is replaced with
the constructed ground-truth object from three different types of splits. In the
“random” split, objects are randomly selected from the entire dataset for eval-
uation. In the “popular” split, objects are chosen from those most frequently
appearing in the dataset. In the “adversarial” split, objects that are highly re-
latedtotheimageobjectsareselectedforevaluation.Weconductourevaluation10 Liu et al.
Table1:CHAIRhallucinationevaluationresultsonthreeLVLMs.CHAIR[31]is
employedastheevaluationmetric,whereasmallernumberindicateslesshallucinations.
OPERA is a decoding method based on beam search, VCD is a decoding method
based on nucleus sampling, and PAI is an inference intervention method that can be
collaborated with any decoding method.
LLAVA [26] Minigpt4 [46] Shikra [8]
Decoding Method
CHAIR CHAIR CHAIR CHAIR CHAIR CHAIR
S I S I S I
Vanilla 46.6 13.4 32.8 11.1 51.2 14.4
Greedy
PAI 24.8 6.9 26.3 8.8 37.6 10.0
Vanilla 46.4 14.3 46.6 13.4 53.0 14.7
Beam SearchOPERA [15] 44.6 14.4 30.1 9.8 36.8 12.4
PAI 21.8 5.6 24.8 6.9 35.8 11.4
Vanilla 58.2 18.2 32.7 11.9 57.9 16.4
Nucleus VCD [18] 51.8 15.1 34.8 11.5 57.6 16.3
PAI 43.4 14.7 26.7 10.3 49.9 13.2
on the COCO dataset with 500 images, with each image having 6 questions for
each split of POPE. We evaluate the performance of the model in object recog-
nition tasks using both accuracy score and F1 score. Besides, in order to more
comprehensively examinehallucinationsinmulti-turn chats,we havestructured
the evaluation in both single-turn and multi-turn dialogue forms.
MMHal-Bench [34]. For further evaluation of our method on some chal-
lenging datasets, we choose MMHal-Bench, which is designed with 96 image-
question pairs, spread across 8 question categories × 12 object topics. It con-
tains eight types of questions about object attributes, adversarial objects, com-
parisons,counting,spatialrelations,environment,holisticdescriptions,andoth-
ers to comprehensively assess the model’s hallucination performance on high-
difficulty datasets. Essentially, it is also a VQA-based evaluation, but unlike the
existence-based examination in POPE, its questions also include some logical
considerations. Therefore, for evaluation on MMHal-Bench, we first need to an-
swer questions and then use GPT-4 to score the answer based on the response
and the ground-truth answer. The evaluation results include the model’s scores
across all question categories, and the overall score represents the average of
these scores.
GPT-4vAssistedEvaluation.Tofurtherevaluatethemodel’sperformancein
imagedescriptiontasks,wecanmovebeyondtheCHAIRmetric,whichisbased
on information extraction and only considers object hallucination. We can use
GPT-4vforopenevaluation.Aswithpreviousevaluations[15,18],wesample50
imagesontheCOCOdatasetforevaluation.Weconstructprompts,inputimages
into GPT-4v, along with the descriptions responses from two assistants. GPT-
4v evaluation takes into account two dimensions: Accuracy and Detailedness,
denoted respectively as C and D. Detailed prompt construction can be found in
the appendix.PAI 11
Table 2: Quantitative comparison on POPE. The best results are in bold.
LLAVA[26] Minigpt4[46] Shikra[8]
Decoding Method
Single-turnMulti-turnSingle-turnMulti-turnSingle-turnMulti-turn
Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1
Vanilla 84.76 85.51 85.69 84.27 74.39 73.78 78.0 77.77 81.56 82.05 78.16 78.77
Greedy
PAI 85.8285.9787.5486.83 75.2 76.2 80.82 81.6 82.3 81.47 78.5779.11
Vanilla 84.9 84.9 83.57 81.13 72.54 67.73 71.78 67.92 81.52 81.89 78.18 78.13
BeamSearch
PAI 86.3385.8986.4285.2773.2269.3674.1571.7282.25 81.01 80.7280.02
Vanilla 80.25 81.32 82.87 81.63 57.52 58.16 64.49 63.89 79.55 80.28 76.54 77.12
Nucleus VCD[18] 79.53 81.02 84.03 83.3 59.97 58.7 69.72 69.38 80.22 80.5 76.38 77.07
PAI 81.7282.8785.0383.97 59.19 60.8 70.2470.2781.4980.7177.9978.47
5.3 Experimental Results
In this section, we analyze the performance of PAI across various hallucina-
tion evaluation tasks, including long image description, simplified VQA answer,
construction of metric evaluation, and leveraging the near-human cognitive ca-
pabilities of GPT-4/GPT-4v as evaluation methods. For further analysis, please
refer to the appendix.
Results on long sequence hallucination evaluation. The experimental
results are presented in Tab. 1. As our approach is an inference intervention
method,itdiffersfrompreviousdecodinghallucinationmitigationmethodsthat
primarily concentrate on improving a single decoding method. We have tested
our method on three decoding techniques. Our method has achieved hallucina-
tionmitigationonallthreedecodingmethodsusedbythethreemodels.However,
when integrated with the nucleus, a sampling-based method, the hallucination
reduction brought about by our method is not significant. This may be because
eventhoughourmethodhasincreasedthepriorityoftrustfultokens,thesample
set during nucleus decoding still contains many hallucination tokens.
Moreover, while OPERA significantly mitigates hallucinations, its time effi-
ciency is considerably higher compared to vanilla. In contrast, our method not
onlyhasalmostthesametimeefficiencyasvanilla,butitalsoperformsbetterin
reducing hallucination issues. Compared to VCD, during the generation process
of long sequence tasks, the introduction of visual uncertainty during decoding
sometimes leads to more hallucination descriptions. However, our method can
reduce the proportion of hallucination words in the sample pool.
Results on single-turn and multi-turn hallucination evaluation. Unlike
the CHAIR evaluation, POPE is in a VQA format, so the response is brief, an-
swering only “Yes” or “No”. The phenomena of text inertia and image neglect
may not be as noticeable under this setting, especially for single-turn. How-
ever, our method still achieved a notable improvement compared to the vanilla
decoding method in single-turn. As for multi-turn evaluation, which involves a
longer context, our method achieves more significant improvement as presented
in Tab. 2.12 Liu et al.
(a) LLAVA (b) Minigpt4 (c) Shikra
Fig.6: Quantitative comparisonontheMMHal-Bench.Higherscoresindicatebet-
ter performance.
Results on hallucination evaluation in comprehensive general scenar-
ios. The experimental results, as shown in Fig. 6, indicate that for some more
image-based question types, such as object attributes, adversarial objects, and
holistic questions, the answers are more accurate when inference intervention
with PAI is applied, and there is a certain degree of improvement across all
models. However, for some logical questions, such as comparisons and relations,
there is no noticeable improvement after intervention. In summary, through the
overallmetric,i.e.,theaverageoftheeightevaluationdimensions,thereisacer-
tain degree of improvement compared to the baseline after incorporating PAI.
Results on human-like GPT-4v assisted hallucination evaluation. The
experimentalresults,asshowninTab.3,indicatethatevenwhenmorecompre-
hensive hallucination evaluation dimensions are added, our method, compared
tothegreedydecodingmethod,canprovidemoreaccurateresponsesonallthree
models without losing detail in the description. Given that GPT-4v’s visual un-
derstanding and language logic capabilities have reached a level close to that of
humans, it can more comprehensively illustrate the performance improvements
brought about by our method.
Table 3: Results on GPT-4V evaluation. The best results are in bold.
LLAVA Minigpt4 Shikra
Method
C D C D C D
Greedy 5.62 5.24 5.8 5.74 5.54 5.25
PAI 6.465.367.04 5.89 6.04 5.05
5.4 Ablation Study
Ourmethod,PAI,consistsoftwostagesofinterventions.Inthefirststage,during
forward inference, the hyperparameter α is utilized to set the scale of interven-
tion.Simultaneously,thelayerprior,representedas“L”,isusedtodeterminethePAI 13
Table4:AblationStudyoftheHyperparameterα.Whenαbecomesexcessively
large, resulting in an unbalanced response, we terminate the experiment and denote
this with a dash (-). F1 values that are considered outliers are highlighted in red.
LLaVA Minigpt4 Shikra
α γ L
CHAIR CHAIR F1 CHAIR CHAIR F1 CHAIR CHAIR F1
S I S I S I
- - - 46.2 13.8 75.9 31.6 10.5 69.4 56.2 15.8 74.6
0.11.1✓ 47.4 13.8 76.9 25.0 8.0 69.9 56.4 15.0 75.7
0.21.1✓ 47.4 13.6 76.9 21.2 7.7 70.3 57.2 15.6 75.2
0.41.1✓ 42.4 12.4 76.9 15.1 6.3 67.4 55.2 14.4 75.3
0.51.1✓ 24.6 6.8 74.7 5.4 2.9 61.5 51.6 13.2 75.8
0.61.1✓ 7.8 4.8 62.6 - - - 37.2 10.6 76.7
0.71.1✓ - - - - - - 14.4 6.1 64.9
attention layer for intervention. The second stage unfolds during the decoding
process, where we mitigate text inertia by subtracting the logits distribution
that results from inputs devoid of image information. In this stage, the scale is
managed by the parameter γ.
We use LLaVA-1.5 as the representative LVLM baseline and the greedy de-
codingmethodasthebasicbaselinetocomparetheimpactofourhyperparame-
tersonthetaskoflongsequenceimagedescription.Toevaluateourmethod,we
choose the CHAIR metric. However, since CHAIR only evaluates the hallucina-
tionproblem,wehaveincorporatedtheF1scoretoconsiderinformationrichness
and accuracy. This makes the comparison with the CHAIR metric fairer when
the F1 scores are similar. In the above, the F1 scores of the various methods
differ slightly, so we have not included this somewhat redundant measure. The
F1 score is calculated by counting the objects included in the description, the
objects in the ground-truth sets, and the hallucinated objects.
Effects of α in Exciting Image Attention. In the process of exciting the
attentionvaluesofimagetokens,weintroduceaparameterαtocontroltheam-
plificationscale.Asshownin Tab.4,differentLVLMsexhibitvaryingsensitivity
totheamplificationscale.Thissensitivitynotonlydependsonthelengthofthe
model’s image tokens (e.g., the image token length of LLaVA-1.5 is 576, while
thatofMinigpt4isonly32),butalsotheoriginalattentionweightsdistribution.
However, a commonality across these models is that an appropriate amplifi-
cation scale can achieve a balance between the number of hallucinated objects
in the description and the amount of information conveyed. If the scale is too
small, the description may still contain many hallucinated objects. Conversely,
if the scale is too large, the amount of information in the response will decrease.
EffectsofLayerPriorinExcitingImageAttention.Wefurtherinvestigate
thecontroloftheinterventionlayerforexcitingimageattention.Asobservedin
Tab. 5, the introduction of this prior does indeed improve the performance of
our method. However, when there is no control over the intervention layer and
interventions are applied to all layers, the performances of different models ex-14 Liu et al.
Table 5: Ablation Study of Hyperparameter γ and Layer Prior “L”. Results
are presented for the models LLAVA (left), Minigpt4 (middle), and Shikra (right).
α γ L CHAIRSCHAIRI F1 α γ L CHAIRSCHAIRI F1 α γ L CHAIRSCHAIRI F1
- - - 46.2 13.8 75.9 - - - 31.6 10.5 69.4 - - - 56.2 15.8 74.6
0.51.1✓ 24.6 6.8 75.7 0.21.1✓ 21.2 7.7 70.3 0.61.1✓ 37.2 10.6 76.7
0.51.2✓ 23.4 6.6 75.7 0.21.2✓ 16.4 6.2 70.3 0.61.2✓ 40.2 10.5 76.0
0.51.3✓ 24.6 7.5 74.4 0.21.3✓ 14.4 6.9 69.5 0.61.3✓ 40.6 10.5 76.0
0.51.5✓ 25.0 9.6 74.6 0.21.5✓ 12.6 5.4 67.1 0.61.5✓ 40.2 10.5 75.3
0.52.0✓ 24.2 7.1 74.0 0.22.0✓ 8.4 4.6 64.3 0.62.0✓ 39.4 10.1 76.2
0.51.1✓ 24.6 6.8 75.7 0.21.1✓ 21.2 7.7 70.3 0.61.1✓ 37.2 10.6 76.7
0.51.1× 20.6 6.7 71.7 0.21.1× 22.3 8.4 70.3 0.61.1× 56.4 15.7 74.9
hibitsomevariations.ForShikra,intheabsenceofthelayerprior,ourmethod’s
results regress to the baseline. For both LLaVA and Minigpt4, the loss of the
layer prior causes some unfavorable fluctuations in both the CHAIR metric and
the F1 score.
Effects of γ in Mitigating Language Prior. Tab. 5 presents the results
of an ablation study focusing on γ, which adjusts the balance between output
distributions from conditioned inputs with excited image tokens and pure text
inputs. Unlike the other two models, the Minigpt4 model is highly sensitive
to γ. When γ is too large, it can lead to uncontrollable model behavior. The
experimental results suggest that maintaining γ within a relatively small range,
such as 1.1 - 1.2, yields the most stable performance.
6 Conclusion and Limitation
In this paper, we first analyze the causes and manifestations of hallucinations
inLVLMs.Weproposeaphenomenontermed"textinertia,"whereinthemodel
continues to produce the same hallucinatory descriptions even when no image
input is provided. This issue fundamentally stems from the model’s neglect of
image tokens. Consequently, we introduce the PAI method to intervene in the
model’s inference process, steering it towards an image-based and trustworthy
direction.Thisisatraining-freemethodanddoesnotrequireanyexternaltools.
Extensive experiments on multiple benchmarks and LVLMs have validated the
effectiveness of PAI in mitigating hallucination issues.
Limitations: (1) The language decoders of existing open-source LVLMs are
primarily models from the LLaMA-family. It is worth exploring whether the
issues of image neglect and text inertia are introduced by LLaMA. (2) As we
described, our method fundamentally alleviates the image neglect issue during
model inference. Its upper limit depends on the capabilities of the well-trained
model. Therefore, it is worth investigating whether incorporating this issue as a
lossduringthetrainingprocesscouldleadtofurtherperformanceimprovements.PAI 15
Acknowledgements
ThisworkissupportedbytheNationalScienceFoundationofChina(62132017),
Zhejiang Provincial Natural Science Foundation of China (LD24F020011).
References
1. Abnar,S.,Zuidema,W.:Quantifyingattentionflowintransformers.arXivpreprint
arXiv:2005.00928 (2020)
2. Alayrac,J.B.,Donahue,J.,Luc,P.,Miech,A.,Barr,I.,Hasson,Y.,Lenc,K.,Men-
sch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T.,
Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S.,Brock, A., Ne-
matzadeh,A.,Sharifzadeh,S.,Binkowski,M.,Barreira,R.,Vinyals,O.,Zisserman,
A., Simonyan, K.: Flamingo: a visual language model for few-shot learning
3. Anisha,G.,Yin,J.,Bas,E.:Detectingandpreventinghallucinationsinlargevision
language models (Aug 2023)
4. Azaria, A., Mitchell, T.: The internal state of an llm knows when its lying (Apr
2023)
5. Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.,Dhariwal,P.,Neelakan-
tan,A.,Shyam,P.,Sastry,G.,Amanda,A.,Agarwal,S.,Herbert-Voss,A.,Krueger,
G., Tom, H., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C.,
Chen, M., Sigler, E., Litwin, M., Gray, S., Benjamin, C., Clark, J., Berner, C.,
Sam, M., Radford, A., Sutskever, I., Amodei, D.: Language models are few-shot
learners. arXiv: Computation and Language,arXiv: Computation and Language
(May 2020)
6. Burns, C., Ye, H., Klein, D., Steinhardt, J.: Discovering latent knowledge in lan-
guage models without supervision (Dec 2022)
7. Chefer, H., Gur, S., Wolf, L.: Transformer interpretability beyond attention visu-
alization. In: Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition. pp. 782–791 (2021)
8. Chen,K.,Zhang,Z.,Zeng,W.,Zhang,R.,Zhu,F.,Zhao,R.,Research,S.,Sklsde,
S.,Beihang,B.,Seiee,S.:Shikra:Unleashingmultimodalllm’sreferentialdialogue
magic
9. Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Muyan, Z., Zhang, Q.,
Zhu,X.,Lu,L.,Li,B.,Luo,P.,Lu,T.,Qiao,Y.,Dai,J.:Internvl:Scalingupvision
foundation models and aligning for generic visual-linguistic tasks (Dec 2023)
10. Dai,W.,Li,J.,Li,D.,Huat,A.,Zhao,J.,Wang,W.,Li,B.,Fung,P.,Hoi,S.:In-
structblip: Towards general-purpose vision-language models with instruction tun-
ing
11. Darcet, T., Oquab, M., Mairal, J., Bojanowski, P.: Vision transformers need reg-
isters. arXiv preprint arXiv:2309.16588 (2023)
12. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidi-
rectional transformers for language understanding. In: Proceedings of the 2019
Conference of the North (Jan 2019). https://doi.org/10.18653/v1/n19-1423,
http://dx.doi.org/10.18653/v1/n19-1423
13. Han, Y., Zhang, C., Chen, X., Yang, X., Wang, Z., Yu, G., Fu, B., Zhang, H.:
Chartllama:Amultimodalllmforchartunderstandingandgeneration(Nov2023)
14. Hu, H., Zhang, J., Zhao, M., Sun, Z.: Ciem: Contrastive instruction evaluation
method for better instruction tuning16 Liu et al.
15. Huang, Q., Dong, X., Zhang, P., Wang, B., He, C., Wang, J., Lin, D., Zhang, W.,
Yu,N.:Opera:Alleviatinghallucinationinmulti-modallargelanguagemodelsvia
over-trust penalty and retrospection-allocation (Nov 2023)
16. Kai,J.,Zhang,T.,Hu,H.,Lin,Z.:Sh2:Self-highlightedhesitationhelpsyoudecode
more truthfully (Jan 2024)
17. Lai, X., Tian, Z., Chen, Y., Li, Y., Yuan, Y., Liu, S., Jia, J., Kong, H., Research,
M.: Lisa: Reasoning segmentation via large language model
18. Leng,S.,Zhang,H.,Chen,G.,Li,X.,Lu,S.,Miao,C.,Bing,L.:Mitigatingobject
hallucinationsinlargevision-languagemodelsthroughvisualcontrastivedecoding
19. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large language models
20. Li, K., Patel, O., Viégas, F., Pfister, H., Wattenberg, M.: Inference-time inter-
vention: Eliciting truthful answers from a language model. Advances in Neural
Information Processing Systems 36 (2024)
21. Li,Y.,Du,Y.,Zhou,K.,Wang,J.,Zhao,W.X.,Wen,J.R.:Evaluatingobjecthal-
lucinationinlargevision-languagemodels.arXivpreprintarXiv:2305.10355(2023)
22. Li, Z., Yang, B., Liu, Q., Ma, Z., Zhang, S., Yang, J., Sun, Y., Liu, Y., Bai, X.:
Monkey:Imageresolutionandtextlabelareimportantthingsforlargemulti-modal
models (Nov 2023)
23. Liu, F., Lin, K., Li, L., Wang, J., Yacoob, Y., Wang, L.: Mitigating hallucina-
tion in large multi-modal models via robust instruction tuning. In: The Twelfth
International Conference on Learning Representations (2023)
24. Liu, F., Lin, K., Li, L., Wang, J., Yacoob, Y., Wang, L.: Mitigating hallucination
in large multi-modal models via robust instruction tuning (Sep 2023)
25. Liu, H., Xue, W., Chen, Y., Chen, D., Zhao, X., Wang, K., Hou, L., Li, R., Peng,
W.: A survey on hallucination in large vision-language models
26. Liu,H.,Li,C.,Li,Y.,Lee,Y.J.:Improvedbaselineswithvisualinstructiontuning.
arXiv preprint arXiv:2310.03744 (2023)
27. Liu, H., Li, C., Wu, Q., Lee, Y., Madison, M., Research, M.: Visual instruction
tuning
28. Mishra, S., Khashabi, D., Baral, C., Hajishirzi, H.: Cross-task generalization via
natural language crowdsourcing instructions. In: Proceedings of the 60th Annual
MeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers)
(Jan 2022). https://doi.org/10.18653/v1/2022.acl-long.244, http://dx.
doi.org/10.18653/v1/2022.acl-long.244
29. Ouyang,L.,Wu,J.,Jiang,X.,Almeida,D.,Wainwright,C.L.,Mishkin,P.,Zhang,
C.,Agarwal,S.,Slama,K.,Ray,A.,Schulman,J.,Hilton,J.,Kelton,F.,Miller,L.,
Simens,M.,Askell,A.,Welinder,P.,Christiano,P.F.,Leike,J.,Lowe,R.:Training
language models to follow instructions with human feedback (2022)
30. Peng, B., Li, C., He, P., Galley, M., Gao, J.: Instruction tuning with gpt-4
31. Rohrbach, A., Hendricks, L.A., Burns, K., Darrell, T., Saenko, K.: Object hallu-
cination in image captioning. In: Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing (Jan 2018). https://doi.org/10.
18653/v1/d18-1437, http://dx.doi.org/10.18653/v1/d18-1437
32. Sanchez,G.,Fan,H.,Spangher,A.,Levi,E.,Ammanamanchi,P.S.,Biderman,S.:
Stayontopicwithclassifier-freeguidance.arXivpreprintarXiv:2306.17806(2023)
33. Sun, Z., Shen, S., Cao, S., Liu, H., Li, C., Shen, Y., Gan, C., Gui, L.Y., Wang,
Y.X., Yang, Y., Keutzer, K., Darrell, T.: Aligning large multimodal models with
factually augmented rlhf (Sep 2023)PAI 17
34. Sun, Z., Shen, S., Cao, S., Liu, H., Li, C., Shen, Y., Gan, C., Gui, L.Y., Wang,
Y.X.,Yang,Y.,etal.:Aligninglargemultimodalmodelswithfactuallyaugmented
rlhf. arXiv preprint arXiv:2309.14525 (2023)
35. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,
Rozi‘ere, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave,
E., Lample, G.: Llama: Open and efficient foundation language models
36. Turner, A., Thiergart, L., Udell, D., Leech, G., Mini, U., MacDiarmid, M.: Ac-
tivation addition: Steering language models without optimization. arXiv preprint
arXiv:2308.10248 (2023)
37. Wang, L., Li, L., Dai, D., Chen, D., Zhou, H., Meng, F., Zhou, J., Sun, X.: Label
words are anchors: An information flow perspective for understanding in-context
learning. arXiv preprint arXiv:2305.14160 (2023)
38. Wei, F., Zhang, X., Zhang, A., Zhang, B., Chu, X.: Lenna: Language enhanced
reasoning detection assistant (Dec 2023)
39. Wei,J.,Bosma,M.,Zhao,V.,Guu,K.,Yu,A.,Lester,B.,Du,N.,Dai,A.,Le,Q.:
Finetuned language models are zero-shot learners. Learning,Learning (Sep 2021)
40. Xiao, G., Tian, Y., Chen, B., Han, S., Lewis, M.: Efficient streaming language
models with attention sinks (Sep 2023)
41. Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi,
Y., Li, C., Xu, Y., Chen, H., Tian, J., Qi, Q., Zhang, J., Huang, F.: mplug-owl:
Modularization empowers large language models with multimodality
42. Yin,S.,Fu,C.,Zhao,S.,Xu,T.,Wang,H.,Sui,D.,Shen,Y.,Li,K.,Sun,X.,Chen,
E.: Woodpecker: Hallucination correction for multimodal large language models
43. You, H., Zhang, H., Gan, Z., Du, X., Zhang, B., Wang, Z., Cao, L., Chang, S.F.,
Yang, Y.: Ferret: Refer and ground anything anywhere at any granularity (Oct
2023)
44. Zhang, Y., Qian, S., Peng, B., Liu, S., Jia, J.: Prompt highlighter: Interactive
control for multi-modal llms. arXiv preprint arXiv:2312.04302 (2023)
45. Zhou, Y., Cui, C., Yoon, J., Zhang, L., Deng, Z., Finn, C., Bansal, M., Yao, H.:
Analyzingandmitigatingobjecthallucinationinlargevision-languagemodels(Oct
2023)
46. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-
language understanding with advanced large language models18 Liu et al.
Appendix
A Text Inertia Detection Process
TodetectthetextinertiaphenomenoninLVLMsduringimagedescriptiontasks,
we approach this issue in two stages. In the first stage, we employ the CHAIR
metrictoidentifytheindicesofhallucinatedobjectsintheLVLMs’descriptions.
In the following stage, we extract the tokens preceding the hallucinated objects,
feed them into the LVLMs, and proceed with generation, excluding the image
fromtheinput.Inotherwords,weonlyprovidetheLVLMswiththetaskinstruc-
tion and history response up to the index of the hallucinated object. Finally, we
extract the first ten tokens from the newly generated output and use GPT-4 to
determine whether the same object has been generated. Our prompt structure
is shown in Tab. S1.
Table S1: The Prompt Used for Text Inertia Detection.
GPT-4 Prompt
Please assist me in determining whether the following descriptions include the
specifiedobject.Simplyrespondwith“Yes”or“No”.Considersynonymsandsimilar
expressions.
[Object] {object}
[Description] {description}
Here, {object} is replaced by the hallucinated object that we extract each
time,and{description}isreplacedbythefirsttentokensofthenewlygenerated
conditioneddescription.Whenthemodelanswers“Yes”,itindicatesthepresence
of the text inertia phenomenon.
Table S2: Performance of Our Method PAI on the LLaVA-1.5 (13B) Model. We
present the average Accuracy and F1 score across the three splits of POPE.
CHAIR POPE
Model Method
CHAIR CHAIR F1 Acc F1
S I
Greedy 44.0 12.7 77.3 85.47 86.60
LLaVA-1.5(13B)
PAI 33.0 9.2 77.886.8287.80
B What if Model Scales Up?
WeperformedexperimentsontheLLaVA-1.5-13Bscalemodeltoassesshowthe
performanceofPAIvarieswithincreasingmodelscale.Asshownin Tab.S2,ourPAI 19
Table S3: Comparison with Different Intervention Method. We present the average
Accuracy and F1 score across the three splits of POPE.
CHAIR POPE
Model Method
CHAIR CHAIR F1 Acc F1
S I
Greedy 46.2 13.8 75.9 84.76 85.59
LLaVA-1.5(7B) PH 52.8 14.2 76.9 84.11 85.20
PAI 23.4 6.6 75.786.1386.42
Greedy 44.0 12.7 77.3 85.47 86.60
LLaVA-1.5(13B)PH 46.6 12.8 77.3 85.69 86.78
PAI 33.0 9.2 77.886.8287.80
method maintains its robustness regardless of the scale of the model. Notably,
it continues to alleviate the hallucination issue in both long sequence and short
VQA tasks as the model scale increases.
C Comparative Experimental Results
We compare our method with a different intervention approach during model
inference. Prompt Highlighter [44] proposes an interactive technique for con-
structingvisualpromptstosteermodelgeneration.Essentially,itaddsavalueof
the same size to the attention mask of the tokens in the user’s guidance area to
influence the model’s output. To automate the intervention process for evalua-
tion, we adopt an approach similar to Prompt Highlighter. This involves adding
a constant value of 1.0 to the attention mask of all image tokens, where 1.0 is
the default value provided by their open-sourced code.
ThefinalexperimentalresultsarepresentedinTab.S3.Thefindingsindicate
thatuniformlyapplyinginterventionacrosstheentireimageislesseffectivethan
enhancing image attention based on the original scale.20 Liu et al.
Input Image LLAVA LLAVA+PAI Input Image LLAVA LLAVA+PAI
Mouse Mouse Motorboat Boat
Input Image LLAVA+PAI LLAVA+PAI Input Image LLAVA+PAI LLAVA+PAI
Dog Table Dog Bicycle
Fig.S1:Visualizationoftheself-attentionmapsforeachobjecttokenwithandwithout
PAI over LLAVA. The hallucinated objects are higlighted in red.
D Obtaining Explainable LVLMs.
The extent to which attention can serve as an explanatory mechanism has been
extensively studied [1,7]. Within the scope of LVLMs, the self-attention maps
in the LLaMA decoder have been regarded as a natural explanation for the
model. However, these explanatory results are only available for LLAVA and
Shikra,duetotheusageoflinearprojectionthatpreservesallimagepatchtoken
features. The model, which employs a resampler to derive the explanation, has
to reverse-engineer the abstracted image token back to the input image patch
token. However, tracing back the image patch after it has been encoded by the
visual encoder and resampled into an image token proves to be challenging.
As illustrated in Fig. S1, the original LLAVA identifies the correct object,
a mouse, and hallucinates an object, a motorboat, both of which have limited
interpretability as the results do not focus on the main body of the described
object.Withourmethodoflayerintervention,theresponsebecomesmoreimage-
centric. The description aligns better with the image location, which can all be
considered as a more faithful explanation.PAI 21
Table S4: The prompt used for GPT-4V evaluation
GPT-4V Prompt
You are required to score the performance of two AI assistants in describing a given
image.Youshouldpayextraattentiontothehallucination,whichreferstothepartof
descriptionsthatareinconsistentwiththeimagecontent,suchasclaimingtheexistence
ofsomethingnotpresentintheimageordescribingincorrectlyintermsofthecounts,
positions, or colors of objects in the image. Please rate the responses of the assistants
on a scale of 1 to 10, where a higher score indicates better performance, according to
the following criteria:
1: Accuracy: whether the response is accurate with respect to the image content. Re-
sponses with fewer hallucinations should be given higher scores.
2:Detailedness:whethertheresponseisrichinnecessarydetails.Notethathallucinated
descriptions should not count as necessary details.
Please output the scores for each criterion, containing only two values indicating the
scores for Assistant 1 and 2, respectively. The two scores are separated by a space.
Following the scores, please provide an explanation of your evaluation, avoiding any
potentialbias andensuringthatthe orderinwhichtheresponseswere presenteddoes
not affect your judgment.
[Assistant 1]
{Response of Assistant 1}
[End of Assistant 1]
[Assistant 2]
{Response of Assistant 2}
[End of Assistant 2]
Output format:
Accuracy: <Scores of the two answers>
Reason:
Detailedness: <Scores of the two answers>
Reason:22 Liu et al.
Instruction: Please describe the image in detail.
LLaVA-1.5(Greedy):
The image features a woman wearing a floral dress, walking through a garden with a parasol
in her hand. She appears to be enjoying her time in the garden, possibly posing for a picture.
A handbag can also be seen in the scene, placed close to the woman, possibly belonging to her.
The overall atmosphere of the image is pleasant and inviting, showcasing the beauty of the
garden.
LLaVA-1.5(PAI):
The image features a smiling woman wearing a floral dress, walking through a garden. She is
holding an umbrella in her hand, and appears to be enjoying her time in the garden. The
woman is walking between two large white pillars, which are part of a garden wall. The scene
Input Image is set in a garden, with a potted plant nearby. The woman's smile and the pleasant atmosphere
of the scene suggest a pleasant and relaxing environment.
Instruction: Please describe the image in detail.
LLaVA-1.5(Greedy):
The image features a train with graffiti on its side, traveling down the tracks. The train is
positioned on the left side of the image, and the graffiti is visible on the side of the train,
covering a significant portion of the surface. There is a person standing near the train, possibly
observing or waiting for it to stop. The scene also includes a bench located on the right side of
the image, providing a place for people to sit and wait for the train.
LLaVA-1.5(PAI):
The image features a train with graffiti on its side, which is parked at a train station. The train
Input Image
is blue and white in color, and it is parked on the tracks. The graffiti on the train is quite
elaborate, covering a significant portion of the train's side. The train is parked on the tracks,
and it appears to be a passenger train, as it is not a freight train. The train is parked at a station,
and it is likely waiting for passengers to board. The graffiti on the train adds a unique and
artistic touch to the otherwise ordinary scene.
Instruction: Please describe the image in detail.
LLaVA-1.5(Greedy):
The image depicts a snowy hillside with a herd of sheep grazing on the grass. There are at
least 14 sheep scattered across the field, with some closer to the foreground and others further
away. The sheep are of various sizes, indicating a mix of adult and younger animals. The
scene captures the beauty of the snow-covered landscape and the peacefulness of the animals
as they graze in the wintery environment.
Input Image LLaVA-1.5(PAI):
The image features a large herd of sheep standing in a snowy field. There are at least 14 sheep
in the scene, with some of them standing close to each other, while others are spread out
across the field. The sheep are standing in the snow, and the scene appears to be a peaceful,
snowy landscape. The image captures the beauty of the winter landscape and the calmness of
the sheep in their natural environment.
Instruction: Please describe the image in detail.
LLaVA-1.5(Greedy):
The image features a wooden bench situated in a serene garden setting. The bench is
positioned near a tree, providing a shaded and relaxing spot for visitors. The garden is
adorned with several potted plants, creating a lush and green atmosphere. In addition to the
bench, there are two people in the scene, one standing near the left side of the bench and the
other closer to the right side.The presence of these individuals adds a sense of life and activity
to the otherwise peaceful garden setting.
LLaVA-1.5(PAI):
The image features a wooden park bench situated in a park-like setting, surrounded by a Input Image
variety of trees and plants. The bench is located in the middle of the scene, with a few trees
and bushes in the background. The bench is surrounded by a few bushes and trees, creating a
serene and peaceful atmosphere. The bench is situated in the middle of the scene, with a few
trees and bushes in the background. The bench is made of wood and is placed on a gravel
surface, adding to the overall charm of the scene.
Fig.S2: PAI’s performance on reducing hallucinations of LLaVA-1.5-7B.PAI 23
Instruction: Please describe the image in detail.
Minigpt4(Greedy):
This image shows a busy street with cars parked on both sides of the road. The buildings on
either side of the street are tall and white, with large billboards on the sides. The sky is cloudy
and dark, with a few dark clouds visible in the distance. The street is lined with trees and
streetlights.
Minigpt4(PAI):
This image shows a busy city street with cars parked on the side of the road. The building in
the background is a tall white building with a large billboard on the side. The billboard has a
blue and white design on it. There are also several motorcycles parked on the side of the road.
Input Image
Instruction: Please describe the image in detail.
Minigpt4(Greedy):
This image shows a pizza with various toppings, including peppers, onions, and mushrooms,
on a piece of parchment paper. The pizza appears to be cut into slices, with some slices
missing. There is a knife and fork next to the pizza, and a bottle of tomato sauce on the table
in the background.
Minigpt4(PAI):
This image shows a pizza that has been cut into slices and is sitting on a piece of parchment
paper. The pizza has a variety of toppings, including peppers, onions, and mushrooms, and is
Input Image
covered in a thick layer of cheese. A knife is visible in the foreground, next to the pizza.
Instruction: Please describe the image in detail.
Minigpt4(Greedy):
The image shows a motorcycle parked in a field next to a tent. The tent is set up on a wooden
platform and has a green tarp covering it. The motorcycle is parked on the grass and has a
helmet on the handlebars. There are trees in the background and the sun is setting behind them.
The sky is orange and pink, and there are clouds in the sky.
Minigpt4(PAI):
Input Image This image shows a motorcycle parked in a field next to a tent. The tent is set up on a wooden
platform and has a green tarp covering it. There are trees in the background and the sun is
setting behind them. The motorcycle is parked on the ground next to the tent, and there are no
other objects in the image.
Instruction: Please describe the image in detail.
Minigpt4(Greedy):
This image shows a stop sign on the side of a road, next to a brick wall. The stop sign has a
red background with white letters that read "stop." There is a gate in the wall, with a black
iron fence and a wooden door. The gate is closed, and there are no people or cars in the image.
The trees on the other side of the road are in full bloom, with pink flowers covering the
branches. The sky is clear and blue, with a few clouds in the distance.
Minigpt4(PAI):
This image shows a stop sign on the side of a road, with a gate in the background. The stop
sign has a red background with white letters that read "stop." The gate in the background is
made of metal and has a black finish. There are some leaves on the ground next to the stop
sign, and there are trees in the background. Input Image
Fig.S3: PAI’s performance on reducing hallucinations of Minigpt4.24 Liu et al.
Instruction: Please describe the image in detail.
Shikra(Greedy):
The image features a green and yellow train traveling down the tracks, passing by a station.
The train appears to be a trolley-type train that is either leaving the station or pulling into it.
There are several people around the train and the station, possibly waiting for the train or just
going about their day.
In addition to the train and people, there are a few traffic lights scattered around the scene.
One traffic light is located near the top right corner, another one is positioned further to the
right, and the third traffic light is on the left side of the image. The presence of these traffic
lights suggests that the train is operating in an urban environment with well-organized
transportation infrastructure.
Input Image Shikra(PAI):
The image depicts a green and yellow train traveling down the train tracks, passing through a
small town. The train is moving along the tracks that are situated above a grassy field. The
scene also features a tree-lined sidewalk adjacent to the train tracks.
In the background, there are some pink flowers in bloom, adding a touch of color to the scene.
The train tracks can be seen extending across the image, with the train positioned in the
middle of the frame. The train appears to be a local commuter train, possibly heading towards
a city or town.
Instruction: Please describe the image in detail.
Shikra(Greedy):
The image features a large elephant standing on a lush green field, eating grass using its trunk.
The elephant is positioned in the center of the scene, and it appears to be enjoying a meal.
In the background, there are several chairs scattered around the field, possibly for people to sit
and observe the elephant. Additionally, there are two people in the scene, one near the left
edge of the image and another closer to the center, both of them likely watching the elephant
as it feeds.
Shikra(PAI):
The image features a large elephant standing on a dirt field, eating a mouthful of green grass. Input Image
The elephant is enjoying a meal consisting mainly of grass, as it is eating the grass in its
mouth. The elephant is standing under a bamboo hut, which provides shade and
shelter.\n\nTheimage shows a large elephant standing in a field, eating grass by placing it in
its mouth using its trunk. The scene captures the unique moment of the elephant enjoying its
meal.
Instruction: Please describe the image in detail.
Shikra(Greedy):
The image features a small boat sailing across a large body of water, likely an ocean or a sea,
under a cloudy sky. The boat is speeding through the water, cutting through the waves and
creating a wake behind it.
There are a few people on the boat, with one person visible on the left side and two other
people on the right side of the boat. They seem to be enjoying their time on the water, possibly
engaged in recreational activities or sightseeing.
Input Image
Shikra(PAI):
The image features a small red boat, resembling a red and white ship, speeding across the
water with a large body of waves around it. The boat is moving quickly through the water,
creating a wake in the ocean. The scene captures the excitement of an adventurous boat ride
on the open ocean, providing a thrilling experience for those on board.
Fig.S4: PAI’s performance on reducing hallucinations of Shikra.