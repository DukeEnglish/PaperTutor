[
    {
        "title": "Extended Fiducial Inference: Toward an Automated Process of Statistical Inference",
        "authors": "Faming LiangSehwan KimYan Sun",
        "links": "http://arxiv.org/abs/2407.21622v1",
        "entry_id": "http://arxiv.org/abs/2407.21622v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21622v1",
        "summary": "While fiducial inference was widely considered a big blunder by R.A. Fisher,\nthe goal he initially set --`inferring the uncertainty of model parameters on\nthe basis of observations' -- has been continually pursued by many\nstatisticians. To this end, we develop a new statistical inference method\ncalled extended Fiducial inference (EFI). The new method achieves the goal of\nfiducial inference by leveraging advanced statistical computing techniques\nwhile remaining scalable for big data. EFI involves jointly imputing random\nerrors realized in observations using stochastic gradient Markov chain Monte\nCarlo and estimating the inverse function using a sparse deep neural network\n(DNN). The consistency of the sparse DNN estimator ensures that the uncertainty\nembedded in observations is properly propagated to model parameters through the\nestimated inverse function, thereby validating downstream statistical\ninference. Compared to frequentist and Bayesian methods, EFI offers significant\nadvantages in parameter estimation and hypothesis testing. Specifically, EFI\nprovides higher fidelity in parameter estimation, especially when outliers are\npresent in the observations; and eliminates the need for theoretical reference\ndistributions in hypothesis testing, thereby automating the statistical\ninference process. EFI also provides an innovative framework for\nsemi-supervised learning.",
        "updated": "2024-07-31 14:15:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21622v1"
    },
    {
        "title": "Transient anisotropic kernel for probabilistic learning on manifolds",
        "authors": "Christian SoizeRoger Ghanem",
        "links": "http://arxiv.org/abs/2407.21435v1",
        "entry_id": "http://arxiv.org/abs/2407.21435v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21435v1",
        "summary": "PLoM (Probabilistic Learning on Manifolds) is a method introduced in 2016 for\nhandling small training datasets by projecting an It\\^o equation from a\nstochastic dissipative Hamiltonian dynamical system, acting as the MCMC\ngenerator, for which the KDE-estimated probability measure with the training\ndataset is the invariant measure. PLoM performs a projection on a reduced-order\nvector basis related to the training dataset, using the diffusion maps (DMAPS)\nbasis constructed with a time-independent isotropic kernel. In this paper, we\npropose a new ISDE projection vector basis built from a transient anisotropic\nkernel, providing an alternative to the DMAPS basis to improve statistical\nsurrogates for stochastic manifolds with heterogeneous data. The construction\nensures that for times near the initial time, the DMAPS basis coincides with\nthe transient basis. For larger times, the differences between the two bases\nare characterized by the angle of their spanned vector subspaces. The optimal\ninstant yielding the optimal transient basis is determined using an estimation\nof mutual information from Information Theory, which is normalized by the\nentropy estimation to account for the effects of the number of realizations\nused in the estimations. Consequently, this new vector basis better represents\nstatistical dependencies in the learned probability measure for any dimension.\nThree applications with varying levels of statistical complexity and data\nheterogeneity validate the proposed theory, showing that the transient\nanisotropic kernel improves the learned probability measure.",
        "updated": "2024-07-31 08:38:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21435v1"
    },
    {
        "title": "Cost-Effective Hallucination Detection for LLMs",
        "authors": "Simon ValentinJinmiao FuGianluca DetommasoShaoyuan XuGiovanni ZappellaBryan Wang",
        "links": "http://arxiv.org/abs/2407.21424v1",
        "entry_id": "http://arxiv.org/abs/2407.21424v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21424v1",
        "summary": "Large language models (LLMs) can be prone to hallucinations - generating\nunreliable outputs that are unfaithful to their inputs, external facts or\ninternally inconsistent. In this work, we address several challenges for\npost-hoc hallucination detection in production settings. Our pipeline for\nhallucination detection entails: first, producing a confidence score\nrepresenting the likelihood that a generated answer is a hallucination; second,\ncalibrating the score conditional on attributes of the inputs and candidate\nresponse; finally, performing detection by thresholding the calibrated score.\nWe benchmark a variety of state-of-the-art scoring methods on different\ndatasets, encompassing question answering, fact checking, and summarization\ntasks. We employ diverse LLMs to ensure a comprehensive assessment of\nperformance. We show that calibrating individual scoring methods is critical\nfor ensuring risk-aware downstream decision making. Based on findings that no\nindividual score performs best in all situations, we propose a multi-scoring\nframework, which combines different scores and achieves top performance across\nall datasets. We further introduce cost-effective multi-scoring, which can\nmatch or even outperform more expensive detection methods, while significantly\nreducing computational overhead.",
        "updated": "2024-07-31 08:19:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21424v1"
    },
    {
        "title": "Whitney extension theorems on symmetric spaces, an example",
        "authors": "Birgit SpehPeter Vang Uttenthal",
        "links": "http://arxiv.org/abs/2407.21420v1",
        "entry_id": "http://arxiv.org/abs/2407.21420v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21420v1",
        "summary": "H. Whitney introduced in 1934 the problem of extending a function on a set of\npoints in $\\mathbb{R}^n$ to an analytic function on the ambient space. In this\narticle we prove Whitney type extension theorems for data on some homogeneous\nspaces. We use harmonic analysis on the homogeneous spaces and representation\ntheory of compact as well as noncompact reductive groups.",
        "updated": "2024-07-31 08:09:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21420v1"
    },
    {
        "title": "Two Completely Parameter-Free Alternating Gradient Projection Algorithms for Nonconvex-(strongly) Concave Minimax Problems",
        "authors": "Junnan YangHuiling ZhangZi Xu",
        "links": "http://arxiv.org/abs/2407.21372v1",
        "entry_id": "http://arxiv.org/abs/2407.21372v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21372v1",
        "summary": "Due to their importance in various emerging applications, efficient\nalgorithms for solving minimax problems have recently received increasing\nattention. However, many existing algorithms require prior knowledge of the\nproblem parameters in order to achieve optimal iteration complexity. In this\npaper, we propose a completely parameter-free alternating gradient projection\n(PF-AGP) algorithm to solve the smooth nonconvex-(strongly) concave minimax\nproblems using a backtracking strategy, which does not require prior knowledge\nof parameters such as the Lipschtiz constant $L$ or the strongly concave\nconstant $\\mu$. The PF-AGP algorithm utilizes a parameter-free gradient\nprojection step to alternately update the outer and inner variables in each\niteration. We show that the total number of gradient calls of the PF-AGP\nalgorithm to obtain an $\\varepsilon$-stationary point for nonconvex-strongly\nconcave minimax problems is upper bounded by $\\mathcal{O}\\left(\nL\\kappa^3\\varepsilon^{-2} \\right)$ where $\\kappa$ is the condition number,\nwhile the total number of gradient calls to obtain an $\\varepsilon$-stationary\npoint for nonconvex-concave minimax problems is upper bounded by\n$\\mathcal{O}\\left( L^4\\varepsilon^{-4} \\right)$. As far as we know, this is the\nfirst completely parameter-free algorithm for solving nonconvex-strongly\nconcave minimax problems, and it is also the completely parameter-free\nalgorithm which achieves the best iteration complexity in single loop method\nfor solving nonconvex-concave minimax problems. Numerical results validate the\nefficiency of the proposed PF-AGP algorithm.",
        "updated": "2024-07-31 06:54:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21372v1"
    }
]