[
    {
        "title": "Generalized Out-of-Distribution Detection and Beyond in Vision Language Model Era: A Survey",
        "authors": "Atsuyuki MiyaiJingkang YangJingyang ZhangYifei MingYueqian LinQing YuGo IrieShafiq JotyYixuan LiHai LiZiwei LiuToshihiko YamasakiKiyoharu Aizawa",
        "links": "http://arxiv.org/abs/2407.21794v1",
        "entry_id": "http://arxiv.org/abs/2407.21794v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21794v1",
        "summary": "Detecting out-of-distribution (OOD) samples is crucial for ensuring the\nsafety of machine learning systems and has shaped the field of OOD detection.\nMeanwhile, several other problems are closely related to OOD detection,\nincluding anomaly detection (AD), novelty detection (ND), open set recognition\n(OSR), and outlier detection (OD). To unify these problems, a generalized OOD\ndetection framework was proposed, taxonomically categorizing these five\nproblems. However, Vision Language Models (VLMs) such as CLIP have\nsignificantly changed the paradigm and blurred the boundaries between these\nfields, again confusing researchers. In this survey, we first present a\ngeneralized OOD detection v2, encapsulating the evolution of AD, ND, OSR, OOD\ndetection, and OD in the VLM era. Our framework reveals that, with some field\ninactivity and integration, the demanding challenges have become OOD detection\nand AD. In addition, we also highlight the significant shift in the definition,\nproblem settings, and benchmarks; we thus feature a comprehensive review of the\nmethodology for OOD detection, including the discussion over other related\ntasks to clarify their relationship to OOD detection. Finally, we explore the\nadvancements in the emerging Large Vision Language Model (LVLM) era, such as\nGPT-4V. We conclude this survey with open challenges and future directions.",
        "updated": "2024-07-31 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21794v1"
    },
    {
        "title": "Vision-Language Model Based Handwriting Verification",
        "authors": "Mihir ChauhanAbhishek SatbhaiMohammad Abuzar HashemiMir Basheer AliBina RamamurthyMingchen GaoSiwei LyuSargur Srihari",
        "links": "http://arxiv.org/abs/2407.21788v1",
        "entry_id": "http://arxiv.org/abs/2407.21788v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21788v1",
        "summary": "Handwriting Verification is a critical in document forensics. Deep learning\nbased approaches often face skepticism from forensic document examiners due to\ntheir lack of explainability and reliance on extensive training data and\nhandcrafted features. This paper explores using Vision Language Models (VLMs),\nsuch as OpenAI's GPT-4o and Google's PaliGemma, to address these challenges. By\nleveraging their Visual Question Answering capabilities and 0-shot\nChain-of-Thought (CoT) reasoning, our goal is to provide clear,\nhuman-understandable explanations for model decisions. Our experiments on the\nCEDAR handwriting dataset demonstrate that VLMs offer enhanced\ninterpretability, reduce the need for large training datasets, and adapt better\nto diverse handwriting styles. However, results show that the CNN-based\nResNet-18 architecture outperforms the 0-shot CoT prompt engineering approach\nwith GPT-4o (Accuracy: 70%) and supervised fine-tuned PaliGemma (Accuracy:\n71%), achieving an accuracy of 84% on the CEDAR AND dataset. These findings\nhighlight the potential of VLMs in generating human-interpretable decisions\nwhile underscoring the need for further advancements to match the performance\nof specialized deep learning models.",
        "updated": "2024-07-31 17:57:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21788v1"
    },
    {
        "title": "The Llama 3 Herd of Models",
        "authors": "Abhimanyu DubeyAbhinav JauhriAbhinav PandeyAbhishek KadianAhmad Al-DahleAiesha LetmanAkhil MathurAlan ScheltenAmy YangAngela FanAnirudh GoyalAnthony HartshornAobo YangArchi MitraArchie SravankumarArtem KorenevArthur HinsvarkArun RaoAston ZhangAurelien RodriguezAusten GregersonAva SpataruBaptiste RoziereBethany BironBinh TangBobbie ChernCharlotte CaucheteuxChaya NayakChloe BiChris MarraChris McConnellChristian KellerChristophe TouretChunyang WuCorinne WongCristian Canton FerrerCyrus NikolaidisDamien AllonsiusDaniel SongDanielle PintzDanny LivshitsDavid EsiobuDhruv ChoudharyDhruv MahajanDiego Garcia-OlanoDiego PerinoDieuwke HupkesEgor LakomkinEhab AlBadawyElina LobanovaEmily DinanEric Michael SmithFilip RadenovicFrank ZhangGabriel SynnaeveGabrielle LeeGeorgia Lewis AndersonGraeme NailGregoire MialonGuan PangGuillem CucurellHailey NguyenHannah KorevaarHu XuHugo TouvronIliyan ZarovImanol Arrieta IbarraIsabel KloumannIshan MisraIvan EvtimovJade CopetJaewon LeeJan GeffertJana VranesJason ParkJay MahadeokarJeet ShahJelmer van der LindeJennifer BillockJenny HongJenya LeeJeremy FuJianfeng ChiJianyu HuangJiawen LiuJie WangJiecao YuJoanna BittonJoe SpisakJongsoo ParkJoseph RoccaJoshua JohnstunJoshua SaxeJunteng JiaKalyan Vasuden AlwalaKartikeya UpasaniKate PlawiakKe LiKenneth HeafieldKevin StoneKhalid El-AriniKrithika IyerKshitiz MalikKuenley ChiuKunal BhallaLauren Rantala-YearyLaurens van der MaatenLawrence ChenLiang TanLiz JenkinsLouis MartinLovish MadaanLubo MaloLukas BlecherLukas LandzaatLuke de OliveiraMadeline MuzziMahesh PasupuletiMannat SinghManohar PaluriMarcin KardasMathew OldhamMathieu RitaMaya PavlovaMelanie KambadurMike LewisMin SiMitesh Kumar SinghMona HassanNaman GoyalNarjes TorabiNikolay BashlykovNikolay BogoychevNiladri ChatterjiOlivier DuchenneOnur ÇelebiPatrick AlrassyPengchuan ZhangPengwei LiPetar VasicPeter WengPrajjwal BhargavaPratik DubalPraveen KrishnanPunit Singh KouraPuxin XuQing HeQingxiao DongRagavan SrinivasanRaj GanapathyRamon CaldererRicardo Silveira CabralRobert StojnicRoberta RaileanuRohit GirdharRohit PatelRomain SauvestreRonnie PolidoroRoshan SumbalyRoss TaylorRuan SilvaRui HouRui WangSaghar HosseiniSahana ChennabasappaSanjay SinghSean BellSeohyun Sonia KimSergey EdunovShaoliang NieSharan NarangSharath RaparthySheng ShenShengye WanShruti BhosaleShun ZhangSimon VandenhendeSoumya BatraSpencer WhitmanSten SootlaStephane CollotSuchin GururanganSydney BorodinskyTamar HermanTara FowlerTarek SheashaThomas GeorgiouThomas ScialomTobias SpeckbacherTodor MihaylovTong XiaoUjjwal KarnVedanuj GoswamiVibhor GuptaVignesh RamanathanViktor KerkezVincent GonguetVirginie DoVish VogetiVladan PetrovicWeiwei ChuWenhan XiongWenyin FuWhitney MeersXavier MartinetXiaodong WangXiaoqing Ellen TanXinfeng XieXuchao JiaXuewei WangYaelle GoldschlagYashesh GaurYasmine BabaeiYi WenYiwen SongYuchen ZhangYue LiYuning MaoZacharie Delpierre CoudertZheng YanZhengxing ChenZoe PapakiposAaditya SinghAaron GrattafioriAbha JainAdam KelseyAdam ShajnfeldAdithya GangidiAdolfo VictoriaAhuva GoldstandAjay MenonAjay SharmaAlex BoesenbergAlex VaughanAlexei BaevskiAllie FeinsteinAmanda KalletAmit SanganiAnam YunusAndrei LupuAndres AlvaradoAndrew CaplesAndrew GuAndrew HoAndrew PoultonAndrew RyanAnkit RamchandaniAnnie FrancoAparajita SarafArkabandhu ChowdhuryAshley GabrielAshwin BharambeAssaf EisenmanAzadeh YazdanBeau JamesBen MaurerBenjamin LeonhardiBernie HuangBeth LoydBeto De PaolaBhargavi ParanjapeBing LiuBo WuBoyu NiBraden HancockBram WastiBrandon SpenceBrani StojkovicBrian GamidoBritt MontalvoCarl ParkerCarly BurtonCatalina MejiaChanghan WangChangkyu KimChao ZhouChester HuChing-Hsiang ChuChris CaiChris TindalChristoph FeichtenhoferDamon CivinDana BeatyDaniel KreymerDaniel LiDanny WyattDavid AdkinsDavid XuDavide TestuggineDelia DavidDevi ParikhDiana LiskovichDidem FossDingkang WangDuc LeDustin HollandEdward DowlingEissa JamilElaine MontgomeryEleonora PresaniEmily HahnEmily WoodErik BrinkmanEsteban ArcauteEvan DunbarEvan SmothersFei SunFelix KreukFeng TianFirat OzgenelFrancesco CaggioniFrancisco GuzmánFrank KanayetFrank SeideGabriela Medina FlorezGabriella SchwarzGada BadeerGeorgia SweeGil HalpernGovind ThattaiGrant HermanGrigory SizovGuangyiZhangGuna LakshminarayananHamid ShojanazeriHan ZouHannah WangHanwen ZhaHaroun HabeebHarrison RudolphHelen SukHenry AspegrenHunter GoldmanIgor MolybogIgor TufanovIrina-Elena VelicheItai GatJake WeissmanJames GeboskiJames KohliJaphet AsherJean-Baptiste GayaJeff MarcusJeff TangJennifer ChanJenny ZhenJeremy ReizensteinJeremy TeboulJessica ZhongJian JinJingyi YangJoe CummingsJon CarvillJon ShepardJonathan McPhieJonathan TorresJosh GinsburgJunjie WangKai WuKam Hou UKaran SaxenaKarthik PrasadKartikay KhandelwalKatayoun ZandKathy MatosichKaushik VeeraraghavanKelly MichelenaKeqian LiKun HuangKunal ChawlaKushal LakhotiaKyle HuangLailin ChenLakshya GargLavender ALeandro SilvaLee BellLei ZhangLiangpeng GuoLicheng YuLiron MoshkovichLuca WehrstedtMadian KhabsaManav AvalaniManish BhattMaria TsimpoukelliMartynas MankusMatan HassonMatthew LennieMatthias ResoMaxim GroshevMaxim NaumovMaya LathiMeghan KeneallyMichael L. SeltzerMichal ValkoMichelle RestrepoMihir PatelMik VyatskovMikayel SamvelyanMike ClarkMike MaceyMike WangMiquel Jubert HermosoMo MetanatMohammad RastegariMunish BansalNandhini SanthanamNatascha ParksNatasha WhiteNavyata BawaNayan SinghalNick EgeboNicolas UsunierNikolay Pavlovich LaptevNing DongNing ZhangNorman ChengOleg ChernoguzOlivia HartOmkar SalpekarOzlem KalinliParkin KentParth ParekhPaul SaabPavan BalajiPedro RittnerPhilip BontragerPierre RouxPiotr DollarPolina ZvyaginaPrashant RatanchandaniPritish YuvrajQian LiangRachad AlaoRachel RodriguezRafi AyubRaghotham MurthyRaghu NayaniRahul MitraRaymond LiRebekkah HoganRobin BatteyRocky WangRohan MaheswariRuss HowesRuty RinottSai Jayesh BonduSamyak DattaSara ChughSara HuntSargun DhillonSasha SidorovSatadru PanSaurabh VermaSeiji YamamotoSharadh RamaswamyShaun LindsayShaun LindsaySheng FengShenghao LinShengxin Cindy ZhaShiva ShankarShuqiang ZhangShuqiang ZhangSinong WangSneha AgarwalSoji SajuyigbeSoumith ChintalaStephanie MaxStephen ChenSteve KehoeSteve SatterfieldSudarshan GovindaprasadSumit GuptaSungmin ChoSunny VirkSuraj SubramanianSy ChoudhurySydney GoldmanTal RemezTamar GlaserTamara BestThilo KohlerThomas RobinsonTianhe LiTianjun ZhangTim MatthewsTimothy ChouTzook ShakedVarun VontimittaVictoria AjayiVictoria MontanezVijai MohanVinay Satish KumarVishal ManglaVlad IonescuVlad PoenaruVlad Tiberiu MihailescuVladimir IvanovWei LiWenchen WangWenwen JiangWes BouazizWill ConstableXiaocheng TangXiaofang WangXiaojian WuXiaolan WangXide XiaXilun WuXinbo GaoYanjun ChenYe HuYe JiaYe QiYenda LiYilin ZhangYing ZhangYossi AdiYoungjin NamYuWangYuchen HaoYundi QianYuzi HeZach RaitZachary DeVitoZef RosnbrickZhaoduo WenZhenyu YangZhiwei Zhao",
        "links": "http://arxiv.org/abs/2407.21783v1",
        "entry_id": "http://arxiv.org/abs/2407.21783v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21783v1",
        "summary": "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.",
        "updated": "2024-07-31 17:54:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21783v1"
    },
    {
        "title": "RainMamba: Enhanced Locality Learning with State Space Models for Video Deraining",
        "authors": "Hongtao WuYijun YangHuihui XuWeiming WangJinni ZhouLei Zhu",
        "links": "http://dx.doi.org/10.1145/3664647.3680916",
        "entry_id": "http://arxiv.org/abs/2407.21773v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21773v1",
        "summary": "The outdoor vision systems are frequently contaminated by rain streaks and\nraindrops, which significantly degenerate the performance of visual tasks and\nmultimedia applications. The nature of videos exhibits redundant temporal cues\nfor rain removal with higher stability. Traditional video deraining methods\nheavily rely on optical flow estimation and kernel-based manners, which have a\nlimited receptive field. Yet, transformer architectures, while enabling\nlong-term dependencies, bring about a significant increase in computational\ncomplexity. Recently, the linear-complexity operator of the state space models\n(SSMs) has contrarily facilitated efficient long-term temporal modeling, which\nis crucial for rain streaks and raindrops removal in videos. Unexpectedly, its\nuni-dimensional sequential process on videos destroys the local correlations\nacross the spatio-temporal dimension by distancing adjacent pixels. To address\nthis, we present an improved SSMs-based video deraining network (RainMamba)\nwith a novel Hilbert scanning mechanism to better capture sequence-level local\ninformation. We also introduce a difference-guided dynamic contrastive locality\nlearning strategy to enhance the patch-level self-similarity learning ability\nof the proposed network. Extensive experiments on four synthesized video\nderaining datasets and real-world rainy videos demonstrate the superiority of\nour network in the removal of rain streaks and raindrops.",
        "updated": "2024-07-31 17:48:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21773v1"
    },
    {
        "title": "Paying More Attention to Image: A Training-Free Method for Alleviating Hallucination in LVLMs",
        "authors": "Shi LiuKecheng ZhengWei Chen",
        "links": "http://arxiv.org/abs/2407.21771v1",
        "entry_id": "http://arxiv.org/abs/2407.21771v1",
        "pdf_url": "http://arxiv.org/pdf/2407.21771v1",
        "summary": "Existing Large Vision-Language Models (LVLMs) primarily align image features\nof vision encoder with Large Language Models (LLMs) to leverage their superior\ntext generation capabilities. However, the scale disparity between vision\nencoder and language model may led to LLMs assuming a predominant role in\nmulti-modal comprehension. This imbalance in LVLMs may result in the instances\nof hallucinatory. Concretely, LVLMs may generate consistent descriptions with\nor without visual input, indicating that certain outputs are influenced solely\nby context text. We refer to this phenomenon as \"text inertia.\" To counteract\nthis issue, we introduce a training-free algorithm to find an equilibrium point\nbetween image comprehension and language inference. Specifically, we adaptively\ninvolve adjusting and amplifying the attention weights assigned to image\ntokens, thereby granting greater prominence to visual elements. Meanwhile, we\nsubtract the logits of multi-modal inputs from ones of pure text input, which\ncan help LVLMs be not biased towards LLMs. By enhancing images tokens and\nreducing the stubborn output of LLM, we can let LVLM pay more attention to\nimages, towards alleviating text inertia and reducing the hallucination in\nLVLMs. Our extensive experiments shows that this method substantially reduces\nthe frequency of hallucinatory outputs in various LVLMs in terms of different\nmetrics. Project page is available at https://lalbj.github.io/projects/PAI/.",
        "updated": "2024-07-31 17:46:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.21771v1"
    }
]