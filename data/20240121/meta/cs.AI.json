[
    {
        "title": "ChatQA: Building GPT-4 Level Conversational QA Models",
        "authors": "Zihan LiuWei PingRajarshi RoyPeng XuMohammad ShoeybiBryan Catanzaro",
        "links": "http://arxiv.org/abs/2401.10225v1",
        "entry_id": "http://arxiv.org/abs/2401.10225v1",
        "pdf_url": "http://arxiv.org/pdf/2401.10225v1",
        "summary": "In this work, we introduce ChatQA, a family of conversational question\nanswering (QA) models, that obtain GPT-4 level accuracies. Specifically, we\npropose a two-stage instruction tuning method that can significantly improve\nthe zero-shot conversational QA results from large language models (LLMs). To\nhandle retrieval in conversational QA, we fine-tune a dense retriever on a\nmulti-turn QA dataset, which provides comparable results to using the\nstate-of-the-art query rewriting model while largely reducing deployment cost.\nNotably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10\nconversational QA datasets (54.14 vs. 53.90), without relying on any synthetic\ndata from OpenAI GPT models.",
        "updated": "2024-01-18 18:59:11 UTC",
        "id": 1
    },
    {
        "title": "Supervised Fine-tuning in turn Improves Visual Foundation Models",
        "authors": "Xiaohu JiangYixiao GeYuying GeChun YuanYing Shan",
        "links": "http://arxiv.org/abs/2401.10222v1",
        "entry_id": "http://arxiv.org/abs/2401.10222v1",
        "pdf_url": "http://arxiv.org/pdf/2401.10222v1",
        "summary": "Image-text training like CLIP has dominated the pretraining of vision\nfoundation models in recent years. Subsequent efforts have been made to\nintroduce region-level visual learning into CLIP's pretraining but face\nscalability challenges due to the lack of large-scale region-level datasets.\nDrawing inspiration from supervised fine-tuning (SFT) in natural language\nprocessing such as instruction tuning, we explore the potential of fine-grained\nSFT in enhancing the generation of vision foundation models after their\npretraining. Thus a two-stage method ViSFT (Vision SFT) is proposed to unleash\nthe fine-grained knowledge of vision foundation models. In ViSFT, the vision\nfoundation model is enhanced by performing visual joint learning on some\nin-domain tasks and then tested on out-of-domain benchmarks. With updating\nusing ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over\n4.4B parameters shows improvements across various out-of-domain benchmarks\nincluding vision and vision-linguistic scenarios.",
        "updated": "2024-01-18 18:58:54 UTC",
        "id": 2
    },
    {
        "title": "Eclectic Rule Extraction for Explainability of Deep Neural Network based Intrusion Detection Systems",
        "authors": "Jesse AblesNathaniel ChildersWilliam AndersonSudip MittalShahram RahimiIoana BanicescuMaria Seale",
        "links": "http://arxiv.org/abs/2401.10207v1",
        "entry_id": "http://arxiv.org/abs/2401.10207v1",
        "pdf_url": "http://arxiv.org/pdf/2401.10207v1",
        "summary": "This paper addresses trust issues created from the ubiquity of black box\nalgorithms and surrogate explainers in Explainable Intrusion Detection Systems\n(X-IDS). While Explainable Artificial Intelligence (XAI) aims to enhance\ntransparency, black box surrogate explainers, such as Local Interpretable\nModel-Agnostic Explanation (LIME) and SHapley Additive exPlanation (SHAP), are\ndifficult to trust. The black box nature of these surrogate explainers makes\nthe process behind explanation generation opaque and difficult to understand.\nTo avoid this problem, one can use transparent white box algorithms such as\nRule Extraction (RE). There are three types of RE algorithms: pedagogical,\ndecompositional, and eclectic. Pedagogical methods offer fast but untrustworthy\nwhite-box explanations, while decompositional RE provides trustworthy\nexplanations with poor scalability. This work explores eclectic rule\nextraction, which strikes a balance between scalability and trustworthiness. By\ncombining techniques from pedagogical and decompositional approaches, eclectic\nrule extraction leverages the advantages of both, while mitigating some of\ntheir drawbacks. The proposed Hybrid X-IDS architecture features eclectic RE as\na white box surrogate explainer for black box Deep Neural Networks (DNN). The\npresented eclectic RE algorithm extracts human-readable rules from hidden\nlayers, facilitating explainable and trustworthy rulesets. Evaluations on\nUNSW-NB15 and CIC-IDS-2017 datasets demonstrate the algorithm's ability to\ngenerate rulesets with 99.9% accuracy, mimicking DNN outputs. The contributions\nof this work include the hybrid X-IDS architecture, the eclectic rule\nextraction algorithm applicable to intrusion detection datasets, and a thorough\nanalysis of performance and explainability, demonstrating the trade-offs\ninvolved in rule extraction speed and accuracy.",
        "updated": "2024-01-18 18:45:29 UTC",
        "id": 3
    },
    {
        "title": "Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction",
        "authors": "Qingyun WangZixuan ZhangHongxiang LiXuan LiuJiawei HanHeng JiHuimin Zhao",
        "links": "http://arxiv.org/abs/2401.10189v1",
        "entry_id": "http://arxiv.org/abs/2401.10189v1",
        "pdf_url": "http://arxiv.org/pdf/2401.10189v1",
        "summary": "Fine-grained few-shot entity extraction in the chemical domain faces two\nunique challenges. First, compared with entity extraction tasks in the general\ndomain, sentences from chemical papers usually contain more entities. Moreover,\nentity extraction models usually have difficulty extracting entities of\nlong-tailed types. In this paper, we propose Chem-FINESE, a novel\nsequence-to-sequence (seq2seq) based few-shot entity extraction approach, to\naddress these two challenges. Our Chem-FINESE has two components: a seq2seq\nentity extractor to extract named entities from the input sentence and a\nseq2seq self-validation module to reconstruct the original input sentence from\nextracted entities. Inspired by the fact that a good entity extraction system\nneeds to extract entities faithfully, our new self-validation module leverages\nentity extraction results to reconstruct the original input sentence. Besides,\nwe design a new contrastive loss to reduce excessive copying during the\nextraction process. Finally, we release ChemNER+, a new fine-grained chemical\nentity extraction dataset that is annotated by domain experts with the ChemNER\nschema. Experiments in few-shot settings with both ChemNER+ and CHEMET datasets\nshow that our newly proposed framework has contributed up to 8.26% and 6.84%\nabsolute F1-score gains respectively.",
        "updated": "2024-01-18 18:20:15 UTC",
        "id": 4
    },
    {
        "title": "Neural Echos: Depthwise Convolutional Filters Replicate Biological Receptive Fields",
        "authors": "Zahra BabaieePeyman M. KiasariDaniela RusRadu Grosu",
        "links": "http://arxiv.org/abs/2401.10178v1",
        "entry_id": "http://arxiv.org/abs/2401.10178v1",
        "pdf_url": "http://arxiv.org/pdf/2401.10178v1",
        "summary": "In this study, we present evidence suggesting that depthwise convolutional\nkernels are effectively replicating the structural intricacies of the\nbiological receptive fields observed in the mammalian retina. We provide\nanalytics of trained kernels from various state-of-the-art models\nsubstantiating this evidence. Inspired by this intriguing discovery, we propose\nan initialization scheme that draws inspiration from the biological receptive\nfields. Experimental analysis of the ImageNet dataset with multiple CNN\narchitectures featuring depthwise convolutions reveals a marked enhancement in\nthe accuracy of the learned model when initialized with biologically derived\nweights. This underlies the potential for biologically inspired computational\nmodels to further our understanding of vision processing systems and to improve\nthe efficacy of convolutional networks.",
        "updated": "2024-01-18 18:06:22 UTC",
        "id": 5
    }
]