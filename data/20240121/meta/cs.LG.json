[
    {
        "title": "A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask Inpainting",
        "authors": "Wouter Van GansbekeBert De Brabandere",
        "links": "http://arxiv.org/abs/2401.10227v1",
        "entry_id": "http://arxiv.org/abs/2401.10227v1",
        "pdf_url": "http://arxiv.org/pdf/2401.10227v1",
        "summary": "Panoptic and instance segmentation networks are often trained with\nspecialized object detection modules, complex loss functions, and ad-hoc\npost-processing steps to handle the permutation-invariance of the instance\nmasks. This work builds upon Stable Diffusion and proposes a latent diffusion\napproach for panoptic segmentation, resulting in a simple architecture which\nomits these complexities. Our training process consists of two steps: (1)\ntraining a shallow autoencoder to project the segmentation masks to latent\nspace; (2) training a diffusion model to allow image-conditioned sampling in\nlatent space. The use of a generative model unlocks the exploration of mask\ncompletion or inpainting, which has applications in interactive segmentation.\nThe experimental validation yields promising results for both panoptic\nsegmentation and mask inpainting. While not setting a new state-of-the-art, our\nmodel's simplicity, generality, and mask completion capability are desirable\nproperties.",
        "updated": "2024-01-18 18:59:19 UTC",
        "id": 1
    },
    {
        "title": "ChatQA: Building GPT-4 Level Conversational QA Models",
        "authors": "Zihan LiuWei PingRajarshi RoyPeng XuMohammad ShoeybiBryan Catanzaro",
        "links": "http://arxiv.org/abs/2401.10225v1",
        "entry_id": "http://arxiv.org/abs/2401.10225v1",
        "pdf_url": "http://arxiv.org/pdf/2401.10225v1",
        "summary": "In this work, we introduce ChatQA, a family of conversational question\nanswering (QA) models, that obtain GPT-4 level accuracies. Specifically, we\npropose a two-stage instruction tuning method that can significantly improve\nthe zero-shot conversational QA results from large language models (LLMs). To\nhandle retrieval in conversational QA, we fine-tune a dense retriever on a\nmulti-turn QA dataset, which provides comparable results to using the\nstate-of-the-art query rewriting model while largely reducing deployment cost.\nNotably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10\nconversational QA datasets (54.14 vs. 53.90), without relying on any synthetic\ndata from OpenAI GPT models.",
        "updated": "2024-01-18 18:59:11 UTC",
        "id": 2
    },
    {
        "title": "AutoFT: Robust Fine-Tuning by Optimizing Hyperparameters on OOD Data",
        "authors": "Caroline ChoiYoonho LeeAnnie ChenAllan ZhouAditi RaghunathanChelsea Finn",
        "links": "http://arxiv.org/abs/2401.10220v1",
        "entry_id": "http://arxiv.org/abs/2401.10220v1",
        "pdf_url": "http://arxiv.org/pdf/2401.10220v1",
        "summary": "Foundation models encode rich representations that can be adapted to a\ndesired task by fine-tuning on task-specific data. However, fine-tuning a model\non one particular data distribution often compromises the model's original\nperformance on other distributions. Current methods for robust fine-tuning\nutilize hand-crafted regularization techniques to constrain the fine-tuning\nprocess towards the base foundation model. Yet, it is hard to precisely specify\nwhat characteristics of the foundation model to retain during fine-tuning, as\nthis depends on how the pre-training, fine-tuning, and evaluation data\ndistributions relate to each other. We propose AutoFT, a data-driven approach\nfor guiding foundation model fine-tuning. AutoFT optimizes fine-tuning\nhyperparameters to maximize performance on a small out-of-distribution (OOD)\nvalidation set. To guide fine-tuning in a granular way, AutoFT searches a\nhighly expressive hyperparameter space that includes weight coefficients for\nmany different losses, in addition to learning rate and weight decay values. We\nevaluate AutoFT on nine natural distribution shifts which include domain shifts\nand subpopulation shifts. Our experiments show that AutoFT significantly\nimproves generalization to new OOD data, outperforming existing robust\nfine-tuning methods. Notably, AutoFT achieves new state-of-the-art performance\non the WILDS-iWildCam and WILDS-FMoW benchmarks, outperforming the previous\nbest methods by $6.0\\%$ and $1.5\\%$, respectively.",
        "updated": "2024-01-18 18:58:49 UTC",
        "id": 3
    },
    {
        "title": "Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt Tensor Products",
        "authors": "Shengjie LuoTianlang ChenAditi S. Krishnapriyan",
        "links": "http://arxiv.org/abs/2401.10216v1",
        "entry_id": "http://arxiv.org/abs/2401.10216v1",
        "pdf_url": "http://arxiv.org/pdf/2401.10216v1",
        "summary": "Developing equivariant neural networks for the E(3) group plays an important\nrole in modeling 3D data across real-world applications. Enforcing this\nequivariance primarily involves the tensor products of irreducible\nrepresentations (irreps). However, the computational complexity of such\noperations increases significantly as higher-order tensors are used. In this\nwork, we propose a systematic approach to substantially accelerate the\ncomputation of the tensor products of irreps. We mathematically connect the\ncommonly used Clebsch-Gordan coefficients to the Gaunt coefficients, which are\nintegrals of products of three spherical harmonics. Through Gaunt coefficients,\nthe tensor product of irreps becomes equivalent to the multiplication between\nspherical functions represented by spherical harmonics. This perspective\nfurther allows us to change the basis for the equivariant operations from\nspherical harmonics to a 2D Fourier basis. Consequently, the multiplication\nbetween spherical functions represented by a 2D Fourier basis can be\nefficiently computed via the convolution theorem and Fast Fourier Transforms.\nThis transformation reduces the complexity of full tensor products of irreps\nfrom $\\mathcal{O}(L^6)$ to $\\mathcal{O}(L^3)$, where $L$ is the max degree of\nirreps. Leveraging this approach, we introduce the Gaunt Tensor Product, which\nserves as a new method to construct efficient equivariant operations across\ndifferent model architectures. Our experiments on the Open Catalyst Project and\n3BPA datasets demonstrate both the increased efficiency and improved\nperformance of our approach.",
        "updated": "2024-01-18 18:57:10 UTC",
        "id": 4
    },
    {
        "title": "Eclectic Rule Extraction for Explainability of Deep Neural Network based Intrusion Detection Systems",
        "authors": "Jesse AblesNathaniel ChildersWilliam AndersonSudip MittalShahram RahimiIoana BanicescuMaria Seale",
        "links": "http://arxiv.org/abs/2401.10207v1",
        "entry_id": "http://arxiv.org/abs/2401.10207v1",
        "pdf_url": "http://arxiv.org/pdf/2401.10207v1",
        "summary": "This paper addresses trust issues created from the ubiquity of black box\nalgorithms and surrogate explainers in Explainable Intrusion Detection Systems\n(X-IDS). While Explainable Artificial Intelligence (XAI) aims to enhance\ntransparency, black box surrogate explainers, such as Local Interpretable\nModel-Agnostic Explanation (LIME) and SHapley Additive exPlanation (SHAP), are\ndifficult to trust. The black box nature of these surrogate explainers makes\nthe process behind explanation generation opaque and difficult to understand.\nTo avoid this problem, one can use transparent white box algorithms such as\nRule Extraction (RE). There are three types of RE algorithms: pedagogical,\ndecompositional, and eclectic. Pedagogical methods offer fast but untrustworthy\nwhite-box explanations, while decompositional RE provides trustworthy\nexplanations with poor scalability. This work explores eclectic rule\nextraction, which strikes a balance between scalability and trustworthiness. By\ncombining techniques from pedagogical and decompositional approaches, eclectic\nrule extraction leverages the advantages of both, while mitigating some of\ntheir drawbacks. The proposed Hybrid X-IDS architecture features eclectic RE as\na white box surrogate explainer for black box Deep Neural Networks (DNN). The\npresented eclectic RE algorithm extracts human-readable rules from hidden\nlayers, facilitating explainable and trustworthy rulesets. Evaluations on\nUNSW-NB15 and CIC-IDS-2017 datasets demonstrate the algorithm's ability to\ngenerate rulesets with 99.9% accuracy, mimicking DNN outputs. The contributions\nof this work include the hybrid X-IDS architecture, the eclectic rule\nextraction algorithm applicable to intrusion detection datasets, and a thorough\nanalysis of performance and explainability, demonstrating the trade-offs\ninvolved in rule extraction speed and accuracy.",
        "updated": "2024-01-18 18:45:29 UTC",
        "id": 5
    }
]