[
    {
        "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards",
        "authors": "Haoxiang WangYong LinWei XiongRui YangShizhe DiaoShuang QiuHan ZhaoTong Zhang",
        "links": "http://arxiv.org/abs/2402.18571v2",
        "entry_id": "http://arxiv.org/abs/2402.18571v2",
        "pdf_url": "http://arxiv.org/pdf/2402.18571v2",
        "summary": "Fine-grained control over large language models (LLMs) remains a significant\nchallenge, hindering their adaptability to diverse user needs. While\nReinforcement Learning from Human Feedback (RLHF) shows promise in aligning\nLLMs, its reliance on scalar rewards often limits its ability to capture\ndiverse user preferences in real-world applications. To address this\nlimitation, we introduce the Directional Preference Alignment (DPA) framework.\nUnlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling\nto represent diverse preference profiles. Additionally, DPA models user\npreferences as directions (i.e., unit vectors) in the reward space to achieve\nuser-dependent preference control. Our method involves training a\nmulti-objective reward model and then fine-tuning the LLM with a\npreference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF\nmethod adopted by Llama 2. This method enjoys a better performance trade-off\nacross various reward objectives. In comparison with the scalar-reward RLHF,\nDPA offers users intuitive control over LLM generation: they can arithmetically\nspecify their desired trade-offs (e.g., more helpfulness with less verbosity).\nWe also validate the effectiveness of DPA with real-world alignment experiments\non Mistral-7B. Our method provides straightforward arithmetic control over the\ntrade-off between helpfulness and verbosity while maintaining competitive\nperformance with strong baselines such as Direct Preference Optimization (DPO).",
        "updated": "2024-02-29 04:33:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.18571v2"
    },
    {
        "title": "Implicit Bias of Next-Token Prediction",
        "authors": "Christos Thrampoulidis",
        "links": "http://arxiv.org/abs/2402.18551v1",
        "entry_id": "http://arxiv.org/abs/2402.18551v1",
        "pdf_url": "http://arxiv.org/pdf/2402.18551v1",
        "summary": "Next-token prediction (NTP), the go-to training paradigm in training large\nlanguage models, involves predicting the next token in a sequence. Departing\nfrom traditional one-hot classification, in NTP, multiple tokens with varying\nfrequencies follow each given context. This work frames NTP training as\ncross-entropy minimization over distinct contexts, each associated with a\nsparse empirical probability vector across a finite vocabulary. It then\naddresses the following question: do gradient-based optimizers exhibit a bias\ntowards solutions with specific structure as the NTP training loss reaches its\nlower bound (entropy)? Specifically, for linear NTP models trained using\ngradient descent (GD), we make the following contributions: Firstly, we\ndetermine NTP-separability conditions on the data, under which GD can attain\nits lower bound. We also demonstrate that these conditions hold under\noverparameterization. Secondly, we establish that the parameters of GD\nprojected onto an appropriate data subspace converge to the unique solution of\na system of linear equations, which requires the logits' difference of\nin-support tokens to be equal to the log-ratio of their respective\nprobabilities. Meanwhile, on the orthogonal subspace, the parameters diverge\nand converge in the direction of the solution of a max-margin quadratic\nprogram, minimizing the Euclidean norm of parameters satisfying the\n\\NTP-separability conditions. Akin to prior research on implicit bias of\none-hot classification, our work opens exciting avenues for future research\nthat can lead to better understanding optimization, generalization and\nrobustness principles of models trained with NTP.",
        "updated": "2024-02-28 18:34:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.18551v1"
    },
    {
        "title": "RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval",
        "authors": "Kaiyue WenXingyu DangKaifeng Lyu",
        "links": "http://arxiv.org/abs/2402.18510v2",
        "entry_id": "http://arxiv.org/abs/2402.18510v2",
        "pdf_url": "http://arxiv.org/pdf/2402.18510v2",
        "summary": "This paper investigates the gap in representation powers of Recurrent Neural\nNetworks (RNNs) and Transformers in the context of solving algorithmic\nproblems. We focus on understanding whether RNNs, known for their memory\nefficiency in handling long sequences, can match the performance of\nTransformers, particularly when enhanced with Chain-of-Thought (CoT) prompting.\nOur theoretical analysis reveals that CoT improves RNNs but is insufficient to\nclose the gap with Transformers. A key bottleneck lies in the inability of RNNs\nto perfectly retrieve information from the context, even with CoT: for several\ntasks that explicitly or implicitly require this capability, such as\nassociative recall and determining if a graph is a tree, we prove that RNNs are\nnot expressive enough to solve the tasks while Transformers can solve them with\nease. Conversely, we prove that adopting techniques to enhance the in-context\nretrieval capability of RNNs, including Retrieval-Augmented Generation (RAG)\nand adding a single Transformer layer, can elevate RNNs to be capable of\nsolving all polynomial-time solvable problems with CoT, hence closing the\nrepresentation gap with Transformers.",
        "updated": "2024-02-29 07:06:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.18510v2"
    },
    {
        "title": "Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes",
        "authors": "Georg MantenCecilia CasoloEmilio FerrucciSøren Wengel MogensenCristopher SalviNiki Kilbertus",
        "links": "http://arxiv.org/abs/2402.18477v1",
        "entry_id": "http://arxiv.org/abs/2402.18477v1",
        "pdf_url": "http://arxiv.org/pdf/2402.18477v1",
        "summary": "Inferring the causal structure underlying stochastic dynamical systems from\nobservational data holds great promise in domains ranging from science and\nhealth to finance. Such processes can often be accurately modeled via\nstochastic differential equations (SDEs), which naturally imply causal\nrelationships via \"which variables enter the differential of which other\nvariables\". In this paper, we develop a kernel-based test of conditional\nindependence (CI) on \"path-space\" -- solutions to SDEs -- by leveraging recent\nadvances in signature kernels. We demonstrate strictly superior performance of\nour proposed CI test compared to existing approaches on path-space. Then, we\ndevelop constraint-based causal discovery algorithms for acyclic stochastic\ndynamical systems (allowing for loops) that leverage temporal information to\nrecover the entire directed graph. Assuming faithfulness and a CI oracle, our\nalgorithm is sound and complete. We empirically verify that our developed CI\ntest in conjunction with the causal discovery algorithm reliably outperforms\nbaselines across a range of settings.",
        "updated": "2024-02-28 16:58:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.18477v1"
    },
    {
        "title": "Unveiling the Potential of Robustness in Evaluating Causal Inference Models",
        "authors": "Yiyan HuangCheuk Hang LeungSiyi WangYijun LiQi Wu",
        "links": "http://arxiv.org/abs/2402.18392v1",
        "entry_id": "http://arxiv.org/abs/2402.18392v1",
        "pdf_url": "http://arxiv.org/pdf/2402.18392v1",
        "summary": "The growing demand for personalized decision-making has led to a surge of\ninterest in estimating the Conditional Average Treatment Effect (CATE). The\nintersection of machine learning and causal inference has yielded various\neffective CATE estimators. However, deploying these estimators in practice is\noften hindered by the absence of counterfactual labels, making it challenging\nto select the desirable CATE estimator using conventional model selection\nprocedures like cross-validation. Existing approaches for CATE estimator\nselection, such as plug-in and pseudo-outcome metrics, face two inherent\nchallenges. Firstly, they are required to determine the metric form and the\nunderlying machine learning models for fitting nuisance parameters or plug-in\nlearners. Secondly, they lack a specific focus on selecting a robust estimator.\nTo address these challenges, this paper introduces a novel approach, the\nDistributionally Robust Metric (DRM), for CATE estimator selection. The\nproposed DRM not only eliminates the need to fit additional models but also\nexcels at selecting a robust CATE estimator. Experimental studies demonstrate\nthe efficacy of the DRM method, showcasing its consistent effectiveness in\nidentifying superior estimators while mitigating the risk of selecting inferior\nones.",
        "updated": "2024-02-28 15:12:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.18392v1"
    }
]