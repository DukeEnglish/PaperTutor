[
    {
        "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards",
        "authors": "Haoxiang WangYong LinWei XiongRui YangShizhe DiaoShuang QiuHan ZhaoTong Zhang",
        "links": "http://arxiv.org/abs/2402.18571v2",
        "entry_id": "http://arxiv.org/abs/2402.18571v2",
        "pdf_url": "http://arxiv.org/pdf/2402.18571v2",
        "summary": "Fine-grained control over large language models (LLMs) remains a significant\nchallenge, hindering their adaptability to diverse user needs. While\nReinforcement Learning from Human Feedback (RLHF) shows promise in aligning\nLLMs, its reliance on scalar rewards often limits its ability to capture\ndiverse user preferences in real-world applications. To address this\nlimitation, we introduce the Directional Preference Alignment (DPA) framework.\nUnlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling\nto represent diverse preference profiles. Additionally, DPA models user\npreferences as directions (i.e., unit vectors) in the reward space to achieve\nuser-dependent preference control. Our method involves training a\nmulti-objective reward model and then fine-tuning the LLM with a\npreference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF\nmethod adopted by Llama 2. This method enjoys a better performance trade-off\nacross various reward objectives. In comparison with the scalar-reward RLHF,\nDPA offers users intuitive control over LLM generation: they can arithmetically\nspecify their desired trade-offs (e.g., more helpfulness with less verbosity).\nWe also validate the effectiveness of DPA with real-world alignment experiments\non Mistral-7B. Our method provides straightforward arithmetic control over the\ntrade-off between helpfulness and verbosity while maintaining competitive\nperformance with strong baselines such as Direct Preference Optimization (DPO).",
        "updated": "2024-02-29 04:33:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.18571v2"
    },
    {
        "title": "Approaching Human-Level Forecasting with Language Models",
        "authors": "Danny HalawiFred ZhangChen Yueh-HanJacob Steinhardt",
        "links": "http://arxiv.org/abs/2402.18563v1",
        "entry_id": "http://arxiv.org/abs/2402.18563v1",
        "pdf_url": "http://arxiv.org/pdf/2402.18563v1",
        "summary": "Forecasting future events is important for policy and decision making. In\nthis work, we study whether language models (LMs) can forecast at the level of\ncompetitive human forecasters. Towards this goal, we develop a\nretrieval-augmented LM system designed to automatically search for relevant\ninformation, generate forecasts, and aggregate predictions. To facilitate our\nstudy, we collect a large dataset of questions from competitive forecasting\nplatforms. Under a test set published after the knowledge cut-offs of our LMs,\nwe evaluate the end-to-end performance of our system against the aggregates of\nhuman forecasts. On average, the system nears the crowd aggregate of\ncompetitive forecasters, and in some settings surpasses it. Our work suggests\nthat using LMs to forecast the future could provide accurate predictions at\nscale and help to inform institutional decision making.",
        "updated": "2024-02-28 18:54:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.18563v1"
    },
    {
        "title": "Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates",
        "authors": "Kaifeng LyuHaoyu ZhaoXinran GuDingli YuAnirudh GoyalSanjeev Arora",
        "links": "http://arxiv.org/abs/2402.18540v1",
        "entry_id": "http://arxiv.org/abs/2402.18540v1",
        "pdf_url": "http://arxiv.org/pdf/2402.18540v1",
        "summary": "Public LLMs such as the Llama 2-Chat have driven huge activity in LLM\nresearch. These models underwent alignment training and were considered safe.\nRecently Qi et al. (2023) reported that even benign fine-tuning (e.g., on\nseemingly safe datasets) can give rise to unsafe behaviors in the models. The\ncurrent paper is about methods and best practices to mitigate such loss of\nalignment. Through extensive experiments on several chat models (Meta's Llama\n2-Chat, Mistral AI's Mistral 7B Instruct v0.2, and OpenAI's GPT-3.5 Turbo),\nthis paper uncovers that the prompt templates used during fine-tuning and\ninference play a crucial role in preserving safety alignment, and proposes the\n\"Pure Tuning, Safe Testing\" (PTST) principle -- fine-tune models without a\nsafety prompt, but include it at test time. Fine-tuning experiments on GSM8K,\nChatDoctor, and OpenOrca show that PTST significantly reduces the rise of\nunsafe behaviors, and even almost eliminates them in some cases.",
        "updated": "2024-02-28 18:23:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.18540v1"
    },
    {
        "title": "Language Models Represent Beliefs of Self and Others",
        "authors": "Wentao ZhuZhining ZhangYizhou Wang",
        "links": "http://arxiv.org/abs/2402.18496v2",
        "entry_id": "http://arxiv.org/abs/2402.18496v2",
        "pdf_url": "http://arxiv.org/pdf/2402.18496v2",
        "summary": "Understanding and attributing mental states, known as Theory of Mind (ToM),\nemerges as a fundamental capability for human social reasoning. While Large\nLanguage Models (LLMs) appear to possess certain ToM abilities, the mechanisms\nunderlying these capabilities remain elusive. In this study, we discover that\nit is possible to linearly decode the belief status from the perspectives of\nvarious agents through neural activations of language models, indicating the\nexistence of internal representations of self and others' beliefs. By\nmanipulating these representations, we observe dramatic changes in the models'\nToM performance, underscoring their pivotal role in the social reasoning\nprocess. Additionally, our findings extend to diverse social reasoning tasks\nthat involve different causal inference patterns, suggesting the potential\ngeneralizability of these representations.",
        "updated": "2024-02-29 13:22:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.18496v2"
    },
    {
        "title": "Human-Centric Aware UAV Trajectory Planning in Search and Rescue Missions Employing Multi-Objective Reinforcement Learning with AHP and Similarity-Based Experience Replay",
        "authors": "Mahya RamezaniJose Luis Sanchez-Lopez",
        "links": "http://arxiv.org/abs/2402.18487v1",
        "entry_id": "http://arxiv.org/abs/2402.18487v1",
        "pdf_url": "http://arxiv.org/pdf/2402.18487v1",
        "summary": "The integration of Unmanned Aerial Vehicles (UAVs) into Search and Rescue\n(SAR) missions presents a promising avenue for enhancing operational efficiency\nand effectiveness. However, the success of these missions is not solely\ndependent on the technical capabilities of the drones but also on their\nacceptance and interaction with humans on the ground. This paper explores the\neffect of human-centric factor in UAV trajectory planning for SAR missions. We\nintroduce a novel approach based on the reinforcement learning augmented with\nAnalytic Hierarchy Process and novel similarity-based experience replay to\noptimize UAV trajectories, balancing operational objectives with human comfort\nand safety considerations. Additionally, through a comprehensive survey, we\ninvestigate the impact of gender cues and anthropomorphism in UAV design on\npublic acceptance and trust, revealing significant implications for drone\ninteraction strategies in SAR. Our contributions include (1) a reinforcement\nlearning framework for UAV trajectory planning that dynamically integrates\nmulti-objective considerations, (2) an analysis of human perceptions towards\ngendered and anthropomorphized drones in SAR contexts, and (3) the application\nof similarity-based experience replay for enhanced learning efficiency in\ncomplex SAR scenarios. The findings offer valuable insights into designing UAV\nsystems that are not only technically proficient but also aligned with\nhuman-centric values.",
        "updated": "2024-02-28 17:10:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.18487v1"
    }
]