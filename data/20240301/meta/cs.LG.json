[
    {
        "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards",
        "authors": "Haoxiang WangYong LinWei XiongRui YangShizhe DiaoShuang QiuHan ZhaoTong Zhang",
        "links": "http://arxiv.org/abs/2402.18571v2",
        "entry_id": "http://arxiv.org/abs/2402.18571v2",
        "pdf_url": "http://arxiv.org/pdf/2402.18571v2",
        "summary": "Fine-grained control over large language models (LLMs) remains a significant\nchallenge, hindering their adaptability to diverse user needs. While\nReinforcement Learning from Human Feedback (RLHF) shows promise in aligning\nLLMs, its reliance on scalar rewards often limits its ability to capture\ndiverse user preferences in real-world applications. To address this\nlimitation, we introduce the Directional Preference Alignment (DPA) framework.\nUnlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling\nto represent diverse preference profiles. Additionally, DPA models user\npreferences as directions (i.e., unit vectors) in the reward space to achieve\nuser-dependent preference control. Our method involves training a\nmulti-objective reward model and then fine-tuning the LLM with a\npreference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF\nmethod adopted by Llama 2. This method enjoys a better performance trade-off\nacross various reward objectives. In comparison with the scalar-reward RLHF,\nDPA offers users intuitive control over LLM generation: they can arithmetically\nspecify their desired trade-offs (e.g., more helpfulness with less verbosity).\nWe also validate the effectiveness of DPA with real-world alignment experiments\non Mistral-7B. Our method provides straightforward arithmetic control over the\ntrade-off between helpfulness and verbosity while maintaining competitive\nperformance with strong baselines such as Direct Preference Optimization (DPO).",
        "updated": "2024-02-29 04:33:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.18571v2"
    },
    {
        "title": "Diffusion Language Models Are Versatile Protein Learners",
        "authors": "Xinyou WangZaixiang ZhengFei YeDongyu XueShujian HuangQuanquan Gu",
        "links": "http://arxiv.org/abs/2402.18567v1",
        "entry_id": "http://arxiv.org/abs/2402.18567v1",
        "pdf_url": "http://arxiv.org/pdf/2402.18567v1",
        "summary": "This paper introduces diffusion protein language model (DPLM), a versatile\nprotein language model that demonstrates strong generative and predictive\ncapabilities for protein sequences. We first pre-train scalable DPLMs from\nevolutionary-scale protein sequences within a generative self-supervised\ndiscrete diffusion probabilistic framework, which generalizes language modeling\nfor proteins in a principled way. After pre-training, DPLM exhibits the ability\nto generate structurally plausible, novel, and diverse protein sequences for\nunconditional generation. We further demonstrate the proposed diffusion\ngenerative pre-training makes DPLM possess a better understanding of proteins,\nmaking it a superior representation learner, which can be fine-tuned for\nvarious predictive tasks, comparing favorably to ESM2 (Lin et al., 2022).\nMoreover, DPLM can be tailored for various needs, which showcases its prowess\nof conditional generation in several ways: (1) conditioning on partial peptide\nsequences, e.g., generating scaffolds for functional motifs with high success\nrate; (2) incorporating other modalities as conditioner, e.g.,\nstructure-conditioned generation for inverse folding; and (3) steering sequence\ngeneration towards desired properties, e.g., satisfying specified secondary\nstructures, through a plug-and-play classifier guidance.",
        "updated": "2024-02-28 18:57:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.18567v1"
    },
    {
        "title": "Approaching Human-Level Forecasting with Language Models",
        "authors": "Danny HalawiFred ZhangChen Yueh-HanJacob Steinhardt",
        "links": "http://arxiv.org/abs/2402.18563v1",
        "entry_id": "http://arxiv.org/abs/2402.18563v1",
        "pdf_url": "http://arxiv.org/pdf/2402.18563v1",
        "summary": "Forecasting future events is important for policy and decision making. In\nthis work, we study whether language models (LMs) can forecast at the level of\ncompetitive human forecasters. Towards this goal, we develop a\nretrieval-augmented LM system designed to automatically search for relevant\ninformation, generate forecasts, and aggregate predictions. To facilitate our\nstudy, we collect a large dataset of questions from competitive forecasting\nplatforms. Under a test set published after the knowledge cut-offs of our LMs,\nwe evaluate the end-to-end performance of our system against the aggregates of\nhuman forecasts. On average, the system nears the crowd aggregate of\ncompetitive forecasters, and in some settings surpasses it. Our work suggests\nthat using LMs to forecast the future could provide accurate predictions at\nscale and help to inform institutional decision making.",
        "updated": "2024-02-28 18:54:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.18563v1"
    },
    {
        "title": "Implicit Bias of Next-Token Prediction",
        "authors": "Christos Thrampoulidis",
        "links": "http://arxiv.org/abs/2402.18551v1",
        "entry_id": "http://arxiv.org/abs/2402.18551v1",
        "pdf_url": "http://arxiv.org/pdf/2402.18551v1",
        "summary": "Next-token prediction (NTP), the go-to training paradigm in training large\nlanguage models, involves predicting the next token in a sequence. Departing\nfrom traditional one-hot classification, in NTP, multiple tokens with varying\nfrequencies follow each given context. This work frames NTP training as\ncross-entropy minimization over distinct contexts, each associated with a\nsparse empirical probability vector across a finite vocabulary. It then\naddresses the following question: do gradient-based optimizers exhibit a bias\ntowards solutions with specific structure as the NTP training loss reaches its\nlower bound (entropy)? Specifically, for linear NTP models trained using\ngradient descent (GD), we make the following contributions: Firstly, we\ndetermine NTP-separability conditions on the data, under which GD can attain\nits lower bound. We also demonstrate that these conditions hold under\noverparameterization. Secondly, we establish that the parameters of GD\nprojected onto an appropriate data subspace converge to the unique solution of\na system of linear equations, which requires the logits' difference of\nin-support tokens to be equal to the log-ratio of their respective\nprobabilities. Meanwhile, on the orthogonal subspace, the parameters diverge\nand converge in the direction of the solution of a max-margin quadratic\nprogram, minimizing the Euclidean norm of parameters satisfying the\n\\NTP-separability conditions. Akin to prior research on implicit bias of\none-hot classification, our work opens exciting avenues for future research\nthat can lead to better understanding optimization, generalization and\nrobustness principles of models trained with NTP.",
        "updated": "2024-02-28 18:34:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.18551v1"
    },
    {
        "title": "Generalizability Under Sensor Failure: Tokenization + Transformers Enable More Robust Latent Spaces",
        "authors": "Geeling ChauYujin AnAhamed Raffey IqbalSoon-Jo ChungYisong YueSabera Talukder",
        "links": "http://arxiv.org/abs/2402.18546v2",
        "entry_id": "http://arxiv.org/abs/2402.18546v2",
        "pdf_url": "http://arxiv.org/pdf/2402.18546v2",
        "summary": "A major goal in neuroscience is to discover neural data representations that\ngeneralize. This goal is challenged by variability along recording sessions\n(e.g. environment), subjects (e.g. varying neural structures), and sensors\n(e.g. sensor noise), among others. Recent work has begun to address\ngeneralization across sessions and subjects, but few study robustness to sensor\nfailure which is highly prevalent in neuroscience experiments. In order to\naddress these generalizability dimensions we first collect our own\nelectroencephalography dataset with numerous sessions, subjects, and sensors,\nthen study two time series models: EEGNet (Lawhern et al., 2018) and TOTEM\n(Talukder et al., 2024). EEGNet is a widely used convolutional neural network,\nwhile TOTEM is a discrete time series tokenizer and transformer model. We find\nthat TOTEM outperforms or matches EEGNet across all generalizability cases.\nFinally through analysis of TOTEM's latent codebook we observe that\ntokenization enables generalization",
        "updated": "2024-02-29 18:35:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.18546v2"
    }
]