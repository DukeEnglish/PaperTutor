[
    {
        "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards",
        "authors": "Haoxiang WangYong LinWei XiongRui YangShizhe DiaoShuang QiuHan ZhaoTong Zhang",
        "links": "http://arxiv.org/abs/2402.18571v2",
        "entry_id": "http://arxiv.org/abs/2402.18571v2",
        "pdf_url": "http://arxiv.org/pdf/2402.18571v2",
        "summary": "Fine-grained control over large language models (LLMs) remains a significant\nchallenge, hindering their adaptability to diverse user needs. While\nReinforcement Learning from Human Feedback (RLHF) shows promise in aligning\nLLMs, its reliance on scalar rewards often limits its ability to capture\ndiverse user preferences in real-world applications. To address this\nlimitation, we introduce the Directional Preference Alignment (DPA) framework.\nUnlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling\nto represent diverse preference profiles. Additionally, DPA models user\npreferences as directions (i.e., unit vectors) in the reward space to achieve\nuser-dependent preference control. Our method involves training a\nmulti-objective reward model and then fine-tuning the LLM with a\npreference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF\nmethod adopted by Llama 2. This method enjoys a better performance trade-off\nacross various reward objectives. In comparison with the scalar-reward RLHF,\nDPA offers users intuitive control over LLM generation: they can arithmetically\nspecify their desired trade-offs (e.g., more helpfulness with less verbosity).\nWe also validate the effectiveness of DPA with real-world alignment experiments\non Mistral-7B. Our method provides straightforward arithmetic control over the\ntrade-off between helpfulness and verbosity while maintaining competitive\nperformance with strong baselines such as Direct Preference Optimization (DPO).",
        "updated": "2024-02-29 04:33:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.18571v2"
    },
    {
        "title": "Approaching Human-Level Forecasting with Language Models",
        "authors": "Danny HalawiFred ZhangChen Yueh-HanJacob Steinhardt",
        "links": "http://arxiv.org/abs/2402.18563v1",
        "entry_id": "http://arxiv.org/abs/2402.18563v1",
        "pdf_url": "http://arxiv.org/pdf/2402.18563v1",
        "summary": "Forecasting future events is important for policy and decision making. In\nthis work, we study whether language models (LMs) can forecast at the level of\ncompetitive human forecasters. Towards this goal, we develop a\nretrieval-augmented LM system designed to automatically search for relevant\ninformation, generate forecasts, and aggregate predictions. To facilitate our\nstudy, we collect a large dataset of questions from competitive forecasting\nplatforms. Under a test set published after the knowledge cut-offs of our LMs,\nwe evaluate the end-to-end performance of our system against the aggregates of\nhuman forecasts. On average, the system nears the crowd aggregate of\ncompetitive forecasters, and in some settings surpasses it. Our work suggests\nthat using LMs to forecast the future could provide accurate predictions at\nscale and help to inform institutional decision making.",
        "updated": "2024-02-28 18:54:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.18563v1"
    },
    {
        "title": "Implicit Bias of Next-Token Prediction",
        "authors": "Christos Thrampoulidis",
        "links": "http://arxiv.org/abs/2402.18551v1",
        "entry_id": "http://arxiv.org/abs/2402.18551v1",
        "pdf_url": "http://arxiv.org/pdf/2402.18551v1",
        "summary": "Next-token prediction (NTP), the go-to training paradigm in training large\nlanguage models, involves predicting the next token in a sequence. Departing\nfrom traditional one-hot classification, in NTP, multiple tokens with varying\nfrequencies follow each given context. This work frames NTP training as\ncross-entropy minimization over distinct contexts, each associated with a\nsparse empirical probability vector across a finite vocabulary. It then\naddresses the following question: do gradient-based optimizers exhibit a bias\ntowards solutions with specific structure as the NTP training loss reaches its\nlower bound (entropy)? Specifically, for linear NTP models trained using\ngradient descent (GD), we make the following contributions: Firstly, we\ndetermine NTP-separability conditions on the data, under which GD can attain\nits lower bound. We also demonstrate that these conditions hold under\noverparameterization. Secondly, we establish that the parameters of GD\nprojected onto an appropriate data subspace converge to the unique solution of\na system of linear equations, which requires the logits' difference of\nin-support tokens to be equal to the log-ratio of their respective\nprobabilities. Meanwhile, on the orthogonal subspace, the parameters diverge\nand converge in the direction of the solution of a max-margin quadratic\nprogram, minimizing the Euclidean norm of parameters satisfying the\n\\NTP-separability conditions. Akin to prior research on implicit bias of\none-hot classification, our work opens exciting avenues for future research\nthat can lead to better understanding optimization, generalization and\nrobustness principles of models trained with NTP.",
        "updated": "2024-02-28 18:34:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.18551v1"
    },
    {
        "title": "Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates",
        "authors": "Kaifeng LyuHaoyu ZhaoXinran GuDingli YuAnirudh GoyalSanjeev Arora",
        "links": "http://arxiv.org/abs/2402.18540v1",
        "entry_id": "http://arxiv.org/abs/2402.18540v1",
        "pdf_url": "http://arxiv.org/pdf/2402.18540v1",
        "summary": "Public LLMs such as the Llama 2-Chat have driven huge activity in LLM\nresearch. These models underwent alignment training and were considered safe.\nRecently Qi et al. (2023) reported that even benign fine-tuning (e.g., on\nseemingly safe datasets) can give rise to unsafe behaviors in the models. The\ncurrent paper is about methods and best practices to mitigate such loss of\nalignment. Through extensive experiments on several chat models (Meta's Llama\n2-Chat, Mistral AI's Mistral 7B Instruct v0.2, and OpenAI's GPT-3.5 Turbo),\nthis paper uncovers that the prompt templates used during fine-tuning and\ninference play a crucial role in preserving safety alignment, and proposes the\n\"Pure Tuning, Safe Testing\" (PTST) principle -- fine-tune models without a\nsafety prompt, but include it at test time. Fine-tuning experiments on GSM8K,\nChatDoctor, and OpenOrca show that PTST significantly reduces the rise of\nunsafe behaviors, and even almost eliminates them in some cases.",
        "updated": "2024-02-28 18:23:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.18540v1"
    },
    {
        "title": "RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval",
        "authors": "Kaiyue WenXingyu DangKaifeng Lyu",
        "links": "http://arxiv.org/abs/2402.18510v2",
        "entry_id": "http://arxiv.org/abs/2402.18510v2",
        "pdf_url": "http://arxiv.org/pdf/2402.18510v2",
        "summary": "This paper investigates the gap in representation powers of Recurrent Neural\nNetworks (RNNs) and Transformers in the context of solving algorithmic\nproblems. We focus on understanding whether RNNs, known for their memory\nefficiency in handling long sequences, can match the performance of\nTransformers, particularly when enhanced with Chain-of-Thought (CoT) prompting.\nOur theoretical analysis reveals that CoT improves RNNs but is insufficient to\nclose the gap with Transformers. A key bottleneck lies in the inability of RNNs\nto perfectly retrieve information from the context, even with CoT: for several\ntasks that explicitly or implicitly require this capability, such as\nassociative recall and determining if a graph is a tree, we prove that RNNs are\nnot expressive enough to solve the tasks while Transformers can solve them with\nease. Conversely, we prove that adopting techniques to enhance the in-context\nretrieval capability of RNNs, including Retrieval-Augmented Generation (RAG)\nand adding a single Transformer layer, can elevate RNNs to be capable of\nsolving all polynomial-time solvable problems with CoT, hence closing the\nrepresentation gap with Transformers.",
        "updated": "2024-02-29 07:06:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.18510v2"
    }
]