UniMODE: Unified Monocular 3D Object Detection
ZhuolingLi1, XiaogangXu2, SerNamLim3, HengshuangZhao1*
1TheUniversityofHongKong2ZhejiangUniversity3UniversityofCentralFlorida
lizhuoling@connect.hku.hk xgxu@zhejianglab.com sernam@ucf.edu hszhao@cs.hku.hk
Abstract
Realizingunifiedmonocular3Dobjectdetection,includ-
ingbothindoorandoutdoorscenes,holdsgreatimportance
in applications like robot navigation. However, involving
variousscenariosofdatatotrainmodelsposeschallenges
(a) (b)
due to their significantly different characteristics, e.g., di-
verse geometry properties and heterogeneous domain dis-
tributions. To address these challenges, we build a detec-
torbasedonthebird‚Äôs-eye-view(BEV)detectionparadigm,
where the explicit feature projection is beneficial to ad-
dressing the geometry learning ambiguity when employ-
ing multiple scenarios of data to train detectors. Then,
(c) (d)
we split the classical BEV detection architecture into two
Figure1. Illustrationofsomechallenges(e.g., diversegeometry
stagesandproposeanunevenBEVgriddesigntohandlethe
properties, heterogeneousdomaindistributions)inunifieddetec-
convergenceinstabilitycausedbytheaforementionedchal-
tion. (1) Comparing sub-figures (a) and (b), indoor objects are
lenges. Moreover,wedevelopasparseBEVfeatureprojec-
smallandclose,whileoutdoorobjectsarefarandsparse.Besides,
tionstrategytoreducecomputationalcostandaunifieddo-
the camera parameters are highly varying. (2) Comparing sub-
main alignment method to handle heterogeneous domains.
figures(a),(b),and(c),whichcorrespondtoareal-worldindoor
Combiningthesetechniques, aunifieddetectorUniMODE image,real-worldoutdoorimage,andsyntheticindoorimage,the
isderived,whichsurpassesthepreviousstate-of-the-arton imagestylesaredifferent. (3)Althoughthecategory‚ÄúPicture‚Äùis
the challenging Omni3D dataset (a large-scale dataset in- labeledinsub-figure(c),itisnotlabeledinsub-figure(d),which
cluding both indoor and outdoor scenes) by 4.9% AP , suggests label conflict among different sub-datasets. Unlabeled
3D
revealingthefirstsuccessfulgeneralizationofaBEVdetec- objectsarehighlightedbyredellipses.
tortounified3Dobjectdetection1.
andtheothersfocusonindoordetection[28]. Despitetheir
1.Introduction common goal of monocular 3D object detection, these de-
tectors exhibit significant differences in their network ar-
Monocular 3D object detection aims to accurately deter- chitectures [5]. This divergence hinders researchers from
mine the precise 3D bounding boxes of targets using only combiningdataofvariousscenariostotrainaunifiedmodel
single images captured by cameras [13, 16]. Compared to thatperformswellindiversescenes,whichisdemandedby
3D object detection based on other modalities such as Li- manyimportantapplicationslikerobotnavigation[30].
DAR point cloud, the monocular-based solution offers ad- The most critical challenge in unified 3D object detec-
vantages in terms of cost-effectiveness and comprehensive tion lies in addressing the distinct characteristics of differ-
semantic features [17, 19]. Moreover, owing to the wide- entscenarios. Forexample,indoorobjectsaresmallerand
rangingapplicationslikeautonomousdriving[8],monocu- closerinproximity,whileoutdoordetectionneedstocovera
lar3Dobjectdetectionhasdrawnmuchattentionrecently. vastperceptionrange.Recently,CubeRCNN[5]hasserved
Thanks to the efforts paid by the research community, as a predecessor in studying this problem. It directly pro-
numerous detectors have been developed. Some are de- duces3Dboxpredictionsinthecameraviewandadoptsa
signedforoutdoorscenarios[9,38]suchasurbandriving, depthdecouplingstrategytotacklethedomaingapamong
scenes. However,weobserveitsuffersseriousconvergence
*Correspondingauthor.
1ThispaperhasbeenacceptedforpublicationinCVPR2024. difficultyandispronetocollapsingduringtraining.
1
4202
beF
82
]VC.sc[
1v37581.2042:viXraToovercometheunstableconvergenceofCubeRCNN, tounifieddetection,seamlesslyintegratingindoorandout-
we employ the recent popular bird‚Äôs-eye-view (BEV) de- door scenes. It showcases the immense potential of BEV
tection paradigm to develop a unified 3D object detector. detection across a broad spectrum of scenarios and under-
ThisisbecausethefeatureprojectionintheBEVparadigm scorestheversatilityofthistechnology.
alignstheimagespacewiththe3Dreal-worldspaceexplic-
itly[15],whichalleviatesthelearningambiguityinmonoc- 2.RelatedWork
ular 3D object detection. Nevertheless, after extensive ex-
Monocular3Dobjectdetection. Duetoitsadvantagesof
ploration,wefindthatnaivelyadoptingexistingBEVdetec-
beingeconomicalandflexible,monocular3Dobjectdetec-
tionarchitectures[15,18]doesnotyieldpromisingperfor-
tiongrabsmuchresearchattention[22]. Existingdetectors
mance,whichismainlyblamedonthefollowingobstacles.
can be broadly categorized into two groups, camera-view
First of all, as shown in Fig. 1 (a) and (b), the geome-
detectors and BEV detectors. Among them, camera-view
tryproperties(e.g.,perceptionranges,targetpositions)be-
detectorsgenerateresultsinthe2Dimageplanebeforecon-
tweenindoorandoutdoorscenesarequitediverse. Specif-
verting them into the 3D real space [10, 25]. This group
ically,indoorobjectsaretypicallyafewmetersawayfrom
is generally easier to implement. However, the conversion
the camera, while outdoor targets can be more than 100m
from the 2D camera plane to 3D physical space can intro-
away. Since a unified BEV detector is required to recog-
duceadditionalerrors[32],whichnegativelyimpactdown-
nize objects in all scenarios, the BEV feature has to cover
streamplanningtaskstypicallyperformedin3D[7].
themaximumpossibleperceptionrange. Meanwhile,asin-
BEVdetectors,ontheotherhand,transformimagefea-
door objects are often small, the BEV grid resolution for
tures from the 2D camera plane to the 3D physical space
indoordetectionneedstobeprecise. Allthesecharacteris-
before generating results in 3D [12]. This approach bene-
tics can lead to unstable convergence and significant com-
fitsdownstreamtasks,asplanningisalsoperformedinthe
putationalburden. Toaddressthesechallenges,wedevelop
3Dspace[18]. However,thechallengewithBEVdetectors
atwo-stage detectionarchitecture. Inthisarchitecture, the
isthatthefeaturetransformationprocessreliesonaccurate
firststageproducesinitialtargetpositionestimation,andthe
depthestimation,whichcanbedifficulttoachievewithonly
secondstagelocatestargetsusingthisestimationaspriorin-
cameraimages[23]. Asaresult,convergencebecomesun-
formation, which helps stabilize the convergence process.
stablewhendealingwithdiversedatascenarios[5].
Moreover, we introduce an innovative uneven BEV grid
Unifiedobjectdetection. Inordertoimprovethegeneral-
splitstrategythatexpandstheBEVspacerangewhilemain-
ization ability of detectors, some works have explored the
tainingamanageableBEVgridsize. Furthermore,asparse
integration of multiple data sources during model training
BEVfeatureprojectionstrategyisdevelopedtoreducethe
[14, 34]. For example, in the field of 2D object detection,
projectioncomputationalcostby82.6%.
SMD [40] improves the performance of detectors through
Anotherobstaclearisesfromtheheterogeneousdomain learning a unified label space. In the 3D object detection
distributions (e.g., image styles, label definitions) across domain, PPT [36] investigates the utilization of extensive
various scenarios. For example, as depicted in Fig. 1 (a), 3D point cloud data from diverse datasets for pre-training
(b),and(c),thedatacanbecollectedinrealscenesorsyn- detectors. In addition, Uni3DETR [35] reveals how to de-
thesizedvirtually. Besides,comparingFig.1(c)and(d),a vise a unified point-based 3D object detector that behaves
classofobjectsmaybeannotatedinascenebutnotlabeled well in different domains. For the camera-based detection
inanotherscene,leadingtoconfusionduringnetworkcon- track,CubeRCNN[5]servesasthesolepredecessorinthe
vergence. To handle these conflicts, we propose a unified studyofunifiedmonocular3Dobjectdetection. However,
domainalignmenttechniqueconsistingoftwoparts,thedo- CubeRCNNisplaguedbytheunstableconvergenceissue,
mainadaptivelayernormalizationtoalignfeatures,andthe necessitatingfurtherin-depthanalysiswithinthistrack.
classalignmentlossforalleviatinglabeldefinitionconflict.
Combining all these innovative techniques, a Unified 3.Method
Monocular Object DEtector named UniMODE is devel-
3.1.OverallFramework
oped, and it achieves state-of-the-art (SOTA) performance
ontheOmni3Dbenchmark. Intheunifieddetectionsetting, TheoverallframeworkofUniMODEisillustratedinFig.2.
UniMODE surpasses the SOTA detector, Cube RCNN, by Asshown,amonocularimageI ‚ààR3√óH√óW sampledfrom
an impressive 4.9% in terms of AP (average precision multiple scenarios (e.g., indoor and outdoor, real and syn-
3D
based on 3D intersection over union). Furthermore, when thetic, daytime and nighttime) are input to the feature ex-
evaluated in indoor and outdoor detection settings individ- traction module (including a backbone and a neck) to pro-
ually, UniMODE outperforms Cube RCNN by 11.9% and duce representative feature F ‚àà RC√ó 1H 6√óW 16. Then, F is
9.1%,respectively.Thisworkrepresentsapioneeringeffort processedby4fullyconvolutionalheads,namely‚Äúdomain
toexplorethegeneralizationofBEVdetectionarchitectures head‚Äù, ‚Äúproposal head‚Äù, ‚Äúfeature head‚Äù, and ‚Äúdepth head‚Äù,
2Domain Confidence
Domain Head
(ùëê",ùëê%,‚Ä¶,ùëê&)
Proposal Map
ùëÄProposal Queries Class Alignment Loss
Proposal Head MLP
ùëÄ+ùëÅQueries
Extracted Feature NRandom Queries BEV Decoder√ó6 Detection Results
B√óùê∂√ó "! #√ó "$ # Feature Head DALN Cross-Attn DALN
Sparse BEV Feature Query FFN
BEV Encoder
Backbone + Neck Projection Self-Attn FFN
Depth Head
Input Images
B√ó3√óùêª√óùëä Sparse BEV Feature Projection Uneven BEV Feature Grid DALN
Domain Confidence
(ùëê",ùëê%,‚Ä¶,ùëê&)
Sparse Tokens Domain Parameters
(ùõº",ùõΩ")
Even Grid (Previous) (ùõº%,ùõΩ%)
(ùõº&,ùõΩ&) Input-dependent Parameter
Sparse Feature Projection Input Layer Norm Mini-adjust Output
Images from Diverse Domains Dense Feature on Uneven Grid Uneven Grid (Ours) Feature Feature
Figure2.TheoveralldetectionframeworkofUniMODE.Theillustratedmodulesproposedinthisworkincludetheproposalhead,sparse
BEVfeatureprojection,unevenBEVfeaturegrid,domainadaptivelayernormalization,andclassalignmentloss.
respectively. Among them, the role of the domain head 3.2.Two-StageDetectionArchitecture
is to predict which pre-defined data domain an input im-
The integration of indoor and outdoor 3D object detection
age is most relevant to, and the classification confidence
ischallengingduetodiversegeometryproperties(e.g.,per-
produced by the domain head is subsequently utilized in
ceptionranges,targetpositions). Indoordetectiontypically
domain alignment. The proposal head aims to estimate
involves close-range targets, while outdoor detection con-
the rough target distribution before the 6 Transformer de-
cernstargetsscatteredoverabroader3Dspace.Asdepicted
coders, and the estimated distribution serves as prior in-
in Fig. 3, the perception ranges and target positions in in-
formation for the second-stage detection. This design al-
doorandoutdoordetectionscenesvarysignificantly,which
leviatesthedistributionmismatchbetweendiversetraining
arechallengingfortraditionalBEV3Dobjectdetectorsbe-
domains (refer to Section 3.2). The proposal head output
causeoftheirfixedBEVfeatureresolutions.
is encoded as M proposal queries. In addition, N queries
arerandomlyinitializedandconcatenatedwiththeproposal Thegeometrypropertydifferenceisidentifiedasanes-
queries for the second-stage detection, leading to M +N sentialreasoncausingtheunstableconvergenceofBEVde-
queriesinthesecondstage. tectors [15]. For example, the target position distribution
Thefeatureheadanddepthheadareresponsibleforpro- difference makes it challenging for Transformer-based de-
jectingtheimagefeatureintotheBEVplaneandobtaining tectors to learn how to update the query reference points
BEV feature. During this projection, we develop a tech- graduallytowardconcernedobjects.Infact,throughvisual-
nique to remove unnecessary projection points, which re- ization,wefindthereferencepointupdatinginthe6Trans-
ducesthecomputingburdenbyabout82.6%(refertoSec- formerdecodersisdisordered. Asaresult,ifweadoptthe
tion3.4). Besides,weproposetheunevenBEVfeature(re- classicaldeformableDETRarchitecture[41]tobuilda3D
fertoSection3.3),whichmeanstheBEVgridsclosertothe objectdetector,thetrainingiseasytocollapseduetothein-
cameraenjoymorepreciseresolution,andthegridsfarther accurate positions of learned reference points, resulting in
to the camera cover broader perception areas. This design suddengradientvanishingorexploding.
wellbalancesthegridsizecontradictionbetweenindoorde- Toovercomethischallenge,weconstructUniMODEin
tectionandoutdoordetectionwithoutextramemoryburden. atwo-stagedetectionfashion. Inthefirststage, wedesign
ObtainingtheprojectedBEVfeature,aBEVencoderis a CenterNet [39] style head (the proposal head in Fig. 2)
employed to further refine the feature, and 6 decoders are to produce detection proposals. Specifically, its predicted
adopted to generate the second-stage detection results. As attributes include the 2D center Gaussion heatmap, offset
mentionedbefore,M+N queriesareusedduringthispro- from2Dcentersto3Dcenters,and3Dcenterdepthsoftar-
cess. After the 6 decoders, the queries are decoded as de- gets. The 3D center coordinates of proposals can be de-
tectionresultsbyqueryingtheFFN.Inthedecoderpart,the rived from these predicted attributes. Then, the proposals
unified domain alignment strategy is devised to align the withtopM confidencesareselectedandencodedasM pro-
dataofvariousscenariosviaboththefeatureandlossper- posalqueriesbyanMLPlayer.Toaccountforanypotential
spectives. RefertoSection3.5formoredetails. missed targets, another N randomly initialized queries are
3
noitcartxE
erutaeFfeaturemustcoveralargeperceptionareawhilestillutiliz-
ingsmallBEVgrids,whichposesamassivechallengedue
tothelimitedGPUmemory.
To overcome this challenge, we propose a solution that
involves partitioning the BEV space into uneven grids, in
contrasttotheevengridsutilizedbyexistingdetectors. As
depicted in the bottom part of Fig. 2, we achieve this by
employingasmallersizeofgridsclosertothecameraand
Indoor Outdoor
larger grids for those farther away. This approach enables
Figure 3. Indoor and outdoor target position distributions in the
UniMODE to effectively perceive a wide range of objects
BEVspace. Thebrighterapointshows,themoretargetsthecor-
whilemaintainingsmallgridsizesforobjectsincloseprox-
respondingBEVgridcontains. Theperceptioncameraislocated
imity. Importantly, this does not increase the total number
atthepointwiththecoordinate(0,0).
ofgrids,therebyavoidinganyadditionalcomputationalbur-
concatenatedwiththeseproposalqueriestoperforminfor- den. Specifically,assumingthereareN gridsinthedepth
z
mationinteractioninthe6decodersofthesecondstage(the axis and the depth range is (z ,z ), the grid size of
min max
Transformerstage). Inthisway,theinitialqueryreference thei gridz issetto:
th i
pointsoftheseconddetectionstageareadjustedadaptively.
z ‚àíz
Our experiments reveal that this two-stage architecture is z =z + max min ¬∑i(i+1). (1)
essentialforstableconvergence. i min N z(N z+1)
Besides,sincethepositionsofqueryreferencepointsare
Notably, the mathematical form in Eq. 1 is similar to
notrandomlyinitialized,theiterativeboundingboxrefine-
thelinear-increasingdiscretizationofdepthbininCaDDN
mentstrategyproposedindeformableDETR[41]isaban-
[26], while the essence is fundamentally different. In
donedasitmayleadtoadeteriorationofreferencepoints‚Äô
CaDDN, the feature projection distribution is adjusted to
quality. Infact,weobservethatthisiterativeboundingbox
allocatemorefeaturestogridsclosertothecamera. Inex-
refinementstrategycouldresultinconvergencecollapse.
periments,weobservethatthisadjustmentresultsinamore
imbalancedBEVfeature,i.e.,denserfeaturesinclosergrids
3.3.UnevenBEVGrid
andmoreemptygridsinfarthergrids. Sincefeaturesinall
Anotabledifferencebetweenindoorandoutdoor3Dobject gridsareextractedbythesamenetwork,thisimbalancede-
detectionliesinthegeometryinformation(e.g.,scale,prox- gradestheperformance. Bycontrast,ourunevenBEVgrid
imity) of objects to the camera during data collection. In- approach enhances detection precision by making the fea-
doorenvironmentstypicallyfeaturesmallerobjectslocated turedensitymorebalanced.
closertothecamera,whereasoutdoorenvironmentsinvolve
3.4.SparseBEVFeatureProjection
largerobjectspositionedatgreaterdistances. Furthermore,
outdoor 3D object detectors must account for a wider per- The step of transforming the camera view feature into
ceptual range of the environment. Consequently, existing the BEV space is quite computationally expensive due to
indoor 3D object detectors typically use smaller voxel or its numerous projection points. Specifically, considering
pillar sizes. For instance, the voxel size of CAGroup3D the image feature F
i
‚àà RCi√ó1√óHf√óWf and depth fea-
[31], a SOTA indoor 3D object detector, is 0.04 meters, ture F
d
‚àà R1√óCd√óHf√óWf, the projection feature F
p
‚àà
and the maximum target depth in the SUN-RGBD dataset RCi√óCd√óHf√óWf is obtained by multiplying F
i
and F d.
[29], a classic indoor dataset, is approximately 8 meters. Therefore,theprojectionpointnumberC √óC √óH √óW
i d f f
Incontrast,outdoordatasetsexhibitmuchlargerperception increasesdramaticallyasthegrowthofC .Theheavycom-
d
ranges. For example, the commonly used outdoor detec- putationalburdenofthisfeatureprojectionsteprestrictsthe
tiondatasetKITTI[8]hasamaximumdepthrangeof100 BEV feature resolution, and thus hinders unifying indoor
meters. Duetothisvastperceptionrangeandlimitedcom- andoutdoor3Dobjectdetection.
putingresources,outdoordetectorsemploylargerBEVgrid In this work, we observe that most projection points in
sizes,e.g.theBEVgridsizeinBEVDepth[11],astate-of- F areunnecessarybecausetheirvaluesarequitetiny. This
p
the-artoutdoor3Dobjectdetector,is0.8meters. isessentiallybecauseofthesmall corresponding valuesin
Therefore,theBEVgridsizesofcurrentoutdoordetec- F ,whichimplythatthemodelpredictsthereisnotargetin
d
torsaretypicallylargetoaccommodatethevastperception thesespecificBEVgrids. Hence,thetimespentonproject-
range,whilethoseofindoordetectorsaresmallbecauseof ingfeaturestotheseunconcernedgridscanbesaved.
theintricateindoorscenes.However,sinceUniMODEaims Based on the above insights, we propose to remove
toaddressbothindoorandoutdoor3Dobjectdetectionus- the unnecessary projection points based on a pre-defined
ingaunifiedmodelstructureandnetworkweight,itsBEV thresholdœÑ. Specifically,weeliminatetheprojectionpoints
4in F whose corresponding depth confidence of F is
p d
smallerthanœÑ. Inthisway,mostprojectionpointsareelim- Not labeled
inated. Forinstance,whensettingœÑ to0.001,about82.6%
ofprojectionpointscanbeexcluded.
3.5.UnifiedDomainAlignment
Heterogeneousdomaindistributionsexistindiversescenar-
(a) ARKitScenes (b) Hypersim
iosandweaddressthischallengeviafeatureandlossviews.
Figure4.Anexampleofheterogeneouslabelconflictamongsub-
Domain adaptive layer normalization. For the feature
datasetsinOmni3D.Asshown,‚ÄúWindow‚ÄùisnotlabeledinARK-
view,weinitializedomain-specificlearnableparametersto itSceneswhilelabeledinHypersim, sotheunlabeledwindowin
addressthevariationsobservedindiversetrainingdatado- (a)couldharmtheconvergencestabilityofdetectors.
mains. However, this strategy must adhere to two crucial
input images self-adaptively, and the increased parameters
requirements.Firstly,thedetectorshouldexhibitrobustper-
are negligible. Additionally, when an image unseen in the
formanceduringinference,evenwhenconfrontedwithim-
trainingsetisinput,DALNstillworkswell,becausetheun-
agesfromdomainsthatarenotencounteredduringtraining.
seenimagecanstillbeclassifiedasaweightedcombination
Secondly,theintroductionofthesedomain-specificparam-
oftheseDdomains.
etersshouldincurminimalcomputationaloverhead.
Althoughthereexistafewprevioustechniquesrelatedto
Consideringthesetworequirements,weproposethedo-
adaptivenormalization,almostallofthemarebasedonre-
mainadaptivelayernormalization(DALN)strategy. Inthis
strategy,wefirstsplitthetrainingdataintoDdomains. For gressinginput-dependentparametersdirectly[36]. So,they
needtobuildaspecialregressionheadforeverynormaliza-
theclassicimplementationoflayernormalization(LN)[2],
denoting the input sequence as X ‚àà RB√óL√óC and its el- tion layer. By contrast, DALN enables all layers to share
l
ement with the index (b,l,c) as x(b,l,c), the corresponding the same domain head, so the computing burden is much
l smaller.Besides,DALNintroducesdomain-specificparam-
outputxÀÜ(b,l,c)ofprocessingx(b,l,c)byLNisobtainedas:
l l eters,whicharemorestabletotrain.
Class alignmentloss. In theloss view, we aimto address
x(b,l,c)‚àí¬µ(b,l)
xÀÜ(b,l,c) = l , (2) the heterogeneous label conflict when combining multiple
l œÉ(b,l) datasources.Specifically,thereare6independentlylabeled
sub-datasetsinOmni3D,andtheirlabelspacesaredifferent.
where
For example, as presented in Fig. 4, although the Window
(cid:118)
C (cid:117) C class is annotated in ARKitScenes, while it is not labeled
¬µ(b,l) = 1 (cid:88) x(b,l,i),œÉ(b,l) =(cid:117) (cid:116)1 (cid:88) (x(b,l,c)‚àí¬µ(b,l))2. inHypersim. AsthelabelspaceofOmni3Distheunionof
C l C l
allclassesinallsubsets,theunlabeledwindowinFig.4(a)
i=1 i=1
(3) becomesamissingtargetthatharmsconvergencestability.
The two-stage detection architecture described in Sec-
In DALN, we build a set of learnable domain-specific tion 3.2 can alleviate the aforementioned problem to some
parameters, i.e., {(Œ± ,Œ≤ )}D , where (Œ± ,Œ≤ ) are the pa- extent, because it helps the detector concentrate on fore-
i i i=1 i i
rameterscorrespondingtothei domain. {Œ± }D areini- groundobjects,andtheunlabeledobjectsareoverlookedto
th i i=1
tializedas1and{Œ≤ }D aresetto0. Then,weestablisha computeloss. Toaddressthisproblemfurther,wedevisea
i i=1
domainheadconsistingofseveralconvolutionallayers. As simple strategy, the class alignment loss. Specifically, de-
showninFig.2,thedomainheadtakesthefeatureF asin- notingthelabelspaceofthei datasetas‚Ñ¶ ,wecompute
th i
putandpredictstheconfidencescoresthattheinputimages lossonthei datasetas:
th
I belong to these D domains. Denoting the confidence of
Œ≥¬∑l(y,y¬Ø),(y¬Ø‚àà/ ‚Ñ¶ )‚àß(y¬Ø=B)
the b th image as {c i}D i=1, the input-dependent parameters L i ={ l(y,y¬Ø),others i , (5)
(Œ±,Œ≤)arecomputedfollowing:
where l(¬∑), y, y¬Ø, B are the loss function, class prediction,
D D
(cid:88) (cid:88) classlabel,andbackgroundclass,respectively. Œ≥isafactor
Œ±= c ¬∑Œ± , Œ≤ = c ¬∑Œ≤ . (4)
i i i i for reducing the punishment to classes not included in the
i=1 i=1
labelspaceofthissample.
Obtaining(Œ±,Œ≤),weemploythemtoadjustthedistribution
4.Experiment
of xÀÜ(b,l,c) with respect to x¬Ø(b,l,c) = Œ±¬∑xÀÜ(b,l,c) +Œ≤, where
l l l
x¬Ø(b,l,c)denotestheupdateddistribution.Inthisway,thefea- Implementation details. The perception ranges in the X-
l
turedistributioninUniMODEcanbeadjustedaccordingto axis,Y-axis,andZ-axisofthecameracoordinatesystemare
5OMNI3D OUT OMNI3D IN OMNI3D
Method
APkit ‚Üë APnus‚Üë APout‚Üë APsun‚Üë APin ‚Üë AP25 ‚Üë AP50 ‚Üë APnear‚Üë APmed‚Üë APfar ‚Üë AP ‚Üë
3D 3D 3D 3D 3D 3D 3D 3D 3D 3D 3D
M3D-RPN[4] 10.4% 17.9% 13.7% - - - - - - - -
SMOKE[19] 25.4% 20.4% 19.5% - - - - - - - 9.6%
FCOS3D[32] 14.6% 20.9% 17.6% - - - - - - - 9.8%
PGD[33] 21.4% 26.3% 22.9% - - - - - - - 11.2%
GUPNet[21] 24.5% 20.5% 19.9% - - - - - - - -
ImVoxelNet[28] 23.5% 23.4% 21.5% 30.6% - - - - - - 9.4%
BEVFormer[15] 23.9% 29.6% 25.9% - -
PETR[18] 30.2% 30.1% 27.8% - -
CubeRCNN[5] 36.0% 32.7% 31.9% 36.2% 15.0% 24.9% 9.5% 27.9% 12.1% 8.5% 23.3%
UniMODE 40.2% 40.0% 39.1% 36.1% 22.3% 28.3% 7.4% 29.7% 12.7% 8.1% 25.5%
UniMODE* 41.3% 43.6% 41.0% 39.8% 26.9% 30.2% 10.6% 31.1% 14.9% 8.7% 28.2%
Table 1. Performance comparison between the proposed UniMODE and other 3D object detectors. In the 2 ‚àº 4 columns, the
nd th
detectors are trained using KITTI and nuScenes. These three columns reflect the detection precision on KITTI, nuScenes, and overall
outdoor detection performance, respectively. The 5 ‚àº 6 columns correspond to indoor detection results. Among them, the 5
th th th
columnistheperformancethatdetectorsaretrainedandvalidatedonSUN-RGBD.Inthe6 column,detectorsaretrainedandevaluated
th
bycombiningSUN-RGBD,ARKitScenes, andHypersim. The7 ‚àº 12 columnsrepresenttheoveralldetectionperformancewhere
th th
detectorsaretrainedandvalidatedutilizingalldatainOmni3D.UniMODEandUniMODE*denotetheproposeddetectors,takingDLA34
andConvNext-Baseasthebackbones,respectively. Thebestresultsgivenvariousmetricsaremarkedinbold. The‚Äú ‚Äùmeansthatthe
modeldoesnotconvergewell,andtheobtainedperformanceisquitepoor.The‚Äú-‚Äùmeansthisresultisreportedinpreviousliterature.
Backbone APsun‚Üë APhyp‚Üë APark‚Üë APobj‚Üë APkit ‚Üë APnus‚Üë aresmallerandtheobjectcategoriesaremorediverse. Hy-
3D 3D 3D 3D 3D 3D
persim, distinctfromtheaforementionedfivedatasets, isa
DLA34 21.0% 6.7% 42.3% 52.5% 27.8% 31.7%
ConvNext 23.0% 8.1% 48.0% 66.1% 29.2% 36.0% virtuallysynthesizeddataset.Thus,Hypersimallowsforthe
annotationofobjectclassesthatarechallengingtolabelin
Table 2. Detailed performance of UniMODE on various sub-
realscenes,suchastransparentobjects(e.g.,windows)and
datasetsinOmni3D.Thedetectorsaretrainedandevaluatedusing
verythinobjects(e.g.,carpets). TheOmni3Ddatasetcom-
thewholeOmni3Dtrainingandtestingdata.Theresultsofadopt-
prisesatotalof98objectcategoriesand3million3Dbox
ingtwodifferentbackbonesarepresented.
annotations,spanning234,000images.Theevaluationmet-
ricisAP ,whichreflectsthe3DIntersectionoverUnion
(‚àí30,30),(‚àí40,40),(0,80)meters,respectively. Ifwith- 3D
(IoU)between3Dboxpredictionandlabel.
outaspecialstatement,theBEVgridresolutionis(60,80).
Experimental settings. As Omni3D is a large-scale
ThefactorŒ≥definedintheclassalignmentlossissetto0.2.
dataset,trainingmodelsonitnecessitatesmanyGPUs. For
M andN aresetto100. TheadoptedoptimizerisAdamW,
andthelearningrateissetto12e‚àí4forabatchsizeof192. example, the authors of Cube RCNN run each experiment
with 48 V100s for 4‚àº5 days. In this work, the experi-
Theexperimentsareprimarilyconductedon4A100GPUs.
ments in Section 4.1 are performed in the high computing
Thetotallossincludestwoparts,theproposalheadlossand
resourcesetting(theinputimageresolutionis1280√ó1024,
thequeryFFNloss. Theproposalheadlossconsistsofthe
the backbone is ConvNext-Base [20], all training data).
heatmap classification loss and depth regression loss. The
Sinceourcomputingresourcesarelimited,unlessexplicitly
query FFN loss comprises the classification loss (a cross-
statedotherwise, theremainingexperimentsareconducted
entropyloss)andregressionloss(L lossforpredicting3D
1
inthelowcomputingresourcesetting(theinputresolution
center, dimension, and orientation). The total loss is the
is640√ó512,thebackboneisDLA34[37],fixed20%ofall
weightedsumoftheselossitems. Nospeciallossissetfor
trainingdatasampledfromall6sub-datasets).
theM proposalquerygeneration.
Dataset. The experiments in this section are performed
4.1.PerformanceComparison
on Omni3D, the sole large-scale 3D object detection
benchmarkencompassingbothindoorandoutdoorscenes. In this part, we compare the performance of the proposed
Omni3D is built upon six well-known datasets includ- detectorwithpreviousmethods.Amongthem,CubeRCNN
ing KITTI [8], SUN-RGBD [29], ARKitScenes [3], Ob- is the sole detector that also explores unified detection.
jectron [1], nuScenes [6], and Hypersim [27]. Among BEVFormer[15]andPETR[18]aretwopopularBEVde-
these datasets, KITTI and nuScenes focus on urban driv- tectors, and we reimplement them in the Omni3D bench-
ing scenes, which are real-world outdoor scenarios. SUN- mark to get the detection scores. The performance of the
RGBD, ARKitScenes, and Objectron primarily pertain to other compared detectors is obtained from [5]. All the
real-world indoor environments. Compared with outdoor results are given in Table 1. In addition, we present the
datasets, the required perception ranges of indoor datasets detailed detection scores of UniMODE on various sub-
6PH UBG SBFP UDA APi 3n D‚Üë APo 3u Dt‚Üë AP 3D‚Üë Improvement GridSize(m) DepthBin APi 3n D‚Üë APo 3u Dt‚Üë AP 3D‚Üë
10.9% 14.3% 12.3% - 1 Even 14.8% 24.5% 17.4%
‚úì 13.4% 22.2% 15.9% 3.6%‚Üë 1 Uneven 12.1% 22.6% 15.3%
‚úì ‚úì 14.00% 23.8% 16.6% 0.7%‚Üë 0.5 Even 15.4% 25.5% 18.1%
‚úì ‚úì ‚úì 13.4% 23.7% 16.6% 0.0%‚Üë 2 Even 14.0% 23.9% 16.5%
‚úì ‚úì ‚úì ‚úì 14.8% 24.5% 17.4% 0.8%‚Üë
Table4.Ablationstudyongridsizeanddepthbinsplitstrategyin
unevenBEVgrid.
Table3. Ablationstudyonproposedstrategies,whichverifythe
effects of proposal head (PH), uneven BEV grid (UBG), sparse œÑ RemoveRatio(%) APi 3n D‚Üë APo 3u Dt‚Üë AP 3D‚Üë
BEV feature projection(SBFP), and unified domain alignment 0 0.0 14.9% 24.7% 17.4%
(UDA). The last column presents the improvement of each row 1e-3 82.6 14.8% 24.5% 17.4%
compared with the top row. APin and APout reflect the in- 1e-2 94.3 12.1% 21.9% 15.0%
3D 3D
door and outdoor detection performance, respectively. Notably, 1e-1 98.3 4.7% 3.6% 4.7%
althoughSBFPdoesnotboostthedetectionprecision,itreduces
Table5.AblationstudyonœÑ insparseBEVfeatureprojection.
thecomputationalcostoftheBEVfeatureprojectionby82.6%.
datasetsinOmni3DasTable2. resourcesettingduetothelimitedcomputingresources.
Accordingtotheresults,wecanobservethatUniMODE AccordingtotheresultsinTable3,wecanobservethat
achieves the best results in all metrics. It surpasses the allthesestrategiesareveryeffective.Amongthem,thepro-
SOTA Cube RCNN by 4.9% given the primary metric posalheadbooststheresultbythemostsignificantmargin.
AP . Besides DLA34, we also try another backbone Specifically, the proposal head enhances the overall detec-
3D
ConvNext-Base. This is because previous papers suggest tion performance metric AP 3D by 3.6%. Meanwhile, the
that DLA34 is commonly used in camera-view detectors indoorandoutdoordetectionmetricsAPi 3n D andAPo 3u Dt are
like Cube RCNN but not suitable for BEV detectors [13]. boostedby2.5%and7.9%separately. AsdiscussedinSec-
Since UniMODE is a BEV detector, only testing the per- tion3.2,theproposalheadisquiteeffectivebecauseitstabi-
formance of UniMODE with DLA34 is unfair. Thus, we lizestheconvergenceprocessofUniMODEandthusfavors
alsotesttheUniMODEwithConvNext-Base,andtheresult detectionaccuracy. Thecollapsedoesnothappenafterus-
suggeststhattheperformanceisboostedsignificantly. Ad- ingtheproposalhead.Inaddition,althoughthesparseBEV
ditionally, the speed of UniMODE is also promising. Test feature projection strategy does not improve the detection
on1A100GPU,theinferencespeedsofUniMODEunder precision,itreducestheprojectioncostby82.6%.
thehighandlowcomputingresourcesettingsare21.41FPS UnevenBEVgrid.WestudytheeffectofBEVfeaturegrid
and43.48FPSseparately. sizeanddepthbinsplitstrategyinunevenBEVgriddesign,
In addition, it can be observed from Table 1 that BEV- andtheresultsarepresentedinTable4. Whenthedepthbin
Former and PETR do not converge well in the unified de- splitisuneven,wesplitthedepthbinrangefollowingEq.1.
tectionsettingwhilebehavingpromisinglytrainedwithout- Comparingthe1 and2 rowsofresultsinTable4,we
st nd
door datasets. This phenomenon implies the difficulty of canfindthatunevendepthbindeterioratesdetectionperfor-
unifyingindoorandoutdoor3Dobjectdetection. Through mance. We speculate this is because this strategy projects
analysis,wefindthatBEVFormerobtainspoorresultswhen morepointsincloserBEVgridswhilefewerpointsinfar-
using data of all domains because its convergence is quite thergrids,whichfurtherincreasestheimbalanceddistribu-
unstable,andthelosscurveoftenbooststoahighvaluedur- tionofprojectionfeatures. Additionally,comparingthe1 ,
st
ingtraining. PETRdoesnotbehavewellsinceitimplicitly 3 , and 4 rows of results in Table 4, it is observed that
rd th
learns the correspondence relation between 2D pixels and smaller BEV grids lead to better performance. We set the
3Dvoxels.Whenthecameraparameterskeepsimilaracross BEV grid size to 1m rather than 0.5m in all the other ex-
allsamplesinadatasetlikenuScenes[6],PETRconverges perimentsduetolimitedcomputingresourcesandthevast
smoothly. Nevertheless,whentrainedinadatasetwithdra- training data volume of Omni3D, i.e., if we decrease the
maticallychangingcameraparameterslikeOmni3D,PETR size of BEV grids, the performance of UniMODE can be
becomesmuchmoredifficulttotrain. furtherboostedcomparedwiththecurrentperformance.
Sparse BEV feature projection. As mentioned in Sec-
4.2.AblationStudies
tion 3.4, the BEV feature projection process is computa-
Keycomponentdesigns.Weablatetheeffectivenessofthe tionally expensive. To reduce this cost, we propose to re-
proposedstrategiesinUniMODE,includingproposalhead, moveunimportantprojectionpoints. Althoughthisstrategy
unevenBEVgrid, sparseBEVfeatureprojection, anduni- enhances network efficiency significantly, it could deteri-
fied domain alignment. The experimental results are pre- orate detection accuracy and convergence stability, which
sented in Table 3. Notably, as mentioned before, the ex- is exactly a trade-off. In this part, we study this trade-off
periments in this part are conducted in the low computing through experiments. Specifically, as introduced in Sec-
7ARKitScenes Hypersim nuScenes
KITTI Objectron SUN-RGBD
Figure5.Visualizationofdetectionresultsonvarioussub-datasetsinOmni3D.
tion3.4,weremoveunimportantprojectionpointsbasedon None DR DALN
Method
apre-definedhyper-parameterœÑ. ThevalueofœÑ isadjusted APark‚Üë APsun‚Üë APark‚Üë APsun‚Üë APark‚Üë APsun‚Üë
3D 3D 3D 3D 3D 3D
to analyze how the removed projection point ratio affects Result 33.6% 12.3% 33.9% 12.1% 35.0% 13.0%
performance. TheresultsarereportedinTable5.
Table6.AnalysisontheeffectivenessofDALN.
ItcanbeobservedfromTable5thatwhenœÑ is0,which
meansnofeatureisdiscarded,thebestperformanceacross Train Zero-Shot Œ¥-Tune
allrowsisarrived. WhenwesetœÑ to1e‚àí3,about82.6%of
APh 3Dyp‚Üë APs 3u Dn‚Üë APa 3r Dk‚Üë APh 3Dyp‚Üë APs 3u Dn‚Üë APa 3r Dk‚Üë
thefeatureisdiscardedwhiletheperformanceofthedetec- Hypersim 14.7% 5.6% 3.6% 14.7% 18.5% 18.9%
SUN-RGBD 3.0% 28.5% 8.8% 7.5% 28.5% 27.2%
tor remains very similar to the one with œÑ = 0. This phe-
ARKitScenes 4.2% 13.0% 35.0% 10.4% 22.8% 35.0%
nomenonsuggeststhatthediscardedfeatureisunimportant
Table7. Cross-domainevaluationinindoorsub-datasets. Inthis
for final detection accuracy. Then, when we increase œÑ to
experiment,thedetectorisfirsttrainedinadomainandtestedon
1e‚àí2 and1e‚àí1, wecanfindthatthecorrespondingperfor-
otherdomainsintwosettings,zero-shotandŒ¥-tune.
mances drop dramatically. This observation indicates that
when we discard superfluous features, the detection preci- domaineffectivenessofDALN.
sionandeventrainingstabilityareinfluencedsignificantly.
4.3.Cross-domainEvaluation
Combiningalltheobservations,wesetœÑ to1e‚àí3 anddrop
82.6%ofunimportantfeatureinUniMODE,whichreduces
WeevaluatethegeneralizationabilityofUniMODEinthis
thecomputationalcostby82.6%whilemaintainingsimilar
part by conducting the cross-domain evaluation. Specifi-
performanceastheonewithoutdroppingfeatures. cally, we train a detector on a sub-dataset in Omni3D and
Effectiveness of DALN. In this experiment, we validate testtheperformanceofthisdetectorondifferentothersub-
the effectiveness of DALN through comparing the perfor- datasets. Theexperimentsareconductedintwosettings. In
mances of the naive baseline without any domain adaptive thezero-shotsetting,thetestdomainiscompletelyunseen.
strategy, the baseline predicting dynamic parameters with In the Œ¥-tune setting, 1% of training set data from the test
direct regression (DR) [24], and the baseline with DALN domainisusedtofine-tunetheQueryFFNinUniMODEfor
(our proposed). All these models are trained using only 1epoch. TheexperimentalresultsarepresentedinTable7.
ARKitScenesandevaluatedwithARKitScenes(in-domain) According to the results in the 2 ‚àº 4 columns of
st th
and SUN-RGBD (out-of-domain) separately. The result is Table 7, we can find that when a detector is trained and
presentedinTable6. ItcanbeobservedthatDRcouldde- validated in the same indoor sub-dataset, its performance
grade the detection accuracy while DALN boosts the per- is promising. However, when evaluated on another com-
formance significantly, which reveals the zero-shot out-of- pletely unseen sub-dataset, the accuracy is limited. This
850 References
UniMODE PETR
45
40 Lossboost [1] AdelAhmadyan,LiangkaiZhang,ArtsiomAblavatski,Jian- 35
GradientNaN ingWei,andMatthiasGrundmann.Objectron:Alargescale
30
25 datasetofobject-centricvideosinthewildwithposeanno-
20 tations. InCVPR,pages7822‚Äì7831,2021. 6
15 [2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
10
ton. Layernormalization. arXivpreprintarXiv:1607.06450,
5
0 2000 4000 6000 8000 10000 12000 2016. 5
Iteration
[3] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Yuri Fei-
Figure6.ThetrainingslosscurvesofUniMODEandPETR. gin, Peter Fu, Thomas Gebauer, Daniel Kurz, Tal Dimry,
BrandonJoffe,ArikSchwartz,etal. Arkitscenes: Adiverse
is partly because monocular 3D depth estimation is an ill-
real-world dataset for 3d indoor scene understanding using
posed problem. When the training and validation data be-
mobilergb-ddata. InNeurIPS,2021. 6
long to differing domains, predicting depth accurately is
[4] GarrickBrazilandXiaomingLiu. M3d-rpn: Monocular3d
challenging,especiallyforthevirtualdatasetHypersim.
regionproposalnetworkforobjectdetection.InICCV,pages
Then, weintroduceanothertestingsetting, Œ¥-tuning. In
9287‚Äì9296,2019. 6
this setting, if a detector is tested on another domain dif-
[5] GarrickBrazil,AbhinavKumar,JulianStraub,NikhilaRavi,
ferent from the train domain, the Query FFN is fine-tuned Justin Johnson, and Georgia Gkioxari. Omni3d: A large
by1%oftrainingdatafromthetestdomain. Theresultsof benchmark and model for 3d object detection in the wild.
thisŒ¥-tuningsettingarereportedinthe5 ‚àº 7 columns InCVPR,pages13154‚Äì13164,2023. 1,2,6
st th
ofTable7. Wecanobservethatwhenfine-tunedwithonly [6] HolgerCaesar,VarunBankiti,AlexHLang,SourabhVora,
ahandfulofdata, theperformanceofUniMODEbecomes Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan,
muchmorepromising. Thisresultsuggeststhatthesuperi- Giancarlo Baldan, and Oscar Beijbom. nuscenes: A mul-
ority of UniMODE in serving as a foundational model. It timodal dataset for autonomous driving. In CVPR, pages
11621‚Äì11631,2020. 6,7
can benefit practical applications by only incorporating a
[7] Laurene Claussmann, Marc Revilloud, Dominique Gruyer,
littletrainingdatafromthetestdomain.
andSe¬¥bastienGlaser.Areviewofmotionplanningforhigh-
4.4.Visualization wayautonomousdriving. IEEETransactionsonIntelligent
TransportationSystems,21(5):1826‚Äì1848,2019. 2
WevisualizethedetectionresultsofUniMODEonvarious [8] AndreasGeiger, PhilipLenz, andRaquelUrtasun. Arewe
sub-datasetsinOmni3D.Theillustratedresultsareshownin ready for autonomous driving? the kitti vision benchmark
Fig.5,whereUniMODEperformsquitewellonallthedata suite. InCVPR,pages3354‚Äì3361,2012. 1,4,6
samples, and accurately captures the 3D object bounding [9] PeixuanLiandHuaiciZhao. Monocular3ddetectionwith
boxesunderbothcomplexindoorandoutdoorscenarios. geometricconstraintembeddingandsemi-supervisedtrain-
Besides, as mentioned before, training instability is the ing. IEEE Robotics and Automation Letters, 6(3):5565‚Äì
primarychallengeinunifyingdiversetrainingdomains. To 5572,2021. 1
explainthemeaningoftraininginstabilitymoreclearly,we [10] YingyanLi,YuntaoChen,JiaweiHe,andZhaoxiangZhang.
Denselyconstraineddepthestimatorformonocular3dobject
present the loss curves of UniMODE and an unstable case
detection. InECCV,pages718‚Äì734,2022. 2
ofPETRinFig.6. Itcanbefoundthatthereexistssudden
[11] Yinhao Li, Zheng Ge, Guanyi Yu, Jinrong Yang, Zengran
loss boost and continuous gradient collapse in the training
Wang,YukangShi,JianjianSun,andZemingLi. Bevdepth:
ofPETR,whileUniMODEconvergessmoothly.
Acquisitionofreliabledepthformulti-view3dobjectdetec-
5.ConclusionandLimitation tion. InAAAI,pages1477‚Äì1485,2023. 4
[12] Yinhao Li, Zheng Ge, Guanyi Yu, Jinrong Yang, Zengran
In this work, we have proposed a unified monocular 3D Wang,YukangShi,JianjianSun,andZemingLi. Bevdepth:
object detector named UniMODE, which contains several Acquisitionofreliabledepthformulti-view3dobjectdetec-
tion. InAAAI,pages1477‚Äì1485,2023. 2
well-designed techniques to address many challenges ob-
[13] ZhuolingLi,ZhanQu,YangZhou,JianzhuangLiu,Haoqian
servedinunified3Dobjectdetection. Theproposeddetec-
Wang,andLihuiJiang. Diversitymatters: Fullyexploiting
torhasachievedSOTAperformanceontheOmni3Dbench-
depth clues for reliable monocular 3d object detection. In
markandpresentedhighefficiency. Extensiveexperiments
CVPR,pages2791‚Äì2800,2022. 1,7
are conducted to verify the effectiveness of the proposed
[14] Zhuoling Li, Haohan Wang, Tymosteusz Swistek, En Yu,
techniques. The limitation of the detector is its zero-shot
andHaoqianWang.Efficientfew-shotclassificationviacon-
generalization ability on unseen data scenarios is still lim- trastivepre-trainingonwebdata. IEEETransactionsonAr-
ited. In the future, we will continue to study how to boost tificialIntelligence,2022. 2
the zero-shot generalization ability of UniMODE through [15] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-
strategieslikescalingupthetrainingdata. hao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer:
9
eulaV
ssoLLearning bird‚Äôs-eye-view representation from multi-camera tionandmobilemanipulation. InAAAI,pages1507‚Äì1514,
imagesviaspatiotemporaltransformers. InECCV,pages1‚Äì 2011. 1
18,2022. 2,3,6 [31] HaiyangWang,ShaocongDong,ShaoshuaiShi,AoxueLi,
[16] Zhuoling Li, Chunrui Han, Zheng Ge, Jinrong Yang, En JiananLi,ZhenguoLi,LiweiWang,etal.Cagroup3d:Class-
Yu,HaoqianWang,HengshuangZhao,andXiangyuZhang. aware grouping for 3d object detection on point clouds.
Grouplane: End-to-end3dlanedetectionwithchannel-wise NeurIPS,35:29975‚Äì29988,2022. 4
grouping. arXivpreprintarXiv:2307.09472,2023. 1 [32] Tai Wang, Xinge Zhu, Jiangmiao Pang, and Dahua Lin.
[17] Zhuoling Li, Chuanrui Zhang, Wei-Chiu Ma, Yipin Zhou, Fcos3d: Fullyconvolutionalone-stagemonocular3dobject
Linyan Huang, Haoqian Wang, SerNam Lim, and Heng- detection. InICCV,pages913‚Äì922,2021. 2,6
shuangZhao. Voxelformer: Bird‚Äôs-eye-viewfeaturegener- [33] Tai Wang, ZHU Xinge, Jiangmiao Pang, and Dahua Lin.
ationbasedondual-viewattentionformulti-view3dobject Probabilisticandgeometricdepth: Detectingobjectsinper-
detection. arXivpreprintarXiv:2304.01054,2023. 1 spective. InCoRL,pages1475‚Äì1485,2022. 6
[18] Yingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun. [34] Xudong Wang, Zhaowei Cai, Dashan Gao, and Nuno Vas-
Petr: Positionembeddingtransformationformulti-view3d concelos. Towardsuniversalobjectdetectionbydomainat-
objectdetection. InECCV,pages531‚Äì548,2022. 2,6 tention. InCVPR,pages7289‚Äì7298,2019. 2
[19] ZechenLiu,ZizhangWu,andRolandTo¬¥th. Smoke:Single- [35] Zhenyu Wang, Ya-Li Li, Xi Chen, Hengshuang Zhao, and
stagemonocular3dobjectdetectionviakeypointestimation. ShengjinWang. Uni3detr:Unified3ddetectiontransformer.
InCVPRWorkshops,pages996‚Äì997,2020. 1,6 InNeurIPS,2023. 2
[20] ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeicht- [36] XiaoyangWu,ZhuotaoTian,XinWen,BohaoPeng,Xihui
enhofer,TrevorDarrell,andSainingXie. Aconvnetforthe Liu, Kaicheng Yu, and Hengshuang Zhao. Towards large-
2020s. InCVPR,pages11976‚Äì11986,2022. 6 scale 3d representation learning with multi-dataset point
[21] YanLu,XinzhuMa,LeiYang,TianzhuZhang,YatingLiu, prompttraining. arXivpreprintarXiv:2308.09718,2023. 2,
Qi Chu, Junjie Yan, and Wanli Ouyang. Geometry uncer- 5
taintyprojectionnetworkformonocular3dobjectdetection. [37] FisherYu,DequanWang,EvanShelhamer,andTrevorDar-
InICCV,pages3111‚Äì3121,2021. 6 rell. Deeplayeraggregation. InCVPR,pages2403‚Äì2412,
[22] Jiageng Mao, Shaoshuai Shi, Xiaogang Wang, and Hong- 2018. 6
sheng Li. 3d object detection for autonomous driving: A [38] YunpengZhang,JiwenLu,andJieZhou. Objectsarediffer-
reviewandnewoutlooks. arXivpreprintarXiv:2206.09474, ent:Flexiblemonocular3dobjectdetection.InCVPR,pages
2022. 2 3289‚Äì3298,2021. 1
[23] Dennis Park, Rares Ambrus, Vitor Guizilini, Jie Li, and [39] XingyiZhou,DequanWang,andPhilippKra¬®henbu¬®hl. Ob-
Adrien Gaidon. Is pseudo-lidar needed for monocular 3d jectsaspoints. arXivpreprintarXiv:1904.07850,2019. 3
objectdetection? InICCV,pages3142‚Äì3152,2021. 2 [40] XingyiZhou,VladlenKoltun,andPhilippKra¬®henbu¬®hl.Sim-
[24] TaesungPark,Ming-YuLiu,Ting-ChunWang,andJun-Yan ple multi-dataset detection. In CVPR, pages 7571‚Äì7580,
Zhu. Semanticimagesynthesiswithspatially-adaptivenor- 2022. 2
malization. InCVPR,pages2337‚Äì2346,2019. 8 [41] XizhouZhu,WeijieSu,LeweiLu,BinLi,XiaogangWang,
[25] Liang Peng, Xiaopei Wu, Zheng Yang, Haifeng Liu, and andJifengDai. Deformabledetr: Deformabletransformers
DengCai. Did-m3d: Decouplinginstancedepthformonoc- forend-to-endobjectdetection. InICLR,2020. 3,4
ular3dobjectdetection. InECCV,pages71‚Äì88,2022. 2
[26] Cody Reading, Ali Harakeh, Julia Chae, and Steven L
Waslander. Categorical depth distribution network for
monocular3dobjectdetection. InCVPR,pages8555‚Äì8564,
2021. 4
[27] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit
Kumar,MiguelAngelBautista,NathanPaczan,RussWebb,
and Joshua M Susskind. Hypersim: A photorealistic syn-
thetic dataset for holistic indoor scene understanding. In
ICCV,pages10912‚Äì10922,2021. 6
[28] DanilaRukhovich,AnnaVorontsova,andAntonKonushin.
Imvoxelnet: Image to voxels projection for monocular and
multi-viewgeneral-purpose3dobjectdetection. InWACV,
pages2397‚Äì2406,2022. 1,6
[29] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao.
Sunrgb-d:Argb-dsceneunderstandingbenchmarksuite.In
CVPR,pages567‚Äì576,2015. 4,6
[30] StefanieTellex,ThomasKollar,StevenDickerson,Matthew
Walter,AshisBanerjee,SethTeller,andNicholasRoy. Un-
derstandingnaturallanguagecommandsforroboticnaviga-
10