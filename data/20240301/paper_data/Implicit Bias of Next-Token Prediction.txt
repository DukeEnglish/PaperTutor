Implicit Bias of Next-Token Prediction
Christos Thrampoulidis
University of British Columbia
February 29, 2024
Abstract
Next-token prediction (NTP), the go-to training paradigm in training large language models,
involves predicting the next token in a sequence. Departing from traditional one-hot classification,
in NTP, multiple tokens with varying frequencies follow each given context. This work frames
NTP training as cross-entropy minimization over distinct contexts, each associated with a sparse
empirical probability vector across a finite vocabulary. It then addresses the following question: do
gradient-basedoptimizersexhibitabiastowardssolutionswithspecificstructureastheNTPtraining
loss reaches its lower bound (entropy)? Specifically, for linear NTP models trained using gradient
descent(GD),wemakethefollowingcontributions: Firstly,wedetermineNTP-separabilityconditions
on the data, under which GD can attain its lower bound. We also demonstrate that these conditions
hold under overparameterization. Secondly, we establish that the parameters of GD projected onto
an appropriate data subspace converge to the unique solution of a system of linear equations, which
requires the logits’ difference of in-support tokens to be equal to the log-ratio of their respective
probabilities. Meanwhile, on the orthogonal subspace, the parameters diverge and converge in the
direction of the solution of a max-margin quadratic program, minimizing the Euclidean norm of
parameters satisfying the NTP-separability conditions. Akin to prior research on implicit bias of
one-hot classification, our work opens exciting avenues for future research that can lead to better
understanding optimization, generalization and robustness principles of models trained with NTP.
1 Introduction
1.1 Motivation
Next-token prediction (NTP) has emerged as a pivotal paradigm in language modeling tasks, revolu-
tionizing various applications such as machine translation, text-summarization, and language generation
[RNS+18]. In NTP, models are trained to predict the most probable token given a sequence of preceding
tokens, commonly referred to as the context. Specifically, the objective is to learn a mapping from the
input context to the probability distribution over the (finite) vocabulary of possible tokens, enabling the
model to generate a token that is contextually appropriate [BDV00, BB00].
Over the past couple of years or so, the paradigm has witnessed remarkable empirical success through
itsutilizationonlarge-scaledeep-learningarchitecturestrainedonvastcorporaofdata[RNS+18,RWC+19,
TMS+23, Ope23]. This has resulted in unprecedented advances in the field, and the swift integration
of these advanced language models into society [Ope22]. Concurrently, researchers have raised critical
concerns about robustness, interpretability, and bias issues arising from our limited understanding of
the fundamental operational principles of these models [BHA+21, Bel24]. Although these issues have
garnered significant research attention, a comprehensive theory elucidating the fundamentals of large
language models, including key components such as the NTP paradigm and the transformer architecture,
with respect to their optimization and generalization principles, remains elusive.
1.2 Approach
This paper initiates an investigation of the optimization principles of the NTP training paradigm.
NTP training. Formalizing the NTP paradigm, consider autoregressive model q parameterized by θ
θ
trained to predict the next-token on sequences of length T using the cross-entropy (CE) loss:
min Eˆ [ ∑ −log(q (z ∣z ,...,z ))]. (1)
θ
z∼Tn θ t 1 t−1
t∈[T]
1
4202
beF
82
]GL.sc[
1v15581.2042:viXraHere, sequences z = (z ,...,z ) consist of tokens z from a finite vocabulary V = {1,...,V} and Eˆ is
1 T t
expectation over training set T of n such sequences sampled from some underlying true distribution over
n
sequences. Typically, the model q outputs probability of the next token computed via softmax applied
θ
on output logits, which are computed by projecting d-dimensional embeddings h to the V-dimensional
θ′
space with a trainable linear decoder W ∈RV×d. Mathematically, 1
1
q (z ∣z ,...,z )=S (Wh (z ,...,z )) = .
θ t 1 t−1 zt θ′ 1 t−1 1+∑ exp((e −e )⊺Wh (z ,...,z ))
z′∈V z′ zt θ′ 1 t−1
z′≠zt
The CE loss is then minimized over θ=(W,θ′) using gradient-based methods, e.g. SGD, Adam.
Implicit-bias question. We pose the following question:
Given training set T , what are the structural properties of the weights θ found by minimizing the NTP
n
cross-entropy objective with gradient-based methods?
Drawing inspiration from a substantial body of research in one-hot supervised classification 2 (e.g.
[ZBH+17, BRT18, SHN+18, JT18]), we specifically target this question in the overparameterized setting,
where the autoregressive CE training objective (1) may have an infinite number of solutions, representing
an infinite number of models θ that minimize the training loss. The central challenge is to discern the
particular solution the optimizer is inherently biased towards. Since this ’bias’ is not explicitly introduced
through regularization but is instead ingrained in the training objective and algorithmic structure, it is
termed ‘implicit bias’ [NTS14].
The exploration of implicit bias has a long history in the traditional supervised classification setting of
one-hot predictions (see Related Work in Sec. 6). In this conventional scenario, the training set comprises
feature-label pairs (x,y), where x∈Rp is a continuous feature, and y represents its unique label. The
optimization process minimizes the following training objective (over W,θ′):
Eˆ [−log(S (Wh′(x)))]. (2)
(x,y) y θ
NTP vs one-hot prediction. Initially, it may seem that the NTP training setting is indistinguishable
from traditional one-hot prediction: both involve minimizing the same CE loss over a model that
parameterizes probabilities using the softmax of logits. The resemblance becomes more apparent when
focusing on a simplified NTP setting, specifically when the objective involves training solely on the last
token. In this scenario, the summation in (1) considers only the Tth-term:
Eˆ [−log(S (Wh (z ,...,z )))]. (3)
z zT θ 1 T−1
Here, the context serves as the feature, and the next token serves as the label. Despite the apparent
similarity in training objectives between NTP and traditional one-hot classification settings, a crucial
distinction lies in the nature of the training data that sets these two scenarios apart.
Specifically, in the traditional setting, each feature (e.g., an image) is assigned a single label (e.g., an
image category). In contrast, in the NTP setting, contexts z ,...,z of finite length sampled from finite
1 t−1
vocabularies are expected to be repeated in the (vast) training set, potentially multiple times, each time
followed by different tokens z . Consequently, the NTP objective involves training over m≤n distinct
t
(non-repetitive) contexts, each followed by a multitude of possible next tokens, appearing at varying
frequencies. For instance, in a language training corpus, the context "She is excellent at her role
as a" may be followed by next tokens such as "doctor," "lawyer," or "mother," each with different
frequencies. Importantly, certain tokens in the vocabulary may not appear after a given context; e.g., in
the aforementioned example, tokens like "run," "and," etc., will not follow.
Model. We delve into the study of NTP training over a finite vocabulary employing the following model.
Given a large training set of n total contexts, we identify m≤n distinct contexts. Each distinct context
j ∈[m] is linked to a V-dimensional empirical probability vector pˆ , which encodes the frequency with
j
which each vocabulary token follows the context throughout its occurrences in the training set. Crucially,
1Throughout,ev isthev-thstandardbasisvectorinRV,andSz(u)∶=e⊺ zS(u)isthez-thindexofthesoftmaxoutput.
2In NTP, the ground-truth next token is inherently embedded within the underlying text (i.e., the labels are implicit),
thusstrictlyspeaking,itfallsundertheself-supervisedlearningparadigm[RNS+18]. However,theutilizationoftheCE
trainingobjectivein(4)bearsstrikingresemblancetosupervisedtraining. Inthispaper,weleveragethisresemblanceand
essentiallyregardtheNTPparadigmasaninstanceofsupervisedlearning. Atthesametime,wemakeitclearthatitis
distinctfromclassicalone-hotencodingsupervision.
2the probability vectors pˆ are sparse, indicating that the support set S of pˆ satisfies ∣S ∣ ≪ ∣V∣ = V.
j j j j
In the extreme case where ∣S ∣ = 1 for all j ∈ [m], the probability vector pˆ becomes an indicator or
j j
one-hot-encodingvector,leadingtoascenarioreminiscentofthetraditionalclassificationsettingdescribed
earlier. However, we advocate that such an extreme case is essentially improbable in practical language
settings.
In fact, framing NTP in this manner reveals a connection between this setting and supervised
classification with soft labels. Unlike most typical benchmark datasets containing hard labels, a line of
research advocates for training models on datasets where each example is associated with a vector of soft
labels (rather than a one-hot vector), such as by averaging multiple annotators’ hard labels [PBGR19].
Soft-label classification also arises in knowledge distillation [HVD15] and label smoothing [SVI+16]. Here,
we assert that soft labels emerge naturally in the NTP setting (specifically, last-token prediction), with a
notablecharacteristicbeingthesparsityofthevectorpˆ ofsoftlabels. Giventhisconnection, ouranalysis
j
can also be interpreted (more broadly) as investigating the implicit bias of soft-label classification.
1.3 Contributions
Recognizing the distinction between NTP and one-hot classification, we initiate a study of implicit bias
questions within the NTP setting. To facilitate this, we utilize the model presented in the previous
section and introduce specific simplifying assumptions, detailed below, with plans to relax them in future
research.
Setting: Last-token, linear model, overparameterization and GD. We focus on objective (3)
that trains a model over predicting the last token and we train the objective using gradient descent
(GD). Throughout our analysis, we adopt a ‘top-down’ approach, assuming that only the decoding
matrix W ∈RV×d is trained, while embeddings remain fixed. This separation allows us to disentangle
the intricacies of the NTP training objective and the underlying architecture responsible for producing
embeddings, enabling a focused examination of the former. Now, the resulting model for the logits is
thus linear and the training objective (3) is convex. Our primary goal is to determine the structure of
the decoder weights W at the end of training. This question is posed within an overparameterized
k
setting, where the dimensionality d is large, resulting in multiple solutions to minimizing (3). We seek to
determine the implicit bias of GD towards one of these solutions.
NTP separability. Concretely, we show that when the embedding dimension d exceeds the number
m of distinct contexts, then almost all datasets satisfy two key conditions: NTP -compatibility and
H
NTP-separability. The former condition mandates that the difference in logits between in-support tokens
equals the log-ratio of their respective probabilities. The latter condition necessitates that the logits of
in-support tokens (i.e. those belonging to S ) are equivalent, while the logits of in-support tokens are
j
strictly greater than those of out-of-support tokens. We show that when these two conditions are met, the
CE loss can approach its lower bound, the empirical conditional entropy, by traversing certain possible
directions towards infinity. This raises the implicit-bias question: which direction is favored by different
iterative algorithms?
NTP-SVM.TogaininsightontheimplicitbiasinNTPsettings,weinvestigatethedirection3 Ŵ /∥Ŵ ∥
λ λ
of the ridge-regularized CE minimizer Ŵ with weight λ. In the limit of vanishing regularization λ→0,
λ
we show that the direction approaches the direction of Wmm, which is the minimum Euclidean-norm
classifier matrix that satisfies the NTP-separability. Wmm generalizes the traditional multiclass max-
margin classifier by incorporating additional subspace constraints Wmm∈F⊥ arising from the potential
presence of multiple possible next-tokens (∣S ∣>1) per context.
j
Implicit bias of GD. Next, we show that under NTP -compatibility and NTP-separability, in the
H
limit of iterations k→∞, the GD iterates grow undoubtedly in norm and converge to a finite W⋆ within
a data subspace F, while simultaneously aligning with Wmm in the complementary subspace F⊥. The
finite component W⋆ ∈F can be found by solving the system of linear equations associated with the
NTP -compatibility condition.
H
Finally, we verify our findings via experiments on synthetic data and discuss future work. In terms
of techniques used in our proofs, after properly framing the NTP paradigm as described in the Model
paragraphinSection1.2(furtherdetailsareavailableinSection2),weleverageratherstandardtechniques
from the implicit bias literature, specifically [JT18, JDST20], with appropriate extensions tailored to the
unique characteristics of the NTP setting (see also Related Work in Sec. 6).
3Unlessotherwisespecified∥⋅∥denotesEuclideannorm. WealsoletW ∶=W/∥W∥.
32 Setup
Let vocabulary V =[V]∶={1,...,V} represent a finite set of V =∣V∣ tokens (e.g. words, characters). Let
z =(z ,...,z ) denote a sequence of t tokens z ∈V. This paper focuses on prediction of the last T-th
1∶t 1 t t
token z given context z ∶=z . To simplify notation, let x=z denote the context and denote the
T <t 1∶t−1 <t
last token simply by z.
WehaveaccesstoatrainingsetconsistingofnsequencesT ∶={(x ,z )} ,suchthatx ∈X ∶=VT−1
n i i i∈[n] i
and z ∈ V for all i ∈ [n]. Let h ∶ X → Rd an embedding map that maps contexts (i.e., sequences of
i
T −1 tokens) to d-dimensional embeddings. The map h can be parameterized (e.g. by a transformer),
but this paper assumes that it is fixed, i.e., not trainable. The next-token is predicted via a linear
model f ∶X →RV parameterized by decoding matrix W ∈RV×d, such that f (x)=Wh(x). When
W W
the model output passes through a softmax, it defines the model’s probability mass function for the
next-token prediction, given as qˆ (⋅∣x)=S(f (x)), where S(⋅)∶RV →∆V−1 is the softmax and ∆V−1
W W
is the V-dimensional simplex. The decoder is trained by minimizing the empirical CE loss
1
CE(W)∶= ∑ −log(qˆ (z ∣x )). (4)
W i i
n
i∈[n]
.
Distinct sequences and next-token distributions. Given dataset T we denote x¯ ,...,x¯ the
n 1 m
m≤n distinct contexts among the (large number of) total n contexts x ,...,x within T . Let πˆ be
1 n n j
the empirical probability of distinct context x¯ . That is, 1≤n⋅πˆ ≤n is the number of contexts x that
j j i
equal x¯ . Furthermore, for each distinct context x¯ ,j ∈[m] let pˆ ∈∆V−1 denote the probability vector
j j j
of conditional next-token distribution, i.e.,
pˆ ∶=pˆ(z∣x¯ ),z∈V,j ∈[m].
j,z j
In other words, n⋅πˆ ⋅pˆ is the number of occurences of token z as a follow-up to context x¯ . Finally,
j j,z j
we denote the support set and size of the support set of these conditional distributions as
S ∶={z∈V∣pˆ >0} and S ∶=∣S ∣.
j j,z j j
Tokens z∈S and v∉S are referred to as ’in-support’ and ’out-of-support’ respectively. Onwards, we
j j
introduce the following mild assumption on the support-sets S .
j
Assumption 1 (Not all tokens are likely after every context). The dataset T is such that for at least
n
one j ∈[m] it holds S <V. This assumption holds throughout the paper.
j
In other words, there exists at least one distinct context x¯ such that at least one token v∈V does
j
not appear as the next-token z of context x =x¯ . This assumption appears rather mild for rich enough
i i j
vocabulary and reasonable in a language setting, e.g. when tokens represent words.
With the above notation, we can express the training loss in Eq. (4) as
CE(W)=− ∑ πˆ ∑pˆ log(S (Wh(x¯ )))=− ∑ πˆ ∑ pˆ log(S (Wh¯ )), (5)
j j,z z j j j,z z j
j∈[m] z∈V j∈[m] z∈Sj
where, in the last line we defined the shorthand h¯ =h(x¯ ) 4. Similarly, we let h =h(x ),i∈[n]. With
j j i i
some abuse of notation, we then obtain the following equivalent descriptions of the training set
{(x ,z )} =∶T ≡T ∶={(h¯ ,πˆ ,pˆ )} .
i i i∈[n] n m j j j,z∈V j∈[m]
Please refer to Sec. B.1 in the appendix for a complete list of notations used in this paper.
Entropy lower-bound. Recall pˆ(⋅∣x) denotes the empirical conditional probability given context x∈T ,
n
i.e. pˆ(z∣x)=pˆ for j ∶x =x and all z∈V. Then, the empirical T-gram entropy (referred to hereafter as
j,z j
entropy for simplicity) of the data is [Sha48]:
H ∶=H∶=Eˆ [−log(pˆ(z∣x))]=− ∑ ∑ πˆ pˆ log(pˆ ).
T (x,z)∼Tn j j,z j,z
j∈[m]z∈Sj
Entropy lower bounds the CE loss since the KL divergence KL(pˆ∣∣qˆ )≥0 is nonnegative and
W
CE(W)=H+KL(pˆ∣∣qˆ ). (6)
W
4Forfixedembeddingmap,wecanequivalentlyidentifydistinctcontextsbasedonembeddingshi (ratherthanrawtoken
(sub)sequencesxi).
43 When can empirical CE reach the entropy lower-bound?
Thefirstquestionweaskis: UnderwhatconditionsonthetrainingdatacanCElossreachitslower-bound
(i.e., entropy)?
To answer this, note by (6) that CE(W) = H if and only if (iff) for all j ∈ [m] and all z ∈ V:
qˆ (z∣x¯ )=pˆ . Equivalently, for all j ∈[m]:
W j j,z
S (Wh¯ )=pˆ , ∀z∈S , (7a)
z j j,z j
S (Wh¯ )=0, ∀v∉S . (7b)
v j j
Beginning with (7a), this is satisfied iff the training data satisfy the NTP -compatibility condition,
H
which is formally defined below. Recall e denotes the v-th standard basis vector in RV.
v
Definition 1 (NTP -compatible). We say that training data T are NTP-entropy-compatible if there
H m
exists V ×d matrix Wp satisfying the following:
∀j ∈[m],z≠z′∈S ∶ (e −e )⊺Wph¯ =log(pˆ /pˆ ). (8)
j z z′ j j,z j,z′
We comment on the independence of the constraints by making the following elementary remark:
Fix any j ∈[m]. Then, the set of constraints (as expressed in Eq. (8)) for all z≠z′ ∈S (yielding (Sj)
j 2
constraints in total) is equivalent to the set of the same constraints for any anchor z ∈S and z′≠z ∈S ,
j j j j
i.e. an effective total of S −1 linearly independent constraints for each j ∈[m]. Additionally, note that
j
the system of equations in (8) constrains Wp with respect to a specific subspace of V ×d matrices:
F =span({(e −e )h¯⊺ ∶ z≠z′∈S ,j ∈[m]}), (9)
z z′ j j
that is defined in terms of the training data, particularly in relation to context embeddings and their
respective support sets. Assuming that Eqs. (8) have a solution, we denote for later reference W⋆∈F as
the unique solution within the subspace F.
Next, we examine Equation (7b), which requires that softmax outputs (or equivalently, output
probabilities) be zero for tokens that never occur following a fixed context throughout the dataset.
According to Assumption 1, this constraint is active for at least one j ∈[m]. Furthermore, due to the
strict positivity of softmax outputs, the constraint is never satisfied for finite W. Consequently, for
all finite W, there exists a gap between the cross-entropy loss and its lower bound, i.e., CE(W)>H.
However, it is possible to approach the constraint as the norm of the weights W grows, provided that we
move in the appropriate direction. The following condition formalizes these suitable directions.
Definition 2 (NTP-separable). We say that training data T are NTP-separable 5 if there exists V ×d
m
matrix Wd satisfying the following:
∀j ∈[m],z≠z′∈S ∶ (e −e )⊺Wdh¯ =0 (10a)
j z z′ j
∀j ∈[m],v∉S ∶ (e −e )⊺Wdh¯ ≥1. (10b)
j z v j
As before, we begin by considering the number of linearly independent constraints in the definition
above. It is easy to see that the constraints in (10) can be equivalently expressed by enforcing (10a)
and (10b) for an anchor z ∈S and all z′∈S /{z } and v∉S , respectively. Consequently, there exist
j j j j j
effectively V −1 linearly independent constraints per context j ∈[m].
We now discuss the interpretation of these constraints. The first set of subspace constraints (equalities
in(10a))projectWd ontothesubspaceF⊥,whichistheorthogonalcomplementofthesubspaceF defined
in (9). By doing so, these constraints ensure that Wd leaves the softmax probabilities of possible next
tokens(insetS )intact,andfullydeterminedbyWp aspertheNTP -compatibilitycondition. Formally,
j H
Wp+Wd continues satisfying (8). Moving on the second set of halfspace constraints (inequalities in
(10b)), we can interpret these using Kesler’s construction as enforcing linear separability in the space
RV×d [HSD00]: Specifically, each d-dimensional context embedding h¯ is mapped to S (V −S ) higher-
j j j
dimensional points (e −e )h¯⊺,z ∈S ,v ∉S . These points collectively for all j ∈[m] must lie within
z v j j j
5While Definition 2 describes NTP linear-separability, we choose to simplify the wording by omitting “linear". A more
generalformulationofthisconditionfornonlinearmodelsisstraightforward: itrequirestheexistenceofbothclassifier
weights and model weights θ that parameterize the embeddings h¯ j =h θ(x¯j), satisfying Eq. (10) simultaneously. An
analogousremarkholdsforDefinition1.
5the interior of the same halfspace induced by the hyperplane ⟨Wd,⋅⟩=0. Refer to Fig. 1(Left) and its
caption for an alternative interpretation of the separating hyperplanes ⟨(Wmm)⊺e ,⋅⟩ in Rd (illustation
v
in d=2). Although the requirement for NTP-separability might seem stringent in general, we will later
demonstrate that such a condition is facilitated by overparameterization.
The impact of NTP-separability on the softmax probability outputs can be understood algebraically
by considering W ∶=γWd and v∉S . We have:
γ j
−1
S v(Wγh¯ j)=(∑ eγ(ez−ev)⊺Wdh¯ j + ∑ eγ(ev−e v′)⊺Wdh¯ j) ≤e−γ, (11)
z∈Sj v′∉Sj
where the inequality follows by using (10b). The upper bound above approaches 0 as γ →∞, thus (7b)
holds asymptotically in γ.
Taking into account the observations made above, the satisfaction of both conditions guarantees
convergence of the cross-entropy loss CE to H. This is formalized in the proposition below.
Proposition 1. Assume training data T is NTP -compatible and NTP-separable, with the respective
m H
matrices Wp and Wd satisfying conditions (8) and (10). While all finite W satisfy CE(W) > H, it
holds for Wγ =Wp+γ⋅Wd that
CE(Wγ)—γ→ ——+∞
→H.
Hence, CE approaches its lower-bound in the limit of a direction Wd∶=Wd/∥Wd∥ and offset Wp
satisfying the constraints of NTP-separability and NTP-compatibility, respectively. In other words,
parameter weights W that minimize the CE loss consist of two components: a finite projection WF ∶=
P F(W)=W⋆ ontothedatasubspaceF andaninfinite-normcomponentontotheorthogonalcomplement
F⊥ in the direction of Wd.
3.1 The role of overparameterization
Thissectionaimstoaddressthequestionofwhenwecanexpectconstraints(8)and(10)tohavesolutions.
We will show that overparameterization provides a sufficient condition for their solvability.
Start with the halfspace constraints in (8) for NTP -compatibility. These can be compactly ex-
H
pressed as E j,zjWph¯
j
=a j,z, where E
j,zj
∈R(Sj−1)×V has rows e
zj
−e′
z
and a
j,zj
∈R(Sj−1) has entries
log(pˆ /pˆ ) for some anchor z ∈ S . Now, since the rows of E are linearly independent, the
j,zj j,z′ j j j,zj
question becomes equivalently that of determining when the following system of linear equations has a
solution:
Wp[h¯ ,...,h¯ ]=[E† a ,...,E† a ]. (12)
1 m 1,z1 1 m,zm m
When d>m and the d×m embedding matrix H¯ =[h¯ ,...,h¯ ] is full rank (m), then (12) has a solution.
1 m
Thus, in this case, we are guaranteed there exists Wp such that condition (8) holds. Moreover, in this
case, H¯⊺ has a nullspace, implying the existence of an infinite number of solutions to (8). These solutions
take the form Wp=W⋆+Wp, where W⋆∈F is the unique solution onto the subspace, and Wp∈F⊥. In
⊥ ⊥
contrastto(8),theconstraintsin(10)involvelinearinequalities. However,asufficientproxyforfeasibility
in this case is that the corresponding system of equations (instead of inequalities) has a solution. By
following the exact same argument as before, we arrive at the same sufficient conditions for the existence
of a solution Wd. We summarize these findings.
Lemma 1 (Overparameterization implies NTP-separability). Assume overparameterization d>m and
full-rank embedding matrix H¯ ∈Rd×m. Then, there exists an infinite number of solutions Wp and Wd
that satisfy conditions (8) and (10), respectively.
Lemma 1 asserts that overparameterization6 d > m, which also generically favors full-rankness of
the embedding matrix [Ver11], implies both NTP -compatibility and NTP-separability. Combined with
H
Proposition 1, it also informs us that there are infinitely many possible directions Wd along which the
empiricalCElossapproachesH, givingrisetotheimplicitbiasquestion: Foraspecificiterativealgorithm
aimed at minimizing the CE loss, which direction does it prefer? We will address this question in the
remainder of the paper.
6The necessity for such large d can be mitigated through the utilization of non-linear architectures, in which the total
numberofparameterscanbeincreasedbyaugmentingthewidthordepth,ratherthandirectlymodifyingtheembedding
dimensiondasinlinearmodels.
6Remark 1. In the trivial case where S = 1 for all j ∈ [m], the setting reduces to one-hot multiclass
j
classification. In this scenario, it is well known that the entropy lower bound is zero and is attained iff
the data is (multiclass) linearly separable. This observation can be trivially recovered from our preceding
discussion: when S =1,j ∈[m], F reduces to the empty set, and NTP-separability simplifies to traditional
j
multiclass separability. For binary classification, [Cov65] showed that d/m>1/2 is sufficient and necessary
fordataingeneralpositiontobelinearlyseparable. Morerecently,severalworkshaveextendedthisanalysis
to structured (random) data, including [CS18, SAH18, MRSY19, DKT22, MKLZ20]. The exact threshold
in corresponding mutliclass settings is more intricate, but [KT21, CMV+23, TB23, ÇLO24] have made
progress in this direction. An interesting question is determining exact thresholds for NTP-separability,
which would improve upon the sufficient condition of Lemma 1.
4 Regularization path
This section investigates the implicit bias for NTP classification by examining the minimization of CE
loss through iterates defined as:
Ŵ ∶=argmin CE(W), (13)
B ∥W∥≤B
for an increasing sequence of positive regularization parameters B. In the literature, studies of this form
(for different settings) are known as “regularization-path analyses”, e.g. [RZH03, JDST20]. Note that
̂
(13) involves minimization of a strictly convex function in a bounded domain; thus, W is unique and
B
well-defined. The section’s main result characterizes the limit of Ŵ as B→∞ under NTP-separability
B
and NTP -compatibility. Before stating the theorem, we define the next-token prediction support-vector
H
machines (SVM) problem.
Definition 3 (NTP-SVM). Given NTP-separable training set T , NTP-SVM solves the following:
m
Wmm∶=argmin ∥W∥ subj. to W ∈RV×d satisfying (10a) and (10b). (NTP-SVM)
W
Note NTP-SVM is a stronly convex quadratic program with mV −∑ S linear inequality and
j∈[m] j
∑ S −m linear equality constraints. Equivalently, the direction Wmm can be defined as the classifier
j∈[m] j
that maximizes the margin between in-support and out-of-support tokens while being constrained to lie
on the orthogonal compelemnt F⊥, i.e.
Wmm=argmax ∥W∥=1,W∈F⊥min j∈[m],z∈Sj,v∉Sj(e z−e v)⊺Wh¯ j.
It turns out this direction determines the preferred limiting direction of the regularization path.
Theorem 1 (Implicit bias of the regularization-path). Assume training data T is NTP -compatible
m H
and NTP-separable. Let Ŵ B be defined as in (13). Then, it holds that lim B→∞ ⟨ ∥W Ŵ ̂B B∥, ∥W Wm mm m∥⟩=1.
Proof. We include here a proof sketch. See App. B.3 for the details.
We first show that Ŵ is on the boundary, i.e. ∥Ŵ ∥=B. If not, then ⟨∇CE(Ŵ ),Wmm⟩=0. But,
B B B
a few algebraic manipulations yield ⟨−∇CE(Ŵ ),Wmm⟩ is equal to
B
∑ πˆ ∑ pˆ ( ∑ s (e −e )⊺Wmmh¯ + ∑ s (e −e )⊺Wmmh¯ ),
j j,z j,z′ z z′ j j,v z v j
j∈[m] z∈Sj z′∈Sj,z′≠z v∉Sj
where we denote s ∶=S (Ŵ h¯ )>0,v∈V,j ∈[m]. The first term in the parenthesis is zero by (10a),
j,v v B j
while the second term is strictly positive by (10b), leading to contradiction.
Now, consider a ‘genie’ point W⋆ =W⋆+R(B)⋅Wmm, where W⋆∈F satisfies (8), and R=R(B) is
B
chosen such that ∥W⋆∥=B. We will show that W⋆ attains a small CE loss as B (hence, R) grows. To
B B
do this, denote for convenience the logits ℓ⋆ ∶=e⊺W⋆h¯ and ℓmm∶=e⊺Wmmh¯ for all for v∈V,j ∈[m],
j,v v j j,v v j
and note that e⊺W⋆h¯ =ℓ⋆ +Rℓmm. By using (8) and (10a):
v B j j,v j,v
∑ e−(ℓ⋆ j,z+Rℓm j,zm−ℓ⋆ j,z′−Rℓm j,zm ′)= ∑ e−(ℓ⋆ j,z−ℓ⋆ j,z′)= ∑ pˆ j,z′ = 1 .
pˆ pˆ
z′∈Sj z′∈Sj z′∈Sj j,z j,z
7√
Moreover, using (10b) and defining C ∶=Ve∥W⋆∥M for M ∶= 2⋅max ∥h¯ ∥, gives:
j∈[m] j
∑ e−(ℓ⋆ j,z+Rℓm j,zm−ℓ⋆ j,v−Rℓm j,vm)≤e−R ∑ e−(ℓ⋆ j,z−ℓ⋆ j,v)≤Ce−R.
v∉Sj v∉Sj
Combining the above and using in Eq. (5), yields
1
CE(W⋆)≤ ∑ πˆ ∑ pˆ log( +Ce−R) ≤H+Ce−R, (14)
B j j,z pˆ
j∈[m] z∈Sj j,z
where to simplify the bound, we used log(1+x)≤x,x>0 and the fact that πˆ ,pˆ are probabilities.
j j,z
Next, towards contradiction, we will show that if Ŵ is not in the direction of Wmm, then it incurs a
B
loss that is larger than CE(W⋆). The trick here is to bound the KL divergence term:
B
pˆ
CE(Ŵ )−H= ∑ πˆ ∑ pˆ log( j,z )
B j j,z S (Ŵ h¯ )
j∈[m] z∈Sj z B j
⎛ ⎞
= ∑ πˆ
j
∑ pˆ j,zlog pˆ j,z( ∑ eℓ j,z′−ℓj,z + ∑ eℓj,v−ℓj,z) , (15)
⎝ ⎠
j∈[m] z∈Sj z′∈Sj v∉Sj
where we denote the logits ℓ ∶=e⊺Ŵ h¯ .
j,v v B j
Assume there exists ϵ>0, such that there exists arbitrarily large B satisfying:
∥Wmm∥
∥ Ŵ −Wmm∥>ϵ. (16)
B
B
Define Ŵ = 1 (Ŵ −W⋆), where R′ = R′(B) > 0 can be chosen so that ∥Ŵ∥ = ∥Wmm∥. Further
R′(B) B
choose B large enough so that Eq. (16) guarantees ∥Ŵ−Wmm∥≥ϵ′, for some ϵ′>0. Since Wmm is the
unique minimizer of (NTP-SVM) and ∥Ŵ∥=∥Wmm∥, there exists δ ∈(0,1) and j ∈[m] such that at
least one of the following is true:
(i) ∃z and z′≠z∈S such that ∣(e −e )⊺Ŵ h¯ ∣≥δ
j z z′ j
(ii) ∃z∈S ,v∉S such that (e −e )⊺Ŵ h¯ ≤1−δ.
j j z v j
Case (i): Without loss of generality (e −e )⊺Ŵ h¯ ≤−δ (otherwise, flip z,z′). Thus, ignoring all but
z z′ j
the (j,z,z′)-term in (15) gives
1
eR′δ
CE(Ŵ B)−H≥πˆ jpˆ j,zlog(pˆ j,ze(ℓ j,z′−ℓj,z))≥ log( ), (17)
n n
where we deduced from (8) and the assumption of case (i) that ℓ −ℓ ≥R′δ+log(pˆ j,z′). Compare
j,z′ j,z pˆj,z
(17) with (14). For large enough B, since R,R′ grow at the same rate, it gives that CE(Ŵ )>CE(W⋆),
B B
a contradiction.
Case (ii): We can assume Ŵ ∈F⊥, since otherwise we are in Case (i). Now, again ignoring all but the
(j,z) term in the CE loss for which the assumption holds for some v∉S , we find
j
CE(Ŵ B)−H≥πˆ jpˆ j,zlog(pˆ j,z( ∑ e(ℓ j,z′−ℓj,z)+e(ℓj,v−ℓj,z))).
z′∈Sj
Using P F(Ŵ B) = W⋆ and (8) yields ∑ z′∈Sje(ℓ j,z′−ℓj,z) = pˆj1
,z
. Moreover, by assumption of Case (ii):
eℓj,v−ℓj,z ≥e−R′(1−δ)eℓ⋆ j,v−ℓ⋆ j,z ≥c′e−R′(1−δ), for c′∶=e−∥W⋆∥M. Putting together yields:
c′e−R′(1−δ)
CE(Ŵ )−H≥πˆ pˆ log(1+pˆ c′e−R′(1−δ))≥ .
B j j,z j,z 2n2
wherethesecondinequalityuseslog(1+x)≥ x ,x>0. Comparethiswith(14). ForlargeenoughB,since
1+x
R,R′ grow at the same rate, it holds c′ e−R′(1−δ)>Ce−R. Thus, CE(Ŵ )>CE(W⋆), a contradiction.
2n2 B B
In either case, we arrive at a contradiction, which completes the proof.
Theorem 1 establishes the limiting direction of Ŵ is Wmm, which lies on F⊥. It can also be
B
shown (see Lemma 4 in the appendix) that lim B→∞P F(W B)=W⋆. These together provide a complete
characterization of the implicit bias of (13).
8A
ℱ
C
B
Figure 1: A toy example of NTP training and its implicit bias in d=2. Left: Visualization of the m=3
distinct context-embeddings h¯ (blue markers) and of their associated support sets S ⊂V =[5] of size
j j
3 (colored text below each marker). Each of the colored hyperplanes corresponds to a normal vector
w⊺ ∶=e⊺Wmm,v∈[5]. The blue and green hyperplanes overlap since w =w . Note that context A is
v v 1 5
not followed by tokens (1,5), which however belong to the support-set of contexts B and C. As expected,
the hyperplanes corresponding to w and w separate A from B and C. As another example, A and C
1 5
are followed by token 2, unlike B; hence, w separates B from A and C. Right: Implicit bias of GD in an
2
underparameterized setting. The upper two graphs confirm the predictions of Lemma 2, while the lower
two graphs adhere to the predictions of Theorem 2.
5 Gradient Descent
This section studies the implicit bias of GD. Denote the GD iterates at time k by W = W −
k k−1
η∇CE(W ) for arbitrary initial point W and constant step-size η > 0 small enough to guarantee
k−1 0
descent. The first observation is that the norm of the GD iterates increases with iterations.
Lemma 2 (Norm growth). If training data are NTP -compatible and NTP-separable, then
H
lim CE(W )=H and lim ∥W ∥=∞.
k→∞ k k→∞ k
This is intuitive because the CE loss is convex in W (thus, GD approaches the objective’s infimum
H), and, in view of Proposition 1, the CE loss at all finite W is bounded away from H. The relevant
question then becomes that of determining the limit of the direction of the GD iterates.
Theorem 2 (Implicit bias of GD). Assume NTP -compatible and NTP-separable training data T . Let
H m
W k betheGDiterates. Then,itholdsthatlim k→∞ ⟨ ∥W Wk k∥, ∥W Wm mm m∥⟩=1.Moreover,lim k→∞P F(W k)=W⋆.
The theorem establishes that in the limit of iterations:
W ≈W⋆+∥P (W )∥Wmm,
k ⊥ k
which is analogous to the result we obtained previously for the regularization path. Although its proof is
more involved compared to the proof of Theorem 1, the proof of its main ingredient (Lemma 5 in the
appendix) is conceptually similar: It involves comparing the loss CE(W ) for large iterations k to the
k
loss evaluated at a “genie” point that is chosen so that: (i) On the subspace F, it agrees with W . This
k
is because it is easy to show that P F(W k) converges to W⋆ by standard gradient descent analysis for
convex functions; (ii) On the orthogonal subspace F⊥, it follows the optimal (with respect to accelerating
loss decrease) max-margin direction Wmm∈F⊥. To establish the loss comparison, it is easier to compare
the values of the adjusted loss CE ⊥(W)∶=CE(W)−CE(P F(W)). See Sec. B.2 in the appendix for
details.
We validate the conclusions of our analysis with experiments on synthetic data detailed in App. A.
For illustration, Fig. 1 shows a toy 2D setting (d = 2) with m = 3 distinct contexts, each followed by
S =3 tokensout ofthe V =5 totalnumber oftokensin thevocabulary. Inthe leftsubfigure, weillustrate
j
the embedding geometry along with the associated support sets for each context A, B, and C. The
9colored hyperplanes represent the hyperplanes normal to the corresponding rows e⊺Wmm,v∈[5] of the
v
NTP-SVMsolution. Thesupportsetsofeachcontextarealsocolor-codedtomatchthesehyperplanes. See
caption for interpretation. The right subfigure shows the results of GD training with respect to training
loss, norm growth, alignment with Wmm, and convergence to W⋆ on F. See Sec. A in the appendix
for further implementation details and additional experiments in overparameterized high-dimensional
settings (d>m).
6 Related work
Implicit bias of one-hot prediction. Our work is inspired and is closely related to the literature on
implicit bias of CE loss in one-hot supervised classification. [SHN+18] show that for linear models and
linearly-separable data, GD converges in direction to the max-margin classifier. This result strengthens
older work by [RZH03] who showed that the direction of ridge-regularized CE minimization converges to
the same limit as the regularization weight tends to zero.
Closer to the nature of our results, [JT18] extend the analysis to encompass general binary data and
[JDST20] formalizes the connection the GD and the regularization paths within this broader setting. In
that scenario, the training data are linearly separable only on a certain subspace, and gradient descent is
shown to converge, in direction, towards the max-margin classifier confined within that subspace. On
the orthogonal subspace, it converges to a finite point. While operationally similar, our findings for the
NTP objective in Theorems 1 and 2 cannot be directly derived from their result since our setting is
neither binary nor one-hot. Nevertheless, our proofs follow the same general recipe as these prior works,
particularly [JDST20], and build upon their foundations.
Numerous other works have also built upon these early influential studies, involving extensions to
more complex architectures (e.g. [LL20, JT20, GLSS18a, GLSS18b]), stochastic and adaptive gradient-
based algorithms [NLG+19, PPVF21, SATA22, ALH21, CKS23], and other supervised learning settings
[AF22, TLZO23]; see [VS21] for a survey.
Theimplicitbiasviewpointhasalsocreatedopportunitiestostudygeneralizationinoverparameterized
settings. Specifically, a rich line of recent research activity, starting with [HMRT19, BLLT19, MRSY19,
DKT22], builds on a two-stage approach to generalization analysis of GD. The first stage (optimization),
building on implicit bias, decouples the intricacies of optimization from the generalization query, by
describing the solution learnt by GD in terms of a max-margin classifier. The second stage (statistics),
investigates the generalization properties of this max-margin classifier under different data and model
settings,e.g.,[MNS+20,WT21,Sha22,WMT21,CGB21,KZSS21,SC19,DRSY22,ZSS20,Sha22,WS24].
Thistwo-stagereciperelyingonimplicitbiashasalsobeenadoptedtostudymodelrobustnesstoadversarial
perturbations [JS22, TPT23, CCG23] and out-of-distribution data [TAP21]. Even more recently, the
approachhasalsobeensuccessfullyleveragedtoinvestigatetheimpactofdifferentlossfunctionsonmodel
fairness under imbalanced data [SRKL20, CLB21, KPOT21, BKVT23]. Our results motivate analogous
extensions in the richer setting of NTP training.
Implicit bias in transformers. Our work is also closely related to [TLTO23], where the authors
investigatetheimplicitbiasofself-attentionintransformers. Theinsightputforthin[TLZO23,TLTO23]
is that softmax attention induces implicit-bias behaviors that bear similarities to vanilla implicit bias of
one-hot prediction. Concretely, [TLTO23] studies GD optimization of one-layer self-attention with fixed
decoder and one-hot binary classification. They show that, in the limit, GD finds attention weights that
converge in direction to the solution of an SVM problem that separates optimal tokens from non-optimal
ones. Their non-convex setting introduces locally optimal SVM directions to which GD may converge
depending on initialization. Different to them, the NTP setting that we study involves predictions over
multiple categories and is not one-hot. Also, while they fix the decoder, here, we fix the embeddings.
In these respects their results are rather different. More similarities arise when [TLTO23] replace the
linear decoder with a MLP, which they note can induce multiple optimal tokens per sequence. This leads
them to formulate a more general token-separating SVM program, which similar to ours confines the
separation on a certain data subspace. However, the operational nature of the programs remains different
astheirsoptimizesattentionweightsandseparatestokenswithinasequence, whileoursoptimizesdecoder
weights and separates context embeddings based on their respective support sets. More importantly,
while [TLTO23] only conjectures the convergence of GD to their general SVM program, we leverage
convexity in our setting to prove an analogous statement rigorously. Eventually, as we move lower in our
top-down approach and consider architecture-specific embeddings generated by attention, we anticipate
10to see integration of our ideas with theirs.
Beyond [TLTO23], there is growing recent research investigating optimization and generalization prin-
ciples of transformers, e.g., [SEO+22, EGKZ21, LCW21, vONR+22, ZFB23, ASA+23, LIPO23, ORST23,
TLZO23, TLTO23, DGTT23, TWCD23, CL24]. These efforts predominantly employ a ‘bottom-up’ ap-
proach that involves isolating shallow transformers, often with simplifications such as removing MLPs,
utilizing single heads instead of multiple, and fixing certain parts while training only a subset of trainable
parameters. Most of these studies have focused on classical one-hot supervised settings, and only a
handful (e.g., [TWCD23, TWZ+23]) have seeked extending these ’bottom-up’ analyses to NTP settings.
Yet, their primary emphasis remains on uncovering the role of attention and how attention weights evolve
during training. Instead, our approach uniquely emphasizes the NTP training paradigm itself, shifting
the focus from the intricacies of specific transformer architectures.
Classification with soft labels. Unlike one-hot classification, soft-label classification associates each
example with a probability vector, where each entry represents the likelihood of a corresponding label
characterizing the example. Although arguably less prevalent than one-hot (or hard-label) classification,
soft-label classification arises in various contexts, including modeling human confusion during crowd-
sourcing [PBGR19, SHLMHLQ16, CBW22], knowledge distillation [HVD15], label smoothing [SVI+16],
and mixup [ZCDLP17]. Our model of last-token prediction also falls within this setting. Specifically, our
approach is most closely related to soft-labels generated by averaging annotators’ hard labels [PBGR19],
rather than following the winner-takes-all rule to assign labels. [PBGR19] and follow-up work have
provided empirical evidence that using probabilistic soft labels generated from crowd annotations for
training leads to improved performance in terms of model generalization, calibration, and robustness to
out-of-distribution data. To the best of our knowledge, no prior work has investigated the implicit bias of
gradient descent in this or other soft-label classification settings; thus, our results are of direct relevance
to these contexts as well.
7 Future Work
We foresee several promising avenues for future research.
Even within the current linear setting and GD, some interesting direction are as follows. Firstly, it
is intriguing to identify exact thresholds for NTP linear-separability under distributional assumptions,
akin to previous work on one-hot separability (see Remark 1). Secondly, we envision using our results to
facilitate the study of generalization in NTP settings by examining the statistical properties (in terms
of generalization and robustness) of the NTP-SVM solution. As discussed in Sec. 6, past research
has successfully undertaken similar investigations for one-hot classification. While we acknowledge
the importance of addressing specific challenges inherent to the NTP setting—such as determining an
appropriate measure of generalization that may differ from the winner-takes-all metric, or establishing
suitable statistical models for embeddings that respect the discrete nature of the underlying token
subsequences defining the context—we believe this direction holds promise for further exploration.
In addition to these aspects, essential extensions include relaxing the linearity assumption, thereby
studying the impact of architecture-specific embeddings from both an optimization and statistics per-
spective. Exploring the NTP implicit bias of adaptive algorithms, such as normalized GD and Adam, is
another avenue for future research. The overall goal is to contribute to a deeper understanding of the
intricacies and yield potential improvements in NTP training and its applications.
Acknowledgements. The author expresses his gratitude to Tina Behnia, Yize Zhao, and Puneesh
Deora, for valuable feedback and inspiring discussions.
References
[AF22] Maksym Andriushchenko and Nicolas Flammarion. Towards understanding sharpness-
aware minimization. In International Conference on Machine Learning, pages 639–668.
PMLR, 2022.
[ALH21] Navid Azizan, Sahin Lale, and Babak Hassibi. Stochastic mirror descent on overparame-
terized nonlinear models. IEEE Transactions on Neural Networks and Learning Systems,
33(12):7717–7727, 2021.
11[ASA+23] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What
learning algorithm is in-context learning? investigations with linear models. In The
Eleventh International Conference on Learning Representations, 2023.
[BB00] Samy Bengio and Yoshua Bengio. Taking on the curse of dimensionality in joint distri-
butions using neural networks. IEEE Transactions on Neural Networks, 11(3):550–557,
2000.
[BDV00] Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. A neural probabilistic language
model. Advances in neural information processing systems, 13, 2000.
[Bel24] MikhailBelkin. Thenecessityofmachinelearningtheoryinmitigatingairisk. ACM/JMS
Journal of Data Science, 2024.
[BHA+21] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al.
On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258,
2021.
[BKVT23] Tina Behnia, Ganesh Ramachandra Kini, Vala Vakilian, and Christos Thrampoulidis.
On the implicit geometry of cross-entropy parameterizations for label-imbalanced data.
In International Conference on Artificial Intelligence and Statistics, pages 10815–10838.
PMLR, 2023.
[BLLT19] PeterLBartlett,PhilipMLong,GáborLugosi,andAlexanderTsigler. Benignoverfitting
in linear regression. arXiv preprint arXiv:1906.11300, 2019.
[BRT18] Mikhail Belkin, Alexander Rakhlin, and Alexandre B Tsybakov. Does data interpolation
contradict statistical optimality? arXiv preprint arXiv:1806.09471, 2018.
[CBW22] Katherine M Collins, Umang Bhatt, and Adrian Weller. Eliciting and learning with
soft labels from every annotator. In Proceedings of the AAAI Conference on Human
Computation and Crowdsourcing, volume 10, pages 40–52, 2022.
[CCG23] Jinghui Chen, Yuan Cao, and Quanquan Gu. Benign overfitting in adversarially robust
linear classification. In Uncertainty in Artificial Intelligence, pages 313–323. PMLR, 2023.
[CGB21] Yuan Cao, Quanquan Gu, and Mikhail Belkin. Risk bounds for over-parameterized
maximummarginclassificationonsub-gaussianmixtures. AdvancesinNeuralInformation
Processing Systems, 34:8407–8418, 2021.
[CKS23] Matias D Cattaneo, Jason M Klusowski, and Boris Shigida. On the implicit bias of adam.
arXiv preprint arXiv:2309.00079, 2023.
[CL24] Sitan Chen and Yuanzhi Li. Provably learning a multi-head attention layer. arXiv
preprint arXiv:2402.04084, 2024.
[CLB21] Niladri S Chatterji, Philip M Long, and Peter L Bartlett. When does gradient descent
withlogisticlossfindinterpolatingtwo-layernetworks? The Journal of Machine Learning
Research, 22(1):7135–7182, 2021.
[ÇLO24] Burak Çakmak, Yue M Lu, and Manfred Opper. A convergence analysis of approximate
messagepassingwithnon-separablefunctionsandapplicationstomulti-classclassification.
arXiv preprint arXiv:2402.08676, 2024.
[CMV+23] Elisabetta Cornacchia, Francesca Mignacco, Rodrigo Veiga, Cédric Gerbelot, Bruno
Loureiro, and Lenka Zdeborová. Learning curves for the multi-class teacher–student
perceptron. Machine Learning: Science and Technology, 4(1):015019, 2023.
[Cov65] Thomas M Cover. Geometrical and statistical properties of systems of linear inequalities
with applications in pattern recognition. IEEE transactions on electronic computers,
pages 326–334, 1965.
12[CS18] Emmanuel J Candès and Pragya Sur. The phase transition for the existence of the
maximum likelihood estimate in high-dimensional logistic regression. arXiv preprint
arXiv:1804.09753, 2018.
[DGTT23] Puneesh Deora, Rouzbeh Ghaderi, Hossein Taheri, and Christos Thrampoulidis. On the
optimizationandgeneralizationofmulti-headattention. arXiv preprint arXiv:2310.12680,
2023.
[DKT22] Zeyu Deng, Abla Kammoun, and Christos Thrampoulidis. A model of double descent for
high-dimensional binary linear classification. Information and Inference: A Journal of
the IMA, 11(2):435–495, 2022.
[DRSY22] Konstantin Donhauser, Nicolo Ruggeri, Stefan Stojanovic, and Fanny Yang. Fast rates
for noisy interpolation require rethinking the effect of inductive bias. In International
Conference on Machine Learning, pages 5397–5428. PMLR, 2022.
[EGKZ21] Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and
variable creation in self-attention mechanisms. arXiv preprint arXiv:2110.10090, 2021.
[GLSS18a] Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit
biasintermsofoptimizationgeometry. InInternational Conference on Machine Learning,
pages 1832–1841. PMLR, 2018.
[GLSS18b] SuriyaGunasekar, Jason DLee, DanielSoudry, andNati Srebro. Implicitbias of gradient
descent on linear convolutional networks. Advances in Neural Information Processing
Systems, 31:9461–9471, 2018.
[HMRT19] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in
high-dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560,
2019.
[HSD00] PeterEHart,DavidGStork,andRichardODuda. Patternclassification. WileyHoboken,
2000.
[HVD15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural
network. arXiv preprint arXiv:1503.02531, 2015.
[JDST20] Ziwei Ji, Miroslav Dudík, Robert E Schapire, and Matus Telgarsky. Gradient descent
follows the regularization path for general losses. In Conference on Learning Theory,
pages 2109–2136. PMLR, 2020.
[JS22] Adel Javanmard and Mahdi Soltanolkotabi. Precise statistical analysis of classification
accuracies for adversarial training. The Annals of Statistics, 50(4):2127–2156, 2022.
[JT18] Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression.
arXiv preprint arXiv:1803.07300, 2018.
[JT20] Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning.
Advances in Neural Information Processing Systems, 33:17176–17186, 2020.
[JT21] Ziwei Ji and Matus Telgarsky. Characterizing the implicit bias via a primal-dual analysis.
In Algorithmic Learning Theory, pages 772–804. PMLR, 2021.
[KPOT21] Ganesh Ramachandra Kini, Orestis Paraskevas, Samet Oymak, and Christos Thram-
poulidis. Label-imbalanced and group-sensitive classification under overparameterization.
Advances in Neural Information Processing Systems, 34:18970–18983, 2021.
[KT21] Ganesh Ramachandra Kini and Christos Thrampoulidis. Phase transitions for one-vs-one
and one-vs-all linear separability in multiclass gaussian mixtures. In ICASSP 2021 - 2021
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
pages 4020–4024, 2021.
13[KZSS21] Frederic Koehler, Lijia Zhou, Danica J Sutherland, and Nathan Srebro. Uniform conver-
gence of interpolators: Gaussian width, norm bounds and benign overfitting. Advances
in Neural Information Processing Systems, 34:20657–20668, 2021.
[LCW21] Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On the expressive
power of self-attention matrices, 2021.
[LIPO23] YingcongLi,M.EmrullahIldiz,DimitrisPapailiopoulos,andSametOymak.Transformers
as algorithms: Generalization and stability in in-context learning, 2023.
[LL20] Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural
networks. In International Conference on Learning Representations, 2020.
[MKLZ20] Francesca Mignacco, Florent Krzakala, Yue M Lu, and Lenka Zdeborová. The role of
regularization in classification of high-dimensional noisy gaussian mixture. arXiv preprint
arXiv:2002.11544, 2020.
[MNS+20] Vidya Muthukumar, Adhyyan Narang, Vignesh Subramanian, Mikhail Belkin, Daniel
Hsu, and Anant Sahai. Classification vs regression in overparameterized regimes: Does
the loss function matter? arXiv preprint arXiv:2005.08054, 2020.
[MRSY19] Andrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan. The generalization error
of max-margin linear classifiers: High-dimensional asymptotics in the overparametrized
regime. arXiv preprint arXiv:1911.01544, 2019.
[NLG+19] Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese,
Nathan Srebro, and Daniel Soudry. Convergence of gradient descent on separable data.
In The 22nd International Conference on Artificial Intelligence and Statistics, pages
3420–3428. PMLR, 2019.
[NTS14] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real in-
ductive bias: On the role of implicit regularization in deep learning. arXiv preprint
arXiv:1412.6614, 2014.
[Ope22] OpenAI. Openai: Introducing chatgpt, 2022.
[Ope23] OpenAI. Gpt-4 technical report, 2023.
[ORST23] Samet Oymak, Ankit Singh Rawat, Mahdi Soltanolkotabi, and Christos Thrampoulidis.
On the role of attention in prompt-tuning. In International Conference of Machine
Learning (ICML), 2023.
[PBGR19] Joshua C Peterson, Ruairidh M Battleday, Thomas L Griffiths, and Olga Russakovsky.
Human uncertainty makes classification more robust. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages 9617–9626, 2019.
[PPVF21] Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of sgd
for diagonal linear networks: a provable benefit of stochasticity. Advances in Neural
Information Processing Systems, 34:29218–29230, 2021.
[RNS+18] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving
language understanding by generative pre-training. OpenAI blog, 2018.
[RWC+19] AlecRadford, JeffreyWu, RewonChild, DavidLuan, DarioAmodei, IlyaSutskever, etal.
Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[RZH03] Saharon Rosset, Ji Zhu, and Trevor J. Hastie. Margin maximizing loss functions. In
NIPS, 2003.
[SAH18] Fariborz Salehi, Ehsan Abbasi, and Babak Hassibi. A precise analysis of phasemax in
phase retrieval. In 2018 IEEE International Symposium on Information Theory (ISIT),
pages 976–980. IEEE, 2018.
14[SATA22] HaoyuanSun,KwangjunAhn,ChristosThrampoulidis,andNavidAzizan. Mirrordescent
maximizes generalized margin and can be implemented efficiently. Advances in Neural
Information Processing Systems, 35:31089–31101, 2022.
[SC19] Pragya Sur and Emmanuel J Candès. A modern maximum-likelihood theory for high-
dimensional logistic regression. Proceedings of the National Academy of Sciences, page
201810420, 2019.
[SEO+22] Arda Sahiner, Tolga Ergen, Batu Ozturkler, John Pauly, Morteza Mardani, and Mert
Pilanci. Unraveling attention via convex duality: Analysis and interpretations of vision
transformers. International Conference on Machine Learning, 2022.
[Sha48] Claude Elwood Shannon. A mathematical theory of communication. The Bell system
technical journal, 27(3):379–423, 1948.
[Sha22] Ohad Shamir. The implicit bias of benign overfitting. In Conference on Learning Theory,
pages 448–478. PMLR, 2022.
[SHLMHLQ16] Viktoriia Sharmanska, Daniel Hernández-Lobato, Jose Miguel Hernandez-Lobato, and
Novi Quadrianto. Ambiguity helps: Classification with disagreements in crowdsourced
annotations. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 2194–2202, 2016.
[SHN+18] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro.
Theimplicitbiasofgradientdescentonseparabledata. The Journal of Machine Learning
Research, 19(1):2822–2878, 2018.
[SRKL20] Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of
why overparameterization exacerbates spurious correlations. In International Conference
on Machine Learning, pages 8346–8356. PMLR, 2020.
[SVI+16] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna.
Rethinking the inception architecture for computer vision. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pages 2818–2826, 2016.
[TAP21] Nilesh Tripuraneni, Ben Adlam, and Jeffrey Pennington. Overparameterization im-
proves robustness to covariate shift in high dimensions. Advances in Neural Information
Processing Systems, 34:13883–13897, 2021.
[TB23] Kai Tan and Pierre C Bellec. Multinomial logistic regression: Asymptotic normality on
null covariates in high-dimensions. arXiv preprint arXiv:2305.17825, 2023.
[TLTO23] Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak.
Transformers as support vector machines, 2023.
[TLZO23] Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak. Max-margin
token selection in attention mechanism, 2023.
[TMS+23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
2023.
[TPT23] Hossein Taheri, Ramtin Pedarsani, and Christos Thrampoulidis. Asymptotic behavior of
adversarialtraininginbinarylinearclassification. IEEE Transactions on Neural Networks
and Learning Systems, 2023.
[TWCD23] YuandongTian,YipingWang,BeidiChen,andSimonDu. Scanandsnap: Understanding
training dynamics and token composition in 1-layer transformer, 2023.
[TWZ+23] Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Du. Joma:
Demystifying multilayer transformers via joint dynamics of mlp and attention. arXiv
preprint arXiv:2310.00535, 2023.
15[Ver11] R.Vershynin.Lecturesingeometricfunctionalanalysis.Unpublishedmanuscript.Available
at http://www-personal. umich. edu/romanv/papers/GFA-book/GFA-book. pdf, 2011.
[vONR+22] Johannes von Oswald, Eyvind Niklasson, E. Randazzo, João Sacramento, Alexander
Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context
by gradient descent. ArXiv, abs/2212.07677, 2022.
[VS21] Gal Vardi and Ohad Shamir. Implicit regularization in relu networks with the square
loss. In Conference on Learning Theory, pages 4224–4258. PMLR, 2021.
[WMT21] Ke Wang, Vidya Muthukumar, and Christos Thrampoulidis. Benign overfitting in
multiclass classification: All roads lead to interpolation. Advances in Neural Information
Processing Systems, 34, 2021.
[WS24] DavidWuandAnantSahai. Preciseasymptoticgeneralizationformulticlassclassification
with overparameterized linear models. Advances in Neural Information Processing
Systems, 36, 2024.
[WT21] Ke Wang and Christos Thrampoulidis. Binary classification of gaussian mixtures:
Abundance of support vectors, benign overfitting and regularization. arXiv preprint
arXiv:2011.09148, 2021.
[ZBH+17] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Under-
standing deep learning requires rethinking generalization, 2017.
[ZCDLP17] HongyiZhang,MoustaphaCisse,YannNDauphin,andDavidLopez-Paz. mixup: Beyond
empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.
[ZFB23] Ruiqi Zhang, Spencer Frei, and Peter L. Bartlett. Trained transformers learn linear
models in-context, 2023.
[ZSS20] LijiaZhou,DanicaJSutherland,andNatiSrebro. Onuniformconvergenceandlow-norm
interpolationlearning. Advances in Neural Information Processing Systems,33:6867–6877,
2020.
16A Experiments
We examine the implicit bias of GD on NTP training over synthetic data generated as follows. We
construct dataset with n=5000 sequences involving m=50 distinct contexts. Each distinct context gets
mapped to a randomly generated embedding of dimension d=60>m. We set vocabulary size V =10 and
each context j ∈[m] is followed by S =6,∀j ∈[m] possible next-tokens. The support sets S ⊂V and the
j j
probabilities pˆ ,z∈S are chosen randomly; see Fig. 2 for representative examples from the training
j,z j
dataset. For a fixed realization of the dataset (for which H≈1.445nats), we run GD, normalized GD
(NGD), and Adam from random LeCun initialization. For GD, we use learning rate η=0.5 and for NGD
and Adam η=0.01. For Adam, we also set β =0.9,β =0.99. We run all algorithms for 1e4 iterations.
1 2
For each case, we plot the following as a function of iterations:
1. Upper Left: CE loss versus entropy lower bound
2. Upper Right: parameter norm growth
3. Lower Left: correlation of Wmm with iterates W and of “corrected” iterates W −W⋆ after
k k
substracting the component on H
4. Lower Right: convergence of the subspace component W k,F =P F(W k).
Fig. 3 shows an instance of these. As predicted by our analysis, in this overparameterized setting: CE
loss converges to its lower-bound, parameter norm increases, iterates align in direction with Wmm, and
the subspace component converges to W⋆.
Figure 2: Eight randomly picked contexts with their associated next-token empirical conditional probabil-
ities pˆ . The indices shown on the x-axis define the support set S of each context.
j j
Figure 4 illustrates the same plots, but this time for training over the same dataset with NGD and
Adam. We observe same implicit bias, but faster convergence. For NGD, this is consistent with analogous
findings (rigorous in that case) for one-hot classification [NLG+19, JT21].
Finally, Figure 1 illustrates a toy 2d example where the embeddings and the hyperplanes defined by
each row of Wmm can be visualized. We used d = 2,m = 3,V = 5 and S = S = S = 3. The support
1 2 3
sets of each embedding are shown in the figure color-coded to match the respective decoder hyperplane.
Probabilities are assigned randomly. For the instance shown therein, the empirical conditional entropy
evaluates to H=0.8811.
17ℱ
Figure 3: Experimental illustration of the implicit bias of GD in NTP over synthetic data with overpa-
rameterization. See App. A for detailed description of the experimental setting. The upper two graphs
confirm the predictions of Lemma 2, while the lower two graphs adhere to the predictions of Theorem 2.
ℱ ℱ
NGD Adam
Figure 4: Implicit bias of normalized GD (Left) and of Adam (Right) in NTP over synthetic data with
overparameterization. Both exhibit the same implicit bias, but converge faster than GD, with Adam
being slightly faster than NGD.
B Proofs
B.1 Notations
Throughout, lowercase and uppercase bold letters (e.g., a and A) represent vectors and matrices,
respectively. ⟨⋅,⋅⟩ and ∥⋅∥ denote Euclidean inner product and norm, respectively. For matrix A, we
denote its pseudoinverse as A†. All logarithms are natural logarithms (base e). We denote e the v-th
v
standard basis vector in RV. ∆V−1 denotes the V-dimensional unit simplex and S()∶RV →∆V−1 the
softmax map:
S(a)=[S (a),...,S (a)]⊺, with S (a)=
ee⊺ va
.
1 V v ∑ v′∈[V]ee⊺ v′a
As explained in Section 2 we represent a training set as
T ∶={(h¯ ,πˆ ,pˆ )} .
m j j j,z∈V j∈[m]
18We assume that embeddings are bounded and denote
√
M ∶= 2max∥h¯ ∥.
j
j∈[m]
Given T , let
m
F =span({(e −e )h¯⊺ ∶ z≠z′∈S ,j ∈[m]})
z z′ j j
asubspaceofV ×dmatricesandF⊥ itsorthogonalcomplement. DenoteP F,P
⊥
theorthogonalprojections
onto F and F⊥, respectively. For convenience, for W ∈RV×d, we denote
W ∶=P (W) and W =P (W).
F F ⊥ ⊥
Define
CEF(W)= ∑ πˆ
j
∑ pˆ j,zlog(1+∑e−(ez−e z′)⊺Wh¯ j). (18)
j∈[m] z∈Sj z≠z
Clearly, for all W ∈RV×d, it holds CE(W)≥CEF(W). Note also that for all W ∈F and for all Wd∈F⊥
that satisfy Eq. (10a), it holds CEF(W) = lim R→∞CE(W +RWd). Thus, under NTP compatibility
and NTP separability,
inf CEF(W)=infCE(W)=H. (19)
W∈F W
B.2 Gradient Descent
Throughout we assume GD is ran with step-size η≤1/(2L) where L is the smoothness of CE loss. This
condition is not explicitly mentioned thereafter.
B.2.1 Auxiliary Lemmata
The following result follows from standard optimization analysis for smooth convex functions specialized
to functions that do not attain their infimum. The version presented here is adopted from [JT20, Lemma
2].
Lemma 3. It holds
lim CE(W )=infCE(W)
k
k→∞ W
and also lim ∥W ∥=∞.
k→∞ k
Inthelemmabelow,wecollectsomeusefulandsimple-to-showpropertiesoftheGDandregularization
paths. Analogousresults,forthedifferentsettingofone-hotbinaryclassificationovergeneralnon-separable
data have been established in [JT18].
Lemma 4. Suppose conditions (10) hold for some Wd. Also, that there exists Wp=W⋆∈F satisfying
condition (8). The following hold:
1. CEF(W⋆)=inf W∈FCEF(W)=H,
2. W⋆ is the unique minimizer of CEF on the subspace F,
3. lim k→∞P F(W k)=W⋆, where W
k
are GD iterates,
4. lim ∥P (W )∥=∞,
k→∞ ⊥ k
5. lim B→∞P F(Ŵ B)=W⋆, where Ŵ
B
is the reguarlized solution (13),
6. lim ∥P (Ŵ )∥=∞.
B→∞ ⊥ B
19Proof. It is easy to check by direct substitution of W⋆ in (18) and use of (8) that CEF(W⋆)=H. This
and (19) show the first claim.
The first claim shows W⋆ is a minimizer. Suppose for the sake of contradiction there is a different
minimizer W⋆ ≠ W
1
∈ F. Then, since CEF(W 1) = H, it also holds for W
R
∶= W
1
+RWd that
lim CE(W )=H. In turn, this implies for all j ∈[m]:
R→∞ R
lim S (W h¯ )=pˆ ,∀z∈S , and lim S (W h¯ )=0,∀v∉S .
z R j j,z j v R j j
R→∞ R→∞
The first condition gives then that W must satisfy (8). Since W⋆ also satisfies these equations, denoting
1
W =W⋆−W ≠0, it holds:
∆ 1
⟨W ,(e −e )⊺h¯ )⟩=0,∀j ∈[m],z≠z′∈S .
∆ z z′ j j
ButW ∈F,sothisformsacontradiction. Hence,W⋆ isuniquesolutioninFof (8)anduniqueminimizer
∆
of CEF on the subspace F.
The proof of the third claim follows the same way as the proof of part (1) of [JDST20, Thm. 15]. For
completeness: Itfollowsbythelemma’sassumptionsandLemma3thatlim CE(W )=H. Combining
k→∞ k
with the first claim of the lemma yields lim k→∞CE(W k)=CEF(W⋆). Since CEF(W k)≤CE(W k), this
finally gives
lim CEF(W k)= lim CEF(P F(W k))=CEF(W⋆).
k→∞ k→∞
Since W⋆ is unique by the second claim, the desired then follows.
For the fourth claim, recall from Lemma 3 that lim ∥W ∥=∞. From the previous claim, we also
k→∞ k
have lim k→∞∥P F(W k)∥<C for some constant C >∥W⋆∥. Thus, the desired follows by applying the fact
that ∥W k∥=∥P F(W k)∥+∥P ⊥(W k)∥.
The proof of the last two claim is exactly same as that of the third and fourth claim. Only now use
the facts that lim CE(W )=H and lim ∥W ∥=∞ (see proof of Theorem 1).
B→∞ B B→∞ B
B.2.2 Key Lemma
Lemma 5. Let W
k
denote the GD iterate at iteration k. Recall the decomposition W
k
= P F(W k)+
P ⊥(W k)=W k,F+W k,⊥. Fix any α∈(0,1). There exists large enough R=R(α) and k
0
=k 0(R) such
that for any k≥k , it holds that ∥W ∥≥R and
0 k,⊥
CE(W k,F+(1+α)∥W k,⊥∥Wmm)≤CE(W k). (20)
Proof. We drop the subscript k to lighten notation.
First, note by Lemma 4.D that, for arbitrary R, we can pick k = k (R) such that for all k ≥ k :
1 1 1
∥W ∥≥R.
⊥
Thus next, we will prove the main claim, i.e. for large enough ∥W ∥ inequality (20) holds. Denote
⊥
R′= ∥W⊥∥ . Substituting in CE expression (5), and using the fact that Wmm∈F⊥ by (10a) yield:
∥Wmm∥
CE(WF+(1+α)R′Wmm)
⎛ ⎞
= ∑ πˆ
j
∑ pˆ j,zlog ∑ e−(ez−e z′)⊺WFh¯ j + ∑ e−(ez−ev)⊺WFh¯ j + ∑ e−(1+α)R′(ez−ev)⊺Wmmh¯ j .
⎝ ⎠
j∈[m] z∈Sj z′∈Sj v∉Sj v∉Sj
⎛ ⎞
= ∑ πˆ ∑ pˆ log
∑e−(ez−ev)⊺WFh¯
j + ∑
e−(1+α)R′(ez−ev)⊺Wmmh¯
j . (21)
j j,z
⎝ ⎠
j∈[m] z∈Sj v∈V v∉Sj
Moreover, decomposing W =WF+W ⊥, and defining
∥Wmm∥ 1
W̃ ∶= W = W ,
⊥ ∥W ∥ ⊥ R ⊥
⊥
we have
⎛ ⎞
CE(W)= ∑ πˆ
j
∑ pˆ j,zlog ∑ e−(ez−e z′)⊺WFh¯ j + ∑ e−(ez−ev)⊺WFh¯ j + ∑ e−R′(ez−ev)⊺W̃ ⊥h¯ j
⎝ ⎠
j∈[m] z∈Sj z′∈Sj v∉Sj v∉Sj
⎛ ⎞
= ∑ πˆ ∑ pˆ log
∑e−(ez−ev)⊺WFh¯
j + ∑
e−R′(ez−ev)⊺W̃ ⊥h¯
j , (22)
j j,z
⎝ ⎠
j∈[m] z∈Sj v∈V v∉Sj
20where we used that, by definition, W ∈F⊥. Thus, our goal becomes showing (21)≤(22), for large enough
⊥
R. To do this, we consider two cases as follows below.
√
For the remaining of the proof recall M ∶=max 2∥h¯ ∥ and use the logits shorthand:
j∈[m] j
ℓ̃ =e⊺W̃ h¯ and ℓmm=e⊺Wmmh¯ .
j,v v ⊥ j j,v v j
Case 1: W is well aligned with Wmm. Suppose
⊥
α
∥Wmm−W̃ ∥≤ϵ∶= . (23)
⊥
M
Using this, linearity of logits, and Cauchy-Schwartz, yields
ℓ̃ −ℓ̃ ≤ℓmm−ℓmm+ϵM, ∀j ∈[m],z∈S ,v∉S .
j,z j,v j,z j,v j j
Thus,
∑
e−R′(ez−ev)⊺W̃ ⊥h¯
j
≥e−ϵMR′
∑
e−R′(ez−ev)⊺Wmmh¯
j
=e−αR′
∑
e−R′(ez−ev)⊺Wmmh¯
j
v∉Sj v∉Sj v∉Sj
Also recall by feasibility of Wmm that
ℓmm−ℓmm≥1,∀j ∈[m],z∈S ,v∉S . (24)
j,z j,v j j
Thus,
∑
e−(1+α)R′(ez−ev)⊺W̃ ⊥h¯
j
≤e−αR′
∑
e−R′(ez−ev)⊺Wmmh¯
j
v∉Sj v∉Sj
Comparing the above two displays yields
∑
e−(1+α)R′(ez−ev)⊺W̃ ⊥h¯
j ≤ ∑
e−R′(ez−ev)⊺W̃ ⊥h¯
j,
v∉Sj v∉Sj
which implies the desired (21)≤(22) for any value of R′ (eqv. ∥W ∥).
⊥
Case 2: No alignment. Suppose now that (23) does not hold. Note that ∥W̃ ∥ = ∥Wmm∥ and since
⊥
(NTP-SVM) has a unique solution it must be that W̃ is not feasible. But W̃ ∈F , thus it satisfies the
⊥ ⊥ ⊥
equality constraints. This then means that there exist δ∶=δ(ϵ) and j ∈[m],v ∉S such that
⋆ ⋆ j⋆
ℓ̃ −ℓ̃ ≤1−δ, ∀z∈S . (25)
j⋆,z j⋆,v⋆ j⋆
(Note the above holds for all z∈S because ℓ̃ =ℓ̃ since W̃ ∈F .)
j⋆ j⋆,z j⋆,z′ ⊥ ⊥
To continue, we introduce the shorthand notation
A ∶=A (W)=
∑e−(ez−ev)⊺WFh¯
j
j,z j,z
v∈V
as well as
A ∶= min A , and A ∶= max A .
min j,z max j,z
j∈[m],z∈Sj j∈[m],z∈Sj
Using (25) we may lower bound (22) as follows:
CE(W)− ∑ πˆ ∑ pˆ
log(∑e−(ez−ev)⊺WFh¯
j)≥πˆ ∑ pˆ
log⎛
1+
e−R′(ez−ev⋆)⊺W̃ ⊥h¯ j⋆⎞
j j,z j⋆ j,z
⎝ A ⎠
j∈[m] z∈Sj v∈V z∈Sj j⋆,z
e−R′(1−δ)
≥πˆ ∑ pˆ log(1+ )
j⋆ j,z
A
z∈Sj max
e−R′(1−δ)
≥ , (26)
n(A +1)
max
where in the last line we used πˆ ≥1/n,∀j ∈[m] as well as log(1+x)≥ x ,x>0.
j 1+x
21On the other hand, using property (24) for max-margin logits, we can upper bound (21) as follows:
CE(W
F
+(1+α)R′Wmm)− ∑ πˆ
j
∑ pˆ j,zlog(∑e−(ez−ev)⊺WFh¯ j)≤log(1+ V
e−R′(1+α)
)
A
j∈[m] z∈Sj v∈V min
V
e−R′(1+α)
≤ , (27)
A
min
where in the last line we used log(1+x)≤x,x>0.
In view of the two last displays, it suffices that
e−R′(1+α) e−R′(1−δ) 1 nV(A +1)
V ≤ ⇐⇒ R′≥ log( max ).
A n(A +1) δ+α A
min max min
All it remains is obtaining bounds for A ,A specifically showing that they do not depend on R. By
min max
Cauchy-Schwartz:
Ve−M∥WF∥≤A ≤A ≤VeM∥WF∥
min max
Further recall by Lemma 4.C that if k is large enough then
∥WF−W⋆∥≤∥W⋆∥ (cid:212)⇒ ∥WF∥≤2∥W⋆∥. (28)
Thus, there exists k =k (∥W ∥) such that for all k≥k :
⋆ ⋆ ⋆ ⋆
Ve−2M∥W⋆∥≤A ≤A ≤Ve2M∥W⋆∥.
min max
Hence, the desired (27)≤(26) holds provided
∥Wmm∥
∥W ∥≥
log(2nVe4∥W⋆∥).
(29)
⊥
α
Set R = R(α) = {RHS of (29)} and k (R) ∶= max{k (R),k }. We have shown this guarantees for
0 1 ⋆
all k ≥k : ∥W ∥≥R and by choice of R also (27)≤(26). This in turn implies (21)≤(22), as desired to
0 ⊥
complete the proof.
B.2.3 Proof of Theorem 2
For the subspace component, see Lemma 4.C. For the directional convergence, the key ingredient of the
proof is Lemma 5. After that, the proof follows identically to [JDST20, Thm. 15(2)]. We include the
details for completeness, but there are no novel aspects in the rest of this section.
Let any ϵ∈(0,1) and choose α=ϵ/(1−ϵ). By Lemma 5, there exists k such that for any k≥k , we
0 0
have
∥W ∥≥max{R(α),1/2}
k,⊥
and
⟨∇CE(W k),W k,⊥−(1+α)∥W k,⊥∥Wmm⟩=⟨∇CE(W k),W k−(W k,F+(1+α)∥W k,⊥∥Wmm)⟩
≥CE(W k)−CE(W k,F+(1+α)∥W k,⊥∥Wmm)≥0,
where we also used convexity of the loss.
Consequently,
⟨W −W ,Wmm⟩=⟨−η∇CE(W ),Wmm⟩
k+1 k k
≥(1−ϵ)⟨−η∇CE(W ),W ⟩
k k,⊥
≥(1−ϵ)⟨W −W ,W ⟩
k+1,⊥ k,⊥ k,⊥
≥(1−ϵ)⟨W −W ,W ⟩
k+1,⊥ k,⊥ k,⊥
(1−ϵ)
= (∥W ∥2−∥W ∥2−∥W −W ∥2)
2∥W ∥ k+1,⊥ k,⊥ k+1,⊥ k,⊥
k,⊥
≥(1−ϵ)(∥W ∥−∥W ∥−2η(CE(W )−CE(W )),
k+1,⊥ k,⊥ k,⊥ k+1,⊥
22where the last step used ∥W ∥≥1/2, the fact that x2−y2≥2y(x−y),∀x,y and smoothness of the CE
k,⊥
loss.
Telescoping the above expression and rearranging yields
∥W ∥ ⟨W ,Wmm⟩−(1−ϵ)∥w ∥−ηCE(W )
⟨W ,Wmm⟩≥(1−ϵ) k,⊥ − k0 k0,⊥ k0
k ∥W ∥ ∥W ∥
k k
≥(1−ϵ)−
∥W k,F∥ 2+⟨W k0,Wmm⟩−(1−ϵ)∥w k0,⊥∥−ηCE(W k0)
∥W ∥
k
Now recall from Lemma 4 that lim k→∞∥W k∥=∞ and lim k→∞∥W k,F∥=∥W⋆∥. Thus,
liminf ⟨W ,Wmm⟩≥1−ϵ. Since ϵ is arbitrary, the desired follows.
k→∞ k
B.3 Regularization Path
We provide a detailed proof of Theorem 1 filling in missing details from the proof sketch in the main
paper.
B.3.1 Proof of Theorem 1
First, we show that Ŵ is on the boundary, i.e. ∥Ŵ ∥=B. Suppose not, then ⟨∇CE(Ŵ ),U⟩=0 for
B B B
all U ∈RV×d. Using the CE expression in (5) and a few algebraic manipulations, yields
⟨−∇CE(Ŵ ),U⟩= ∑ πˆ ∑ pˆ ( ∑ s (e −e )⊺Uh¯ + ∑ s (e −e )⊺Uh¯ ), (30)
B j j,z j,z′ z z′ j j,v z v j
j∈[m] z∈Sj z′∈Sj v∉Sj
z′≠z
where we denote the output probabilities at Ŵ as s ∶=S (Ŵ h¯ ),v∈V,j ∈[m]. Choose U =Wmm
B j,v v B j
in (30). Then, the first term in the parenthesis in (30) is zero by (10a), while the second term is strictly
positive by (10b) and strict positivity of softmax entries, leading to contradiction.
Now, consider point W⋆ =W⋆+R(B)⋅Wmm, where, W⋆∈T satisfies (8), and R=R(B) is chosen
B
such that ∥W⋆∥=B. Concretely, for B>∥W⋆∥, set
B
√
1
R= B2−∥W⋆∥2.
∥Wmm∥
Note also that R/B→1/∥Wmm∥ as B→∞. We will show that W⋆ attains a small CE loss as B (hence,
B
R) grows. To do this, denote for convenience the logits for all v∈V,j ∈[m]∶
ℓ⋆ ∶=e⊺W⋆h¯ and ℓmm∶=e⊺Wmmh¯ ,
j,v v j j,v v j
and note that e⊺W⋆h¯ =ℓ⋆ +Rℓmm. By using (8) and (10a):
v B j j,v j,v
∑ e−(ℓ⋆ j,z+Rℓm j,zm−ℓ⋆ j,z′−Rℓm j,zm ′)= 1 .
pˆ
z′∈Sj j
Moreover, using (10b)
∑ e−(ℓ⋆ j,z+Rℓm j,zm−ℓ⋆ j,v−Rℓm j,vm)≤e−R ∑ e−(ℓ⋆ j,z−ℓ⋆ j,v)≤Ce−R,
v∉Sj v∉Sj
√
where we define constant (independent of R) C ∶=Ve∥W⋆∥M, for M ∶= 2⋅max ∥h¯ ∥.
j/∈[m] j
Combining the above displays and using in Eq. (5), yields
1 1
CE(W⋆)≤ ∑ πˆ ∑ pˆ log( +Ce−R) ≤ ∑ πˆ ∑ pˆ (log( )+pˆ Ce−R)
B j j,z pˆ j j,z pˆ j,z
j∈[m] z∈Sj j,z j∈[m] z∈Sj j,z
≤H+Ce−R, (31)
where, the second line uses log(1+x)≤x,x>0, and the third line uses πˆ ,pˆ are probabilities.
j j,z
23Next, towards arriving at a contradiction, we will show that if Ŵ is not in the direction of Wmm,
B
then it incurs a loss that is larger than CE(W⋆). Concretely, assuming the statement of the theorem is
B
not true, we we will upper bound
pˆ
CE(Ŵ )−H= ∑ πˆ ∑ pˆ log( j,z ). (32)
B j j,z S (Ŵ h¯ )
j∈[m] z∈Sj z B j
By our assumption, there exists ϵ>0, such that there exists arbitrarily large B satisfying:
∥Wmm∥
∥ Ŵ −Wmm∥>ϵ. (33)
B
B
Define
1
Ŵ = (Ŵ −W⋆),
R′(B) B
where, R′=R′(B)>0 is chosen so that ∥Ŵ∥=∥Wmm∥. Concretely, for large enough B≥2∥Wmm∥, set
¿
R′=`
`(cid:192)
B2
−2B⟨W ,Wmm⟩+1.
∥Wmm∥2 B
Notethatitholdslim R′/B=1/∥Wmm∥.Thus,wecanalwayschooseB largeenoughsothatEq. (33)
B→∞
guarantees ∥Ŵ−Wmm∥≥ϵ′, for some ϵ′>0. Since Wmm is the unique minimizer of (NTP-SVM) and
∥Ŵ∥=∥Wmm∥, it follows that there exists δ∈(0,1) and j ∈[m] such that at least one of the following is
true
(i) ∃z and z′≠z∈S such that
j
∣(e −e )⊺Ŵ h¯ ∣≥δ, (34)
z z′ j
(ii) ∃z∈S ,v∉S such that
j j
(e −e )⊺Ŵ h¯ ≤1−δ. (35)
z v j
Case (i): Without loss of generality (e −e )⊺Ŵ h¯ ≤−δ (otherwise, flip z,z′). Thus, ignoring all but
z z′ j
one term in (32) gives
pˆ
CE(Ŵ B)−H≥πˆ jpˆ j,zlog(
S
(Ŵj,z
h¯
))≥πˆ jpˆ j,zlog(pˆ j,ze(ℓ j,z′−ℓj,z)), (36)
z B j
where we use ℓ =e⊺Ŵ h¯ ,v∈V to denote logits of Ŵ . Using (8) and (34), yields
j,v v B j B
pˆ
ℓ −ℓ =(e −e )⊺(R′Ŵ+W⋆)h¯ ≥R′δ+log( j,z′ ).
j,z′ j,z z′ z j
pˆ
j,z
Put in (32) and using pˆ ≥πˆ pˆ ≥1/n shows
j,z j j,z
1
eR′δ
CE(Ŵ )≥H+ log( )
B
n n
Compare this with (31). For large enough B, it is clear that πˆ pˆ log(pˆ ceR′δ) > Ce−R. Thus,
j j,z j,z
CE(Ŵ )>CE(W⋆), a contradiction.
B B
Case (ii): We can assume Ŵ ∈T , since otherwise we are in Case (i). Now, again ignoring all but the
⊥
(j,z) term in the CE loss for which (35) holds for some v∉S , we find
j
CE(Ŵ B)−H≥πˆ jpˆ j,zlog(pˆ j,z( ∑ e(ℓ j,z′−ℓj,z)+e(ℓj,v−ℓj,z))).
z′∈Sj
Using P (Ŵ )=W⋆ yields
T B
pˆ 1
∑ e(ℓ j,z′−ℓj,z)= ∑ j,z′ = .
pˆ pˆ
z′∈Sj z′∈Sj j,z j,z
24Moreover, by (35):
eℓj,v−ℓj,z ≥e−R′(1−δ)eℓ⋆ j,v−ℓ⋆
j,z
≥c′e−R′(1−δ),
for constant (independent of B)
c′∶=e−∥W⋆∥M.
Putting the above together yield:
c′e−R′(1−δ)
CE(Ŵ )−H≥πˆ pˆ log(1+pˆ c′e−R′(1−δ))≥ .
B j j,z j,z 2n2
where the second inequality uses log(1+x)≥ x ,x>0.
1+x
Comparethiswith(31). ForlargeenoughB,(recallR,R′growatthesamerate)itholds c′ e−R′(1−δ)>
2n2
Ce−R. Thus, CE(Ŵ )>CE(W⋆), a contradiction.
B B
In either case, we arrive at a contradiction, which completes the proof.
25