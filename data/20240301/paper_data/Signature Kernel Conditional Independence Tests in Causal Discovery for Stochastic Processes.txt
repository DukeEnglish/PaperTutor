Signature Kernel Conditional Independence Tests in Causal
Discovery for Stochastic Processes
Georg Manten†,1,2,3, Cecilia Casolo†,1,2,3, Emilio Ferrucci4, Søren Wengel Mogensen5,
Cristopher Salvi6, Niki Kilbertus1,2,3
1Technical University of Munich
2Helmholtz Munich
3Munich Center for Machine Learning
4Mathematical Institute, University of Oxford
5Department of Automatic Control, Lund University
6Department of Mathematics, Imperial College London
†equal contribution
Abstract
Inferring the causal structure underlying stochastic dynamical systems from observational
data holds great promise in domains ranging from science and health to finance. Such
processes can often be accurately modeled via stochastic differential equations (SDEs),
which naturally imply causal relationships via ‘which variables enter the differential of
which other variables’. In this paper, we develop a kernel-based test of conditional
independence (CI) on ‘path-space’—solutions to SDEs—by leveraging recent advances in
signature kernels. We demonstrate strictly superior performance of our proposed CI test
compared to existing approaches on path-space. Then, we develop constraint-based causal
discovery algorithms for acyclic stochastic dynamical systems (allowing for loops) that
leverage temporal information to recover the entire directed graph. Assuming faithfulness
and a CI oracle, our algorithm is sound and complete. We empirically verify that our
developed CI test in conjunction with the causal discovery algorithm reliably outperforms
baselines across a range of settings.
1 Introduction
Understanding cause-effect relationships from observational data can help identify causal
drivers for disease progression in longitudinal data and aid the development of new treatments,
understand the underlying influences of stock prices to support lucrative trading strategies,
or speed up scientific discovery by uncovering interactions in complex biological or chemical
systems such as gene regulatory pathways.
Causal discovery (or causal structure learning) has received continued attention from the
scientific community for at least two decades (Glymour et al., 2019, Spirtes et al., 2000, Vowels
et al., 2022) with a notable uptick in previous years particularly regarding differentiable
score-based causal discovery (Annadani et al., 2023, Brouillard et al., 2020, Charpentier
et al., 2022, Ha¨gele et al., 2023, Lorch et al., 2021, Zheng et al., 2018, 2020), score matching
(Montagna et al., 2023a,b, Rolland et al., 2022), and other deep-learning based approaches
(Chen et al., 2022, Ke et al., 2020, 2023, Yu et al., 2019). Many of these approaches aim at
improving the scalability of causal discovery in the number of variables and observations as
well as at incorporating uncertainty or efficiently making use of interventional data.
However, causal discovery from time series data has received much less attention in general
and been mostly neglected in particular in these recent advances. At a fundamental level,
causal effects can only ‘point into the future’, and it may seem that time resolved data should
1
4202
beF
82
]GL.sc[
1v77481.2042:viXramake causal discovery simpler. However, except for simple cases, this is not necessarily true
(Lawrenceetal.,2021,Rungeetal.,2023,2019,Singer,1992)asdynamicalsystemsmayexhibit
temporally changing causal dependencies as well as confounding introduced by discrepancies
in the intrinsic dynamical timescales and the sampling frequency. The bulk of existing work
assumes observations to be sampled on a regular time grid with a discrete (auto-regressive) law
for how past observations (X ,...,X ) determine the present X = f(X ,...,X ,ε )
t−τ t−1 t t−τ t−1 t
for some time lag τ ∈ N and a fixed function f (Assaad et al., 2022, Hasan et al., 2023).
Overcoming the fundamental limitations imposed by this ‘discrete-time’ assumption is a major
challenge (Runge et al., 2019), which we tackle in this work.
Data generating process. We assume observational data to follow a stochastic process
X := (X1,...,Xd), with Xk taking values in Rn k (n k ≥ 1), and X satisfying the following
system of path-dependent SDEs
(cid:40)
dXk = µk(X )dt+σk(X )dWk,
t [0,t] [0,t] t (1)
Xk = xk for k ∈ [d] := {1,...,d}.
0 0
We call this the SDE model; a very similar model, the stochastic causal kinetic model, was
recently introduced in Peters et al. (2022). The subscript [0,t] at X means that µk (the
‘drift’) and σk (the ‘diffusion’) are functions of the entire solution up to time t, i.e., they are
defined on C([0,+∞),Rn) (or some suitable subspace thereof), where n := n +...+n .
1 k
Each Wk is an m -dimensional Brownian motion, and σk maps to n × m -dimensional
k k k
matrices. The noises Wk together with the (possibly random) initial conditions xk are jointly
0
independent.1 Hence, the SDE together with a distribution over initial conditions defines
our assumed ground truth data generating process. Individual observations are paths, which
in practice translate to stochastic, potentially irregularly sampled time-series observations
for Xk with a maximum observation time T, see Figure 1. The choice not to make the
[0,T]
coefficients explicitly time-dependent is deliberate and motivated by the requirement of causal
stationary (the causal relations between variables do not change over time).
Induced causal graph. Eq. (1) naturally implies cause-effect relationships: we call i ∈ [d]
a parent of j ∈ [d] (i ∈ pa ) when either µj or σj is not constant in the i-th argument.
j
These parental relationships define a directed graph G = (V,E) which we also call dependency
graph of the SDE model. The goal of causal discovery is then to infer the G induced by
the SDE model from a sample of observed solution paths, depicted in Figure 1. Here, we
only consider data generating processes leading to directed graphs without cycles of length
greater than one (i.e., only loops Xk → Xk are allowed). We will still call these directed
acyclic graphs (DAGs) for simplicity. The additional challenges arising from dropping the
acyclicity assumption is discussed in Appendix E as an interesting direction for future work.
Moreover, we assume all relevant variables X to be observed. When only partial observations
are available, i.e., some processes are unobserved, any constraint-based algorithm based on
asymmetric local independence requires exponentially many oracle calls (Mogensen, 2023,
Mogensen and Hansen, 2022). The partially observed case is therefore challenging in the local
independence framework, however, the symmetric conditional independence framework we
describe may be extended directly to the partially observed setting.
Despite these limitations, eq. (1) goes far beyond common assumptions in causal discovery
from time-series data, also in comparison with similar models as in Peters et al. (2022): (a)
It does not rely on the ‘discrete-time’ assumption (neither in the underlying model nor the
actual observations). (b) Path-dependence (µk,σk may depend on the entire previous history
X of paths), including delayed SDEs, is typically not captured in existing approaches. (c)
[0,t]
It goes beyond ‘additive observation noise’ as it incorporates ‘driving noise’ where causal
1See Rogers and Williams (2000) for details, Evans (2006) for a concise introduction, and Appendix A.1 for
more intuition.
2A
es
at i
ner n
d
e u
g c
B e s
C
causal discovery
Figure 1: Overview of causal discovery in our setting. The data generating mechanism (A)
generates samples (B) and induces a causal graph (C). The goal of constraint-based causal
discovery is to use conditional independence relations on the observed trajectories (B) to
determine the causal graph (C).
dependence may also come from the diffusion, which cannot be captured by the mean of
observed paths.
Contributions. Wedevelopsoundandcompleteconstraint-basedalgorithmsfor(a)inferring
the full DAG, (b) inferring the completed partially directed acyclic graph (CPDAG) repre-
senting the Markov equivalence class from path-valued observations (assuming a conditional
independence test oracle and faithfulness). While (a) yields strictly stronger results, it relies
on a different independence model. To render these algorithms applicable, we also propose
flexibleCItestsonpath-spacebuildingonrecentadvancesinsignaturekernels. Afterextensive
empirical evaluations, demonstrating the superior performance of our CI tests (for different
types of CI models), we combine our test with the causal discovery algorithms to achieve
promising performance in graph discovery for systems with up to 5 variables. Finally, we
discuss benefits and drawbacks of the two algorithms.
2 Background and Related Work
2.1 Signature Kernels
Kernel methods are at the core of many key techniques in machine learning. A kernel defines
a feature map that lifts observations into a (possibly infinite-dimensional) Hilbert space in
which non-linear optimization problems on the original data often become linear and solutions
can be expressed entirely in terms of kernel evaluations of the data. Selecting an effective
kernel for a particular task and data modality is often challenging, particularly when the data
is sequential. Signature kernels Kira´ly and Oberhauser (2019), Salvi et al. (2021a) are a class
of universal kernels on sequential data which have received attention in recent years thanks
to their efficiency in handling path-dependent problems Cirone et al. (2023), Cochrane et al.
(2021), Issa et al. (2023), Lemercier et al. (2021), Salvi et al. (2021b,c).
The definition of the signature kernel requires an initial algebraic setup. Let ⟨·,·⟩ be the
1
Euclidean inner product on Rd. Denote by ⊗ the standard tensor product of vector spaces.
3For any n ∈ N, the canonical Hilbert-Schmidt inner product ⟨·,·⟩ on (Rd)⊗n defined for any
n
a = (a ,...,a ) and b = (b ,...,b ) in (Rd)⊗n as ⟨a,b⟩ = (cid:81)n ⟨a ,b ⟩ . Define the direct
1 n 1 n n i=1 i i 1
sum of vector spaces T(Rd) := (cid:76)∞ (Rd)⊗n, where it is understood that the direct sum runs
n=1
over finitely many non-zero levels. The inner product ⟨·,·⟩ on (Rd)⊗n can be extended by
n
linearity to an inner product ⟨·,·⟩ on T(Rd) defined for any a = (a ,a ,...) and b = (b ,b ,...)
0 1 0 1
in T(Rd) as ⟨a,b⟩ = (cid:80)∞ ⟨a ,b ⟩ . Other choices of linear extensions have been recently
n=0 n n n
studied by Cass et al. (2023). We denote by T(Rd) the Hilbert space obtained by completing
T(Rd) with respect to ⟨·,·⟩.
Thesignature transform, aclassicalpath-transformfromstochasticanalysisforanysubinterval
[s,t] ⊂ [0,T] and any continous path X ∈ C ([0,T],Rd) of finite p-variation for 1 ≤ p < 3 is
p
(canonically) defined by
S(X) := (cid:0) 1,S(X)(1) ,...,S(X)(n) ,...(cid:1) ∈ T(Rd),
s,t s,t s,t
where S(X)(n) ∈ (Rd)⊗n is the n-fold iterated integral
s,t
(cid:90)
(n)
S(X) = dX ⊗dX ⊗...⊗dX .
s,t u1 u2 un
s<u1<...<un<t
The signature kernel
K : C ([s,t],Rd)×C ([s′,t′],Rd) → R (2)
S p p
is the following symmetric function
(cid:10) (cid:11)
K (X,Y) = S(X) ,S(Y) .
S s,t s′,t′
A kernel-trick was derived by Salvi et al. (2021a, Theorem 2.5) who showed that the signature
kernel satisfies the following path-dependent integral equation
(cid:90) t(cid:90) t′
f(t,t′) = 1+ f(u,v)⟨dX ,dY ⟩ . (3)
u v 1
s s′
Several off-the-shelf python packages offer efficient PDE solvers for eq. (3) such as sigkerax.
When restricted to compact sets in C([0,T],Rd), the signature kernel is characteristic (Cass
et al., 2023, Proposition 3.3), and this assumption can be relaxed further (Chevyrev and
Oberhauser, 2022). Many results in kernel methods crucially depend on characteristic kernels,
including tests for conditional independence (Muandet et al., 2017).
2.2 Conditional Independence Tests
Conditional independence (CI) is a fundamental notion in computational statistics and forms
thefoundationofgraphicalmodels. CItestinginvolvesassessingwhethertworandomvariables,
X and Y, are independent conditioned on a third random variables, Z, i.e., whether the null
hypothesis H : X⊥⊥Y | Z can be rejected against the alternative H : X ⊥̸⊥ Y | Z. When Z
0 1
is discrete, conditional independence testing simplifies to a collection of unconditional tests
for each value of Z (Tsamardinos and Borboudakis, 2010). When Z is continuous, most CI
tests either rely on parametric assumptions about the underlying distributions (Baba et al.,
2004) or rely primarily on kernel methods for nonparametric approaches (Li and Fan, 2020,
Zhang et al., 2017).
Kernel methods for CI testing typically leverage either kernel (ridge) regression or kernel
mean embeddings and the Hilbert-Schmidt (conditional) independence criterion (HS(C)IC)
to measure the reproducing kernel Hilbert space (RKHS) distance of the (conditional) mean
embeddings of the joint distribution and the product of marginal distributions (Gretton et al.,
2007, Muandet et al., 2017, Park and Muandet, 2020). Examples include KCIT (Zhang et al.,
42011) that draws on partial associations (Daudin, 1980) of regression functions connecting X,
Y, Z, together with analytical or empirical (bootstrapped) expressions for the asymptotic
distribution of the test statistic under the null. Strobl et al. (2019) approximate kernel
computations in KCIT via random Fourier features for improved efficiency.
Other approaches rely on reducing CI to a two-sample test for the equivalent null H :
0
P(X,Y,Z) = P(X | Z)P(Y | Z)P(Z). KCIPT (Doran et al., 2014) simulates samples
from P(X | Z)P(Y | Z)P(Z) by learning a permutation of the samples that approximately
preserves Z values. Lee and Honavar (2017) propose a modified, unbiased estimate of the
Maximum Mean Discrepancy (MMD) (Gretton et al., 2006) as the test statistic in KCIPT
claiming improved calibration. Finally, other permutation-based approaches require large
sample sizes (Sen et al., 2017) or rely on densities (Kim et al., 2022), which do not exist in our
setting. Recently, Shah and Peters (2020) proposed a simple test based on double regression
for increased robustness and later generalized this technique to a kernel-based regression
techniques (Lundborg et al., 2022), which also applies to functional data via function-on-
function ridge regressions. In our setting, these regressions are difficult to perform as they
would have to map between arbitrary time segments of multivariate SDE solution paths.
Laumann et al. (2023), perhaps the closest work to ours, develop a conditional independence
test for functional data based on HSCIC (Park and Muandet, 2020) as the test statistic paired
with a permutation test (Berrett et al., 2019). They further combine existing techniques such
as the PC-algorithm (Glymour et al., 2019) with a regression-based method (Hoyer et al.,
2008) for a causal discovery heuristic. Crucially, their method assumes knowledge of P , a
X|Z
major challenge in our SDE setting. We focus on constraint-based methods leveraging the
ability of the signature kernel to capture filtrations on arbitrary intervals. Since Laumann
et al. (2023) propose the only other method we are aware of that can be applied in our setting,
we benchmark our CI test against them.
On a practical level, large conditioning sets inevitably imply curse-of-dimensionality type
issues as the ‘number possible values to condition on’ grows exponentially. Moreover, existing
approaches often assume observations in real Euclidean vector spaces and do not directly
generalize to path-valued random variables where no densities exist. On a theoretical level, CI
testing faces a fundamental impossibility in that there cannot be a universally powerful test
for all distributions while also controlling the type I error (Lundborg et al., 2022, Shah and
Peters, 2020). This ‘no free lunch’ statement highlights the importance of carefully selecting
an appropriate test for the setting under consideration. Currently, there exists no practical
CI test tailored to our case—a gap we fill.
2.3 Causal Discovery on Time Series
Discrete-time models generally rely on explicit functional relations between variables across a
fixed homogeneous time grid, i.e., X = f(X ,...,X ). This can be interpreted as a static
t t−τ t−1
structural causal model (SCM) over the ‘unrolled graph’, where variables at different time
steps are considered as distinct nodes (Peters et al., 2017). The direction of time implies that
edges across time steps can only point into the future (a maximum ‘lookback window’, τ ∈ N,
is common). This has been exploited in Granger-type approaches (Eichler, 2007, Eichler and
Didelez, 2010). Related techniques for causal discovery in the discrete-time model include
constraint-based methods (Runge et al., 2023, 2019, 2020) with extensions to scenarios lacking
causal sufficiency (Entner and Hoyer, 2010, Gerhardus and Runge, 2020, Malinsky and Spirtes,
2018), and score-based methods (Pamfil et al., 2020). Based on Granger causality (Granger,
1969), different works detect lagged causal influences in linear (Diks and Panchenko, 2006,
Granger, 1980) or non-linear (Marinazzo et al., 2008, Runge, 2020, Shojaie and Fox, 2022)
functional relationships. Due to the fundamental challenges encountered in discrete-time
systems (Runge, 2018), causal modeling in continuous-time dynamical systems has received
increasing attention.
5A growing body of work considers differential equations at equilibrium to obtain ‘solvable’
structural causal models that may include cycles (Bongers et al., 2018, Bongers and Mooij,
2018, Mooij et al., 2013). Due to their assumptions about the data generating mechanism, this
work does not apply to our setting. Other works instead aim at learning the full dynamical
law by leveraging invariance under the assumption of mass-action-kinetics (Peters et al.,
2022) or relying on non-convex optimization of heavily overparameterized neural network
models for reconstruction performance (Aliee et al., 2022, 2021, Bellot et al., 2022). In
continuous-time stochastic processes, so-called (conditional) local independence defines an
asymmetric independence relation (Mogensen et al., 2018, Schweder, 1970). Tests of local
independence may be used to learn a (partial) causal graph in, e.g., point process models and
diffusions (Didelez, 2008, Meek, 2014, Mogensen and Hansen, 2020, Mogensen et al., 2018).
3 Method
3.1 Causal Discovery in the Acyclic SDE Model
Assumption3.1. Weassumetheexistenceofaconditionalindependenceoracleforstatements
of the form
XI⊥⊥XJ | XK (4)
I J K
for disjoint I,J,K ⊂ {1,...,d} where only K may be empty, closed intervals I,J,K ⊂
[0,T], and where XH denotes the C([a,b],R|H|)-valued random variable ω (cid:55)→ ([a,b] ∋ t (cid:55)→
[a,b]
Xh(ω)h∈H). We assume access to finitely but arbitrarily many queries to the oracle.
t
Asymptotically, such an oracle can realistically be replaced in practice by a consistent finite-
sample CI test for path-valued random variables (see Section 3.2). The following example
demonstrates that constraint-based causal discovery of the full dependency graph is impossible
using just Assumption 3.1 when allowing for cycles.
Example 3.2. The oracle in Assumption 3.1 is not powerful enough to rule out a directed
edge X1 → X3 for an SDE model with the following dependency graph:
X1 X2 X3
We go through this example and describe a different type of oracle that would be required
for causal discovery in cyclic SDE models in Appendix A.2. Example 3.2 provides additional
motivation for considering acyclic SDE models, still allowing for loops. Loops are crucial in
the dynamic setting, as they represent dependencies of a variable on itself, infinitesimally into
the past.
We will use the oracle in three distinct ways (i,j ∈ [d], K ⊂ [d], {i,j}∩K = ∅):
• Xi is symmetrically conditionally independent of Xj given XK on [0,T], Xi⊥⊥ Xj | XK
sym
if
Xi ⊥⊥Xj | XK
[0,T] [0,T] [0,T]
• Xi is future-extended h-locally conditionally independent of Xj given XK at s, Xi⊥⊥+ Xj |
s,h
XK if2
Xi ⊥⊥Xj | Xj ,XK
[0,s] [s,s+h] [0,s] [0,s+h]
2The ‘+’ superscript is meant to emphasise the fact that we are also conditioning on the future of XK,
which is not usually done (cf. the definition of global non-causality in Florens and Fougere (1996)), and is
motivated by our causal discovery procedure.
6• Xi is conditionally h-locally self-independent given XK at s, Xi⊥⊥⟲ | XK if
s,h
Xi ⊥⊥Xi | XK
[0,s] [s,s+h] [0,s+h]
We will only need to work on two intervals, [0,s] and [s,s + h], with h > 0. Define the
lifted dependency graph G(cid:101)= (V(cid:101),E(cid:101)) by setting V(cid:101) := V 0⊔V 1, where ⊔ denotes disjoint union
and V ,V are two copies of V, whose elements we subscript with 0 and 1; include an edge
0 1
(i
0
→ i 1) ∈ E(cid:101) iff there is a loop (i → i) ∈ E, and for each edge (i → j) ∈ E with i ̸= j, include
edges (i
0
→ j 0),(i
1
→ j 1),(i
0
→ j 1) ∈ E(cid:101). Conversely, we say that the graph with edges E is
obtained by collapsing the graph with edges E(cid:101).
Proposition 3.3 (Markov property). Assume G is directed and acyclic except for loops. Then
G(cid:101) is acyclic, and the assignment of X [i
0,s]
to the node i
0
and X [i
s,s+h]
to i 1, for each i ∈ V
defines a fully observed SCM (Peters et al., 2017). In particular, its observational distribution
satisfies the global Markov property w.r.t. to G(cid:101). 3
Our proof is in Appendix A.3. Under Assumption 3.1 and assuming faithfulness for the SCM
of Proposition 3.3, we now develop modified versions of the PC algorithm. More details on
faithfulness and minimality can be found in Appendix A.4.
Algorithm 1 Causal discovery for acyclic SDE models.
1: V(cid:101) ←{k 0,k
1
|k ∈V}
E(cid:101) ←{i
0
→j 0, i
1
→j
1
|i,j ∈V, i̸=j}
∪{i →j |i,j ∈V}
0 1
2: for c=0,...,d−2 do ▷ edge recovery modulo loops
3: for i,j ∈V, i̸=j do
4: for K ⊆V \{i,j}, |K|=c,
s.t. (k
0
→j 1)∈E(cid:101) for k ∈K do
5: if Xi⊥⊥+ Xj |XK then
s,h
6: E(cid:101) ←E(cid:101)\{i
0
→j 0,i
1
→j 1,i
0
→j 1}
7: G =(V,E)←collapse(V(cid:101),E(cid:101))
8: for k ∈V do ▷ removing loops
9: if Xk⊥⊥⟲
s,h
|XpaG k\{k} then
10: E ←E\{k →k}
11: return G
Discovering the full DAG G. To discover the full graph including loops we propose
Algorithm1thatmakesuseofthetime-orderingviaourh-localindependencemodels(consider
s,h > 0 arbitrary but fixed). Algorithm 1 is kept conceptually simple and not an optimized
version. As suggested by Spirtes et al. (2000), we could for instance restrict the conditioning
set (K) to nodes that lie on undirected paths between i and j or introduce an additional
for-loop that slowly grows the cardinality of the conditioning set in the second stage (line 8 of
Algorithm 1 as well).
Theorem 3.4 (Causal discovery for acyclic SDEs). Algorithm 1 is sound and complete for the
SDE model eq. (1), assuming acyclicity except for loops and faithfulness: its output coincides
with the dependency graph G of the SDE.
The proof of the theorem can be found in Appendix A.6.
Remark 3.5. Ifweknewinadvancethatthereisnopath-dependency,thetestXi⊥⊥+ Xj (with
s,h
noadditionalconditioningset)isequivalenttothetestonvaluesoftheprocessXi⊥⊥Xj | Xj:
s s+h s
this is because Xj, for u ∈ [s,t] only depends on Xj via Xj. However, this kind of statement
u [s,t] s
no longer works in the conditional case (by arguments similar to Example 3.2), and testing
on whole paths is strictly necessary when allowing for path-dependency.
3We use the term Markov for the Markov property in causality, not as in ‘Markov process’.
7Remark 3.6. CI testing of SDEs is conceptually similar to static CI on discretely sampled data,
when the sampling rate is lower than the actual frequency at which causal effects propagate.
This is because, in continuous time and for any h > 0, there are going to be causal effects
that occur at time scales less than h. Whenever we just discretely sample time series with no
instantaneous effects, Peters et al. (2017, Thm. 10.3) provides full causal discovery, even in
the presence of cycles in the summary graph, by performing d2 tests Xi⊥⊥Xj | X[d]\{i,j} .
s s+∆s [0,s]
Potentially absent loops could then be removed as in Algorithm 1. Ignoring that this might
suffer from the conditioning set being too large, our signature method can be of help in
this setting too, if there is path-dependence: the conditioning variable can be taken to be
S(X[d]\{i,j}) .
0,s
Discovering and post-processing the CPDAG. Besides causal discovery of the full DAG
G using the asymmetric future-extended h-local independence criterion, we also propose a
constraint-based causal discovery algorithm based on the symmetric independence condition
that recovers the completed partially directed acyclic graph (CPDAG) of G with self-loops
removed, which represents the Markov equivalence class of G (Peters et al., 2017). We present
the algorithm together with a soundness and completeness proof in Appendix A.7. While
Algorithm 1 is ‘stronger’ in that it orients all edges, it (a) typically requires more CI tests, (b)
has larger conditioning sets in the ⊥⊥+ -CI tests, (c) requires the choice of a extra parameters
s,h
s,h for ⊥⊥+ , (d) runs CI tests on shorter time segments, potentially ‘losing information’
s,h
compared to the symmetric test ⊥⊥ . All these factors empirically negatively affect the
sym
reliability of the CI test, which may indicate that causal discovery of the CPDAG using ⊥⊥
sym
may be more reliable, despite yielding a weaker result at first.
Given these potential benefits, we propose to further augment the CPDAG discovery by a
post-processing step in which we aim at orienting all remaining unoriented edges (i,j) within
the Markov equivalence class represented by the CPDAG leveraging the following corollary.
Corollary 3.7 (post-processing). Let X = (X1,...,Xd) be a stochastic process satisfying
the SDE in eq. (1) such that the induced graph G = (V,E) is a DAG. Let G˜= (V,E˜) be the
CPDAG representing the Markov equivalence class of G. Then for all (i,j) ∈ E ⊂ E˜ with
(j,i) ∈ E˜ and i ̸= j it holds that Xj ⊥̸⊥ Xi but Xi ⊥⊥Xj.
[0,T] 0 [0,T] 0
This allows us to determine the orientation of remaining edges in a post-processing step that
crucially only requires unconditional independence tests only for the edges that are not yet
oriented in the CPDAG. Hence, we obtain an alternative algorithm to discover the full graph
that is sound and complete under the oracle assumption and faithfulness by (a) constructing
the CPDAG with the PC-algorithm using ⊥⊥ , and (b) a post-processing step where we test
sym
the unconditional independencies in Corollary 3.7 for all unoriented edges. We compare both
versions in our experiments.
3.2 Signature Kernel Conditional Independence Test
Our theoretical results in Section 3.1 make use of the assumption of a CI oracle. To make our
proposed algorithms practically applicable, we propose a flexible and powerful CI test on path-
valued random variables (on different time intervals), i.e., the conditions in Assumption 3.1.
The general idea of using the signature kernel for kernel-based hypothesis tests for stochastic
processes has recently also been proposed by Chevyrev and Oberhauser (2022), Salvi et al.
(2021c), but has been limited to standard two-sample tests or conditional independence tests
that do not take into account time ordering. The idea of using terms of the signature to
detect causality (not specifically with hypothesis testing) was also proposed in Giusti and Lee
(2020) and developed further in Glad and Woolf (2021). Assume we have observed n samples
(i) (i) (i)
of (potentially multi-variate) path segments X , Y , and Z of components X,Y,Z of
I J K
a stochastic process on arbitrary intervals I,J,K ⊂ [0,T] for i ∈ [n]. Using the signature
kernel K we compute the Gram matrices k ,k ,k ∈ Rn×n for these n samples, based
S XX YY ZZ
8SigKern=20 SigKern=60 Laun=20 Laun=60
SigKern=40 SigKern=80 Laun=40 Laun=80
1.00
0.75
0.50
0.25
0.00
0.5 1.0 1.5 2.0
ratioa21
a22
Figure 2: Test power when testing H : X1 ⊥̸⊥ X2 for different ratios a21 in the bivariate
0 [0,t] [0,t] a22
setting X1 → X2, where our CI test (SigKer) consistently outperforms the baseline from
Laumann et al. (2023) (Lau). Lines show means over 1000 SDEs (see main text) and shaded
regions show standard errors.
Table 1: SHD (×102) results of our method (SigKer) and the baselines CCM, PCMCI and
Granger causality in four different bivariate settings: linear SDEs, path-dependent SDEs,
non-linear SDEs and SDEs with diffusion dependency.
linear path-dependent non-linear diffusiondependency
(×102) n=200 n=400 n=200 n=400 n=200 n=400 n=200 n=400
CCM 100.1±5.0 78.9±4.9 107.7±4.9 138.5±4.6 19.8±3.0 15.8±2.7 79.2±4.9 76.7±4.8
Granger 103.0±2.5 116.7±2.2 92.3±1.9 100.0±2.4 54.4±3.6 57.4±3.8 95.0±2.7 89.1±2.4
PCMCI 125.9±4.8 55.5±4.4 88.1±4.4 114.7±4.4 71.3±2.3 85.1±1.7 74.3±2.2 90.0±1.5
SigKer 82.4±1.9 28.6±2.8 52.7±2.4 24.3±2.1 11.1±1.6 0.0±0.0 74.3±2.1 62.5±2.3
on which we can directly run kernel-based permutation CI tests such as KCIPT (Doran
et al., 2014) or SDCIT (Lee and Honavar, 2017) to test X ⊥⊥Y | Z . While the existing
I J K
consistency proof for KCIPT (also applying to SDCIT) makes use of the existence of a density,
we extend this proof to path-valued random variables.
Theorem 3.8 (informal). KCIPT with the signature kernel on path-space is consistent for
the tests in Assumption 3.1.
We provide the precise conditions and a proof in Appendix A.8. While such consistency proofs
provide theoretical grounding, as described in Section 2, CI tests must be carefully selected
and validated in a given domain empirically. Therefore, the extensive evaluation of our CI
test in the next section is perhaps more important in practice.
4 Experiments
First, we compare how different kernel-based CI tests perform in conjunction with the
signature kernel in Appendix C. Based on results in Figure 5 and the consistency statement
in Theorem 3.8, we use KCIPT in all subsequent experiments.
Baselines. We compare against established causal discovery methods in time series, specifi-
cally CCM (Sugihara et al., 2012), PCMCI (Runge et al., 2019), Granger causality (Granger,
1969), and the recent approach by Laumann et al. (2023). CCM and Granger causality
are restricted to bivariate settings and Laumann et al. (2023) conditional independence test
requires additional knowledge about the underlying distribution and is therefore not suited
for a comparison outside the unconditional case. Details about the baseline implementations
are in Appendix C.
9
rewoptsetTable 2: Test performance on the key building blocks for causal discovery. Here, ⊥⊥ refers
to ⊥⊥ except for X1⊥⊥X1 which means X1 ⊥⊥X1 . We use KCIPT for CI tests and
sym p f [0,t/2] [t/2,t]
bootstrapped HSIC unconditional independence. We simulate 40 samples of 512 trajectories
for 10 different SDEs (400 tests per column) with 25% of the 128 original observations dropped
uniformly at random. The error represent type I or type II error when the null hypothesis
H should be rejected (✗) or accepted (✓), respectively.
0
graph X1 X2 X3 X1 X2 X3 X1 X2 X3 X1 X1 X2 X1 X2
H0 X1⊥⊥X2|X3 X1⊥⊥X3|X2 X1⊥⊥X2|X3 X1⊥⊥X3|X2 X1⊥⊥X3|X2 Xp1⊥⊥Xf1 X1⊥⊥X2 X1⊥⊥X2
should
✓ ✗ ✓ ✓ ✗ ✓ ✗ ✓
reject
error 0.0025 0.1775 0.0000 0.0025 0.1250 0.0000 0.0450 0.0000
Metrics, model choices, hyperparameters. In all experiments, we use KCIPT for
conditional and the permutation-based version of HSIC for unconditional tests. We use
sigkerax for the signature kernel with an RBF lifting kernel that operates pointwise on
sequences. The length scale is selected via a median heuristic on pairwise Euclidean distances
between all observed time points across all samples, which we motivate in Appendix B.1. In
all bivariate settings, we fix the causal structure as X1 → X2 and generate multiple SDEs
from which we sample different numbers of trajectories. For causal discovery in d ∈ {3,4,5},
the DAG adjacency structures are sampled from a Erdo¨s–R´enyi model with edge probability
0.7. Following the causal discovery literature, we compute the Structural Hamming Distance
(SHD) as our main performance metric, where correctly predicting the absence of an edge or a
correctly oriented edge counts 0, predicting an edge where there should be none or vice versa
adds 1, and predicting an edge in the wrong direction counts 2 (see Appendix B.7). Except
for non-linear experiments, the assumed SDE model is (A,B ∈ Rd×d and c,d ∈ Rd)
dX =(AX +c)dt+Diag(BX +d)dW . (5)
t t t t
4.1 Power Analysis of the Bivariate Independence Test
In the first series of experiments, we measure the test power using the unconditional test in
the case X1 → X2 for SDEs of the form eq. (5), with constant diffusion B ≡ 0, d = 0.4
i
Figure 2 shows that test power swiftly approaches 1 for as few as 40-60 samples as the ratio
between causal- and self-interaction strength a21 at X2 increases, so as soon as the causal
a22
interaction of X1 feeding into X2 reaches a substantial level compared to X2 self-driven
dynamics. Our method consistently outperforms Laumann et al. (2023) across sample sizes.
In addition, since the signature kernel naturally handles irregularly observed paths, Figure 6
shows that our test also consistently outperforms the baselines even at high levels of data
missingness (for details see Appendix B.4).
4.2 Leveraging the Direction of Time
Next, we explore a bivariate scenario with X1 → X2, where we leverage the direction of time
to orient the edge using the future-extended h-local conditional independence with K = ∅ for
different interaction types.
Linear case. In the case of linear drift-interaction, SDEs are drawn from eq. (5) with
a ∼ U([0.8,1]), a ,a ∼ U([0,0.5]), a = 0, B ≡ 0, d ∼ U([0.3,0.4]). Table 1 (setting
21 11 22 12 i
linear) shows that our criterion not only accurately captures the presence of the edge but also
captures the direction of dependence for sufficient amounts of samples, outperforming both
CCM, Granger causality and PCMCI.
10future-hd=3 PCMCId=3 cond.ind.d=3 PCMCId=3 sympred=3 sympostd=4
future-hd=4 PCMCId=4 cond.ind.d=4 PCMCId=4 sympostd=3 sympred=5
future-hd=5 PCMCId=5 cond.ind.d=5 PCMCId=5 sympred=4 sympostd=5
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
400 425 450 475 500 525 550 575 600 400 425 450 475 500 525 550 575 600 0 100 200 300 400 500 600
numberofsamples numberofsamples numberofsamples
Figure 3: SHD on predicted graphs over sample sizes for different numbers of nodes d.
norm
Left: Full graph discovery using Algorithm 1 compared to PCMCI (evaluated against the full
ground truth graph). Middle: CPDAG discovery with the PC-algorithm and ⊥⊥ compared
sym
to PCMCI (evaluated against the CPDAG of the ground truth graph). Right: Full graph
discovery by post-processing the CPDAG via Corollary 3.7 (evaluated against the full ground
truth graph without loops). We outperform PCMCI both in inferring the full graph (with
both methods) and in inferring the CPDAG. Post-processing the CPDAG performs best across
settings, arguably due to the comparably ‘simple’ and fewer (conditional) independence tests
required. Lines are means of SHD and shaded areas standard deviations over 100 settings.
Path-dependent case. Sampling from the 3-dimensional SDE in eq. (5), with B ≡ 0, c ≡ 0,
all entries of A being zero except a ,a ∼ U([1,1.1]) and d = (d ,d ,0)⊤, d ∼ U([0.3,0.4])
23 31 1 2 i
effectively generates a bivariate scenario X1 → X2 with a path-dependency in X2 of the form
dX2 = µ2(X1 )dt+d dW1. Denoted by setting path-dependent, Table 1 again shows that
t [0,t] 2 t
the criterion captures the direction of dependence better than CCM, PCMCI and Granger.
Non-linear case. Our proposed criterion is also able to almost perfectly predict the graph
for non-linear SDEs as
(cid:18) X1(cid:19) (cid:18) −rωsin(ωt)(cid:19)
d t = dt+Diag(d⊤)dW
X2 rωtanh(X2) t
t t
where ν ∼ U([3,4]), ω = 2πν, r ∼ U([2,2.5]) and d ∼ U([0.3,0.4]), outperforming the
i
baselines (see Table 1).
Diffusion case. In the final bivariate experiment, we test the prediction-capability of our
independence criterion in setting (5) where all entries of A,B are zero except for a ,a ∼
11 22
U([0.5,1]) and b ∼ U([2.5,3]), and d ≡ 0. As the only method capable of handling this type
21
of dependence, we again outperform the baselines (see Table 1).
4.3 Building Blocks for Causal Discovery
The performance of the conditional and unconditional independence tests was evaluated
using widely recognized causal structures that are at the foundation of causal discovery.
Specifically, we examined scenarios involving two variables, both connected and unconnected,
and three-variable configurations comprising chain, fork, and collider structures. More details
on the experiments are found in Appendix B.5. The analysis of type I and type II errors, as
shown in Table 2, indicates robust performance for both conditional KCIPT and unconditional
HSIC tests. These results refer to the symmetric conditional independence ⊥⊥ , while
sym
future-extended h-local conditional independence are presented in Table 6. In Appendix B.5,
further results in the diffusion dependency setting (Table 5).
4.4 Causal Discovery in Higher Dimensions
After having dealt with the bivariate case and the building blocks of causal discovery, we
now test the performance of our CI test in conjunction with our causal discovery algorithms.
11
mronDHS mronDHS mronDHSTable 3: Comparison of pairs trading performance metrics demonstrating superiority of our
method over ADF, Granger, and their combined approach (ADF & Granger) in most metrics.
The symbol ↑ indicates ‘higher is better’, while ↓ signifies ‘lower is better’.
return ↑ APR ↑ Sharpe ↑ maxDD ↓ maxDDD ↓
ADF 0.004 0.004 0.090 0.087 230
Granger -0.010 -0.011 -0.230 0.056 219
ADF & Granger 0.008 0.008 0.242 0.022 153
SigKer (ours) 0.076 0.077 1.500 0.027 21
We sample 100 DAG adjacency structures for dimensions d ∈ {3,4,5} and draw samples
from d-dimensional SDEs as in (5) satisfying this given adjacency structure and a ∼
ij
U([−2,−1]∪[1,2]),j ̸= i, a ∼ U([−0.5,0.5]).
ii
First,werunourfullgraphdiscoveryAlgorithm1andshowinFigure3(left)thatitconsistently
outperforms PCMCI on full graph discovery (including loops). We then test the symmetric CI
criterion⊥⊥ withtheregularPC-algorithm(soundandcompleteasshowninAppendixA.7).
sym
WeevaluatethefoundCPDAGagainstthegroundtruthDAGviathenormalizedSHDwithout
testing for self-loops. Figure 3(middle) shows that across all evaluated dimensions and sample
sizes, our method consistently outperforms PCMCI. Note that in this comparison, we only
consider whether the CPDAG has been predicted correctly, i.e., the SHD is less stringent
norm
in this case compared to a full DAG comparison. In Figure 3(right), we then show how the
inferred CPDAGs match the ground truth DAG (now SHD compares to the actual ground
norm
truth DAG) and how much we gain from the post-processing step that aims at orienting
remaining unoriented edges. The ultimate performance of the post-processed CPDAG is the
strongest in our empirical findings.
4.5 Real-World Pairs Trading Example
Finally, we apply our method on real-world data for evaluating pair trading strategies on
ten stocks from the VBR Small-Cap ETF over a three-year period from January 1, 2010, to
December 31, 2012. While financial data typically lacks a ground truth for validation, we
assess the effectiveness of our method by quantifying the product-and-loss (P&L) profile of
the pair trading strategy generated via our model. We select pairs based on the pairwise
p-values of different hypothesis tests: 1) cointegration, tested via the classical Augmented
Dickey-Fuller (ADF) test, 2) classical Granger causality test and 3) our method. As shown
in Table 3, the P&L of our trading strategy outperforms the P&L of the two benchmarks in
terms of most classical trading metrics: total return, APR, Sharpe ratio, maxDDD. More
details on the implementation can be found in Appendix B.6.
5 Conclusions and Future Work
In this work, we introduced a kernel-based test for conditional independence on path-space,
utilizing signature kernels, and applied it to constraint-based causal discovery in stochastic
dynamical systems. We developed an algorithm for comprehensive causal discovery in acyclic
systems. This algorithm operates under the assumptions of faithfulness and the availability of
a conditional independence oracle.
To deploy this algorithm in practice, we developed a consistent conditional independence test
based on the signature kernel and extensively evaluated its performance empirically. Our
results indicate that our method outperforms existing baselines for causal discovery on time
series, highlighting its potential in complex dynamic systems. Additionally, we successfully
12applied our method to a real-world pairs trading example, where it also demonstrated strong
performance.
An interesting direction for future work involves extending our approach to cyclic settings
and accommodating dynamics with temporally changing causal structure or with partial
observations. Future research could expand on these applications in real-world scenarios, to
evaluate the method’s practical impact in concrete domains. Moreover, the field of causality
represents just one of the many potential applications of conditional independence tests for
path-valued random variables. Exploring other applications in different domains could also
catalyze new insights and developments.
13Acknowledgments
Cecilia Casolo is supported by the DAAD programme Konrad Zuse Schools of Excellence
in Artificial Intelligence, sponsored by the Federal Ministry of Education and Research.
Emilio Ferrucci is supported by UKRI EPSRC Programme Grant EP/S026347/1. Søren
Wengel Mogensen is supported by Independent Research Fund Denmark (DFF-International
Postdoctoral Grant 0164-00023B) and a member of the ELLIIT Strategic Research Area at
Lund University.
References
Aliee, H., Richter, T., Solonin, M., Ibarra, I., Theis, F., and Kilbertus, N. (2022). Sparsity in
continuous-depth neural networks. Advances in Neural Information Processing Systems,
35:901–914. 6
Aliee, H., Theis, F. J., and Kilbertus, N. (2021). Beyond predictions in neural odes: Identifi-
cation and interventions. arXiv preprint arXiv:2106.12430. 6
Annadani, Y., Pawlowski, N., Jennings, J., Bauer, S., Zhang, C., and Gong, W. (2023).
Bayesdag: Gradient-based posterior sampling for causal discovery. arXiv preprint
arXiv:2307.13917. 1
Assaad, C. K., Devijver, E., and Gaussier, E. (2022). Discovery of extended summary graphs
in time series. In Uncertainty in Artificial Intelligence, pages 96–106. PMLR. 2
Baba, K., Shibata, R., and Sibuya, M. (2004). Partial correlation and conditional correlation
as measures of conditional independence. Australian & New Zealand Journal of Statistics,
46(4):657–664. 4
Bellot, A., Branson, K., and van der Schaar, M. (2022). Neural graphical modelling in
continuous-time: consistency guarantees and algorithms. In International Conference on
Learning Representations. 6
Berrett, T., Wang, Y., Barber, R., and Samworth, R. (2019). The conditional permutation
test for independence while controlling for confounders. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), 82. 5
Bongers, S., Blom, T., and Mooij, J. M. (2018). Causal modeling of dynamical systems. arXiv
preprint arXiv:1803.08784. 6
Bongers, S., Forr´e, P., Peters, J., and Mooij, J. M. (2021). Foundations of structural causal
models with cycles and latent variables. The Annals of Statistics, 49(5):2885 – 2915. 21, 32
Bongers, S. and Mooij, J. M. (2018). From random differential equations to structural causal
models: The stochastic case. arXiv preprint arXiv:1803.08784. 6
Brouillard, P., Lachapelle, S., Lacoste, A., Lacoste-Julien, S., and Drouin, A. (2020). Differen-
tiable causal discovery from interventional data. Advances in Neural Information Processing
Systems, 33:21865–21877. 1
Cass, T., Lyons, T., and Xu, X. (2023). Weighted signature kernels. Annals of Applied
Probability. 4, 26
Charpentier, B., Kibler, S., and Gu¨nnemann, S. (2022). Differentiable DAG sampling. In
International Conference on Learning Representations. 1
Chen, H., Du, K., Yang, X., and Li, C. (2022). A review and roadmap of deep learning causal
discovery in different variable paradigms. arXiv preprint arXiv:2209.06367. 1
14Chevyrev, I. and Oberhauser, H. (2022). Signature moments to characterize laws of stochastic
processes. J. Mach. Learn. Res., 23(1). 4, 8
Christgau, A. M., Petersen, L., and Hansen, N. R. (2023). Nonparametric conditional local
independence testing. The Annals of Statistics, 51(5):2116 – 2144. 22
Cirone, N. M., Lemercier, M., and Salvi, C. (2023). Neural signature kernels as infinite-width-
depth-limits of controlled resnets. In Proceedings of the 40th International Conference on
Machine Learning, ICML’23. JMLR.org. 3
Cochrane, T., Foster, P., Chhabra, V., Lemercier, M., Lyons, T., and Salvi, C. (2021). Sk-tree:
a systematic malware detection algorithm on streaming trees via the signature kernel. In
2021 IEEE international conference on cyber security and resilience (CSR), pages 35–40.
IEEE. 3
Daudin, J. (1980). Partial association measures and an application to qualitative regression.
Biometrika, 67(3):581–590. 5
Didelez, V. (2008). Graphical models for marked point processes based on local independence.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(1):245–264.
6
Diks, C. and Panchenko, V. (2006). A new statistic and practical guidelines for nonparametric
granger causality testing. Journal of Economic Dynamics and Control, 30(9-10):1647–1669.
5
Doran, G., Muandet, K., Zhang, K., and Sch¨olkopf, B. (2014). A permutation-based kernel
conditional independence test. In UAI, pages 132–141. 5, 9, 26, 27, 29
Eichler, M. (2007). Granger causality and path diagrams for multivariate time series. Journal
of Econometrics, 137(2):334–353. 5
Eichler, M. and Didelez, V. (2010). On granger causality and the effect of interventions in
time series. Lifetime data analysis, 16:3–32. 5
Entner, D. and Hoyer, P. O. (2010). On causal discovery from time series data using fci.
Probabilistic graphical models, pages 121–128. 5
Evans, L. C. (2006). An introduction to stochastic differential equations version 1.2. Lecture
Notes, UC Berkeley. 2
Florens,J.-P.andFougere,D.(1996). Noncausalityincontinuoustime. Econometrica: Journal
of the Econometric Society, pages 1195–1212. 6, 22
G´egout-Petit, A. and Commenges, D. (2010). A general definition of influence between
stochastic processes. Lifetime data analysis, 16(1):33–44. 22
Gerhardus, A.andRunge, J.(2020). High-recallcausaldiscoveryforautocorrelatedtimeseries
with latent confounders. Advances in Neural Information Processing Systems, 33:12615–
12625. 5
Giusti, C. and Lee, D. (2020). Iterated integrals and population time series analysis. In Baas,
N. A., Carlsson, G. E., Quick, G., Szymik, M., and Thaule, M., editors, Topological Data
Analysis, pages 219–246, Cham. Springer International Publishing. 8
Glad, W. and Woolf, T. (2021). Path signature area-based causal discovery in coupled time
series. In Ma, S. and Kummerfeld, E., editors, Proceedings of The 2021 Causal Analysis
Workshop Series, volume 160 of Proceedings of Machine Learning Research, pages 21–38.
PMLR. 8
15Glymour, C., Zhang, K., and Spirtes, P. (2019). Review of causal discovery methods based on
graphical models. Frontiers in genetics, 10:524. 1, 5
Granger, C.W.(1969). Investigatingcausalrelationsbyeconometricmodelsandcross-spectral
methods. Econometrica: journal of the Econometric Society, pages 424–438. 5, 9
Granger, C. W. (1980). Testing for causality: A personal viewpoint. Journal of Economic
Dynamics and control, 2:329–352. 5
Gretton, A., Borgwardt, K., Rasch, M., Scho¨lkopf, B., and Smola, A. (2006). A kernel method
for the two-sample-problem. Advances in neural information processing systems, 19. 5
Gretton, A., Fukumizu, K., Teo, C., Song, L., Scho¨lkopf, B., and Smola, A. (2007). A kernel
statistical test of independence. Advances in neural information processing systems, 20. 4,
29
H¨agele, A., Rothfuss, J., Lorch, L., Somnath, V. R., Sch¨olkopf, B., and Krause, A. (2023).
Bacadi: Bayesian causal discovery with unknown interventions. In International Conference
on Artificial Intelligence and Statistics, pages 1411–1436. PMLR. 1
Hasan, U., Hossain, E., and Gani, M. O. (2023). A survey on causal discovery methods for iid
and time series data. Transactions on Machine Learning Research. 2
Hoyer, P., Janzing, D., Mooij, J. M., Peters, J., and Sch¨olkopf, B. (2008). Nonlinear causal
discovery with additive noise models. Advances in neural information processing systems,
21. 5
Issa, Z., Horvath, B., Lemercier, M., and Salvi, C. (2023). Non-adversarial training of neural
sdes with signature kernel scores. Advances in Neural Information Processing Systems. 3
Janzing, D., Balduzzi, D., Grosse-Wentrup, M., and Scho¨lkopf, B. (2013). Quantifying causal
influences. THE ANNALS of STATISTICS, pages 2324–2358. 28
Javier, P. J. E. (2021). causal-ccm a Python implementation of Convergent Cross Mapping.
32
Ke, N. R., Bilaniuk, O., Goyal, A., Bauer, S., Larochelle, H., Pal, C., and Bengio, Y. (2020).
Learning neural causal models from unknown interventions. 1
Ke, N. R., Chiappa, S., Wang, J. X., Bornschein, J., Goyal, A., Rey, M., Weber, T., Botvinick,
M., Mozer, M. C., and Rezende, D. J. (2023). Learning to induce causal structure. In
International Conference on Learning Representations. 1
Kim, I., Neykov, M., Balakrishnan, S., and Wasserman, L. (2022). Local permutation tests
for conditional independence. The Annals of Statistics, 50(6):3388–3414. 5
Kir´aly, F. J. and Oberhauser, H. (2019). Kernels for sequentially ordered data. Journal of
Machine Learning Research, 20. 3
Laumann,F.,VonKu¨gelgen,J.,Park,J.,Scho¨lkopf,B.,andBarahona,M.(2023).Kernel-based
independence tests for causal structure learning on functional data. Entropy, 25(12):1597.
5, 9, 10, 28, 30, 32
Lauritzen, S. and Sadeghi, K. (2018). Unifying Markov properties for graphical models. The
Annals of Statistics, 46(5):2251 – 2278. 25
Lauritzen, S. L., Dawid, A. P., Larsen, B. N., and Leimer, H.-G. (1990). Independence
properties of directed markov fields. Networks, 20(5):491–505. 26
16Lawrence, A. R., Kaiser, M., Sampaio, R., and Sipos, M. (2021). Data generating process to
evaluate causal discovery techniques for time series data. arXiv preprint arXiv:2104.08043.
2
Lee, S. and Honavar, V. G. (2017). Self-discrepancy conditional independence test. In
Uncertainty in artificial intelligence, volume 33. 5, 9, 29
Lemercier, M., Salvi, C., Damoulas, T., Bonilla, E., and Lyons, T. (2021). Distribution
regression for sequential data. In International Conference on Artificial Intelligence and
Statistics, pages 3754–3762. PMLR. 3
Li, C. and Fan, X. (2020). On nonparametric conditional independence tests for continuous
variables. Wiley Interdisciplinary Reviews: Computational Statistics, 12(3):e1489. 4
Lorch, L., Rothfuss, J., Sch¨olkopf, B., and Krause, A. (2021). Dibs: Differentiable bayesian
structure learning. Advances in Neural Information Processing Systems, 34:24111–24123. 1
Lundborg, A. R., Shah, R. D., and Peters, J. (2022). Conditional independence testing in
hilbert spaces with applications to functional data analysis. Journal of the Royal Statistical
Society Series B: Statistical Methodology, 84(5):1821–1850. 5
Malinsky, D. and Spirtes, P. (2018). Causal structure learning from multivariate time series
in settings with unmeasured confounding. In Proceedings of 2018 ACM SIGKDD workshop
on causal discovery, pages 23–47. PMLR. 5
Marinazzo, D., Pellicoro, M., and Stramaglia, S. (2008). Kernel method for nonlinear granger
causality. Physical review letters, 100(14):144103. 5
Meek, C. (2014). Toward learning graphical and causal process models. In CI@ UAI, pages
43–48. 6
Mogensen, S. W. (2023). Weak equivalence of local independence graphs. arXiv preprint
arXiv:2302.12541. 2
Mogensen, S. W. and Hansen, N. R. (2020). Markov equivalence of marginalized local
independence graphs. The Annals of Statistics, 48(1):539 – 559. 6
Mogensen, S. W. and Hansen, N. R. (2022). Graphical modeling of stochastic processes driven
by correlated noise. Bernoulli, 28(4):3023 – 3050. 2
Mogensen, S. W., Malinsky, D., and Hansen, N. R. (2018). Causal learning for partially ob-
servedstochasticdynamicalsystems. InConference on Uncertainty in Artificial Intelligence,,
pages 350–360. 6
Montagna, F., Mastakouri, A. A., Eulig, E., Noceti, N., Rosasco, L., Janzing, D., Aragam, B.,
and Locatello, F. (2023a). Assumption violations in causal discovery and the robustness of
score matching. In Thirty-seventh Conference on Neural Information Processing Systems. 1
Montagna, F., Noceti, N., Rosasco, L., Zhang, K., and Locatello, F. (2023b). Scalable causal
discovery with score matching. In van der Schaar, M., Zhang, C., and Janzing, D., editors,
Proceedings of the Second Conference on Causal Learning and Reasoning, volume 213 of
Proceedings of Machine Learning Research, pages 752–771. PMLR. 1
Mooij, J. M., Janzing, D., and Sch¨olkopf, B. (2013). From ordinary differential equations
to structural causal models: the deterministic case. In Proceedings of the Twenty-Ninth
Conference on Uncertainty in Artificial Intelligence, UAI’13, page 440–448, Arlington,
Virginia, USA. AUAI Press. 6
17Muandet, K., Fukumizu, K., Sriperumbudur, B., Sch¨olkopf, B., et al. (2017). Kernel mean
embedding of distributions: A review and beyond. Foundations and Trends® in Machine
Learning, 10(1-2):1–141. 4
Pamfil, R., Sriwattanaworachai, N., Desai, S., Pilgerstorfer, P., Georgatzis, K., Beaumont,
P., and Aragam, B. (2020). Dynotears: Structure learning from time-series data. In
International Conference on Artificial Intelligence and Statistics, pages 1595–1605. PMLR.
5
Park, J. and Muandet, K. (2020). A measure-theoretic approach to kernel conditional mean
embeddings. Advances in neural information processing systems, 33:21247–21259. 4, 5, 27
Pearl, J. (2009). Causality. Cambridge university press. 26
Peters, J., Bauer, S., and Pfister, N. (2022). Causal models for dynamical systems. In
Probabilistic and Causal Inference: The Works of Judea Pearl, pages 671–690. Association
for Computing Machinery. 2, 6, 21
Peters, J., Janzing, D., and Sch¨olkopf, B. (2017). Elements of causal inference: foundations
and learning algorithms. The MIT Press. 5, 7, 8, 22, 23
Rogers, L. C. G. and Williams, D. (2000). Diffusions, Markov processes, and martingales.
Vol. 2. Cambridge Mathematical Library. Cambridge University Press, Cambridge. Itˆo
calculus, Reprint of the second (1994) edition. 2, 21, 23, 24
Rolland, P., Cevher, V., Kleindessner, M., Russell, C., Janzing, D., Sch¨olkopf, B., and
Locatello, F. (2022). Score matching enables causal discovery of nonlinear additive noise
models. In International Conference on Machine Learning, pages 18741–18753. PMLR. 1
Runge, J. (2018). Causal network reconstruction from time series: From theoretical assump-
tions to practical estimation. Chaos: An Interdisciplinary Journal of Nonlinear Science,
28(7):075310. 5
Runge, J. (2020). Discovering contemporaneous and lagged causal relations in autocorrelated
nonlinear time series datasets. In Conference on Uncertainty in Artificial Intelligence, pages
1388–1397. PMLR. 5
Runge, J., Gerhardus, A., Varando, G., Eyring, V., and Camps-Valls, G. (2023). Causal
inference for time series. Nature Reviews Earth & Environment, 4(7):487–505. 2, 5
Runge, J., Nowack, P., Kretschmer, M., Flaxman, S., and Sejdinovic, D. (2019). Detecting
and quantifying causal associations in large nonlinear time series datasets. Science advances,
5(11):eaau4996. 2, 5, 9, 32
Runge, J., Tibau, X.-A., Bruhns, M., Mun˜oz-Mar´ı, J., and Camps-Valls, G. (2020). The
causality for climate competition. In NeurIPS 2019 Competition and Demonstration Track,
pages 110–120. PMLR. 5
Salvi, C., Cass, T., Foster, J., Lyons, T., and Y., W. (2021a). The signature kernel is the
solution of a Goursat PDE. SIAM Journal on Mathematics of Data Science, 3(3):873–899.
3, 4
Salvi, C., Lemercier, M., Cass, T., Bonilla, E. V., Damoulas, T., and Lyons, T. J. (2021b).
Siggpde: Scaling sparse gaussian processes on sequential data. In International Conference
on Machine Learning, pages 6233–6242. PMLR. 3
Salvi, C., Lemercier, M., Liu, C., Horvath, B., Damoulas, T., and Lyons, T. (2021c). Higher
order kernel mean embeddings to capture filtrations of stochastic processes. Advances in
Neural Information Processing Systems, 34:16635–16647. 3, 8
18Schweder, T. (1970). Composable markov processes. Journal of applied probability, 7(2):400–
410. 6, 22
Seabold, S. and Perktold, J. (2010). Statsmodels: Econometric and statistical modeling with
python. In Proceedings of the 9th Python in Science Conference, volume 57, pages 10–25080.
Austin, TX. 32
Sen, R., Suresh, A. T., Shanmugam, K., Dimakis, A. G., and Shakkottai, S. (2017). Model-
powered conditional independence test. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach,
H., Fergus, R., Vishwanathan, S., and Garnett, R., editors, Advances in Neural Information
Processing Systems, volume 30. Curran Associates, Inc. 5
Shah, R. D. and Peters, J. (2020). The hardness of conditional independence testing and the
generalised covariance measure. The Annals of Statistics, 48(3):1514–1538. 5
Shojaie, A. and Fox, E. B. (2022). Granger causality: A review and recent advances. Annual
Review of Statistics and Its Application, 9:289–319. 5
Singer, H. (1992). Dynamic structural equations in discrete and continuous time. In Economic
Evolution and Demographic Change: Formal Models in Social Sciences, pages 306–320.
Springer. 2
Sokol, A. and Hansen, N. (2013). Causal interpretation of stochastic differential equations.
Electronic Journal of Probability, 19. 22
Spirtes, P., Glymour, C. N., and Scheines, R. (2000). Causation, prediction, and search. MIT
press. 1, 7, 23
Strobl, E. V., Zhang, K., and Visweswaran, S. (2019). Approximate kernel-based conditional
independence tests for fast non-parametric causal discovery. Journal of Causal Inference,
7(1). 5
Sugihara, G., May, R., Ye, H., Hsieh, C.-h., Deyle, E., Fogarty, M., and Munch, S. (2012).
Detecting causality in complex ecosystems. science, 338(6106):496–500. 9
Sz´ekely, G. J., Rizzo, M. L., and Bakirov, N. K. (2007). Measuring and testing dependence by
correlation of distances. Ann. Statist. 35 (6) 2769 - 2794. 32
Tsamardinos, I. and Borboudakis, G. (2010). Permutation testing improves bayesian network
learning. In Joint European conference on machine learning and knowledge discovery in
databases, pages 322–337. Springer. 4
Verma, T. and Pearl, J. (1990). Equivalence and synthesis of causal models. In Proceedings
of the Sixth Annual Conference on Uncertainty in Artificial Intelligence, UAI ’90, page
255–270, USA. Elsevier Science Inc. 23, 24
Vowels, M. J., Camgoz, N. C., and Bowden, R. (2022). D’ya like dags? a survey on structure
learning and causal discovery. ACM Computing Surveys, 55(4):1–36. 1
Yu, Y., Chen, J., Gao, T., and Yu, M. (2019). Dag-gnn: Dag structure learning with graph
neural networks. In International Conference on Machine Learning, pages 7154–7163.
PMLR. 1
Zhang, K., Peters, J., Janzing, D., and Sch¨olkopf, B. (2011). Kernel-based conditional
independence test and application in causal discovery. In Proceedings of the Twenty-Seventh
Conference on Uncertainty in Artificial Intelligence, UAI’11, page 804–813, Arlington,
Virginia, USA. AUAI Press. 4, 29
19Zhang, Q., Filippi, S., Flaxman, S., and Sejdinovic, D. (2017). Feature-to-feature regression
for a two-step conditional independence test. 4
Zheng, X., Aragam, B., Ravikumar, P. K., and Xing, E. P. (2018). Dags with no tears:
Continuous optimization for structure learning. Advances in neural information processing
systems, 31. 1
Zheng, X., Dan, C., Aragam, B., Ravikumar, P., and Xing, E. (2020). Learning sparse
nonparametric dags. In International Conference on Artificial Intelligence and Statistics,
pages 3414–3425. PMLR. 1
20A Proofs and theoretical digressions
A.1 Intuition and Details for the SDE Model
Making the actual dependence of µk and σk on their arguments in eq. (1) explicit, we can
rewrite it as
(cid:40) dXk = µk(Xpa k)dt+σk(Xpa k)dWk,
t [0,t] [0,t] t (6)
Xk = xk k ∈ [d].
0 0
Thenµk,σk arefunctionsdefinedonC([0,+∞),Rdim(pa k))(orsomesuitablesubspacethereof):
Lipschitz conditions on the coefficients that guarantee existence and uniqueness for this type of
SDE can be found in Rogers and Williams (2000), which we assume to hold throughout. Each
Wk is an m -dimensional Brownian motion (a collection of m independent 1-dimensional
k k
Brownian motions, that is), σk maps to the space of n ×m -dimensional matrices, and the
k k
noises Wk together with the (possibly random) initial conditions xk are jointly independent.
0
In other words, the system can be written as an n := n +...+n -dimensional SDE driven
1 d
by an m := m +...+m -dimensional Brownian motion, with a block-diagonal diffusion
1 d
coefficient σ (since the noise is unobserved, this structure has to be imposed if we wish not to
deal with unobserved confounding). The superscript pa refers to the parents of the kth node
k
in G, which may (and most often does) contain k itself. Intuitively, all of this means that, for
all times t and small increments ∆t, the increment of the solution Xk −Xk is random with
t+∆t t
distribution that is well-approximated by a multivariate normal with mean
µk(Xpa
k)∆t and
[0,t]
covariance function
σk(Xpa k)σk(Xpa k)⊺
∆t, and independent of the history of the system up
[0,t] [0,t]
to time t. This interpretation can actually be made precise by showing that these piecewise
constant paths converge in law to the true solution (usually known as the weak solution, when
viewed in this way).
We will refer to G as the dependency graph of the eq. (1), which we refer to as the SDE model
for brevity. Compared to that of Peters et al. (2022), this model is slightly more general in
that (i) it allows for path-dependency and (ii) for each node to represent a multidimensional
process; the special case of state-dependent SDE—i.e., in which µ and σ only depend on X ,
t
the value of X at time t—continues to be an important special case, although our model also
accommodates delayed SDEs (the coefficients depend on the value of X at a prior instant in
time, e.g., X for fixed or possibly random/time-dependent τ > 0), and SDEs that depend
t−τ
on quantities involving the whole past of X, such as the average
t−1(cid:82)t
X ds. We note that
0 s
the choice not to make the coefficients explicitly time-dependent is deliberate and motivated
by the requirement that the system be causally stationary (the causal relations between the
variables do not change over time).
Given that we are considering the dynamic setting, it is generally natural to allow for G to
have cycles. This comes at no additional requirement of consistency constraints as it does in
the static case (see for example Bongers et al. (2021)), since the causal arrows in the model
eq. (1) should be thought of as ‘pointing towards the infinitesimal future, with infinitesimal
magnitude’, integrated over the whole time interval considered. Indeed, the system of SDEs
does not require any global consistency to be well-posed, other than the regularity and growth
conditions that guarantee global-in-time existence and uniqueness. On the other hand, we will
be interested in the potential for constraint-based causal discovery of such systems, qualified
by the following assumption on the types of conditional independencies that we allow to be
tested in continuous time:
A.2 Counterexample for Cyclic Causal Discovery in the SDE Model
We restate our example, namely that the oracle in Assumption 3.1 is not powerful enough to
rule out a directed edge X1 → X3 for an SDE model with the following dependency graph:
21X1 X2 X3
Testing X1 ⊥⊥X3 | X2,3 , will not remove the edge, due to the open path
[0,s] [s,s+h] [0,s]
X1 → X1 → X2 → X3 .
[0,s] [s,s+h] [s,s+h] [s,s+h]
Testing X1 ⊥⊥X3 | X3 ,X2 , on the other hand, runs into the collider
[0,s] [s,s+h] [0,s] [0,s+h]
X1 → X1 → X2 ← X3 .
[0,s] [s,s+h] [s,s+h] [s,s+h]
There is no other way of splitting up the time interval (even allowing for more than 2 subin-
tervals) that overcomes both these problems: when testing X1 ⊥⊥X3 (as is necessary
[0,s] [s,s+h]
in order to rely on time to obtain direction of the arrow), if X2 is not conditioned on over
[s,s+h] it will be a mediator, and if it is, it will be a collider.
X1
X2
X3
Figure 4: The blue highlighter is for intervals over which the path is being tested for
independence, while orange is for conditioning. The first figure illustrates the path opened
by the unconditioned-on mediator X2 , while the second illustrates the path opened by
[s,s+h]
the same variable acting as a conditioned-on collider. While not quite possible to draw
figures this way, it is helpful to think of the arrows as pointing from values to increments
X → (Y −Y ), infinitesimally, with the total causal effect accrued over time.
t t+dt t
What would be needed to detect the edge in the above example (and to perform causal
discovery more generally in the cyclic case), is the availability of tests of strong instantaneous
non-causality in the Granger sense as defined in Florens and Fougere (1996). This reflects the
fact that the SDE model is ‘acyclic at infinitesimal scales’. Unfortunately, such a property—
which has to do with the Doob-Meyer decompositions of functions of the process w.r.t. two
different filtrations—is much more difficult to test for, as it is infinitesimal in nature. Recent
progress in this direction was made in Christgau et al. (2023) for the case of SDEs driven
by jump processes; this local independence criterion, which goes back to Schweder (1970), is
however weak in the sense that it only takes into account the Doob-Meyer decomposition of
X and not functions of it, and is not therefore able, for example, to detect dependence in
the diffusion coefficient G´egout-Petit and Commenges (2010). Here we take the view that a
continuously-indexed path, queried over an interval, can be considered as an observational
distribution (cf. the discussion in Sokol and Hansen (2013) on whether the infinitesimal
generator can be considered ‘observational’).
A.3 Proof of Proposition 3.3
Proof of Proposition 3.3. Since G is acyclic, a directed cycle in G(cid:101) that is not a loop must
contain nodes both in V ,V . But there can be no such directed cycles, since edges only travel
0 1
in the direction V
0
→ V 1, by construction of G(cid:101). Since G(cid:101) also free of loops, it is a DAG. We
now show that there exists an SCM over G(cid:101) with the path-valued random variables of the
statement: Markovianity will then follow from Peters et al. (2017, Proposition 6.31), original
22to Verma and Pearl (1990). In other words, we must show there exist Borel-measurable
functions (setting t := s+h)
Fk: Rdim(pa k)×C([0,s],Rm k)×C([0,s],Rdim(pa k\{k})) ⇀ C([0,s],Rn k)
0
(xpa(k) ,Wk ;Xpa k\{k} ) (cid:55)→ Xk
0 [0,s] [0,s] [0,s]
and
Fk: C([s,t],Rm k)×C([0,s],Rdim(pa k))×C([s,t],Rdim(pa k\{k})) ⇀ C([0,s],Rn k)
1
(Wk ;Xpa k,Xpa k\{k} ) (cid:55)→ Xk
[s,t] [0,s] [s,t] [0,s]
withtheinitialconditionsx0andBrownianpathsegmentsWk ,Wk collectivelyindependent.
[0,s] [s,t]
This independence follows from the definition of the SDE model and from independence of
Brownian increments over disjoint or consecutive intervals. These functions are partial in that
they are not defined on the whole space of continuous functions on the interval: we only need
to show them to be defined on a measurable set of paths that contains all solutions of SDEs
on the required interval. Fk is defined as follows, with care to make explicit the dependence
1
on the solution on the two intervals via the operation of path-concatenation ∗:
(cid:90) u
Fk(Xk ) = solution of Xk = Xk + µk(Xpa k\{k} ∗Xpa k\{k} ,Xk ∗Xk )dt
1 [s,t] u s [0,s] [s,u] [0,s] [s,u]
0
+σk(Xpa k\{k} ∗Xpa k\{k} ,Xk ∗Xk )dWk
[0,s] [s,u] [0,s] [s,u] t
We have also separated out the k-th component of the solution from the rest of the arguments
on which σk,µk are dependent, which we consider part of the measurable adapted dependence
on Wpa\{k} needed in the existence and uniqueness theorem. Fk is defined similarly (with no
0
dependence on past path-segments and with xk replacing Xk). Such functions are well-known
0 s
to be well-defined and measurable Rogers and Williams (2000, Theorem 10.4).
A.4 Faithfulness and causal minimality
We proceed with a brief and informal discussion of causal minimiality and faithfulness, a
notably delicate topic even in the case of standard SCMs. We remain in our setting of
acyclicity + loops. It is not difficult to convert the classical counterexamples of faithfulness
(Example 6.34 in Peters et al. (2017)) into ones for our SDE model. It is also reasonable
to conjecture, following Theorem 3.2 in Spirtes et al. (2000) that, allowing for the diffusion
coefficients σ to range in some ‘reasonable’ finite-dimensional family (such as linear or affine
functions) and drawing the parameters of such family from a distribution which has positive
densityw.r.t.theLebesguemeasure(andindependentlyofthenoise), theresultingdistribution
on path-space, split over [0,s] and [s,t], will be a.s. faithful, for any 0 < s < t w.r.t. the
lifted dependency graph. We do not attempt a proof of such statement here, which might
require quite some effort and technique. Causal minimality, on the other hand, is generally
easier to understand and can be expected to hold for the arrow Xi → Xj whenever
[0,s] [s,t]
the following condition is satisfied: the conditional distribution X uj|Xpa j\{i} = xj admits a
positive density on some submanifold Mx of Rn and for all u there exist xi,xi ∈ Mx, xi ≠ xi,
u 1 2 u 1 2
s.t. (µj,σj)(x,xi) ̸= (µj,σj)(x,xi).
1 2
Example A.1 (Causal minimality). The fact that we are allowing for variables to have more
than one dimension means the causal minimality condition is not a trivial one, as the following
examples demonstrate. Let X1 = (W,0) where W is a 1-dimensional Brownian motion (so
that n = 2), and let µ2 and/or σ2 depend on X1 only through its second coordinate. Then,
1
even though this dependence may be non-trivial, causal minimality does not hold. This may
lead one to believe that one can generically only expect causal minimality to hold if m = n,
but this is not the case. Take, for example, d = 4 = n and assume σ3 ≡ 0, i.e., X3 has
23no driving Brownian motion, so that m is only 3. Assume, furthermore, that pa = {4}.
4
Then, by H¨ormander’s Lie bracket-generating condition (see for example §V.38 in Rogers
and Williams (2000)), (X1,X2,X3) has a density in R3 for any choice of an initial condition,
if (σ1(x),0,0),(0,σ2(x),0) span R2 for all x ∈ R3 and [(σ1(x),0,0),µ] = σ1∂ µ−µi∂ σ1 or
1 i
[(σ2(x),0,0),µ] span the third direction. Arrows going from the first three nodes to the fourth
will be necessary for Markovianity as long as the coefficients of X4 depend on the respective
variables on the support of such density. The point is that even though (X1,X2,X3) is only
driven by a two-dimensional Brownian motion, the coefficients of the SDE (at the initial
condition) impart a ‘twist’ to the solution, making causal minimality a trivial condition on
(σ4,µ4) which is verified as soon as there is any dependency.
A.5 Proof of Corollary 3.7
Proof. Thisisclearfromthedatageneratingmechanismineq.(1)whereallBrownianmotions
dWk and initial conditions Xk are jointly independent. Since Xj is fully determined by
t 0 [0,T]
{Xanj,dWanj}
where an are the ancestors of j in G, it follows that i ∈ an if and only if
0 t j j
Xj ⊥̸⊥ Xi. Since we already know from the CPDAG that i and j are adjacent and the
[0,T] 0
graph is acyclic, i ∈ an implies i → j.
j
A.6 Proof of Theorem 3.4
Proof of Theorem 3.4. We begin with proving correctness of the recovery of the skeleton
modulo loops. This will follow if we show that for (i → j) ̸∈ G for i,j ∈ V distinct
i ⊥⊥ j | {j }∪{k ,k | k ∈ paG \{j}},
0 G(cid:101) 1 0 0 1 j
where ⊥⊥ denotes d-separation in the graph G(cid:101). Indeed, eventually, the set K in the algorithm
G(cid:101)
will take the value {k ,k | k ∈ paG\{j}}, at which point the three edges i → j , i → j ,
0 1 j 0 0 1 1
i
0
→ j
1
(which are either all present or all absent, inductively, by construction of E(cid:101)) are
deleted. (Of course it may be that these edges are deleted before this, if a smaller or different
d-separating set is found, but note that edges are never added.) All undirected paths between
i and j factor as i ···h → l ···j , where the dots stand for a possibly empty undirected
0 1 0 0 1 1
path. All such paths with l = j are blocked by {j }∪{k | k ∈ paG \{j}}, so we focus our
1 0 0 j
attention on the case in which l ···j is non-empty. Here we follow a similar argument to
1 1
that of Verma and Pearl (1990, Lemma 1). If l ···j is of the form l1···lr → j (with l1 = l )
1 1 1 1 1 1 1
it is blocked by {k | k ∈ paG \{j}}. Assume instead it is of the form l1···lr ← j and let
1 j 1 1 1
1 ≤ q ≤ r be such that lq is the first collider on the entire path i ···j , starting from j and
1 0 1 1
travelling back: certainly this exists (and belongs to V ), thanks to the arrows h → l1 and
1 0 1
lr ← j . For the path i ···j to be open given the conditioning set, it must be the case that
1 1 0 1
l 1p must be an ancestor (in G(cid:101)) or member of {k
1
| k ∈ paG
j
\{j}} (l 1p cannot be an ancestor of
nodes in V ): this would yield a directed cycle, which is impossible.
0
We argue similarly for the loop-removal phase. If (i → i) ̸∈ G ⇐⇒ (i
0
→ i 1) ̸∈ G(cid:101), we must
show
i ⊥⊥ i | {k ,k | k ∈ paG\{i}}
0 G(cid:101) 1 0 1 i
Consider a path i 0···h
0
→ l 1···i
1
in G(cid:101). If the segment l 1···i
1
is non-empty we conclude
that the path is blocked by the same argument as above (with i replacing j ). Assume
1 1
that the path is of the form i 0···h
0
→ i
1
(with i
0
̸= h
0
since (i
0
→ i 1) ̸∈ E(cid:101)): in this case
h ∈ paG \{i} and the path is again blocked. Thus all paths are blocked and the proof is
i
complete.
24A.7 Global Markov Property for the Symmetric Independence Criterion
In this section, we provide a proof for the global Markov property of the symmetric indepen-
dence criterion in order for the PC-algorithm to be applicable. It is based on the notion of
independence models:
∼
Definition A.2. Let V = {1,...,n} be a set. An independence model J(V) over V is a
ternary relation over disjoint subsets of V,
J(V) ⊂ {(A,B,C) | A,B,C ⊂ V disjoint}
When (A,B,C) is a triple in J(V), (A,B,C) ∈ J(V), we also use ⟨A,B | C⟩ to denote
(A,B,C). This notation highlights the fact that C is a conditioning set. An independence
model J(V) is called a semigraphoid if 1.-4. hold for all disjoint A,B,C ⊂ V.
1. (symmetry) ⟨A,B | C⟩ ∈ J(V) ⇒ ⟨B,A | C⟩ ∈ J(V)
2. (decomposition) ⟨A,B | C⟩ ∈ J(V) and D ⊂ B ⇒ ⟨A,D | C⟩ ∈ J(V)
3. (weak union) ⟨A,B∪D | C⟩ ∈ J(V) ⇒ ⟨A,B | C ∪D⟩ ∈ J(V)
4. (contraction) ⟨A,B | C⟩ ∈ J(V) and ⟨A,D | B∪C⟩ ∈ J(V) ⇒ ⟨A,B∪D | C⟩ ∈ J(V)
A semigraphoid J(V) is called graphoid if 5. holds.
5. (intersection) ⟨A,B | C∪D⟩ ∈ J(V) and ⟨A,D | B∪C⟩ ∈ J(V) ⇒ ⟨A,B∪D | C⟩ ∈ J(V)
An independence model J(V) called compositional if 6. holds.
6. (composition) ⟨A,B | C⟩ ∈ J(V) and ⟨A,D | C⟩ ∈ J(V) ⇒ ⟨A,B∪D | C⟩ ∈ J(V)
The graphical criterion of d-separation defines a independence model on a DAG G = (V,E) by
J (G) = {(A,B,C) | A,B,C ⊂ V disjoint,A⊥⊥ B | C} (7)
d−sep d−sep
Theorem A.3 (Lauritzen and Sadeghi (2018)). For any DAG G = (V,E), the independence
model J (G) is a compositional graphoid.
d−sep
Let V = {1,...,n}. Given random variables Xi : (Ω,A,P) → (Xi,A ), i ∈ V, and A,B,C ⊂
i
V disjoint, conditional independence
XA⊥⊥ XB | XC :⇔ σ(XA)⊥⊥ σ(XB) | σ(XC)
P P
defines the probabilistic independence model
J(P) = {(A,B,C) | A,B,C ⊂ V disjoint,X ⊥⊥ X | X }. (8)
A P B C
We will use this as a symmetric conditional independence relation. Note that Xi may take
values in a path-space in which case it is a stochastic process. The independence model J(P)
is a semigraphoid which is an immediate consequence of sub-σ-algebra properties.
For v ∈ V, we let nd denote the set of nondescendants of v, i.e., the set of nodes i such that
v
there is no directed path from v to i.
Definition A.4 (Directed global and local Markov properties). Let G = (V,E) be a DAG
and J(V) be an independence model over V. The independence model J(V) satisfies the
global Markov property w.r.t. G :⇔
⟨A,B | C⟩ ∈ J (G) ⇒ ⟨A,B | C⟩ ∈ J(V) ∀A,B,C ⊂ V disjoint (9)
d−sep
The independence model J(V) satisfies the directed local Markov property w.r.t. G :⇔
⟨{v},(nd \pa ) | pa ⟩ ∈ J(V) for all v ∈ V (10)
v v v
25Theorem A.5 (Lauritzen et al. (1990)). Let G = (V,E) be a DAG, let and J(V) a semi-
graphoid over V. The directed global and local Markov properties are equivalent, that is,
J(V) satisfies eq. (9) w.r.t. G ⇔ J(V) satisfies eq. (10) w.r.t. G (11)
We therefore only have to establish the local Markov property for the symmetric independence
model ⊥⊥ :
sym
Proposition A.6. Let X be a set of variables induces by the model in eq. (6) with a constant
and diagonal diffusion matrix s.t. it induces the DAG G = (V,E). Then the system satisfies
the directed local Markov property w.r.t G,
Xi⊥⊥ nd \pa | pa ∀i ∈ {1,...,n} (12)
sym Xi Xi Xi
Proof. The idea of the proof is that all information about node X is contained in its parents
i
pa := pa , theBrownianmotionW , andtheinitialconditionXi. Asimilarideacanbeused
i Xi i 0
to show the result in the case of a SCM Pearl (2009). We let J˜(P) denote the semigraphoid
induced by ⊥⊥ on the variable set V˜ = {X1,...,Xn}∪{W1,...,Wn}∪{X1,...,Xn}, and
sym 0 0
we let J(P) denote the semigraphoid induced by ⊥⊥ on the variable set V = {X1,...,Xn}.
sym
We define the DAG
(cid:16) (cid:17)
G˜= V˜ = {X1,...,Xn}∪{W1,...,Wn}∪{X1,...,Xn}, E˜= E ∪{Xj ← Wj}∪{Xj ← Xj} .
0 0 0
(13)
By construction, F t(Xi) ⊂ F t(Wi)∨F t(Xpa i)∨σ(X 0i), and therefore
F t(Xi)⊥⊥F t(Xndi\pa i ∨W−i∨X 0−i) | F t(Xpa i)∨F t(Wi)∨σ(X 0i)
wheretheparentandnondescendantsetsaretobereadinthegraphG, W−i = {W ,...,W }\
1 n
{W }, and X−i = {X1,...,Xn}\{Xi}. The above statement corresponds to eq. (10) when
i 0 0 0 0
v ∈ {X ,...,X }. When v = W or v = Xi, v has no parents in G˜ and eq. (10) also holds.
1 n i 0
Therefore, the independence model J˜(P) satisfies the local directed Markov property w.r.t.
to G˜.
The independence model J˜(P) is a semigraphoid, and by Theorem A.5 it satisfies eq. (9)
w.r.t. G˜. We have Xi⊥⊥ d−sepXndi\pa i | Xpa i in G˜, and one therefore has
F t(Xi)⊥⊥F t(Xndi\pa i) | F t(Xpai )
establishing eq. (10) for J(P) w.r.t. G.
A.8 Consistency of the conditional independence test
In this section, we argue, that under certain assumptions on our system, Theorem 1 of Doran
et al. (2014) still holds even without assuming the existence of a density. At first, it is to be
noted that the signature kernel (2) is bounded on compact sets C ⊂ BV(C0(I,Rdx)) of BV
paths, anassumptionthatcanbemadeasweevaluateourSDEsoverfinite, discretetimesteps.
Satisfyingthisboundedness,itdefinesanRKHSoverthesecompactsets. Inaddition,thesigna-
ture kernel is characteristic on these bounded sets (see Proposition 3.3 Cass et al. (2023)). We
use the notation X : (Ω,A,P) → (X,A ), Y : (Ω,A,P) → (Y,A ), Z : (Ω,A,P) → (Z,A )
X Y Z
with cX,Y,Z being compact subsets in BV(C0(I,Rdx)),BV(C0(I,Rdy)),BV(C0(I,Rdz)) and
denote the RKHS’s defined by the signature kernel as (H ,k ),(H ,k ),(H ,k ).
X X Y Y Z Z
26Our measure-theoretical argumentation now follows the definitions in Park and Muandet
(2020) and the original proof in Doran et al. (2014). Let H be a Banach space, E ⊂ F a
sub-σ-algebra of the Borel-σ-algebra.
Definition A.7. Let H : (Ω,A,P) → (H,F) a Bochner P-integrable random variable. The
conditional expectation of H w.r.t E is a Bochner P-integrable RV H′ : (Ω,A,P) → (H,E)
such that
(cid:90) (cid:90)
∀A ∈ E HdP = H′dP .
A A
Due to the existence and (almost sure) uniqueness, such an H′ is denoted E[H | E] with the
notation E[H | Z] := E[H | σ(Z)] for a general Z : (Ω,A,P) → (Z,A ) which in turn can be
Z
used to define the conditional expectation P(A | E) := E[1 | E] and can be used to define the
A
conditional kernel mean embedding
µ := µ := E [k (X, ·) | Z]
X|Z P X|Z X|Z X
As proven in Theorem 5.8 of Park and Muandet (2020) under the assumption, that P :=
X|Z
E[·,| Z] admits a regular version (meaning its can be written as a transition kernel, see Park
and Muandet (2020)) and k ⊗k is characteristic, it holds that
X Y
X⊥⊥Y | Z ⇔ ∥µ −µ ⊗µ ∥ = 0 ⇔ ∥µ ⊗µ −µ ⊗µ ⊗µ ∥ = 0
XY|Z X|Z Y|Z HX⊗HY XY|Z Z X|Z Y|Z Z HX⊗HY⊗HZ
where the second equivalence is trivial. Let now σ ∈ S be a permutation such that its
n
permutation matrix M = (m )
σ ij
(cid:40)
1 if j = σ(i),
m =
ij
0 if j ̸= σ(i).
satisfies Tr(M ) = 0. The empirical estimates for the given sample {(x ,y ,z )} and its
σ i i i i∈[n]
permuted version under σ
n
1 (cid:88)
µˆ := k (x ,·)⊗k (y ,·)⊗k (z ,·), (14)
PX,Y,Z
n
X i Y i Z i
i=1
n
1 (cid:88)
µˆ := k (x ,·)⊗k (y ,·)⊗k (z ,·). (15)
P X′ ,Y,Z n X i Y σ(i) Z i
i=1
We can now write the distance between test statistic and the approximated test statistic in
termsofthesumofthedistancebetweentheempiricalestimatesanditsrespectivecounterparts
by using the inverse and the regular triangle inequality
(cid:12) (cid:12)
(cid:12)∥µ ⊗µ −µ ⊗µ ⊗µ ∥ −∥µˆ −µˆ ∥ (cid:12)
(cid:12) XY|Z Z X|Z Y|Z Z HX⊗HY⊗HZ PX,Y,Z P X′
,Y,Z
HX⊗HY⊗HZ(cid:12)
(cid:12) (cid:12)
≤ (cid:12)∥µ ⊗µ −µ ⊗µ ⊗µ −µˆ +µˆ ∥ (cid:12)
(cid:12) XY|Z Z X|Z Y|Z Z PX,Y,Z P X′
,Y,Z
HX⊗HY⊗HZ(cid:12)
≤ ∥µ ⊗µ −µˆ ∥ +∥µ ⊗µ ⊗µ −µˆ ∥ .
XY|Z Z PX,Y,Z HX⊗HY⊗HZ X|Z Y|Z Z P X′
,Y,Z
HX⊗HY⊗HZ
By Park and Muandet (2020), the first term tends to zero for n → ∞. With a similar line of
argumentation as in Doran et al. (2014) on can establish a majorant for the estimated HSCIC,
27namely
∥µˆ −µˆ ∥
PX,Y,Z P X′
,Y,Z
HX⊗HY⊗HZ
(cid:13) (cid:13)
n n
(cid:13)1 (cid:88) 1 (cid:88) (cid:13)
= (cid:13) k (x ,·)⊗k (y ,·)⊗k (z ,·)− k (x ,·)⊗k (y ,·)⊗k (z ,·)(cid:13)
(cid:13)n X i Y i Z i n X i Y σ(i) Z i (cid:13)
(cid:13) (cid:13)
i=1 i=1 HX⊗HY⊗HZ
(cid:13) (cid:13)
(cid:13) n n (cid:13)
(cid:13)1 (cid:88) 1 (cid:88) (cid:13)
= (cid:13)
(cid:13)n
k X(x i,·)⊗k Y(y i,·)⊗k Z(z i,·)−
n
k X(x σ−1(i),·)⊗k Y(y i,·)⊗k Z(z σ−1(i),·)(cid:13)
(cid:13)
(cid:13) i=1 i=σ(j),j=1 (cid:13)
HX⊗HY⊗HZ
n
1 (cid:88)(cid:13)(cid:0) (cid:1) (cid:0) (cid:1)(cid:13)
≤
n
(cid:13) k X(x i,·)−k X(x σ−1(i),·) ⊗k Y(y i,·)⊗ k Z(z i,·)−k Z(z σ−1(i),·) (cid:13)
HX⊗HY⊗HZ
i=1
n
1 (cid:88)(cid:13) (cid:13) (cid:13)(cid:0) (cid:1)(cid:13)
≤
n
(cid:13)k X(x i,·)−k X(x σ−1(i),·)(cid:13)
HX
∥k Y(y i,·)∥
HY
(cid:13) k Z(z i,·)−k Z(z σ−1(i),·) (cid:13)
HZ
i=1(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
≤2Mx ≤My
(cid:18) (cid:19)
1
≤ 2M M Tr(PDRKHS)
x y
n
where the assumptions can be made due to the boundedness of the kernels. Hence, under the
assumption that
n
1 (cid:88)
δ → P ⊗P ⊗P (16)
n (xi,y σ(i),zi) X|Z Y|Z Z
i=1
a statement that holds true in the case of densities (see supplementary material of Janzing
et al. (2013)), since P is regular, µˆ → µ ⊗µ ⊗µ .
|Z P′ X|Z Y|Z Z
X,Y,Z
Hence under the assumptions that P admits a regular version (an assumption also made by
·|Z
Laumann et al. (2023)), the considered random variables operate on compact sets of BV-paths
and (16) holds, the test is consistent if and only if 1 Tr(PDRKHS) → 0.
n
B Experiments and Evaluation
This section delves into the experimental framework, elaborating on the data generation
process, the parameters and architectures employed, and presents additional results.
B.1 Kernel Heuristic and Interval Choice
In kernel methods, besides selecting the right kernel function, the correct choice of its
parameters is vital. A reasonable choice for radially symmetric kernels k(x ,x ) = f(∥x −
i j i
x ∥/σ), f : R → R is to choose σ to lie in the same order of magnitude as the pairwise
j + +
distances (∥x −x ∥) . Choosing the bandwidth for the RBF kernel in our setting with
i j i,j
samples (x ) , x ∈ C0(I,Rd) is not straight forward, as the RBF kernel ultimately acts on
i i∈[n] i
signatures and there is no immediate way in obtaining a ‘typical scale’ of pairwise distances
of signatures. To still set the bandwidth purely based on observed data, there are two natural
ways to implement a median heuristic based on two different collections of pairwise distances.
The first option is to choose
(cid:8) (cid:12) (cid:9)
σ
1
= Median ∥x i−x j∥ L2(cid:12)i,j ∈ [n] ,
where ∥·∥ is the is the L2 norm of entire paths, i.e., integrated over the time interval I.
L2
Given that in practice we observe the paths x still at fixed time points t < ... < t within
i 1 m
the interval I, we can also consider the heuristic
(cid:8) (cid:12) (cid:9)
σ
2
= Median ∥x
i,t
−x
j,t
∥ 2(cid:12)i,j ∈ [n],l,k ∈ [m] ,
l k
28Table 4: Performance of future-extended h-locally conditionally independence criterion in
the bivariate setting for the two heuristic σ and σ for different sample sizes. The SHD is
1 2
measured for different values of s in ⊥⊥+ with t = 1, defining different lengths of intervals in
s,t
the past and future. Overall the performance is best of shorter values of the past intervals
compared to the future (s < t).
2
σ σ
1 2
(×102) n = 400 n = 600 n = 400 n = 600
s = 0.1 76.2±2.5 58.4±2.7 14.9±2.0 15.8±2.0
s = 0.2 77.3±2.7 72.2±2.8 12.9±1.7 10.9±1.7
s = 0.3 80.2±2.3 76.2±2.6 21.8±2.2 17.8±2.0
s = 0.4 89.1±2.0 92.0±2.1 31.7±2.4 25.7±2.2
s = 0.5 89.1±2.0 98.1±2.0 38.6±2.5 27.7±2.2
s = 0.6 82.4±1.9 96.0±2.1 60.0±2.7 48.5±2.9
s = 0.7 102.0±1.2 104.0±2.0 80.2±2.2 61.4±2.7
s = 0.8 104.0±1.4 95.0±1.5 81.1±2.0 87.1±2.4
s = 0.9 95.0±1.6 104.4±1.8 102.0±1.2 101.0±2.1
1.0
0.5
0.8
0.4
0.6
0.3
0.4 0.2
0.2 0.1
0.0
0.0
HSICb HSIC KCIPT KCITb KCIT SDCIT HSICb HSIC KCIPT KCITb KCIT SDCIT
test type test type
Figure 5: Performance in terms of type I and type II errors of different unconditional (HSIC
b
and HSIC ) and conditional tests (KCIPT, KCIT , KCIT , SDCIT). Among the conditional
γ b γ
tests, KCIPT balances type I and type II errors well compared to SDCIT. Among the
unconditional tests, HSIC outperforms HSIC
b γ
where ∥·∥ is the Euclidean norm in Rd and we consider pairwise distances of all observations
2
across all paths and time points. In early benchmarks, we have empirically found σ to
2
yield a better heuristic by comparing the performance of both heuristics by evaluation of the
SHD in a causal discovery task in the linear bivariate setting X1 → X2 without diffusion
interaction with a ∼ U((−2.5,−1]∪[1,2.5)), a ,a ∼ U((−0.5,0.5)) and σ ∼ U([0,0.5))
21 11 22 i
(see 4). The experiment is performed for different choices of intervals [0,s],[s,s+h] in the
future-extendedh-localconditionalindependence. Ascanbeseenfrom4,σ and[0,0.1],[0.1,1]
2
seem to perform best on average and are therefore used in the rest of the experiments.
B.2 Empirical Comparison of Different Tests
Different kernel-based conditional and unconditional tests are compared. HSIC with boot-
strap (HSIC ) and with γ-distribution approximation (HSIC ) (Gretton et al., 2007) were
b γ
implemented for the unconditional setting, while KCIPT (Doran et al., 2014), KCIT with
bootstrap (KCIT ), KCIT with γ approximation (KCIT ) (Zhang et al., 2011) and SDCIT
b γ
for the conditional setting (Lee and Honavar, 2017). Figure 5 shows that KCIPT and HSIC
b
are overall the best performing ones in terms of type I and type II error.
29
rorre
I
epyT
rorre
II
epyT1.0
0.8
0.6
0.4
0.2 SigKern=60σ=1.0
Laun=60σ=1.0
0.0
0.0 0.2 0.4 0.6 0.8
missingness
Figure 6: Test power over the missingness level (percentage of observations that is being
dropped at random). While both our method as well as Laumann et al. (2023) maintain their
power up to high levels of missingness, our test still consistently outperforms the baseline.
Lines show means over 1000 settings in Section 4.1 and shaded regions show standard errors.
B.3 Simulated Datasets
The data-generating process corresponding to Table 4 consists in linear Stochastic Differential
Equations (SDEs) as defined in Equation 5. The drift matrix elements are created using a
uniform distribution, with a and a following U[−0.5,0.5]. The off-diagonal elements, a
11 22 21
and a , are sampled from U[(−2.5,−1.0)∪(1.0,2.5)]. Additionally, we set both drift and
12
diffusion biases by sampling b and d from U[−0.1,0.1] and U[−0.2,0.2], respectively. It’s also
i i
important to note that the diffusion matrix in these simulations contains only zero elements.
B.4 Additional Experimental Results for the Bivariate Case
Figure 6 shows that in the simple unconditional bivariate setting of Section 4.1 our test
maintains high power up to substantial levels of missingness and only starts degrading once
more than 80% of observations are dropped at random starting from 64 time steps over the
interval [0,1] for 1000 different SDEs.
B.5 Building blocks for causal discovery
Similarly to the results in Table 4, we perform the same experiments for the setting in
which the causal dependency only comes from the diffusion. Specifically, referring to the
linear SDE in eq. (5), a = a = 0, a ,a ∼ U[−0.5,0.5], σ ,σ ∼ U[(−2.5,−1.0) ∪
ij ji 11 22 12 21
(1.0,2.5)] and σ ,σ ∼ U[−0.5,0.5]. Moreover, b and d are sampled from U[−0.1,0.1] and
11 22 i i
U[−0.2,0.2]. Table 5 includes the results for all the different causal structures. We also assess
the performance of the future-extended h-locally conditionally independence criterion (⊥⊥+ )
s,t
in the different three-dimensional causal structures: chain, collider and fork. The results are
presented respectively in Table 6. The tables show that the test is able to reconstruct the
presence and the directionality of the edges overall all building blocks of causal structure
learning.
B.6 Real-World Pairs Trading Example
In the pairs trading experiment, stock price data is downloaded from Yahoo Finance for a
predefined list of stocks over a specific period, divided into training (1st January 2010 to 31st
December 2011) and trading intervals (1st January 2012 to 31st December 2012). The chose
stocks are Trinity Industries (TRN), Brandywine Realty Trust (BDN), Commercial Metals
Company (CMC), The New York Times Company (NYT), New York Community Bancorp
(NYCB), The Wendy’s Company (WEN), CNX Resources Corporation (CNX). Logarithms
30
rewoptsetTable 5: Test performance on the key building blocks for causal discovery. Here, ⊥⊥ refers to
⊥⊥ except for X1⊥⊥X1 which means X1 ⊥⊥X1 in a setting where the dependency
sym p f [0,t/2] [t/2,t]
only comes from the diffusion. We use KCIPT for CI tests and bootstrapped HSIC uncondi-
tional independence. We simulate 40 samples of 512 trajectories for 10 different SDEs (400
tests per column) with 25% of the 128 original observations dropped uniformly at random.
The error represent type I or type II error when the null hypothesis H should be rejected
0
(✗) or accepted (✓), respectively.
graph X1 X2 X3 X1 X2 X3 X1 X2 X3 X1 X1 X2 X1 X2
H0 X1⊥⊥X2|X3 X1⊥⊥X3|X2 X1⊥⊥X2|X3 X1⊥⊥X3|X2 X1⊥⊥X3|X2 Xp1⊥⊥Xf1 X1⊥⊥X2 X1⊥⊥X2
shouldreject ✓ ✗ ✓ ✓ ✗ ✓ ✗ ✓
error 0.33 0.05 0.11 0.35 0.11 0.00 0.07 0.01
Table 6: Test performance on the chain, fork and collider causal structures for the future-
extended h-locally conditionally independence criterion. Here, ⊥⊥ refers to ⊥⊥+ and we use
s,h
KCIPT. We simulate 10 samples of 512 trajectories for 10 different SDEs (100 tests per
column). The error represent type I or type II error when the null hypothesis H should be
0
rejected (✗) or accepted (✓), respectively.
X1 X2 X3
graph
H 0 X1⊥⊥X2|X3 X2⊥⊥X1|X3 X1⊥⊥X3|X2 X3⊥⊥X1|X2 X2⊥⊥X3|X1 X3⊥⊥X2|X1
should reject ✗ ✓ ✗ ✗ ✗ ✓
error 0.07 0.33 0.04 0.04 0.03 0.00
X1 X2 X3
graph
H 0 X1⊥⊥X2|X3 X2⊥⊥X1|X3 X1⊥⊥X3|X2 X3⊥⊥X1|X2 X2⊥⊥X3|X1 X3⊥⊥X2|X1
should reject ✓ ✗ ✗ ✗ ✗ ✓
error 0.21 0.08 0.06 0.16 0.08 0.30
X1 X2 X3
graph
H 0 X1⊥⊥X2|X3 X2⊥⊥X1|X3 X1⊥⊥X3|X2 X3⊥⊥X1|X2 X2⊥⊥X3|X1 X3⊥⊥X2|X1
should reject ✗ ✓ ✓ ✓ ✓ ✗
error 0.10 0.00 0.36 0.46 0.00 0.08
31of the stock prices are computed to stabilize variance and normalize the prices. During the
training period, pairs of stocks are selected based on various statistical tests: Cointegration
Test (Engle-Granger) to determine if a pair of stocks is cointegrated, suggesting a long-term
equilibrium relationship; Augmented Dickey-Fuller (ADF) Test to assess the stationarity of
the spread (ratio) of a pair’s prices; and Granger Causality Test to check if the price of one
stock in the pair can predict the price of the other. Pairs are selected based on the p-values
from these tests, with a threshold of 0.05 determining significance. In the trading phase, a
rolling window is used to continuously recalculate the mean and standard deviation of the
spread between the selected pairs. Trades are initiated based on the z-score of the spread,
where a high z-score indicates a short position and a low z-score indicates a long position.
Positions are managed and closed when the spread returns to a predefined z-score threshold.
The strategy’s performance is evaluated based on total return, annual percentage rate (APR),
Sharpe ratio, maximum drawdown (maxDD), and maximum drawdown duration (maxDDD).
B.7 Evaluation Metrics
Following common conventions in the causal discovery literature, we evaluate our algorithms
using the following metrics.
Structural Hamming Distance (SHD) Given two directed graphs G = (V,E ), G =
1 1 2
(V,E ) over the same nodes with different sets of edges and let A ,A ∈ {0,1}n×n be their
2 1 2
adjacency matrices. Then we define the structural hamming distance (SHD) as
(cid:88)
SHD = |(A ) −(A ) |
1 ij 2 ij
i,j∈[n]
Normalized SHD In order to compare the recovery performance of our algorithms for
different types of graph-sizes, the normalized SHD is defined as
SHD
SHD =
norm
d(d−1)
C Baselines
In the experiment section Section 4, we benchmark our method against other baselines,
including Granger causality, CCM, PCMCI, and Laumann (Laumann et al., 2023). For the
latter, we used their implementation provided in the causal-fda package. For the Granger-
implementation for two variables (d = 2), we used Seabold and Perktold (2010), for CCM we
used Javier (2021), and for PCMCI we used the tigramite package (Runge et al., 2019). In
PCMCI, tests for edges are conducted by applying distance correlation-based independence
tests (Sz´ekely et al., 2007) between the variables’ residuals after regressing out other nodes
using Gaussian processes.
D Summary of notations
A summary of notation is provided in Table 7.
E Extension to the Cyclic Setting
Given that we are considering the dynamic setting, it is generally natural to allow for G to
have cycles. This comes at no additional requirement of consistency constraints as it does in
the static case (see for example Bongers et al. (2021)), since the causal arrows in the model
eq. (1) should be thought of as ‘pointing towards the infinitesimal future, with infinitesimal
32Table 7: Summary of notations
Symbol Meaning
d total number of components in stochastic process X = (X1,...,Xd)/ nodes in graph
n dimension of the k-th component Xk
k
m dimension of the k-th Brownian motion Wk
k
n total dimension of the stochastic process X with n = n +...+n
1 d
T ∈ R maximum observation time
>0
t,s ∈ [0,T] some arbitrary time points
G = (V,E) directed graph G with nodes V and edges E
[m] short for {1,...,m}
magnitude’, integrated over the whole time interval considered. Indeed, the system of SDEs
does not require any global consistency to be well-posed, other than the regularity and growth
conditions that guarantee global-in-time existence and uniqueness.
33