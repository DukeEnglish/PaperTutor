Independent Learning in Constrained Markov Potential Games
Philip Jordan Anas Barakat Niao He
ETH Zu¨rich ETH Zu¨rich ETH Zu¨rich
Abstract agent aims to maximize their own individual reward
which may however depend on all players’1 decisions.
Besides reward maximization, agents may also con-
Constrained Markov games offer a formal
tendwithsatisfyingconstraintsthatareoftendictated
mathematicalframeworkformodelingmulti-
by multi-agent RL applications. Prominent such real-
agentreinforcementlearningproblemswhere
world applications include multi-robot control on co-
the behavior of the agents is subject to
operativetasks(Guetal.,2023)aswellasautonomous
constraints. In this work, we focus on
driving (Shalev-Shwartz et al., 2016; Liu et al., 2023)
the recently introduced class of constrained
where physical system constraints and safety consid-
Markov Potential Games. While centralized
erationssuchascollisionavoidanceareofprimaryim-
algorithms have been proposed for solving portance. In other applications, agents may be sub-
such constrained games, the design of con-
ject to soft constraints such as average users’ total la-
verging independent learning algorithms tai-
tencythresholdsinwirelessnetworksoraveragepower
lored for the constrained setting remains an
constraints in signal transmission. Each agent seeks
open question. We propose an independent
to maximize their reward while also accounting for
policy gradient algorithm for learning ap-
constraints which are coupled among agents. Con-
proximate constrained Nash equilibria: Each strained Markov games (Altman and Shwartz, 2000)
agentobservestheirownactionsandrewards,
offer a mathematical framework to model multi-agent
alongwithasharedstate. Inspiredbytheop-
RL problems incorporating coupled constraints.
timizationliterature, ouralgorithmperforms
proximal-point-like updates augmented with In this work, we focus on a particular class of struc-
a regularized constraint set. Each proxi- turedconstrainedMarkovgames: constrainedMarkov
mal step is solved inexactly using a stochas- Potential Games (CMPGs). Recently introduced
tic switching gradient algorithm. Notably, in Alatur et al. (2023) to incorporate constraints,
our algorithm can be implemented indepen- CMPGs naturally extend the class of Markov Poten-
dently without a centralized coordination tialGames(MPGs)thathasbeenactivelyinvestigated
mechanism requiring turn-based agent up- in the last few years (Macua et al., 2018; Leonardos
dates. Undersometechnicalconstraintqual- etal.,2022;Foxetal.,2022;Zhangetal.,2022b;Song
ificationconditions,weestablishconvergence et al., 2022; Ding et al., 2022; Zhang et al., 2022a;
guarantees towards constrained approximate Maheshwari et al., 2023; Zhou et al., 2023). Inter-
Nash equilibria. We perform simulations to estingly, this class of games is a class of mixed co-
illustrate our results. operative/competitive Markov games including pure
identical interest Markov games (in which all the re-
ward and cost functions of the agents are identical) as
1 INTRODUCTION a particular case. The ability to cooperate between
learning agents is crucial to improve their joint wel-
fareandachievesocialwelfareforartificialintelligence
In multi-agent reinforcement learning (RL), several
(see Dafoe et al. (2020, 2021) for an extensive discus-
agents interact within a shared dynamic and uncer-
sion about the need for promoting cooperative AI).
tainenvironmentevolvingovertimedependingonthe
individual strategic decisions of all the agents. Each Independentlearninghasrecentlyattractedincreasing
attention thanks to its versatility as a learning proto-
Proceedings of the 27thInternational Conference on Artifi- col. We refer the reader to a recent nice survey on
cial Intelligence and Statistics (AISTATS) 2024, Valencia, the topic (Ozdaglar et al., 2021). In this protocol,
Spain. PMLR: Volume 238. Copyright 2024 by the au-
thor(s).
1We will use player and agent interchangeably.
4202
beF
72
]GL.sc[
1v58871.2042:viXraIndependent Learning in Constrained Markov Potential Games
agents can only observe the realized state and their In this paper, we answer this question in the affirma-
own reward and action in each stage to individually tive. Our contributions are as follows:
optimize their return. In particular, each agent does
not observe actions or policies from any other agent. • We design an algorithm for independent learn-
This protocol offers several advantages including the ing of constrained ϵ-approximate Nash equilib-
following aspects: (a) Scaling: independent learning ria (NE) in CMPGs. Inspired by recent works
dynamics do not scale exponentially with the num- in nonconvex optimization under nonconvex con-
ber of players in the game (also known as the curse straints, our algorithm implements an inexact
of multi-agents); (b) Privacy protection: agents may proximal-point update augmented with a regu-
avoid sharing their local data and information to pro- larized constraint set. In particular, the inexact
tect their privacy and autonomy; (c) Communication proximal step is computed using a stochastic gra-
cost: a central node that can bidirectionally commu- dientswitchingalgorithmforsolvingtheresulting
nicate with all agents may not exist or may be too subproblemwhereboththeobjectiveandthecon-
expensive to afford. Therefore, this protocol is partic- straint functions are strongly convex. Notably,
ularly appealing in several applications where agents the algorithm can be run independently by the
need to make decisions independently, in a decentral- different agents without taking turns.
ized manner. For example, dynamic load balancing,
• We analyze the proposed algorithm and establish
which consists in evenly assigning clients to servers
its sample complexity to converge to an ϵ- ap-
in distributed computing, demands for learning algo-
proximate NE of the CMPG with polynomial de-
rithms that minimize communication overhead to en-
pendence on problem parameters. Our analysis
able low-latency response times and scalability across
requires new technical developments that do not
large data centers. This task has been modeled as
rely on results from the CMDP literature.
an MPG (Yao and Ding, 2022). In other applications
suchasthepollutiontaxmodelanddistributedenergy • Weillustratetheperformanceofouralgorithmon
marketplace detailed in section 5, coordination is in-
two simple CMPG applications: a pollution tax
herentlyruledoutduetothecompetitivenatureofthe
model and a marketplace for distributed energy
players’interactions. Independentlearningalgorithms
resources.
have been proposed for unconstrained multi-agent RL
problems such as zero-sum Markov games (Daskalakis
Table 1: Position of our work in the literature. ‘cen-
et al., 2020; Sayin et al., 2021; Chen et al., 2023) as
tralized’ means that the algorithm requires coordina-
well as for unconstrained MPGs in a recent line of
tion between players who take turns in updating their
works (Leonardos et al., 2022; Zhang et al., 2022a,b;
policy; for ‘independent’ learning, see section 2.
Ding et al., 2022; Maheshwari et al., 2023).
However, for constrained MPGs, existing algorithms
centralized independent
with convergence guarantees require coordination be-
tween players. Indeed, inspired by Song et al. (2022), Independent PGA
Alaturetal.(2023)recentlyproposedacoordinateas- Nash-CA Leonardos et al. (2022)
MPG
cent algorithm for CMPGs in which each agent up- Song et al. (2022) Zhang et al. (2022b)
dates their policy in turn. At each time step, the Ding et al. (2022)
policies of other agents are fixed while the updating CA-CMPG Algorithm 1
CMPG
agent faces a constrained Markov Decision Process Alatur et al. (2023) This work
(CMDP) to solve. When this coordination is not pos-
sibleasintheindependentlearningprotocol,theprob-
lem becomes more challenging as the environment is
Related Works We refer the reader to Table 1 for
no longer stationary from the viewpoint of each agent
a schematic positioning of our work in the recent lit-
andtheproblemdoesnotreducetosolvingaCMDPat
erature. We next discuss some closely related work.
eachtimestep. Thismotivatesthefollowingquestion:
Markov Potential Games MPGs have been in-
Can we design an independent learning al- troduced as a natural extension of normal form
gorithm for constrained MPGs with non- potential games (Monderer and Shapley, 1996) to
asymptotic global convergence guarantees? the dynamic setting starting with state-based po-
tential games (Marden, 2012) and later Markov
games(Macuaetal.,2018). Leonardosetal.(2022)in-
troducedavariantofMPGsandproposedindependent
stochastic policy gradient methods with an O(ϵ−6)Philip Jordan, Anas Barakat, Niao He
sample complexity to reach an ϵ-approximate NE. Inexact Proximal-Point The idea of using inex-
Similar results were shown in Zhang et al. (2022b) act proximal-point methods to solve nonconvex prob-
with model-based algorithms. This result was later lems has been fruitfully exploited in the literature
improved to an O(ϵ−5) sample complexity for large for a couple of decades (see e.g., Hare and Sagas-
state-action spaces with linear function approxima- tiz´abal (2009); Davis and Grimmer (2019)). A re-
tion (Ding et al., 2022) and further to an O(ϵ−4.5) by cent line of works (Boob et al. (2023); Ma et al.
reducingthevarianceoftheagent-wisestochasticpol- (2020); and also Jia and Grimmer (2023)) extended
icy gradients (Mao et al., 2022). Zhang et al. (2022a) this idea in order to solve nonconvex optimization
explored the use of the softmax policy parametriza- problems with nonconvex functional constraints. The
tion instead of the direct parametrization. In partic- initial nonconvex problem is transformed into a se-
ular, they established an O(ϵ−2) iteration complexity quence of convex problems by adding quadratic regu-
inthedeterministicsettingandshowedthebenefitsof larization terms to both the objective and constraints.
using regularization to improve the convergence rate. These works also established convergence rates to
Maheshwarietal.(2023)proposedafullyindependent Karush–Kuhn–Tucker (KKT) points under constraint
and decentralized two timescale algorithm for MPGs qualification conditions. Our present work is inspired
with asymptotic guarantees where players may not by this recent line of research. We point out though
even know the existence of other players. Narasimha that we deal with a multi-agent RL problem and we
et al. (2022) provided verifiable structural assump- provide convergence guarantees to approximate con-
tionsunderwhichaMarkovgameisanMPGandfur- strained NE. In these regards, our independent algo-
ther provided several algorithms for solving MPGs in rithmdesignandouranalysisrequireseveralnewtech-
the deterministic setting. Song et al. (2022) proposed nical developments.
an O(ϵ−3) sample complexity coordinate ascent algo-
rithm(Nash-CA)whichrequirescoordinationbetween
2 PRELIMINARIES
players. Guoetal.(2023)recentlyintroducedtheclass
of α-MPGs which relaxes the definition of MPGs by
We consider an m-player constrained Markov Game
allowing α-deviations with respect to (w.r.t.) the po-
where the players repeatedly select actions for maxi-
tential function. More recently, Zhou et al. (2023) in-
mizing their individual value functions while satisfy-
troduced a class of networked MPGs for which they
ing some constraints defined as cost value function
proposed a localized actor-critic algorithm with linear
bounds. More formally, the tabular game with ran-
functionapproximation. Alltheaforementionedworks
dom stopping, which we focus on, is described by a
focused on the unconstrained setting.
tuple G =(S,N,{A ,r ,c } ,α,µ,P,κ) with:
i i i i∈N
• AfinitesetofstatesS ofcardinalityS :=|S|and
Constrained Markov Games and CMPGs a finite set of m agents N :={1,...,m}.
Therehasbeenavastarrayofworksinmulti-agentRL
• A finite set of actions A of cardinality A :=|A |
with safety constraints in practice (see e.g., ElSayed- i i i
for all i∈N with A :=max A . The joint
Aly et al. (2021); Gu et al. (2023) and the references max i∈N i
(cid:81)
action space is denoted by A:= A .
therein). Altman and Shwartz (2000) defined con- i∈N i
strained Markov games and provided sufficient con-
• A reward function r : S ×A → [0,1] and a cost
ditionsfortheexistenceofstationaryconstrainedNE. i
function c :S×A→[0,1] for each agent i∈N.
Nonasymptotic theoretical convergence guarantees to i
Throughout this paper, we will suppose that all
gametheoreticsolutionconceptsforconstrainedmulti-
the cost functions are identical across the agents
agent RL are relatively scarce in the literature. Chen
and equal to a single cost function c.2
et al. (2022) introduced a notion of correlated equi-
libria for general constrained Markov games and pro- • Adistributionµoverstatesfromwhichtheinitial
vided a primal-dual algorithm for learning those equi- state of the game is drawn.
libria. Dingetal.(2023)establishedregretguarantees
for episodic two-player zero-sum constrained Markov • A probability transition kernel P: For any
games. Alatur et al. (2023) introduced the class of state s ∈ S and any joint action a ∈ A, the
constrainedMPGs. InspiredbyNash-CA(Songetal., game transitions from state s to a state s′ ∈ S
2022), they proposed a constrained variant of the
algorithm which enjoys an O(ϵ−5) sample complex- 2The case of multiple such common costs can be ad-
dressed with our approach with minor modifications. The
ity. Crucially, this algorithm requires coordination
case where cost functions may differ between players is
between agents and cannot be implemented indepen- more challenging and left for future work. See Remark 1
dently by the agents. for details.Independent Learning in Constrained Markov Potential Games
with probability P(s′|s,a) and the game termi- The joint policies of the agents are constrained to the
nates with probability κ > 0. We further de- setΠ :={π ∈Π|V (π)≤α}offeasiblepolicies. The
s,a c c
fine κ:=min κ and γ :=1−κ. setoffeasiblepoliciesforagenti∈N whenthepolicy
s∈S,a∈A s,a
of the other agents is fixed to π ∈ Π−i is denoted
−i
Ateachtimestept≥0ofagivenepisodeofthegame, by Πi c(π −i):=(cid:8) π i ∈Πi |(π i,π −i)∈Π c(cid:9) .
alltheagentsobserveasharedstates ∈S andchoose
t
a joint action a ∈ A. Then, each agent i ∈ N re- Nash Equilibria For any ϵ≥0, a joint policy π∗ ∈
t
ceives a reward r (s ,a ) and incurs a cost c(s ,a ). Π is called an ϵ-approximate constrained NE if for
i t t t t c
Thegameeitherstopsattimetwithprobabilityκ every i ∈ N and any policy π′ ∈ Πi(π∗ ), we have
st,at i c −i
or proceeds by transitioning to a state s drawn V (π∗)−V (π′,π∗ ) ≤ ϵ. When ϵ = 0, such a pol-
t+1 ri ri i −i
from the distribution P(·|s ,a ). We denote by T icy π∗ is called a constrained NE policy and no agent
t t e
the random stopping time when the episode termi- hasanincentivetodeviateunilaterallyfromaNEpol-
nates.3 Forasimilarsettingintheunconstrainedcase, icy π∗. Observe that unilateral deviations are only
see Daskalakis et al. (2020); Giannou et al. (2022). allowed within the set of feasible policies in our con-
strained setting. We refer the reader to Altman and
Policies and Value Functions Each agent i ∈ Shwartz (2000) for the existence of stationary con-
N chooses their actions according to a randomized strained NEs.
stationary policy denoted by π ∈ Πi := ∆(A )S
i i
where ∆(A i) is the probability simplex over the fi- Independent Learning Protocol All the players
nite action space A i. The set of joint policies π = interactviaexecutingtheirpoliciesforafixednumber
(π i) i∈N is denoted by Π := × i∈NΠi and we fur- ofepisodesinordertofindanapproximateconstrained
ther use the notation π −i = (π j) j∈N\{i} ∈ Π−i := NE. Importantly, during the learning procedure, each
× Πj forjointpoliciesofallagentsotherthani. playerexecutestheirpolicyateachepisodeofthegame
j∈N\{i}
For any u ∈ {r |i∈N} ∪ {c} and any joint pol- to sample a trajectory and exclusively observes their
i
icyπ ∈Π,wedefinethevaluefunctionV (π)forevery own trajectory (s ,a ,r (s ,a ),c(s ,a )) . In
u t i,t i t t t t 0≤t≤Te
states∈S byV (π):=E[(cid:80)Te u(s ,a )|s =s].The particular,aplayerdoesnothaveaccesstothepolicies
u,s t=0 t t 0
shorthand notation V (π) will stand for V (π) := of other players or their chosen actions. Such a proto-
u u,µ
E [V (π)]. For any policy π ∈ Π and s,s′ ∈ S, colwasconsideredforinstanceintwo-playerzero-sum
s∼µ u,s
thestatevisitationdistributionisdefinedbydπ(s′):= Markov games in Daskalakis et al. (2020); Chen et al.
s
E[(cid:80)Te 1 |s =s] where 1 is the indicator func- (2023) as well as for unconstrained MPGs (Leonardos
tiont a= n0
d
w{s et= ws r′} ite0
dπ(s′)=E [dπ(s′)]. etal.,2022;Dingetal.,2022;Maheshwarietal.,2023).
µ s∼µ s
Intherestofthispaper,wewillaimtominimizeboth
rewards and costs to align with conventions from the 3 INDEPENDENT ALGORITHM
constrained optimization literature. The equivalence FOR CONSTRAINED MPGs
to the common RL reward maximization formulation
followsfromconsideringrewardfunctions1−r instead
i Inthissection,wepresentourindependentiProxCMPG
of r for each i∈N .
i algorithmforlearningconstrainedNEinCMPGs. Be-
fore describing our approach, we discuss an alterna-
Constrained MPGs In this paper, we consider an tive, natural but unsuccessful, approach to motivate
m-player constrained MPG (CMPG) (Alatur et al., our algorithm design. This will allow us to highlight
2023) which is a constrained version of a Markov Po- the challenges arising from the combination of (a) the
tential Game (Macua et al., 2018; Leonardos et al., presence of coupled constraints, (b) the multi-player
2022). In an MPG, for each state s ∈ S, there ex- setting, and (c) the independent learning protocol.
ists a so-called potential function Φ : Π → R such
s
Our starting point is the known result that any max-
that for all i ∈ N, it holds that V (π ,π ) −
V (π′,π )=Φ (π ,π )−Φ (π′,π
r )i, fs
or
i any− pi
oli- imizer of the potential function is a NE of the game.
cir ei s,s (πi ,π−i
) ∈
Πs
,
ani
d
π− ′i
∈
Πis
.
Wi
e
w−i
ill also use the
ThisresultwasinitiallyprovedbyMondererandShap-
i −i i
notation Φ(π) := E [Φ (π)]. Notice that the fully ley (1996) for normal form potential games and later
s∼µ s
generalized to MPGs by Leonardos et al. (2022) and
cooperative setting when all the reward functions of
to constrained MPGs more recently (Alatur et al.,
the players are identical is a particular instance of an
2023). Therefore, in order to find an (approximate)
MPG. Note also that the potential function is typi-
constrainedNEforourCMPG4,wewillconsidersolv-
cally unknown for the players interacting in the game.
3Thediscountedinfinitehorizonsettingcanalsobead- 4Approximate KKT points of this problem will be re-
dressed with minor adaptations. lated to approximate constrained NE of our CMPG.Philip Jordan, Anas Barakat, Niao He
ing the following constrained optimization problem: We now describe our approach which takes a dif-
ferent route. Our algorithm is inspired by recent
min Φ(π), (1) workinnonconvexoptimizationundernonconvexcon-
π∈Πc
straints (Boob et al., 2023; Ma et al., 2020; Jia and
whereΦisthepotentialfunctionforourCMPGusing Grimmer,2023). Followingtheirideas,weconsiderthe
the notations introduced in section 2. This problem following proximal update with penalized constraints:
involves a nonconvex objective with a nonconvex con-
(cid:110) 1 (cid:13) (cid:13)2(cid:12)
straintsincethevaluefunctionisanonconvexfunction π(t+1) =argmin Φ(π)+ (cid:13)π−π(t)(cid:13) (cid:12)
of the policy in general (see e.g., Lemma 1 in Agar- π∈Π 2η (cid:13) (cid:13) (cid:12)
wal et al. (2021)). However, although nonconvex op- 1 (cid:13) (cid:13)2 (cid:111)
V (π)+ (cid:13)π−π(t)(cid:13) +β ≤α
timization problems with nonconvex constraints are c 2η (cid:13) (cid:13)
notoriously hard, it turns out that problem (1) is still (2)
tractable in the single agent setting. In this case, the where π(0) is a given initial joint policy, η > 0 is
problem boils down to a CMDP problem. Despite its a step size and β > 0 an additional slack. Ob-
nonconvexity, the problem can be recast as a linear serve that V (π(t+1))+β−α≤−∥π(t+1)−π(t)∥2/2η.
c
program in the space of occupancy measures which is Hence, the policy π(t) is feasible with slack β, i.e.,
a convex set (see Chapter 3 in Altman (1999)). Then, V (π(t))+β ≤ α, for every t ≥ 0. We introduce two
c
strong duality permits to design primal-dual policy additional notations for convenience. Define for any
gradient algorithms to solve the problem with conver- joint policies π,π′ ∈Π, and η >0,
gence guarantees (see e.g., Paternain et al. (2019)).
1
Φ (π):=Φ(π)+
∥π−π′∥2
,
GiventhosepositiveresultsforsingleagentCMDPs,a η,π′ 2η
natural approach is to derive a primal-dual algorithm 1
for our multi-agent problem (1) as it was proposed V ηc ,π′(π):=V c(π)+
2η
∥π−π′∥2 ,
by Diddigi et al. (2020). In the latter work, a primal- Πc :=(cid:8) π ∈Π|Vc (π)+β ≤α(cid:9) .
dualpolicygradientalgorithmwasproposedusingthe η,π′ η,π′
Lagrangian function L(π,λ) := Φ(π)+λ(V (π)−α)
c Our update rule in (2) can then be rewritten as:
where λ ≥ 0 is a Lagrange multiplier. This algorithm
can then be run independently by the different agents π(t+1) = argmin Φ (π). (3)
η,π(t)
usingexistingindependentlearningalgorithmsforthe π∈Πc
η,π(t)
unconstrained setting (Leonardos et al., 2022; Zhang
et al., 2022b; Ding et al., 2022). Unfortunately, it has We immediately observe that the above update rule is
beenrecentlyshownbyAlaturetal.(2023)thatstrong well-defined since Φ η,π(t) and V ηc ,π(t) are strongly con-
duality does not hold in general for the CMPG prob- vexforeveryt≥0forasuitablestepsizeη. Thisisin
lem. As a consequence, it is not clear how to obtain contrastwiththeoriginalproblemwhereboththepo-
guarantees for convergence to constrained NE using tential function Φ and the constraint function V c are
this duality approach. This is due to the multi-agent smooth but nonconvex. We also remark that if π(t)
(cid:13) (cid:13)
nature of our problem. In particular, since the con- converges,thentheregularizationterm(cid:13)π(t+1)−π(t)(cid:13)
straint couples the agents’ individual policies, the set becomessmallandthesurrogatefeasibleregionΠc
η,π(t)
of state-action occupancy measures induced by joint approachestheoriginalconstraintsetΠ uptothead-
c
policies of the players cannot be obviously split into ditional slack β.
severalconvexproblemsinvolvingtheoccupancymea-
Now, we discuss how to solve the proximal problem
suresinducedbyeachoneoftheplayers’policies. The
in (3) defining our main update rule. To solve this
well-knownchallengeofnonstationarityoftheenviron-
strongly convex problem with strongly convex con-
ment in multi-agent RL makes the design of indepen-
straint, we adapt a gradient switching algorithm pro-
dentlearningalgorithmsdifficult. Asaremedy,Alatur
posedinLanandZhou(2020). Ateachiterationk,our
etal.(2023) resorttocoordinationamongplayersand
algorithm performs a projected gradient descent step
proposeacoordinateascentalgorithmforCMDPs. At
alongeitherthegradientofthe(regularized)objective
each time step and for every player i, by fixing the
orthegradientoftheconstraintfunctiondependingon
policy of other players but player i to π , player i
−i whetheranestimateoftheconstraintfunctionsatisfies
canlearna“best-response”policybysolvingaCMDP
the relaxed constraint V (π(t,k))+β −α ≤ δ where
since the environment now becomes stationary from c k
(δ ) is a decreasing sequence converging to zero and
agent i’s viewpoint. k
henceprogressivelyenforcingtheconstraint. However,
Recall now that our main objective is to design an it is not immediate from the above procedure how to
independent learning algorithm in the sense of sec- obtain an independent learning algorithm specifying
tion2inordertolearnconstrainedNEforourCMPG. an update rule for each player without coordinationIndependent Learning in Constrained Markov Potential Games
between the players. Recall for instance that the po- sume that for each s ∈ S, there exists a so-
tentialfunctionΦisunknowntotheplayersingeneral called cost potential function Φ : Π → R such
c,s
andfullgradientsofboththepotentialandconstraint that for all i ∈ N, it holds that V (π ,π )−
ci,s i −i
functions w.r.t. the joint policy cannot be available to V (π′,π ) = Φ (π ,π ) − Φ (π′,π ) for
ci,s i −i c,s i −i c,s i −i
each agent since we exclude coordination and central- any policies (π ,π )∈Π, and π′ ∈Πi.
i −i i
ization. To obtain our independent iProxCMPG, see Note that in order to use the gradient switching
Algorithm 1, we propose to use agent-wise updates subroutine in our algorithm, it is essential that
where each agent runs the gradient switching algo- all agents are able to estimate whether or not the
rithmindependentlyusingonlypartialgradientsofthe constraint holds for the current joint policy. In
potential and constraint functions w.r.t. their individ- the case of a potential cost, it is not clear how to
ual policy. Notice that our subroutine algorithm devi- provide such estimates unless agents have knowl-
atesfromtheoneproposedinLanandZhou(2020)in edge of the potential (e.g. as a known function
that we use the estimate of the constraint function V of the cost). This is an interesting question that
c
instead of the regularized constraint function Vc . merits further investigation.
η,π(t)
This is because the regularized constraint function in-
• Playerwise cost thresholds. Suppose each
volves the joint policy in the regularization while the
player i ∈ N has an individual feasibility thresh-
constraint value function can be estimated indepen-
oldα . Thesetoffeasiblepoliciesisthenredefined
dently. We further remark that for our analysis, the i
index kˆ sampled in line 7 of Algorithm 1 is supposed as Φ c :={π ∈Π|∀i∈N,V ci(π)≤α i}.
In this case, if all the cost functions c are identi-
to be picked the same by all the players (see also Re- i
caland ifthethresholdsα canbecommunicated
mark 3 in Appendix A for further details). i
among players, one can consider the hardest con-
straint α := min α and use our approach to
Stochastic Setting When exact gradients and i∈N i
find a policy solving this stricter problem (if such
value functions are not available, we estimate them
policyexists). Otherwise,ifcostfunctionsarenot
using sampled trajectories. For each joint pol-
icy π(t,k), every player i samples a trajectory τ := identical or if each agent has their private thresh-
i
(s(t,k),a(t,k),r(t,k),c(t,k)) oflengthT +1byex- old, our algorithm and analysis need further ad-
j i,j i,j j 0≤j≤Te e justments.
ecuting their own policy π(t,k). Here, s(t,k) ∼ µ and
i 0
r(t,k),c(t,k) respectively refer to the reward and cost
i,j j 4 CONVERGENCE ANALYSIS
incurred by the i-th player at the j-th step. The gra-
AND SAMPLE COMPLEXITY
dients∇ V (π(t,k))and∇ V (π(t,k))arereplacedby
πi ri πi c
their sample estimates
In this section, we establish the iteration complexity
∇ˆVri(π(t,k)):=R(Te,t,k)ψTe , ofAlgorithm1inthedeterministicsettingbeforestat-
πi i π(t,k)
i (4) ingitssamplecomplexityinthestochasticsetting. We
∇ˆVc(π(t,k)):=C(Te,t,k)ψTe , first introduce our assumptions. The first one guaran-
πi π(t,k)
i tees the existence of a strictly feasible policy that is
where R(Te,t,k) := (cid:80)Te r(t,k), C(Te,t,k) := (cid:80)Te c(t,k) available to the agents for initialization.
i j=0 i,j j=0 j
(cid:16) (cid:17)
and ψTe := (cid:80)Te ∇ logπ(t,k) a(t,k) |s(t,k) . Assumption 1. The initial policy π(0) satis-
Each
aπ gi(t e, nk)
t
estimatej s=0
V
π (πi (t,k))i
by
Vˆi,j (π(t,k)j
) :=
fies V c(π(0))<α.
c c
C(Te,t,k) independently, using the cost feedback infor- Afewremarksareinorderregardingthisassumption:
mation they receive. Note that details of trajectory
sampling are omitted in Algorithm 1 for more com- • Similar assumptions have been made in the re-
pactpresentation(forthefullversion,seeAlgorithm2
lated constrained optimization literature when
in Appendix A).
dealing with nonconvex constraints (Boob et al.,
Remark 1. As potential avenues for future work, we 2023; Ma et al., 2020; Jia and Grimmer, 2023).
would like to point out two possible generalizations of Otherwise, satisfying a constraint may require
the considered CMPG setting in which our current finding a global minimizer which is computation-
iProx-CMPG algorithm and analysis are not directly ally intractable in a general nonconvex setting.
applicable: Inourcase, thiscorrespondstofindingtheglobal
minimizer of a potential function in a fully co-
• Potential cost constraints. Suppose we do operative unconstrained MPG. While this can
not require the cost functions c to be identi- be achieved in a single agent setting thanks to
i
cal across all players i ∈ N but instead as- the gradient dominance property (Agarwal et al.,Philip Jordan, Anas Barakat, Niao He
Algorithm 1 iProxCMPG: independent Proximal-policy algorithm for CMPGs
1: initialization: π(0) ∈Πξ s.t. V (π(0))<α and suitably chosen η,β,T,K,{(ν ,δ ,ρ )}
c k k k 0≤k≤K
2: for t=0,...,T −1 do
3: π(t,0) =π(t) for i∈N
i i
4: for k =0,...,K−1 and i∈N simultaneously do
 (cid:104) (cid:105)
5: π i(t,k+1) = PP ΠΠ ii ,, ξξ (cid:104) ππ ii (( tt ,, kk )) −− νν kk ∇∇ ˆˆ ππ ii VV cri (( ππ (t( ,t k,k )) )) −−
ν
ην kηk (( ππ i(i t( ,t k,k )) −− ππ i(i t( )t) )) (cid:105) i of thVˆ ec r( wπ( it s, ek))+β−α≤δ k
6: B(t) ={⌊K/2⌋≤k ≤K |Vˆ (π(t,k))≤δ }
c k
7: π(t+1) =π(t,kˆ) where kˆ =1 if B(t) =∅ and else sampled s.t. for k ∈B(t), P(kˆ =k)=(cid:0)(cid:80) ρ (cid:1)−1 ρ
i i k∈B(t) k k
8: output: π(T) for i∈N
i
2021; Xiao, 2022), such a global optimality result policy π such that V (π) ≤ α − diam(Π)2 where
c η
is not available in the literature for our multi- diam(Π):=max ∥π−π′∥.
π,π′∈Π
agent setting to the best of our knowledge.
• A uniform strict feasibility assumption similar
• While finding a strictly feasible policy is involved
to Assumption 2 was used for centralized NE-
in general, it may be possible to find such a pol-
learning, see Alatur et al. (2023), Assumption 2.
icy in some special cases, such as when the state
space can be factored, the probability transitions
ExactGradientsCase Inthenoiselesssettingwith
are independent across agents and the constraint
access to exact gradients, we achieve the following it-
costfunctionsareseparable(seeexamples1and2
eration complexity result.
in Alatur et al. (2023) for more details).
Theorem 1. Let Assumptions 1 and 2 hold and
let the distribution mismatch coefficient D :=
In addition to initial feasibility, we require that
(cid:13) (cid:13)
Slater’s condition holds for each subproblem given by
max π∈Π(cid:13)dπ µ/µ(cid:13)
∞
be finite. For any ϵ > 0, af-
ter running iProxCMPG, Algorithm 1, for ξ = 0,
a proximal-point update. This is ensured by the fol-
suitably chosen η,β,T,K, and {(ν ,δ ,ρ )} ,
lowing uniform Slater’s condition. k k k 0≤k≤K
there exists t ∈ [T], such that π(t) is a con-
Assumption2. Letη = 1 whereL isthesmooth-
2LΦ Φ strained ϵ-NE. The total iteration complexity is given
ness parameter5 of Φ. Then, there exists ζ > 0 such by O(ϵ−4) where O(·) hides polynomial dependencies
that for any strictly feasible π′ ∈ Π, i.e., V (π′) < α,
c in m,S,A ,D,1−γ, and ζ.
max
there exists π ∈Π with Vc (π)≤α−ζ.
η,π′
The full proof of Theorem 1 is deferred to Ap-
We make the following comments:
pendix B.1. We briefly outline the key steps below.
• First, we point out that a strictly feasible π′ sat-
Proof idea. First, we show that K = O(ϵ−2) itera-
isfies Vc (π′) = V (π′) < α, i.e., existence of
η,π′ c tions of the inner loop yield a policy that is feasible
a strictly feasible policy for the regularized con-
and achieves potential value sufficiently close to the
straint function Vc is trivially given. Assump-
η,π′ exact proximal update (3). For T = O(ϵ−2), stan-
tion 2 additionally ensures that strict feasibility
dard arguments then imply existence of t ∈ [T] such
holds with slack ζ where ζ is independent of π′.
that ∥π(t+1)−π(t)∥ = O(ϵ). It can further be shown
• Similar constraint qualification conditions have that such π(t+1) satisfies a particular form of approx-
imate CMPG-specific KKT conditions for the origi-
been widely used in the nonconvex constrained
nal constrained optimization problem (1). We then
optimization literature, see Boob et al. (2022),
leverage the multi-agent structure to argue that for
Table 1 for an overview. In particular, Assump-
all i ∈ N, similar KKT conditions also hold w.r.t.
tion 2 is similar to the uniform Slater’s condition
of Ma et al. (2020). Assumption 3 in Boob et al. the playerwise problem min πi∈Πi c(π −(t i+1))V ri(π i,π −(t i+1))
(2023)isastrongfeasibilityassumptionwhichim- where π(t+1) is fixed. Finally, using playerwise gradi-
plies Assumption 2, and hence could also replace −i
entdominance(seee.g.,LemmaD.3inLeonardosetal.
it here. Strong feasibility assumes existence of a
(2022) or Lemma 2 in Giannou et al. (2022)), one can
5See also Lemma 8, item 3 for an expression of L in bound the duality gap of player i’s constrained prob-
Φ
terms of m,γ, and A . lem for all i ∈ N which implies that π(t+1) is a con-
maxIndependent Learning in Constrained Markov Potential Games
strained ϵ-NE. The total iteration complexity is given 5 SIMULATIONS
by T ·K =O(ϵ−4).
We test our stochastic iProxCMPG algorithm in two
Finite Sample Case In the stochastic setting, simple applications that can be modeled as CMPGs
when exact gradients are not available, the vari- and for which coordination among players is unrealis-
ance of the stochastic policy gradients in (4) can be tic. Bothexamplesareinspiredbyunconstrainedvari-
unbounded if the policies get closer to the bound- ants presented in Narasimha et al. (2022) who study
aries of the simplex (see e.g., Eq. (13) in Giannou MPGs. Our code is publicly available6.
et al. (2022)). Therefore, we consider exploratory ξ-
greedy policies to address this issue as in prior work
Pollution Tax Model Consider a simple environ-
(Daskalakis et al., 2020; Leonardos et al., 2022; Ding
ment with m agents representing e.g. factories, two
et al., 2022; Giannou et al., 2022). Define for any
states, pollution-free and polluted, and two actions,
ξ ≥0,i∈N the subset of ξ-greedy policies
clean and dirty corresponding to low and high pro-
Πi,ξ :={π ∈Π|∀s∈S :π (·|s)≥ξ/A } , duction volume. Starting in the pollution-free state,
i i
in each round, the environment transitions to the pol-
which is used in Algorithm 1. We are now ready to luted state if and only if at least one agent chooses
state our sample complexity result. dirty. Each agent’s reward is the sum of its profit mi-
Theorem2. LetAssumptions1and2hold,andletD nus a pollution tax. In either state, the profit is P c
(as in Theorem 1) be finite. Then, for any ϵ>0, after whenchoosingclean andP d whenchoosingdirty. The
running iProxCMPG based on finite sample estimates pollution tax is zero in the pollution-free, and T p in
(see Algorithm 2) for suitably chosen η,β,ξ,T,K,B, thepolluted state. AspointedoutbyNarasimhaetal.
and {(ν ,δ ,ρ )} , there exists t∈[T], such that (2022), due to rewards being separable in the sense
k k k 0≤k≤K
in expectation, π(t) is a constrained ϵ-NE. The total that r i(s,a i,a −i)=r i′(s)+r i′′(a i,a −i) and state tran-
sample complexity is given by O˜(ϵ−7) where O˜(·) hides sition probabilities being state independent, the pol-
polynomialdependenciesinm,S,A ,D,1−γ,andζ, lution tax model satisfies a sufficient condition under
max
as well as logarithmic dependencies in 1/ϵ. whichaMarkovgameisanMPG.Foroursimulations,
we set P = 2,P = 4, and T = 4. Due to the lack
c d p
We refer the reader to Appendix B.2 for the proof of of incentives for agents to cooperate when promoting
Theorem 2. Below, we briefly explain how we obtain environmentalsustainability,requiringcoordinationis
our sample complexity result. unrealistic in this example. Moreover, note that the
purpose of the pollution tax is to counteract pollu-
Proof idea. As in the exact gradients case, we require tion by penalizing dirty actions. However, in practice,
T = O(ϵ−2) iterations of the outer loop. In the there may be additional global requirements on the
stochastic setting, our independent implementation of minimum total production volume. To model this as
the CSA algorithm (Lan and Zhou, 2020) still con- a CMPG, we charge a cost C per agent that chooses
verges at a O(1/K)-rate due to strong convexity, but clean and impose the constraint V (π) ≤ α for ap-
c C
requires sampling a batch of size B = O(ϵ−2) for es- propriately chosen α .
C
timating constraint function values at each iteration.
We run iProxCMPG on the resulting m-agent CMPG
To counteract the variance of ξ-greedy gradient esti-
for m ∈ {2,4,8} and with C = 1,α = 12. Hyper-
mates (which in our case grows as O(ϵ−1)), we need C
parameter choices are reported in Appendix E and
to set K =O(ϵ−3). All in all, we end up with sample
Table 2. Fig. 1 shows the mean and standard devi-
complexity T ·K ·B = O(ϵ−7) for proving existence
of t ∈ [T] such that
E(cid:2)(cid:13) (cid:13)π(t)−π(t+1)(cid:13) (cid:13)(cid:3)
= O(ϵ). Us-
ation (shaded region) across independent runs of per-
iteration potential and constraint values. Note that
ing similar arguments as for Theorem 1, this implies
unlike in the theory part, we use the potential maxi-
that π(t+1) is a constrained ϵ-NE in expectation.
mizationperspectiveforexperiments. Weobservecon-
Remark 2. Comparing our result to the state-of-the- vergence to a constrained NE under which the mini-
art in the unconstrained case (O(ϵ−5), Ding et al. mum production requirements are approximately sat-
(2022)), accounting for constraints comes at a cost, isfied.
increasing the sample complexity by a O(ϵ−2)-factor.
Inthecentralizedsetting,asimilargapcanbeobserved Marketplace for Distributed Energy Resources
between best known results for unconstrained (O(ϵ−3), Asmoreandmoresmall-scaleelectricityproducersen-
Song et al. (2022)) vs. constrained (O(ϵ−5), Alatur ter the electrical grids, a marketplace emerges. Each
et al. (2023)) NE-learning. Whether this O(ϵ−2)-gap participant needs to decide how much energy to sell
can be narrowed is an interesting open question for
both centralized and independent learning. 6https://github.com/philip-jordan/iProx-CMPGPhilip Jordan, Anas Barakat, Niao He
WeextendthisgameintoaCMPGbyhavingthesys-
tem incur a cost per unit of energy provided to the
(cid:80)
grid, i.e., by defining c(s,a) = a for all s ∈ S,
1.0 12 i∈N i
andrequiringV (π)≤α wherewesetα =16. Fig.2
0.8 c e e
shows convergence to a constrained NE where players
11 0.6
satisfy the energy provision bound on average.
0.4
10
0.2 6 CONCLUSION
0.0 9
0 50 100 0 50 100
iterationt iterationt Inthispaper,weproposedanindependentlearningal-
m=2 m=4 m=8 αC gorithm for learning constrained NE in CMPGs. Our
workopensupanumberofavenuesforfuturework. It
would be interesting to investigate whether our sam-
ple complexity can be improved to match the better
Figure1: Potential(left,scaledto[0,1])andconstraint
sample complexity of centralized algorithms. Our al-
(right)valuesof iProxCMPGforthem-agentpollution
gorithm and theoretical guarantees require the agents
tax model.
to run the same algorithm: This may be seen as im-
plicitcoordinationbetweenagents. Designingfullyin-
dependent learning dynamics for our constrained set-
given the current supply and demand. The compet-
ting, where the players may not even be aware of the
itive nature of such marketplaces motivates studying
existence of other players is an interesting direction.
theconvergenceofindependentalgorithmstoNEsun-
Going beyond the class of CMPGs for learning con-
der the constraints imposed by market rules. The
strainedNEisanotherresearchdirectionthatisworth
CMPG we consider has states S = {0,...,S−1} in-
exploring. Using function approximation to scale to
dicating the grid’s current energy demand from high
large state-action spaces beyond the tabular setting is
at 0 to low at S−1. Action a ∈A ={0,...,A −1}
i i i also a promising prospect for future work.
represents the units of energy agent i contributes, for
which it is rewarded with profit r (s,a ,a )=c a2−
c a2(cid:80) a −a cs where c ,c ,i c arei m− oi del pa0 rai m- Acknowledgements
1 i i∈N i i 2 0 1 2
eters. State transitions are modeled by first sampling We would like to thank the anonymous reviewers for
w ∼ U({0,1,...,W}) which models uncertainty due their valuable comments. The work is supported by
toe.g.weather,andthensettings′ =max{0,min{S− ETH research grant, Swiss National Science Founda-
1,(cid:80)
i∈N
a i−w}}withprobability0.9ands′ =w oth- tion(SNSF)ProjectFundingNo. 200021-207343, and
erwise. For our simulations, we set S = A = W = SNSF Starting Grant. A.B. acknowledges support
5,c 0 = 2,c 1 = 0.25, and c 2 = 1.25. Narasimha et al. from the ETH Foundations of Data Science (ETH-
(2022)showthatthedescribedgameisindeedanMPG FDS) postdoctoral fellowship.
with Φ(π) = E [(cid:80)Te ϕ (a )] and ϕ (a ,a ) =
c 0(cid:80)
i∈N
a i−c
1π (cid:80),s0∼ i∈µ
N
a2
it= −0
c
1st(cid:80) 1t ≤i<j≤mas
ia
ji −− mi
cs 2. References
Agarwal, A., Kakade, S. M., Lee, J. D., and Mahajan,
G.(2021).Onthetheoryofpolicygradientmethods:
Optimality, approximation, and distribution shift.
JournalofMachineLearningResearch,22(98):1–76.
1.0
15
5, 6, 20, 22, 23
0.8
0.6 10 Alatur, P., Ramponi, G., He, N., and Krause, A.
(2023). Provably learning nash policies in con-
0.4
5 strained markov potential games. In Sixteenth Eu-
0.2
ropean Workshop on Reinforcement Learning. 1, 2,
0.0
0 3, 4, 5, 7, 8
100 101 102 100 101 102
iterationt iterationt Altman, E. (1999). Constrained Markov decision pro-
m=2 m=4 m=8 αe
cesses, volume 7. CRC press. 5
Altman, E. and Shwartz, A. (2000). Constrained
Markov Games: Nash Equilibria. In Filar, J. A.,
Figure 2: Potential (left) and constraint (right) values
Gaitsgory, V., and Mizukami, K., editors, Advances
of iProxCMPG for the m-agent energy marketplace.
in Dynamic Games and Applications, Annals of
))t(π(Φ
))t(π(Φ
))t(π(cV
))t(π(cVIndependent Learning in Constrained Markov Potential Games
the International Society of Dynamic Games, pages ElSayed-Aly,I.,Bharadwaj,S.,Amato,C.,Ehlers,R.,
213–221, Boston, MA. Birkha¨user. 1, 3, 4 Topcu,U.,andFeng,L.(2021). Safemulti-agentre-
inforcementlearningviashielding. InProceedingsof
Boob, D., Deng, Q., and Lan, G. (2022). Level con-
the 20th International Conference on Autonomous
strainedfirstordermethodsforfunctionconstrained
Agents and MultiAgent Systems, AAMAS ’21, page
optimization. arXiv preprint arXiv:2205.08011. 7
483–491,Richland,SC.InternationalFoundationfor
Boob, D., Deng, Q., and Lan, G. (2023). Stochastic Autonomous Agents and Multiagent Systems. 3
first-order methods for convex and nonconvex func-
Fox, R., Mcaleer, S. M., Overman, W., and Panageas,
tional constrained optimization. Mathematical Pro-
I. (2022). Independent natural policy gradient al-
gramming, 197(1):215–279. 3, 5, 6, 7, 17
ways converges in markov potential games. In In-
Chen, Z., Ma, S., and Zhou, Y. (2022). Finding cor- ternationalConferenceonArtificialIntelligenceand
related equilibrium of constrained markov game: A Statistics, pages 4414–4425. PMLR. 1
primal-dualapproach. InAdvances in Neural Infor-
Giannou, A., Lotidis, K., Mertikopoulos, P., and
mation Processing Systems. 3
Vlatakis-Gkaragkounis, E.-V. (2022). On the con-
Chen, Z., Zhang, K., Mazumdar, E., Ozdaglar, A. E., vergenceofpolicygradientmethodstonashequilib-
and Wierman, A. (2023). A finite-sample analy- ria in general stochastic games. Advances in Neural
sisofpayoff-basedindependentlearninginzero-sum Information Processing Systems, 35:7128–7141. 4,
stochastic games. In Thirty-seventh Conference on 7, 8
Neural Information Processing Systems. 2, 4
Gu, S., Grudzien Kuba, J., Chen, Y., Du, Y., Yang,
Dafoe, A., Bachrach, Y., Hadfield, G., Horvitz, E., L.,Knoll,A.,andYang,Y.(2023). Safemulti-agent
Larson, K., and Graepel, T. (2021). Cooperative reinforcement learning for multi-robot control. Ar-
ai: machines must learn to find common ground. tificial Intelligence, 319:103905. 1, 3
Nature, 593(7857):33–36. 1
Guo, X., Li, X., Maheshwari, C., Sastry, S., and Wu,
Dafoe, A., Hughes, E., Bachrach, Y., Collins, T., Mc-
M. (2023). Markov alpha-potential games: Equi-
Kee, K. R., Leibo, J. Z., Larson, K., and Graepel,
librium approximation and regret analysis. arXiv
T. (2020). Open problems in cooperative ai. arXiv
preprint arXiv:2305.12553. 3
preprint arXiv:2012.08630. 1
Hare, W. and Sagastiz´abal, C. (2009). Computing
Daskalakis,C.,Foster,D.J.,andGolowich,N.(2020).
proximal points of nonconvex functions. Mathemat-
Independent policy gradient methods for competi-
ical Programming, 116(1-2):221–258. 3
tive reinforcement learning. Advances in neural in-
formation processing systems,33:5527–5540. 2,4,8, Jia, Z. and Grimmer, B. (2023). First-Order Meth-
24 ods for Nonsmooth Nonconvex Functional Con-
strainedOptimizationwithorwithoutSlaterPoints.
Davis, D.andGrimmer, B.(2019). Proximallyguided
arXiv:2212.00927 [math]. 3, 5, 6, 17
stochastic subgradient method for nonsmooth, non-
convex problems. SIAM Journal on Optimization, Lan, G. and Zhou, Z. (2020). Algorithms for stochas-
29(3):1908–1930. 3 tic optimization with function or expectation con-
straints. Computational Optimization and Applica-
Diddigi, R. B., Danda, S. K. R., J., P. K., and
tions, 76(2):461–498. 5, 6, 8, 25, 26, 27
Bhatnagar, S. (2020). Actor-Critic Algorithms for
Constrained Multi-agent Reinforcement Learning. Leonardos, S., Overman, W., Panageas, I., and Pil-
arXiv:1905.02907 [cs]. 5 iouras,G.(2022). Globalconvergenceofmulti-agent
policygradientinmarkovpotentialgames. InInter-
Ding, D., Wei, C.-Y., Zhang, K., and Jovanovic, M.
nationalConferenceonLearningRepresentations.1,
(2022).IndependentPolicyGradientforLarge-Scale
2, 4, 5, 7, 8, 15, 23, 24
Markov Potential Games: Sharper Rates, Func-
tion Approximation, and Game-Agnostic Conver- Liu, M., Kolmanovsky, I., Tseng, H. E., Huang,
gence. In Proceedings of the 39th International S., Filev, D., and Girard, A. (2023). Potential
Conference on Machine Learning, pages 5166–5220. game-based decision-making for autonomous driv-
PMLR. ISSN: 2640-3498. 1, 2, 3, 4, 5, 8 ing. IEEE Transactions on Intelligent Transporta-
tion Systems. 1
Ding, D., Wei, X., Yang, Z., Wang, Z., and Jovanovic,
M. (2023). Provably efficient generalized lagrangian Ma, R., Lin, Q., and Yang, T. (2020). Quadratically
policy optimization for safe multi-agent reinforce- regularized subgradient methods for weakly convex
ment learning. In Learning for Dynamics and Con- optimization with weakly convex constraints. pages
trol Conference, pages 315–332. PMLR. 3 6554–6564. PMLR. 3, 5, 6, 7, 18Philip Jordan, Anas Barakat, Niao He
Macua, S. V., Zazo, J., and Zazo, S. (2018). Learning Xiao, L. (2022). On the convergence rates of policy
parametric closed-loop policies for markov poten- gradientmethods. JournalofMachineLearningRe-
tialgames. InInternationalConferenceonLearning search, 23(282):1–36. 7
Representations. 1, 2, 4
Yao, Z.andDing, Z.(2022). Learningdistributedand
Maheshwari, C., Wu, M., Pai, D., and Sastry, S. fair policies for network load balancing as markov
(2023). Independent and Decentralized Learning potential game. Advances in Neural Information
in Markov Potential Games. arXiv:2205.14590 [cs, Processing Systems, 35:28815–28828. 2
eess]. 1, 2, 3, 4
Zhang,R.,Mei,J.,Dai,B.,Schuurmans,D.,andLi,N.
Mao, W., Yang, L., Zhang, K., and Basar, T. (2022). (2022a). On the global convergence rates of decen-
On improving model-free algorithms for decentral- tralized softmax gradient play in markov potential
ized multi-agent reinforcement learning. In Inter- games. Advances in Neural Information Processing
national Conference on Machine Learning, pages Systems, 35:1923–1935. 1, 2, 3, 23
15007–15049. PMLR. 3 Zhang, R. C., Ren, Z., and Li, N. (2022b). Gradi-
Marden, J. R. (2012). State based potential games. ent play in stochastic games: Stationary points and
Automatica, 48(12):3075–3088. 2 local geometry. IFAC-PapersOnLine, 55(30):73–
78. 25thInternationalSymposiumonMathematical
Monderer, D. and Shapley, L. S. (1996). Potential
Theory of Networks and Systems MTNS 2022. 1, 2,
games. Games and economic behavior, 14(1):124–
3, 5
143. 2, 4
Zhou, Z., Chen, Z., Lin, Y., and Wierman, A. (2023).
Narasimha, D., Lee, K., Kalathil, D., and Shakkottai,
Convergence rates for localized actor-critic in net-
S. (2022). Multi-agent learning via markov poten-
workedMarkovpotentialgames.InEvans,R.J.and
tialgamesinmarketplacesfordistributedenergyre-
Shpitser,I.,editors,Proceedings of the Thirty-Ninth
sources. In2022 IEEE 61st Conference on Decision
ConferenceonUncertaintyinArtificialIntelligence,
and Control (CDC), pages 6350–6357. IEEE. 3, 8,
volume 216 of Proceedings of Machine Learning Re-
9
search, pages 2563–2573. PMLR. 1, 3
Ozdaglar, A., Sayin, M. O., and Zhang, K. (2021).
Independent learning in stochastic games. In-
vited chapter for the International Congress of Checklist
Mathematicians 2022 (ICM 2022), arXiv preprint
arXiv:2111.11743. 1 1. For all models and algorithms presented, check if
you include:
Paternain, S., Chamon, L., Calvo-Fullana, M., and
Ribeiro,A.(2019).Constrainedreinforcementlearn- (a) A clear description of the mathematical set-
ing has zero duality gap. Advances in Neural Infor- ting, assumptions, algorithm, and/or model.
mation Processing Systems, 32. 5 [Yes]
Polyak, B. T. (1967). A general method for solving (b) An analysis of the properties and complexity
extremalproblems. InDoklady Akademii Nauk,vol- (time, space, sample size) of any algorithm.
ume174,pages33–36.RussianAcademyofSciences. [Yes]
25 (c) (Optional) Anonymized source code, with
specification of all dependencies, including
Sayin, M., Zhang, K., Leslie, D., Basar, T., and
external libraries. [Yes]
Ozdaglar, A. (2021). Decentralized q-learning in
zero-sum markov games. Advances in Neural In-
2. For any theoretical claim, check if you include:
formation Processing Systems, 34:18320–18334. 2
(a) Statements of the full set of assumptions of
Shalev-Shwartz, S., Shammah, S., and Shashua, A.
all theoretical results. [Yes]
(2016). Safe, multi-agent, reinforcement learn-
(b) Complete proofs of all theoretical results.
ing for autonomous driving. arXiv preprint
[Yes]
arXiv:1610.03295. 1
(c) Clearexplanationsofanyassumptions. [Yes]
Song, Z., Mei, S., and Bai, Y. (2022). When can we
learn general-sum markov games with a large num- 3. For all figures and tables that present empirical
ber of players sample-efficiently? In International results, check if you include:
Conference on Learning Representations. 1, 2, 3, 8
(a) The code, data, and instructions needed to
Vershynin, R. (2018). High-dimensional probability: reproduce the main experimental results (ei-
An introduction with applications in data science, ther in the supplemental material or as a
volume 47. Cambridge university press. 25 URL). [Yes]Independent Learning in Constrained Markov Potential Games
(b) All the training details (e.g., data splits, hy-
perparameters, how they were chosen). [Yes]
(c) A clear definition of the specific measure or
statisticsanderrorbars(e.g.,withrespectto
the random seed after running experiments
multiple times). [Yes]
(d) Adescriptionofthecomputinginfrastructure
used. (e.g.,typeofGPUs,internalcluster,or
cloud provider). [Yes]
4. If you are using existing assets (e.g., code, data,
models)orcurating/releasingnewassets, checkif
you include:
(a) CitationsofthecreatorIfyourworkusesex-
isting assets. [Yes]
(b) The license information of the assets, if ap-
plicable. [Not Applicable]
(c) New assets either in the supplemental mate-
rial or as a URL, if applicable. [Yes]
(d) Information about consent from data
providers/curators. [Not Applicable]
(e) Discussion of sensible content if applicable,
e.g.,personallyidentifiableinformationorof-
fensive content. [Not Applicable]
5. If you used crowdsourcing or conducted research
with human subjects, check if you include:
(a) The full text of instructions given to partici-
pants and screenshots. [Not Applicable]
(b) Descriptions of potential participant risks,
with links to Institutional Review Board
(IRB) approvals if applicable. [Not Appli-
cable]
(c) The estimated hourly wage paid to partici-
pants and the total amount spent on partic-
ipant compensation. [Not Applicable]Supplementary Materials
CONTENTS
A iProxCMPG: FULL STOCHASTIC ALGORITHM 13
B PROOFS FOR SECTION 4 13
B.1 Proof of Theorem 1 — Exact Gradients Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
B.2 Proof of Theorem 2 — Finite Sample Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
B.3 Other Technical Lemmas. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C STRONGLY CONVEX STOCHASTIC OPTIMIZATION WITH STRONGLY CONVEX
EXPECTATION CONSTRAINT 25
C.1 A Primal Gradient Switching Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
C.2 Convergence and Sample Complexity Guarantee . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
D BACKGROUND IN CONSTRAINED OPTIMIZATION AND A NOVEL TECHNICAL
LEMMA 30
D.1 Approximate KKT Points in Constrained Optimization . . . . . . . . . . . . . . . . . . . . . . . 30
D.2 A Novel Technical Lemma for Approximate Optimality under Gradient Dominance . . . . . . . . 31
E ADDITIONAL DETAILS ABOUT SIMULATIONS 32
A iProxCMPG: FULL STOCHASTIC ALGORITHM
In this section, for the convenience of the reader, we report the full pseudo-code of Algorithm 1 in the stochastic
setting where exact gradients are not available. See Algorithm 2.
Remark 3. For our analysis, the index kˆ sampled in line 11 of Algorithm 2 is supposed to be picked the same by
all the players. This sampling step does not require any state-action-reward samples, it is just an index sampling
that is useful for our proofs (even in the exact gradients case), see the constraint satisfaction inequality in the
proof of Theorem 3, page 28.
B PROOFS FOR SECTION 4
Notation For any integer n≥1, we use the notation [n]:={1,...,n} throughout the proofs.
In this section, we provide complete proofs of our main results. We begin with the exact gradients case before
addressing the more involved finite sample case.
B.1 Proof of Theorem 1 — Exact Gradients Case
First, we restate Theorem 1.Independent Learning in Constrained Markov Potential Games
Algorithm 2 iProxCMPG: independent Proximal-policy algorithm for CMPGs
1: initialization: π(0) ∈Πξ s.t. V (π(0))<α and suitably chosen η,β,ξ,T,K,{(ν ,δ ,ρ )}
c k k k 0≤k≤K
2: for t=0,...,T −1 do
3:
π(t,0) =π(t)
i i
4: for k =0,...,K−1 and i∈N simultaneously do
5: sample B trajectories {{(a(b),s(b),r(b),c(b))}Tˆ e(b) }B by following π(t,k)
i,j j i,j j j=0 b=1 i
6: set Vˆ (π(t,k))= 1 (cid:80)B (cid:80)Tˆ e(b) r(b) and Vˆ (π(t,k))= 1 (cid:80)B (cid:80)Tˆ e(b) c(b)
ri B b=1 j=0 i,j c B b=1 j=0 j
7: ∇ˆV πr ii(π(t,k))=Vˆ ri(π(t,k))· B1 (cid:80)B b=1(cid:80)T jˆ =e(b 1) ∇logπ i(a i( ,b j) |s( jb))
8: ∇ˆVc(π(t,k))=Vˆ (π(t,k))· 1 (cid:80)B (cid:80)Tˆ e(b) ∇logπ (a(b) |s(b))
πi

c
(cid:104)
B b=1 j=1 i i,j j
(cid:105)
9: π i(t,k+1) = P PΠ Πi i, ,ξ ξ(cid:104)π πi i( (t t, ,k k) )− −ν νk k∇ ∇ˆ ˆπ πi iV Vr ci (( ππ (t( ,t k,k )) )) −−
ν
ην kηk (( ππ i(i t( ,t k,k )) −− ππ i(i t( )t) )) (cid:105) i of thVˆ ec r( wπ( it s, ek))+β−α≤δ k
10: B(t) ={⌊K/2⌋≤k ≤K |Vˆ (π(t,k))≤δ }
c k
11: π(t+1) =π(t,kˆ) where kˆ =1 if B(t) =∅ and else sampled s.t. for k ∈B(t), P(kˆ =k)=(cid:0)(cid:80) ρ (cid:1)−1 ρ
i i k∈B(t) k k
12: output: π(T) for i∈N
i
(cid:13) (cid:13)
Theorem 1. LetAssumptions1and2holdandletthedistributionmismatchcoefficientD :=max π∈Π(cid:13)dπ µ/µ(cid:13)
∞
be finite. For any ϵ > 0, after running iProxCMPG, Algorithm 1, with ξ = 0, suitably chosen η,β,T,K, and
{(ν ,δ ,ρ )} , there exists t∈[T], such that π(t) is a constrained ϵ-NE in expectation7. The total iteration
k k k 0≤k≤K
complexity is given by
O(cid:0) ϵ−4(cid:1)
where O(·) hides polynomial dependencies in m,S,A ,D,1−γ, and ζ.
max
BeforeanalyzingtheouterloopofAlgorithm1,webeginbyfocusingontheproximal-pointupdatestep. Wefirst
introducesomeusefulnotation. Then,weexplainhowwecanusetheswitchinggradientalgorithminAppendixC
forapproximatelysolvingtheproximal-pointupdatestepindependently. Weproceedbyestablishingguarantees
that will be important in the analysis of the outer loop of Algorithm 1.
Notation Recall that for any policies π,π′ ∈ Π and any η > 0, Φ (π) = Φ(π)+ 1 ∥π−π′∥2, Vc (π) =
η,π′ 2η η,π′
V (π)+ 1 ∥π−π′∥2 and Πc = (cid:8) π ∈Π|Vc (π)≤α−β(cid:9) . Moreover, recall the following constrained opti-
c 2η η,π′ η,π′
mization problem:
min Φ (π). (ProxPb(η,π′))
η,π′
π∈Πc
η,π′
Inthefollowing“≲”denotesinequalityuptonumericalconstants. Moreover,letL bethesmoothnessconstant
Φ
ofthefunctionsΦandV (seeLemma8)andletΦ beanupperbound8onΦ. RecallthatunderAssumption1,
c max
the initial policy π(0) is strictly feasible. We denote the respective slack by ζ¯ >0, i.e., ζ¯ :=α−V (π(0)).
0 0 c
Next, we state and prove the guarantees provided by our proximal-point update subroutine.
Lemma 1. Let Assumption 1 hold and let 0 < ϵ¯ ≤ ζ¯. Set β = ϵ¯, η = 1 , and ξ = 0. Denote
by π˜(t+1) the unique optimal solution to (ProxPb(η,π(t))).0 There exist K = O(cid:0) ϵ¯2 −L 2Φ(cid:1) and suitable choices of
{(ν ,δ ,ρ )} , such that lines 4-6 of Algorithm 1 guarantee that for any t∈[T −1],
k k k 0≤k≤K
(cid:104) (cid:105)
E Φ (π(t+1))−Φ (π˜(t+1)) ≤ϵ¯2,
η,π(t) η,π(t)
(5)
(cid:104) (cid:105)
E V (π(t+1)) ≤α,
c
where the expectation is with respect to the randomness induced by the sampling of kˆ in line 11 of Algorithm 2.
Proof. We divide the proof into two steps.
7Notice that here we take the expectation w.r.t. the randomness which is induced by the sampling of kˆ in line 11 of
Algorithm 2.
8Such a bound is always trivially available.Philip Jordan, Anas Barakat, Niao He
• Step 1: Equivalent centralized update rule for our algorithm. First, we argue that independently
running the subroutine given by the inner loop of Algorithm 1, i.e., lines 4-6, is equivalent to a centralized
executionofthestochasticswitchingsubgradientalgorithm(seeAlgorithm3)appliedtoourproximal-point
update problem. Crucially, as observed by Leonardos et al. (2022), Proposition B.1, for any i ∈ N and
π ∈Π, it holds that ∇ Φ(π)=∇ V (π). Wecan extend this observation to ourregularized potential and
πi πi ri
value functions, namely for any π′ ∈Π,
1
∇ Φ (π)=∇ Φ(π)+ (π −π′)
πi η,π′ πi η i i
1
=∇ V (π)+ (π −π′),
πi ri η i i
which is an expression that can be evaluated independently by player i, since access to the joint policy π is
not required. Together with separability of the projection operator P , see e.g. Leonardos et al. (2022),
Πξ
Lemma D.1, we have
(cid:16) (cid:104) (cid:105)(cid:17) (cid:104) (cid:105)
P π(t,k)−ν ∇ Vri (π(t,k)) =P π(t,k)−ν ∇ Φ (π(t,k)) ,
Πi,ξ i k πi η,π(t,k)
i∈N
Πξ k π η,π(t,k)
and similarly, for the constraint value function,
(cid:16) (cid:104) (cid:105)(cid:17) (cid:104) (cid:105)
P π(t,k)−ν ∇ Vc (π(t,k)) =P π(t,k)−ν ∇ Vc (π(t,k)) .
Πi,ξ i k πi η,π(t,k)
i∈N
Πξ k π η,π(t,k)
Moreover, since V (π(t,k)) can be estimated equally by each player due to the cooperative nature of our
c
constraint, we can conclude that Algorithm 1 is equivalent to a centralized version where the independent,
simultaneous update in line 5 is replaced by the following centralized version:
 P (cid:104) π(t,k)−ν ∇ˆ Φ (π(t,k))(cid:105) if Vˆ (π(t,k))+β−α≤δ ,
 Πξ k π η,π(t,k) c k
π(t,k+1) =
(cid:104) (cid:105)
P π(t,k)−ν ∇ˆ Vc (π(t,k)) otherwise.
Πξ k π η,π(t,k)
• Step 2: Induction on t. Next, to prove the claimed guarantee for all t∈[T−1], we proceed by induction
on t. We will invoke results on the stochastic switching gradient algorithm (see CSA, Algorithm 3) that
are separately presented in Appendix C in the context of constrained optimization. By Assumption 1,
since ϵ¯ ≤ ζ¯ and β = ϵ¯, we have V (π(0)) ≤ α−β. That is, for t = 0, the initial feasibility condition
0 c
of our CSA result, Theorem 3 in Appendix C, holds for π(t). Note further that in our deterministic case,
Assumption 3 (which is required for Theorem 3) holds, since by Lemma 8 we have a bound on objective
and constraint gradient norms.
Hence, we can apply Theorem 3 in the deterministic setting, i.e., with batch size J = 1 and ac-
cess to exact gradients and constraint function values, to Φ and Vc with µ = L and M2 ≲
η,π(t) η,π(t) Φ
max(cid:8) M2 +µ2∆4,M2 +µ2∆4(cid:9) ≲ M2 +L2diam(Π)4, in the notation of Theorem 3. After plugging in
G G F F c Φ
the bounds on M ,L , and diam(Π) from Lemma 8, and choosing K as in the statement of this lemma,
c Φ
Theorem 3 implies the desired bounds on constraint violation and optimality gap w.r.t. π˜(t+1) in (5). This
concludes the base case of the induction.
Asinductionhypothesis,supposenowthat(5)holdsforsomet∈[T−1]. Then,duetoβ ≥ϵ¯,V (π(t+1))+β ≤
c
α+ϵ¯impliesthattheinitialfeasibilityconditionofTheorem3issatisfiedandhencewiththesameargument
as above regarding Assumption 3, we can apply Theorem 3 to conclude that at the end of iteration t+1 of
Algorithm 1, the inner loop guarantees that
(cid:104) (cid:105)
E Φ (π(t+2))−Φ (π˜(t+2)) ≤ϵ¯2,
η,π(t+2) η,π(t+2)
(cid:104) (cid:105)
E V (π(t+2)) ≤α,
c
i.e., the inductive hypothesis also holds for t+1.Independent Learning in Constrained Markov Potential Games
We next determine the number of iterations of the outer loop of Algorithm 1 required for convergence in the
following sense.
Lemma 2. Let ϵ > 0 and set η = 1 . Suppose K is chosen such that the guarantee from Lemma 1 holds
2LΦ
for ϵ¯2 = ϵ2. Then, after T = 4ηΦmax iterations of the outer loop of Algorithm 1 where Φ is an upper bound
4η ϵ2 max
of the potential function (i.e., ∀π ∈Π,Φ(π)≤Φ ), there exists 0≤t≤T −1 such that E[∥π(t+1)−π(t)∥]≤ϵ.
max
Remark 4. Notice that we need an expectation in Lemma 2 in the exact gradients case because of the random
index sampling in Algorithm 1. See also Remark 3.
Proof. Let F denote the σ-field generated by the random variables given by the iterates π(t) up to iteration t.
t
Notice that this randomness is induced by the sampling of kˆ in line 11 of Algorithm 2. By Lemma 1, the inner
loop of Algorithm 1 guarantees that for any 0≤t≤T −1,
(cid:20) 1 (cid:21) (cid:104) (cid:105)
E Φ(π(t+1))+ ∥π(t+1)−π(t)∥2 |F =E Φ (π(t+1))|F
2η t η,π(t) t
(cid:104) (cid:105)
≤E Φ (π˜(t+1))|F +ϵ¯2
η,π(t) t
≤Φ (π(t))+ϵ¯2
η,π(t)
=Φ(π(t))+ϵ¯2
where the second inequality is due to E(cid:104) Vc (π(t))|F (cid:105) = E(cid:2) V (π(t))(cid:3) ≤ α. Taking total expectation in the
η,π(t) t c
above inequality, we obtain
(cid:104) (cid:105) (cid:16) (cid:104) (cid:105) (cid:104) (cid:105)(cid:17)
E ∥π(t+1)−π(t)∥2 ≤2η E Φ(π(t)) −E Φ(π(t+1)) .
Summing the above inequality over 0 ≤ t ≤ T −1, using the upper bound Φ on the potential function and
max
plugging in our choices of η, T, and ϵ¯, we obtain
T−1 (cid:32) T−1 (cid:33)
1 (cid:88) (cid:104) (cid:105) 1 (cid:88) (cid:104) (cid:105) (cid:104) (cid:105)
E ∥π(t+1)−π(t)∥2 ≤2η ϵ¯2+ E Φ(π(t)) −E Φ(π(t+1))
T T
t=0 t=0
2ηΦ
≤ max +2ηϵ¯2
T
≤ϵ2.
Using Jensen’s inequality, we conclude that there exists t∈[T −1] such that
E(cid:2) ∥π(t+1)−π(t)∥(cid:3)
≤ϵ.
Next, we aim to prove that the event ∥π(t+1)−π(t)∥≤ϵ implies Nash-gap(π(t+1))=O(ϵ) where the constrained
Nash-gap is defined as
Nash-gap(π∗):=max max V (π∗)−V (π′,π∗ ). (6)
i∈N π′∈Πi(π∗ )
ri ri i −i
i c −i
As a result, we will be able to argue that
E(cid:2) ∥π(t+1)−π(t)∥(cid:3)
≤ϵ implies
E(cid:2) Nash-gap(π(t+1))(cid:3)
=O(ϵ), i.e., that
the policy π(t+1) is a constrained O(ϵ)-NE in expectation.
Towards this goal, we first show that a policy π(t+1) satisfying ∥π(t+1) − π(t)∥ = O(ϵ) (as in the previous
(cid:93) (cid:93)
lemma) is a O(ϵ)-KKT policy for our initial constrained minimization problem. The ϵ-KKT conditions are a
slight modification of the standard ϵ-KKT conditions adapted to our specific requirements (see Definition 1 and
Definition 2 in Appendix D). In the following lemma, we will be referring to (in-)exact solutions as well as KKT
(cid:93)
and KKT conditions for different problems. Therefore, we first introduce additional useful notation for clarity.
Notation We refer to the following constrained optimization problem as (InitPb):
minΦ(π). (InitPb)
π∈ΠPhilip Jordan, Anas Barakat, Niao He
For the previously introduced (ProxPb(η,π(t))), we distinguish between the inexact solution resulting from the
update which we denote by π(t+1), and the exact solution which will be denoted by π˜(t+1) in the proof below.
Furthermore, we define the Lagrangians for the two problems as
L(π,λ)=Φ(π)+λ(V (π)−α) , (InitPb–L)
c
L (π,λ)=Φ (π)+λ(cid:0) Vc (π)−α+β(cid:1) . (ProxPb(η,π′)–L)
η,π′ η,π′ η,π′
Using Lemma 1 and Lemma 2, the following lemma shows that Algorithm 1 is guaranteed to generate an O(ϵ)-
(cid:93)
KKTpolicy. Partsoftheproofhaveappearedinasimilarformintheoptimizationliterature(seeLemma3.5and
Theorem 3.2 in Jia and Grimmer (2023), and Theorem 5 in Boob et al. (2023)). The lemma below differs from
(cid:93)
theseresults, sinceweareinasmoothsettingandproveconvergencew.r.t.ournotionofKKTconditionsrather
than towards a point that is near an ϵ-KKT point. Moreover, our guarantee for the proximal update subroutine
is somewhat weaker due to the relaxed constraint satisfaction condition that we use to switch between update
types in the inner loop, see Lemma 1. Additionally, in order to achieve exact primal feasibility (instead of
ϵ-approximate), we employ a feasibility margin β.
Lemma 3. Let Assumptions 1 and 2 hold. Let ϵ > 0 and choose ϵ¯,K,β as in Lemma 1 and 2. If π(t+1) is a
policy such that ∥π(t+1)−π(t)∥≤ϵ for some t∈[T−1], then π(t+1) is a (C ϵ)-K(cid:93) KT policy of (InitPb) where
KKT
C KKT is a positive constant such that C KKT ≲ (m 1−2.5 γA )41 m .. 5a5 √xS ζ.
Proof. First, note that (ProxPb(η,π(t))) is a strongly convex optimization problem with strongly convex con-
straints, which is sufficient for the existence of a unique optimum π˜(t+1). Since by Assumption 2, Slater’s
condition holds for (ProxPb(η,π′)) for any π′ ∈Π, strong duality is given for (ProxPb(η,π(t))) and hence there
existsafinitedualvariableλ˜(t+1) ≥0formingaKKTpairwithπ˜(t+1). Wefirstclaimthat∥π˜(t+1)−π(t+1)∥≤ϵ.
This can be seen as follows: By optimality of (π˜(t+1),λ˜(t+1)) for (ProxPb(η,π(t))), we have
(cid:16) (cid:17)
λ˜(t+1) Vc (π˜(t+1))−α+β =0, (7)
η,π(t)
(cid:68) (cid:69)
∇ L (π˜(t+1),λ˜(t+1)),π(t+1)−π˜(t+1) ≥0. (8)
π η,π(t)
From Lemma 8, we know that L (·,λ˜(t+1)) is L (1+λ˜(t))-strongly convex. Therefore, after rearranging the
η,π(t) Φ
standard strong convexity lower bound, we get
L (cid:16) (cid:17)
Φ 1+λ˜(t+1) ∥π˜(t+1)−π(t+1)∥2
2
(cid:68) (cid:69)
≤L (π(t+1),λ˜(t+1))−L (π˜(t+1),λ˜(t+1))− ∇ L (π˜(t+1),λ˜(t+1)),π(t+1)−π˜(t+1)
η,π(t) η,π(t) π η,π(t)
(a) (cid:16) (cid:17)
≤ Φ (π(t+1))−Φ (π˜(t+1))+λ˜(t+1) V (π(t+1))−α−β
η,π(t) η,π(t) c
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
≤ϵ¯2 ≤0
(b)
≤ ϵ¯2,
where step (a) follows by applying (7) and (8), and step (b) by Lemma 1, i.e. the guarantee for π(t+1) provided
by the algorithm’s inner loop. Then, it follows from the previous inequality that
(cid:115)
2ϵ¯2 ϵ
∥π˜(t+1)−π(t+1)∥≤ ≤ √ ≤ϵ. (9)
L Φ 2
Using the fact that (π˜(t+1),λ˜(t+1)) is a KKT pair for (ProxPb(η,π(t))), we now argue that (π(t+1),λ˜(t+1)) is a
(cid:93)
(C ϵ)-KKT pair for (InitPb) (see Definition 2 in Appendix D). We check each one of the requirements of the
KKT
definition in what follows.
• Exact primal feasibility: By Lemma 1, we know that V (π(t+1))≤α for any 0≤t≤T −1.
cIndependent Learning in Constrained Markov Potential Games
• Dual feasibility: This immediately holds by dual feasibility of (π˜(t+1),λ˜(t+1)) for (ProxPb(η,π(t))).
• Complementary slackness: When λ˜(t+1) = 0, we clearly have |λ˜(t+1)(cid:0) V (π(t+1))−α(cid:1) | = 0 ≤ ϵ. Other-
c
wise, we have
(a)
V (π(t+1)) ≥ V (π˜(t+1))−M ϵ
c c c
( =b) α−β− 1 ∥π˜(t+1)−π(t)∥2−M ϵ
2η c (10)
(c) ϵ2 (cid:18) 1 (cid:19)
≥ α− − M + √ ϵ,
η c 2 η
where (a) follows from M Lipschitz continuity of V (see Lemma 8-item (1)) and Eq. (9), (b) stems from
c c
complementaryslacknessof(π˜(t+1),λ˜(t+1))for(ProxPb(η,π(t)))whichstatesthatVc (π˜(t+1))−α+β =0.
η,π(t)
To obtain inequality (c), observe that using the bound from (9), our assumption on ∥π(t+1)−π(t)∥, and the
triangle inequality, we have ∥π˜(t+1)−π(t)∥≤∥π˜(t+1)−π(t+1)∥+∥π(t+1)−π(t)∥≤2ϵ.
Combining (10) with the upper bound V (π(t+1))≤α from primal feasibility, we get
c
(cid:12) (cid:16) (cid:17)(cid:12) (cid:18) ϵ2 ϵ (cid:19)
(cid:12)λ˜(t+1) V (π(t+1))−α (cid:12)≤λ˜(t+1) +M ϵ+ √ . (11)
(cid:12) c (cid:12) η c 2 η
We now show that the dual variable λ˜(t+1) is bounded by a constant depending on ζ using Assumption 2
and strong duality. Indeed, we have
(cid:13) (cid:13) (cid:13) (cid:13)
λ˜(t+1) ≤
(cid:13)∇ πΦ(π˜(t+1))(cid:13)+η−1(cid:13)π˜(t+1)−π(t)(cid:13)
≤
M c+4ϵη−1
, (12)
(cid:112) (cid:112)
ζη−1 ζη−1
where the first inequality follows from the proof of Lemma 1 in Ma et al. (2020), whereas the second
(cid:13) (cid:13)
inequalityusesLipschitznessofthepotentialfunction(seeLemma8)andthefactthat(cid:13)π˜(t+1)−π(t)(cid:13)≤2ϵ.
Combining(11)and(12),andusingtheboundsonM andL fromLemma8,weobtainthedesiredC ϵ-
c Φ KKT
complementary slackness.
• Variational Lagrangian stationarity: Suppose by contradiction that the Lagrangian stationarity con-
dition that comes with the
2ϵ(1+λ˜(t+1))-KKT
conditions does not hold for π˜(t+1) and (InitPb). Then there
η
exists ν ∈N (π˜(t+1),λ˜(t+1)) (normal cone to the convex set of policies Π) such that
Π
(cid:13) (cid:13) 2ϵ(1+λ˜(t+1))
∇ L (π˜(t+1),λ˜(t+1))+ν =0 and (cid:13)∇ L(π˜(t+1),λ˜(t+1))+ν(cid:13)>
π η,π(t) (cid:13) π (cid:13) η
where the equality is by Lagrangian stationarity of π˜(t+1) for (ProxPb(η,π(t))) and the inequality is due
to the above assumed lack of Lagrangian stationarity of π˜(t+1) for (InitPb). Plugging in the definition of
L (π˜(t+1),λ˜(t+1)) and combining the equality and inequality above, one can conclude that
η,π(t)
2ϵ(1+λ˜(t+1)) (cid:13) (cid:13) 1+λ˜(t+1) (cid:13) (cid:13)
<(cid:13)∇ L(π˜(t+1),λ˜(t+1))+ν(cid:13)= (cid:13)π˜(t+1)−π(t)(cid:13) ,
η (cid:13) π (cid:13) η (cid:13) (cid:13)
which contradicts the inequality (cid:13) (cid:13)π˜(t+1)−π(t)(cid:13) (cid:13) ≤ 2ϵ. Hence using the bound on λ˜(t+1) from (12), the
(cid:18) (cid:19)
policy π˜(t+1) is a C˜ϵ-KKT policy for (InitPb) with C˜ = 2 1+ M√c+2ϵη−1 .
η ζη−1
By Lemma 13, this implies that
(cid:68) (cid:69)
max π˜(t+1)−π′,∇ L(π˜(t+1),λ˜(t+1)) ≤diam(Π)C˜ϵ. (13)
π
π′∈Π
Then, in view of showing the variational Lagrangian stationarity for the pair (π(t+1),λ˜(t+1)) for (InitPb),Philip Jordan, Anas Barakat, Niao He
we can write
(cid:68) (cid:69)
max π(t+1)−π′,∇ L(π(t+1),λ˜(t+1))
π
π′∈Π
(cid:68) (cid:69) (cid:68) (cid:69)
=max π˜(t+1)−π′,∇ L(π(t+1),λ˜(t+1)) + π(t+1)−π˜(t+1),∇ L(π(t+1),λ˜(t+1))
π π
π′∈Π
(cid:68) (cid:69) (cid:13) (cid:13) (cid:13) (cid:13)
≤max π˜(t+1)−π′,∇ L(π(t+1),λ˜(t+1)) +(cid:13)π(t+1)−π˜(t+1)(cid:13)·(cid:13)∇ L(π(t+1),λ˜(t+1))(cid:13)
π (cid:13) (cid:13) (cid:13) π (cid:13)
π′∈Π
(cid:68) (cid:69)
≤max π˜(t+1)−π′,∇ L(π(t+1),λ˜(t+1)) +ϵ(1+λ˜(t+1))M . (14)
π c
π′∈Π
We now bound the first term in the above inequality by using 2L (1+λ˜(t+1))-smoothness of ∇ L(·,λ˜(t+1))
Φ π
and (13). Using the fact that max (A(π)+B(π)) ≤ max A(π)+max B(π) for any functions
π∈Π π∈Π π∈Π
A(π),B(π), we have
(cid:68) (cid:69)
max π˜(t+1)−π′,∇ L(π(t+1),λ˜(t+1))
π
π′∈Π
(cid:68) (cid:69) (cid:68) (cid:69)
≤max π˜(t+1)−π′,∇ L(π(t+1),λ˜(t+1))−∇ L(π˜(t+1),λ˜(t+1)) +max π˜(t+1)−π′,∇ L(π˜(t+1),λ˜(t+1))
π π π
π′∈Π π′∈Π
(cid:13) (cid:13) (cid:13) (cid:13)
≤max(cid:13)π˜(t+1)−π′(cid:13)·2L (1+λ˜(t+1))(cid:13)π˜(t+1)−π(t+1)(cid:13)+diam(Π)C˜ϵ
(cid:13) (cid:13) Φ (cid:13) (cid:13)
π′∈Π
(cid:16) (cid:17)
≤ 2diam(Π)L (1+λ˜(t+1))+diam(Π)C˜ ϵ.
Φ
Combining the above inequality with (14), we obtain
(cid:68) (cid:69) (cid:16) (cid:17)
max π(t+1)−π′,∇ L(π(t+1),λ˜(t+1)) ≤ (1+λ˜(t+1))M +2diam(Π)L (1+λ˜(t+1))+diam(Π)C˜ ϵ.
π c Φ
π′∈Π
Finally, we use the bound on λ˜(t+1) from (12), as well as bounds on diam(Π), L , and M from Lemma 8, to
Φ c
conclude that π(t+1) is a C ϵ-K(cid:93) KT policy for (InitPb).
KKT
(cid:93)
Tocompletetheanalysis,itnowremainstoshowthatanO(ϵ)-KKTpolicyof (InitPb)isaconstrainedO(ϵ)-NE.
For this, we leverage the playerwise gradient domination property satisfied by the potential function and the
constraint value function. We first introduce some notations.
Notation For each player i∈N and each policy π ∈Π−i, consider the playerwise constrained optimization
−i
problem given by
min V (π ,π ) . (PlayerPb(π ))
πi∈Πi c(π−i)
ri i −i −i
The respective Lagrangian L :Πi×R →R is defined for every π ∈Πi and every λ≥0 by
π−i ≥0 i
L (π ,λ)=Φ(π ,π )+λ(V (π ,π )−α) . (PlayerPb(π )–L)
π−i i i −i c i −i −i
Lemma 4. Letπ ∈Πbeanϵ-K(cid:93) KTpolicyof (InitPb). Thenπ isaconstrainedC ϵ-NEwhereC ≲ D +1.
NE NE 1−γ
Proof. The proof of the lemma proceeds in two steps:
• Step 1. Weshowthatifπ isanO(ϵ)-K(cid:93) KTpolicyof (InitPb), thenforalli∈N, π isanO(ϵ)-K(cid:93) KTpolicy
i
of (PlayerPb(π )).
−i
• Step2. Weconcludethateachplayercannotsignificantlyimproveitspolicyπ whilestayingwithinΠi(π )
i c −i
which means π is a constrained O(ϵ)-NE.
We provide a proof of each one of the steps successively.Independent Learning in Constrained Markov Potential Games
• Step 1: Let λ ≥ 0 be a dual variable such that (π,λ) is an ϵ-K(cid:93) KT pair of (InitPb), and let i ∈ N
(cid:93)
be arbitrary. We show that (π ,λ) is an ϵ-KKT pair of (PlayerPb(π )) by checking that the respective
i −i
(cid:93)
KKT conditions hold. For dual and exact primal feasibility, as well as complementary slackness, this is
immediatesincetheconditionsareequivalentfor(InitPb)and(PlayerPb(π )). ForvariationalLagrangian
−i
stationarity, observe that
max (cid:10) π −π′,∇ L (π ,λ)(cid:11) = max ⟨π−(π′,π ),∇ L(π,λ)⟩
π′∈Πi
i i πi π−i i
π′∈Πi
i −i π
i i
≤max⟨π−π′,∇ L(π,λ)⟩
π
π′∈Π
≤ϵ,
wherethefirstequalityisdueto∇ L(π,λ)=∇ L (π ,λ)andthefactthatalltermsexceptforπ −π′
πi πi π−i i i i
vanish in the first argument of the scalar product. The second inequality is because (π′,π )∈Π, and the
i −i
final step is by Lagrangian stationarity of π for (InitPb).
• Step 2: Let i ∈ N and consider the MDP M , λ ≥ 0, with state space S, action space A , probability
λ i
transition kernel P , reward r , and initial distribution µ where
λ λ
P (s′ |s,a ):=E [P(s′ |s,(a ,a ))]
λ i a−i∼π−i((ai,·)|s) i −i
r (s,a ):=E [r (s,(a ,a ))+λc(s,(a ,a ))].
λ i a−i∼π−i((ai,·)|s) i i −i i −i
Observe that L (π ,λ) is the value function associated to the policy π in the MDP M , and hence
π−i i i λ
gradient domination holds (Agarwal et al., 2021), i.e., we have
L (π ,λ)− min L (π˜ ,λ)≤ D max (cid:10) π −π′,∇ L (π ,λ)(cid:11) ,
π−i i π˜i∈Πi π−i i 1−γ π i′∈Πi i i πi π−i i
where D is the distribution mismatch coefficient, supposed to be finite. Using Proposition 2 in Appendix D
for C =0, and using the definition of the playerwise primal optimum, see (PlayerPb(π )), we then get
1 −i
(cid:18) (cid:19)
D
V (π)− min V (π∗,π )≤ +1 ϵ.
ri π i∗∈Πci(π−i) i i −i 1−γ
Since additionally we have exact primal feasibility of π for (InitPb), the result follows by definition of the
constrained ϵ-NE.
Finally, we put together above lemmas to prove the main theorem.
Proof of Theorem 1. Let ϵ¯2 = LΦϵ2 . Then with K = O(cid:0) ϵ−2(cid:1) , and T = O(cid:0) ϵ−2(cid:1) , Lemma 1 and Lemma 2
C2 C2
imply that there exists 0 ≤ t ≤ TKK −T 1NE such that E(cid:2)(cid:13) (cid:13)π(t+1)−π(t)(cid:13) (cid:13)(cid:3) ≤ ϵ . We use Lemma 3 to conclude
CKKTCNE
that π(t+1) is a ϵ/C -K(cid:93) KT policy of (InitPb). Then, by Lemma 4, π(t+1) is a constrained ϵ-NE. The total
NE
iteration complexity is bounded by T ·K
=O(cid:0) ϵ−4(cid:1)
.
B.2 Proof of Theorem 2 — Finite Sample Case
Moving on to the stochastic setting, we first restate Theorem 2.
Theorem 2. Let Assumptions 1 and 2 hold, and let D (as in Theorem 1) be finite. Then, for any ϵ>0, after
running iProxCMPG based on finite sample estimates (see Algorithm 2) for suitably chosen η,β,ξ,T,K,B, and
{(ν ,δ ,ρ )} , there exists t ∈ [T], such that in expectation, π(t) is a constrained ϵ-NE. The total sample
k k k 0≤k≤K
complexity is given by O˜(cid:0) ϵ−7(cid:1) where O˜(·) hides polynomial dependencies in m,S,A ,D,1−γ, and ζ, as well
max
as logarithmic dependencies in 1/ϵ.
Similartothedeterministiccase, webeginbyprovingtheguaranteesprovidedbytheinnerloopofAlgorithm2.Philip Jordan, Anas Barakat, Niao He
√
Lemma 5. Let Assumption 1 hold, let ϵ¯>0 and set β =ϵ¯,η = 1 ,ξ =ϵ¯ 2η. Then, there exist K =O˜(cid:0) ϵ¯−3(cid:1) ,
B
=O˜(cid:0) ϵ¯−2(cid:1)
, and suitable choices of {(ν ,δ ,ρ )} , such
th2L aΦ
t lines 4-11 of Algorithm 2 guarantee that for
k k k 0≤k≤K
any t∈[T −1],
(cid:104) (cid:105)
E Φ (π(t+1))−Φ (π˜(t+1)) ≤ϵ¯2,
η,π(t) η,π(t)
(15)
(cid:104) (cid:105)
E V (π(t+1)) ≤α,
c
where π˜(t+1) denotes the unique optimal solution to (ProxPb(η,π(t))).
Proof. TheresultfollowssimilarlyasforLemma1inthedeterministiccase. Wehenceonlypointoutdifferences.
Inordertoensureboundednormsofgradientestimates,weuseξ-greedypolicies. Then,accordingtoLemma10,
the second moment of value and constraint gradient estimates is bounded by O(1/ξ). The concentration result
shown in Lemma 11 ensures that constraint value estimates follow a sub-exponential distribution. Therefore,
Assumption 3, see Appendix C on guarantees for our subroutine, is satisfied. We can thus apply the respective
Theorem 3 for optimizing over Πξ, and with µ = L Φ and M2 ≲ max(cid:8) M G2 +µ2 G∆4,M F2 +µ2 F∆4(cid:9) ≲ ξ2 (4 1A −2 m γa )x 4 +
L2diam(Π)4. After plugging bounds on L , and diam(Π) from Lemma 8, and choosing K and B as stated,
Φ Φ
Theorem 3 implies the desired bounds via the same arguments as in the proof of Lemma 1.
Next, we analyze the convergence of our main proximal-point method, Algorithm 2. More concretely, we
bound the required sample complexity for ensuring that for some ϵ > 0, there exist iterates π(t),π(t+1) such
(cid:13) (cid:13)
that (cid:13)π(t)−π(t+1)(cid:13) = O(ϵ). In the following, we will then prove that this implies convergence to a con-
strained O(ϵ)-NE.
Similarly to the deterministic case, we next determine the number of updates needed until convergence in the
following sense. The next lemma is analogous to its deterministic counterpart Lemma 2.
Lemma 6. Let ϵ > 0 and set η = 1 . Suppose K is chosen such that the guarantee from Lemma 5 holds
2LΦ
for ϵ¯2 = ϵ2. Then after T = 4ηΦmax iterations of the outer loop of Algorithm 2 where Φ is an upper bound of
4η ϵ2 max
the potential function (i.e., ∀π ∈Π,Φ(π)≤Φ ), there exists 0≤t≤T −1 such that
E(cid:2) ∥π(t+1)−π(t)∥(cid:3)
≤ϵ.
max
Proof. The proof follows the same lines as the proof of Lemma 2 upon replacing Lemma 1 by Lemma 5. We do
not reproduce it here for conciseness.
Next,asintheexactgradientscase,weaimtoprovethattheevent∥π(t+1)−π(t)∥≤ϵimpliesNash-gap(π(t+1))=
O(ϵ), in order to argue that
E(cid:2) ∥π(t+1)−π(t)∥(cid:3)
≤ϵ implies
E(cid:2) Nash-gap(π(t+1))(cid:3)
=O(ϵ).
Recall that in Lemma 3 we have already shown ∥π(t+1) − π(t)∥ ≤ ϵ to imply that π(t+1) is a C ϵ-K(cid:93) KT
KKT
(cid:93)
policy of (InitPb) which equivalently holds in the stochastic ξ-greedy setting. Arguing that a ϵ-KKT policy is
a constrained O(ϵ)-NE however requires an adapted proof, since here in each iteration we solve the subproblem
overΠξ insteadofΠ,i.e.,theK(cid:93) KTconditionsholdw.r.t.Πξ. ThefollowinglemmaisanadjustmentofLemma4
for this fact.
Lemma 7. Let π ∈ Πξ be an ϵ-K(cid:93) KT policy of (InitPb) (where K(cid:93) KT are w.r.t. Πξ) and ξ = ϵ. Then π is a
√
constrained Cˆ NEϵ-NE where Cˆ NE ≲ 1−D γ + m (1S −A γm )4a .5xD +1.
Proof. We divide the proof into two steps:
• Step 1: Analogouslytostep 1ofLemma4, onecanshow that(π ,λ)isan ϵ-K(cid:93) KTpairof (PlayerPb(π )).
i −i
• Step 2: Let i ∈ N and consider the MDP M˜ (for λ ≥ 0) with state space S, action space A , transition
λ i
probability kernel P˜, reward r˜ , discount factor γ, and initial distribution µ where
λ
P˜(s′ |s,a ):=E [P(s′ |s,(a ,a ))] ,
i a−i∼π−i(·|s) i −i
r˜ (s,a ):=E [r (s,(a ,a ))+λc(s,(a ,a ))].
λ i a−i∼π−i(·|s) i i −i i −iIndependent Learning in Constrained Markov Potential Games
Observe that L (π ,λ) is the value function associated to the policy π for M˜ , and hence gradient
π−i i i λ
domination holds (Agarwal et al., 2021). In our particular case of π being a ξ-greedy policy, we can also
show a similar inequality, even w.r.t. the non-ξ-greedy optimum. Let
πˆ :=argmax(cid:10) π −π′,∇ L (π ,λ)(cid:11) ,
i i i πi π−i i
π′∈Πi
i
πˆξ :=argmax(cid:10) π −π′,∇ L (π ,λ)(cid:11) .
i i i πi π−i i
π′∈Πi,ξ
i
Then, we have
L (π ,λ)− min L (π∗,λ)
π−i i
π∗∈Πi
π−i i
i
≤ 1
(cid:13)
(cid:13) (cid:13)d µπ
i∗,π−i(cid:13)
(cid:13) (cid:13) (cid:10) π −πˆ ∇ L (π ,λ)(cid:11)
1−γ (cid:13) µ (cid:13) i i πi π−i i
(cid:13) (cid:13)
∞
=
1
(cid:13)
(cid:13) (cid:13)d µπ
i∗,π−i(cid:13)
(cid:13)
(cid:13)
(cid:68)
π −πˆξ,∇ L (π
,λ)(cid:69) +(cid:68)
πˆξ−πˆ ,∇ L (π
,λ)(cid:69)
.
1−γ (cid:13) µ (cid:13) i i πi π−i i i i πi π−i i
(cid:13) (cid:13)
∞
We further bound the last term above as follows
(cid:68) πˆξ−πˆ ,∇ L (π ,λ)(cid:69) = max (cid:68) πξ,∇ L (π ,λ)(cid:69) − max (cid:10) π ,∇ L (π ,λ)(cid:11)
i i πi π−i i
π iξ∈Πi,ξ
i πi π−i i
πi∈Πi
i πi π−i i
(a) √ (cid:13) (cid:13)
≤ ξ S(cid:13)∇ πiL π−i(π i,λ)(cid:13)
(b) √
≤ ξ S(1+λ)M .
c
In the above inequalities, (a) follows from using Lemma 9 to obtain
max (cid:68) πξ,∇ L (π ,λ)(cid:69) ≤ max (cid:10) π ,∇ L (π ,λ)(cid:11) +(cid:88) ξ [∇ L (π ,λ)](a |s).
π iξ∈Πi,ξ i πi π−i i πi∈Πi i πi π−i i ai,s A i πi π−i i i
√
Using the fact that for any x∈Rd, ∥x∥ ≤ d∥x∥ , we further get
1 2
(cid:88) ξ ξ ξ (cid:112) √
[∇ L (π ,λ)](a |s)= ∥∇ L (π ,λ)∥ ≤ A S∥∇ L (π ,λ)∥≤ζ S∥∇ L (π ,λ)∥.
A πi π−i i i A πi π−i i 1 A i πi π−i i πi π−i i
i i i
ai,s
The bound used in (b) follows from Lipschitz continuity, see Lemma 8. We conclude that
L (π ,λ)− min L (π∗,λ)≤ 1
(cid:13)
(cid:13) (cid:13)dπ
µi∗,π−i(cid:13)
(cid:13) (cid:13) (cid:104)(cid:10) π −πˆ ∇ L (π ,λ)(cid:11) +ξ√ S(1+λ)M (cid:105) . (16)
π−i i π i∗∈Πi π−i i 1−γ (cid:13) (cid:13) µ (cid:13) (cid:13)
∞
i i πi π−i i c
Applying Proposition 2, see Appendix D.2, with ξ = ϵ, bounding the distribution mismatch coefficient by
D, and using the definition of the playerwise primal optimum, see (PlayerPb(π )), we then get
−i
(cid:32) √ (cid:33)
D (1+λ) SM D
V (π)− min V (π∗,π )≤ + c +1 ϵ
i π i∗∈Πci(π−i) i i −i 1−γ 1−γ
(cid:32) √ (cid:33)
D m SA D
≤ + max +1 ϵ.
1−γ (1−γ)4.5
whereforthesecondinequalityweuse(12)andourboundsonM andL fromLemma8. Sinceadditionally,
c Φ
(cid:93)
we have exact primal feasibility of π for (InitPb) by the KKT conditions, the result follows by definition of
a constrained ϵ-NE.Philip Jordan, Anas Barakat, Niao He
Finally, we put together above lemmas to prove our main theorem in the stochastic setting.
Proof of Theorem 2. Let ϵ¯2 = LΦϵ2 . Then with K = O˜(cid:0) ϵ−3(cid:1) , B = O˜(cid:0) ϵ−2(cid:1) , and T = O(cid:0) ϵ−2(cid:1) , Lemma 1
C2 Cˆ2
and Lemma 2 imply that there eK xK iT stsNE t ∈ [T −1] such that E(cid:2)(cid:13) (cid:13)π(t+1)−π(t)(cid:13) (cid:13)(cid:3) ≤ ϵ . We use Lemma 3
CKKTCˆ
NE
to conclude that π(t+1) is a ϵ/Cˆ -K(cid:93) KT policy of (InitPb) in expectation. Then, by Lemma 4, π(t+1) is a
NE
constrained ϵ-NE in expectation. The total sample complexity is bounded by T ·K·B
=O˜(cid:0) ϵ−7(cid:1)
.
B.3 Other Technical Lemmas
The next lemma collects standard regularity properties of the value and potential functions.
Lemma 8. The following statements hold true.
√
1. The functions Φ and V are M -Lipschitz continuous over Π with M = mAmax. This immediately implies
c c c (1−γ)2
that ∥∇Φ(π)∥≤M and ∥∇V (π)∥≤M , for all π ∈Π.
c c c
2. For any i ∈ N and any π ∈ Π , the function V (·,π ) is L -smooth with L = 2γAi and hence L -
−i −i ri −i i i (1−γ)3 i
weakly convex.
3. The functions Φ and V are L -smooth with L =m·max L = 2mγAmax and hence L -weakly convex.
c Φ Φ i i (1−γ)3 Φ
4. For η = 1 and any π′ ∈Π, the regularized function Φ (π)=Φ(π)+L ∥π−π′∥2 is L -strongly convex
and the f2 uL nΦ ctions Φ ,Vc are both 2L -smooth. η,π′ Φ Φ
η,π′ η,π′ Φ
5. For any λ ∈ R,π′ ∈ Π and η = 1 , L(·,λ) = Φ(·) + λV (·) is L (1 + λ)-smooth, and L (·,λ) =
Φ (·)+λVc (·) is 2L (1+λ)-sm2L ooΦ th. Hence L (·,λ) isc also L (Φ 1+λ)-strongly convex. η,π′
η,π′ η,π′ Φ η,π′ Φ
Proof. Item 2 has been proved in Agarwal et al. (2021), Lemma D.3. Item 3 has been reported in Leonardos
et al. (2022), Lemma D.4. Item 4 immediately follows from item 3. We now prove item 5 for L, the result
for L follows similarly. Using the definition of the Lagrangian and the triangle inequality, for any λ∈R and
η,π′
π,π′ ∈Π,
∥∇ L(π,λ)−∇ L(π′,λ)∥≤∥∇Φ(π)−∇Φ(π′)∥+λ∥∇V (π)−∇V (π′)∥
π π c c
≤L ∥π−π′∥+λL ∥π−π′∥
Φ Φ
≤L (1+λ)∥π−π′∥.
Φ
To show item 1, Lipschitz continuity of Φ and V , observe that for any i ∈ N, π ∈ Π and π′ ∈ Πi, by using
c i
Lemma 32 of Zhang et al. (2022a) in the second step, we have
|Φ(π ,π )−Φ(π′,π )|=|V (π ,π )−V (π′,π )|
i −i i −i ri i −i ri i −i
1
≤ max∥π (·|s)−π′(·|s)∥
(1−γ)2 s∈S i i 1
√
A
≤ i max∥π (·|s)−π′(·|s)∥
(1−γ)2 s∈S i i 2
√
A
≤ i ∥π −π′∥
(1−γ)2 i i 2
√
where in the third step we use the fact that for any x∈Rd, ∥x∥ ≤ d∥x∥ . Then, the following decomposition
1 2Independent Learning in Constrained Markov Potential Games
yields the result: For any π,π′ ∈Π,
(cid:12) (cid:12)
(cid:12)(cid:88) (cid:12)
|Φ(π)−Φ(π′)|=(cid:12) Φ(π′,...,π′ ,π ,π ,...,π )−Φ(π′,...,π′ ,π′,π ,...,π )(cid:12)
(cid:12) 1 i−1 i i+1 m 1 i−1 i i+1 m (cid:12)
(cid:12) (cid:12)
i∈N
≤ (cid:88)(cid:12) (cid:12)Φ(π 1′,...,π i′ −1,π i,π i+1,...,π m)−Φ(π 1′,...,π i′ −1,π i′,π i+1,...,π m)(cid:12) (cid:12)
i∈N
1 (cid:88)(cid:112)
≤ A ∥π −π′∥
(1−γ)2 i i i
i∈N
√
mA
≤ max ∥π−π′∥ ,
(1−γ)2
whereinthesecondinequalityweapplytheaboveresultforplayerwisedeviations, andthelaststepisagaindue
√
to the fact that for any x∈Rd, ∥x∥ ≤ d∥x∥ . The result follows similarly for V .
1 2 c
The next lemma is an immediate result showing that any ξ-greedy playerwise policy π ∈Πi,ξ (see definition in
i
the main part p. 7 which defines this set as a set of lower bounded policies away from zero) can be represented
as a convex combination of a uniform distribution over the action space A and a policy π ∈Πi.
i i
Lemma 9. For any ξ >0, i∈N,
Πi,ξ ⊆(cid:8) π ∈Πi |∃π′ ∈Πi,∀a ∈A ,∀s∈S : π (a |s)=ξ/A +(1−ξ)π′(a |s)(cid:9) .
i i i i i i i i i
Proof. Let ξ >0, i∈N and let πξ ∈Πi,ξ. Then for all a ∈A and s∈S, set
i i i
πξ(a |s)−ξ/A
π (a |s):= i i i.
i i 1−ξ
Indeed π ∈Πi, since for all a ∈A and s∈S, we have π (a |s)≥0 due to πξ(a |s)≥ξ and
i i i i i i i
(cid:32) (cid:33)
(cid:88) 1 (cid:88) (cid:88)
π (a |s)= πξ(a |s)− ξ/A =1.
i i 1−ξ i i i
ai∈Ai ai∈Ai ai∈Ai
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=1 =ξ
Thefollowinglemmashowsthattheestimatorsweusefortheplayerwisepolicygradientsareunbiasedandenjoy
a bounded variance.
Lemma 10 (Daskalakis et al. (2020); Leonardos et al. (2022)). For any ξ >0 and π ∈Πξ, we have
(cid:104) (cid:105)
E ∇ˆVri(π) =∇ V (π)=∇ Φ(π),
π πi πi ri πi
(cid:20)(cid:13) (cid:13)2(cid:21) 24A2
E (cid:13)∇ˆVri(π)(cid:13) ≤ max ,
π (cid:13) πi (cid:13) ξ(1−γ)4
(cid:104) (cid:105) 1
E Vˆ (π)2 ≤ .
π c (1−γ)2
The same holds for ∇ˆVc(π) w.r.t. ∇ V (π).
πi πi c
Finally, the following lemma shows that our constraint function estimates concentrate around their mean.
Lemma 11. For π ∈ Πξ, let Vˆ(1),...,Vˆ(B) be independent copies of Vˆ (π), and let Vˆ := 1 (cid:80)B Vˆ(i). Then
c c c c B i=1 c
there exists C >0 such that for any λ≥0,
P(cid:18)(cid:12) (cid:12)Vˆ −V (π)(cid:12) (cid:12)> √λ (cid:19) ≤4exp(−C(1−γ)λ)+2exp(cid:0) −C2(1−γ)2λ2(cid:1) .
(cid:12) c c (cid:12)
BPhilip Jordan, Anas Barakat, Niao He
Proof. For i ∈ [B], we decompose Vˆ(i) = cˆ(i) +Vˆ(i) where cˆ(i) is the cost incurred at step 0, and let cˆ :=
c 0 c,≥1 0 0
1 (cid:80)B c(i), Vˆ := 1 (cid:80)B Vˆ(i) . Since 0 ≤ cˆ ≤ 1, by Hoeffding’s inequality, there exists C > 0 such that
B i=1 0 c,≥1 B i=1 c,≥1 0 0
for any λ≥0,
(cid:18) (cid:19)
P |cˆ −E[cˆ ]|> √λ ≤2exp(cid:0) −C λ2(cid:1) . (17)
0 0 0
2 B
Moreover, note that since for all s ∈ S,a ∈ A, 0 ≤ c(s,a) ≤ 1, we have that for all i ∈ [B], V(i) ≤ T(i)
c,≥1 e
where T(i) is the stopping time of the respective episode and an independent copy of T . Assuming κ =
e e s,a
min κ = 1−γ for all s ∈ S,a ∈ A, T follows a geometric distribution with parameter 1−γ. By
s∈S,a∈A s,a e
definition of the geometric distribution and elementary computations, we get for any λ≥0,
(cid:18) (cid:19)
1−γ
P(T ≥λ)≤γ⌈λ⌉ ≤exp(⌈λ⌉logγ)≤exp −⌈λ⌉ ≤exp(−(1−γ)λ/3)
e 3
which by a standard characterization of sub-exponential random variables, see Proposition 2.7.1 in Vershynin
(2018), implies that T , and therefore also V(i) for all i ∈ [B] are sub-exponential. Moreover, by the so-
e c,≥1
called centering lemma for sub-exponential distributions, see section 2.7 in Vershynin (2018), for any random
variable X that is sub-exponential with parameter σ, there exists an absolute constant c such that X −E[X]
is sub-exponential with parameter cσ. Thus for all i ∈ [B], V(i) −E[Vˆ(i) ] is sub-exponential with parameter
c,≥1 c,≥1
in O(1/(1−γ)). Then, we apply Bernstein’s inequality, see Theorem 2.8.1 in Vershynin (2018), to show that
there exist C ,C >0 such that for any λ≥0,
1 2
P(cid:18)(cid:12) (cid:12)Vˆ −E[Vˆ ](cid:12) (cid:12)> √λ (cid:19) ≤2exp(−C (1−γ)λ)+2exp(cid:0) −C2(1−γ)2λ2(cid:1) . (18)
(cid:12) c,≥1 c,≥1 (cid:12) 1 2
2 B
Finally, using a union bound we combine (17) and (18) to get the desired bound.
C STRONGLY CONVEX STOCHASTIC OPTIMIZATION WITH STRONGLY
CONVEX EXPECTATION CONSTRAINT
In this section, we describe a stochastic gradient switching algorithm for stochastic constrained optimization
under expectation constraints. Up to the modification of using a relaxed constraint (which is crucial for
enabling its independent implementation), our algorithm and analysis follow the Cooperative Stochastic Ap-
proximation (CSA) algorithm presented in Lan and Zhou (2020) which is inspired by Polyak’s subgradient
method(Polyak,1967). LanandZhou(2020)hintatthefactthata1/K convergencerateoftheCSAalgorithm
can be shown in the case of strongly convex objective and under expectation constraints. Here we explicitly
carry out this analysis by deriving a result in expectation and under a somewhat weaker assumption on the
distribution of the constraint function estimates.
Let X ⊆ Rd be a convex and compact set with diameter ∆ := max ∥x−x′∥. Suppose θ are random
x,x′∈X
vectors supported on Θ⊂Rp, and let F :X×Θ→R,G:X×Θ→R be functions such that F(·,θ) and G(·,θ)
are µ and µ -weakly convex, respectively. For any x′ ∈ X, we define F (x,θ) := F(x,θ)+µ ∥x−x′∥2
F G µ,x′ F
and G (x,θ) := G(x,θ)+µ ∥x−x′∥2. Let f(x) := E [F(x,θ)],g(x) := E [G(x,θ)] (where expectations are
µ,x′ G θ θ
supposed to be well-defined and finite) and f (x) := f(x)+µ ∥x−x′∥2,g (x) := g(x)+µ ∥x−x′∥2 for
µ,x′ F µ,x′ G
every x∈X.
The problem we aim to solve9 is given by
minf (x):=E [F (x,θ)]
µ,x′ θ µ,x′
x∈X (19)
s.t. g (x):=E [G (x,θ)]≤0.
µ,x′ θ µ,x′
Recall that such a problem needs to be solved at each time step in our iProxCMPG algorithm. The point x′ is
arbitrarily fixed throughout the rest of this section.
Suppose we are only given access to first-order information of f ,g and zeroth-order information of g via
µ,x′ µ,x′
a stochastic oracle that outputs unbiased and bounded-variance estimates.
9Notethatthefinalguaranteeswewillobtainareactuallyintermsofarelaxedconstraintsatisfactionbound. Thisis
due to our modification of the original CSA algorithm.Independent Learning in Constrained Markov Potential Games
Assumption 3. For every x ∈ X, the estimators F′ (x,θ),G′ (x,θ) and G(x,θ) are unbiased, i.e.,
µ,x′ µ,x′
E (cid:2) F′ (x,θ)(cid:3) = ∇f (x),E (cid:2) G′ (x,θ)(cid:3) = ∇g (x) and E [G(x,θ)] = g(x). Moreover, there ex-
θ µ,x′ µ,x′ θ µ,x′ µ,x′ θ
ist M ,M >0 such that
F G
(cid:104) (cid:105) (cid:104) (cid:105)
E ∥F′(x,θ)∥2 ≤M2 ; E ∥G′(x,θ)∥2 ≤M2 .
θ F θ G
Furthermore, we suppose to have access to independent unbiased estimators Gˆ(1),...,Gˆ(J) of G(x,·) for which
there exists σ >0 such that for any λ≥0, it holds that
P (cid:16) |Gˆ−g(x)|>λ/√ J(cid:17) ≤4exp(−λ/σ)+2exp(cid:0) −λ2/σ2(cid:1) , (20)
θ
where Gˆ := 1 (cid:80)J Gˆ(j).
J j=1
It can be easily seen that Assumption 3 also implies existence of M˜ ,M˜ such that
F G
E θ(cid:104)(cid:13) (cid:13)F µ′ ,x′(x,θ)(cid:13) (cid:13)2(cid:105) ≤2E θ(cid:104) ∥F′(x,θ)∥2(cid:105) +2µ2
F
∥x−x′∥4 ≤2M F2 +2µ2 F∆4 =:M˜ F2,
E θ(cid:104)(cid:13) (cid:13)G′ µ,x′(x,θ)(cid:13) (cid:13)2(cid:105) ≤2E θ(cid:104) ∥G′(x,θ)∥2(cid:105) +2µ2 G∥x−x′∥4 ≤2M G2 +2µ2 G∆4 =:M˜ G2.
Remark 5. Notice that the concentration requirement of (20) is relaxed compared to the sub-Gaussian assump-
tion made in Lan and Zhou (2020) which is too strong to hold in our case. We refer the reader to Lemma 11
where we prove that this weaker tail bound assumption holds for our constraint function estimates.
C.1 A Primal Gradient Switching Algorithm
Algorithm3isdesignedasaprimalalgorithmthatswitchesbetweentakingastepalongtheobjectiveorconstraint
gradient, depending on whether the constraint is currently (estimated to be) satisfied or not.
Algorithm 3 CSA (adapted from Lan and Zhou (2020))
1: initialization: x ∈X s.t. g(x )≤ϵ and {δ } ,{ν } ,{ρ } ,s∈[N]
1 1 k k∈[N] k k∈[N] k k∈[N]
2: for k =1,...,N −1 do
3: sample Gˆ(1),...,Gˆ(J) from G(x ,·) and set Gˆ = 1 (cid:80)J Gˆ(j)
k k k k J j=1 k
(cid:40) P (cid:2) x −ν F′ (x ,θ )(cid:3) if Gˆ ≤δ
4: x k+1 = PX (cid:2) xk −νk Gµ ′,x′ (xk ,θk )(cid:3) elsek k
X k k µ,x′ k k
5: let B :={s≤k ≤N |Gˆ ≤δ }
s k k
6: output: x where kˆ =1 if B =∅ and otherwise sampled s.t. for k ∈B , P(kˆ =k)=(cid:0)(cid:80) ρ (cid:1)−1 ρ
kˆ s s k∈Bs k k
In the analysis, we will denote M :={s≤k ≤N |k ̸∈B } and B :=B , M:=M .
s s 1 1
We point out the following differences between Algorithm 3 and the original CSA algorithm, see Lan and Zhou
(2020), Algorithm 1.
(a) We relax the switching condition in line 4 by using an estimate of g(x ) instead of g (x ) if we were to
k µ,x′ k
exactlyusethealgorithmproposedinLanandZhou(2020). Thismodificationiscrucialforourapplication
as subroutine of an independent learning algorithm, as described in the proof of Lemma 1, see section B.1.
As a result, compared to Lan and Zhou (2020), we get a weaker guarantee in terms of constraint violation
which however is still sufficient for our purposes.
(b) Instead of constructing the output as a ρ -weighted average over iterates x , we sample an iterate from
k k
a ρ -weighted distribution, see line 6. This is because our relaxed constraint function g is not necessarily
k
convex (unlike g ) and hence we cannot easily bound the constraint value at an average over iterates.
µ,x′Philip Jordan, Anas Barakat, Niao He
C.2 Convergence and Sample Complexity Guarantee
ThefollowinganalysisusesthetechniquespresentedinLanandZhou(2020)appliedtothestronglyconvexcase
with expectation constraint, under our modified Assumption 3 and Algorithm 3. The proofs follow along the
same lines, we highlight differences when appropriate.
First, we establish a basic recursion about CSA iterates that will be used repeatedly throughout the rest of the
analysis.
Proposition 1. For any s∈[N], x∈X, and a as defined by (21), it holds that
s
(cid:88) (cid:88)
ρ (G (x ,θ )−G (x,θ ))+ ρ (F (x ,θ )−F (x,θ ))
k µ,x′ k k µ,x′ k k µ,x′ k k µ,x′ k
k∈Ms k∈Bs
≤(1−a s)∆2+ 1
2
(cid:88) ρ kν k(cid:13) (cid:13)F µ′ ,x′(x k,θ k)(cid:13) (cid:13)2 + 21 (cid:88) ρ kν k(cid:13) (cid:13)G′ µ,x′(x k,θ k)(cid:13) (cid:13)2 .
k∈Bs k∈Bs
Proof. Let s∈[N] and k ∈B . Then, by non-expansiveness of the projection P and strong convexity,
s X
∥x k+1−x∥2 ≤∥x k−x∥2−ν k(cid:10) F µ′ ,x′(x k,θ k),x k−x(cid:11) + 21 ν k2(cid:13) (cid:13)F µ′ ,x′(x k,θ k)(cid:13) (cid:13)2
≤∥x k−x∥2−ν k(cid:104) F µ,x′(x k,θ k)−F µ,x′(x,θ k)+ µ 2F ∥x k−x∥2(cid:105) + 21 ν k2(cid:13) (cid:13)F µ′ ,x′(x k,θ k)(cid:13) (cid:13)2
≤(cid:16) 1− ν k 2µ F(cid:17) ∥x k−x∥2−ν k[F µ,x′(x k,θ k)−F µ,x′(x,θ k)]+ 21 ν k2(cid:13) (cid:13)F µ′ ,x′(x k,θ k)(cid:13) (cid:13)2 .
Similarly, if k ∈M ,
s
∥x k+1−x∥2 ≤(cid:16) 1− ν k 2µ G(cid:17) ∥x k−x∥2−ν k[G µ,x′(x k,θ k)−G µ,x′(x,θ k)]+ 21 ν k2(cid:13) (cid:13)G′ µ,x′(x k,θ k)(cid:13) (cid:13)2 .
After defining
(cid:40) (cid:40)
µ ν if k ∈B 1 if k =1 ν
a = F k ; A = ; ρ = k ; (21)
k µ ν if k ∈M k (1−a )A if k ≥2 k A
G k k k−1 k
the result follows by application of Lemma 21, Lan and Zhou (2020).
Thenextlemmaprovidesaconditionon{ν ,δ ,ρ } thatguaranteeseitherlowregretintermsofobjective
k k k s≤k≤N
value or that a large number of iterates satisfy the constraint with high probability.
Lemma 12. Let x∗ be an optimal solution of (19). If for some s∈[N] and λ≥0,
N −s+1 min ρ δ >(1−a )∆2+ 1 (cid:88) ρ ν M˜2 + 1 (cid:88) ρ ν M˜2 + √λ (cid:88) ρ , (22)
2 k∈Ms k k s 2
k∈Ms
k k G 2
k∈Bs
k k F J
k∈Ms
k
then one of the following statements hold,
(a) P (|B |≥(N −s+1)/2)≥1−|M |(cid:0) 4exp(−λ/σ)+2exp(cid:0) −λ2/σ2(cid:1)(cid:1) , or,
θ s s
(b) (cid:80) ρ (f (x )−f (x∗))≤0.
k∈Bs k µ,x′ k µ,x′
Note that unlike in Lan and Zhou (2020), due to our modified choice of Algorithm 3’s output, well-definedness
of x does not require B ̸=∅.
kˆ s
Proof. In Proposition 1, set x=x∗, take expectation w.r.t. θ on both sides, and apply E θ(cid:13) (cid:13)F µ′ ,x′(x,θ)(cid:13) (cid:13)2 ≤M˜ F2,
E θ(cid:13) (cid:13)G′ µ,x′(x,θ)(cid:13) (cid:13)2 ≤M˜ G2. Then,
(cid:88) (cid:88)
ρ (g (x )−g (x∗))+ ρ (f (x )−f (x∗))
k µ,x′ k µ,x′ k µ,x′ k µ,x′
k∈Ms k∈Bs
(23)
≤(1−a )∆2+ 1 (cid:88) ρ ν M˜2 + 1 (cid:88) ρ ν M˜2.
s 2 k k G 2 k k F
k∈Ms k∈BsIndependent Learning in Constrained Markov Potential Games
If (cid:80) ρ (f (x )−f (x∗)) ≤ 0, then (b) holds. Otherwise, we make three observations. First, we have
thatk g∈Bs (xk ∗)µ ≤,x′
0.
k Seconµ d, ,x′
it holds that g(x ) ≤ g (x ). Third, for k ∈ M , by Assumption 3 and due to
µ,x′ k µ,x′ k s
Gˆ >δ , we get
k k
(cid:18) (cid:19)
P g(x )<δ − √λ ≤4exp(−λ/σ)+2exp(cid:0) −λ2/σ2(cid:1) . (24)
θ k k
J
By a union bound this inequality holds for all k ∈ M with probability at most
s
|M
|(cid:0) 4exp(−λ/σ)+2exp(cid:0) −λ2/σ2(cid:1)(cid:1)
. Combining these three observations with (23) yields that with
s
probability at least 1−|M
|(cid:0) 4exp(−λ/σ)+2exp(cid:0) −λ2/σ2(cid:1)(cid:1)
, it holds that
s
(cid:88) ρ δ ≤(1−a )∆2+ 1 (cid:88) ρ ν M˜2 + 1 (cid:88) ρ ν M˜2 + √λ .
k k s 2 k k G 2 k k F J
k∈Ms k∈Ms k∈Bs
Above inequality then implies (a) because if |B | < (N −s+1)/2, i.e., |M | ≥ (N −s+1)/2, then condition
s s
(22) implies that
(cid:88) ρ δ ≥ N −s+1 min ρ δ >(1−a )∆2+ 1 (cid:88) ρ ν M˜2 + 1 (cid:88) ρ ν M˜2 + √λ (cid:88) ρ ,
k∈Ms
k k 2 k∈Ms k k s 2
k∈Ms
k k G 2
k∈Bs
k k F J
k∈Ms
k
which is a contradiction.
Next, we state and prove the main guarantees provided by Algorithm 3.
Theorem 3. UnderAssumption3, letϵ>0, supposex issuchthatg(x )≤ϵ, andletf >0suchthatforall
1 1 max
x∈X, 0≤f(x)≤f . Choose s=N/2,λ=σ2log(N2/(4f )), set M =max{M˜ ,M˜ }, µ=min{µ ,µ },
max max G F G F
and
(cid:40) 2 if k ∈B λ 1 (cid:18) 4∆2 16M2(cid:19) (cid:40) µ if k ∈B
ν = µF(k+1) ; δ = √ + + · F ;
k 2 if k ∈M k J 2k k µ2 µ if k ∈M
µG(k+1) G
(cid:40) (cid:40)
µ ν if k ∈B 1 if k =1 ν
a = F k ; A = ; ρ = k
k µ ν if k ∈M k (1−a )A if k ≥2 k A
G k k k−1 k
(cid:40) 64µ M2 (cid:112) 32∆2µ 32σµ (cid:41) (cid:26) 9λ2 32σµ (cid:27)
N =max F , F, F ; J =max , F .
µ2ϵ2 ϵ µϵ2 ϵ2 µϵ2
Then Algorithm 3 guarantees that
E(cid:2) f (x )−f (x∗)(cid:3) ≤ϵ2, (25)
µ,x′ kˆ µ,x′
E(cid:2)
g(x
)(cid:3)
≤ϵ. (26)
kˆ
Proof. First, we observe that for any k ∈M ,
s
(cid:104)√ (cid:105) (cid:90) ∞(cid:16) (cid:16)√ (cid:17)(cid:17)
E J(g(x )−δ ) = 1−P J(g(x )−δ )≤z dz
k k k k
0
(cid:90) 0 (cid:16)√ (cid:17)
− P J(g(x )−δ )≤z dz
k k
−∞ (27)
(cid:90) 0
≥−
4exp(z/σ)+2exp(cid:0) z2/σ2(cid:1)
dz
−∞
≥−6σ
where the first inequality is by (24). Therefore, we have E[g(x k)]≥δ k− √6σ. Moreover, by an argument similar
J
to our derivation in Lemma 11 but with Bernstein’s inequality applied to the sum (cid:0)(cid:80) ρ (cid:1)−1(cid:80) ρ Gˆ ,
k∈Ms k k∈Ms k k
(cid:32) (cid:33)
P (cid:88) ρ (cid:16) Gˆ −g(x )(cid:17) > λ (cid:88) ρ ≤4exp(−λ/σ)+2exp(cid:0) −λ2/σ2(cid:1) .
k k k (cid:112) k
J|M |
k∈Ms s k∈MsPhilip Jordan, Anas Barakat, Niao He
Therefore, following (27), we get
(cid:34) (cid:35)
(cid:88) (cid:88) 6σ (cid:88)
E ρ g(x ) ≥ ρ δ − ρ . (28)
k k k k (cid:112) k
J|M |
k∈Ms k∈Ms s k∈Ms
Next, we derive (25). Note that (22) holds for our choices of s,ν ,δ ,ρ . Then, if part (b) of Lemma 12 holds,
k k k
we have
E(cid:2) f(x )−f(x∗)(cid:3) =E (cid:104) E(cid:104) f(x )−f(x∗)|kˆ =k(cid:105)(cid:105)
kˆ kˆ k
(cid:32) (cid:33)−1
(cid:88) (cid:88)
≤ ρ ρ E[f(x )−f(x∗)]
k k k
k∈Bs k∈Bs
≤0.
Otherwise, if part (a) holds, then using above bound on E[g(x )] together with convexity of f , (23) and (28),
k µ,x′
it follows that
(cid:88)
ρ δ −
6σ (cid:88)
ρ +
(cid:88)
ρ
E(cid:2)
f (x )−f
(x∗)(cid:3)
k k (cid:112)
J|M |
k k µ,x′ kˆ µ,x′
k∈Ms s k∈Ms k∈Bs
≤ (cid:88) ρ E[g(x )]+ (cid:88) ρ E(cid:2) f (x )−f (x∗)(cid:3)
k k k µ,x′ kˆ µ,x′
k∈Ms k∈Bs
(cid:88) (cid:88)
≤ ρ E[g(x )]+ ρ E[f (x )−f (x∗)]
k k k µ,x′ k µ,x′
k∈Ms k∈Bs
≤(1−a )∆2+ 1 (cid:88) ρ ν M˜2 + 1 (cid:88) ρ ν M˜2.
s 2 k k G 2 k k F
k∈Ms k∈Bs
Denote by E the event that |B | ≥ (N −s+1)/2. Then, using the law of total expectation, our choice of
Bs s
λ=σ2log(N2/(4f )), ρ δ ≥0, and above inequality, we have
max k k
E(cid:2)
f(x
)−f(x∗)(cid:3)
kˆ
≤E(cid:2) f(x )−f(x∗)|E (cid:3) ·P(E ) +E(cid:2) f(x )−f(x∗)|E (cid:3) · P(cid:0) E (cid:1)
kˆ Bs Bs kˆ Bs Bs
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
≤1 ≤|Ms|(4exp(−λ/σ)+2exp(−λ2/σ2))
(cid:32) (cid:33)−1(cid:32) (cid:33)
≤ (cid:88) ρ (1−a )∆2+ 1 (cid:88) ρ ν M˜2 + 1 (cid:88) ρ ν M˜2 + 6σ (cid:88) ρ + 1
k s 2 k k G 2 k k F (cid:112) J|M | k N
k∈Bs k∈Ms k∈Bs s k∈Ms
≤(cid:18)
N −s+1 minρ
(cid:19)−1(cid:32)
(1−a )∆2+ 1 (cid:88) ρ ν M˜2 + 1 (cid:88) ρ ν M˜2 + 6σ (cid:88) ρ
(cid:33)
+ 1 .
2 k∈Bs k s 2
k∈Ms
k k G 2
k∈Bs
k k F (cid:112) J|M s|
k∈Ms
k N
In order to show the constraint violation bound, note that by a similar argument as (27), for any k ∈ B ,
s
E[g(x k)]≤δ k+ √6σ, and therefore
J
(cid:80)
E(cid:2) g(x )(cid:3) =E (cid:104) E(cid:104) g(x )|k =kˆ(cid:105)(cid:105) ≤E (cid:2) δ (cid:3) + √6σ = k∈Bsρ kδ k + √6σ .
kˆ kˆ k kˆ kˆ J (cid:80) k∈Bsρ k J
In order to derive the guarantees (25) and (26), we plug in the choices of ν ,δ ,a ,ρ ,N, and J stated in
k k k k
Theorem 3. Observing that for any s ≤ k ≤ N, we have A = 2 , that for any k ∈ B, we have ρ = 2k as
k k(k+1) k µFIndependent Learning in Constrained Markov Potential Games
well as ρ ν = 4 , and for any k ∈M, ρ = 2k,ρ ν = 4 .
k k µ2
F
k µG k k µ2
G
∆2+2Nµ−2M˜2 +2Nµ−2M˜2 + √2σN3/2µ−1
E(cid:2) f(x )−f(x∗)(cid:3) ≤ F F G G J G
kˆ N2/4·µ−1
F
∆2+4Nµ−2M2+ √2σN3/2µ−1
1
≤ J +
N2/4·µ−1 N
F
4µ ∆2 16µ µ−2M2 8σµ−1µ 1
≤ F + F + √ F +
N2 N JN N
≤ϵ2/4+ϵ2/4+ϵ2/4+ϵ2/4.
Moreover, for the constraint bound, it holds that
E(cid:2)
g(x
)(cid:3)
≤
(cid:80) k∈Bs(cid:0) 4∆2/k+16M2/µ2(cid:1)
+
√6σ
kˆ (cid:80)
k∈Bs2k/µ F J
8∆2µ 16M2µ 6σ
≤ F + F + √
N2 µ2N J
≤ϵ.
D BACKGROUND IN CONSTRAINED OPTIMIZATION AND A NOVEL
TECHNICAL LEMMA
Notation For any non-empty subset Y ⊂ Rd and any vector x ∈ Rd, the distance from x to the set Y is
defined as dist(x,Y):=inf ∥x−y∥ where ∥·∥ is the standard 2-norm of the Euclidean space Rd.
y∈Y
Inthissection,werecallsomeusefuldefinitionsforconstrainedoptimization. Inparticular,werecallthedefinition
of an approximate Karush-Kuhn-Tucker (KKT) point and a variation thereof. Then we prove a new technical
result that will be useful in our analysis.
D.1 Approximate KKT Points in Constrained Optimization
Let X ⊂Rd be a closed convex set. Consider the following constrained optimization problem:
P∗ = minf(x)
x∈X (ConstrOpt)
s.t. f (x)≤0,
c
where f,f :X →R are differentiable (possibly nonconvex) functions.
c
TheassociatedLagrangianfunctionL:X×R →Risdefinedforanyx∈X,λ≥0byL(x,λ)=f(x)+λf (x).
≥0 c
The primal and dual problems can be respectively written as
P∗ = inf supL(x,λ) ,
x∈Xλ≥0
D∗ =sup inf L(x,λ) .
λ≥0x∈X
(cid:124) (cid:123)(cid:122) (cid:125)
=:d(λ)
By weak duality, we know that P∗ ≥D∗.
For any x∈Rd, the normal cone to the set X at x is defined by:
N (x):=(cid:8) g ∈Rd |∀y ∈X,⟨g,y−x⟩≤0(cid:9) .
XPhilip Jordan, Anas Barakat, Niao He
Definition 1. Let ϵ ≥ 0. A point x ∈ X is an ϵ-KKT point of (ConstrOpt) if there exists a real λ such that
the following conditions hold:
f (x)≤ϵ, (primal feasibility)
c
λ≥0, (dual feasibility)
|λf (x)|≤ϵ, (complementary slackness)
c
dist(∇ L(x,λ),−N (x))≤ϵ. (Lagrangian stationarity)
x X
We also call (x,λ) an ϵ-KKT pair. The point x is simply a KKT point of (ConstrOpt) if moreover ϵ=0.
We additionally define a slight modification of the above standard KKT conditions which turns out to be useful
inouranalysis. Moreprecisely,thedefinitionreplacesapproximateLagrangianstationaritybyavariationalform
thereof. Moreover, primal feasibility is now supposed to be exact. Other conditions remain unchanged.
Definition 2. Let ϵ≥0. A point x˜∈X is an ϵ-K(cid:93) KT point of (ConstrOpt) if there exists a real λ˜ such that the
following conditions hold:
f (x˜)≤0, (exact primal feasibility)
c
λ˜ ≥0, (dual feasibility)
(cid:12) (cid:12)
(cid:12)λ˜f (x˜)(cid:12)≤ϵ, (complementary slackness)
(cid:12) c (cid:12)
(cid:68) (cid:69)
max x˜−x′,∇ L(x˜,λ˜) ≤ϵ. (variational Lagrangian stationarity)
x
x′∈X
(cid:93)
In particular, the point x˜ is said to be a KKT point of (ConstrOpt) when ϵ=0.
The next lemma connects the first stationarity condition with a variational form thereof. In particular, this
result allows to connect the two definitions of approximate KKT points above.
Lemma 13. Let X ⊆ Rd be a convex and compact set. Let ϵ > 0 and let x,g ∈ Rd. If dist(g,−N (x)) ≤ ϵ,
X
then max ⟨x−x′,g⟩≤∆ϵ, where ∆:=max ∥x−x′∥ is the diameter of the set X.
x′∈X x,x′∈X
Proof. Let y ∈−N (x). For any x′ ∈X, we have
0 X
⟨x−x′,g⟩=⟨x−x′,g−y ⟩+⟨−y ,x′−x⟩,
0 0
≤⟨x′−x,g−y ⟩,
0
≤∥x′−x∥·∥g−y ∥,
0
wherethefirstinequalityfollowsfromthefactthaty ∈−N (x), thesecondinequalitystemsfromtheCauchy-
0 X
Schwarz inequality. Taking the infimum with respect to y in the last inequality gives the desired inequality
0
since dist(g,−N (x))=inf ∥g−y∥≤ϵ.
X y∈−NX(x)
D.2 A Novel Technical Lemma for Approximate Optimality under Gradient Dominance
Wenowstateourtechnicallemma. ThisresultsshowsthatanapproximateKKTpointof (ConstrOpt)atwhich
a gradient domination inequality holds for the Lagrangian function is approximately optimal for the objective
function to be minimized.
(cid:93)
Proposition 2. Let ϵ > 0 and let x˜ ∈ X be an ϵ-KKT point of (ConstrOpt). Suppose there exist con-
stants C ,C ≥ 0 such that the Lagrangian function associated to (ConstrOpt) satisfies for all λ ≥ 0 and for
0 1
all x∈X,
(cid:68) (cid:69)
L(x,λ)−L(x∗,λ)≤C max x−x′,∇ L(x˜,λ˜) +C ϵ, (29)
λ 0 x 1
x′∈X
where x∗ is a minimizer of L(·,λ). Then, we have
λ
f(x˜)−P∗ ≤(C +C +1)ϵ.
0 1Independent Learning in Constrained Markov Potential Games
Proof. Let (x˜,λ˜) be an ϵ-K(cid:93) KT pair. Then, we have
D∗ ( =a) maxd(λ)≥d(λ˜)
λ≥0
( =b) minL(x,λ˜)
x∈X
(c)
≥ L(x˜,λ˜)−(C +C )ϵ
0 1
=f(x˜)+λ˜f (x˜)−(C +C )ϵ
c 0 1
(d)
≥ f(x˜)−(C +C +1)ϵ
0 1
where (a) and (b) are by definition, and (d) is due to complementary slackness. To see (c), observe that by
Lagrangian stationarity and (29),
(cid:68) (cid:69) 1 (cid:16) (cid:17)
ϵ≥ max x˜−x′,∇ L(x˜,λ˜) ≥ L(x˜,λ˜)−L(x∗,λ˜)−C ϵ ,
x′∈X x C 0 λ˜ 1
which implies that10
L(x˜,λ˜)−L(x∗,λ˜)≤(C +C )ϵ.
λ˜ 0 1
Finally, we use weak duality, i.e. P∗ ≥D∗, to conclude that
f(x˜)−P∗ = f(x˜)−D∗ +D∗−P∗ ≤(C +C +1)ϵ.
0 1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
≤(C0+C1+1)ϵ ≤0
E ADDITIONAL DETAILS ABOUT SIMULATIONS
We provide additional details regarding the implementation of our iProxCMPG (Algorithm 2) in practice:
(a) In our experiments, each episode terminates after a fixed number of steps T = 10 corresponding to a
e
discount factor γ =0.9.
(b) In order to reduce the variance and enable the usage of larger step sizes, all constraint and value (gradient)
estimates are obtained by sampling a batch of B trajectories.
(c) For the subroutine, i.e. as solution to the proximal-point update, we do not consider a ρ -weighted average
k
over iterates but simply use the last iterate π(t,K).
(d) We choose δ =0 for all k ∈N.
k
Hyperparameters We report hyperparameter choices for our simulations in Table 2. Note that to ensure
convergence, as indicated by our theoretical results, a larger number of players m requires smaller step sizes and
larger sample batches. Step sizes η and ν were chosen by tuning over the range [0,1].
k
Error Bars and Reproducibility The plots in Figs. 1 and 2 show the means of estimated potential values
across 10 independent runs, and the corresponding shaded region displays the respective standard deviation.
Obtaining results for all presented experiments thus requires simulating 60 runs in total. All experiments are
fully reproducible using the provided code and specified seeds.
Computing Infrastructure In order to reduce computation time by executing all runs in parallel, we con-
ducted the simulations within less than 4 hours on a cluster of 15 4-core Intel(R) Xeon(R) CPU E3-1284L v4
clocked at 2.90GHz and equipped with 8Gbs of memory.
10If C =0, the same inequality immediately holds from (29).
0Philip Jordan, Anas Barakat, Niao He
Table 2: Overview of hyperparameters used in our simulations.
Hyperparameters Number of players m Pollution tax Energy marketplace
Step size η (outer loop) - 0.1 0.1
2 0.005 0.002
Step size ν (inner loop) 4 0.002 0.001
k
8 0.0007 0.0003
2 1000 100
Sample batch size B 4 1000 150
8 2500 200
K (#iterations inner loop) - 20 20
T (#iterations outer loop) - 20 60
Discount factor γ - 0.9 0.9
Episode length T - 10 10
e
Notation As used in the main part, U({1,··· ,W}) refers to the uniform distribution over the finite
set {1,··· ,W} where W ≥2 is an integer.