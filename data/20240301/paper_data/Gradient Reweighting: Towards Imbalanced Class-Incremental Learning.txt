Gradient Reweighting: Towards Imbalanced Class-Incremental Learning
Learned classes
Newclasses
JiangpengHe FengqingZhu
he416@purdue.edu zhu0@purdue.edu
ElmoreFamilySchoolofElectricalandComputerEngineering,PurdueUniversity,USA
Abstract Learned classes intra-phase imbalance Fc layer
Newclasses Feature extractor
number of
training data
Class-IncrementalLearning(CIL)trainsamodeltocon-
tinually recognize new classes from non-stationary data memory exemplar set
budget training
while retaining learned knowledge. A major challenge of
CILariseswhenapplyingtoreal-worlddatacharacterized
inter-phase imbalance
bynon-uniformdistribution,whichintroducesadualimbal- Figure1. Theillustrationofimbalancedclass-incrementalwitha
anceprobleminvolving(i)disparitiesbetweenstoredexem- dual imbalance issue including the intra-phase imbalance within
plars of old tasks and new class data (inter-phase imbal- eachnewtaskT andinter-phaseimbalancebetweenoldtasksex-
emplarsandnewtasktrainingdata. Mt referstothemodelafter
ance),and(ii)severeclassimbalanceswithineachindivid-
learningthenewtaskTt.
ual task (intra-phase imbalance). We show that this dual
imbalance issue causes skewed gradient updates with bi-
asedweightsinFClayers,thusinducingover/under-fitting tionssuchason-devicelearning[16]. However, deepneu-
andcatastrophicforgettinginCIL.Ourmethodaddressesit ral network suffers from catastrophic forgetting [32] when
byreweightingthegradientstowardsbalancedoptimization learning new classes, where the performance on learned
and unbiased classifier learning. Additionally, we observe classes decreases significantly due to the unavailability of
imbalanced forgetting where paradoxically the instance- oldtrainingdata. Moreover, conventionalCILmethodolo-
richclassessufferhigherperformancedegradationduring giestypicallytacklethischallengebyassumingabalanced
CILduetoalargeramountoftrainingdatabecomingun- data distribution, where each class has roughly the same
availableinsubsequentlearningphases. Totacklethis,we number of training samples. Nevertheless, this assump-
further introduce a distribution-aware knowledge distilla- tion is misaligned with real-world scenarios where data is
tionlosstomitigateforgettingbyaligningoutputlogitspro- usually long-tail distributed with significant disparity be-
portionally with the distribution of lost training data. We tween instance-rich (head) classes and instance-rare (tail)
validate our method on CIFAR-100, ImageNetSubset, and classes. Such imbalance presents unique challenges that
Food101 across various evaluation protocols and demon- conventionalCILmethodscannotadequatelyaddress,thus
strateconsistentimprovementscomparedtoexistingworks, reducingtheireffectivenessandbroaderapplicability.
showinggreatpotentialtoapplyCILinreal-worldscenar- As shown in Figure 1, CIL with imbalanced data intro-
ioswithenhancedrobustnessandeffectiveness. duces a dual challenge encompassing both inter-phase and
intra-phaseimbalances. ConventionalCILapproachespri-
marily address the inter-phase imbalance while the intra-
1.Introduction phase imbalance emerges distinctly in the context of im-
balanced CIL. In general, the inter-phase imbalance arises
The ever-evolving and unpredictable nature of real-world fromthedisparitiesbetweennewclassdataandthelearned
environments drives the imperative to develop Class- task data preserved in the exemplar set for knowledge re-
IncrementalLearning(CIL)systemswiththecapabilityof play[29,37]. Suchimbalancescanskewthelearningpro-
acquiringknowledgecontinuouslyfromnon-stationarydata cess of the classifier between new and old classes[11, 15]
where new classes appear sequentially over time. The ad- (e.g., the learned weights in the FC layer are heavily bi-
vantageofCILresidesinbothmemoryandcomputational ased [46, 53]), leading to biased predictions towards the
efficiency which eliminates the requirement of storing all newerclassesandthusinducingcatastrophicforgetting. In
previouslylearneddataorretrainingthemodelfromscratch thecaseofintra-phaseimbalance,eitherthenewclassesor
entirely,makingCILapplicabletovariouspracticalapplica- the stored exemplars individually present their own imbal-
4202
beF
82
]VC.sc[
1v82581.2042:viXraanceddistributions(theimbalanceoftheexemplarsetarises inference phase, i.e. a single head classifier [30] is used
when the number of training data is less than the memory for all classes seen so far. Existing CIL has been studied
budgetforeachclass). Thisnotonlyaffectsthelearningof inbothonlineandofflinescenarioswhereeachdataisob-
newknowledgebutalsoexacerbatestheforgettingissueby servedonlyoncebythemodelintheformercase.
intensifyingtheinter-phasebiases. Conventional CIL approaches can be mainly catego-
Oneofthemajorchallengesposedbylearningfromim- rized into three groups including regularization, memory
balanced data is the biased gradient updates in FC layers replay, and parameter isolation. The regularization-based
that are dominated by instance-rich classes. Specifically, methodsaimtorestrictdrasticalterationstomodelparam-
in gradient-based optimization such as stochastic gradient eters that are important for learned classes. A representa-
descent (SGD), the weight update step in each iteration is tivetechniqueistoapplyknowledgedistillation[19]based
heavily influenced by the magnitude of gradients, which on output logits [10, 24, 37, 46] or the intermediate lay-
significantly depends on the data distribution of the sam- ers [14, 20, 40]. In addition, recent studies [1, 5, 46] re-
pled mini-batch. As a result, instance-rich classes tend to vealthebiasedweightsinFClayertowardsnewerlearned
receive gradients with larger magnitudes thus skewing the classes and address it by applying post-hoc biased logits
optimizationprocesstomaketheclassifierover-fitonhead correction[5,17,46,53]orusingseparatedsoftmax[1]and
classes while under-fit on tail classes. This problem be- cosine normalized classifier [20]. The replay-based meth-
comes more challenging under the context of CIL as the odsstoreasmallsetofexemplardatatoperformknowledge
trainingdatadistributionchangesovertime,andthelearned rehearsal in subsequent phases. Herding algorithm [45] is
classessufferfromcatastrophicforgetting. Besides,weob- widelyemployedinCIL[10,37,46]forexemplarselection
serve that the forgetting could also be imbalanced where basedontheclassmean. Later, alearning-basedexemplar
theheadclassesusuallysuffermoreperformancedegrada- selection method is introduced in [27]. In the online set-
tion as a substantial portion of their training data becomes ting,theexemplarselectionisperformedattheendofeach
unavailable in subsequent incremental learning stages. To trainingiteration[2,3,15,29,36]withoutknowingthedis-
this end, our work aims to address the dual imbalance is- tribution of training data. Finally, the parameter isolation
sue by reweighting the gradient updates in the FC layer, relatedapproachesgraduallyexpandthenetworksizeorin-
whichrecalibratestheoptimizationprocessandfostersthe creasemodelparameters[28,44,47,49,50]toprovideded-
learningofunbiasedclassifiers. Furthermore,weintroduce icatedspaceforlearningnewknowledgewhileensuringthe
adistribution-awareknowledgedistillationlosstoalleviate previouslylearnedinformationremainsundisturbed.
the imbalanced catastrophic forgetting problem by consid- Imbalanced CIL remains less explored compared to
eringthedistributionoflosttrainingdatatoimposestronger conventionalCILduetotheintricatechallengeofaddress-
regularizationeffectsontheheadclasses. Themaincontri- ing both catastrophic forgetting and imbalanced learning
butionsofthisworkaresummarizedinthefollowing, simultaneously. The earlier works focused on imbalanced
• We study CIL under realistic class-imbalanced scenar- CILundermulti-label[22],semi-supervised[6],andonline
iosandintroduceanewend-to-endgradientreweighting scenarios[12].Themostrecentstudy[26]formulatedlong-
frameworktotacklebothintra-phaseandinter-phaseim- tailedCILforbothshuffledandorderedcasesandproposed
balance challenges by rebalancing the optimization pro- a two-stage technique to decouple representation learning
cessinFClayers. andclassifierlearning. Later, adynamicresidualclassifier
• A new distribution-aware knowledge distillation loss is isintroducedin[11]tohandletheimbalancebetweennew
proposed to further mitigate imbalance catastrophic for- and learned classes. In this work, we primarily focus on
gettingcausedbytheunevennumberoflosttrainingdata CILfollowingtheshuffledlong-tailedcase[26]toaddress
duringimbalancedCIL. the imbalance issue from the perspective of balancing the
• Theefficacyofourproposedmethodisvalidatedthrough gradient updates in FC layer. In addition, contrasting the
extensive experiments across a variety of CIL settings, findingsin[22], weobservedimbalancedforgettingwhere
whichachievesnotableimprovementsoverexistingmeth- theheadclassesinsteadsuffermoreperformancedegrada-
odsinbothCILandlong-tailedrecognitiontasks. tion attributed to the larger volume of lost training data in
subsequent incremental phases. Therefore, we incorporate
2.RelatedWork
a distribution-aware knowledge distillation loss to impose
Inthissection,wereviewtheexistingstudiesthataremost
a more stringent regularization on head classes to further
relatedtoourworkincludingclass-incrementallearningin
mitigatetheforgettingissue.
Section2.1andlong-tailedrecognitioninSection2.2.
2.2.Long-tailedLearning
2.1.Class-IncrementalLearning
Class-incremental learning (CIL) is a subarea of continual The deep long-tailed learning for visual recognition has
learning where the task index is not provided during the been studied comprehensively [48, 52] over the decadesduetotheprevalenceofclass-imbalanceddataobtainedin Number of training data per class Task 1 New classes
Task 2 Learned classes
real-world scenarios. The major challenge arises from the Task 3
1000
fact that the performance of deep learning models tends
to be dominated by head classes, leaving the learning for
500
tail classes significantly under-realized. In this work, we
center on class-rebalanced approaches with the goal of re-
20
balancing the influence resulting from imbalanced train-
Average magnitude of gradients
ing samples, which mainly consists of re-sampling, and
class-sensitivelearning. Specifically,there-samplingbased
methods [8, 35, 43] aim to construct a balanced training
batchbyincreasingtheprobabilityoftailclassestobesam-
pled. Theclass-sensitivelearningseekstoadjustthevalue
oftraininglosstorebalancetheunevenlearningeffectsby
reweighting class influence [9, 13, 33, 34, 38]. In addi- Class Index
tion, recent studies [15, 41, 42] argue that the imbalance Figure2. Theaveragemagnitudesofgradient||∇ Lce(Wj)||for
between positive and negative gradients severely inhibits each class j by incrementally learning 3 tasks T1,T2,T3 with
cross-entropyandmemorybudgetn =20exemplarsperclass.
balanced learning and they address it by adjusting the loss ε
influence across different classes. Our proposed method
stemsfromasimilarobservationbutidentifyingthemagni- imbalanceandinter-phaseimbalance. Specifically,thefor-
tudeofgradientvectorsdivergessignificantlybetweenhead mer case represents the discrepancy within each task Tt
and tail classes during the learning process. This discrep- wheregivenaheadclassjandatailclasskwithj,k ∈Yt,
ancyleadstounevenoptimizationsteps,subsequentlycaus- we have the corresponding training data n j ≫ n k. Fur-
ingthemodeltoover-fittheheadclasseswhileunder-fitting thermore,theexemplarset|Et|mayalsoexhibitthisclass-
thetailclasses.Toaddressthis,ourgoalistoequilibratethe imbalance issue especially given a larger memory budget
gradient updates, ensuring a more balanced learning pro- n ε as more training from head classes will be retained
cess. Furthermore, existing long-tailed learning methods comparedtotailclasses,therebyamplifyingthischallenge.
donotconsiderpreviouslyacquiredknowledgeforlearned The latter case refers to the discrepancy between new task
classes,makingthemunsuitableunderthecontextofCIL. data and stored exemplars when t > 1 where |Xt| ≫
|X1:t−1|. In general, the intra-phase imbalance can re-
e
3.Preliminaries strict the model’s capacity to learn new knowledge while
The CIL can be formulated as learning a sequence of N the inter-phase imbalance can lead to catastrophic forget-
tasks {T1,T2,...TN}. Each task t ∈ [1,N] refers to tingasthemodelmightexhibitabiastowardsnewclasses.
one learning phase in CIL, which can be represented as
3.1.ImbalancedGradients
Tt = {Xt,Yt} where Xt = {xt,...,xt } denotes the
1 |Xt|
set of total |Xt| number of training data and Yt denotes Imbalanced gradient updates pose a significant challenge
the associated class labels belonging to |Yt| classes. In when learning from imbalanced data. Generally, consider
general,wehavetotal(cid:80)N |Xt|trainingdatacorrespond- the weights of a classifier W ∈ Rdf×c with input feature
ing to (cid:80)N |Yt| classest= .1 After learning each task Tt, dimensiond f andoutputclassesc. GivenalossfunctionL,
t=1 thegradient-basedweightupdateprocessisformulatedas
the model is evaluated on test data belonging to all seen
classesY1:t withoutknowingthetaskidentifiert. Astrong W =W −η∇ (W ) (1)
i+1 i L i
constraint in CIL when incrementally learning a new task
Tt(t > 1)istheunavailabilityofpreviouslylearnedtasks where i denotes the index of iteration and η is the learn-
data{X1,...,Xt−1},whichcausesthecatastrophicforget- ing rate. The gradient matrix ∇ L(W i) ∈ Rdf×c deter-
ting on learned tasks T1:t−1. Therefore, given a memory mines the magnitude of the update η||∇ L(W i)|| and the
budgetn dataperclass,thereplay-basedmethodsemploy direction of the update ∇L(Wi) where ||·|| denotes the
an exempε lar set Et = {X1:t−1,Y1:t−1} to store both data l norm. In general, the|| m∇L a( gW nii t) u|| de of the gradient is pro-
e e 2
X1:t−1 and their class labels Y1:t−1 to combine with new portional ||∇ (W )|| ∝ L to the loss value which is usu-
e e L i
data{X1:t−1∪Xt}×{Y1:t−1∪Yt}duringeachincremen- ally computed as the average loss over all data samples.
e e
tallearningphasetomaintainthelearnedknowledge. However, challenges arise when the gradient is estimated
Imbalanced CIL refers to the case where the number from a mini-batch containing class-imbalanced data, e.g.,
of training data varies a lot among different classes while n ≫ n givenaheadclassj andtailclassk. Inthiscase,
j k
the test data remain balanced. This induces a dual imbal- the gradient magnitude for the head class tends to be sig-
ance issue in the training phase including the intra-phase nificantly larger than that for the tail class ||∇ (Wj)|| ≫
L i𝛽
Accumulated gradientsΦ Imbalanced gradients
𝛼’ balance
Accumulated gradientsΦ
𝛼( 𝛼
𝛼)
𝜵𝑳𝒄𝒆(𝑾)
𝛼* clao sl sd es 𝜵𝑳𝒄𝒆(𝑾) 𝜵𝑳𝒅𝒂𝒌𝒅(𝑾)
reweighting 𝑟
back prop
𝑟 stability
𝛼
𝑾 𝜶𝜵𝑳𝒄𝒆(𝑾) cln ae sw
se s
𝜵𝑳𝒄𝒆(𝑾) task-balance ratio
class-balance ratio
plasticity loss-balance ratio
Balanced gradients
Intra-phase gradient reweighting Inter-phase decoupled gradient reweighting
Figure3.TheoverviewofgradientreweightingunderimbalancedCIL.GiventheclassifierW,theintra-phasegradientweightingisguided
byscalingthegradientmatrix∇ (W)withclassbalanceratiosαderivedfromthecumulativegradientsΦoveriterations.Concurrently,
Lce
theinter-phaseDecomposedGradientReweighting(DGR)balancestheplasticitylearningbyseparatelyadjustinggradientswithclass-
balanceratiosαandtask-balanceratiosr.Followedbytuningthestability-plasticitytrade-offwithalossbalanceratioβ.
||∇ (Wk)||. In Figure 2, we highlight the gradient im- training in subsequent learning phases after they were ini-
L i
balances across different classes by calculating the aver- tially observed. Therefore, simply balancing all gradients
age gradient magnitude at the end of each task by incre- without accounting for this will result in knowledge for-
mentallylearning3newtasks(withcross-entropyloss)on getting for learned classes, which could be imbalanced as
ImageNetSubset-LT[26]witheachtaskcontains5classes. wellduetotheunevennumberoflosttrainingdatabetween
This disparity in gradient magnitudes leads to an imbal- headandtailclasses. Second, thetrainingobjectivevaries
anced optimization process where larger gradients from betweennewandoldtaskswherethelearnedtasksT1:t−1
head classes cause larger optimization steps in weight up- builduponknowledgeaccumulatedfrompreviouslearning
dates, thus potentially leading to over-fitting. Conversely, phases, while the new task Tt starts from scratch without
smaller updates for tail classes may result in under-fitting. any prior knowledge. Consequently, balancing the gradi-
Furthermore,thelargergradientmagnitudescanalsoinduce ent updates for all classes may not be sufficient due to the
biases in the norm of weight vectors as ||W −W || ∝ intrinsicimbalanceinoptimization, whichrequiresamore
i+1 i
||∇ (W )||,resultinginbiasedpredictiontowardsinstance- adaptiveandflexibleapproachtoensurethemodelcanef-
L i
rich or newly learned classes under CIL. In the following fectivelylearnnewtaskswithoutforgettingtheknowledge
section, we illustrate our proposed method to address the frompreviousones.
aforementionedissuesbyre-weightingthebiasedgradients Inthefollowingsections,weillustratehowtoeffectively
tofacilitatelearningofabalancedclassifier. determine the balance vector α under CIL for intra-phase
scenarioinSection4.1,andinter-phasecasetotackleboth
4.Method biasedgradientsandimbalancedforgettinginSection4.2.
Overview.AsillustratedinFigure3,weaimtoreweightthe 4.1.Intra-PhaseClass-Imbalance
imbalanced gradient matrix by classwisely multiplying it
ConsiderthecaseoflearningthefirsttaskT1 ={X1,Y1}
withabalancevectorα = [α1,α2,...αc]fortotalcclasses
fromascratchmodelM0(f ,W)wheref andW refersto
seen so far. Therefore, the gradient update process can be θ θ
thefeatureextractorandclassifier,respectively. Thecross-
modifiedas
entropylossisformulatedas
Wj =Wj −ηαj∇ (Wj) (2)
i+1 i L i
whereWj referstotheweightvectorinFClayerforclass 1 | (cid:88)X1|| (cid:88)Y1|
i L =− yˆ log(p ) (3)
j and αj∇ (Wj) denotes the corresponding reweighted ce |X1| j j
L i k=1j=1
balanced gradient. However, it is non-trivial to obtain
the appropriate balance vector due to the intricate data- where p j is the Softmax of the jth output logit z j =
dependentoptimizationprocess. Inaddition,whenlearning [WTf θ(x k)] j andyˆ j denotestheclasslabel. Thegoalisto
a new task Tt(t > 1) with the present of learned classes determinetheclass-wisebalancevectorα = [α1,...α|Y1|].
belonging to Y1:t−1, it poses two additional challenges. Recognizing that a static balance vector may not suffi-
First, the training distribution, denoted as D, changed for ciently capture the biases inherent in the gradients matrix
learned classes during CIL where D(X1:t−1,Y1:t−1) ̸= thatevolveasthelearningprogresses,weproposeasimple
e e
D(X1:t−1,Y1:t−1)asonlylimitedexemplarsarestoredfor yeteffectivesolutioninthisworktodeterminethebalance
class
indexvectoradaptivelybyleveraginghistoricalaccumulatedgra- wherepˆτ =Softmax(zˆ /τ)issoftened(τ =2)outputprob-
j j
dients. In detail, we calculate the class balance ratio α ij at ability from the teacher model Mˆt−1 with frozen parame-
iterationibasedontheaccumulatedmagnitudeofgradients tersobtainedinthelastlearningphaset−1. However, as
correspondingtoeachclassj as mentioned earlier in Section 1, catastrophic forgetting can
α ij =
mm ∈i
Y
Φn
1
jΦm i
, Φj
i
=
(cid:88)i
||∇ Lce(W nj)|| (4)
b cle ai sm sb ba el ca on mce ed ua ns aa vala ilr ag be lr ev io nlu cm ome po af ri in ss ot nan tc oe ts hf ero tam ilth cle ah ssea id
n
i n=1 thesubsequentincrementallearningphasesduetothefixed
Bydoingso,wedynamicallyrecalibratetheweightupdates, memorybudget. Thoughthemostintuitivewaytoaddress
ensuringthatbothmajorityandminorityclassescontribute thisissueisstoringmoredataforheadclassesandlessdata
equallytothelearningprocessandaddressthebiasedissue. fortailclasses,itposesanothernontrivialchallengearising
Regularized softmax cross-entropy. While the gradi- fromaclass-imbalancedexemplarsetthatpotentiallyinten-
ent update adjustments help in emphasizing tail classes, it sifies both over-fitting and under-fitting issues. Therefore,
alsoinadvertentlyleadstoincreasedgradientstowardshead we introduce the Distribution-Aware Knowledge Distilla-
classesduetothedecreaseofoutputlogitz jofheadclasses, tion(DAKD)tomaintaintheknowledgebytakingintoac-
making the cross-entropy loss put more efforts to increase countthedistributionofthelosttrainingdata. Specifically,
its output logits. Motivated by [33], we alleviate this side weobtainthelosttrainingdatadistributionsbycalculating
effectbyapplyingregularizedcross-entropyusingamodi- s =|Xj|−|Xj|foreachclassj ∈Y1:t−1where|Xj|and
j e
fiedsoftmaxequation |Xj|denotethenumberoforiginaltrainingdataandstored
e
exemplars for class j, respectively. Motivated by [54], we
exp(z +logπ ) Φj
p = j j , π = i (5) decouple the original distillation loss into a weighted sum
j (cid:80)| mY =1| 1exp(z m+logπ m) j (cid:80)| mY =1| 1Φm
i
of two parts using a ratio σ ∈ [0,1] (1 indicates balanced
distribution) measured by the entropy of s. The DAKD is
where logπ is the per-label offsets added to the original
j thenformulatedas
outputlogit.Generally,theinstance-richclasseshavelarger
π withanincreaseofsoftmaxoutputp tocompensatefor L (z,zˆ|s)=σL (z,zˆ)+(1−σ)Limb(z˜,zˆ) (7)
j j dakd kd kd
the side effect of down-weighting the gradients during the
where L denotes the balanced part calculated using the
kd
training process. In this work, we consistently use the ac-
original output logit z, and Limb denotes the imbalanced
cumulated gradient Φj to calculate π at each iteration i, kd
i j partwithadjustedoutputlogitsz˜determinedby
which ensures a balanced training by adjusting the output
s s
logitforheadclassestomitigatetheover-amplifiedefforts. z˜ = j z +(1− j )zˆ
j (cid:80)|Y1:t−1|s j (cid:80)|Y1:t−1|s j (8)
m m m m
4.2.Inter-PhaseClass-Imbalance
wherethiscalibrateddistributionofoutputlogitsz˜demon-
Weextendourintra-phasemethodtoaddresstheinter-phase strateslargerdiscrepancy(|z˜ −zˆ |)forclassjwithahigher
j j
imbalanceinCILbyaddingtwomodules:(i)aDistribution- volume of lost training data (i.e., head classes), thereby
Aware Knowledge Distillation (DAKD) loss to maintain assigning those classes with more efforts for knowledge
theinformationfromtheoriginaltrainingdistributionwhile distillation compared to those with less data lost (i.e., tail
mitigating the imbalanced forgetting (Section 4.2.1), and classes) which only require a subtler distillation interven-
(ii) a Decoupled Gradient Reweighting (DGR) framework tiontomaintainthelearnedknowledge. Thistailoredlogit
toseparatelymanagethegradientsbetweennewandalready adjustmentensuresthattheextentofknowledgedistillation
learned tasks to ensure a balance between the stability of is appropriately aligned with the level of data lost experi-
pastknowledgeandtheplasticityrequiredforlearningnew encedbyeachclass. Theoveralllossfunctionforlearning
classes(Section4.2.2). anewtaskTt,t>1canbeexpressedas
L=L +λL (9)
4.2.1 Distribution-AwareKnowledgeDistillation ce dakd
(cid:112)
Similar to existing bias correction methods [1, 46, 53], We use λ = λ b |X old|/|X new| to adjust the influence of
we consider adding the knowledge distillation loss [19] to knowledge distillation, which increases as more data have
maintain the learned knowledge due to the changing of beenobserved. |X old|and|X new|denotethenumberofold
training distribution for learned classes. The original dis- andnewclassestrainingdataandλ bisafixedscalar.
tillationlossforlearninganewtasktcanbeexpressedas
4.2.2 DecoupledGradientReweighting
|Xt+X1:t−1||Y1:t−1|
1 (cid:88)e (cid:88)
L (z,zˆ)=− pˆτlog(pτ) The overview of Decoupled Gradient Reweighting (DGR)
kd |Xt+X e1:t−1|
k=1 j=1
j j isshowninFigure3,whichaddressestheinter-phaseimbal-
(6) anceissuebystrikingabalancebetweenstability(maintainpastknowledge)andplasticity(learnnewclasses). Gener- asitcontainsinstrumentalinformationobtainedfromprevi-
ally, the gradients from cross-entropy loss (∇ ) in CIL oustrainingdistributiontohelpmaintainthediscrimination
Lce
represent the plasticity that enables the model to adapt to of learned classes during CIL as studied in [53]. Later in
newtrainingdatadistributionbyincorporatingbothexem- Section5.3,wewillshowthatreweightingthegradientsof
plarsofoldclassesanddatafromnewclasses. Incontrast, knowledgedistillationcouldharmtheoverallperformance.
thestabilityisintroducedbythegradientsfromknowledge 5.Experiments
distillationloss(∇ )toguidethemodeltowardsaso-
Ldakd 5.1.ExperimentalSetups
lutionthatalignswiththetrainingdistributionfromthepre-
vious learning phases. Therefore, the DGR address inter- Evaluation protocol. In this work, we adopt two widely
phase imbalance in two folds by first reweighting gradient usedprotocolsforCILincluding(1)learningfromscratch
∇ fromL toensureunbiasedplasticityandthenmod- (LFS)[37]and(2)learningfromhalf(LFH)[14,20]. The
Lce ce
ulatingtheinteractionbetween∇L and∇L toattain LFS equally split all the classes into N tasks to incremen-
ce dakd
abalancedequilibriumbetweenplasticityandstability. tally train a model from scratch. The LFH first trains the
Forplasticity, theDGRfirstseparatelyreweightsthegra- model with the initial half of the classes and then equally
dients∇ forlearnedclassesj ∈Y1:t−1andnewclasses dividestheremaininghalfoftheclassesintoN tasks. For
Lce
for j ∈ Yt by calculating class-balance ratios αj respec- bothcases,themodelisevaluatedonallclassesseensofar
i
tively as specified in Equation 4. Following this, a task- afterlearningeachtaskwithoutknowingthetaskidentifier.
balance ratio rj is introduced for tuning between new and Datasets. We evaluate our method on three public
i
learnedtasksas datasets including CIFAR100 [23], ImageNetSubset with
(cid:40) min{1, 1 } j ∈Y1:t−1 100 classes from ImageNet [39] and Food101 [7]. Specif-
rj = rΦi ically, we first follow [26] to construct CIFAR100-LT,
i min{1,r Φi ×exp(−γ|X |X1: 1t :− t|1|)} j ∈Yt ImageNetSubset-LT and Food101-LT, which are the long-
(10) tailed versions of theoriginal balanced datasets by remov-
where exp(−γ|X1:t−1|) is the attenuation factor and γ > ing training samples with an imbalance factor [13] ρ =
0 Φj
i
Φi ∈s jY ∈1a Y:t t−h 1yp ie sr- tp ha er| aX rm a1 t: iet o| ter ofto mea ad njus at cci uts mm ula ag ten ditu gd re a. dier nΦ tsi fo=
r
a tn n in vm m d ea i lnx ym .= i Fn oim r10 au0 lm ltw hnh ru ee emre db an e trm asoa ex f tsta ,rn wad i enn i unm sg ein Ndad t =aen po {et 1e r 0t ,ch 2le a 0s }m s, ta arx sei ksm spu e fom c r-
i j∈Y1:t−1 j∈Yt LFS andN = {5,10}tasksforLFH.Thetestsetsremain
learnedclassesΦ tonewclassesΦ .
i i unchangedwiththeoriginalclass-balanceddistributions.
The advantage of DGR with an attenuation factor lies
Implementation details. We utilize ResNet-32 [18] for
in promoting a more equitable optimization based on the
CIFAR100-LTandResNet-18[18]forImageNetSubset-LT
fact that the learned classes are built upon the knowledge
andFood101-LT.Toensureafaircomparison,weadoptthe
learned in prior learning phases, whereas the new classes
same class order and setups from [26] where we train 160
are being trained from scratch. Therefore, merely balanc-
epochsforCIFAR100-LTwithaninitiallearningrateof0.1
ingthegradientcontributionsbetweennewandoldclasses
and then reduced by a factor of 10 at the 80th and 120th
couldinadvertentlyleadtotheunder-fittingofnewclasses
epochs. ForImageNetSubset-LTandFood101-LT,wetrain
as they may not receive adequate training emphasis. Rec-
90 epochs with an initial learning rate of 0.1 which is re-
ognizing this, the attenuation factor adjusts the gradient
ducedby10atthe30thand60thepoch. Aconsistentbatch
reweighting ratio in favor of new classes proportional to
sizeof128withanSGDoptimizerisusedacrossallexper-
the volume of data that has already been observed so far
iments. For simplicity, we set our hyper-parameters with
|X1:t−1|,therebyprovidingacalibratedboostinsupportof
γ = 1andλ = 1forallexperiments. Westoren exem-
b ε
thenewclassestomitigatethelearningdisparities.
plars per class selected by Herding [37] and conduct each
Fortuningbetweenstabilityandplasticity,weadjustthe
experimentthreetimestoreportaverageperformance. All
magnitude of gradients from distillation ||∇ (W )|| to
Ldakd i resultsareobtainedbyreproducingtheoriginalmethodsun-
makeitbalancewithreweightedcross-entropybyincluding
derthesamesettingbasedontheframeworkin[26,31].
alossbalanceratioβ iatiterationiasβ i = || |α |∇ir Li d∇ aL kdce (W(W i)i |) |||.
5.2.ExperimentalResults
TheoverallDGRcanbeformulatedas
Table 1 summarizes the Average Accuracy (ACC) [29] on
j∈Y1:t−1
(cid:122) (cid:125)(cid:124) (cid:123) CIFAR100-LT, ImageNetSubset-LT, and Food101-LT with
W ij +1 =W ij −η(α ijr ij∇ Lce(W ij)+β i∇ Ldakd(W ij)) imbalance factor ρ = 100, memory budget n ε = 20 per
(cid:124) (cid:123)(cid:122) (cid:125) class. We observe the LFS is more challenging for CIL
j∈Yt
(11) astheperformanceunderLFHprotocolconsistentlyhigher
Note that in this work, we intentionally avoid classwisely thanLFS,underscoringtheadvantageofpre-trainingwitha
reweighting the gradients from knowledge distillation loss largerportionofclassesintheinitialincrementalstep. TheImageNetSubset-LT Food101-LT
LFS(𝑵=𝟐𝟎,𝒏𝜺=𝟏𝟎) LFS(𝑵=𝟐𝟎,𝒏𝜺=𝟓𝟎) LFH(𝑵=𝟏𝟎,𝝆=𝟓𝟎) LFH(𝑵=𝟏𝟎,𝝆=𝟏𝟓𝟎)
class index class index class index class index
Figure4. Theclassificationaccuracy(%)ontestdatabelongingtoallclassesseensofarateachincrementalstepbyvaryingthememory
budgetn ∈{10,50}onImageNetSubset-LTandimbalancefactorρ∈{50,150}onFood101-LT.
ε
Datasets CIFAR100-LT ImageNetSubset-LT Food101-LT
Evaluationprotocol LFS LFH LFS LFH LFS LFH
TotaltasksN 10 20 5 10 10 20 5 10 10 20 5 10
iCaRL[37] 21.83 24.28 28.68 28.33 33.75 29.71 41.82 40.21 18.13 12.50 21.83 21.31
IL2M[5] 31.37 29.99 34.90 33.42 31.70 25.20 40.75 39.08 16.11 16.27 23.93 22.48
BiC[46] 28.89 20.10 25.68 25.95 33.31 30.86 33.18 29.23 16.94 16.81 22.80 20.75
WA[53] 27.63 23.48 32.07 26.85 32.58 29.03 32.62 28.10 16.58 15.99 18.45 19.45
SSIL[1] 26.07 26.15 30.72 29.21 30.38 25.99 38.97 35.18 16.86 15.65 21.65 19.03
FOSTER[44] 30.43 29.96 37.25 37.91 34.38 29.75 46.51 43.88 24.27 20.45 32.39 31.46
MAFDRC[11] 32.67 31.95 37.94 38.51 40.01 34.48 48.23 44.12 26.93 19.21 34.22 30.91
EEIL-2stage[10,26] 33.64 32.25 36.40 34.91 36.84 30.39 43.62 41.49 19.75 20.02 22.65 22.83
LUCIR-2stage[20,26] 31.09 31.03 38.47 37.86 39.87 34.79 48.97 47.39 27.65 24.68 36.05 35.06
PODNet-2stage[14,26] 30.41 30.37 38.38 38.45 35.47 31.71 48.02 47.74 23.78 21.13 35.42 35.22
FOSTER-2stage[26,44] 31.27 30.68 40.26 39.43 36.47 33.95 48.89 46.93 25.82 22.28 35.69 33.48
Ours 35.66 34.35 40.18 39.11 45.12 40.79 50.57 49.13 29.05 26.42 36.84 36.19
Table1.Resultsofaverageaccuracy(%)onCIFAR100-LT,ImageNetSubset-LTandFood101-LTwithimbalancefactorρ=100,memory
budgetn =20evaluatedunderLearningFromScratch(LFS)andLearningFromHalf(LFH).BestandSecondBestresultsaremarked.
ε
2-stage module [26] shows the effectiveness in addressing bothmemorybudgetandimbalancefactorstoconsistently
imbalanceddataduetoitsadditionallearnablelayertomit- achievethebestperformanceateachincrementalstep. Ad-
igate bias. Notably, LUCIR-2stage [20], which employs a ditionalresultscanbefoundinsupplementarymaterials.
cosineclassifierwithnormalizedweights,performswellin
5.3.AblationStudy
handling imbalanced data. Our method achieves promis-
ingresultsunderLFH andsignificantimprovementsunder Inthissection,weevaluatetheeffectivenessofourmethod
LFSevenwithoutnecessitatingextratrainingstagesandpa- toaddress(i)intra-phaseimbalance,(ii)inter-phaseimbal-
rameters. Thisdemonstratesthepotentialofourend-to-end ance,and(iii)biasedweightsinFClayers.
approachformoreefficienttrainingmethodologiesinCIL, Intra-phase Imbalance: We evaluate our gredient
especially as the task complexity scales up to learn an in- reweighting for its efficacy in solving the imbalanced im-
creasingnumberofclassesfromascratchmodel. age classification beyond CIL by conducting comparative
analyses with established methods in long-tailed recogni-
In Figure 4, we visualize the classification accuracy on tion. We denote our regularized softmax output illustrated
all classes seen so far at each incremental step by varying inSection4.1asRS.Specifically, wetrainaResNet-32to
the imbalance factor ρ ∈ {50,150} and the memory bud- classify the 100 classes in CIFAR100-LT with various im-
get n ∈ {10,50}. While generally increasing the mem- balancefactorρ∈{10,50,100}followingthetrainingpro-
ε
orybudgetresultsinbetterperformance,theimprovements tocol as in [13]. The results are summarized in Table 2.
couldbe marginalforsomemethods suchasSSIL[1] and Our method shows competitive performance even without
MAFDRC [11]. This paradox is attributed to the exacer- the use of regularized softmax output. Upon integrating
bationofexemplarsetimbalancebyusingalargermemory the regularized softmax output, which further balances the
budgetn wheretheinstance-richclassesretainmoreexem- learning process, we consistently achieve improved classi-
ε
plarsthaninstance-rareclasses. Ontheotherhand, thein- ficationaccuracyandoutperformexistingmethods.
creaseofimbalancefactorρresultsinanoticeabledegrada- Inter-phaseImbalance: Weevaluateeachofourcom-
tioninperformance. Despitethesechallenges,ourmethod ponentsincluding(a)theadditionofourDAKDloss,(b)the
is able to manage diverse learning environments posed by Decomposed Gradient Reweighting (DGR) to separately
)%(ycaruccACIFAR100-LT ρ=100 nε=20 Food101-LT ImageNetSubset-LT
ρ=100 ρ=50 ρ=10 LFS LFH
ROS[43] 36.32 41.28 55.12 DAKD DGR △(DAKD) N=10 N=20 N=5 N=10
✓ 21.42 18.93 41.68 40.35
FocalLoss[25] 𝝆=3𝟏8𝟎.𝟎9,1 𝒏𝜺=43𝟐.𝟎2,6 𝑵=55𝟏.𝟎08
✓ ✓ 26.08 24.51 47.79 46.37
LDAM[9] 40.82 45.68 57.32 ✓ (GR) ✓ 26.42 24.48 48.01 47.16
CBLoss[13] 39.62 46.29 57.29 ✓ ✓ ✓ 29.05 26.42 50.57 49.13
IBLoss[34] 43.62 46.80 58.01
Table3.AblationstudyonFood101-LTandImageNetSubset-LT
BSLoss[38] 44.12 49.25 59.38
EQLv2[42] 43.81 48.25 57.06 ImageNetSubset-LT(𝑵=𝟏𝟎,𝝆=𝟏𝟎𝟎,𝒏𝜺=𝟐𝟎)
CMO[35] 43.54 47.92 58.97 𝒍𝟐norm
Oursw/oRS 43.84 47.39 57.95
Ours 45.27 49.02 60.71
Table2. Long-tailedrecognitionaccuracy(%)onCIFAR100-LT
withimbalancefactorρ∈{100,50,10}
ImageNetSubset-LT ( LFS, 𝑵=𝟏𝟎) Food101-LT ( LFH, 𝑵=𝟏𝟎)
𝒍𝟐norm
class index class index
Figure 5. The Forgetting [29] (%) at each incremental step by
class index class index
comparingourproposedDAKDwithvariantsofdistillation. The Figure6. Thel normoflearnedweightvectorsafterincremen-
2
averageclassificationaccuracy(ACC)isshowninthelegend(•). tally learning N = 10 tasks on ImageNetSubset-LT under LFS
protocol.Theshadedareashowsthevariationsofeachcurve.
balance the gradients for new and learned tasks instead of
treating all classes together (GR) as for intra-phase im- bestACCbyfactoringlosttrainingdatadistribution.
balance, and (3) the significance of maintaining the dis- Biascorrectioneffects: Finally, weexaminetheeffec-
crimination of gradients from knowledge distillation loss tiveness of our method in rectifying the biased weights in
(△(DAKD))asdescribedinSection4.2.2. Theresultsare thefullyconnectedlayersunderCIL.Specifically,wecom-
summarizedinTable3,whereweobserveperformanceim- pare with Baseline (fine-tuning), BiC [46] and WA [53] to
provements contributed by each component. Notably, the showthevarianceofl normforweightvectorscorrespond-
2
GRexhibitsinferiorperformancecomparedtoDGRdueto ing to each class during CIL as shown in Figure 6. The
thepotentialunderfittingofnewclasseswhichreceivedin- Baseline method shows significant variation both within
sufficientemphasisduringCIL.Wealsoobserveimproved andbetweentasks.WhileBiCandWAaddressthevariation
performance by maintaining the discriminativeness of the betweentasksthroughpost-hocbiascorrection,theydonot
gradient from DAKD to capture and leverage information effectivelyresolvethevariationwithinindividualtasks.Our
fromtheoriginaltrainingdistribution,thuseffectivelypre- method,incontrast,achievesmoreuniformweightvectors
servinglearnedknowledge. across both intra-task and inter-task learning, contributing
Additionally, we assess the effectiveness of DAKD to toourbestoverallperformanceinimbalancedCIL.
mitigate forgetting under CIL by replacing it with exist-
6.Conclusion
ing variants of logit-based knowledge distillation loss in-
cluding(i)thebalancedknowledgedistillation(BKD)[51], In this work, we study class-incremental learning (CIL)
(ii)multi-levellogitdistillation(MKD)[21],(iii)decoupled whenappliedtoimbalanceddatatoaddressbothintra-phase
knowledgedistillation(DCKD)[54], and(iv)decomposed andinter-phaseimbalancesbyreweightingthegradientsin
knowledge distillation(DPKD) [4]. The results are visual- FC layer to foster a balanced optimization and learn unbi-
ized in Figure 5 (ρ = 100,n = 20) where we measure ased classifiers. Additionally, we introduce a distribution-
ε
theforgettingrate[29]ateachincrementallearningphase. aware knowledge distillation loss that dynamically modu-
Remarkably, we notice that the MKD, DCKD, and BKD lates the loss intensity in proportion to the extent of train-
achieve comparable or even worse results compared to the ingdataattritiontofurthermitigateimbalancedcatastrophic
original KD loss as they assume the student and teacher forgetting. Our method shows consistent improvements
model observe the same training distribution, which does under CIL and proves effective in long-tailed recognition.
not hold under CIL. While DPKD marks some improve- Overall,ourfindingsunderscoretheimportanceofaddress-
ments, its effectiveness remains hampered by imbalanced ingdataimbalanceinCILandpavethewayformorerobust
data.Overall,ourDAKDachievesthelowestforgettingand andequitableclass-incrementallearningmodels.
)%(
gnittegroFReferences [14] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas
Robert,andEduardoValle. Podnet: Pooledoutputsdistilla-
[1] Hongjoon Ahn, Jihwan Kwak, Subin Lim, Hyeonsu Bang,
tionforsmall-tasksincrementallearning.Proceedingsofthe
Hyojun Kim, and Taesup Moon. Ss-il: Separated softmax
European Conference on Computer Vision, pages 86–102,
forincrementallearning. ProceedingsoftheIEEE/CVFIn-
2020. 2,6,7
ternationalconferenceoncomputervision,pages844–853,
[15] Yiduo Guo, Bing Liu, and Dongyan Zhao. Dealing with
2021. 2,5,7
cross-taskclassdiscriminationinonlinecontinuallearning.
[2] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Lau-
ProceedingsoftheIEEE/CVFConferenceonComputerVi-
rent Charlin, Massimo Caccia, Min Lin, and Lucas Page-
sionandPatternRecognition,pages11878–11887,2023. 1,
Caccia. Onlinecontinuallearningwithmaximalinterfered
2,3
retrieval. Advances in Neural Information Processing Sys-
[16] Tyler L. Hayes and Christopher Kanan. Online continual
tems,32,2019. 2
learning for embedded devices. Conference on Lifelong
[3] RahafAljundi,MinLin,BaptisteGoujaud,andYoshuaBen- LearningAgents,2022. 1
gio. Gradient based sample selection for online continual
[17] JiangpengHeandFengqingZhu. Onlinecontinuallearning
learning. Advances in Neural Information Processing Sys-
via candidates voting. Proceedings of the IEEE/CVF Win-
tems,32,2019. 2
terConferenceonApplicationsofComputerVision(WACV),
[4] DonghyeonBaek,YoungminOh,SanghoonLee,Junghyup pages3154–3163,2022. 2
Lee, and Bumsub Ham. Decomposed knowledge distilla-
[18] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.
tionforclass-incrementalsemanticsegmentation. Advances
Deepresiduallearningforimagerecognition. Proceedings
inNeuralInformationProcessingSystems,35:10380–10392,
of the IEEE Conference on Computer Vision and Pattern
2022. 8
Recognition,pages770–778,2016. 6
[5] EdenBelouadahandAdrianPopescu.Il2m:Classincremen-
[19] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distill-
tallearningwithdualmemory. ProceedingsoftheIEEEIn-
ingtheknowledgeinaneuralnetwork. Proceedingsofthe
ternationalConferenceonComputerVision,pages583–592,
NIPS Deep Learning and Representation Learning Work-
2019. 2,7
shop,2015. 2,5
[6] Eden Belouadah, Adrian Popescu, Umang Aggarwal, and
[20] SaihuiHou,XinyuPan,ChenChangeLoy,ZileiWang,and
Le´oSaci. Activeclassincrementallearningforimbalanced
Dahua Lin. Learning a unified classifier incrementally via
datasets. EuropeanConferenceonComputerVision, pages rebalancing. ProceedingsoftheIEEEConferenceonCom-
146–162,2020. 2 puterVisionandPatternRecognition,pages831–839,2019.
[7] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. 2,6,7
Food-101–miningdiscriminativecomponentswithrandom [21] YingJin,JiaqiWang,andDahuaLin. Multi-levellogitdis-
forests. ProceedingsoftheEuropeanConferenceonCom- tillation.ProceedingsoftheIEEE/CVFConferenceonCom-
puterVision,2014. 6 puterVisionandPatternRecognition, pages24276–24285,
[8] MateuszBuda,AtsutoMaki,andMaciejAMazurowski. A 2023. 8
systematic study of the class imbalance problem in convo- [22] Chris Dongjoo Kim, Jinseo Jeong, and Gunhee Kim. Im-
lutional neural networks. Neural networks, 106:249–259, balancedcontinuallearningwithpartitioningreservoirsam-
2018. 3 pling. European Conference on Computer Vision, pages
[9] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, 411–428,2020. 2
and Tengyu Ma. Learning imbalanced datasets with label- [23] AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiple
distribution-awaremarginloss.Advancesinneuralinforma- layersoffeaturesfromtinyimages. TechnicalReport,2009.
tionprocessingsystems,32,2019. 3,8 6
[10] Francisco M. Castro, Manuel J. Marin-Jimenez, Nicolas [24] ZhizhongLiandDerekHoiem.Learningwithoutforgetting.
Guil,CordeliaSchmid,andKarteekAlahari. End-to-endin- IEEETransactionsonPatternAnalysisandMachineIntelli-
crementallearning.ProceedingsoftheEuropeanConference gence,40(12):2935–2947,2017. 2
onComputerVision,2018. 2,7 [25] Tsung-YiLin,PriyaGoyal,RossGirshick,KaimingHe,and
[11] Xiuwei Chen and Xiaobin Chang. Dynamic residual clas- Piotr Dolla´r. Focal loss for dense object detection. Pro-
sifier for class incremental learning. Proceedings of the ceedingsoftheIEEEinternationalconferenceoncomputer
IEEE/CVF International Conference on Computer Vision, vision,pages2980–2988,2017. 8
pages18743–18752,2023. 1,2,7 [26] Xialei Liu, Yu-Song Hu, Xu-Sheng Cao, Andrew D Bag-
[12] Aristotelis Chrysakis and Marie-Francine Moens. Online danov,KeLi,andMing-MingCheng. Long-tailedclassin-
continuallearningfromimbalanceddata.InternationalCon- crementallearning. EuropeanConferenceonComputerVi-
ferenceonMachineLearning,pages1952–1961,2020. 2 sion,pages495–512,2022. 2,4,6,7
[13] YinCui,MenglinJia,Tsung-YiLin,YangSong,andSerge [27] Yaoyao Liu, Yuting Su, An-An Liu, Bernt Schiele, and
Belongie. Class-balanced loss based on effective number Qianru Sun. Mnemonics training: Multi-class incremental
of samples. Proceedings of the IEEE/CVF conference on learningwithoutforgetting. ProceedingsoftheIEEECon-
computervisionandpatternrecognition,pages9268–9277, ferenceonComputerVisionandPatternRecognition,pages
2019. 3,6,7,8 12245–12254,2020. 2[28] YaoyaoLiu,BerntSchiele,andQianruSun.Adaptiveaggre- [41] JingruTan,ChangbaoWang,BuyuLi,QuanquanLi,Wanli
gationnetworksforclass-incrementallearning. Proceedings Ouyang, Changqing Yin, and Junjie Yan. Equalization
of the IEEE/CVF conference on Computer Vision and Pat- loss for long-tailed object recognition. Proceedings of
ternRecognition,pages2544–2553,2021. 2 the IEEE/CVF conference on computer vision and pattern
[29] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient recognition,pages11662–11671,2020. 3
episodic memory for continual learning. Advances in neu- [42] JingruTan,XinLu,GangZhang,ChangqingYin,andQuan-
ralinformationprocessingsystems,pages6467–6476,2017. quanLi. Equalizationlossv2: Anewgradientbalanceap-
1,2,6,8 proach for long-tailed object detection. pages 1685–1694,
[30] DavideMaltoniandVincenzoLomonaco.Continuouslearn- 2021. 3,8
inginsingle-incremental-taskscenarios. NeuralNetworks, [43] JasonVanHulse,TaghiMKhoshgoftaar,andAmriNapoli-
116:56–73,2019. 2 tano. Experimental perspectives on learning from imbal-
[31] Marc Masana, Xialei Liu, Bartłomiej Twardowski, Mikel anceddata.Proceedingsofthe24thinternationalconference
Menta, Andrew D Bagdanov, and Joost Van De Weijer. onMachinelearning,pages935–942,2007. 3,8
Class-incrementallearning: surveyandperformanceevalu- [44] Fu-Yun Wang, Da-Wei Zhou, Han-Jia Ye, and De-Chuan
ationonimageclassification. IEEETransactionsonPattern Zhan. Foster: Featureboostingandcompressionforclass-
AnalysisandMachineIntelligence,45(5):5513–5533,2022. incrementallearning. ProceedingsoftheEuropeanConfer-
6 enceonComputerVision,pages398–414,2022. 2,7
[32] MichaelMcCloskeyandNealJCohen. Catastrophicinter- [45] MaxWelling.Herdingdynamicalweightstolearn.Proceed-
ference in connectionist networks: The sequential learning ingsoftheInternationalConferenceonMachineLearning,
problem. pages109–165.Elsevier,1989. 1 pages1121–1128,2009. 2
[33] Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh [46] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye,
Rawat, Himanshu Jain, Andreas Veit, and Sanjiv Kumar. Zicheng Liu, Yandong Guo, and Yun Fu. Large scale in-
Long-taillearningvialogitadjustment. InternationalCon- crementallearning. ProceedingsoftheIEEEConferenceon
ferenceonLearningRepresentations,2021. 3,5 ComputerVisionandPatternRecognition,2019.1,2,5,7,8
[34] Seulki Park, Jongin Lim, Younghan Jeon, and Jin Young [47] ShipengYan,JiangweiXie,andXumingHe. Der: Dynam-
Choi. Influence-balanced loss for imbalanced visual clas- icallyexpandablerepresentationforclassincrementallearn-
sification. ProceedingsoftheIEEE/CVFInternationalCon- ing.ProceedingsoftheIEEE/CVFConferenceonComputer
ferenceonComputerVision,pages735–744,2021. 3,8 VisionandPatternRecognition,pages3014–3023,2021. 2
[35] SeulkiPark,YoungkyuHong,ByeonghoHeo,SangdooYun, [48] LuYang,HeJiang,QingSong,andJunGuo. Asurveyon
and Jin Young Choi. The majority can help the minority: long-tailedvisualrecognition.InternationalJournalofCom-
Context-richminorityoversamplingforlong-tailedclassifi- puterVision,130(7):1837–1872,2022. 2
cation. ProceedingsoftheIEEE/CVFConferenceonCom- [49] Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju
puter Vision and Pattern Recognition, pages 6887–6896, Hwang.Lifelonglearningwithdynamicallyexpandablenet-
2022. 3,8 works. International Conference on Learning Representa-
[36] Ameya Prabhu, Philip HS Torr, and Puneet K Dokania. tions,2018. 2
Gdumb: A simple approach that questions our progress in [50] Friedemann Zenke, Ben Poole, and Surya Ganguli. Con-
continuallearning.ProceedingsoftheEuropeanConference tinuallearningthroughsynapticintelligence. International
onComputerVision,pages524–540,2020. 2 ConferenceonMachineLearning,pages3987–3995,2017.
[37] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg 2
Sperl,andChristophH.Lampert. iCaRL:Incrementalclas- [51] Shaoyu Zhang, Chen Chen, Xiyuan Hu, and Silong Peng.
sifierandrepresentationlearning. ProceedingsoftheIEEE Balanced knowledge distillation for long-tailed learning.
Conference on Computer Vision and Pattern Recognition, Neurocomputing,527:36–46,2023. 8
2017. 1,2,6,7 [52] YifanZhang,BingyiKang,BryanHooi,ShuichengYan,and
[38] Jiawei Ren, Cunjun Yu, Xiao Ma, Haiyu Zhao, Shuai Yi, Jiashi Feng. Deep long-tailed learning: A survey. IEEE
et al. Balanced meta-softmax for long-tailed visual recog- TransactionsonPatternAnalysisandMachineIntelligence,
nition. Advancesinneuralinformationprocessingsystems, 2023. 2
33:4175–4186,2020. 3,8 [53] Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shu-
[39] OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,San- Tao Xia. Maintaining discrimination and fairness in class
jeevSatheesh,SeanMa,ZhihengHuang,AndrejKarpathy, incrementallearning. ProceedingsoftheIEEEConference
AdityaKhosla,MichaelBernstein, AlexanderC.Berg, and onComputerVisionandPatternRecognition,pages13208–
LiFei-Fei. ImageNetLargeScaleVisualRecognitionChal- 13217,2020. 1,2,5,6,7,8
lenge. International Journal of Computer Vision, 115(3): [54] Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, and Jiajun
211–252,2015. 6 Liang. Decoupled knowledge distillation. Proceedings of
[40] ChristianSimon,PiotrKoniusz,andMehrtashHarandi. On the IEEE/CVF Conference on computer vision and pattern
learning the geodesic path for incremental learning. Pro- recognition,pages11953–11962,2022. 5,8
ceedings of the IEEE/CVF conference on Computer Vision
andPatternRecognition,pages1591–1600,2021. 2