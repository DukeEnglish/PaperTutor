Diffusion Language Models Are Versatile Protein Learners
XinyouWang*♢♡ ZaixiangZheng*♡ FeiYe♡ DongyuXue♡ ShujianHuang♢ QuanquanGu♡
Abstract guageshavelongbeenrecognized(Yangetal.,2019;Ferruz
&Ho¨cker,2022). Drawinginspirationfromtheremarkable
Thispaperintroducesdiffusionproteinlanguage
progressinNLPachievedbylanguagemodels(LMs;Devlin
model (DPLM), a versatile protein language
etal.,2019;Radfordetal.,2018;OpenAI,2023)thanksto
model that demonstrates strong generative and
thescalabilityofTransformers(Vaswanietal.,2017)and
predictivecapabilitiesforproteinsequences. We
theexistenceoflarge-scaletextdata,recentexplorationsin
firstpre-trainscalableDPLMsfromevolutionary-
proteinhasalsodemonstratedtheimpressivecapabilitiesof
scaleproteinsequenceswithinagenerativeself-
proteinlanguagemodels(Rivesetal.,2019;Linetal.,2022;
superviseddiscretediffusionprobabilisticframe-
Huetal.,2022),learnedfromtheuniverseofevolutionary-
work,whichgeneralizeslanguagemodelingfor
scale protein sequences. As a result, protein LMs have
proteinsinaprincipledway. Afterpre-training,
becomeoneofthemostimportantcornerstonesinAIfor
DPLM exhibits the ability to generate struc-
proteinresearch,servingapivotalrolenotonlyinpredictive
turally plausible, novel and diverse protein se-
tasks (e.g., probing functional properties, and predicting quences for unconditional generation. We fur-
protein structures from single sequences without explicit
therdemonstratetheproposeddiffusiongenera-
evolutionary homologs) but also in generative tasks (e.g.,
tive pre-training make DPLM possess a better
redesigningsequencesgivenproteinbackbonestructures,or
understanding of proteins, making it a superior
synthesizingcompletelynewproteinsequences).
representation learner, which can be fine-tuned
forvariouspredictivetasks,comparingfavorably While current protein LMs have made significant strides,
toESM2(Linetal.,2022).Moreover,DPLMcan theyhavenotyetreachedtheirfullestpotential. Oneofthe
betailoredforvariousneeds,whichshowcasesits fundamentalproblemsisrootedinthewidely-usedpretrain-
prowessofconditionalgenerationinseveralways: ingobjectives,i.e.,maskedpredictionvs.autoregression:
(1)conditioningonpartialpeptidesequences,e.g., (i) For masked prediction, masked language models
generating scaffolds for functional motifs with (Masked-LMs, e.g., ESM family; Rives et al., 2019;
highsuccessrate;(2)incorporatingothermodal- Linetal.,2022)excelinsequenceunderstandingfor
ities as conditioner, e.g., structure-conditioned proteinpredictivetasks,thankstotheirbi-directional
generationforinversefolding;and(3)steeringse- receptivefield. However,Masked-LMsareunableto
quencegenerationtowardsdesiredproperties,e.g., performproteinsequencegeneration,duetothelack
satisfyingspecifiedsecondarystructures,through ofawell-definedformulationforgenerativemodeling.
aplug-and-playclassifierguidance. Wefurtherpostulatethatthiscouldevencaptheirpre-
dictivepower,sinceapowerfulgenerativemodelthat
1 Introduction
cancreatenewsamplesbylearningtheunderlyingdata
Proteins, whichare3D-foldedlinearsequencesofamino distribution, is expected to simultaneously acquire a
acids, play a pivotal role in regulating various biological deep understanding of the data. As a famous quote,
functions,includingtranscription,translation,signaling,and “whatyoucannotcreate,youdonotunderstand.”
thecontrolofthecellcycle. Recently,thepromiseoflearn- (ii) For autoregression, autoregressive language mod-
ingtounderstandanddesignproteinsviadata-drivengener- els (AR-LMs, e.g., ProGen; Nijkamp et al., 2022),
ativedeeplearninghasinitiatedasignificantparadigmshift albeit good at generation, often fall short in under-
apartfromthelong-establishedphysics-basedmethods. standingsequencedata(Radfordetal.,2018)includ-
Theanalogiesbetweenproteinsequencesandhumanlan- ingproteins(Elnaggaretal.,2021). Moreimportantly,
proteinsarestructuralmacromoleculesratherthansim-
*Equal contribution ♢Dept. of Computer Science, Nanjing ple linear strings. Consequently, while effective as
University (this work was done during Xinyou’s internship at
ByteDanceResearch)♡ByteDanceResearch.Correspondenceto:
aninductivebiasfortext,AR-LMsareconstrainedby
theiruni-directionalreceptivefield,onlyaccessingone-
QuanquanGu<quanquan.gu@bytedance.com>.
sided sequence context. This limitation stems from
Preprint.Copyright2024bytheauthor(s).
1
4202
beF
82
]GL.sc[
1v76581.2042:viXraDiffusionLanguageModelsAreVersatileProteinLearners
A evolutionary scale UniRef-50 (~4 x107) length: 1000
Tranformer Layer x N pLDDT: 89.66
diffusion protein LM
pre-training MLP
DPLM
& unconditional Bidirectional pre-training on
generation Multihead ATTN reverse denoising generation evolutionary scale
bioRxiv preprint doi: https://doi.org/10.1101/2022.07.20.500902; this version posted December 21, 2022. The copyright holder for this preprint sequence data
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
bioRxiv preprint doi: https://doi.org/10.1101/2022.07.20.500902; this version posted December 21, 2022. The copyright holder for this preprint available under aCC-BY-NC-ND 4.0 International license. (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made bioRxiv preprint doi: https://doi.org/10.1101 a/ v2 a0 il2 a2 b. l0 e7 u.2 n0 d. e5 r0 a0 C90 C2 -B; t Yh -is N Cve -Nrs Dio n 4 .p 0o Is nt te ed rn D ae tioce nm al b lie cr e 2 n1 se, .2022. The cxo(Tpy)right hold ⋯er for this prexp(tr)int x(t-1) ⋯ x(0)
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
Evolutionary-scalaevparileadbilcet uionndeorf aaCtoCm-BicY-lNevCe-lNpDr o4t.e0i nInstetrrnuacttiuornealw liictehnasel.anguagemodel
ESMFold
Evolutionary-scalepredictionofatomiclevelproteinstructurewithalangu✘ag✘e✘m✘✘od✘e✘l✘✘✘ ✘K✘✘RQE✘✘KY✘A MKT✘RQE✘✘KYRA MKTVRQERLKYRA
Evolutionary-scalepredictionofatomiclevelproteinstructurewithalanguagemodel
forward discrete diffusion
B learned representation C conditional generation
for predictive tasks
(1) conditioned on partial sequence (2) conditioned on other modality
e.g., motif-conditioned scaffolding e.g., structure-conditioned inverse folding
seq-level classification
residue-level classification DPLM DPLM
{ ✘✘✘✘✘RQER✘✘✘
sequence-level regression MTKYAKRQERYAR Adapter MKTAYRVA
contact prediction MKTVRQERLKYRA Transformer
⋯⋯ Layer x N
DPLM
✘✘✘✘✘✘✘✘
(3) controllable generation with discrete classifier guidance
e.g., secondary structure guided generation sec. struct. ESMFold
sequence classifier
embedding
DPLM guide
x(0)
MKTYRKYVA
template secondary
MKTVRQERLKYRA template protein
structure annotations
✘✘✘✘✘✘✘✘
Figure1.Overallillustrationof DPLM.(A):modeling,pre-trainingandunconditionalgeneration;(B):proteinsequencerepresentationfor
predictivetasks;(C):conditionalgeneration,including(1)sequenceconditioning(e.g.,motif-scaffolding),(2)cross-modalconditioning
(e.g.,inversefolding),and(3)plug-and-playcontrollablegenerationwithdiscreteclassifierguidance(e.g.,secondarystructure).
capturing the complex global interactions of amino Thesemakediffusionmodelsanappealinggenerativefoun-
acids,therebyhinderingbothgenerativeandpredictive dationforproteinlanguagemodeling. However,directlyap-
capabilitiesofproteinLMs. plyingconventionalGaussiandiffusiontoproteinsequences
Thishighlightsthedemandforageneral-purposeandver- necessitatesadditionalcontinuousrelaxations(Lisanzaetal.,
satileproteinLMthatcombinespredictiveandgenerative 2023),whichdoesnotfitthediscretenatureofproteinse-
capabilities. Providedtheaforementionedanalysis,werea- quenceandhasnotyetprovensuccessfulinpractice.
sonthat,thekeyingredientsforsuchaversatileproteinLM Inthispaper,wepresentdiffusionproteinlanguagemodel
liein(1)strong&scalablegenerativemodelingframework (DPLM), a novel approach aimed at achieving a unified
to best digest the universe of massive protein sequences; andversatileproteinLMthroughdiffusiongenerativepre-
and (2) bi-directional receptive field for better modeling trainingonevolutionary-scaleproteinsequences. DPLM
residue-wiseglobalinteractions. isgroundedinadiscretediffusionprobabilisticframework,
On the other hand, diffusion models (Ho et al., 2020; servingasaprincipledgenerativegeneralizationoflanguage
Song et al., 2020) have shown great success in generat- modeling. Duringpre-training, DPLM istaskedwithde-
ingcontinuousdata,especiallyinrenderingphotorealistic noisingtheinputproteinsequenceatdifferentnoiselevels,
images(Rombachetal.,2021,interalia). Theyhavefur- ranging from completely noisy to clean ones, enforcing
thermanifestedincredibleachievementinmodelingprotein DPLMtobestthemodelcomplexintrinsicdependencies
structures (Yim et al., 2023; Watson et al., 2023; Ingra- ofaminoacidsequences. Afterpre-training,DPLMcanbe
hametal.,2023). Thiscanbeattributedtotheirfavorable usedforproteinsequencegenerationandprovidingeffec-
Figure2.SinglesequencestructurepredictionwithESMFold.(A)ESMFoldmpordoeplaerrcthieitsecotufrne.oAn-rarouwtosrsehgorwesthsievienfdoermnoatiisoinngflogweninerationwith tive representations for downstream predictive tasks. We
Fthigeunreetw2.oSriknfgrloemsetqhueelnacnegustarguectmuroedperletdoicthtieonfowldiitnhgEtSruMnFkotlod.th(eAs)tEruScMtuFreolmdomdioutdeleeralwtahirviccehhitroeecufittupnrueetm.sA3eDrnrotcwaoonsrdsdhigonwlaotetbhsaealnirndefoccroemnpfiatitdiveoenncfifleeos.wld(B.in)Besides,de- highlightourcontributionsasfollows:
FEthiSegMunreFetow2ld.oSrpkirnofgdrlouemcseestqhauececlnaucnreagtusetaraguteocmtmuirocedrpeerlseotdoluictthitoieonfnopwrldeiditnhicgEtitoSrunMsn,Fkwotloidth.th(seAims)tiErlauSrcMtaucFrceoulrmdacomydnouto odleeiR slwo ianhsreicgcththa aitF uoeouctl ottduperuo netn.scCAo3ADdrriMoncwoE gosOrhsd.haiWonswah ateetn lhsoeMannignSdfAohcsorimsna trfiaoetdiraeoybnnlcfafleotesord.wr(fBeoinpr)resentation • WeproposeDPLM,aversatileproteinLMunderdis-
AEthSlepMhnaFeFtowoldlodrpkaronfddrouRmceosstheatectaclaFunroagltdue,aapgteoermfmoirocmdreealsnotcoleuttohifoenthfpoerlmdedionicdgteitolrsnudsn,ekgwtroiathdthessei.msStirlcauarctttaeucrrc-epulrmoatcosydc lut eool ameR rwp noa ihsr neiect gtEha (FSo VoMul itd nFpouo clnt eds nC3( txAD e-Ma tcxo aEiso lO) .r ,d.p 2iWrne 0adh 1tieec 0nst ,iMao inn nSds tA ewc rsoitna ahrfi leA id aaelb )pn ,lhca waetFes hd.o i(lf ldBo e2r) recentstud- cretediffusionframework,withmodelsizeupto3B,
A(EySl-paMhxaFiFso)ol,dldcpoarlonoddreuRdceobssyeatcltaacFnugroaultdae,gapeteormmfooircdmerealsnpoceleurptoilfoenxthipetyrme.doPicdrtoeitloesnidsn,esgwwriatidhthessil.moSwiclaaprtetaercrpc-lpuelrxoaitctsyyctsoocmoRrpoeasrseeitmtEaiFSloaMrldlFyootlnodC(AxAl-pMahxEaisFO)o.plWrde2dh.iec(ntCiMo)nSMsAwosditaehrleApalLbpDlhaaDtFeTdolfdvo2sr.
ieshaveverifiedthatdiffusion-basedgenerativemodelscan pre-trained on evolutionary-scale protein sequences.
(Atryul-peahxLaisFD)o,DlcdToal(noldreefRdt)obasynedtltaarnFegolaultdai,vgpeeeprmeforofrdomeralmnpacenercpoelfeaxthgietayim.nsPotdrAoetllespihdnaesFgwroaliddthe(srl.oigSwhctap)tetoernrp-lpCelxAoittMsycEsocOmo.rpepaLrseiDmEDiSlTaMrilFsyoatlodw(Aexll-lpachxaaislFi)boprlradet2ed.dic(etCiso)tnimMswaotdieteholfAppLlprDehdaDiFcTotilovdn2s.
(atrycu-ceauxrLiasDc)y,D.cT(oDl(o)lerTefotd)pbasynhdolawrnesglatuetaisvgte-espemteropfdroeerdlmipcatenirocpnelseaxogiftayEi.nSsPMtrAoFtloeplihdnasiFnwotleidtahl(,rlgoigwrhotup)neodrnptlrCeuxtAhitMyinbEsge cOroae.ryefp,fLsae iDnc mdtDiiAlvTaelrpilsyhsaae tFolwfoA-ledsllul2pcp hpaaerlFeirdbovirlicadstt2ieeo.ddn(seClsie )tnaimMgrnraoete deerneso.lfPp(pC LinrDehkdDesinhcTtoiweovtnss.al., 2024a). We further develop multiple conditioning strategies
latorcuwceuprLarDecdyDi.cT(tDed()leLTfDot)pDasTnhdofowrreslbatoetitsvhte-EspeSteMrpfrFoeordmlidcatanioncdnesAaoglfpahEinaSsFMtoAlFdlo2pl.hdBaiFonotttleodaml(,rsgihgrohowut)nsodcnotrmCutpAhlMeixnEpgOrrea.dypi,cLatiDnodnDsATolpnishaaaFdwiomledle2lrcp(a7rlLeidbQircMattie)odnansedsintaimgteraettreeanmo.fePrpin(r7ekdQsiYhcotMiwo)ns;
ElaocSwcMuprFareocdyldi.c(ptDerde)dLTicDotpiDosnThsofaowrresbctoeotshlto-ErseeSdtMpbrFyeodclihdcataiinnodnIsDAolafpnhEdaSFoMovelFdrol2al.didBionotnttoegamrlo,sughnroodwutnsrudctohtrmu(gpthrlaeiyxn)p.grDreadoyic,ckatiQnodn(s3A9ol)pnhsacaoFdrioemlsde2arr(pe7rrLeedQpiocMrtit)oednansfdoinratgthereetreiannmt.eePrrainc(7tkiQosnYhsoM;wi)ns;
tElhoSweMcpaFrseoedldoicfptetrhdeedLitcDettiDroanTmsfeaorrre7bQcooYthloMEreS,dtMhbeFyoscclhdoaraiennidIsDAthlaepnhadavFoevoraeldgrl2ea.iodBfoostnctoogrmreossuhonvodwetrrsuinctohtemr(gapcrlateiyxn)gp.rDcehdoaiccikntQi-opna(s3ir9os).nsacodriemsearr(e7rLeQpoMrt)edanfdorattheetrianmteerrac(7tiQonYsM;i)n; 2
tEhSeMcaFsoeldofptrheeditcettiroanmsearre7QcoYloMre,dthbeyscchoarieniIsDthaenadvoevraegrleaiodfosncogrreosuonvdetrriunthter(agcratiyn)g.DchoaciknQ-pa(3ir9s).scoresarereportedfortheinteractions;in
thecaseofthetetramer7QYM,thescoreistheaverageofscoresoverinteractingchain-pairs.
5
5
5
The copyright holder for this preprint this version posted December 21, 2022. ; https://doi.org/10.1101/2022.07.20.500902 doi: bioRxiv preprint
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made The copyright holder for this preprint this version posted December 21, 2022. ; https://doi.org/10.1101/2022.07.20.500902 doi: bioRxiv preprint . CC-BY-NC-ND 4.0 International license available under a (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
. CC-BY-NC-ND 4.0 International license available under a
Evolutionary-scalepredictionofatomiclevelproteinstructurewithalanguagemodel
Evolutionary-scalepredictionofatomiclevelproteinstructurewithalanguagemodel
SinglesequencestructurepredictionwithESMFold. Figure2.(A)ESMFoldmodelarchitecture.Arrowsshowtheinformationflowin
thenetworkfromthelanguagemodeltothefoldingtrunktothestructuremodulewhichoutputs3Dcoordinatesandconfidences.(B) SinglesequencestructurepredictionwithESMFold. Figure2.(A)ESMFoldmodelarchitecture.Arrowsshowtheinformationflowin
ESMFoldproducesaccurateatomicresolutionpredictions,withsimilaraccuracytoRosettaFoldonCAMEO.WhenMSAsareablatedfor thenetworkfromthelanguagemodeltothefoldingtrunktothestructuremodulewhichoutputs3Dcoordinatesandconfidences.(B)
AlphaFoldandRosettaFold,performanceofthemodelsdegrades.Scatter-plotscompareESMFold(x-axis)predictionswithAlphaFold2 ESMFoldproducesaccurateatomicresolutionpredictions,withsimilaraccuracytoRosettaFoldonCAMEO.WhenMSAsareablatedfor
(y-axis),coloredbylanguagemodelperplexity.ProteinswithlowperplexityscoresimilarlytoAlphaFold2.(C)ModelpLDDTvs. AlphaFoldandRosettaFold,performanceofthemodelsdegrades.Scatter-plotscompareESMFold(x-axis)predictionswithAlphaFold2
trueLDDT(left)andrelativeperformanceagainstAlphaFold(right)onCAMEO.pLDDTisawellcalibratedestimateofprediction (y-axis),coloredbylanguagemodelperplexity.ProteinswithlowperplexityscoresimilarlytoAlphaFold2.(C)ModelpLDDTvs.
accuracy.(D)Topshowstest-setpredictionsofESMFoldinteal,groundtruthingray,andAlphaFold2predictionsingreen.Pinkshows trueLDDT(left)andrelativeperformanceagainstAlphaFold(right)onCAMEO.pLDDTisawellcalibratedestimateofprediction
lowpredictedLDDTforbothESMFoldandAlphaFold2.Bottomshowscomplexpredictionsonadimer(7LQM)andatetramer(7QYM); accuracy.(D)Topshowstest-setpredictionsofESMFoldinteal,groundtruthingray,andAlphaFold2predictionsingreen.Pinkshows
ESMFoldpredictionsarecoloredbychainIDandoverlaidongroundtruth(gray).DockQ(39)scoresarereportedfortheinteractions;in lowpredictedLDDTforbothESMFoldandAlphaFold2.Bottomshowscomplexpredictionsonadimer(7LQM)andatetramer(7QYM);
thecaseofthetetramer7QYM,thescoreistheaverageofscoresoverinteractingchain-pairs. ESMFoldpredictionsarecoloredbychainIDandoverlaidongroundtruth(gray).DockQ(39)scoresarereportedfortheinteractions;in
thecaseofthetetramer7QYM,thescoreistheaverageofscoresoverinteractingchain-pairs.
55DiffusionLanguageModelsAreVersatileProteinLearners
coveringvarioususeneeds,especiallydiscreteclassi- autoencodingmanner,
fierguidanceforcontrollablegeneration. Asaresult,
E logp (x)=E (cid:80) b ·logp (x |x¯ ), (1)
DPLM combines the best of both worlds, i.e., the q(x) θ q(x) 1≤i≤L i θ i m
scalable expressiveness of language models and the
stronggenerativepowerofdiffusionmodels,serving
where b i = 1 x¯i=[X] derived from a fixed chance (e.g.,
widely-adopted 15%) of masking x with a special mask
asaversatilebiologicalfoundationmodel(Fig.1,§3).
symbol [X], resulting in the masked observation x¯ . A
m
• WeshowthatDPLMiscapableofgeneratinghighly
per-tokenconditionalindependenceassumptionismadeas
structurally plausible (i.e., averaged pLDDT > 80),
well. Masked-LMssignificantlyexceltheperformanceofa
novelanddiverseforunconditionalproteinsequence
widerangeofsequenceunderstandingtasksforbothnatural
generation,suggestingthatDPLMwellcapturesthe
languageandprotein. However,itsbidirectionalitynature
universeofproteinsequencedata(Fig.1A,§4.1).
makesitdifficulttoapplytosequencegeneration.
• WedemonstratethatDPLMunderstandsproteinbetter,
Autoregression. AR-LMsareprevailingintherealmse-
servingasasuperiorrepresentationlearner,whichcan
quencegeneration(OpenAI,2023;Nijkampetal.,2022),
befine-tunedforvariousdownstreamtasks,comparing
whichadoptsasequentialfactorizationoverthesequenceus-
favorablywithwidely-usedproteinsequenceencoder
ingtheprobabilitychainrule.Inthiscase,thelog-likelihood
models,e.g.,ESM-2(Linetal.,2022)(Fig.1B,§4.2).
ofsuchmodelsismaximizedoverthedatasetgivenby:
• DPLMcanbefurtherexploitedforconditionalgener-
ationforavarietyofneeds: DPLMcan(1)condition E logp (x)=E (cid:80) logp (x |x ), (2)
q(x) θ q(x) 1≤i≤L θ i <i
onpre-specifiedpartialsequence,e.g.,scaffoldingfor
functional motifs with high success rate; (2) incor- wherecausalmaskingisusedtoensuresequentialdepen-
porateothermodalitiesasconditions, e.g., structure- dencystructure. TosamplefromAR-LMs,itrequiresances-
conditionedgenerationforinversefolding;(3)generate tralsamplingforLiterativestepsfromx 1 ∼p θ(x 1),x 2 ∼
proteinsequencestowardsdesiredpropertieswithplug- p θ(x 2|x 1)towardsx L ∼p(x L|x 1,...,x L−1)inastrictleft-
and-playclassifier-guidance,e.g.,steeringDPLMto to-rightunidirectionalmanner.
synthesizeproteinsthatsatisfyarbitraryuser-defined
2.2 DiffusionProbabilisticModels
secondarystructureannotations(Fig.1C,§4.3).
Diffusion models (Sohl-Dickstein et al., 2015; Ho et al.,
2020; Song et al., 2020) are a class of generative models
characterized by a pair of Markov processes, i.e., a for-
2 Preliminaries
warddiffusionprocessandabackwarddenoisingprocess.
Theforwardprocessq(x(1:T)|x(0))=(cid:81)T q(x(t)|x(t−1))
2.1 LanguageModelingforProtein t=1
graduallyperturbthedatax(0) ∼q(x(0))intoastationary
Language modeling aims to estimate the underlying dis- distributionx(T) ∼ q withT increasinglynoisysteps
tribution x ∼ q(x) of the sequence data of our interest, x(1:T) =x ,...,x(t−n 1o )is ,e x(t),...,x(T). Thelearnedback-
1
e.g., text or protein sequence, by learning a probabilis- ward process p (x(0:T))=p(x(t))(cid:81)T p (x(t−1)|x(t)),
tic model p (x). Here the language model (LM) θ is pa- θ t=1 θ
θ reversely,graduallydenoisesthesamplestowardsthedata
rameterizedbyaneuralnetwork,inparticularTransform- distribution. Tofitthemodelp (x(0))tothedatadistribu-
θ
ers(Vaswanietal.,2017),whichhavebecomethedefacto tionq(x(0)),thedenoisermodelistypicallyoptimizedby
choicedominatingdifferentdomainswithscalableandper-
thevariationalboundofthelog-likelihood(Hoetal.,2020):
forming expressiveness. In this work, we are interested
in language modeling for protein sequences, for which E (cid:2) logp (x(0))(cid:3) ≥E (cid:20) log p θ(x(0:T)) (cid:21)
x = (x 1,x 2,...,x L) ∈ {0,1}L×|V| is a sequence com- q(x(0)) θ q(x(0:T)) q(x(1:T)|x(0))
(cid:104)
posing L elements, V is the vocabulary within a discrete =E logp (x(0)|x(1))+const.
q(x(0)) θ
datasupportof20aminoacidsV ={1,...,20}. Onething (cid:80)T −KL(cid:2) q(x(t−1)|x(t),x(0))∥p (x(t−1)|x(t))(cid:3)(cid:105) .
wemostcareaboutisthegenerativeandrepresentational t=2 θ
capabilities of protein LMs. Here we review the typical (cid:124) (cid:123)(cid:122) (cid:125)
Jt
probabilisticparadigmsforlanguagemodeling,i.e.,masked
predictionandautoregression,andtheirprosandconsas Afterwards,itgeneratesbyfirstsamplingfromq (x(T)),
noise
thefoundationforproteinLMs,asfollows. followedbyiterativedenoisingwithp (x(t−1)|x(t)).
θ
Masked Prediction. Masked language models (Masked-
3 DPLM:AVersatileProteinLM
LMs or MLMs), e.g., BERT (Devlin et al., 2019) and its
variants for protein sequence (ESM family, Rives et al., Motivation. Continuous diffusion with Gaussian pertur-
2019;Linetal.,2022),employabidirectionaltransformer bation kernel has demonstrated impressive performance
totakeintoaccountboththeleftandrightcontexttopre- in generating continuous data in Euclidean space (Rom-
dict the masked (amino acid) symbols in a mask-predict bach et al., 2021; Ho et al., 2022), and the more general
3DiffusionLanguageModelsAreVersatileProteinLearners
Riemannianmanifolds(DeBortolietal.,2022). Recently, twocategoricalsintoreweightedcross-entropies:
continuous diffusion has shown to rival in modeling pro- J =E −KL(cid:2) q(x(t−1)|x(t),x(0))∥p (x(t−1)|x(t))(cid:3)
teinstructures(Watsonetal.,2023;Ingrahametal.,2023,
t q(x(0)) θ
(cid:104) (cid:105)
interalia),whereinitsbidirectionalreceptivefield iside- =E
q(x(0))
λ(t)(cid:80) 1≤i≤Lb i(t)·logp θ(x( i0)|x(t)) , (4)
ally suited for modeling residue-wise global interactions.
Thismotivatesustoblenddiffusionmodels,whicharewell-
whereλ(t)isaweightingcoefficientinducedfromthespe-
suitedforproteinasdiscussedabove,andlanguagemodels, cificnoisingschedule.Eq.(4)revealsthatMasked-LMs(i.e.,
whicharewellknownasscalableandexpressivesequence x(t) ≜ x¯ m inEq.(1))and AR-LMs(i.e.,x(t) ≜ x <t and
learners.ThisleadstoourpursuitofadiffusionproteinLM, b i ≜ 1 in Eq. (2)) can be considered as special cases in
takingthebestofbothworlds. thisgeneralizedformofdiscretediffusionLMs,contingent
ontheirrespectivespecificationsofthenoise-inducedcon-
Adirectuseofcontinuousdiffusion,however,isnotnec-
figurations. Asaresult,theprocessoflearningaccording
essarily the best choice for modeling discrete sequence
toEq.(4)inherentlyencapsulatesbothMasked-LMsand
data(Lietal.,2022;Dielemanetal.,2022;Lisanzaetal.,
2023),duetothepitfallofdiscretenessthatmakesGaussian
AR-LMswithintheambitoftheproposedDPLM.
diffusionhardlymodelthediscretenatureofsequencedata Evolutionary-scale Pre-training. The pre-training pro-
inembeddingspace(Yeetal.,2023b). Tothisend,discrete cedurefor DPLM utilizestheUniRef50database(Suzek
diffusion(Hoogeboometal.,2021b;Austinetal.,2021)that etal.,2015),whichcomprisesaround45millionproteinse-
directlyoperatesoverthediscretestatespace, becomesa quences,totalingabout14billionaminoacidtokens. Inthe
morewell-suitedprobabilisticmodelforproteinsequences. caseofexceedinglylengthyproteinsequences,weemulate
ESM2(Linetal.,2022)bytruncatingtheseproteinstoa
3.1 ProteinLanguageModelingw/ DiscreteDiffusion
randomsequenceof1024tokens. Besides,weadheretothe
Modeling. LetCat(x;p)beacategoricaldistributionon settingformodelarchitectureandscalesasESM2,which
proteinsequencexparameterizedbyavectorpon(|V|−1)-
correspondto DPLM withsizesof150M,650Mand3B.
dimensionalprobabilitysimplex. Theforwardprocessof Wetrainallmodelsfor100Kupdates,employingbatchsize
discretediffusiondefinesaMarkovprocessgovernedbythe of320Kfor150Mmodel,while1Mtokensfor650Mand
transitionkernel: 3Bmodels.
q(x(t)|x(t−1))=Cat(cid:0) x(t);β x(t−1)+(1−β )q (cid:1) , Generation. GivenatrainedDPLM,itcansynthesizenew
t t noise amino acid sequences by the reverse iterative denoising
whereq istheprobabilityvectorofstationarydistribu- process of discrete diffusion (Hoogeboom et al., 2021b;
noise
tionq (x(t)),i.e.,q(x(t)) = Cat(x(t);p = q ),and Austinetal.,2021). Formally,discretediffusionsamples
noise noise
0 ≪ β < 1 is the noise schedule controlling the degree fromthefollowingdistribution,
t
of corruption at timestep t. In this case, the distribution p (x(t−1)|x(t))=(cid:80) q(x(t−1)|x(t),xˆ )p (xˆ |x(t)).
ofcorruptedsamplex(t) givenitsoriginaldatax(0) hasa
θ xˆ0 0 θ 0
closed-formexpression:
Inparticular,attimet,wefirstgeneratexˆ 0fromp θ(·|x(t)),
thenalessnoisyx(t−1) issampledbyq(·|x(t),x(0) =xˆ )
0
q(x(t)|x(0))=Cat(cid:0) x(t);α tx(0)+(1−α t)q noise(cid:1) , (3) givenx(t)andxˆ 0.ThisprocessisrepeatedfromT to1.The
generativedenoisingprocessofDPLMcanbeviewedasan
whereα
=(cid:81)t
β suchthatlim α →0,whichpre- iterativemask-predictapproach. Specifically,thestartingse-
t i=1 i t→T t
servesnoinformationfromthedataandconvergestothe quenceisinitializedas100%-noisystate(i.e.,all[X]’s).At
stationarydistributionq attimestepT. Thisshowsthat eachiteration,asubsetofmaskedtokensisupdatedbasedon
noise
the diffusion process is intuitively a convex combination themodel’spredictionxˆ ,whiletheremainingtokensarere-
0
betweendataandthestationarynoisepriordistribution. Dif- masked,accordingtorankedlogp (xˆ |x(t))(Ghazvinine-
θ 0
ferentstationarydistributionsq leadtodifferentformu- jadetal.,2019;Zhengetal.,2023a).
noise
lationsofdiscretediffusionmodels. Hereweprimarilycon- Representation. DPLMistaskedwithdenoisingtheinput
sidertheabsorbingdiffusionwithq(x(t)) = {1ifx(t) = proteinsequenceatallnoiselevels,includingtheoriginal
[X]; 0ifx(t) ̸= [X]},where[X]isanabsorbingstate, noise-freedata(e.g.,noiselevelat0%). Asaresult,DPLM
akintoMasked-LMs. TheformulationofEq.(3)resultsin cansimultaneouslyserveasaproteinsequencerepresenta-
x(t)eitherbeingmaskedorthesameasx(0),withamasking
tionlearnerovermassiveproteinsequencedata,providing
ratio(1−α t). usefulsequenceembeddingforvariousproteinpredictive
Learning. As stated in Austin et al. (2021), discrete dif- downstream tasks, e.g., sequence/residue-level classifica-
fusion inherently connects to AR-LM and Masked-LM, tion/regression. Thesequenceembeddingcanbeattained
whilistZhengetal.(2023a)furthersimplifiesthelearning bysimplylettingDPLMtakeasinputthegivenaminoacid
objectiveofdiscretediffusion,withtheirproposedreparam- sequencex: h(x)← DPLM (x,t=0)∈RL×d,whered
θ
eterizedbackwardtransition,fromKLdivergencesbetween isthedimensionofembedding.
4DiffusionLanguageModelsAreVersatileProteinLearners
3.2 Conditioning fusion. Inspired by continuous diffusion classifier guid-
ance and DiGress on guided graph diffusion (Vignac
Being able to efficiently sample realistic proteins is nec- et al., 2022), here we introduce classifier-guided con-
essarybutnotsufficientfordownstreamapplicationssuch ditional generation for discrete diffusion LMs. Con-
as therapeutic development, since unconditional samples cretely,wewanttosamplefromtheconditionaldistribution
areunlikelytopossessdesiredfunctionalproperties. Here of q(x(t−1)|x(t),y) ∝ q(x(t−1)|x(t))q(y|x(t−1)), which
weelaborateonhowtomakeDPLMpracticallyusefulby is approximated by p (x(t−1)|x(t))p (y|x(t−1)) where
θ ϕ
conditioningforvariousneeds,whichcoversmostcommon p (y|x(t−1)) is a discriminative guidance model (classi-
ϕ
scenarios,i.e.,sequenceconditioning,cross-modalcondi- fierorregressorw.r.t.user’sdesiredproperties). However,
tioning,andplug-and-playpreference-guidedconditioning. p (y|x(t−1))cannotbefactorizedasaproductoverallposi-
ϕ
Case I: Conditioning on partial sequence (Fig. 1C-1).
tions,prohibitingevaluationofallpossiblevaluesofx(t−1).
Protein generation containing pre-specified polypeptides Tothisend,weresorttoanapproximationwithfirst-order
corresponds to various use cases such as generating scaf- Taylorexpansionaroundx(t)(Dhariwal&Nichol,2021a),
folds for given functional motifs, infilling antibody CDR wherewetreatxasacontinuousone-hotvariableonproba-
loops,orimposingexpertknowledgea-priori. Thisimplies bilitysimplextomake∇ xavalidoperator,thereby,
ourdesireforDPLMtosamplefromthisconditionaldis-
tribution x ∼ p (x|x¯) =
(cid:81)L
b ·p (x |x¯), which has
logq(y|x(t−1))
θ i=1 i θ i
alreadybeenlearnedthroughEq.(4). Theobservedpartial ≈ logq(y|x(t))+⟨∇ logq(y|x(t)),x(t−1)−x(t)⟩
x
sequencex¯ ={x¯ i ∈V ifb i =0;[X]ifb i =1|i∈[1,L]}. ≈ (cid:80) ⟨∇ logq(y|x(t)),x(t−1)⟩+C(x(t)),
Namely, b ∈ {0,1} indicates whether the predicted se- 1≤i≤L xi i
i
quencemustpreservetheobservationforthei-thresidue
whereC(x(t))isaconstantthatdoesnotdependonx(t−1).
suchthatx i =x¯ i. Weusep (y|x(t))toestimateq(y|x(t))andplugitintothe
ϕ
CaseII:AdaptingDPLMtoconditionedonothermodal- aboveexpression. Wecannowsamplefromtheresulting
ities(Fig.1C-2). Generatingproteinsequencesubjectto conditionaldistributioninsteadateachtimestept,
cross-modalconstraintsc,i.e.,x∼p (x|c),hasprofound
θ
value in practice, such as inverse protein folding where x(t−1) ∼p θ(x(t−1)|x(t))p ϕ(y|x(t−1))η (5)
s pe aq rau sen ec te as l.a ,r 2e 0g 2e 2n ;e Zra ht ee nd gfo er tag li .v ,e 2n 0b 2a 3c bk )b ,o on re cost nr du ic tt iu or ne in(D ga ou n-
∝p
θ(x(t−1)|x(t))e(cid:0) η·(cid:80) i⟨∇xilogpϕ(y|x(t)),xt (i− )1⟩(cid:1)
,
smallmoleculeligandsforbinderdesign(Dauparasetal.,
whereatunableηcontrolsthestrengthofguidance.
2023). Given that DPLM primarily operates over amino
acidtokens,inthesecases,wecanequipDPLMwithcross- 3.3 ComparisonswithTheMostRelatedWork
modal conditioning by adapter-tuning with a pre-trained Comprehensiverepresentationsforproteinsequenceunder-
modality expert encoder E (c) and a newly-added cross- standingareachievedbypre-trainingonproteinsequence
ϕ
attention-basedadapterfollowingZhengetal.(2023b). Dur- dataviamaskedlanguagemodeling(Devlinetal.,2019),
ingtraining,wefreezetheparametersofthemodalityen- akintolanguageunderstanding. Amongthose,thefamilyof
coderand DPLM,andonlyupdatetheparametersofthe ESM-1b/ESM2(Rivesetal.,2019;Linetal.,2022)serves
adapterviasupervisedfine-tuninggivenacertainamount asthepioneer&cornerstonesequenceembeddingmodels
ofpaireddata(x,c). Wecanattainaconditional DPLM forextensiveproteinpredictivetasks.Therefore,DPLMfol-
forp (x|E (c))makingthefullpotentialsofbothDPLM lowsthebestpracticeofESM2innetworkarchitectureand
θ ϕ
and the modality expert E (c). In §C.3, we also develop pre-trainingstrategies. DPLMtakesasignificantleapfrom
ϕ
classifier-freeguidanceforsuchadapter-tuned DPLM as ESM2withimmediatestronggenerativecapabilities,with-
an immediately available booster for cross-modal condi- outexpensiveneedsforMonteCarlomethods(Verkuiletal.,
tionalgenerationwithoutintricateconditiondropoutduring 2022)orGibbssampler(Johnsonetal.,2021),whichtreat
training. Masked-LMasMarkovrandomfields(Wang&Cho,2019).
Case III: Plug-and-play controllable generation with Besides,asverifiedfrompredictiveexperiments(§4.2),the
discrete classifier guidance (Fig. 1C-3). Directly build- generative ability of DPLM further enables its enhanced
ing a conditional model is prohibitive in most cases due representationlearning,echoingRichardFeynman’sfamous
to data scarcity. Thus, incorporating classifier guidance quote“WhatIcannotcreate,Idonotunderstand”.
into continuous diffusion models (Dhariwal & Nichol, Regardingproteinsequencegeneration,EvoDiff(Alamdari
2021a)provesparticularlyuseful. Thisintegrationwithpre- etal.,2023)isthemostrelevantapproach,whichusesorder-
trainedclassifiersenablessteeringgenerationtowardsde- agnosticautoregressivediffusionmodels(OADM,Hooge-
siredpreferences. However,continuousclassifierguidance boometal.,2021a)forunconditionalgeneration,withcondi-
requiresvaliddefinitionof∇ logp (x),or“score”(Song tionalapplicationsonintrinsicdisorderedsequenceinfilling
x θ
& Ermon, 2019), which does not exist for discrete dif- andmotif-scaffolding,whereasattainingbetterperformance
5DiffusionLanguageModelsAreVersatileProteinLearners
A- foldability (pLDDT, ↑) B- novelty (pdb-TM, ↓) C- diversity (inner-TM, ↓)
Length: 1000
D- secondary structure statistics E - comparison of DPLM model scales F- comparison of probabilistic framework G - comparison of pre-training pLDDT: 89.66
wrt sequence lengths (pLDDT, ↑) for protein LMs (pLDDT, ↑) strategy (pLDDT, ↑)
Length: 900
pLDDT: 87.92
H - ESMFold predicted structures of unconditional sampled sequences
Length: 100 Length: 200 Length: 300 Length: 400 Length: 500 Length: 600 Length: 700 Length: 800
pLDDT: 84.80 pLDDT: 91.67 pLDDT: 88.65 pLDDT: 95.85 pLDDT: 91.65 pLDDT: 88.94 pLDDT: 88.98 pLDDT: 88.41
Figure2.Evaluationofunconditionalgeneration.HereweuseESMFoldasthefoldingmodeltopredictstructuresandcalculatepLDDT
forallthesampledsequences.Wemeasurethe(structural)noveltyofthegeneratedsequencesagainstallknownstructuresinPDBby
TM-score(i.e.,pdb-TM,andmeasurethe(structural)diversitywithinthesampledcandidatesforeachmodel(i.e.,inner-TM).
necessitatesmultiplesequencealignments(MSAs)based hold for protein language modeling. Please refer to the
onaMSA-Transformer(Raoetal.,2021)parameterization. Appendixformoredetailedexperimentalsettings.
DPLMdiffersfromEvoDiffinseveralaspects: (1)DPLM
manifests superior representation learning, which, to the 4.1 EvaluationofUnconditionalGeneration
bestofourknowledge,isthefirsttimeforproteindiffusion Fig.2showstheresultsofDPLMforunconditionalgenera-
models,eveningenerallanguagelearningregime,showing tion,whereweevaluatetheperformanceregardingasetof
DPLM’s appealing versatility; (2) DPLM is based on a lengths[100,200,...,900,1000]inintervalsof100. There-
moreprincipleddiscretediffusionframeworkbeyondthe verseprocessofDPLMforsamplingiteratesfor500steps.
special(order-agnostic)autoregressivediffusion,whichis Meanwhile,wealsorandomlypickthenaturalsequences
notcompatiblewithrefiningintermediatepredictionsand ofthesamelengthfromUniRef50asreference(denotedas
requires expensive O(L) decoding overhead. (3) DPLM UR50)Wehighlightourprimaryfindingsasfollows:
canaccommodateextensiveconditioning,especiallycondi- (1) On Foldability: DPLM is capable of generating pro-
tioningonothermodalitiesandprogrammablegeneration tein sequences with reasonable predicted structures. We
steered by discrete classifier guidance, as opposed to the examinethestructuralplausibilityorfoldabilityofprotein
vanilla sequence conditioning in EvoDiff. This enables sequencesusingthestate-of-the-artsingle-sequencestruc-
broaderapplicationsofDPLMinpractice. ture prediction model, i.e., ESMFold (Lin et al., 2022),
PleaserefertoAppendix§Dforamoredetaileddiscussion and measured by the predicted local distance difference
oftherelatedwork. test (pLDDT) score, which is considered high confidence
ifpLDDT>70. Wecanfindthatproteinsequencesgener-
4 Experiments
atedbyDPLMachievethehighestpLDDTscoreacrossall
WeevaluateDPLMonextensivegenerativeandunderstand- lengths(Fig.2A).Plus,secondarystructureanalysisofthe
ingtasks,spanningunconditionalgeneration(§4.1),avari- sequencesgeneratedbyDPLMrevealsahigherproportion
etyofproteinpredictivedownstreamtasks(§4.2),andcon- ofbeta-strands(Fig.2D),andoverallsimilartothestatistics
ditionaltasks,includingmotif-scaffolding(§4.3.1),inverse- of known protein structures in Protein Data Bank (PDB;
foldingtask(§4.3.2),andsecondarystructureguidedcon- Berman et al., 2000). Moreover, we can see that scaling
trollable generation (§4.3.3). We find that, in general, DPLMleadstobetterfoldabilityperformance,especially
DPLM with larger model scales can attain better results forverylongproteins(Fig.2E).
thansmallerones,demonstratingthescalinglawcanalso (2)OnNovelty. WeinvestigatewhetherDPLMcansample
6DiffusionLanguageModelsAreVersatileProteinLearners
Table1.Performanceonvariousproteinpredictivedownstreamtasks.†:benchmarkedresultsarequotedfromSuetal.(2023).
GO DeepLoc
Thermostability HumanPPI MetalIonBinding EC SSP
Models
MF BP CC Subcellular Binary CASP12
Spearman’sρ Acc(%) Acc(%) Fmax Fmax Fmax Fmax Acc(%) Acc(%) Acc(%)
†SaProt(*structureprovided) 0.724 86.41 75.75 0.884 0.678 0.356 0.414 85.57 93.55 -
†ESM-1b(Rivesetal.,2019) 0.708 82.22 73.57 0.859 0.661 0.320 0.392 80.33 92.83 -
†MIF-ST(Yangetal.,2022b) 0.694 75.54 75.08 0.803 0.627 0.239 0.248 78.96 91.76 -
Masked-LM(ESM2-650M) 0.691 84.78 71.88 0.866 0.676 0.344 0.402 83.68 92.28 0.80
AR-LM(650M) 0.638 68.48 61.16 0.691 0.566 0.258 0.287 68.53 88.31 -
DPLM(150M) 0.687 80.98 72.17 0.822 0.662 0.328 0.379 82.41 92.63 -
DPLM(650M) 0.695 86.41 75.15 0.875 0.680 0.357 0.409 84.56 93.09 0.82
DPLM(3B) 0.703 - - - 0.687 0.362 0.438 84.88 93.93 -
sequencespossessingnovelstructures,wherewecompare teingetslonger,thecomplexityofitsstructurewillincrease,
thestructuralsimilarityagainstknownstructuresinPDB containingrichhelicesandsheets. WealsofindthatDPLM
withTMScore. ThehighestTMscoreisusedtomeasure cansampleproteinscomposedoftandemrepeatssuchas
thenoveltyofeachsequence,whichwerefertoaspdb-TM beta-barrelorKelchrepeatdomain.
score. Overall, DPLM hasrelativelyhigherpdbTMthan
EvoDiffandnaturalsequences,asshowninFig.2B.Interest-
4.2 EvaluationofProteinRepresentationLearningon
ingly,thepdbTMscoreofDPLMwilldecreaseasprotein
DownstreamPredictiveTasks
getslongerthan300whilemaintainingthepLDDT > 75.
Weevaluate DPLM acrossavarietyofproteinpredictive
ThisindicatesthatDPLMpossessestheabilitytosample
tasks (Su et al., 2023; Dallago et al., 2021; Xu et al.,
sequenceswithstructuresnotsimilartoPDBacrossvari-
2022), including protein function prediction (Thermosta-
ous lengths, with the discrepancy becoming increasingly
bilityandMetalIonBinding),proteinlocalizationpredic-
apparentasthesequencelengthextends.
tion(DeepLoc),proteinannotationprediction(ECandGO),
(3)OnDiversity. Wequantifythediversityofsequences
protein-proteininteractionprediction(HumanPPI),where
sampledbyDPLMbyinner-TMscore. Specifically,for
weperformfull-parameterssupervisedfine-tuningoneach
each sampled candidate, we use ESMFold to predict its
dataset. Wealsoincludelinearprobingforsecondarystruc-
structure and compute TMscore against the rest. The
turefromTAPE(Raoetal.,2019).
average TMscore is considered as the diversity. As
DPLM is a superior protein sequence representation
showninFig.2C,DPLMhasaconsiderablylowaverage
inner-TM,demonstratingthattheDPLMcansynthesize learner. AsdemonstratedinTab.1, DPLM outperforms
ESM2acrossalltasks. Thisimprovedperformanceisdueto
structurallydiversesequences.
theproposeddiffusionpre-training,whichrequiresDPLM
(4)OnLearning: Discretediffusionisthebest-suitedprob-
toadeptlylearntoreconstructthenativesequencefroma
abilisticframeworkforproteinsequencegeneration,com-
varied proportion of masking, including very high noise
pared to Masked-LM and AR-LM. As shown in Fig. 2F,
level, in contrast to ESM2 of a fixed 15% masking ratio.
DPLMoutperformsMasked-LMandAR-LMintermsof
Underthiscircumstance,itbecomesamuchmorechalleng-
foldability,verifyingourmotivationtopursueadiffusion
ingmissingaminoacidreconstructiontaskencouragingthe
protein LM that diffusion is a more proper probabilistic
modeltocapturethedeepdependenciesfromtheverycon-
frameworkforproteinmodeling. Moreover,AR-LMalso
text. Besides,wesurprisinglyfindthatDPLMalsoclosely
falls short of precisely controlling the length of sampled
approaches the performance of SaProt (Su et al., 2023),
sequences,makingitlessflexibleinpractice. Asrevealed
whichisastructure-awareLMthatincorporatesexplicitly
inFig.2G,wefindthatdespiteattainingimprovedgenera-
protein structures based on Foldseek (van Kempen et al.,
tionqualityoverESM2withdirectlypre-trainingDPLM
2023) and folding models like AlphaFold (Jumper et al.,
fromscratch(DPLM-FS),itcanbringadditionallearning
2021). Thisimpliesthat DPLM mayimplicitlylearnthe
challengesandtrainingoverheads. Assuch,weleveragea
protein structures from massive sequence data. Integrat-
2-stagetrainingstrategy,whichconsistsofmaskedlanguage
ingexplicitstructuralinformationintoDPLMlikeSuetal.
modelingasthefirststageobjective,followedbydiffusion
(2023) may bring further benefits, which deserve further
objective,solvingthisproblemandobtaininghigh-quality
exploration. Ourresultssubstantiateourinitialpremisethat
generationwithpLDDTcloselyapproaching90.
DPLM gains a deeper understanding of protein through
(5)CaseStudy. InFig.2H,weshowcaseproteinssampled
thegenerativelearningprocess,i.e.,itlearnstobetterun-
byDPLMacrossvariouslengths,rangingfrom100to1000,
derstandproteinsbylearningtogeneratethem,leadingto
whilemorecasesarepresentedintheAppendix. Asthepro-
improvedpredictiveperformance.
7DiffusionLanguageModelsAreVersatileProteinLearners
A sequence-only B C D
EvoDiff DPLM
structure-cond.
RFDiffusion DPLM
E 1PRW: binding site of compact calmodulin 7MRX: binding domain of barnase ribonuclease inhibitor 5YUI: binding site of carbonic anhydrase metalloenzyme
native sequence native sequence native sequence
ADQLTEEQIAEFKEAFSLFDKDGDGTITTKELGTVMRSLG MKKAVINGEQIRSISDLHQTLKKELALPEYYGENL HWGYGKHNGPEHWHKDFPIAKGERQSPVDIDTHTAKYDPSLKP
QNPTEAELQDMINEVDADGNGTIDFPEFLTMMARKMKDTD DALWDALTGWVEYPLVLEWRQFEQSKQLTENGAES LSVSYDQATSLRILNNGHAFNVEFDDSQDKAVLKGGPLDGTYR
SEEEIREAFRVFDKDGNGYISAAELRHVMTNLGELTDEEV VLQVFREAKAEGADITIILS LIQFHFHWGSLDGQGSEHTVDKKKYAAELHLVHWNTKYGDFGK
DEMIREADIDGDGQVNYEEFVQMMTAK AVQQPDGLAVLGIFLKVGSAKPGLQKVVDVLDSIKTKGKSADF
TNFDPRGLLPESLDYWTYPGSLTTPPLLECVWIVLKEPISVSS
EQVLKFRKLNFNGEGEPEELMVDNWRPAQPLKNRQIKASFK
generated sequence generated sequence generated sequence
SLFDKDGDGTITTKELGTVMRSLGQNPSESELQDMINEVD MHLLDGRRMRTKADLHRELKRVLALPEYYGENLDA SVQVNLDDSSGIKGGGLPDTYKLKQFHFHWGSANDRGSEHTVD
ADGNGTIDFPEFLTMMARKMKDTDSEEEIREAFSLFDKDG LWDALTGWVEPPTVLLWTHWSVVAQAMPRHAQTTL GEKFPAELHLVHWNTKYDSFAEAASKADGLAVLGFFLKVGAEN
DGTITTKELGTVMRSLGQNPSESELQDMINEVDADGNGTI QVLAEAAEYWRDEGHPFTVLVEDGPVDYEIPELQD KELQKITDALKDVKTKGETSFPNFNPSSLLPSDRSAYWRYSGS
DFPEFLTMMARKMKD LTTPPCSESVTWTVFKDSVEVSQSQLDAFTSLLGENKRPAQPL
NDRPVVASFRPSV
pLDDT: 86.84 motif-RMSD: 0.61 TMscore: 0.57 pLDDT: 93.85 motif-RMSD: 0.58 TMscore: 0.73 pLDDT: 87.79 motif-RMSD: 0.77 TMscore: 0.85
Figure3.Evaluationofmotif-scaffolding.(A)comparisonregardingoverallsuccessrateandnumberofsolvedproblems;(B)comparison
betweensequence-onlyapproaches(DPLMvs.EvoDiff);(C)comparisonbetweenstructure-awareapproaches(structure-conditioned
DPLMvs.RFDffusion);and(D)comparisonbetweensequence-onlyvs.structure-conditionedDPLM.(E)casestudyforthreeproblems.
4.3 EvaluationofConditionalGeneration showsbetterresultsin6problems,especiallyfor1PRWand
5YUI).WefindthatutilizingmotifstructurehelpsDPLM
4.3.1 SEQUENCE-COND.: MOTIF-SCAFFOLDING
make a further improvement on 4 problems compared to
Thegoalofmotif-scaffoldingrequiresavalidscaffoldto theoriginalsequence-onlyDPLM,whiledecreasingperfor-
maintainthestructureofthegivenmotifsuchthattheorig- manceontheother6problems. Thisimpliesthatforsome
inal function can be preserved. Here, we follow the ex- specificmotifs,scaffoldinginsequencespacemaybebetter.
perimentalsettinginAlamdarietal.(2023),wherewe(1) Thedetailedanalysisunveiledacommonbiologicalproperty
initiallydeterminethelengthofascaffoldandfillthescaf- amongthemotifsobservedinthesetwocases. Specifically,
foldpositionswiththemasktoken;then(2)keepthemotif themotifsequencedisplayedaremarkablelevelofevolu-
fragmentfixedduringinference,andsamplescaffoldcondi- tionaryconservation,playingpivotalrolesinbindingcritical
tionedonthemotif;andfinallyuseOmegaFold(Wuetal., signalpassengers(1PRW:calmodulinEFhandforcalcium
2022b)topredictthestructureofthesampledsequences. A bindingand5YUI:carbonicanhydraseIIforCO2binding).
scaffoldisconsideredsuccessfulwhenitmeetstwocondi- Notably,themotifstructurespredominantlycomprisedflex-
tions: (1)theRMSDbetweenthepredictedmotifstructure ibleloops. Conversely,5TPN,6VW1,and2KL8,whichex-
andthegroundtruth,referredtoasmotif-RMSD < 1A˚; hibitedadistinctadvantageinmotifscaffoldingasindicated
and(2)thestructureshouldhaveanoverallpLDDT > 70. by the RFdiffusion, featured rigid helical structures that
Overall,weexamine17motif-scaffoldingproblems,andfor lackedfunctionalevolutionaryconservations. Thisintrigu-
eachproblem,wesample100sequencesandthencalculate ingphenomenonsuggeststhatDPLMholdsgreatpromise
thesuccessrateaccordingtotheabovecriterion. asasuperiormethodforconstructingstructurallyflexible
DPLMcangeneratereasonablescaffoldsforthegiven yetevolutionarilyconservedfunctionalmotifscaffolding.
functionalmotifs. AsshowninFig.3,wefindthatDPLM
4.3.2 STRUCTURE-CONDITIONED: INVERSEFOLDING
outperformsEvoDiffintermsofthenumberofsolvedprob-
lemsandtheaveragesuccessrate. Moreover,ontheprob- The goal of inverse folding is to find an amino acid se-
lemsthatbothDPLMandEvoDiffcansolve,thesuccess quencethatcanfoldtoagivenproteinbackbonestructure.
rateofDPLMishigherthanEvoDiff,except3ixt. This Wefollow LM-DESIGN (Zhengetal.,2023b)toimplant
indicatesthatDPLMexcelsinmotif-scaffolding,preserv- astructuraladapterintothelastnetworklayerof DPLM,
ingthemotifstructureduringscaffoldgeneration. Togain and use GVP-Transformer Encoder (Hsu et al., 2022) as
more insights, we compare DPLM with structure condi- theexpertproteinbackbonestructureencoder. Weassess
tioning(see§4.3.2)withstate-of-the-artstructuredesigner DPLM on CATH 4.2 and 4.3 (Orengo et al., 1997). We
RFDiffusion (Watson et al., 2023). We find that DPLM use amino acid recovery (AAR) for sequence evaluation,
8DiffusionLanguageModelsAreVersatileProteinLearners
Table2.Performance comparison between DPLM and differ- template: 3F4M secondary structure-guided sampling
ent baseline approaches on CATH 4.2 and CATH 4.3 datasets.
DPLM’sresultsareobtainedbyargmaxdecoding(i.e.,nosam-
pling).†:benchmarkedresultsarequotedfromGaoetal.(2022b).
struct.eval.
Models Trainable AAR
Params. scTM pLDDT 6 helices
†StructTrans(Ingrahametal.,2019) 1.6M/1.6M 35.82 - - template: 5CW9
†GVP(Jingetal.,2020) 1.0M/1.0M 39.47 - -
†ProteinMPNN(Dauparasetal.,2022) 1.9M/1.9M 45.96 - -
PiFold(Gaoetal.,2022b) 6.6M/6.6M 51.66 - -
ProteinMPNN+CMLM 1.9M/1.9M 48.62 - -
S<--H<--S<--H<--S
LM-DESIGN(w/ProtMPNNencoder) 5.0M/650M 54.41 0.88 77.07 S->-H->-S->-H->-S
DPLM(w/ProtMPNNencoder) 5.0M/650M 54.54 0.88 77.12 Figure4.Secondarystructureguidedconditionalsampling. The
PiFold(Gaoetal.,2022b) 6.6M/6.6M 51.66 - - firstcasecontains6alpha-helices,Thesecondcaseismuchmore
GVP-Transformer(Hsuetal.,2022) 142M/142M 51.60 - -
complicatedasagloballytwistedstructurewithinterleavedalpha-
LM-DESIGN(w/GVP-Transencoder) 6.3M/650M 56.49 0.85 74.89
helicesandbeta-strands,wheretheN-terminusandC-terminusare
DPLM-150M(w/GVPTransencoder) 3.1M/150M 53.27 0.85 75.31
DPLM-650M(w/GVPTransencoder) 6.3M/650M 56.61 0.86 76.78 structurallycontiguous.
DPLM-3B(w/GVPTransencoder) 68.2M/3.0B 58.10 0.86 76.95
models, but for discrete data. This flexibility to swiftly
whilstforstructureevaluation,wefirstpredictthestructure adapttotheevolvingneedsofusersacrossabroadspectrum
ofthegeneratedsequenceusingESMFold,thencalculate of preferences is also significant in practical applications
the pLDDT score and self-consistency TM-score (scTM) withtimeandcomputationalparamount.
betweenpredictedstructureandtheinputone.
5 Discussions
DPLMyieldssequencesthatcanconfidentlyfoldintothe
givenbackbonestructure. AsshowninTab.2,DPLMcan Inthispaper,weintroducediffusionproteinLM(DPLM),
outperformorbeonparwithourstrongbaselines,includ-
a versatile protein LM that is capable of both protein se-
ingthestate-of-the-artapproachLM-DESIGNZhengetal.
quencegenerationandrepresentationlearning. Wefurther
(2023b)),manifestinginAAR,andmostimportantly,decent
develop several conditioning strategies for various needs
performanceregardingstructureevaluation(scTM=0.85
ofconditionalgeneration,includingsequenceconditioning,
andpLDDT>76). Wesuggestthisderivesfromthewell-
cross-modal conditioning, and programmable generation
learnedproteinsequenceknowledgeofDPLM.Whengiven
withplug-and-playdiscreteclassifierguidance.
structurebackboneinformation,DPLMcanleveragethis
Despitethesepromisingresults,thereremainseverallimita-
advantageandgeneratethesequencewhosestructureisboth
tionsandfutureworkdirectionsdeservingtobeexplored.
plausibleandsimilartothereference.
(i) ExploringDPLM’sconditionalgenerationforwider
applications. Wecanfurtherextendthecross-modal
4.3.3 CONTROLLABLEGENERATION: SECONDARY
conditioningstrategyofDPLMtomorediversemodal-
STRUCTUREGUIDEDPROTEINSAMPLING
ities as conditioners, including MSA-conditioned
Classifierguidanceispreferredforitsflexiblecontrolover homologous sequence generation, small molecule-
thegenerationprocesswithoutretrainingforeachnewcon- conditioned binder design for ligands, antigen-
dition, especially beneficial in scenarios with too limited conditionedantibodyCDRdesign,amongothers.Also,
labeleddatatodirectlyattainconditionalmodels. Herewe the inclusion of demonstrations featuring plug-and-
showcasehowtoguide DPLM togenerateproteinssatis- playclassifier-guidedcontrollablegenerationisessen-
fying desiredsecondary structures. We train a secondary tialformorescenariostowarddiverseuserpreferences,
structureprediction(SSP)modelasasequencelabelingtask e.g.,structuralsymmetry,superfamily,bindingaffinity,
onTAPEdataset. WethenintegratethisSSPdiscriminative thermostability,fluorescence,andbeyond.
modelintoDPLMtoprovideguidingsignals. (ii) DPLM can further benefit from best practices of
DPLM enjoysplug-and-playprogrammability. Fig.4 cutting-edge technical advancement in the vastness
showcases that the proposed discrete classifier guidance of large language models (LLMs). For example, (1)
helpssteerapre-trainedDPLMtogeneratesamplessatis- longcontextextension(Chenetal.,2023b)canrapidly
fying provided secondary structure annotations extracted adaptDPLMtohandleverylongproteinsbeyondits
fromtemplatenaturalproteins. Thesefindingssuggestthat traininglengthlimit, andofferingpotentialformod-
DPLM is highly programmable, and its full potential of eling exceptionally long biological sequences such
generativecapabilitiescanberealizedinaplug-and-play as DNAs and RNAs, unifying and deciphering the
fashion,indicatingthatDPLMpreservestheappealingchar- languages associated with the central dogma of life;
acteristic of controllable generation inherent in diffusion (2)fine-tuningDPLMwithhumanfeedbackoreven
9
2.4HTAC
3.4HTACDiffusionLanguageModelsAreVersatileProteinLearners
wet-labexperimentalfeedback,leveragingreinforce- Askell,A.,etal. Languagemodelsarefew-shotlearners.
mentlearning(RL;Ouyangetal.,2022),directpref- volume33,pp.1877–1901,2020.
erenceoptimization(DPO;Rafailovetal.,2024),and
Chen, J., Zhang, A., Li, M., Smola, A., and Yang, D. A
self-play fine-tuning (Chen et al., 2024b); (3) elicit-
cheaper and better diffusion language model with soft-
inginstruction-followingandin-contextlearning(Wei
maskednoise. arXivpreprintarXiv:2304.04746,2023a.
et al., 2022a) analogs for protein LMs can also be
a promising direction would fully harness DPLM’s
Chen, S., Wong, S., Chen, L., and Tian, Y. Extending
learnedknowledge.
contextwindowoflargelanguagemodelsviapositional
(iii) It is imperative to integrate protein structure mod-
interpolation. arXivpreprintarXiv:2306.15595,2023b.
eling into DPLM. The advance of protein structure
modelingmanifesttremendoussuccess,includingAl- Chen, X., Liu, Z., Xie, S., and He, K. Deconstructing
phaFold (Jumper et al., 2021), ESMFold (Lin et al., denoisingdiffusionmodelsforself-supervisedlearning.
2022)forstructureprediction,RFDIffusion(Watson arXivpreprintarXiv:2401.14404,2024a.
etal.,2023),Chroma(Ingrahametal.,2023)forstruc-
Chen,Z.,Deng,Y.,Yuan,H.,Ji,K.,andGu,Q. Self-play
turedesign, andevenfull-atommolecularmodeling,
fine-tuningconvertsweaklanguagemodelstostronglan-
e.g., the latest generation of AlphaFold (DeepMind,
guagemodels. arXivpreprintarXiv:2401.01335,2024b.
2023) and RF-AA (Krishna et al., 2023). Develop-
ingauniversalproteinlanguagemodelwiththenext-
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,
generationDPLM,whichaccountsforbothsequence
H.,Zheng,L.,Zhuang,S.,Zhuang,Y.,Gonzalez,J.E.,
andstructure,isaparticularlypromisingavenue.
Stoica, I., and Xing, E. P. Vicuna: An open-source
Weleavetheseexcitingdirectionsasfuturework. chatbot impressing gpt-4 with 90%* chatgpt quality,
March 2023. URL https://lmsys.org/blog/
Acknowledgements
2023-03-30-vicuna/.
WewouldliketoespeciallythankDr. HangLiforinsightful
Dallago,C.,Mou,J.,Johnston,K.E.,Wittmann,B.J.,Bhat-
discussionsontheprojectandfeedbackonthemanuscript
tacharya,N.,Goldman,S.,Madani,A.,andYang,K.K.
thathelpshapethisstudy. WethankYiZhou,JingYuan,Dr.
Flip: Benchmarktasksinfitnesslandscapeinferencefor
YilaiLiandJiashengYefortheirvaluablecomments.
proteins. bioRxiv,pp.2021–11,2021.
References
Dauparas,J.,Anishchenko,I.,Bennett,N.,Bai,H.,Ragotte,
Alamdari,S.,Thakkar,N.,vandenBerg,R.,Lu,A.X.,Fusi, R.J., Milles, L.F., Wicky, B.I., Courbet, A., deHaas,
N.,Amini,A.P.,andYang,K.K. Proteingenerationwith R.J.,Bethel,N.,etal. Robustdeeplearning–basedpro-
evolutionarydiffusion: sequenceisallyouneed. bioRxiv, teinsequencedesignusingproteinmpnn. Science, 378
pp.2023–09,2023. (6615):49–56,2022.
Austin,J.,Johnson,D.D.,Ho,J.,Tarlow,D.,andvanden Dauparas,J.,Lee,G.R.,Pecoraro,R.,An,L.,Anishchenko,
Berg, R. Structured denoising diffusion models in dis- I., Glasscock, C., and Baker, D. Atomic context-
crete state-spaces. In Advances in Neural Information conditionedproteinsequencedesignusingligandmpnn.
ProcessingSystems,volume34,pp.17981–17993,2021. Biorxiv,pp.2023–12,2023.
Bengio,Y.,Ducharme,R.,andVincent,P. Aneuralproba- DeBortoli,V.,Mathieu,E.,Hutchinson,M.,Thornton,J.,
bilisticlanguagemodel. Advancesinneuralinformation Teh, Y. W., and Doucet, A. Riemannian score-based
processingsystems,13,2000. generativemodelling. AdvancesinNeuralInformation
ProcessingSystems,35:2406–2422,2022.
Berman,H.M.,Westbrook,J.,Feng,Z.,Gilliland,G.,Bhat,
T.N.,Weissig,H.,Shindyalov,I.N.,andBourne,P.E. DeepMind,G. Performanceandstructuralcoverageofthe
The protein data bank. Nucleic acids research, 28(1): latest,in-developmentalphafoldmodel. 2023.
235–242,2000.
Devlin,J.,Chang,M.-W.,Lee,K.,andToutanova,K.BERT:
Brandes,N.,Ofer,D.,Peleg,Y.,Rappoport,N.,andLinial, Pre-training of deep bidirectional transformers for lan-
M. Proteinbert: auniversaldeep-learningmodelofpro- guageunderstanding. InProceedingsofthe2019Con-
teinsequenceandfunction. Bioinformatics,38(8):2102– ferenceoftheNorthAmericanChapteroftheAssocia-
2110,2022. tion for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp.
Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D., 4171–4186,Minneapolis,Minnesota,June2019.Asso-
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., ciation for Computational Linguistics. doi: 10.18653/
10DiffusionLanguageModelsAreVersatileProteinLearners
v1/N19-1423. URL https://www.aclweb.org/ Gu, J., Bradbury, J., Xiong, C., Li, V. O., and Socher, R.
anthology/N19-1423. Non-autoregressiveneuralmachinetranslation. InInter-
nationalConferenceonLearningRepresentations,2018.
Dhariwal, P. and Nichol, A. Diffusion models beat gans
on image synthesis. Advances in neural information Han, X., Kumar, S., and Tsvetkov, Y. Ssd-lm: Semi-
processingsystems,34:8780–8794,2021a. autoregressivesimplex-baseddiffusionlanguagemodel
fortextgenerationandmodularcontrol. arXivpreprint
Dhariwal, P. and Nichol, A. Diffusion models beat gans
arXiv:2210.17432,2022.
on image synthesis. Advances in neural information
processingsystems,34:8780–8794,2021b.
He,L.,Zhang,S.,Wu,L.,Xia,H.,Ju,F.,Zhang,H.,Liu,
S., Xia, Y., Zhu, J., Deng, P., et al. Pre-training co-
Dieleman, S., Sartran, L., Roshannai, A., Savinov, N.,
evolutionaryproteinrepresentationviaapairwisemasked
Ganin, Y., Richemond, P. H., Doucet, A., Strudel, R.,
languagemodel. arXivpreprintarXiv:2110.15527,2021.
Dyer, C., Durkan, C., et al. Continuous diffusion for
categoricaldata. arXivpreprintarXiv:2211.15089,2022.
He,Z.,Sun,T.,Wang,K.,Huang,X.,andQiu,X. Diffu-
Elnaggar, A., Heinzinger, M., Dallago, C., Rehawi, G., sionbert: Improvinggenerativemaskedlanguagemodels
Wang, Y., Jones, L., Gibbs, T., Feher, T., Angerer, C., withdiffusionmodels. 2023.
Steinegger,M.,etal.Prottrans:Towardunderstandingthe
Ho,J.andSalimans,T. Classifier-freediffusionguidance.
languageoflifethroughself-supervisedlearning. IEEE
InNeurIPS2021WorkshoponDeepGenerativeModels
transactionsonpatternanalysisandmachineintelligence,
andDownstreamApplications,2021.
44(10):7112–7127,2021.
Ferruz,N.andHo¨cker,B. Controllableproteindesignwith Ho, J., Jain, A., and Abbeel, P. Denoising diffu-
language models. Nature Machine Intelligence, 4(6): sion probabilistic models. Advances in Neu-
521–532,2022. ral Information Processing Systems, 33:6840–
6851, 2020. URL https://proceedings.
Ferruz,N.,Schmidt,S.,andHo¨cker,B. Protgpt2isadeep neurips.cc/paper/2020/file/
unsupervisedlanguagemodelforproteindesign. Nature 4c5bcfec8584af0d967f1ab10179ca4b-Paper.
communications,13(1):4348,2022. pdf.
Fu,Y.,Peng,H.,Ou,L.,Sabharwal,A.,andKhot,T. Spe-
Ho,J.,Chan,W.,Saharia,C.,Whang,J.,Gao,R.,Gritsenko,
cializingsmallerlanguagemodelstowardsmulti-steprea-
A.,Kingma,D.P.,Poole,B.,Norouzi,M.,Fleet,D.J.,
soning. arXivpreprintarXiv:2301.12726,2023.
et al. Imagen video: High definition video generation
withdiffusionmodels. arXivpreprintarXiv:2210.02303,
Gao, Z., Guo, J., Tan, X., Zhu, Y., Zhang, F., Bian, J.,
2022.
and Xu, L. Difformer: Empowering diffusion model
onembeddingspacefortextgeneration. arXivpreprint
Hoffmann,J.,Borgeaud,S.,Mensch,A.,Buchatskaya,E.,
arXiv:2212.09412,2022a.
Cai,T.,Rutherford,E.,Casas,D.d.L.,Hendricks,L.A.,
Gao, Z., Tan, C., and Li, S. Z. Pifold: Toward effec- Welbl, J., Clark, A., et al. Training compute-optimal
tiveandefficientproteininversefolding. arXivpreprint largelanguagemodels. arXivpreprintarXiv:2203.15556,
arXiv:2209.12643,2022b. 2022.
Ghazvininejad,M.,Levy,O.,Liu,Y.,andZettlemoyer,L. Hoogeboom,E.,Gritsenko,A.A.,Bastings,J.,Poole,B.,
Mask-predict: Paralleldecodingofconditionalmasked vandenBerg,R.,andSalimans,T. Autoregressivedif-
language models. In Proceedings of the 2019 Confer- fusionmodels. InInternationalConferenceonLearning
ence on Empirical Methods in Natural Language Pro- Representations,2021a.
cessing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP), pp. Hoogeboom,E.,Nielsen,D.,Jaini,P.,Forre´,P.,andWelling,
6112–6121, Hong Kong, China, November 2019. As- M. Argmaxflowsandmultinomialdiffusion: Learning
sociationforComputationalLinguistics. doi: 10.18653/ categoricaldistributions.AdvancesinNeuralInformation
v1/D19-1633. URL https://www.aclweb.org/ ProcessingSystems,34:12454–12465,2021b.
anthology/D19-1633.
Hoogeboom,E.,Satorras,V.G.,Vignac,C.,andWelling,
Gong,S.,Li,M.,Feng,J.,Wu,Z.,andKong,L. Diffuseq: M. Equivariantdiffusionformoleculegenerationin3d.
Sequencetosequencetextgenerationwithdiffusionmod- In International Conference on Machine Learning, pp.
els. arXivpreprintarXiv:2210.08933,2022. 8867–8887.PMLR,2022.
11DiffusionLanguageModelsAreVersatileProteinLearners
Hsu,C.,Verkuil,R.,Liu,J.,Lin,Z.,Hie,B.,Sercu,T.,Lerer, Kojima,T.,Gu,S.S.,Reid,M.,Matsuo,Y.,andIwasawa,
A.,andRives,A. Learninginversefoldingfrommillions Y. Largelanguagemodelsarezero-shotreasoners. Ad-
of predicted structures. In Chaudhuri, K., Jegelka, S., vances in neural information processing systems, 35:
Song,L.,Szepesvari,C.,Niu,G.,andSabato,S.(eds.), 22199–22213,2022.
Proceedingsofthe39thInternationalConferenceonMa-
Krishna,R.,Wang,J.,Ahern,W.,Sturmfels,P.,Venkatesh,
chineLearning,volume162ofProceedingsofMachine
P., Kalvet, I., Lee, G. R., Morey-Burrows, F. S., An-
Learning Research, pp. 8946–8970. PMLR, 17–23 Jul
2022.URLhttps://proceedings.mlr.press/ ishchenko, I., Humphreys, I. R., et al. Generalized
v162/hsu22a.html. biomolecularmodelinganddesignwithrosettafoldall-
atom. bioRxiv,pp.2023–10,2023.
Hu, M., Yuan, F., Yang, K. K., Ju, F., Su, J., Wang, H.,
Lee,J.S.,Kim,J.,andKim,P.M. Proteinsgm: Score-based
Yang,F.,andDing,Q. Exploringevolution-aware&-free
generativemodelingfordenovoproteindesign. bioRxiv,
proteinlanguagemodelsasproteinfunctionpredictors.
pp.2022–07,2022.
InAdvancesinNeuralInformationProcessingSystems,
2022. Li, X. L., Thickstun, J., Gulrajani, I., Liang, P., and
Hashimoto,T. Diffusion-lmimprovescontrollabletext
Huang, F., Ke, P., andHuang, M. Directedacyclictrans- generation. InAdvancesinNeuralInformationProcess-
formerpre-trainingforhigh-qualitynon-autoregressive ingSystems,volumeabs/2205.14217,2022.
textgeneration. TransactionsoftheAssociationforCom-
putationalLinguistics,2023. Lin,Y.andAlQuraishi,M. Generatingnovel,designable,
anddiverseproteinstructuresbyequivariantlydiffusing
Ingraham,J.,Garg,V.,Barzilay,R.,andJaakkola,T. Gener- orientedresidueclouds.arXivpreprintarXiv:2301.12485,
ativemodelsforgraph-basedproteindesign. InAdvances 2023.
inneuralinformationprocessingsystems,2019.
Lin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., dos
SantosCosta,A.,Fazel-Zarandi,M.,Sercu,T.,Candido,
Ingraham,J.B.,Baranov,M.,Costello,Z.,Barber,K.W.,
S., etal. Languagemodelsofproteinsequencesatthe
Wang, W., Ismail, A., Frappier, V., Lord, D. M., Ng-
scale of evolution enable accurate structure prediction.
Thow-Hing, C., Van Vlack, E. R., et al. Illuminating
BioRxiv,2022.
protein space with a programmable generative model.
Nature,pp.1–9,2023.
Lisanza,S.L.,Gershon,J.M.,Tipps,S.W.K.,Arnoldt,L.,
Hendel,S.,Sims,J.N.,Li,X.,andBaker,D. Jointgener-
Jing,B.,Eismann,S.,Suriana,P.,Townshend,R.J.L.,and
ationofproteinsequenceandstructurewithrosettafold
Dror, R. Learningfromproteinstructurewithgeomet-
sequencespacediffusion. bioRxiv,pp.2023–05,2023.
ricvectorperceptrons. InInternationalConferenceon
LearningRepresentations,2020. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
Levy,O.,Lewis,M.,Zettlemoyer,L.,andStoyanov,V.
Johnson,S.R.,Monaco,S.,Massie,K.,andSyed,Z. Gen- Roberta: Arobustlyoptimizedbertpretrainingapproach.
eratingnovelproteinsequencesusinggibbssamplingof arXivpreprintarXiv:1907.11692,2019.
maskedlanguagemodels. bioRxiv,pp.2021–01,2021.
Lu,A.X.,Zhang,H.,Ghassemi,M.,andMoses,A. Self-
Jumper,J.,Evans,R.,Pritzel,A.,Green,T.,Figurnov,M., supervisedcontrastivelearningofproteinrepresentations
Ronneberger,O.,Tunyasuvunakool,K.,Bates,R.,Zˇ´ıdek, bymutualinformationmaximization. BioRxiv,pp.2020–
A.,Potapenko,A.,etal. Highlyaccurateproteinstructure 09,2020.
predictionwithalphafold. Nature,596(7873):583–589,
Madani, A., Krause, B., Greene, E. R., Subramanian, S.,
2021.
Mohr, B. P., Holton, J. M., Olmos Jr, J. L., Xiong, C.,
Sun,Z.Z.,Socher,R.,etal. Deepneurallanguagemodel-
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,
ingenablesfunctionalproteingenerationacrossfamilies.
Chess,B.,Child,R.,Gray,S.,Radford,A.,Wu,J.,and
bioRxiv,pp.2021–07,2021.
Amodei, D. Scaling laws for neural language models.
arXivpreprintarXiv:2001.08361,2020.
McDermott,M.,Yap,B.,Hsu,H.,Jin,D.,andSzolovits,P.
Adversarialcontrastivepre-trainingforproteinsequences.
Kim,J.,Kong,J.,andSon,J. Conditionalvariationalau-
arXivpreprintarXiv:2102.00466,2021.
toencoderwithadversariallearningforend-to-endtext-to-
speech. InInternationalConferenceonMachineLearn- Meier,J.,Rao,R.,Verkuil,R.,Liu,J.,Sercu,T.,andRives,
ing,pp.5530–5540.PMLR,2021. A. Languagemodelsenablezero-shotpredictionofthe
12DiffusionLanguageModelsAreVersatileProteinLearners
effects of mutations on protein function. In Advances Peters,M.E.,Neumann,M.,Iyyer,M.,Gardner,M.,Clark,
in Neural Information Processing Systems, pp. 29287– C., Lee, K., and Zettlemoyer, L. Deep contextualized
29303,2021. wordrepresentations. InProceedingsofthe2018Confer-
enceoftheNorthAmericanChapteroftheAssociation
Melnyk, I., Chenthamarakshan, V., Chen, P.-Y., Das, P.,
forComputationalLinguistics: HumanLanguageTech-
Dhurandhar,A.,Padhi,I.,andDas,D. Reprogramming
nologies,Volume1(LongPapers),pp.2227–2237,New
largepretrainedlanguagemodelsforantibodysequence
Orleans,Louisiana,June2018.AssociationforCompu-
infilling. arXivpreprintarXiv:2210.07144,2022.
tationalLinguistics. doi: 10.18653/v1/N18-1202. URL
https://aclanthology.org/N18-1202.
Mikolov,T.,Chen,K.,Corrado,G.,andDean,J. Efficient
estimation of word representations in vector space. In Qian, L., Zhou, Y., Zheng, Z., Zhu, Y., Lin, Z., Feng, J.,
Bengio,Y.andLeCun,Y.(eds.),1stInternationalConfer- Cheng,S.,Li,L.,Wang,M.,andZhou,H. Thevolctrans
enceonLearningRepresentations,ICLR2013,Scottsdale, glatsystem:Non-autoregressivetranslationmeetswmt21.
Arizona,USA,May2-4,2013,WorkshopTrackProceed- WMT2021,pp.187,2021.
ings,2013. URLhttp://arxiv.org/abs/1301.
Qian,L.,Wang,M.,Liu,Y.,andZhou,H. Diff-glat: Dif-
3781.
fusionglancingtransformerforparallelsequencetose-
Min,S.,Park,S.,Kim,S.,Choi,H.-S.,Lee,B.,andYoon, quencelearning. arXivpreprintarXiv:2212.10240,2022.
S. Pre-training of deep bidirectional protein sequence
Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.,
representationswithstructuralinformation. IEEEAccess,
etal. Improvinglanguageunderstandingbygenerative
9:123912–123926,2021.
pre-training. 2018.
Muennighoff,N.,Rush,A.M.,Barak,B.,Scao,T.L.,Piktus, Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
A.,Tazi,N.,Pyysalo,S.,Wolf,T.,andRaffel,C. Scal- Sutskever,I.,etal. Languagemodelsareunsupervised
ing data-constrained language models. arXiv preprint multitasklearners. OpenAIblog,1(8):9,2019.
arXiv:2305.16264,2023.
Rafailov,R.,Sharma,A.,Mitchell,E.,Manning,C.D.,Er-
Nambiar,A.,Heflin,M.,Liu,S.,Maslov,S.,Hopkins,M., mon, S., and Finn, C. Direct preference optimization:
and Ritz, A. Transforming the language of life: trans- Your language model is secretly a reward model. Ad-
formerneuralnetworksforproteinpredictiontasks. In vances in Neural Information Processing Systems, 36,
Proceedingsofthe11thACMinternationalconference 2024.
onbioinformatics,computationalbiologyandhealthin-
Rao,R.,Bhattacharya,N.,Thomas,N.,Duan,Y.,Chen,P.,
formatics,pp.1–8,2020.
Canny, J., Abbeel, P., andSong, Y. Evaluatingprotein
Nijkamp, E., Ruffolo, J., Weinstein, E. N., Naik, N., and transferlearningwithtape. Advancesinneuralinforma-
Madani, A. Progen2: exploringtheboundariesofpro- tionprocessingsystems,32,2019.
teinlanguagemodels. arXivpreprintarXiv:2206.13517,
Rao,R.M.,Liu,J.,Verkuil,R.,Meier,J.,Canny,J.,Abbeel,
2022.
P.,Sercu,T.,andRives,A. Msatransformer. InInterna-
tionalConferenceonMachineLearning,pp.8844–8856.
Nourani, E., Asgari, E., McHardy, A. C., and Mofrad,
PMLR,2021.
M.R. Tripletprot: deeprepresentationlearningofpro-
teinsbasedonsiamesenetworks. IEEE/ACMTransac- Rives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J.,
tionsonComputationalBiologyandBioinformatics,19 Guo,D.,Ott,M.,Zitnick,C.L.,Ma,J.,andFergus,R.Bi-
(6):3744–3753,2021. ologicalstructureandfunctionemergefromscalingunsu-
pervisedlearningto250millionproteinsequences.PNAS,
OpenAI. Gpt-4technicalreport,2023.
2019. doi: 10.1101/622803. URL https://www.
biorxiv.org/content/10.1101/622803v4.
Orengo, C. A., Michie, A. D., Jones, S., Jones, D. T.,
Swindells, M. B., and Thornton, J. M. Cath–a hierar- Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
chicclassificationofproteindomainstructures. Structure, Ommer,B. High-resolutionimagesynthesiswithlatent
5(8):1093–1109,1997. diffusionmodels,2021.
Ouyang,L.,Wu,J.,Jiang,X.,Almeida,D.,Wainwright,C., Sanh,V.,Webson,A.,Raffel,C.,Bach,S.H.,Sutawika,L.,
Mishkin,P.,Zhang,C.,Agarwal,S.,Slama,K.,Ray,A., Alyafeai,Z.,Chaffin,A.,Stiegler,A.,LeScao,T.,Raja,
et al. Training language models to follow instructions A.,etal. Multitaskpromptedtrainingenableszero-shot
withhumanfeedback. AdvancesinNeuralInformation task generalization. In ICLR 2022-Tenth International
ProcessingSystems,35:27730–27744,2022. ConferenceonLearningRepresentations,2022.
13DiffusionLanguageModelsAreVersatileProteinLearners
Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and model. https://github.com/tatsu-lab/
Ganguli,S.Deepunsupervisedlearningusingnonequilib- stanford_alpaca,2023.
riumthermodynamics. InBach,F.andBlei,D.(eds.),In-
ternationalConferenceonMachineLearning,volume37 Touvron,H.,Lavril,T.,Izacard,G.,Martinet,X.,Lachaux,
ofProceedingsofMachineLearningResearch,pp.2256– M.-A.,Lacroix,T.,Rozie`re,B.,Goyal,N.,Hambro,E.,
2265, Lille, France, 07–09 Jul 2015. PMLR, PMLR. Azhar,F.,Rodriguez,A.,Joulin,A.,Grave,E.,andLam-
URLhttps://proceedings.mlr.press/v37/ ple,G. Llama: Openandefficientfoundationlanguage
sohl-dickstein15.html. models,2023a.
Song,Y.andErmon,S. Generativemodelingbyestimating Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
gradients of the data distribution. Advances in Neural A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,
InformationProcessingSystems,32,2019. Bhosale,S.,etal. Llama2: Openfoundationandfine-
tuned chat models. arXiv preprint arXiv:2307.09288,
Song,Y.,Sohl-Dickstein,J.,Kingma,D.P.,Kumar,A.,Er-
2023b.
mon,S.,andPoole,B. Score-basedgenerativemodeling
throughstochasticdifferentialequations. InInternational
Trippe,B.L.,Yim,J.,Tischer,D.,Baker,D.,Broderick,T.,
ConferenceonLearningRepresentations,2020.
Barzilay,R.,andJaakkola,T.Diffusionprobabilisticmod-
Strodthoff, N., Wagner, P., Wenzel, M., and Samek, W. elingofproteinbackbonesin3dforthemotif-scaffolding
Udsmprot: universaldeepsequencemodelsforprotein problem. arXivpreprintarXiv:2206.04119,2022.
classification. Bioinformatics,36(8):2401–2409,2020.
Unsal,S.,Atas,H.,Albayrak,M.,Turhan,K.,Acar,A.C.,
Sturmfels,P.,Vig,J.,Madani,A.,andRajani,N.F. Profile andDog˘an,T. Learningfunctionalpropertiesofproteins
prediction: Analignment-basedpre-trainingtaskforpro- withlanguagemodels. NatureMachineIntelligence,4
teinsequencemodels. arXivpreprintarXiv:2012.00195, (3):227–245,2022.
2020.
vanKempen,M.,Kim,S.S.,Tumescheit,C.,Mirdita,M.,
Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu,
Lee, J., Gilchrist, C. L., So¨ding, J., and Steinegger, M.
Y. Roformer: Enhancedtransformerwithrotaryposition
Fastandaccurateproteinstructuresearchwithfoldseek.
embedding. arXivpreprintarXiv:2104.09864,2021.
NatureBiotechnology,pp.1–4,2023.
Su,J.,Han,C.,Zhou,Y.,Shan,J.,Zhou,X.,andYuan,F.
Saprot: Proteinlanguagemodelingwithstructure-aware Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,
vocabulary. bioRxiv,pp.2023–10,2023. L.,Gomez,A.N.,Kaiser,L.,andPolosukhin,I.Attention
isallyouneed. InGuyon,I.,vonLuxburg,U.,Bengio,
Sun, T. and Qiu, X. Moss. https://github.com/ S.,Wallach,H.M.,Fergus,R.,Vishwanathan,S.V.N.,
OpenLMLab/MOSS,2023. andGarnett,R.(eds.),AdvancesinNeuralInformation
Processing Systems 30: Annual Conference on Neural
Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to
Information Processing Systems 2017, December 4-9,
sequencelearningwithneuralnetworks. InGhahramani,
2017,LongBeach,CA,USA,volume30,pp.5998–6008,
Z.,Welling,M.,Cortes,C.,Lawrence,N.D.,andWein-
2017.
berger, K. Q. (eds.), Advances in Neural Information
Processing Systems 27: Annual Conference on Neural
Verkuil,R.,Kabeli,O.,Du,Y.,Wicky,B.I.,Milles,L.F.,
Information Processing Systems 2014, December 8-13
Dauparas,J.,Baker,D.,Ovchinnikov,S.,Sercu,T.,and
2014,Montreal,Quebec,Canada,volume27,pp.3104–
Rives, A. Languagemodelsgeneralizebeyondnatural
3112, 2014. URL https://proceedings.
proteins. bioRxiv,pp.2022–12,2022.
neurips.cc/paper/2014/hash/
a14ac55a4f27472c5d894ec1c3c743d2-Abstract.
Vignac,C.,Krawczuk,I.,Siraudin,A.,Wang,B.,Cevher,
html.
V., and Frossard, P. Digress: Discrete denoising diffu-
sionforgraphgeneration. InTheEleventhInternational
Suzek,B.E.,Wang,Y.,Huang,H.,McGarvey,P.B.,Wu,
ConferenceonLearningRepresentations,2022.
C.H.,andConsortium,U. Unirefclusters: acomprehen-
siveandscalablealternativeforimprovingsequencesim-
Vincent,P.,Larochelle,H.,Lajoie,I.,Bengio,Y.,Manzagol,
ilaritysearches. Bioinformatics,31(6):926–932,2015.
P.-A., and Bottou, L. Stacked denoising autoencoders:
Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, Learningusefulrepresentationsinadeepnetworkwith
X., Guestrin, C., Liang, P., and Hashimoto, T. B. alocaldenoisingcriterion. Journalofmachinelearning
Stanford alpaca: An instruction-following llama research,11(12),2010.
14DiffusionLanguageModelsAreVersatileProteinLearners
Wang, A. and Cho, K. BERT has a mouth, and it must Yang, K. K., Lu, A. X., and Fusi, N. Convolutions are
speak: BERTasaMarkovrandomfieldlanguagemodel. competitivewithtransformersforproteinsequencepre-
In Proceedings of the Workshop on Methods for Opti- training. bioRxiv,pp.2022–05,2022a.
mizing and Evaluating Neural Language Generation,
Yang,K.K.,Zanichelli,N.,andYeh,H. Maskedinverse
pp. 30–36, Minneapolis, Minnesota, June 2019. Asso-
foldingwithsequencetransferforproteinrepresentation
ciation for Computational Linguistics. doi: 10.18653/
v1/W19-2304. URLhttps://www.aclweb.org/ learning. bioRxiv,pp.2022–05,2022b.
anthology/W19-2304.
Ye,J.,Zheng,Z.,Bao,Y.,Qian,L.,andGu,Q. Diffusion
languagemodelscanperformmanytaskswithscalingand
Watson,J.L.,Juergens,D.,Bennett,N.R.,Trippe,B.L.,
instruction-finetuning. arXivpreprintarXiv:2308.12219,
Yim,J.,Eisenach,H.E.,Ahern,W.,Borst,A.J.,Ragotte,
2023a.
R.J.,Milles,L.F.,etal. Denovodesignofproteinstruc-
ture and function with rfdiffusion. Nature, 620(7976):
Ye,J.,Zheng,Z.,Bao,Y.,Qian,L.,andWang,M. Dinoiser:
1089–1100,2023.
Diffusedconditionalsequencelearningbymanipulating
noises. arXivpreprintarXiv:2302.10025,2023b.
Wei,J.,Bosma,M.,Zhao,V.,Guu,K.,Yu,A.W.,Lester,
B., Du, N., Dai, A. M., and Le, Q. V. Finetuned lan-
Yi,K.,Zhou,B.,Shen,Y.,Lio,P.,andWang,Y.G. Graph
guage models are zero-shot learners. In International
denoisingdiffusionforinverseproteinfolding. InThirty-
ConferenceonLearningRepresentations,2021.
seventh Conference on Neural Information Processing
Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B.,
Systems,2023. URLhttps://openreview.net/
Borgeaud,S.,Yogatama,D.,Bosma,M.,Zhou,D.,Met-
forum?id=u4YXKKG5dX.
zler,D.,etal.Emergentabilitiesoflargelanguagemodels.
Yim,J.,Trippe,B.L.,DeBortoli,V.,Mathieu,E.,Doucet,
TransactionsonMachineLearningResearch,2022a.
A.,Barzilay,R.,andJaakkola,T. Se(3)diffusionmodel
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., withapplicationtoproteinbackbonegeneration. arXiv
Chi,E.H.,Le,Q.V.,Zhou,D.,etal. Chain-of-thought preprintarXiv:2302.02277,2023.
prompting elicits reasoning in large language models.
Yuan,H.,Yuan,Z.,Tan,C.,Huang,F.,andHuang,S.Seqdif-
InAdvancesinNeuralInformationProcessingSystems,
fuseq: Textdiffusionwithencoder-decodertransformers.
volume35,pp.24824–24837,2022b.
arXivpreprintarXiv:2212.10325,2022.
Wu,K.E.,Yang,K.K.,Berg,R.v.d.,Zou,J.Y.,Lu,A.X.,
Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M.,
andAmini,A.P. Proteinstructuregenerationviafolding
Yang,Z.,Xu,Y.,Zheng,W.,Xia,X.,etal. Glm-130b:
diffusion. arXivpreprintarXiv:2209.15611,2022a.
An open bilingual pre-trained model. arXiv preprint
Wu, R., Ding, F., Wang, R., Shen, R., Zhang, X., Luo, arXiv:2210.02414,2022.
S., Su, C., Wu, Z., Xie, Q., Berger, B., et al. High-
Zheng,L.,Yuan,J.,Yu,L.,andKong,L. Areparameter-
resolutiondenovostructurepredictionfromprimaryse-
izeddiscretediffusionmodelfortextgeneration. arXiv
quence. BioRxiv,pp.2022–07,2022b.
preprintarXiv:2302.05737,2023a.
Wu,T.,Fan,Z.,Liu,X.,Gong,Y.,Shen,Y.,Jiao,J.,Zheng,
Zheng,Z.,Deng,Y.,Xue,D.,Zhou,Y.,YE,F.,andGu,Q.
H.-T.,Li,J.,Wei,Z.,Guo,J.,etal. Ar-diffusion: Auto-
Structure-informedlanguagemodelsareproteindesign-
regressive diffusion model for text generation. arXiv
ers. InInternationalConferenceonMachineLearning,
preprintarXiv:2305.09515,2023.
2023b.
Xiao,Y.,Qiu,J.,Li,Z.,Hsieh,C.-Y.,andTang,J. Modeling
proteinusinglarge-scalepretrainlanguagemodel. arXiv
preprintarXiv:2108.07435,2021.
Xu,M.,Zhang,Z.,Lu,J.,Zhu,Z.,Zhang,Y.,Chang,M.,
Liu,R.,andTang,J. Peer: acomprehensiveandmulti-
taskbenchmarkforproteinsequenceunderstanding. Ad-
vances in Neural Information Processing Systems, 35:
35156–35173,2022.
Yang,K.K.,Wu,Z.,andArnold,F.H. Machine-learning-
guideddirectedevolutionforproteinengineering. Nature
methods,16(8):687–694,2019.
15DiffusionLanguageModelsAreVersatileProteinLearners
A ReparameterizaedDiscreteDiffusion Algorithm1SamplingfromRDM
Models(RDM) Input: trainednetworkf θ(·)andtemperatureτ.
Output: generatedsamplex(0).
DPLM uses reparameterized discrete diffusion model
forn=1,2,...,N do
(RDM) as its discrete diffusion framework (Zheng et al.,
Initializex ∼q ;
2023a). Herewebrieflysummarizeitsbasictrainingand T,n noise
Initializeb =0;
T,n
sampling. Please refer to Zheng et al. (2023a) for more
endfor
details.
fort=T,...,1do
Zhengetal.(2023a)showsthatthebackwardtransitionof forn=1,2,...,N do
discretediffusionmodelsq(x t−1|x t,x 0)canberewritten
Drawx ∼Categorical(f (x )/τ);
(cid:101)0,n θ t,n
as Generatev accordingtologp(x )
t−1,n (cid:101)0,n
ifb
t,n
=1then
q(x t−1|x t,x 0) Drawu(1) ∼q ;
= Cat(cid:16) (cid:16)x t−1;λ( t−1) 1x t+(1−λ( t−1) 1)q noise(cid:17) ,if (cid:17)x t =x 0 x t−1,n =t,n v t( −1) 1,n no xise t,n+(cid:16) 1−v t( −1) 1,n(cid:17) u( t,1 n) ;
Cat x t−1;λ( t−2) 1x t+(1−λ( t−2) 1)q noise(x t) ,ifx
t
̸=x
0
else
Drawu(2) ∼q (x );
t,n noise t,n
(cid:16) (cid:17)
where q noise(x t) = β tx t +(1−β t)q noise, and both λ( t−1) 1 x t−1,n =v t( −2) 1,nx (cid:101)0,n+ 1−v t( −2) 1,n x( t,2 n) ;
and λ(2) are constants relating to β and β . This re- endif
t−1 t t−1 Letb =b ∧v(1) ∨v(2) ;
formulationinterpretsthebackwardtransitionasamixture t−1,n t,n t−1,n t−1,n
endfor
distribution. Samplingfromitisequivalenttofirstsampling
endfor
from a Bernoulli distribution and then the corresponding
componentdistribution,i.e.,
Returnx 0,1:N.
(cid:16) (cid:17)
v(1) ∼Bernoulli λ(1) , u(1) ∼Cat(u;p=q ), B AdditionalExperimentalDetails
t−1 t−1 t noise
v(2) ∼Bernoulli(cid:16) λ(2) (cid:17) , u(2) ∼Cat(u;p=q (x )), B.1 Pre-trainingofDPLM
t−1 t−1 t noise t
Duringthetrainingphase,weinvestigatetwodifferentap-
proaches: (1) training with the diffusion objective from
 (cid:16) (cid:17)
x
=v t( −1) 1x t+ 1−v t( −1)
1
u( t1), ifx
t
=x
0 .
s cc or na st ic sh ts,a on fd in(2 it) iaw lh tra at iw nie ngre wfe ir thto ta hs et mw ao s- kst ea dge lat nra gi un ain gg e, mw oh dic eh
l-
t−1 (cid:16) (cid:17)
v t( −2) 1x t+ 1−v t( −2)
1
u( t2), ifx
t
̸=x
0
ing(MLM)objectivefollowedbycontinuoustrainingwith
thediffusionobjective. WefindthattrainingDPLMfrom
scratchisabitchallengingandconvergesslowly. Training
Thisreparameterizesthetransitionsintoq(x ,v |x ,x )
t t−1 t 0
with MLM first allows the model to get a better starting
and p (x ,v |x ). With this reparameterization, the
θ t−1 t−1 t
point,andthengetthefinalmodelthroughdiffusionadapta-
trainingobjectiveofdiffusionmodels(i.e.,thevariational
tion,sharingsimilarideaasYeetal.(2023a). Thiskindof
boundofnegativelog-likelihood)becomes
strategysimilartocurriculumlearningmakesthetraining
(cid:20) (cid:21) effect better. We can also use community available pre-
p (x ,x ,v )
−E (x ,v |x ) log θ 0 1:T 1:T trained Masked-LMs such as ESM2 (Lin et al., 2022) as
q 1:T 1:T 0 q(x ,v |x )
1:T 1:T 0 ourfirst-stagemodel. Wefoundthatthevalidationlossof
T
(cid:88) two-stagetrainingfromourin-housetrainedMasked-LM
=L + L +const.,
1 t andpre-trainedESM2isveryclose,andasshowninFig,
t=2 thereisalsoalmostnoperformancegapintheunconditional
sampling.
where L = −E [logp (x |x )] and Zheng et al.
1 q(x1|x0) θ 0 1
(2023a)showsthatL tcanbesimplifiedintoNotably,train- B.2 AR-LMbaseline
ingwithdifferentnoiseschedulesonlydiffersintheweight- WepretrainaAR-LMusingautoregressivetrainingobjec-
ingoftheobjective. tive. InordertobecomparablewithDPLM,theautoregres-
Duringsampling,RDMleveragesthisobservationandpro- sivelanguagemodelwetrainedadoptsthesamearchitecture
poses to employ a discriminative approach. Specifically, asDPLM,whichisthesamearchitectureasESM2. Tobe
it denoises a token only when it receives a top-k score capableofadaptingtheautoregressivetraining,wemodify
(log-probability) from the network where k in each step the mask matrix of the attention module to causal mask,
isdeterminedbyadenoisingschedule. whichguaranteeseachtokencanonlyattendtheprevious
16DiffusionLanguageModelsAreVersatileProteinLearners
positionandkeepunseenforfuture. Thetrainingobjective Table3.Resultsofthesuccessrateofeachproblem,thenumber
isnextwordprediction,andweprocesstheinputsequence of the solved problems and the average success rate across 17
withteacherforcingforefficientparalleltraining. During motif-scaffoldingproblems.Herewefollowpreviousworktouse
decoding, we start with <bos> token, sample one token OmegaFoldasthefoldingmodel.
eachtimestepfromlefttoright,andthesampledtokenin seq-only struct-cond.
thecurrenttimestepwillbeconcatenatedtotheendofthe
EvoDiff DPLM RFDiffusion DPLM
sequence,becominginputfornexttimestep. Thedecoding
processterminatesuntil<eos>tokenissampled. Because 1bcf 0.39 0.99 1.00 1.00
1prw 0.87 0.96 0.08 0.99
we can not know when to obtain the <eos> token in ad-
1qjg 0.00 0.00 0.00 0.00
vance,wecannotdecidethelengthofsampledsequence.
1ycr 0.13 0.52 0.74 0.78
Weattempttoforcethesamplinglengthbymodifyingthe 2kl8 0.03 0.05 0.88 0.05
samplingprobability: theprobabilityof<eos>is1when 3ixt 0.26 0.20 0.25 0.33
andonlywhenthesequencelengthisuptothepredefined 4jhw 0.00 0.00 0.00 0.00
length,while0inalltheprevioustimesteps. However,we 4zyp 0.00 0.01 0.40 0.01
observethiswilldeclinethequalityofsampledsequence 5ius 0.00 0.10 0.02 0.10
5tpn 0.00 0.00 0.61 0.00
significantly.
5trv 0.00 0.00 0.22 0.00
B.3 Amodifiedunconditionalsamplingstrategy 5wn9 0.00 0.01 0.00 0.01
5yui 0.06 0.42 0.00 0.63
ThesamplingalgorithmproposedintheZhengetal.(2023a)
6e6r 0.16 0.84 0.71 0.84
is to unmask positions with top-k prediction score (log-
6exz 0.00 0.01 0.42 0.01
probability) predicted by p θ(x(0)|x(t)), and mask all the 6vw1 0.00 0.00 0.69 0.00
restpositionineachdenoisingstep. However,wefindthat 7mrx 0.00 0.59 0.07 0.59
ifweusethissamplingalgorithmtosamplesequenceun-
passrate 7/17 12/27 13/17 12/17
conditionally,thesampledsequencewillcollapsetotrivial avg.successrate 0.09 0.27 0.36 0.31
pattern, such as repeating with a single amino acid. We
suggestthisisbecause,withoutanyadditionalconditions,
themodelinitiallytendstogiveahigherpredictionscore Gumbel-Maxtrickhelpsussampleanaminoacidwitha
to the amino acids that appear frequently in the training slightly modified prediction score while maintaining the
set. Subsequently, based on these high-frequency amino originaldistribution. Asaresult,thepreviouslydominant
acid tokens, the model will continue to sample the same amino acid with the highest prediction score may be dis-
tokensbesidethesetokenswithhighconfidence. Theother carded,andavarietyofotheraminoacidsmayberetained,
aminoacidcanalsobesampled,butpossiblywithalower therebyavoidingfallingintoatrivialpatternsuchasrepeat-
predictionscore,therebyleadingtobedroppedaccording ingwithasingleaminoacid.Wefindthatthistechniquecan
tothetop-ksamplingalgorithm. Then,thisaminoacidwill significantlyreducethenumberoftrivialcasesandfurther
spreadthroughouttheentiresequencelikeavirus,forming improvethediversity.
asequencecomposedentirelyofthesameaminoacids.
C AdditionalExperimentalResults
In response, we impose a slight disturbance during sam-
pling,utilizingtheGumbel-Maxtrick. TheGumbel-Max
C.1 Sequence-conditionalgeneration:
trickisaprocedurefordrawingasamplefromacategori-
motif-scaffolding
caldistributionusingGumbel-distributedrandomvariables.
Theoverallmotif-scaffoldingresultsareshowninTab.3.
Let’sassumewehaveadiscreterandomvariableX with
We sample 100 scaffold sequences for each motif scaf-
distributionp (x = i) = p fori = 1,...,K. Now,con-
θ i
folding case, and compute the success rate according to
sider the variables g = −log(−logU ) where U is a
i i i
the standard mentioned in section 4.3.1. Furthermore,
variableuniformlydistributedon(0,1]. Theg arerandom
i
we also show the pass rate (e.g. the number of solved
variablesfollowingaGumbeldistribution. Thekeytothe
problems) and the average success rate for all problems.
Gumbel-Maxtrickisthisrelationship:
Weusesequence-onlyandstructure-conditionedsampling
i∗ =argmax{p˜ i}, wherep˜∝expg i+logp i (6) paradigms. Forsequence-onlysampling,DPLMgenerates
i scaffold according to the motif sequence fragment. For
Thisoperationprovidesasamplefromthediscretedistribu- structure-conditionedsampling,DPLMmakesgeneration
tionp (x=i). Inotherwords,thecategorycorresponding byleveragingbothsequenceandstructureinformationof
θ
to the maximum value is the results of sampling. But in motif. Specifically,asnotedinsection4.3.2,weutilizethe
theotherhand,themaximumvalue,i.e.g +logp ,isnot pre-trainedGVPTransformerEncoderandstructuraladapter
i i
equaltotheoriginallog-probability,whichisactuallythe toprocessthemotifstructure. DPLMisabletosolve12of
predictionscoreinoursamplingalgorithm. Therefore,the 17motifscaffoldingproblems. Theoverallsuccessrateis
17DiffusionLanguageModelsAreVersatileProteinLearners
Table4. Motif-scaffoldingresultsevaluatedbyESMFold. awareness,wefollow LM-DESIGN (Zhengetal.,2023b)
andplaceastructuraladapterafterthelastlayerofDPLM,
seq-only struct-cond.
whichcanattachthestructureinformationtotheoriginal
EvoDiff DPLM DPLM outputprobability. Theoverallarchitectureofthestructural
1bcf 0.38 1.00 1.00 adapter is constituted by three components, i.e., a struc-
1prw 0.36 0.75 0.81 tureencoder,apLMassequencedecoder,andastructural
1qjg 0.00 0.00 0.00 adapterthatbridgesboth. Wecanutilizeanarbitrarypre-
1ycr 0.03 0.27 0.48 trainedstructureencodertoprocessthe3Dcoordinatesand
2kl8 0.00 0.01 0.01 provide structure information for DPLM. For pLMs as
3ixt 0.09 0.15 0.37 thesequencedecoderside,weprimarilyusedtheDPLM,
4jhw 0.00 0.00 0.00 withitspretrainedmodelweights. Thestructuraladapter
4zyp 0.00 0.00 0.01 composesamulti-headattentionthatqueriesstructureinfor-
5ius 0.00 0.00 0.00
mationfromthestructureencoder,followedbyabottleneck
5tpn 0.00 0.00 0.00
feedforward network (FFN) to impose non-linearity and
5trv 0.00 0.00 0.00
abstract features/representations. ROPE (Su et al., 2021)
5wn9 0.00 0.00 0.00
was used the supplement multi-head attention for better
5yui 0.05 0.94 0.94
modelingofpositionalinformation. Inallourexperiments,
6e6r 0.03 0.79 0.79
onlyonestructuraladapterwasplacedafterthelastlayerof
6exz 0.00 0.01 0.01
6vw1 0.00 0.00 0.00 DPLM,followingZhengetal.(2023b).
7mrx 0.00 0.54 0.54 Trainingandinferencedetails. Duringtraining,wefreeze
theparametersofthestructureencoderand DPLM,only
passrate 6/17 9/27 10/17
optimizing the structural adapter with the simplified dis-
avg.successrate 0.06 0.26 0.29
crete diffusion objective (Zheng et al., 2023a). However,
wefindthatthereisanexposurebiasproblemhere: DPLM
Table5. AblationstudyontheCATH4.3benchmark,whichw/
learnsthereversedenoisingprocessbasedontheground
draftmeansthatthereverseprocessisbasedonthex d(t r) aft. truthcontext,i.e.x(t),whichisobtainedbyaddingnoise
struct.eval. onthegroundtruthsequence,i.e.x(0). Duringinference,
Models Trainable AAR
Params. scTM pLDDT DPLMhastodenoisegiventhecontextpredicted,which
isnotalwaysright,leadingtotraining-inferenceinconsis-
LM-DESIGN 6.3M/650M 56.49 0.85 74.89
tency. Therefore,weslightlymodifythetrainingobjective
DPLM 6.3M/650M 55.75 0.83 73.72 inEq.(4). Specifically,weobtainx(t)byaddingnoiseon
DPLM(w/ draft) 6.3M/650M 56.61 0.86 76.78
thedraftsequencegeneratedbythepretrainedstructureen-
coder,ratherthanthegroundtruthx(0),whichwereferto
0.27forsequence-onlysampling,while0.31forstructure-
asx(t)
. ThenDPLMwilllearnthereverseprocessthat
draft
conditionedsampling. Itshouldbenotedthatnotallprob- reconstructsthex(0)giventhex(t)
,asshowninEq.(7).
draft
lemsaresuitableforusingstructureinformation. Werec-
Sincethedraftsequenceisavailablebothintrainingandin-
ommendusingstructure-conditionedsamplingfor1YCR,
ferencetime,theissueofexposurebiasismitigated.Wefind
1PRW,3IXTand5YUI,whilesequence-onlysamplingfor
thistechniquecanfurtherboosttheperformanceofDPLM
others.
intheinversefoldingtask,asillustratedintheTab.5
Evaluationwithmoreadvancedfoldingmodel.Moreover,
wealsoinvestigateevaluationwithotherstructurepredic-
tionmodels,suchasESMFold(Linetal.,2022). Results J =E (cid:20) λ(t) (cid:88) b (t)·logp (x(0)|x(t) )(cid:21) ,
areshowninTab.4,weconsidertheAlphaCarbon(CA) t q(x(0)) i θ i draft
pLDDTscorepredictedbyESMFoldastheoverallpLDDT 1≤i≤L
(7)
scoreoftheaminoacid. WeobservethatESMFoldjudges
morestrictlythanOmegaFold. Whenweevaluatescaffold
byESMFold,thereisaslightdeclineintheoverallpassrate Atinferencetime,wefollowtheDPLMgenerativeprocess,
andaveragesuccessrate,comparedwiththeevaluationof except that we obtain protein sequence via greedy deter-
OmegaFold. ministic decoding, instead of random sampling from the
distribution. Besides,consideringthatwehavehadanun-
C.2 Structure-conditionalgeneration: inversefolding
conditionalmodel,i.e. theDPLMitself,andaconditional
Modelarchitecture. DPLMonlytakesaminoacidtokens model,i.e.,theDPLMwithstructuraladapter,wecanalso
as input, instead of structure formats such as 3D coordi- seamlesslyutilizetheclassifier-freeguidanceparadigmdur-
nates. Therefore,inordertoendowDPLMwithstructural inginference.
18DiffusionLanguageModelsAreVersatileProteinLearners
C.3 Classifier-freeguidance D RelatedWork
Classifier-freeguidance(Ho&Salimans,2021)hasbeen
D.1 LanguageModels
shownasaneffectivewaytoenhanceconditionaldiffusion
Thedominantparadigmoflanguagemodelsisautoregres-
models. Likewise, for DPLM, we can derive an implicit
sivelanguagemodels,whichbreaksdownthemutualdistri-
classifierusingtheBayesrule
butionoverthetokensofasequenceintoconditionalproba-
bilitiesviathechainrulep(x[1:N])=(cid:81)N p(x[i]|x[1:i−1])
q(y|x(t−1))=q(y|x(t−1),x(t)) i=1
and generates tokens by ancestral sampling from left to
q(x(t−1)|x(t),y) right(Bengioetal.,2000;Sutskeveretal.,2014;Vaswani
= q(y|x(t)).
q(x(t−1)|x(t)) et al., 2017). Recently, researchers propose the non-
autoregressivelanguagemodelsasanalternative(Guetal.,
2018). Thesemodelsdonotneedtoobeythelefttoright
Ifwealreadyhaveanunconditionalmodelp (x(t−1)|x(t))
θ generation order (Qian et al., 2022; Huang et al., 2023)
andaconditionalmodelp (x(t−1)|x(t),y)astheestimates,
θ anddemonstratecompetitiveorsuperiorperformancecom-
then by substituting this implicit classifier into Eq. 5, we
paredtotheirautoregressivecounterpartacrossawiderange
canobtain
ofdomainsincludinglanguages(Qianetal.,2021;Huang
etal.,2023;Qianetal.,2022;Huangetal.,2023;Zheng
x(t−1) ∼p θ(x(t−1)|x(t))p ϕ(y|x(t−1))η etal.,2023a),speeches(Kimetal.,2021),proteins(Zheng
∝p
(x(t−1)|x(t))(cid:0)p θ(x(t−1)|x(t),y)(cid:1)η et al., 2023b), and molecules (Hoogeboom et al., 2022).
θ p (x(t−1)|x(t)) Among the numerous non-autoregressive language mod-
θ
els,diffusionlanguagemodels(Lietal.,2022;Gongetal.,
=p (x(t−1)|x(t),y)η·p (x(t−1)|x(t))(1−η),
θ θ 2022; Zheng et al., 2023a) have emerged as a solid and
promising framework. Pretraining language models on a
wherein when η = 1, it is equivalent to sampling from massive scale of unlabeled data markedly improves their
theoriginalconditionalDPLMwithoutguidance,whereas downstreamtaskperformance(Mikolovetal.,2013;Peters
η >1,wenotonlyprioritizetheconditionalmodeltocon- etal.,2018;Radfordetal.,2018;Devlinetal.,2019). As
tributemorebutalsodiscouragethesamplesfrommoving datavolumeandmodelsizesscaleup,thetraininglossof
awayfromtheunconditionaldistribution. Inotherwords, languagemodelspredictablydeclines(Kaplanetal.,2020;
it reduces the chance of generating samples that do not Hoffmannetal.,2022;Muennighoffetal.,2023),anden-
useconditioninginformation,infavorofthesamplesthat hancingdownstreamtaskperformanceevenwithoutspecific
explicitlydo. tuning(Radfordetal.,2019). GPT3(Brownetal.,2020)
NotethatwhenweuseadaptertuningtoadaptDPLMfor isasignificantpointinthejourney,takingmodelsizesto
conditionalgeneration,weonlyfinetunethenewly-added 175Bparameters,proposingin-contextlearningtobolster
parameters,whichmeansthatwecanalreadyaccessboth languagemodels’competenceinsolvingcertaintaskswith
theunconditionalmodel(originalDPLM)andconditional onlyahandfulofdemonstrations. Furthermore,Weietal.
model (the adapter-tuned model) simultaneously for free. (2021);Sanhetal.(2022);Ouyangetal.(2022)introduce
AsdemonstratedinFig.5onstructure-conditionedsequence instructiontuning,finetuningpretrainedlanguagemodelson
generation, wecanfindthat DPLM asadiffusionmodel seriesoftasksdescribedviainstructions,whichelicitsthe
canbenefitfromclassifier-freeguidance,improvingitscon- instructionfollowingabilityofmodelsandsignificantlyen-
ditionalgenerationimmediately. hancestheirzero-shotperformanceonunseentasks. More
impressively,sufficientlylargelanguagemodelsexhibitthe
56.2 emergent abilities such as multi-step reasoning (Kojima
et al.,2022; Wei et al.,2022a;b), which small modelsdo
56.1
notpossess(Fuetal.,2023). Empoweredbylargelanguage
56.0 models,helpfulapplicationssuchasconversationalAIsys-
tems1andautonomousagents2havegarneredmuchinterest.
55.9
Although the most capable models at the moment are re-
55.8 strictedinaccess,open-sourcedefforts(Zengetal.,2022;
55.7 Touvron et al., 2023a;b; Taori et al., 2023; Chiang et al.,
2023;Sun&Qiu,2023)havelargelyenhancedthepublic
0.00 0.05 0.10 0.15 0.20 0.25 0.30
accessibilityofpowerfullargelanguagemodels.
1
Figure5.Classifier-freeguidanceenhancesstructure-conditioned 1https://chat.openai.com/
sequencegeneration(inversefolding). 2https://github.com/Significant-Gravitas/
Auto-GPT
19
RAADiffusionLanguageModelsAreVersatileProteinLearners
D.2 ProteinLanguageModels (2023); Zheng et al. (2023b), there are close relationship
between discrete diffusion models and masked language
Thanksfortheabundanceof1Daminoacidsequences,there
models(MLM),awidelyadoptedpretrainingparadigmin
isgrowinginterestindevelopingproteinLMsatthescale
NLP(Devlinetal.,2019;Liuetal.,2019). Followingthis
ofevolution,suchastheseriesofESM(Rivesetal.,2019;
line, Ye et al. (2023a) propose scaling discrete diffusion
Linetal.,2022), TAPE(Raoetal.,2019), ProtTrans(El-
LMswithdiffusiveadaptation,showingstrongperformance
naggar et al., 2021), PRoBERTa (Nambiar et al., 2020),
onseveralconditionaltextgenerationtasks,andaccessing
PMLM (He et al., 2021), ProteinLM (Xiao et al., 2021),
zero-shotinstructionfollowing,few-shotin-contextlearn-
PLUS (Min et al., 2021), Adversarial Masked LMs (Mc-
ingandthepromiseofstructuredreasoningwithinstruction
Dermottetal.,2021),ProteinBERT(Brandesetal.,2022),
tuning.
CARP(Yangetal.,2022a)inmaskedlanguagemodeling
(MLM)paradigm,ProtGPT2(Ferruzetal.,2022)incausal D.4 ProteinStructureDiffusionModels
languagemodelingparadigm,andseveralothers(Melnyk
Diffusionmodelshavebecomepopulartoolsinstructural
etal.,2022;Madanietal.,2021;Unsaletal.,2022;Nourani
biology for protein generation, and their utility has been
etal.,2021;Luetal.,2020;Sturmfelsetal.,2020;Strodthoff
demonstratedacrossarangeofgenerativetasksinrecent
etal.,2020). Theseproteinlanguagemodelsexhibitremark-
years. Trippeetal. (2022), alongwithothers, have intro-
ablegeneralizationabilityonvariousdownstreamtasksand
ducedseveraldiffusionmodelvariants,eachwithitsunique
beabletocaptureevolutionaryinformationaboutsecondary
approach. Forinstance,whilesomemodelsfocusongener-
andtertiarystructuresfromsequencesalone. Meanwhile,
atingtheproteinbackbonebydiffusingoverproteincoordi-
recentstudyshowsthesemodels’potencyinrevealingpro-
nates,others,suchasthoseproposedbyWuetal.(2022b),
tein structures (Lin et al., 2022), predicting the effect of
targetinter-residueangles. Lin&AlQuraishi(2023)and
sequence variation on function (Meier et al., 2021), anti-
Yimetal.(2023)havedevelopedmodelsthathandleboth
bodyinfilling(Melnyketal.,2022)andmanyothergeneral
the position and orientation of residue frames. RFDiffu-
purposes(Rivesetal.,2019). Simultaneously,Verkuiletal.
sion(Watsonetal.,2023)isamodelthatassistsindesigning
(2022)demonstratethatthelargescaleproteinLMscangen-
proteinstructuresforspecificfunctions,suchasenzymes.
eratedenovoproteinsbygeneralizingbeyondnaturalpro-
Itisversatileinproteindesignandhasbeenusedtocreate
teins,boththeoreticallyandexperimentallyvalidatingtheir
therapeuticproteins,withsomedesignsbeingconfirmedin
hypothesisinexhaustivedetail,inwhichpLMsdemonstrate
thelaboratory. ProteinSGM(Leeetal.,2022)isamodel
competency in designing protein structure despite being
that uses 2D matrices, which represent the distances and
exclusivelytrainedonsequences.
anglesbetweenproteinparts,tocreate3Dproteinstructures
fornovelproteindesigns. FoldingDiff(Wuetal.,2022a)is
D.3 DiffusionLanguageModels
amodelthatgeneratesproteinsequencesexpectedtofold
Derivedfromdiffusionmodels(Sohl-Dicksteinetal.,2015),
intoaspecificstructure. Thesesequencesareverifiedwith
diffusionlanguagemodelsisavarietyofgenerativemodel
predictiontools,althoughtheyhavenotbeenexperimentally
that samples data via an iterative denoising process from
confirmedyet. Chroma(Ingrahametal.,2023)isamodel
noise. Theycanbedividedintocontinuous(Hoetal.,2020;
designedforcreatinglargeproteinsandproteincomplexes,
Songetal.,2020)anddiscrete(Hoogeboometal.,2021b;
consideringvariousconstraintslikedistancesandsymmetry.
Austinetal.,2021)categoriesaccordingtothedistribution
It transforms a collapsed polymer into protein backbone
theymodel. Continuousdiffusionmodelsmakegreatsuc-
and sequence more quickly than older methods, thereby
cessinvision(Dhariwal&Nichol,2021b;Rombachetal.,
allowingfortheefficientgenerationoflargestructures.
2021;Hoetal.,2022), buttheystruggleinlanguagesfor
operatingoncontinuoussurrogatesofdiscretetokens(Li D.5 ProteinInverseFolding
etal.,2022;Gongetal.,2022;Hanetal.,2022;Dieleman The structure-based protein sequence design is typically
etal.,2022;Yuanetal.,2022;Gaoetal.,2022a;Yeetal., formulatedasaconditionalsequencegenerationproblem
2023b;Chenetal.,2023a;Wuetal.,2023),whichhasdiffi- by deep generative modeling, wherein protein 3D struc-
cultybypassingthepitfallofdiscreteness(Yeetal.,2023b) turesareusuallydepictedasak-NNgraph(Ingrahametal.,
and still lags behind autoregressive language models. In 2019). Theproteingraphestablishesedgefeaturesbetween
contrast, discrete diffusion models, albeit having limited adjacentresiduesandencodesresidueinformationasnode
progressinlarge-scaleapplications,areinnatelysuitedto features,modeledbygraphneuralnetworks(GNNs).Graph-
thedatatypeinherenttolanguages(i.e.,sequencesofdis- Trans(Ingrahametal.,2019)andGVP(Jingetal.,2020)
crete tokens). Zheng et al. (2023a) makes commendable utilizesthegraphattentionencoderandautoregressivede-
strides in discrete diffusion models and enhancing these coder for protein design. Recently, ProteinMPNN (Dau-
modelstoyieldcomparableperformancewithautoregres- parasetal.,2022)andPiFold(Gaoetal.,2022b)introduce
sivemodelsontypicallanguagegenerationbenchmarkslike morecomplexproteinfeaturesandexpressiveGNNs,result-
machine translation. Furthermore, as shown by He et al. inginsignificantimprovements. Furthermore,inaddition
20DiffusionLanguageModelsAreVersatileProteinLearners
totheprimarygenerativepurpose,thistaskcanalsobeused
asaproxyforprotein(structure-aware)representationlearn-
ing(Yangetal.,2022b). Acriticalandsignificantchallenge
hereinisthelackofsufficientproteinstructuredata. Tothis
end,ESM-IF(Hsuetal.,2022)alleviatethisissuewitheffec-
tivedataaugmentationbyback-translationwithAlphaFold
2 (Jumper et al., 2021). , resulting in dramatic improve-
ments. Ontheotherhand,Zhengetal.(2023b)demonstrate
howtoefficientlysteeringlargepretrainedproteinLMsinto
astructure-informedsequencegenerativemodelsinamask-
predictgenerativemanner,attainingstate-of-the-artresults
onsingle-chainandcomplexproteinbenchmark. Mostre-
cently,graphdiffusionmodelshavealsobeenstudiedfor
inversefoldingproblem(Yietal.,2023).
21DiffusionLanguageModelsAreVersatileProteinLearners
E VisualizationofUnconditionalSamples
Figure6. Visualizedexamplesfrom50to500inlength.
Figure7. Visualizedexamplesfrom600to1000inlength.
22