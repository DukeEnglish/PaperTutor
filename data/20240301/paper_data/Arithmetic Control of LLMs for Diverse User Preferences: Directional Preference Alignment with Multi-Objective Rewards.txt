Arithmetic Control of LLMs for Diverse User Preferences:
Directional Preference Alignment with Multi-Objective Rewards
HaoxiangWang∗1 YongLin∗2 WeiXiong∗1 RuiYang2 ShizheDiao2 ShuangQiu2
HanZhao1 TongZhang1
1UniversityofIllinoisUrbana-Champaign 2TheHongKongUniversityofScienceandTechnology
Abstract
Fine-grainedcontroloverlargelanguagemodels(LLMs)remainsasignificantchallenge,hindering
their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback
(RLHF)showspromiseinaligningLLMs,itsrelianceonscalarrewardsoftenlimitsitsabilityto
capturediverseuserpreferencesinreal-worldapplications. Toaddressthislimitation,weintroduce
the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA
incorporatesmulti-objectiverewardmodelingtorepresentdiversepreferenceprofiles. Additionally,
DPAmodelsuserpreferencesasdirections(i.e.,unitvectors)intherewardspacetoachieveuser-
dependentpreferencecontrol. Ourmethodinvolvestrainingamulti-objectiverewardmodeland
thenfine-tuningtheLLMwithapreference-conditionedvariantofRejectionSamplingFinetuning
(RSF),anRLHFmethodadoptedbyLlama2. Thismethodenjoysabetterperformancetrade-off
acrossvariousrewardobjectives. Incomparisonwiththescalar-rewardRLHF,DPAoffersusers
intuitivecontroloverLLMgeneration: theycanarithmeticallyspecifytheirdesiredtrade-offs(e.g.,
morehelpfulnesswithlessverbosity). WealsovalidatetheeffectivenessofDPAwithreal-world
alignmentexperimentsonMistral-7B.Ourmethodprovidesstraightforwardarithmeticcontrolover
thetrade-offbetweenhelpfulnessandverbositywhilemaintainingcompetitiveperformancewith
strongbaselinessuchasDirectPreferenceOptimization(DPO).Thecodeandtrainedmodelare
releasedatgithub.com/Haoxiang-Wang/directional-preference-alignment.
1. Introduction
Largelanguagemodels(LLMs)(OpenAI,2023;Anthropic,2023)havedemonstratedremarkable
capabilitiesacrossvariousdomainsandtasks,suchasmathematicalreasoning(Weietal.,2022)and
medicalquestionanswering(Singhaletal.,2023a;Wangetal.,2023a;Thirunavukarasuetal.,2023).
However, for an assistant to be truly useful, it must align with human preferences, such as being
helpful,honest,harmless,andmanagingverbosity.
ReinforcementLearningfromHumanFeedback(RLHF)(Christianoetal.,2017;Ziegleretal.,
2019;Ouyangetal.,2022;Baietal.,2022b;Leeetal.,2023),istheleadingapproachtoadaptLLMs
towardsthesecomplex,oftenimplicitly-definedgoals. Typically,themostpopularRLHFframework
(Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022) first constructs a scalar reward
modeltorepresentthedifficult-to-specifygoalofbeingpreferredbyhumanandthenusethisreward
model to provide signals for the subsequent reward optimization stage. Its success spans various
practicalapplications,includingrecommendationsystems(Pereiraetal.,2019),imagegeneration
(Hao et al., 2022; Wu et al., 2023a; Dong et al., 2023a), robotics (Brown et al., 2019), and most
∗Equalcontribution.Correspondanceto:HaoxiangWang(hwang264@illinois.edu)
1
4202
beF
92
]GL.sc[
2v17581.2042:viXraRLHF (DPO) v.s. Ours
Input Input
System Prompt: You are a helpful assistant.
System Prompt: You are a helpful assistant.
Your response should maximize weighted
User: Please summarize “Romeo and Juliet” rating = helpfulness*0.8 - verbosity*0.6
User: Please summarize “Romeo and Juliet”
Generation Generation
869 characters 140 characters
Assistant: Romeo and Juliet is a classic Assistant: "Romeo and Juliet" is a play about
tragedy written by William Shakespeare. The two young lovers who come from feuding
story revolves around two young and families. They meet, fall in love, and ultimately
passionate lovers, Romeo Montague and Juliet meet a tragic end.
Capulet, who come from feuding families in
Verona, Italy during the Renaissance era.
Against the wishes of their families, Romeo
and Juliet secretly marry and consume
themselves with an all-consuming love for
each other… (495 more characters)
Figure1: ArithmeticPromptingforPreference-ConditionalGeneralization: Comparisonbetween
conventionalRLHFmethodssuchasDPOandourDirectionalPreferenceAlignment(DPA).Inthe
case of DPO (left), it is capable of generating helpful responses, but these tend to be excessively
verbose. Conversely,withourDPA(right),itallowsforarithmeticcontrolofLLMstomeetvarious
user preferences. For instance, setting the directional preference (unit vector) to v = ⟨0.8,−0.6⟩
leadstolessverboseresponsesfromouralignedLLM.
notably, aligning LLMs with human values and preferences, such as ChatGPT (OpenAI, 2023),
Claude(Anthropic,2023),Llama2(Touvronetal.,2023)andGemini(Teametal.,2023).
WhilerecentadvancementsinRLHFarenoteworthy,afundamentalchallengepersistsdueto
problem misspecification. This means that a single reward function may not sufficiently capture
complexhumanvalues. Forexample,agenerativemodelalignedbyRLHFforhelpfulnesstendsto
produceverboseresponsesasshowninFigure1(Left)Singhaletal.(2023b), eventhoughmany
userspreferanswersthatarebothhelpfulandconcise. Assumingae-objectiverewardimpliesatotal
orderoverpreferences,whichishardtosatisfywhenthepreferenceisaggregatedacrossadiverse
setofhumangroups(May,1954;Tversky,1969),becausehumanstypicallyhaveasetofintricateor
evencontradictorytargets(BiyikandSadigh,2018). Inreal-worldapplications,thescalar-reward
RLHF tends to align the LLMs toward an “average-user” preference, which cannot capture the
complicatednatureofhumanpreferencesandcanbeunfairfortheunder-representedgroups(Feffer
etal.,2023). Forexample,considerUser-1,2,3,andresponsesA,B,C inFig.2(Left). User-1and
3preferresponseB overC (B ≺ C),whileUser-2prefersC overB (C ≺ B). Thiscouldoccur
asresponseC ismoreverbosethanB,whileUser-2prefersconciseanswers. Whenthesediverse
preferencesareaggregatedacrosshumangroups,thetypicalrewardmodelswithscalarrewardstend
to learn the “average-user” preference (which is B ≺ C in this case), overlooking the individual
preferenceofUser-2,asshowninFigure2(Middle). Thisisalsoknownasthe“Condorcetparadox”
inthetheoryofsocialchoiceGehrlein(2002). Ingeneral,humanopinionsandexpertisecanvary
significantly(Coello,2000;Bobuetal.,2023;Bansaletal.,2023). Meanwhile,theimportanceof
thesetargetsmayalsochangeovertime,dependingontheusersandtheirexpectations.
2Traditional RLHF Ours
Labelling Scalar Reward Modelling Directional Preferences
Multi-Objective Reward Space
Prompt Explain the moon landing User-1 A ≺ B ≺ C
(e.g., helpfulness & verbosity)
Responses User-2 A ≺ C ≺ B
A B C
Helpfulness
User-3 A ≺ B ≺ C User-1
Preferences C
of Labelers C
Conflicts User-2 ≺ B
B
User-1 A ≺ B ≺ C Reward Reward Model ≺
User-3
User-2 A ≺ C ≺ B Modelling A ≺ B ≺ C A A
B C
User-3 A ≺ B ≺ C RL ≺C ≺A A ≺ B ≺
Policy Model Verbosity
Figure2: (Left)Theillustrationdepictspreferenceconflictsamongdifferentusers,whereUser-1and
User-3favorresponseBoverresponseC,whileUser-2prefersCoverB.(Middle)Generally,the
scalar-rewardRLHFframeworktendstoaligntowardtheaverage-userpreference,thusfavoringB
overC,whichoverlooksthepreferenceofUser-2. (Right)OurDirectionalPreferenceAlignment
(DPA)enablesuserstospecifytheirpreferencevectorinamulti-dimensionalspace,allowingeach
user’spreferencetobewellrepresentedwithinthiscontext.
Toaddressthelimitationsoftheexistingscalarrewardmodel,previousworkssuggesttheuseof
multi-objectiverewardsthatcharacterizehumanpreferencesfromdifferentaspects(e.g.,helpfulness,
verbosity, harmlessness) (Pan et al., 2023; Rame et al., 2023). One common way is to take the
human feedback as a multi-dimensional reward vector and each dimension models one objective
(Rame et al., 2023; Dong et al., 2023b). Then, one may apply a linear combination to transform
themulti-objectiverewardsintoascalarforLLMalignment(Bakkeretal.,2022;Wuetal.,2023b).
However,thisapproachstillcannothandletheuser-dependentneedsfromadiverseuserpopulation
andcanbeunfairforminoritygroups. Onemayfurtheradoptauser-dependentlinearcombinationto
multi-objectiverewardsforaligningamodelforeachuserpreference(Rameetal.,2023;Jangetal.,
2023). However, this approach is quite inference-unfriendly because we have to switch between
different models in response to the different user preferences. Finally, in social choice theory, a
game-basedformulationwasstudiedunderthenamemaximallotteries(Sternberg,1965;Fishburn,
1984),aswellasthesubsequentworksinRLHF(Wangetal.,2023b;Swamyetal.,2024;Yeetal.,
2024),tohandlethediversityofuserpreferences. Weremarkthattheirframeworkisfundamentally
differentfromthemulti-objectiverewardsandcannotofferauser-dependentpreferencecontrolin
theinferencestage,either. RefertoSection2.3foramoredetaileddiscussionwithexistingmethods.
Inrecognitionoftheaforementionedlimitations, weproposeanovelandpracticalalignment
approach,DirectionalPreferenceAlignment(DPA),toenhancetheadaptabilityandcontrollability
ofasingleLLM.OuralignedLLMenjoystheflexibilitytobecontrolledwithdifferentpreferences
embeddednumericallyintothesystemprompt. Theabilitytocontrolpreferencescansignificantly
enhancethemodel’spersonalizationabilityduringinference. Forexample,asthemodelisaligned
with DPA with helpfulness and verbosity in consideration, a user could simply control
themodel’sgenerationbyspecifyingadirectionalpreferencev = ⟨v ,v ⟩that∥v∥ = 1,andthe
1 2 2
modelwillgenerateresponsesthatmaximizereward = v ×helpfulness+v ×verbosity
1 2
wherehelpfulnessandverbosityarerewardsscoredfromdifferentperspectivesasshownin
3Reward Model Training Iterative Rejection-Sampling Finetuning
Prompt Summarize “‘Romeo and Julie” Prompt Summarize “‘Romeo and Julie” Best-of-n
Sampling
Response A B C R Sae msp po len dse w ith D E F Select the highest reward
Policy Model
Helpful 41 57 69 E
Helpful 56 40 67
Rating Rated with
Verbose 52 45 60 Multi-Objective Verbose 43 51 62
Reward Model
… … … …
… … … … Finetuning
Regression Linear Scalarization
v 1=0.8,v 2=0.6
Training with Sampled v
Multi-Objective R=v 1⋅helpfulness W Re ei wgh at re dd 8.2 15 18 P Mo oli dc ey l
Reward Model +v 2⋅verbosity
Figure3: IllustrationoftheDirectionalPreferenceAlignmentprocedure
Figure1(Right). Figure2(Right)furthershowsthatthepreferencesofUser-1,User-2,andUser-3
canbeaccuratelyrepresentedbyspecifyingthepreferencevectorinthe2-dimensionalspace. Thisis
ascenariowhereDPAcanalleviatetheproblemofmisspecificationinRLHF.
Ourapproachfeaturestwocrucialaspects: 1). Multi-ObjectiveRewards,whichinvolvelearning
withmultipledifferentpreferencetargetssimultaneously,and2). DirectionalPreferenceAlignment,
whichencodesuserpreferencesasunitvectorsforpreference-awareLLMalignment. Specifically,
wesummarizeourcontributionsasfollows.
• We identify the limitations of existing popular RLHF frameworks: 1) the limited capacity
for capturing the real-world complicated human preference; 2) lacking in adaptability for user-
dependentpreference;
• WeproposeDirectionalPreferenceAlignment(DPA):anovelalignmentapproachthatallowsa
singleLLMtoaccommodateuserswithvaryingpreferences.
• Weconsiderbothhelpfulnessandverbosityrewards,andalignMistral-7B(Jiangetal.,2023)
withourDPA:empiricalevaluationsshowthatDPAofferseffectivearithmeticcontroloverthe
trade-offbetweenhelpfulnessandverbosity,whilemaintainingcompetitiveperformancewithDPO
(Rafailovetal.,2023).
2. DirectionalPreferenceAlignment
InatypicalRLHFpipeline(Ouyangetal.,2022;Baietal.,2022a;Touvronetal.,2023),wefirst
constructarewardmodelbasedonalabeledpreferencedataset(e.g.,preferenceA ≺ B ≺ C anno-
tatedbyalabeler)andthenusetherewardmodeltoprovidesupervisionforthesubsequentreward
optimizationstage. Inthissection,wefirstpresenttheproblemsetup,whereweadditionallyconsider
multi-objectiverewardsanduserpreferencesintheframework. Then,wepresentouralgorithm,the
DirectionalPreferenceAlignment,tohandletheproblemofpreference-awarealignment.
Notation. WedenotethepromptspaceandtheresponsespaceasX andY,respectively. Sk = {v ∈
Rk : ∥v∥ = 1}istheunitsphereunderthe∥·∥ norm. Weuseπ todenotethepolicy(generative)
2 2 θ
LLMwhoseparameterisθ.
42.1 Multi-ObjectiveRewardModel
Weconsiderk-objectiverewardforaresponsey givenpromptxas
r(x,y) = ⟨r (x,y),...,r (x,y)⟩ ∈ Rk
1 k
whereeachr (x,y)istheratingforasingleattributesuchashelpfulness,correctness,andverbosity.
i
Weuser todenoter(x,y)forshortwhenitisclearfromthecontext. LetD denotethedistribution
r
of(x,y,r)(Wangetal.,2023c;Ko¨pfetal.,2023). Wethentrainamulti-objectiverewardmodelr˜
withregressionloss(Dongetal.,2023b):
minE ∥r˜(x,y)−r(x,y)∥2. (1)
r˜
(x,y,r)∼Dr 2
Thetrainedrewardmodelr˜canrateanyprompt-responsepair(x,y)acrossk attributes.
2.2 DirectionalPreferenceAlignment
OurworkaimstolearnacollectionofpoliciesthatcantraversetheParetofrontasefficientlyaspos-
sible. Moreover,weintendtorelatethelearnedpoliciestotheuser’spreferencesconcerningvarious
objectivesandcontrolthelearningprocessaccordingtosuchpreferences. Tomakemulti-objective
optimizationtractableandcontrollable,acommonapproachislinearscalarization(Caruana,1997;
Ghane-KanafiandKhorram,2015;Huetal.,2023),whichtakesalinearcombinationofmultiple
objectives. Throughexploringalldifferentlinearcombinations,thesolutionstotheseproblemscan
sufficientlycoverasignificantareaoftheParetofront,whichjustifiestheapplicationofthelinear
scalarizationapproach.
Directional Preference. To achieve a fine-grained representation of the preference signal, we
model user preference as a direction in the multi-objective reward space, that is, a unit vector
v = ⟨v ,...,v ⟩ ∈ Sk. Then,thepreference-conditionedrewardis
1 k
k
(cid:88)
R(x,v,y) = v⊤r(x,y) = v r (x,y). (2)
i i
i=1
To incorporate user preference into the language model, we condition the text generation on v in
additiontox,suchthattheresponseisgeneratedaccordingtoy ∼ π (·|x,v). Foraspecificv,the
θ
preference-conditionalrewardobjectiveis
J(v,π ) = E [R(x,v,y)] (3)
θ x∼Dx,y∼π θ(·|x,v)
WemodelthedirectionalpreferencesofourtargeteduserpopulationasP ,aprobabilitydistribution
v
overSn. Finally,weoptimizeθ bymaximizingtheexpectedrewardwithrespecttoP :
v
maxE [J(v,π )]. (4)
v∼Pv θ
θ
Reward Optimization via Rejection Sampling. We now proceed to discuss the algorithmic
designsforoptimizingtheRLobjectiveinEq.(4). WhilePPOisthemostpredominantapproach
forafixedrewardfunction(OpenAI,2023;Anthropic,2023),itisknownthatPPOisunstableand
sample-inefficientinaligningLLMs(Choshenetal.,2019)andimposesaheavyburdenonGPU
memoryresources(Ouyangetal.,2022;Yuanetal.,2023). Hence,PPOrequiresextensiveefforts
to be tuned to its best performance. In light of the above limitations, we resort to an alternative
approach,RejectionSamplingFine-tuning(RSF)(Dongetal.,2023a;Yuanetal.,2023;Gulcehre
etal.,2023),aRLHFalgorithmusedintheLlama2project(Touvronetal.,2023),withappealing
5ALIGNMENTMETHODS MULTI-OBJECTIVEREWARDS PREFERENCEARITHMETIC SINGLEMODEL FEASIBILITYGUARANTEE
PPO(SCHULMANETAL.,2017) ✗ ✗ ✓ ✓
DPO(RAFAILOVETAL.,2023) ✗ ✗ ✓ ✓
REWARDSOUP(RAMEETAL.,2023) ✓ ✓ ✗ ✓
STEERLM(DONGETAL.,2023B) ✓ ✓ ✓ ✗
OURS ✓ ✓ ✓ ✓
Table1: ComparisonamongdifferentRLHFalgorithms. Multi-objectiverewards: ifthealgorithm
considers multiple reward objectives. Preference arithmetic: if the model allows for arithmetic
controlofthepreference. Singlemodel: ifthealgorithmcanhandledifferentpreferenceswitha
singleLLM.FeasibilityGuarantee: Whetherthemodelisfreefromthefeasibilityissuethatthe
specifiedcontrolvector(prompt)couldbeunreachable(refertoSection2.3fordetails).
simplicity, stability, and comparable reward gains. In essence, the original RSF learns from the
best-of-npolicycreatedbytherewardfunction. Initially,wegeneratenresponsesusingabaseLLM
and then rank them using the reward model to select the responses with the highest reward. We
furtherfinetuneourLLMbasedontheseselectedsamples,andthisprocesscanberepeatedmultiple
times.
Inourscenario,toaddressthemulti-objectivenatureanduser-dependentpreferences,weitera-
tivelyalternateamongthefollowingstepsfort = 1,...,T iterations:
0. Preparation. InitializeanemptydatasetD = ∅. Preparepolicymodelπ obtainedfromlast
t θt−1
iteration.
1. RejectionSampling. Foreachrandomlysampledpromptxanddirectionalpreferencev,generate
nresponses{y ,...y }byπ (·|x,v)andcomputetheirmulti-objectiverewardsbyr˜(x,y).
1 n θt−1
Obtain the linear scalarization of r˜(x,y) by R(x,v,y ) = vTr˜(x,y ). Then, rank y ,...,y
i i 1 n
accordingtoR(x,v,y )andselectthehighest-rankresponsey⋆. Add(x,v,y⋆)toD .
i t
2. Finetuning. TrainonD :
t
θ ← argmaxE [π (y|x,v)].
t (x,v,y)∼Dt θ
θ
ThewholeprocedureofourmethodsissummarizedinFigure3.
2.3 DiscussionwithExistingMethods
Comparison with SteerLM Dong et al. (2023b). Recall that we have multi-objective reward
r = ⟨r ,r ,...,r ⟩ of each response y to the prompt x. Dong et al. (2023b) first fine-tunes the
1 2 k
generativemodeltomaximizethelikelihoodofy bytakingbothxandr astheinputprompts:
maxE logP (y|x,r).
θ
(x,y,r)∼Dr θ
When presented with a new input x¯, SteerLM aims to produce a response that aligns with the
newly assigned multi-dimensional r¯. Particularly, a user could specify r¯as “(helpfulness =
10,verbosity = 1)”, namely high helpfulness but low verbosity, for a new prompt x¯ =
“Pleasesummarize‘RomeoandJuliet’”. SteerLMcouldthengenerateanswersaccordingtor¯. How-
ever,SteerLMwillencounterasignificantchallengewhenauser-specifiedr¯fallsoutsidethefeasible
regionofrewardsforthegivenx¯,i.e.,r¯∈/ {r : (x¯,y,r) ∈ D }. Inthiscase,ifausersetsar¯that
r
is not achievable given x¯, SteerLM may generate uncontrolled responses due to the infeasibility
ofr¯underx¯. Forexample,“(helpfulness = 10,verbosity = 1)”couldbeinfeasibleforx¯
6accordingtothesetS sinceitwillbedifficultorimpossibletogenerateahelpfulsummarizationof
‘RomeoandJuliet’inveryfewwords.
ComparisonwithSoupMethodsRameetal.(2023);Jangetal.(2023). Soupmethodstrainsa
policyθ foreachrewardobjective. Letr (x,y)denotethei-thobjective,wehave:
i i
θ = argmaxE E r (x,y)
i x∼Dx y∼π θ(·|x) i
θ
Duringinference,whenauserspecifiesthecombinationvector⟨v ,v ,...,v ⟩ ∈ Sk,rewardsoups
1 2 k
(cid:80)
firstcombinetheweightofkmodelsastheirinterpolation v θ andthenquerytheinterpolationfor
i i i
response. Comparedwithourmethod,rewardedsoupcancausesignificantstorageandcomputation
overheadbecausetheyneedtomaintaink LLMsandcalculatedifferentinterpolationswhenevera
newcombinationvectorisassigned.
3. EmpiricalResults
We conduct experiments on Mistral-7B (Jiang et al., 2023), focusing on two reward objectives:
helpfulness and verbosity. Our proposed DPA achieves arithmetic control of LLM gen-
erationsfordifferenthelpfulness-verbositypreferenceswhiledemonstratinganexcellentbalance
betweenthetwoobjectives.
Verbosity Bias. Recently, the verbosity bias in LLMs and humans, meaning that LLMs and
humans sometimes prefer more verbose answers even though they are of similar qualities, has
attractedconsiderableattention(Saitoetal.,2023;Singhaletal.,2023b). Ithasbeenexploitedor
even “hacked” by the RLHF-aligned models. For instance, Kabir et al. (2023) demonstrated that
77%ofChatGPTanswersareverbose,whileYuanetal.(2024)foundthattheaverageoutputlength
increasesto2.5timesastheDPOiterates. Preliminaryexperimentshavebeenconductedinresponse
tothisbias,suchasthosebyChenetal.(2024),whichexplicitlyconsiderverbosityasaresponse
feature. BenchmarkcreatorslikeAlpacaEval(Lietal.,2023)andMT-Bench(Zhengetal.,2023)
haveobservedverbositybiasintheirLLMjudges(typicallyGPT-4),andAlpacaEval-2.0hasadjusted
toaccountforoutputlength1.
3.1 Implementation
Datasets. Weusetwodatasetsforexperiments: HelpSteerandUltraFeedback. Bothdatasetsare
usedforrewardmodeltraining2,whileonlyUltraFeedbackisusedforfinetuning.
• HelpSteerWangetal.(2023d)comprises10Kpromptsand37Kannotatedresponseswithfive
attributes: helpfulness, correctness, coherence, complexity, and verbosity.
A43Bclosed-sourceLLMgeneratedresponses,andhumanlabelersannotatedeachresponseona
scaleof0-4forthefiveattributes.
• UltraFeedback Cui et al. (2023) includes 64K prompts, each of them are associated with
4 responses of five attributes: honesty, truthfulness, instruction-following,
helpfulness and overall-score. GPT-4 was employed to label these responses. We
usethesametraining-validationpromptsplit3 asZephyr(Tunstalletal.,2023).
1tatsu-lab.github.io/alpaca_eval/
2WeincludeHelpSteersinceithasverbosityannotations.
3hf.co/datasets/HuggingFaceH4/ultrafeedback_binarized
7Reward Modeling. We train a multi-objective reward model on the union of HelpSteer and
UltraFeedback,initializingwithMistral-7B.Specifically,wefollowSteerLM-v2practices4 (Wang
et al., 2023c), attaching a linear regression head layer on the last hidden state of Mistral-7B. We
includebothregressionandtraditionallanguagemodelinglossesintherewardmodeltraining,aswe
findthelatterimprovesaccuracywithoutadditionalobservedcosts. Therewardmodelhas10output
dimensions: thefirsthalfcorrespondstoHelpSteer’sfiveattributes,whiletheotherhalfaccountsfor
UltraFeedback’sattributes. Rewardsineachdimensionarerescaledtotherangeof0-100inthedata
preprocessingstage.
Alignment Setup. For a fair comparison with DPO (Rafailov et al., 2023), we conduct a head-
to-head comparison with Zephyr-β (Tunstall et al., 2023), a DPO-trained Mistral-7B model that
wasstate-of-the-art(7B)atitsrelease. Zephyr-β usessupervisedfine-tuning(SFT)onUltraChat-
200K (Ding et al., 2023) followed by DPO on UltraFeedback (Cui et al., 2023). Since RLHF
typically begins with SFT models, we initialize with the SFT checkpoint of Zephyr-β and apply
DPAonUltraFeedback. FollowingpracticesofCuietal.(2023);Tunstalletal.(2023),weaverage
instruction-following, truthfulness, honesty, andhelpfulnessratingsof Ul-
traFeedbackfortheoverallhelpfulnessobjective. WeuseHelpSteer’sverbosityattributeforthe
verbosityobjective. Ourmulti-objectiverewardmodelannotateshelpfulnessandverbosityforall
UltraFeedbackdataandself-generatedresponses.
Rewards and Directional Preferences. We denote the reward objectives for helpfulness and
verbosityasr andr ,respectively. AsnotedbySinghaletal.(2023b),r andr correlatepositively.
1 2 1 2
Therefore,aligninganLLMtomaximizer (helpfulness)willalsotendtoincreaser (verbosity),a
1 2
trenddocumentedinrecentworks(Yuanetal.,2024;Chenetal.,2024). Consequently,whenusing
thepreference-conditionalrewardv⊤r = v r +v r ,wearguethatitisunnecessarytohavev > 0
1 1 2 2 2
(i.e.,toexplicitlyencourageverbosity). Instead,weproposesamplingthedistributionof⟨v ,v ⟩as
√ √ 1 2
arctan(v2) ∼ Uniform(−π,0)withv ∈ [ 2/2,1]andv ∈ [− 2/2,0]. Intuitively,thisletsthe
v1 4 1 2
userpreferencedirection⟨v ,v ⟩beuniformlysampledbetween⟨1,0⟩(purefocusonhelpfulness)
√ √ 1 2
(cid:10) (cid:11)
and 2/2,− 2/2 (abalancefavoringlessverbosity)ontheunitcircle.
DatasetSplitting. IterativeRLHFmethodstypicallysampleresponsesforunseenpromptsineach
newiterationtopreventthemodelfromsimplymemorizingandrepeatingtheresponses(Dongetal.,
2023a;Xiongetal.,2023;Yuanetal.,2024). Inviewofthis,wesplitUltraFeedbackdatasetinto
twodisjointsubsets,D andD ,containinganequalnumberofuniqueprompts. Ineachiterationt,
1 2
weinitializethepolicymodelπ fromanSFTcheckpointratherthanπ ,andweuseadifferent
θt θt−1
subsetfromthelastiteration. Theuseofalternativesubsetsensuresthatthepolicymodelπ for
θt
responsesamplinginiterationt+1hasnotencounteredthepromptsbefore.
RejectionSampling. Weconductrejectionsamplingfollowingouriterativealgorithmdetailedin
Sec. 2.2. Noticethattolaunchtrainingint = 1,weneedπ forsamplingresponsesforadiverse
θt=0
setofhelpfulness-verbositypreferences. However,Zephyr-β-SFTisnotdesignedforpreference-
conditionalgeneration,makingitnotagoodchoiceforπ . Toresolvethis,wetrainaSteerLM
θt=0
modelonD (ahalfofUltraFeedback)thatcangenerateresponsesconditionedonbothuserpromptx
2
(sampledfromD )andrewardobjectivesr ,r . Weusethismodelforrejectionsamplinginiteration
1 1 2
t = 1 to obtain π (for each prompt, we generate 80 responses for diverse reward combinations
θ1
4TheauthorsofSteerLM(Dongetal.,2023b)improvedtheoriginaltrainingrecipeinafollow-upwork(Wangetal.,
2023c),whichwedenoteasSteerLM-v2.
8Rewards on Validation Set
Ours [t=0] Ours [t=4]
80.0 Ours [t=1] SteerLM
Ours [t=2] SFT [Zephyr- ]
77.5 Ours [t=3] DPO [Zephyr- ]
75.0
72.5
70.0
67.5
65.0
62.5
40 45 50 55
Verbosity Reward
Figure4: Thevalidationrewardofdifferentmethods. Whent ≥ 1,ourDPAmodelPareto-dominates
SFT,DPO,andSteerLM.Further,DPAatiterationtPareto-dominatesmodelsatpreviousiterationt′
witht′ < t.
(r ,r )). Inallthefollowingiterations,foreachprompt,wesample5directionalpreferences⟨v ,v ⟩,
1 2 1 2
anduseπ togenerate16responsesperpreference,thenkeepthehighest-rewardresponseand
θt−1
rejecttherest15.
Fine-tuning. Fortheresponsedataobtainedthroughrejectionsampling, weprependtheuser’s
directionalpreferencetothesystemprompt,asillustratedinFig.1,tomakethemodelawareofthe
user preference. The fine-tuning process then follows the same approach as SFT, optimizing the
next-tokenpredictionlossacrossthetextcorpus. ItisalsoworthnotingthatRLHFoftenleadsto
performance degradation or knowledge forgetting, a phenomenon referred to as alignment tax in
the literature (Askell et al., 2021; Lin et al., 2023). To mitigate this issue, we adopt the memory
replaytechniquessuggestedinInstruct-GPT(Ouyangetal.,2022)andLlama2(Touvronetal.,2023)
that can effectively reduce alignment tax (Lin et al., 2023). Specifically, we incorporate original
responsesfromUltraFeedback,whichconstituteabout15%ofourfinetuningdataforeachiteration.
Ouralgorithmisappliedforiterationst = 1,...,4.
Software,HardwareandHyperparameters WeusePyTorch(Paszkeetal.,2019)withHugging-
Face’sTRLframework(vonWerraetal.,2020)forallfine-tuningexperimentsacrosst = 0,...,T.
Allexperimentsareconductedon8xA6000GPUs. ThetrainingcostofeachDPAiterationisabout
60GPUhours. TheAdamWoptimizer(LoshchilovandHutter,2019)isemployedwithalearning
rateof10−5 andacosinelearningrateschedule(20warmupsteps). Weuseacontextwindowof
4096tokenswithsample-packing(packingshortresponseswithinthecontextwindow). Thetraining
takes2epochswithaglobalbatchsizeof64. WeusevLLM(Kwonetal.,2023)forinference. Inthe
rejectionsamplingprocess,weconductinferencewithtemperature1.0. Inevaluation(Sec. 3.2),we
usetemperature0.7.
9
draweR
ssenlufpleHAlpacaEval-2.0
12 Model
DPO [Zephyr- ]
SFT [Zephyr- ]
11
SteerLM
Ours [t=4]
10
9
8
7
6
5
400 600 800 1000 1200 1400 1600 1800
Average Response Length
Figure5: AlpacaEval-2.0evaluationresults.
3.2 Evaluation
RewardsonValidationSet Forvalidation,weused2000promptsfromUltraFeedbackandconsid-
√ √
(cid:10) (cid:11)
ered10uniformlysampleddirectionalpreferencesrangingfromv = ⟨1,0⟩tov = 2/2, 2/2 .
Foreachprompt-preferencecombination,ourDPA-alignedmodelsgeneratedtworesponses. We
then calculated the average helpfulness and verbosity rewards for all 2000 responses per prefer-
enceusingourrewardmodel. ForSteerLM5,fiveverbosityrewardvaluesweresampled, andthe
highestcorrespondinghelpfulnessrewardfromUltraFeedbackwasidentifiedforeachvalue. These
verbosity-helpfulness pairs were then used to condition SteerLM’s generation, with the average
rewardscomputedacrossprompts. InthecaseofZephyr-β’sDPOandSFTmodels,wegenerated
responses using their original prompt templates and averaged the rewards across the validation
set. The results, illustrated in Fig. 4, show that as t ≥ 1, our DPA model Pareto-dominates SFT,
DPO, SteerLM, and DPA at iteration t Pareto-dominates the models of previous iterations. This
demonstratesDPA’seffectivearithmeticcontrolfordifferentuserpreferences,andwithincreasing
finetuningiterationst,theempiricalfrontofDPA(i.e.,eachcurveinFig.4)expands,indicatingthat
ourfinetuningapproachsuccessfullymaximizesrewardsforalluserpreferencesofconsideration.
Notably,ourDPA’sempiricalfrontsignificantlysurpassesthatofSteerLMandDPO,eventhoughall
modelsweretrainedonthesameUltraFeedbackdatasetandoriginatedfromthesameSFTmodel.
AlpacaEval-2.0 Evaluation AlpacaEval-2.0 (Li et al., 2023) is an LLM-based automatic eval-
uation benchmark that employs GPT-4-turbo as the LLM judge. It includes 805 prompts, and
modelresponsestothesepromptsarecomparedwithreferenceanswersprovidedbyGPT-4-turbo.
Subsequently,thewin-rateagainstthereferenceanswersiscalculatedasametricforthemodels’
5WetrainedaSteerLMmodel(initializedwiththeSFTcheckpointofZephyr-β)onUltraFeedback,followingpractices
ofWangetal.(2023c).
10
obruT
4-TPG
tsniaga
)%(
etaR
niWinstruction-following capabilities. We evaluated SteerLM and our DPA (at t = 4) conditioned
withvarioususerpreferencesandreportthewinrateandaverageresponselengthinFig.5,along
with DPO and SFT results for reference. Fig. 5 demonstrates that our DPA model outperforms
SteerLM and achieves competitive performance against DPO while providing arithmetic control
for diverse user preferences. The discrepancy between the validation reward evaluation results
andtheAlpacaEval-2.0outcomesmayarisebecauseourrewardmodelhasdifferentbehaviorsand
preferencescomparedtoGPT-4-turbo. WhileDPAcancloselyfittherewardmodel,thisdoesnot
necessarilyguaranteegeneralizationtoGPT-4-turboevaluations.
4. RelatedWorks
Large Language Models. The landscape of natural language processing has been profoundly
transformedinrecentyearsthroughthedevelopmentoflargelanguagemodels(LLMs),showcas-
ing human-level proficiency across a range of tasks including text classification, generation, and
complex reasoning. This progress stems from extensive pre-training on vast datasets, enabling
thesemodelstoaddressdiversechallenges. Despitetheirachievements,adistinctionarisesbetween
closed-sourcemodels(e.g.,GPT-3(Brownetal.,2020),Bard(Google,2023),Claude(Anthropic,
2023),andPaLM(Chowdheryetal.,2023)),oftensurpassingtheiropen-sourcecounterparts(e.g.,
megatron-turing-530b(Smithetal.,2022),andBloom(Workshopetal.,2022))inperformance(Liang
et al., 2022), which poses challenges for open-source research. However, initiatives like Meta’s
LLaMA(Touvronetal.,2023)andsubsequentworkssuchasAlpaca(Taorietal.,2023),Vicuna(Chi-
angetal.,2023),andLMFlow(Diaoetal.,2023),demonstratesignificantopen-sourcecontributions
thatcontinuetopushtheboundariesofwhat’spossiblewithLLMs. Theseadvancementsenabled
bythefine-tuningtechniques,aimtoimproveLLMs’abilityandadapttoawiderangeofdomains
and tasks. Nonetheless, as these generative foundation models advance, they still face problems
likeimplicitbiases,underscoringtheneedforongoingalignmentandethicalconsiderationsintheir
developmentandapplication. Inthispaper,wefocusonhowtoalignLLMswithhumanpreferences,
includingtheprinciplesofbeinghelpful,honest,andharmlessasoutlinedby(Askelletal.,2021).
ThisprocedureisoftenachievedbyReinforcementLearningwithHumanFeedback(RLHF)Ouyang
etal.(2022).
RLHF Algorithmic Designs. Policy Optimization (PPO) (Schulman et al., 2017) is the most
predominant approach, with its tremendous success in Chat-GPT (OpenAI, 2023) and Claude
(Anthropic,2023). However,PPOissignificantlylessefficientandstablecomparedtosupervised
finetuning(Choshenetal.,2019),andisalsosensitivetotheparameterandcode-levelimplementation
(Engstrometal.,2020). Therefore,tuningthePPOtoitsbestperformanceisverychallengingin
practiceandtheresultsofChat-GPT(OpenAI,2023)havenotbeenwidelyreproducedsofar. In
viewofthis,effortshavebeenmadetodevelopsupervised-learning-basedmethodsasanalternative
approach to the PPO, and we review them as follows. Rejection sampling finetuning (RSF) is
proposedin(Dongetal.,2023a;Yuanetal.,2023;Gulcehreetal.,2023)withdifferentvariants,but
essentially,theylearnfromthepositivesamplesselectedbyalearnedrewardmodel. RSFwasapplied
totheRLHFofLLaMA2project(Touvronetal.,2023)andweadopttheiterativeimplementation
as suggested in Dong et al. (2023a); Touvron et al. (2023); Gulcehre et al. (2023). There is also
another line of work designing algorithms from the KL-constraint reward optimization (Rafailov
etal.,2023;Zhaoetal.,2023;Azaretal.,2023;Xiongetal.,2023),whichadditionallyrequiresthe
resultingmodeltobeclosetotheinitialmodel. Amongthem,theDirectPreferenceOptimization
11(DPO)(Rafailovetal.,2023)hasattractedconsiderableattentionduetoitssimplicityandstability,
andeffectiveness. Weremarkthatitisalsopossibletoincorporatethesealgorithmicideasintoour
DPAframeworkandweleavethealgorithmicdesignbeyondRSFtofuturework.
Fine-grainedPreferenceRepresentationandAlgorithmicdesign. Thescalar-reward-modelhas
beencriticizedmainlyduetoitslimitedcapacity(Wuetal.,2023b;Casperetal.,2023;Munosetal.,
2023)(seethediscussionofpreferenceintransitivityinSection1foranillustrativeexample). Aline
ofworkshasconsideredmulti-objectiverewardstocapturethedifferentaspectsofhumanpreferences
(Zhouetal.,2023;Jangetal.,2023;Touvronetal.,2023;Wuetal.,2023b;Ko¨pfetal.,2023;Rame
et al., 2023). However, the multi-objective rewards are then combined in a fixed way (e.g., Wu
etal.,2023b;Touvronetal.,2023),mainlytorepresentapreferenceaveragedoverdifferenthuman
groups,failingtocapturetheuser-dependentpreference. Byintroducingtheuserpreferenceasaunit
vector(direction)intothedirectionalpreferencealignmentframework,weachieveafine-grained
anduser-dependentrepresentationforthecomplicatedhumanpreference. Notably,insocialchoice
theory(Sternberg,1965;Fishburn,1984),aswellassomeveryrecentstudiesinRLHF(Wangetal.,
2023b;Swamyetal.,2024;Yeetal.,2024),theRLHFisformulatedasagamebetweentwoLLMs
to partially handle the diversity of preferences in the population-level. The learning objective is
accordinglyadjustedtobesolvingtheNashequilibriumofthegame. Incomparison,ourtechniques
arefundamentallydifferentfromtheirsandmayoffercomputationaladvantagessincegame-based
formulationisfarmorecomplicated.
5. Limitations
AprimaryconstraintofourDPAframeworkisitsrelianceonarobustmulti-objectiverewardmodel.
The efficacy of DPA is intrinsically linked to the precision and discriminative capability of this
rewardmodel. Shouldtherewardmodelnotadequatelycapturethesubtletiesofspecificpreferences
orexhibitbiasinitsrewarddistribution,theDPAmightinadvertentlyexacerbatetheseshortcomings
throughout the fine-tuning process. Furthermore, if the reward model fails to recognize harmful
content,itcouldleadthealignedmodeltoproducesuchcontentduringinference.
6. Conclusion
Inthispaper,weintroduceDirectionalPreferenceAlignment(DPA)toincorporatemultidimensional
userpreferences. DPAaddressesthelimitationofconventionalscalarrewardmodelsbyalleviating
conflicting user preferences through a high-dimensional preference vector in a multidimensional
space. We demonstrate that DPA efficiently explores the Pareto front in the multidimensional
rewardspace,revealingamoreeffectivetrade-offbetweenhelpfulnessandverbosityonMistral-7B
comparedtoexistingstrongbaselinessuchasDPO.
12References
Anthropic. Introducing claude. 2023. URL https://www.anthropic.com/index/
introducing-claude.
A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann,
N.DasSarma,etal. Agenerallanguageassistantasalaboratoryforalignment. arXivpreprint
arXiv:2112.00861,2021.
M.G.Azar,M.Rowland,B.Piot,D.Guo,D.Calandriello,M.Valko,andR.Munos. Ageneraltheo-
reticalparadigmtounderstandlearningfromhumanpreferences. arXivpreprintarXiv:2310.12036,
2023.
Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli,
T.Henighan,etal. Trainingahelpfulandharmlessassistantwithreinforcementlearningfrom
humanfeedback. arXivpreprintarXiv:2204.05862,2022a.
Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirho-
seini, C. McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint
arXiv:2212.08073,2022b.
M. Bakker, M. Chadwick, H. Sheahan, M. Tessler, L. Campbell-Gillingham, J. Balaguer,
N. McAleese, A. Glaese, J. Aslanides, M. Botvinick, et al. Fine-tuning language models to
findagreementamonghumanswithdiversepreferences. AdvancesinNeuralInformationProcess-
ingSystems,35:38176–38189,2022.
H.Bansal,J.Dang,andA.Grover. Peeringthroughpreferences: Unravelingfeedbackacquisition
foraligninglargelanguagemodels. arXivpreprintarXiv:2308.15812,2023.
E.BiyikandD.Sadigh. Batchactivepreference-basedlearningofrewardfunctions. InConference
onrobotlearning,pages519–528.PMLR,2018.
A.Bobu,A.Peng,P.Agrawal,J.Shah,andA.D.Dragan. Aligningrobotandhumanrepresentations.
arXivpreprintarXiv:2302.01928,2023.
D.Brown,W.Goo,P.Nagarajan,andS.Niekum. Extrapolatingbeyondsuboptimaldemonstrations
via inverse reinforcement learning from observations. In International conference on machine
learning,pages783–792.PMLR,2019.
T.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal,A.Neelakantan,P.Shyam,
G.Sastry,A.Askell,etal. Languagemodelsarefew-shotlearners. Advancesinneuralinformation
processingsystems,33:1877–1901,2020.
R.Caruana. Multitasklearning. Machinelearning,28:41–75,1997.
S.Casper,X.Davies,C.Shi,T.K.Gilbert,J.Scheurer,J.Rando,R.Freedman,T.Korbak,D.Lindner,
P.Freire,etal. Openproblemsandfundamentallimitationsofreinforcementlearningfromhuman
feedback. arXivpreprintarXiv:2307.15217,2023.
L.Chen,C.Zhu,D.Soselia,J.Chen,T.Zhou,T.Goldstein,H.Huang,M.Shoeybi,andB.Catanzaro.
Odin: Disentangledrewardmitigateshackinginrlhf,2024.
13W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E.
Gonzalez,I.Stoica,andE.P.Xing. Vicuna: Anopen-sourcechatbotimpressinggpt-4with90%*
chatgptquality,March2023. URLhttps://lmsys.org/blog/2023-03-30-vicuna/.
L.Choshen,L.Fox,Z.Aizenbud,andO.Abend. Ontheweaknessesofreinforcementlearningfor
neuralmachinetranslation. arXivpreprintarXiv:1907.01752,2019.
A.Chowdhery,S.Narang,J.Devlin,M.Bosma,G.Mishra,A.Roberts,P.Barham,H.W.Chung,
C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of
MachineLearningResearch,24(240):1–113,2023.
P.F.Christiano,J.Leike,T.Brown,M.Martic,S.Legg,andD.Amodei. Deepreinforcementlearning
fromhumanpreferences. Advancesinneuralinformationprocessingsystems,30,2017.
C. C. Coello. Handling preferences in evolutionary multiobjective optimization: A survey. In
Proceedings of the 2000 congress on evolutionary computation. CEC00 (Cat. No. 00TH8512),
volume1,pages30–37.IEEE,2000.
G. Cui, L. Yuan, N. Ding, G. Yao, W. Zhu, Y. Ni, G. Xie, Z. Liu, and M. Sun. Ultrafeedback:
Boostinglanguagemodelswithhigh-qualityfeedback,2023.
S.Diao,R.Pan,H.Dong,K.S.Shum,J.Zhang,W.Xiong,andT.Zhang. Lmflow: Anextensible
toolkitforfinetuningandinferenceoflargefoundationmodels. arXivpreprintarXiv:2306.12420,
2023.
N.Ding,Y.Chen,B.Xu,Y.Qin,Z.Zheng,S.Hu,Z.Liu,M.Sun,andB.Zhou. Enhancingchatlan-
guagemodelsbyscalinghigh-qualityinstructionalconversations.arXivpreprintarXiv:2305.14233,
2023.
H.Dong,W.Xiong,D.Goyal,Y.Zhang,W.Chow,R.Pan,S.Diao,J.Zhang,K.SHUM,andT.Zhang.
RAFT:Rewardrankedfinetuningforgenerativefoundationmodelalignment. Transactionson
MachineLearningResearch,2023a. ISSN2835-8856. URLhttps://openreview.net/
forum?id=m7p5O7zblY.
Y.Dong,Z.Wang,M.N.Sreedhar,X.Wu,andO.Kuchaiev. Steerlm: Attributeconditionedsftas
an(user-steerable)alternativetorlhf. arXivpreprintarXiv:2310.05344,2023b.
L.Engstrom,A.Ilyas,S.Santurkar,D.Tsipras,F.Janoos,L.Rudolph,andA.Madry.Implementation
mattersindeeppolicygradients: Acasestudyonppoandtrpo. arXivpreprintarXiv:2005.12729,
2020.
M.Feffer,H.Heidari,andZ.C.Lipton. Moralmachineortyrannyofthemajority? arXivpreprint
arXiv:2305.17319,2023.
P. C. Fishburn. Probabilistic social choice based on simple voting comparisons. The Review of
EconomicStudies,51(4):683–692,1984.
W.V.Gehrlein. Condorcet’sparadoxandthelikelihoodofitsoccurrence: differentperspectiveson
balancedpreferences. Theoryanddecision,52:171–199,2002.
14A.Ghane-KanafiandE.Khorram. Anewscalarizationmethodforfindingtheefficientfrontierin
non-convexmulti-objectiveproblems. AppliedMathematicalModelling,39(23-24):7483–7498,
2015.
Google. Bard. 2023. URLhttps://bard.google.com/.
C. Gulcehre, T. L. Paine, S. Srinivasan, K. Konyushkova, L. Weerts, A. Sharma, A. Siddhant,
A.Ahern,M.Wang,C.Gu,etal. Reinforcedself-training(rest)forlanguagemodeling. arXiv
preprintarXiv:2308.08998,2023.
Y. Hao, Z. Chi, L. Dong, and F. Wei. Optimizing prompts for text-to-image generation. arXiv
preprintarXiv:2212.09611,2022.
Y.Hu,R.Xian,Q.Wu,Q.Fan,L.Yin,andH.Zhao. Revisitingscalarizationinmulti-tasklearning:
Atheoreticalperspective. InThirty-seventhConferenceonNeuralInformationProcessingSystems,
2023. URLhttps://openreview.net/forum?id=6EqUpqMnwl.
J.Jang,S.Kim,B.Y.Lin,Y.Wang,J.Hessel,L.Zettlemoyer,H.Hajishirzi,Y.Choi,andP.Am-
manabrolu. Personalized soups: Personalized large language model alignment via post-hoc
parametermerging. arXivpreprintarXiv:2310.11564,2023.
A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand,
G.Lengyel,G.Lample,L.Saulnier,etal. Mistral7b. arXivpreprintarXiv:2310.06825,2023.
S. Kabir, D. N. Udo-Imeh, B. Kou, and T. Zhang. Who answers it better? an in-depth analy-
sis of chatgpt and stack overflow answers to software engineering questions. arXiv preprint
arXiv:2308.02312,2023.
A. Ko¨pf, Y. Kilcher, D. von Ru¨tte, S. Anagnostidis, Z. R. Tam, K. Stevens, A. Barhoum, D. M.
Nguyen, O. Stanley, R. Nagyfi, S. ES, S. Suri, D. A. Glushkov, A. V. Dantuluri, A. Maguire,
C. Schuhmann, H. Nguyen, and A. J. Mattick. Openassistant conversations - democratizing
largelanguagemodelalignment. InThirty-seventhConferenceonNeuralInformationProcessing
SystemsDatasetsandBenchmarksTrack,2023. URLhttps://openreview.net/forum?
id=VSJotgbPHF.
W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Sto-
ica. Efficient memory management for large language model serving with pagedattention. In
ProceedingsoftheACMSIGOPS29thSymposiumonOperatingSystemsPrinciples,2023.
H. Lee, S. Phatale, H. Mansoor, K. Lu, T. Mesnard, C. Bishop, V. Carbune, and A. Rastogi.
Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint
arXiv:2309.00267,2023.
X. Li, T. Zhang, Y. Dubois, R. Taori, I. Gulrajani, C. Guestrin, P. Liang, and T. B. Hashimoto.
Alpacaeval: Anautomaticevaluatorofinstruction-followingmodels. https://github.com/
tatsu-lab/alpaca_eval,2023.
P.Liang,R.Bommasani,T.Lee,D.Tsipras,D.Soylu,M.Yasunaga,Y.Zhang,D.Narayanan,Y.Wu,
A.Kumar,etal. Holisticevaluationoflanguagemodels. arXivpreprintarXiv:2211.09110,2022.
15Y.Lin,L.Tan,H.Lin,Z.Zheng,R.Pi,J.Zhang,S.Diao,H.Wang,H.Zhao,Y.Yao,etal. Speciality
vs generality: An empirical study on catastrophic forgetting in fine-tuning foundation models.
arXivpreprintarXiv:2309.06256,2023.
I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Confer-
enceonLearningRepresentations,2019. URLhttps://openreview.net/forum?id=
Bkg6RiCqY7.
K.O.May. Intransitivity,utility,andtheaggregationofpreferencepatterns. Econometrica: Journal
oftheEconometricSociety,pages1–13,1954.
R. Munos, M. Valko, D. Calandriello, M. G. Azar, M. Rowland, Z. D. Guo, Y. Tang, M. Geist,
T.Mesnard,A.Michi,etal.Nashlearningfromhumanfeedback.arXivpreprintarXiv:2312.00886,
2023.
OpenAI. Gpt-4technicalreport. ArXiv,abs/2303.08774,2023.
L.Ouyang,J.Wu,X.Jiang,D.Almeida,C.Wainwright,P.Mishkin,C.Zhang,S.Agarwal,K.Slama,
A.Ray,etal. Traininglanguagemodelstofollowinstructionswithhumanfeedback. Advancesin
NeuralInformationProcessingSystems,35:27730–27744,2022.
A.Pan,J.S.Chan,A.Zou,N.Li,S.Basart,T.Woodside,H.Zhang,S.Emmons,andD.Hendrycks.
Dotherewardsjustifythemeans? measuringtrade-offsbetweenrewardsandethicalbehaviorinthe
machiavellibenchmark. InInternationalConferenceonMachineLearning,pages26837–26867.
PMLR,2023.
A.Paszke,S.Gross,F.Massa,A.Lerer,J.Bradbury,G.Chanan,T.Killeen,Z.Lin,N.Gimelshein,
L.Antiga,etal. Pytorch: Animperativestyle,high-performancedeeplearninglibrary. Advances
inneuralinformationprocessingsystems,32,2019.
B.L.Pereira,A.Ueda,G.Penha,R.L.Santos,andN.Ziviani. Onlinelearningtorankforsequential
musicrecommendation. InProceedingsofthe13thACMConferenceonRecommenderSystems,
pages237–245,2019.
R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference
optimization: Yourlanguagemodelissecretlyarewardmodel. arXivpreprintarXiv:2305.18290,
2023.
A. Rame, G. Couairon, M. Shukor, C. Dancette, J.-B. Gaya, L. Soulier, and M. Cord. Rewarded
soups: towardspareto-optimalalignmentbyinterpolatingweightsfine-tunedondiverserewards.
arXivpreprintarXiv:2306.04488,2023.
K. Saito, A. Wachi, K. Wataoka, and Y. Akimoto. Verbosity bias in preference labeling by large
languagemodels. arXivpreprintarXiv:2310.10076,2023.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
algorithms. arXivpreprintarXiv:1707.06347,2017.
K.Singhal,T.Tu,J.Gottweis,R.Sayres,E.Wulczyn,L.Hou,K.Clark,S.Pfohl,H.Cole-Lewis,
D. Neal, et al. Towards expert-level medical question answering with large language models.
arXivpreprintarXiv:2305.09617,2023a.
16P.Singhal,T.Goyal,J.Xu,andG.Durrett. Alongwaytogo: Investigatinglengthcorrelationsin
rlhf. arXivpreprintarXiv:2310.03716,2023b.
S.Smith,M.Patwary,B.Norick,P.LeGresley,S.Rajbhandari,J.Casper,Z.Liu,S.Prabhumoye,
G.Zerveas,V.Korthikanti,etal. Usingdeepspeedandmegatrontotrainmegatron-turingnlg530b,
alarge-scalegenerativelanguagemodel. arXivpreprintarXiv:2201.11990,2022.
S.H.Sternberg. MathematicsandSocialSciences: ProceedingsoftheSeminarsofMenthon-Saint-
Bernard,France(1-27July,1960)andofGo¨sing,Austria(3-27July,1961),volume1. Mouton,
1965.
G. Swamy, C. Dann, R. Kidambi, Z. S. Wu, and A. Agarwal. A minimaximalist approach to
reinforcementlearningfromhumanfeedback. arXivpreprintarXiv:2401.04056,2024.
R.Taori,I.Gulrajani,T.Zhang,Y.Dubois,X.Li,C.Guestrin,P.Liang,andT.B.Hashimoto. Alpaca:
Astrong,replicableinstruction-followingmodel. StanfordCenterforResearchonFoundation
Models.https://crfm.stanford.edu/2023/03/13/alpaca.html,3(6):7,2023.
G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M.
Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint
arXiv:2312.11805,2023.
A.J.Thirunavukarasu,D.S.J.Ting,K.Elangovan,L.Gutierrez,T.F.Tan,andD.S.W.Ting. Large
languagemodelsinmedicine. Naturemedicine,29(8):1930–1940,2023.
H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,
P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv
preprintarXiv:2307.09288,2023.
L. Tunstall, E. Beeching, N. Lambert, N. Rajani, K. Rasul, Y. Belkada, S. Huang, L. von Werra,
C. Fourrier, N. Habib, et al. Zephyr: Direct distillation of lm alignment. arXiv preprint
arXiv:2310.16944,2023.
A.Tversky. Intransitivityofpreferences. Psychologicalreview,76(1):31,1969.
L. von Werra, Y. Belkada, L. Tunstall, E. Beeching, T. Thrush, N. Lambert, and S. Huang. Trl:
Transformerreinforcementlearning. https://github.com/huggingface/trl,2020.
B. Wang, Q. Xie, J. Pei, Z. Chen, P. Tiwari, Z. Li, and J. Fu. Pre-trained language models in
biomedicaldomain: Asystematicsurvey. ACMComputingSurveys,56(3):1–52,2023a.
Y.Wang,Q.Liu,andC.Jin. Isrlhfmoredifficultthanstandardrl? arXivpreprintarXiv:2306.14111,
2023b.
Z. Wang, Y. Dong, J. Zeng, V. Adams, M. N. Sreedhar, D. Egert, O. Delalleau, J. P. Scowcroft,
N.Kant,A.Swope,etal. Helpsteer: Multi-attributehelpfulnessdatasetforsteerlm. arXivpreprint
arXiv:2311.09528,2023c.
Z. Wang, Y. Dong, J. Zeng, V. Adams, M. N. Sreedhar, D. Egert, O. Delalleau, J. P. Scowcroft,
N.Kant,A.Swope,etal. Helpsteer: Multi-attributehelpfulnessdatasetforsteerlm. arXivpreprint
arXiv:2311.09528,2023d.
17J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-
thoughtpromptingelicitsreasoninginlargelanguagemodels. AdvancesinNeuralInformation
ProcessingSystems,35:24824–24837,2022.
B. Workshop, T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic´, D. Hesslow, R. Castagne´, A. S.
Luccioni, F.Yvon, etal. Bloom: A176b-parameteropen-accessmultilinguallanguagemodel.
arXivpreprintarXiv:2211.05100,2022.
X. Wu, K. Sun, F. Zhu, R. Zhao, and H. Li. Better aligning text-to-image models with human
preference. arXivpreprintarXiv:2303.14420,2023a.
Z. Wu, Y. Hu, W. Shi, N. Dziri, A. Suhr, P. Ammanabrolu, N. A. Smith, M. Ostendorf, and
H. Hajishirzi. Fine-grained human feedback gives better rewards for language model training.
arXivpreprintarXiv:2306.01693,2023b.
W.Xiong,H.Dong,C.Ye,H.Zhong,N.Jiang,andT.Zhang. Gibbssamplingfromhumanfeedback:
Aprovablekl-constrainedframeworkforrlhf. arXivpreprintarXiv:2312.11456,2023.
C.Ye,W.Xiong,Y.Zhang,N.Jiang,andT.Zhang. Atheoreticalanalysisofnashlearningfrom
humanfeedbackundergeneralkl-regularizedpreference. arXivpreprintarXiv:2402.07314,2024.
W.Yuan,R.Y.Pang,K.Cho,S.Sukhbaatar,J.Xu,andJ.Weston. Self-rewardinglanguagemodels.
arXivpreprintarXiv:2401.10020,2024.
Z.Yuan,H.Yuan,C.Tan,W.Wang,S.Huang,andF.Huang. Rrhf: Rankresponsestoalignlanguage
modelswithhumanfeedbackwithouttears. arXivpreprintarXiv:2304.05302,2023.
Y. Zhao, R. Joshi, T. Liu, M. Khalman, M. Saleh, and P. J. Liu. Slic-hf: Sequence likelihood
calibrationwithhumanfeedback. arXivpreprintarXiv:2305.10425,2023.
L.Zheng,W.-L.Chiang,Y.Sheng,S.Zhuang,Z.Wu,Y.Zhuang,Z.Lin,Z.Li,D.Li,E.Xing,etal.
Judgingllm-as-a-judgewithmt-benchandchatbotarena. arXivpreprintarXiv:2306.05685,2023.
Z.Zhou,J.Liu,C.Yang,J.Shao,Y.Liu,X.Yue,W.Ouyang,andY.Qiao. Beyondone-preference-
for-all: Multi-objectivedirectpreferenceoptimization. arXivpreprintarXiv:2310.03708,2023.
D.M.Ziegler,N.Stiennon,J.Wu,T.B.Brown,A.Radford,D.Amodei,P.Christiano,andG.Irving.
Fine-tuninglanguagemodelsfromhumanpreferences. arXivpreprintarXiv:1909.08593,2019.
18