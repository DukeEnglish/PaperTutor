Selection of appropriate multispectral camera exposure settings and
radiometric calibration methods for applications in phenotyping and
precision agriculture
Vaishali Swaminathana,*, J Alex Thomassonb,a, Robert G Hardina, Nithya Rajanc
*Corresponding author: Vaishali Swaminathan (vaishaliswaminathan@tamu.edu)
a Department of Biological and Agricultural Engineering, Texas A&M University, College Station,
Texas, USA
b Department of Agricultural and Biological Engineering, Mississippi State University, Starkville,
Mississippi, USA
c Depart of Soil and Crop Sciences, Texas A&M University, College Station, Texas, USA
Abstract
Radiometric accuracy of data is crucial in quantitative precision agriculture, to produce
reliable and repeatable data for modeling and decision making. The effect of exposure time and
gain settings on the radiometric accuracy of multispectral images was not explored enough. The
goal of this study was to determine if having a fixed exposure (FE) time during image acquisition
improved radiometric accuracy of images, compared to the default auto-exposure (AE) settings.
This involved quantifying the errors from auto-exposure and determining ideal exposure values
within which radiometric mean absolute percentage error (MAPE) were minimal (< 5%). The
results showed that FE orthomosaic was closer to ground-truth (higher R2 and lower MAPE) than
AE orthomosaic. An ideal exposure range was determined for capturing canopy and soil objects,
without loss of information from under-exposure or saturation from over-exposure. A simulation
of errors from AE showed that MAPE < 5% for the blue, green, red, and NIR bands and < 7%
for the red edge band for exposure settings within the determined ideal ranges and increased
1exponentially beyond the ideal exposure upper limit. Further, prediction of total plant nitrogen
uptake (g/plant) using vegetation indices (VIs) from two different growing seasons were closer
to the ground truth (mostly, R2 > 0.40, and MAPE = 12 to 14%, p < 0.05) when FE was used,
compared to the prediction from AE images (mostly, R2 < 0.13, MAPE = 15 to 18%, p >= 0.05).
Keywords. Unmanned Aerial Vehicles, multispectral imaging, camera calibration, precision
agriculture, quantitative imaging, exposure time, radiometric calibration
Highlights
1. Determined the use of default auto-exposure settings on multispectral cameras affected
radiometric accuracy.
2. Fixed exposure setting for a Micasense RedEdge-3 camera was determined.
3. The object-based ELM calibration worked on the auto-exposure and fixed-exposure images
where the focus was on plant and soil reflectance.
4. Fixed exposure multispectral camera settings resulted in better spatiotemporal radiometric
accuracy than auto-exposure settings.
1. Introduction
Multispectral cameras are widely used in agricultural remote sensing due to their ability
to capture a wide range of information – spectral signatures, spatial patterns, structural details,
temporal variations – especially when mounted on platforms that provide fast-coverage like
unmanned aerial vehicles (UAVs), piloted aircraft, and satellites. They often operate at relatively
narrow (e.g., 10 to 40 nm) bandwidths that are sensitive to plant-canopy and soil reflectance and
usually include the visible (blue, green, red) and infrared (red edge, NIR, SWIR, etc.) regions of
the spectrum. Some popular multispectral cameras for agriculture are the Micasense RedEdge
and Altum series, the Parrot Sequoia, Sentera sensors, and the DJI P4, all of which have been
used extensively on UAVs. Since UAV-based multispectral images are captured from higher
altitudes, they are subjected to atmospheric attenuation and reduction in spatial resolution, for
which calibration is necessary.
2Multispectral data can be classified as qualitative or quantitative based on the application.
Examples of qualitative applications are spatial pattern recognition for disease detection [1] and
detection and counting of fruiting organs, plants, and invasive species [2-4], where contrast,
brightness, sharpness, and resolution are more important than the exact values of the pixels.
Quantitative applications involve extracting measurable plant spectral and morphological
features for precision input management [5-7], yield estimation [2,8,9], and phenotyping [10,11].
Quantitative spectral features include spectral/radiometric features like band reflectance
measures and vegetation indices (VIs). The light reflected from field crops needs to be estimated
accurately to perform reliable quantitative analysis, but it tends to change with season, weather,
and time of the day.
Radiometric calibration is the standardization of images acquired from various sensors
used at different locations and times under various environmental conditions. The methods
adopted to minimize effects of spatiotemporal and environmental factors on UAV multispectral
images include scheduling data collection under clear sky or constant illumination within a
timeframe close to solar noon to minimize long shadows and intercept irradiance and reflectance
perpendicular to the camera irrespective of the location. Raw digital numbers (DNs) that are
camera-specific (dependent on light sensitivity or ISO, radiometric resolution, focal length, etc.)
are converted to radiance and reflectance. Radiance is a measure of light reflected by objects in
the scene, and reflectance is a unitless ratio of object-reflected radiance to solar irradiance.
Hence, accurate determination of atmospheric attenuation factors, lens intrinsics(focal length and
principal point), lens distortion, and other camera related parameters (pixel size, resolution,
vignette model, exposure settings), and is necessary for accurate and precise determination of
radiance or reflectance. periodic in-lab lens calibration using calibration frames [12] and
3integrating spheres [13,14] were recommended to account for changes in camera intrinsic
parameters and image vignette and row gradient, respectively, due to mechanical wear and tear
from repeated usage and harsh conditions. Cao et al. (2020) [15] used a look-up table approach
to convert DNs from a RedEdge camera to radiance.
Radiometric calibration with ground truth values obtained from known-reflectance
targets with approximately Lambertian reflectance is widely used to correct atmospheric effects
in UAV images and other errors not accounted for by camera calibration. This method involves
deriving a linear relationship between known and estimated radiance or reflectance of the targets
and is broadly termed as the empirical line method (ELM) [16] or linear regression method
(LRM) [17], and it varies depending on the number of targets used. Mamaghani and Salvaggio
(2019a) [13] described 1-point and multi-point ELM based on manufacturer-provided calibration
panels and in-field color gradient calibration targets, respectively. Camera manufacturers
generally provide a single calibration panel to calibrate reflectance based on images of panels
captured at ground level before and after UAV flights. However, this method does not account
for atmospheric attenuation, camera dark current, and lens imperfections. A comparative study of
different radiometric calibration methods on Parrot Sequoia images found that the multi-point
ELM methods applied in post processing calibration of orthomosaics were the most accurate
[18]. Guo et al. (2019) [17] noted that atmospheric attenuation of reflectance increased with
flight altitude, and three color-graded (dark, moderate, white) reflectance targets were sufficient
for the MCA camera used in their study. Chakhvashvili et al. (2021) [19] used nine panels with
gradients ranging from black to white to calibrate a Micasense dual camera system and
concluded that multi-point ELM produced 50% lower errors than 1-point ELM in estimating
reflectance. Sub-band ELM calibration has been described as a band-wise multi-point ELM
4method in which the low-reflecting visible bands are calibrated with a power model and the
high-reflecting infrared bands are calibrated with linear models [20]. Sub-band ELM was
generalized by Luo et al. (2022) [21] and named piecewise-ELM, in which power and linear
models for all bands were chosen depending on the reflectance range of the objects in the image.
These studies did not probe whether the non-linear relationship between image pixel values and
the reflectance of calibration targets in a given band were due to saturation from over-exposure,
which can happen if the objects in the scene were predominantly of low reflectivity.
The spectral reflectance of field crops during the vegetative and reproductive phases were
found to be < 25 % and 25 to 90 % in the visible and infrared bands, respectively [22-24].
Likewise, soil reflectance is generally low (< 25 %) in the visible bands and varies somewhat
with changes in moisture content, but reflectance in the IR region is greatly influenced by
moisture content, with dry soil reflecting > 25 % and moist soil reflecting < 15 % [25]. Zhang et
al. (2012) [23] also noted that spectral characteristics of crops can dynamically change over time,
wherein a 40 % increase in green reflectance occurred in the post-reproductive growth stages.
Therefore, in agricultural fields, where plants and soil predominantly constitute the scene, bright
pixels like those of white calibration targets tend to be saturated (information loss), especially in
the visible bands. Olsson et al. (2021) [26] observed pixel saturation of calibration panels in their
study and recommend installing larger panels that can prompt cameras to adjust their exposure
settings and thus the dynamic range to accommodate the high-reflectance objects. However, this
may result in datasets with very large variations in exposure time and gain.
Among the various camera related parameters stated previously, the effect of exposure
settings on radiometric accuracy of images is the least researched subject. The default and widely
used automatic exposure (auto-exposure) settings in cameras are programmed to optimize the
5dynamic range of the overall scene captured by them. Bagnall et al. (2023) [27] demonstrated
that UAV-based RedEdge-3 images with fixed gain and exposure-time settings had lower
radiometric calibration errors compared to images obtained with auto-exposure settings, but the
reason behind the differences needs elucidation. It is currently unclear how variations in
exposure time and gain affect radiometric calibration accuracy.
The overall goal of this study was to further clarify whether fixed camera exposure
settings provide more consistent and accurate estimates of reflectance from UAV-borne
multispectral cameras compared to the usually implemented auto-exposure settings, as well as to
posit reasons for any differences found. The goal was broken down into four objectives: (1) to
determine the ideal camera settings to perform ELM for each band by gradually varying the
camera’s exposure time and gain, (2) to check for potential errors from autoexposure by cross-
calibration (i.e., applying ELMs derived from different exposure time-gain combinations on
calibration target images), (3) to compare spatiotemporal radiometric accuracies of the fixed and
auto-exposure orthomosaics based on distributed in-field reflectance calibration targets, and (4)
to assess the practical implications of accuracy differences between fixed and auto-exposure
settings by comparing total nitrogen (N) estimated from multiple VIs obtained from fixed and
auto-exposure images.
2. Methods
2.1 Experiment Setup
A RedEdge-3 five-band multispectral camera with a downwelling light sensor (DLS-2)
(AgEagle Aerial Systems, Wichita, KS, USA) was mounted on a Matrice 100 UAV (DJI,
Shenzhen, Guangdong, China) and used to collect aerial images from a cotton field at the Texas
A&M Agrilife Research farm near College Station, TX, USA (Latitude: 30.550338, Longitude: -
696.435214). The camera consisted of five imaging detectors with narrow-band optical filters
having central wavelength and full-width at half-maximum (FWHM) range as follows: blue (475
nm; 20 nm), green (560 nm; 20 nm), red (668 nm; 10 nm), red edge (RE: 717 nm; 10 nm), and
near infrared (NIR: 840 nm; 40 nm). The DLS-2 (referred to as DLS hereafter) included a light
sensor and GPS/IMU unit and was synchronized with the camera to record irradiance at each
waveband along with geographic coordinates at the time of image capture. A calibration
reflectance panel (CRP) (Figure 1 right) was provided by the camera manufacturer for 1-point
linear calibration of the multispectral images.
Multiple in-field reflectance calibration targets (60 cm x 60 cm) (Figure 1 left) were
distributed across the field to verify the accuracy of reflectance estimated from aerial
multispectral images. Each ground reference target was composed of black (B, low reflective),
gray (G, medium reflective), and white (W, high reflective) rubber mats coated with matte-finish
paints to minimize non-Lambertian (specular) reflection. The reflectance values of the
calibration targets were measured a few days prior to the flights with a hand-held ASD
spectroradiometer (FieldSpec-4, Analytical Spectral Device Inc., Boulder, CO, USA). The
reflectance values collected with 1 nm resolution from 350 nm to 2500 nm from the
spectroradiometer were averaged over the bandwidth corresponding to the FWHM of the bands
(Table 1). Additionally, ground control points (GCPs) were placed inside the field for
georectification, and their coordinates were measured with < 2 cm accuracy based on RTK
GNSS (Reach RS2, Emlid, Budapest, Hungary) measurements.
7Figure 1 In-field reflectance calibration targets with white (W, high), gray (G, medium), and black (B, low)
reflectance calibration targets (left) and Micasense calibrated reflectance panel (CRP) (right).
Table 1 ASD FieldSpec-4 spectroradiometer measured average reflectance ratios of the black (B), gray (G),
and white (W) in-field calibration targets in the blue, green, red, red edge, and NIR bands for years 2021 and
2022.
Bands Year 2021 Year 2022
B G W B G W
Blue 0.08 0.28 0.53 0.08 0.33 0.86
Green 0.08 0.26 0.50 0.08 0.31 0.86
Red 0.08 0.23 0.46 0.08 0.28 0.84
Red edge 0.09 0.23 0.45 0.08 0.27 0.82
NIR 0.09 0.21 0.42 0.08 0.25 0.85
2.2 Data Collection
2.2.1 Exposure time and gain sweep from a stationary UAV
To study the effect of camera brightness parameters (exposure time and gain) on
reflectance estimation, an exposure sweep experiment was conducted in which the UAV was
controlled to hover at 30 m above ground level (AGL) over one set of in-field reflectance
calibration targets under clear sky conditions within ± 30 mins of local solar noon (13:30 hours,
approx.), and the camera exposure was remotely adjusted across 15 values from 0.067 ms to 4.30
ms and 0.0067 ms to 2.00 ms, for gain values 1x and 2x, respectively. Exposure and gain for the
five bands were adjusted independently. Just before the UAV was deployed, images of the CRP
were also collected close to ground level by holding the UAV over the panel while changing the
8exposure and gain settings manually to the same values that were used for UAV image
acquisition.
2.2.2 UAV flights
The UAV flight missions were executed with UgCS mission planning software (SPH
Engineering, Riga, Latvia) at 30 m AGL with > 80 % forward and > 70 % lateral overlaps. The
ground sampling distance of the camera at 30 m AGL was 2.08 cm. The flights were conducted
within ±1 hour of solar noon to minimize shadows and bidirectional reflectance and under clear
conditions to maintain consistent illumination. One flight was conducted with the camera’s auto-
exposure settings, and another was conducted with manually fixed exposure settings, determined
for each band from the exposure sweep experiment described previously. Fixed exposure
datasets were acquired with 1x gain, and the exposure settings for the five bands in each dataset
are listed in Table 2.The fixed and auto-exposure images were collected on 17 June 2021 and 16
June 2022 from a cotton field when the crops were at similar phenological stages. The 2021
dataset had no variations in illumination, while the 2022 auto-exposure dataset had minor
changes in illumination during the flight due to a thin layer of moving cloud. Further, the
agronomic relevance of the auto-exposure and fixed exposure image collection methods were
considered. Ground truth data from a nitrogen management trial within the cotton field were
referenced, and destructive biomass analysis through combustion was performed to derive total
biomass nitrogen (N) content, which was then used as a parameter to assess the performance of
fixed and autoexposure images in precision N management.
9Table 2 Exposure settings applied to the blue, green, red, red edge, and NIR bands on the 17 June 2021 and 16
June 2022 fixed exposure flights.
Wavelength Exposure settings (ms), gain 1x
Bands 17 June 2021 16 June 2022
Blue 0.58 0.76
Green 0.58 0.58
Red 0.76 0.76
Red edge 1.00 0.76
NIR 1.00 0.76
2.3 Data processing and analysis
2.3.1 Reflectance from raw images
The individual images of the in-field reflectance target obtained from the UAV were
processed with Python based on the Micasense image processing library (GitHub repo:
https://github.com/micasense/imageprocessing/tree/master/micasense). The raw image data
(digital numbers, or DN) from the camera were converted to the quantitative measure of radiance
(L, W/m2/sr/nm), denoting the amount of radiation reaching the camera from ground level. The
conversion was based on the camera’s intrinsic parameters, brightness settings, a pre-determined
vignette model, and calibration coefficients stored in the image metadata (equations 1 to 4).
𝑎 𝑃−P
1 BL
𝐿 = 𝑉(𝑥,𝑦)∗ ∗ (1)
𝑔 𝑡 +𝑎 ∗𝑦−𝑎 ∗𝑡 ∗𝑦
𝑒 2 3 𝑒
1
𝑉(𝑥,𝑦) = (2)
𝑘
𝑘 = 1+𝑘 𝑟+𝑘 𝑟2 +𝑘 𝑟3 +𝑘 𝑟4 +𝑘 𝑟5 +𝑘 𝑟6 (3)
0 1 2 3 4 5
𝑟 = √(𝑥−𝐶 )2 −(𝑦−𝐶
)2
(4)
𝑥 𝑦
where,
V(x,y) was the vignette model
x, y represented the pixel coordinates
P and P was the normalized raw pixel and black level values
BL
10g and t were gain and exposure time
e
a , a , a , were radiometric calibration coefficients
1 2 3
k , k , k , k3, k , k were camera’s vignette correction coefficients
0 1 2 4 5
r was the Euclidean distance of pixel (x,y) from the center
C , C were the center pixel coordinates
x y
The CRP based calibration was performed to offset intrinsic camera errors. This is an
optional step, as intrinsic errors can be compensated along with atmospheric errors in post-
processing calibration. The radiance values of the CRP images in all bands were converted to
nominal reflectance (ρ ) based on the DLS recorded direct irradiance (E , µW/cm2/nm) at
CRP CRP
the time of image acquisition (equation 5). Then a correction factor (F) for each band was
obtained by dividing estimated CRP reflectance by known reflectance (ρ ), providing 1-
CRP_known
point ELM calibration equations of all images in the dataset (equation 6).
𝐿
𝐶𝑅𝑃
𝜌 = 100∗𝜋∗ (5)
𝐶𝑅𝑃 𝐸
𝐶𝑅𝑃
𝜌
𝐶𝑅𝑃
𝐹 = (6)
𝜌
𝐶𝑅𝑃_𝑘𝑛𝑜𝑤𝑛
The correction factor was applied to the UAV images to obtain a CRP-calibrated
reflectance value (equation 7).
𝐿
𝜌 = 100∗𝜋∗𝐹 ∗ (7)
𝐸
Here E is the at-altitude irradiance measured by the DLS light sensor when the UAV
images were captured. Note that it was important to convert radiance to reflectance for further
analysis, because the ground-truth reference data obtained from the spectroradiometer were also
in terms of reflectance. The Python workflow implemented parallel processing through multi-
threading to improve the processing speed of the script.
11An OpenCV script was deployed to draw square bounding boxes that extracted average
estimated reflectance of the black, gray, and white targets upon manual identification of target
centers. The estimated reflectance of the targets was compared to spectrometer-measured
reflectance to analyze the impact of exposure and gain settings on reflectance estimation
accuracy and dynamic range of the images, which indicated the radiometric resolution captured
by the camera. This information was used to derive ideal exposure time and gain settings to
perform full scale multipoint ELM that covered the entire reflectance scale from 0 to 1 and
object-based ELM calibration, where the calibration range was customized to focus on the
reflectance of canopy, soil, and other objects of interest (described below under post-processing
calibration).
A set of five images was sampled for each exposure-gain setting combination and
categorized into reference and target sets. While both sets had the same images, the reference set
was used to derive object-based ELM, and the target set was used to assess the errors from ELM
calibration. We use the term ‘cross-calibration’ to describe this process, which was performed to
quantify radiometric calibration errors from variations in exposure and gain during auto-
exposure flights. Mean absolute percentage error (MAPE) < 5 % was used as the acceptable
threshold to test whether object-based ELM achieved better results than similar studies
performed by Deng et al. (2018) [20] and Luo et al. (2022) [21], who had errors over 10 %.
2.3.2 Photogrammetric processing and post-processing calibration
Photogrammetric processing of the auto-exposure and fixed-exposure UAV flight images
was performed in Metashape Pro software (Agisoft LLC, St. Petersburg, Russia). The first step
in the workflow was tie-point matching, which used tie-point matching algorithms like SIFT,
SURF, ORB, etc., in conjunction with structure from motion (SfM), the details of which are not
12disclosed by the software developers. The GCPs were manually identified in the aerial images
and tagged for geo-rectification of the scene. The geo-rectified tie-points were filtered to remove
outliers, and a densified 3-D point-cloud was obtained and used to generate a high-resolution
digital elevation model (DEM) and a 5-band orthomosaic of the entire field. The highest possible
accuracy settings in Metashape Pro were used for each step in the workflow. An initial 1-point
ELM with the CRP panel images was performed to convert the raw DNs to roughly estimated
reflectance before generating orthomosaic images. As stated in the previous section, this
conversion was performed to remove camera induced errors and was an optional step, as camera
errors could have been accounted for in post-processing calibration, discussed below.
The orthomosaic images were calibrated in post-processing to remove atmospheric noise
in reflectance estimation and camera biases if CRP-based 1-point ELM was not performed. The
in-field reflectance calibration targets were manually identified, and average reflectance of each
reflectance target was obtained from shapefile polygons (35 cm x 35 cm) situated at the center of
each target. An object-based ELM approach (Table 3) was used to calibrate the individual bands
of the orthomosaic in postprocessing. The object-based ELM was adopted from the sub-band
ELM method of Deng et al. (2018) [20] and the piecewise ELM approach of Luo et al. (2022)
[21], in which calibration equations were determined to be linear or power models based on
whether the objects studied were high or low reflecting. In the previous studies, both methods
used auto-exposure settings for image acquisition, performed calibration on the entire dynamic
range (black to white) for all bands, and applied non-linear calibration equations in low-
reflecting bands. The object-based ELM implemented in the current study was linear, and the
calibration was performed by selecting targets that tightly bounded the expected range of soil and
canopy reflectance (Table 3). For example, the green band was calibrated between black and
13gray targets due to the low green reflectance of canopy and soil, while the NIR band was
calibrated with black, gray, and white targets due to higher NIR reflectance of both soil and
canopy.
Table 3 Object-based calibration range for the five bands of the Micasense RedEdge-3 multispectral camera
set using the black (B), gray (G), and white (W) color gradients of the in-field reflectance calibration targets.
Wavelength Bands Object-based calibration range
Blue B – G
Green B – G
Red B – G
Red edge B – G – W
NIR G – W
Images from the auto-exposure flight dataset were selected at random and segregated into
three categories: predominantly canopy, predominantly soil, and in-field reflectance calibration
targets. The exposure time and gain distribution in each category was studied. To verify the
spatiotemporal uniformity of reflectance estimated from the auto and fixed exposure flights,
averaged reflectance values of the calibration targets obtained from the orthomosaics were
compared to spectroradiometer measured reflectance values. Spatial uniformity (precision) of
reflectance was determined by the coefficient of determination (R2), which indicated the
goodness of fit of the target reflectance along the empirical line obtained from regression The
mean absolute percentage error (MAPE) after post-processing calibration was used to determine
spatiotemporal accuracy in reflectance estimation as compared to spectroradiometer measured
values.
𝑛
∑ (𝑦̂ −𝑦 )2
𝑅2 = 1− 𝑖=1 𝑖 𝑖 (8)
∑𝑛 (𝑦 −𝑦̅ )2
𝑖=1 𝑖 𝑖
𝑛
1 𝑦̂ −𝑦
𝑖 𝑖
𝑀𝐴𝑃𝐸 = ∑| |×100 (9)
𝑛 𝑦
𝑖
𝑖=1
142.3.3 Vegetation Index for Total N Estimation
To test the performance of fixed and autoexposure settings in real world applications –
specifically, in a cotton nitrogen management study – VIs (Table 4) were computed from the
respective orthomosaics. The seven VIs are sensitive to canopy structure, leaf chlorophyll, and
nitrogen content, all of which are indicators of accumulated total nitrogen in the plants (g/plant).
The VIs from fixed and autoexposure data obtained on 17 June 2021 and 16 June 2022 were
compared with total biomass N. These dates represented similar growth stages with data acquired
under different conditions to validate the consistency of the calibration methods. Linear
regression was performed between plot-level averages of the VIs and plot-level total N . The R2
and MAPE were used as metrics to assess the performance of the two exposure-setting methods
in estimating total biomass N content in cotton.
Table 4 Vegetation indices (VI) and their formula for monitoring cotton nitrogen status based on observed
reflectance (ρ) in the blue, green, red, red edge, and NIR bands of the RedEdge-3 multispectral camera.
Index Formula
𝜌 − 𝜌
Normalized difference 𝑁𝐼𝑅 𝑟𝑒𝑑
𝜌 + 𝜌
vegetation index (NDVI) 𝑁𝐼𝑅 𝑟𝑒𝑑
𝜌 − 𝜌
Normalized difference red 𝑁𝐼𝑅 𝑟𝑒𝑑𝑒𝑑𝑔𝑒
𝜌 + 𝜌
edge index (NDRE) 𝑁𝐼𝑅 𝑟𝑒𝑑𝑒𝑑𝑔𝑒
Triangular greenness index −0.5[(668−475)(𝜌 − 𝜌 )
𝑟𝑒𝑑 𝑔𝑟𝑒𝑒𝑛
(TGI)
−(668−560)(𝜌 −𝜌 )]
𝑟𝑒𝑑 𝑏𝑙𝑢𝑒
𝜌 − 𝜌
Green Normalized difference 𝑁𝐼𝑅 𝑔𝑟𝑒𝑒𝑛
𝜌 + 𝜌
Vegetation Index (GNDVI) 𝑁𝐼𝑅 𝑔𝑟𝑒𝑒𝑛
𝜌
Chlorophyll index red edge 𝑁𝐼𝑅
−1
𝜌
(CI ) 𝑟𝑒𝑑𝑒𝑑𝑔𝑒
rededge
𝜌
Chlorophyll Index green 𝑁𝐼𝑅
−1
𝜌
(CI ) 𝑔𝑟𝑒𝑒𝑛
green
𝜌 − 𝜌
Renormalized difference 𝑁𝐼𝑅 𝑟𝑒𝑑
√𝜌 + 𝜌
vegetation index (RDVI) 𝑁𝐼𝑅 𝑟𝑒𝑑
153. Results
3.1 Exposure and gain variations in auto-exposure flights
Large variations in exposure time and gain were observed when auto-exposure settings
were used for capturing different field objects (in-field reflectance calibration targets,
predominantly canopy, and predominantly soil pixels) at 30 m AGL on 17 June 2021 (Figure 2)
and 16 June 2022 (Figure 3), respectively. The red edge band had the least variability in
exposure time and gain, followed by the NIR and green bands. The most variability in exposure
and gain were in the blue and red bands. Overall, there was more variability in exposure and gain
in the 2022 dataset than the 2021 dataset for all bands. The red edge band exposure settings were
similar (< ± 0.10 ms) among the three objects for both years, although the 2022 dataset had
marginally higher variability. The NIR band exposure distribution in 2021 was less than 1.00 ms,
but in 2022 the values were predominantly greater than 1.00 ms for all categories. The green
exposure values for both years were similar, although the variations were greater in 2022. The
gain value for the red edge band was predominantly 2x, and the green and NIR gains were
mostly 1x. In 2021 , the red band exposure variability was minimal and differences in exposure
were observed only for soil category, but in 2022 more pronounced variations in exposure time
and gain were observed. The blue band displayed the most drastic changes in exposure and gain
among all three object categories, and these changes were more pronounced in the 2022 images.
For 2021 and 2022 combined, the exposure time ranges for the blue band were 1.00 to 2.00 ms
(1x gain) and 1.00 to 1.30 ms (2x gain). The combined exposure ranges for the green, red, red
edge, and NIR bands were 0.80 to 1.40 (1x), 1.40 to 2.00 (1x), 1.00 to 1.20 (2x), and 0.90 to 1.30
ms (1x), respectively.
16Figure 2 Exposure and gain distribution for images containing in-field reflectance calibration targets (tarps),
predominantly canopy, and predominantly soil pixels in each of the five bands of the RedEdge-3 multispectral
camera, collected on 17 June 2021 during a 30 m AGL autoexposure UAV flight. Only the blue band was
automatically captured with 1x and 2x gains for all objects. All other bands were captured either completely
with 1x (green, NIR) or 2x (red edge) or with only some objects having both 1x and 2x (red) gains.
Figure 3 Exposure and gain distribution for images containing in-field reflectance calibration targets (tarps),
predominantly canopy, and predominantly soil pixels in each of the five bands of the RedEdge-3 multispectral
camera, collected on 16 June 2022 during a 30 m AGL autoexposure UAV flight. Only the blue and red bands
were automatically captured with 1x and 2x gains for all objects. All other bands were captured either
completely with 1x (green, NIR) or with only some objects having both 1x and 2x (red edge) gains.
3.2 Reflectance response to exposure time and gain
The graphs in Figure 4 and Figure 5 depict the variations in estimated reflectance of the
black, gray, and white calibration targets in response to the exposure sweep experiment for gains
1x and 2x, respectively. The estimated white target reflectance across all bands was observed to
be marginally lower than the spectroradiometer measured white target reflectance, possibly due
to dust accumulation on the target surfaces under field conditions. It is also likely that the CRP
calibrated UAV images were subject to atmospheric distortion that resulted in lowered estimates
17of the white targets. It was observed that increasing the exposure time beyond a threshold (blue
and red vertical dotted lines in Figure 4 and Figure 5 ) decreased the dynamic range/resolution of
the estimated reflectance and gradually resulted in saturation at 0.50 reflectance ratio. It was also
noted that divergence and saturation occurred faster for the highly reflective white targets,
followed by the gray and black targets. For example, the divergence in estimated reflectance
from ground truth in the blue band was first observed for the white target with 1x gain at
approximately 0.50 ms, then the gray target at 1.00 ms, and finally the black target at 1.40 ms.
Also, saturation for all three reflectance targets occurred at lower exposure times as the gain
increased from 1x to 2x. Table 5 lists ideal exposure settings under 1x and 2x gains for the
Micasense RedEdge-3 multispectral camera at 30 m AGL in order to perform full-scale (red
dotted lines in Figure 4 and Figure 5) and object-based (blue dotted lines in Figure 4 and Figure
5) reflectance calibration. For object-based image acquisition, the upper limits of exposure time
for the visible bands were selected such that the estimated gray target reflectance did not deviate
from the ground truth and the white target reflectance did not saturate at 0.50. The upper limits
of object-based captures are generally higher than full-scale captures, where the limits are set by
the white target. The upper limits of the red edge and NIR bands were the same for full-scale and
object-based ELM due to the wider range of canopy and soil reflectance observed in them. The
thresholds for red edge and NIR were chosen such that the estimated reflectance of all the targets
remained close to ground-truth. While the estimated reflectance of all three targets remained
consistent at extremely low exposure times, lower limits were set to avoid loss of details due to
under-exposure of low reflecting canopy and soil objects.
18Figure 4 Reflectance estimation of the in-field reflectance calibration targets in the (top to bottom) blue, green,
red, red edge, NIR bands for different exposure settings when gain was set to 1x. The circles represent the CRP
calibrated estimated reflectance and the lines represent actual reflectance of the targets estimated with the
handheld ASD FieldSpec-4 spectroradiometer. The red dash line and blue dotted line were used to mark the
upper limits for full-scale (FS) multipoint ELM reflectance calibration and object-based (OB) reflectance
calibration, respectively.
19Figure 5 Reflectance estimation of the in-field reflectance calibration targets in the (top to bottom) blue, green,
red, red edge, NIR bands for different exposure settings when gain was set to 12x. The circles represent the
CRP calibrated estimated reflectance and the lines represent actual reflectance of the black, gray, and white
targets estimated with the handheld ASD FieldSpec-4 spectroradiometer. The red dash line and blue dotted
line were used to mark the upper limits for full-scale (FS) multipoint ELM reflectance calibration and object-
based (OB) reflectance calibration, respectively.
20Table 5 Ideal exposure time for Micasense RedEdge-3 multispectral camera at 30 m AGL with gains 1x and 2x
for capturing reflectance of all objects without distortion (full-scale reflectance) and capturing canopy and soil
objects without distortion.
Bands Ideal exp. time for full-scale Ideal exp. time for object-based
reflectance (ms) (canopy and soil) reflectance (ms)
Gain 1x Gain 2x Gain 1x Gain 2x
Blue 0.25 – 0.43 0.09 – 0.18 0.25 – 0.59 0.09 – 0.31
Green 0.18 – 0.31 0.09 – 0.14 0.18 – 0.43 0.09 – 0.18
Red 0.31 – 0.77 0.18 – 0.31 0.31 – 0.99 0.18 – 0.42
Red edge 0.43– 0.99 0.14 – 0.43 0.43 – 0.99 0.18 – 0.43
NIR 0.43 – 0. 77 0.18 – 0.42 0.42 – 0.77 0.18 – 0.43
3.3 Cross-calibration accuracy matrix
To quantify the errors associated with variations in exposure time and gain during auto-
exposure flights, cross-calibration was performed. Figure 6 through Figure 10 show the cross-
calibration MAPE matrices for the five bands (in order of NIR, red edge, blue, green, and red)
when object-based ELM from the reference images (vertical axis) was applied to target images
(horizontal axis). All images in the top left and bottom right quadrants had 1x and 2x gains,
respectively. The top right and bottom left quadrants represented scenarios where the reference
and target gains were different. The color gradient from green to red in the matrices denote low
to high MAPE from cross-calibration, respectively. The cells on the diagonals (marked by dotted
black boxes) of the top left and bottom right quadrants, where the target and reference images
had the same exposure and gain, had < 1 % MAPE in the visible and NIR bands. However, the
diagonal cells in the red edge band had > 5 % errors, possibly from using all three calibration
targets to derive ELM, resulting in a wider calibration range and greater errors compared to other
bands. Additionally, larger solid black boxes were drawn to highlight the camera settings for
which cross-calibration MAPE were ≤ 5 % for the visible and NIR bands, and ≤ 10 % for the red
edge band. The exposure upper limit for the reference and target images within the back boxes
corresponded to the ideal exposure upper limits (Table 5, Object-based ELM) for the respective
gains. The box upper limits in the top right and bottom left quadrants of the blue (Figure 8) and
21green (Figure 9) bands did not coincide with the ideal upper limit but still were in the immediate
neighborhood (±1 cell) of the ideal upper limit. For exposure values exceeding the upper limits,
MAPE increased exponentially. An example can be seen with the NIR band (Figure 6), where
the ideal exposure upper limit for 1x gain and 2x gain were determined to be 0.77 ms and 0.43
ms, respectively. Within the same figure, cell A (solid red box), for which the reference and
target settings were within the ideal range, had 2.82 % error, as compared to cell B (solid cyan
box), where the target image exposure time was beyond the ideal range, shows 19.87 % error.
Likewise, cell C (maroon box), where the reference and target gains were 1x and 2x,
respectively, and the exposure times were within limits, the error was 1.45 %; whereas cell D
(blue box) had 27.84 % error as the reference image exposure was outside the upper limit for the
NIR bands. While errors rates grew exponentially when exposure time increased, it was observed
that the MAPE in most cases was within 10 – 15 % in the cells adjacent to the upper limits set by
the bounding box.
Figure 6 Object-based cross-calibration MAPE for the NIR band for various exposure and gain combinations,
where the acceptable range is marked by the back box.
22Figure 7 Object-based cross-calibration MAPE for the red edge band for various exposure and gain
combinations, where the acceptable range is marked by the back box.
Figure 8 Object-based cross-calibration MAPE for the blue band for various exposure and gain combinations,
where the acceptable range is marked by the back box.
23Figure 9 Object-based cross-calibration MAPE for the green band for various exposure and gain combinations,
where the acceptable range is marked by the back box.
Figure 10 Object-based cross-calibration MAPE for the red band for various exposure and gain combinations,
where the acceptable range is marked by the back box.
3.4 Reflectance Accuracy in Auto-Exposure and Fixed Exposure Orthomosaics
Figure 11 and Figure 12 compare ground-truth spectral reflectance with estimated
reflectance of the in-field calibration targets after post-processing calibration of the 17 June 2021
24and 16 June 2022 orthomosaics, respectively. The graphs contain the object-based ELM
equation, R2 based on linear regression, and MAPE after the ELM calibration. It should be noted
that the white target reflectance in 2021 was only about 0.50, while in 2020 it was close to 0.85,
thereby providing a much wider range to assess reflectance characteristics, especially in the red
edge and NIR bands. In 2021, fixed exposure settings ensured higher spatial uniformity (R2 =
0.99) and reduction in error by at least half that of auto-exposure. The NIR band was an
exception, wherein the R2 and MAPE values were similar for fixed and autoexposure
orthomosaics. In 2022, the overall performance of the fixed exposure images was better than that
of autoexposure, however not greatly different in the green and red bands. The red edge and NIR
bands in 2022 had noticeably better R2 and MAPE values for fixed exposure relative to
autoexposure. In both datasets, the fixed exposure images’ blue band (2021: R2 = 0.99 and
MAPE = 3.11; 2022: R2 = 0.97 and MAPE = 5.91) clearly outperformed the autoexposure
images (2021: R2 = 0.75 and MAPE = 24.25; 2022: R2 = 0.79 and MAPE = 25.06). The 2022
fixed exposure images did not achieve the same level or accuracy and spatial uniformity
(precision) as the 2021 fixed exposure images in the blue, green, and red bands, but they
performed better than autoexposure in both years.
25Figure 11 Object-based ELM performed on the blue, green, red, red edge, and NIR (left to right) bands of fixed
exposure (top) and auto-exposure (bottom) datasets acquired on 17 June 2021. The ELM performance is
denoted by the empirical line equation, coefficient of determination (R2) and MAPE.
Figure 12 Object-based ELM performed on the blue, green, red, red edge, and NIR (left to right) bands of fixed
exposure (top) and auto-exposure (bottom) datasets acquired on 16 June 2022. The ELM performance is
denoted by the empirical line equation, coefficient of determination (R2) and MAPE.
263.5 Vegetation Indices for Total N Estimation
Vegetation indices derived from 17 June 2021 and 16 June 2021 fixed and autoexposure
orthomosiacs (Table 4) were used for total accumulated biomass N estimation. Figure 13 shows
a comparison of the actual and predicted total N to compare the performance of VIs calculated
from uncalibrated and calibrated orthomosaics of fixed and autoexposure datasets. Noticeably
higher R2, lower MAPE, and significant correlation (p < 0.05) with total N were observed for
uncalibrated and calibrated fixed exposure VIs compared to autoexposure VIs. NDVI and RDVI,
which had similar equations, did not differ significantly in terms of R2 and MAPE between fixed
exposure and uncalibrated autoexposure. Calibrated auto-exposure VIs, which had the poorest
correlation with total N among the four, showed no significant correlation with total N except for
CI and RDVI. It was also observed that calibrated autoexposure CI and NDRE, based
green rededge
on the red edge and NIR bands, respectively, had the lowest R2 values. Although calibrated fixed
exposure had the best R2 values among all VIs, the improvement in MAPE compared to
calibrated and uncalibrated autoexposure was only 2 to 6 %).
27Figure 13 Comparison of actual total N content in the nitrogen management plots with estimated total N from
six VIs (rows top to bottom: NDRE, NDVI, GNDVI, CI , CI , and RDVI) collected on 17 June 2021 and
rededge green
16 June 2022 using four different methods (columns left to right: fixed exposure, fixed exposure calibrated,
auto-exposure, and auto-exposure calibrated).
4. Discussion
4.1 Radiometric calibration and object-based ELM
Radiometric calibration is a necessary step in image-based remote sensing to standardize
and improve the accuracy of data collected from different sensors, at different times, and
different locations. Many studies have used reference panel-based and panel-less calibration of
UAV-collected aerial multispectral images, but panel-based multi-point ELM has proven to be
the most reliable calibration method [18]. The CRP-based 1-point ELM calibration, which is
widely recommended by camera manufacturers, is prone to calibration inaccuracies because it
assumes the calibration equation regression line passes through the origin, but that assumption
may not be valid due to the influence of lighting and atmospheric conditions that may result in a
28non-zero intercept. Also, the wear and tear on a camera’s lens (distortion parameters) and
electronic components can affect its sensitivity over time. The various multi-point ELM methods
listed in the Introduction section used two or more reflectance levels to accurately determine the
slope and intercept for UAV image calibration. In this study, we proposed an object-based ELM
calibration, a type of multi-point ELM, in which the calibration range was selected to focus on
crop canopy and soil reflectances. This meant the blue, green, and red bands were calibrated
within a narrow region bounded by the low (~0.07) and medium (~0.30) reflectance targets, as
canopy and soil reflectance in these bands did not exceed 0.30. The NIR band was calibrated
between medium and high (~0.80) reflectance, and the red edge band was calibrated with all
targets to cover the wider range of expected soil and canopy reflectance in these bands. The
object-based ELM complemented the fixed exposure-settings used in this study that optimized
on the reflectance range of soil and canopy objects (Table 5). Therefore, if the exposure setting
were chosen to capture the entire reflectance range from 0.00 to 1.00, object-based ELM in that
case will be the same as multi-point ELM, where all the targets (B, G, and W) are included for
calibration. Object-based ELM worked reasonably well on the auto-exposure data set, too,
because the auto-exposure settings optimized the dynamic range of the images based on the soil
and canopy objects on ground. Here again, the object-based ELM complemented the exposure
settings used to acquire images.
4.2 Problems with auto-exposure
Most studies on radiometric calibration of UAV-borne multispectral cameras for
precision agriculture used the default auto-exposure settings, and calibration performance was
assessed based on errors in estimating reflectance of known reference targets. While
autoexposure settings help to minimize loss of low-intensity data through histogram equalization,
29the influence of exposure settings on radiance or reflectance estimation accuracy, especially in
scenes with drastic variations in radiance intensities, remained heretofore unexplored.
Comparing the estimated and actual reflectances of calibration targets in the orthomosaic showed
reasonable accuracy (MAPE < 10 %) and spatial uniformity (R2 > 0.96) for autoexposure in the
green, red, and NIR bands, and relatively high MAPE in the blue and red edge bands (Figure
11:bottom rows and Figure 12:bottom rows). However, due to significant variations in exposure
time and gain when capturing different objects like canopy, soil, and in-field reflectance
calibration targets (Figure 2 and Figure 3), we probed the effectiveness of the equations
recommended by the camera manufacturer (equations 1 to 7) in providing consistent measures
on radiance or reflectance under auto-exposure settings. The first observation was that the blue
and red edge bands, which had lower R2 and higher MAPE, were also prone to relatively higher
variations in exposure time and gain when autoexposure was used. Further, the band-wise cross-
calibration MAPE matrix (Figure 6 to Figure 10) quantified the radiometric MAPE from
applying object-based ELM derived from one exposure setting to images with different exposure
settings. Here, it was observed that the cross-calibration MAPE remained low (< 5 % or <10 %
for red edge) only when the exposure values were within the ideal exposure upper limits (Table
5). In actuality, however, the auto-exposure settings of the camera were commonly beyond the
prescribed upper limits when capturing the different objects (Figure 2 and Figure 3). It was
observed that higher exposure times progressed towards saturation of target reflectance values at
0.50 ratio (Figure 4 and Figure 5), resulting in unrecoverable loss of details. Therefore, the non-
linear relationship between estimated and actual reflectance in low reflecting bands in other
studies [20,21] can be attributed to reflectance of the high reflecting targets tending towards
saturation in response to the higher exposure times. These results and observations highlight the
30importance of exposure time on radiometric accuracy, which can be compromised if exposure
settings (1) change drastically from scene-to-scene and object-to-object and (2) are beyond the
ideal limits. Since it is difficult to control exposure variations in autoexposure flights, we
recommend the usage of fixed exposure settings to have greater control over the radiometric
accuracy of the images.
4.3 Fixed exposure for UAV image acquisition
Two sets of ideal exposure time ranges are listed in Table 5 for full-scale multipoint and
object-based ELM, respectively. However, full-scale reflectance (0 – 1) calibration is not
required under field conditions, especially in the visible spectral bands where canopy and soil
reflectance generally do not exceed 0.30. The ideal exposure values for object-based ELM were
determined such that the estimated reflectances of the preferred targets were as close as possible
to ground-truth (straight line), while ensuring reflectances of other targets did not saturate at 0.50
ratio. The RedEdge-3 exposure settings during UAV missions were set close to the upper limits
of the ideal range at 1x gain to capture low reflecting objects with sufficient radiometric
resolution while also avoiding saturation. The actual exposure times for the green and NIR bands
in 2021 and blue and green bands in 2022 (Table 2) were marginally more (one setting higher)
than the prescribed ideal exposure upper limits (Table 5). Since the ideal exposure settings were
determined at 30 m AGL under clear sky conditions, and they may vary for different altitudes
and illumination conditions.
Since soil and canopy were the objects of interest, fixed exposure ensured that
radiometric readings were not distorted by the presence of other (highly reflective) objects within
the field. Fixed exposure ensured that the raw DNs, radiance, and reflectance estimated for
similar objects stayed consistent throughout the image acquisition process. Fixed exposure along
31with object-based ELM produced better spatial uniformity (higher R2) and accuracy (lower
MAPE) in target reflectance estimation for all bands, and it also produced VIs with better
correlation to total N (g/plant) than did auto-exposure (Figure 13). The results here demonstrate
that fixed exposure time and gain settings improved radiometric accuracy, which is important for
quantitative remote sensing.
4.4 Future work
Ihe reflectance saturation occurred at 0.50 instead of the maximum reflectance value
(1.00) because of the equations that converted DNs to radiance or reflectance (equations 1 to 7)
and the subsequent CRP-based calibration. This study did not explore the mathematical reason
behind reflectance saturation at half the maximum reflectance, so exploration of that
phenomenon is left to future study. It should be noted that although the VIs from calibrated fixed
exposure images had higher correlation with total N than VIs from auto-exposure images
(uncalibrated and calibrated), the R2 values were still less than 0.5 in most cases. This was
because the image datasets for both years were acquired early in the cotton-growth cycle when
the canopy had not attained full closure, and plants had thus just started responding to the
different N treatments. In future, these experiments should be repeated to determine the effect of
exposure settings on radiometric calibration accuracy in mid growth stages. Also, this study was
conducted with three calibration targets to select the calibration range and ideal exposure time. It
would be interesting to include more color graded calibration targets in between the existing
black, gray, and white targets, which would increase the resolution of the calibration panels to
precisely fine-tune the calibration range and exposure time at different growth stages for
different types of plants. We carried out the fixed exposure experiments with 1x gain only, and
the ideal exposure times listed in Table 5 are relevant only for the RedEdge-3 camera used in this
32study. Future work should investigate in detail the effect of gain settings on radiometric
calibration. It would also be interesting to develop ways to automatically choose exposure time
and gain settings for each band based on conditions prevalent in the field.
5. Conclusions
This study was conducted to analyze the impact of exposure time on radiometric calibration
accuracy of UAV-based multispectral images, specifically those obtained from the Micasense
RedEdge-3 camera. It was observed that exposure time had a significant impact on the radiometric
resolution and reflectance accuracy of image based reflectance estimates. The results also showed
that autoexposure led to drastic variability in camera exposure settings, which can be detrimental
to the spatiotemporal consistency in radiometric data. Hence, we concluded that the conventional
way of using autoexposure settings for image acquisition is more prone to errors. The use of fixed
exposure settings is recommended for canopy and soil reflectance estimation to avoid radiometric
inaccuracies arising from drastic scene-to-scene or object-to-object variations in camera exposure
time and gain.
33References
[1] Wang T, Thomasson JA, Yang C, Isakeit T, Nichols RL. Automatic classification of cotton
root rot disease based on UAV remote sensing. Remote Sensing 2020;12(8):1310.
[2] Yeom J, Jung J, Chang A, Maeda M, Landivar J. Automated open cotton boll detection for
yield estimation using unmanned aircraft vehicle (UAV) data. Remote Sensing
2018;10(12):1895.
[3] Osco LP, De Arruda, Mauro dos Santos, Junior JM, Da Silva NB, Ramos APM, Moryia
ÉAS, et al. A convolutional neural network approach for counting and geolocating citrus-trees in
UAV multispectral imagery. ISPRS Journal of Photogrammetry and Remote Sensing
2020;160:97-106.
[4] Yadav PK, Thomasson JA, Hardin R, Searcy SW, Braga-Neto U, Popescu SC, et al.
Detecting volunteer cotton plants in a corn field with deep learning on UAV remote-sensing
imagery. Comput.Electron.Agric. 2023;204:107551.
[5] Zhang L, Zhang H, Niu Y, Han W. Mapping maize water stress based on UAV multispectral
remote sensing. Remote Sensing 2019;11(6):605.
[6] Bian J, Zhang Z, Chen J, Chen H, Cui C, Li X, et al. Simplified evaluation of cotton water
stress using high resolution unmanned aerial vehicle thermal imagery. Remote Sensing
2019;11(3):267.
[7] Early estimation of nitrogen stress and uptake in cotton based on spectral and morphological
features extracted from UAV multispectral images. Autonomous Air and Ground Sensing
Systems for Agricultural Optimization and Phenotyping VII: SPIE; 2022.
[8] Zhou X, Kono Y, Win A, Matsui T, Tanaka TS. Predicting within-field variability in grain
yield and protein content of winter wheat using UAV-based multispectral imagery and machine
learning approaches. Plant Production Science 2021;24(2):137-51.
[9] Siegfried J, Adams CB, Rajan N, Hague S, Schnell R, Hardin R. Combining a cotton ‘Boll
Area Index’with in-season unmanned aerial multispectral and thermal imagery for yield
estimation. Field Crops Res. 2023;291:108765.
[10] Xie C, Yang C. A review on plant high-throughput phenotyping traits using UAV-based
sensors. Comput.Electron.Agric. 2020;178:105731.
[11] Shi Y, Thomasson JA, Murray SC, Pugh NA, Rooney WL, Shafian S, et al. Unmanned
aerial vehicles for high-throughput phenotyping and agronomic research. PloS one
2016;11(7):e0159781.
34[12] Frontera F, Smith MJ, Marsh S. Preliminary investigation into the geometric calibration of
the micasense rededge-m multispectral camera. ISPRS-International Archives of the
Photogrammetry, Remote Sensing and Spatial Information Sciences 2020;43.
[13] Mamaghani B, Salvaggio C. Multispectral sensor calibration and characterization for sUAS
remote sensing. Sensors 2019;19(20):4453.
[14] Barker JB, Woldt WE, Wardlow BD, Neale CM, Maguire MS, Leavitt BC, et al. Calibration
of a common shortwave multispectral camera system for quantitative agricultural applications.
Precision Agriculture 2020;21:922-35.
[15] Cao H, Gu X, Wei X, Yu T, Zhang H. Lookup table approach for radiometric calibration of
miniaturized multispectral camera mounted on an unmanned aerial vehicle. Remote Sensing
2020;12(24):4012.
[16] Mamaghani B, Salvaggio C. Comparative study of panel and panelless-based reflectance
conversion techniques for agricultural remote sensing. arXiv preprint arXiv:1910.03734 2019.
[17] Guo Y, Senthilnath J, Wu W, Zhang X, Zeng Z, Huang H. Radiometric calibration for
multispectral camera of different imaging conditions mounted on a UAV platform. Sustainability
2019;11(4):978.
[18] Poncet AM, Knappenberger T, Brodbeck C, Fogle Jr M, Shaw JN, Ortiz BV. Multispectral
UAS data accuracy for different radiometric calibration methods. Remote Sensing
2019;11(16):1917.
[19] Comparison of reflectance calibration workflows for a uav-mounted multi-camera array
system. 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS: IEEE;
2021.
[20] Deng L, Hao X, Mao Z, Yan Y, Sun J, Zhang A. A subband radiometric calibration method
for UAV-based multispectral remote sensing. IEEE Journal of Selected Topics in Applied Earth
Observations and Remote Sensing 2018;11(8):2869-80.
[21] Luo S, Jiang X, Yang K, Li Y, Fang S. Multispectral remote sensing for accurate acquisition
of rice phenotypes: Impacts of radiometric calibration and unmanned aerial vehicle flying
altitudes. Frontiers in Plant Science 2022;13.
[22] Nidamanuri RR, Zbell B. Existence of characteristic spectral signatures for agricultural
crops–potential for automated crop mapping by hyperspectral imaging. Geocarto Int.
2012;27(2):103-18.
[23] Zhang H, Lan Y, Suh CP, Westbrook JK, Lacey R, Hoffmann WC. Differentiation of cotton
from other crops at different growth stages using spectral properties and discriminant analysis.
Transactions of the ASABE 2012;55(4):1623-30.
35[24] Arafat SM, Aboelghar MA, Ahmed EF. Crop discrimination using field hyper spectral
remotely sensed data. 2013.
[25] Bunnik N. Spectral reflectance characteristics of agricultural crops and application to crop
growth monitoring. Advances in Space Research 1981;1(10):21-40.
[26] Olsson P, Vivekar A, Adler K, Garcia Millan VE, Koc A, Alamrani M, et al. Radiometric
correction of multispectral uas images: Evaluating the accuracy of the parrot sequoia camera and
sunshine sensor. Remote Sensing 2021;13(4):577.
[27] Bagnall GC, Thomasson JA, Yang C, Wang T, Han X, Sima C, et al. Uncrewed aerial
vehicle radiometric calibration: A comparison of autoexposure and fixed‐exposure images. The
Plant Phenome Journal 2023;6(1):e20082.
36