Unveiling the Potential of Robustness in Evaluating Causal Inference Models
YiyanHuang1 CheukHangLeung2 SiyiWang2 YijunLi2 QiWu2
Abstract Bicaetal.,2021;Kinyanjui&Johansson,2022),andfinan-
cialapplication(Bottouetal.,2013;Chuetal.,2021;Huang
Thegrowingdemandforpersonalizeddecision-
etal.,2021;Donnellyetal.,2021;Ferna´ndez-Lor´ıaetal.,
makinghasledtoasurgeofinterestinestimat-
2023). Theprimarygoalinpersonalizeddecision-making
ing the Conditional Average Treatment Effect
istoquantifytheindividualizedcausaleffectofaspecific
(CATE). The intersection of machine learning
treatment(orpolicy/intervention)onthetargetoutcome.For
and causal inference has yielded various effec-
example,inhealthcare,thefocusliesinunderstandingthe
tiveCATEestimators. However,deployingthese
effect of a medical treatment on a patient’s health status;
estimators in practice is often hindered by the
ineconomics,theemphasisisonevaluatinghowapublic
absenceofcounterfactuallabels,makingitchal-
policywouldaffectone’swelfare;inthedigitalmarket,re-
lengingtoselectthedesirableCATEestimatorus-
searchersexploretheinfluenceofanE-commerceretailer’s
ingconventionalmodelselectionprocedureslike
lending strategy on a consumer’s potential consumption.
cross-validation. ExistingapproachesforCATE
Understandingsuchcausaleffectsiscloselyconnectedwith
estimatorselection,suchasplug-inandpseudo-
identifyingacrucialconceptincausalinference,whichis
outcome metrics, face two inherent challenges.
knownastheConditionalAverageTreatmentEffect(CATE).
Firstly,theyarerequiredtodeterminethemetric
formandtheunderlyingmachinelearningmodels Identifying the CATE in observational studies, however,
forfittingnuisanceparametersorplug-inlearners. inevitablyfacesasignificanthurdle: theabsenceofcoun-
Secondly,theylackaspecificfocusonselecting terfactualknowledge. AccordingtoRubinCausalModel
arobustestimator. Toaddressthesechallenges, (Rubin,2005),theCATEisdeterminedbycomparingpo-
thispaperintroducesanovelapproach,theDis- tentialoutcomesunderdifferenttreatmentassignments(i.e.,
tributionally Robust Metric (DRM), for CATE treatandcontrol)forthesameindividual. Nonetheless,in
estimatorselection. TheproposedDRMnotonly real-world applications, this comparison isunfeasible be-
eliminates the need to fit additional models but causewecanonlyobservethepotentialoutcomeunderthe
alsoexcelsatselectingarobustCATEestimator. actual treatment (i.e., factual outcome), while the poten-
Experimentalstudiesdemonstratetheefficacyof tial outcome under the alternative treatment that was not
theDRMmethod,showcasingitsconsistenteffec- assigned(i.e.,counterfactualoutcome)remainsunobserved.
tivenessinidentifyingsuperiorestimatorswhile Theunavailabilityofthecounterfactualoutcomeiswidely
mitigatingtheriskofselectinginferiorones. recognizedasthefundamentalproblemincausalinference
(Holland,1986),makingitdifficulttoaccuratelydetermine
thetruevalueoftheCATE.
1.Introduction
The advancement of machine learning (ML) has opened
The escalating demand for decision-making has sparked
upapromisingopportunitytoimprovetheCATEestima-
an increasing interest in Causal Inference across various
tion from observational data. Several innovative CATE
researchdomains,suchaseconomics(Farrell,2015;Cher-
estimation approaches, such as meta-learners and causal
nozhukovetal.,2018;Kitagawa&Tetenov,2018;Abadie
MLmodels,havebeenproposedtotacklethefundamental
etal.,2023),statistics(Wager&Athey,2018;Li&Wager,
challengeincausalinferenceandenhancethepredictiveac-
2022;Foster&Syrgkanis,2023;Kennedy,2023),health-
curacyofCATEestimates(asdiscussedinSection2). Nev-
care(Zhangetal.,2019;Fosteretal.,2011;Qianetal.,2021;
ertheless,theemergenceofavarietyofCATEestimation
methodspresentsanewpredicamentfordecision-makers:
1DepartmentofAppliedMathematics,HongKongPolytech-
nic University: yiyhuang@polyu.edu.hk 2School of Data Sci- Given multifariousoptionsforCATEestimators, which
ence,CityUniversityofHongKong: chleung87@cityu.edu.hk, estimatorshouldbechosen? Standardmodelselectionpro-
swang348-c@my.cityu.edu.hk,yijunli5-c@my.cityu.edu.hk.Cor- cedures,suchascross-validation,arecommonlyemployed
respondenceto:QiWu<qi.wu@cityu.edu.hk>.
to select the desirable model by evaluating each model’s
performance on a validation set using metrics like mean
Underreview.
1
4202
beF
82
]GL.sc[
1v29381.2042:viXraSubmissionandFormattingInstructionsforICML2024
squareerror(MSE).Unfortunately,theseprocedurescannot learnersmainlyincludetraditionallearnerssuchasS-learner,
beappliedtoCATEestimatorselectionduetotheinherent T-learner,PS-learner,andIPW-learner,aswellasnewlearn-
fundamentalproblemincausalinference,i.e.,onlylabels ers such as X-learner (Ku¨nzel et al., 2019), DR-learner
for factual outcomes, but not counterfactual outcomes or (Kennedy,2023;Foster&Syrgkanis,2023),R-learner(Nie
CATE,areavailableinobserveddata. Therefore,exploring &Wager,2021),andRA-learner(Curth&vanderSchaar,
proper metrics for CATE estimator selection remains an 2021b).Thespecificdetailsofthesemeta-learnersarestated
essentialyetchallengingresearchtopicincausalinference. inAppendixA.1. Additionally,somestudiesalsofocuson
developinginnovativecausalMLmodelsforCATEestima-
Recentresearchhasemphasizedthesignificanceofmodel
tion,suchasCausalBART(Hahnetal.,2020),CausalForest
selectionforCATEestimators,ashighlightedin(Schuler
(Wager&Athey,2018;ATHEYetal.,2019;Oprescuetal.,
etal.,2018;Curth&VanDerSchaar,2023;Mahajanetal.,
2019),generativemodelslikeCEVAE(Louizosetal.,2017)
2024). These works have proposed and summarized two
andGANITE(Yoonetal.,2018),representationlearning
typesofcriteriaforCATEestimatorselection: plug-inand
netsincludingSITE(Yaoetal.,2018),TARNet(Shalitetal.,
pseudo-outcome metrics, and their large-scale empirical
2017),Dragonnet(Shietal.,2019),FlexTENet(Curth&
studieshaveshownthatthesemetrics,tosomeextent,are
vanderSchaar,2021a),andHTCE(Bica&vanderSchaar,
helpfulinselectingtheoptimalCATEestimator. However,
2022),disentangledlearningnetslikeD2VD(Kuangetal.,
therestillremaintwocriticaldilemmaswhenutilizingthese
2017; 2020), DeR-CFR (Wu et al., 2022), and DR-CFR
metricsforCATEestimatorselection. Thefirstdilemmalies
(Hassanpour&Greiner,2019),andrepresentationbalanc-
inthequandaryofdeterminingtheformofevaluationmetric
ing nets such as BNN (Johansson et al., 2016), CFRNet
anditsunderlyingMLalgorithm. AsdiscussedinSection
(Shalitetal.,2017),DKLITE(Zhangetal.,2020),IGNITE
2, there are multiple choices available for metric forms
(Guoetal.,2021),BWCFR(Assaadetal.,2021),andDRRB
andMLalgorithms,yetoptingforaspecificchoiceamong
(Huangetal.,2023). Recentsurveys(Guoetal.,2020;Yao
them can inevitably lead us back to the initial estimator
etal.,2021;Nogueiraetal.,2022)havealsoconducteda
selectionproblem. Theseconddilemmaisthelackoffocus
systematicreviewofvariouscausalinferencemethods.
onselecting arobust CATE estimator. Robust estimators
areexpectedtodemonstratethenecessarygeneralizability
totheunseen(uncertain)counterfactualdistributionwhile CATE estimator selection. Compared to the diverse
maintainingstableperformanceacrossdifferentscenarios. range of CATE estimation methods, selecting CATE es-
In real-world applications, it can be futile to prioritize a timators has received limited attention in existing causal
”stellar”estimatoroverarobustestimator,becausewecan inference research. Current methods for selecting CATE
neveraccuratelydeterminetheactualbestestimatorwithout estimators can be broadly classified into two main cate-
knowledgeofthegroundtruthCATE. gories. Thefirstcategory,whichisalsoconsideredinthis
paper,involvesusingplug-inandpseudo-outcomemethods
Contributions. Inlightofabovechallenges,wepropose
to evaluate CATE estimators. These methods share two
anovelDistributionallyRobustMetric(DRM)methodfor
common characteristics: 1) Both methods require fitting
CATEestimatorselection. Thecontributionsareasfollows:
newMLmodelsonavalidationsetandthenimplementing
(1)WeestablishanovelupperboundforCATEestimation
thelearnedMLmodelsineithertheplug-insurrogateorthe
error (i.e., the PEHE defined in Section 3) in a distribu-
pseudo-outcomesurrogate;2)Bothmethodsserveassur-
tionally robust manner, serving as the foundation for the
rogatesfortheexpectederrorbetweentheCATEestimator
establishmentofDRM.(2)TheDRMisamodel-freemetric andthetrueCATE,i.e.,Roracle(τˆ)inequation(1). Thedif-
thateliminatestheneedtofitadditionalmodelsfornuisance
ferencebetweenthetwomethodsisthattheplug-inmethod
parametersorplug-inlearners. (3)TheDRMisdesigned
directlyapproximatesthetrueCATEfunction,whereonly
toprioritizeselectingarobustCATEestimator. (4)Wepro-
covariatevariablesareinvolved,whilethepseudo-outcome
videfinitesampleanalysisfortheproposeddistributionally
methodtypicallyconstructsaspecificformulaincorporating
robust value Vˆ(τˆ), showing it decays to V(τˆ) at a rate of
covariates, treatment, and outcome variables. Recent re-
n−1/2. (5)Experimentalresultsdemonstratetheconsistent
search(Schuleretal.,2018;Curth&VanDerSchaar,2023;
effectiveness of DRM in identifying superior estimators
Mahajan et al., 2024) has conducted thorough empirical
whilemitigatingtheriskofselectinginferiorones.
investigationsintotheutilizationofthesetwomethodsfor
selectingCATEestimators. Theirfindingssuggestthatno
2.RelatedWork singleselectioncriterioncanuniversallyoutperformothers
in all scenarios in the task of selecting CATE estimators.
CATE estimation. Recent advancements in ML have
Particularly,Curth&VanDerSchaarprovidevaluablein-
emergedaspowerfultoolsforestimatingCATEfromob-
sights into the advantages and disadvantages of different
servational data, and researchers pay particular attention
plug-inandpseudo-outcomemetrics,highlightingtheneed
to meta-learners and causal ML models. Existing meta-
tofurtherexploreCATEestimatorselectionmethods. More
2SubmissionandFormattingInstructionsforICML2024
detailsofthetwoselectionmethodsarestatedinAppendix neousEffects(PEHE)w.r.t. estimatorτˆ(Hill,2011;Shalit
A.2. The second category considers leveraging the data etal.,2017). Regrettably,duetotheunavailabilityofτ ,
true
generatingprocess(DGP)togeneratesyntheticdatawith Roracle(τˆ)merelyservesasanoraclemetricthatcannotbe
aknowntrueCATE,allowingthevalidationofCATEesti- utilizedtoevaluateCATEestimators’performancesinreal
mators’performanceonthissyntheticdata. Forexample, applications. Previousstudieshaveintroducedalternative
Advani et al. find that placebo and structured empirical plug-inandpseudo-outcomemetricstotacklethischallenge
MonteCarlomethodsarehelpfulforestimatorselectionun- for selecting CATE estimators. In the following, we will
dersomerestrictiveconditions. Schuleretal.;Atheyetal.; provideanoverviewofthesetwometrics.
Parikhetal. focusontraininggenerativemodelstoenforce
One can establish a plug-in estimator τ˜ or construct a
the generated data to approximate the distribution of the
pseudo-outcome estimator Y˜ using the validation data.
observeddata. However,theDGP-basedmethodstillfaces
Then, an estimator can be selected based on the follow-
some limitations in CATE estimator selection due to two
ingtwocriteria: τˆ =argmin Rplug(τˆ)or
keyfactors: i)itonlyguaranteestheresemblanceofthegen- select τˆ∈{τˆ1,...,τˆJ} τ˜
τˆ =argmin Rpseudo(τˆ)suchthat
erateddatatothefactualdistribution,withoutconsidering select τˆ∈{τˆ1,...,τˆJ} Y˜
thecounterfactualdistribution, andii)thereisapotential
(cid:118)
r this ek go ef nt eh re atm ive eth mo od df ea lv so (r Cin ug rte hst ei tm aa lt .o ,r 2s 0t 2h 1a )t .closelyresemble Rp τ˜lug(τˆ)=(cid:117) (cid:117) (cid:116) n1 (cid:88)n (τˆ(X i)−τ˜(X i))2, (2)
i=1
(cid:118)
3.Background Rp Y˜seudo(τˆ)=(cid:117) (cid:117) (cid:116) n1 (cid:88)n (τˆ(X i)−Y˜ i)2. (3)
3.1.Preliminary i=1
Notations. Supposetheobservationaldatacontainni.i.d. Notably,boththeplug-inandpseudo-outcomemetricsne-
samples{(x i,t i,y i)}n i=1,withtheassociatedrandomvari- cessitate the fitting of nuisance parameters η˜ (e.g., η˜ =
ablesbeing{(X i,T i,Y i)}n i=1. Foreachuniti,X i ∈ X ⊂ (µ˜ 1,µ˜ 0,π˜)) using off-the-shelf ML models. Once η˜ is
Rd is d-dimensional covariates and T i ∈ {0,1} is the bi- obtained, we can proceed to establish τ˜ and Y˜. For the
narytreatment. Potentialoutcomesfortreat(T = 1)and plug-inmetric,τ˜canbeconstructedusinganyCATEesti-
control (T = 0) are denoted by Y1,Y0 ∈ Y ⊂ R. The mator discussed in Appendix A.1, yielding various met-
propensityscore(Rosenbaum&Rubin,1983)isdefinedas rics such as plug-T, plug-DR, etc. For instance, the T-
π(x):=P(T =1|X =x). Theconditionalmeanpoten- learner can be employed to create the plug-in estimator
tialoutcomesurfaceisdefinedasµ t(x):=E[Yt |X =x] τ˜(X)=µ˜ 1(X)−µ˜ 0(X),whichcanbefurthercompared
fort∈{0,1}. ThetrueCATEisdefinedas withthecandidateestimatorτˆonthevalidationdatasetus-
τ (x):=E(cid:2) Y1−Y0 |X =x(cid:3) =µ (x)−µ (x). ing equation(2). This isreferred toas theplug-T metric.
true 1 0 Regardingthepseudo-outcomemetric,Y˜ canbeconstructed
Followingthestandardandnecessaryassumptionsinpoten-
usingaspecificformuladiscussedinAppendixA.2,yield-
tialoutcomeframework(Rubin,2005),weimposeAssump-
ingvariousmetricssuchaspseudo-DR,pseudo-R,etc. For
tion3.1thatensuretreatmenteffectsareidentifiable. instance,thedoublyrobust(DR)formulaestablishesY˜ =
Assumption3.1(Consistency,Overlap,andUnconfounded- µ˜ (X)−µ˜ (X)+ T (Y−µ˜ (X))− 1−T (Y−µ˜ (X)),
1 0 π˜(X) 1 1−π˜(X) 0
ness). Consistency: Ifthetreatmentist,thentheobserved whichcanbefurthercomparedwiththecandidateestima-
outcome Y equals Yt. Overlap: The propensity score is torτˆonthevalidationdatasetusingequation(3). Thisis
bounded away from 0 to 1, i.e., 0 < π(x) < 1, ∀x ∈ X. referredtoasthepseudo-DRmetric. Notethatinlinewith
Unconfoundedness: Yt ⊥⊥T |X, ∀t∈{0,1}. (Curth & Van Der Schaar, 2023), the evaluation metrics
basedontheinfluencefunction(Alaa&VanDerSchaar,
3.2.CATEEstimatorSelection 2019)andtheR-learnerobjective(Nie&Wager,2021)are
categorizedintothepseudo-outcomemetric.
ThegoalofCATEestimatorselectionistoidentifythebest
CATEestimator,denotedbyτˆ ,fromasetofJ candidate
best
Challenges. However,theaforementionedCATEestima-
estimators{τˆ ,...,τˆ }. Thebestestimatorshouldsatisfy
1 J
tor selection criteria, the plug-in metric and the pseudo-
τˆ =argmin Roracle(τˆ)suchthat
best τˆ∈{τˆ1,...,τˆJ} outcomemetric,encountertwocrucialdilemmas:
(cid:118)
(cid:117) n
(cid:117)1 (cid:88) Thefirstdilemmaliesindeterminingthemetricformand
Roracle(τˆ):=(cid:116)
n
(τˆ(X i)−τ true(X i))2. (1)
selecting the underlying ML models. As previously dis-
i=1
cussed,plug-inmetricshavevariousformssuchasplug-T,
Here,Roracle(τˆ)isassociatedwithE[(τˆ(X)−τ (X))2], as do pseudo-outcome metrics such as pseudo-DR. How-
true
which is known as the Precision of Estimating Heteroge- ever,studieslike(Mahajanetal.,2024)havedemonstrated
3SubmissionandFormattingInstructionsforICML2024
thatnosinglemetricformisaglobalwinner, leavingthe candidateestimators. Theunderlyingrationaleisrootedin
choiceofmetricformanopenproblem. Additionally,both theinsightthatreducingtheupperboundcantheoretically
plug-inandpseudo-outcomemetricsrelyonestimatingnew resultinalowerPEHE(Shalitetal.,2017;Johanssonetal.,
nuisance parameters η˜ using ML algorithms such as lin- 2022). However, using equation (4) for CATE estimator
ear models, tree-based models, etc. Plug-in metrics even selectionposestwochallenges. Firstly,thisupperboundis
needtofitanadditionalMLmodelfortheplug-inlearner specificallydesignedforselectingrepresentationbalancing
τ˜. However,withoutpriorknowledgeofthetruedatagen- models,ratherthangenericCATEestimators. Secondly,it
eratingprocess,choosingappropriateMLalgorithmsfrom cannotbeutilizedforselectingdirectCATElearnerssince
theplethoricoptionscanbeexceedinglychallenging. Con- itrequiresestimatingtwopotentialoutcomesurfaces. Mo-
sequently,thequandaryofdeterminingthemetricformand tivated by the insights gained from the upper bound and
underlyingMLmodelbringsusbacktotheoriginalestima- aimingtoaddresstheaforementionedchallenges,wewill
torselectionproblem(Curth&VanDerSchaar,2023). present a new upper bound for PEHE in the forthcoming
section. Thisnovelboundnotonlyenablestheselectionof
The second dilemma is that these metrics are not well-
generalCATEestimatorsbutalsosatisfiesthetworequire-
targetedforselectingarobustCATEestimator. Sincethe
mentsdiscussedinSection3.2.
counterfactualdistributionisinherentlyunobservable,mod-
elstrainedonobservedfactualdatacannotguaranteetheir
suitabilityforunobservedcounterfactualdata,leadingtoun- 4.Method
certaintyinCATEestimates.Giventhecrucialroleofcausal
Inthissection,weproposeanovelDistributionallyRobust
inference in decision-making, the desiderata for a robust
Metric(DRM)methodforCATEestimatorselection. We
estimatorthatexhibitsgeneralizabilitytotheunseencounter-
beginbyoutliningtheconstructionofanewPEHEupper
factualdistributionandmaintainsstableperformanceacross
bound in a distributionally robust manner in Section 4.1.
differentscenariosbecomesparamountinrealapplications.
Subsequently, weestablishtheDRMbasedonthisnovel
Thisneedevenoutweighsthepursuitofanideal“stellar”
upperboundinSection4.2.
estimator because we can never know which estimator is
trulysuperbwithoutaccesstocounterfactuallabels.
4.1.DerivingthePEHEUpperBound
Inlightofthesechallenges,weaimtoexploreanevaluation
Inequation(4),thePEHEisboundedbythefactualerror
metricforselectingCATEestimatorsthatfulfillsthefollow-
anddistributionaldistancebetweentreatandcontrolrepre-
ingtworequirements: (1)Themetricshouldbemodel-free,
sentations,whileweboundthePEHEinadistinctway. To
eliminatingtheneedforadditionalmodellearningfornui-
establishtheboundstatedinCorollary4.3,wefirstbound
sanceparametersorplug-inlearners. (2)Themetricshould
thePEHEusingthefollowingProposition4.1.
possess the ability to measure the robustness of a CATE
estimator. Interestingly, the PEHE upper bound in repre- Proposition4.1. ThePEHEw.r.t. theCATEestimatorτˆ
sentationbalancingliterature(Shalitetal.,2017;Johansson satisfiesthefollowinginequality:
etal.,2022)meetsthesetworequirements. Inthefollow- E[(τˆ(X)−τ (X))2]≤2(cid:0)E[(τˆ(X)−Y1)2]+ζ(cid:1) ,
true
ing discussion, we will provide a brief overview of how
(5)
the PEHE upper bound can be utilized as a heuristic for
where
selectingCATEestimators.
ζ =V[µ (X)]−V[Y1]+V[µ (X)]+(E[µ (X)])2.
1 0 0
3.3.CATEEstimatorSelection: Inspirationfromthe
TheproofisdeferredtoAppendixB.1.
PEHEUpperBoundinRepresentationBalancing
Proposition 4.1 indicates that the PEHE can be upper
Consider a function h(Φ(X),t) that estimates Yt, where
boundedbytwoterms,E[(τˆ(X)−Y1)2]andζ,whereζ is
Φisarepresentationencoder. Accordingto(Shalitetal.,
thesumoffourconstantsthatareindependentofτˆ. Further,
2017;Johanssonetal.,2022),thePEHEw.r.t. (h,Φ)can
wedecomposethetermE[(τˆ(X)−Y1)2]asfollows:
beupperboundedbytheerrorinfactualoutcomesandthe
distancebetweenthetreatandcontrolrepresentations: E[(τˆ(X)−Y1)2]
ϵ (h,Φ)≤2(cid:0) ϵT=1(h,Φ)+ϵT=0(h,Φ) =E[(τˆ(X)−Y1)2|T =1]P(T =1) (6)
PEHE F F
(4) (cid:124) (cid:123)(cid:122) (cid:125)
+B ·Dist(PT,PC)−2σ2(cid:1) . Empiricallycomputable
Φ Φ Φ Y
+E[(τˆ(X)−Y1)2|T =0]P(T =0). (7)
MoredetailsofthisboundareprovidedinAppendixA.3. (cid:124) (cid:123)(cid:122) (cid:125)
Equation(4)intuitivelyoffersanaturalapproachforselect- Empiricallyuncomputable
ing a CATE estimator: the ideal estimator should be the Equation(6)canbecomputedempiricallysincethepoten-
onethatachievesthesmallestupperboundvalueamongall tialoutcomeY1 isobservableintreatsamples. However,
4SubmissionandFormattingInstructionsforICML2024
equation (7) is empirically uncomputable due to the un- 4.2.EstablishingDistributionallyRobustMetric
availability of Y1 in control samples. Our objective now
Inthissection,wewillpresentthespecificdetailsinvolved
istoestablishanupperboundfortheuncomputableterm
inconstructingDRMbasedonCorollary4.3. Theconstruc-
E[(τˆ(X)−Y1)2|T = 0]. Toachievethis, wefirstdefine
tionprocesscanbesummarizedinthreesteps.
anambiguitysetbasedontheKullback-Leibler(KL)diver-
genceinDefinition4.2.
Step1: EstablishingcomputationaltractabilityofV(τˆ).
Definition4.2(KLambiguityset). Giventwodistributions Theprimalprobleminequation(9)isinfinite-dimensional,
QandP,lettheambiguityradiusϵ>0quantifythemag- posing a computational challenge. To overcome this, we
nitude of distribution shifts of Q relative to P. The KL can reformulate the primal problem to a feasible finite-
ambiguity(uncertainty)setB ϵ(P)isdefinedas dimensionalproblem,asdemonstratedinTheorem4.4.
Theorem 4.4. The distributionally robust value V(τˆ) in
B (P):={Q:D (Q||P)≤ϵ}, (8)
ϵ KL equation(9)isequivalentto
(cid:90) q(x)
where D (Q||P)= q(x)log dx
KL p(x) V(τˆ)=minλϵ+λlogEPT [exp((τˆ(X)−Y1)2/λ)].
X
λ>0
(11)
denotestheKLdivergence,measuringthedifferenceofthe
TheproofisdeferredtoAppendixB.3.
arbitrarydistributionQfromthereferencedistributionP.
Basedonfinitesampledata, thecorrespondingempirical
Now we can bound E[(τˆ(X) − Y1)2|T = 0]. Define estimateV(τˆ)canbeobtainedasfollows:
PT :=P(X,Y1|T =1)andPC :=P(X,Y1|T =0)for
n ito yt ra ati do in ua sl insim Dep fili nc ii tt iy o. nB 4y .2s ,e tt hti eng qua an na tid te yq Eu Pat Cel [y (τˆla (r Xge )−am Yb 1ig )2u ]-
Vˆ(τˆ)=m λ>in 0λϵ+λlog
n1
t
(cid:88)n
T iexp((τˆ(X i)−Y i)2/λ).
inequation(7)satisfies i=1
(12)
Notethatinequation(12),thepotentialoutcomeY1 isre-
EPC [(τˆ(X)−Y1)2]≤V(τˆ), placedbytheobservedoutcomeY becausethesummation
where V(τˆ):= sup EQ[(τˆ(X)−Y1)2]. (9) isovertreatsamples,whichfollowstheConsistencyassump-
Q∈Bϵ(PT) tioninAssumption3.1. SinceweuseV(τˆ)toapproximate
V(τˆ) based on observational data, it is also necessary to
Equation(9)revealsthatthequantityEPC[(τˆ(X)−Y1)2] provide a finite sample analysis of the gap between Vˆ(τˆ)
andV(τˆ). InthefollowingTheorem4.5,wedemonstrate
can be upper bounded by V(τˆ), which represents the dis-
tributionallyrobust(worst-case)valueofE[(τˆ(X)−Y1)2]
that|Vˆ(τˆ)−V(τˆ)|decaysatarateofn−1/2.
overtheuncertaintysetB (PT). Theintuitionbehindthis Theorem4.5. Letu:=P(T =1). Assume0<λ≤λ≤
is: sincethetargetdistribuϵ tionPC isunavailable,wecan λ¯ and(τˆ(X)−Y)2 isboundedwithintherange¯ ofM to
constructanambiguitysetcenteredaroundtheavailableref- M¯. Forn≥2/u2log(2/δ),withprobability1−δ,we¯ have
erencedistributionPT,suchthattheambiguitysetislarge
(cid:32)(cid:114) (cid:33)
enoughtocontaintheunavailabletargetdistributionPC.By 2λ¯2log(2/δ)
|Vˆ(τˆ)−V(τˆ)|≤O
doingso,wecanensurethatthevalueoftheuncomputable nu2
quantityEPC[(τˆ(X)−Y1)2]willbeatmostV(τˆ).
(cid:115)  (13)
8λ¯2log2
As the value V(τˆ) indicates as a distributionally robust +O δ (cid:0) exp(M¯/λ−M/λ¯)(cid:1)2 .
nu2 ¯ ¯
(worst-case)performance,itcannaturallymeasurethero-
bustness of the CATE estimator τˆ against distributional
shiftsfromthereferencedistributionPT tothedeployment TheproofisdeferredtoAppendixB.4.
distributionQ. Consequently,wecanobtainthePEHEup-
per bound based on this distributionally robust value (or Step2: Determiningtheambiguityradiusϵ. Theambi-
worst-casevalue)V(τˆ),whichispresentedinCorollary4.3. guityradiusϵplaysacriticalroleinreal-worldapplications
(MohajerinEsfahani&Kuhn,2018;Maetal.,2020;Pflug,
Corollary4.3. LetV(τˆ)bethequantitydefinedinequation
2023). However, determining an appropriate value for ϵ
(9), ζ be the constant given in Proposition 4.1, and u :=
canbechallengingasitrequiresstrikingabalancebetween
P(T =1). ThePEHEw.r.t. τˆcanbeupperboundedby
ensuringtheboundinequation(9)holdsandmaintaining
itstightness. Specifically,ifϵissettoosmall,itfailstoguar-
E[(τˆ(X)−τ (X))2]
true anteethatthetargetdistributionPC iscontainedwithinthe
(10)
≤2(uEPT [(τˆ(X)−Y1)2]+(1−u)V(τˆ)+ζ). ballB ϵ(PT)(theboundinequation(9)doesnotnecessarily
5SubmissionandFormattingInstructionsforICML2024
hold). Ontheotherhand,ifϵissettoolarge,eventhough Algorithm1UsingDRMforCATEEstimatorSelection
theballB ϵ(PT)canencompassmoredistributionstoensure Input: ThecandidateCATEestimators{τˆ 1,...,τˆ J}. The
thetargetdistributionPC iscontained(theboundinequa- validationdatasetwithni.i.d. observationalsamples
tion(9)holds),theboundbecomeslesstight. Ideally,the {(X ,T ,Y )}n . The number of iterations K. The
i i i i=1
radiusshouldsatisfyϵ∗ =D KL(PC||PT),whichensures initializationλ 0.
thattheboundholdsandistight. However,inobservational 1: Computeϵˆ∗ =D KL(Pˆ XC||Pˆ XT),withPˆ XCandPˆ XT being
data,calculatingD KL(PC||PT)isunattainableduetothe theempiricaldistributionsofP XC andP XT,respectively.
unseennatureofthetargetdistributionPC. 2: forj =1toJ do
Inthispaper,insteadofdirectlypursuingPC,wepresentan 3: fork =0toK−1do
intriguingalternativeapproachtoacquireD (PC||PT) 4: ComputeFˆ(λ k,ϵˆ∗;τˆ j)byequation(15a).
throughthefollowingProposition4.6. KL 5: Compute∂Fˆ(λ k,ϵˆ∗;τˆ j)/∂λ k byequation(15b).
Proposition 4.6. Let PT := P(X|T = 1) and PC := 6: Compute the update using λ k+1 = max{λ k −
X X Fˆ(λ ,ϵˆ∗;τˆ )/(∂Fˆ(λ ,ϵˆ∗;τˆ )/∂λ ),λ }.
P(X|T = 0) denote the distribution of covariates in the k j k j k 0
treatandcontrolgroup,respectively. Assumingthatrandom 7: SaveVˆ(τˆ j)[k]=Fˆ(λ k+1,ϵˆ∗;τˆ j).
variables(X,T,Y1,Y0)satisfyAssumption3.1,wehave 8: ReturnVˆ(τˆ j)=argmin k∈{0,...,K−1}Vˆ(τˆ j)[k].
D (PC||PT)=D (PC||PT). (14) 9: UseVˆ(τˆ j)tocomputeRDRM(τˆ j)byequation(16).
KL KL X X Output: τˆ =argmin RDRM(τˆ).
select τˆ∈{τˆ1,...,τˆJ}
TheproofisdeferredtoAppendixB.2.
Proposition 4.6 provides an important insight into the re- elsand8meta-learners. Specifically,thechosenbaseML
lationshipbetweentheuncomputabletermD (PC||PT) models are Linear Regression (LR), Support Vector Ma-
KL
andacomputablequantityD (PC||PT),wherePC and chine(SVM),RandomForests(RF),andNeuralNets(NN).
KL X X X
PT areempiricallyobservable. Asaresult,wecandeter- WeconsidertheseMLmodelsforCATEestimatorsbecause
X
minetheoptimalradiusbysettingϵ∗ =D (PC||PT). they are representative of both rigid and flexible models,
KL X X
witheachencodeddistinctinductivebiases,ashighlighted
Step3: FinalizingDistributionallyRobustMetric. We by(Curth&vanderSchaar,2021a;Curth&VanDerSchaar,
firstdefinetwofunctionsthatareusefulinobtainingV(τˆ):
2023). NotethatfortheLRmethod,weemployRidgere-
n gressionforregressiontasksandLogisticregressionforclas-
Fˆ(λ,ϵ;τˆ)=λϵ+λlog n1 (cid:88) T ieZ λi ; (15a) sificationtasks. Asfortheremainingmethods,weutilize
t
i=1 theircorrespondingregressorsandclassifiersforregression
∂Fˆ(λ,ϵ;τˆ) =ϵ+log(cid:88)n T ieZ λi
−
(cid:80)n i=1T iZ ieZ λi
. (15b)
andclassificationtasks,respectively. Regardingthemeta-
∂λ
i=1
n t λ(cid:80)n i=1T ieZ λi learners,weselectasetofbothtraditionalbasiclearners(S-,
T-,PS-,andIPW-learners)andrecentlydevelopedlearners
Here,Zdenotes(τˆ(X)−Y)2fornotationalsimplicity.Fur-
(X-,DR-,R-,andRA-learners),asdetailedinAppendixA.1.
thermore, weobtainϵˆ∗ byempiricallyapproximatingthe
We consider 13 CATE selectors, consisting of 8 plug-in
KLdivergenceD (PC||PT)usingtheNearest-Neighbor
KL X X methodsthatrelyontheabove8learners,3pseudo-outcome
algorithm(Wangetal.,2006;Nohetal.,2014). Wethenuse
methods (pseudo-DR, -R, and -IF), the random selection
theNewton-Raphsonmethodtofindtheempiricalsolution
strategy, and our proposed DRM. The specific details of
forVˆ(τˆ),exploitingtheconvexityofFˆ(λ,ϵ;τˆ)w.r.t. λ. Fi-
baselineselectorsarestatedinAppendixA.2. Weemploy
nally,aCATEestimatorcanbeselectedbasedonthePEHE
the eXtreme Gradient Boosting (XGB) method (Chen &
upperboundvalueinCorollary4.3,whichcorrespondsto
Guestrin,2016)astheunderlyingMLmodelforbothplug-
τˆ =argmin RDRM(τˆ)suchthat
select τˆ∈{τˆ1,...,τˆJ} inandpseudo-outcomemethods. WechooseXGBbecause:
(cid:118) i)itdemonstratessuperiorperformanceacrossvarioussce-
(cid:117) n
RDRM(τˆ)=(cid:117) (cid:116) n1 (cid:88) T i(τˆ(X i)−Y i)2+Vˆ(τˆ)n nc. (16) n sea lr eio cts oa rsn ;d iit )as tk hs e, ne en es dur ti ong ava og idoo pd otp ee nr tf io ar lm ca on nc ge eno if ab lia tyse bli in ae
s
i=1
thatmayarisefromusingthesimilarMLmodelsemployed
The complete algorithm of using the DRM method for
inCATEestimators(Curth&VanDerSchaar,2023);iii)
CATEestimatorselectionispresentedinAlgorithm1.
aligningwith(Alaa&VanDerSchaar,2019)whereXGB
is used for their proposed pseudo-IF metric. Following
5.Experiments (Curth&VanDerSchaar,2023),weadoptgridsearchfor
hyperparametertuningwhenevermodeltrainingisrequired.
5.1.ExpeirmentalSetup.
Estimators&Selectors. Weconsideratotalof32CATE Dataset. SincethegroundtruthofCATEisunavailable
estimators,comprisingthecombinationof4baseMLmod- inreal-worlddata,previousstudiescommonlyutilizesemi-
6SubmissionandFormattingInstructionsforICML2024
Table1.ComparisonofdifferentselectorsacrossSettingsA,B,andC.Reportedvalues(mean±standarddeviation)arecomputedover
√
100experiments.Bolddenotesthebestresultamongallselectors. PEHE&Regret:Smallerisbetter.Rankcorrelation:Largerisbetter.
√
Criterion PEHE(withRoracle(τˆbest)provided) Regret RankCorrelation
Setting A(0.24±0.04) B(15.49±1.84) C(16.10±2.49) A B C A B C
Plug-S 29.07±5.36 27.92±5.28 35.11±6.72 122.99±32.59 0.80±0.25 1.20±0.37 -0.14±0.17 0.75±0.07 0.71±0.04
Plug-PS 29.35±5.39 28.41±5.13 36.00±5.98 124.20±32.87 0.83±0.23 1.26±0.32 -0.17±0.17 0.74±0.07 0.70±0.04
Plug-T 13.64±4.00 25.76±5.21 17.73±5.09 57.17±19.76 0.67±0.29 0.11±0.33 0.79±0.11 0.81±0.06 0.85±0.04
Plug-X 18.82±9.45 25.02±6.21 31.56±8.53 79.78±44.55 0.61±0.35 0.98±0.51 0.57±0.19 0.84±0.07 0.77±0.05
Plug-IPW 29.12±3.49 61.01±12.28 63.48±17.82 123.27±27.76 2.93±0.54 2.96±1.00 0.16±0.16 0.32±0.10 0.40±0.12
Plug-DR 13.63±4.32 23.72±5.58 17.36±5.25 57.02±20.43 0.54±0.32 0.08±0.30 0.79±0.11 0.85±0.07 0.85±0.04
Plug-R 37.89±5.12 159.18±21.16 164.11±21.59 160.73±37.79 9.27±0.52 9.30±1.23 -0.54±0.14 -0.46±0.08 -0.51±0.05
Plug-RA 13.72±4.24 23.97±5.52 17.91±5.67 57.40±20.27 0.55±0.33 0.12±0.36 0.79±0.11 0.84±0.06 0.85±0.04
Pseudo-DR 12.57±2.99 21.91±4.05 16.80±3.14 52.53±15.66 0.42±0.24 0.05±0.18 0.81±0.09 0.88±0.05 0.86±0.03
Pseudo-R 7.29±11.76 32.20±8.25 42.99±8.61 30.97±53.76 1.09±0.52 1.70±0.53 0.73±0.27 0.61±0.13 0.64±0.12
Pseudo-IF 28.72±3.77 84.44±14.30 82.31±12.27 121.69±28.65 4.45±0.64 4.17±0.72 0.32±0.15 0.33±0.09 0.41±0.09
Random 1781±7173 487±3142 2603±11334 7195±29304 39.4±266.0 170.4±738.2 -0.01±0.10 0.15±0.09 0.09±0.08
DRM 8.48±5.99 19.69±6.47 16.62±3.03 35.01±26.22 0.27±0.39 0.04±0.15 0.81±0.08 0.87±0.06 0.81±0.07
√
synthetic datasets to compare model performance. In Asevidencedbyresultsregarding PEHEandRegretin
line with (Curth & van der Schaar, 2021a; Curth & Van Table1,ourproposedDRMmethoddemonstrateseffective
DerSchaar,2023),wecollectthecovariateswithn=4802 andstableperformance. Notably,inbothsettingsBandC,
data points from ACIC2016 dataset (Dorie et al., 2019). DRMoutperformsallotherselectors,achievingthesmall-
√
Then,wegenerate(X ,T ,Y )usingthefollowingDGP: est PEHE and Regret values. Specifically, in setting B,
i i i
DRM-selectedestimatorsexhibitsignificantlyloweraver-
T |X ∼Bern(σ(β′ X )),
i i T i ageRegretcomparedtootherselectors,whileinsettingC,
√
Y i =β Y′ X i+T iτ(X i)+ξ i, ξ i ∼N(0,1), the average PEHE of DRM-selected estimators closely
(17)
Linear:τ(X )=β′X ; matchesthatoftheoracleestimator. Althoughpseudo-R
i τ i
performsbetterthanDRMinsettingA,itdoesnotdemon-
Nonlinear:τ(X )=β′ exp(sgn(X )◦|X |γ/5).
i τ i i stratetheoverallstabilityexhibitedbyDRMinsettingsB
Here, Bern indicates the Bernoulli distribution, σ(z) = and C. It is worth mentioning that pseudo-DR, plug-DR,
1/(1 + exp(−z)) denotes the sigmoid function, sgn in- andplug-RAalsoshowcommendableperformance.
dicates the sign function, and ◦ represents the Hadamard
product. Thecoefficientvaluesaresetasfollows: β′ =1d, PartII:Robustnessexamination. Nowweanalyzethe
T
β′ ∼ U((0,1)d), and β′ ∼ U((0,10)d). The parameter capability of each selector in capturing robustness from
Y τ
γ represents the nonlinearity of the CATE function, with the following two aspects: (i) the ability to consistently
γ = 1 denoting slight nonlinearity and γ = 3 denoting select superior estimators; and (ii) the ability to mitigate
strong nonlinearity. This allows us to make comparisons the risk of selecting inferior estimators. In each of the
betweenDRMandbaselineselectorsunderthreedifferent 100experiments,wesortall32estimatorsinascendingor-
scenarios: linearity (Setting A), slight nonlinearity (Set- derbasedontheirRoracle(τˆ)values,yieldingasortedlist
tingB),andstrongnonlinearity(SettingC).Weadoptthe [Roracle(τˆ ),...,Roracle(τˆ )]. Wethendeterminetheac-
1 J
aboveDGPtogenerate100datasetsrandomly,eachwitha tualrankoftheselectedestimatorbasedonthislistforeach
training/validation/testingratioof35%/35%/30%. ofthe100experimentsandplotthedistributionofthese100
ranksinastackedbarchart(Figure1). Thefollowingob-
5.2.ExperimentalResults servationshighlightthesignificanceofrobustnessinCATE
estimatorselection.
Part I: PEHE comparison. The CATE estimator τˆ is
believedbetterifitachievessmallerRoracle(τˆ). Simulta- Firstly,ourDRMexcelsnotonlyinidentifyingthebestesti-
neously,abetterCATEestimatorτˆalsoimpliesasmaller matorsbutalsoinmitigatingtheriskofselectingpoorones.
difference between Roracle(τˆ) and Roracle(τˆ ), where Overall,DRMconsistentlyoutperformsotherselectorsin
best
τˆ is the actual best estimator defined before equation choosingthetop3estimatorsacrossallsettings. Although
best
(1). Consequently,weproposethefollowingtwocriteriato itmayhaveaslightdisadvantagecomparedtopseudo-Rin
compareestimatorschosenbydifferentselectors: settingA,noneoftheestimatorsselectedbyDRMperform
√ worsethanrank10.Similarobservationscanbemadeinset-
PEHE=Roracle(τˆ ),
select tingC.InsettingB,whilethereareinstanceswhereDRM
Roracle(τˆ )−Roracle(τˆ ) selects estimators ranked within 11-19, it still surpasses
Regret= select best .
Roracle(τˆ ) othermetricsinselectingthetop3estimators.
best
7SubmissionandFormattingInstructionsforICML2024
Figure1.Thestackedbarchartshowingthedistributionoftheselectedestimator’srankforeachevaluationmetricacrossrankintervals:
[1-3],[3-10],[11-19],[20-29],and[30-32].Thegreener(orredder)colorindicatesthattheselectedestimatorrankshigher(orlower).For
example,thedarkred(orgreen)indicatesthepercentageofcases(outof100experiments)wheretheselectedestimatorranksamongthe
worst3estimators,specificallyasranks30,31,or32.(oramongthebest3estimators,specificallyasranks1,2,or3).
Secondly,whentheceilingperformancesofdifferentselec- approachinfutureresearch.
torsarecomparable,theworst-caseperformancebecomesa
Wealsoconductsimilarcomparisonsofselectedestimators
dominantfactor. InsettingC,pseudo-DR,plug-RA,plug-
amongthetotalof8candidateestimators,withtheunderly-
DR, and DRM exhibit similarly strong performances in
ingMLmodelfixedasLR,SVM,RF,orNN.Therelevant
selecting the top 3 estimators. However, pseudo-DR and
√ resultsarereportedinAppendixCforfurtherreference.
DRMoutperformtheothertwometricsintermsof PEHE
and Regret, as all of their selected estimators are ranked
6.Conclusion
withinthetop10.
Thispapershedslightonthepotentialofrobustnessinse-
Thirdly,theconsistencyofgoodperformancesdependson
lectingCATEestimators. Weproposeanovelevaluation
robustness. InsettingA,thepseudo-Rselectoroftensuc-
method called the distributionally robust metric (DRM),
ceedsinchoosingthetop3estimators,resultinginlower
√
whichoffersseveraladvantagesoverexistingmetrics. No-
valuesof PEHEandRegret. However,itfailstomaintain
tably,ourapproachismodel-free,eliminatingtheneedfor
consistentlyexcellentperformanceinsettingsBandC.In
estimatingnuisanceparametersorplug-inlearners. More-
contrast, our DRM consistently demonstrates superiority
√
over, it is specifically designed to select a robust CATE
invariousaspectsinsettingsBandC,including PEHE,
estimator,ratherthanpursuea“stellar”one,whichispracti-
Regret,andtheabilitytoselectthebestestimators.
callymeaningful. Wealsoprovideafinitesampleanalysis
thatdemonstratesadecayrateofn−1/2forthegapbetween
PartIII:Rankingability. Toassesstherankingabilityof Vˆ(τˆ)andV(τˆ). Theexperimentalresultsconfirmtheeffi-
eachevaluationmetric,wecalculatetheRankCorrelation
cacyofourDRMapproachinCATEestimatorselection.
using Spearman rank correlation between the rank order
determined by the oracle metric Roracle(τˆ) and the rank Limitations.WemustacknowledgetheDRMmethodisnot
orderdeterminedbyeachevaluationmetric(e.g.,Rplug(τˆ), aone-size-fits-allsolutionandstillfacesseveralchallenges
τ˜
Rpseudo(τˆ),andRDRM(τˆ)). InTable1,ourDRMmethod that require further research and attention. For example,
Y˜
exploringalternativeapproachestoderiveatighterPEHE
consistentlyexhibitsgoodperformanceinrankingestima-
bound and enhancing the ranking ability of DRM can be
tors,outperformingothermetricsinsettingAwhileremain-
apromisingavenueforfutureinvestigation. Moreover,in-
ing comparable in settings B and C. However, DRM is
spired by the robust optimization literature (Hu & Hong,
relativelyinferiortopseudo-DR,whichachievesthebest
2013;MohajerinEsfahani&Kuhn,2018;Kuhnetal.,2019;
performanceinrankingestimatorsacrossallsettings. This
Levy et al., 2020; Wang et al., 2021), considering diver-
disparitymaystemfromDRMselectingestimatorsbased
gences other than KL for DRM would be an interesting
ontheirworst-caseperformance. Whileworst-caseperfor-
nextstep. WhileourDRMmethodrevealsthepotentialof
mancecanserveasagoodbenchmarkforselectingplayers
robustnessinselectingCATEestimators,itisimportantto
toattendOlympics,itmightnotbeareasonablestandardto
notethatthepurposeofthispaperisnottoclaimthatwe
rankplayers’overallperformance. Thisisbecauseranking
havediscoveredasilverbulletforCATEestimatorselection.
inherentlyinvolvestheconceptofexpected(average)per-
Ourmainintentistoprovidevaluableinsightsforpractical
formance,whichisdeterminedbyneitherthebestnorthe
decision-makersandstimulateincreasingattentiontoward
worstperformancealone. Itwouldbeintriguingtoexplore
thetopicofmodelselectionincausalinference.
methodsthatcanenhancetherankingabilityofourDRM
8SubmissionandFormattingInstructionsforICML2024
References Chu, Z., Rathbun, S. L., and Li, S. Graph infomax ad-
versarial learning for treatment effect estimation with
Abadie,A.,Athey,S.,Imbens,G.W.,andWooldridge,J.M.
networkedobservationaldata. InProceedingsofthe27th
Whenshouldyouadjuststandarderrorsforclustering?
ACMSIGKDDConferenceonKnowledgeDiscovery&
TheQuarterlyJournalofEconomics,138(1):1–35,2023.
DataMining,pp.176–184,2021.
Advani,A.,Kitagawa,T.,andSłoczyn´ski,T. Mostlyharm-
Curth, A. and van der Schaar, M. On inductive biases
lesssimulations? usingmontecarlostudiesforestimator
forheterogeneoustreatmenteffectestimation. Advances
selection. JournalofAppliedEconometrics,34(6):893–
in Neural Information Processing Systems, 34:15883–
910,2019.
15894,2021a.
Alaa,A.andVanDerSchaar,M. Validatingcausalinfer-
Curth, A.andvanderSchaar, M. Nonparametricestima-
ence models via influence functions. In International
tionofheterogeneoustreatmenteffects: Fromtheoryto
ConferenceonMachineLearning,pp.191–201.PMLR,
learningalgorithms. InInternationalConferenceonArti-
2019.
ficialIntelligenceandStatistics,pp.1810–1818.PMLR,
2021b.
Assaad,S.,Zeng,S.,Tao,C.,Datta,S.,Mehta,N.,Henao,
R., Li, F., and Carin, L. Counterfactual representation
Curth, A. and Van Der Schaar, M. In search of insights,
learningwithbalancingweights. InInternationalCon-
notmagicbullets: Towardsdemystificationofthemodel
ferenceonArtificialIntelligenceandStatistics,pp.1972–
selectiondilemmainheterogeneoustreatmenteffectesti-
1980.PMLR,2021.
mation. InProceedingsofthe40thInternationalConfer-
enceonMachineLearning,volume202ofProceedings
ATHEY,S.,TIBSHIRANI,J.,andWAGER,S. Generalized
ofMachineLearningResearch,pp.6623–6642.PMLR,
random forests. The Annals of Statistics, 47(2):1148–
23–29Jul2023.
1178,2019.
Curth,A.,Svensson,D.,Weatherall,J.,andvanderSchaar,
Athey,S.,Imbens,G.W.,Metzger,J.,andMunro,E. Using
M.Reallydoinggreatatestimatingcate?acriticallookat
wassersteingenerativeadversarialnetworksforthedesign
mlbenchmarkingpracticesintreatmenteffectestimation.
of monte carlo simulations. Journal of Econometrics,
InThirty-fifthconferenceonneuralinformationprocess-
2021.
ing systems datasets and benchmarks track (round 2),
2021.
Bica,I.andvanderSchaar,M. Transferlearningonhetero-
geneousfeaturespacesfortreatmenteffectsestimation.
Donnelly,R.,Blei,D.,Athey,S.,etal. Correctionto: Coun-
AdvancesinNeuralInformationProcessingSystems,35:
terfactual inference for consumer choice across many
37184–37198,2022.
product categories. Quantitative Marketing and Eco-
nomics,19(3-4):409–409,2021.
Bica, I., Alaa, A. M., Lambert, C., and Van Der Schaar,
M. Fromreal-worldpatientdatatoindividualizedtreat-
Dorie, V., Hill, J., Shalit, U., Scott, M., and Cervone, D.
menteffectsusingmachinelearning: currentandfuture
Automatedversusdo-it-yourselfmethodsforcausalinfer-
methodstoaddressunderlyingchallenges. ClinicalPhar-
ence: Lessonslearnedfromadataanalysiscompetition.
macology&Therapeutics,109(1):87–100,2021.
StatisticalScience,34(1):43–68,2019.
Bottou,L.,Peters,J.,Quin˜onero-Candela,J.,Charles,D.X.,
Farrell,M.H. Robustinferenceonaveragetreatmenteffects
Chickering,D.M.,Portugaly,E.,Ray,D.,Simard,P.,and
withpossiblymorecovariatesthanobservations. Journal
Snelson,E. Counterfactualreasoningandlearningsys-
ofEconometrics,189(1):1–23,2015.
tems:Theexampleofcomputationaladvertising. Journal
ofMachineLearningResearch,14(11),2013. Ferna´ndez-Lor´ıa,C.,Provost,F.,Anderton,J.,Carterette,B.,
andChandar,P. Acomparisonofmethodsfortreatment
Chen,T.andGuestrin,C.Xgboost:Ascalabletreeboosting
assignmentwithanapplicationtoplaylistgeneration. In-
system. In Proceedings of the 22nd acm sigkdd inter-
formationSystemsResearch,34(2):786–803,2023.
national conference on knowledge discovery and data
mining,pp.785–794,2016. Foster,D.J.andSyrgkanis,V. Orthogonalstatisticallearn-
ing. TheAnnalsofStatistics,51(3):879–908,2023.
Chernozhukov,V.,Chetverikov,D.,Demirer,M.,Duflo,E.,
Hansen,C.,Newey,W.,andRobins,J. Double/debiased Foster,J.C.,Taylor,J.M.,andRuberg,S.J. Subgroupiden-
machinelearningfortreatmentandstructuralparameters, tificationfromrandomizedclinicaltrialdata. Statisticsin
2018. medicine,30(24):2867–2880,2011.
9SubmissionandFormattingInstructionsforICML2024
Guo,R.,Cheng,L.,Li,J.,Hahn,P.R.,andLiu,H. Asurvey Kinyanjui, N. M. and Johansson, F. D. Adcb: An
oflearningcausalitywithdata: Problemsandmethods. alzheimer’s disease simulator for benchmarking obser-
ACMComputingSurveys(CSUR),53(4):1–37,2020. vationalestimatorsofcausaleffects. InConferenceon
Health, Inference, and Learning, pp. 103–118. PMLR,
Guo,R.,Li,J.,Li,Y.,Candan,K.S.,Raglin,A.,andLiu, 2022.
H. Ignite: A minimax game toward learning individ-
ualtreatmenteffectsfromnetworkedobservationaldata. Kitagawa,T.andTetenov,A.Whoshouldbetreated?empir-
In Proceedingsofthe Twenty-NinthInternationalCon- icalwelfaremaximizationmethodsfortreatmentchoice.
ferenceonInternationalJointConferencesonArtificial Econometrica,86(2):591–616,2018.
Intelligence,pp.4534–4540,2021.
Kuang,K.,Cui,P.,Li,B.,Jiang,M.,Yang,S.,andWang,
F. Treatmenteffectestimationwithdata-drivenvariable
Hahn,P.R.,Murray,J.S.,andCarvalho,C.M. Bayesianre-
decomposition. InProceedingsoftheAAAIConference
gressiontreemodelsforcausalinference: Regularization,
onArtificialIntelligence,volume31,2017.
confounding,andheterogeneouseffects(withdiscussion).
BayesianAnalysis,15(3):965–1056,2020. Kuang,K.,Cui,P.,Zou,H.,Li,B.,Tao,J.,Wu,F.,andYang,
S. Data-drivenvariabledecompositionfortreatmentef-
Hassanpour,N.andGreiner,R.Learningdisentangledrepre-
fectestimation. IEEETransactionsonKnowledgeand
sentationsforcounterfactualregression. InInternational
DataEngineering,34(5):2120–2134,2020.
ConferenceonLearningRepresentations,2019.
Kuhn,D.,Esfahani,P.M.,Nguyen,V.A.,andShafieezadeh-
Hill,J.L.Bayesiannonparametricmodelingforcausalinfer-
Abadeh,S. Wassersteindistributionallyrobustoptimiza-
ence. JournalofComputationalandGraphicalStatistics,
tion: Theory and applications in machine learning. In
20(1):217–240,2011.
Operationsresearch&managementscienceintheageof
analytics,pp.130–166.Informs,2019.
Holland,P.W. Statisticsandcausalinference. Journalof
theAmericanstatisticalAssociation,81(396):945–960, Ku¨nzel,S.R.,Sekhon,J.S.,Bickel,P.J.,andYu,B. Met-
1986. alearnersforestimatingheterogeneoustreatmenteffects
using machine learning. Proceedings of the national
Hu, Z. and Hong, L. J. Kullback-leibler divergence con-
academyofsciences,116(10):4156–4165,2019.
straineddistributionallyrobustoptimization. Availableat
OptimizationOnline,1(2):9,2013. Levy,D.,Carmon,Y.,Duchi,J.C.,andSidford,A. Large-
scale methods for distributionally robust optimization.
Huang,Y.,Leung,C.H.,Yan,X.,Wu,Q.,Peng,N.,Wang, AdvancesinNeuralInformationProcessingSystems,33:
D., and Huang, Z. The causal learning of retail delin- 8847–8860,2020.
quency. InProceedingsoftheAAAIConferenceonArtifi-
Li,S.andWager,S. Randomgraphasymptoticsfortreat-
cialIntelligence,volume35,pp.204–212,2021.
menteffectestimationundernetworkinterference. The
Huang,Y.,Leung,C.H.,Ma,S.,Yuan,Z.,Wu,Q.,Wang,S., AnnalsofStatistics,50(4):2334–2358,2022.
Wang,D.,andHuang,Z. Towardsbalancedrepresenta-
Louizos, C., Shalit, U., Mooij, J. M., Sontag, D., Zemel,
tionlearningforcreditpolicyevaluation. InInternational
R., andWelling, M. Causaleffectinferencewithdeep
Conference on Artificial Intelligence and Statistics, pp.
latent-variablemodels. Advancesinneuralinformation
3677–3692.PMLR,2023.
processingsystems,30,2017.
Johansson, F., Shalit, U., andSontag, D. Learningrepre-
Ma,S.,Leung,C.H.,Wu,Q.,Liu,W.,andPeng,N. Under-
sentationsforcounterfactualinference. InInternational
standingdistributionalambiguityvianon-robustchance
conferenceonmachinelearning,pp.3020–3029.PMLR,
constraint. InProceedingsoftheFirstACMInternational
2016.
ConferenceonAIinFinance,pp.1–8,2020.
Johansson, F. D., Shalit, U., Kallus, N., and Sontag, D. Mahajan, D., Mitliagkas, I., Neal, B., and Syrgkanis, V.
Generalization bounds and representation learning for Empiricalanalysisofmodelselectionforheterogenous
estimationofpotentialoutcomesandcausaleffects. The causal effect estimation. International Conference on
JournalofMachineLearningResearch,23(1):7489–7538, LearningRepresentations,2024.
2022.
MohajerinEsfahani,P.andKuhn,D. Data-drivendistribu-
Kennedy,E.H. Towardsoptimaldoublyrobustestimation tionally robust optimization using the wasserstein met-
of heterogeneous causal effects. Electronic Journal of ric: performanceguaranteesandtractablereformulations.
Statistics,17(2):3008–3049,2023. MathematicalProgramming,171(1-2):115–166,2018.
10SubmissionandFormattingInstructionsforICML2024
Nie, X.andWager, S. Quasi-oracleestimationofhetero- Shi,C.,Blei,D.,andVeitch,V. Adaptingneuralnetworks
geneoustreatmenteffects. Biometrika,108(2):299–319, fortheestimationoftreatmenteffects.Advancesinneural
2021. informationprocessingsystems,32,2019.
Nogueira,A.R.,Pugnana,A.,Ruggieri,S.,Pedreschi,D., Wager,S.andAthey,S. Estimationandinferenceofhetero-
andGama,J. Methodsandtoolsforcausaldiscoveryand geneoustreatmenteffectsusingrandomforests. Journal
causalinference. Wileyinterdisciplinaryreviews: data oftheAmericanStatisticalAssociation,113(523):1228–
miningandknowledgediscovery,12(2):e1449,2022. 1242,2018.
Wang, J., Gao, R., and Xie, Y. Sinkhorn distributionally
Noh, Y.-K., Sugiyama, M., Liu, S., Plessis, M. C., Park,
robustoptimization. arXivpreprintarXiv:2109.11926,
F. C., and Lee, D. D. Bias reduction and metric learn-
2021.
ing for nearest-neighbor estimation of kullback-leibler
divergence. InArtificialIntelligenceandStatistics, pp.
Wang,Q.,Kulkarni,S.R.,andVerdu´,S.Anearest-neighbor
669–677.PMLR,2014.
approachtoestimatingdivergencebetweencontinuous
randomvectors. In2006IEEEInternationalSymposium
Oprescu, M., Syrgkanis, V., and Wu, Z. S. Orthogonal
onInformationTheory,pp.242–246.IEEE,2006.
randomforestforcausalinference. InInternationalCon-
ference on Machine Learning, pp. 4932–4941. PMLR, Wu, A., Yuan, J., Kuang, K., Li, B., Wu, R., Zhu, Q.,
2019. Zhuang,Y.,andWu,F. Learningdecomposedrepresenta-
tionsfortreatmenteffectestimation. IEEETransactions
Parikh,H.,Varjao,C.,Xu,L.,andTchetgen,E.T.Validating onKnowledgeandDataEngineering,35(5):4989–5001,
causalinferencemethods. InInternationalConference
2022.
onMachineLearning,pp.17346–17358.PMLR,2022.
Yao, L., Li, S., Li, Y., Huai, M., Gao, J., and Zhang, A.
Pflug,G.C. Multistagestochasticdecisionproblems: Ap- Representationlearningfortreatmenteffectestimation
proximationbyrecursivestructuresandambiguitymod- fromobservationaldata. Advancesinneuralinformation
eling. EuropeanJournalofOperationalResearch,306 processingsystems,31,2018.
(3):1027–1039,2023.
Yao, L., Chu, Z., Li, S., Li, Y., Gao, J., and Zhang, A.
Qian,Z.,Zhang,Y.,Bica,I.,Wood,A.,andvanderSchaar, A survey on causal inference. ACM Transactions on
M. Synctwin: Treatmenteffectestimationwithlongitudi- Knowledge Discovery from Data (TKDD), 15(5):1–46,
naloutcomes. AdvancesinNeuralInformationProcess- 2021.
ingSystems,34:3178–3190,2021.
Yoon,J.,Jordon,J.,andVanDerSchaar,M.Ganite:Estima-
tionofindividualizedtreatmenteffectsusinggenerative
Rosenbaum, P. R. and Rubin, D. B. The central role of
adversarialnets. InInternationalconferenceonlearning
thepropensityscoreinobservationalstudiesforcausal
representations,2018.
effects. Biometrika,70(1):41–55,1983.
Zhang,L.,Wang,Y.,Ostropolets,A.,Mulgrave,J.J.,Blei,
Rubin, D. B. Causal inference using potential outcomes:
D. M., and Hripcsak, G. The medical deconfounder:
Design, modeling, decisions. JournaloftheAmerican
assessingtreatmenteffectswithelectronichealthrecords.
StatisticalAssociation,100(469):322–331,2005.
InMachineLearningforHealthcareConference,pp.490–
512.PMLR,2019.
Schuler, A., Jung, K., Tibshirani, R., Hastie, T., and
Shah, N. Synth-validation: Selecting the best causal
Zhang,Y.,Bellot,A.,andSchaar,M. Learningoverlapping
inference method for a given dataset. arXiv preprint
representationsfortheestimationofindividualizedtreat-
arXiv:1711.00083,2017.
menteffects. InInternationalConferenceonArtificial
IntelligenceandStatistics,pp.1005–1014.PMLR,2020.
Schuler, A., Baiocchi, M., Tibshirani, R., and Shah, N.
Acomparisonofmethodsformodelselectionwhenes-
timating individual treatment effects. arXiv preprint
arXiv:1804.05146,2018.
Shalit, U., Johansson, F. D., and Sontag, D. Estimating
individual treatment effect: generalization bounds and
algorithms.InInternationalconferenceonmachinelearn-
ing,pp.3076–3085.PMLR,2017.
11SubmissionandFormattingInstructionsforICML2024
A.CATEEstimationStrategies
A.1.CATELearners
WenowdetailhowtoconstructCATElearnersusingtheobservedsamples{(X ,T ,Y )}n . NotethatCATElearnersare
i i i i=1
learnedonthetrainingset,sothesamplesizenhereequalsthetrainingsamplesize. Denoten bythesamplesizeinthe
t
treatgroup,andn bythesamplesizeinthecontrolgroupsuchthatn=n +n .
c t c
• S-learner: Letpredictors=(X,T),response=Y. Trainamodelµˆ(X,T). Thenweobtainτˆ (X):
S
τˆ (X)=µˆ(X,1)−µˆ(X,0).
S
• T-learner: Letpredictors=XT (covariatesinthetreat),response=YT (outcomeinthetreat). Trainamodelµˆ (X). Let
1
predictors=XC (covariatesinthecontrol),response=YC (outcomeinthecontrol). Trainamodelµˆ (X). Thenwe
0
obtainτˆ (X):
T
τˆ (X)=µˆ (X)−µˆ (X).
T 1 0
• PS-learner: Fisrt-step: Trainτˆ (X)usingtheabove-mentionedstepinS-learner. Second-step: Letpredictors=X,
S
response=τˆ (X). Trainamodelτˆ (X)fromthefollowingobjective:
S PS
n
1 (cid:88)
τˆ =argmin (τ(X )−τˆ (X ))2.
PS n i S i
τ
i=1
• IPW-learner: First-step: letpredictors=X,response=T. Trainapropensityscoremodelπˆ(X). Constructsurrogateof
CATEusingpseudo-outcomeswithinversepropensityweighting(IPW)formula: Y1,0 = Y1 −Y0 ,where
IPW IPW IPW
Y1 = TY andY0 = (1−T)Y. Trainamodelτˆ (X)fromthefollowingobjective:
IPW πˆ(X) IPW 1−πˆ(X) IPW
n
1 (cid:88)
τˆ =argmin (τ(X )−Y1,0 )2.
IPW n i i,IPW
τ
i=1
• X-learner (Ku¨nzel et al., 2019): First-step: Train µˆ (X) and µˆ (X) using the the above-mentioned procedure in
1 0
T-learner. Trainapropensityscoremodelπˆ(X)usingthetheabove-mentionedprocedureinIPW-learner. Second-step:
Letpredictors=XT,response=µˆ (XT)−YT,andpredictors=XC,response=µˆ (XC)−YC. Obtainamodelτˆ (X)
1 0 X
bylearningtwoseparatefunctionsτˆ1(X)andτˆ0(X):
X X
τˆ (X)=(1−πˆ(X))τˆ1(X)+πˆ(X)τˆ0(X),
X X X
1
(cid:88)nt
τˆ1 =argmin (τ(X )−(Y −µˆ (X )))2,
X n i i 0 i
τ t
i=1
1
(cid:88)nc
τˆ0 =argmin (τ(X )−(µˆ (X )−Y ))2.
X n i 1 i i
τ c
i=1
• DR-learner (Kennedy, 2023; Foster & Syrgkanis, 2023): First-step: Train µˆ (X) and µˆ (X) using the the above-
1 0
mentionedprocedureinT-learner. Trainapropensityscoremodelπˆ(X)usingthetheabove-mentionedprocedurein
IPW-learner. Second-step: ConstructsurrogateofCATEusingpseudo-outcomeswithdoublyrobust(DR)formula:
Y1,0 =Y1 −Y0 ,whereY1 =µˆ (X)+ T (Y −µˆ (X))andY0 =µˆ (X)+ 1−T (Y −µˆ (X)). Train
DR DR DR DR 1 πˆ(X) 1 DR 0 1−πˆ(X) 0
amodelτˆ (X)fromthefollowingobjective:
DR
n
1 (cid:88)
τˆ =argmin (τ(X )−Y1,0 )2.
DR n i i,DR
τ
i=1
12SubmissionandFormattingInstructionsforICML2024
• R-learner(Nie&Wager,2021): First-step: Letpredictors=X,response=Y. Trainamodelµˆ(X)toapproximatethe
conditionalmeanoutcomeE[Y|X]. Trainapropensityscoremodelπˆ(X)usingthetheabove-mentionedprocedure
inIPW-learner. Second-step: Computetheoutcomeresidualξ =Y −µˆ(X)andtreatmentresidualν =T −πˆ(X).
Trainamodelτˆ (X)fromthefollowingobjective:
R
n
1 (cid:88)
τˆ =argmin (ξ −ν τ(X ))2.
R n i i i
τ
i=1
• RA-learner (Curth & van der Schaar, 2021b): First-step: Train µˆ (X) and µˆ (X) using the the above-mentioned
1 0
procedureinT-learner. Second-step: ConstructsurrogateofCATEusingpseudo-outcomeswithregressionadjustment
(RA)formula: Y =T(Y −µˆ (X))+(1−T)(µˆ (X)−Y). Trainamodelτˆ (X)fromthefollowingobjective:
RA 0 1 RA
n
1 (cid:88)
τˆ =argmin (τ(X )−Y )2.
RA n i i,RA
τ
i=1
A.2.CATESelectors
WenowdetailhowtoconstructCATEselectorsusingtheobservedsamples{(X ,T ,Y )}n . NotethatCATEselectors
i i i i=1
areconstructedonthevalidationset,sothesamplesizenhereequalsthevalidationsamplesize.
• Plug-inselector: ObtainanyCATElearnersτ˜usingtheobservationalvalidationdata. Thenplug-inτ˜intothefollowing
metricRplug(τˆ):
τ˜
(cid:118)
(cid:117) n
Rp τ˜lug(τˆ)=(cid:117) (cid:116) n1 (cid:88) (τˆ(X i)−τ˜(X i))2.
i=1
Foreachplug-inselectorτ˜,theselectedj∗-thCATEestimatorisτˆ ,wherej∗ =argmin Rplug(τˆ ).
j∗ j∈{1,...,J} τ˜ j
• Pseudo-outcomeselector:
1. Pseudo-DR:Utilizevalidationdatatoestimatenuisanceparameters(µ˜ ,µ˜ ,π˜),followingtheproceduredescribed
1 0
in Section A.1. Y˜ = Y˜1 − Y˜0 , where Y˜1 = µ˜ (X) + T (Y − µ˜ (X)) and Y˜0 = µ˜ (X) +
DR DR DR DR 1 π˜(X) 1 DR 0
1−T (Y −µ˜ (X)). Thenthepseudo-DRmetricis
1−π˜(X) 0
(cid:118)
(cid:117) n
Rp Ds Reudo(τˆ)=(cid:117) (cid:116) n1 (cid:88) (τˆ(X i)−Y˜ i,DR)2.
i=1
Forpseudo-DRselector,theselectedj∗-thCATEestimatorisτˆ ,wherej∗ =argmin Rpseudo(τˆ ).
j∗ j∈{1,...,J} DR j
2. Pseudo-R:Utilizevalidationdatatoestimatenuisanceparameters(µ˜,π˜),followingtheproceduredescribedin
SectionA.1. Thenthepseudo-Rmetricis
(cid:118)
(cid:117) n
Rp Rseudo(τˆ)=(cid:117) (cid:116) n1 (cid:88) ((Y i−µ˜(X i))−τˆ(X i)(T i−π˜(X i)))2.
i=1
Forpseudo-Rselector,theselectedj∗-thCATEestimatorisτˆ ,wherej∗ =argmin Rpseudo(τˆ ).
j∗ j∈{1,...,J} R j
3. Pseudo-IF(Alaa&VanDerSchaar,2019): Utilizevalidationdatatoestimatenuisanceparameters(µ˜ ,µ˜ ,π˜),
1 0
followingtheproceduredescribedinSectionA.1. Letτ˜(X)=(µ˜ (X)−µ˜ (X)). Thenthepseudo-IFmetricis
1 0
(cid:118)
(cid:117) n
Rp IFseudo(τˆ)=(cid:117) (cid:116) n1 (cid:88) (1−B i)τ˜2(X i)+B iY i(τ˜(X i)−τˆ(X i))−A i(τ˜(X i)−τˆ(X i))2+τˆ2(X i),
i=1
whereA =T −π˜(X ), B =2T (T −π˜(X ))C−1, C =π˜(X )(1−π˜(X )).
i i i i i i i i i i i
Forpseudo-IFselector,theselectedj∗-thCATEestimatorisτˆ ,wherej∗ =argmin Rpseudo(τˆ ).
j∗ j∈{1,...,J} IF j
13SubmissionandFormattingInstructionsforICML2024
4. Otherpseudo-outcomeselector: BymanipulatingtheformulaofY˜,itispossibletocreateadditionalpseudo-
outcomeselectors,suchasthepseudo-IPWselector. Inourpaper,wechoosepseudo-DRasthebaselinebecause
itisrepresentativeinthecausalinferenceliteratureanditoftendemonstratessuperiorperformance,owingtoits
doublyrobustproperty.
A.3.SupplementaryforthePEHEUpperBoundofRepresentationBalancingMethods
LetΦ:X →Rdenotetherepresentationfunction,h:R×{0,1}→Y denotetheoutcomefunctionsuchthath(Φ(X),t)
estimates Yt for t ∈ {0,1}, Dist(PT,PC) denote the distributional distance (e.g., MMD distance) between treat and
Φ Φ
controlrepresentations,andCATEisestimatedbyτˆ(X)=h(Φ(X),1)−h(Φ(X),0). Accordingto(Shalitetal.,2017;
Johanssonetal.,2022),thePEHEw.r.t. (h,Φ)canbeupperboundedasfollows:
ϵ (h,Φ)≤2(cid:0) ϵT=1(h,Φ)+ϵT=0(h,Φ)+B ·Dist(PT,PC)−2σ2(cid:1) . (18)
PEHE F F Φ Φ Φ Y
Here,ϵ (h,Φ)=E[(h(Φ(X),1)−h(Φ(X),0)−τ (X))2]isthesameasR(h(Φ(·),1)−h(Φ(·),0))inequation
PEHE true
(1). ϵT=t(h,Φ) = E[(h(Φ(X),t) − Y)2|T = t] denotes the factual outcome estimation error in the treat (t = 1)
F
or control (t = 0) group. Dist(PT,PC) denotes the distributional distance (e.g., MMD distance) between treat and
Φ Φ
control groups in the representation space. B ≥ 0 is a constant such that for t ∈ {0,1}, the function g (r,t) :=
Φ Φ,h
1 ·ℓ (Ψ(r),t)∈G,whereΨistheinverseofΦandG isthefamilyof1-Lipschitzfunctions. σ2isaconstantsuchthat
σB 2Φ = h m,Φ in{σ2 (p(x,t)),σ2 (p(x,1−t))},whereσ2 (p(x,t)) = (cid:82) (yt −τt(x))2p(yt|xy )p(x,t)dytdxdt. This
y yt yt yt X×{0,1}×Y
boundoffersaguidingprincipleforrepresentationbalancingmethodsmentionedinSection2: Duetotheunavailability
ofthetruePEHEϵ (h,Φ),theobjectivefunctionofarepresentationbalancingmodelshouldaimtominimizethe
PEHE
PEHEupperboundthatinvolvestheerrorinfactualoutcomesandthedistributionaldistancebetweenthetreatandcontrol
representations.
B.Proofs
B.1.ProofofProposition4.1
Proof.
E[(τˆ(X)−τ (X))2]
true
=E[(τˆ(X)−(µ (X)−µ (X)))2]
1 0
=E[(τˆ(X)−µ (X)+µ (X))2]
1 0
≤2(cid:2)E[(τˆ(X)−µ (X))2]+E[(µ (X))2](cid:3)
1 0
=2(cid:2)E[(τˆ(X)−Y1+Y1−µ (X))2]+E[(µ (X))2](cid:3)
1 0
=2(cid:2)E[(τˆ(X)−Y1)2]+E[(Y1−µ (X))2]+2E[(τˆ(X)−Y1)(Y1−µ (X))]+E[(µ (X))2](cid:3)
1 1 0
=2(cid:2)E[(τˆ(X)−Y1)2]+E[(Y1−µ (X))2]+2E[τˆ(X)Y1−τˆ(X)µ (X)−(Y1)2+Y1µ (X)]+E[(µ (X))2](cid:3)
1 1 1 0
=2(cid:2)E[(τˆ(X)−Y1)2]+E[(Y1−µ (X))2]+2E[E[τˆ(X)Y1−τˆ(X)µ (X)−(Y1)2+Y1µ (X)|X]]+E[(µ (X))2](cid:3)
1 1 1 0
=2(cid:2)E[(τˆ(X)−Y1)2]+E[(Y1−µ (X))2]+2E[τˆ(X)E[Y1|X]−τˆ(X)µ (X)−E[(Y1)2|X]+E[Y1|X]µ (X)]+E[(µ (X))2](cid:3)
1 1 1 0
=2(cid:2)E[(τˆ(X)−Y1)2]+E[(Y1−µ (X))2]+2E[τˆ(X)µ (X)−τˆ(X)µ (X)−E[(Y1)2|X]+µ (X)µ (X)]+E[(µ (X))2](cid:3)
1 1 1 1 1 0
=2(cid:2)E[(τˆ(X)−Y1)2]+E[(Y1−µ (X))2]+2E[(µ (X))2]−2E[(Y1)2]+E[(µ (X))2](cid:3)
1 1 0
=2(cid:2)E[(τˆ(X)−Y1)2]+E[(Y1)2]+E[(µ (X))2]−2E[Y1µ (X)]+2E[(µ (X))2]−2E[(Y1)2]+E[(µ (X))2](cid:3)
1 1 1 0
=2(cid:2)E[(τˆ(X)−Y1)2]+E[(Y1)2]+E[(µ (X))2]−2E[µ (X)E[Y1|X]]+2E[(µ (X))2]−2E[(Y1)2]+E[(µ (X))2](cid:3)
1 1 1 0
=2(cid:2)E[(τˆ(X)−Y1)2]+E[(µ (X))2]−E[(Y1)2]+E[(µ (X))2](cid:3)
1 0
=2(cid:2)E[(τˆ(X)−Y1)2]+V[µ (X)]+(E[µ (X)])2−V[Y1]−(E[Y1])2+V[µ (X)]+(E[µ (X)])2(cid:3)
1 1 0 0
=2(cid:2)E[(τˆ(X)−Y1)2]+V[µ (X)]−V[Y1]+V[µ (X)]+(E[µ (X)])2(cid:3)
1 0 0
=2(cid:2)E[(τˆ(X)−Y1)2]+ζ(cid:3)
.
14SubmissionandFormattingInstructionsforICML2024
B.2.ProofofProposition4.6
WefirstproposethefollowingPropositionthatisusefulinprovingProposition4.6.
PropositionB.1. Assumetherandomvariabletuple(X,T,Y1,Y0)satisfiesAssumption3.1. Foranyt∈{0,1},wehave
P(X,Y1|T =t)=P(Y1|X)P(X|T =t). (19)
Proof.
P(X,Y1|T =t)
=P(Y1|X,T =t)P(X|T =t)
=P(Y1|X)P(X|T =t). (Unconfoundedness)
NowwecanproveProposition4.6.
Proof.
D (P(X,Y1|T =0)||P(X,Y1|T =1))
KL
(cid:90) (cid:90) p(x,y1|T =0)
= p(x,y1|T =0)log dy1dx
p(x,y1|T =1)
X Y1
(cid:90) (cid:90) p(y1|x)p(x|T =0)
= p(y1|x)p(x|T =0)log dy1dx (ByPropositionB.1)
p(y1|x)p(x|T =1)
X Y1
(cid:90) (cid:90) p(x|T =0)
= p(y1|x)p(x|T =0)log dy1dx
p(x|T =1)
X Y1
(cid:90) (cid:18)(cid:90) (cid:19) p(x|T =0)
= p(y1|x)dy1 p(x|T =0)log dx
p(x|T =1)
X Y1
(cid:90) p(x|T =0)
= p(x|T =0)log dx
p(x|T =1)
X
=D (P(X|T =0)||P(X|T =1)).
KL
B.3.ProofofTheorem4.4
Wefirstrecalltheresultsin(Hu&Hong,2013).
LemmaB.2(Theorem1in(Hu&Hong,2013)). Letf (x)denotethelossfunctionofxanditisboundedalmostsurely.
θ
θ ∈Θrepresentsthemodelparametersofthefunctionf (x). LetB (P)betheuncertaintyballcenteredatdistributionP
θ ϵ
withambiguityradiusϵ. DefineκasthemassofthedistributionP onitsessentialsupremum(Proposition2in(Hu&Hong,
2013)). Assumef (x)isboundedandlogκ+ϵ<0,thenwehave
θ
V := sup EQ[f (x)]=minλϵ+λlogEP[exp(f (x)/λ)].
θ θ
Q∈Bϵ(P) λ>0
OurTheorem4.4followsbydirectlyapplyingtheaboveLemmaB.2.
B.4.ProofofTheorem4.5
Fornotationalsimplicity,wedenoteZ =(τˆ(X)−Y)2,W =(Z,T)∈W,andg(λ;W)=T exp(Z/λ). Then,wedefine
thefollowingtwofunctions:
G(λ;W)=E[g(λ;W)]; (20)
n
Gˆ(λ;W)= 1 (cid:88) g(λ;W ). (21)
n i
i=1
ThenwehavethefollowinglemmathatguaranteestheconvergenceforGˆ(λ;W).
15SubmissionandFormattingInstructionsforICML2024
LemmaB.3. Assumingthat0<λ≤λ≤λ¯and(τˆ(X)−Y)2isupperboundedbyM¯. Thenwithprobability1−δ,we
¯
have
(cid:115) 
2log2(cid:0) exp(cid:0) M¯/λ(cid:1)(cid:1)2
|Gˆ(λ;W)−G(λ;W)|≤O δ ¯ .
n
Proof. Denoteh(W ,W ,...,W )= 1 (cid:80)n g(λ;W ). First,wenoticethath(W ,W ,...,W )satisfiesthebounded
1 2 n n i=1 i 1 2 n
differenceinequality:
sup |h(W ,...,W ,··· ,W )−h(W ,...,W′,··· ,W )|
1 i n 1 i n
W1,...,Wn,W i′∈W
|g(λ;W )−g(λ;W′)|
= sup i i
n (22)
Wi,W i′∈W
|g(λ;W )|
2exp(cid:0) M¯/λ(cid:1)
≤2 sup i ≤ .
n n
Wi∈W
Notethat|Gˆ(λ;W)−G(λ;W)|=|h(W ,W ,...,W )−E[h(W ,W ,...,W )]|. ThenusingMcDiarmid’sinequality,
1 2 n 1 2 n
foranyϵ>0,wehave
(cid:16)(cid:12) (cid:12) (cid:17)
P (cid:12)Gˆ(λ;W)−G(λ;W)(cid:12)≥ϵ
(cid:12) (cid:12)
=P (|h(W ,W ,...,W )−E[h(W ,W ,...,W )]|≥ϵ)
1 2 n 1 2 n
  (cid:32) (cid:33) (23)
2ϵ2 −nϵ2
≤2exp− =2exp .
n(2exp(M¯/λ)
)2
2(cid:0) exp(cid:0) M¯/λ(cid:1)(cid:1)2
n
Forsomeδ >0,wehave
(cid:32) (cid:33) (cid:32) (cid:33)
(cid:16)(cid:12) (cid:12) (cid:17) −nϵ2 −nϵ2
P (cid:12)Gˆ(λ;W)−G(λ;W)(cid:12)≥ϵ ≤2exp ≤2exp ≤δ. (24)
(cid:12) (cid:12) 2(cid:0) exp(cid:0) M¯/λ(cid:1)(cid:1)2 2(cid:0) exp(cid:0) M¯/λ(cid:1)(cid:1)2
¯
Thissolvesϵsuchthat
(cid:115)
2log2(cid:0) exp(cid:0) M¯/λ(cid:1)(cid:1)2
ϵ≥ δ ¯ . (25)
n
(cid:12) (cid:12)
Inthefollowingcontent,wewillboundtheterm(cid:12)log(Gˆ(λ;W))−log(G(λ;W))(cid:12). WefirstintroduceLemmaB.4thatis
(cid:12) (cid:12)
(cid:12) (cid:12)
usefulforbounding(cid:12)log(Gˆ(λ;W))−log(G(λ;W))(cid:12).
(cid:12) (cid:12)
LemmaB.4. Letcbeaconstant. Foranyx ,x suchthatx ,x ≥c>0,wehave|log(x )−log(x )|≤ 1|x −x |.
1 2 1 2 1 2 c 1 2
Proof. Withoutlossofgenerality,assume0<c≤x ≤x . Wethenhave
1 2
x x x x −x x −x
log(x )−log(x )=log( 2)=log(1+ 2 −1)≤ 2 −1= 2 1 ≤ 2 1.
2 1 x x x x c
1 1 1 1
Takingtheabsolutevalueofboththeleft-handsideandtheright-handside,wehave
1
|log(x )−log(x )|≤ |x −x |.
1 2 c 1 2
(cid:12) (cid:12)
Next,weintroduceLemmaB.5thatboundstheterm(cid:12)log(Gˆ(λ;W))−log(G(λ;W))(cid:12).
(cid:12) (cid:12)
16SubmissionandFormattingInstructionsforICML2024
LemmaB.5. Letudenotetheprobabilityoftreat,i.e.,u=P(T =1). Assumingthatλ∈Λ:=(0,λ¯]and(τˆ(X)−Y)2is
boundedbelowbyM. Thenforn≥ 2 log(cid:0)2(cid:1) ,withprobability1−δ,wehave
¯ u2 δ
(cid:12) (cid:12)
(cid:12)log(Gˆ(λ;W))−log(G(λ;W))(cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) 1 (cid:88)n (cid:12)
=(cid:12)log( T exp(Z /λ))−log(E[T exp(Z/λ)])(cid:12)
(cid:12) n i i (cid:12) (26)
(cid:12) (cid:12)
i=1
(cid:12) (cid:12)
2 (cid:12)1 (cid:88)n (cid:12)
≤ (cid:12) T exp(Z /λ)−E[T exp(Z/λ)](cid:12).
uexp(cid:0) M/λ¯(cid:1)(cid:12)n i i (cid:12)
(cid:12) (cid:12)
¯ i=1
Proof. AsbothGˆ(λ;W)andG(λ;W)areboundedbelowandgreaterthan0. Therefore,byLemmaB.4,wehave
(cid:12) (cid:12) 1(cid:12) (cid:12)
(cid:12)log(Gˆ(λ;W))−log(G(λ;W))(cid:12)≤ (cid:12)Gˆ(λ;W)−G(λ;W)(cid:12),
(cid:12) (cid:12) c (cid:12) (cid:12)
(cid:26) (cid:27) (27)
wherec=min inf Gˆ(λ;W), inf G(λ;W) .
λ∈Λ,W∈W λ∈Λ,W∈W
Moreover,foranyλ∈Λ,wehave
G(λ;W)=E[T exp(Z/λ)]=E[exp(Z/λ)]P(T =1)≥E[exp(M/λ¯)]u=exp(M/λ¯)u; (28)
¯ ¯
n n n
Gˆ(λ;W)= 1 (cid:88) T exp(Z /λ)≥ 1 (cid:88) T exp(M/λ¯)=exp(M/λ¯)1 (cid:88) T =exp(M/λ¯)uˆ. (29)
n i i n i ¯ ¯ n i ¯
i=1 i=1 i=1
Givenuˆ= 1 (cid:80)n T andu=E[1 (cid:80)n T ],usingHoeffding’sinequality,wehave
n i=1 i n i=1 i
P
(cid:32)(cid:12) (cid:12) (cid:12)1 (cid:88)n
T
−E[1 (cid:88)n
T
](cid:12) (cid:12)
(cid:12)≥
E[ n1 (cid:80)n i=1T i](cid:33) ≤2exp(cid:18) −2(u 2)2(cid:19)
≤δ. (30)
(cid:12)n i n i (cid:12) 2 n(1)2
(cid:12) i=1 i=1 (cid:12) n
Wecansolvenby
(cid:18) nu2(cid:19)
2
(cid:18) 2(cid:19)
2exp − ≤δ ⇒n≥ log . (31)
2 u2 δ
Thisindicatesthatuˆ ≥ u/2withprobability1−δ whenn ≥ 2 log(cid:0)2(cid:1) . Combiningthiswithequations(28)and(29),
u2 δ
withprobability1−δ,whenn≥ 2 log(cid:0)2(cid:1) ,wehave
u2 δ
inf G(λ;W)≥exp(M/λ¯)u; (32)
λ∈Λ,W∈W ¯
inf Gˆ(λ;W)≥exp(M/λ¯)uˆ≥exp(M/λ¯)u/2. (33)
λ∈Λ,W∈W ¯ ¯
Therefore,withprobability1−δ,whenn≥ 2 log(cid:0)2(cid:1) ,wehave
u2 δ
(cid:12) (cid:12) 1(cid:12) (cid:12)
(cid:12)log(Gˆ(λ;W))−log(G(λ;W))(cid:12)≤ (cid:12)Gˆ(λ;W)−G(λ;W)(cid:12),
(cid:12) (cid:12) c (cid:12) (cid:12)
(cid:26) (cid:27) (34)
wherec=min inf Gˆ(λ;W), inf G(λ;W) =exp(M/λ¯)u/2.
W∈W W∈W ¯
ThiscompletestheproofofLemmaB.5.
Additionally,thefollowingLemmaB.6providestheboundof|log(uˆ)−log(u)|.
LemmaB.6. Letuˆ= 1 (cid:80)n T andu=E[1 (cid:80)n T ]. Forn≥ 2 log(cid:0)2(cid:1) ,withprobability1−δ,wehave
n i=1 i n i=1 i u2 δ
(cid:115) 
2log(2)
|log(uˆ)−log(u)|≤O δ . (35)
nu2
17SubmissionandFormattingInstructionsforICML2024
Proof. UsingHoeffding’sinequality,wehave
P(|uˆ−u|≥ϵ)=P
(cid:32)(cid:12) (cid:12) (cid:12)1 (cid:88)n
T
−E[1 (cid:88)n
T
](cid:12) (cid:12) (cid:12)≥ϵ(cid:33) ≤2exp(cid:0) −2nϵ2(cid:1)
,
(cid:12)n i n i (cid:12)
(cid:12) (cid:12)
i=1 i=1
(cid:115)
log(2)
2exp(cid:0) −2nϵ2(cid:1) ≤δ solves ϵ≥ δ .
2n
Notably,usingtheresultsinequation(31),weknowforn≥ 2 log(cid:0)2(cid:1) ,uˆ≥u/2. Therefore,wehave
u2 δ
1
|log(uˆ)−log(u)|≤ |uˆ−u|. (ByLemmaB.4)
min{uˆ,u}
(cid:115)  (cid:115)  (36)
2 2 log(2) 2log(2)
= |uˆ−u|≤ O δ =O δ .
u u 2n nu2
Inthefollowing,wewillboundtheterm|Vˆ(τˆ)−V(τˆ)|usingabovelemmas. WefirstdefinetwofunctionsF(λ)andFˆ(λ):
(cid:18) (cid:19)
1
F(λ)=λϵ+λlog(EPT[exp((τˆ(X)−Y)2/λ)])=λϵ+λlog E[T exp((τˆ(X)−Y)2/λ)] ; (37)
u
n (cid:32) n (cid:33)
Fˆ(λ)=λϵ+λlog( 1 (cid:88) T exp((τˆ(X )−Y )2/λ))=λϵ+λlog 1 (cid:88) T exp((τˆ(X )−Y )2/λ) . (38)
n i i i nuˆ i i i
t
i=1 i=1
ThefollowingLemmaB.7boundstheterm|Fˆ(λ)−F(λ)|.
LemmaB.7. Letu:=P(T =1). Assumingthat0<λ≤λ≤λ¯and(τˆ(X)−Y)2isboundedwithintherangeofM to
M¯,forn≥2/u2log(2/δ),withprobability1−δ,weh¯
ave
¯
(cid:115)  (cid:115) 
8λ2log2 2λ2log(2)
|Fˆ(λ)−F(λ)|≤O δ (cid:0) exp(M¯/λ−M/λ¯))(cid:1)2 +O δ . (39)
nu2 ¯ ¯ nu2
Proof.
|Fˆ(λ)−F(λ)|
(cid:12) (cid:12) (cid:32) (cid:18) 1 (cid:19) (cid:32) 1 (cid:88)n (cid:33)(cid:33)(cid:12) (cid:12)
=(cid:12)λ log E[T exp((τˆ(X)−Y)2/λ)] −log T exp((τˆ(X )−Y )2/λ) (cid:12)
(cid:12) u nuˆ i i i (cid:12)
(cid:12) (cid:12)
i=1
=λ(cid:12) (cid:12) (cid:12)log(cid:0)E[T exp((τˆ(X)−Y)2/λ)](cid:1) −log(cid:32) 1 (cid:88)n T exp((τˆ(X )−Y )2/λ)(cid:33) +log(uˆ)−log(u)(cid:12) (cid:12) (cid:12)
(cid:12) n i i i (cid:12)
(cid:12) (cid:12)
i=1
≤λ(cid:12) (cid:12) (cid:12)log(cid:0)E[T exp((τˆ(X)−Y)2/λ)](cid:1) −log(cid:32) 1 (cid:88)n T exp((τˆ(X )−Y )2/λ)(cid:33)(cid:12) (cid:12) (cid:12)+λ|log(uˆ)−log(u)|
(cid:12) n i i i (cid:12)
(cid:12) (cid:12)
i=1 (40)
(cid:12) (cid:12)
≤ uexp2 (λ M/λ¯)(cid:12) (cid:12) (cid:12)n1 (cid:88)n T ie(τˆ(Xi λ)−Yi)2 −E[Te(τˆ(X) λ−Y)2 ](cid:12) (cid:12) (cid:12)+λ|log(uˆ)−log(u)| (ByLemmaB.5)
(cid:12) (cid:12)
¯ i=1
(cid:115)  (cid:115) 
2λ 2log2(cid:0) exp(cid:0) M¯/λ(cid:1)(cid:1)2 2λ2log(2)
≤ uexp(M/λ¯)O δ
n
¯ +O
nu2
δ  (ByLemmaB.3andLemmaB.6)
¯
(cid:115)  (cid:115) 
8λ2log2 2λ2log(2)
=O δ (cid:0) exp(M¯/λ−M/λ¯)(cid:1)2 +O δ .
nu2 ¯ ¯ nu2
18SubmissionandFormattingInstructionsforICML2024
Now,wecanprovetheresultinTheorem4.5.
Proof. Letλˆ =argmin Fˆ(λ)andλ∗ =argmin F(λ). Thenwehave
λ λ
V(τˆ)−Vˆ(τˆ)=F(λ∗)−Fˆ(λˆ)
=F(λ∗)−Fˆ(λˆ)+F(λˆ)−F(λˆ)
=F(λˆ)−Fˆ(λˆ)+F(λ∗)−F(λˆ)
(41)
≤|F(λˆ)−Fˆ(λˆ)|+0
≤sup|F(λ)−Fˆ(λ)|.
λ
Vˆ(τˆ)−V(τˆ)=Fˆ(λˆ)−F(λ∗)
=Fˆ(λˆ)−F(λ∗)+Fˆ(λ∗)−Fˆ(λ∗)
=Fˆ(λ∗)−F(λ∗)+Fˆ(λˆ)−Fˆ(λ∗)
(42)
≤|Fˆ(λ∗)−F(λ∗)|+0
≤sup|Fˆ(λ)−F(λ)|.
λ
Basedontheabovetwoinequalitiesandtakingthesupoperationforbothfirstandlasttermsofequation(40),wehave
|Vˆ(τˆ)−V(τˆ)|≤sup|Fˆ(λ)−F(λ)|
λ
(cid:115)  (cid:115) 
8λ¯2log2 2λ¯2log(2) (43)
≤O δ (cid:0) exp(M¯/λ−M/λ¯)(cid:1)2 +O δ . (ByLemmaB.7)
nu2 ¯ ¯ nu2
C.AdditionalExperimentalResults
Table2.ComparisonofdifferentselectorsacrossSettingsA,B,andC.Theestimatorselectionisover8candidateestimators,with
underlyingMLmodelfixedasLR.Reportedvalues(mean±standarddeviation)arecomputedover100experiments.Bolddenotesthe
√
bestresultsamongallselectors. PEHE&Regret:Smallerisbetter.Rankcorrelation:Largerisbetter.
√
Criterion PEHE(withR(τˆbest)provided) Regret RankCorrelation
Setting A(0.24±0.04) B(15.49±1.84) C(16.10±2.49) A B C A B C
Plug-S 29.07±5.36 27.92±5.28 35.11±6.72 122.99±32.59 0.80±0.25 1.20±0.37 -0.14±0.17 0.75±0.07 0.71±0.04
Plug-PS 29.35±5.39 28.41±5.13 36.00±5.98 124.20±32.87 0.83±0.23 1.26±0.32 -0.17±0.17 0.74±0.07 0.70±0.04
Plug-T 13.64±4.00 25.76±5.21 17.73±5.09 57.17±19.76 0.67±0.29 0.11±0.33 0.79±0.11 0.81±0.06 0.85±0.04
Plug-X 18.82±9.45 25.02±6.21 31.56±8.53 79.78±44.55 0.61±0.35 0.98±0.51 0.57±0.19 0.84±0.07 0.77±0.05
Plug-IPW 29.12±3.49 61.01±12.28 63.48±17.82 123.27±27.76 2.93±0.54 2.96±1.00 0.16±0.16 0.32±0.10 0.40±0.12
Plug-DR 13.63±4.32 23.72±5.58 17.36±5.25 57.02±20.43 0.54±0.32 0.08±0.30 0.79±0.11 0.85±0.07 0.85±0.04
Plug-R 37.89±5.12 159.18±21.16 164.11±21.59 160.73±37.79 9.27±0.52 9.30±1.23 -0.54±0.14 -0.46±0.08 -0.51±0.05
Plug-RA 13.72±4.24 23.97±5.52 17.91±5.67 57.40±20.27 0.55±0.33 0.12±0.36 0.79±0.11 0.84±0.06 0.85±0.04
Pseudo-DR 12.57±2.99 21.91±4.05 16.80±3.14 52.53±15.66 0.42±0.24 0.05±0.18 0.81±0.09 0.88±0.05 0.86±0.03
Pseudo-R 7.29±11.76 32.20±8.25 42.99±8.61 30.97±53.76 1.09±0.52 1.70±0.53 0.73±0.27 0.61±0.13 0.64±0.12
Pseudo-IF 28.72±3.77 84.44±14.30 82.31±12.27 121.69±28.65 4.45±0.64 4.17±0.72 0.32±0.15 0.33±0.09 0.41±0.09
Random 1780.99±7173.08 486.98±3141.69 2602.77±11333.92 7195.34±29304.08 39.39±266.19 170.40±738.22 -0.01±0.10 0.15±0.09 0.09±0.08
DRM 8.48±5.99 19.69±6.47 16.62±3.03 35.01±26.22 0.27±0.39 0.04±0.15 0.81±0.08 0.87±0.06 0.81±0.07
19SubmissionandFormattingInstructionsforICML2024
Table3.ComparisonofdifferentselectorsacrossSettingsA,B,andC.Theestimatorselectionisover8candidateestimators,with
underlyingMLmodelfixedasSVM.Reportedvalues(mean±standarddeviation)arecomputedover100experiments.Bolddenotesthe
√
bestresultsamongallselectors. PEHE&Regret:Smallerisbetter.Rankcorrelation:Largerisbetter.
√
Criterion PEHE(withR(τˆ best)provided) Regret RankCorrelation
Setting A(15.22±10.39) B(24.52±4.63) C(31.29±4.20) A B C A B C
Plug-S 29.69±5.84 26.17±5.21 33.94±4.74 1.97±1.77 0.07±0.06 0.08±0.03 -0.40±0.47 0.79±0.09 0.78±0.06
Plug-PS 30.32±5.25 26.17±5.21 33.94±4.74 2.04±1.76 0.07±0.06 0.08±0.03 -0.46±0.43 0.79±0.09 0.78±0.05
Plug-T 16.89±10.18 25.26±4.80 31.30±4.20 0.29±0.99 0.03±0.09 0.00±0.00 0.85±0.11 0.93±0.07 0.99±0.01
Plug-X 16.80±10.75 25.62±5.09 33.64±4.68 0.18±0.57 0.04±0.05 0.07±0.03 0.71±0.22 0.82±0.09 0.80±0.07
Plug-IPW 29.96±3.66 62.11±11.27 63.52±13.09 1.91±1.49 1.61±0.67 1.03±0.31 0.59±0.17 0.08±0.39 0.27±0.42
Plug-DR 16.81±10.11 25.19±4.76 31.30±4.20 0.28±0.96 0.03±0.09 0.00±0.00 0.85±0.10 0.93±0.07 0.99±0.02
Plug-R 36.10±4.67 158.86±21.12 164.22±21.61 2.50±1.80 5.68±1.52 4.26±0.26 -0.90±0.06 -0.94±0.06 -0.99±0.01
Plug-RA 16.94±10.13 25.19±4.76 31.30±4.20 0.30±0.99 0.03±0.09 0.00±0.00 0.85±0.11 0.93±0.07 0.99±0.02
Pseudo-DR 16.68±10.22 25.26±4.80 31.30±4.20 0.26±0.96 0.03±0.09 0.00±0.00 0.86±0.10 0.96±0.05 0.99±0.01
Pseudo-R 18.36±11.75 28.84±11.54 33.25±9.16 0.38±0.99 0.19±0.46 0.07±0.31 0.67±0.55 0.82±0.31 0.91±0.23
Pseudo-IF 29.96±3.66 62.11±11.27 63.52±13.09 1.91±1.49 1.61±0.67 1.03±0.31 0.62±0.13 0.10±0.37 0.45±0.40
Random 33.04±8.14 75.15±24.27 93.42±25.16 2.18±1.81 2.14±1.13 1.98±0.69 -0.22±0.20 -0.33±0.14 -0.36±0.13
DRM 15.91±10.40 25.26±4.80 36.40±12.26 0.07±0.22 0.03±0.09 0.16±0.34 0.89±0.08 0.95±0.05 0.94±0.10
Table4.ComparisonofdifferentselectorsacrossSettingsA,B,andC.Theestimatorselectionisover8candidateestimators,with
underlyingMLmodelfixedasRF.Reportedvalues(mean±standarddeviation)arecomputedover100experiments.Bolddenotesthe
√
bestresultsamongallselectors. PEHE&Regret:Smallerisbetter.Rankcorrelation:Largerisbetter.
√
Criterion PEHE(withR(τˆ best)provided) Regret RankCorrelation
Setting A(32.15±4.08) B(28.45±4.23) C(38.46±5.43) A B C A B C
Plug-S 34.50±4.84 29.57±4.59 41.18±6.04 0.07±0.03 0.04±0.02 0.07±0.03 0.48±0.22 0.87±0.06 0.73±0.15
Plug-PS 34.50±4.84 29.59±4.58 41.25±6.09 0.07±0.03 0.04±0.02 0.07±0.03 0.47±0.22 0.87±0.06 0.73±0.15
Plug-T 32.39±4.07 28.45±4.22 38.60±5.42 0.01±0.01 0.00±0.00 0.00±0.01 0.48±0.23 0.94±0.04 0.90±0.12
Plug-X 34.00±4.62 29.12±4.44 41.11±5.97 0.06±0.03 0.02±0.02 0.07±0.04 0.64±0.18 0.90±0.05 0.74±0.14
Plug-IPW 32.48±4.01 45.17±6.34 43.49±6.19 0.01±0.02 0.59±0.12 0.13±0.08 0.39±0.22 0.37±0.07 0.54±0.18
Plug-DR 32.39±4.07 28.45±4.23 38.59±5.41 0.01±0.01 0.00±0.00 0.00±0.01 0.49±0.23 0.95±0.04 0.90±0.12
Plug-R 33.08±4.21 155.76±20.88 161.45±21.49 0.03±0.04 4.50±0.29 3.21±0.19 0.41±0.35 -0.38±0.08 -0.30±0.17
Plug-RA 32.39±4.07 28.45±4.23 38.60±5.42 0.01±0.01 0.00±0.00 0.00±0.01 0.49±0.23 0.95±0.04 0.90±0.12
Pseudo-DR 32.38±4.07 28.45±4.22 38.57±5.42 0.01±0.01 0.00±0.00 0.00±0.01 0.49±0.23 0.94±0.04 0.90±0.12
Pseudo-R 32.94±4.25 32.82±6.89 42.22±6.14 0.02±0.03 0.16±0.18 0.10±0.04 0.70±0.32 0.40±0.12 0.34±0.22
Pseudo-IF 32.48±4.01 45.17±6.34 43.49±6.19 0.01±0.02 0.59±0.12 0.13±0.08 0.39±0.22 0.39±0.06 0.59±0.18
Random 35.17±4.80 31.17±13.71 46.32±23.67 0.09±0.04 0.09±0.42 0.21±0.63 -0.45±0.18 0.11±0.12 -0.08±0.17
DRM 32.75±4.11 28.62±4.26 39.46±5.57 0.02±0.02 0.01±0.01 0.03±0.05 0.32±0.24 0.85±0.09 0.73±0.20
Table5.ComparisonofdifferentselectorsacrossSettingsA,B,andC.Theestimatorselectionisover8candidateestimators,with
underlyingMLmodelfixedasNN.Reportedvalues(mean±standarddeviation)arecomputedover100experiments.Bolddenotesthe
√
bestresultsamongallselectors. PEHE&Regret:Smallerisbetter.Rankcorrelation:Largerisbetter.
√
Criterion PEHE(withR(τˆbest)provided) Regret RankCorrelation
Setting A(12.43±2.85) B(21.62±2.97) C(16.14±2.51) A B C A B C
Plug-S 31.41±5.09 24.07±4.54 21.29±5.40 1.60±0.49 0.11±0.14 0.34±0.36 0.37±0.25 0.82±0.15 0.93±0.05
Plug-PS 31.60±4.56 24.60±4.65 21.65±5.28 1.62±0.46 0.14±0.16 0.36±0.37 0.36±0.26 0.81±0.15 0.93±0.05
Plug-T 12.43±2.85 21.91±3.25 16.72±3.04 0.00±0.00 0.01±0.04 0.04±0.17 0.84±0.20 0.85±0.16 0.97±0.03
Plug-X 20.29±9.66 22.33±3.64 17.86±5.39 0.68±0.85 0.03±0.08 0.11±0.32 0.61±0.23 0.87±0.12 0.96±0.05
Plug-IPW 29.12±3.49 80.53±20.65 82.15±12.95 1.43±0.45 2.74±0.90 4.15±0.77 0.55±0.27 0.10±0.22 -0.01±0.19
Plug-DR 12.43±2.85 21.86±3.16 16.62±3.00 0.00±0.00 0.01±0.04 0.04±0.16 0.84±0.19 0.86±0.15 0.97±0.03
Plug-R 32.05±4.77 85.44±14.27 59.61±11.02 1.66±0.48 2.96±0.55 2.74±0.73 0.03±0.30 -0.07±0.22 -0.17±0.06
Plug-RA 12.43±2.85 21.86±3.16 16.89±3.38 0.00±0.00 0.01±0.04 0.05±0.21 0.84±0.20 0.86±0.15 0.97±0.03
Pseudo-DR 12.43±2.85 21.90±3.24 16.80±3.14 0.00±0.00 0.01±0.04 0.05±0.18 0.83±0.20 0.85±0.15 0.97±0.03
Pseudo-R 15.21±7.28 28.27±7.00 22.72±12.36 0.25±0.65 0.32±0.34 0.42±0.76 0.58±0.25 0.68±0.24 0.84±0.22
Pseudo-IF 28.72±3.77 86.43±11.97 82.62±11.44 1.39±0.43 3.02±0.44 4.18±0.68 0.59±0.26 0.10±0.20 0.02±0.17
Random 1784.56±7172.20 481.95±3142.32 2597.08±11335.12 154.63±621.66 27.84±191.10 170.06±738.30 0.13±0.34 0.25±0.31 -0.00±0.16
DRM 12.43±2.85 27.10±6.20 16.62±3.03 0.00±0.00 0.26±0.24 0.03±0.15 0.79±0.22 0.77±0.18 0.83±0.04
20SubmissionandFormattingInstructionsforICML2024
Figure2.Thestackedbarchartshowingthedistributionoftheselectedestimator’srankforeachevaluationmetricacrossrankintervals:
[1],[2-4],[5-7],[8].Theestimatorselectionisover8candidateestimators,withtheunderlyingMLmodelfixedasLR,SVM,RF,orNN.
21