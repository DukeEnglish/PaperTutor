RNNS ARE NOT TRANSFORMERS (YET):
THE KEY BOTTLENECK ON IN-CONTEXT RETRIEVAL
KaiyueWen1∗ XingyuDang1∗ KaifengLyu2†
1InstituteforInterdisciplinaryInformationSciences,TsinghuaUniversity
2DepartmentofComputerScience&PrincetonLanguageandIntelligence,PrincetonUniversity
{wenky20,dangxy20}@mails.tsinghua.edu.cn
klyu@cs.princeton.edu
ABSTRACT
ThispaperinvestigatesthegapinrepresentationpowersofRecurrentNeuralNetworks(RNNs)and
Transformersinthecontextofsolvingalgorithmicproblems. Wefocusonunderstandingwhether
RNNs,knownfortheirmemoryefficiencyinhandlinglongsequences,canmatchtheperformanceof
Transformers,particularlywhenenhancedwithChain-of-Thought(CoT)prompting. Ourtheoretical
analysisrevealsthatCoTimprovesRNNsbutisinsufficienttoclosethegapwithTransformers. Akey
bottleneckliesintheinabilityofRNNstoperfectlyretrieveinformationfromthecontext,evenwith
CoT:forseveraltasksthatexplicitlyorimplicitlyrequirethiscapability,suchasassociativerecalland
determiningifagraphisatree,weprovethatRNNsarenotexpressiveenoughtosolvethetaskswhile
Transformerscansolvethemwithease. Conversely,weprovethatadoptingtechniquestoenhance
thein-contextretrievalcapabilityofRNNs,includingRetrieval-AugmentedGeneration(RAG)and
addingasingleTransformerlayer,canelevateRNNstobecapableofsolvingallpolynomial-time
solvableproblemswithCoT,henceclosingtherepresentationgapwithTransformers.3
1 Introduction
Transformermodels(Vaswanietal.,2017)havebecomethedominantchoiceofthebackboneforlargelanguagemodels
(LLMs). ThecorecomponentofTransformersisitsself-attentionmodule,whichallowsthemodeltorouteinformation
denselyacrosstheentiresequence. However,thisdesignleadstohighinferencecostsformodelinglongsequences,
includingamemorycostthatisatleastlinearinthesequencelengthduetotheneedformaintainingintermediate
attentionkeysandvaluesforeachtoken,andatimecostquadraticinthesequencelengthforcomputingtheattention
scoreforeachpairoftokens.
Recently,RecurrentNeuralNetworks(RNNs)havebeenanincreasinglypopularchoiceinsequencemodelingtasks
duetotheirabilitytomaintainamemorysizeconstantinsequencelengthduringinference,thusbeingmorememory
efficientthanTransformers. Katharopoulosetal.(2020)showedthatTransformerswithaspecialtypeofkernelized
linearattentioncanbeexpressedasRNNs. Guetal.(2022)tookadifferentpathtodesignRNNsbystructuringlatent
statesasStateSpaceModels(SSMs)fromcontroltheory. Theseideashaveledtoaseriesofdevelopmentofmodern
RNNs,includingRWKV(Pengetal.,2023),RetNet(Sunetal.,2023),andMamba(Gu&Dao,2023). Mostnotably,
MambacanachievecompetitiveperformancewithTransformersonseveralsequencemodelingtaskswithlineartime
andconstantmemorycomplexityinsequencelength.
CanRNNsreplaceTransformersyet? TheriseofthesemodernRNNshasledtoaninterestinunderstandingtheir
limitations. ArecentworkbyAroraetal.(2023)showedthatanimportantfamilyofRNNs,input-independentgating
SSMs,areempiricallyinferiortoTransformersinataskthathasalonghistoryinartificialintelligence,associative
recall(AR)(Willshawetal.,1969;Hopfield,1982;Hinton&Anderson,2014): Givenaseriesofkey-valuepairsasa
string,themodelisrequiredtorecallthevaluegivenakey. Onthetheoryside,Sanfordetal.(2023)andJelassietal.
∗Equalcontribution
†Correspondingauthor
3Codeisavailableathttps://github.com/dangxingyu/rnn-icrag
4202
beF
92
]GL.sc[
2v01581.2042:viXraPREPRINT
Transformer+CoT⊃P RNN+In-contextRAG⊃P
RNN+CoT̸⊃P
VanillaTransformer
VanillaRNN
Transformer RNN(Thiswork)
Figure1: HierarchyofRepresentationPower. WhileRNNwithchain-of-thought(CoT)withO(logn)bitmemory
provably has strictly stronger representation power than RNN without CoT under mild complexity assumptions
(Theorem4.1), itisstillexponentiallyweakerthanTransformerwithCoTinrepresentingsolutionstoalgorithmic
problems(Theorem4.7). WeproceedtoshowthattheincapabilityofRNNsinIn-contextRetrievalistherootcauseof
thegapandproposetwoformsofIn-contextRetrievalAugmentedGeneration(In-contextRAG)toclosethegapby
illustratingtheirpowertosimulateanypolynomial-timeTuringmachines(Theorems5.4and5.8).
(2024)demonstratedthatconstant-memoryRNNsdonothavesufficientrepresentationpowertosolvethetasksof
averagingagivensubsetofinputvectors(q-sparseaveraging)andrepeatingtheinputsequence(copying),respectively,
whilethereexistshallowTransformersthatcansolvethesetasks.
However,theaboveresultsdonotexcludethepossibilitythatenhancingRNNswithadditionalpromptingtechniquesor
minorarchitecturalchangesmayclosethegapwithTransformers.Infact,Transformersthemselvesarenotperfecteither
andmayneedadditionaltechniquesatinferencetimetoperformwelloncertaintasks. Asanotableexample,Chain-
of-Thought(CoT)(Weietal.,2023),apromptingtechniquethatasksthemodeltogenerateaseriesofintermediate
tokensbeforegivingthefinalanswer,hasbeenknowntobecrucialforTransformerstoperformwellontasksthat
requiremathematicaloralgorithmicreasoning. Fengetal.(2023);Lietal.(2024)explainedthisfromtheperspective
ofrepresentationpower: Transformersalonedonothavesufficientrepresentationpowertosolveproblemsbeyonda
certaincircuitcomplexityclass(TC0),butwithCoT,theycanevensimulateanypolynomial-timeTuringmachines.
TheeffectivenessofCoTonTransformersnaturallyleadstothefollowingquestion:
Cansimilarenhancements,suchasadoptingCoT,improveRNNssothattheycanbeonparwithTransformers?
OurContributions. Thispaperanswerstheabovequestionfromtheorybyexaminingvariouswaystoclosethegap
intherepresentationpowersofRNNsandTransformersonalgorithmicproblems. Throughaseriesoflowerandupper
boundresults,weshowthatCoTimprovestherepresentationpowerofRNNs,buttoclosethegapwithTransformers,
CoTaloneisnotenoughtoovercomeakeybottleneckofRNNs: theirinabilitytoretrieveinformationfromthecontext,
whichwecallin-contextretrievalforshort. Wefurtherillustratethataddressingthisin-contextretrievalbottleneckis
sufficienttoclosethisgap: RNNscansolveallpolynomial-timesolvableproblemsifadoptingtechniquestoenhance
thein-contextretrievalcapability,includinginvolvingRetrieval-AugmentedGeneration(RAG)andappendingasingle
Transformerlayer. Ourmaincontributionsarelistedasfollows:
1. CoTimprovesRNNsbutcannotclosetherepresentationgapwithTransformers. (Section4)
• Onthepositiveside,weprovethatCoTmakesRNNsstrictlymoreexpressiveundermildassumptionsfrom
circuitcomplexity.
• Onthenegativeside,weshowthatadoptingCoTisnotenoughtoclosetherepresentationgapbetweenRNNs
andTransformers: thememoryefficiencyofRNNsfundamentallylimitstheirabilitytoperformin-context
retrieval,evenwithCoT.ThispointismadeconcretebyprovingthatRNNswithCoTcannotsolveasetof
fundamentalalgorithmicproblemsthatdirectlyaskforin-contextretrieval,includingassociativerecall. We
furtherexemplifythatin-contextretrievalcanbeimplicitlyrequiredintasksthatappearunrelated,byproving
theinabilityofRNNstosolvetheclassicproblemofdeterminingifagraphisatree(IsTree).
• Ontheotherhand,weprovethatTransformershavetherepresentationpowertosolvemanyoftheabovetasks
withease,includingIsTree. Moreover,TransformerswithCoTcanevensimulateRNNswithCoTefficiently,
withonlyasmallmultiplicativefactorinthenumberofparameters.
2PREPRINT
Figure2: WetrainRNNs(Mamba)andTransformers(LLaMA2Touvronetal.(2023))withafrozenwordembedding
anddecodingheadofthreedifferentmodelsizes(0.5M,1M,2M)onIsTreewiththreedifferentsizesofgraph(16,
32, 64) under three different setups. Vanilla means the model directly predicts the label. COT means the model
will generate a chain-of-thought process based on DFS (see Algorithm 1) before prediction. Retrieval means the
modelwillgeneratethechainofsearchqueriesandreasoningbeforeprediction(seeAlgorithm2). Weobservethat
(1)BothTransformerandRNNscan’tsolvetheIsTreequestionwithoutachainofthought;(2)RNNs’performance
withchain-of-thoughtdecaysquicklywhenthenumberofnodesincrease,whichisconsistentwithourtheory;(3)All
modelsreachalmostperfectaccuracywhenenhancedwithretrieval.
Technically,thekeyinsightforthefirstseparationisthatRNNswithoutCoTisashallowcircuit,whileRNNswith
CoTcanbeanexponentiallydeepercircuit. Butonthenegativeside,RNNsaresomemoryefficientthattheycan
triggerstreaminglowerbounds(Sanfordetal.,2023),especiallyforproblemsthatrequirein-contextretrieval.
2. Enhancingthein-contextretrievalcapabilityofRNNscanclosetherepresentationgap. (Section5)
• WeprovethatallowingRNNstoinvokefunctioncallstoperformacertainprimitiveofin-contextretrievalis
sufficienttoboosttheirrepresentationpowertosolveallpolynomial-timesolvableproblemswithCoT,hence
closingtherepresentationgapbetweenRNNsandTransformers.
• Alternatively,asonelayeroftransformerissufficienttoperformmanyin-contextretrievaloperations,weprove
thatimplicitlyenhancingthein-contextretrievalcapabilityofRNNsbyaddingjustonetransformerlayeratthe
endofthearchitectureisalsosufficienttoclosetherepresentationgap.
Technically,thekeyinsightfortheaboveupperboundsisthatRNNcanfocusonthelocalreasoningstepsanduse
thein-contextretrievalmoduletoadaptivelyfetchtherelevantinformationfromthecontext.
2 RelatedWorks
StateSpaceMachinesandLinearTransformers. Therehasbeenarecentsurgeofinterestinstatespacemachines
and(kernalized)lineartransformers(Guetal.,2022;Katharopoulosetal.,2020;Pengetal.,2023;Sunetal.,2023;
Gu&Dao,2023;Fuetal.,2023;Polietal.,2023;Luoetal.,2021;Pengetal.,2021;Wangetal.,2020),whicharea
classofmodelsthatcombinetheparallelizabilityoftheTransformerwiththememoryefficiencyoftheRNN.These
modelscanprocessbothasequentialandarecurrentform,andcanusetheformerforfastparallelizabletrainingand
thelatterformemory-efficientinference. However,thesemodelsarestillempiricallyinferiortotheTransformerin
termsofperformance. Ourworkinvestigatesthereasonsbehindthisgapandproposestocloseitbyenhancingthe
in-contextretrievalcapability.
ChainofThought(CoT). Chainofthought(Weietal.,2023;Nyeetal.,2021;Kojimaetal.,2023;Wang&Zhou,
2024)isanaugmentationtotheTransformer,thatallowsittosolvemorecomplexreasoningtasksbygeneratinga
reasoningprocessbeforeoutputtingtheanswer. IthasbeenshownthatTransformerswithCoTprovablyhavemore
expressivepowerthantheoriginalTransformerwithoutCoT(Fengetal.,2023;Lietal.,2024). However,theexpressive
powerofRNNswithCoThasnotyetbeensystematicallystudied. TheoremF.1inFengetal.(2023)showsthatRNN
3PREPRINT
cannot output a particular format of CoT for evaluating arithmetic expressions and solving linear equations while
Transformers with the same amount of parameters can. Concurrent work (Yang et al., 2024) discovers that linear
Transformers,aspecialclassofRNNs,arenotabletosolvesomedynamicprogrammingproblemswithCoT,unless
thenumberofparametersgrowswiththelengthoftheinput. Onehigh-levelmessageourworkconveysissimilarto
theirs: RNNshavelimitedrepresentationpowertoperformreasoningwithCoT.However,weshowthatsuchlimitation
isnotspecifictotheoutputformatorarchitectureandapplytoolsfromstreamingcomplexitytoprovelowerboundson
abroaderrangeoftasksandmemory-efficientarchitectures.
StreamingAlgorithms. Ourlowerboundleveragesthetechniqueinstreamingalgorithms. Streamingalgorithmsare
algorithmsthattakeconstant(typicallyjust1)passovertheinputandusesublinearspace,henceincludingRNNswith
fixedstatespaceasaspecialcase. Worksinstreamingalgorithmsdatebacktothe1980s(Munro&Paterson,1980)and
havebeenformalizedandpopularizedinthe1990s(Alonetal.,1996)duetotheneedtoprocesslargedatastreams.
ThelowerboundinourworkisadirectapplicationofthetechniqueinstreamingalgorithmstothestudyofRNNsand
wemainlyconsiderthestreamingalgorithmsfor(1)indexingtheinput(Munro&Paterson,1980)and(2)determining
whethertheinputisatree(Henzingeretal.,1998).
RetrievalAugmentedGeneration. Ourworkproposestouseretrievalaugmentationtoclosetherepresentationgap
betweenRNNsandTransformers. Thisisconsistentwiththerecenttrendofretrievalaugmentedgeneration(Guuetal.,
2020;Borgeaudetal.,2022;Rubin&Berant,2023). Empirically,retrievalaugmentedgenerationhasbeenshown
toimprovetheperformanceofrecurrentmodelsinvarioustasks(Kuratovetal.,2024;Akyüreketal.,2024)andour
workprovidesatheoreticalfoundationforthisphenomenon. Ourworkalsoshowsthatanattentionlayercanbeused
tosimulatetheretrievalprocess,whichisconsistentwiththefindingthatattentioncanimprovetheperformanceof
RNNs(Vaswanietal.,2017;Aroraetal.,2023;Parketal.,2024;Pengetal.,2023;Haoetal.,2019). Ithasalsobeen
shownempiricallythatattentioncanbeusedtosimulatecomplexretrievalprocess(Jiangetal.,2022).
Comparison Between Transformers and RNNs (Without CoT). A line of works focused on the comparison
betweenRNNsandTransformersintermsofrecognizingorgeneratingformallanguages(Bhattamishraetal.,2020;
Hahn,2020;Merrilletal.,2022). TheseworksshowthatthelackofrecurrentstructureinTransformersmakesthem
failtorecognizesomeformallanguagesthatRNNscanrecognize. However,Liuetal.(2023);Yaoetal.(2023);Hao
etal.(2022)showthatsuchlimitationcanbemitigatedwhenweconsiderboundedlengthofinputorboundedgrammar
depth. OurworkdiffersfromtheseworksinthatweconsidertheexpressivepowerofRNNsandTransformerswith
CoTandshowthatinthiscase,thegapbetweenRNNsandTransformersisone-sided(Theorem4.8).
Priorwork(Aroraetal.,2023)hasshownthatinput-independentgatingSSMsareinferiortoTransformersinthetask
calledassociativerecall(Willshawetal.,1969;Hopfield,1982;Hinton&Anderson,2014). Thetaskrequiresthe
modeltorecallapreviouslyseenpatterngivenapartialinput. Theyshowthatinput-dependentgatingSSMshavebetter
performanceinassociativerecallandalsoproposeahybridarchitecturethatcombinesinput-independentstatespace
machineswithattentiontoachievebetterperformance. Ourworkdiffersfromthisworkinthefollowingways: (1)Our
workstudiesassociativerecallfromatheoreticalperspectiveandprovesformallowerboundsonthememorysizeof
RNNsnecessaryforsolvingassociativerecallandotherretrievaltasks;(2)Wealsostudyhybridarchitecturesbutwe
provideaproofthatappendingasingleTransformerlayertoRNNscanmakethemexpressiveenough;(3)Ourtheory
appliestonotonlyinput-independentgatingSSMsbutalsoallRNNswitho(n)-bitmemory.
Prior work (Jelassi et al., 2024) proves a representation gap between RNNs and Transformers in repeating a long
sequence,whichcanbeseenasaretrievaltask. TheyshowthatRNNshavedifficultyperformingthetaskduetotheir
limitedmemory. OurworkfurtherprovesthatRNNsarelimitedinsolvingmanyotherretrievaltasks,evenwithCoT.
Technically,akeyingredientintheirproofisacountingargumentontheoutputsequencetoshowalimitedmemory
sizeisnotenoughtoproducetoomanydifferentoutputsequences,butourproofcanhandleretrievaltasksthatonly
requireoutputtingasingletoken.
Notably,Sanfordetal.(2023)applycommunicationcomplexitytoprovecircuitsizeormemorysizelowerbounds
forRNNsandTransformersonthetaskofsparseaveraging. Sanfordetal.(2024)extendthistechniquetoanother
taskcalledhop ,ageneralizationoftheassociativerecalltask. Ourtechniqueissimilartotheirssinceourproofis
k
alsobasedoncommunicationcomplexity. Butweconsiderabroaderrangeoftasksincludingseeminglyirrelevant
reasoningtaskssuchasIsTree,andfurtherexplorevariouswaystoclosetherepresentationgap.
RepresentationTheoryofRNNs. Anotherlineofworks(Lietal.,2021,2022;Albertietal.,2023)studiesthe
universalapproximationpowerofRNNs. TheyshowthattheupperboundoftheapproximationpoweroflinearRNNs
willbeconstrainedbythedimensionofthehiddenstates. Theirworksonthehighlevelareconsistentwithourfindings
butarenotdirectlycomparablebecauseweareconsideringfiniteprecisioncomputemodelswiththeassistanceofCoT
orIn-contextRAG.
4PREPRINT
3 Preliminaries
WeintroducethedefinitionsthatarenecessaryforunderstandingourresultsanddeferotherdefinitionstoAppendixA.
VocabularyandEmbeddings. AvocabularyV isafinitesetoftokens. Awordembeddingforatokenv ∈ V is
avectorW(E) ∈Rdthatrepresentsthetoken,andapositionembeddingforthek-thtokeninasequenceisavector
v
W(P) ∈ Rd thatrepresentstheposition. GivenasequenceoftokensS,anembeddingfunctionEmb(S)mapseach
k
token to a vector in Rd by mixing word and position embeddings, resulting in a sequence of vectors. To ease our
comparisonbetweenRNNsandTransformers,inthispaper,weassumefixedwordandpositionembeddingsanddonot
learnthemduringtraining. SeeAppendixA.3fortheformaldefinitions. Thisiscommonpracticeandhasbeenusedin
manypreviousworksstudyingthetheoryofTransformers(Lietal.,2023;Tianetal.,2023).
Many of the problems we study involve natural numbers up to n, where the input sequence length is linear in n.
For simplicity, we assume the vocabulary contains [n] = {1,...,n} and the word embedding for i is defined as
W(E) =iw ,wherew isthefirstcoordinatevector. Butinpractice,thevocabularysizedoesnotincreasewithnand
i 1 1
numbersmaybetokenizedintoafewtokensaccordingtotheirdecimalrepresentations. Wenotethatourresultscanbe
easilyextendedtothismorepracticalcasesinceourlowerboundsdonotrelyonthespecificformofthevocabularyand
embeddingsandfortheupperbounds,ourembeddingcanbeeasilyrepresentedbyafewRNNorTransformerlayers.
NumericalPrecision. Wewillconsidercomputationmodelswithfixednumericalprecisioninthispaper. Wewill
useptodenotetheprecisionofthenumberofbitstorepresentrealnumbersanduseR todenotethesetofallreal
p
numbersthatcanberepresentedbyp-bitfloatingpointnumbers. WedeferthedetailstoAppendixA.2. Wewillassume
p = O(logn) in this paper and state the constant explicitly when necessary. This is a common assumption when
studyingthefiniteprecisionneuralnetworks(Fengetal.,2023;Merrill&Sabharwal,2023).
LanguageModeling. WeuseV∗andV+todenotethesetofallfinitesequencesandallnon-emptyfinitesequences
oftokensinV,respectively. Westudylanguagemodelsthatcanpredictthenexttokengivenaprefixoftokens. For
this,wedefinealanguagemodel(LM)asafunctionM :V+ →P thatmapsanon-emptysequencetoaprobability
V
distributionoverthenexttoken,whereP istheprobabilitysimplexoverV. Wespecificallystudythecasewherethe
V
languagemodelisrealizedbydeepneuralnetworks: firstmaptheinputsequenceS intoasequenceofembeddings
Emb(S),andthenapplyaneuralnetwork,suchasaTransformerorRNN,toprocesstheembeddingsandoutputthe
probabilitydistribution. Wewouldcallaseriesofparameterizedmodelswithincreasinginputsizeafamilyofmodels.
Transformer. WewillfirstdefinetheTransformerarchitectureusedinthetheoreticalanalysisinthispaper.
Definition3.1(TransformerBlock). LetX ∈Rd×lbetheinputmatrix,wherelisthesequencelength. Theoutputofa
Transformerblockf isdefinedas:
f(X)=X+A(X)+g(X+A(X)),
(cid:88)H (cid:32)(cid:0) W(K,h)X(cid:1)⊤ W(Q,h)X (cid:33)
A(X)= W(V,h)Xsoftmax √ +C , (1)
d
h=1
where g is a column-wise ReGLU feed-forward network 4 with width w and output dimension d, A is the scaled
dot-product attention, softmax is the column-wise softmax function, W(K,h), W(Q,h), W(V,h) are the learnable
 0 0 ... 0
−∞ 0 ... 0
parametersandH isthenumberofheads,andC =  . . . . ∈Rl×l isamasktopreventtheattention
 . . . . . . . .
−∞ −∞ ... 0
fromattendingtofuturetokens.
Inthecontextoflanguagemodeling,givenasequenceoftokensS,aTransformerT(S)isdefinedas:
Definition3.2(Transformer). LetS ∈|V|l bethetokenizedinputsequence,theoutputofaTransformerisdefinedas:
(cid:16) (cid:17)
T(S)=softmax W(E)(f (...f (Emb(S)))) . (2)
L 1
:,l
wheresoftmaxisthecolumn-wisesoftmaxfunction,f isthei-thTransformerblock. Wewillcallthei-thTransformer
i
blockthei-thlayeroftheTransformeranddenoteitsfeed-forwardlayerandattentionlayerasg andA respectively.
i i
4ReGLUmeansσ(x)=ReLU(W x+b )⊗(W x+b ),thisisasurrogateforthecommonlyusedSwiGLUactivationand
1 1 2 2
allowsthemodeltoperformmultiplicationoftwocoordinates.
5PREPRINT
RecurrentNeuralNetworks Recentlytherehasbeenalotofinterestinthelinear-timeTransformer,whichreplaces
thefull-attentioncalculationwithlinear-timealternatives. Thesevariantsaremostlyspecialformsofrecurrentneural
networks(RNNs)thatareparallelizable. HerewedefineageneralformofRNNscontainingallofthesevariants;hence,
ouranalysiswillapplytoallofthem.
Definition 3.3 (RNN). An RNN architecture is characterized by two functions: state transition function t : Θ →
(cid:0)RΛ×Rd →RΛ(cid:1)
and output function o : Θ →
(cid:0)RΛ →Rd(cid:1)
, where Λ is the dimension of the state and Θ is the
p p p p p
parameterspace. LetS ∈|V|l betheinputsequence,theoutputofarecurrentneuralnetworkwithparameterθ ∈Θis
definedas:
(cid:16) (cid:17)
R (S)=softmax W(E)o (s ) ,
θ θ l
∀k ∈[l],s =t (s ,Emb(S) ),
k θ k−1 :,k
wheres ∈ RΛ isavectordeterminedbyθ andW(E) isthewordembeddingmatrix. Wewillomitthesubscriptθ
0 p
whenitisclearfromthecontext.
WecancharacterizethecomplexityofanRNNarchitecturewiththefollowingthreemeasures,
1. ParametersizeP: thenumberofbitofparametersdeterminingtando.
2. StatememorysizeM: thenumberofbitstorecordthestateoftheRNN,inthiscase,isΛ×p.
3. CircuitsizeC: thenumberofbit-wisearithmeticoperationsneededtocalculatetando.
WefurtherconstrainourselvestoRNNswiththefollowingproperties,whichstillcontainallthemodernvariantsof
linear-timeTransformerstothebestofourknowledge.
Definition3.4. WesaythatanRNNarchitectureisregularifC=Θ(P),P=Ω(M),andP=Poly(M).
WenotethattheabovedefinitionofRNNsisgeneralenoughtoalsocontainmanyrecentrecurrentarchitectureusing
streamingcontextwindowsorfiniteKVcache(Xiaoetal.,2023;Kuratovetal.,2024;Bulatovetal.,2022;Orenetal.,
2024).
ForourupperboundonRNNs,wewillconsiderthefollowinglinearRNNs,whichisaspecialcaseofregularRNNs.
Definition3.5(RNNblock). ALinearRNNblockisdefinedasfollows:
f(X)=X+LU(X)+g(X+LU(X)),
wheregisacolumn-wiseReGLUfeed-forwardnetworkwithwidthwandoutputdimensiondandLUisalinearunit,
definedas
h =0,h =Ah +BX ,LU(X )=h .
0 :,t :,t−1 :,t :,1:l :,1:l
Definition3.6(LinearRNN). ALinearRNNisarecurrentneuralnetwork
(cid:16) (cid:17)
R(S)=softmax W(E)(f (...f (Emb(S)))) . (3)
L 1
:,l
wheresoftmaxisthecolumn-wisesoftmaxfunction,f isthei-thLinearRNNblock. Wewillcallthei-thLinearRNN
i
blockthei-thlayeroftheLinearRNNanddenoteitsfeed-forwardlayerandlinearunitlayerasg andLU respectively.
i i
Language Models for Algorithmic Problems. An algorithmic problem is a problem that may be solved by an
algorithm. Inthispaper, wefocusonalgorithmicproblemsf : V+ → V thatasksforcomputingf(S )givena
A in
sequenceoftokensS astheinput,whereV isthesetofpossibleanswers. WesaythatanLMM can(directly)solve
in A
analgorithmictaskf if,giventhesequenceS ,theprobabilitydistributionM(S )forthenexttokenispeakedatthe
in in
correctoutputtokenf(S ),i.e.,argmax M(S )[j]=f(S ).
in j∈V in in
Chain-of-Thought(CoT)reasoningisatechniquethatallowstheLMtoproduceintermediatestepsbeforethefinal
output. WeprimarilyconcernwiththeabilityoflanguagemodelstosolvealgorithmicproblemswithCoT,sincemany
algorithmicproblemsmaybetoochallengingtosolveinjustonestep(Fengetal.,2023;Lietal.,2024)andtestingwith
CoTisnowstandardinthefield(Weietal.,2023;Nyeetal.,2021;Kojimaetal.,2023;Wang&Zhou,2024). Inour
context,wesaythatanLMM cansolveanalgorithmicproblemf withCoTifthefollowingprocessterminateswitha
sequenceendedwithf(S ). First,letS =S . Foralli≥0,decodethenexttokensnext =argmax M(S )[j]
in 0 in i j∈V i
fromM,andappendittothecurrentsequenceS = S ⊕snext. Ifsnext ∈ V ,thentheprocessterminateswith
i+1 i i i A
S withistepsofCoT;otherwisetheprocesscontinues.
i+1
ItisevidentthatifanLMcansolveanalgorithmproblemwith0stepsofCoT,thenanLMM can(directly)solvethe
problem. Inthiscase,wealsosaythattheLMcansolvetheproblemwithoutCoT.
6PREPRINT
4 CanCoTimprovetheRepresentationPowerofRNNs?
Inthissection,weaimtounderstandtherepresentationpowerofRNNswithCoT.Wefirstshowthepositiveresultthat
RNNswithCoTcansolvetasksthatareimpossibleforRNNswithoutCoTfixingthestatesize. Wethenproceedto
understandwhetherCoTcanmakeRNNsasexpressiveasTransformers. Weshowthat,evenwithCoT,RNNsstill
struggletosolveproblemsthatexplicitlyrequirein-contextretrievalandthisrepresentationgappropagatestoseemingly
retrieval-irrelevantreasoningtaskssuchasIsTree. Finally,weshowthatthisgapisindeedone-sided: thereonlyexist
taskswhereTransformersrequireexponentiallylessparametersthanRNNs,butnottheotherwayaround.
4.1 CoTStrictlyImprovesRNNs
Onthepositiveside,weshowthatCoTbroadenstherepresentationpowerofRNNsundermildcomplexityassumption.
Theorem4.1. AssumingPSPACE̸⊂P/Poly,thereexistsataskwithinputlengthnthatcanbesolvedbyaLinear
RNN(Definition3.6)familywithΘ(logn)bitmemorywithpolynomiallengthCOTbutcannotbesolvedbyanyregular
RNN(Definition3.4)familywithΘ(logn)bitmemorywithoutCoT.
WenotethatthisresultisinthesameveinastherecentworkonthebenefitofCoTforTransformers(Fengetal.,
2023; Li et al., 2024), which shows that Transformer with constant size and O(logn) or constant precision can’t
simulateanypolynomialsizecircuitfamilywhileTransformerwithCoTcan. Hereweutilizeadifferentcomplexity
hypothesis,whichintuitivelystatesthatnotallpolynomialspacecomplexityproblems(PSPACE)canbesolvedbya
polynomial-sizecircuitfamily(P/Poly). WeshowthatthishypothesisimpliesthatRNNscanbenefitfromCoTin
solvingreasoningtasks.
ProofSketch. TheintuitionbehindtheproofisthatRNNswithoutCoTareshallowcircuitswithsizePoly(logn).
However, RNNs with CoT can simulate a Turing machine with O(logn) space perfectly within exp(O(logn)) =
Poly(n) steps. Hence, we only need to show that there exists a problem with linear space complexity that cannot
be solved by a polynomial-size circuit family and this can be induced by the (seemingly weaker) assumption that
PSPACE̸⊂P/Poly(seeLemmaB.25).
4.2 CoTCannotClosetheRepresentationGapwithTransformers
Inthissection,weaimtounderstandtherepresentationgapbetweenRNNsandTransformers. WejustifythatRNNs,
evenwithCoT,struggletosolvealgorithmicproblemsthatrequirethecapabilityofretrievinginformationfromthe
currentcontext,whichwecallIn-contextRetrievalforshort. Thislimitationiscausedbythememoryefficiencyof
RNNs: if the memory has at most o(n) bits, then we can involve techniques from streaming complexity to prove
impossibliityresultsforin-contextretrievalproblems.
4.2.1 SimpleProblemsonIn-ContextRetrieval
First,weprovethatRNNshaveasignificantrepresentationgapwithTransformersinsolvingseveralsimplealgorithmic
problemsthatdirectlytestthein-contextretrievalcapability.
Definition4.2(Index). Indexisaproblemthatgivenasequenceoftokenswithlengthnandaquerytokeni∈[n],
requiresthemodeltooutputthetypeofthei-thtoken.
Definition4.3(AssociativeRecall). AssociativeRecall(AR)isaproblemthatgivenasequenceoftokenswithlength
nconsistingoftokensin[n]andaquerytokenq ∈[n],requiresthemodeltooutputthenexttokenofqinthesequence.
Definition4.4(c-gramRetrieval). Anc-gramisacontiguoussubsequenceofctokensinasequenceoftokens. c-gram
retrievalisaproblemthatgivenasequenceoftokenswithlengthnandaquery(c−1)-gramthatistheprefixofa
c-graminthesequence,requiresthemodeltooutputthelasttokenofthatc-gram.
Definition4.5(Counting). Countingisaproblemthatgivenasequenceoftokenswithlengthn,aquerytokenq ∈[n],
andaquerynumbert∈N,requiresthemodeltooutput0or1toindicatewhetherthenumberofoccurrencesofqin
thesequenceisgreaterthant.
Here,IndexandARareperhapsthemostbasicproblemsinretrieval,whereIndexasksforretrievingatokenfrom
theinputsequenceviewedasalineararrayoftokens, andARasksforretrievingatokenfromtheinputsequence
viewedasanassociativearray. Thesetwoproblemshavebeenstudiedextensivelybydifferentcommunities. Indexis
aclassicprobleminstreamingandcommunicationcomplexity(Munro&Paterson,1980),knowntobeimpossible
tosolvewitho(n)bitsofmemoryforstreamingalgorithms. ARhasbeenregardedasafundamentalproblemthat
anartificialintelligencesystemshouldbeabletosolve(Willshawetal.,1969;Hopfield,1982;Hinton&Anderson,
7PREPRINT
2014;Gravesetal.,2014;Baetal.,2016). InthecontextofLLMs,ARhasbeenobservedtocorrelatewithin-context
learningperformance(Elhageetal.,2021)andhasalsobeenusedextensivelyassyntheticsurrogatetasksforpretraining
performance(Fuetal.,2023;Polietal.,2023;Lutatietal.,2023). BesidesIndexandAR,c-gramretrievalisanatural
extensionofARtothecasewherethequerykeycancontainmultipletokens: insteadofretrievingatokengivena
single-tokenkey,c-gramretrievalasksforretrievingatokenwhenthegivenkeyisa(c−1)-gram. Thistaskhasbeen
studiedempirically,butnottheoreticallyinJelassietal.(2024). Countingisaproblemthatasksforthenumberof
occurrencesofatoken,therebytestingthemodel’scapabilitytoretrievesomestatisticsofrelevantinformationfromthe
inputsequence.
ThefollowingtheoremsshowthatRNNswithO(logn)bitmemorycannotsolveanyofthefourtasks,whileTrans-
formerscansolvethemperfectlywithaconstantsizeandO(logn)precision.
Theorem4.6. FortaskT ∈{Index,AR,c-gramretrieval,Counting},thereexistsaTransformerfamilywithconstant
sizeandO(logn)precisionthatcansolveT ofsizen. Ontheotherhand,foranyRNNfamilyRwitho(n)bitmemory,
RcannotsolveT ofsizenwithanylengthofCoTforlargeenoughn.
ProofSketch. ThekeyideaofthelowerboundoftheproofistoputRNNsintotheframeworkofcommunication
complexityanduseinformation-theoreticargumentstoprovealowerbound. RNNshavethefollowingpropertyifparty
AsimulatestheRNNonthefirstpartoftheinputandsendsthestatetopartyB,thenpartyBcansimulatetheRNNon
thesecondpartoftheinputwiththestatereceivedfrompartyAtorecovertheoutputoftheRNNperfectly. Hence,in
theabovetwotheorems,iftheRNNcansolvetheproblemwitho(n)inputsize,thentheinformationabouttheinput
canbecompressedtoo(n)bittoproducetheoutput,whichcontradictstheinformation-theoreticlowerbound.
Fortheupperbound,weshowthattheTransformercansolvetheproblembyutilizinganattentionmechanismcalled
Match that takes the query token and attends to previous keys that match the query token on certain predefined
coordinates. ThismechanismallowstheTransformertoreaditscontextwindowlikeakey-valuedictionaryandhence
cansolvetheproblemsperfectly. Forthecountingproblem,weadditionallyuseaCOUNTattentionmechanismthat
countsthenumberofoccurrencesofthequeriedtokenbyattendingevenlytoeachappearanceofthequeriedtoken.
4.2.2 UnderstandingtheRepresentationPowerofRNNsBeyondSimpleIn-contextRetrievalProblems
Anaturalquestionwouldbeifanalgorithmicproblemdoesnotdirectlytestthein-contextretrievalcapability,can
we hope that RNNs would have the representation power to solve it? Do RNNs and Transformers have the same
representationpowerinthiscase? WeshowthatthelimitedmemorysizeinRNNscanstillbeabottleneckinsolving
algorithmicproblems. Eveniftheretrievalcapabilityisnotexplicitlytestedinanalgorithmicproblem,itmaystillbe
requiredimplicitlyforreasoningabouttheanswer.
Wedemonstratethisgaponaminimalexampleofalgorithmicproblems,calledIsTree: givenanundirectedgraphGof
nnodes,determinewhetherGisatree,i.e.,whethereverypairofnodesisconnectedbyexactlyonesimplepath. A
classicalsolutiontoIsTreeisrunningDepthFirstSearch(DFS),whichtakesO(n)time.
Inthecontextoflanguagemodeling,wecanwritethegraphGasasequenceoftokens,andthenthetaskofIsTreeisto
determinewhetherGisatreebypredictingaYES/NOtokenwithorwithoutCoT.Weusethefollowingtokenization
forthegraphG:
Tokenize(G)={<s>,u ,∼,v ,u ,∼v ,...,u ,∼,v }, (4)
1 1 2 2 m m
where<s>and∼aretwospecialtokensrepresentingthestartofthesentenceandanedge,andu ,v arenumbers
i i
denotingthenodesofthegraph.
Our result states that RNN with o(n) bit memory cannot solve IsTree, even with an arbitrary choice of chain of
thought. Ontheotherhand,thereexistsaTransformerwithconstantsizeandO(logn)precisionthatcangeneratea
chain-of-thoughtoflengthnfollowingDFSandperfectlysolvethesamequestion.
Theorem4.7. ForanyRNNfamilyRwitho(n)bitmemory,RcannotperfectlysolveIsTreeofsizenforlargeenough
n,withanylengthofCoT.Ontheotherhand,thereexistsaTransformerT withconstantdimensionanddepth,and
1
O(logn)precisionthatcansolveIsTreeofsizenperfectlywithChainofThoughtoflengthO(n).
ProofSketch. ThemainideaoftheproofisthatthetaskofIsTreerequiresthemodeltoreasonabouttheglobal
structureofthegraph,whichisbeyondthecapabilityofRNNswithlimitedmemory. Weprovethelowerboundby
constructingagraphfromabinarysequenceandshowingthatRNNswitho(n)memorycannotsolvetheproblembya
reductiontoaninformation-theoreticlowerbound. Fortheupperbound,weshowthattheTransformercansimulatethe
DFSalgorithmbyoutputtingtheEulertouroftheconnectedcomponentsofvertex1andthencheckthelengthofthe
EulertourwithitscapabilityofIn-contextRetrieval.
8PREPRINT
Thekeyideaofthelowerboundoftheproofistoagainutilizetheinformation-theoreticlowerbound. Thisidealies
inthecoreofstreamingcomplexityliteratureandinvestigationontheIsTreeproblemdatesbacktoHenzingeretal.
(1998). Weherebyrestatetheproofforcompleteness. Givenanybinarysequencexoflengthn−2andanindex
k ∈[n−3],wecanconstructagraphasfollows: thegraphhasnnodes,andvertexaisconnectedtovertexx +n−1
a
foranya ∈ [n−2]. Moreover,vertexk isconnectedtovertexk+1. Thegraphisatreeifandonlyifx ̸= x .
k k+1
NowassumingthereisanRNNwitho(n)memorythatcansolveIsTree,considertwopartiesAandBeachholding
BinaryMessage: 0101,Index: 2 EulerTour: 1,5,3,2,6,4,6,2,3,5,1,IsTree: Yes
8
3
v v v v v v v v
1 2 3 4 1 2 3 4
1 4 9
Tokenize(G) 6
=<s>,2,∼,6,1,∼,5,
2 5
3,∼,5,4,∼,6,2,∼,3. 10 7
v v v v
5 6 5 6
G DFSonG
Figure 3: An example of the graph constructed from the binary sequence x = 0101 and the index k = 2 and the
correspondingDFStour.
the sequence x and the index k, they can construct two parts of the graph using their information and then A can
simulateRNNonthefirstpartofthegraphandsendthestatetoB,andBcansimulateRNN(potentiallywithCoT)on
thesecondpartofthegraphtorecovertheoutputoftheIsTreeproblem,whichisequivalenttowhetherx ̸=x .
k k+1
However,notethatkisneversenttoA,andhenceactuallyBcangetwhetherx ̸=x foranyk ∈[n−3],which
k k+1
contradictstheinformation-theoreticlowerbound.
Nowfortheupperbound,wewilllettheTransformersimulatetheDFSalgorithmbyoutputtingtheEulertourofthe
connectedcomponentsofvertex1andthencheckthelengthoftheEulertour(seeAlgorithm1). Tosimulatethetour,
wewillimplementtwofunctionsthroughtheTransformerBlock:
1. Givenaprefixofthetour,findtheparentofthelastvertexinthetour. Thiscanbeimplementedbycopyingeach
token’spredecessor’stypetothattokenandthenusingtheMatchmechanismtomatchthefirstoccurrenceofthe
currenttokeninthesequence.
2. Giventhetokenizedinputgraphandanedge(u,v),findthenextedgeafter(u,v)containingvintheinputgraph.
WewilluseanotherattentionmechanismcalledCOUNTtocount, foreachedgee = (a,b)intokenizedinput
graph,thenumberofoccurrencesofaandbuptothatedgeandstore1/(n +1)and1/(n +1)inthetoken
e,a e,b
correspondingtotheedge,wheren andn arethecorrespondingcounts. Thengiventheedge(u,v),wecan
e,a e,b
usetheMatchmechanismtofind1/(n +1). Thenwewilluseafeed-forwardlayerwithgatedreluactivation,
(u,v),v
constantdepth,andconstantwidthtoapproximate1/(n +2)andthenusetheMatchmechanismagainto
(u,v),v
findthenextedgecontainingv.
Throughtheabovetwofunctions,theTransformercansimulatetheDFSalgorithmandhencesolvetheIsTreeproblem
perfectly.
4.2.3 TransformersareStrictlyMoreExpressiveThanRNNs
TheabovetheoremsshowtheexistenceoftaskswhereTransformersrequireexponentiallylessmemorythanRNNs.
However,theyhavenotruleoutthepossibilitythatthereexistsacorrespondingtaskwheretheTransformerwillbe
moreredundantandrequireexponentiallymoreparametersthanRNNs. However,thefollowingtheoremconfirmsthat
suchataskdoesn’texistforregularRNN(Definition3.4).
The theorem is in the same vein as the recent work on the CoT for Transformer (Li et al. (2024)), which shows
theconstantsizeandconstantprecisionTransformerwithapolynomial-sizepositionembeddingcansimulateany
polynomialsizecircuit. Themajordifferenceofourtheoremisthat(1)weconsideraTransformerwithfixedword
9PREPRINT
andpositionembedding,henceallowingtheparameternumbertobelogarithmicintheinputsize,and(2)weconsider
simulatingRNNs,whichisaspecialkindofcircuitfamilyandhencewecanusemoresuccinctrepresentationutilizing
thestructuralpropertyattachedtotherecursiveprocess.
Theorem4.8. GivenanyconstantA>0,constantwordwidthandnumberofspecialsymbolsd,n >0,foranyn,
S
precisionp=Θ(Alogn)andRNNRwithwordembeddingW(E) ∈R(n+nS)×dsuchthateachrecurrentiterationcan
p
becalculatedwithacircuitwithsizeP(n)≤2p/2,thereexistsaTransformerT withO(P(n)logmax{P(n),n})bit
parameterandwordembedding(cid:2) W(E) 0(n+nS)×d(cid:3) thatcansimulatetheRNNwithatmostnAstepchain-of-thought
precisely,usingatmost(P(n)+1)nAstepchainofthoughtoneveryinputwithlengthn.
ProofSketch. ThekeyideaistoencodetheRNNcircuitintotheTransformer’sweightandsimulatethecircuitgate
bygate,utilizingtheMatchmechanismtofetchtheinputofeachgate. Inthismanner,althoughthenaivecircuitto
simulatetheRNNfornAstepswouldrequireO(nA)parameter,theTransformeronlyneedstostoreoneinstanceofthe
RNNcircuitinitsweightandhenceweonlyneedO(P(n)logmax{P(n),n})parameter,whichisatmostpolynomial
intheparametersizeoftheRNN.
5 EnhancingtheIn-contextRetrievalCapabilityClosestheRepresentationGap
InSection4.2,weshowthatRNNsaredeficientatIn-contextRetrieval,henceleadingtoasignificantrepresentation
gapwithTransformers. Inthissection, weaimtounderstand: ifweenhancetheIn-contextRetrievalcapabilityof
RNNs,doRNNsremaintohaveanyrepresentationgapwithTransformers? Weanswerthisquestionbyconsidering
anexplicitandanimplicitwaytoenhancetheIn-contextRetrievalcapacityandshowingthatbothwayscanclosethe
representationgapbetweenRNNsandTransformersinsolvingalgorithmicproblems.
Retriever
Context SearchQuery SearchResult RNN ... RNN RNN ... RNN RNN
StartSearch EndSearch
RNN ... RNN RNN RNN ... RNN RNN RNN ... RNN Attention
StartSearch EndSearch
TheretrievalaugmentedRNN Thehybridarchitecture
Figure4: In-contextRAG. TheretrievalaugmentedRNN(left)andthehybridarchitecture(right)closetherepresenta-
tiongapbetweenRNNsandTransformers.
5.1 ExplicitRetrievalThroughRegularExpression
First,weexplorethepowerofRNNswithRetrievalAugmentedGeneration(RAG),whichgivesanLMthecapability
toretrieverelevantinformationtoassistgeneration. Inourcontext,wearespecificallyinterestedinallowingLMsto
callfunctionstoretrieveinformationfromtheircontext,whichwecallIn-contextRetrievalAugmentedGeneration
(In-contextRAG).
Wewillfirstshowthataddingfunctioncallstoassociativerecallisnotenoughtoclosetherepresentationgapbetween
RNNsandTransformers.
Proposition5.1. ForanyRNNfamilywithO(logn)bitmemoryandO(logn)parameterwithanoracletoreceive
resultsfortheARproblem(Definition4.3)foranyqueries,forlargeenoughn,theRNNcan’tsolvetheindexproblem
(Definition4.2)withlengthninanyCoTsteps.
Proof. Consideraspecialtypeofindexproblemwhereeverytokenattheevenpositionoftheinputsequenceisa
specialtokenκandtherestofthetokensareuniformlyrandom. ThentheoraclefortheARproblemcanbesimulated
bytheRNNbysimplyoutputtingtheκwhenthequeryisnotκandoutputtingthethirdtokenwhenthequeryisκ.
However,followingsimilarproofofTheorem4.6,wecanshowthattheRNNcan’tsolvethisspecialformofindex
problemwithlengthninanyCoTsteps.
10PREPRINT
In this light, we need to consider a more general form of In-contextRetrieval capability. We specifically consider
a special form of In-contextRAG that enables an LM to perform regular expression matching because the regular
expressionisaflexibleprimitivethatcanbeusedtodescribeawiderangeofretrievaltasksandcanbeimplemented
efficientlyonmodernhardware.
GivenanLMM withvocabularyV (containingtwoadditionalspecialtokens,<StartSearch>and<EndSearch>)and
thetokenizedinputsequenceS
in
∈|V|l0,theLMM withIn-contextRAGgeneratesfollowingsequenceoftokenized
sequence:
S =S , snext =argmaxM(S )[j],
0 in i i
j∈V
(cid:26) S ⊕snext, ifsnext ̸=<EndSearch>
S = i i i
i+1 S ⊕snext⊕RETRIEVE(S ), otherwise.
i i i
HereRETRIEVElooksforthelastoccurrenceof<StartSearch>atpositionl and<EndSearch>inS atposition
s
l andtreatDetokenize(S )asaregularexpression,whereDetokenizemapsthetokenizedsequencebacktothe
e ls:le
string,insertingaspacebetweeneverypairofadjacenttokens. Thealgorithmthenrunsaregularexpressionmatching
onDetokenize(S ),findsthefirstmatchingsubstring,andreturnsthefirstcapturinggroupaccordingtotheregular
1:ls−1
expression(i.e.,contentembracedbyapairbracketintheregularexpression). Whiletherearemanygrammarstandards
ofregularexpressions,weadheretothestandardspecifiedintherelibraryofPython.Thatis,weevaluatethefollowing
Pythoncodetogettheresultoftheregularexpressionmatching:
re.search(pattern, string).group(1)
whereDetokenize(S )isthepatternandDetokenize(S )isthestring.
ls:le 1:ls−1
First, we show that In-contextRAG with regular expression is powerful enough for RNNs to solve the two
In-contextRetrievalproblemsinSection4.2.1inO(1)CoTsteps.
Theorem5.2. FortaskT ∈{Index,AR,c-gramretrieval,Counting},thereexistsaLinearRNNfamilywithO(logn)
bitmemoryandO(logn)parameter,thatcansolveT withIn-contextRAGinO(1)CoTsteps.
ProofSketch. FortheIndexproblem,lettheRNNoutputtheregularexpression^(?:\S\s*){a}(\S),where
a=k−1. ForAR,lettheRNNoutput\bq\b(\S+)\b,whereqisthenumberinquery. Forc-gramretrieval,letthe
RNNoutput\bq ... q \b(\S+)\b,whereq isthei-thnumberinthequery. ForCounting,lettheRNNoutput
1 c−1 i
(\bv\b){k+1},wherevisthequerytokenandkisthequerythreshold.
BeyondthesesimpleIn-contextRetrievalproblems,inTheorem4.7,wehaveshownthatRNNscannotsolveIsTree
duetoitsimplicitrequirementofIn-contextRetrievalcapability. WenowshowthatIn-contextRAGcanhelplinear
RNNsolveIsTreeinO(n)CoTsteps. SeeAppendixB.9fortheproof.
Theorem5.3. ThereexistsaLinearRNNfamilywithO(logn)bitmemoryandO(logn)parameter,thatcansolve
IsTreeofsizenwithIn-contextRAGinO(n)CoTsteps.
Tofurthershowthepoweroftheexplicitretrieval,thefollowingtheoremfurtherprovesageneralresult,showingthat
In-contextRAGempowersRNNswithO(logn)bitmemorytosimulatepolynomial-timeTuringmachines.
Theorem5.4. GivenanyconstantA,B,foranypolynomial-timeTuringmachineT ∈TIME(nA)withBstatesand
vocabularysizeB,thereexistsaretrievalaugmentedLinearRNNfamily(seeDefinitions3.6andA.5)withvocabulary
ofBspecialsymbol,O(Alogn)bitprecisionandmemory,andO(AB2logn)bitparameters,thatcansimulatethe
resultofT onanyinputwithlengthnwithachainofthoughtoflengthO(nA).
ProofSketch. TheretrievalaugmentedRNNcansimulatetheTuringmachinebymaintainingthestateoftheTuring
machineandthepositionofthetapeheadinitsmemoryandwritingdownthetapeinthecontextintheformofcvcto
indicatethevalueofthetapeatpositioncisupdatedtov. GiventheinputtapeTAPE[1,i],theretrievalaugmented
RNNwillfirstwritedowntheinitialtapeinthepreviousformofiTAPE[1,i]iusingtheregularexpressionusedin
theIndexproblem. TheRNNcanthengeneratethesearchquerieswithformsofp (.) p .*?$withpbeingthe
pointertoretrievetheinformationfromthecontext.
Asafinalnote,ourfocushereistounderstandtherepresentationpowerofRNNsgivenanappropriateRAG,butnotto
proposeamethodthatimmediatelyleadstopracticalapplications. WhiletheaboveresultsshowthatIn-contextRAG
canclosetherepresentationgapbetweenRNNsandTransformersinsolvingalgorithmicproblems,alimitationhereis
thatIn-contextRAGisnotanimmediatepracticalsolution,asthereisnoexistingtrainingdataforthisIn-contextRAG.
11PREPRINT
5.2 ImplicitRetrievalbyAppendingJustOneTransformerLayer
SinceBahdanauetal.(2016),attentionmechanismshavebeenunderstoodasaformofcompensationforthefixed
memorysizeofRNNs,allowingthemodeltoattendtotheentirecontext. Weshowinthissectionformallythatthis
formofimplicitretrievalcanclosetherepresentationgapbetweenRNNsandTransformersinsolvingalgorithmic
problems. Weconsiderthefollowinghybridarchitecture,whichcombinestheRNNandtheTransformerbyappending
asingleTransformerlayertotheRNNoutput.
Definition5.5(HybridRNN). AhybridRNNisamodelthatconsistsofanRNNwithtransitionandoutputfunction
t,oandoneTransformerlayerf,theoutputoftheRNNisusedastheinputoftheTransformerlayerandtheoutputof
theTransformerlayerisusedtoproducethenexttoken. Concretely,giventheinputsequenceS ,theoutputofthe
in
hybridarchitectureis:
H(S)=softmax(cid:16) W(E)f(cid:0)
[o(s )]
(cid:1)(cid:17)
,
k k∈[l]
:,l
∀k ∈[l],s =t(s ,Emb(S) ),
k k−1 :,k
First,weshowthathybridRNNscansolvetheIn-contextRetrievalproblemsinSection4.2.1withoutCoT.
Theorem5.6. FortaskT ∈{Index,AR,c-gramretrieval,Counting},thereexistsahybridLinearRNN(Definitions3.6
and5.5)familywithO(logn)bitmemoryandO(logn)parameter,thatcansolveT withoutCoT.
ProofSketch. TheproofissimilartotheproofofTheorem4.6,usingtheappendedTransformerlayertosimulatethe
MatchfunctionandCOUNTfunctionintheRNN.
SimilartothesituationinSection5.1,theimplicitretrievalmethodcanenpowerthehybridlinearRNNtosolveIsTree
withCoT.SeeAppendixB.12fortheproof.
Theorem5.7. ThereexistsahybridLinearRNNwithO(logn)bitmemoryandO(logn)parameter,thatcansolve
IsTreeofsizenwithachainofthoughtoflengthO(nlogn). Moreover,thehybridRNNcansolvetheIsTreeproblem
definedonbinarysequence(seeproofofTheorem4.7)withoutCoT.
Further,weshowthatthishybridarchitecturewithonlyoneattentionblockispowerfulenoughtoevensimulateany
polynomial-timeTuringmachinewithCoT.
Theorem 5.8. Given any constant A,B, for any polynomial-time Turing machine T ∈ TIME(nA) with B states
andvocabularysizeB,thereexistsahybridLinearRNN(seeDefinition5.5)withvocabularyofB specialsymbol,
O(Alogn)bitprecisionandmemory,andO(AB2logn)bitparameters,thatcansimulatetheresultofT onanyinput
withlengthninO(nA)CoTsteps.
ProofSketch. TheproofissimilartotheproofofTheorem5.4. Insteadofusingregularexpressionstoretrievethe
informationfromthecontext, thehybridarchitecturecanusetheattentionmechanismintheTransformerlayerto
implementtheMatchfunctiontoretrievetheinformationfromthecontext.
6 Experiments
We tested our theory on the IsTree task. To generate the graph, we follow the procedure described in the proof
of Theorem 4.7 (see Figure 3). The CoT data is generated using Algorithm 1 and the retrieval data is generated
usingAlgorithm2. FortheCoTmodel,wedecodethereasoningpathduringinferencetimeuntilwereachthefirst
YESorNOuptoamaxtokenlimitgreaterthanthelengthofallgroundtruthCoT.Forthedatapointsthatthemodel
failstogiveaprediction,weassumethemodelgetsitcorrectwith0.5probability. Fortheretrievaltask,weomitthe
explicitformatoftheregularexpressionandonlyaskthemodeltogeneratetheverticesandspecialtokensintheregular
expressiontoshortenthelengthoftheinputsequence. Thereportedaccuracyiscalculatedoveravalidationsetof5000
samplesusingthelastiterationofthemodel.
Wetrainthreedifferentarchitectures: (1)LLaMAarchitecture(Touvronetal.,2023)representingTransformers,(2)
Mamba architecture (Gu & Dao, 2023) representing RNNs, and (3) Mamba with one additional layer of LLaMA
blockrepresentinghybridarchitectures. Followingourtheory,wefreezeandweight-tiethepredictionheadandword
embedding in all the models. For ease of training, we use a different embedding function mapping i−th token to
[sin( i ),cos( i )] withN beingthenumberofdifferenttokensandusestandardRoPE(Suetal.,
10000j/d 10000j/d j∈[d/2]
2024)aspositionembedding. Wetraineverymodelwithatleast1MsamplestoguaranteeconvergenceusingAdam
withacosinelearningrate. Ifthemodeldoesn’tconverge, weretrainusing5Msamples. Afteragridsearchover
learningrates,wetrainalltheTransformermodelswithlearningrates1e-3andtherestofthemodelswithlearning
rates3e-4.
12PREPRINT
Figure5: WetrainMambawithoneadditionallayerofattentionwithafrozenwordembeddinganddecodingheadof
threedifferentmodelsizes(0.5M,1M,2M)onIsTreewiththreedifferentsizesofgraph(16,32,64)underthreedifferent
setups. Vanillameansthemodeldirectlypredictsthelabel. COTmeansthemodelwillgenerateachain-of-thought
processbasedonDFS(seeAlgorithm1)beforeprediction. Retrievalmeansthemodelwillgeneratethechainofsearch
queriesandreasoningbeforeprediction(seeAlgorithm2). Weobservethatallmodelsreachnear-perfectaccuracyon
thevalidationset.
TheresultsareshowninFigures3and5. Weobservethat:
1. CoT improves the performance for both Transformers and RNNs. However, the RNNs’ performance degrades
sharplyasthegraphsizeincreasesandtheTransformersconsistentlyoutperformstheRNNs. Thisisconsistentwith
ourtheorythatCoTcanimprovetheexpressivenessoftheRNNmodelsbuttheexpressivenessisstillnotenoughto
solvetheIsTreetask(seeTheorems4.1and4.7).
2. RetrievalAugmentationviaregularexpressionallowsallthemodelstoreachnear-perfectaccuracy.Thisisconsistent
withourtheorythatretrievalaugmentationviaregularexpressioncanimprovetheexpressivenessoftheRNNmodels
tosolvealgorithmictasks(seeTheorems5.3and5.4).
3. Thehybridmodelshowsthebestperformanceamongallthemodels,reachingnear-perfectaccuracyevenwithout
CoTorexplicitretrievalaugmentation,whichisalsoreflectedinthetheoreticalresults(seeTheorems5.7and5.8).
7 ConclusionandDiscussion
This paper studies the representation gap between RNNs and Transformers on algorithmic problems. It is proved
thatCoTcanimproveRNNsbutisinsufficienttoclosethegapwithTransformers: theinabilityofRNNstoperform
in-contextretrievalisakeybottleneck. Toaddressthis,weshowthatadoptingIn-ContextRAGorappendingasingle
TransformerlayertoRNNscanenhancetheirin-contextretrievalcapabilitiesandclosetherepresentationgapwith
Transformers.
OnelimitationofthisworkisthatthesolutionofIn-ContextRAGthroughregularexpressionisforthepurposeof
understandingthebottleneckoftherepresentationpowerofRNNs,andmaynotbeapracticalmethodbeyondIsTree
since there is no existing training data for this type of RAG. Effectively enabling or eliciting LLMs’ capability to
perform in-context RAG or other types of RAG is an interesting direction for future work. Second, appending a
singleTransformerlayertoRNNsisaminimalexampleofmakingarchitecturalchangestoRNNstoimprovetheir
representationpowerwhilemarginallyincreasingthememorycost.Itisleftunexploredwhatotherarchitecturalchanges
canposeasimilareffectorenjoyabettertrade-offbetweenrepresentationpowerandmemoryefficiency. Finally,we
onlystudytheaspectofrepresentationpower,anddonotanalyzethetrainingdynamicsandgeneralizationofRNNs
andTransformers. Webelievethisisthemostchallengingbutimportantdirectionforfuturework.
13PREPRINT
References
EkinAkyürek,BailinWang,YoonKim,andJacobAndreas. In-contextlanguagelearning:Architecturesandalgorithms,
2024.
SilasAlberti,NiclasDern,LauraThesing,andGittaKutyniok. Sumformer: Universalapproximationforefficient
transformers,2023.
NogaAlon,YossiMatias,andMarioSzegedy. Thespacecomplexityofapproximatingthefrequencymoments. In
Proceedingsofthetwenty-eighthannualACMsymposiumonTheoryofcomputing,pp.20–29,1996.
SimranArora,SabriEyuboglu,AmanTimalsina,IsysJohnson,MichaelPoli,JamesZou,AtriRudra,andChristopher
Ré. Zoology: Measuringandimprovingrecallinefficientlanguagemodels. arXivpreprintarXiv:2312.04927,2023.
JimmyBa,GeoffreyEHinton,VolodymyrMnih,JoelZLeibo,andCatalinIonescu. Usingfastweightstoattendtothe
recentpast. Advancesinneuralinformationprocessingsystems,29,2016.
DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointlylearningtoalignand
translate,2016.
SatwikBhattamishra,KabirAhuja,andNavinGoyal. Ontheabilityandlimitationsoftransformerstorecognizeformal
languages,2020.
SebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMillican,Georgevanden
Driessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,DiegodeLasCasas,AureliaGuy,JacobMenick,
RomanRing,TomHennigan,SaffronHuang,LorenMaggiore,ChrisJones,AlbinCassirer,AndyBrock,Michela
Paganini,GeoffreyIrving,OriolVinyals,SimonOsindero,KarenSimonyan,JackW.Rae,ErichElsen,andLaurent
Sifre. Improvinglanguagemodelsbyretrievingfromtrillionsoftokens,2022.
AydarBulatov,YuriKuratov,andMikhailS.Burtsev. Recurrentmemorytransformer,2022.
NelsonElhage,NeelNanda,CatherineOlsson,TomHenighan,NicholasJoseph,BenMann,AmandaAskell,Yuntao
Bai,AnnaChen,TomConerly,NovaDasSarma,DawnDrain,DeepGanguli,ZacHatfield-Dodds,DannyHernandez,
AndyJones,JacksonKernion,LianeLovitt,KamalNdousse,DarioAmodei,TomBrown,JackClark,JaredKaplan,
SamMcCandlish,andChrisOlah. Amathematicalframeworkfortransformercircuits. TransformerCircuitsThread,
2021. https://transformer-circuits.pub/2021/framework/index.html.
GuhaoFeng,BohangZhang,YuntianGu,HaotianYe,DiHe,andLiweiWang. Towardsrevealingthemysterybehind
chainofthought: Atheoreticalperspective. InThirty-seventhConferenceonNeuralInformationProcessingSystems,
2023. URLhttps://openreview.net/forum?id=qHrADgAdYu.
DanielY.Fu,TriDao,KhaledK.Saab,ArminW.Thomas,AtriRudra,andChristopherRé. Hungryhungryhippos:
Towardslanguagemodelingwithstatespacemodels,2023.
AlexGraves,GregWayne,andIvoDanihelka. Neuralturingmachines. arXivpreprintarXiv:1410.5401,2014.
AlbertGuandTriDao. Mamba: Linear-timesequencemodelingwithselectivestatespaces,2023.
AlbertGu, KaranGoel, andChristopherRe. Efficientlymodelinglongsequenceswithstructuredstatespaces. In
InternationalConferenceonLearningRepresentations,2022. URLhttps://openreview.net/forum?id=
uYLFoz1vlAC.
KelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMing-WeiChang. Realm: Retrieval-augmentedlanguage
modelpre-training,2020.
MichaelHahn. Theoreticallimitationsofself-attentioninneuralsequencemodels. TransactionsoftheAssociation
forComputationalLinguistics,8:156–171,December2020. ISSN2307-387X. doi: 10.1162/tacl_a_00306. URL
http://dx.doi.org/10.1162/tacl_a_00306.
Jie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, and Zhaopeng Tu. Modeling recurrence for
transformer. InJillBurstein,ChristyDoran,andThamarSolorio(eds.),Proceedingsofthe2019Conferenceofthe
NorthAmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies,Volume
1(LongandShortPapers),pp.1198–1207,Minneapolis,Minnesota,June2019.AssociationforComputational
Linguistics. doi: 10.18653/v1/N19-1122. URLhttps://aclanthology.org/N19-1122.
14PREPRINT
YidingHao,DanaAngluin,andRobertFrank.Formallanguagerecognitionbyhardattentiontransformers:Perspectives
fromcircuitcomplexity,2022.
Monika Rauch Henzinger, Prabhakar Raghavan, and Sridhar Rajagopalan. Computing on data streams. External
memoryalgorithms,50:107–118,1998.
GeoffreyEHintonandJamesAAnderson. Parallelmodelsofassociativememory: updatededition. Psychologypress,
2014.
JohnJHopfield. Neuralnetworksandphysicalsystemswithemergentcollectivecomputationalabilities. Proceedings
ofthenationalacademyofsciences,79(8):2554–2558,1982.
SamyJelassi,DavidBrandfonbrener,ShamMKakade,andEranMalach. Repeatafterme: Transformersarebetterthan
statespacemodelsatcopying. arXivpreprintarXiv:2402.01032,2024.
ZhengbaoJiang,LuyuGao,ZhiruoWang,JunAraki,HaiboDing,JamieCallan,andGrahamNeubig. Retrievalasatten-
tion: End-to-endlearningofretrievalandreadingwithinasingletransformer. InYoavGoldberg,ZornitsaKozareva,
andYueZhang(eds.),Proceedingsofthe2022ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,
pp.2336–2349,AbuDhabi,UnitedArabEmirates,December2022.AssociationforComputationalLinguistics. doi:
10.18653/v1/2022.emnlp-main.149. URLhttps://aclanthology.org/2022.emnlp-main.149.
AngelosKatharopoulos,ApoorvVyas,NikolaosPappas,andFrançoisFleuret. TransformersareRNNs:Fastautoregres-
sivetransformerswithlinearattention. InHalDauméIIIandAartiSingh(eds.),Proceedingsofthe37thInternational
Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 5156–5165.
PMLR,13–18Jul2020. URLhttps://proceedings.mlr.press/v119/katharopoulos20a.html.
TakeshiKojima,ShixiangShaneGu,MachelReid,YutakaMatsuo,andYusukeIwasawa. Largelanguagemodelsare
zero-shotreasoners,2023.
Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev. In search of
needlesina11mhaystack: Recurrentmemoryfindswhatllmsmiss,2024.
Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic structure: Towards a mechanistic
understanding,2023.
ZhiyuanLi,HongLiu,DennyZhou,andTengyuMa. Chainofthoughtempowerstransformerstosolveinherently
serialproblems,2024.
ZhongLi,JiequnHan,WeinanE,andQianxiaoLi.Onthecurseofmemoryinrecurrentneuralnetworks:Approximation
and optimization analysis. In International Conference on Learning Representations, 2021. URL https://
openreview.net/forum?id=8Sqhl-nF50.
ZhongLi,HaotianJiang,andQianxiaoLi. Ontheapproximationpropertiesofrecurrentencoder-decoderarchitectures.
InInternationalConferenceonLearningRepresentations,2022. URLhttps://openreview.net/forum?
id=xDIvIqQ3DXD.
Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts
to automata. In The Eleventh International Conference on Learning Representations, 2023. URL https://
openreview.net/forum?id=De4FYqjFueZ.
ShengjieLuo,ShandaLi,TianleCai,DiHe,DinglanPeng,ShuxinZheng,GuolinKe,LiweiWang,andTie-YanLiu.
Stable,fastandaccurate: Kernelizedattentionwithrelativepositionalencoding,2021.
ShaharLutati,ItamarZimerman,andLiorWolf. Focusyourattention(withadaptiveiirfilters),2023.
William Merrill and Ashish Sabharwal. The Parallelism Tradeoff: Limitations of Log-Precision Transformers.
Transactions of the Association for Computational Linguistics, 11:531–545, 06 2023. ISSN 2307-387X. doi:
10.1162/tacl_a_00562. URLhttps://doi.org/10.1162/tacl_a_00562.
WilliamMerrill,AshishSabharwal,andNoahA.Smith. Saturatedtransformersareconstant-depththresholdcircuits,
2022.
JIanMunroandMikeSPaterson. Selectionandsortingwithlimitedstorage. Theoreticalcomputerscience,12(3):
315–323,1980.
15PREPRINT
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David
Dohan,AitorLewkowycz,MaartenBosma,DavidLuan,CharlesSutton,andAugustusOdena. Showyourwork:
Scratchpadsforintermediatecomputationwithlanguagemodels,2021.
MatanelOren,MichaelHassid,YossiAdi,andRoySchwartz. Transformersaremulti-staternns,2024.
Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, and
DimitrisPapailiopoulos. Canmambalearnhowtolearn? acomparativestudyonin-contextlearningtasks,2024.
BoPeng,EricAlcaide,QuentinAnthony,AlonAlbalak,SamuelArcadinho,StellaBiderman,HuanqiCao,XinCheng,
MichaelChung,MatteoGrella,KranthiKiranGV,XuzhengHe,HaowenHou,JiajuLin,PrzemyslawKazienko,Jan
Kocon,JiamingKong,BartlomiejKoptyra,HaydenLau,KrishnaSriIpsitMantri,FerdinandMom,AtsushiSaito,
GuangyuSong,XiangruTang,BolunWang,JohanS.Wind,StanislawWozniak,RuichongZhang,ZhenyuanZhang,
QihangZhao,PengZhou,QinghuaZhou,JianZhu,andRui-JieZhu. Rwkv: Reinventingrnnsforthetransformer
era,2023.
HaoPeng,NikolaosPappas,DaniYogatama,RoySchwartz,NoahA.Smith,andLingpengKong. Randomfeature
attention,2021.
MichaelPoli,StefanoMassaroli,EricNguyen,DanielY.Fu,TriDao,StephenBaccus,YoshuaBengio,StefanoErmon,
andChristopherRé. Hyenahierarchy: Towardslargerconvolutionallanguagemodels,2023.
OhadRubinandJonathanBerant. Long-rangelanguagemodelingwithself-retrieval,2023.
ClaytonSanford,DanielHsu,andMatusTelgarsky. Representationalstrengthsandlimitationsoftransformers,2023.
ClaytonSanford,DanielHsu,andMatusTelgarsky. Transformers,parallelcomputation,andlogarithmicdepth,2024.
JianlinSu,MurtadhaAhmed,YuLu,ShengfengPan,WenBo,andYunfengLiu. Roformer: Enhancedtransformerwith
rotarypositionembedding. Neurocomputing,568:127063,2024.
YutaoSun,LiDong,ShaohanHuang,ShumingMa,YuqingXia,JilongXue,JianyongWang,andFuruWei. Retentive
network: Asuccessortotransformerforlargelanguagemodels,2023.
YuandongTian,YipingWang,BeidiChen,andSimonDu. Scanandsnap: Understandingtrainingdynamicsandtoken
compositionin1-layertransformer,2023.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanBikel,LukasBlecher,CristianCantonFerrer,MoyaChen,
Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj
Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor
Kerkez,MadianKhabsa,IsabelKloumann,ArtemKorenev,PunitSinghKoura,Marie-AnneLachaux,ThibautLavril,
JenyaLee, DianaLiskovich, YinghaiLu, YuningMao, XavierMartinet, TodorMihaylov, PushkarMishra, Igor
Molybog,YixinNie,AndrewPoulton,JeremyReizenstein,RashiRungta,KalyanSaladi,AlanSchelten,RuanSilva,
EricMichaelSmith,RanjanSubramanian,XiaoqingEllenTan,BinhTang,RossTaylor,AdinaWilliams,JianXiang
Kuan,PuxinXu,ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,MelanieKambadur,SharanNarang,Aurelien
Rodriguez,RobertStojnic,SergeyEdunov,andThomasScialom. Llama2: Openfoundationandfine-tunedchat
models,2023.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,and
Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus,
S.Vishwanathan,andR.Garnett(eds.),AdvancesinNeuralInformationProcessingSystems,volume30.CurranAs-
sociates,Inc.,2017. URLhttps://proceedings.neurips.cc/paper_files/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
SinongWang,BelindaZ.Li,MadianKhabsa,HanFang,andHaoMa. Linformer: Self-attentionwithlinearcomplexity,
2020.
XuezhiWangandDennyZhou. Chain-of-thoughtreasoningwithoutprompting,2024.
JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,BrianIchter,FeiXia,EdChi,QuocLe,andDennyZhou.
Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels,2023.
DavidJWillshaw,OPeterBuneman,andHughChristopherLonguet-Higgins. Non-holographicassociativememory.
Nature,222(5197):960–962,1969.
16PREPRINT
GuangxuanXiao,YuandongTian,BeidiChen,SongHan,andMikeLewis. Efficientstreaminglanguagemodelswith
attentionsinks,2023.
KaiYang,JanAckermann,ZhenyuHe,GuhaoFeng,BohangZhang,YunzhenFeng,QiweiYe,DiHe,andLiweiWang.
Doefficienttransformersreallysavecomputation?,2024.
ShunyuYao,BinghuiPeng,ChristosPapadimitriou,andKarthikNarasimhan. Self-attentionnetworkscanprocess
boundedhierarchicallanguages,2023.
17PREPRINT
Contents
1 Introduction 1
2 RelatedWorks 3
3 Preliminaries 5
4 CanCoTimprovetheRepresentationPowerofRNNs? 7
4.1 CoTStrictlyImprovesRNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4.2 CoTCannotClosetheRepresentationGapwithTransformers . . . . . . . . . . . . . . . . . . . . . 7
4.2.1 SimpleProblemsonIn-ContextRetrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4.2.2 UnderstandingtheRepresentationPowerofRNNsBeyondSimpleIn-contextRetrievalProblems 8
4.2.3 TransformersareStrictlyMoreExpressiveThanRNNs . . . . . . . . . . . . . . . . . . . . . 9
5 EnhancingtheIn-contextRetrievalCapabilityClosestheRepresentationGap 10
5.1 ExplicitRetrievalThroughRegularExpression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
5.2 ImplicitRetrievalbyAppendingJustOneTransformerLayer . . . . . . . . . . . . . . . . . . . . . . 12
6 Experiments 12
7 ConclusionandDiscussion 13
A AdditionalDefinitions 20
A.1 ReasoningTasksonGraphs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
A.2 MoreonNumericPrecisions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
A.3 Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
A.4 LanguageModelsforReasoning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
B OmittedProof 22
B.1 BuildingBlocksofFFNsConstruction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.2 BuildingBlocksofTransformersConstruction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
B.3 BuildingBlocksofRNNsConstruction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
B.4 ProofofTheorem4.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
B.5 ProofofTheorem4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
B.6 ProofofTheorem4.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
B.6.1 ProofofLemmaB.26 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
B.6.2 ProofofLemmaB.27 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
B.7 ProofofTheorem4.8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
B.8 ProofofTheorem5.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
B.9 ProofofTheorem5.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
B.10 ProofofTheorem5.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
B.11 ProofofTheorem5.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
18PREPRINT
B.12 ProofofTheorem5.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
B.13 ProofofTheorem5.8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
19PREPRINT
A AdditionalDefinitions
Wewillnowdefinesomedefinitionsusedintheproofs.
A.1 ReasoningTasksonGraphs.
Whenresaoningongraphs,withoutotherwisespecified,wewillusenasthenumberofverticesandmasthenumber
ofedges. Withoutlossofgenerality,wewillassumetheverticesarelabeledby[n].
Wewillfocusondecisionproblemsongraphs,whicharedefinedasfollows:
DefinitionA.1(DecisionProblemonGraphs). Adecisionproblemongraphsisafunctionf : G → {YES,NO},
whereG isthesetofallpossiblegraphs.
Wewillusethefollowingdecisionproblemasourmainexample:
DefinitionA.2(IsTree). IsTree(G)=YESifGisatree,andIsTree(G)=NOotherwise.
OnecanviewIsTreeasaminimalexampleofreasoningtasks. OneoftheclassicalsolutionstoIsTreeisrunningDepth
FirstSearchandthisalgorithmtakesO(n)time.
A.2 MoreonNumericPrecisions.
WewilluseROUND(x,p)todenotetheroundingfunctionthatroundsxtothenearestnumberinR . Wewillassume
p
pisanoddnumberwithoutlossofgenerality.
(cid:40) (cid:32)p−1 (cid:33) (cid:41)
(cid:88)
R = (2b −1) b 2(p−1)/2−i :∀i∈[p],b ∈{0,1} . (5)
p p i i
i=1
ForcalculationoverR ,wewillassumethecalculationisexactandtheresultisroundedtoR attheend,thatis,for
p p
operator⊕,wewillhave
ROUND(x,p)⊕ ROUND(y,p)
p
=ROUND(ROUND(x,p)⊕ROUND(y,p),p)).
WewilladditionallydefineZ asthesetofallintegersthatcanberepresentedbyp-bitfloatingpointnumbers. Wewill
p
define1/[m]asthesetofunitfractional{1} . Further,wewilldefineROUND(1/[m],p)astheroundingof1/[m]
i i∈[m]
toR p. Wewilladditionallydefineforanyrealnumberx∈{0,1},next(x)= m1
+1
wherem=argmin k∈Z|x− k1|.
A.3 Models
Tokenization. Totokenizeastring,wewilltokenizeallthewordsseparatedbythespacecharacterintoasequence
oftokens. TotokenizeagraphG,wewillorderitsedgesE={(u ,v )|u <v }randomlyandtokenizeitintothe
i i i i
followingstring:
Tokenize(G)={<s>,u ,∼,v ,...,u ,∼,v }. (6)
1 1 m m
Weherebyassumethereareconstantmorespecialtokensthatarenotthesameasanynumbertoken,whicharelisted
below:
• <s>: thefirstspecialtoken,indicatingthestartofasentence.
• ∼: thesecondspecialtoken,indicatinganedge.
• YES: thethirdspecialtoken,indicatingtheanswerisyes.
• NO: thefourthspecialtoken,indicatingtheanswerisno.
• <StartSearch>: thefifthspecialtoken,indicatingthestartofasearchquery.
• <EndSearch>: thesixthspecialtoken,indicatingtheendofasearchquery.
Wewilldenotethetotalnumberofspecialtokensasn andthetotalvocabularysizeas|V|=n+n . Wewillfurther
S S
definethedetokenizationfunctionDetokenize,
Detokenize(S)=“S S ... S ”.
1 2 l
HereeachS iseitheranumberoraspecialtoken,whichwewilltreatasaword.
i
20PREPRINT
EmbeddingFunctions. Wewillusedtodenotethedimensionoftheembeddingandw todenotethei-thcoordinate
i
vectorinRd.
Wewillseparatetheembeddingfunctionintotwoparts: thewordembeddingandthepositionembedding. Fortheword
embedding,wewilluseiw ∈Rdtorepresenttheembeddingoftheverticeiinthetokenizationofthegraph. Forthe
1
k-thspecialtoken,wewillusew torepresentitsembedding. Forexample,theembeddingof∼isw . Wewill
2+k 2
denotethewordembeddingmatrixasW(E) ∈R|V|×d.
Forthepositionembedding,wewilluselw torepresentthepositionembeddingofthel-thtokeninthetokenizationof
d
thegraph,whichisahyperparameter. Thefinalembeddingofanytokensequenceisthesumofthewordembedding
andthepositionembedding. WewilluseEmbtodenotetheembeddingfunction.
Thisembeddingfunctionwillbefixedandsharedacrossallmodelsweconsiderinthispaperandwillnotbelearned
duringtraining,hencewewillnotconsideritaspartofthemodelparameters.
Language Modeling. In this work, we will consider the difference between Transformer and Recurrent Neural
Networksonreasoningtasks,whichisaspecialcaseoflanguagemodeling. Wewilldefinelanguagemodelingas
follows:
DefinitionA.3(LanguageModel). AlanguagemodelisafunctionM :∪L |V|l →P ,where|V|isthevocabulary
l=1 |V|
size,listhesequencelength,andP istheprobabilitysimplexover|V|.
|V|
A.4 LanguageModelsforReasoning.
ChainofThought. Wewillnowdefinehowweuselanguagemodelstosolvereasoningtasksutilizingthefollowing
techniquecalledchainofthought.
DefinitionA.4(ChainofThought). Givenalanguagemodel,M withvocabularyV andthetokenizedinputsequence
S
in
∈|V|l0,chainofthought(CoT)generatesthefollowingsequenceoftokenizedsequence:
S =S , (7)
0 in
snext =argmaxM(S )[j], (8)
i i
j∈V
S =S ⊕snext,∀i≥0. (9)
i+1 i i
TheprocessterminatesatS whenargmax M(S )[j]isYESorNO. Thelanguagemodelcansolvethereasoning
i j∈V i
taskwithinT stepsofCoTiftheprocessterminatesatS wherei≤T andthefinaloutputiscorrect. Wewillcallthe
i
specialcasewherethelanguagemodelsolvesthereasoningtaskwithin0stepsofCoTassolvingthereasoningtask
withoutCoT.
RetrievalAugmentation. Wewillshowinthispaperthatretrievalaugmentationisanecessarytechniquetosolve
reasoningtasksforrecurrentneuralnetworks. Wewilldefineretrievalaugmentationasfollows:
Definition A.5 (Retrieval Augmented Generation). Retrieval Augmented Generation means giving the language
model the capability to retrieve relevant information to assist generation. We formally described the process here.
GivenalanguagemodelM withvocabularyV (containingtwoadditionalspecialtokenscalled<StartSearch>and
<EndSearch>) and the tokenized input sequence S
in
∈ |V|l0, retrieval augmented generation generates following
sequenceoftokenizedsequence:
S =S ,
0 in
snext =argmaxM(S )[j],
i i
j∈V
(cid:26) S ⊕snext,snext ̸=<EndSearch>
S = i i i
i+1 S ⊕snext⊕RETRIEVE(S ),otherwise.
i i i
Here RETRIEVE looks for the last occurrence of <StartSearch> at position l and <EndSearch> in S at posi-
s
ton l and treat Detokenize(S ) as a regular expression. The algorithm then uses the regular expression on
e ls:le
Detokenize(S ). Iftheregularexpressionevermatches,theRETRIEVEwillreturnthematch. Iftheregular
1:ls−1
expressionnevermatches,RETRIEVEwillreturnaspecialtokenFAILED.
SimilartoDefinitionA.4,wecandefinethenotionofsolvingthereasoningtaskwithinT stepsofretrievalaugmented
generationandsolvingthereasoningtaskwithoutretrievalaugmentedgeneration.
Wewillnotethatassuming|V|=O(n)andeverysearchqueryandtheresultisoflengthO(1),theregularexpression
evaluationcantypicallybeevaluatedinO(n)time.
21PREPRINT
B OmittedProof
B.1 BuildingBlocksofFFNsConstruction
WewillfirstshowsomebasicoperationsthatmultiplelayersoffeedforwardneuralnetworkswithReGLUactivation
canperformthatwillbeusedinthefollowingproofs.
LemmaB.1(Multiplication). Giventwodimensionsi ,i ,thereexistsaparameterconfigurationofa1-layerfeedfor-
1 2
wardneuralnetworkwithReGLUactivationthatforanyinputx∈Rdandconstantwidth,computestheproductofx
i1
andx .
i2
g(x)=[x ×x ,0,...,0]⊤.
i1 i2
Proof. WecanconstructthefollowingfeedforwardneuralnetworkwithReGLUactivation:
W x=[x −x 0... 0]⊤ ,
1 i1 i1
W x=[x x 0... 0]⊤ ,
2 i2 i2
W h=[h +h 0 0 0 ... 0]⊤ ,
3 1 2
b =b =b =0.
1 2 3
g(x)=W (ReLU(W x)⊗ReLU(W x))=[x ×x ,0,...,0]⊤.
3 1 2 i1 i2
LemmaB.2(LinearOperations). GivenalineartransformationW ∈Rd×d,thereexistsaparameterconfigurationof
a1-layerfeedforwardneuralnetworkwithReGLUactivationandwidthw =dthatforanyinputx∈Rd,computes
Wx.
Proof.
b =1w,b =0,b =0,
1 2 3
W =0,W =W,W =I
1 2 3 d×d
LemmaB.3(Indicator). GivenaconstantintegerB ≤dandadimensioni,thereexistsaparameterconfigurationof
a1-layerfeedforwardneuralnetworkwithReGLUactivationandwidth4thatforanyinputx ∈ Rd,computesthe
indicatorfunctionofx =Bwhenx isaninteger.
i i
Proof.
b =1w,b =[−B−0.5,−B+0.5,B−0.6,B+0.4]⊤,b =10,
2 1 3
W =0,W x=[x ,x ,−x ,−x ]⊤,
2 1 i i i i
ReLU(W x+b )=[ReLU(x −B−0.5) ReLU(x −B+0.5) ReLU(B−x −0.6) ReLU(B−x +0.4)]
1 1 i i i i
W =10[1 −1 −1 1]⊤ .
3
Then,
g(x)=W ReLU(W x+b )+b
3 1 1 3
=10(ReLU(x −B−0.5)−ReLU(x −B+0.5))+10(ReLU(B−x −0.6)−ReLU(B−x +0.4))+10
i i i i

10×0+10×−1+10=0, ifx <B,
 i
= 10×−0.5+10×−0.4+10=1, ifx =B,
i
10×−1+10×0+10=0, ifx >B.
i
LemmaB.4(LookupTable). ForconstantB andksuchthatkB ≤ d,givenalookuptablewhichkeyistupleofk
integersboundedbyB,andvalueisascalarinR ,thereexistsaparameterconfigurationofa1−layerfeedforward
p
neuralnetworkwithReGLUactivationwithwidthO(Bk)thatforanyinputx∈Rd,computesthevalueofthelookup
tableatthekeyx ,x ,...,x .
i1 i2 ik
22PREPRINT
Proof. Wecancalculatex +B×x +B2×x +...+Bk−1×x ,andthenscaleBk indicatorfunctionstoget
i1 i2 i3 ik
thevalueofthelookuptableatthekeyx ,x ,...,x .
i1 i2 ik
LemmaB.5(Threshold). Givenanythresholduandconstantϵ>0,thereexistsaparameterconfigurationofa1-layer
feedforwardneuralnetworkwithReGLUactivationandwidth2thatforanyinputx ∈ Rd,computestheindicator
functionx >uonx ∈[−∞,u−ϵ]∪[u,∞].
i i
Proof.
b =1w,b =[−u+ϵ,−u+0.5ϵ]⊤,b =0,
2 1 3
W =0,W x=[x ,x ]⊤,
2 1 i i
ReLU(W x+b )=[ReLU(x −u+ϵ) ReLU(x −u+0.5ϵ)]
1 1 i i
2
W = [1 −1 −1 1]⊤ .
3 ϵ
Then,
g(x)=W ReLU(W x+b )+b
3 1 1 3
=2/ϵ(ReLU(x −u−ϵ)−ReLU(x −u+0.5))
i i
(cid:26)
0 ifx <u−ϵ,
= i
1, ifx >u,
i
Lemma B.6 (Interval). Given any constant u,v and ϵ > 0, there exists a parameter configuration of a 1-layer
feedforwardneuralnetworkwithReGLUactivationandwidth4thatforanyinputx ∈ Rd,computestheindicator
functionx ∈[u,v]onx ∈[−∞,u−ϵ]∪∪[u,v]∪[v,∞].
i i
Proof. Theintervalfunctionherecanbewrittenasthedifferenceoftwothresholdfunctions. Wecanusethesame
construction as in Lemma B.5 to approximate the indicator function x > u and x > v + ϵ and then take the
i i
difference.
B.2 BuildingBlocksofTransformersConstruction
WewillshowinthissectionsomeconstructionforbasicfunctionalityusingTransformerBlocks. Thisconstructionwill
beusedinthefollowingsectionstoprovethemainresults.
WewillalwaysuseX ∈Rd×l astheinputtotheTransformerBlock,wheredisthedimensionoftheinput,andlisthe
p
lengthofthesequence. Wewillfirstoutlineallthebuildingfunctionalityandthenshowhowtoimplementthem.
DefinitionB.7(CopyingFunction). Forintegers,indexsetI ,I ⊂[d−20]satisfying|I |=|I |,acopyingfunction
1 2 1 2
COPY[s,I ,I ]satisfiesthefollowing,∀X ∈Rd×l,then
1 2 p
COPY[s,I ,I ](X) =x ∀k ≤[m]
1 2 I2,k I1,max{k−s,0}
COPY[s,I ,I ](X) =0 ∀r ∈[m]
1 2 Ic,k
2
DefinitionB.8(CountingFunction). ForindexsetI ,I ⊂[d−20],|I |=|I |≤10andindexi,acountingfunction
1 2 1 2
COUNT[I ,I ,i]satisfiesthefollowing,if∀v ∈I ∪I ,k ∈[l],X ∈Z andX ̸=0,then
1 2 1 2 v,k p v,k
1
COUNT[I ,I ,i](X) = ∀k ∈[l].
1 2 i,k (cid:80)k
1[X =X ]+1
h=1 I1,h I2,k
COUNT[I ,I ,i](X) =0 ∀k ∈[l].
1 2 ic,k
DefinitionB.9(MatchingFunction). ForindexsetI ,I ,I ,I ⊂[d−20],|I |=|I |≤10,|I |=|I |,amatching
1 2 3 4 1 2 3 4
functionMatch[I ,I ,I ,I ]satisfiesthefollowing,if∀v ∈I ∪I ,k ∈[l],X ∈Z ,then
1 2 3 4 1 2 v,k p
Match[I ,I ,I ,I ](x) =X ∀k ∈[l]
1 2 3 4 I3,k I4,k∗
(cid:26)
min{h|X =X },{h|X =X }≠ ∅
wherek∗ = I1,h I2,k I1,h I2,k .
1,otherwise
23PREPRINT
DefinitionB.10(MatchingClosestFunction). ForindexsetI ,I ,I ,I ⊂[d−20],|I |=|I |≤10,|I |=|I |,a
1 2 3 4 1 2 3 4
matchingclosestfunctionMatch[I ,I ,I ,I ]satisfiesthefollowing,if∀v ∈I ∪I ,k ∈[l],X ∈Z ,then
1 2 3 4 1 2 v,k p
MatchClose[I ,I ,I ,I ](x) =X ∀k ∈[l]
1 2 3 4 I3,k I4,k∗
(cid:26)
max{h|X =X },{h|X =X }≠ ∅
wherek∗ = I1,h I2,k I1,h I2,k .
1,otherwise
DefinitionB.11(MatchingNearestFunction). ForindexsetI ,I ,I ,I ⊂ [d−20],|I | = |I | ≤ 10,|I | = |I |
1 2 3 4 1 2 3 4
andindexi,amatchingnearestfunctionMatchNearest[I ,I ,I ,I ,i]satisfiesthefollowing,if∀v ∈ I ∪I ,k ∈
1 2 3 4 1 2
[l],X ∈Z ,then
v,k p
MatchNearest[I ,I ,I ,I ](x) =X ∀k ∈[l]
1 2 3 4 I3,k I4,k∗
(cid:40)
argmin |h−X |,{h|X =X }≠ ∅
wherek∗ = h∈{h|XI1,h=XI2,k} i,k I1,h I2,k .
1,otherwise
DefinitionB.12(MatchingNextFunction). GivenanyintergerconstantA,assumingp > 10Alogn,forindexset
I ,I ,I ,I ⊂ [d−20],|I | = |I | ≤ 10,|I | = |I |, and a special counting index a, a matching next function
1 2 3 4 1 2 3 4
MatchNext[I ,I ,I ,I ,a]satisfiesthefollowing,ifX satisfiesthefollowingcondition:
1 2 3 4
1. ∀v ∈I ∪I ,k ∈[l],X ∈Z ,
1 2 v,k p
2. X ∈ROUND(1/[nA],p)∪{0},
a,k
3. Foranyk ∈ [l],givenanyk ≥ k,thecountingindexmultisetS = {X | X = X }takesconsecutive
k a,k′ I1,k′ I2,k
and disjoint values in ROUND(1/[nA],p), that is, there exists u ,v ∈ ROUND(1/[nA],p) such that S =
k k k
[u ,v ]∩ROUND(1/[nA],p).
k k
then,wehave
MatchNext[I ,I ,I ,I ,a](X) =X ∀k ∈[l]
1 2 3 4 I3,k I4,k∗
wherek∗ =arg min |X −next(X )|.
a,h a,k
h∈{h|XI1,h=XI2,k}∪{1}
NowwewillshowhowtoimplementthesefunctionsusingTransformerBlocks. Theconstructionhereismotivatedby
theconstructioninFengetal.(2023)withsomemodifications.
LemmaB.13(CopyingBlocks). Forintegers,indexsetI ,I ⊂[d−10]satisfying|I |=|I |,acopyingfunction
1 2 1 2
COPY[s,I ,I ] can be implemented with 1 feedforward block g and 1 attention block A with 1 attention head.
1 2
Formally,whenX =k,itholdsthat
d,k
A(g(X)+X)=COPY[s,I ,I ](X).
1 2
ProofofLemmaB.13. WewillusethefeedforwardblocktocalculateX2 and1(LemmaB.1)andhave
k,d
(g(X)+X) =k2
d−1,k
(g(X)+X) =1.
d−2,k
∀i̸∈{d−1,d−2},(g(X)+X) =X .
i,k i,k
WewilluseX′todenoteg(X)+X. ThenwewillchooseW(K),W(Q)suchthat
 
1
W(K)X :′
,k′
=nk′

k′2
 
−(k2+s2−2sk)
W(Q)X :′ ,k = 2k−2s 
−1
24PREPRINT
Hence
(cid:18)(cid:16) (cid:17)⊤(cid:16) (cid:17)(cid:19)
W(K)X W(Q)X
k′,k
=−n(cid:0) k′2−k′(2k−2s)+k2+s2−2sk(cid:1)
=−n(k−s−k′)2
Hencewehave
(cid:18)(cid:16) (cid:17)⊤(cid:16) (cid:17)(cid:19)
argmax W(K)X W(Q)X =max{k−s,0}.
k′<k
k′,k
Also,foranyk′ ≤k,k′ ̸=max{k−s,0},wehave
(cid:18)(cid:16) (cid:17)⊤(cid:16) (cid:17)(cid:19) (cid:18)(cid:16) (cid:17)⊤(cid:16) (cid:17)(cid:19)
W(K)X W(Q)X − W(K)X W(Q)X <−n.
k′,k max{k−s,0},k
Henceafterthecolumn-wisesoftmaxandroundingtop=O(logn)bit,wehave
(cid:18) (cid:18)(cid:16) (cid:17)⊤(cid:16) (cid:17)(cid:19)(cid:19)
softmax W(K)X W(Q)X =1[k′ =max{k−s,0}].
k′,k
WewillthenchooseW(V)suchthat
W(V)X′ =X′ =X ∀k′ ∈[l].
I2,k′ I1,k′ I1,k′
W(V)X′ =0 ∀k′ ∈[l].
Ic,k′
2
Thisthenconcludesthat
A(g(X)+X)=COPY[s,I ,I ](X).
1 2
Theproofiscomplete.
Lemma B.14 (Counting Blocks). For index set I ⊂ [d − 20] satisfying |I | = |I | ≤ 10, a counting function
1 2
COUNT[i,I ,I ] can be approximated with 1 feedforward block g and 1 attention block A with 1 attention head.
1 2
Formally,whenX =kandX =1[k =1],X =0,itholdsthat
d,k 3,k I1,1
A(g(X)+X) =ROUND(COUNT[s,I ,I ](X) ,p).
i,k 1 2 i,k
A(g(X)+X) =0.
ic,k
ProofofLemmaB.14. WewillusethefeedforwardblocktocalculateX2 ,v ∈I ∪I (LemmaB.1)andhave
v,k 1 2
(g(X)+X) =X2 ,i∈[|I|].
d−i,k I1[i],k
(g(X)+X) =X2 ,i∈[|I|].
d−|I|−i,k I2[i],k
(g(X)+X) =1.
d−2|I|−1,k
∀i̸∈{d−i|i∈[2|I|+1]},(g(X)+X) =X .
i,k i,k
WewilluseX′todenoteg(X)+X. ThenwewillchooseW(K),W(Q)suchthat
 1+1[k′ =1]
W(K)X :′ ,k′ =n X I1[i],k′ 
X2
I1[i],k′ i∈[I]
 X2 
W(Q)X :′
,k
=−XI2 I2[i [] i, ]k
,k
1
i∈[I]
25PREPRINT
Hence,
(cid:18)(cid:16) (cid:17)⊤(cid:16) (cid:17)(cid:19)
W(K)X W(Q)X
k′,k
|I| |I|
(cid:88)(cid:16) (cid:17) (cid:88)
=−n X′2 −X (2X )+X2 +n1[k′ =1] X2 .
I2[i],k′ I1[i],k′ I2[i],k I2[i],k I[i],k
i=1 i=1
|I| |I|
(cid:88) (cid:88)
=−n (X −X )2+n1[k′ =1] X2 .
I1[i],k′ I2[i],k I2[i],k
i=1 i=1
Hencewehave
(cid:18)(cid:16) (cid:17)⊤(cid:16) (cid:17)(cid:19)
max W(K)X W(Q)X =0.
k′<k
k′,k
Equalityholdswhenk′ =1orX =X foralli∈[|I |].
I1[i],k′ I2[i],k 1
Also,foranyk′ ≤k,k′ ̸=1orX ̸=X forsomei∈[|I |],wehave
I1[i],k′ I2[i],k 1
(cid:18)(cid:16) (cid:17)⊤(cid:16) (cid:17)(cid:19)
W(K)X W(Q)X <−n.
k′,k
Henceafterthecolumn-wisesoftmaxandroundingtop=O(logn)bit,wehave
(cid:18) (cid:18)(cid:16) (cid:17)⊤(cid:16) (cid:17)(cid:19)(cid:19) (cid:32) 1 (cid:33)
softmax W(K)X W(Q)X =ROUND ,p
(cid:80)k
1[X =X ]+1
k′,k h=1 I1,h I2,k
HeretheO(cid:0) 1 (cid:1) termcomesfromthefactthatthesoftmaxisroundedtop=O(logn)bit.
nA
WewillthenchooseW(V)suchthat
W(V)X′ =X′ =1[k′ =1] ∀k′ ∈[l].
i,k′ 3,k′
W(V)X′ =0 ∀k′ ∈[l].
Ic,k′
Thisthenconcludesthat
A(g(X)+X) =ROUND(COUNT[s,I ,I ](X) ,p).
i,k 1 2 i,k
A(g(X)+X) =0.
ic,k
LemmaB.15(MatchingBlocks). Givenanyconstantc,forindexsetI ,I ,I ,I ⊂[d−20],|I |=|I |≤10,|I |=
1 2 3 4 1 2 3
|I |,amatchingfunctionMatch[I ,I ,I ,I ]canbeimplementedwith1feedforwardblockgand1attentionblockA
4 1 2 3 4
with1attentionhead. Formally,whenX =k,X =1[k =1],X =0andk ≤nc,itholdsthat
d,k 3,k I1,1
A(g(X)+X)=Match[I ,I ,I ,I ](X)
1 2 3 4
Proof. Wewillusethefeedforwardblocktocalculatek2,X2 ,v ∈∪I ∪I asintheproofofLemmasB.13andB.14.
v,d 1 2
WethenchooseW(K),W(Q)suchthat
(cid:18)(cid:16) (cid:17)⊤(cid:16) (cid:17)(cid:19)
W(K)X W(Q)X
k′,k
|I|
(cid:88)
=−n4c+1 (X −X )2−nk′2
I1[i],k′ I2[i],k
i=1
 
|I|
(cid:88)
+1[k′ =1]n4c+1 X2 +n−n2c+2 .
I2[i],k
i=1
26PREPRINT
ThedetailedconstructionofW(K),W(Q)isomittedheresinceitissimilartotheproofofLemmasB.13andB.14.
We will discuss several cases for the distribution of
(cid:16)(cid:0) W(K)X(cid:1)⊤(cid:0) W(Q)X(cid:1)(cid:17)
. It always holds that
k′,k
(cid:16)(cid:0) W(K)X(cid:1)⊤(cid:0) W(Q)X(cid:1)(cid:17) =−n2c+2.
1,k
1. Iftheredoesn’texistsk′,suchthatX = X ,thenforanyi > 1,wehave(cid:16)(cid:0) W(K)X(cid:1)⊤(cid:0) W(Q)X(cid:1)(cid:17) <
k′,I1 k,I2
i,k
−n4c+1.
2. If there exists k′, such that X = X , then for such k′, we have (cid:16)(cid:0) W(K)X(cid:1)⊤(cid:0) W(Q)X(cid:1)(cid:17) =
k′,I1 k,I2
k′,k
−nk′2 > −n2c+1. The rest of the entries are all smaller than −n4c+1. Finally, the largest k′ satisfying that
X =X
willcorrespondstoa(cid:16)(cid:0) W(K)X(cid:1)⊤(cid:0) W(Q)X(cid:1)(cid:17)
thatisatleastnlargerthanthesecondlargest
k′,I1 k,I2
k′,k
(cid:16)(cid:0) W(K)X(cid:1)⊤(cid:0) W(Q)X(cid:1)(cid:17)
,asintheproofofLemmaB.13.
k′,k
Concludingtheabovediscussion,wehaveafterthecolumn-wisesoftmaxandroundingtop=O(logn)bit,
(cid:18) (cid:18)(cid:16) (cid:17)⊤(cid:16) (cid:17)(cid:19)(cid:19) (cid:26) 1[k′ =min{h|X =X }],{h|X =X }≠ ∅
softmax W(K)X W(Q)X = I1,h I2,k I1,h I2,k
1[k′ =1],otherwise
k′,k
Further,wewillchooseW(V)suchthat
W(V)X′ =X′ =X ∀k′ ∈[l].
I3,k′ I4,k′ I4,k′
W(V)X′ =0 ∀k′ ∈[l].
Ic,k′
3
Thisthenconcludesthat
A(g(X)+X)=Match[I ,I ,I ,I ](X)
1 2 3 4
Thisconcludestheproof.
LemmaB.16(MatchingClosestBlocks). Givenanyconstantc,forindexsetI ,I ,I ,I ⊂[d−20],|I |=|I |≤
1 2 3 4 1 2
10,|I |=|I |,amatchingclosestfunctionMatchClose[I ,I ,I ,I ]canbeimplementedwith1feedforwardblockg
3 4 1 2 3 4
and1attentionblockAwith1attentionhead. Formally,whenX =k,X =1[k =1],X =0andk ≤nc,it
d,k 3,k I1,1
holdsthat
A(g(X)+X)=MatchClose[I ,I ,I ,I ](X)
1 2 3 4
Proof. TheproofissimilartotheproofofLemmaB.15,andonecandesigntheattentionpatternas
(cid:18)(cid:16) (cid:17)⊤(cid:16) (cid:17)(cid:19)
W(K)X W(Q)X
k′,k
|I|
(cid:88)
=−n4c+1 (X −X )2−n(k−k′)2
I1[i],k′ I2[i],k
i=1
 
|I|
(cid:88)
+1[k′ =1]n4c+1 X2 +n(k−1)2−n2c+2 .
I2[i],k
i=1
Therestoftheproofisomittedhere.
Lemma B.17 (Matching Nearest Blocks). Given any constant c, for index set I ,I ,I ,I ⊂ [d − 20],|I | =
1 2 3 4 1
|I | ≤ 10,|I | = |I |andindexi,amatchingnearestfunctionMatchNearest[I ,I ,I ,I ,i]canbeimplemented
2 3 4 1 2 3 4
with 1 feedforward block g and 1 attention block A with 1 attention head. Formally, when X = k,X =
d,k 3,k
1[k =1],X =0andk ≤nc,itholdsthat
I1,1
A(g(X)+X)=MatchNearest[I ,I ,I ,I ,i](X)
1 2 3 4
27PREPRINT
Proof. TheproofissimilartotheproofofLemmaB.15,andonecandesigntheattentionpatternas
(cid:18)(cid:16) (cid:17)⊤(cid:16) (cid:17)(cid:19)
W(K)X W(Q)X
k′,k
|I|
(cid:88)
=−n4c+1 (X −X )2−n(X −k′)2
I1[u],k′ I2[u],k i,k
u=1
 
|I|
(cid:88)
+1[k′ =1]n4c+1 X I2 2[u],k+n(1−X i,k)2−n2c+2 .
u=1
Therestoftheproofisomittedhere.
LemmaB.18(MatchingNextBlocks). GivenanyconstantA,c,forindexsetI ,I ,I ,I ⊂[d−20],|I |=|I |≤
1 2 3 4 1 2
10,|I | = |I |andaspecialcountingindexa,amatchingnextfunctionMatchNext[I ,I ,I ,I ,a]canimplement
3 4 1 2 3 4
with 1 feedforward block g and 1 attention block A with 1 attention head. Formally, when X = k,X =
d,k 3,k
1[k =1],X =0andk ≤nc,itholdsthat
I1,1
A(g(X)+X)=MatchNext[I ,I ,I ,I ,a](X)
1 2 3 4
Proof. Wewillusethefeedforwardblocktocalculatethefollowingnextfunction,where
1, x≥ 2.
12
, 3
>x3
> 2.
next(x)= 3 5 5
1, 7 >x> 3
 x4
−x2+x3,
x20
≤ 11.
10
40
Thevaluecanbearbitraryforx∈[11, 3 ]∪[2, 7 ]∪[3,2]. Thisfunctionisachievablebyafeedforwardblockthrough
40 10 5 20 5 3
combinationofLemmasB.1andB.6.
Thepurposeofthisistoapproximatethenextfunctionforx∈ROUND(1/[nA],p),andwehavethefollowinglemma.
LemmaB.19. Forlargeenoughnandanyx∈ROUND(1/[nA],p),wehave
(cid:18) (cid:19)
1
|next(x)−next(x)|≤next(x)3+O .
n10A
Proof. We always have ROUND(1/[nA],p)∩(cid:0) [11, 3 ]∪[2, 7 ]∪[3,2](cid:1) = ∅. We will discuss several cases for
40 10 5 20 5 3
x∈ROUND(1/[nA],p).
1. Ifx≥ 3 ,thennext(x)=next(x).
10
2. Ifx≤ 7 ,itholdsthat|x−1/m|≤1/n10A,m≥3,then
20
(cid:18) (cid:19)
1 1 1 1
next(x)=x−x2 = − + +O
m m2 m3 n10A
(cid:18) (cid:19)
1 1 1
= − +O
m+1 m3(m+1) n10A
Thisthenconcludestheproof.
WethenchooseW(K),W(Q)suchthat
(cid:18)(cid:16) (cid:17)⊤(cid:16) (cid:17)(cid:19)
W(K)X W(Q)X
k′,k
|I|
(cid:88)
=−n4A+3 (X −X )2−n4A+1(next(X )−X )2
I1[i],k′ I2[i],k a,k a,k′
i=1
 
|I|
(cid:88)
+1[k′ =1]n4A+3 X I2 2[i],k+n4A+1X a2 ,k−n4A+2 .
i=1
28PREPRINT
Again, the detailed construction of W(K),W(Q) is omitted here since it is similar to the proof of Lemmas B.13
andB.14.
We will discuss several cases for the distribution of
(cid:16)(cid:0) W(K)X(cid:1)⊤(cid:0) W(Q)X(cid:1)(cid:17)
. It always holds that
k′,k
(cid:16)(cid:0) W(K)X(cid:1)⊤(cid:0) W(Q)X(cid:1)(cid:17) =−n4A+2.
1,k
1. Iftheredoesn’texistsk′,suchthatX = X ,thenforanyi > 1,wehave(cid:16)(cid:0) W(K)X(cid:1)⊤(cid:0) W(Q)X(cid:1)(cid:17) <
k′,I1 k,I2
i,k
−n4A+3.
2. If there exists k′, such that X = X , then for such k′, we have (cid:16)(cid:0) W(K)X(cid:1)⊤(cid:0) W(Q)X(cid:1)(cid:17) =
k′,I1 k,I2
k′,k
−n3A(next(X )−X )2 >−n4A+1. Therestoftheentriesareallsmallerthan−n4A+2.
a,k a,k′
Itremainstodiscussthedistributionof(cid:16)(cid:0) W(K)X(cid:1)⊤(cid:0) W(Q)X(cid:1)(cid:17) fork′ satisfyingX = X . WhenX
k′,k
k′,I1 k,I2
satisfies the condition in Definition B.12, we have that S = {X | X = X } takes consecutive and
k a,k′ k′,I1 k,I2
disjointvaluesinROUND(1/[nA],p). Hence,if|S |>2,supposey,z ∈S satisfiesthat
k k
|y−next(X )|= min|x−next(X )|
a,k a,k
x∈Sk
|z−next(X )|= min |x−next(X )|.
a,k a,k
x∈Sk,x̸=y
Wewilldiscussseveralcasesfory,z.
• Ify−next(X )andz−next(X )arebothnegative,theny >z,wehave,
a,k a,k
(cid:0) (cid:1)2 (cid:0) (cid:1)2
y−next(X ) − z−next(X ) =(y−z)(y+z−2next(X ))
a,k a,k a,k
1
≤−(y−z)2 ≤− .
4n4A
.
• Ify−next(X )andz−next(X )arebothpositive,theny <z,andsameasabovewehave
a,k a,k
(cid:0) (cid:1)2 (cid:0) (cid:1)2
y−next(X ) − z−next(X ) =(y−z)(y+z−2next(X ))
a,k a,k a,k
1
≤−(y−z)2 ≤− .
4n4A
.
• If y −next(X ) and z −next(X ) have different signs, then according to Lemma B.19, we have, y =
a,k a,k
ROUND(next(X ),p)becauseS takesconsecutiveanddisjointvaluesinROUND(1/[nA],p). Thisthen
a,k k
impliesthat
(cid:0) (cid:1)2 (cid:0) (cid:1)2
y−next(X ) − z−next(X )
a,k a,k
1 1
(cid:18)
1
(cid:19)2
≤O( )+ −
n10A next6(X ) next(X )(next(X )+1)
a,k a,k a,k
1
≤− .
4n4A
Concluding,wealwayshaveforanyk′′ ̸=k∗ =argmax (cid:16)(cid:0) W(K)X(cid:1)⊤(cid:0) W(Q)X(cid:1)(cid:17)
k′,k
k′,k
(cid:18)(cid:16) (cid:17)⊤(cid:16) (cid:17)(cid:19) (cid:18)(cid:16) (cid:17)⊤(cid:16) (cid:17)(cid:19) n
W(K)X W(Q)X − W(K)X W(Q)X ≤− .
4
k′,k k∗,k
Concludingtheabovediscussion,wehaveafterthecolumn-wisesoftmaxandroundingtop=O(logn)bit,
(cid:18) (cid:18)(cid:16) (cid:17)⊤(cid:16) (cid:17)(cid:19)(cid:19) (cid:20) (cid:21)
softmax W(K)X W(Q)X =1 k′ =arg min |X −next(X )| .
a,h a,k
k′,k h∈{h|XI1,h=XI2,k}∪{1}
29PREPRINT
Further,wewillchooseW(V)suchthat
W(V)X′ =X′ =X ∀k′ ∈[l].
I3,k′ I4,k′ I4,k′
W(V)X′ =0 ∀k′ ∈[l].
Ic,k′
3
Thisthenconcludestheproof.
B.3 BuildingBlocksofRNNsConstruction
WewillnowdescribethebuildingblocksofLinearRNNsconstruction. Wewillintroducesomebasicoperationsthat
willbeusedtobuildmorecomplexRNNsfamily.
LemmaB.20(RecentInputMemorizing). Givenanyconstantk andC,thereexistsaparameterconfigurationof
linearunitthatmaintainsC dimensionsoflastkinputvectorsinthestate.
Proof. Supposetheinputsequenceisx ∈Rd,andthedimensionsthatthestateshouldmemorizeared ,d ,...,d .
1:t 1 2 C
Wecanconstructthefollowinglinearunit:
(cid:2) (cid:3)
h = x ... x h ... h h ... h .
t t−1,d1 t−1,dC t−1,1 t−1,C×(k−1) t−1,C×k+1 t−1,d
LemmaB.21(Summation). GivenanyconstantkandC,thereexistsaparameterconfigurationoflinearunitthat
maintainsthesumofonedimensionofthelastkinputvectorsinthestate.
Proof. SupposeWLOGtheinputsequenceisx ∈Rd,andthedimensionthatthestateshouldmemorizeis1. We
1:t
canconstructthefollowinglinearunit:
h =[x +h h ... h ].
t t−1,1 t−1,1 t−1,2 t−1,d
LemmaB.22(SpecialPositionMemorizing). GivenanyconstantkandC,thereexistsaparameterconfigurationof
linearunitandaFFNwithCkwidththatmaintainstheC dimensionsoftheinputvectoratposition1tokinthestate.
Proof. ThisisadirectcombinationofLemmaB.21andLemmasB.1andB.4. TheFFNcanassignalltheinputvectors
withpositiongreaterthank to0,andpermutethecorrespondingdimensionsoffirstk inputvectorstothefirstCk
dimensionsofthestate. Thelinearunitcanthenmaintainthestate.
LemmaB.23(ReciteFixedSequence). GivenanyconstantintegerkandC,thereexistsaFFNwithwidthkC that
canoutputfixedsequenceofscalarsthattakesvaluesin[C]onafixedsetofpositionsl ,...l .
1 k
Proof. ThisisadirectconsequenceofLemmaB.4.
B.4 ProofofTheorem4.6
Wewillfirstrestatethetheoremforclarity.
Theorem4.6. FortaskT ∈{Index,AR,c-gramretrieval,Counting},thereexistsaTransformerfamilywithconstant
sizeandO(logn)precisionthatcansolveT ofsizen. Ontheotherhand,foranyRNNfamilyRwitho(n)bitmemory,
RcannotsolveT ofsizenwithanylengthofCoTforlargeenoughn.
Proof. Wewilldiscussbycases.
When T is Index, we will first show why RNN cannot solve the Index question without Ω(n) memory. The key
observationisthattherecurrentformofRNNsallowedthealgorithmtoberuninastreamingfashionwitho(n)bit
memory. Herestreamingmeansthatthealgorithmgetstolookateachbitofthememorysequentiallyandcanonly
updateaconstantsizeofmemory.
Lemma B.24. Consider the following two-partygame, where Alice receives string x ∈ {0,1}n and Bob receives
anintegerk,andBobwantstoknowthevalueofx . IfonlyAliceisallowedtosendasignaltoBob,thenΩ(n)bit
k
communicationcomplexityisrequired.
30PREPRINT
ProofofLemmaB.24. SupposethereexistsacommunicationprotocolwhereBonlyreceiveso(n)bitandcanperfectly
decidex . BecauseAlicedoesn’tknowk,theprotocolmustsendthesamemessagetoBobforallk. HenceBobcan
k
reconstructthewholestringxwithnbitwitho(n)bitcommunication. Thisisacontradiction.
NowifRNNcansolvetheIndexproblemwitho(n)bitmemory,thenitcanalsosolvetheIndexproblemwitho(n)bit
communicationcomplexity. ThisisbecauseAlicecansimplyruntheRNNoninputxandsendthehiddenstateto
Bob. ThenBobcanruntheRNNwiththehiddenstateandktogettheanswer. ThisisacontradictiontoLemmaB.24.
HenceRNNcannotsolvetheIndexproblemwitho(n)bitmemory.
Ontheotherhand,wewillshowthatTransformerscansolvetheIndexproblemwithO(logn)bitparameters. Thisis
becauseusing2layersofTransformer,wewillimplementaMatchBlock(LemmaB.15)thatcanmatchthelastquery
tokenwiththepositionoftheprevioustokenandretrievethetypeofthematchedtokentothequerytoken.
WhenT isAR,wthoutlossofgenerality,weassumethatniseven. Theproofissimilartotheproofoftheproofof
theIndexproblem. Astherearendifferenttypesoftokens,wecanlabelthemas[n]. Nowforanybooleansequence
x∈{0,1}n/2,solvingARforthefollowinginputisequivalenttosolvingtheIndexproblemforx:
S =<s>,1,x +n/2,2,x +n/2,...,n/2,x +n/2,k
in 1 2 n/2
ThisthenimpliesthatRNNcannotsolveARwitho(n)bitmemory. Transformers,ontheotherhand,canstillsolveAR
withO(logn)bitparameters,wewilluseonelayerofcopyingfunctiontocopyeachtoken’sprevioustoken’stypetoit.
ThenwecanusetheMatchBlocktomatchthelastquerytokenwiththepositionoftheprevioustokenandretrievethe
typeofthematchedtokentothequerytoken.
WhenT isc-gramretrieval,withoutlossofgenerality,weassumethatnisamultipleofc. Theproofissimilartothe
proofofTheorem4.6. Astherearendifferenttypesoftokens,wecanlabelthemas[n]. Nowforanybooleansequence
x∈{0,1}n/2,solvingARforthefollowinginputisequivalenttosolvingtheIndexproblemforx:
S =<s>,1,...,1,x +n/c,2,...,2,x +n/c,...,n/c,...,n/c,x +n/c,k,...,k
in 1 2 n/c
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
c−1 c−1 c−1 c−1
ThisthenimpliesthatRNNcannotsolvec-gramretrievalwitho(n)bitmemory. Transformers,ontheotherhand,can
stillsolvec-gramretrievalwithO(logn)bitparameters,wewilluseonelayerofcopyingfunctiontocopyeachtoken’s
previousc−1tokens’typetoit. ThenwecanusetheMatchBlocktomatchthelastquerytokenwiththepositionof
theprevioustokenandretrievethetypeofthematchedtokentothequerytoken.
WhenT isCounting,wewillfirstshowwhyRNNcannotsolvetheCountingquestionwithoutΩ(n)memory. Consider
the following setup, given any x ∈ {0,1}n, the input string is j j ...j where {j ...j } = {j | x = 1}, then
1 2 k i k j
solvingtheCountingquestionforthisinputstringforqueriedthreshold1isequivalenttosolvingtheIndexproblemfor
x. ThisthenimpliesthatRNNcannotsolvetheCountingquestionwitho(n)bitmemory.
Ontheotherhand, wewillshowthatTransformerscansolvetheCountingquestionwith O(logn)bitparameters.
This is because using 2 layers of Transformer, we can first use a COPY block to copy the last query token to the
tokencorrespondstothethreshold,andthenuseaCOUNTblock(LemmaB.14)thatcancountthenumbermofthe
appearanceofthelastquerytokenintheinputsequence,andthenwrite1/(m+1)tooneofthedimension. Finally,we
canusetheFeedForwardNetworkonthelastlayertomultiplythreshold+1withthisvalueandcomparetheresultto1
togettheanswer.
B.5 ProofofTheorem4.1
Wewillrestatethetheoremforclarity.
Theorem4.1. AssumingPSPACE̸⊂P/Poly,thereexistsataskwithinputlengthnthatcanbesolvedbyaLinear
RNN(Definition3.6)familywithΘ(logn)bitmemorywithpolynomiallengthCOTbutcannotbesolvedbyanyregular
RNN(Definition3.4)familywithΘ(logn)bitmemorywithoutCoT.
Proof. Wewillfirstprovealemma,
LemmaB.25. IfPSPACE̸⊂P/Poly,thenthereexistsaTuringmachineM withlinearspacecomplexitythatcannot
besimulatedbyapolynomial-sizecircuitfamily.
31PREPRINT
Proof. Wewillprovethisbycontradiction. AssumingforeveryTuringmachineM withlinearspacecomplexity,there
existsapolynomial-sizecircuitfamily{C }thatcansimulateM. Wewillconstructapolynomial-sizecircuitfamily
n
{C′}thatcandecidePSPACE,whichcontradictstheassumptionthatPSPACE̸⊂P/Poly.
n
GivenanylanguageL∈PSPACE,wecandecideLbyaTuringmachineM withspaceO(nk)forsomeconstantk.
L
WecanconsideranotherlanguageL′ = {x1|x|k | x ∈ L}. WecandecideL′ byaTuringmachineM withlinear
L′
space complexity by checking the length of the input and then simulating M . By the assumption, there exists a
L
polynomial-sizecircuitfamily{C }thatcansimulateM . Wecanthenconstructapolynomial-sizecircuitfamily
n L′
{C′}thatcandecideLbyfirstcountingthelengthoftheinputandthensimulatingC ontheextendedinput. This
n n
contradictstheassumptionthatPSPACE̸⊂P/Poly.
By Lemma B.25, we know that if PSPACE ̸⊂ P/Poly, then there exists a Turing machine M with linear space
complexitythatcannotbesimulatedbyapolynomial-sizecircuitfamily. WewillusethisresulttoproveTheorem4.1.
Wewilldesignthetaskasfollows,foranyn,letm=⌊log n⌋,foranybooleaninputxoflengthm,wewillchoose
2
inputsequenceasS =0n−mxandthelabelasy =YESifM(x)=1andy =NOotherwise.
in
BecauseweareconsideringregularRNNwithO(m)memory,weknowthatwecancomputetheresultofRNNwithout
CoTthroughacircuitfamilywithsizePoly(m). However,weknowthatM cannotbesimulatedbyapolynomial-size
circuitfamily. Hence,noRNNfamilywithO(m)memorycansolvethetaskforalln.
Ontheotherhand,wecansimulateM bytheRNNbymaintainingthestate,thepointer,andthetapeoftheM inside
thestateoftheRNN.TheRNNcanthenmaintainthetransitionfunctionoftheTuringmachineinitsoutputfunctionas
alookuptableLemmaB.4andwritedowntheupdatedstate,thedirectionofthepointermovement,andtheupdated
cellvalueatthepointerinitscontext. PairedwiththeabilitytomemorizetherecentinputLemmaB.20,theRNNcan
thensimulatetherunningoftheTuringmachine.
BecausethespacecomplexityofM islinearinm,thetimecomplexityofM isexp(O(m))whichispolynomialinn.
Hence,wecansolvethetaskbyanRNNwithCoTandO(m)memoryandpolynomial-sizecircuitfamily.
B.6 ProofofTheorem4.7
Wewillnowproceedtoproveourmaintheorem,whichstatesthatTransformerswithchain-of-thoughtcansolveIsTree
perfectly,whileRNNscannot. Wewillfirstrestatethetheoremhere.
Theorem4.7. ForanyRNNfamilyRwitho(n)bitmemory,RcannotperfectlysolveIsTreeofsizenforlargeenough
n,withanylengthofCoT.Ontheotherhand,thereexistsaTransformerT withconstantdimensionanddepth,and
1
O(logn)precisionthatcansolveIsTreeofsizenperfectlywithChainofThoughtoflengthO(n).
ProofofTheorem4.7. Wewillprovethistheorembyprovingthefollowinglemmas.
LemmaB.26. ForanynandRNNRwitho(n)memory,Rcan’tperfectlysolveIsTreeofsizen.
LemmaB.27. ThereexistsaTransformerT withconstantdepthandwidth,andO(logn)precision,thatcansolve
IsTreeofsizenperfectlywithChainofThought.
ThisproofisadirectcombinationofLemmasB.26andB.27.
B.6.1 ProofofLemmaB.26
ProofofLemmaB.26. WewillfirstreduceanotherproblemincommunicationcomplexitytoIsTree.
Lemma B.28. Consider the following two-partygame, where Alice receives string x ∈ {0,1}n and Bob receives
an integer k, and Bob wants to know whether x = x . If only Alice is allowed to send information, Ω(n) bit
k k−1
communicationcomplexityisrequired.
ProofofLemmaB.28. We can reduce this problem to the problem in Lemma B.24. Considering the game
in Lemma B.24, given any x ∈ {0,1}n, we can construct x˜ = (cid:80)i x mod 2. Then x˜ is a string of length
i j=1 i
n with only 0 and 1. Moreover, x = x if and only if x˜ = x˜ . Hence, if Bob can solve the problem
k k−1 k k−1
inLemmaB.28witho(n)bit,hecansolvetheprobleminLemmaB.24. Thisisacontradiction.
32PREPRINT
NowsupposethatwehaveastreamingalgorithmforIsTreewithonlyo(n)memory. WeshallproveAliceandBob
inLemmaB.28canuseittosolvetheoriginalquestionwitho(n)memory.
Considerthefollowinggraphwithn+2nodes. Thereisanodeicorrespondingtoeachx fori∈[n]andtwospecial
i
nodesn+1,n+2. Nodeiwillbeconnectedton+1ifx =0andton+2ifx =1. Moreover,k−1andkwillbe
i i
connected. NowtheoriginalanswerBobwantsisFalseifandonlyifthegraphisaTree. Hence,givenaccesstothe
streamingalgorithm,AlicecanrunitontheedgesthatsheknowsexistandsendthememorytoBob. Bobcanthenrun
itontheedgesthatheknowsexist. Combiningtheywillbeabletosolvetheoriginalproblem. Thisisacontradiction.
Moreover,asRNNwithchain-of-thoughtisalsoastreamingalgorithm,italsorequiresΩ(n)memory.
B.6.2 ProofofLemmaB.27
ProofofLemmaB.27. Theproofistwo-folded. WewillfirstdefineanalgorithmthatcansolveIsTreebygeneratinga
sequenceofverticesoflengthO(n),andthenwewillshowthatthissequencecanbegeneratedbyaTransformerwith
constantdepthandwidth,andO(logn)precisionasaChainofThought.
Algorithm1Depth-FirstSearchAlgorithm
Require: AgraphG=(V,E)withnverticesandE hasanorderinge ,...,e .
1 m
1: InitializetwostacksofverticesS 1,S 2withS 1 =[v 1],S 2 =∅.
2: whileS 1isnotemptydo
3: LetvbethetopofS 1. Yieldv.
4: ifthereexistsaneighboruofvnotinS 1∪S 2then
5: Chooseusuchthatedge(u,v)hasthesmallestpossibleorderandpushutoS 1.
6: else
7: PopvfromS 1andpushvtoS 2.
8: endif
9: endwhile
Algorithm for IsTree. We define Algorithm 1 as a depth-first search algorithm that can generate a sequence of
verticesoflengthO(n)thatcanbeusedtosolveIsTree. WewillusetwostacksS ,S tostorethevertices. S willbe
1 2 1
usedtostoretheverticesthatarenotyetvisited,andS willbeusedtostoretheverticesthatarealreadyvisited. The
2
algorithmwillstartwithS =[v ]andS =∅. Ateachstep,thealgorithmwillpopthetopofS andpushittoS .
1 1 2 1 2
ThenitwillpushalltheneighborsofthepoppedvertexthatarenotinS ∪S toS . Thealgorithmwillterminatewhen
1 2 1
S isempty. WewilldenotetheyieldedverticesequenceforGasA(G). Thefollowinglemmashowstheconnection
1
betweentheresultofthealgorithmandtheIsTreeproblem.
LemmaB.29. ForanygraphG,A(G)isatreetraversalofaspanningtreeoftheconnectedcomponentofGcontaining
v . HenceIsTree(G)isTrueifandonlyifGhasn−1edgesandA(G)contains2n−1vertices.
1
ProofofLemmaB.29. First, every vertex in the connected component of G containing v will be visited. This is
1
becausethealgorithmwillalwayspushalltheneighborsofthepoppedvertexthatarenotinS ∪S toS . Hence,the
1 2 1
algorithmwillterminatewhenalltheverticesintheconnectedcomponentofGcontainingv arevisited.
1
Second, everytwoconsecutiveverticesintheyieldedsequencewillbeconnectedbyanedge. Thisisbecausethe
algorithmwillalwayspushoneoftheneighborsofthepoppedvertexthatisnotinS ∪S toS . Hence,everytwo
1 2 1
consecutiveverticesintheyieldedsequencewillbeconnectedbyanedge. Ontheotherhand,thecombinationof
theseedgeswillformatreebecausethealgorithmwillneverpushavertexthatisalreadyinS ∪S toS . Hence,the
1 2 1
yieldedsequenceisatreetraversalofaspanningtreeoftheconnectedcomponentofGcontainingv .
1
ConstructionofTransformer. WewillnowshowthattheyieldedsequenceofAlgorithm1canbegeneratedby
aTransformerwithconstantdepthandwidth,andO(logn)precisionasaChainofThought. TheTransformerwill
generateavalidyieldedsequencebutcanterminateearlyifthegraphisnotatree.WewillnowdescribetheTransformer
indetail. WewillassumetheinputtokensequenceS isasfollows,
S =Tokenize(G),v ,...v (10)
1 r
forsomer ≥0andv ...v isavalidyieldedsequence. ThelengthofTokenize(G)is3n−2with3tokensforeach
1 r
edgesand1specialtoken<s>. WewillfurtherdenotetheinputtothefirstlayerX asEmb(S). Wewillsimilarly
denotetheinputtolayerℓasX(ℓ). WewillalsodenotetheoutputofthelastlayerasXout.
33PREPRINT
1. Layer1andLayer2Attention. TheattentionatLayer1willoutputzeroandtheFFNatLayer1andAttention
atLayer2willimplementacountingfunction(DefinitionB.8)tocountthenumberofverticesnappearsinthe
previoustokensequenceandwriteROUND(cid:0)1,p(cid:1)
inanewdimensioni asaresult.
n 1
2. Layer2FFNandLayer3Attention. TheFFNatLayer2andAttentionatLayer3willimplementacopying
function(DefinitionB.7)copyingthefirstdimensionandthecountingdimensioni ofeachtokentoitssuccessorat
1
twonewdimensionsi andi . Foreachedge,thismovesthetypeofthefirstverticeandthenumberoftimesthefirst
2 3
verticeappearsto∼. Foreveryverticeinthechainofthought,thismovesthetypeofthepreviousverticetothem.
3. Layer3FFNandLayer4Attention. TheFFNatLayer3andAttentionatLayer4willimplementanothercopying
function,copyingthedimensionsi andi ofeachtokentoitssuccessorattwonewdimensionsi andi .Especially,
2 3 4 5
foreachedge,thismovesthetypeofthefirstverticeandthenumberoftimesthefirstverticeappearstotheposition
correspondingtothesecondvertices.
4. Layer4FFN.ThisFFNwillprocesstheinformationgatheredfromthepreviouslayerandprepareforthenextlayer.
ItwillmakesurethefollowingpropertiesholdforX(5),
• Foreverytoken,thepositionnumber,itssquare,and1willbekeptinthelastthreedimensions.
• Forthefirstverticesineachedges,∼and<s>Therestdimensionwillbezero.
• For the second vertices of each edges (a,b), there will be four dimensions i ,i ,i ,i with value a,b and
6 7 8 9
n ,n ,wheren =ROUND( 1 ,1).
a,e b,e a,e 1+#aappearsuptocurrentedge
• For vertice v in v ,...,v , there will be four dimensions i ,i ,i ,i with value v ,v and v2,v2
l 1 r 10 11 12 13 l l−1 l l−1
(v =0).
0
5. Layer5Attention. CombiningwiththepreviousLayer4FFNlayer,wewillimplementtwomatchfunctionswith
twoattentionheadsmatching(i ,i )or(i ,i )with(i ,i )atLayer5Attention,i.e. findingtheedgeininput
10 11 11 10 6 7
foreachstepinthechainofthought,wewillthencopyn todimensionsi andi .
vl,(vl,vl−1) 8 9
6. Layer6. WewilluseLayer5FFNandLayer6Attentiontoimplementthematchfunctionthatmatchesdimension
i ofthecurrenttokentoi intheprevioustoken. Thiswillmatchv tothefirstappearanceofv inthechainof
10 10 l l
thoughtandwewillcopyi ofthematchedtokentoi . Thisdimensionwillbethefirstpredecessorofv inthe
11 22 l
chainofthought(0forv ). Wewilldenotethispredecessorofv tobef(v )asitisthefatherofv inthetree. Now
1 l l l
wewillneedtosplitintotwocasesdependingonwhetherv isf(v ). Ifv =f(v )orv =0(forv ),we
l−1 l l−1 l l−1 1
willsetdimensioni tobe1andi tobe0. Otherwise,wewillkeepdimensioni andi asn .
8 9 8 9 vl,(vl,vl−1)
7. Layer 7. Now we will use Layer 6 FFN and Layer 7 Attention with two attention heads to implement two
MatchNextfunctions5 (DefinitionB.12)whichusei ori asthecountingindex,andmatchv ati toi ori
8 9 l 10 6 7
respectively. Wewillthencopydimensionsi toi ofthematchedtokenstoi toi (becausetherewillbetwoof
6 9 14 21
them).
Thematchnextfunctionwillbeabletoretrievethefirstedgecontainingv . Foranyi ≥ 2,oneofthematches
1
nextfunctionwillbeabletoretrievethenextedgecontainingv after(v ,v )ifitexists. Ifitdoesn’texist,the
i i i+1
correspondingcountingdimensionwilleitherbezeroornosmallerthann . WewilluseLayer6FFNto
vl,(vl,vl−1)
decidewhetherthenextedgeexistsandsetdimensioni oftheoutputofLayer6tobetheotheredgeinthenext
14
edgeifitexists,or0otherwise,andi ,i oftheoutputoflayer6tobethecountingdimensionofthenextedgeif
15 16
itexists,or0otherwise. Foreachedgeintheoriginalinput,wewillalsosetdimensioni ,i tobethecounting
15 16
dimensionoftheedge.
8. Layer8AttentionWewillgrabthenextedgeagain,inthesamemannerasLayer6,butthistimeusingdimension
i andi . Thenecessityofthisstepisthatthenextedgecontaining(v ,v )intheoriginalgraphcanbethe
15 16 i−1 i
sameasthe(f(v ),v )andinsuchcaseweneedtocheckwhetherthenextedgeafterthisedge.
l i
9. Layer8FFN.Wenowhave,ateachpositioncorrespondingtov ,thefirstedge(f(v ),v )intheyieldedsequence
l l l
containingv andtheothervertexintheedgecontainingv thathasn’tbeenvisitedifitexists. Iftheydon’texist,the
l l
correspondingdimensionwillbezero. ThisallowsustouseLayer8sFFNtodecidethenextvertexintheyielded
sequence,whichisexactlythefirstvertexdifferentwithf(v )inthetwoedgesiftheyexist,orf(v )otherwise. We
l l
willuseLayer8FFNtocalculatethepotentialnextvertexandputitindimensioni anditssquareini .
23 24
10. Layer9Attention. CombiningwithLayer8FFN,wewillmatchi ofthecurrenttokentoi oftheprevious
23 10
tokentofindthefirstappearanceofthepotentialnextvertexinthechainofthought. Wewillthencopydimensiond
5TheconstantAhereinDefinitionB.12is1
34PREPRINT
ofthematchedtokentoi . Thisvaluebeing1willimplythisvertexhasneverappearedinthechainofthought
25
beforeandanyothervaluewillimplythevertexhasappearedbefore.
11. Layer9FFN.Wecannowcheckseveralcases,
• Ifthepotentialnextvertexviseitherf(v )̸=0orneverappearsinthechain-of-thoughtsequence,thenLayer9
r
willoutputn[−v,1,−1,...−1],whichwilldecodestov.
• Ifthepotentialnextvertexvisnotf(v )andappearsinthechain-of-thoughtsequence,thenLayer9willoutput
r
nw ,whichwilldecodestoNO,becausethechainofthoughthasalreadyvisitedvandhencethegraphisnota
6
tree.
• Ifv =1andthepotentialnextvertexvisf(v )=0,thismeansthechainofthoughthasfinished. Inthiscase,
r r
layer9FFNwillcheckwhetherthepositionis3n−2+2n−1=5n−3andoutputnw ifitis,oroutputnw
5 6
otherwise,whichwilldecodetoYESandNOrespectively.
ThisconcludestheconstructionoftheTransformer.
B.7 ProofofTheorem4.8
WewillnowproveTheorem4.8. Wewillfirstrestatethetheoremforconvenience.
Theorem4.8. GivenanyconstantA>0,constantwordwidthandnumberofspecialsymbolsd,n >0,foranyn,
S
precisionp=Θ(Alogn)andRNNRwithwordembeddingW(E) ∈R(n+nS)×dsuchthateachrecurrentiterationcan
p
becalculatedwithacircuitwithsizeP(n)≤2p/2,thereexistsaTransformerT withO(P(n)logmax{P(n),n})bit
parameterandwordembedding(cid:2) W(E) 0(n+nS)×d(cid:3) thatcansimulatetheRNNwithatmostnAstepchain-of-thought
precisely,usingatmost(P(n)+1)nAstepchainofthoughtoneveryinputwithlengthn.
Proof. TheproofismotivatedbytheTheoremB.30fromLietal.(2024).
TheoremB.30(Theorem3.3ofLietal.(2024)). ForanynandanycircuitwithsizeT(n)andinputsizen,thereexists
aTransformerwithconstantdepthandprecision,O(logn)width,andapositionembeddingwithsizeO(T(n)logn),
suchthatforanyinputS oflengthn,theTransformercomputestheoutputofthecircuitonS usingT(n)steps.
However, direct utilization of Theorem B.30 is not feasible because we are interested in (1) O(logn) precision
Transformer, and (2) simulating the RNN for nA step, which would correspond to a circuit with nAP(n) in size.
However,asthecalculationisrecurrent,wecanencodetheRNNcircuitinO(P(n))parameterinstead.
Todoso,wewillunrollthecircuitofeachrecurrentstepoftheRNNintoP(n)gates. Wewillthenassigneachgatea
uniqueidin[P(n)]andassumethecircuitiscalculatedintheorderofthegateidinthefollowingmanner.
1. Eachgatehasatypet(i),whichiseitheraconstantgateoutputting1,aninputgate,ahiddenstategate,anAND
gate,oranXORgate.
2. Eachgatei’soutputdependsontwovaluesl(i)andr(i). Ift(i)isaconstantgate,thenl(i)andr(i)areassignedto
be0. Whenitisaninputgate,l(i)willbeassignedtobethecoordinateoftheinputembeddingandr(i)willbe
assignedtobetheindexofthebitofthevalueatl(i)coordinate. Whenitisahiddenstategate,l(i)willbeassigned
tobethecoordinateofthehiddenstateembedding,andr(i)willbeassignedtobetheindexofthebitofthevalue
atl(i)coordinate. IfitisanANDgateoranXORgate,l(i)andr(i)willbeassignedtobetheidofthetwogates
thatitdependson.
WewillfurtherassumewithoutlossofgeneralitythatthehiddenstategateisthefirstpΛgate. TheoutputofthelastpΛ
gatewillbethenexthiddenstate. Wewillalsoassumethatthelastp(Λ+d)topΛ−1gatesaretheoutputgates. We
willnowfirstdescribethechainofthoughtthattheTransformerwilloutputandthenconstructtheTransformer.
Chainofthought TakinganyinputS withlengthn,theTransformerwilloutputasequenceof0and1tokens. The
firstntokenswillbethesameastheinputsequence. Foreacha ≥ 0andb ∈ [P(n)+1],then+a(P(n)+1)+b
tokenis
1. theoutputofgatebwhenRNNcircuitiscalculatingtheoutputatapositionplus1,ifb≤P(n).
2. then+a+1tokenintheRNNchainofthought,ifb=P(n)+1.
35PREPRINT
ConstructionoftheTransformer.
1. Layer1. ThefirstattentionlayerwilloutputzeroandthefirstFFNlayerwillbeofwidthO(P(n)),encodingallthe
gateinformation. Theoutputofthefirstlayeratpositionn+a(P(n)+1)+bwillhavethefollowingcoordinate:
• Theinputiwillbeencodedinthefirstdimensions.
• a,a2,b,b2willbeencodedinfourdifferentdimensions.
• The gate type t(s(b)) will be encoded in the next dimension, where s(b) = (b + 1) mod (P(n) + 1) If
b=P(n)−1,thenthegatetypewillbeencodedas0.
• Thenecessarydependencel(s(b)),l2(s(b))andr(s(b)),r2(s(b))willbeencodedinthenexttwodimensions.
• Aconstant1willbeencodedinthenextdimension.
2. Layer2Attention. TogetherwiththeLayer1FFN,theLayer2Attentionwillimplementtwomatchfunctions
(DefinitionB.9)tocopytheoutputofgatel(b+1)andr(b+1)whenRNNcircuitiscalculatingtheoutputata
position. Whenthetypeofgateb+1isnotANDorXOR,theresultwillnotbeusedinthefollowingcalculation.
3. Layer2FFNLayer2FFNwillbeofwidthO(1). Theoutputofthelayerwillbe
• Whenb<P(n)andt(s(b))isANDorXORorconstant,onedimensionoftheoutputwillbetheoutputofgate
b+1whenRNNcircuitiscalculatingtheoutputataposition.
• Whenb<P(n)andt(s(b))isaninputorhiddenstategateorb=P(n)+1,onedimensionoftheoutputwill
bethepositioninthecurrentchainofthoughtwheretheinputbitorhiddenstatebitiscopiedfromandtheother
dimensionwillbethesquareofthatposition
• Whenb=P(n),theoutputremainsthesameastheinputtoLayer2FFN.
4. Layer3Attention. Layer3AttentionwillbeofwidthO(1). TogetherwithLayer2FFN,Layer3Attentionwill
implementMatchheads(DefinitionB.9)tocopytheoutputatthepositionwheretheinputbitorhiddenstatebit
iscopiedfrom. Whenthetypeofgateb+1isnotinputorhiddenstategate, theresultwillnotbeusedinthe
followingcalculation.
5. Layer3FFNLayer3FFNwillbeofwidthO(1). Theoutputofthelayerwillbe
• Whenb̸=P(n),onedimensionoftheoutputwillbeoutputofgates(b)whenRNNcircuitiscalculatingthe
outputata+1[b=P(n)+1]position.
• Whenb=P(n),theoutputremainsthesameastheinputtoLayer3FFN.
6. Layer4Layer4Attentionwillhavep−1headsandeachheadwillbeofwidthO(1). Headh∈[p−1]willcopy
thefirstdimensionoftheoutputofLayer3FFNatpositionn+a(P(n)+1)+b−(p−h)andweighteachof
themby2−h+(p−1)/2andaddtheminonedimension. TheLayer4FFNwillcalculaterwhenthefirstdimensionof
theinputis1and−rotherwise. Hence,foreacha≥0,then+a(P(n)+1)−hp,h∈[Λ:Λ+d]tokencontains
adimensioni whichisthek−ΛdimensionoftheoutputoftheRNNatpositiona.
1
7. Layer5Layer5Attentionwillhaved+1headsandeachheadwillbeofwidthO(1). Headh∈[d+1]willcopy
thedimensioni oftheoutputofLayer4FFNatpositionn+a(P(n)+1)+b−(h+Λ)ptoadisjointdimension
1
i . TheLayer5FFNwillthenmakesuretheoutputofLayer5satisfiesthefollowing:
h+1
• When b ̸= P(n), one dimension of the output will be n times the output of gate s(b) when RNN circuit is
calculatingtheoutputata+1[b=P(n)+1]positionplus1,whichwilldecodetothecorrespondingvalue.
• Whenb=P(n),thefirstddimensionoftheoutputwillbethesameastheoutputoftheRNNatpositiona,and
therestdimensionwillbe0,whichwilldecodetothesametokenasthechainofthoughtoftheRNNatposition
a+1.
ThisconcludestheconstructionoftheTransformer.
B.8 ProofofTheorem5.2
Wewillfirstrestatethetheoremforclarity.
Theorem5.2. FortaskT ∈{Index,AR,c-gramretrieval,Counting},thereexistsaLinearRNNfamilywithO(logn)
bitmemoryandO(logn)parameter,thatcansolveT withIn-contextRAGinO(1)CoTsteps.
36PREPRINT
Proof. WhentaskT isIndex,givenaninputsequencethatendswithaquerytokenk,RNNwillgeneratethefollowing
searchquerysequence:<StartSearch>,^(?:\S\s*){k−1}(\S),<EndSearch>.
Thentheregularexpressionwillmatchthek-thtokenintheinputsequence. TheRNNneedstorecitetheformatofthe
query,remembertheindexkandcalculatek−1togeneratetheregularexpression. AswehaveshowninLemmasB.20
andB.23,RNNcanreciteafixedsequenceatfixedpositionandmemorizetherecentinput,theabovesequencecanbe
generatedbyanRNN.TheexplicitconstructionoftheRNNisomittedhere.
WhentaskT isAR,givenaninputsequencethatendswithaquerytokenk,RNNwillgeneratethefollowingsearch
querysequence: <StartSearch>,\bk\b(\S+)\b,<EndSearch>.
Thentheregularexpressionwillmatchthenexttokenafterthefirstoccurrenceofkintheinputsequence. TheRNN
needstorecitetheformatofthequeryandrememberthequerytokenktogeneratetheregularexpression. Theexplicit
constructionoftheRNNisomittedhere.
WhentaskT isc-gramretrieval,givenaninputsequencethatendswithquerytokens,RNNwillgeneratethefollowing
searchquerysequence: <StartSearch>,\bk ...k \b(\S+)\b,<EndSearch>.
1 c−1
Thentheregularexpressionwillmatchthenexttokenafterthefirstoccurrenceofk ,...k intheinputsequence.
1 c−1
TheRNNneedstorecitetheformatofthequeryandrememberthequerytokensk ,...k togeneratetheregular
1 c−1
expression. TheexplicitconstructionoftheRNNisomittedhere.
WhentaskT isCounting,givenaninputsequencethatendswithaquerytokenvandaquerythresholdk,RNNwill
generatethefollowingsearchquerysequence<StartSearch>,(\bv\b){k+1},<EndSearch>.
Thentheregularexpressionwillmatchthek-thoccurrenceofvintheinputsequence. TheRNNneedstorecitethe
formatofthequeryandrememberthequerytokenvandthethresholdktogeneratetheregularexpression. TheRNN
canthencheckwhethertheretrievalresultisFAILEDtodetermineifthecountislessthank. Theexplicitconstruction
oftheRNNisomittedhere.
B.9 ProofofTheorem5.3
Inthissection,wewillproveTheorem5.3. Wewillfirstrestatethetheoremforconvenience.
Theorem5.3. ThereexistsaLinearRNNfamilywithO(logn)bitmemoryandO(logn)parameter,thatcansolve
IsTreeofsizenwithIn-contextRAGinO(n)CoTsteps.
ProofofTheorem5.3. We will first define the sequence that the retrieval-augmented RNN will generate and then
constructanRNNthatcangeneratesuchasequence.
SequenceGeneration. WewilluseavariantofAlgorithm1togeneratethesequenceandwewillusetheconcatenation
ofthetokenizationofthesequencereturnedbyAlgorithm2asthesequencethattheretrievalaugmentedRNNwill
generate.
RNNConstruction. WecanusesimilarproofinTheorem5.4byhavingtheRNNmemorizelocalsequencesand
determinethephaseofAlgorithm2itisin. TheRNNwillmaintainthelengthofS (LemmaB.21)andthetopofS in
2 1
thestate(LemmaB.20)anditiseasytocheckthattheretrievalfunctionwillretrievethecorrectresultforeachsearch
query. ThewaytodeterminethenextvertexinthestackisthesameasintheproofofLemmaB.27. Wewillomitthe
simplebuttediousdetailedconstructionhere.
B.10 ProofofTheorem5.4
Inthissection,wewillproveTheorem5.4. Wewillfirstrestatethetheoremforconvenience.
Theorem5.4. GivenanyconstantA,B,foranypolynomial-timeTuringmachineT ∈TIME(nA)withBstatesand
vocabularysizeB,thereexistsaretrievalaugmentedLinearRNNfamily(seeDefinitions3.6andA.5)withvocabulary
ofBspecialsymbol,O(Alogn)bitprecisionandmemory,andO(AB2logn)bitparameters,thatcansimulatethe
resultofT onanyinputwithlengthnwithachainofthoughtoflengthO(nA).
ProofofTheorem5.4. WewilldenotethestateofT as1,...,B(wewilluse1astheinitialstate)andthevocabulary
ofT as1,...,B. WewillassumeT operatesonaninfinitetapeTAPE,whichisasequenceofcellsindexedbyZ. We
willalsoassumethatthetapeisinitializedwithallcellsbeing0exceptforthencellstartingat1. TheTuringmachine
37PREPRINT
Algorithm2Depth-FirstSearchAlgorithmwithRetrieving
Require: AgraphG=(V,E)withnverticesandE hasanorderinge ,...,e .
1 m
1: InitializetwostacksofverticesS 1,S 2withS 1 =[v 1],S 2 =∅,alistLwithL=∅,andavertexv′ =FAILED.
2: whileS 1isnotemptydo
3: LetvbethetopofS 1. PushvtoL.
4: Generatetheregularexpressionr 2 =\b(\S+)\b−\bv\b.
5: Letf(v)bethepredecessorofvinS 1forthefirsttimeandFAILEDwhenv =v 1.
6: Push<StartSearch>,r 2,<EndSearch>,f(v)toL.
7: ifv′ ̸=f(v)then
8: Generatetheregularexpression
(cid:16) (cid:17) (cid:16) (cid:17)
r 1= \bv′\b∼\bv\b|\bv\b∼\bv′\b .*? \b(\S+)\b∼\bv\b|\bv\b∼\b(\S+)\b
9: else
10: Generatetheregularexpressionr 1 =\b(\S+)\b∼\bv\b|\bv\b∼\b(\S+)\b
11: endif
12: Push<StartSearch>,r 1,<EndSearch>toL.
13: if thereexistsaneighboruofvsuchthat(u,v)haslargerorderthan(v,v′)whenv′ ̸=f(v)orthereexistsa
neighboruofvsuchthatu̸=f(v)whenv′ =f(v)then
14: Chooseusuchthatedge(u,v)hasthesmallestpossibleorderandpushutoL. Letv′′ =u.
15: else
16: PushFAILEDtoL. Letv′′ =FAILED.
17: endif
18: ifv′′ =f(v)̸=FAILEDthen
19: Generatetheregularexpression
(cid:16) (cid:17) (cid:16) (cid:17)
r 3= \bv′′\b∼\bv\b|\bv\b∼\bv′′\b .*? \b(\S+)\b∼\bv\b|\bv\b∼\b(\S+)\b
20: Push<StartSearch>,r 3,<EndSearch>toL.
21: if thereexistsaneighboruofvsuchthat(u,v)haslargerorderthan(v,v′′)then
22: Chooseusuchthatedge(u,v)hasthesmallestpossibleorderandpushutoL. Letv′′ =u.
23: else
24: PushFAILEDtoL. Letv′′ =FAILED.
25: endif
26: endif
27: ifv′′ =FAILEDthen
28: PopvfromS 1. PushvtoS 2. Letv′ =v.
29: else
30: Generatetheregularexpressionr 4 =\b(\S+)\b−\bv′′\b
31: Push<StartSearch>,r 4,<EndSearch>,0toL.
32: ifv′′isnotinS 1then
33: PushFAILED,v,−,v′′toL.
34: Pushv′′toS 1. Letv′ =v.
35: else
36: Letf(v′′)bethepredecessorofvinS 1forthefirsttimeandFAILEDwhenv′′ =v 1.
37: Pushf(v′′),NOtoL.
38: returnL.
39: endif
40: endif
41: endwhile
42: ifS 2hasnverticesthen
43: PushYEStoL.
44: returnL.
45: else
46: PushNOtoL.
47: returnL.
48: endif
38PREPRINT
alsohasapointerpthatpointstoacellinthetape. Thepointerisinitializedto1. Ateachtimestep,theTuringmachine
readsthecellpointedbyPOINTERandupdatesthecellpointedbyPOINTERandthepointerpaccordingtothe
transitionfunctionδ :[B+1]×[B]→[B]×[B]×{−1,1},whichtakesthecurrentstateandthecurrentcellvalue
(couldbeempty,whichcorrespondstoB+1)asinputandoutputsthenewstate,thenewcellvalueandthedirection
tomovethepointer. TheTuringmachinehaltswhenthestateisB. BecauseT ∈ TIME(nA),theTuringmachine
willalwayshaltinnAsteps. WewilluseTAPE[t,i]asthevalueonthei-thcellonTAPEbeforethettimestep. We
willusePOINTER[t]asthevalueofthepointerbeforethettimestepandState[t]asthestateoftheTuringmachine
beforethettimestep. WewillfurtheruseDirection[t]asthedirectionofthepointermovementbeforethettimestep.
Wewillfirstdefinethesequencethattheretrieval-augmentedRNNwillgenerateandthenconstructanRNNthatcan
generatesuchasequence.
Sequencegeneration. TheinputtokensequenceS willbeasfollowed,
in
S =<s>,TAPE[1,1],TAPE[1,2],...,TAPE[1,n]
in
Here all the symbols on the tape are represented by one special symbol in the vocabulary. Given this input token
sequence,theretrievalaugmentedRNNwillgeneratethefollowingoutputtokensequence,
S =S in,<StartSearch>,^(?:\S\s*).{1}(\S),<EndSearch>,
TAPE[1,1]
1,TAPE[1,1],1,
...
<StartSearch>,^(?:\S\s*).{n}(\S),<EndSearch>,
TAPE[1,n]
n,TAPE[1,n],n,
<StartSearch>,((POINTER[1] (.) POINTER[1].*?$)),<EndSearch>,
SearchResult(1),
POINTER[1],TAPE[2,POINTER[1]],POINTER[1]
State[2],Direction[2],
...
<StartSearch>,(POINTER[t] (.) POINTER[t].*?$),<EndSearch>,
SearchResult(t)
POINTER[t],TAPE[t+1,POINTER[t]],POINTER[t],
State[t+1],Direction[t+1],
HereSearchResult(t)isdefinedas
(cid:26)
FAILED;ifPOINTER[t]isemptycellbeforet
SearchResult(t)=
TAPE[t,POINTER[t]];otherwise
TheoutputtokensequencesimulatestheTuringmachineT onthetapeTAPEduetothefollowinglemma.
LemmaB.31. Givenanyt∈[nA]andi∈[nA],thelaststringinS thatcontainsi,iasasubstringisi,TAPE[t,i],i
ifTAPE[t,i]isnotemptyandistheemptystringotherwise.
Proof. Theproofisbyinduction,fort=1,theresultholds.Foranyt≥2,weonlyneedtonoticethatPOINTER[t−1]
istheonlycellthatcanbeupdatedattimet−1.
ConstructionofRNN Giventheinput,theRNNcanfirstiterateover1tonandgeneratethefirstnsearchqueries
andresultsbymaintainingacounterinitsstateandmemorizingthemostrecentsearchresult(LemmaB.20). Thenitis
easytoseethattheretrievaloraclewillgeneratethecorrectSearchResult(t)giventheinputS. Therefore,wewill
onlyneedtoconstructanRNNthatcangeneratetherestpartofS.
WewillassumetheRNNmaintainsthestateandpointeroftheTuringmachineinitsstateandshowthattheycanbe
updated.
Based on Lemma B.20, the RNN can maintain constant recent token types in its state, we will assume the RNN
memorizethelasttokensuptothemostrecent<StartSearch>andalsocalculatethepositionrelativetothemostrecent
39PREPRINT
<StartSearch>.ByalookuptableintheFFNLemmaB.4,theRNNcanoutputthefixedformatofthesearchquery.Sim-
ilarly,RNNcanoutputthePOINTER[t]. TogeneratetheupdateTAPE[t+1,POINTER[t]],State[t],Direction[t],
theRNNcanuseaFFNwithO(B2)widthtomemorizethetransitionfunctionoftheTuringmachineandoutputthe
update. Then,theRNNcanusethememorizedrecentinputtoupdatethestateandthepointeroftheTuringmachineat
thenext<StartSearch>. Theproofisthencomplete.
B.11 ProofofTheorem5.6
Theorem5.6. FortaskT ∈{Index,AR,c-gramretrieval,Counting},thereexistsahybridLinearRNN(Definitions3.6
and5.5)familywithO(logn)bitmemoryandO(logn)parameter,thatcansolveT withoutCoT.
Proof. TheproofhereisessentiallythesameastheconstructionoftheTransformerinTheorem4.6. Wewouldusethe
sameTransformerlayertosolveT. TheonlydifferenceisthatwewouldusetheoutputoftheRNN,insteadofFFN,as
theinputoftheTransformerlayer.AlsoforCounting,insteadofusingaCOPYfunction,wewritethequerytokenin
thestateoftheRNN(LemmaB.20).
B.12 ProofofTheorem5.7
Theorem5.7. ThereexistsahybridLinearRNNwithO(logn)bitmemoryandO(logn)parameter,thatcansolve
IsTreeofsizenwithachainofthoughtoflengthO(nlogn). Moreover,thehybridRNNcansolvetheIsTreeproblem
definedonbinarysequence(seeproofofTheorem4.7)withoutCoT.
Proof. WewillfirstprovetheresultfortheIsTreeproblemdefinedonbinarysequence. Bymemorizingthefirstfour
edges,theRNNcanatleastdetermineoneofthespecialverticesrepresenting0or1(LemmaB.22). ThentheRNN
willmemorizethefirstfouredgesthatdon’tcontainthisspecialvertexandcanthendeterminetheotherspecialvertex
(LemmasB.3andB.21). Theremainingedgethatdoesn’tcontainanyofthespecialverticeswillbethespecialedge
containingtwoverticescorrespondingtotheindex. WecanthenusetheTransformerlayertoretrievetheparentsofthe
edgesinthespecialedgeandcheckifthegraphisatree.
TheproofforthegeneralcaseissimilartotheproofofTheorem5.3. However,insteadofusingregularexpressionto
retrievethenextneighborandparent,wewillneedtousetheTransformerlayer. TheTransformerlayercanretrievethe
parentthroughanattentionheadimplementingthematchclosesthead(LemmaB.16)iftheRNNpartmaintainsthe
predecessorofeachnodeinthechainofthought.
RetrievingthenextneighborismorecomplicatedandwewilluseO(logn)stepsofthechainofthoughttodothat.
Givenanedge(v,v′),wewillfirstuseonematchheadtoretrievethepositionpof(v,v′)intheinputsequenceand
writeittothechainofthought. ThenwewillusetwoMatchCloseheadstoretrievetheedgethatcontainsv andis
closesttop+2ifori=0,1,...,log niterativelyuntiltheheadsreturnanedgethatisnot(v,v′)orireacheslog n.
2 2
Here2icanbecomputedthroughdoublingoneofthedimensionsinthestateoftheRNNandresetthatdimensionto1
aftertermination. Wewillthencomparetheretrievededgewiththefatherofv tocheckifitisthesame. Ifitisthe
same,wewillsearchthenextneighborofvaftertheparentofvinthesameway. Theotherpartoftheproofissimilar
totheproofofTheorem5.3.
B.13 ProofofTheorem5.8
Theorem 5.8. Given any constant A,B, for any polynomial-time Turing machine T ∈ TIME(nA) with B states
andvocabularysizeB,thereexistsahybridLinearRNN(seeDefinition5.5)withvocabularyofB specialsymbol,
O(Alogn)bitprecisionandmemory,andO(AB2logn)bitparameters,thatcansimulatetheresultofT onanyinput
withlengthninO(nA)CoTsteps.
Proof. SequenceGeneration. UnderthesameformulationofproofoftheTheorem5.4. ThehybridRNNwilloutput
thefollowingsequence.
S =S ,POINTER[1],TAPE[2,POINTER[1]],POINTER[1]
in
State[2],Direction[2],
...
POINTER[t],TAPE[t+1,POINTER[t]],POINTER[t],
State[t+1],Direction[t+1],
40PREPRINT
NotethatLemmaB.31stillholds. Weonlyneedtoprovethatthehybridarchitecturecangeneratetheabovesequence.
HybridConstruction. ThewayRNNmaintainsthepointersandthestatesisthesameastheproofofTheorem5.4.
Giveneachpointervalue,wecanretrievethelastvalueofthecellatthepointerthroughtheonelayerofattentionby
implementingamatchclosesthead(LemmaB.16).
41