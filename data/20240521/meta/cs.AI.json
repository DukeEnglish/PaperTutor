[
    {
        "title": "Adapting Large Multimodal Models to Distribution Shifts: The Role of In-Context Learning",
        "authors": "Guanglin ZhouZhongyi HanShiming ChenBiwei HuangLiming ZhuSalman KhanXin GaoLina Yao",
        "links": "http://arxiv.org/abs/2405.12217v1",
        "entry_id": "http://arxiv.org/abs/2405.12217v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12217v1",
        "summary": "Recent studies indicate that large multimodal models (LMMs) are highly robust\nagainst natural distribution shifts, often surpassing previous baselines.\nDespite this, domain-specific adaptation is still necessary, particularly in\nspecialized areas like healthcare. Due to the impracticality of fine-tuning\nLMMs given their vast parameter space, this work investigates in-context\nlearning (ICL) as an effective alternative for enhancing LMMs' adaptability. We\nfind that the success of ICL heavily relies on the choice of demonstration,\nmirroring challenges seen in large language models but introducing unique\ncomplexities for LMMs facing distribution shifts. Our study addresses this by\nevaluating an unsupervised ICL method, TopKNearestPR, which selects in-context\nexamples through a nearest example search based on feature similarity. We\nuncover that its effectiveness is limited by the deficiencies of pre-trained\nvision encoders under distribution shift scenarios. To address these\nchallenges, we propose InvariantSelectPR, a novel method leveraging\nClass-conditioned Contrastive Invariance (CCI) for more robust demonstration\nselection. Specifically, CCI enhances pre-trained vision encoders by improving\ntheir discriminative capabilities across different classes and ensuring\ninvariance to domain-specific variations. This enhancement allows the encoders\nto effectively identify and retrieve the most informative examples, which are\nthen used to guide LMMs in adapting to new query samples under varying\ndistributions. Our experiments show that InvariantSelectPR substantially\nimproves the adaptability of LMMs, achieving significant performance gains on\nbenchmark datasets, with a 34.2%$\\uparrow$ accuracy increase in 7-shot on\nCamelyon17 and 16.9%$\\uparrow$ increase in 7-shot on HAM10000 compared to the\nbaseline zero-shot performance.",
        "updated": "2024-05-20 17:59:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12217v1"
    },
    {
        "title": "Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving",
        "authors": "Aniket DidolkarAnirudh GoyalNan Rosemary KeSiyuan GuoMichal ValkoTimothy LillicrapDanilo RezendeYoshua BengioMichael MozerSanjeev Arora",
        "links": "http://arxiv.org/abs/2405.12205v1",
        "entry_id": "http://arxiv.org/abs/2405.12205v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12205v1",
        "summary": "Metacognitive knowledge refers to humans' intuitive knowledge of their own\nthinking and reasoning processes. Today's best LLMs clearly possess some\nreasoning processes. The paper gives evidence that they also have metacognitive\nknowledge, including ability to name skills and procedures to apply given a\ntask. We explore this primarily in context of math reasoning, developing a\nprompt-guided interaction procedure to get a powerful LLM to assign sensible\nskill labels to math questions, followed by having it perform semantic\nclustering to obtain coarser families of skill labels. These coarse skill\nlabels look interpretable to humans.\n  To validate that these skill labels are meaningful and relevant to the LLM's\nreasoning processes we perform the following experiments. (a) We ask GPT-4 to\nassign skill labels to training questions in math datasets GSM8K and MATH. (b)\nWhen using an LLM to solve the test questions, we present it with the full list\nof skill labels and ask it to identify the skill needed. Then it is presented\nwith randomly selected exemplar solved questions associated with that skill\nlabel. This improves accuracy on GSM8k and MATH for several strong LLMs,\nincluding code-assisted models. The methodology presented is domain-agnostic,\neven though this article applies it to math problems.",
        "updated": "2024-05-20 17:45:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12205v1"
    },
    {
        "title": "Hierarchical Neural Operator Transformer with Learnable Frequency-aware Loss Prior for Arbitrary-scale Super-resolution",
        "authors": "Xihaier LuoXiaoning QianByung-Jun Yoon",
        "links": "http://arxiv.org/abs/2405.12202v1",
        "entry_id": "http://arxiv.org/abs/2405.12202v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12202v1",
        "summary": "In this work, we present an arbitrary-scale super-resolution (SR) method to\nenhance the resolution of scientific data, which often involves complex\nchallenges such as continuity, multi-scale physics, and the intricacies of\nhigh-frequency signals. Grounded in operator learning, the proposed method is\nresolution-invariant. The core of our model is a hierarchical neural operator\nthat leverages a Galerkin-type self-attention mechanism, enabling efficient\nlearning of mappings between function spaces. Sinc filters are used to\nfacilitate the information transfer across different levels in the hierarchy,\nthereby ensuring representation equivalence in the proposed neural operator.\nAdditionally, we introduce a learnable prior structure that is derived from the\nspectral resizing of the input data. This loss prior is model-agnostic and is\ndesigned to dynamically adjust the weighting of pixel contributions, thereby\nbalancing gradients effectively across the model. We conduct extensive\nexperiments on diverse datasets from different domains and demonstrate\nconsistent improvements compared to strong baselines, which consist of various\nstate-of-the-art SR methods.",
        "updated": "2024-05-20 17:39:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12202v1"
    },
    {
        "title": "Multi-order Graph Clustering with Adaptive Node-level Weight Learning",
        "authors": "Ye LiuXuelei LinYejia ChenReynold Cheng",
        "links": "http://arxiv.org/abs/2405.12183v1",
        "entry_id": "http://arxiv.org/abs/2405.12183v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12183v1",
        "summary": "Current graph clustering methods emphasize individual node and edge con\nnections, while ignoring higher-order organization at the level of motif. Re\ncently, higher-order graph clustering approaches have been designed by motif\nbased hypergraphs. However, these approaches often suffer from hypergraph\nfragmentation issue seriously, which degrades the clustering performance\ngreatly. Moreover, real-world graphs usually contain diverse motifs, with nodes\nparticipating in multiple motifs. A key challenge is how to achieve precise\nclustering results by integrating information from multiple motifs at the node\nlevel. In this paper, we propose a multi-order graph clustering model (MOGC) to\nintegrate multiple higher-order structures and edge connections at node level.\nMOGC employs an adaptive weight learning mechanism to au tomatically adjust the\ncontributions of different motifs for each node. This not only tackles\nhypergraph fragmentation issue but enhances clustering accuracy. MOGC is\nefficiently solved by an alternating minimization algo rithm. Experiments on\nseven real-world datasets illustrate the effectiveness of MOGC.",
        "updated": "2024-05-20 17:09:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12183v1"
    },
    {
        "title": "Building Temporal Kernels with Orthogonal Polynomials",
        "authors": "Yan Ru PeiOlivier Coenen",
        "links": "http://arxiv.org/abs/2405.12179v1",
        "entry_id": "http://arxiv.org/abs/2405.12179v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12179v1",
        "summary": "We introduce a class of models named PLEIADES (PoLynomial Expansion In\nAdaptive Distributed Event-based Systems), which contains temporal convolution\nkernels generated from orthogonal polynomial basis functions. We focus on\ninterfacing these networks with event-based data to perform online\nspatiotemporal classification and detection with low latency. By virtue of\nusing structured temporal kernels and event-based data, we have the freedom to\nvary the sample rate of the data along with the discretization step-size of the\nnetwork without additional finetuning. We experimented with three event-based\nbenchmarks and obtained state-of-the-art results on all three by large margins\nwith significantly smaller memory and compute costs. We achieved: 1) 99.59%\naccuracy with 192K parameters on the DVS128 hand gesture recognition dataset\nand 100% with a small additional output filter; 2) 99.58% test accuracy with\n277K parameters on the AIS 2024 eye tracking challenge; and 3) 0.556 mAP with\n576k parameters on the PROPHESEE 1 Megapixel Automotive Detection Dataset.",
        "updated": "2024-05-20 17:06:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12179v1"
    }
]