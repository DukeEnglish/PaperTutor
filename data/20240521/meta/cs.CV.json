[
    {
        "title": "Images that Sound: Composing Images and Sounds on a Single Canvas",
        "authors": "Ziyang ChenDaniel GengAndrew Owens",
        "links": "http://arxiv.org/abs/2405.12221v1",
        "entry_id": "http://arxiv.org/abs/2405.12221v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12221v1",
        "summary": "Spectrograms are 2D representations of sound that look very different from\nthe images found in our visual world. And natural images, when played as\nspectrograms, make unnatural sounds. In this paper, we show that it is possible\nto synthesize spectrograms that simultaneously look like natural images and\nsound like natural audio. We call these spectrograms images that sound. Our\napproach is simple and zero-shot, and it leverages pre-trained text-to-image\nand text-to-spectrogram diffusion models that operate in a shared latent space.\nDuring the reverse process, we denoise noisy latents with both the audio and\nimage diffusion models in parallel, resulting in a sample that is likely under\nboth models. Through quantitative evaluations and perceptual studies, we find\nthat our method successfully generates spectrograms that align with a desired\naudio prompt while also taking the visual appearance of a desired image prompt.\nPlease see our project page for video results:\nhttps://ificl.github.io/images-that-sound/",
        "updated": "2024-05-20 17:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12221v1"
    },
    {
        "title": "Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo",
        "authors": "Tianqi LiuGuangcong WangShoukang HuLiao ShenXinyi YeYuhang ZangZhiguo CaoWei LiZiwei Liu",
        "links": "http://arxiv.org/abs/2405.12218v1",
        "entry_id": "http://arxiv.org/abs/2405.12218v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12218v1",
        "summary": "We present MVSGaussian, a new generalizable 3D Gaussian representation\napproach derived from Multi-View Stereo (MVS) that can efficiently reconstruct\nunseen scenes. Specifically, 1) we leverage MVS to encode geometry-aware\nGaussian representations and decode them into Gaussian parameters. 2) To\nfurther enhance performance, we propose a hybrid Gaussian rendering that\nintegrates an efficient volume rendering design for novel view synthesis. 3) To\nsupport fast fine-tuning for specific scenes, we introduce a multi-view\ngeometric consistent aggregation strategy to effectively aggregate the point\nclouds generated by the generalizable model, serving as the initialization for\nper-scene optimization. Compared with previous generalizable NeRF-based\nmethods, which typically require minutes of fine-tuning and seconds of\nrendering per image, MVSGaussian achieves real-time rendering with better\nsynthesis quality for each scene. Compared with the vanilla 3D-GS, MVSGaussian\nachieves better view synthesis with less training computational cost. Extensive\nexperiments on DTU, Real Forward-facing, NeRF Synthetic, and Tanks and Temples\ndatasets validate that MVSGaussian attains state-of-the-art performance with\nconvincing generalizability, real-time rendering speed, and fast per-scene\noptimization.",
        "updated": "2024-05-20 17:59:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12218v1"
    },
    {
        "title": "Adapting Large Multimodal Models to Distribution Shifts: The Role of In-Context Learning",
        "authors": "Guanglin ZhouZhongyi HanShiming ChenBiwei HuangLiming ZhuSalman KhanXin GaoLina Yao",
        "links": "http://arxiv.org/abs/2405.12217v1",
        "entry_id": "http://arxiv.org/abs/2405.12217v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12217v1",
        "summary": "Recent studies indicate that large multimodal models (LMMs) are highly robust\nagainst natural distribution shifts, often surpassing previous baselines.\nDespite this, domain-specific adaptation is still necessary, particularly in\nspecialized areas like healthcare. Due to the impracticality of fine-tuning\nLMMs given their vast parameter space, this work investigates in-context\nlearning (ICL) as an effective alternative for enhancing LMMs' adaptability. We\nfind that the success of ICL heavily relies on the choice of demonstration,\nmirroring challenges seen in large language models but introducing unique\ncomplexities for LMMs facing distribution shifts. Our study addresses this by\nevaluating an unsupervised ICL method, TopKNearestPR, which selects in-context\nexamples through a nearest example search based on feature similarity. We\nuncover that its effectiveness is limited by the deficiencies of pre-trained\nvision encoders under distribution shift scenarios. To address these\nchallenges, we propose InvariantSelectPR, a novel method leveraging\nClass-conditioned Contrastive Invariance (CCI) for more robust demonstration\nselection. Specifically, CCI enhances pre-trained vision encoders by improving\ntheir discriminative capabilities across different classes and ensuring\ninvariance to domain-specific variations. This enhancement allows the encoders\nto effectively identify and retrieve the most informative examples, which are\nthen used to guide LMMs in adapting to new query samples under varying\ndistributions. Our experiments show that InvariantSelectPR substantially\nimproves the adaptability of LMMs, achieving significant performance gains on\nbenchmark datasets, with a 34.2%$\\uparrow$ accuracy increase in 7-shot on\nCamelyon17 and 16.9%$\\uparrow$ increase in 7-shot on HAM10000 compared to the\nbaseline zero-shot performance.",
        "updated": "2024-05-20 17:59:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12217v1"
    },
    {
        "title": "Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models Using Spatio-Temporal Slices",
        "authors": "Nathaniel CohenVladimir KulikovMatan KleinerInbar Huberman-SpiegelglasTomer Michaeli",
        "links": "http://arxiv.org/abs/2405.12211v1",
        "entry_id": "http://arxiv.org/abs/2405.12211v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12211v1",
        "summary": "Text-to-image (T2I) diffusion models achieve state-of-the-art results in\nimage synthesis and editing. However, leveraging such pretrained models for\nvideo editing is considered a major challenge. Many existing works attempt to\nenforce temporal consistency in the edited video through explicit\ncorrespondence mechanisms, either in pixel space or between deep features.\nThese methods, however, struggle with strong nonrigid motion. In this paper, we\nintroduce a fundamentally different approach, which is based on the observation\nthat spatiotemporal slices of natural videos exhibit similar characteristics to\nnatural images. Thus, the same T2I diffusion model that is normally used only\nas a prior on video frames, can also serve as a strong prior for enhancing\ntemporal consistency by applying it on spatiotemporal slices. Based on this\nobservation, we present Slicedit, a method for text-based video editing that\nutilizes a pretrained T2I diffusion model to process both spatial and\nspatiotemporal slices. Our method generates videos that retain the structure\nand motion of the original video while adhering to the target text. Through\nextensive experiments, we demonstrate Slicedit's ability to edit a wide range\nof real-world videos, confirming its clear advantages compared to existing\ncompeting methods. Webpage: https://matankleiner.github.io/slicedit/",
        "updated": "2024-05-20 17:55:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12211v1"
    },
    {
        "title": "Hierarchical Neural Operator Transformer with Learnable Frequency-aware Loss Prior for Arbitrary-scale Super-resolution",
        "authors": "Xihaier LuoXiaoning QianByung-Jun Yoon",
        "links": "http://arxiv.org/abs/2405.12202v1",
        "entry_id": "http://arxiv.org/abs/2405.12202v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12202v1",
        "summary": "In this work, we present an arbitrary-scale super-resolution (SR) method to\nenhance the resolution of scientific data, which often involves complex\nchallenges such as continuity, multi-scale physics, and the intricacies of\nhigh-frequency signals. Grounded in operator learning, the proposed method is\nresolution-invariant. The core of our model is a hierarchical neural operator\nthat leverages a Galerkin-type self-attention mechanism, enabling efficient\nlearning of mappings between function spaces. Sinc filters are used to\nfacilitate the information transfer across different levels in the hierarchy,\nthereby ensuring representation equivalence in the proposed neural operator.\nAdditionally, we introduce a learnable prior structure that is derived from the\nspectral resizing of the input data. This loss prior is model-agnostic and is\ndesigned to dynamically adjust the weighting of pixel contributions, thereby\nbalancing gradients effectively across the model. We conduct extensive\nexperiments on diverse datasets from different domains and demonstrate\nconsistent improvements compared to strong baselines, which consist of various\nstate-of-the-art SR methods.",
        "updated": "2024-05-20 17:39:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12202v1"
    }
]