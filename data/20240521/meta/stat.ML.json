[
    {
        "title": "Alternators For Sequence Modeling",
        "authors": "Mohammad Reza RezaeiAdji Bousso Dieng",
        "links": "http://arxiv.org/abs/2405.11848v1",
        "entry_id": "http://arxiv.org/abs/2405.11848v1",
        "pdf_url": "http://arxiv.org/pdf/2405.11848v1",
        "summary": "This paper introduces alternators, a novel family of non-Markovian dynamical\nmodels for sequences. An alternator features two neural networks: the\nobservation trajectory network (OTN) and the feature trajectory network (FTN).\nThe OTN and the FTN work in conjunction, alternating between outputting samples\nin the observation space and some feature space, respectively, over a cycle.\nThe parameters of the OTN and the FTN are not time-dependent and are learned\nvia a minimum cross-entropy criterion over the trajectories. Alternators are\nversatile. They can be used as dynamical latent-variable generative models or\nas sequence-to-sequence predictors. When alternators are used as generative\nmodels, the FTN produces interpretable low-dimensional latent variables that\ncapture the dynamics governing the observations. When alternators are used as\nsequence-to-sequence predictors, the FTN learns to predict the observed\nfeatures. In both cases, the OTN learns to produce sequences that match the\ndata. Alternators can uncover the latent dynamics underlying complex sequential\ndata, accurately forecast and impute missing data, and sample new trajectories.\nWe showcase the capabilities of alternators in three applications. We first\nused alternators to model the Lorenz equations, often used to describe chaotic\nbehavior. We then applied alternators to Neuroscience, to map brain activity to\nphysical activity. Finally, we applied alternators to Climate Science, focusing\non sea-surface temperature forecasting. In all our experiments, we found\nalternators are stable to train, fast to sample from, yield high-quality\ngenerated samples and latent variables, and outperform strong baselines such as\nneural ODEs and diffusion models in the domains we studied.",
        "updated": "2024-05-20 07:47:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.11848v1"
    },
    {
        "title": "Application of time-series quantum generative model to financial data",
        "authors": "Shun OkumuraMasayuki OhzekiMasaya Abe",
        "links": "http://arxiv.org/abs/2405.11795v1",
        "entry_id": "http://arxiv.org/abs/2405.11795v1",
        "pdf_url": "http://arxiv.org/pdf/2405.11795v1",
        "summary": "Despite proposing a quantum generative model for time series that\nsuccessfully learns correlated series with multiple Brownian motions, the model\nhas not been adapted and evaluated for financial problems. In this study, a\ntime-series generative model was applied as a quantum generative model to\nactual financial data. Future data for two correlated time series were\ngenerated and compared with classical methods such as long short-term memory\nand vector autoregression. Furthermore, numerical experiments were performed to\ncomplete missing values. Based on the results, we evaluated the practical\napplications of the time-series quantum generation model. It was observed that\nfewer parameter values were required compared with the classical method. In\naddition, the quantum time-series generation model was feasible for both\nstationary and nonstationary data. These results suggest that several\nparameters can be applied to various types of time-series data.",
        "updated": "2024-05-20 05:29:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.11795v1"
    },
    {
        "title": "General bounds on the quality of Bayesian coresets",
        "authors": "Trevor Campbell",
        "links": "http://arxiv.org/abs/2405.11780v1",
        "entry_id": "http://arxiv.org/abs/2405.11780v1",
        "pdf_url": "http://arxiv.org/pdf/2405.11780v1",
        "summary": "Bayesian coresets speed up posterior inference in the large-scale data regime\nby approximating the full-data log-likelihood function with a surrogate\nlog-likelihood based on a small, weighted subset of the data. But while\nBayesian coresets and methods for construction are applicable in a wide range\nof models, existing theoretical analysis of the posterior inferential error\nincurred by coreset approximations only apply in restrictive settings -- i.e.,\nexponential family models, or models with strong log-concavity and smoothness\nassumptions. This work presents general upper and lower bounds on the\nKullback-Leibler (KL) divergence of coreset approximations that reflect the\nfull range of applicability of Bayesian coresets. The lower bounds require only\nmild model assumptions typical of Bayesian asymptotic analyses, while the upper\nbounds require the log-likelihood functions to satisfy a generalized\nsubexponentiality criterion that is weaker than conditions used in earlier\nwork. The lower bounds are applied to obtain fundamental limitations on the\nquality of coreset approximations, and to provide a theoretical explanation for\nthe previously-observed poor empirical performance of importance sampling-based\nconstruction methods. The upper bounds are used to analyze the performance of\nrecent subsample-optimize methods. The flexibility of the theory is\ndemonstrated in validation experiments involving multimodal, unidentifiable,\nheavy-tailed Bayesian posterior distributions.",
        "updated": "2024-05-20 04:46:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.11780v1"
    },
    {
        "title": "Asymptotic theory of in-context learning by linear attention",
        "authors": "Yue M. LuMary I. LeteyJacob A. Zavatone-VethAnindita MaitiCengiz Pehlevan",
        "links": "http://arxiv.org/abs/2405.11751v1",
        "entry_id": "http://arxiv.org/abs/2405.11751v1",
        "pdf_url": "http://arxiv.org/pdf/2405.11751v1",
        "summary": "Transformers have a remarkable ability to learn and execute tasks based on\nexamples provided within the input itself, without explicit prior training. It\nhas been argued that this capability, known as in-context learning (ICL), is a\ncornerstone of Transformers' success, yet questions about the necessary sample\ncomplexity, pretraining task diversity, and context length for successful ICL\nremain unresolved. Here, we provide a precise answer to these questions in an\nexactly solvable model of ICL of a linear regression task by linear attention.\nWe derive sharp asymptotics for the learning curve in a phenomenologically-rich\nscaling regime where the token dimension is taken to infinity; the context\nlength and pretraining task diversity scale proportionally with the token\ndimension; and the number of pretraining examples scales quadratically. We\ndemonstrate a double-descent learning curve with increasing pretraining\nexamples, and uncover a phase transition in the model's behavior between low\nand high task diversity regimes: In the low diversity regime, the model tends\ntoward memorization of training tasks, whereas in the high diversity regime, it\nachieves genuine in-context learning and generalization beyond the scope of\npretrained tasks. These theoretical insights are empirically validated through\nexperiments with both linear attention and full nonlinear Transformer\narchitectures.",
        "updated": "2024-05-20 03:24:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.11751v1"
    },
    {
        "title": "Inference with non-differentiable surrogate loss in a general high-dimensional classification framework",
        "authors": "Muxuan LiangYang NingMaureen A SmithYing-Qi Zhao",
        "links": "http://arxiv.org/abs/2405.11723v1",
        "entry_id": "http://arxiv.org/abs/2405.11723v1",
        "pdf_url": "http://arxiv.org/pdf/2405.11723v1",
        "summary": "Penalized empirical risk minimization with a surrogate loss function is often\nused to derive a high-dimensional linear decision rule in classification\nproblems. Although much of the literature focuses on the generalization error,\nthere is a lack of valid inference procedures to identify the driving factors\nof the estimated decision rule, especially when the surrogate loss is\nnon-differentiable. In this work, we propose a kernel-smoothed decorrelated\nscore to construct hypothesis testing and interval estimations for the linear\ndecision rule estimated using a piece-wise linear surrogate loss, which has a\ndiscontinuous gradient and non-regular Hessian. Specifically, we adopt kernel\napproximations to smooth the discontinuous gradient near discontinuity points\nand approximate the non-regular Hessian of the surrogate loss. In applications\nwhere additional nuisance parameters are involved, we propose a novel\ncross-fitted version to accommodate flexible nuisance estimates and kernel\napproximations. We establish the limiting distribution of the kernel-smoothed\ndecorrelated score and its cross-fitted version in a high-dimensional setup.\nSimulation and real data analysis are conducted to demonstrate the validity and\nsuperiority of the proposed method.",
        "updated": "2024-05-20 01:50:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.11723v1"
    }
]