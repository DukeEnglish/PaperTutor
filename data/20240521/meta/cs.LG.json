[
    {
        "title": "Images that Sound: Composing Images and Sounds on a Single Canvas",
        "authors": "Ziyang ChenDaniel GengAndrew Owens",
        "links": "http://arxiv.org/abs/2405.12221v1",
        "entry_id": "http://arxiv.org/abs/2405.12221v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12221v1",
        "summary": "Spectrograms are 2D representations of sound that look very different from\nthe images found in our visual world. And natural images, when played as\nspectrograms, make unnatural sounds. In this paper, we show that it is possible\nto synthesize spectrograms that simultaneously look like natural images and\nsound like natural audio. We call these spectrograms images that sound. Our\napproach is simple and zero-shot, and it leverages pre-trained text-to-image\nand text-to-spectrogram diffusion models that operate in a shared latent space.\nDuring the reverse process, we denoise noisy latents with both the audio and\nimage diffusion models in parallel, resulting in a sample that is likely under\nboth models. Through quantitative evaluations and perceptual studies, we find\nthat our method successfully generates spectrograms that align with a desired\naudio prompt while also taking the visual appearance of a desired image prompt.\nPlease see our project page for video results:\nhttps://ificl.github.io/images-that-sound/",
        "updated": "2024-05-20 17:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12221v1"
    },
    {
        "title": "Adapting Large Multimodal Models to Distribution Shifts: The Role of In-Context Learning",
        "authors": "Guanglin ZhouZhongyi HanShiming ChenBiwei HuangLiming ZhuSalman KhanXin GaoLina Yao",
        "links": "http://arxiv.org/abs/2405.12217v1",
        "entry_id": "http://arxiv.org/abs/2405.12217v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12217v1",
        "summary": "Recent studies indicate that large multimodal models (LMMs) are highly robust\nagainst natural distribution shifts, often surpassing previous baselines.\nDespite this, domain-specific adaptation is still necessary, particularly in\nspecialized areas like healthcare. Due to the impracticality of fine-tuning\nLMMs given their vast parameter space, this work investigates in-context\nlearning (ICL) as an effective alternative for enhancing LMMs' adaptability. We\nfind that the success of ICL heavily relies on the choice of demonstration,\nmirroring challenges seen in large language models but introducing unique\ncomplexities for LMMs facing distribution shifts. Our study addresses this by\nevaluating an unsupervised ICL method, TopKNearestPR, which selects in-context\nexamples through a nearest example search based on feature similarity. We\nuncover that its effectiveness is limited by the deficiencies of pre-trained\nvision encoders under distribution shift scenarios. To address these\nchallenges, we propose InvariantSelectPR, a novel method leveraging\nClass-conditioned Contrastive Invariance (CCI) for more robust demonstration\nselection. Specifically, CCI enhances pre-trained vision encoders by improving\ntheir discriminative capabilities across different classes and ensuring\ninvariance to domain-specific variations. This enhancement allows the encoders\nto effectively identify and retrieve the most informative examples, which are\nthen used to guide LMMs in adapting to new query samples under varying\ndistributions. Our experiments show that InvariantSelectPR substantially\nimproves the adaptability of LMMs, achieving significant performance gains on\nbenchmark datasets, with a 34.2%$\\uparrow$ accuracy increase in 7-shot on\nCamelyon17 and 16.9%$\\uparrow$ increase in 7-shot on HAM10000 compared to the\nbaseline zero-shot performance.",
        "updated": "2024-05-20 17:59:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12217v1"
    },
    {
        "title": "Octo: An Open-Source Generalist Robot Policy",
        "authors": "Octo Model TeamDibya GhoshHomer WalkeKarl PertschKevin BlackOier MeesSudeep DasariJoey HejnaTobias KreimanCharles XuJianlan LuoYou Liang TanPannag SanketiQuan VuongTed XiaoDorsa SadighChelsea FinnSergey Levine",
        "links": "http://arxiv.org/abs/2405.12213v1",
        "entry_id": "http://arxiv.org/abs/2405.12213v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12213v1",
        "summary": "Large policies pretrained on diverse robot datasets have the potential to\ntransform robotic learning: instead of training new policies from scratch, such\ngeneralist robot policies may be finetuned with only a little in-domain data,\nyet generalize broadly. However, to be widely applicable across a range of\nrobotic learning scenarios, environments, and tasks, such policies need to\nhandle diverse sensors and action spaces, accommodate a variety of commonly\nused robotic platforms, and finetune readily and efficiently to new domains. In\nthis work, we aim to lay the groundwork for developing open-source, widely\napplicable, generalist policies for robotic manipulation. As a first step, we\nintroduce Octo, a large transformer-based policy trained on 800k trajectories\nfrom the Open X-Embodiment dataset, the largest robot manipulation dataset to\ndate. It can be instructed via language commands or goal images and can be\neffectively finetuned to robot setups with new sensory inputs and action spaces\nwithin a few hours on standard consumer GPUs. In experiments across 9 robotic\nplatforms, we demonstrate that Octo serves as a versatile policy initialization\nthat can be effectively finetuned to new observation and action spaces. We also\nperform detailed ablations of design decisions for the Octo model, from\narchitecture to training data, to guide future research on building generalist\nrobot models.",
        "updated": "2024-05-20 17:57:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12213v1"
    },
    {
        "title": "Optimistic Query Routing in Clustering-based Approximate Maximum Inner Product Search",
        "authors": "Sebastian BruchAditya KrishnanFranco Maria Nardini",
        "links": "http://arxiv.org/abs/2405.12207v1",
        "entry_id": "http://arxiv.org/abs/2405.12207v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12207v1",
        "summary": "Clustering-based nearest neighbor search is a simple yet effective method in\nwhich data points are partitioned into geometric shards to form an index, and\nonly a few shards are searched during query processing to find an approximate\nset of top-$k$ vectors. Even though the search efficacy is heavily influenced\nby the algorithm that identifies the set of shards to probe, it has received\nlittle attention in the literature. This work attempts to bridge that gap by\nstudying the problem of routing in clustering-based maximum inner product\nsearch (MIPS). We begin by unpacking existing routing protocols and notice the\nsurprising contribution of optimism. We then take a page from the sequential\ndecision making literature and formalize that insight following the principle\nof ``optimism in the face of uncertainty.'' In particular, we present a new\nframework that incorporates the moments of the distribution of inner products\nwithin each shard to optimistically estimate the maximum inner product. We then\npresent a simple instance of our algorithm that uses only the first two moments\nto reach the same accuracy as state-of-the-art routers such as \\scann by\nprobing up to $50%$ fewer points on a suite of benchmark MIPS datasets. Our\nalgorithm is also space-efficient: we design a sketch of the second moment\nwhose size is independent of the number of points and in practice requires\nstoring only $O(1)$ additional vectors per shard.",
        "updated": "2024-05-20 17:47:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12207v1"
    },
    {
        "title": "Modeling citation worthiness by using attention-based bidirectional long short-term memory networks and interpretable models",
        "authors": "Tong ZengDaniel E. Acuna",
        "links": "http://dx.doi.org/10.1007/s11192-020-03421-9",
        "entry_id": "http://arxiv.org/abs/2405.12206v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12206v1",
        "summary": "Scientist learn early on how to cite scientific sources to support their\nclaims. Sometimes, however, scientists have challenges determining where a\ncitation should be situated -- or, even worse, fail to cite a source\naltogether. Automatically detecting sentences that need a citation (i.e.,\ncitation worthiness) could solve both of these issues, leading to more robust\nand well-constructed scientific arguments. Previous researchers have applied\nmachine learning to this task but have used small datasets and models that do\nnot take advantage of recent algorithmic developments such as attention\nmechanisms in deep learning. We hypothesize that we can develop significantly\naccurate deep learning architectures that learn from large supervised datasets\nconstructed from open access publications. In this work, we propose a\nBidirectional Long Short-Term Memory (BiLSTM) network with attention mechanism\nand contextual information to detect sentences that need citations. We also\nproduce a new, large dataset (PMOA-CITE) based on PubMed Open Access Subset,\nwhich is orders of magnitude larger than previous datasets. Our experiments\nshow that our architecture achieves state of the art performance on the\nstandard ACL-ARC dataset ($F_{1}=0.507$) and exhibits high performance\n($F_{1}=0.856$) on the new PMOA-CITE. Moreover, we show that it can transfer\nlearning across these datasets. We further use interpretable models to\nilluminate how specific language is used to promote and inhibit citations. We\ndiscover that sections and surrounding sentences are crucial for our improved\npredictions. We further examined purported mispredictions of the model, and\nuncovered systematic human mistakes in citation behavior and source data. This\nopens the door for our model to check documents during pre-submission and\npre-archival procedures. We make this new dataset, the code, and a web-based\ntool available to the community.",
        "updated": "2024-05-20 17:45:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12206v1"
    }
]