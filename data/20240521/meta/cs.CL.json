[
    {
        "title": "MathBench: Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics Benchmark",
        "authors": "Hongwei LiuZilong ZhengYuxuan QiaoHaodong DuanZhiwei FeiFengzhe ZhouWenwei ZhangSongyang ZhangDahua LinKai Chen",
        "links": "http://arxiv.org/abs/2405.12209v1",
        "entry_id": "http://arxiv.org/abs/2405.12209v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12209v1",
        "summary": "Recent advancements in large language models (LLMs) have showcased\nsignificant improvements in mathematics. However, traditional math benchmarks\nlike GSM8k offer a unidimensional perspective, falling short in providing a\nholistic assessment of the LLMs' math capabilities. To address this gap, we\nintroduce MathBench, a new benchmark that rigorously assesses the mathematical\ncapabilities of large language models. MathBench spans a wide range of\nmathematical disciplines, offering a detailed evaluation of both theoretical\nunderstanding and practical problem-solving skills. The benchmark progresses\nthrough five distinct stages, from basic arithmetic to college mathematics, and\nis structured to evaluate models at various depths of knowledge. Each stage\nincludes theoretical questions and application problems, allowing us to measure\na model's mathematical proficiency and its ability to apply concepts in\npractical scenarios. MathBench aims to enhance the evaluation of LLMs'\nmathematical abilities, providing a nuanced view of their knowledge\nunderstanding levels and problem solving skills in a bilingual context. The\nproject is released at https://github.com/open-compass/MathBench .",
        "updated": "2024-05-20 17:52:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12209v1"
    },
    {
        "title": "Modeling citation worthiness by using attention-based bidirectional long short-term memory networks and interpretable models",
        "authors": "Tong ZengDaniel E. Acuna",
        "links": "http://dx.doi.org/10.1007/s11192-020-03421-9",
        "entry_id": "http://arxiv.org/abs/2405.12206v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12206v1",
        "summary": "Scientist learn early on how to cite scientific sources to support their\nclaims. Sometimes, however, scientists have challenges determining where a\ncitation should be situated -- or, even worse, fail to cite a source\naltogether. Automatically detecting sentences that need a citation (i.e.,\ncitation worthiness) could solve both of these issues, leading to more robust\nand well-constructed scientific arguments. Previous researchers have applied\nmachine learning to this task but have used small datasets and models that do\nnot take advantage of recent algorithmic developments such as attention\nmechanisms in deep learning. We hypothesize that we can develop significantly\naccurate deep learning architectures that learn from large supervised datasets\nconstructed from open access publications. In this work, we propose a\nBidirectional Long Short-Term Memory (BiLSTM) network with attention mechanism\nand contextual information to detect sentences that need citations. We also\nproduce a new, large dataset (PMOA-CITE) based on PubMed Open Access Subset,\nwhich is orders of magnitude larger than previous datasets. Our experiments\nshow that our architecture achieves state of the art performance on the\nstandard ACL-ARC dataset ($F_{1}=0.507$) and exhibits high performance\n($F_{1}=0.856$) on the new PMOA-CITE. Moreover, we show that it can transfer\nlearning across these datasets. We further use interpretable models to\nilluminate how specific language is used to promote and inhibit citations. We\ndiscover that sections and surrounding sentences are crucial for our improved\npredictions. We further examined purported mispredictions of the model, and\nuncovered systematic human mistakes in citation behavior and source data. This\nopens the door for our model to check documents during pre-submission and\npre-archival procedures. We make this new dataset, the code, and a web-based\ntool available to the community.",
        "updated": "2024-05-20 17:45:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12206v1"
    },
    {
        "title": "CT-Eval: Benchmarking Chinese Text-to-Table Performance in Large Language Models",
        "authors": "Haoxiang ShiJiaan WangJiarong XuCen WangTetsuya Sakai",
        "links": "http://arxiv.org/abs/2405.12174v1",
        "entry_id": "http://arxiv.org/abs/2405.12174v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12174v1",
        "summary": "Text-to-Table aims to generate structured tables to convey the key\ninformation from unstructured documents. Existing text-to-table datasets are\ntypically oriented English, limiting the research in non-English languages.\nMeanwhile, the emergence of large language models (LLMs) has shown great\nsuccess as general task solvers in multi-lingual settings (e.g., ChatGPT),\ntheoretically enabling text-to-table in other languages. In this paper, we\npropose a Chinese text-to-table dataset, CT-Eval, to benchmark LLMs on this\ntask. Our preliminary analysis of English text-to-table datasets highlights two\nkey factors for dataset construction: data diversity and data hallucination.\nInspired by this, the CT-Eval dataset selects a popular Chinese\nmultidisciplinary online encyclopedia as the source and covers 28 domains to\nensure data diversity. To minimize data hallucination, we first train an LLM to\njudge and filter out the task samples with hallucination, then employ human\nannotators to clean the hallucinations in the validation and testing sets.\nAfter this process, CT-Eval contains 88.6K task samples. Using CT-Eval, we\nevaluate the performance of open-source and closed-source LLMs. Our results\nreveal that zero-shot LLMs (including GPT-4) still have a significant\nperformance gap compared with human judgment. Furthermore, after fine-tuning,\nopen-source LLMs can significantly improve their text-to-table ability,\noutperforming GPT-4 by a large margin. In short, CT-Eval not only helps\nresearchers evaluate and quickly understand the Chinese text-to-table ability\nof existing LLMs but also serves as a valuable resource to significantly\nimprove the text-to-table performance of LLMs.",
        "updated": "2024-05-20 16:58:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12174v1"
    },
    {
        "title": "Fennec: Fine-grained Language Model Evaluation and Correction Extended through Branching and Bridging",
        "authors": "Xiaobo LiangHaoke ZhangHelan huJuntao LiJun XuMin Zhang",
        "links": "http://arxiv.org/abs/2405.12163v1",
        "entry_id": "http://arxiv.org/abs/2405.12163v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12163v1",
        "summary": "The rapid advancement of large language models has given rise to a plethora\nof applications across a myriad of real-world tasks, mainly centered on\naligning with human intent. However, the complexities inherent in human intent\nnecessitate a dependence on labor-intensive and time-consuming human\nevaluation. To alleviate this constraint, we delve into the paradigm of\nemploying open-source large language models as evaluators, aligning with the\nprevailing trend of utilizing GPT-4. Particularly, we present a step-by-step\nevaluation framework: \\textbf{Fennec}, capable of \\textbf{F}ine-grained\n\\textbf{E}valuatio\\textbf{N} and correctio\\textbf{N} \\textbf{E}xtended through\nbran\\textbf{C}hing and bridging. Specifically, the branching operation dissects\nthe evaluation task into various dimensions and granularities, thereby\nalleviating the challenges associated with evaluation. Concurrently, the\nbridging operation amalgamates diverse training datasets, augmenting the\nvariety of evaluation tasks. In experimental trials, our 7B model consistently\noutperforms open-source larger-scale evaluation models across various widely\nadopted benchmarks in terms of both \\textit{Agreement} and\n\\textit{Consistency}, closely approaching the capabilities of GPT-4. We employ\nthe fine-grained correction capabilities induced by the evaluation model to\nrefine multiple model responses, and the results show that the refinement\nelevates the quality of responses, leading to an improvement of 1-2 points on\nthe MT-Bench. Our code is available at\nGithub\\footnote{\\url{https://github.com/dropreg/Fennec}}.",
        "updated": "2024-05-20 16:47:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12163v1"
    },
    {
        "title": "Eliciting Problem Specifications via Large Language Models",
        "authors": "Robert E. WrayJames R. KirkJohn E. Laird",
        "links": "http://arxiv.org/abs/2405.12147v1",
        "entry_id": "http://arxiv.org/abs/2405.12147v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12147v1",
        "summary": "Cognitive systems generally require a human to translate a problem definition\ninto some specification that the cognitive system can use to attempt to solve\nthe problem or perform the task. In this paper, we illustrate that large\nlanguage models (LLMs) can be utilized to map a problem class, defined in\nnatural language, into a semi-formal specification that can then be utilized by\nan existing reasoning and learning system to solve instances from the problem\nclass. We present the design of LLM-enabled cognitive task analyst agent(s).\nImplemented with LLM agents, this system produces a definition of problem\nspaces for tasks specified in natural language. LLM prompts are derived from\nthe definition of problem spaces in the AI literature and general\nproblem-solving strategies (Polya's How to Solve It). A cognitive system can\nthen use the problem-space specification, applying domain-general problem\nsolving strategies (\"weak methods\" such as search), to solve multiple instances\nof problems from the problem class. This result, while preliminary, suggests\nthe potential for speeding cognitive systems research via disintermediation of\nproblem formulation while also retaining core capabilities of cognitive\nsystems, such as robust inference and online learning.",
        "updated": "2024-05-20 16:19:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12147v1"
    }
]