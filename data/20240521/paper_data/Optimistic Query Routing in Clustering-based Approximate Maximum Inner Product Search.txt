Optimistic Query Routing in Clustering-based
Approximate Maximum Inner Product Search
SebastianBruch AdityaKrishnan FrancoMariaNardini
Pinecone Pinecone ISTI-CNR
NewYork,NY,USA NewYork,NY,USA Pisa,Italy
sbruch@acm.org aditya@pinecone.io francomaria.nardini@isti.cnr.it
Abstract
Clustering-basednearestneighborsearchisasimpleyeteffectivemethodinwhich
datapointsarepartitionedintogeometricshardstoformanindex,andonlyafew
shardsaresearchedduringqueryprocessingtofindanapproximatesetoftop-k
vectors. Eventhoughthesearchefficacyisheavilyinfluencedbythealgorithm
that identifies the set of shards to probe, it has received little attention in the
literature. Thisworkattemptstobridgethatgapbystudyingtheproblemofrouting
inclustering-basedmaximuminnerproductsearch(MIPS).Webeginbyunpacking
existingroutingprotocolsandnoticethesurprisingcontributionofoptimism. We
thentakeapagefromthesequentialdecisionmakingliteratureandformalizethat
insightfollowingtheprincipleof“optimisminthefaceofuncertainty.”Inparticular,
wepresentanewframeworkthatincorporatesthemomentsofthedistributionof
innerproductswithineachshardtooptimisticallyestimatethemaximuminner
product. Wethenpresentasimpleinstanceofouralgorithmthatusesonlythefirst
twomomentstoreachthesameaccuracyasstate-of-the-artrouterssuchasSCANN
byprobingupto50%fewerpointsonasuiteofbenchmarkMIPSdatasets. Our
algorithmisalsospace-efficient: wedesignasketchofthesecondmomentwhose
sizeisindependentofthenumberofpointsandinpracticerequiresstoringonly
O(1)additionalvectorspershard.
1 Introduction
Afundamentaloperationinmoderninformationretrievalanddatabasesystemsiswhatisknownas
NearestNeighborsearchortop-kvectorretrieval[Bruch,2024]. Itisdefinedasfollows: Givena
collection ofmdatapointsinRd,wewishtofindthekclosestpointstoaqueryq Rd,where
X ∈
closeness is determined by some notion of vector similarity or distance. In this work, we focus
exclusivelyoninnerproductasameasureofsimilarity,leadingtothefollowingformaldefinition
knownasMaximumInnerProductSearch(MIPS):
(k)
=argmax q,u . (1)
S ⟨ ⟩
u∈X
Unsurprisingly,itisoftentoodifficulttosolvethisproblemexactlywithinareasonabletimebudget,
especially as m or d increases. As such, the problem is often relaxed to its approximate variant,
wherewetolerateerrorintheretrievedsettoallowfasterqueryprocessing. Thisapproximateversion
oftheproblemisaptlynamedApproximateNearestNeighbor(ANN)search,whoseeffectivenessis
characterizedbythefractionoftruenearestneighborsrecalledintheretrievedset: ˜ /k,where
˜isthesetreturnedbyanANNalgorithm. |S∩S|
S
Preprint.Underreview.
4202
yaM
02
]GL.sc[
1v70221.5042:viXra1.1 Clustering-basedANNsearch
AlgorithmsthatsolvetheANNsearchproblemefficientlyandeffectivelycomeinvariousflavors,from
trees[Bentley,1975,DasguptaandSinha,2015],LSH[IndykandMotwani,1998],tographs[Malkov
andYashunin,2020,JayaramSubramanyaetal.,2019]andmore. Forathoroughtreatmentofthis
subject,wereferthereaderto[Bruch,2024].
The method relevant to this work is the clustering-based approach, also known as Inverted File
(IVF)[Jégouetal.,2011],whichhasproveneffectiveinpractice[Auvolatetal.,2015,Babenkoand
Lempitsky,2012,Chierichettietal.,2007,Bruchetal.,2024,Douzeetal.,2024]. Inthisparadigm,
datapointsarepartitionedintoC shardsusingaclusteringfunctionζ :Rd [C]on . Atypical
→ X
choiceforζ istheKMeansalgorithmwithC = (√m). Thisformstheindexdatastructure.
O
Accompanyingtheindexisaroutingfunctionτ :Rd [C]ℓ. Ittakesaqueryqandreturnsℓshards
→
thataremorelikelytocontainitsnearestneighbors. Acommonly-usedrouterisdefinedasfollows:
(ℓ)
τ(q)=argmax q,µ , (2)
i
⟨ ⟩
i∈[C]
whereµ isthemeanofthei-thshard.
i
Processingaqueryq involvestwosubroutines. Thefirststage,whichwecall“routing,”obtainsa
listofℓshardsusingτ(q),andthesubsequentstep,whichwerefertoas“search,”performsANN
searchovertheunionoftheselectedshards. Whileagreatdealofresearchhasfocusedonthelatter
step[Jégouetal.,2011,Geetal.,2014,Wuetal.,2017,Andreetal.,2021,KalantidisandAvrithis,
2014,Johnsonetal.,2021,NorouziandFleet,2013],theformerstephasreceivedrelativelylittle
attention. Inthiswork,weturnsquarelytothestudyofthefirststep: Routingqueriestoshards.
1.2 Theimportanceofrouting
Thehistoricalfocusonthesearchstepmakesagreatdealofsense. Afterall,eventhoughtherouting
stepnarrowsdownthesearchspace,oftendramaticallyso,selectedshardsmaynonethelesscontaina
largenumberofpoints. Itisthusimperativethatthesearchstagebeefficientandeffective.
Wearguethattheoft-overlookedroutingstepisimportantinitsownright. Thefirstandobvious
reasonisthat,themoreaccurately1 arouterchoosesasubsetofshards,thefewerdatapointsthe
searchstagemustexamine. Forexample,ifshardsarebalancedinsize,accesstoanoraclerouter
meansthatthesearchstageneedonlyexaminem/C pointstofindthetop-1point.
Thesecondandmoreimportantreasonpertainstoscale. Ascollectionsgrowinsizeanddimen-
sionality,itisofteninfeasibletokeeptheentireindexinmemory,inspiteofadvancedcompression
techniquessuchasProductQuantization[Jégouetal.,2011]. Muchoftheindexmustthereforerest
onsecondarystorage—inparticular,cheapbuthigh-latencystoragesuchasdiskorblobstorage—and
accessedonlywhennecessary. Thatlineofreasoninghasledtotheemergenceofdisk-basedgraph
indexes[JayaramSubramanyaetal.,2019,Singhetal.,2021,Jaiswaletal.,2022]andthelike.
Translatingthesamerationaletotheclustering-basedparadigmimpliesthatshardsrestoutsideof
themainmemory,andthatwhenarouteridentifiesasubsetofshards,thesearchstagemustfetch
thoseshardsfromstorageforfurtherprocessing. Amoreaccuraterouterthuslowersthevolumeof
datathatmustbetransferredbetweenstorageandmemory. Furthermore,becausequeryprocessing
isadditionallybottleneckedbystoragebandwidthandmemorycapacity,fetchingfewershardsper
queryisarequisiteforachievingahigherthroughput.
Interestingly,dependingonoperationalfactorssuchasqueryload,storageI/Obandwidth,datatransfer
rate,andmemoryutilization,itwouldbeacceptablefortheroutingstagetobemorecomputationally
expensiveaslongasitidentifiesshardsmoreaccurately. Whereasthein-memoryclustering-based
paradigmrequiresroutingtobehighlyefficient,thenewtrade-offspaceofferedbyitsstorage-based
realizationopensthedoortomorenuancedresearch.
1WequantifytheaccuracyofarouterastheANNrecallinasetupwherethesearchstageperformsalinear
scanovertheidentifiedshards.Assuch,theonlysourceofANNerroristheroutingprocedure,ratherthaninner
productcomputation.
20.975
NormalizedMean
Mean
0.950 DotProduct 102
102
0.925
0.900
101 101
0.875
0.850 NormalizedMean
Mean 100 100
0.5 1.0 1.5 2.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0
%pointsprobed − Score − Score
(a) (b)
Figure1: (a)Top-1recallversusthenumberofpointsprobedontheTEXT2IMAGEdatasetwhere
datapointshavevaryingnorms; (b)Distributionofinnerproductsbetweenashardandasample
queryonGLOVE. OverlaidarethescorescomputedbyMEANandNORMALIZEDMEANrouters.
1.3 Existingroutersandthesurprisingroleofoptimism
Wehavearguedthatroutingaccuracyisincreasinglyrelevant. Surprisingly,withtheexceptionof
onerecentworkthatexploressupervisedlearning-to-rankforrouting[Vecchiatoetal.,2024],the
fewexistingunsupervisedrouterstakethenaïveformofEquation(2). Ineffect,theydeterminethe
relativepotential“reward”ofeveryshardwithapointestimate.
TakeEquation(2)asthemostprominentexample. WhatwerefertoastheMEANroutersummarizes
eachshardwithitsmeanpoint(i.e.,µ forshardi). Thisisnotanunreasonablechoiceasthemeanis
i
theminimizerofthevariance,andis,infact,naturalifKMeansistheclusteringalgorithmζ.
Anothercommonrouter,whichwecallNORMALIZEDMEAN,belongstothesamefamily,butwhere
shardrepresentativesaretheL -normalizedmeans,ratherthantheunnormalizedmeanvectors:
2
τ(q)=argm(ℓ) ax q, µ i . (3)
⟨ µ ⟩
i∈[C] i 2
∥ ∥
ThisformulationisinheritedfromthefamiliarSphericalKMeans[DhillonandModha,2001],which
is identical to the standard KMeans iterative algorithm but, at the end of every iteration, cluster
centroidsareprojectedontotheunitsphere. Becausewecanassumethat q =1withoutlossof
2
∥ ∥
generality,itiseasytoseethatEquation(3)routesbytheanglebetweenqandthemeanvectors.
Intuitively,NORMALIZEDMEANseemsappropriateforANNsearchoverasphere: Whennormsdo
notaffecttheoutcomeofANNsearch,thenallthatmattersinEquation(1)istheanglebetweenq
anddatapoints,renderingitreasonabletorankshardsbytheanglebetweentheirmeanandq.
Intriguingly,inmanycircumstancesthatdeviatefromthatsituation,NORMALIZEDMEANtendsto
performmoreaccuratelythantheMEANrouter,asevidencedinFigure1(a). Becauseunpackingthis
phenomenonmotivatesourproposal,wetakeabriefmomenttotakeacloserlook.
Consider a query point q and a single shard with mean µ. We visualize in Figure 1(b) the
P
distributionofinnerproductsbetweenqandeverypointin . Overlaidwiththatdistributionisthe
P
innerproductbetweenqandthemeanofpointsin ,aswellastheirnormalizedmean. Weobserve
P
that q,µ/ µ landsintherighttailofthedistribution.
2
⟨ ∥ ∥ ⟩
Thatisnotsurprising,atleastforthecasewhere u =1forallu . Clearly µ 1,sothat
2 2
∥ ∥ ∈P ∥ ∥ ≤
q,µ/ µ
2
= q,µ / µ
2
q,µ ;theNORMALIZEDMEANrouteramplifiestheMEANrouter
⟨ ∥ ∥ ⟩ ⟨ ⟩ ∥ ∥ ≥ ⟨ ⟩
byafactorof1/ µ . Whatisinteresting,however,isthatthemagnitudeofthis“boost”correlates
2
∥ ∥
withthevarianceofthedatapointswithin : Themoreconcentrated isaroundthedirectionof
P P
itsmean,thecloserthemeanistothesurfaceofthesphere,sothatNORMALIZEDMEANappliesa
smalleramplificationto q,µ . Theoppositeistruewhenpointsarespreadout. Asaresult,shards
⟨ ⟩
withahighervariancereceivealargerliftbytheNORMALIZEDMEANrouter!
3
ycaruccA1-poT stnioPforebmuN stnioPforebmuNThishasitsdownsides. First, itishardtoexplainthebehavioronpointsetswithvaryingnorms.
Second,asweobserveinFigure1(b),NORMALIZEDMEANaggressivelyoverestimates. Nonetheless,
theinsightthatarouter’sscoreforashardcanbeinfluencedbytheshard’svarianceisworthexploring.
Wecansummarizeourobservationasfollows. TheestimategivenbytheNORMALIZEDMEANrouter
paintsanoptimisticpictureofwhatthemaximuminnerproductbetweenqandpointsin couldbe.
P
ContrastthatwiththeMEANrouter,whichisnaturallyaconservativeestimate.
Thisworkinvestigatestheramificationsofthatinsight. Inparticular,theresearchquestionwewishto
studyiswhetheramoreprincipledapproachtodesigningoptimisticrouterscanleadtomoreaccurate
routingdecisionsonvectorsetswithvariablenorms. Thenatureofthisquestionisnotunlikethose
askedintheonlinelearningliterature[LattimoreandSzepesvári,2020],soitisnotsurprisingthat
ouranswertothisquestiondrawsfromthe“principleofoptimisminthefaceofuncertainty.”
1.4 Contributionsandoutline
WeapplytheOptimismPrincipletoroutinginclustering-basedMIPS.Ourformulation,presentedin
Section2,restsontheconcentrationoftheinnerproductdistributionbetweenaqueryqandasetof
pointsinthesameshard. Inparticular,weestimateascore,θ ,forthei-thshardsuchthat,withsome
i
confidence,themaximuminnerproductofqwithpointsinthatshardisatmostθ . Shardsarethen
i
rankedbytheirscoreforrouting. Noticethat,whenθ
i
=µ i,werecovertheMEANrouter,andwhen
θ >µ routingisoptimisticwiththeconfidenceparameterdeterminingthedegreeofoptimism.
i i
Buildingonourproposedformalismofanoptimisticrouter,weoutlineageneralframeworkthat
can incorporate as much information as is available about the data distribution to estimate the
aforementionedscores. Wethenpresentaconcrete,assumption-freeinstanceofouralgorithmthat
usesthefirstandsecondmomentsoftheempiricalinnerproductdistributiononly. Furthermore,we
maketheresultingalgorithmspace-efficientbydesigningasketch[Woodruff,2014]ofthesecond
moment. Theend-resultisapracticalalgorithmthatisstraightforwardtoimplement.
We put our proposal to the test in Section 3 on a variety of ANN benchmark datasets. As our
experimentsshow,ouroptimisticrouterachievesthesameANNrecallasstate-of-the-artroutersbut
withuptoa50%reductioninthetotalnumberofdatapointsevaluatedperquery. Weconcludethis
workinSection4.
2 RoutingbytheOptimismPrinciple: ourproposal
Asweobserved,NORMALIZEDMEANisanoptimisticestimator,thoughitsbehaviorisunpredictable.
Ourgoalistodesignanoptimisticestimatorthatisstatisticallyprincipled,thuswell-behaved.
Beforewebegin,letusbrieflycommentonourterminologyandnotation. Throughoutthissection,
wefixaunitqueryvectorq;alldiscussionsareinthecontextofq. Wedenotethei-thshardby ,
i
P
andwriteS forthesetofinnerproductscoresbetweenqandpointsin : S = q,u :u .
i i i i
P {⟨ ⟩ ∈P }
2.1 Formalizingthenotionofoptimism
Wewishtofindthesmallestthresholdθ q,µ forthei-thshardsuchthattheprobabilitythata
i i
≥⟨ ⟩
samplefromS fallstotheleftofθ isatleast(1+δ)/2,forsomearbitraryδ [0,1]. Formally,we
i i
∈
aimtocomputeasolutionθ tothethefollowingoptimizationproblem:
i
Problem1(OptimisticestimatorofthemaximuminnerproductinS ).
i
1+δ
inf θ :θ q,µ suchthat Pr [s θ ] .
{ i i ≥⟨ i ⟩} s∼Si ≤ i ≥ 2
Onecaninterprettheoptimalθ asaprobabilisticupper-boundonthemaximumattainablevaluein
i
S ;thatis,withsomeconfidence,wecanassertthatnovalueinS isgreaterthanθ .
i i i
Equippedwithθ ’s,werouteqbysortingallshardsbytheirestimatedthresholdsindescendingorder
i
andsubsequentlyselectingthetopshardsintheresultingrankedlist. Ourrouter,dubbedOPTIMIST,
isdefinedasfollow:
(ℓ)
τ(q)=argmax θ . (4)
i
i∈[C]
42.2 Understandingtheroutingbehavior
SupposeforamomentthatwehavethesolutiontoProblem1,andletusexpandontheexpected
behavioroftheOPTIMISTrouter. Itiseasytoseethat,whenδ 0,theoptimalsolutionapproaches
→
θ = q,µ . Assuch,ifwewishtoobtainthemostconservativeestimateofthemaximuminner
i i
⟨ ⟩
productbetweenqandpointsin i,theroutingprocedurecollapsestotheMEANrouter.
P
Asδ 1,θ becomeslarger,renderingEquation(4)anenthusiasticallyoptimisticrouter. Atthe
i
→
extreme,theoptimalsolutionisthemaximuminnerproductitself.
Clearlythen,δcontrolstheamountofoptimismonebestowsontotherouter. Itisinterestingtonote
that, whenthedatadistributionisfullyknown, thenδ = 1isanappropriatechoice: Ifweknow
theexactdistribution,wecanexpecttobefullyconfidentaboutthemaximuminnerproduct. On
theotherhand,whenverylittleaboutthedistributionisknown(e.g.,whenallweknowisthefirst
momentofthedistribution),thenδ =0isasensiblechoice. Ineffect,thevalueofδisastatementon
ourknowledgeoftheunderlyingdatadistribution.
WhatislefttoaddressisthesolutiontoProblem1,whichisthetopicoftheremainderofthissection.
Wedeferadescriptionofageneralapproachthatusesasmuchoraslittleinformationasavailable
aboutthedatadistributiontoAppendixBduetospaceconstraints. Inthenextsection,wepresenta
morepracticalapproachthatisthefoundationoftherestofthiswork.
2.3 Practicalsolutionviaconcentrationinequalitiesandsketching
NotingthatProblem1iscapturedbytheconceptofconcentrationofmeasure,weresorttoresults
from that literature to find acceptable estimates of θ ’s. In particular, we obtain a solution via a
i
straightforwardapplicationoftheone-sidedChebyshev’sinequality,resultinginthefollowinglemma.
Lemma1. Denotebyµ andΣ themeanandcovarianceofthedistributionof . Anupper-bound
i i i
P
onthesolutiontoProblem1forδ (0,1)is:
∈
(cid:114)
1+δ
θ = q,µ + q⊤Σ q. (5)
i ⟨ i ⟩ 1 δ i
−
Proof. The result follows immediately by applying the one-sided Chebyshev’s inequality to the
distribution,S ,ofinnerproductsbetweenqandpointsin :
i i
P
q⊤Σ q 1+δ 1+δ
Pr [s q,µ ϵ] 1 i = = ϵ2 = q⊤Σ q.
s∼Si −⟨ i ⟩≤ ≥ − q⊤Σ iq+ϵ2 2 ⇒ 1 δ i
−
RearrangingthetermstomatchtheexpressionofProblem1givesθ = q,µ +ϵ,asdesired.
i i
⟨ ⟩
We emphasize that the solution obtained by Lemma 1 is not necessarily optimal for Problem 1.
Instead,itgivesthebestupper-boundontheoptimalvalueofθ thatcanbeobtainedgivenlimited
i
informationaboutthedatadistribution. Asweseelater,however, eventhissub-optimalsolution
proveseffectiveinpractice.
Approximatingthecovariancematrix. WhileEquation(5)givesusanalgorithmtoapproximate
the threshold θ , storing Σ can be prohibitive in practice. That is because the size of the matrix
i i
grows as d2 for each partition. Contrast that with the cost for other routers, such as MEAN and
NORMALIZEDMEAN,whichonlystoreasingled-dimensionalvectorperpartition. Toremedythis
inefficiency,wereducethecostofrepresentingΣ bystoringasmallsketchthatapproximatesit.
i
Sincetheprocedurewedescribeisindependentlyappliedtoeachpartition,wedropthesubscriptfor
Σ anddescribetheprocedureforasinglepartition. WeseekamatrixΣ˜ Rd×dthatapproximates
i
∈
Σ. Wedefinetheapproximationerrorasfollows:
err(Σ,Σ˜)= sup v⊤Σv v⊤Σ˜v .
| − |
∥v∥2=1
Astandardmechanismtoapproximatelargematricesinordertominimizethiserroristocompute
alow-rankapproximationoftheoriginalmatrix. Tothatend,letΣ = VΛV⊤ betheeigendecom-
positionofthepositivesemi-definitematrixΣ; wherethecolumnsofV containtheorthonormal
5eigenvectorsofΣandΛ, ad ddiagonalmatrix, containsthenon-negativeeigenvaluesofΣin
×
non-increasingorder. TheEckhart-Young-MirskyTheoremshowsthat
err (Σ)= inf err(Σ,Σ˜), (6)
t
Σ˜∈Rd×d s.t.rank(Σ˜)=t
isminimizedwhenΣ˜ =[V√Λ] [V√Λ]⊤,where[] correspondstotheoperatorthatselectsthefirst
t t ·t
tcolumnsofthematrix.
While (6) is a well-known fact, in practice, it is often the case that the diagonal of the matrix Σ
containsimportantinformationandpreservingitfullyleadstobetterapproximationsofthematrix.
Assuch,wedecomposeΣasasumoftwomatrices: Dadiagonalmatrixcontainingthediagonal
entriesofΣ,andtheresidualR=Σ D. WepreservethediagonalDfullyandapproximateRby
−
computingalow-rankapproximationofasymmetrizationofR. Specifically,wedecompose
Σ=D+R=D21(I+D− 21RD−1 2)D21,
andcomputealow-rankapproximationtoM =D−1 2RD− 21. NoticethatsinceM isasymmetric
real-valuedmatrix,ithasaneigendecompositionoftheformM =QΛQ⊤whereQisad dmatrix
×
withorthonormalcolumns(correspondingtotheeigenvectorsofM)andΛisad ddiagonalmatrix
×
containingthe(possiblynegative)eigenvaluesofM alongthediagonal. Givensometargetsketch
sizet d,wesketchΣas:
≤
Σ˜ =D+D1 2[Q] t[Λ] t[Q]⊤
t
D1 2. (7)
WhilethisisnotastandardmechanismtosketchPSDmatricesandcanintheworstcaseperform
worsethanthe“optimal”low-ranksketch,weshowthatundercertainpracticalassumptions,this
sketchcanhaveerrorlowerthanthatofstandardlow-rankapproximation.
Lemma 2. Let Σ Rd×d be a PSD matrix with diagonal D and the property that
∈
min D /max D 1 ϵ for some ϵ > 0. For every 1 t d 1 such that the
i∈[d] ii j∈[d] jj
≥ − ≤ ≤ −
(t+1)-steigenvalueofD−1/2ΣD−1/2isgreaterthan2,thesketchΣ˜ definedin(7)hastheproperty
thaterr(Σ,Σ˜) err (Σ) 1 .
≤ t · 1−ϵ
WeprovideaprooffortheaboveresultinAppendixCandshowthatforpracticaldatasets,including
theonesweuseinthiswork,theassumptionsinthelemmaarevalid.
2.4 Thefinalalgorithm
UsingoursolutionfromLemma1forProblem1andoursketchdefinedin(7),wedescribeourfull
algorithminAlgorithm1forbuildingourrouterandscoringapartitionforagivenquery. Noticethat,
sinceweonlystoreµ,D,[Λ] and[Q] ,therouterrequiresjustt+2vectors2inRdperpartition. In
t t
ourexperiments,wechooset 10foralldatasetsexceptoneandshowthatmuchoftheperformance
≤
gainsfromusingthewholecovariancematrixcanbepreservedevenbychoosingasmallvalueoft
independentofd.
3 Experimentalevaluation
Inthissection,weputourargumentstothetestandexperimentallyevaluateOPTIMIST.
3.1 Setup
Datasets: We use the following suite of benchmark ANN datasets: TEXT2IMAGE (10 million
points,d = 200); MUSIC(1m,d = 100); DEEPIMAGE(10m,d = 96); GLOVE(1.2m,d = 200);
MSMARCO-MINILM(8.8m,d=384);and,NQ-ADA2(2.7m,d=1,536). Wedeferacomplete
descriptionofthesedatasetstoAppendixAduetospaceconstraints.
Clustering: Forourmainresults,wepartitiondatasetswithSphericalKMeans[DhillonandModha,
2001].Weincludeintheappendixresultsfromsimilarexperimentsbutwheretheclusteringalgorithm
2Since[Λ] canbe“absorbed”into[Q] withsomecaretakenforthesignsoftheeigenvalues.
t t
6Algorithm1IndexingandscoringasinglepartitionwithOPTIMIST
Input: Setofnpoints =x ,...,x definingthepartitionandtargetranktsuchthatt d.
1 n
X ≤
1: procedureBUILDROUTER( ,t)
2 3:
:
C Co om mp pu ut te ea Σnd ←sto n1r (cid:80)eµ
n i=←
1(X xn1 i(cid:80) −n
i µ= )1
(xx ii.
−µ)⊤. ▷Thisdoesnotneedtobestored
4: DecomposeΣintodiagonalDandresidualR ▷Σ=D+R
5: ComputeeigendecompositionofD−1/2RD−1/2 =QΛQ⊤
6: SortcolumnsofQ,Λinnon-increasingorder.
7: StoreD,Λ t [Λ] tandQ t [Q] t
← ←
Input: Queryq Rdandconfidenceparameterδ >0.
∈
8: procedureSCORE(q,δ)
9: Computeq˜ q D1/2 ▷Element-wiseproductwithdiagonalofD1/2
← (cid:113)◦
10: return ⟨q,µ ⟩+ 11 −+δ
δ
·( ∥q˜ ∥2 2+q˜⊤Q tΛ tQ⊤
t
q˜)
0.95
0.90
0.90
0.90 0.85
0.85
NormalizedMean
0.80
Mean
0.85 Scann(.5)
0.80 SubPartition(30)
0.75 Optimist(d,.8)
Optimist(30,.8)
0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0
%pointsprobed %pointsprobed %pointsprobed
Figure 2: Top-k recall on NQ-ADA2 versus the number of probed data points. The dataset is
partitionedwithSphericalKMeans. SCANNhasparameterT,SUBPARTITIONt(leadingtot+2
sub-partitionspershard),andOPTIMISTranktanddegreeofoptimismδ.
isstandardKMeansandGaussianMixtureModel(GMM).Weclustereachdatasetinto√mshards,
wheremisthenumberofdatapointsinthedataset.
Evaluation: Onceadatasetispartitioned,weevaluatearouterτ asfollows. Foreachtestquery,we
identifythesetofshardstoprobeusingτ. Wethenperformexactsearchovertheselectedshards,
obtainthetop-k points, andcomputerecallwithrespecttotheexacttop-k set. Becausetheonly
sourceoferroristheinaccuracyoftherouter,themeasuredrecallgaugestheeffectivenessofτ.
Wereportrecallasafunctionofthenumberofdatapointsprobed,ratherthanthenumberofshards
probed. Inthisway,acomparisonoftheefficacyofdifferentroutersisunaffectedbyanyimbalance
inshardsizes,sothataroutercannottriviallyoutperformanotherbysimplyprioritizinglargershards.
Routers: Weevaluatethefollowingroutersinourexperiments:
• MEANandNORMALIZEDMEAN: DefinedinEquations(2)and(3),respectively;
• SCANN (T): Similar to MEAN and NORMALIZEDMEAN, but where routing is determined by
innerproductbetweenaqueryandtheSCANNcentroids(c.f.,Theorem4.2in[Guoetal.,2020]).
SCANNhasasinglehyperparameterT,whichwesetto0.5aftertuning;
• SUBPARTITION(t): RecallthatOPTIMISTstorest+2vectorsperpartition,wheretistherankin
Equation(7). Weaskifsimplypartitioningeachshardindependentlyintot+2sub-partitions,and
recordingthesub-partitions’centroidsastherepresentativesofthatshardattainsthesamerouting
accuracyasOPTIMIST. Atquerytime,wetakethemaximuminnerproductofthequerywitha
shard’srepresentativesasitsscore,andsortshardsaccordingtothisscore;and,
• OPTIMIST(t,δ): TheOPTIMISTroutergiveninAlgorithm1. Theparametertdeterminestherank
ofthesketchofthecovariancematrix, andδ determinesthedegreeofoptimism. Wesetttoa
7
ycaruccA1-poT ycaruccA01-poT ycaruccA001-poT0.925
0.90 0.975
0.900
0.88 0.950
0.875
0.925
0.86 NormalizedMean 0.850 NormalizedMean NormalizedMean
Mean Mean Mean
Scann(.5) Scann(.5) 0.900 Scann(.5)
0.825
0.84 SubPartition(4) SubPartition(8) SubPartition(2)
Optimist(d,.8) Optimist(d,.8) 0.875 Optimist(d,.8)
Optimist(4,.8) 0.800 Optimist(8,.8) Optimist(2,.8)
0.82
4 6 8 0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0
%pointsprobed %pointsprobed %pointsprobed
(a)GLOVE (b)MSMARCO-MINILM (c)MUSIC
Figure3: Top-100recallversusthenumberofprobeddatapoints. SetupisthesameasFigure2.
Table1: TherelativesavingsinamountofdataprobedbetweenOPTIMIST(t d,.8)andNOR-
≪
MALIZEDMEANtoachieveafixedrecall. Asavingofx%meansthatOPTIMISTscansx%fewer
datapointsthanNORMALIZEDMEANforthesamerecall.
RECALL NQ-ADA2 GLOVE MSMARCO-MINILM DEEPIMAGE MUSIC TEXT2IMAGE
90% 18% 11% 22% 0 38% 23%
95% 20% 5.5% 7.7% 0 54% 22%
maximumof2%ofdbutstudyitseffectinAppendixE.t=dindicatesthatthefullcovariance
matrixisused(i.e.,withoutsketching).
Code: WehaveimplementedallbaselineandproposedroutersintheRustprogramminglanguage.
Weintendtoopen-sourceourcodealongwithexperimentalconfigurationtofacilitatereproducibility.
3.2 Mainresults
Figure2plotstop-k recallfork 1,10,100 versusthepercentageofdatapointsexaminedfor
∈ { }
NQ-ADA2—AppendixD.1givesfullresultsforalldatasets. Notethat,partitioningisbySpherical
KMeans,withsimilarplotsforstandardKMeansinAppendixD.2andGMMinAppendixD.3.
We summarize a few key observations. First, among baselines, NORMALIZEDMEAN generally
outperforms MEAN and SCANN, savefor MUSIC where MEAN reachesahigherrecall. Second,
withveryfewexceptions, OPTIMIST withthefullcovariance(i.e.,t = d)doesatleastaswellas
NORMALIZEDMEAN,andoftenoutperformsitsignificantly. Interestingly,thegapbetweenbaselines
andOPTIMISTwidensasretrievaldepth(k)increases;aphenomenonthatisnotsurprising.
Finally,whileOPTIMISTwitht dshowssomedegradationwithrespecttothet=dconfiguration—
≪
asanticipated—itstillachievesahigherrecallthanbaselinesforlargerk. Whenk issmaller,the
SUBPARTITIONrouterwitht+2vectorsbecomesastrongcompetitor.
Wehighlightthat,OPTIMISTshineswhendatapointsarenotonthesurfaceofasphere(orconcen-
tratedclosetoit). ThisphenomenonisillustratedinFigure3,showingtop-100recallforasubsetof
datasetspartitionedwithSphericalKMeans—refertoAppendixDforresultsonotherdatasets.
Inparticular,onMUSIC,at95%top-100recall,OPTIMISTwitht dneedstoprobe54%fewer
≪
datapointsthanNORMALIZEDMEAN;onaverageOPTIMISTprobes6,666datapointstoreach95%
top-100recallwhereasNORMALIZEDMEANexamines14,463points. Wepresenttherelativesavings
onalldatasetsinTable1. Notethat,onDEEPIMAGE,nomethodoutperformsNORMALIZEDMEAN.
We conclude this section by noting that, in Appendix E, we study the effect of t and δ on the
performance of OPTIMIST. We exclude the full discussion from the main prose due to space
constraints,butmentionthattheobservationsareunsurprising.
8
ycaruccA001-poT ycaruccA001-poT ycaruccA001-poT1.0 1.50
N Mo er am nalizedMean S Ou pb tP ima ir st tit (dio ,.n 8)(4) N Mo er am nalizedMean S Ou pb tP ima ir st tit (dio ,.n 8)(8) 1.5 N Mo er am nalizedMean S Ou pb tP ima ir st tit (dio ,.n 8)(2)
0.8 Scann(.5) Optimist(4,.8) 1.25 Scann(.5) Optimist(8,.8) Scann(.5) Optimist(2,.8)
1.00
0.6 1.0
0.75
0.4
0.50
0.5
0.2
0.25
0.0 10−1 100 101 102 0.00 10−1 100 101 102 0.0 10−1 100 101 102
%Shards %Shards %Shards
(a)GLOVE (b)MSMARCO-MINILM (c)MUSIC
Figure4: MeanpredictionerrorofEquation(8)vs. ℓ(percentoftotalnumberofshards,log-scale).
3.3 Maximuminnerproductprediction
WeclaimedthatOPTIMISTisstatisticallyprincipled. ThatentailsthatOPTIMISTshouldgiveamore
accurateestimateofwhatthemaximuminnerproductcanbeforanygivenquery-partitionpair. We
examinethatclaiminthissectionandquantifythepredictionerrorforallrouters.
Fixadataset,whoseC partitionsaredenotedby fori [C],alongwitharouterτ andqueryq.
i
P ∈
Wewriteτ
i
todenotethescorecomputedbyτ forpartition
i
andq (e.g., NORMALIZEDMEAN
P
computes q,µ i/ µ
i 2
andOPTIMISTgivesθ iperProblem1).
⟨ ∥ ∥ ⟩
Notethat,theτ ’sinduceanorderingamongpartitions. Wedenotethispermutationbyπ,sothat
i
τ τ . Wequantifythepredictionerrorforℓ [C]as:
πi
≥
πi+1
∈
(cid:12) (cid:12)
(τ,q)=
1(cid:88)ℓ (cid:12)
(cid:12)
τ
πi
1(cid:12)
(cid:12). (8)
Eℓ ℓ i=1(cid:12) (cid:12)max u∈Pπi⟨q,u
⟩
− (cid:12) (cid:12)
Thiserroris0whenscoresproducedbytherouterperfectlymatchthemaximuminnerproduct. The
roleofℓistoallowustofactorintherankofpartitionsinourcharacterizationofthepredictionerror.
Inotherwords,wecanmeasuretheerroronlyforthetopℓshardsaccordingtoτ. Inthisway,if
wedecidethatitisnotimperativeforaroutertoaccuratelypredictthemaximuminnerproductin
low-rankingshards,wecanreflectthatchoiceinourcalculation.
WemeasureEquation(8)onalldatasetspartitionedbySphericalKMeans,andallroutersconsidered
inthiswork. TheresultsareshowninFigure4,withtheremainingdatasetsinAppendixF,wherefor
eachchoiceofℓ,weplotE [ (τ,q)]usingthetestquerydistribution.
q ℓ
E
Fromthefigures,withtheexceptionofSCANNonTEXT2IMAGE,itisclearthatallrouterssuffera
greatererrorasℓ C(i.e.,%shardsapproaches100%). Interestingly,OPTIMIST(t=d, )degrades
→ ·
muchlessseverely. Remarkably,whent d,thesamepatternpersists;MUSICexcepted,where,
≪
witht=2,OPTIMISTbecomeshighlyinaccuratewhenℓ 8%ofthetotalnumberofshards.
≥
4 Concludingremarks
Motivated by our observation that NORMALIZEDMEAN is an over-estimator of maximum inner
product, we formalized the notion of optimism for query routing in clustering-based MIPS and
presentedaprincipledoptimisticalgorithmthatestimatesthemaximuminnerproductwithmuch
greateraccuracyandexhibitsamorereliablebehavior. Resultsonasuiteofbenchmarkdatasets
confirmourclaims.
Wehighlightthatouralgorithmismoresuitableforsettingswhereindividualshardsrestonsome
external,high-latencystorage,sothatspendingmorecomputeonroutingcanbetoleratedinexchange
forfeweramountofdatatransferredbetweenstorageandmainmemory.
Weleavetofutureworkanexplorationofamorecompactsketchingofthecovariancematrix;and,
anefficientrealizationofourgeneralsolutionoutlinedinAppendixB.
9
rorrEnoitciderP rorrEnoitciderP rorrEnoitciderPReferences
NirAilonandBernardChazelle. Thefastjohnson–lindenstrausstransformandapproximatenearest
neighbors. SIAMJournaloncomputing,39(1):302–322,2009.
NirAilonandEdoLiberty. Analmostoptimalunrestrictedfastjohnson-lindenstrausstransform.
ACMTransactionsonAlgorithms(TALG),9(3):1–12,2013.
Fabien Andre, Anne-Marie Kermarrec, and Nicolas Le Scouarnec. Quicker adc: Unlocking the
hiddenpotentialofproductquantizationwithsimd. IEEETransactionsonPatternAnalysisand
MachineIntelligence,43(5):1666–1677,52021.
AlexAuvolat,SarathChandar,PascalVincent,HugoLarochelle,andYoshuaBengio. Clusteringis
efficientforapproximatemaximuminnerproductsearch,2015.
Artem Babenko and Victor Lempitsky. The inverted multi-index. In 2012 IEEE Conference on
ComputerVisionandPatternRecognition,pages3069–3076,2012.
JonLouisBentley. Multidimensionalbinarysearchtreesusedforassociativesearching. Communica-
tionsoftheACM,18(9):509–517,91975.
Vladimir Braverman, Aditya Krishnan, and Christopher Musco. Sublinear time spectral density
estimation. InProceedingsofthe54thAnnualACMSIGACTSymposiumonTheoryofComputing,
pages1144–1157,2022.
SebastianBruch. FoundationsofVectorRetrieval. SpringerNatureSwitzerland,2024.
SebastianBruch,FrancoMariaNardini,AmirIngber,andEdoLiberty. Bridgingdenseandsparse
maximuminnerproductsearch. ACMTransactionsonInformationSystems,2024. (toappear).
FlavioChierichetti,AlessandroPanconesi,PrabhakarRaghavan,MauroSozio,AlessandroTiberi,
andEliUpfal. Findingnearneighborsthroughclusterpruning. InProceedingsofthe26thACM
SIGMOD-SIGACT-SIGARTSymposiumonPrinciplesofDatabaseSystems,pages103–112,2007.
Sanjoy Dasgupta and Kaushik Sinha. Randomized partition trees for nearest neighbor search.
Algorithmica,72(1):237–263,52015.
InderjitS.DhillonandDharmendraS.Modha. Conceptdecompositionsforlargesparsetextdata
usingclustering. MachineLearning,42(1):143–175,January2001.
MatthijsDouze,AlexandrGuzhva,ChengqiDeng,JeffJohnson,GergelySzilvasy,Pierre-Emmanuel
Mazaré,MariaLomeli,LucasHosseini,andHervéJégou. Thefaisslibrary,2024.
TiezhengGe,KaimingHe,QifaKe,andJianSun.Optimizedproductquantization.IEEETransactions
onPatternAnalysisandMachineIntelligence,36(4):744–755,2014.
RuiqiGuo,PhilipSun,ErikLindgren,QuanGeng,DavidSimcha,FelixChern,andSanjivKumar.
Acceleratinglarge-scaleinferencewithanisotropicvectorquantization. InProceedingsofthe37th
InternationalConferenceonMachineLearning,2020.
PiotrIndykandRajeevMotwani. Approximatenearestneighbors: Towardsremovingthecurseof
dimensionality. InProceedingsofthe30thAnnualACMSymposiumonTheoryofComputing,
pages604–613,1998.
ShikharJaiswal,RavishankarKrishnaswamy,AnkitGarg,HarshaVardhanSimhadri,andSheshansh
Agrawal. Ood-diskann: Efficientandscalablegraphannsforout-of-distributionqueries,2022.
SuhasJayaramSubramanya,FnuDevvrit,HarshaVardhanSimhadri,RavishankarKrishnawamy,and
RohanKadekodi. Diskann: Fastaccuratebillion-pointnearestneighborsearchonasinglenode.
InAdvancesinNeuralInformationProcessingSystems,volume32,2019.
HerveJégou,MatthijsDouze,andCordeliaSchmid. Productquantizationfornearestneighborsearch.
IEEETransactionsonPatternAnalysisandMachineIntelligence,33(1):117–128,2011.
JeffJohnson,MatthijsDouze,andHervéJégou. Billion-scalesimilaritysearchwithgpus. IEEE
TransactionsonBigData,7(3):535–547,2021.
10Yannis Kalantidis and Yannis Avrithis. Locally optimized product quantization for approximate
nearestneighborsearch. In2014IEEEConferenceonComputerVisionandPatternRecognition,
pages2329–2336,2014.
WeihaoKongandGregoryValiant. Spectrumestimationfromsamples. TheAnnalsofStatistics,45
(5):2218–2247,2017.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris
Alberti,DanielleEpstein,IlliaPolosukhin,JacobDevlin,KentonLee,KristinaToutanova,Llion
Jones,MatthewKelcey,Ming-WeiChang,AndrewM.Dai,JakobUszkoreit,QuocLe,andSlav
Petrov. Naturalquestions: Abenchmarkforquestionansweringresearch. Transactionsofthe
AssociationforComputationalLinguistics,7:452–466,2019.
TorLattimoreandCsabaSzepesvári. Banditalgorithms. CambridgeUniversityPress,2020.
YuA.MalkovandD.A.Yashunin. Efficientandrobustapproximatenearestneighborsearchusing
hierarchicalnavigablesmallworldgraphs. IEEETransactionsonPatternAnalysisandMachine
Intelligence,42(4):824–836,42020.
StanislavMorozovandArtemBabenko. Non-metricsimilaritygraphsformaximuminnerproduct
search. InAdvancesinNeuralInformationProcessingSystems,volume31.CurranAssociates,
Inc.,2018.
Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and
Li Deng. Ms marco: A human generated machine reading comprehension dataset, November
2016.
Mohammad Norouzi and David J. Fleet. Cartesian k-means. In Proceedings of the 2013 IEEE
ConferenceonComputerVisionandPatternRecognition,pages3017–3024,2013.
KarlPearson. Methodofmomentsandmethodofmaximumlikelihood. Biometrika,28(1/2):34–59,
1936.
JeffreyPennington, RichardSocher, andChristopherManning. GloVe: Globalvectorsforword
representation. InProceedingsofthe2014ConferenceonEmpiricalMethodsinNaturalLanguage
Processing,pages1532–1543,Doha,Qatar,October2014.
HarshaVardhanSimhadri, GeorgeWilliams, MartinAumüller, MatthijsDouze, ArtemBabenko,
Dmitry Baranchuk, Qi Chen, Lucas Hosseini, Ravishankar Krishnaswamny, Gopal Srinivasa,
SuhasJayaramSubramanya,andJingdongWang. Resultsoftheneurips’21challengeonbillion-
scaleapproximatenearestneighborsearch. InProceedingsoftheNeurIPS2021Competitionsand
DemonstrationsTrack,volume176ofProceedingsofMachineLearningResearch,pages177–189,
Dec2022.
AditiSingh,SuhasJayaramSubramanya,RavishankarKrishnaswamy,andHarshaVardhanSimhadri.
Freshdiskann: Afastandaccurategraph-basedannindexforstreamingsimilaritysearch,2021.
ThomasVecchiato,ClaudioLucchese,FrancoMariaNardini,andSebastianBruch. Alearning-to-
rankformulationofclustering-basedapproximatenearestneighborsearch. InProceedingsofthe
47thInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval,
2024. (toappear).
DavidP.Woodruff. Sketchingasatoolfornumericallinearalgebra. FoundationsandTrendsin
TheoreticalComputerScience,10(1–2):1–157,Oct2014. ISSN1551-305X.
XiangWu,RuiqiGuo,AnandaTheerthaSuresh,SanjivKumar,DanielNHoltmann-Rice,David
Simcha,andFelixYu. Multiscalequantizationforfastsimilaritysearch. InAdvancesinNeural
InformationProcessingSystems,volume30,2017.
ArtemBabenkoYandexandVictorLempitsky. Efficientindexingofbillion-scaledatasetsofdeep
descriptors. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, pages
2055–2063,2016.
11A Descriptionofdatasets
Weusethefollowingbenchmarkdatasetsinthiswork:
• TEXT2IMAGE: A cross-modal dataset, where data and query points may have different dis-
tributions in a shared space [Simhadri et al., 2022]. We use a subset consisting of 10 million
200-dimensionaldatapointsalongwithasubsetof10,000testqueries;
• MUSIC: 1million100-dimensionalpoints[MorozovandBabenko,2018]with1,000queries;
• DEEPIMAGE: Asubsetof10million96-dimensionalpointsfromthebilliondeepimagefeatures
dataset[YandexandLempitsky,2016]with10,000testqueries;
• GLOVE: 1.2 million, 200-dimensional word embeddings trained on tweets [Pennington et al.,
2014]with10,000testqueries;
• MSMARCO-MINILM: MS MARCO PassageRetrievalv1[Nguyenetal.,2016]isaquestion-
answeringdatasetconsistingof8.8millionshortpassagesinEnglish. Weusethe“dev”setof
queriesforretrieval,madeupof6,980questions. Weembedindividualpassagesandqueriesusing
theALL-MINILM-L6-V2model3toforma384-dimensionalvectorcollection;and,
• NQ-ADA2: 2.7 million, 1,536-dimensional embeddings of the NQ natural questions
dataset[Kwiatkowskietal.,2019]withtheADA-002model.4
Wenotethatthelastfourdatasetsareintendedforcosinesimilaritysearch. Assuchwenormalize
thesecollectionspriortoindexing,reducingthetasktoMIPSofEquation(1).
B Generalsolution
Let beanunknowndistributionoverRdfromwhichshard issampled. Itisclearthat,ifweare
i
D P
abletoaccuratelyapproximatethequantilesofS ,wecanobtainanestimateθ satisfyingProblem1.
i i
Letusmotivateourapproachbyconsideringaspecialcasewhere is (µ ,Σ ),aGaussianwith
i i
D N
meanµ RdandcovarianceΣ Rd×d. Inthiscase,S followsaunivariateGaussiandistribution
i i i
withmea∈
n q,µ
andvarianceq⊤∈
Σ q. Inthissetup,asolutiontoProblem1issimply:
i i
⟨ ⟩
(cid:18) (cid:19)
(cid:112) 1+δ
θ = q,µ + q⊤Σ q Φ−1 , (9)
i ⟨ i ⟩ i · N(0,1) 2
whichfollowsbyexpressingthecumulativedistributionfunction(CDF)ofS intermsoftheCDFof
i
aunitGaussian,denotedbyΦ .
N(0,1)
Noticeintheaboveexamplethat,wefirstmodeledthemomentsofS usingthemomentsof . We
i
D
thenapproximatedtheCDF(orequivalently,thequantilefunction)ofthedistributionofS usingits
i
moments—intheGaussiancase,theapproximationwiththefirsttwomomentsis,infact,exact.
Ourgeneralsolutionfollowsthatsamelogic. Inthefirststep,wecanobtainthefirstrmomentsof
thedistributionofS ,denotedbym (S )forj [r],fromthemomentsof ,denotedbym ( ). In
i j i j
∈ D D
asubsequentstep,weusem (S )’stoapproximatetheCDFofS .
j i i
Itiseasytoseethatthej-thmomentofS canbewrittenasfollows:
i
1 (cid:88)
m (S )=q⊗j m ( ) q⊗j x⊗j,
j i ⊙ j D ≈ n · ⊙
u1,...,un∼D
where⊗j isthej-foldtensorproduct,and tensorinnerproduct.
⊙
In our second step, we wish to find a distribution S˜ such that m (S˜) m (S ) for all j [r].
i j i j i
≈ ∈
Thiscanbedoneusingthe“methodofmoments,”aclassictechniqueinstatisticsPearson[1936]
thatoffersguarantees[KongandValiant,2017,Bravermanetal.,2022]intermsofadistributional
distancesuchastheWasserstein-1distance.
3Checkpointathttps://huggingface.co/sentence-transformers/all-MiniLM-L6-v2.
4https://openai.com/index/new-and-improved-embedding-model/
12Whileusinghigher-ordermomentscanleadtoabetterapproximation,computingthej-thmoment
forj >2canbehighlyexpensiveconsideringthedimensionalityofdatasetsseeninpractice,asthe
memoryrequirementtostorethetensorm ( )growsasdj. Hence,weleaveexplorationanddesign
j
D
ofanefficientversionofthistwo-stepapproachasfuturework.
C Sketchingthecovariancematrix
Let Σ = D +R be the decomposition of the PSD covariance matrix Σ into its diagonal D and
residualR=Σ D. LetQΛQ⊤betheorthogonaleigendecompositionofD−1/2RD−1/2. Recall
thatwedefineΣ˜−
,thesketch,forsome1 t das
≤ ≤
Σ˜ :=D+D1/2[Q] [Λ] [Q]⊤D1/2
t t t
WestartbyprovidingaproofforLemma2giventheassumptionsinthelemmaaretrue;i.e.the(t+1)-
theigenvalueofD−1/2ΣD−1/2isgreaterthanorequalto2andmin Σ /max Σ (1 ϵ)
i∈[d] ii j∈[d] jj
≥ −
forsome0 ϵ<1. Wethenproceedtojustifytheseassumptionsinthefollowingsection.
≤
C.1 ProofofLemma2
First,westatethreefactsthatwewillusetosimplifyourproofforLemma2.
Fact1. ForanysymmetricmatrixM Rd×d witheigendecompositionUSU⊤,wehavethatfor
anyv Rd, ∈
∈
d
(cid:88)
v⊤Mv = S v,U 2.
i i
·⟨ ⟩
i=1
Fact2. AssumingtheeigenvaluesΛofD−1/2RD−1/2aresortedinnon-increasingorder,wehave
thatI+D−1/2RD−1/2 =Q(I+Λ)Q⊤. Inwords,theeigenvectorsofI+D−1/2RD−1/2arethe
sameasD−1/2RD−1/2andthei-theigenvalueisΛ +1.
i
As a corollary, since I +D−1/2RD−1/2 is positive semidefinite, we have that Λ 1 for all
i
≥ −
i [d].
∈
Proof. ThisfollowseasilyafternoticingthatI =QQ⊤becauseQisad dmatrixwithorthonormal
×
columns(androws).
Fact3. ForadiagonalmatrixS Rd×dwithboundedpositiveentries,i.e. 0<l S uforall
ii
i [d],andarbitraryvectorv
R∈d,wehavethatl
v Sv u v .
≤ ≤
2 2 2
∈ ∈ ∥ ∥ ≤∥ ∥ ≤ ∥ ∥
Nowwearereadytoprovethelemma. Foranyarbitraryvectorv Rdwehavethat
∈
v⊤(D− 21ΣD−1 2 −D−1 2Σ˜D−1 2)v =v⊤(D−1 2RD− 21 −[Q] t[Λ] t[Q]⊤ t )v (10)
d
(cid:88)
= Λ Q ,v 2 (11)
i i
·⟨ ⟩
i=t+1
where the first equality follows by the definition of Σ˜ and the second by Fact 1. Hence, by the
definitionoferr(, ),wehavethaterr(D−1 2ΣD−1 2,D−1 2Σ˜D−1 2)=max
l∈[t+1,d]
Λ
l
.
· · | |
UsingFact2,wealsohavethat
err t(I+D−1 2RD−1 2)=err(I+D− 21RD− 21,[Q] t[I+Λ] t[Q]⊤
t
)
= max 1+Λ =1+Λ 0.
l t+1
l∈[t+1,d]| | ≥
Byourassumptionforthelemma,wehavethatif1+Λ 2,thenΛ 1. Hence,wehavethat
t+1 t+1
≥ ≥
max Λ 1+Λ sinceΛ 1foralll [d]. Hence,whenΛ 1,wehavethat
l∈[t+1,d] l t+1 l t+1
| |≤ ≥− ∈ ≥
err(D−1 2ΣD−1 2,D−1 2Σ˜D−1 2) err t(D− 21ΣD−1 2). (12)
≤
13Inordertoprovethelemma,weexpandoutthedefinitionoferr(, )
· ·
v⊤D−1/2(Σ Σ˜)D−1/2v
err(D−1 2ΣD−1 2,D−1 2Σ˜D−1 2)= sup | − |.
v 2
v∈Rd ∥ ∥2
SincewehavethatDhasstrictlypositiveentriesonthediagonal,wecandoachangeofvariables,
settingu=D1/2v. Denotingmax D by D ,thisgivesus
i∈[d] ii ∞
∥ ∥
v⊤(Σ Σ˜)v 1 v⊤(Σ Σ˜)v
err(D−1 2ΣD−1 2,D−1 2Σ˜D−1 2)= sup | − | sup | − |
u∈Rd ∥D1/2v ∥2 2 ≥ ∥D ∥∞ · u∈Rd ∥v ∥2 2
err(Σ,Σ˜)
=
D
∞
∥ ∥
wheretheinequalityfollowsbyFact3.
Similarly,sinceD (1 ϵ) D foralli [d],wecanbound
ii ∞
≥ − ∥ ∥ ∈
err (Σ)
err (D−1/2ΣD−1/2) t .
t ≤ (1 ϵ) D
∞
− ·∥ ∥
Puttingthesetworesultstogetherwiththeboundfrom(12)givesthelemma.
C.2 AssumptionsofLemma2
RecallthatwemaketwoassumptionsaboutthecovariancematrixΣandtheeigendecompositionof
itssymmetrization,D−1/2ΣD−1/2:
1. The (t+1)-th eigenvalue of D−1/2ΣD−1/2 is greater than or equal to 2. In particular,
lettingD−1/2ΣD−1/2 =Q(I+Λ)Q⊤betheorthogonaleigendecompositionofthesym-
metrization,weassume1+Λ 2.
t+1
≥
2. The diagonal of Σ has the property that min Σ (1 ϵ)max Σ for some
i∈[d] ii j∈[d] jj
≥ −
ϵ (0,1].
∈
Firstassumption. NoticethatsinceD−1/2ΣD−1/2 = I +D−1/2RD−1/2 isasymmetricPSD
matrix,wehavethattr(D−1/2ΣD−1/2)=(cid:80)d
1+Λ =d. Hencebydefinitiontheremustexist
i=1 i
sometforwhich1+Λ 1. Whileintheworsecasewecannothopefortheexistenceofan
t+1
≥
eigenvaluelargerthanthis,inpractice,includingforthedatasetsweconsiderinthiswork,itcanbe
shownthatinfacttheeigenvaluesofD−1/2ΣD−1/2 arelargerthan2acrossshardsanddatasets–
seeFigure5.
Secondassumption. SinceΣ= 1 (cid:80)n (x µ)(x µ)⊤forthenpointsx ,...,x Rdand
meanµ=(cid:80)n
x
/nintheshard,n itisi= in1 faci t− positivei −
semi-definite.
Inparticul1 artheenn tr∈
iesofthe
i=1 i
diagonalarenon-negative. Whileintheworstcase,thediagonalcanhavearbitrarilylargeentries
comparedtoitssmallestentries,inpracticethisisrarelythecase. Whilewedonotexplorehowto
removethisassumption,thereareseveralmechanismstodosoinpracticesuchasapplyingarandom
rotationsorpseudo-randomrotationsAilonandChazelle[2009],Woodruff[2014],AilonandLiberty
[2013]tothedatapointsx ,...,x ineachshardbeforeusingAlgorithm1. Itiswellknown(e.g.
1 n
Lemma1inAilonandChazelle[2009])thatafterapplyingsuchtransformsthatthecoordinatesof
thevectorsare“roughlyequal,”therebyensuringthatthediagonalofthecovariancehasentriesof
comparablemagnitude. Weleavetheexplorationofremovingthisassumptiontofuturework.
14600 1000
150
800
400 100
600
400
200 50
200
0 0 0
2 4 6 8 2 4 6 5 10 15 20
λ3(D−1/2ΣD−1/2) λ5(D−1/2ΣD−1/2) λ9(D−1/2ΣD−1/2)
(a)DEEPIMAGE (b)GLOVE (c)MSMARCO-MINILM
125 800 400
100
600 300
75
400 200
50
200 100
25
0 0 0
2.5 5.0 7.5 10.0 5 10 15 2.5 5.0 7.5 10.0
λ3(D−1/2ΣD−1/2) λ31(D−1/2ΣD−1/2) λ5(D−1/2ΣD−1/2)
(d)MUSIC (e)NQ-ADA2 (f)TEXT2IMAGE
Figure5: Histogramofthe(t+1)-theigenvalueforeachdataset. Foreachdataset, wepickthe
partitioningandtwhichweuseintheplotsfromFigure6. Plotsshowthatalmostallshardsforall
datasetshave(t+1)-theigenvalueboundedawayfrom2,exceptforsomeshardsforGLOVEand
oneforNQ-ADA2.
15
sdrahSforebmuN
sdrahSforebmuN
sdrahSforebmuN
sdrahSforebmuN
sdrahSforebmuN
sdrahSforebmuND Additionalexperimentalresults
Thissectionpresentsourfullexperimentalresultsforcompleteness.
D.1 ClusteringbySphericalKMeans
0.95
0.90
0.90
0.90 0.85
0.85
NormalizedMean
0.80
Mean
0.85 Scann(.5)
0.80 SubPartition(30)
0.75 Optimist(d,.8)
Optimist(30,.8)
0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0
%pointsprobed %pointsprobed %pointsprobed
(a)NQ-ADA2
0.98
0.98 0.975
0.96
0.96 0.950
0.94 0.94 0.925
NormalizedMean
Mean
0.92 0.92 0.900 Scann(.5)
SubPartition(2)
0.90 0.90 0.875 Optimist(d,.8)
Optimist(2,.8)
0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0
%pointsprobed %pointsprobed %pointsprobed
(b)MUSIC
0.94
0.90
0.94
0.92
0.88
0.92
0.90 0.86 NormalizedMean
Mean
0.90 Scann(.5)
0.88 0.84 SubPartition(4)
Optimist(d,.8)
0.88 Optimist(4,.8)
0.82
4 6 8 4 6 8 4 6 8
%pointsprobed %pointsprobed %pointsprobed
(c)GLOVE
Figure 6: Top-k recall versus the number of probed data points. Datasets are partitioned with
SphericalKMeans. SCANNhasparameterT,SUBPARTITIONt(leadingtot+2sub-partitionsper
shard),andOPTIMISTranktanddegreeofoptimismδ.
16
ycaruccA1-poT
ycaruccA1-poT
ycaruccA1-poT
ycaruccA01-poT
ycaruccA01-poT
ycaruccA01-poT
ycaruccA001-poT
ycaruccA001-poT
ycaruccA001-poT0.95
0.95 0.95
0.90
0.90 0.90
NormalizedMean
0.85 Mean
0.85 Scann(.5)
0.85
SubPartition(4)
0.80 Optimist(d,.8)
0.80 Optimist(4,.8)
0.80
0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0
%pointsprobed %pointsprobed %pointsprobed
(d)TEXT2IMAGE
0.925
0.96 0.94
0.900
0.92
0.94 0.875
0.90
0.850 NormalizedMean
Mean
0.92 0.88
Scann(.5)
0.825
SubPartition(8)
0.86 Optimist(d,.8)
0.90 0.800 Optimist(8,.8)
0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0
%pointsprobed %pointsprobed %pointsprobed
(e)MSMARCO-MINILM
0.975 0.95
0.95
0.950
0.90
0.925 NormalizedMean
0.90 Mean
0.900 Scann(.5)
0.85 SubPartition(2)
Optimist(d,.8)
0.875 0.85 Optimist(2,.8)
0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0
%pointsprobed %pointsprobed %pointsprobed
(f)DEEPIMAGE
Figure 6: Top-k recall versus the number of probed data points. Datasets are partitioned with
SphericalKMeans. SCANNhasparameterT,SUBPARTITIONt(leadingtot+2sub-partitionsper
shard),andOPTIMISTranktanddegreeofoptimismδ.
17
ycaruccA1-poT
ycaruccA1-poT
ycaruccA1-poT
ycaruccA01-poT
ycaruccA01-poT
ycaruccA01-poT
ycaruccA001-poT
ycaruccA001-poT
ycaruccA001-poTD.2 ClusteringbyStandardKMeans
0.95
0.90
0.90
0.90 0.85
0.85
NormalizedMean
0.80 Mean
0.85 Scann(.5)
0.80 SubPartition(30)
0.75 Optimist(d,.8)
Optimist(30,.8)
0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0
%pointsprobed %pointsprobed %pointsprobed
(a)NQ-ADA2
0.96 0.96 0.96
0.94 0.94 0.94
0.92
0.92 0.92
NormalizedMean
0.90 Mean
0.90 0.90 Scann(.5)
0.88 SubPartition(2)
0.88 0.88 Optimist(d,.8)
0.86 Optimist(2,.8)
0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0
%pointsprobed %pointsprobed %pointsprobed
(b)MUSIC
0.94
0.94 0.90
0.92
0.92 0.88
0.90
0.90 0.86 NormalizedMean
Mean
0.88 Scann(.5)
0.88 0.84 SubPartition(4)
Optimist(d,.8)
0.86 Optimist(4,.8)
0.86 0.82
4 6 8 4 6 8 4 6 8
%pointsprobed %pointsprobed %pointsprobed
(c)GLOVE
Figure7:Top-krecallversusthenumberofprobeddatapoints.DatasetsarepartitionedwithStandard
KMeans. SCANNhasparameterT,SUBPARTITIONt(leadingtot+2sub-partitionspershard),and
OPTIMISTranktanddegreeofoptimismδ.
18
ycaruccA1-poT
ycaruccA1-poT
ycaruccA1-poT
ycaruccA01-poT
ycaruccA01-poT
ycaruccA01-poT
ycaruccA001-poT
ycaruccA001-poT
ycaruccA001-poT0.95
0.95 0.95
0.90
0.90 0.90
NormalizedMean
0.85 Mean
0.85 0.85 Scann(.5)
SubPartition(4)
0.80 Optimist(d,.8)
0.80 0.80 Optimist(4,.8)
0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0
%pointsprobed %pointsprobed %pointsprobed
(d)TEXT2IMAGE
0.950 0.925
0.96
0.900 0.925
0.94
0.875
0.900
0.92 0.850 NormalizedMean
Mean
0.875
0.90 0.825 Scann(.5)
SubPartition(8)
0.850 Optimist(d,.8)
0.800
0.88 Optimist(8,.8)
0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0
%pointsprobed %pointsprobed %pointsprobed
(e)MSMARCO-MINILM
1.000
0.975 0.95
0.95
0.950
0.90
0.925 NormalizedMean
0.90 Mean
Scann(.5)
0.900
0.85 SubPartition(2)
Optimist(d,.8)
0.875 0.85 Optimist(2,.8)
0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0
%pointsprobed %pointsprobed %pointsprobed
(f)DEEPIMAGE
Figure7:Top-krecallversusthenumberofprobeddatapoints.DatasetsarepartitionedwithStandard
KMeans. SCANNhasparameterT,SUBPARTITIONt(leadingtot+2sub-partitionspershard),and
OPTIMISTranktanddegreeofoptimismδ.
19
ycaruccA1-poT
ycaruccA1-poT
ycaruccA1-poT
ycaruccA01-poT
ycaruccA01-poT
ycaruccA01-poT
ycaruccA001-poT
ycaruccA001-poT
ycaruccA001-poTD.3 ClusteringbyGMM
Wenotethat,duetothedimensionalityoftheNQ-ADA2dataset,wewereunabletocompleteGMM
clusteringonthisparticulardataset.
1.00 1.000
0.98
0.98 0.975
0.96
0.96
0.950
0.94 0.94 NormalizedMean
0.925 Mean
0.92 0.92 Scann(.5)
0.900 SubPartition(2)
0.90 0.90 Optimist(d,.8)
Optimist(2,.8)
0.875
0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0
%pointsprobed %pointsprobed %pointsprobed
(a)MUSIC
0.94
0.94 0.90
0.92
0.88
0.92
0.90 0.86
0.90 NormalizedMean
Mean
0.88 0.84 Scann(.5)
0.88 SubPartition(4)
0.82 Optimist(d,.8)
0.86 Optimist(4,.8)
4 6 8 4 6 8 4 6 8
%pointsprobed %pointsprobed %pointsprobed
(b)GLOVE
Figure8:Top-krecallversusthenumberofprobeddatapoints.DatasetsarepartitionedwithGaussian
Mixture Model. SCANN has parameter T, SUBPARTITION t (leading to t+2 sub-partitions per
shard),andOPTIMISTranktanddegreeofoptimismδ.
20
ycaruccA1-poT
ycaruccA1-poT
ycaruccA01-poT
ycaruccA01-poT
ycaruccA001-poT
ycaruccA001-poT0.95
0.95 0.95
0.90
0.90 0.90
0.85 0.85 0.85 NormalizedMean
Mean
0.80 0.80 0.80 S Sc ua bPnn ar( t.5 i) tion(4)
Optimist(d,.8)
0.75 0.75 0.75 Optimist(4,.8)
0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0
%pointsprobed %pointsprobed %pointsprobed
(c)TEXT2IMAGE
0.950
0.96
0.925 0.90
0.94
0.92 0.900 NormalizedMean
0.85 Mean
Scann(.5)
0.875
0.90 SubPartition(8)
Optimist(d,.8)
0.88 0.850 0.80 Optimist(8,.8)
0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0
%pointsprobed %pointsprobed %pointsprobed
(d)MSMARCO-MINILM
1.00
0.95 0.95
0.95
0.90
0.90
0.90 NormalizedMean
0.85 Mean
Scann(.5)
0.85 SubPartition(2)
0.85
0.80 Optimist(d,.8)
Optimist(2,.8)
0.80
0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0
%pointsprobed %pointsprobed %pointsprobed
(e)DEEPIMAGE
Figure8:Top-krecallversusthenumberofprobeddatapoints.DatasetsarepartitionedwithGaussian
Mixture Model. SCANN has parameter T, SUBPARTITION t (leading to t+2 sub-partitions per
shard),andOPTIMISTranktanddegreeofoptimismδ.
E Effectofparameters
Recall that the final OPTIMIST algorithm takes two configurable parameters: t, the rank of the
covariancesketchinEquation(7);andδ,thedegreeofoptimisminProblem1. Inthissection,we
examinetheeffectoftheseparametersontheperformanceofOPTIMIST.
Figure9visualizestheroleplayedbyt. Itcomesasnosurprisethatlargervaluesoftleadtoabetter
approximation of the covariance matrix. What we found interesting, however, is the remarkable
effectivenessofasketchthatsimplyretainsthediagonalofthecovariance,denotedbyOPTIMIST(0, ),
·
inthesettingsofkweexperimentedwith(i.e.,k 1,10,100 ).
∈{ }
Inthesamefigure,wehavealsoincludedtwoconfigurationsofSUBPARTITION: onewithjust2sub-
partitions,denotedbySUBPARTITION(0),andanotherwitht+2sub-partitions,SUBPARTITION(t).
ThesehelpputtheperformanceofOPTIMISTwithvariousranksinperspective. Inparticular,wegive
theSUBPARTITIONbaselinethesameamountofinformationandcontrastitsrecallwithOPTIMIST.
21
ycaruccA1-poT
ycaruccA1-poT
ycaruccA1-poT
ycaruccA01-poT
ycaruccA01-poT
ycaruccA01-poT
ycaruccA001-poT
ycaruccA001-poT
ycaruccA001-poT0.90 0.92
0.90
0.88
0.88 0.90
0.86
0.86 Optimist(d,.8) Optimist(d,.8) Optimist(d,.8)
Optimist(0,.8) Optimist(0,.8) Optimist(0,.8)
0.84
Optimist(16,.8) Optimist(2,.8) 0.88 Optimist(4,.8)
Optimist(30,.8) Optimist(4,.8) Optimist(8,.8)
0.84
SubPartition(0) SubPartition(0) SubPartition(0)
0.82
SubPartition(30) SubPartition(4) SubPartition(8)
1.0 1.5 2.0 4 6 8 1.0 1.5 2.0
%pointsprobed %pointsprobed %pointsprobed
(a)NQ-ADA2 (b)GLOVE (c)MSMARCO-MINILM
0.98
0.98
0.96
0.96 0.97
0.94
Optimist(d,.8)
Optimist(d,.8) 0.96 Optimist(d,.8) Optimist(0,.8)
0.94 Optimist(0,.8) Optimist(0,.8) Optimist(2,.8)
Optimist(4,.8) Optimist(2,.8) Optimist(4,.8)
S Su ub bP Pa ar rt ti it ti io on n( (0 2) ) 0.95 S Su ub bP Pa ar rt ti it ti io on n( (0 2) ) 0.92 S Su ub bP Pa ar rt ti it ti io on n( (0 4) )
1.0 1.5 2.0 1.0 1.5 2.0 1.0 1.5 2.0
%pointsprobed %pointsprobed %pointsprobed
(d)DEEPIMAGE (e)MUSIC (f)TEXT2IMAGE
Figure 9: Top-100 recall versus the number of probed data points. Datasets are partitioned with
SphericalKMeans. Wecomparearangeofvaluesfortherankofthecovariancesketch(t)withSUB-
PARTITIONwiththesamecapacityasOPTIMISTwiththesmallestandlargestranks. OPTIMIST(0, )
·
denotes a sketch that only retains the diagonal of the covariance matrix; and SUBPARTITION(t)
meanseveryshardispartitionedintot+2sub-partitions.
22
ycaruccA001-poT
ycaruccA001-poT
ycaruccA001-poT
ycaruccA001-poT
ycaruccA001-poT
ycaruccA001-poTWe turn to Figure 10 to understand the impact of δ. It is clear that encouraging OPTIMIST to
be too optimistic can lead to sub-optimal performance. That is because of our reliance on the
Chebyshev’sinequality,whichcanprovetooloose,leadingtoanoverestimationofthemaximum
value. Interestingly,δ (0.6,0.8)appearstoyieldbetterrecallacrossdatasets.
∈
0.90
0.92
0.90
0.88 0.91
0.88
0.86 0.90
0.86 Optimist(d,.2) Optimist(d,.2) Optimist(d,.2)
Optimist(d,.4) Optimist(d,.4) Optimist(d,.4)
Optimist(d,.6) Optimist(d,.6) 0.89 Optimist(d,.6)
Optimist(d,.8) 0.84 Optimist(d,.8) Optimist(d,.8)
0.84
Optimist(d,.9) Optimist(d,.9) Optimist(d,.9)
SubPartition(30) SubPartition(4) 0.88 SubPartition(8)
1.0 1.5 2.0 4 6 8 1.0 1.5 2.0
%pointsprobed %pointsprobed %pointsprobed
(a)NQ-ADA2 (b)GLOVE (c)MSMARCO-MINILM
0.98
0.98 0.96
0.96
0.97
0.94
0.94 Optimist(d,.2) Optimist(d,.2) Optimist(d,.2)
Optimist(d,.4) Optimist(d,.4) Optimist(d,.4)
Optimist(d,.6) 0.96 Optimist(d,.6) Optimist(d,.6)
Optimist(d,.8) Optimist(d,.8) Optimist(d,.8)
0.92 Optimist(d,.9) Optimist(d,.9) 0.92 Optimist(d,.9)
SubPartition(2) SubPartition(2) SubPartition(4)
1.0 1.5 2.0 1.0 1.5 2.0 1.0 1.5 2.0
%pointsprobed %pointsprobed %pointsprobed
(d)DEEPIMAGE (e)MUSIC (f)TEXT2IMAGE
Figure10: Top-100recallversusthenumberofprobeddatapoints. Datasetsarepartitionedwith
SphericalKMeans. Wecomparearangeofvaluesfortheoptimismparameter(δ)withthestrongest
baseline(SUBPARTITION)fromAppendixD.1. Asδ 1,OPTIMISTbecomesmoreoptimistic.
→
23
ycaruccA001-poT
ycaruccA001-poT
ycaruccA001-poT
ycaruccA001-poT
ycaruccA001-poT
ycaruccA001-poTF Predictionerror
0.20 1.0
NormalizedMean SubPartition(30) NormalizedMean SubPartition(4)
Mean Optimist(d,.8) Mean Optimist(d,.8)
Scann(.5) Optimist(30,.8) 0.8 Scann(.5) Optimist(4,.8)
0.15
0.6
0.10
0.4
0.05
0.2
0.00 0.0
10 1 100 101 102 10 1 100 101 102
− −
% Shards % Shards
(a)NQ-ADA2 (b)GLOVE
1.50 1.0
NormalizedMean SubPartition(8) NormalizedMean SubPartition(2)
Mean Optimist(d,.8) Mean Optimist(d,.8)
1.25
Scann(.5) Optimist(8,.8) 0.8 Scann(.5) Optimist(2,.8)
1.00
0.6
0.75
0.4
0.50
0.2
0.25
0.00 0.0
10 1 100 101 102 10 1 100 101 102
− −
% Shards % Shards
(c)MSMARCO-MINILM (d)DEEPIMAGE
1.0
NormalizedMean SubPartition(2)
NormalizedMean SubPartition(4)
1.5 Mean Optimist(d,.8) Mean Optimist(d,.8)
Scann(.5) Optimist(2,.8) 0.8 Scann(.5) Optimist(4,.8)
1.0 0.6
0.4
0.5
0.2
0.0 0.0
10 1 100 101 102 10 1 100 101 102
− −
% Shards % Shards
(e)MUSIC (f)TEXT2IMAGE
Figure11: Meanpredictionerror (τ, ),definedinEquation(8),versusℓ(expressedaspercentof
ℓ
E ·
totalnumberofshards),forvariousroutersanddatasets.
24
rorrE
noitciderP
rorrE
noitciderP
rorrE
noitciderP
rorrE
noitciderP
rorrE
noitciderP
rorrE
noitciderP