Images that Sound:
Composing Images and Sounds on a Single Canvas
ZiyangChen DanielGeng AndrewOwens
UniversityofMichigan
https://ificl.github.io/images-that-sound/
”castle with bell towers, grayscale, lithograph style” ”tigers, grayscale, black background”
”bell ringing” ”tiger growling”
Time Time
Figure1:Imagesthatsound.Weusediffusionmodelstogeneratespectrograms(secondrow)thatlooklike
naturalimages,whichwecallimagesthatsound.Thesespectrogramscanbeconvertedintonaturalsounds(third
row)usingapretrainedvocoder,orcolorizedtoobtainmorevisuallypleasingresults(firstrow).Pleasereferto
ourwebsitetolistentothesounds.
Abstract
Spectrogramsare2Drepresentationsofsoundthatlookverydifferentfromtheim-
agesfoundinourvisualworld. Andnaturalimages,whenplayedasspectrograms,
make unnatural sounds. In this paper, we show that it is possible to synthesize
spectrogramsthatsimultaneouslylooklikenaturalimagesandsoundlikenatural
audio. We call these spectrograms images that sound. Our approach is simple
andzero-shot,anditleveragespre-trainedtext-to-imageandtext-to-spectrogram
diffusionmodelsthatoperateinasharedlatentspace. Duringthereverseprocess,
wedenoisenoisylatentswithboththeaudioandimagediffusionmodelsinparal-
lel,resultinginasamplethatislikelyunderbothmodels. Throughquantitative
evaluationsandperceptualstudies,wefindthatourmethodsuccessfullygenerates
spectrogramsthatalignwithadesiredaudiopromptwhilealsotakingthevisual
appearanceofadesiredimageprompt.Pleaseseeourprojectpageforvideoresults:
https://ificl.github.io/images-that-sound/
Preprint.Underreview.
4202
yaM
02
]VC.sc[
1v12221.5042:viXra
egami
roloC
margortcepS
mrofevaW
ycneuqerFIMAGES SPECTROGRAMS IMAGESTHATSOUND
“castle” “bell ringing” “castle”&“bell ringing”
“kitten” “cat meowing” “kitten”&“cat meowing”
Figure2:Imagesvs.spectrograms.WeshowgrayscaleimagesgeneratedfromStableDiffusion[86]onthe
left,followedbylog-melspectrogramsgeneratedfromAuffusion[106]inthemiddle,andourgeneratedimages
thatsoundresultsontheright.
1 Introduction
Thespectrogramisaubiquitouslow-dimensionalrepresentationforaudiomachinelearningthatplots
theenergywithindifferentfrequenciesovertime. Butitisalsowidelyusedasatoolforconverting
soundintoavisualformthatcanbe—atleastpartially—perceivedbysight. Forexample,inthis
representation(Fig.2), eventonsetslooktoahumanobserverlikelines, andspeechlookslikea
sequenceofwavesandbands. Thisinsightiscommonlyusedwithintheaudiocommunity,which
frequentlyrepurposespretrainedvisualnetworksforaudiotasks,oftenwithonlyrelativelyminor
modifications[44,80,62,61,31,106,101].
Wehypothesizethatthesuccessofspectrogramsintheserolesisdueinparttothefactthattheyshare
manystatisticalpropertieswiththedistributionofnaturalimages,providingvisualstructureslike
edgesandtexturesthatthehumanvisualsystemcanreadilyprocess. Giventhestatisticalsimilarities
betweenimagesandsounds,weaskwhetheritispossibletoautomaticallygenerateexamplesthatlie
attheintersectionofbothmodalities. Wecreateimagesthatsound(Fig.1),2Dmatricesthatlook
semanticallymeaningfulwhenviewedasimages,butthatalsosoundmeaningfulwhenplayedas
aspectrogram. Thisgenerativemodelingproblemischallenging,becauseitrequiresmodelinga
distributionthatisinducedbytwoverydifferentdatasources,andnorelevantpaireddataisavailable.
We are motivated by the “spectrogram art” that has been made by a variety of artists [9], most
famouslybymusiciansAphexTwin[2]andNineInchNails[76]. Theseartistsmanipulatetheir
songstodisplayadesiredimagewhentheyarevisualizedasspectrograms,suchasbyshowingthe
artist’sfaceoralbumart. Incurrentpractice,thereisasteeptrade-offbetweenthequalityofthe
imageandthesound,sinceitisdifficulttosimultaneouslycontroltheinterpretationofasignalin
bothmodalities. Asaresult,existingartworkoftencomesacrosstothelistenerasdissonantoras
randomnoise, ratherthanasnaturalsounds.1 Bycontrast,weaimtogeneratesignalsthatareas
naturalaspossibleinbothmodalities,suchastowersthatsimultaneouslysoundlikeringingbellsor
imagesoftigersthatmakearoaringsound(Fig.1).
Inthiswork,weposethisproblemasamultimodalcompositionalgenerationtaskandproposea
simple,zero-shotmethodthatcomposesoff-the-shelftext-to-spectrogramandtext-to-imagediffusion
modelsfromdifferentmodalities. Inspiredbypriorworkoncompositionalityindiffusionmodels[65,
26,38,37],wedenoiseusingbothanoiseestimatefromthespectrogrammodelandanoiseestimate
fromtheimagemodel. Thisispossiblebecausethesetwomodelsperformdiffusioninthesamelatent
space. Theresultisasamplethatissimultaneouslylikelyunderthe(text-conditional)distribution
ofspectrogramsandimages. Thespectrogramsarethenconvertedtowaveformsusingapretrained
vocoder. Inaddition,weshowthattheseblack-and-whiteimagesmaybecolorized,resultingincolor
imageswhosegrayscaleversionscanbeplayedasspectrograms.
Surprisingly, we find that off-the-shelf diffusion models trained on different modalities can be
composedtogethertoobtainsamplesthatfunctionasbothanimageandasound.Oftentheseexamples
reusevisualelementsinunexpectedways(e.g.,inFig.1,alineisboththeonsetofabellchimeand
thecontourofabelltower). Weprovidequalitativeresults,aswellasquantitativecomparisonsand
humanstudyresultsagainstbaselines,indicatingthatourmethodproducesspectrogramsthatbetter
alignwithboththeaudioandimageprompts. Ourcontributionsaresummarizedasfollows:
1Weencouragethereadertolistentopopularexamplesofspectrogramart[9].
2• Weproposeimagesthatsound,atypeofmultimodalartthatcanbebothunderstoodasanimage
orplayedasasound.
• Weshowthatwecancomposepretraineddiffusionmodelsfromdifferentmodalitiesinazero-shot
fashiontoproducetheseimagesthataresimultaneouslyspectrograms.
• Weproposealternativemethodsforgeneratingimagesthatsound,onebasedonscoredistillation
sampling[10,83]andanotherbasedonsimplysubtractinganimagefromaspectrogram.
• Wefindthroughqualitativeandquantitativeexperimentsthatourmethodoutperformsbaseline
approachesandgenerateshigh-qualitysamples.
2 RelatedWork
Diffusion models. Diffusion models [91, 49, 95, 24, 93] are a class of generative models that
learntoreverseaforwardprocessthatiterativelycorruptsdata. Typically,thisforwardprocessadds
Gaussian noise and the reverse process learns to denoise the data by predicting the added noise.
Diffusionmodelshavefoundapplicationinavarietyofapplications,includingtext-conditionedimage
generation[24,86,75,23,88],videogeneration[48,51,90,6,41,103],imageandvideoediting[84,
87,70,45,8,28,36],audiogeneration[106,63,64,29,68,39],pointscloudgeneration[66],material
generation[109],andcameraposeestimation[110]. Inthiswork,weuseStableDiffusion[86],a
latent diffusion model trained for text-conditioned image generation, as well as Auffusion [106],
atext-conditionedaudiogenerationmodeltrainedtoproducelog-melspectrograms. Auffusionis
finetunedfromStableDiffusion,similartoRiffusion[31],andasaresult,thetwomethodssharea
latentspace. Thisiscrucialforourtechnique,whichjointlydiffusesthesesharedlatents.
Compositionalgeneration. Onepropertyofdiffusionmodelsisthattheyadmitastraightforward
techniquetocomposeconceptsbysummingnoiseestimates. Thismaybeunderstoodbyviewing
noiseestimatesasgradientsofaconditionaldatadistribution[94,95],andthesumofthesegradients
aspointinginthedirectionthatmaximizesmultipleconditionallikelihoods. Thisapproachhasbeen
appliedtoenablecompositionsoftextpromptsglobally[65],spatially[7,26],transformationsof
images [38], and image components [37]. We go beyond these works by showing that diffusion
modelsfromtwodifferentmodalitiescansuccessfullybecomposedtogether.
Audio-visuallearning. Avarietyofworkshavelearnedcross-modalassociationsbetweenvision
and sound. Some approaches establish semantic correspondence, i.e., which sounds and visuals
are commonly associated with one another [3, 97]. Previous work has used this cue to learn
cross-modal representations [5, 74, 71, 42, 40, 62, 61] and audio-visual sound localization [4,
52, 72, 53, 89, 81, 67]. Some researchers focus on the temporal correspondence between audio
and visual streams [58, 78, 30, 14, 96, 56] to study source separation [111, 1, 34], Foley sound
generation [55, 25, 105, 69], and action recognition [35, 54, 77]. Others also explore the spatial
correspondencebetweenthem[18,32,108,19,68]includingspatialsoundgeneration[33,73,12,60]
andaudio-visualacousticlearning[11,92,21,16,17]. Differingfromtheworksabove,ourfocusis
toexploretheintersectionofthedistributionsbetweenspectrogramsandimages,wherewecreate
spectrogramsthatcanbeunderstoodasvisualimagesandcanalsobeplayedassounds.
Audiosteganography. Audiosteganographyisthepracticeofconcealinginformationwithinan
audiosignal. Artistshaveexploreditforcreativeexpression[100,99]. AphexTwinembeddeda
visualofhisfaceintheaudiowaveformofthetrack“Formula”[2]. NoamOxmancreatesanimal
portraits made of musical notations [79]. Other work has proposed deep learning methods for
steganography,suchashidingvideocontentinsideaudiofileswithinvertiblegenerativemodels[107],
hidingaudiodatainsideanidentityimage[112],andaudiowatermarking[13]. Ourapproachcanbe
viewedasasteganographymethodthathidesanimagewithinanaudiotrack,andisonlyrevealed
whenthetrackisconvertedtoaspectrogram.
3 Method
Ourgoalistogeneratespectrogramsthatsimultaneouslyrepresentbothasoundandanimage,each
ofwhichisspecifiedbyatextprompt. Whenthespectrogramisconvertedintoawaveform, the
soundmatchestheaudioprompt,whilewhenitisvisuallyinspected,itshouldtaketheappearance
ofagivenvisualprompt(Fig.1). Todothis,wesamplefromthejointdistributionofimagesand
spectrograms,usingoff-the-shelfdiffusionmodelstrainedoneachmodalityindependently.
3”castle with bell towers, grayscale”
Latent
Image Diffusion
λ( vt)
ϵ(t) Weighted
shared latent space v Avg.
z t Latent λ(t) ϵ(t)
a
Audio Diffusion
ϵ(t) !
”bell ringing” a
iterative denoising
Latent
Vocoder
Decoder
z
0 waveform
log-mel spectrogram
Figure3:Composingaudioandvisualdiffusionmodels.Wegenerateaspectrogramthatcanbevisualizedas
animageorplayedasasound.Givenanoisylatentz ,weapplyvisualandaudiodiffusionmodels,eachguided
t
byatextprompt,tocomputenoiseestimatesϵ(t)andϵ(t)respectively.Weobtainthemultimodalnoiseestimate
v a
˜ϵ(t)byaweightedaverage,thenuseitaspartoftheiterativedenoisingprocess.Finally,wedecodetheclean
latentz toaspectrogramandconvertitintoawaveformusingapretrainedvocoder(orbyGriffin-Lim[43]).
0
3.1 Preliminaries
Diffusionmodels. Diffusionmodels[49,95]iterativelydenoisestandardGaussiannoise,x ∼
T
N(0,I),togeneratecleansamples,x ,fromsomelearneddatadistribution. Attimesteptinthe
0
reverse diffusion process, the noise predictor, ϵ , takes the intermediate noisy sample, x , and
θ t
the condition y, such as a text prompt embedding, to estimate the noise ϵ (x ;y,t). Following
θ t
DDIM[93],weobtainthenext,lessnoisy,samplex attheprevioustimestepvia:
t−1
√
√ x − 1−α ·ˆϵ (x ;y,t)
x = α t √t θ t + 1−α −σ2·ˆϵ (x ;y,t)+σ ϵ , (1)
t−1 t−1 α t−1 t θ t t t
(cid:18) t (cid:19) (cid:113)
whereϵ isindependentGaussiannoise,α isapredefinedcoefficient,andσ controlstherandomness
t t t
level which we set to 0 for deterministic sampling. We may also optionally apply classifier-free
guidance(CFG)[50]bymodifyingthenoiseestimateas:
ˆϵ (x ;y,t)=ϵ (x ;∅,t)+γ(ϵ (x ;y,t)−ϵ (x ;∅,t)), (2)
θ t θ t θ t θ t
whereγ denotesthestrengthoftheconditionalguidanceand∅istheunconditionalembeddingof
theemptystring. Thisoftenresultsinmuchhigher-qualitysamples.
Latentdiffusion. LatentDiffusionModels(LDMs)[86]performthediffusionprocessinalatent
spaceratherthaninpixelspace. Apretrainedencoderanddecoderpair,E andD,translatesbetween
pixelspaceandlatentspace. Thelatentspaceistypicallymuchmorecompactandinformation-dense,
whichmakesdiffusioninthisspacemoreefficient. WeusepretrainedLDMsinourapproach,dueto
theavailabilityofaudioandvisualmodelswiththesamelatentspace.
3.2 MultimodalDenoising
Our goal is to generate an example x ∈ Rm×n that would be likely to appear under both visual
andaudiodistributions, p (·)andp (·). Weformulatethisassamplingfromaproductofexpert
a v
models2[47]: p (x)∝p (x)p (x). Wefollowrecentworkonthecompositionalgenerationthat
av a v
samplesfromthisdistributionusingthescorefunctionsfrompretraineddiffusionmodels[27]. In
contrasttotheseapproaches,however,ourtwomodelsaretrainedontwodifferentmodalities.
2Recent work has called this a conjunction [27], since conceptually the samples are roughly from the
intersectionofbothdistributions.
4Wecreateourspectrogramsusingtwopretrainedlatentdiffusionmodels. Onetrainedtogenerate
images,ϵ (·,·,·),andtheothertogeneratespectrograms,ϵ (·,·,·),bothoperatinginthesame
ϕ,v ϕ,a
latentspace. WeshowanoverviewofourmethodinFig.3. Givenanoisylatent,z ,andtextprompts
t
y andy correspondingtothedesiredimageandspectrogrampromptrespectively,wecomputetwo
v a
CFGnoiseestimates(Eq.(2)):
ϵ(t) =ϵ (z ;∅,t)+γ (ϵ (z ;y ,t)−ϵ (z ;∅,t)), (3)
v ϕ,v t v ϕ,v t v ϕ,v t
ϵ(t) =ϵ (z ;∅,t)+γ (ϵ (z ;y ,t)−ϵ (z ;∅,t)), (4)
a ϕ,a t a ϕ,a t a ϕ,a t
where γ and γ are the corresponding visual and audio guidance scales. We then combine the
v a
noiseestimatesfrombothmodalitiesbyapplyingweightedaveraging,producingamultimodalnoise
estimatethatsteersthedenoisingprocesstowardasamplethatislikelyunderthedistributionofboth
imagesandspectrograms:
˜ϵ(t) =λ(t)ϵ(t)+λ(t)ϵ(t), (5)
a a v v
whereλ(t)andλ(t)aretheweightsoftheaudioandvisualnoiseestimatesattimesteptrespectively.
a v
Withthisnewnoiseestimate˜ϵ(t),weperformastepofDDIM(Eq.(1))toobtainalessnoisylatent,
z . Repeatingthisprocessweobtainthecleanlatentz ,whichisthendecodedusingthedecoder
t−1 0
Dtoobtainthespectrogramxˆ=D(z ). Thisspectrogramcanfurtherbeconvertedtoawaveform
0
usingapretrainedvocoderorcolorizedtoanRGBimagewhosegrayscaleversionisthespectrogram.
Warm-starting. Wefinditusefultowarm-startthedenoisingprocess. InSec.4.5,weexperiment
withwarm-startingusingonlythespectrogramnoiseestimatesoronlytheimagenoiseestimates.
Thiscanberepresentedbyusingw(t) andw(t) astherelativeweightontheaudioandthevisual
a v
noiseestimatesrespectively. Welet
w(t) w(t)
λ(t) = a , λ(t) = v , (6)
a w(t)+w(t) v w(t)+w(t)
a v a v
with w(t) = H(t T −t) and w(t) = H(t T −t) being Heaviside step functions, and t and t
a a v v a v
indicatingtheproportionofthereverseprocessthathasaudioorvisualdenoisingrespectively. When
t <1.0andt =1.0,wewarm-startwithonlyimagedenoising,andvice-versa. Theaboveensures
a v
thattheweightsλ(t)andλ(t)sumtoone,andareequallyweightedafterwarm-starting.
a v
Colorization. Afterwegenerateaspectrogram,xˆ,wecanoptionallycolorizeittocreateamore
visuallyappealingresult. Sinceourspectrogramsfalloutsidethedistributionofpre-trainedcoloriza-
tionmodels,weuseFactorizedDiffusion[37]tocolorize,whichsamplesadiffusionmodelwhile
projectingthenoisyintermediateimagessuchthattheyequalxˆwhenturnedintograyscale. Indoing
so,thedenoisingprocesssynthesizesonlythe“colorcomponent”ofthesampledimage,whilethe
“grayscalecomponent”isconstrainedtoequalthegeneratedspectrogram. Notethatthismethodis
similartopriorwork[57,20,95,102]. Wechoosethisparticularmethodduetoitssimplicity.
4 Experiments
Weevaluateourmethodsusingquantitativemetricsandhumanstudies. Wealsopresentqualitative
comparisonsandananalysisofourmethod,andwhyitworks.
4.1 ImplementationDetails
Models. Weselectapairofoff-the-shelflatentdiffusionimageandaudiomodelsthatsharethe
samelatentspace,encoder,anddecoder. Fortheimagemodel,weuseStableDiffusionv1.53[86].
Fortheaudiomodel,weuseAuffusion4 [106],whichfinetunesStableDiffusionv1.5onlog-mel
spectrograms.Tosynthesizeaudiofromthelog-melspectrograms,weconsidertwooptions:following
[106]andusingoff-the-shelfHiFi-GAN[59]vocoder,ortheGriffin-Limalgorithm[43,82]. Weuse
HiFi-GANforourmainexperiments. InSec.4.5,weevaluatethechoiceofvocoderandverifythat
ourresultantwaveformsdoindeedencodetoavisuallyinterpretablespectrogram.
3Stable Diffusion v1-5 hugging face model card 4Auffusion hugging face model card
5Table1:Quantitativeevaluationonimagesthatsound.WereportCLIPandCLAPscores.Wealsoreport
95%confidenceintervalingray.Thebestresultsareinbold.
Method Modality CLIP(%)↑ CLAP(%)↑
StableDiffusion[86] V 34.5(±0.1) 2.2(±0.2)
Auffusion[106] A 22.5(±0.1) 48.3(±0.6)
Imprint A&V 27.2(±0.2) 32.3(±1.0)
SDS A&V 25.4(±0.2) 23.4(±1.4)
Ours A&V 28.2(±0.1) 33.5(±0.9)
Hyperparameters. Webeginthereverseprocesswithrandomlatentnoisez ∈R4×32×128,the
T
sameshapethatAuffusionwastrainedon. Despitetheimagemodelnotbeingtrainedonthisspecific
size,wefoundthatitneverthelessproducesvisuallyappealingresults. Wesettheclassifierguidance
scales γ and γ to be between 7.5 and 10 and denoise the latents for 100 inference steps with
v a
warm-startparametersoft =1.0,t =0.9topreserveaudiopriors. Wedecodethelatentvariables
a v
intoimagesofdimension3×256×1024. Byaveragingacrosseachchannel,weobtainspectrograms
correspondingto10secondsofaudio. Were-normalizethespectrogramsforvisualization.
Baselines. Asthereisnopreviousworkinthisdomain,weproposetwobaselineapproaches. The
first,inspiredbyDiffusionIllusionsofBurgertetal.[10],usesmultimodalscoredistillationsampling
(SDS).Weoptimizeasingle-channelimagex=g(θ),wheregisanimplicitfunctionparameterized
byθ,usingtwoSDSlosses: onefromtheimagediffusionmodelϕ andtheotherfromtheaudio
v
diffusionmodelϕ . Thisresultsinagradientof:
a
∂x ∂x
∇ L (x=g(θ))=λ E ω (t) ϵ(t)−ϵ +E ω (t) ϵ(t)−ϵ , (7)
θ SDS sds t,ϵ v v ∂θ t,ϵ a a ∂θ
(cid:20) (cid:16) (cid:17) (cid:21) (cid:20) (cid:16) (cid:17) (cid:21)
whereλ istheweightoftheimageSDSgradientandϵisthenoiseaddedtotheimageorlatents.
sds
Weimplementthiswithpixel-baseddiffusionmodelDeepFloydIF[23],aswefinditperformsbetter
thanStableDiffusionwiththeSDSloss,andAuffusion[106]. Thismodelthusdoesnotrequirea
sharedlatentspacebetweenvisionandaudio. WerefertothisbaselineastheSDS.
Thesecondbaselineinvolvestakingexistingimagesandsubtractingthemfromexistingspectrograms,
multipliedbysomescalingfactor,inspiredby[22]. Thisworkswhenthespectrogramshavehigh
power,asthesubtractiondoesnotsignificantlyaffecttheaudiobutstillimprintsanimageintothe
spectrogram.WeobtainspectrogramsandimagesforthisbaselineviaAuffusionandStableDiffusion.
Thisapproach,whichwecallimprint,issimplebutcanbesurprisinglyeffective. Allmethodsusethe
samevocoderandpost-processingforfairness. PleaseseeAppendixA.1formoredetails.
4.2 QuantitativeEvaluation
Westartbyquantitativelyevaluatingthequalityofourgeneratedimagesthatsound,examininghow
wellthegeneratedexamplesmatchtheprovidedtextpromptsforeachmodality.
Experimentalsetup. Toevaluateourapproachandbaselines,wecreatetwosetsoftextprompt
pairs. Werandomlyselect10commonclassesfromVGGSound[15]andusetheclassnamefor
Table 2: Human study. We show win-rates of our spectrograms against those generated by the SDS and
imprintbaselines. Thefirstrowindicateswhichaudiovisualpromptpairisevaluated,formattedas[audio
prompt]/[visual prompt],withthelastcolumnbeingtheaverageofallsevenpromptpairs.Notethat
50%win-rateischanceperformance,andassuchourmethodoutperformsthebaselinesinthevastmajorityof
cases.Allresultsreportedare%win-rateagainstthebaselinewitha95%confidenceintervalingray(N =100).
Baseline Metric bell/castle bark/dog birds/garden meow/kitten racecar/racecar tiger/tiger train/train Average
audioquality 53.1(±4.9) 69.4(±4.2) 95.9(±0.8) 75.5(±3.7) 88.8(±2.0) 70.4(±4.1) 88.8(±2.0) 77.4(±0.7)
SDS visualquality 60.2(±4.7) 51.0(±4.9) 98.0(±0.4) 32.7(±4.4) 69.4(±4.2) 68.4(±4.3) 94.9(±1.0) 67.8(±0.8)
alignment 58.2(±4.8) 63.3(±4.6) 93.9(±1.1) 62.2(±4.7) 82.7(±2.8) 59.2(±4.8) 91.8(±1.5) 73.0(±0.8)
audioquality 82.1(±3.0) 73.7(±3.9) 53.7(±5.0) 54.7(±5.0) 86.3(±2.4) 85.3(±2.5) 85.3(±2.5) 74.4(±0.7)
Imprint visualquality 92.6(±1.4) 86.3(±2.4) 66.3(±4.5) 68.4(±4.3) 66.3(±4.5) 77.9(±3.5) 56.8(±4.9) 73.5(±0.8)
alignment 88.4(±2.1) 87.4(±2.2) 60.0(±4.8) 65.3(±4.6) 86.3(±2.4) 80.0(±3.2) 85.3(±2.5) 78.9(±0.6)
6Ours Imprint baseline SDS
”a painting of castle towers, grayscale” & ”bell ringing”
”a painting of furry kittens, grayscale” & ”a kitten meowing for attention”
”a painting of auto racing game, grayscale” & ”a race car passing by and disappearing”
Figure4:Qualitativecomparison.WeshowourqualitativeresultsalongwiththeimprintandSDSbaselines
givenvisual(first)andaudio(second)prompts.Pleasezoominforbetterviewing.
audio prompts. We randomly select 10 objects and scenes for image prompts, formatted as “a
painting of [class], grayscale”. Thisyieldsatotalof100promptpairs. Wealsoinclude
theperformancesofStableDiffusionandAuffusionassingle-modalitymodelstoestablishupperand
lowerboundsforeachmodality. Wegenerate10samplesforeachpromptpair,exceptfortheSDS
baseline,forwhichwegenerate4samplesduetoitsslowerspeed.
Evaluationmetric. Following[46],weusetheCLIP[85]scoretomeasurethealignmentbetween
spectrogramsandimagetextprompts,andanalogouslyweusetheCLAP[104]scoretoevaluatethe
alignmentofaudiowithaudiotextprompts. Anidealmethodshouldexcelatbothsimultaneously.
Results. We show our quantitative results in Tab. 1. Our method outperforms the baselines in
both metrics and demonstrates comparable performance to single-modality models, which serve
asroughupperboundsforeachmodality. Thisindicatesthatourapproachsuccessfullygenerates
meaningfulimagesthatsound,andcansamplefromtheintersectionofthedistributionofnatural
imagesandthedistributionofnaturalspectrograms. StableDiffusionachievesalowCLAPscore,
indicatinghowpoorlyarandomlysamplednaturalimageactsasaspectrogram. Weobservethat
theSDSbaselineoftenfailstooptimizebothmodalitiestogether,producingeitherspectrogramor
imagecontentonly. Incontrast,ourmethodachievesahighersuccessrateandgeneratesmorediverse
results. Ourmethodissignificantlyfaster,generatingonesamplein10secondscomparedtotheSDS
baseline’s2-houroptimizationtimeusingNVIDIAL40s. Theimprintbaselineimprintstheimage
ontothespectrogram,potentiallydegradingthesoundpatternandleadingtoalowerCLAPscore.
4.3 HumanStudies
Experimentalsetup. Wealsoperformtwo-alternativeforcedchoice(2AFC)studiestoevaluate
ourresults. Weconstructsevenpairedtextpromptsbyhand,ensuringsemanticcorrelationsbetween
imageandaudioprompts,suchaspairingavisualofdogswiththesoundofdogsbarking.Usingthese
prompts,wegeneratesamplesusingourmethod,theSDSbaseline,andtheimprintbaseline,and
hand-pickthebestexamplesforevaluation. Participants,recruitedusingMTurk,arethenpresented
withonesamplefromourmethod,andacorrespondingsamplefromabaseline,andareaskedto
choose(1)thesamplethatlooksmostliketheimageprompt,(2)thesamplethatsoundsmostlikethe
audioprompt,and(3)thesampleinwhichtheaudiocontentbestalignswiththevisualcontentover
7”a pond full of water lilies, grayscale,
”a painting of corgis, grayscale, black background” lithograph style”
”dog barking” ”frog croaking”
”a colorful photo of corgis” ”a colorful photo of a water-lily pond”
”a painting of auto racing game, grayscale” ”a painting of a blooming garden, grayscale”
”a race car passing by and disappearing” ”birds singing sweetly”
”a colorful photo of an auto racing game” ”a colorful photo of a blooming garden”
Time
Figure5: Qualitativeexampleswithcolorizationresults. Wepresent4examplesalongsidetheirimage
prompts,audioprompts,andcolorizationprompts.Pleaserefertoourwebsiteforvideoresults.
time. ThefirsttwoquestionsactasperceptualversionsofCLIPandCLAPscores. Thethirdquestion
isdesignedtoevaluatehowwelltheaudiomatcheswiththevisualcuesasthespectrogramisplayed.
PleaseseeAppendixA.2forfurtherdetailsanddiscussion.
Results. Win-ratesbetweenourmethodandbaselinesarepresentedinTab.2, brokendownby
promptpair. Wealsoincludeaveragedwin-ratesoverallpromptpairsinthefinalcolumn. Ascan
be seen, our method outperforms the two baselines in the majority of cases. Human evaluators
consistentlyrateourspectrogramsasbeinghigherinaudioandvisualquality,andasbeingbetter
“synced”;onaverageourmethodis2-3timesaslikelytobechosenasthebettersamplethanbaselines.
4.4 QualitativeResults
Results. WepresentqualitativeresultsfromourmethodaswellasbaselinesinFig.4,withadditional
resultsfromourmethodavailableinFigs.5,8and9intheappendix. Audioofallresultscanbe
foundonourwebsite. Ascanbeseen(andheard)ourapproachgeneratesmorevisuallyappealing
sampleswithbettersoundqualitythancomparedtothebaselines. TheSDSbaselineoftenfocuseson
onemodality,tothedetrimentoftheother,andingeneral,generatesaudiooflowerquality. Moreover,
themethodsuffersfromthecharacteristicoversaturationofSDS-basedresults. Theperformance
oftheimprintbaselineishighlydependentontheindependentlygeneratedspectrogramandimage
andtendstofailwhenthetwoaremisaligned,asinthecastleexample,orwhenthespectrogram
alreadyhaslowenergy,asthesubtractedimageishardtoseeinalreadylowenergyregionsofthe
spectrogram,suchaswiththekittenexample. Interestingly,wefoundthatourmethodoftencombines
visualandacousticelements. Forexample,theonsetsofthebellsringinginFig.4coincidewiththe
towersofthecastle,andthespectrogrampatternsofmeowingarehiddenasstripesandedgesonthe
kittens.
Weshowadditionalhand-pickedresultsfromourapproachinFig.5withcolorizationresults, in
whichwecanseemoreexamplesofourmethodblendingacousticandvisualelements,suchasthe
8
margortcepS
mrofevaW
egami
roloC
margortcepS
mrofevaW
egami
roloC
ycneuqerF
ycneuqerFTable3:AblationsWeconducttheablationstudyoftheguidancescale(left)andthewarm-starting(right).
Method Variation CLIP(%)↑ CLAP(%)↑ Method tv ta CLIP(%)↑ CLAP(%)↑
1.0 1.0 28.7 30.8
γv,γa=5.0 28.0 29.3
1.0 0.9 29.0 27.4
Ours γv,γa=7.5 28.2 31.9 Ours
0.9 1.0 28.2 33.5
γv,γa=10 28.2 33.5
0.8 1.0 27.4 35.9
waterliliescorrespondingtothefrogscroaking,thecorgiscorrespondingtothedogsbarking,andthe
flowerscorrespondingtothebirdschirping. PleaseseemoreresultsinFig.8ofAppendixA.2.
Multimodalcompositionality. Priorwork[65,26,7,38,37]showsthatdiffusionmodelsmaybe
“composed”togeneratesamplesthatarelikelyundertwoormoredifferentprobabilitydistributions.
Ourmethodcanbeseenasextendingthisideaofcompositionalitytomultiplemodalities. Onthe
faceofit,thedistributionofspectrogramsandthedistributionofnaturalimageswouldseemtobe
completelydisjoint. However,asourresultsshow,perhapssurprisingly,thereissomeoverlap. We
believethatthisispossiblefortworeasons. First,spectrogramsandimagesarebothfairlyflexible,
allowingforsignificantamountsofperturbationorchangesinstylebeforebecomingunrecognizable.
Andsecond,imagesandspectrogramssharecertainlow-levelcharacteristics,suchasedges,curves,
andcorners,indicatingacertainamountofsimilarity.
However,wefindthatnotallcompositionscanbesuccessfulasshowninFig.9ofAppendixA.2.
Moreover,carefulselectionofpromptsiscrucialtocreatinggoodresults. Forexample,incorporating
termssuchas“lithograph style”or“black background”encouragesthevisualmodel
tocreateareasofsilence,whichresultsinbetterqualityaudio,asshowninFig.5.
4.5 Ablations
Vocoder. ToextractwaveformsfromourgeneratedspectrogramsweuseHiFi-GAN[59],aneural
vocoder. Giventhatourspectrogramsareincrediblyoutofdistribution,oneconcernwiththissetupis
thatourmodelhassimplyfoundadversarialexamplesagainstthisvocoder.Toamelioratethisconcern,
weconductacycleconsistencycheckbyre-encodingtheneuralvocoder’spredictedwaveformback
intoaspectrogrambyperforminganSTFT.AscanbeseeninFig.6,therecomputedspectrogramsare
verysimilartotheoriginalspectrogram,withonlyslightlylesssharpnessandsomeblurredtextures5.
Thissuggeststhatoursamplesarenotsimplyadversarialexamples,butrathertrulyspectrogramsthat
looklikeimages. WealsoexperimentwithusingGriffin-Lim[82]asavocoder,withsimilarresults
toHiFi-GANasshowninFig.6. WeopttouseHiFi-GANasourdefaultvocoderasitoutperforms
Griffin-Liminaudioquality,withGriffin-LimattainingaCLAPscoreof0.302,comparedto0.335
obtainedfromHiFi-GAN.PleaseseemoreresultsinFig.7ofAppendixA.2.
Warm-starting. Wealsoconductanablationstudyonourwarm-startingstrategybyvaryingwhich
modalityiswarm-startedandbyhowmanysteps. ResultsarepresentedinTab.3,wheret andt
a v
aredefinedinSec.3.2. Wefindthatwarm-startingthedenoisingprocesswitheitherimageoraudio
diffusionyieldshigherscoresinthecorrespondingmodality,asthatmodalityeffectivelygetsfree
reigntosetthehigh-levelfeaturesofthefinalresult. Wefindthatallowingtheaudiodiffusionmodel
todenoisealoneforthefirst10%ofthetimestepsresultsinanattractivebalancebetweenCLIPand
CLAPscores. Therefore,weadoptt =0.9andt =1.0forourmainexperiments.
v a
Guidancescale. Wealsoexploredifferentguidancescalesγ andγ forourmethod. Wepresent
v a
resultsinTab.3. Wefindthathigherguidancescalesgenerallyyieldbetterresultsonbothmodalities.
Wehypothesizethatthehigherguidancescalesmorestronglyencouragethesampletocomefrom
the“intersection”oftheconditionalspectrogramandconditionalimagedistributions,resultingin
betteralignmentwithbothtextprompts.
5 DiscussionandConclusion
In this work, we demonstrate that, perhaps surprisingly, there is non-trivial overlap between the
distributionofnaturalimagesandthedistributionofnaturalspectrograms. Weshowthisbysampling
fromtheintersectionofthesetwodistributions,resultinginspectrogramsthatlooklikerealimagesbut
5Wenotethatperfectcycleconsistencyisnotgenerallypossiblesincevocodersarefundamentallylossy.
9ORIGINAL HIFI-GAN GRIFFIN-LIM
Figure6:Cycleconsistencycheckonthevocoders.Weshowtheoriginallogmel-spectrogramdecodedfrom
latentsandlogmel-spectrogramsobtainedfromwaveformssynthesizedbyHiFi-GANorGriffin-Lim.
alsosoundlikerealsounds. Themethodweproposetodothisissimpleandzero-shot,andleverages
the compositional nature of diffusion with cross-modal models. We see our work as advancing
multimodalcompositionalgenerationandopeningupnewpossibilitiesforartisticexpressionthrough
multimodalart.
Limitations. One limitation of our method is that it cannot generate examples that have both
high-fidelityaudioandimage. Weshowfailurecases,whichoccurformanyprompts,inFig.9. Some
ofthesefailuresmaybeduetothestrictconstraintsoftheproblem,sincerealisticexamplesmaynot
alwaysexistattheintersectionofbothdistributions. Ourmethodisalsolimitedbythequalityofthe
audiodiffusionmodel,whoseperformancelagsbehindthatofvisualmodels.
Potential negative societal impacts. The image and audio generation models that our method
leveragesarebecomingprogressivelymorepowerful,andcaremustbetakenintheirdeployment.
Moreover,ourmethodcouldpotentiallybeusedforsteganography,secretlyembeddingimageswithin
audio. Thiscapabilitymaybeusedfordeception,andwebelieveitdeservesfurtherconsideration.
Acknowledgements. WethankAngCao,LinyiJin,JeongsooPark,ChrisDonahue,AlexeiEfros,
PremSeetharaman,JustinSalamon,JulieZhu,andJohnGranzowfortheirhelpfuldiscussions. This
projectissupportedbytheSonyResearchAwardandCiscoSystems. Danielissupportedbythe
NationalScienceFoundationGraduateResearchFellowshipunderGrantNo. 1841052.
References
[1] T.Afouras,A.Owens,J.S.Chung,andA.Zisserman. Self-supervisedlearningofaudio-visualobjects
fromvideo. EuropeanConferenceonComputerVision(ECCV),2020. 3
[2] AphexTwin. Formula,1994. audiotrack. 2,3
[3] R.ArandjelovicandA.Zisserman. Look,listenandlearn. InProceedingsoftheIEEEinternational
conferenceoncomputervision,pages609–617,2017. 3
[4] R.ArandjelovicandA.Zisserman. Objectsthatsound. InProceedingsoftheEuropeanconferenceon
computervision(ECCV),pages435–451,2018. 3
[5] Y.Asano,M.Patrick,C.Rupprecht,andA.Vedaldi. Labellingunlabelledvideosfromscratchwith
multi-modalself-supervision. AdvancesinNeuralInformationProcessingSystems,33:4660–4671,2020.
3
[6] O.Bar-Tal,H.Chefer,O.Tov,C.Herrmann,R.Paiss,S.Zada,A.Ephrat,J.Hur,Y.Li,T.Michaeli,etal.
Lumiere:Aspace-timediffusionmodelforvideogeneration. arXivpreprintarXiv:2401.12945,2024. 3
[7] O.Bar-Tal,L.Yariv,Y.Lipman,andT.Dekel. Multidiffusion: Fusingdiffusionpathsforcontrolled
imagegeneration. arXivpreprintarXiv:2302.08113,2023. 3,9
[8] T.Brooks,A.Holynski,andA.A.Efros. Instructpix2pix:Learningtofollowimageeditinginstructions.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages18392–
18402,2023. 3
[9] B.Buckle.Spectrogramart:Ashorthistoryofmusicianshidingvisualsinsidetheirtracks.Availablefrom:
https://mixmag.net/feature/spectrogram-art-music-aphex-twin, 2022. Mix-
magarticle. 2
[10] R.Burgert,X.Li,A.Leite,K.Ranasinghe,andM.S.Ryoo. Diffusionillusions:Hidingimagesinplain
sight. arXivpreprintarXiv:2312.03817,2023. 3,6,16
[11] C.Chen,R.Gao,P.Calamia,andK.Grauman. Visualacousticmatching. InConferenceonComputer
VisionandPatternRecognition(CVPR),2022. 3
10[12] C.Chen,A.Richard,R.Shapovalov,V.K.Ithapu,N.Neverova,K.Grauman,andA.Vedaldi. Novel-
viewacousticsynthesis. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages6409–6419,2023. 3
[13] G.Chen,Y.Wu,S.Liu,T.Liu,X.Du,andF.Wei. Wavmark:Watermarkingforaudiogeneration. arXiv
preprintarXiv:2308.12770,2023. 3
[14] H.Chen,W.Xie,T.Afouras,A.Nagrani,A.Vedaldi,andA.Zisserman. Audio-visualsynchronisationin
thewild. arXivpreprintarXiv:2112.04432,2021. 3
[15] H.Chen,W.Xie,A.Vedaldi,andA.Zisserman.Vggsound:Alarge-scaleaudio-visualdataset.InICASSP
2020-2020IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages
721–725.IEEE,2020. 6
[16] M.Chen,K.Su,andE.Shlizerman. Beeverywhere-heareverything(bee):Audioscenereconstructionby
sparseaudio-visualsamples. InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision,pages7853–7862,2023. 3
[17] Z.Chen,I.D.Gebru,C.Richardt,A.Kumar,W.Laney,A.Owens,andA.Richard. Realacousticfields:
Anaudio-visualroomacousticsdatasetandbenchmark. InTheIEEE/CVFComputerVisionandPattern
RecognitionConference(CVPR),2024. 3
[18] Z.Chen,X.Hu,andA.Owens. Structurefromsilence:Learningscenestructurefromambientsound. In
5thAnnualConferenceonRobotLearning,2021. 3
[19] Z.Chen,S.Qian,andA.Owens. Soundlocalizationfrommotion:Jointlylearningsounddirectionand
camerarotation. InInternationalConferenceonComputerVision(ICCV),2023. 3
[20] J.Choi,S.Kim,Y.Jeong,Y.Gwon,andS.Yoon. Ilvr: Conditioningmethodfordenoisingdiffusion
probabilisticmodels. arXivpreprintarXiv:2108.02938,2021. 5,16
[21] S.Chowdhury,S.Ghosh,S.Dasgupta,A.Ratnarajah,U.Tyagi,andD.Manocha. Adverb: Visually
guidedaudiodereverberation. InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision,pages7884–7896,2023. 3
[22] ClassicalMusicReimagined. Funwithspectrograms! howtomakeanimageusingsoundandmusic.
Availablefrom:https://www.youtube.com/watch?v=N2DQFfID6eY,2017. Youtubevideo.
6
[23] DeepFloydLabatStabilityAI. DeepFloydIF:anovelstate-of-the-artopen-sourcetext-to-imagemodel
withahighdegreeofphotorealismandlanguageunderstanding. https://www.deepfloyd.ai/
deepfloyd-if,2023. 3,6,16
[24] P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. Advances in neural
informationprocessingsystems,34:8780–8794,2021. 3
[25] Y.Du,Z.Chen,J.Salamon,B.Russell,andA.Owens. Conditionalgenerationofaudiofromvideovia
foleyanalogies. ComputerVisionandPatternRecognition(CVPR),2023. 3
[26] Y.Du,C.Durkan,R.Strudel,J.B.Tenenbaum,S.Dieleman,R.Fergus,J.Sohl-Dickstein,A.Doucet,and
W.S.Grathwohl. Reduce,reuse,recycle:Compositionalgenerationwithenergy-baseddiffusionmodels
andmcmc. InInternationalConferenceonMachineLearning,pages8489–8510.PMLR,2023. 2,3,9
[27] Y.Du,S.Li,andI.Mordatch. Compositionalvisualgenerationwithenergybasedmodels. Advancesin
NeuralInformationProcessingSystems,33:6637–6647,2020. 4
[28] D.Epstein,A.Jabri,B.Poole,A.Efros,andA.Holynski. Diffusionself-guidanceforcontrollableimage
generation. AdvancesinNeuralInformationProcessingSystems,36:16222–16239,2023. 3
[29] Z.Evans,C.Carr,J.Taylor,S.H.Hawley,andJ.Pons. Fasttiming-conditionedlatentaudiodiffusion.
arXivpreprintarXiv:2402.04825,2024. 3
[30] C.Feng,Z.Chen,andA.Owens. Self-supervisedvideoforensicsbyaudio-visualanomalydetection.
ComputerVisionandPatternRecognition(CVPR),2023. 3
[31] S.ForsgrenandH.Martiros. Riffusion-Stablediffusionforreal-timemusicgeneration,2022. 2,3
[32] R.Gao,C.Chen,Z.Al-Halah,C.Schissler,andK.Grauman. Visualechoes:Spatialvisualrepresentation
learningthroughecholocation. InEuropeanConferenceonComputerVision(ECCV),2020. 3
[33] R.GaoandK.Grauman. 2.5dvisualsound. InConferenceonComputerVisionandPatternRecognition
(CVPR),2019. 3
[34] R.GaoandK.Grauman. Visualvoice:Audio-visualspeechseparationwithcross-modalconsistency. In
ConferenceonComputerVisionandPatternRecognition(CVPR),2021. 3
[35] R.Gao,T.-H.Oh,K.Grauman,andL.Torresani. Listentolook: Actionrecognitionbypreviewing
audio. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages
10457–10467,2020. 3
11[36] D.GengandA.Owens. Motionguidance: Diffusion-basedimageeditingwithdifferentiablemotion
estimators. arXivpreprintarXiv:2401.18085,2024. 3
[37] D.Geng,I.Park,andA.Owens. Factorizeddiffusion: Perceptualillusionsbynoisedecomposition.
arXiv:2404.11615,April2024. 2,3,5,9,16
[38] D.Geng,I.Park,andA.Owens. Visualanagrams:Generatingmulti-viewopticalillusionswithdiffusion
models. InCVPR,2024. 2,3,9
[39] D.Ghosal,N.Majumder,A.Mehrish,andS.Poria. Text-to-audiogenerationusinginstructiontunedllm
andlatentdiffusionmodel. arXivpreprintarXiv:2304.13731,2023. 3
[40] R.Girdhar,A.El-Nouby,Z.Liu,M.Singh,K.V.Alwala,A.Joulin,andI.Misra. Imagebind: One
embeddingspacetobindthemall. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages15180–15190,2023. 3
[41] R.Girdhar,M.Singh,A.Brown,Q.Duval,S.Azadi,S.S.Rambhatla,A.Shah,X.Yin,D.Parikh,and
I.Misra. Emuvideo:Factorizingtext-to-videogenerationbyexplicitimageconditioning. arXivpreprint
arXiv:2311.10709,2023. 3
[42] Y.Gong,A.Rouditchenko,A.H.Liu,D.Harwath,L.Karlinsky,H.Kuehne,andJ.Glass. Contrastive
audio-visualmaskedautoencoder. arXivpreprintarXiv:2210.07839,2022. 3
[43] D.GriffinandJ.Lim. Signalestimationfrommodifiedshort-timefouriertransform. IEEETransactions
onacoustics,speech,andsignalprocessing,32(2):236–243,1984. 4,5
[44] G.GwardysandD.Grzywczak. Deepimagefeaturesinmusicinformationretrieval. International
JournalofElectronicsandTelecommunications,60:321–326,2014. 2
[45] A.Hertz,R.Mokady,J.Tenenbaum,K.Aberman,Y.Pritch,andD.Cohen-Or. Prompt-to-promptimage
editingwithcrossattentioncontrol. arXivpreprintarXiv:2208.01626,2022. 3
[46] J.Hessel,A.Holtzman,M.Forbes,R.L.Bras,andY.Choi.Clipscore:Areference-freeevaluationmetric
forimagecaptioning. arXivpreprintarXiv:2104.08718,2021. 7
[47] G.E.Hinton. Trainingproductsofexpertsbyminimizingcontrastivedivergence. Neuralcomputation,
2002. 4
[48] J.Ho, W.Chan, C.Saharia, J.Whang, R.Gao, A.Gritsenko, D.P.Kingma, B.Poole, M.Norouzi,
D.J.Fleet,etal. Imagenvideo:Highdefinitionvideogenerationwithdiffusionmodels. arXivpreprint
arXiv:2210.02303,2022. 3
[49] J.Ho,A.Jain,andP.Abbeel. Denoisingdiffusionprobabilisticmodels. Advancesinneuralinformation
processingsystems,33:6840–6851,2020. 3,4
[50] J.HoandT.Salimans. Classifier-freediffusionguidance. arXivpreprintarXiv:2207.12598,2022. 4
[51] J.Ho,T.Salimans,A.Gritsenko,W.Chan,M.Norouzi,andD.J.Fleet.Videodiffusionmodels.Advances
inNeuralInformationProcessingSystems,35:8633–8646,2022. 3
[52] X.Hu,Z.Chen,andA.Owens. Mixandlocalize: Localizingsoundsourcesinmixtures. Computer
VisionandPatternRecognition(CVPR),2022. 3
[53] C.Huang,Y.Tian,A.Kumar,andC.Xu. Egocentricaudio-visualobjectlocalization. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages22910–22921,2023. 3
[54] J.Huh,J.Chalk,E.Kazakos,D.Damen,andA.Zisserman. Epic-sounds:Alarge-scaledatasetofactions
thatsound. InICASSP2023-2023IEEEInternationalConferenceonAcoustics, SpeechandSignal
Processing(ICASSP),pages1–5.IEEE,2023. 3
[55] V.IashinandE.Rahtu. Tamingvisuallyguidedsoundgeneration. arXivpreprintarXiv:2110.08791,
2021. 3
[56] V.Iashin,W.Xie,E.Rahtu,andA.Zisserman. Synchformer:Efficientsynchronizationfromsparsecues.
arXivpreprintarXiv:2401.16423,2024. 3
[57] B.Kawar,M.Elad,S.Ermon,andJ.Song. Denoisingdiffusionrestorationmodels. AdvancesinNeural
InformationProcessingSystems,35:23593–23606,2022. 5,16
[58] E.Kidron,Y.Y.Schechner,andM.Elad. Pixelsthatsound. In2005IEEEComputerSocietyConference
onComputerVisionandPatternRecognition(CVPR’05),volume1,pages88–95.IEEE,2005. 3
[59] J.Kong,J.Kim,andJ.Bae. Hifi-gan: Generativeadversarialnetworksforefficientandhighfidelity
speechsynthesis. Advancesinneuralinformationprocessingsystems,33:17022–17033,2020. 5,9
[60] S.Liang, C. Huang, Y. Tian, A. Kumar, andC. Xu. Av-nerf: Learningneuralfieldsfor real-world
audio-visualscenesynthesis. AdvancesinNeuralInformationProcessingSystems,36,2024. 3
[61] Y.-B.LinandG.Bertasius.Siamesevisiontransformersarescalableaudio-visuallearners.arXivpreprint
arXiv:2403.19638,2024. 2,3
12[62] Y.-B.Lin,Y.-L.Sung,J.Lei,M.Bansal,andG.Bertasius. Visiontransformersareparameter-efficient
audio-visuallearners. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages2299–2309,2023. 2,3
[63] H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang, and M. D. Plumbley. Audioldm:
Text-to-audiogenerationwithlatentdiffusionmodels. arXivpreprintarXiv:2301.12503,2023. 3
[64] H.Liu,Q.Tian,Y.Yuan,X.Liu,X.Mei,Q.Kong,Y.Wang,W.Wang,Y.Wang,andM.D.Plumb-
ley. Audioldm2: Learningholisticaudiogenerationwithself-supervisedpretraining. arXivpreprint
arXiv:2308.05734,2023. 3
[65] N.Liu,S.Li,Y.Du,A.Torralba,andJ.B.Tenenbaum.Compositionalvisualgenerationwithcomposable
diffusionmodels. InEuropeanConferenceonComputerVision,pages423–439.Springer,2022. 2,3,9
[66] S.LuoandW.Hu. Diffusionprobabilisticmodelsfor3dpointcloudgeneration. InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pages2837–2845,2021. 3
[67] T.Mahmud,Y.Tian,andD.Marculescu. T-vsl:Text-guidedvisualsoundsourcelocalizationinmixtures.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,2024. 3
[68] S.Majumder,Z.Al-Halah,andK.Grauman. Learningspatialfeaturesfromaudio-visualcorrespondence
inegocentricvideos. arXivpreprintarXiv:2307.04760,2023. 3
[69] X.Mei,V.Nagaraja,G.L.Lan,Z.Ni,E.Chang,Y.Shi,andV.Chandra. Foleygen: Visually-guided
audiogeneration. arXivpreprintarXiv:2309.10537,2023. 3
[70] C.Meng,Y.He,Y.Song,J.Song,J.Wu,J.-Y.Zhu,andS.Ermon. Sdedit:Guidedimagesynthesisand
editingwithstochasticdifferentialequations. arXivpreprintarXiv:2108.01073,2021. 3
[71] H.Mittal,P.Morgado,U.Jain,andA.Gupta. Learningstate-awarevisualrepresentationsfromaudible
interactions. AdvancesinNeuralInformationProcessingSystems,35:23765–23779,2022. 3
[72] S.MoandP.Morgado. Localizingvisualsoundstheeasyway. InEuropeanConferenceonComputer
Vision,pages218–234.Springer,2022. 3
[73] P.Morgado,N.Nvasconcelos,T.Langlois,andO.Wang. Self-supervisedgenerationofspatialaudiofor
360video. Advancesinneuralinformationprocessingsystems,31,2018. 3
[74] P. Morgado, N. Vasconcelos, and I. Misra. Audio-visual instance discrimination with cross-modal
agreement. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,
pages12475–12486,2021. 3
[75] A.Nichol,P.Dhariwal,A.Ramesh,P.Shyam,P.Mishkin,B.McGrew,I.Sutskever,andM.Chen. Glide:
Towardsphotorealisticimagegenerationandeditingwithtext-guideddiffusionmodels,2021. 3
[76] NineInchNails. Yearzero,2007. MusicAlbum. 2
[77] M.A.Nugroho,S.Woo,S.Lee,andC.Kim. Audio-visualglancenetworkforefficientvideorecognition.
InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages10150–10159,
2023. 3
[78] A.OwensandA.A.Efros. Audio-visualsceneanalysiswithself-supervisedmultisensoryfeatures. In
ProceedingsoftheEuropeanconferenceoncomputervision(ECCV),pages631–648,2018. 3
[79] N.Oxman. Sympawnies:animalportraitsmadeofmusicalnotations. Availablefrom:https://www.
youtube.com/@Sympawnies,2023. YoutubeChannel. 3
[80] K.Palanisamy,D.Singhania,andA.Yao. Rethinkingcnnmodelsforaudioclassification. arXivpreprint
arXiv:2007.11154,2020. 2
[81] S.Park,A.Senocak,andJ.S.Chung. Cancliphelpsoundsourcelocalization? InProceedingsofthe
IEEE/CVFWinterConferenceonApplicationsofComputerVision,pages5711–5720,2024. 3
[82] N.Perraudin,P.Balazs,andP.L.Søndergaard. Afastgriffin-limalgorithm. In2013IEEEworkshopon
applicationsofsignalprocessingtoaudioandacoustics,pages1–4.IEEE,2013. 5,9
[83] B.Poole,A.Jain,J.T.Barron,andB.Mildenhall. Dreamfusion:Text-to-3dusing2ddiffusion. ICLR,
2023. 3
[84] C.Qi,X.Cun,Y.Zhang,C.Lei,X.Wang,Y.Shan,andQ.Chen.Fatezero:Fusingattentionsforzero-shot
text-basedvideoediting. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,
pages15932–15942,2023. 3
[85] A.Radford,J.W.Kim,C.Hallacy,A.Ramesh,G.Goh,S.Agarwal,G.Sastry,A.Askell,P.Mishkin,
J.Clark,etal. Learningtransferablevisualmodelsfromnaturallanguagesupervision. InInternational
conferenceonmachinelearning,pages8748–8763.PMLR,2021. 7
[86] R.Rombach,A.Blattmann,D.Lorenz,P.Esser,andB.Ommer. High-resolutionimagesynthesiswith
latentdiffusionmodels. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pages10684–10695,2022. 2,3,4,5,6
13[87] C.Saharia,W.Chan,H.Chang,C.Lee,J.Ho,T.Salimans,D.Fleet,andM.Norouzi. Palette:Image-to-
imagediffusionmodels. InACMSIGGRAPH2022conferenceproceedings,pages1–10,2022. 3
[88] C.Saharia,W.Chan,S.Saxena,L.Li,J.Whang,E.Denton,S.K.S.Ghasemipour,B.K.Ayan,S.S.
Mahdavi,R.G.Lopes,T.Salimans,J.Ho,D.J.Fleet,andM.Norouzi. Photorealistictext-to-image
diffusionmodelswithdeeplanguageunderstanding,2022. 3
[89] A.Senocak,H.Ryu,J.Kim,T.-H.Oh,H.Pfister,andJ.S.Chung. Soundsourcelocalizationisallabout
cross-modalalignment. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,
pages7777–7787,2023. 3
[90] U.Singer,A.Polyak,T.Hayes,X.Yin,J.An,S.Zhang,Q.Hu,H.Yang,O.Ashual,O.Gafni,etal.
Make-a-video:Text-to-videogenerationwithouttext-videodata. arXivpreprintarXiv:2209.14792,2022.
3
[91] J.Sohl-Dickstein,E.Weiss,N.Maheswaranathan,andS.Ganguli. Deepunsupervisedlearningusing
nonequilibriumthermodynamics. InF.BachandD.Blei,editors,Proceedingsofthe32ndInternational
ConferenceonMachineLearning, volume37ofProceedingsofMachineLearningResearch, pages
2256–2265,Lille,France,07–09Jul2015.PMLR. 3
[92] A.Somayazulu,C.Chen,andK.Grauman.Self-supervisedvisualacousticmatching.AdvancesinNeural
InformationProcessingSystems,36,2024. 3
[93] J.Song,C.Meng,andS.Ermon. Denoisingdiffusionimplicitmodels. arXivpreprintarXiv:2010.02502,
2020. 3,4
[94] Y.SongandS.Ermon. Generativemodelingbyestimatinggradientsofthedatadistribution. Advancesin
neuralinformationprocessingsystems,32,2019. 3
[95] Y.Song,J.Sohl-Dickstein,D.P.Kingma,A.Kumar,S.Ermon,andB.Poole. Score-basedgenerative
modelingthroughstochasticdifferentialequations. arXivpreprintarXiv:2011.13456,2020. 3,4,5,16
[96] J.Sun,L.Deng,T.Afouras,A.Owens,andA.Davis. Eventfulnessforinteractivevideoalignment. ACM
TransactionsonGraphics(TOG),42(4):1–10,2023. 3
[97] K. Sung-Bin, A. Senocak, H. Ha, A. Owens, and T.-H. Oh. Sound to visual scene generation by
audio-to-visuallatentalignment. ComputerVisionandPatternRecognition(CVPR),2023. 3
[98] M.Tancik,P.Srinivasan,B.Mildenhall,S.Fridovich-Keil,N.Raghavan,U.Singhal,R.Ramamoorthi,
J.Barron,andR.Ng. Fourierfeaturesletnetworkslearnhighfrequencyfunctionsinlowdimensional
domains. Advancesinneuralinformationprocessingsystems,33:7537–7547,2020. 16
[99] TheBeatles. Lucyintheskywithdiamonds,1967. 3
[100] Tool. 10,000days,2006. VolcanoEntertainment. 3
[101] D.Ulyanov. Audiotexturesynthesisandstyletransfer. https://dmitryulyanov.github.io/
audio-texture-synthesis-and-style-transfer,2016. 2
[102] Y.Wang,J.Yu,andJ.Zhang. Zero-shotimagerestorationusingdenoisingdiffusionnull-spacemodel.
TheEleventhInternationalConferenceonLearningRepresentations,2023. 5,16
[103] J.Z.Wu,Y.Ge,X.Wang,S.W.Lei,Y.Gu,Y.Shi,W.Hsu,Y.Shan,X.Qie,andM.Z.Shou. Tune-a-
video: One-shottuningofimagediffusionmodelsfortext-to-videogeneration. InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision,pages7623–7633,2023. 3
[104] Y.Wu,K.Chen,T.Zhang,Y.Hui,T.Berg-Kirkpatrick,andS.Dubnov. Large-scalecontrastivelanguage-
audiopretrainingwithfeaturefusionandkeyword-to-captionaugmentation. InICASSP2023-2023IEEE
InternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages1–5.IEEE,2023.
7
[105] Z.Xie,S.Yu,M.Li,Q.He,C.Chen,andY.-G.Jiang.Sonicvisionlm:Playingsoundwithvisionlanguage
models. arXivpreprintarXiv:2401.04394,2024. 3
[106] J.Xue,Y.Deng,Y.Gao,andY.Li. Auffusion: Leveragingthepowerofdiffusionandlargelanguage
modelsfortext-to-audiogeneration. arXivpreprintarXiv:2401.01044,2024. 2,3,5,6,16
[107] H.Yang,H.Ouyang,V.Koltun,andQ.Chen. Hidingvideoinaudioviareversiblegenerativemodels. In
ProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,pages1100–1109,2019. 3
[108] K.Yang,B.Russell,andJ.Salamon. Tellingleftfromright:Learningspatialcorrespondenceofsightand
sound. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages
9932–9941,2020. 3
[109] M.Yang,K.Cho,A.Merchant,P.Abbeel,D.Schuurmans,I.Mordatch,andE.D.Cubuk. Scalable
diffusionformaterialsgeneration. arXivpreprintarXiv:2311.09235,2023. 3
[110] J. Y. Zhang, A. Lin, M. Kumar, T.-H. Yang, D. Ramanan, and S. Tulsiani. Cameras as rays: Pose
estimationviaraydiffusion. InInternationalConferenceonLearningRepresentations(ICLR),2024. 3
14[111] H.Zhao,C.Gan,W.-C.Ma,andA.Torralba. Thesoundofmotions. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,pages1735–1744,2019. 3
[112] L.Zhao,H.Li,X.Ning,andX.Jiang. Thinimg:Cross-modalsteganographyforpresentingtalkingheads
inimages. InProceedingsoftheIEEE/CVFWinterConferenceonApplicationsofComputerVision,
pages5553–5562,2024. 3
15A.1 ImplementationDetails
Colorization. We use DeepFloyd IF6 [23] following Factorized Diffusion [37] for colorizing
spectrograms. Thistechniquecolorizesagrayscaleimagebyusingapretraineddiffusionmodel
zero-shottogeneratethecolorcomponent,andissimilartopriorworksuchas[57,20,95,102]. We
useitduetoitssimplicity. Wecolorizespectrogramsofsize1×256×1024bydirectlyfeeding
theseintothediffusionmodel,whichwefoundproducedreasonableresultsdespitethefactthatthe
modelwasnottrainedforthissize. Weusepromptsoftheform“a colorful photo of [image
prompt]”anddenoisefor30stepswithaguidancescaleof10. Additionally,wefoundthatstarting
thedenoisingatstep7of30gavebetterresults,whichwehypothesizeworksbecauseitgivesthe
modelastrongerpriorforwhatthestructureoftheimageisthanstartingfrompurenoise.
SDSbaseline. WefollowDiffusionIllusions[10]toimplementourSDSbaselinewithanimplicit
image representation. We use Fourier Features Networks [98] with a learnable MLP to generate
imagesofsize1×256×1024. WeusestageIofDeepFloydIF-Mtoperformimagescoredistillation
sampling. We randomly make eight overlapping 256×256 crops and resize them to 64×64 to
computetheaveragedimageSDSlosswithaguidancescaleof80. Fortheaudiomodality,weuse
Auffusion[106]. AsAuffusionisalatentdiffusionmodel,weencodetheimagesinto4×32×128
latentsandperformtheaudioSDSlosswithaguidancescaleof10,whichwefoundgavethebest
performanceintheaudio-onlygeneration. WesettheweightoftheimageSDSlossλ to0.4to
sds
ensurebalancedoptimizationforbothmodalities. WeusetheAdamWoptimizerwithalearningrate
of10−4andweightdecayof10−3,andoptimizetheFourierFeatureNetworkfor40,000steps. We
alsoapplythewarm-startstrategytothismethodbyoptimizingtheaudioSDSlossonlyforthefirst
5,000stepsbysettingλ tozero. Wenotethismethoddoesnotrequireasharedlatentcodebook
sds
betweenimageandaudiodiffusionmodels.
Imprintbaseline. Webeginbygeneratingimages,
Algorithm1PseudocodeinaPyTorch-likestyle
x , and spectrograms, x , of size 256 × 1024
img spec fortheimprintbaseline.
using Stable diffusion and Auffusion, respectively,
both with a guidance scale of 7.5. Next, we use # get images and specs from LDMs
thegeneratedimagesasmasksbyconvertingthem img = stable_diffusion(text_v)
spec = auffusion(text_a)
into inverse grayscale images and scaling them by # reduce the energy give image masks
a factor ρ. This mask is then applied to the gener- mask = 1 - rho * (1.0 - img.mean(0))
spec = mask * spec
atedspectrogramtoobtainthefinalresult,givenby audio = vocoder(spec)
x (1−ρgray(1−x )). Thehyperparameterρ
spec img
controlsthestrengthofenergyreduction: largervaluesyieldclearervisualpatternsbutpooreraudio
quality, andviceversa. Tostrikeagoodbalance, wesetρ = 0.5. Theimprintbaselinetakes10
secondstogenerateasampleonNVIDIAL40s.
Prompts. WepresenttheimageandaudiopromptsusedforthequantitativeevaluationinTab.4.
Weuse10promptsforeachmodality,foratotalof100promptpairs.
Table4:Textpromptsforthequantitativeevaluation.
Imageprompts Audioprompts
apaintingofcastles,grayscale dogbarking
apaintingofdogs,grayscale catmeowing
apaintingofkittens,grayscale birdchirping,tweeting
apaintingoftigers,grayscale tigergrowling
apaintingofautoracinggame,grayscale churchbellringing
apaintingofmountains,grayscale racecar,autoracing
apaintingofagarden,grayscale trainwhistling
apaintingofaforest,grayscale fireworksbanging
apaintingofafarm,grayscale peoplecheering
apaintingofabeach,grayscale playingacousticguitar
6https://huggingface.co/DeepFloyd
16ORIGINAL HIFI-GAN GRIFFIN-LIM
Figure7:Moreresultsonthevocodercycleconsistencycheck.Weshowtheoriginallogmel-spectrogram
decodedfromlatentsandlogmel-spectrogramsobtainedfromwaveformssynthesizedbyHiFi-GANandGriffin-
Lim.
A.2 Qualitativeresults
Morequalitativeresults. Weshowmorequalitativeresultsfromourmethodwithdifferentprompts
inFig.8. Pleaseseeourwebsiteforvideoresults. Wealsoproviderandomexampleswithrandom
promptpairsinFig.9withthelasttworowsasfailurecases. Forthefailurecases,wecanseethat
theyeitherhavegoodaudioqualitybutloseclearvisualpatterns(mountains/fireworks)orhaveclear
visualappearancesbutnoisyaudio(dogs/trains).
Vocoders. WeshowmoreexamplesofthevocodercycleconsistencyexperimentinFig.7. Ascan
beseen,thespectrogramsfromHiFi-GANarequitesimilartotheoriginalonesdecodedfromlatents,
indicatingthatourmethoddoesnotfindadversarialexamplesagainstthevocoder,buttrulydoesfind
spectrogramsthatlooklikeimages.
A.3 HumanStudies
ParticipantsforthehumanstudywererecruitedfromAmazonMechanicalTurk(MTurk),andwere
paid0.50USDforatasklastinglessthan5min. Weuseatotalofsevenpromptpairsandcompare
themagainsttwobaselines: theSDSbaselineandtheimprintbaseline. Foreachmethodandprompt
pair, wehand-selectedtwohigh-qualitysamplesforatotalof84videos. Eachvideoisabout10
secondslong,andincludesaverticallinemovingfromlefttoright,indicatingthecurrenttemporal
positioninthespectrogram. Allparticipantswereshown14pairsofvideos–sevenpairscomparing
ourmethodtotheSDSbaseline,andsevenpairscomparingourmethodtotheimprintbaseline,all
randomlyselectedandblinded. Theparticipantsarethenaskedtoanswerthreequestions:
1. WhichvideoLOOKSmostlikea[visual prompt]?
2. WhichvideoSOUNDSmostlikea[audio prompt]?
3. In the video, we play the image as a sound, from left to right. In which video does the
[visual prompt]betteralignwiththe[audio prompt]sounds?
Thefirsttwoquestionsaredesignedtoevaluatethequalityoftheaudioandimagegeneratedbythe
methods,andtheiralignmentwiththerespectiveprompts. Thethirdquestionseekstounderstand
17”a painting of cute dogs, grayscale” & ”dog barking”
”a painting of a blooming garden with many bird, grayscale” & ”birds singing sweetly”
”a painting of tigers, grayscale” & ”tiger growling”
”a painting of trains, grayscale” & ”train whistling”
Figure8:Morequalitativeresults.Weshowmorequalitativeresultsofourapproach.Pleasezoominforbetter
viewing.
howwellthegeneratedimagecontentandspectrogramcontentalign. However,notewewerenot
abletoguaranteethattheparticipantshadpriorexperiencewithspectrograms. Tomitigatethisto
anextent, weincludethedescriptionasapreambletothethirdquestion. Also, notethatweuse
abbreviatedversionsoftheaudioandvisualpromptstoavoidexcessivelylongquestions. Weprovide
thepromptpairsweusedforhumanstudiesinTab.5forreference,andscreenshotsofoursurvey
includingthetitleblockaswellasthefirstvideopairandassociatedquestionsinFig.10.
Table5:Textpromptsforthehumanstudies.Wenotethatthepromptsarepaired.
Imageprompts Audioprompts
apaintingofcastletowers,grayscale bellringing
apaintingofcutedogs,grayscale dogbarking
apaintingofabloominggardenwithmanybirds,grayscale birdssingingsweetly
apaintingoffurrykittens,grayscale akittenmeowingforattention
apaintingofautoracinggame,grayscale aracecarpassingbyanddisappearing
apaintingoftrains,grayscale trainwhistling
apaintingoftigers,grayscale tigergrowling
18”a painting of kittens, grayscale” & ”dog barking”
”a painting of a farm, grayscale” & ”bird chirping, tweeting”
”a painting of a beach, grayscale” & ”people cheering”
”a painting of mountains, grayscale” & ”fireworks banging”
”a painting of dogs, grayscale” & ”train whistling”
Figure9:Randomresults.Weshowrandomresultsfromourapproach,usingrandomaudioandvisualtext
promptpairs.Weprovidefailurecasesinthelasttworows.Pleasezoominforbetterviewing.
19Figure10:Humanstudyscreenshots. Weshowscreenshotsfromourhumanstudysurvey.Hereweshowthe
titleblock,aswellasthefirstpairofvideos.Thefullsurveycontains14videopairs.
20