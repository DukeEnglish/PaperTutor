Adapting Large Multimodal Models to Distribution
Shifts: The Role of In-Context Learning
GuanglinZhou1†∗,ZhongyiHan2†,ShimingChen3,BiweiHuang4,
LimingZhu5,SalmanKhan3,6,XinGao2*,LinaYao5,1,7*
1UniversityofNewSouthWales,2KingAbdullahUniversityofScienceandTechnology,
3MohamedbinZayedUniversityofAI,4UniversityofCalifornia,SanDiego,
5Data61,CSIRO,6AustralianNationalUniversity,7MacquarieUniversity
jameszhou.ustc@gmail.com
Abstract
Recentstudiesindicatethatlargemultimodalmodels(LMMs)arehighlyrobust
againstnaturaldistributionshifts,oftensurpassingpreviousbaselines. Despitethis,
domain-specificadaptationisstillnecessary,particularlyinspecializedareaslike
healthcare. Duetotheimpracticalityoffine-tuningLMMsgiventheirvastparame-
terspace,thisworkinvestigatesin-contextlearning(ICL)asaneffectivealternative
forenhancingLMMs’adaptability. WefindthatthesuccessofICLheavilyrelies
onthechoiceofdemonstration,mirroringchallengesseeninlargelanguagemodels
butintroducinguniquecomplexitiesforLMMsfacingdistributionshifts. Ourstudy
addressesthisbyevaluatinganunsupervisedICLmethod,TopKNearestPR,which
selectsin-contextexamplesthroughanearestexamplesearchbasedonfeaturesim-
ilarity. Weuncoverthatitseffectivenessislimitedbythedeficienciesofpre-trained
vision encoders under distribution shift scenarios, evidenced by their zero-shot
capabilitiesbarelyoutperformingrandomguesses. Toaddressthesechallenges,
weproposeInvariantSelectPR,anovelmethodleveragingClass-conditionedCon-
trastiveInvariance(CCI)formorerobustdemonstrationselection.Specifically,CCI
enhancespre-trainedvisionencodersbyimprovingtheirdiscriminativecapabili-
tiesacrossdifferentclassesandensuringinvariancetodomain-specificvariations.
Thisenhancementallowstheencoderstoeffectivelyidentifyandretrievethemost
informative examples, which are then used to guide LMMs in adapting to new
querysamplesundervaryingdistributions. OurexperimentsshowthatInvariantS-
electPRsubstantiallyimprovestheadaptabilityofLMMs,achievingsignificant
performance gains on benchmark datasets, with a 34.2%↑ accuracy increase in
7-shotonCamelyon17and16.9%↑increasein7-shotonHAM10000compared
to the baseline zero-shot performance. Our code will be publicly available at:
https://github.com/jameszhou-gl/icl-distribution-shift.
1 Introduction
Machine learning models are essential in areas such as climate modeling, biomedicine, and au-
tonomousdriving,wheretheyneedtoreliablymanagedeviationsfromtheirtrainingdataknown
asdistributionshifts[Parketal.,2021,Zhouetal.,2022,2023b]. Traditionalmethodslikedomain
adaptationanddomaingeneralizationhavebeensomewhateffectivebutstillfallshortinaddressing
theseshifts,asconfirmedbyseveralempiricalstudies[GulrajaniandLopez-Paz,2020,Wilesetal.,
2022]. However,theemergenceoffoundationmodels,characterizedbytheirextensiveanddiverse
∗Correspondingauthors
†Theseauthorscontributedequallytothiswork.
Preprint.Underreview.
4202
yaM
02
]VC.sc[
1v71221.5042:viXraText + Query LMMs Output Text + Query LMMs Output
Prompt Sample Prompt Sample
Fromacloser ICL Guide
Largedistributionshift distribution Sample
(a)Zero-shot (b)In-contextlearning
What’stheclasslabelof
Query theimage?
Sample Selecttheanswerfromthe : The lesion is round/oval, uniformly flat, and light
choiceslist. brown,whicharecharacteristicsofmelanocyticnevi.
Closer
Distribution
: Thequeryimageshowsalesionwithasymmetry,an
ICL ICLimage1ismelanoma. LMMs irregular border, variegated color, and diameter greater
What’stheclasslabelofthe
Sample queryimage? than6mm.Similartotheimage1,theseareallfeatures
Selecttheanswerfromthe ofmelanoma.
choiceslist. (c)ICL Adaptation Case
Figure 1: Comparative illustration of (a) zero-shot transfer, which relies on LMMs’ pre-trained
knowledgetorespondtoqueries,potentiallyleadingtoalargedistributiongap,and(b)in-context
learning(ICL),whichintroducesanexamplefromacloserdistributiontobridgethisgap. Thiswork
investigates different retrieval methods for selecting effective ICL examples. (c) The efficacy of
one-shotICLisshowcasedinguidingLMMswithshifteddistributions.
pretraining,offersnewpossibilitiesforenhancingadaptabilitytothesechallenges[Bommasanietal.,
2021,Radfordetal.,2021,Shuetal.,2023]. Specifically,largemultimodalmodels(LMMs)[Yang
etal.,2023b]suchasGPT-4V[OpenAI,2023],andGemini[Teametal.,2023]haveshownsuperior
adaptability. Theirzero-shot2capabilitieshavebeenfoundtofrequentlyoutperformtheperformance
oftraditionalfine-tunedmodelsinnaturaldatasets[Hanetal.,2024].
Despiterecentadvances,domain-specificadaptationremainsasignificantchallenge,especiallyin
healthcareandscientificresearch[Hanetal.,2024]. WhileLMMslikeGoogleDeepMind’sMed-
Geminiofferfine-tunedversionsformedicaltasks[Saabetal.,2024],theirblock-boxnatureand
massiveparametersetsmaketraditionalfine-tuningimpracticalforresearcherswithoutextensive
computationalresources. Thishighlightstheurgentneedformorefeasibleadaptationtechniques. In-
contextlearning(ICL),whichallowsmodelstoadaptduringinferencewithoutparameteradjustments,
emerges as a promising alternative [Brown et al., 2020, Liu et al., 2021, Min et al., 2022, Dong
etal.,2022]. WhiletheeffectivenessofICLisrecognizedwithinlargelanguagemodels(LLMs),its
applicationforimprovingadaptabilityinLMMsunderdistributionshiftsislessexplored.
AsdepictedinFigure1,wehypothesizethatequippingLMMswithcontextexamplesthatincludetask-
specificinformationanddetailsaboutthequerysamplecansubstantiallyenhancetheirperformance.
Our research starts with a thorough evaluation of ICL’s capacity to tailor LMMs (§2) to specific
domains,particularlyhealthcareresearch,wherethereisaclearnecessityfordomain-specificfine-
tuning [Hanet al., 2024]. We discoverthat the success ofICL heavily depends on thechoice of
demonstrations,supportingfindingsfrompreviousLLMsstudies[Liuetal.,2021,Minetal.,2022,
Dongetal.,2022]. AlthoughthesignificantimpactofdemonstrationselectiononICLperformance
isnotentirelyunexpected,thechallengeofselectingdemonstrationsforLMMsunderdistribution
shiftsremainsunexplored. Toaddressthis,were-examinetheunsupervisedretrievalofin-context
examples(§3.1),TopKNearestPR,traditionallyusedinLLMs. Thismethod,discussedin[Liuetal.,
2021,Zhangetal.,2024],usesfeaturesimilaritytopinpointcontextuallyrelevantICLexamples.
However, the TopKNearestPR approach faces considerable challenges when applied to LMMs
during distribution shifts. Notably, using pretrained vision encoders like CLIP-ViT3, zero-shot
performanceoftenremainsatlevelscomparabletorandomguessinginspecializeddomains,asshown
in a recent study [Han et al., 2024]. This poor performance reveals a critical limitation in these
encoders: theystruggletorecognizeandadapttothesubtlevariationsinnewdistributions,which
compromisesthereliabilityofvisualfeaturesimilaritiesforselectingeffectivedemonstrations. To
tackle these challenges, we propose InvariantSelectPR, a novel method designed specifically for
scenariosinvolvingdistributionshifts(§3.2). ThisapproachemploysClass-conditionedContrastive
2Weadoptthezero-shotsettingintheCLIPstudy[Radfordetal.,2021],whichenableszero-shottransfervia
pairedimage-textfeatureswithoutextralinearprobes.Itdiffersfromtheclassicalzero-shotofgeneralizingto
unseencategories[Chenetal.,2021,2022,2023].
3https://huggingface.co/openai/clip-vit-base-patch16
2(a) Performance comparison between Zero-shot and RandomPR (b) Demonstration selection is critical to ICL’s effectiveness
Figure2: Motivationillustration: (a)PerformancecomparisonbetweenZero-shotandRandomPR,
illustratingthelimitationsofrandomin-contextexampleselectionacrossfourdatasets,whereone-
shotRandomPRoftenunderperformscomparedtozero-shot. (b)Analysisof77querysamplesfrom
thetargetdomain, hospital_3inCamelyon17, using50distinctone-shotexamplestoexamine
performancevariability. Meanvaluesaremarkedinblue,andvarianceisrepresentedbyblacklines,
highlightingthesignificantimpactofexampleselectiononmodelaccuracy. Ifappropriatein-context
samplesarechosen,thereisapotentialforgainsupto40.25%.
Invariance(CCI)tochoosedemonstrationsbasedondomain-invariantfeatures,whichareinherently
robusttodistributionalchanges[Zhouetal.,2024]. Thisretrievermethodisdistinctivelycrafted
fordistributionshifts,ensuringtheresilienceofselectedin-contextexamplesinvaryingconditions.
OurempiricalresultsdemonstratethatInvariantSelectPRsignificantlyimprovestheadaptabilityof
LMMs,achievingnotableaccuracyimprovements,i.e.,a34.2%accuracyimprovementin7-shoton
Camelyon17anda16.9%accuracyincreasein7-shotonHAM10000overthezero-shotbaseline.
Ourcontributionsandthekeyfindingsaresummarizedasfollows: (1)Tothebestofourknowledge,
this work takes the first step towards deeply understanding the role of in-context learning as an
effective strategy for enhancing the adaptability of LMMs under distribution shifts (§2). (2) We
introduceInvariantSelectPR,anovelin-contextretrievalframeworkspecificallydevelopedtotackle
distributionshifts(§3). (3)Throughextensiveexperimentsonfourbenchmarkdatasets(§4), our
InvariantSelectPRmethodshowssubstantialenhancementsoverzero-shotgeneralizationcapabilities.
2 Motivation: ICLDemonstrationsunderDistributionShifts
Inthissection,weevaluateICL’scapabilitytoenhanceLMMadaptability. StartingwithRandomPR,
werandomlyselectin-contextexamplesfromsourcedomaindatawithoutrelevancetothetarget
task. OurevaluationusestheGeminimodel,notedforitszero-shotcapabilities,acrossfourmedical
datasets typically needing domain-specific fine-tuning [Han et al., 2024]. We compare one-shot
RandomPRwiththezero-shotbaselineforapreliminaryinvestigation. AccordingtoFigure2(a),
while RandomPR presents a slight decrease of 0.3% on the Camelyon17 dataset, it leads to a
substantialperformancedeclineof4.2%,3.7%,andeven8.2%ontheHAM10000,NIH_Chest,and
COVIDdatasetsrespectively. Despiteitsconceptualsimplicity,ourempiricalresultssuggestthat
randomin-contextexampleselectionoftenfailstofulfilltheessentialrequirementforeffectivemodel
adaptation—providinginformativeandcontextuallyappropriatedemonstrations.
TounravelthevariableefficacyofRandomPR,weconductanexperimentusingtheCamelyon17
dataset,focusingon77querysamplesfromthetargetdomainhospital_3. Wetesttheinfluenceof
introducing50distinctexamplesfromthesourcedomainsonthepredictionsforeachquerysample.
The results in Figure 2(b), display both the mean and standard deviation, indicating significant
variabilityinperformancebasedonthein-contextexamplesused. Notably,whilezero-shotaccuracy
is54.55%(44/77),ouranalysisrevealsthatupto73querysamplescouldbeaccuratelyclassified
withtheaptin-contextsamples,potentiallyboostingaccuracyby40.25%.Furthermore,thevariability
observed—suchasameanaccuracyof70%anda21%varianceforthe25thquery—highlightsthe
varyingeffectsofdifferentICLexamples.ThesefindingshighlighttheinconsistenciesinRandomPR’s
performanceandunderscoretheneedforadvancedmethodologiesinICLexampleselection.
3Manifold1 Manifold2 Manifold3 Class1 Class2 Domain1 Domain2 QuerySample
Domain 1
Pre-trainedvisualspace 𝓛 Distribution-alignedspace
𝒄𝒍𝒔
Head
Domain 2
ViT
𝓛
𝑪𝑪𝑰
Query sample TopKNearestPR RandomPR InvariantSelectPR
Example matches
key features: border,
color,spots,size.
Query sample TopKNearestPR RandomPR InvariantSelectPR
Figure3: Overviewofthreeretrievalmethods: RandomPR,TopKNearestPR,andInvariantSelectPR.
RandomPRselectsexampleswithoutspecificcriteria,oftenoverlookinginformativeones. TopKNear-
estPRusesfeaturesimilaritiesforselection,yetstruggleswithdomain-specifictaskswherepre-trained
encoderfeatureslacksufficientdetail. Incontrast,InvariantSelectPRusesaclass-conditionedcon-
trastiveinvariance(CCI)frameworktoenhancevisionencoders, effectivelyidentifyingthemost
representativesamplesbyfocusingonkeyinvariantfeatures.
3 Methodology
Upon identifying the limitations of RandomPR, we developed two advanced methods for more
effective ICL example selection: TopKNearestPR and InvariantSelectPR, illustrated in Figure 3.
ThesemethodsaimtoenhancetheadaptabilityofLMMstodistributionshiftsthroughthestrategic
selectionofdemonstrativeexamples. Wedetailtheseselectionmethodsbelow.
3.1 TopKNearestPR:EnhancingContextRelevance
TopKNearestPRadoptsanunsupervisedstrategytoidentifyin-contextexamplesbymeasuringthe
similaritybetweenthefeaturevectorsofatargetqueryimagex andthoseacrossM sourcedomains.
q
ThedatasetS includesdomainsSi = {(xi,yi)}ni ,wherexrepresentsfeaturevectorsandy is
j j j=1
classlabels. Thecosinesimilaritybetweenthefeaturevectorsfromthequeryimagex andany
q
imagexi fromthedataset,calculatedusingapre-trainedvisionencoderlikeCLIP-ViT,isgivenby:
j
z(x )·z(xi)
sim(x ,xi)= q j (1)
q j ∥z(x )∥∥z(xi)∥
q j
Here,z(x)referstothefeaturevectorextractedbytheencoder. ThetopK imagesthatexhibitthe
highestsimilaritytothequeryareselectedusing:
top (cid:0) {sim(x ,xi):i=1,...,M;j =1,...,n }(cid:1) (2)
K q j i
wheretop denotestheoperationofselectingtheindicesoftheK largestvaluesfromtheset. The
K
selectedimagesserveasthein-contextexamplesfortheLMMs,aimingtoenhancetheirunderstanding
andperformanceonanalogoustaskswithoutfurthertraining.
3.2 InvariantSelectPR:TailoredforDistributionShiftAdaptation
TopKNearestPRfocusesonrelevancebyutilizingfeaturesimilarities,butitseffectivenesscanbe
constrainedbythegranularityoffeaturesfromconventionalencoders. Pretrainedvisionencoders,
4suchasCLIP-ViT,whilerobustingeneralscenarios, oftenstruggletodifferentiateeffectivelyin
domain-specifictasks. Thislimitationmanifestsaszero-shotperformancesthatareonlymarginally
betterthanrandomguesses,leadingtotheselectionofsuboptimalin-contextexampleswhenrelying
solelyonpre-trainedmodels.Thus,weproposeInvariantSelectPR,anewmethoddesignedtoenhance
robustnessacrossdistributionshifts.
FacilitatingClass-conditionedContrastiveInvariance. InvariantSelectPRiscenteredaroundthe
Class-conditionedContrastiveInvariance(CCI)mechanism,whichaimstoimprovethemodel’sability
todistinguishbetweenclasseswhilemaintainingstabilityacrossdomain-specificvariations[Zhou
et al., 2024]. This is achieved by promoting similarity among instances of the same class from
differentdomainsandhighlightingdifferencesbetweenclasses. Usingtheclasstokenembedding
[CLS],x ,fromthefinalvisiontransformer(ViT)layer,theCCIlossisdefinedas:
N
(cid:34) (cid:35)
exp(z ·z /τ)
L =−E log N N′ (3)
CCI (cid:80) exp(z ·z /τ)
k̸=N N k
Here,z isapositivesampleofz fromthesameclassbutpossiblyadifferentdomain,andz
N′ N k
signifiesanegativesampleofz fromadifferentclass. τ denotesthetemperatureparameterin
N
contrastivelearning[Chenetal.,2020,Zhouetal.,2023a]. Thisformulationensuresthatthelearned
representationsarebothdiscriminativeandinvariant,crucialforadaptingtonewdistributions.
This approach combines this CCI loss L with a classification loss L to enhance the vision
CCI cls
encoder’sabilitytomanagedistributionshiftseffectively. Theclassificationlossusescross-entropy
toalignthefinalclasstokenembeddingx withtheground-truthlabely,bolsteringthemodel’s
N
discriminativepower:
C
(cid:88)
L =− y log(Head(x ) ) (4)
cls i N i
i=1
L =L +λL (5)
total cls CCI
whereC representsthetotalnumberofclassesinthedataset,andHead(·)isaneuralclassification
head that maps the class token x to a predicted class probability distribution. λ is a tuning
N
hyper-parametertocontroltheweightoftheCCIloss.
In-ContextSelectionThroughEnhancedInvariance. Afterfine-tuningthevisionencoderwith
thecombinedlossesinEq. (5),weleveragerefinedfeaturestoassessthesimilaritybetweenthetarget
samplesandin-contextexamples. Byensuringthesesimilaritiesreflectbothvisualresemblanceand
domaininvariance,thek-shotexampleswiththehighestsimilarityscoresarethenselected.
4 Experiments
DatasetsOverview. Weusefourbenchmarkdatasetstoexploredistributionshifts,particularly
emphasizingdomain-specificfine-tuning[Hanetal.,2024]. Camelyon17[Bandietal.,2018]features
450,000patchesfrombreastcancerimagesacrossfivehospitals. HAM10000[Tschandletal.,2018]
offersdermatoscopicimagescriticalforskincancerdetection. TheNIH_Chestdataset[Wangetal.,
2017]includesover112,000X-rayimagesannotatedforthoracicdiseases. TheCOVIDdataset[Han
etal.,2021]providesdiversepneumoniadetectiondata,includingCOVID-19cases,fromvarious
hospitals. Weanalyzeapracticalsubset,random_1,with450samples4.
Implementation Details. We compare three retrieval methods—RandomPR, TopKNear-
estPR and InvariantSelectPR—against the baseline zero-shot capability. We employ
vit_large_patch14_224_clip_laion2b configuration from the timm library, exploring vari-
ationsinbackboneconfigurationsfurtherin§4.3.2. TheGeminimodelisemployedastheprimary
LMMduetoitssuperiorzero-shotperformanceacrossvarieddatasets[Hanetal.,2024]anditsstable
log-linearimprovementinperformancewithanincreasingnumberofICLexamples,asobserved
inaconcurrentstudy[Jiangetal.,2024]. Ourmainresults(§4.1)focusonone-shotperformance,
withadditionalinsightsontheimpactofdifferentnumbersofshotsin§4.3.3. Wealsoincludeother
leadingLMMs,suchasGPT-4VandClaude[Anthropic,2023],inourextendedanalysisin§4.3.4.
4Availableathttps://github.com/jameszhou-gl/gpt-4v-distribution-shift
5Table1: Performancecomparisonofthreeretrievalmethodsagainstthezero-shotapproach,illustrat-
ingaccuracyimprovementsordecreasesonCamelyon17andCOVIDdatasets. Meanandstandard
deviationvaluesarecalculatedoverthreeindependentruns,withthebestresultshighlightedinbold.
Camelyon17 COVID
Method
Hosp0 Hosp1 Hosp2 Hosp3 Hosp4 Acc Sou Tar Acc
Zero-shot 52.00 51.93 56.44 54.55 56.67 54.17±0.5 62.19 44.19 52.75±1.3
RandomPR 50.50 53.03 54.59 53.28 58.55 53.87±1.1 38.66 49.86 44.52±0.9
TopKNearestPR 62.24 58.65 58.62 59.65 60.15 59.91±1.8 41.59 60.88 51.70±2.7
InvariantSelectPR 60.12 63.96 62.68 63.39 64.36 62.77±1.1 39.94 67.05 54.15±1.0
Table2: Performancecomparisonofthreeretrievalmethodsagainstthezero-shotapproach,illustrat-
ingaccuracyimprovementsordecreasesonHAM10000andNIH_Chestdatasets. Meanandstandard
deviationvaluesarecalculatedoverthreeindependentruns,withthebestresultshighlightedinbold.
HAM10000 NIH_Chest
Method
RD VMod VMol VDis Acc PA AP Acc
Zero-shot 28.54 37.39 26.42 42.22 32.62±1.2 12.41 14.26 13.31±0.7
RandomPR 21.48 32.81 27.88 12.64 25.71±1.9 7.91 10.12 8.98±2.1
TopKNearestPR 23.03 33.39 31.73 28.03 28.72±1.0 10.34 10.43 10.39±1.0
InvariantSelectPR 38.20 49.43 38.96 25.19 40.91±1.0 13.22 13.63 13.42±0.7
Training Protocol. We train InvariantSelectPR for 100 epoches using the AdamW opti-
mizer [Loshchilov and Hutter, 2017], with a learning rate of 1e-5 and a weight decay of 0.01.
Forclarity,boththetemperatureparameterτ andthelossweightλarefixedat1.0. Eachdatasetis
specificallyfine-tunedtooptimizethevisionencoderforitsrespectivedomains. Experimentsare
conductedonaLinuxserverequippedwithanIntelXeonCPU,NVIDIAA5000andV100GPUs.
4.1 MainResults
InTables1and2,ouranalysisoffourbenchmarkdatasetsprovidesadetailedexaminationofhow
different ICL methods perform under distribution shifts. The zero-shot approach highlights the
inherentabilityofLMMstoadapttonewdomainswithoutretraining. However,theeffectivenessof
thisadaptabilityvariessignificantlywithdifferentICLstrategies. TheRandomPRstrategy,which
employs a stochastic method for selecting in-context examples, yields inconsistent results. For
instance,ontheCamelyon17dataset,itleadstoaslightdecreaseof0.3%inaccuracy,butitlargely
underperformsontheCOVID,HAM10000,andNIH_Chestdatasets,withaccuracydecreasesof
8.2%,4.2%,and3.7%,respectively. ThishighlightstheunpredictableperformanceofRandomPR
acrossdifferentconditions. Conversely,TopKNearestPRusesapre-trainedvisionencodertoidentify
featuresimilaritiesforexampleselection,leadingtoa5.74%improvementontheCamelyon17,which
demonstratesthebenefitsofamoretargetedapproachinexampleselection. Despitethissuccess,the
methodseesdeclinesof3.9%and2.9%ontheHAM10000andNIH_Chestdatasets,respectively,
indicatingalackofconsistentperformanceacrossalltestscenarios. Themosteffectivestrategy,
InvariantSelectPR,consistentlyoutperformsothermethods,significantlyexceedingthezero-shot
baselineacrossalldatasets,especiallyachievingremarkablegainsof8.3%onHAM10000and8.6%
onCamelyon17. Theseresultsunderscoretheimportanceofadvancedin-contextexampleselection
techniquesinadaptingLMMstodistributionshifts. Despitenotablegains,theimprovementswith
InvariantSelectPRonNIH_ChestandCOVIDaremodest. InFigure4,theincrementalimprovements
byInvariantSelectPRalignwiththosefromfine-tunedencoders,whichgenerallysurpassthefine-
tuningapproachby1%to6%. Thissuggeststhatwhenfine-tuningitselfisminimallyeffective,ICL
strategiesyieldlimitedenhancements. Futureresearchthusfocusesonmoresophisticatedmethods
toenhanceinvariance,beyondfundamentaldomain-invarianceinthiswork.
64.2 AblationStudy
Toassesstheimpactofenhancedinvarianceonmodeladaptability,weconductanablationstudy
focusingontheGeminimodel’sone-shotperformance. Thisstudycomparesthreeconfigurations:
abaselineusingTopKNearestPR,thebaselineonlywithL , andthefullInvariantSelectPRthat
cls
incorporatesbothL andL . Table3displaysincrementalperformancegainsacrossdatasets
cls CCI
withthesuccessiveadditionsofL andL . TheadditionofL aloneleadstoamodestincrease
cls CCI cls
in performance by 1.77%. However, when incorporated with L , there is a more substantial
CCI
performance boost of 4.01%, confirming the effectiveness of CCI loss in improving the models’
adaptability.
Table3: Ablationstudyonlossterms. ThebaselineisTopKNearestPR.
Configurations Camelyon17 COVID HAM10000 NIH_Chest Average
baseline 61.96 54.22 29.84 10.49 39.13
baseline+L (w/oCCI) 61.59 52.00 38.93 11.11 40.90
cls
baseline+L +L (full) 63.90 54.44 41.56 12.67 43.14
cls CCI
4.3 In-depthAnalysis
4.3.1 ICLvs. TraditionalSupervisedFinetuning(SFT)
RecentadvancesinLMMsdemonstratetheirimpressivezero-shotgeneralization,oftenoutperforming
fine-tunedmodelsinnaturaldistributionshifts[Hanetal.,2024]. Thisraisesquestionsaboutwhether
LMMs with in-context learning, can exceed fine-tuned model performance in scientific datasets,
traditionallyreliantondomain-specificfine-tuning. WeassessourICLstrategyagainsttraditional
SFTtoexplorethis. ForSFT,wefocusonmaintainingdomaininvarianceandtargetingtheclass
predictionobjective,similartoEq. (5). Wefine-tuneaCLIP-ViTonsourcedomaindataandthen
applyittopredictoutcomesontargetexamples. Thiscomparisondirectlymeasurestheeffectiveness
ofICLversusconventionalSFT.ForInvariantSelectPR,wechooseICLexamplesrangingfromone
tosevenandreportthebestaccuracy. Figure4displaysthecomparativeperformanceacrossfour
datasets. SFT demonstrates a substantial improvement over zero-shot capabilities with accuracy
improvements of 32.4%, and 12.3% on Camelyon17 and HAM10000, but underperforms 1.5%
and5.3%onNIH_ChestandCOVID.Incontrast,ourproposedInvariantSelectPRwithfew-shot
examples,consistentlyexceedsSFT,withgainsof1.4%,4.4%,1.6%,and6.4%inthesamedatasets.
Accuracies between Zero-shot, Fine-tuning and InvariantSelectPR
100
+32.4+34.2 Zero-shot
80 Fine-tuning
InvariantSelectPR
60 +1.1
+16.9 -5.3
+12.3
40
20
-1.5 +0.1
0
Camelyon17 HAM10000 NIH_Chest COVID
Figure4: ComparativeaccuraciesbetweenZero-shot,SupervisedFinetuning,andInvariantSelectPR
methodsacrossvariousdatasets,illustratingthesuperiorperformanceofInvariantSelectPRoverboth
zero-shotandsupervisedfine-tuning.
4.3.2 BackboneEvaluation
WeevaluatetheimpactofdifferentvisionencoderbackbonesontheeffectivenessofourInvariantSe-
lectPRmethodcomparedtoTopKNearestPR.ThisincludesViTmodelspretrainedonImageNet-21K
7
)%(
ycaruccAandtrainedwiththeself-supervisedDINOmethodonImageNet-1K.Table4showsthatInvariantSe-
lectPRconsistentlyoutperformsTopKNearestPRacrossdatasets,withanaverageaccuracyof42.9%
versus39.7%. Thisunderscoresthelimitationsofrelyingsolelyonpretrainedvisualsimilarityfor
selectingmeaningfulin-contextexamples. InvariantSelectPRalsodemonstratesmoreconsistentper-
formance,withlessdeviationfrommeanaccuracy(under1%)comparedtoTopKNearestPR(nearly
2%). Animportantobservationistheenhancedperformanceofbackbonesutilizingself-supervised
orcontrastivelearningmethods,supportingtheeffectivenessofself-supervisedlearningincapturing
generalizablefeaturesthatcontributetomorerobustICLperformance,assuggestedinstudies[Chen
etal.,2020,Radfordetal.,2021,Wangetal.,2023a].
Table4: PerformancecomparisonofICLmethodswithdifferentvisionencoderbackbones.
Methods Backbones Camelyon17 COVID HAM10000 NIH_Chest Average
vit-l/14-clip 61.96 54.22 29.84 10.49 39.13
TopKNearestPR vit-l/16-in21k 60.45 49.78 30.80 10.24 37.82
vit-b/16-dino 63.72 59.68 32.74 10.89 41.76
vit-l/14-clip 63.90 54.44 41.56 12.67 43.14
InvariantSelectPR vit-l/16-in21k 61.76 52.78 38.15 12.89 41.40
vit-b/16-dino 64.48 53.72 44.55 11.36 43.53
4.3.3 ICLExampleswithVariousShots
ToassesstheimpactofthenumberofICLexamples,weperformanempiricalstudyusingtheCame-
lyon17andHAM10000datasets,varyingthenumberofshotsfrom1to7foreachdatasetinFigure5.
Thisanalysisrevealsthatincreasingthenumberofshotsleadstoadecreaseintheperformanceof
theRandomPRmethod,implyingthatadditionalexamplesmightintroduceunhelpfulinformation.
Incontrast,theTopKNearestPRmethodtypicallyimproveswithmoreshotsbutshowsadeclinein
performancewhenmovingfrom3-shotto5-shotontheHAM10000dataset,suggestingpotential
issues with example selection or redundancy. On the other hand, our InvariantSelectPR method
consistentlyimprovesperformanceasthenumberofshotsincreases,demonstratingitseffectiveness
inutilizinginformationfromsourcedomains. Notably,thismethodachievesaperformanceboostof
approximately24.6%whentheshotcountincreasesfrom1to7onCamelyon17.
Figure5: PerformancecomparisonwithvaryingnumbersofICLexamples(shots)onCamelyon17
andHAM10000datasets.
4.3.4 EvaluationAcrossDifferentLMMs
Theopen-sourceLMMslikeIDEFICS[Laurençonetal.,2024]andOpenFlamingo[Awadallaetal.,
2023]primarilyfocusontextandignoretheinputsignalofimages[BertiniBaldassinietal.,2024].
Furthermore,theseLMMslackinstruction-followingabilitytochoosetheresponsefromtheanswer
list. Thus,weusethreeproprietaryLMMsinthiscomparativeanalysis: GeminiPro,GPT-4V,and
Claude3Opus[Anthropic,2024]. Duetothehighcomputationaldemandsandassociatedcostsof
GPT-4VandClaude3Opus,welimitourtestingtoasingledataset,HAM10000,andperforma
one-shotevaluation. Figure6demonstratesthatInvariantSelectPRconsistentlyoutperformsother
8methods across all three LMMs. This method not only exceeds baseline zero-shot performance
butalsosignificantlyenhancesadaptability. BothGPT-4VandClaude3Opusexhibitsubstantial
improvements using all ICL methods over their zero-shot capabilities, suggesting that ICL can
effectivelyboosttheadaptabilityofLMMs.ThisanalysishighlightsthecapacityofInvariantSelectPR
toleveragedomain-invariantfeaturestoenhanceLMMsperformanceundervariableconditions.
Zero-shot RandomPR TopKNearestPR InvariantSelectPR
50
+9.5 +7.5
40 +4.7+4.7
34.8 +20.6
32.1
-2.3
30 -4.2
+8.9
+6.0
20
14.5
10
0
Gemini Pro GPT-4V Claude 3 Opus
Figure6:PerformancecomparisonofZero-shot,RandomPR,TopKNearestPR,andInvariantSelectPR
ontheHAM10000datasetacrossthreeLMMs,demonstratingtheimpactofone-shotdemonstrations.
4.3.5 DistanceMetricEvaluation
Weexaminetheeffectsofemployingvariousdistancemetrics, includingCosine, Euclidean, and
Manhattan,withinTopKNearestPRandInvariantSelectPRmethods,asillustratedinTable5. Our
findingsindicatethattheInvariantSelectPRmethodconsistentlyachieveshigherperformancethan
TopKNearestPRacrossallmetricstestedonboththeCamelyon17andHAM10000datasets.
Table5: Performancecomparisonusingdifferentdistancemetrics.
Camelyon17 HAM10000
Method
Cosine Euclidean Manhattan Avg Cosine Euclidean Manhattan Avg
TopKNearestPR 61.96 60.77 59.18 60.64 29.84 29.46 30.80 30.03
InvariantSelectPR 63.90 61.09 62.70 62.56 41.56 37.22 38.93 39.24
4.3.6 ComputationalEfficiency
Table 6 illustrates the trade-off between computational cost and accuracy improvement. While
InvariantSelectPRincursaslightlyhigherinferencetimeandGPUusagethanthezero-shotbaseline
but offers an 8.60% accuracy improvement. The increased cost is due to the model loading and
similarity calculation. InvariantSelectPR’ lower inference time compared to TopKNearestPR is
becauseitloadsthevisionencoderonceperenvironmentinsteadofforeachtargetsample. Future
workwillfocusonoptimizingthesestepstoreduceinferencetimewhilemaintainingaccuracygains.
Table6: Performancecomparisonofdifferentone-shotICLmethodsontheCamelyon17dataset,in
termsofinferencetime,GPUusage,andaccuracyimprovementoverthezero-shotbaseline.
Method InferenceTime(s/query) GPUUsage(GB) AccuracyImprovement
Zero-shot 5.23 - -
RandomPR 5.35 - -0.30%
TopKNearestPR 15.52 2.41 +5.74%
InvariantSelectPR 11.79 3.55 +8.60%
9
)%(
ycaruccA5 ConclusionandDiscussion
Concusion. Weinvestigatedtheefficacyofin-contextlearning(ICL)toimprovetheadaptabilityof
LMMstodistributionshiftsthroughournovelICLapproach,InvariantSelectPR.Thismethodnot
onlyoutperformsstandardzero-shotcapabilitiesbutalsoexceedsothermethodslikeRandomPRand
TopKNearestPRinhandlingdomain-specificshifts. Evaluationsacrossfourdatasetsconfirmedthat
InvariantSelectPRenhancesLMMadaptabilitybyoptimallyselectingdemonstrativeexamples. Our
studyoffersinsightsforfutureworkondistributionshiftsinfoundationmodels.
Limitations. Ourstudyisconstrainedbyseveralfactors.Firstly,weconfinedouranalysistoasmall
selectionofbenchmarkdatasetsandreliedexclusivelyoncommercialandproprietarymodels,suchas
GeminiPro,GPT-4V,andClaude3Opus. Thelimitedavailabilityofcomprehensivedocumentation
forthesemodelsconstrainsourunderstandingoftheirpre-trainingdata,architecture,andinherent
biases. This is critical as some broadly used open-source LMMs can not effectively understand
multipleimageslikeFlamingo[Alayracetal.,2022]andsimultaneouslyfollowinstructionslike
LLaVA[Liuetal.,2023],necessitatingtheuseofcommercialmodels. Additionally,thesubstantial
financialandcomputationalresourcesrequiredtoaccesstheseproprietarymodelsmayrestrictfurther
validationandanalysis. Secondly,ourempiricaltestsinvolvedjust450samples,which,despiteprior
researchsuggestingstabilityrangingfrom180to1800cases[Hanetal.,2024],mightnotreveal
scalabilityissuesorsubtlebiasesinlargerdatasets. Thirdly,theprevalenceofnumerousdomainsin
healthcare[Yangetal.,2023a]andscientificresearch[Jietal.,2022]presentspotentialchallengesin
scalingourmethodacrossmultipledomains.
BoarderImpacts. ThisworkintroducesanovelICLmethodtoenhancetheadaptabilityofLMMs
todistributionshifts,particularlyinscientificandhealthcaredomains. Whilethismethodpromises
improvedmodelaccuracyandreliability,itsmisusecouldamplifybiasesoryieldunreliableoutputs
fromLMMs,leadingtonegativeconsequencesinthesecriticalfields.
10References
J.-B.Alayrac,J.Donahue,P.Luc,A.Miech,I.Barr,Y.Hasson,K.Lenc,A.Mensch,K.Millican,
M.Reynolds,etal. Flamingo: avisuallanguagemodelforfew-shotlearning. AdvancesinNeural
InformationProcessingSystems,35:23716–23736,2022.
Anthropic. Modelcardandevaluationsforclaudemodels. https://www-cdn.anthropic.com/
files/4zrzovbb/website/bd2a28d2535bfb0494cc8e2a3bf135d2e7523226.pdf, 2023.
Accessed: 2024-03-07.
Anthropic. Claude3haiku: ourfastestmodelyet. 2024. Availableat: https://www.anthropic.
com/news/claude-3-haiku.
A. Awadalla, I. Gao, J. Gardner, J. Hessel, Y. Hanafy, W. Zhu, K. Marathe, Y. Bitton, S. Gadre,
J.Jitsev,S.Kornblith,P.W.Koh,G.Ilharco,M.Wortsman,andL.Schmidt. Openflamingo,Mar.
2023. URLhttps://doi.org/10.5281/zenodo.7733589.
Y.Balaji,S.Sankaranarayanan,andR.Chellappa. Metareg: Towardsdomaingeneralizationusing
meta-regularization. InNeurIPS,2018.
I.Balazevic,D.Steiner,N.Parthasarathy,R.Arandjelovic´,andO.Henaff. Towardsin-contextscene
understanding. AdvancesinNeuralInformationProcessingSystems,36,2024.
P.Bandi,O.Geessink,Q.Manson,M.VanDijk,M.Balkenhol,M.Hermsen,B.E.Bejnordi,B.Lee,
K.Paeng,A.Zhong,etal. Fromdetectionofindividualmetastasestoclassificationoflymphnode
statusatthepatientlevel: thecamelyon17challenge. IEEEtransactionsonmedicalimaging,38
(2):550–560,2018.
A.Bar,Y.Gandelsman,T.Darrell,A.Globerson,andA.Efros.Visualpromptingviaimageinpainting.
AdvancesinNeuralInformationProcessingSystems,35:25005–25017,2022.
S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira. Analysis of representations for domain
adaptation. Advancesinneuralinformationprocessingsystems,19,2006.
F.BertiniBaldassini,M.Shukor,M.Cord,L.Soulier,andB.Piwowarski. Whatmakesmultimodal
in-contextlearningwork? arXive-prints,pagesarXiv–2404,2024.
R.Bommasani,D.A.Hudson,E.Adeli,R.Altman,S.Arora,S.vonArx,M.S.Bernstein,J.Bohg,
A.Bosselut,E.Brunskill,etal.Ontheopportunitiesandrisksoffoundationmodels.arXivpreprint
arXiv:2108.07258,2021.
T.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal,A.Neelakantan,P.Shyam,
G.Sastry,A.Askell,etal. Languagemodelsarefew-shotlearners. Advancesinneuralinformation
processingsystems,33:1877–1901,2020.
F.M.Carlucci,A.D’Innocente,S.Bucci,B.Caputo,andT.Tommasi. Domaingeneralizationby
solvingjigsawpuzzles. InCVPR,2019.
P.Chattopadhyay,Y.Balaji,andJ.Hoffman. Learningtobalancespecificityandinvarianceforin
andoutofdomaingeneralization. InECCV,2020.
S.Chen,W.Wang,B.Xia,Q.Peng,X.You,F.Zheng,andL.Shao. Free: Featurerefinementfor
generalized zero-shot learning. In Proceedings of the IEEE/CVF international conference on
computervision,pages122–131,2021.
S.Chen,Z.Hong,Y.Liu,G.-S.Xie,B.Sun,H.Li,Q.Peng,K.Lu,andX.You. Transzero: Attribute-
guidedtransformerforzero-shotlearning. InProceedingsoftheAAAIconferenceonartificial
intelligence,volume36,pages330–338,2022.
S.Chen,W.Hou,Z.Hong,X.Ding,Y.Song,X.You,T.Liu,andK.Zhang. Evolvingsemantic
prototype improves generative zero-shot learning. In International Conference on Machine
Learning,pages4611–4622.PMLR,2023.
11T.Chen,S.Kornblith,M.Norouzi,andG.Hinton. Asimpleframeworkforcontrastivelearningof
visualrepresentations. InInternationalconferenceonmachinelearning,pages1597–1607.PMLR,
2020.
Q.Dong,L.Li,D.Dai,C.Zheng,Z.Wu,B.Chang,X.Sun,J.Xu,andZ.Sui.Asurveyforin-context
learning. arXivpreprintarXiv:2301.00234,2022.
A.Dosovitskiy,L.Beyer,A.Kolesnikov,D.Weissenborn,X.Zhai,T.Unterthiner,M.Dehghani,
M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16
words: Transformersforimagerecognitionatscale. ArXiv,abs/2010.11929,2020. URLhttps:
//api.semanticscholar.org/CorpusID:225039882.
Z.Fang,X.Li,X.Li,J.M.Buhmann,C.C.Loy,andM.Liu. Explorein-contextlearningfor3d
pointcloudunderstanding. AdvancesinNeuralInformationProcessingSystems,36,2024.
Y.GaninandV.Lempitsky. Unsuperviseddomainadaptationbybackpropagation. InInternational
conferenceonmachinelearning,pages1180–1189.PMLR,2015.
I.GulrajaniandD.Lopez-Paz. Insearchoflostdomaingeneralization. InInternationalConference
onLearningRepresentations,2020.
Z.Han, R.He, T.Li, B.Wei, J.Wang, andY.Yin. Semi-supervisedscreeningofcovid-19from
positiveandunlabeleddatawithconstraintnon-negativeriskestimator. InInformationProcessing
inMedicalImaging: 27thInternationalConference,IPMI2021,VirtualEvent,June28–June30,
2021,Proceedings27,pages611–623.Springer,2021.
Z.Han,X.-J.Gui,H.Sun,Y.Yin,andS.Li. Towardsaccurateandrobustdomainadaptationunder
multiplenoisyenvironments. IEEETransactionsonPatternAnalysisandMachineIntelligence,45
(5):6460–6479,2022a.
Z.Han,H.Sun,andY.Yin. Learningtransferableparametersforunsuperviseddomainadaptation.
IEEETransactionsonImageProcessing,31:6424–6439,2022b.
Z. Han, G. Zhou, R. He, J. Wang, X. Xie, T. Wu, Y. Yin, S. Khan, L. Yao, T. Liu, et al. How
welldoesgpt-4v(ision)adapttodistributionshifts? apreliminaryinvestigation. arXivpreprint
arXiv:2312.07424,2023.
Z. Han, G. Zhou, R. He, J. Wang, T. Wu, Y. Yin, S. Khan, L. Yao, T. Liu, and K. Zhang. How
welldoesGPT-4v(ision)adapttodistributionshifts? apreliminaryinvestigation. InICLR2024
Workshop on Mathematical and Empirical Understanding of Foundation Models, 2024. URL
https://openreview.net/forum?id=J8V4EwZkez.
Y.Ji,L.Zhang,J.Wu,B.Wu,L.-K.Huang,T.Xu,Y.Rong,L.Li,J.Ren,D.Xue,etal. Drugood:
Out-of-distribution(ood)datasetcuratorandbenchmarkforai-aideddrugdiscovery–afocuson
affinitypredictionproblemswithnoiseannotations. arXivpreprintarXiv:2201.09637,2022.
Y.Jiang, J.Irvin, J.H.Wang, M.A.Chaudhry, J.H.Chen, andA.Y.Ng. Many-shotin-context
learninginmultimodalfoundationmodels,2024.
H.Laurençon,L.Saulnier,L.Tronchon,S.Bekman,A.Singh,A.Lozhkov,T.Wang,S.Karamcheti,
A.Rush,D.Kiela,etal. Obelics: Anopenweb-scalefiltereddatasetofinterleavedimage-text
documents. AdvancesinNeuralInformationProcessingSystems,36,2024.
D.Li,Y.Yang,Y.-Z.Song,andT.Hospedales. Learningtogeneralize: Meta-learningfordomain
generalization. InAAAI,2018a.
Y.Li, X.Tian, M.Gong, Y.Liu, T.Liu, K.Zhang, andD.Tao. Deepdomaingeneralizationvia
conditionalinvariantadversarialnetworks. InECCV,2018b.
H.Liu,C.Li,Q.Wu,andY.J.Lee. Visualinstructiontuning. arXivpreprintarXiv:2304.08485,
2023.
J.Liu,D.Shen,Y.Zhang,B.Dolan,L.Carin,andW.Chen. Whatmakesgoodin-contextexamples
forgpt-3? arXivpreprintarXiv:2101.06804,2021.
12I.LoshchilovandF.Hutter. Decoupledweightdecayregularization. InInternationalConferenceon
LearningRepresentations,2017.
Y.Lu,M.Bartolo,A.Moore,S.Riedel,andP.Stenetorp. Fantasticallyorderedpromptsandwhereto
findthem: Overcomingfew-shotpromptordersensitivity. arXivpreprintarXiv:2104.08786,2021.
Z.MaoandY.Yu. Tuningllmswithcontrastivealignmentinstructionsformachinetranslationin
unseen,low-resourcelanguages. arXivpreprintarXiv:2401.05811,2024.
S.Min,X.Lyu,A.Holtzman,M.Artetxe,M.Lewis,H.Hajishirzi,andL.Zettlemoyer.Rethinkingthe
roleofdemonstrations: Whatmakesin-contextlearningwork? arXivpreprintarXiv:2202.12837,
2022.
OpenAI. Gpt-4v(ision) system card. 2023. URL https://cdn.openai.com/papers/GPTV_
System_Card.pdf.
C.Park,A.Awadalla,T.Kohno,andS.Patel. Reliableandtrustworthymachinelearningforhealth
usingdatasetshiftdetection. AdvancesinNeuralInformationProcessingSystems,34:3043–3056,
2021.
X.Peng,Q.Bai,X.Xia,Z.Huang,K.Saenko,andB.Wang. Momentmatchingformulti-source
domainadaptation. InICCV,2019.
V. Piratla, P. Netrapalli, and S. Sarawagi. Efficient domain generalization via common-specific
low-rankdecomposition. InICML,2020.
A.Radford,J.W.Kim,C.Hallacy,A.Ramesh,G.Goh,S.Agarwal,G.Sastry,A.Askell,P.Mishkin,
J. Clark, et al. Learning transferable visual models from natural language supervision. In
Internationalconferenceonmachinelearning,pages8748–8763.PMLR,2021.
K.Saab,T.Tu,W.-H.Weng,R.Tanno,D.Stutz,E.Wulczyn,F.Zhang,T.Strother,C.Park,E.Vedadi,
etal. Capabilitiesofgeminimodelsinmedicine. arXivpreprintarXiv:2404.18416,2024.
Y. Shu, X. Guo, J. Wu, X. Wang, J. Wang, and M. Long. CLIPood: Generalizing CLIP to out-
of-distributions. InA.Krause, E.Brunskill, K.Cho, B.Engelhardt, S.Sabato, andJ.Scarlett,
editors,Proceedingsofthe40thInternationalConferenceonMachineLearning,volume202of
ProceedingsofMachineLearningResearch,pages31716–31731.PMLR,23–29Jul2023. URL
https://proceedings.mlr.press/v202/shu23a.html.
B.SunandK.Saenko. Deepcoral: Correlationalignmentfordeepdomainadaptation. InComputer
Vision–ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016,
Proceedings,PartIII14,pages443–450.Springer,2016.
G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M.
Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint
arXiv:2312.11805,2023.
P.Tschandl,C.Rosendahl,andH.Kittler. Theham10000dataset,alargecollectionofmulti-source
dermatoscopicimagesofcommonpigmentedskinlesions. Scientificdata,5(1):1–9,2018.
L.VanderMaatenandG.Hinton. Visualizingdatausingt-sne. Journalofmachinelearningresearch,
9(11),2008.
H. Venkateswara, J. Eusebio, S. Chakraborty, and S. Panchanathan. Deep hashing network for
unsuperviseddomainadaptation. InCVPR,2017.
R.Volpi,H.Namkoong,O.Sener,J.C.Duchi,V.Murino,andS.Savarese. Generalizingtounseen
domainsviaadversarialdataaugmentation. InNeurIPS,2018.
H.Wang,T.Fu,Y.Du,W.Gao,K.Huang,Z.Liu,P.Chandak,S.Liu,P.VanKatwyk,A.Deac,etal.
Scientificdiscoveryintheageofartificialintelligence. Nature,620(7972):47–60,2023a.
X.Wang,Y.Peng,L.Lu,Z.Lu,M.Bagheri,andR.M.Summers. Chestx-ray8: Hospital-scalechest
x-raydatabaseandbenchmarksonweakly-supervisedclassificationandlocalizationofcommon
thoraxdiseases. InCVPR,pages2097–2106,2017.
13X.Wang,W.Wang,Y.Cao,C.Shen,andT.Huang. Imagesspeakinimages: Ageneralistpainterfor
in-contextvisuallearning. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages6830–6839,2023b.
Z.Wang,Y.Jiang,Y.Lu,P.He,W.Chen,Z.Wang,M.Zhou,etal. In-contextlearningunlockedfor
diffusionmodels. AdvancesinNeuralInformationProcessingSystems,36,2024.
J.Wei,Y.Tay,R.Bommasani,C.Raffel,B.Zoph,S.Borgeaud,D.Yogatama,M.Bosma,D.Zhou,
D.Metzler,etal. Emergentabilitiesoflargelanguagemodels. arXivpreprintarXiv:2206.07682,
2022a.
J.Wei,X.Wang,D.Schuurmans,M.Bosma,F.Xia,E.Chi,Q.V.Le,D.Zhou,etal.Chain-of-thought
promptingelicitsreasoninginlargelanguagemodels. AdvancesinNeuralInformationProcessing
Systems,35:24824–24837,2022b.
N.Wies,Y.Levine,andA.Shashua. Thelearnabilityofin-contextlearning. AdvancesinNeural
InformationProcessingSystems,36,2024.
O.Wiles,S.Gowal,F.Stimberg,S.-A.Rebuffi,I.Ktena,K.D.Dvijotham,andA.T.Cemgil. Afine-
grainedanalysisondistributionshift. InInternationalConferenceonLearningRepresentations,
2022. URLhttps://openreview.net/forum?id=Dl4LetuLdyK.
Y.Wolf,N.Wies,Y.Levine,andA.Shashua. Fundamentallimitationsofalignmentinlargelanguage
models. arXivpreprintarXiv:2304.11082,2023.
J. Wu, T. Yu, R. Wang, Z. Song, R. Zhang, H. Zhao, C. Lu, S. Li, and R. Henao. Infoprompt:
Information-theoreticsoftprompttuningfornaturallanguageunderstanding. AdvancesinNeural
InformationProcessingSystems,36,2024.
S.M.Xie,A.Raghunathan,P.Liang,andT.Ma. Anexplanationofin-contextlearningasimplicit
bayesianinference. arXivpreprintarXiv:2111.02080,2021.
C.Yang,M.B.Westover,andJ.Sun. ManyDG:Many-domaingeneralizationforhealthcareappli-
cations. InTheEleventhInternationalConferenceonLearningRepresentations, 2023a. URL
https://openreview.net/forum?id=lcSfirnflpW.
Z.Yang,L.Li,K.Lin,J.Wang,C.-C.Lin,Z.Liu,andL.Wang. Thedawnoflmms: Preliminary
explorationswithgpt-4v(ision). arXivpreprintarXiv:2309.17421,2023b.
Y.Zhang,K.Zhou,andZ.Liu. Whatmakesgoodexamplesforvisualin-contextlearning? Advances
inNeuralInformationProcessingSystems,36,2024.
G. Zhou, C. Huang, X. Chen, X. Xu, C. Wang, L. Zhu, and L. Yao. Contrastive counterfactual
learningforcausality-awareinterpretablerecommendersystems. InProceedingsofthe32ndACM
InternationalConferenceonInformationandKnowledgeManagement,pages3564–3573,2023a.
G. Zhou, S. Xie, G. Hao, S. Chen, B. Huang, X. Xu, C. Wang, L. Zhu, L. Yao, and K. Zhang.
Emergingsynergiesincausalityanddeepgenerativemodels: Asurvey. arXivpreprintarXiv,2301,
2023b.
G. Zhou, Z. Han, S. Chen, B. Huang, L. Zhu, T. Liu, L. Yao, and K. Zhang. Hcvp: Leveraging
hierarchicalcontrastivevisualpromptfordomaingeneralization. arXivpreprintarXiv:2401.09716,
2024.
K. Zhou, Z. Liu, Y. Qiao, T. Xiang, and C. C. Loy. Domain generalization: A survey. IEEE
TransactionsonPatternAnalysisandMachineIntelligence,2022.
14Appendix
Morecontentsareputintheappendix,including:
A. RelatedWork.
B. ExperimentalDetails.
C. t-SNEVisualizationsofVisualFeatures.
A RelatedWork
DistributionShifts. Theliteratureondistributionshiftscategorizesmitigationapproachesinto
twoprimarystrategies: domainadaptationanddomaingeneralization. Domainadaptationtechniques,
well-establishedforscenarioswherethetargetdomainisknownduringtraining,recalibratemodels
according to the target data’s statistical properties [Ben-David et al., 2006]. These techniques
encompass deep transfer learning, which aligns feature distributions between source and target
domains[SunandSaenko,2016],unsupervisedmethodsthatminimizedomaindiscrepancies[Ganin
andLempitsky,2015],andtheuseofbenchmarkssuchasOffice-Home[Venkateswaraetal.,2017]
andDomainNet[Pengetal.,2019]. Thesebenchmarkshavepropelledadvancesbyintroducingmore
complexanddiversescenarios[Hanetal.,2022a,b]. Incontrast,domaingeneralizationaddresses
themoredauntingchallengeofexcellingincompletelyunseendomains. Strategieshereinclude
aligning features across multiple source domains [Li et al., 2018b], separating domain-specific
fromdomain-generalfeatures[Piratlaetal.,2020,Chattopadhyayetal.,2020], employingmeta-
learningforoptimizationacrossvariousdomains[Lietal.,2018a,Balajietal.,2018],andusingdata
augmentationtomimicdomainvariability[Volpietal.,2018,Carluccietal.,2019]. Recentstudies
have observed that LLMs have demonstrated exceptional adaptability when dealing with natural
distributionshiftsbutcannothandlethedistributionshiftsinspecializedareassuchashealthcareand
scientificresearch[Radfordetal.,2021,Hanetal.,2023]. Thisobservationmotivatesthispaper’s
explorationofICLandthedevelopmentofnewICLstrategiesunderdistributionshifts.
In-Context Learning. In-context learning (ICL), particularly defined in GPT-3 [Brown et al.,
2020],originatedinLLMsfornaturallanguageprocessing(NLP)tasks. ICLisaproveneffective
paradigmthatleveragescontextaugmentedwithafewexamplestoenableLLMstomakepredictions
[Dong et al., 2022, Lu et al., 2021, Wei et al., 2022b, Min et al., 2022, Dong et al., 2022, Wei
etal.,2022a,Wolfetal.,2023,Wiesetal.,2024,Xieetal.,2021]. Thechoiceofthesein-context
examplescriticallyimpactsperformance,asevidencedbystudiesdemonstratingthatselectingnearest
neighborsbasedonsentenceencoderscansignificantlyenhancethefew-shotcapabilitiesofmodels
like GPT-3 [Liu et al., 2021, Wu et al., 2024, Mao and Yu, 2024]. While ICL is established in
NLP, it is emerging in visual and multimodal LLMs. The study Flamingo [Alayrac et al., 2022]
markstheearliestexplorationofvisualICL,withsubsequentstudiesvalidatingtheimportanceof
exampleselectioninimagepaintingmodels[Baretal.,2022,Wangetal.,2023b,Zhangetal.,2024],
visualunderstanding[Balazevicetal.,2024,Fangetal.,2024],anddiffusionmodels[Wangetal.,
2024]. Unlikepriorwork,thispaperuniquelyfocusesondeeplyunderstandingtheroleofICLunder
distributionshifts,takingafirststepinthisdirection.
B ExperimentalDetails
B.1 Datasets
Thissectionprovidesdetailedinformationaboutthedatasetsusedintheexperiments,includingtheir
statistics,preprocessingsteps,anddomainsplits.
DatasetStatistics. WepresentthedatastatisticsinTable7. Besides,weusethedatasetsavailable
inhttps://huggingface.co/datasets/jameszhou-gl/gpt-4v-distribution-shiftand
evaluaterandom_1subsetwith450cases.
PreprocessingSteps. Wedonotadoptadditionalpreprocessingstepsfortheimages,adhering
insteadtotheguidelinesprovidedbyeachlargemultimodalmodel. Forinstance,theGeminimodel
15Table7: Detailedstatisticsofthedatasetsusedintheexperiments.
Dataset Category PredictionTask DomainType #Domains #Classes #Samples ExampleImage
Camelyon17 Medical Tumordetection Hospital 5 2 450
HAM10000 Medical Skindiseaseclassification Hospital 4 7 450
NIH_Chest Medical Lungdiseasediagnosis Hospital 2 15 450
COVID Medical Pneumoniatypeclassification Hospital 2 3 450
acceptsPIL.Image.open(img_path)asinputwithoutrequiringspecificimagesize,normalization,
oraugmentation5.
B.2 Prompts
WeutilizethefollowingbasicprompttemplateinallICLexperiments,referingto[Hanetal.,2024].
image_descriptions = [f"Image {i+1} is {image_class}" for i,
(desc, image_class) in enumerate(source_images)]
images_description = ". ".join(image_descriptions)
prompt = f"""Given the images, answer the following question, using the
specified format.
{images_description}.
Question: What is the class of the next image?
Choices: {’, ’.join(class_names)}.
Please respond with the following format for each image:
---BEGIN FORMAT TEMPLATE---
Answer Choice: [Your Answer Choice Here]
Confidence Score: [Your Numerical Prediction Confidence Score Here
From 0 To 1]
Reasoning: [Your Reasoning Behind This Answer Here]
---END FORMAT TEMPLATE---
Do not deviate from the above format.
Repeat the format template for the answer.
"""
5https://ai.google.dev/gemini-api/docs/get-started/python
16B.3 ImplementationDetails
Visionencoderarchitectures. Weemploythevit_large_patch14_224_clip_laion2bcon-
figurationfromthetimmlibrary6,whichutilizestheVisionTransformer(ViT)architecture[Doso-
vitskiyetal.,2020]. Thismodeldivides224x224pixelimagesinto14x14patchesandprocesses
themusingatransformerencoder,leveragingextensivepretrainingontheLAION-2Bdatasetwith
theCLIPapproachtoenhancevisualandtextualunderstanding.
DataandCode. Ourcodecanbefoundatthesupplementarymaterialanddataispubliclyavailable
inhttps://huggingface.co/datasets/jameszhou-gl/gpt-4v-distribution-shift.
C t-SNEVisualizationsofVisualFeatures
Inthissection,wepresnett-SNE[VanderMaatenandHinton,2008]visualizationsofthevisual
features to illustrate how class-conditioned contrastive invariance (CCI) contributes to domain
invarianceanddiscriminativecapabilities. Thevisualizationsarebasedontheoriginalvisionencoder
andourfine-tunedvisionencoderonthreedatasets,asshowninFigure7. Thevisualfeaturesare
extracted using both the pretrained and fine-tuned ViT models. The t-SNE plots are created to
highlighttheclusteringbehaviorofthefeaturesfromthetargetdomain. Bycomparingtheplots,we
canvisuallyassesstheimpactofthefine-tuningwithCCIontheseparationofdifferentclassesand
thecompactnessoffeatureclusters.
Camelyon17 HAM10000 COVID
Figure7: t-SNEvisualizationsofvisualfeaturesfromthetargetdomainforthreedatasets. Thelower
rowshowsfeaturesextractedusingthepretrainedViTmodel,whiletheupperrowshowsfeatures
extractedusingtheourfine-tunedViTmodel.
Theset-SNEvisualizations,particularlyfortheCamelyon17dataset,clearlydemonstratethatfine-
tuningwithclass-conditionedcontrastiveinvariance(CCI)significantlyenhancesthemodel’sability
togeneralize acrossunseen domains. The fine-tuningprocess improvesdiscriminative powerby
betteraligningfeaturerepresentationswithclasslabels. Thisvisualizationunderscoresthecritical
importanceofincorporatingCCItorefinethevisionencoder. Byenhancingthealignmentoffeature
representations with their corresponding classes, CCI contributes to a more robust and domain-
invariant model. Furthermore, this refined vision encoder facilitates the selection of in-context
learning(ICL)examples,enablinglargemultimodalmodelstoadaptmoreeffectively.
6https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/
vision_transformer.py
17
TiVdenut-eniF
TiV
deniart-erP