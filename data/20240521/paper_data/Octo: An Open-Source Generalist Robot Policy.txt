Octo: An Open-Source Generalist Robot Policy
Octo Model Team
Dibya Ghosh∗,1 Homer Walke∗,1 Karl Pertsch∗,1,2 Kevin Black∗,1 Oier Mees∗,1
Sudeep Dasari3 Joey Hejna2 Tobias Kreiman1 Ria Doshi1 Charles Xu1 Jianlan Luo1 You Liang Tan1
Pannag Sanketi4 Quan Vuong4 Ted Xiao4 Dorsa Sadigh2 Chelsea Finn2 Sergey Levine1
1UC Berkeley 2Stanford 3Carnegie Mellon University 4Google Deepmind
https://octo-models.github.io
Flexible Task Definitions Flexible Action Spaces
800k Robot Trajectories
Goal Image Language
Instruction
Octo End-Effector Control
Flexible Observations
Generalist Robot Policy
Wrist & 3rd Person
Proprio Joint Control
Camera
Out-of-the-box Multi-Robot Control Efficient Finetuning with new observation + action spaces
WidowX Rearrange UR5 Table Top RT-1 Robot Berkeley Insertion Stanford Coffee CMU Baking Berkeley Bimanual
Fig. 1: We introduce Octo, an open-source, generalist policy for robotic manipulation. Octo is a transformer-based policy pretrained on 800k
diverse robot episodes from the Open X-Embodiment dataset [67]. It supports flexible task and observation definitions and can be quickly
finetuned to new observation and action spaces.
Abstract—Large policies pretrained on diverse robot datasets experimentsacross9roboticplatforms,wedemonstratethatOcto
have the potential to transform robotic learning: instead of serves as a versatile policy initialization that can be effectively
training new policies from scratch, such generalist robot policies finetuned to new observation and action spaces. We also perform
may be finetuned with only a little in-domain data, yet generalize detailed ablations of design decisions for the Octo model, from
broadly. However, to be widely applicable across a range of architecturetotrainingdata,toguidefutureresearchonbuilding
robotic learning scenarios, environments, and tasks, such policies generalist robot models.
need to handle diverse sensors and action spaces, accommodate a
variety of commonly used robotic platforms, and finetune readily I. INTRODUCTION
and efficiently to new domains. In this work, we aim to lay Thecommonapproachforroboticlearningistotrainpolicies
the groundwork for developing open-source, widely applicable,
on datasets collected for the specific robot and task at hand.
generalist policies for robotic manipulation. As a first step, we
Learning from scratch in this way requires significant data
introduce Octo, a large transformer-based policy trained on
800k trajectories from the Open X-Embodiment dataset, the collectioneffortforeachtask,andtheresultingpoliciesusually
largest robot manipulation dataset to date. It can be instructed exhibit only narrow generalization. In principle, collected
via language commands or goal images and can be effectively
finetuned to robot setups with new sensory inputs and action ∗Leadauthors,orderedalphabetically,seeSectionAforlistofcontributions.
spaces within a few hours on standard consumer GPUs. In Correspondenceto{dibya.ghosh, homer_walke, pertsch, kvablack,
oier.mees}@berkeley.edu
4202
yaM
02
]OR.sc[
1v31221.5042:viXraexperience from other robots and tasks offers a possible particular combination of these components into a powerful
solution, exposing models to a diverse set of robotic control generalist robot policy is unique and novel.
problems that may improve generalization and performance on We demonstrate through extensive experiments on 9 robots
downstream tasks. However, even as general-purpose models across 4 institutions that our combined system leads to state-
become ubiquitous in natural language [68, 88]) and computer of-the-art performance for out-of-the-box multi-robot control
vision[76,44],ithasprovenchallengingtobuildtheanalogous for single and dual-arm manipulation tasks and that Octo can
“general-purpose robot model” that can control many robots be used as an effective initialization for finetuning to unseen
for many tasks. Training a unified control policy in robotics setups with new observation and action spaces. In the process,
presents unique challenges, requiring handling different robot we carefully study the effect of different design decisions
embodiments, sensor setups, action spaces, task specifications, when pretraining GRPs; we evaluate how the choice of data
environments, and compute budgets. distribution, model architecture, and policy formulation affects
Towards this direction, several works have proposed robotic the quality of the pretrained GRP. Our evaluation highlights
foundation models that directly map robot observations to the utility of scale and flexibility: our best models are those
actionsandprovidezero-shotorfew-shotgeneralizationtonew trained on the widest data mixtures, with the least restrictive
domains and robots. We broadly refer to these models as “gen- inductive biases, and with policy objectives that can fit the
eralist robot policies” (GRPs), emphasizing their ability to per- diversity of behaviors in the pretraining data.
form low-level visuomotor control across tasks, environments, Along with this paper, we release all resources required
and robotic systems [75, 9, 23, 103, 10, 81, 1, 91, 35, 94, 45]. to train, use, reproduce, and finetune an Octo model. We
For example, the GNM model [80] generalizes across different providepretrainedOctomodelcheckpointswith27Mand93M
robotic navigation scenarios, the RoboCat model [9] handles parameters that, out of the box, support multiple RGB camera
different robot embodiments for goal-conditioned tasks, and inputs as well as both language and goal image task speci-
the RT-X model [67] performs language-conditioned manipu- fication. We also provide scripts for finetuning these models
lation across five robot embodiments. Although these models on new domains, as well as our complete pretraining pipeline,
representsignificantstepstowardatrue“general-purposerobot including optimized data loaders, transformer implementations
model,” they have been limited in multiple important aspects: for multimodal inputs, and tools to monitor training progress.
they typically constrain downstream users to a pre-defined
and often restrictive set of input observations, e.g., a single
II. RELATEDWORK
camera stream; they lack support for effective finetuning to Manyworkstrainpoliciesusingalargedatasetoftrajectories
new domains; and importantly, the largest of these models are collected from a robot, from early efforts using autonomous
not available to the general public. data collection for scaling policy training [71, 48, 41, 19, 27,
We design a system for pretraining generalist robot policies 30] to more recent efforts that explore the combination of
more suitable for the diversity of interfaces in downstream modern transformer-based policies with large demonstration
robotic applications. The core of our model is a transformer datasets [10, 40, 98, 28, 83, 86]. These works primarily focus
architecture that maps arbitrary input tokens (created from on a single embodiment, while Octo trains policies on robot
observations and tasks) to output tokens (then decoded into datasets assembled across multiple embodiments, increasing
actions), which can be trained on a diverse dataset of robots theeffectivesizeofthetrainingdatasetandallowingfinetuning
and tasks. With no additional training, this policy can accept to a range of robot setups.
different camera configurations (e.g., workspace or wrist More recently, papers have focused on broadening the gen-
cameras), can control different robots, and can be guided via eralization abilities of robot policies. Multiple works leverage
either language commands or goal images — all by simply diversenon-robotdataorpretrainedvision-languagefoundation
changingwhichtokensarefedintothemodel.Mostimportantly, models to boost policy generalization to new scenes and
themodelcanbeadaptedtonewrobotsetupswithnewsensory tasks [86, 103, 96, 16, 38, 11, 84, 36, 4, 37, 7, 3, 46, 15, 23].
inputs, action spaces, or morphologies by adding appropriate More closely related to Octo are recent works that train robot
adapters and finetuning with a small target domain dataset and policies across data from multiple robot embodiments: the
an accessible compute budget. GNMmodel[81,80]generalizesacrossrobotnavigationsetups
OurprimarycontributionisOcto,atransformer-basedpolicy while RoboCat [9] and RT-X [67] control multiple single-arm
pretrained on the largest robot manipulation dataset to date: manipulation robots. While these models deliver impressive
800k robot demonstrations from the Open X-Embodiment policy learning results, a key issue is their lack of flexibility:
dataset [67]. Octo is the first GRP that can be effectively they typically require users to stick to the sensory inputs
finetuned to new observations and action spaces and the first and action space used during pretraining and do not support
generalist robot manipulation policy that is fully open-source, adaptation to new observation and action spaces. Furthermore,
including the training pipeline, model checkpoints, and data. the largest models are not publicly accessible. Octo differs
Finally, while the individual components that comprise Octo from these works in multiple aspects: it is trained on a larger
— a transformer backbone, support for both language and goal and more diverse robot data mix, it supports a wider range of
image specification, and a diffusion head to model expressive downstream applications via efficient finetuning to new robot
action distributions — have been discussed in prior work, the setups, and it is fully open source and reproducible.Task Observation Readout Observation Readout Observation
Task Tokens
Put theknife ontheplate
Language Encoder
Octo Transformer
p
T<<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""ccccRRRRpppp1111ggggeeeeZZZZuuuuYYYYuuuuBBBBjjjjRRRRGGGGffffjjjjJJJJvvvvggggVVVVXXXXYYYY////3333AAAAnnnnoooo===="""">>>>AAAAAAAAAAAABBBB9999HHHHiiiiccccbbbbVVVVDDDDLLLLSSSSggggMMMMxxxxFFFFLLLL1111TTTTXXXX7777WWWW++++qqqqiiii7777ddddBBBBIIIIvvvvggggqqqqssssyyyyIIIIooooMMMMuuuuiiiiGGGG5555ccccVVVV++++ooooJJJJ2222KKKKJJJJkkkk0000bbbbUUUUMMMMzzzzmmmmTTTTGGGG5555UUUUyyyyhhhhDDDDvvvv8888OOOONNNNCCCC0000XXXXcccc++++jjjjHHHHuuuu////BBBBsssszzzz7777SSSSyyyy00009999UUUUDDDDggggccccMMMM666699993333JJJJMMMMTTTTxxxxFFFFIIIIYYYYddddNNNN1111vvvvpppp7777CCCCxxxxuuuubbbbWWWW9999UUUU9999wwwwtttt7777eeee0000ffffHHHHBBBB6666VVVVjjjj00009999aaaaJJJJkkkkoooo0000444400000000WWWWyyyyUUUUhhhh3333AAAAmmmmqqqq4444FFFFIIIIoooo3333UUUUaaaaDDDDkkkknnnnVVVVhhhhzzzzGGGGggggaaaaSSSStttt4444PPPPJJJJffffeeeeaaaa3333pppp1111wwwwbbbbEEEEaaaakkkkGGGGzzzzmmmmLLLLuuuuhhhh3333SSSSkkkkxxxxFFFFAAAAwwwwiiiillllbbbbyyyyeeeeyyyyHHHHFFFFMMMMaaaaMMMMyyyybbbbcccczzzz7777jjjjXXXX666655554444llllbbbbddddBBBBccccgggg66668888XXXXJJJJSSSSggggRRRRzzzz1111ffffvvvvmmmmrrrrNNNN4444hhhhYYYYEEEEnnnnKKKKFFFFTTTTFFFFJJJJjjjjuuuupppp4444bbbboooo55559999SSSSjjjjYYYYJJJJJJJJPPPPiiii////1111EEEEssssNNNNjjjjyyyyiiiiZZZZ0000xxxxLLLLuuuuWWWWKKKKhhhhppppyyyy44446666eeeeLLLL0000HHHHNNNNyyyyYYYYZZZZUUUUBBBBGGGGUUUUbbbbaaaaPPPPooooVVVVkkkkooooffff7777eeeeSSSSGGGGlllloooozzzzCCCCwwwwMMMM7777GGGGQQQQWWWW0000qqqqxxxx6666mmmmffffiiiiffff111100001111wwwweeeeOOOOuuuunnnnQQQQssssUUUUJJJJccccssssWWWWWWWWhhhh4444aaaaJJJJJJJJBBBBiiiiRRRRrrrrAAAAEEEEyyyyEEEEJJJJoooozzzzllllDDDDNNNNLLLLKKKKNNNNPPPPCCCCZZZZiiiiVVVVssssTTTTDDDDVVVVllllaaaaHHHHssssqqqq2222RRRRKKKK88881111SSSS++++vvvvkkkk9999ZZZZVVVV1111XXXXOOOOrrrr3333uuuuNNNN1111ppppXXXXaaaaXXXX11111111GGGGEEEEMMMMzzzziiiiHHHHSSSS////DDDDggggBBBBmmmmrrrrwwwwAAAAHHHHVVVVooooAAAAooooMMMMnnnneeeeIIIIZZZZXXXXeeeeHHHHOOOOmmmmzzzzoooovvvvzzzz7777nnnnwwwwssssRRRRwwwwttttOOOOvvvvnnnnMMMMKKKKffff++++BBBB8888////ggggDDDDvvvv3333ZZZZIIIIxxxx<<<<////llllaaaatttteeeexxxxiiiitttt>>>> T
Action Head a Action Head a
Observation Tokens Pre-Training
Finetuning
New Observation
New Action Space
Octo Transformer
CNN
p
New Action Head a
Fig. 2: Model architecture. Left: Octo tokenizes task descriptions (green) and input observations (blue) using a pretrained
language model and a lightweight CNN, respectively. Top: The transformer backbone processes the sequence of task and
observation tokens and produces readout tokens (purple) that get passed to output heads to produce actions. Bottom: The
block-wise attention structure of the transformer allows us to add or remove inputs and outputs during finetuning: for example,
we can add new observations (blue, dashed) or action spaces (purple, dashed) without modifying any pretrained parameters.
Octo’s design is inspired by several recent advances in wecurate800kforOctotraining.WenotethattheRT-Xmodel
robot imitation learning and scalable transformer training, [67] used a more restricted subset of 350K episodes, so to the
including the use of denoising diffusion objectives [34] for best of our knowledge, Octo is trained on the largest robotics
actiondecoding[17,31,85],thepredictionof“actionchunks”, manipulation demonstration dataset to date.
i.e.,sequencesoffutureactions[98,17,28],andmodellayouts
andlearningrateschedulesinspiredbytheliteratureonscalable
III. THEOCTOMODEL
vision transformer training [22, 97]. Our work is the first to Inthissection,wedescribetheOctomodel,ouropen-source
leverage these approaches in the context of learning cross- generalist robot policy that can be adapted to new robots and
embodied generalist policies and we find that they can lead to tasks — including new sensory inputs and action spaces —
substantial performance improvements. In our evaluation, we via finetuning. We discuss the key design decisions, training
presentablationstoassesstheimportanceofthesecomponents, objectives, training dataset, and infrastructure. The design of
alongside a more comprehensive list of what we found to be the Octo model emphasizes flexibility and scale: it supports
(un)important in Appendix E; we hope our findings are useful a variety of commonly used robots, sensor configurations,
for future research on generalist policy learning. and actions while providing a generic and scalable recipe
A key ingredient for training generalist robot policies is that can be trained on large amounts of data. It also supports
robot training data. In contrast to vision and language data naturallanguageinstructions,goalimages,observationhistories,
that can be scraped from the web, obtaining robot data at and multi-modal, chunked action prediction via diffusion
scale is challenging and often involves significant investments decoding [17]. Furthermore, we designed Octo specifically
in hardware and human labor. There are multiple large robot to enable efficient finetuning to new robot setups, including
navigationandautonomousdrivingdatasets[29,95,13,87,80, robots with different action spaces and different combinations
43, 89]. In recent years, there have also been multiple efforts of cameras and proprioceptive information. This design was
for building robot manipulation datasets of increasing scale selected to make Octo a flexible and broadly applicable
and diversity, either collected via scripted and autonomous generalist robot policy that can be utilized for a variety of
policies [19, 41, 42, 12, 71, 30] or human teleoperation [59, downstream robotics applications and research projects.
60, 25, 90, 39, 10, 26, 6, 77, 63, 79]. Octo is trained on the
A. Architecture
Open X-Embodiment dataset [67], a recent effort that pooled
many of these aforementioned robot datasets. The Open-X At its core, Octo is a transformer-based policy π. It
dataset contains approximately 1.5M robot episodes, of which consists of three key parts: input tokenizers that transformDATASET SAMPLING WEIGHTS
language instructions ℓ, goals g, and observation sequences Fanuc
(cid:2) (cid:3) Iamlab Mutex CMU Stretch
o ,...,o intotokens , , (Fig.2,left);atransformer
1 H l g o DLR Edan
T T T
backbone that processes the tokens and produces embeddings Fractal
e l,e g,e o = T( l, g, o) (Fig. 2, top); and readout heads BC-Z
T T T
R(e) that produce the desired outputs, i.e., actions a. Austin Sirius
Task and observation tokenizers: We convert task defi- Austin Sailor
nitions (e.g., language instructions ℓ and goal images g) and UCSD Kitchen
observations o (e.g., wrist and third-person camera streams) Furniture Bench
into a common “tokenized” format using modality-specific NYU Franka
tokenizers (see Fig. 2, left): Austin Buds
Kuka
• Language inputs are tokenized, then passed through a Hydra
pretrained transformer that produces a sequence of lan-
guage embedding tokens. We use the t5-base (111M)
Language Table
model [74].
• Image observations and goals are passed through a
Toto
shallow convolution stack, then split into a sequence of
Autolab
flattened patches [22]. Viola Roboturk Bridge
Weassembletheinputsequenceofthetransformerbyadding NYU Door Cable Routing
Jaco Play Taco Play
learnablepositionembeddingsptotaskandobservationtokens
(cid:2) (cid:3) Fig. 3: Training dataset composition. We curate a subset of
and then arranging them sequentially , , ,... .
TT To,1 To,2 25 datasets from the Open X-Embodiment dataset that have
Transformer backbone and readout heads: Once the
image observations, end-effector actions, and show diverse
inputs have been cast to a unified token sequence, they are
behaviors. The pie chart visualizes the fractions that each
processed by a transformer (see Fig. 2, top). This is similar to
dataset contributes to every training batch on average. The
prior works that train transformer-based policies on sequences
dataset weights are determined by the number of samples in
of observations and actions [92, 73]. The attention pattern
each dataset with small modifications to balance dataset size
of the Octo transformer is block-wise masked: observation
and diversity (see Section III-B for details).
tokens can only attend causally to tokens from the same or
earlier time steps as well as task tokens (green).
o,0:t T
T T
Tokens corresponding to non-existing observations are fully
action configurations during pretraining, being able to adapt
maskedout(e.g.,adatasetwithoutlanguageinstructions).This
Octo’sinputsandoutputsduringfinetuningmakesitaversatile
modular design enables us to add and remove observations
tool for the robotics community. Prior model designs that use
or tasks during finetuning (see below). In addition to these
standard transformer backbones or fuse visual encoders with
input token blocks, we insert learned readout tokens
R,t
T MLPoutputheadslockinthetypeandorderofinputsexpected
(purple). A readout token at attends to observation and
R,t
T by the model. In contrast, switching the observation or task
task tokens before it in the sequence, but is not attended to by
for Octo does not require re-initializing most of the model.
anyobservationortasktoken—hence,theycanonlypassively
readandprocessinternalembeddingswithoutinfluencingthem.
B. Training data
Readout tokens act similarly to the [CLS] token in BERT,
serving as a compact vector embedding of the observation We train Octo on a mixture of 25 datasets from the Open X-
sequencethusfar.Alightweight“actionhead”thatimplements EmbodimentDataset[67],adiversecollectionofrobotlearning
the diffusion process is applied to the embeddings for the datasets. Our training mixture includes demonstration data of
readout tokens. This action head predicts a “chunk" of several a variety of tasks from several robot embodiments and scenes.
consecutive actions, similar to prior work [98, 17]. These datasets are heterogeneous not just in terms of the robot
Ourdesignallowsustoflexiblyaddnewtaskandobservation type, but also in the sensors (e.g., including or not including
inputs or action output heads to the model during downstream wrist cameras) and labels (e.g., including or not including
finetuning. When adding new tasks, observations, or loss language instructions). See Fig. 3 and Appendix C for the
functions downstream, we can wholly retain the pretrained detailed mixture. To create our training mixture D, we curate
weights for the transformer, only adding new positional the data by first removing all Open-X datasets that contain
embeddings, a new lightweight encoder, or the parameters no image streams, as well as those that do not use delta end-
of the new head as necessitated by the change in specification effectorcontrol.Wealsoremovedatasetsthataretoorepetitive,
(see Fig. 2, bottom). This is in contrast to prior architectures have a low image resolution, or consist of excessively niche
[10,81],whereaddingorremovinganimageinputorchanging tasks. For the remaining datasets, we roughly categorize them
thetaskspecificationwouldrequirere-initializingorre-training into “more diverse” and “less diverse” datasets based on the
large components of the pre-trained model. tasks and environments, and then double the weight of the
This flexibility is crucial to make Octo a truly “generalist” more diverse datasets during training. We also down-weight a
model: since we cannot cover all possible robot sensor and fewdatasetswithmanyrepetitiveepisodestoavoiddominatingthe mixture. Finally, we zero-pad any missing camera channels relabeling [2], which selects a state uniformly from the future
and align the gripper action spaces between the datasets such in the trajectory to assign as the goal image, similar to prior
that a gripper command of +1 means “the gripper is open” and work [54, 90, 81, 77, 63]. We apply common image data
0 means “the gripper is closed.” While we found the resulting augmentations during training, and randomly zero out the
training mixture to work well, future work should perform a language instruction or goal image per training example to
more thorough analysis of data mixture quality for pretraining enable Octo to be conditioned on either language instructions
general robot policies. or goal images. For datasets without language annotations, we
alwaysusegoalimageconditioning. Thisenablesourmodelto
C. Training objective
learn control mostly from self-supervised visual observations
We use a conditional diffusion decoding head to predict con- andreducestheburdenonlanguageannotation,similartoprior
tinuous, multi-modal action distributions [34, 17]. Importantly, work on multi-context imitation learning [54, 62, 61, 63]. For
onlyoneforwardpassofthetransformerbackboneisperformed moredetailsonthechoiceofhyperparameters,seeAppendixD.
per action prediction, after which the multi-step denoising
process is carried out entirely within the small diffusion E. Model Checkpoints & Code
head. We found this policy parameterization to outperform We open-source all resources required to train, finetune and
policies trained with MSE action heads or discretized action run our model (see https://octo-models.github.io):
distributions [10] in both zero-shot and finetuning evaluations.
• Pretrained Octo checkpoints for Octo-Small
To generate an action, we sample a Gaussian noise vector
(27M params) and Octo-Base (93M params).
(cid:0) (cid:1)
xK 0,I and apply K steps of denoising with a learned
∼N • Finetuning scripts for Octo models, in JAX.
denoisingnetworkϵ (xk,e,k)thatisconditionedontheoutput
θ • Model pretraining pipeline for Octo pretraining on the
xk of the previous denoising step, the step index k, and the
Open X-Embodiment dataset, in JAX.
output embedding e of the transformer action readout:
• Standalone data loaders for Open X-Embodiment data,
xk−1 =α(xk γϵ (xk,e,k)+ (cid:0) 0,σ2I(cid:1) ). (1) compatible with JAX and PyTorch.
θ
− N
We provide a simple example for loading and running a
The hyperparameters α, γ, and σ correspond to the noise
pretrained Octo model in Appendix B.
schedule: we use the standard cosine schedule from [66]. We
train the diffusion head using the standard DDPM objective IV. EXPERIMENTS
first proposed in [34], where we add Gaussian noise to the
Our experiments provide an empirical analysis of Octo,
dataset actions and train the denoising network ϵ (xk,e,k)
θ evaluating its ability to serve as a general robotic foundation
to reconstruct the original action. For a detailed explanation
model across several axes:
of diffusion policy training, see Chi et al. [17]. We list all
1) Can Octo control multiple robot embodiments and solve
hyperparameters in Appendix D.
language and goal tasks out of the box?
We use the same diffusion training objective during finetun-
2) Do Octo weights serve as a good initialization for data-
ing and update the full model, a recipe which outperformed
efficient finetuning to new tasks and robots, and does it
those that freeze subsets of the pretrained parameters. In all
improve over training from scratch and commonly used
finetuning experiments, we employ the same recipe: given a
pretrained representations?
small target domain dataset with around 100 trajectories, we
3) Which design decisions in Octo matter most for building
finetune for 50k steps using a cosine decay learning rate decay
generalist robot policies?
with linear warmup.
Evaluation setups: We evaluate Octo’s capabilities across
D. Training Details
a representative spectrum of 9 robot learning setups at 4 insti-
We trained two variants of our model: Octo-Small with a tutions (see Fig. 4). We test Octo’s ability to control different
transformer backbone that mirrors the size of a ViT-S, and robotsout-of-the-box(“zero-shot”)forlanguageandgoalimage
Octo-Base with a transformer backbone that mirrors the size tasks using robot setups that match the pretraining data, where
of a ViT-B [22]. all robots are controlled with delta end-effector control actions
We use the AdamW optimizer [51] with an inverse square and the observation spaces are RGB images. We also evaluate
root decay learning rate schedule [97], with weight decay of Octo for data-efficient finetuning to new environments and
0.1 and gradient clipping of 1.0. The ViT-B was trained for tasks, including with new observations (force-torque inputs
300k steps with a batch size of 2048 using a TPU v4-128 pod, in “Berkeley Insertion”), new action spaces (joint position
which took 14 hours. A finetuning run of the same model on control in “Berkeley Pick-Up”) and new robot embodiments
a single NVIDIA A5000 GPU with 24GB of VRAM takes (“Berkeley Coke” and “Berkeley Bimanual”). Each of the
approximately 5 hours and can be sped up with multi-GPU finetuning setups uses 100 in-domain demonstrations and
∼
training. finetunes in <5 hours on a NVIDIA A5000 GPU, using the
We train using 2 frames of observation history; in our same hyperparameters across all setups (see Appendix D). Our
preliminary experiments, we found significantly diminishing evaluation tasks test Octo’s ability to interact with diverse
gains beyond the first additional frame. We use hindsight goal objects (e.g., “WidowX BridgeV2”), solve long-horizon tasksWidowX BridgeV2 UR5 Tabletop RT-1 Robot Berkeley Insert Stanford Coffee CMU Baking
Berkeley Pick-up Berkeley Coke Berkeley Bimanual
0-Shot Evaluation Finetuning Evaluation
Fig. 4: Evaluation Tasks. We evaluate Octo on 9 real robot setups across 4 institutions. Our evaluations capture diverse object
interactions (e.g., “WidowX BridgeV2”), long task horizons (e.g., “Stanford Coffee”) and precise manipulation (e.g., “Berkeley
Peg Insertion”). We evaluate Octo’s capabilities to control robots in environments from the pretraining data out-of-the-box
and to efficiently finetune to new tasks and environments with small target domain datasets. We also test finetuning with new
observations (force-torque inputs for “Berkeley Peg Insertion”), action spaces (joint position control in “Berkeley Pick-Up” and
“Berkeley Bimanual”) and new robot embodiments (e.g., “Berkeley Bimanual” and “Berkeley Coke”).
(e.g.,“StanfordCoffee”)andperformprecisemanipulation(e.g., Majumdar et al. [57]. A ViT-B visual encoder is initialized to
“BerkeleyInsertion”).Formoredetailsoneachevaluationsetup, the VC-1 weights [57], a state-of-the-art visual representation
see Appendix F. pretrained on 4,000 hours of ego-centric videos and ImageNet,
Comparisons: We compare Octo’s ability to control and combined with an MLP action decoder. The full model is
multiple robots out-of-the-box to the best openly available trained to predict expert actions using an MSE loss (“VC-1”).
generalist robot policy, RT-1-X [67], using the released
A. Octo Controls Multiple Robots Out-of-the-Box
checkpoint. Similar to Octo, RT-1-X is pretrained on the Open
X-Embodiment robot dataset and aims to control multiple
robots zero-shot, thus providing a natural point of comparison. Success
We also compare the zero-shot capabilities of Octo to RT- Rate
2-X, a 55 billion parameter vision-language model finetuned RT-1-X (35M)
on the Open X-Embodiment dataset to produce robot actions. 1.0 Octo (93M)
The RT-1-X and RT-2-X models [67] are trained on a more RT-2-X (55B)
0.8
restricted subset of 350K episodes (compared to 800k episodes
for Octo). We further compare Octo’s performance as a policy 0.6
initialization for data efficient finetuning to two common
0.4
approaches: (1) training on the target domain demonstrations
from scratch and (2) using pretrained visual representations.
0.2
Whileanumberofpriorworkshaveproposedotherpretraining
schemesforimitationfinetuning[25,24,26],toourknowledge
WidowX UR5 RT-1 Robot
no prior method provides a pretrained policy that has been
demonstrated to finetune successfully to new observation and Fig.5:Zero-ShotEvaluation.Out-of-the-box,Octocancontrol
action spaces. However, pretrained visual representations such multiple robots in environments from the pretraining data.
as VC-1 [56] have been used in this way, and therefore we Whenusingnaturallanguagetospecifytasks,Octooutperforms
use these methods as another point of comparison. RT-1-X [67], the current best openly available generalist robot
For finetuning, we found that training our large transformer policy across three different robot embodiments and setups.
architecture from scratch overfit quickly on the small datasets. Octo also performs similarly to RT-2-X [103] on the tested
Instead, we obtained better from-scratch results using a WidowX and RT-1 Robot tasks.1
canonicalpolicyarchitectureemployedbymanypriorworks:a
ResNet visual encoder with FiLM [70] language conditioning, We compare the zero-shot manipulation capabilities of Octo,
combined with a small transformer action decoder trained RT-1-X, and RT-2-X in Fig. 5. We evaluated on several tasks
with a diffusion objective, similar to [10, 98, 17, 55]. Our selected from the pre-training dataset including picking and
instantiation of this architecture has 28M parameters (similar
to RT-1 [10]). We adopt this as our from-scratch baseline 1For the WidowX, since RT-2-X is not openly available, we report the
RT-2-Xnumbersfrom[7](dashedbar)andusethesametasksfortheOcto
(“ResNet+Transformer Scratch”). We also compare to a
andRT-1-Xevaluations.FortheRT-1Robot,theauthorsofRT-2-Xkindly
pretrained visual representation following the procedure of performedtheevaluationsforus.BerkeleyInsertion∗ StanfordCoffee CMUBaking BerkeleyPick-Up† BerkeleyCoke BerkeleyBimanual† Average
ResNet+TransformerScratch 10% 45% 25% 0% 20% 20% 20%
VC-1[57] 5% 0% 30% 0% 10% 50% 15%
Octo(Ours) 70% 75% 50% 60% 100% 80% 72%
TABLE I: Finetuning Evaluation. Octo enables data-efficient finetuning to new domains and out-performs training from
scratch as well as state-of-the-art pretrained visual representations. Each domain uses 100 target demonstrations and the same
∼
finetuning hyperparameters. In each domain, success rates are averaged over 20 trials. : New observation input (force-torque
∗
proprioception). : New action space (joint position control).
†
placing, wiping a table with a cloth, and opening and closing AggregatePerformance
drawers. For each robot, we selected two language tasks from Octo-Small(Ours) 83%
thecorrespondingOXEdatasetandperformed10trialspertask
RT-Xdatasetmix[67] 60%
with varying initial conditions (details in Appendix F). The
Singlerobotdataset(BridgeData) 43%
chosen tasks are “in-distribution” from the pre-training data,
buttheevaluationrequiresmethodstogeneralizetonewobject DiscretizedActionPrediction[67] 18%
positions, lighting conditions, backgrounds, and distractor ContinuousActionPrediction(MSE) 35%
objects. While all methods acted reasonably across tasks in the
Resnet-50+Transformer[67] 70%
pretraining environments, we found that on average Octo had
a 29% higher success rate than RT-1-X (35M parameters). For
the WidowX and RT-1 Robot evaluations, we also compared TABLE II: Model Ablations. We achieve best performance
to RT-2-X (55 billion parameters) [103] and found that Octo when using the ViT architecture, diffusion action head, and
performed similarly. widetrainingdatamixture.Allevaluationsareperformedonthe
Additionally, while RT-1-X and RT-2-X only support condi- WidowXsetup.Successratesareaveragedover40trialsacross
tioning on language instructions, Octo also supports condition- twolanguage-conditionedtasksandtwogoal-conditionedtasks.
ing on goal images. We evaluated our model on the WidowX
tasks using goal image conditioning and found that it achieved
a 25% higher success rate than when evaluated with language C. Design Decisions for Generalist Robot Policy Training
conditioning. This is likely because goal images provide more
We have demonstrated the effectiveness of Octo as a zero-
information about how to achieve the task. In the BridgeV2
shot multi-robot controller and as an initialization for policy
domain, we performed a fine-grained analysis of the zero-shot
finetuning. We next analyze the effects of different design
capabilitiesinTableVII;measuringperformanceonsetupsseen
decisions on the performance of the Octo policy. Concretely,
in the dataset, and for novel environments, scenes, and skills.
we focus on the following aspects: (1) model architecture,
While the Octo model achieves high success on novel objects,
(2) training data, (3) training objective, and (4) model scale.
zero-shot performance slightly degrades in a new scene, and
Unless noted otherwise, we perform all ablations on the Octo-
high degradation for novel behaviors like flipping or precise
Small model due to our compute budget.
insertion.
Model architecture: Prior transformer-based policy de-
signs typically encode input images with large ResNet-
B. Octo Enables Data-Efficient Learning in New Domains style [32] encoders and fuse the resulting image features with
a comparatively small transformer [10, 67, 81, 17, 98, 61, 83].
We report data-efficient finetuning results to new domains Instead, we opt for a “transformer-first” architecture that uses
in Table I. We find that finetuning Octo leads to better veryshallowCNNpatchencodersandconcentratesmostofthe
policies than starting from scratch or with the pretrained VC-1 parameters and FLOPS in the transformer backbone, similar
weights. On average across the six evaluation setups (detailed to canonical vision transformer architectures [22]. In Table II
in Appendix F), Octo outperforms the next best baseline by we show that this scalable architecture leads to substantially
52%.Importantly,weusethesamerecipeandhyperparameters improved performance when training on the full Open X-
for fine-tuning Octo on all evaluation tasks (see Section III-C), Embodiment data mix. Importantly, we found ResNet-based
making this a good default configuration. architecturestoperformbetterthanViTswhentrainingonsmall
The results also underline Octo’s ability to accommodate datasets, e.g., in our “from scratch” comparisons, underlining
newobservations(force-torqueinputsfor“BerkeleyInsertion”), that large transformer policies are uniquely suited for scalable
action spaces (joint position control for “Berkeley Pick-Up”) training on diverse datasets.
and new robot embodiments (“Berkeley Coke” and “Berkeley Training data: Octo is trained on the most diverse cross-
Bimanual”). This makes Octo applicable to a wide range of embodied robot dataset to date, a mix of 25 datasets that we
single and dual arm robotic manipulation problems that go manually curated from the Open X-Embodiment dataset [67]
beyond a single camera input and end-effector position control. (see Section III-B). We ablate the impact of this training mix
ATAD
YCILOP
HCRASuccess Rate design enables finetuning to new inputs and action spaces,
(0-shot) Octo making Octo a versatile initialization for a wide range of
Base robotic control problems. Apart from the model itself, we have
0.8 released our full training and finetuning code, alongside tools
that make it easier to train on large robot datasets.
While Octo achieves strong performance in both zero-shot
0.6
Octo
and finetuning evaluations, we find that the current model still
Small
has several short-comings, which we attribute in large parts
0.4 to characteristics of the training data. First, we found that the
Octo UR5 current Octo model struggles with adequately processing wrist
0.2 Tiny WidowX camera information. Often finetuning results were stronger
when using only a third person camera instead of combining
third person and wrist camera. Additionally, we notice a large
difference between language-conditioned policy performance
20 40 60 80 100 # Params and goal-conditioned policy performance. In both cases, a
(M) lack of the respective modalities in the training data is the
likely reason: only 27% of the data contains wrist camera
Fig.6:ModelScaling.TheperformanceofOctoimproveswith
information and only 56% of the pretraining data contains
larger model sizes on both UR5 and WidowX tasks. Success
language annotations.
rates are averaged over 10 trials on one language-conditioned
Expanding the data used to train Octo is a natural avenue
task per robot.
of improvement. Since the Open X-Embodiment dataset is
comprised of optimal robot demonstrations, the current model
trains via imitation; future work may consider learning from
by comparing to Octo models trained on a smaller mix of
sub-optimal or online interaction data that require alternative
11datasetsusedintrainingtheRT-Xmodels[67]andabaseline
objectives. Further, while we trained and evaluated Octo
trained only on data from the target robot domain. In Table II
exclusively on single and dual-arm manipulators; expanding
weshowthattheperformanceofOctoincreasesasweincrease
to a wider set of robots that perform navigation or mobile
the number of training datasets. This suggests that expanding
manipulation would be an direction of high opportunity.
the data mix to even more datasets may further improve policy
While Octo represents a step towards building generalist
performance. We will leave this for future work, along with a
robot policies that work out-of-the-box on diverse robot setups,
more thorough investigation of best practices for data curation.
there remains work to improve the model, including better
Trainingobjective: WecompareOcto’sdiffusiondecoding
language conditioning, improved support for wrist cameras,
training objective (see Section III-C) to common alternatives
and incorporating data beyond optimal demonstrations. We
from prior work: simple MSE loss [8, 47] and cross-entropy
hope that Octo offers a simple launchpad for researchers and
loss on discretized actions [10, 103]. In Table II we find
practitioners to access larger robotic datasets and leverage
that Octo’s diffusion training objective leads to substantially
pretrained robotics models for efficient learning of new tasks
improved performance. This improvement is likely because
and broad generalization.
the diffusion head can model multi-modal action distributions
(unlike the MSE head) while retaining the precision of
REFERENCES
continuous actions (unlike the discrete head). Qualitatively,
the policy acts more decisively than MSE-trained policies, and [1] Scale AI. Introducing scale’s automotive foundation
more precisely than those trained with discretized actions. model, 2023. URL https://scale.com/blog/afm1.
Model scale: We compare Octo models of three different [2] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas
sizes following the ladder of common vision transformer mod- Schneider, Rachel Fong, Peter Welinder, Bob McGrew,
els [97]: Octo-Tiny (10M), Octo-Small (27M), and Octo-Base Josh Tobin, Pieter Abbeel, and Wojciech Zaremba.
(93M). In Figure 6 we show that the zero-shot performance of Hindsight experience replay. In NeurIPS, 2017.
the policy scales with increasing model size. We find that the [3] Shikhar Bahl, Abhinav Gupta, and Deepak Pathak.
Base model is more robust to initial scene configuration than Human-to-robot imitation in the wild. arXiv preprint
the Small model, and is less prone to early grasp attempts, arXiv:2207.09450, 2022.
indicating the larger model has better visual scene perception. [4] Shikhar Bahl, Russell Mendonca, Lili Chen, Unnat Jain,
and Deepak Pathak. Affordances from human videos as
V. DISCUSSIONANDFUTUREWORK
a versatile representation for robotics. In CVPR, 2023.
We introduced Octo, a large transformer-based policy pre- [5] Suneel Belkhale, Yuchen Cui, and Dorsa Sadigh. Hydra:
trained on the largest robot manipulation dataset to date, 800k Hybrid robot actions for imitation learning. arxiv, 2023.
robottrajectories.WedemonstratedthatOctocansolveavariety [6] Homanga Bharadhwaj, Jay Vakil, Mohit Sharma, Ab-
of tasks out-of-the-box and showed how Octo’s compositional hinav Gupta, Shubham Tulsiani, and Vikash Kumar.Roboagent:Generalizationandefficiencyinrobotmanip- [17] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric
ulation via semantic augmentations and action chunking. Cousineau, Benjamin Burchfiel, and Shuran Song. Dif-
arXiv preprint arXiv:2309.01918, 2023. fusion policy: Visuomotor policy learning via action
[7] Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, diffusion. In Proceedings of Robotics: Science and
Homer Walke, Chelsea Finn, Aviral Kumar, and Sergey Systems (RSS), 2023.
Levine. Zero-shot robotic manipulation with pre- [18] Zichen Jeff Cui, Yibin Wang, Nur Muhammad Mahi
trained image-editing diffusion models. arXiv preprint Shafiullah, and Lerrel Pinto. From play to policy:
arXiv:2310.10639, 2023. Conditional behavior generation from uncurated robot
[8] Mariusz Bojarski, Davide Del Testa, Daniel data. In The Eleventh International Conference on
Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Learning Representations, 2022.
Goyal, Lawrence D Jackel, Mathew Monfort, Urs [19] SudeepDasari,FrederikEbert,StephenTian,SurajNair,
Muller, Jiakai Zhang, et al. End to end learning for BernadetteBucher,KarlSchmeckpeper,SiddharthSingh,
self-driving cars. arXiv preprint arXiv:1604.07316, Sergey Levine, and Chelsea Finn. Robonet: Large-scale
2016. multi-robot learning. In Conference on Robot Learning,
[9] Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, pages 885–897. PMLR, 2020.
ColineDevin,AlexXLee,MariaBauza,TodorDavchev, [20] Shivin Dass, Jullian Yapeter, Jesse Zhang, Jiahui Zhang,
YuxiangZhou,AgrimGupta,AkhilRaju,etal. Robocat: Karl Pertsch, Stefanos Nikolaidis, and Joseph J. Lim.
A self-improving foundation agent for robotic manipu- CLVR jaco play dataset, 2023. URL https://github.com/
lation. arXiv preprint arXiv:2306.11706, 2023. clvrai/clvr_jaco_play_dataset.
[10] Anthony Brohan, Noah Brown, Justice Carbajal, Yev- [21] Pim de Haan, Dinesh Jayaraman, and Sergey Levine.
gen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Causal confusion in imitation learning. NeurIPS, 2019.
Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine [22] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,
Hsu, et al. Rt-1: Robotics transformer for real-world Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
control at scale. arXiv preprint arXiv:2212.06817, 2022. Mostafa Dehghani, Matthias Minderer, Georg Heigold,
[11] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Sylvain Gelly, et al. An image is worth 16x16 words:
Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Transformers for image recognition at scale. arXiv
AlexIrpan,EricJang,RyanJulian,etal. Doasican,not preprint arXiv:2010.11929, 2020.
as i say: Grounding language in robotic affordances. In [23] DannyDriess,FeiXia,MehdiSMSajjadi,CoreyLynch,
Conference on Robot Learning, pages 287–318. PMLR, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,
2023. JonathanTompson,QuanVuong,TianheYu,etal. Palm-
[12] Serkan Cabi, Sergio Gómez Colmenarejo, Alexander e: An embodied multimodal language model. arXiv
Novikov, Ksenia Konyushkova, Scott Reed, Rae Jeong, preprint arXiv:2303.03378, 2023.
Konrad Zolna, Yusuf Aytar, David Budden, Mel Vecerik, [24] Maximilian Du, Suraj Nair, Dorsa Sadigh, and Chelsea
Oleg Sushkov, David Barker, Jonathan Scholz, Misha Finn. Behavior retrieval: Few-shot imitation learning
Denil, Nando de Freitas, and Ziyu Wang. Scaling data- by querying unlabeled datasets. ArXiv, abs/2304.08742,
driven robotics with reward sketching and batch rein- 2023. URL https://api.semanticscholar.org/CorpusID:
forcement learning. arXiv preprint arXiv:1909.12200, 258186973.
2019. [25] Frederik Ebert, Yanlai Yang, Karl Schmeckpeper,
[13] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Bernadette Bucher, Georgios Georgakis, Kostas Dani-
Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, ilidis, Chelsea Finn, and Sergey Levine. Bridge data:
YuPan,GiancarloBaldan,andOscarBeijbom. nuscenes: Boosting generalization of robotic skills with cross-
A multimodal dataset for autonomous driving. In domaindatasets. arXivpreprintarXiv:2109.13396,2021.
Proceedings of the IEEE/CVF conference on computer [26] Hao-Shu Fang, Hongjie Fang, Zhenyu Tang, Jirong Liu,
vision and pattern recognition, pages 11621–11631, Chenxi Wang, Junbo Wang, Haoyi Zhu, and Cewu Lu.
2020. Rh20t: A comprehensive robotic dataset for learning
[14] Lawrence Yunliang Chen, Simeon Adebola, and Ken diverse skills in one-shot. Towards Generalist Robots:
Goldberg. Berkeley UR5 demonstration dataset. https: Learning Paradigms for Scalable Skill Acquisition@
//sites.google.com/view/berkeley-ur5/home. CoRL2023, 3:5, 2023.
[15] William Chen, Oier Mees, Aviral Kumar, and Sergey [27] Chelsea Finn and Sergey Levine. Deep visual foresight
Levine. Vision-languagemodelsprovidepromptablerep- for planning robot motion. In 2017 IEEE International
resentations for reinforcement learning. arXiv preprint Conference on Robotics and Automation (ICRA), pages
arXiv:2402.02651, 2024. 2786–2793. IEEE, 2017.
[16] Zoey Chen, Sho Kiami, Abhishek Gupta, and Vikash [28] Zipeng Fu, Tony Z Zhao, and Chelsea Finn. Mobile
Kumar. Genaug: Retargeting behaviors to unseen aloha: Learning bimanual mobile manipulation with
situations via generative augmentation. arXiv preprint low-cost whole-body teleoperation. arXiv preprint
arXiv:2302.06671, 2023. arXiv:2401.02117, 2024.[29] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are 23–29Jul2023. URLhttps://proceedings.mlr.press/v202/
we ready for autonomous driving? the kitti vision jiang23b.html.
benchmark suite. In 2012 IEEE conference on computer [41] Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian
vision and pattern recognition, pages 3354–3361. IEEE, Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen,
2012. Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke,
[30] Abhinav Gupta, Adithyavairavan Murali, et al. QT-Opt: Scalable deep reinforcement learning
Dhiraj Prakashchand Gandhi, and Lerrel Pinto. for vision-based robotic manipulation. arXiv preprint
Robot learning in homes: Improving generalization and arXiv:1806.10293, 2018.
reducing dataset bias. Advances in neural information [42] Dmitry Kalashnikov, Jake Varley, Yevgen Chebotar,
processing systems, 31, 2018. Benjamin Swanson, Rico Jonschkowski, Chelsea Finn,
[31] HuyHa,PeteFlorence,andShuranSong. Scalingupand Sergey Levine, and Karol Hausman. Scaling up multi-
distilling down: Language-guided robot skill acquisition. task robotic reinforcement learning. In 5th Annual
In Conference on Robot Learning, pages 3766–3777. Conference on Robot Learning, 2021.
PMLR, 2023. [43] Haresh Karnan, Anirudh Nair, Xuesu Xiao, Garrett
[32] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Warnell, Sören Pirk, Alexander Toshev, Justin Hart,
Sun. Deep residual learning for image recognition. In Joydeep Biswas, and Peter Stone. Socially compliant
Proceedings of the IEEE conference on computer vision navigation dataset (scand): A large-scale dataset of
and pattern recognition, pages 770–778, 2016. demonstrations for social navigation. IEEE Robotics
[33] Minho Heo, Youngwoon Lee, Doohyun Lee, and and Automation Letters, 7(4):11807–11814, 2022.
Joseph J. Lim. Furniturebench: Reproducible real-world [44] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi
benchmark for long-horizon complex manipulation. In Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,
Robotics: Science and Systems, 2023. Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo,
[34] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising et al. Segment Anything, April 2023.
diffusion probabilistic models. Advances in neural [45] Vikash Kumar, Rutav Shah, Gaoyue Zhou, Vincent
information processing systems, 33:6840–6851, 2020. Moens, Vittorio Caggiano, Abhishek Gupta, and Ar-
[35] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, avind Rajeswaran. Robohive: A unified framework
George Fedoseev, Alex Kendall, Jamie Shotton, and for robot learning. In Thirty-seventh Conference on
Gianluca Corrado. Gaia-1: A generative world model Neural Information Processing Systems Datasets and
for autonomous driving, 2023. Benchmarks Track, 2023. URL https://openreview.net/
[36] ChenguangHuang,OierMees,AndyZeng,andWolfram forum?id=0H5fRQcpQ7.
Burgard. Visual language maps for robot navigation. In [46] Teyun Kwon, Norman Di Palo, and Edward Johns.
2023 IEEE International Conference on Robotics and Language models as zero-shot trajectory generators.
Automation (ICRA), pages 10608–10615. IEEE, 2023. arXiv preprint arXiv:2310.11604, 2023.
[37] ChenguangHuang,OierMees,AndyZeng,andWolfram [47] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter
Burgard. Audio visual language maps for robot naviga- Abbeel. End-to-endtrainingofdeepvisuomotorpolicies.
tion. In Proceedings of the International Symposium on TheJournalofMachineLearningResearch,17(1):1334–
Experimental Robotics (ISER), Chiang Mai, Thailand, 1373, 2016.
2023. [48] Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian
[38] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Ibarz, and Deirdre Quillen. Learning hand-eye coor-
Li, Jiajun Wu, and Li Fei-Fei. Voxposer: Composable dination for robotic grasping with deep learning and
3d value maps for robotic manipulation with language large-scale data collection. The International journal of
models. arXiv preprint arXiv:2307.05973, 2023. robotics research, 37(4-5):421–436, 2018.
[39] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, [49] Yixin Lin, Austin S. Wang, Giovanni Sutanto, Ak-
FrederikEbert,CoreyLynch,SergeyLevine,andChelsea shara Rai, and Franziska Meier. Polymetis. https:
Finn. Bc-z: Zero-shot task generalization with robotic //facebookresearch.github.io/fairo/polymetis/, 2021.
imitation learning. In Conference on Robot Learning, [50] Huihan Liu, Soroush Nasiriany, Lance Zhang, Zhiyao
pages 991–1002. PMLR, 2022. Bao, and Yuke Zhu. Robot learning on the job: Human-
[40] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi in-the-loop autonomy and learning during deployment.
Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima In Robotics: Science and Systems (RSS), 2023.
Anandkumar, Yuke Zhu, and Linxi Fan. VIMA: Robot [51] Ilya Loshchilov and Frank Hutter. Decoupled weight
manipulation with multimodal prompts. In Andreas decay regularization. In International Conference on
Krause, Emma Brunskill, Kyunghyun Cho, Barbara Learning Representations, 2018.
Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, [52] Jianlan Luo, Charles Xu, Xinyang Geng, Gilbert Feng,
Proceedings of the 40th International Conference on KuanFang,LiamTan,StefanSchaal,andSergeyLevine.
Machine Learning, volume 202 of Proceedings of Ma- Multi-stage cable routing through hierarchical imitation
chine Learning Research, pages 14975–15022. PMLR, learning. arXiv preprint arXiv:2307.08927, 2023.[53] Jianlan Luo, Charles Xu, Fangchen Liu, Liam Tan, UK, 2023.
Zipeng Lin, Jeffrey Wu, Pieter Abbeel, and Sergey [64] Russell Mendonca, Shikhar Bahl, and Deepak Pathak.
Levine. FMB: A functional manipulation bench- Structured world models from human videos. CoRL,
mark for generalizable robotic learning. https:// 2023.
functional-manipulation-benchmark.github.io, 2023. [65] Soroush Nasiriany, Tian Gao, Ajay Mandlekar, and
[54] CoreyLynchandPierreSermanet.Languageconditioned Yuke Zhu. Learning and retrieval from prior data for
imitation learning over unstructured data. In RSS, 2021. skill-based imitation learning. In Conference on Robot
[55] Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Learning (CoRL), 2022.
Ding, James Betker, Robert Baruch, Travis Armstrong, [66] Alexander Quinn Nichol and Prafulla Dhariwal. Im-
and Pete Florence. Interactive language: Talking to proved denoising diffusion probabilistic models. In
robots in real time. IEEE Robotics and Automation International Conference on Machine Learning, pages
Letters, 2023. 8162–8171. PMLR, 2021.
[56] Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, [67] Open X-Embodiment Collaboration, Abhishek Padalkar,
Yecheng Jason Ma, Claire Chen, Sneha Silwal, Aryan Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Her-
Jain, Vincent-Pierre Berges, Pieter Abbeel, Jitendra zog, Alex Irpan, Alexander Khazatsky, Anant Rai,
Malik, Dhruv Batra, Yixin Lin, Oleksandr Maksymets, AnikaitSingh,AnthonyBrohan,AntoninRaffin,Ayzaan
AravindRajeswaran,andFranziskaMeier. Wherearewe Wahid,BenBurgess-Limerick,BeomjoonKim,Bernhard
in the search for an artificial visual cortex for embodied Schölkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea
intelligence? 2023. Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang,
[57] Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin,
Yecheng Jason Ma, Claire Chen, Sneha Silwal, Aryan Danny Driess, Deepak Pathak, Dhruv Shah, Dieter
Jain, Vincent-Pierre Berges, Pieter Abbeel, Jitendra Büchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward
Malik, et al. Where are we in the search for an artificial Johns, Federico Ceola, Fei Xia, Freek Stulp, Gaoyue
visual cortex for embodied intelligence? arXiv preprint Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan,
arXiv:2303.18240, 2023. Giulio Schiavi, Hao Su, Hao-Shu Fang, Haochen Shi,
[58] Ajay Mandlekar, Yuke Zhu, Animesh Garg, Jonathan Heni Ben Amor, Henrik I Christensen, Hiroki Furuta,
Booher, Max Spero, Albert Tung, Julian Gao, John Homer Walke, Hongjie Fang, Igor Mordatch, Ilija
Emmons, Anchit Gupta, Emre Orbay, Silvio Savarese, Radosavovic, Isabel Leal, Jacky Liang, Jaehyung Kim,
and Li Fei-Fei. RoboTurk: A crowdsourcing platform Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey
for robotic skill learning through imitation. CoRR, Bingham,JiajunWu,JialinWu,JianlanLuo,JiayuanGu,
abs/1811.02790, 2018. URL http://arxiv.org/abs/1811. Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson,
02790. Jonathan Yang, Joseph J. Lim, João Silvério, Junhyek
[59] Ajay Mandlekar, Yuke Zhu, Animesh Garg, Jonathan Han, Kanishka Rao, Karl Pertsch, Karol Hausman,
Booher, Max Spero, Albert Tung, Julian Gao, John Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg,
Emmons,AnchitGupta,EmreOrbay,etal. Roboturk:A Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka,
crowdsourcingplatformforroboticskilllearningthrough Kevin Zhang, Keyvan Majd, Krishan Rana, Krishnan
imitation. In Conference on Robot Learning, pages 879– Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto,
893. PMLR, 2018. Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka,
[60] Ajay Mandlekar, Soroush Nasiriany, Bowen Wen, Ireti- MaximilianDu,MichaelAhn,MingtongZhang,Mingyu
ayo Akinola, Yashraj Narang, Linxi Fan, Yuke Zhu, and Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin
Dieter Fox. Mimicgen: A data generation system for Kim,NaoakiKanazawa,NicklasHansen,NicolasHeess,
scalable robot learning using human demonstrations. In Nikhil J Joshi, Niko Suenderhauf, Norman Di Palo,
7th Annual Conference on Robot Learning, 2023. Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver
[61] Oier Mees, Lukas Hermann, and Wolfram Burgard. Kroemer, Pannag R Sanketi, Paul Wohlhart, Peng Xu,
What matters in language conditioned robotic imitation Pierre Sermanet, Priya Sundaresan, Quan Vuong, Rafael
learning over unstructured data. IEEE Robotics and Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín,
Automation Letters, 7(4):11205–11212, 2022. Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Ju-
[62] Oier Mees, Lukas Hermann, Erick Rosete-Beas, and lian, Samuel Bustamante, Sean Kirmani, Sergey Levine,
Wolfram Burgard. Calvin: A benchmark for language- Sherry Moore, Shikhar Bahl, Shivin Dass, Shuran Song,
conditioned policy learning for long-horizon robot ma- Sichun Xu, Siddhant Haldar, Simeon Adebola, Simon
nipulation tasks. IEEE Robotics and Automation Letters Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker,
(RA-L), 7(3):7327–7334, 2022. StephenTian,SudeepDasari,SuneelBelkhale,Takayuki
[63] Oier Mees, Jessica Borja-Diaz, and Wolfram Burgard. Osa, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao,
Grounding language with visual affordances over un- Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao,
structureddata. InProceedingsoftheIEEEInternational Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent
ConferenceonRoboticsandAutomation(ICRA),London, Vanhoucke,WeiZhan,WenxuanZhou,WolframBurgard,Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, [79] Nur Muhammad Mahi Shafiullah, Anant Rai, Haritheja
Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Etukuru,YiqianLiu,IshanMisra,SoumithChintala,and
Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Lerrel Pinto. On bringing robots home, 2023.
Cho, Youngwoon Lee, Yuchen Cui, Yueh hua Wu, [80] Dhruv Shah, Ajay Sridhar, Arjun Bhorkar, Noriaki
Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Hirose, and Sergey Levine. Gnm: A general navigation
Yutaka Matsuo, Zhuo Xu, and Zichen Jeff Cui. Open model to drive any robot. In 2023 IEEE International
X-Embodiment: Robotic learning datasets and RT-X Conference on Robotics and Automation (ICRA), pages
models. https://arxiv.org/abs/2310.08864, 2023. 7226–7233. IEEE, 2023.
[68] OpenAI. GPT-4 Technical Report, March 2023. [81] Dhruv Shah, Ajay Sridhar, Nitish Dashora, Kyle Sta-
[69] Jyothish Pari, Nur Muhammad Shafiullah, Sridhar Pan- chowicz, Kevin Black, Noriaki Hirose, and Sergey
dian Arunachalam, and Lerrel Pinto. The surprising Levine. ViNT:Afoundationmodelforvisualnavigation.
effectiveness of representation learning for visual imita- In 7th Annual Conference on Robot Learning, 2023.
tion, 2021. URL https://arxiv.org/abs/2306.14846.
[70] Ethan Perez, Florian Strub, Harm De Vries, Vincent [82] Rutav Shah, Roberto Martín-Martín, and Yuke Zhu.
Dumoulin, and Aaron Courville. Film: Visual reasoning MUTEX: Learning unified policies from multimodal
with a general conditioning layer. In Proceedings of the task specifications. In 7th Annual Conference on Robot
AAAI conference on artificial intelligence, volume 32, Learning, 2023. URL https://openreview.net/forum?id=
2018. PwqiqaaEzJ.
[71] Lerrel Pinto and Abhinav Gupta. Supersizing self- [83] Mohit Shridhar, Lucas Manuelli, and Dieter Fox.
supervision: Learning to grasp from 50k tries and 700 Perceiver-actor: A multi-task transformer for robotic
robot hours. In 2016 IEEE international conference manipulation. In Conference on Robot Learning, pages
on robotics and automation (ICRA), pages 3406–3413. 785–799. PMLR, 2023.
IEEE, 2016. [84] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit
[72] Gabriel Quere, Annette Hagengruber, Maged Iskandar, Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse
Samuel Bustamante, Daniel Leidner, Freek Stulp, and Thomason, and Animesh Garg. Progprompt: Generating
Joern Vogel. Shared Control Templates for Assistive situated robot task plans using large language models.
Robotics. In 2020 IEEE International Conference on In2023IEEEInternationalConferenceonRoboticsand
Robotics and Automation (ICRA), page 7, Paris, France, Automation (ICRA), pages 11523–11530. IEEE, 2023.
2020. [85] Ajay Sridhar, Dhruv Shah, Catherine Glossop, and
[73] IlijaRadosavovic,BaifengShi,LetianFu,KenGoldberg, Sergey Levine. Nomad: Goal masked diffusion poli-
Trevor Darrell, and Jitendra Malik. Robot learning cies for navigation and exploration. arXiv preprint
with sensorimotor pre-training. Conference on Robot arXiv:2310.07896, 2023.
Learning, 2023. [86] AustinStone,TedXiao,YaoLu,KeerthanaGopalakrish-
[74] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine nan,Kuang-HueiLee,QuanVuong,PaulWohlhart,Sean
Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Kirmani, Brianna Zitkovich, Fei Xia, et al. Open-world
Li, and Peter J. Liu. Exploring the limits of transfer object manipulation using pre-trained vision-language
learning with a unified text-to-text transformer. Journal models. In 7th Annual Conference on Robot Learning,
of Machine Learning Research, 21(140):1–67, 2020. 2023.
URL http://jmlr.org/papers/v21/20-074.html. [87] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aure-
[75] Scott Reed, Konrad Zolna, Emilio Parisotto, Ser- lien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo,
gio Gómez Colmenarejo, Alexander Novikov, Gabriel Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalabil-
Barth-maron, Mai Giménez, Yury Sulsky, Jackie Kay, ity in perception for autonomous driving: Waymo open
Jost Tobias Springenberg, et al. A generalist agent. dataset. In Proceedings of the IEEE/CVF conference on
Transactions on Machine Learning Research, 2022. computer vision and pattern recognition, pages 2446–
[76] Robin Rombach, Andreas Blattmann, Dominik Lorenz, 2454, 2020.
PatrickEsser,andBjörnOmmer. High-ResolutionImage [88] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Synthesis with Latent Diffusion Models, April 2022. Martinet, Marie-Anne Lachaux, Timothée Lacroix, Bap-
[77] ErickRosete-Beas,OierMees,GabrielKalweit,Joschka tiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar,
Boedecker, and Wolfram Burgard. Latent plans for task etal. LLaMA:OpenandEfficientFoundationLanguage
agnostic offline reinforcement learning. In Proceedings Models, February 2023.
of the 6th Conference on Robot Learning (CoRL), 2022. [89] Samuel Triest, Matthew Sivaprakasam, Sean J Wang,
[78] Saumya Saxena, Mohit Sharma, and Oliver Kroemer. Wenshan Wang, Aaron M Johnson, and Sebastian
Multi-resolutionsensingforreal-timecontrolwithvision- Scherer. Tartandrive: A large-scale dataset for learning
language models. In 7th Annual Conference on Robot off-road dynamics models. In 2022 International
Learning, 2023. URL https://openreview.net/forum?id= Conference on Robotics and Automation (ICRA), pages
WuBv9-IGDUA. 2546–2552. IEEE, 2022.[90] HomerWalke,KevinBlack,AbrahamLee,MooJinKim, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan
Max Du, Chongyi Zheng, Tony Zhao, Philippe Hansen- Welker, Ayzaan Wahid, et al. Rt-2: Vision-language-
Estruch, Quan Vuong, Andre He, Vivek Myers, Kuan actionmodelstransferwebknowledgetoroboticcontrol.
Fang, Chelsea Finn, and Sergey Levine. Bridgedata v2: In 7th Annual Conference on Robot Learning, 2023.
A dataset for robot learning at scale, 2023.
[91] Wayve. Lingo: Natural language for autonomous
driving, 2023. URL https://wayve.ai/thinking/
lingo-natural-language-autonomous-driving/.
[92] Philipp Wu, Arjun Majumdar, Kevin Stone, Yixin Lin,
Igor Mordatch, Pieter Abbeel, and Aravind Rajeswaran.
Masked trajectory models for prediction, representation,
and control. International Conference on Machine
Learning, 2023.
[93] Ge Yan, Kris Wu, and Xiaolong Wang. ucsd kitchens
Dataset. August 2023.
[94] Jonathan Heewon Yang, Dorsa Sadigh, and Chelsea
Finn. Polybot: Training one policy across robots while
embracing variability. In 7th Annual Conference on
Robot Learning, 2023. URL https://openreview.net/
forum?id=HEIRj51lcS.
[95] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian,
Yingying Chen, Fangchen Liu, Vashisht Madhavan, and
Trevor Darrell. Bdd100k: A diverse driving dataset for
heterogeneous multitask learning. In Proceedings of the
IEEE/CVF conference on computer vision and pattern
recognition, pages 2636–2645, 2020.
[96] Tianhe Yu, Ted Xiao, Austin Stone, Jonathan Tompson,
Anthony Brohan, Su Wang, Jaspiar Singh, Clayton Tan,
JodilynPeralta,BrianIchter,etal. Scalingrobotlearning
with semantically imagined experience. arXiv preprint
arXiv:2302.11550, 2023.
[97] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and
LucasBeyer.Scalingvisiontransformers.InProceedings
of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 12104–12113, 2022.
[98] TonyZZhao,VikashKumar,SergeyLevine,andChelsea
Finn. Learning fine-grained bimanual manipulation with
low-cost hardware. arXiv preprint arXiv:2304.13705,
2023.
[99] Gaoyue Zhou, Victoria Dean, Mohan Kumar Srirama,
Aravind Rajeswaran, Jyothish Pari, Kyle Hatch, Aryan
Jain, Tianhe Yu, Pieter Abbeel, Lerrel Pinto, Chelsea
Finn, and Abhinav Gupta. Train offline, test online: A
real robot learning benchmark, 2023.
[100] Xinghao Zhu, Ran Tian, Chenfeng Xu, Mingyu Ding,
Wei Zhan, and Masayoshi Tomizuka. Fanuc manipu-
lation: A dataset for learning-based manipulation with
fanuc mate 200id robot. 2023.
[101] Yifeng Zhu, Peter Stone, and Yuke Zhu. Bottom-up
skill discovery from unsegmented demonstrations for
long-horizon robot manipulation. IEEE Robotics and
Automation Letters, 7(2):4126–4133, 2022.
[102] Yifeng Zhu, Abhishek Joshi, Peter Stone, and Yuke Zhu.
Viola: Imitation learning for vision-based manipulation
with object proposal priors, 2023.
[103] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu,APPENDIXA determined by the relative size of the datasets with a few
CONTRIBUTIONS manual adjustments (see Section III-B).
Dibya Ghosh: led model development, proposed and imple-
mented large parts of the final model design, babysitted the OctoPretrainingDatasetMixture
training runs and touched all parts of the codebase, helped Fractal[10] 17.0%
with model evaluations and tech report writing. Kuka[41] 17.0%
Bridge[25,90] 17.0%
Homer Walke: led model evaluations, designed the main
BC-Z[39] 9.1%
Bridge evaluation benchmark, contributed to the initial model StanfordHydraDataset[5] 6.0%
implementation and ran many of the evals for this tech report. LanguageTable[55] 5.9%
TacoPlay[77,63] 3.6%
Karl Pertsch: managed the overall project, led Open-X
FurnitureBenchDataset[33] 3.3%
data integration and curation, led writing of this tech report, UTAustinMutex[82] 3.0%
contributed to model development and implementation and ran AustinSailorDataset[65] 2.9%
Roboturk[58] 2.8%
model evaluations for the tech report.
Toto[99] 2.4%
Kevin Black: led data loading and training infrastructure, AustinSiriusDataset[50] 2.3%
managed TPU pod training, created the project website, BerkeleyAutolabUR5[14] 1.5%
IAMLabCMUPickupInsert[78] 1.2%
contributed to model development and implementation, and
Viola[102] 1.2%
helped with robot evaluations and tech report writing. BerkeleyFanucManipulation[100] 1.0%
Oier Mees: ran countless ablations for model development, NYUFrankaPlayDataset[18] 0.9%
UCSDKitchenDataset[93] <0.1%
contributed to model implementation, helped with evals and
JacoPlay[20] 0.6%
writing of the tech report. BerkeleyCableRouting[52] 0.3%
Sudeep Dasari: contributed the model evaluations at CMU, AustinBudsDataset[101] 0.3%
CMUStretch[64] 0.2%
experimented with pretrained encoders.
NYUDoorOpening[69] 0.1%
Joey Hejna: contributed the model evaluations at Stanford. DLREDANSharedControl[72] 0.1%
Tobias Kreiman: contributed model evaluations in simulated
TABLE III: Octo pretraining data mixture using datasets from
environments.
the Open X-Embodiment dataset [67].
Ria Doshi: contributed to explorations into cross-morphology
training.
Charles Xu: contributed the model evaluations for Berkeley
APPENDIXD
Peg Insert.
TRAININGHYPERPARAMETERS
Jianlan Luo: contributed the model evaluations for Berkeley
Peg Insert. We mostly follow documented practices for training vision
You Liang Tan: helped diagnose and resolve bottlenecks in transformers [97]. We use the AdamW optimizer [51] with an
data loading. inversesquarerootdecaylearningrateschedule[97]andlinear
Pannag Sanketi, Quan Vuong, Ted Xiao: contributed model learning rate warm-up. We list hyperparamaters used during
training in Table IV and the model parameters for the different
evaluations on the Google Robot.
Dorsa Sadigh, Chelsea Finn, Sergey Levine: provided sizes in Table V. We apply standard image augmentations
duringtraining.Concretely,forthe3rdpersoncameraweapply
guidance throughout the project and feedback on the writing
stochasticcropsfollowedbearesizeto256 256,followedby
of this tech report.
×
colorjitter.Finally,wenormalizetheinputimagetohavepixels
APPENDIXB with float values between -1.0 and 1.0. For the wrist camera,
OCTOCODEEXAMPLE we apply the same procedure except without the random crop
Loading a pretrained Octo model and performing inference and resizing to 128 128 instead.
×
requires little code:
Hyperparameter Value
1 importjax
2 fromocto.model.octo_modelimportOctoModel
3 LearningRate 3e-4
4 model=OctoModel.load_pretrained("hf://rail-berkeley/octo-base") WarmupSteps 2000
5 print(model.get_pretty_spec())#Printouttheinput-outputspec
6 observation={"image_primary":img} LRScheduler reciprocalsquare-root
7 task=model.create_tasks(texts=["pickupthefork"]) WeightDecay 0.1
8 action=model.sample_actions(
9 observation,task,rng=jax.random.PRNGKey(0)) GradientClipThreshold 1
Listing 1: Example Python code to perform inference with a BatchSize 2048
pretrained Octo model.
TABLE IV: Hyperparameters used during training.
APPENDIXC
The images are passed through a shallow convolution stack,
DATAMIXTURE
then split into a sequence of flattened patches [22] of size
We list the detailed training mixture used for training the 16 16. This results in 256 tokens for the 3rd person camera
×
Octo models in Table III. The sampling weights are mostly imagesand64tokensforthewristcameraimages.Fordatasetscontaininglanguageannotations,weuseapretrainedt5-base • Discrete Action Heads: Discretizing actions into 256
(111M) transformer model [74] that produces a sequence of bins per dimension and training with cross-entropy loss
16 language embedding tokens. like in Brohan et al. [10] led to more “decisive” policies,
yet they lacked precision and often missed the grasp.
Model Layers Hidden size D MLP size Heads Params • ResNet Encoders: did not scale as well to larger datasets
in our evaluations (see Table II), though they did outper-
Octo-Small 12 384 1536 6 27M
form our ViT architecture when training from scratch on
Octo-Base 12 768 3072 12 93M
a small dataset (around 100 demonstrations).
TABLE V: Architecture details of Octo model variants. • Pretrained Encoders: ImageNet pretrained ResNet en-
coders did not provide benefit on zero-shot evals, though
may be confounded with ResNet architectures underper-
The diffusion action head consists of a 3-layer MLP
forming as mentioned above.
with a hidden dimension of 256, residual connections, and
layer normalization. We use the standard DDPM objective as • RelativeGripperActionRepresentation:Whenaligning
the gripper action representations of the different datasets,
introduced by [34] with a cosine noise schedule [66] and 20
we tried (A) absolute gripper actions, i.e., actions are +1
diffusion steps.
when the gripper is open and 0 if it is closed, and (B)
APPENDIXE relative gripper actions, i.e., gripper action is +1/0 only
THINGSTHATWORKEDANDDIDNOTWORK(YET) in the timestep when the gripper opens/closes and 0.5
otherwise. We found that the latter tends to open/close
Things we found improved performance:
the gripper less often since most of the training data
• Adding history during pretraining: Models with one represents “do not change gripper” actions, leading to a
frame of history as context performed better in zero-shot
slightly higher grasp success rate. At the same time, the
evals than models pretrained without history. We did not
relative representation led to less retrying behavior after a
observe benefits of increasing the history length further
grasp failed, which was ultimately worse. Thus, we chose
on the few tasks we evaluated on, though other tasks may
the absolute gripper action representation.
benefit.
• Adding Proprioceptive Inputs: Policies trained with
• Usingactionchunking:Wefoundithelpfultouse“action propioceptive observations seemed generally worse, po-
chunking” [98], i.e., to predict multiple actions into the
tentially due to a strong correlation between states and
future, for getting more coherent policy movements. In
future actions. We hypothesize this might be due to a
our evaluations, we did not find temporal ensembling
causal confusion between the proprioceptive information
of future actions to provide additional benefits beyond
and the target actions [21].
receding horizon control.
• Finetuning Language Model: In order to improve the
• Decreasing patch size Tokenizing images into patches of visuo-lingual grounding of Octo we experimented with:
size 16 16 led to improved performance over patches
× i) varying sizes of the T5 encoder [74]: small (30M),
of size 32 32, particularly for grasping and other
× base (111M), and large (386M) as well as ii) finetun-
fine-grained tasks. This does add compute complexity
ing the last two layers of the encoder. Using the frozen
(the number of tokens is 4 ), so understanding how to
× base model resulted in the best language-conditioned
balance compute costs and resolution remains a problem
policies. We did not find improvements when using larger
of interest.
encoders or finetuning the encoder. We hypothesize this
• Increasing shuffle buffer size: Loading data from might be due to the lack of rich, diverse, free-form
25 datasets in parallel is a challenge. Specifically, we
language annotations in most of the datasets.
found that achieving good shuffling of frames during
training was crucial — zero-shot performance with a
APPENDIXF
small shuffle buffer (20k) and trajectory-level interleaving
EXPERIMENTALSETUPS
suffered significantly. We solved this issue by shuffling
and interleaving frames from different trajectories before A. Zero-Shot Evaluations
decoding the images, allowing us to fit a much larger
WidowXBridgeV2: UsesthesetupofWalkeetal.[90],in
shuffle buffer (up to 500k). We also subsample at most
which a Trossen WidowX 250 6-DOF robot performs diverse
100 randomly chosen steps from each training trajectory
manipulation tasks. The observation consists of a single third
during data loading to avoid “over-crowding” the shuffle
person camera stream and the action space is end-effector
buffer with single, very long episodes.
position deltas. We evaluated two language-conditioned tasks
Things that did not work (yet): in which a the robot needs to “place the carrot on plate”,
• MSE Action Heads: Replacing our diffusion decoding and “put the eggplant in the pot.” While these tasks are in-
head with a simple L2 loss led to “hedging” policies that distribution, the policy must still generalize to novel object
move very slowly and e.g., fail to rotate the gripper in positions. We performed 10 trials per task and varied objects
WidowX evaluations. positions between trials.WidowX BridgeV2 UR5 Tabletop RT-1 Robot Berkeley Peg Insert Stanford Coffee CMU Baking Berkeley Pick-up Berkeley Bi-Manual Berkeley Coke
0-Shot Evaluation Finetuning Evaluation
Fig. 7: Evaluation Tasks. Replicated from the main text for convenience. We evaluate Octo on 9 real robot setups across
4 institutions in zero-shot and finetuning scenarios.
UR5: Uses the setup of Chen et al. [14], in which a UR5 delta).Observationscomefromthe3rd-personfront-facingZed
robot arm performs multiple table top manipulation tasks. The camera. Actions are predicted at 15 Hz, and executed on the
observationconsistsofasinglethirdpersoncamerastreamand robot using the R2D2 Franka controller. The finetuning dataset
the action space is end-effector position deltas. We evaluated consists of 120 demos collected via expert VR tele-operation,
twplanguage-conditionedtasks:pickingatoytigerfromabowl and every policy was evaluated using 20 trials (4 novel test
and placing it into a different bowl as well as wiping a table objects with 5 positions each).
with a cloth. While these tasks are in-distribution, the policy Stanford Coffee: The robot is tasked with picking up one
muststillgeneralizetonovelobjectpositions,distractorobjects, of four different Keurig Coffee Pods and placing it inside of a
and lighting. Since the training data was collected months ago Keurig machine. This task requires both generalization across
and the robot setup was taken down and re-assembled, the initialpositions andcolors ofthe coffeepod, aswell asprecise
policy must also generalize to other miscellaneous changes placement in the Keurig machine. We use an end effector delta
in the environment like a slightly different camera view and action space with an open source controller running at 10 Hz
background.Weperformed10trialspertaskandvariedobjects basedonPolymetis[49](foundhere).Weuseonlyasingle3rd-
positions between trials. person wrist observation. Our training dataset contained 118
RT-1 Robot: Uses the setup of Brohan et al. [10], in expert demonstrations from varied coffee pods and positions
which a proprietary robot performs multiple table top and collected via VR tele-operation. We evaluated policies for 20
furniture manipulation tasks. The observation consists of a episodes, five episodes for each of four different color coffee
single third person camera stream and the action space is end- pods.
effector position deltas. We evaluated on the task of picking
Berkeley Peg Insertion: The task is to insert a pre-
up a 7up can, apple, blue chip bag, or brown chip bag, as well
grasped 3D-printed peg into a matching slot on a 3D-printed
as the task of opening or closing drawers on a cabinet. While
board inside the bin. The matching tolerance between the
these tasks are in-distribution, the policy must still generalize
peg and the hole is 1.5mm; which makes it a contact-rich
to novel object positions. Additionally, since the robot setup
precise part-mating task. The robot must learn an appropriate
was moved between buildings, the policy must also generalize
policy to “search" for the matching opening through contact,
to miscellaneous changes in the environment like a slightly
which necessitates the use of force/torque measurements. The
different camera view and background. We performed 10 trials
observation space of the policy consists of a single side-view
per task and varied objects positions between trials.
camera image, the end-effector twist, and the end-effector
B. Model Ablations force/torque reading. The policy sends action commands as
the robot’s end-effector twists at 5 HZ, tracked at 1000 HZ
All of our model ablations were evaluated on the WidowX
by a low-level impedance controller. Our finetuning dataset
setup. We present a more detailed breakdown of the success
is composed of 100 human demonstrations from the FMB
rates per task in Table VI. We evaluated on two language-
dataset [53]. We evaluated trained policies for 20 trials with
conditioned tasks (put carrot on plate and put eggplant in pot)
randomized board positions.
and two goal-conditioned tasks (put bread on plate and put
Berkeley Pick Up: We use the setup of Radosavovic et al.
spoon on glove). The goal-conditioned tasks contain objects
[73]. The robot needs to pick up a block from a table top
not seen in the Bridge dataset. Additionally, we analyze the
surface after being trained on a dataset of 100 pickups of
generalizationcapabilitiesofOctoacrossseveraldifferentaxes,
various objects. The robot uses joint position control with an
such as novel objects, novel environments and novel skills in
underlyingPolymetiscontrolstack[49](here).Itisconditioned
Table VII.
on a wrist camera input image, as well as the proprioceptive
C. Finetuning Evaluations readings of the robot. We evaluted on the task of picking up
CMUBaking: Therobotmustpickupthetoybreadobject, the yellow cube and used 20 trials.
place it in the toaster, and shut the toaster. This task requires Berkeley Coke: A Trossen Robotics ViperX robot is
generalization across initial positions (of both the toaster and tasked with picking up a coke can from a table. The ViperX
object) and the shape of the target toy bread object. We use an is completely unseen in the training mixture, thus this task
end-effector delta action space (Cartesian position + rotation tests the model’s capabilities to generalize to a new robotLanguage-conditioned Goal-conditioned
Putcarrotonplate Puteggplantinpot Putbreadonplate Putspoononglove Average
Octo-small(Ours) 80% 90% 70% 90% 83%
RT-Xdatasetmix[67] 80% 80% 40% 40% 60%
Singlerobotdataset(BridgeData) 20% 70% 60% 20% 43%
DiscretizedActionPrediction[67] 0% 20% 10% 40% 18%
ContinuousActionPrediction(MSE) 70% 30% 0% 40% 35%
Resnet-50+Transformer[67] 80% 60% 100% 40% 70%
TABLE VI: Model Ablations. We achieve best performance when using the ViT architecture, diffusion action head, and wide
training data mixture. All evaluations are performed on the WidowX setup. Success rates are averaged over 40 trials across two
language-conditioned tasks and two goal-conditioned tasks.
GeneralizationType Task SuccessRate Average
Putcarrotonplate 80%
In-distribution 85%
Puteggplantinpot 90%
Putbreadonplate 70%
Novelobjects 80%
Putspoononglove 90%
Putmushroominpot 20%
Novelenvironment 40%
Putspoononcloth 60%
Flipcuponitsside 10%
Novelskill 5%
Putblockinslot 0%
TABLE VII: Zero-shot Generalization Analysis. Using the WidowX setup, we analyze the generalization capabilities of
Octo-small along several different axes. Octo performs the best on in-distribution tasks and generalizes well to novel objects
and environments. However, Octo struggles to generalize to skills not seen in the WidowX embodiment. Success rates are
averaged over 20 trials across two tasks.
embodiment. The task also requires generalization across
different positions of the coke can with the ViperX robot
using end-effector delta control at 5Hz. The policy uses image
observations from a third person and a wrist camera. Our
training dataset contained 115 expert demonstrations collected
via VR tele-operation from varied positions.
Berkeley Bimanual: The task requires an ALOHA biman-
ualrobot[98],consistingoftwoViperXrobotarms,topickup
a sharpie marker with the right hand from the workbench and
remove its cap with the left hand. Since Octo was only pre-
trainedwithsingle-armrobotdata,were-initializeanewaction
head that maps to the 14-dimensional action space of ALOHA
(2x 6 joint positions + 2x gripper position). A successful
policy needs to use image observations from the left and right
wrist cameras to predict precise manipulation behaviors. Best
performance for both our methods and baselines was achieved
by predicting an action chunk of 64 during training, and then
during test-time executing 12 actions with receding horizon
control before re-planning. We evaluated the uncapping task
using10trials,wherethepenwasplacedinadifferentlocation
for each trial.
ATAD
YCILOP
HCRA