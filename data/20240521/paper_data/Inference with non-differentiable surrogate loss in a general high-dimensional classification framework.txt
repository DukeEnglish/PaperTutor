Inference with non-differentiable surrogate loss in a
general high-dimensional classification framework
Muxuan Liang∗ Yang Ning† Maureen A Smith‡ Ying-Qi Zhao§
Abstract
Penalized empirical risk minimization with a surrogate loss function is often used to derive a
high-dimensional linear decision rule in classification problems. Although much of the literature
focuses on the generalization error, there is a lack of valid inference procedures to identify the driving
factors of the estimated decision rule, especially when the surrogate loss is non-differentiable. In this
work, we propose a kernel-smoothed decorrelated score to construct hypothesis testing and interval
estimations for the linear decision rule estimated using a piece-wise linear surrogate loss, which has
a discontinuous gradient and non-regular Hessian. Specifically, we adopt kernel approximations to
smooth the discontinuous gradient near discontinuity points and approximate the non-regular Hessian
of the surrogate loss. In applications where additional nuisance parameters are involved, we propose
a novel cross-fitted version to accommodate flexible nuisance estimates and kernel approximations.
We establish the limiting distribution of the kernel-smoothed decorrelated score and its cross-fitted
version in a high-dimensional setup. Simulation and real data analysis are conducted to demonstrate
the validity and superiority of the proposed method.
Keywords: Classification,High-dimensionalinference,Non-differentiableloss,Personalizedmedicine,
Double machine learning.
∗Department of Biostatistics, University of Florida
†Department of Statistics and Data Science, Cornell University
‡DepartmentsofPopulationHealthSciencesandFamilyMedicine&CommunityHealth,UniversityofWisconsin-Madison
§Public Health Sciences Divisions, Fred Hutchinson Cancer Center
4202
yaM
02
]EM.tats[
1v32711.5042:viXra1 Introduction
Classification identifies which of a set of labels an observation belongs to. Well-known classification
methods include logistic regression, supportvector machines, and many others. When the dimension-
ality of the covariate space is high, which is common due to the increasing adoption of large data
sets in biomedical applications, the classification task is more challenging. Additionally, it has been
shown that many problems can be formulated within a general classification framework. In general,
the goal is to derive a data-driven decision rule that minimizes a loss function defined according to
the problem setups.
Empirical risk minimization (ERM) is often used to estimate such a data-driven decision rule by
minimizing a convex surrogate of the loss function. While there are large amounts of work proposed
for conducting classifications and learning the generalization error of an ERM with a convex surrogate
(Bartlett et al., 2006), statistical inference of the estimated decision rule within a high-dimensional
classification framework is less investigated. Van de Geer et al. (2014) proposed a debiased Lasso
estimator and showed its asymptotic normality for generalized linear models. Ning and Liu (2017)
proposed a decorrelated score to test a low-dimensional projection of high-dimensional coefficient
vectors, which can be applied to M-estimation under a strictly convex and differentiable loss function.
Dezeure et al.(2017)proposedabootstrapproceduretoconductsimultaneousinferenceforparameters
in groups with diverging group sizes. More recently, partial penalized tests proposed in Shi et al.
(2019) can test hypotheses involving a growing number of coefficients. Ma et al. (2021) considered
the global hypothesis testing and multiple testing procedure for high-dimensional logistic regression
models. Wu et al. (2021) proposed an inference procedure for single-index models with differentiable
link functions. However, none of these methods can be applied to the inference problem involving
non-differentiable loss.
Deriving an inference procedure for an ERM with a convex surrogate that is non-differentiable is
more complicated. One such example is the popular classification method support vector machine
(SVM) (Cortes and Vapnik, 1995). It employs the hinge loss as a surrogate loss, which is continuous
butnon-differentiable. ThemajorityoftheexistingSVMliteraturefocusedontheconsistency, andthe
convergencerateoftheriskunderthederivedclassifierstotheBayesrisk(Lin,2004;Zhang et al.,2004;
Steinwart, 2005; Lin, 2000; Zhang et al., 2004; Bartlett et al., 2006; Steinwart et al., 2007; Vert et al.,
2006; Blanchard et al., 2008). Peng et al. (2016) provided the error bound for a penalized SVM in
ultra-high dimension; Zhang et al. (2016a) and Zhang et al. (2016b) focused on the variable selec-
tion for SVM in moderately high dimension. The literature on the asymptotic distribution of the
2estimators is limited. Koo et al. (2008) investigated the Bahadur representation of a linear SVM,
which implies the asymptotic normality of the estimator in the low-dimensional setup. Due to the
non-differentiability of the hinge loss, they proposed a non-parametric estimator for the asymptotic
variance. Wang et al. (2019) proposed a distributed inference procedure for a linear SVM. To handle
thenon-differentiability, they used asmoothed loss function to approximate the hingeloss and showed
the asymptotic normality of the estimator under p n 0, where n is the total sample size, and p
{ Ñ
is the number of the covariates. However, an associated inference procedure in the high-dimensional
setup is still lacking.
Furthermore, the classification framework has been adapted to solve problems incorporating addi-
tional weights. For example, to conduct classification with missing labels, weights depending on the
missing data mechanism are often introduced to leverage information from those subjects who have
covariates observedbutlabelsmissing. Anotherexampleistodevelopanindividualizedtreatmentrule
(ITR) in precision medicine, which recommends treatment according to patient characteristics. There
has been much literature proposingto learn ITRs from a weighted classification framework, where the
weights arerelated to theobserved clinical outcomes (Zhao et al.,2012;Zhou et al.,2017;Chen et al.,
2016; Zhao et al., 2014, 2019; Pan and Zhao, 2021; Xue et al., 2020). In recent work, Zhao et al.
(2019) and Liang et al. (2022) introduced both the outcome regression models and propensity score
as nuisance parameters in the weights. To avoid model misspecification, the nuisance parameters
are estimated via nonparametric or flexible machine learning algorithms. These algorithms may lead
to nuisance parameter estimates with slow convergence rates. These flexible estimates of the nui-
sance parameters with possible slow convergence rates create a large barrier in statistical inference.
Liang et al. (2022) proposed an inference procedure that can handle strictly convex differentiable loss
functions. However, inference procedure for non-differentiable loss functions is still lacking.
In this work, we propose a novel inference procedure for linear decision rules under a general
classification framework in a high-dimensional setup, which can deal with non-differentiable convex
surrogate loss functions. We introduce a kernel-smoothed decorrelated score for the inference proce-
dure, which utilizes a local kernel function to smooth the discontinuous gradient near discontinuity
points, where the loss function is not differentiable, and a global kernel function to approximate the
non-regular Hessian. By using these kernel functions, the proposed procedure can be applied to any
piece-wise linear convex loss functions. Furthermore, unlike existing literature (Wang et al., 2019;
Koo et al., 2008), the proposed procedure is valid even when p n and can be extended to test
{ Ñ `8
a hypothesis involving a growing number of projections. For the general classification problems, ad-
ditional nuisance parameters may be involved in the loss function. Motivated by Chernozhukov et al.
3(2018), we further propose a new cross-fitting algorithm to efficiently accommodate these nuisance
parameters. We show the uniform validity of the kernel-smoothed decorrelated score based procedure
even when the nuisance parameters are estimated using nonparametric or flexible machine learning
methods. Simulations and real data examples show the superiority of the proposed method.
Thispaperisorganizedasfollows. Section2and3introducethekernelsmootheddecorrelatedscore
for the classification problem and the general classification framework. Section 4 provides theoretical
justifications for the proposed procedure. Section 5 and 6 present simulations and real data analyses.
Section 7 concludes the paper and provides a discussion on future directions.
2 Statistical inference for classification problems
2.1 A classification problem
We observe the covariates X and a label of interest A 1,1 . A decision rule, d X , is a mapping
P t´ u p q
from the covariate space Rp to the label space 1,1 . In this paper, we mainly focus on the high-
t´ u
dimensional setting with p n . The goal is to identify the optimal decision rule that minimizes
{ Ñ `8
the classification error, defined as
L d E 1 A d X .
p q “ r t ‰ p qus
The minimizer of the classification error is d X sgn P A 1 X 1 2 , which is known as
opt
p q “ t p “ | q´ { u
the Bayes decision rule.
With the observed data, we aim to optimize the empirical analogue of L d , denoted by L d
p q p q “
E 1 A d X , where E is the empirical expectation. However, L d is hard to optimize due
n n
r t ‰ p qus r¨s p q p
to the discontinuity of the zero-one loss, and the searching space of the unconstrained decision rules is
p p p
large. Totacklethesechallengesincomputation,weconsiderarelaxationbyreplacingthezero-oneloss
with a piece-wise convex loss function. Examples of piece-wise convex loss functions include the hinge
loss and the modifiedhingeloss usedfor classification with a rejection option (Bartlett and Wegkamp,
2008). Furthermore, we focus on the linear decision rule for its interpretability. We thus optimize
L β E φ AXJβ ,
φ n
p q “ r p qs
p p
where φ is the surrogate loss.
p¨q
Define the minimizer of L β as β˚, where L β E φ AXJβ . With high-dimensional data,
φ p q φ φ p q “ r p qs
we adopt the penalized empirical risk minimization to estimate β˚. Specifically, we consider
φ
Lλn β E φ AXJβ λ β ,
φ p q “ n r p qs` n } }1
p p
4where λ is a tuning parameter. Denote the minimizer of Lλn β as β . Through the estimator
n φ p q φ
β , we aim to construct a hypothesis testing procedure and confidence interval for a low-dimensional
φ p p
projection of β˚, i.e. ηJβ˚, where η is a known sparse vector.
p φ φ
2.2 Kernel-smoothed decorrelated score
For the convenience of illustration, we consider a hypothesis testing problem H : β˚ 0 versus
0 φ,l “
H :β˚ 0, whereβ˚ is the l-th coordinate of theβ˚. Theproposedapproach can beeasily applied
a φ,l ­“ φ,l φ
to test H :ηJβ˚ 0 versus H :ηJβ˚ 0.
0 φ “ a φ ­“
We first review a decorrelated score, proposed in Ning and Liu (2017), which can be used to test
this hypothesis when the surrogate loss φ is differentiable. The key of the decorrelated score is to
decouple the estimation error of the high-dimensional component β˚ with the estimation of β˚ ,
φ,´l φ,l
where β˚ is the sub-vector of β˚ without the l-th coordinate. Define the minimizer of
φ,´l φ
L φ2 ,lpw
q“
E φ2 AXJβ φ˚ pX l ´X ´J lw q2
” ı
` ˘
as w˚ . The decorrelated score is defined as
φ,l
S φ1 ,lpβ;w φ˚ ,l
q“
E n Aφ1 AXJβ pX l ´X ´J lw φ˚ ,l
q
.
” ı
` ˘
p
Let β be a vector equals β except the l-th coordinate is fixed at 0. Under the null hypothesis,
φ,nullplq φ
it ca pn be shown that ?nS φ1 ,lpβ pφ,nullplq;w φ˚
,lq
ÝÑd N p0,σ l2 q, where σ l2 is some constant. However, this
procedurecannotbeappliedtothenon-differentiableloss functionsduetothenon-existence ofregular
p
1 2
φ and φ .
p¨q p¨q
For a piece-wise linear convex loss function φ, although φ is not differentiable at the discontinuity
points,thegradientiswelldefinedonanyopenintervalswithoutdiscontinuitypoints. Mathematically,
1 1
supposethat t t arethejumpdiscontinuity pointsofφ, thenφ canbedefined
1 J
´8 ă ă ¨¨¨ ă ă `8
as ∆ J ∆ 1 t t 0 on any open intervals. In addition, a Hessian φ2 can be defined using
0 ` j“1 j t ´ j ě u
the Diracřfunction δ pt
q
as J j“1∆ jδ pt ´t
j
q, which is non-regular because it achieves
`8
at 0 and
vanishes at all other points.ř
We use kernel functions to smooth the gradient near discontinuity points and approximate the
1
Hessian of the surrogate loss function. Specifically, we obtain a smoothed gradient φ by a local
kernel function, a smooth function whose derivative has a support contained in a compact interval.
ConsiderH satisfying H t 1ift 1 andH t 0 ift 1; thus, its derivative has asupporton
p¨q p q “ ě p q“ ď ´
1,1 . As the bandwidth h 0, the functions H t h and H1 t h h approach the indicator
lo lo lo lo
r´ s Ñ p { q p { q{
5function 1 t 0 and the Dirac function δ t , respectively. One example of such kernel is
p ě q p q
0 if t 1,
ď ´
$
H t ’ ’1 15 t 2t3 1t5 if t 1,
p q “ ’ ’2 ` 16p ´ 3 ` 5 q | | ă
’
&
1 if t 1.
’ ě
’
’
’
’
Define %
J
1 t t j
φ t ∆ ∆ H ´ ,
0 j
p q “ ` h
lo
j“1 ˆ ˙
ÿ
1 1 r 1 1 1
where ∆ φ t φ t . For any open interval where φ exists, φ t is different from φ
j j j
“ p `q ´ p ´q p q
only on J t h ,t h . Thus, the smoothed gradient of L β can be naturally defined by
j“1r j ´ lo j ` lo s φ p q r
E Aφ1 AŤXJβ X . The smoothed score function of β φ˚
,l
is E Aφ1 AXJβ
φ,nullplq
X
l
. However,
in”high`dimensi˘on,ıit is biased due to the estimation error of the”high´-dimensional c¯ompıonent β .
r r p φ,´l
Following the idea of the decorrelated score, we decouple the smoothed score function of β˚ with the
φ,l p
estimation error of β . Define w˚ as the minimizer of
φ,´l φ,l
p J
L φ2 ,lpw
q“
E
«
∆ jδ pt j ´AXJβ φ˚ qpX l ´X ´J lw q2 ff.
j“1
ÿ
We construct the kernel-smoothed decorrelated score
S φ1 ,lpβ;w φ˚ ,l
q“
E n Aφ1 AXJβ pX l ´X ´J lw φ˚ ,l
q
.
” ı
` ˘
To construct a score tr est for β φ˚ ,l “p 0 usir ng S φ1 ,lpβ;w φ˚ ,lq, we need to estimate w φ˚ ,l, which is
nontrivial due to the non-regularity of the δ t function. We propose to use a global kernel function
p q r
withasupportontheentirereallinetoapproximateδ t . SupposethatG t isaglobalkernelfunction
p q p q
satisfying that 1) G t 0, t; 2) G t dt 1; 3) tG t dt 0. Define G t h´1G t h , where
p q ą @ p q “ p q “ hgbp q “ gb p { gb q
h is the bandwidth of the globaşl kernel. The esştimated w˚ , denoted by w , can be obtained by
gb φ,l φ,l
minimizing
J p
E ∆ G t AXJβ X XJw 2 µ w ,
n j hgbp j
´
φ
qp
l
´
´l
q `
n
}
}1
« ff
j“1
ÿ
p p
where µ is a tuning parameter. The above objective function is strictly convex and smooth with
n
respect to w given that G t 0, and thus can be easily minimized.
p q ą
Remark 1 A straightforward idea is to use the gradient of the local kernel to approximate δ t .
p q
However, only data points near the discontinuity points will contribute to the estimation of w˚ in
φ,l
this case, because further derivative of
φ˜1
t is zero when t is away from the discontinuity points. In
p q
supplementary material, we show that using the local kernel function to approximate the non-regular
6Hessian will lead to a slower convergence rate of the estimator and require more restrictive conditions
in the inference procedure.
Replacingthew φ˚ ,l byw φ,l inS φ1 ,lpβ;w φ˚ ,lq,wecancalculate thevalueoftheestimateddecorrelated
score function S φ1 ,lpβ φ,nul plplq;w φ,l qrto test the null hypothesis. However, additional challenges arise
from the correlation between β and the loss function used to estimate w˚ , as well as the correlation
r p pφ φ,l
between β φ and the kernel-sm poothed decorrelated score function S φ1 ,lp¨q. We design a novel sample-
splitting strategy, where the estimator β is independent from the data used to estimate w˚ , as
p φ r φ,l
well as the data used to construct S 1 . In addition, instead of averaging over multiple estimators
φ ,lp¨qp
as in the cross-fitting procedure (Chernozhukov et al., 2018), we average over the loss functions to
r
estimate w , which is more computationally robust. The entire inference procedure is summarized
φ,l
in Algorithm 1. In our simulation and real data analysis, we set K 2. For the bandwidth selection,
p “
we choose h 1 ?nlogn and h logp n ´1{5 based on the theoretical results in Section 4.1
lo gb
“ { “ p { q
and 4.2.
2.3 Construction of confidence interval
Due to the penalization, the β is biased and cannot be directly used to construct an interval esti-
φ,l
mation. To remove the bias, the key idea is to consider a one-step estimator based on an unbiased
p
estimating equation. Motivated by the fact that the kernel-smoothed decorrelated score is an asymp-
totically unbiased estimating equation for β˚ , when the bandwidth shrinks to 0, we consider
φ,l
β φ,l
“
β¯ φ,l ´S φ˜1 ,lpβ φp´kq ;w φ,l q{I l,
where r p p p
K K
β¯ φ,l
“
K1 β φp´ ,lkq , S φ˜1 ,lpβ;w φ,l
q“
K1 Ep nkq Aφ1 AXJβ pX l ´X ´J lw φ,l
q
,
k ÿ“1 k ÿ“1 ”
` ˘
ı
1 K p p p r p
I Epkq ∆ G t AXJβp´kq X XJw 2 ,
l “ K n « j hgb j ´ φ p l ´ ´l φ,l q ff
k ÿ“1 ÿj ´ ¯
can be calp culated in Step p 4 of Algorithm 1. Thenp 95%-confidence inpterval can be constructed by
β 1.96n´1{2σ I ,β 1.96n´1{2σ I .
φ,l l l φ,l l l
´ { ` {
´ ¯
3r
A
geneprap lr classificaptip
on framework
Weighted classification assigns class-specific weights, which reflect the relative importance of each
class. More broadly, weights can depend on the covariate X that each individual has specific weights.
Mathematically, the task is to learn a decision rule, d X , which minimizes a weighted zero-one loss
p q
L d;W ,W E W X 1 d X 1 W X 1 d X 1 , (1)
1 ´1 1 ´1
p q “ r p q t p q ‰ u` p q t p q ‰ ´ us
7Algorithm 1: Inference of β˚.
φ
Input: A random seed; n samples; a positive integer K.
Output: A p-value for H : β˚ “ 0.
0 φ,l
1 Randomly split data into K parts I ,¨¨¨ ,I with equal size, and set k “ 1;
1 K
2 Estimate β˚ using data in Ic by
φ k
Ep´kqrφpAXJβqs`λp´kq}β} ,
n n 1
where Ep´kq r¨s is the empirical average on Ic, and denote the estimator as βp´kq . The parameter
n p k φ
λp´kq is tuned by cross-validation; we obtain βp´kq for each k;
n p φ p
3 Obtain an estimator w for w˚ by minimizing
φ,l φ,l p
K J
1
Epkq ∆ G pt ´AXJβp´kq q pX ´XJwq2 `µ }w} ,
K np j hgb j φ l ´l n 1
«# + ff
k“1 j“1
ÿ ÿ
where µ is tunedpby cross-validation, and Epkq rp¨s is the empirical average on I ;
n n k
J
4 Let βpkq equal to βpkq except its l-th coordinate replaced by 0. Construct the
φ,nullplq φ p
ker´nel-smoot¯hed decorrelated score test statistic as
p p K
1
S φ1 ,nullplq “ K Ep nkq Aφ1 AXJβ φp´ ,nk uq llplq pX l ´X ´J lw φ,lq ,
k ÿ“1 ” ´ ¯ ı
and the estimator ofr the variance σ2p “ 1 rK Epkq p φ1 AXJβp´kq pXp´XJw q 2 ;
l K k“1 n φ l ´l φ,l
„ 
! ´ ¯ )
5 Calculate the p-value by 2 1´Φpn p´1{2|S φ1ř ,nullplqp|{σ lq ,rwhere Φpp¨q is the cumulativ pe distribution
function of a standard nor´mal distribution. ¯
r
p
8where W X ’s are pre-specified weights depending on the problem of interest.
a
p q
Under the proposed general classification framework, the minimizer of the loss (1) can be derived
as
d X sgn W X W X ,
opt 1 ´1
p q “ t p q´ p qu
where sgn t 1 if t 0; sgn t 1, otherwise.
p q“ ě p q “ ´
We are interested in developing an inference procedure for a low-dimensional projection of β˚,
φ
where β˚ is the minimizer of
φ
L β;W ,W E W φ XJβ W φ XJβ ,
φ 1 ´1 1 ´1
p q “ p q` p´ q
“ ‰
whereforsimplicity weusethenotation W andW . Theweights aretypically notdirectly observed,
1 ´1
and need to be estimated. Specifically, to estimate β˚, we minimize
φ
Lλn β;W ,W E W φ XJβ W φ XJβ λ β ,
φ p 1 ´1 q “ n 1 p q` ´1 p´ q ` n } }1
” ı
where W ’s are tphe estimxatexd weightps usxing the observxed data.
a
The general weighted classification framework has been broadly used in many applications. We
x
provide two examples below.
• Classification with missing labels. In a classification problem, it is likely that only partial
samples are fully observed with X,A , and we only observe the covariate information X for the
p q
remaining samples. For example, to predict patient-reported outcomes, covariate information is
collected at baseline, whereas the outcomes are collected by a survey after intervention. We can
only observe outcomes for those patients who fill out the survey, and other patients’ outcomes
are missing.
Let R be the missing indicator. We assume missing at random (MAR), i.e., A R X. By
K |
leveraging both the labeled and unlabeled samples, an estimator of the classification error is
E W X,A;π,p 1 d X 1 W X,A;π,p 1 d X 1 . (2)
n 1 ´1
p q t p q ‰ u` p q t p q ‰ ´ u
” ı
Here, p x p p x p p
1 R 1,A a 1 R 1 π X
1
W X,A;π,p t “ “ u t “ u´ p qp X ,
a a
p q“ π X ´ π X p q
1 1
p q p q
p
where π X ixs an estimpatpe of the nuisance parameter π P R p1 X and p X is an
1 1 a
p q p p “ p “ | q p q
estimateofthenuisanceparameterp X P A a X . Letπ x andp x bethepoint-wise
a 1 a
p p q “ p “ | q p q p q p
limits of the π x and p x . Define
1 a
p q p q s s
1 R 1,A a 1 R 1 π X
1
pW X,A;pπ,p t “ “ u t “ u´ p qp X .
a a
p q“ π X ´ π X p q
1 1
p q p q
s
Ď s s s
9
s sUnder the class of linear decision rules, we can minimize
Lλn β;W ,W E W X,A;π,p φ XJβ W X,A;π,p φ XJβ λ β .
φ p 1 ´1 q “ n 1 p q p q` ´1 p q p´ q ` n } }1
” ı
Wp e assumx e thx at therep exisx ts a constpanpt c such thatx 0 c π p,πp. The target of the inference
1 1
ă ă
procedure is a low-dimensional projection of β˚, which minimizes
φ
p
L β;W ,W E W X,A;π,p φ XJβ W X,A;π,p φ XJβ . (3)
φ 1 ´1 1 ´1
p q “ p q p q` p q p´ q
“ ‰
Further, due tĎ o thĎ e constructĎ ion of thesWs X,A;π,pĎ ’s, if eithersπs π or p p , then
a 1 1 a a
p q “ “
E W X W X P A a X and the objective function (3) equals to
a a
r | s“ p q ” p “ | q Ď s s s s
Ď L β;W ,W E W φ XJβ W φ XJβ .
φ 1 ´1 1 ´1
p q “ p q` p´ q
“ ‰
• Estimation of individualized treatment rules. An individualized treatment rule d X
p q
maps the covariate space Rp to the treatment space 1,1 , which is the label space here. To
t´ u
define the objective function, we adopt the potential outcome framework (Rubin, 1974, 2005).
Denote the potential outcome under treatment a 1,1 as Y a , and the potential outcome
P t´ u p q
under an individualized treatment rule d as Y d . Assume larger outcomes are more preferable.
p q
The goal is to learn the optimal individualized treatment rule that maximizes E Y d .
r p qs
We observe the covariate information X, the assigned treatment A, and the outcome Y. We
assume the following conditions: 1) the Stable Unit Treatment Value Assumption (SUTVA)
(Imbens and Rubin, 2015); 2) the strong ignorability Y 1 ,Y 1 A X; 3) Consistency
p´ q p q K |
Y Y a if A a. SUTVA condition assumes that the potential outcomes for a patient do not
“ p q “
varywiththeotherpatients’treatments. Italsoimpliesthattherearenodifferentversionsofthe
treatment. The strong ignorability condition means that there is no unmeasured confounding
between the potential outcomes and the treatment. The consistency ensures that the observed
outcome is the potential outcome under the assigned treatment. Under these conditions, an
augmented inverse probability weighted estimator of E Y d is
r p qs
E W Y,X,A;p,Q 1 d X 1 W Y,X,A;p,Q 1 d X 1 . (4)
n 1 ´1
p q t p q “ q` p q t p q “ ´ q
” ı
Here, p x p p x p p
Y1 A a 1 A a p X
a
W Y,X,A;p,Q t “ u t “ u´ p qQ X ,
a a
p q “ p X ` p X p q
a a
p q p q
p
where p X anx d Q X arp e ep stimators for the nuisance parameters p p X P A a X
a a p p a
p q p q p q “ p “ | q
and Q X E Y X,A a , respectively. We assume that there exists a constant c 0 such
a
pp q “ p |p “ q ą
10that c p X ,p X 1 c. Let p and Q be the point-wise limit of p and Q . Define
a a a a a a
ă p q p q ă ´
Y1 A a 1 A a p X
Wp Y,X,A;p,Q s t “s u t “ u´ a p qQ Xp . p
a a
p q “ p X ` p X p q
a a
p q p q
s
Ď s s s
Consider the class of linear decision rules, we can minimize
s s
Lλn β;W ,W E W Y,X,A;p,Q φ aXJβ λ β .
φ p 1 ´1 q “ n » a p q p qfi` n } }1
aPt´1,1u
ÿ
p x x p – x p p fl
The target of the inference procedure is a low-dimensional projection of β˚ defined as the mini-
φ
mizer of
L β;W ,W E W X,A;p,Q φ XJβ W X,A;p,Q φ XJβ .
φ 1 ´1 1 ´1
p q “ p q p q` p q p´ q
“ ‰
Furthermore,Ď ifeitĎ herp X Ď p X ors Qs X Q XĎ ,thenE Ws s X W X Q X ,
a a a a a a a
p q “ p q p q “ p q r | s “ p q ” p q
and the above objective function is equivalent to
s s Ď
L β;W ,W E W φ XJβ W φ XJβ .
φ 1 ´1 1 ´1
p q “ p q` p´ q
“ ‰
In the general classification framework where nuisance parameters are involved, nonparametric
or machine learning algorithms are commonly employed to fit them to avoid model misspecification.
However, the convergence rates of the W ’s may be slower than O n´1{2 . We adopt a cross-fitting
a p
p q
procedure(Chernozhukov et al., 2017) to tackle this issue. We split the entire dataset into two halves.
x
The first half is used to fit the nuisance parameters, and to estimate the weights, W . We then
a
implement Algorithm 1 on the second half of the data to obtain the estimated coefficients and p-
x
values. Similarly, we can then fit the nuisance parameters on the second half and use the first half to
estimate the coefficients and conduct inference. Finally, to compensate for the efficiency loss due to
the cross-splitting, we can average the estimates and the kernel-smoothed decorrelated scores. The
details of this algorithm can be found in Algorithm 2.
4 Theoretical properties
In this section, we investigate the asymptotic properties of the proposed procedures under an ERM
with non-differentiable loss, potentially involving nuisance parameters. We focus on the uniform
validity oftheproposedprocedurestotestalow-dimensionalhypothesis. Fortestingahypothesiswith
a growing dimension, we propose a bootstrap procedure and prove its validity in the supplementary
material. In the supplementary material, we also provide the convergence rates of β and w .
φ φ,l
p p
11Algorithm 2: Inference of β˚ with nuisance parameters.
φ,1
Input: A random seed; n samples; a positive integer K.
Output: A p-value for H : β˚ “ 0.
0 φ,l
1 Randomly split data into halves I and J with equal sizes;
2 Estimate nuisance parameters using data in I by kernel regression after variable screening, and
r r
construct the estimated weights WpIq ’s on the samples in J using the estimated nuisance
a
r
r
parameters;
x r
3 On J, we implement Algorithm with weights W apIq ’s and denote the kernel-smoothed decorrelated
score and its variance estimate as SpJq and r σ2 ;
1
r φ ,nullplq x pJq,l
r
4 Similarly, we can obtain SpIq and σ2 . Aggregate them by
1 r r
φ ,nullplq pIq,l p
S φ1 ,nullplqrr “ S φpI 1 ,q nullplpq ` r S φpJ 1 ,q nullplq {2, σ l2 “ σ p2 Iq,l `σ p2 Jq,l {2.
Calculate the p-val rue by 2 1´´ rΦr pn´1{2|S φ1 r,r nullplq|{¯σ lq , where´Φp¨q r is the rcu¯mulative distribution
p p p
function of a standard norm´al distribution. ¯
r
p
4.1 Asymptotic properties without nuisance parameters
First,weconsiderthesituationwithoutnuisanceparameters. Weassumethefollowingconditionshold
on each split dataset in the sample-splitting (and cross-fitting) procedure. For notation simplicity, we
omit the subscript indicating the split dataset being used.
(C1) Thedesignmatrixisbounded,i.e., X M withprobability1;thereisaconstantC suchthat
8
} } ď
|xJβ φ˚ | ď C, and a constant c ą 0 such that |β φ˚ ,j0| ě c for some index j 0. Let f xj0|x ´j0px j0,a q
be the conditional density function of X given X and A. We assume that f1 x
j0 ´j0 xj0|x ´j0,ap j0q
and f2 x are bounded for both a 1 and 1.
xj0|x ´j0,ap j0q “ ´
(C2) There exists a positive constant γ such that for all t t 0,
0
ą ą
sup P t aXJβ˚ t τtγ,
j φ
p| ´ | ď qď
j,aPt´1,1u
where τ and t are some constants.
0
(C3) We assume that the eigenvalues of
2
J
E ∆ ∆ 1 AXJβ˚ t 0 XXJ
» ˜ 0 ` j φ ´ j ě ¸ fi
j“1
ÿ (
– fl
is bounded away from and 0 by some constants.
`8
12Condition (C1) assumesaboundeddesign whichis acommon condition inhigh-dimensionallitera-
ture(Ning and Liu,2017;Van de Geer et al.,2014;Dezeure et al.,2017). Inaddition, Condition(C1)
also assumes β˚ 0 and some regularity conditions on the conditional density function of the co-
φ ­“
variates; these conditions are firstly introduced in Koo et al. (2008) and then adopted in Peng et al.
(2016); Wang et al. (2019) to ensurethat the hessian of the L β is well defined and continuous in β.
φ
p q
Condition(C2)assumesthatsamplesdonotconcentrate onthejumpdiscontinuity points. Thisissat-
isfied when at least one covariate with a non-zero coefficient is continuous and has a bounded density
function. Condition(C2)ensurestheL-2convergence ofthe1 t aXJβ 0 to1 t aXJβ˚ 0 .
t j ´ φ ě u t j ´ φ ě u
Condition (C3) guarantees the uniform convergence of the variance estimator σ .
p l
Now, we provide the uniform validity of the kernel-smoothed decorrelated score under the null
p
hypothesis.
Theorem 1 Denote s1 max w˚ . Assume that β β˚ ∆ with probability approaching
“
l
}
φ,l}0
}
φ
´
φ}2
ď
β,2
to 1, s1 logp nh o 1 and max xJ w˚ is bounded by R o n1{6 . Taking µ δ Rh2
{p gb q“ p q l | ´l φ,l| p “ p q n — n ` gb`
R∆ ,awhere δ R logp nh 1{2. Further assume that ?nR h ?logp∆2γ{pγ`2q o 1 ,
β,2 n “ p {p gb qq p lo ` β,2 q “ p q
?n s1µ logp n h ∆ o 1 , and Rs1µ R2 logp n R2∆ R2h ?logp o 1 .
n lo β,2 n β,2 lo
p qp { ` ` q“ p q p ` { ` ` q “ p q
If Conditioans (C1)- (C3) are satisfied, under the null hypoathesis, we have
m lPHax
0
αPs pu 0p
,1q
P n1{2σ l´1S φ1 ,nullplq ď Φ´1 p1 ´α {2 q ´p1 ´α q “ o p p1 q,
ˇ ´ˇ ˇ ¯ ˇ
where H is the index set ofˇ ˇzeroˇ ˇcoeffipcienr ts in βˇ ˇ˚ under the null hypothesis. ˇ ˇ
0 φ
Define
2
J
σ2 E ∆ ∆ 1 AXJβ˚ t 0 X XJw˚ 2 .
l “ » 0 ` j φ ´ j ě p l ´ ´l φ,l q fi
˜ ¸
j“1
ÿ (
– fl
Theorem 1 implies that the asymptotic variance σ2 can be estimated by σ2 in Algorithm 1. The
l l
following corollary provides the uniform validity of the confidence interval constructed through the
p
one-step debiased estimator β .
φ,l
Corollary 1 Assume the samre conditions in Theorem 1, we have
max sup P n1{2σ´1I β β˚ Φ´1 1 α 2 1 α o 1 .
l αPp0,1q l l φ,l ´ φ,l ď p ´ { q ´p ´ q “ p p q
ˇ ´ˇ ´ ¯ˇ ¯ ˇ
ˇ ˇ ˇ ˇ
Define ˇ ˇ p p r ˇ ˇ
σ2 σ2 I2, I2 E ∆ δ t AXJβ˚ X XJw˚ 2 .
l
“
l
{
l l
“ «
j
p
j
´
φ
qp
l
´
´l φ,l
q ff
j
ÿ
r
Corollary 1 implies that the asymptotic variance σ2 can be estimated by σ2 I2.
{
1r3 p pRemark 2 Compared with the theoretical conditions for the marginal validity of the decorrelated score
under a differentiable strictly convex loss function without nuisance parameters, our conditions for the
uniform validity under a non-differentiable loss function assumes a more sparse model. To see this,
Ning and Liu (2017) show that the condition required for the marginal validity of the decorrelated score
is max s1,s˚ logp ?n 0; this is equivalent to ?n w w˚ 2 0 and ?n∆2 0. For the
t u { Ñ } φ,l ´ φ,l}2 Ñ β,2 Ñ
hinge loss, Theorem 1 requires that ?ns1µ ∆ 0 in addition to ?n∆2 0; this is equivalent
n β,2 Ñ p β,2 Ñ
to ?n w w˚ ∆ 0 and ?n∆2 0 (see the supplementary material for the convergence
} φ,l ´ φ,l}1 β,2 Ñ β,2 Ñ
rate of w). If w w˚ 2 w w˚ ∆ , the conditions in Theorem 1 and 2 are more
p }
φ,l
´
φ,l}2
À }
φ,l
´
φ,l}1 β,2
restrictive than Ning and Liu (2017). In supplementary material (the Proof of Theorem 2), we show
p p p
that under a dedicated sample-splitting algorithm, we can reduce this requirement to a weaker condition
than Ning and Liu (2017). With this sample-splitting algorithm, we only require that ?n w
φ,l
} ´
w˚ ∆ 0 in addition to ?n∆2 0. The requirements that ?n w w˚ ∆ 0 and
φ,l}2 β,2 Ñ β,2 Ñ } φ,l ´ φ,l}2 β,2 Ñp
?n∆2 0 are weaker than ?n w w˚ 2 0 and ?n∆2 0, when ∆ w w˚ .
β,2 Ñ } φ,l ´ φ,l}2 Ñ β,2 Ñ p β,2 À } φ,l ´ φ,l}2
However, the dedicated sample-splitting algorithm leads to a significant increase in computation time.
p p
Hence, we mainly focus on the proposed procedure in the paper.
4.2 Asymptotic properties with nuisance parameters
In this section, we investigate the theoretical property of the proposed inference procedure when
nuisance parameters exist.
(C4) There is a constant C such that max W ,W C. W x and 2 W x are bounded.
t
1 ´1
uď
Bxj0 a
p q
Bxj0 a
p q
(C5) There exist positive constants η and ζ such that
sup W W O n´ζ , sup E W X x W O n´η ,
a a p a a p
x,a,y ´ “ p q x | “ ´ “ p q
ˇ ˇ ˇ ” ı ˇ
ˇ ˇ ˇ ˇ
x Ď x
ˇ ˇ ˇ ˇ
where W is the point-wise limit of W as n .
a a
Ñ 8
ConditionĎ(C4) assumes that the weigxhts are bounded and smooth. Condition (C5) assumes that
the convergence rates of the nuisance parameters are upper bounded by O n´ζ ; the expectation of
p
p q
the weights with estimated nuisance parameters approximates W X faster than O n´η .
a p
p q p q
We now provide the uniform validity of the kernel-smoothed decorrelated score under the null
hypothesis when there exist nuisance parameters.
Theorem 2 Denote s1 max w˚ and δ R logp nh 1{2 Rn´η h . Assume that β
“
l
}
φ,l}0 n
“ p {p
gb
qq ` {
gb
}
φ
´
β˚ ∆ with probability approaching to 1, s1 logp nh o 1 and max xJ w˚ is bounded
φ}2 ď β,2 {p gb q “ p q l | ´1 φ,l| p
byR o n1{6 . Takingµ δ Rh2 R∆ . Faurtherassumethat?nR h n´η ?logp∆2γ{pγ`2q
“ p q n — n ` gb` β,2 p lo ` ` β,2 q“
14o 1 , ?n s1µ n´η logp n h ∆ o 1 , and Rs1µ R2 logp n R2n´ζ R2∆
n lo β,2 n β,2
p q p qp ` { ` ` q“ p q p ` { ` ` `
R2h ?logp o 1 . Iaf Conditions (C1)- (C5), under the null hypotheasis, we have
lo
q “ p q
m lPHax
0
αPs pu 0p
,1q
P n1{2σ l´1S φ1 ,nullplq ď Φ´1 p1 ´α {2 q ´p1 ´α q “ o p p1 q,
ˇ ´ˇ ˇ ¯ ˇ
ˇ ˇ r ˇ ˇ
where H is the index set ofˇzeroˇcoeffip cients in βˇ˚ under the null hypothesis aˇnd
0 φ
2
J
σ2 E aW ∆ ∆ 1 aXJβ˚ t 0 X XJw˚ 2 .
l “ » # a ˜ 0 ` j φ ´ j ě ¸+ p l ´ ´l φ,l q fi
a j“1
ÿ ÿ (
– fl
Compared with Theorem 1, the δ also involves the Rn´η h due to the additional nuisance param-
n gb
{
eters.
Corollary 2 Assume the same conditions in Theorem 2, we have
max sup P n1{2σ´1I β β˚ Φ´1 1 α 2 1 α o 1 .
l αPp0,1q l l φ,l ´ φ,l ď p ´ { q ´p ´ q “ p p q
ˇ ´ˇ ´ ¯ˇ ¯ ˇ
ˇ ˇ ˇ ˇ
ˇ ˇ p p r ˇ ˇ
Define
σ2 σ2 I2, I2 E ∆ W δ t aXJβ˚ X XJw˚ 2 .
l
“
l
{
l l
“
j a
p
j
´
φ
qp
l
´
´l φ,l
q
« a j ff
ÿÿ
r
Theorem2 and Corollary 2 imply that the asymptotic variance component σ2 and σ2 I2 can be esti-
l l{ l
mated by σ2 and σ2 I2, respectively. They require that R2n´ζ?logp o 1 , where n´ζ corresponds
l l{ l “ p q
to the slowest convergence rate of the nuisance parameters. This ensures that the I converges fast
p p p l
enough to construct a uniformly valid testing procedure/debiased estimator. Furthermore, compared
p
with Theorem 1, Theorem 2 also requires that ?nRn´η 0 and ?n s1µ n´η 0. These require-
n
Ñ p q Ñ
ments ensure that the uncertainty of estimating the nuisance parameters is asymptotically ignorable,
and does not affect the asymptotic variance. Under a doubly robust formulation, these requirements
are not restrictive and can be satisfied even when the nuisance parameter estimates have a slow
convergence rate (see Remark 3 for concrete examples).
Remark 3 In Theorem 2 and Corollary 2, we require that ?nRn´η 0. In classification with
Ñ
missing labels example, assume that there exists α,β 0 such that supx π
1
x π x O
p
n´α
ą | p q´ p q| “ p q
and supx p
a
x p
a
x O
p
n´β . When we assume that both π
1
π
1
and p
a
p a, the condition
| p q´ p q| “ p q “ p “s
that ?nRn´η 0 holds with η α β if ?nRn´α´β 0, since
p Ñ s “ ` Ñ s s
sup E W X,A;π,p X x W x O n´α´β .
a a p
x r p q| “ s´ p q “ p q
ˇ ˇ
ˇ ˇ
ˇ x p p ˇ
15When the missing mechanism is known, i.e., π π is known, the condition that ?nRn´η 0
1 1
“ Ñ
automatically holds since
p
sup E W X,A;π,p X x W x 0,
a a
x r p q | “ s´ p q “
ˇ ˇ
ˇ ˇ
for any p a. ˇ x p p ˇ
In the inference of individualized treatment rule example, assume that there exists α,β 0 such
p ą
that supx p
a
x p
a
x O
p
n´α and supx Q
a
x Q
a
x O
p
n´β . When we assume that
| p q´ p q| “ p q | p q´ p q| “ p q
both p p and Q Q , the condition ?nRn´η 0 holds with η α β if ?nRn´α´β 0 since
a a a a
“ p s“ pÑ s “ ` Ñ
s s sup E W a X,A;p,Q X x W a x O p n´α´β .
x r p q | “ s´ p q “ p q
ˇ ˇ
ˇ ˇ
When the treatment assiˇgnmxent mechpanpism is known, i.e., pˇa p
a
is known (for example, in ran-
“
domized clinical trials), the condition that ?nRn´η 0 automatically holds since
Ñ p
sup E W X,A;p,Q X x W x 0,
a a
x r p q| “ s´ p q “
ˇ ˇ
ˇ ˇ
for any Q a. ˇ x p p ˇ
With a bounded R, requirement on ?nRn´α´β 0 is equivalent to α β 1 2, which can be
p Ñ ` ą {
satisfied by many estimation methods. For example, in the inference of the individualized treatment
rule example, if we adopt generalized linear models with lasso penalties to estimate the propensity score
and the outcome regression models, it is satisfied when slogp ?n o 1 , where s is an upper bound
{ “ p q
of the number of the non-zero coefficients in the propensity score and the outcome regression models.
r r
If the propensity score is estimated by a regression spline estimator and is known to be p -dimensional
π
(low-dimensional) by design, we have α 1 3 if π belongs to the H¨older class with a smoothness
“ {
parameter greater than 5p (Newey, 1997). In this case, we only need β 1 6.
π
ą {
5 Simulation
In this section, we conduct simulation studies to examine the performance of the proposed inference
procedure. We consider two simulation scenarios: classification without nuisance parameters (see
Section 2.1)andestimating ITRwith nuisanceparameters (seeSection 3). Specifically, thepopulation
is a mixture of two subgroups with a probability 0.4 from Group I and probability 0.6 from Group
II. For Group I, the covariate vector is generated from N ξµ ,I 0.1e eJ ; for Group II, the
0 pˆp 1 1
p ´ q
covariate vector is generated from N ξµ ,I , where µ 1,1, 0.5,0.5,0, ,0 J and µ
1 pˆp 0 1
p q “ p´ ´ ¨¨¨ q “
1, 1, 1, 1,0, ,0 J.
p ´ ´ ´ ¨¨¨ q
(a) The label for Group I is A 1, and the label for Group II is A 1. The goal is to classify
“ “ ´
Group I from Group II based on the data;
16(b) ThetreatmentassignmentmechanismfollowsP A 1 X exp 0.25 X2 X2 X X 1
1 2 1 2
p “ | q “ p ˆp ` ` q{p `
0.25 X2 X2 X X . The observed outcome Y Y a if the treatment a is assigned to
1 2 1 2
ˆp ` ` qq “ p q
the patient, Y a XJγ 2 C X I a 1 ǫ, where γ 0.4, 0.4,0.4, 0.4,0, ,0 J
p q “ p q ` p q p “ q` “ p´ ´ ´ ¨¨¨ q
and ǫ follows a standard normal distribution. Here, C X X 0.5 for Group I patients and
1
p q “ | |`
C X X 0.5 for Group II patients. The goal is to estimate the optimal individualized
1
p q “ ´p| |` q
treatment rules that lead to the maximized outcome.
For each scenario, the parameter ξ controls the magnitude of the overlapping of two subgroups. Two
subgroups are easier to separate by a linear decision rule for a larger ξ. We gradually increase ξ from
0.1 to 1 (by 0.1) and the dimension of the covariate p from 500 to 1600, which lead to 30 settings in
total for each scenario.
We compare our proposed method with an ad-hoc method. We first use the hinge loss with the
lasso penalty to identify covariates with non-zero coefficients’ estimates. Then, we refit the hinge loss
without any penalty using the identified covariates and first 8 covariates, and employ the inference
procedure for low-dimensional settings (Koo et al., 2008) to construct test statistics and confidence
intervalsforidentifiedandfirst8covariates. InScenarioII,wefurthercombinethead-hocmethodwith
the cross-fitting algorithm proposed in Chernozhukov et al. (2017) as the competitor. For Scenario II,
we use the kernel regression after the variable screening to estimate nuisance parameters for both
the proposed and ad-hoc methods. For each simulation setting, we repeatedly simulate the training
samples 500 times, the sample sizes of which vary from 500 to 1600. To evaluate the inference
procedure,we reporttype I error (testing H :β˚ 0, wherel 5,6,7,8), power (testing H :β˚
0 φ,l “ “ 0 φ,l “
0, where l 1,2,3,4), the averaged coverage of the 95% confidence intervals for the eight coordinates
“
in β˚, and the averaged length of the 95% confidence intervals. The true value of β˚ is determined
φ φ
by the average over 500 replicates with n 2500. The coordinates of β˚ is set to zero if the absolute
“ φ
value of its average estimates is less than 0.01.
Figures 1 and 2 show the simulation results for Scenario I. The results show that the proposed
method has controlled Type-I errors and higher powers than the ad-hoc method. The ad-hoc method
has inflated type-I errors, which indicate that the decorrelation and sample-splitting procedures to
construct valid test statistics are necessary. From Figure 2, the proposed method achieves nominal
coverages and has shorter confidence intervals than the ad-hoc method across all settings. The simu-
lation results for Scenario II are presented in Figures 3 and 4. Again, the proposed method leads to
better results in terms of type-I errors control, coverage and lengths of confidence intervals.
171.00 1.00 1.00
0.75 0.75 0.75
0.50 0.50 0.50
0.25 0.25 0.25
800 1200 1600 800 1200 1600 0.25 0.50 0.75 1.00
Sample size Dimension Magnitude of signal−to−noise−ratio
0.5 0.5 0.5
0.4 0.4 0.4
0.3 0.3 0.3
0.2 0.2 0.2
0.1 0.1 0.1
0.0 0.0 0.0
800 1200 1600 800 1200 1600 0.25 0.50 0.75 1.00
Sample size Dimension Magnitude of signal−to−noise−ratio
Hinge Hinge−AdHoc
Figure 1: Testing results for Scenario I with the change of sample size when ξ “ 0.4 and p “ 800, the
change of ξ (n “ 800; p “ 1600) and the change of p (n “ 800; ξ “ 0.4). Line styles represent different
coefficients.
6 Real data examples
In this section, we apply our proposed inference approach in real world problems. Specifically, we con-
sider two scientific questions: 1) whether we can identify the risk factors associated with uncontrolled
HbA1c in a year, given the patients baseline characteristics; 2) whether we can identify drivingfactors
to inform better treatment strategies.
The data we used comes from the electronic medical records linked with Medicare claims data
on Type-II diabetic patients with complex commodities. These data are collected through the Heath
InnovationProgramattheUniversityofWisconsin. Itcontains n 9101patientswithmanycovariate
“
information. In order to answer the research questions highlighted above, we include 40 covariates
includingsociodemographic variables, disease history, baseline HbA1c levels, etc., and their first-order
18
rewoP
rorrE
I
epyT
rewoP
rorrE
I
epyT
rewoP
rorrE
I
epyT1.00 1.00 1.0
0.9
0.95 0.95
0.8
0.90 0.90
0.7
0.85 0.85
0.6
0.80 0.80 0.5
800 1200 1600 800 1200 1600 0.25 0.50 0.75 1.00
Sample size Dimension Magnitude of signal−to−noise−ratio
2.5 2.5 2.5
2.0 2.0 2.0
1.5 1.5 1.5
1.0 1.0 1.0
0.5 0.5 0.5
0.0 0.0 0.0
800 1200 1600 800 1200 1600 0.25 0.50 0.75 1.00
Sample size Dimension Magnitude of signal−to−noise−ratio
hinge hinge_adhoc
Figure 2: Coverage results for Scenario I with the change of sample size when ξ “ 0.4 and p “ 800, the
change of ξ (n “ 800; p “ 1600) and the change of p (n “ 800; ξ “ 0.4).
interactions in our analysis. As a variable screening procedure, we rank these covariates and their
interactions by the variances and select p 120 covariates with highest variances.The outcome of
“
interest in both questions is whether the patient successfully controlled his or her HbA1c below 8%
after one year.
6.1 Identify risk factors associated with uncontrolled HbA1c for
Type-II diabetic patients
Our goal is to identify patients and risk factors associated with uncontrolled HbA1c after one-year of
follow-up if following current clinical guideline. This can be considered as a classification problem.
Thelabelto predict, denoted as A, indicates whetherpatients haveuncontrolled HbA1cafter one-year
of follow-up. Specifically, we set A 1 if patients successfully control the HbA1c level after one-year
“
follow-up; and set A 1, otherwise.
“ ´
19
egarevoC
degarevA
htgneL
degarevA
egarevoC
degarevA
htgneL
degarevA
egarevoC
degarevA
htgneL
degarevA1.0 1.0 1.00
0.8 0.75
0.8
0.50
0.6
0.6
0.25
0.4
800 1200 1600 800 1200 1600 0.25 0.50 0.75 1.00
Sample size Dimension Magnitude of signal−to−noise−ratio
0.5 0.5 0.5
0.4 0.4 0.4
0.3 0.3 0.3
0.2 0.2 0.2
0.1 0.1 0.1
0.0 0.0 0.0
800 1200 1600 800 1200 1600 0.25 0.50 0.75 1.00
Sample size Dimension Magnitude of signal−to−noise−ratio
Hinge Hinge−AdHoc
Figure 3: Testing results for Scenario II with the change of sample size when ξ “ 0.8 and p “ 800, the
change of ξ (n “ 800; p “ 1600) and the change of p (n “ 800; ξ “ 0.8). Line styles represent different
coefficients.
We first use the linear SVM with a lasso penalty to estimate the decision rule and then conduct
inference on the estimated decision rule. Under a cross-validation procedure, the estimated decision
rule achieves a prediction accuracy of 0.895 with a standard deviation of 0.003. After controlling
the false discovery rate (FDR) at 0.05 by Benjamini–Hochberg procedure (Benjamini and Hochberg,
1995), we find that patients with valvular disease are more likely to suffer from uncontrolled HbA1c’s
(see Table 1); minorities with hypertension are more likely to have HbA1c under control after treat-
ment, which is probably due to the full consideration for patients with hypertension as one common
comorbidity in current clinical guideline. After controlling for the FDR, the ad-hoc method reveals
that 112 covariates out of 120 covariates are significant. However, when evaluating the length of the
constructed confidence intervals, the average interval length for the proposed method is much shorter
at 0.122 compared to that of the ad-hoc method, which is 0.244. The high number of identified risk
20
rewoP
rorrE
I
epyT
rewoP
rorrE
I
epyT
rewoP
rorrE
I
epyT1.00 1.00 1.0
0.9
0.95 0.95
0.8
0.90 0.90
0.7
0.85 0.85
0.6
0.80 0.80 0.5
800 1200 1600 800 1200 1600 0.25 0.50 0.75 1.00
Sample size Dimension Magnitude of signal−to−noise−ratio
2.5 2.5 8
2.0 2.0
6
1.5 1.5
4
1.0 1.0
2
0.5 0.5
0.0 0.0 0
800 1200 1600 800 1200 1600 0.25 0.50 0.75 1.00
Sample size Dimension Magnitude of signal−to−noise−ratio
hinge hinge_adhoc
Figure 4: Coverage results for Scenario II with the change of sample size when ξ “ 0.8 and p “ 800, the
change of ξ (n “ 800; p “ 1600) and the change of p (n “ 800; ξ “ 0.8).
factors and long length of confidence intervals may be attributed to biased test statistics and interval
estimations, as discussed in Section 5.
6.2 Identify driving factors of the estimated ITR to inform better
clinical guideline for Type-II diabetic patients
We aim to estimate theoptimal ITRandidentify thedrivingfactors to informfutureclinical guideline
for Type-II diabetic Medicare patients. We consider two treatment options: hypoglycemic agents
versus usual care. We set A 1 if the patient received hypoglycemic agent at baseline and A 1
“ “ ´
otherwise. The outcome Y is whether the patient successfully controlled his or her HbA1c below 8%
after one year. Under these specifications, the weighted classification problem proposed in Section 3
aimstoestimateanITRsuchthatunderthederivedITR,thechanceofsuccessfullycontrollingHbA1c
below 8% after one year is maximized for each individual.
To demonstrate that the estimated ITR informs a better treatment strategy than the current
21
egarevoC
degarevA
htgneL
degarevA
egarevoC
degarevA
htgneL
degarevA
egarevoC
degarevA
htgneL
degarevATable 1: Coefficients and p-value for the identified significant covariates of the estimated decision rule after
FDR control.
Covariate Coef P-value 95% - CI
Valvular disease (Yes) -2.079 1.528 10´99 [-2.118,-2.041]
ˆ
Hypertension (Yes) : Race (other) 2.126 3.687 10´88 [2.091,2.161]
ˆ
Table 2: Results for comparisons on averaged value functions from the cross-validation procedure.
Method Observed Hinge Logistic Q-Learning
Mean (Sd) 0.860 (0.008) 0.871 (0.013) 0.867 (0.011) 0.869 (0.019)
clinical practice, we use the cross-validation procedure to calculate the chance of successfully con-
trolling HbA1c if the estimated ITR were implemented. The results show that the current clinical
practice has a success rate of 0.860 with a standard deviation of 0.008; the estimated ITR has a
success rate of 0.871 with a standard deviation of 0.013. In terms of the identified driving factors of
the estimated ITR, using the proposed inference method, after controlling FDR at 0.05, we find that
patients having hypertension, especially with higher A1c levels at baseline, are more likely to benefit
from hypoglycemic agents in controlling HbA1c, which confirmed the importance of hypertension in
Type-II diabetes care. In addition, we also find that female patients with chronic complications, are
morelikely to benefitfromhypoglycemic agents in controlling HbA1c, seeTable3. We alsoimplement
the ad-hoc method as a comparison. After controlling for the FDR, the ad-hoc method identifies 107
significant driving factors out of 120 predictors. The averaged length of confidence intervals for the
ad-hoc method is 0.423, longer than that of the proposed method, which is 0.122.
7 Discussion
We propose a high-dimensional inference procedure for a non-differentiable convex loss function in
a general classification framework, which can be utilized to discover the driving factors in decision
making. In particular, combined with the cross-fitting algorithm, our procedure can accommodate
weights involving additional nuisanceparameters, which may beestimated vianonparametric or other
machine learning algorithms.
There are multiple directions that could be further studied in the future. First, although we allow
22Table 3: Coefficients and p-value for the identified significant covariates of the estimated optimal ITR after
FDR control. Special chronic conditions refer to chronic conditions including amputation, chronic blood
loss, drug abuse, lymphoma, metastatistic cancer, and peptic ulcer disease.
Covariate Coef P-value 95% - CI
Hypertension (Yes) -0.0451 8.32 10´4 [-0.0918,0.0016]
ˆ
Chronic Complications (Yes) : Female 0.1244 4.51 10´6 [0.0947,0.154]
ˆ
Hypertension (Yes) : Baseline A1c 0.0627 6.95 10´7 [0.0205,0.105]
ˆ
a non-differentiable surrogate loss, we still require the convexity of the surrogate loss. It would be
interesting to see how the proposed method can be extended to deal with a non-convex surrogate loss
or even zero-one loss without any relaxation. Second, we may consider distributed inference or online
inference. When the sample size is large, the computation may be infeasible on a single machine
due to limited resources. In this case, distributed inference can leverage the computation power of a
cluster of machines and reduce the runtime. When the sample size is limited, it would be important
to understand how to utilize newly collected samples to improve the learning process of the decision
rule. Third, in this work, we only consider bounded designs; we may consider extending these results
to sub-gaussian designs or designs with heavy tails.
23Supplemental Materials
Supplementary material: Supplementarymaterialcontainstheproofsofalltheorems,anextension
to testing high-dimensional hypothesis, and the convergence rate of β when using a hinge loss.
φ
(pdf file)
p
R-package for SVM Inference: R-packageSVMInferencecontainingcodetoperformtheproposed
methods (for hinge loss) described in the article. (GNU zipped tar file)
References
Bartlett, P. L., Jordan, M. I., and McAuliffe, J. D. (2006). Convexity, classification, and risk bounds.
Journal of the American Statistical Association, 101(473):138–156.
Bartlett, P. L. and Wegkamp, M. H. (2008). Classification with a reject option using a hinge loss.
Journal of Machine Learning Research, 9(8):1823–1840.
Benjamini, Y. and Hochberg, Y. (1995). Controlling the false discovery rate: a practical and powerful
approach to multiple testing. Journal of the Royal statistical society: series B (Methodological),
57(1):289–300.
Blanchard, G., Bousquet, O., Massart, P., et al. (2008). Statistical performance of support vector
machines. Annals of Statistics, 36(2):489–531.
Chen, G., Zeng, D., and Kosorok, M. R. (2016). Personalized dose finding using outcome weighted
learning. Journal of the American Statistical Association, 111(516):1509–1521.
Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., and Newey, W. (2017).
Double/debiased/neyman machine learning of treatment effects. American Economic Review,
107(5):261–65.
Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., and Robins, J.
(2018). Double/debiased machine learning for treatment and structural parameters. The Econo-
metrics Journal, 21(1):C1–C68.
Cortes, C. and Vapnik, V. (1995). Support-vector networks. Machine learning, 20(3):273–297.
Dezeure, R., Bu¨hlmann, P., and Zhang, C.-H. (2017). High-dimensional simultaneous inference with
the bootstrap. TEST, 26(4):685–719.
24Imbens, G. W. and Rubin, D. B. (2015). Causal Inference for Statistics, Social, and Biomedical
Sciences: An Introduction. Cambridge University Press.
Koo, J.-Y., Lee, Y., Kim, Y., and Park, C. (2008). A bahadur representation of the linear support
vector machine. Journal of Machine Learning Research, 9:1343–1368.
Liang, M., Choi, Y.-G., Ning, Y., Smith, M. A., and Zhao, Y.-Q. (2022). Estimation and inference
on high-dimensional individualized treatment rule in observational data using split-and-pooled de-
correlated score. Journal of Machine Learning Research (In print).
Lin, Y. (2000). Some asymptotic properties of the support vector machine. University of Wisconsin,
Madison.
Lin, Y. (2004). A note on margin-based loss functions in classification. Statistics & probability letters,
68(1):73–82.
Ma, R., Tony Cai, T., and Li, H. (2021). Global and simultaneous hypothesis testing for
high-dimensional logistic regression models. Journal of the American Statistical Association,
116(534):984–998.
Newey, W. K. (1997). Convergence rates and asymptotic normality for series estimators. Journal of
Econometrics, 79(1):147–168.
Ning, Y. and Liu, H. (2017). A general theory of hypothesis tests and confidence regions for sparse
high dimensional models. Annals of Statistics, 45(1):158–195.
Pan,Y.andZhao,Y.-Q.(2021). Improveddoublyrobustestimation inlearningoptimalindividualized
treatment rules. Journal of the American Statistical Association, 116(533):283–294.
Peng,B.,Wang,L.,andWu,Y.(2016). Anerrorboundforl1-normsupportvectormachinecoefficients
in ultra-high dimension. Journal of Machine Learning Research, 17(1):8279–8304.
Rubin, D. B. (1974). Estimating causal effects of treatments in randomized and nonrandomized
studies. Journal of educational Psychology, 66(5):688.
Rubin, D. B. (2005). Causal inference using potential outcomes. Journal of the American Statistical
Association, 100(469):322–331.
Shi, C., Song, R., Chen, Z., Li, R., et al. (2019). Linear hypothesis testing for high dimensional
generalized linear models. Annals of statistics, 47(5):2671–2703.
25Steinwart, I. (2005). Consistency of support vector machines and other regularized kernel classifiers.
IEEE transactions on information theory, 51(1):128–142.
Steinwart, I., Scovel, C., et al. (2007). Fast rates for support vector machines using gaussian kernels.
Annals of Statistics, 35(2):575–607.
Van de Geer, S., Bu¨hlmann, P., Ritov, Y., and Dezeure, R. (2014). On asymptotically optimal
confidence regions and tests for high-dimensional models. Annals of Statistics, 42(3):1166–1202.
Vert, R., Vert, J.-P., and Scho¨lkopf, B. (2006). Consistency and convergence rates of one-class svms
and related algorithms. Journal of Machine Learning Research, 7(5).
Wang, X., Yang, Z., Chen, X., and Liu, W. (2019). Distributed inference for linear support vector
machine. Journal of machine learning research, 20(113):1–41.
Wu, Y., Wang, L., andFu, H.(2021). Model-assisted uniformlyhonestinferenceforoptimaltreatment
regimes in high dimension. Journal of the American Statistical Association (In print).
Xue, F., Zhang, Y., Zhou, W., Fu, H., and Qu, A. (2020). Multicategory angle-based learning
for estimating optimal dynamic treatment regimes with censored data. Journal of the American
Statistical Association (In print).
Zhang, T. et al. (2004). Statistical behavior and consistency of classification methods based on convex
risk minimization. Annals of Statistics, 32(1):56–85.
Zhang, X., Wu, Y., Wang, L., and Li, R. (2016a). A consistent information criterion for support
vector machines in diverging model spaces. Journal of Machine Learning Research, 17(1):466–491.
Zhang, X., Wu, Y., Wang, L., and Li, R. (2016b). Variable selection for support vector machines in
moderately high dimensions. Journal of the Royal Statistical Society: Series B (Statistical Method-
ology), 78(1):53–76.
Zhao,Y., Zeng,D., Rush,A.J.,andKosorok, M.R.(2012). Estimatingindividualizedtreatmentrules
using outcome weighted learning. Journal of the American Statistical Association, 107(499):1106–
1118.
Zhao, Y.-Q., Laber, E. B., Ning, Y., Saha, S., and Sands, B. E. (2019). Efficient augmentation and
relaxation learning for individualized treatment rules using observational data. Journal of Machine
Learning Research, 20(1):1821–1843.
26Zhao, Y. Q., Zeng, D., Laber, E. B., Song, R., Yuan, M., and Kosorok, M. R. (2014). Doubly robust
learning for estimating individualized treatment with censored data. Biometrika, 102(1):151–168.
Zhou, X., Mayer-Hamblett, N., Khan, U., and Kosorok, M. R. (2017). Residual weighted learning
for estimating individualized treatment rules. Journal of the American Statistical Association,
112(517):169–187.
27