Alternators For Sequence Modeling
Mohammad Reza Rezaei1,3 and Adji Bousso Dieng2,3
1Institute of Biomedical Engineering, University of Toronto
2Department of Computer Science, Princeton University
3Vertaix
May 21, 2024
Abstract
Thispaperintroducesalternators,anovelfamilyofnon-Markoviandynamical
modelsforsequences. Analternatorfeaturestwoneuralnetworks: theobser-
vationtrajectorynetwork(OTN)andthefeaturetrajectorynetwork(FTN). The
OTNandtheFTNworkinconjunction,alternatingbetweenoutputtingsamples
intheobservationspaceandsomefeaturespace,respectively,overacycle1.
TheparametersoftheOTNandtheFTNarenottime-dependentandarelearned
viaaminimumcross-entropycriterionoverthetrajectories. Alternatorsare
versatile. Theycanbeusedasdynamicallatent-variablegenerativemodelsor
assequence-to-sequencepredictors. Whenalternatorsareusedasgenerative
models,theFTNproducesinterpretablelow-dimensionallatentvariablesthat
capturethedynamicsgoverningtheobservations. Whenalternatorsareused
as sequence-to-sequence predictors, the FTN learns to predict the observed
features. Inbothcases,theOTNlearnstoproducesequencesthatmatchthe
data. Alternators can uncover the latent dynamics underlying complex se-
quentialdata,accuratelyforecastandimputemissingdata,andsamplenew
trajectories. Weshowcasethecapabilitiesofalternatorsinthreeapplications.
WefirstusedalternatorstomodeltheLorenzequations,oftenusedtodescribe
chaoticbehavior. WethenappliedalternatorstoNeuroscience,tomapbrain
activitytophysicalactivity. Finally,weappliedalternatorstoClimateScience,
focusingonsea-surfacetemperatureforecasting. Inallourexperiments,we
foundalternatorsarestabletotrain,fasttosamplefrom,yieldhigh-quality
generatedsamplesandlatentvariables,andoutperformstrongbaselinessuch
asneuralODEsanddiffusionmodelsinthedomainswestudied.
Keywords: Alternators,TimeSeries,DynamicalSystems,GenerativeModels,
ChaoticSystems,Neuroscience,ClimateScience,MachineLearning
1 Introduction
Timeunderpinsmanyscientificprocessesandphenomena. Theseareoftenmodeled
using differential equations (Schrödinger, 1926; Lorenz, 1963; McLean, 2012).
1Weusedthename"alternator"becausewecandrawananalogywithelectromagnetism. The
OTNandtheFTNareanalogoustothemechanicalpartofanelectricalgenerator. Incontrast,the
trajectoriesareanalogoustothealternatingcurrentsthatresultfromturningmechanicalenergyinto
electricalenergy. SeeFigure1foranillustration.
1
4202
yaM
02
]LM.tats[
1v84811.5042:viXraFigure1: GenerativeprocessofanalternatorwithacycleoflengthT =3. Aninitial
randomfeature z isgeneratedfromafixeddistribution,e.g. astandardGaussian.
0
The rest of the observations x and features z are generated by alternating
1:T 1:T
betweensamplingfromthe OTN andthe FTN,respectively.
Developingtheseequationsrequiressignificantdomainknowledge. Overtheyears,
scientists have developed various families of differential equations for modeling
specific classes of problems. The interpretability of these equations makes them
appealing. However,differentialequationsareoftenintractable. Numericalsolvers
havebeendevelopedtofindapproximatesolutions,oftenwithsignificantcompu-
tationoverhead(WannerandHairer,1996;HopkinsandFurber,2015). Several
workshaveleveragedneuralnetworkstospeeduporreplacenumericalsolvers. For
example,neuraloperatorshavebeendevelopedtoapproximatelysolvedifferential
equations(Kovachkietal.,2023). Neuraloperatorsextendtraditionalneuralnet-
workstooperateonfunctionsinsteadoffixed-sizevectors. Theycanapproximate
solutionstocomplexfunctionalrelationshipsdescribedaspartialdifferentialequa-
tions. However,neuraloperatorsstillrequiredatafromnumericalsolverstotrain
theirneuralnetworks. Theymayfacechallengesingeneralizingtounseendataand
aresensitivetohyperparameters(Lietal.,2021;Kontolatietal.,2023).
Beyond their intractability, differential equations as a framework may not be
amenabletoalltime-dependentproblems. Forexample,itisnotclearhowtomodel
language,whichisinherentlysequential,usingdifferentialequations. Forsuchgen-
eralproblemsthatareinherentlytime-dependent,fullydata-drivenmethodsbecome
appealing. These methods are faced with the complexities that time-dependent
dataoftenexhibit,includinghighstochasticity,highdimensionality,andnontrivial
temporaldependencies. Generativemodelingisadata-drivenframeworkthathas
beenwidelyusedtomodelsequences. Severaldynamicalgenerativemodelshave
beenproposedovertheyears(Gregoretal.,2014;Fraccaroetal.,2016;Duetal.,
2016; Dieng et al., 2016, 2019; Kobyzev et al., 2020; Ho et al., 2020; Kobyzev
et al., 2020; Rasul et al., 2021; Yan et al., 2021; Dutordoir et al., 2022; Li et al.,
2022b;Neklyudovetal.,2022;Linetal.,2023;Lietal.,2024). Unlikedifferential
equations,generativemodelscanaccountforthestochasticityinobservations,don’t
requiredomainknowledge,andcanbeeasytogeneratedatafrom. However,they
arelessinterpretablethandifferentialequations,mayrequiresignificanttraining
data, and often fail to produce predictions and samples that are faithful to the
underlyingdynamics.
2Thispaperintroducesalternators,anewframeworkformodelingtime-dependent
data. Alternatorsmodeldynamicsusingtwoneuralnetworkscalledtheobservation
trajectory network (OTN) and the feature trajectory network (FTN), that alternate
betweengeneratingobservationsandfeaturesovertime,respectively. Thesetwo
neuralnetworksarefitbyminimizingthecrossentropyoftwodistributionsdefined
overtheobservationandfeaturetrajectories. Thisframeworkoffersgreatflexibility.
Alternatorscanbeusedasgenerativemodels,inwhichcasethefeaturescorrespond
tointerpretablelatentvariablesthatcapturethelow-dimensionalhiddendynamics
governingtheobservedsequences. Alternatorscanalsobeusedtomapanobserved
sequencetoanassociatedobservedsequence,forsupervisedlearning. Inthiscase,
the features represent low-dimensional representations of the input sequences.
These features are then used to predict the output sequences. Alternators can
beusedtoefficientlyimputemissingdata,forecast,samplenewtrajectories,and
encodesequences. Figure1illustratesthegenerativeprocessofanalternatorover
threetimesteps.
Section4showcasesthecapabilitiesofalternatorsinthreedifferentapplications:
the Lorenz attractor, neural decoding of brain activity, and sea-surface tempera-
tureforecasting. Inalltheseapplications,wefoundalternatorsoutperformother
sequence models, including neural ordinary differential equations (NODEs) and
generative diffusion models, and produce trajectories and predictions that prove
theirsuperiorabilitytogeneralize.
2 Alternators
Weareinterestedinmodelingtime-dependentdatainageneralandflexibleway.
We seek to be able to sample new plausible sequences fast, impute missing data,
forecastthefuture,learnthedynamicsunderlyingobservedsequences,learngood
low-dimensional representations of observed sequences, and accurately predict
sequences. Wenowdescribealternators,anewframeworkformodelingsequences
thatoffersallthecapabilitiesdescribedabove.
Generative Modeling. Weassumethedataarefromanunknownsequencedistri-
bution,whichwedenoteby p(x ),with T beingapre-specifiedsequencelength.
1:T
Here each x ∈ (cid:82)D x. We approximate p(x ) with a model with the following
t 1:T
generativeprocess:
1. Sample z ∼p(z ).
0 0
2. For t =1,...,T:
(a) Sample x
t
∼pθ(x t|z t−1).
(b) Sample z
t
∼pφ(z t|z t−1,x t).
Here z isasequenceoflow-dimensionallatentvariablesthatgoverntheobser-
0:T
vation dynamics. Each z ∈(cid:82)D z, with D << D . The distribution p(z ) is a prior
t z x 0
over the initial latent variable z 0. It is fixed. The distributions pθ(x t|z t−1) and
pφ(z t|z t−1,x t)relatetheobservationsandthelatentvariablesateachtimestep.
Theyareparameterizedbyθ andφ,whichareunknown. Thelatentvariable z t−1
3actsasadynamicmemoryusedtopredictthenextobservation x attime t andto
t
updateitsstateto z usingthenewlyobserved x .
t t
Thegenerativeprocessdescribedaboveinducesavalidjointdistributionoverthe
datatrajectory x andthelatenttrajectory z ,
1:T 0:T
T
(cid:89)
pθ,φ(x 1:T,z 0:T)=p(z 0) pθ(x t|z t−1)pφ(z t|z t−1,x t). (1)
t=1
Thisjointyieldsvalidmarginalsoverthelatenttrajectoryanddatatrajectory,
(cid:90) (cid:168) T (cid:171)
(cid:89)
pθ,φ(x 1:T)= p(z 0) pθ(x t|z t−1)pφ(z t|z t−1,x t) dz
0:T
(2)
t=1
(cid:90) (cid:168) T (cid:171)
(cid:89)
pθ,φ(z 0:T)= p(z 0) pθ(x t|z t−1)pφ(z t|z t−1,x t) dx
1:T
(3)
t=1
These two marginals describe flexible models over the data and latent trajecto-
ries. Even though the model is amenable to any distribution, here we describe
distributionsformodelingcontinuousdata. Wedefine
p(z )=(cid:78) (0,I) (4)
0
(cid:128)(cid:113) (cid:138)
pθ(x t|z t−1)=(cid:78) (1−σ2 x)· fθ(z t−1), D xσ2
x
(5)
pφ(z t|z t−1,x t)=(cid:78)
(cid:128)(cid:112)α
t
·gφ(x
t)+(cid:113)
(1−α
t
−σ z2)·z t−1, D zσ
z2(cid:138)
, (6)
where fθ(·)and gφ(·)aretwoneuralnetworks,calledthe OTN andthe FTN,respec-
tively. Hereσ2 andσ2 arehyperparameterssuchthatσ2<σ2. Thesequenceα
x z z x 1:T
isalsofixedandpre-specified. Eachα issuchthat0≤α ≤1−σ2.
t t z
Learning. Traditionally,latent-variablemodelssuchastheonedescribedaboveare
learnedusingvariationalinference(Bleietal.,2017). Hereweproceeddifferently
andfitalternatorsbyminimizingthecross-entropybetweenthejointdistribution
definingthemodel pθ,φ(x 1:T,z 0:T)andthejointdistributiondefinedastheproduct
of the marginal distribution over the latent trajectories pθ,φ(z 0:T) and the data
distribution p(x ). Thatis,welearnthemodelparametersθ andφ byminimizing
1:T
thefollowingobjective:
(cid:76)(θ,φ)=−(cid:69) p(x 1:T)·pθ,φ(z 0:T)(cid:2) logpθ,φ(x 1:T,z 0:T)(cid:3) . (7)
To gain more intuition on why minimizing (cid:76)(θ,φ) is a good thing to do, let’s
expanditusingBayes’rule,
(cid:76)(θ,φ)=−(cid:69) p(x 1:T)·pθ,φ(z 0:T)(cid:2) logpθ,φ(z 0:T)+logpθ,φ(x 1:T|z 0:T)(cid:3) (8)
=(cid:72)(pθ,φ(z 0:T))+(cid:69) pθ,φ(z 0:T)(cid:2) KL(p(x 1:T)∥pθ,φ(x 1:T|z 0:T)(cid:3) . (9)
Here (cid:72)(pθ,φ(z 0:T)) is the entropy of the marginal over the latent trajectory and
thesecondtermistheexpectedKullback-Leibler(KL)divergencebetweenthedata
distribution p(x 1:T) and pθ,φ(x 1:T|z 0:T), the conditional distribution of the data
trajectorygiventhelatenttrajectory.
4Eq. 9isilluminating. Indeed,itsaysthatminimizing(cid:76)(θ,φ)withrespecttoθ and
φ minimizestheentropyofthemarginaloverthelatenttrajectory,whichmaximizes
the information gain on the latent trajectories. This leads to good latent repre-
sentations. Ontheotherhand,minimizing(cid:76)(θ,φ)alsominimizestheexpected
KLbetweenthedatadistributionandtheconditionaldistributionoftheobserved
sequencegiventhelatenttrajectory. Thisforcesthe OTN tolearnparametersettings
thatgenerateplausiblesequencesandforcesthe FTN togeneratelatenttrajectories
thatyieldgooddatatrajectories.
It may be tempting to view Eq. 9 as the evidence lower bound (ELBO) objective
function optimized by a variational auto-encoder (VAE) (Kingma et al., 2019).
That would be incorrect for two reasons. First, interpreting Eq. 9 as an ELBO
wouldrequireinterpreting pθ,φ(z 0:T)asanapproximateposteriordistribution,ora
variationaldistribution,whichwecan’tdosince pθ,φ(z 0:T)dependsexplicitlyon
modelparameters. Second,Eq. 9isthesumoftheentropyof pθ,φ(z 0:T)andthe
expected log-likelihood of the observed sequence, whereas an ELBO would have
beenthesumoftheentropyofthevariationaldistributionandtheexpectedlog-joint
oftheobservedsequenceandthelatenttrajectory.
Tominimize(cid:76)(θ,φ)weexpanditfurtherusingthespecificdistributionswedefined
inEq. 4,Eq. 5,andEq. 6,
(cid:76)(θ,φ)=(cid:69) p(x 1:T)·pθ,φ(z
0:T)(cid:150) (cid:88)T
(cid:13) (cid:13)z t −µ z t(cid:13) (cid:13)2 2+
DD zσ σz2
2 ·(cid:13) (cid:13)x t −µ x t(cid:13) (cid:13)2
2(cid:153)
(10)
t=1 x x
µ
x
t
=(cid:113) (1−σ2 x)· fθ(z t−1)andµ
z
t
=(cid:112)α
t
·gφ(x t)+(cid:113) (1−α
t
−σ z2)·z
t−1
(11)
Although(cid:76)(θ,φ)isintractable—itstilldependsonexpectations—wecanapproxi-
mateitunbiasedlyusingMonteCarlo,
(cid:76)(θ,φ)≈ B1 (cid:88) b=B 1(cid:88) t=T 1(cid:20) (cid:13) (cid:13) (cid:13)z t(b)−µ
z
t(b)(cid:13) (cid:13) (cid:13)2 2+ DD xzσ σz2
2
x
·(cid:13) (cid:13) (cid:13)x t(b)−µ x( tb)(cid:13) (cid:13) (cid:13)2 2(cid:21) , (12)
where
x(1) ,...,x(B)
aredatatrajectoriessampledfromthedatadistribution2 and
1:T 1:T
z 0(1 :T) ,...,z 0(B :T) are latent trajectories sampled from the marginal pθ,φ(z 0:T) using
ancestralsamplingonEq. 3.
Algorithm 1 summarizes the procedure for dynamical generative modeling with
alternators. At each time step t, the OTN tries to produce its best guess for the
observation x t using the current memory z t−1. The output from the OTN is then
passed as input to the FTN to update the dynamic memory from z t−1 to z t. This
updateismodulatedbyα ,whichdetermineshowmuchwerelyonthememory
t
z t−1 comparedtothenewobservation x t. Whendealingwithdatasequencesfor
which we know certain time steps correspond to more noisy observations than
others,wecanuseα t torelymoreonthememory z t−1 thanthenoisyobservation
x . Whenthenoiseintheobservedsequencesisnotknown,whichisoftenthecase,
t
2Althoughthetruedatadistributionp(x )isunknown,wehavesomesamplesfromitwhichare
1:T
theobservedsequences,whichwecanusetoapproximatetheexpectation.
5Algorithm 1:DynamicalGenerativeModelingwithAlternators
Inputs: Samplesfrom p(x ),batchsize B,variancesσ2 andσ2,scheduleα
1:T x z 1:T
Initializemodelparametersθ andφ
whilenotconvergeddo
for b=1,...,B do
Drawinitiallatent z(b)∼(cid:78) (0,I )
0 D z
for t =1,...,T do
Drawnoisevariablesε(b)∼(cid:78) (0,I )andε(b)∼(cid:78) (0,I )
xt D x zt D z
Draw x t(b)= (cid:112)(cid:198)(1−σ2 x)· fθ(z t( −b) 1)+σ
x
·ε( xb t)
Draw z t(b)= α
t
·gφ(x t(b))+(cid:198)(1−α
t
−σ z2)·z t( −b) 1+σ z·ε( zb t)
end
end
Computeloss(cid:76)(θ,φ)inEq. 12using z1:B anddatasamplesfrom p(x )
0:T 1:T
Backpropagatetoget∇ θ(cid:76)(θ,φ)and∇ φ(cid:76)(θ,φ)
Updateparametersθ andφ usingstochasticoptimization,e.g. Adam.
end
we set α fixed across time. The ability to change α across time steps provides
t t
alternatorswithanenhancedabilitytohandlenoisyobservationscomparedtoother
generativemodelingapproachestosequencemodeling.
Sequence-To-Sequence Prediction. Whengivenpairedsequences x and y ,
1:T 1:T
wecanusealternatorstopredict y given x andvice-versa. Wesimplyreplace
1:T 1:T
p(x 1:T)pθ,φ(z 0:T)withtheproductofthejointdatadistribution, p(x 1:T,y 1:T)and
p(z ). Theobjectiveremainsthecrossentropy,
0
(cid:76)(θ,φ)=−(cid:69) p(x ,y )p(z )(cid:2) logpθ,φ(x 1:T,y 1:T,z 0)(cid:3) . (13)
1:T 1:T 0
ThisleadstothesametractableobjectiveasEq. 12,replacing z with y ,
1:T 1:T
(cid:76)(θ,φ)≈ B1 (cid:88) b=B 1(cid:88) t=T 1(cid:150) (cid:13) (cid:13) (cid:13)y t(b)−µ
y
t(b)(cid:13) (cid:13) (cid:13)2 2+ D
D
xyσ σ2
2
xy ·(cid:13) (cid:13) (cid:13)x t(b)−µ x( tb)(cid:13) (cid:13) (cid:13)2 2(cid:153) , (14)
(1) (B) (1) (B)
where x ,...,x and y ,...,y are sequence pairs sampled from the data
1:T 1:T 1:T 1:T (cid:112)
distribution,µ
x
t
=(cid:198)(1−σ2 x)·fθ(y t−1)andµ
y
t
= α t·gφ(x t)+(cid:113)(1−α
t
−σ2 y)·
y t−1. Algorithm2summarizestheprocedureforsequence-to-sequenceprediction
withalternators.
Imputation and forecasting. Imputing missing values and forecasting future
eventsaresimpleusingalternators. Wesimplyfollowthegenerativeprocessofan
alternator,eachtimeusing x
t
whenitisobservedorsamplingitfrom pθ(x t|z t−1)
whenitismissing.
Encoding sequences. Itiseasytogetalow-dimensionalsequentialrepresentation
∗ ∗
ofanewsequence x : wesimplyplug x ateachtimestep t inthemeanofthe
1:T t
distribution pφ(z t|z t−1,x t)inEq. 6,
z t∗=(cid:112)α
t
·gφ(x t∗)+(cid:113) (1−α
t
−σ z2)·z t∗ −1. (15)
6Algorithm 2:Sequence-To-SequencePredictionwithAlternators
Inputs: Samplesfrom p(x ,y ),batchsize B,σ2 andσ2,scheduleα
1:T 1:T x y 1:T
Initializemodelparametersθ andφ
whilenotconvergeddo
for b=1,...,B do
Drawinitiallatent z(b)∼(cid:78) (0,I )
0 D z
for t =1,...,T do
Computeµ( xb)=(cid:198)(1−σ2 x)· fθ(y t( −b) 1)
t (cid:112)
Computeµ( yb)= α
t
·gφ(x t(b))+(cid:113)(1−α
t
−σ2 y)·y t( −b)
1
t
end
end
Computeloss(cid:76)(θ,φ)inEq. 14usingsamplesfrom p(x ,y )
1:T 1:T
Backpropagatetoget∇ θ((cid:76)(θ,φ))and∇ φ((cid:76)(θ,φ))
Updateparametersθ andφ usingstochasticoptimization,e.g. Adam.
end
∗ ∗
Thesequencez isthelow-dimensionalrepresentationofx givenbythealterna-
1:T 1:T
(1)∗ (B)∗
tor. Touncoverthedynamicsunderlyingacollectionof B sequences x ,...,x
1:T 1:T
(b)∗
instead,wecansimplyuseEq. 15foreachsequence x andtakethemeanfor
1:T
eachtimestep. Theresultingsequenceisacompactrepresentationofthedynamics
governingtheinputsequences.
3 Related Work
Alternatorsareafamilyofmodelsfortime-dependentdata. Assuch,theyarerelated
tomanyexistingdynamicalmodels.
Autoregressivemodels(ARs)defineaprobabilitydistributionforthenextelement
in a sequence based on past elements, making them effective for modeling high-
dimensional,structureddata(Gregoretal.,2014). Theyhavebeenwidelyusedin
applications such as speech recognition Chung et al. (2019), language modeling
Blacketal.(2022),andimagegenerativemodelingChenetal.(2018b). However,
ARsdon’thavelatentvariables,whichlimitstheirflexibility.
Temporal point processes (TPPs) were introduced to model event data (Du et al.,
2016). TPPs model both event timings and associated markers by defining an
intensity function that is a nonlinear function of history using recurrent neural
networks (RNNs). However, TPPs lack latent variables and are only amenable to
discretedata,whichlimitstheirapplicability.
Dynamical variational auto-encoders (VAEs) such as variational recurrent neural
networks(VRNNs)(Chungetal.,2015)andstochasticrecurrentneuralnetworks
(SRNNs) (Fraccaro et al., 2016) model sequences by parameterizing VAEs with
RNNs and bidirectional RNNs, respectively. This enables these methods to learn
good representations of time-dependent data by maximizing the evidence lower
bound(ELBO). However,theyfailtogeneralizeandstrugglewithgeneratinggood
7observationsduetotheirparameterizationsofthesamplingprocess.
Differential equations are the traditional way dynamics are modeled in the sci-
ences. However,theymaybeslowtoresolve. Recently,neuraloperatorshavebeen
developed to extend traditional neural networks to operate on functions instead
of fixed-size vectors (Kovachki et al., 2023). They can approximate solutions to
complexfunctionalrelationshipsmodeledaspartialdifferentialequations. However,
neuraloperatorsrelyonnumericalsolverstotraintheirneuralnetworks. Theymay
struggletogeneralizetounseendataandaresensitivetohyperparameters(Lietal.,
2021;Kontolatietal.,2023).
Neuralordinarydifferentialequations(NODEs)modeltime-dependentdatausing
a neural network to predict an initial latent state which is then used to initialize
anumericalsolverthatproducestrajectories(Chenetal.,2018a). NODEsenable
continuous-time modeling of complex temporal patterns. They provide a more
flexible framework than traditional ODE solvers for modeling time series data.
However, NODEs are still computationally costly and can be challenging to train
since they require careful tuning of hyperparameters and still rely on numerical
solvers to ensure stability and convergence (Finlay et al., 2020). Furthermore,
NODEsaredeterministic;stochasticityin NODEsisonlymodeledintheinitialstate.
Thismakes NODEsnotidealformodelingnoisyobservations.
Probability flows are generative models that utilize invertible transformations to
convertsimplebasedistributionsintocomplex,multimodaldistributions(Kobyzev
etal.,2020). Theyemploycontinuous-timestochasticprocessestomodeldynam-
ics. Thesemodelsexplicitlyrepresentprobabilitydistributionsusingnormalizing
flow(Papamakariosetal.,2021). Whilenormalizingflowsofferadvantagessuchas
tractablecomputationoflog-likelihoods,theyhavehigh-dimensionallatentvariables
andrequireinvertibility,whichhindersflexibility.
Recently, diffusionmodelshavebeenusedtomodelsequences(Linet al.,2023).
For example denoising diffusion probabilistic models (DDPMs) can be used to de-
noise a sequence of noise-perturbed data by iteratively removing the noise from
the sequence (Rasul et al., 2021; Yan et al., 2021; Biloš et al., 2022; Lim et al.,
2023). Thisiterativerefinementenables DDPMstogeneratehigh-qualitysamples.
TimeGradisadiffusion-basedapproachthatintroducesnoiseateachtimestepand
graduallydenoisesitthroughabackwardtransitionkernelconditionedonhistorical
timeseries(Rasuletal.,2021). ScoreGradfollowsasimilarstrategybutextends
thediffusionprocesstoacontinuousdomain,replacingdiscretestepswithinterval-
basedintegration(Yanetal.,2021). Neuraldiffusionprocesses(NDPs)areanother
typeofdiffusionprocessesthatextenddiffusionmodelstoGaussianprocesses,de-
scribingdistributionsoverfunctionswithobservableinputsandoutputs(Dutordoir
etal.,2022). Discretestochasticdiffusionprocesses(DSDPs)viewmultivariatetime
series data as values from a continuous underlying function (Biloš et al., 2022).
Unliketraditionaldiffusionmodels,whichoperateonvectorobservationsateach
time point, DSDPs inject and remove noise using a continuous function. D3VAE
is yet another diffusion-based model for sequences (Li et al., 2022a). It starts by
employingacoupleddiffusionprocessfordataaugmentation,whichaidsincreating
additionaldatapointsandreducingnoise. Themodelthenutilizesabidirectional
auto-encoder (BVAE) alongside denoising score matching to further enhance the
8Table1: Alternatorsoutperformseveraldynamicalmodelsonpredictingthedynam-
icsdefinedbytheLorenzequationsintermsofmeanabsoluteerror(MAE),mean
squarederror(MSE),andPearsoncoefficientofcorrelation(CC).
Method MAE↓ MSE↓ CC↑
SRNN
0.052±0.017 0.148±0.007 0.955±0.001
VRNN
0.074±0.003 0.173±0.002 0.963±0.001
NODE
0.044±0.013 0.220±0.012 0.888±0.012
Alternator 0.030 ± 0.005 0.076 ± 0.003 0.977 ± 0.001
quality of the generated samples. Finally, TSGM is a diffusion-based approach to
sequence modeling that uses three neural networks to generate sequences (Lim
etal.,2023). Anencoderispretrainedtomaptheunderlyingtimeseriesdatainto
a latent space. Subsequently, a conditional score-matching network samples the
hiddenstates,whichadecoderthenmapsbacktothesequence. Thismethodology
enables TSGM togenerategoodsequences. Allthesediffusion-basedmethodslack
low-dimensional dynamical latent variables and are slow to sample from as they
oftenrelyonLangevindynamics.
Actionmatching(AM)isamethodthatlearnsasystem’scontinuousdynamicsfrom
snapshotsofitstemporalmarginals,usingcross-sectionalsamplesthatareuncor-
related over time (Neklyudov et al., 2022). AM allows sampling from a system’s
timeevolutionwithoutrelyingonexplicitassumptionsabouttheunderlyingdynam-
icsorrequiringcomplexcomputationslikeback-propagationthroughdifferential
equations. However, AM doesnothavelow-dimensionaldynamicallatentvariables,
whichmaylimititsflexibility.
Alternators differ from the approaches above and are versatile, enabling various
tasksandusecaseswithinasingleframework. Inparticular,alternatorsadmitlow-
dimensionaldynamicallatentvariablesandprovideamechanismfordealingwith
noisydata,whichgivesthemsomeinterpretabilityandflexibilitywhenmodeling
time-dependentdata.
4 Experiments
Wenowshowcasethecapabilitiesofalternatorsinthreedifferentdomains. Wefirst
studiedtheLorenzattractor,whichexhibitscomplexchaoticdynamics. Wefound
alternators are better at capturing these dynamics than baselines such as VRNNs,
SRNNs,andNODEs. Wealsousedalternatorsforneuraldecodingonthreedatasetsto
mapbrainactivitytomovements. Wefoundalternatorsoutperform VRNNs, SRNNs,
and NODEs. Finally, we show alternators can produce more accurate sea-surface
temperatureforecaststhandiffusionmodelswhileonlytakingafractionofthetime
requiredbydiffusionmodels.
9Figure2: AlternatorsarebetterattrackingthechaoticdynamicsdefinedbyaLorenz
attractor, especially during transitions between attraction points, than baselines
suchas VRNNs, SRNNs,and NODEs.
4.1 Model System: The Lorenz Attractor
TheLorenzattractorisachaoticsystemwithnonlineardynamicsdescribedbyaset
ofdifferentialequations(Lorenz,1963). Weusetheattractortosimulatefeatures
z ,with z ∈(cid:82)3 forall t ∈{1,...,T}and T =400. WesimulatefromtheLorenz
1:T t
equationsbyaddingnoisevariablesε ,ε ,ε tothecoordinates,
1 2 3
dz
1(t)=σ·(z (t)−z (t))+ε , ε ∼(cid:78) (0,1)
2 1 1 1
dt
dz
2(t)=z (t)·(ρ−z (t))−z (t)+ε , ε ∼(cid:78) (0,1)
1 3 2 2 2
dt
dz
3(t)=z (t)·z (t)−β·z (t)+ε , ε ∼(cid:78) (0,1).
1 2 3 3 3
dt
Theparametersσ,ρ,β controlthedynamics. Herewesetσ=10,ρ=28,β =8/3
todefinecomplexdynamicswhichwehopetocapturewellwithalternators. Given
the features z , we simulated x , with each x ∈ {0,1}100, by sampling from
1:T 1:T t
a time-dependent Poisson point process. We selected the time resolution small
enoughtoensure x ∈{0,1}forall t. WeusethePoissonprocesstomimicspiking
t
activitydata. Empiricalstudieshaveshownthatspikecountswithinfixedintervals
oftenalignwellwiththePoissondistribution,makingitapracticalandwidelyused
modelinneuroscience(Rezaeietal.,2021;Truccoloetal.,2005). Theintensityof
thepointprocessisanonlinearfunctionofthefeatures,λˆ (z,t)=λ (z)∗λ (t),
j j j,H
wherewedefine
λ (z)=exp(cid:150) a −(cid:88) (z t −µ j,z t)2(cid:153) andλ (t)= (cid:88) 1−exp(cid:130) −(t−s n)2(cid:140)
j j 2σ2 j,H 2σ2
z t∈z j,z
t
s n∈S
j
j
for j ∈{1,...,100}. Here µ and σ2 are the center and width of the receptive
j,z t j,z t
fieldmodelofz , a isthemaximumfiringrate,andS isthecollectionofallthe
t j j
10Mean Absolute Error Mean Squared Error Correlation Coefficient
0.14
0.8
0.12 0.4
0.10 0.6
0.3
0.08
0.4
0.06 0.2
0.04
0.1 0.2
0.02
0.00 0.0 0.0
0.150 0.4 0.8
0.125 0.3 0.6
0.100
0.075 0.2 0.4
0.050
0.1 0.2
0.025
0.000 0.0 0.0
0.20
0.6 0.6
0.15 0.5 0.5
0.4 0.4
0.10 0.3 0.3
0.2 0.2
0.05
0.1 0.1
0.00 0.0 0.0
Alternator NODE VRNN SRNN Alternator NODE VRNN SRNN Alternator NODE VRNN SRNN
Figure3: AlternatorsoutperformVRNNs,SRNNs,andNODEsontrajectoryprediction
intheneuraldecodingtaskonallthreedatasetsintermsof MAE, MSE,and CC.
spiketimesofthe jthchannel. Theyaredrawnfrompriors,
µ ∼U(µ(z )−2∗σ(z ),µ(z )+2∗σ(z )) (16)
j,z t t t t
t
σ ∼U(σ ,1/100), σ ∼U(σ ,1/100), and a ∼U(fr ,fr ). (17)
j,z min j min j min max
t
We set fr = 0, fr = 10, and σ = 0.001. We then used the paired data
min max min
(x ,z )inasequence-to-sequencepredictiontasktotrainanalternatoraswell
1:T 1:T
asa NODE,an SRNN,anda VRNN. Wedidn’tincludeadiffusionmodelasabaseline
heresinceitlacksadynamicallatentprocessthatcanbeinferredfromthespiking
activitiesforsequence-to-sequenceprediction.
We evaluate each model by simulating 100 new paired sequences following the
samesimulationprocedure. Weusedthenewobservationstopredicttheassociated
simulatedfeatures. Weassessfeaturetrajectorypredictionperformanceusingthree
metricsthatcomparepredictionsfromeachmodelwiththegroundtruthfeatures:
MAE, MSE,and CC.
Weused2-layerattentionmodels,eachfollowedbyahiddenlayercontaining10
units for both the observation trajectory network (OTN) and the feature trajectory
network(FTN). Wesetσ
z
=0.1,σ
x
=0.3,andα
t
=0.3isfixedforallt. Themodels
weretrainedfor500epochsusingtheAdamoptimizerwithaninitiallearningrate
of 0.01. We applied a cosine annealing learning rate scheduler with a minimum
11
yrosnesotamoS
xetroC
rotoM
supmacoppiHMean Absolute Error Mean Squared Error Correlation Coefficient
0.30 0.6 0.25
0.25 0.5 0.20
0.20 0.4
0.15
0.15 0.3
0.10
0.10 0.2
0.05
0.05 0.1
0.00 0.0 0.00
0.7
0.4 0.35
0.6 0.30
0.3 0.5 0.25
0.4 0.20
0.2
0.3 0.15
0.2 0.10
0.1
0.1 0.05
0.0 0.0 0.00
0.6 0.8
0.4 0.5
0.6
0.4 0.3
0.3 0.4
0.2
0.2
0.2 0.1
0.1
0.0 0.0 0.0
Alternator VRNN SRNN Alternator VRNN SRNN Alternator VRNN SRNN
Figure 4: Alternators outperform VRNNs and SRNNs on forecasting in the neural
decodingtaskonallthreedatasetsintermsof MAE, MSE,and CC. Theresultsare
averagedacrossseveralforecastingsettings,wherewevariedtheforecastingrate
from 10% to 50%. The standard errors are shown as vertical bars. The NODE
baselineisn’tshownsince NODEsaren’treadilyamenabletoforecastingtasks.
learningrateof1e-4and10warm-upepochs.
Figure2showsthesimulatedfeatures,alongwithfitsfromanalternator,an SRNN,
a VRNN,anda NODE. Thealternatorisbetteratpredictingthetruelatenttrajectory
comparedtothebaselines. Specifically,alternatorstrackwellthechaoticdynamics
characterizedbytheLorenzattractor,especiallyduringtransitionsbetweenattrac-
tionpoints. TheresultspresentedinTable1quantifythis,withalternatorsachieving
better MAE, MSE,and CC thanthebaselines.
4.2 Neural Decoding: Mapping Brain Activity To Movement
Neural decoding is a fundamental challenge in neuroscience that helps increase
our understanding of the mechanisms linking brain function and behavior. In
neuraldecoding,neuraldataaretranslatedintoinformationaboutvariablessuch
asmovement,decision-making,perception,orcognitivefunctions(Donneretal.,
2009;Linetal.,2022;Rezaeietal.,2018,2023).
Weusealternatorstodecodeneuralactivitiesfromthreeexperiments. Inthefirst
experiment, the data recorded are the 2D velocity of a monkey that controlled a
12
xetroC
rotoM
yrosnesotamoS
supmacoppiHMean Absolute Error Mean Squared Error Correlation Coefficient
0.7
0.35 0.8
0.6
0.30
0.25 0.5 0.6
0.20 0.4
0.4
0.15 0.3
0.10 0.2 0.2
0.05 0.1
0.00 0.0 0.0
0.35 0.7 0.8
0.30 0.6
0.25 0.5 0.6
0.20 0.4
0.4
0.15 0.3
0.10 0.2 0.2
0.05 0.1
0.00 0.0 0.0
0.4 0.8 0.6
0.5
0.3 0.6
0.4
0.2 0.4 0.3
0.2
0.1 0.2
0.1
0.0 0.0 0.0
Alternator VRNN SRNN Alternator VRNN SRNN Alternator VRNN SRNN
Figure 5: Alternatorsoutperform VRNNsand SRNNsonmissingvalueimputationin
theneuraldecodingtaskonallthreedatasetsintermsof MAE, MSE,and CC. The
resultsareaveragedacrossseveralimputationsettings,wherewevariedthemissing
valueratefrom10%to95%. Thestandarderrorsareshownasverticalbars. The
NODE baseline isn’t shown since NODEs aren’t readily amenable to missing value
imputation.
cursoronascreenalongwitha21-minuterecordingofthemotorcortex,containing
164neurons. Inthesecondexperiment,thedataarethe2Dvelocityofthesame
monkeypairedwithrecordingfromthesomatosensorycortex,insteadofthemotor
cortex. The recording was 51 minutes long and contained 52 neurons. Finally,
the third experiment yielded data on the 2D positions of a rat chasing rewards
onaplatformpairedwithrecordingsfromthehippocampus. Thisrecordingis75
minuteslongandhas46neurons. WereferthereadertoGlaseretal.(2020,2018)
formoredetailsonhowthesedatawerecollected. Fortheseexperiments,thetime
horizonsweredividedinto1-secondwindowsfordecoding,withatimeresolution
of5ms. Weusethefirst70%ofeachrecordingfortrainingandtheremaining30%
asthetestset.
SimilarlytotheLorenzexperiment,weusedattentionmodelscomprisingtwolayers,
eachfollowedbyahiddenlayercontaining10unitsforboththeOTNandtheFTN.
Wesetσ =0.1,σ =0.2,andα =0.4wasfixedforall t. Themodelunderwent
z x t
trainingfor1500,1500,and1000epochsforMotorCortex,Somatosensory,and
Hippocampusdatasets; respectively. WeusedtheAdamoptimizerwithaninitial
13
xetroC
rotoM
yrosnesotamoS
supmacoppiHFigure 6: A set of 20 trajectories sampled from different models conditional on
spikingactivities. Thethreedatasetsare: A)Motorcortex,B)Somatosensorycortex,
andC)Hippocampus. Thealternatorproducessamplesthatareclosertotheground
truthdynamicsshowninblack.
learningrateof0.01. Wealsousedacosineannealinglearningrateschedulerwith
aminimumlearningrateof1e-4and5warm-upepochs.
Inthisexperiment,wedefinethefeaturesasthevelocity/positionandtheobserva-
tionsastheneuralactivitydata. Webenchmarkedalternatorsagainststate-of-the-art
models,including VRNNs, SRNNs,and NODEsontheirabilitytoaccuratelypredict
velocity/positiongivenneuralactivity. Wedidnotperformtheexperimentswith
missing values in the observations (imputations and forecasting) with the NODE
baselinebecause NODE usesan RNN toencodeallobservationsintoalatentspace,
andRNNslackamechanismtoaddressmissingvalues. Wedidn’tincludeadiffusion
modelbaselineforthesamereasonasintheLorenzexperiment,whichwasalsoa
supervisedlearningtask. WeusedthesamemetricsasfortheLorenzexperiment.
TheresultsareshowninFigure4,Figure5,andFigure6. Alternatorsarebetterat
decodingneuralactivitythanthebaselinesonallthreedatasets.
4.3 Sea-Surface Temperature Forecasting
Accurate sea-surface temperature (SST) dynamics prediction is indispensable for
weatherandclimateforecastingandcoastalactivityplanning. Weusedalternators
to forecast SST. The SST dataset we consider here is the NOAA OISSTv2 dataset,
whichcomprisesdailyweatherimageswithhigh-resolution SST datafrom1982to
2021(Huangetal.,2021). Weuseddatafrom1982to2019(15,048datapoints)
14Table 2: Performanceofdifferentmodelsonsea-surfacetemperatureforecasting1
to7daysahead. Numbersareaveragedovertheevaluationhorizon. Forspread-skill
ratio(SSR),avaluecloserto1isbetter. Thetimecolumnrepresentsthetimeneeded
toforecastall7timestepsforasinglebatch. Alternatorsperformsimilarlytothe
state-of-the-artapproach MCVD whilebeingsignificantlyfaster.
Method CRPS ↓ MSE↓ SSR Time[s]↓
DDPM-P 0.281±0.004 0.180±0.011 0.411±0.046 0.4241
DDPM-D 0.267±0.003 0.164±0.004 0.406±0.042 0.4241
DDPM 0.246±0.005 0.177±0.005 0.674±0.011 0.3054
Dyffusion 0.224±0.001 0.173±0.001 1.033±0.005 4.6722
MCVD 0.216 0.161 0.926 79.167
Alternator 0.221±0.031 0.144±0.045 1.325±0.314 0.7524
fortraining,datafromtheyear2020(396datapoints)forvalidation,anddatafrom
2021(396datapoints)fortesting. Wefurtherturnedthetrainingdataintoregional
imagepatches,selecting11boxeswitharesolutionof60×60(latitude×longitude)
intheeasterntropicalPacificOcean. Specifically,wepartitionedtheglobeintoa
grid,creating60×60(latitude×longitude)tiles(Cachayetal.,2023). Elevengrid
tilesarestrategicallysubsampled,withafocusontheeasterntropicalPacificregion,
establishingarefinedandconsistentdatasetforsubsequent SST forecasting1to7
daysintothefuture.
We used an ADM (Dhariwal and Nichol, 2021) to jointly model the OTN and the
FTN. TheADMisaspecificU-Netarchitecturethatincorporatesattentionlayersafter
eachintermediate CNN unitintheU-Net. Weselected128basechannels,2ResNet
blocks,andchannelmultipliersof{1,2,2}. Wetrainedthemodelwithabatchsize
of10for800epochs,settingσ =0.2,σ =0.3,andfixedα =0.6forall t. We
z x t
usedtheAdamoptimizerwithaninitiallearningrateof0.001andappliedacosine
annealing learning rate scheduler with a minimum learning rate of 1e−4 and 5
warm-upepochs.
We compared the alternator against several baselines: DDPM (Ho et al., 2020),
MCVD (Voleti et al., 2022), DDPM with dropout enabled at inference time (Gal
andGhahramani,2016)(DDPM-D), DDPM withrandomperturbationsoftheinitial
conditions/inputswithafixedvariance(DDPM-P)(Pathaketal.,2022),anddyffu-
sion(Cachayetal.,2023). Weusedseveralperformancemetrics. Onesuchmetric
isthecontinuousrankedprobabilityscore(CRPS)(MathesonandWinkler,1976),a
properscoringrulewidelyusedintheprobabilisticforecastingliterature(Gneiting
andKatzfuss,2014;deBézenacetal.,2020). Inadditionto CRPS,wealsoused MSE
and SSR. SSR assessesthereliabilityoftheensembleandisdefinedastheratioof
thesquarerootoftheensemblevariancetothecorrespondingensemblerootmean
squarederror(RMSE). Itservesasameasureofthedispersioncharacteristics,with
valueslessthan1indicatingunderdispersion(i.e.,overconfidenceinprobabilistic
forecasts)andlargervaluesdenotingoverdispersion(Fortinetal.,2014). Weused
a50-memberensembleforthepredictionsforeachmethod. MSEiscomputedbased
ontheensemblemeanprediction.
15Table2showstheresults. Thealternatorachievessimilarpredictionperformanceto
thestate-of-the-artbaseline, MCVD,whilebeingmorethan100timesfaster.
5 Conclusion
Weintroducedalternators,anewflexiblefamilyofnon-Markoviandynamicalmod-
els for sequences. Alternators admit two neural networks, called the observation
trajectory network (OTN) and the feature trajectory network (FTN), that work in
conjunction to produce observation and feature trajectories, respectively. These
neuralnetworksarefitbyminimizingcross-entropybetweentwojointdistributions
overthetrajectories,thejointdistributiondefiningthemodelandthejointdistri-
butiondefinedastheproductofthemarginaldistributionofthefeaturesandthe
marginal distribution of the observations—the data distribution. We showcased
thecapabilitiesofalternatorsinthreedifferentapplications: theLorenzattractor,
neuraldecoding,andsea-surfacetemperatureprediction. Wefoundalternatorsto
bestabletotrain,fasttosamplefrom,andaccurate,outperformingseveralstrong
baselinesinthedomainswestudied.
Acknowledgements
Adji Bousso Dieng acknowledges support from the National Science Foundation,
OfficeofAdvancedCyberinfrastructure(OAC):#2118201.
Dedication
ThispaperisdedicatedtoThomasIsidoreNoëlSankara.
16References
Biloš, M., Rasul, K., Schneider, A., Nevmyvaka, Y., and Günnemann, S. (2022).
Modeling temporal data as continuous functions with process diffusion. arXiv
preprintarXiv:2211.02590.
Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L., He, H.,
Leahy,C.,McDonell,K.,Phang,J.,etal.(2022). Gpt-neox-20b: Anopen-source
autoregressivelanguagemodel. arXivpreprintarXiv:2204.06745.
Blei,D.M.,Kucukelbir,A.,andMcAuliffe,J.D.(2017). Variationalinference: Are-
viewforstatisticians.JournaloftheAmericanstatisticalAssociation,112(518):859–
877.
Cachay, S. R., Zhao, B., James, H., and Yu, R. (2023). Dyffusion: A dynamics-
informed diffusion model for spatiotemporal forecasting. arXiv preprint
arXiv:2306.01984.
Chen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud, D. K. (2018a). Neural
ordinarydifferentialequations. Advancesinneuralinformationprocessingsystems,
31.
Chen, X., Mishra, N., Rohaninejad, M., and Abbeel, P. (2018b). Pixelsnail: An
improvedautoregressivegenerativemodel. InInternationalconferenceonmachine
learning,pages864–872.PMLR.
Chung,J.,Kastner,K.,Dinh,L.,Goel,K.,Courville,A.C.,andBengio,Y.(2015). A
recurrentlatentvariablemodelforsequentialdata. Advancesinneuralinformation
processingsystems,28.
Chung,Y.-A.,Hsu,W.-N.,Tang,H.,andGlass,J.(2019).Anunsupervisedautoregres-
sivemodelforspeechrepresentationlearning. arXivpreprintarXiv:1904.03240.
de Bézenac, E., Rangapuram, S. S., Benidis, K., Bohlke-Schneider, M., Kurle, R.,
Stella, L., Hasson, H., Gallinari, P., and Januschowski, T. (2020). Normalizing
kalmanfiltersformultivariatetimeseriesanalysis. AdvancesinNeuralInformation
ProcessingSystems,33:2995–3007.
Dhariwal,P.andNichol,A.(2021). Diffusionmodelsbeatgansonimagesynthesis.
Advancesinneuralinformationprocessingsystems,34:8780–8794.
Dieng,A.B.,Ruiz,F.J.,andBlei,D.M.(2019). Thedynamicembeddedtopicmodel.
arXivpreprintarXiv:1907.05545.
Dieng,A.B.,Wang,C.,Gao,J.,andPaisley,J.(2016). Topicrnn: Arecurrentneural
networkwithlong-rangesemanticdependency. arXivpreprintarXiv:1611.01702.
Donner, T. H., Siegel, M., Fries, P., and Engel, A. K. (2009). Buildup of choice-
predictive activity in human motor cortex during perceptual decision making.
CurrentBiology,19(18):1581–1585.
Du, N., Dai, H., Trivedi, R., Upadhyay, U., Gomez-Rodriguez, M., and Song, L.
(2016). Recurrentmarkedtemporalpointprocesses: Embeddingeventhistory
to vector. In Proceedings of the 22nd ACM SIGKDD international conference on
knowledgediscoveryanddatamining,pages1555–1564.
17Dutordoir,V.,Saul,A.,Ghahramani,Z.,andSimpson,F.(2022). Neuraldiffusion
processes. arXivpreprintarXiv:2206.03992.
Finlay,C.,Jacobsen,J.-H.,Nurbekyan,L.,andOberman,A.(2020). Howtotrain
yourneuralode: theworldofjacobianandkineticregularization. InInternational
conferenceonmachinelearning,pages3154–3164.PMLR.
Fortin, V., Abaza, M., Anctil, F., and Turcotte, R. (2014). Why should ensemble
spread match the rmse of the ensemble mean? Journal of Hydrometeorology,
15(4):1708–1713.
Fraccaro,M.,Sønderby,S.K.,Paquet,U.,andWinther,O.(2016). Sequentialneural
modelswithstochasticlayers. Advancesinneuralinformationprocessingsystems,
29.
Gal,Y.andGhahramani,Z.(2016). Dropoutasabayesianapproximation: Repre-
sentingmodeluncertaintyindeeplearning. Ininternationalconferenceonmachine
learning,pages1050–1059.PMLR.
Glaser, J. I., Benjamin, A. S., Chowdhury, R. H., Perich, M. G., Miller, L. E., and
Kording,K.P.(2020). Machinelearningforneuraldecoding. Eneuro,7(4).
Glaser, J. I., Perich, M. G., Ramkumar, P., Miller, L. E., and Kording, K. P. (2018).
Population coding of conditional probability distributions in dorsal premotor
cortex. Naturecommunications,9(1):1788.
Gneiting, T. and Katzfuss, M. (2014). Probabilistic forecasting. Annual Review of
StatisticsandItsApplication,1:125–151.
Gregor, K., Danihelka, I., Mnih, A., Blundell, C., and Wierstra, D. (2014). Deep
autoregressivenetworks. InInternationalConferenceonMachineLearning,pages
1242–1250.PMLR.
Ho, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic models.
Advancesinneuralinformationprocessingsystems,33:6840–6851.
Hopkins,M.andFurber,S.(2015). Accuracyandefficiencyinfixed-pointneural
odesolvers. Neuralcomputation,27(10):2148–2182.
Huang, B., Liu, C., Banzon, V., Freeman, E., Graham, G., Hankins, B., Smith, T.,
andZhang,H.-M.(2021). Improvementsofthedailyoptimuminterpolationsea
surfacetemperature(doisst)version2.1. JournalofClimate,34(8):2923–2939.
Kingma,D.P.,Welling,M.,etal.(2019).Anintroductiontovariationalautoencoders.
FoundationsandTrends®inMachineLearning,12(4):307–392.
Kobyzev, I., Prince, S. J., and Brubaker, M. A. (2020). Normalizing flows: An
introductionandreviewofcurrentmethods. IEEEtransactionsonpatternanalysis
andmachineintelligence,43(11):3964–3979.
Kontolati,K.,Goswami,S.,Shields,M.D.,andKarniadakis,G.E.(2023). Onthe
influenceofover-parameterizationinmanifoldbasedsurrogatesanddeepneural
operators. JournalofComputationalPhysics,479:112008.
18Kovachki,N.,Li,Z.,Liu,B.,Azizzadenesheli,K.,Bhattacharya,K.,Stuart,A.,and
Anandkumar,A.(2023).Neuraloperator: Learningmapsbetweenfunctionspaces
withapplicationstopdes. JournalofMachineLearningResearch,24(89):1–97.
Li, A., Ding, Z., Dieng, A. B., and Beeson, R. (2024). Efficient and guaranteed-
safenon-convextrajectoryoptimizationwithconstraineddiffusionmodel. arXiv
preprintarXiv:2403.05571.
Li,X.L.,Thickstun,J.,Gulrajani,I.,Liang,P.,andHashimoto,T.B.(2022a).Diffusion-
lmimprovescontrollabletextgeneration. arXivpreprintarXiv:2205.14217.
Li, Y., Lu, X., Wang, Y., and Dou, D. (2022b). Generative time series forecasting
with diffusion, denoise, and disentanglement. Advances in Neural Information
ProcessingSystems,35:23009–23022.
Li,Z.,Zheng,H.,Kovachki,N.,Jin,D.,Chen,H.,Liu,B.,Azizzadenesheli,K.,and
Anandkumar,A.(2021). Physics-informedneuraloperatorforlearningpartial
differentialequations. ACM/JMSJournalofDataScience.
Lim, H., Kim, M., Park, S., and Park, N. (2023). Regular time-series generation
usingsgm. arXivpreprintarXiv:2301.08518.
Lin,B.,Bouneffouf,D.,andCecchi,G.(2022). Predictinghumandecisionmaking
inpsychologicaltaskswithrecurrentneuralnetworks. PloSone,17(5):e0267907.
Lin, L., Li, Z., Li, R., Li, X., and Gao, J. (2023). Diffusion models for time series
applications: Asurvey. arXivpreprintarXiv:2305.00624.
Lorenz,E.N.(1963). Deterministicnonperiodicflow. Journalofatmosphericsciences,
20(2):130–141.
Matheson,J.E.andWinkler,R.L.(1976). Scoringrulesforcontinuousprobability
distributions. Managementscience,22(10):1087–1096.
McLean,D.(2012). Continuumfluidmechanicsandthenavier-stokesequations.
UnderstandingAerodynamics: ArguingfromtheRealPhysics,pages13–78.
Neklyudov, K., Severo, D., and Makhzani, A. (2022). Action matching: A vari-
ational method for learning stochastic dynamics from samples. arXiv preprint
arXiv:2210.06662.
Papamakarios, G., Nalisnick, E., Rezende, D. J., Mohamed, S., and Lakshmi-
narayanan,B.(2021). Normalizingflowsforprobabilisticmodelingandinference.
JournalofMachineLearningResearch,22(57):1–64.
Pathak,J.,Subramanian,S.,Harrington,P.,Raja,S.,Chattopadhyay,A.,Mardani,
M.,Kurth,T.,Hall,D.,Li,Z.,Azizzadenesheli,K.,etal.(2022). Fourcastnet: A
globaldata-drivenhigh-resolutionweathermodelusingadaptivefourierneural
operators. arXivpreprintarXiv:2202.11214.
Rasul,K.,Seward,C.,Schuster,I.,andVollgraf,R.(2021). Autoregressivedenois-
ing diffusion models for multivariate probabilistic time series forecasting. In
InternationalConferenceonMachineLearning,pages8857–8868.PMLR.
19Rezaei,M.R.,Arai,K.,Frank,L.M.,Eden,U.T.,andYousefi,A.(2021). Real-time
pointprocessfilterformultidimensionaldecodingproblemsusingmixturemodels.
Journalofneurosciencemethods,348:109006.
Rezaei, M. R., Gillespie, A. K., Guidera, J. A., Nazari, B., Sadri, S., Frank, L. M.,
Eden, U. T., and Yousefi, A. (2018). A comparison study of point-process filter
anddeeplearningperformanceinestimatingratpositionusinganensembleof
placecells. In201840thAnnualInternationalConferenceoftheIEEEEngineering
inMedicineandBiologySociety(EMBC),pages4732–4735.IEEE.
Rezaei,M.R.,Jeoung,H.,Gharamani,A.,Saha,U.,Bhat,V.,Popovic,M.R.,Yousefi,
A., Chen, R., and Lankarany, M. (2023). Inferring cognitive state underlying
conflictchoicesinverbalstrooptaskusingheterogeneousinputdiscriminative-
generativedecodermodel. JournalofNeuralEngineering,20(5):056016.
Schrödinger, E. (1926). An undulatory theory of the mechanics of atoms and
molecules. Physicalreview,28(6):1049.
Truccolo,W.,Eden,U.T.,Fellows,M.R.,Donoghue,J.P.,andBrown,E.N.(2005).
Apointprocessframeworkforrelatingneuralspikingactivitytospikinghistory,
neural ensemble, and extrinsic covariate effects. Journal of neurophysiology,
93(2):1074–1089.
Voleti, V., Jolicoeur-Martineau, A., and Pal, C. (2022). Mcvd-masked conditional
videodiffusionforprediction,generation,andinterpolation. AdvancesinNeural
InformationProcessingSystems,35:23371–23385.
Wanner,G.andHairer,E.(1996). SolvingordinarydifferentialequationsII,volume
375. SpringerBerlinHeidelbergNewYork.
Yan,T.,Zhang,H.,Zhou,T.,Zhan,Y.,andXia,Y.(2021). Scoregrad: Multivariate
probabilistic time series forecasting with continuous energy-based generative
models. arXivpreprintarXiv:2106.10121.
A Appendix
Estimating the log-likelihood of a new sequence. Sometimes,scientistsmaybe
interestedinscoringagivensequenceusingamodelfitondatatostudyhowthe
newinputsequencedeviatesfromthedata. Alternatorsprovideawaytodothis
∗
usingthelog-likelihood. Assumegivenanewinputsequence x . Wecanestimate
1:T
20itslikelihoodundertheAlternatorasfollows:
(cid:90)
logpθ,φ(x 1∗ :T)=log pθ,φ(x 1∗ :T,z 0:T) dz
0:T
(18)
(cid:90) T
(cid:89)
=log pθ,φ(z 0:T)·pθ(x 1∗|z 0)) pθ(x t∗|z t−1)) (19)
t=2
(cid:150) T (cid:153)
(cid:88)
=log(cid:69) pθ,φ(z 0:T)exp logpθ(x 1∗|z 0)+ logpθ(x t∗|z t−1) (20)
t=2
K (cid:150) T (cid:153)
≈log K1 (cid:88) exp logpθ(x 1∗|z 0(k))+(cid:88) logpθ(x t∗|z t( −k) 1) , (21)
k=1 t=2
where z 0(1 :T) ,...,z 0(K :T) are K samples from the marginal pθ,φ(z 0:T). Eq. 21 is a se-
quencescoringfunctionanditcanbecomputedinanumericallystablewayusing
thefunctionlogsumexp(·).
21