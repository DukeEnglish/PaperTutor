Asymptotic theory of in-context learning by
linear attention
Yue M. Lu∗a, Mary I. Leteya, Jacob A. Zavatone-Veth†a,b,c, Anindita Maiti†d, and
Cengiz Pehlevan*a,c,e
aThe John A. Paulson School of Engineering and Applied Sciences, Harvard University
bDepartment of Physics, Harvard University
cCenter for Brain Science, Harvard University
dPerimeter Institute for Theoretical Physics
eThe Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University
May 19, 2024
Abstract
Transformers have a remarkable ability to learn and execute tasks based on examples provided
within the input itself, without explicit prior training. It has been argued that this capability,
known as in-context learning (ICL), is a cornerstone of Transformers’ success, yet questions
about the necessary sample complexity, pretraining task diversity, and context length for suc-
cessful ICL remain unresolved. Here, we provide a precise answer to these questions in an
exactly solvable model of ICL of a linear regression task by linear attention. We derive sharp
asymptotics for the learning curve in a phenomenologically-rich scaling regime where the token
dimensionistakentoinfinity;thecontextlengthandpretrainingtaskdiversityscaleproportion-
allywiththetokendimension;andthenumberofpretrainingexamplesscalesquadratically. We
demonstrateadouble-descentlearningcurvewithincreasingpretrainingexamples, anduncover
a phase transition in the model’s behavior between low and high task diversity regimes: In the
lowdiversityregime,themodeltendstowardmemorizationoftrainingtasks,whereasinthehigh
diversity regime, it achieves genuine in-context learning and generalization beyond the scope of
pretrained tasks. These theoretical insights are empirically validated through experiments with
both linear attention and full nonlinear Transformer architectures.
1 Introduction
Since their introduction by Vaswani et al. in 2017 [1], Transformers have become a cornerstone of
modernartificialintelligence(AI).Originallydesignedforsequencemodelingtasks,suchaslanguage
modeling and machine translation, Transformers achieve state-of-the art performance across many
domains, even those that are not inherently sequential [2]. Most strikingly, they underpin the
breakthroughs achieved by large language models such as BERT [3], LLaMA [4], and the GPT
series [5–8].
∗To whom correspondence should be addressed. E-mail: (Y.M.L.) yuelu@seas.harvard.edu and (C.P.) cpehle-
van@seas.harvard.edu
†J.A.Z-V.(Author Three) contributed equally to this work with A.M. (Author Four)
1
4202
yaM
02
]LM.tats[
1v15711.5042:viXraThe technological advancements enabled by Transformers have inspired a substantial body of
research aimed at understanding their working principles. One key observation is that language
models gain new behaviors and skills as their number of parameters and the size of their training
datasets grow [7, 9–11]. A particularly important emergent skill is in-context learning (ICL), which
describes the model’s ability to learn and execute tasks based on the context provided within the
input itself, without the need for explicit prior training on those specific tasks. To give an example
from natural language processing, a pretrained large language model might be able to successfully
translate English to Italian after being prompted with a few example translations, even if it has not
been specifically pretrained on that translation task [7]. ICL enables language models to perform
new, specialized tasks without retraining, which is arguably a key reason for their general-purpose
abilities.
Despite many recent studies on understanding ICL, important questions about how and when
ICL emerges in large language models are still mostly open. Large language models are trained (or
pretrained) with a next token prediction objective. How do the different algorithmic and hyperpa-
rameterchoicesthatgointothepretrainingprocedureaffectICLperformance? Whatalgorithmsdo
Transformers implement for ICL? How many pretraining examples are required for ICL to emerge?
How many examples should be provided within the input for the model to be able to solve an
in-context task? How diverse should the tasks in the training dataset be for in-context learning of
truly new tasks not observed in the training dataset?
In this paper, we address these questions by investigating the ICL capabilities of a linear atten-
tion module for linear regression tasks. This model setting allows us to derive an asymptotically
precise theory of the learning curve. In the remainder of this section, we first provide a comprehen-
sive overview of related works on ICL. Following this, we summarize our main contributions.
1.1 Related Works
ICL in Transformer architectures. The striking ICL abilities of Transformers were thrust to
the fore by Brown et al. [7]’s work on GPT-3. Focusing on natural language processing (NLP)
tasks, they showed that ICL performance dramatically improves with an increase in the number of
model parameters, with an increase in the number of examples in the model’s context, and with the
addition of a natural language task description. In subsequent work, Wei et al. [11] proposed that
the emergence of ICL with increasing scale is an abrupt, unpredictable transition. This perspective
has substantially influenced proposed accounts for the emergence of ICL [12]. However, Schaeffer
et al. [13] have disputed the idea that the emergence of ICL is unpredictable; they suggest that
appropriately-chosen measures of otherwise hidden progress [14] reveal that ICL gradually develops
with scale.
Empirical studies of synthetic ICL tasks. Though ICL in NLP is both impressive and useful,
these natural data do not allow precise experimental control. Towards a fine-grained understand-
ing of the conditions required for ICL, many recent works have explored ICL of parametrically-
controllable synthetic tasks, notably linear regression and classification. These works have iden-
tified various features of pretraining data distributions that contribute to the emergence of ICL
[15–19]. Closely related to our work is a study of ICL of linear regression by Raventós et al. [18].
Their work identified a task diversity threshold for the emergence of ICL, below which a pretrained
TransformerbehavesasaBayesianestimatorwithpriordeterminedbythelimitedsetofpretraining
tasks. Above this threshold, the model’s performance matches that of within-context ridge regres-
sion, corresponding to a Gaussian prior over all tasks, including those not seen during pretraining.
This work underscores the roles of task diversity, regularization, model capacity, and data structure
2in the emergence of ICL; a motivating objective of our work is to provide a theoretical account of
their results.
Theoretical studies of ICL. Many theoretical studies of ICL have centered on the idea that
Transformerslearnaparticularalgorithmduringpretraining,whichisthenflexiblydeployedtosolve
in-context tasks. In broad strokes, papers from this program of research often consider a particular
algorithm for solving an in-context task, prove that Transformers can approximately implement
this algorithm, and then empirically compare the ICL performance of a pre-trained Transformer to
the performance of that algorithm [20–27]. A clear consensus on which algorithm underlies ICL of
linear regression in full transformers has yet to emerge [20–27]. Within this line of research, closest
to our work are a series of papers that consider ICL of linear regression by simplified Transformers
using linear, rather than softmax, attention modules [23, 25–30]. Zhang et al. [27] study these
models in the limit of infinite pretraining dataset size (i.e., the population risk limit), and show
that their performance on in-context linear regression nearly matches that of the Bayes-optimal
estimator for the ICL task. However, they found that linear Transformers are not robust to shifts in
the within-context covariate distribution. Zhang et al. [26] then showed that any optimizer of the
within-context risk for a linear Transformer solves the ICL task with an approximation to one step
of gradient descent from a learnable initialization, and that the resulting estimator can saturate the
Bayes error for tasks with a Gaussian prior and non-zero mean. As we will discuss in Section 2,
our reduction of the linear attention module is inspired in part by these works. In very recent
work, Duraisamy [30] has studied the fine-sample risk of in-context linear regression with a single
step of gradient descent, without directly analyzing Transformers. Ahn et al. [23] and Wu et al.
[28] investigated how linear Transformers adapt to limited pretraining data and context length,
again showing that in certain cases nearly-optimal error is achievable. Like these studies, our work
considers linear attention, but our analysis, with its asymptotically sharp predictions of the ICL
performance, allows us to pinpoint when and how the transition from memorization to ICL of linear
regression occurs. We thus in closing highlight work by Reddy [19] on in-context classification, who
analyzed the transition to ICL using a phenomenological model.
1.2 Summary of contributions
We now summarize the primary contributions of our paper, relative to the prior art reviewed above.
Following the recent literature, we focus on a simplified model of a Transformer that captures
its key architectural motif: the linear self-attention module [23, 25–29]. Linear attention includes
the quadratic interaction between inputs that lies at the heart of softmax attention, but does not
include the normalization steps or fully-connected layers. This simplification makes the model
more amenable for theoretical analysis. Our main result is a sharp asymptotic analysis of ICL of
linear regression by linear attention, resulting in a more precisely predictive theory than previous
population risk analyses or finite-sample bounds [26, 27]. The main contributions of our paper are
structured as follows:
1. We begin in §2 by developing a simplified parameterization of linear self-attention that allows
pretraining on the ICL linear regression task to be performed using ridge regression.
2. Within this simplified model, it is easy to identify the non-trivial scaling limit in which per-
formance should be analyzed (§3): as the token dimension tends to infinity, the number of
pretraining examples should scale quadratically (with the token dimension), while the context
length and pretraining task diversity should scale linearly. In this joint limit, we can compute
sharp asymptotics for ICL performance using random matrix theory.
33. Our theoretical results reveal several interesting phenomena (§3). First, we observe double-
descent in the model’s ICL generalization performance as a function of pretraining dataset size,
reflecting our assumption that it is pretrained to interpolation. Second, we uncover a transition
to in-context learning as the pretraining task diversity increases. Concretely, there is a threshold
task diversity above which linear attention saturates the Bayes error for the ICL regression task.
Below that threshold, the model tends to memorize the limited set of pretraining tasks, and its
excess risk is substantial. This transition recapitulates the empirical findings of Raventós et al.
[18] in full Transformer models.
4. In §4, we show through numerical experiments that the insights from our theory derived on a
simplified model transfer to full Transformer models with softmax self-attention. In particular,
the scaling of pretraining sample complexity and task diversity with token dimension required
for successful ICL is consistent.
More broadly, the study of solvable models is crucial for enhancing our grasp of how machine
learning algorithms learn and generalize across various tasks. Understanding the mechanistic un-
derpinnings of ICL of well-controlled synthetic tasks is an important prerequisite to understanding
how it emerges from pretraining on natural data [19].
2 Problem formulation
We start by describing the setting of our study.
2.1 ICL of linear regression
In an ICL task, the model takes as input a sequence of tokens {x ,y ,x ,y ,...,x ,y ,x }, and
1 1 2 2 ℓ ℓ ℓ+1
outputs a prediction of y . We will often refer to an input sequence as a “context.” The pairs
ℓ+1
{x ,y }ℓ+1 are i.i.d. samples from a context-dependent joint distribution P(x,y). Hence, the model
i i i=1
needs to gather information about P(x,y) from the first ℓ examples and use this information to
predict y from x . We will refer to ℓ as the “context length”.
ℓ+1 ℓ+1
In this work, we focus on an approximately linear mapping between x ∈ Rd and y ∈ R:
i i
y = ⟨x ,w⟩+ϵ , (1)
i i i
where ϵ is a Gaussian noise with mean zero and variance ρ, and w ∈ Rd is referred to as a task
i
vector. We note that the task vector w is fixed within a context, but can change between different
contexts. The model has to learn w from the ℓ pairs presented within the context, and use it to
predict y from x .
ℓ+1 ℓ+1
2.2 Linear self-attention
Themodelthatwewillanalyticallystudyisthelinearself-attentionblock[31]. Linearself-attention
takes as input an embedding matrix Z, whose columns hold the sequence tokens. The mapping of
sequences to matrices is not unique. Here, following the convention in [27, 28, 31], we will embed
the input sequence {x ,y ,x ,y ,...,x ,y ,x } as:
1 1 2 2 ℓ ℓ ℓ+1
(cid:34) (cid:35)
x x ... x x
Z = 1 2 ℓ ℓ+1 ∈ R(d+1)×(ℓ+1), (2)
y y ... y 0
1 2 ℓ
4where 0 in the lower-right corner is a token that prompts the missing value y to be predicted.
ℓ+1
For appropriately sized key, query, and value matrices K,Q,V, the output of a linear-attention
block [31–33] is given by
1
A := Z + VZ(KZ)⊤(QZ).
ℓ
TheoutputAisamatrixwhileourgoalistopredictascalar,y . Followingthechoiceofpositional
ℓ+1
encoding in eq. (2), we will take A , the element of A corresponding to the 0 prompt, as the
d+1,ℓ+1
prediction for y :
ℓ+1
yˆ:= A . (3)
d+1,ℓ+1
2.3 Pretraining data
The model is pretrained on n sample sequences, where the µth sample is a collection of ℓ + 1
vector-scalar pairs {xµ ∈ Rd,yµ ∈ R}ℓ+1 related by the approximate linear mapping in eq. (1):
i i i=1
yµ = ⟨xµ,wµ⟩+ϵµ. Here, wµ denotes the task vector associated with the µth sample.
i i i
We make the following statistical assumptions:
1. xµ are d-dimensional random vectors, sampled i.i.d. over both i and µ from a Gaussian distri-
i
bution N(0,I /d).
d
2. For 1 ≤ µ ≤ n, the task vector wµ associated with the µth sample context is uniformly sam-
pled from a finite set with k elements, denoted by {w ,...,w }. The elements of this set are
1 k
independently drawn once at the beginning of training from
√
w ∼ Unif(Sd−1( d)), (4)
i i.i.d.
√ √ √
where Unif(Sd−1( d)) denotes the uniform distribution on the sphere Sd−1( d) of radius d.
The variable k controls the task diversity in the pretraining data set. Importantly, k can be less
than n, in which case the same task vector may be repeated multiple times.
3. The noise terms ϵµ are i.i.d. over both i and µ, and drawn from a normal distribution N(0,ρ).
i
We denote a sample from this distribution by (Z,y ) ∼ P .
ℓ+1 train
2.4 Parameter reduction
Before specifying a training procedure, it is insightful to examine the prediction mechanism of
the linear attention module for the ICL task. This turns out to be a fruitful exercise, shedding
light on critical questions: Can linear self-attention learn linear regression in-context? If so, what
informationdomodelparameterslearnfromdatainsolvingthisICLproblem? Bycloselyexamining
these aspects, we can also formulate a simplified problem that lends itself to analytical study.
We start by rewriting the output of the linear attention module, eq. (3), in an alternative form.
Following Zhang et al. [27], we define
(cid:34) (cid:35) (cid:34) (cid:35)
V v M m
V = 11 12 , M = 11 12 := K⊤Q, (5)
v⊤ v m⊤ m
21 22 21 22
5where V ∈ Rd×d, v ,v ∈ Rd×1, v ∈ R, M ∈ Rd×d, m ,m ∈ Rd×1, and m ∈ R. The
11 12 21 22 11 12 21 22
expression we desire is
(cid:42) ℓ ℓ ℓ+1 ℓ (cid:43)
1 (cid:88) (cid:88) (cid:88) (cid:88)
yˆ= x ,v M⊤ y x +v m y2+M⊤ x x⊤v +m y x⊤v ,
ℓ ℓ+1 22 11 i i 22 21 i 11 i i 21 21 i i 21
i=1 i=1 i=1 i=1
where ⟨·,·⟩ stands for the inner product.
This expression reveals several interesting points. First, not all parameters in (5) contribute to
the output: We can ignore all the parameters except the last row of V and the first d columns of
M. Second, the first term
ℓ
1 (cid:88)
v M⊤ y x
ℓ 22 11 i i
i=1
offers a hint about how the linear attention module might be solving the task. The sum 1 (cid:80) y x
ℓ i≤ℓ i i
is a noisy estimate of E[xx⊤]w for that context. Hence, if the parameters of the model are such
that v M⊤ is approximately E[xx⊤]−1, this term alone makes a good prediction for the output.
22 11
Third, the third term does not depend on outputs y, and thus does not directly contribute to the
ICL task that relies on the relationship between x and y. Fourth, the last term only considers a
one dimensional projection of x onto v . Because the task vectors w and x are isotropic in the
21
statistical models that we consider, there are no special directions in the problem. Consequently,
we expect the optimal v to be approximately zero by symmetry considerations.
21
Motivated by these observations, and for analytical tractability, we study the linear attention
module with the constraint v = 0. In this case, collecting the remaining parameters in a matrix
21
(cid:104) (cid:105)
Γ := v M⊤/d m ∈ Rd×(d+1) (6)
22 11 21
and the input sequence in another matrix H , defined as
Z
(cid:104) (cid:105)
H := x d (cid:80) y x⊤ 1 (cid:80) y2 ∈ Rd×(d+1), (7)
Z ℓ+1 ℓ i≤ℓ i i ℓ i≤ℓ i
we can rewrite the predicted label as
yˆ= ⟨Γ,H ⟩. (8)
Z
The 1/d scaling of M in Γ is chosen so that the columns of H scale similarly; it does not affect
11 Z
the final predictor yˆ.
We note that Zhang et al. [27] provide an analysis of population risk (whereas we focus on
empirical risk) for a related reduced model in which they set v = 0 and m = 0. Consequently,
21 21
the predictors they study differ from ours (8) by an additive term. They justify this choice through
an optimization argument: if these parameters are initialized to zero, they remain zero under
gradient descent optimization of the population risk, given certain conditions.
In the remainder of this paper, we will examine the ICL performance of the reduced model
given in (7) and (8), except when making comparisons to a full, nonlinear Transformer architecture.
Henceforth, unless explicitly stated otherwise, we will refer to this reduced model as the linear
attention module.
2.5 Model pretraining
The parameters of the linear attention module are learned from n samples of input sequences,
{xµ,yµ,...,xµ ,yµ }, µ = 1,...,n.
1 1 ℓ+1 ℓ+1
6We estimate model parameters using ridge regression, giving
n
Γ∗ = arg min
(cid:88)(cid:16)
yµ −⟨Γ,H
⟩(cid:17)2
+
n
λ∥Γ∥2 , (9)
ℓ+1 Zµ d F
Γ
µ=1
where λ > 0 is a regularization parameter, and H refers to the input matrix (7) populated with
Zµ
the µth sample sequence. The factor n/d in front of λ makes sure that, when we take the d → ∞ or
n → ∞limits later, there isstill a meaningful ridgeregularization. Thesolution tothe optimization
problem in (9) can be expressed explicitly as
 −1
n n
vec(Γ∗) = n dλI +(cid:88) vec(H Zµ)vec(H Zµ)⊤  (cid:88) y ℓµ +1vec(H Zµ),
µ=1 µ=1
where vec(·) denotes the vectorization operation. Throughout this paper, we adopt the row-major
convention. Thus, for a d 1×d
2
matrix A, vec(A) is a vector in Rd1d2, formed by stacking the rows
of A together.
2.6 Evaluation
For a given set of parameters Γ, the model’s generalization error is defined as
(cid:104) (cid:105)
e(Γ) = E (cid:0) y −⟨Γ,H ⟩(cid:1)2 ,
Ptest ℓ+1 Z
where (Z,y ) ∼ P is a new sample drawn from the distribution of the test data set. We
ℓ+1 test
consider two different test data distributions P :
test
1. ICL task: x and ϵ are i.i.d. Gaussians as in the pretraining case. However, for each 1 ≤
i i
µ ≤ n, the task vector wµ associated with the µth input sequence is drawn independently from
√
Unif(Sd−1( d)). We will denote the test error under this setting by eICL(Γ).
2. In-distribution generalization (IDG) task: The test data are generated in exactly the same man-
ner as the training data, i.e., P = P , hence the term in-distribution generalization. In
test train
particular, the set of unique task vectors {w ,...,w } is identical to that used in the pretraining
1 k
data. We will denote the test error under this setting by eIDG(Γ).
The ICL task evaluates the true in-context learning performance of the linear attention module.
The task vectors in the test set differ from those seen in training, requiring the model to infer them
from context. The IDG task assesses the model’s performance on task vectors encountered during
pretraining. High performance on the IDG task but low performance on the ICL task indicates
that the model memorizes the training task vectors. Conversely, high performance on the ICL task
indicates that the model can learn task vectors from the provided context.
TounderstandtheperformanceofourmodelonbothICLandIDGtasks,wewillneedtoevaluate
these expressions for the pretrained attention matrix Γ∗. An asymptotically precise prediction of
eICL(Γ∗) and eIDG(Γ∗) will be a main result of this work.
2.7 Bayes optimal estimators
Following Raventós et al. [18], it is useful to compare the predictions made by the trained linear
attention to optimal estimators that use only the current context information. These estimators do
7not rely on data outside of the given context for their predictions. Under the mean square loss, the
optimal Bayesian estimator yˆ = E [y |x ,y ,x ,y ,...,x ,y ,x ] in our setting has the
Bayes Ptest ℓ+1 1 1 2 2 ℓ ℓ ℓ+1
form
yˆ = (wBayes)⊤x ,
Bayes ℓ+1
where wBayes is the Bayes estimator of the task vector w.
For the ICL task, the Bayes-optimal ridge regression estimator is given by
 −1 
ℓ ℓ
w rB iday ge es := (cid:88) x ix⊤ i +ρI d (cid:88) y ix i,
i=1 i=1
where the ridge is set to the noise variance ρ. We will refer to it as the ridge estimator. For the
IDG task, the Bayes-optimal estimator is given by
(cid:80)k
w
e− 21 ρ(cid:80)ℓ i=1(cid:16) yi−w j⊤xi(cid:17)2
wBayes := j=1 j . (10)
dMMSE (cid:80)k e− 21 ρ(cid:80)ℓ i=1(cid:16) yi−w j⊤xi(cid:17)2
j=1
Here, we assume that the training task vectors {w ,...,w } are known to the estimator. Following
1 k
[18],wewillrefertothisestimatorasthediscrete minimummean squared error (dMMSE)estimator.
The test performance of these estimators are calculated by
(cid:20) (cid:21)
(cid:16) (cid:17)2
eBayes = E y −(wBayes)⊤x ,
Ptest ℓ+1 ℓ+1
where P can be the ICL or IDG task, and wBayes can be the ridge or the dMMSE estimator. To
test
avoid possible confusion, we emphasize that we will sometimes plot the performance of an estimator
on a task for which it is not optimal. For example, we will test the dMMSE estimator, which is
Bayes-optimal for the pretraining distribution, on the ICL task, where it is not optimal. This will
be done for benchmarking purposes.
3 Theoretical results
To answer the questions raised in the introduction, we provide a precise asymptotic analysis of the
learning curves of the linear attention module for ICL of linear regression. We then verify through
simulations that the primary insights gained from our theoretical analysis extend to more realistic
nonlinear Transformers.
3.1 Joint asymptotic limit
We have now defined both the structure of the training data as well as the parameters to be
optimized. For our theoretical analysis, we consider a joint asymptotic limit in which the input
dimension d, the pretraining dataset size n, the context length ℓ, and number of task vectors in the
training set k, go to infinity together such that
ℓ k n
:= α = Θ(1), := κ = Θ(1), := τ = Θ(1). (11)
d d d2
8Identification of these scalings constitutes one of the main results of our paper. As we will see, the
linear attention module exhibits rich learning phenomena in this limit.
The intuition for these scaling parameters can be seen as follows. Standard results in linear
regression [34–36] show that to estimate a d-dimensional task vector w from the ℓ samples within
a context, one needs at least ℓ = Θ(d). The number of unique task vectors that must be seen to
estimate the true d-dimensional Gaussian task distribution should also scale with d, i.e. k = Θ(d).
Finally, we see from (6) that the number of linear attention parameters to be learned is Θ(d2).
This suggests that the number of individual contexts the model sees during pretraining should scale
similarly, i.e., n = Θ(d2).
3.2 Learning curves for ICL of linear regression by a linear attention module
Our theoretical analysis, explained in detail in the Supplementary Information, leads to asymptot-
ically precise expressions for the generalization error under the two test distributions under study.
Specifically, our theory predicts that, as d,n,ℓ,k → ∞ in the joint limit given in (11),
eICL(Γ∗) −→ eICL(τ,α,κ,ρ,λ) almost surely,
and
eIDG(Γ∗) −→ eIDG(τ,α,κ,ρ,λ) almost surely,
where eICL(τ,α,κ,ρ,λ) and eIDG(τ,α,κ,ρ,λ) are two deterministic functions of the parameters τ,
α, κ, ρ and λ. The exact expressions of these two functions can be found in SI.5.2 and SI.5.3,
respectively. For simplicity, we only present in what follows the ridgeless limit (i.e., λ → 0+) of the
asymptotic generalization errors.
Result 1 (ICL generalization error in the ridgeless limit). Let
q∗ := 1+ρ , m∗ := M (cid:0) q∗(cid:1) , and µ∗ := q∗M (q∗), (12)
α κ κ/τ
where M (·), defined in (B.3), is a function related to the Stieltjes transform of the Marchenko-
κ
Pastur law. Then
eICL := lim eICL(τ,α,κ,ρ,λ)
ridgeless
λ→0+

τ(1+q∗) (cid:2) 1−τ(1−µ∗)2+µ∗(ρ/q∗−1)(cid:3) −2τ(1−µ∗)+(1+ρ) τ < 1
1−τ
= (cid:16) (cid:17) ,
(q∗+1) 1−2q∗m∗−(q∗)2M′ (q∗)+ (ρ+q∗−(q∗)2m∗)m∗ −2(1−q∗m∗)+(1+ρ) τ > 1
 κ τ−1
where M′ (·) denotes the derivative of M (q) with respect to q.
κ κ
Result 2 (IDG generalization error in the ridgeless limit). Let q∗, m∗, and µ∗ be the scalars defined
in (12). We have
eIDG := lim eIDG(τ,α,κ,ρ,λ)
ridgeless
λ→0+
 (cid:16) (cid:17)
 τ ρ+q∗−2q∗(1−τ)(q∗/ξ∗+1) + τµ∗(q∗+ξ∗)2 τ < 1
= 1−τ 1−p∗(1−τ) q∗ ,
 τ [ρ+q∗(1−q∗m∗)] τ > 1
τ−1
where ξ∗ = (1−τ)q∗ and p∗ = (cid:0) 1−κ(cid:0)κξ∗ +1(cid:1)−2(cid:1)−1 .
τµ∗ 1−τ
9Wederivetheseresultsusingtechniquesfromrandommatrixtheory. Thefullsetupandtechnical
details are presented in the Supplementary Information in SI.1 through SI.5. A key technical
component of our analysis involves characterizing the spectral properties of the sample covariance
matrix of n = Θ(d2) i.i.d. random vectors in dimension Θ(d2). Each of these vectors is constructed
as the vectorized version of the matrix in (7). Related but simpler versions of this type of random
matrices involving the tensor product of i.i.d. random vectors have been studied in recent work
[37]. Some of our derivations are based on non-rigorous yet technically plausible heuristics. We
support these predictions with numerical simulations and discuss in the Supplementary Information
the steps required to achieve a fully rigorous proof.
Before discussing the implications of our theoretical results, we first note that if we take the
limit of κ → ∞ and α → ∞ in Result 1 (in either order), the ICL generalization error reduces to
the generalization error of simple ridgeless interpolation with isotropic Gaussian covariates in d2
dimensions [36, 38]:
 ρ
1−τ + τ < 1,
 1−τ
lim lim eICL = lim lim eICL =
ridgeless ridgeless ρτ
α→∞κ→∞ κ→∞α→∞  τ > 1.

τ −1
This limiting result makes sense, given that in this limit the ICL generalization problem reduces to
the generalization error of ridge regression in d2 dimensions with covariates formed as the tensor
product of i.i.d. Gaussian vectors, which by universality results in [37] should in turn be asymptot-
ically equal to that for isotropic Gaussian covariates [36].
3.3 Sample-wise double-descent
How large should n, the pretraining dataset size, be for the linear attention to succesfully learn
the ICL and IDG tasks? In Figure 1, we plot our theoretical predictions for the ICL and IDG
generalization error as a function of τ = n/d2 and verify them with numerical simulations. Our
results demonstrate that the quadratic scaling of sample size with input dimensions is indeed the
appropriate regime for nontrivial learning phenomena to occur.
As apparent in Figure 1, we find that the generalization error for both ICL and IDG tasks are
not monotonic in the number of samples. In the ridgeless limit, both ICL and IDG errors diverge
at τ = 1, with the leading order behavior in the τ ↑ 1 (respectively τ ↓ 1) limit given by c1
1−τ
(respectively c2 ), where c (respectively c ) is a τ-independent constant. This leads to a “double-
τ−1 1 2
descent” behavior [36, 39] in the number of samples. As in other models exhibiting double-descent
[36,38,39],thelocationofthedivergenceisattheinterpolationthreshold: thenumberofparameters
of the model (elements of Γ) is, to leading order in d, equal to d2, which matches the number of
pretraining samples at τ = 1. Further, we can investigate the effect of ridge regularisation on the
steepness of the double descent, as illustrated in Figure 1c for the ICL task. As we would expect
from other models exhibiting double-descent [36, 38, 39], increasing the regularization strength
suppresses the peak in error around the interpolation threshold.
3.4 The ICL error can have non-monotonic dependence on context length
How large should the context length be? In Figure 2, we plot our theoretical results verified with
experiments. We observe that we have correctly identified the regime where ICL appears: context
length scales linearly with input dimensions. An interesting observation is that the ICL error does
not always monotonically decrease with context length. There are parameter configurations with
κ < 1 (blue curve in Figure 2a) for which the ICL error is minimal at some finite α.
10=1
2.5 =1
6 =10
=10
=100
=100
2.0
5
4 1.5
3
1.0
2
0.5
1
0.0
0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0
=n/d2 =n/d2
(a) Ridgeless ICL error against τ (b) Ridgeless IDG error against τ
3.0
Numerical simulation
=1
2.5 =0.1
=0
2.0
1.5
1.0
0.5
0.0 0.5 1.0 1.5 2.0 2.5 3.0
=n/d2
(c) Finite ridge IDG error against τ
Figure1: ICLperformanceasafunctionofτ: theory(solidlines)vssimulations(dots). Plotsof(a),
(c)ICL erroreICL(Γ∗)and(b) IDGerroreIDG(Γ∗)vsτ atoptimalΓ∗. Parameters: d = 80,κ = 0.5,
and ρ = 0.01. Averages and standard deviations are computed over 10 runs.
3.5 Memorization to ICL transition with increasing pretraining task diversity
Recall that the parameter κ = k/d controls the diversity of the training task vectors. How large
should it be for ICL to emerge? Our theory corroborates a phenomenon that was empirically
observed in a recent study [18]. Figure 3 shows a transition in the nature of the predictions that the
linear attention module makes. For low κ, the model’s performance is close to that of the dMMSE
estimator. This indicates that the model inherently assumes the task vector is one of the k vectors
encountered in its pretraining dataset, effectively memorizing these task vectors. As κ increases
beyond 1, the model’s performance approaches that of the ridge estimator. In this regime, the
model generalizes to task vectors beyond its pretraining dataset, behaving as if it has learned the
true prior on the task vectors despite having only seen a finite subset in the pretraining dataset.
11
)*
(LCIe
)*
(LCIe
)*
(GDIe1.2
0.2
Numerical test error Numerical test error
1.0 =0.5 =0.5
=0.75 =0.75
=1 0.1 =1
0.8
=10 =10
0.6 0.05
0.4
0.02
0.2
0.0
5 20 100 500 5 20 100 500
= /d = /d
(a) ICL error against α (b) IDG error against α
Figure 2: ICL performance as a function of α: theory (solid lines) vs simulations (dots). Plots of
(a) ICL error eICL(Γ∗) and (b) IDG error eIDG(Γ∗) vs α at optimal Γ∗. We highlight that, while the
g g
IDG error is monotonic in α, the ICL error for κ = 0.5 and κ = 0.75 is non-monotonic. Parameters:
d = 70,τ = 20,ρ = 0.01, ridgeless. Averages and standard deviations are computed over 10 runs.
1.75
1.50
1.25
1.00 dMMSE
=10
0.75 =20
=200
0.50
0.25
0.00
0.2 1 10 100
=k/d
Figure 3: Comparison of linear Transformer generalization error with the dMMSE estimator given
by eq. (10): theory (solid lines) vs simulations (dots, triangles). Each value plotted is the excess
value of generalisation error over the noise level ρ. Parameters: τ = 0.2α,ρ = 0.01, ridgeless.
Averages and standard deviations for linear model are computed over 10 runs.
To further understand the role of κ in the solution learned by the linear attention mechanism,
consider the regime where τ,α → ∞ with τ/α = c∗ kept fixed. Under this setting, we have
 (cid:16) (cid:17)
ρ+(1−κ) 1+ ρ c∗ κ < 1
lim eICL = 1+ρ .
ατ→ →∞
∞
ridgeless ρ κ > 1
This change in analytical behavior indicates a phase transition at κ = 1. Further, the κ > 1
12
)*
(LCIe
roolf
esion
revo
rorre
ssecxE
)*
(GDIe4.0
Nonlinear model error Experimental interpolation threshold
Interpolation threshold 2 Layers, best fit*
3.5
3 Layers, best fit*
3.0
104
2.5
2.0
1.5
1.0 103
0 20 40 60 80 10 15 20 30
=n/d2 Token dimension d
(a) Nonlinear model exhibits double descent of test (b) Interpolation threshold follows predicted n ∝ d2
ICL error in scaling parameter τ. scaling.
Figure 4: Experimental verification of both scaling definitions and double descent behaviour in n.
Figure 4a: Increasing n will increase error until an interpolation threshold is reached.
Figure 4b which occurs for n proportional to d2, as predicted by the linear theory. Best fit lines (∗)
correspond to fitting log(n) = alog(d)+b giving a = 1.82,b = 3.55 for 2-layer model and a =
2 2 3
2.22,b = 2.81for3-layermodel. Interpolationthresholdwascomputedempiricallybysearchingfor
3
location in τ of sharp increase in value and variance of training error at a fixed number of gradient
steps. Parameters: d = 10,α = 5,κ = ∞,ρ = 0.252. For fig. 4a: 2-layer architecture; variance
showncomesfrommodeltrainedoverdifferentsamplesofpretrainingdata; linesshowaveragesover
10 runs and shaded region shows standard deviation.
branch approaches ρ, the error of the Bayes-optimal ridge estimator in this limit. The smooth
memorization-ICL transition observed in Figure 3 for the finite α,τ case stems from this phase
transition.
4 Experiments with full, nonlinear Transformers
As our theoretical results are derived in a simplified setting, we aim to test if these insights are
applicable to a full, nonlinear Transformer model. Specifically, we will investigate: (1) whether
we have identified the correct scaling regime for non-trivial learning in an ICL task; (2) if the full
Transformer exhibits a sample-wise double descent, and whether the location of the peak error
scales quadratically with input dimensions as predicted by our theory; and (3) if the transition from
memorization to generalization occurs, with the transition point being around κ = 1.
Our experiments1 are done with a standard Transformer architecture consisting of blocks with:
(1) a single-head softmax self-attention with K,Q,V ∈ Rhd×ℓ(d+1) matrices2, followed by (2) a
two-layer dense MLP with GELU activation and hidden layer of size 10d [1]. Residual connections
are used between the input vector, the pre-MLP output, and the MLP output. Each sample takes
the form given by eq. (2). We use either two or three Transformer blocks before returning the final
1Code to reproduce all experiments is available at https://github.com/Pehlevan-Group/icl-asymptotic.
2Providedthatthehiddenlayerdimensionhdisgreaterthand,itdoesnotaffecttheexpressivityoftheattention
mechanism. In our experiments, we use h=10.
13
LCIe
rorrE
tseT
n
selpmas
fo
rebmuN2.00 dMMSE
Nonlinear model (2 layers)
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.2 1 10 50
=k/d
Figure5: Experimentin2-layernonlinearnetworkdemonstratesasharptransitionbetweendMMSE
estimator and ridge estimator, familiar from the linear theory. Parameters: d = 20,τ = 50,α =
5,ρ = 0.01. Variance shown comes from model trained over different samples of pretraining data;
lines show averages over 10 runs and shaded region shows standard deviation.
logit corresponding to the (d+1,ℓ+1)th element in the embedding. The loss function is the mean
squarederror(MSE)betweenthepredictedlabel(theoutputofthemodelforagivensampleZ)and
the true yˆ value. We train the model in an offline setting with n samples Z ,··· ,Z , using the
ℓ+1 1 n
Adam optimizer [40] with a learning rate 10−4 until the training error converges, typically requiring
300000-500000gradientsteps. Thestructureofthepretrainingandtestdistributionsexactlyfollows
the setup described in Section 2.
In Figure 4a, we plot the generalization error and observe the double descent behavior of a full,
nonlinear Transformer for the ICL task as the number of pretraining samples varies (plotted as a
function of τ). We find that the peak of this curve occurs at the interpolation threshold, identified
by tracking when the training loss is non-zero (see figure caption). Our theory predicts that the
number of samples n at the peak of the curve, as well as the interpolation threshold, should scale
with d2. This scaling is indeed observed in Figure 4b for the full, nonlinear Transformers. These
observations suggest that the nonlinear Transformer operates within the scaling regime we have
identified.
Further,asobservedbeforeinRaventósetal.[18],werecoverthememorization-to-ICLtransition
as a function of pretraining task diversity, shown in Figure 3. We note that this transition happens
near κ = 1 in the nonlinear model, consistent with our theoretical predictions on the linear model.
5 Discussion
In this paper, we computed sharp asymptotics for the in-context learning (ICL) performance in a
simplified model of ICL for linear regression using linear attention. This exactly solvable model
demonstrates a transition from memorizing pretraining task vectors to generalizing beyond them
as the diversity of pretraining tasks increases, echoing empirical findings in full Transformers [18].
Additionally, we observe a sample-wise double descent as the amount of pretraining data increases.
Our numerical experiments show that full, nonlinear Transformers exhibit similar behavior in the
scaling regime relevant to our solvable model. Our work represents a first step towards a detailed
14
egdir
revo
rorre
ssecxEtheoretical understanding of the conditions required for ICL to emerge [19].
Our paper falls within a broader program of research that seeks sharp asymptotic character-
izations of the performance of machine learning algorithms. This program has a long history in
statistical physics [38, 41, 42], and has in recent years attracted substantial attention in machine
learning [36, 38, 43–49]. For simplicity, we have assumed that the covariates in the in-context re-
gression problem are drawn from an isotropic Gaussian. However, our technical approach could be
extended to anisotropic covariates, and, perhaps more interestingly, to featurized linear attention
models in which the inputs are passed through some feature map before linear attention is applied
[32, 33]. This extension would be possible thanks to an appropriate form of Gaussian universality:
for certain classes of regression problems, the asymptotic error coincides with that of a model where
the true features are replaced with Gaussian features of matched mean and covariance [36, 37, 43–
48, 50]. This would allow for a theoretical characterization of ICL for realistic data structure in a
close approximation of full softmax attention, yielding more precise predictions of how performance
scales in real Transformers.
In our analysis, we have assumed that the model is trained to interpolation on a fixed dataset.
Thisallowsustocastoursimplifiedformoflinearattentionpretrainingasaridgeregressionproblem,
which in turn enables our random matrix analysis. In contrast, Transformer-based large language
models are usually trained in a nearly-online setting, where each gradient update is estimated using
fresh examples with no repeating data [51]. Some of our findings, such as double-descent in the
learning curve as a function of the number of pretraining examples, are unlikely to generalize to the
fully-online setting. It will be interesting to probe these potential differences in future work.
Finally, our results have some bearing on the broad question of what architectural features are
required for ICL [7, 11, 19]. Our work shows that a full Transformer—or indeed even full linear
attention—is not required for ICL of linear regression. However, our simplified model retains the
structured quadratic interaction between inputs that is at the heart of the attention mechanism. It
is this quadratic interaction that allows the model to solve the ICL regression task, which it does
essentially by reversing the data correlation. One would therefore hypothesize that our model is
minimal in the sense that further simplifications within this model class would impair its ability
to solve this ICL task. In the specific context of regression with isotropic data, a simple point of
comparison would be to fix Γ = I , which gives a pretraining-free model that should perform well
d
when the context length is very long. However, this further-reduced model would perform poorly
if the covariates of the in-context task are anisotropic. More generally, it would be interesting
to investigate when models lacking this precisely-engineered quadratic interaction can learn linear
regressionin-context,andiftheyarelesssample-efficientthantheattention-basedmodelsconsidered
here.
Acknowledgements
YML was supported by NSF Award CCF-1910410, and by the Harvard FAS Dean’s Fund for
Promising Scholarship. JAZV and CP were supported by NSF Award DMS-2134157 and NSF
CAREER Award IIS-2239780. CP is further supported by a Sloan Research Fellowship. AM
acknowledges support from Perimeter Institute, which is supported in part by the Government of
Canada through the Department of Innovation, Science and Economic Development and by the
Province of Ontario through the Ministry of Colleges and Universities. This work has been made
possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner
InstitutefortheStudyofNaturalandArtificialIntelligence. Thisresearchwassupportedinpartby
grants NSF PHY-1748958 and PHY-2309135 to the Kavli Institute for Theoretical Physics (KITP),
15through the authors’ participation in the Fall 2023 program “Deep Learning from the Perspective
of Physics and Neuroscience.”
References
[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems, 30, 2017.
[2] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image
recognition at scale. arXiv, 2021.
[3] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
2018.
[4] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open
and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[5] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language
understanding by generative pre-training, 2018.
[6] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodels
are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
[8] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4
technical report. arXiv preprint arXiv:2303.08774, 2023.
[9] Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom
Conerly,NovaDassarma,DawnDrain,NelsonElhage,etal. Predictabilityandsurpriseinlarge
generative models. In Proceedings of the 2022 ACM Conference on Fairness, Accountability,
and Transparency, pages 1747–1764, 2022.
[10] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid,
Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al.
Beyondtheimitationgame: Quantifyingandextrapolatingthecapabilitiesoflanguagemodels.
arXiv preprint arXiv:2206.04615, 2022.
[11] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani
Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto,
Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language
models. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https:
//openreview.net/forum?id=yzkSU5zdwD.
16[12] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn
Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones,
Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark,
Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction
heads. Transformer Circuits Thread, 2022. URL https://transformer-circuits.pub/2022/
in-context-learning-and-induction-heads/index.html.
[13] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language
models a mirage? In Thirty-seventh Conference on Neural Information Processing Systems,
2023. URL https://openreview.net/forum?id=ITw9edRDlD.
[14] Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang.
Hidden progress in deep learning: Sgd learns parities near the computational limit. In
S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances
in Neural Information Processing Systems, volume 35, pages 21750–21764. Curran Asso-
ciates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/
884baf65392170763b27c914087bde01-Paper-Conference.pdf.
[15] Stephanie C. Y. Chan, Adam Santoro, Andrew K. Lampinen, Jane X. Wang, Aaditya Singh,
Pierre H. Richemond, Jay McClelland, and Felix Hill. Data distributional properties drive
emergent in-context learning in transformers, 2022.
[16] Aaditya K. Singh, Stephanie C. Y. Chan, Ted Moskovitz, Erin Grant, Andrew M. Saxe, and
Felix Hill. The transient nature of emergent in-context learning in transformers, 2023.
[17] AlbertoBietti,VivienCabannes,DianeBouchacourt,HerveJegou,andLeonBottou. Birthofa
transformer: Amemoryviewpoint. InA.Oh,T.Naumann,A.Globerson,K.Saenko,M.Hardt,
and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages
1560–1588. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_
files/paper/2023/file/0561738a239a995c8cd2ef0e50cfa4fd-Paper-Conference.pdf.
[18] Allan Raventós, Mansheej Paul, Feng Chen, and Surya Ganguli. Pretraining task di-
versity and the emergence of non-bayesian in-context learning for regression. In A. Oh,
T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances
in Neural Information Processing Systems, volume 36, pages 14228–14246. Curran Asso-
ciates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/
2e10b2c2e1aa4f8083c37dfe269873f8-Paper-Conference.pdf.
[19] GautamReddy. Themechanisticbasisofdatadependenceandabruptlearninginanin-context
classificationtask. InThe Twelfth International Conference on Learning Representations,2024.
URL https://openreview.net/forum?id=aN4Jf6Cx69.
[20] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statis-
ticians: Provable in-context learning with in-context algorithm selection. In A. Oh,
T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances
in Neural Information Processing Systems, volume 36, pages 57125–57211. Curran Asso-
ciates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/
b2e63e36c57e153b9015fece2352a9f9-Paper-Conference.pdf.
[21] Yingcong Li, M. Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as
algorithms: Generalization and stability in in-context learning, 2023.
17[22] EkinAkyürek, DaleSchuurmans, JacobAndreas, TengyuMa, andDennyZhou. Whatlearning
algorithm is in-context learning? investigations with linear models. In The Eleventh Interna-
tional Conference on Learning Representations, 2023. URL https://openreview.net/forum?
id=0g0X4H8yN4I.
[23] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to
implement preconditioned gradient descent for in-context learning, 2023.
[24] Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. Transformers learn higher-order
optimization methods for in-context learning: A study with linear models, 2023.
[25] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander
Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by
gradient descent. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engel-
hardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International
Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research,
pages 35151–35174. PMLR, 23–29 Jul 2023. URL https://proceedings.mlr.press/v202/
von-oswald23a.html.
[26] Ruiqi Zhang, Jingfeng Wu, and Peter L. Bartlett. In-context learning of a linear transformer
block: Benefits of the mlp component and one-step gd initialization, 2024.
[27] Ruiqi Zhang, Spencer Frei, and Peter L. Bartlett. Trained transformers learn linear models
in-context. Journal of Machine Learning Research, 25(49):1–55, 2024. URLhttp://jmlr.org/
papers/v25/23-1042.html.
[28] Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter
Bartlett. How many pretraining tasks are needed for in-context learning of linear regres-
sion? In The Twelfth International Conference on Learning Representations, 2024. URL
https://openreview.net/forum?id=vSh5ePa0ph.
[29] Pritam Chandra, Tanmay Kumar Sinha, Kabir Ahuja, Ankit Garg, and Navin Goyal. To-
wards analyzing self-attention via linear neural network, 2024. URL https://openreview.
net/forum?id=4fVuBf5HE9.
[30] Karthik Duraisamy. Finite sample analysis and bounds of generalization error of gradient
descent in in-context linear regression. arXiv preprint arXiv:2405.02462, 2024.
[31] SinongWang,BelindaZ.Li,MadianKhabsa,HanFang,andHaoMa. Linformer: Self-attention
with linear complexity, 2020.
[32] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention:
Attention with linear complexities. In Proceedings of the IEEE/CVF winter conference on
applications of computer vision, pages 3531–3539, 2021.
[33] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers
are RNNs: Fast autoregressive transformers with linear attention. In International conference
on machine learning, pages 5156–5165. PMLR, 2020.
[34] Vladimir Alexandrovich Marchenko and Leonid Andreevich Pastur. Distribution of eigenvalues
for some sets of random matrices. Matematicheskii Sbornik, 114(4):507–536, 1967.
18[35] Zhidong Bai and Jack W Silverstein. Spectral analysis of large dimensional random matrices,
volume 20. Springer, 2010.
[36] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. Surprises in high-
dimensional ridgeless least squares interpolation. The Annals of Statistics, 50(2):949 – 986,
2022. doi: 10.1214/21-AOS2133. URL https://doi.org/10.1214/21-AOS2133.
[37] Sofiia Dubova, Yue M. Lu, Benjamin McKenna, and Horng-Tzer Yau. Universality for the
global spectrum of random inner-product kernel matrices in the polynomial regime. arXiv,
2023.
[38] Alexander B. Atanasov, Jacob A. Zavatone-Veth, and Cengiz Pehlevan. Scaling and renormal-
ization in high-dimensional regression. arXiv, 2024.
[39] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learningpracticeandtheclassicalbias–variancetrade-off. Proceedings of the National Academy
of Sciences, 116(32):15849–15854, 2019.
[40] DiederikKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InInternational
Conference on Learning Representations (ICLR), San Diega, CA, USA, 2015.
[41] Timothy L. H. Watkin, Albrecht Rau, and Michael Biehl. The statistical mechanics of learning
a rule. Rev. Mod. Phys., 65:499–556, Apr 1993. doi: 10.1103/RevModPhys.65.499. URL
https://link.aps.org/doi/10.1103/RevModPhys.65.499.
[42] Andreas Engel and Christian van den Broeck. Statistical Mechanics of Learning. Cambridge
University Press, 2001. doi: https://doi.org/10.1017/CBO9781139164542.
[43] Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mézard, and Lenka Zdeborová. Gen-
eralisation error in learning with random features and the hidden manifold model. In Interna-
tional Conference on Machine Learning, pages 3452–3462. PMLR, 2020.
[44] Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mezard,
and Lenka Zdeborová. Learning curves of generic features maps for realistic datasets with a
teacher-student model. Advances in Neural Information Processing Systems, 34:18137–18151,
2021.
[45] Song Mei and Andrea Montanari. The generalization error of random features regression:
Precise asymptotics and the double descent curve. Communications on Pure and Applied
Mathematics, 75(4):667–766, 2022.
[46] HongHuandYueMLu. Universalitylawsforhigh-dimensionallearningwithrandomfeatures.
IEEE Transactions on Information Theory, 69(3):1932–1964, 2022.
[47] Hong Hu, Yue M. Lu, and Theodor Misiakiewicz. Asymptotics of random feature regression
beyond the linear scaling regime. arXiv:2403.08160, 2024.
[48] Oussama Dhifallah and Yue M Lu. A precise performance analysis of learning with random
features. arXiv preprint arXiv:2008.11904, 2020.
[49] Hugo Cui, Freya Behrens, Florent Krzakala, and Lenka Zdeborová. A phase transition between
positional and semantic learning in a solvable model of dot-product attention. arXiv, 2024.
19[50] Andrea Montanari and Basil N. Saeed. Universality of empirical risk minimization. In Po-Ling
Loh and Maxim Raginsky, editors, Proceedings of Thirty Fifth Conference on Learning Theory,
volume 178 of Proceedings of Machine Learning Research, pages 4310–4312. PMLR, 02–05 Jul
2022. URL https://proceedings.mlr.press/v178/montanari22a.html.
[51] NiklasMuennighoff, AlexanderMRush, BoazBarak,TevenLeScao, NouamaneTazi,Aleksan-
draPiktus, SampoPyysalo, ThomasWolf, andColinRaffel. Scalingdata-constrainedlanguage
models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL
https://openreview.net/forum?id=j5BuTrEj35.
[52] László Erdős, Antti Knowles, Horng-Tzer Yau, and Jun Yin. The local semicircle law for a
general class of random matrices. Electronic Journal of Probability, 18(none):1 – 58, 2013. doi:
10.1214/EJP.v18-2473. URL https://doi.org/10.1214/EJP.v18-2473.
[53] László Erdős and Horng-Tzer Yau. A dynamical approach to random matrix theory, volume 28.
American Mathematical Soc., 2017.
[54] Alston S. Householder. Unitary triangularization of a nonsymmetric matrix. J. ACM, 5(4):
339–342, oct 1958. ISSN 0004-5411. doi: 10.1145/320941.320947. URL https://doi.org/10.
1145/320941.320947.
[55] Yue M. Lu. Householder dice: A matrix-free algorithm for simulating dynamics on Gaussian
and random orthogonal ensembles. IEEE Transactions on Information Theory, 67(12):8264–
8272, 2021. doi: 10.1109/TIT.2021.3114351.
[56] Lloyd N. Trefethen and David Bau, III. Numerical Linear Algebra. Society for Industrial and
Applied Mathematics, Philadelphia, PA, 1997. doi: 10.1137/1.9780898719574. URL https:
//epubs.siam.org/doi/abs/10.1137/1.9780898719574.
20Supplementary Information
Someofthederivationsinthisdocumentarebasedonnon-rigorousyettechnicallysoundheuris-
ticargumentsfromrandommatrixtheory. Wesupportthesepredictionswithnumericalsimulations
and discuss the steps required to achieve a fully rigorous proof. All rigorously proven results will
be clearly stated as lemmas, propositions, and the like.
SI.1 Notation
√
Sets, vectors and matrices: For each n ∈ N, [n] := {1,2,...,n}. The sphere in Rd with radius d is
√
expressed as Sd−1( d). For a vector v ∈ Rd, its ℓ norm is denoted by∥v∥. For a matrix A ∈ Rd×d,
2
∥A∥ and∥A∥ denote the operator (spectral) norm and the Frobenius norm of A, respectively.
op F
(cid:12) (cid:12)
Additionally,∥A∥
∞
:= max i,j∈[n](cid:12)A(i,j)(cid:12) denotes the entry-wise ℓ
∞
norm. We use e
1
to denote the
first natural basis vector (1,0,...,0), and I is an identity matrix. Their dimensions can be inferred
from the context. The trace of A is written as tr(A).
Our derivations will frequently use the vectorization operation, denoted by vec(·). It maps a
d 1×d
2
matrixA ∈ Rd1×d2 toavectorv
A
= vec(A)inRd1d2. Notethatweshalladopttherow-major
convention, and thus the rows of A are stacked together to form v . We also recall the standard
A
identity:
vec(E E E ) = (E ⊗E⊤)vec(E ), (SI.1.1)
1 2 3 1 3 2
where ⊗ denotes the matrix Kronecker product, and E ,E ,E are matrices whose dimensions are
1 2 3
compatible for the multiplication operation. For any square matrix A ∈ R(L+1)×(L+1), we introduce
the notation
[M] ∈ RL×L (SI.1.2)
\0
to denote the principal minor of M after removing its first row and column.
Stochastic order notation: In our analysis, we use a concept of high-probability bounds known
as stochastic domination. This notion, first introduced in [52, 53], provides a convenient way to
account for low-probability exceptional events where some bounds may not hold. Consider two
families of nonnegative random variables:
X = (cid:0) X(d)(u) : d ∈ N,u ∈ U(d)(cid:1) , Y = (cid:0) Y(d)(u) : d ∈ N,u ∈ U(d)(cid:1) ,
where U(d) is a possibly d-dependent parameter set. We say that X is stochastically dominated by
Y, uniformly in u, if for every (small) ε > 0 and (large) D > 0 we have
sup P[X(d)(u) > dεY(d)(u)] ≤ d−D
u∈U(d)
for sufficiently large d ≥ d (ε,D). If X is stochastically dominated by Y, uniformly in u, we use
0
the notation X ≺ Y. Moreover, if for some family X we have|X| ≺ Y, we also write X = O (Y).
≺
We also use the notation X ≃ Y to indicate that two families of random variables X,Y are
asymptotically equivalent. Precisely, X ≃ Y, if there exists ε > 0 such that for every D > 0 we
have
(cid:104) (cid:105)
P |X −Y| > d−ε ≤ d−D (SI.1.3)
for all sufficiently large d > d (ε,D).
0
21SI.2 Moment Calculations and Generalization Errors
For a given set of parameters Γ, its generalization error is defined as
(cid:104) (cid:105)
e(Γ) = E (cid:0) y −⟨Γ,H ⟩(cid:1)2 , (SI.2.1)
Ptest ℓ+1 Z
where (Z,y ) ∼ P is a new sample drawn from the distribution of the test data set. Recall
ℓ+1 test
that Z is the input embedding matrix defined in (2) in the main text, and y denotes the missing
ℓ+1
value to be predicted. The goal of this section is to derive an expression for the generalization error
e(Γ).
Note that the test distribution P crucially depends on the probability distribution of the task
test √
vector w used in the linear model in (1). For the ICL task, we have w ∼ Unif(Sd−1( d)), the
√
uniform distribution on the sphere Sd−1( d). For the IDG task, w is sampled uniformly from the
set {w ,w ,...,w }, where these k vectors are the same as those used in the training data [see (4)].
1 2 k
In what follows, we slightly abuse the notation by writing w ∼ P to indicate that w is sampled
test
from the task vector distribution associated with P .
test
Let w be the task vector used in the input matrix Z. Throughout the paper, we use E [·] to
w
denote the conditional expectation with respect to the randomness in the data vectors {x }
i i∈[ℓ+1]
and the noise {ϵ } , with the task vector w kept fixed. We have the following expressions for
i i∈[ℓ+1]
the first two conditional moments of (H ,y ).
Z ℓ+1
√
Lemma 1 (Conditional moments). Let the task vector w ∈ Sd−1( d) be fixed. We have
E [y ] = 0, and E [H ] = 0. (SI.2.2)
w ℓ+1 w Z
Moreover,
1 (cid:104) (cid:105)
E [y H ] = w w⊤, 1+ρ (SI.2.3)
w ℓ+1 Z
d
and
(cid:34) (cid:35)
(cid:104) (cid:105) (1+ρ) dI +(1+ℓ−1)(1+ρ)−1ww⊤ (1+2ℓ−1)w
E vec(H )vec(H )⊤ = I ⊗ ℓ d .
w Z Z d d (1+2ℓ−1)w⊤ (1+2ℓ−1)(1+ρ)
(SI.2.4)
Proof. Using the equivalent representations in (A.1) and (A.2), it is straightforward to verify the
estimates of the first (conditional) moments in (SI.2.2). To show (SI.2.3), we note that
H = (d/ℓ)z z⊤,
Z a b
where
(cid:34) (cid:35) (cid:34) (cid:35)
s M h
z = M and z = √ w √ √ .
a w u b (θ a/ d+θ )2/ d+θ2/ d
w ϵ q
Using the representation in (A.2), we have
(cid:104) (cid:105)
E [y H ] = (d/ℓ)E [y z ]E z⊤ .
w ℓ+1 Z w ℓ+1 a w b
(cid:104) (cid:105)
ComputingtheexpectationsE [y z ]andE z⊤ thengivesus(SI.2.3). Next, weshow (SI.2.4).
w ℓ+1 a w b
Since z and z are independent,
a b
(cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
E vec(H )vec(H )⊤ = (d/ℓ)2E z z⊤ ⊗E z z⊤ .
Z Z a a b b
22Thefirstexpectationontheright-handsideiseasytocompute. SinceM isanorthonormalmatrix,
w
(cid:104) (cid:105)
E z z⊤ = I (SI.2.5)
w a a d
To obtain the second expectation on the right-hand side of the above expression, we can first verify
that
(cid:20) (cid:21)
(cid:104) (cid:105) ℓ (ℓ+1)
E M hh⊤M = (1+ρ)I + ww⊤ . (SI.2.6)
w w w d2 d d
Moreover,
(cid:20) (cid:16) √ √ √ (cid:17)(cid:21) ℓ(ℓ+2)(1+ρ)
E M h (a/ d+θ )2/ d+θ2/ d = w (SI.2.7)
w w ϵ q d3
and
(cid:20) (cid:16) √ √ √ (cid:17)2(cid:21) ℓ(ℓ+2)(1+ρ)2
E (a/ d+θ )2/ d+θ2/ d = . (SI.2.8)
w ϵ q d3
Combining (SI.2.6), (SI.2.7), and (SI.2.8), we have
(cid:34) (cid:35)
(cid:104) (cid:105) (ℓ/d)2(1+ρ) dI +(1+ℓ−1)(1+ρ)−1ww⊤ (1+2ℓ−1)w
E z z⊤ = ℓ d . (SI.2.9)
b b d (1+2ℓ−1)w⊤ (1+2ℓ−1)(1+ρ)
Substituting (SI.2.5) and (SI.2.9) into (SI.2.4), we reach the formula in (SI.2.4).
Proposition 1 (Generalization error). For a given weight matrix Γ, the generalization error of the
linear transformer is
 
(cid:34) (cid:35)
1+ρ dI +(1+ℓ−1)(1+ρ)−1R (1+2ℓ−1)b
e(Γ) = trΓ ℓ d test test Γ⊤ 
d (1+2ℓ−1)b⊤ (1+2ℓ−1)(1+ρ)
test
 
(cid:34) (cid:35)
2 R
− trΓ test +1+ρ,
d (1+ρ)b⊤
test
where
(cid:104) (cid:105)
b := E [w] and R := E ww⊤ . (SI.2.10)
test w∼Ptest test w∼Ptest
Remark 1. We use w ∼ P to indicate that w is sampled from the task vector distribution
test √
associated with P . Recall our discussions in Section 2.6. For the ICL task, w ∼ Unif(Sd−1( d)).
test
It is straightforward to check that, in this case,
(ICL) : b = 0 and R = I . (SI.2.11)
test test d
For the IDG task, we have
1 (cid:88) 1 (cid:88)
(IDG) : b = w and R = w w⊤, (SI.2.12)
test k i test k i i
i i∈[k]
where {w } is the set of fixed task vectors used in the training data.
i i∈[k]
23Proof. Recall the definition of the generalization error in (SI.2.1). We start by writing
(cid:104) (cid:105) (cid:104) (cid:105)
e(Γ) = vec(Γ)⊤E vec(H )vec(H )⊤ vec(Γ)−2vec(Γ)⊤vec(E[y H ])+E y2 ,
Z Z N+1 Z ℓ+1
where H is a matrix in the form of (7) and H is independent of Γ. Since y = x⊤ w+ϵ, with
Z Z ℓ+1 ℓ+1
ϵ ∼ N(0,ρ) denoting the noise, it is straightforward to check that
(cid:104) (cid:105)
E y2 = 1+ρ.
ℓ+1
Using the moment estimate (SI.2.4) in Lemma 1 and the identity (SI.1.1), we have
(cid:104) (cid:105)
vec(Γ)⊤E vec(H )vec(H )⊤ vec(Γ)
Z Z
 
(cid:34) (cid:35)
1+ρ dI +(1+ℓ−1)(1+ρ)−1R (1+2ℓ−1)b
= trΓ ℓ d test test Γ⊤ .
d (1+2ℓ−1)b⊤ (1+2ℓ−1)(1+ρ)
test
Moreover, by (SI.2.3),
 
(cid:34) (cid:35)
vec(Γ)⊤vec(cid:0)E[y ℓ+1H Z](cid:1) = d1 trΓ (1+R t ρe )s bt
⊤
.
test
Corollary 1. For a given set of parameters Γ, its generalization error can be written as
1 (cid:16) (cid:17) 2 (cid:16) (cid:17)
e(Γ) = tr ΓB Γ⊤ − tr ΓA⊤ +(1+ρ)+E, (SI.2.13)
d test d test
where
(cid:104) (cid:105)
A := R (1+ρ)b , (SI.2.14)
test test test
(cid:34) (cid:35)
1(1+ρ)I +R (1+ρ)b
B := α d test test , (SI.2.15)
test (1+ρ)b⊤ (1+ρ)2
test
and R ,b are as defined in (SI.2.10). Moreover, E denotes an “error” term such that
test test
(cid:110) (cid:111)(cid:16) (cid:17)
C max ∥R ∥ ,∥b ∥,1 ∥Γ∥2/d
α,ρ test op test F
|E| ≤ , (SI.2.16)
d
where C is some constant that only depends on α and ρ.
α,ρ
Proof. Let
(cid:34) (cid:35)
d(1+ρ)I +(1+ℓ−1)R (1+2ℓ−1)(1+ρ)b
∆ = ℓ d test test −B .
(1+2ℓ−1)(1+ρ)b⊤ (1+2ℓ−1)(1+ρ)2 test
test
It is straightforward to check that
1 (cid:16) (cid:17)
E = tr Γ∆Γ⊤
d
1
= vec(Γ)⊤(I ⊗∆)vec(Γ)
d
d
∥Γ∥2
≤∥∆∥ F.
op d
(cid:110) (cid:111)
The bound in (SI.2.16) follows from the estimate that∥∆∥ ≤ C max ∥R ∥ ,∥b ∥,1 /d.
op α,ρ test op test
24Remark 2. Consider the optimal weight matrix Γ∗ obtained by solving the ridge regression problem
in (9). Since Γ∗ is the optimal solution of (9), we must have
n λ(cid:13) (cid:13)Γ∗(cid:13) (cid:13)2 ≤ (cid:88) (yµ )2,
d F ℓ+1
µ∈[n]
where the right-hand side is the value of the objective function of (9) when we choose Γ to be the
all-zero matrix. It follows that
∥Γ∗∥2 (cid:80) (yµ )2
F ≤ µ∈[n] ℓ+1 .
d λn
By the law of large numbers, (cid:80) µ∈[n]y µ2 → 1+ρ as n → ∞. Thus,∥Γ∗∥2/d is asymptotically bounded
n F
by the constant (1+ρ)/λ. Furthermore, it is easy to check that∥R ∥ = O(1) and∥b ∥ = O(1)
test op test
for both ICL [see (SI.2.11)] and IDG [see (SI.2.12)]. It then follows from Corollary 1 that the
generalization error associated with the optimal parameers Γ∗ is asymptotically determined by the
first three terms on the right-hand side of (SI.2.13).
SI.3 Analysis of Ridge Regression: Extended Resolvent Matrices
WeseefromCorollary1andRemark2thatthetwokeyquantitiesindeterminingthegeneralization
error e(Γ∗) are
1 1
tr(Γ∗A⊤ ) and tr(Γ∗B (Γ∗)⊤), (SI.3.1)
d test d test
whereA andB arethematricesdefinedin(SI.2.14)and(SI.2.15),respectively. Inthissection,
test test
we show that the two quantities in (SI.3.1) can be obtained by studying a parameterized family of
extended resolvent matrices.
To start, we observe that the ridge regression problem in (9) admits the following closed-form
solution:
(cid:16) (cid:17)
vec(Γ∗) = G (cid:80) y vec(H ) /d, (SI.3.2)
µ∈[n] µ µ
where G is a resolvent matrix defined as
(cid:16) (cid:17)−1
G = (cid:80) vec(H )vec(H )⊤/d+τλI . (SI.3.3)
µ∈[n] µ µ
Forourlateranalysisofthegeneralizationerror,weneedtoconsideramoregeneral,“parameterized”
version of G, defined as
(cid:16) (cid:17)−1
G(π) = (cid:80) vec(H )vec(H )⊤/d+πΩ+τλI , (SI.3.4)
µ∈[n] µ µ
where Ω ∈ R(d2+d)×(d2+d) is a symmetric positive-semidefinite matrix and π is a nonnegative scalar.
The original resolvent G in (SI.3.3) is a special case, corresponding to π = 0.
The objects in (SI.3.2) and (SI.3.4) are the submatrices of an extended resolvent matrix, which
we construct as follows. For each µ ∈ [n], let
(cid:34) (cid:35)
y /d
z = µ √ (SI.3.5)
µ
vec(H )/ d
µ
be an (d2+d+1)-dimensional vector. Let
(cid:34) (cid:35)
0
Ω = , (SI.3.6)
e Ω
25where Ω is the (d2+d)×(d2+d) matrix in (SI.3.4). Define an extended resolvent matrix
1
G (π) = . (SI.3.7)
e (cid:80) z z⊤+πΩ +τλI
µ∈[n] µ µ e
By block-matrix inversion, it is straightforward to check that
(cid:34) (cid:35)
c(π) −c(π)q⊤(π)
G (π) = , (SI.3.8)
e −c(π)q(π) G(π)+c(π)q(π)q⊤(π)
where
1 (cid:16) (cid:17)
(cid:80)
q(π) := G(π) y vec(H ) (SI.3.9)
d3/2 µ∈[n] µ µ
is a vector in Rd(d+1), and c(π) is a scalar such that
1 1 (cid:88) 1 (cid:88)
= y2 +τλ− y y vec(H )⊤G(π)vec(H ). (SI.3.10)
c(π) d2 µ d3 µ ν µ ν
µ∈[n] µ,ν∈[n]
By comparing (SI.3.9) with (SI.3.2), we see that
√
vec(Γ∗) = dq(0). (SI.3.11)
Moreover, as shown in the following lemma, the two key quantities in (SI.3.1) can also be obtained
from the extended resolvent G (π).
e
Lemma 2. For any matrix A ∈ Rd×(d+1),
1 −1 (cid:104) (cid:105)
tr(Γ∗A⊤) = √ 0 vec(A)T G (0)e , (SI.3.12)
e 1
d c(0) d
where e denotes the first natural basis vector in Rd2+d+1. Moreover, for any symmetric and positive
1
semidefinite matrix B ∈ R(d+1)×(d+1), if we set
Ω = I ⊗B (SI.3.13)
d
in (SI.3.6), then
(cid:18) (cid:19)(cid:12)
1 tr(Γ∗B(Γ∗)⊤) = d 1 (cid:12) (cid:12) . (SI.3.14)
d dπ c(π) (cid:12)
π=0
Proof. The identity (SI.3.12) follows immediately from the block form of G (π) in (SI.3.8) and the
e
observationin(SI.3.11). Toshow (SI.3.14), wetakethederivativeof1/c(π)withrespecttoπ. From
(SI.3.10), and using the identity
d
G(π) = −G(π)ΩG(π),
dπ
we have
(cid:18) (cid:19)
d 1 1 (cid:88)
= y y vec(H )⊤G(π)ΩG(π)vec(H )
dπ c(π) d3 µ ν µ ν
µ,ν∈[n]
= q⊤(π)Ωq(π).
26Thus, by (SI.3.11),
(cid:18) (cid:19)(cid:12)
d 1 (cid:12) (cid:12) = 1 (cid:0) vec(Γ∗)(cid:1)⊤ Ωvec(Γ∗)
dπ c(π) (cid:12) d
π=0
= 1 (cid:0) vec(Γ∗)(cid:1)⊤ (I ⊗B)vec(Γ∗).
d
d
Applyingtheidentityin(SI.1.1)totheright-handsideoftheaboveequation,wereach(SI.3.14).
Remark 3. To lighten the notation, we will often write G (π) [resp. G(π)] as G [resp. G], leaving
e e
their dependence on the parameter π implicit.
Remark 4. In light of (SI.3.13) and (SI.3.14), we will always choose
Ω = I ⊗B , (SI.3.15)
d test
where B is the matrix defined in (SI.2.15).
test
SI.4 An Asymptotic Equivalent of the Extended Resolvent Matrix
In this section, we derive an asymptotic equivalent of the extended resolvent G defined in (SI.3.7).
e
From this equivalent version, we can then obtain the asymptotic limits of the right-hand sides of
(SI.3.12)and(SI.3.14). Ouranalysisreliesonnon-rigorousbuttechnicallysoundheuristicarguments
from random matrix theory. Therefore, we refer to our theoretical predictions as results rather than
propositions.
Recall that there are k unique task vectors {w } in the training set. Let
i i∈[k]
1 (cid:88) 1 (cid:88)
b := w and R := w w⊤ (SI.4.1)
tr k i tr k i i
i∈[k] i∈[k]
denote the empirical mean and correlation matrix of these k regression vectors, respectively. Define
(cid:104) (cid:105)
A := R (1+ρ)b . (SI.4.2)
tr tr tr
and
(cid:34) (cid:35)
(1+ρ)
I +R (1+ρ)b
E := α d tr tr . (SI.4.3)
tr (1+ρ)b⊤ (1+ρ)2
tr
Definition 1. Consider the extended resolvent G (π) in (SI.3.7), with Ω chosen in the forms of
e e
(SI.3.6) and (SI.3.15). Let G(cid:101)e be another matrix of the same size as G e(π). We say that G(cid:101)e and
G (π) are asymptotically equivalent, if the following conditions hold.
e
(1) For any two deterministic and unit-norm vectors u,v ∈ Rd2+d+1,
u⊤G e(π)v ≃ u⊤G(cid:101)ev, (SI.4.4)
where ≃ is the asymptotic equivalent notation defined in (SI.1.3).
(cid:104) (cid:105)
(2) Let A = R (1+ρ)b . For any deterministic, unit-norm vector v ∈ Rd2+d+1,
tr tr tr
1 (cid:104) (cid:105) 1 (cid:104) (cid:105)
√ 0 vec(A tr)⊤ G e(π)v ≃ √ 0 vec(A tr)⊤ G(cid:101)ev. (SI.4.5)
d d
27(3) Recall the notation introduced in (SI.1.2). We have
(cid:18) (cid:19)
d1
2
tr(cid:16) (cid:2) G e(π)(cid:3) \0·[I ⊗E tr](cid:17) = d1
2
tr (cid:104) G(cid:101)e(cid:105) \0·[I ⊗E tr] +O ≺(d−1/2), (SI.4.6)
(cid:2) (cid:3) (cid:2) (cid:3)
where G (π) and G (π) denote the principal minors of G (π) and G (π), respectively.
e \0 e \0 e e
Result 3. Let χ denote the unique positive solution to the equation
π
(cid:20) (cid:21)
1 (cid:16) τ (cid:17)−1
χ = tr E +πB +λτI E , (SI.4.7)
π tr test d tr
d 1+χ
π
where B is the positive-semidefinite matrix in (SI.2.15), with b ,R chosen accroding to
test test test
(SI.2.11) or (SI.2.12). The extended resolvent G (π) in (SI.3.7) is asymptotically equivalent to
e

 
−1
(cid:18)
(cid:104)
(cid:105)(cid:19)⊤
G e(π) :=

  1+τ χ    (cid:18) (cid:104)
1+ρ
(cid:105)(cid:19)
√1
d
vec R
tr
(1+ρ)b
tr   +πΩ
e+τλI
   ,
 π  √1 vec R
tr
(1+ρ)b
tr
I d⊗E
tr
 
d
(SI.4.8)
in the sense of Definition 1. In the above expression, Ω is the matrix in (SI.3.6) with Ω = I ⊗B .
e d test
In what follows, we present the steps in reaching the asymptotic equivalent G (π) given in
e
[µ]
(SI.4.8). To start, let G to denote a “leave-one-out” version of G , defined as
e e
1
G[µ] = .
e (cid:80) z z⊤+πΩ +τλI
ν̸=µ ν ν e
By (SI.3.7), we have
(cid:16) (cid:17)
G (cid:80) z z⊤+πΩ +τλI = I.
e µ∈[n] µ µ e
Applying the Woodbury matrix identity then gives us
(cid:88) 1
G[µ]z z⊤+G (πΩ +τλI) = I. (SI.4.9)
1+z⊤G[µ]
z
e µ µ e e
µ∈[n] µ e µ
To proceed, we study the quadratic form
z⊤G[µ]
z . Let w denotes the task vector associated with
µ e µ µ
z . Conditioned on w and Gµ, the quadratic form z⊤G[µ] z concentrates around its conditional
µ µ e µ e µ
expectation with respect to the remaining randomness in z . Specifically,
µ
z⊤G[µ]z = χµ(w )+O (d−1/2),
µ e µ µ ≺
where
χµ(w ) := 1 tr(cid:16) [Gµ] ·(cid:2) I ⊗E(w )(cid:3)(cid:17) , (SI.4.10)
µ d2 e \0 µ
and
(cid:34) (cid:35)
1+ρI +ww⊤ (1+ρ)w
E(w) := α d . (SI.4.11)
(1+ρ)w⊤ (1+ρ)2
28Substituting z⊤G[µ] z in (SI.4.9) by χµ(w ), we get
µ e µ µ
(cid:88) 1
G[µ]z z⊤+G (πΩ +τλI) = I +∆ , (SI.4.12)
1+χµ(w ) e µ µ e e 1
µ
µ∈[n]
where
∆ :=
(cid:88) z µ⊤G[ eµ] z µ−χµ(w µ)
G[µ]z z⊤
1 (1+χµ(w ))(1+z⊤G[µ] z ) e µ µ
µ∈[n] µ µ e µ
is a matrix that captures the approximation error of the above substitution.
(cid:104) (cid:105)
Next,wereplacez z⊤ontheleft-handsideof (SI.4.12)byitsconditional expectationE z z⊤ ,
µ µ wµ µ µ
conditioned on the task vector w . This allows us to rewrite (SI.4.12) as
µ
(cid:88) 1 (cid:104) (cid:105)
G[µ]E z z⊤ +G (πΩ +τλI) = I +∆ +∆ , (SI.4.13)
1+χµ(w ) e wµ µ µ e e 1 2
µ
µ∈[n]
where
(cid:18) (cid:19)
(cid:88) 1 (cid:104) (cid:105)
∆ := G[µ] E z z⊤ −z z⊤
2 1+χµ(w ) e wµ µ µ µ µ
µ
µ∈[n]
captures the corresponding approximation error. Recall the definition of z in (SI.3.5). Using the
µ
moment estimates in Lemma 1, we have
 
(cid:104) (cid:105)
E (cid:104) z z⊤(cid:105) = 1  
1+ (cid:34)ρ
(cid:35)
√1 dw µ⊤⊗ w µ⊤ 1+ρ
 + 1
(cid:34)
0
(cid:35)
, (SI.4.14)
wµ µ µ d2  √1 dw µ⊗ 1w +µ
ρ
I d⊗E(w µ)   d2 I d⊗E µ
where E(w ) is the matrix defined in (SI.4.11) and
µ
(cid:34) (cid:35)
1 w w⊤ 2(1+ρ)w
E = µ µ µ .
µ ℓ 2(1+ρ)w⊤ 2(1+ρ)2
µ
(cid:104) (cid:105)
Replacing the conditional expectation E z z⊤ in (SI.4.13) by the main (i.e. the first) term on
wµ µ µ
the right-hand side of (SI.4.14), we can transform (SI.4.13) to
 
(cid:104) (cid:105)
1+ρ √1 w⊤⊗ w⊤ 1+ρ
τ (cid:88) 1 G[µ]  (cid:34) (cid:35) d µ µ  +G (πΩ +τλI) = I+∆ +∆ +∆ ,
n
µ∈[n]
1+χµ(w µ) e  √1 dw µ⊗ 1w +µ
ρ
I d⊗E(w µ)   e e 1 2 3
(SI.4.15)
where we recall τ = n/d2, and we use ∆ to capture the approximation error associated with E .
3 µ
Next, we replace the “leave-one-out” terms Gµ and χµ(w ) in (SI.4.15) by their “full” versions.
e µ
Specifically, we replace Gµ by G , and χµ(w ) by
e e µ
1 (cid:16) (cid:2) (cid:3)(cid:17)
χ(w ) := tr [G ] · I ⊗E(w ) . (SI.4.16)
µ d2 e \0 µ
29It is important to note the difference between (SI.4.10) and (SI.4.16): the former uses Gµ and the
e
latter G . After these replacements and using ∆ to capture the approximation errors, we have
e 4
 
 
(cid:104) (cid:105)
1+ρ √1 w⊤⊗ w⊤ 1+ρ
G e    nτ µ(cid:88)
∈[n]
1+χ1 (w µ)    √1 dw µ⊗(cid:34) 1w +µ ρ(cid:35) d µ I d⊗Eµ (w µ)    +πΩ e+τλI   

= I +(cid:88) j≤4∆ j.
(SI.4.17)
Recall that there are k unique task vectors {w } in the training set consisting of n input
i 1≤i≤k
samples. Each sample is associated with one of these task vectors, sampled uniformly from the set
{w } . In our analysis, we shall assume that k divides n and that each unique task vector is
i 1≤i≤k
associated with exactly n/k input samples. (We note that this assumption merely serves to simplify
the notation. The asymptotic characterization of the random matrix G remains the same even
e
without this assumption.) Observe that there are only k unique terms in the sum on the left-hand
side of (SI.4.17). Thus,
 
 
(cid:104) (cid:105)
1+ρ √1 w⊤⊗ w⊤ 1+ρ
G e    kτ i(cid:88)
∈[k]
1+χ1 (w i)    √1 dw i⊗(cid:34) 1w +i ρ(cid:35) d i I d⊗Ei (w i)    +πΩ e+τλI   

= I +(cid:88) j≤4∆ j.
(SI.4.18)
So far, we have been treating the k task vectors {w } as fixed vectors, only using the
i i∈[k]
randomnessintheinputsamplesthatareassociatedwiththedatavectors(cid:8) xµ(cid:9)
. Tofurthersimplify
i
our asymptotic characterization, we take advantage of the fact that {w } are independently
√ i i∈[k]
sampled from Unif(Sd−1( d)). To that end, we can first show that χ(w ) in (SI.4.16) concentrates
i
around its expectation. Specifically,
(cid:20) (cid:21)
χ(w ) = E 1 tr(cid:16) [G ] ·(cid:2) I ⊗E(w )(cid:3)(cid:17) +O (d−1/2).
i d2 e \0 i ≺
By symmetry, we must have
(cid:20) (cid:21) (cid:20) (cid:21)
E 1 tr(cid:16) [G ] ·(cid:2) I ⊗E(w )(cid:3)(cid:17) = E 1 tr(cid:16) [G ] ·(cid:2) I ⊗E(w )(cid:3)(cid:17)
d2 e \0 i d2 e \0 j
(cid:12) (cid:12)
for any 1 ≤ i < j ≤ k. It follows that(cid:12)χ(w i)−χ(w j)(cid:12) = O ≺(d−1/2), and thus, by a union bound,
max(cid:12) (cid:12)χ(w k1)−χ (cid:98)ave(cid:12) (cid:12) = O ≺(d−1/2), (SI.4.19)
i∈[k]
where
1 (cid:88)
χ := χ(w ). (SI.4.20)
(cid:98)ave i
k
i∈[k]
Upon substituting (SI.4.16) into (SI.4.20), it is straightforward to verify the following characteriza-
tion of χ :
(cid:98)ave
1 (cid:16) (cid:17)
χ = tr [G ] ·[I ⊗E ] . (SI.4.21)
(cid:98)ave d2 e \0 tr
Theestimatein(SI.4.19)promptsustoreplacethetermsχ(w )intheright-handsideof (SI.4.18)
i
by the common value χ . As before, we introduce a matrix ∆ to capture the approximation error
(cid:98)ave 5
30associated with this step. Using the newly introduced notation E ,b and R in (SI.4.3) and
tr tr tr
(SI.4.1), we can then simplify (SI.4.18) as
 
 
(cid:18)
(cid:104)
(cid:105)(cid:19)⊤
G
e
  1+τ χ    (cid:18) (cid:104)
1+ρ
(cid:105)(cid:19)
√1
d
vec R
tr
(1+ρ)b
tr   +πΩ
e+τλI
  
 (cid:98)ave  √1 vec R
tr
(1+ρ)b
tr
I d⊗E
tr
 
d
(cid:88)
= I + ∆ .
j
1≤j≤5
Define

 
−1
(cid:18)
(cid:104)
(cid:105)(cid:19)⊤
G(cid:98)e(π) :=

  1+τ χ    (cid:18) (cid:104)
1+ρ
(cid:105)(cid:19)
√1
d
vec R
tr
(1+ρ)b
tr   +πΩ
e+τλI
   .
 (cid:98)ave  √1 vec R
tr
(1+ρ)b
tr
I d⊗E
tr
 
d
(SI.4.22)
Then
G
e
= G(cid:98)e(π)+G(cid:98)e(π)(∆ 1+∆ 2+∆ 3+∆ 4+∆ 5). (SI.4.23)
(cid:124) (cid:123)(cid:122) (cid:125)
approximationerrors
Remark 5. We claim that G(cid:98)e is asymptotically equivalent to G e, in the sense of Definition 1. Given
(SI.4.23), proving this claim requires showing that, for j = 1,2,...,5,
(cid:16) (cid:17)
u⊤ G(cid:98)e(π)∆
j
v ≃ 0,
1 (cid:104) (cid:105)(cid:16) (cid:17)
√ 0 vec(A tr)⊤ G(cid:98)e(π)∆
j
v ≃ 0,
d
and
(cid:18) (cid:19)
1 (cid:104) (cid:105)
d2
tr G(cid:98)e(π)∆
j
\0·[I ⊗E tr] ≃ 0,
(cid:104) (cid:105)
for any deterministic and unit-norm vectors u,v and for A = R (1+ρ)b .
tr tr tr
We note the equivalent matrix G(cid:98)e(π) still involves one scalar χ
(cid:98)ave
that depends on the original
resolvent G (π). Next, we show that χ can be replaced by χ , the unique positive solution to
e (cid:98)ave π
(SI.4.7). To that end, we recall the characterization in (SI.4.21). Using the claim that G (π) and
e
G(cid:98)e(π) are asymptotically equivalent (in particular, in the sense of (SI.4.6)), we have
(cid:18) (cid:19)
1 (cid:104) (cid:105)
χ
(cid:98)ave
≃
d2
tr G(cid:98)e(π) \0·[I ⊗E tr] . (SI.4.25)
To compute the first term on the right-hand side of the above estimate, we directly invert the block
matrix G(cid:98)e(π) in (SI.4.22). Recall that Ω
e
is chosen in the forms of (SI.3.6) and (SI.3.13). It is then
straightforward to verify that
(cid:34) (cid:35)
c¯ −c¯q¯⊤
G(cid:98)e =
−c¯q¯ I ⊗F (χ )+c¯q¯q¯⊤
, (SI.4.26)
E (cid:98)ave
31where F (χ) is a matrix valued function such that
E
(cid:16) τ (cid:17)−1
F (χ) = E +πB+λτI , (SI.4.27)
E tr d+1
1+χ
(cid:18) (cid:19)
τ (cid:104) (cid:105)
q¯= √ vec R (1+ρ)b F (χ ) ,
tr tr E (cid:98)ave
(1+χ ) d
(cid:98)ave
and
τ(1+ρ) τ2 (cid:18) (cid:104) (cid:105) (cid:104) (cid:105)⊤(cid:19)
1/c¯= +λτ − tr R (1+ρ)b F (χ ) R (1+ρ)b .
1+χ (1+χ )2d tr tr E (cid:98)ave tr tr
(cid:98)ave (cid:98)ave
Using (SI.4.26), we can now write the equation (SI.4.25) as
1 (cid:0) (cid:1)
χ ≃ tr F (χ )E
(cid:98)ave E (cid:98)ave tr
d
c¯τ2 (cid:18) (cid:104) (cid:105) (cid:104) (cid:105)⊤(cid:19) (SI.4.28)
+ tr R (1+ρ)b F (χ )E F (χ ) R (1+ρ)b .
(1+χ )2d3 tr tr E (cid:98)ave tr E (cid:98)ave tr tr
(cid:98)ave
The second term on the right-hand side of (SI.4.28) is negligible. Indeed,
(cid:18) (cid:19)
(cid:104) (cid:105) (cid:104) (cid:105)⊤
tr R (1+ρ)b F (χ )E F (χ ) R (1+ρ)b
tr tr E (cid:98)ave tr E (cid:98)ave tr tr
≤(cid:13) (cid:13)F E(χ (cid:98)ave)E trF E(χ (cid:98)ave)(cid:13) (cid:13) op(∥R tr∥2 F+(1+ρ)2∥b tr∥2).
(cid:13) (cid:13)
Byconstruction,(cid:13)F E(χ (cid:98)ave)(cid:13)
op
≤ √(λτ)−1. Moreover, sincethetaskvectors{w i}
i∈[k]
areindependent
vectors sampled from Unif(Sd−1( d)), it is easy to verify that
√
∥E ∥ = O (1), ∥R ∥ = O ( d) and ∥b ∥ = O (1).
tr op ≺ tr F ≺ tr 2 ≺
(cid:13) (cid:13)
Finally, since c¯is an element of G(cid:98)e, we must have|c¯| ≤(cid:13) (cid:13)G(cid:98)e(cid:13)
(cid:13)
≤ (τλ)−1. Combining these estimates
op
gives us
c¯τ2 (cid:18) (cid:104) (cid:105) (cid:104) (cid:105)⊤(cid:19)
tr R (1+ρ)b F (χ )E F (χ ) R (1+ρ)b = O (d−2),
(1+χ )2d3 tr tr E (cid:98)ave tr E (cid:98)ave tr tr ≺
(cid:98)ave
and thus we can simplify (SI.4.28) as
(cid:20) (cid:21)
1 (cid:16) τ (cid:17)−1
χ ≃ tr E +πB+λτI E . (SI.4.29)
(cid:98)ave tr d tr
d 1+χ
(cid:98)ave
Observe that (SI.4.29) is a small perturbation of the self-consistent equation in (SI.4.7). By the
stability of the equation (SI.4.7), we then have
χ ≃ χ , (SI.4.30)
(cid:98)ave π
where χ is the unique positive solution to (SI.4.7).
π
32Recall the definitions of G e(π) and G(cid:98)e(π) in (SI.4.22) and (SI.4.8), respectively. By the standard
resolvent identity,
G(cid:98)e(π)−G e(π)
 
(cid:18)
(cid:104)
(cid:105)(cid:19)⊤
τ[χ −χ ] 
1+ρ √1 vec R
tr
(1+ρ)b
tr 
= [1+χ(cid:98)av ]e [1+χπ ]G(cid:98)e(π)
 (cid:18) (cid:104) (cid:105)(cid:19)
d  G e(π).
π (cid:98)ave  √1 vec R
tr
(1+ρ)b
tr
I d⊗E
tr

d
(SI.4.31)
(cid:13) (cid:13)
(cid:13) (cid:13)
By construction,(cid:13) (cid:13)G(cid:98)e(π)(cid:13)
(cid:13)
≤ 1/(τλ) and(cid:13)G e(π)(cid:13)
op
≤ 1/(τλ). Moreover,∥E tr∥
op
≺ 1 and
op
(cid:13) (cid:13)
(cid:13) 1 (cid:18) (cid:104) (cid:105)(cid:19)(cid:13)
(cid:13)√ vec R (1+ρ)b (cid:13) ≺ 1.
(cid:13) tr tr (cid:13)
(cid:13) d (cid:13)
It then follows from (SI.4.30) and (SI.4.31) that
(cid:13) (cid:13)
(cid:13) (cid:13)G(cid:98)e(π)−G e(π)(cid:13)
(cid:13)
≃ 0. (SI.4.32)
op
If G(cid:98)e(π) satisfies the equivalent conditions (SI.4.4), (SI.4.5) and (SI.4.6) (as claimed in our analysis
above), then the estimate in (SI.4.32) allows us to easily check that G (π) also satisfies (SI.4.4),
e
(SI.4.5) and (SI.4.6). Thus, we claim that G (π) is asymptotically equivalent to the extended
e
resolvent matrix G (π) in the sense of Definition 1.
e
SI.5 Asymptotic Limits of the Generalization Errors
In this section, we use the characterization in Result 3 to derive the asymptotic limits of the
generalization errors of associated with the set of parameters Γ∗ learned from ridge regression.
SI.5.1 Asymptotic Limits of the Linear and Quadratic Terms
From Corollary 1 and the discussions in Remark 2, characterizing the test error e(Γ∗) boils down to
(cid:16) (cid:17) (cid:16) (cid:17)
computing the linear term 1 tr Γ∗A⊤ and the quadratic term 1 tr Γ∗B (Γ∗)⊤ , where A
d test d test test
and B are the matrices defined in (SI.2.14) and (SI.2.15), respectively.
test
WeconsidertwodifferenttypesoftestdatadistributionsP : ICLandIDG.[SeeSection2.6in
test
the main text for details.] From (SI.2.11) and (SI.2.12), these two settings correspond to choosing
(cid:34) (cid:35)
(cid:104) (cid:105) (1+ρ +1)I
(ICL) : A = I 0 and B = α d . (SI.5.1)
test d test (1+ρ)2
and
(IDG) : A = A and B = E , (SI.5.2)
test tr test tr
respectively. In (SI.5.2), A and E are the matrices defined in (SI.4.2) and (SI.4.3).
tr tr
Result 4. Let Γ∗ be the set of parameters learned from the ridge regression problem in (9). Let
A ∈ Rd×(d+1) and B ∈ R(d+1)×(d+1) be two matrices constructed as in (SI.5.1) or (SI.5.2).
test test
We have
1 1 (cid:16) (cid:17)
tr(Γ∗A⊤ ) ≃ tr Γ∗ A⊤ , (SI.5.3)
d test d eq test
33and
(cid:18) (cid:19)
1 1 c (cid:104) (cid:105)
tr(Γ∗B (Γ∗)⊤) ≃ tr(Γ∗ B (Γ∗ )T)− e tr B (E +ξI)−1−ξ(E +ξI)−2 .
d test d eq test eq d test tr tr
(SI.5.4)
In the above displays, Γ∗ is an asymptotic equivalent of Γ∗, defined as
eq
(cid:104) (cid:105)
Γ∗ := R (1+ρ)b (E +ξI)−1, (SI.5.5)
eq tr tr tr
where ξ is the unique positive solution to the self-consistent equation
(cid:18) (cid:19)
1+ρ τλ
ξM +ξ − = 1−τ, (SI.5.6)
κ
α ξ
and M (·) is the function defined in (B.3). Moreover, the scalar c in (SI.5.4) is defined as
κ e
ρ+ν −ν2M (ν)−ξ(cid:2) 1−2νM (ν)−ν2M′ (ν)(cid:3)
κ κ κ
c = , (SI.5.7)
e 1−2ξM (ν)−ξ2M′ (ν)−τ
κ κ
where
1+ρ
ν := +ξ. (SI.5.8)
α
To derive the asymptotic characterizations (SI.5.3) and (SI.5.4) in Result 4, we first use block-
matrix inversion to rewrite G (π) in (SI.4.8) as
e
(cid:34) (cid:35)
c∗(π) −c∗(π)(q∗(π))⊤
G (π) = , (SI.5.9)
e −c∗(π)q∗(π) I ⊗F (χ )+c∗(π)q∗(π)(q∗(π))⊤
E π
where F (·) is the matrix-valued function defined in (SI.4.27), i.e.,
E
(cid:16) τ (cid:17)−1
F (χ ) = E +πB +λτI .
E π tr test d+1
1+χ
π
Moreover,
(cid:18) (cid:19)
τ (cid:104) (cid:105)
q∗(π) = √ vec R (1+ρ)b F (χ ) , (SI.5.10)
tr tr E π
(1+χ ) d
π
and
1 τ(1+ρ) τ2 (cid:18) (cid:104) (cid:105) (cid:104) (cid:105)⊤(cid:19)
= +λτ − tr R (1+ρ)b F (χ ) R (1+ρ)b . (SI.5.11)
c∗(π) 1+χ (1+χ )2d tr tr E π tr tr
π π
Observethatthereisaone-to-onecorrespondencebetweenthetermsin(SI.5.9)andthosein(SI.3.8).
To derive the asymptotic characterization given in (SI.5.3), we note that
1 −1 (cid:104) (cid:105)
tr(Γ∗A⊤ ) ≃ √ 0 vec(A )T G (0)e (SI.5.12)
d test c(0) d test e 1
c∗(0) 1 (cid:18) (cid:104) (cid:105) (cid:19)
= · tr R (1+ρ)b (E +λ(1+χ )I)−1A⊤ (SI.5.13)
c(0) d tr tr tr 0 test
(cid:18) (cid:19)
1 (cid:104) (cid:105)
≃ tr R (1+ρ)b (E +λ(1+χ )I)−1A⊤ . (SI.5.14)
d tr tr tr 0 test
34In the above display, (SI.5.12) follows from (SI.3.12) and the asymptotic equivalence between G (0)
e
andG (0). Theequalityin(SI.5.13)isdueto(SI.5.9)and(SI.5.10). Toreach(SI.5.14), wenotethat
e
c(0) = e⊤G (0)e and c∗(0) = e⊤G (0)e . Thus, c(0) ≃ c∗(0) due to the asymptotic equivalence
1 e 1 1 e 1
between G (0) and G (0). In Appendix B, we show that
e e
λ(1+χ ) ≃ ξ, (SI.5.15)
0
where ξ is the scalar defined in (SI.5.6). The asymptotic characterization given in (SI.5.3) then
follows from (SI.5.14) and from the definition of Γ∗ given in (SI.5.5).
eq
Next,weuse(SI.3.14)toderivetheasymptoticcharacterizationofthequadratictermin(SI.5.4).
Taking the derivative of (SI.5.11) gives us
(cid:18) (cid:19)(cid:12)
dd
π
c∗(1
π)
(cid:12) (cid:12)
(cid:12)
= d1 tr(Γ∗ eqB test(Γ∗ eq)⊤)
π=0
τχ′ (cid:18) 2 1 (cid:19)
− 0 1+ρ− tr(A (E +ξI)−1AT)+ tr(A (E +ξI)−1E (E +ξI)−1AT)
(1+χ )2 d tr tr tr d tr tr tr tr tr
0
1 τχ′ (cid:18) 1 ξ (cid:19)
= tr(Γ∗ B (Γ∗ )⊤)− 0 1+ρ− tr(Γ∗ AT)− tr(Γ∗ (Γ∗ )⊤) , (SI.5.16)
d eq test eq (1+χ )2 d eq tr d eq eq
0
where A is the matrix defined in (SI.4.2). In reaching the above expression, we have also used the
tr
estimate in (SI.5.15).
To further simplify our formula, we note that
(cid:18) (cid:19)
(cid:16)1+ρ (cid:17)
A = S E +ξI − +ξ I , (SI.5.17)
tr tr d+1 d+1
α
where S is a d×(d+1) matrix obtained by removing the last row of I . Using this identity, we
d+1
can rewrite the matrix Γ∗ in (SI.5.5) as
eq
(cid:18) (cid:19)
(cid:16)1+ρ (cid:17)
Γ∗ = S I − +ξ (E +ξI)−1 (SI.5.18)
eq α tr
(cid:104) (cid:105)
= I −νF (ν)−a∗(1+ρ)2νF (ν)b b⊤F (ν) a∗(1+ρ)νF (ν)b , (SI.5.19)
R R tr tr R R tr
where F (·) is the function defined in (B.1), and ν is the parameter given in (SI.5.8). The second
R
equality (SI.5.19) is obtained from the explicit formula for (E +ξI)−1 in (B.6).
tr
From (SI.5.17) and (SI.5.18), it is straightforward to check that
1 1
tr(Γ∗ AT) = 1−ν +ν2 tr(S(E +ξI)−1S⊤),
d eq tr d tr
and
(cid:20) (cid:21)
ξ 1 1
tr(Γ∗ (Γ∗ )⊤) = ξ 1−2ν tr(S(E +ξI)−1S⊤)+ν2 tr(S(E +ξI)−2S⊤ .
d eq eq d tr d tr
By using the asymptotic characterizations given in (B.15) and (B.16), we then have
1
tr(Γ∗ AT) ≃ 1−ν +ν2M (ν), (SI.5.20)
d eq tr κ
and
ξ (cid:104) (cid:105)
tr(Γ∗ (Γ∗ )⊤) ≃ ξ 1−2νM (ν)−ν2M′ (ν) . (SI.5.21)
d eq eq κ κ
35Substituting (SI.5.20), (SI.5.21), and (B.17) into (SI.5.16) yields
(cid:18) (cid:19)(cid:12) (cid:18) (cid:19)
dd
π
c∗(1
π)
(cid:12) (cid:12)
(cid:12)
≃ d1 tr(Γ∗ eqB test(Γ∗ eq)T)− c de tr B test(cid:104) (E tr+ξI)−1−ξ(E tr+ξI)−2(cid:105) ,
π=0
where c is the scalar defined in (SI.5.7). The asymptotic characterization of the quadratic term in
e
(SI.5.4) then follows from (SI.3.14) and the claim that
(cid:18) (cid:19)(cid:12) (cid:18) (cid:19)(cid:12)
d 1 (cid:12) d 1 (cid:12)
(cid:12) ≃ (cid:12) .
dπ c(π) (cid:12) dπ c∗(π) (cid:12)
π=0 π=0
SI.5.2 The Generalization Error of In-Context Learning
Result 5. Consider the test distribution P associated with the ICL task. We have
test
e(Γ∗) ≃ eICL(τ,α,κ,ρ,λ), (SI.5.22)
where
(cid:18) (cid:19)
eICL(τ,α,κ,ρ,λ) := 1+ρ +1 (cid:16) 1−2νM (ν)−ν2M′ (ν)−c (cid:2) M (ν)+ξM′ (ν)(cid:3)(cid:17)
α κ κ e κ κ
(cid:2) (cid:3)
−2 1−νM (ν) +1+ρ,
κ
and c is the constant given in (SI.5.7).
e
Remark 6. Recall the definition of the asymptotic equivalence notation “≃” introduced in Sec-
tion SI.1. The characterization given in (SI.5.22) implies that, as d → ∞, the generalization error
e(Γ∗) converges almost surely to the deterministic quantity eICL(τ,α,κ,ρ,λ).
To derive (SI.5.22), our starting point is the estimate
1 (cid:16) (cid:17) 2 (cid:16) (cid:17)
e(Γ∗) ≃ tr Γ∗B (Γ∗)⊤ − tr Γ∗A⊤ +1+ρ, (SI.5.23)
d test d test
which follows from Corollary 1 and the discussions in Remark 2. We consider the ICL task here,
and thus A and B are given in (SI.5.1). The asymptotic limits of the first two terms on the
test test
right-hand side of the above equation can be obtained by the characterizations given in Result 4.
Using (SI.5.3) and the expressions in (SI.5.19) and (SI.5.1), we have
1 1 (cid:16) (cid:17)
tr(Γ∗A⊤ ) ≃ tr Γ∗ A⊤
d test d eq test
(cid:13) (cid:13)2
= 1−
ν
trF
(ν)−a∗(1+ρ)2ν(cid:13)F R(ν)b tr(cid:13)
R
d d
≃ 1−νM (ν), (SI.5.24)
κ
where ν is the constant defined in (SI.5.8). To reach the last step, we have used the estimate given
in (B.15).
Next, we use (SI.5.4) to characterize the first term on the right-hand side of (SI.5.23). From the
formulas in (SI.5.19) and (SI.5.1), we can check that
(cid:18) (cid:19)
1 tr(cid:16) Γ∗ B (Γ∗ )⊤(cid:17) ≃ 1+ρ +1 1 tr(cid:0) I −νF(ν)(cid:1)2
d eq test eq α d
(cid:18) (cid:19)
1+ρ (cid:16) (cid:17)
≃ +1 1−2νM (ν)−ν2M′ (ν) , (SI.5.25)
α κ κ
36where the second step follows from (B.15) and (B.16). From (B.6),
(cid:18) (cid:19) (cid:18) (cid:19)
1 1+ρ 1 1+ρ
tr(B (E +ξI)−1) ≃ +1 trF (ν) ≃ +1 M (ν). (SI.5.26)
test tr R κ
d α d α
Similarly, we can check that
(cid:18) (cid:19) (cid:18) (cid:19)
1 1+ρ 1 1+ρ
tr(B (E +ξI)−2) ≃ +1 trF2(ν) ≃ − +1 M′ (ν). (SI.5.27)
d test tr α d R α κ
Substituting (SI.5.25), (SI.5.26), and (SI.5.27) into (SI.5.4) gives us
(cid:18) (cid:19)
1 tr(Γ∗B(Γ∗)⊤) ≃ 1+ρ +1 (cid:16) 1−2νM (ν)−ν2M′ (ν)−c (cid:2) M (ν)+ξM′ (ν)(cid:3)(cid:17) , (SI.5.28)
d α κ κ e κ κ
where c is the constant given in (SI.5.7). Combining (SI.5.24), (SI.5.28), and (SI.5.23), we are
e
done.
Inwhatfollows, wefurthersimplifythecharacterizationsinResult5byconsideringtheridgeless
limit, i.e., when λ → 0+.
Result 6. Let
q∗ := 1+ρ , m∗ := M (cid:0) q∗(cid:1) , and µ∗ := q∗M (q∗), (SI.5.29)
α κ κ/τ
where M (x) is the function defined in (B.3). Then
κ
eICL := lim eICL(τ,α,κ,ρ,λ)
ridgeless
λ→0+

τ(1+q∗) (cid:2) 1−τ(1−µ∗)2+µ∗(ρ/q∗−1)(cid:3) −2τ(1−µ∗)+(1+ρ) τ < 1
1−τ
= (cid:16) (cid:17) ,
(q∗+1) 1−2q∗m∗−(q∗)2M′ (q∗)+ (ρ+q∗−(q∗)2m∗)m∗ −2(1−q∗m∗)+(1+ρ) τ > 1
 κ τ−1
(SI.5.30)
where M′ (·) denotes the derivative of M (x) with respect to x.
κ κ
We start with the case of τ < 1. Examining the self-consistent equation in (SI.5.6), we can see
that the parameter ξ tends to a nonzero constant, denoted by ξ∗, as λ → 0+. It follows that the
original equation in (SI.5.6) reduces to
(cid:18) (cid:19)
1+ρ
ξ∗M +ξ∗ = 1−τ. (SI.5.31)
κ
α
Introduce a change of variables
(1−τ)(1+ρ)
µ∗ := .
ατξ∗
By combining (SI.5.31) and the characterization in (B.4), we can directly solve for µ and get
µ∗ = q∗M (q∗) as given in (SI.5.29). The characterization in (SI.5.30) (for the case of τ < 1)
κ/τ
then directly follows from (SI.5.24), (SI.5.28), and (4) after some lengthy calculations.
Next, we consider the case of τ > 1. It is straightforward to verify from (SI.5.6) that
τ
ξ = λ+O(λ2).
τ −1
Thus, when τ > 1, ξ → 0 as λ → 0+. It follows that
(cid:18) (cid:19)
1+ρ
lim ν = lim +ξ = q∗ and lim M (ν) = m∗.
κ
λ→0+ λ→0+ α λ→0+
Substituting these estimates into (SI.5.24), (SI.5.28), and (4), we then reach the characterizations
in (SI.5.30) for the case of τ > 1.
37SI.5.3 The Generalization Error of In-Distribution Generalization
In what follows, we derive the asymptotic limit of the generalization error for the IDG task.
Result 7. Consider the test distribution P associated with the IDG task. We have
test
ρ+ν −ν2M (ν)−ξ(cid:2) 1−2νM (ν)−ν2M′ (ν)(cid:3)
e(Γ∗) ≃ eIDG(τ,α,κ,ρ,λ) := τ κ κ κ , (SI.5.32)
(cid:2) (cid:3)
τ − 1−2ξM (ν)−ξ2M′ (ν)
κ κ
where ξ the unique positive solution to the self-consistent equation (SI.5.6) and ν is the constant
given in (SI.5.8).
Similar to our derivation of Result 5, we only need to use (SI.5.3) and (SI.5.4) to characterize
the asymptotic limits of the first and second terms on the right-hand side of (SI.5.23). Note that,
for the IDG task, A = A . It follows from (SI.5.3) and (SI.5.20) that
test tr
1
tr(Γ∗A⊤ ) ≃ 1−ν +ν2M (ν). (SI.5.33)
d test κ
Similarly, since B = E , we can verify from (SI.5.5) that
test tr
1 (cid:16) (cid:17) 1 ξ
tr Γ∗ B (Γ∗ )⊤ = tr(Γ∗ A⊤)− tr(Γ∗ (Γ∗ )⊤) (SI.5.34)
d eq test eq d eq tr d eq eq
(cid:104) (cid:105)
≃ 1−ν +ν2M (ν)−ξ 1−2νM (ν)−ν2M′ (ν) ,
κ κ κ
where the second step follows from (SI.5.20) and (SI.5.21). Moreover,
(cid:18) (cid:19)
1 (cid:104) (cid:105)
tr B (E +ξI)−1−ξ(E +ξI)−2 = 1−2ξM (ν)−ξ2M′ (ν). (SI.5.35)
d test tr tr κ κ
Substituting (SI.5.34) and (SI.5.35) into (SI.5.4), we have
1
tr(Γ∗B(Γ∗)⊤)
d
ρ+ν −ν2M (ν)−ξ(cid:2) 1−2νM (ν)−ν2M′ (ν)(cid:3)
≃ τ κ κ κ +2(1−ν +ν2M (ν))−(1+ρ).
(cid:2) (cid:3) κ
τ − 1−2ξM (ν)−ξ2M′ (ν)
κ κ
The final result in (SI.5.32) then follows from combining the above expression with (SI.5.33) and
(SI.5.23).
Finally, we derive the ridgeless limit of the characterization given in Result 7.
Result 8. Let q∗, m∗, and µ∗ be the scalars defined in (SI.5.29). We have
eIDG := lim eIDG(τ,α,κ,ρ,λ)
ridgeless
λ→0+
 (cid:16) (cid:17)
 τ ρ+q∗−2q∗(1−τ)(q∗/ξ∗+1) + τµ∗(q∗+ξ∗)2 τ < 1
= 1−τ 1−p∗(1−τ) q∗ , (SI.5.36)
 τ [ρ+q∗(1−q∗m∗)] τ > 1
τ−1
where ξ∗ = (1−τ)q∗ and p∗ = (cid:0) 1−κ(cid:0)κξ∗ +1(cid:1)−2(cid:1)−1 .
τµ∗ 1−τ
The derivation of this result closely follows that of Result 6. We analyze the cases of τ < 1 and
τ > 1 separately. For τ < 1, the equation in (SI.5.6) simplifies to (SI.5.31) as λ → 0+. For τ > 1,
ξ approaches zero as λ → 0+. Substituting these estimates into (SI.5.32) then yields (SI.5.36) after
some detailed calculations.
38A Equivalent Statistical Representations
In this appendix, we present an equivalent (but simplified) statistical model for the regression
vector H defined in (7). This statistically-equivalent model will simplify the moment calculations
Z
in Section SI.2 and the random matrix analysis in Section SI.4.
√
Lemma 3. Let w be a given task vector with∥w∥ = d. Meanwhile, let a ∼ N(0,1), s ∼ N(0,1),
ϵ ∼ N(0,ρ) be three scalar normal random variables, and q ∼ N(0,I ), g ∼ N(0,I ), u ∼
ℓ−1 d−1
N(0,I ), and v ∼ N(0,ρI ) be isotropic normal random vectors. Moreover, w and all of the
d−1 ϵ ℓ
above random variables are mutually independent. We have the following equivalent statistical rep-
resentation of the pair (H ,y ):
Z ℓ+1
(cid:34) (cid:35)
(d) s (cid:104) √ √ √ (cid:105)
H = (d/ℓ)M h⊤M , (a/ d+θ )2/ d+θ2/ d , (A.1)
Z w u w ϵ q
and
(d)
y = s+ϵ. (A.2)
ℓ+1
In the above displays, M denotes a symmetric and orthonormal matrix such that
w
w
(M )e = , (A.3)
w 1
∥w∥
where e denotes the first natural basis vector in Rd; h ∈ Rd is a vector defined as
1
 
θ √ϵa + a2 +θ2
h :=  √d d q √ ; (A.4)
(cid:2)
(θ +a/
d)2+θ2(cid:3)1/2
g/ d
ϵ q
and θ , θ are scalars such that
ϵ q
√ √
θ =∥v ∥/ d and θ =∥q∥/ d.
ϵ ϵ q
(d)
Remark 7. For two random variables A and B, the notation A = B indicates that A and B have
identical probability distributions. Note that A and B can be either scalars [as in the case of (A.2)],
or matrices of matching dimensions [as in the case of (A.1)].
Remark 8. A concrete construction of the symmetric and orthonormal matrix M satisfying (A.3)
w
can be based on the Householder transformation [54–56].
Proof. Recallthat the datavectorx is independent ofthe task vectorw. Then, by therotational
ℓ+1
symmetry of the isotropic normal distribution, we can rewrite
(cid:34) (cid:35)
(d) 1 s
x = √ M , (A.5)
ℓ+1 d w u
where s ∼ N(0,1) and u ∼ N(0,I ) are two independent normal random variables (vectors),
d−1
and M is the symmetric orthonormal matrix specified in (A.3). Note that y = x⊤ w+ϵ, with
w ℓ+1 ℓ+1
ϵ ∼ N(0,ρ) denoting the noise. The representation in (A.2) then follows immediately from (A.5)
and the identity in (A.3).
39To show (A.1), we first reparameterize the d×ℓ Gaussian data matrix X as
(cid:34) (cid:35)
a q⊤ √
X = M M / d. (A.6)
w p U vϵ
In the above display, a ∼ N(0,1), p ∼ N(0,I ), q ∼ N(0,I ); U ∈ R(d−1)×(ℓ−1) is a matrix
d−1 ℓ−1
with iid standard normal entries; and M is a symmetric orthonormal matrix such that
vϵ
v
ϵ
M e = , (A.7)
vϵ 1
∥v ∥
ϵ
where e denotes the first natural basis vector in Rℓ. Since the data matrix X, the task vector w,
1
and the noise vector v are mutually independent, it is straightforward to verify via the rotational
ϵ
symmetry of the isotropic normal distribution that both sides of (A.6) have identical probability
distributions. Using this new representation, we have
(cid:34) (cid:35)
a
Xv = θ M .
ϵ ϵ w p
Meanwhile,
(cid:34) (cid:35)
a
X⊤w = M , (A.8)
vϵ q
and thus
(cid:34) (cid:35)
1 a2+∥q∥2
XX⊤w = √ M . (A.9)
w
d ap+Uq
Combining (A.8) and (A.9) yields
Xy = XX⊤w+Xv
ϵ
(cid:34) √ √ (cid:35)
θ a+a2/ d+θ2 d
= M ϵ √ q √ .
w
(θ +a/ d)p+Uq/ d
ϵ
√
(d)
Observe that Uq/ d = θ p′, where p′ ∼ N(0,I ) is a normal random variable independent of
q d−1√
everything else. Using this reparametrization for Uq/ d and the fact that p,p′ are two independent
Gaussian vectors, we can conclude that
1 (d)
√ Xy = M h, (A.10)
w
d
where h is the random vector defined in (A.4).
Lastly, we consider the term y⊤y in (7). Since y = X⊤w+v ,
ϵ
(cid:13) (cid:13)2
y⊤y =(cid:13)X⊤w+v (cid:13)
(cid:13) ϵ(cid:13)
(cid:13) √ (cid:13)2
=(cid:13)X⊤w+θ dM e (cid:13)
(cid:13) ϵ vϵ 1(cid:13)
√
= (a+θ d)2+θ2d, (A.11)
ϵ q
where the second equality follows from (A.7) and to reach the last equality we have used the
representation in (A.8). To show (A.1), we recall the definition of H in (7). Substituting (A.5),
Z
(A.10) and (A.11) into (7), we are done.
40B The Stieltjes Transforms of Wishart Ensembles
Inthisappendix,wefirstrecallseveralstandardresultsrelatedtotheStieltjestransformsofWishart
ensembles. In our problem, we assume that there are k unique task vectors {w } in the training
i i∈[k]
set. Moreover, these task vectors {w } are independently sampled from the uniform distribution
√ i √i∈[k]
on the sphere Sd−1( d) with radius d. Let
F (ν) := (R +νI )−1, (B.1)
R tr d
whereR isthesamplecovariancematrixofthetaskvectorsasdefinedin(SI.4.1)andν isapositive
tr
scalar.
Note that the distribution of R is asymptotically equivalent to that of a Wishart ensemble. By
tr
standard random matrix results on the Stieltjes transforms of Wishart ensembles (see, e.g., [35]),
we have
1
trF (ν) ≃ M (ν) (B.2)
R κ
d
as d,k → ∞ with k/d = κ. Here,
2
M (ν) := . (B.3)
κ
ν
+1−1/κ+(cid:2)
(ν
+1−1/κ)2+4ν/κ(cid:3)1/2
is the solution to the self-consistent equation
1 1
= +ν. (B.4)
M (ν) 1+M (ν)/κ
κ κ
Moreover,
1 M2(ν)
trF2(ν) ≃ −M′ (ν) = κ .
d κ 1− κM2 κ(ν)
[κ+Mκ(ν)]2
For the remainder of this appendix, we will further explore the self-consistent equation given by
(SI.4.7). We will show that the solution χ and its derivative d χ , at π = 0, can be characterized
π dπ π
bythefunctionM (ν)in(B.3). Tostart, notethatatπ = 0, theequationin(SI.4.7)canbewritten
κ
as
τχ λ(1+χ )
0 = (1+1/d)− 0 tr(E +λ(1+χ )I)−1. (B.5)
tr 0
1+χ d
0
Recall the definition of E given in (SI.4.3). It is straightforward to verify that
tr
(cid:34) (cid:35)
F (ν )+a∗(1+ρ)2F (ν )b b⊤F (ν ) −a∗(1+ρ)F (ν )b
(E +λ(1+χ )I )−1 = R 0 R 0 tr tr R 0 R 0 tr , (B.6)
tr 0 d+1 −a∗(1+ρ)b⊤F (ν ) a∗
tr R 0
where F (·) is the function defined in (B.1),
R
1+ρ
ν = +λ(1+χ ) (B.7)
0 0
α
and
1
= (1+ρ)2+λ(1+χ )−(1+ρ)2b⊤F (ν )b . (B.8)
a∗ 0 tr R 0 tr
From (B.6), the equation (B.5) becomes
τχ 0
= (1+1/d)−
λ(1+χ 0)
trF R(ν
0)−(1+ρ)2a∗λ(1+χ 0)(cid:13)
(cid:13)F R(ν 0)b
tr(cid:13) (cid:13)2
. (B.9)
1+χ d d
0
41By the construction of F (ν ) and b , we can verify that
R 0 tr
b⊤ trF R(ν 0)b
tr
≤ 1 and (cid:13) (cid:13)F R(ν 0)b tr(cid:13) (cid:13)2 ≤ ν1 ≤ 1+α ρ. (B.10)
0
Substituting the first inequality above into (B.8) gives us
a∗λ(1+χ ) ≤ 1.
0
Combiningthisestimatewiththesecondinequalityin(B.10), wecanconcludethatthelasttermon
theright-handsideof (B.9)isnegligibleasd → ∞. Moreover,usingtheasymptoticcharacterization
given in (B.2), the equation (B.9) leads to
τχ
0
≃ 1−λ(1+χ )M (ν ). (B.11)
0 κ 0
1+χ
0
Introducing a change of variables
ξ = λ(1+χ ),
0 0
and also recalling the definition of ν in (B.7), we can further transform (B.11) to
0
(cid:18) (cid:19)
1+ρ τλ
ξ M +ξ − ≃ 1−τ.
0 κ 0
α ξ
0
Observethattheaboveisidenticaltotheequationin(SI.5.6),exceptforasmallerrortermcaptured
by ≃. By the stability of (SI.5.6), we can then conclude that
ξ ≃ ξ, (B.12)
0
thus verifying (SI.5.15).
Next,wecomputeχ′,thederivativeofχ (withrespecttoπ)evaluatedatπ = 0. Differentiating
0 π
(SI.4.7) give us
(cid:34) (cid:35)
1 (cid:16) (1+χ )2 (cid:17)
τχ′ = tr (E +ξ I)−1 χ′E − 0 B (E +ξ I)−1E . (B.13)
0 d tr 0 0 tr τ test tr 0 tr
Thus,
τχ′ 1 tr(cid:0) B [(E +ξI)−1−ξ(E +ξI)−2](cid:1)
0 ≃ d test tr tr , (B.14)
(1+χ )2 1−2ξtr(E +ξI)−1/d+ξ2tr(E +ξI)−2/d−τ
0 tr tr
where we have used (B.12) to replace ξ in (B.13) by ξ, with the latter being the solution to the self-
0
consistent equation in (SI.5.6). Using the decomposition in (B.6) and following similar arguments
that allowed us to simplify (B.9) to (B.11), we can check that
(cid:18) (cid:19) (cid:18) (cid:19)
1 1 1 1+ρ 1+ρ
tr(E +ξI)−1 ≃ trS(E +ξI)−1S⊤ ≃ trF +ξ ≃ M +ξ , (B.15)
tr tr κ
d d d α α
and
(cid:18) (cid:19) (cid:18) (cid:19)
1 1 1 1+ρ 1+ρ
tr(E +ξI)−2 ≃ trS(E +ξI)−2S⊤ ≃ trF2 +ξ ≃ −M′ +ξ , (B.16)
d tr d tr d α κ α
where S is a d×(d+1) matrix obtained by removing the last row of I , and M (·) is the function
d+1 κ
defined in (B.3). Substituting (B.15) and (B.16) into (B.14) yields
τχ′ 1 tr(cid:0) B [(E +ξI)−1−ξ(E +ξI)−2](cid:1)
0 ≃ d test tr tr . (B.17)
(cid:16) (cid:17) (cid:16) (cid:17)
(1+χ 0)2 1−2ξM 1+ρ +ξ −ξ2M′ 1+ρ +ξ −τ
κ α κ α
42