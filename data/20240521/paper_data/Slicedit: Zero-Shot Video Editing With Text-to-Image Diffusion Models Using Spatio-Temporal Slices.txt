Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models Using
Spatio-Temporal Slices
NathanielCohen*12 VladimirKulikov*2 MatanKleiner*2 InbarHuberman-Spiegelglas2 TomerMichaeli2
A man is jumping A man is surfing
t
u
p
n
I
A humanoid robotis jumping Iron Man is surfing
d
e
t
i d A shiny silver robotis jumping Spider Man is surfing
E
Figure1: Slicedit. Wepresentazero-shotmethodfortext-basedvideoeditingbasedonapretrainedtext-to-imagediffusion
model. Ourmethodcaneditchallenginglongvideoswithcomplexnonrigidmotionandocclusions,whilepreservingthe
regionsnotspecifiedinthetextprompt(e.g.changingonlythepersonintoarobotwithoutaffectingthebackground). See
ourwebsiteforvideoresults.
Abstract similarcharacteristicstonaturalimages.Thus,the
sameT2Idiffusionmodelthatisnormallyused
Text-to-image (T2I) diffusion models achieve
onlyasaprioronvideoframes,canalsoserveas
state-of-the-artresultsinimagesynthesisandedit-
astrongpriorforenhancingtemporalconsistency
ing. However,leveragingsuchpretrainedmodels
byapplyingitonspatiotemporalslices. Basedon
forvideoeditingisconsideredamajorchallenge.
this observation, we present Slicedit, a method
Manyexistingworksattempttoenforcetemporal
for text-based video editing that utilizes a pre-
consistencyintheeditedvideothroughexplicit
trainedT2Idiffusionmodeltoprocessbothspatial
correspondencemechanisms,eitherinpixelspace
andspatiotemporalslices. Ourmethodgenerates
orbetweendeepfeatures. Thesemethods,how-
videosthatretainthestructureandmotionofthe
ever, struggle with strong nonrigid motion. In
original video while adhering to the target text.
thispaper,weintroduceafundamentallydiffer-
Throughextensiveexperiments,wedemonstrate
entapproach,whichisbasedontheobservation
Slicedit‚Äôsabilitytoeditawiderangeofreal-world
thatspatiotemporalslicesofnaturalvideosexhibit
videos,confirmingitsclearadvantagescompared
toexistingcompetingmethods.
*Equal contribution 1Mines Paris ‚Äì PSL Research Uni-
versity, Paris, France 2Technion ‚Äì Israel Institute of Technol-
ogy, Haifa, Israel. Correspondence to: Vladimir Kulikov
<vladimir.k@campus.technion.ac.il>.
1.Introduction
Proceedings of the 41st International Conference on Machine
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by Text-to-image(T2I)diffusionmodelshavereachedremark-
theauthor(s). ablecapabilities,enablinghigh-qualityimagesynthesisthat
1
4202
yaM
02
]VC.sc[
1v11221.5042:viXraSlicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
canbecontrolledbyhighlydescriptivetextprompts(Rom- denoised frames. This multi-axis denoising is key to our
bach et al., 2022; Saharia et al., 2022; Dai et al., 2023; method‚Äôsabilitytoalteronlyspecifiedregionsinavideo
Betker et al., 2023). These capabilities have been shown whilekeepingtheremainingcontentfixed.
to enable text-based editing of real images (Hertz et al.,
Asweshow,ourmethodcaneditvideoswhilemaintaining
2023;Kawaretal.,2023;Brooksetal.,2023;Tumanyan
temporal consistency, even in cases with strong nonrigid
etal.,2023;Huberman-Spiegelglasetal.,2023;Zhangetal.,
motionorocclusions. AsillustratedinFig.1,ourmethod
2023). However,usingT2Imodelsinazero-shotmanner
preservesvideostructureandlayoutinuneditedregions,e.g.
forvideoeditingisstillconsideredanopenchallenge,es-
editingonlythemanwhilekeepinghissurroundingsunal-
peciallywhenitcomestolongvideoswithstrongnonrigid
tered. WeevaluateSliceditonreal-worldvideos,bothshort
motionandocclusions.
andlong,andacrossadiverserangeofmotiontypes. Com-
ThenaiveapproachofusingT2Imodelsforeditingavideo paredwithstate-of-the-artzero-shotvideoeditingmethods,
frame-by-frameleadstotemporalinconsistencies(Wuetal., ourresultsdemonstrateaclearadvantageforutilizingthe
2023a),bothovershortperiodsoftime(e.g.flickering)and untappedpotentialofspatiotemporalslices.
overlongdurations(e.g.driftinappearance). Tomitigate
temporal inconsistencies, previous and concurrent video
2.Relatedwork
editingmethodsuseextendedattention,whichenablesedit-
ingmultipleframesjointlyandhenceimprovestemporal Tuning T2I models for video editing. A common ap-
consistency(Wuetal.,2023a;QIetal.,2023;Ceylanetal., proachforharnessingpretrainedT2Imodelsforvideoedit-
2023;Khachatryanetal.,2023). However,relyingsolely ing,istointroducearchitecturalchangestothemodeland
onextendedattentionoftenresultsininconsistenteditingof thentuneitonvideos(Wuetal.,2023a;Liuetal.,2023;
texturesandfinedetails. Otherconcurrentmethodstackle Wangetal.,2023;Xingetal.,2023;Esseretal.,2023;Feng
temporal inconsistency by using feature correspondence etal.,2023). Thearchitecturalmodificationsareusuallyin
acrossframes(Yangetal.,2023;Geyeretal.,2024). These theformofinsertionof(spatio)temporallayers,likedepth-
methodstendtofailinscenarioswherethecorrespondences wise3Dconvolution,extendedattentionacrossframes,and
are weak, e.g. in long videos or in videos with fast and 1Dself-attentionacrosstime. Thosemodificationsenhance
complexnonrigidmotion. themodel‚Äôsabilitytomaintaintemporalconsistency. The
tuning process usually starts from the weights of the pre-
Hereweintroduceanewapproach,whichwecoinSlicedit1,
trainedT2Imodel,whereinsomecasesonlytheweights
to edit videos in a zero-shot manner. Similarly to recent
ofthenewlayersaretuned,andusesalargedatasetoftext-
andconcurrentworks, ourmethod‚Äúinflates‚Äùapretrained
videopairs. Thisresultsinacomputationallyintensiveand
T2Ilatentdiffusionmodelintoamodelsuitableforvideo
timeconsumingtrainingprocess. Alternatively,thetuning
editing, doingsoinazero-shotmanner(i.e.withoutfine-
canbedoneonasinglevideo(theonebeingedited), yet
tuningthemodel). Toeditarealvideo,weusetheDDPM
thisapproachistimeconsumingatinference,aseachvideo
inversion method of Huberman-Spiegelglas et al. (2023)
requiresitsowntuningprocess.
withourinflateddenoisertoobtainthesequenceofnoise
vectors that causes the diffusion process to generate that
video. Wethenre-generatethevideowiththetextprompt
T2Imodelsforzero-shotvideoediting. Somemethods
provided by the user, while fixing the noise vectors and
use T2I models for video editing without any parameter
injectingfeaturesobtainedfromtheinversionofthesource
tuning, in a zero-shot manner. Our work falls under this
video,aspreviouslyshowntobeusefulbyQIetal.(2023).
category. Similarly to methods that require fine-tuning,
OurinflateddenoiserdeviatesfromthepretrainedT2Imodel mostzero-shotmethodsadjusttheattentionmodulesofthe
in two fundamental aspects: (i) Similarly to concurrent model to capture associations across time (Khachatryan
works,theself-attentionmodulesareconvertedtoextended etal.,2023;QIetal.,2023;Ceylanetal.,2023;Yangetal.,
attention,capturingthedynamicsbetweenframes. (ii)More 2023; Geyer et al., 2024; Cong et al., 2024; Zhang et al.,
importantly, in addition to denoising frames, we enforce 2024). Relying on extended attention for temporal con-
temporal consistency by also processing spatiotemporal sistency, as done in (Khachatryan et al., 2023; QI et al.,
slices of the video volume. Specifically, we observe that 2023), results in inconsistent editing of textures and fine
spatiotemporalslicessharesimilaritieswithnaturalimages details. Thus, some methods (Ceylan et al., 2023; Yang
(Fig.2),promptingustoleveragethesamepretrainedT2I etal.,2023;Zhangetal.,2024)alsouseothermechanisms,
denoiserfortheirdenoising. Thus,weapplytheT2Imodel like conditioning on depth or edge maps, usually using a
on x t or y t slices within the (x,y,t) space-time pretrainedControlNet(Zhangetal.,2023). Thisimproves
volum‚àí e(leftpan‚àí eofFig.2)andmergetheresultwiththe thequalityandtemporalconsistencyoftheeditedvideo,but
introducesartifactswhenthecontrolsignalispartialorinac-
1Sliceditcanbepronounced‚Äúslice-edit‚Äùor‚Äúsliced-it‚Äù.
curate. Anothermethod,usedbyYangetal.(2023);Geyer
2Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
ùë¶‚àíùë°slices extracted from Images sampled from
natural videos Stable Diffusion
unconditional
samples
ùë¶ ‚Äúpainting of geological
rock folding in
sedimentary layers‚Äù
‚Äúmotion blur‚Äù
ùë• ùë°
Figure2: Diffusionmodelasaspatiotemporalsliceprior. Theleftpaneshowsthe(x,y,t)space-timevolumeofavideo.
Themiddlepaneshowsy tslicesofnaturalvideos. TherightpaneshowsimagesgeneratedbyStableDiffusionusing
‚àí
severaltext-prompts. Thegeneratedimageshavesimilarcharacteristicstospatiotemporalslicesofnaturalvideos. This
suggeststhatapretrainedtext-to-imagemodelcanserveasagoodpriorforspatiotemporalvideoslices.
etal.(2024),istoeditonlykeyframesandpropagatetheir canbeconditionedonvarioustypesofsignals,usuallyvia
features to all other video frames. This method strongly a cross-attention module. Here we use models that are
enforcestemporalconsistency,however,itbreaksunderfast conditionedonatextprompt,p,yieldingœµ (,p). Thedif-
Œ∏
¬∑
motion,severeocclusions,andwhenthecorrespondences fusionprocesscanbeappliedinpixelspaceorinthelatent
weaken over time. Our method also uses extended atten- spaceofsomeencoder,whichresultsinalatentdiffusion
tion,howeverwealsousespatiotemporalslicesasawayof model(Rombachetal.,2022). Inthelattercase,thegener-
enforcingtemporalconsistency. atedsamplesaredecodedbacktopixelspacebyadecoder.
ApretrainedDDPMcanbeusednotonlyforgeneratingsyn-
Spatiotemporal slices of the space-time volume. The theticimages,butalsoforeditingrealimages. Acommon
x tory tslicesofspace-timevideovolumeshavebeen approachtodosoinvolvesaninversionprocess,whichex-
‚àí ‚àí
usedforvariousdifferenttasks, includingenergymodels tractsnoisevectorsthatgeneratethegivenimagewhenused
ofmotionperception(Adelson&Bergen,1985),epipolar inthesamplingprocess. Manymethodsfocusoninversion
plane image analysis (Bolles et al., 1987), motion anal- for the DDIM sampling scheme. Huberman-Spiegelglas
ysis (Ngo et al., 2003), video mosaics (Rav-Acha et al., etal.(2023)introducedaneditingapproachthatisbased
2005), video quality assessment (Vu & Chandler, 2014) oninversionoftheDDPMscheme. Thismethodhasbeen
andtemporalsuperresolution(Zuckermanetal.,2020). In illustrated to more strongly preserve the structure of the
particular,Rav-Achaetal.(2005)usedthespatiotemporal sourceimagethanDDIMinversion,andthustoleadtobet-
slicesofavideotogeneratenewvideos, leveragingtheir tereditingresults. ThemethodofHuberman-Spiegelglas
similaritytonaturalimages. Inaddition,Zuckermanetal. et al. (2023) starts by generating increasingly noisy ver-
(2020)observedthatsmallpatchesinnaturalvideosaresim- sionsof theinputimage, one foreach diffusiontimestep.
ilaracrossboththespatialandspatiotemporaldimensions Subsequently, thenoisevectorforeachtimestepœÑ ofthe
ofthespace-timevolume. Ourmethodalsoleveragesthe generation process is extracted from x and œµ (x ,p)
œÑ‚àí1 Œ∏ œÑ
similaritybetweenspatiotemporalslicesandnaturalimages, (seeApp.H.1formoredetails). Duringtheinversionpro-
inthatweuseanimagepriortoregularizespatiotemporal cess, the T2I denoiser can be optionally conditioned on
slicessoastoenforcetemporalconsistency. a source prompt, p , describing the input image. After
src
thenoisevectorshavebeenextracted,theyareusedinthe
3.Preliminaries DDPMgenerationprocess,whileconditioningthedenoiser
onatargetprompt,p ,describingthedesiredoutput. The
tar
Denoising Diffusion Probabilistic Models (DDPMs) (Ho editedimagepreservesfidelitytotheoriginalimagewhile
et al., 2020) are a class of generative models that aim to adheringtothenewprompt.
approximate a data distribution through a progressive de-
noisingprocess. Generatingasamplex fromthosemodels
0 4.Method
startsbyrandomlydrawingGaussiannoise,x (0,I),
T
‚àºN
andrecursivelydenoisingitinT denoisingsteps. Thede- Ourmethodrequiresthefollowinginputs: avideovolume
noiserœµ Œ∏( ¬∑)isusuallyaU-Net(Ronnebergeretal.,2015), I 0,asourcepromptp
src
describingthevideo,andatarget
trainedtopredictthenoiseateachtimestep. Thisdenoiser
3Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
Theglobalkey-framefacilitatecollaborativeeditingacross
Frames theentirevideo,whilethelocalkey-frameshelppreserve
1.0
Slices temporalconsistencyamongcloselysituatedframes. The
PermutedFrames newdenoiser,denotedœµEA(,p),doesnotrequireanyfurther
Œ∏ ¬∑
0.8 training,similarlytopreviousworks(QIetal.,2023;Ceylan
etal.,2023;Geyeretal.,2024). Moredetailscanbefound
inApp.FandanillustartioncanbefoundinFig.S8.
0.6
Spatiotemporalslices. WeobservethatStableDiffusion
canproduceimagesthatsharesimilaritieswithspatiotem-
0.4
poralslicesofrealvideos. Thisobservationisillustrated
inFig.2,wherey tplanesfromrandomlychosennatu-
‚àí
0.2 ralvideosarepresentedalongsideimagesgeneratedusing
Stable Diffusion. This suggests that spatiotemporal slice
imagesexistwithinthegenerativemanifoldofStableDif-
0.0 200 400 600 800 1000 fusion so that the same pretrained T2I denoiser, œµ Œ∏(,p),
¬∑
Noise Level can effectively function as a denoiser for spatiotemporal
slices. ThisobservationissupportedbyFig.3,whichshows
thattheMSEachievedbythepretrainedStableDiffusion
Figure3: Applyingapretrainedimagedenoisertospa-
denoiserwhenappliedtospatiotemporalslices(yellow)is
tiotemporalslices. TheplotshowstheMSEobtainedwhen
comparableto(andevenlowerthan)theMSEitachieves
applying a pretrained Stable Diffusion denoiser to noisy
for video frames (blue). For comparison, when applying
videoframes, spatiotemporalslicesandpermutedframes
thedenoiseronout-of-distributiondatalikerandomlyper-
(allinlatentspace). TheMSEforspatiotemporalslicesis
mutedframes,theMSEismuchhigher(green). Formore
comparableto(evenlowerthan)theMSEforframes,and
details see App. G. With this observation, we apply the
botharelowerthantheMSEforpermutedframes,which
denoiser œµ (,p) separately on each y t slice, utilizing
areout-of-distributionforthedenoiser. Œ∏
¬∑ ‚àí
anemptypromptpforconditioning(makingthedenoiser
unconditional). This choice aligns with the observations
demonstratedinFig.2andissupportedbyanablationstudy
promptp describingthedesirededitedvideo. Thegoalis
tar
providedinApp.D.Werefertothisspatiotemporaldenoiser
togenerateaneditedvideoJ thatadherestop whilepre-
0 tar
asœµS(,p). Notethattheoriginaldenoiserœµ (,p)operates
servingtheoriginalmotionandlayoutofI 0. Asdepictedin Œ∏ ¬∑ Œ∏ ¬∑
overrepresentationsofdimension(64,64,4),implyingthat
Fig.4,theaxesofthevideospace-timevolumearedenoted
œµS(,p)canonlybeappliedtovideoswith64frames. We
by(x,y,t),wherex yplanescorrespondtovideoframes Œ∏ ¬∑
‚àí explainhowtohandleshorterandlongervideosinSec.5.
andy tplanesarereferredtoasspatiotemporalslices.
‚àí
Combinedzero-shotvideodenoiser. Intuitively,theex-
4.1.InflatedDenoiser
tendedattentiondenoiser,œµEA(,p)thatoperatesonframes
Œ∏ ¬∑
We inflate the T2I denoiser œµ Œ∏(,p) into a video denoiser producesaspatiallycoherentvolume,whilethespatiotem-
¬∑
œµV(,p)byperformingthefollowingmodifications. poraldenoiser,œµS(,‚Äú‚Äù),generatesatemporallyconsistent
Œ∏ ¬∑ Œ∏ ¬∑
volume. Thus,acombinationofbothshouldenforceboth
Extended attention. The original denoiser, œµ (,p), is spatialcoherenceandtemporalconsistency. Followingthis
Œ∏
¬∑ intuition,wesetthecombinedvideodenoisertobe
a U-Net, comprised of residual, cross-attention and self-
attention blocks (Rombach et al., 2022). Following the (cid:112)
œµV(,p)=‚àöŒ≥œµEA(,p)+ 1 Œ≥œµS(,‚Äú‚Äù). (1)
generalapproachproposedbyWuetal.(2023a)andused Œ∏ ¬∑ Œ∏ ¬∑ ‚àí Œ∏ ¬∑
byothers(Geyeretal.,2024),wemodifytheself-attention
ThehyperparameterŒ≥ balancesbetweenthetwodenoisers
within each transformer block into an extended attention
whilepreservingthevarianceofthepredictednoise. Note
module. Thisenablestheattentionmoduletoprocessmul-
thateachdenoiserservesadistinctpurpose,hencerequiring
tiple frames together, resulting in an attention map with
different prompts. The right pane of Fig. 4 depicts this
correspondence between multiple frames. Our extended
combineddenoiser.
attention is calculated between each frame and a set of 3
key-frames, consisting of a global frame strategically po-
4.2.VideoEditing
sitioned at half of the video length, along with two local
frames which are chosen as the 2nd and 5th from within Toeditaninputvideo,I ,weadopttherecentlyproposed
0
the6-frameprocessingwindowcontainingthetargetframe. DDPM inversion method of Huberman-Spiegelglas et al.
4
ESMSlicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
ùêº - ùëù!"# Inflated denoiser ùúñ !" ",ùëù
Acar on the road
ùúñ$% ",ùëù
DDPM inversion ! ùë¶ Predicted noise
using ùúñ!"*,ùëù#$%
ùõæ
$ $ ùë• ùë°
ùêΩ - Noise!"# Extende! d" # ùë¶ ùúñ !# ","" ùë¶
Space A Intt jee cn tt ii oo nn ùë• ùë° ùë¶ 1 ‚àí ùõæ ùë• ùë°
DDPM sampling
using ùúñ!"*,ùëù&‚Äô$
ùë• ùë° Self attention
ùëù$%" Extended attention
Apink car on the road
Figure4: Sliceditoverview. Left: ToeditavideoI ,weapplyDDPMinversionusingourvideo-denoisingmodel,whichis
0
aninflatedversionoftheT2Imodel. Thisprocessextractsnoisevolumesandattentionmapsforeachdiffusiontimestep.
Subsequently, werunDDPMsamplingusingtheextractednoisespace, whileinjectingtheextendedattentionmapsat
specifictimesteps. Theinversionandsamplingareperformedwhileconditioningtheinflateddenoiseronthesourceand
targettextprompts,respectively. Right: Ourinflateddenoiseremploystwoversionsofthepretrainedimagedenoiser. A
versionwithextendedattentionisappliedtox yslices(blue),andtheoriginaldenoiserisappliedtoy tslices(red).
‚àí ‚àí
Thetwopredictednoisevolumesarethencombinedintothefinalpredictednoisevolume(markedingreen).
(2023). Wefirstinverttheentirevideovolumebyextracting 64frames. Forvideoswithlessthan64frames(andmore
thenoisevolumesforalldiffusiontimestepsusingœµV(,p ). than32),weuseRIFE(Huangetal.,2022),astate-of-the-
Œ∏ ¬∑ src
Notethatthisstepisfundamentallydifferentfrommethods art frame interpolation method, to double the number of
thatuseper-frameinversion(Geyeretal.,2024). Then,we frames. Afterediting,wetemporallysubsamplethevideo
performDDPMsamplingwithœµV(,p ),whileusingthe backtotheoriginalnumberofframes. Forvideosexceed-
Œ∏ ¬∑ tar
noisevolumesobtainedfromtheinversion. Notethatp ing64frames,wesplitthevideovolumeintooverlapping
src
andp arepromptsthatdescribetheinputvideoandthe segments. Theextentofthisoverlapisdeterminedbythe
tar
desirededitedvideo,respectively. Therefore,theyarefed totalnumberofframes. Ateverydiffusionstep,weaverage,
inœµEA(,p). Asnotedearlier,theemptypromptinœµS(,‚Äú‚Äù) whilepreservingvariance,thepredictednoisevolumesfrom
Œ∏ ¬∑ Œ∏ ¬∑
isusedintheinversionaswellasinthesamplingprocesses. theoverlappingsegments.
Inaddition,duringthesamplingprocessweinjecttheex-
WeuseDDPMinversionwithT =50steps. Forthegenera-
tendedattentionmapsofthesourcevideointotheextended
tionprocess,weuseDDPMsamplingstartingfromtimestep
attentionmapsoftheeditedvideo. Thisinjection,inspired
T T ,whereinallourexperimentswefixT =8.The
byTumanyanetal.(2023),helpstopreservethestructure ‚àí skip skip
parameterT controlstheextenttowhichtheeditedvideo
andmotionoftheoriginalvideointheeditedresult. While skip
adherestotheinputvideo. Wesettheclassifierfreeguid-
reminiscentoftheapproachemployedinFateZero(QIetal.,
ance(Ho&Salimans,2021)strengthparameterto10inœµEA
2023), our methodology differs in the treatment of atten-
andto1inœµS. Moreover,weinjecttheextendedattention
tion maps. Unlike the blending process in FateZero, our
featuresfromthesourcevideotothetargetvideoin85%
methodinvolvesdirectlycopyingattentionmapsfromthe
ofthesamplingprocess. WesetŒ≥,thebalancingparameter
inputvideointotheeditedone.Figure4depictsanoverview
inEq.(1),to0.8. Allresultsinthepaper,intheSM,and
oftheentirevideoeditingprocess. Oureditingalgorithmis
onourwebsitewereproducedwiththesehyperparameters.
summarizedinApp.H.2.
Foranablationstudyonourdesignchoicespleasereferto
Sec.5.5andApp.D.Theeffectofdifferenthyperparameter
5.Experiments configurationsisdiscussedinApp.E.
5.1.ImplementationDetails
5.2.Comparisons
We use the official weights of Stable Diffusion v2.12 as
Dataset. Weevaluateourmethodonadatasetofvideos,
ourpretrainedT2Idenoiser. AsmentionedinSec.4,this
which we collected from the DAVIS dataset (Pont-Tuset
model‚Äôslatentspacedimensionsare(64,64,4). Thus,naive
etal.,2017),theLOVEU-TGVEdataset(Wuetal.,2023b)
applicationofourmethodispossibleonlyforvideoswith
and from the internet. These videos vary in length and
2https://huggingface.co/stabilityai/ aspectratioandtheydepictanimals,objectsandhumans,
stable-diffusion-2-1 exhibiting different types of motion. Their lengths vary
5Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
A cat on the grass in the sun Swans are swimming in the lake A man is running
tu
p
n
I
A cheetahon the grass in the sun Flamingosare swimming in the lake Usain Bolt is running
d
e
tid
E
A man is spinning a basketball on his finger A person is hiking in front of mountains
tu
p
n
I
Steph Curryis spinning a basketball on his finger A shiny silver robot is hiking in front of mountains
d
e
tid
E
A small dog is looking out a car window A bird is standing in the water
tu
p
n
I
A small leopardis looking out a car window A pink origami bird is standing in the water
d
e
tid
E
Figure5: SliceditResults. Ourmethodeditsonlythespecifiedregionsoftheinputvideoaccordingtothetargetprompt
whilekeepingtheunspecifiedregionsthesame. Theoutputvideomaintainscoherencebetweenframes(i.e.,thesamerobot,
origamirabbitandcheetahacrosstheframes). Videoresultsareavailableonourwebsite.
between32and210frames. Ourdatasetcomprises60text- maps and edge maps. Therefore, their method generates
videopairs,forwhichwemanuallyspecifiedthesourceand videosthatadhereonlytothemotionandstructureofthe
targetprompts.Weusethisdatasettoevaluateallcompeting originalvideo,butdonotpreserveitstexturesandcolors.
methods. Somemethodsremoveafewframesfromtheend More details about the competing methods can be found
oftheinputvideo. Forfaircomparisonweensurealledited in App. C. We excluded Text2Video Zero (Khachatryan
videoshavethesamenumberofframes,byremovingframes etal.,2023)andFateZero(QIetal.,2023)fromourcom-
fromtheresultsofothermethods. parisons due to their memory requirements. On a single
RTXA6000GPU,theoneweusedforrunningallmethods
includingours,thesemethodscouldeditvideosofonlyup
Competing methods. We compare our method against to30frames. Additionally,TuneAVideo(Wuetal.,2023a)
state-of-the-artrecentandconcurrentmethods,whosecode and Flatten (Cong et al., 2024) were omitted due to their
ispubliclyavailableatthetimeofwriting. Thecompeting extended processing time and memory requirements, re-
methodsarePix2Video(Ceylanetal.,2023),RerenderA spectively,whichdonotpermitlarge-scalecomparisons. A
Video(Yangetal.,2023),TokenFlow(Geyeretal.,2024) qualitativecomparisontoboththesemethodscanbefound
andControlVideo(Zhangetal.,2024). ControlVideoaims inFigs.S4,S5.
togenerateanewvideobasedonatextpromptandacondi-
tioningsignalextractedfromtheoriginalvideo,e.g. depth
6Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
A man is jumping ‚ÜíA shiny silver robot is jumping A cat ‚ÜíAlion
t
u
p
n
I
o
e
d
iV
2
x
iP
o
ep
da
iVM
lo h
rt
tp
ne
oD
C
r
eo
de
nd
eiV
r
e A
R
w
o
lF
n
e
k
o
T
s
r
u
O
Figure6: Qualitativecomparison. Wecompareourmethodagainstotherstate-of-the-artzero-shotvideoeditingmethods.
Ourmethodeditsonlythespecifiedregion,accordingtothetextprompt,andkeepstheunspecifiedregionsunchanged. The
competingmethodsoftenchangetheentireframe,specifiedandunspecifiedregionsalike. Seevideosonourwebsite.
5.3.QualitativeEvaluation ground, even though the target prompt only refers to the
person. Pix2VideoandTokenFlowturnthebackgroundinto
Figures1and5presenteditingresultsobtainedwithSlicedit
agray, CGI-styledcity, achangenotspecifiedinthetext
onchallengingvideosinvolvingcameramotionandcom-
prompt. Rerender A Video, while retaining some of the
plex nonrigid object motion, including occlusions. In all
background‚Äôscolorpalette,alsomakessignificantchanges.
cases, our method manages to successfully edit the input
videosaccordingtothetextprompt. Inthecaseofthecatvideo,whichexhibitssmoothermotion,
RerenderAVideoandTokenFlowgenerateblurryimages
Figure 6 presents a comparison between our method and
whilePix2VidandControlVideoshowinconsistencyinthe
thecompetingmethods. ForControlVideo,weuseddepth
motion. MoreframecomparisonsareprovidedinApp.B
conditioning. Ourmethodadherestothetextpromptwhile
andvideocomparisonsappearonourwebsite.
doingabetterjobthantheothermethodsatpreservingthe
unspecifiedregions. Thisistrueforbothcomplexmotion
5.4.QuantitativeEvaluation
(left),andforsmoothmotion(right).
Weconductanumericalevaluationusingmetricsthatquan-
Specifically, in the case of the parkour video, Pix2Video,
tify editing fidelity, structure preservation, and temporal
RerenderAVideoandControlVideoallexhibitmajorin-
consistency. Followingpriorworks, weevaluatetheedit-
consistenciesinboththeeditedrobotandthebackground.
ingfidelityusingtheaveragecosinesimilaritybetweenthe
IntheresultofRerenderAVideo,therobotfadesawayin
imageCLIPembeddingsoftheframesandthetextCLIP
thelastpartoftheeditedvideo,ascanbeseenintheright
embeddingofthetargetprompt. Forstructurepreservation,
frame. TokenFlow‚Äôseditissomewhatmoresuccessful,yet
wereporttheLPIPSdistancebetweenthesourceandedited
therobotisjitteryandblurry. Inaddition,itcanbeclearly
frames. TheveryhighLPIPSscoreofControlVideoisdue
seenthatallcompetingmethodsdrasticallychangetheback-
7Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
ControlVideo(Edge)
ControlVideo(Edge)
0.7 ControlVideo(Depth)
2.5
ControlVideo(Depth) RerenderAVideo
2
0.6
1.5
0.5
Pix2Video 1
0.4 RerenderAVideo
TokenFlow
Pix2Video
0.3 0.5
TokenFlow
0.2
Slicedit Slicedit
0.1 0.2
0.310 0.315 0.320 0.325 0.330 0.335 0.340 0.345 0.350 0.310 0.315 0.320 0.325 0.330 0.335 0.340 0.345 0.350
CLIP CLIP
‚àí‚Üí ‚àí‚Üí
Figure7: Numericalcomparisontocompetingmethods. Wecompareourmethodandtothecompetingmethodson
60 text-video pairs. The left pane shows the editing fidelity, measured via the CLIP score, vs. the faithfulness to the
originalvideo,measuredwithLPIPS.Therightpaneshowstheeditingfidelityvs.temporalconsistency,measuredviaflow
error. SliceditisshownwithtwooptionsforthehyperparametersT ,classifier-freeguidancestrength,andpercentof
skip
extended-attentioninjection. Fromlefttoright,thevaluesoftheseparametersare(8,10,85)and(8,14,85). Thefirstsetting
istheoneusedtoproducealltheresultsinthepaper,appendix,andthewebsite.
Rerender A in motion and appearance (see Tab. S4). Our zero-shot
Slicedit 82.6% 17.4%
Video editingdemonstratessuperiorLPIPSscoresandflowerror,
successfullyadheringtothetextpromptforvideoediting.
Slicedit 67.9% 32.1% TokenFlow
Incontrast,thecompetingmethodsexhibitvideoeditswith
somelossoffidelitytotheoriginalcontentandpoorertem-
Figure 8: User study. We report the percentage of users
poralconsistency.
whopreferredourmethodoverRerenderAVideoandTo-
kenFlow,whenansweringwhicheditedvideobestpreserves Weadditionallyevaluatedourmethodviaauserstudy,in
theessenceoftheoriginalvideo. whicheachparticipantwasshowntheoriginalvideo, the
target prompt, and our edited result next to a competing
editedresult. Theorderbetweenthetwoeditedresultswas
to its conditioning only on a signal (e.g. depth or edges) random. Users were instructed to select the edited video
fromanexistingvideo. Toassesstemporalconsistency,we that best preserves the essence of the original video. A
compute the error between the optical flow fields of the screenshot from the user study can be found in Fig. S6.
sourceandeditedvideos. Specifically,wecomputeoptical WeconductedtwoseparateuserstudiesthroughAmazon
flowusingRAFT(Teed&Deng,2020)betweeneverypair MechanicalTurk,where50workerscomparedourmethod
ofconsecutiveframesinboththeoriginalandeditedvideos. againstTokenFlowandRerenderAVideooverourentire
Subsequently,wecalculatemeanL distancebetweenthese dataset.Theuserstudyresult,reportedinFig.8,revealsthat
2
flowfields,whileremovingpixelswithleft-rightinconsis- humansclearlypreferoureditedvideosoverthoseofthe
tenciesinthesourcevideo. Importantly,unlikethepopular competingmethodsintermsofpreservationofunspecified
warpingerror(Laietal.,2018;Ceylanetal.,2023;Geyer regions.
et al., 2024), which is computed by warping the edited
framesusingtheflowfieldoftheoriginalone,theflowerror 5.5.AblationStudy
weusecapturesonlymotionpreservation,andiscompletely
Wenextstudytheimportanceofeachofthecomponentsthat
disentangledfromappearanceinconsistencies.
weusedforimprovingtemporalconsistencyandpreserving
Figure7presentsourquantitativeresultscomparedtothe structure: (1)denoisingspatio-temporalslices,(2)extended
competingmethods. Weshowresultsforourmethodwith attentioninjection,(3)DDPMinversionoverDDIMinver-
twoparameterconfigurations,whichleadtodifferentbal- sion, and (4) inversion for the entire volume rather than
ancesbetweentextadherenceandfidelitytotheinputvideo
8
SPIPL
‚àí‚Üê
rorrE
wolF
‚àí‚ÜêSlicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
A cow ‚ÜíAn origami cow A small dog is looking out a car window
tu tu
p n p n
I I
A small elephantis looking out a car window
S o /wùúñ =1
ùõæ
d e
tid
E
d
e d n e tx
e
o /w
n o itn
e
ttan o itc
e jn
i
F tui rg eu pre re1 s1 er: vF ina gilu edr ie tsc ,a as ne ds. caO nu nr om tmet oh do id fyis e.l gim .ait de od gto ins tt oru ac n-
elephant. Failurecasevideosareavailableonourwebsite.
n
M IDo is
r
e
Dv
n
I video. WhenusingDDIMinsteadofDDPMinversionwith
tid thesameparameters(fourthrow)ourmethodisnotableto
e c successfullyeditthevideoaccordingtothetextprompt.
ilS
Forevaluatingtheeffectofperformingvolumeratherthan
per-frame inversion, it is not feasible to use our inflated
Figure 9: Ablation study. Each row displays the results
denoiser. Hence,westudythiswiththeTokenFlowmethod,
without a key component of our method. The last row,
whichinherentlyreliesonper-frameinversion. Figure10
displaystheresultsachievedbySlicedit.Videocomparisons
showstheresultsofTokenFlowwithper-frameDDIMand
areavailableonourwebsite.
withper-frameDDPMinversion. Ascanbeseen,bothcases
lead to blurry results. This highlights the importance of
performingvolumeinversion.
per-frame. Wenotethattheincorporationofextendedat-
tentionintheinflateddenoiseralsocontributestothefinal Foramoredetailedablation, includingchangingthetext
results,howeverthismechanismisalreadywellestablished promptofthespatio-temporaldenoiserandinversionwith
andhasbeenablatedinpreviousworks. Assuch,wewill an empty source text, comparison of quantitative metrics
notprovideanablationforithere. for each configuration and more frame comparisons, see
App.D.Videocomparisonsareavailableonourwebsite.
AscanbeseeninFig.9,removingthespatiotemporalde-
noising(secondrow)harmsthetemporalconsistencyofthe
6.Conclusion
edited video, causing the edited object to change appear-
anceoverframes. Withoutinjectingtheextendedattention
WeintroducedSlicedit, azero-shottext-basedvideoedit-
mapsoftheoriginalvideo(thirdrow),theresultingedited
ing method utilizing a pretrained text-to-image diffusion
videoisnotloyaltothelayoutandmotionoftheoriginal
model. Ourmethodinflatesthemodeltoworkonvideos
usingseveralmodifications. Mostimportantly,itappliesthe
pretraineddenoiser,initiallydesignedforimages,alsoon
Aman is jumping ‚ÜíA shiny silver robot is jumping
spatiotemporalslicesofthevideo. Toeditvideos,weuse
ourinflateddenoiserinaDDPMinversionprocess,incon-
tu
p junctionwithinjectionoftheextendedattentionfromthe
n
I
sourcevideotothetargetvideo. Ourmethodoutperforms
existingtechniques,successfullyeditingthevideowhilepre-
w.v
o lF nn I
M
servingtheunspecifiedregionswithoutcompromisingon
e kID temporalconsistency. Weevaluateditbymeasuringediting
oTD
fidelity,structurereservation,andtemporalconsistencymet-
w
o lF
n.v
n I
M
r ai tc ps, resu sep rp vl ie nm ge tn ht eed stb ruy ca tuu rs eer os ft tu hd ey. inW puh til ve ido eu or ,m itet eh no cd ouex nc tee rl ss
eP
k oTD
D challengeswithglobaleditingtasks,suchasconvertingthe
frames of a natural video into paintings. In addition, our
Figure10: Volumeinversionablation. TokenFlowresults
methodislimitedtostructure-preservingedits. Thisstems
for DDPM and DDIM per-frame inversion. Videos are
fromusingDDPMinversionwithattentioninjection. An
availableonourwebsite.
examplefailurecaseisshowninFig.11.
9Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
SocietalImpact Dai, X., Hou, J., Ma, C.-Y., Tsai, S., Wang, J., Wang,
R., Zhang, P., Vandenhende, S., Wang, X., Dubey, A.,
Ourgoalinthisworkistosuggestadifferentmethodfor
et al. Emu: Enhancing image generation models us-
leveragingT2Ifoundationmodelsforvideoediting,over-
ing photogenic needles in a haystack. arXiv preprint
comingsomeoftheweaknessesofpreviousandconcurrent
arXiv:2309.15807,2023.
works. Ourandsimilarmethodsmaypotentiallybeusedto
editvideostocreatefakeorharmfulcontent. Webelieve
Esser,P.,Chiu,J.,Atighehchian,P.,Granskog,J.,andGer-
that it is crucial to develop and apply tools for detecting
manidis,A. Structureandcontent-guidedvideosynthesis
videoseditedusinggenerativeAImethods,suchasours. In
withdiffusionmodels. InProceedingsoftheIEEE/CVF
addition,mitigatingbiasesandNSFWcontentfromlarge
InternationalConferenceonComputerVision,pp.7346‚Äì
datasets,usedfortrainingfoundationmodels,willalsocon-
7356,2023.
tributeforsafergenerativeAIusage,includingusingT2I
modelsforvideoediting.
Feng,R.,Weng,W.,Wang,Y.,Yuan,Y.,Bao,J.,Luo,C.,
Chen, Z., and Guo, B. Ccedit: Creative and control-
Acknowledgements. Thisresearchwassupportedbythe lablevideoeditingviadiffusionmodels. arXivpreprint
IsraelScienceFoundation(grantno.2318/22)andbytheOl- arXiv:2309.16496,2023.
lendorffMinervaCenter,ECEfaculty,Technion. Nathaniel
Cohen‚ÄôsstayattheTechnionwassupportedinpartbythe Geyer,M.,Bar-Tal,O.,Bagon,S.,andDekel,T. Tokenflow:
MinesParisFoundation. Consistentdiffusionfeaturesforconsistentvideoediting.
In The Twelfth International Conference on Learning
References Representations,2024. URLhttps://openreview.
net/forum?id=lKK50q2MtV.
Adelson, E. H. and Bergen, J. R. Spatiotemporal energy
modelsfortheperceptionofmotion. Josaa,2(2):284‚Äì Hertz,A.,Mokady,R.,Tenenbaum,J.,Aberman,K.,Pritch,
299,1985. Y.,andCohen-Or,D. Prompt-to-promptimageediting
withcross-attentioncontrol. InInternationalConference
Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L.,
onLearningRepresentations(ICLR),2023.
Ouyang,L.,Zhuang,J.,Lee,J.,Guo,Y.,etal. Improving
imagegenerationwithbettercaptions. ComputerScience.
Ho,J.andSalimans,T. Classifier-freediffusionguidance.
https://cdn.openai.com/papers/dall-e-3.pdf,2023.
InNeurIPS2021WorkshoponDeepGenerativeModels
Bolles,R.C.,Baker,H.H.,andMarimont,D.H. Epipolar- andDownstreamApplications,2021.
planeimageanalysis: Anapproachtodeterminingstruc-
ture from motion. International journal of computer Ho,J.,Jain,A.,andAbbeel,P. Denoisingdiffusionproba-
vision,1(1):7‚Äì55,1987. bilisticmodels,2020.
Brooks,T.,Holynski,A.,andEfros,A.A. InstructPix2pix: Huang, Z., Zhang, T., Heng, W., Shi, B., and Zhou, S.
Learning to follow image editing instructions. In Pro- Real-timeintermediateflowestimationforvideoframe
ceedingsoftheIEEEConferenceonComputerVisionand interpolation. In European Conference on Computer
PatternRecognition(CVPR),pp.18392‚Äì18402,2023. Vision,pp.624‚Äì642.Springer,2022.
Canny, J. A computational approach to edge detection.
Huberman-Spiegelglas,I.,Kulikov,V.,andMichaeli,T. An
IEEETransactionsonpatternanalysisandmachinein-
editfriendlyDDPMnoisespace: Inversionandmanipu-
telligence,(6):679‚Äì698,1986.
lations,2023.
Ceylan, D., Huang, C.-H.P., andMitra, N.J. Pix2video:
Kawar,B.,Zada,S.,Lang,O.,Tov,O.,Chang,H.,Dekel,
Videoeditingusingimagediffusion. InProceedingsof
T., Mosseri, I., and Irani, M. Imagic: Text-based real
the IEEE/CVF International Conference on Computer
imageeditingwithdiffusionmodels. InConferenceon
Vision(ICCV),pp.23206‚Äì23217,October2023.
ComputerVisionandPatternRecognition(CVPR),2023.
Cong, Y., Xu, M., christian simon, Chen, S., Ren, J.,
Xie, Y., Perez-Rua, J.-M., Rosenhahn, B., Xiang, T., Khachatryan,L.,Movsisyan,A.,Tadevosyan,V.,Henschel,
and He, S. FLATTEN: optical FLow-guided ATTEN- R.,Wang,Z.,Navasardyan,S.,andShi,H. Text2video-
tionforconsistenttext-to-videoediting. InTheTwelfth zero: Text-to-imagediffusionmodelsarezero-shotvideo
InternationalConferenceonLearningRepresentations, generators. In Proceedings of the IEEE/CVF Interna-
2024. URLhttps://openreview.net/forum? tionalConferenceonComputerVision(ICCV),pp.15954‚Äì
id=JgqftqZQZ7. 15964,October2023.
10Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
Lai,W.-S.,Huang,J.-B.,Wang,O.,Shechtman,E.,Yumer, Saharia,C.,Chan,W.,Saxena,S.,Li,L.,Whang,J.,Denton,
E.,andYang,M.-H. Learningblindvideotemporalcon- E.L.,Ghasemipour,K.,GontijoLopes,R.,KaragolAyan,
sistency. InProceedingsoftheEuropeanconferenceon B.,Salimans,T.,etal. Photorealistictext-to-imagedif-
computervision(ECCV),pp.170‚Äì185,2018. fusion models with deep language understanding. Ad-
vances in Neural Information Processing Systems, 35:
Lin, Z., Feng, M., dos Santos, C. N., Yu, M., Xiang,
36479‚Äì36494,2022.
B., Zhou, B., and Bengio, Y. A STRUCTURED
SELF-ATTENTIVE SENTENCE EMBEDDING. In Song, J., Meng, C., and Ermon, S. Denoising diffusion
InternationalConferenceonLearningRepresentations, implicitmodels.InInternationalConferenceonLearning
2017. URLhttps://openreview.net/forum? Representations,2020.
id=BJC_jUqxe.
Teed,Z.andDeng,J. Raft: Recurrentall-pairsfieldtrans-
Liu,S.,Zhang,Y.,Li,W.,Lin,Z.,andJia,J. Video-p2p: formsforopticalflow. InComputerVision‚ÄìECCV2020:
Videoeditingwithcross-attentioncontrol. arXivpreprint 16thEuropeanConference,Glasgow,UK,August23‚Äì28,
arXiv:2303.04761,2023. 2020, Proceedings, Part II 16, pp. 402‚Äì419. Springer,
2020.
Ngo,C.-W.,Pong,T.-C.,andZhang,H.-J. Motionanalysis
andsegmentationthroughspatio-temporalslicesprocess-
Tumanyan,N.,Geyer,M.,Bagon,S.,andDekel,T. Plug-
ing. IEEE Transactions on Image Processing, 12(3):
and-play diffusion features for text-driven image-to-
341‚Äì355,2003.
image translation. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition
Pont-Tuset,J.,Perazzi,F.,Caelles,S.,Arbela¬¥ez,P.,Sorkine-
(CVPR),pp.1921‚Äì1930,June2023.
Hornung, A., and Van Gool, L. The 2017 davis chal-
lenge on video object segmentation. arXiv preprint
Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,
arXiv:1704.00675,2017.
L., Gomez, A. N., Kaiser, ≈Å., and Polosukhin, I. At-
QI,C.,Cun,X.,Zhang,Y.,Lei,C.,Wang,X.,Shan,Y.,and tentionisallyouneed. Advancesinneuralinformation
Chen,Q. Fatezero: Fusingattentionsforzero-shottext-
processingsystems,30,2017.
based video editing. In Proceedings of the IEEE/CVF
Vu, P. V. and Chandler, D. M. Vis 3: an algorithm for
InternationalConferenceonComputerVision(ICCV),pp.
videoqualityassessmentviaanalysisofspatialandspa-
15932‚Äì15942,October2023.
tiotemporalslices. JournalofElectronicImaging,23(1):
Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G., 013016‚Äì013016,2014.
Agarwal,S.,Sastry,G.,Askell,A.,Mishkin,P.,Clark,J.,
Wang,W.,Xie,k.,Liu,Z.,Chen,H.,Cao,Y.,Wang,X.,and
etal. Learningtransferablevisualmodelsfromnatural
Shen,C. Zero-shotvideoeditingusingoff-the-shelfim-
language supervision. In International conference on
agediffusionmodels. arXivpreprintarXiv:2303.17599,
machinelearning,pp.8748‚Äì8763.PMLR,2021.
2023.
Ranftl,R.,Bochkovskiy,A.,andKoltun,V. Visiontrans-
Wu, J. Z., Ge, Y., Wang, X., Lei, S. W., Gu, Y., Shi, Y.,
formers for dense prediction. In Proceedings of the
Hsu, W., Shan, Y., Qie, X., and Shou, M. Z. Tune-a-
IEEE/CVFinternationalconferenceoncomputervision,
video: One-shot tuning of image diffusion models for
pp.12179‚Äì12188,2021.
text-to-video generation. In Proceedings of the IEEE
Rav-Acha,A.,Pritch,Y.,Lischinski,D.,andPeleg,S. Dy- InternationalConferenceonComputerVision(ICCV),pp.
namosaics: Videomosaicswithnon-chronologicaltime. 7623‚Äì7633,October2023a.
In2005IEEEComputerSocietyConferenceonComputer
VisionandPatternRecognition(CVPR‚Äô05),volume1,pp. Wu, J. Z., Li, X., Gao, D., Dong, Z., Bai, J., Singh, A.,
58‚Äì65.IEEE,2005. Xiang,X.,Li,Y.,Huang,Z.,Sun,Y.,etal. Cvpr2023
text guided video editing competition. arXiv preprint
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and arXiv:2310.16003,2023b.
Ommer,B. High-resolutionimagesynthesiswithlatent
diffusionmodels. InProceedingsoftheIEEEConference Xing,Z.,Dai,Q.,Hu,H.,Wu,Z.,andJiang,Y.-G. Simda:
onComputerVisionandPatternRecognition(CVPR),pp. Simplediffusionadapterforefficientvideogeneration.
10684‚Äì10695,2022. arXivpreprintarXiv:2308.09710,2023.
Ronneberger, O., Fischer, P., and Brox, T. U-net: Con- Yang, S., Zhou, Y., Liu, Z., , and Loy, C. C. Rerender a
volutionalnetworksforbiomedicalimagesegmentation, video: Zero-shottext-guidedvideo-to-videotranslation.
2015. InACMSIGGRAPHAsiaConferenceProceedings,2023.
11Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
Zhang,L.,Rao,A.,andAgrawala,M. Addingconditional
controltotext-to-imagediffusionmodels. InProceedings
oftheIEEE/CVFInternationalConferenceonComputer
Vision,pp.3836‚Äì3847,2023.
Zhang, Y., Wei, Y., Jiang, D., ZHANG,X., Zuo, W., and
Tian,Q. Controlvideo: Training-freecontrollabletext-to-
videogeneration. InTheTwelfthInternationalConfer-
enceonLearningRepresentations,2024. URLhttps:
//openreview.net/forum?id=5a79AqFr0c.
Zuckerman,L.P.,Naor,E.,Pisha,G.,Bagon,S.,andIrani,
M.Acrossscalesandacrossdimensions:Temporalsuper-
resolution using deep internal learning. In Computer
Vision‚ÄìECCV2020:16thEuropeanConference,Glasgow,
UK,August23‚Äì28,2020,Proceedings,PartVII16,pp.
52‚Äì68.Springer,2020.
12Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
A.AdditionalResults
Fig.S1displaysadditionalresultsforourvideoediting.
A rabbit is eating A lotus in the water on a rainy day Goldfish are swimming
tu
p
n
I
An origamirabbit is eating A poppy flowerin the water on a rainy day Sharksare swimming
d
e
tid
E
A car in the road A man is dancing
tu
p
n
I
A pinkcar in the road A bronze statue is dancing
d
e
tid
E
A man on the beach is holding a surfboard and a kite A woman is walking in a park
tu
p
n
I
A silver statueon the beach is holding a surfboard and a kite An anime woman is walking in a park
d
e
tid
E
FigureS1: Additionaleditingresultsusingourmethod.
13Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
B.AdditionalVisualComparisons
AsmentionedinSec.5,weomittedTuneAVideofromthequantitativecomparisonduetotheirextendedprocessingtime,
asshowninTab.S1. Hereweprovideaqualitativecomparisonforthismethodon4differenttext-videopairs. Ascanbe
seeninFigs.S4,S5,TuneAVideoisabletoadheretoatextpromptthroughoptimization. However,thisoptimization
processoftenresultsinundesiredglobalchangesintheeditedvideo. Forexample,thephotorealisticwavessurroundingthe
surfingmanintherightpaneofFig.S4turnintopainted-likewaves.
WealsoprovideherequantitativecomparisonstoFlattenforvideoswith32frames,whichcanbeseeninFig.S4. Flatten
resultskeeptheunspecifiedregionssimilartotheoriginalvideo. Forthecatvideo,Flatteneditdidnotkeeptheidentityof
thelionacrossdifferentframes. Thelion‚Äôsfaceinthemiddleframeisverysimilartotheoriginalcat‚Äôs,andisquitedifferent
fromthelion‚Äôsfaceontherightframe. AsFlatten‚Äôsmemoryrequirementmadeitnotpossibletoeditvideoslongerthan32
framesonasingleRTXA6000GPU,whichweusedforourmethodandallcompetingmethods,wedonotprovidehere
moreresultsofthismethod.
ControlVideoaimstogenerateavideobasedonatextpromptandaconditionsignalfromtheoriginalvideo. Therefore,its
resultsadhereonlytotheoriginalvideomotionandstructure,butnottotheoriginalvideocolorsandandtextures. Asa
result,theunspecifiedregionsintheirresultsarealwaysdifferentthantheoriginalvideo‚Äôs,ascanbeseeninallthefigures
below,HereweprovidequalitativeresultsforControlVideowithdepthmapsconditioning. Fordepthestimationweused
MiDaSDPT-Hybrid(Ranftletal.,2021),asintheControlVideopaper. Quantitativeresultsforcannyedge(Canny,1986)
conditioningareavailableatFig.7andatTab.S2.
Figs.S2-S5alsoillustratethatthecompetingmethodsoftenintroduceglobalchangestotheentirevideo,resultinginediting
ofregionsunrelatedtothetargettexttogetherwiththerelatedregions. Theseglobalchangesalmostalwaysincludecolor
changes,illustratedbytheeagleexampleinFig.S3andthecowsexampleinFig.S4. Italsoincludeschangingobjects‚Äô
shapesasthegrassinthecatexampleinFig.S4,andchangingthebackground. Backgroundchangescanbeseeninthe
dancingmanexampleinFig.S2andthepenguinexampleinFig.S5,wherethecompetingmethodscompletelychanged
thebackground. Moresubtlechangestothebackgroundalsooccurs,asintherunningmanexampleinFig.S2,whereall
competingmethodschangedthevillagebehindtherunningman.
Itcanalsobenoticedthatsomeofthecompetingmethodsfailedtoadheretotextprompt(Pix2Videointheeagleeaxmple
inFig.S3,TokenFlowinthebirdexampleinFig.S3),totheoriginalvideomotion(TokenFlowinthecowsexamplein
Fig.S4),orproduceinconsistenteditingorblurryoutput(ControlVideoinFigs.S2,S3,RerenderAVideoandTokenFlow
inthecatexampleinFig.S4).
Videoresultsofthecomparisonscanbefoundonourwebsite.
14Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
A man is dancing ‚ÜíA shiny silver robot is dancing A man is running ‚ÜíUsain Bolt is running
t
u
p
n
I
o
e
d
iV
2
x
iP
o
e
d
iV
lo
r
t
n
o
C
r e d n eo e d iV
r
e A
R
w
o
lF
n
e
k
o
T
s
r
u
O
FigureS2: Additionalqualitativecomparisons.
15Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
An eagle ‚ÜíA Pixar styleeagle A bird ‚ÜíAsquirrel
tu
p
n
I
o
e
d
iV
2
x
iP
o
e
d
iV
lo
rtn
o
C
re
d n e
re
Ro
e d iV
A
w
o
lF
n
e
k
oT
sru
O
FigureS3: Additionalqualitativecomparisons.
16Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
Cows are grazing ‚Üí Sheep are grazing A cat ‚Üí A lion
t
u
p
n
I
A
eo
e
nd
uiV
T
o
e
d
iV
l
o
r
t
n
o
C
n
e
t
t
a
l
F
o
e
d
iV
2
x
i
P
r eo
de
nd
eiV
r
e A
R
w
o
l
F
n
e
k
o
T
s
r
u
O
FigureS4: Additionalqualitativecomparisons.
17Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
A man is surfing ‚Üí A silver robot is surfing A penguin ‚Üí An origami of a penguin
t
u
p
n
I
A
eo
e
nd
uiV
T
o
e
d
iV
l
o
r
t
n
o
C
o
e
d
iV
2
x
i
P
r eo
de
nd
eiV
r
e A
R
w
o
l
F
n
e
k
o
T
s
r
u
O
FigureS5: Additionalqualitativecomparisons.
18Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
C.Comparisons
C.1.CompetingMethods
We compare our method with competing methods whose official implementation was publicly available at the time of
writing: Pix2Video,TokenFlow,RerenderAVideoandControlVideo. Pix2Video(Ceylanetal.,2023)changestheself
attentionofapretrainedT2Imodelintoextendedattention. ItalsousesdepthestimationaspartoftheinputtotheT2Imodel.
TokenFlow(Geyeretal.,2024)andRerenderAVideo(Yangetal.,2023)editkeyframesandpropagatetheirfeaturestothe
restoftheframesinazero-shotmanner. RerenderAVideoalsoutilizesopticalflowandedgemapstoguidetheediting
process. ControlVideo(Zhangetal.,2024)utilizeatrainedControlNet(Zhangetal.,2023)forcontrollabletexttovideo
generation,usingatextpromptandconditionsignalfromtheoriginalvideo. WecomparedheretoControlVideowithdepth
mapsandcannyedge(Canny,1986)conditioning,assuggestedintheirpaper. WeusedMiDaSDPT-Hybrid(Ranftletal.,
2021)fordepthestimation.
Wefollowedtheinstructionsprovidedbytheauthorsintheirofficialimplementationforsettingandexecutingtheircode.
ControlVideoForPix2Video,ControlVideoandTokenFlowweusedthehyperparametersdescribedintheirpaperandused
intheirofficialimplementation. ForTokenFlowitincludedsamplingwithnegativeprompt. ForControlVideoitincluded
samplingwithpositivepromptandnegativeprompt.
As detailed in their paper, Rerender A Video tunes their editing hyperparameter per video. In addition, their official
implementationincludessamplingwith‚Äúnegative‚Äùand‚Äúadditive‚Äùpromptsthataretunedpervideo. Additivepromptsare
promptsthatincludewordsthatareknowntoimproveT2Imodelresults,as‚Äúhigh-quality‚Äù. Asitisimpracticalinourcase
totunehyperparametersforeachtext-videopair,ornegativeandadditivepromptsforeachtext-videopair,weusedthemost
commonhyperparametersfoundintheofficialimplementationofRerenderAVideo,anddidnotsamplewithnegativeor
additiveprompts.
C.2.RunningTime
TableS1comparestherunningtimeofourmethodagainstthecompetingmethods. Weusedthesame64framesvideofor
allmethods,whichweexecutedonthesameRTXA6000GPU.Weconsiderrunningtimeasprocessingtime(inversion,
tuning)andeditingtimetogether.
TherunningtimeofRerenderAVideochangesaccordingtothenumberofkeyframes. Thishyperparametercanbetuned
pervideo. Accordingtotheirpaper,therecommendednumberofkeyframesisbetween5to20,wheretheyused10key
frames. Wefollowedtheirselectionandused10keyframesinallourevaluations,aswellasinthisrunningtimecomparison.
TherunningtimeofFlattenisgivenfor32framesvideo,asitsmemoryrequirementsmadeitinapplicabletolongervideos
ontheRTXA6000GPUweused. Wemarkeditbywith*inthetable.
TableS1: Runningtimecomparison
‚Äî TuneAVideo ControlVideo Flatten* Pix2Video RerenderAVideo TokenFlow Ours
Time[min] 359 6 10 15.5 9 16.25 33.7
19Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
C.3.MetricComparison
InadditiontothemetricspresentedinSec.5,wealsomeasurevideocoherenceusingCLIP3 (Radfordetal.,2021). We
embedeachpairofconsecutiveframesoftheeditedvideointoCLIPimagespaceandreporttheaveragecosinesimilarityof
allvideoframepairs. Thismetriccanalsobecomputedovertheoriginalvideos,providingareferenceforthetypicalvalues
associatedwithavideoexhibitingnaturalcoherence.
InthecontextofFlowerror,explainedinSec.5.4,weremovepixelsthatdonotmatchtheleft-rightconsistenciescalculated
overtheoriginalvideo. Specifically,Wecalculatethedistancebetweentheflowineachframeandthesubsequentframeas
wellasthereversedirection. Pixelsareexcludedfromtheerrorcalculationifthedisparitybetweentheflowsexceeds1
pixel.
ThefollowingtableincludesthemetricnumericalresultsasusedforcreatingFig.7inadditiontothemetricmentionedabove.
ItisworthnotingthattheCLIP-consistencymetricyieldsnearlyidenticalvaluesacrossallmethods,closelyresemblingthe
CLIPconsistencyoftheoriginalvideos.
TableS2: QuantitativeComparison
Method CLIP-Text CLIP-Consistency FlowErr. LPIPS
‚Üë ‚Üì ‚Üì
Original ‚Äî 0.982 ‚Äî ‚Äî
Pix2Video 0.343 0.982 0.55 0.45
RerenderAVideo 0.316 0.983 2.5 0.4
TokenFlow 0.334 0.986 0.4 0.35
ControlVideo(edge) 0.322 0.975 2.88 0.65
ControlVideo(depth) 0.321 0.977 3.014 0.71
Ours 0.329 0.982 0.252 0.159
3https://github.com/openai/CLIPwithViT-B/32
20Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
C.4.UserStudy
Belowwepresentascreenshotfromouruserstudyinterface. Theuserisgiventhesourcevideoalongwithoureditedvideo
andanothercompetingmethod‚Äôseditedvideo(RerenderAVideoorTokenFlow). Theuserisinstructedtochoosetheedited
videothatmaintainstheessenceoftheoriginalvideo.
FigureS6: Ascreenshotfromouruserstudy.
21Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
D.AblationStudy
Inthissection,weanalyzetheeffectivenessofourdesignchoicesthroughanablationstudy. Notethatwheneverwemodify
aspecificaspectofthemethod,allothercomponentsremainunchanged. Weevaluateeachmodificationonourdataset
(describedinSec.5). ThequantitativeresultsaresummarizedinTab.S3.
D.1.SpatiotemporalSlices
Tojustifytheutilizationofspatiotemporalslicesasacomponentofourzero-shotvideoeditingmethod,weconductedan
ablationstudybyexcludingœµS fromtheinversionandsamplingprocess. Moreover, weextendedtheablationstudyby
Œ∏
includingadenoiseroverx tslicesinadditiontoy tslices. Inthisscenario,œµS iscomputedasanaverageofthesetwo
‚àí ‚àí Œ∏
denoisers. Weconductedadditionalexperimentsbyemployingadifferenttextdenoiserthantheemptystring. Specifically,
thepromptsusedaredepictedintherightpaneofFig.2,wheretext =‚Äúpaintingofgeologicalrockfoldinginsedimentary
1
layers‚Äùandtext =‚Äúmotionblur‚Äù.
2
TableS3showstheeffectivenessofthespatiotemporalslicescomponentinourmethod. Excludingthiscomponentresultsin
ahigherFlowerror(firstrow). Incorporatingthex tslicesintothecalculationsdoesyieldaslightimprovementinthe
‚àí
FlowerrorandLPIPSscore(secondrow),yet,imposesalongerrunningtime. Finally,theresultsobtainedwithdifferent
textpromptsforœµS exhibitminimalinfluence(thirdandfourthrows).
Œ∏
D.2.InversionandSamplingviaDDIM
WeperformedanadditionalablationbyreplacingtheDDPMinversionwithDDIMinversion(Songetal.,2020). The
inflateddenoiserexplainedinSec.4isbeingusedaswellastheextendedattentioninjection. However,theinversionandthe
samplingprocessusetheDDIMschemes. Wekeepthehyperparametersasbeforewhileacknowledgingthatscanningfor
theoptimalhyperparametersmayaffectresults. TheresultshowninTab.S3,fifthrow,highlightsthesignificanceofDDPM
inversioninourmethodineditingability(CLIP-Textscore)andtemporalconsistency(Flowerror).
D.3.ExtendedAttentionInjection
Toassesstheeffectivenessofinjectingextendedattentionfromthesourcetothetargetvideo,weevaluateitbyexcluding
thiscomponentfromthemethod. Asanticipated,theresultsinTab.S3,sixthrow,demonstratetheimportanceofthismodule
inourmethodfortimeconsistency.
D.4.EmptyPromptInversion
Weadditionallyassesstheimportanceofthesourcepromptusedduringtheinversionphase. Theresults,detailedinTableS3
undertheseventhrow,indicatethattheyarelessoptimal.
Table S3: Ablation study. Evaluation when changing some of the method‚Äôs design choices. The last row of the table
representsthemethodwithoutanychange.
CLIP-Text CLIP-Consistency FlowErr. LPIPS
‚Üë ‚Üì ‚Üì
Original ‚Äî 0.982 ‚Äî ‚Äî
withouty tslices(Œ≥ =1) 0.335 0.982 0.279 0.207
‚àí
addingx tslice 0.329 0.982 0.248 0.155
‚àí
œµS(,text ) 0.30 0.982 0.252 0.161
1
¬∑
œµS(,text ) 0.30 0.982 0.252 0.161
2
¬∑
DDIM-inversion 0.29 0.97 0.55 0.19
excludeext-attinjection 0.348 0.982 0.452 0.292
emptypromptinversion 0.323 0.983 0.253 0.182
Ours 0.329 0.982 0.252 0.159
22Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
D.5.VisualIllustrationoftheEffectsofourDesignChoices
FigureS7illustratestheimportanceofeachcomponentofourmethod. Ascanbeseenintheframecomparison,andina
clearerwayinthevideocomparisonavailableonourwebsite,withoutthespatiotemporalslices,theresultedvideolacks
temporalconsistency. Specifically,eachframedisplaysadifferenteditoutcome,resultinginajitteryvideo. Withoutthe
extendedattentioninjectiontheresultedvideoadherestothetextpromptbutdisregardstheoriginalvideolayout. When
usingDDIMinversionwiththesameparametersasusedforDDPMinversionourmethodisnotabletosuccessfullyedit
thevideoaccordingtothetextprompt. ForadditionalresultswithdifferentconfigurationsofDDIMinversionandmore
comparisonsbetweenDDPMandDDIMinversionseeoursupplementalwebsite.
A man is jumping ‚Üí A humanoid robot is jumping
t
u
p
n
I
S 1
ùúñ
o=
/
wùõæ
d
e
dnn
noo
ei ti
t
tnc
xee
etj
tn
oai
/
w
n
Mo
i
s
I Dr
e
Dv
n
I
t
i
d
e
c
i
l
S
FigureS7: Ablationstudy.
23Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
E.Hyperparameters
We evaluate the experiment (described in Sec. 5) with two different hyperparameters values of T , strength, and the
skip
percentageofextendedattentioninjection
TableS4providesasummaryofthequantitativeresults. Whilethe(8,14,85)configurationyieldsaslightlyhigherCLIP-
Textscore,the(8,10,85)configurationexhibitssuperiortemporalconsistencyandminimizesLPIPS.Wechosethelatter
configurationasthedefaultforourmethod,employedtoproducealltheresultsinthepaper,theAppendix,andthewebsite.
ItisimportanttonotethatwedidnotexploredifferentstrengthsforœµS andœµEA. Figure7illustratestheresultsobtainedwith
Œ∏ Œ∏
thevariedhyperparameters,providingacomparisonwiththecompetingmethods.
TableS4: Metricsoverdifferenthyperparameterset. Tableshowsthemetricsoveradifferenthyperparameterset(T ,
skip
strength,injection).
CLIP-Text CLIP-Consistency FlowErr. LPIPS
‚Üë ‚Üì ‚Üì
Original ‚Äî 0.982 ‚Äî ‚Äî
Ours(8,14,85) 0.332 0.982 0.275 0.22
Ours(8,10,85) 0.329 0.982 0.252 0.159
24Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
F.ExtendedAttention
Self-attention(Linetal.,2017;Vaswanietal.,2017)isattentionmechanismthatallowsthemodeltorelatetodifferentparts
ofthesameinput. Inimages,self-attention,considerspixellocationinfeaturemapstocalculatesimilarcorrelation,fora
givenimage.
Givenavideoframe,xt,oritslatentrepresentation,z ,theselfattentionoperationstartsbyprojectingittoqueriesQ,keys
xt
K andvaluesV ofdimensiondbyusinglearnableprojectionmatrices,WQ,WK,WV. Theattentionisthencomputedas
follows
(cid:18) QKT(cid:19)
Attention(Q,K,V)=Softmax V. (S1)
‚àöd ¬∑
Thisself-attentionmechanismissuitedforanimage,orasingleframe,butnotforhandlingmultipleframestogether. Thus,
anextendedattentionmechanism,asparseversionofcausalattention,applyingtheattentionmechanismtomultipleframes
wassuggestedbyWuetal.(2023a)forvideoediting. Thisattentionmechanismorsomeversionofit,dubbedinprevious
andconcurrentworksasspatiotemporalattention,cross-frameattentionandextendedattention(asweuseinthiswork),
isusedinvariousimageeditingmethods,onesthatrequiretuningandzero-shotalike(Wuetal.,2023a;QIetal.,2023;
Ceylanetal.,2023;Liuetal.,2023;Wangetal.,2023;Yangetal.,2023;Geyeretal.,2024;Xingetal.,2023)
Extendedattentionenablestheattentionmoduletoprocessmultipleframestogether,resultinginanattentionmapwith
correspondencebetweenmultipleframes. Givenmultiplevideoframes, xt,xt+a,xt+b,xt+c,... where a,b,c,... ,are
{ } { }
somescalars,ortheirlatentrepresentation z ,z ,z ,z ,... ,thequeries,keysandvaluesare
xt xt+a xt+b xt+c
{ }
Q=WQ z ,
xt
¬∑
KE =WK [z ,z ,z ,z ,...], (S2)
xt xt+a xt+b xt+c
¬∑
VE =WV [z ,z ,z ,z ,...],
xt xt+a xt+b xt+c
¬∑
where[]denotestheconcatenationoperation.
¬∑
Thus,theextendedattentioncanbeformulatedas
(cid:18) QKE(cid:19)
Extended-Attention(Q,KE,VE)=Softmax VE. (S3)
‚àöd ¬∑
Ourextendedattentioniscalculatedbetweeneachframeandasetofthreekey-frames,consistingofoneglobalframes
positionedathalfofthevideolength,alongwithtwolocalframeswhicharechosenfromwithintheprocessingbatch. Our
extendedattentionimplementationisillustratedinFig.S8.
Notethatwhileweimplementextendedattentioninalltransformerblocks,weuseextendedattentioninjectiononlyin
layers4-11intheupsampleblocksoftheU-Net,similarlyto Tumanyanetal.(2023).
25Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
(a)
Processing batch
Frame Global key Frame Local key Frame
(b) CrossAttnBlock2D
Transformer2DModel
cross attention A
self attention E
ResnetBlock
Transformer2DModel
cross attention
A
self attention E
ResnetBlock
Transformer2DModel
cross attention
A
self attention E
FigureS8: Extendedattention. (a)Wecalculatetheextendedattentionbetweeneachframe(circle)withinaprocessing
batch(rectangle), andasetofthreekey-frames. Inourimplementation, theprocessingbatchis6. Thekey-framesare
composedofafixedglobalkey-frame(redcircles)andtwolocalkey-frames(bluecircles). Theglobalkey-frameislocated
athalfofthevideoduration. TheLocalkeyframesarethe2ndand5thframeswithintheprocessingbatch. (b)TheU-Net
network(left)iscomposedofaCrossAttnBlock2D(right)ineachlayer. Asshown,allselfattentionlayersarechangedinto
extendedattention(markedingray).
26Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
G.DenoiserExperiment
WeevaluatedtheperformanceoftheStableDiffusiondenoiseracrossspatiotemporalslicesinanexperiment. Forthis
experiment, we used 80 natural videos with different type of humans, animals and vehicles motion and of different
environments,takenfromhttps://www.pexels.com/videos/. Initially,weencodedeachvideoframebyframe
intothelatentspaceofStableDiffusionusingapretrainedencoder. Fromtheselatentvideos,weextractedframesand
spatiotemporalslices(y tslices). Additionally,werandomlypermutedthepixelsofeachextractedframetocreatea
‚àí
permutedframe. Thepermutedframes,whichhavenoise-likepatterns,serveasanoutofdistributionexamplsetovalidate
ourexperiment. Inthelatentspaceweaddeddifferentlevelsofnoise,followingStableDiffusionnoisescheduler,toeach
frame,spatiotemporalsliceandpermutedframe. WethenusedStableDiffusionpretraineddenoisertopredicttheadded
noise. Werepeatedthisprocessacrossvideosfor10000samplesandcalculatedthemeansquareerror(MSE)betweenthe
predictednoiseandtheaddednoise.
Thisexperimentresults,displayedinFig.3,illustratethatStableDiffusionpretraineddenoisercansuccessfullypredictthe
noiseaddedtospatiotemporalslices. Itcanbeseenthatforhighnoiselevelstheresultsforframes,spatiotemporalslicesand
permutedframesarealmostthesame. Forlowerlevelofnoise,itcanbeseenthatStableDiffusiondenoisercansuccessfully
predictthenoiseaddedtospatiotemporalslices,andthatitevenpredictsitbetterthanitpredictsthenoiseaddedtoframes.
Italsocanbeseenthatthepretraineddenoiserstruggleswithpredictingthenoiseaddedtothepermutedframes,whichare
completelyoutofdistributionexamples.
Asweusedthepretraineddenoiseroverthespatiotemporalslicesonlytoinducesmoothnessalongthetemporaldirection,
itsabilitytosuccessfullypredicttheaddednoiseisallweneed.
27Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
H.AlgorithmPseudoCode
InthissectionweprovidepseudocodeforeditingavideousingSlicedit. WefirstcoverthepseudocodeforDDPMinversion
andeditingusingDDPMinversion,aspresentedbyHuberman-Spiegelglasetal.(2023),asitisakeycomponentofour
videoeditingalgorithm. ThenweprovidethepseudocodeforSlicedit.
H.1.EditingusingDDPMInversion
WenotethatthissectionisnotadetailedexplanationabouteditingusingDDPMinversion,butashorterone,relyingon
somepreviousknowledgeaboutdiffusionreverseandforwardprocess. ForadetaileddiscussionaboutDDPMinversion
seeHuberman-Spiegelglasetal.(2023).
Generatinganimageusingdiffusionprocessstartsfromarandomnoisevector,x (0,I)anditerativelydenoisesit
T
‚àºN
usingtherecursion
x =¬µÀÜ (x )+œÉ z , œÑ =T,...,1 (S4)
œÑ‚àí1 œÑ œÑ œÑ œÑ
where z areiidstandardnormalvectorsand
œÑ
{ }
¬µÀÜ (x ,p)=‚àöŒ±¬Ø P(œµ (x ,p))+D(œµ (x ,p)). (S5)
œÑ œÑ œÑ‚àí1 Œ∏ œÑ Œ∏ œÑ
Where, œµ (,p) is the denoiser, p is a text prompt, P(œµ (x )) = (x ‚àö1 Œ±¬Ø œµ (x ))/‚àöŒ±¬Ø is the predicted x and
Œ∏ Œ∏ œÑ œÑ œÑ Œ∏ œÑ œÑ 0
¬∑ (cid:112) ‚àí ‚àí
D(œµ (x ))= 1 Œ±¬Ø œÉ 2œµ (x )isadirectionpointingtox .
Œ∏ œÑ œÑ‚àí1 œÑ Œ∏ œÑ œÑ
‚àí ‚àí
Inordertoeditarealsignal, x , itfirstneedstobeinvertedintoadiffusionmodelnoisespace, meaningtoextractthe
0
x ,z ,...,z thatproducedthissignal.Thesenoisevectorsareinthelatentspace,meanstheirdimensionsare64 64 4.
T T 1
{ } √ó √ó
Huberman-Spiegelglasetal.(2023)proposedtheeditfriendlyDDPMnoisespace,whichcanbeextractedusingAlg.1. In
Alg.1thedenoiserisconditionedonatextpromptdescribingtherealsignal,p ,however,thenoisespacecanbeextracted
src
withouttextconditioning.
Algorithm1EditfriendlyDDPMinversion
Input: x ,p
0 src
Output: x ,z ,...,z
T T 1
{ }
forœÑ =1toT do
œµÀú (0, 1)
‚àºN
x ‚àöŒ±¬Ø x +‚àö1 Œ±¬Ø œµÀú
œÑ œÑ 0 œÑ
‚Üê ‚àí
endfor
forœÑ =1toT do
z (x ¬µÀÜ (x ,p ))/œÉ //shape 64 64 4
œÑ œÑ‚àí1 œÑ œÑ src œÑ
‚Üê ‚àí √ó √ó
endfor
return x ,z ,...,z
T T 1
{ }
Afterextractingthenoisespace,thesignalcanbeeditedbyapplyingthediffusionprocessandconditionthedenoiserona
targetprompt,describingthedesirededit,assummarizedinAlg.2.
Algorithm2Editing
Input: x ,z ,...,z ,p
T T 1 tar
{ }
Output: xÀú
0
xÀú x
T T
‚Üê
forœÑ =T to1do
xÀú ¬µÀÜ (xÀú ,p )+œÉ z
œÑ‚àí1 œÑ œÑ tar œÑ œÑ
‚Üê
endfor
returnxÀú
0
28Slicedit:Zero-ShotVideoEditingWithText-to-ImageDiffusionModelsUsingSpatio-TemporalSlices
H.2.Slicedit
Sliceditfollowsasimilarstructure,mainlyfirstinvertingtheinputvideo,I ,consistingN frames,intoeditfriendlynoise
0
spaceandthenapplyingthediffusionprocess,whileconditioningthedenoiseronatextprompt. Akeydifferenceisthe
denoiseritself,asinSliceditthedenoiserisœµV(,p),whichisdescribedindetailinSec.4.
Œ∏ ¬∑
Theresultingalgorithm,Alg.3,givenwiththenotationsfromthemainpaperisasfollows
Algorithm3Sliceditvideoediting
Input: I ,p ,p
0 src tar
Output: J
0
// Video inversion
forœÑ =1toT do
œµÀú (0, 1)
‚àºN
I ‚àöŒ±¬Ø I +‚àö1 Œ±¬Ø œµÀú
œÑ œÑ 0 œÑ
‚Üê ‚àí
endfor
forœÑ =1toT do
Z (I ¬µÀÜV(I ,p ))/œÉ //shape N 64 64 4
œÑ ‚Üê œÑ‚àí1 ‚àí œÑ œÑ src œÑ √ó √ó √ó
Q ,K œµV(,p ) // Keep keys and queries of the input video
œÑ œÑ ‚Üê Œ∏ ¬∑ src
endfor
// Video editing
J I
T T
‚Üê
forœÑ =T to1do
œµV(,p ) Q ,K // Inject input video keys and queries to the edited video
Œ∏ ¬∑ tar ‚Üê œÑ œÑ
J ¬µÀÜV(J ,p )+œÉ Z
œÑ‚àí1 ‚Üê œÑ œÑ tar œÑ œÑ
endfor
returnJ
0
where¬µÀÜV œÑ( ¬∑,p)=‚àöŒ±¬Ø œÑ‚àí1P(œµV Œ∏( ¬∑,p))+D(œµV Œ∏( ¬∑,p))andœµV Œ∏( ¬∑,p)isdefinedinEq.1.
29