Configurable Mirror Descent: Towards a Unification of Decision Making
PengdengLi1 ShuxinLi1* ChangYang2* XinrunWang1 ShuyueHu3 XiaoHuang2 HauChan4 BoAn15
Abstract 1.Introduction
Decision-makingproblems,categorizedassingle- Decision-makingproblemsarepervasiveintherealworld
agent, e.g., Atari, cooperative multi-agent, e.g., (Sutton&Barto,2018;Shoham&Leyton-Brown,2008),
Hanabi, competitivemulti-agent, e.g., Hold’em whichcanbegenerallycategorizedintosingle-agent,e.g.,
poker, and mixed cooperative and competitive, Atari (Mnih et al., 2015), cooperative multi-agent, e.g.,
e.g., football, are ubiquitous in the real world. Hanabigame(Bardetal.,2020),competitivemulti-agent,
Variousmethodsareproposedtoaddressthespe- e.g., Hold’em poker (Brown & Sandholm, 2018; 2019),
cificdecision-makingproblems. Despitethesuc- andmixedcooperativeandcompetitive(MCC),e.g.,foot-
cessesinspecificcategories,thesemethodstyp- ball(Kurachetal.,2020;Liuetal.,2022a). Tosolvethese
icallyevolveindependentlyandcannotgeneral- problems,variousmethodsareproposedwherenotableex-
ize to other categories. Therefore, a fundamen- amplesincludePPO(Schulmanetal.,2017)forsingle-agent
talquestionfordecision-makingis: Canwede- category,QMIX(Rashidetal.,2018)forcooperativemulti-
velopasinglealgorithmtotackleALLcategories agentcategoryandPSRO(Lanctotetal.,2017)forcompeti-
ofdecision-makingproblems? Thereareseveral tivecategory. Despitethesuccessesinspecificcategories,
mainchallengestoaddressthisquestion: i)dif- thesemethodsaredevelopedalmostindependentlyandcan-
ferentdecision-makingcategoriesinvolvediffer- notgeneralizetoothercategories. Therefore,afundamental
entnumbersofagentsanddifferentrelationships questionfordecisionmakingtoansweris:
betweenagents,ii)differentcategorieshavedif-
CanwedevelopasinglealgorithmtotackleALL
ferentsolutionconceptsandevaluationmeasures,
categoriesofdecision-makingproblems?
andiii)therelacksacomprehensivebenchmark
coveringallthecategories. Thisworkpresentsa Agent Solution
Cooperative
preliminaryattempttoaddressthequestionwith Number Multi-agent Concepts
threemaincontributions. i)Weproposethegener-
Single Decision MCC
alizedmirrordescent(GMD),ageneralizationof
Agent Making Multi-agent
MDvariants,whichconsidersmultiplehistorical
policiesandworkswithabroaderclassofBreg- Competitive
Agent Multi-agent Evaluation
mandivergences. ii)Weproposetheconfigurable Relations Measures
mirror descent (CMD) where a meta-controller
is introduced to dynamically adjust the hyper- Figure1.Overviewofthecategoriesofdecisionmakingandthe
parametersinGMDconditionalontheevaluation fourdesideratafortherequiredmethodtosatisfy.
measures. iii) We construct the GAMEBENCH
with 15 academic-friendly games across differ- Thereareseveralcriticalchallengestoaddressthisfunda-
entdecision-makingcategories. Extensiveexper- mentalquestion. First,thedifferentcategoriesofdecision-
iments demonstrate that CMD achieves empiri- makingproblemsincludedifferentnumbersofagentsand
callycompetitiveorbetteroutcomescomparedto differentrelationshipsbetweenagents. Thereisoneagent
baselineswhileprovidingthecapabilityofexplor- forthesingle-agentcategory,whilemultipleagentsforthe
ingdiversedimensionsofdecisionmaking. other three categories, therefore, the reinforcement learn-
ingmethods,e.g.,PPO,mainlydevelopedforsingle-agent
*Equalcontribution 1NanyangTechnologicalUniversity2The decision-making problems, cannot be directly applied to
Hong Kong Polytechnic University 3Shanghai Artifcial Intelli- multi-agentcategories. Furthermore,evenformulti-agent
genceLaboratory4UniversityofNebraska-Lincoln5SkyworkAI. categories,QMIX(Rashidetal.,2018)isdevelopedtohan-
Correspondenceto:XinrunWang<xinrun.wang@ntu.edu.sg>.
dlethecooperativemulti-agentcategoryandcannotbeap-
Proceedings of the 41st International Conference on Machine pliedtothecompetitivecategory.Second,differentdecision-
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by makingcategorieshavedifferentsolutionconcepts,where
theauthor(s). theoptimal(joint)policyisconsideredinthesingle-agent
1
4202
yaM
02
]IA.sc[
1v64711.5042:viXraConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
andcooperativemulti-agentcategories,whileforthecom- 2.AReal-WorldMotivatingScenario
petitiveandMCCmulti-agentcategories,Nashequilibrium
Weprovideanillustrativeexampletohighlighttheimpor-
(NE) (Nash, 1951) is the canonical solution concept and
tance and real-world implications of a unified algorithm
other solution concepts, e.g., correlated equilibrium (Au-
framework. Considerthataroboticcompanyisdeveloping
mann, 1987) are also considered. Furthermore, even for
and selling generalist domestic robots to users. The user
onesolutionconcept,e.g.,NE,therearedifferentevaluation
mayasktherobottolearntocompletedifferentnoveltasks,
measures,e.g.,NashConvorNashConvwithsocialwelfare
includingsingle-agent,cooperative,competitive,andMCC
andfairness1. Tosummarizethechallenges,weproposethe
categories,byspecifyingtheobjective. Therefore,ifwecan
fourdesideratathatthemethodsshouldsatisfy:
deployaunifiedalgorithmintotherobot,therobotcanlearn
tocompletedifferentnoveltaskswithasinglealgorithm.
• D1: Applicabletosingle-andmulti-agentcategories
• D2: Applicabletocoop.,comp.,&MCCcategories Developinganddeployingsuchaunifiedalgorithmwould
benefit both the development and users. For the develop-
• D3: Applicabletodifferentsolutionconcepts
mentside,asonlyasinglepolicylearningruleisrequired,
• D4: Applicabletodifferentevaluationmeasures
thedeploymentanduserinterfacedesigncouldbelargely
simplified,whichwouldbemorecost-efficientthandeploy-
An overall illustration of the categories of the decision
ingdifferentspecializedalgorithmssuchasMAPPOand
makingandthedesiderataisdisplayedinFigure1. Third,
PSROastheymaycomplicatethedevelopmentpipelineand
existingbenchmarksaretypicallyspecializedforspecific
userinterfacedesign. Fortheuserside,theuseronlyneeds
decision-makingcategories,whileacomprehensivebench-
toconfigureonesetofparametersfordifferentnoveltasks,
markthatsatisfiesthefollowingtwodesiderataislacking.
e.g.,onlyneedstospecifytheoptimizationobjectiveofthe
meta-controllerinourproposedCMDalgorithm.
• D5: (Comprehensive)Itcoversallcategories
• D6: (Academic-friendly)Itislessresource-intensive
3.RelatedWork
Inthiswork,wemakeapreliminaryattempttoaddressthese Therelatedliteratureistoovasttocoverinitsentirety. We
challengesandprovidethreemaincontributions. i)Wepro- presentanoverviewbelowtoemphasizeourcontributions
posethegeneralizedmirrordescent(GMD),ageneralization whilemorerelatedworkscanbefoundinAppendixB.
of existing MD algorithms (Nemirovskij & Yudin, 1983;
DecisionMaking. Substantialprogresshasbeenachieved
Beck&Teboulle,2003),whichincorporatesmultiplehistor-
indevelopingalgorithmstoaddressdifferentcategoriesof
icalpoliciesintothepolicyupdatingandisabletoexplore
decision-makingproblems,e.g.,DQN(Mnihetal.,2015)
abroaderclassofBregmandivergencebyaddressingthe
andPPO(Schulmanetal.,2017)forsingle-agentcategory,
Karush–Kuhn–Tucker(KKT)conditionsateachiteration.
QMIX(Rashidetal.,2018)andMAPPO(Yuetal.,2022)for
AsGMDisadoptedbyeachagentindependently,itcanbe
cooperativemulti-agentcategory,self-play(Tesauroetal.,
appliedtodifferentdecision-makingcategoriesinvolving
1995)andPSRO(Lanctotetal.,2017)forcompetitiveand
differentnumbersofagentsanddifferentrelationshipsbe-
MCCcategories,tonamejustafew. Despitethesuccesses
tweenagents(D1andD2). ii)Weproposetheconfigurable
inspecificcategories,thesemethodsoftencannotdirectly
mirrordescent(CMD)byintroducingameta-controllerto
generalizetodifferentcategories. Inthiswork,wemakea
dynamically adjust the hyper-parameters in GMD condi-
preliminaryattempttodevelopasinglealgorithmcapableof
tionalontheevaluationmeasures,allowingustostudydif-
tacklingallcategoriesofdecision-makingproblemswhich
ferentsolutionconceptsaswellasevaluationmeasures(D3
typicallyinvolvedifferentnumbersofagents,differentre-
andD4),withminimalmodifications. iii)Weconstructthe
lationshipsbetweenagents,differentsolutionconceptsas
GAMEBENCHconsistingof15gameswhichcoverallthe
wellasdifferentevaluationmeasures.
decision-makingcategories(D5)andaredeliberatelycon-
structedwiththeprinciplethatrunningalgorithmsonthese Mirror Descent. Mirror descent (MD) (Nemirovskij &
gamesdoesnotrequiremuchcomputationalresource(D6), Yudin,1983;Beck&Teboulle,2003;Vuraletal.,2022)has
andhence,formingacomprehensiveandacademic-friendly showneffectivenessinlearningoptimalpoliciesinsingle-
testbedforresearcherstoefficientlydevelopandtestnovel agentRL(Tomaretal.,2022)andprovedthelast-iteratecon-
algorithms. Extensive experiments on the GAMEBENCH vergenceinlearningapproximateequilibriuminzero-sum
showthatCMDachievesempiricallycompetitiveorbetter games(Bailey&Piliouras,2018;Kangarshahietal.,2018;
outcomescomparedtobaselineswhileofferingtheability Wibisonoetal.,2022;Kozunoetal.,2021;Leeetal.,2021;
toinvestigatediversedimensionsofdecisionmaking. Jainetal.,2022;Aoetal.,2023;Liuetal.,2023;Cenetal.,
2023;Sokotaetal.,2023)andsomeclassesofgeneral-sum
1Thisisrelatedtotheequilibriumselectionproblem(Harsanyi
games,e.g.,polymatrixandpotentialgames(Anagnostides
etal.,1988)anddifferentmeasuresleadtodifferentequilibria.
2ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
etal.,2022b). Despitetheprogress,existingworkstypically wehaveπ Π . Thejointpolicyofallagentsisdenoted
i i
∈
focusonsomespecificBregmandivergencesuchastheKL asπ =π π andπ ΠwhereΠdenotesthejoint
1 N
⊙···⊙ ∈
divergence. WerelaxthispremisebyaddressingtheKKT policyspaceofallagents. Aspecialcaseofjointpolicyis
conditionsateachiteration,enablingustoexploreabroader theproductpolicydenotedasπ =π π . Also,let
1 N
×···×
classofBregmandivergence. Moreover,byintroducinga π =π π π π denotethejointpolicy
i 1 i 1 i+1 N
− ⊙··· − ⊙ ···⊙
meta-controllertodynamicallyadjustthehyper-parameters, of all agents except i. Given the initial state s = s, the
0
ourCMDcanbeappliedtodifferentsolutionconceptsand valuefunctionofagentiisV i(s,π):=E[ ∞ t=0γtr it |s,π]
evaluationmeasureswithminimalmodifications. wherertistheagenti’srewardattimet 0. Furthermore,
wehavei V (ν,π):=E [V (s,π)]. ≥(cid:80)
Hyper-Parameter Tuning. Existing works typically de- i s ∼ν i
terminethehyper-parametervaluesoftheMDalgorithms Solution Concepts. The policy of an agent is said to be
dependingondomainknowledge(Sokotaetal.,2023;Anag- optimalifitisoptimalineverydecisionpointbelongingto
nostidesetal.,2022b;Hsiehetal.,2021),whichmaynot theagent. Insingle-agentandcooperativecategories,this
beeasytogeneralizetodifferentgames. Ontheotherhand, optimalpolicymaximizestheexpectedreturnfortheagent
gradient-based hyper-parameter tuning methods such as ortheteam. Inmulti-agentcompetitiveandmixedcoopera-
STAC(Zahavyetal.,2020)arelessapplicableastheevalu- tiveandcompetitivecategories,weconsidertwocommon
ationmeasures,e.g.,NashConv,couldbenon-differentiable equilibriumconcepts: Nashequilibrium(NE)(Nash,1951)
withrespecttothehyper-parameters. Toaddresstheissue, andcoarsecorrelatedequilibrium(CCE)(Moulin&Vial,
weproposeasimpleyeteffectivezero-orderoptimization 1978). Letπ π denotetheproductpolicyandπ π
i i i i
× − ⊙ −
methodwheretheperformancedifferencebetweentwocan- denotethejointpolicy. Then,π iscalledanNEifforeach
∗
didatesisusedtoonlydeterminetheupdatedirectionofthe agentiitsatisfies: π Π ,V (ν,π ) V (ν,π π ).
hyper-parametersratherthantheupdatemagnitude,which Similarly,π
iscal∀ ledi′ a∈ CCi Eifi forea∗ ch≥ agei ntiiti′ s× atisfi−∗ ei
s:
∗
ismoreeffectivethanexistingmethods(Wangetal.,2022) π Π ,V (ν,π ) V (ν,π π ).
whenthevalueoftheperformanceisextremelysmall.
∀
i′
∈
i i ∗
≥
i i′
⊙
−∗i
EvaluationMeasures. Let (π)denotemeasuresusedto
L
evaluatea(joint)policyπ. Insingle-agentandcooperative
4.Preliminaries categories,themeasureisthedistanceofthe(joint)policy
totheoptimal(joint)policyπ ,whichisdefinedas (π)=
In this section, we first introduce the model of decision ∗ L
OptGap(π)=V(ν,π ) V(ν,π). Inothercategories,we
makingandthesolutionconceptsandevaluationmeasures ∗ −
considermultipleevaluationmeasures. Thefirstoneisthe
consideredinourwork. Then,wepresenttheclassicmirror
distanceofthejointpolicytotheequilibrium(NEorCCE).
descentalgorithm(Beck&Teboulle,2003).
For NE, we refer to this distance as NashConv, and for
CCE,werefertoitasCCEGap,asisconventioninprevious
4.1.DecisionMaking
works (Lanctot et al., 2017; Marris et al., 2021). More
POSG. A decision-making problem, either single-agent, specifically,wehaveNashConv(π)= i [V i(ν,π iBR ×
c po eto ip tie vr ea cti av te e, goc ro ym ,cp ae ntit biv ee d, eo scr rm ibi ex ded asc ao po ap re tr iaa lt li yve oba sn ed rvc ao bm le- π π− −i i)
)
−− VV i(i ν(ν ,π,π )]) ,] wa hn ed reC πC iBE RG isap th( eπ b) es=
tre
(cid:80)(cid:80) spi o∈∈ nNN se[V (i B( Rν, )π piB oR lic⊙
y
stochasticgame(POSG)(Oliehoek&Amato,2016)denoted ofagentiagainstallotheragents. Thesecondevaluation
as , , , ,Ω,P,R,γ,ν . = 1, ,N istheset measureweconsideristhesocialwelfare(SW)(Davis&
ofa⟨ =N genS ts.A
S
iO st wh he efi rn eitese at no df⟩ thN e as rt eat te h{ s e. fiA· n· i· =
tes×
ei t}
∈ oN faA
ci tia on nd
s
Whinston,1962),denotedas L(π)= i
∈N
V i(ν,π).
O ×i ∈NOi Ai Oi (cid:80)
andobservationsofagenti,respectively. Leta denote 4.2.MirrorDescent
∈A
thejointactionofagentswherea isagenti’saction.
i ∈Ai From a single agent’s perspective, the condition for the
Ω = Ω whereΩ : istheobservation
×i ∈N i i S ×A → Oi optimalorequilibriumpolicycanbeexpressedbythefol-
functionspecifyingagenti’sobservationo whenall
i ∈Oi lowingoptimizationproblemateachdecisionpointofthe
agentstakea atstates .P : ∆( )isthe
∈A ∈S S×A→ S agent(Tomaretal.,2022;Sokotaetal.,2023): τt t,
transitionfunctionwhichspecifiestheprobabilityoftransit- ∀ i ∈Ti
i dn eg noto tes
s′ th∈ eS
simw ph le exn .a Rge =nts rta ike
i
a
∈ whA
era et rs ita :tes ∈S. ∆ R(
i·
s) π i∗(τ it)=ar πg i∈m Πa ixE
a ∼πi(τ
it)Q(τ it,a,π
i
⊙π −∗i), (1)
{ }∈N S×A→
t fh ace tr oe rw
.
νardf ∆un (cti )o dn eo nf oa teg sen tht ei da in sd triγ bu∈ tio[0 n,1 ov) ei rs it nh ie tid ai ls sc to au ten st
.
w ach te iore n-Q va( lτ uit e,a fu, nπ c) ti= onE o[ fth∞ h e= at+ ct1 ioγ nhr aih |τ it,at i at= tha e, dπ e] ci is sit oh ne
Attimest∈ ept S 0,eachagenthasanaction-observationhis- pointτt. Withoutlosso(cid:80) fgenerality,w∈ ewA ii llonlyfocuson
≥ i
tory(i.e.,adecisionpoint)τ it ∈Titwhere Tit =( Oi ×Ai)t thepolicylearningofasingleagentiinasingledecision
a on wd nc ro en tus rt nru .c Lts etit Πs ip do eli nc oy teπ ti h: eT pi ot li→ cy∆ sp( aA cei) ot fo agm ea nx ti im ,ti hz ae ti it ss , p asoi tn ht eτ yit ar∈ eT cli et aa rn fd roh men tc he efo cr oth n, tet xh te ,i an nd dex wi ita hn ad sτ li it ga hr te ai bg un so ere od f
3ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
Table1.Comparisonofdifferentmethods.∗NotethatMMDcanberegardedasthemethodthatcanconsidermultiplepreviouspolicies
bysettingthemagnetpolicytotheinitialpolicy(typicallyauniformpolicy).
Multipleprevious Workingonany Workingonany Configurablefor
Method
policies Bregmandivergence solutionconcept anymeasure
MD(Nemirovskij&Yudin,1983) (cid:37) (cid:37) (cid:37) (cid:37)
MMD (Sokotaetal.,2023) (cid:33) (cid:37) (cid:37) (cid:37)
∗
GMD(Thiswork) (cid:33) (cid:33) (cid:37) (cid:37)
CMD(Thiswork) (cid:33) (cid:33) (cid:33) (cid:33)
notation,wedenote theactionset ofagenti,π Π Meta-Controller (MC)
i
A A ∈
theagent’spolicy,andQ(a)(orQ(a,π))theaction-value
GMD
π1 α1
of the action a . Then, to learn the optimal or equi- ···
∈ A .
libriumpolicy,weaimtosolvetheoptimizationproblem: ··· . . π th∗ is= pra or bg lem max isπ ∈ thΠ eE ma i∼ rrπ oQ r( da e) s. cA enf te (a Msi Dbl )e ,m we ht ih co hd tato kes so tl hv ee upα
date
eπ v1 a·l·u·aπ tD
e
πD GMD sam·· p·
le
αD
form(Beck&Teboulle,2003;Tomaretal.,2022):
α α
M 1 0
− ······
π k+1 =argmax π ∈Π ⟨Q(π k),π ⟩−α Bϕ(π,π k), (2) π k −M+1
······
π k
where 1 k K is the iteration, Q(π ) is the action-
π k+1=argmax
π
∈Π⟨Q(π k),π
⟩−
M τ=−01α τBϕ(π,π
k
−τ)
k
valuevect≤ orind≤ ucedbyπ k (forsimplicity,weletQ(π k)= Generalized Mirror DesP cent (GMD)
(Q(a,π )) ),αistheregularizationintensity, isthe
k a ϕ
Bregmandi∈ vA ergencewithrespecttothemirrormB apϕ,de- Figure2. OverviewofCMD.
finedas (x;y)=ϕ(x) ϕ(y) ϕ(y),x y with
ϕ
B − −⟨∇ − ⟩ ⟨·⟩
beingthestandardinnerproductandx,y ∆( ).
∈ A 5.1.GeneralizedMirrorDescent
ThoughexistingMDalgorithms,e.g.,Eq. (2),canbealso
5.ConfigurableMirrorDescent
executedbyeachagentindependently,theycouldnotgen-
In this section, we propose a novel algorithm which sat- eralizewelltosatisfythedesiderataD1andD2. Themain
isfies the four desiderata (D1–D4) presented in the Intro- reasonsaretwo-fold. First,classicMDalgorithms(Beck&
duction. First,weproposethegeneralizedmirrordescent Teboulle,2003;Tomaretal.,2022)typicallyonlyconsider
(GMD),ageneralizationofexistingMDalgorithms,which thecurrentpolicywhenderivingthenewpolicyateachitera-
whenindependentlyexecutedbyeachagent,caneffectively tion.However,ithasbeenshownthatincorporatingmultiple
tackledifferentdecision-makingcategoriesinvolvingdiffer- previouspolicies(e.g.,theinitialandcurrentpolicies)could
entnumbersofagentsanddifferentrelationshipsbetween bepowerfulinsolvingtwo-playerzero-sumgames(Sokota
agents(D1andD2). Second,weproposetheconfigurable etal.,2023;Liuetal.,2023). Second,eventhoughmultiple
mirrordescent(CMD)whereameta-controllerisintroduced previouspoliciesareconsidered,existingMDalgorithms
todynamicallyadjustthehyper-parametersofGMDcondi- typicallyfocusonsomespecificBregmandivergencesby
tionalontheevaluationmeasures,whichcanbeconfigured restrictingϕtocertainconvexfunctionswhichmaynotbe
toaccountfordifferentsolutionconceptsaswellasevalu- the optimal choices across different decision-making cat-
ationmeasures(D3andD4),withminimalmodifications. egories. Toaddressthesechallenges, weproposeamore
CMDsharessimilaritieswiththecentralizedtrainingand generalMDmethodsatisfyingthedesiderataD1andD2.
decentralizedexecution(CTDE)(Loweetal.,2017)since
AGeneralMDMethod. WeproposeamoregeneralMD
themeta-controllerconsidersallagentstooptimizethetar-
approach which takes multiple historical policies into ac-
getedevaluationmeasures(“centralized”trainingfromthe
countwhenderivingthenewpolicy,asgivenbelow:
controller’sperspective)whileGMDisexecutedbyeach
agentindependently(“decentralized”executionfromeach M 1
π =argmax Q(π ),π − α (π,π ),
k+1 k τ ϕ k τ
agent’s perspective). The overview of CMD is shown in π Π ⟨ ⟩− τ=0 B −
Figure2andTable1presentsacomparisontomoreclearly ∈ (cid:88)
s.t. π (a)=1andπ (a) 0, a , (3)
k k
positionourmethodsinthecontextofrelatedliterature. a ≥ ∀ ∈A
∈A
(cid:88)
4
)π(
LConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
where M 1 is the number of historical policies, α exploringabroaderclassofBregmandivergenceas,viathe
τ
≥ ∈
(0,1]istheregularizationintensityofπ ,0 τ M 1, numericalmethodtocomputethevalueofλ,itiscapableof
k τ
− ≤ ≤ −
andletα=(α ) . Notethatsolvingtheproblem takingmorepossibleconvexfunctionsintoaccount. Table2
τ 0 τ M 1
≤ ≤ −
(3)toderivethepolicyupdatingrulecouldbechallenging. presentsthefunctionsconsideredinourwork. See(Boyd&
Inpractice,theproblemcouldhaveaclosed-formsolution Vandenberghe,2004)formoreexamples. x2 (i.e.,n = 2)
onlyincertainsettingssuchastheconvexfunctionϕisthe andxlnxrespectivelycorrespondtotheEuclideannorm
negativeentropyandtheconstraintsareremoved(i.e.,the andentropy. MoredetailscanbefoundinAppendixD.1.
unconstraineddomains(Sokotaetal.,2023)). Toaddress
thisissue,weproposeanovelmethodtosolvetheproblem Table2.Listofconvexfunctionsandrelatedfunctions,x (0,1].
∈
(3), which does not rely on the availability of the closed-
ψ(x) ψ (x) ψ 1(x) [ψ 1](x)
formsolutionandhence,canconsidermorepossibleoptions ′ ′− ′− ′
ofϕ. Tothisend,first,wehavethefollowingresult: xn,n>1 nxn −1 ( nx)n−1 1 n1 1( nx)2 n− −n 1
Proposition5.1. Assumethati)π(a) ≥ϵ, ∀a ∈A,whereϵ xlnx lnx+1 ex −1 − ex −1
isasmallpositivevalueandii)theϕ(π)definedonΠcanbe
d coe nc vo em xp fo unse cd tioto n2 dϕ efi(π ne) d= on[0a ,∈1A].ψ T( hπ en(a ,) s) olw vih ne gre thψ epis ros bo lm eme 0<− nxn <,
1
−nxn −1 ( −nx)n−1 1
1
−1 n( −nx)2 n− −n 1
(cid:80)
(3)canbeconvertedtosolvethefollowingequation: ekx,k >0 kekx ln(x/k)/k 1
x
π(a)= ψ ′−1(A(a) λ)/B)=1, (4)
a a − GMDSummary. Thepseudo-codeofGMDisprovidedin
∈A ∈A
(cid:88) (cid:88) Algorithm1. ComparedtoexistingMDalgorithms,GMD
whereA=Q(π k)+ M τ=− 01α τϕ ′(π k τ),B = M τ=− 01α τ, couldsatisfywellthedesiderataD1andD2asitnotonly
λisthedualvariable,ψ ′−1istheinver−sefunctionofψ ′(the
takesmultiplepreviouspoliciesintoaccountbutisalsoca-
(cid:80) (cid:80)
derivativeofψ),andπ(a)=ψ ′−1(A(a) λ)/B).
pableofleveragingmorepossibleBregmandivergencesthat
−
maybebetterthanexistingchoicessuchasKLdivergence
ThisresultisobtainedviatheKarush–Kuhn–Tucker(KKT)
acrossdifferentdecision-makingcategories.
conditionsoftheLagrangefunctionobtainedbyapplying
theLagrangemultiplierλ(i.e.,thedualvariable)totheprob-
Algorithm1GeneralizedMirrorDescent(GMD)
lem(3). ThefullderivationcanbefoundinAppendixD.1.
1: Givenψ,initialpolicyπ 1,M,α,ϵ
Numerical Method for Computing λ. Now we need to 2: fork =1, ,K do
···
solveEq. (4)toobtainthevalueofλ. Unfortunately,this 3: ComputeAandBwithπ k andα
typically cannot be solved analytically, rendering it less 4: ComputeλviaNewtonmethod(Algorithm3)
possibletoderivethepolicyupdatingrulewithouttheavail- 5: Computeπ(a)=ψ ′−1(A(a) λ)/B), a
− ∀ ∈A
abilityoftheclosed-formsolution. Toaddressthisissue,we 6: Computeπ k+1(a)viaprojectionoperation, a
∀ ∈A
usetheNewtonmethod(Ypma,1995)tocomputethevalue 7: endfor
ofλ: repeatedlyexecutingλ=λ g(λ)/g (λ)forC >0
′
−
iterations,whereg(λ)= ψ 1(A(a) λ)/B) 1,
g (λ) = 1[ψ 1](Aa ∈(Aa) ′− λ)/B),− and[ψ 1]− is 5.2.Meta-ControllerforDifferentMeasures
th′ ederivativa e∈Aof− ψB 1.′− T(cid:2) h(cid:80)′ epseud− o-codecanbefo′ u− n(cid:3) d′
in
′− WhileGMDcansatisfythedesiderataD1andD2,itcannot
(cid:80)
Algorithm3inAppendixD.1.
satisfy well the last two desiderata D3 and D4. The pri-
ProjectionOperation. Aftercomputingthevalueofλ,we maryreasonisthatwheneachagentindependentlyexecutes
cangetthepolicyπ(a)bysubstitutingitintotheexpression GMD,itisnotimmediatelyfeasibletoinvestigatedifferent
ofπ(a)aspresentedinEq. (4). Furthermore,weemploya solution concepts and evaluation measures as no explicit
projectionoperationtoensurethatπ(a) ϵ. Specifically, objectiveregardingthedifferentmeasuresarisesinsucha
wehave: ∀a ∈A,π k+1(a)= (cid:80) a′m ∈Aax m{ aϵ, xπ {≥ ( ϵa ,π) } (a′) }. “ led ae dce tn otr da il fi fz ee rd e” ntex soec luu tt ii oo nn cp oro nc ce es ps ts(d ai nff der he en nt cm efe oa rs tu hr ,e ws eco wu il ld
l
DifferentBregmanDivergences. Inadditiontotakingmul- onlyfocusonthedifferentmeasures). Toaddressthisprob-
tiple historical policies into consideration, GMD3 further lem, we propose the configurable mirror descent (CMD)
generalizesexistingMDalgorithmswiththecapabilityof byintroducingameta-controller(MC)toadjustthehyper-
parametersinGMDconditionalontheevaluationmeasures,
2Thesumofconvexfunctionsisstillaconvexfunction.Further-
whichisa“centralized”processfromthemeta-controller’s
more,thenegativeentropyandsquaredEuclideannormaretwo
perspective as it considers all the agents (joint policy) to
specialvariantsthathavebeenextensivelyadoptedinliterature.
3ThetermGMDisalsousedin(Radhakrishnanetal.,2020), optimizethetargetedevaluationmeasure(andhence, the
whichdiffersfromourmethod. targetedsolutionconcept),i.e.,D3andD4.
5055
056
057
058
059
060
061
ConfigurableMirrorDescent:Tow0a6r2dsaUnificationofDecisionMaking
063
Zero-OrderMeta-Controller. AsshowninEq. (3),given064 6. GAMEBENCH
065
thenumberofhistoricalpoliciesM 1,theonlycontrol-066
lablevariableisthehyper-parameters≥ α=(α ) .067 Inthissection,wepresenttheGAMEBENCH,anovelbench-
Attheiterationk,letπ
=GMD(α)4denotetheτ jo0 ≤inτ t≤pM ol−ic1 y0 06 68
9
markwhichconsistsof15gamescoveringallcategoriesof
derivedfromthepreviousjointpolicyπ byusingGMD070 decisionmakingandincludesdifferentevaluationmeasures
k 071 anddifferentalgorithms,whichisshowninFigure3.
withthegivenα,andtheperformanceofthisjointpolicyis072
denotedas (π). Notably,optimizingα,unfortunately,is073
L 074 MirrorDescent CFR non-trivialastheevaluationmeasure isnon-differentiable075
L 076 CMD GMD MMD MD CFR CFR+
withrespecttoα. Toaddressthisissue,weconstructanef-
077 This This (Sokota (Nemirovskij& (Zinkevich (Tammelin,
ficientzero-orderMCbyleveragingazero-ordermethodto078 work work etal.,2023) Yudin,1983) etal.,2007) 2014)
079
optimizeα. AsshowninFigure2,MCupdatesαthrough 080 Optimality Equilibria
threesteps: i)itsamplesDcandidates αj D byperturb-081 OptGap SocialWalfare NashConv CCEGap
in πg jt =he Gc Murr De (n αt jα
)
a Dnd bth ye en md pe lr oi yv ie ns gD G{ Mne Dw} ,j i= j io )1 i in tt evp ao ll uic ai te es s0 0 08 8 82 3
4
V V(ν (, νπ ,∗ π) )− V(ν,π) Pπi −∈ iN)[ −Vi V(ν i(, νπ ,iB πR )× ] Pπi −∈ iN)[ −Vi V(ν i(, νπ ,iB πR )⊙ ]
{ thesenewjointpo} lij c= ie1 s {L(πj) }D j=1,andiii)itupdatesα0 08 85 6 Sin Kg ule h- na -g Aent TC ino yo Hp ae nra abti iv -Ae Ze Kro u- hs num G Ben are gr aa il n- is nu gm MCM CKC uC
hn-A
basedontheperformanceofthesenewjointpolicies. 087 Kuhn-B TinyHanabi-B Leduc TradeComm MCCKuhn-B
088 Goofspiel-S TinyHanabi-C Goofspiel Battleship Goofspiel
Direction-GuidedUpdate.
Letα1andα2denotethetwo089
090
candidates sampled by perturbing the current α and the091 Figure3. OverviewofGAMEBENCH.
corresponding joint policies π1 and π2 are obtained via092
093
GMD. Existing zero-order methods such as the random094 Motivation. Althoughvariousbenchmarkshavebeensug-
search(RS)(Liuetal.,2020)typicallyupdatetheαdirectly095 gestedinliterature,theyaretypicallyspecializedforspe-
096
basedontheperformancedifferencebetweenthetwocan-097 cific decision-making categories, e.g., Atari (Bellemare
didates δ = (π1) (π2), which could be ineffective098 etal.,2013)forsingle-agentcategory,Hanabi(Bardetal.,
asthevalueoL f co− uldL betoosmall(asshowninourex-0 19 09 0 2020)forcooperativecategory,Hold’empoker(Brown&
periments) to dL erive an effective update. To address this101 Sandholm,2018;2019)forcompetitivecategory,andfoot-
102
problem,weproposetoupdateαbasedonthesignofthe103 ball(Kurachetal.,2020)formixedcooperativeandcompet-
performance difference. Precisely, δ only determines the104 itivecategory. Ontheotherhand,asMDalgorithmsrequire
105
updatedirection,nottheupdatemagnitude,whichismore106 toexecutethepolicyupdatingateachdecisionpointateach
effective when the value of is too small. We call this107 iteration,runningthemontheexistingbenchmarkscouldbe
simpleyeteffectivetechniqueL thedirection-guidedupdate.1 10 08 9 resource-intensiveasthenumberofdecisionpointsinthe
Inourexperiments,weconstructanMC–direction-guided environmentscouldbeextremelylarge(e.2g.,itisimpractical
randomsearch(DRS)–byapplyingthismethodtotheexist- toenumeratetheobservationsinAtariastheyareimages).
ingRS(Wangetal.,2022). MoredetailsondifferentMCs
Desiderata. Motivatedbytheabovefacts,weconstructa
canbefoundinAppendixD.4.
newbenchmark–GAMEBENCH.Itsatisfiesthetwodesider-
ataD5andD6presentedintheIntroduction. Thatis,itcov-
Algorithm2ConfigurableMirrorDescent(CMD) ersallcategoriesofdecision-makingproblems(comprehen-
1: Given ,ψ,initial(joint)policyπ 1,M,D,ϵ sive),andrunningMDalgorithms(orotheralgorithmssuch
L
2: fork =1, ,K do asCFR(Zinkevichetal.,2007))onallthegamesdoesnot
···
3: SampleDcandidates αj D requiremuchcomputationalresource(academic-friendly).
{ }j=1
4: Derivenewjointpolicies πj =GMD(αj) D ThecomponentsofGAMEBENCHaregivenbelow.
{ }j=1
5: Evaluatenewjointpolicies {L(πj) }D j=1 Games. WecuratetheGAMEBENCHontopoftheOpen-
6: Updateαbasedon (πj) D Spiel(Lanctotetal.,2019). Thereare15gameswhichare
{L }j=1
7: Computeπ k+1viaGMDwiththeupdatedα dividedinto5categories: single-agent,cooperativemulti-
8: endfor agent,competitivemulti-agentzero-sum,competitivemulti-
agentgeneral-sum,andmixedcooperativeandcompetitive
(MCC)categories. InourGAMEBENCH,theoriginalcom-
CMDSummary. ByincorporatingtheMCintoGMD,we
petitivecategoryisfurtherdividedintotwosubcategories–
establishtheCMD.Intuitively,CMDcanbeconfiguredto
zero-sumandgeneral-sum–astheycaninvolvedifferent
applytodifferentevaluationmeasuresandhence,cansatisfy
solutionconceptsandevaluationmeasures(givenbelow).
thedesiderataD3andD4whileonlyminimalmodifications
Weconstructall15gamesundertwoprimaryprinciples: i)
arerequired: specifyingtheMC’soptimizationobjective .
L thesegamesinvolveasmanyaspectsofdecisionmakingas
Thepseudo-codeofCMDisshowninAlgorithm2.
possible,e.g.,thenumberofagents(singleormultiple)and
4αisappliedtoalltheagentsinmulti-agentcategories. therelationshipbetweenagents(cooperative,competitive,
6
smhtiroglA
serusaeM
semaGConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
 . X K Q  $  . X K Q  %  * R R I V S L H O  6  7 L Q \ + D Q D E L  $  7 L Q \ + D Q D E L  %
                   
                   
                   
                                                  
                   
                    
      
          
      
          
      
          
      
          
      
          
 7 L Q \ + D Q D E L  &  . X K Q  / H G X F  * R R I V S L H O  % D U J D L Q L Q J
                   
       
   
       
               
                   
                   
               
       
   
       
      
          
      
          
   
           
      
          
   
           
 7 U D G H & R P P  % D W W O H V K L S  0 & & . X K Q  $  0 & & . X K Q  %  0 & & * R R I V S L H O
                   
                   
                   
                                                     
                   
                       
      
          
      
          
      
          
   
           
      
          
 & 0 '  * 0 '  0 0 '  . /  0 0 '  ( 8  & ) 5  & ) 5 
Figure4.Summaryofresults.Thefirst6figurescorrespondtosingle-agentandcooperativecategorieswherethey-axisisOptGap.The
restfigurescorrespondtoothercategorieswherethey-axisisNashConv.Forallthefigures,thex-axisisthenumberofiterations.
ormixed),andii)thesegamesarerelativelysimple,have 7.Experiments
alowbarriertoentry,andyetcomplexenough,andhence,
Inthissection,weevaluateourmethodonGAMEBENCH.
runningalgorithmsonthesegamesislessresource-intensive.
Wefirstdescribetheexperimentalsetup. Then,wepresent
Thedetailsoftheconstructionsandthestatisticsofthe15
theresultsbyansweringseveralresearchquestions(RQs)
gamescanbefoundinAppendixE.2.
Setup. Wecomparethefollowingmethods: i)CMD:our
Measures. AsGAMEBENCHincludesdifferentcategories
methodwherethehyper-parametersaredeterminedbythe
ofdecision-makingproblems,itisindispensabletoconsider
meta-controller introduced in Section 5.2, ii) GMD: the
multipleevaluationmeasures. Roughlyspeaking,thereare
hyper-parametersarefixedtoα =1/M,0 τ M 1
twotypesofmeasures: i)thenotionofoptimality,including τ ≤ ≤ −
(a uniform distribution), iii) MMD-KL: the state-of-the-
OptGapandsocialwelfare,andii)thenotionofequilibrium,
art method called magnetic mirror descent (Sokota et al.,
includingNashConvandCCEGap. Notethatcomputingthe
2023)wherethepolicyupdatingruleisinducedwithKL
equilibrium-typemeasuresfortheMCCcategoryrequiresa
divergence, iv) MMD-EU: similar to MMD-KL but the
newmethodtocomputetheteam’sbestresponse(BR)(not
policyupdatingruleisinducedwithsquaredEuclideannorm
asingleagent’s). ThedetailscanbefoundinAppendixE.3.
(seeAppendixD.3forthederivation),v)CFR:thepolicy
Algorithms. WeincorporatedifferentMDalgorithmsinto isupdatedbasedonregret(Zinkevichetal.,2007),andvi)
theGAMEBENCH,includingthestate-of-the-artbaselines. CFR+: anadvancedversionofCFR(Tammelin,2014). In
ThecomparisonbetweentheseMDalgorithmscanbefound CMDandGMD,wealsoincludeamagnetpolicy,whichhas
inTable1.Inaddition,wealsoincludeCFR-typealgorithms beenargueddesirable(Sokotaetal.,2023;Liuetal.,2023).
asthebaselines,includingtheCFR(Zinkevichetal.,2007) Nevertheless,wenotethatthisdoesnotcauseinconsistency
andCFR+(Tammelin,2014). Althoughthesealgorithms with our method as we can equivalently obtain them by
needtoupdatethepolicyateachdecisionpoint,sincethe settingM andα (seeAppendixD.2). Moreover,without
τ
numbersofdecisionpointsofthegamesinGAMEBENCH explicitlyspecifying,theresultsareobtainedunderψ(x)=
arenottoolarge,runningthesealgorithmsonthesegames xlnx,x (0,1].InRQ4,westudymorepossibleBregman
∈
isrelativelyeasy(academic-friendly). divergencesbysettingdifferentψinTable2.
7ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
7.1.ResultsandAnalysis      . X K Q      / H G X F
RQ1. (DesiderataD1andD2)CanCMDeffectivelytackle
            
        a shll oc wat te hg eo lr eie as rno if nd ge pc eis ri fo on r- mm aa nk ci eng ofpr do ifb fl ee rm ens? tmIn etF hi og du sre a4 c, row se
s
          '
 '
 5 6 5  * 6
 / ' 6
             '
 '
 5 6 5  * 6
 / ' 6
1 ti5 veg la ym se os l. veFr ao lm lct ah tee gr oe rs iu el sts o, fw de ec ca isn ios nee -mth aa kt i, nC gM pD robc la en me sf :fe ic n-           *  *
 &  & )
 ) /  /
 5
 5 '  '   6              *  *
 &  & )
 ) /  /
 5
 5 '  '   6
single-agentandcooperativecategories,CMDcanfindthe
      
                      
   
                       
 , W H U D W L R Q  , W H U D W L R Q
approximateoptimal(joint)policy(theOptGapconverges
toanextremelysmallvalue),andinothercategories,CMD Figure5. ResultsfordifferenttypesofMC.
canfindtheapproximateNashequilibrium(theNashConv
convergestoanextremelysmallvalue). weshowthelearningcurvesofdifferentinstancesofCMD
instantiatedwithdifferentconvexfunctions. Fromthere-
RQ2. (ComparisonwithBaselines)HowdoesCMDper-
sults,wecanseethattheKLdivergence(ψ(x) = xlnx),
formcomparedwithbaselines? AsshowninFigure4,by
thoughhasbeenwidelyadopted,couldbenottheoptimal
comparison,wecanobtainthefollowingtakeaways.
choice across all the decision-making categories. To our
• Incorporatingmultiplehistoricalpoliciesanddynamically knowledge,CMDisthefirstalgorithmthatisendowedwith
adjustingthehyper-parameterscouldacceleratepolicy thecapabilityof(empirically)exploringabroaderclassof
learningintermsofthenumberofiterations. Thiscanbe Bregmandivergence, aprominentfeaturecomparedwith
verifiedbycomparingCMDwithMMD-KLwhereCMD existingMDmethods. SeeAppendixF.6formoreresults.
canconvergetoasimilarOptGaporNashConvvaluewith
MMD-KLusingfeweriterations. Thisadvantageholds  * R R I V S L H O  0 & & * R R I V S L H O
       
evenwithouttuningthehyper-parameters: inmostofthe     x xl 2nx exx0.1  0  0 0  0 '  '    .  ( 8 /  &  & )  ) 5  5      x xl 2nx exx0.1  0  0 0  0 '  '    .  ( 8 /  &  & )  ) 5  5 
games,GMD(thehyper-parametersarefixed)canalso        
       
converge to a similar OptGap or NashConv value with        
MMD-KLusingfeweriterations.ForGMDwithdifferent        
         
heuristichyper-parameteradjustmentstrategiessuchas
         
lineardecaycanbefoundinAppendixF.4.       
                      
      
                      
 , W H U D W L R Q  , W H U D W L R Q
• Introducingthemeta-controllerisimportantasitnotonly
acceleratespolicylearningbutalsocouldachievecompet- Figure6. Resultsfordifferentconvexfunctions.
itiveorbetteroutcomes. Specifically,intermsofNash-
Conv,CMDoutperformsthebaselinesinMCCKuhn-A RQ5. (DesiderataD3andD4)CanCMDgeneralizetocon-
andMCCKuhn-Bandperformsonparwiththebaselines siderdifferentsolutionconceptsandevaluationmeasures?
inallothergames. However,inthemostdifficultLeduc InFigure7,weapplyCMDtotwodifferentmeasures:CCE-
poker,GMDcannoteffectivelydecreasetheNashConv, Gap(topline)andsocialwelfare(bottomline). Theresults
showcasingtheindispensabilityoftheMC. verify the effectiveness of incorporating multiple histori-
calpoliciesanddynamicallyadjustingthehyper-parameter
• RegardingtheCFR-typealgorithms,theresultssimilarto
valuesconditionalontheevaluationmeasureswhenconsid-
previousworks(Sokotaetal.,2023)arealsoobserved: i)
eringotherevaluationmeasuresbeyondNashConv. More
ThevanillaCFRistypicallyinferiortoCFR+andCMD
resultsandanalysiscanbefoundinAppendixF.8. Notably,
in all the games, and ii) in most of the games, CFR+
ourCMDcanbeeasilyappliedtodifferentmeasureswith
outperforms CMD over short iteration horizons but is
minimalmodifications: changingtheMC’sobjective .
quicklycaughtbyCMDforlongerhorizons. L
RQ6. (DesiderataD5andD6)Isrunningthedifferentalgo-
RQ3. (DifferentMCs)Isthedirection-guidedupdateinthe
rithmscomputationallydifficult? Wefoundthat,although
meta-controllerimportant? InFigure5,wecompareDRS
extraoperationsmayberequired,runningtheMDandCFR
proposedinSection5.2withDGLDS,RS,GLDS,andGLD
algorithmsonGAMEBENCHdoesnotcausemuchburden
(seeAppendixD.4fordetailsontheseMCs).Aswecansee,
onthecomputationalresource. Theanalysisofthecompu-
ourproposedMCsignificantlyoutperformsotherseitherin
tationalcomplexitycanbefoundinAppendixF.9.
convergencerateorfinalperformance. InAppendixF.5,we
visualizetheevolutionofthehyper-parametervaluesduring
thepolicylearning,verifyingtheintuitionthatourDRScan 8.Limitations,FutureWorks,andConclusions
moreefficientlyadjustthehyper-parameters.
In this section, we discuss the limitations of the current
RQ4. (DifferentBregmanDivergences)HowdoesCMD versionofourapproachandpresentthefuturedirections,
performunderdifferentBregmandivergences? InFigure6, followedbyconclusionsofthiswork.
8
 Y Q R & K V D 1
 Y Q R & K V D 1
 Y Q R & K V D 1
 Y Q R & K V D 1ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
 . X K Q  * R R I V S L H O Thirdly,thoughourmethodsarecapableofconsideringa
       
     &  * 0  0 '
 '
 0  0 0  0 '  '    .  ( 8 /  &  & )  ) 5
 5 
     &  * 0  0 '
 '
 0  0 0  0 '  '    .  ( 8 /  &  & )  ) 5
 5 
broaderclassofBregmandivergence,theystillrequirethe
        mathematicalformulationoftheconvexfunctionψascom-
                     p mu et ti hn og dt rh ee qv ua irl eu se ao sf et th oe fd reu la al tev da fr uia nb cl te iob ny sdu es ri in vg edth fe roN mew ψt (o an
s
       
        showninTable2). Inotherwords,thenumberofBregman
                                                            divergencesconsideredinourmethodisstilllimited. An
 , W H U D W L R Q  , W H U D W L R Q interesting future direction is to develop a novel method
 7 U D G H & R P P  % D W W O H V K L S tomoreeffectivelyexploretheentirespaceoftheconvex
           
       &  * 0  0 '  '  0  0 0  0 '  '    .  ( 8 /  &  & )  ) 5  5         &  * 0  0 '  '  0  0 0  0 '  '    .  ( 8 /  &  & )  ) 5  5  function, such as using a neural network to represent the
            convexfunctionψ,whichleadstotheneuralBregmandi-
     
      vergence(Luetal.,2023;Siahkamarietal.,2020;Cilingir
     
      et al., 2020; Amos et al., 2017). Moreover, using neural
     
            Bregmandivergencecouldalsobeapossiblesolutionfor
                                                                automaticallychoosingtheBregmandivergencesfordiffer-
 , W H U D W L R Q  , W H U D W L R Q
entdecision-makingcategories. Nevertheless,itcouldbe
Figure7.ResultsforthemeasuresCCEGapandsocialwelfare. non-trivialtointegratetheneuralBregmandivergenceas
trainingtheneuralnetworktowellapproximatetheoptimal
convexfunctionforthegivencategorymaynotbeeasyand
8.1.LimitationsandFutureWorks thusmayrequirenewtreatment.
Thisworkaimstodevelopasinglealgorithmtoeffectively Finally,initscurrentversion,GAMEBENCHconsistsof15
tackleallcategoriesofdecision-makingproblems: single- academic-friendlygamescoveringallcategoriesofdecision-
agent,cooperativemulti-agent,competitivemulti-agent,and makingproblems,differentevaluationmeasures,andseveral
mixedcooperativeandcompetitivemulti-agentcategories. baselinealgorithms. Webelievefurtherextensionscouldbe
As a preliminary attempt, there are still some limitations valuable.i)Wecouldincludemoregameswithvaryingcom-
thatareworthinvestigatinginfutureworks. plexity(e.g.,differentnumbersofdecisionpoints)(Lanctot
etal.,2023). ii)Wecouldincludemoreevaluationmeasures
Firstly,asthemeta-controllerdeterminesthevaluesofthe
suchasfairness(Rabin,1993). iii)Wecouldincludemore
hyper-parameter by sampling multiple candidates, extra
algorithms. In particular, we may include deep learning-
computationalcostisneededtoevaluatetheperformanceof
based algorithms (Schulman et al., 2017; Yu et al., 2022;
thesecandidates. InAppendixF.9,wepresenttherunning
Lanctot et al., 2017) and investigate whether there exists
timeofaniterationofdifferentmethods. Whilerequiring
asingledeeplearning-basedalgorithmthatcaneffectively
extra computational cost, we view this as one of the fu-
solveallcategoriesofdecision-makingproblems. iv)Re-
turedirections: developingmorecomputationallyefficient
cently,muchattentionhasbeendrawntostudyingtheability
hyper-parametervalueupdatingmethodswithoutsacrific-
oflargelanguagemodels(LLMs)tosolvevariousdecision-
ing performance. For example, in contrast to the current
makingproblems(Hongetal.,2023;Bakhtinetal.,2022;
samplingmethodwherethehistoricalsamplesareentirely
Maoetal.,2023). Therefore,aninterestingextensionisto
ignoredaftereachupdate,thesehistoricalsamplescouldbe
includeLLMsasthebaselines, andifnecessary, develop
usedtoguidetheselectionofthenewhyper-parameterval-
newLLMstomoreeffectivelysolvedifferentcategoriesof
ues,e.g.,viaBayesianoptimization(Lindaueretal.,2022)
decision-makingproblems.
orofflinelearningapproaches(Chenetal.,2022). Asare-
sult,wemaynotneedtosamplemultiplecandidates,which
couldfurtherreducetheextracomputationalcost. 8.2.Conclusions
Secondly,weevaluatedCMDprimarilyfromtheempirical Inthiswork,wemakethepreliminaryattempttodevelop
a single algorithm to tackle ALL categories of decision-
perspective,andtheresultsdemonstrateitspromiseinsolv-
ingallcategoriesofdecision-makingproblems. Theoretical making problems and provide three contributions: i) the
analysisofthebehavior(e.g.,theconvergencerate)ofCMD GMD,ageneralizationofexitingMDalgorithms,whichcan
couldbeaninterestingproblemandmayrequirenoveltools beappliedtodifferentdecision-makingcategoriesinvolv-
that may be different from existing works since the pol- ingdifferentnumbersofagentsanddifferentrelationships
icyupdatingruleofCMDisestablishedwithanumerical betweenagents(D1andD2),ii)theCMDwhichcanbecon-
method,ratherthandependsontheclosed-formsolutionto figuredtoapplytodifferentsolutionconceptsandevaluation
theregularizedoptimizationproblemineachdecisionpoint measures(D3andD4),andiii)thecomprehensive(D5)and
under some specific Bregman divergence (Sokota et al., academic-friendly(D6)benchmark–GAMEBENCH. Exten-
2023;Liuetal.,2023;Leeetal.,2021). siveexperimentsdemonstratetheeffectivenessofCMD.
9
 S D * ( & &
 H U D I O H :  O D L F R 6
 S D * ( & &
 H U D I O H :  O D L F R 6ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
Acknowledgements Bailey,J.P.andPiliouras,G. Fastandfuriouslearningin
zero-sum games: Vanishing regret with non-vanishing
ThisworkissupportedbytheNationalResearchFounda-
stepsizes. InNeurIPS,pp.12997–13007,2019.
tion,SingaporeunderitsIndustryAlignmentFund–Pre-
positioning (IAF-PP) Funding Initiative. Any opinions, Bakhtin, A., Brown, N., Dinan, E., Farina, G., Flaherty,
findings and conclusions, or recommendations expressed C., Fried, D., Goff, A., Gray, J., Hu, H., Jacob, A. P.,
in this material are those of the author(s) and do not re- Komeili, M., Konath, K., Kwon, M., Lerer, A., Lewis,
flecttheviewsofNationalResearchFoundation,Singapore. M.,Miller,A.H.,Mitts,S.,Renduchintala,A.,Roller,S.,
Hau Chan is supported by the National Institute of Gen- Rowe,D.,Shi,W.,Spisak,J.,Wei,A.,Wu,D.,Zhang,
eralMedicalSciencesoftheNationalInstitutesofHealth H., and Zijlstra, M. Human-level play in the game of
[P20GM130461],theRuralDrugAddictionResearchCen- Diplomacybycombininglanguagemodelswithstrategic
terattheUniversityofNebraska-Lincoln,andtheNational reasoning. Science,378(6624):1067–1074,2022.
ScienceFoundationundergrantIIS:RI#2302999. Thecon-
Bard, N., Foerster, J. N., Chandar, S., Burch, N., Lanc-
tentissolelytheresponsibilityoftheauthorsanddoesnot
tot,M.,Song,H.F.,Parisotto,E.,Dumoulin,V.,Moitra,
necessarilyrepresenttheofficialviewsofthefundingagen-
S.,Hughes,E.,Dunning,I.,Mourad,S.,Larochelle,H.,
cies. This work is also supported by Shanghai Artificial
Bellemare, M. G., and Bowling, M. The Hanabi chal-
IntelligenceLaboratory.
lenge: AnewfrontierforAIresearch. ArtificialIntelli-
gence,280:103216,2020.
ImpactStatement
Beck, A.andTeboulle, M. Mirrordescentandnonlinear
Thispaperpresentsworkwhosegoalistoadvancethefield
projectedsubgradientmethodsforconvexoptimization.
ofdecisionmaking. Therearemanypotentialsocietalcon-
OperationsResearchLetters,31(3):167–175,2003.
sequences of our work, none of which we feel must be
specificallyhighlightedhere. Bellemare,M.G.,Naddaf,Y.,Veness,J.,andBowling,M.
The arcade learning environment: An evaluation plat-
formforgeneralagents. JournalofArtificialIntelligence
References
Research,47:253–279,2013.
Albrecht, J., Fetterman, A., Fogelman, B., Kitanidis, E.,
Wróblewski, B., Seo, N., Rosenthal, M., Knutins, M., Berner,C.,Brockman,G.,Chan,B.,Cheung,V.,De˛biak,P.,
Polizzi, Z., Simon, J., and Qiu, K. Avalon: A bench- Dennison,C.,Farhi,D.,Fischer,Q.,Hashme,S.,Hesse,
markforRLgeneralizationusingprocedurallygenerated C., Józefowicz, R., Gray, S., Olsson, C., Pachocki, J.,
worlds. InNeurIPSDatasetsandBenchmarksTrack,pp. Petrov,M.,Pinto,H.P.d.O.,Raiman,J.,Salimans,T.,
12813–12825,2022. Schlatter,J.,Schneider,J.,Sidor,S.,Sutskever,I.,Tang,
J.,Wolski,F.,andZhang,S. Dota2withlargescaledeep
Amos, B., Xu, L., and Kolter, J. Z. Input convex neural reinforcementlearning.arXivpreprintarXiv:1912.06680,
networks. InICML,pp.146–155,2017. 2019.
Boyd, S. P. and Vandenberghe, L. Convex Optimization.
Anagnostides,I.,Farina,G.,Panageas,I.,andSandholm,T.
CambridgeUniversityPress,2004.
OptimisticmirrordescenteitherconvergestoNashorto
strongcoarsecorrelatedequilibriainbimatrixgames. In Brown,N.andSandholm,T. SuperhumanAIforheads-up
NeurIPS,pp.16439–16454,2022a. no-limitpoker: Libratusbeatstopprofessionals. Science,
359(6374):418–424,2018.
Anagnostides,I.,Panageas,I.,Farina,G.,andSandholm,T.
Onlast-iterateconvergencebeyondzero-sumgames. In Brown,N.andSandholm,T.SuperhumanAIformultiplayer
ICML,pp.536–581,2022b. poker. Science,365(6456):885–890,2019.
Campbell,M.,HoaneJr,A.J.,andHsu,F.-h. DeepBlue.
Ao,R.,Cen,S.,andChi,Y. Asynchronousgradientplayin
ArtificialIntelligence,134(1-2):57–83,2002.
zero-summulti-agentgames. InICLR,2023.
Carminati,L.,Zhang,B.H.,Farina,G.,Gatti,N.,andSand-
Aumann, R. J. Correlated equilibrium as an expression
holm,T. Hidden-rolegames: Equilibriumconceptsand
of Bayesian rationality. Econometrica: Journal of the computation. arXivpreprintarXiv:2308.16017,2023.
EconometricSociety,55(1):1–18,1987.
Cen,S.,Chi,Y.,Du,S.S.,andXiao,L. Fasterlast-iterate
Bailey,J.P.andPiliouras,G. Multiplicativeweightsupdate convergenceofpolicyoptimizationinzero-sumMarkov
inzero-sumgames. InEC,pp.321–338,2018. games. InICLR,2023.
10ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
Chen,Y.,Song,X.,Lee,C.,Wang,Z.,Zhang,R.,Dohan, Jain,R.,Piliouras,G.,andSim,R. Matrixmultiplicative
D.,Kawakami,K.,Kochanski,G.,Doucet,A.,Ranzato, weights updates in quantum zero-sum games: Conser-
M., et al. Towards learning universal hyperparameter vationlaws&recurrence. InNeurIPS,pp.4123–4135,
optimizers with transformers. In NeurIPS, pp. 32053– 2022.
32068,2022.
Kangarshahi,E.A.,Hsieh,Y.-P.,Sahin,M.F.,andCevher,
Cilingir,H.K.,Manzelli,R.,andKulis,B. Deepdivergence V. Let’sbehonest: Anoptimalno-regretframeworkfor
learning. InICML,pp.2027–2037,2020. zero-sumgames. InICML,pp.2488–2496,2018.
Davis,O.A.andWhinston,A. Externalities,welfare,and Kozuno,T.,Ménard,P.,Munos,R.,andValko,M. Model-
thetheoryofgames. JournalofPoliticalEconomy,70(3): freelearningfortwo-playerzero-sumpartiallyobservable
241–262,1962. Markovgameswithperfectrecall.InNeurIPS,pp.11987–
11998,2021.
deWitt,C.S.,Gupta,T.,Makoviichuk,D.,Makoviychuk,
V., Torr, P. H., Sun, M., and Whiteson, S. Is indepen- Kuhn,H.W. Asimplifiedtwo-personpoker. Contributions
dent learning all you need in the StarCraft multi-agent totheTheoryofGames,1:97–103,1950.
challenge? arXivpreprintarXiv:2011.09533,2020.
Kurach,K.,Raichuk,A.,Stan´czyk,P.,Zaja˛c,M.,Bachem,
Fang,Y.,Tang,Z.,Ren,K.,Liu,W.,Zhao,L.,Bian,J.,Li, O.,Espeholt,L.,Riquelme,C.,Vincent,D.,Michalski,
D., Zhang, W., Yu, Y., and Liu, T.-Y. Learning multi- M.,Bousquet,O.,andGelly,S. Googleresearchfootball:
agentintention-awarecommunicationforoptimalmulti- Anovelreinforcementlearningenvironment. InAAAI,
order execution in finance. In KDD, pp. 4003–4012, pp.4501–4510,2020.
2023.
Lanctot, M., Zambaldi, V., Gruslys, A., Lazaridou, A.,
Farina,G.,Bianchi,T.,andSandholm,T.Coarsecorrelation Tuyls,K.,Pérolat,J.,Silver,D.,andGraepel,T. Auni-
inextensive-formgames. InAAAI,pp.1934–1941,2020. fiedgame-theoreticapproachtomultiagentreinforcement
learning. InNeurIPS,pp.4190–4203,2017.
Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., and
Lanctot, M., Lockhart, E., Lespiau, J.-B., Zambaldi, V.,
Whiteson,S. Counterfactualmulti-agentpolicygradients.
InAAAI,pp.2974–2982,2018. Upadhyay, S., Pérolat, J., Srinivasan, S., Timbers, F.,
Tuyls, K., Omidshafiei, S., Hennes, D., Morrill, D.,
Foerster,J.,Song,F.,Hughes,E.,Burch,N.,Dunning,I., Muller, P., Ewalds, T., Faulkner, R., Kramár, J., De
Whiteson,S.,Botvinick,M.,andBowling,M. Bayesian Vylder,B.,Saeta,B.,Bradbury,J.,Ding,D.,Borgeaud,
actiondecoderfordeepmulti-agentreinforcementlearn- S., Lai, M., Schrittwieser, J., Anthony, T., Hughes, E.,
ing. InICML,pp.1942–1951,2019. Danihelka,I.,andRyan-Davis,J. OpenSpiel: Aframe-
workforreinforcementlearningingames. arXivpreprint
Golowich,N.,Pattathil,S.,andDaskalakis,C. Tightlast- arXiv:1908.09453,2019.
iterateconvergenceratesforno-regretlearninginmulti-
playergames. InNeurIPS,pp.20766–20778,2020. Lanctot,M.,Schultz,J.,Burch,N.,Smith,M.O.,Hennes,
D.,Anthony,T.,andPerolat,J. Population-basedevalua-
Harsanyi, J. C., Selten, R., et al. A General Theory of tioninrepeatedrock-paper-scissorsasabenchmarkfor
EquilibriumSelectioninGames. TheMITPress,1988. multiagentreinforcementlearning. TMLR,2023. ISSN
2835-8856.
Hong,S.,Zheng,X.,Chen,J.,Cheng,Y.,Wang,J.,Zhang,
C.,Wang,Z.,Yau,S.K.S.,Lin,Z.,Zhou,L.,Ran,C., Lee,C.-W.,Kroer,C.,andLuo,H. Last-iterateconvergence
Xiao,L.,Wu,C.,andSchmidhuber,J. MetaGPT:Meta inextensive-formgames. InNeurIPS,pp.14293–14305,
programming for multi-agent collaborative framework. 2021.
arXivpreprintarXiv:2308.00352,2023.
Lewis,M.,Yarats,D.,Dauphin,Y.,Parikh,D.,andBatra,
Hsieh, Y.-G., Antonakopoulos, K., and Mertikopoulos, P. D. Dealornodeal? End-to-endlearningofnegotiation
Adaptivelearningincontinuousgames: Optimalregret dialogues. InEMNLP,pp.2443–2453,2017.
boundsandconvergencetoNashequilibrium. InCOLT,
pp.2388–2422,2021. Lindauer, M., Eggensperger, K., Feurer, M., Biedenkapp,
A.,Deng,D.,Benjamins,C.,Ruhkopf,T.,Sass,R.,and
Ilyas,A.,Engstrom,L.,Athalye,A.,andLin,J. Black-box Hutter, F. SMAC3: A versatile Bayesian optimization
adversarialattackswithlimitedqueriesandinformation. packageforhyperparameteroptimization. TheJournal
InICML,pp.2137–2146,2018. ofMachineLearningResearch,23(1):2475–2483,2022.
11ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
Liu,M.,Ozdaglar,A.E.,Yu,T.,andZhang,K. Thepower Nemirovskij, A. S. and Yudin, D. B. Problem Com-
of regularization in solving extensive-form games. In plexity and Method Efficiency in Optimization. Wiley-
ICLR,2023. Interscience,1983.
Liu, S., Chen, P.-Y., Kailkhura, B., Zhang, G., Hero III, Oliehoek,F.A.andAmato,C. AConciseIntroductionto
A. O., and Varshney, P. K. A primer on zeroth-order DecentralizedPOMDPs. Springer,2016.
optimizationinsignalprocessingandmachinelearning:
Perolat,J.,DeVylder,B.,Hennes,D.,Tarassov,E.,Strub,
Principals,recentadvances,andapplications. IEEESig-
F., de Boer, V., Muller, P., Connor, J. T., Burch, N.,
nalProcessingMagazine,37(5):43–54,2020.
Anthony, T., McAleer, S., Elie, R., Cen, S. H., Wang,
Liu, S., Lever, G., Wang, Z., Merel, J., Eslami, S. A., Z., Gruslys, A., Malysheva, A., Khan, M., Ozair, S.,
Hennes,D.,Czarnecki,W.M.,Tassa,Y.,Omidshafiei,S., Timbers,F.,Pohlen,T.,Eccles,T.,Rowland,M.,Lanctot,
Abdolmaleki,A.,etal. Frommotorcontroltoteamplay M., Lespiau, J.-B., Piot, B., Omidshafiei, S., Lockhart,
insimulatedhumanoidfootball. ScienceRobotics,7(69): E.,Sifre,L.,Beauguerlange,N.,Munos,R.,Silver,D.,
eabo0235,2022a. Singh,S.,Hassabis,D.,andTuyls,K.Masteringthegame
of Stratego with model-free multiagent reinforcement
Liu,W.,Jiang,H.,Li,B.,andLi,H. Equivalenceanalysis
learning. Science,378(6623):990–996,2022.
betweencounterfactualregretminimizationandonline
mirrordescent. InICML,pp.13717–13745,2022b. Rabin, M. Incorporating fairness into game theory and
economics.TheAmericanEconomicReview,83(5):1281–
Lowe,R.,Wu,Y.,Tamar,A.,Harb,J.,Abbeel,P.,andMor-
1302,1993.
datch,I. Multi-agentactor-criticformixedcooperative-
competitiveenvironments. InNeurIPS,pp.6382–6393, Radhakrishnan,A.,Belkin,M.,andUhler,C.Linearconver-
2017. genceofgeneralizedmirrordescentwithtime-dependent
mirrors. arXivpreprintarXiv:2009.08574,2020.
Lu,F.,Raff,E.,andFerraro,F.NeuralBregmandivergences
fordistancelearning. InICLR,2023. Rashid, T., Samvelyan, M., Schroeder, C., Farquhar, G.,
Foerster,J.,andWhiteson,S. QMIX:Monotonicvalue
Mao, S., Cai, Y., Xia, Y., Wu, W., Wang, X., Wang, F.,
functionfactorisationfordeepmulti-agentreinforcement
Ge, T., and Wei, F. Alympics: Language agents meet
learning. InICML,pp.4295–4304,2018.
gametheory–exploringstrategicdecision-makingwith
AIagents. arXivpreprintarXiv:2311.03220,2023. Rashid, T., Farquhar, G., Peng, B., and Whiteson, S.
WeightedQMIX:Expandingmonotonicvaluefunction
Marris, L., Muller, P., Lanctot, M., Tuyls, K., and Grae-
factorisationfordeepmulti-agentreinforcementlearning.
pel,T. Multi-agenttrainingbeyondzero-sumwithcorre-
InNeurIPS,pp.10199–10210,2020.
latedequilibriummeta-solvers. InICML,pp.7480–7491,
2021. Rizk,Y.,Awad,M.,andTunstel,E.W. Cooperativehetero-
geneousmulti-robotsystems: Asurvey. ACMComputing
Mertikopoulos,P.,Lecouat,B.,Zenati,H.,Foo,C.-S.,Chan-
Surveys,52(2):1–31,2019.
drasekhar,V.,andPiliouras,G. Optimisticmirrordescent
insaddle-pointproblems:Goingtheextra(gradient)mile. Ross,S.M. Goofspiel–thegameofpurestrategy. Journal
InICLR,2019. ofAppliedProbability,8(3):621–625,1971.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Ve- Samvelyan, M., Rashid, T., De Witt, C. S., Farquhar, G.,
ness, J., Bellemare, M.G., Graves, A., Riedmiller, M., Nardelli, N., Rudner, T. G., Hung, C.-M., Torr, P. H.,
Fidjeland,A.K.,Ostrovski,G.,Petersen,S.,Beattie,C., Foerster,J.,andWhiteson,S. TheStarCraftmulti-agent
Sadik,A.,Antonoglou,I.,King,H.,Kumaran,D.,Wier- challenge. arXivpreprintarXiv:1902.04043,2019.
stra,D.,Legg,S.,andHassabis,D. Human-levelcontrol
Schmid,M.,Moravcik,M.,Burch,N.,Kadlec,R.,David-
throughdeepreinforcementlearning. Nature,518(7540):
son, J., Waugh, K., Bard, N., Timbers, F., Lanctot, M.,
529–533,2015.
ZachariasHolland,G.,Davoodi,E.,Christianson,A.,and
Moulin, H. and Vial, J. P. Strategically zero-sum games: Bowling,M. Studentofgames: Aunifiedlearningalgo-
Theclassofgameswhosecompletelymixedequilibria rithmforbothperfectandimperfectinformationgames.
cannotbeimprovedupon. InternationalJournalofGame ScienceAdvances,9(46):eadg3256,2023.
Theory,7:201–221,1978.
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
Nash,J. Non-cooperativegames. AnnalsofMathematics, Klimov, O. Proximal policy optimization algorithms.
54(2):286–295,1951. arXivpreprintarXiv:1707.06347,2017.
12ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
Serrino,J.,Kleiman-Weiner,M.,Parkes,D.C.,andTenen- Tammelin,O. Solvinglargeimperfectinformationgames
baum,J. Findingfriendandfoeinmulti-agentgames. In usingCFR+. arXivpreprintarXiv:1407.5042,2014.
NeurIPS,pp.1251–1261,2019.
Tesauro, G. et al. Temporal difference learning and TD-
Shoham, Y. and Leyton-Brown, K. Multiagent Systems: Gammon. Communications of the ACM, 38(3):58–68,
Algorithmic,Game-theoretic,andLogicalFoundations. 1995.
CambridgeUniversityPress,2008.
Tomar, M., Shani, L., Efroni, Y., and Ghavamzadeh, M.
Siahkamari,A.,XIA,X.,Saligrama,V.,Castañón,D.,and Mirrordescentpolicyoptimization. InICLR,2022.
Kulis,B. LearningtoapproximateaBregmandivergence.
Tsai, Y.-Y., Chen, P.-Y., and Ho, T.-Y. Transfer learning
InNeurIPS,pp.3603–3612,2020.
without knowing: Reprogramming black-box machine
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, learningmodelswithscarcedataandlimitedresources.
I.,Huang,A.,Guez,A.,Hubert,T.,Baker,L.,Lai,M., InICML,pp.9614–9624,2020.
Bolton, A., Chen, Y., Lillicrap, T., Hui, F., Sifre, L.,
v.Neumann,J. Zurtheoriedergesellschaftsspiele. Mathe-
van den Driessche, G., Graepel, T., and Hassabis, D.
matischeAnnalen,100(1):295–320,1928.
Mastering the game of Go without human knowledge.
Nature,550(7676):354–359,2017.
Vural,N.M.,Yu,L.,Balasubramanian,K.,Volgushev,S.,
andErdogdu,M.A. Mirrordescentstrikesagain: Opti-
Singh, B., Kumar, R., and Singh, V. P. Reinforcement
malstochasticconvexoptimizationunderinfinitenoise
learninginroboticapplications: Acomprehensivesurvey.
variance. InCOLT,pp.65–102,2022.
ArtificialIntelligenceReview,55(2):945–990,2022.
Wang,J.,Ren,Z.,Liu,T.,Yu,Y.,andZhang,C. QPLEX:
Sokota,S.,Lockhart,E.,Timbers,F.,Davoodi,E.,D’Orazio,
Duplexduelingmulti-agentQ-learning. InICLR,2021a.
R.,Burch,N.,Schmid,M.,Bowling,M.,andLanctot,M.
Solvingcommon-payoffgameswithapproximatepolicy Wang,J.,Xu,W.,Gu,Y.,Song,W.,andGreen,T.C. Multi-
iteration. InAAAI,pp.9695–9703,2021. agentreinforcementlearningforactivevoltagecontrolon
powerdistributionnetworks. InNeurIPS,pp.3271–3284,
Sokota,S.,D’Orazio,R.,Kolter,J.Z.,Loizou,N.,Lanctot,
2021b.
M., Mitliagkas, I., Brown, N., and Kroer, C. A uni-
fiedapproachtoreinforcementlearning,quantalresponse Wang,T.andKaneko,T. Applicationofdeepreinforcement
equilibria, and two-player zero-sum games. In ICLR, learninginwerewolfgameagents. InTAAI,pp.28–33,
2023. 2018.
Son,K.,Kim,D.,Kang,W.J.,Hostallero,D.E.,andYi,Y. Wang,X.,Guo,W.,Su,J.,Yang,X.,andYan,J. ZARTS:
QTRAN:Learningtofactorizewithtransformationfor Onzero-orderoptimizationforneuralarchitecturesearch.
cooperativemulti-agentreinforcementlearning. InICML, InNeurIPS,pp.12868–12880,2022.
pp.5887–5896,2019.
Wibisono,A.,Tao,M.,andPiliouras,G. Alternatingmirror
Song,X.,Gao,W.,Yang,Y.,Choromanski,K.,Pacchiano, descentforconstrainedmin-maxgames. InNeurIPS,pp.
A.,andTang,Y. ES-MAML:SimpleHessian-freemeta 35201–35212,2022.
learning. InICLR,2020.
Xu,B.,Wang,Y.,Wang,Z.,Jia,H.,andLu,Z. Hierarchi-
Su,H.,Zhong,Y.D.,Dey,B.,andChakraborty,A. Emv- callyandcooperativelylearningtrafficsignalcontrol. In
light: Adecentralizedreinforcementlearningframework AAAI,pp.669–677,2021.
forefficientpassageofemergencyvehicles. InAAAI,pp.
4593–4601,2022. Xu,Z.,Liang,Y.,Yu,C.,Wang,Y.,andWu,Y. Fictitious
cross-play: LearningglobalNashequilibriuminmixed
Sun,M.,Devlin,S.,Beck,J.,Hofmann,K.,andWhiteson, cooperative-competitive games. In AAMAS, pp. 1053–
S. TrustregionboundsfordecentralizedPPOundernon- 1061,2023.
stationarity. InAAMAS,pp.5–13,2023a.
Ypma,T.J. HistoricaldevelopmentoftheNewton-Raphson
Sun,S.,Wang,R.,andAn,B. Reinforcementlearningfor method. SIAMReview,37(4):531–551,1995.
quantitative trading. ACM Transactions on Intelligent
SystemsandTechnology,14(3):1–29,2023b. Yu, C., Velu, A., Vinitsky, E., Gao, J., Wang, Y., Bayen,
A.,andWu,Y. ThesurprisingeffectivenessofPPOin
Sutton,R.S.andBarto,A.G. ReinforcementLearning: An cooperativemulti-agentgames. InNeurIPSDatasetsand
Introduction. MITpress,2018. BenchmarksTrack,pp.24611–24624,2022.
13ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
Zahavy, T., Xu, Z., Veeriah, V., Hessel, M., Oh, J., van
Hasselt,H.,Silver,D.,andSingh,S. Aself-tuningactor-
criticalgorithm. InNeurIPS,pp.20913–20924,2020.
Zhou,Z.,Mertikopoulos,P.,Athey,S.,Bambos,N.,Glynn,
P.W.,andYe,Y. Learningingameswithlossyfeedback.
InNeurIPS,pp.5134–5144,2018.
Zinkevich,M.,Johanson,M.,Bowling,M.,andPiccione,C.
Regretminimizationingameswithincompleteinforma-
tion. InNeurIPS,pp.1729–1736,2007.
14ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
A.CodeRepository
Codeforexperimentsisavailableathttps://github.com/IpadLi/CMD.
B.MoreRelatedWorks
Single-AgentCategory. Inthesingle-agentcategory, reinforcementlearning(RL)(Sutton&Barto,2018)hasproved
successful in many real-world applications. The power of RL is further amplified with the integration of deep neural
networks,leadingtovariousdeepRLalgorithmsthathavebeensuccessfullyappliedtovariousapplicationdomainssuch
asvideogames(Mnihetal.,2015), robotnavigation(Singhetal.,2022), andfinancialtechnology(Sunetal.,2023b).
Amongthesealgorithms,PPO(Schulmanetal.,2017)isoneofthemostcommonlyusedmethodstosolvesingle-agentRL
problems. RecentworkshaveshownthatindependentPPO(deWittetal.,2020;Sunetal.,2023a)caneffectivelysolve
single-agentandcooperativemulti-agentRLproblems. Inaddition,avariantofPPOisalsoshowntobeeffectiveinsolving
two-playerzero-sumgameswhenbothplayersadoptthisalgorithm(Sokotaetal.,2023). Nevertheless,itremainselusive
whetherthesesingle-agentalgorithmscanbeappliedtosolveothercategoriesofdecision-makingproblemswhichmay
involvedifferentpropertiesincludingdifferentnumbersofagents,differentrelationshipsbetweenagents,differentsolution
concepts,anddifferentevaluationmeasures. Inthiswork,weaimtodevelopasinglealgorithmthat,whenexecutedbyeach
agent,providesaneffectiveapproachtoaddressdifferentcategoriesofdecision-makingproblems.
CooperativeMulti-AgentCategory. Cooperativemulti-agentRL(MARL)hasbeendemonstratedsuccessfulinsolving
manyreal-worldcooperativetaskssuchastrafficsignalcontrol(Xuetal.,2021;Suetal.,2022),powermanagement(Wang
etal.,2021b),finance(Fangetal.,2023),andmulti-robotcooperation(Rizketal.,2019). Inthepastdecade,avarietyof
MARLalgorithms,e.g.,QMIX(Rashidetal.,2018)anditsvariants(Sonetal.,2019;Rashidetal.,2020;Wangetal.,
2021a),MADDPG(Loweetal.,2017),COMA(Foersteretal.,2018),andMAPPO(Yuetal.,2022),tonamejustafew,
havebeenproposedandachievedsignificantperformanceinvariousmulti-agentbenchmarks,e.g.,SMAC(Samvelyan
etal.,2019)andDotaII(Berneretal.,2019). Thesealgorithmstypicallyfollowtheprincipleofcentralizedtrainingand
decentralizedexecution(CTDE)whereglobalinformationisonlyavailableduringtraining. Despitetheirsuccess,they
cannotbedirectlyappliedtocompetitiveandmixedcooperativeandcompetitivecategories.Inthiswork,ourproposedCMD
canbeappliedtodifferentdecision-makingcategoriesandsharesimilaritieswiththeCTDEparadigm: themeta-controller
takesalltheagents(i.e.,thejointpolicy)intoaccounttooptimizethehyper-parametersconditionalonthetargetedevaluation
measure(a“centralized”process)whileeachagentintheenvironmentindependentlyexecutetheGMDwiththegiven
hyper-parameterstoupdatethepolicy(a“decentralized”process).
CompetitiveMulti-AgentCategory. Therehaslongbeenahistoryofresearcherspursuingartificialintelligence(AI)
agentsthatcanachievehuman-levelorsuper-human-levelperformanceinsolvingvariouscompetitivemulti-agentgames
suchaschess(Campbelletal.,2002),Go(Silveretal.,2017),poker(Brown&Sandholm,2019),andStratego(Perolatetal.,
2022). Duetothecompetitivenature,thedevelopmentoflearningalgorithmsforsolvingthesegamesistypicallylargely
differentfromsingle-agentandcooperativeMARL.Amongothers,counterfactualregretminimization(CFR)(Zinkevich
etal.,2007)andpolicy-spaceresponseoracles(PSRO)(Lanctotetal.,2017)aretworepresentativealgorithmsthathave
beenwidelyusedtosolvecomplexgames(Schmidetal.,2023). Anothercategoryofalgorithmthathasdrawnincreasing
attentionrecentlyisthemirrordescent(MD)(Nemirovskij&Yudin,1983;Beck&Teboulle,2003). IncontrasttoCFR
and PSRO which are “average-iterate” algorithms, MD has proved the “last-iterate” convergence property in solving
two-player zero-sum games (Bailey & Piliouras, 2018; Kangarshahi et al., 2018; Wibisono et al., 2022; Kozuno et al.,
2021;Leeetal.,2021;Jainetal.,2022;Aoetal.,2023;Liuetal.,2023;Cenetal.,2023;Sokotaetal.,2023)andsome
classesofgeneral-sumgames(Anagnostidesetal.,2022b). Moreover,MDhasalsobeendemonstratedeffectiveinsolving
single-agentRLproblems(Tomaretal.,2022). Despitetheirsuccess,existingMDalgorithmstypicallyfocusonsome
specific Bregman divergences which may not be the optimal choices across different decision-making categories. Our
proposedCMDgeneralizesexistingMDalgorithmstoconsiderabroaderclassofBregmandivergence,whichcouldachieve
betterlearningperformanceinaddressingdifferentcategoriesofdecision-makingproblems.
MixedCooperativeandCompetitiveCategory. Insomereal-worldscenarios,therelationshipbetweenagentscouldbe
neitherpurelycooperativenorpurelycompetitive. Forexample,inafootballgame,theagentsbelongingtothesameteam
needtocooperatewhilealsocompetingwiththeotherteam(Kurachetal.,2020). Inhidden-rolegames(Carminatietal.,
2023),eachagenttriestoidentifytheir(unknown)teammatesandcompetewithother(unknown)adversaries(Wang&
Kaneko,2018;Serrinoetal.,2019;Albrechtetal.,2022). However,incontrasttotheotherthreecategories(single-agent,
purelycooperative,andpurelycompetitive),mixedcooperativeandcompetitive(MCC)gamesarelargelyunstudied(Xu
15ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
etal.,2023). Furthermore,asMDalgorithmstypicallyrequireupdatingthepolicyateachdecisionpoint,runningthemon
thecurrentbenchmarkgamessuchasfootball(Kurachetal.,2020)couldbecomputationallyprohibited. Inthepresent
work,weconstruct3MCCgamesthatareacademic-friendly–theirnumbersofdecisionpointsarenottoolargeandhence,
runningMDalgorithms(andotheralgorithmssuchasCFR-type(Zinkevichetal.,2007;Tammelin,2014))onthesegames
doesnotrequiremuchcomputationalresource(e.g.,runningtimeandmemoryusage).
Hyper-ParameterTuning. Existingworkstypicallydeterminethehyper-parametervaluesoftheMDalgorithmsdepending
onthedomainknowledge(Sokotaetal.,2023;Anagnostidesetal.,2022b;Hsiehetal.,2021;Zhouetal.,2018;Mertikopoulos
et al., 2019; Bailey & Piliouras, 2019; Golowich et al., 2020), which, though convenient for theoretical analysis, may
not be easy to generalize to different games. On the other hand, as the evaluation measures, e.g., NashConv, could be
non-differentiablewithrespecttothehyper-parameters,thegradient-basedmethodssuchasSTAC(Zahavyetal.,2020)
couldalsobelessapplicable. Inthissense,amorefeasiblemethodisthezero-orderhyper-parameteroptimizationwhichcan
updatetheparametersofinterestwithoutaccesstothetruegradient,whichhasbeenextensivelyadoptedintheadversarial
robustnessofdeepneuralnetworks(Ilyasetal.,2018), meta-learning(Songetal.,2020), transferlearning(Tsaietal.,
2020),andneuralarchitecturesearch(NSA)(Wangetal.,2022). Nevertheless,wefoundthatdirectlyapplyingexisting
zero-ordermethodscouldbeineffectiveaswhenthevalueoftheevaluationmeasureistoosmall,theymaynotbeable
toderiveaneffectiveupdateforthehyper-parameter. Toaddressthisissue,weproposeasimpleyeteffectivetechnique–
direction-guidedupdate–wheretheperformancedifferencebetweentwocandidatesisusedtoonlydeterminetheupdate
directionofthehyper-parametersratherthantheupdatemagnitude,whichismoreeffectivethanexistingmethods(Wang
etal.,2022)whenthevalueoftheperformanceisextremelysmall.
16ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
C.NotationTable
Table3. NotationTable.
= 1, ,N ,thesetofN agents.
N N { ··· }
thefinitesetofstates.
S
= where isthefinitesetofactionsofagenti.
i i i
A A ×∈NA A
= where isthefinitesetofobservationsofagenti.
i i i
O O ×∈NO O
Ω Ω= Ω whereΩ : istheobservationfunctionofagenti.
i i i i
×∈N S×A→O
P P : ∆( ),thestatetransitionfunction.
R R=S× r A→ wheS rer : Ristherewardfunctionofagenti.
i i i
{ }∈N S×A→
γ γ [0,1),thediscountfactor.
∈
ν ν ∆( ),theinitialstatedistribution.
∈ S
τt thedecisionpoint(action-observationhistory)ofagentiattimet,τt t.
i i ∈Ti
t t =( )t,thespaceofdecisionpointsofagentiattimestept.
Ti Ti Oi ×Ai
Π Π= Π whereΠ isthepolicyspaceofagenti.
i i i
×∈N
π π =π π ,thejointpolicy,π =π π ,theproductpolicy.
1 N 1 N
V (s,π),V (ν,π) thevalue⊙ fu· n·· ct⊙ ionsofagenti,V (ν,π):=E ×· [V·· (× s,π)].
i i i s ν i
∼
(π) theevaluationmeasureofthejointpolicyπ.
L
π thesingleagent’spolicyattheiterationkofanalgorithm.
k
π thejointpolicyattheiterationkofanalgorithm.
k
Q(π ) Q(π )=(Q(a,π )) ,theaction-valuevectorofasingleagentinducedbyπ .
k k k a k
∈A
(x;y) (x;y)=ϕ(x) ϕ(y) ϕ(y),x y ,theBregmandivergencewithrespecttoϕ.
ϕ ϕ
B B − −⟨∇ − ⟩
K thenumberofiterationsofanalgorithm.
M M 1,thenumberofhistoricalpolicies.
≥
α α=(α ) ,α istheregularizationintensityofπ .
τ 0 τ M 1 τ k τ
≤ ≤ − −
ϵ ϵ>0,thesmallestprobabilityofanaction.
ϕ(π) ϕ(π)= ψ(π(a)),ψissomeconvexfunctiondefinedon[0,1].
a
λ,β β =(β ) ∈A,thedualvariables.
a a
A,B A=Q(π(cid:80) k∈ )A + M τ=− 01α τϕ ′(π k τ),B = M τ=− 01α τ,whereϕ ′isthederivativeofϕ.
ψ 1 theinversefunctionofψ (thed− erivativeofψ).
′− ′
(cid:80) (cid:80)
C C >0,thenumberofiterationsfortheNewtonmethod.
g(λ),g (λ) g(λ)= ψ 1(A(a) λ)/B) 1,g (λ)= 1[ψ 1](A(a) λ)/B).
′ a ′− − − ′ a −B ′− ′ −
[ψ 1] thederivative∈oAfψ 1. ∈A
′− ′ ′−
(cid:2)(cid:80) (cid:3) (cid:80)
D thenumberofsampledcandidateα’s.
αj D Dcandidatesbyperturbingthecurrentα.
{ }j=1
πj D πj =GMD(αj) D ,DnewjointpoliciesderivedviaGMD.
{ }j=1 { }j=1
µ thesmoothingparameterinDRSandRS.
[r ,r ] theintervaloftheradiusesofthespheresinDGLDS,GLDS,andGLD.
L H
uj D Dcandidateupdatessampledfromasphericallysymmetricdistributionuj q.
{ }j=1 ∼
αj ,αj αj =CLIP1(α+µuj),αj =CLIP1(α µuj),thecandidatesbyperturbingthecurrentα.
π+ j,πj− πj+ =GMDι (αj ),πj =G− MD(αj ),ι then− ewjointpoliciesobtainedviaGMD.
+ + +
δj − δj = (πj) (πj− ),theperform− ancedifferencebetweenπj andπj .
u thefinL alu+ pda− te.L − + −
∗
Sgn Sgn(z)=1ifz >0,Sgn(z)= 1ifz <0,otherwise,Sgn(z)=0.
−
CLIP1 CLIP1(z)=ιifz <ι,CLIP1(z)=1ifz >1,otherwise,CLIP1(z)=z,where0<ι<1.
ι ι ι ι
κ κ 1,updatetheαeveryκiterations.
≥
ρ themagnetpolicyinMMD.
ξ ξ >0,theregularizationintensityofthemagnetpolicy.
η η >0,thestepsizeinMMD.
η˜ η˜>0,thestepsizeofthemagnetpolicyinMMD.
17ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
D.ConfigurableMirrorDescent
Inthissection,wepresentallthedetailsofourmethods. InSectionD.1,wepresentthedetailsofGMD.InSectionD.2,we
establishsomeconnectionsbetweenGMDandexistingMDalgorithms. InSectionD.3,werestricttheconvexfunction
ϕtothesquaredEuclideannormandderivetheclosed-formsolutionundertheMMDpolicyupdatingrule. Finally,in
SectionD.4,wepresentthedetailsofdifferentmeta-controllers.
D.1.GeneralizedMirrorDescent
Inthissection,wepresenttheproofofProposition5.1,thepseudo-codeofNewton’smethodforcomputingthevalueofthe
dualvariable,andtheconvexfunctionsconsideredinourwork.
ProofofProposition5.1. Considertheoptimizationproblem(3). BythedefinitionofBregmandivergence,wehave:
M 1
π =argmax Q(π ),π − α (π,π ), (5)
k+1 π Π k τ ϕ k τ
∈ ⟨ ⟩− τ=0 B −
(cid:88)M 1
π
k+1
=argmax
π Π
Q(π k),π − α τ[ϕ(π) ϕ(π
k
τ) ϕ ′(π
k
τ),π π
k τ
], (6)
⇒ ∈ ⟨ ⟩− τ=0 − − −⟨ − − − ⟩
(cid:88)M 1 M 1
π
k+1
=argmax
π Π
Q(π k)+ − α τϕ ′(π
k
τ),π ϕ(π) − α
τ
+const., (7)
⇒ ∈ ⟨ τ=0 − ⟩− τ=0
(cid:88) (cid:88)
where“const.” summarizesalltermsthatareirrelevanttoπ. LetA=Q(π k)+ M τ=− 01α τϕ ′(π k τ)andB = τM =− 01α τ
−
whicharefixedatthecurrentiterationk. Then,wecanconvertEq. (3)tothefollowingoptimizationproblem:
(cid:80) (cid:80)
π =argmax A,π Bϕ(π)+const.,
k+1 π ∈Π⟨ ⟩−
(8)
s.t. π (a)=1andπ (a) 0, a .
k k
a ≥ ∀ ∈A
∈A
(cid:88)
Tosolvetheconstrainedoptimizationproblem(8),wecanapplytheLagrangemultiplier,whichgivesus:
L(π,λ,β)= A,π Bϕ(π)+const. λ π(a) 1 + β π(a), (9)
a
⟨ ⟩− − a − a
(cid:16)(cid:88) ∈A (cid:17) (cid:88) ∈A
whereλandβ =(β ) arethedualvariables.Forsuchproblems,wecangettheKarush–Kuhn–Tucker(KKT)conditions
a a
∈A
foreachcomponent(action)a asfollows:
∈A
A(a)+Bϕ ′(π)(a) λ+β
a
=0, (10a)
−
π(a)=1, (10b)
a
∈A
(cid:88) β π(a)=0, (10c)
a
π(a) 0,β 0. (10d)
a
≥ ≥
Thentheproblemistofindapolicyπsuchthatitsatisfiesalltheaboveconditions,whichcouldbedifficultowingtotwo
reasons: i)itsimultaneouslyinvolvesthetwodualvariablesλandβ ,andii)inEq. (10a),computingthevalueϕ(π)(a)
a ′
involvesallthecomponents(actions)asϕisdefinedonthepolicyπ,nottheindividualcomponent(action)a .
∈A
Toaddressthechallenges,weapplythetwoconditions: π(a) ϵandϕ(π)= ψ(π(a)). Then,wehaveϕ(π)(a)=
≥ a ′
ψ (π(a)). Asaresult,theproblem(10a–10d)issimplifiedtothefollowingproblem∈A:
′
(cid:80)
A(a)+Bψ ′(π(a)) λ=0, (11a)
−
π(a)=1, (11b)
a
∈A
(cid:88) π(a) ϵ. (11c)
≥
FromEq. (11a),wecangetthat: a ,
∀ ∈A
A(a) λ
π(a)=ψ ′−1 − , (12)
B
(cid:18) (cid:19)
18ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
whereψ 1istheinversefunctionofψ . Substitutingtheaboveexpressionforπ(a)intotheconstraint(10b),wehave:
′− ′
A(a) λ
π(a)= ψ ′−1 − =1, (13)
a a B
∈A ∈A (cid:18) (cid:19)
(cid:88) (cid:88)
whichcompletestheproof.
NumericalMethodforComputingλ. Notably,Eq. (13)typicallycannotbesolvedanalytically. Toaddressthisproblem,
weproposetouseanumericalmethodtocomputethevalueofλ,offeringthepossibilityofexploringabroaderclassof
Bregmandivergence. Specifically,foranyconvexfunctionψ,weemploytheNewtonmethod(Ypma,1995)tocomputethe
valueofλ,whichisshowninAlgorithm3,whereC isthenumberofiterations.
Algorithm3Newtonmethodforcomputingthevalueofλ
1: Givenψ,A,andB. Randomlyinitializethevalueofλ
2: forC iterationsdo
3: λ=λ g(λ)
− g′(λ)
4: endfor
DifferentBregmanDivergences. InTable2,welisttheconvexfunctionsconsideredinourwork. Tobemoreintuitive,we
plottheseconvexfunctionsinFigure8.
                
            
        
            
                
                
    
       
        
            
                
                                                                                                   
x x x x
(a) xlnx (b) x2 (c) x0.1 (d) ex
−
Figure8. Plotsfordifferentconvexfunctionsψ.
D.2.ConnectionBetweenGMDandExistingMDAlgorithms
Inthissection,wepresentsomediscussionontheconnectionbetweenGMDandexistingMDalgorithms. InTable4,we
presenttheconditionsforconvertingGMDtodifferentMDalgorithmsandtheirformulations.
Table4. ConnectionbetweenGMDandexistingMDalgorithms.
Method Conditions Formulation
MD M =1,α (0,1] π =argmax Q(π ),π α (π,π )
0 k+1 π Π k 0 ϕ k
∈ ∈ ⟨ ⟩− B
M =k,α ,α (0,1],
MMD α
τ
=0,0k <−1 τ <0 k∈ 1 π k+1 =argmax π ∈Π ⟨Q(π k),π ⟩−α k −1 Bϕ(π,π 1) −α 0 Bϕ(π,π k)
−
GMD MD.ItistrivialtogettheMDalgorithm(Nemirovskij&Yudin,1983;Beck&Teboulle,2003)bysettingM =1
→
andα >0,thatis,MDonlyconsidersthecurrentpolicyπ whenderivingthenewpolicyπ .
0 k k+1
GMD MMD.ToobtainMMD(Sokotaetal.,2023),wecansetM =kandthenonlyletα k 1 andα 0 tobepositive,
→ −
whileallothertermsare0. Thatis,MMDconsiderstwopreviouspolicies–theinitialpolicyπ andthecurrentpolicyπ –
1 k
whenderivingthenewpolicy. IntheMMD’sterminology,theinitialpolicyπ servesasthemagnetpolicy.
1
Inpractice,wecangetmorevariantsbysettingtheM andα,whichshowsthatGMDisageneralmethod. Forexample,we
canconsiderbothM <kpreviouspoliciesandtheinitialpolicyπ (i.e.,addingamagnetpolicy)whenderivingthenew
1
19
)x( )x( )x( )x(ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
policyπ ,whichistakenasthedefaultchoiceforinstantiatingtheGMDinourexperiments,thatis,
k+1
M 1
−
π =argmax Q(π ),π α (π,π ) α (π,π ). (14)
k+1 k k 1 ϕ 1 τ ϕ k τ
π Π⟨ ⟩− − B − B −
∈ τ=0
(cid:88)
InAppendixF.7,weperformanablationstudytoshowtheeffectivenessofaddingsuchamagnetpolicy. Nevertheless,itis
worthnotingthatthisparticularchoiceshouldnotbeconfusedwiththeoriginalMMDevenwhenM = 1asthepolicy
updatingruleisderivedviaanumericalmethod,insteadofrelyingontheclosed-formsolution(Sokotaetal.,2023).
D.3.DerivationofMMD-EU
Inthissection,wepresentthedetailsofthebaseline,MMD-EU,usedinourexperiments. Thisbaselinefollowsthespiritof
MMD-KL(Sokotaetal.,2023). Considerthefollowingproblem:
1
π =argmax Q(π ),π ξ (π,ρ) (π,π ), (15)
k+1 k ϕ ϕ k
π Π⟨ ⟩− B − ηB
∈
whereξ >0istheregularizationintensity,η >0isthestepsize,andρisthemagnetpolicy. Letϕ(π)= 1 π(a) 2,
a 2∥ ∥
i.e.,thesquaredEuclideannorm. Then,weneedtooptimizethefollowingobjective: ∈A
(cid:80)
ξ 1
Q(π ),π π ρ 2 π π 2, (16)
⟨ k ⟩− 2|| − ||2− 2η|| − k ||2
withtheconstraint π(a)=1andπ(a)>0. WecanusetheLagrangemultipliertogetthefollowingobjective:
a
∈A
(cid:80)
ξ 1
Q(π ),π π ρ 2 π π 2+λ 1 π(a) . (17)
⟨ k ⟩− 2|| − ||2− 2η|| − k ||2 (cid:32) − (cid:33)
a
(cid:88)∈A
Takingthederivativeofbothπandλ,wehave:
1
Q(a,π ) ξ(π(a) ρ(a)) (π(a) π (a)) λ=0, a , (18)
k k
− − − η − − ∀ ∈A
π(a)=1. (19)
a
(cid:88)∈A
ThereforefromEq.(18),wehave:
ξρ(a)+ 1π (a)+Q(a,π ) λ
π(a)= η k k − . (20)
(ξ+ 1)
η
SubstitutingtheaboveequationtoEq.(19),wehave:
ξρ(a)+ 1π (a)+Q(a,π ) λ
η k k − =1, (21)
(ξ+ 1)
a η
(cid:88)∈A
1 1
ξρ(a)+ π (a)+Q(a,π ) =(ξ+ )+ λ. (22)
k k
⇒ η η
a (cid:20) (cid:21) a
(cid:88)∈A (cid:88)∈A
Notethat ξρ(a)+ 1π (a) =ξ+ 1,wehave:
a η k η
∈A
(cid:104) (cid:105)
(cid:80)
Q(a,π )
λ= a ∈A k . (23)
(cid:80) |A|
Thenwecancomputethenewpolicyasfollows:
ξρ(a)+ 1π (a)+Q(a,π ) 1 Q(a,π )
π(a)= η k k − |A| a′ ∈A ′ k . (24)
(ξ+ 1)
η (cid:80)
20ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
Theoretically,wenotethatbychoosingthesuitablevaluesforξ andη,wecanalwaysensurethatπiswell-defined,i.e.,
π(a) 0, a . Inexperiments,wecanuseaprojectionoperationtoensurethiscondition(ζ =1e 10isusedtoavoid
≥ ∀ ∈A −
divisionbyzero):
max 0,π(a) +ζ
π (a)= { } . (25)
k+1
max 0,π(a) +ζ
a′ { ′ }
∈A
Inaddition,similartoMMD-KL,themagnetpolicyisupdatedas(i.e.,movingmagnet):
(cid:80)
ρ (a)=(1 η˜)ρ (a)+η˜π (a), (26)
k+1 k k+1
−
whereη˜>0isthelearningrateforthemagnetpolicyρ. Inpractice,theinitialmagnetpolicyρ canbesettotheinitial
1
policyπ whichistypicallyauniformpolicy.
1
D.4.Meta-ControllerforDifferentMeasures
GMDgeneralizesexitingMDalgorithmsintwoaspects: i)ittakesmultiplepreviouspoliciesintoaccountandcanrecover
someoftheexistingMDalgorithmsbysettingtheM andα,andii)itcanconsiderabroaderclassofBregmandivergence
bysettingϕtomorepossibleconvexfunctions(Table2). Asaconsequence,wearguethatwhenGMDisexecutedbyeach
agentindependently,itcouldsatisfythefirsttwodesiderataD1andD2presentedintheIntroduction. However,asmentioned
inSection5.2,sincethereisnoexplicitobjectiveregardingdifferentevaluationmeasures(anddifferentsolutionconcepts)
arisesinthis“decentralized”executionprocess, GMDitselfcannotsatisfywellthelasttwodesiderataD3andD4. To
addressthechallenges,oursolutionisthezero-ordermeta-controller(MC)whichdynamicallyadjuststhehyper-parameters
conditionalontheevaluationmeasures(Section5.2). Inthissection,wepresentthedetailsofdifferentMCs.
Direction-GuidedRandomSearch(DRS).OurDRSmethodisobtainedbyapplyingthedirection-guidedupdate(Sec-
tion5.2)totheexistingRSmethodpresentedin(Wangetal.,2022). Specifically, attheiterationk, wefirstsampleD
candidateupdates uj D fromasphericallysymmetricdistributionuj q. Then,weupdateαasfollows:
{ }j=1 ∼
αj =CLIP1(α+µuj), αj =CLIP1(α µuj), 1 j D,
+ ι − ι − ≤ ≤
πj =GMD(αj ), πj =GMD(αj ), 1 j D,
+ + − − ≤ ≤
δj = (πj) (πj ), 1 j D,
L + −L − ≤ ≤ (DRS)
D
u
∗
= Sgn(δj)uj,
− j=1
α ←CL(cid:88)IP1 ι(α+u ∗).
Sgn(z)isdefinedas: Sgn(z)=1ifz >0,Sgn(z)= 1ifz <0,otherwise,Sgn(z)=0. µisthesmoothingparameter
determiningtheradiusofthesphere. CLIP1 istheel− ement-wiseclippingoperationdefinedas: CLIP1(z) = ιifz < ι,
ι ι
CLIP1(z) = 1ifz > 1,otherwise,CLIP1(z) = z,where0 < ι < 1. Notethattheclippingoperationwhichboundsα
ι ι
aboveι>0isnecessaryasthetermBisusedasthedenominatorinEq. (12). Inaddition,theoperationSgn()playsan
·
importantroleanddifferentiatesourDRSfromthevanillaRS(Wangetal.,2022). Intuitively,inthegameswherethevalue
oftheevaluationmeasure isextremelysmallandconvergesquickly,themagnitudeofδj wouldbetoosmalltoderivean
effectiveupdate. IncontrasL t,byusingtheoperationSgn(),thedifferencebetweentheperformanceofαj andαj willonly
determinetheupdatedirection,nottheupdatemagnitud· e,whichcouldbemoreeffective. + −
RandomSearch(RS).ThevanillaRSwhichisadaptedfrom(Wangetal.,2022). TheonlydifferencefromDRSisit
updatesαdirectlybasedontheperformancedifferenceδj. Precisely,wehave:
αj =CLIP1(α+µuj), αj =CLIP1(α µuj), 1 j D,
+ ι − ι − ≤ ≤
πj =GMD(αj ), πj =GMD(αj ), 1 j D,
+ + − − ≤ ≤
δj = (πj) (πj ), 1 j D,
L + −L − ≤ ≤ (RS)
D
u = δjuj,
∗
− j=1
α ←CL(cid:88)IP1 ι(α+u ∗).
GradientLessDescent(GLD).Thismethodisadaptedfrom(Wangetal.,2022). Attheiterationk,wefirstsampleD
candidateupdates uj D . DifferentfromRSwhichsamplesthecandidatesfromafixedradius(thesmoothingparameter
{ }j=1
21ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
µinDRSandRS),weindependentlysamplethecandidatesonsphereswithvariousradiusesuniformlysampledfromthe
interval[r ,r ]. Then,weupdateαasfollows:
L H
αj =CLIP1(α+uj), πj =GMD(αj ), 1 j D,
+ ι + + ≤ ≤
j =argmin (πj) D ,
∗ j {L + }j=1 (GLD)
u
=uj∗
,
∗
α ←CLIP1 ι(α+u ∗).
Intuitively,bycomparingtheperformanceofDcandidates,αisupdatedbythecandidatewiththesmallestvalueof .
L
GradientLessDescentwithSummation(GLDS).DifferentfromGLDwhichusesonlyoneoftheDsamplestoupdatethe
α,wecanfollowtheideaofRS/DRStotakeallthecandidatesintoaccountbysummation. Specifically,let (π )denote
k
L
theperformanceofthecurrentpolicyπ ,thenwehave:
k
αj =CLIP1(α+uj), πj =GMD(αj ), 1 j D,
+ ι + + ≤ ≤
δj = (πj) (π ), 1 j D,
L + −L k ≤ ≤
D (GLDS)
u = δjuj,
∗
− j=1
α ←CL(cid:88)IP1 ι(α+u ∗).
Direction-GuidedGLDS(DGLDS).Applyingthedirection-guidedupdatetotheGLDS,wecangetthismethod. Precisely,
let (π )denotetheperformanceofthecurrentpolicyπ ,thenwehave:
k k
L
αj =CLIP1(α+uj), πj =GMD(αj ), 1 j D,
+ ι + + ≤ ≤
δj = (πj) (π ), 1 j D,
L + −L k ≤ ≤
D (DGLDS)
u
∗
= Sgn(δj)uj,
− j=1
α ←CL(cid:88)IP1 ι(α+u ∗).
Asthemeta-controllerneedstoevaluatetheperformanceofthecandidates,extracomputationalcostisrequired. Inour
experiments, to trade-off between the learning performance and running time, we update α every κ 1 iteration. In
≥
addition,duringthefirstM 1iterations,i.e.,k < M,asthereareonlyk < M historicalpolicies,wesetα = 1 for
− τ k
0 τ k 1. Inotherwords,MCwillstarttoupdateαonlyafterM iterations. Algorithm2inthemaintextisthe
≤ ≤ −
simplifiedversionwhichshowstheprimaryprincipleofCMD.InAlgorithm4,wepresentthefulldetailsofCMD.
Algorithm4ConfigurableMirrorDescent(CMD)
1: Given ,ψ,initial(joint)policyπ 1,M,D,ϵ,ι,
L
2: fork =1, ,K do
···
3: ifk M then
≤
4: α τ = k1, ∀0 ≤τ ≤k −1
5: else
6: ifk%κ=0then
7: SampleDcandidates αj D
{ }j=1
8: Derivenewjointpolicies πj =GMD(αj) D
{ }j=1
9: Evaluatenewjointpolicies (πj) D
{L }j=1
10: Updateαbasedon (πj) D
{L }j=1
11: endif
12: endif
13: Computeπ k+1viaGMDwiththeupdatedα
14: endfor
22ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
E. GAMEBENCH
In this section, we present the details of GAMEBENCH (see Figure 3 for an overview). In Section E.1, we discuss the
motivation and desiderata by briefly reviewing the games that have been employed to test existing MD algorithms. In
SectionE.2,wepresentthedetailsoftheconstructionofall15games. Finally,inSectionE.3,wepresenttheevaluation
measuresconsideredinthiswork.
E.1.MotivationandDesiderata
AsmentionedinSection6,existingbenchmarksfordecisionmakingaretypicallyspecializedforsomespecificcategories.
Furthermore,runningMDalgorithmsonthesebenchmarkscouldbecomputationallyprohibitiveasthenumberofdecision
pointsintheenvironmentscouldbeextremelylarge. Ontheotherhand,thoughMDalgorithmshavebeendemonstrated
powerfulinsingle-agentRL(Tomaretal.,2022)andtwo-playerzero-sumgames(Wibisonoetal.,2022;Kozunoetal.,
2021;Leeetal.,2021;Liuetal.,2022b;Jainetal.,2022;Aoetal.,2023;Liuetal.,2023;Cenetal.,2023;Sokotaetal.,
2023)inrecentworks,theirexperimentsaretypicallyconductedonahandfulofgames. Itremainselusivehowwillthese
MDalgorithmsperformwhenappliedtoothercategoriesofdecision-makingproblems. InTable5,webrieflyreviewthe
gamesthathavebeenusedinsomerecentworks.
Table5.ThegamesthathavebeenusedinrecentworksonMDalgorithms.Notethatthislistdoesnotincludethegamesthatareused
tobenchmarkdeeplearning-basedalgorithmsinthesereferences. 1Thisgameismadetobeageneral-sumgameviaatie-breaking
mechanisminthisreference.2Thisgameismadetobeazero-sumgameinthisreference.
Reference Game Category
KuhnPoker Two-PlayerZero-Sum
LeducPoker Two-PlayerZero-Sum
(Sokotaetal.,2023)
2x2AbruptDarkHex Two-PlayerZero-Sum
4-SidedLiar’sDice Two-PlayerZero-Sum
KuhnPoker Two-PlayerZero-Sum
(Liuetal.,2023)
LeducPoker Two-PlayerZero-Sum
KuhnPoker Two-PlayerZero-Sum
(Anagnostidesetal.,2022b)
LeducPoker Two-PlayerZero-Sum
Sheriff Two-PlayerGeneral-Sum
Battleship Two-PlayerGeneral-Sum
(Anagnostidesetal.,2022a)
Goofspiel1 Two-PlayerGeneral-Sum
Liar’sDice Two-PlayerZero-Sum
KuhnPoker Two-PlayerZero-Sum
(Leeetal.,2021) LeducPoker Two-PlayerZero-Sum
Pursuit-Evasion Two-PlayerZero-Sum
LeducPoker Two-PlayerZero-Sum
Goofspiel Two-PlayerZero-Sum
(Liuetal.,2022b)
Liar’sDice Two-PlayerZero-Sum
Battleship2 Two-PlayerZero-Sum
Inviewoftheabovefacts,weaimtoconstructanovelbenchmarkwhichshouldsatisfytwodesiderata(D5andD6presented
intheIntroduction): i)itshouldcoverallcategoriesofdecisionmaking(comprehensive),andii)thegamesarerelatively
simpleandrunningMDalgorithmsonthesegamesdoesnotrequiremuchcomputationalresource(academic-friendly).
E.2.Games
Inthissection,wepresentthedetailsoftheconstructionofall15gamesinourGAMEBENCH. Allthegamesaredividedinto
5categories: single-agent,cooperativemulti-agent,competitivemulti-agentzero-sum(zero-sum),competitivemulti-agent
general-sum(general-sum),andmixedcooperativeandcompetitive(MCC)categories. InTable6,wegiveanoverview
23ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
ofallthegames. WecuratetheGAMEBENCHontopofOpenSpiel(Lanctotetal.,2019). Forcooperative,zero-sum,and
general-sumcategories,weconstructthegamebypassingtheconfigurationstothegamesimplementedinOpenSpiel. The
configurationsforthesegamesaredeliberatelyselectedsuchthattheinstancesofthesegamesareacademic-friendly(i.e.,
theirnumbersofdecisionpointsarenottoolarge). Forsingle-agentandMCCcategories,weobtainthegamesbymodifying
theoriginalgamesinOpenSpiel. Inthefollowing,wepresentthedetailsofeachcategory.
Table6.ThegamesandtheirstatisticsinGAMEBENCH.N isthenumberofplayersand“#DP”standsforthenumberofdecisionpoints.
Category NameofGamew/Config. Shorthand N #DP EvaluationMeasure
single_agent_kuhn_a Kuhn-A 1 6 OptGap
Single-Agent single_agent_kuhn_b Kuhn-B 1 6 OptGap
single_agent_goofspiel Goofspiel-S 1 8 OptGap
tiny_hanabi_game_a TinyHanabi-A 2 8 OptGap
Cooperative tiny_hanabi_game_b TinyHanabi-B 2 6 OptGap
tiny_hanabi_game_c TinyHanabi-C 2 6 OptGap
kuhn_poker(players=3) Kuhn 3 48 NashConv,CCEGap
Zero-Sum leduc_poker(players=2) Leduc 2 936 NashConv
goofspiel(players=3) Goofspiel 3 30 NashConv,CCEGap
bargaining(max_turns=2) Bargaining 2 178 NashConv,SW
General-Sum trade_comm(num_items=2) TradeComm 2 22 NashConv,SW
battleship Battleship 2 210 NashConv,SW
mix_kuhn_3p_game_a MCCKuhn-A 3 48 NashConv
MCC mix_kuhn_3p_game_b MCCKuhn-B 3 48 NashConv
mix_goofspiel_3p MCCGoofspiel 3 30 NashConv
Single-Agent. Weconstructthreesingle-agentgames: Kuhn-A,Kuhn-B,andGoofspiel-S,fromtheoriginaltwo-player
KuhnpokerandGoofspielinOpenSpiel. Consideratwo-playerKuhnpokergame. Toobtainasingle-agentcounterpart,we
fixoneplayer’spolicyastheuniformpolicy(calledthebackgroundplayer)whileonlyupdatingtheotherplayer’spolicy
(calledthefocalplayer)ateachiteration. InKuhn-A,player1isselectedasthefocalplayerwhileinKuhn-B,player2is
chosenasthefocalplayer,asthetwoplayersareasymmetric(Kuhn,1950). Similarly,wecangetGoofspiel-S.Asthetwo
playersaresymmetricinGoofspiel(Ross,1971),wechooseplayer1asthefocalplayerwithoutlossofgenerality.
Cooperative. For cooperative games, we consider the following three two-player tiny Hanabi games (Foerster et al.,
2019;Sokotaetal.,2021): TinyHanabi-A,TinyHanabi-B,andTinyHanabi-C.Thepayoffmatricesalongwiththeoptimal
valuesofthesegamesaregiveninFigure9. ThesegamesareeasytoobtaininOpenSpielbysettingthethreeparameters:
num_chance,num_actions,andpayoff. Fornum_chance,theyare2,2,and2,respectively. Fornum_actions,
theyare3,2,and2,respectively.
CompetitiveZero-SumandGeneral-Sum. Weconsiderthefollowingthreezero-sumgames: three-playerKuhn,two-
playerLeduc,andthree-playerGoofspiel,andthefollowingthreegeneral-sumgames: two-playerBattleship(Farinaetal.,
2020),two-playerTradeComm(Sokotaetal.,2021),andtwo-playerBargaining(Lewisetal.,2017),whichareimplemented
inOpenSpiel. TheconfigurationsofthesegamesaregiveninthesecondcolumninTable6. Notethatincontrasttomostof
theexistingworkswhichonlyfocusontwo-playergames,wesetthenumberofplayerstomorethantwoplayersinsomeof
thegames: KuhnandGoofspielarethree-playergames.
Mixed Cooperative and Competitive (MCC). We construct the following three-player MCC games: MCCKuhn-A,
MCCKuhn-B,andMCCGoofspiel,fromtheoriginalthree-playerKuhnpokerandthree-playerGoofspielinOpenSpiel.
Considerathree-playerKuhnpokergame. ToobtainanMCCcounterpart,wepartitionthethreeplayersintotwoteams:
Team1includestwoplayerswhileTeam2onlyconsistsofoneplayer(i.e.,twovs. one). Whencomputingtherewardsof
theplayers,inTeam1,eachplayerwillgettheaveragerewardoftheteam. Precisely,letrteam =r1+r2denotetheteam
rewardwhichisthesumoftheoriginalrewardsofthetwoteammembers. Then,thetruerewardsofthetwoplayersare
r˜1 =r˜2 =rteam/2. InMCCKuhn-A,Team1includesplayers1and2(i.e.,{1,2}vs. 3),whileinMCCKuhn-B,Team1
includesplayers1and3(i.e.,{1,3}vs. 2). Similarly,wecangetMCCGoofspielinthesamemanner.
24ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
TinyHanabi-A TinyHanabi-B TinyHanabi-C
Opt: 10 Opt: 1 Opt: 2.5
10 0 0 0 0 10
)ts
r 4 8 4 4 8 4
if
s
tc 10 0 0 0 0 10 1 0 0 1 3 0 3 1
a
(
1
r 0 0 10 10 0 0
1 0 0 0 1 3 3 0
e
y
a
lP 4 8 4 4 8 4 0 1 1 0 3 2 0 1
0 0 0 10 0 0 0 0 1 0 0 2 0 0
Player 2 (acts second)
Figure9. PayoffmatricesandoptimalvaluesofthethreetinyHanabigames.
AsshowninTable6,thenumberofdecisionpoints(#DP)variesacrossdifferentcategories,whichshowsthatGAMEBENCH
includesdiverseenoughenvironmentsas,tosomeextent,thenumberofdecisionpointsreflectsthedifficultyofthegame.
E.3.EvaluationMeasures
AsshowninFigure3,weconsidermultipleevaluationmeasuresinGAMEBENCH. Therearetwotypesofmeasures: i)
thenotionofoptimality,includingOptGapandsocialwelfare,andii)thenotionofequilibrium,includingNashConvand
CCEGap. InthelastcolumnofTable6,wepresentthemeasuresemployedineachgame. Insingle-agentandcooperative
categories,weuseOptGapasthemeasurewhichcapturesthedistanceofthecurrent(joint)policytotheoptimal(joint)
policy. Intheotherthreecategories,theprimarymeasureisNashConvwhichcapturesthedistanceofthecurrentjoint
policytotheNashequilibrium. Inaddition,wealsoconsiderothersolutionconceptsandevaluationmeasuresinsomeofthe
games. Forzero-sumKuhnandGoofspiel,astherearethreeplayers,wealsoconsiderthemeasureCCEGapwhichcaptures
thedistanceofthecurrentjointpolicytothecoarsecorrelatedequilibrium(CCE).Forgeneral-sumgames,wealsoconsider
thesocialwelfare(SW)ofalltheagents.
ExceptfortheMCCcategory,allthemeasurescanbeeasilycomputedbyusingthebuilt-inimplementationfunctionsin
OpenSpiel. However,tocomputetheNashConvintheMCCgames,weneedtocomputethebestresponsepolicyofthe
team,i.e.,ajointpolicyoftheteammembers,ratherthanthepolicyofasingleagent. Thisisincompatiblewiththebuilt-in
implementationinOpenSpiel,whichonlycomputesthebestresponsepolicyofasingleagent. Inotherwords,ifwedirectly
adoptthebuilt-inimplementation,theNashConvwillcorrespondtotheoriginalthree-playergame,notthemodifiedgame.
Unfortunately,computingtheexactjointpolicyoftheteammembersisnoteasyinpractice. Nevertheless,itisworthnoting
thatfromourexperiments,wefoundthatMMD-KLcaneffectivelysolvecooperativedecision-makingproblems. Asaresult,
wecanapplyMMD-KLtocomputetheapproximatebestresponseoftheteamasitisapurelycooperativeenvironmentfrom
theteam’sperspective(theotherteam’spolicyisfixedwhencomputingthebestresponseoftheteam). Forateamthatonly
hasasingleplayer,weusethebuilt-inimplementationinOpenSpieltocomputetheexactbestresponsepolicyoftheplayer.
Insummary,duringthepolicylearningprocess,whentheevaluationofthecurrentjointpolicyisneeded,weuseMMD-KL
asasubroutinetocomputeateam’sapproximatebestresponsewhileusingbuilt-inimplementationtocomputeasingle
player’sexactbestresponse. IntheMMD-KLsubroutine,thestartingpointofthebestresponseissettothecurrentjoint
policyoftheteammembers. Inexperiments,tobalancetheaccuracyoftheapproximatebestresponseandrunningtime,the
numberofupdatesintheMMD-KLsubroutineissetto100(thereturnedjointpolicycanbealsocalledabetterresponse).
Forexample,inMCCKuhn-A,supposethecurrentjointpolicyisπ =π π whereπ =π π istheteam’sjoint
team 3 team 1 2
× ⊙
policy. Thebuilt-inimplementationinOpenSpielcanonlycomputethebestresponsepolicyforeverysingleagentand
hence,theresultingNashConv(π)= 3 [V (ν,πBR π ) V (ν,π)]correspondstotheoriginalthree-playergame. In
i=1 i i × −i − i
contrast,inourmethod,weuseMMD-KLtocomputetheteam’sbestresponseratherthanthesingleagent’s. Therefore,the
(cid:80)
NashConvofπis:
NashConv(π)=V (ν,πBR π ) V (ν,π)
team team× 3 − team (27)
+V (ν,π πBR) V (ν,π),
3 team × 3 − 3
whereπBR istheteam’sBRpolicycomputedviaMMD-KLgiventhatplayer3isfixedtoπ (thatis,player3isapartof
team 3
theenvironmentfromtheteam’sperspective). Asplayers1and2arefullycooperative,theysharethesamevalueV .
team
25ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
F.MoreExperimentalResults
Inthissection,weprovidemoreexperimentaldetails,results,andanalysis. Webrieflysummarizeeachsectionbelow.
• SectionF.1. Moredetailsontheexperimentalsetup,includingthehyper-parametersettingsfordifferentmethods.
• SectionF.2. SearchingofM (thenumberofpreviouspolicies)andµ(thesmoothingparameterinDRS)(D1andD2).
• SectionF.3. Investigationofperformancew.r.t. thenumberofjointactions(D1andD2).
• SectionF.4. InvestigationofGMDwithdifferentheuristicstrategiesforadjustingα(D1andD2).
• SectionF.5. Investigationofdifferentmeta-controllers(D1andD2).
• SectionF.6. InvestigationofdifferentBregmandivergences(D1andD2).
• SectionF.7. Investigationoftheeffectivenessofaddingthemagnetpolicy(D1andD2).
• SectionF.8. Investigationofdifferentevaluationmeasuresanddifferentsolutionconcepts(D3andD4).
• SectionF.9. AnalysisofthecomputationalcomplexityforrunningdifferentalgorithmsonGAMEBENCH(D5andD6).
F.1.ExperimentalSetup
Hyper-parameters. Table7providesthedefaultvaluesofhyper-parametersusedindifferentmethods. IntheRS-type
meta-controllers (RSand DRS),the spherically symmetric distribution q is astandard multivariatenormal distribution
N(0,I).ForCMD/GMD,therearetwocriticalhyper-parameters:thenumberofpreviouspoliciesM 1andthesmoothing
≥
parameterµinDRS.InSectionF.2,weperformanablationstudytodeterminetheirdefaultvalues(giveninTable7),which
willbefixedinotherexperiments. Thespecificsetupsforeachexperimentwillbegivenineachofthefollowingsections.
Baselines. WeconsidertheMMD-type(MMD-KLandMMD-EU)andCFR-type(CFRandCFR+)algorithmsasthe
baselines. ItisworthnotingthatCFR-typealgorithmscanbealsoappliedtosingle-agentandcooperativecategories.
ComputationalResources. Experimentsareperformedonamachinewitha24-corei9andNVIDIAA4000. ForCMD,the
resultsareobtainedwith3randomseeds. Forothermethods,asthereisnorandomness,nomultiplerunsareneeded.
Table7.Defaultvaluesofthehyper-parametersindifferentmethods.Allthehyper-parametersinGMD–C,ι,andM –arealsousedin
CMD.ForCMD,itshyper-parametersalsoincludei)D(thenumberofsamples)andκ(updateinterval),whicharesharedfordifferent
MCs,ii)µintheDRSandRS,andiii)r Landr H intheGLD,GLDS,andDGLDS.
CMD MMD-KL/-EU
GMD Shared (D)RS (D)GLD(S)
Game K ϵ C ι M D κ µ r r ξ η η˜
L H
Kuhn-A 100000 1e-10 50 1e-6 1 5 10 0.05 0.01 0.05 1 0.1 0.05
Kuhn-B 100000 1e-10 50 1e-6 1 5 10 0.05 0.01 0.05 1 0.1 0.05
Goofspiel-S 100000 1e-10 50 1e-6 1 5 10 0.05 0.01 0.05 1 0.1 0.05
TinyHanabi-A 100000 1e-10 50 1e-6 3 5 10 0.05 0.01 0.05 1 0.1 0.05
TinyHanabi-B 100000 1e-10 50 1e-6 1 5 10 0.05 0.01 0.05 1 0.1 0.05
TinyHanabi-C 100000 1e-10 50 1e-6 1 5 10 0.05 0.01 0.05 1 0.1 0.05
Kuhn 100000 1e-10 50 1e-6 5 5 10 0.01 0.01 0.05 1 0.1 0.05
Leduc 100000 1e-10 50 1e-6 3 5 10 0.05 0.01 0.05 1 0.1 0.05
Goofspiel 100000 1e-10 50 1e-6 3 5 10 0.01 0.01 0.05 1 0.1 0.05
Bargaining 100000 1e-10 50 1e-6 5 5 10 0.05 0.01 0.05 1 0.1 0.05
TradeComm 100000 1e-10 50 1e-6 1 5 10 0.01 0.01 0.05 1 0.1 0.05
Battleship 100000 1e-10 50 1e-6 1 5 10 0.05 0.01 0.05 1 0.1 0.05
MCCKuhn-A 100000 1e-10 50 1e-6 1 5 10 0.01 0.01 0.05 1 0.1 0.05
MCCKuhn-B 100000 1e-10 50 1e-6 1 5 10 0.01 0.01 0.05 1 0.1 0.05
MCCGoofspiel 100000 1e-10 50 1e-6 1 5 10 0.01 0.01 0.05 1 0.1 0.05
26ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
F.2.NumberofHistoricalPoliciesandSmoothingParameter
Inthissection,weexploretheinfluenceofthenumberofpreviouspoliciesM andthesmoothingparameterµinDRSon
thelearningperformance. WeconsiderM 1,3,5 andµ 0.01,0.05 andthus,thereare6combinationsof(M,µ).
∈{ } ∈{ }
NotethatitwouldbeimpracticaltoenumerateallthecombinationsasM canbeanyintegergreaterthan0andµcanbeany
realnumbergreaterthan0.
TheexperimentalresultsareshowninFigure10. Fromtheresults,wecanseethatdifferentdecision-makingproblemsmay
requiredifferentM andµ. Notably,M =1,i.e.,onlyconsideringthecurrentpolicywhenderivingthenewpolicywhich
iscommoninexistingMDalgorithms,isnotalwaystheoptimalchoiceacrossdifferentdecision-makingproblems. For
example,inthemostdifficultLeducpokergame,whenM =1,CMDcannotdecreasetheNashConv,meaningthatonly
consideringthecurrentpolicyisineffectiveinsolvingthisgame. Bycomparison,wedeterminethedefaultvaluesofM and
µfordifferentgames,whicharegiveninTable7andwillbefixedinotherexperiments.
 . X K Q  $  . X K Q  %  * R R I V S L H O  6  7 L Q \ + D Q D E L  $  7 L Q \ + D Q D E L  %
                   
                   
                   
                                                  
                   
                    
      
          
      
          
      
          
      
          
      
          
 7 L Q \ + D Q D E L  &  . X K Q  / H G X F  * R R I V S L H O  % D U J D L Q L Q J
                   
                   
                   
                   
                   
                   
                   
      
          
      
          
   
           
      
          
   
           
 7 U D G H & R P P  % D W W O H V K L S  0 & & . X K Q  $  0 & & . X K Q  %  0 & & * R R I V S L H O
                   
                   
                   
                   
                   
                   
                    
      
          
      
          
      
          
   
           
      
          
                                                           
Figure10.Experimentalresultsforthecombinationsof(M,µ).Thefirst6figurescorrespondtosingle-agentandcooperativecategories
wherethey-axisisOptGap.Therestfigurescorrespondtoothercategorieswherethey-axisisNashConv.Forallthefigures,thex-axisis
thenumberofiterations.
27ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
F.3.Performancew.r.t. theNumberofJointActions
InFigure11,weplottheperformanceofdifferentmethodswithrespecttothenumberofjointactionsinvolvedineach
iteration. AsbothMD-typeandCFR-typealgorithmswilltraversethewholegametree,thenumberofjointactionsfor
agivenjointpolicyisthesame. Therefore,inthefigure,weonlyneedtochangethescaleofthex-axisfortheCMDby
multiplyingtheconstantD (thenumberofjointpoliciesevaluatedateachiteration), whilekeepingthescalesofother
methodsunchanged. WenotethatasDissmallinourexperiments(D = 5,i.e.,sample5candidatejointpolicies),the
conclusionsintermsofthenumberofiterationspresentedinthemaintextstillholdintermsofthenumberofjointactions.
AsdiscussedinSection8,oneofthefuturedirectionsofourworkwouldbethedevelopmentofamoreefficientmethodfor
updatingtheα,e.g.,amethodthatonlyneedstosampleonecandidate(inthiscase,thenumberofjointactionswillbe
thesameforbothCMDandotherbaselines). Nevertheless,comparedtobaselineMDandCFR-typealgorithms,CMD
providesafeasiblewaytostudydifferentsolutionconceptsandevaluationmeasures, though, inthecurrentversion, it
requiresevaluatingmultiplecandidatesateachiteration.
 . X K Q  $  . X K Q  %  * R R I V S L H O  6  7 L Q \ + D Q D E L  $  7 L Q \ + D Q D E L  %
                   
                   
                   
                                                  
                   
                    
      
          
      
          
      
          
      
          
      
          
 7 L Q \ + D Q D E L  &  . X K Q  / H G X F  * R R I V S L H O  % D U J D L Q L Q J
                   
       
   
       
               
                   
                   
               
       
   
       
      
          
      
          
   
           
      
          
   
           
 7 U D G H & R P P  % D W W O H V K L S  0 & & . X K Q  $  0 & & . X K Q  %  0 & & * R R I V S L H O
                   
                   
                   
                                                     
                   
                       
      
          
      
          
      
          
   
           
      
          
 & 0 '  * 0 '  0 0 '  . /  0 0 '  ( 8  & ) 5  & ) 5 
Figure11.Performance of different methods w.r.t. the number of joint actions. The first 6 figures correspond to single-agent and
cooperativecategorieswherethey-axisisOptGap.Therestfigurescorrespondtoothercategorieswherethey-axisisNashConv.Forall
thefigures,thex-axisisthenumberofiterations.
28ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
F.4.DifferentHeuristicStrategiesforAdjustingαinGMD
Inthemaintext,thebaselinemethodGMDemploysafixedstrategy–auniformdistribution–todeterminethevalueofα.
Inthissection,weconsidertwomoreheuristicstrategies: i)“GMD(LD)”denotesthattheαislinearlydecayedwiththe
iteration,andii)“GMD(ISR)”denotesthattheαisdecayedwiththeiterationintheformofinversesquarerootfunction
α = 1 ,wherek isthek-thiteration. TheresultsareshowninFigure12. Fromtheresults,wecanseethatdifferent
τ √k
heuristicstrategiescanperformdifferentlyindifferentdecision-makingscenarios;onecanbeatothersinsomescenarios
whileitcanalsobebeatenbyothersinotherscenarios.
 . X K Q  $  . X K Q  %  * R R I V S L H O  6  7 L Q \ + D Q D E L  $  7 L Q \ + D Q D E L  %
                   
                   
                   
                                                  
                   
                    
      
          
      
          
      
          
      
          
      
          
 7 L Q \ + D Q D E L  &  . X K Q  / H G X F  * R R I V S L H O  % D U J D L Q L Q J
                   
                   
       
   
       
                                       
       
   
       
                   
      
          
      
          
   
           
      
          
   
           
 7 U D G H & R P P  % D W W O H V K L S  0 & & . X K Q  $  0 & & . X K Q  %  0 & & * R R I V S L H O
                   
                   
                   
                                                     
                   
                       
      
          
      
          
      
          
   
           
      
          
 & 0 '  * 0 '  * 0 '  / '   * 0 '  , 6 5   0 0 '  . /  0 0 '  ( 8  & ) 5  & ) 5 
Figure12.ExperimentalresultsforGMDwithdifferentheuristicstrategiesforadjustingα.Thefirst6figurescorrespondtosingle-agent
andcooperativecategorieswherethey-axisisOptGap.Therestfigurescorrespondtoothercategorieswherethey-axisisNashConv.For
allthefigures,thex-axisisthenumberofiterations.
29ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
F.5.DifferentMeta-Controllers
Inthissection,weinvestigatetheeffectivenessofdifferentMCs,andtheresultsareshowninFigure13. Fromtheresults,we
canseethatDRScanconsistentlyoutperformalltheotherbaselineMCsacrossalmostallofthedecision-makingproblems.
Particularly,inLeducandMCCKuhn-B,DRSachievesasignificantlybetterconvergentperformancethanotherbaseline
MCs. AlthoughinMCCKuhn-A,GLDfinallyconvergestoalowerNashConvthanDRS,itcanperformmuchworsein
othergames,e.g.,inBattleship,GLDcannotdecreasetheNashConv,inLeducandMCCKuhn-B,itonlyconvergestoa
highvalueofNashConv. Inotherwords,GLDcannotconsistentlyworkwellacrossallthedecision-makingcategories. In
addition,inmostofthegames,theRS-typeMCstypicallyperformbetterthantheGLD-typeMCs. Wehypothesizethatthe
RS-typeMCsaremoreefficientinexploringtheparameterspaceastheyusemoresamples(αj andαj foreachuj)to
+
obtainthefinalupdateforthehyper-parameters. −
 . X K Q  $  . X K Q  %  * R R I V S L H O  6  7 L Q \ + D Q D E L  $  7 L Q \ + D Q D E L  %
                   
                   
                   
                                                  
                   
                    
      
          
      
          
      
          
      
          
      
          
 7 L Q \ + D Q D E L  &  . X K Q  / H G X F  * R R I V S L H O  % D U J D L Q L Q J
                   
                   
       
   
       
                                       
       
   
       
                   
      
          
      
          
   
           
      
          
   
           
 7 U D G H & R P P  % D W W O H V K L S  0 & & . X K Q  $  0 & & . X K Q  %  0 & & * R R I V S L H O
                   
                   
                   
                                                     
                   
                       
      
          
      
          
      
          
   
           
      
          
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '  & ) 5  & ) 5 
Figure13.ExperimentalresultsfordifferentMCs.Thefirst6figurescorrespondtosingle-agentandcooperativecategorieswherethe
y-axisisOptGap. Therestfigurescorrespondtoothercategorieswherethey-axisisNashConv. Forallthefigures,thex-axisisthe
numberofiterations.
EvolutionofHyper-Parameters. ThecriticalobservationthatsupportsourproposedDRSisthatthevalueoftheevaluation
measure isextremelysmallandconvergesrelativelyquickly,whichmakestheoriginalzero-ordermethodsstrugglein
L
adjustingthehyper-parametersastheytypicallyadjustthehyper-parametersdirectlybasedontheperformance. Tofurther
verify this intuition, we visualize the evolution of the hyper-parameter α over the learning process, which is shown in
Figure14–Figure18(respectivelycorrespondstosingle-agent,cooperative,zero-sum,general-sum,andMCCcategories).
Weuseindex0torepresentthemagnetpolicyandtherecentM historicalpoliciesareindexedby 1, ,M . Fromthe
{ ··· }
results,wecanseethatinallthegamesexceptLeduc,thevalueofαdeterminedbyRSalmostdoesnotchangeoverthe
learningprocess(thesamephenomenonisobservedforGLDSasitfollowsthesameideaofRS).InLeduc,thisvaluetends
todecreaseto0overthelearningprocess. Inotherwords,theregularizationisvanishing,whichexplainswhyRSandGLDS
cannotdecreasetheNashConvinthisgameasaddingregularizationhasbeenprovenimportanttosolvetwo-playerzero-sum
games(Sokotaetal.,2023;Liuetal.,2023). Inallthegames,DRSandDGLDSsharesomesimilaritiesindeterminingthe
valueofαanddifferfromGLD.Nevertheless,theconvergenceresultsinFigure13showthatDRSisthebestchoiceamong
themasitcanconsistentlyworkwellacrossallcategoriesofdecision-makingproblems.
30ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
 . X K Q  $
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
                   
                   
                   
                   
                                                           
 . X K Q  %
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
                   
                   
                   
                   
                                                           
 * R R I V S L H O  6
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
                   
                   
                   
                   
                                                           
Figure14.Theevolutionofthehyper-parametervaluesofdifferentMCsintheSingle-Agentcategory.They-axisisthevalueofα.The
x-axisisthenumberofiterations.
 7 L Q \ + D Q D E L  $
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
         
                   
         
                   
                   
                   
                                                           
 7 L Q \ + D Q D E L  %
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
                   
                   
                   
                   
                                                           
 7 L Q \ + D Q D E L  &
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
                   
                   
                   
                   
                                                           
Figure15.Theevolutionofthehyper-parametervaluesofdifferentMCsintheCooperativecategory.They-axisisthevalueofα.The
x-axisisthenumberofiterations.
31ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
 . X K Q
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
         
                   
         
                             
         
                   
                   
                                                           
 / H G X F
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
         
                   
         
                   
                   
                   
                                                           
 * R R I V S L H O
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
         
                   
         
                   
                   
                   
                                                           
Figure16.Theevolutionofthehyper-parametervaluesofdifferentMCsintheZero-Sumcategory.They-axisisthevalueofα.The
x-axisisthenumberofiterations.
 % D U J D L Q L Q J
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
         
                   
         
                             
         
                   
                   
                                                           
 7 U D G H & R P P
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
                   
                   
                   
                   
                                                           
 % D W W O H V K L S
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
                   
                   
                   
                   
                                                           
Figure17.Theevolutionofthehyper-parametervaluesofdifferentMCsintheGeneral-Sumcategory.They-axisisthevalueofα.The
x-axisisthenumberofiterations.
32ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
 0 & & . X K Q  $
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
                   
                   
                   
                   
                                                           
 0 & & . X K Q  %
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
                   
                   
                   
                   
                                                           
 0 & & * R R I V S L H O
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
                   
                   
                   
                   
                                                           
Figure18.Theevolutionofthehyper-parametervaluesofdifferentMCsintheMCCcategory.They-axisisthevalueofα.Thex-axisis
thenumberofiterations.
33ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
F.6.DifferentBregmanDivergences
OneoftheprominentfeaturesofourCMD(GMD)isthatitiscapableofexploringmorepossibleBregmandivergences.
Inthissection,weinvestigatehowCMDperformsunderthedifferentBregmandivergencesinducedbydifferentconvex
functionsinTable2(theplotsfortheseconvexfunctionsareshowninFigure8).
TheexperimentalresultsareshowninFigure19. Fromtheresults,wecanseethattheentropyfunctionxlnxisstillagood
choiceacrossallthegames. Nevertheless,insomegames,thereexistotherconvexfunctionsthatarebetterchoices. For
example,inKuhn-A,TinyHanabi-B,TinyHanabi-C,Goofspiel,TradeComm,andMCCGoofspiel,x2isbetterthanxlnx.
Furthermore, inMCCGoofspiel, ex isalsobetterthanxlnx, whichverifiesthattheKLdivergence(xlnx)orsquared
Euclideannorm(x2)couldbenotalwaysthebestchoiceacrossdifferentgames. Ontheotherhand,evenundertheentropy
functionxlnx,ourCMDcouldalsooutperformMMD-KLinsomegamessuchasMCCKuhn-AandMCCKuhn-B.
 . X K Q  $  . X K Q  %  * R R I V S L H O  6  7 L Q \ + D Q D E L  $  7 L Q \ + D Q D E L  %
                   
                   
                   
                                               
                     
                     
      
          
      
          
      
          
      
          
      
          
 7 L Q \ + D Q D E L  &  . X K Q  / H G X F  * R R I V S L H O  % D U J D L Q L Q J
                   
                   
       
   
       
                                       
        
   
        
                     
      
          
      
          
   
           
      
          
   
           
 7 U D G H & R P P  % D W W O H V K L S  0 & & . X K Q  $  0 & & . X K Q  %  0 & & * R R I V S L H O
                   
                   
                   
                                               
                     
                      
      
          
      
          
      
          
   
           
      
          
xlnx x2 x0.1 ex  0 0 '  . /  0 0 '  ( 8  & ) 5  & ) 5 
Figure19.ExperimentalresultsfordifferentBregmandivergences. Thefirst6figurescorrespondtothesingle-agentandcooperative
categorieswherethey-axisisOptGap.Therestfigurescorrespondtoothercategorieswherethey-axisisNashConv.Forallthefigures,
thex-axisisthenumberofiterations.
34ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
F.7.EffectivenessofMagnet
AsmentionedinSectionD.2,inourexperiments,wheninstantiatingCMD,webydefaultaddamagnetpolicy(theinitial
policy)intothepolicyupdatingasithasbeendemonstratedthataddingamagnetpolicyispowerfulinsolvingtwo-player
zero-sumgames(Sokotaetal.,2023;Liuetal.,2023). Toverifythis,weconductanablationstudywhere“CMDw/oMag”
denotesthemethodthatonlyconsidersthemostrecentM historicalpolicieswithoutaddingtheinitialpolicy.
TheexperimentalresultsareshowninFigure20. Fromtheresults,wecanseethati)Forsingle-agentandcooperative
categories,addingthemagnetpolicycouldresultinaslightlyslowerconvergencerate;wehypothesizethatthismaybedue
tothefactthatthesingle-agentandcooperativegamesarerelativelysimplerthantheothergames(asshowninTable6,
thenumbersofdecisionpointsofsingle-agentandcooperativegamesaresmallerthantheothergames). ii)Fortheother
threecategories,addingthemagnetpolicyisnecessaryforCMDtoworkconsistentlywellacrossallthegames;thoughin
MCCGoofspiel,CMDfinallyconvergestoalowerNashConvwithoutthemagnet,itcouldperformworseinsomeother
games,e.g.,inBattleship,itfinallydivergeswithoutthemagnet,andinLeduc,MCCKuhn-A,andMCCKuhn-B,itconverges
toahighNashConvwithoutthemagnet. Nevertheless,aspointedoutinSectionD.2,thisdefaultinstanceofCMD(GMD)
shouldnotbeconfusedwiththeoriginalMMDevenwhenM =1asthepolicyupdatingruleisderivedviaanumerical
method,insteadofrelyingontheclosed-formsolution(Sokotaetal.,2023).
 . X K Q  $  . X K Q  %  * R R I V S L H O  6  7 L Q \ + D Q D E L  $  7 L Q \ + D Q D E L  %
                   
                   
                   
                                                  
                   
                    
      
          
      
          
      
          
      
          
      
          
 7 L Q \ + D Q D E L  &  . X K Q  / H G X F  * R R I V S L H O  % D U J D L Q L Q J
                   
       
   
       
               
                   
                   
               
       
   
       
      
          
      
          
   
           
      
          
   
           
 7 U D G H & R P P  % D W W O H V K L S  0 & & . X K Q  $  0 & & . X K Q  %  0 & & * R R I V S L H O
                   
                   
   
   
       
   
                                  
   
   
       
   
                      
      
          
      
          
      
          
   
           
      
          
 & 0 '  & 0 '  Z  R  0 D J  0 0 '  . /  0 0 '  ( 8  & ) 5  & ) 5 
Figure20.Experimentalresultsfortheeffectivenessofaddingthemagnetpolicy. Thefirst6figurescorrespondtosingle-agentand
cooperativecategorieswherethey-axisisOptGap.Therestfigurescorrespondtoothercategorieswherethey-axisisNashConv.Forall
thefigures,thex-axisisthenumberofiterations.
35ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
F.8.DifferentMeasures
Inthissection,weapplyourCMDtodifferentsolutionconceptsandevaluationmeasures(i.e.,thedesiderataD3andD4
presentedintheIntroduction). NotethatwhenrunningCMDfordifferentevaluationmeasures,onlyminimalmodifications
arerequired: changingtheMC’soptimizationobjective . WefirstinvestigatetheCCEGap(Moulin&Vial,1978;Marris
L
etal.,2021)inSectionF.8.1andthenthesocialwelfare(Davis&Whinston,1962)inSectionF.8.2.
F.8.1.CCEGAP
Notethatintwo-playerzero-sumgames,NEandCCEcanbeshowntobepayoffequivalent(v.Neumann,1928). Therefore,
weconductexperimentsonthethree-playerKuhnandGoofspiel. WefollowthesameexperimentalpipelineofOptGapand
NashConv: i)investigatingthecombinationofM andµ,ii)investigatingdifferentMCs,iii)investigatingdifferentBregman
divergences,andiv)investigatingtheeffectivenessofmagnet.
NumberofHistoricalPoliciesandSmoothingParameter. Wefirstinvestigatetheinfluenceofthenumberofprevious
policiesM andthesmoothingparameterµinDRSonthelearningperformance. SimilartoOptGap/NashConv,weconsider
M 1,3,5 andµ 0.01,0.05 ,andtheresultsareshowninFigure21. Wecangetthesameconclusion: different
∈ { } ∈ { }
gamesmayrequiredifferentM andµ. Bycomparison,wedeterminetheirdefaultvalueswhichwillbefixedintheother
experiments: (M,µ) = (3,0.01)forbothKuhnandGoofspiel. Alltheotherhyper-parametersettingsarethesameas
OptGap/NashConvgiveninTable7.
 . X K Q  * R R I V S L H O
       
                           
                   
       
                   
                             
                             
                             
         
                                                             
 , W H U D W L R Q  , W H U D W L R Q
Figure21. Experimentalresultsforthecombinationsof(M,µ)underthemeasureCCEGap.
DifferentMeta-Controllers. Then,weinvestigatetheeffectivenessofdifferentMCs. InFigure22,wepresentthelearning
curvesoftheperformanceofdifferentMCs,andinFigure23,wepresenttheevolutionofthevalueofαdeterminedby
differentMCsoverthelearningprocess. WecangetthesameconclusionasOptGap/NashConv: DRSisthebestchoice
amongthe5MCs. FromtheevolutionofαweobservethatDRSandDGLDSfollowtwodifferentpatternstodeterminethe
valueofα,whichisnotthecaseforOptGap/NashConv(seeFigure14–Figure18). Incontrast,wefoundthatsinceGLD
followsasimilarpatterntoDRS,itperformsonparwithorbetterthanDGLDS.
 . X K Q  * R R I V S L H O
       
     '  ' 5  * 6
 / ' 6
 5  * 6
 / ' 6
 *  & ) /  5 '  & ) 5       '  ' 5  * 6
 / ' 6
 5  * 6
 / ' 6
 *  & ) /  5 '  & ) 5 
       
       
       
       
       
      
                      
      
                      
 , W H U D W L R Q  , W H U D W L R Q
Figure22. Experimentalresultsfordifferentmeta-controllersunderthemeasureCCEGap.
36
 S D * ( & &
 S D * ( & &
 S D * ( & &
 S D * ( & &ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
 . X K Q
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
         
                             
                   
                   
                                                                               
 * R R I V S L H O
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
         
                             
                   
                   
                                                                               
Figure23.Evolutionofthehyper-parametersofdifferentMCs.They-axisisthevalueofα.Thex-axisisthenumberofiterations.
DifferentBregmanDivergences. Next,weinvestigatehowCMDperformsunderdifferentBregmandivergencesinduced
bydifferentconvexfunctionsinTable2,andtheresultsaregiveninFigure24. Wecangetthesameconclusionsasfor
OptGap/NashConv: i)theentropyfunctionψ(x)=xlnxisstillagoodchoiceindifferentgames,ii)therecouldexistother
convexfunctionsthatarebetterthantheentropyfunction,e.g.,x2andexinGoofspiel,iii)evenundertheentropyfunction
ψ(x)=xlnx,ourCMDcanconvergefasterthantheSOTAMMD-KLintermsofthenumberofiterations.
 . X K Q  * R R I V S L H O
       
    x xl 2nx exx0.1  0  0 0  0 '  '    .  ( 8 /  &  & )  ) 5
 5 
    x xl 2nx exx0.1  0  0 0  0 '  '    .  ( 8 /  &  & )  ) 5
 5 
       
   
   
   
   
   
       
       
      
                      
      
                      
 , W H U D W L R Q  , W H U D W L R Q
Figure24. ExperimentalresultsfordifferentBregmandivergencesunderthemeasureCCEGap.
EffectivenessofMagnet. Finally,weinvestigatetheeffectivenessofaddingthemagnetpolicytothepolicyupdating,and
theresultsarepresentedinFigure25. WecanobserveasimilarphenomenontoNashConv: inKuhn,CMDconverges
remarkablyfaster(intermsofthenumberofiterations)thanalltheothermethods,andinGoofspiel,itconvergesremarkably
faster(intermsofthenumberofiterations)thanalltheothermethodsexceptthe“CMDw/oMag”. Nevertheless,wecan
stillconcludethataddingthemagnetpolicyisnecessaryforourCMD.
 . X K Q  * R R I V S L H O
       
          &  & 0  0 '  '  Z  R  0 D J  0  0 0  0 '  '    .  ( 8 /  &  & )  ) 5  5               &  & 0  0 '  '  Z  R  0 D J  0  0 0  0 '  '    .  ( 8 /  &  & )  ) 5  5 
       
       
            
        
      
                      
      
                      
 , W H U D W L R Q  , W H U D W L R Q
Figure25. ExperimentalresultsfortheeffectivenessofthemagnetpolicyunderthemeasureCCEGap.
37
 S D * ( & &
 S D * ( & &
 S D * ( & &
 S D * ( & &ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
F.8.2.SOCIALWELFARE
Inthissection,weapplyourmethodstotheevaluationmeasure–socialwelfare(Davis&Whinston,1962). Weconduct
experimentsonthegeneral-sumgames,andtheresultsareshowninFigure26. Inthisexperiment,weusethedefaultvalues
inTable7forthehyper-parameters.
Fromthetoplineofthefigure,wecanseethatCMD/GMDcanempiricallyachievecompetitiveorbettersocialwelfare
comparedtootherbaselines,demonstratingtheeffectivenessofourmethodwhenconsideringdifferentmeasures.
Inthemedianlineofthefigure,weplottheNashConvwhentheMC’sobjective issocialwelfare. Wecanseethatin
L
BargainingandTradeComm,thelearningstillcanconvergetotheapproximateNEeventhoughtheMC’sobjectiveissocial
welfare,nottheNashConv. InBattleship,whileCMDcangetahigher(average)socialwelfare,thefinaljointpolicyis
notanNEasitsNashConvcannotconverge. Inotherwords,anefficient(intermsofsocialwelfare)jointpolicycould
notnecessarilybeanNE.Thissuggestsoneofthefuturedirections: howtoefficientlylearntheNEwithmaximumsocial
welfare,whichinvolvestheequilibriumselectionproblem(Harsanyietal.,1988).
AnotherintuitiveconsequenceofsettingtheMC’soptimizationobjectivetosocialwelfareisthatthelearningcouldnot
convergetotheNEorconvergeslowerthanthecasewheretheMC’sobjectiveisdirectlytheNashConv. Asshowninthe
bottomlineofthefigure,weplottheNashConvofthetwocases. InBargainingandTradeComm,CMDwithNashConvas
theMC’sobjectivecanconvergetotheNEfasterthanthatwithSWastheMC’sobjective. InBattleship,CMDwithSWas
theMC’sobjective,thoughcouldachieveahighersocialwelfare,cannotconvergetheNashequilibrium.
 % D U J D L Q L Q J  7 U D G H & R P P  % D W W O H V K L S
              
 & 0 '  0 0 '  . /  & ) 5  & 0 '  0 0 '  . /  & ) 5
           * 0 '  0 0 '  ( 8  & ) 5        * 0 '  0 0 '  ( 8  & ) 5 
         
    
         
    
         
    
         
      &  * 0  0 '  '  0  0 0  0 '  '    .  ( 8 /  &  & )  ) 5  5           
             
                                                                             
 , W H U D W L R Q  , W H U D W L R Q  , W H U D W L R Q
     % D U J D L Q L Q J      7 U D G H & R P P      % D W W O H V K L S
     & 0 '  0 0 '  . /  & ) 5      & 0 '  0 0 '  . /  & ) 5      & 0 '  0 0 '  . /  & ) 5
     * 0 '  0 0 '  ( 8  & ) 5       * 0 '  0 0 '  ( 8  & ) 5       * 0 '  0 0 '  ( 8  & ) 5 
           
                                
           
           
   
                       
      
                      
      
                      
 , W H U D W L R Q  , W H U D W L R Q  , W H U D W L R Q
     % D U J D L Q L Q J      7 U D G H & R P P      % D W W O H V K L S
     & 0 '   6 :   & 0 '   1 D V K & R Q Y       & 0 '   6 :   & 0 '   1 D V K & R Q Y       & 0 '   6 :   & 0 '   1 D V K & R Q Y 
   
   
   
       
   
   
   
   
   
   
           
   
                       
      
                      
      
                      
 , W H U D W L R Q  , W H U D W L R Q  , W H U D W L R Q
Figure26. Experimentalresultsfortheevaluationmeasure–socialwelfare.
38
 H U D I O H :  O D L F R 6
 Y Q R & K V D 1
 Y Q R & K V D 1
 H U D I O H :  O D L F R 6
 Y Q R & K V D 1
 Y Q R & K V D 1
 H U D I O H :  O D L F R 6
 Y Q R & K V D 1
 Y Q R & K V D 1ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
F.9.ComputationalComplexity
Inthissection,wegivesomeremarksonthecomputationalcomplexityofdifferentmethods. InSectionF.9.1,wefocus
ontherunningtimeofdifferentmethods,andinSectionF.9.2wepresentthememoryconsumptionofdifferentmethods.
Note that i) these numbers are obtained under the default values of hyper-parameters (Table 7), and ii) these numbers
arenotabsoluteanddependonthepropertyofthegame(seeTable6)andthecomputationalresourcesusedtorunthe
experiments(seeSectionF.1);theyonlyprovideintuitiononthecomputationalcomplexityofdifferentmethodstoshow
thatourGAMEBENCHcansatisfythedesiderataD5andD6mentionedintheIntroduction.
F.9.1.RUNNINGTIME
TherunningtimeofdifferentmethodsindifferentgamesisshowninTable8. Fromtheresults,wecanseethatinmostof
thegames,runningallthealgorithmsdoesnotcauseaverylongrunningtime(thedesiderataD5andD6presentedinthe
Introduction),evenforourmethodswhereasetofextraoperationsisrequired: GMDrequirescomputingthevalueofthe
dualvariableviaanumericalmethodandCMDfurtherrequirestoevaluatemultiplecandidatesofthehyper-parameters.
Notably,weemphasizethat: i)Ourmethods(CMD/GMD)providethecapabilityofexploringmoredimensionsofdecision
making, though they require extra computational cost (the major limitation of the current version of our methods); ii)
ComparingCMDandGMD,wecanseethatthemajorcostcomesfromevaluatingmultiplesamples. Therefore,aspointed
out in Section 8, we view this as a future direction: developing more computationally efficient hyper-parameter value
updatingmethodswithoutsacrificingperformance. Inthisregard,othertechniquessuchasBayesianoptimization(Lindauer
etal.,2022)orofflinehyper-parameteroptimizationapproaches(Chenetal.,2022)mayberequired.
Table8. Therunningtimeofoneiterationofdifferentmethodsindifferentgames(second).
MMD MMD CMD
Game CFR CFR+ GMD
-KL -EU
DRS RS DGLDS GLDS GLD
Kuhn-A 0.0004 0.0004 0.0003 0.0003 0.0034 0.0372 0.0372 0.0204 0.0204 0.0203
Kuhn-B 0.0004 0.0004 0.0003 0.0003 0.0033 0.0370 0.0365 0.0202 0.0201 0.0202
Goofspiel-S 0.0007 0.0006 0.0004 0.0004 0.0046 0.0491 0.0489 0.0269 0.0275 0.0270
TinyHanabi-A 0.0006 0.0006 0.0004 0.0004 0.0045 0.0474 0.0472 0.0262 0.0268 0.0260
TinyHanabi-B 0.0004 0.0004 0.0003 0.0003 0.0033 0.0366 0.0370 0.0198 0.0192 0.0197
TinyHanabi-C 0.0004 0.0004 0.0003 0.0003 0.0032 0.0364 0.0359 0.0195 0.0195 0.0199
Kuhn 0.0084 0.0082 0.0022 0.0021 0.0267 0.4102 0.4058 0.2273 0.2282 0.2288
Leduc 0.0942 0.0961 0.0422 0.0412 0.5146 6.8897 6.9749 3.8188 3.8116 3.8744
Goofspiel 0.0072 0.0073 0.0014 0.0014 0.0167 0.2879 0.2899 0.1636 0.1610 0.1625
Bargaining 0.0279 0.0273 0.0130 0.0116 0.1093 1.5311 1.5308 0.8428 0.8579 0.8516
TradeComm 0.0028 0.0029 0.0011 0.0010 0.0121 0.1704 0.1699 0.0957 0.0942 0.0939
Battleship 0.0245 0.0248 0.0097 0.0094 0.1125 1.5570 1.5771 0.8833 0.8774 0.8835
MCCKuhn-A 0.0083 0.0084 0.0021 0.0021 0.0264 6.9054 6.8377 4.0453 4.1461 4.1034
MCCKuhn-B 0.0080 0.0083 0.0021 0.0021 0.0260 6.8100 6.7847 4.1047 4.0944 4.1104
MCCGoofspiel 0.0070 0.0073 0.0014 0.0014 0.0167 5.3541 5.3852 3.1817 3.1945 3.1836
39ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
F.9.2.MEMORYUSAGE
ThememoryusageofdifferentmethodsindifferentgamesisprovidedinTable9. Fromtheresults,wecanseethatrunning
allthealgorithmsdoesnotcausemuchmemoryconsumption,whichshowsthatourGAMEBENCHisacademic-friendly.
Table9. Thememoryusageofdifferentmethodsindifferentgames(MB).
MMD MMD CMD
Game CFR CFR+ GMD
-KL -EU
DRS RS DGLDS GLDS GLD
Kuhn-A 0.8750 0.9805 0.3711 0.3750 0.9492 1.0352 1.0859 0.3750 0.3750 0.3750
Kuhn-B 0.8672 0.8672 0.4297 0.3750 0.7461 1.1289 1.0352 0.3750 0.3750 0.3164
Goofspiel-S 1.1875 1.2969 1.2617 1.2539 1.2578 1.2578 1.2578 1.6992 1.6953 1.6992
TinyHanabi-A 1.0352 1.0156 0.4805 0.4258 0.4883 1.0312 1.1836 0.4453 0.4297 0.4883
TinyHanabi-B 0.9922 1.0898 0.4922 0.4844 0.4336 0.4922 0.5430 0.4922 0.4297 0.4922
TinyHanabi-C 0.9805 0.9922 0.4336 0.4297 0.4336 0.4922 0.4336 0.4336 0.4336 0.4336
Kuhn 1.9961 2.0078 3.0352 3.1367 3.5586 3.7734 3.7227 3.5156 3.5352 3.5273
Leduc 26.262 26.344 51.664 52.465 58.273 59.063 58.555 52.688 51.949 51.359
Goofspiel 2.4844 2.4297 3.5039 3.4961 4.3555 4.3359 4.3047 4.0391 4.0430 4.0898
Bargaining 10.633 10.578 24.816 24.852 35.129 35.422 35.020 31.941 32.344 31.781
TradeComm 1.4961 1.5430 2.0156 2.0703 2.0664 2.0078 2.0117 2.2461 2.2461 2.3633
Battleship 6.9102 6.9023 13.539 13.422 13.543 13.543 13.484 12.098 12.148 12.332
MCCKuhn-A 2.5742 2.5195 2.0312 2.0273 2.5586 2.6719 2.7266 2.3047 2.2930 2.2930
MCCKuhn-B 2.4688 2.5703 2.0273 1.9648 1.9727 2.6562 2.7617 2.2383 2.2305 2.1797
MCCGoofspiel 2.7500 2.7500 3.0078 3.0820 3.0781 3.0195 3.1289 2.8203 2.8047 2.8672
40