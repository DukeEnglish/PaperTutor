PublishedasaconferencepaperatICLR2024
EFFICIENT MULTI-AGENT REINFORCEMENT LEARN-
ING BY PLANNING
QihanLiu1∗,JianingYe2∗,XiaotengMa1∗,JunYang1†,BinLiang1,ChongjieZhang3†
1DepartmentofAutomation,TsinghuaUniversity
2InstituteforInterdisciplinaryInformationSciences,TsinghuaUniversity
3DepartmentofComputerScience&Engineering,WashingtonUniversityinSt. Louis
{lqh20, yejn21, ma-xt17}@mails.tsinghua.edu.cn
{yangjun603, bliang}@tsinghua.edu.cn
chongjie@wustl.edu
ABSTRACT
Multi-agent reinforcement learning (MARL) algorithms have accomplished re-
markable breakthroughs in solving large-scale decision-making tasks. Nonethe-
less, most existing MARL algorithms are model-free, limiting sample efficiency
and hindering their applicability in more challenging scenarios. In contrast,
model-basedreinforcementlearning(MBRL),particularlyalgorithmsintegrating
planning,suchasMuZero,hasdemonstratedsuperhumanperformancewithlim-
iteddatainmanytasks.Hence,weaimtoboostthesampleefficiencyofMARLby
adopting model-based approaches. However, incorporating planning and search
methods into multi-agent systems poses significant challenges. The expansive
action space of multi-agent systems often necessitates leveraging the nearly-
independent property of agents to accelerate learning. To tackle this issue, we
proposetheMAZeroalgorithm,whichcombinesacentralizedmodelwithMonte
CarloTreeSearch(MCTS)forpolicysearch.Wedesignanovelnetworkstructure
tofacilitatedistributedexecutionandparametersharing. Toenhancesearcheffi-
ciencyindeterministicenvironmentswithsizableactionspaces,weintroducetwo
novel techniques: Optimistic Search Lambda (OS(λ)) and Advantage-Weighted
Policy Optimization (AWPO). Extensive experiments on the SMAC benchmark
demonstratethatMAZerooutperformsmodel-freeapproachesintermsofsample
efficiency and provides comparable or better performance than existing model-
basedmethodsintermsofbothsampleandcomputationalefficiency. Ourcodeis
availableat https://github.com/liuqh16/MAZero.
1 INTRODUCTION
Multi-AgentReinforcementLearning(MARL)hasseensignificantsuccessinrecentyears,withap-
plicationsinreal-timestrategygames(Arulkumaranetal.,2019;Yeetal.,2020),cardgames(Bard
etal.,2020),sportsgames(Kurachetal.,2020),autonomousdriving(Zhouetal.,2020),andmulti-
robotnavigation(Longetal.,2018). Nonetheless,challengeswithinmulti-agentenvironmentshave
ledtotheproblemofsampleinefficiency. Onekeyissueisthenon-stationarityofmulti-agentset-
tings,whereagentscontinuouslyupdatetheirpoliciesbasedonobservationsandrewards,resulting
inachangingenvironmentforindividualagents(Nguyenetal.,2020). Additionally,thejointaction
space’s dimension can exponentially increase with the number of agents, leading to an immense
policy search space (Hernandez-Leal et al., 2020). These challenges, combined with issues such
as partial observation, coordination, and credit assignment, necessitate a considerable demand for
samplesinMARLforeffectivetraining(Gronauer&Diepold,2022).
Conversely, Model-Based Reinforcement Learning (MBRL) has demonstrated its worth in terms
of sample efficiency within single-agent RL scenarios, both in practice (Wang et al., 2019) and
*Equalcontribution.
†Correspondingauthors.
1
4202
yaM
02
]GL.sc[
1v87711.5042:viXraPublishedasaconferencepaperatICLR2024
theory(Sunetal.,2019). Unlikemodel-freemethods,MBRLapproachestypicallyfocusonlearn-
ing a parameterized model that characterizes the transition or reward functions of the real envi-
ronment (Sutton & Barto, 2018; Corneil et al., 2018; Ha & Schmidhuber, 2018). Based on the
usageofthelearnedmodel, MBRLmethodscanberoughlydividedintotwolines: planningwith
model(Hewingetal.,2020;Nagabandietal.,2018;Wang&Ba,2019;Schrittwieseretal.,2020;
Hansenetal.,2022)anddataaugmentationwithmodel(Kurutachetal.,2018;Janneretal.,2019;
Hafneretal.,2019;2020;2023). Duetotheforesightinherentinplanningmethodsandtheirtheo-
reticallyguaranteedconvergenceproperties,MBRLwithplanningoftendemonstratessignificantly
highersampleefficiencyandconvergesmorerapidly(Zhangetal.,2020). Awell-knownplanning-
basedMBRLmethodisMuZero,whichconductsMonteCarloTreeSearch(MCTS)withavalue-
equivalentlearnedmodelandachievessuperhumanperformanceinvideogameslikeAtariandboard
gamesincludingGo,ChessandShogi(Schrittwieseretal.,2020).
Despite the success of MBRL in single-agent settings, planning with the multi-agent environment
modelremainsinitsearlystagesofdevelopment. Recenteffortshaveemergedtobridgethegapby
combining single-agent MBRL algorithms with MARL frameworks (Willemsen et al., 2021; Bar-
giacchi et al., 2021; Mahajan et al., 2021; Egorov & Shpilman, 2022). However, the extension of
single-agentMBRLmethodstomulti-agentsettingspresentsformidablechallenges. Ononehand,
existing model designs for single-agent algorithms do not account for the unique biases inherent
tomulti-agentenvironments,suchasthenearly-independentpropertyofagents. Consequently,di-
rectly employing models from single-agent MBRL algorithms typically falls short in supporting
efficient learning within real-world multi-agent environments, underscoring the paramount impor-
tance of model redesign. On the other hand, multi-agent environments possess action spaces sig-
nificantly more intricate than their single-agent counterparts, thereby exponentially escalating the
searchcomplexitywithinmulti-agentsettings. Thisnecessitatesexplorationintospecializedplan-
ningalgorithmstailoredtothesecomplexactionspaces.
Inthispaper,weproposeMAZero,thefirstempiricallyeffectiveapproachthatextendstheMuZero
paradigmintomulti-agentcooperativeenvironments. Inparticular,ourcontributionsarefourfold:
1. InspiredbytheCentralizedTrainingwithDecentralizedExecution(CTDE)concept,wedevelop
a centralized-value, individual-dynamic model with shared parameters among all agents. We
incorporateanadditionalcommunicationblockusingtheattentionmechanismtopromotecoop-
erationduringthemodelunrolling(seeSection4.1).
2. Given the deterministic characteristics of the learned model, we have devised the Optimistic
SearchLambda(OS(λ))approach(seeSection4.2). Itoptimisticallyestimatesthesampledre-
turnswhilemitigatingunrollingerrorsatlargerdepthsusingtheparameterλ.
3. WeproposeanovelpolicylossnamedAdvantage-WeightedPolicyOptimization(AWPO)(see
Section4.3)utilizingthevalueinformationcalculatedbyOS(λ)toimprovethesampledactions.
4. We conduct extensive experiments on the SMAC benchmark, showing that MAZero achieves
superiorsampleefficiencycomparedtomodel-freeapproachesandprovidesbetterorcompara-
ble performance than existing model-based methods in terms of both sample and computation
efficiency(seeSection5).
2 BACKGROUND
Thissectionintroducesessentialnotations,andrelatedworkswillbediscussedinAppendixA.
POMDP ReinforcementLearning(RL)addressestheproblemofanagentlearningtoactinanen-
vironmentinordertomaximizeascalarrewardsignal,whichischaracterizedasapartiallyobserv-
able Markov decision process (POMDP) (Kaelbling et al., 1998) defined by (S,A,T,U,Ω,O,γ),
whereS isasetofstates,Aisasetofpossibleactions,T :S×A×S →[0,1]isatransitionfunc-
tionovernextstatesgiventheactionsatcurrentstates,U :S×A→Ristherewardfunction. Ωis
thesetofobservationsfortheagentandobservingfunctionO :S →Ωmapsstatestoobservations.
γ ∈[0,1)isthediscountedfactor. Ateachtimestept,theagentacquireanobservationo =O(s )
t t
based on current state s , choose action a upon the history of observations o and receive cor-
t t ≤t
responding reward u = U(s ,a ) from the environment. The objective of the agent is to learn a
t t t
policyπthatmaximizestheexpecteddiscountedreturnJ(π)=E [(cid:80)∞ γtu |a ∼π(·|o )].
π t=0 t t ≤t
2PublishedasaconferencepaperatICLR2024
MuZero MuZero (Schrittwieser et al., 2020) is an MBRL algorithm for single-agent settings
that amalgamates a learned model of environmental dynamics with MCTS planning algorithm.
MuZero’slearnedmodelconsistsofthreefunctions: arepresentationfunctionh,adynamicsfunc-
tiong,andapredictionfunctionf. Themodelisconditionedontheobservationhistoryo anda
≤t
sequenceoffutureactionsa andistrainedtopredictrewardsr ,valuesv andpolicies
t:t+K t,0:K t,0:K
p ,wherer ,v ,p aremodelpredictionsbasedonimaginaryfuturestates unrollingk
t,0:K t,k t,k t,k t,k
steps from current time t. Specifically, the representation function maps the current observation
historyo intoahiddenstates ,whichisusedastherootnodeoftheMCTStree. Thedynamic
≤t t,0
functionginputstheprevioushiddenstates withanactiona andoutputsthenexthiddenstate
t,k t+k
s andthepredictedrewardr . Thepredictionfunctionf computesthevaluev andpolicy
t,k+1 t,k t,k
p at each hidden state s . To perform MCTS, MuZero runs N simulation steps where each
t,k t,k
consists of 3 stages: Selection, Expansion and Backup. During the Selection stage, MuZero starts
traversing from the root node and selects action by employing the probabilistic Upper Confidence
Tree(pUCT)rule(Kocsis&Szepesva´ri,2006;Silveretal.,2016)untilreachingaleafnode:
(cid:34) (cid:112)(cid:80) (cid:35)
N(s,b)
a=argmax Q(s,a)+P(s,a)· b ·c(s) (1)
a 1+N(s,a)
where Q(s,a) denotes the estimation for Q-value, N(s,a) the visit count, P(s,a) the prior prob-
ability of selecting action a received from the prediction function, and c(s) is used to control the
influence of the prior relative to the Q-value. The target search policy π(·|s ) is the normalized
t,0
distributionofvisitcountsattherootnodes . Themodelistrainedminimizethefollowing3loss:
t,0
rewardlossl ,valuelossl andpolicylossl betweentruevaluesandnetworkpredictions.
r v p
K
(cid:88)
LMuZero = [l (u ,r )+l (z ,v )+l (π ,p )] (2)
r t+k t,k v t+k t,k p t+k t,k
k=0
whereu istherealrewardfromenvironment,z =(cid:80)n−1γiu +γt+k+nν isn-step
t+k t+k i=0 t+k+i t+k+n
returnconsistsofrewardandMCTSsearchvalue,π istheMCTSsearchpolicy.
t+k
EfficientZero(Yeetal.,2021)proposesanasynchronousparallelworkflowtoalleviatethecomputa-
tionaldemandsofthereanalysisMCTS.EfficientZeroincorporatestheself-supervisedconsistency
lossl =∥s −s ∥usingtheSimSiamnetworktoensureconsistencybetweenthehiddenstate
s t,k t+k,0
fromthekthstep’sdynamics andthedirectrepresentationofthefutureobservations .
t,k t+k,0
Sampled MuZero Since the efficiency of MCTS planning is intricately tied to the number of
simulations performed, the application of MuZero has traditionally been limited to domains with
relativelysmallactionspaces,whichcanbefullyenumeratedoneachnodeduringthetreesearch.
Totacklelargeractionspaces,SampledMuZero(Hubertetal.,2021)introducesasampling-based
frameworkwherepolicyimprovementandevaluationarecomputedoversmallsubsetsofsampled
actions. Specifically,everytimeintheExpansionstage,onlyasubsetT(s)ofthefullactionspace
Aisexpanded,whereT(s)ison-timeresampledfromapolicyβ baseonthepriorpolicyπ. After
that,intheSelectionstage,anactionispickedaccordingtothemodifiedpUCTformula.
(cid:34) βˆ (cid:112)(cid:80) N(s,b) (cid:35)
a= argmax Q(s,a)+ P(s,a)· b ·c(s) (3)
β 1+N(s,a)
a∈T(s)⊂A
whereβˆistheempiricaldistributionofsampledactions,whichmeansitssupportisT(s).
Inthisway,whenmakingactualdecisionsatsomerootnodestates ,SampledMCTSwillyielda
t,0
policyω(·|s )afterNsimulations,whichisastochasticpolicysupportedinT(s ).Theactualim-
t,0 t,0
provedpolicyisdenotedasπMCTS(·|s )=E[ω(·|s )],whichistheexpectationofω(·|s ). The
t,0 t,0 t,0
randomness that the expectation is taken over is from all sampling operations in Sampled MCTS.
Asdemonstratedintheoriginalpaper,SampledMuZerocanbeseamlesslyextendedtomulti-agent
settingsbyconsideringeachagent’spolicyasadistinctdimensionforasingleagent’smulti-discrete
actionspaceduringcentralizedplanning. TheimprovedpolicyisdenotedasπππMCTS(·|sss ).1
t,0
1Forthesakeofsimplicity,wewillnotemphasizethedifferencesbetweensearchpolicyπππMCTS(·|sss )in
t,0
multi-agentsettingsandπMCTS(·|s )insingle-agentsettingsapartfromthemathematicalexpressions.
t,0
3PublishedasaconferencepaperatICLR2024
3 CHALLENGES IN PLANNING-BASED MULTI-AGENT MODEL-BASED RL
Extendingsingle-agentPlanning-basedMBRLmethodstomulti-agentenvironmentsishighlynon-
trivial. On one hand, existing model designs for single-agent algorithms do not account for the
unique biases inherent to multi-agent environments, such as the nearly-independent property of
agents. This renders the direct employing a flattened model from single-agent MBRL algorithms
typicallyfallsshortinsupportingefficientlearningwithinreal-worldmulti-agentenvironments. On
the other hand, multi-agent environments possess state-action spaces significantly more intricate
thantheirsingle-agentcounterparts,therebyexponentiallyescalatingthesearchcomplexitywithin
multi-agentsettings. This, inturn, compelsustoexplorespecializedsearchalgorithmstailoredto
complexactionspaces. Moreover,theformofthemodel,intandemwithitsgeneralizationability,
collectivelyconstrainstheformandefficiencyofthesearchalgorithm, andviceversa, makingthe
designofthemodelandthedesignofthesearchalgorithmhighlyinterrelated.
Inthissection,weshalldiscussthechallengesencounteredinthecurrentdesignofplanning-based
MBRL algorithms, encompassing both model design and searching algorithm aspects. We will
elucidatehowourMAZeroalgorithmsystematicallyaddressesthesetwoissuesintheSection4.
3.1 MODELDESIGN
Withintheparadigmofcentralizedtrainingwithdecentralizedexecution(CTDE),onestraightfor-
wardapproachistolearnajointmodelthatenablesagentstodocentralizedplanningwithinthejoint
policy space. However, given the large size of the state-action space in multi-agent environments,
thedirectapplicationofaflattenedmodelfromsingle-agentMBRLalgorithmstendstorenderthe
learning process inefficient (Figure 6). This inefficiency stems from the inadequacy of a flattened
modelinaccommodatingtheuniquebiasesinherentinmulti-agentenvironments.
In numerous real-world scenarios featuring multi-agent environments, agents typically engage in
independent decision-making for the majority of instances, with collaboration reserved for excep-
tionalcircumstances. Furthermore,akinagentsoftenexhibithomogeneousbehavior(e.g.,focusing
fire in the SMAC environment). For the former, a series of algorithms, including IPPO (Yu et al.,
2022), have demonstrated the efficacy of independent learning in a multitude of MARL settings.
Regarding the latter, previous research in multi-agent model-free approaches has underscored the
hugesuccessofparameter-sharing(Rashidetal.,2020;Sunehagetal.,2017)withinMARL,which
can be regarded as an exploitation of agents’ homogenous biases. Hence, encoding these biases
effectivelywithinthedesignofthemodelbecomesonemainchallengeinmulti-agentMBRL.
3.2 PLANNINGINEXPONENTIALSIZEDACTIONSPACE
The action space in multi-agent environments
Bandit
growsexponentiallywiththenumberofagents, 100
rendering it immensely challenging for vanilla
MCTSalgorithmstoexpandallactions. While
80
SampledMuZero,inpriorsingle-agentMBRL
AWPO (Ours)
work,attemptedtoaddressscenarioswithlarge
BC
60
action spaces by utilizing action sampling, the
sizeoftheenvironmentsittackled(e.g.,Go,∼
0 200 400 600
300 actions) still exhibits substantial disparity OptimSteps
compared to typical multi-agent environments
(e.g.,SMAC-27m vs 30m,∼1042actions). Figure 1: Bandit Experiment We compare the
Behavior Cloning (BC) loss and our Advantage-
InpracticalapplicationslikeMARL,duetothe
WeightedPolicyOptimization(AWPO)lossona
significantly fewer samples taken compared to
banditwithactionspace|A| = 100andsampling
theoriginalactionspacesize,theunderestima-
time B = 2. It is evident that AWPO converges
tion issue of Sampled MCTS becomes more
much faster than BC, owing to the effective uti-
pronounced, making it less than ideal in terms
lizationofvalues.
ofsearchingefficiency.Thismotivatesustode-
signamoreoptimisticsearchprocess.
4
eulaVPublishedasaconferencepaperatICLR2024
Moreover,thebehaviorcloning(BC)lossthatSampledMCTSalgorithmsadoptdisregardsthevalue
information. This is because the target policy ω(·|s ) only encapsulates the relative magnitude
t,0
of sampled action values while disregarding the information of the absolute action values. The
issueisnotparticularlyseverewhendealingwithasmalleractionspace. However, inmulti-agent
environments, the disparity between the sampling time B and |A| becomes significant, making it
impossibletooverlooktherepercussionsofdisregardingvalueinformation(Figure1).
4 MAZERO ALGORITHM
4.1 MODELSTRUCTURE
TheMAZeromodelcomprises6keyfunctions: arepresentationfunctionh formappingthecur-
θ
rent observation history oi of agent i to an individual latent state si , a communication func-
≤t t,0
tion e which generates additional cooperative features ei for each agent i through the attention
θ t,k
mechanism, a dynamic function g tasked with deriving the subsequent local latent state si
θ t,k+1
basedontheagent’sindividualstatesi ,futureactionai andcommunicationfeatureei ,are-
t,k t+k t,k
wardpredictionfunctionforecastingthecooperativeteamrewardr fromtheglobalhiddenstate
t,k
sss = (s1 ,...,sN ) and joint actionaaa = (a1 ,...,aN ), a value prediction function V
t,k t,k t,k t+k t+k t+k θ
aimedatpredictingvaluev foreachglobalhiddenstatesss ,andapolicypredictionfunctionP
t,k t,k θ
which given an individual state si and generates the corresponding policy pi for agent i. The
t,k t,k
modelequationsareshowninEquation(4).
 Representation: si =h (oi )

C Do ym nam mu icn :ication: e s1
tt
i
t,, ,k0
k+,. 1.
=.,θ
e gN
t
θ,≤
k
(st
=
i
t,ke
,θ
a(
i
ts +1
t k,k
,, e.
i
t. ,k. ),sN t,k,a1 t+k,...,aN t+k)
(4)
 R
V
Poae
l
lw
u
ica
e
yr Pd PrP
re
edr de
i
icd cti tic iot oni no
:
:n: r
v
pt
t
i, ,k
k
=
=
=R
V
Pθθ
(
((
s
ss
1
t
i1 t ,, kk
,
), .. .. .. ,, ss
N
tN t ,, kk ),a1 t+k,...,aN t+k)
t,k θ t,k
Notably, the representation function h , the dynamic function g , and the policy prediction func-
θ θ
tion P all operate using local information, enabling distributed execution. Conversely, the other
θ
functions deal with value information and team cooperation, necessitating centralized training for
effectivelearning.
𝑣 !,# 𝑝 !$ ,#, …, 𝑝 !% ,# 𝑟 !,$ 𝑣 !,$ (𝑝 !$ ,$, …, 𝑝 !% ,$) 𝑠!$ ,#
Shared Individual
V P … … P RewardmodelR V P … … P Dynamic Networkg
𝒔 !,#= 𝑠 !$ ,#, …, 𝑠 !% ,# 𝒂 != 𝑎 !$,…,𝑎 !$ 𝒔 !,$= (𝑠 !$ ,$, …, 𝑠 !% ,$) 𝑒 !$ ,#,…,𝑒 !% ,# 𝑒!$ ,# (𝑠!$ ,#,𝑎!$ ,#)
Communication
h … … h Centralized h … … h Networke
Dynamic
Block
𝒐 != 𝑜 !$, … , 𝑜 !% 𝒐 !&$=(𝑜 !$ &$, …, 𝑜 !% &$) 𝑠$ ,…,𝑠% 𝑎$ ,…,𝑎%
!,# !,# !,# !,#
Figure 2: MAZero model structure Given the current observations oi for each agent, the model
t
separately maps them into local hidden states si using a shared representation network h. Value
t,0
prediction v is computed based on the global hidden state sss while policy priors pi are in-
t,0 t,0 t,0
dividually calculated for each agent using their corresponding local hidden states. Agents use the
communicationnetworketoaccessteaminformationei andgeneratenextlocalhiddenstatessi
t,0 t,1
viathesharedindividualdynamicnetworkg,subsequentlyderivingrewardr ,valuev andpoli-
t,1 t,1
cies pi . During the training stage, real future observationsooo can be obtained to generate the
t,1 t+1
targetforthenexthiddenstate,denotedassss .
t+1,0
5PublishedasaconferencepaperatICLR2024
4.2 OPTIMISTICSEARCHLAMBDA
Having discerned that MAZero has acquired a deterministic world model, we devise a Optimistic
SearchLambda(OS(λ))approachtobetterharnessthischaracteristicofthemodel.
Inpreviousendeavors,theselectionstageofMCTSemployedtwometrics(valuescoreandexplo-
rationbonus)togaugeourinterestinaparticularaction. Inthiscontext,thevaluescoreutilizedthe
meanestimateofvaluesobtainedfromallsimulationswithinthatnode’ssubtree. However,within
thedeterministicmodel,themeanestimateappearsexcessivelyconservative. Contemplatingasce-
nario where the tree degenerates into a multi-armed bandit, if pulling an arm consistently yields a
deterministicoutcome,therearisesnonecessity,akintotheUpperConfidenceBound(UCB)algo-
rithm,torepeatedlysamplethesamearmandcalculateitsaverage. Similarly,withinthetree-based
versionoftheUCT(UpperConfidenceTrees)algorithm,averagingvaluesacrossallsimulationsina
subtreemaybesubstitutedwithamoreoptimisticestimationwhentheenvironmentisdeterministic.
Hence, courtesy of the deterministic model, our focus narrows to managing model generalization
errorsratherthancontendingwitherrorsintroducedbyenvironmentalstochasticity. Builtuponthis
conceptualfoundation,wehavedevisedamethodologyforcalculatingthevaluescorethatplacesa
heightenedemphasisonoptimisticvalues.
Specifically,foreachnodes,wedefine:
(cid:40) (cid:12) (cid:41)
(cid:88) (cid:12)
U (s)= γkr +γdv(s′)(cid:12)s′ :dep(s′)=dep(s)+d (5)
d k (cid:12)
(cid:12)
k<d
Uρ(s)=Top(1−ρ)valuesinU (6)
d d
(cid:88) (cid:88) (cid:88)
Vρ(s)= λdx/ λd|Uρ(s)| (7)
λ d
d x∈Uρ(s) d
d
Aρ(s,a)=r(s,a)+γVρ(Dynamic(s,a))−v(s) (8)
λ λ
wheredep(u)denotesthedepthofnodeuintheMCTStree,ρ,λ∈[0,1]arehyperparameters,r,v,
andDynamicareallmodelpredictionscalculatedaccordingtoEquation(4).
Toofferabriefelucidationonthismatter,Equation(6)isthesetofoptimisticvaluesatdepthd,the
degreeofoptimismiscontrolledbythequantileparameterρ. Sincethemodelerrorsmagnifywith
increasingdepthinpractice,Equation(7)calculatesaweightedmeanofalloptimisticvaluesinthe
subtreeasthevalueestimationofs, wheretheweightofdifferentestimationsarediscountedbya
factorλoverthedepth.
Finally,weusetheoptimisticadvantage(Equation(8))toreplacethevaluescoreinEquation(3)for
optimisticsearch,thatis:
(cid:34) βˆ (cid:112)(cid:80) N(s,b) (cid:35)
a= argmax Aρ(s,a)+ P(s,a)· b ·c(s) (9)
λ β 1+N(s,a)
a∈T(s)⊂A
4.3 ADVANTAGE-WEIGHTEDPOLICYOPTIMIZATION
In order to utilize the value information calculated by OS(λ), we propose Advantage-Weighted
PolicyOptimization(AWPO),anovelpolicylossfunctionthatincorporatestheoptimisticadvantage
(Equation(8))intothebehaviorcloningloss.2
(cid:16) Aρ(s,a)(cid:17)
Intermsofoutcome,AWPOlossweightstheper-actionbehaviorcloninglossbyexp λ :
α
 
(cid:88)
(cid:18) Aρ(s,a)(cid:19)
l p(θ;π,ω,Aρ λ)=−E  ω(a|s)exp λ
α
logπ(a|s;θ) (10)
a∈T(s)
2Inthissection,theexpectationoperatoristakenovertherandomnessofOS(λ).Forthesakeofsimplicity,
wewillomitthisintheformula.
6PublishedasaconferencepaperatICLR2024
whereθ denotesparametersofthelearnedmodel, π(·|s;θ)isthenetworkpredictpolicytobeim-
proved,ω(·|s)isthesearchpolicysupportedinactionsubsetT(s),Aρ istheoptimisticadvantage
λ
derived from OS(λ) and α > 0 is a hyperparameter controlling the degree of optimism. Theoret-
ically, AWPO can be regarded as a cross-entropy loss between η∗ and π,3 where η∗ is the non-
parametricsolutionofthefollowingconstrainedoptimizationproblem:
maximize E [Aρ(s,a)]
a∼η(·|s) λ
η (11)
s.t.
KL(cid:0) η(·|s)∥πMCTS(·|s)(cid:1)
≤ϵ
Tobemorespecific,byLagrangianmethod,wehave:
(cid:18) Aρ(s,a)(cid:19) (cid:20) (cid:18) Aρ(s,a)(cid:19)(cid:21)
η∗(a|s)∝πMCTS(a|s)exp λ =E ω(a|s)exp λ (12)
α α
Thereby,minimizingtheKLdivergencebetweenη∗andπgivestheformofAWPOloss:
argminKL(η∗(·|s)∥π(·|s;θ))
θ
1 (cid:88)
(cid:18) Aρ(s,a)(cid:19)
=argmin− πMCTS(a|s)exp λ logπ(a|s;θ)−H(η∗(·|s))
Z(s) α
θ
a
=argminl (θ;π,ω,Aρ)
p λ
θ
whereZ(s)=(cid:80) πMCTS(a|s)exp(cid:16) Aρ λ(s,a)(cid:17) isanormalizingfactor,H(η∗)standsfortheentropy
a α
ofη∗,whichisaconstant.
Equation (11) shows that AWPO aims to optimize the value improvement of π in proximity to
πMCTS, which effectively combines the improved policy obtained from OS(λ) with the optimistic
value,therebycompensatingfortheshortcomingsofBClossandenhancingtheefficiencyofpolicy
optimization.
DetailsofthederivationcanbefoundinAppendixG.
4.4 MODELTRAINING
TheMAZeromodelisunrolledandtrainedinanend-to-endschemasimilartoMuZero.Specifically,
given a trajectory sequence of length K +1 for observationooo , joint actionsaaa , rewards
t:t+K t:t+K
u , value targets z , policy targets πππMCTS and optimistic advantages Aρ, the model is
t:t+K t:t+K t:t+K λ
unrolledforK stepsasisshowninFigure2andistrainedtominimizethefollowingloss:
K K
(cid:88) (cid:88)
L= (l +l +l )+ l (13)
r v s p
k=1 k=0
wherel = ∥r −u ∥andl = ∥v −z ∥arerewardandvaluelossessimilartoMuZero,
r t,k t+k v t,k t+k
l = ∥sss −sss ∥ is the consistency loss akin to EfficientZero and l is the AWPO policy loss
s t,k t+k,0 p
(Equation(10))undertherealmofmulti-agentjointactionspace:
l =−
(cid:88)
ω(aaa|sss
)exp(cid:18) Aρ λ(sss t+k,0,aaa)(cid:19)
logπππ(aaa|sss ;θ) (14)
p t+k,0 α t,k
aaa∈T(ssst+k,0)
whereOS(λ)isperformedunderthehiddenstatesss directlyrepresentedfromtheactualfuture
t+k,0
observationooo , deriving corresponding action subset T(sss ), search policy ω(aaa|sss ) and
t+k t+k,0 t+k,0
optimisticadvantageAρ(sss ,aaa). πππ(aaa|sss ;θ) = (cid:81)N P (ai|si )denotesthejointpolicytobe
λ t+k,0 t,k i=1 θ t,k
improvedatthekthstep’shiddenstatesss unrollingfromdynamicfunction.
t,k
3ItisequivalenttominimizingtheKLdivergenceofη∗andπ.
7PublishedasaconferencepaperatICLR2024
5 EXPERIMENTS
5.1 STARCRAFTMULTI-AGENTCHALLENGE
Baselines We compare MAZero with both model-based and model-free baseline methods
on StarCraft Multi-Agent Challenge (SMAC) environments. The model-based baseline is
MAMBA(Egorov&Shpilman,2022),arecentlyintroducedmulti-agentMARLalgorithmbasedon
DreamerV2(Hafneretal.,2020)knownforitsstate-of-the-artsampleefficiencyinvariousSMAC
tasks. Themodel-freeMARLmethodsincludesQMIX(Rashidetal.,2020),QPLEX(Wangetal.,
2020a),RODE(Wangetal.,2020b),CDS(Lietal.,2021),andMAPPO(Yuetal.,2022).
MAZero(Ours) MAMBA MAPPO QPLEX RODE CDS QMIX
2c_vs_64zg 5m_vs_6m 8m_vs_9m 10m_vs_11m
1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0 1 2 3 4 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
EnvSteps 1e5 EnvSteps 1e6 EnvSteps 1e6 EnvSteps 1e6
3m 2m_vs_1z so_many_baneling 2s_vs_1sc
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5 0.0 0.2 0.4 0.6 0.8 1.0
EnvSteps 1e4 EnvSteps 1e4 EnvSteps 1e4 EnvSteps 1e5
Figure3: ComparisonsagainstbaselinesinSMAC.YaxisdenotesthewinrateandXaxisdenotes
thenumberofstepstakenintheenvironment. Eachalgorithmisexecutedwith10randomseeds.
Figure 3 illustrates the results in the SMAC environments. Among the 23 available scenarios, we
havechosen8forpresentationinthispaper. Specifically, wehaveselectedfourrandomscenarios
categorizedasEasytasksand4Hardtasks.ItisevidentthatMAZerooutperformsallbaselinealgo-
rithmsacrosseightscenarioswithagivennumberofstepstakenintheenvironment. Notably,both
MAZeroandMAMBA,whicharecategorizedasmodel-basedmethods,exhibitmarkedlysuperior
sampleefficiencyineasytaskswhencomparedtomodel-freebaselines. Furthermore,MAZerodis-
plays a more stable training curve and smoother win rate during evaluation than MAMBA. In the
realmofhardtasks,MAZerosurpassesMAMBAintermsofoverallperformance,withanotewor-
thyemphasisonthe2c vs 64zgscenario. Inthisparticularscenario, MAZeroattainsahigherwin
rate with a significantly reduced sample size when contrasted with other methods. This scenario
involves a unique property, featuring only two agents and an expansive action space of up to 70
foreachagent,incontrasttootherscenarioswheretheagent’sactionspaceisgenerallyfewerthan
20. ThisdistinctivecharacteristicenhancestheroleofplanninginMAZerocomponents,similarto
MuZero’soutstandingperformanceinthedomainofGo.
As MAZero builds upon the MuZero framework, we perform end-to-end training directly using
planningresultsfromthereplaybuffer. Consequently,thisapproachcircumventsthetimeoverhead
for data augmentation, as seen in Dreamer-based methods. Figure 4 illustrates the superior per-
formanceofMAZerowithrespecttothetemporalcostinSMACenvironmentswhencomparedto
MAMBA.
5.2 ABLATION
Weperformseveralablationexperimentsonthetwoproposedtechniques: OS(λ)andAWPO.The
results with algorithms executed with three random seeds are reported in Figure 5. In particular,
weexaminewhetherdisablingOS(λ),AWPO,orbothofthem(i.e.,usingoriginalSampledMCTS
andBClossfunction)impairsfinalperformance. Significantly,bothtechniquesgreatlyimpactfinal
performanceandlearningefficiency.
8
etaRniW
etaRniW
etaRniW
etaRniW
etaRniW
etaRniW
etaRniW
etaRniWPublishedasaconferencepaperatICLR2024
MAZero(Ours) MAMBA
2m_vs_1z 3m so_many_baneling
1.0 1.0 1.0
0.5 0.5 0.5
0.0 0.0 0.0
0 2 4 6 8 0.0 2.5 5.0 7.5 10.0 12.5 0 5 10 15 20
Timecost(hours) Timecost(hours) Timecost(hours)
2s_vs_1sc 2c_vs_64zg 5m_vs_6m
1.0 0.6
0.4
0.5 0.5
0.2
0.0 0.0 0.0
0 5 10 15 20 0 5 10 15 20 25 30 0 5 10 15 20 25 30
Timecost(hours) Timecost(hours) Timecost(hours)
Figure4: ComparisonsagainstMBRLbaselinesinSMAC.They-axisdenotesthewinrate,andthe
X-axisdenotesthecumulativeruntimeofalgorithmsinthesameplatform.
MAZero No OS() No AWPO No OS() and AWPO
8m 2c_vs_64zg
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
EnvSteps 1e6 EnvSteps 1e6
Figure5: Ablationstudyonproposedapproachesforplanning.
InourablationoftheMAZeronetworkstructure(Figure6),wehavediscernedthesubstantialimpact
oftwocomponents,“communication”and“sharing”,onalgorithmicperformance. Instarkcontrast,
the flattened model employed in single-agent MBRL, due to its failure to encapsulate the unique
biasesofmulti-agentenvironments,canonlylearnvictoriousstrategiesinthemostrudimentaryof
scenarios,suchasthe2m vs 1zmap.
MAZero No communication No sharing Flattened
2m_vs_1z 2c_vs_64zg 5m_vs_6m 10m_vs_11m
1.0
0.8 0.8 0.8
0.8
0.6 0.6 0.6
0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
EnvSteps 1e5 EnvSteps 1e6 EnvSteps 1e6 EnvSteps 1e6
Figure6: Ablationstudyonnetworkstructure.
6 CONCLUSION
Inthispaper,weintroduceamodel-basedmulti-agentalgorithm,MAZero,whichutilizestheCTDE
frameworkandMCTSplanning.Thisapproachboastssuperiorsampleefficiencycomparedtostate-
of-the-artmodel-freemethodsandprovidescomparableorbetterperformancethanexistingmodel-
based methods in terms of both sample and computational efficiency. We also develop two novel
approaches,OS(λ)andAWPO,toimprovesearchefficiencyinvastactionspacesbasedonsampled
MCTS.Inthefuture,weaimtoaddressthisissuethroughreducingthedimensionalityoftheaction
spaceinsearch,suchasactionrepresentation.
9
etaRniW
etaRniW
etaRniW
etaRniW
etaRniW
etaRniW
etaRniW
etaRniW
etaRniW
etaRniW
etaRniW
etaRniWPublishedasaconferencepaperatICLR2024
7 REPRODUCIBILITY
Weensurethereproducibilityofourresearchbyprovidingcomprehensiveinformationandresources
that allow others to replicate our findings. All experimental settings in this paper are available in
AppendixB,includingdetailsonenvironmentalsetup, networkstructure, trainingprocedures, hy-
perparameters,andmore. Thesourcecodeutilizedinourexperimentsalongwithclearinstructions
on how to reproduce our results will be made available upon finalization of the camera-ready ver-
sion. Thebenchmarkemployedinthisinvestigationiseitherpubliclyaccessibleorcanbeobtained
by contacting the appropriate data providers. Additionally, detailed derivations for our theoretical
claims can be found in Appendix G. We are dedicated to addressing any concerns or inquiries re-
latedtoreproducibilityandareopentocollaboratingwithotherstofurthervalidateandverifyour
findings.
ACKNOWLEDGMENTS
ThisworkwassupportedbytheNationalScienceandTechnologyInnovation2030-MajorProject
(GrantNo. 2022ZD0208804).
REFERENCES
RishabhAgarwal,MaxSchwarzer,PabloSamuelCastro,AaronCCourville,andMarcBellemare.
Deepreinforcementlearningattheedgeofthestatisticalprecipice. Advancesinneuralinforma-
tionprocessingsystems,34:29304–29320,2021.
Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and
treesearch. Advancesinneuralinformationprocessingsystems,30,2017.
IoannisAntonoglou,JulianSchrittwieser,SherjilOzair,ThomasKHubert,andDavidSilver. Plan-
ninginstochasticenvironmentswithalearnedmodel. InInternationalConferenceonLearning
Representations,2021.
Kai Arulkumaran, Antoine Cully, and Julian Togelius. Alphastar: An evolutionary computation
perspective. InProceedingsofthegeneticandevolutionarycomputationconferencecompanion,
pp.314–315,2019.
NolanBard,JakobNFoerster,SarathChandar,NeilBurch,MarcLanctot,HFrancisSong,Emilio
Parisotto,VincentDumoulin,SubhodeepMoitra,EdwardHughes,etal. Thehanabichallenge: A
newfrontierforairesearch. ArtificialIntelligence,280:103216,2020.
EugenioBargiacchi,TimothyVerstraeten,andDiederikMRoijers. Cooperativeprioritizedsweep-
ing. InAAMAS,pp.160–168,2021.
Noam Brown, Anton Bakhtin, Adam Lerer, and Qucheng Gong. Combining deep reinforcement
learningandsearchforimperfect-informationgames.AdvancesinNeuralInformationProcessing
Systems,33:17057–17069,2020.
Dane Corneil, Wulfram Gerstner, and Johanni Brea. Efficient model-based deep reinforcement
learningwithvariationalstatetabulation. InInternationalConferenceonMachineLearning,pp.
1049–1058.PMLR,2018.
IvoDanihelka,ArthurGuez,JulianSchrittwieser,andDavidSilver.Policyimprovementbyplanning
withgumbel. InInternationalConferenceonLearningRepresentations,2021.
VladimirEgorovandAlekseiShpilman. Scalablemulti-agentmodel-basedreinforcementlearning.
arXivpreprintarXiv:2205.15023,2022.
Sven Gronauer and Klaus Diepold. Multi-agent deep reinforcement learning: a survey. Artificial
IntelligenceReview,pp.1–49,2022.
DavidHaandJu¨rgenSchmidhuber. Worldmodels. arXivpreprintarXiv:1803.10122,2018.
DanijarHafner,TimothyLillicrap,JimmyBa,andMohammadNorouzi.Dreamtocontrol:Learning
behaviorsbylatentimagination. arXivpreprintarXiv:1912.01603,2019.
10PublishedasaconferencepaperatICLR2024
DanijarHafner,TimothyLillicrap,MohammadNorouzi,andJimmyBa. Masteringatariwithdis-
creteworldmodels. arXivpreprintarXiv:2010.02193,2020.
Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains
throughworldmodels. arXivpreprintarXiv:2301.04104,2023.
Nicklas Hansen, Xiaolong Wang, and Hao Su. Temporal difference learning for model predictive
control. arXivpreprintarXiv:2203.04955,2022.
Jinke He, Thomas M Moerland, and Frans A Oliehoek. What model does muzero learn? arXiv
preprintarXiv:2306.00840,2023.
PabloHernandez-Leal,BilalKartal,andMatthewETaylor. Averycondensedsurveyandcritique
ofmultiagentdeepreinforcementlearning. InProceedingsofthe19thinternationalconference
onautonomousagentsandmultiagentsystems,pp.2146–2148,2020.
MatteoHessel,IvoDanihelka,FabioViola,ArthurGuez,SimonSchmitt,LaurentSifre,Theophane
Weber, David Silver, and Hado Van Hasselt. Muesli: Combining improvements in policy opti-
mization. InInternationalconferenceonmachinelearning,pp.4214–4226.PMLR,2021.
LukasHewing,KimPWabersich,MarcelMenner,andMelanieNZeilinger. Learning-basedmodel
predictive control: Toward safe learning in control. Annual Review of Control, Robotics, and
AutonomousSystems,3:269–296,2020.
Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Mohammadamin Barekatain, Simon
Schmitt, and David Silver. Learning and planning in complex action spaces. In International
ConferenceonMachineLearning,pp.4476–4486.PMLR,2021.
MichaelJanner, JustinFu, MarvinZhang, andSergeyLevine. Whentotrustyourmodel: Model-
basedpolicyoptimization. Advancesinneuralinformationprocessingsystems,32,2019.
Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in
partiallyobservablestochasticdomains. Artificialintelligence,101(1-2):99–134,1998.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980,2014.
LeventeKocsisandCsabaSzepesva´ri. Banditbasedmonte-carloplanning. InEuropeanconference
onmachinelearning,pp.282–293.Springer,2006.
KarolKurach,AntonRaichuk,PiotrStanczyk,MichalZajac,OlivierBachem,LasseEspeholt,Car-
los Riquelme, Damien Vincent, Marcin Michalski, Olivier Bousquet, et al. Google research
football: Anovelreinforcementlearningenvironment. InProceedingsoftheAAAIconferenceon
artificialintelligence,volume34,pp.4501–4510,2020.
Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble
trust-regionpolicyoptimization. arXivpreprintarXiv:1802.10592,2018.
ChenghaoLi,TonghanWang,ChengjieWu,QianchuanZhao,JunYang,andChongjieZhang. Cel-
ebratingdiversityinsharedmulti-agentreinforcementlearning. AdvancesinNeuralInformation
ProcessingSystems,34:3991–4002,2021.
Yong Liu, Weixun Wang, Yujing Hu, Jianye Hao, Xingguo Chen, and Yang Gao. Multi-agent
gameabstractionviagraphattentionneuralnetwork. InProceedingsoftheAAAIConferenceon
ArtificialIntelligence,volume34,pp.7211–7218,2020.
PinxinLong, TingxiangFan, XinyiLiao, WenxiLiu, HaoZhang, andJiaPan. Towardsoptimally
decentralized multi-robot collision avoidance via deep reinforcement learning. In 2018 IEEE
internationalconferenceonroboticsandautomation(ICRA),pp.6252–6259.IEEE,2018.
Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-
agentactor-criticformixedcooperative-competitiveenvironments. Advancesinneuralinforma-
tionprocessingsystems,30,2017.
11PublishedasaconferencepaperatICLR2024
AnujMahajan,MikayelSamvelyan,LeiMao,ViktorMakoviychuk,AnimeshGarg,JeanKossaifi,
Shimon Whiteson, Yuke Zhu, and Animashree Anandkumar. Tesseract: Tensorised actors for
multi-agentreinforcementlearning.InInternationalConferenceonMachineLearning,pp.7301–
7312.PMLR,2021.
Yixuan Mei, Jiaxuan Gao, Weirui Ye, Shaohuai Liu, Yang Gao, and Yi Wu. Speedyzero: Mas-
tering atari with limited data and time. In The Eleventh International Conference on Learning
Representations,2022.
AnushaNagabandi,GregoryKahn,RonaldSFearing,andSergeyLevine.Neuralnetworkdynamics
formodel-baseddeepreinforcementlearningwithmodel-freefine-tuning. In2018IEEEinterna-
tionalconferenceonroboticsandautomation(ICRA),pp.7559–7566.IEEE,2018.
AshvinNair,AbhishekGupta,MurtazaDalal,andSergeyLevine. Awac: Acceleratingonlinerein-
forcementlearningwithofflinedatasets,2021.
Thanh Thi Nguyen, Ngoc Duy Nguyen, and Saeid Nahavandi. Deep reinforcement learning for
multiagent systems: A review of challenges, solutions, and applications. IEEE transactions on
cybernetics,50(9):3826–3839,2020.
Frans A Oliehoek, Christopher Amato, et al. A concise introduction to decentralized POMDPs,
volume1. Springer,2016.
Bei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr,
WendelinBo¨hmer,andShimonWhiteson. Facmac: Factoredmulti-agentcentralisedpolicygra-
dients. AdvancesinNeuralInformationProcessingSystems,34:12208–12221,2021.
TabishRashid,MikayelSamvelyan,ChristianSchroederDeWitt,GregoryFarquhar,JakobFoerster,
andShimonWhiteson.Monotonicvaluefunctionfactorisationfordeepmulti-agentreinforcement
learning. TheJournalofMachineLearningResearch,21(1):7234–7284,2020.
Heechang Ryu, Hayong Shin, and Jinkyoo Park. Multi-agent actor-critic with hierarchical graph
attentionnetwork. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume34,
pp.7236–7243,2020.
TomSchaul,JohnQuan,IoannisAntonoglou,andDavidSilver.Prioritizedexperiencereplay.arXiv
preprintarXiv:1511.05952,2015.
JulianSchrittwieser, IoannisAntonoglou, ThomasHubert, KarenSimonyan, LaurentSifre, Simon
Schmitt,ArthurGuez,EdwardLockhart,DemisHassabis,ThoreGraepel,etal. Masteringatari,
go,chessandshogibyplanningwithalearnedmodel. Nature,588(7839):604–609,2020.
DavidSilver,AjaHuang,ChrisJMaddison,ArthurGuez,LaurentSifre,GeorgeVanDenDriessche,
JulianSchrittwieser,IoannisAntonoglou,VedaPanneershelvam,MarcLanctot,etal. Mastering
thegameofgowithdeepneuralnetworksandtreesearch. nature,529(7587):484–489,2016.
DavidSilver,JulianSchrittwieser,KarenSimonyan,IoannisAntonoglou,AjaHuang,ArthurGuez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
withouthumanknowledge. nature,550(7676):354–359,2017.
DavidSilver,ThomasHubert,JulianSchrittwieser,IoannisAntonoglou,MatthewLai,ArthurGuez,
MarcLanctot,LaurentSifre,DharshanKumaran,ThoreGraepel,etal. Ageneralreinforcement
learningalgorithmthatmasterschess,shogi,andgothroughself-play. Science,362(6419):1140–
1144,2018.
KyunghwanSon,DaewooKim,WanJuKang,DavidEarlHostallero,andYungYi.Qtran:Learning
tofactorizewithtransformationforcooperativemulti-agentreinforcementlearning. InInterna-
tionalconferenceonmachinelearning,pp.5887–5896.PMLR,2019.
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based
rl in contextual decision processes: Pac bounds and exponential improvements over model-free
approaches. InConferenceonlearningtheory,pp.2898–2933.PMLR,2019.
12PublishedasaconferencepaperatICLR2024
PeterSunehag,GuyLever,AudrunasGruslys,WojciechMarianCzarnecki,ViniciusZambaldi,Max
Jaderberg,MarcLanctot,NicolasSonnerat,JoelZLeibo,KarlTuyls,etal. Value-decomposition
networksforcooperativemulti-agentlearning. arXivpreprintarXiv:1706.05296,2017.
RichardSSuttonandAndrewGBarto. Reinforcementlearning: Anintroduction. MITpress,2018.
Gerald Tesauro. Td-gammon, a self-teaching backgammon program, achieves master-level play.
Neuralcomputation,6(2):215–219,1994.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-
tionprocessingsystems,30,2017.
Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling
multi-agentq-learning. arXivpreprintarXiv:2008.01062,2020a.
TingwuWangandJimmyBa.Exploringmodel-basedplanningwithpolicynetworks.arXivpreprint
arXiv:1906.08649,2019.
Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi
Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-based reinforce-
mentlearning. arXivpreprintarXiv:1907.02057,2019.
Tonghan Wang, Tarun Gupta, Anuj Mahajan, Bei Peng, Shimon Whiteson, and Chongjie Zhang.
Rode: Learningrolestodecomposemulti-agenttasks. arXivpreprintarXiv:2010.01523,2020b.
Danie¨l Willemsen, Mario Coppola, and Guido CHE de Croon. Mambpo: Sample-efficient multi-
robotreinforcementlearningusinglearnedworldmodels. In2021IEEE/RSJInternationalCon-
ferenceonIntelligentRobotsandSystems(IROS),pp.5635–5640.IEEE,2021.
Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao
Tang. Qatten: A general framework for cooperative multiagent reinforcement learning. arXiv
preprintarXiv:2002.03939,2020.
DehengYe, ZhaoLiu, MingfeiSun, BeiShi, PeilinZhao, HaoWu, HongshengYu, ShaojieYang,
XipengWu,QingweiGuo,etal. Masteringcomplexcontrolinmobagameswithdeepreinforce-
mentlearning. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume34,pp.
6672–6679,2020.
JianingYe,ChenghaoLi,JianhaoWang,andChongjieZhang. Towardsglobaloptimalityincoop-
erativemarlwiththetransformationanddistillationframework,2023.
WeiruiYe,ShaohuaiLiu,ThanardKurutach,PieterAbbeel,andYangGao. Masteringatarigames
withlimiteddata. AdvancesinNeuralInformationProcessingSystems,34:25476–25488,2021.
ChaoYu,AkashVelu,EugeneVinitsky,JiaxuanGao,YuWang,AlexandreBayen,andYiWu. The
surprisingeffectivenessofppoincooperativemulti-agentgames.AdvancesinNeuralInformation
ProcessingSystems,35:24611–24624,2022.
KaiqingZhang,ShamKakade,TamerBasar,andLinYang. Model-basedmulti-agentrlinzero-sum
markovgameswithnear-optimalsamplecomplexity.AdvancesinNeuralInformationProcessing
Systems,33:1166–1178,2020.
MingZhou,JunLuo,JulianVillella,YaodongYang,DavidRusu,JiayuMiao,WeinanZhang,Mont-
gomery Alban, Iman Fadakar, Zheng Chen, et al. Smarts: Scalable multi-agent reinforcement
learningtrainingschoolforautonomousdriving. arXivpreprintarXiv:2010.09776,2020.
13PublishedasaconferencepaperatICLR2024
A RELATED WORKS
Dec-POMDP Inthiswork,wefocusonthefullycooperativemulti-agentsystemsthatcanbefor-
malizedasDecentralizedPartiallyObservableMarkovDecisionProcess(Dec-POMDP)(Oliehoek
et al., 2016), which are defined by (N,S,A1:N,T,U,Ω1:N,O1:N,γ), where N is the number of
agents, S is the global state space, T a global transition function, U a shared reward function
and Ai,Ωi,Oi are the action space, observation space and observing function for agent i. Given
state s at timestep t, agent i can only acquire local observation oi = Oi(s ) and choose action
t t t
ai ∈ Ai according to its policy πi based on local observation history oi . The environment then
t ≤t
shiftstothenextstates ∼ T(·|s ,a )andreturnsascalarrewardu = U(s ,a ). Theobjec-
t+1 t t t t t
tive for all agents is to learn a joint policyπππ that maximizes the expectation of discounted return
J(πππ)=E (cid:2)(cid:80)∞ γtu |ai ∼πi(·|oi ),i=1,...,N(cid:3) .
πππ t=0 t t ≤t
Combiningreinforcementlearningandplanningalgorithms Theintegrationofreinforcement
learningandplanningwithinacommonparadigmhasyieldedsuperhumanperformanceindiverse
domains(Tesauro,1994;Silveretal.,2017;Anthonyetal.,2017;Silveretal.,2018;Schrittwieser
et al., 2020; Brown et al., 2020). In this approach, RL acquires knowledge through the learning
ofvalueandpolicynetworks, whichinturnguidetheplanningprocess. Simultaneously, planning
generatesexperttargetsthatfacilitateRLtraining. Forexample,MuZero-basedalgorithms(Schrit-
twieseretal.,2020;Hubertetal.,2021;Antonoglouetal.,2021;Yeetal.,2021;Meietal.,2022)
combineMonte-CarloTreeSearch(MCTS),TD-MPC(Hansenetal.,2022)integratesModelPre-
dictive Control (MPC), and ReBeL (Brown et al., 2020) incorporate a fusion of Counterfactual
Regret Minimization (CFR). Nevertheless, the prevailing approach among these algorithms typi-
callyinvolvestreatingtheplanningresultsastargetsandsubsequentlyemployingbehaviorcloning
toenabletheRLnetworktoemulatethesetargets. Muesli(Hesseletal.,2021)firstcombinesregu-
larizedpolicyoptimizationwithmodellearningasanauxiliarylosswithintheMuZeroframework.
However, itfocusessolelyonthepolicygradientlosswhileneglectingthepotentialadvantagesof
planning. Consequently,itattainsatmostaperformancelevelequivalenttothatofMuZero. Tothe
bestofourknowledge,wearethefirsttocombinedpolicygradientandMCTSplanningtoaccurate
policylearninginmodel-basedreinforcementlearning.
Variants of MuZero-based algorithms While the original MuZero algorithm (Schrittwieser
etal.,2020)hasachievedsuperhumanperformancelevelsinGoandAtari,itisimportanttonotethat
itharborsseverallimitations. Toaddressandovercometheselimitations,subsequentadvancements
havebeenmadeinthefield. EfficientZero(Yeetal.,2021),forinstance,introducesself-supervised
consistencyloss,valueprefixprediction,andoff-policycorrectionmechanismstoexpediteandsta-
bilizethetrainingprocess.SampledMuZero(Hubertetal.,2021)extendsitscapabilitiestocomplex
action spaces through the incorporation of sample-based policy iteration. Gumbel MuZero (Dani-
helka et al., 2021) leverages the Gumbel-Top-k trick and modifies the planning process using Se-
quential Halving, thereby reducing the demands of MCTS simulations. SpeedyZero (Mei et al.,
2022)integratesadistributedRLframeworktofacilitateparalleltraining, furtherenhancingtrain-
ing efficiency. He et al. (2023) demonstrate that the model acquired by MuZero typically lacks
accuracy when employed for policy evaluation, rendering it ineffective in generalizing to assess
previouslyunseenpolicies.
Multi-agentreinforcementlearning ThetypicalapproachforMARLincooperativesettingsin-
volvescentralizedtraininganddecentralizedexecution(CTDE).Duringthetrainingphase,thisap-
proachleveragesglobalorcommunicationinformation,whileduringtheexecutionphase,itrestricts
itselftotheobservationinformationrelevanttothecurrentagent. Thisparadigmencompassesboth
value-based(Sunehagetal.,2017;Rashidetal.,2020;Sonetal.,2019;Yangetal.,2020;Wangetal.,
2020a)andpolicy-based(Loweetal.,2017;Liuetal.,2020;Pengetal.,2021;Ryuetal.,2020;Ye
etal.,2023)MARLmethodswithinthecontextofmodel-freescenarios. Model-basedMARLalgo-
rithmsremainsrelativelyunderexploredwithonlyafewmethodsasfollows:MAMBPO(Willemsen
etal.,2021)pioneersthefusionoftheCTDEframeworkwithDyan-stylemodel-basedtechniques,
emphasizing the utilization of generated data closely resembling real-world data. CPS (Bargiac-
chi et al., 2021) introduces dynamic and reward models to determine data priorities based on the
MAMBPO approach. Tesseract (Mahajan et al., 2021) dissects states and actions into low-rank
tensorsandevaluatestheQfunctionusingDynamicProgramming(DP)withinanapproximateen-
14PublishedasaconferencepaperatICLR2024
vironmentmodel. MAMBA(Egorov&Shpilman,2022)incorporatesthewordmodelproposedin
DreamerV2 (Hafner et al., 2020) and introduces an attention mechanism to facilitate communica-
tion.Thisintegrationleadstonoteworthyperformanceimprovementsandasubstantialenhancement
insampleefficiencywhencomparedtopreviousmodel-freemethods. Tothebestofourknowledge,
wearethefirsttoexpandtheMuZeroframeworkandincorporateMCTSplanningintothecontext
ofmodel-basedMARLsettings.
B IMPLEMENTATION DETAILS
B.1 NETWORKSTRUCTURE
For the SMAC scenarios where the input observations are 1 dimensional vectors (as opposed to 2
dimensionalimagesforboardgamesorAtariusedbyMuZero),weuseavariationoftheMuZero
model architecture in which all convolutions are replaced by fully connected layers. The model
consistsof6modules: representationfunction,communicationfunction,dynamicfunction,reward
prediction function, value prediction function, and policy prediction function, which are all repre-
sented as neural networks. All the modules except the communication block are implemented as
Multi-LayerPerception(MLP)networks,whereeachlinerlayerinMLPisfollowedbyaRectified
Linear Unit (ReLU) activation and a Layer Normalisation (LN) layer. Specifically, we use a hid-
den state size of 128 for all SMAC scenarios, and the hidden layers for each MLP module are as
follows: RepresentationNetwork=[128,128],DynamicNetwork=[128,128],RewardNetwork=
[32],ValueNetwork=[32]andPolicyNetwork=[32]. WeuseTransformerarchitecture(Vaswani
et al., 2017) with three stacked layers for encoding state-action pairs. These encodings are then
usedbytheagentstoprocesslocaldynamicandmakepredictions. Weuseadropouttechniquewith
probabilityp=0.1topreventthemodelfromover-fittingandusepositionalencodingtodistinguish
agentsinhomogeneoussettings.
Fortherepresentationnetwork,westackthelastfourlocalobservationstogetherasinputforeach
agent to deal with partial observability. Additionally, the concatenated observations are processed
byanextraLNtonormalizeobservedfeaturesbeforerepresentation.
Thedynamicfunctionfirstconcatenatesthecurrentlocalstate,theindividualaction,andthecom-
municationencodingbasedoncurrentstate-actionpairsasinputfeatures. Toalleviatetheproblem
thatgradientstendtozeroduringthecontinuousunrollofthemodel,thedynamicfunctionemploys
aresidualconnectionbetweenthenexthiddenstateandthecurrenthiddenstate.
Forvalueandrewardprediction,wefollowinscalingtargetsusinganinvertibletransformh(x) =
√
sign(x) 1+x−1+0.001·xandusethecategoricalrepresentationintroducedinMuZero(Schrit-
twieseretal.,2020). Weusetenbinsforboththevalueandtherewardpredictions,withthepredic-
tionsbeingabletorepresentvaluesbetween[−5,5]. Weusedn−stepbootstrappingwithn=5and
adiscountof0.99.
B.2 TRAININGDETAILS
MAZero employs a similar pipeline to EfficientZero (Ye et al., 2021) while asynchronizing the
parallelstagesofdatacollection(Self-playworkers), reanalysis(Reanalyzeworkers), andtraining
(MainThread)tomaintainthereproducibilityofthesamerandomseed.
We use the Adam optimizer (Kingma & Ba, 2014) for training, with a batch size of 256 and a
constant learning rate of 10−4. Samples are drawn from the replay buffer according to prioritized
replay(Schauletal.,2015)usingthesamepriorityandhyperparametersasinMuZero.
In practice, we re-execute OS(λ) using a delayed target model θˆin the reanalysis stage to reduce
off-policyerror. Consequently,theAWPOlossfunctionactuallyusedisthefollowingequation:
(cid:32) (cid:33)
l =−
(cid:88)
ωˆ(aaaˆ|ssˆs )exp
Aˆρ λ(ssˆs t+k,0,aaaˆ)
logπππ(aaaˆ|sss ;θ)
p t+k,0 α t,k
aaaˆ∈Tˆ(ssˆst+k,0)
Forotherdetails,weprovidehyper-parametersinTable1.
15PublishedasaconferencepaperatICLR2024
Parameter Setting
Observationsstacked 4
Discountfactor 0.99
Minibatchsize 256
Optimizer Adam
Optimizer: learningrate 10−4
Optimizer: RMSpropepsilon 10−5
Optimizer: weightdecay 0
Maxgradientnorm 5
Priorityexponent(c ) 0.6
α
Prioritycorrection(c ) 0.4→1
β
Evaluationepisodes 32
Minreplaysizeforsampling 300
Targetnetworkupdatinginterval 200
Unrollsteps 5
TDsteps(n) 5
NumberofMCTSsampledactions(K) 10
NumberofMCTSsimulations(N) 100
QuantileinMCTSvalueestimation(ρ) 0.75
DecaylambdainMCTSvalueestimation(λ) 0.8
ExponentialfactorinWeighted-Advantage(α) 3
Table1: Hyper-parametersforMAZeroinSMACenvironments
B.3 DETAILSOFBASELINEALGORITHMSIMPLEMENTATION
MAMBA (Egorov & Shpilman, 2022) is executed based on the open-source implementation:
https://github.com/jbr-ai-labs/mambawiththehyper-parametersinTable2.
Parameter Setting
Batchsize 256
GAEλ 0.95
Entropycoefficient 0.001
Entropyannealing 0.99998
Numberofupdates 4
Epochsperupdate 5
Updateclippingparameter 0.2
ActorLearningrate 5×10−4
CriticLearningrate 5×10−4
Discountfactor 0.99
ModelLearningrate 2×10−4
Numberofepochs 60
Numberofsampledrollouts 40
Sequencelength 20
RollouthorizonH 15
Buffersize 2.5×105
Numberofcategoricals 32
Numberofclasses 32
KLbalancingentropyweight 0.2
KLbalancingcrossentropyweight 0.8
Gradientclipping 100
Trajectoriesbetweenupdates 1
Hiddensize 256
Table2: Hyper-parametersforMAMBAinSMACenvironments
16PublishedasaconferencepaperatICLR2024
QMIX (Rashid et al., 2020) is executed based on the open-source implementation: https://
github.com/oxwhirl/pymarlwiththehyper-parametersinTable3.
Parameter Setting
Batchsize 32
Buffersize 5000
Discountfactor 0.99
ActorLearningrate 5×10−4
CriticLearningrate 5×10−4
Optimizer RMSProp
RMSPropα 0.99
RMSPropϵ 10−5
Gradientclipping 10
ϵ-greedy 1.0→0.05
ϵannealingtime 50000
Table3: Hyper-parametersforQMIXinSMACenvironments
QPLEX (Wang et al., 2020a) is executed based on the open-source implementation: https://
github.com/wjh720/QPLEXwiththehyper-parametersinTable4.
Parameter Setting
Batchsize 32
Buffersize 5000
Discountfactor 0.99
ActorLearningrate 5×10−4
CriticLearningrate 5×10−4
Optimizer RMSProp
RMSPropα 0.99
RMSPropϵ 10−5
Gradientclipping 10
ϵ-greedy 1.0→0.05
ϵannealingtime 50000
Table4: Hyper-parametersforQPLEXinSMACenvironments
RODE (Wang et al., 2020b) is executed based on the open-source implementation: https://
github.com/TonghanWang/RODEwiththehyper-parametersinTable5.
Parameter Setting
Batchsize 32
Buffersize 5000
Discountfactor 0.99
ActorLearningrate 5×10−4
CriticLearningrate 5×10−4
Optimizer RMSProp
RMSPropα 0.99
RMSPropϵ 10−5
Gradientclipping 10
ϵ-greedy 1.0→0.05
ϵannealingtime 50K ∼500K
numberofclusters 2∼5
Table5: Hyper-parametersforRODEinSMACenvironments
17PublishedasaconferencepaperatICLR2024
CDS(Lietal.,2021)isexecutedbasedontheopen-sourceimplementation: https://github.
com/lich14/CDSwiththehyper-parametersinTable6.
Parameter Setting
Batchsize 32
Buffersize 5000
Discountfactor 0.99
ActorLearningrate 5×10−4
CriticLearningrate 5×10−4
Optimizer RMSProp
RMSPropα 0.99
RMSPropϵ 10−5
Gradientclipping 10
ϵ-greedy 1.0→0.05
β 0.05
β 0.5
1
β 0.5
2
λ 0.1
attentionregulationcoefficient 10−3
Table6: Hyper-parametersforCDSinSMACenvironments
MAPPO (Yu et al., 2022) is executed based on the open-source implementation: https://
github.com/marlbenchmark/on-policywiththehyper-parametersinTable7.
Parameter Setting
Recurrentdatachunklength 10
Gradientclipping 10
GAEλ 0.95
Discountfactor 0.99
Valueloss huberloss
Huberdelta 10.0
Batchsize numenvs×bufferlength×numagents
Minibatchsize batchsize/mini-batch
Optimizer Adam
Optimizer: learningrate 5×10−4
Optimizer: RMSpropepsilon 10−5
Optimizer: weightdecay 0
Table7: Hyper-parametersforMAPPOinSMACenvironments
B.4 DETAILSOFTHEBANDITEXPERIMENT
ThebanditexperimentshowedinFigure1comparesthebehaviorcloninglossandtheAWPOloss
ona100-armedbandit,withactionvalues0,··· ,99andsamplingtimeB =2. ThepolicyπBCand
πAWPOareparameterizedbySoftmaxpolicy,whichmeans
exp(θBC)
πBC(a;θBC)= a
(cid:80) exp(θBC)
b b
exp(θAWPO)
πAWPO(a;θAWPO)= a
(cid:80) exp(θAWPO)
b b
whereθBC,θAWPO ∈R100arerandomlyinitializedandareidenticalinthebeginning.
18PublishedasaconferencepaperatICLR2024
To exclude the influence of stochasticity in the search algorithm and facilitate a more precise and
faircomparisonofthedifferencesinlossfunctions,wemadethreetargetedadjustments.
1. Let the subset of sampled action be T, we denote the target policy ω(a) = I(a =
argmax value(b)).
b∈T
2. WecalculatetheexpectationofthelossovertherandomnessofallpossibleTs.
3. WenormalizetheadvantageinAWPOinto0-mean-1-stdandchooseα=1.
Formally,wehave
(cid:88)
lBC(θ)=− logπ (a)t (a)
θ θ
(cid:88)
lBC(θ)=− logπ (a)t (a)exp(adv(a))
θ θ
(cid:16) (cid:80) (cid:17)K (cid:0)(cid:80) (cid:1)K
wheret(a) = π (b) − π (b) istheexpectationofω(a),theoverlinestands
b≥a θ b>a θ
(cid:113)
forstop-gradient,adv(a)=(cid:0) a− 99(cid:1) 3 isthenormalizedadvantage.
2 2525
C STANDARDISED PERFORMANCE EVALUATION PROTOCOL
We report our experiments based on the standardised performance evaluation protocol (Agarwal
etal.,2021).
Figure 7: Aggregate metrics on the SMAC benchmark with 95% stratified bootstrap confidence
intervals. Higher median, interquartile mean (IQM), and mean, but lower optimality gap indicate
betterperformance.
Figure8: Probabilitiesofimprovement,i.e. howlikelyitisforMAZerotooutperformbaselineson
theSMACbenchmark.
D MORE ABLATIONS
Intheexperimentsection,welistsomeablationstudiestoprovetheeffectivenessofeachcomponent
inMAZero. Inthissection,wewilldisplaymoreresultsfortheablationstudyabouthyperparame-
ters.
19PublishedasaconferencepaperatICLR2024
First, we perform an ablation study on the training stage optimizers, contrasting SGD and Adam.
The SGD optimizer is prevalent in most MuZero-based algorithms (Schrittwieser et al., 2020; Ye
et al., 2021), while Adam is frequently used in MARL environments (Rashid et al., 2020; Wang
etal.,2020a;Yuetal.,2022). Figure9indicatesthatAdamdemonstratessuperiorperformanceand
moreconsistentstabilitycomparedtoSGD.
Adam SGD
8m 5m_vs_6m
0.30
0.8 0.25
0.20
0.6
0.15
0.4
0.10
0.2
0.05
0.0 0.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0
EnvSteps 1e5 EnvSteps 1e5
Figure9: Ablationonoptimizer
K=10,N=100 K=5,N=50
8m 5m_vs_6m
1.0
0.8
0.8
0.6
0.6
0.4
0.4
0.2 0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
EnvSteps 1e6 EnvSteps 1e6
Figure10: Ablationonsampledscale
=0.75 =0.65 =0.85 =0.95 =0.8 =0.7 =0.9 =1.0
8m 8m
1.0
0.8
0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0 1 2 3 4 5 6 7 8 0.0 0.2 0.4 0.6 0.8 1.0
EnvSteps 1e5 EnvSteps 1e6
(a)Ablationonρ (b)Ablationonλ
Figure11: AblationforHyper-parametersofOS(λ)
In addition, we perform an ablation study on the sampled scale (the action sampling times K and
simulation numbers N in MCTS), which has been shown to be essential for final performance in
SampledMuZero(Hubertetal.,2021). Giventhelimitationsoftimeandcomputationalresources,
we only compare two cases: K = 5,N = 50 and K = 10,N = 100 for easy map 8m and hard
20
etaRniW
etaRniW
etaRniW
etaRniW
etaRniW
etaRniWPublishedasaconferencepaperatICLR2024
map5m vs 6m. Figure10revealsthatourmethodyieldsbetterperformancewithalargerscaleof
samples.
We also test the impact of ρ and λ in our Optimistic Search Lambda (OS(λ)) algorithm on map
8m. Wetestρ = 0.65,0.75,0.85,0.95byfixingλ = 0.8, andtestλ = 0.7,0.8,0.9,1.0byfixing
ρ = 0.75 for MAZero. Figure 11 shows that the optimistic approach stably improves the sample
efficiency,andtheλtermisusefulwhendealingwiththemodelerror.
WeperformanablationstudyaboutMCTSplanningintheevaluationstage.TheMAZeroalgorithm
isdesignedundertheCTDEframework,whichmeanstheglobalrewardallowstheagentstolearn
andoptimizetheirpoliciescollectivelyduringcentralizedtraining. Thepredictedglobalrewardis
usedinMCTSplanningtosearchforabetterpolicybasedonthenetworkprior. Table8showsthat
agents maintain comparable performance without MCTS planning during evaluation, i.e., directly
using the final model and local observations without global reward, communication and MCTS
planning.
Map Envsteps wMCTS w/oMCTS performanceratio
3m 50k 0.985±0.015 0.936±0.107 95.0±10.8%
2m vs 1z 50k 1.0±0.0 1.0±0.0 100±0.0%
so many baneling 50k 0.959±0.023 0.938±0.045 97.8±4.7%
2s vs 1sc 100k 0.948±0.072 0.623±0.185 65.7±19.5%
2c vs 64zg 400k 0.893±0.114 0.768±0.182 86.0±20.4%
5m vs 6m 1M 0.875±0.031 0.821±0.165 93.8±18.9%
8m vs 9m 1M 0.906±0.092 0.855±0.127 94.4±14.0%
10m vs 11m 1M 0.922±0.064 0.863±0.023 93.6±2.5%
averageperformance 100% 90.1±11.3%
Table8: AblationforusingMCTSduringevaluationinSMACenvironments
E EXPERIMENTS ON SINGLE-AGENT ENVIRONMENTS
WehaveconsidereddemonstratingtheeffectivenessofOS(λ)andAWPOinsingle-agentdecision
problemswithlargeactionspaces. Thismighthelpestablishthegeneralapplicabilityofthesetech-
niquesbeyondthemulti-agentSMACbenchmark.
We choose the classical LunarLander environment as the single-agent benchmark, but discretize
the 2-dimensional continuous action space into 400 discrete actions. Additionally, we select the
Walker2D scenario in MuJoCo environment and discretize each dimension of continuous action
spaceinto7discreteactions,i.e.,67 ≈280,000legalactions. Tables9and10illustratestheresults
wherebothtechniquesgreatlyimpactlearningefficiencyandfinalperformance.
Environment Envsteps MAZero w/oOS(λ)andAWPO
250k 184.6±22.8 104.0±87.5
LunarLander 500k 259.8±12.9 227.7±56.3
1M 276.9±2.9 274.1±3.5
Table9: AblationforusingOS(λ)andAWPOinLunarLanderenvironments
Environment Envsteps MAZero w/oOS(λ)andAWPO TD3 SAC
300k 3424±246 2302±472 1101±386 1989±500
Walker2D 500k 4507±411 3859±424 2878±343 3381±329
1M 5189±382 4266±509 3946±292 4314±256
Table10: AblationforusingOS(λ)andAWPOinWalker2Denvironments
21PublishedasaconferencepaperatICLR2024
F EXPERIMENTS ON OTHER MULTI-AGENT ENVIRONMENTS
It is beneficial to validate the performance of MAZero in other tasks beyond the SMAC bench-
mark. We further benchmark MAZero on Google Research Football(GRF) (Kurach et al., 2020),
academy pass and shoot with keeper scenario and compare our methods with several model-free
baseline algorithms. Table 11 shows that our method outperforms baselines in terms of sample
efficiency.
Environment Envsteps MAZero CDS RODE QMIX
500k 0.123±0.089 0.069±0.041 0 0
academy pass and
1M 0.214±0.072 0.148±0.117 0 0
shoot with keeper
2M 0.619±0.114 0.426±0.083 0.290±0.104 0
Table11: ComparisonsagainstbaselinesinGRF.
G DERIVATION OF AWPO LOSS
ToprovethatAWPOloss(Equation(10))isessentiallysolvingthecorrespondingconstrainedop-
timization problem (Equation (11)), we only need to prove the closed form of Equation (11) is
Equation(12).
WefollowthederivationinNairetal.(2021). Notethatthefollowingoptimizationproblem
η∗ =argmaxE [A(s,a)]
a∼η(·|s)
η
s.t. KL(η(·|s)∥π(·|s))≤ϵ (15)
(cid:90)
η(a|s)da=1.
a
hasLagrangian
L(η,λ,α)=E [A(s,a)]
a∼η(·|s)
+λ(ϵ−D (η(·|s)∥π(·|s)))
KL (16)
(cid:18) (cid:90) (cid:19)
+α 1− η(a|s)da .
a
ApplyingKKTcondition,wehave
∂L
=A(s,a)+λlogπ(a|s)−λlogη(a|s)+λ−α=0 (17)
∂η
Solvingtheaboveequationgives
(cid:18) (cid:19)
1 1
η∗(a|s)= π(a|s)exp A(s,a) (18)
Z(s) λ
whereZ(s)isanormalizingfactor.
TomaketheKKTconditionhold, wecanletη > 0andusetheLICQ(Linearindependencecon-
straintqualification)condition.
Pluggingtheoriginalproblem(Equation(11))intoEquation(15)completesourproof.
22