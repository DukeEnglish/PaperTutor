General bounds on the quality of Bayesian coresets
TrevorCampbell∗
DepartmentofStatistics
UniversityofBritishColumbia
trevor@stat.ubc.ca
Abstract
Bayesiancoresetsspeedupposteriorinferenceinthelarge-scaledataregimeby
approximatingthefull-datalog-likelihoodfunctionwithasurrogatelog-likelihood
basedonasmall,weightedsubsetofthedata. ButwhileBayesiancoresetsand
methodsforconstructionareapplicableinawiderangeofmodels,existingtheoret-
icalanalysisoftheposteriorinferentialerrorincurredbycoresetapproximations
onlyapplyinrestrictivesettings—i.e.,exponentialfamilymodels,ormodelswith
strong log-concavity and smoothness assumptions. This work presents general
upperandlowerboundsontheKullback-Leibler(KL)divergenceofcoresetap-
proximationsthatreflectthefullrangeofapplicabilityofBayesiancoresets. The
lowerboundsrequireonlymildmodelassumptionstypicalofBayesianasymptotic
analyses, whiletheupperboundsrequirethelog-likelihoodfunctionstosatisfy
ageneralizedsubexponentialitycriterionthatisweakerthanconditionsusedin
earlierwork. Thelowerboundsareappliedtoobtainfundamentallimitationson
thequalityofcoresetapproximations,andtoprovideatheoreticalexplanationfor
thepreviously-observedpoorempiricalperformanceofimportancesampling-based
constructionmethods. Theupperboundsareusedtoanalyzetheperformanceof
recentsubsample-optimizemethods.Theflexibilityofthetheoryisdemonstratedin
validationexperimentsinvolvingmultimodal,unidentifiable,heavy-tailedBayesian
posteriordistributions.
1 Introduction
Large-scaledataisnowcommonplaceinscientificandcommericalapplicationsofBayesianstatistics.
Butdespiteitsprevalence,andthecorrespondingwealthofresearchdedicatedtoscalableBayesian
inference,therearestillsuprisinglyfewgeneralmethodsthatprovablyprovideinferentialresults,
withinsomereasonabletoleratederror,atasignificantcomputationalcostsavings. ExactMarkov
chainMonteCarlo(MCMC)methodsrequiremanyfullpassesoverthedata[1,Ch.6–12,2,Ch.11–
12], limiting the utility of these methods when even a single pass is expensive. A wide range of
MCMCmethodsthataccessonlyasubsetofdataperiteration,e.g.,viadelayedacceptance[3–6],
pseudomarginalorauxiliaryvariablemethods[7–9],andbasicsubsampling[10–13],provideatmost
aminorimprovementoverfull-dataMCMC[14–16]. Ontheotherhand,methodsincludingcarefully
constructedlog-likelihoodfunctioncontrolvariatescanprovidesubstantialgains[17–19]. However,
black-boxcontrolvariateconstructionsforlarge-scaledataoftenrelyonassumptionssuchasposterior
densitydifferentiabilityandunimodalitythatdonotholdinmanypopularmodels,e.g.,thosewith
discretevariablesormultimodality. See[15,20]forasurveyofscalableMCMCmethods. Parametric
approximationsviavariationalinference[21]ortheLaplaceapproximation[22,23]canbeobtained
scalablyusingstochasticoptimizationmethods,butexistinggeneraltheoreticalguaranteesforthese
methodsagaintypicallyrelyonposteriornormalityassumptions[24,p.141–144,25–30](see[21,31]
forareview).
∗https://trevorcampbell.me
Preprint.Underreview.
4202
yaM
02
]LM.tats[
1v08711.5042:viXraAlthoughmanyexistingmethodsrelyonparticularposteriorstructure(e.g.,approximatenormality)
inthelarge-scaledataregime,theproblemofhandlinglarge-scaledatainBayesianinferencedoesnot
fundamentallyinvolvesuchstructure. Instead,itrequiresexploitingredundancyinthedata(i.e.,the
existenceofgoodapproximatesufficientstatistics),whichcanbeusedtodrawprincipledconclusions
aboutalargedatasetbasedonlyonasmallfractionofexamples. Indeed,whileposteriornormality
oftendoesnotholdinmodelswithlatentdiscreteorcombinatorialobjects,weaklyidentifiableor
unidentifiableparameters,persistingheavytails,multimodality,etc.,suchmodelscanandregularly
doexhibitsignificantredundancyinthedatathatcanbeexploitedforfasterlarge-scaleinference.
Bayesian coresets [32]—which involve replacing the full dataset during inference with a sparse
weightedsubset—arebasedonthisnotionofexploitingdataredundancy. Empiricalstudieshave
shown the existence of high-quality coreset posterior approximations constructed from a small
fraction of the data, even in models that violate posterior normality assumptions and for which
standard control variate techniques work poorly [33–37]. However, existing theoretical support
forBayesiancoresetsintheliteratureislimited. ThereexistnolowerboundsonBayesiancoreset
approximationerror,andwhileupperboundsdoexist,theycurrentlyimposerestrictiveassumptions.
Inparticular,thebestavailabletheoreticalupperboundstodateapplytoexponentialfamilymodels
[36,38]andmodelswithstronglylog-concaveandlocallysmoothlog-densities[37].
ThisarticlepresentsnewtheoreticaltechniquesandresultsregardingthequalityofBayesiancoreset
approximations. The main results are two general large-data asymptotic lower bounds on the
KL divergence (Theorems 3.3 and 3.5), as well as a general upper bound on the KL divergence
(Theorem5.3)undertheassumptionthatthelog-likelihoodssatisfyamultivariategeneralization
ofsubexponentiality(Definition5.2). Themaingeneralresultsinthispaperleadtovariousnovel
insightsaboutspecificBayesiancoresetconstructionmethods. Undermildassumptions,
• commonimportance-weightedcoresetconstructions(e.g.[32])requireacoresetsizeM
proportionaltothedatasetsizeN (Corollary4.1),evenwithpost-hocoptimalweightscaling
(Corollary4.2),andthusyieldanegligibleimprovementoverfull-datainference;
• anyconstructionalgorithmrequiresacoresetsizeM >dwhenthelog-likelihoodfunction
isdeterminedbydparameterslocallyaroundapointofconcentration(Corollary4.3);
• subsample-optimizecoresetconstructionalgorithms(e.g.[36–39])achieveanasymptotically
boundederrorwithacoresetsizepolylogN inawidevarietyofmodels(Corollary6.1).
Thepaperincludesempiricalvalidationofthemaintheoreticalclaimsontwomodelsthatviolate
commonassumptionsmadeintheliterature: amultimodal,unidentifiableCauchylocationmodel
withaheavy-tailedprior,andanunidentifiablelogisticregressionmodelwithaheavy-tailedprior
andpersistingposteriorheavytails. AllexperimentswereperformedonacomputerwithanIntel
Corei7-8700Kand32GBofRAM.
2 Background
DefineatargetprobabilitydistributionπonaspaceΘcomprisedofasumofNpotentialsℓ :Θ→R,
n
n=1,...,N andabasedistributionπ (dθ),
0
N
1 (cid:88)
π(dθ)= exp(ℓ(θ))π (dθ), ℓ(θ)= ℓ (θ), θ ∈Θ,
Z 0 n
n=1
wherethenormalizationconstantZ isnotknown. IntheBayesiancontext,thisdistributioncorre-
spondstoaBayesianposteriordistributionforastatisticalmodelwithpriorπ andconditionally
0
i.i.d. data X , where ℓ (θ) = logp(X |θ). The goal is to compute or approximate expectations
n n n
underπ;butthelikelihoodℓ(anditsgradient)becomesexpensivetoevaluatewhenN islarge. To
avoidthiscost,Bayesiancoresets[32–37]involvereplacingthetargetwithasurrogatedensity
N
1 (cid:88)
π (dθ)= exp(ℓ (θ))π (dθ), ℓ (θ)= w ℓ (θ), θ ∈Θ,
w Z(w) w 0 w n n
n=1
wherew ∈RN,w ≥0areasetofweights,andZ(w)isthenewnormalizingconstant. Ifwhasat
(cid:80)
mostM ≪ N nonzeros,theO(M)costofevaluating w ℓ (anditsgradient)isasignificant
n n n
improvement upon the original O(N) cost. In this work, the problem of coreset construction is
formulatedinthedata-asymptoticlimit;acoresetconstructionmethodshould
2• runino(N)timeandmemory(oratmostO(N)withasmallleadingconstant),
• produceasmallcoresetofsizeM =o(N),
• produceacoresetwithO(1)posteriorforward/reverseKLdivergenceasN →∞.
Thesethreedesiderataensurethattheeffortspentconstructingandsamplingfromthecoresetposterior
isworthwhile: thecoresetprovidesameaningfulreductionincomputationalcostcomparedwith
standardMarkovchainMonteCarloalgorithms,andhasaboundedapproximationerror.
3 Lowerboundsonapproximationerror
This section presents lower bounds on the KL divergence of coreset approximations for general
modelsanddatageneratingprocesses. Thefirstkeystepsintheanalysisaretowriteallexpectations
intermsofdistributionsthatdonotdependonw,andtoremovethedifficult-to-controlinfluenceof
thetailsofπandπ byrestrictingcertainintegralstosomesmallsubsetB ⊆Θoftheparameter
w
space. Lemma3.1,thekeytheoreticaltoolusedinthissection,achievesbothofthesetwogoals;note
thattheresulthasnomajorassumptionsandappliesgenerallyinanysettingthataBayesiancoreset
canbeused. Forconvenience,define
KL(w):=min{KL(π ||π),KL(π||π )}.
w w
Lemma3.1(BasicKLLowerBound). ForallmeasurableB ⊆Θandcoresetweightsw,
KL(w)≥f(J (w))≥0,
B
wheref(x)=−logmin(1,x)+min(1,x)−1isdecreasingandnonnegativeonx≥0,and
J (w)=
(cid:82) Bπ 0exp 21(ℓ+ℓ w) +(cid:112)
π(Bc).
B (cid:113)
(cid:82) (cid:82)
π exp(ℓ) π exp(ℓ )
0 0 w
NotethatwhiletheintegralsinthefractiondenominatorinJ (w)rangeoverthewholeΘspace,a
B
furtherlowerboundonKL(w)canbeobtainedbyrestrictingtheirdomainsarbitrarily.Also,crucially,
theboundinLemma3.1doesnotdependonπ (Bc),whichwouldbedifficulttoanalyzewithout
w
detailedknowledgeofthetailbehaviourofπ asafunctionofthecoresetweightsw. Although
w
theboundinLemma3.1appliesgenerally,itismostusefulwhenB issmall(sothatsimplelocal
approximationsofℓandℓ canbeused),πconcentratesonB (sothatπ(Bc)≈0),andπandπ
w w
areverydifferentwhenrestrictedtoB;thebehaviouroftheboundinthiscaseisroughly(seethe
proofinAppendixA)f(J (w))≈−log(1−TV(π,π )). Finally,notethatLemma3.1remains
B w
validifonereplacesℓ withℓ −candℓwithℓ−c′foranyconstantsc,c′thatdonotdependonθ
w w
butmaydependonthedataandcoresetweightsw.
Fortheremainderofthissection,considerthesettingwhereΘisameasurablesubsetofRdforsome
d∈N,fixsomeθ ∈Θ,andassumeeachℓ isdifferentiableinaneighbourhoodofθ . Let
0 n 0
(cid:88)
w = w g =∇ℓ(θ ) g =∇ℓ (θ ).
n 0 w w 0
n
Theorems3.3and3.5characterizeKLdivergencelowerboundsintermsofthesumofthecoreset
weightswandthelog-likelihoodgradientsg,g . Intuitivelyforthefulldatasetwhereallw =1
w n
andw =N,andani.i.d.datageneratingprocessfromthelikelihoodwithparameterθ ,thecentral
0
limittheoremassertsundermildconditionsthatg /w →p 0atarateofN−1/2. Theorems3.3and3.5
w
belowprovideKLlowerboundswhenthecoresetconstructionalgorithmdoesnotmatchthisbehavior.
p
Inparticular,Theorem3.3providesresultsthatareusefulwheng /w →0occursreasonablyquickly
w
butslowerthanN−1/2,whileTheorem3.5strengthenstheconclusionwheng /w →p 0veryslowly
w
ornotatall. ThemajorbenefitofTheorems3.3and3.5foranalyzingcoresetconstructionmethods
isthattheyreducetheproblemofanalyzingposteriorKLdivergencetothemucheasierproblemof
analyzingthe2-norm∥·∥ ofaweightedsumofrandomvectorsinRd.
2
Considerasequencer →0asN →∞,andforafixedmatrixH ≻0let
B ={θ :(θ−θ )TH(θ−θ )≤r2}
0 0
3beasequenceofneighbourhoodsaroundθ ;thesewillappearinAssumptions3.2and3.4andTheo-
0
rems3.3and3.5below. Notethatthroughout,allasymptoticswillbetakenasN →∞,andvarious
sequences(e.g.,randB)areimplicitlyindexedbyN. Tosimplifynotation,thisdependencewillbe
leftimplicit. Assumption3.2makessomeweakassumptionsaboutthemodelanddatagenerating
process:itintuitivelyassertsthatthepotentialfunctionsaresufficientlysmootharoundθ ,thatr →0
0
slowly,andthatπconcentratesatθ atausualrate. NotethatAssumption3.2doesnotassumedata
0
aregeneratedi.i.d.andplacesnoconditionsonthecoresetconstructionalgorithm.
Assumption 3.2. π (dθ) has a density with respect to the Lebesgue measure, π (θ ) > 0, each
0 0 0
ℓ (θ)andπ (θ)aretwicedifferentiableinBforsufficientlylargeN,and
n 0
θs ∈u Bp(cid:13) (cid:13) (cid:13) (cid:13)− N1 ∇2ℓ(θ)−H(cid:13) (cid:13) (cid:13) (cid:13) 2 =o p(1), (cid:13) (cid:13) (cid:13)Ng (cid:13) (cid:13) (cid:13) 2 =O p(cid:16) N−1/2(cid:17) , Nr2 =ω(1).
Twoadditionalassumptionsrelatedtothecoresetconstructionalgorithm—namely,thatitworkswell
enoughthat 1 (cid:80) w ∇2ℓ (θ)→p H andg /w →p 0ataratefasterthanr →0—leadtoasymptotic
w n n n w
lowerboundsonthebestpossiblequalityofcoresetsproducedbythealgorithm,aswellaslower
boundsevenafteroptimalpost-hocscalingoftheweights.
Theorem3.3. SupposeAssumption3.2holds. If
θs ∈u Bp(cid:13) (cid:13) (cid:13) (cid:13)− w1 ∇2ℓ w(θ)−H(cid:13) (cid:13) (cid:13) (cid:13) 2 =o p(1), (cid:13) (cid:13) (cid:13)g ww(cid:13) (cid:13) (cid:13) 2 =o p(r),
then
(cid:26) Nw (cid:13) g g (cid:13)2 (N +w)2 (cid:27)
KL(w)≥O (1)+Ω (1)min −logπ(Bc), (cid:13) − w(cid:13) +dlog
p p N +w(cid:13)N w (cid:13) 2 Nmax{w,1/r2}
(cid:26) (cid:18) (cid:13) g g (cid:13)2(cid:19)(cid:27)
minKL(αw)≥O (1)+Ω (1)min −logπ(Bc),dlog N(cid:13) − w(cid:13) .
α≥0 p p (cid:13)N w (cid:13) 2
Theorem 3.3 is restricted to the case where the coreset algorithm is performing reasonably well.
Theorem3.5extendstheboundstothecasewherethealgorithmisperformingpoorly,inthesense
thatitisunabletomake gw →p 0ataratefasterthanr →0(orperhaps gw doesnotconvergeto0at
w w
all). Inordertodrawconclusionsinthissetting,weneedaweakglobalassumptiononthepotential
functions. Afunctionf :Θ→RisL-smoothbelowatθ if
0
L
∀θ ∈Θ, f(θ)≥f(θ )+∇f(θ )T(θ−θ )− ∥θ−θ ∥2. (1)
0 0 0 2 0 2
Note that L-smoothness below is weaker than both Lipschitz smoothness and strong concavity;
Eq.(1)restrictsthegrowthofthefunctiononlyinthenegativedirection,andonlywhentheexpansion
istakenatθ . Assumption3.4assertsthatthepotentialfunctionsaresmoothbelow.
0
Assumption3.4. ThereexistL ,...,L ,L>0suchthatlogπ isL2-smoothbelowatθ ,foreach
0 N 0 0 0
n∈[N]ℓ isL2-smoothbelowatθ ,and 1 (cid:80)N L2 →p L2.
n n 0 N n=1 n
Theorem3.5usesAssumptions3.2and3.4andadditionalassumptionsrelatedtothecoresetcon-
structionalgorithmtoobtainlowerboundsinasettingthatrelaxesthe“performance”conditionsin
Theorem3.3: −1 (cid:80) w ∇2ℓ (θ)nolongerneedstoconvergetoH inprobability,andg /wcan
w n n n w
convergeto0slowlyornotatall.
Theorem3.5. SupposeAssumptions3.2and3.4hold. Ifthereexistα,β >0suchthat
P(cid:18)
∀θ ∈B,
−1
∇2ℓ
(θ)⪰αH(cid:19)
→1,
P(cid:32) 1 (cid:88)
w L2
≤βL2(cid:33)
→1,
(cid:13) (cid:13)g w(cid:13)
(cid:13)=ω (r),
w w w n n (cid:13) w (cid:13) p
n
then
(cid:26) (cid:18) (cid:26)(cid:13)g (cid:13)2 (cid:27)(cid:19)(cid:27)
KL(w)=O (1)+Ω (1)min −logπ(Bc),dlog Nmin (cid:13) w(cid:13) ,1 .
p p (cid:13) w (cid:13)
AnimportantfinalnoteinthissectionisthatwhileTheorems3.3and3.5,asstated,requirechoosing
ΘtobesomemeasurablesubsetofRd andthattheposteriorπconcentratesaroundsomepointof
interestθ ∈Rd,theseresultscanbegeneralizedtoawiderclassofmodelsandspaces. Inparticular,
0
Corollary3.6demonstratesthatifΘisarbitrary, butthepotentialfunctionsℓ onlydependonθ
n
throughsomeotherfunctionη :Θ→Rd,thattheconclusionsofTheorems3.3and3.5stillhold.
4(a) (b)
Figure1: Exampleunnormalizedposteriordensitiesgiven50datapointsfor(1a)theCauchylocation
model and (1b) the logistic regression model. The orange and blue dashed lines in (1b) indicate
one-dimensionalslicesthatareshownintherightmostpanels.
Corollary3.6. SupposeΘisanarbitrarymeasurablespace,andthepotentialfunctionstakethe
formℓ (η(θ))forsomemeasurablefunctionη :Θ→Rd. TheniftheassumptionsofTheorems3.3
n
and3.5holdforpotentials(ℓ )N asfunctionsonRdandpushforwardpriorηπ onRd,thestated
n n=1 0
lowerboundsalsoholdformin{KL(π||π ),KL(π ||π)}.
w w
4 Lowerboundapplications
In this section, the general theoretical results from Section 3 are applied to specific algorithms,
Bayesianmodels,anddatageneratingprocessestoexplainpreviouslyobservedempiricalbehaviour
of coreset construction, as well as to place fundamental limits on the necessary size of coresets.
Consider a setting where the data X arise as an i.i.d. sequence drawn from some probability
n
distributionν,ℓ (η(θ))=logp(X |η(θ))forη :Θ→Rd,η =η(θ ),andthefollowingtechnical
n n 0 0
criteriahold(whereEdenotesexpectationunderthedatageneratingprocess):
(A1) E[∇ℓ (η )]=0andH =E(cid:2) −∇2ℓ (η )(cid:3) =E(cid:2) ∇ℓ (η )∇ℓ (η )T(cid:3) ≻0.
n 0 n 0 n 0 n 0
(A2) E(cid:2) ∥∇ℓ (θ )∥2+δ(cid:3) <∞forsomeδ >0andE(cid:2) ∥∇2ℓ (θ )∥2(cid:3) <∞.
n 0 2 n 0 F
(A3) Onaneighbourhoodofη ,∥∇2ℓ (η)−∇2ℓ (η )∥ ≤R(X )∥η−η ∥ ,E[R(X )]<∞.
0 n n 0 2 n 0 2 n
(A4) ηπ istwicedifferentiableaneighbourhoodofη ,andπ(η )>0.
0 0 0
(A5) Forallr →0suchthatr =Ω (N−1/2),−logηπ(∥η−η ∥ >r)=Ω (Nr2).
p 0 2 p
Theseconditionsapplytoawiderangeofmodels,e.g.,anunidentifiable,multimodallocationmodel
posteriorwithheavytailsonΘ=R,wheretheBayesianmodelisspecifiedby
θ ∼Cauchy(0,1) (X )N ∼iid Cauchy(θ2,1), (2)
n n=1
andthedataaregeneratedfromthelikelihoodwithparameterθ =5,andanunidentifiablelogistic
0
regressionposteriorwithheavytailsonR2,wheretheBayesianmodelisspecifiedby
(cid:18) (cid:19) (cid:20) (cid:21)
ind 1 1 1
θ ∼Cauchy(0,I) Y ∼ Bern A= , (3)
n 1+e−X nTAθ 1 1
thecovariatesaregeneratedviaX ∼iid Unif({x ∈ R2 : ∥x∥ ≤ 1}),andtheobservationsY are
n 2 n
generatedfromthelikelihoodwithparameterθ = [1 6]T . Exampleposteriorlog-densitiesfor
0
thesemodelsaredisplayedinFig.1.
4.1 Minimumcoresetsizeforimportance-weightedcoresets
Apopularalgorithmforcoresetconstructionthathasappearedinawidevarietyofdomains—e.g.,
Bayesianinference[32,33,Section4.1],frequentistinference(e.g.,[40–44]),andoptimization(see
5Algorithm1Importance-weightedcoresetconstruction
Computeprobabilities(p )N (maydependonthedataandmodel)
n n=1
iid
DrawI ,...,I ∼Categorical(p ,...,p )
1 M 1 N
Foreachn,setw = 1 (cid:80)M 1[I =n].
n Mpn m=1 m
return(w )N
n n=1
Algorithm2Scaledimportance-weightedcoresetconstruction
Obtaincoresetweights(w )N viaAlgorithm1
n n=1
Computeα⋆ =argmin KL(π ||π)
α≥0 αw
return(α⋆w )N
n n=1
[45]forarecentsurvey)—involvessubsamplingofthedatafollowedbyanimportance-weighting
correction. ThepseudocodeisgiveninAlgorithm1. NotethatE[w ] = 1,andsoE[ℓ ] = ℓ;the
n w
coresetpotentialisanunbiasedestimateoftheexactpotential. Theadvantageofthismethodisthatit
isstraightforwardandcomputationallyefficient. Ifthesamplingprobabilitiesareuniformp
n
=1/N,
thenAlgorithm1constructsacoresetinO(M)timeandO(M)memory. Nonuniformprobabilities
p requireO(N)timeandmemory: atleastasingleO(N)passoverthefulldatasettocompute
n
eachp [32,41], followedbysamplingthecoresetinO(N +M)timeandmemory, e.g., viaan
n
aliastable[46,47]. However,empiricalresultsproducedbythismethodologyhavegenerallybeen
underwhelming,evenwithcarefullychosensamplingprobabilities;see,e.g.,Figure2of[32].
Corollary4.1explainsthesepoorresults: BayesiancoresetsconstructedviaAlgorithm1mustsatisfy
M ∝N inordertomaintainaboundedKL(w)inthedata-asymptoticlimit. Inotherwords,such
coresetsdonotsatisfythedesideratainSection2. Theonlyrestrictionisthatthereexistconstants
c,C >0suchthatforallN ∈N,thesamplingprobabilities(p )N satisfy
n n=1
(A6) 0<c≤minNp ≤maxNp ≤C <∞ a.s. (4)
n n
n n
Thelowerthresholdensuresthatthevarianceoftheimportance-weightedlog-likelihoodisnottoo
large, whiletheupperthresholdensuressufficientdiversityinthedrawsfromsubsampling. The
conditioninEq.(4)isnotamajorrestriction,inthesensethatperformanceshoulddeteriorateeven
furtherwhenitdoesnothold. The(p )N mayotherwisedependarbitrarilyonthedataandmodel.
n n=1
Corollary4.1. Given(A1-6),M →∞,andM =o(N),coresetsproducedbyAlgorithm1satisfy
(cid:18) (cid:19)
N
KL(w)=Ω . (5)
p M
TheintuitionbehindCorollary4.1isthatboththetrueposteriorandtheimportance-weightedcoreset
posteriorareasymptoticallyapproximatelynormalwithvariance∝1/N asN →∞;however,the
coresetposteriormeanisroughly∝M−1/2awayfromtheposteriormean,becausethesubsample
isofsizeM. TheKLdivergencebetweentwoGaussiansislower-boundedbytheinversevariance
timesthemeandifferencesquared,yielding≈N/M asinEq.(5).
Giventheintuitionthatthecoresetposteriormeanisfarfromtheposteriormeanrelativetotheir
variances, it is worth asking whether one can apply a small amount of effort to “correct” the
importance-weightedcoresetbyscalingtheweights(andhencethevariance)down, asshownin
Algorithm2. Unfortunately,Corollary4.2demonstratesthatevenwithoptimalscaling,M ∝N is
stillrequiredinordertomaintainaboundedKLdivergenceasN →∞.
Corollary4.2. Given(A1-6),M →∞,andM =o(N),coresetsproducedbyAlgorithm1satisfy
(cid:18) (cid:19)
N
minKL(αw)=Ω log .
α>0 p M
Fig.2providesempiricalconfirmationofCorollaries4.1and4.2ontheCauchylocationandlogisticre-
gressionmodelsinEqs.(2)and(3). Inparticular,thesefiguresshowthattheempiricalratesofgrowth
ofKLasafunctionofN closelymatchesΩ (N)forimportance-weightedcoresets,andΩ (log N)
p M p M
6Figure2: Importance-weightedcoresetquality,showingtheminimumoftheforwardandreverse
KLdivergencesontheverticalaxisasafunctionofdatasetsizeN for3coresetsizes: logN (black),
√
N (blue), and1/2N (red). DashedlinesindicatepredictionsfromthetheoryinCorollaries4.1
and4.2,solidlinesindicatethemeanover10trials,anderrorbarsindicatestandarderror. Thetop
rowshowsthequalityofbasicimportance-weightedcoresets(notethatbothhorizontalandvertical
axesareinlogscale),whilethebottomrowshowsthequalitywithoptimalpost-hocscaling(note
thatonlythehorizontalaxisisinlogscale). TheleftcolumncorrespondstotheCauchylocation
model,whiletherightcolumncorrespondstothelogisticregressionmodel. Samplingprobabilities
p forbothmodelsaresetproportionaltoX2,thresholdedtoliebetween0.1/N and10/N.
n n
√
forthesamewithpost-hocscaling,forawiderangeofcoresetsizesM ∈{logN, N ,1/2N}. Thus,
importanceweightedcoresetconstructionmethodsdonotsatisfythedesideratainSection2fora
widerangeofmodels,andalternatemethodsshouldbeconsidered.
4.2 Minimumcoresetsizeforanycoresetconstruction
Thissectionextendstheminimumcoresetsizeresultsfromimportance-weightedschemestoany
coresetconstructionalgorithm. Inparticular,Corollary4.3showsthatunder(A7)—astrengthening
of (A3) and Assumption 3.4—and (A8)—which asserts that ∇ℓ (η ),...,∇ℓ (η ) are linearly
1 0 M 0
independenta.s.andsatisfyatechnicalmomentcondition—atleastdcoresetpointsarerequiredto
keeptheKLdivergenceboundedasN →∞.
(A7) Assumption3.4holdsandthereexistsγ >0suchthatforallsufficientlylargeN ∈N,
∀η ∈B,n∈[N], −∇2ℓ (η)⪰γH and L2 <γ−1L2.
n n
(A8) ForallcoresetsizesM <d,thereexistsaδ >0suchthat
E(cid:104)(cid:0) 1T(GTG)−11(cid:1)M+δ(cid:105)
<∞ G=[∇ℓ (η ) ... ∇ℓ (η )]∈Rd×M.
1 0 M 0
Corollary4.3. ForafixedcoresetsizeM <d,given(A1-5,7,8),
min KL(w)=Ω (logN).
p
w∈RN +:∥w∥0≤M
75 Upperboundsonapproximationerror
ThissectionpresentsupperboundsontheKLdivergenceofcoresetapproximations. AsinSection3,
thefirststepistowriteallexpectationsintermsofdistributionsthatdonotdependonw. Lemma5.1
doessowithoutimposinganymajorassumptions;theresultagainappliesgenerallyinanysetting
thataBayesiancoresetcanbeused. Forconvenience,define
KL(w):=max{KL(π ||π),KL(π||π )}.
w w
Lemma5.1(BasicKLUpperBound). Forallcoresetweightsw,
KL(w)≤ inf 1 log(cid:90) πexp(cid:0) (1+λ)(ℓ¯ −ℓ¯)(cid:1) ,
λ>0λ w
whereforalln∈[N],ℓ¯ =ℓ −(cid:82) πℓ ,ℓ¯=(cid:80) ℓ¯ ,andℓ¯ =(cid:80) w ℓ¯ .
n n n n n w n n n
TheupperboundinLemma5.1isnonvacuous(i.e.,finite)aslongasthereexistsaα>1suchthat
theαRe´nyidivergenceD (π ||π)[48,p.3799]isfinite. NotethatasinLemma3.1,theboundin
α w
Lemma5.1remainsvalidifonereplacesℓ withℓ −candℓwithℓ−c′foranyconstantsc,c′that
w w
donotdependonθbutmaydependonthecoresetweightswanddata.
Morepracticalboundsnecessitateanassumptionaboutthebehaviourofthepotentials(ℓ )N . Defi-
n n=1
nition5.2belowassertsthatthemultivariatemomentgeneratingfunctionof(ℓ )N isboundedwhen
n n=1
thevectoriscloseto0. Thisdefinitionisageneralizationoftheusualdefinitionofsubexponentiality
fortheunivariatesetting(e.g.,[49,Sec.2.7]). Theorem5.3subsequentlyshowsthatDefinition5.2is
sufficienttoobtainsimpleboundsonKL.
Definition 5.2. For A ∈ RN×N, A ⪰ 0, and monotone function f : R → R such that
+ +
lim f(x)=f(0)=0,thepotentials(ℓ )N are(f,A)-subexponentialif
x→0 n n=1
(cid:90)
∀w ∈RN :wTAw ≤1, πexp(cid:0) ℓ¯ (cid:1) ≤exp(cid:0) f(wTAw)(cid:1) .
w
Theorem5.3. Ifthepotentials(ℓ )N are(f,A)-subexponential,then
n n=1
∀w ∈RN :4(w−1)TA(w−1)≤1, KL(w)≤f(4(w−1)TA(w−1)).
+
Definition 5.2, the key assumption in Theorem 5.3, is satisfied by a wide range of models when
choosingf(x)=xandA∝Cov (cid:0) (ℓ )N (cid:1) ,asdemonstratedbyProposition5.4. Becausethiscase
π n n=1
applieswidely,letA-subexponentialbeshorthandfor(f,A)-subexponentialitywithf(x)=x.
Proposition5.4. Ifforallw inaballcenteredattheorigin,(cid:82) πexp(ℓ¯ ) < ∞,thenthereexists
w
β >0suchthatthepotentials(ℓ )N areβCov (cid:0) (ℓ )N (cid:1) -subexponential.
n n=1 π n n=1
Inotherwords,intuitively,ifacoresetconstructionalgorithmproducesweightssuchthatVar (ℓ¯ −ℓ¯)
π w
issmall,thenKL(w)isalsosmall. Thatbeingsaid,thegeneralityofDefinition5.2toallowarbitrary
f,Aisstillhelpfulinobtainingupperboundsinspecificcases;see,e.g.,PropositionsA.1andA.2.
6 Upperboundapplication: subsample-optimizecoresets
A strategy to construct Bayesian coresets that has recently emerged in the literature, shown in
Algorithm3,istofirstsubsamplethedatatoselectM datapoints,andsubsequentlyoptimizethe
weightsforthoseselecteddatapoints[36–38]. Thesubsamplingstepservestopickareasonably
flexible basis of log-likelihood functions for coreset approximation, and avoids the slow greedy
selectionroutinesfromearlierwork[33–35]. Theoptimizationsteptunestheweightsfortheselected
basis, avoidingthepoorapproximationsofimportance-weightingmethods. Indeed, Algorithm3
createsexactcoresetswithhighprobabilityinGaussianlocationmodels[36,Prop.3.1]andfinite-
dimensionalexponentialfamilymodels[37,Thm.4.1],andnear-exactcoresetswithhighprobability
instronglylog-concavemodels[37,Thm.4.2]andBayesianlinearregression[38,Prop.3].
Corollary6.1generalizestheseresultssubstantially, anddemonstratesthatcoresetsofsizeM =
O(polylog(N))producedbythesubsample-optimizemethodinAlgorithm3maintainabounded
KL divergence as N → ∞. Two key assumptions are subexponentiality of the potentials and a
8Algorithm3Subsample-optimizecoresetconstruction
Computeprobabilities(p )N (maydependonthedataandmodel)
n n=1
iid
DrawI ,...,I ∼Categorical(p ,...,p ),andsetI ={I ,...,I }
1 M 1 N 1 M
Computew⋆ =argmin KL(π ||π) s.t. w ̸=0onlyifn∈I.
w∈RN w n
+
return(w⋆)N
n n=1
Figure3: Subsample-optimizecoresetquality,showingthemaximumoftheforwardandreverse
KLdivergencesontheverticalaxisasafunctionofdatasetsizeN forcoresetsofsize5+2logN.
Solidlinesindicatethemeanover70trials,anderrorbarsindicatestandarderror. Theleftpanelis
fortheCauchylocationmodel,whiletherightpanelisforthelogisticregressionmodel. Sampling
probabilities are uniform p = 1/N, and coreset weights were optimized by nonnegative least
n
squaresforlog-likelihoodsdiscretizedviasamplesfromπ[34,Eq.4].
polynomial(inN)growthofVar (ℓ(θ));theseconditionsarenotstringentandshouldholdfora
π
widerangeofBayesianmodelsandi.i.d.datageneratingprocesses.ThelastkeyassumptioninEq.(6)
isthatarandomly-chosenpotentialfunctionℓ ,I ∼Categorical(p ,...,p )(withprobabilitiesas
I 1 N
inAlgorithm3)iswell-alignedwiththeresidualcoreseterrorfunction. Similaralignmentconditions
haveappearedinpastresultsformorerestrictivesettings(see,e.g.,J(δ)in[37,Thm.4.1]).
Corollary 6.1. Suppose there exist β,α > 0 and 0 ≤ ρ,ϵ < 1 such that the potential func-
tions (ℓ )N are βCov ((ℓ )N )-subexponential with probability increasing to 1 as N → ∞,
n n=1 π n n=1
Var π(ℓ(θ))=O p(Nα),M =(logN)1−1 ρ,and
P(cid:16) max(cid:8) 0,Corr (cid:0) ℓ (θ),ℓ(θ)−ℓ⋆ (θ)(cid:1)(cid:9)2 ≥1−ϵ(cid:12) (cid:12)(ℓ )N (cid:17) =ω (M−ρ) (6)
π IM M−1 (cid:12) n n=1 p
ℓ⋆ (θ)= argmin Var (ℓ(θ)−g(θ)) I ,...,I ∼iid Categorical(p ,...,p ).
M−1 π 1 M 1 N
g∈cone{ℓI1,...,ℓIM−1}
ThenAlgorithm3producesacoresetwithKL(w)=O (1)asN →∞.
p
Fig.3confirmsthatsubsample-optimizecoresetconstructionmethodsappliedtothelogisticregres-
sionandCauchylocationmodelsinEqs.(2)and(3)(whichbothviolatetheconditionsofpastupper
boundsintheliterature)areabletoprovidehigh-qualityposteriorapproximationsforverysmall
coresets—inthiscase,M ∝logN.
7 Conclusions
This article presented new general lower and upper bounds on the quality of Bayesian coreset
approximations,asmeasuredbytheKLdivergence.Theseresultswereusedtodrawnovelconclusions
regardingimportance-weightedandsubsample-optimizecoresetmethods,whichalignwithsimulation
experimentsontwosyntheticmodelsthatviolatetheassumptionsofpasttheoreticalresults. Avenues
forfutureworkincludegeneralboundsonthesubexponentialityconstantβinProposition5.4,aswell
asthealignmentprobabilityinEq.(6),inthesettingofBayesianmodelswithi.i.d.datagenerating
processes. Alimitationofthisworkisthatbothquantitiescurrentlyrequirecase-by-caseanalysis.
9AcknowledgmentsandDisclosureofFunding
TheauthorgratefullyacknowledgesthesupportofanNSERCDiscoveryGrant(RGPIN-2019-03962).
References
[1] ChristianRobertandGeorgeCasella. MonteCarloStatisticalMethods. Springer,2ndedition,2004.
[2] AndrewGelman,JohnCarlin,HalStern,DavidDunson,AkiVehtari,andDonaldRubin. Bayesiandata
analysis. CRCPress,3rdedition,2013.
[3] J. Andre´s Christen and Colin Fox. Markov chain Monte Carlo using an approximation. Journal of
ComputationalandGraphicalStatistics,14(4):795–810,2005.
[4] MarcoBanterle,ClaraGrazian,AnthonyLee,andChristianP.Robert. AcceleratingMetropolis-Hastings
algorithmsbydelayedacceptance. FoundationsofDataScience,1(2):103–128,2019.
[5] Richard Payne and Bani Mallick. Bayesian big data classification: a review with complements.
arXiv:1411.5653,2014.
[6] ChrisSherlock,AndrewGolightly,andDanielHenderson. Adaptive,delayed-acceptanceMCMCfor
targetswithexpensivelikelihoods. JournalofComputationalandGraphicalStatistics,26(2):434–444,
2017.
[7] ArnaudDoucet,MichaelPitt,GeorgeDeligiannidis,andRobertKohn.EfficientimplementationofMarkov
chainMonteCarlowhenusinganunbiasedlikelihoodestimator. Biometrika,102(2):295–313,2015.
[8] DougalMaclaurinandRyanAdams.FireflyMonteCarlo:exactMCMCwithsubsetsofdata.InConference
onUncertaintyinArtificialIntelligence,2014.
[9] MatiasQuiroz,Minh-NgocTran,MattiasVillani,RobertKohn,andKhue-DungDang. Theblock-Poisson
estimator for optimally tuned exact subsampling MCMC. Journal of Computational and Graphical
Statistics,30(4):877–888,2021.
[10] Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient Langevin dynamics. In
InternationalConferenceonMachineLearning,2011.
[11] SungjinAhn,AnoopKorattikara,andMaxWelling. Bayesianposteriorsamplingviastochasticgradient
Fisherscoring. InInternationalConferenceonMachineLearning,2012.
[12] AnoopKorattikara,YutianChen,andMaxWelling. AusterityinMCMCland: cuttingtheMetropolis-
Hastingsbudget. InInternationalConferenceonMachineLearning,2014.
[13] TianqiChen,EmilyFox,andCarlosGuestrin. StochasticgradientHamiltonianMonteCarlo. InInterna-
tionalConferenceonMachineLearning,2015.
[14] James Johndrow, Natesh Pillai, and Aaron Smith. No free lunch for approximate MCMC.
arXiv:2010.12514,2020.
[15] Re´miBardenet,ArnaudDoucet,andChrisHolmes. OnMarkovchainMonteCarlomethodsfortalldata.
JournalofMachineLearningResearch,18:1–43,2017.
[16] TigranNagapetyan, AndrewDuncan, LeonardHasenclever, SebastianVollmer, LukaszSzpruch, and
KonstantinosZygalakis. ThetruecostofstochasticgradientLangevindynamics. arXiv:1706.02692,2017.
[17] JackBaker,PaulFearnhead,EmilyFox,andChristopherNemeth. Controlvariatesforstochasticgradient
MCMC. StatisticsandComputing,29:599–615,2019.
[18] ChristopherNemethandPaulFearnhead. StochasticgradientMarkovChainMonteCarlo. Journalofthe
AmericanStatisticalAssociation,116(533):433–450,2021.
[19] MatiasQuiroz,RobertKohn,MattiasVillani,andMinh-NgocTran. SpeedingupMCMCbyefficientdata
subsampling. JournaloftheAmericanStatisticalAssociation,114(526):831–843,2019.
[20] MatiasQuiroz,RobertKohn,andKhue-DungDang. SubsamplingMCMC—anintroductionforthesurvey
statistician. Sankhya:TheIndianJournalofStatistics,80-A:S33–S69,2018.
[21] DavidBlei,AlpKucukelbir,andJonMcAuliffe. Variationalinference:Areviewforstatisticians. Journal
oftheAmericanStatisticalAssociation,112(518):859–877,2017.
10[22] ZhenmingShunandPeterMcCullagh. Laplaceapproximationofhighdimensionalintegrals. Journalof
theRoyalStatisticalSociety:SeriesB,57(4):749–760,1995.
[23] PeterHall,TungPham,MattWand,andShenS.J.Wang. Asymptoticnormalityandvalidinferencefor
gaussianvariationalapproximation. TheAnnalsofStatistics,39(5):2502–2532,2011.
[24] AadvanderVaart. AsymptoticStatistics. CambridgeUniversityPress,2000.
[25] Yixin Wang and David Blei. Frequentist consistency of variational Bayes. Journal of the American
StatisticalAssociation,114(527):1147–1161,2018.
[26] Badr-Eddine Che´rief-Abdellatif and Pierre Alquier. Consistency of variational Bayes inference for
estimationandmodelselectioninmixtures. ElectronicJournalofStatistics,12:2995–3035,2018.
[27] YunYang,DebdeepPati,andAnirbanBhattacharya. α-variationalinferencewithstatisticalguarantees.
TheAnnalsofStatistics,2018.
[28] PierreAlquierandJamesRidgway. Concentrationoftemperedposteriorsandoftheirvariationalapproxi-
mations. TheAnnalsofStatistics,48(3):1475–1497,2020.
[29] ZuhengXuandTrevorCampbell. ThecomputationalasymptoticsofGaussianvariationalinferenceand
theLaplaceapproximation. StatisticsandComputing,32(63),2022.
[30] JeffreyMiller. Asymptoticnormality,concentration,andcoverageofgeneralizedposteriors. Journalof
MachineLearningResearch,22:1–53,2021.
[31] ChengZhang,JudithBu¨tepage,HedvigKjellstro¨m,andStephanMandt. Advancesinvariationalinference.
IEEETransactionsonPatternAnalysisandMachineIntelligence,41(8):2008–2026,2019.
[32] Jonathan Huggins, Trevor Campbell, and Tamara Broderick. Coresets for scalable Bayesian logistic
regression. InAdvancesinNeuralInformationProcessingSystems,2016.
[33] TrevorCampbellandTamaraBroderick. AutomatedscalableBayesianinferenceviaHilbertcoresets.
JournalofMachineLearningResearch,20(15):1–38,2019.
[34] TrevorCampbellandBoyanBeronov. Sparsevariationalinference:Bayesiancoresetsfromscratch. In
AdvancesinNeuralInformationProcessingSystems,2019.
[35] TrevorCampbellandTamaraBroderick.Bayesiancoresetconstructionviagreedyiterativegeodesicascent.
InInternationalConferenceonMachineLearning,2018.
[36] NaitongChen,ZuhengXu,andTrevorCampbell. BayesianinferenceviasparseHamiltonianflows. In
AdvancesinNeuralInformationProcessingSystems,2022.
[37] CianNaik,JudithRousseau,andTrevorCampbell. FastBayesiancoresetsviasubsamplingandquasi-
Newtonrefinement. InAdvancesinNeuralInformationProcessingSystems,2022.
[38] MartinJankowiakandDuPhan. Surrogatelikelihoodsforvariationalannealedimportancesampling. In
InternationalConferenceonMachineLearning,2022.
[39] NaitongChenandTrevorCampbell. CoresetMarkovchainMonteCarlo. InInternationalConferenceon
ArtificialIntelligenceandStatistics,2024.
[40] PingMa,MichaelMahoney,andBinYu. Astatisticalperspectiveonalgorithmicleveraging. Journalof
MachineLearningResearch,16:861–911,2015.
[41] HaiYingWang,RongZhu,andPingMa.Optimalsubsamplingforlargesamplelogisticregression.Journal
oftheAmericanStatisticalAssociation,113(522):829–844,2018.
[42] HaiYingWang. Moreefficientestimationforlogisticregressionwithoptimalsubsamples. Journalof
MachineLearningResearch,20:1–59,2019.
[43] MingyaoAi,JunYu,HuimingZhang,andHaiYingWang. Optimalsubsamplingalgorithmsforbigdata
regressions. StatisticaSinica,31(2):749–772,2021.
[44] HaiYingWangandYanyuanMa. Optimalsubsamplingforquantileregressioninbigdata. Biometrika,
108(1):99–112,2021.
[45] DanFeldman. Introductiontocore-sets:anupdatedsurvey. arXiv:2011.09384,2020.
11[46] Alastair Walker. New fast method for generating discrete random numbers with arbitrary frequency
distributions. ElectronicsLetters,10(8):127–128,1974.
[47] AlastairWalker. Anefficientmethodforgeneratingdiscreterandomvariableswithgeneraldistributions.
ACMTransactionsonMathematicalSoftware,3(3):253–256,1977.
[48] TimvanErvenandPeterHarre¨mos.Re´nyidivergenceandKullback-Leiblerdivergence.IEEETransactions
onInformationTheory,60(7):3797–3820,2014.
[49] Roman Vershynin. High-dimensional probability: an introduction with applications in data science.
CambridgeUniversityPress,2020.
[50] IgorVajda. Noteondiscriminationinformationandvariation. IEEETransactionsonInformationTheory,
16(6):771–773,1970.
[51] DavidPollard. Auser’sguidetoprobabilitytheory. Cambridgeseriesinstatisticalandprobabilistic
mathematics.CambridgeUniversityPress,7thedition,2002.
[52] RobertKeener. Theoreticalstatistics:topicsforacorecourse. Springer,2010.
[53] AndreBulinski.Conditionalcentrallimittheorem.TheoryofProbability&itsApplications,61(4):613–631,
2017.
A Proofs
ProofofLemma3.1. ByVajda’sinequality[50],
1+TV(π,π ) 2TV(π,π )
KL(w)≥log w − w
1−TV(π,π ) 1+TV(π,π )
w w
≥−log(1−TV(π,π ))−TV(π,π )
w w
≥0.
TheboundismonotoneincreasinginTV(π,π );thereforebecausethesquaredHellingerdistancesatisfiesthe
w
inequality[51,p.61],
H2(π,π )= 1(cid:90) (cid:0)√ π −√ π (cid:1)2 ≤ 1(cid:90) |π−π |=TV(π,π ),
w 2 w 2 w w
wehavethat
KL(w)≥−log(cid:0) 1−H2(π,π )(cid:1) −H2(π,π ).
w w
WesubstitutethevalueofthesquaredHellingerdistancetofindthat
(cid:18)(cid:90) √ (cid:19) (cid:90) √
KL(w)≥−log ππ + ππ −1≥0.
w w
(cid:82) √
Notethat ππ ≤1,so
w
(cid:18) (cid:90) √ (cid:19) (cid:90) √
KL(w)≥−log min{1, ππ } +min{1, ππ }−1≥0.
w w
(cid:82) √ (cid:82) √
Theboundismonotonedecreasingin ππ ,sowerequireanupperboundon ππ . Toobtainthe
w w
requiredbound,wesplittheintegralintotwoparts—oneonthesetB,andtheotheronBc—andthenusethe
Cauchy-SchwarzinequalitytoboundthepartonBc.Notethatbydefinitionπandπ aremutuallydominating,
w
sothedensityratioπ /πiswell-definedandmeasurable.
w
(cid:90) √ (cid:90) √ (cid:90) √
ππ = ππ + ππ
w w w
B Bc
(cid:90) √ (cid:90) (cid:114) π
= ππ w + π πw 1 Bc
B
(cid:90) √ (cid:112)
≤ ππ + π(Bc)
w
B
=
(cid:82) Bπ 0exp1 2(ℓ+ℓ w) +(cid:112)
π(Bc).
(cid:113)
(cid:82) (cid:82)
π exp(ℓ) π exp(ℓ )
0 0 w
Theresultfollows.
12ProofofLemma5.1. WefirstconsidertheforwardKLdivergence.Bydefinition,
(cid:90) (cid:82) π exp(ℓ )
KL(π||π w)= π(ℓ−ℓ w)+log (cid:82) π0 exp(ℓw
)
0
(cid:90) (cid:90)
= π(ℓ−ℓ )+log πexp(ℓ −ℓ).
w w
SincetheKLispositive,forλ>0,
1+λ(cid:90) 1+λ (cid:90)
KL(π||π )≤ π(ℓ−ℓ )+ log πexp(ℓ −ℓ)
w λ w λ w
1+λ(cid:90) 1 (cid:90)
≤ π(ℓ−ℓ )+ log πexp((1+λ)(ℓ −ℓ))
λ w λ w
1 (cid:90)
= log πexp((1+λ)(ℓ¯ −ℓ¯)),
λ w
byJensen’sinequality.NextweconsiderthereverseKLdivergence.Foranyλ̸=0,
(cid:90) (cid:82) π exp(ℓ)
KL(π w||π)= π w(ℓ w−ℓ)+log(cid:82) π0
exp(ℓ )
0 w
1 (cid:90) (cid:90)
= π λ(ℓ −ℓ)−log πexp(ℓ −ℓ).
λ w w w
ByJensen’sinequality,forλ>0,
1 (cid:90) (cid:90)
KL(π ||π)≤ log π exp(λ(ℓ −ℓ))−log πexp(ℓ −ℓ)
w λ w w w
= λ1 log(cid:82) πe (cid:82)xp π(( e1 xp+ (ℓλ)( −ℓ w ℓ)−ℓ)) −log(cid:90) πexp(ℓ w−ℓ)
w
1 (cid:90) 1+λ (cid:90)
= log πexp((1+λ)(ℓ −ℓ))− log πexp(ℓ −ℓ)
λ w λ w
1+λ(cid:90) 1 (cid:90)
≤ π(ℓ−ℓ )+ log πexp((1+λ)(ℓ −ℓ))
λ w λ w
1 (cid:90)
= log πexp((1+λ)(ℓ¯ −ℓ¯)).
λ w
ThisisthesameboundasintheforwardKLdivergencecase.Sincetheboundappliesforallλ>0,wecantake
theinfimum.
ProofofTheorem3.3. By replacing the integrals over the whole space Θ in the denominator of J (w) in
B
Lemma3.1withintegralsoverthesubsetB,
KL(w)≥−logmin(1,J (w))+min(1,J (w))−1
B B
≥O (1)−logJ (w)
p B
(cid:110) (cid:112) (cid:111)
≥O (1)+min G (w),−log π(Bc)
p B
(cid:90) 1 (cid:90) 1 (cid:90)
G (w)=−log π exp((1/2)(ℓ+ℓ ))+ log π exp(ℓ)+ log π exp(ℓ ).
B 0 w 2 0 2 0 w
B B B
(cid:82)
SotoobtainthestatedlowerboundontheKLdivergence,werequireanupperboundonlog π exp((1/2)(ℓ+
(cid:82) (cid:82) B 0
ℓ )),andlowerboundsonlog π exp(ℓ)andlog π exp(ℓ ). ByTaylor’stheorem,Assumption3.2,
w B 0 B 0 w
andtheassumptionon∇2ℓ (θ),forallθ∈B,
w
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)ℓ(θ)−ℓ(θ 0)−gT(θ−θ 0)+ N
2
(θ−θ 0)TH(θ−θ 0)(cid:12) (cid:12) (cid:12)≤ No 2p(1) (θ−θ 0)TH(θ−θ 0)
(7)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)ℓ w(θ)−ℓ w(θ 0)−g wT(θ−θ 0)+ w 2(θ−θ 0)TH(θ−θ 0)(cid:12) (cid:12) (cid:12)≤ wo p 2(1) (θ−θ 0)TH(θ−θ 0).
WeshifttheexponentialargumentsinG (w)by(1/2)(ℓ(θ )+ℓ (θ )),notethatπ iscontinuousandpositive
B 0 w 0 0
aroundθ ,andandapplytheTaylorexpansionsinEq.(7)toobtainanupperboundonthefirstterm:
0
(cid:90) (cid:90)
log π 0e1 2(ℓ−ℓ(θ0)+ℓw−ℓw(θ0)) ≤O p(1)+log e1 2((g+gw)T(θ−θ0)−(∼1)( 4N+w)(θ−θ0)TH(θ−θ0),
B B
13where(∼1)denotesaquantitythatconvergesinprobabilityto1asN →∞. Wecantransformvariablesto
x=CT(θ−θ ),whereH =CCT istheCholeskyfactorizationofH,andsubsequentlycompletethesquare:
0
log(cid:90) π 0e1 2(...) ≤O p(1)+ (∼1)∥ 4C (N−1 +(g w+ )g w)∥2 +log(cid:90) e−(∼1)( 4N+w)(cid:13) (cid:13) (cid:13) (cid:13)x−(∼1)C (N− +1( wg )+gw)(cid:13) (cid:13) (cid:13) (cid:13)2 .
B ∥x∥2≤r2
(8)
Wecanobtainlowerboundsontheothertwotermsusingasimilartechnique:
log(cid:90)
π eℓ−ℓ(θ0) ≥O (1)+
(∼1)∥C−1g∥2 +log(cid:90) e−(∼1 2)N(cid:13) (cid:13)
(cid:13)
(cid:13)x−(∼1) NC−1g(cid:13) (cid:13)
(cid:13)
(cid:13)2
(9)
0 p 2N
B ∥x∥2≤r2
log(cid:90)
π eℓw−ℓw(θ0) ≥O (1)+
(∼1)∥C−1g w∥2 +log(cid:90) e−(∼1 2)w(cid:13) (cid:13)
(cid:13)
(cid:13)x−(∼1)C w−1gw(cid:13) (cid:13)
(cid:13)
(cid:13)2
. (10)
0 p 2w
B ∥x∥2≤r2
(cid:82)
Itremainstoanalyzethethreelog ... terms.WeboundtheintegralterminEq.(8)withtheintegraloverthe
wholespace:
log(cid:90) e−(∼1)( 4N+w)∥...∥2
≤O p(1)−
d
2log(N +w).
∥x∥2≤r2
FortheintegralterminEq.(9),notethatsinceNr2 =ω(1)and∥C−1g/N∥=O (N−1/2),wehave
p
(cid:90)
log
e−(∼1 2)N∥...∥2
∥x∥2≤r2
(cid:32) (cid:33)
=log
(cid:90) e−(∼1 2)N(...)−(cid:90) e−(∼1 2)N∥x−(∼1) NC−1g∥2
∥x∥2>r2
≥log(cid:32)(cid:18) (∼1)2π(cid:19)d/2 −e−(∼1 4)N min∥x∥≥r(cid:13) (cid:13) (cid:13) (cid:13)x−(∼1) NC−1g(cid:13) (cid:13) (cid:13) (cid:13)2(cid:90) e−(∼1 4)N∥x−(∼1) NC−1g∥2(cid:33)
N
=log(cid:32)(cid:18) (∼1)2π(cid:19)d/2
−e−Ωp(N
4r2)(cid:18) (∼1)4π(cid:19)d/2(cid:33)
N N
d
=− log(N)+O (1).
2 p
FortheintegralterminEq.(10),weconsidertwocases:onewherewislarge,andonewhereitissmall.First
assumewr2 > 8dlog2;thenbyasimilartechniqueasusedinthefirstlowerbound,since∥C−1g /w∥ =
w
o (r),
p
(cid:90)
log
e−(∼1 2)w∥...∥2
∥x∥2≤r2
≥log(cid:32)(cid:18) (∼1)2π(cid:19)d/2 −e−(∼1 4)wmin∥x∥≥r(cid:13) (cid:13) (cid:13) (cid:13)x−(∼1)C w−1gw(cid:13) (cid:13) (cid:13) (cid:13)2(cid:90) e−(∼1 4)w∥x−(∼1)C w−1gw∥2(cid:33)
w
(cid:32)(cid:18) (∼1)2π(cid:19)d/2 (cid:18) (∼1)4π(cid:19)d/2(cid:33)
≥log −e−2dlog2(∼1)
w w
d
≥− logw+O (1).
2 p
Whenwr2 ≤8dlog2,wetransformvariablesy=x/rtofindthatsince∥C−1g /w∥=o (r),
w p
log(cid:90) e−(∼1 2)w∥...∥2
=
d logr2+log(cid:90) e−(∼1) 2wr2(cid:13) (cid:13) (cid:13) (cid:13)y−(∼1)C rw−1gw(cid:13) (cid:13) (cid:13) (cid:13)2
2
∥x∥2≤r2 ∥y∥2≤1
≥
d logr2+loge−8dlog 22(∼1)(cid:18) 2+2(cid:13) (cid:13) (cid:13) (cid:13)(∼1)C rw−1gw(cid:13) (cid:13) (cid:13) (cid:13)2(cid:19)(cid:32) (cid:90) 1(cid:33)
2
∥y∥2≤1
d
= logr2+O (1).
2 p
Thereforeregardlessofthevalueofw,
log(cid:90) e−(∼1 2)w∥...∥2 ≥−d 2log(cid:0) max{w,1/r2}(cid:1)
+O p(1).
∥x∥2≤r2
14Sothereforecombiningallpreviousresults,
(∼1)(cid:18) ∥C−1g∥2 ∥C−1g ∥2 ∥C−1(g+g )∥2(cid:19) d (N +w)2
G (w)≥O (1)+ + w − w + log
B p 4 N w N +w 4 Nmax{w,1/r2}
(∼1)(cid:18) w∥C−1g∥2 N∥C−1g ∥2 2gTH−1g (cid:19) d (N +w)2
=O (1)+ + w − w + log
p 4 N(N +w) w(N +w) N +w 4 Nmax{w,1/r2}
=O p(1)+
(∼ 41)(cid:32) NN +w w(cid:13) (cid:13)
(cid:13)
(cid:13)C N−1g
−
C− w1g w(cid:13) (cid:13)
(cid:13)
(cid:13)2(cid:33)
+
d
4log
Nm(N ax{+ ww ,1) /2
r2}
(cid:18) Nw (cid:13) g g (cid:13)2 (N +w)2 (cid:19)
=O (1)+Ω (1) (cid:13) − w(cid:13) +dlog .
p p N +w(cid:13)N w (cid:13) Nmax{w,1/r2}
Wenowconsidertheminimumoverα≥0.SinceneitherO (1)orΩ (1)abovedependsonw,wehavethat
p p
(cid:26) (cid:18) Nαw (cid:13) g g (cid:13)2 (N +αw)2 (cid:19)(cid:27)
minKL(αw)≥O (1)+Ω (1)min −logπ(Bc), min (cid:13) − w(cid:13) +dlog .
α≥0 p p α≥0 N +αw(cid:13)N w (cid:13) Nmax{αw,1/r2}
Onthe1/r2branchoftheobjectivefunction,thederivativeinαisalwayspositive,andhencetheminimum
occursatα=0,andso
min(...)≥dlog(Nr2).
α≥0
Ontheαwbranchoftheobjectivefunction,
Nαw (cid:13) g g (cid:13)2 (N +αw)2 Nαw (cid:13) g g (cid:13)2 (N +αw)
min (cid:13) − w(cid:13) +dlog ≥min (cid:13) − w(cid:13) +dlog +dlogN.
α≥0 N +αw(cid:13)N w (cid:13) Nαw α≥0 N +αw(cid:13)N w (cid:13) Nαw
Fora,b>0andx≥0,thefunctionax−blogxisconvexinxwithminimumatx⋆ =b/a,andso
(cid:18) (cid:13) g g (cid:13)2(cid:19)
min(...)≥dlog N(cid:13) − w(cid:13) .
α≥0 (cid:13)N w (cid:13)
Byassumption,∥g∥=o (r)and∥gw∥=o (r),andhencetheαwbranchhastheasymptoticminimum:
N p w p
(cid:26) (cid:18) (cid:13) g g (cid:13)2(cid:19)(cid:27)
minKL(αw)≥O (1)+Ω (1)min −logπ(Bc),dlog N(cid:13) − w(cid:13) .
α≥0 p p (cid:13)N w (cid:13)
ProofofTheorem3.5. ByLemma3.1,
KL(w)≥−logmin(1,J (w))+min(1,J (w))−1
B B
(cid:110) (cid:112) (cid:111)
≥O (1)+min G (w),−log π(Bc)
p B
(cid:90) 1 (cid:90) 1 (cid:90)
G (w)=−log π exp((1/2)(ℓ+ℓ ))+ log π exp(ℓ)+ log π exp(ℓ ).
B 0 w 2 0 2 0 w
B
Note that G in this proof is subtly different from the G used in the proof of Theorem 3.3; the latter
B B
twointegralsareoverthewholespace(directlyfromLemma3.1),ratherthanB. Weshifttheexponential
argumentsinG (w)by(1/2)(ℓ(θ )+ℓ (θ )).Wefirstprovidelowerboundsontwooftheintegraltermsvia
B 0 w 0
Assumption3.4:
log(cid:90) π 0eℓ−ℓ(θ0) ≥O p(1)+log(cid:90) e(g+g0)T(θ−θ0)−(∼1)(N 2+1)L′2 ∥θ−θ0∥2 ,
where(∼1)denotesaquantitythatconvergesinprobabilityto1,g = ∇logπ (θ ),andL′2 = NL2+L2 0.
0 0 0 N+1
Transformingvariablesviax=L′(θ−θ ),
0
(cid:90) (cid:90)
log π 0eℓ−ℓ(θ0) ≥O p(1)+log e(g+g0)Tx/L′−(∼1)( 2N+1)∥x∥2
=O p(1)+log(cid:90) e−(∼1)( 2N+1)(cid:13) (cid:13) (cid:13)x− (Ng ++ 1g )0 L′(cid:13) (cid:13) (cid:13)2 +(∼1)( 2N+1)∥ (Ng ++ 1g )0 L′∥2
=O p(1)+
(∼1) 2( LN ′2+1)(cid:13) (cid:13)
(cid:13)
(cid:13)g N+ +g 10(cid:13) (cid:13)
(cid:13)
(cid:13)2
−
d
2log(N +1)
≥O p(1)+
2( m∼ a1 x)( {N L2+ ,L1 2) }(cid:13) (cid:13)
(cid:13)
(cid:13)g N+ +g 10(cid:13) (cid:13)
(cid:13)
(cid:13)2
−
d
2log(N +1).
0
15LetL2 = 1 (cid:80) w L2.Usingthesametechnique,withL′2 = wL2 w+L2 0 andx=L′ (θ−θ ),
w w n n n w w+1 w 0
(cid:90) (cid:90)
log π 0eℓw−ℓw(θ0) ≥log e(gw+g0)T(θ−θ0)−(∼1)( 2w+1)L′ w2∥θ−θ0∥2
≥O p(1)+ w 2L+ ′21(cid:13) (cid:13) (cid:13) (cid:13)g ww+ +g 10(cid:13) (cid:13) (cid:13) (cid:13)2 +log(cid:90) e−(w 2+1)(cid:13) (cid:13) (cid:13) (cid:13)x− (wgw +1+ )g L0 ′ w(cid:13) (cid:13) (cid:13) (cid:13)2
w
≥O p(1)+ w 2L+
′
w21(cid:13) (cid:13) (cid:13) (cid:13)g ww+ +g 10(cid:13) (cid:13) (cid:13) (cid:13)2 +log(cid:90)
∥x− (wgw +1+ )g L0 ′
w∥≤(w+1)−1/3e−w 2+1(cid:13) (cid:13) (cid:13) (cid:13)x− (wgw +1+ )g L0 ′ w(cid:13) (cid:13) (cid:13) (cid:13)2
=O p(1)+
w 2L+ ′21(cid:13) (cid:13)
(cid:13)
(cid:13)g ww+ +g 10(cid:13) (cid:13)
(cid:13)
(cid:13)2
−
d
2log(w+1)
w
≥O p(1)+
2maxw {β+ L1 2,L2}(cid:13) (cid:13)
(cid:13)
(cid:13)g ww+ +g 10(cid:13) (cid:13)
(cid:13)
(cid:13)2
−
d
2log(w+1).
0
For the upper bound on the first term, we use a local quadratic expansion around θ , where H =
0 0
−∇2logπ (θ ),
0 0
log(cid:90) π 0e1 2(ℓ−ℓ(θ0)+ℓw−ℓw(θ0)) ≤O p(1)+log(cid:90) e1 2((g+gw+2g0)T(θ−θ0)−(∼1)(N 4+w+2)(θ−θ0)T(cid:16)(N+ Nαw +) wH ++ 22H0(cid:17) (θ−θ0).
B B
BecauseH ≻0,wehave(N+αw)H+2H ≻0eventually;wecantransformvariablestox=CT(θ−θ ),
0 0
where (N+αw)H+2H0 =CCT istheCholeskyfactorization,andsubsequentlycompletethesquare.Notethat
N+w+2
(cid:112) (cid:112)
min{min(α,1)λ H,λ H } ≤λ C ≤λ C ≤ max{max(α,1)λ H,λ H }
min min 0 min max max max 0
so
λ H
log|C|=O (1) λ C−1HC−T ≥ min =η>0,
p min max{max(α,1)λ H,λ H }
max max 0
andtherefore
(cid:90)
log π
0e21(...)
B
≤O p(1)+ (∼1)(N 4+w+2)(cid:13) (cid:13) (cid:13) (cid:13)C−1 N(g ++ wg w ++ 22g 0)(cid:13) (cid:13) (cid:13) (cid:13)2 +log(cid:90) ∥x∥2≤r2η−1e−(∼1)(N 4+w+2)(cid:13) (cid:13) (cid:13) (cid:13)x−(∼1)C− N1 +(g w+ +g 2w+2g0)(cid:13) (cid:13) (cid:13) (cid:13)2 .
(11)
Supposefirstthatw+1≤N/(4∥C−1∥2max{βL2,L2}). InthiscaseweboundtheintegralinEq.(11)by
0
integratingoverthewholespace:
log(cid:90) π 0e1 2(...) ≤O p(1)+ (∼1)∥C−1∥2 4(N +w+2)(cid:13) (cid:13) (cid:13) (cid:13)g N+ +g w w+ +2 2g 0(cid:13) (cid:13) (cid:13) (cid:13)2 − d 2log(N +w+2).
B
Combiningthiswiththepreviousresultsyields
G (w)≥O (1)
B p
−
(∼1)(N +w+2) ∥C−1∥2(cid:13) (cid:13) (cid:13)g+g w+2g 0(cid:13) (cid:13) (cid:13)2
4 (cid:13) N +w+2 (cid:13)
+
d
log
(N +w+2)2
+
w+1 (cid:13) (cid:13) (cid:13)g w+g 0(cid:13) (cid:13) (cid:13)2
+
(N +1) (cid:13) (cid:13) (cid:13)g+g 0(cid:13) (cid:13) (cid:13)2
4 (N +1)(w+1) 4max{βL2,L2}(cid:13) w+1 (cid:13) 4max{L2,L2}(cid:13)N +1(cid:13)
0 0
≥O p(1)+
d
4log
(N(N ++ 1w )(w+ +2) 12
)
+
w+
4
1(cid:13) (cid:13)
(cid:13)
(cid:13)g ww+ +g 10(cid:13) (cid:13)
(cid:13)
(cid:13)2(cid:18) max{β1
L2,L2}
−
2∥C N− +1∥ w2(w ++
2
1)(cid:19)
0
≥O p(1)+
d
4log
(N(N ++ 1w )(w+ +2) 12
)
+
8maxw {β+ L1 2,L2}(cid:13) (cid:13)
(cid:13)
(cid:13)g ww+ +g 10(cid:13) (cid:13)
(cid:13)
(cid:13)2
.
0
√
Boundingthelasttermbelowby0andminimizingoverwsuchthatw≤ N yields
d √ d
G (w)≥O (1)+ log N =O (1)+ logN.
B p 4 p 8
√
Bounding(N +w+2)/(N +1)≥1andminimizingoverwsuchthatw≥ N yields
G B(w)≥O p(1)+
d
4logN −
d
4log(w+1)+
8maxw {β+ L1 2,L2}(cid:13) (cid:13)
(cid:13)
(cid:13)g ww+ +g 10(cid:13) (cid:13)
(cid:13)
(cid:13)2
0
16≥O p(1)+
d 4logN(cid:13) (cid:13)
(cid:13)
(cid:13)g ww+ +g 10(cid:13) (cid:13)
(cid:13)
(cid:13)2
d (cid:13)g (cid:13)2
=O (1)+ logN(cid:13) w(cid:13) ,
p 4 (cid:13)w (cid:13)
wherethesecondlinefollowsbecausefora,b>0andx≥0,thefunctionax−blogxisconvexinxwith
minimumatx⋆ =b/a.Thereforeforw+1≤N/(...),
(cid:18) (cid:26)(cid:13)g (cid:13)2 (cid:27)(cid:19)
KL(w)≥O (1)+Ω (1)dlog Nmin (cid:13) w(cid:13) ,1 .
p p (cid:13)w (cid:13)
Nextsupposew+1≥N/(4∥C−1∥2max{βL2,L2}).AsecondupperboundonEq.(11)canbeobtainedby
0
takingthemaximumoftheintegrandovertheintegrationregion∥x∥2 ≤r2.Notethatsince∥g /w∥=ω (r),
w p
w=Ω (N),g/N =O (N−1/2),andNr2 =ω (1),wehavethat∥(g+g +2g )/(N+w+2)∥=ω (r),
p p p w 0 p
andso
(cid:90)
log π
0e21(...)
B
≤O p(1)+ (∼1)(N 4+w+2)(cid:13) (cid:13) (cid:13) (cid:13)C−1 N(g ++ wg w ++ 22g 0)(cid:13) (cid:13) (cid:13) (cid:13)2 − (∼1)(N 4+w+2)(cid:18)(cid:13) (cid:13) (cid:13) (cid:13)C−1 N(g ++ wg w ++ 22g 0)(cid:13) (cid:13) (cid:13) (cid:13)−r(cid:19)2 + d 2logr2
=O p(1)− (∼1)(N 4+w+2) r2+ (∼1)(N +
2
w+2)r(cid:13) (cid:13) (cid:13) (cid:13)C−1 N(g ++ wg w ++ 22g 0)(cid:13) (cid:13) (cid:13) (cid:13)+ d 2logr2.
Sothereforecombiningthisresultwiththepreviousboundsandminimizingoverwyields
G B(w)≥O p(1)+ (∼1)(N 4+w+2) r2− (∼1)(N +
2
w+2)r(cid:13) (cid:13) (cid:13) (cid:13)C−1 N(g ++ wg w ++ 22g 0)(cid:13) (cid:13) (cid:13)
(cid:13)
− d log((N +1)(w+1)r4)+ w+1 (cid:13) (cid:13) (cid:13)g w+g 0(cid:13) (cid:13) (cid:13)2 + (N +1) (cid:13) (cid:13) (cid:13)g+g 0(cid:13) (cid:13) (cid:13)2
4 4max{βL2,L2}(cid:13) w+1 (cid:13) 4max{L2,L2}(cid:13)N +1(cid:13)
0 0
d (∼1)N(cid:16)(cid:13) g (cid:13) (cid:17)2 d (∼1)w(cid:16)(cid:13)g (cid:13) (cid:17)2
≥O (1)− log(Nr2)+ (cid:13) (cid:13)−r − log(wr2)+ (cid:13) w(cid:13)−r
p 4 4 (cid:13)N(cid:13) 4 4 (cid:13)w (cid:13)
d (∼1) d d (cid:13)g (cid:13)2
≥O (1)− log(Nr2)+ Nr2− log(r2)+ log(cid:13) w(cid:13)
p 4 4 4 4 (cid:13)w (cid:13)
d (cid:13)g (cid:13)2
≥O (1)+ logN(cid:13) w(cid:13) .
p 4 (cid:13)w (cid:13)
CombiningwiththeearlierboundandnotingthatNmin{∥g /w∥,1}=ω (1)yieldsthefinalresult.
w p
ProofofCorollary3.6. TheprooffollowsdirectlyfromTheorems3.3and3.5bythedataprocessinginequality
appliedtoKL(w).
ProofofTheorem5.3. ByLemma5.1,
KL(w)≤ inf 1 log(cid:90) πexp(cid:0) (1+λ)(ℓ¯ −ℓ¯)(cid:1)
λ>0λ w
= inf 1 log(cid:90) πexp(cid:0) ℓ¯ (cid:1) .
λ>0λ (1+λ)(w−1)
Since(ℓ )N are(f,A)-subexponential,if
n n=1
(1+λ)2(w−1)TA(w−1)≤1,
then
(cid:90) πexp(cid:0) ℓ¯ (cid:1) ≤exp(cid:16) f((1+λ)2(w−1)TA(w−1))(cid:17) .
(1+λ)(w−1)
Byassumption,theconditionholdswhenλ=1;theresultfollows.
ProofofProposition5.4. LetC(w)=log(cid:82) πexp(ℓ¯ ).Bythefinitenesscondition,[52,Theorem2.4]asserts
w
that C(w) is continuous, and has derivatives of all orders that can be obtained by passing differentiation
through the integral within the set ∥w∥ < α. Let U = Cov ((ℓ )N ), and S = span{w ∈ RN :
2 π n n=1
wT(ℓ¯ )N =0 π-a.s.}. NotethatS =kerU:sincewTUw=Var (wT(ℓ )N ),wTUw=0ifandonly
n n=1 π n n=1
ifwT(ℓ¯ )N =0π-a.s.;andsinceU issymmetricpositivesemidefinite,wTUw=0ifandonlyifw∈kerU.
n n=1
ThereforeC(w)iscontinuous,hasderivativesofallorders,andderivativescanbepassedthroughtheintegral
17withintheset{w ∈ RN : w = v+u,∥v∥ < α/2,u ∈ kerU}. Foravectorw = v+u, v ⊥ kerU,
2
u∈kerU,andminimumpositiveeigenvalueλ ofU,
+
α2λ α2λ α
wTUw≤ + =⇒ vTUv≤ + =⇒ ∥v∥ ≤ ,
4 4 2 2
andsoC(w)iscontinuous,hasderivativesofallorders,andderivativescanbepassedthroughtheintegralwithin
theset{w∈RN :wTUw≤ α2λ+}.ByTaylor’stheorem,foranywinthisset,thereexistsadistributionν
4 w
withdensityproportionaltoπexp(ℓ¯ w′)forsomew′onthesegmentfromtheorigintowsuchthat
(cid:90) 1 1 (cid:104) (cid:105)
C(w)=log πexp(ℓ¯ )= wTUw+ E (wT(ℓ¯ )N )3 .
w 2 6 νw n n=1
Bydefinitionofν ,w ∈kerU impliesthatwT(ℓ¯ )N =0ν -a.s.andhence 1E (cid:2) (wT(ℓ¯ )N )3(cid:3) =0.
w n n=1 w 6 νw n n=1
Therefore,forwTUw≤ α2λ+,
4
 
C(w)≤
1 wTUw
1+ max
1E νw(cid:2) (wT(ℓ¯ n)N n=1)3(cid:3) 

2  wTUw≤α2λ+ 6 wTUw 
4
w⊥kerU
≤
1 wTUw(cid:32)
1+ max
1∥w∥ 2(cid:13) (cid:13)E νw(cid:2) (ℓ¯ n)N n=1⊗(ℓ¯ n)N n=1⊗(ℓ¯ n)N n=1(cid:3)(cid:13) (cid:13) 2(cid:33)
2 ∥w∥2≤α 2 6 λ +
1 (cid:18) α (cid:13) (cid:104) (cid:105)(cid:13)(cid:19)
≤ wTUw 1+ max (cid:13)E (ℓ¯ )N ⊗(ℓ¯ )N ⊗(ℓ¯ )N (cid:13) ,
2 12λ + ∥w∥2≤α 2(cid:13) νw n n=1 n n=1 n n=1 (cid:13)
where⊗denotesouterproductstoformatensor.Bycontinuityofderivativesofallorderswithintheneighbour-
hood∥w∥ <α,theresultfollowsbyselectingasufficientlysmallα.
2
PropositionA.1. Supposethereexistc ∈ R,α,δ > 0,and0 < ϵ < 1suchthatℓ ≤ candforallcoreset
weights w satisfying αwT Cov ((ℓ )N )w ≤ 1, |ℓ¯ | ≤ ϵ|ℓ−c|+δ. Then the potentials (ℓ )N are
π n n=1 w n n=1
(f,αCov ((ℓ )N ))-subexponentialwithf(x)= 1x+ eδ+cϵ x1−ϵ.
π n n=1 2 (cid:82)π0eϵℓ
ProofofPropositionA.1. Letℓ′ =ℓ−c.Sinceℓ′ ≤0and|ℓ¯ |≤ϵ|ℓ′|+δforsomeϵ<1,δ>0,
w
(cid:90)
πexp(ℓ¯ )=1+
1(cid:90)
π(ℓ¯
)2+(cid:90) π(cid:88)∞ 1
(ℓ¯ )k−2(1−ϵ)(ℓ¯ )2(1−ϵ)
w 2 w k! w w
k=3
≤1+
1(cid:90)
π(ℓ¯
)2+(cid:90) π(cid:88)∞ 1
(ϵ|ℓ′|+δ)k−2(1−ϵ)|ℓ¯ |2(1−ϵ)
2 w k! w
k=3
1(cid:90) (cid:90) (cid:32) eϵ|ℓ′|+δ−1−(ϵ|ℓ′|+δ)− 1(ϵ|ℓ′|+δ)2(cid:33)
=1+ π(ℓ¯ )2+ π 2 |ℓ¯ |2(1−ϵ)
2 w (ϵ|ℓ′|+δ)2(1−ϵ) w
≤1+
1(cid:90)
π(ℓ¯
)2+(cid:90)
πeϵ|ℓ′|+δ|ℓ¯ |2(1−ϵ)
2 w w
=1+
1 2(cid:90)
π(ℓ¯ w)2+
(cid:82) π 0e(1− (cid:82)ϵ)ℓ π′+ eδ ℓ| ′ℓ¯ w|2(1−ϵ)
0
1(cid:90)
(cid:16)(cid:82)
π 0eℓ′ |ℓ¯
w|2(cid:17)1−ϵ
≤1+ 2 π(ℓ¯ w)2+eδ (cid:82) π eℓ′
0
=1+
1(cid:90)
π(ℓ¯
)2+eδ(cid:18)(cid:90)
π
eℓ′(cid:19)−ϵ(cid:18)(cid:90)
π(ℓ¯
)2(cid:19)1−ϵ
2 w 0 w
1(cid:90) (cid:18)(cid:90) (cid:19)−ϵ(cid:18)(cid:90) (cid:19)1−ϵ
=1+ π(ℓ¯ )2+eδ+cϵ π eℓ π(ℓ¯ )2
2 w 0 w
(cid:16) (cid:17)
≤exp f(wT Cov (ℓ)w) ,
π
wheref(x)= 1x+ eδ+cϵ x1−ϵ.
2 (cid:82)π0eϵℓ
PropositionA.2. SupposeΘ = Rd,ℓ¯isG-stronglyconcave,andthereexistsL < G,α > 0,andθ ∈ Θ
0
suchthatforallcoresetweightswsatisfyingαwT Cov ((ℓ )N )w≤1,ℓ¯ isL-Lipschitzsmooth,andboth
π n n=1 w
∥∇ℓ (θ )∥andℓ¯ (θ )arebounded. Thenforany(L/G)<ϵ<1,thereexistsc∈R,δ >0suchthatthe
w 0 w 0
potentials(ℓ )N are(f,αCov ((ℓ )N ))-subexponentialwiththesamef asinPropositionA.1.
n n=1 π n n=1
18ProofofPropositionA.2. Sinceℓ¯isG-stronglyconcaveandℓ¯ isL-Lipschitzsmooth,wecanwrite
w
G
ℓ(θ)≤ℓ(θ )+∇ℓ(θ )T(θ−θ )− ∥θ−θ ∥2
0 0 0 2 0
=ℓ(θ 0)+ G 2(cid:13) (cid:13)G−1∇ℓ(θ 0)(cid:13) (cid:13)2 − G 2(cid:13) (cid:13)θ−θ 0−G−1∇ℓ(θ 0)(cid:13) (cid:13)2
L
|ℓ¯ (θ)|≤|ℓ¯ (θ )+∇ℓ (θ )T(θ−θ )|+ ∥θ−θ ∥2.
w w 0 w 0 0 2 0
Sosettingc=ℓ(θ 0)+ G 2(cid:13) (cid:13)G−1∇ℓ(θ 0)(cid:13) (cid:13)2impliesℓ−cisanonpositivefunctionasrequired.Then
ϵ L−ϵG
|ℓ¯ (θ)|−ϵ|ℓ(θ)−c|≤|ℓ¯ (θ )|+ ∥∇ℓ(θ )∥2+(∥∇ℓ ∥+ϵ∥∇ℓ(θ )∥)∥θ−θ ∥+ ∥θ−θ ∥2.
w w 0 2G 0 w 0 0 2 0
For0<a<G−L,settingϵ= L+a andthenmaximizingover∥θ−θ ∥yields
G 0
ϵ a
|ℓ¯ (θ)|−ϵ|ℓ(θ)−c|≤|ℓ¯ (θ )|+ ∥∇ℓ(θ )∥2+(∥∇ℓ ∥+ϵ∥∇ℓ(θ )∥)∥θ−θ ∥− ∥θ−θ ∥2
w w 0 2G 0 w 0 0 2 0
ϵ (∥∇ℓ ∥+ϵ∥∇ℓ(θ )∥)2
≤|ℓ¯ (θ )|+ ∥∇ℓ(θ )∥2+ w 0 .
w 0 2G 0 2a
Bytheboundednessofℓ¯ (θ )and∇ℓ (θ ),maximizingoverwyieldsavalueofδ<∞.
w 0 w 0
LemmaA.3. LetX ,X ,... bei.i.d.randomvariablesinRwithEX =0,anddefinetheresampledsum
1 2 n
N
S =(cid:88) M n X
N Mp n
n
n=1
where(M ,...,M )∼Multi(M,(p ,...,p )),withstrictlypositiveresamplingprobabilitiesp ,...,p
1 N 1 N 1 N
thatmaydependonX ,...,X andN.Ifthereexistsaδ>0suchthatasN →∞,
1 N
1 (cid:88) |X n|2+δ
=O (1),
1 (cid:88) X n2
=Ω (1), and M →∞,
N (Np )1+δ p N Np p
n n
n n
then
√ 1S − 1 (cid:80) X
M N N N n n →d N(0,1).
(cid:113)
1 (cid:80) Xn2
N n Npn
Proof. Wecanrewrite
M
S = 1 (cid:88) X Im
N M p
m=1
Im
whereI ∼iidCategorical(p ,...,p ).ConsiderS +B whereB isindependentofS ,B =±1with
m 1 N N N N N N
probability 1 ,andB =0otherwise.SoifwesetA =σ(X ,...,X ),[53,Cor.3]assertsthat
2(NM)1+δ N N 1 N
S +B −E[S |A ]
N N N N →d N(0,1) N →∞.
(cid:112)
(NM)−(1+δ)+Var[S |A ]
N N
aslongasforallN largeenough,
(cid:20) (cid:21)
1 X
Var Im|A <∞ a.s.,
M p N
Im
andasN →∞,
(NM)−(1+δ)+(cid:80)M E(cid:20)(cid:12) (cid:12) 1 XIm −E(cid:104) 1 XIm|A (cid:105)(cid:12) (cid:12)2+δ |A (cid:21)
m=1 (cid:12)M pIm M pIm N (cid:12) N
p
→0.
((NM)−(1+δ)+Var[S |A ])(2+δ)/2
N N
Notethattheconditionalmeanandvariancehavetheform
(cid:20) (cid:21)
E[S |A ]=E X Im|A =(cid:88) X
N N p N n
Im
n
(cid:20) (cid:21) (cid:32) (cid:33)2
Var[S |A ]= 1 Var X Im|A = 1 (cid:88) p X n −(cid:88) X ,
N N M p N M n p n
Im
n
n
n
19(cid:104) (cid:105)
whichimpliesthatVar 1 XIm|A < ∞a.s., sincep ,...,p arestrictlynonnegativeandEX = 0
M pIm N 1 N n
impliesX isfinitealmostsurely.Next,notethat
n
(NM)−(1+δ)+(cid:80)M E(cid:20)(cid:12) (cid:12) 1 XIm −E(cid:104) 1 XIm|A (cid:105)(cid:12) (cid:12)2+δ |A (cid:21)
m=1 (cid:12)M pIm M pIm N (cid:12) N
((NM)−(1+δ)+Var[S |A ])(2+δ)/2
N N
(NM)−(1+δ)+22+δ(cid:80)M m=1(cid:18) E(cid:20)(cid:12) (cid:12) (cid:12)M1 X pII mm(cid:12) (cid:12) (cid:12)2+δ |A N(cid:21) +(cid:12) (cid:12) M1 (cid:80) nX n(cid:12) (cid:12)2+δ(cid:19)
≤
((NM)−(1+δ)+Var[S |A ])(2+δ)/2
N N
=M−δ/2N−(3+2δ)+22+δ(cid:16) N1 (cid:80)
n
(| NX pn n| )2 1+ +δ
δ
+(cid:12) (cid:12) N1 (cid:80) nX n(cid:12) (cid:12)2+δ(cid:17)
.
(cid:16) M−δN−(3+δ)+ 1 (cid:80) Xn2 −(cid:0)1 (cid:80) X (cid:1)2(cid:17)(2+δ)/2
N n Npn N n n
Theaboveexpressionconvergesinprobabilityto0bythetechnicalassumptionsinthestatementoftheresultas
wellasthefactthat 1 (cid:80) X a→.s.0bythelawoflargenumbers. Onceagainbythetechnicalassumptions,
N n n
Var[S |A ]=Ω (N2/M),so
N N p
Var[S N|A N] →p
1
(NM)−(1+δ)+Var[S |A ]
N N
B N →p 0,
(NM)−(1+δ)+Var[S |A ]
N N
andhencebySlutsky’stheorem,
S −E[S |A ]
N N N →d N(0,1) N →∞.
(cid:112)
Var[S |A ]
N N
UsingSlutsky’stheoremagainwith 1 (cid:80) X →p 0andrearrangingyieldsthefinalresult.
N n n
LemmaA.4. SupposecoresetweightsaregeneratedusingtheimportanceweightedconstructioninAlgorithm1.
Letg=∇ℓ(η ),g =∇ℓ (η ),andH =−E(cid:2) ∇2ℓ (η )(cid:3) .IfconditionsA(1-6)inSection4hold,M =o(N),
0 w w 0 n 0
andM =ω(1),then
(cid:13) (cid:13) g (cid:13) (cid:13) =Θ (cid:16) N−1/2(cid:17) , (cid:13) (cid:13)g w(cid:13) (cid:13) =Θ (cid:16) M−1/2(cid:17) , w →p 1,
(cid:13)N(cid:13) 2 p (cid:13)w (cid:13) 2 p N
and
(cid:13) (cid:13) (cid:13) (cid:13)
sup (cid:13) (cid:13) (cid:13)− N1 ∇2ℓ(η)−H(cid:13) (cid:13)
(cid:13)
→p 0, sup (cid:13) (cid:13) (cid:13)− w1 ∇2ℓ w(η)−H(cid:13) (cid:13)
(cid:13)
→p 0.
∥η−η0∥2≤r 2 ∥η−η0∥2≤r 2
Proof. First,sincew=(cid:80) Mn ,Ew=N,and
n Mpn
(cid:34)(cid:32) (cid:33)2(cid:35)
E(cid:2) (w−N)2(cid:3) = N2 E (cid:88) M (cid:0) (Np )−1−1(cid:1)
M2 n n
n
 
= MN2 2(cid:88) ((Np n)−1−1)2EM n2+ (cid:88) ((Np n)−1−1)((Np n′)−1−1)E[M nM n′]
n n̸=n′
(cid:32) (cid:32) (cid:33)2(cid:33)
= N2 (cid:88) ((Np )−1−1)2p − (cid:88) (1/N −p )
M n n n
n n′
(cid:32) (cid:33)
= 1 (cid:88) p (p−1−N)2
M n n
n
1 (cid:16) (cid:17)
≤ max(p−1−N)2
M n n
N2
≤ O(1),
M
p
wherethelastlinefollowsbyassumptionA6.ThereforebyChebyshev’sinequalityandM →∞,w/N →1.
Sincethedataarei.i.d.,byconditionsA1andA2,thecentrallimittheoremholdsforthesumof∇ℓ (η )such
n 0
20√
thatg/ N convergesindistributiontoanormal,andhence(cid:13) (cid:13) Ng(cid:13) (cid:13)=Θ p(N−1/2).ByconditionsA1,A2,and
A6,LemmaA.3holdssuchthatforanyt∈Rd,
√ 1tTg − 1tTg
M N w N =Θ (1).
(cid:113) p
1 (cid:80) (tT∇ℓn(η0))2
N n Npn
SinceconditionA6assertsthatC >Np ≥c>0,thelawoflargenumbers,conditionA1,andM/N →0
n
implythat
√
M
tTg =Θ (1).
N w p
Summingoverabasisofvectorst ,...,t showsthat
1 d
√
M √ (cid:13)g (cid:13)
∥g ∥ =Θ(1) M(cid:13) w(cid:13) Θ (1).
N w 2 (cid:13)w (cid:13) 2 p
Thiscompletesthefirstthreeresults.Next,byconditionA3,forsufficientlylargeN suchthattheneighbourhood
containstheballofradiusraroundη ,
0
(cid:13) (cid:13)
sup (cid:13) (cid:13) (cid:13)N1 ∇2ℓ(η)− N1 ∇2ℓ(η 0)(cid:13) (cid:13)
(cid:13)
≤r N1 (cid:88) R(X n)
∥η−η0∥2≤r 2 n
(cid:13) (cid:13)
sup (cid:13) (cid:13) (cid:13)N1 ∇2ℓ w(η)− N1 ∇2ℓ w(η 0)(cid:13) (cid:13)
(cid:13)
≤r N1 (cid:88) w nR(X n),
∥η−η0∥2≤r 2 n
and
(cid:34) (cid:35) (cid:34) (cid:35)
1 (cid:88) 1 (cid:88)
E r R(X ) =E r w R(X ) =rE[R(X)]→0,
N n N n n
n n
sothatwehavethatboth
(cid:13) (cid:13) (cid:13) (cid:13)
sup (cid:13) (cid:13) (cid:13)N1 ∇2ℓ(η)− N1 ∇2ℓ(η 0)(cid:13) (cid:13)
(cid:13)
→p 0 and sup (cid:13) (cid:13) (cid:13)N1 ∇2ℓ w(η)− N1 ∇2ℓ w(η 0)(cid:13) (cid:13)
(cid:13)
→p 0
∥η−η0∥2≤r 2 ∥η−η0∥2≤r 2
by Markov’s inequality. Finally, by the bounded variance in A2, sampling probability bounds in A6, and
M →∞,thevariancesof 1∇2ℓ (η )and 1∇2ℓ(η )bothconvergeto0asN →∞,andsincebothofthese
quantitiesareunbiasedestiN matesw ofE0 (cid:2) ∇2ℓN
(η
)(cid:3) ,C0
hebyshev’sinequalityyieldsthedesiredconvergencein
n 0
probability.
Lemma A.5. Suppose (X )N are N i.i.d. random vectors in Rd. Fix M ∈ N, M < d and define
n n=1
X =(cid:2) X X ... X (cid:3) ∈Rd×M.Ifthereexistsδ>0suchthat
1 2 M
(cid:104) (cid:105)
E (1T(XTX)−11)M+δ <∞,
where1denotesavectorofall1entries,thenasN →∞,
 (cid:13) (cid:13)(cid:80)N
w X
(cid:13) (cid:13)2 
  wm ∈Rin N +(cid:13) (cid:13) (cid:13) (cid:80)n= N n1 =1wn nn(cid:13) (cid:13) (cid:13)   =ω p(cid:18) N−M M+ +δ/ δ2(cid:19) .
 s.t. (cid:88) 1[w >0]<M.
n
n
Proof. Foranyϵ>0,bytheunionboundoversubsetsof[N]ofsizeM,
(cid:32) (cid:33) (cid:32) N(cid:33) (cid:18) wTXTXw (cid:19)
P min ···≤ϵ ≤ P min ≤ϵ
w∈RN M w∈RM wT11Tw
+
(cid:32) (cid:33) (cid:18) (cid:19)
N
≤ P max min wTXTXw−λ(1Tw−1)≤ϵ
M λ w∈RM
(cid:32) N(cid:33) (cid:18) λ2 (cid:19)
= P maxλ− 1T(XTX)−11≤ϵ
M λ 4
(cid:32) (cid:33)
N (cid:16) (cid:17)
= P 1T(XTX)−11≥ϵ−1 .
M
21ByMarkov’sinequalityand(cid:0)N(cid:1) ≤(eN/M)M,
M
(cid:32) (cid:33) (cid:18) eN(cid:19)M (cid:104) (cid:105)
P min ···≤ϵ ≤ ϵM+δE (1T(XTX)−11)M+δ
w∈RN M
+
=(cid:32) eNϵM M+δ (cid:33)M E(cid:104) (1T(XTX)−11)M+δ(cid:105)
.
M
Settingϵ=N−M M+ +δ/ δ2
yields
P(cid:32)
min
···≤N−M M+ +δ/ δ2(cid:33) ≤(cid:32) eN− 2Mδ (cid:33)M E(cid:104) (1T(XTX)−11)M+δ(cid:105)
.
w∈RN M
+
Theright-handsideconvergesto0asN →∞,yieldingthestatedresult.
√
ProofofCorollary4.1andCorollary4.2. InTheorem3.3,setr=logM/ M,anduseAssumptions(A1-6)
to verify the conditions of Lemmas A.3 and A.4, which then yields conditions sufficient for the results in
Theorem3.3.WecompletetheproofbytransferringfromKL(w)ontheη-pushforwardmodeltoKL(w)onthe
originalmodelusingCorollary3.6.
ProofofCorollary4.3. By(A8),LemmaA.5holdssuchthat
(cid:13)
(cid:13)
(cid:13)g ww(cid:13)
(cid:13)
(cid:13)2
=ω
p(cid:18) N−M M+ +δ/ δ2(cid:19)
.
InTheorem3.5,set
r2 =N−M M+3 +δ δ/4 .
Therefore(cid:13) (cid:13)g ww(cid:13) (cid:13)=ω p(r)andNr2 =ω(1)asrequired.Assumptions(A1-5)aresufficientforAssumption3.2,
and(A7)issufficientforthetwoadditionalconditionsinTheorem3.5.ThusTheorem3.5holds,so
KL(w)=Ω
p(cid:18) log(cid:18) N(cid:13)
(cid:13)
(cid:13)g ww(cid:13)
(cid:13)
(cid:13)2(cid:19)(cid:19)
=Ω
p(cid:18) logN1−M M+ +δ/ δ2(cid:19)
=Ω p(logN).
WecompletetheproofbytransferringfromKL(w)ontheη-pushforwardmodeltoKL(w)ontheoriginal
modelusingCorollary3.6.
LemmaA.6. Fixvectorsu,u ,...,u inaseparableHilbertspacewithinnerproductdenoteda·bandnorm
1 N
denoted∥∥.Letv ,...,v bedrawnfrom{u ,...,u }withprobabilitiesp ,...,p eitherwithorwithout
1 M 1 N 1 N
replacement(ifwithoutreplacement,theprobabilitiesarerenormalizedaftereverydraw).Thenforallϵ≥0,
P min(cid:13) (cid:13)
(cid:13)
(cid:13)(cid:88)M
w mv
m−u(cid:13) (cid:13)
(cid:13)
(cid:13)2 >ϵM(cid:16)q(M 2,ϵ)(cid:17) +1∥u∥2 ≤e−(cid:16)1−lo 2g(2)(cid:17)
M,
w≥0(cid:13) (cid:13)
m=1
where
(cid:32) (cid:26)
v (u−x
)(cid:27)2 (cid:33)
q(M,ϵ)=P 1−max 0, M · M−1 ≤ϵ x = argmin ∥x−u∥2.
∥v ∥ ∥u−x ∥ M−1
M M−1 x∈cone{v1,...,vM−1}
Proof. Firstnotethatitsufficestoanalyzethecasewithreplacement,sincethiscaseprovidesanupperbound
on the case without replacement. To demonstrate this, we couple two probability spaces—one that draws
v ,...,v withreplacement,andonewithoutreplacement.First,drawanidenticalvectorv forbothcopies.
1 M 1
Oneachsubsequentiterationm>1,the“withreplacement”copyfirstdrawswhetherornotitselectsavector
thatwaspreviouslyselectedbythe“withoutreplacement”copy.Ifitdoes,itdrawsthatvectorindependently;
ifitdoesnot,itselectsthesamevectorasthe“withoutreplacement”copy. Inanycase,ateachiterationm,
thevectorsdrawnbythe“withreplacement”copyarealwaysasubsetofthevectorsdrawnbythe“without
replacement”copy,andhencetheminimumoverw≥0isgreaterforthatcopy.Itthereforesufficestoanalyze
thecasewithreplacement.
Toobtainanupperboundontheprobabilitywhensamplingwithreplacement,insteadofminimizingoverall
w≥0jointly,supposeweusethefollowingiterativealgorithm.Setx =0.Atthefirstiteration,wedrawv
0 1
andsettheweightw byoptimizingoverw ≥0:
1 1
(cid:32) (cid:26)
v ·u
(cid:27)2(cid:33)
min∥w v −u∥2 =∥u∥2 1−max 0, 1 .
w1>0 1 1 ∥v 1∥∥u∥
22Setx = w v ,andnotethat(u−x )·x = 0. Thenateachsubsequentiterationk,assumetheprevious
1 1 1 1 1
iterateisoptimizedoverallnonnegativeweights,andhencesatisfies(u−x )·x =0.Wedrawanother
k−1 k−1
vectorv ,andboundtheerorrofthenextiteratex byoptimizingoveronlytheweightw forthenewvector
k k k
v .Then
k
∥u−x ∥2 = min
(cid:13) (cid:13) (cid:13)(cid:88)k
w v
−u(cid:13) (cid:13) (cid:13)2
≤ min∥w v +x −u∥2
k (cid:13) m m (cid:13) k k k−1
w1,...,wk≥0(cid:13)
m=1
(cid:13) wk>0
(cid:32) (cid:26)
v ·(u−x )
(cid:27)2(cid:33)
=∥u−x ∥2 1−max 0, k k−1 .
k−1 ∥v ∥∥u−x ∥
k k−1
Therefore,
P min(cid:13) (cid:13)
(cid:13)
(cid:13)(cid:88)M
w mv
m−u(cid:13) (cid:13)
(cid:13)
(cid:13)2 ≤ϵK∥u∥2

w≥0(cid:13) (cid:13)
m=1
≥P(cid:0) inatleastKiterations,∥x −u∥2 ≤ϵ∥x −u∥2(cid:1)
k k−1
(cid:32) (cid:26)
v ·(u−x )
(cid:27)2 (cid:33)
≥P inatleastKiterations,1−max 0, k k−1 ≤ϵ
∥v ∥∥u−x ∥
k k−1
=
(cid:88)
P(cid:32)
k∈K ⇐⇒
1−max(cid:26)
0,
v k·(u−x k−1)
(cid:27)2 ≤ϵ(cid:33)
∥v ∥∥u−x ∥
k k−1
K⊆[M],|K|≥K
≥ (cid:88) qk(1−q)M−k
K⊆[M],|K|≥K
M (cid:32) (cid:33)
= (cid:88) M qk(1−q)M−k,
k
k=K
where
(cid:32) (cid:26)
v ·(u−x )
(cid:27)2 (cid:33)
q=P 1−max 0, M M−1 ≤ϵ
∥v ∥∥u−x ∥
M M−1
x = argmin ∥x−u∥2
M−1
x∈cone{v1,...,vM−1}
Soforall0≤K ≤M,
P min(cid:13) (cid:13)
(cid:13)
(cid:13)(cid:88)M
w mv
m−u(cid:13) (cid:13)
(cid:13)
(cid:13)2 >ϵK∥u∥2
≤Binom(M,K−1,q).
w≥0(cid:13) (cid:13)
m=1
UsingtheChernoffboundonthebinomialCDF,forallK−1≤Mq,
P min(cid:13) (cid:13)
(cid:13)
(cid:13)(cid:88)M
w mv
m−u(cid:13) (cid:13)
(cid:13)
(cid:13)2 >ϵK∥u∥2 ≤e−M(cid:32) K M−1logK M− q1+(1−K M−1)log1− 1K −M q−1(cid:33)
w≥0(cid:13) (cid:13)
m=1
=e−(K−1)logK M− q1−(M−(K−1))logM M− (( 1K −− q)1)
.
SubstitutingK−1=Mq/2yields
=eM((q/2)log2−(1−q/2)log1 (− 1−q/ q)2) ≤e−(cid:16)1−lo 2g(2)(cid:17) M.
ProofofCorollary6.1. SincethepotentialsareβCov ((ℓ )N )subexponential,Theorem5.3guaranteesthat
π n n=1
∀w∈RN : 4β(w−1)T Cov ((ℓ )N )(w−1)≤1, KL(w)≤4β(w−1)T Cov ((ℓ )N )(w−1).
+ π n n=1 π n n=1
We apply Lemma A.6 with vectors ℓ ,...,ℓ (in equivalence classes specified up to a additive constant)
1 N
and inner product between ℓ ,ℓ defined by Cov (ℓ ,ℓ ). In the notation of Lemma A.6, by assumption,
i j π i j
∥u∥2 =O (Nα)andq(M,ϵ)=ω (M−ρ).SubstitutingM =(logN)1/(1−ρ),wefindthat
p p
(cid:16) (cid:17)
P 4β(w−1)T Cov ((ℓ )N )(w−1)≥ϵ−ωp(logN)+αlogN →0.
π n n=1
CombiningthisresultwiththeKLboundaboveyieldsthefinalresult.
23