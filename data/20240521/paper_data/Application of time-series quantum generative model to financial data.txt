Application of time-series quantum generative model
to financial data
Shun Okumura1, Masayuki Ohzeki1,2,3,*, and Masaya Abe4
1GraduateSchoolofInformationSciences,TohokuUniversity,Miyagi980-8564,Japan
2DepartmentofPhysics,TokyoInstituteofTechnology,Tokyo,152-8551,Japan
3Sigma-iCo.,Ltd.,Tokyo,108-0075,Japan
4NomuraAssetManagementCo.,Ltd.,Tokyo,135-0061,Japan
*mohzeki@tohoku.ac.jp
ABSTRACT
DespiteproposingaquantumgenerativemodelfortimeseriesthatsuccessfullylearnscorrelatedserieswithmultipleBrownian
motions,themodelhasnotbeenadaptedandevaluatedforfinancialproblems. Inthisstudy,atime-seriesgenerativemodel
wasappliedasaquantumgenerativemodeltoactualfinancialdata. Futuredatafortwocorrelatedtimeseriesweregenerated
andcomparedwithclassicalmethodssuchaslongshort-termmemoryandvectorautoregression. Furthermore,numerical
experimentswereperformedtocompletemissingvalues. Basedontheresults,weevaluatedthepracticalapplicationsofthe
time-seriesquantumgenerationmodel. Itwasobservedthatfewerparametervalueswererequiredcomparedwiththeclassical
method. Inaddition,thequantumtime-seriesgenerationmodelwasfeasibleforbothstationaryandnonstationarydata. These
resultssuggestthatseveralparameterscanbeappliedtovarioustypesoftime-seriesdata.
Introduction
A sequence of data {x}T observed between timest =1 andt =T is referred to as time-series data. These data are
t t=1
availableinvariousfields,includingfinance. Animportanttopicinthisareaisanalyzingtime-seriesdata,suchasstockprices,
forinvestmentandriskmanagementpurposes. Manytime-seriesdatasetsbehavesimilarlytorandomwalks1. Consequently,
predicting future data from time-series data is difficult. However, with recent developments in computer science, many
methodshavebeenproposedtopredictfuturevaluesbysuccessfullycapturingthecorrelationsbetweeneachtimeseries. The
autoregressive(AR)modelisthemostfundamentalmodel. Aregressionanalysisisperformedusingdatafromaspecified
timeinterval. Next,predictionsaremadebasedontheregressionresults. Byiteratingthisprocess,predictionscanbemade
uptoacertainnumberofsteps. Recurrentneuralnetworks(RNNs)havealsobeenproposed2. RNNcanimproveprediction
accuracy via the nonlinear transformation of models, such as the AR model. However, RNNs are incapable of long-term
memorybecauseofgradientlossandotherproblems3. Therefore,longtime-seriesdatacannotbeusedfortraining. Many
modelshavebeenproposedthatsolvethisproblem;thelongshort-termmemory(LSTM)isatypicalmodel4. Inaddition,
significantdevelopmentshaveoccurredinmethodsaimedatlearningtheprobabilitydistributiongoverningthedata,called
generativemodels. Inthiscontext,generativemodelshavesuccessfullyforecasttime-seriesdata5.
Recently,researchhasbeenconductedonquantumcomputersbasedonquantummechanics. Quantumcomputersoften
performcalculationsfasterthanclassicalcomputers. Primefactorizationandinversematrixcalculationsaretypicalexamples6,7.
Therefore,quantumcomputerscancontributetothedevelopmentofmachinelearninginthefuture. However,thesecomputers
arestillunderdevelopment,andfewcomputationalresourcesareavailable. Consequently,large-scalecalculationsarehindered
owingtonoiseduringcomputation. Therefore,activeresearchhasbeenconductedonsmall-scalequantumcomputerscalled
intermediate-scalequantumdevices(NISQ)8.WeexpectthatNISQcanefficientlygenerateprobabilitydistributionsthatare
difficulttogenerateusingclassicalcomputers9. However,itisuncertainwhetherreal-worldvaluablealgorithmssuchasprime
factorizationcanbeacceleratedusingNISQ.
NISQ is often used in connection with classical computers because of its limited computational resources. Quantum
machinelearningusingthishybridapproachhasbeenpreviouslyproposed10,11. Severalstudieshavereportedthatquantum
machinelearningcanachievethesamepredictionaccuracywithfewerparametersthanthatrequiredformachinelearningon
aclassicalcomputer12. Approachesthatforecasttime-seriesdatausingquantummachinelearningalsoexist. Thequantum
generativemodelispromisingowingtoitsabilitytogenerateprobabilitydistributions,whichisdifficultinthecaseofNISQ.
Quantum generative models have been suggested to reduce the number of parameters compared to classical methods by
exploitingentanglement,apropertyuniquetoquantummechanics12. Inaddition, citecopulaproposedaquantumgenerative
modelofcopulasusedinfinance. Aquantumgenerativemodelfortimeserieshasalsobeenproposedthatsuccessfullylearns
4202
yaM
02
]hp-tnauq[
1v59711.5042:viXracorrelatedserieswithmultipleBrownianmotions14. However,thistime-seriesquantumgenerativemodelhasnotbeenadapted
andevaluatedforfinancialproblems. Inthisstudy,weadaptedthistime-seriesquantumgenerationmodeltocorrelatetime
seriesinfinance. Wediscusstheperformanceandpracticalityofitsuseinpredictingthefutureandcomplementingmissing
values.
Thispaperisdividedintofourparts. First, abackgroundisprovidedintheintroduction. Subsequently, thenecessary
knowledge,includingthetime-seriesquantumgenerativemodel,isintroduced. Thereafter,numericalexperimentsarepresented,
whereinthetime-seriesquantumgenerativemodelisusedtopredictthefutureandcomplementmissingvalues,andtheresults
arediscussed. Finally,wesummarisethestudyanddiscussfuturedevelopments.
Preliminaries
Quantumgenerativemodel
QuantumCircuitBornMachine
QuantumcircuitBornmachines(QCBM)arefundamentaltoquantumgenerativemodels15. ThismodelfollowsBorn’s
rulesofquantummechanics. Theprobabilityofobservingaquantumstateislearned,andsamplingisgeneratedaccordingly.
Thequantumstate|ψ⟩isdefinedbasedonBorn’sruleasfollows:
|ψ⟩=c |00...0⟩+c |00...1⟩+···+c |11...1⟩ (1)
00...0 00...1 11...1
wherec ∈Cdenotesthecomplexprobabilityamplitude. Therefore, p :=|c |2istheprobabilityofobservingthequantum
bbb bbb bbb
state |bbb⟩ and satisfies ∑ bbbp bbb =1. In addition, {|00...0⟩,|00...1⟩,...,|11...1⟩} represents the computational basis. The
probability p ofobtainingaquantumstate|bbb⟩canbeexplicitlydescribedas
bbb
p =Tr[|ψ⟩⟨ψ|P ] (2)
bbb bbb
whereP =|bbb⟩⟨bbb|denotestheprojectionoperator. Basedonthisprobability,theclassicalbitsequencebbbisobtainedfromthe
bbb
measurement. Weparameterizedthequantumstate|ψ(θθθ)⟩byusingaquantumcomputertolearntheprobabilities. Thiscanbe
describedusingtheparameterisedunitaryoperatorU(θθθ)andtheinitialstate|ψ ⟩asfollows:
0
|ψ(θθθ)⟩=U(θθθ)|ψ ⟩. (3)
0
IftheparameterizedunitaryoperatorU(θθθ)satisfiescertainconditions,optimizationmaybecomedifficult16. Therefore,caution
shouldbeexercisedwhendesigningU(θθθ). Parameterizedprobabilities p (θθθ)canbeestimatedbymeasuringtheparameterized
bbb
quantumcircuitandsamplingbitstrings. ThisprobabilityisusedforlearningbycalculatingandminimizingtheKLdivergence
withthetargetprobabilityq . Thederivativeforoptimizingtheparameterscanbeeasilycomputedusingtheparametershift
bbb
rule10.
Time-seriesquantumgenerativemodel
Severalmodelshavebeenproposedfortime-seriesanalysisinquantummachinelearning. Thetime-seriesgeneration
modelproposedin14 wasusedinthisstudy. Openquantumsystemsinspiredthismodel,andstochastictransitionmatrices
werelearned. Itsstructurefacilitatestheencodingofthetimeseriesinaquantumcircuit,thusprovidingexcellentexplanatory
propertiesforthemodel. Weconsideredadiscretestates∈S={1,2,...,m}andassumedtransitionstoelementswithinSwith
acertainprobability. Thestochastictransitionmatrixrevealedtheprobabilitydistributionofagiveninitialstates ∈Safterk
0
steps. Usingthestateprobabilitydistributioncorrespondingtotheinitialstates andthestochastictransitionmatrixT,the
0
stateprobabilitydistributionafterkstepsisexpressedasfollows:
 (k)  (0)
p p
1 1
p(k) p(0)


2
.
 =Tk

2
.

 (4)
 .   . 
. .
   
(k) (0)
p p
m m
(l)
where p istheprobabilityofobtainingstateiafterl steps. ThisrelationshipwasappliedtoQCBM.Usingtheparameterized
i
unitaryoperator,thequantumstateobtainedafterkstepsfromtheinitialquantumstate|ψ ⟩is
0
|ψ(θθθ)⟩=Uk(θθθ)|ψ ⟩. (5)
0
2/12whereUk(θθθ)correspondstokiterationsofthequantumgate. Thus,thequantumcircuitlengthenedwitheachincreaseinthe
numberofsteps. ThisproblemisundesirableinNISQ,wherenoiseispresent. ThesolutionleveragesthattheunitarymatrixU
canbediagonalizedbytheunitarymatrixV.
Uk(θθθ)=V(φφφ)Σ(γγγ)kV†(φφφ)=V(φφφ)Σ(kγγγ)V†(φφφ) (6)
whereΣisadiagonalmatrix,andγγγ,φφφ aretheparameters. Inaddition,weaddedauxiliaryqubits|ψ ⟩totheinitialquantum
ancilla
state|ψ ⟩:
0
|ψ(θθθ)⟩=Uk(θθθ)(|ψ ⟩⊗|ψ ⟩). (7)
0 ancilla
Thisoperationcreatedacorrelationbetweenthetargetandenvironmentalsystems. Themeasurementswereperformedonly
onthetargetsystem. Theseprocessesweresimilartothoseofopenquantumsystemsandimprovedtheexpressivepower
ofquantummachinelearningmodels. Theadditionofancillaryqubitsissometimesdescribedasahiddenlayerinclassical
machinelearning.
Numericalexperiment
Setting
We applied the time-series quantum generative model to financial time-series data. In particular, it generated data for
forecastingandmissingvaluecompletion. Weevaluatedthepotentialutilityofthetime-seriesquantumgenerativemodelbased
onnumericalexperiments.
WeusedGOOGLEandIBMstockclosingdatafor2016–2020. Figure1showsthestockpricesofGOOGLEandIBM.
ThesedatawereobtainedfromYahooFinance17. Becausethesetimeseriesarenonstationary,theyweretransformedinto
stationarytimeseriesforfutureforecasts. Forthispurpose,weusedthelogarithmicdifferencer definedas
t
r =logx −logx . (8)
t t t−1
Theconversionofstockpricesintologarithmicdifferencesyieldsastationarytimeseries.Bycontrast,missingvaluecompletion
doesnotuselogarithmicdifferences,remainsnonstationary,andusesdata.Thisisbecausemanyclassicalmethodsareunsuitable
fornonstationarydata. Moreover,thisstudywasfocusedonverifyingtheperformanceoftime-seriesgenerativemodelson
suchdata.
GOOGLEandIBMstockpricesweredividedintofivedatasetsbyyear. Thedatasetfortheyear20yywasdenotedas
D20yy. Inthecaseoffutureprediction,D20yywasusedfortrainingtopredictthe10stepsnotincludedinthedataset. The
predictionaccuracywascomparedusingclassicalmethodslikevectorautoregression(VAR)andlongshort-termmemory
(LSTM).ThenumberoflagsinVARwassetto50orfewer,accordingtoAIC.LSTMhasfourlayersandusesAdamwitha
learningrate0.01forparameteroptimization. TheaccuracyofthesepredictionswasevaluatedintermsofMSEbyperforming
theinversetransformationofEq. (8).
Inmissingvaluecompletion,thedatafromthe50th–59thstepsofD20yyweremissing,andthesedatawerecompletedusing
thequantumtime-seriesgenerativemodel. Thedatamustbediscretizedfortrainingusingatime-seriesquantumgenerative
model. Forthispurpose,25%ofthedatainD20yywasassignedadiscretevalues={0,1,2,3}foreachintervalanddiscretized
to22. Thequantumtime-seriesgenerationmodelwasexecutedonasimulator. WeusedastandardPennyLanesimulator18.
The initial state |ψ ⟩ was encoded using X gates. StronglyEntanglingLayers in PennyLane was used as a parameterized
0
unitaryoperatorU(θθθ)19. TheparameterizedoperatorU(θθθ)comprisesasinglequbitrotationandanentanglementlayer. The
parameterizedunitaryoperatorcanincreasethenumberofparameterswhileimprovingexpressiveness. Here,fourqubitswere
usedasauxiliaryqubits;thus,thequantumcircuithadeightqubits. WeusedAdamwithalearningrateof0.1forparameter
optimization. Figure2showsthequantumcircuitusedinthenumericalexperiments,andFigure3showsthequantumcircuit
inStronglyEntanglingLayers. Figure4showsthequantumcircuitthatformedthediagonalmatrixΣusedinthenumerical
experiments.
Results
First,theresultsforfuturedatagenerationarediscussed. Figure5showsthetraininglossforLSTMandthetime-series
quantum generative model. We confirmed early convergence in both the models and datasets. The number of learning
stepsrequiredwasthesameforbothmodels. However,thelearningrateofAdamwasdifferent. Table1liststheresultsin
termsofMSEforthevaluespredictedbythetime-seriesquantumgenerativemodel,LSTM,andVARovertensteps. The
inversetransformationofthediscretizationrestoredthepredictivedatageneratedbythetime-seriesquantumgenerativemodel.
Therefore,thereconstructedvaluesweredependentonthetrainingdatastatistics. Thelogarithmicdifferencepredictedbyeach
modelwastransformedintotime-seriesdatausinganinversetransformation.
3/12(a)
180
GOOG
160 IBM
140
120
100
80
60
40
2016 2017 2018 2019 2020 2021
Date
(b)
GOOG
0.10
IBM
0.05
0.00
0.05
0.10
0 200 400 600 800 1000 1200
Steps
Figure1. (a)GOOGandIBMstockpricesfor2016–2020(Close). GOOGexhibitsanincreasingtrend,whereasIBM
exhibitsadecreasingtrend. (b)Logarithmicdifferenceof(a. Thelogarithmicdifferencemakesitastationarytimeseries.
4/12
ecnereffid
goL
)esolc(
ecirp
kcotSFigure2. Quantumcircuitsfornumericalexperiments. ThediscretizedstatesofGOOGandIBMareencodedasbitstringsas
bbb =b(1) b(1) andbbb =b(2) b(2) ,respectively. Consideringthenumberofksteps,thestatedistributionafterkstepscan
GOOG 1 2 IBM 1 2
beobtainedthroughmeasurement.
Figure3. QuantumcircuitdiagramofV(θθθ)forfourqubits. ThisquantumcircuitiscalledStronglyEntanglingLayersin
PennyLane. ItcomprisesasinglequbitrotationandanentanglementlayercomprisedofCNOT.
Figure4. QuantumcircuitdiagramofΣ(γγγ)forfourqubits. ItcomprisesonlyZrotation. Thisunitaryoperatorisadiagonal
matrix.
5/12Figure5. Variationoflossat300steps. (a)and(b)showthetime-seriesquantumgenerativemodelandthelossofLSTM.The
time-seriesquantumgenerativemodelconvergesfasterthanLSTM.
6/12(a) (b)
Dataset t-QGEN LSTM VAR Dataset t-QGEN LSTM VAR
D2016 2.57(0.467) 1.973(1.659) 111...666444444 D2016 222...555999111(32.508) 3.239(2.571) 3.594
D2017 7.106(1.885) 666...222222888(4.735) 8.159 D2017 86.932(38.354) 70.307(65.664) 666888...777333888
D2018 3.266(1.078) 3.089(1.963) 111...555777222 D2018 333666...666333666(3.345) 39.247(23.577) 38.045
D2019 13.478(1.741) 16.083(12.636) 111000...777555666 D2019 10.65(4.379) 10.628(6.457) 222...000999
D2020 3.223(35.884) 1.721(2.018) 111...555999777 D2020 13.747(22.037) 777...999333777(3.93) 14.403
Table1. Mean(standarddeviation)offuturedatagenerationresults. (a)and(b)showthepredictionsofGOOGLEandIBM,
respectively. t-QGEisatime-seriesquantumgenerativemodel. Theboldvaluesdenotethebestresultinthecorresponding
dataset. Inmanycases,VARprovidedthebestoutcome.
Inmostcases,VARexhibitedthebestpredictionaccuracybecausethelogarithmicdifferencingresultswereascloseas
possibletothestationarytimeseries,andthelagnumberswereappropriatelytuned. Suchresultshavebeenobservedtoappear
inspecifictime-seriesforecastingdatasetsusingtheclassicalgenerativemodel12. Thetime-seriesgenerativemodelandLSTM
canmodifythenumberofhiddenlayers,improvingaccuracy. However,consideringtheoverallresults,wecanassumethatthe
quantumgenerativemodelcangeneratedatawiththesamelevelofaccuracyasthatofLSTM.Thus,thequantumtime-series
generationmodelrequiresfewerparametersthantheclassicalmethods. Theresultsgeneratedbythequantumtime-series
generationmodelwerediscretized. Asanevaluationfunction,weusedtheManhattandistanceD ,whichisdefinedasfollows:
1
1 10
D = ∑|x −y| (9)
1 i i
10
i=1
wherex,y ∈{0,1,2,3}arethetrueandpredictedvaluesoftheithstep,respectively. Table2,Figure3,andTable3presentthe
i i
ManhattandistanceforeachdatasetandtheincreaseintheManhattandistanceateachstep. Formostdatasets,theManhattan
Dataset GOOG IBM
D2016 14.6(4.72) 14.8(1.72)
D2017 16.8(2.79) 11.0(1.41)
D2018 10.6(1.85) 18.2(2.4)
D2019 15.2(2.32) 11.4(3.01)
D2020 13.8(2.32) 13.8(2.32)
Table2. Mean(standarddeviation)ofpredictionaccuracyoftime-seriesquantumgenerativemodelsbyManhattandistance
D . AstheManhattandistanceissmallerthan15inmostcases,significantpredictionfailuresareconsideredtoberare.
1
Dataset GOOG IBM
D2016 1.43(0.99) 1.44(0.993)
D2017 1.77(0.995) 1.18(0.997)
D2018 0.92(0.987) 1.79(0.993)
D2019 1.49(0.997) 1.15(0.998)
D2020 1.36(0.988) 1.49(0.992)
Table3. Slope(coefficientofdetermination)forcumulativeManhattandistance. Inallcases,itisalinearfunction. Therefore,
themagnitudeoftheerrorgeneratedateachstepisapproximatelythesame.
distancewaslessthan15. TheincreaseintheManhattandistancewaslinearandunlikelytoresultinsignificanterrorsateach
step. Theseresultssupporttheideathatthequantumtime-seriesgenerationmodelcancapturethetrendsofthecorrelatedtime
series. ThevonNeumannentropybetweenGOOGLEandtheothersystemsatt=1–t=5isshowninFigure. 7. Theresults
suggestthatentanglementaidsinthecaptureofthecorrelationbetweenGOOGLEandIBM.
Next,wediscusstheresultsofcompletingthemissingvalues. Numericalexperimentswereperformedonthetime-series
quantumgenerativemodelwithL=1andL=3. Weevaluatedtheresultsusingdiscretevaluesbecauseinversetransformation
dependsonthetrainingdataandthediscretizationmethod. Table4,Figure8,andTable5presenttheaccuracyofcompletion
basedontheManhattandistanceandtheincreaseinManhattandistanceateachstep.
7/12Figure6. AccumulationofManhattandistanceateachstep. (a)and(b)showtheresultsforGOOGLEandIBM,respectively.
Bothofthesearelinearlyincreasing.
8/12Figure7. vonNeumannentropybetweenGOOGLEandtheothersystems. Statisticswereobtainedbysamplingfromall
statesatt=1˘5. Heremaxisthecaseofmaximumentanglement. Weconfirmedtheexistenceofentanglementinalldatasets.
(a) (b)
Dataset GOOG IBM Dataset GOOG IBM
D2016 10.6(2.5) 9.8(1.47) D2016 13.2(1.6) 11.4(2.58)
D2017 6.4(1.62) 15.0(5.22) D2017 6.4(1.02) 15.6(3.38)
D2018 16.0(3.1) 9.0(3.29) D2018 9.2(1.47) 12.2(3.19)
D2019 10.8(2.23) 7.4(2.42) D2019 9.2(3.06) 10.0(1.41)
D2020 11.2(5.27) 14.4(4.32) D2020 8.4(3.01) 14.0(2.83)
Table4. Mean(standarddeviation)ofCompletionofmissingvaluesresults. (a)and(b)showthecompletionresultswhenthe
layersofthequantumcircuitareL=1andL=3,respectively. Inbothcases,theManhattandistanceisapproximately10. In
addition,thestandarddeviationbecomessmallerasthenumberoflayersincreases.
(a) (b)
Dataset GOOG IBM Dataset GOOG IBM
D2016 0.92(0.979) 1.3(0.986) D2016 1.4(0.993) 1.25(0.988)
D2017 0.59(0.989) 1.67(0.982) D2017 0.7(0.994) 1.65(0.999)
D2018 0.9(0.994) 0.98(0.979) D2018 0.84(0.984) 0.98(0.979)
D2019 1.11(0.995) 0.23(0.975) D2019 0.9(0.994) 0.98(0.979)
D2020 1.13(0.993) 1.41(0.986) D2020 0.92(0.975) 1.37(0.996)
Table5. Slope(coefficientofdetermination)ofthecumulativeManhattandistanceforeachlayerofthequantumcircuitfor
L=1andL=3. Forallresults,alinearfunctionisobserved.
9/12Figure8. AccumulationofManhattandistanceateachstep. (a)and(b)showtheresultsforGOOGLE(L=1)and
GOOG(L=3),respectively. Bothofthesearelinearlyincreasing.
10/12WeconsideredappropriatecomplementationsuccessfulbecausetheManhattandistanceislessthan10forbothlayers,
L=1andL=3. Inaddition,asinfutureprediction,theincreaseinerrorateachstepwaslinear. Asthenumberoflayers
increased,thestandarddeviationdecreased,andtheextenttowhichaccuracyscatteredfromonetrainingtoanotherdecreased.
Thisresultindicatesthatevenwithonelayer,completionoftensucceeded,butthecertaintyincreasedwiththenumberoflayers.
Therefore,morelayersarepreferredtoobtainreliableresults.
Becausethedatausedherewerenonstationary,itwaschallengingtouseVARorLSTM.Therefore,thismaybeavaluable
casefortime-seriesquantumgenerativemodels.
Conclusion
Thisstudyadoptedatime-seriesquantumgenerativemodelforfinancialdata. Forfuturedatapredictions,wecompared
themwithtypicalclassicalmethods: LSTMandVAR.Consequently,weconfirmedthatthetime-seriesquantumgenerative
modelyieldedthesameaccuracyasLSTMwithfewerparameters. ByevaluatingtheManhattandistance,weconfirmedthatthe
time-seriesquantumgenerativemodelcouldcapturethetrendofthecorrelatedtimeseries. Theseresultsmaybeattributedto
thefunctioningofentanglement. ThecompletionofmissingvaluesusingtheManhattandistanceconfirmedtheeffectivenessof
thetime-seriesquantumgenerativemodel. Inaddition,weobtainedresultssuggestingthatthenumberoflayersinthequantum
circuitcouldbecontrolledbylearningtocontrolscattering.
Weconfirmedthatthetime-seriesquantumgenerativemodelappliedtostationaryandnonstationarydatathroughnumerical
experiments. Inparticular,becausenonstationarydataarenotwell-suitedtotypicalclassicalmethods,theabilitytogenerate
datawithasmallnumberofparametersmaybeadvantageous. However,thecoarsenessofthediscretizationmustbefine
tocomputetruepredictionsfromtheinversetransformofthepredicteddiscretevalues. Moreover,thisoperationshouldbe
performedcautiouslybecauseitincreasesthenumberofquantumbitsandcomputationalcomplexity. Theproposalofan
appropriatediscretizationandrestorationmethodtosolvesuchproblemsischallenging. Itwouldalsobeinterestingtoconsider
themodificationoflossfunctions. Inrecentyears,lossfunctionsbasedonoptimaltransporthavebeensuccessfullyapplied
inquantumgenerativemodels20. Byusingsuchlossfunctions,itmaybepossibletoreducethetrainingtimewithoutlosing
accuracybyfullyusingmini-batches.
References
1. Osborne,M.F.M.Brownianmotioninthestockmarket.Oper.Res.777(2),145(1959).
2. Werbos,P.J.Backpropagationthroughtime: whatitdoesandhowtodoit.Proc.IEEE777888(10),1550(1990).
3. Pascanu,T.,Mikolov,T.andBengio,Y.Onthedifficultyoftrainingrecurrentneuralnetworks.Proc.30thInt.Conf.Mach.
Learn.PMLR222888(3),1310(2013).
4. Hochreiter,S.andSchmidhuber,J.Longshort-termmemory.NeuralComput.999(8),1735(1997).
5. Yan, T., Zhang, H., Zhou, T., Zhan, Y.andXia, Y.ScoreGrad: Multivariateprobabilistictimeseriesforecastingwith
continuousenergy-basedgenerativemodels.arXiv:2106.10121.
6. Shor,P.W.Algorithmsforquantumcomputation: discretelogarithmsandfactoring.Proc.35thAnn.Symp.Found.Comp.
Sci.111222444(1994).
7. Harrow, A.W., Hassidim, A.andLloyd, S.Quantumalgorithmforlinearsystemsofequations.Phys.Rev.Lett.111000333,
150502(2009).
8. Preskill,J.QuantumcomputingintheNISQeraandbeyond.Quantum22279(2018).
9. Bremner,M.J.,Montanaro,A.andShepherd,D.J.Achievingquantumsupremacywithsparseandnoisycommuting
quantumcomputations.Quantum111,8(2017).
10. Mitarai,K.,Negoro,M.,Kitagawa,M.andFujii,K.Quantumcircuitlearning.Phys.Rev.A999888,032309(2018).
11. Schuld,M.,Bocharov,A.,Svore,K.M.andWiebe,N.Circuit-centricquantumclassifiers.Phys.Rev.A111000111,032308
(2020)
12. Coyle,B.,Henderson,M.,Le,J.C.J.,Kumar,N.,Paini,M.andKashefi,E.Quantumversusclassicalgenerativemodelling
infinance.arXiv:2008.00691.
13. Zhu,E.Y.,Johri,S.,Bacon,D.,Esencan,M.,Kim,J.,Muir,M.,Murgai,N.,Nguyen,J.,Pisenti,N.,Schouela,A.,Sosnova,
K.andWright,K.Generativequantumlearningofjointprobabilitydistributionfunctions.Phys.Rev.Res.444043092(2022).
14. Horowitz,H.,Rao,P.andRadha,S.K.Aquantumgenerativemodelformulti-dimensionaltimeseriesusingHamiltonian
learning.arXiv:2204.06150.
11/1215. ]LiuJ.andWang,L.differentiablelearningofquantumcircuitbornmachines.Phys.Rev.A999888,062324(2018).
16. McClean,J.R.,Boixo,S.,Smelyanskiy,V.N.,Babbush,R.andNeven,H.Barrenplateausinquantumneuralnetwork
traininglandscapes.Nat.Commun.999,p.4812(2018).
17. Yahoo,Yahoo! finance,https://finance.yahoo.com/.
18. Xanadu,PennyLane,https://pennylane.ai/.
19. Schuld,M.,Bocharov,A.,Svore,K.andWiebe,N.Circuit-centricquantumclassifiers.Phys.Rev.A111000111,032308(2020).
20. Tezuka,H.,Uno,S.andYamamoto,N.Generativemodelforlearningquantumensembleviaoptimaltransportloss.Quant.
Mach.Intell.666,6(2024).
Acknowledgements
This study was supported by JSPS KAKENHI Grant No. 23H01432. This study was financially supported by the
Public\PrivateR&DInvestmentStrategicExpansionPrograM(PRISM)andprogramsforbridgingthegapbetweenR&Dand
IDealsociety(Society5.0)andGeneratingEconomicandsocialvalue(BRIDGE)fromtheCabinetOffice.
Author contributions statement
S.O.conceivedandconductedtheexperimentsandanalyzedtheresults. M.A.andM.O.supervisedthestudy. Allthe
authorsreviewedthedraftmanuscriptandcriticallyreviseditforintellectualcontent. Allauthorshaveapprovedthefinal
versionofthemanuscriptforpublication.
12/12