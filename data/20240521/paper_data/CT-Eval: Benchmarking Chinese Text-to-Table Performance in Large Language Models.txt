CT-Eval: Benchmarking Chinese Text-to-Table Performance in Large
Language Models
HaoxiangShi1,JiaanWang,JiarongXu2,CenWang3 andTetsuyaSakai1
1 WasedaUniversity,Tokyo,Japan 2 FudanUniversity
3 KDDIResearchInc.,Japan
hollis.shi@toki.waseda.jp,tetsuya@waseda.jp,xce-wang@kddi.com,
Abstract Basic IE sub-tasks, such as, named entity recog-
nitionandentitylinking, generallyoperateatthe
Text-to-Table aims to generate structured ta- sentencelevel,whichignoremodels’abilitytoun-
bles to convey the key information from un- derstandthedocument-levelmeaning. Incontrast,
structured documents. Existing text-to-table
Text-to-Table,anemergingsub-taskofIE,requires
datasetsaretypicallyorientedEnglish,limiting
models to understand information within a given
theresearchinnon-Englishlanguages. Mean-
documentandthengeneratestructuredtables. De-
while, the emergence of large language mod-
spitegreatsuccessinthedomainWuetal.(2022);
els (LLMs) has shown great success as gen-
eraltasksolversinmulti-lingualsettings(e.g., Li et al. (2023), previous studies are oriented to
ChatGPT),theoreticallyenablingtext-to-table English,limitingtheresearchinotherlanguages.
inotherlanguages. Inthispaper,wepropose
Recently, large language models (LLMs) have
a Chinese text-to-table dataset, CT-Eval, to
demonstrated powerful performance across vari-
benchmarkLLMsonthistask.Ourpreliminary
ous NLP tasks (Zhao et al., 2023; Wang et al.,
analysisofEnglishtext-to-tabledatasetshigh-
2023a,b,c). Somestudies(González-Gallardoetal.,
lighttwokeyfactorsfordatasetconstruction:
datadiversityanddatahallucination. Inspired 2023;Gaoetal.,2023)havealsoutilizedLLMsfor
bythis,theCT-Evaldatasetselectsapopular several IE sub-tasks, and found that compared to
Chinesemultidisciplinaryonlineencyclopedia supervisedbaselines,theperformanceofLLMsis
asthesourceandcovers28domainstoensure sub-optimal. However,theirperformanceofLLMs
datadiversity. Tominimizedatahallucination,
onText-to-Tableremainslargelyunexplored. Addi-
wefirsttrainanLLMtojudgeandfilteroutthe
tionally,therapidadvancementsinLLMshavefa-
tasksampleswithhallucination,thenemploy
cilitatedtheirwidespreadadoptioninmulti-lingual
human annotators to clean the hallucinations
in the validation and testing sets. After this settings,enablinglanguagemodelingabilitiestobe
process,CT-Evalcontains88.6Ktasksamples. sharedacrossdifferentlanguages. Consequently,it
istheoreticallypossibletofeasibletoemployLLMs
UsingCT-Eval, weevaluatetheperformance
for text-to-table in other languages, an area that
ofopen-sourceandclosed-sourceLLMs. Our
resultsrevealthatzero-shotLLMs(including hasyettobethoroughlyinvestigated.
GPT-4)stillhaveasignificantperformancegap Motivatedbytheaforementionedconsiderations,
comparedwithhumanjudgment. Furthermore,
wedecidetobenchmarkLLMsonChinesetext-to-
afterfine-tuning,open-sourceLLMscansignif-
table. Aheuristicapproachinvolvestranslatingex-
icantlyimprovetheirtext-to-tableability,out-
istingEnglishdatasetsintoChineseforsubsequent
performingGPT-4byalargemargin. Inshort,
performance evaluation. There are four datasets
CT-Eval not only helps researchers evaluate
and quickly understand the Chinese text-to- widelyusedintext-to-table: E2E(Novikovaetal.,
tableabilityofexistingLLMsbutalsoserves 2017)isarestaurantdomaindatasetencompassing
asavaluableresourcetosignificantlyimprove 51.5K samples about restaurant information like
thetext-to-tableperformanceofLLMs.1
names,addresses,ratescores,etc. Rotowire(Wise-
manetal.,2017)isasportsdomaindatasetderived
1 Introduction
from NBA basketball games with 4.9K samples
about NBA team information. WikiBio (Lebret
Informationextraction(IE)aimstoidentifyandex-
etal.,2016),compiledfromWikipedia,containing
tractstructuredinformationfromunstructuredtext.
72.6Kdocumentsandcorrespondingstructuredta-
1Codesanddatawillbepubliclyavailableonceaccepted. blesinthebiographydomain. WikiTableText(Bao
4202
yaM
02
]LC.sc[
1v47121.5042:viXraet al., 2018) is also derived from Wikipedia, and bothzero-shotandfine-tunedLLMs,highlighting
involves13.3Kmultidisciplinarysamplesspanning achallengeinusingLLMsastext-to-tablesystems.
fields like finance and politics. However, accord- FutureworkcouldnotonlyevaluateLLMs’perfor-
ingtoourpreliminaryanalysis,thesedatasetsalso manceontext-to-tableviaourCT-Evalbenchmark
sufferfromtheissuesoflessdiversityorhighhal- dataset, but also leverage its training data to im-
lucination,makingthemunsuitabletobenchmark proveLLMs’text-to-tableabilityviafine-tuning.
LLMs. Specifically,(1)E2E,Rotowire,andWik-
2 RelatedWork
iBioexhibitalackofdiversityastheyfocusona
single domain, violating the core principle of in-
2.1 Text-to-tableTasks
struction tuning in LLMs, that is, diversity. (2)
AlthoughWikiTableTextincorporatesmultipledo- Wu et al. (2022) pioneer the text-to-table task.
mainsfordiversity, ouranalysis(§3.5)indicates Given the absence of a dedicated text-to-table
that 18.83% of samples exhibit hallucination in dataset,Wuetal.(2022)repurposeexistingtable-
thegoldentables,thatis,containingadditionalin-
to-textdatasets,i.e.,WikiBio(Lebretetal.,2016),
formation beyond the provided documents. This E2E (Novikova et al., 2017), Rotowire (Wise-
arisesfromtreatingWikipediainfoboxesasgolden man et al., 2017) and WikiTableText (Bao et al.,
tables,createdcollaborativelybyonlineusersand 2018), for text-to-table tasks by reversing their
potentiallycontainingadditionalbasicinformation. input-output pairs. They fine-tune BART (Lewis
etal.,2019)toperformtext-to-tableinasequence-
In this paper, we propose the Chinese Text-to-
to-sequence manner, and find that the fine-tuned
TableEvaluation(CT-Eval)dataset,whichiscon-
BART outperforms the pipeline baselines using
structedthroughthreestepstoensuredatadiversity
relation extraction and named entity extraction.
and minimize hallucination. To ensure diversity,
STable(Pietruszkaetal.,2022)employstwopre-
thefirststepinvolvescollectingmultidisciplinary
trainedlanguagemodels(PLMs)(T5(Raffeletal.,
document-tablepairs. WechoosetheBaiduBaike
2020)andTILT(Powalskietal.,2021))fortext-to-
asthedatasource,whichisapopularChinesemul-
table,anddesignsapermutation-baseddecoderto
tidisciplinary online encyclopedia. Each page in
enhancethePLMs’tablegenerationability. Subse-
thissourcecontainstextandaninfoboxsummariz-
quently,Lietal.(2023)findthatthepredefinedrow
ingthecorrespondingkeyinformation. Second,to
orderingoldentablesintroducesbiasintotext-to-
minimizedatahallucination,wetrainanLLM,asa
tablemodels. Consequently,theytraintableheader
hallucinationjudger,tofilterouttasksampleswith
and table body generators separately to produce
hallucinationintheirgoldentables(infoboxes). Fi-
final tables. While these studies achieve notable
nally, we obtain 88.6K samples with an average
success,theyprimarilyexploredtext-to-tableper-
lengthof911.46Chinesecharacters. Wesplitthem
formance before the LLM era. In addition, their
into86.6K,1Kand1Kfortraining,validationand
evaluationdatasetsgenerallyfocusonasingledo-
testing. Forvalidationandtestingsamples,human
main,andadaptfromthetable-to-textdatasets,re-
annotators further clean data hallucination in the
sultinginhallucinationissues. Thus,theyareun-
goldentablestoensureevaluationreliability.
suitableforbenchmarkingLLMsintext-to-table.
BasedontheproposedCT-Eval,webenchmark
2.2 LargeLanguageModels
various mainstream LLMs in both zero-shot (for
both open- and closed-source LLMs) and fine- TheadventofadvancedLLMs,e.g.,ChatGPT(Ope-
tuning(onlyforopen-sourceLLMs)scenarios. Our nAI,2022),GPT-4(OpenAI,2023),marksapivotal
experimentsrevealthat(1)GPT-4achievesthebest momentthatpropelsthefieldofNLPintoaboom
zero-shotperformanceamongallLLMs. However, phase. Zhong et al. (2023) show that LLMs can
its performance remains a discernible disparity achieve decent performance on benchmarks like
comparedtohumanjudgment. (2)Afterfine-tuning GLUE(Wangetal.,2018),whichspanseightrep-
on the training set of CT-Eval, all open-source resentativeNLPunderstandingtasks. Concurrently,
LLMsdemonstrateasignificantperformanceim- several studies have scrutinized the performance
provement, outperforming zero-shot GPT-4 by a of LLMs across various IE tasks. For example,
large margin, indicating the effectiveness of CT- Gao et al. (2023) test the capability of ChatGPT
Eval. In-depthanalysesofLLM-generatedtables ineventextraction. Similarly,González-Gallardo
reveal the persistence of hallucination issues in etal.(2023)employChatGPTonhistoricalentityrecognition. However,theresultsoftheseIEsub- § There is a document and a golden table that
summarize the information contained in this
tasksconsistentlyshowthatLLMsunderperform document.Pleasehelpmeidentifywhetherthe
golden table contains additional information
comparedtostate-of-the-artsupervisedapproaches.
beyondthedocument.Givethereasonfirstand
Thetext-to-tabletaskwefocusedonismorecom- thenprovideyourjudgment.
plexthanthepreviousbasicIEsub-tasks,however,
it remains an unexplored area for evaluation on Figure1: Illustrationofjudgmentprompt.
LLMs.
Tocontrolthehallucinationinthetasksamples,
3 CT-Eval
we decide to employ an LLM as a hallucination
judgertofilteroutsamplesexhibitinghallucination.
Inthissection,wefirstdiscussthedatasourcefor
WhileusingGPT-4directlyastheLLMhallucina-
buildingCT-Eval(§3.1). Then,wegivethedetails
tionjudgerisastraightforwardapproach,utilizing
ofhowtocontrolthehallucinationintheprelimi-
official APIs can be costly. Therefore, we first
narycollecteddata,includingLLMhallucination
randomlyselect5Ksamplesfromthedatabefore
judger(§3.2)andhumancleaningprocesses(§3.3).
cleaning. Then,weuseGPT-4toassesswhetherthe
Finally,weformulatethetext-to-tabletask(§3.4)
goldentablescontainadditionalinformation. The
andprovidethedetailsofdatastatistics(§3.5).
judgmentpromptisillustratedinFigure1,where
3.1 DataSource theLLMistaskedwithevaluatinghallucinationin
achain-of-thoughtmanner. Next,weusetheGPT-
FollowingthesuccessofWikiTableText(Baoetal.,
4 judgment results to train an open-source LLM,
2018), we also choose a multidisciplinary online
employingthetrainedmodeltoevaluatetheremain-
encyclopediaasthedatasourcetoensuredatadi-
ingsamples. Samplescontaininghallucinationsare
versity. AftercarefullycomparingexistingChinese
discarded. Given the capacity for understanding
online encyclopedias, we choose Baidu Baike2,
lengthydocumentsofexistingChineseLLMs(Bai
which is one of the Chinese encyclopedias with
et al., 2023b), we select ChatGLM3-6B-32k3 as
themostentriesintheworld.
theopen-sourceLLMhallucinationjudger. Finally,
WeobtaintheBaiduBaikedatafromthedumps
there are 88.6K samples after the data cleaning
provided by Xu et al. (2017). The data contains
bytheLLMhallucinationjudger. Thesesamples
over 9M entity pages, each of which includes an
totallycover28domains,e.g.,physicsandreligion.
infoboxandthecorrespondingtextualdescription,
formingatext-to-tablesample. Utilizingthisdata, 3.3 HumanDataCleaning
we implement the following rule-based strategy
We split the LLM-cleaned samples into the train-
forpreliminarydatacleaning: (1)Eachpagemust
ing, validation, and test sets with 84.6K, 1K and
containatleastoneinfobox;otherwise,thegolden
1Ksamples,respectively. Inthevalidationandtest
tableismissing. (2)Thenumberoftabularcellsin
sets, we balance the number of samples in each
theinfoboxshouldexceedthreetoensurevalidity.
domaintoensureacomprehensiveevaluation. Fur-
(3) The length of the textual description should
thermore,tomitigatethehallucinationissueinthe
surpass 200 tokens. After that, 200K document-
validationandtestsets,weemployhumanannota-
tablepairsareremainingforfurtherprocessing.
torstomanuallycleansetheirgoldentables.
Inthisphase,weemployfivehumanannotators
3.2 LLMDataCleaning
andonedataexpert,allofwhomarenativeChinese
After the initial cleaning process, the hallucina- speakerswithadvancededucationalqualifications.
tionissuepersistsduetoBaiduBaike’ssubmission Thedataexpertisaresearcherwithextensiveexpe-
rules,whereinmostofthepagetextandinfoboxes rienceinIEresearch. Wefirstorganizeatutorial
are edited and maintained by individuals. Thus, for the five annotators to ensure alignment of an-
thecontentsintheinfoboxesmaynotalwaysalign notationrequirements. Thisincludesclarifyingthe
preciselywiththetextualdocuments. Forinstance, conceptofthehallucinationissue,emphasizingthe
someinfoboxesmayincludeadditionalknowledge additionalinformationpresentintablesthatcannot
unrelated to the text, potentially misleading the bedirectlyextractedfromthecorrespondingdoc-
modelfromlearningthetext-to-tabletask. uments. Next,foreachdocument-tablepairinthe
2https://baike.baidu.com/ 3https://huggingface.co/THUDM/chatglm3-6b-32kIndustry
Informatics OriginalItem Translation
Physics
Astronomy “标题”: “东坡牛肉”, “”章节“: [{”标题“: ”东坡牛肉的做法“},
Traffic Engineering 内容“: ”葱段、姜片、蒜头、辣椒、香叶、大料、盐、白糖、番茄酱、排骨
L Sif ue p 1p 1.or 6t % 3O %.5. 24.0S %TE
M
B M C R H
G
E
Bdi u e ui eeo sl ul
o
sd i tl t icg o go ui nac i r rg r eo y
a
ta ey
i
sn pl
o
s
hS nycience 酱 将 东、 牛 坡辣 腩 牛椒 在 肉酱 冷、 水
材
牛
白高 中
料
肉
萝汤 多
，：
卜材（ 泡
胡
，料用 一
萝 大卜
料煮 会
，
，…牛 “肉 }的
1
中
成汤
.牛
煮
方即
肉
至
块可
洗
七）
净
成做。
放
熟法\
入
，n
清
捞做
水
出法
锅
切：\n1
炖
山
烂、
牛
楂
得先
肉
进
快来 厨
时
去
些处 师
，
，
，理 一
可
这
而一 点
以
样
且下 通
放
牛
有牛
几
肉
股肉
个
会
山，
CT-Eval F Min aa nn ac ge ement 香叶 2. … 楂的清香。
Movie and Book
Science Fiction
Music OriginalItem Translation
Soc 6i 0al .9S %cience A D
T Cr
an w
a
ri ve em l
e
ela
l
ri it n nio g gn “
I sn
uT
g
git
r
al
e
re
d
,”
i
t:
e on
m“ tD
s a：
to on
D
sg aip
c
uo
e cd
eB ,ge
rr
ie
e
bf e”
n
s, auo“
cn
eio” ,C
n c,
hh ia
g
lp iint se
g
ar
e
u“ r:
c,
eg[ ,{
a
b”
r
rlT
i
oci tt
,
hle
.ch
C“
i
ol:
i
okp” ieD
np
go pn
e
mg r,p eo
tc ho
orB diae :1ne ,df
e
firP
r,
sr ta
d
,c
a
lt
s
ei hc
ti
'e s,“ ds}
a
e,
l at
l:
,
Healthy
Food withthebeef,soakthebrisketincoldwaterforawhilelonger..."}
Cosmetics
Profile Ingredients Cooking method Cooking Tips
S Me ic lu itr ai rt yy
Ingredients:
1. W pua
t
s ih
t
it nh te
o
b ae pe of ta on fd W canh e pn
u
s
t
t ae w fei wng
h
b ae we tf h, oy ro nu
Figure2: DomaindistributioninCT-EVAL D Beo en fgpo B r sa pe d ie cisf e, h
s
c ,
,
a C ar lr h lso i pnts ie c, s ew e hite w i ft
i
sa i hste s or e ut vo
t
e anb
n
o dmi l ca u utun
t
rt iei tl , i w hn ait wlo l ti r ht o,
o
ts rfo na st fh t re aa r gt a rt ah n ne d
c
b h ee .ae vf e a
into squares…
2. …
validationandtestsets,threeannotatorsareasked
to remove the hallucination information from ta- Figure3: Text-to-tableexamplefromCT-EVAL.
bles. Thedataexpertreviews10%ofthemanually
cleanedsamplesfromeachannotator. Iftheaccu-
branchexhibitsthehighestproportion,encompass-
racyratefallsbelow95%,therespectiveannotator
ing domains such as culture and religion. Con-
will be required to redo the annotation. Finally,
versely,the“Other”branch,whichincludestopics
foreachsample,ifthethreemanuallycleanedre-
related to cosmetics and profiles, represents the
sultsareconsistent,theresultsaresavedasthefinal
smallestportion. The“STEM”(science,technol-
data. Otherwise,theresultsaredecidedbyagroup
ogy,engineering,andmathematics)branchfeatures
meetingamongallannotatorsandthedataexpert.
datarelatedtotopicssuchasphysicalsciencesand
3.4 TaskOverview astronomy. The “Life Support” branch is highly
GivenadocumentD = {w ,w ,...,w },where relevanttoeverydaylife,comprises11.60%ofthe
1 2 |D|
w isthei-thwordinD,thetext-to-tabletaskaims data, and primarily focuses on domains such as
i
toextractkeyinformationfromDandoutputsata- dwellingandhealth. Anillustrationofadatasam-
bleT = {c ,c ,...,c ,c ,c ,...,c ,..., ple from CT-Eval is depicted in Figure 3. Each
0,0 0,1 0,n 1,0 1,1 1,n
c ,...,c },wherec (k ∈ {0,1,...,n})repre- document-table pair, akin to the example shown,
i,j m,n 0,k
sentsthecolumnheaderandc (k ∈ {0,1,...,n}) consistsofatextualdescriptionandagoldentable
k,0
denotesastherowheader. c (i > 0,j > 0)rep- withvaryingnumbersofcolumnsandrows.
i,j
resentsthetextofthecellinthei-throwandj-th To assess the quality of our dataset compared
columninthetablewithmrowsandncolumns. topreviousones,werandomlyselect200samples
from the training, validation and test sets of CT-
3.5 DataStatistics
Eval, and previous E2E, Rotowire, WikiBio, and
We compare CT-Eval with the previous datasets WikiTableText,respectively. Then,wecomputethe
across several metrics including language, num- hallucinationrateforeachdatasetthroughhuman
berofentries,averagelength,numberofdomains, annotation. Specifically,threeannotatorsproficient
hallucinationrate,andaveragenumberofcells. in both English and Chinese judged whether the
As shown in Table 1, compared to previous goldentablescontainedhallucinationinformation
datasetsthatmostlyfocusononedomain,CT-Eval followingtheguidanceoutlinedinSection3.3. The
covers 28 domains, offering a valuable resource hallucination rate for each dataset is determined
toevaluatethetext-to-tablecapabilitiesofLLMs bytheaverageproportionofhallucinatedsamples
across multiple domains. These 28 domains can judged by all three annotators. We observe that
be categorized into four branches: STEM, social the hallucination rate of the CT-Eval training set
science,lifesupportandothers. Toprovideacom- is6.83%,similartoRotowire(6.17%). Moreover,
prehensive understanding of domain coverage in withthehelpofourhumancleaning,thehallucina-
CT-Eval, we present the distributions of each do- tionratesinourtestandvalidationsetsdecreaseto
main in Figure 2. Notably, the “Social Science” 1.00%and1.50%,respectively,significantlylowerDataset Language N.Entries Avg.Length N.Domains Hallu.R Avg.Cells
E2E English 42,061 90.58 Restaurant 4.17% 4.46
Rotowire English 3,398 1311.01 Sports 6.17% 40.49
WikiBio English 582,659 416.71 Biography 8.67% 4.19
WikiTableText English 10,000 59.76 MultipleSubclasses 18.83% 4.25
CT-Eval(train) Chinese 84,603 911.46 28Subclasses 6.83% 11.40
CT-Eval(val.) Chinese 1,000 813.32 28Subclasses 1.00% 10.78
CT-Eval(test) Chinese 1,000 845.22 28Subclasses 1.50% 10.86
Table1: DataStatisticsofCT-Evalandprevioustext-to-tabledatasets. “N.Entries”denotesthenumberofentries.
“Hallu. R”indicateshallucinationrate. “Avg.Cells”indicatestheaveragenumberoftablecells
than those in other datasets. Therefore, the reli-
§ Please extract key information from the data and
ability of our dataset can be verified. Regarding generate a tabletoconveythekeyinformation. Here
is an example of desired table: [example table]
domains,theE2E,RotowireandWikiBiodatasets
focusonsingledomains,whereasWikiTableText
Figure4: Illustrationoftext-to-tableprompt.
and CT-Eval encompass multiple domains to en-
suredatadiversity. Concerningthelengthofthein-
putdocuments,wenotethatpreviousE2EandWik-
lingual abilities in downstream tasks (Yang
iTableTextprimarilycontainshortdocumentswith
et al., 2023). (8) Llama-Chinese-2-7B and (9)
anaveragelengthofunder100words,whereasdoc-
LLama-Chinese-2-13B (Cui et al., 2023) are de-
umentsinourdatasetandRotowireexceedwords
velopedbasedonoriginalLlama-2(Touvronetal.,
orcharacters,presentingmorecomplexinputsfor
2023) with an expansion of the Chinese vocabu-
text-to-tablemodels. Insummary,CT-Evalstands
laryandincrementalpre-trainingwithlarge-scale
asthesoledatasetfulfillingcriteriaofdatadiversity,
Chinese corpus. (10) Qwen-7B-Chat and (11)
lengthydocuments,andlowhallucination.
Qwen-14B-Chat(Baietal.,2023a)aretwoLLMs
of Qwen family that have been pretrained with
4 Experiment
multilingualcorpuswith2.4trillionand3trillion,
4.1 ModelSelection respectively,coveringawiderangeofdomains.
Weconductexperimentsusingtwotypesofback-
bones: closed-sourceandopen-sourceLLMs.
4.2 ExperimentSetups
Closed-SourceLLMs. (1)GPT-3.5-turbo(Ope-
nAI,2022)isoneoftheLLMsfromGPT-3.5fam- 4.2.1 Text-to-tablePrompt
ilydevelopedbyOpenAI.Itistrainedthroughrein-
forcementlearningfromhumanfeedback(RLHF). Recentstudies(Liuetal.,2023;Gaoetal.,2020;
Renowned for its robust ability as a general task Shin et al., 2020) demonstrate that an LLM can
solver,weuseitsnewestversion,i.e.,gpt-3.5-turbo- effectively leverage a small number of examples
1106. (2) GPT-4 (OpenAI, 2023) is the most ad- or task-specific instructions, often referred to as
vanced GPT version inherited from GPT-3.5 and a“prompt”,toguideitsperformanceacrossava-
achievesthestate-of-the-artzero-shotperformance riety of tasks. This approach frequently leads to
across a wide range of NLP tasks. However, this improvedperformanceoutcomes.
model also suffers from hallucinations (OpenAI,
Inourexperiments,forbothzero-shotandfine-
2023). Weuseitsnewestversion,i.e.,gpt-4-1106.
tuningscenarios,weutilizeatext-to-tableprompt
Open-Source LLMs. (3) ChatGLM-6B, (4) designedtoenhancemodel’scapacitytoconstruct
ChatGLM2-6B and (5) ChatGLM3-6B, are a series atablefromtheprovidedtext. Thepreciseprompt
of bilingual dialogue LLMs based on the Gen- we employ is illustrated in Figure 4, where “[ex-
eral Language Model (GLM) architecture (Du ample table]” is an example of the golden table.
et al., 2022). (6) Baichuan2-7B-Chat and This example table is retrieved from the training
(7) Baichuan2-13B-Chat is also trained with setofCT-Eval,providingLLMswithinsightinto
RLHF. This model shows its superior multi- theexpectedoutputstyles.4.2.2 ImplementationDetails intoprecisionsandrecalls. Inthetext-to-tabletask,
Zero-ShotPromptingDetails. (1)Fortheclosed- highrecallssuggestthattheLLM’sabilitytoderive
sourceLLMs(ChatGPTandGPT-4),weaccessthe mostofthetargetinformationpresentinthegolden
official APIs provided by OpenAI4, setting the table; in contrast, low precisions may stem from
temperature to 0, while leaving the other hyper- theLLMover-collectingorcreatingirrelevantin-
parametersattheirdefaultsettings. (2)Regarding formationwhichcanberegardedashallucination.
the open-source LLMs, we also set the temper- Weanalyzetheresultsfromthefollowingaspects:
ature to 0, with the remaining hyper-parameters
GPT-3.5-turbo vs. GPT-4 It can be observed
retained at their default values. Model inference
thatbothGPT-4andGPT-3.5-turbooutperformthe
is performed using a single NVIDIA Tesla V100
other open-source LLMs in most metrics. Com-
GPUwith32GBofmemory.
pared with GPT-3.5-turbo, GPT-4 always shows
Fine-Tuning Details We set the number of train-
asuperiorperformance,notablyachievinga4.63-
ing epochs as two and set the learning rate to 1e-
pointimprovementintheoverallF1-score. TheSo-
5. Additionally,weimplementawarm-upsched-
cialSciencecategoryhasthehighestimprovement
ule(Gotmareetal.,2018)duringtraining. Theop-
of7.77pointsintermoftheF1-scorebroughtby
timizeremployedisAdam(KingmaandBa,2014).
GPT-4. TheseresultscorrespondwithOpenAI’sre-
Furthermore,weutilizetheDeepSpeedoptimiza-
portedperformancecomparisonsbetweenthetwo
tion (Rasley et al., 2020), and configure ZeRO-2
LLMs(OpenAI,2023).
optimizationforfine-tuningallLLMs.
Open-sourceLLMs AssummarizedinTable2,
4.2.3 Metrics
amongtheopen-scoureLLMs,Baichuan2-7B-Chat
FollowingWuetal.(2022),weevaluatethetext-to- outperforms the other LLMs in terms of overall
tableperformancebasedonthenumberofcorrect F1-scoreusingE-matching,whileLlama-Chinese-
non-headercells. Supposingapredictedtablewith 2-7Bdemonstratesthebestperformanceinterms
mrowsandncolumnshasN c = m×ncellsand of overall F1-score using Chrf- and BERT-score.
thecorrespondinggroundtruthwithm∗ rowsand TheseLLMsvaryinperformanceondifferentcat-
n∗ columnshasN c∗ = m∗×n∗ cells,wecalculate egories,withLlama-Chinese-2-7Boutperforming
thesimilarityS betweenthepredictedtableandthe theothersinmostcases. Notably,inthecategories
groundtruthasfollows: ofSTEM andSocialScience,Llama-Chinese-2-7B
exhibits10.39-pointand9.09-pointimprovements
m n m∗ n∗ overthesecond-bestopen-sourceLLMintermsof
1 (cid:88)(cid:88)(cid:88)(cid:88)
S = O(c ,c∗ ) (1) BERT-score,respectively.
N ×N∗ i,j k,l
c c i=1 j=1k=1 l=1 ComparedwithGPT-4,Baichuan2-7B-Chatex-
periences a significant performance gap of 4.76
where c and c∗ (i,j,k,l > 0) refer to the pre-
i,j k,l points using E-matching, and Llama-Chinese-2-
dicted and ground-truth non-header cell content,
7Bexperiencesgapsof5.30and6.32pointsusing
respectively. O(·) refers to there metrics used in
Chrf-andBERT-scoremetrics,respectively. Inthe
ourexperiments: (1)E-Matching: string-levelex-
categoriesofSocialScienceandOthers,whenus-
act matches are conducted between the texts of
ingChrf-andBERT-scoremetrics,Llama-Chinese-
c and c∗ . (2) Chrf-score: the character-level
i,j k,l 2-7BandQwen-14B-ChatevenoutperformGPT-4
n-gram (n = 6) similarity between the texts of
in the F1-score. This suggests that these LLMs
c and c∗ are calculated. (3) BERT-score: the
i,j k,l maybenefitfromthepretrainingthroughthecorre-
vector similarity is operated between the BERT
spondingsupplementedcorpusinChinese.
embeddingsofc andc∗ .
i,j k,l
BadCaseAnalysis Toinvestigatethecausesbe-
5 Results&Analyses
hind the aforementioned results, we additionally
conductthebadcaseanalysis. Specifically,wese-
5.1 Zero-shotResults
lect100samplesoftablesgeneratedbyzero-shot
Table2liststhezero-shotperformanceofclosed-
andmanuallyperformthebadcaseselections. We
sourceandopen-sourceLLMsonCT-Eval. Inad-
classify a sample as erroneous if it falls into any
dition to F1-score, Table 2 also provides insights
ofthetypeslabeledfrom ERROR A to ERROR E:
4https://openai.com/blog/openai-api ERROR A signifies that containing some halluci-STEM SocialSci. LifeSupp. Others Overall
Model
P R F1 P R F1 P R F1 P R F1 P R F1
GPT3.5-turbo 6.08 17.18 8.08 0.60 7.14 1.10 0.00 0.00 0.00 0.00 0.00 0.00 2.09 5.52 2.87
GPT-4 7.46 33.33 11.18 5.40 35.71 8.87 2.14 14.55 3.30 4.03 20.00 6.44 5.07 22.10 7.50
ChatGLM-6B 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
ChatGLM2-6B 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
ChatGLM3-6B 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
Baichuan2-7B-Chat 1.39 2.78 1.85 0.00 0.00 0.00 0.02 0.61 0.05 6.67 13.33 8.89 2.01 5.24 2.74
Baichuan2-13B-Chat 1.95 4.52 2.71 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.48 1.19 0.57
Llama-Chinese-2-7B 2.68 6.57 3.73 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.62 4.07 2.21
Llama-Chinese-2-13B 1.18 2.55 1.61 0.00 0.00 0.00 0.91 0.91 0.91 0.00 0.00 0.00 1.13 2.70 1.47
Qwen-7B-Chat 0.00 0.00 0.00 0.00 0.00 0.00 0.91 0.91 0.91 0.00 0.00 0.00 0.57 1.03 0.69
Qwen-14B-Chat 1.00 2.08 1.35 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.23 0.62 0.29
GPT3.5-turbo 23.54 59.00 31.58 10.22 63.19 15.90 9.81 30.28 12.56 13.89 51.90 20.88 15.96 48.60 21.71
GPT-4 23.26 63.80 30.82 18.60 68.96 27.34 9.85 31.29 12.36 15.77 52.59 23.58 18.76 54.19 25.57
ChatGLM-6B 4.54 8.22 5.65 3.37 8.86 4.79 2.05 3.87 2.61 2.15 4.61 2.88 3.07 6.38 4.02
ChatGLM2-6B 2.27 4.16 2.67 0.71 3.53 1.01 0.81 2.59 1.02 0.83 2.41 1.04 1.29 3.37 1.63
ChatGLM3-6B 2.22 6.45 2.96 1.23 4.63 1.79 1.42 4.81 1.96 1.21 6.02 1.95 1.55 5.25 2.16
Baichuan2-7B-Chat 13.18 22.57 15.31 4.74 17.80 6.79 7.43 11.83 7.59 13.39 42.24 18.82 11.5 25.55 14.35
Baichuan2-14B-Chat 8.89 21.01 11.43 1.36 4.86 2.01 5.43 12.02 5.74 6.98 17.66 9.89 4.08 10.12 4.83
Llama-Chinese-2-7B 24.27 48.06 28.43 24.86 52.73 29.13 10.76 17.66 9.41 20.78 41.08 23.20 16.65 39.76 20.27
Llama-Chinese-2-13B 16.79 34.15 20.60 7.09 34.74 10.89 5.81 13.21 7.04 5.43 12.02 5.74 12.18 33.59 15.81
Qwen-7B-Chat 17.18 23.09 17.01 17.18 23.09 17.01 14.66 11.93 11.92 19.28 39.35 24.05 17.85 25.08 18.14
Qwen-14B-Chat 10.51 15.71 10.09 12.82 28.71 14.65 11.65 9.70 8.96 25.01 32.39 26.52 16.62 24.25 16.97
GPT3.5-turbo 25.64 63.09 34.35 14.98 64.36 22.78 11.54 33.61 15.34 14.22 51.95 21.37 16.82 49.54 23.06
GPT-4 28.54 72.81 37.56 21.83 72.91 32.46 11.93 36.31 15.84 17.16 64.85 26.36 20.83 61.02 28.85
ChatGLM-6B 3.26 4.98 3.61 2.19 6.40 3.21 3.97 2.02 2.52 1.15 3.40 1.67 2.52 5.57 3.24
ChatGLM2-6B 2.15 5.07 2.44 1.12 9.05 1.94 1.89 6.24 2.52 2.22 5.24 2.70 2.07 7.29 2.84
ChatGLM3-6B 6.49 20.29 8.73 2.57 14.51 4.18 3.07 14.27 4.63 3.99 19.34 6.26 4.11 17.05 6.03
Baichuan2-7B-Chat 13.26 24.73 16.60 6.81 21.92 9.93 10.29 17.21 11.08 15.47 47.12 21.95 12.64 28.98 16.23
Baichuan2-14B-Chat 10.92 26.68 14.81 2.81 14.10 4.58 5.38 13.87 6.10 7.53 19.05 10.67 12.32 27.67 15.56
Llama-Chinese-2-7B 26.46 48.85 31.26 29.53 55.04 35.29 13.65 21.10 12.74 21.98 45.49 24.93 18.52 41.08 22.53
Llama-Chinese-2-13B 18.88 38.95 22.38 10.04 37.90 14.84 8.93 19.30 11.03 6.85 23.43 9.82 15.11 38.17 19.39
Qwen-7B-Chat 20.08 28.20 20.87 24.81 36.03 26.20 16.78 15.70 14.78 23.64 49.01 29.85 19.39 27.28 19.94
Qwen-14B-Chat 11.79 12.93 10.72 15.30 32.81 18.23 12.24 11.68 10.10 28.10 39.50 30.25 18.60 26.78 19.11
Table2: BenchmarkLLMsonCT-Evalinthezero-shotmanner,wherethebestscoresareinbold. STEMdenotes
Science,Technology,EngineeringandMathematicssubject;SocialSci. denotesSocialScience;LifeSupp. denotes
LifeSupport. P,RandF1indicatesprecision,recallandF1-score,respectively.
LLMs ErrorRate ERRORA ERRORB ERRORC ERRORD ERRORE
GPT-3.5-turbo 74% 28% 44% 2% 0% 0%
GPT-4 58% 4% 34% 6% 14% 0%
Baichuan2-7B-Chat 94% 70% 20% 2% 2% 0%
Llama-Chinese-2-7B 88% 0% 73% 3% 7% 5%
Qwen-7B-Chat 96% 72% 16% 6% 2% 0%
Table3: Badcaseanalysisforzero-shot. ERROR A:containingsomehallucinations;ERROR B:theoutputsare
over similar to the inputs; ERROR C: returning completely irrelevant information; ERROR D: the summarized
informationisinconsistentwiththegoldentable;ERRORE:returningapropertablewithduplication.
nations;ERROR Boccurswhentheoutputsofthe alsoelucidatethehighrecallrates.
LLM are overly similar to the inputs; ERROR C Asfortheopen-souceLLMs,itcanbeseenthat
denotesthattheLLMreturnscompletelyirrelevant the error rate of Llama-Chinese-2-7B is the low-
information;ERRORDindicatesthattheLLMsum- est among the open-source LLMs, however, it is
marizesinconsistentinformationwiththegolden still14%higherthanthatofGPT-3.5-turbo. Sim-
table; ERROR E arises when the LLM generates ilartotheOpenAILLMs, ERROR B occursmost
a proper table however with duplicated informa- frequently in Llama-Chinese-2-7B. Conversely,
tion. Thetotalerrorrateanderrorrateofeachtype Baichuan2-7B-Chat and Qwen-7B-Chat mainly
are shown in Table 3. For closed-source LLMs, sufferfromthehallucinations.
GPT-4 has a lower total error rate than GPT-3.5-
turbo,andGPT-4suppressesmorehallucinations. 5.2 Fine-tuningResults
BoththeLLMssufferfrom ERROR B,suggesting Wefine-tuneopen-sourceLLMsusingtheCT-Eval
a lack of effective understanding of the objective trainingset. Table5showstheevaluationresults.
of the text-to-table tasks. This observation could Compared with the zero-shot LLMs, there is
gnihctam-E
erocs-frhC
erocs-TREBSTEM SocialSci. LifeSupp. Others Overall
Model
P R F1 P R F1 P R F1 P R F1 P R F1
ChatGLM-6B 30.57 25.55 27.25 1.79 7.14 2.86 8.78 24.30 12.31 0.00 0.00 0.00 15.13 22.84 15.96
ChatGLM2-6B 24.42 21.42 22.60 0.00 0.00 0.00 9.57 30.12 13.96 12.30 48.85 19.06 10.83 16.25 11.72
ChatGLM3-6B 25.01 22.23 23.33 0.00 0.00 0.00 7.56 24.42 11.02 1.11 3.33 1.67 10.92 15.97 11.67
Baichuan2-7B-Chat 19.48 10.99 13.85 0.00 0.00 0.00 3.03 5.15 3.58 0.00 0.00 0.00 12.14 13.15 11.26
Baichuan2-13B-Chat 26.34 23.30 24.38 0.00 0.00 0.00 8.89 27.52 12.85 0.00 0.00 0.00 10.60 15.44 11.32
Llama-Chinese-2-7B 22.52 15.61 18.16 0.00 0.00 0.00 8.18 22.85 11.41 0.00 0.00 0.00 13.30 17.68 13.26
Llama-Chinese-2-13B 24.87 23.23 22.75 0.00 0.00 0.00 8.96 31.09 13.41 0.00 0.00 0.00 10.59 15.60 11.30
Qwen-7B-Chat 27.47 28.46 26.27 0.00 0.00 0.00 9.15 30.18 13.46 1.67 3.33 2.22 10.99 16.42 11.77
Qwen-14B-Chat 25.61 25.18 24.00 0.00 0.00 0.00 8.03 26.85 11.91 0.00 0.00 0.00 9.95 14.73 10.58
ChatGLM-6B 41.77 40.77 39.74 11.24 36.19 16.68 21.11 47.76 27.27 7.23 22.71 10.54 25.03 42.10 28.19
ChatGLM2-6B 39.79 55.73 42.41 15.05 64.70 23.94 25.12 65.45 34.74 12.30 48.85 19.06 26.33 60.24 33.41
ChatGLM3-6B 40.73 58.31 43.62 15.87 62.06 24.39 22.47 61.67 31.84 17.15 51.11 24.60 26.47 59.34 33.43
Baichuan2-7B-Chat 34.79 24.50 27.55 7.31 17.27 10.06 16.35 29.64 19.95 7.57 15.91 10.09 23.73 34.13 25.08
Baichuan2-13B-Chat 38.70 53.98 41.34 16.42 66.53 25.95 8.89 27.52 12.85 13.93 46.10 20.06 26.49 59.39 33.49
Llama-Chinese-2-7B 36.41 31.68 31.97 6.08 21.53 9.37 19.69 46.15 25.90 5.96 15.77 8.44 22.49 35.59 24.50
Llama-Chinese-2-13B 40.45 60.11 43.63 17.94 67.86 27.94 24.83 66.25 34.94 13.89 45.73 20.55 26.61 59.85 33.64
Qwen-7B-Chat 42.29 59.77 45.62 16.85 63.38 26.30 24.91 65.26 34.62 17.35 57.06 25.78 26.58 60.33 33.67
Qwen-14B-Chat 40.57 57.64 43.30 15.70 63.35 24.85 23.55 63.64 32.99 13.61 47.50 20.10 25.51 59.05 32.46
ChatGLM-6B 43.60 44.18 42.39 11.32 39.13 18.13 24.20 51.11 30.36 9.30 25.58 12.80 27.03 45.36 31.28
ChatGLM2-6B 37.74 49.29 40.40 12.86 51.74 20.04 24.71 59.39 33.14 10.58 36.03 15.44 24.63 50.99 30.57
ChatGLM3-6B 37.85 50.44 40.71 12.49 49.48 19.51 22.83 56.74 30.98 13.52 37.51 18.94 24.75 50.47 30.58
Baichuan2-7B 37.62 30.52 32.34 9.72 23.24 13.39 17.70 30.28 20.91 10.83 24.17 14.73 24.96 29.49 26.74
Baichuan2-13B 35.53 46.58 37.96 14.28 56.03 22.39 23.38 56.79 31.45 10.01 29.69 14.08 10.60 15.44 11.32
Llama-Chinese-2-7B 39.42 25.89 36.58 7.22 25.89 11.16 21.80 47.28 27.74 9.37 18.53 11.77 24.98 39.29 27.66
Llama-Chinese-2-13B 38.00 51.45 40.93 15.27 54.60 23.38 24.43 60.85 33.37 14.47 33.09 19.03 24.90 50.89 30.78
Qwen-7B-Chat 38.93 52.41 41.95 13.52 50.11 20.97 24.27 59.05 32.67 16.92 45.65 23.60 24.93 51.40 30.93
Qwen-14B-Chat 37.04 49.78 39.27 12.62 49.15 19.68 23.49 56.37 31.70 11.27 34.84 15.97 23.98 49.92 29.70
Table4: EnhancetheperformanceoftheLLMsthroughfine-tuning,wherethebestscoresareinbold.
LLMs ErrorRate ERRORA ERRORB ERRORC ERRORD ERRORE
ChatGLM-6B 18% 2% 0% 4% 12% 0%
Baichuan2-7B 30% 14% 0% 4% 12% 0%
Llama-Chinese-2-7B 22% 12% 0% 0% 10% 0%
Table5: Badcaseanalysisoffine-tuning.
a significant improvement in the performance of 7Bprimarilyexperiencehallucinations. Incontrast,
the open-source LLMs. ChatGLM-6B demon- ChatGLM-6B successfully suppressed hallucina-
stratesthehighestoverallF1-scoresforE-matching tions;however,itsabilitytopreciselyextractand
andBERT-scoremetrics,exhibitingincrementsof organizeinformationmayrequirefurtherenhance-
15.96 and 28.04 points, respectively; Qwen-7B- mentthroughadditionalparadigms.
ChatachievesthehighestoverallF1-scoreforChrf-
score metric with an increment of 15.53 points. 6 Conclusion
Notably, after fine-tuning, ChatGLM-6B outper-
forms GPT-4 by a substantial margin across all ToassesstheLLMsinthetext-to-tabletasksand
thesimilaritymetrics. Thesefindingssuggestthat further enhance the corresponding performance,
theopen-sourceLLMsmaynotnaturallyexcelin weproposethefirstChinesetext-to-tabledataset,
supportingcomplexIEtasks,andthattheCT-Eval CT-Eval,encompassing28domains. Tominimize
trainingsetcaneffectivelyenhancethetext-to-table hallucination,LLMdatacleaningandhumanclean-
abilityofLLMs. Thiscouldalsoguidethefuture ingareemployed. Ourcomprehensiveexperiments
LLM research that the specific complex IE tasks coverclosed-andopen-sourceLLMsinbothzero-
shouldbeconsideredintheirdevelopments. shotandfine-tuningsettings. Experimentalresults
demonstratethatinzero-shotsettings,GPT-4out-
BadCaseAnalysis Furthermore,weinvestigate performsbestamongallLLMs. Afterfinetuning
thepossiblechangesinthetypesofthebadcases the open-source LLMs, significant performance
after fine-tuning, as illustrated in Table 5. The improvementsareobserved,surpassingGPT-4by
ERROR B and ERROR E types are no longer ex- considerable margins. Bad case analysis further
ists. The error rates of all the LLMs decreases, reveals the persistence of hallucinations in both
with ChatGLM-6B exhibiting the lowest error zero-shotandfine-tuningsettings. Thesefindings
rate. WhileBaichuan2-7BandLlama-Chinese-2- indicatethatcurrentLLMsfacechallengesintext-
gnihctam-E
erocs-frhC
enuteniFto-tabletasks,positioningCT-Evalasaneffective JunGao,HuanZhao,ChanglongYu,andRuifengXu.
benchmark for evaluating text-to-table tasks and 2023. Exploringthefeasibilityofchatgptforevent
extraction. arXivpreprintarXiv:2303.03836.
valuableresourceenhancingLLMperformance.
Tianyu Gao, Adam Fisch, and Danqi Chen. 2020.
Limitations
Makingpre-trainedlanguagemodelsbetterfew-shot
learners. arXivpreprintarXiv:2012.15723.
While we propose the first Chinese text-to-table
benchmark dataset, there are some limitations Carlos-EmilianoGonzález-Gallardo,EmanuelaBoros,
worthconsideringinfuturework: (1)CT-Evalonly NancyGirdhar,AhmedHamdi,JoseGMoreno,and
Antoine Doucet. 2023. Yes but.. can chatgpt iden-
adopts human cleaning in the validation and test
tifyentitiesinhistoricaldocuments? arXivpreprint
sets. Thus, the training set still involves halluci-
arXiv:2303.17322.
nationissue. (2)TheinputdocumentsinCT-Eval
onlyinvolvetextualinformationwhileignorethe Akhilesh Gotmare, Nitish Shirish Keskar, Caiming
Xiong, and Richard Socher. 2018. A closer
multi-modalinformationlikefigures. Futurework
look at deep learning heuristics: Learning rate
couldexplorethemulti-modaltext-to-tableperfor- restarts, warmup and distillation. arXiv preprint
mancebeyondCT-Eval. arXiv:1810.13243.
EthicalConsiderations Diederik P Kingma and Jimmy Ba. 2014. Adam: A
methodforstochasticoptimization. arXivpreprint
Inthispaper,weproposetheCT-Evaldataset. Dur- arXiv:1412.6980.
ing human cleaning process, the salary for each
RémiLebret,DavidGrangier,andMichaelAuli.2016.
humanannotatorisdeterminedbytheaveragetime Neuraltextgenerationfromstructureddatawithap-
of annotation and local labor compensation stan- plication to the biography domain. arXiv preprint
dards. ForBaiduBaike,weutilizethedatadumps arXiv:1603.07771.
providedby Xuetal.(2017). Accordingtotheir
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
officialwebsite’sdescription,thisdumpsisautho- Ghazvininejad,AbdelrahmanMohamed,OmerLevy,
rizedforacademicusage.5 VesStoyanov,andLukeZettlemoyer.2019. Bart:De-
noisingsequence-to-sequencepre-trainingfornatural
languagegeneration,translation,andcomprehension.
arXivpreprintarXiv:1910.13461.
References
JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang, TongLi,ZhihaoWang,LiangyingShao,XulingZheng,
XiaodongDeng,YangFan,WenbinGe,YuHan,Fei Xiaoli Wang, and Jinsong Su. 2023. A sequence-
Huang,etal.2023a. Qwentechnicalreport. arXiv to-sequence&setmodelfortext-to-tablegeneration.
preprintarXiv:2309.16609. arXivpreprintarXiv:2306.00137.
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,
JiankaiTang,ZhidianHuang,ZhengxiaoDu,Xiao Hiroaki Hayashi, and Graham Neubig. 2023. Pre-
Liu,AohanZeng,LeiHou,etal.2023b. Longbench: train, prompt, and predict: A systematic survey of
A bilingual, multitask benchmark for long context promptingmethodsinnaturallanguageprocessing.
understanding. arXivpreprintarXiv:2308.14508. ACMComputingSurveys,55(9):1–35.
JunweiBao,DuyuTang,NanDuan,ZhaoYan,Yuanhua JekaterinaNovikova,OndˇrejDušek,andVerenaRieser.
Lv, Ming Zhou, and Tiejun Zhao. 2018. Table-to- 2017. Thee2edataset: Newchallengesforend-to-
text: Describingtableregionwithnaturallanguage. endgeneration. arXivpreprintarXiv:1706.09254.
InProceedingsoftheAAAIconferenceonartificial
intelligence,volume32. OpenAI.2022. Introducingchatgpt. https://openai.
com/blog/chatgpt.
YimingCui,ZiqingYang,andXinYao.2023. Efficient
and effective text encoding for chinese llama and OpenAI. 2023. Gpt-4 technical report. ArXiv,
alpaca. arXivpreprintarXiv:2304.08177. abs/2303.08774.
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, MichałPietruszka,MichałTurski,ŁukaszBorchmann,
JiezhongQiu,ZhilinYang,andJieTang.2022. Glm: TomaszDwojak,GabrielaPałka,KarolinaSzyndler,
Generallanguagemodelpretrainingwithautoregres- DawidJurkiewicz,andŁukaszGarncarek.2022. Sta-
siveblankinfilling. InProceedingsofthe60thAn- ble:Tablegenerationframeworkforencoder-decoder
nualMeetingoftheAssociationforComputational models. arXivpreprintarXiv:2206.04045.
Linguistics(Volume1: LongPapers),pages320–335.
RafałPowalski,ŁukaszBorchmann,DawidJurkiewicz,
5http://kw.fudan.edu.cn/cndbpedia/download/ Tomasz Dwojak, Michał Pietruszka, and GabrielaPałka.2021. Goingfull-tiltboogieondocumentun- Bo Xu, Yong Xu, Jiaqing Liang, Chenhao Xie, Bin
derstandingwithtext-image-layouttransformer. In Liang,WanyunCui,andYanghuaXiao.2017. Cn-
DocumentAnalysisandRecognition–ICDAR2021: dbpedia: Anever-endingchineseknowledgeextrac-
16th International Conference, Lausanne, Switzer- tionsystem. InInternationalConferenceonIndus-
land,September5–10,2021,Proceedings,PartII16, trial,EngineeringandOtherApplicationsofApplied
pages732–747.Springer. IntelligentSystems,pages428–438.Springer.
ColinRaffel,NoamShazeer,AdamRoberts,Katherine AiyuanYang,BinXiao,BingningWang,BorongZhang,
Lee,SharanNarang,MichaelMatena,YanqiZhou, CeBian,ChaoYin,ChenxuLv,DaPan,DianWang,
WeiLi,andPeterJLiu.2020. Exploringthelimits DongYan,etal.2023. Baichuan2: Openlarge-scale
oftransferlearningwithaunifiedtext-to-texttrans- languagemodels. arXivpreprintarXiv:2309.10305.
former. TheJournalofMachineLearningResearch,
21(1):5485–5551. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
XiaoleiWang,YupengHou,YingqianMin,Beichen
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Zhang, Junjie Zhang, Zican Dong, et al. 2023. A
and Yuxiong He. 2020. Deepspeed: System opti- survey of large language models. arXiv preprint
mizationsenabletrainingdeeplearningmodelswith arXiv:2303.18223.
over100billionparameters. Proceedingsofthe26th
ACMSIGKDDInternationalConferenceonKnowl- Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and
edgeDiscovery&DataMining. Dacheng Tao. 2023. Can chatgpt understand too?
acomparativestudyonchatgptandfine-tunedbert.
Taylor Shin, Yasaman Razeghi, Robert L Logan IV, arXivpreprintarXiv:2302.10198.
EricWallace,andSameerSingh.2020. Autoprompt:
Eliciting knowledge from language models with
automatically generated prompts. arXiv preprint
arXiv:2010.15980.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Bhosale, et al. 2023. Llama 2: Open foundation
andfine-tunedchatmodels,2023. URLhttps://arxiv.
org/abs/2307.09288.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R Bowman. 2018.
Glue: Amulti-taskbenchmarkandanalysisplatform
fornaturallanguageunderstanding. arXivpreprint
arXiv:1804.07461.
JiaanWang,YunlongLiang,FandongMeng,Zengkui
Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng
Qu, and Jie Zhou. 2023a. Is chatgpt a good nlg
evaluator? a preliminary study. arXiv preprint
arXiv:2303.04048.
JiaanWang,YunlongLiang,FandongMeng,BeiqiZou,
ZhixuLi,JianfengQu,andJieZhou.2023b. Zero-
shotcross-lingualsummarizationvialargelanguage
models. arXivpreprintarXiv:2302.14229.
JiaanWang,YunlongLiang,ZengkuiSun,YuxuanCao,
and Jiarong Xu. 2023c. Cross-lingual knowledge
editing in large language models. arXiv preprint
arXiv:2309.08952.
Sam Wiseman, Stuart M Shieber, and Alexander M
Rush.2017. Challengesindata-to-documentgenera-
tion. arXivpreprintarXiv:1707.08052.
XueqingWu,JiachengZhang,andHangLi.2022. Text-
to-table: A new way of information extraction. In
Proceedings of the60th Annual Meeting of the As-
sociationforComputationalLinguistics(Volume1:
LongPapers),pages2518–2533,Dublin,Ireland.As-
sociationforComputationalLinguistics.