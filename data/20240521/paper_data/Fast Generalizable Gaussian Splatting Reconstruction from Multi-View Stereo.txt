Fast Generalizable Gaussian Splatting
Reconstruction from Multi-View Stereo
Tianqi Liu1, Guangcong Wang2,3, Shoukang Hu2, Liao Shen1,
Xinyi Ye1, Yuhang Zang4, Zhiguo Cao1, Wei Li2, and Ziwei Liu2
1 Huazhong University of Science and Technology
2 Nanyang Technological University
3 Great Bay University
4 Shanghai AI Laboratory
tq_liu@hust.edu.cn
https://mvsgaussian.github.io/
Abstract. WepresentMVSGaussian,anewgeneralizable3DGaussian
representationapproachderivedfromMulti-ViewStereo(MVS)thatcan
efficientlyreconstructunseenscenes.Specifically,1)weleverageMVSto
encode geometry-aware Gaussian representations and decode them into
Gaussian parameters. 2) To further enhance performance, we propose a
hybrid Gaussian rendering that integrates an efficient volume rendering
design for novel view synthesis. 3) To support fast fine-tuning for spe-
cific scenes, we introduce a multi-view geometric consistent aggregation
strategy to effectively aggregate the point clouds generated by the gen-
eralizablemodel,servingastheinitializationforper-sceneoptimization.
ComparedwithpreviousgeneralizableNeRF-basedmethods,whichtyp-
icallyrequireminutesoffine-tuningandsecondsofrenderingperimage,
MVSGaussian achieves real-time rendering with better synthesis qual-
ity for each scene. Compared with the vanilla 3D-GS, MVSGaussian
achievesbetterviewsynthesiswithlesstrainingcomputationalcost.Ex-
tensiveexperimentsonDTU,RealForward-facing,NeRFSynthetic,and
TanksandTemplesdatasetsvalidatethatMVSGaussianattainsstate-of-
the-artperformancewithconvincinggeneralizability,real-timerendering
speed, and fast per-scene optimization.
Keywords: GeneralizableGaussianSplatting·Multi-ViewStereo·Neu-
ral Radiance Field · Novel View Synthesis
1 Introduction
Novelviewsynthesis(NVS)aimstoproducerealisticimagesatnovelviewpoints
from a set of source images. By encoding scenes into implicit radiance fields,
NeRF[26]hasachievedremarkablesuccess.However,thisimplicitrepresentation
is time-consuming due to the necessity of querying dense points for rendering.
Recently, 3D Gaussian Splatting (3D-GS) [16] utilizes anisotropic 3D Gaussians
4202
yaM
02
]VC.sc[
1v81221.5042:viXra2 T. Liu et al.
generalization per-scene optimization
Ours
Ours 45s
(14.1, 24.07)
NeRF
10.2h 20.80db / 2min 29.15db / 10min
ENeRF MVSNeRF
(11.7, 23.63) 15min
M (0.a 5t ,c h 2N 2.e 4R 3)F 1IB hRNet 1E hNeRF 26.98db / - 29.16db / 1h
3D-GS
10min
MVSNeRF (0.2, 21.93)
IBRNet (0.1, 21.79) FPS ↑ FPS ↑ 28.13db / - 31.74db / 45s
0 0.5 10 20 0 0.2 10 300
(a) (b) (c)
Fig.1: Comparison with existing methods. (a) We present the generalizable re-
sults on the Real Forward-facing dataset [25]. Compared with other competitors, our
method achieves better performance at a faster inference speed. (b) The results after
per-scene optimization, where circle size represents optimization time. Our method
achieves optimal performance in just 45 seconds. (c) We illustrate a scene (“room”),
showcasing the (PSNR/optimization time) of synthesized views, with "-" indicating
results from direct inference using the generalizable model.
to explicitly represent scenes, achieving real-time and high-quality rendering
throughadifferentiabletile-basedrasterizer.However,3D-GSreliesonper-scene
optimization for several minutes, which limits its applications.
To remedy this issue, some initial attempts have been made to generalize
Gaussian Splatting to unseen scenes. Generalizable Gaussian Splatting meth-
ods directly regress Gaussian parameters in a feed-forward manner instead of
per-scene optimization. The general paradigm involves encoding features for 3D
pointsinascene-agnosticmanner,followedbydecodingthesefeaturestoobtain
Gaussian parameters. PixelSplat [4] leverages an epipolar Transformer [34] to
address scale ambiguity and encode features. However, it focuses on image pairs
as input, and the introduction of Transformers results in significant computa-
tional overhead. GPS-Gaussian [52] draws inspiration from stereo matching by
first performing epipolar rectification on input image pairs, followed by dispar-
ity estimation and feature encoding. However, it focuses on human novel view
synthesis and requires ground-truth depth maps. Splatter Image [32] introduces
a single-view reconstruction approach based on Gaussian Splatting but focuses
on object-centric reconstruction rather than generalizing to unseen scenes.
Due to the inefficiency of existing methods and their limitation to object-
centric reconstruction, in this paper, we aim to develop an efficient generaliz-
able Gaussian Splatting framework for novel view synthesis in unseen general
scenes, which faces several critical challenges: First, unlike NeRFs that use an
implicit representation, 3D-GS is a parameterized explicit representation that
uses millions of 3D Gaussians to overfit a scene. When applying the pre-trained
3D-GS to an unseen scene, the parameters of 3D Gaussians, such as locations
and colors, are significantly different. It is a non-trivial problem to design a
↑
RNSP
52
42
32
22
12
↑
RNSP
72
62
52
42
32
SG-D3
FReNE
sruOMVSGaussian 3
generalizable representation to tailor 3D-GS. Second, previous generalizable
NeRFs [6,8,19,36,50] have achieved impressive view synthesis results through
volumerendering.However,thegeneralizationcapabilityofsplattingremainsun-
explored.Duringsplatting,eachGaussiancontributestomultiplepixelswithina
certain region in the image, and each pixel’s color is determined by the accumu-
lated contributions from multiple Gaussians. The color correspondence between
Gaussians and pixels is a more complex many-to-many mapping, which poses a
challenge for model generalization. Third, generalizable NeRFs show that fur-
ther fine-tuning for specific scenes can greatly improve the synthesized image
qualitybutrequireslengthyoptimization.Although3D-GSisfasterthanNeRF,
it still remains time-consuming. Designing a fast optimization approach based
on the generalizable 3D-GS model is promising.
We address these challenges point by point. First, we propose leveraging
MVS for geometry reasoning and encoding features for 3D points to establish
pixel-aligned Gaussian representations. The point-wise features are aggregated
from multi-view features, and the spatial awareness is enhanced through a 2D
UNet, as each Gaussian contributes to multiple pixels. Second, with the en-
codedpoint-wisefeatures,wecandecodethemintoGaussianparametersthrough
an MLP. Rather than solely relying on splatting, we propose adding a simple
yet effective depth-aware volume rendering approach to enhance generalization.
Third, with the trained generalizable model, lots of 3D Gaussians can be gen-
erated from multiple views. These Gaussian point clouds can serve as an ini-
tialization for subsequent per-scene optimization. However, the generated 3D
Gaussians from the generalizable model are not perfect. Directly concatenating
such a large number of Gaussians as initialization for per-scene optimization
leads to unexpected computational costs because these Gaussians further split
and clone during optimization. One approach is to downsample the point cloud,
such as voxel downsampling, which can reduce noise but also result in the loss
of effective information. Therefore, we introduce a strategy to aggregate point
clouds by preserving multi-view geometric consistency. Specifically, we filter out
noisypointsbycomputingthereprojectionerrorofthedepthofGaussiansfrom
different viewpoints. This strategy can filter out noisy points while preserving
effectiveones,providingahigh-qualityinitializationforsubsequentoptimization.
Tosummarize,wepresentanewfastgeneralizableGaussianSplattingmethod.
We evaluate our method on the widely-used DTU [1], Real Forward-facing [25],
NeRF Synthetic [26], and Tanks and Temples [18] datasets. Extensive experi-
mentsshowthatourgeneralizablemethodoutperformsothergeneralizablemeth-
ods.After a shortperiodof per-sceneoptimization, ourmethod achieves perfor-
mancecomparabletoorevenbetterthanothermethodswithlongeroptimization
times,asshowninFig.1.OnasingleRTX3090GPU,comparedwiththevanilla
3D-GS, our proposed method achieves better novel view synthesis with similar
rendering speed (300+ FPS) and 13.3× less training computational cost (45s).
Our main contributions can be summarized as follows:
– We present MVSGaussian, a generalizable Gaussian Splatting method de-
rived from Multi-View Stereo and a pixel-aligned Gaussian representation.4 T. Liu et al.
– WefurtherproposeanefficienthybridGaussianrenderingapproachtoboost
generalization learning.
– We introduce a consistent aggregation strategy to provide high-quality ini-
tialization for fast per-scene optimization.
2 Related Work
Multi-View Stereo (MVS) aims to reconstruct a dense 3D representation
from multiple views. Traditional MVS methods [11,12,30,31] rely on hand-
crafted features and similarity metrics, which limits their performance. With
the advancement of deep learning in 3D perception, MVSNet [47] first proposes
anend-to-endpipeline,withthekeyideabeingtheconstructionofacostvolume
to aggregate 2D information into a 3D geometry-aware representation. Subse-
quentworksfollowthiscostvolume-basedpipelineandmakeimprovementsfrom
variousaspects,e.g.reducingmemoryconsumptionwithrecurrentplanesweep-
ing [43,48] or coarse-to-fine architectures [9,13,45], optimizing cost aggrega-
tion [37,39], enhancing feature representations [10,22], and improving decoding
strategy [28,49]. As the cost volume encodes the consistency of multi-view fea-
turesandnaturallyperformscorrespondencematching,inthispaper,wedevelop
a new generalizable Gaussian Spatting representation derived from MVS.
Generalizable NeRF. By implicitly representing scenes as continuous color
and density fields using MLPs, Neural Radiance Fields (NeRF) achieve impres-
sive rendering results with volume rendering techniques. Follow-up works [2,5,
27,35,41,42] extend it to various tasks and achieve promising results. However,
they all require time-consuming per-scene optimization. To address this issue,
some generalizable NeRF methods have been proposed. The general paradigm
involves encoding features for each 3D point and then decoding these features
to obtain volume density and radiance. According to the encoded features, gen-
eralizable NeRFs can be categorized into appearance features [50], aggregated
multi-view features [19,21,33,36], cost volume-based features [6,19,21,23], and
correspondence matching features [8]. Despite achieving significant results, per-
formance remains limited, with slow optimization and rendering speeds.
3D Gaussian Splatting (3D-GS) utilizes anisotropic Gaussians to explicitly
represent scenes and achieves real-time rendering through differentiable raster-
ization. Motivated by this, several studies have applied it to various tasks, e.g.
editing [3,7], dynamic scenes [24,40,46], and avatars [14,15,29]. However, the
essenceofGaussianSplattingstillliesinoverfittingthescene.Toremedythis,a
few concurrent works make initial attempts to generalize Gaussian Splatting to
unseen scenes. The goal of Generalizable Gaussian Splatting is to predict Gaus-
sian parameters in a feed-forward manner instead of per-scene optimization.
PixelSplat [4] addresses scale ambiguity by leveraging an epipolar Transformer
to encode features and subsequently decode them into Gaussian parameters.
However, it focuses on image pairs as input and the Transformer incurs sig-
nificant computational costs. GPS-Gaussian [52] draws inspiration from stereo
matching and performs epipolar rectification and disparity estimation on inputMVSGaussian 5
image pairs. However, it focuses on human novel view synthesis and requires
ground-truthdepthmaps.SpatterImage[32]introducesasingle-view3Drecon-
struction approach. However, it focuses on object-centric reconstruction rather
than generalizing to unseen scenes. Overall, these methods are constrained by
inefficiency,limitedtoobjectreconstruction,andrestrictedtoeitherimagepairs
or a single view. To this end, in this paper, we aim to study an efficient gener-
alizable Gaussian Splatting for novel view synthesis in unseen general scenes.
3 Preliminary
3D Gaussian Splatting represents a 3D scene as a mixture of anisotropic 3D
Gaussians, each of which is defined with a 3D covariance matrix Σ and mean µ:
G(X)=e−1 2(X−µ)TΣ−1(X−µ). (1)
The covariance matrix Σ holds physical meaning only when it is positive semi-
definite. Therefore, for effective optimization through gradient descent, Σ is
decomposed into a scaling matrix S and a rotation matrix R, as Σ =RSSTRT.
TosplatGaussiansfrom3Dspacetoa2Dplane,theviewtransformationW and
the Jacobian matrix J representing the affine approximation of the projective
transformation are utilized to obtain the covariance matrix Σ′ in 2D space, as
Σ′ = JWΣWTJT. Subsequently, a point-based alpha-blend rendering can be
performed to obtain the color of each pixel:
i−1
(cid:88) (cid:89)
C = c α (1−α ), (2)
i i i
i j=1
where c is the color of each point, defined by spherical harmonics (SH) coeffi-
i
cients. The density α is computed by the multiplication of 2D Gaussians and a
i
learnable point-wise opacity.
During optimization, the learnable attributes of each Gaussian are updated
through gradient descent, including 1) a 3D position µ∈R3, 2) a scaling vector
s∈R3, 3) a quaternion rotation vector r ∈R4, 4) a color defined by SH c∈Rk
+
(where k is the freedom), and 5) an opacity α∈[0,1]. Additionally, an adaptive
density control module is introduced to improve rendering quality, comprising
mainly the following three operations: 1) split into smaller Gaussians if the
magnitude of the scaling exceeds a threshold, 2) clone if the magnitude of the
scaling is smaller than a threshold, and 3) prune Gaussians with excessively
small opacity or overly large scaling magnitudes.
4 MVSGaussian
4.1 Overview
Givenasetofsourceviews{I }N ,NVSaimstosynthesizeatargetviewfroma
i i=1
novel camera pose. The overview of our proposed generalizable Gaussian Splat-
tingframeworkisdepictedinFig.2.WefirstutilizeaFeaturePyramidNetwork6 T. Liu et al.
Depth Estimation from Multi-View Stereo
M 3D CNN
FPN W
cost volume probability volume
U
Pixel-aligned Gaussian Representation
P
UNet MLP
source views
W warp Efficient Hybrid Gaussian Rendering
M merge
MLP
P pooling
U unproject volume rendering rendered view splatting Gaussians
Fig.2:TheoverviewofgeneralizableGaussianSplattingframework.Wefirst
extractfeatures{f }N frominputsourceviews{I }N usingFPN.Thesemulti-view
i i=1 i i=1
features are then aggregated into a cost volume, regularized by 3D CNNs to produce
depthestimations.Subsequently,foreach3Dpointattheestimateddepth,weutilizea
poolingnetworktoaggregatewarpedsourcefeatures,obtainingtheaggregatedfeature
f . This feature is then enhanced using a 2D UNet, yielding the enhanced feature f .
v g
f is decoded into Gaussian parameters for splatting, while f is decoded into volume
g v
density and radiance for depth-aware volume rendering with a single sampled point.
Finally, the twocroennsidsteernecyd cihmecakges are averaged to produce the final rendered result.
(FPN) [20] to extract multi-scale features from source views. These features
are then warped onto the target camera frustum to construct a cost volume
via differentiable homography, followed by 3D CNNs for regularization to pro-
duce the depth map. Based on the obtained depth map,cownceatencode features for
each pixel-aligned 3D point by aggregating multi-view and spatial information.
The encoded features can be then decoded for rendering. However, Gaussian
sss aggregated
Splatting is a region-based explicit representation and is designed for tile-based
point cloud
rendering, involves a complex many-to-many mapping between Gaussians and
pixels, posing challenges for generalizable learnifnilgte.redTo address this, we propose
anefficd ie ep nth t m hap ybridpo rin et ncl dou ed ringbyinm tea gsk ratingasipmoipntl celodudepth-awarevolumerender-
ingmodule,whereonlyonepointissampledperray.Werendertwoviewsusing
GaussianSplattingandvolumerendering,thenaveragethesetworenderedviews
into the final view. This pipeline is further constructed in a cascade structure,
propagating the depth map and rendered view in a coarse-to-fine manner.
4.2 MVS-based Gaussian Splatting Representation
Depth Estimation from MVS.Thedepthmapisacrucialcomponentofour
pipeline,asitbridges2Dimagesand3Dscenerepresentation.Followinglearning-
basedMVSmethods[47],wefirstestablishmultiplefronto-parallelplanesatthe
target view. Then, we warp the features of source views onto these sweepingMVSGaussian 7
planes using differentiable homography as:
(R−1t −R−1t )aTR
H (z)=K R (I+ i i t t t)R−1K−1, (3)
i i i z t t
where [K ,R ,t ] and [K ,R ,t ] are the camera intrinsic, rotation and transla-
i i i t t t
tion of the source view I and target view, respectively. The a represents the
i
principal axis of the target view camera, I denotes the identity matrix and z is
the sampled depth. With the warped features from source views, a cost volume
is constructed by computing their variance, which encodes the consistency of
multi-view features. Then, the cost volume is fed into 3D CNNs for regulariza-
tion to obtain the probability volume. With this depth probability distribution,
we weight each depth hypothesis to obtain the final depth.
Pixel-aligned Gaussian Representation. With the estimated depth, each
pixel can be unprojected to a 3D point, which is the position of the 3D Gaus-
sian. The subsequent step is encoding features for these 3D points to establish
a pixel-aligned Gaussian representation. Specifically, we first warp the features
from source views to the target camera frustum using Eq. (3), and then utilize
a pooling network ρ [19,36] to aggregate these multi-view features into features
f = ρ({f }N ). Considering the properties of splatting, each Gaussian con-
v i i=1
tributes to the color values of pixels in a specific region of the image. However,
the aggregated feature f only encodes multi-view information for individual
v
pixels, lacking spatial awareness. Therefore, we utilize a 2D UNet for spatial
enhancement, yielding f . With the encoded features, we can decode them to
g
obtain Gaussian parameters for rendering. Specifically, each Gaussian is charac-
terized by attributes {µ,s,r,α,c} as described in Sec. 3. For the position µ, it
can be obtained by unprojecting pixels according to the estimated depth as:
µ=Π−1(x,d), (4)
where Π−1 represents the unprojection operation. x and d represent the coor-
dinates and estimated depth of the pixel, respectively. For scaling s, rotation r,
and opacity α, they can be decoded from the encoded features, given by:
s=Softplus(h (f )),
s g
r =Norm(h (f )), (5)
r g
α=Sigmoid(h (f )),
α g
where h , h , and h represent the scaling head, rotation head, and opacity
s r α
head, respectively, instantiated as MLPs. For the last attribute, color c, 3D
Gaussian Splatting [16] utilizes spherical harmonic (SH) coefficients to define
it. However, the generalization of learning SH coefficients from features is not
robust (Sec. 5.4). Instead, we directly regress color from features as:
c=Sigmoid(h (f )), (6)
c g
where h represents the color head.
c8 T. Liu et al.
concat
·
aggregated

`
depth map  0  0 c  o0 nsis  t0 ency  c1 hec  k1  1  1 mask point cloud filtered
Fig.3: Consistent aggregation. With the depth maps and Gaussian point clouds
producedbythegeneralizablemodel,wefirstconductmulti-viewgeometricconsistency
checks on the depth maps to derive masks for filtering out unreliable points. Subse-
quently, the filtered point clouds are concatenated to construct a point cloud, serving
as the initialization for per-scene optimization.
Efficient Hybrid Gaussian Rendering. With the aforementioned Gaussian
parameters,anovelviewcanberenderedusingthesplattingtechnique.However,
the obtained view lacks fine details, and this approach exhibits limited gener-
alization performance. Our insight is that the splatting approach introduces a
complex many-to-many relationship between 3D Gaussians and pixels in terms
of color contribution, which poses challenges for generalization. Therefore, we
propose using a simple one-to-one correspondence between 3D Gaussians and
pixels to predict colors for refinements. In this case, the splatting degenerates
intothevolumerenderingwithasingledepth-awaresamplingpoint.Specifically,
following [19,36], we obtain radiance and volume density by decoding f , fol-
v
lowedbyvolumerenderingtoobtainarenderedview.Thefinalrenderedviewis
formedbyaveragingtheviewsrenderedthroughsplattingandvolumerendering.
4.3 Consistent Aggregation for Per-Scene Optimization
The generalizable model can reconstruct a reasonable 3D Gaussian represen-
tation for an unseen scene. We can further optimize this Gaussian representa-
tion for specific scenes using optimization strategies described in Sec. 3. Since
the aforementioned generalizable model reconstructs Gaussian representations
atseveralgivennovelviewpoints,theprimarychallengeishowtoeffectivelyag-
gregate these Gaussian representations into a single Gaussian representation for
efficientrendering.DuetotheinherentlimitationsoftheMVSmethod,thedepth
predictedbythegeneralizablemodelmaynotbeentirelyaccurate,leadingtothe
presence of noise in the resulting Gaussian point cloud. Directly concatenating
these Gaussian point clouds results in a significant amount of noise. Addition-
ally, a large number of points slow down subsequent optimization and rendering
speeds. An intuitive solution is to downsample the concatenated point cloud.
However, while reducing noise, it also diminishes the number of effective points.
Our insight is that a good aggregation strategy should minimize noisy points
and retain effective ones as much as possible, while also ensuring that the totalMVSGaussian 9
number of points is not excessively large. To this end, we introduce an aggrega-
tionstrategybasedonmulti-viewgeometricconsistency.Thepredicteddepthfor
the same 3D point across different viewpoints should demonstrate consistency.
Otherwise, the predicted depth is considered unreliable. This geometric consis-
tency can be measured by calculating the reprojection error between different
views. Specifically, as illustrated in Fig 3, given a reference depth map D to
0
be examined and a depth map D from a nearby viewpoint, we first project the
1
pixel p in D to the nearby view to obtain the projected point q as:
0
1
q = Π (p,D (p)), (7)
d 0−1 0
where Π represents the transformation from D to D , and d is the depth
0−1 0 1
from projection. In turn, we back-project the obtained pixel q with estimated
depth D (q) onto the reference view to obtain the reprojected point p′ as:
1
1
p′ = Π (q,D (q)), (8)
d′ 1−0 1
where Π represents the transformation from D to D , and d′ is the depth
1−0 1 0
of the reprojected pixel. Then, the reprojection errors are calculated by:
ξ =∥p−p′∥ ,
p 2 (9)
ξ =∥D (p)−d′∥ /D (p),
d 0 1 0
Thereferenceimagewillbecomparedpairwisewitheachoftheremainingimages
to calculate the reprojection error. Inspired by [22,44], we adopt the dynamic
consistency checking algorithm to select the valid depth values. The main idea
is that the estimated depth is reliable when it has very a low reprojection error
in a minority of views or a relatively low error in the majority of views. It can
be formulated as follows:
ξ <θ (n),ξ <θ (n), (10)
p p d d
where θ (n) and θ (n) represent predefined thresholds, whose values increase as
p d
the number of views n increases. The depth is reliable when there are n nearby
viewsthatmeetthecorrespondingthresholdsθ (n)andθ (n).Wefilteroutnoise
p d
points that do not meet the conditions and store the correctly reliable points.
4.4 Full Objective
Our model is trained end-to-end using only RGB images as supervision. We
optimize the generalizable model with the mean squared error (mse) loss, SSIM
loss [38], and perceptual loss [51], as follows:
Lk =L +λ L +λ L , (11)
mse s ssim p perc
where Lk represents the loss for the kth stage of the coarse-to-fine framework.
λ andλ denotethelossweights.Theoveralllossisthesumoflossesfromeach
s p
stage, given by:
(cid:88)
L= λkLk, (12)10 T. Liu et al.
where λk represents the loss weight for the kth stage. During per-scene opti-
mization, following [16], we optimize Gaussian point clouds using the L loss
1
combined with a D-SSIM term:
L =(1−λ )L +λ L , (13)
ft ft 1 ft D−SSIM
where λ is the loss weight.
ft
5 Experiments
5.1 Settings
Datasets. Following MVSNeRF [6], we train the generalizable model on the
DTUtrainingset[1]andevaluateitontheDTUtestset.Subsequently,wecon-
duct further evaluations on the Real Forward-facing [25], NeRF Synthetic [26],
and Tanks and Temples [18] datasets. For each test scene, we select 20 nearby
views, with 16 views comprising the working set and the remaining 4 views
as testing views. The quality of synthesized views is measured by widely-used
PSNR, SSIM [38], and LPIPS [51] metrics.
Baselines. We compare our method with state-of-the-art generalizable NeRF
methods[6,8,19,36,50],aswellastherecentgeneralizableGaussianmethod[4].
For the generalization comparison, we follow the same experimental settings
as[6,8,19]andborrowsomeresultsreportedin[6,8].For[19]and[4],weevaluate
them using their officially released code and pre-trained models. For per-scene
optimizationexperiments,weincludeNeRF[26]and3D-GS[16]forcomparison.
Implementation Details. Following [19], we employ a two-stage cascaded
framework. For depth estimation, we sample 64 and 8 depth planes for the
coarse and fine stages, respectively. We set λ = 0.1 and λ = 0.05 in Eq. (11),
s p
λ1 = 0.5 and λ2 = 1 in Eq. (12), and λ = 0.2 in Eq. (13). The generaliz-
ft
able model is trained using the Adam optimizer [17] on four RTX 3090 GPUs.
During the per-scene optimization stage, for fair comparison, our optimization
strategy and hyperparameters settings remain consistent with the vanilla 3D-
GS [16], except for the number of iterations. For the initialization of 3D-GS, we
use COLMAP [30] to reconstruct the point cloud from the working set.
5.2 Generalization Results
We train the generalizable model on the DTU training set and report quanti-
tative results on the DTU test set in Table 1, and the quantitative results on
three additional datasets in Table 2. Due to the MVS-based pixel-aligned Gaus-
sian representation and the efficient hybrid Gaussian rendering, our method
achieves optimal performance at a fast inference speed. Due to the introduction
of the epipolar Transformer, PixelSplat [4] has slow speed and large memory
consumption. Additionally, it focuses on natural scenes with image pairs as in-
put, and its performance significantly decreases when applied to object-centric
datasets [1,26]. For NeRF-based methods, ENeRF [19] enjoys promising speedsMVSGaussian 11
Table 1: Quantitative results of generalization on the DTU test set [1].FPS
andMemaremeasuredundera3-viewinput,whileFPS∗ andMem∗ aremeasuredun-
dera2-viewinput.Thebestresultisinbold,andthesecond-bestoneisinunderlined.
3-view 2-view
Method Mem(GB)↓ FPS↑
PSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓
PixelNeRF[50] 19.31 0.789 0.382 - - - - 0.019
IBRNet[36] 26.04 0.917 0.191 - - - - 0.217
MVSNeRF[6] 26.63 0.931 0.168 24.03 0.914 0.192 - 0.416
ENeRF[19] 27.61 0.957 0.089 25.48 0.942 0.107 2.183 19.5
MatchNeRF[8] 26.91 0.934 0.159 25.03 0.919 0.181 - 1.04
PixelSplat[4] - - - 14.01 0.662 0.389 11.827∗ 1.13∗
Ours 28.21 0.963 0.076 25.78 0.947 0.095 0.876/0.866∗ 21.5/24.5∗
by sampling only 2 points per ray, however, its performance is limited and con-
sumes higher memory overhead. The remaining methods render images by sam-
pling rays due to their high memory consumption, as they cannot process the
entireimageatonce.ThequalitativeresultsarepresentedinFig.4.Ourmethod
produces high-quality views with more scene details and fewer artifacts.
Table 2: Quantitative results of generalization on Real Forward-facing [25],
NeRF Synthetic [26], and Tanks and Temples [18] datasets.Duetothesignif-
icant memory consumption of PixelSplat [4], we conduct performance evaluation and
comparison on low-resolution (512×512) images, denoted as PixelSplat∗ and Ours∗.
The best result is in bold, and the second-best one is in underlined.
RealForward-facing[25] NeRFSynthetic[26] TanksandTemples[18]
Method Settings
PSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓
PixelNeRF[50] 11.24 0.486 0.671 7.39 0.658 0.411 - - -
IBRNet[36] 21.79 0.786 0.279 22.44 0.874 0.195 20.74 0.759 0.283
MVSNeRF[6] 21.93 0.795 0.252 23.62 0.897 0.176 20.87 0.823 0.260
3-view
ENeRF[19] 23.63 0.843 0.182 26.17 0.943 0.085 22.53 0.854 0.184
MatchNeRF[8] 22.43 0.805 0.244 23.20 0.897 0.164 20.80 0.793 0.300
Ours 24.07 0.857 0.164 26.46 0.948 0.071 23.28 0.877 0.139
MVSNeRF[6] 20.22 0.763 0.287 20.56 0.856 0.243 18.92 0.756 0.326
ENeRF[19] 22.78 0.821 0.191 24.83 0.931 0.117 22.51 0.835 0.193
2-view
MatchNeRF[8] 20.59 0.775 0.276 20.57 0.864 0.200 19.88 0.773 0.334
Ours 23.11 0.834 0.175 25.06 0.937 0.079 22.67 0.844 0.162
PixelSplat∗[4] 22.99 0.810 0.190 15.77 0.755 0.314 19.40 0.689 0.223
2-view
Ours∗ 23.30 0.835 0.152 25.34 0.935 0.071 23.18 0.849 0.130
5.3 Per-Scene Optimization Results
The quantitative results after per-scene optimization are reported in Table 3.
For per-scene optimization, one strategy is to optimize the entire pipeline, sim-
ilar to NeRF-based methods. Another approach is to optimize only the initial
Gaussianpointcloudprovidedbythegeneralizablemodel.Whenoptimizingthe
entire pipeline, Our method can achieve better performance with faster infer-
encespeedscomparedtopreviousgeneralizableNeRFmethods.itdemonstrates
comparableresultstoNeRF,showcasingtherobustrepresentationcapabilitiesof12 T. Liu et al.
Ground Truth MVSNeRF ENeRF MatchNeRF Ours
Fig.4:Qualitativecomparisonofrenderingqualityundergeneralizationand
3-view settings with state-of-the-art methods [6,8,19].
Table 3: Quantitative results after per-scene optimization. Time represents
ft
thetimeforfine-tuning.Thebestresultisinbold,andsecond-bestoneisinunderlined.
Method Optimization RealForward-facing[25] NeRFSynthetic[26] TanksandTemples[18]
PSNR↑SSIM↑LPIPS↓Timeft↓FPS↑PSNR↑SSIM↑LPIPS↓Timeft↓FPS↑PSNR↑SSIM↑LPIPS↓Timeft↓FPS↑
NeRF[26] 25.97 0.870 0.236 10.2h 0.08 30.63 0.962 0.093 10.2h 0.07 21.42 0.702 0.558 10.2h 0.08
IBRNet[36] 24.88 0.861 0.189 1.0h 0.10 25.62 0.939 0.111 1.0h 0.10 22.22 0.813 0.221 1.0h 0.10
MVSNeRF[6] Pipeline 25.45 0.877 0.192 15min 0.20 27.07 0.931 0.168 15min 0.19 21.83 0.841 0.235 15min 0.20
ENeRF[19] 24.89 0.865 0.159 1.0h 11.7 27.57 0.954 0.063 1.0h 10.5 24.18 0.885 0.145 1.0h 11.7
Ours 25.92 0.891 0.135 1.0h 14.1 27.87 0.956 0.061 1.0h 12.5 24.35 0.888 0.125 1.0h 14.0
3D-GS7k[16] 22.15 0.808 0.243 2min 370 32.15 0.971 0.048 1min15s 450 20.13 0.778 0.319 2min30s 320
3D-GS30k[16] Gaussians 23.92 0.822 0.213 10min 350 31.87 0.969 0.050 7min 430 23.65 0.867 0.184 15min 270
Ours 26.98 0.913 0.113 45s 350 32.20 0.972 0.043 50s 470 24.58 0.903 0.137 90s 330
ourmethod.Incontrast,optimizingonlytheGaussianscansignificantlyimprove
optimizationandrenderingspeedbecauseiteliminatesthetime-consumingfeed-
forward neural network. Moreover, performance can benefit from the adaptive
density control module described in Sec. 3. Due to the excellent initialization
provided by the generalizable model and the effective aggregation strategy, we
achieve optimal performance within a short optimization period, approximately
one-tenth of that of 3D-GS [16]. Especially on the Real Forward-facing dataset,
ourmethodachievessuperiorperformancewithonly45secondsofoptimization,
compared to 10 minutes for 3D-GS and 10 hours for NeRF [26]. Additionally,
our method’s inference speed is comparable to that of 3D-GS and significantly
outperforms NeRF-based methods. As shown in Fig. 5, our method is capable
of producing high-fidelity views with finer details.MVSGaussian 13
Ground Truth MVSNeRF ENeRF 3D-GS Ours
Fig.5: Qualitative comparison of rendering quality with state-of-the-art
methods [6,16,19] after per-scene optimization.
5.4 Ablations and Analysis
Ablation studies. As shown in Table 4, we conduct ablation studies to eval-
uate the effectiveness of our designs. Firstly, comparing No.1 and No.2, the
cascaded structure demonstrates a significant role. Additionally, adopting the
hybridGaussianrenderingapproach(No.4)notablyenhancesperformancecom-
pared to utilizing splatting (No.2) or volume rendering (No.3) alone. Regarding
color representation, we directly decode RGB values instead of spherical har-
monic (SH) coefficients (No.5), as decoding coefficients may result in a degrada-
tion of generalization, especially notable on the NeRF Synthetic dataset.
Table 4: Ablation studies. The terms “gs” and “vr” represent Gaussian Splatting
andvolumerendering,respectively.PSNR ,PSNR ,PSNR ,andPSNR are
dtu llff nerf tnt
the PSNR metrics for different datasets [1,18,25,26].
CascadeDecodingColorPSNR PSNR PSNR PSNR
dtu llff nerf tnt
No.1 ✗ gs rgb 26.71 22.57 24.90 21.06
No.2 (cid:34) gs rgb 27.48 23.15 25.48 21.70
No.3 (cid:34) vr rgb 27.39 23.80 25.65 22.76
No.4 (cid:34) gs+vr rgb 28.21 24.07 26.46 23.28
No.5 (cid:34) gs+vr sh 28.19 23.74 24.27 22.70
Aggregation strategies. As shown in Table 5, we investigate the impact of
different point cloud aggregation strategies, which provide varying qualities of
initialization and significantly affect subsequent optimization. The direct con-
catenation approach leads to an excessively large initial point set, hindering
optimization and rendering speeds. Downsampling the point cloud can mitigate1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
14 T. Liu et al.
0.2 0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 3D0-G.0S 106..286 / 500.04 0.6 170.5.81 / 1010.00 18.70 / 1500 20.14 / 2000
1.0
26
0.8
24
0.6
22
20 0.4
18 Ours 0.2
3D-GS
16 0.0
500 1  , W0  H0  U D0  W L R Q V1500 2000 Ou0r.s0 20 3. .2
83 /
50 0. 04 0.6 240 .9.8
6 /
101 0. 00
25.79 / 1500 26.27 / 2000
(a) (b)
Fig.6: Analysis of the Optimization process. (a) The evolution of view quality
(PSNR)ontheRealForward-facing[25]datasetduringthefirst2000iterationsofour
method and 3D-GS [16]. (b) Qualitative comparison of our method (bottom) and 3D-
GS (top) on the “trex” scene, where (PSNR/iteration number) is shown.
this issue while also improving performance, as it reduces contamination from
noisy points. However, performance remains limited as it also simultaneously
reducessomevalidpoints.Employingtheconsistencycheckstrategycanfurther
boost performance, as it filters out noisy points while preserving valid points.
Table 5: Comparison of different aggregation strategies.Wereportthequanti-
tativeresultsobtainedwithdifferentstrategiesontheRealForward-facingdataset[25].
For downsampling aggregation, we employ widely-used voxel downsampling with a
voxel size set to 2. The iteration number for all aggregation strategies is set to 2k.
Aggregation PSNR↑SSIM↑LPIPS↓Time ↓FPS↑
ft
directconcatenation 26.18 0.901 0.122 90s 220
downsampling 26.72 0.909 0.121 60s 340
consistencycheck 26.98 0.913 0.113 45s 350
Optimizationprocess.WeillustratetheoptimizationprocessinFig.6.Thanks
to the excellent initialization provided by the generalizable model, our method
quickly attains good performance and rapidly improves.
6 Conclusion
In this paper, we present MVSGaussian, a novel generalizable Gaussian Splat-
ting approach for reconstructing scene representations from MVS. Specifically,
weleverageMVStoencodegeometry-awarefeatures,establishingapixel-aligned
Gaussian representation. Additionally, we propose a hybrid Gaussian rendering
approachthatintegratesefficientdepth-awarevolumerenderingtoenhancegen-
eralization. Apart from its remarkable generalizability, our model can be eas-
ily fine-tuned for specific scenes. To facilitate fast optimization, we introduce
 5 1 6 3MVSGaussian 15
a multi-view geometric consistent aggregation strategy to provide high-quality
initialization.ComparedwithgeneralizableNeRFs,whichtypicallyrequiremin-
utes of fine-tuning and seconds of rendering per image, MVSGaussian achieves
real-time rendering with superior synthesis quality. Moreover, compared with
the vanilla 3D-GS, MVSGaussian achieves better view synthesis with reduced
trainingcomputationalcost.ExtensiveexperimentsvalidatethatMVSGaussian
achieves state-of-the-art performance with convincing generalizability, real-time
rendering speed, and fast per-scene optimization.
Limitations. As our method relies on MVS for depth estimation, it inherits
limitations from MVS, such as decreased depth accuracy in areas with weak
textures or specular reflections, resulting in degraded view quality.
References
1. Aanaes,H.,Jensen,R.R.,Vogiatzis,G.,Tola,E.,Dahl,A.B.:Large-scaledatafor
multiple-view stereopsis. IJCV 120, 153–168 (2016)
2. Boss, M., Braun, R., Jampani, V., Barron, J.T., Liu, C., Lensch, H.: Nerd: Neu-
ral reflectance decomposition from image collections. In: ICCV. pp. 12684–12694
(2021)
3. Cen, J., Fang, J., Yang, C., Xie, L., Zhang, X., Shen, W., Tian, Q.: Segment any
3d gaussians. arXiv preprint arXiv:2312.00860 (2023)
4. Charatan,D.,Li,S.,Tagliasacchi,A.,Sitzmann,V.:pixelsplat:3dgaussiansplats
from image pairs for scalable generalizable 3d reconstruction. In: arXiv (2023)
5. Chen, A., Liu, R., Xie, L., Chen, Z., Su, H., Yu, J.: Sofgan: A portrait image
generator with dynamic styling. ACM Trans. Graph. 41(1), 1–26 (2022)
6. Chen, A., Xu, Z., Zhao, F., Zhang, X., Xiang, F., Yu, J., Su, H.: Mvsnerf: Fast
generalizable radiance field reconstruction from multi-view stereo. In: ICCV. pp.
14124–14133 (2021)
7. Chen, Y., Chen, Z., Zhang, C., Wang, F., Yang, X., Wang, Y., Cai, Z., Yang, L.,
Liu, H., Lin, G.: Gaussianeditor: Swift and controllable 3d editing with gaussian
splatting. arXiv preprint arXiv:2311.14521 (2023)
8. Chen,Y.,Xu,H.,Wu,Q.,Zheng,C.,Cham,T.J.,Cai,J.:Explicitcorrespondence
matchingforgeneralizableneuralradiancefields.arXivpreprintarXiv:2304.12294
(2023)
9. Cheng, S., Xu, Z., Zhu, S., Li, Z., Li, L.E., Ramamoorthi, R., Su, H.: Deep stereo
usingadaptivethinvolumerepresentationwithuncertaintyawareness.In:CVPR.
pp. 2524–2534 (2020)
10. Ding,Y.,Yuan,W.,Zhu,Q.,Zhang,H.,Liu,X.,Wang,Y.,Liu,X.:Transmvsnet
global context-aware multi-view stereo network with transformers. In: CVPR. pp.
8585–8594 (2022)
11. Fua, P., Leclerc, Y.G.: Object-centered surface reconstruction combining multi-
image stereo and shading. IJCV 16(ARTICLE), 35–56 (1995)
12. Galliani,S.,Lasinger,K.,Schindler,K.:Massivelyparallelmultiviewstereopsisby
surface normal diffusion. In: ICCV. pp. 873–881 (2015)
13. Gu, X., Fan, Z., Zhu, S., Dai, Z., Tan, F., Tan, P.: Cascade cost volume for high-
resolutionmulti-viewstereoandstereomatching.In:CVPR.pp.2495–2504(2020)
14. Hu,L.,Zhang,H.,Zhang,Y.,Zhou,B.,Liu,B.,Zhang,S.,Nie,L.:Gaussianavatar:
Towards realistic human avatar modeling from a single video via animatable 3d
gaussians. arXiv preprint arXiv:2312.02134 (2023)16 T. Liu et al.
15. Hu,S.,Liu,Z.:Gauhuman:Articulatedgaussiansplattingfrommonocularhuman
videos. arXiv preprint arXiv: (2023)
16. Kerbl, B., Kopanas, G., Leimkühler, T., Drettakis, G.: 3d gaussian splatting for
real-time radiance field rendering. ACM Transactions on Graphics 42(4) (2023)
17. Kingma,D.P.,Ba,J.:Adam:Amethodforstochasticoptimization.arXivpreprint
arXiv:1412.6980 (2014)
18. Knapitsch,A.,Park,J.,Zhou,Q.Y.,Koltun,V.:Tanksandtemplesbenchmarking
large-scale scene reconstruction. ACM Trans. Graph. 36(4), 1–13 (2017)
19. Lin,H.,Peng,S.,Xu,Z.,Yan,Y.,Shuai,Q.,Bao,H.,Zhou,X.:Efficientneuralra-
diancefieldsforinteractivefree-viewpointvideo.In:SIGGRAPHAsiaConference
Proceedings (2022)
20. Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature
pyramid networks for object detection. In: CVPR. pp. 2117–2125 (2017)
21. Liu, T., Ye, X., Shi, M., Huang, Z., Pan, Z., Peng, Z., Cao, Z.: Geometry-aware
reconstructionandfusion-refinedrenderingforgeneralizableneuralradiancefields.
arXiv preprint arXiv:2404.17528 (2024)
22. Liu, T., Ye, X., Zhao, W., Pan, Z., Shi, M., Cao, Z.: When epipolar constraint
meetsnon-localoperatorsinmulti-viewstereo.In:ICCV.pp.18088–18097(2023)
23. Liu,Y.,Peng,S.,Liu,L.,Wang,Q.,Wang,P.,Theobalt,C.,Zhou,X.,Wang,W.:
Neural rays for occlusion-aware image-based rendering. In: CVPR (2022)
24. Luiten,J.,Kopanas,G.,Leibe,B.,Ramanan,D.:Dynamic3dgaussians:Tracking
by persistent dynamic view synthesis. In: 3DV (2024)
25. Mildenhall, B., Srinivasan, P.P., Ortiz-Cayon, R., Kalantari, N.K., Ramamoorthi,
R.,Ng,R.,Kar,A.:Locallightfieldfusion:Practicalviewsynthesiswithprescrip-
tive sampling guidelines. ACM Trans. Graph. 38(4), 1–14 (2019)
26. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,
R.:Nerf:Representingscenesasneuralradiancefieldsforviewsynthesis.In:ECCV
(2020)
27. Park,K.,Sinha,U.,Barron,J.T.,Bouaziz,S.,Goldman,D.B.,Seitz,S.M.,Martin-
Brualla, R.: Nerfies: Deformable neural radiance fields. In: ICCV. pp. 5865–5874
(2021)
28. Peng,R.,Wang,R.,Wang,Z.,Lai,Y.,Wang,R.:Rethinkingdepthestimationfor
multi-view stereo a unified representation. In: CVPR. pp. 8645–8654 (2022)
29. Qian, Z., Wang, S., Mihajlovic, M., Geiger, A., Tang, S.: 3dgs-avatar: Animat-
ableavatarsviadeformable3dgaussiansplatting.arXivpreprintarXiv:2312.09228
(2023)
30. Schonberger, J.L., Frahm, J.M.: Structure-from-motion revisited. In: CVPR. pp.
4104–4113 (2016)
31. Schonberger, J.L., Zheng, E., Frahm, J.M., Pollefeys, M.: Pixelwise view selection
for unstructured multi-view stereo. In: ECCV. pp. 501–518. Springer (2016)
32. Szymanowicz,S.,Rupprecht,C.,Vedaldi,A.:Splatterimage:Ultra-fastsingle-view
3d reconstruction. In: arXiv (2023)
33. T, M.V., Wang, P., Chen, X., Chen, T., Venugopalan, S., Wang, Z.: Is attention
all that neRF needs? In: ICLR (2023)
34. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,
Ł., Polosukhin, I.: Attention is all you need. Advances in neural information pro-
cessing systems 30 (2017)
35. Wang,G.,Chen,Z.,Loy,C.C.,Liu,Z.:Sparsenerf:Distillingdepthrankingforfew-
shot novel view synthesis. In: IEEE/CVF International Conference on Computer
Vision (ICCV) (2023)MVSGaussian 17
36. Wang, Q., Wang, Z., Genova, K., Srinivasan, P., Zhou, H., Barron, J.T., Martin-
Brualla,R.,Snavely,N.,Funkhouser,T.:Ibrnet:Learningmulti-viewimage-based
rendering. In: CVPR (2021)
37. Wang, X., Zhu, Z., Huang, G., Qin, F., Ye, Y., He, Y., Chi, X., Wang, X.:
Mvster epipolar transformer for efficient multi-view stereo. In: ECCV. pp. 573–
591. Springer (2022)
38. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment:
from error visibility to structural similarity. IEEE TIP 13(4), 600–612 (2004)
39. Wei, Z., Zhu, Q., Min, C., Chen, Y., Wang, G.: Aa-rmvsnet adaptive aggregation
recurrent multi-view stereo network. In: ICCV. pp. 6187–6196 (2021)
40. Wu,G.,Yi,T.,Fang,J.,Xie,L.,Zhang,X.,Wei,W.,Liu,W.,Tian,Q.,Xinggang,
W.: 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint
arXiv:2310.08528 (2023)
41. Xian, W., Huang, J.B., Kopf, J., Kim, C.: Space-time neural irradiance fields for
free-viewpoint video. In: CVPR. pp. 9421–9431 (2021)
42. Xiang, F., Xu, Z., Hasan, M., Hold-Geoffroy, Y., Sunkavalli, K., Su, H.: Neutex:
Neuraltexturemappingforvolumetricneuralrendering.In:CVPR.pp.7119–7128
(2021)
43. Yan, J., Wei, Z., Yi, H., Ding, M., Zhang, R., Chen, Y., Wang, G., Tai, Y.W.:
Dense hybrid recurrent multi-view stereo net with dynamic consistency checking.
In: ECCV. pp. 674–689. Springer (2020)
44. Yan, J., Wei, Z., Yi, H., Ding, M., Zhang, R., Chen, Y., Wang, G., Tai, Y.W.:
Dense hybrid recurrent multi-view stereo net with dynamic consistency checking.
In: ECCV. pp. 674–689. Springer (2020)
45. Yang, J., Mao, W., Alvarez, J.M., Liu, M.: Cost volume pyramid based depth
inference for multi-view stereo. In: CVPR. pp. 4877–4886 (2020)
46. Yang, Z., Gao, X., Zhou, W., Jiao, S., Zhang, Y., Jin, X.: Deformable 3d gaus-
sians for high-fidelity monocular dynamic scene reconstruction. arXiv preprint
arXiv:2309.13101 (2023)
47. Yao,Y.,Luo,Z.,Li,S.,Fang,T.,Quan,L.:Mvsnetdepthinferenceforunstructured
multi-view stereo. In: ECCV. pp. 767–783 (2018)
48. Yao, Y., Luo, Z., Li, S., Shen, T., Fang, T., Quan, L.: Recurrent mvsnet for high-
resolution multi-view stereo depth inference. In: CVPR. pp. 5525–5534 (2019)
49. Ye, X., Zhao, W., Liu, T., Huang, Z., Cao, Z., Li, X.: Constraining depth map
geometry for multi-view stereo: A dual-depth approach with saddle-shaped depth
cells. In: ICCV. pp. 17661–17670 (2023)
50. Yu,A.,Ye,V.,Tancik,M.,Kanazawa,A.:pixelNeRF:Neuralradiancefieldsfrom
one or few images. In: CVPR (2021)
51. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable
effectivenessofdeepfeaturesasaperceptualmetric.In:CVPR.pp.586–595(2018)
52. Zheng, S., Zhou, B., Shao, R., Liu, B., Zhang, S., Nie, L., Liu, Y.: Gps-gaussian:
Generalizablepixel-wise3dgaussiansplattingforreal-timehumannovelviewsyn-
thesis. arXiv (2023)
53. Zhu,Z.,Fan,Z.,Jiang,Y.,Wang,Z.:Fsgs:Real-timefew-shotviewsynthesisusing
gaussian splatting (2023)18 T. Liu et al.
A Implementation and Network Details
Implementation Details. Following ENeRF [19], we partition the DTU [1]
dataset into 88 training scenes and 16 test scenes. We train the generalizable
model on four RTX 3090 GPUs using the Adam [17] optimizer, with an initial
learningratesetto5e−4.Thelearningrateishalvedevery50kiterations.During
the training process, we select 2, 3, and 4 source views as inputs with respective
probabilities of 0.1, 0.8, and 0.1. For evaluation, we follow the criteria estab-
lishedinpriorworkssuchasENeRF[19]andMVSNeRF[6].Specifically,forthe
DTUtestset,segmentationmasksareemployedtoevaluateperformance,defined
basedontheavailabilityofground-truthdepthateachpixel.ForRealForward-
facing dataset [25], where the marginal region of images is typically invisible to
input images, we evaluate the 80% area in the center of the images. This eval-
uation methodology is also applied to the Tanks and Temples dataset [18]. The
imageresolutionsoftheDTU,theRealForward-facing,theNeRFSynthetic[26],
and the Tanks and Temples datasets are 512×640, 640×960, 800×800, and
640×960 respectively. As discussed in Sec. 4.3 of the main text, we employ a
consistencychecktofilteroutnoisypointsforhigh-qualityinitialization.Specif-
ically, we apply a dynamic consistency checking algorithm [22,43], the details
of which are provided in Algorithm 1. The predefined thresholds {θ (n)}Nθ
p n=1
are set to {n}Nθ , and {θ (n)}Nθ are set to {n}Nθ . For 3D-GS [16], follow-
4 n=1 d n=1 10 n=1
ing [53], we use COLMAP [30] to reconstruct the point cloud from the working
set (training views) as initialization. Specifically, we employ COLMAP’s auto-
matic reconstruction to achieve the reconstruction of sparse point clouds. Some
examples are shown in Fig. 7.
NetworkDetails.AsmentionedinSec.4.2ofthemaintext,weapplyapooling
networkρtoaggregatemulti-viewfeaturestoobtaintheaggregatedfeaturesvia
f = ρ({f }N ). The implementation details are consistent with [19]: initially,
v i i=1
the mean µ and variance v of {f }N are computed. Subsequently, µ and v are
i i=1
concatenated with each f and an MLP is applied to generate weights. The f
i v
is then blended using a soft-argmax operator, combining the obtained weights
and multi-view features ({f }N ).
i i=1
B Additional Ablation Experiments
Numbers of Views. Existing generalizable Gaussian methods, such as Pixel-
Splat [4] and GPS-Gaussian [52], focus on image pairs as input, while Splatter
Image [32] prioritize single-view reconstruction. Our method is view-agnostic,
capable of supporting varying numbers of views as input. We report the perfor-
mance with varying numbers of input views in Table 6. As the number of views
increases, the model can leverage more scene information, leading to improved
performance.Meanwhile,increasingthenumberofviewsonlyintroducesaslight
increase in computational cost and memory consumption.
Numbers of Sampled Points. In the main text, we apply a pixel-align Gaus-
sian representation, where each pixel is unprojected into 3D space based on theMVSGaussian 19
Algorithm 1: Dynamic Consistency Checking
Input: Camera parameters, Depth maps D and {D }N , predefined
0 i i=1
thresholds {θ (n)}Nθ and {θ (n)}Nθ
p n=1 d n=1
Output: Mask
1 Initialization: Mask←0
2 for i in (1,...,N) do
3 Err pi ←zeros(H,W),Err di ←zeros(H,W)
4 for p in (0,0) to (H−1,W −1) do
5 ξ pi ←∥p−p′∥ 2, ▷ calculate the reprojetcion error between D 0 and D i
6 ξ di ←∥D 0(p)−d′∥ 1/D 0(p)
7 Err pi(p)←ξ pi
8 Err di(p)←ξ di
9 end
10 for n in (1,...,N θ) do
11 Mask ni ←(Err pi <θ p(n))&(Err di <θ d(n))
12 end
13 end
14 for n in (1,...,N θ) do
15 Mask n ←0
16 for i in (1,...,N) do
17 Mask n ←Mask n+Mask ni
18 end
19 Mask n ←(Mask n >n)
20 Mask←Mask∪Mask n
21 end
Fig.7: Visualization of camera calibration and point cloud reconstruction
by COLMAP.20 T. Liu et al.
estimateddepth,correspondingtoa3DGaussian.Analternativeapproachisto
sample M depths centered at the estimated depth map, resulting in each pixel
being unprojected into M Gaussians. As shown in Table 7, increasing the num-
ber of 3D sampled points improves performance but raises computational costs.
To strike a balance between cost and performance, we set M =1.
Depth Analysis. Benefiting from the explicit geometry reasoning of MVS,
our method can produce reasonable depth maps, as illustrated in Fig. 8. The
quantitativeresultsareshowninTable8.Comparedwithpreviousgeneralizable
NeRF methods, our method can achieve the most accurate depth estimation.
Point Cloud Analysis. As discussed in Sec. 5.4 of the main text, different
point cloud aggregation strategies can provide varying-quality initialization for
subsequent per-scene optimization. Here, we report the initial and final num-
bers of point clouds in Table 9 and provide the visual comparison in Fig. 10.
The direct concatenation approach leads to excessively large initialization point
clouds,whichslowdownoptimizationandrenderingspeeds.Thedown-sampling
approach can reduce the total number of points and mitigate noisy points, but
it also leads to a reduction in effective points. Our applied consistency check
strategy can filter out noisy points while retaining effective ones.
InferenceSpeedAnalysis.AsshowninTable1ofthemaintext,theinference
speed of our generalizable model is 21.5FPS. Here, we present the inference
time breakdown result in Table 10. The primary time overhead comes from the
neural network, while the subsequent rendering process incurs minimal time
overhead. Therefore, we discard the neural network component during the per-
scene optimization stage, resulting in a significant increase in speed.
C More Qualitative Results
QualitativeResultsundertheGeneralizationSetting.AsshowninFig.9,
we present qualitative comparisons of the generalization results obtained by dif-
ferentmethods.Ourmethodiscapableofproducinghigher-fidelityviews,partic-
ularly in some challenging areas. For instance, in geometrically complex scenes,
around objects’ edges, and in reflective areas, our method can reconstruct more
details while exhibiting fewer artifacts.
QualitativeResultsunderthePer-sceneOptimizationSetting.Asshown
inFig.11,wepresentthevisualcomparisonafterfine-tuning.Benefitingfromthe
strong initialization provided by our generalizable model, excellent performance
can be achieved with just a short fine-tuning period. The views synthesized by
our method preserve more scene details and exhibit fewer artifacts.
D Per-scene Breakdown
As shown in Tables 11, 14, 13, and 12, we present the per-scene breakdown
results of DTU [1], NeRF Synthetic [26], Real Forward-facing [25], and Tanks
andTemples[18]datasets.Theseresultsalignwiththeaveragedresultspresented
in the main text.MVSGaussian 21
Table 6: The performance of our method with varying numbers of input
views on the DTU, Real Forward-facing, and NeRF Synthetic datasets.
“Mem” and “FPS” are measured under the image resolution of 512×640.
DTU[1] RealForward-facing[25] NeRFSynthetic[26]
Views Mem(GB)↓FPS↑
PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓
2 25.78 0.947 0.095 23.11 0.834 0.175 25.06 0.937 0.079 0.866 24.5
3 28.21 0.963 0.076 24.07 0.857 0.164 26.46 0.948 0.071 0.876 21.5
4 28.43 0.965 0.075 24.46 0.870 0.164 26.50 0.949 0.071 1.106 19.1
Table 7: The performance of different numbers of sampled points on
the DTU, Real Forward-facing, NeRF Synthetic, and Tanks and Temples
datasets. Here, “Samples" represents the number of 3D points sampled along the ray
for each pixel.
DTU[1] RealForward-facing[25] NeRFSynthetic[26] TanksandTemples[18]
Samples Mem(GB)↓FPS↑
PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓
1 28.21 0.963 0.076 24.07 0.857 0.164 26.46 0.948 0.071 23.28 0.877 0.139 0.876 21.5
2 28.26 0.963 0.075 24.20 0.861 0.163 26.64 0.949 0.070 23.20 0.879 0.151 1.508 19.0
Table 8: Quantitative comparison of depth reconstruction on the DTU test
set. MVSNet is trained using depth supervision, while other methods are trained
with only RGB image supervision. “Abs err” refers to the average absolute error, and
“Acc(X)” denotes the percentage of pixels with an error less than X mm.
Referenceview Novelview
Method
Abserr↓Acc(2)↑Acc(10)↑Abserr↓Acc(2)↑Acc(10)↑
MVSNet[47] 3.60 0.603 0.955 - - -
PixelNeRF[50] 49 0.037 0.176 47.8 0.039 0.187
IBRNet[36] 338 0.000 0.913 324 0.000 0.866
MVSNeRF[6] 4.60 0.746 0.913 7.00 0.717 0.866
ENeRF-MVS[19] 3.80 0.823 0.937 4.80 0.778 0.915
ENeRF-NeRF[19] 3.80 0.837 0.939 4.60 0.792 0.917
Ours 3.11 0.866 0.956 3.66 0.838 0.945
Table 9: Comparison of point cloud quantities under different aggregation
strategies on the Real Forward-facing dataset. For downsampling, we employ
widely-used voxel downsampling, with a voxel size set to 2. The iteration number for
all strategies is set to 2k.
Strategy initialpoints(k)finalpoints(k)
directconcatenation 2458 2176
downsampling 836 839
consistencycheck 860 913
Table 10: Time overhead for each module (in milliseconds).
Modules coarsestagefinestage
Featureextractor 1.3
Depthestimation 8.1 7.9
Gaussianrepresentation - 24.0
Gaussianrendering - 4.422 T. Liu et al.
Fig.8: Depth maps visualization.
Ground Truth MVSNeRF ENeRF MatchNeRF Ours
Fig.9: Qualitative comparison of rendering quality with state-of-the-art
methods [6,8,19] under generalization and three views settings.MVSGaussian 23
direct concatentation down-sampling consistency check
Fig.10: Point cloud visualization.24 T. Liu et al.
Ground Truth ENeRF 3D-GS Ours
Fig.11: Qualitative comparison of rendering quality with state-of-the-art
methods [16,19] after per-scene optimization.MVSGaussian 25
Table 11: Quantitative per-scene breakdown results on the DTU test set.
PixelSplat∗ and Ours∗ represent the results obtained with a 2-view input, while the
others are the results obtained with a 3-view input.
Scan #1 #8 #21 #103 #114 #30 #31 #34 #38 #40 #41 #45 #55 #63 #82 #110
Metric PSNR↑
PixelNeRF[50] 21.64 23.70 16.04 16.76 18.40 - - - - - - - - - - -
IBRNet[36] 25.97 27.45 20.94 27.91 27.91 - - - - - - - - - - -
MVSNeRF[6] 26.96 27.43 21.55 29.25 27.99 - - - - - - - - - - -
ENeRF[19] 28.85 29.05 22.53 30.51 28.86 29.20 25.13 26.77 28.61 25.67 29.51 24.83 30.26 27.22 26.83 27.97
MatchNeRF[8] 27.69 27.76 22.75 29.35 28.16 29.16 24.26 25.66 27.52 25.16 28.27 23.94 26.64 29.40 27.65 27.15
Ours 29.6729.6523.2430.6029.2630.1025.9426.8229.2726.1330.33 24.55 31.40 28.46 27.8228.15
PixelSplat∗[4] 14.65 14.72 10.69 16.88 15.31 10.93 13.28 14.70 14.81 13.26 16.09 12.62 15.76 12.18 12.11 16.18
Ours∗ 27.22 26.88 20.49 28.25 27.89 27.55 22.96 25.32 27.13 22.89 27.71 21.78 28.85 27.01 24.64 25.92
Metric SSIM↑
PixelNeRF[50] 0.827 0.829 0.691 0.836 0.763 - - - - - - - - - - -
IBRNet[36] 0.918 0.903 0.873 0.950 0.943 - - - - - - - - - - -
MVSNeRF[6] 0.937 0.922 0.890 0.962 0.949 - - - - - - - - - - -
ENeRF[19] 0.958 0.955 0.916 0.968 0.961 0.981 0.937 0.934 0.946 0.947 0.960 0.948 0.973 0.978 0.971 0.974
MatchNeRF[8] 0.936 0.918 0.901 0.961 0.948 0.974 0.921 0.874 0.902 0.903 0.936 0.934 0.929 0.976 0.966 0.962
Ours 0.9660.9610.9300.9700.9630.9830.9460.9470.9540.9570.9670.9540.9790.9800.9740.976
PixelSplat∗[4] 0.690 0.706 0.492 0.778 0.651 0.782 0.624 0.534 0.513 0.571 0.714 0.541 0.624 0.807 0.769 0.802
Ours∗ 0.950 0.948 0.895 0.963 0.954 0.977 0.919 0.925 0.933 0.928 0.951 0.933 0.967 0.974 0.965 0.966
Metric LPIPS↓
PixelNeRF[50] 0.373 0.384 0.407 0.376 0.372 - - - - - - - - - - -
IBRNet[36] 0.190 0.252 0.179 0.195 0.136 - - - - - - - - - - -
MVSNeRF[6] 0.155 0.220 0.166 0.165 0.135 - - - - - - - - - - -
ENeRF[19] 0.086 0.119 0.107 0.107 0.076 0.052 0.108 0.117 0.118 0.120 0.091 0.077 0.069 0.048 0.066 0.069
MatchNeRF[8] 0.157 0.227 0.149 0.179 0.132 0.085 0.169 0.234 0.220 0.216 0.174 0.127 0.164 0.077 0.093 0.141
Ours 0.0690.1020.0880.0980.0700.0480.0930.0970.0980.1010.0750.0670.0550.0410.0570.057
PixelSplat∗[4] 0.423 0.366 0.471 0.357 0.366 0.329 0.429 0.435 0.493 0.427 0.438 0.488 0.343 0.278 0.326 0.254
Ours∗ 0.087 0.118 0.121 0.114 0.079 0.057 0.126 0.118 0.126 0.132 0.093 0.090 0.074 0.049 0.067 0.079
Table 12: Quantitative per-scene breakdown results on the Tanks and Tem-
ples dataset. PixelSplat∗ and Ours∗ represent the results obtained with a 2-view
input and low-resolution images, while the other generalizable results are obtained
with a 3-view input.
Train Truck
Scene
PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓
IBRNet[36] 22.35 0.763 0.285 19.13 0.755 0.280
MVSNeRF[6] 20.58 0.816 0.278 21.16 0.830 0.242
ENeRF[19] 22.54 0.851 0.204 22.53 0.856 0.163
MatchNeRF[8] 20.44 0.789 0.332 21.16 0.796 0.269
Ours 23.00 0.872 0.154 23.55 0.883 0.124
PixelSplat∗[4] 18.21 0.638 0.252 20.58 0.741 0.195
Ours∗ 23.67 0.864 0.132 22.68 0.834 0.127
NeRF[26] 21.02 0.707 0.538 21.82 0.696 0.577
IBRNet [36] 23.92 0.816 0.229 20.51 0.810 0.212
ft−1.0h
MVSNeRF [6] 21.34 0.831 0.253 22.32 0.850 0.217
ft−15min
ENeRF [19] 24.35 0.884 0.148 24.01 0.885 0.141
ft−1.0h
3D-GS [16] 21.07 0.825 0.255 19.19 0.731 0.384
ft−2min30s
3D-GS ft−15min[16] 24.89 0.897 0.152 22.42 0.838 0.215
Ours ft−90s 24.73 0.910 0.133 24.43 0.896 0.14026 T. Liu et al.
Table 13: Quantitative per-scene breakdown results on the Real Forward-
facing dataset. PixelSplat∗ and Ours∗ represent the results obtained with a 2-view
inputandlow-resolutionimages,whiletheothergeneralizableresultsareobtainedwith
a 3-view input.
Scene Fern FlowerFortressHornsLeavesOrchidsRoom Trex
Metric PSNR↑
PixelNeRF[50] 12.40 10.00 14.07 11.07 9.85 9.62 11.75 10.55
IBRNet[36] 20.83 22.38 27.67 22.06 18.75 15.29 27.26 20.06
MVSNeRF[6] 21.15 24.74 26.03 23.57 17.51 17.85 26.95 23.20
ENeRF[19] 21.92 24.28 30.43 24.49 19.01 17.94 29.75 21.21
MatchNeRF[8] 20.98 23.97 27.44 23.14 18.62 18.07 26.77 20.47
Ours 22.45 25.66 30.46 24.70 19.81 17.86 29.86 21.75
PixelSplat∗[4] 22.41 24.48 27.00 25.02 19.80 18.39 27.56 19.28
Ours∗ 22.47 23.96 30.00 23.97 19.42 17.06 28.59 20.95
NeRF ft−10.2h[26] 23.87 26.84 31.37 25.96 21.21 19.81 33.5425.19
IBRNet [36] 22.64 26.55 30.34 25.01 22.07 19.01 31.05 22.34
ft−1.0h
MVSNeRF [6] 23.10 27.23 30.43 26.35 21.54 20.51 30.12 24.32
ft−15min
ENeRF ft−1.0h[19] 22.08 27.74 29.58 25.50 21.26 19.50 30.07 23.39
3D-GS ft−2min[16] 24.62 23.23 28.94 20.49 15.81 22.76 22.17 19.19
3D-GS [16] 24.58 24.90 29.27 21.90 15.77 22.42 31.45 21.09
ft−10min
Ours ft−45s 24.32 27.66 31.05 30.30 22.53 22.38 33.11 24.51
Metric SSIM↑
PixelNeRF[50] 0.531 0.433 0.674 0.516 0.268 0.317 0.691 0.458
IBRNet[36] 0.710 0.854 0.894 0.840 0.705 0.571 0.950 0.768
MVSNeRF[6] 0.638 0.888 0.872 0.868 0.667 0.657 0.951 0.868
ENeRF[19] 0.774 0.893 0.948 0.905 0.744 0.681 0.971 0.826
MatchNeRF[8] 0.726 0.861 0.906 0.870 0.690 0.675 0.949 0.767
Ours 0.792 0.908 0.948 0.913 0.784 0.701 0.973 0.841
PixelSplat∗[4] 0.754 0.868 0.891 0.884 0.747 0.673 0.952 0.712
Ours∗ 0.787 0.877 0.937 0.896 0.772 0.649 0.962 0.798
NeRF [26] 0.828 0.897 0.945 0.900 0.792 0.721 0.978 0.899
ft−10.2h
IBRNet [36] 0.774 0.909 0.937 0.904 0.843 0.705 0.972 0.842
ft−1.0h
MVSNeRF [6] 0.795 0.912 0.943 0.917 0.826 0.732 0.966 0.895
ft−15min
ENeRF [19] 0.770 0.923 0.940 0.904 0.827 0.725 0.965 0.869
ft−1.0h
3D-GS ft−2min[16] 0.845 0.850 0.918 0.813 0.495 0.850 0.930 0.759
3D-GS [16] 0.841 0.870 0.934 0.820 0.490 0.843 0.975 0.807
ft−10min
Ours ft−45s 0.835 0.937 0.963 0.962 0.871 0.844 0.9860.911
Metric LPIPS↓
PixelNeRF[50] 0.650 0.708 0.608 0.705 0.695 0.721 0.611 0.667
IBRNet[36] 0.349 0.224 0.196 0.285 0.292 0.413 0.161 0.314
MVSNeRF[6] 0.238 0.196 0.208 0.237 0.313 0.274 0.172 0.184
ENeRF[19] 0.224 0.164 0.092 0.161 0.216 0.289 0.120 0.192
MatchNeRF[8] 0.285 0.202 0.169 0.234 0.277 0.325 0.167 0.294
Ours 0.193 0.133 0.096 0.148 0.189 0.275 0.1040.177
PixelSplat∗[4] 0.181 0.158 0.149 0.160 0.214 0.275 0.128 0.258
Ours∗ 0.173 0.124 0.082 0.142 0.182 0.261 0.083 0.167
NeRF [26] 0.291 0.176 0.147 0.247 0.301 0.321 0.157 0.245
ft−10.2h
IBRNet [36] 0.266 0.146 0.133 0.190 0.180 0.286 0.089 0.222
ft−1.0h
MVSNeRF [6] 0.253 0.143 0.134 0.188 0.222 0.258 0.149 0.187
ft−15min
ENeRF [19] 0.197 0.121 0.101 0.155 0.168 0.247 0.113 0.169
ft−1.0h
3D-GS ft−2min[16] 0.154 0.204 0.146 0.338 0.425 0.142 0.222 0.309
3D-GS ft−10min[16] 0.147 0.183 0.121 0.289 0.421 0.145 0.123 0.276
Ours ft−45s 0.161 0.097 0.077 0.091 0.143 0.145 0.0790.113MVSGaussian 27
Table14:Quantitativeper-scenebreakdownresultsontheNeRFSynthetic
dataset.PixelSplat∗ andOurs∗ representtheresultsobtainedwitha2-viewandlow-
resolutioninput,whiletheothergeneralizableresultsareobtainedwitha3-viewinput.
Scene ChairDrums Ficus Hotdog Lego Materials Mic Ship
Metric PSNR↑
PixelNeRF[50] 7.18 8.15 6.61 6.80 7.74 7.61 7.71 7.30
IBRNet[36] 24.20 18.63 21.59 27.70 22.01 20.91 22.10 22.36
MVSNeRF[6] 23.35 20.71 21.98 28.44 23.18 20.05 22.62 23.35
ENeRF[19] 28.29 21.71 23.83 34.20 24.97 24.01 26.62 25.73
MatchNeRF[8] 25.23 19.97 22.72 24.19 23.77 23.12 24.46 22.11
Ours 28.93 22.20 23.55 35.01 24.97 24.49 26.8025.75
PixelSplat∗[4] 16.45 15.40 17.47 13.25 16.86 15.88 16.83 14.06
Ours∗ 27.95 21.20 23.22 33.79 24.23 24.55 24.22 23.54
NeRF[26] 31.07 25.46 29.73 34.63 32.66 30.22 31.81 29.49
IBRNet [36] 28.18 21.93 25.01 31.48 25.34 24.27 27.29 21.48
ft−1.0h
MVSNeRF [6] 26.80 22.48 26.24 32.65 26.62 25.28 29.78 26.73
ft−15min
ENeRF [19] 28.94 25.33 24.71 35.63 25.39 24.98 29.25 26.36
ft−1.0h
3D-GS ft−1min15s[16] 31.90 26.56 34.21 34.21 36.28 29.80 34.56 29.70
3D-GS [16] 31.20 26.26 33.93 34.30 36.10 29.53 34.39 28.90
ft−7min
Ours ft−50s 32.80 25.91 31.54 36.85 35.68 29.83 33.92 31.09
Metric SSIM↑
PixelNeRF[50] 0.624 0.670 0.669 0.669 0.671 0.644 0.729 0.584
IBRNet[36] 0.888 0.836 0.881 0.923 0.874 0.872 0.927 0.794
MVSNeRF[6] 0.876 0.886 0.898 0.962 0.902 0.893 0.923 0.886
ENeRF[19] 0.965 0.918 0.932 0.981 0.948 0.937 0.969 0.891
MatchNeRF[8] 0.908 0.868 0.897 0.943 0.903 0.908 0.947 0.806
Ours 0.969 0.927 0.935 0.984 0.953 0.946 0.9740.895
PixelSplat∗[4] 0.816 0.787 0.857 0.644 0.799 0.764 0.861 0.508
Ours∗ 0.962 0.909 0.920 0.978 0.940 0.940 0.957 0.873
NeRF[26] 0.971 0.943 0.969 0.980 0.975 0.968 0.981 0.908
IBRNet [36] 0.955 0.913 0.940 0.978 0.940 0.937 0.974 0.877
ft−1.0h
MVSNeRF [6] 0.934 0.898 0.944 0.971 0.924 0.927 0.970 0.879
ft−15min
ENeRF ft−1.0h[19] 0.971 0.960 0.939 0.985 0.949 0.947 0.985 0.893
3D-GS ft−1min15s[16] 0.981 0.956 0.986 0.983 0.987 0.970 0.991 0.918
3D-GS ft−7min[16] 0.977 0.951 0.985 0.981 0.987 0.968 0.992 0.909
Ours ft−50s 0.983 0.952 0.981 0.987 0.988 0.970 0.9920.921
Metric LPIPS↓
PixelNeRF[50] 0.386 0.421 0.335 0.433 0.427 0.432 0.329 0.526
IBRNet[36] 0.144 0.241 0.159 0.175 0.202 0.164 0.103 0.369
MVSNeRF[6] 0.282 0.187 0.211 0.173 0.204 0.216 0.177 0.244
ENeRF[19] 0.055 0.110 0.076 0.059 0.075 0.084 0.039 0.183
MatchNeRF[8] 0.107 0.185 0.117 0.162 0.160 0.119 0.060 0.398
Ours 0.036 0.091 0.069 0.040 0.066 0.063 0.0270.179
PixelSplat∗[4] 0.260 0.287 0.282 0.365 0.273 0.309 0.241 0.493
Ours∗ 0.039 0.098 0.066 0.038 0.071 0.050 0.038 0.170
NeRF[26] 0.055 0.101 0.047 0.089 0.054 0.105 0.033 0.263
IBRNet [36] 0.079 0.133 0.082 0.093 0.105 0.093 0.040 0.257
ft−1.0h
MVSNeRF [6] 0.129 0.197 0.171 0.094 0.176 0.167 0.117 0.294
ft−15min
ENeRF ft−1.0h[19] 0.030 0.045 0.071 0.028 0.070 0.059 0.017 0.183
3D-GS ft−1min15s[16] 0.022 0.059 0.016 0.042 0.021 0.041 0.010 0.180
3D-GS ft−7min[16] 0.026 0.062 0.018 0.044 0.021 0.043 0.009 0.172
Ours ft−50s 0.021 0.059 0.022 0.032 0.021 0.038 0.010 0.138