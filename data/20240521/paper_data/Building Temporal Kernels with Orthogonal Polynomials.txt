Building Temporal Kernels with Orthogonal
Polynomials
YanRuPei OlivierCoenen
BrainchipInc. BrainchipInc.
LagunaHills,CA92653 LagunaHills,CA92653
ypei@brainchip.com ocoenen@brainchip.com
Abstract
We introduce a class of models named PLEIADES (PoLynomial Expansion In
AdaptiveDistributedEvent-basedSystems),whichcontainstemporalconvolution
kernels generated from orthogonal polynomial basis functions. We focus on
interfacingthesenetworkswithevent-baseddatatoperformonlinespatiotemporal
classificationanddetectionwithlowlatency.Byvirtueofusingstructuredtemporal
kernels and event-based data, we have the freedom to vary the sample rate of
thedataalongwiththediscretizationstep-sizeofthenetworkwithoutadditional
finetuning.Weexperimentedwiththreeevent-basedbenchmarksandobtainedstate-
of-the-artresultsonallthreebylargemarginswithsignificantlysmallermemory
andcomputecosts. Weachieved: 1)99.59%accuracywith192Kparameterson
theDVS128handgesturerecognitiondatasetand100%withasmalladditional
outputfilter;2)99.58%testaccuracywith277KparametersontheAIS2024eye
trackingchallenge;and3)0.556mAPwith576kparametersonthePROPHESEE
1MegapixelAutomotiveDetectionDataset.
1 Introduction
Temporalconvolutionalnetworks(TCNs)[19]havebeenastapleforprocessingtimeseriesdata
fromspeechenhancement[23]toactionsegmentation[18]. However,inmostcases,thetemporal
kernelisveryshort(usuallysizeof3),makingthenetworkunabletocapturelong-rangetemporal
correlationseffectively. Areasonwhythetemporalkernelisintentionallykeptshortisthedifficulty
fortypicalconvolutionalneuralnetworks(CNNs)tohandlelongtemporalconvolutions,asmaking
alargenumberoftemporalkernelvaluestrainableusuallyleadstoinstabilityduringtraining,and
requiresalargeamountofmemoryforweightstorageduringinference. Onepopularsolutionforthis
hasbeentoparameterizethetemporalkernelfunctionwithasimplemultilayerperceptron(MLP),
whichpromotesstability[29],butoftentradesoffparametercountforincreasedcomputationalload.
Here,weintroduceaparameterizationoftemporalkernels,namedPLEIADES(PoLynomialExpan-
sionInAdaptiveDistributedEvent-basedSystems),thatcaninmanycasesreducethememoryand
computationalcostscomparedtoexplicitconvolutions. Thedesignisfairlymodular, andcanbe
usedasadrop-inreplacementforany1D-likeconvolutionallayers. Infact,weaugmentapreviously
proposed(1+2)Dcausalspatiotemporalnetwork[24]byreplacingitstemporalkernelswiththisnew
polynomialparameterization,allowingittoperformlongtemporalconvolutionseffectively. This
networkcanserveasthebackboneforawiderangeofonlinespatiotemporaltasksrangingfrom
actionrecognitiontoobjectdetection.
Eventhoughournetworkcanbeusedforanyspatiotemporaldata(mosttypicallyvideoscaptured
withconventionalcameras),inthiswork,weinvestigatemainlytheperformanceofournetworkon
event-baseddata(e.g. datacapturedbyaneventcamera). Eventcamerasaresensorsthatgenerate
outputsevents{−1,+1}respondingtoopticalchangesinthescene’sluminance[5],andcangenerate
Preprint.Underreview.
4202
yaM
02
]GL.sc[
1v97121.5042:viXrasparsedataonanincrediblyshorttimescale,usuallyhavingatemporalresolutionof1microsecond.
Eventcamerascanproduceveryrichtemporalfeaturescapturingsubtlemotionpatterns,andallow
forflexibleadjustmentintheeffectiveFPSintoournetworkbyvaryingthebinningwindowsize.
Thismakesitsuitableforustotestnetworkswithlongtemporalkernelssampledatdifferentstep
sizes.
InSection2,wediscussearlierandconcurrentworksthatarerelatedtoourpolynomialparameteri-
zationoftemporalkernels,includingthemodernvariantsofdeepstate-spacemodels. InSection3,
wediscusshowthegenerationanddiscretizationoftemporalkernelsareperformed,andaddition-
allyusingtheeinsumnotationtoperformtheoptimalorderofoperationstoreducememoryand
computationalloads. InSection4,webrieflydescribethenetworkbackbonearchitecture,including
somearchitecturalnoveltiesbesidesourpolynomialtemporalkernels. InSection5,werunthree
event-basedbenchmarks: 1)theIBMDVS128handgesturerecognitiondataset,2)theCVPR2024
AISevent-basedeyetrackingchallenge,3)andthePROPHESEE1megapixelautomotivedetection
dataset(PropheseeGEN4Dataset). WeachievedSOTAresultsonallthreebenchmarks.
Thecodeforbuildingthestructuredtemporalkernels,alongwithapre-trainedPLEIADESnetwork
forevaluationontheDVS128datasetisavailablehere: https://github.com/PeaBrane/Pleiades.
2 RelatedWork
2.1 LongTemporalConvolutionsandParameterizationofKernels
Whentraininganeuralnetworkcontainingconvolutionswithlong(temporal)kernels,itisusuallynot
desirabletoexplicitlyparameterizethekernelvaluesforeachtimestep.Firstofall,theinputdatamay
notbeuniformlysampled,meaningthatthekernelsneedtobecontinuousinnature,makingexplicit
parameterizationimpossible1. Inthiscase,thekernelistreatedasamappingfromaneventtimestamp
toakernelvalue,whereitsmappingisusuallyachievedviaasimpleMLP[27,29,26]. Inthecase
wheretheinputfeaturesareuniformlysampled,explicitparameterizationofthevaluesforeachtime
stepbecomespossible. However,certainregularizationproceduresneedtobeapplied[4],otherwise
thetrainingmaybecomeunstableduetothelargenumberoftrainableweights. Furthermore,this
requires us to store a large number of temporal kernel weights, which may not be favorable in
memory-constrainedenvironments(foredgeormobiledevices).
Theseminalworkproposingtheparameterizationoftemporalkernelsusingorthogonalpolynomials
istheLegendreMemoryUnit(LMU)[34],whereLegendrepolynomials(aspecialcaseofJacobi
polynomials)areused. TheHiPPOformalism[11]thengeneralizedthistootherorthogonalfunctions
includingChebyshevpolynomials,Laguerrepolynomials,andFouriermodes. Later,thissparked
a cornucopia of works interfacing with deep state space models including S4 [12], H3 [3], and
Mamba[10],achievingimpressiveresultsonawiderangeoftasksfromaudiogenerationtolanguage
modeling. ThereareseveralcommonthemesamongthesenetworksthatPLEIADESdifferfrom.
First,thesemodelstypicallyonlyinterfacewith1Dtemporaldata,andusuallytrytoflattenhigh
dimensionaldatainto1Ddatabeforeprocessing[12,38],withsomeexceptions[22]. Second,instead
ofexplicitlyperformingfinite-windowtemporalconvolutions,arunningapproximationoftheeffects
ofsuchconvolutionsareperformed,essentiallyyieldingasystemwithinfiniteimpulseresponses
wheretheeffectivepolynomialstructuresaredistorted[32,11]. Andinthemorerecentworks,the
polynomialstructuresaretenuouslyusedonlyforinitialization,butthenmadefullytrainable. Finally,
thesenetworksmostlyuseanunderlyingdepthwisestructure[14]forlongconvolutions,whichmay
limitthenetworkcapacity,albeitreducingthecomputerequirementofthenetwork.
2.2 SpatiotemporalNetworks
Thereareseveralclassesofneuralnetworksthatcanprocessspatiotemporaldata(i.e.videosandevent
frames). Forexample,aclassofnetworkscombinescomponentsfromspatialconvolutionalnetworks
andrecurrentnetworks,withthemostprominentnetworkbeingConvLSTM[30]. Thesetypesof
modelsinterfacewellwithstreamingspatiotemporaldata,butareoftentimesdifficulttotrain(aswith
recurrentnetworksingeneral). Ontheotherhand,wehaveaclassofeasilytrainablenetworksthat
1Onewouldrequireanuncountable-infinitenumberof“weights”toexplicitlyparameterizeacontinuous
function.
2perform(separable)spatiotemporalconvolutionssuchastheR(2+1)DandP3Dnetworks[33,28],but
theywereoriginallydifficulttoconfigureforonlineinferenceastheydoassumecausality. However,
it is easy to configure the temporal convolutional layers as causal during training, such that the
networkcanperformefficientonlineinferencewithstreamingdataviatheuseofFIFObuffering[24]
orincorporatingspike-based[31]orevent-based[16]processing.
2.3 Event-basedDataandNetworks
AneventcanbesuccinctlyrepresentedasatupleE = (p,x,y,t),wherepdenotesthepolarity,x
andyarethehorizontalandverticalpixelcoordinates,andtisthetime. Acollectionofeventscan
thenbeexpressedasE ={E ,E ,...}. Tofeedevent-baseddataintoconventionalneuralnetworks,
1 2
itisoftennecessarytobinthemintouniformgrids,ortensorsgenerallyshaped(2,H,W,T). Many
differenteventbinningmethodshavebeenexploredinthepast. Thesimplestapproachistosimply
count the number in each bin [20]. Other methods include replacing each event with a fixed or
trainablekernel[37,6]beforeevaluatingthecontributionofthatkerneltoagivenbin. Here,weonly
usethedirectbinningandevent-volumebinningmethods,yieldingthe4dtensor(2,H,W,T)toour
network[24],notingthatweretainthepolaritychannelunlikepreviousworks[37].
Themostpopularclassofevent-basednetworksisspikingneuralnetworks,whichpropagateeventor
spikesignalswithcontinuoustimestampsthroughoutthenetwork,usuallyundertheassumptionof
somefixedinternaldynamicsfortheneurons[7]. Thesenetworkscanbeefficientduringinference,
astypicallytheyonlyneedtopropagate1-bitsignals,buttheyarealsoincrediblydifficulttotrain
withoutspecializedtechniquestoefficientlysimulatetheneuraldynamicsandamelioratethespiking
behaviors. The SLAYER model [31] represents the spike-response model (SRM) as an impulse-
response kernel, which can then be used to convolve temporally with the input signal. It then
usescustomizedCUDAkernelstoimplementthedelayedresponses. Otherworkshaveproposed
ameliorating the spiking behaviors with differentiable functions which can interface easier with
backpropagation(surrogategradients)[15,21],sothespikingnetworkcanbetrainedlikeastandard
neuralnetwork. Notethatanetworkorhardwarecanbefullyevent-basedwithoutnecessarilyusing
spike-basedprocessing,withaprominentexamplebeingBrainchip’sAkidahardware[16]. Such
hardwarecangenerallysupportmostmodernneuralnetworks,withsparsity-awareprocessingwhere
onlynonzeroinputfeaturesareprocessedateachlayer. Thenetworkweproposeinthisworkisa
standardneuralnetwork(notspike-based),butcanefficientlyleverageevent-basedprocessinggiven
sufficientlyhighsparsity2(seeAppendixB.1.1).
3 TemporalConvolutionswithPolynomials
In this section, we discuss: 1) how the temporal kernels are generated by the weighted sums of
polynomialbasisfunctions;2)howthetemporalkernelsarediscretizedtobeusedinaneuralnetwork;
3)howtheconvolutionwiththeinputfeaturetensorcanbeoptimizedwithrespecttotheorderof
operations. Fromhereon,wewillindextheinputchannelwithc,theoutputchannelwithd,the
polynomialdegreeorbasiswithn,thespatialdimensionswithxandy,theinputtimestampwitht,
theoutputtimestampwitht′,andthetemporalkerneltimestampwithτ.
3.1 Buildingtemporalkernelsfromorthogonalpolynomials
JacobipolynomialsP(α,β)(τ)areaclassofpolynomialsthatareorthogonalinthefollowingsense:
n
(cid:90) 1
P(α,β)(τ)P(α,β)(τ)(1−τ)α(1+τ)βdτ =δ h(α,β), (1)
n m nm n
−1
whereδ istheKroneckerdeltathatequatesto1ifn = m, andequatesto0ifn ̸= m, hence
n,m
establishingtheorthogonalitycondition. h(α,β)issomenormalizationconstantthatisnotimportant
n
forourdiscussion. Acontinuousfunctioncanbeparameterizedbytakingtheweightedsumofthese
polynomialsuptoagivendegreeN,wheretheweightingfactors(orcoefficients){γ ,γ ,...,γ }
0 1 N
aretrainable.
2Thiscanbeachievedwithinterfacingwithsparseinputdata(e.g.event-cameradata),sparsitypromoting
activationfunctions(e.g.ReLU),intermediatelossfunctions(e.g.L1activationregularization),andnetwork
quantization.
3Whenthisparameterizationisusedinan1Dconvolutionallayertypicallyinvolvingmultipleinput
and output channels, then naturally we require a set of coefficients for each pairing of input and
outputchannels. Moreformally,ifweindextheinputchannelswithcandtheoutputchannelswithd,
thenthecontinuouskernelconnectingctodcanbeexpressedas
N
(cid:88)
k (τ)= γ P(α,β)(τ). (2)
cd cd,n n
n=0
Wenotethattrainingwithsuchstructuredtemporalkernelswillstrictlyloseusexpressivitycompared
toexplicitlyparameterizedtemporalkernels. However,thereareseveralkeyadvantagesofusing
structuredkernelswhichwilllargelyovercompensateforthelossofexpressivity. First,usingthis
formofimplicitparameterizationwillallownaturalresamplingofthekernelsduringdiscretization,
meaningthatthenetworkcaninterfacewithdatasampledatdifferentrateswithoutadditionalfine-
tuning(seeSection3.2). Second,havingafunctionalbasiswillallowanintermediatesubspaceto
storefeatureprojection,whichinsomecasescanimprovememoryandcomputationalefficiency(see
Section3.3). Finally,sinceaJacobipolynomialbasisisassociatedwithanunderlyingSturm-Louville
equation,thiswillinjectgood“physical”inductivebiasesforournetwork,tomakethetrainingmore
stableandbeguidedtoabetteroptimum(seeSection5.1foranempiricalproof).
3.2 Discretizationofkernels
Inthecurrentimplementationofournetwork,whichinterfaceswithinputsthatarebinnedintime,
weneedtoperformdiscretizationofthetemporalkernelsaccordingly. Onemethodistotakethe
integralofthetemporalkernelsoverthetimebinofinterest.
Westartbydefiningtheantiderivativeofthetemporalkernelsas
(cid:90) τ (cid:90) τ N
(cid:88)
K (τ)= k (τ′)dτ′ = γ P(α,β)(τ′)dτ′
cd cd cd,n n
−1 −1n=0
(3)
N
=(cid:88) γ cd,n (cid:2) P(α,β)(τ)−const.(cid:3) .
(n+1)! n+1
n=0
Ifweneedtonowevaluatetheintegralofk (τ)inthetimebin[τ ,τ +∆τ],whichwecandenote
cd 0 0
asthediscretek [τ ],wecansimplytakethedifference
cd 0
k [τ ]=K (τ +∆τ)−K (τ )
cd 0 cd 0 cd 0
=
(cid:88)N
γ
P n(α +, 1β)(τ 0+∆τ)−P n(α +, 1β)(τ 0)
cd,n (n+1)! (4)
n=0
N
(cid:88) (α,β)
= γ P [τ ],
cd,n n 0
n=0
whereP istheappropriatelydefineddiscretepolynomials,recoveringthesameformasEq.2. Note
thatEq.4canbeconsideredageneralizedmatrixmultiplicationoperationwherethedimensionn
(thepolynomialbasisdimension)iscontracted,discussedfurtherinSection3.3. SeeFig.1fora
schematicrepresentationoftheoperationsofgeneratingtemporalkernelsformultiplechannels.
Under this discretization scheme, it is very easy to perform resampling of the temporal kernels
(eitherdownsamplingorupsampling),tointerfacewithdatasampledatarbitraryrates,suchasusing
variousbin-sizesforevent-baseddata. Notethatthismeansthatthenetworkcanbetrainedatagiven
step-size∆τ,butadaptedtoperforminferenceatdifferentrates(eitherfasterorslower),withoutany
additionaltuning. Onesimplyhastoregeneratethediscretizedbasispolynomialsusingtheequations
abovewiththenew∆τ,andeverythingelseinthenetworkcanbekeptthesame3.
3Thisistrueifthescaleoftheinputdataisinvariantunderresampling.Forevent-baseddataaccumulated
intobins,thismeansthebinvalueshavetoberescaledbyafactorreciprocaltothesizeofthenewbinsize
relativetotheoriginalone.Forexample,ifthebinsizeisdoubled,thenthebinvaluesneedtobeappropriately
halvedtomaintainthesamescale.
4discretized values discretized values
coefficients
channels × = channels
Figure 1: An example of generating discrete temporal kernels for multiple channels, based on
trainablecoefficientsandfixedbasisorthogonalpolynomials. Here,weareconsidering(depthwise)
convolution with 3 channels, 4 basis polynomials, and a kernel size of 5. The shaded areas can
beinterpretedasdiscretizedvalues. Thecoefficientscanbeorganizedasa3×4matrix,andthe
discretizedbasispolynomialscanbeorganizedasa4×5matrix. Thematrixmultiplicationofthe
twothenyieldsthefinaldiscretizedkernelsforthechannelsasa3×5matrix.
3.3 Optimalorderofoperations
Nowthatthekernelsarediscretized,tolessentheburdenofnotation,wecanemploytheEinstein
summationnotation,oreinsum. toreducetheaboveequationto
k =γ P , (5)
cdτ cdn nτ
wheretherepeatingindexnisassumedtobesummedover(orcontracted)ontheright-handside,
correspondingtosummingoverthepolynomialbasis. SeeAppendixA.1foradetaileddescriptionof
thecontractionrules. Ifwenowwishtoconvolvethetemporalkernelk withaspatiotemporal
ijτ
inputfeaturetensoruwhichgivesustheoutputy,theoperationbecomes
y =u M (P)γ , (6)
dxyt′ cxyt ntt′ cdn
where M(P) is the convolution operator matrix, a sparse Toeplitz matrix generated from P (see
AppendixA.3). Ifadepthwiseconvolutionisperformed[14],thentheequationsimplifiesto
y =u M (P)γ , (7)
cxyt cxyt ntt′ cn
asweonlyhaveparallelconnectionsbetweeninputandoutputchannels(bothdenotedbyc). Note
thatthetemporalkernelsdonotinteractwiththespatialindicesxandy,meaningthateachtemporal
kernelisbeingappliedseparatelytoeveryspatialpixel.
Alltheeinsumoperationsareassociativeandcommutative4,sowehavefullfreedomovertheorderof
contractions.Forexample,wecanfirstgeneratethetemporalkernelsfromtheorthogonalpolynomials,
thenperformtheconvolutionswiththeinputfeatures(whichistheorderofoperationsweassumed
bydefault). Butequallyvalid,wecanalsofirstprojecttheinputfeaturesontothebasispolynomials
separately, then weigh and accumulate these results using the polynomial coefficients. This can
bewritten as (u M )γ = x γ = y in einsum form, where x representsthe
cxyt ntt′ cdn cxynt′ cdn dxyt′
intermediateprojections. Notethatthiscontractionorderingfreedomisnotallowedforunstructured
temporalkernels,asthereisnointermediatebasisntoprojectanythingto.
In practice, we select the contraction path based on optimizing memory or computational usage
[9],dependingonthetraininghardwareandcostlimitations. Thisispossiblebecausethememory
and computational costs can be calculated for any contraction path, given the dimensions of the
contractionindices(tensorshapes). SeeAppendixA.2forhowthesecostscanbecalculated. The
choiceoftheoptimalcontractionpathcanbeautomaticallyselectedusingtheopt_einsumlibrary5,
undercertainmodificationsofthecostestimationrules(seeAppendixA.3).
4Matrix multiplications, or tensor contractions in general, are not commutative. However, the einsum
notationrestoresthecommutativitybyexplicitlykeepingtrackofthecontractionindices,sothatthecontraction
operationsareinvariantundercommutation.
5Thislibraryiswell-integratedwithPyTorchsocanbeeasilycalled. However,acaveathereisthatthe
optimalcontractionpathinrealityalsodependsonsoftwaresupportandhardwarearchitecture,andthereare
scenarioswherechoosingthemostcompute-efficientcontractionpathdoesnotnecessarilyleadtospeedups,
especiallyiftheoperationsarememory-bound.
54 NetworkArchitecture
Temporal block Spatial block output
channels
full DWS full DWS
conv conv conv conv (kernel
event polarity Spatio-temporal block size)
8 16 32 48 64 80 96 112 128 256 256 128 3 stride
(10,1,1)(1,3,3) (10,1,1)(1,3,3) (10,1,1)(1,3,3) (10,1,1)(1,3,3) (10,1,1)(1,3,3) (10,1,1)(1,3,3)(1,1,1)
time
1 2 1 2 1 2 1 2 1 2 1 1 1
width
Figure2: Arepresentativenetworkusedforeyetracking. Thebackboneconsistsof5spatiotemporal
blocks. ThedetectionheadisinspiredbyCenterNet,withthemodificationthatthe3×3convolution
ismadedepthwise-separableandatemporallayerisprependedtoit.
Themainnetworkblockisaspatiotemporalconvolutionblock,factorizedasa(1+2)Dconvolution.
In other words, we perform a temporal convolution on each spatial pixel followed by a spatial
convolutiononeachtemporalframe,similarinformtotheR(2+1)Dconvolutionblock[33],ora
previouslyproposed(1+2)Dnetworkforonlineeyetracking[24]. Furthermore,eachofthetemporal
andspatialconvolutionallayerscanbeadditionallyfactorizedasadepthwise-separable(DWS)layer
[14],tofurtherreducethecomputationalcosts. Foreverytemporalkernel(foreverychannelpairing,
everylayer,andeverynetworkvariant),weuseα=−0.25andβ =−0.25fortheJacobipolynomial
basisfunctions,withdegreesupto4.
Severalotherminordesignchoiceswemadeforournetworks(besidespolynomialkernels)include:
• Wekeepeveryoperationinournetworkfullycausal,suchthatthenetworkcanbeeasily
adaptedforonlineinferencewithminimallatency. Importantly,weperformonlycausal
temporalconvolutions.
• After every temporal convolution, we perform a causal Group Normalization [24] with
groups=4. Andaftereveryspatialconvolution,weperformaBatchNormalization. This
strategyofusingamixtureofstaticanddynamicisshowntoimproveperformance[8].
• WeapplyaReLUactivationaftereveryconvolutionlayers, andalsowithineveryDWS
layer. Theactivationfunctionisintentionallykeptsimple,foreaseofimplementationon
mobileoredgedevices,andtopromoteactivationsparsityinthenetwork.
Fortasksrequiringobjectiontrackingordetection(seeSections5.2and5.3),weattachatemporally
smoothed CenterNet detection head to the backbone (see Fig. 2), consisting of a DWS temporal
layer,a3×3DWSspatiallayer,andafinalpointwiselayer[36],withReLUactivationsinbetween
Sinceourbackboneisalreadyspatiotemporalinnatureandcapableofcapturinglong-rangetemporal
correlations,wedonotuseanyadditionalrecurrentheads(e.g. ConvLSTMs)ortemporal-basedloss
functions[25].
5 Experiments
Weconductexperimentsonstandardcomputervisiontaskswithevent-baseddatasets. Forallbaseline
experiments,wepreprocesstheeventdatainto4dtensorsofshape(2,H,W,T),withthe2polarity
channelsretained. GeneraldetailsofdataandtrainingpipelinesaregiveninAppendixB.
5.1 DVS128HandGestureRecognition
TheDVS128datasetcontainsrecordingsof10handgestureclassesperformedbydifferentsubjects
[1],recordedwitha128×128DVScamera.Weuseasimplebackboneconsistingof5spatiotemporal
blocks. ThenetworkarchitectureisalmostthesameasthatshowninFig.2withtheexceptionthat
thedetectionheadisreplacedbyaspatialglobal-averagepoolinglayerfollowedbyasimple2-layer
MLPtoproduceclassificationlogits(technicallyapointwiseConv1Dlayerduringtraining). This
means that the output produces raw predictions at 10 ms intervals, which already by themselves
aresurprisinglyhigh-quality. Withadditionaloutputfilteringonthenetworkpredictions,thetest
accuracycanbepushedto100%(seeTable1). Inaddition,wecomparethePLEIADESnetwork
6
thgiehwithacounterpartthatusesunstructuredtemporalkernels,orsimplyaConv(1+2)Dnetwork[24],
andfindthatPLEIADEShasbetterperformancewithasmallernumberofparameters(duetothe
polynomialcompression).
Table 1: The raw 10-class test accuracy of several networks on the DVS128 dataset. With the
exception of models marked with an asterisk, no output filtering is performed on the networks.
PLEIADESisevaluatedonoutputpredictionswherealltemporallayersprocessnonzerovalidframes,
whichincursanaturalwarm-uplatencyof0.44seconds(seeSection5.1). Additionally,amajority
filterofwindow0.15secondsisappliedtotherawPLEIADESpredictions.
Model Accuracy(%) Parameters
PLEIADES+filtering* 100.00 192K
PLEIADES 99.59 192K
Conv(1+2)D 99.17 196K
ANN-Rollouts[17] 97.16 500K
TrueNorthCNN*[1] 96.59 18M
SLAYER[31] 93.64 -
Unfortunately,previousstudieslackedaunifiedstandardforperformingtheevaluationsonthedataset,
soitisnotentirelyclearthemetricsbeingreported. Inparticular,somenetworksperformonline
inference,andtheothersprocessentirerecordingsegmentsbeforeproducingaprediction. Here,we
produceamoregeneralaccuracyvs. latencyrelationshipforournetworkvariants,astoestablish
multipleParetofrontiersforcomparisons. Inthecontextofthisexperiment,thelatencyissimply
thenumberofeventframes(fromthebeginningoftherecording)multipliedbythebinsize,thatthe
networkhas“seen”beforemakingaprediction.
Note that if we wish to guarantee that every temporal layer is working with nonzero valid input
features, then the network will have a natural latency of equal to (numberoftemporallayers)×
(temporalkernelsize − 1), meaning that the baseline network would have a latency of 440 ms.
However, if we relax this requirement, by assuming implicit zero paddings for missing data (or
equivalently zero initialization of the FIFO buffers), we can then allow the network to perform
inferenceatmuchlowerlatencies. Onthecontrary,iflatencyisnotofprimaryconcern,thenwe
canalsoapplyoutputfilteringtothenetwork[1]toboostperformanceatthecostofhighersystem
latencies. SeeAppendixB.1fordetailsonhowthenetworkcanbeconfiguredtorunatdifferent
latencies.
100 100
90 90
80 80
70 70
60 60
10ms step-size 10ms stepsize
50 10ms step-size with mask 50 20ms stepsize (resampled)
5ms step-size 5ms stepsize (resampled)
5ms step-size with mask free kernels
40 40
10 20 30 50 70 100 200 300 5007001000 100 200 300 400 500 600700 1000
Latency (ms) Latency (ms)
Figure3: (Left)Theaccuracyvs. latencycurvesfordifferentPLEIADESvariantswithakernel
sizeof10butdifferentstepsizes. Amaskingaugmentationisoptionallyusedtorandomlymask
outthestartingframesofdatasetsegmentsduringtraining, inordertostimulatefasterresponses
inthenetwork. (Right)Theaccuracyvs. latencycurvesfordifferentPLEIADESvariantswithan
effectivetemporalwindowof100msforeachtemporallayer,buthavingdifferentstepsizes. The
benchmarknetworkistrainedwithakernelsizeof10andastepsizeof10ms,andtheothervariants
are resampled without additional fine-tuning. A network variant trained without any structured
temporalkernelisalsodisplayedasabaselinereference.
7
)%(
ycaruccA
)%(
ycaruccAThereareseveralwaysto“force”thenetworktorespondfaster. Thenaturalwayistosimplyusea
smallerkernelsizeorbinningwindow(temporalstepsize). Here,wetesttwomodelvariantswith
abinningwindowof5msand10ms,keepingthetemporalkernelsizefixedat10(meaningthat
theformervarianthasashortereffectivetemporalwindow). Anotherwayistorandomlymaskout
framesstartingfromthebeginningoftheinputsegment,toforcethenetworkto“respond”onlyto
themorerecentinputframes,suchthattheeffectiveresponsewindowofthenetworkisshorter6. See
theleftplotofFig.3oftheeffectsofthesetwoapproachesforstimulatingfasternetworkresponse
times. SeeAppendixB.1fordetailsontherandommaskingaugmentation.
Thebenchmarknetworkistrainedwithstep-size∆τ of10ms,whichisalsotheeventdatabin-size.
Here,wechangethestep-sizesto5msand20ms(upsamplinganddownsampling)withoutfine-
tuningthenetwork, andsimplyre-discretizethebasispolynomialsunderthenewstep-sizes(see
Section3.2)andre-bintheeventdata. Notethatthismeansthatthe5msstep-sizenetworkwould
haveakernelsizeof20,andthe20msstep-sizenetworkwouldhaveakernelsizeof5. Weseefrom
therightplotofFig.3thattheaccuracyvs. latencycurvedoesnotvarymuchunderthetime-step
resampling. Inthesameplot,wealsoshowtheperformancefortheConv(1+2)Dbaselinenetwork
withunstructuredkernels,denotedas“freekernels”.
5.2 AIS2024Event-basedEyeTracking
Table2: The10-pixel,5-pixel,and3-pixeltolerancesfortheCVPR2024AISeyetrackingchallenge.
Theperformancesofothermodelsareextractedfrom[35].
Model p10 p5 p3 Parameters
PLEIADES+CenterNet 99.58 97.95 94.94 277K
MambaPupil 99.42 97.05 90.73 -
CETM 99.26 96.31 83.83 7.1M
Conv(1+2)D 99.00 97.97 94.58 1.1M
ERVT 98,21 94.94 87.26 150K
PEPNet 97.95 80.67 49.08 640K
We use the same backbone as the network for the DVS128 hand gesture recognition, but with a
temporalstep-sizeof5ms. Wesimplyreplacethe2-layerMLPclassificationheadwiththeCenterNet
detectionheadandlossadaptedfrom[24]. Notethatweomitpredictionsoftheboundingboxsizes,
andonlypredictcenterpointsofpupilsforthischallenge. SeeFig.2foradrawingofthenetwork
architecture.
5.3 PropheseeGEN4RoadsceneObjectDetection
ThePropheseeGEN4Datasetisaroad-sceneobjectdetectiondatasetcollectedwithamegapixel
eventcamera[25]. Thedatasethasaround14hoursofrecordingwithbothdaytimeandnighttime,
and both rural and urban driving scenarios. It contains 7 classes, but we evaluate the mAP only
on2classes: carsandpedestrians,tobeconsistentwiththeoriginalevaluationpipeline[25]. See
AppendixB.2fordetailsonthemodelarchitectureusedandthetrainingpipeline. Thebackbone
networkisanhourglassnetworkbuiltfromastackofspatiotemporalblockswithatemporalstepsize
of10ms. ThedetectionheadisagaintheCenterNetdetectionheadasdescribedinSection4. Wedo
notuseanynon-maxsuppressionontheboundingboxoutputs,assuggestedintheoriginalCenterNet
pipeline being robust against spurious bounding box predictions, which is further augmented by
the implicit temporal consistency of our network. See Appendix B.2 for details of the network
architecture.
6Note this does not decrease the theoretical latency of the network, but rather improves the prediction
accuracyofthenetworkwhenfedwithfewerinputframes(hencehavingless“effective”latencyforagiven
accuracy). However, itislikelythisaugmentationwilldegradeaccuracywhenmoreframesarefedtothe
newtork,whencomparedtoacounterpartnottrainedwiththismaskingaugmentation.
8Table3: TheperformanceofPLEIADESwithaCenterNetdetectorcomparedtothemodelsintro-
ducedintheoriginalbenchmarkpaper.
Model mAP Parameters
PLEIADES+CenterNet 0.556 0.576M
RED 0.43 24.1M
Gray-RetinaNet 0.43 32.8M
6 Limitations
Since the temporal layers of our network work like a finite-window filter with FIFO buffers, a
practical limitation may be the high memory cost of explicitly buffering the moving window of
recentinputfeatures,whichisworsenedifthetemporalkernelsizeofspatialdimensionsislarge.
However,byvirtueofthepolynomialstructuresofourtemporalkernels,wecanderiveestimates
ofanonlinerunningprojectionofthepastinputfeaturesontothefixedpolynomialbasisfunctions
[32,11]. Thesecompressedcoefficientsareanalogoustointernalstatesinrecurrentnetworks,which
wecanthenperformapointwiseoperationwithourpre-trainedkernelcoefficientstoestimatethe
would-beoutputoftheoriginalfinite-windowconvolution7. Thisissimilarinspirittodeepstate-
spacemodeling[13,12],andtheremaybeawaytopotentiallyintegratesuchdevelopmentsintoour
networkarchitecturetoconvertitintoarecurrentnetworkforbetteronlineinferenceefficiency.
7 Conclusion
Weintroducedaspatiotemporalnetworkwithtemporalkernelsbuiltfromorthogonalpolynomials.
The network achieved state-of-the-art results on all the event-based benchmarks we tested, and
its performance is shown to be stable under temporal resampling without additional fine-tuning.
Currently,thenetworkisconfiguredasastandardneuralnetwork,whichbyitselfisalreadyultra-
light in memory and computational costs. To truly leverage the full advantage of event-based
processing,wecanconsiderusingintermediatelossfunctionstopromoteactivationsparsity[24].
Anotherdirectionistoadapt/convertthisarchitectureintoaspikingsystemviaLebesguesampling
[2]ofthestructuredtemporalkernels, tomakeefficientcomputations/predictionsoffuturespike
timingsateachtemporallayer,forevenfurtheredge-compatibility.
8 Acknowledgement
WewouldliketoacknowledgeNolanArdolino,KristoforCarlson,M.AnthonyLewis,andAnup
Varase(listedinalphabeticalorder)fordiscussingideasandofferinginsightsforthisproject. We
would also like to thank Daniel Endraws for performing quantization studies on the PLEIADES
network,andSasskiaBrüersforhelpwithproducingthefigures.
References
[1] ArnonAmir,BrianTaba,DavidBerg,TimothyMelano,JeffreyMcKinstry,CarmeloDiNolfo,
TapanNayak,AlexanderAndreopoulos,GuillaumeGarreau,MarcelaMendoza,etal. Alow
power,fullyevent-basedgesturerecognitionsystem. InProceedingsoftheIEEEconferenceon
computervisionandpatternrecognition,pages7243–7252,2017.
[2] KarlJohanÅströmandBoBernhardsson. Systemswithlebesguesampling. InDirectionsin
mathematicalsystemstheoryandoptimization,pages1–13.Springer,2002.
[3] Daniel Y Fu, Tri Dao, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher Ré.
Hungryhungryhippos: Towardslanguagemodelingwithstatespacemodels. arXivpreprint
arXiv:2212.14052,2022.
7Thisisanalogoustotheconvolutiontheorem(atleastintheforwarddirection),whichstatesthataconvo-
lutionintheoriginaldomainbecomesapointwiseproductofthecoefficientsinanappropriatelytransformed
domain.
9[4] DanielYFu,ElliotLEpstein,EricNguyen,ArminWThomas,MichaelZhang,TriDao,Atri
Rudra,andChristopherRé. Simplehardware-efficientlongconvolutionsforsequencemodeling.
InInternationalConferenceonMachineLearning,pages10373–10391.PMLR,2023.
[5] GuillermoGallego,TobiDelbrück,GarrickOrchard,ChiaraBartolozzi,BrianTaba,Andrea
Censi,StefanLeutenegger,AndrewJDavison,JörgConradt,KostasDaniilidis,etal. Event-
based vision: A survey. IEEE transactions on pattern analysis and machine intelligence,
44(1):154–180,2020.
[6] DanielGehrig,AntonioLoquercio,KonstantinosGDerpanis,andDavideScaramuzza. End-
to-endlearningofrepresentationsforasynchronousevent-baseddata. InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision,pages5633–5643,2019.
[7] WulframGerstner,WernerMKistler,RichardNaud,andLiamPaninski. Neuronaldynamics:
Fromsingleneuronstonetworksandmodelsofcognition. CambridgeUniversityPress,2014.
[8] ArielGordon,HanhanLi,RicoJonschkowski,andAneliaAngelova. Depthfromvideosinthe
wild: Unsupervisedmonoculardepthlearningfromunknowncameras. InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision,pages8977–8986,2019.
[9] JohnnieGrayandStefanosKourtis. Hyper-optimizedtensornetworkcontraction. Quantum,
5:410,2021.
[10] AlbertGuandTriDao. Mamba: Linear-timesequencemodelingwithselectivestatespaces.
arXivpreprintarXiv:2312.00752,2023.
[11] AlbertGu,TriDao,StefanoErmon,AtriRudra,andChristopherRé. Hippo: Recurrentmemory
with optimal polynomial projections. Advances in neural information processing systems,
33:1474–1487,2020.
[12] AlbertGu,KaranGoel,andChristopherRé.Efficientlymodelinglongsequenceswithstructured
statespaces. arXivpreprintarXiv:2111.00396,2021.
[13] AlbertGu,IsysJohnson,KaranGoel,KhaledSaab,TriDao,AtriRudra,andChristopherRé.
Combiningrecurrent,convolutional,andcontinuous-timemodelswithlinearstatespacelayers.
Advancesinneuralinformationprocessingsystems,34:572–585,2021.
[14] AndrewGHoward, MenglongZhu, BoChen, DmitryKalenichenko, WeijunWang, Tobias
Weyand, MarcoAndreetto, andHartwigAdam. Mobilenets: Efficientconvolutionalneural
networksformobilevisionapplications. arXivpreprintarXiv:1704.04861,2017.
[15] Dongsung Huh and Terrence J Sejnowski. Gradient descent for spiking neural networks.
Advancesinneuralinformationprocessingsystems,31,2018.
[16] DmitryIvanov,AleksandrChezhegov,andDenisLarionov. Neuromorphicartificialintelligence
systems. FrontiersinNeuroscience,16:959626,2022.
[17] AlexanderKugele,ThomasPfeil,MichaelPfeiffer,andElisabettaChicca. Efficientprocessing
of spatio-temporal data streams with spiking neural networks. Frontiers in neuroscience,
14:512192,2020.
[18] Colin Lea, Michael D Flynn, Rene Vidal, Austin Reiter, and Gregory D Hager. Temporal
convolutional networks for action segmentation and detection. In proceedings of the IEEE
ConferenceonComputerVisionandPatternRecognition,pages156–165,2017.
[19] ColinLea,ReneVidal,AustinReiter,andGregoryDHager. Temporalconvolutionalnetworks:
A unified approach to action segmentation. In Computer Vision–ECCV 2016 Workshops:
Amsterdam,TheNetherlands,October8-10and15-16,2016,Proceedings,PartIII14,pages
47–54.Springer,2016.
[20] AnaIMaqueda,AntonioLoquercio,GuillermoGallego,NarcisoGarcía,andDavideScara-
muzza. Event-basedvisionmeetsdeeplearningonsteeringpredictionforself-drivingcars.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
5419–5427,2018.
[21] EmreONeftci,HeshamMostafa,andFriedemannZenke.Surrogategradientlearninginspiking
neuralnetworks: Bringingthepowerofgradient-basedoptimizationtospikingneuralnetworks.
IEEESignalProcessingMagazine,36(6):51–63,2019.
[22] Eric Nguyen, Karan Goel, Albert Gu, Gordon W Downs, Preey Shah, Tri Dao, Stephen A
Baccus,andChristopherRé. S4nd: Modelingimagesandvideosasmultidimensionalsignals
usingstatespaces. arXivpreprintarXiv:2210.06583,2022.
[23] Ashutosh Pandey and DeLiang Wang. Tcnn: Temporal convolutional neural network for
real-timespeechenhancementinthetimedomain. InICASSP2019-2019IEEEInternational
ConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages6875–6879.IEEE,
2019.
10[24] YanRuPei,SasskiaBrüers,SébastienCrouzet,DouglasMcLelland,andOlivierCoenen. A
LightweightSpatiotemporalNetworkforOnlineEyeTrackingwithEventCamera. InProceed-
ingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognitionWorkshops,
2024.
[25] EtiennePerot,PierreDeTournemire,DavideNitti,JonathanMasci,andAmosSironi. Learning
todetectobjectswitha1megapixeleventcamera. AdvancesinNeuralInformationProcessing
Systems,33:16639–16652,2020.
[26] MichaelPoli,StefanoMassaroli,EricNguyen,DanielYFu,TriDao,StephenBaccus,Yoshua
Bengio,StefanoErmon,andChristopherRé. Hyenahierarchy: Towardslargerconvolutional
language models. In International Conference on Machine Learning, pages 28043–28078.
PMLR,2023.
[27] CharlesRQi,HaoSu,KaichunMo,andLeonidasJGuibas. Pointnet: Deeplearningonpoint
setsfor3dclassificationandsegmentation. InProceedingsoftheIEEEconferenceoncomputer
visionandpatternrecognition,pages652–660,2017.
[28] ZhaofanQiu,TingYao,andTaoMei. Learningspatio-temporalrepresentationwithpseudo-3d
residualnetworks. InproceedingsoftheIEEEInternationalConferenceonComputerVision,
pages5533–5541,2017.
[29] DavidWRomero,AnnaKuzina,ErikJBekkers,JakubMTomczak,andMarkHoogendoorn.
Ckconv: Continuouskernelconvolutionforsequentialdata. arXivpreprintarXiv:2102.02611,
2021.
[30] XingjianShi,ZhourongChen,HaoWang,Dit-YanYeung,Wai-KinWong,andWang-chun
Woo. Convolutionallstmnetwork: Amachinelearningapproachforprecipitationnowcasting.
Advancesinneuralinformationprocessingsystems,28,2015.
[31] SumitBShresthaandGarrickOrchard.Slayer:Spikelayererrorreassignmentintime.Advances
inneuralinformationprocessingsystems,31,2018.
[32] AndreasStöckel. Discretefunctionbasesandconvolutionalneuralnetworks. arXivpreprint
arXiv:2103.05609,2021.
[33] DuTran,HengWang,LorenzoTorresani,JamieRay,YannLeCun,andManoharPaluri. A
closerlookatspatiotemporalconvolutionsforactionrecognition. InProceedingsoftheIEEE
conferenceonComputerVisionandPatternRecognition,pages6450–6459,2018.
[34] AaronVoelker,IvanaKajic´,andChrisEliasmith. Legendrememoryunits: Continuous-time
representationinrecurrentneuralnetworks. Advancesinneuralinformationprocessingsystems,
32,2019.
[35] Zuowen Wang, Chang Gao, Zongwei Wu, Marcos V. Conde, Radu Timofte, Shih-Chii Liu,
QinyuChen,etal. Event-BasedEyeTracking.AIS2024ChallengeSurvey. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognitionWorkshops,2024.
[36] Xingyi Zhou, Dequan Wang, and Philipp Krähenbühl. Objects as points. arXiv preprint
arXiv:1904.07850,2019.
[37] Alex Zihao Zhu, Liangzhe Yuan, Kenneth Chaney, and Kostas Daniilidis. Unsupervised
event-basedlearningofopticalflow,depth,andegomotion. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages989–997,2019.
[38] LianghuiZhu,BenchengLiao,QianZhang,XinlongWang,WenyuLiu,andXinggangWang.
Visionmamba: Efficientvisualrepresentationlearningwithbidirectionalstatespacemodel.
arXivpreprintarXiv:2401.09417,2024.
11A OptimalContractionOrderMemoryandCompute
A.1 TheRulesofEinsum
Therulesofcontractinganeinsumexpressioncanbesummarizedasfollows:
• At every contraction step, any two operands can be contracted together, as the einsum
operationisassociativeandcommutative.
• For any indices appearing in the two contracted operands but not the output and other
operands,theseindicescanbesummedawayfortheintermediatecontractionresults.
Anyorderingofcontractions(oracontractionpath)followingtheserulesareguaranteedtoyield
equivalentresults.
Asimpleexampleismultiplyingthreematricestogether,orD =A B C . Inthefirststage,we
il ij jk kl
canfirstchoosetocontractA andB ,whichwouldyieldanintermediateresultofM ,where
ij jk ik
theindexj iscontractedawayasitdoesnotappearintheoutputD . Inthesecondstage,wethen
il
contractM andC toarriveattheoutputD .
ik kl il
Wecanalsochoosetodothecontractionsinanyotherorder,andtheresultwillremainthesame. As
amoreextremeexample,wecanevenperformanouterproductfirstM =A C ,notingthatwe
ijkl ij kl
cannotcontractawaythej andkindicesyetastheyappearinB still. Thecontractionsofj andk
jk
thenneedtobelefttothesecondstagecontraction,D =M B . Intuitively,wefeelthatthisis
il ijkl jk
averysuboptimalwayofdoingmultiplicationofthreematrices,andwecanformalizewhythisisby
lookingatthememoryandcomputecomplexitiesofperformingacontraction.
A.2 MemoryandComputeRequirementsofaContraction
Ifweassumethatwearenotperforminganykernelfusion,andexplicitlymaterializingandsaving
allintermediatetensorsforbackpropagation,thentheextramemoryandcomputeincurredbyeach
contractionstepisasfollows:
• The memory needed to store the intermediate result is simply the size of the tensor, or
equivalentlytheproductofthesizesofitsindices.
• Thecomputeneededtoevaluatetheintermediateresultistheproductofthesizesofall
indicesinvolvedinthecontraction(repeatedindicesarecountedonlyonce).
Again,wecanusetheD =A B C ,whereweassumethattheindexsizestobe8i,j,k,andl.
il ij jk kl
DoingthefirststagecontractionM =A B willrequireikunitsofextramemoryandijkunits
ik ij jk
ofcompute,anddoingthesecondstagecontractionD = M C willrequirenoextramemory
il ik kl
besides that for storing the output and ikl units of compute. This gives us a total extra memory
requirementofikunitsandatotalcomputerequirementofijk+iklunits.
Ontheotherhand,ifweperformtheouterproductM =A C first,thiswillrequireijklunits
ijkl ij kl
of extra memory and ijkl units of compute. The second stage contraction D = M C will
il ijkl kl
requireijklunitsofcompute. Therfore,thetotalmemoryrequirementofthiscontractionpathisijkl
unitsandthetotalcomputerequirementis2ijklunits,bothbeingsignificantlyworsethanthefirst
contractionpath,regardlessofthesizesofthetensors.
Amoresubtleexample(theonlyremainingcontractionpath)iscontractingtheoperandsfrombackto
front,whichwecanverifyrequiresatotalmemoryofjlunitsandatotalcomputeofjkl+ijlunits.
Sothisisonlymorememoryoptimalthanthefirstcontractionpathifjl <ik,andmorecompute
optimalifjkl+ijl < ijk+ikl, whichmaynotbeimmediatelyobviousfrominspectionasthe
optimalitynowdependsonthesizesofthetensors.
Notethatitisassumedthateverytensorinvolvedintheeinsumexpressionrequiresgradientfrom
backpropagation,inthecontextofneuralnetworktraining. Thisiswhyweidentifythesizeofeach
intermediateresultas“additionalmemory”,astheyneedtobestoredastensorsusedforgradient
computation. Inaddition,itisnotdifficulttoseethatforeinsumoperations,thecomputationalcosts
8Fromhereon,wewillconsistentlyusethisabuseofnotationwherethesameletterwillbeusedtodenote
boththeindexandthecorrespondingdimensionalsizeofthatindex.
12requiredforbackpropagationisexactlydoublethatoftheforwardcomputation. Therefore,weonly
needtoconsidertheforwardpassoftheeinsumexpression,whichiswhatwehavebeendoing.
Importantly,notethatthisargumentformemoryandcomputationalcostsofgradientcomputationsis
onlytrueundertheassumptionofreverse-modeautomaticdifferentiation(backpropagation),whichis
whatisusedinalmostallmodernmachinelearningframeworks. Inotherwords,wedonotconsider
moregeneralformsofautomaticdifferentiationsuchastheforward-modevariant. Anotherimportant
noteisthatinpracticeiftheoperationsarememory-bound,thenthecomputationalcostestimates
maynotbeusefulfortrainingtimeestimation.
A.3 ConvolutionwithaParameterizedTemporalKernel
Recall in the main text that the equation for performing a full convolution with a polynomially
parameterizedtemporalkernelis
y =u γ M (P), (8)
dxyt cxyt dnc nt′t
wheretheconvolutionoperatortensorM(P)basedonthediscretizedpolynomialbasisfunctionsP
isgivenbythefollowingToeplitzmatrixforeachdegreeorbasisn(assumingthatkernelsizeis5
withthediscretizedtimestampsbeing{τ ,τ ,τ ,τ ,τ }):
0 1 2 3 4
 
P[τ ] 0 0 0 0 0 ... 0
0
P[τ 1] P[τ 0] 0 0 0 0 ... 0 
 
P[τ ] P[τ ] P[τ ] 0 0 0 ... 0 
 2 1 0 
P[τ ] P[τ ] P[τ ] P[τ ] 0 0 ... 0 
M(P) n = P[τ3 ] P[τ2 ] P[τ1 ] P[τ0 ] P[τ ] 0 ... 0   (9)
 4 3 2 1 0 
 0 P[τ ] P[τ ] P[τ ] P[τ ] P[τ ] ... 0 
 4 3 2 1 0 
 ... 
0 0 ... P[τ ] P[τ ] P[τ ] P[τ ] P[τ ],
4 3 2 1 0
whereforavalid-typeconvolutionweomitthefirstfourrowsofthematrix.
Note that we need to make two modifications to the memory and compute calculation rules in
SectionA.2toadaptforthesparseandToeplitzstructureoftheconvolutionmatrixM. Firstisthat
thememoryrequiredforstoringanytensorcontainingbotht,t′ isguaranteedtobesomeformof
convolutionkernel,soitshouldonlycontributeamemoryfactorofN (thekernelsize)insteadof
τ
N N . Secondisthatanycontractionoftwotensorswithonecontainingtandtheothercontaining
t t′
t,t′isguaranteedtobeatemporalconvolution,soshouldsimilarlycontributeacomputefactorof
N N forvalid-typeconvolutionsandN N forsame-typeconvolutions. Forourimplementation,
t′ τ t τ
wemonkeypatchthesemodificationsintotheopt_einsumpackageusedtoprovidememoryand
FLOPestimationsofeinsumexpressinos.
Table 4: The memory and compute requirements for each possible contraction path, where we
areusingaslightabuseofnotationbyallowingtheindextorepresentthedimensionalsizeofthat
indexinthe“extramemory”and“totalcompute”columns. Theinitialequationcxyt,dnc,nt’tis
alwaysassumed. WeassumeherethatN =N forsimplicity(equivalenttoperformingsame-type
t t′
convolutions).
ContractionPath ExtraMemory TotalCompute
-> dnxyt,nt’t -> dxyt’ dnxyt nxyt(dc+dτ)
-> ncxyt’,dnc -> dxyt’ ncxyt nxyt(cτ +dc)
-> cxyt,dct’t -> dxyt’ cxyt dncτ +dcxytτ
Followingtheprescriptiongivenaboveforcalculatingthememoryandcomputerequirementsfor
performingcontractions,wesummarizetherequirementsofeachcontractionpathforthetemporal
convolutioninTable4. Weonlyconsiderthecaseforfullconvolutions,butthecasefordepthwise
convolutionsisanalagous.
Thefirstcontractionpathfirstcontractstheinputwiththepolynomialcoefficients,thenconvolves
the intermediate result with the basis functions. The second contraction path first convolves the
inputwiththebasisfunctions,thencontractstheintermediateresultwiththepolynomialcoefficients.
Thethirdcontractionpathfirstgeneratesthetemporalkernelsfromthepolynoimalcoefficientsand
13basis functions, then convolves them with the input features. In most cases, we see that the last
contractionpathismostmemoryefficientintypicalcases,orwhenc<dn. However,theoptimality
forcomputationalefficiencyismoresubtleandrequirescomparisonofdn(c+τ),nc(τ +d),and
dcτ.
B DetailsofExperiments
Toconverteventsintoframes,wechoosethebinningwindowtobe10ms,unlessotherwisespecified.
Thistimestepiskeptfixedthroughoutournetwork,aswedonotperformanytemporalresampling
throughthenetwork. FortheDVS128andAIS2024eyetrackingexperiments,weperformsimple
direct binning along with random affine augmentations (with rotation angles up to 10 degrees,
translation factors up to 0.1, and spatial scaling factors up to 1.1). For the Prophesee roadscene
detection,weperformevent-volumebinning(analogoustobilinearinterpolation),withaugmentations
consistingofhorizontalflipsat0.5probabilityandrandomscalingwithfactorsfrom0.9to1.1.
Recallthatournetworkperformsvalid-typecausaltemporalconvolutionswhichreducesthenumber
offramesby(kernelsize−1)pertemporalconvolution. Toavoidintroducinganyimplicittemporal
paddingstoournetwork,weprependextraframes(relativetothelabels)tothebeginningoftheinput
segment. Thetotalnumberofextraframesisthen(numberoftepmorallayers)×(kernelsize−1).
Foralltrainingruns,weusetheAdamWoptimizerwithalearningrateof0.001andweightdecayof
0.001(withPyTorchdefaultkeywords),alongwiththecosinedecaylearningratescheduler(updated
everystep)withawarmupperiodofaround0.01ofthetotaltrainingsteps. Therunsareperformed
withautomaticmixedprecision(float16)withthemodeltorch.compile’d. Alltrainingjobsare
doneonasingleNVIDIAA30GPU.
B.1 DVSHandGestureRecognition
Followingthestandardbenchmarkingprocedureonthisdataset,weonlytrainandevaluateonthe
first1.5secondsofeachtrial, andfilteroutthe“other”classwherethesubjectperformsrandom
gesturesnotfallingintothe10predefinedclasses.
As mentioned, the network requires at least (numberoftepmorallayers)×(kernelsize−1)+1
framesofinputstoguaranteethateverytemporallayerisprocessing“valid”nonzeroinputfeatures.
Togenerateoutputpredictionswithlessthanthisnumberofframes,wecanprependzerostolayer
inputswhereneededtomatchthekernelsize. ThissimulatesthebehaviorofinitializingtheFIFO
buffersofthetemporallayerswithzerosduringonlineinference.
Ifthenumberofinputframesisgreaterthan(numberoftepmorallayers)×(kernelsize−1)+1,
thenthenetworkwillproducemorethanoneoutputpredictions. Ifthelatencybudgetallows,we
canapplyamajorityfiltertotheclassificationpredictionsofthenetwork,suchthatthereismore
confidenceinthepredictions.
Toforcethenetworktorespondfaster,weapplyacustomrandomtemporalmaskingaugmentation
sample-wise with 1/2 probability. The random masking operation works by selecting a frame
uniformlyrandomfromthefirstframetothemiddleframeofthesegment,thentheselectedframe
andeveryframeprecedingitiscompletelysettozero. Thismeansthatthenetworkwillbeartificially
biasedtorespondtomorerecentinputfeaturesduringinference,therebyeffectivelydecreasingits
responselatency.
B.1.1 InputSparsity
Weperform4-bitquantization(withquantizationawaretraining)onthegesturerecognitionnetwork,
andfindthatthenetworkcanachieveveryhighsparsityevenwithoutapplyinganyregularization
loss, given that it interfaces with event-based data and uses ReLU activations (which is sparsity
promoting).
B.2 PropheseeGEN4RoadsceneObjectDetection
Followingarecipesimilartotheoriginalpaper,weremoveboundingboxesthatarelessthan60pixels
inthediagonal. Inaddition,weperformevent-volumebinningwhichsimultaneouslyperformsspatial
14Table 5: Input sparsity for each layer of the gesture recognition network backbone under 4-bit
quantization.
Layer InputSparsity
Conv(1+2)D 0.99
Conv(1+2)D 0.94
Conv(1+2)D 0.94
Conv(1+2)D 0.79
Conv(1+2)D 0.68
resizingfrom(720,1280)to(160,320)andtemporalbinningof10ms. Fordataaugmentations,we
performhorizontalflipsat0.5probabilityandrandomscalingwithfactorsfrom0.9to1.1.
TheCenterNetdetectionheadproducesfeatureframeswhereeachframeisspatiallyshaped(40,80).
Eachpixelcontains7+2+2outputscontaining7classlogits(centerpointheatmaps),thebounding
boxheightandwidthscales,andtheboundingboxcenterpointx andy offsets. Weperformevalua-
c c
tionsdirectlyontheserawpredictions,withoutanyoutputfiltering(e.g. nonon-maxsuppression).
Thenetworkistrainedonthefull7road-sceneclassesofthedataset,andthemAPisevaluatedonthe
carsandpedestriansclasses,atconfidencethresholdsfrom0.05to0.95instepsof0.05andaveraged
usingtrapezoidintegration.
SeeTable.6fordetailsonthemodelarchitecture,whichresembelsanhourglassstructure. Unless
otherwiseindicated,thetemporalkernelsizeisassumedtobe10,causalandvalid-type. Thespatial
kernelsizeisassumedtobe3×3,wherethespatialstridecanbeinferredfromtheoutputshapeof
thelayer. DWSdenotesboththetemporalandspatiallayersintheConv(1+2)Dblockisdepthwise-
separable. TheBottleNeckblockissimilar(butnotidentical)totheIRBblockinMobileNetV2;it
isaresidualblockwiththeresidualpathcontainingthreeConv2DlayerswithReLUactivationsin
between: adepthwise3×3Conv2DfollowedbyapointwiseConv2Dquadruplingthechannels
followedbyapointwiseConv2Dquarteringthechannels.
Beforeeachdecoderlayer,theinputfeatureisfirstupsampledspatiallybyafactorof2×2. Itisthen
summedwithanintermediateoutputfeaturefromanencoderlayerthathasthesamespatialshape.
Tomatchthetemporalshapes,thebeginningframesaretruncatedifnecessary. Theremainingframes
areprojectedwithapointwiseconvolutionallayer(along-rangeskipconnection).
Table6: ThePLEIADES+CenterNetarchitectureusedforthePropheseedataset.
Layer OutputShape Channels
Input (2,T,160,320)
Encoder
Conv(1+2)D (32,T −9, 80,160) 2→16→32
BottleNeck2D (32,T −9, 80,160) 32→32→128→32
DWSConv(1+2)D (64,T −18, 40,80) 32→48→64
BottleNeck2D (64,T −18,40, 80) 64→64→256→64
DWSConv(1+2)D (96,T −27, 20,40) 64→80→96
DWSConv2D (128,T −27, 10,20) 96→128
DWSConv2D (256,T −27, 5,10) 128→256
Decoder
Upsample (256,T −27, 10,20)
DWSConv2D (256,T −27, 10,20) 256→256
Upsample (256,T −27, 20,40)
DWSConv2D (256,T −27, 20,40) 256→256
Upsample (256,T −27, 40,80)
DWSConv2D (256,T −27, 40,80) 256→256
CenterNetHead
DWSConv(1+2)D (128,T −27, 40,80) 256→256→128
pointwiseConv (11,T −27, 40,80) 128→11
15