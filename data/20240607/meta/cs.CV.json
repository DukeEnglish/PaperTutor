[
    {
        "title": "Convolutional Neural Networks and Vision Transformers for Fashion MNIST Classification: A Literature Review",
        "authors": "Sonia BbouzidiGhazala HciniImen JdeyFadoua Drira",
        "links": "http://arxiv.org/abs/2406.03478v1",
        "entry_id": "http://arxiv.org/abs/2406.03478v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03478v1",
        "summary": "Our review explores the comparative analysis between Convolutional Neural\nNetworks (CNNs) and Vision Transformers (ViTs) in the domain of image\nclassification, with a particular focus on clothing classification within the\ne-commerce sector. Utilizing the Fashion MNIST dataset, we delve into the\nunique attributes of CNNs and ViTs. While CNNs have long been the cornerstone\nof image classification, ViTs introduce an innovative self-attention mechanism\nenabling nuanced weighting of different input data components. Historically,\ntransformers have primarily been associated with Natural Language Processing\n(NLP) tasks. Through a comprehensive examination of existing literature, our\naim is to unveil the distinctions between ViTs and CNNs in the context of image\nclassification. Our analysis meticulously scrutinizes state-of-the-art\nmethodologies employing both architectures, striving to identify the factors\ninfluencing their performance. These factors encompass dataset characteristics,\nimage dimensions, the number of target classes, hardware infrastructure, and\nthe specific architectures along with their respective top results. Our key\ngoal is to determine the most appropriate architecture between ViT and CNN for\nclassifying images in the Fashion MNIST dataset within the e-commerce industry,\nwhile taking into account specific conditions and needs. We highlight the\nimportance of combining these two architectures with different forms to enhance\noverall performance. By uniting these architectures, we can take advantage of\ntheir unique strengths, which may lead to more precise and reliable models for\ne-commerce applications. CNNs are skilled at recognizing local patterns, while\nViTs are effective at grasping overall context, making their combination a\npromising strategy for boosting image classification performance.",
        "updated": "2024-06-05 17:32:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03478v1"
    },
    {
        "title": "AD-H: Autonomous Driving with Hierarchical Agents",
        "authors": "Zaibin ZhangShiyu TangYuanhang ZhangTalas FuYifan WangYang LiuDong WangJing ShaoLijun WangHuchuan Lu",
        "links": "http://arxiv.org/abs/2406.03474v1",
        "entry_id": "http://arxiv.org/abs/2406.03474v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03474v1",
        "summary": "Due to the impressive capabilities of multimodal large language models\n(MLLMs), recent works have focused on employing MLLM-based agents for\nautonomous driving in large-scale and dynamic environments. However, prevalent\napproaches often directly translate high-level instructions into low-level\nvehicle control signals, which deviates from the inherent language generation\nparadigm of MLLMs and fails to fully harness their emergent powers. As a\nresult, the generalizability of these methods is highly restricted by\nautonomous driving datasets used during fine-tuning. To tackle this challenge,\nwe propose to connect high-level instructions and low-level control signals\nwith mid-level language-driven commands, which are more fine-grained than\nhigh-level instructions but more universal and explainable than control\nsignals, and thus can effectively bridge the gap in between. We implement this\nidea through a hierarchical multi-agent driving system named AD-H, including a\nMLLM planner for high-level reasoning and a lightweight controller for\nlow-level execution. The hierarchical design liberates the MLLM from low-level\ncontrol signal decoding and therefore fully releases their emergent capability\nin high-level perception, reasoning, and planning. We build a new dataset with\naction hierarchy annotations. Comprehensive closed-loop evaluations demonstrate\nseveral key advantages of our proposed AD-H system. First, AD-H can notably\noutperform state-of-the-art methods in achieving exceptional driving\nperformance, even exhibiting self-correction capabilities during vehicle\noperation, a scenario not encountered in the training dataset. Second, AD-H\ndemonstrates superior generalization under long-horizon instructions and novel\nenvironmental conditions, significantly surpassing current state-of-the-art\nmethods. We will make our data and code publicly accessible at\nhttps://github.com/zhangzaibin/AD-H",
        "updated": "2024-06-05 17:25:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03474v1"
    },
    {
        "title": "Polarization Wavefront Lidar: Learning Large Scene Reconstruction from Polarized Wavefronts",
        "authors": "Dominik ScheubleChenyang LeiSeung-Hwan BaekMario BijelicFelix Heide",
        "links": "http://arxiv.org/abs/2406.03461v1",
        "entry_id": "http://arxiv.org/abs/2406.03461v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03461v1",
        "summary": "Lidar has become a cornerstone sensing modality for 3D vision, especially for\nlarge outdoor scenarios and autonomous driving. Conventional lidar sensors are\ncapable of providing centimeter-accurate distance information by emitting laser\npulses into a scene and measuring the time-of-flight (ToF) of the reflection.\nHowever, the polarization of the received light that depends on the surface\norientation and material properties is usually not considered. As such, the\npolarization modality has the potential to improve scene reconstruction beyond\ndistance measurements. In this work, we introduce a novel long-range\npolarization wavefront lidar sensor (PolLidar) that modulates the polarization\nof the emitted and received light. Departing from conventional lidar sensors,\nPolLidar allows access to the raw time-resolved polarimetric wavefronts. We\nleverage polarimetric wavefronts to estimate normals, distance, and material\nproperties in outdoor scenarios with a novel learned reconstruction method. To\ntrain and evaluate the method, we introduce a simulated and real-world\nlong-range dataset with paired raw lidar data, ground truth distance, and\nnormal maps. We find that the proposed method improves normal and distance\nreconstruction by 53\\% mean angular error and 41\\% mean absolute error compared\nto existing shape-from-polarization (SfP) and ToF methods. Code and data are\nopen-sourced at https://light.princeton.edu/pollidar.",
        "updated": "2024-06-05 17:09:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03461v1"
    },
    {
        "title": "LW-DETR: A Transformer Replacement to YOLO for Real-Time Detection",
        "authors": "Qiang ChenXiangbo SuXinyu ZhangJian WangJiahui ChenYunpeng ShenChuchu HanZiliang ChenWeixiang XuFanrong LiShan ZhangKun YaoErrui DingGang ZhangJingdong Wang",
        "links": "http://arxiv.org/abs/2406.03459v1",
        "entry_id": "http://arxiv.org/abs/2406.03459v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03459v1",
        "summary": "In this paper, we present a light-weight detection transformer, LW-DETR,\nwhich outperforms YOLOs for real-time object detection. The architecture is a\nsimple stack of a ViT encoder, a projector, and a shallow DETR decoder. Our\napproach leverages recent advanced techniques, such as training-effective\ntechniques, e.g., improved loss and pretraining, and interleaved window and\nglobal attentions for reducing the ViT encoder complexity. We improve the ViT\nencoder by aggregating multi-level feature maps, and the intermediate and final\nfeature maps in the ViT encoder, forming richer feature maps, and introduce\nwindow-major feature map organization for improving the efficiency of\ninterleaved attention computation. Experimental results demonstrate that the\nproposed approach is superior over existing real-time detectors, e.g., YOLO and\nits variants, on COCO and other benchmark datasets. Code and models are\navailable at (https://github.com/Atten4Vis/LW-DETR).",
        "updated": "2024-06-05 17:07:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03459v1"
    },
    {
        "title": "FILS: Self-Supervised Video Feature Prediction In Semantic Language Space",
        "authors": "Mona AhmadianFrank GuerinAndrew Gilbert",
        "links": "http://arxiv.org/abs/2406.03447v1",
        "entry_id": "http://arxiv.org/abs/2406.03447v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03447v1",
        "summary": "This paper demonstrates a self-supervised approach for learning semantic\nvideo representations. Recent vision studies show that a masking strategy for\nvision and natural language supervision has contributed to developing\ntransferable visual pretraining. Our goal is to achieve a more semantic video\nrepresentation by leveraging the text related to the video content during the\npretraining in a fully self-supervised manner. To this end, we present FILS, a\nnovel self-supervised video Feature prediction In semantic Language Space\n(FILS). The vision model can capture valuable structured information by\ncorrectly predicting masked feature semantics in language space. It is learned\nusing a patch-wise video-text contrastive strategy, in which the text\nrepresentations act as prototypes for transforming vision features into a\nlanguage space, which are then used as targets for semantically meaningful\nfeature prediction using our masked encoder-decoder structure. FILS\ndemonstrates remarkable transferability on downstream action recognition tasks,\nachieving state-of-the-art on challenging egocentric datasets, like\nEpic-Kitchens, Something-SomethingV2, Charades-Ego, and EGTEA, using ViT-Base.\nOur efficient method requires less computation and smaller batches compared to\nprevious works.",
        "updated": "2024-06-05 16:44:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03447v1"
    }
]