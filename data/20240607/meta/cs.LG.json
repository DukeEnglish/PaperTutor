[
    {
        "title": "Wings: Learning Multimodal LLMs without Text-only Forgetting",
        "authors": "Yi-Kai ZhangShiyin LuYang LiYanqing MaQing-Guo ChenZhao XuWeihua LuoKaifu ZhangDe-Chuan ZhanHan-Jia Ye",
        "links": "http://arxiv.org/abs/2406.03496v1",
        "entry_id": "http://arxiv.org/abs/2406.03496v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03496v1",
        "summary": "Multimodal large language models (MLLMs), initiated with a trained LLM, first\nalign images with text and then fine-tune on multimodal mixed inputs. However,\nthe MLLM catastrophically forgets the text-only instructions, which do not\ninclude images and can be addressed within the initial LLM. In this paper, we\npresent Wings, a novel MLLM that excels in both text-only dialogues and\nmultimodal comprehension. Analyzing MLLM attention in multimodal instructions\nreveals that text-only forgetting is related to the attention shifts from\npre-image to post-image text. From that, we construct extra modules that act as\nthe boosted learner to compensate for the attention shift. The complementary\nvisual and textual learners, like \"wings\" on either side, are connected in\nparallel within each layer's attention block. Initially, image and text inputs\nare aligned with visual learners operating alongside the main attention,\nbalancing focus on visual elements. Textual learners are later collaboratively\nintegrated with attention-based routing to blend the outputs of the visual and\ntextual learners. We design the Low-Rank Residual Attention (LoRRA) to\nguarantee high efficiency for learners. Our experimental results demonstrate\nthat Wings outperforms equally-scaled MLLMs in both text-only and visual\nquestion-answering tasks. On a newly constructed Interleaved Image-Text (IIT)\nbenchmark, Wings exhibits superior performance from text-only-rich to\nmultimodal-rich question-answering tasks.",
        "updated": "2024-06-05 17:59:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03496v1"
    },
    {
        "title": "Grokking Modular Polynomials",
        "authors": "Darshil DoshiTianyu HeAritra DasAndrey Gromov",
        "links": "http://arxiv.org/abs/2406.03495v1",
        "entry_id": "http://arxiv.org/abs/2406.03495v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03495v1",
        "summary": "Neural networks readily learn a subset of the modular arithmetic tasks, while\nfailing to generalize on the rest. This limitation remains unmoved by the\nchoice of architecture and training strategies. On the other hand, an\nanalytical solution for the weights of Multi-layer Perceptron (MLP) networks\nthat generalize on the modular addition task is known in the literature. In\nthis work, we (i) extend the class of analytical solutions to include modular\nmultiplication as well as modular addition with many terms. Additionally, we\nshow that real networks trained on these datasets learn similar solutions upon\ngeneralization (grokking). (ii) We combine these \"expert\" solutions to\nconstruct networks that generalize on arbitrary modular polynomials. (iii) We\nhypothesize a classification of modular polynomials into learnable and\nnon-learnable via neural networks training; and provide experimental evidence\nsupporting our claims.",
        "updated": "2024-06-05 17:59:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03495v1"
    },
    {
        "title": "Solving Poisson Equations using Neural Walk-on-Spheres",
        "authors": "Hong Chul NamJulius BernerAnima Anandkumar",
        "links": "http://arxiv.org/abs/2406.03494v1",
        "entry_id": "http://arxiv.org/abs/2406.03494v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03494v1",
        "summary": "We propose Neural Walk-on-Spheres (NWoS), a novel neural PDE solver for the\nefficient solution of high-dimensional Poisson equations. Leveraging stochastic\nrepresentations and Walk-on-Spheres methods, we develop novel losses for neural\nnetworks based on the recursive solution of Poisson equations on spheres inside\nthe domain. The resulting method is highly parallelizable and does not require\nspatial gradients for the loss. We provide a comprehensive comparison against\ncompeting methods based on PINNs, the Deep Ritz method, and (backward)\nstochastic differential equations. In several challenging, high-dimensional\nnumerical examples, we demonstrate the superiority of NWoS in accuracy, speed,\nand computational costs. Compared to commonly used PINNs, our approach can\nreduce memory usage and errors by orders of magnitude. Furthermore, we apply\nNWoS to problems in PDE-constrained optimization and molecular dynamics to show\nits efficiency in practical applications.",
        "updated": "2024-06-05 17:59:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03494v1"
    },
    {
        "title": "Highway Value Iteration Networks",
        "authors": "Yuhui WangWeida LiFrancesco FaccioQingyuan WuJürgen Schmidhuber",
        "links": "http://arxiv.org/abs/2406.03485v1",
        "entry_id": "http://arxiv.org/abs/2406.03485v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03485v1",
        "summary": "Value iteration networks (VINs) enable end-to-end learning for planning tasks\nby employing a differentiable \"planning module\" that approximates the value\niteration algorithm. However, long-term planning remains a challenge because\ntraining very deep VINs is difficult. To address this problem, we embed highway\nvalue iteration -- a recent algorithm designed to facilitate long-term credit\nassignment -- into the structure of VINs. This improvement augments the\n\"planning module\" of the VIN with three additional components: 1) an \"aggregate\ngate,\" which constructs skip connections to improve information flow across\nmany layers; 2) an \"exploration module,\" crafted to increase the diversity of\ninformation and gradient flow in spatial dimensions; 3) a \"filter gate\"\ndesigned to ensure safe exploration. The resulting novel highway VIN can be\ntrained effectively with hundreds of layers using standard backpropagation. In\nlong-term planning tasks requiring hundreds of planning steps, deep highway\nVINs outperform both traditional VINs and several advanced, very deep NNs.",
        "updated": "2024-06-05 17:46:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03485v1"
    },
    {
        "title": "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead",
        "authors": "Amir ZandiehMajid DaliriInsu Han",
        "links": "http://arxiv.org/abs/2406.03482v1",
        "entry_id": "http://arxiv.org/abs/2406.03482v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03482v1",
        "summary": "Serving LLMs requires substantial memory due to the storage requirements of\nKey-Value (KV) embeddings in the KV cache, which grows with sequence length. An\neffective approach to compress KV cache is quantization. However, traditional\nquantization methods face significant memory overhead due to the need to store\nquantization constants (at least a zero point and a scale) in full precision\nper data block. Depending on the block size, this overhead can add 1 or 2 bits\nper quantized number. We introduce QJL, a new quantization approach that\nconsists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit\nquantization. In contrast to existing methods, QJL eliminates memory overheads\nby removing the need for storing quantization constants. We propose an\nasymmetric estimator for the inner product of two vectors and demonstrate that\napplying QJL to one vector and a standard JL transform without quantization to\nthe other provides an unbiased estimator with minimal distortion. We have\ndeveloped an efficient implementation of the QJL sketch and its corresponding\ninner product estimator, incorporating a lightweight CUDA kernel for optimized\ncomputation. When applied across various LLMs and NLP tasks to quantize the KV\ncache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV\ncache memory usage without compromising accuracy, all while achieving faster\nruntime. Codes are available at \\url{https://github.com/amirzandieh/QJL}.",
        "updated": "2024-06-05 17:42:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03482v1"
    }
]