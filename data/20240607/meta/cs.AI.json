[
    {
        "title": "Wings: Learning Multimodal LLMs without Text-only Forgetting",
        "authors": "Yi-Kai ZhangShiyin LuYang LiYanqing MaQing-Guo ChenZhao XuWeihua LuoKaifu ZhangDe-Chuan ZhanHan-Jia Ye",
        "links": "http://arxiv.org/abs/2406.03496v1",
        "entry_id": "http://arxiv.org/abs/2406.03496v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03496v1",
        "summary": "Multimodal large language models (MLLMs), initiated with a trained LLM, first\nalign images with text and then fine-tune on multimodal mixed inputs. However,\nthe MLLM catastrophically forgets the text-only instructions, which do not\ninclude images and can be addressed within the initial LLM. In this paper, we\npresent Wings, a novel MLLM that excels in both text-only dialogues and\nmultimodal comprehension. Analyzing MLLM attention in multimodal instructions\nreveals that text-only forgetting is related to the attention shifts from\npre-image to post-image text. From that, we construct extra modules that act as\nthe boosted learner to compensate for the attention shift. The complementary\nvisual and textual learners, like \"wings\" on either side, are connected in\nparallel within each layer's attention block. Initially, image and text inputs\nare aligned with visual learners operating alongside the main attention,\nbalancing focus on visual elements. Textual learners are later collaboratively\nintegrated with attention-based routing to blend the outputs of the visual and\ntextual learners. We design the Low-Rank Residual Attention (LoRRA) to\nguarantee high efficiency for learners. Our experimental results demonstrate\nthat Wings outperforms equally-scaled MLLMs in both text-only and visual\nquestion-answering tasks. On a newly constructed Interleaved Image-Text (IIT)\nbenchmark, Wings exhibits superior performance from text-only-rich to\nmultimodal-rich question-answering tasks.",
        "updated": "2024-06-05 17:59:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03496v1"
    },
    {
        "title": "Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends",
        "authors": "Sanjana RamprasadElisa FerracaneZachary C. Lipton",
        "links": "http://arxiv.org/abs/2406.03487v1",
        "entry_id": "http://arxiv.org/abs/2406.03487v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03487v1",
        "summary": "Recent advancements in large language models (LLMs) have considerably\nadvanced the capabilities of summarization systems. However, they continue to\nface concerns about hallucinations. While prior work has evaluated LLMs\nextensively in news domains, most evaluation of dialogue summarization has\nfocused on BART-based models, leaving a gap in our understanding of their\nfaithfulness. Our work benchmarks the faithfulness of LLMs for dialogue\nsummarization, using human annotations and focusing on identifying and\ncategorizing span-level inconsistencies. Specifically, we focus on two\nprominent LLMs: GPT-4 and Alpaca-13B. Our evaluation reveals subtleties as to\nwhat constitutes a hallucination: LLMs often generate plausible inferences,\nsupported by circumstantial evidence in the conversation, that lack direct\nevidence, a pattern that is less prevalent in older models. We propose a\nrefined taxonomy of errors, coining the category of \"Circumstantial Inference\"\nto bucket these LLM behaviors and release the dataset. Using our taxonomy, we\ncompare the behavioral differences between LLMs and older fine-tuned models.\nAdditionally, we systematically assess the efficacy of automatic error\ndetection methods on LLM summaries and find that they struggle to detect these\nnuanced errors. To address this, we introduce two prompt-based approaches for\nfine-grained error detection that outperform existing metrics, particularly for\nidentifying \"Circumstantial Inference.\"",
        "updated": "2024-06-05 17:49:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03487v1"
    },
    {
        "title": "Highway Value Iteration Networks",
        "authors": "Yuhui WangWeida LiFrancesco FaccioQingyuan WuJürgen Schmidhuber",
        "links": "http://arxiv.org/abs/2406.03485v1",
        "entry_id": "http://arxiv.org/abs/2406.03485v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03485v1",
        "summary": "Value iteration networks (VINs) enable end-to-end learning for planning tasks\nby employing a differentiable \"planning module\" that approximates the value\niteration algorithm. However, long-term planning remains a challenge because\ntraining very deep VINs is difficult. To address this problem, we embed highway\nvalue iteration -- a recent algorithm designed to facilitate long-term credit\nassignment -- into the structure of VINs. This improvement augments the\n\"planning module\" of the VIN with three additional components: 1) an \"aggregate\ngate,\" which constructs skip connections to improve information flow across\nmany layers; 2) an \"exploration module,\" crafted to increase the diversity of\ninformation and gradient flow in spatial dimensions; 3) a \"filter gate\"\ndesigned to ensure safe exploration. The resulting novel highway VIN can be\ntrained effectively with hundreds of layers using standard backpropagation. In\nlong-term planning tasks requiring hundreds of planning steps, deep highway\nVINs outperform both traditional VINs and several advanced, very deep NNs.",
        "updated": "2024-06-05 17:46:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03485v1"
    },
    {
        "title": "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead",
        "authors": "Amir ZandiehMajid DaliriInsu Han",
        "links": "http://arxiv.org/abs/2406.03482v1",
        "entry_id": "http://arxiv.org/abs/2406.03482v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03482v1",
        "summary": "Serving LLMs requires substantial memory due to the storage requirements of\nKey-Value (KV) embeddings in the KV cache, which grows with sequence length. An\neffective approach to compress KV cache is quantization. However, traditional\nquantization methods face significant memory overhead due to the need to store\nquantization constants (at least a zero point and a scale) in full precision\nper data block. Depending on the block size, this overhead can add 1 or 2 bits\nper quantized number. We introduce QJL, a new quantization approach that\nconsists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit\nquantization. In contrast to existing methods, QJL eliminates memory overheads\nby removing the need for storing quantization constants. We propose an\nasymmetric estimator for the inner product of two vectors and demonstrate that\napplying QJL to one vector and a standard JL transform without quantization to\nthe other provides an unbiased estimator with minimal distortion. We have\ndeveloped an efficient implementation of the QJL sketch and its corresponding\ninner product estimator, incorporating a lightweight CUDA kernel for optimized\ncomputation. When applied across various LLMs and NLP tasks to quantize the KV\ncache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV\ncache memory usage without compromising accuracy, all while achieving faster\nruntime. Codes are available at \\url{https://github.com/amirzandieh/QJL}.",
        "updated": "2024-06-05 17:42:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03482v1"
    },
    {
        "title": "What is the Best Way for ChatGPT to Translate Poetry?",
        "authors": "Shanshan WangDerek F. WongJingming YaoLidia S. Chao",
        "links": "http://arxiv.org/abs/2406.03450v1",
        "entry_id": "http://arxiv.org/abs/2406.03450v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03450v1",
        "summary": "Machine translation (MT) has historically faced significant challenges when\napplied to literary works, particularly in the domain of poetry translation.\nThe advent of Large Language Models such as ChatGPT holds potential for\ninnovation in this field. This study examines ChatGPT's capabilities in\nEnglish-Chinese poetry translation tasks, utilizing targeted prompts and small\nsample scenarios to ascertain optimal performance. Despite promising outcomes,\nour analysis reveals persistent issues in the translations generated by ChatGPT\nthat warrant attention. To address these shortcomings, we propose an\nExplanation-Assisted Poetry Machine Translation (EAPMT) method, which leverages\nmonolingual poetry explanation as a guiding information for the translation\nprocess. Furthermore, we refine existing evaluation criteria to better suit the\nnuances of modern poetry translation. We engaged a panel of professional poets\nfor assessments, complemented evaluations by using GPT-4. The results from both\nhuman and machine evaluations demonstrate that our EAPMT method outperforms\ntraditional translation methods of ChatGPT and the existing online systems.\nThis paper validates the efficacy of our method and contributes a novel\nperspective to machine-assisted literary translation.",
        "updated": "2024-06-05 16:48:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03450v1"
    }
]