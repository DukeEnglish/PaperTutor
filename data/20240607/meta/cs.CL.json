[
    {
        "title": "Wings: Learning Multimodal LLMs without Text-only Forgetting",
        "authors": "Yi-Kai ZhangShiyin LuYang LiYanqing MaQing-Guo ChenZhao XuWeihua LuoKaifu ZhangDe-Chuan ZhanHan-Jia Ye",
        "links": "http://arxiv.org/abs/2406.03496v1",
        "entry_id": "http://arxiv.org/abs/2406.03496v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03496v1",
        "summary": "Multimodal large language models (MLLMs), initiated with a trained LLM, first\nalign images with text and then fine-tune on multimodal mixed inputs. However,\nthe MLLM catastrophically forgets the text-only instructions, which do not\ninclude images and can be addressed within the initial LLM. In this paper, we\npresent Wings, a novel MLLM that excels in both text-only dialogues and\nmultimodal comprehension. Analyzing MLLM attention in multimodal instructions\nreveals that text-only forgetting is related to the attention shifts from\npre-image to post-image text. From that, we construct extra modules that act as\nthe boosted learner to compensate for the attention shift. The complementary\nvisual and textual learners, like \"wings\" on either side, are connected in\nparallel within each layer's attention block. Initially, image and text inputs\nare aligned with visual learners operating alongside the main attention,\nbalancing focus on visual elements. Textual learners are later collaboratively\nintegrated with attention-based routing to blend the outputs of the visual and\ntextual learners. We design the Low-Rank Residual Attention (LoRRA) to\nguarantee high efficiency for learners. Our experimental results demonstrate\nthat Wings outperforms equally-scaled MLLMs in both text-only and visual\nquestion-answering tasks. On a newly constructed Interleaved Image-Text (IIT)\nbenchmark, Wings exhibits superior performance from text-only-rich to\nmultimodal-rich question-answering tasks.",
        "updated": "2024-06-05 17:59:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03496v1"
    },
    {
        "title": "Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends",
        "authors": "Sanjana RamprasadElisa FerracaneZachary C. Lipton",
        "links": "http://arxiv.org/abs/2406.03487v1",
        "entry_id": "http://arxiv.org/abs/2406.03487v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03487v1",
        "summary": "Recent advancements in large language models (LLMs) have considerably\nadvanced the capabilities of summarization systems. However, they continue to\nface concerns about hallucinations. While prior work has evaluated LLMs\nextensively in news domains, most evaluation of dialogue summarization has\nfocused on BART-based models, leaving a gap in our understanding of their\nfaithfulness. Our work benchmarks the faithfulness of LLMs for dialogue\nsummarization, using human annotations and focusing on identifying and\ncategorizing span-level inconsistencies. Specifically, we focus on two\nprominent LLMs: GPT-4 and Alpaca-13B. Our evaluation reveals subtleties as to\nwhat constitutes a hallucination: LLMs often generate plausible inferences,\nsupported by circumstantial evidence in the conversation, that lack direct\nevidence, a pattern that is less prevalent in older models. We propose a\nrefined taxonomy of errors, coining the category of \"Circumstantial Inference\"\nto bucket these LLM behaviors and release the dataset. Using our taxonomy, we\ncompare the behavioral differences between LLMs and older fine-tuned models.\nAdditionally, we systematically assess the efficacy of automatic error\ndetection methods on LLM summaries and find that they struggle to detect these\nnuanced errors. To address this, we introduce two prompt-based approaches for\nfine-grained error detection that outperform existing metrics, particularly for\nidentifying \"Circumstantial Inference.\"",
        "updated": "2024-06-05 17:49:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03487v1"
    },
    {
        "title": "BIPED: Pedagogically Informed Tutoring System for ESL Education",
        "authors": "Soonwoo KwonSojung KimMinju ParkSeunghyun LeeKyuseok Kim",
        "links": "http://arxiv.org/abs/2406.03486v1",
        "entry_id": "http://arxiv.org/abs/2406.03486v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03486v1",
        "summary": "Large Language Models (LLMs) have a great potential to serve as readily\navailable and cost-efficient Conversational Intelligent Tutoring Systems (CITS)\nfor teaching L2 learners of English. Existing CITS, however, are designed to\nteach only simple concepts or lack the pedagogical depth necessary to address\ndiverse learning strategies. To develop a more pedagogically informed CITS\ncapable of teaching complex concepts, we construct a BIlingual\nPEDagogically-informed Tutoring Dataset (BIPED) of one-on-one, human-to-human\nEnglish tutoring interactions. Through post-hoc analysis of the tutoring\ninteractions, we come up with a lexicon of dialogue acts (34 tutor acts and 9\nstudent acts), which we use to further annotate the collected dataset. Based on\na two-step framework of first predicting the appropriate tutor act then\ngenerating the corresponding response, we implemented two CITS models using\nGPT-4 and SOLAR-KO, respectively. We experimentally demonstrate that the\nimplemented models not only replicate the style of human teachers but also\nemploy diverse and contextually appropriate pedagogical strategies.",
        "updated": "2024-06-05 17:49:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03486v1"
    },
    {
        "title": "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead",
        "authors": "Amir ZandiehMajid DaliriInsu Han",
        "links": "http://arxiv.org/abs/2406.03482v1",
        "entry_id": "http://arxiv.org/abs/2406.03482v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03482v1",
        "summary": "Serving LLMs requires substantial memory due to the storage requirements of\nKey-Value (KV) embeddings in the KV cache, which grows with sequence length. An\neffective approach to compress KV cache is quantization. However, traditional\nquantization methods face significant memory overhead due to the need to store\nquantization constants (at least a zero point and a scale) in full precision\nper data block. Depending on the block size, this overhead can add 1 or 2 bits\nper quantized number. We introduce QJL, a new quantization approach that\nconsists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit\nquantization. In contrast to existing methods, QJL eliminates memory overheads\nby removing the need for storing quantization constants. We propose an\nasymmetric estimator for the inner product of two vectors and demonstrate that\napplying QJL to one vector and a standard JL transform without quantization to\nthe other provides an unbiased estimator with minimal distortion. We have\ndeveloped an efficient implementation of the QJL sketch and its corresponding\ninner product estimator, incorporating a lightweight CUDA kernel for optimized\ncomputation. When applied across various LLMs and NLP tasks to quantize the KV\ncache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV\ncache memory usage without compromising accuracy, all while achieving faster\nruntime. Codes are available at \\url{https://github.com/amirzandieh/QJL}.",
        "updated": "2024-06-05 17:42:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03482v1"
    },
    {
        "title": "MODABS: Multi-Objective Learning for Dynamic Aspect-Based Summarization",
        "authors": "Xiaobo GuoSoroush Vosoughi",
        "links": "http://arxiv.org/abs/2406.03479v1",
        "entry_id": "http://arxiv.org/abs/2406.03479v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03479v1",
        "summary": "The rapid proliferation of online content necessitates effective\nsummarization methods, among which dynamic aspect-based summarization stands\nout. Unlike its traditional counterpart, which assumes a fixed set of known\naspects, this approach adapts to the varied aspects of the input text. We\nintroduce a novel multi-objective learning framework employing a\nLongformer-Encoder-Decoder for this task. The framework optimizes aspect number\nprediction, minimizes disparity between generated and reference summaries for\neach aspect, and maximizes dissimilarity across aspect-specific summaries.\nExtensive experiments show our method significantly outperforms baselines on\nthree diverse datasets, largely due to the effective alignment of generated and\nreference aspect counts without sacrificing single-aspect summarization\nquality.",
        "updated": "2024-06-05 17:32:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03479v1"
    }
]