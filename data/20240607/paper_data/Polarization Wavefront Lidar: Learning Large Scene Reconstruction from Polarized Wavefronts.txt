Polarization Wavefront Lidar:
Learning Large Scene Reconstruction from Polarized Wavefronts
DominikScheuble1,2* ChenyangLei5 Seung-HwanBaek4 MarioBijelic3,5 FelixHeide3,5
∗
1Mercedes-BenzAG 2TUDarmstadt 3TorcRobotics 4POSTECH 5PrincetonUniversity
Abstract fromthescene. Theemittedlightistypicallypolarizedand
thepolarizationchangesuponreflectiondependingonsur-
Lidar has become a cornerstone sensing modality for facenormalsandmaterialproperties[3,35]. Off-the-shelf
3D vision, especially for large outdoor scenarios and au- lidar sensors only detect intensity, as such, ignore the ad-
tonomousdriving. Conventionallidarsensorsarecapable ditional polarization information. In this paper, we revisit
of providing centimeter-accurate distance information by the abandoned geometric and material information in the
emittinglaserpulsesintoasceneandmeasuringthetime- polarizationstateforthereconstructionoflargeautomotive
of-flight (ToF) of the reflection. However, the polarization scenesupto100mrange.
of the received light that depends on the surface orienta- Although the benefit of polarization has been investi-
tion and material properties is usually not considered. As gated extensively in other fields [14, 35, 38], polarization
such,thepolarizationmodalityhasthepotentialtoimprove is largely unexplored in the context of lidar sensing in vi-
scenereconstructionbeyonddistancemeasurements.Inthis sion and robotics. Specifically, lidar and polarization have
work, we introduce a novel long-range polarization wave- been explored in meteorology [48, 49, 51], biology [28]
front lidar sensor (PolLidar) that modulates the polariza- and maritime sciences [57] by analyzing the depolariza-
tionoftheemittedandreceivedlight. Departingfromcon- tion. Besides,alineofworkinvestigatespolarizationcam-
ventional lidar sensors, PolLidar allows access to the raw eraimagesforshapeestimation [1,10,29,36,46], stereo
time-resolvedpolarimetricwavefronts. Weleveragepolari- depth estimation [54], depth completion [59], and dehaz-
metric wavefronts to estimate normals, distance, and ma- ing [5, 21, 22, 50, 56]. These methods have in common
terialpropertiesinoutdoorscenarioswithanovellearned that they utilize passive sensors, making them ineffective
reconstruction method. To train and evaluate the method, at night time. Only a few existing works [5, 6] use active
weintroduceasimulatedandreal-worldlong-rangedataset polarimetric ToF systems for scene reconstruction. How-
withpairedrawlidardata,groundtruthdistance,andnor- ever,theseexistingtime-resolvedpolarizationmethodsare
mal maps. We find that the proposed method improves designedforindoorsceneswithobject-levelcontents, pro-
normalanddistancereconstructionby53%meanangular hibitingthemeasurementoflargeoutdoorscenes.
error and 41% mean absolute error compared to existing
Inthispaper,weintroduceanovelsensingmodalitythat
shape-from-polarization(SfP)andToFmethods. Codeand
combinespolarizationanalysiswithlidarsensorsforscene
dataareopen-sourcedhere1.
reconstruction, illustrated in Fig. 1. We devise a polariza-
tionwavefrontlidarsensor(PolLidar)thatiscapableofop-
eratinginoutdoorsettings. Theproposedsensormodulates
1.Introduction the polarization of the emitted and received light. In con-
trasttopolarizationcameras,thePolLidarisnotlimitedto
Sensing and reconstructing large scenes is crucial for
adiscretenumberofpolarizationstatesbutcanmeasurepo-
safety-critical applications in autonomous driving [17, 53,
larizationcontinuouslybyfinelycontrollingwaveplatesand
60], drones [33, 45], remote sensing [20, 58], scene un-
linearpolarizersbasicallyabletoperformfullellipsometry
derstanding [8, 24, 55] and dataset generation [11, 12, 37]
[13,25]. Thesensorreadstherawwavefrontsignaldirectly
for 3D vision. Scanning lidar sensors have been broadly
asavoltagefromtheAvalanchePhotodiode(APD).Weem-
adopted as a cornerstone sensing modality that provides
ploythissensingtechniquetocaptureapolarizationdataset
precise distance information. These sensors operate by
consisting of long-range automotive scenes to assess the
measuringtheToFoflaserpulsesemittedintoandreturned
benefitofpolarization. Alongwiththerawwavefronts,we
providepairwisegroundtruthdistanceandnormalinforma-
*Theseauthorscontributedequallytothiswork.
1https://light.princeton.edu/pollidar/ tionfromaVelodyneVLS-128referencesensor,seeFig.2.
4202
nuJ
5
]VC.sc[
1v16430.6042:viXraReceiver Wavefront Wavefront
Reconstruction
APD Bias
Protoype
Time
Point Cloud
with Normals
DMD
Emitter
Scene
MEMS
ToF
HWP
QWP
Laser power LP
Setup Laser Optics
Filter Normals
Figure1.PolLidarsensingandscenereconstruction.WedesignourPolLidarsensorwithauniquecapability:itmodulatesthepolariza-
tionoflightduringboththeemissionandreceptionstages. Tothisend,aHWPandQWPareusedtoemitlightofacertainpolarization,
whereasaQWPandLPareusedtodeterminethepolarizationofthereceivedlight.Tocapturethereceivedsignal,weemployanADCat
theAPDforpreciserawwavefrontmeasurement. ThisisunliketraditionalLidarsystemsthatprimarilyfocusondistancemeasurements
anddonotprovideboththepolarizationcharacteristicsandthewavefrontofthelight.Subsequently,anovellidargeometryreconstruction
approachpredictingnormals,distanceandmaterialpropertiesisintroducedinSec.4.
Torecoverscenepropertiesfrompolarizationwavefront 2.RelatedWork
measurements, we combine the proposed sensor with a
novelreconstructionapproachthatoperatesontherawpo- Polarization Lidars. Polarization lidar sensors have been
larimetricwavefronts. Theproposedreconstructionmethod explored in diverse fields. Early studies, such as Schot-
uses the polarized wavefronts to estimate surface normals land’s[51],leveragedthesepolarimetricmeasurementsfor
and accurate distance. The estimated normals can then cloud property analysis, while approaches as [51] study
be utilized for predicting material properties, including in- thebioaerosolsintheatmosphere[28]andin[57]thescat-
dex of refraction, diffuse and specular albedo, and surface tering coefficient of oceans are measured using polariza-
roughness. For training, we extend the CARLA simula- tion lidar [57]. Recently, Baek et al. [5, 6] combine a
tor[26]witharealisticpolarizationmodeloflighttogener- prototypicalpolarizationlidarwithatemporal-polarimetric
ateasyntheticlong-rangepolarizationdataset. BRDFmodeltoachieveaccuratescenereconstruction.Jeon
et al. [31] propose a polarimetric indirect ToF imaging
We assess the method with experiments on both syn-
method that utilizes polarization to improve depth estima-
thetic data and real-world data. We find that the proposed
tionsthroughscatteringmedia. However,theimagingtech-
method improves distance estimation by 41% mean abso-
nique, i.e., thedesignoftheopticalpathin [5,6], andthe
luteerrorcomparedtoconventionalToFmethodsand53%
indirectToFmeasurementprinciplein[31],fundamentally
meanangularerrorfornormalestimationcomparedtoSfP
limit these devices to indoor usage. In contrast, the pro-
andpointcloudbaselinesonautomotivescenes.
posedmethodisthefirstdesignedforscenereconstruction
inlargeoutdoorscenesupto100m.
Specifically,wemakethefollowingcontributions
SceneReconstructionwithPassivePolarizationSensors.
• We devise a polarization wavefront lidar sensing ap- Exploiting the relationship between the polarization of re-
proach that measures time-resolved polarization proper- flected light and the surface normals, shape from polar-
ties to recover precise distance and normals for long- ization (SfP) methods have achieved scene reconstruction
rangescenariosasfoundinautomotivescenes. from polarization images captured by linear-polarization
• Weproposeaneuralreconstructionapproachfordistance cameras [2, 39, 43]. Early SfP methods focus on esti-
andnormalsoperatingdirectlyonrawwavefrontsinstead matingthesurfacenormalofobjectsunderassumptionsof
ofpost-processedToFpeaks. either pure specular reflection [43] or pure diffuse reflec-
• We introduce the first automotive polarization lidar tion[1,39]. Thesemethodsusuallyassumeanunpolarized
dataset,consistingofreal-worlddataandsimulationdata. lightsourceandsufferfrompolarizationambiguityissues.
Wevalidateourmodelwiththeproposeddatasetforlong- Recentworks[3,30,34,36]leveragedeeplearningtosolve
range distance estimation and dense normal reconstruc- theambiguityproblem. Bytrainingonreal-worlddatasets,
tion. Comparedtobaselinemethods,ourmodelimproves the network can better distinguish the ambiguity and mit-
distance and normal reconstruction by 41% mean abso- igate the need for inputting unknown material properties
luteerrorand53%meanangularerror,respectively. suchasrefractiveindex. Baeketal.[4]performjointopti-
ADC
. . .
ytisnetnIPolLidar Ground Truth
mizationofappearance,normals,andrefractiveindex. De-
Camera Intensity ToF ToF Normal
schaintreetal.[16]proposealearning-basedinverselearn-
ing framework with the front-flash illumination. Dave et
al.[15]combinepolarizationwithimplicitneuralrepresen-
tationstocollectivelyreconstructthegeometryandappear-
ancefrommultipleimages. Ingeneral,thesereconstruction
methods focus on scenes with few objects that are placed
to exhibit strong polarization cues with a high degree of
polarization (DoP). In outdoor scenes, however, the DoP
varies significantly limiting the quality of the reconstruc-
tion to high DoP regions. The proposed method allows to
exploittheexploitationofpolarizationcuesinbothhighand
lowDoPregions.
In [32, 59], passive polarization sensors are combined
with other imaging modalities. Kadambi et al. [32] utilize
normals from polarization to enhance the details of depth
fromaMicrosoftKinectsensor. Yoshidaetal.[59]usepo- Intensity [V] Distance [m] Normals
0 0.23 0.45 0 15 30
larizationtofillinmissingregionsinthedepthmaps. Fur-
thermore, polarization cues are leveraged to augment low- Figure2.PolLidardataset.Wecapturealong-rangepolarimetric
lidardatasetintypicalautomotivesceneswithobjectdistancesup
quality depth maps from two-view stereo [23, 62], recip-
to100m.Ontheleftisacamerareferenceimage,followedbyPol-
rocal image pairs [18], multi-view stereo [14, 40], or li-
Lidarintensityforthehorizontallypolarizedstateθ{1,2,3,4} = 0
dars [52]. Recently, Huang et al. [27] and Tian et al. [54] 1
andsensor-derivedToFdistances. Ontheright,groundtruthdata
proposestereopolarimetricmethods,whichutilizetwopo-
fromaccumulatedscansfromaVelodyneVLS-128lidar,provid-
larizationimagestosolvetheambiguityinSfP.However,as
ingToFandsurfacenormalsforcomparison.
passivesensorsaredependentonambientlight,thesemeth-
ods struggle in low-light conditions. The proposed active
sensing method allows for accurate reconstructions inde- togalvo-mirrorseffectivelyreducinglightloss.
pendentlyofambientillumination. To make outdoor applications possible, we operate at a
wavelengthof1064nmandaddedanarrowbandpassfilter,
3.PolarimetricWavefrontLidar
leaving the visible band targeted by existing work. These
modifications are essential to suppress ambient light and
Inenvironmentalscience,polarimetriclidarsareemployed
render the emitted light invisible to the human eye, align-
forgatheringpolarizationdataoverextensiveranges,often
ingwithautomotiveilluminationstandards. Themaximum
spanning several kilometers but with a trade-off in spatial
poweroutputadherestoClass-1eyesafetyregulations.The
resolution. Contrarily, polarimetric lidars for scene recon-
laser power remains adjustable according to scenario re-
struction usually support high spatial resolution, yet their
quirements, offering a balance between achieving maxi-
range is limited to a few meters. The proposed PolLidar
mumrangeandminimizingsaturationwhichoffersalevel
sensorinFig.1uniquelybridgestheseapplicationdomains.
ofcontroltypicallynotavailableinoff-the-shelflidars.
Itisdesignedtoallowforabalancedperformanceoptimal
forbothlong-rangecapabilitiesupto223mandhighspa- On the emission side, the horizontally polarized laser
tialresolutionof150rowsand236columnsovera23.95° lightundergoesmodulationbypassingthroughbothahalf
and 31.53° vertical and horizontal field-of-view, making it (HWP)andquarter-waveplate(QWP).Thereceivingmod-
particularlysuitableforautonomousdrivingapplications. uleisdesignedtocapturechangesinpolarization,facilitated
Our sensor differs from the ToF systems described in byasequenceofaQWP,alinearpolarizer(LP),andaband-
[5,6]. Specifically,weproposeseparatemodulesforemis- passfilter,asillustratedinFig.1. Therotationofeachpo-
sion and reception instead of a shared optical setup. This larizationelementisfinelyadjustableinincrementsof0.01
separationallowsforalargeropticalapertureineachmod- degrees. Weuseaback-sideilluminatedAvalanchePhoto-
ule, enhancing optical sensitivity and extending the oper- diode (APD) with an adjustable bias for sensitivity adjust-
ational range in outdoor scenarios. Instead of the galvo- mentsandreadtherawsignalwithanattachedPCIe-5764
mirror used in [5], a MEMS micro-mirror is used in the FlexRIO-Digitizeranalog-to-digitalconverter(ADC),sam-
emitter for scene scanning. The receiver employs a dig- plingat1Gs/s. Thisallowsustomeasurerawwavefronts
ital micro-mirror device (DMD) following [47] to selec- withalengthof1488binsof1nswidth,i.e.,15cmperbin,
tively deflect the returning light towards the photodiode. andarangeof223meters.
UsingtheDMDallowsforapertures( 0.55”)comparable Our prototype design is optimized for a highly config-
(cid:31)Synthetic Generator Pol. Reflectance Pol. Modulation Synthetic Generator
Supersampled Ray-Tracer Specular Emitter Downsample
Reflection
Noise
Material ID
Ground-Truth Material Maps
Diffuse Receiver
Reflection
Figure3.PolLidarforwardmodelandsimulator.Temporalpolarimetricreflectanceofthescenecanbemodeledasthesumofspecular
MsanddiffuseMdreflection. ReceiverandemitterofthePolLidarcanbedescribedwiththeMuellermatricesPiandAithatarefunc-
tionsoftherotationanglesθ{1,2,3,4}ofHWP,QWPsandLP,respectively.WeemploytheresultingPolLidarforwardmodelinasimulator
i
based on CARLA that generates synthetic polarimetric raw wavefronts. To this end, we extract material properties and normals from
CARLAandfeedthemintotheforwardmodel.Theresultingtemporalwavefrontsaresubsequentlydownsampledinspatialdimensionto
modelbeamdivergenceandnoiseisaddedtosimulateAPDandADC.
urable selection of polarization states by finely controlling Mueller matrices for incident and outgoing light, depend-
the polarization elements at the expense of measurement ingonrefractiveindexη. DsandDdarethedepolarization
time. The acquisition of a frame, as described in Sec. 4.1, Muellermatricesforspecularanddiffusereflections [6].
resultsinacapturetimeof5min. WerefertotheSupple- Given its long-range working distance, we can assume
mentary Material for an analysis on how future setups can thattheincidentandoutgoingdirectionoflightinoursen-
achieve real-time capability. Although adding polarization sorareidentical,andweapproximatethereflectancemodel
requires additional complexity, we argue that the potential (1) with a single viewing direction ω = ω = ω . After
i o
benefitsextendbeyondthescopeofthispaper,aidingrecon- scalingMbythecosineshadingtermcosϕandattenuation
structioninscenarioswithmulti-pathreflectionsorscatter- suchthatH(τ,ω,ω)= cosϕ/d2 M(τ,ω,ω),thelidar
ingmedia[5]. forwardmodelcanbewrittenas
(cid:0) (cid:1)
t′
3.1.PolarimetricLidarForwardModel
I(t,ω)= H(τ,ω,ω)s laser(ω,t ′ τ)dτ , (4)
WemodelpolarizationwiththeStokes-Muellerformalism,
(cid:34) (cid:90)0 − (cid:35)
0
with light and reflectance described by a Stokes vector where t = t 2d/c, d is the distance between laser and
′
s
∈
R4 ×1 and a Mueller matrix M
∈
R4 ×4. [7, 13]. Re- scene, and c is− the speed of light. s
laser
denotes the Stokes
cently, Baek et al. [6] introduced a temporal-polarimetric vectoroftheemittedlaserlight. Theoperator[...] denotes
0
reflectance model M(τ,ω i,ω o) describing how light po- takingthefirstelementoftheresultingvector.
larizationandintensitychangewhenimpingingonasurface Weuserotatingellipsometrytoinferallelementsofthe
withgivenincidentandoutgoingdirectionoflight(ω i and Stokesvectors[13]. AsillustratedbyFig.3,aHWPanda
ω o), and with temporal delay (τ) of diffuse reflection. As QWParerotatedtomodulatethepolarizationoftheemitted
showninFig.3,thereflectanceMcanbemodeledasasum light. Analogousonthereceivingside,aQWPandaLPare
ofspecularanddiffusereflection(M sandM d) used to measure light with a specific polarization incident
on the APD. Hence, the image formation of the PolLidar
M(τ,ω ,ω )=M (τ,ω ,ω )+M (τ,ω ,ω ) (1)
i o s i o d i o canbemodelledas
D(θ ;m)G(θ ,θ ;m)
M s(τ,ω i,ω o)= h
4cosθ
coi sθo Ds(τ)F
R
(2) I i(t,ω)=[A iH(t ′,ω)P is laser(ω,0)] 0, (5)
i o
M d(τ,ω i,ω o)=C n oFo TDd(τ)Fi TC i n, (3) where A i and P i are the i-th Mueller matrices of the an-
→ → alyzing optics and the polarizing optics defined as A =
i
where θ h = cos −1(h
·
n), θ i = cos −1(n
·
ω i), θ o = L(θ i4)Q(θ i3)andP i =Q(θ i2)W(θ i1),withθ i{1,2,3,4 } asthe
cos 1(n ω ) and n is the surface normal. D and G are rotation angles of the emitter HWP and QWP and the re-
− o
·
functionstodescribethesurface,wheremistheroughness. ceiver QWP and LP, respectively. W, Q, and L are the
C and C are the coordinate-conversion Mueller MuellermatricesoftheHWP,QWP,andLP[13]. Theinte-
i n n o
ma→ trices [13],→ and Fi , Fo are the Fresnel transmission gralisomittedasaresultofusingpulsedlaserillumination.
T TMeasure Wavefronts for all Peak-Based Segmentation Ellipsometric Neural Geometry Reconstruction
Reconstruction
0.45
0
neg. pos.
Priors
Distance
Scene View Encoding GT Normals GT Distance
Figure 4. Neural polarization wavefront lidar reconstruction. We capture raw polarization wavefronts of the scene I. We apply a
peak-basedsegmentationtechniquetoobtainaslicedpolarizationwavefront˜Ianddistancepriorsd.
Viaellipsometricreconstruction,we
estimateaslicedMuellermatrixH meas.Finally,weconcatenateallthepolarizationpriorswithviewingdirectionVastheinputtoaneural
networkpredictingdistanceandnormalsforthescene.Wesupervisedthenetworkwithanormalloss andadistanceloss .
normal dist
L L
3.2.PolarimetricLidarSimulator 4.1.PreprocessingWavefronts
In order to use the PolLidar in a learning-based frame- When capturing a frame, we perform rotating ellipsome-
work,asufficientamountoftrainingdataisrequired. How- try by collecting raw wavefronts for 36 different rotation
ever, the finely controllable polarization elements come at angles θ subsequently denoted as I = I 36 , where
i { i }i=1
the cost of longer measurement times as the motors move I
i
RH ×W ×T with H=150, W=236 and T=1488. The
∈
relatively slow. To acquire a large polarization wavefront temporal resolution T and the repeated measurement for
dataset, we integrate the lidar forward model from Eq. (5) eachangleθ resultsin53,568samplesforeachrayinI. To
i
into the CARLA simulator [19] to generate vast amounts tacklethislargedimensionalspace, wefirstperformpeak-
of synthetic training data. Specifically, we extend the full basedsegmentationtoobtainslicedwavefrontsasshownin
wavefront lidar model for CARLA as introduced by [26]. Fig. 4. Specifically, to reduce the temporal dimension, we
AspresentedinFig.3,weextractthematerialpropertiesm, firstlocatethepeakwithinthewavefront.Then,wesegment
Ds andDd usingcustommaterialcameras. However,ma- awindowofsize51centeredaroundthepeak,resultingina
terialsinCARLAdonothaverefractiveindicesµassigned slicedwavefront˜I = {I˜
i
}3 i=6 1,whereI˜
i
∈
RH ×W ×51. We
by default. We circumvent this problem by extending the preservethetemporalindexofthepeakt asitcontains
peak
ray-tracertoreturnthematerialIDofeachhitpoint. Based thedistanceinformationd R36 ×H ×W ×1.
onthematerialID,welook-upthecorrespondingrefractive
Astherawwavefront˜I∈
implicitlyencodesthepolariza-
indexµinadatabase[42]. Additionally,weextendtheray- tionopticsfromemitterandreceiver,weapplyellipsometric
tracertoreturnnormalsnforeachhitpoint. reconstruction to recover the time-dependent Mueller ma-
With material properties and normals in hand, we sim- trix H. To this end, we use the temporal measurements I
i
ulatethesceneusingthepolarimetriclidarforwardmodel. collectedatvariousrotationanglesofthepolarizingoptics
Tomodelthebeamdivergenceofthelaserbeam,wedown- to invert the image formation model presented in Eq. (5).
sampleneighboringraystoeventuallyrenderthetemporally Following the approach of Baek et al. [7], we recover the
resolvedpolarimetricrawwavefronts. Next,wemodelshot MuellermatrixH
meas
RH ×W ×51 ×16 bysolvingaleast-
∈
andread-outnoisebyapplyingPoissonandGaussiannoise squaresoptimizationproblemasfollows
to the wavefronts, respectively. We tune the noise charac-
teristicssuchthattheycloselyresembletherealdevice.Ad- N
minimize (I [A H P s ] )2. (6)
ditionaldetailsareprovidedintheSupplementaryMaterial. Hmeas
i=1
i − i meas i laser 0
(cid:88)
4.NeuralPolarizationLidarReconstruction
4.2.NeuralGeometryReconstruction
To leverage polarized raw wavefront data, we devise a Subsequenttothepre-processing,wereconstructthegeom-
learning-based approach for reconstructing normals and etryofthescenebyinputtingthesignalstoaneuralrecon-
distance as presented by Fig. 4. First, we preprocess the struction network. To this end, the temporal dimension is
wavefronts as described in Sec. 4.1. Next, we train a neu- flattenedandtheallinputsareconcatenatedasinputx
ralnetworktopredictnormalsanddistancefrompolarized
wavefrontsasdiscussedinSec.4.2. x=˜I d H V, (7)
meas
⊕ ⊕ ⊕
]V[
ytisnetnIGT Baek et al. PCA Proposed Material Reconstruction
Window
(Estimation / GT)
Car
Pole
Road
Figure5. Qualitativeevaluationonsyntheticdata. Baeketal.[6]isunabletoreconstructnormalsinareaswithlowDoP,e.g.,wallsof
buildingsfacingthesensor. PCA[61]appliedinthissettingarestronglydependentonpointclouddensity. Thus,distantpolesandcars
inthesecondrowcannotbereconstructedaccurately.Theproposedapproachleveragespolarizationcuestoreconstructnormalsinsparse
regionsandisrobustagainstlowDoPareas.Weestimateaccuratematerialpropertiesfordifferentsurfacesandobjects(right).
where denotesconcatenationalongthefeaturedimension
Method AngularError[°] Accuracy[%]
⊕ ↓ ↑
and V is the viewing direction. We then predict normals Mean Median RMSE 3.0◦ 5.0◦ 10.0◦
nˆ and distance dˆ with a neural network. The network is a
SfP-DoP[2] 49.82 35.00 65.39 4.29 7.60 12.63
variationofaTransUnetthatcombinestheU-Netandtrans-
Baeketal.[6] 31.03 8.32 53.21 27.12 44.44 61.03
formerarchitecturecomponents. Specifically,weuse3en-
PCA[61] 18.64 8.02 33.60 55.89 60.64 66.84
coder layers to encode the features. At the bottleneck, we
Proposed 8.71 4.31 17.65 65.49 70.19 78.15
use 8 transformer layers. At last, we use 3 decoder layers
withskip-connectiontopredictnormalsanddistance. Table1.Quantitativeevaluationfornormalsonsyntheticdata.
Totrainthenetwork,wesupervisenormalsanddistance TheSfPbaseline[2]isunabletoreconstructnormalsinreal-world
predictionswithacosinesimilaritylossforthesurfacenor- astheunderlyingassumptionsdonottranslatetoreal-worldsce-
malsandameanabsolutelossfordistance narios. Baek et al. [6] is designed for object-level ToF imaging
andfailsinlowDoPregions.PCA[61]achievesimprovedresults
= 1 (c n ) (c nˆ)) , (8) butwithqualitydependingonpointclouddensity. Theproposed
normal gt 1
L | − ⊙ · ⊙ | methodleveragesboththeneighborhoodofpointsandthepolar-
= c d c dˆ) , (9)
dist gt 1 izationcues;thusoutperformingallthebaselines.
L | ⊙ − ⊙ |
where c is the confidence mask for the normals where
compareagainstthreeSfPbaselinemethodstoevaluatethe
groundtruthnormalsarenotavailable.
qualityofthereconstructednormals. Specifically,weeval-
We implement the proposed method in PyTorch. We
uateagainstBaeketal.[6]asabaselinedesignedforobject-
trainthemodelfor200epochsonaNvidiaA100GPU.We
levelscenereconstructionforapolarimetricToFprototype.
usetheAdamoptimizerwithalearningrateof1e-4andwe
This approach fits the recovered Mueller matrix H to
setthebatchsizeto1. Wecropimagesto128×128patches meas
the polarimetric lidar forward model by jointly estimating
ineachiterationforaugmentation. Weapplydifferentlaser
materialpropertiesandnormals. Next,wecompareagainst
powers and biases during training to increase robustness
the classical SfP approach from [2], which recovers sur-
againstsaturationandlow-intensityreadings. Moredetails
facenormalsfromtheDoPbyassumingascene-widecon-
arepresentedintheSupplementaryDocumentation.
stant refractive index and diffusive reflection. As reported
inTab.1,classicalSfPapproachesdonotgeneralizewellto
5.Assessment
outsidescenes. Thiscanbeattributedtoreal-worldgeom-
To assess the effectiveness of the proposed reconstruction etryexhibitingregionsofhighbutalsoverylowDoP.Low
method,wefirstvalidatethemethodonsyntheticdatawith DoPregionsoccurwhenthesurfacenormalandtheviewing
perfect groundtruth. Next, we discuss material estimation directionofthelidaralign,seeSupplementaryDcoumenta-
beforevalidatingthemethodwiththeexperimentaldevice. tion. Highlighted by the qualitative findings in Fig. 5, the
Finally,weablatethedifferentinputstoshowthebenefitof method from Baek et al. [6] is unable to reconstruct nor-
polarizedrawwavefronts. mals in low DoP regions, e.g., buildings of walls that face
the sensor, whereas for high DoP regions, as e.g., the side
5.1.SyntheticEvaluation
ofavehicle,satisfyingperformanceisachieved.
Wefirstvalidatetheproposedneuralgeometryreconstruc- Moreover,wecompareagainstconventionallidarbyav-
tionmethodonsyntheticdatawithperfectgroundtruth.We eragingwavefrontsfromallpolarizationstatesandapplyingGround Truth PCA Proposed GT PCA Proposed
Scene
Scene
Scene Normals
Figure6. Qualitativeevaluationonexperimentaldata. PCA[61]appliedonmeasuredcapturesfromourPolLidarresultsinerroneous
predictions of surface normals, especially prominent for the fine structures visible in the zoom-ins of the first two rows, see. e.g., the
transitionofgroundandmetalrampinthefirstrowandthemetalroofsupportinthesecondrow.Incontrast,theproposedmethodisable
toresolvethesefinedetails. Inthelastrow,weshowalostcargoscenariowithanuprightobjectblockingtheroadin50mdistance. Our
methodcorrectlyclassifiestheobjectasfacingtowardthevehicle,whereasPCApredictsaflatsurfacewithdownwardsorientednormals.
peak-finding. TreatingourPolLidarasconventionalmakes 5.2.MaterialPropertyEstimation
foranadequatecomparisonasthenumberofscannedpoints
Withestimatedsurfacenormalsinhand,wereconstructthe
isequalforbothconventionalandPolLidar. Subsequentfor
material properties, namely index of refraction µ, rough-
normalreconstruction,weapplyPCA[61]asapoint-cloud
nessmandthedepolarizationmatricesDs andDd, ofthe
basedmethodthatconsidersaneighborhoodofpoints.This
polarimetric lidar forward model. To this end, we follow
methodperformswellinareaswithflatgeometryandhigh
Baeketal.[6]andestimatematerialpropertiesbyrendering
pointdensitybutdegradessignificantlyatlongrangeswith
theMuellermatrixH =Hs +Hd thatbestex-
sparsedistance,e.g.,carsinfardistancesinthesecondrow render render render
plainsthereconstructedMuellermatrixH . Inthelarge
ofFig.5andgeometrytransitionregions,e.g.,theareabe- meas
sceneswetackle, wefindthattheDoPismostlygoverned
tweenroadandcar. PCAalsostruggleswiththinstructures
bydiffusereflection. Weleveragethisheuristictodisentan-
like the pole in the second row of Fig. 5. The proposed
glethespecularanddiffusiveMuellermatrices.Tothisend,
methodleveragestheadditionalpolarizationcuestoresolve
wesolvethefollowingminimizationproblem
normalsinregionswithsparsepoints. Wealsoachievesat-
isfying reconstruction results for regions with weak polar- minimize λd c (H Hd ) +
izationinformationbytakingalocalneighborhoodandcues µ,m, |Ds |, |Dd | dop ⊙ meas − render 1 (10)
from a normal-dependent widened pulse into account. As λs(cid:12) (cid:12)(H meas −Hd render) −Hs ren(cid:12) (cid:12) der 1,
a result, the proposed approach outperforms PCA [61] by
53%onthemeanangularerrorasshowninTab.1. where λd, λs are sc(cid:12) (cid:12)alar weights and c
dop
a mas(cid:12) (cid:12)k focusing
For evaluating distance estimation, we compare against onregionswithhighdiffusiveDoP.Theweightsarechosen
the conventional argmax-peak-finding typically performed suchthatinthefirstphaseoftheminimization,thediffusive
directlyonthedevicebylow-levelelectronics[9]. Thisap- lossdrivestheestimationoftheindexofrefractionµwhich
proach is limited by the temporal resolution of the sensor laterhelpstobetterdisentanglematerialpropertiesthatoc-
and we find a mean absolute distance error of 32cm. The cursolelyinthespecularcomponentoftheMuellermatrix.
proposedmethodleveragestherawwavefrontdataandthe Note that in our simulation, only the scalar amplitude, de-
relationshipbetweendistanceandnormalstogeneratehigh- notedby Ds and Dd ,ofthedepolarizationmatricesvary
| | | |
quality distance. We find that the proposed method yields andaresubsequentlyoptimizedfor. Fig.5validatesthatthe
a mean absolute distance error of 19cm outperforming the proposedapproachisabletosuccessfullyrecoverthemate-
conventionalapproachby41%meanabsoluteerror. Addi- rial properties of different objects and surfaces. As we do
tionalmetricsareprovidedintheSupplementaryMaterial. not optimize the surface normals, this further validates theMethod AngularError[°] Accuracy[%] Ablatedmodules Meanangularerror[◦]
↓ ↑
Mean Median RMSE 3.0◦ 5.0◦ 10.0◦ Wavefront Polarization Mueller
SfP-DoP[2] 65.92 63.76 70.13 0.02 0.05 0.24 ✓ 10.64
Baeketal.[6] 37.04 13.97 57.25 17.51 27.93 43.36 ✓ ✓ 9.73
PCA[61] 18.75 4.86 35.73 41.18 52.87 63.74 ✓ ✓ 9.47
Proposed 15.76 4.27 28.73 45.94 55.42 63.80 ✓ ✓ ✓ 8.71
Table2. Quantitativeevaluationfornormalsonexperimental
Table 3. Ablation studies for different modules on synthetic
data. For normal reconstruction with the real device, compara-
data. Thequalityoftheproposedmethoddegradeswhenthepo-
bletrendstosyntheticdataareobservable. Duetonoisierground
larizationinformation,Muellermatrix,orwavefrontiswithheld.
truthandsensorimperfections,theoverallerrorisslightlylarger.
However,theproposedmethodrecoversaccuratenormalsonreal
experimentaldata,outperformingallbaselineapproaches. remove the polarization information by replacing the raw
wavefronts˜I with the mean over the different θ . Remov-
qualityofthereconstructednormalsasrecoveringmaterial i
ing the polarization cues, increases mean angular error by
propertieswithoutaccuratenormalsisinfeasible.
22%. Furthermore,weablatetheellipsometricreconstruc-
5.3.ExperimentalEvaluation tion. Specifically, we remove the Mueller matrix from the
inputs. Asthenetworkneedstolearntodisentanglethepo-
Next, we evaluate the proposed approach on real-world larizationopticsoftheemitterandreceiverfromthescene,
data. We pair the PolLidar sensor with a Velodyne VLS- themeanangularerrorofsurfacenormalincreasesby12%.
128referencelidar. Fig.2showsPolLidardatawithground Finally, weanalyzetheimpactofusingrawwavefrontsby
truth distance and normals. In total, we capture 60 frames setting the window size to 1. Tab. 3 shows that the wave-
with 3 biases each and scene-adjusted laser power paired frontcarriescrucialinformationforscenereconstruction.
with ground-truth distance and normal information. For
ground truth, we accumulate point clouds from the refer-
6.Conclusion
ence lidar, generate dense lidar maps, and extract normals
fromthemeshedlidarmap.
Thispaperintroducesanovellong-rangepolarizationwave-
Fig.6reportsqualitativereconstructionresults. Similar
frontlidarsensorthatmeasurestime-resolvedpolarization-
tothesyntheticevaluations, PCA[61]introducesartifacts,
modulatedwavefronts.Torecoverhigh-resolutionscenein-
whereas the proposed approach is able to recover the sur-
formation from these raw polarimetric wavefronts, we de-
facegeometrycorrectly, e.g., thefirstrowofFig.6forthe
visealearning-basedapproachtorecoverdistance,surface
transition area between ground and metal ramp. Further-
normals,andmaterialproperties. Totrainandevaluatethe
more,theproposedapproachisabletoreconstructnormals
method, we introduce a large synthetic dataset and a real-
insparseregions,e.g. forthemetalsupportstructureofthe
worldlong-rangedatasetwithpairedrawlidardata,ground
roof in the second row. These findings are consistent with
truthdepthandnormalmaps. Wevalidatethattheproposed
the quantitative results in Tab. 2, where the proposed ap-
methodimprovesnormalanddepthreconstructionby53%
proachoutperformsthebestbaselineby16%meanangular
and41%inmeanangularerrorandmeanabsolutedistance
error.Forautonomousdriving,accuratenormalsallowusto
error compared to existing shape-from-polarization (SfP)
distinguishobstaclesfromtheroadandarecrucialfordeter-
andToFmethods. Confirmingthepotentialoftheproposed
miningifareasoftheroadcanbeoverridden,e.g.,detecting
polarimetric wavefront sensing method with a sequential
lost-cargo objects on roads [41, 44]. We show such a sce-
acquisition setup, future work may devise parallelized ac-
nariointhelastrowofFig.6,wherenormalsofaroadblock
quisitionsetupsthatcaptureasubsetofpolarizationstates,
in 50m distance are predicted correctly as facing towards
allowingforreal-timepolarimetriclidarcaptures.
thevehiclebytheproposedapproach.Incontrast,PCA[61]
estimatestheroadblockasflatwithdownwardpointingnor-
malslikelymisclassifyingtheobjectastraversable.
Acknowledgements ThisworkwassupportedbytheAI-
Fordistanceestimation,themeanabsoluteerrorofcon-
SEEprojectwithfundingfromtheFFG,BMBF,andNRC-
ventionalargmax-peak-findingamountsto24cm,whereas
IRA.ChenyangLeiwassupportedbytheInnoHKprogram.
our method yields a mean absolute error of 20 cm outper-
Seung-HwanBaekwassupportedbyKoreaNRFgrant(RS-
formingtheconventionaldistanceestimationby17%.
2023-00211658, 2022R1A6A1A03052954). Felix Heide
was supported by an NSF CAREER Award (2047359), a
5.4.AblationExperiments
Packard Foundation Fellowship, a Sloan Research Fellow-
We further provide an ablation study in Tab. 3. First, the ship,aSonyYoungFacultyAward,aProjectXInnovation
impact of polarization cues is studied. In particular, we Award,andanAmazonScienceResearchAward.References [17] AyushDewan,TimCaselitz,GianDiegoTipaldi,andWol-
fram Burgard. Motion-based detection and tracking in 3D
[1] GaryA.Atkinson.Polarisationphotometricstereo.Comput.
LiDARscans.InIEEEInternationalConferenceonRobotics
Vis.ImageUnderst.,160:158–167,2017. 1,2
andAutomation(ICRA),2016. 1
[2] GaryA.AtkinsonandEdwinR.Hancock. Recoveryofsur-
[18] YuqiDing,YuJi,MingyuanZhou,SingBingKang,andJin-
faceorientationfromdiffusepolarization. IEEETrans.Im-
weiYe. Polarimetrichelmholtzstereopsis. InCVPR,2021.
ageProcess.,15(6):1653–1664,2006. 2,6,8
3
[3] Yunhao Ba, Alex Gilbert, Franklin Wang, Jinfa Yang,
[19] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Anto-
Rui Chen, Yiqin Wang, Lei Yan, Boxin Shi, and Achuta
nioLopez,andVladlenKoltun. Carla: Anopenurbandriv-
Kadambi. Deep shape from polarization. In ECCV, 2020.
ingsimulator. InConferenceonrobotlearning,pages1–16.
1,2
PMLR,2017. 5
[4] Seung-Hwan Baek, Daniel S. Jeon, Xin Tong, and Min H.
[20] RalphODubayahandJasonBDrake. Lidarremotesensing
Kim.SimultaneousacquisitionofpolarimetricSVBRDFand
forforestry. Journalofforestry,98(6):44–46,2000. 1
normals. ACMTrans.Graph.,37(6):268:1–268:15,2018. 2
[5] Seung-Hwan Baek and Felix Heide. Polarimetric spatio- [21] ShuaiFang,XiuShanXia,XingHuo,andChangWenChen.
temporal light transport probing. ACM Transactions on Image dehazing using polarization effects of objects and
Graphics(Proc.SIGGRAPHAsia),40(6),2021. 1,2,3,4 airlight. Opticsexpress,22(16):19523–19537,2014. 1
[6] Seung-HwanBaekandFelixHeide.All-photonpolarimetric [22] Qiang Fu, Wei Yang, Linlin Si, Meng Zhang, Yue Zhang,
time-of-flightimaging.ProceedingsoftheIEEEConference KaimingLuo,JuntongZhan,andSuZhang. Studyofmulti-
onComputerVisionandPatternRecognition(CVPR),2022. spectralpolarizationimaginginseafogenvironment. Fron-
1,2,3,4,6,7,8 tiersinPhysics,11,2023. 1
[7] Seung-Hwan Baek, Tizian Zeltner, Hyunjin Ku, Inseung [23] Yoshiki Fukao, Ryo Kawahara, Shohei Nobuhara, and Ko
Hwang,XinTong,WenzelJakob,andMinHKim. Image- Nishino. Polarimetricnormalstereo. InCVPR,2021. 3
basedacquisitionandmodelingofpolarimetricreflectance. [24] Andreas Geiger, Martin Lauer, Christian Wojek, Christoph
ACMTrans.Graph.,39(4):139,2020. 4,5 Stiller, andRaquelUrtasun. 3dtrafficsceneunderstanding
[8] Jens Behley, Martin Garbade, Andres Milioto, Jan Quen- frommovableplatforms.IEEEtransactionsonpatternanal-
zel, Sven Behnke, Ju¨rgen Gall, and Cyrill Stachniss. To- ysisandmachineintelligence,36(5):1012–1025,2013. 1
wards 3d lidar-based semantic scene understanding of 3d
[25] DennisGoldstein. PolarizedLight. CRCPress,3rdedition
point cloud sequences: The semantickitti dataset. The In-
edition,2011. 1
ternationalJournalofRoboticsResearch,40(8-9):959–967,
[26] FelixGoudreault,DominikScheuble,MarioBijelic,Nicolas
2021. 1
Robidoux,andFelixHeide. Lidar-in-the-loophyperparame-
[9] Behnam Behroozpour, Phillip AM Sandborn, Ming C Wu,
teroptimization. 2023. 2,5
andBernhardEBoser. Lidarsystemarchitecturesandcir-
[27] Tianyu Huang, Haoang Li, Kejing He, Congying Sui, Bin
cuits. IEEE Communications Magazine, 55(10):135–142,
Li, and Yun-Hui Liu. Learning accurate 3d shape based
2017. 7
on stereo polarimetric imaging. In Proceedings of the
[10] Kai Berger, Randolph Voorhies, and Larry H. Matthies.
IEEE/CVF Conference on Computer Vision and Pattern
Depthfromstereopolarizationinspecularscenesforurban
Recognition,pages17287–17296,2023. 3
robotics. InICRA,2017. 1
[28] JAlexHuffman,AnneEPerring,NicoleJSavage,Bernard
[11] Alexander Carballo, Jacob Lambert, Abraham Monrroy,
Clot,BenoˆıtCrouzy,FionaTummon,OfirShoshanim,Brian
David Wong, Patiphon Narksri, Yuki Kitsukawa, Eijiro
Damit, Johannes Schneider, Vasanthi Sivaprakasam, et al.
Takeuchi, Shinpei Kato, and Kazuya Takeda. Libre: The
Real-time sensing of bioaerosols: Review and current per-
multiple3dlidardataset. In2020IEEEIntelligentVehicles
spectives. AerosolScienceandTechnology,54(5):465–495,
Symposium(IV),pages1094–1101.IEEE,2020. 1
2020. 1,2
[12] Nicholas Carlevaris-Bianco, Arash K Ushani, and Ryan M
[29] Cong Phuoc Huynh, Antonio Robles-Kelly, and Edwin R.
Eustice. Universityofmichigannorthcampuslong-termvi-
sionandlidardataset. TheInternationalJournalofRobotics Hancock. Shapeandrefractiveindexrecoveryfromsingle-
Research,35(9):1023–1035,2016. 1 viewpolarisationimages. InCVPR,2010. 1
[13] EdwardCollett. Fieldguidetopolarization. SpieBelling- [30] KeiIkemura,YimingHuang,FelixHeide,ZhaoxiangZhang,
ham,WA,2005. 1,4 QifengChen,andChenyangLei.Robustdepthenhancement
[14] Zhaopeng Cui, Jinwei Gu, Boxin Shi, Ping Tan, and Jan viapolarizationpromptfusiontuning,2024. 2
Kautz. Polarimetricmulti-viewstereo. InCVPR,2017. 1,3 [31] DanielSJeon,Andre´asMeuleman,Seung-HwanBaek,and
[15] AkshatDave,YongyiZhao,andAshokVeeraraghavan.Pan- MinHKim.Polarimetricitof:Measuringhigh-fidelitydepth
dora: Polarization-aided neural decomposition of radiance. throughscatteringmedia. InProceedingsoftheIEEE/CVF
In European Conference on Computer Vision, pages 538– Conference on Computer Vision and Pattern Recognition,
556.Springer,2022. 3 pages12353–12362,2023. 2
[16] Valentin Deschaintre, Yiming Lin, and Abhijeet Ghosh. [32] AchutaKadambi,VageTaamazyan,BoxinShi,andRamesh
Deep polarization imaging for 3d shape and svbrdf acqui- Raskar. Polarized3d: High-qualitydepthsensingwithpo-
sition. InCVPR,2021. 3 larizationcues. InICCV,2015. 3[33] James R Kellner, John Armston, Markus Birrer, KC Cush- [47] Santiago Royo Royo and Jordi Riu Gras. System, method
man,LauraDuncanson,ChristophEck,ChristophFalleger, and computer program for receiving a light beam, United
Benedikt Imbach, Kamil Kra´l, Martin Kru˘cˇek, et al. New StatesPatent9689667B22013. 3
opportunities for forest remote sensing through ultra-high- [48] KennethSassen. Thepolarizationlidartechniqueforcloud
density drone lidar. Surveys in Geophysics, 40:959–977, research: Areviewandcurrentassessment. Bulletinofthe
2019. 1 AmericanMeteorologicalSociety,72(12):1848–1866,1991.
[34] Yuhi Kondo, Taishi Ono, Legong Sun, Yasutaka Hirasawa, 1
and Jun Murayama. Accurate polarimetric BRDF for real [49] Kenneth Sassen. Polarization in lidar. In LIDAR: Range-
polarizationscenerendering. InECCV,2020. 2 resolvedopticalremotesensingoftheatmosphere,pages19–
42.Springer,2005. 1
[35] Chenyang Lei, Xuhua Huang, Mengdi Zhang, Qiong Yan,
[50] YoavY.Schechner,SrinivasaG.Narasimhan,andShreeK.
WenxiuSun,andQifengChen. Polarizedreflectionremoval
Nayar. Instant dehazing of images using polarization. In
withperfectalignmentinthewild. InCVPR,2020. 1
CVPR,2001. 1
[36] Chenyang Lei, Chenyang Qi, Jiaxin Xie, Na Fan, Vladlen
[51] RichardMSchotland, KennethSassen, andRichardStone.
Koltun,andQifengChen. Shapefrompolarizationforcom-
Observationsbylidaroflineardepolarizationratiosforhy-
plex scenes in the wild. In Proceedings of the IEEE/CVF
drometeors. JournalofAppliedMeteorologyandClimatol-
Conference on Computer Vision and Pattern Recognition,
ogy,10(5):1011–1017,1971. 1,2
pages12632–12641,2022. 1,2
[52] Moein Shakeri, Shing Yang Loo, Hong Zhang, and
[37] ClemensLinnhoff,DominikScheuble,MarioBijelic,Lukas
Kangkang Hu. Polarimetric monocular dense mapping us-
Elster,PhilippRosenberger,WernerRitter,DengxinDai,and
ingrelativedeepdepthprior.IEEERoboticsandAutomation
HermannWinner. Simulatingroadsprayeffectsinautomo-
Letters,6(3):4512–4519,2021. 3
tivelidarsensormodels. arXivpreprintarXiv:2212.08558,
[53] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jian-
2022. 1
pingShi, XiaogangWang, andHongshengLi. PV-RCNN:
[38] Youwei Lyu, Zhaopeng Cui, Si Li, Marc Pollefeys, and Point-voxel feature set abstraction for 3D object detection.
BoxinShi. Reflectionseparationusingapairofunpolarized In IEEE/CVF Conference on Computer Vision and Pattern
andpolarizedimages. InNeurIPS,2019. 1 Recognition(CVPR),2020. 1
[39] DaisukeMiyazaki,RobbyT.Tan,KenjiHara,andKatsushi [54] Chaoran Tian, Weihong Pan, Zimo Wang, Mao Mao,
Ikeuchi. Polarization-basedinverserenderingfromasingle Guofeng Zhang, Hujun Bao, Ping Tan, and Zhaopeng Cui.
view. InICCV,2003. 2 Dps-net: Deep polarimetric stereo depth estimation. In
[40] Daisuke Miyazaki, Takuya Shigetomi, Masashi Baba, Ryo ProceedingsoftheIEEE/CVFInternationalConferenceon
Furukawa,ShinsakuHiura,andNaokiAsada. Surfacenor- ComputerVision,pages3569–3579,2023. 1,3
malestimationofblackspecularobjectsfrommultiviewpo- [55] ShishunTian,MinghuoZheng,WenbinZou,XiaLi,andLu
larizationimages.OpticalEngineering,56(4):041303,2016. Zhang. Dynamiccrosswalksceneunderstandingforthevi-
3 sually impaired. IEEE transactions on neural systems and
[41] Peter Pinggera, Sebastian Ramos, Stefan Gehrig, Uwe rehabilitationengineering,29:1478–1486,2021. 1
Franke, Carsten Rother, and Rudolf Mester. Lost and [56] Tali Treibitz and Yoav Y. Schechner. Active polarization
found: detecting small road hazards for self-driving vehi- descattering. IEEE Trans. Pattern Anal. Mach. Intell., 31
cles. In2016IEEE/RSJInternationalConferenceonIntel- (3):385–399,2009. 1
ligentRobotsandSystems(IROS),pages1099–1106.IEEE, [57] Alexander P Vasilkov, Yury A Goldin, Boris A Gureev,
2016. 8 FrankEHoge,RobertNSwift,andCWayneWright. Air-
borne polarized lidar detection of scattering layers in the
[42] MikhailN.Polyanskiy. Refractiveindexdatabase. https:
ocean. AppliedOptics,40(24):4353–4364,2001. 1,2
//refractiveindex.info. Accessedon2023-11-12.
[58] ClausWeitkamp.Lidar:range-resolvedopticalremotesens-
5
ingoftheatmosphere. SpringerScience&Business,2006.
[43] Stefan Rahmann and Nikos Canterakis. Reconstruction of
1
specular surfaces using polarization imaging. In CVPR,
[59] TomonariYoshida,VladislavGolyanik,OliverWasenmu¨ller,
2001. 2
and Didier Stricker. Improving time-of-flight sensor for
[44] Sebastian Ramos, Stefan Gehrig, Peter Pinggera, Uwe
specularsurfaceswithshapefrompolarization.In201825th
Franke, and Carsten Rother. Detecting unexpected obsta-
IEEEInternationalConferenceonImageProcessing(ICIP),
clesforself-drivingcars: Fusingdeeplearningandgeomet-
pages1558–1562.IEEE,2018. 1,3
ricmodeling. In2017IEEEIntelligentVehiclesSymposium
[60] JiZhangandSanjivSingh. LOAM:LiDARodometryand
(IV),pages1025–1032.IEEE,2017. 8
mappinginreal-time. Robotics: ScienceandSystemsCon-
[45] OleRisbølandLarsGustavsen.Lidarfromdronesemployed ference(RSS),2014. 1
formappingarchaeology–potential,benefitsandchallenges. [61] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3D:
ArchaeologicalProspection,25(4):329–338,2018. 1 A modern library for 3D data processing. CoRR,
[46] Je´re´myRiviere,IlyaReshetouski,LukaFilipi,andAbhijeet abs/1801.09847,2018. 6,7,8
Ghosh.Polarizationimagingreflectometryinthewild.ACM [62] DizhongZhuandWilliamA.P.Smith. Depthfromapolari-
Trans.Graph.,36(6):206:1–206:14,2017. 1 sation+RGBstereopair. InCVPR,2019. 3Polarization Wavefront Lidar:
Learning Large Scene Reconstruction from Polarized Wavefronts
(Supplementary Information)
DominikScheuble1,2* ChenyangLei5 Seung-HwanBaek4 MarioBijelic3,5 FelixHeide3,5
∗
1Mercedes-BenzAG 2TUDarmstadt 3TorcRobotics 4POSTECH 5PrincetonUniversity
In this supplemental document, we present additional details on the PolLidar prototype, the simulation and reconstruction
method, the testing and training datasets, and the training process. We also provide additional quantitative and qualitative
resultsinsupportofthefindingsfromthemainmanuscript.
Contents
1.PolLidarPrototype 1
1.1.PrototypeConstruction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2.SystemResponseandNoiseCharacteristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.3.AssessmentofPolarizationCues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.4.Real-TimeCapability. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.LidarForwardModelandSimulator 5
2.1.Temporal-PolarimetricLidarForwardModel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.2.ImplementationDetailsofPolarizationCARLASimulator . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.3.SimulatingSensorNoise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3.AdditionalDetailonDataset 8
3.1.GroundTruth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3.2.Acquisition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
4.Reconstruction 10
4.1.EllipsometricReconstruction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.2.NetworkDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.3.TrainingDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
5.AdditionalQuantitativeandQualitativeResults 11
5.1.AdditionalSyntheticResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
5.2.AdditionalReal-WorldResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
5.3.AdditionalDistance-BinnedEvaluationofNormalReconstruction . . . . . . . . . . . . . . . . . . . . . . . . 13
5.4.AdditionalMetricsforDistanceEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
1.PolLidarPrototype
AsthePolLidarisanentirelynovelprototype,weprovideadditionaldetailsaboutitsconstruction(Sec.1.1);subsequently,
we share insight into key characteristics of the sensor (Sec. 1.2); then, we validate that the surface-induced polarization
cuescanbecapturedwiththeproposedprototype(Sec.1.3); finally,wediscussimprovementsthatwillbemadeforfuture
prototypes(Sec.1.4).
*Theseauthorscontributedequallytothiswork.
11.1.PrototypeConstruction
The PolLidar extends the concept from Beamagines L3CAM lidar 1 starting from traditional ToF and adding polarization
capabilities. However, the emitter and receiver are a redesign with custom opto-mechanics to fit waveplates and polarizer
necessarytomodulatethepolarization. Fig.1illustratesasectionedviewofemitterandreceivertoshowcasethepolarization
optics. All used parts are listed in Tab. 1. The optics are designed to minimize the angle of incidence (AoI) onto the
waveplatesandpolarizer. ThemaximumAoIontheemittersideis2°,whereasonthereceivingside,themaximumAoIis
8°. Theopticsontheemittingsideallowforbeamdivergenceof0.364°. Onthereceivingside,anentrancepupilof6.8mm
isrealized. AnoverviewofsensorkeyspecificationsisprovidedbyTab.2.
The raw wavefront is captured with a National Instruments PCIe-5764 FlexRIO-Digitizer ADC, sampling at 1 Gs/s.
The sampling frequency allows for 1 ns wide bins resulting in a 15cm range resolution. The ADC is interfaced from a
LabView application which triggers the acquisition after the piezoelectric motors are finished rotating the waveplates or
linear polarizer to their respective rotation angle θ . Raw wavefronts for a single frame with 150 rows and 236 columns
i
amounttoapproximately100MBofrawbinarydatawheretwobytesperbinareusedtoencodethemeasuredintensity. For
theacquisition,aPythonAPIinterfacingtheLabViewapplicationhasbeendevelopedthatcanbeleveragedinfutureworks
fore.g. onlineoptimizationasdiscussedin[6].
Item# PartDescription Quantity ModelName
1 Silicon-basedAvalanchePhotoDiode(APD) 1 Proprietary
2 PulsedLasersource 1 Proprietary
3 Polarization-maintaining(PM)Fiber10.5µm 1 CoherentPM1060L
4 EmitterfiberColiminator 1 ThorlabsF230APC-1064
5 EmitterLens1 1 EdmundOptics67-537
6 EmitterLens2 1 EdmundOptics67-998
7 EmitterLens3 1 EdmundOptics68-001
8 ReceiverLens1 1 EdmundOptics68-001
10 ReceiverOptics 3 Proprietary
12 Band-passFilter1064nm 1 Proprietary
13 Quarter-wavePlate 2 ThorlabsWPQ10M-1064
14 Half-wavePlate 1 ThorlabsWPH10M-1064
15 LinearPolarizer 1 ThrolabsLPNIRB100
16 PiezoelectricMotors 4 ThorlabsELL14K
17 MEMSMicroMirrorScanningSystem 1 Proprietary
18 MicroMirrorReceivingSystem 1 Proprietary
Table1.Listofusedpartsforreceiverandemitter.
Specification Value
Verticalresolution 150rowsover23.95°
Horizontalresolution 236columnsover31.05°
Temporalresolution 1488binsover223m
Rotationangleθ resolution 0.01°
i
Wavelength 1064nm
Beamdivergence 0.326°
Max. AoIonpolarizingoptics 2°(emitter),8°(receiver)
Table2.KeyspecificationofconstructedPolLidarprototype.
1.2.SystemResponseandNoiseCharacteristics
ThepolarizationcuesnecessaryforShape-from-Polarization(SfP)approachesarereflectedintheintensityreadingsofthe
ADC.Dependingonthedegree-of-polarization(DoP),theintensitychangefordifferentpolarizationstatescanbe,asfunction
1https://beamagine.com/product.Emitter
Receiver
3
12 16, 15 16, 13
4
16, 14
18
10
8
17
1
5 16, 13 6 7
Figure 1. Sectioned view of emitter (left) and receiver (right). Parts numbers can be referenced from Tab. 1. For the protection of
intellectualproperty,partsofthesectionedviewarecoveredandreplacedbyschematicplaceholders.
ofgeometry,materialandviewingdirection,verysubtle. Thus,accurateintensityreadingsarecrucialforasuccessfulrecon-
struction. Hence, we provide inside in the system response of the PolLidar towards the key parameters of laser power and
biasvoltageoftheAPD.Furthermore,weinvestigatethenoisecharacteristicsofthesensortobetterdistinguishpolarization-
inducedintensitychangefromsensornoise.
System Response To obtain meaningful intensity readings, laser power and bias need to be adjusted according to the
scene. Incontrasttoconventionallidarsensors,weallowedaccesstotheselow-levelparameters. Weinvestigatethesystem
responsetolaserpowerandbiasinacontrolledenvironmentasshowninFig.2. Here,werepeatedlymeasureasinglepixel
onacalibrated90%reflectiontargetfordifferentbiasesandlaserpowers. Weaverage50framesperlaserpower/biastolimit
theeffectofnoise. Wefindthatlargebiasvaluesquicklyleadtosaturationataround0.4V.Thiswillrendertheintensities
meaningless for SfP as all polarization states will likely be saturated making reconstruction unfeasible. In contrast, small
biases result in low intensity readings below the noise floor and subsequently dropped points preventing reconstruction
altogether. As indicated by Fig. 2, we observe an exponential increase in intensity depending on the bias. Thus, the bias
operating point needs to be selected with care to allow for successful reconstruction. On the other hand, we find that the
laserpoweriswellbehavedinthisregardandfindanapproximatelylinearrelationshipbetweenlaserpowerandintensity.
MotivatedbythefindingsinFig.2,weopttoselectonelaserpowerpercapturedframeandbutalwayscollectframeswith3
differentbiases. DetailsonacquisitionareprovidedinSec.3.2.
0.4
Laser Power 600mV
Laser Power 700mV
0.3 Laser Power 800mV
Laser Power 900mV
0.2
0.1
90% Reflection Target 0.0
800 Bias [mV] 2700
Figure2.Systemresponsetowardsbiasandlaserpower.Weobserveanapproximatelyexponentialrelationshipbetweenbiasandintensity
butonlyalinearonebetweenlaserpowerandintensityforasinglepixelona90%reflectiontarget.Weaverage50framesperlaserpower,
biaspairtoreducetheeffectofnoise.
]V[
ytisnetnINoiseModel Furthermore,weinvestigatethenoisecharacteristicsofthesensor. Tothisend,wecontinuouslycapturethe
samescene800timesasshownbyFig.3. OntherightofFig.3,weshowintensitydistributionsforfourselectedpixelson
interesting targets. We observe that the noise can be described as approximately Gaussian centered around a single mean.
Thereby, we ensure to operate the laser diode and detector with sufficient warm up, such that over the measurements no
temperaturedriftispossible.However,thereisasubstantialdeviationoftheintensitywithstandarddeviationsupto0.03mV
whichiscoherentwiththespecificationsoftheinstalledAPD.Thereforeforsomeregionsofacapturedframe,theintensity
deviationfromnoisewilllikelyovershadowthedesirabledeviationobservablefordifferentpolarizationstates. Asaresult,
we opt to average 10 frames to increase resilience from sensor noise, as further discussed in Sec. 3.2. In addition, we test
thesensorforrepeatabilitybyreturningtothesamerotationanglesθ aftersettingaseriesofdifferentrotationangles. We
i
observenodriftinthisregard.
Scene
Intensity
Intensity [mV]
Figure 3. Noise characteristics of the PolLidar. We repeatedly measure the same scene 800 times. On the right, we show intensity
distributionsforselectedpixels. Theintensitiesarecenteredaroundameanindicatingnoe.g. temperature-dependentdrift. However,we
observesubstantialdeviationsaroundthemean,indicatingthatmultiplewavefrontsmustbeaveragedtoreducethesensornoise.
1.3.AssessmentofPolarizationCues
OurproposedSfPreconstructionmethodreliesonrotatingellipsometryforacquisition. However, theresultsfromrotating
ellipsometry are not directly interpretable and do not allow an intuitive assessment weather the PolLidar is able to capture
scene-dependentpolarizationcues. Tothisend,wefollowtheexperimentsfrom[1]. WefixtherotationanglesofHWPand
QWPθ1andθ2toθ1 =θ2 =0. Whenassumingthescenetobesolelydiffusive,theazimuthangleofthesurfacenormalcan
bedirectlyfoundusingtheLP.Intuitively,theobservedintensityI aftertheLPwillhavemaximawheretherotationangle
θ4 oftheLPandtheangleofpolarizationofthereflectedlightalign. Intheory,theintensityvariessinusoidallywithrespect
to θ4. The phase-shift ϕ or position of the maxima translates directly to the azimuth angle of the surface normal up to an
ambiguityof180°. ThemeasuredintensityI attheAPDisgivenas
I +I I I
I(ϕ,θ4)= max min + max − min cos 2θ4 2ϕ , (1)
2 2 −
(cid:0) (cid:1)
where I and I are the maximal and minimal observed intensities, respectively. Note that for our setup, the rotation
max min
angleθ3mustbesettoθ3 =θ4toeliminatetheeffectoftheQWPinthereceiver.
WeusethisacquisitionapproachasanintuitiveassessmentofthePolLidar. Tothisend,wesetupsceneswithflattargets
placedwithdifferentazimuthanglesinfrontofthesensor. AnexampleisshowninFig.4,wheretheobjectinfocusisthe
bluemetalplateorientedtotherightandleft,respectively. Next,weincreasetherotationangleθ4oftheLPinstepsof2°and
average10wavefrontsperrotationangletolimittheeffectofsensornoise. AsshownontherightofFig.4,weobservethat
the intensity for both scenes can be described as a sinusoid. Furthermore, the phase-shift between left- and right-oriented
target is clearly visible and is close to the difference in the azimuth angle of the two metal plates. As such, we find that
PolLidarisabletoscanasceneaccuratelyenoughtocapturepolarizationcues.
1.4.Real-TimeCapability
By design, the PolLidar prototype is not real-time capable. To allow the largest possible flexibility we opted for finely
controllable orientations of the wave plates, allowing us to adjust the polarization states instead and capturing all possible
ecnaruccO+60° yaw
-60° yaw
+60° yaw
-60° yaw
LP Rotation Angle
Figure4. AssessmentofPolarizationCues. WeincreasetherotationangleoftheLPθ4 instepsof2°andobserveasinglepixelonthe
right-andleftwardsorientedbluemetalplate.Wefindthattheintensityvariessinusoidallywithrespecttoθ4andweobserveaphaseshift
dependingonthenormal,adheringwellwiththetheoryandexperimentsfrom[1].
combinations of emitted and captured polarization states. As a consequence setting the polarization state requires consid-
erable time. However, we imagine running the measurement in parallel as in passive cameras which employ four constant
states.2. Indetail,thiswouldrequiremultiplelinearpolarizersatfixedrotationangles,whichareplacedinfrontofanarray
ofAPDssimilartoaBayer-PatternforRGBcameras. Insummary,thisworkcontributesafirstproofshowcasingpolariza-
tionlidarbridgingpreviousapplicationdomainsallowingrequireddistancesandresolutionsforautonmousdrivingvehicles
especiallyenhancingthegeometricrepresentationforhighdistantobjects.
Futureworksmayoptimizethenumberofrequiredrotationanglesforreal-timecapablereconstructioninautomotivescenar-
ios.
2.LidarForwardModelandSimulator
Inthefollowing,weformalizethecomponentsofthepolarimetriclidarforwardmodelinSection2.1. Then,wediscussthe
necessarychangesmadetotheCARLAsimulatorandlastlyshowhowwetunenoisecharacteristics.
2.1.Temporal-PolarimetricLidarForwardModel
Following the formulation from Baek et al. [2], we provide an overview of the individual components of the lidar forward
modelinthefollowing.
LinearPolarizer AlinearpolarizeratarotationangleθwithrespecttoareferenceaxishasaMuellermatrixgivenas
1 cos2θ sin2θ 0
1 cos2θ cos22θ cos2θsin2θ 0
L=  . (2)
2 sin2θ cos2θsin2θ sin22θ 0
 0 0 0 0 
 
2see for example the Sony IMX253MZR image , https://www.sony-semicon.com/files/62/flyer_industry/IMX250_264_
253MZR_MYR_Flyer_en.pdf.
]V[
ytisnetnIWavePlate TheMuellermatrixofawaveplatewithretardanceϕatangleθtothehorizontalisdefinedas
1 0 0 0
0 R R R
R= 11 12 13 ,
0 R R R
21 22 23
 0 R R R 
 31 32 33 
R = cos22θ+sin22θcosϕ, 
11
R =sin2θcos2θ(1 cosϕ),
12
−
R =sin2θsinϕ,
13
R =sin2θcos2θ(1 cosϕ),
21
−
R =cos22θcosϕ+sin22θ,
22
R = cos2θsinϕ,
23
−
R = sin2θsinϕ,
31
−
R =cos2θsinϕ,
32
R =cosϕ. (3)
33
Idealhalf/quarter-waveplateshavearetardancevaluesofπandπ/2,respectively,resultingintheMuellermatrices
W(θ)=R(θ,ϕ=π), (4)
Q(θ)=R(θ,ϕ=π/2). (5)
Surface Fresnel Reflection and Transmission The transmitted and reflected components are represented as the Fresnel
Muellermatrices
F =
T,R
F⊥+ 2F∥ F⊥−2F∥ 0 0

F⊥−2F∥ F⊥+ 2F∥ 0 0
. (6)
0 0 √F F cosδ √F F sinδ
⊥ ∥ ⊥ ∥
 
 0 0 √F F sinδ √F F cosδ 
 − ⊥ ∥ ⊥ ∥ 
 
WecomputetheperpendicularandparallelFresnelcoefficientsF , forreflectionandtransmission[4],respectively. δisthe
⊥∥
phaseshiftthathasthevalueofϕor0forthedielectriccomponent.
CoordinateConversion Acoordinate-conversionMuellermatrixhasthefollowingform
1 0 0 0
0 cos2θ sin2θ 0
C= , (7)
0 sin2θ cos2θ 0
−
 0 0 0 1 
 
 
whereθistherotationangleoftheinputxbasisvector.
Depolarization Matrices The depolarization matrices for diffuse Dd and specular reflection Ds are diagonal matrices
wherethej-thdiagonalentryisgivenby
(τ t2 )
D{ jd,s }(τ)= |D |{d,s }exp − 2σ2peak !. (8)
Asdenotedinthemainpaper, Dd and Ds describetheamplitudeofthedepolarizationmatricesandt isthetemporalin-
peak
| | | |
dexofthewavefrontpeak. Theparameterσdefinesthepulsewidthwhichwetunesuchthatthepulsewidthofdownsampled
syntheticraysresemblethepulsewidthoftherealdevice.Micro-facetDistribution WeusetheSmithShadingandmaskingtermGfrom[7]andtheGGXfacetdistributiontermD
from[11]definedasfollows
m2
D(θ ;m)= ,
h πcos4θ (m2+tan2θ )2
h h
2 2
G(θ ,θ ;m)= , (9)
i o
1+ 1+m2tan2θ 1+ 1+m2tan2θ
i o
whereθ isthehalf-wayangle,θ andθ aretheincpidentandoutgoingangples. misthesurfaceroughness.
h i o
LidarForwardModel Withallindividualcomponentsofthelidarforwardmodelinhand, theMuellermatrixMofthe
sceneisgivenassumofspecularanddiffusereflectionas
D(θ ;m)G(θ ,θ ;m)
M(τ)= h i o Ds(τ)F +C FoDd(τ)Fi C , (10)
4cosθ icosθ
o
R n →ω T T ω →n
diffuse
specular
| {z }
whereθ = cos 1(h n),θ = co|s 1(n ω),θ {=z cos 1(n ω),}nisthesurfacenormalandω istheviewingdirection.
h − i − o −
· · ·
Afterapplyingdistancedependentattenuationandthecosineshading,thetemporal-polarimetricMuellermatrixofthescene
canbewrittenas
n ω
H= · M. (11)
d2
TheMuellermatrixoftheemitterP ,consistingofHWPandQWP,isdefinedas
i
P =Q(θ2)W(θ1). (12)
i i i
TheMuellermatrixofthereceiverA ,consistingofQWPandLP,isdefinedas
i
A =L(θ4)Q(θ3). (13)
i i i
ThelaseroutputshorizontallypolarizedlightdefinedbytheStokesvectors = [1,1,0,0]T. Hence,theStokesvectorof
laser
thelightreceivedattheAPDcanbemodeledas
s
APD
=P i(θ i)HP i(θ i)s
laser
(14)
wherethefirstelementofs equalstheintensitymeasuredbytheAPD.
APD
2.2.ImplementationDetailsofPolarizationCARLASimulator
We implement the previously discussed polarimetric lidar forward in CARLA. To this end, we rely on the full-wavefront
lidarsimulatorpresentedby[6]. Weextractthenecessarynormalsn,indexofrefractionµ,diffuseandspeculardepolariza-
tion amplitude Dd , Ds and roughness m from CARLA. Analogous to [6], we use custom material cameras for diffuse
| | | |
amplitude Dd ,specularamplitude Ds ,roughnessm. Whenfollowingthenotationof[6],thediffuseamplitudeequalsd,
| | | |
thespecularamplitudeequalssandroughnesstranslatestoα.
However,theindexofrefractionµisnotassignedtomaterialsinCARLAbydefault. Tothisend,wemodifytheCARLA
ray-tracertoreturnamaterialIDforeachhitpoint. Whentheray-tracerreturnsahitpoint, wequerythefaceofthemesh
at this hit point for the name of the assigned material. Worlds in CARLA have more than 5000 assigned materials. We
cluster the materials based on their name and their respective parent material. In total, we define 10 clusters and assign
an index of refraction to each cluster, which we subsequently look-up during rendering. This method is suitable for static
objects like buildings, infrastructure, or vegetation. However, moving objects such as vehicles in CARLA are considered
only with a simplified mesh to reduce the computational cost for the ray-tracing. This simplified mesh does not have any
materialsassignedtoitasitwasnotintendedforrenderingbutonlyforextractingdistanceinformationwiththeray-tracer.
WecircumventthisproblembymanuallyassigningmaterialstoeachfaceofavehiclemeshasshowninFig.5.
Finally,weextendtheray-tracertoreturnthesurfacenormalnforeachhitpoint. TheUnrealEngineunderlyingCARLA
providesastraightforwardinterfaceforqueryingthemeshfornormalsatahitpoint. Forextractingthenormal, weaddan
extrachannelforthenormalstothelidarmodelintheC++codeandadapttheauxiliaryPythoninterfaceaccordingly. Asthe
UnrealEnginereturnsnormalsinworld-coordinates,wetransformthenormalsintothelocalsensorframe.Glass
Rubber
Steel
Figure5.AssignedmaterialtocarmeshesinCARLA.Theray-tracerinCARLAusessimplifiedmeshesforcarstoreducerenderingcosts.
Thesemeshesdonothaveamaterialassignedtothem.Wemanuallyassign3materialstoeachfaceofthecarmeshes,e.g.,glass,steeland
rubber.
2.3.SimulatingSensorNoise
Thewavefronts˜IobtainedfromourCarlasimulatorarenoise-free,andwecorruptthesyntheticimagesbyaddingsimulated
sensornoise. Therawwavefrontsareaddedbyanoiseη (˜I),whichconsistsofaPoissoniansignal-dependentnoiseand
SENSOR
aGaussiansignal-independentcomponent
η (˜I)=η (˜I,a )+η (σ ), (15)
SENSOR p p g g
whereη aPoissoniansignal-dependentcomponent,andη aGaussiansignal-independentcomponent.Tochoosethecorrect
p g
parameters for a and σ , we rely on the noise characteristics presented in Sec. 1.2 and set a and σ to 1e-3 and 1e-4,
p g p g
respectively.
3.AdditionalDetailonDataset
In the following, we provide additional details about ground truth generation and acquisition of the real-world large-scene
polarimetriclidardataset.
3.1.GroundTruth
AsshowninFig.6,forgeneratinggroundtruthdistanceandnormals,wepairthePolLidarsensorwithaVelodyneVLS-128
reference lidar. Both lidars are mounted on a movable platform allowing to scan a scene from different positions. After
PolLidaracquisitioniscompleted, wemovethereferencelidarthroughthescenetoobtainmultiplereferencepointclouds
fromdifferentpositions. WeaccumulatethereferencepointcloudsafterpreviousregistrationwiththeIterative-Closest-Point
(ICP)algorithmpresentedin [10]toobtainadenseaccumulatedlidarmap.
Distance Thelidarmapisthenusedtogenerategroundtruthdistancesd . Tothisend,wefirsttransformthelidarmapto
gt
PolLidarcoordinatesusingthetransformationT R4x4. WeobtainTbyiterativelyaligningreferenceandPolLidarpoint
∈
clouds for different scenes by means of the ICP algorithm as implemented by [12]. Next, we can extract the ground truth
distancebycalculatingazimuthandelevationangleforeachpointofthelidarmapandthenapplyingbilinearinterpolation
withtheviewencodingV ofthePolLidar. Aswemovetheprototypethroughthescene, thelidarmapmighthaveseveral
valid distances per viewing direction that are occluded from one viewpoint but visible from another. This would severely
distort the ground truth when applying interpolation to the full lidar map. Thus, before interpolation, we remove hidden
points that are invisible from the PolLidar viewpoint by means of [9]. Eventually, we output a ground truth distance map
d
gt
RH ×W encodingthegroundtruthdistanceforeveryviewingdirection.
∈
Normals For generating ground truth normals, we first mesh the lidar map using [8] and then query the mesh at the
ground truth point locations for their normals. Compared to ray-tracing with the viewing direction, querying is beneficial
as the meshing method often introduces artifacts around object edges enlarging the object. Ray-tracing would produce
erroneousnormalsasthetrueobjectispossiblyoccludedbyanenlargededge. Finally,weoutputagroundtruthnormalmap
n
gt
RH ×W ×3encodingthegroundtruthnormalforeveryviewingdirection.
∈Reference lidar Reference lidar map Scene
Accumulation
with Registration
GT normals
Mesh with normals
Mesh with
Query
NKSR
Transform
GT point cloud
View encoding GT distance
Bilinear
PolLidar Interpolation
ToF Mask
Figure6.AcquisitionofGroundTruthData.WemovePolLidarandreferencelidarthroughascenetocapturedensereferencelidarmaps.
WeinterpolatethelidarmapswiththeviewingdirectionofthePolLidartoextractGTdistanced information. ForGTnormalsn ,we
gt gt
firstmeshthelidarmapandextractnormalsfromthemeshbyqueryingwiththeGTpointcloud.
Mask Forcertainpixels/viewdirections,noinformationcanbeextractedfromtherawwavefront,whene.g. noobjectwas
hit,orthemeasuredintensityfallsbelowthenoisefloor. WeusetheToFmapdextractedfromthesensorwithconventional
peak-finding to exclude these points from training. To this end, we compare if the ToF map and ground truth distance are
withinacertainboundanddefinethemaskofvalidpixels/viewdirectionsas
1 if d d <d
c= | − GT | thresh , (16)
(0 otherwise
wherec RH ×W denotesthebinarymaskofvalidviewdirectionsandd threshisathresholdwedefineas0.8m. Furthermore,
∈
the mask c is helpful for eliminating erroneous ground truth. For instance, erroneous ground truth distance are likely to
appeararoundobjectedgesduetoaccumulationerrorsandresolutionlimitsofthereferencelidar,resultinginawideningof
objectedges. ThemaskexcludesthesepointsasshowninFig.6forthesilhouetteofthecar.
3.2.Acquisition
Inthefollowing,weprovidefurtherdetailsonhowweacquireframeswiththePolLidaranddescribethesettingsused.When
acquiringaframe, weperformrotatingellipsometryasdescribedine.g. [4]. Tothisend, wemeasure36differentrotation
anglecombinationsθ ,wherethesubscripti 0,1,...,35 isusedtodistinguishthedifferentcombinations. Theresulting
i
∈ { }
stokesvectoroftheemittedlightisvisualizedinthePoincare´sphereinFig.7. Asshown,thepolarizationstateoftheemitted
lightisuniformlydistributedalongthesphere. Fortheemitter,theHWPissettoθ1 =0andtheQWPtoθ2 =5 i. Forthe
i i ◦ ·
receiver,theQWPisrotatedtoθ3 =25 iandtheLPtoθ4 =0.
i ◦ · i
We opt to capture 10 frames for aggregation per rotation angle θ , as we find that it is a good compromise between
i
acquisitiontime,amountofgenerateddata,anddenoising.
Furthermore,wechoosealaserpowerof600mVand900mVforindoorandoutdoorscenesrespectively. Duetothehigh
sensitivityofintensitytowardsthebiasvoltage,wecapturethesamescenariowiththebiasvoltages 1980,2000,2020 mV.
{ }
AfteracquisitionwiththePolLidariscomplete,wecaptureareferencelidarmaptoextractgroundtruthasdiscussedin
Sec.3.1. Tothisend, wemovethesetupthroughthesceneryuntilwecoverasimilarareavisiblefromthePolLidarinthe
initialposition.Figure7.Poincare´spherevisualizingtheStokesvectorofthe36kindsofdifferentlypolarizedemittedlight.
4.Reconstruction
Theproposedreconstructionapproachisatwo-stepapproach. First, weapplyclassicalellipsometricreconstructiontodis-
entanglethescenefromthepolarizingopticsoftheemitterandreceiver. Fromthis,weobtaintheMuellermatrixHofthe
scene. Additionaldetailsonthis, areprovidedinSec.4.1. Next, wefeedthisasaninputtoaneuralnetworkthatpredicts
distanceoffsetsandnormals. AdditionaldetailsonthenetworkareprovidedinSec.4.3.
4.1.EllipsometricReconstruction
EllipsometricreconstructionisusedtodisentangletheMuellermatrixofthescenefromtheMuellermatricesofemitterP
i
and receiver A . As discussed in the paper, the additional ellipsometric reconstruction helps the network to learn a better
i
scenereconstruction. Inthissection,weprovidesomeadditionalellipsometricreconstructionresults.
InordertorecovertheMuellermatrixofthesceneH ,wesolvealeast-squaresoptimizationproblemasdefinedby
meas
N
minimize (I [A ,H ,P s ] )2, (17)
Hmeas
i=1
i − i meas i laser 0
X
see[2]. WevisualizethereconstructionapproachinFig.8, wheretheindividualelementsoftheMuellermatrixH are
meas
shown on the right. In order to validate the correctness of the reconstructed Mueller matrix, we then render the intensity
image using the lidar forward model and compare it with the measured intensities for the 36 different rotation angles θ .
i
This is visualized for the left of Fig. 4.1. On the top, we show the measured intensity for θ . On the bottom, we show
0
fortwoselectedpixels,there-rendered/reconstructedintensitiesoverthe36differentmeasurementsusingthepolarimetric
lidar forward model. We find that the reconstructed intensities are in good agreement with the measured ones. The small
deviationsarelikelyduetotheinherentlynoisymeasurements. Asidefromthedisentanglementfromthepolarizationoptics,
wethusbelievethattheellipsometricreconstructionprovidesadditionalbenefitasanadditionaldenoisingstep.
4.2.NetworkDetails
WepresentthedetailsofournetworkarchitectureinTab.3. Specifically,wefirstusetwoconvolutionlayerstoprocessthe
inputfeatures. Wethenuse4encoderlayerstoencodethefeatures,eachlayerconsistsofamax-poolingandtwoconvolution
layers. Atthebottleneck,weuse8transformerlayers[5]. Atlast,weuse4decoderlayerswithskip-connectiontotheprior
layer. Finally,weusea1 1convolutionlayertogetafour-channeloutput,whichisthenormalsanddistance,respectively.
×
4.3.TrainingDetails
We evaluate our method on both the simulation data and experimental data. Our simulated Carla dataset consists of 62
different scenes with different IDs. The contents of each scene are generally different. We select 44 scenes and 18 scenes
fortrainingandevaluationrespectively,leadingto1430and539testframes. Weapplydifferentlaserbiasesduringtraining.
Specifically,werandomsamplethebiasofoursimulatedPolLidarfrom10to900. Theintensitiesanddistancesfordifferent
biasesareshowninFig.9.neg. pos.
Intensity Reconstructed Mueller Matrix
0.18
0.12
0.23
0.21
0.19
0 5 10 15 20 25 30 35
Measurement Reconstructed
Measured
Figure8.EllipsometricReconstruction.WeshowtheindividualelementsofthereconstructedMuellermatrixontheright.Tovalidatethe
correctnessofthereconstruction,werendertheintensitiesusingthediscussedpolarimetriclidarforwardmodel,asshownontheleft. We
findagreementbetweenre-rendered/reconstructedandmeasuredintensities. Notethatdifferentcolorscalesareappliedtoeachelement
forbettervisualization.
Bias=10 Bias=30 Bias=100 Bias=300
0.4
0
150
0
Figure9.Frameswithdifferentbiases.Whenthebiasislow,alimitednumberofpointsaredetectedastheintensityfallsbelowthenoise
floor. Whenthebiasislarge,weseesaturationeffectsinregionsclosetothesensororinregionsofhighreflectivity. Weusedifferent
biasesattrainingtimetoincreasetherobustnessagainstsaturationandlow-intensityreadings.
5.AdditionalQuantitativeandQualitativeResults
Inthefollowing,weprovideadditionalquantitativeandqualitativeresultsonrealandsyntheticdatatovalidatetheproposed
methodfurther.
5.1.AdditionalSyntheticResults
In Fig. 11, we provide additional synthetic results. Consistent with the main paper, we find that existing SfP methods
underperforminlowDoPregions. Tofurthervalidatethis,weshowtheDoPdefinedas
H2 +H2
0,1 0,2
DoP= , (18)
q H
0,0
ytisnetnI
Intensity[mV]
Distance[m]Name Layersetting Outputdimension
Conv[3 3,64],InstanceNormalization,ReLU
inputconv × H W 64
Conv[3 3,64],InstanceNormalization,ReLU × ×
×
MaxPooling,stride2
encoder1 Conv[3 3,128],InstanceNormalization,ReLU 1H 1W 128
× 2 × 2 ×
Conv[3 3,128],InstanceNormalization,ReLU
×
MaxPooling,stride2
encoder2 Conv[3 3,256],InstanceNormalization,ReLU 1H 1W 256
× 4 × 4 ×
Conv[3 3,256],InstanceNormalization,ReLU
×
MaxPooling,stride2
encoder3 Conv[3 3,512],InstanceNormalization,ReLU 1H 1W 512
× 8 × 8 ×
Conv[3 3,512],InstanceNormalization,ReLU
×
MaxPooling,stride2
encoder4 Conv[3 3,512],InstanceNormalization,ReLU 1H 1W 512
× 16 × 16 ×
Conv[3 3,512],InstanceNormalization,ReLU
×
attention TransformerBlock 8 1H 1W 512
× 16 × 16 ×
Concat[encoder4outputs,attentionoutputs]
BilinearUpsamplingwithscale2
decoder4 1H 1W 512
Conv[3 3,512],InstanceNormalization,ReLU 8 × 8 ×
×
Conv[3 3,512],InstanceNormalization,ReLU
×
Concat[encoder3outputs,decoder4outputs]
BilinearUpsamplingwithscale2
decoder3 1H 1W 256
Conv[3 3,256],InstanceNormalization,ReLU 4 × 4 ×
×
Conv[3 3,256],InstanceNormalization,ReLU
×
Concat[encoder2outputs,decoder3outputs]
BilinearUpsamplingwithscale2
decoder2 1H 1W 128
Conv[3 3,128],InstanceNormalization,ReLU 2 × 2 ×
×
Conv[3 3,128],InstanceNormalization,ReLU
×
Concat[encoder1outputs,decoder2outputs]
BilinearUpsamplingwithscale2
decoder1 H W 64
Conv[3 3,64],InstanceNormalization,ReLU × ×
×
Conv[3 3,64],InstanceNormalization,ReLU
×
outputconv Conv[1 1,4] H W 4
× × ×
Table3.Detailsofournetworkarchitecture.“Concat”operationmeansthatweconcatenatetwoelementsalongthechanneldimension.
wherethesubscriptdenotestherespectiveindexoftheMuellermatrixH. WeshowtheDoPforthetwoselectedscenesof
Fig.5inthemainpaperinFig.10. Whennormalsandviewingdirectionarealigned,asvisualizedinthetworightcolumns,
theDoPislow. Thisistruefore.g. buildingsandpartsofthecarthatfacethesensor. Contrarily,thesideorhoodofthecar
is for instance a high DoP region as viewing direction and normal are almost perpendicular. Consequently, Baek et al. [2]
areunabletoreconstructnormalsintheselowDoPregions,asshowninFig.11. ForhighDoPregions,however,satisfying
performanceisachieved.
WealsocomparethenormalreconstructiontoPCAasapoint-cloudbasedmethodthatconsidersaneighborhoodofpoints.
Thismethodperformswellinareaswithflatgeometryandhighpointdensitybutdegradessignificantlyatlongranges,e.g.,
carsinfardistances,andgeometrytransitionregions,e.g.,theareabetweenroadandcar. Theproposedmethodleveragesthe
additionalcuesfrompolarizationtoresolvenormalsinregionswithsparsepointsandintransitionregions. Furthermore,the
proposedmethodachievessatisfyingreconstructionresultsforregionswithlittlepolarizationinformationbytakingalocal
neighborhoodintoaccount.
5.2.AdditionalReal-WorldResults
Weprovideadditionalqualitativeresultsfromthereal-worlddatasetinFig.12. Wefindsimilartrendsasinthemainpaper.
TheproposedmethodconsistentlyoutperformsPCAinareaswherethepointcloudissparse. Thisisvisibleinthezoom-ins
onthefinestructures,e.g.,carroofs,wheretheneighborhoodistoosparseforPCAtoreconstructcorrectsurfacenormals.
Thisisvisiblefortheroofstructureinthefourthroworthecarontheleftinthefifthrow. ThepointcloudvisualizationalsoNormals View Direction DoP
Normals
DoP
0.4
0.2
0
Figure10. Degree-of-Polarization(DoP):Ontheright,weshowtheDoPforthetwoscenesofFig.5inthemainpaper. Whencomparing
withnormalsandviewingdirection,wefindlowDoPregionsonobjectswherenormalsandviewingdirectionalign. Thisisproblematic
forotherSfPreconstructionapproachesaslittlepolarizationcuesarepresentintheseareasforasuccessfulreconstruction.
revealsthebenefitofthedistancereconstructioncomparedtoconventionalargmax-methodsinthecontextofreconstructing
finedetails. ThisispronouncedinthefourthrowofFig.12forthebumper/headlightareaofthecarontheleft. Theproposed
methodisabletoreconstructthepocketinthechassiswheretheheadlightislocatedcorrectly. ThisdetailislostinthePCA
results,ashere,conventionalpeak-findingisapplied. Furthermore,ourmethodprovideshigh-qualityreconstructionresults
in the area around the grill of the car. Additionally, we show data in night-time conditions where the PolLidar expectedly
showssimilarreconstructionresultsasduringday-timeconditions.
5.3.AdditionalDistance-BinnedEvaluationofNormalReconstruction
Tofurtheranalyzetheadvantageofourapproachovertheexistingbaselineinregionsoflowpointdensity, weanalyzethe
meanangularerrorfornormalreconstructionandbintheresultsperdistancebin.Specifically,thisanalysisallowsustostudy
furtherdistances inmore detail, wherepoint distancedue toconstant angularsamplingincreases further, makingPCA fail
duetothemissingneighborhood. QuantitativeresultsarevisualizedinFig.13,showingin(a)anincreasedperformancedue
topolarizationinclosedistancebyapprox. factoroftwocomparedtoPCA.Thereby,forfurtherdistancesPCAdegenerates
by75%beingoutperformedbyafactorof2.5. Furthermore,ourmethodisabletocopewithsparserpointcloudsandshows
asubstantiallylowerdependencyofreconstructionperformanceondistance. InFig.13(b),weplottherelativemeanangular
errorbetweenPCAandtheproposedmethod. Weseethatthegaininreconstructionqualityiscloselyrelatedtothedistance.
5.4.AdditionalMetricsforDistanceEvaluation
Toevaluatethedistanceestimation,wecompareagainsttheconventionalargmax-peak-findingtypicallyperformeddirectly
onthedevicebylow-levelelectronics[3]. WelistallevaluatedmetricsonthedistancereconstructioninTab.4andTab.5.
Inadditiontotheresultsinthemainpaper, wepresentheremedianandRMSEdistanceerrors. Wefindthatourapproach
outperforms conventional peak-finding on these metrics by large margins on both synthetic and real data, outperforming
previousresultsby41%onsyntheticdataand17%onreal-worlddataformeanabsolutedistanceerror. Thecomparatively
lowergaininreal-worlddataislikelyrelatedtothequalityofthegroundtruth. Forsyntheticdata,wehaveperfectground
truthaswehavecontrolovertheentirerenderingpipelineincludingtheunderlyinggeometry. However, forthereal-world
data,wearelimitedbyourappliedgeometryreconstructiondescribedinSec.3.1. Thishasinherentsensornoiseobfuscating
distance and inaccuracies of the accumulation of our ground-truth though pose estimation errors, beam divergence broad-
eningobjectdimensions, andmanymore. Consequently, themedianerrorisincreasedforbothconventionalandproposed
distanceestimationmethods. IncontradictiontothatisthehigherRMSEinthesyntheticdatawhichcanbeexplainedwith
outliers in the distance error. More specifically, if the conventional peak-finding method selects the wrong peak or misses
thepeakaltogether,e.g.,duetolowintensitiesclosetothenoisefloor,thedistanceerrorwillbeofmanymetersforboththe
conventionalandproposedmethod,thusincreasingtheRMSEsignificantly. Fortherealdata,theeffectofincreasedRMSE
isnotasprominent,asweapplythemaskdependentond effectivelysuppressingtheseoutliers.
threshGroundtruth Baeketal. PCA Proposed
Figure11. AdditionalSyntheticResults. Baeketal.[2]isunabletoreconstructnormalsinareaswithlowDoP,e.g.,wallsofbuildings
facingthesensor. PCA[13]appliedinthissettingisstronglydependentonpointclouddensity. Thisisvisiblefore.g. thepolesinfar
distances.TheproposedapproachleveragespolarizationcuestoreconstructnormalsinsparseregionsandisrobustagainstlowDoPareas.Ground Truth PCA Proposed GT PCA Proposed
Scene
Scene
Scene
Scene
Scene
Scene
Scene
Normals
Figure12.AdditionalReal-worldResults.Weshowday-andnight-timeautomotivescenes.PCAgenerateshigh-qualitysurfacesinareas
withhighpointdensity. However,forfinedetailsorobjectsfaraway,wherethepointcloudissparse,thereconstructionisoflowquality.
Theproposedmethod,however,leveragesthepolarizationcuesintheseareasandthusoutperformsPCAinregionsoflow-pointdensity.
Thiscanbeseeninthezoom-insshownontheright,e.g.bumperareaofthecarinthefourthroworcarroofsinthefifthandsixthrows.20.0
PCA
Proposed
17.5
9
15.0
8 12.5
10.0
7
7.5
6
5.0
2.5
5
0.0
10 20 30 40 50 60 70 80 90 100 10 20 30 40 50 60 70 80 90 100
Distance (m) Distance (m)
(a)Absolutemeanangularerror (b)Margin
Figure13. Meanangularerrordependentondistance. In(a),PCAachievesreasonableperformanceforcloseregionsasthepointclouds
aremoredensehere. However,asthedistanceincreases,themeanangularerrorofPCAincreasesrapidly. In(b),weplotofthemargin
betweenPCAandtheproposedapproach.Wecanseetheperformancegainiscloselycorrelatedwiththedistance.
Method DistanceError[m]
↓
Mean Median RMSE
Conventional 0.32 0.12 3.25
Proposed 0.19 0.03 3.06
Table4. QuantitativeDistanceErroronSyntheticData. Ourapproachoutperformsexistingbaselinesonallevaluateddistancemetrics.
Conventionaldistanceestimationislimitedbythetemporalresolutionofthesensor. Ourapproachleveragesthewavefrontinformation,
thusoutperformingtheconventionaldistanceestimationapproach.
Method DistanceError[m]
↓
Mean Median RMSE
Conventional 0.24 0.20 0.29
Proposed 0.20 0.18 0.26
Table5.QuantitativeDistanceErroronReal-worldData.Duetonoisiergroundtruthandsensorimperfections,theoverallerrorisslightly
largercomparedtosyntheticdata,whileconfirmingthetrendfromthesyntheticevaluation.Theproposedmethodreliesonwavefrontdata
andpointneighborhoods,outperformingtheconventionalapproach.
)o(
rorre
ralugna
naeM
)o(
nigraMReferences
[1] GaryA.AtkinsonandEdwinR.Hancock. Recoveryofsurfaceorientationfromdiffusepolarization. IEEETrans.ImageProcess.,
15(6):1653–1664,2006. 4,5
[2] Seung-HwanBaekandFelixHeide. All-photonpolarimetrictime-of-flightimaging. ProceedingsoftheIEEEConferenceonCom-
puterVisionandPatternRecognition(CVPR),2022. 5,10,12,14
[3] BehnamBehroozpour, PhillipAMSandborn, MingCWu, andBernhardEBoser. Lidarsystemarchitecturesandcircuits. IEEE
CommunicationsMagazine,55(10):135–142,2017. 13
[4] EdwardCollett. Fieldguidetopolarization. SpieBellingham,WA,2005. 6,9
[5] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,ThomasUnterthiner,MostafaDehghani,
MatthiasMinderer, GeorgHeigold, SylvainGelly, etal. Animageisworth16x16words: Transformersforimagerecognitionat
scale. arXivpreprintarXiv:2010.11929,2020. 10
[6] FelixGoudreault,DominikScheuble,MarioBijelic,NicolasRobidoux,andFelixHeide.Lidar-in-the-loophyperparameteroptimiza-
tion. 2023. 2,7
[7] EricHeitz. Understandingthemasking-shadowingfunctioninmicrofacet-basedbrdfs. JournalofComputerGraphicsTechniques,
3(2):32–91,2014. 7
[8] JiahuiHuang,ZanGojcic,MatanAtzmon,OrLitany,SanjaFidler,andFrancisWilliams. Neuralkernelsurfacereconstruction. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages4369–4379,2023. 8
[9] SagiKatz,AyelletTal,andRonenBasri. Directvisibilityofpointsets. InACMSIGGRAPH2007papers,pages24–es.2007. 8
[10] IgnacioVizzo,TizianoGuadagnino,BenediktMersch,LouisWiesmann,JensBehley,andCyrillStachniss. Kiss-icp: Indefenseof
point-to-pointicp–simple,accurate,androbustregistrationifdonetherightway. IEEERoboticsandAutomationLetters,8(2):1029–
1036,2023. 8
[11] BruceWalter,StephenRMarschner,HongsongLi,andKennethETorrance.Microfacetmodelsforrefractionthroughroughsurfaces.
InProceedingsofthe18thEurographicsconferenceonRenderingTechniques,pages195–206,2007. 7
[12] Qian-YiZhou,JaesikPark,andVladlenKoltun. Open3D:Amodernlibraryfor3Ddataprocessing. CoRR,abs/1801.09847,2018. 8
[13] YufanZhu,WeishengDong,LeidaLi,JinjianWu,XinLi,andGuangmingShi. Robustdepthcompletionwithuncertainty-driven
lossfunctions. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume36,pages3626–3634,2022. 14