Solving Poisson Equations using Neural Walk-on-Spheres
HongChulNam*1 JuliusBerner*2 AnimaAnandkumar2
Abstract
100 Neural WoS (Ours)
WeproposeNeuralWalk-on-Spheres(NWoS),a DeepRitz
novelneuralPDEsolverfortheefficientsolution 10 1 D PIi Nff Nusion Loss
of high-dimensional Poisson equations. Lever-
Neural Cache
aging stochastic representations and Walk-on- 10 2
Spheres methods, we develop novel losses for
neuralnetworksbasedontherecursivesolution
10 3
of Poisson equations on spheres inside the do-
main. Theresultingmethodishighlyparalleliz-
10 4
able and does not require spatial gradients for
theloss. Weprovideacomprehensivecompari-
0 200 400 600 800 1000
sonagainstcompetingmethodsbasedonPINNs, Time (s)
theDeepRitzmethod, and(backward)stochas- Figure1.ConvergenceoftherelativeL2-errorwhensolvingthe
ticdifferentialequations. Inseveralchallenging, 10dLaplaceequationinSection5usingourconsideredmethods.
high-dimensionalnumericalexamples,wedemon-
stratethesuperiorityofNWoSinaccuracy,speed,
andcomputationalcosts. Comparedtocommonly particular,grid-basedmethods,suchasfinite-element,finite-
used PINNs, our approach can reduce memory volume,orfinite-differencemethods,scaleexponentiallyin
usage and errors by orders of magnitude. Fur- theunderlyingdimension. Ontheotherhand,deeplearning
thermore,weapplyNWoStoproblemsinPDE- approaches have been shown to overcome this so-called
constrainedoptimizationandmoleculardynamics curseofdimensionality(DeRyck&Mishra,2022;Duan
toshowitsefficiencyinpracticalapplications. etal.,2021;Berneretal.,2020b).Correspondingalgorithms
aretypicallybasedonMonteCarlo(MC)approximations
ofvariationalformulationsofPDEs.
1.Introduction
Inthiswork,wefocusonhigh-dimensionalPoissonequa-
tionsongeneraldomains. Wenotethattheaccuratenumer-
PartialDifferentialEquations(PDE)arefoundationaltoour
ical solution of such types of PDEs is crucial for a large
modernscientificunderstandinginawiderangeofdomains.
varietyofareas. Forinstance,Poissonequationsarepromi-
Whiledecadesofresearchhavebeendevotedtothistopic,
nentingeometryprocessing(Sawhney&Crane,2020),as
numerical methods to solve PDEs remain expensive for
wellasmanyareasoftheoreticalphysics,e.g.,electrostatics
many PDEs. In recent years, deep learning has helped
and quantum mechanics (Bahrami et al., 2014). In high
toacceleratethesolutionofPDEs(Azzizadeneshelietal.,
dimensions,theygovernimportantquantitiesinmolecular
2023; Zhang et al., 2023b; Cuomo et al., 2022) as well
dynamics,suchaslikelytransitionpathwaysandtransition
astackle PDEs, whichhadbeen entirelyoutof rangefor
ratesbetweenregionsorconformationsofinterest(Vanden-
classicalmethods(Hanetal.,2018;Scherbelaetal.,2022;
Eijndenetal.,2006;Lu&Nolen,2015).
Nu¨sken&Richter,2021b).
Severaldeeplearningmethodsareamendabletothenumer-
AmongthebiggestchallengesforclassicalnumericalPDE
icalsolutionofPoissonequations. Thisincludesphysics-
solvers are complex geometries and high dimensions. In
informedneuralnetworks(PINNs)orDeepGalerkinmeth-
*Equalcontribution 1ETHZurich2Caltech. Correspondence ods(Raissietal.,2019;Sirignano&Spiliopoulos,2018),
to: HongChulNam<honam@student.ethz.ch>,JuliusBerner theDeepRitzmethod(Eetal.,2017),aswellasapproaches
<jberner@caltech.edu>.
basedon(backward)stochasticdifferentialequations(Han
et al., 2017; Nu¨sken & Richter, 2021a; Han et al., 2020).
Proceedings of the 41st International Conference on Machine
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by However,previousmethodssufferfromunnecessarilyhigh
theauthor(s). computationalcosts,bias,orinstabilities,seeSection3.
1
4202
nuJ
5
]GL.sc[
1v49430.6042:viXra
rorre
2L
evitaleRSolvingPoissonEquationsusingNeuralWalk-on-Spheres
Table1.ComparisonofneuralPDEsolverforPoissonequations.
#Derivatives,#Lossterms,andCostdenotetheorderofspatial
derivatives,thenumberoftermsrequiredinthelossfunction,and
thecomputationalcostforonegradientstep.Propagationspeed
describeshowquicklyboundaryinformationcanpropagatetothe
interiorofthedomain,seeSection3fordetails.
Method #Derivatives #Loss Cost Propagation
terms speed
PINN 2 2 medium slow
Figure2.Left:Time-discretizationofthesolutionXξtotheSDE
DeepRitz 1 2 low slow
in(6)withstoppingtimeτ(Ω,ξ)in(7)forthedomainΩ=[0,1]2.
Feynman-Kac 0 1 high fast
Right:RealizationoftheWalk-on-SpheresalgorithminSection4. BSDE 1 1 high fast
Diffusionloss 1 2 medium medium
Walk-on-Spheres(WoS): Toovercometheabovechal- NWoS(ours)1 0 1 low fast
lenges, we propose a novel approach based on so-called
Walk-on-Spheres(WoS)methods(Muller,1956). TheWoS
Neural WoS (Ours)
method is a Monte Carlo method specifically tailored to-
104 DeepRitz
wardPoissonequationsbyrewritingtheirsolutionsasan Diffusion Loss
PINN
expectationoverBrownianmotionsstoppedatthebound-
aryofthedomain. SimulatingtheBrownianmotionusing
103
time-discretizations either is slow or introduces bias (de-
pendingonthechosentimestep). Leveragingtheisotropy
ofBrownianmotion,WoSacceleratesthisprocessbyiter-
102
ativelysamplingfromspheresaroundthecurrentposition
untilreachingtheboundary,seeFigure2.
10 100 1000
However,aswithallMonteCarlomethods,WoScanonly
Dimension (d)
obtain pointwise estimates and suffers from slow conver-
Figure3.PeakGPUmemoryusageofdifferentmethodsduring
gencew.r.t.thenumberoftrajectories. Inparticular,every
trainingwithbatchsize512forthePoissonequationinSection5
sufficiently accurate estimate of the solution on a single indifferentdimensionsd.
point takes a considerable amount of time. This is pro-
hibitiveifmanysolutionevaluationsareneededsequentially,
Ourcontributionscanbesummarizedasfollows:
e.g.,inPDE-constrainedoptimizationproblems.
• WeanalyzepreviousneuralPDEsolverandtheirshort-
Our approach (NWoS): We develop Neural Walk-on-
comingswhenappliedtohigh-dimensionalellipticPDEs,
Spheres(NWoS),aversionofWoSthatcanbecombined
suchasPoissonequations(Section3).
with neural networks to learn the solution to (parametric
familiesof)Poissonequationsonthewholedomain. Our • Wedevisenovelvariationalformulationsforthesolution
methodamortizesthecostofWoSduringtrainingsothat of Poisson equations based on WoS methods and pro-
thesolution,anditsgradients,canbeevaluatedinfractions vide corresponding theoretical guarantees and efficient
ofsecondsafterward(andatarbitrarypointsinthedomain). implementations(Section4).
In particular, in order to obtain accuracy ε, the standard • Wecompareagainstpreviousapproachesonaseriesof
WoS method incurs a cost of O(ε−2) trajectories for the benchmarksanddemonstratesignificantimprovementsin
evaluationofthesolutionwhileNWoShasareducedcost termsofaccuracy,speed,andscalability(Section5).
ofasingleO(1)forward-passofourmodel.
Usingthepartiallytrainedmodelasanestimator,wecan 2.Relatedworks
limit the number of simulations and WoS steps for train-
Neural PDE solver: We provide an in-depth compari-
ingwithoutintroducinghighbiasorvariance. Theresult-
sontocompetingdeeplearningapproachestosolveellip-
ingobjectiveismoreefficientandscalablethancompeting
tic PDEs in Section 3. These include physics-informed
methods,withouttheneedtobalancepenaltytermsforthe
neural networks (PINNs) (Raissi et al., 2019; Sirignano
boundaryconditionorcomputespatialderivatives(Table1).
& Spiliopoulos, 2018), the Deep Ritz method (Jin et al.,
Inparticular,wedemonstrateasignificantreductionofGPU
memoryusageincomparisontoPINNs(Figure3)andup
1WhilenotnecessaryforNWoS,wenotethatthegradientof
toordersofmagnitudebetterperformanceforagiventime the model and an additional boundary loss can still be used to
andcomputebudget(Figure1). improveperformance,seeSection4.5.
2
)BM(
eziS
yromeMSolvingPoissonEquationsusingNeuralWalk-on-Spheres
2017), and the diffusion loss (Nu¨sken & Richter, 2021a), highlevel,theyproposedifferentvariationalformulations
see also Table 1. The diffusion loss can be viewed as an min L[v]withthepropertythattheminimizerovera
v∈V
interpolationbetweenPINNsandlossesbasedonbackward suitable function space V ⊂ C(Ω) is a solution u to the
SDEs(BSDEs)(Hanetal.,2017;Eetal.,2017;Becketal., PDEin(1). ThespaceV isthentypicallyapproximatedby
2019). Methods based on BSDEs and the Feynman-Kac asetofneuralnetworkswithagivenarchitecture,suchthat
formula (Beck et al., 2018; Berner et al., 2020a; Richter theminimizationproblemcanbetackledusingvariantsof
&Berner,2022)havebeeninvestigatedforthesolutionof stochasticgradientdescent.
parabolicPDEs, wheretheSDEisstoppedatagiventer-
minaltime. Duetocostlysimulationtimes,theycannotbe 3.1.StrongandweakformulationsofellipticPDEs
appliedefficientlytoellipticproblems. Tocombatthisissue,
Letusstartwithmethodsbasedonstrongorweakformula-
wedrawinspirationfromWalk-on-Spheresmethods.
tionsofthePDEin(1).
MonteCarlo(MC)methods: Sincegrid-basedmethods
Physics-informedneuralnetworks(PINNs): Initsba-
cannottacklehigh-dimensionalPDEs,MCmethodsaretyp-
sic form, the loss of PINNs (Raissi et al., 2019) or Deep
ically used. For Poisson equations, the WoS method has
Galerkin methods (Sirignano & Spiliopoulos, 2018), is
beendevelopedbyMuller(1956)andhassincebeensuc-
givenby
cessfullyusedinvariousscientificsettings(Sabelfeld,2017;
Jubaetal.,2016;Bossyetal.,2010)aswellasrecentlyin
L
[v]:=E(cid:2) (P[v](ξ)−f(ξ))2(cid:3)
+βL [v], (3)
computergraphics(Qietal.,2022;Sawhneyetal.,2022).In PINN bnd
thelatterdomain,cachesbasedonboundaryvalues(Miller
where
etal.,2023)andneuralnetworks(Lietal.,2023)havebeen
L
[v]:=E(cid:2) (v(ζ)−g(ζ))2(cid:3)
. (4)
bnd
proposedtoestimatethePDEsolutionacrossthedomain
andaccelerateconvergence. Ourobjectivecanbescaledto In the above, β ∈ (0,∞) is a penalty parameter, and ξ
high-dimensionalparametricPDEsandguaranteesthatits and ζ are suitable random variables distributed on Ω and
minimizerapproximatesthesolutiononthewholedomain. ∂Ω,respectively. Whileimprovedsamplingmethodshave
WerefertoHermannetal.(2020);Bezneaetal.(2022)for beeninvestigated,see,e.g.,Tangetal.(2023);Chenetal.
relatedneuralnetworkapproximationresults. (2023),thedefaultchoiceistopickuniformdistributions.
TheexpectationsarethenapproximatedwithstandardMC
3.NeuralPDESolverforEllipticPDEs orquasi-MCmethodsbasedonasuitablesetofsamples.
Byminimizingthepoint-wiseresidualofthePDE,PINNs
Westartbydefiningourproblemanddescribingprevious
havegainedpopularityasauniversalandsimplemethod.
deeplearningmethodsforitssolution. Ourgoalistoap-
proximate the solution2 u ∈ C(Ω) to elliptic PDEs with However,PINNsaresensitivetohyperparameterchoices,
suchasβ,andsufferfromtraininginstabilitiesorhighvari-
Dirichletboundaryconditionsoftheform
ance(Wangetal.,2021;Krishnapriyanetal.,2021;Nu¨sken
(cid:40)
P[u]=f, on Ω, &Richter,2021b). Moreover,theobjectivein(3)requires
(1) theevaluationofthederivativesappearinginP[v]. While
u=g, on ∂Ω,
thiscanbedoneexactlyusingautomaticdifferentiation,it
withdifferentialoperator leadstohighcomputationalcosts,seeFigure3.
DeepRitzmethod: ForthePoissonequationin(2),one
P[u]:= 1Tr(σσ⊤Hess )+µ·∇u.
2 u canavoidthiscostbyleveragingweakvariationalformula-
Intheabove,Ω⊂Rdisanopen,bounded,connected,and tions,see,e.g.,Evans(2010). Ratherthandirectlyoptimiz-
ingtheregressionlossin(3),theDeepRitzmethod(Eetal.,
sufficientlyregulardomain,see,e.g.,Baldi(2017);Karatzas
2017)proposestominimizetheobjective
&Shreve(2014);Schilling&Partzsch(2014)forsuitable
regularity assumptions. Note that the formulati √on in (1) (cid:20) ∥∇v(ξ)∥2 (cid:21)
includesthePoissonequationforµ=0andσ = 2I,i.e., L [v]:=E −f(ξ)v(ξ) +βL [v]. (5)
Ritz 2 bnd
(cid:40)
∆u=f, on Ω,
(2) Under suitable assumptions, the minimizer again corre-
u=g, on ∂Ω. spondstothesolutiontothePDEin(2). Theobjectiveonly
requirescomputingthegradient∇vinsteadoftheLaplacian
Inthefollowing,wewillsummarizeexistingneuralPDE
∆v. Usingbackwardmodeautomaticdifferentiation,this
solversforthesePDEs,seeTable1foranoverview. Ona
reducesthenumberofbackwardpassesfromd+1toone,
2Forsimplicity,weassumethatasufficientlysmooth,strong seealsothereducedcostinFigure3. Moreover,wenote
solutionexists. that the loss in (5) allows for weak solutions that are not
3SolvingPoissonEquationsusingNeuralWalk-on-Spheres
twicedifferentiable. WerefertoChenetal.(2020),foran whichgoesbacktoKakutani’sTheorem(Kakutani,1944)
extensivecomparisonoftheDeepRitzmethodtoPINNs andisaspecialcaseoftheFeynman-Kacformula.
forellipticPDEswithdifferentboundaryconditions.
Whiletherepresentationin(9)leadstoMCmethodsforthe
Finally,wementionthatbothmethodssufferfromthefact pointwiseapproximationofuatagivenpointx∈Ω,italso
thattheinteriorlossesonlyconsiderlocal,pointwiseinfor- allowsustoderivevariationalformulationforlearninguon
mation at samples x ∈ Ω. At the beginning of training, thewholedomainΩ. Basedontheaboveresults,wecan
theinteriorlossmightthusnotbemeaningful. Specifically, derivethefollowingthreelosses.
theboundaryconditiong firstneedstobelearnedviathe
Feynman-Kacloss: TheFeynman-Kaclossisgivenby
boundarylossL ,andthenpropagatefromtheboundary
bnd
∂Ωtointeriorpointsxviathelocalinteriorloss.Thereexist
L
[v]:=E(cid:104)(cid:0) v(ξ)−g(Xξ)+Fξ(cid:1)2(cid:105)
(10)
someheuristicstomitigatethisissueby,e.g.,progressively FK τ τ
learning the solution, see Penwarden et al. (2023) for an
and follows from the fact that the solution to a quadratic
overview. Thenextsectiondescribesmoreprincipledways
regression problem as in (10) is given by the conditional
ofincludingboundaryinformationinthelossanddirectly
expectation in (9). Notably, this variational formulation
informingtheinteriorpointsoftheboundarycondition.
does neither require a derivative of the function v nor an
extraboundarylossL .
3.2.StochasticformulationsofellipticPDEs bnd
BSDEloss: Sincetheformulain(8)holdsifandonlyif
From weak solutions, we will now proceed to stochastic
usolvesthePDEin(1),wecanderivetheBSDEloss
representationsofellipticPDEsin(1). Tothisend,consider
the3solutionXξ tothestochasticdifferentialequation
L
[v]:=E(cid:104)(cid:0) v(ξ)−g(Xξ)+Fξ+Sv(cid:1)2(cid:105)
. (11)
BSDE τ τ τ
dXξ =µ(Xξ)dt+σ(Xξ)dW , Xξ =ξ, (6)
t t t t 0
Compared to the Feynman-Kac loss in (10), the BSDE
where W is a standard d-dimensional Brownian motion. loss requires computing the gradient of v at every time-
Moreover, we define the stopping time τ as the first exit discretizationoftheSDEXξ inordertocomputeSv. How-
τ
timeofthestochasticprocessXξ fromthedomainΩ,i.e., ever, due to (8), Sv acts as a control variates and causes
τ
the variance of the MC estimator of (11) to vanish at the
τ =τ(Ω,ξ):=inf{t∈[0,∞): Xξ ∈/ Ω}. (7)
t optimum,seeRichter&Berner(2022)fordetails.
AnapplicationofItoˆ’slemmatotheprocessu(Xξ )shows Fortheprevioustwolosses,boundaryinformationisdirectly
t∧τ
thatwealmostsurelyhavethat propagatedalongthetrajectoryoftheSDEXξtotheinterior.
(cid:90) τ However,simulatingabatchofrealizationsoftheSDEuntil
u(Xξ)=u(Xξ)+ P[u](Xξ)dt+Su, theyreachtheboundary∂Ω,i.e.,untilthestoppingtimeτ,
τ 0 t τ
0 canincurprohibitivelyhighcosts.
whereSuisthestochasticintegral
τ Diffusion loss: The diffusion loss (Nu¨sken & Richter,
(cid:90) τ 2021a)circumventslongsimulationtimesbystoppingthe
S τu := (σ⊤∇u)(X tξ)·dW t. SDEats=τ∧T,i.e.,attheminimumofaprescribedtime
0 T ∈(0,∞)andthestoppingtimeτ. Sincethetrajectories
UsingthefactthatXξ =ξandassumingthatusolvesthe mightnotreachtheboundary,thelossissupplementedwith
0
ellipticPDEin(1),wearriveattheformula aboundaryloss. Thisyieldsthevariationalformulation
g(Xξ)=u(ξ)+Fξ+Su, (8) L [v]:=E(cid:2)(cid:0) v(ξ)−v(Xξ)+Fξ+Sv(cid:1)2(cid:3) +βL [v].
τ τ τ Diff s s s bnd
whereweusedtheabbreviation
Notethatthiscanbeviewedasaninterpolationbetweenthe
(cid:90) τ BSDEloss(fors→∞)andthePINNloss(fors→0and
F τξ := f(X tξ)dt. rescalingbys−2). Inthesameway,italsobalancesthead-
0
vantagesanddisadvantagesofbothlosses,seealsoTable1.
SincethestochasticintegralSτ haszeroexpectation,see,
u
e.g.,Baldi(2017,Theorem10.2),wecanrewrite(8)asa
4.NeuralWalk-on-Spheres(NWoS)Method
stochasticrepresentation,i.e.,
u(x)=E(cid:2) g(Xξ)−Fξ(cid:12)
(cid:12)ξ
=x(cid:3)
, (9)
Inthissection,wewillpresentamoreefficientwayofsimu-
τ τ latingtheSDEtrajectoriesforthecaseofPoisson-typePDEs
3Weassumethatthereexistsauniquesolution,see,e.g.,LeGall asin(2). OurlossisbasedontheFKlossin(10),which
(2016)forcorrespondingconditions. doesnotrequirethecomputationofanyspatialderivatives
4SolvingPoissonEquationsusingNeuralWalk-on-Spheres
oftheneuralnetworkv. However,wereducethenumberof and stop at step κ when reaching an ε-shell, i.e., when
stepsforsimulatingtheprocessXξ whilestillreachingthe r < ε for a prescribed ε ∈ (0,∞). This allows us to
κ
boundary(differentfromthediffusionloss). “walk”fromspheretosphereuntil(approximately)reaching
theboundary,suchthatwecanestimatethefirsttermin(13).
4.1.RecursionofellipticPDEsonsub-domains Specifically,thevalueu(Xξ)in(13)isapproximatedbythe
τ
boundaryvalueg(ξ¯ ),where
κ
First,weoutlinehowtocastthesolutionofthePDEin(1)
into nested subproblems of solving elliptic PDEs on sub- ξ¯ :=argmin∥x−ξ ∥
κ κ
domains. Specifically,letΩ 0 ⊂Ωbeanopensub-domain x∈∂Ω
containing4 ξ := ξ and let τ := τ(Ω ,ξ) be the corre-
0 0 0 istheprojectiontotheboundary.
sponding stopping time, defined as in (7). Analogously
to(9),weobtainthat We note that the bias from introducing the stopping tol-
erance ε can be estimated as O(ε) (Mascagni & Hwang,
u(ξ)=E(cid:2) u(X τξ 0)−F τξ 0(cid:12) (cid:12)ξ(cid:3) . (12) 2003). Moreover,forwell-behaved,e.g.,convex,domains
Ω,theaveragenumberofstepsκbehaveslikeO(log(ε−1))
Notethatthisisarecursivedefinitionsincethesolutionuto
(Motoo, 1959; Binder & Braverman, 2012). This shows
thePDEin(1)appearsagainintheexpectation. Toresolve that ε can be chosen sufficiently small without incurring
the recurrence, we define the random variable ξ ∼ Xξ
1 τ0 toomuchadditionalcomputationalcost. Wenotethatthis
andchooseanotheropensub-domainΩ ⊂ Ωcontaining
1 leadstomuchfasterconvergencethantime-discretizations
ξ . Consideringthestoppingtimeτ :=τ(Ω ,ξ ),wecan
1 1 1 1 oftheBrownianmotion. Inordertohaveacomparablebias,
calculatethevalueofuappearingintheinnerexpectation wewouldneedtotakestepsofsizeO(ε),requiringΩ(ε−2)
u(X τξ 0)∼u(ξ 1)=E(cid:2) u(X τξ 11)−F τξ 11(cid:12) (cid:12)ξ 1(cid:3) stepstoconverge.
4.3.Sourceterm
Wecannowiteratethisprocessfork ∈Nandcombinethe
resultwith(12)toobtain Tocomputethesecondtermin(13),weneedtoaccumulate
valuesoftheform
(cid:34) (cid:12) (cid:35)
(cid:88) (cid:12)
u(ξ)=E g(X τξ)− F τξ kk(cid:12)
(cid:12)
(cid:12)ξ . (13) v(z):=E(cid:104)
−F τz
(B,z)(cid:105)
(14)
k≥0
In the above, we used the strong Markov property of the withagivenballB =B r(z). By(9),weobservethatvis
SDE solution and the tower property of the conditional thesolutionofaPoissonequationontheballB withzero
expectation, see also Hermann et al. (2020). This nested Dirichletboundaryconditionevaluatedatz ∈ Ω. Wecan
stochasticrepresentationcanbecomparedtotheonein(9). thususeclassicalresultsbyBoggio(1905),seealsoGaz-
Thenextsectionshowshowthisprovidesapracticalalgo- zolaetal.(2010),towritethesolutionintermsofGreen’s
rithmthatterminatesinfinitelymanysteps. functions. Specifically,wehavethat
v(z)=−|B (z)|E[f(γ)G (γ,z)], (15)
4.2.Walk-on-Spheres r r
Wetackletheproblemo √fsolvingthePoissonequationin(2), whereγ ∼U(B r(z))and
i.e., µ = 0 and σ = 2I. Then, the SDE in (6) is just
a scaled Brownian motion starting at ξ. Picking Ω := (cid:40) 1 log r , d=2,
k G (y,z):= 2π ∥y−z∥
B rk(ξ k)tobeaballofradiusr
k
∈(0,∞)aroundξ
k
inthe r Γ(d/2−1)(cid:0) ∥y−z∥2−d−r2−d(cid:1)
, d>2,
k-thstep,theisotropyofBrownianmotionensuresthat 4πd/2
seeAppendixAforfurtherdetails. Inpractice,wecannow
ξ k+1 ∼X τξ kk ∼U(∂B rk(ξ k)). approximatetheexpectationin(15)usinganMCestimate.
In other words, we can just sample ξ uniformly from
k+1
4.4.LearningProblem
a sphere of radius r around the previous value ξ . To
k k
terminate after finitely many steps, we pick the maximal Basedonthepreviousderivations,wecanestablishavari-
radiusineachstep,i.e., ationalformulation,wheretheminimizerisguaranteedto
approximatethePoissonequationin(2)onthewholedo-
r k :=dist(ξ k,∂Ω), mainΩ. Specifically,wedefine
and4 tS hi enc ste atξ emis ea ntra in sd toom bev ua nri da eb rl se to, oth de fs ou rb e- ad co hm rea ain lizΩ a0 tiois n.random,
L
NWoS[v]:=E(cid:104)(cid:0) v(ξ)−WoS(ξ)(cid:1)2(cid:105)
, (16)
5SolvingPoissonEquationsusingNeuralWalk-on-Spheres
&+,
3 minMSE[!% − ; < = >% ? %%,>% ,- %% ]
' - !" ( - !" ( ( ' *
()*
!!
$
! #! "" ! $" 2 !% =#$ % &% , ( % &%,)Ω <,
" - (%%), else
"! 1 ' &
"
!!
" !"
$ %"! " !" "
" !! ! !! ! !" $ %""
#
1
$ %!! $ %!"
!
"!
Figure4.NeuralWalk-on-Spheres(NWoS):OuralgorithmforlearningthesolutiontoPoissonequations∆u = f onΩ ⊂ Rdand
u| =g. 1 Ineachgradientdescentstep,wesampleabatchofrandompoints(ξi)m inthedomainΩandsimulateBrownianmotions
∂Ω 0 i=1
byiterativelysamplingξi fromspheresB inscribedinthedomain. Toaccountforthesourcetermf,wesampleγi ∼U(B )to
k ri k ri
k k
computeanMCapproximation|B |f(γi)G (ξi,γi)tothesolutionofthePoissonequationonthesphereB usingtheGreen’s
ri k ri k k ri
k k k
functionG inSection4.3. 2 WestopafterafixednumberofmaximumstepsK andeitherevaluateourneuralnetworkv orthe
ri θ
k
boundaryconditiongifwereachanε-shellof∂Ω. 3 Ifv satisfiesthePDE,themean-valuepropertyimpliesthatv (ξi)isapproximated
θ θ 0
bytheexpectedvalueofyiminustheaccumulatedsourcetermcontributions.Wethusminimizethecorrespondingmeansquarederror
overtheparametersθusinggradientdescent.
wherethesingle-trajectoryWoSmethodWoS(ξ)withran- thecurseofdimensionalitywhenminimizingtheempirical
dominitialpointξisgivenby risk,i.e.,anMCapproximationof(16),overasuitableset
ofneuralnetworksv . Specifically,thenumberofrequired
κ−1 θ
WoS(ξ):=g(ξ¯ )−(cid:88) |B (ξ )|f(γ )G (γ ,ξ ). samplesofξtoguaranteethattheempiricalminimizerap-
κ rk k k rk k k
proximatesthesolutionuuptoagivenaccuracyalsoscales
k=0
onlypolynomiallywithdimensionandaccuracy.
Intheabove,γ ∼U(B (ξ )),andtherandomvariables
κ, ξ , ξ¯ , and
rk
are
defirk nedk
as in Section 4.2. From the
k κ k
4.5.Implementation
stochasticformulationofthesolutionin(13)andProposi-
tion3.5inHermannetal.(2020),itfollowsthatthemini- In this section, we discuss implementations for the loss
mizerof(16),i.e.,x(cid:55)→E(cid:2) WoS(ξ)(cid:12)
(cid:12)ξ
=x(cid:3)
,approximates L in(16)describedintheprevioussection. Wesum-
NWoS
thesolutionuin(2)intheuniformnormuptoerrorO(ε),
marizeouralgorithminFigure4andprovidepseudocode
whereεisthestoppingtolerance,seeSection4.2. Wealso for the vanilla version in Algorithm 1. In the following,
remarkthat,intheory,thelossrequiresonlyasingleWoS we present strategies to trade-off accuracy and computa-
trajectorypersampleofξsincetheminimizeroftheregres-
tionalcostandtoreducethevarianceofMCestimators. We
sionproblemin(16)averagesoutthenoise. providepseudocodeforNWoSwiththeseimprovementsin
Algorithm2andAlgorithm3intheappendix.
Having established a learning problem, we can analyze
both approximation and generalization errors. For the WoSwithmaximumnumberofsteps: Forsufficiently
former, Hermann et al. (2020) and Beznea et al. (2022) regulargeometries,theprobabilityofawalktakingmore
boundedthesizeofneuralnetworksv θ toapproximatethe thankstepsisexponentiallydecayingink(Binder&Braver-
solutionuuptoagivenaccuracy. Inparticular,thenumber man,2012). However,ifasinglewalkinourbatchneeds
of required parameters θ only scales polynomially in the significantly more steps, it slows down the overall train-
dimensiondandthereciprocalaccuracy,aslongasthefunc-
ing. Wethusintroduceadeterministicmaximumnumberof
tionsf,g,anddist(·,∂Ω)canbeefficientlyapproximated stepsK ∈N;seeBezneaetal.(2022)foracorresponding
byneuralnetworks. erroranalysis. However,wedonotwanttointroducenon-
negligiblebiasby,e.g.,justprojectingtotheclosestpoint
OnecanthenleverageresultsbyBerneretal.(2020b)to
ontheboundary.
show that also the generalization error does not underlie
6SolvingPoissonEquationsusingNeuralWalk-on-Spheres
Table2.RelativeL2-error(andstandarddeviationsover5independentruns)ofourconsideredmethods,estimatedusingMCintegration
on106uniformlydistributed(unseen)pointsinΩ.
Method Problem
Laplace(10d) Committor(10d) PoissonRect. (10d) Poisson(50d)
PINN 7.42e−4±1.84e−4 4.10−3±1.11e−3 1.35e−2±1.57e−3 7.70e−3±2.25e−3
DeepRitz 8.43e−4±6.29e−5 6.15e−3±5.30e−4 1.06e−2±6.20e−4 1.05e−3±1.70e−4
Diffusionloss 1.57e−4±7.74e−6 4.48e−2±6.93e−3 9.69e−2±1.03e−2 5.96e−4±1.06e−5
NeuralCache 3.99−4±4.08e−5 1.26e−3±5.82e−5 4.98e−2±1.80e−2 1.63e−2±1.42e−2
WoS 1.08e−3±1.34e−6 1.99e−3±9.79e−6 2.32e−1±2.09e−1 4.50e−3±7.38e−4
NWoS(ours) 4.29e−5±2.02e−6 6.56e−4±2.42e−5 2.60e−3±9.99e−5 4.82e−4±1.32e−5
Instead, we want to enforce the mean-value property on whereξiarei.i.d.samplesofξandWoSn(ξi)arei.i.d. sam-
subdomainsofΩbasedonourrecursioninSection4.1. We plesofWoS(ξi), i.e., N trajectorieswiththesameinitial
thus propose to use the model v instead of the boundary point ξi, see (16). Note that we vectorize the WoS sim-
conditiongifthewalkdoesnotconvergeafterK steps,i.e., ulationsacrossboththeinitialpointsandthetrajectories,
wedefine5 makingourNWoSmethodhighlyparallelizableandscal-
(cid:40) abletolargebatchsizes.
v(ξ ), d(ξ ,∂Ω)>ε,
yξ,v := K K
g(ξ¯ ), else. Wefurtherintroducecontrolvariatestoreducethevariance
K
ofestimatingWoS(x), wherewefocusonafixedx ∈ Ω
Wecanthenreplacethesecondtermin(16)by foreaseofpresentation. Controlvariatesseektoreducethe
variancebyusinganestimatoroftheform
K−1
(cid:88)
WoS(ξ,v):=yξ,v− |B rk(ξ k)|f(γ k)G rk(γ k,ξ k).
1
(cid:88)N
k=0
E[WoS(x)]≈E[δ]+ WoSn(x)−δn,
N
n=1
This helps to reduce the bias when d(ξ K,∂Ω) is non- where δn are i.i.d. samples of a random variable δ with
negligible and leads to faster convergence assuming that
knownexpectation.
weobtainincreasinglygoodapproximationsv ≈uduring
θ
trainingofaneuralnetworkv . Ourapproachbearssimilar- MotivatedbySawhney&Crane(2020),weuseanapproxi-
θ
itytothediffusionloss,seeSection3;however,wedonot mationofthefirst-ordertermofaTaylorseriesofuinthe
needtouseatime-discretizationoftheSDE. directionofthefirstWoSstep. Weassumethat∇v θ pro-
videsanincreasinglyaccurateapproximationofthegradient
BoundaryLoss: Wefindempiricallythatanadditional
∇uduringtrainingandproposetouse
boundarylosscanimprovetheperformanceofourmethod.
Whiletheoreticallynotrequired,itcanespeciallyhelpfor δn :=∇v θ(x)·(ξ 1n−x),
asmallernumberK ofmaximumsteps(seetheprevious whereξn isthefirststepofWoSn(x). Inparticular,ξn ∼
1 1
paragraph). In general, we thus sample a fraction of the U(∂B (x))andthusE[δ]=0holdsforanyfunctionv .
pointsontheboundary∂Ωandoptimize
r1 θ
Whileweneedtocomputethegradient∇v (x)forthecon-
θ
L NWoS[v]+βL bnd[v], trolvariate,wementionthatthisoperationcanbedetached
fromthecomputationalgraph. Inparticular,wedonotneed
whereL isdefined6asin(4).
bnd
tocomputethederivativeof∇v (x)w.r.t.theparameters
θ
Variance-reduction: While not necessarily needed for θ as is necessary for PINNs, the Deep Ritz method, the
the objective in (16), we can still average multiple WoS diffusionloss,andtheBSDEloss. InAppendixC,weem-
trajectoriesN ∈Npersampleofξtoreducethevariance. piricallyshowthattheoverheadofusingthecontrolvariates
Thisleadstotheestimator isinsignificant.
(cid:32) m N (cid:33)
1 (cid:88) 1 (cid:88) Buffer: MotivatedbyLietal.(2023),wecanuseabuffer
L(cid:98)NWoS[v]:=
m
v(ξi)−
N
WoSn(ξi) ,
tocachetrainingpoints
i=1 n=1
N
5Since we stop the walk when reaching an ε-shell, the first (cid:16) ξ(i), 1 (cid:88) WoSn(ξ(i))(cid:17)B . (17)
conditioncanalsobewrittenasK <κ. N i=1
n=1
6NotethatL canbeinterpretedasaspecialcaseofL
bnd NWoS
wheretheWoSmethoddirectlyterminatessincetheinitialpoints Since we only update the buffer after a given number of
aresampledontheboundary. training steps L ∈ N, this accelerates the training. Note
7SolvingPoissonEquationsusingNeuralWalk-on-Spheres
Algorithm1TrainingofvanillaNWoSmethod trainingandranagridsearchoveraseriesofhyperparame-
Input: neural network v with initial parameters θ, opti- terconfigurationsforeachmethod. Then,weperformed5
θ
mizermethodstepforupdatingtheparameters,number independentrunsforthebestconfigurationsw.r.t.therela-
ofiterationsT, batchsizem, sourcetermf, boundary tiveL2-error. Moredetailsonthehyperparametersandour
termg,stoppingtoleranceε implementations7canbefoundinAppendixB.
Output: optimizedparametersθ
LaplaceEquation: ThefirstPDEisaLaplaceequation
fork ←0,...,T do
onasquaredomaingivenby
x ←samplefromξ⊗m ▷SamplepointsinΩ
Ω
x←x f(x)=0, g(x)=(cid:80)d/2x x , x∈Ω=(0,1)d.
Ω i=0 2i 2i+1
r ←dist(x,∂Ω) ▷Computedistancesto∂Ω
Totestourmodels,wecompareagainsttheanalyticsolution
whiler >εdo
asu(x)=(cid:80)d/2x
x . FollowingJinetal.(2017),we
γ ←samplefromU(B (x)) ▷Estimatesource i=0 2k 2k+1
r
considerthecased=10.
s←s−|B (x)|f(γ)G (x,γ)
r r
u←samplefromU(∂B r(x)) PoissonEquation: Next,weconsiderthePoissonequa-
x←x+u ▷Walktonextpoints tionpresentedinJinetal.(2017),i.e.,
r ←dist(x,∂Ω) ▷Computedistancesto∂Ω
f(x)=2d, g(x)=(cid:80)d x2, x∈Ω=(0,1)d,
endwhile i=1 i
x←projectxto∂Ω ▷Findclosestpointsin∂Ω
y
Ω
←s+g(x) ▷Estimateboundary withanalyticsolutionu(x)=(cid:80)d i=1x2 i. Wechoosed=50
andpresentresultswith8d∈{100,500}inAppendixD.
L(cid:98)NWoS ←MSE(v θ(x Ω),y Ω) ▷NWoSloss
(cid:0) (cid:1)
θ ←step γ,∇ θL(cid:98)NWoS ▷SGDstep PoissonEquationwithRectangularAnnulus: Wealso
endfor consideraPoissonequationonarectangularannulusΩ=
(−1,1)d\[−c,c]dwithsinusoidalboundaryconditionand
sourceterm
that this is not possible for the other methods since they
requireevaluationofthecurrentmodeloritsgradients. In
1(cid:88)d 4π2 (cid:88)d
g(x)= sin(2πx ), f(x)=− sin(2πx ).
everybufferupdate,weaverageoveradditionaltrajectories, d i d i
i=1 i=1
i.e.,increaseN in(17),forafractionofpointstoimprove
theiraccuracy. However,differentfromLietal.(2023),we Wechoosec=0.25d1 andd=10,andnotethattheanalytic
alsoevictafractionofpointsfromthebufferandreplace solutionisgivenbyu(x)=
d1(cid:80)d
i=1sin(2πx i).
themwithWoSestimatesonnewlysampledpointsξ(i) in
Committor Function: The fourth equation deals with
thedomainΩtobalancethediversityandaccuracyofthe
committorfunctionsfrommoleculardynamics. Thesefunc-
trainingdatainthebuffer.
tionsspecifylikelytransitionpathwaysandtransitionrates
Remark4.1. TheNeuralCachemethodbyLietal.(2023)
between(potentiallymetastable)regionsorconformations
usesarelatedapproachtoaccelerateWoSmethodsforap- ofinterest(Vanden-Eijndenetal.,2006;Lu&Nolen,2015).
plications in computer graphics. However, their method Theyaretypicallyhigh-dimensionalandknowntobechal-
neverreplacesanypointξ(i)inthebuffer,i.e.,onlyupdates
lengingtocompute. TocompareNWoS,weconsiderthe
estimatesinthebuffer. Weobservedthatthemodelisthus settinginNu¨sken&Richter(2021a). Thetaskistoestimate
pronetooverfittingonthepointsinthebuffer,especiallyin theprobabilityofaparticlehittingtheoutersurfaceofan
highdimensions,preventingitfromachievinghighaccura- annulusΩ={x∈Rd :a<∥x∥<b}witha,b∈(0,∞),
ciesacrossthedomainΩ.
beforetheinnersurface.
TheproblemcanthenbeformulatedassolvingtheLaplace
5.Experiments
equationgivenby
In this section, we compare the performance of NWoS, f(x)=0, g(x)=1 , x∈Ω.
{∥x∥=b}
PINN,DeepRitz,Diffusionloss,andNeuralCacheonvar-
ForthisspecificΩ,areferencesolutioncanbecomputedas
ious problems across dimensions from 10d to 50d. We
donotconsidertheFKandBSDElossessincetheyincur a2−∥x∥2−da2
u(x)= .
prohibitivelylongruntimesforsimulatingtheSDEswith a2−b2−da2
sufficientprecision. Tocompareagainstthebaselines,we
7Our PyTorch code can be found at https://github.
considerbenchmarksfromtheworksproposingtheDeep
com/bizoffermark/neural_wos.
Ritzanddiffusionlosses(Jinetal.,2017;Nu¨sken&Richter,
8Whiled = 100isconsideredbyJinetal.(2017), wefind
2021a). For a fair comparison, we set a fixed runtime of thatasimpleprojectionoutperformsallmodelsinsufficientlyhigh
25d+750secondsandGPUmemorybudgetof2GiBfor dimensionsforthisbenchmark,seeAppendixD.
8SolvingPoissonEquationsusingNeuralWalk-on-Spheres
Initialization Prediction Ground Truth The PDE-constrained optimization problem shows that
1.0 0.8 NWoS can be extended to parametric problems, where a
0.8
0.6 wholefamilyofPoissonequationsissolvedsimultaneously.
0.6
0.4 Weobservethatforthis5-dimensionalproblem(twospatial
0.4 dimensionsandthree-parameterdimensions),NWoScon-
0.2
0.2 verges within 20 minutes to a relative L2-error of 0.79%
0.0 0.0 (averagedoverD×Ω).Thetrainednetworkcanthenbeused
x1 x1 x1 tosolvetheoptimizationproblemdirectly(whereweuse
L-BFGS)withoutrequiringaninnerloopforthePDEsolver.
Figure5.Qualitative assessment of the solution to the PDE-
constrainedoptimizationproblem. (Left)Initialfunctionu for
TheresultsshowapromisingrelativeL2-errorof1.30%for
c
randomparametersc∈D.(Middle)Predictedfunctionu forthe estimatingtheparametersc∗leadingtoanaccuratepredic-
cˆ
parameterscˆobtainedafterafewgradientdescentstepsusingthe tionoftheminimizer,seeFigure5andAppendixCforan
approximationofthesolutiontotheparametricPoissonequation ablationstudy.
obtainedwithNWoS.(Right)Thegroundtruthsolutionu c∗.
6.Conclusion
WefurtherusethesettingbyNu¨sken&Richter(2021a)and
choosea=1,b=2,andd=10. WehavedevelopedNeuralWalk-on-Spheres,anovelway
ofsolvinghigh-dimensionalPoissonequationsusingneural
PDE-Constrained Optimization: Finally, we want to
networks. Specifically,weprovideavariationalformulation
solvetheoptimizationproblem
with theoretical guarantees that amortizes the cost of the
1(cid:90) α(cid:90) standardWalk-on-Spheresalgorithmtolearnsolutionson
u∈H
01(Ωm ),i mn
∈L2(Ω) 2
Ω(u−u d)2dx+
2
Ωm2dx t eh ffie cfu iel nl tun thd ae nrly ci on mg pd eo tm ina gin m. eT th he odre ssu (Plt Ii Nng Ne ss ,ti tm heat Dor eeis pm Ro ir tze
constraint to u being a solution to the Poisson equation method,andthediffusionloss)whileachievingbetterperfor-
with g(x) = 0 and f(x) = −m(x) for x ∈ Ω = (0,1)2. manceatlowercomputationalcostsandfasterconvergence.
The goal of the optimization problem is to balance the WeshowthatNWoSalsoperformsbetteronaseriesofchal-
energy of the input control m with the proximity of the lenging,high-dimensionalproblemsandparametricPDEs.
state u and the target state u while satisfying the PDE Thisalsohighlightsitspotentialforapplicationswheresuch
d
constraint. Following Hwang et al. (2022), we choose problemsareprominent,e.g.,inmoleculardynamicsand
u = 1 sin(πx )sin(πx )astargetstate. PDE-constraintoptimization.
d 2π 1 2
To tackle this problem and showcase the capabilities of Extensionsandlimitations: NWoSiscurrentlyonlyap-
NWoS,wefirstsolveaparametricPoissonequation,where plicabletoPoissonequationswithDirichletboundarycon-
weparametrizethecontrolasm =c sin(c x )sin(c x ) ditions. WhilethisPDEappearsfrequentlyinapplications,
c 1 2 1 3 2
with c ∈ D := [0.5,1.0]×[2.5,3.5]2. Similar to Berner we also believe that future work can extend our method.
etal.(2020a),wecansamplerandomc ∈ Dineverygra- Forinstance,onecantrytoleverageadaptationsofWoSto
dientdescentsteptouseNWoSforsolvingawholefamily spatiallyvaryingcoefficients(Sawhneyetal.,2022),drift-
ofPoissonequations. Freezingthetrainedneuralnetwork diffusionproblems(Sabelfeld,2017),Neumannboundary
parameters afterward, we can reduce the PDE-constraint conditions (Sawhney et al., 2023; Simonov, 2007), frac-
optimization problem to a problem over c ∈ D. In this tional Laplacians (Kyprianou et al., 2018), the screened
illustrativeexample,wecancomputetheground-truthpa- PoissonorHelmholtzequation(Sawhney&Crane,2020;
rametersasc∗ =(cid:0) 1 ,π,π(cid:1) andchooseα=10−3. Cheshkova,1993),aswellaslinearizedPoisson-Bolzmann
1+4απ4
equations(Hwang&Mascagni,2001;Bossyetal.,2010).
Moreover, one can also take other elementary shapes in
5.1.Results
eachstep,e.g.,rectanglesorstars(Deaconu&Lejay,2006;
We present our results in Table 2. We first note that we Sawhney et al., 2023), and omit the need for ε-shells for
improve the Deep Ritz method and the diffusion loss by certaingeometriesusingtheGreen’sfunctionfirst-passage
almost an order of magnitude compared to the results re- algorithm(Givenetal.,1997).
portedbyJinetal.(2017);Nu¨sken&Richter(2021a). Still,
Finally,whileNWoScantackleparametricPDEs,weneed
ourNWoSapproachcanoutperformallothermethodson
tohaveafixedparametrizationofthesourceorboundary
ourconsideredbenchmarks. Inadditiontotheseresults,we
functions. It would be promising to extend the ideas to
highlight that the efficient objective of NWoS also leads
neuraloperators,whichcurrentlyonlyuselossesbasedon
to faster convergence, see Figure 1. We provide ablation
PINNs(Goswamietal.,2022;Lietal.,2021)ordiffusion
studiesinAppendixCandadditionalnumericalevidence
lossesforparabolicPDEs(Zhangetal.,2023a).
inAppendixD.
9
2x
0.0 2.0 4.0 6.0 8.0 0.1 0.0 2.0 4.0 6.0 8.0 0.1 0.0 2.0 4.0 6.0 8.0 0.1SolvingPoissonEquationsusingNeuralWalk-on-Spheres
Acknowledgements worksapproximationsofboundaryvalueproblems.arXiv
preprintarXiv:2209.01432,2022.
TheauthorsthankRohanSawhneyforhelpfuldiscussions.
J.BerneracknowledgessupportfromtheWallyBaerand Binder,I.andBraverman,M.Therateofconvergenceofthe
Jeri Weiss Postdoctoral Fellowship. A. Anandkumar is walkonspheresalgorithm. GeometricandFunctional
supportedinpartbyBrenendowedchairandbytheAI2050 Analysis,22(3):558–587,2012.
seniorfellowprogramatSchmidtSciences.
Boggio,T. Sullefunzionidigreend’ordinem. Rendiconti
delCircoloMatematicodiPalermo(1884-1940),20:97–
ImpactStatement
135,1905.
The aim of this work is to advance the field of machine Bossy,M.,Champagnat,N.,Maire,S.,andTalay,D. Proba-
learning and scientific computing. While there are many bilisticinterpretationandrandomwalkonspheresalgo-
potentialsocietalconsequencesofourwork,noneofthem rithmsforthePoisson-Boltzmannequationinmolecular
areimmediatetorequirebeingspecificallyhighlightedhere. dynamics. ESAIM:MathematicalModellingandNumer-
icalAnalysis,44(5):997–1048,2010.
References
Chen,J.,Du,R.,andWu,K. Acomparisonstudyofdeep
Azzizadenesheli,K.,Kovachki,N.,Li,Z.,Liu-Schiaffini, GalerkinmethodanddeepRitzmethodforellipticprob-
M.,Kossaifi,J.,andAnandkumar,A. Neuraloperators lemswithdifferentboundaryconditions. arXivpreprint
foracceleratingscientificsimulationsanddesign. arXiv arXiv:2005.04554,2020.
preprintarXiv:2309.15325,2023.
Chen,X.,Cen,J.,andZou,Q. Adaptivetrajectoriessam-
Bahrami,M.,Großardt,A.,Donadi,S.,andBassi,A. The plingforsolvingpdeswithdeeplearningmethods. arXiv
Schro¨dinger–Newtonequationanditsfoundations. New preprintarXiv:2303.15704,2023.
JournalofPhysics,16(11):115007,2014.
Cheshkova,A. “walkonspheres”algorithmsforsolving
Baldi,P.StochasticCalculus:AnIntroductionThroughThe- helmholtzequation. BulletinoftheNovosibirskComput-
oryandExercises. Universitext.SpringerInternational ingCenter: Numericalanalysis,(4):7,1993.
Publishing,2017.
Cuomo,S.,DiCola,V.S.,Giampaolo,F.,Rozza,G.,Raissi,
M.,andPiccialli,F. Scientificmachinelearningthrough
Beck,C.,Becker,S.,Grohs,P.,Jaafari,N.,andJentzen,A.
physics–informed neural networks: Where we are and
SolvingstochasticdifferentialequationsandKolmogorov
what’snext. JournalofScientificComputing,92(3):88,
equations by means of deep learning. arXiv preprint
2022.
arXiv:1806.00421,2018.
De Ryck, T. and Mishra, S. Error analysis for physics-
Beck,C.,E,W.,andJentzen,A. Machinelearningapproxi-
informed neural networks (PINNs) approximating kol-
mationalgorithmsforhigh-dimensionalfullynonlinear
mogorovPDEs. AdvancesinComputationalMathemat-
partialdifferentialequationsandsecond-orderbackward
ics,48(6):1–40,2022.
stochasticdifferentialequations. JournalofNonlinear
Science,29(4):1563–1619,2019.
Deaconu,M.andLejay,A. Arandomwalkonrectangles
algorithm. MethodologyandComputinginAppliedProb-
Berner,J.,Dablander,M.,andGrohs,P. Numericallysolv-
ability,8:135–151,2006.
ingparametricfamiliesofhigh-dimensionalKolmogorov
partial differential equations via deep learning. In Ad- Duan, C., Jiao, Y., Lai, Y., Lu, X., andYang, Z. Conver-
vances in Neural Information Processing Systems, pp. gencerateanalysisfordeepritzmethod. arXivpreprint
16615–16627,2020a. arXiv:2103.13330,2021.
Berner, J., Grohs, P., and Jentzen, A. Analysis of the E,W.andYu, B. Thedeepritzmethod: adeeplearning-
generalization error: Empirical risk minimization over basednumericalalgorithmforsolvingvariationalprob-
deep artificial neural networks overcomes the curse of lems. CommunicationsinMathematicsandStatistics,6
dimensionalityinthenumericalapproximationofBlack– (1):1–12,2018.
Scholespartialdifferentialequations. SIAMJournalon
E, W., Han, J., and Jentzen, A. Deep learning-based nu-
MathematicsofDataScience,2(3):631–657,2020b. doi:
mericalmethodsforhigh-dimensionalparabolicpartial
10.1109/IWOBI.2017.7985525.
differentialequationsandbackwardstochasticdifferential
Beznea, L., Cimpean, I., Lupascu-Stamate, O., Popescu, equations. CommunicationsinMathematicsandStatis-
I., and Zarnescu, A. From Monte Carlo to neural net- tics,5(4):349–380,2017.
10SolvingPoissonEquationsusingNeuralWalk-on-Spheres
Evans, L. C. Partial Differential Equations, volume 19. Karatzas,I.andShreve,S. Brownianmotionandstochastic
AmericanMathematicalSoc.,2010. calculus,volume113. springer,2014.
Gazzola,F.,Grunau,H.-C.,andSweers,G. Polyharmonic Krishnapriyan, A., Gholami, A., Zhe, S., Kirby, R., and
boundaryvalueproblems: positivitypreservingandnon-
Mahoney,M.W. Characterizingpossiblefailuremodes
linearhigherorderellipticequationsinboundeddomains. inphysics-informedneuralnetworks.AdvancesinNeural
SpringerScience&BusinessMedia,2010. InformationProcessingSystems,34,2021.
Given, J. A., Hubbard, J. B., and Douglas, J. F. A first-
Kyprianou,A.E.,Osojnik,A.,andShardlow,T. Unbiased
passage algorithm for the hydrodynamic friction and
‘walk-on-spheres’MonteCarlomethodsforthefractional
diffusion-limitedreactionrateofmacromolecules. The
Laplacian. IMA Journal of Numerical Analysis, 38(3):
Journalofchemicalphysics,106(9):3761–3771,1997.
1550–1578,2018.
Goswami, S., Bora, A., Yu, Y., and Karniadakis, G. E.
Physics-informed neural operators. arXiv preprint LeGall,J.-F. Brownianmotion,martingales,andstochastic
arXiv:2207.05748,2022. calculus. Springer,2016.
Han,J.,Jentzen,A.,etal. Deeplearning-basednumerical Li, Z., Zheng, H., Kovachki, N., Jin, D., Chen, H., Liu,
methodsforhigh-dimensionalparabolicpartialdifferen- B.,Azizzadenesheli,K.,andAnandkumar,A. Physics-
tialequationsandbackwardstochasticdifferentialequa- informedneuraloperatorforlearningpartialdifferential
tions. Communicationsinmathematicsandstatistics,5 equations. arXivpreprintarXiv:2111.03794,2021.
(4):349–380,2017.
Li,Z.,Yang,G.,Deng,X.,DeSa,C.,Hariharan,B.,and
Han,J.,Jentzen,A.,andE,W. Solvinghigh-dimensional Marschner, S. Neural caches for Monte Carlo partial
partial differential equations using deep learning. Pro- differentialequationsolvers. InSIGGRAPHAsia2023
ceedingsoftheNationalAcademyofSciences,115(34): ConferencePapers,pp.1–10,2023.
8505–8510,2018.
Lu,J.andNolen,J. Reactivetrajectoriesandthetransition
Han,J.,Nica,M.,andStinchcombe,A.R. Aderivative-free
pathprocess. ProbabilityTheoryandRelatedFields,161
methodforsolvingellipticpartialdifferentialequations
(1-2):195–244,2015.
with deep neural networks. Journal of Computational
Physics,419:109672,2020.
Mascagni,M.andHwang,C.-O. ϵ-shellerroranalysisfor
Hermann,J.,Scha¨tzle,Z.,andNoe´,F. Deep-neural-network “walkonspheres”algorithms. Mathematicsandcomput-
solutionoftheelectronicSchro¨dingerequation. Nature ersinsimulation,63(2):93–104,2003.
Chemistry,12(10):891–897,2020.
Miller, B., Sawhney, R., Crane, K., and Gkioulekas, I.
Hwang, C.-O. and Mascagni, M. Efficient modified Boundary value caching for walk on spheres. arXiv
“walkonspheres”algorithmforthelinearizedPoisson– preprintarXiv:2302.11825,2023.
Bolzmannequation. AppliedPhysicsLetters,78(6):787–
789,2001. Motoo,M. SomeevaluationsforcontinuousMonteCarlo
methodbyusingbrownianhittingprocess. Annalsofthe
Hwang,R.,Lee,J.Y.,Shin,J.Y.,andHwang,H.J. Solving InstituteofStatisticalMathematics,11:49–54,1959.
PDE-constrainedcontrolproblemsusingoperatorlearn-
ing. InProceedingsoftheAAAIConferenceonArtificial Muller, M. E. Some continuous Monte Carlo methods
Intelligence,volume36,pp.4504–4512,2022. forthedirichletproblem. TheAnnalsofMathematical
Statistics,pp.569–589,1956.
Jin, K. H., McCann, M. T., Froustey, E., and Unser, M.
Deepconvolutionalneuralnetworkforinverseproblems
Nu¨sken, N. and Richter, L. Interpolating between
inimaging. IEEETransactionsonImageProcessing,26
BSDEs and PINNs: deep learning for elliptic and
(9):4509–4522,2017.
parabolic boundary value problems. arXiv preprint
Juba,D.,Keyrouz,W.,Mascagni,M.,andBrady,M. Ac-
arXiv:2112.03749,2021a.
celeration and parallelization of zeno/walk-on-spheres.
Nu¨sken, N. and Richter, L. Solving high-dimensional
Procediacomputerscience,80:269–278,2016.
Hamilton–Jacobi–BellmanPDEsusingneuralnetworks:
Kakutani,S. Two-dimensionalBrownianmotionandhar- perspectivesfromthetheoryofcontrolleddiffusionsand
monicfunctions. ProceedingsoftheImperialAcademy, measuresonpathspace. PartialDifferentialEquations
20(10):706–714,1944. andApplications,2(4):1–48,2021b.
11SolvingPoissonEquationsusingNeuralWalk-on-Spheres
Penwarden,M.,Jagtap,A.D.,Zhe,S.,Karniadakis,G.E., Tang, K., Wan, X., and Yang, C. DAS-PINNs: A deep
and Kirby, R. M. A unified scalable framework for adaptivesamplingmethodforsolvinghigh-dimensional
causalsweepingstrategiesforphysics-informedneural partialdifferentialequations. JournalofComputational
networks (PINNs) and their temporal decompositions. Physics,476:111868,2023.
arXivpreprintarXiv:2302.14227,2023.
Vanden-Eijnden, E. et al. Towards a theory of transition
Qi,Y.,Seyb,D.,Bitterli,B.,andJarosz,W. Abidirectional paths. Journal of statistical physics, 123(3):503–523,
formulation for walk on spheres. In Computer Graph- 2006.
icsForum,volume41,pp.51–62.WileyOnlineLibrary,
2022. Wang,S.,Teng,Y.,andPerdikaris,P. Understandingand
mitigatinggradientflowpathologiesinphysics-informed
Raissi,M.,Perdikaris,P.,andKarniadakis,G.E. Physics- neuralnetworks. SIAMJournalonScientificComputing,
informedneuralnetworks:Adeeplearningframeworkfor 43(5):A3055–A3081,2021.
solvingforwardandinverseproblemsinvolvingnonlinear
partialdifferentialequations. JournalofComputational Zhang,R.,Meng,Q.,Zhu,R.,Wang,Y.,Shi,W.,Zhang,S.,
Physics,378:686–707,2019. Ma,Z.-M.,andLiu,T.-Y. MonteCarloneuraloperator
forlearningpdesviaprobabilisticrepresentation. arXiv
Richter,L.andBerner,J. RobustSDE-basedvariationalfor- preprintarXiv:2302.05104,2023a.
mulationsforsolvinglinearPDEsviadeeplearning. In
Proceedingsofthe39thInternationalConferenceonMa- Zhang,X.,Wang,L.,Helwig,J.,Luo,Y.,Fu,C.,Xie,Y.,
chineLearning,volume162ofProceedingsofMachine Liu,M.,Lin,Y.,Xu,Z.,Yan,K.,etal. Artificialintelli-
LearningResearch,pp.18649–18666.PMLR,2022. genceforscienceinquantum,atomistic,andcontinuum
systems. arXivpreprintarXiv:2307.08423,2023b.
Sabelfeld, K. K. Random walk on spheres algorithm for
solvingtransientdrift-diffusion-reactionproblems.Monte
CarloMethodsandApplications,23(3):189–212,2017.
Sawhney, R. and Crane, K. Monte Carlo geometry pro-
cessing: Agrid-freeapproachtoPDE-basedmethodson
volumetricdomains. ACMTransactionsonGraphics,39
(4),2020.
Sawhney,R.,Seyb,D.,Jarosz,W.,andCrane,K. Grid-free
MonteCarloforPDEswithspatiallyvaryingcoefficients.
ACMTransactionsonGraphics(TOG),41(4):1–17,2022.
Sawhney, R., Miller, B., Gkioulekas, I., and Crane, K.
Walk on stars: A grid-free Monte Carlo method for
PDEswithNeumannboundaryconditions.arXivpreprint
arXiv:2302.11815,2023.
Scherbela,M.,Reisenhofer,R.,Gerard,L.,Marquetand,P.,
andGrohs,P.SolvingtheelectronicSchro¨dingerequation
formultiplenucleargeometrieswithweight-sharingdeep
neural networks. Nature Computational Science, 2(5):
331–341,2022.
Schilling, R. L. and Partzsch, L. Brownian motion: an
introductiontostochasticprocesses. WalterdeGruyter
GmbH&CoKG,2014.
Simonov, N. Random walk-on-spheres algorithms for
solvingmixedandNeumannboundary-valueproblems.
SibirskiiZhurnalVychislitel’noiMatematiki,10(2):209–
220,2007.
Sirignano,J.andSpiliopoulos,K.DGM:Adeeplearningal-
gorithmforsolvingpartialdifferentialequations. Journal
ofcomputationalphysics,375:1339–1364,2018.
12SolvingPoissonEquationsusingNeuralWalk-on-Spheres
A.Green’sfunctionfortheBall seeEvans(2010, Chapter2.2). Basedon(9)andthefact
thatΦ isconstantattheboundaryofB =z+B (0),we
z r
Forthesakeofcompleteness,thissectionprovidesdetails
cancomputethevalueofthecorrectorfunctionϕ ,i.e.,
z
onthederivationinSection4.3. Tocomputeintegralsofthe

form(14),welookataspecialcaseofaPoissonequationon  1 logr, d=2,
aballB =B (z)withzeroDirichletboundarycondition, ϕ (y)=E[Φ (Xy )]= 2π
i.e.,
r
(cid:40)
z z τ(B,y) − (2r −d d− )2 ωd, d>2.
∆v =f, on B,
This shows that the value of the Green’s function at the
v =0, on ∂B. centerzoftheballBisgivenby
Analogouslyto(9),weobtainthat
Φ (y)−ϕ (y)=−G (y,z),
z z r
(cid:34) (cid:35)
(cid:90) τ(B,z) which,togetherwith(20),establishestheclaim.
v(z)=E − f(Xz)dt , (18)
t
0
A.1.StableImplementation
whereτ(B,z)isthecorrespondingstoppingtime,see(7).
For numerical stability, we directly compute the quantity
However,sincewesimplifiedthedomaintoasimpleball, G˜ (γ,z):=|B (z)|G (γ,z)inpractice,asneededin(15).
r r r
we can write the solution in terms of Green’s functions.
Thevolumeofthehyper-sphere|B (z)|isgivenby
r
Specifically,wehavethat
(cid:90) |B (z)|=
πd
2 rd,
v(z)=− f(y)G r(y,z)dy (19) r Γ(d +1)
2
B
suchthatweobtain
where

(cid:40) 1 log r , d=2, G˜
(γ,z):=:=r 22 log ∥γ−r z∥, d=2,
G r(y,z):= Γ2π (d/2−1∥ )y− (cid:0) ∥z y∥ −z∥2−d−r2−d(cid:1)
, d>2.
r  d(dr −d 2)(cid:0) ∥γ−z∥2−d−r2−d(cid:1) , d>2.
4πd/2
Wenotethat(19)isequivalentto(15).
B.ImplementationDetails
While this is a classical result by Boggio (1905), see
WeimplementedallmethodsinPyTorchandprovidepseu-
also Gazzola et al. (2010), we will sketch a proof in the
docodeinAlgorithms2and3. Theexperimentshavebeen
following. WeconsidertheLaplaceequation∆Φ =δ for
x x
givenx ∈ Rd inthedistributionalsense. Itiswellknown conductedonA100GPUs.
thatthefundamentalsolutionΦ xisgivenby Forallourtraining,weusetheAdamoptimizerandlimit
theruntimeto25d+750secondsforafaircomparison. In
(cid:40)
1 log∥y−x∥, d=2,
Φ (y)= 2π everystep,wesampleuniformlydistributedsamples(ξ,ζ)
x −∥y−x∥2−d
, d>2, inthedomainΩandontheboundary∂Ωtoapproximate
(d−2)ωd
theexpectationsofthelossandboundaryterms. Moreover,
where weemployanexponentiallydecayinglearningrate,which
reducestheinitiallearningratebytwoordersofmagnitude
2πd/2 4πd/2
ω =|∂B (0)|= = throughouttraining. Wechooseafeedforwardneuralnet-
d 1 Γ(d/2) (d−2)Γ(d/2−1) work with residual connections, 6 layers, a width of 256,
andaGELUactivationfunction. Wealsoperformthegrid
isthesurfacemeasureofthed-dimensionalunitballB (0).
1 searchfortheboundarylosspenaltyterm,i.e.,
Undersuitableconditions,itfurtherholdsthatthesolution
to(18)isgivenby β ∈{0.5,1,5,50,100,500,1000,5000}.
(cid:90) Wefurtherincludethebatchsizem∈{2i}17 inourgrid
v(x)= f(y)(Φ (y)−ϕ (y)) dy (20) i=7
x x search. Forafaircomparison,wesetafixedGPUmemory
B
budgetof2GiBfortraining,leadingtodifferentmaximal
foreveryx∈B,wherethecorrectorfunctionϕ xsatisfies batch-sizes depending on the method; see also Figure 3.
theLaplaceequation Unlessotherwisespecified,10%ofthebatchsizeisused
for boundary points. Moreover, we set ε = 10−4 for all
(cid:40)
∆ϕ x =0, on B, methodsusinganε-shell. Letusdetailthehyperparameter
ϕ =Φ , on ∂B, choicesspecifictoeachmethodinthefollowing.
x x
13SolvingPoissonEquationsusingNeuralWalk-on-Spheres
Algorithm2TrainingofourNWoSmethod Algorithm3Walk-on-Spheres(WoS)
Input: neural network v with initial parameters θ, opti- Input: neuralnetworkv , sourcetermf, boundaryterm
θ θ
mizer method step for updating the parameters, WoS g,pointforevaluationx,maximumnumberofstepsK,
method WoS in Algorithm 3, number of iterations T, stoppingtoleranceε,numberoftrajectoriesN
batchsizesm andm fordomainandboundarypoints, Output: estimatorvofsolutionvtoPDEin(2)atx
d b (cid:98)
bufferB ofsizeB, boundaryfunctiong, bufferupdate vˆ←0
intervalL,boundarypenaltyparameterβ fori←1,...,N do ▷Batchedinimplementation
Output: optimizedparametersθ s←0
x ←samplefromζ⊗B ▷Samplepointsin∂Ω fort←1,...,K do
∂Ω
B ←initializewith(x ,g(x )) ▷Initializebuffer r ←dist(x,∂Ω) ▷Computedistanceto∂Ω
∂Ω ∂Ω
fork ←0,...,T do ifr <εthen
ifk mod L=0then Break ▷Reachboundary
x
Ω
←samplefromξ⊗md ▷SamplepointsinΩ endif
x ←samplefromB ▷SamplepointsinB γ ←samplefromU(B (x)) ▷Estimatesource
B r
x←[x ,x ] ▷Concatenatepoints s←s−|B (x)|f(γ)G (x,γ)
Ω B r r
[y ,y ]←vmap[WoS(x,v )] ▷WoS u←samplefromU(∂B (x))
Ω B θ r
B ←updatewith(x ,y ) ▷Updateestimates ift=0 & use control variatethen
B B
B ←replacewith(x ,y ) ▷Replacepoints s←s−∇ v (x)·u ▷Controlvariate
Ω Ω x θ
endif endif
x
∂Ω
←samplefromζ⊗mb ▷Samplepointsin∂Ω x←x+u ▷Walktonextpoint
(x ,y )←samplefromB ▷SamplepointsinB endfor
B B
L(cid:98)NWoS ←MSE(v θ(x B),y B) ▷Domainloss ifr <εthen ▷Estimatesolutionatx
L(cid:98)bnd ←MSE(v θ(x ∂Ω),g(x ∂Ω)) ▷Boundaryloss x←projectxto∂Ω ▷Findclosestpointin∂Ω
v ←s+g(x)
L(cid:98)=L(cid:98)NWoS+βL(cid:98)bnd (cid:98)
(cid:0) (cid:1) else
θ ←step γ,∇ θL(cid:98) ▷SGDstep
v ←s+v (x)
endfor (cid:98) θ
endif
endfor
vˆ← 1vˆ ▷ComputeMCestimate
Walk-on-Spheres(WoS): ForWoS(Muller,1956),we N
directlyapproximatethesolutionattheevaluationpoints.
We batch trajectories to saturate the memory budget and
presentthebestresultfordifferentconfigurationswithinthe
numberoftrajectoriesforeachupdate.
givenruntime. Specifically,wepickthenumberoftrajec-
toriesN inthegrid{1,10,100,1000,10000,100000}and Diffusionloss: Forthediffusionloss(Nu¨sken&Richter,
themaximumnumberofstepsK in{0,1,10,100,1000}. 2021a),weperformagridsearchoverthetime-steps∆t∈
{10−3,10−4,10−5} of the Euler-Maruyama scheme and
Neural Walk-on-Spheres (NWoS): For NWoS, we try
themaximumnumberofstepsin{1,5,10,50}.
the different extensions in Section 4.5. Specifically,
we fix the buffer size B to 10 times that of the batch PINNs: For PINNs (Raissi et al., 2019; Sirignano &
size m, and sweep the number of gradient steps be- Spiliopoulos, 2018), we use automatic differentiation to
tween buffer updates L ∈ {10,100,1000}. We also computetheLaplacian∆v .
θ
include the maximum number of WoS steps K ∈
DeepRitz: FortheDeepRitzmethod(Eetal.,2017),we
{0,1,5,10,50,100} and the number of trajectories per
experimentwiththeoriginalnetworkarchitectureproposed
update N ∈ {1,10,100,200,300,400,500,1000} in our
intheirpaper. Wesweepthenumberofblocksin{4,6,8},
grid search. If using a boundary loss, we sweep over
thenumberoflayersin{2,4},andthehiddendimensionin
{0.1,0.2,0.3,0.4,0.5} in the grid search to find the opti-
{64,128,256}. Moreover,wereplacetheactivationfunc-
malproportionofthebatchsizefortheboundaryloss.
tionwithGELU.
Neural Cache: For the neural cache method (Li et al.,
2023), we use the best configuration for different buffer C.AblationStudies
sizes, update intervals, and number of trajectories within
the given time and memory constraints. Specifically, we In this section, we provide additional ablation studies on
try buffer sizes B ∈ {10000,20000,100000,1000000}, thethecontributionofouradditionalimprovementsinSec-
intervals L ∈ {1,10,100,1000,5000,10000} to update tion4.5,namelythecontrolvariatesaswellastheneural
thebuffer,andN ∈ {1,10,20,30,40,50,100,500,1000} network evaluation for trajectories that did not reach the
14SolvingPoissonEquationsusingNeuralWalk-on-Spheres
Table3.Ablationstudyofthecontributionofcontrolvariatesand Table4.RelativeL2-error(andstandarddeviationsover5inde-
theneuralnetworkevaluationfortrajectoriesthatdidnotconverge pendent runs) of our considered methods, estimated using MC
afteragivenmaximumnumberofstepsK.Wereporttherelative integrationon106uniformlydistributed(unseen)pointsinΩ.
L2-error for the parameter estimation in our PDE-constrained
optimizationproblem. Method Problem
Poisson(100d) Poisson(500d)
Method RelativeL2-error
PINN 1.49e−3±3.21e−5 2.42−2±6.06e−4
BaseNWoS 2.89% DeepRitz 1.77e−2±1.94e−4 9.92e−3±2.56e−5
+ControlVariate 1.72% Diffusionloss 6.71e−4±1.31e−5 9.47e−3±3.81e−5
Projection 2.92e−4±5.17e−7 1.19e−5±1.67e−8
+TerminalEval 1.70%
NWoS(ours) 6.22e−4±1.18e−5 9.14−3±6.31e−5
+Both 1.30%
used,weobtainthebestrelativeerrorof1.30%,indicating
Base
thattheycanefficientlydecreasebiasandvariance.
0.04 Control Variate
Terminal Eval Figure6decomposesthetrainingtimeperiterationintothe
0.03 timeforthebaseNWoSalgorithmandthetimeforthead-
ditionalextensionsfromSection4.5. Weassumethebatch
0.02 sizetobefixedtom=512andtestonthePoissonequation
inSection4.5in100d. Weobservethatourproposedexten-
0.01
sionsincurcomparablysmalloverheads. Figure7further
compares NWoS with DeepRitz, NSDE, and PINN with
0.00
different maximum number of steps K, see Section 4.5.
1 5 10
Maximum Steps Consideringthelogarithmicscalingoftheplot,eachitera-
tionofNWoSissignificantlyfasterthanPINNandNSDE
Figure6.Decompositionofthetrainingtimeforoneiterationof
butslowerthanDeepRitzforalargermaximumnumberof
NWoS in the plain version (Section 4.4), as well as using our
stepsK. ChoosingK,wecanbalancehighaccuracyand
improvementsfromSection4.5, i.e., thecontrolvariatesanda
neuralnetworkevaluationfortrajectoriesthatdidnotconverge fasttraining.
afteragivenmaximumnumberofstepsK.
D.FurtherEvaluations
4×10 2 Inthissection,weprovidefurthernumericalevidence. We
3×10 2
reporttheconvergenceoftherelativeL2-errorfortheother
PDEsandevaluateourmethodonthePoissonequationin
2×10 2 Deep Ritz 100dand500d.
Diffusion Loss
Figures8to10demonstratethatneuralWoSachievesthe
PINN
fastestconvergenceincomparisontoallbaselinemethods
10 2 withintheprovidedtimeandmemoryconstraints.
Table4providesresultsforourconsideredmethodsonthe
Poissonequationin100dand500dasproposedbyE&Yu
1 5 10
(2018). WedemonstratethatourNWoSmethodachieves
Maximum Steps
lower relative L2-error than the baselines. However, we
Figure7.Trainingtimeofourconsideredmethodsforonegradi- discoverempiricallythat,forthisbenchmark,asimplepro-
entstep. ForNWoS,wepresentthecomparisonforadifferent
jectiontotheboundaryachievesthehighestaccuracy. This
maximumnumberofstepsK.
canbemotivatedbythesmoothnessofthesolutionandthe
factthatuniformlydistributedevaluationsamplesconcen-
boundary. WealsoanalyzethespeedofNWoSandperform trateattheboundaryinhighdimensions.
comparisonswithPINNs,DeepRitz,andthediffusionloss.
InTable3,weperformanablationstudyonthecontribution
ofourimprovementsinourPDE-constrainedoptimization
problem. WeobservethattheycandecreasetherelativeL2-
errorfrom2.89%to1.72%and1.70%,respectively. Ifboth
the control variate and the neural network evaluation are
15
)s(
noitareti
rep
emiT
)s(
noitareti
rep
emiTSolvingPoissonEquationsusingNeuralWalk-on-Spheres
100 Neural WoS (Ours)
DeepRitz
Diffusion Loss
PINN
10 1 Neural Cache
10 2
10 3
0 200 400 600 800 1000
Time (s)
Figure8.ConvergenceoftherelativeL2-errorwhensolvingthe
Committorfunctionin10dusingourconsideredmethods.
Neural WoS (Ours)
DeepRitz
100 Diffusion Loss
PINN
Neural Cache
10 1
10 2
10 3
0 200 400 600 800 1000
Time (s)
Figure9.ConvergenceoftherelativeL2-errorwhensolvingthe
Poissonequationin10dwithrectangulartorususingourconsid-
eredmethods.
100
Neural WoS (Ours)
DeepRitz
Diffusion Loss
10 1 PINN
Neural Cache
10 2
10 3
0 250 500 750 1000 1250 1500 1750 2000
Time (s)
Figure10.ConvergenceoftherelativeL2-errorwhensolvingthe
Poissonequationin50dusingourconsideredmethods.
16
rorre
2L
evitaleR
rorre
2L
evitaleR
rorre
2L
evitaleR