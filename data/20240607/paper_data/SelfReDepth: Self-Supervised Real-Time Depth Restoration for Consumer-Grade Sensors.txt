Journal of Real-Time Image Processing manuscript No.
(will be inserted by the editor)
SelfReDepth
Self-Supervised Real-Time Depth Restoration for Consumer-Grade Sensors
Alexandre Duarte · Francisco Fernandes · Jo˜ao M. Pereira · Catarina
Moreira · Jacinto C. Nascimento · Joaquim Jorge
Received:September14,2023/Accepted: June3,2024
Abstract Depth maps produced by consumer-grade noising and hole-filling by inpainting of full-depth maps
sensorssufferfrominaccuratemeasurementsandmissing captured with RGB-D sensors. The algorithm targets
data from either system or scene-specific sources. Data- depth data in video streams, utilizing multiple sequen-
driven denoising algorithms can mitigate such problems, tial depth frames coupled with color data to achieve
However, they require vast amounts of ground truth high-quality depth videos with temporal coherence. Fi-
depth data. Recent research has tackled this limitation nally, SelfReDepth is designed to be compatible with
usingself-supervisedlearningtechniques,butitrequires variousRGB-Dsensorsandusableinreal-timescenarios
multiple RGB-D sensors. Moreover, most existing ap- as a pre-processing step before applying other depth-
proaches focus on denoising single isolated depth maps dependent algorithms. Our results demonstrate our ap-
or specific subjects of interest highlighting a need for proach’s real-time performance on real-world datasets
methodsthatcaneffectivelydenoisedepthmapsinreal- shows that it outperforms state-of-the-art methods in
time dynamic environments. This paper extends state- denoising and restoration performance at over 30fps on
of-the-art approaches for depth-denoising commodity Commercial Depth Cameras, with potential benefits for
depth devices, proposing SelfReDepth, a self-supervised augmented and mixed-reality applications.
deep learning technique for depth restoration, via de-
Keywords Deep learning · Self-supervised learning ·
Image denoising · Image reconstruction · RGB-D
AlexandreDuarte 1
sensors
E-mail:alexandre.a.duarte@tecnico.ulisboa.pt
Francisco Fernandes2 MathematicsSubjectClassification(2020) 68T07·
E-mail:francisco.fernandes@tecnico.ulisboa.pt 94A08
Jo˜aoM.Pereira1,2
E-mail:jap@inesc-id.pt
1 Introduction
CatarinaMoreira2,4
E-mail:catarina.pintomoreira@uts.edu.au
Depth information is pivotal in many applications, from
JacintoC.Nascimento 1,3
digital entertainment to virtual and augmented real-
E-mail:jacinto.nascimento@tecnico.ulisboa.pt
ity [21]. It is the backbone for digital object and en-
JoaquimJorge 1,2
vironment modeling [42,8] and cost-effective motion
E-mail:jaj@inesc-id.pt
capture solutions [18].
1 Instituto Superior T´ecnico, Universidade de Lisboa (IST-
Poseestimationderivedfromdepthdatafindsutility
UL),Lisbon,1000-029,Portugal.
2 Instituto de Engenharia de Sistemas e Computadores, In- in diverse fields such as physiotherapy [17,5], video sur-
vestiga¸ca˜oeDesenvolvimento(INESC-ID),Lisbon,1000-029 veillance [63,34], and human-computer interaction [46].
Portugal. Depthdataalsoaidsautonomousnavigation[15]anden-
3 InstituteforSystemandRobotics(ISR),InstitutoSuperior
hances security measures through facial recognition [43].
T´ecnico,UniversidadedeLisboa(IST-UL),Lisbon,1049-001,
Consumer depth devices, often employing low-cost
Portugal.
4 HumanTechnologyInstitute,UniversityofTechnologySyd- LiDAR, Structured Light, or Time-of-Flight technolo-
ney, Sydney,Australia. gies, are instrumental in these applications. Among
4202
nuJ
5
]VC.sc[
1v88330.6042:viXra2 AlexandreDuarteet al.
these, the Microsoft Kinect v2 stands out for its bal- Denoising vs. inpainting: The distinction between
ance of quality, availability, and affordability. However, denoising and inpainting is important to be stressed, as
consumer-grade sensors like Kinect v2 still grapple with these terms will be used throughout this work consti-
noisy and incomplete data issues. tuting important stages of the proposed methodology.
Efforts to address these quality issues span tradi- Denoising and inpainting are two core image process-
tional smoothing techniques to data-driven machine ing problems. As the name suggests, denoising removes
learning algorithms. Many adopt supervised learning noise from an observed noisy image, while inpainting
with neural networks, training models on noisy-clean aims to estimate missing image pixels. Both denoising
data pairs (xˆ,y) to minimize empirical risk. andinpaintingareinverseproblems:thecommongoalis
However, acquiring clean training data is non-trivial. to infer an underlying image from incomplete/imperfect
Recentattentionhasthusshiftedtowardsself-supervised observations. Formally, in both problems the observed
techniques, such as Noise2Noise [27], which leverages image Y ∈ RM′×N′ is modeled as Y = F(X) + η
noisy-noisy data pairs (xˆ,yˆ) for training, and minimiz- where X∈RM×N is the unknown (original) image and
ing the cost function g(θ)=argmin
(cid:80)NL(f
(xˆ ),yˆ), η is the observed noise. The difference between the de-
θ i θ i i
where the network f (xˆ ) is parameterize by θ. noising and the inpainting emerges from the mapping
θ i
Despite their efficacy in various domains, self-super- F : RM×N (cid:55)→ RM′×N′ that expresses a linear degra-
vised methods for depth data restoration remain under- dation operator that could represent a convolution or
explored, largely due to the intricate noise patterns in a masking process. In concrete, the denoising process
consumer-grade sensors. means that F is an identity projector, having F = H,
Our paper introduces SelfReDepth (SReD), a such that H = I, (with I the identity matrix). In the
novel self-supervised, real-time depth data restoration inpaintingprocess,F isaselection operator.Inpractice,
technique optimized for the Kinect v2. SelfReDepth this corresponds to having the same H as before. How-
introduces a convolutional autoencoder architecture in- ever, this matrix only contains a subset of the rows of I,
spired by U-Net, specifically designed to process sequen- accounting for the loss of pixels.
tial depth frames efficiently. This design choice directly Having formally defined the two concepts, we stress
respondstotheneedformaintainingtemporalcoherence that both are used in restoration problems, as we pro-
indynamicscenes,agapoftenleftunaddressedbytradi- poseinthiswork.Inthispaper,twomajorcontributions
tional single-frame denoising approaches. Furthermore, are offered for restoration, concretely: (i) a new denois-
SelfReDepth incorporates RGB data into the depth ing method. Contrasting with Noise2Noise [27] that
restoration process as an innovative way to enhance applies denoising in traditional images, we extend the
the accuracy of inpainting missing pixels by providing framework to depth images that require a new learning
contextual color information. This method significantly strategy to handle depth information (see top branch in
improvestherestorationqualitybyprovidingadditional Fig. 2 and Sec. 3.2), and (ii) inpainting approach where
context that depth data alone lacks. Our contributions we integrate a new two-stage pipeline comprising an
are fourfold: (1) We employ a convolutional autoen- RGB-Depth registration and a Fast Marching Method
coder with an architecture akin to U-Net [47] to process stage (see a bottom branch in Fig. 2).
sequential frames. (2) Our method achieves real-time
performance and temporal coherence by adopting a
Problems with low-quality depth: Despite all the
video-centric approach. (3) We incorporate RGB data
progress made in in-depth sensing hardware for con-
to guide an inpainting algorithm during training, en-
sumer devices, depth cameras (such as the Kinect v2)
hancing the model’s ability to complete missing depth
still suffer from many of the same problems that pre-
pixels. (4) Our approach maintains a 30fps real-time
vious iterations also did, namely, noisy measurements
rate while outperforming state-of-the-art techniques.
and depth holes [38].
These depth holes typical of Time-Of-Flight (ToF)
2 Background and Related Work devices have multiple causes [20,50] including: (1) Mea-
suring regions that are outside the distance range of
In recent years, depth-sensing technology has emerged the sensor, (2) Highly reflective objects in the scene,
as a pivotal tool in various applications, from gaming (3) Measurements near the edges of the camera’s field-
to augmented reality and robotics. The promise of cap- of-view (FoV). Smaller holes, on the other hand, can
turing the third dimension, depth, has opened up new appear in one of two types: (1) Isolated points caused
horizons in computer vision, augmented reality, and by physical and lighting interferences on the sensor, (2)
human-computer interaction. Next, we introduce some Thin outlines around objects due to the scattering of
concepts and methodologies related to the present work. infrared rays at shallow angles and sharp edges.SelfReDepth 3
(a) training (b) inference
Fig. 1 Multi-Frame-to-Frame(MF2F)[13]architecturewithdistinctstrategies from thetrainingandinferencesteps.
Besides missing depth values, the measurement inac- Self-supervisedDenoisers:Noise2Noise[27]pioneered
curacyinthesuccessfullycapturedpointsisaconcerning self-supervised image denoising, showing that a denois-
issue, leading to noisy depth maps. The noise produced ing model trained with only noisy data can achieve
byKinectv2issignificantlylessseverethanintheprevi- quality results on par with supervised learning strate-
ousgeneration.Nevertheless,itisstillverymuchpresent. gies. The shift in the learning method from supervision
Experimental analysis [61,26,54] has shown that there to self-supervision resides primarily in the training data.
is a direct correlation between the noise observed in Specifically, Noise2Noise [27] uses input-target pairs of
Kinect v2’s depth maps and various physical factors, the form (xˆ,yˆ)=(x+n ,x+n ), where x is the base
1 2
includingdistance,angle,material,color,warm-uptime, signal (the undamaged data that we want to uncover)
to quote a few. Furthermore, there is a general consen- and n and n are two independent noise instances fol-
1 2
sus [12,25,1] that this noise can be described as the lowingthesamestatisticaldistribution.Fromtheabove,
sum of two different sources of noise: (1) Random noise, the Noise2Noise strategy differs from the noise-clean
associated with pixel-based local distortions caused by datapairs(xˆ,y)usedinsupervisedlearningandhasthe
physical factors like color and others mentioned above, advantage of not requiring clean target images.
(2) Systematic bias, associated with the wiggling error
Nonetheless, Noise2Noise [27] has some data limita-
that radially increases as the measurements get closer
tions, encouraging subsequent works to propose further
to the edges of the sensor’s FoV.
improvements. Towards this challenge, [4] proposes two
data permutation techniques to increase the number of
Deterministic denoisers, or manualdenoising algo- noisytrainingpairs.Ontheotherhand,Noise2Void[23]
rithms that do not rely on machine learning were the eliminates the need for quasi-similar input-target pairs
first noise reduction techniques to be developed tar- (not always easy to obtain) by training the model to
geting depth data [14]. These can generally be divided predict a central pixel using noisy-void training pairs
into three main categories: (i) filter-based denoisers, (ii) (xˆ,−) and a blind-spot mask to avoid learning the iden-
outlier removal techniques, and (iii) calibration methods. tity. Noise2Self [2] later expanded on this by proposing
Filter-based denoisers work by applying smoothing a more generalized model.
and sharpening filters, such as bilateral filters [37,62], Going further, some works, namely Probabilistic
joint bilateral filters [10,9], anisotropic filters [35] and Noise2Void [24], SURE [39], Noisier2Noise [41] and
zero block filters [35], to leverage spatial pixel neighbor- NoiseBreaker [28], managed to improve denoising for
hoods through sliding pixel windows (i.e. kernels). The specificdistributions.Ontheotherhand,GAN2GAN[6]
preservation of edge sharpness is particularly difficult combines a generative model, Self2Self [45] introduces
to achieve using filters; thus, some denoisers introduce trainingwithasingledatasample,andGainTuning[40]
specialized techniques, such as RGB-D alignment [10] proposes an ever-adapting model.
and contextual image partitioning [9].
Other works, instead, focus exclusively on remov- Spatio-temporalDenoisersareanextensionofimage
ing incorrect or low-quality depth points rather than denoising where the coherence of temporal locality is
correcting them. This can be seen, for instance, in [12] also considered to provide visual continuity in the final
for cleaning hand depth scans and in [57] for cleaning denoised videos. Likewise, clean data may also not be
bodyscans,latermergedtoformacompletebodypoint easy to obtain for this task, and thus, blind training
cloud. Finally, some works tackle the denoising problem comes with great interest. A simple multi-frame self-
from a calibration standpoint, focusing on alleviating supervised strategy for denoising can be extrapolated
systematic errors affecting consumer-grade sensors by from Noise2Stack [44], in which a self-supervised ap-
fitting planes or splines to the raw measurements [25, proach is proposed to denoise MRI data using adjacent
26] or generating specialized noise correction maps [1]. sets from a stack of layered MRI brain scans.4 AlexandreDuarteet al.
Self-supervised denoising techniques targeting color are commonly linked with autonomous driving. Thus,
videos have also been developed, using the multi-frame more advanced techniques have been developed, relying
input concept described in Noise2Stack [44] combined on both supervised [29] and self-supervised [36,11,16]
withadditionalself-supervisiontechniques.Multi-Frame- deep convolutional neural networks assisted by color
to-Frame (MF2F) [13] (see Fig. 1) takes the FastDVD- information to fill large depth gaps.
Net [52] supervised video denoising network, composed
by cascaded U-Net [47] autoencoders, and applies its
own self-supervised loss. 3 Our Approach
Similarly, UDVD [48] uses a cascaded structure akin
Building upon recent advancements in self-supervised
to FastDVDNet [52] but performs the network pass 4-
data denoising research, the proposed SReD offers a
fold, each with the input frames at a different rotation
(0º, 90º, 180º and 270º). The four outputs generated, novel approach for denoising and inpainting low-quality
depth maps. It leverages the flexibility and adaptability
one for each of the four rotations, are then rotated back
of deep learning models while eliminating the need for
to 0 degrees and combined to form the final production.
reference data - a highly desirable feature also found
Depth Completion: Alongside inaccurate data points,
in deterministic denoisers. SReD was designed with a
low-quality depth maps also suffer from missing or in-
specificpracticalusecaseinmind,incorporatingseveral
valid data. Depth completion, also known as hole-filling,
additional requirements during its design and develop-
is a well-known and vastly researched area that falls
ment.Specifically,ourtechniqueaimsto:(i)denoiseand
under the umbrella of image inpainting [60,32,59].
restoreasmuchoftheinitialdepthmapsaspossible,(ii)
The effect of using Noise2Noise and similar algo-
operate with a single RGB-D device, (iii) facilitate di-
rithms over images with missing data without any prior
rect sensor data streaming, and (iv) strive for temporal
inpainting is that the majority of depth holes remain
coherence and real-time performance.
untreated in the final images, and even in methods that
Naturally, these requirements posed challenges that
deal with multiple consecutive frames, there is insuffi-
influenced the architectural decisions. For example, to
cient data to fill these gaps in most cases.
achieve depth video denoising with temporal coherence,
As in-depth denoising, depth completion has been
it is logical to design a method that utilizes multiple
approached using traditional and deep neural method-
sequential frames, similar to MF2F [13]. However, given
ologies. Traditional techniques typically rely on either
real-time constraints, only frames up to the most re-
filtering algorithms, which classify the holes and apply
cent one are considered. This contrasts with MF2F,
dedicated filters, such as PDJB, DJBF, and FCRN, or
which incorporates two subsequent frames at the cost of
boundary-extending algorithms, based on FMM or the
adding considerable lag. Additionally, an architecture
Navier-Stokes equation [3].
with faster inference is preferable over a more complex
The Fast Marching Method (FMM) inpainting, in
one to meet real-time performance criteria.
particular, was originally proposed for color image in-
painting [53] and works by progressively shrinking the
boundaries of hole regions inwards, until all pixels have
3.1 SelfReDepth’s Architecture
been filled, using the equation
(cid:80)
w(p,q)·[I(q)+∇I(q)·(p−q)] SReD’sarchitecture,particularlyitsneuralnetworkand
I(p)= q∈N(p) (1) learning method, takes inspiration from previous self-
(cid:80)
w(p,q) superviseddenoisers,mainlyNoise2Noise[27],MF2F[13]
q∈N(p)
andNoise2Stack[44],adaptingtheirproposeddenoising
where p is the pixel being inpainted, N(p) is a neighbor- models to the specific case of online denoising in depth
hood pixels of p, w(p,q) is a function that determines map sequences. As depicted in Fig. 2, the architecture
how much pixel q contributes to the inpainting of p, I, differs between its training and inference stages. The
and ∇I represent the image and discrete gradient of model uses a dilated input during the training stage,
the image, respectively. This algorithm was extended as proposed in MF2F [13]. The autoencoder is trained
to depth completion, introducing improvements like the with noisy pairs (xˆ,yˆ) = ([d ,d ,d ],d ), where
t−4 t−2 t t−1
use of aligned color as a guiding factor for the weight d is the depth frame at time instant t − k, with
t−k
function and to define the order of computations [33], k ∈{0,1,2,4}. This technique improves the denoising
and the use of a pixel-wise confidence factor [30]. results during inference and prevents the network from
Sparse depth maps, generally captured with LiDAR learning the identity by hiding frame d from the in-
t−1
sensors, suffer especially from large patches of missing put. Since d and d are consecutive time frames, it
t t−1
depthdataandhaveparticulartimelimitations,asthey is also plausible to assume they are similar in contentSelfReDepth 5
(a) training (b) Inference
Fig. 2 Full overviewofSelfReDepth’sarchitecture.
while having different instances of noise, making them requiresacquiringtheextrinsic and intrinsicparameters
a suitable image pair for the Noise2Noise-style training. of the device, namely:
Moreover,noisydepthframesfrequentlyhaveregions
(cid:2) (cid:3)⊤
– Focal length f = f f and principal point
persistently composed of depth holes in both the input d d,x d,y
(cid:2) (cid:3)⊤
and target frames, making these regions impossible to c d = c d,x c d,y of the depth/IR sensor,
“denoise”usingastandalonedenoisingnetwork.Assuch, – Focal length f = (cid:2) f f (cid:3)⊤ and principal
rgb rgb,x rgb,y
during training, the target frame d
t−1
is inpainted with
point c
=(cid:2)
c c
(cid:3)⊤
of the RGB sensor,
rgb rgb,x rgb,y
an FMM inpainting algorithm guided by the registered
– Rotation matrix R, which encodes the rotation from
color frame RGB , providing a way for the denoiser
t−1 the RGB sensor view to the depth/IR sensor,
to learn how to fill depth-holes.
– Translation vector T translates from the RGB sen-
In summary, SReD’s training architecture has two
sor’s position to the depth/IR sensor’s position.
distinct main blocks, depicted in Fig. 2a: (i) a denois-
ing convolutional autoencoder with dilated input; and Following [64], RGB-D registration is performed
(ii) a target generation pipeline responsible for creat- throughaseriesofcoordinatetransformationsthatmap
ing inpainted targets. During inference (Fig. 2b), the depth values captured from the depth sensor’s point-of-
target generation is removed, contributing to faster per- view to color values in the RGB camera’s point-of-view.
formance and the denoiser shifts to non-dilated input Toachievethis,thedepthdata,givenasadepthmap,is
(i.e., taking the frames [d ,d ,d ]), estimating a firstconvertedtoapointformatwhereforeachpixelco-
t−2 t−1 t
(cid:2) (cid:3)⊤ (cid:2) (cid:3)⊤
denoised/inpainted instance of the frame d . ordinate x y exists a 3-d point X = x y z
t d d d d d d
with z = depth(x ,y ), and then transformed from
d d d
Depth Image Coordinate Space to RGB Image Coordi-
3.2 Target Generation
nate Space, X , using the following equalities:
rgb
Thedenoiserrequiresalearningstrategytohandledepth (x −c )·z 
x′ d d,x d
holes. To achieve this, SReD generates target frames d  f d,x 
through deterministic inpainting. This deterministic ap-    
proachconsistsoftwostages:(i)computingaregistered
X d′ =

y d′

=

(y d− fc d,y)·z d


(2)
 d,y 
RGB image and (ii) using the previous result to apply z′
d z
guided inpainting to the damaged depth frame. The se- d
lectionofthisstrategyrestsonthreeprimaryreasons:(1)
Theprevalenceofdepthholesingeneralconsumerdepth X′ =R−1·(X′ −T) (3)
rgb d
data is sufficiently low for a deterministic approach to
yieldacceptableinpaintingresults.(2)Itavoidstheneed
forreferencedata.(3)Fromatemporalperformanceper-  x′ ·f 
spective, it only introduces computational time during x  rgb rgb,x +c
rgb  z′ rgb,x
t Rra Gin Bin -g D.
Registration: RGB-D devices collect color
X rgb =   y rgb   =    y r′ gb z·r ′g fb rgb,y +c rgb,y   

(4)
and depth with physically separate sensors/cameras, z rgb  rgb z′ 
rgb
and often also different resolutions and FoV. Therefore,
aligning the RGB and depth frames simultaneously cap- Points X in (4) can then be mapped to a 2D
rgb
tured must be done with a registration algorithm and W × H size image, forming a registered depth
rgb rgb6 AlexandreDuarteet al.
image. This process summarizes the standard registra- value of neighboring pixels, so that homogeneous areas
tion algorithm. However, the computation of registered are inpainted before other regions more likely to be
RGB images is needed for target generation. So the transitive or edge areas. However, the priority function
RGB (cid:55)→D mappings produced by Eqs. (2)–(4) are re- used in SReD’s inpainting for target generation, Pr(p),
versed to build a 2D W ×H image of color values introduces a new normalization variable T , leading
d d max
instead. Of course, doing this still leaves depth holes to the final equation:
withnoRGBvalueattributed,weakeningthewholepur-
T(p)
pose of performing RGB-D registration. To overcome Pr(p)=(1−λ)· +λ·(1−S (p)) (9)
T g
this, depth holes are filled with pixel interpolation and max
blurring to create smooth transitions between edges of
known depth regions.
1 (cid:88)
S (p)= · w (p,q) (10)
g |N(p)| g
Inpainting: After completing the RGB-D registration,
q∈N(p)
weemployacolor-guidedFastMarchingMethod(FMM)
where, S (p) (Eq. (10)) gives the local guide similarity
inpainting algorithm to generate the target frames for g
at pixel p, |N(p)| denotes the number of known pixels
training the denoising network. Following the original
in the neighborhood of p, T is the greatest value in
FMM inpainting technique [53], the algorithm starts by max
distance map T, and λ is a mixing parameter. (Note:
delineating the boundaries of all hole regions within the
lower Pr values denote greater priority)
image. Subsequently, it performs inpainting from the
outer pixels of these boundaries inwards, ensuring that
all hole regions are filled.
3.3 Denoising Network
Our FMM inpainting technique combines ideas pre-
sented in [53,33,30] and introduces novel elements that
The denoising neural network implemented in SReD
enable better results in consumer depth maps. Specifi-
adopts a convolutional autoencoder architecture based
cally, the pixel weighting function (see Eq. (5)) is differ-
on the U-Net design [47]. This architecture is primar-
ent from the original FMM inpainting [53]. Concretely,
ily influenced by features from MF2F [13], FastDVD-
weprioritizethedistancefactorw whiledroppingthe
dst
net [52], and Noise2Noise [27]. During inference, the
factorsw andw .Additionally,weincludetwonovel
lev dir
network takes as input three sequential depth frames,
weights: w , relating to color guidance [33]; and conf,
g
specifically d ,d ,d . Conversely, during training,
a confidence factor as in [30]. All these new insights t−2 t−1 t
theinputframesaredilated,namelyd ,d ,d .This
contribute to the following novel functions: t−4 t−2 t
setupenablesusinganinpaintedversionofframed as
t−1
w(p,q)=w2 (p,q)·w (p,q)·conf(q) (5) thetarget,therebypreventingthenetworkfromlearning
dst g
the identity function. The network employs the Mean
Absolute Error (MAE or L ) loss function to measure
1
w (p,q)=
d2
0 (6)
the discrepancy between the inpainted target d∗
t−1
and
dst ∥p−q∥2 theinputdepthframed t.Innoisyregions,thisapproach
replicates the effects of Noise2Noise [27], and for depth
holes, the network learns inpainting techniques.
(cid:18) ∥G(p)−G(q)∥2(cid:19)
Using a U-Net [47] helps with image denoising. This
w (p,q)=exp − (7)
g 2·σ2 is because the skip-connections enable passing higher
g
frequency details from the encoding stage to the decod-
ing stages via layer concatenation. This propagation
1
conf(q)= (8) allows the network to “flatten” noise areas while still
1+2·T (q)
out preservingsharpimagefeatures,suchasobjectcontours.
where d is the minimum inter-pixel distance, usually Regardingthelayerstructure,thegenerallayoutloosely
0
1, G denotes the guiding image, and σ2 is its standard follows the model presented in Noise2Noise [27], differ-
g
deviation. Additionally, T is a distance map that stores ing mainly in the number of channels at each network
thedistanceofeachpixeltotheclosestinitialholepatch blockandthedownsampling/upsamplinglayers.Instead
boundary, and T is a function that zeroes pixels in of max pooling and 2D upsampling layers, SReD uses
out
thesetofinitialholesΩ andassignsT totheremaining. 2D convolutions with stride two and transposed 2D
Furthermore, as in GFMM [33], the pixel inpainting convolutions, giving the model more learning flexibility.
priority takes into account both the distance to the Additionally, like in FastDVDnet [52], the network
initial hole boundary, given by T(p), and the guidance applies a final residual operation between the inputSelfReDepth 7
frame d and the frame resulting from the last convolu- using the developed Kinect v1 noise model from Handa
t
tional layer in the model d , yielding the depth frame et al. [19], which combines Gaussian noise, bilinear in-
last
prediction d =d −d . terpolation, and quantization to produce noisier pixels
pred t last
at higher distances and missing depth values at pixels
whose corresponding normals are close to perpendicular
4 Evaluation to the camera’s viewing direction.
To further assess our approach’s feasibility on these
Weconductedacomprehensiveevaluationofourmethod
data, we evaluated it against the Total Variation (TV)
to assess the algorithm’s performance. A quantitative
method [7]. This denoising technique reduces the total
evaluation was performed using a reference-independent
magnitudeoftheimage’scolourintensitygradientwhile
noise metric to measure SReD’s depth-restoration capa-
simultaneously trying to keep object boundaries. The
bilitiesobjectively.Wealsoperformedseveraltestsusing
regularization parameter weight used in the algorithmic
asyntheticdatasetthatprovidesusableartificialground-
implementation from [56], which controls the denoising
truth data. The algorithm’s time performance was also
strength at the expense of fidelity to the original image,
assessedtodetermineitssuitabilityforreal-timeapplica-
was set to 0.4 as this value maximized the mean scores
tions.Furthermore,weevaluatedthemethod’stemporal
among both datasets in our experiments.
coherence using a specialized metric. Finally, we com-
Furthermore, we assess temporal coherence using
pared SReD to other relevant reference-independent
straightforwardimagedifferences,M =mean(I −
temp t+1
restoration algorithms to situate its performance within
I ). We address depth value oscillations over time by
t
the broader landscape of available techniques.
analysing granular noise values on a frame-by-frame
basis over contiguous video sequences from the dataset.
4.1 Data and Metrics
4.2 Experimental Setup
Identifying an appropriate combination of data and
metrics for evaluating SReD proved to be a non-trivial
We ran all experiments on a Windows 10 desktop ma-
task. Ideally, we would have access to a consumer-grade
chine with an NVIDIA GeForce RTX 3080 GPU, a
depth video dataset featuring raw frame sequences and
Ryzen 7 3700x 8-core CPU, 16 GB of RAM, and an
reference depth data, perhaps captured using a high-
SSD disk drive. We developed and tested SReD us-
precision laser sensor. However, such a dataset is not
ing Python 3.10.8 and tensorflow-gpu 2.10, along with
readily available. This very challenge underscores the
CUDA 11.2 and cuDNN 8.1.
importanceofdevelopingself-superviseddepthdenoisers
like SReD.
We evaluated SReD ona depth video dataset devoid 4.3 Results
of reference depth. The evaluation also used appropri-
ate reference-independent metrics. We conducted com- We trained SReD with batch sizes of 16 and 200 epochs
prehensive tests on the CoRBS dataset [58], explicitly on the CoRBS [58] dataset, which we also thoroughly
focusing on the Kinect v2 subset. These data include shuffled and set with validation and test splits of 0.1
five distinct RGB-D frame sequences capturing a sta- and 0.04, respectively.
tionary scene with a mobile camera, resulting in an ag- In the produced denoised depth maps, in Fig. 5, it
gregate of approximately 14,000 depth frames. In terms can be seen that the model learned how to attenuate
of evaluation metrics, the denoised depth frames under- the noise in the original depth map and fill depth holes.
wentquantitativeassessmentconcerningnoisethrougha Moreover, on the hardware used for evaluation, the
“non-reference metric for image denoising” [22] (NMID), model takes, on average, 9ms to denoise each depth
a robust measure based on structure similarity maps map. Given that a regular RGB-D sensor, such as the
from both homogeneous and highly-structured regions, Kinect v2, records data at a frequency of 30 frames-per-
in the absence of the original clean data. second (33ms per frame), this evaluation confirms that
Additional tests relied on synthetic depth data from themodelcanachievethedesiredreal-timeperformance
theInteriorNetdatasetpublishedin[31],whichprovides during inference.
computer-rendered RGB and depth images for various We also compared SReD against other approaches,
indoorscenes.Forevaluatingthissyntheticground-truth including two deep-learning methods, Noise2Stack [44]
data, we employed proper reference metrics for the com- and Noise2Noise [27] and two deterministic approaches,
parisons: MSE, PSNR, and SSI. Since this dataset does the Total Variation method [7] and a combination of
not provide noisy data, we introduced synthetic noise a pair of methods that applies FMM inpainting [53]8 AlexandreDuarteet al.
Fig. 3 Examplescenesfromtheground-truthdatasetdemonstrating(fromleft toright)RGBcolorimage,realdepthmap,
depth mapwith syntheticnoise added (missingvalues in black color), resultsfrom the Total Variationmethod,SelfReDepth,
anderrormapfromour approach.
Fig. 4 Temporalanalysisofthemeandepthvaluedifferencesacrossframes.Thenoisydataweregeneratedfromasample
video sequence extracted from the InteriorNet synthetic dataset and restored using the SelfReDepth and Total Variation
methods.
Fig. 5 Visual comparisonbetweenfour imagerestoration algorithms applied toan exampleimage taken fromreal datafrom
theCoRBSdataset.Fromlefttoright:SelfReDepth,Noise2Noise, Noise2StackandFMM+BF.
followed by Bilateral Filtering [55] denoising. We chose The results can be seen in Table 1. From the measured
Noise2Noise to evaluate how the implemented tech- values, we note that SReD attained promising results,
niquediffersfromtheoriginalself-supervisedU-Net[47] rivalling the significantly more computationally expen-
denoiser and what benefits were secured by target- sivedeterministicalgorithmswiththeNMIDmetricand
ing specifically the denoising of depth. Similarly, we achieving the best results with the temporal coherence
chose Noise2Stack [44] to compare SReD against an- in both datasets as expected, since it relies on multiple
other spatio-temporal depth denoiser. Last, we used the consecutive frames for its inference process. In addi-
deterministic FMM+BF combination to evaluate how tion, results in Fig. 4 show that original per-frame noise
SReD fares against more traditional approaches that discrepancies are mostly fixed, and yield temporarily
perform both denoising and inpainting. consistent values after denoising.
Asalreadymentioned,weusedanon-referencenoise AsshowninFig.5,Noise2NoiseandNoise2Stackcan
metric, NMID [22], to quantify the denoising quality, only perform pixel denoising and not depth completion.
and applied a direct image difference metric to eval- As for the deterministic algorithm combination, while
uate temporal coherence on contiguous depth videos. it was capable of denoising and inpainting the depthSelfReDepth 9
maps, it can also be visually seen that both the edge the model requires, on average, 9ms to denoise each
preservation and depth completion results are inferior depth map. Given that commercial off-the-shelf RGB-
to those of SReD. D sensors, such as the Kinect v2, generate data at 30
frames-per-second (33ms per frame), our technique can
achievereal-timeperformanceatevenhigherframerates.
4.4 Discussion The modular design of the U-Net architecture allows
for straightforward scalability to accommodate larger
Based on the experiments and metrics, SReD effectively
image sizes without a significant impact on computa-
learned to reduce noise in depth maps. However, some
tional time, thereby maintaining real-time performance,
image details still need to be recovered, evident in the
as will be detailed in the following section.
blurredfeaturesofthedollintheCoRBS[58]dataset.In
thesecaseswherelargeblackdepthholesarepresent,in-
SelfReDepth Complexity: Our approach is made up
painting struggles to effectively reconstruct this missing
of three main blocks as follows: inpainting (eqs. (1) and
data due to the absence of depth information, resulting
(5)), registration (eqs. (2) - (4)) and denoising (U-Net
in over-smoothed restorations. While the results are
network) procedures. As already stated in Sec. 3.1, our
promising, they highlight the need for further work on
proposal is designed to satisfy real-time requirements,
detail preservation.
as we can modify the SReD architecture during the
Additionally, our method seems to struggle with inferencestage,whichistheonethathasadirectimpact
accuratelyrestoringthedepthofobjectsveryclosetothe on the complexity budget. Specifically, (i) remove the
camera,asseeninthetoprowofFig.3anditsrightmost target generation, and (ii) the denoise shift to non-
error profile. This scenario where the objects are almost dilated input. Thus, only the U-Net (denoiser) needs to
touching the camera was not seen in the training data be carefully addressed since it is the unique block that
but is very frequent in the synthetic dataset, signalling affects a constrained time budget requirement.
the need to extend the training set to a wider range of
scenarios. Time Complexity: We detail the architecture adap-
The metrics pitched SReD promisingly against the tation under a given complexity budget. The choice
other four algorithms evaluated. The visual analysis of of the U-Net provides flexibility because it is possi-
the denoised data aligns well with the NMID metric ble to adapt its architecture under a predefined bud-
values, reinforcing its reliability. Although not optimal, get. The designs of network architectures should ex-
our method also performed very favourably in the syn- hibit tradeoffs among several of its components, , i.e.
thetic ground-truth dataset regarding MSE, PSNR, and depth, numbers of filters, and filter sizes, from which
SSI scores. The subpar results on this synthetic dataset the scalability is accomplished. From the above, depth
could be related to the a posteriori added synthetic is the most influential concerning the accuracy. Al-
noisebasedonaKinectv1noisemodel,whileSReDwas though it is not a straightforward observation, previous
trained on Kinect v2 noise. This discrepancy, limited by work [49,51] has demonstrated its impact. The total
the nonexistence of a usable v2 noise model implemen- time complexity of all the convolutional layers is given
tation, might explain the better results achieved in this as O(cid:16) (cid:80)d c .s2 .f .m2(cid:17) where d is the depth of
dataset by more general image restoration approaches l=1 l−1 l l l
the network, (i.e. the number of convolutional layers), l
not based on deep learning.
indexestheconvolutionallayer,c isthenumberofinput
In qualitative terms, upon visual inspection, SReD l
channels in the l-th layer, f is the number of filters in
achieved both consistent inpainting and denoising be- l
thel-thlayer,(i.e.,thewidth),s andm arethespatial
haviour and outperforms both deterministic approaches, l l
sizeofthefilterandthesizeoftheoutputofthefeature
namely the FMM+BF algorithm, particularly when fill-
map, respectively. The time cost of fully connected and
ingmissingareasandsharpeningobjectboundaries,and
pooling layers is not considered since these layers take
TV, as this last method over-smooths the overall depth
about 5-10% computational time. The time complexity
image, failing to preserve object details.
above is the basis of the network designs, from which
we consider the tradeoffs between the depth d and filter
sizes f , inferring how the network scales in time.
4.5 Real-Time Performance l
Concretely, we design a model by replacing the lay-
Finally, the implemented algorithm can produce de- ers in our experimental evaluation. This means that
noised frames at frequencies higher than 30 frames per when we replace a few layers with some other layers, we
second, thus making SReD suitable for real-time use. mustguaranteethatthecomplexityispreservedwithout
Indeed, on the computer hardware used for evaluation, changing the remaining layers in the architecture. To10 AlexandreDuarteet al.
Table 1 Benchmark of several metrics for all the tested methods: NMID (higher is better), temporal difference (lower is
better),PSNR(higherisbetter),MSEandSSI(lowerisbetter).Thelastthreemetricsarereportedforbothreal(CoRBS)and
synthetic (InteriorNet) datasets while the first two are only available for the synthetic dataset since this is the only one which
provides ground-truth depthdata.
CoRBS InteriorNet
NMID Temporal NMID Temporal PSNR MSE SSI
SelfReDepth 0.735 0.858 0.1611 0.0024 38.663 0.00036 0.830
Noise2Noise -0.154 1.085 0.1009 0.0039 39.020 0.00013 0.937
Noise2Stack -0.166 1.012 -0.0504 0.0038 37.582 0.00020 0.918
FMM+BF 0.735 0.926 0.2153 0.0032 43.753 0.00008 0.971
Total Variation 0.165 1.057 0.0129 0.0026 42.979 0.00006 0.980
design such a replacement, we progressively modify the – First Block: 2 x conv2D
32,1
modelandobservethechangesinaccuracy.Ourmethod – ith Down Block: 1 x conv2D + 1 x conv2D
Fi,2 Fi,1
addresses the following tradeoffs: – ith Up Block: 2 x conv2D + 1 x conv2D⊤
Fi,1 Fi,2
– Last Block: 2 x conv2D + 1 x conv2D
1. depth d and filter sizes s , 32,1 1,1
l
2. depth d and width f l, and We use five blocks for each Down and Up stage, thus
3. width f l and filter sizes s l. having i ∈ {1,...,5}. The number of filters for each
(cid:2) (cid:3) (cid:2) (cid:3)
block is F = F ... F = 3232484864128 , where
We illustrate one of the steps above, the remaining 0 5
F accounts for the filter in the First and Last blocks.
with an analogous procedure, only changing the corre- 0
Now, it is straightforward to determine convolutions.
sponding parameters accordingly. For instance, as an
Assuming an image size of W ×H, we have:
illustrativeofstep1(tradeoffbetweendepthdandfilter
size s), we replace a larger filter, say s , with a cascade
1 – First Block: 2·F ·W ·H
0
of smaller filters, say s 2. Denoting the layer configura- – Down Block i: 2·F ·W ·H ·2−2i
i
tion as above, L conf =c l−1.s2 l .f l and considering two – Up Block i: 3·F i·W ·H ·2−2i
instancesoffiltersizes,e.g.s =3s =2,andc =N,
1 2 l−1 – Last Block: (2·F +1)·W ·H
0
f l =N, we have the following complexities: – Total: (cid:16) 1+4F +5(cid:80)5 F ·2−2i(cid:17) W ·H
0 i=1 i
O =N2 .s2 =189.625W ·H
1 1
O =2.(N2 .s2)
2 2 This means that, for a ∆-increment in the image
resolution (W +∆)(H +∆), we have a complexity of
This replacement a s ×s layer with N input/output
1 1 O(∆2).So,roughlyspeaking,atwofoldincreaseinimage
channels is replaced by two s ×s layers with N in-
2 2
resolution would entail a fourfold increase in image
put/output channels. After the above replacement, the
processingtimeusingthesamearchitectureandmemory
complexity involved in these layers is nearly unchanged,
footprint. Assuming that in the worst case, 80% of the
with the reduction fraction of 2s2/s2 ≈1.
1 2 CPU time is spent on running the Neural Network, the
With the strategy above, we can “deepen” the net-
processing time per frame would be around 30ms for an
work under the same complexity time budget. This al-
effective frame rate of 30Hz, which is still reasonable.
lows us to obtain several architectures and pick the best
accuracy. In concrete, from our experimental evaluation
yielding the times mentioned in Section 4.5, we found
the best accuracy using the configuration in our final 5 Limitations
network, which has 31 layers, 1729 convolutional size
three filters, yielding 1 260 865 trainable parameters. While our technique has proven to be very effective at
restoring depth values from noisy RGB-D images, it
Scalability: Now, we delve into how the architecture can be improved in several ways. A notable limitation
scales with the size of input images. First, let us intro- involves adequately addressing high-frequency temporal
duce some basic notation: noise. While effective for general noise reduction, aver-
aging pixel values across frames falls short in capturing
– conv2D : 2D (contraction) convolution with F
F,st and mitigating these rapid fluctuations. This suggests
number of filters and with stride st
potential for future refinement. More sophisticated tech-
– conv2D⊤ : 2D (expansion) transpose convolution
F,st niques should be capable of discerning and smoothing
with F number of filters and with stride st
out high-frequency temporal noise without compromis-
Our U-Net network includes the following main blocks: ing the dynamic content of the scenes.SelfReDepth 11
6 Conclusions and future work Acknowledgements Theworkreportedin thisarticle was
partiallysupportedundertheauspicesoftheUNESCOChair
on AI & VR by national funds through Funda¸c˜ao para a
We introduced SelfReDepth, a self-supervised approach
Ciˆencia e a Tecnologia with references DOI:10.54499/UIDB/-
for denoising and completing low-quality depth maps
50021/2020,DOI:10.54499/DL57/2016/CP1368/CT0002and
generated from consumer-grade sensors. Our technique 2022.09212.PTDC(XAVIERproject).
advances self-supervised learning in-depth data denois- TheSelfReDepthsource codeis publiclyavailableat:
https://github.com/alexduarte23/sred
ing, offering a precise, data-driven architecture without
referencedata.ThisflexibilitymakesSelfReDeptheasily
adaptable across various environments and applications.
Conflict of interest
SelfReDepth’s architecture features two main ele-
ments: a denoising network and a target generation Theauthorsdeclarethattheyhavenoconflictofinterest.
component. The denoising network is inspired by the
originalNoise2Noise[27]andMF2F[13]videodenoisers
References
and is responsible for learning how to denoise depth
data without the need for reference data. Meanwhile,
1. Basso,F.,Menegatti,E.,Pretto,A.:Robustintrinsicand
the target generation component fills in the gaps in
extrinsiccalibrationofrgb-dcameras. IEEETransactions
targetdepthframesusingcolor-guidedFMMinpainting. onRobotics34(5),1315–1332(2018)
The technique can denoise inaccurate depth values and 2. Batson,J.,Royer,L.:Noise2self:Blinddenoisingbyself-
supervision. In: Proceedings of the 36th International
paint out missing ones with this structure.
ConferenceonMachineLearning,pp.524–533(2019)
We also implemented and assessed SelfReDepth for 3. Bertalmio,M.,Bertozzi,A.L.,Sapiro,G.:Navier-stokes,
both denoising efficacy and time performance. Results fluid dynamics, and image and video inpainting. In: Pro-
ceedingsoftheIEEEConferenceonComputerVisionand
indicate real-time noise elimination and successful in-
Pattern Recognition,vol.1, pp.I–I(2001)
paintingofdepthgaps.Futureworkwillfocusonpreserv-
4. Calvarons, A.F.: Improved noise2noise denoising with
ing image details compromised by denoising. Training limiteddata. In:IEEE/CVFConf.onComputerVision
withsyntheticdatamightalsoimprovedepthinpainting andPatternRecognitionWorkshops,pp.796–805(2021)
5. Capecci, M., Ceravolo, M.G., Ferracuti, F., Iarlori, S.,
performance and dampen oscillations.
Kyrki, V., Longhi, S., Romeo, L., Verdini, F.: Physical
In future work, we aim to explore controllable image rehabilitationexercisesassessmentbasedonhiddensemi-
denoising to generate clean sample frames with human markovmodelbykinectv2.In:IEEE-EMBSInternational
Conference on Biomedical and Health Informatics, pp.
perceptualpriorsandbalancesharpnessandsmoothness.
256–259(2016)
In most common filter-based denoising approaches, this
6. Cha,S.,Park,T.,Kim,B.,Baek,J.,Moon,T.:Gan2gan:
can be straightforwardly achieved by regulating the Generative noiselearningfor blinddenoisingwithsingle
filtering strength. However, for deep neural networks noisyimages. arXivpreprint:1905.10488(2019)
7. Chambolle,A.:Analgorithmfortotalvariationminimiza-
(DNN), regulating the final denoising strength requires
tionandapplications. Journalof Mathematicalimaging
performing network inference each time. This of course,
andvision20,89–97(2004)
hampersthereal-timeuserinteraction.Furtherworkwill 8. Chang,A.,Dai,A.,Funkhouser,T.,Halber,M.,Niessner,
addressreal-timecontrollabledenoising,tobeintegrated M., Savva, M., Song, S., Zeng, A., Zhang, Y.: Matter-
port3d:Learningfromrgb-ddatainindoorenvironments.
into a video denoising pipeline that provides a fully
2017International Conferenceon3DVision(2017)
controllable user interface to edit arbitrary denoising 9. Chaudhary, R., Dasgupta, H.: An approach for noise re-
levels in real-time with only one-time DNN inference. movalondepthimages. arXivpreprint:1602.05168(2016)
10. Chen, L., Lin, H., Li, S.: Depth image enhancement
SelfReDepth represents a significant advancement in
for kinect using region growing and bilateral filter. In:
data denoising, tackling noise and depth hole challenges ICPR2012,pp.3070–3073(2012)
with notable efficiency. The outcomes of our research 11. Choi,J.,Jung,D.,Lee,Y.,Kim,D.,Manocha,D.,Lee,D.:
are encouraging, illustrating the algorithm’s capacity to Selfdeco:Self-supervisedmonoculardepth completionin
challengingindoorenvironments.In:IEEEInt.Conference
mitigate these problems. However, the concomitant loss
onRoboticsandAutomation,pp.467–474(2021)
of certain image details in the process highlights areas 12. Dai,Y., Fu,Y.,Li, B.,Zhang,X.,Yu,T.,Wang, W.:A
forpotentialimprovement.Thisobservationunderscores new filtering system for using a consumer depth camera
the need for additional investigation while pointing to atcloserange. Sensors 19(16),3460(2019)
13. Dewil, V., Anger, J., Davy, A., Ehret, T., Facciolo, G.,
clear pathways for refining future algorithm iterations.
Arias, P.: Self-supervised training for blind multi-frame
Suchenhancementsaimtoimprovethebalancebetween videodenoising. In:IEEEWinterConferenceonApplica-
our denoising algorithm’s robustness and critical image tionsofComputerVision, pp.2724–2734(2021)
14. Essmaeel,K.,Gallo,L.,Damiani,E.,DePietro,G.,Di-
detail preservation, enhancing its already remarkable
panda, A.:Temporaldenoising ofkinect depth data. In:
efficiencyandmakingitmoreapplicabletoverycomplex
EighthIntl.ConferenceonSignalImageTechnologyand
scenarios. Internet BasedSystems,pp.47–52.IEEE(2012)12 AlexandreDuarteet al.
15. Feng, D., Rosenbaum, L., Dietmayer, K.: Towards safe 32. Liu, G., Reda, F.A., Shih, K.J., Wang, T.C., Tao, A.,
autonomous driving: Capture uncertainty in the deep Catanzaro,B.: Imageinpainting forirregularholesusing
neuralnetworkforlidar3dvehicledetection. In:201821st partialconvolutions. In:ComputerVision–ECCV2018,
International Conference on Intelligent Transportation pp.89–105(2018)
Systems,pp.3266–3273(2018) 33. Liu,J.,Gong,X.,Liu,J.:Guidedinpaintingandfiltering
16. Feng, Z., Jing, L., Yin, P., Tian, Y., Li, B.: Advancing for kinect depth maps. In: Proceedings of the 21st Int.
self-supervisedmonoculardepthlearningwithsparselidar. ConferenceonPatternRecognition,pp.2055–2058(2012)
arXivpreprint:2109.09628 (2021) 34. Liu, J., Liu, Y., Zhang, G., Zhu, P., Chen, Y.Q.: Detect-
17. Gabel, M., Gilad-Bachrach, R., Renshaw, E., Schuster, ing and tracking people in real time with rgb-d camera.
A.: Full body gait analysis with kinect. In: Annual Inter- Pattern RecognitionLetters53,16–23(2015)
35. Liu, S., Chen, C., Kehtarnavaz, N.: A computationally
nationalConferenceoftheIEEEEngineeringinMedicine
efficientdenoisingandhole-fillingmethodfordepthimage
andBiologySociety,pp.1964–1967(2012)
enhancement. In: Real-time image and video processing
18. Gao,Z., Yu,Y., Zhou,Y., Du,S.:Leveragingtwokinect
2016,vol. 9897,pp.235–243.SPIE(2016)
sensors for accurate full-body motion capture. Sensors
36. Ma, F., Cavalheiro, G.V., Karaman, S.: Self-supervised
15(9),24297–24317(2015)
sparse-to-dense: Self-supervised depth completion from
19. Handa,A.,Whelan,T.,McDonald,J.,Davison,A.J.:A
lidarandmonocularcamera. In:InternationalConference
benchmarkforrgb-d visual odometry,3dreconstruction
onRoboticsandAutomation,pp.3288–3295(2019)
andslam. In:IEEEinternationalconference onRobotics
37. Maimone,A., Bidwell,J.,Peng, K.,Fuchs,H.: Enhanced
andautomation,pp.1524–1531.IEEE(2014)
personal autostereoscopic telepresence system using com-
20. Jiang, L., Xiao, S., He, C.: Kinect depth map inpainting
modity depth cameras. Computers & Graphics 36(7),
using a multi-scale deep convolutional neural network.
791–807(2012)
In: Proceedings of the 2018 International Conference on 38. Mallick,T.,Das,P.P.,Majumdar,A.K.:Characterizations
ImageandGraphics Processing,pp.91––95(2018) ofnoise in kinectdepth images:A review. IEEESensors
21. Jorge, J., Anjos, R.K.D., Silva, R.: Dynamic occlusion journal14(6),1731–1740(2014)
handling for real-time ar applications. In: Proceedings 39. Metzler, C.A., Mousavi, A., Heckel, R., Baraniuk, R.G.:
ofthe17thInternationalConferenceonVirtual-Reality Unsupervisedlearningwithstein’sunbiasedriskestimator.
ContinuumandItsApplicationsinIndustry(2019) arXivpreprint:1805.10531(2018)
22. Kong, X., Li, K., Yang, Q., Wenyin, L., Yang, M.H.: A 40. Mohan, S., Vincent, J.L., Manzorro, R., Crozier, P.,
new image quality metric for image auto-denoising. In: Fernandez-Granda, C., Simoncelli, E.P.: Adaptive denois-
IEEEInternationalConferenceonComputerVision,pp. ingviagaintuning. In:Thirty-FifthConferenceonNeural
2888–2895(2013) InformationProcessingSystems(2021)
23. Krull,A.,Buchholz,T.O.,Jug,F.:Noise2void-learning 41. Moran, N., Schmidt, D., Zhong, Y., Coady, P.: Nois-
denoisingfromsingle noisyimages. In:IEEE/CVFCon- ier2noise:Learningto denoisefromunpairednoisydata.
ferenceonComputerVisionandPatternRecognition,pp. In: IEEE/CVFConferenceon ComputerVision andPat-
2124–2132(2019) ternRecognition,pp. 12061–12069(2020)
24. Krull,A.,Viˇcar,T.,Prakash,M.,Lalit,M.,Jug,F.:Proba- 42. Newcombe,R.A.,Izadi,S.,Hilliges,O.,Molyneaux,D.,
bilisticnoise2void: Unsupervisedcontent-aware denoising. Kim, D., Davison, A.J., Kohi, P., Shotton, J., Hodges,
FrontiersinComputerScience2,5(2020) S., Fitzgibbon, A.: Kinectfusion: Real-time dense surface
25. Kweon, I.S., Jung, J., Lee, J.Y.: Noise aware depth de- mappingandtracking. In:10thinternationalsymposium
noising for a time-of-flight camera. In: 20th Korea-Japan onmixedandaugmentedreality,pp.127–136(2011)
43. Oyedotun, O.K., Demisse, G., El Rahman Shabayek, A.,
JointWorkshoponFrontiersofComputer Vision(2014)
Aouada,D.,Ottersten,B.:Facialexpressionrecognition
26. Lachat, E., Macher, H., Landes, T., Grussenmeyer, P.:
via joint deep learning of rgb-depthmap latentrepresen-
Assessment and calibration of a rgb-d camera (kinect
tations. In:IEEEInternationalConferenceonComputer
v2 sensor) towards a potential use for close-range 3d
VisionWorkshops,pp.3161–3168(2017)
modeling. RemoteSensing 7(10),13070–13097(2015)
44. Papkov, M., Roberts, K., Madissoon, L.A., Shilts,
27. Lehtinen,J.,Munkberg,J.,Hasselgren,J.,Laine,S.,Kar-
J., Bayraktar, O., Fishman, D., Palo, K., Parts, L.:
ras,T.,Aittala,M.,Aila,T.:Noise2noise:Learningimage
Noise2stack:Improvingimagerestorationbylearningfrom
restorationwithoutcleandata. In:InternationalConfer-
volumetricdata. In:Intl.WorkshopMachineLearningfor
ence on Machine Learning, pp. 2965–2974.PMLR (2018)
MedicalImageReconstruction, pp.99–108(2021)
28. Lemarchand,F.,Findeli,T.,Nogues,E.,Pelcat,M.:Noise-
45. Quan, Y., Chen, M., Pang, T., Ji, H.: Self2self with
breaker:Gradualimagedenoisingguidedbynoiseanalysis.
dropout: Learning self-supervised denoising from single
In: IEEE 22nd International Workshop on Multimedia
image. In: IEEE/CVF Conference on Computer Vision
SignalProcessing,pp.1–6(2020)
andPattern Recognition,pp.1887–1895(2020)
29. Li, A., Yuan, Z., Ling, Y., Chit, W., Zhang, S., Zhang, 46. Ren,Z.,Yuan,J.,Zhang,Z.:Robusthandgesturerecog-
C.: Fastcompletion: A cascade network with multiscale nition based on finger-earth mover’s distance with a com-
group-fused inputs for real-time depth completion. In: modity depthcamera. In: Proceedings ofthe 19th inter-
25thInternationalConferenceonPatternRecognition,pp. nationalconferenceonMultimedia,pp.1093–1096(2011)
866–872(2021) 47. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolu-
30. Li,L.,Wu,H.,Chen,Z.:Depthimagerestorationmethod tional networks for biomedical image segmentation. In:
basedonimprovedfmmalgorithm. In:202113thInterna- International Conference on Medical image computing
tional Conference onMachine Learningand Computing, andcomputer-assistedintervention,pp.234–241(2015)
ICMLC2021,pp.349–355(2021) 48. Sheth, D.Y., Mohan, S., Vincent, J.L., Manzorro, R.,
31. Li,W.,Saeedi,S.,McCormac,J.,Clark,R.,Tzoumanikas, Crozier,P.A.,Khapra,M.M.,Simoncelli,E.P.,Fernandez-
D.,Ye,Q.,Huang,Y.,Tang,R.,Leutenegger,S.:Interior- Granda, C.: Unsupervised deep video denoising. In: Pro-
net:Mega-scalemulti-sensorphoto-realisticindoorscenes ceedingsoftheIEEE/CVFInternationalConferenceon
dataset. arXivpreprint:1809.00716 (2018) ComputerVision,pp. 1759–1768(2021)SelfReDepth 13
49. Simonyan, K., Zisserman, A.: Very deep convolutional
networks for large-scale image recognition. arXiv
preprint:1409.1556(2014)
50. Song,W.,Le,A.V.,Yun,S.,Jung,S.W.,Won,C.S.:Depth
completion for kinect v2 sensor. Multimedia Tools and
Applications 76(3),4357–4380(2017)
51. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,
Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich,
A.: Going deeper with convolutions. In: Proceedings of
the IEEE conference on computer vision and pattern
recognition,pp.1–9(2015)
52. Tassano,M.,Delon,J.,Veit,T.:Fastdvdnet:Towardsreal-
time deep video denoising without flow estimation. In:
IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pp.1354–1363(2020)
53. Telea, A.: An image inpainting technique based on the
fast marching method. Journal of Graphics Tools 9(1),
23–34(2004)
54. T¨olgyessy, M., Dekan, M., Chovanec, L., Hubinsky`, P.:
Evaluation of the azure kinect and its comparison to
kinectv1andkinectv2. Sensors21(2),413(2021)
55. Tomasi,C.,Manduchi,R.:Bilateralfilteringforgrayand
colorimages.In:6thinternationalconferenceoncomputer
vision(IEEECat. No.98CH36271),pp.839–846(1998)
56. Van der Walt, S., Sch¨onberger, J.L., Nunez-Iglesias, J.,
Boulogne, F., Warner, J.D., Yager, N., Gouillart, E., Yu,
T.: scikit-image: image processing in python. PeerJ 2,
e453(2014)
57. Wan, Y.,Li,Y., Jiang,J.,Xu,B.: Edgevoxelerosion for
noise removal in 3d point clouds collected by kinect©.
In:Proceedingsofthe20202ndInternationalConference
onImage,VideoandSignalProcessing,pp.59–63(2020)
58. Wasenmu¨ller, O., Meyer, M., Stricker, D.: Corbs: Com-
prehensive rgb-dbenchmark for slamusing kinect v2. In:
IEEE Winter Conference on Applications of Computer
Vision,pp. 1–7 (2016)
59. Xiong, W., Yu, J., Lin, Z., Yang, J., Lu, X., Barnes,
C., Luo, J.: Foreground-aware image inpainting. In:
IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pp.5833–5841(2019)
60. Yu,J.,Lin,Z., Yang,J., Shen,X., Lu,X., Huang,T.S.:
Generativeimageinpaintingwithcontextualattention.In:
IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pp.5505–5514(2018)
61. Zennaro, S., Munaro, M., Milani, S., Zanuttigh, P.,
Bernardi, A., Ghidoni, S., Menegatti, E.: Performance
evaluationofthe1stand2ndgenerationkinectformul-
timedia applications. In: IEEEInternational Conference
onMultimediaand Expo,pp.1–6(2015)
62. Zhang, B., Allebach, J.P.: Adaptive bilateral filter for
sharpnessenhancementandnoiseremoval. IEEEInterna-
tionalConferenceon ImageProcessing 4,417–420(2007)
63. Zhang, X., Yan, J., Feng, S., Lei, Z., Yi, D., Li, S.Z.:
Waterfilling:Unsupervisedpeoplecountingviavertical
kinect sensor. In: IEEE 9th intl. conference on advanced
videoandsignal-basedsurveillance,pp.215–220(2012)
64. Zhou,X.:Astudyofmicrosoftkinectcalibration.Dept.of
Comp. Science, George Mason University, Fairfax (2012)