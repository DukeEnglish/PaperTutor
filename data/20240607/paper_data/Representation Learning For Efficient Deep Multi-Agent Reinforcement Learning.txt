Representation Learning For Efficient Deep
Multi-Agent Reinforcement Learning
DomHuh1 PrasantMohapatra1,2
1UCDavis 2UniversityofSouthFlorida
dhuh@ucdavis.edu1
pmohapatra@usf.edu2
Abstract
Sampleefficiencyremainsakeychallengeinmulti-agentreinforcementlearning
(MARL). A promising approach is to learn a meaningful latent representation
space through auxiliary learning objectives alongside the MARL objective to
aid in learning a successful control policy. In our work, we present MAPO-
LSO(Multi-AgentPolicyOptimizationwithLatentSpaceOptimization)which
appliesaformofcomprehensiverepresentationlearningdevisedtosupplement
MARLtraining. Specifically,MAPO-LSOproposesamulti-agentextensionof
transitiondynamicsreconstructionandself-predictivelearningthatconstructsa
latentstateoptimizationschemethatcanbetriviallyextendedtocurrentstate-of-
the-artMARLalgorithms. EmpiricalresultsdemonstrateMAPO-LSOtoshow
notableimprovementsinsampleefficiencyandlearningperformancecompared
toitsvanillaMARLcounterpartwithoutanyadditionalMARLhyperparameter
tuningonadiversesuiteofMARLtasks.
1 Introduction
A multi-agent control system consists of multiple decision-making entities within a shared envi-
ronment, each tasked with achieving some objectives defined by a reward signal. Multi-agent
reinforcement learning (MARL) offers a learning paradigm that optimizes for emergent rational
behaviorswithinagentsthroughinteractionswiththeenvironmentandoneanothertoachievean
equilibrium[23]. Inrecentyears,deepMARLhasprovensuccessfulinnumerousdomains,including
roboticsteams[22],networkingapplications[38],andvarioussocialscenariosthatrequiremulti-
agentinteractions[3]. However,deepreinforcementlearning(RL)hashistoricallysufferedfrom
sampleinefficiency,requiringacostlyamountofinteractionexperiencetolearnvaluablebehaviors.
ThischallengestemslargelyfromthehighvarianceinexistingRLalgorithmspairedwiththedata-
intensivenatureofdeepneuralnetworks[46]. Unfortunately,MARLapplicationsfaceadditional
learningpathologiesandcomplexities[36]suchasexponentialcomputationalscalingwithrespectto
thenumberofagentsandthedynamicchallengeofequilibriumcomputation[8].
Toremedythisissue,recentMARLeffortshaveconcentratedontheconceptofcentralizedtraining
anddecentralizedexecution(CTDE)[34;29;53]. InCTDE,agentsaretrainedwithaccesstoglobal
state information while retaining autonomy, meaning the agents can make decisions based only
onlocalinformationduringexecution. DespitetheempiricalimprovementsfromCTDE,sample
inefficiencyremainsanelusivechallenge. WearguethattheCTDEparadigmdoesnotfullyaddress
theunderlyinglimitationsofRLalgorithms,i.e. thesparsityandvarianceofitslearningsignals.
A natural solution to address this issue is to curate additional learning signals that supplement
andenrichtheRLlearningprocess. Thisapproachofimposingfurtherinductivebiashasproven
effectiveinpriorworksatenhancingthetrainingofcontrolpoliciesinsingle-agentRL[24]. Thenew
objectivesthatareintroducedrangefromreinforcingsimilaritiesanddissimilaritieswithintemporal
Preprint.Underreview.
4202
nuJ
5
]AM.sc[
1v09820.6042:viXraFigure1:Ahigh-levelillustrationoftheMAPO-LSOframework.Foreachagenti={0,...,N},the
encoders( )embedtheirobservationsoi andpropagatestheirencodingsthroughacommunication
t
block( )thatissubjecttoacommunicationnetworkG(s ). Oncetheagentscommunicate,the
t
latentstatezi( )iscomputedandusedasinputsforitspolicy( )andvaluefunction( ). Forour
t
MA-LSOprocedure,thelatentstatesareoptimizedusingMA-TransitionDynamicsReconstruction
(MA-TDR)andMA-Self-PredictiveLearning(MA-SPL).Thesetwolearningprocessesareoutlined
inSection4andlooselycanbethoughtofasinstillingthecapabilityofinferringtheobservations
andthenextlatentstatesofallagentsfromthecurrentlatentstate.
orspatiallocality[30;44]toinstillinginformationregardingdifferentaspectsofthetasks,suchas
thetransitiondynamics[39],intothelatentstatespace. Importantly,themaintakeawayfromthese
effortsistolearnarichlatentstatespacethatunderstandsandiscoherentwiththetaskdynamicsand
itself[35]. However,muchofthesetechniquesofrepresentationlearninghasyettobefullyrealized
andextendedtoaMARLcontext.
Inthiswork,weproposeMAPO-LSO(Multi-AgentPolicyOptimizationwithLatentSpaceOptimiza-
tion),ageneralizedMARLframework,outlinedinFigure2,thatleverageslatentspaceoptimization
(LSO)inamulti-agentsettingundertheCTDEframework. Specifically,weshowthatcurrentstate-
of-the-artMARLalgorithms,suchasMAPPO[53],HAPPO[29],MASAC[17],andMADDPG
[34]benefitfromourmulti-agentLSO(MA-LSO)learningprocesswithtrivialmodifications. Our
experimentsdemonstratesignificantimprovementsinnotonlythesampleefficiencybutalsointhe
overallperformanceover18diversetasksinVMAS[1]and5roboticteamtasksinIsaacTeams[22]
overallalgorithmsunderfixedmodelarchitecturesandMARLhyperparameterssetting.
Ourcontributionsareasfollows:
1. WeintroduceanovelMARLframework,MAPO-LSO,thatintegratesMA-LSO,acompre-
hensiveformofrepresentationlearningintotheMARLtraining. MA-LSOisbrokendown
intotwoparts: MA-TransitionDynamicsReconstructionandMA-Self-PredictiveLearning.
Hence,weprovideanewperspectiveontheintuitionbehindtheusageandintegrationof
bothlearningprocessesinamulti-agentcontrolsetting.
2. Westudytheapplicationofpretraining,uncertainty-awaremodelingtechniquesforagent-
modelingandphasicoptimizationwithinourMAPO-LSOframeworktoimprovelearning
performance,specificallyintermsofconvergenceandstability.
3. Weextendandexperimentusingseveralstate-of-the-artMARLalgorithmsonourMAPO-
LSOframeworkonavarietyoftaskswithdiversenatureofinteractionsandmulti-modal
data,presentingfurtherablationstudiesondesignchoicestoshowcasetheimprovementsof
ourMAPO-LSOframework.
22 RelatedWorks
Sample-EfficiencyinMARL Anumberofrecentworkshaveaddressedthesampleefficiency
problemindeepMARLrangingfromdevelopingvectorizedandparallelizablesimulationplatforms
[22;1],improvingexplorationstrategiestocollectdiverseandusefulsamples[32],pre-trainingona
datasetofdemonstrations[37],utilizingoff-policyand/ormodel-basedapproaches[33],andlearning
onofflinedatasets[52]. Whiletheseprioreffortsarenotnecessarilyorthogonaltoourefforts,the
focusofthispaperisonintroducingaformofmulti-agentrepresentationlearningthatimproveshow
muchislearnedfromeachsamplebyguidingtheoptimizationofthelatentstatespaceforMARL
tasks.
RepresentationLearninginMARL Theconceptofrepresentationlearninghaspreviouslybeen
appliedinMARLapplicationsthroughmaskedobservationreconstruction[27;43],auxiliarytask-
specificpredictions[41],self-predictivelearninginjointlatentspace[10],andcontrastivelearning
on the observation embedding space [21]. In our study, our proposed MA-LSO takes a more
comprehensivemeasurebyapplyingtwoformsofrepresentationlearningthatenforceconsistency
between the latent state space and the transition dynamics and within itself as a self-predictive
representationspace.
3 Preliminaries
Inthiswork,weconsideranextensionofthestochasticgameframework[42]calledthenetworked
Bayesiangame[20;23].
Definition1 AnetworkedBayesiangameisdefinedbyatuple⟨I,S,O,A,T,R,ω,G⟩.
• I ={0,...,N}isthesetofN agents.
• S istheglobalstatespace.
• O = (cid:81) Oiisthejointobservationspace,whereOiistheobservationspaceofagenti.
i∈I
• A= (cid:81) Aiisthejointactionspace,whereOiistheactionspaceofagenti.
i∈I
• T :S×A(cid:55)→P(S)isthestatetransitionoperator,mappingthestate-actionspacetothe
probabilityofthenextstates.
• R= (cid:81) Ri,isthejointrewardfunction,whereRi :S×A(cid:55)→Ristherewardfunctionfor
i∈I
agenti.
• ω = (cid:81) ωiisthejointtype/beliefspace,whereωiisthebeliefspaceofagenti.
i∈I
• G : S (cid:55)→ I ×I isthe mappingfromthestate toanadjacency matrixthatdefinesthe
communicationgraphbetweenallagents.
We optimize for the Bayes-Nash equilibrium, where each agent i learns a best-response policy
πi :Oi×ωi (cid:55)→Ai,bymaximizingtheexpectedexintermreturnofindividualagentGi.
(cid:88)
∀i∈I,Gi =E [ Ri(s ,a )]whereτ ={s ,a ,...} (1)
τ∼T πi t t 0 0
t=0
DeepReinforcementLearning ThefieldofdeepRLpresentsgeneralcontroloptimizationalgo-
rithmsusingdeepneuralnetworkapproximations: canonicallyexistingintheformofQ-learning,
policygradient,andactor-criticmethods[46]. WithQ-learningapproaches,theoptimalstate-valueor
action-valuefunctionQπ∗(s,a)islearned,whereQπ(s,a)mapsthestate-actionpairtoitsexpected
returnfollowingapolicyπ. Policygradientmethodsdirectlyoptimizethepolicyviagradientascent
overtheexpectedreturn. Actor-criticmethodsstabilizethepolicygradientbyapproximatingthe
3offsetreinforcementwithalearnedQ-functionunderthecurrentpolicy. Theseoptimizationschemes
havebeenextendedtoandstudiedunderamulti-agentcontext,demonstratingpromisingresults[53].
LatentSpaceOptimization Latentspaceoptimization(LSO)isaformofrepresentationallearning
that is often used in unison with generative modeling [54], where LSO leverages model-based
optimizationthatlearnsanapproximationoftheobjectivefunctionunderalearnedlatentspace[48].
InRL,LSOisoftenusedtomapvariousaspectsoftheenvironmentmodelintoalatentspaceto
assistwiththeRLtraining[18]. Inthiswork,weexplorethispretensewithinaMARLsetting.
4 Multi-agentLatentSpaceOptimization
Ourapproach,MA-LSO,optimizesalatentstaterepresentationzi ∈ Zi foreachagenti ∈ I to
t t
supplement the “sample-inefficient" MARL optimization. To achieve this goal, we employ two
processes: MA-TransitionDynamicsReconstruction(MA-TDR)andMA-Self-PredictiveLearning
(MA-SPL).Theseprocessesdrawinspirationfrompreviousworkonsingle-agentmodel-basedRL
[18]andrepresentationallearningmethodsforRL[39;15;16],unifyingtheconceptsofTDRand
SPLinamannerthatcomplementsoneanotherwhileconsideringthemulti-agentnatureofthetask.
4.1 MA-TransitionDynamicsReconstruction
MA-TDRlearnsanapproximationofthetransitiondynamicsoftheenvironmentbymappingthe
underlying“true"statetoalatentstaterepresentationspaceZ ,groundingZ totherealitiesofthe
t t
task’sdynamics. Toimplementthis,wemakeuseofrecurrentmodelingandmulti-agentpredictive
representationlearning(MA-PRL).
RecurrentModeling Foreachagenti,wemaintain
arecurrentstatehi thatholdsinformationregarding
t
its history and is realized using a recurrent neural
networkRNN.
hi = RNNi( zi ,ai ;hi )
t t−1 t−1 t−1
(cid:74) (cid:75)
From the observation oi, an encoder computes an
t
embeddingtobepassedintoacommunicationblock
togenerateei. ThelatentstatespaceZiiscomputed
t t
usingamulti-layerperceptronMLPthatprocessesthe
embeddingei andtherecurrentstatehi.
t t
ei =CommunicationBlock(Encoderi(oi)|G(s ))
t t t
zi ∼Zi =P(MLPi( ei,hi ))
t t z t t
(cid:74) (cid:75)
where Zi is a mixture of categorical distributions
t
[19]. The purpose of this recurrent modeling is to
ensurethatthelatentstateisexpressiveenoughsuch
that it is sufficient to recollect information needed
fordecision-makingfromtheagent’shistoryandcan
Figure2: AdetailedvisualizationoftheMA-
tractablyperformtheotherauxiliarytasksposedin
TDRmodelingprocedurewiththeauxiliary
MA-PRLandMA-SPL.
modulesusedtoreconstructtransitiondynam-
icsforrecurrentmodelingandMA-PRL.
MA-PredictiveRepresentationLearning InMA-
PRL, we take explicit measures to ensure that the
latentstatez containsinformationregardingthetransitiondynamicsbyreconstructingandinferring
t
variousaspectsofthetransitiondynamics–namely,theobservationo ,rewardr andterminationd
t t t
–fromthelatentstatez .
t
Firstly,MA-PRLincorporatesCURL[30],acontrastivelearningframeworkthatguidesthelatent
stateziproducedbyoi tobesimilartothezˆiproducedbyanaugmentedoˆi.
t t t t
Next,wetaskeachagentitomaintainabeliefoveritsownaswellastheotheragents’observations,
policies,rewards,andtermination. Thesebeliefsarecomputedasafunctionoftheirlatentstatezi.
t
4Toensurethefeasibilityofthesebeliefs,weexperimentwithMonte-Carlodropouttoaddressthe
inherentepistemicuncertainty[12;28].
ωi(oj t)=Decoderi,j(z ti) ωi(aj t)= MLPi a, cj t(z ti) ωi(r tj)= MLPi re,j w(z ti) ωi(cj t)= MLPi c, oj nt(z ti)
wherecj =(1−dj)isthecontinuesignalforagentj. Intermsofourimplementation,weadhereto
t t
thesameprotocolssetin[19],approximatingtherewardusingasymlogtwohotdistributionandthe
continuesignalusingonehotdistribution. Additionally,wetemporally-smoothedtherewardsignals
withGaussian-smoothingtoeasethetaskofrewarddistributionapproximation[31].
Thecombinationofthetwoconcepts,recurrentmodelingandMA-PRL,makesuptheMA-TDR
process. TheoveralllossforMA-TDRisdefinedas:
. (cid:88) exp(s(zˆi,zi))
L =E [ + t t −lnP(ωi(oj)=oj)
tdr (ot,at,rt,ct)∼D (cid:80) exp(s(zˆi,zk)) t t
i,j∈I
k∈I
t t (cid:124) obsl(cid:123) o(cid:122)
gloss
(cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
CURLloss
+H(ωi(aj),aj)−lnP(ωi(rj)=rj)−lnP(ωi(cj)=cj)] (2)
t t t t t t
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
actionloss rewardlogloss continuelogloss
wheres(·)isthesimilaritymeasureadoptedfrom[30],Disanexperiencereplaybuffer,H(·)isHuber
lossandsg(·)isthestop-gradientoperator. Foreachlossterm,weappendascalinghyperparameter
toeachlosstermtoavoiddominatinggradientsandgeneralperformancereasons.
4.2 MA-Self-PredictiveLearning
ThedesideratumofMA-SPListolearnaZ thatissufficienttopredicttheexpectedZ [13;45].
t t+1
Intuitively,thelearnedlatentstatespaceisoptimizedtobeconsistentwithitselfanditsownlatent
dynamics[47]. Moreover,weextendtheconceptofSPL[39]toamulti-agentsetting,wherenow,we
considerthepresenceofotheragentsintheenvironmentandtherebyenforceastructuralrelation
[49]andconsistencyamongsttheagentsinacentralizedmanner.
MA-MaskedReconstruction(MA-MR) Inspiredby[27;43],MA-MRencouragesinter-predictive
representationbetweenagents’latentspace. Similarto[51],MA-MRtreatstheagents’latentstates
asasequence. Concretely,MA-MRutilizesacontrastivelearningparadigmsuchthatamaskedjoint
latentstatecanreconstructthejointlatentstatez . Thismaskingprocessisappliedontheagent-level.
t
Hence,ifthelatentstateofagentiismaskedm (z
),thejointlatentspacesoftheotheragentszI\i
i t t
issufficienttoreconstructzi. Inourimplementation,weadopttheframeworkfrom[43],usinga
t
self-attentivereconstructionmodelR(·)toprocessthemaskedlatentstateasshowninFigure3.
z¯ =R(m (z ))
t i t
Figure3: ThethreeMA-SPLsubprocessesofMA-MR,MA-FDMandMA-IDMareshown.
5MA-ForwardDynamicsModeling(MA-FDM) TheobjectiveofMA-FDMistoensurethatthe
informationcontainedinthecurrentjointlatentstatez andthejointactiona issufficienttoinfer
t t
thenextjointlatentstatez [10]. Toimplementthis,wedefineatransitionheadT (·)whichis
t+1 z
realizedusingacross-attentionhead[50]thatmapsthejointlatentstateandthejointactiontothe
nextjointlatentspace.
z¯ =T (z ,a )
t+1 z t t
MA-InverseDynamicsModeling(MA-IDM) MA-IDMaimstoachievethefollowingobjective:
Giventhecurrentandnextjointlatentstate, thejointactionthatrealizedthattransitionfromthe
currenttothenextjointlatentstatecanbededuced. Inourwork,weuseaninverseheadI(·)which
isrealizedusingaself-attentivemodelthatmapsthecurrentandnextjointlatentstatetothejoint
actionspace.
a¯ =I(z ,z )
t t t+1
TheoveralllossforMA-SPLisdefinedas:
. (cid:88) exp(s(z¯i,zi))
L
spl
=E (ot,at,ot+1)∼D[
i∈I
(cid:80)
exp(st
(z¯
ti,t
z
tj))+
(cid:124)
H(z¯
t+
(cid:123)(cid:122)1,z
t+1
(cid:125))+
(cid:124)
H( (cid:123)a¯ (cid:122)t,a
t
(cid:125))] (3)
j∈I MA-FDMLoss MA-IDMLoss
(cid:124) (cid:123)(cid:122) (cid:125)
/L
MA-FDM
/L
MA-IDM
MA-MRLoss/L
MA-MR
MLPHeads Followingrecentworksoncontrastivelearningframeworks[5;4],weintroduceMLP
projectionheadsforCURL,MA-MR,MA-FDM,andMA-IDMlearningprocesses. Moreover,we
adoptamomentum-likeupdatesimilartothesepriorefforts. ThisadditionisshowninFigure3for
MA-MR,MA-FDM,andMA-IDM.
4.3 IntegratingMA-LSOtoMulti-agentPolicyOptimization
Ourproposedapproach,MA-LSO,caneasilybeappendedtopopularMARLalgorithmswithminor
adjustmentstoformMAPO-LSO.Thecentralchallengeistheuseofrecurrentmodeling,whichraises
severalimplementationchallengesforsomealgorithms[26]involvingadjustmentsintheexperience
replaybufferDandmaintenanceoftherecurrentstate. Otherwise,appendingtheMA-LSOlearning
processistrivialandcanbeperformedconcurrentlywiththeMARLtraining.
On-policy MARL In general, we define a shared replay buffer D that we sample batches of
transitionsfromtocomputebothMA-LSOandMARLobjectives. However,foron-policyMARL
algorithmsthatcannotlearnontheofflinedata,wefoundthatlearningonofflinedataduringthe
MA-LSOprocessisnecessarytopromotegoodgeneralizationandstablelearningbylearningona
diversedataset. Therefore,weensurethatwemaintainbothon-policyandoff-policydatainDsuch
thatonlinedataisusedfortheMARLtrainingbutoff-policydataisstillavailablefortheMA-LSO
process.
PhasicOptimization ForcertainMARLalgorithms,notablyMADDPGandMASAC,wechoseto
followthetrainingmethodologyoutlinedin[11]. Thisinvolvedtheutilizationoftargetnetworksand
delayedpolicyupdates. Thesetechniquesareintendedtomitigatethelearningvarianceandenhance
overallperformance. However,despitetheseefforts,westillobservedthatthetrainingremainedtoo
sensitivetohyperparameters,likelyduetotheuseofamodelarchitecturethatsharesparameters
betweenthepolicyandvaluefunction(i.e. theencoder)andthephasicnatureoflearning.
Tomitigatethisinstability,werecognizedtheneedtoincorporateaphasicregularizationterminspired
bytheworkof[6]. Thisregularizationtermconstrainsthepolicydivergenceduringallnon-policy
updatesandtherebypromotesamorestablelearningenvironment. ForHAPPO,wealsoenforce
thisregularizationtermduringthesequentialpolicyupdatessuchthatthesharedencoder, which
existswithinthecentralizedcritic,doesnotdivergefromtheotheragents’behaviorsthatarenot
beingupdated. Inourstudy,insteadofusingaKLdivergence,weuseHuberlosstoconstrainthe
divergenceofactions(i.e. ofthepolicies)utilizingtheoldandnewencoders.
Pre-training TheMA-LSOobjectivecanbeusedasapre-trainingparadigmsimilarto[40],where
Dispre-filledwithanexploratory/randompolicyoftransitionsandistrainedonL andL .
tdr spl
65 Experiments
For our experiments, we use the tasks in Vectorized Multi-agent Simulator (VMAS) tasks and
IsaacTeams(IST)toprovideacomprehensiveevaluationofadiversecollectionofmulti-agenttasks,
selecting18diversetasksfromVMASand5tasksfromISTasshowninAppendixA.
ExperimentalSetup ThescenariosparametersforVMASandISTenvironmentsaretakenfrom
priorworks[1;2;22]. ThefourMARLalgorithmschosenforourexperimentsareMAPPO[53],
HAPPO[29],MASAC[17],andMADDPG[34];allofwhichareconsideredcompetitiveMARL
baselines. Forallexperiments,theMARLhyperparametersareinitiallytunedusingarandomsearch
for the vanilla MARL algorithms (i.e. without MA-LSO), then kept fixed and trained with our
MAPO-LSOmethodforthatspecifictask. FurtherimplementationdetailsareprovidedinAppendix
CandD.Allexperimentspresentedinthisworkwereexecutedon3NvidiaRTXA6000andIntel
XeonSilver4214R@2.4GHz.
5.1 Results
Inthissection,weevaluatetheoverallperformanceandsampleefficiencyofMAPO-LSOpairedwith
popularMARLalgorithms. Here,performancereferstothecollectivereturnachievedandthesample
efficiencyismeasuredbytheperformancewithrespecttothenumberofdatasamplesused,meaning
thebettertheperformanceatagivennumberofenvironmenttransitionslearnedon,thehigherthe
sampleefficiency. Weadditionallyconductfurtherablationstudiestoinvestigateandanalyzeeach
componentofourMAPO-LSOmethodandstudyifanyotherimprovementsordegradationsare
realizedatamoregranularlevel.
Toprovideaconcisecomparisonagainstourmethod,wepresentmuchofourresultsinanormalized
scale. Thisinvolvesaggregatingandscalingtheresultsfromeachexperiment,algorithm,andtask
[7;14].
EfficacyofMAPO-LSO AsdepictedinFigure4a,theMAPO-LSOframeworkdemonstratesa
significantimprovementinthecollectivereturn, reachinga+35.68%differenceinconvergence
fromthebaselinewithoutMA-LSO.Additionally,intermsofsampleefficiency,ourMAPO-LSO
achievedthemaxconvergenceofthebaselinein285.7%lesssamples. However,toachievethis,we
discussthedesignchoicesmadethatenabledthisimprovement.
5.1.1 DesignChoicesinMAPO-LSO
PhasicOptimization WeconfirmourhypothesisstatedinSection4.3withFigure4b,wherewe
found training inefficiencies with a shared encoder between the actors and critics in the MARL
algorithms (i.e. HAPPO, MADDPG and MASAC) with phasic learning. Aforementioned, this
concernisnotnovel[6]andinourwork,weaddressedthisissuethroughphasicregularizationwith
(a)MAPO-LSO (b)PhasicRegularization (c)UncertaintyModeling
Figure 4: The graphs compare the collective returns under a normalized scale between various
componentsintroducedinthiswork—namely,MAPO-LSO,phasicregularization,anduncertainty
modeling(U.M.)—overallVMASandISTtasksandMARLalgorithms,exceptforFigure4b,which
normalizesoverHAPPO,MADDPGandMASAC.Theerrorbarsindicate±1stddeviations. The
resultsfortheindividualrunsofallexperimentsareprovidedinAppendixE,FandHrespectively.
7(a)BeliefSpace (b)MA-TDRLosses
Figure 6: Left: A comparison of normalized collective returns between MAPO-LSO with and
withoutuncertaintymodeling(U.M.),wheretheactionsinferredbyeachagent’sbeliefspaceare
used. Wenormalizethesumofthecollectivereturnsoverallagent’simaginedjointpoliciesonthe
VMASandISTbenchmarks. Right: ThefourgraphsshowthenormalizedMA-TDRlossesbetween
MAPO-LSOusingandnotusinguncertaintymodeling. Forbothplots,theerrorbarsindicate±1std
deviationsoverallruns.
significantimprovements. Moreover,weencouragefutureworkstoexplorefurthermethodologies
thatcanfacilitateasharedencoderparadigm,aswedidfindthattherobustnessofhyperparameters
canstillbeimprovedupon.
EpistemicUncertaintyModeling ReferringtoFigure
6b,theuncertaintymodelingwithintheMA-TDRheads
demonstratesimprovementsintheaccuracyofthebeliefs
ofobservation,action,rewardandcontinuesignals,most
notablyhavingthelargestimpactontheaccuracyofin-
ferring the actions. Furthermore, we evaluate the imag-
inedpoliciesrealizedwithineachagent’sbeliefspaceby
rollingouttrajectoriesusingthejointactionswithinthe
agent’s belief space. We find the uncertainty modeling
doesinfluencethebehaviorslearnedwithineachagent’s
beliefspaces, asshowninFigure6a, exhibitingimpres-
siveperformanceevenusingtheseimaginedjointpolicies.
Unsurprisingly,thisuncertaintymodelingalsoimproved
theexpectedcollectivereturnaswell,asseeninFigure4c. Figure5: MAPO-LSOasapre-training
Infutureworks,afurtherevaluationandstudyintothedi- processisevaluated,normalizedonall
versityandsocialbehaviorslearnedwithintheseimagined runslistedinAppendixGwiththeerror
jointpolicieswouldbefruitful. barsshowingthe±1stddeviation.
Pre-training We study the efforts of using MA-TDR
andMA-SPLobjectivesasapre-trainingprocess. First,wecollectedadatasetof10Ktrajectories
usingarandompolicyandpre-trainedthemodelontheMA-LSOobjectivesfor100epochs. As
showninFigure5,wefindthattheinclusionofpre-trainingprovidesanimprovementof+21.0%in
thecollectivereturnachieved.
MA-LSOAblations WeassesstheeffectivenessofeachcomponentwithinourMA-LSOframe-
work byconductingevaluations thatincludeomissions oftheMA-TDRand MA-SPLprocesses.
For MA-SPL, we exclude its sub-processes individually: MA-MR, MA-FDM, and MA-IDM. A
key contribution of this work is the integration of these auxiliary objectives and their symbiotic
relationship, which Table 1 confirms. Hence, the results demonstrate that all of the components
in our MA-LSO framework not only contribute to the demonstrated improvements but also are
interdependent. Specifically,MA-SPLhasthegreatestimpactintermsofoverallperformance,with
8L L
tdr spl
case L L L MaxReturn
MA-MR MA-FDM MA-IDM
MA-LSO 0.023±0.055 0.033±0.028 0.058±0.025 0.048±0.022 0.954±0.046
noMA-TDR 0.895±0.079 0.288±0.104 0.192±0.138 0.191±0.159 0.847±0.112
noMA-MR 0.188±0.129 0.716±0.086 0.164±0.124 0.102±0.131 0.887±0.122
noMA-FDM 0.092±0.110 0.165±0.030 0.726±0.073 0.159±0.128 0.911±0.185
noMA-IDM 0.152±0.134 0.247±0.055 0.193±0.104 0.793±0.124 0.904±0.114
noMA-SPL 0.331±0.127 0.848±0.132 0.899±0.053 0.933±0.092 0.819±0.198
noMA-LSO 0.970±0.054 0.964±0.088 0.958±0.080 0.908±0.072 0.598±0.201
Table1: EmpiricalresultsfromourablationstudiesonthecomponentsofMA-LSO,comparingthe
losstermsandthemaximumnormalizedreturnachievedwithitsrespective±1stddeviation. For
moredetails,refertoAppendixI.
MA-MRbeingthemostimportantoutofitssub-processes. Thishighlightstheimportanceofthe
relationalinformationinstilledbyMA-SPLandMA-MRandtheconsistencytheyendowwithinthe
latentstatespacebetweentheagents.
Moreover, excluding any processes within MA-LSO results in a notable decline in the training
efficiency of other processes. This suggests a form of amortization similar to that observed in
multi-taskapplications[25],evidentfromoptimalperformanceofeachcomponentisonlyachieved
whenbothMA-TDRandMA-SPLareappliedinunison. Theinterdependenceofthesecomponents
isunderscoredbythefactthattheconvergenceofMA-TDRandMA-SPLlossesdeteriorateswhen
theyareseparated. Specifically,withoutMA-SPL,theconvergenceofMA-TDRdecreasesby30.9%,
whiletheabsenceofMA-TDRleadstodegradationofMA-SPLsubprocessesby25.5%,13.4%,and
14.3%onMA-MR,MA-FDM,andMA-IDMrespectively.
6 Conclusion
WeintroduceageneralizedMARLtrainingparadigm,MAPO-LSO,thatutilizesauxiliarylearning
objectivestoenrichtheMARLlearningprocesswithmulti-agenttransition-dynamicsreconstruction
andself-predictivelearning. Ourapproachimprovesits"non-LSO"counterpartinawidevarietyof
MARLbenchmarktasksusingseveralstate-of-the-artMARLalgorithms. Forfuturedirections,there
remainpromisingavenuestostudyotheraspectsofthemulti-agentnatureofMARLtasks,suchas
ad-hocperformanceandsociallearning,withourMAPO-LSOframework.
References
[1] M.Bettini,R.Kortvelesy,J.Blumenkamp,andA.Prorok. Vmas: Avectorizedmulti-agent
simulator for collective robot learning. The 16th International Symposium on Distributed
AutonomousRoboticSystems,2022.
[2] M.Bettini,A.Prorok,andV.Moens. BenchMARL:BenchmarkingMulti-AgentReinforcement
Learning. arXivpreprintarXiv:2312.01472,2023.
[3] L.Bus¸oniu,R.Babuška,andB.DeSchutter. Multi-agentreinforcementlearning: Anoverview.
Innovationsinmulti-agentsystemsandapplications-1,pages183–221,2010.
[4] T.Chen,S.Kornblith,M.Norouzi,andG.Hinton. Asimpleframeworkforcontrastivelearning
ofvisualrepresentations. InInternationalconferenceonmachinelearning,pages1597–1607.
PMLR,2020.
[5] X. Chen, H. Fan, R. Girshick, and K. He. Improved baselines with momentum contrastive
learning.arxiv2020. arXivpreprintarXiv:2003.04297,2003.
[6] K.W.Cobbe,J.Hilton,O.Klimov,andJ.Schulman. Phasicpolicygradient. InInternational
ConferenceonMachineLearning,pages2020–2027.PMLR,2021.
9[7] C.Colas,O.Sigaud,andP.-Y.Oudeyer. Howmanyrandomseeds? statisticalpoweranalysisin
deepreinforcementlearningexperiments. arXivpreprintarXiv:1806.08295,2018.
[8] C.Daskalakis,N.Golowich,andK.Zhang. Thecomplexityofmarkovequilibriuminstochastic
games. InTheThirtySixthAnnualConferenceonLearningTheory,pages4180–4234.PMLR,
2023.
[9] V.EgorovandA.Shpilman. Scalablemulti-agentmodel-basedreinforcementlearning. arXiv
preprintarXiv:2205.15023,2022.
[10] M. Feng, W. Zhou, Y. Yang, and H. Li. Joint-predictive representations for multi-agent
reinforcementlearning. 2023.
[11] S.Fujimoto,H.Hoof,andD.Meger. Addressingfunctionapproximationerrorinactor-critic
methods. InInternationalconferenceonmachinelearning,pages1587–1596.PMLR,2018.
[12] Y.GalandZ.Ghahramani. Dropoutasabayesianapproximation: Representingmodeluncer-
taintyindeeplearning,2016.
[13] R. Givan, T. Dean, and M. Greig. Equivalence notions and model minimization in markov
decisionprocesses. ArtificialIntelligence,147(1-2):163–223,2003.
[14] R. Gorsane, O. Mahjoub, R. J. de Kock, R. Dubb, S. Singh, and A. Pretorius. Towards
a standardised performance evaluation protocol for cooperative marl. Advances in Neural
InformationProcessingSystems,35:5510–5521,2022.
[15] J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch,
B. Avila Pires, Z. Guo, M. Gheshlaghi Azar, et al. Bootstrap your own latent-a new ap-
proachtoself-supervisedlearning. Advancesinneuralinformationprocessingsystems, 33:
21271–21284,2020.
[16] Z.D.Guo,B.A.Pires,B.Piot,J.-B.Grill,F.Altché,R.Munos,andM.G.Azar.Bootstraplatent-
predictiverepresentationsformultitaskreinforcementlearning. InInternationalConferenceon
MachineLearning,pages3875–3886.PMLR,2020.
[17] T.Haarnoja,A.Zhou,P.Abbeel,andS.Levine. Softactor-critic: Off-policymaximumentropy
deepreinforcementlearningwithastochasticactor. InInternationalconferenceonmachine
learning,pages1861–1870.PMLR,2018.
[18] D.Hafner,T.Lillicrap,J.Ba,andM.Norouzi. Dreamtocontrol: Learningbehaviorsbylatent
imagination. arXivpreprintarXiv:1912.01603,2019.
[19] D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap. Mastering diverse domains through world
models. arXivpreprintarXiv:2301.04104,2023.
[20] J.C.Harsanyi. Gameswithincompleteinformationplayedby“bayesian”players,i–iiiparti.
thebasicmodel. Managementscience,14(3):159–182,1967.
[21] Z.Hu,Z.Zhang,H.Li,C.Chen,H.Ding,andZ.Wang. Attention-guidedcontrastiverolerep-
resentationsformulti-agentreinforcementlearning. InTheTwelfthInternationalConferenceon
LearningRepresentations,2024. URLhttps://openreview.net/forum?id=LWmuPfEYhH.
[22] D.HuhandP.Mohapatra. Isaacteams: Extendinggpu-basedphysicssimulatorformulti-agent
learning,2023.
[23] D.HuhandP.Mohapatra. Multi-agentreinforcementlearning: Acomprehensivesurvey,2024.
[24] M.Jaderberg,V.Mnih,W.M.Czarnecki,T.Schaul,J.Z.Leibo,D.Silver,andK.Kavukcuoglu.
Reinforcementlearningwithunsupervisedauxiliarytasks. arXivpreprintarXiv:1611.05397,
2016.
[25] D.Kalashnikov,J.Varley,Y.Chebotar,B.Swanson,R.Jonschkowski,C.Finn,S.Levine,and
K.Hausman. Mt-opt: Continuousmulti-taskroboticreinforcementlearningatscale. arXiv
preprintarXiv:2104.08212,2021.
10[26] S.Kapturowski,G.Ostrovski,J.Quan,R.Munos,andW.Dabney. Recurrentexperiencereplay
indistributedreinforcementlearning. InInternationalconferenceonlearningrepresentations,
2018.
[27] J.I.Kim,Y.J.Lee,J.Heo,J.Park,J.Kim,S.R.Lim,J.Jeong,andS.B.Kim. Sample-efficient
multi-agentreinforcementlearningwithmaskedreconstruction. PloSone,18(9):e0291545,
2023.
[28] R.Krishnan,P.Esposito,andM.Subedar. Bayesian-torch: Bayesianneuralnetworklayersfor
uncertaintyestimation,Jan.2022. URLhttps://doi.org/10.5281/zenodo.5908307.
[29] J. G. Kuba, R. Chen, M. Wen, Y. Wen, F. Sun, J. Wang, and Y. Yang. Trust region policy
optimisationinmulti-agentreinforcementlearning. arXivpreprintarXiv:2109.11251,2021.
[30] M. Laskin, A. Srinivas, and P. Abbeel. Curl: Contrastive unsupervised representations for
reinforcementlearning. InInternationalConferenceonMachineLearning,pages5639–5650.
PMLR,2020.
[31] V.Lee,P.Abbeel,andY.Lee. Dreamsmooth: Improvingmodel-basedreinforcementlearning
viarewardsmoothing. InTheTwelfthInternationalConferenceonLearningRepresentations,
2024. URLhttps://openreview.net/forum?id=GruDNzQ4ux.
[32] J. Li, K. Kuang, B. Wang, X. Li, F. Wu, J. Xiao, and L. Chen. Two heads are better than
one: A simple exploration framework for efficient multi-agent reinforcement learning. In
A.Oh,T.Neumann,A.Globerson,K.Saenko,M.Hardt,andS.Levine,editors,Advancesin
NeuralInformationProcessingSystems,volume36,pages20038–20053.CurranAssociates,
Inc.,2023. URLhttps://proceedings.neurips.cc/paper_files/paper/2023/file/
3fa2d2b637122007845a2fbb7c21453b-Paper-Conference.pdf.
[33] Q.Liu,J.Ye,X.Ma,J.Yang,B.Liang,andC.Zhang. Efficientmulti-agentreinforcement
learningbyplanning. InTheTwelfthInternationalConferenceonLearningRepresentations,
2023.
[34] R.Lowe,Y.I.Wu,A.Tamar,J.Harb,O.PieterAbbeel,andI.Mordatch.Multi-agentactor-critic
formixedcooperative-competitiveenvironments. Advancesinneuralinformationprocessing
systems,30,2017.
[35] T. Ni, B. Eysenbach, E. Seyedsalehi, M. Ma, C. Gehring, A. Mahajan, and P.-L. Bacon.
Bridgingstateandhistoryrepresentations: Understandingself-predictiverl. arXivpreprint
arXiv:2401.08898,2024.
[36] G.Palmer. IndependentLearningApproaches: OvercomingMulti-AgentLearningPathologies
InTeam-Games. PhDthesis,UniversityofLiverpool,2020.
[37] Y.Qiu,Y.Zhan,Y.Jin,J.Wang,andX.Zhang. Sample-efficientmulti-agentreinforcement
learningwithdemonstrationsforflockingcontrol. In2022IEEE96thVehicularTechnology
Conference(VTC2022-Fall),pages1–7.IEEE,2022.
[38] G. Qu, A. Wierman, and N. Li. Scalable reinforcement learning for multiagent networked
systems. OperationsResearch,70(6):3601–3628,2022.
[39] M.Schwarzer,A.Anand,R.Goel,R.D.Hjelm,A.Courville,andP.Bachman. Data-efficient
reinforcementlearningwithself-predictiverepresentations. arXivpreprintarXiv:2007.05929,
2020.
[40] M.Schwarzer,N.Rajkumar,M.Noukhovitch,A.Anand,L.Charlin,R.D.Hjelm,P.Bachman,
and A. C. Courville. Pretraining representations for data-efficient reinforcement learning.
AdvancesinNeuralInformationProcessingSystems,34:12686–12699,2021.
[41] W.Shang,L.Espeholt,A.Raichuk,andT.Salimans. Agent-centricrepresentationsformulti-
agentreinforcementlearning. arXivpreprintarXiv:2104.09402,2021.
[42] L.S.Shapley. Stochasticgames*. ProceedingsoftheNationalAcademyofSciences,39(10):
1095–1100,1953. doi: 10.1073/pnas.39.10.1095. URLhttps://www.pnas.org/doi/abs/
10.1073/pnas.39.10.1095.
11[43] H. Song, M. Feng, W. Zhou, and H. Li. Ma2cl: Masked attentive contrastive learning for
multi-agentreinforcementlearning. arXivpreprintarXiv:2306.02006,2023.
[44] A. Stooke, K. Lee, P. Abbeel, and M. Laskin. Decoupling representation learning from
reinforcementlearning. InInternationalConferenceonMachineLearning,pages9870–9879.
PMLR,2021.
[45] J. Subramanian, A. Sinha, R. Seraj, and A. Mahajan. Approximate information state for
approximateplanningandreinforcementlearninginpartiallyobservedsystems. TheJournalof
MachineLearningResearch,23(1):483–565,2022.
[46] R.S.SuttonandA.G.Barto. Reinforcementlearning: Anintroduction. MITpress,2018.
[47] Y.Tang,Z.D.Guo,P.H.Richemond,B.A.Pires,Y.Chandak,R.Munos,M.Rowland,M.G.
Azar, C. Le Lan, C. Lyle, et al. Understanding self-predictive learning for reinforcement
learning. InInternationalConferenceonMachineLearning,pages33632–33656.PMLR,2023.
[48] A. Tripp, E. Daxberger, and J. M. Hernández-Lobato. Sample-efficient optimization in the
latentspaceofdeepgenerativemodelsviaweightedretraining. AdvancesinNeuralInformation
ProcessingSystems,33:11259–11272,2020.
[49] W.-C.Tseng,T.-H.J.Wang,Y.-C.Lin,andP.Isola. Offlinemulti-agentreinforcementlearning
withknowledgedistillation. AdvancesinNeuralInformationProcessingSystems,35:226–237,
2022.
[50] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and
I.Polosukhin. Attentionisallyouneed. Advancesinneuralinformationprocessingsystems,
30,2017.
[51] M.Wen,J.Kuba,R.Lin,W.Zhang,Y.Wen,J.Wang,andY.Yang. Multi-agentreinforcement
learningisasequencemodelingproblem. AdvancesinNeuralInformationProcessingSystems,
35:16509–16521,2022.
[52] Y.Yang,X.Ma,C.Li,Z.Zheng,Q.Zhang,G.Huang,J.Yang,andQ.Zhao. Believewhatyou
see: Implicitconstraintapproachforofflinemulti-agentreinforcementlearning. Advancesin
NeuralInformationProcessingSystems,34:10299–10312,2021.
[53] C.Yu,A.Velu,E.Vinitsky,J.Gao,Y.Wang,A.Bayen,andY.Wu. Thesurprisingeffectiveness
ofppoincooperativemulti-agentgames. AdvancesinNeuralInformationProcessingSystems,
35:24611–24624,2022.
[54] L. Zhou, M. Poli, W. Xu, S. Massaroli, and S. Ermon. Deep latent state space models for
time-seriesgeneration. InInternationalConferenceonMachineLearning,pages42625–42643.
PMLR,2023.
12A MARLEnvironments
A.1 VectorizedMulti-AgentEnvironments
Figure 7: The 18 VMAS tasks used for our evaluations. Their full descriptions can be found in
[1]. Foreachtask,weuse1024parallelenvironmentsfortrainingand16forevaluationandranthe
trainingfor[100t,200t,500t,1000t]time-steps,dependingonthelearningperformance,wheretis
thetimehorizonforeachtask.
13A.2 IsaacTeams
(a)abb-reacher-2 (b)franka-reacher-2 (c)kuka-reacher-2
(d)afk-reacher-3 (e)visual-afk-reacher-3
Figure 8: The 5 IST tasks used for our evaluation. For all tasks, the objective is to move the
end-effectorstotheirrespectivetargetspheres,wheretherewardfunctionisshapedtominimizeℓ
2
distance. Thesetargetspheresarepositionedrandomly. Theobservationspaceincludetherobotic
arm’s proprioceptive information as well as the information regarding its target sphere. For the
visual-afk-reacher-3task,thevisualinput(withresolutionof32×32)thatisshownatthebottom
ofFigure8eisappendedtotheobservationspaceoftherespectiveagent,wherethevisualization
showninFigure8ehasincreasedresolutionforpresentationpurposes. Theactionspacecontrols
thejointactuationoftheroboticarms. Alltasksdefineacommunicationnetworkthatenablesfull
communication. ThetrainingprocedureotherwisefollowstheonesetforVMAS,exceptforthe
visual-afk-reacher-3,whereweuse128parallelenvironmentsfortraining.
14B ImplementationofMARLAlgorithms
MAPPO Multi-AgentProximalPolicyOptimization(MAPPO)[53]isaCTDEextensionofthe
ProximalPolicyOptimization(PPO)algorithmthatemploysdecentralizedpolicieswithcentralized
valuefunctions. Inourimplementation,wefollowtheoriginalpaper’simplementationbutwitha
centralizedcriticsharedbetweenallagents.
HAPPO Heterogenous-Agent Proximal Policy Optimization [29] refines MAPPO, imposing a
random-order sequential-update scheme to ensure monotonic improvements unrestricted to the
assumptionofhomogeneityofagents. Ourimplementationfollowstheoriginalwork,butthemain
differencesstemlargelyfromthesharedencoderbetweenthepolicyandvaluefunction. Toensure
morestablelearning,largelyduetothesharedencoder,weupdatethevaluefunctionuponeachagent
updateandreducethelearningrateoftheencoder.
MADDPG Similar to MAPPO, Multi-Agent Deep Deterministic Policy Gradient (MADDPG)
[34] is a CTDE extension of the Deep Deterministic Policy Gradient (DDPG) algorithm. The
maindifferenceinourimplementationfollows[11],includingdelayedpolicyupdates,targetpolicy
smoothing,clippeddoublelearning,stochasticactorsandasharedcriticbetweenallagents.
MASAC Multi-Agent Soft Actor Critic is a CTDE extension of the Soft Actor Critic (SAC)
algorithm[17]. OurimplementationissimilartoourMADDPGimplementationwithadjustmentsfor
auto-tunedentropymaximization.
C ModelArchitecture
Forallalgorithms,wefollowthesamemodelarchitecturewedescribedbelow.
Encoder TheencoderisresponsibleforembeddingtheinputdataandfollowstheDreamerV3
encoderarchitecture[19]thatmostmatchesthe12Mparametermodel. Formulti-modaldata,we
processthedifferentmodalityofdataseparately,i.e. imageswithaCNNandstructureddatawith
aMLP,andaggregatetheembeddingswithasumoperator. Wedefineaseparateencoderforeach
agent.
Communication Block The communication block propagates the embeddings between agents
dependentonthecommunicationgraphtoproducethelatentspace. Wemodeledthiscomponent
afterMAMBA’scommunicationblock[9],althoughweoptedtohaveasmallermodel. Forpartial
communicationgraphs, wemasktheembeddingsoftheunconnectedagents. Thepolicyofeach
agentusestheirownlatentstatetocomputetheiractions,andthecentralizedcriticconcatenatesthe
latentstateofallagentstocomputethevalueforallagents.
MA-TDR Similar to the encoder, the components such as the decoder and the ac-
tion/reward/continue(ARC)headswereallmodeledfollowingtheDreamerV3architecture[19]. The
ARCheadsthatmodeledbeliefsofotheragentswereappendedwithanMC-Dropoutlayer[12]when
uncertaintymodelingisused.
MA-SPL ForMA-MR,wefollowthesamesetupasCURL[30]andforMA-FDMandMA-IDM,
wemostlyadheretothesameprocedureandmodelarchitectureassingle-agentSPL[40]withrandom
noiseaugmentation. ForR,T ,I,weuseamulti-headedattentionhead,where:
z
R(m (z ))=MultiHeadedAttn(q =m (z ),k =m (z ),v =m (z ))
i t i t i t i t
T (z ,a )=MultiHeadedAttn(q =a ,k =z ,v =z )
z t t t t t
I(z ,z )=MultiHeadedAttn(q = z ,z ,k = z ,z ,v = z ,z )
t t+1 t t+1 t t+1 t t+1
(cid:74) (cid:75) (cid:74) (cid:75) (cid:74) (cid:75)
where z ,z concatenatesz andz intoasinglesequence.
t t+1 t t+1
(cid:74) (cid:75)
15D Hyperparameters
Foreachtask,weinitiallyranrandomsearchoverthefollowinghyperparametersandfollowedup
withfurthertuningusingqualitativeexaminationsovertheseruns.
Table2: MAPPO/HAPPO
Name Value
learningrate [1e-3,5e-4,1e-4,5e-5,1e-5,1e-6]
entropycoef. [1e-5,1e-3,3e-4]
clipcoef. [0.05,0.1,0.15,0.2,0.3,0.5]
discountfactor 0.99
num. ofupdates 30
targetKL [0.01,None]
gradientnorm [0.5,1.0,None]
λ-return 0.95
Table3: MADDPG
Name Value
learningrate [1e-3,5e-4,1e-4,5e-5,1e-5,1e-6]
explorationnoise [0.01,0.1,0.5]
learningstarts [t,2t]
smoothingnoise [0.1,0.2,0.5]
smoothingnoiseclip [0.01,0.1]
num. ofupdates 50
policyupdatefrequency 2
gradientnorm [0.5,1.0,None]
targetnetworkupdateτ 0.005
targetnetworkfrequency 2
Table4: MASAC
Name Value
learningrate [1e-3,5e-4,1e-4,5e-5,1e-5,1e-6]
learningstarts [t,2t]
num. ofupdates 50
policyupdatefrequency 2
gradientnorm [0.5,1.0,None]
targetnetworkupdateτ 0.005
targetnetworkfrequency 2
Table5: MA-TDR(ForMAPO-LSO)
Name Value
α [1e-3,0.1,0.5,1]
obs
α [1e-3,0.1,0.5,1]
act
α [1e-3,0.1,0.5,1]
rew
α [1e-3,0.1,0.5,1]
cont
α [1e-3,0.1,0.5,1]
curl
dropout [0,0.1,0.2,0.5,0.8]
Table6: MA-SPL(ForMAPO-LSO)
Name Value
α [1e-3,0.1,0.5,1]
mr
α [1e-3,0.1,0.5,1]
fdm
α [1e-3,0.1,0.5,1]
idm
Table7:HyperparametersforMAPPO,HAPPO,MADDPG,andMASAC,andtheMA-LSOlearning
processes,wheretisthelengthofafulltrajectory.Thebatch-sizewassetbasedonthemaximumload
possibleonourGPU,whichdifferedforalltasks. ForMAPO-LSOtrainings,thehyperparameters
fortheMARLalgorithmsarefixed.
16E FullResults: EfficiacyofMAPO-LSO
Figure9: VMAS/ISTResultsForMAPO-LSO:Theseplotscomparetheperformanceofthetradi-
tionalMARLalgorithms(shownintheblueline)versusitsLSOcounterpart(shownintheredline)
ineachtasktestedintheVMAS/ISTbenchmarkunderanormalizedscale. Eachcolumnofplotsuses
thesameMARLalgorithmandeachrowevaluatesonthesametask. They-axisisthenormalized
collectivereturnandthex-axisisthenormalizedtime-steps,withevaluationranover16random
seeds. Theerrorbarsshowtheminandmaxreturnsoverthose16runs.
17F FullResults: PhasicOptimizationForHAPPO/MADDPG/MASAC
Figure10: VMAS/ISTResultsForPhasicOptimization: Theseplotscomparestheperformanceof
MAPO-LSOwith(shownintheredline)andwithout(showninthegreenline)phasicoptimization
ineachtasktestedintheVMAS/ISTbenchmarkunderanormalizedscale. Wenotethatthesame
hyperparametersareusedforboth,buttheyaretunedforMAPO-LSOwithphasicoptimization. The
formatfollowsFigure9.
18G FullResults: PretrainingExperiments
Figure11: VMAS/ISTResultsForPretraining: TheseplotscomparestheperformanceofMAPO-
LSOwith(shownintheorangeline)andwithout(shownintheredline)pre-trainingineachtask
testedintheVMAS/ISTbenchmarkunderanormalizedscale.Wenotethatthesamehyperparameters
areusedforboth,buttheyaretunedforMAPO-LSOwithoutpretraining. TheformatfollowsFigure
9.
19H FullResults: UncertaintyModelingExperiments
Figure12: VMAS/ISTResultsForUncertaintyModeling: Theseplotscomparestheperformanceof
MAPO-LSOwith(shownintheredline)andwithout(showninthepurpleline)uncertaintymodeling
ineachtasktestedintheVMAS/ISTbenchmarkunderanormalizedscale. Wenotethatthesame
hyperparametersareusedforboth,buttheyaretunedforMAPO-LSOwithuncertaintymodeling.
TheformatfollowsFigure9.
20I FullResults: MAPO-LSOAbalations
Figure13: VMASResultsForMAPO-LSOAbalations: Theseplotscomparestheperformanceof
MAPO-LSOwithvariouscomponentsmissingineachtasktestedintheVMASbenchmarkunder
anormalizedscale. Wenotethatthesamehyperparametersareusedforall,buttheyaretunedfor
MAPO-LSOwithallcomponents(LSO)andthevanillaMARLalgorithm(noLSO).Theformat
followsFigure9.
21