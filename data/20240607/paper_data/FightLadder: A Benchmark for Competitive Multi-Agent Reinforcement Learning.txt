FightLadder: A Benchmark for Competitive
Multi-Agent Reinforcement Learning
WenzheLi1,ZihanDing1,SethKarten1,andChiJin1
1PrincetonUniversity*
Abstract
Recentadvancesinreinforcementlearning(RL)heavilyrelyonavarietyof
well-designedbenchmarks,whichprovideenvironmentalplatformsandconsistent
criteriatoevaluateexistingandnovelalgorithms.Specifically,inmulti-agentRL
(MARL),aplethoraofbenchmarksbasedoncooperativegameshavespurredthe
developmentofalgorithmsthatimprovethescalabilityofcooperativemulti-agent
systems. However, forthecompetitivesetting, alightweightandopen-sourced
benchmarkwithchallenginggamingdynamicsandvisualinputshasnotyetbeen
established. In this work, we present FightLadder, a real-time fighting game
platform, to empower competitive MARL research. Along with the platform,
weprovideimplementationsofstate-of-the-artMARLalgorithmsforcompetitive
games, as well as a set of evaluation metrics to characterize the performance
andexploitabilityofagents. Wedemonstratethefeasibilityofthisplatformby
trainingageneralagentthatconsistentlydefeats12built-incharactersinsingle-
playermode,andexposethedifficultyoftraininganon-exploitableagentwithout
humanknowledgeanddemonstrationsintwo-playermode.FightLadderprovides
meticulouslydesignedenvironmentstoaddresscriticalchallengesincompetitive
MARLresearch,aimingtocatalyzeaneweraofdiscoveryandadvancementinthe
field.Videosandcodeathttps://sites.google.com/view/fightladder/home.
1 Introduction
Asanactivebranchofartificialintelligence(AI),deepreinforcementlearning(DRL)
has achieved significant success in various domains, including, but not limited to,
strategicgames(Silveretal.,2016;Lietal.,2020;Moravvcíketal.,2017;Vinyals
etal.,2019;Berneretal.,2019),roboticscontrol(Lillicrapetal.,2015;Andrychowicz
et al., 2020b; Brohan et al., 2022), and large language models alignment (Ouyang
et al., 2022). Underpinning these rapid advances are not only the development of
sample-efficientRLalgorithmsbutalsotheavailabilityofwell-designedbenchmarks.
These benchmarks provide environmental platforms, unify evaluation protocols, en-
ablecomparisonsofstate-of-the-artmethods,motivateimprovedsolutions,andguide
*Email:{wenzhe.li,zihand,sethkarten,chij}@princeton.edu.
1
4202
nuJ
4
]AM.sc[
1v18020.6042:viXraFigure1: FightLaddercurrentlysupportsvariouscross-platformvideofightinggames:
StreetFighterII (Genesisplatform),StreetFighterIII (Arcadeplatform),FatalFury2
(Genesisplatform),MortalKombat(Genesisplatform),andTheKingofFighters’97
(NeoGeoplatform).
practicalapplications. Asanexample,policyproximaloptimization(PPO)(Schulman
et al., 2017) demonstrates its superior performance across different single-agent RL
benchmarks,hencebeingconsideredasoneofthemostwidelyadoptedsingle-agent
RLalgorithms(Andrychowiczetal.,2020a). Intherealmofmulti-agentreinforcement
learning(MARL),whileaseriesofbenchmarkshavealsobeenproposed,mostofthem
focus on fully cooperative settings. For competitive environments, some platforms
simulategameswithtabularrepresentationsandrelativelysimpledynamics,suchas
boardgames,whileothers,basedoncomplexgameengines,requiresignificantcompu-
tationalresourcesandexpertknowledge,suchasStarcraftIIandDOTA.Toadvance
research on competitive multi-agent reinforcement learning (MARL) and transform
game-theoreticalresultsintopracticalapplications,afullycompetitivegameplatform
thatstrikestherightbalancebetweencomplexity,efficiency,andgeneralityisurgently
needed.
Multi-agentgamesareknowntobemorechallengingthansingle-agentonesdue
to the additional non-stationarity introduced by the interactions with other players.
Amongdifferenttypesofinteractions,fullycompetitivesettingscanberatherdifficult.
People have a long history of designing and playing competitive games, as well as
buildingstrongAIopponentstomakethegamemorechallengingandhenceintriguing.
PreviousAIresearchhasinvestigatedthesolutionsofcompetitivegamesusingRL,but
mostlyforsmall-scalegameslikeBackgammon(Tesauroetal.,1995)orotherboard
games(Schrittwieseretal.,2020;Brown&Sandholm,2018,2019). Moreover,this
lineofworkmostlyusesstatevectorsasinputs,whichisarguablyeasierthandirectly
learning from raw pixel inputs that commonly appear in most popular video games.
Incontrast,thispaperconsidersfightinggames,whichfeaturerichpolicyspace,and
significantdepthinstrategy—includingcatchingspecifictiming,counter-attackby
exploitingthestiffnessoftheopponents,managingenergyresources,etc. Moreover,
2thesegamesalsohavearatherlargenumberofcharacterswithdistinctmove-setswhich
addanotherlayerofcomplexityforAIagentstomasterthegame. Asaresult,weare
motivated to build a platform for a series of fighting games, with image inputs and
complexfightingdynamics,toserveasachallengingcompetitivemulti-playerplatform
forthebroadAIresearchcommunity.
Apartfromthegameplatform, theevaluationcriteriaandbenchmarkresultsfor
certain game settings are essential for boosting the field. MARL has been greatly
investigatedinthepastfewyearsforsolvingmulti-playergames,fromboththeoretical
andempiricalperspectives. Alargenumberofalgorithmshavebeenproposedaccording
tospecificsettings(Sunehagetal.,2017;Yuetal.,2022;Loweetal.,2017;Silveretal.,
2018;Lanctotetal.,2017;Vinyalsetal.,2019;Dingetal.,2022). Nonetheless, for
competitivegamesettings,thereisalackofunifiedevaluationcriteriawiththorough
comparisonsamongdifferentapproaches.
Inthiswork,wepresentFightLadder,acompetitivetwo-playergamesbenchmark.
Our contributions are three-fold: We build the FightLadder platform to support five
two-player fighting games, with ease to extend to other games in the future. The
gamessupportvariousobservationspacesinvolvingrenderedimages. Basedonprior
work, weprovideimplementationsofthemostpopularalgorithmsforsolvingthese
competitivegames,includinganAlphaStarleaguetrainingalgorithm(Vinyalsetal.,
2019)andpolicyspaceresponseoracle(Lanctotetal.,2017). Furthermore,aunified
evaluationframeworkwithEloratingandexploitabilitytestsareprovidedalongside
the game platforms and algorithm library. We report experimental results using the
abovetoolkitstoserveasthebaselinesfortwo-playercompetitivegamesettings. One
importantchallengeofMARLisitsdiversenature,whichincludescollaborativegames,
competitivegames,two-playergames,andmultiplayergames,allofwhichhaverather
differentproblemstructures,properties,andsolutionconcepts. Whileitispromisingto
developaunifiedsolutionthataddressesthemalltogether,inthiswork,weempirically
demonstratethattosomeextent,existingmethodsarestilllimitedinsolvingcompetitive
two-player zero-sum games alone when combined with visual input, rich strategy
space,andlackofextensivehumandemonstration. WehopethatFightLadder,which
particularlyfocusesonthisfundamentaltwo-playersetting,canserveasasteppingstone
fortheresearchcommunitytodevelopeffectiveself-playstylealgorithmstotackleit
firstbeforemovingontoevenmorecomplicatedscenarios,andinspirefuturedirections
thatinvolvemoretypesofinteractions.
2 Related Work
MARL Environments. MARL environments can be categorized into three types
accordingtothepayoffstructureofthegame: fullycooperative,fullycompetitive,and
general.
Existingenvironmentsforfullycooperativegamesaredesignedforvariousscenar-
ios,includingsimulatedgameslikeMAMuJoCo(Pengetal.,2021),cardgameslike
Hanabi(Bardetal.,2020),videogameslikesmall-scaleStarCraftSMAC(Samvelyan
etal.,2019)andGoogleResearchFootball(Kurachetal.,2020),aswellaspractical
scenarioslikeTrafficJunction(Sukhbaataretal.,2016)inagridworld,Flatland(Mo-
3hantyetal.,2020)forrailwaynetworks,networkloadbalancing(Yao&Ding,2022)
andCityFlow(Zhangetal.,2019)forcitytraffic. Cooperativeenvironmentsfeaturea
singlerewardfunctionsharedbyallagents,whichmakesthemdistinctfromcompetitive
games.
Ontheotherhand,thefullycompetitivegamebenchmarksarerelativelyunderde-
veloped. Prior competitive environments are either on games with low-dimensional
or discrete state space such as Pommerman (Resnick et al., 2018) and board games
(Tesauroetal.,1995;Schrittwieseretal.,2020;Brown&Sandholm,2018);orcomplex
gameswithimageinputthatrequireasignificantamountofcomputationalresources,
suchasStarcraftII(Vinyalsetal.,2019)orDOTA(Berneretal.,2019). Thefighting
gameenvironmentsproposedinthispaperstriketherightbalancebetweencomplexity,
efficiency, andgenerality. Afewpreviousworksalsohaveexploredfightinggames:
Goetal.(2023)focusesondevelopinganalgorithmforasinglefightinggame—street
fighter,asopposedtothispaperwhichprovidesanenvironmentthatsupportsvarious
fightinggames. WhilePalmas(2022)providesaplatformforfightinggames,mostof
itseffortshavebeenfocusedonthesingle-agentsetting. Itlacksexplicitcriteriafortwo-
playerscenarioswithadaptiveopponents,anddoesnotprovideabenchmarkevaluating
existingcompetitiveMARLalgorithms. Khanetal.(2022)focusesonfightinggames
intheblindsettingwhereagentshavetorelyonacousticinputstoplay.
Finally,therearealsoanumberofenvironmentsforgeneralmultiagentgamesthat
featurebothcooperationandcompetition,includingMPE(Mordatch&Abbeel,2018),
MAgent(Zhengetal.,2018),Hide-and-Seek(Bakeretal.,2019),DMLab2D(Beattie
etal.,2020),Arena(Songetal.,2020),Smarts(Zhouetal.,2020),NeuralMMO(Suarez
etal.,2021),PettingZoo(Terryetal.,2021),MATE(Panetal.,2022),etc.Genericmulti-
agentgeneral-sumgamesareratherchallengingtoevaluate—eventheoptimalsolution
conceptsremainelusive. Incontrast,thefullycompetitivesettingconsideredinthis
paperpresentscleargame-theoreticpropertiesandwell-definedsolutionconcepts. We
alsoremarkthatwhileanumberoftheplatformsabovesupportseveralfullycompetitive
games,theydidnotprovidecarefullydesignedevaluationtoolkitsaswellasextensive
baselinesforcompetitiveMARLalgorithms.
MARLAlgorithmsandEvaluation. Tosolvemulti-agentlearningtasks,researchers
have proposed algorithms and built libraries for ease of usage and evaluation. Py-
MARL(Samvelyanetal.,2019)isaninitialMARLlibrarybuiltforsolvingSMAC
tasks,whilePyMARL2(Huetal.,2021)extendsPyMARLwithQMIX(Rashidetal.,
2020). EPyMARL (Papoudakis et al., 2020) is also an extension of PyMARL, as a
unifiedlibraryforcooperativegamessupportingdifferentlearningparadigmsincluding
centralizedanddecentralizedlearning,valuedecomposition,etc. MARLlib(Huetal.,
2023)includesmajorcooperativeMARLalgorithmslikeVDN(Sunehagetal.,2017),
MAPPO(Yuetal.,2022),MADDPG(Loweetal.,2017),etc. Morerecentlibraries
includePantheonrl(Sarkaretal.,2022),MAlib(Zhouetal.,2023),etc. Theselibraries
mainlysupportMARLalgorithmsforcooperativegames,lackingsupportforsolving
competitivegames.
Ontheotherhand,thereisalineofresearchforsolvingcompetitivegameswith
algorithmslikeself-play(Silveretal.,2018),fictitiousplay(Brown,1951),NashQ-
4learning(Hu&Wellman,2003;Dingetal.,2022),doubleoracle(McMahanetal.,2003),
policyspaceresponseoracle(PSRO)(Lanctotetal.,2017)andleaguetraining(Vinyals
et al., 2019). A unified benchmark remains missing to compare and evaluate the
efficiencythesealgorithmsonthesamesetoftasks,especiallywhencombinedwithdeep
RL.Thispaperaddressesthisissueinthefullycompetitivesetting. Weconcentrateon
two-playerzero-sumgames,andproposeaplatformforfighting-stylefullycompetitive
games,alongwithabaselineimplementationandevaluationofpopularalgorithms.
3 Multi-Agent Reinforcement Learning
FightLadderisdesignedtomotivatenovelalgorithmsforfullycompetitivetwo-player
gamesinthedomainsofMARLandgametheory. MarkovGames(MGs)(Shapley,
1953)generalizesingle-playerMarkovDecisionProcesses(MDPs)intomulti-player
settings. Each player has its own utility and optimizes its policy to maximize the
utility. Thetwo-playerzero-sumsettinginMGrepresentsacompetitiverelationship
betweenthetwoplayers. Withashapeddensereward,thegamescanbegeneralizedto
general-sum.
We denote a finite-horizon two-player general-sum partially observable MG as
POMG(S,O,A,B,P,O,{r}2 ,H). S is the state space, which can be partially
i=1
observableandtransformedthroughanobservationemissionfunctionO: S →Otothe
observationspaceO.AandBareactionspacesfortwoplayers,respectively.P(·|s,a,b)
isthestatetransitiondistribution,r :S×A×B →Ristherewardfunctionforthe
i
i-thplayer. Inthezero-sumsetting,tworewardfunctionssatisfythezero-sumpayoff
structurer +r =0. H isthehorizonlength. Wedenotethepoliciesoftwoplayersas
1 2
µandν,respectively. Vµ,ν: S →Rrepresentsthevaluefunctionforplayerievaluated
i
withpoliciesµandν,whichcanbeexpandedastheexpectedcumulativerewardstarting
fromthestates,
V iµ,ν(s):=E µ,ν(cid:2)(cid:80)∞ h=1r i(s h,a h,b h)(cid:12) (cid:12)s
1
=s(cid:3) .
In zero-sum games, we have Vµ,ν(s) = −Vµ,ν(s),∀s ∈ S and define Vµ,ν(s) =
1 2
Vµ,ν(s)forsimplicity.
1
Definition3.1(BestResponse). Foranypolicyofthefirstplayerµ,thereexistsabest
response (BR) against it from the second player, which is a policy ν†(µ) satisfying
Vµ,ν†(µ)(s)=max Vµ,ν(s)forany(s,h)∈S ×[H]. WedenoteVµ,† :=Vµ,ν†(µ)
2,h ν 2,h 2,h 2,h
forsimplification. Vµ,ν(s)isthevaluefunctionofthesecondplayer. BRagainstthe
2,h
secondplayercanbedefinedsimilarly.
Definition3.2(NashEquilibrium). TheNashequilibrium(NE)inzero-sumsettingis
definedasapairofpolicies(µ⋆,ν⋆)satisfyingthefollowingminimaxequation:
maxminVµ,ν(s)=Vµ⋆,ν⋆ (s)=minmaxVµ,ν(s).
µ ν ν µ
Definition 3.3 (Exploitability). The exploitability for a policy µ of the first player
is defined as Vµ,†(s ) − Vµ⋆,ν⋆ (s ), i.e., the value of its BR policy ν†(µ) or the
2 1 2 1
5suboptimalitygapfromtheNEvalue. Theexploitabilityoftheothersidepolicyν can
bedefinedaccordingly.
NotethatNEstrategieswillalwaysleadtozeroexploitability,thusapproachingthe
non-exploitablestrategiesisareasonablepursuitforthegame.
4 FightLadder
Inthissection,wepresenttechnicaldetailsofFightLadder. Inthefollowingpart,we
firstintroducedifferentgamesettingsofFightLadder,followedbyelaboratingelements
ofMGscorrespondingtotheenvironment,andconcludewithhighlightingfeaturesof
ourbenchmark.
4.1 Scenarios
FightLadderprovidesaflexibleinterfacebetweenmoderngameemulators(Murphy,
2013;Nicholetal.,2018)andalgorithmdevelopers. Thankstoitsflexibility,FightLad-
dercansupportawiderangeofclassicalfightinggamesoverthepastdecades,including
StreetFighter,MortalKombat,FatalFury,andTheKingofFighters,someofwhich
arestillverypopularnowadays. Figure1showsscreenshotsofseveralfightinggames
provided by FightLadder. With this diverse set of supported games, we can bench-
markalgorithmsonvariousfightingscenariosdifferinginbackgrounds,characters,and
movingdynamics,whichcanfurthermotivatenovelalgorithmsthataregeneralrather
thanoverfittingtoonespecificgame. Forbetterreadabilityandclarity,wewoulduse
StreetFighterasanexampleforillustrationandevaluationintherestofthepaper. The
otherfightinggamesareverysimilar,andreaderscouldrefertoAppendixA.2formore
details. Wenameeachscenariointheform[gamealias]_[characterleft]_vs_[character
right],forexamplesf_ryu_vs_ryuinStreetFighter.
WhileFightLaddermainlyfocusesonthecompetitivetwo-playersetting,thenature
offightinggamesallowsittobeseamlesslydeployedtothesingle-playerscenariowhere
the agent’s task is to compete against a built-in game AI (e.g., sf_ryu_vs_ryu(cpu)).
Underthissingle-playersetting,usershavethefreedomtochoosecharactersandset
upthedifficultyofthescriptedAIopponent. Moreover,ourbenchmarkalsosupports
traininginamuchmorechallengingfull-gamescenario(e.g.,sf_ryu_full_game),where
theagentneedstodefeatall12characterscontrolledbycomputerswiththedifficulty
progressivelyincreasing. Asweshallseeinlaterexperiments,thisscenariocouldalso
serve as a sanity check for our baseline algorithms to see whether they could learn
effectivebehaviorsfromtheenvironment.
4.2 StateandObservations
WedefinethestatespaceS asthecompletesetofattributesstoredinthegameemulator
aftereachstepofaction. Sameashumanplayers,theagentisnotallowedtoaccessthe
underlyingfullstatebutcanonlyaccesstheobservationspaceOofpixels,whichforms
a128×100RGBimagecorrespondingtotherenderedscreen. Thisimageincludesthe
6Figure2: Motionandattackactionspacesoffightinggames. Imagesareadaptedfrom
InstructionManualofStreetFighterII.
positionandmovementofbothsidesoftheplayers,aswellasthehit-pointbarandthe
roundtimeronthetopofthescreen. Ateverystep,aconfigurablenumberofimages
arestackedastheinputoftheagent.
Whileweusepixelsasdefaultobservations,wealsoprovideaninterfaceforusers
toaccessadditionalinformationaboutthegamestatus,includingposition,hit-point,and
exactcountdownnumberforagentsonbothsides. Userscanleveragetheseattributesto
betterunderstandtheagent’sbehaviororaugmentfeaturerepresentations. Moredetails
areprovidedinAppendixA.2.
4.3 ActionSpace
Infightinggames,twoplayerssharethesameactionspaceA. Thenativehumanaction
space A is designed to mimic the joystick control of arcade games, which is a
human
12-dimensionalbinaryspace([’B’,’A’,’MODE’,’START’,’UP’,’DOWN’,’LEFT’,
’RIGHT’,’C’,’Y’,’X’,’Z’])witheachdimensionrepresentingabuttonbeingpressed
ornot. Notethatduetothenatureoffightinggameengines,thisspacecontainsmany
redundantactionsthatareinvalid,forinstance,movinginoppositedirectionsormoving
andattackingatthesamemoment. Tofilterouttheseredundantactionsandtoconstruct
a more structured space, we develop a categorical transformed action space A
trans
through an encoding function F : A → A . Specifically, A is the joint
human trans trans
setofadirectionmovesetA ={defense,forward,jump,crouch,backflip,front
motion
flip,offensivecrouch,defensivecrouch}andanattackmovesetA ={lightpunch,
attack
mediumpunch,hardpunch,lightkick,mediumkick,hardkick},asshowninFigure2.
Each action will remain a number of frames according to users’ configuration. The
gamesalsohavespecialtechniquescalledcloseattack,i.e.,ThrowsandHolds,which
canbeappliedincertainregionsneartheopponent.
In addition to the standard move set, one signature element of fighting games is
specialmoves,whichisakindofpowerfulattackormaneuverthatrequirestheplayer
tofollowaspecificactionsequence(i.e.,sequentialkeyscombination,orcombination
ofkeyholdingandkeypressing),withanexampledepictedinFigure3. Thesemoves
usuallyhavespecialproperties(e.g., invincibilityframes, largercoverage, etc.) and
playacriticalroleinthestrategyanddepthofthegame. Theyareespeciallyusefulfor
higherlevelsofplay,fromwhichplayerscouldcreatecomplexcombosandoutperform
7Figure3: ExampleofspecialmovesforcharacterRyuinStreetFighterII(lefttoright):
Fireball,DragonPunch,HurricaneKick. ImagesareadaptedfromInstructionManual
ofStreetFighterII.
opponents. However,weobservethatlearningtoperformspecialmovesfromscratch
canbechallengingtobaselinealgorithms,asitrequirestheagenttomemorizeframes
and actions in previous steps and accurately perform the next action in the action
sequenceofspecialmoves. Moreover,thespecialmovescanbedifferentfromcharacter
to character, which increases the difficulty of the game. Therefore, to alleviate this
challenge,wealsoincludehard-codedspecialmovelistsasonepartoftheactionspace
sothattheagentcandirectlyaccessspecialmoveswithonesingleaction.
4.4 Rewards
Sparse Reward. Both sides of the agents are to maximize their win rate for each
roundofthegame. Thesparserewardr assigns+1forthewinnerand-1forthe
sparse
loserattheendofeachepisode. Inthesparserewardsetting,allfightinggamesare
two-playerzero-sumgames,whicharetheoreticallyguaranteedtoexistatleastoneNash
Equilibrium(Filar&Vrieze,2012),whichdirectlyinducesapairofnon-exploitable
policies.
WinRate. FortwoplayersAandB,policyπ winningagainstpolicyπ canbe
A B
defined as a reward relationship rA (π ,π ) > rB (π ,π ) in a single match,
sparse A B sparse A B
withrA andrB asthesparserewardforplayersAandBinthezero-sumsetting.
sparse sparse
Thewinrateisdefinedastheprobabilityofwinningasp(π ≻π ).
A B
ShapedDenseReward. Whilesparserewardisstraightforwardforevaluation,we
discoverthatbaselinealgorithmscouldnoteffectivelylearntobehavewellfromsuch
asparsesignal. Toaddressthisissue,weintroduceashapeddensereward r for
dense
training, which is a weighted sum of the hit-point damage inflicted by the agent on
theopponentandthedamageitreceives,togetherwithabonus(penalty)forwinning
(losing)thegame. SpecificformatofthisrewardreferstoAppendixA.1. Thedense
rewardr ischosentocoincidewiththewinrateofthepolicy,suchthatπ ≻π
dense A B
willalwaysleadtorA (π ,π )>rB (π ,π )inexpectation. Thedensereward
dense A B dense A B
also offers some flexibility, that the user can control the agent’s aggressiveness by
configuringtheweighingscalesintherewardfunction.
8Table1: FPSandmemoryusageofseveralopen-sourcedplatforms.
Environment Speed(FPS) Memory(MB)
FightLadder(Ours) 1935.76 195.46
SMACv2 146.72 876.96
PettingZooAtari 6268.18 32.13
DMLab2D 1144.27 47.41
4.5 Features
We remark on the following features of the proposed benchmark that could benefit
MARLresearch.
RichStrategySpace. Onekeyfeatureofourbenchmarkistherichstrategyspace
asthenatureoffightinggames,whichisparticularlybeneficialtothedevelopmentof
game-theoreticalalgorithms. Tonameafew,fightinggamesrequireplayerstoconsider
(a)characterdiversity: eachcharacterhasauniqueskillsetwithdifferentstrengths
andweaknesses,sooneneedstomasterthestrategyandcounter-strategyofallpossible
opponents, and even reason how to select and order characters when they have the
freedom to do so; (b) complexity of mechanics: fighting games are designed with
sophisticatemechanicssuchasinvincibilityframe,hitboxes,andcombosystems,which
arechallengingformicromanagementofcharacters;and(c)adversarialopponents:
opponents may progressively adapt their policies to players’ policies, thus finding
non-exploitablepoliciesiscrucialinmasteringfightinggames.
VariousDifficultyLevels. FightLadderprovidesseveralkindsofscenarios: single-
playermodeagainstoneCPUplayer(e.g.,sf_ryu_vs_ryu(cpu)),single-playermode
fullgame(e.g.,sf_ryu_full_game),two-playermode(e.g.,sf_ryu_vs_ryu),teammode
(supported in some games such as The King of Fighters). The difficulty levels are
increasing in this order, as two-player mode (no CPU) introduces additional non-
stationary(opponentscanbeadaptive),andteammodeoffersaricherstrategyspace.
Moreover,FightLaddersupportsspecifyingarbitrarydifficultylevelsofCPUsand
arbitrarycharactersforboththeplayeranditsopponent. Thisenrichesthefeaturesof
ourplatformandthediversityofstrategyspace.
Computational Efficiency. FightLadder also enjoys efficient computation for its
usage, and the comparison with several other popular game environments is shown
in Table 1. The frame rate is 13 times faster than SMACv2, with one-fourth usage
ofthememory. WhileitislessefficientthanFightLadderisthePettingZooAtari,it
providesmoregamecomplexity. Thebalanceofcomplexityandlowcomputationalcost
isimportantforevaluatingalgorithmsatscale.
Fidelity and Popularity. FightLadder allows testing agents in full-length fighting
games with an interface similar to human perception, thus providing a high-fidelity
evaluationofcompetitiveRLalgorithms. Moreover,fightinggameshavebeengaining
9popularity since they were released, making it easier to test the learned RL agents
againsthumanexpertplayers.
Open-SourceandCompatibility. FightLadderisdesignedforthebroadRLresearch
community,sowemakeeffortstoimprovetheeaseofusageandmakeitaccessibleto
allpotentialusers. ItiscompatiblewiththeGym(Brockmanetal.,2016)interfaceso
thatuserscanleverageoff-the-shelfRLalgorithmsimplementation.
Customization, Extension, andFlexibility. FightLadderisextremelyflexiblefor
configurationandextension. Forcustomization,theuserscancustomizeactionspaces
(human/transformedaction),rewardfunctions(sparse/tunableshapeddensereward),
numberofframestobeobservedperstep,aswellasaccesstoadditionalinformationto
helptraining. Moreover,ourplatformisbuiltuponpopularmoderngameemulatorsso
thatitiseasytoextendtoothergamesnotprovidedbyus. Specifically,itsupportsGym
RetroandMAMEToolkit,whichalreadysupportawiderangeofgames. Thisextension
capabilityofdiversegamesisprovidedbyourplatformwithminimalengineeringefforts.
Pleasecheckouropen-sourceproject1formoredetails.
5 Evaluation Metrics
VersusBuilt-InGameAIs. Directlycompetingwiththebuilt-inAIsofthegames
providesastraightforwardwayofmeasuringpolicyperformance. Typically,fighting
gamesofferahierarchicalstructureoflevels,enablingplayerstoadjustthedifficulty
setting(forexample,StreetFighterfeatureseightdistinctlevels). Thisstructureallows
fortheempiricalevaluationofthepolicyagainstthegame’sscriptedAIatvaryinglevels
ofchallenge. Itisimportanttoacknowledge,however,thatthelimitationsassociated
withhard-codedadversariesrestricttheextenttowhichthismetriccanaccuratelyreflect
thepolicy’srealcapability. Forbrevity,weshallrefertosuchagentsasCPU.
EloRatings. TheskillsofagentscanberankedthroughtheFIDEratingsystem(Elo&
Sloan,1978),whichisanincrementallearningsystemthatincreasestheEloofwinners
anddecreasestheElooflosers. ThelargerthedifferenceinElobetweenplayersAand
B,thehighertheprobabilitythattheplayerwiththehigherElo,A,beatstheplayer
withthelowerElo,B. TheEloscorecalculationtakesthefollowingprocedures:
First,theprobabilityofplayerAwinningisestimatedwith,
p
A
:=p(π
A
≻π
B)=(1.0+10EloB 4− 00EloA)−1.
ThentheEloratingforplayerAasElo willbeupdatedwithfollowingformula:
A
Elo =Elo +k·(1[winner=A]−p ),
A A A
wherekisaconstantofupdaterate. TheupdateissymmetricforplayerB,aswellas
anyotherplayerintherankingsystem.
1https://sites.google.com/view/fightladder/home
10VersusAIExploiters. AsdiscussedinSection3,exploitability(asDefinition3.3)
measuresthedistanceofapolicytotheNashequilibriumofthegame. Specifically,the
exploitabilityofapolicyµismeasuredbythewinrateofitsBRpolicyν†(µ)against
µ, since Vµ⋆,ν⋆(s ) = 0 for symmetric zero-sum game and Vµ,†(s ) = 1·p(ν ≻
1 2 1
µ)+0·p(ν ⪯ µ) = p(ν ≻ µ)forsparserewardsetting. Inpractice,wecanuseany
single-agentdeepRLalgorithmasanexploitertoapproximatelylearntheBRpolicy
ν†(µ).Forfaircomparisons,weshoulduseoneconsistentexploiter(sameRLalgorithm
withsameconfigurations)toevaluatetheexploitabilityofdifferentbaselines.
Versus Human Players. While Definition 3.3 is a general metric to measure ex-
ploitability,itmaybelimitedtothecapabilityofdeepRLalgorithmsinusage. There-
fore,wealsoprovideaninterfaceforhumanplayerssuchthattheycanplaywithany
learnedmodelwithconvenience. Thisfeaturewillshowthestrengthsandweaknesses
of agents directly and visibly, and motivate developers to improve their algorithms
tobemorenon-exploitableingeneral. GiventheremarkablesuccessofmodernRL
algorithmsoutperformingexperthumanplayersinvariousvideogames(Mnihetal.,
2013;Vinyalsetal.,2019;Berneretal.,2019),webelievethatFightLadderwillemerge
asapromisingplatformforthebroadcompetitiveMARLcommunityandresearchers
willeventuallybuildAIagentsthatcouldbeatworldchampionsinamuchrichersetof
strategicgameswithsignificantlylessengineeringefforts.
6 FightLadder-Baselines
Fortheconvenienceofthecommunitytoevaluateexistingmethodsandnewalgorithms
onFightLadderplatform,weopen-sourcetheimplementationofseveralstate-of-the-art
(SOTA)competitiveMARLalgorithms,includingindependentlearning(deWittetal.,
2020),two-timescalelearning(Daskalakisetal.,2020),fictitiousself-play(Heinrich
et al., 2015), policy-space response oracle (Lanctot et al., 2017) and league train-
ing(Vinyalsetal.,2019). Ourcodebasesupportsdecentralizedlearningacrossmultiple
GPUs, and it is built upon Stable-Baselines3 (Raffin et al., 2021) so that users can
leverage off-the-shelf implementations of RL algorithms. We choose proximal pol-
icyoptimization(PPO)(Schulmanetal.,2017)asthebackbonepolicyoptimization
algorithminourexperiments. MoredetailsofbaselinealgorithmsrefertoAppendixB.
7 Results
Inthissection,weprovidebenchmarkresultsonaselectedgameinFightLadder–the
StreetFighter. Weaimtoanswerthefollowingquestionsthroughourbenchmark: (a)
CanexistingRLalgorithmssolvethefullvideogameinthesingle-playerscenario?
(b)Howdoestheperformanceofstate-of-the-artbaselinealgorithmsinthetwo-player
competitivesettingcompare? and(c)Doesmulti-agenttraininghelptoimprovethe
non-exploitability?
11Level 1 (Guile) Level 2 (Ken) Level 3 (Chun-Li) Level 5 (Zangief) Level 6 (Dhalsim) Level 7 (Ryu)
1.0 1.0
0.5 0.5
0.0 0.0
Level 9 (E. Honda) Level 10 (Blanka) Level 11 (Balrog) Level 13 (Vega) Level 14 (Sagat) Level 15 (M. Bison)
1.0 1.0
0.5 0.5
0.00 10 20 0 10 20 0 10 20 0 10 20 0 10 20 0 10 20 0.0
Epoch Epoch Epoch Epoch Epoch Epoch
Figure4: Thewinratecurvesandschedulingdistributionbarplotinsf_ryu_full_game
viatheproposedPPOwithcurriculumlearning. Opponentsofdifferentcharactersare
markedwithdifferentlevels. Levels4,8,and12areomittedastheyarebonuslevels
withoutfighting.
7.1 Single-PlayerFullVideoGame
Toanswerquestion(a),weevaluatePPO’sperformanceinthescenariosf_ryu_full_game
withhumanactionspaceasafeasibilitycheck. AsmentionedinSection4,thissce-
nariorequirestheagenttolearnageneralizablepolicytocompeteagainstalldifferent
characters with increasing difficulty levels. Curriculum learning is applied to train
the policy from easy to hard cases. Furthermore, to improve learning efficiency we
developacurriculumschedulerforopponentsamplingtomatchwiththelearnerafter
eachepoch. Morespecifically,forthecurrentlearnerLwithpolicyπ ,wesampleits
L
opponentCfromtheentirecharactersetC,withthefollowinginverse-weightscheduling
distribution:
C ∼∆(C)∝1−p(π ≻π ),
L C
wherep(π ≻π )isthewinrateofthelearneragainsttheopponentand∆(·)isthe
L C
simplex. Intuitively,suchacurriculumwillencouragetheagenttofocusonthehardest
opponents,similarlytoprioritizedexperienceplay(Schauletal.,2015). Wedeferother
implementationdetailstoAppendixC.
Figure4showstheperformanceofourproposedmethodduringtraining. With20
epochsoftraining(eachepochinvolves10Mtrainingstepscompetingwithopponents
sampledfromthecurriculumschedulerinparallel),theagentiscapableofdefeating
charactersateachlevelwithawinratecloseto1. Inadditiontobeatingeachcharacter
withahighprobability,thetrainedpolicycancompletethefullvideogamewithover
0.6winrate,outperforminghumanplayerswithhoursofplayingexperience. Thisresult
showsthatexistingRLalgorithmscanalreadylearnawell-behavedpolicytosolvethe
fullsingle-playervideogame,whichprovidesagoodstartingpointforexploringthe
multi-agentsetting.
Asanadditionalexperiment,wealsotesttheinclusionofhard-codedspecialmove
listsinthissettingwithexactlythesamealgorithmimplementation. Althoughitcould
beeasierfortheagenttolearnmoreoffensivepolicies,significantimprovementinthe
12
etaR
niW
etaR
niW
.tsiD
eludehcS
.tsiD
eludehcSoverallwinrateisnotobserved. Itindicatesthattheagentswithoutencodedspecial
moves can also effectively learn policies against CPUs. Constantly playing special
moveswillleadtoavulnerablesituationfortheagent,whereasthedefensivestrategy
alsomattersgreatlyinthegame. Moreover,giventhatanexperiencedhumanplayercan
performspecialmoveseasily(byexecutingtheactionsequencesalmostinstantly),we
donotthinkthathard-codedspecialmovelistswillbecometheadvantageoftrained
agentsoverhumanplayers.
7.2 PerformanceofTwo-PlayerBaselineAlgorithms
To answer question (b), we evaluate five SOTA algorithms mentioned in Section 6:
independentPPO(IPPO),two-timescaleIPPO(2Timescale),fictitiousself-play(FSP),
policy-space response oracles (PSRO), and league training (League) in the scenario
sf_ryu_vs_ryu. IPPOand2Timescalecanbecategorizedintotheindependentlearning
paradigm,whileFSP,PSRO,andLeaguecanbecategorizedintothepopulation-based
learningparadigm. Foreachalgorithm,weinitializethepopulationofagentswitha
pretrainedpolicyinsf_ryu_vs_ryu(cpu)againstthemostdifficultCPU2.Weusethe
transformedactionsA withhard-codedspecialmovestounleashthefullpotential
trans
foragents.Asafaircomparison,weusethesamecodebase(FightLadder-Baselines)and
fixthehyperparametersofthebackbonePPOalgorithm. WetrainIPPOand2Timescale
for approximately 50M steps until the Elos saturate across all three seeds, FSP and
PSROforapproximately250Msteps,andLeagueforapproximately700Mstepsdueto
alargerpopulation. Asliceoftheleagueduringtheleaguetrainingprocessisvisualized
inFigure5PleaserefertoAppendixCformoreimplementationdetails.
Foreachalgorithm,wereportthetrainingElosofagentsinthepopulationduringthe
courseoftraining,respectively. TheresultsareshowninAppendixD,whichrevealthat
allbaselinealgorithmsareimprovingtheirpoliciesattheonsetoftraining.Subsequently,
IPPOand2TimescalegraduallyconvergeandoscillatearoundthepeakElos,where
FSP,PSRO,andLeaguecontinuetoincreasetheirscores. ThissuggeststhatIPPOand
2Timescalemaysufferfromoptimizationissuesduringtrainingandpopulation-based
methodsmaybemoresuitableforpolicylearninginfightinggames.
To compare different baseline algorithms, we select the top ten agents (five on
eachleftorrightside)fromeachalgorithmtoformanewpopulation, andcompute
the test Elos for this group of agents and CPU policies. We report the highest Elos
for each algorithm in Table 2 and the distribution of these agents’ Elos in Figure 6,
where we find that League and PSRO significantly outperform other baselines, and
population-basedmethodsdeliverbetterresultsthanindependentlearningcounterparts,
whichisalignedwithourpreviousobservationinspectingElosofbaselinesindividually.
Ontheotherhand,wenoticethatCPUpoliciesmaydefeatmostoftheagentsinthis
groupexceptforafewbest-performingagents,suggestingthatitisstillverychallenging
forexistingSOTAalgorithmstoreachanadvancedorsuperhumanlevelofperformance
inthesefightinggames. Wealsonoticedthattwosidesofagentsrevealasymmetric
strengthsintermsofElosinbothindividualevaluationforeachalgorithm(AppendixD
2Wedonotpre-traininsf_ryu_full_gameassf_ryu_vs_ryudoesnotrequireskillstocompetewithother
charactersratherthanRyu.
131.0
LE0_left 0.94 0.66 0.87 0.97 0.93 0.97
LE0_left_h_10M 0.09 0.10 0.08
LE0_left_h_20M 0.29 0.08 0.21 0.8
LE1_left 0.86 0.47 0.84 0.90 0.82 0.86
LE1_left_h_10M 0.21 0.03 0.04
0.6
LE1_left_h_20M 0.35 0.41 0.38
MA0_left 0.93 0.91 0.77 0.95 0.95 0.45 0.88 0.90
0.4
MA0_left_h_0M 0.03 0.02 0.00 0.02
MA0_left_h_10M 0.14 0.25 0.22 0.17
MA0_left_h_20M 0.25 0.26 0.25 0.29 0.2
ME0_left 0.43 0.98 1.00 0.06
ME0_left_h_10M 0.26 0.31 0.31
ME0_right_h_10M ME0_ri Mg Ah 0t _right_h_ M20 A0M _right_h_1 M0 AM 0_right_h_0M MA0_ri Lg Eh 1t _right_h_10M LE1_ri Lg Eh 0t _right_h_10M LE0_right 0.0
Figure5: ThepayoffmatrixforeachpairofagentsatacertainstageofLeaguetraining.
Forleaguetraining,thereisonemainagent(MA),twoleagueexploiters(LE0,LE1),
andonemainexploiter(ME)foreachside(leftorright).Thenameofeachrowindicates
theagentinformationasCharacter_Side_Checkpoint. Checkpoint=h_xM
representsahistoricalversionofagentsavedatxmillionsteps. Thevalueindicates
the win rate of the left (row) player against the right (column) player. For instance,
ME0_rightwinsallMA0_left_h_xMwithhighprobability,indicatingthatmain
exploitersintheleaguecanfullyexploitpreviousmainagents. Alsothehighwinrate
ofMA0_leftagainstallrightagents(exceptMA0_right)showsthatthemainagent
atcurrentstepsoutperformsotheragentsintheleague.
Figure10-14)andoverallevaluationsacrossalgorithms(Table2). Suchanimbalance
mayresultfromvariousfactors,forinstance,optimizinginstability,variancefromthe
populationorEloscomputation, etc, andcanbeaninterestingresearchquestionfor
futurework.
14Left
5
IPPO
League
4
2Timescale
3 PSRO
FSP
2 CPU
1
0
0 250 500 750 1000 1250 1500 1750
Right
6
4
2
0
0 250 500 750 1000 1250 1500 1750
Elo
Figure6: ThedistributionofEloratingsfortoptenagentsfromeachbaseline.
7.3 Non-ExploitabilityofTrainedAgents
Toanswerquestion(c),wemeasurethenon-exploitabilityofbaselinealgorithmsaccord-
ingtotheevaluationapproachesproposedinSection5. Morespecifically,wechoose
modelswiththehighestElosfromeachtwo-playerbaselinealgorithmrespectively,and
comparetheirexploitabilitywiththesingle-playerpretrainedmodelusedforinitializing
thepopulation-basedmethodsinSection7.2.
Thepracticalexploitabilityiscalculatedbysettingthetrainedpolicyfixedonone
side,anddeployingaPPOagentontheothersideasanexploiter. ThePPOexploiter
willbetraineduntilconvergence,andthesuccessrateoftheexploiteristheestimated
exploitabilityoftheoriginalpolicy,accordingtoDefinition3.3.
Single-AgentRLExploiters. WeusePPOasthealgorithmfortrainingexploiters,
givenitsdecentperformanceinbothsingle-playerandtwo-playerscenariosshownin
previousexperiments. Table3showstheexploitabilityofcomparingmethodsevaluated
15
tnuoC
tnuoCTable2: ComparisonoftrainingstepsandthebestEloratingsamongbaselines,with
CPU’sElosasreferences.
Method TrainingSteps(Left/Right) Elo(Left/Right)
IPPO 46M/46M 1082/1164
League 647M/630M 1682/1503
2Timescale 51M/46M 1080/919
PSRO 176M/161M 1262/1517
FSP 262M/244M 1079/1150
CPU N/A 1395/1541
Table3:Comparisonofmethods’exploitability. Alowernumberindicatestheevaluated
policyismorerobusttoexploitation.
Method Exploitability(Left/Right)
IPPO 0.96±0.03/0.91±0.03
League 0.94±0.05/0.94±0.00
2Timescale 0.96±0.02/0.90±0.05
PSRO 0.97±0.02/0.88±0.05
FSP 1.00±0.00/0.95±0.01
PPO 0.99±0.02/0.99±0.01
acrossthreeseeds,fromwhichweobservethatthesingle-playerpretrainedpolicyvia
PPOiseasiertoexploitandsuffersfromhigherexploitabilitythanalmostallselected
policies from two-player baselines. Therefore, this result indicates that two-player
learning algorithms such as League and PSRO can help to improve the robustness
of learned policies. On the other hand, the PPO exploiter eventually learns to beat
policiesfromallbaselines(withawinrategreaterthan0.5),whichmeansthatnoneof
thesealgorithmscanresultintheexactNashequilibriumpolicies,orevenclosetoit.
Therefore,closingthisgapisachallengingdirectionforfutureresearch.
Human Players as Exploiters. In addition to exploiting the learned models with
RL algorithms, we also attempt to exploit their policies with human effort. During
humanevaluations,theevaluatedmodelsrevealsomerobustnesstohumanplayers(e.g.,
defendwhenahumanplayerattacks),butsomesimplestrategies(e.g.,defensiveposture
combinedwithlowkicksatpropertiming)couldstilldefeatthemratherconsistently.
VisualizationsareprovidedinAppendixE.
Therefore,basedontwoexploitingexperiments,weobservethatexistingcompeti-
tiveMARLalgorithmsarefoundhardtolearnnon-exploitablestrategiesincompetitive
fightinggameslikeStreetFighter,thusraisinganewchallengefortheresearchcommu-
nity.
168 Conclusion and Limitation
Inthispaper,wepresenttheFightLadderplatformandevaluationbenchmarksasanovel
testbedforcompetitiveMARLresearch. Theplatformsupportsvariousvideoaction
gamesincludingthepopularStreetFighterseries,withflexiblesupportfornewgame
integration.
WefurtherprovideexperimentalevaluationsofpresentRLandMARLalgorithms
inbothsingle-playerandtwo-playermodesofonespecificgameStreetFighter. Inthe
single-playersetting,weproposedalearningschemebasedoncurriculumlearning. It
trainsageneralRLagentthatcanconsistentlybeatCPUsacrossdifferentcharacters. In
thetwo-playersetting,theEloratingandexploitabilitytestareconductedaspartofthe
proposedevaluationcriteria. OurimplementationofleaguetrainingandPSROprovides
strongeragentsthanFSPandIPPOintermsofEloratings. However,bothsingle-agent
RLandhumanplayersarecapableofexploitingallagentslearnedbycurrentwidely
adoptedalgorithms.
Ourcurrentworkislimitedtofullycompetitivetwo-playergames. Oneimportant
challenge of MARL is its diverse nature, which includes collaborative games, com-
petitive games, two-player games, and multiplayer games, all of which have rather
differentproblemstructuresandsolutionconcepts. Themoregeneralsetting,whichin-
volvesmorethantwoplayersandbothcooperationandcompetition,isnotyetexplored
andshouldbeanimportantfuturedirection. AlthoughFightLaddersupportsmultiple
fightinggames,ourcurrentresultsaremostlyconductedonStreetFighter,andweare
curioustoseemoreresultsonothergames.
This work motivates further research in developing more efficient and effective
self-play algorithms finding non-exploitable strategies. We hope that our platform
promptsgeneralinterestandmoreextensiveresearchincompetitiveMARLandserves
asastandardbenchmarkfordevelopingpracticallyusefulself-playtrainingparadigms.
17Acknowledgements
ThisworkwassupportedbyOfficeofNavalResearchN00014-22-1-2253,National
ScienceFoundationGrantNSF-IIS-2107304,andNationalScienceFoundationGraduate
ResearchFellowshipProgramunderGrantNo. DGE-2039656.
Impact Statement
ThisworkmayadvancethefieldofgameAI,thushaspotentialstoaffectthegaming
experienceforhumanplayers. ThestrongAIagentsforpopularfightinggamesmay
attract people’s attention to get involved in these games, or make them feel that the
gamescanbeevenmorechallengingforhuman. Anotherpositiveimpactisthatour
studypromotestheresearchforrobustsystemsagainstadversarialattacks.
References
Andrychowicz, M., Raichuk, A., Stan´czyk, P., Orsini, M., Girgin, S., Marinier, R.,
Hussenot, L., Geist, M., Pietquin, O., Michalski, M., et al. What matters in
on-policy reinforcement learning? a large-scale empirical study. arXiv preprint
arXiv:2006.05990,2020a.
Andrychowicz,O.M.,Baker,B.,Chociej,M.,Jozefowicz,R.,McGrew,B.,Pachocki,
J.,Petron,A.,Plappert,M.,Powell,G.,Ray,A.,etal. Learningdexterousin-hand
manipulation. TheInternationalJournalofRoboticsResearch,39(1):3–20,2020b.
Baker,B.,Kanitscheider,I.,Markov,T.,Wu,Y.,Powell,G.,McGrew,B.,andMordatch,
I.Emergenttoolusefrommulti-agentautocurricula.arXivpreprintarXiv:1909.07528,
2019.
Bard,N.,Foerster,J.N.,Chandar,S.,Burch,N.,Lanctot,M.,Song,H.F.,Parisotto,E.,
Dumoulin,V.,Moitra,S.,Hughes,E.,etal. Thehanabichallenge: Anewfrontierfor
airesearch. ArtificialIntelligence,280:103216,2020.
Beattie,C.,Köppe,T.,Duéñez-Guzmán,E.A.,andLeibo,J.Z. Deepmindlab2d. arXiv
preprintarXiv:2011.07027,2020.
Berner,C.,Brockman,G.,Chan,B.,Cheung,V.,De˛biak,P.,Dennison,C.,Farhi,D.,
Fischer,Q.,Hashme,S.,Hesse,C.,etal. Dota2withlargescaledeepreinforcement
learning. arXivpreprintarXiv:1912.06680,2019.
Brockman,G.,Cheung,V.,Pettersson,L.,Schneider,J.,Schulman,J.,Tang,J.,and
Zaremba,W. Openaigym. arXivpreprintarXiv:1606.01540,2016.
Brohan,A.,Brown,N.,Carbajal,J.,Chebotar,Y.,Dabis,J.,Finn,C.,Gopalakrishnan,
K.,Hausman,K.,Herzog,A.,Hsu,J.,etal. Rt-1:Roboticstransformerforreal-world
controlatscale. arXivpreprintarXiv:2212.06817,2022.
18Brown,G.W. Iterativesolutionofgamesbyfictitiousplay. Act.Anal.ProdAllocation,
13(1):374,1951.
Brown,N.andSandholm,T. Superhumanaiforheads-upno-limitpoker: Libratusbeats
topprofessionals. Science,359(6374):418–424,2018.
Brown,N.andSandholm,T. Superhumanaiformultiplayerpoker. Science,365(6456):
885–890,2019.
Daskalakis,C.,Foster,D.J.,andGolowich,N. Independentpolicygradientmethods
forcompetitivereinforcementlearning. Advancesinneuralinformationprocessing
systems,33:5527–5540,2020.
de Witt, C. S., Gupta, T., Makoviichuk, D., Makoviychuk, V., Torr, P. H., Sun, M.,
andWhiteson,S. Isindependentlearningallyouneedinthestarcraftmulti-agent
challenge? arXivpreprintarXiv:2011.09533,2020.
Ding,Z.,Su,D.,Liu,Q.,andJin,C.Adeepreinforcementlearningapproachforfinding
non-exploitablestrategiesintwo-playeratarigames.arXivpreprintarXiv:2207.08894,
2022.
Domahidi,A.,Chu,E.,andBoyd,S. ECOS:AnSOCPsolverforembeddedsystems.
InEuropeanControlConference(ECC),pp.3071–3076,2013.
Dresher, M., Shapley, L.S., andTucker, A.W. AdvancesinGameTheory.(AM-52),
Volume52,volume52. PrincetonUniversityPress,2016.
Elo,A.E.andSloan,S. Theratingofchessplayers: Pastandpresent. (NoTitle),1978.
Filar,J.andVrieze,K. CompetitiveMarkovdecisionprocesses. SpringerScience&
BusinessMedia,2012.
Foerster,J.,Farquhar,G.,Afouras,T.,Nardelli,N.,andWhiteson,S. Counterfactual
multi-agentpolicygradients. InProceedingsoftheAAAIconferenceonartificial
intelligence,volume32,2018.
Go,S.-X.,Jiang,Y.,andLoke,D.K.Aphase-changememristivereinforcementlearning
for rapidly outperforming champion street-fighter players. Advanced Intelligent
Systems,5(11):2300335,2023.
Heinrich,J.,Lanctot,M.,andSilver,D. Fictitiousself-playinextensive-formgames. In
Bach,F.andBlei,D.(eds.),Proceedingsofthe32ndInternationalConferenceon
MachineLearning,volume37ofProceedingsofMachineLearningResearch,pp.
805–813,Lille,France,07–09Jul2015.PMLR. URLhttps://proceedings.
mlr.press/v37/heinrich15.html.
Hu,J.andWellman,M.P. Nashq-learningforgeneral-sumstochasticgames. Journal
ofmachinelearningresearch,4(Nov):1039–1069,2003.
19Hu,J.,Jiang,S.,Harding,S.A.,Wu,H.,andLiao,S.-w. Rethinkingtheimplementation
tricksandmonotonicityconstraintincooperativemulti-agentreinforcementlearning.
arXivpreprintarXiv:2102.03479,2021.
Hu, S., Zhong, Y., Gao, M., Wang, W., Dong, H., Liang, X., Li, Z., Chang, X., and
Yang,Y. Marllib: Ascalableandefficientmulti-agentreinforcementlearninglibrary.
JournalofMachineLearningResearch,24(315):1–23,2023.
Khan,I.,VanNguyen,T.,Dai,X.,andThawonmas,R. Darefightingicecompetition: A
fightinggamesounddesignandaicompetition. In2022IEEEConferenceonGames
(CoG),pp.478–485.IEEE,2022.
Kurach,K.,Raichuk,A.,Stan´czyk,P.,Zaja˛c,M.,Bachem,O.,Espeholt,L.,Riquelme,
C., Vincent, D., Michalski, M., Bousquet, O., et al. Google research football: A
novelreinforcementlearningenvironment. InProceedingsoftheAAAIconference
onartificialintelligence,volume34,pp.4501–4510,2020.
Lanctot, M., Zambaldi, V., Gruslys, A., Lazaridou, A., Tuyls, K., Pérolat, J., Silver,
D.,andGraepel,T. Aunifiedgame-theoreticapproachtomultiagentreinforcement
learning. Advancesinneuralinformationprocessingsystems,30,2017.
Li,J.,Koyamada,S.,Ye,Q.,Liu,G.,Wang,C.,Yang,R.,Zhao,L.,Qin,T.,Liu,T.-Y.,
andHon,H.-W. Suphx: Masteringmahjongwithdeepreinforcementlearning. arXiv
preprintarXiv:2003.13590,2020.
Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and
Wierstra,D. Continuouscontrolwithdeepreinforcementlearning. arXivpreprint
arXiv:1509.02971,2015.
Lowe,R.,Wu,Y.I.,Tamar,A.,Harb,J.,PieterAbbeel,O.,andMordatch,I.Multi-agent
actor-critic for mixed cooperative-competitive environments. Advances in neural
informationprocessingsystems,30,2017.
McMahan,H.B.,Gordon,G.J.,andBlum,A.Planninginthepresenceofcostfunctions
controlledbyanadversary. InProceedingsofthe20thInternationalConferenceon
MachineLearning(ICML-03),pp.536–543,2003.
Mnih,V.,Kavukcuoglu,K.,Silver,D.,Graves,A.,Antonoglou,I.,Wierstra,D.,and
Riedmiller, M. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602,2013.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G.,
Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level
controlthroughdeepreinforcementlearning. nature,518(7540):529–533,2015.
Mohanty, S., Nygren, E., Laurent, F., Schneider, M., Scheller, C., Bhattacharya, N.,
Watson,J.,Egli,A.,Eichenberger,C.,Baumberger,C.,etal. Flatland-rl: Multi-agent
reinforcementlearningontrains. arXivpreprintarXiv:2012.05893,2020.
20Moravvcík,M.,Schmid,M.,Burch,N.,Lisy`,V.,Morrill,D.,Bard,N.,Davis,T.,Waugh,
K.,Johanson,M.,andBowling,M. Deepstack: Expert-levelartificialintelligencein
heads-upno-limitpoker. Science,356(6337):508–513,2017.
Mordatch,I.andAbbeel,P. Emergenceofgroundedcompositionallanguageinmulti-
agentpopulations. InProceedingsoftheAAAIconferenceonartificialintelligence,
volume32,2018.
Murphy, D. Hacking public memory: Understanding the multiple arcade machine
emulator. GamesandCulture,8(1):43–53,2013.
Nichol,A.,Pfau,V.,Hesse,C.,Klimov,O.,andSchulman,J. Gottalearnfast: Anew
benchmarkforgeneralizationinrl. arXivpreprintarXiv:1804.03720,2018.
Ouyang,L.,Wu,J.,Jiang,X.,Almeida,D.,Wainwright,C.,Mishkin,P.,Zhang,C.,
Agarwal,S.,Slama,K.,Ray,A.,etal.Traininglanguagemodelstofollowinstructions
withhumanfeedback. Advancesinneuralinformationprocessingsystems,35:27730–
27744,2022.
Palmas,A. Diambraarena: anewreinforcementlearningplatformforresearchand
experimentation. arXivpreprintarXiv:2210.10595,2022.
Pan,X.,Liu,M.,Zhong,F.,Yang,Y.,Zhu,S.-C.,andWang,Y. Mate: Benchmarking
multi-agentreinforcementlearningindistributedtargetcoveragecontrol. Advances
inNeuralInformationProcessingSystems,35:27862–27879,2022.
Papoudakis,G.,Christianos,F.,Schäfer,L.,andAlbrecht,S.V. Benchmarkingmulti-
agentdeepreinforcementlearningalgorithmsincooperativetasks. arXivpreprint
arXiv:2006.07869,2020.
Peng,B.,Rashid,T.,SchroederdeWitt,C.,Kamienny,P.-A.,Torr,P.,Böhmer,W.,and
Whiteson,S. Facmac: Factoredmulti-agentcentralisedpolicygradients. Advancesin
NeuralInformationProcessingSystems,34:12208–12221,2021.
Raffin,A.,Hill,A.,Gleave,A.,Kanervisto,A.,Ernestus,M.,andDormann,N. Stable-
baselines3: Reliablereinforcementlearningimplementations. JournalofMachine
Learning Research, 22(268):1–8, 2021. URL http://jmlr.org/papers/
v22/20-1364.html.
Rashid,T.,Samvelyan,M.,DeWitt,C.S.,Farquhar,G.,Foerster,J.,andWhiteson,S.
Monotonicvaluefunctionfactorisationfordeepmulti-agentreinforcementlearning.
TheJournalofMachineLearningResearch,21(1):7234–7284,2020.
Resnick, C., Eldridge, W., Ha, D., Britz, D., Foerster, J., Togelius, J., Cho, K., and
Bruna,J. Pommerman: Amulti-agentplayground. arXivpreprintarXiv:1809.07124,
2018.
Samvelyan,M.,Rashid,T.,DeWitt,C.S.,Farquhar,G.,Nardelli,N.,Rudner,T.G.,
Hung, C.-M., Torr, P.H., Foerster, J., andWhiteson, S. Thestarcraftmulti-agent
challenge. arXivpreprintarXiv:1902.04043,2019.
21Sarkar,B.,Talati,A.,Shih,A.,andSadigh,D. Pantheonrl: Amarllibraryfordynamic
traininginteractions.InProceedingsoftheAAAIConferenceonArtificialIntelligence,
volume36,pp.13221–13223,2022.
Schaul,T.,Quan,J.,Antonoglou,I.,andSilver,D. Prioritizedexperiencereplay. arXiv
preprintarXiv:1511.05952,2015.
Schrittwieser,J.,Antonoglou,I.,Hubert,T.,Simonyan,K.,Sifre,L.,Schmitt,S.,Guez,
A.,Lockhart,E.,Hassabis,D.,Graepel,T.,etal. Masteringatari,go,chessandshogi
byplanningwithalearnedmodel. Nature,588(7839):604–609,2020.
Schulman,J.,Wolski,F.,Dhariwal,P.,Radford,A.,andKlimov,O. Proximalpolicy
optimizationalgorithms. arXivpreprintarXiv:1707.06347,2017.
Shapley,L.S. Stochasticgames. Proceedingsofthenationalacademyofsciences,39
(10):1095–1100,1953.
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G.,
Schrittwieser,J.,Antonoglou,I.,Panneershelvam,V.,Lanctot,M.,etal.Masteringthe
gameofgowithdeepneuralnetworksandtreesearch. nature,529(7587):484–489,
2016.
Silver,D.,Hubert,T.,Schrittwieser,J.,Antonoglou,I.,Lai,M.,Guez,A.,Lanctot,M.,
Sifre,L.,Kumaran,D.,Graepel,T.,etal. Ageneralreinforcementlearningalgorithm
thatmasterschess,shogi,andgothroughself-play. Science,362(6419):1140–1144,
2018.
Song,Y.,Wojcicki,A.,Lukasiewicz,T.,Wang,J.,Aryan,A.,Xu,Z.,Xu,M.,Ding,
Z.,andWu,L. Arena: Ageneralevaluationplatformandbuildingtoolkitformulti-
agentintelligence. InProceedingsoftheAAAIconferenceonartificialintelligence,
volume34,pp.7253–7260,2020.
Suarez,J.,Du,Y.,Zhu,C.,Mordatch,I.,andIsola,P. Theneuralmmoplatformfor
massivelymultiagentresearch. arXivpreprintarXiv:2110.07594,2021.
Sukhbaatar,S.,Fergus,R.,etal. Learningmultiagentcommunicationwithbackpropa-
gation. Advancesinneuralinformationprocessingsystems,29,2016.
Sunehag,P.,Lever,G.,Gruslys,A.,Czarnecki,W.M.,Zambaldi,V.,Jaderberg,M.,
Lanctot,M.,Sonnerat,N.,Leibo,J.Z.,Tuyls,K.,etal.Value-decompositionnetworks
forcooperativemulti-agentlearning. arXivpreprintarXiv:1706.05296,2017.
Tan,M. Multi-agentreinforcementlearning: Independentvs.cooperativeagents. In
Proceedingsofthetenthinternationalconferenceonmachinelearning,pp.330–337,
1993.
Terry,J.,Black,B.,Grammel,N.,Jayakumar,M.,Hari,A.,Sullivan,R.,Santos,L.S.,
Dieffendahl,C.,Horsch,C.,Perez-Vicente,R.,etal.Pettingzoo:Gymformulti-agent
reinforcement learning. Advances in Neural InformationProcessing Systems, 34:
15032–15043,2021.
22Tesauro,G.etal. Temporaldifferencelearningandtd-gammon. Communicationsofthe
ACM,38(3):58–68,1995.
Vinyals, O., Babuschkin, I., Czarnecki, W.M., Mathieu, M., Dudzik, A., Chung, J.,
Choi,D.H.,Powell,R.,Ewalds,T.,Georgiev,P.,etal. Grandmasterlevelinstarcraft
iiusingmulti-agentreinforcementlearning. Nature,575(7782):350–354,2019.
Yao,Z.andDing,Z. Learningdistributedandfairpoliciesfornetworkloadbalancing
asmarkovpotentialgame. AdvancesinNeuralInformationProcessingSystems,35:
28815–28828,2022.
Yu, C., Velu, A., Vinitsky, E., Gao, J., Wang, Y., Bayen, A., and Wu, Y. The sur-
prisingeffectivenessofppoincooperativemulti-agentgames. AdvancesinNeural
InformationProcessingSystems,35:24611–24624,2022.
Zhang,H.,Feng,S.,Liu,C.,Ding,Y.,Zhu,Y.,Zhou,Z.,Zhang,W.,Yu,Y.,Jin,H.,and
Li,Z. Cityflow: Amulti-agentreinforcementlearningenvironmentforlargescale
citytrafficscenario. InTheworldwidewebconference,pp.3620–3624,2019.
Zheng,L.,Yang,J.,Cai,H.,Zhou,M.,Zhang,W.,Wang,J.,andYu,Y. Magent: A
many-agentreinforcementlearningplatformforartificialcollectiveintelligence. In
ProceedingsoftheAAAIconferenceonartificialintelligence,volume32,2018.
Zhou, M., Luo, J., Villella, J., Yang, Y., Rusu, D., Miao, J., Zhang, W., Alban, M.,
Fadakar, I., Chen, Z., et al. Smarts: Scalable multi-agent reinforcement learning
trainingschoolforautonomousdriving. arXivpreprintarXiv:2010.09776,2020.
Zhou, M., Wan, Z., Wang, H., Wen, M., Wu, R., Wen, Y., Yang, Y., Yu, Y., Wang,
J., andZhang, W. Malib: Aparallelframeworkforpopulation-basedmulti-agent
reinforcementlearning. JournalofMachineLearningResearch,24(150):1–12,2023.
23A Details of FightLadder
A.1 DenseReward
Theshapeddenserewardforthei-thagentatsteptisdefinedasfollows:
r =α[λ(HP −HP )−(HP −HP )+r ], (1)
i,t −i,t−1 −i,t i,t−1 i,t i,bonus
whereαisascalingfactor,HP denotesagenti’shit-pointatsteptandλcontrolthe
i,t
aggressivenessoflearnedagents,and−idenotestheopponentagent. Attheendofthe
game,theagentiwillreceiveabonusrewardr ,whichispositivelycorrelatedto
i,bonus
HP ifitwinsandnegativelycorrelatedtoHP ifitloses. Bydefault,wechooseλ=3
i −i
inSF2,FF2,andMK,andλ=1inSF3andKOF97,fortheconsiderationofpractical
performances.
A.2 GameSettings
Table4illustratestheobservation,action,andrewardsaswellasotherelementsinthe
environment for all supported games — Street Fighter II (SF2), Fatal Fury 2 (FF2),
MortalKombat(MK),StreetFighterIII(SF3),andTheKingofFighters’97(KOF97).
Table4: SpecificationofsupportedgamesinFightLadder.
SF2 FF2 MK SF3 KOF97
Observation(Pixels) 100×128×3 112×128×3 112×160×3 112×192×3 112×192×3
HumanActionSupported Yes Yes Yes Yes Yes
TransformedActionSupported Yes Yes Yes No No
ShapedDenseReward Yes Yes Yes Yes Yes
DefaultFramesPerStep 8 8 8 3 3
DefaultFramesStacked3 12 12 12 9 9
HPs,Countdown, HPs,Countdown HPs,Countdown, HPs HPs,Countdown,
AdditionalAvailableInfo
Scoreboard,Positions Scoreboard Positions,PowerStatus
A.3 ComparisonofMARLGamePlatforms
Table5comparesourFightLadderwithseveralpopularMARLgameplatformsmostly
focusingoncompetitivesettings,intermsofobservationspace,actionspace,whether
baselinemethodsareincludedandthenumberofagentsingames. Fortheobserva-
tionspace,‘Continuous’indicatesavector-formlatentstateinformationofthegame
withcontinuousnumericalvalues,and‘Image’indicatesvisualRGBinformationas
observations. PommerMan(Resnicketal.,2018)usesgridenvironmentsthereforeits
observation only has discrete values. For the action space, most of the games only
involvesdiscreteactionvaluesexceptforArena(Songetal.,2020). Forthenumber
ofagentsintheseplatforms,MPEprovidediversecompetitivesettingslike1v1,1vN,
1v1v1andsoon. MAgentincludes1millionagentscompetingagaintseachother,and
forNeuralMMO(Suarezetal.,2021)thenumberofagentsis256or1024. Theteam
modeinourFightLadderandArenasupportsthecompetitivesettingsoftwoteams,
whereeachteamincludesmultiplecharacterstobecontrolledbyoneteampolicyor
separateagentpolicies.
3Weuniformlysamplethestackedframesasobservationstoimprovethecomputationalefficiency.
24Table5: ComparisonofpopularMARLgameplatforms.
Env ObservationSpace ActionSpace Baselines #Agents
MPE(Mordatch&Abbeel,2018) Continuous Discrete Yes 1v1,1vNand1v1v1...
MAgent(Zhengetal.,2018) Continuous+Image Discrete Yes 1million
Arena(Songetal.,2020) Continuous+Image Continuous/Discrete Yes 1v1,NvNandteammode
NeuralMMO(Suarezetal.,2021) Continuous Discrete Yes 256and1024
PettingZooAtari(Terryetal.,2021) Continuous+Image Discrete No 1v1
PommerMan(Resnicketal.,2018) Discrete Discrete No 2v2
FightLadder(Ours) Continuous+Image Discrete Yes 1v1andteammode
B Baseline Algorithms of FightLadder-Baselines
IndependentLearning(IPPO). Independentlearningisastraightforwardextension
ofsingle-agentRLintoMARL.Itdecomposesthejointoptimizationintoindividual
ones for each agent while regarding all other agents as part of the environment. It
canbeimplementedeasilybysimultaneouslyrunningsingle-agentRLalgorithmsfor
eachplayer. Theoretically,thisindependentlearningparadigmsuffersfromsuboptimal-
ity(Tan,1993;Foersteretal.,2018),becausetheenvironmentbecomesnon-stationary
whileotheragentsareupdatingtheirpolicies. However, recentwork(deWittetal.,
2020;Yuetal.,2022)findsthatwithmodesthyperparametertuning,IPPOcanserve
asastrongbaselinecomparedtootherstate-of-the-artalgorithmsinsomecooperative
MARLtasks.
Two-timescaleLearning(2Timescale). Two-timescalelearningfollowstheindepen-
dentlearningparadigm,butrequirestwoplayerstoupdategradientsaccordingtothe
two-timescalerule,i.e.,oneplayerusesamuchsmallerstepsizethantheotherone.
Asaresultofthismodification,two-timescalelearningenjoyssomenicetheoretical
properties—itisproventhatundersomemildassumptions,independentpolicygradi-
entalgorithmssatisfyingtwo-timescaleconvergetoaNashequilibriumintwo-player
zero-sumstochasticgames(Daskalakisetal.,2020).
Population-BasedMethods. Theindependentlearningframeworkisonlytraining
agents against the current version of their opponents, which may fail or converge
slowlyduetothelackofdiversity(Dresheretal.,2016). Population-basedmethods
areproposedtoincreasepolicydiversitybymaintainingapoolofpoliciesinprevious
iterations,andusingthemasacurriculumtoupdatethecurrentpolicy.Morespecifically,
fort-thupdate,theagentµt playswithpreviousversionsofitsopponentν˜sampled
fromthemeta-strategyρ ,whichisadistributionoverν0,ν1,...,νt−1. Algorithm1
ν
presentsthepseudo-codeforgeneralpopulation-basedmethods. Withdifferentchoices
ofsamplingdistribution,wecanrecoverseveralstate-of-the-artbaselines:
• FictitiousSelf-Play(FSP),whereρ istheuniformdistribution(Heinrichetal.,
ν
2015): Uniform(ν0,ν1,...,νt−1).
• Policy-SpaceResponseOracles(PSRO),where(µ˜,ν˜)aresampledfromthe
meta-strategy(ρ ,ρ )bysolvingNashequilibriumofthepayoffmatrixgame
µ ν
betweenµ0,µ1,...,µt−1andν0,ν1,...,νt−1(Lanctotetal.,2017).
25• LeagueTraining(League),wherethreetypesofagents—mainagents,league
exploiters,andmainexploiters,areintroducedintothepopulation. Mainagents
trainagainstthemselvesaswellasallpreviousversionsofagentsinthepopulation;
leagueexploiterstrainagainstallpreviousagents;andmainexploitersoptimize
thebestresponseofmainagents. Eachtypeofagentadoptsadifferentsampling
distributionwhichisamixtureofself-playandprioritizedfictitiousself-play. We
referreadersto(Vinyalsetal.,2019)formoreimplementationdetails.
Algorithm1Population-BasedMethodsforMGs
1: Initializepoliciesµ0 ={µ h},ν0 ={ν h},h∈[H]
2: Initializepolicysets: µ={µ0},ν ={ν0}
3: Initializemeta-strategies: ρ µ =[1.],ρ ν =[1.]
4: fort=1,...,T do
5: ift%2==0then
6: νt = BEST_RESPONSE(ρ µ,µ)
7: ν =ν(cid:83) {νt}
8: Updateρ ν accordingtospecificalgorithms
9: else
10: µt = BEST_RESPONSE(ρ ν,ν)
11: µ=µ(cid:83) {µt}
12: Updateρ µaccordingtospecificalgorithms
13: endif
14: endfor
15: Returnµ,ρ µ,ν,ρ ν
C Experiment Details
C.1 Hyperparameters(Table6and7)
C.2 TrainingDetails
Figure7,8,and9reportthepayoffmatrixofpolicieswithinthepopulationforFSP,
PSRO,andLeague, respectively, withthevaluerepresentingthewinrateoftheleft
playeragainsttherightplayer. Wetrainedallouragentsononeserverwith192CPUs
and8A6000GPUs.
26Hyperparameters Value
featureextractor CNN(Mnihetal.,2015)
rolloutstepsforeachenvironment 512
batchsize 1024
epochsperupdate 4
γ 0.94
GAEλ 0.95
learningrate linearschedulefrom2.5e-4to2.5e-6
clippingrange linearschedulefrom0.15to0.025
advantagenormalization True
entropycoefficient 0.0
gradientclipping 0.5
valuefunctioncoefficient 0.5
Table6:TraininghyperparametersforPPO,whichisthebackboneforbothsingle-player
andtwo-playeralgorithmsintheexperiment.
FSP PSRO League
#envsperlearner 24 #envsperlearner 24 #envsperlearner 24
stepsforBR 10M stepsforBR 10M stepsforBR 10M
totalsteps 50M totalsteps 250M totalsteps 700M
#mainagent 1 #mainagent 1 #mainagent 1
Nashsolver ECOS #mainexploiter 1
(Domahidietal.,2013) #leagueexploiter 2
Table7: TraininghyperparametersforFSP,PSRO,andLeague. Weomitthedetails
ofLeague’sopponentschedulinghereasitstrictlyfollowsthepseudocodeprovided
in(Vinyalsetal.,2019).
D Individual Elo Results
D.1 IPPO(Figure10)
D.2 2Timescale(Figure11)
D.3 FSP(Figure12)
D.4 PSRO(Figure13)
D.5 League(Figure14)
E Visualization of Human Exploiters
Figure 15 visualizes how human players can exploit learned models with a simple
strategy. Fullvideosareprovidedinthesupplementarymaterial.
271.0 1.0
FSP0_left 0.91 0.99
0.8 0.8
FSP0_left 0.62
0.6 0.6
FSP0_left_h_0M 0.00
0.4 0.4
FSP0_left_h_0M 0.52
0.2 FSP0_left_h_10M 0.11 0.2
FSP0_right_h_0M FSP0_right 0.0 FSP0_right_h_10M FSP0_right_h_0M FSP0_right 0.0
1.0 1.0
FSP0_left 0.76 0.84 0.95 0.99 1.00
FSP0_left 0.99 0.99 0.99
0.8 FSP0_left_h_0M 0.00 0.8
FSP0_left_h_0M 0.00
FSP0_left_h_10M 0.03
0.6 0.6
FSP0_left_h_10M 0.05 FSP0_left_h_20M 0.07
0.4 0.4
FSP0_left_h_30M 0.06
FSP0_left_h_20M 0.08
0.2 FSP0_left_h_40M 0.22 0.2
FSP0_left_h_30M 0.48
FSP0_left_h_50M 0.40
FSP0_right_h_20M FSP0_right_h_10M FSP0_right_h_0M FSP0_right 0.0 FSP0_right_h_40M FSP0_right_h_30M FSP0_right_h_20M FSP0_right_h_10M FSP0_right_h_0M FSP0_right 0.0
1.0 1.0
FSP0_left 0.64 0.88 0.94 0.96 0.96 0.97 1.00 FSP0_left 0.96 0.97 0.94 0.97 1.00 1.00 1.00 1.00
FSP0_left_h_0M 0.00
FSP0_left_h_0M 0.01 0.8 0.8
FSP0_left_h_10M 0.02
FSP0_left_h_10M 0.02
FSP0_left_h_20M 0.03
0.6 0.6
FSP0_left_h_20M 0.04 FSP0_left_h_30M 0.02
FSP0_left_h_30M 0.06 FSP0_left_h_40M 0.11
0.4 0.4
FSP0_left_h_50M 0.19
FSP0_left_h_40M 0.16
FSP0_left_h_60M 0.09
FSP0_left_h_50M 0.21 0.2 0.2
FSP0_left_h_70M 0.04
FSP0_left_h_60M 0.04 FSP0_left_h_80M 0.50
FSP0_right_h_60M FSP0_right_h_50M FSP0_right_h_40M FSP0_right_h_30M FSP0_right_h_20M FSP0_right_h_10M FSP0_right_h_0M FSP0_right 0.0 FSP0_right_h_70 FSM P0_right_h_60 FSM P0_right_h_50 FSM P0_right_h_40 FSM P0_right_h_30 FSM P0_right_h_20 FSM P0_right_h_10 FM SP0_right_h_0M FSP0_right 0.0
Figure7: FSPdetails(trainingorderfromtoplefttobottomright): ForFSP,thereisone
agentforeachside(leftorright). Thenameofeachrowindicatestheagentinformation
asCharacter_Side_Checkpoint. Checkpoint=h_xMrepresentsaprevious
versionofagentsavedatxmillionsteps. Thevalueindicatesthewinrateoftheleft
(row)playeragainsttheright(column)player.
281.0 1.0
PSRO0_left 1.00 0.96
0.8 0.8
PSRO0_left 0.63
0.6 0.6
PSRO0_left_h_0M 0.02 0.65 0.02
0.4 0.4
PSRO0_left_h_0M 0.65 0.47
0.2 PSRO0_left_h_10M 0.31 0.98 0.03 0.2
PSRO0_right_h_0M PSRO0_right 0.0 PSRO0_right_h_10M PSRO0_right_h_0M PSRO0_right 0.0
1.0 1.0
PSRO0_left 0.93 0.98 1.00 0.96 PSRO0_left 0.95 0.92 0.89 0.99 1.00 0.96
0.8 PSRO0_left_h_0M 0.02 0.01 0.02 0.07 0.02 0.65 0.04 0.8
PSRO0_left_h_0M 0.02 0.07 0.02 0.65 0.04
PSRO0_left_h_10M 0.06 0.01 0.03 0.02 0.31 0.98 0.04
0.6 0.6
PSRO0_left_h_10M 0.03 0.02 0.31 0.98 0.04 PSRO0_left_h_20M 0.08 0.00 0.01 0.63 0.99 0.93 0.01
0.4 0.4
PSRO0_left_h_30M 0.04 0.01 0.07 0.93 1.00 0.96 0.00
PSRO0_left_h_20M 0.01 0.63 0.99 0.93 0.01
0.2 PSRO0_left_h_40M 0.16 0.39 0.87 0.94 0.94 0.99 0.07 0.2
PSRO0_left_h_30M 0.07 0.93 1.00 0.96 0.00 PSRO0_left_h_50M 0.08 0.97 0.90 0.82 0.91 0.98 0.06
PSRO0_right_h_30M PSRO0_right_h_20M PSRO0_right_h_10M PSRO0_right_h_0M PSRO0_right 0.0 PSRO0_right_h_50M PSRO0_right_h_40M PSRO0_right_h_30M PSRO0_right_h_20M PSRO0_right_h_10M PSRO0_right_h_0M PSRO0_right 0.0
1.0 1.0
PSRO0_left 0.95 0.98 0.92 0.88 1.00 1.00 0.96 PSRO0_left 0.92 0.91 0.97 0.98 0.92 0.94 1.00 1.00 0.96
PSRO0_left_h_0M 0.09 0.02 0.01 0.02 0.07 0.02 0.65 0.04 PSRO0_left_h_0M 0.00 0.00 0.09 0.02 0.01 0.02 0.07 0.02 0.65 0.04
0.8 0.8
PSRO0_left_h_10M 0.35 0.06 0.01 0.03 0.02 0.31 0.98 0.00 PSRO0_left_h_10M 0.02 0.00 0.35 0.06 0.01 0.03 0.02 0.31 0.98 0.00
PSRO0_left_h_20M 0.04 0.06 0.22 0.08 0.00 0.01 0.63 0.99 0.93 0.01
PSRO0_left_h_20M 0.22 0.08 0.00 0.01 0.63 0.99 0.93 0.01 0.6 0.6
PSRO0_left_h_30M 0.05 0.05 0.18 0.04 0.01 0.07 0.93 1.00 0.96 0.00
PSRO0_left_h_30M 0.18 0.04 0.01 0.07 0.93 1.00 0.96 0.00
PSRO0_left_h_40M 0.04 0.04 0.02 0.16 0.39 0.87 0.94 0.94 0.99 0.01
PSRO0_left_h_40M 0.02 0.16 0.39 0.87 0.94 0.94 0.99 0.01 0.4 0.4
PSRO0_left_h_50M 0.02 0.06 0.23 0.08 0.97 0.90 0.82 0.91 0.98 0.03
PSRO0_left_h_50M 0.23 0.08 0.97 0.90 0.82 0.91 0.98 0.07 PSRO0_left_h_60M 0.03 0.09 0.12 1.00 0.91 0.65 0.53 0.79 0.94 0.01
0.2 0.2
PSRO0_left_h_60M 0.12 1.00 0.91 0.65 0.53 0.79 0.94 0.10 PSRO0_left_h_70M 0.19 0.08 0.96 0.99 0.92 0.86 1.00 0.98 0.98 0.18
PSRO0_left_h_70M 0.96 0.99 0.92 0.86 1.00 0.98 0.98 0.17 PSRO0_left_h_80M 0.23 0.93 0.84 0.91 0.93 0.87 0.98 0.99 1.00 0.04
PSRO0_right_h_6 P0 SM RO0_right_h_5 P0 SM RO0_right_h_4 P0 SM RO0_right_h_3 P0 SM RO0_right_h_2 P0 SM RO0_right_h_10 PM SRO0_right_h_0M PSRO0_right 0.0 PSRO0_right_h_ P8 S0 RM O0_right_h_ P7 S0 RM O0_right_h_ P6 S0 RM O0_right_h_ P5 S0 RM O0_right_h_ P4 S0 RM O0_right_h_ P3 S0 RM O0_right_h_ P2 S0 RM O0_right_h_1 P0 SM RO0_right_h_0M PSRO0_right 0.0
Figure8: PSROdetails(trainingorderfromtoplefttobottomright): ForPSRO,there
isoneagentforeachside(leftorright). Thenameofeachrowindicatestheagentin-
formationasCharacter_Side_Checkpoint. Checkpoint=h_xMrepresents
apreviousversionofagentsavedatxmillionsteps. Thevalueindicatesthewinrateof
theleft(row)playeragainsttheright(column)player.
291.0 1.0
LE0_left 0.53 0.93
LE0_left 0.61
LE0_left_h_10M 0.15 0.19 0.40
0.8 0.8
LE1_left 0.54 LE1_left 0.25 0.73
0.6 LE1_left_h_10M 0.16 0.27 0.30 0.6
MA0_left 0.58 0.56 0.57 MA0_left 0.85 0.91 0.97 0.80
0.4 MA0_left_h_0M 0.08 0.03 0.00 0.02 0.4
MA0_left_h_0M 0.48 0.52 0.53 MA0_left_h_10M 0.61 0.34 0.50 0.58
0.2 ME0_left 0.97 0.06 0.2
ME0_left 0.48
ME0_left_h_10M 0.44 0.71 0.70
ME0_right MA0_right_h_0M MA0_right LE1_right LE0_right 0.0 ME0_right_h_10M ME0_right MA0_right_h_0M MA0_right LE1_right LE0_right 0.0
1.0 1.0
LE0_left0.94 0.66 0.87 0.97 0.93 0.97 LE0_left0.890.86 0.340.890.940.97 0.780.93 0.680.720.94
LE0_left_h_10M 0.09 0.06 0.01
LE0_left_h_10M 0.09 0.10 0.08
LE0_left_h_20M 0.07 0.09 0.70
LE0_left_h_20M 0.29 0.08 0.21 0.8 LE0_left_h_30M 0.06 0.22 0.79 0.8
LE1_left0.86 0.47 0.84 0.90 0.82 0.86 LE1_left0.940.91 0.760.920.940.98 0.910.98 0.810.880.90
LE1_left_h_10M 0.10 0.02 0.72
LE1_left_h_10M 0.21 0.03 0.04 0.6 LE1_left_h_20M 0.09 0.38 0.06 0.6
LE1_left_h_20M 0.35 0.41 0.38 LE1_left_h_30M 0.28 0.54 0.83
MA0_left0.93 0.91 0.77 0.95 0.95 0.45 0.88 0.90 MA0_left0.840.940.910.800.850.900.980.430.900.92 0.840.880.94
MA0_left_h_0M 0.03 0.02 0.00 0.02 0.4 MA0_left_h_0M 0.16 0.02 0.05 0.02 0.4
MA0_left_h_10M 0.30 0.14 0.13 0.76
MA0_left_h_10M 0.14 0.25 0.22 0.17 MA0_left_h_20M 0.28 0.11 0.12 0.76
MA0_left_h_20M 0.25 0.26 0.25 0.29 0.2 MA0_left_h_30M 0.44 0.16 0.28 0.87 0.2
ME0_left 0.310.670.280.820.06
ME0_left 0.43 0.98 1.00 0.06
ME0_left_h_10M 0.12 0.17 0.76
ME0_left_h_10M 0.26 0.31 0.31 ME0_left_h_30M 0.06 0.05 0.70
ME0_right_h_10M ME0_ri Mg Ah 0t _right_h_ M20 A0M _right_h_1 M0 AM 0_right_h_0M MA0_ri Lg Eh 1t _right_h_10M LE1_ri Lg Eh 0t _right_h_10M LE0_right 0.0 ME0_right_ Mh E_ 03 _r0 iM ght_h_10M ME M0 A_r 0i _g riht ght_ Mh A_ 03 _r0 iM ght_ Mh A_ 02 _r0 iM ght_h M_ A1 00 _rM ight_h_0M MA L0 E_r 1i _g riht ght_ Lh E_ 12 _r0 iM ght_h_10M LE L1 E_r 0i _g riht ght_ Lh E_ 03 _r0 iM ght_ Lh E_ 02 _r0 iM ght_h_10M LE0_right 0.0
LE0_left0.960.91 0.610.560.780.910.99 0.660.870.93 0.840.870.98 1.0 LE0_left0.620.870.88 0.310.310.310.610.880.89 0.570.800.90 0.410.750.730.83 1.0
LE0_left_h_10M 0.05 0.10 0.19 L LE E0 0_ _l le ef ft t_ _h h_ _1 20 0M M 0 0. .0 05 6 0 0. .0 09 5 0 0. .3 16 4
LE0_left_h_20M 0.06 0.05 0.15 LE0_left_h_30M 0.09 0.05 0.14
LE0_left_h_30M 0.06 0.11 0.14 0.8 LE0_left_h_50M 0.07 0.26 0.38 0.8
LE1_left0.940.96 0.500.910.960.940.99 0.610.950.92 0.920.900.92 LE1_leftL _E h1 _1_l 0e Mft0.920.960.92 0.630.830.900.940.971.00 0.040.850.990.95 0.110.860.920.970.95 0.29
LE1_left_h_10M 0.05 0.11 0.23 LE1_left_h_20M 0.08 0.14 0.44
LE1_left_h_20M 0.09 0.27 0.24 LE1_left_h_30M 0.11 0.12 0.40
LE1_left_h_30M 0.14 0.26 0.31 0.6 L LE E1 1_ _l le ef ft t_ _h h_ _4 50 0M M 0 0. .1 10 1 0 0. .1 37 1 0 0. .5 69 1 0.6
LE1_left_h_40M 0.17 0.31 0.37 LE1_left_h_60M 0.26 0.36 0.83
MA0_left0.950.940.910.810.870.930.970.980.510.750.930.94 0.880.840.94 MA0_left0.930.950.940.910.790.870.860.940.950.970.480.920.970.97 0.910.960.960.98
MA0_left_h_0M 0.13 0.02 0.03 0.14 0.4 MM AA 0_0 l_ el fe tf _t h_ _h 1_ 00 MM 0 0. .1 41 4 0 0. .0 11 1 0 0. .0 13 1 0 0. .2 40 4 0.4
MA0_left_h_10M 0.17 0.13 0.15 0.31 MA0_left_h_20M 0.22 0.05 0.15 0.40
MA0_left_h_20M 0.12 0.09 0.15 0.25 MA0_left_h_30M 0.29 0.07 0.28 0.55
MA0_left_h_30M 0.27 0.17 0.34 0.36 M MA A0 0_ _l le ef ft t_ _h h_ _4 50 0M M 0 0. .3 57 2 0 0. .2 19 9 0 0. .3 58 5 0 0. .6 76 4
MA0_left_h_40M 0.32 0.29 0.63 0.43 0.2 MA0_left_h_60M 0.75 0.40 0.69 0.85 0.2
ME0_left 0.530.650.870.910.990.06 ME0_left 0.640.690.630.810.770.950.06
ME0_left_h_10M 0.10 0.26 0.34 M ME E0 0_ _l le ef ft t_ _h h_ _1 30 0M M 0 0. .1 01 7 0 0. .1 01 4 0 0. .5 12 8
ME0_lef Mt E_ 0h _ri_ g3 ht0 M_ EM h 0_ _3 ri0 gM ht_h_10M M ME A0 0_r _i rig ght ht M_ Ah 0_ _4 ri0 gM ht M_ Ah 0_ _3 ri0 gM ht M_ Ah 0_ _2 ri0 gM ht_ Mh A_ 01 _0 riM ght_h_0M M LA E0 1_r _i rig0 ght. ht1 L_ E3 h 1_ _4 ri0 gM ht L_ Eh 1_ _2 ri0 gM ht_h_10M L LE E1 0_r _i rig0 ght. ht0 L_ E6 h 0_ _3 ri0 gM ht L_ Eh 0_ _2 ri0 gM ht_h_10M LE0_rig0 ht.22 0.0 ME0_le Mf Et 0_ _h ri_ g Mh5 Et 0_0 _h rM i_ g5 Mh0 Et 0_M _h ri_ g3 h0 t_M h_ M1 M0 AE 0M _0 r_ iri g Mg h Ath 0_t _h ri_ g M5 h0 At 0_M _h ri_ g M4 h0 At 0_M _h ri_ g M3 h0 At 0_M _h ri_ g2 h M0 t A_M 0h __ ri1 g0 hM t_h_ LM0 EA 1M _0 r_ iri g Lg h E0 th 1_t. _1 h ri_1 g4 Lh0 Et 1_M _h ri_ g2 h0 t_M h_1 LL0 EE 0M _1 r_ iri g Lg h E0 th 0_t. _2 h ri_3 g5 Lh0 Et 0_M _h ri_ g3 Lh0 Et 0_M _h ri_ g2 h0 t_M h_1 L0 EM 0_rig0 ht.55 0.0
Figure 9: League training details (training order from top left to bottom right): For
leaguetraining,thereisonemainagent(MA),twoleagueexploiters(LE0,LE1),and
onemainexploiter(ME)foreachside(leftorright). Thenameofeachrowindicates
theagentinformationasCharacter_Side_Checkpoint. Checkpoint=h_xM
representsapreviousversionofagentsavedatxmillionsteps. Thevalueindicatesthe
winrateoftheleft(row)playeragainsttheright(column)player.
30IPPO: Left IPPO: Left IPPO: Left
1.0
0.8 5 25 2 3 3MM MM 1 21 21 91 9 M MM M 1 31 37 57 5 M MM M 1500 8 S Se ee ed d 1 2 0.6 4411 MM 4477 MM 6 Seed 3 1000 0.4 4
0.2 500 2
0.0 600 800 1000 1200 1400 1600 1800 0 0 1 2 3 4 5 0 400 600 800 1000120014001600
Elo Steps 1e7 Elo
IPPO: Right IPPO: Right IPPO: Right
1.0 55 MM 1111 MM 1177 MM 8 Seed 1
00 .. 68 2 42 43 13 1 M MM M 2 42 49 79 7 M MM M 3355 MM 1500 6 S Se ee ed d 2 3
1000 4 0.4
0.2 500 2
0.0 400 600 800 1000 1200 1400 1600 0 0 1 2 3 4 5 0 400 600 800 1000120014001600
Elo Steps 1e7 Elo
Figure10: TheEloratingforthepopulationofagentstrainedwithIPPOalgorithm.
Theupperthreeplotsareforleft-sideplayerandthebottomthreearefortheright-side
player. TheEloratingisplottedagainstthewinningrateovermatchedpolicies(left
figures),trainingsteps(middlefigures)andthenumberofpolicies(rightfigures).
2Timescale: Left 2Timescale: Left 2Timescale: Left
1.0
55 MM 1133 MM 2211 MM 1500 15 Seed 1
0.8 2299 MM 3377 MM 4455 MM Seed 2
0.6 5533 MM 6611 MM 1000 10 Seed 3
0.4
500 5
0.2
0.0 400 600 800 1000 1200 1400 1600 0 0 1 2 3 4 5 0 400 600 800 1000120014001600
Elo Steps 1e7 Elo
2Timescale: Right 2Timescale: Right 2Timescale: Right
1.0 55 MM 1133 MM 2211 MM 1500 10 Seed 1
0.8 2299 MM 3377 MM 4455 MM 8 Seed 2 0.6 5533 MM 6611 MM 1000 6 Seed 3
0.4 4
500
0.2 2
0.0200 400 600 800 1000 1200 1400 0 0 1 2 3 4 5 0 400 600 800 1000120014001600
Elo Steps 1e7 Elo
Figure11:TheEloratingforthepopulationofagentstrainedwith2Timescalealgorithm.
Theupperthreeplotsareforleft-sideplayerandthebottomthreearefortheright-side
player. TheEloratingisplottedagainstthewinningrateovermatchedpolicies(left
figures),trainingsteps(middlefigures)andthenumberofpolicies(rightfigures).
31
etaR niW
etaR
niW
etaR
niW
etaR
niW
olE
olE
olE
olE
tnuoC
tnuoC
tnuoC
tnuoCFSP: Left FSP: Left FSP: Left
1.0 55 MM 3388 MM 7711 MM 1500 3 FSP
0.8 110044 MM 113377 MM 117700 MM
0.6 220033 MM 223366 MM 1000 2
0.4
500 1
0.2
0.0 400 600 800 1000 1200 1400 00.0 0.5 1.0 1.5 2.0 2.5 0 400 600 800 1000120014001600
Elo Steps 1e8 Elo
FSP: Right FSP: Right FSP: Right
1.0 55 MM 3388 MM 7711 MM 1500 3 FSP
0.8 110044 MM 113377 MM 117700 MM
0.6 220033 MM 223366 MM 1000 2
0.4
500 1
0.2
0.0400 600 800 1000 1200 1400 00.0 0.5 1.0 1.5 2.0 2.5 0 400 600 800 1000120014001600
Elo Steps 1e8 Elo
Figure12: TheEloratingforthepopulationofagentstrainedwithFSPalgorithm. The
upperthreeplotsareforleft-sideplayerandthebottomthreearefortheright-sideplayer.
TheEloratingisplottedagainstthewinningrateovermatchedpolicies(leftfigures),
trainingsteps(middlefigures)andthenumberofpolicies(rightfigures).
PSRO: Left PSRO: Left PSRO: Left
1.0 55 MM 3366 MM 6677 MM 1500 3 PSRO
0.8 9988 MM 112299 MM 116600 MM
0.6 119911 MM 222222 MM 1000 2
0.4
500 1
0.2
0.0400 600 800 1000 1200 00.0 0.5 1.0 1.5 2.0 0 400 600 800 1000120014001600
Elo Steps 1e8 Elo
PSRO: Right PSRO: Right PSRO: Right
1.0 55 MM 3366 MM 6677 MM 1500 8 PSRO
00 .. 68 9 19 18 98 9 1 1MM MM 1 21 22 22 29 29 2 M MM M 116600 MM 1000 6
4 0.4
500
0.2 2
0.0 400 600 800 1000 1200 1400 00.0 0.5 1.0 1.5 2.0 2.5 0 400 600 800 1000120014001600
Elo Steps 1e8 Elo
Figure13: TheEloratingforthepopulationofagentstrainedwithPSROalgorithm.
Theupperthreeplotsareforleft-sideplayerandthebottomthreearefortheright-side
player. TheEloratingisplottedagainstthewinningrateovermatchedpolicies(left
figures),trainingsteps(middlefigures)andthenumberofpolicies(rightfigures).
32
etaR
niW
etaR
niW
etaR
niW
etaR
niW
olE
olE
olE
olE
tnuoC
tnuoC
tnuoC
tnuoCLeague: Left League: Left League: Left
1.0 25
55 MM 111166 MM 1500 MA0
0.8 222277 MM 333388 MM 20 ME0
0.6 444499 MM 556600 MM 1000 15 L LE E0 1
0.4 10
500
0.2 5
0.0 600 800 1000 1200 1400 0 0 1 2 3 4 5 6 7 0 400 600 800 1000120014001600
Elo Steps 1e8 Elo
League: Right League: Right League: Right
1.0
55 MM 111166 MM 1500 30 MA0
0.8 222277 MM 333388 MM ME0
0.6 444499 MM 556600 MM 1000 20 L LE E0 1
0.4
500 10
0.2
0.0600 800 1000 1200 0 0 1 2 3 4 5 6 7 0 400 600 800 1000120014001600
Elo Steps 1e8 Elo
Figure14: TheEloratingforthepopulationofagentstrainedwithLeaguetraining.
Theupperthreeplotsareforleft-sideplayerandthebottomthreearefortheright-side
player. TheEloratingisplottedagainstthewinningrateovermatchedpolicies(left
figures),trainingsteps(middlefigures)andthenumberofpolicies(rightfigures).
Figure15: Demonstrationoftheexploitingstrategyofonehumanplayer. Thehuman
player(Ryuontherightinwhite)defendswhentheAIopponent(Ryuontheleftin
gray)attacks,andinflictsdamagewithlowkicks.
33
etaR
niW
etaR
niW
olE
olE
tnuoC
tnuoC