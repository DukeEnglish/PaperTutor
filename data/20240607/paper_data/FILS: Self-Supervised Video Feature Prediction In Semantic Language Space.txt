FILS:SELF-SUPERVISEDVIDEOFEATUREPREDICTIONINSEMANTICLANGUAGESPACE1
FILS: Self-Supervised Video Feature
Prediction In Semantic Language Space
MonaAhmadian UniversityofSurrey
m.ahmadian@surrey.ac.uk Guildford,UK
FrankGuerin
f.guerin@surrey.ac.uk
AndrewGilbert
a.gilbert@surrey.ac.uk
Abstract
Thispaperdemonstratesaself-supervisedapproachforlearningsemanticvideorepre-
sentations. Recent vision studies show that a masking strategy for vision and natural
languagesupervisionhascontributedtodevelopingtransferablevisualpretraining. Our
goalistoachieveamoresemanticvideorepresentationbyleveragingthetextrelatedto
thevideocontentduringthepretraininginafullyself-supervisedmanner. Tothisend,
wepresentFILS,anovelself-supervisedvideoFeaturepredictionInsemanticLanguage
Space(FILS).Thevisionmodelcancapturevaluablestructuredinformationbycorrectly
predictingmaskedfeaturesemanticsinlanguagespace. Itislearnedusingapatch-wise
video-text contrastive strategy, in which the text representations act as prototypes for
transforming vision features into a language space, which are then used as targets for
semantically meaningful feature prediction using our masked encoder-decoder struc-
ture. FILS demonstrates remarkable transferability on downstream action recognition
tasks,achievingstate-of-the-artonchallengingegocentricdatasets,likeEpic-Kitchens,
Something-SomethingV2, Charades-Ego, and EGTEA, using ViT-Base. Our efficient
methodrequireslesscomputationandsmallerbatchescomparedtopreviousworks.
1 Introduction
Self-supervised pretraining, mainly through masked reconstruction, has demonstrated sig-
nificantsuccessinnaturallanguageprocessing[7,14]andmorerecentlyinvideo[52]. Ad-
ditionally, using web text for self-supervision in visual learning has been promising [43],
with applications extending to video [69]. Video, however, has some unique characteris-
tics: dense data with frame-to-frame redundancy and crucial action segments pivotal for
comprehension. Therefore, applying techniques borrowed from the image domain may be
suboptimalduetoalackoffocusontheessentialpartoftheframe. Ourchallengeliesinef-
fectivelyintegratingrecentlydevelopedself-supervisionconceptssuchasmaskingandtext
supervisiontailoredforvideorepresentation.
Addressing text supervision, early approaches relied on labeled text from supervised
datasets, restricting models to predefined categories and labels [55]. Recent advancements
haveshiftedtowardsleveragingLargeLanguageModels(LLMs)andvision-languagemod-
elsfortextsupervision,eitherthroughatextbag[34]ordensecaptions[69]. Toeffectively
apply text supervision, contrastive learning on related images and captions, such as CLIP,
©2024.Thecopyrightofthisdocumentresideswithitsauthors.
Itmaybedistributedunchangedfreelyinprintorelectronicforms.
4202
nuJ
5
]VC.sc[
1v74430.6042:viXra2FILS:SELF-SUPERVISEDVIDEOFEATUREPREDICTIONINSEMANTICLANGUAGESPACE
hasproveneffectiveinbuildingpowerfulrepresentations[43]. Nevertheless,theircounter-
parts in the video domain do not show the same generality [41, 55] and do not consider
temporality,akeyaspectforunderstandingavideo.
Definingtheobjectiveforreconstructingmaskedvideoposesachallenge.Tongetal.[52]
initiallyperformedreconstructioninpixelspacewithmaskedautoencoders. However, Tan
etal.[51]suggestreconstructinginthelatentspaceofaquantized-variationalautoencoder
to avoid overfitting on low-level visual information. Similarly, Assran et al. [4] found that
learninginahighersemanticspaceleadsamodeltolearnmoresemanticfeaturesinimages.
Yang et al. [62] take this to language semantics, reconstructing image patches by mapping
themtoadistributionovertextualfeaturestopredictthesemanticsofmaskedpatcheswithin
alinguisticcontext. Theyemployimage-textcontrastivelearningandincorporatearecon-
structive loss. However, their random selection of image patches may not be optimal for
videorepresentationlearning,whereonlycertainpartsofthescenecapturetheactivitythat
shouldalignwiththevideo’stextcaption. Ifweconcentratecontrastivelearningoncrucial
video parts for activity comprehension, video-text alignment can happen more efficiently,
emphasizingessentialsemanticfeaturesnecessaryforcomprehensivevideounderstanding.
Theimpressiveprogressintheseresearchdirectionsencouragedustoconsiderthefol-
lowing:Canwemergeamaskingstrategyandlanguageguidancetoenhancevisualpretrain-
ing? Self-supervisedobjectivescanoperatewithinalatentspaceestablishedwithlanguage,
maintaining the alignment of language with learned visual representations. This improves
theinterpretabilityandsemanticaspectsoftherepresentations. Astraightforwardapproach
toachievingthisobjectiveisintegratingamaskingstrategywithvideo-textcontrastivelearn-
ing for multi-task learning. Still, this simple combination cannot fully cover the potential
synergiesbetweenthesetwoobjectives.
Video clipMasking Recon Feature
Masking Prediction
Video clip Contra Text Contra
Video clip
Masking Recon Text
Video clip
Contra Text
Figure 1: Architecture comparisons between MAE, CLIP, MAE+CLIP, and FILS. Contra
indicatesvideo-textcontrastiveloss. Theredarrowpointstothelanguagespace,whilethe
blackonesindicatetheknowledgeflowinthevisionspace.
Weaddressthesecombinedissuesbydrawingonthebestpracticesfromrelatedworks
anddevelopingFILStoinheritthebenefitsoftheseapproaches. FILSconductsvideofea-
turepredictioninthesemanticlanguagespace,completelyself-supervised. Weusenatural
descriptionsprovidedbyanoff-the-shelfvideocaptionmodelfortextsupervision[65]and
train our video encoder in the CLIP language space [43] employing a contrastive learning
objective to learn high-level semantic features. We aim to predict masked video features
withinthissharedspace. Wenarrowourfocusonthevideoregionswheresignificantaction
unfoldsbyidentifyingactionareausingamethodthatdetectsmotionfromopticalflowbut
ignores camera motion [1]. We introduce ActCLIP, which can exclusively utilize patches
fromtheseregionsduringcontrastivelearning,aligningthemcloselywithlanguageembed-
dingsofthevideocaptions. Thisstrategyensuresthatourmodelcapturestheessenceofthe
depicted activities, emphasizing objects and actions relevant to the identified action areas.
Whilecontrastivelearningbenefitsfromlargerbatchsizes, ourActCLIPshowspromisein
surpassingtheCLIPperformanceforvideorepresentationlearningwhilemaintainingbatch
sizes and computation requirements low. In Fig. 1, you can see the semantic comparisons
aera
noitcAFILS:SELF-SUPERVISEDVIDEOFEATUREPREDICTIONINSEMANTICLANGUAGESPACE3
betweenMAE,CLIP,asimplemaskingstrategywithvideo-textcontrastivelearningtermed
MAE+CLIP,andourproposedFILS.Ourmethodoutperformsallpreviousself-supervised
learning techniques involving ViT-b and has excellent generalization capability for down-
streamtaskslikeactionrecognition. Thefollowingbrieflydescribesourprimarycontribu-
tions:
• Anewself-supervisedmethodforvideorepresentationlearningfromunlabeledvideos
andtheircaptions;afeaturepredictionstrategyonmaskedvideoandpatch-wisecon-
trastive learning within potential action areas that enriches our video encoder with
richer,abstractrepresentations.
• In ActCLIP, we integrate motion’s importance for action identification in videos by
contrastivelearningbetweenpatchesinrecognizedactionregionsandassociatedtex-
tualcontext.
• Demonstratingtheeffectivenessofusingmutualinformationtoprioritizepatchesthat
conveysemanticandaction-relateddetails,enhancingtheutilityoflearnedrepresen-
tationsacrossvarioustasks,includingachievingbetterperformanceinactionrecogni-
tionthroughfeatureprediction-basedvisualrepresentations.
2 Related Works
Videoself-supervisedlearningVideounderstandingisafundamentalaspectofvisualrecog-
nition,involvingtheanalysisofvideoinputs. Videounderstandingtaskslikeactionclassi-
fication, detection, and object segmentation [1, 25, 29, 50, 52] require handling complex
spatial and temporal data. Rather than building representations using costly and limited
human-defined annotations, self-supervised learning extracts discriminative video features
from unlabeled data and avoids expensive annotations. The standard vision-language pre-
training pipeline, which involves initial pretraining and finetuning afterwards, is designed
to develop a general multimodal feature representation suitable for multiple downstream
tasks[4,5,52,58]. Recentgenerativeself-supervisedmethods[24,58,60]haveadvanced
the field. Masked Auto-Encoders (MAEs) [20, 24, 52] randomly mask input pixel patches
andreconstructthembyminimizingreconstructionerrorsinpixelspace,showingcompeti-
tiveperformancewhenfinetuned. Otherworkreconstructsdirectlyinthelatentspace[4,5]
oraimstopredictcontextualizedlatentrepresentationscontaininginformationfromtheen-
tireinputthroughmaskedprediction[5].
Vision-languagerepresentationlearningInmultimodalvideo-languagelearningandalso
language-guidedvideocomprehension,incorporatinglanguagealongsidevideos[3,18,28,
50, 51, 67, 69] has introduced many intriguing challenges. Numerous attempts have been
undertakentomergecomputervisionandlanguage,usingthecombinedknowledgeforvar-
iousmultimodalapplications. Earlyworkexploredlossfunctionsandarchitecturestograsp
semanticvision-languagealignments[54, 70]. Evenbeforedeeplearningbecamepopular,
early research investigated the process of learning visual representations from image cap-
tions [42]. Vision-language pretraining [37, 43, 49, 50] has become prevalent for learning
transferablemultimodalrepresentationstoenhancevideo-texttaskslikecaptioning[66],vi-
sualquestionanswering[39],andreferringsegmentation[63,64]andothers.Beforemasked
autoencoders,contrastivelearningjointlylearnedvisionandlanguagerepresentations.Ano-
tableexampleisContrastiveLanguage-ImagePretraining(CLIP)[43], whichalignsimage
andtextembeddingsviacontrastiveloss, achievingsupervised-comparableresults. Itused
modality-specificencodersprojectingtoasharedembeddingspacewithimage-textpairsas
targets.Worklike[25]combinedCLIP’slanguageguidancewithself-supervisedcontrastive
learning for semantically-aligned pixel embeddings, boosting finetuning, and showcasing
language’sgeneralityinsupervisingvision. Recentworks[15,30,62]exploreincorporating
maskingintosuchpretraining,withstrongresultswhenfinetunedonextensivelabeleddata.4FILS:SELF-SUPERVISEDVIDEOFEATUREPREDICTIONINSEMANTICLANGUAGESPACE
a person is putting
a blue lid on a bowl
cv Bw/ patches within the
cv action area and text
Time Time
//
Language space
GT feature corresponding
Downsampled video clip Tube masking with an extremely high ratio to the masked patches
Action area and finding the action area Target video features
Keeping Masking Exponential Moving Average (EMA)
Figure2:Overviewofourmethod.Weperformself-supervisedfeaturepredictionandvideo-
textcontrastivelearningsimultaneously. Theredarrowdenotesthefeaturesofthepatches
withintheactionarea.
3 Self-Supervised Video Feature Prediction in Semantic
Language Space
We introduce FILS, a framework for deeper video understanding that leverages a unified
embeddingspacethatintegratesvideoandnaturallanguageformultimodallearning. Fig.2
presentsanoverviewofourmethod;ithastwoobjectives: 1)FeaturePrediction,wherethe
inputismaskedandencoded(thestudentmodewithrandominitialisation),andthepredictor
predicts representations. Then, we create representations of all the input data, which are
meanttobetargetedforthelearningtask(theteachermode). AswewilldiscussinSec.3.1,
topreventthecollapse,theteachertracksstudentparameters,andtheirweightsarederived
from the exponentially moving average of the student [22, 23]. 2) ActCLIP, an auxiliary
CLIP-based self-supervised objective performing contrastive learning between motion or
actionareapatchesandrelevanttext,aligningvideoandlanguagespacestolearnsemantic
context. MoreinformationisinSec.3.2.
3.1 ModelArchitecture
Video Encoder. In our approach, we cast video representation learning as a feature pre-
dictionobjectiveinvolvingcomparingpredictedfeaturesfromthepredictorandtheircorre-
sponding ground truth features. To ensure stable representation during this task, we adopt
ateacher/studentapproach. Asistypicalwithself-supervisedworks[52,53],weutilisean
encoder(Enc )tocomputetherepresentationofthemaskedinputusingtheunmasked
VStudent
(remaining) patches. As tube modelling has demonstrated superior capability in capturing
temporalandspatialinformationcomparedtoframemodeling[52],weutilisetubeembed-
ding for a video clip to further concentrate on spatiotemporal saliency. The input to the
studentencoderismaskedwithahighproportionofvideo(V)patchesusingspatiotemporal
tubemasking.Aftermasking,thestudentvisionencoderencodestheunmaskedpatches(V ),
u
resultingina fu embedding. Thegroundtruthofmaskedpatchesinthevideoinput(V )is
m
encodedbyateacherencoder,resultingina fm embedding. Eq.1formulatestheprocessof
visionencoders:
fu=Enc (V ), u∈[1,N ]
VStudent u u
(1)
fm=Enc (V ), m∈[1,N ]
VTeacher m m
mandudemonstratethepatchindexrelatedtomaskedandunmaskedpatches. N andN
u m
...
snekot
elbisiV
...
...
... ... ... ...FILS:SELF-SUPERVISEDVIDEOFEATUREPREDICTIONINSEMANTICLANGUAGESPACE5
indicatethenumberofunmasked(visible)patchesandthenumberofmaskedpatchesinthat
order.
Predictor. Thismoduleismadeupoftransformerblocks. Usingalearnable[MASK]token
andtheunmaskedencodedfeature f ofthemaskedvideoasinputs, itdecodesorpredicts
u
the features of the masked patches from a masked view. This results in pm, the predicted
featuresofthemaskedinput,whereN indicatesthenumberofunmasked(visible)patches.
u
pm=Predictor(fu,[MASK]), u∈[1,N ], (2)
u
TeacherParameterization.Toavoidrepresentationcollapse,anexponentiallymovingaver-
age(EMA)ofthestudentmodelparametersθ,withthemodel’sweightsintargetweight∆,is
usedtoparameterizetheteachermodeltoencodethevideoclippatches:∆←τ∆+(1−τ)θ.
Weemployascheduleforτ thatincreasesthisparameterlinearlythroughoutthefirstτ up-
n
dates, from τ to the target value τ . The value is then maintained constant for the rest of
0 e
the training. When the model is random at the beginning of training, this strategy leads to
morefrequentupdatestotheteacher;however,whensuitableparametershavealreadybeen
learned,thefrequencyofupdatestotheteacherdecreases.
Text Encoder. To enable the integration of textual information with our vision encoder,
we utilise a text encoder that converts textual inputs into a latent representation for joint
processing. Astackoftransformerlayerstokenizestherepresentationofinputtext(T)into
theglobalrepresentationoftheinputtexts,h=Enc (T).
L
3.2 TrainingObjectives
We propose two loss functions in our frameworks: a contrastive video-to-text loss and a
featurepredictionloss. Theselossesarecrucialinaligningvideoandtextmodalitieswithin
thelanguageembeddingspace,fosteringeffectivecross-modalunderstanding.
ActCLIP.Recognisingtheimportanceofmotioninself-supervisedvideotechniquesforac-
tionrecognition,weintegrateitthroughcontrastivelearningbetweenpatcheswithinmotion
or action area and relevant text, resulting in a unified language-vision space fostering co-
herent content understanding. Motion areas are identified following an approach in [1] to
representmotion preciselyandare termedaction areasinour work. Then, tolearn thepa-
rametersofthesharedvison-languageembeddingspace,weleverageapatch-wisevideo-text
contrastivelossbetweenpatcheswithintheactionareatoalignbothmodalitiesinashared
embedding space. Given this contrastive objective happens in the detected action area, we
calledthisActCLIP.Initially, wetakethemean-pooledvideofeatureofpatcheswithinthe
detectedmotionarea:
1 Na
f¯= ∑ fa (3)
N
i=1
aindicatespatchesinsidetheactionarea,N isthenumberofpatchesinthedetectedaction
a
area,and faistheencodedfeaturerepresentationofthepatchesina. Thevideofeaturesare
furthermappedbyprojectionheadtothelanguagespaceandnormalisationonthem,andthe
textfeaturecomesafter:
zV =∥θ(f¯)∥,
(4)
zT =∥h∥
∥.∥ and θ(.) denote the normalisation operation and the projection head (mapping) from
videototext,respectively.6FILS:SELF-SUPERVISEDVIDEOFEATUREPREDICTIONINSEMANTICLANGUAGESPACE
Thebidirectionalcontrastivelossbetweenvideoandtextcanberepresentedasfollows:
1 B exp(⟨zV,zT⟩/σ)
L =− ∑log i i ,
V2T B ∑B exp(⟨zV,zT⟩/σ)
i=1 j=1 j j
(5)
1 B exp(⟨zT,zV⟩/σ)
L =− ∑log i i
T2V B ∑B exp(⟨zT,zV)⟩/σ)
i=1 j=1 j j
σ is a learnable parameter that is cooperatively trained throughout the pretraining; B de-
notesthebatchsize,andiandjrepresenttheindexinsideamini-batch. Thefollowingisa
formulationofthetotallossofvideo-textcontrastivelearning:
1
L = (L +L ) (6)
ActCLIP V2T T2V
2
Feature Prediction in Language Space. Our reconstruction space is built by taking a
vision-language perspective. We perform masked visual reconstruction in this language
space,usingthetextfeaturesasnaturalsemanticinformationforthevideopatches. There-
fore,giventheencodedpatchfeaturesfromtheTeacherencodercorrespondingtothemasked
patches(fm)andpredictedpatchfeatures(pm)(reconstructedones),wheremistheindexof
themaskedpatch,wefirstmapandnormalisebothfeaturestothelanguagefeaturespace:
p˜m=∥θ(pm)∥, g˜m=∥θ(fm)∥ (7)
i i i i
where i indicates data in the batch and θ(.) represents the same vision mapping in Eq. 4.
So, both the masked prediction and its corresponding target are mapped into this language
semanticspace.OurproposedlossfunctionforthepredictionisFeaturePrediction(FP)loss,
whichistheaverageL1distancebetweenthemappedpredictedpatch-levelrepresentations
via predictor p˜m, and the mapped target (ground truth) patch-level representation coming
fromtheTeacherencoderg˜m;
1 Nm
L =D(p˜,g˜)= ∑∥(p˜,g˜)∥ (8)
FP i i 1
N
mi=1
Overall Objective Function. The final objective of FILS is a combination of video-text
contrastive loss and feature prediction loss. λ and λ adjust the weighting between our
1 2
proposedcontrastiveloss(ActCLIP)andfeaturepredictionloss(FP).
L =λ L +λ L (9)
FILS 1 ActCLIP 2 FP
4 Experiments
WeassessthequalityoftherepresentationslearntbyFILSonthechallengingactionrecog-
nitiontask, whichisdifficulttoautomatedespitebeingeasyforhumans. Ourexperiments
showFILSachievessuperiorperformanceacrossallmetricsontheutilizedactionrecogni-
tion dataset, demonstrating how vision self-supervision enhances the vision-language con-
trastiveapproach. Furtheranalysis, datasetdetails, metrics, andimplementationdetailsare
providedinthesupplementarymaterial.FILS:SELF-SUPERVISEDVIDEOFEATUREPREDICTIONINSEMANTICLANGUAGESPACE7
Table1: PerformanceofactionrecognitiononEK100andSSV2. FILSoutperformsall
priorworksregardingaction-leveltop-1accuracy. Inthetablebelow,p-dataandLmention
pretrainingdatautilizedtheincorporationoflanguageduringthetraining,respectively.
(a)Epic-Kitchens
(b)Something-SomethingV2
Verb Noun Action
Method Backbone p-data L Method Backbone p-data L Top-1
Top-1 Top-1 Top-1
VV O iMS M Vi dAL Fdl m eMiTo e Vf ae IV I oo Mw n Vo LPS IT Mir i OLM TF i Svm S V V L Aoa (Nw i[e F a os r T[ 5 [ Er et i E3 u6 [ [7n *[[ 62 6[ r1[]31 [ 51[ 9] s8]2 [87 3 9 )9 ] ] 5]]] 5 ]] 2] ] R R Me SS Te V V VVVVs wws I SN T i i iiiiN 3 T T TTTT ii FVe nn D - - --e -- -t -- - B B BBLL Bt1 BB B50 01 II NI W WN N W+- - I IK2 2 EET TI TKKK K1N 1 4 KK+ +SK k 444 60- E E 11-+1 000 00+ 6 00g gK 000 0K +K 0 00o oS M4 4 44 U0 D D0 00 N ✓ ✓ ✓ ✓ ✓× × × × × × × × 76 6 6 6 6 6 6 7 6 6 7 25 7 7 7 6 8 9 1 9 9 0 - .. . . . . . . . . . . 26 9 1 8 4 6 5 4 9 0 0 65 4 5 5 5 5 6 6 6 5 5 10 9 7 7 6 1 1 0 3 8 9 - .. . . . . . . . . . . 70 0 6 0 8 2 7 3 9 4 8 4 53 3 4 4 4 4 4 4 5 4 4 8 18 8 4 6 4 1 9 8 0 6 9 . .. . . . . . . . . . . 5 05 3 1 1 0 0 9 4 5 9 1 VT iV V O dOViS M Vi i em m Fd dBl mI oiTo f e eM Ie V n MEow no LoS S iir V iMP MM TF f A Svm S Ao T oa (w A EAe Fr os C r[ m r [ et i E3 uEE Vn 5[[ 2 [e r[3 61 [ 5 [ 2[ 1r [] s2 258 ]7 3 1 )9[ ] [02]] 5 ]6 ] 5]]]] 3] R Re S SSe VV VVV VV Vs w wwsN ii iii ii iN TT TTT TT Ti iie n nn -- --e - -- -t - -- BB BBL LL Lt1 B BB50 01 UI I II I nHN N NN N lao- - -- -2 1I b1w2 2N S SKKK 1 K eK1 1T S SlK- k k444 eo2 + V V++ +000 d+ 11 000 K K H2 2K K0 SK 04 S 4 y4 4M0 0 bv0 00 0 r20 0 id ✓ ✓× × × × × × × × × × × 6 6 6 6 6 6 6 7 7 6 7 7 73 3 2 8 9 5 8 0 0 9 1 1 2. . . . . . . . . . . . .1 4 4 1 6 9 1 6 8 5 4 2
1
Table2: Charades-EgoandEGTEAActionRecognition. FILSachievessubstantialim-
provementsinthistask,outperformingpriorworks,whileCharades-EgoandEGTEAvideos
visuallydifferfromEK100videos,whichFILSispretrainedon. Thetableshowsp-dataand
L,referringtothepretrainingdatautilizedandlanguageincorporationduringtraining. Note
thatthe*modelhasbeentrainedbyususingthecodeprovidedbyitsauthors.
(a)Charades-Ego
Method Backbone p-data L mAP (b)EGTEA
ActorObserverNet[46] ResNet-152 Charades × 20 Method Backbone p-data L Top-1Acc. MeanAcc.
SSDA[11] I3D Charades-Ego × 25.8
Ego-Exo[29] SlowFast-R101 Kinetics-400 × 30.1 Lietal.[31] I3D K400 × - 53.30
H HE i ieg er ro V VV L LL - -AP Sv A[ g33 [[ 3] 3 ]] VVT ii TTS --F BB- aaB ss ee E EEg ggo oo4 44D DD ✓ ✓ ✓ 3 3 32 2 3. . .1 6 8 LML aI VS TPT iCL LA N A[5[ [4 7 [2 68 ] 7 9] ] ] C So Tln oIv Sw3L FD FS -BaT sM t K W4 I0 TI 0KN ++4 E- V01 gG0k oG 4D-S ✓ ✓ ✓× 6 7 71 3 7-. . .8 5 46 9 5 5 6 763 5 00. . ..0 8 11 250 7
EgoVLPv2[40] TSF-B EgoClip ✓ 34.1 FILS(ours) ViT-Base EK100 ✓ 78.48 71.20
LaViLA[69] TSF-B WIT+Ego4D ✓ 33.7
FILS(ours) ViT-Base EK100 ✓ 34.4
4.1 ActionRecognitionTask
Weassessthelearnedvideorepresentationbyfinetuningthevideoencoderforactionclassi-
fication.Inlinewithpreviousstudies[52,69],afterfinetuningthevideoencoder,top-1accu-
racyisreportedonverbs,nouns,andactionsforEpic-Kitchens(EK100)[12]andactionsfor
Something-SomethingV2(SSV2)[21]. ForEGTEA[31],inadditiontotop-1accuracy,we
includemeanclassaccuracy,utilizingtheinitialtrain/testsplit,andforCharades-Ego[47],
theevaluationmetricismeanaverageprecision(mAP).TherecognitionaccuracyforEK100
and SSV2 datasets are reported in Table 1(a) and Table 1(b), respectively. Our proposed
method notably enhances the effectiveness of the existing supervised and self-supervised
techniquesforactionrecognitionoverViT-B.Wehaveatleast1.9%improvementonaction,
verb,andnounonEK100overthebestresultsintheliterature,AVION[68]andLaViLa[69].
Similarly,onSSV2action,theperformanceincreaseis+0.9%comparedtothehighestac-
curacyintheliteratureusingViT-B.Tofurtherevaluateitstransferability,weusetheFILS
modelthatpretrainedonEK100andfinetuneditontheCharades-EgoandEGTEAdatasets,
whicharevisuallydistinctfromEK100. FILSimprovesmAPonCharades-Egoby+1.3%
and top-1 accuracy on EGTEA by +1.03% compared to the baseline LAVILA model [69]
afterfinetuning. Table4(a)andTable5(b)demonstrateoursuperiorperformancecompared
tothecurrentstate-of-the-artonthesedatasets. Therefore,insteadofrelyingonlarge-scale
datasetsliketheKinetics[8,26]dataset,whichincludesURLlinksforupto650,000video
clips,ourmethodimprovedperformanceevenwhentrainedonaapproximately10xsmaller
dataset.8FILS:SELF-SUPERVISEDVIDEOFEATUREPREDICTIONINSEMANTICLANGUAGESPACE
Table3: Ablationstudyontwopatch-wisecontrastivescenariosandmaskingratio.
(b)Maskingratio
51.5
Epic-Kitchens
(a)ActCLIPstrategies 51
51
Method strategy numberofiteration ActionTop-1Acc.
50.5 50.4
FILS patch 10 38.0
FILS patch 30 43.5 50 49.8
FILS patch 50 46.3
FILS patch-average 1 51.0 49.5 49.45
49
65 70 75 80 85 90 95 100
Maskingratio(%)
4.2 AblationStudy
Self-supervisionStrategyThecoreinsightofFILSistoperformmaskedfeatureprediction
inlanguagesemanticspace. InSec.3.2,weintroduceourinitialobjective,FP,whichentails
predictingfeatures.Todemonstratethebenefitsofpredictingpatchfeaturesinsteadofrecon-
structingmissingpatchesinthepixeldomainusingmeansquareerrorloss, wetrainapair
of ViT-B/16 models on the EK100 dataset using feature prediction loss and mean-squared
errorlosswithoutthecontrastivelanguagecontribution. Toevaluatetheactionrecognition
accuracyofmodelstrainedwithFPandMSEobjectives,wefinetunedapretrainedmodelon
EK100. ThemodeltrainedwiththeFPobjectiveexhibitssuperioractionaccuracy,50.3%,
comparedtothemodeltrainedwithMSE,48.5%. Botharelowerthanthe51.0%achieved
onEK100actionrecognitionviaourfullFILSmodels,evidenceoftheeffectivenessofour
proposedmethod,whichinvolvesfeaturepredictionwithinthelanguagespace.
ActCLIPstrategies. Inourpatch-wiseActCLIPframework,weappliedcontrastivelearn-
ingbetweentheaveragefeatureextractedfrompatcheswithintheactionareaandtextfea-
tures. Toexaminetheimpactofthenumberofpatchesused,weconductedanablationstudy
(Table3(a)). Thisinvolvedrandomlyselectingonepatchwithintheactionarea(patchstrat-
egy)insteadofaveragingacrossallpatcheswithintheactionarea(patch-averagestrategy).
We repeated this process several times and documented the results alongside the number
of iterations in Table 3(a). As anticipated, increasing the number of iterations improved
performance,approachingthepatch-averagestrategyutilizedinActCLIP.
MaskingRatio. WecomparedifferentmaskingratiosinTable3(b)onourproposedmethod
usingEpic-Kitchensdataset. Increasingthemaskingratiofrom70%to95%fortubemask-
Figure3:Attentionheatmapsgeneratedfortheinitial,central,andfinalframesoftheEK100
usingthelasttransformerlayerofthemodeltrainedwithself-supervisedstrategiesincluding
FILS,oursecondobjective(FP),andpixel-domainreconstruction(MSE)aftermasking.
)%(noitingocernoitcanoycaruccA1-poTFILS:SELF-SUPERVISEDVIDEOFEATUREPREDICTIONINSEMANTICLANGUAGESPACE9
rinse plate open drawer put down cutting board open drawer cut carrots stir vegetable
Random frame
from a video
ActCLIP
FILS
Figure4: visualizationofthesimilaritybetweentextandvideofeaturesforEK100dataset.
Theprovidedtextistheactionlabelofthevideoweused.
ing,wefindthattheperformanceishigherwithanextremelyhighratioof90%.
4.3 AttentionVisualization
TogaindeeperinsightintothelearnedrepresentationsbyFILS,weemployGrad-CAMs[45]
to visualize the prominent areas that significantly contribute to the accomplishment of the
action recognition task. This visualization helps us better comprehend the spatiotemporal
cuesacquiredduringtheself-supervisedlearningstep. InFig.3, weillustrateattentionvi-
sualizationforafewsamplevideosselectedfromtheEpic-Kitchens-100dataset; visualize
attentionheatmapsofthefirst,middle,andlastframeofthevideousingthemodelstrained
withthesetrainingstrategies: ourproposedFILS,ourfirstobjective,whichisFP,andMSE
inthepixeldomain. Sec.4.2discoversthecomparisonamongthesestrategies. Weselected
instances that FILS correctly classified, whereas feature prediction (FP) and mean squared
error (MSE) failed to do so. More attention visualizations on other datasets can be found
inthesupplementalmaterial. Ourvisualizationstudyrevealsthatcomparedtotheattention
heatmaps computed by ViTs trained with MSE and FP objectives, ViTs trained with FILS
produceattentionheatmapsthatemphasizetheareawheretheactionhappensandaremore
effectiveatclassifyingthevideos’actions. Thesefindingsfurtherdemonstratetheeffective-
ness of conducting contrastive learning between the patches within the action area and the
relevanttextinourproposedself-supervisedtechnique(FILS).
4.4 FILSlearnssemanticrepresentations
Ourproposedapproachpredictsvisualfeaturesinlanguagespaceconstructedthroughcon-
trastivelearningonpatcheswithintheidentifiedactionareaalongsideitscorrespondingtext,
thusreceivingimplicitguidancefromthetextualcontext. UsingvideosfromtheEK100,we
calculatethesimilarityoffeaturesbetweenthevideopatchesandcorrespondingtextfeatures,
whichistheactionlabel. Fig.6comparesourproposedcontrastivestrategy(firstobjective-
ActCLIP)andourFILS.FILSsignificantlyenhancesthecontrastiveobjectivebetweenlan-
guageandvision. Thisisevidentintheheatmap’simprovedlocalizationandreducednoise,
whichnowexhibitsgreaterconcentrationaroundtheobjectandactionregions. Toimprove
visualization,wesmoothedthepatch-wisedattentionblockswithGaussianblurring.10FILS:SELF-SUPERVISEDVIDEOFEATUREPREDICTIONINSEMANTICLANGUAGESPACE
5 Conclusion
Inthiswork,weintroduceFILS,anovelself-supervisedstrategycombiningtwoobjectives:
1) ActCLIP - expanding image-based CLIP to video by capturing long-range temporal de-
pendencies between frames using video-text contrastive loss in detected action areas, and
2)FeaturepredictioninthelanguagesemanticspacebuiltbyActCLIP.Thesecomplemen-
taryobjectivesyieldsynergisticbenefits. ExperimentsshowFILSeffectivelysolvesdown-
streamtaskslikevideoactionrecognition,whichrequireprecisecomprehensionofmotion.
Duetoconstraints,weevaluatedsmallerdatasetsusingViT-B.Still,followingpriorresearch
demonstratingcontrastivelearning’sbenefitsfromscale,webelievethatusinglargerdatasets
containingmillionsofexamplesandahigherversionofthevisiontransformerwillfurther
boostperformance.
References
[1] Mona Ahmadian, Frank Guerin, and Andrew Gilbert. Mofo: Motion focused self-
supervisionforvideounderstanding. NeurIPSWorkshop: Self-SupervisedLearning-
TheoryandPractice,2023.
[2] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucˇic´, and
CordeliaSchmid. Vivit: Avideovisiontransformer. InProceedingsoftheIEEE/CVF
internationalconferenceoncomputervision,pages6836–6846,2021.
[3] Kumar Ashutosh, Rohit Girdhar, Lorenzo Torresani, and Kristen Grauman. Hiervl:
Learning hierarchical video-language embeddings. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages23066–23078,2023.
[4] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent,
MichaelRabbat,YannLeCun,andNicolasBallas. Self-supervisedlearningfromim-
ageswithajoint-embeddingpredictivearchitecture. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages15619–15629,2023.
[5] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael
Auli. Data2vec: A general framework for self-supervised learning in speech, vision
and language. In International Conference on Machine Learning, pages 1298–1312.
PMLR,2022.
[6] GedasBertasius, HengWang, andLorenzoTorresani. Isspace-timeattentionallyou
needforvideounderstanding? InICML,volume2,page4,2021.
[7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Pra-
fulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark,ChristopherBerner,SamMcCandlish,AlecRadford,IlyaSutskever,andDario
Amodei. Languagemodelsarefew-shotlearners,2020.
[8] JoaoCarreira,EricNoland,ChloeHillier,andAndrewZisserman. Ashortnoteonthe
kinetics-700humanactiondataset. arXivpreprintarXiv:1907.06987,2019.
[9] TianqiChen,BingXu,ChiyuanZhang,andCarlosGuestrin. Trainingdeepnetswith
sublinearmemorycost. arXivpreprintarXiv:1604.06174,2016.FILS:SELF-SUPERVISEDVIDEOFEATUREPREDICTIONINSEMANTICLANGUAGESPACE11
[10] RewonChild,ScottGray,AlecRadford,andIlyaSutskever.Generatinglongsequences
withsparsetransformers. arXivpreprintarXiv:1904.10509,2019.
[11] JinwooChoi, GauravSharma, ManmohanChandraker, andJia-BinHuang. Unsuper-
vised and semi-supervised domain adaptation for action recognition from drones. In
ProceedingsoftheIEEE/CVFWinterConferenceonApplicationsofComputerVision,
pages1717–1726,2020.
[12] DimaDamen,HazelDoughty,GiovanniMariaFarinella,AntoninoFurnari,Evangelos
Kazakos,JianMa,DavideMoltisanti,JonathanMunro,TobyPerrett,WillPrice,etal.
Rescalingegocentricvision:Collection,pipelineandchallengesforepic-kitchens-100.
InternationalJournalofComputerVision,pages1–23,2022.
[13] TriDao,DanFu,StefanoErmon,AtriRudra,andChristopherRé. Flashattention: Fast
andmemory-efficientexactattentionwithio-awareness. AdvancesinNeuralInforma-
tionProcessingSystems,35:16344–16359,2022.
[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-
trainingofdeepbidirectionaltransformersforlanguageunderstanding. InProceedings
ofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-
tionalLinguistics:HumanLanguageTechnologies,Volume1(LongandShortPapers),
volume1,pages4171–4186.AssociationforComputationalLinguistics,2019.
[15] XiaoyiDong, JianminBao, YinglinZheng, TingZhang, DongdongChen, HaoYang,
Ming Zeng, Weiming Zhang, Lu Yuan, Dong Chen, et al. Maskclip: Masked self-
distillation advances contrastive language-image pretraining. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10995–
11005,2023.
[16] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,Xiaohua
Zhai,ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl-
vainGelly,etal. Animageisworth16x16words: Transformersforimagerecognition
atscale. arXivpreprintarXiv:2010.11929,2020.
[17] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast net-
worksforvideorecognition.InProceedingsoftheIEEE/CVFinternationalconference
oncomputervision,pages6202–6211,2019.
[18] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and
Zicheng Liu. Violet: End-to-end video-language transformers with masked visual-
tokenmodeling. arXivpreprintarXiv:2111.12681,2021.
[19] RohitGirdhar,MannatSingh,NikhilaRavi,LaurensvanderMaaten,ArmandJoulin,
andIshanMisra. Omnivore: Asinglemodelformanyvisualmodalities. InProceed-
ingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
16102–16112,2022.
[20] RohitGirdhar, AlaaeldinEl-Nouby, MannatSingh, KalyanVasudevAlwala, Armand
Joulin, and Ishan Misra. Omnimae: Single model masked pretraining on images and
videos. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages10406–10417,2023.
[21] RaghavGoyal,SamiraEbrahimiKahou,VincentMichalski,JoannaMaterzynska,Su-
sanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz
Mueller-Freitag, et al. The" something something" video database for learning and
evaluatingvisualcommonsense. InProceedingsoftheIEEEinternationalconference
oncomputervision,pages5842–5850,2017.12FILS:SELF-SUPERVISEDVIDEOFEATUREPREDICTIONINSEMANTICLANGUAGESPACE
[22] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond,
Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad
Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised
learning. Advancesinneuralinformationprocessingsystems,33:21271–21284,2020.
[23] KaimingHe,HaoqiFan,YuxinWu,SainingXie,andRossGirshick. Momentumcon-
trastforunsupervisedvisualrepresentationlearning. InProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition,pages9729–9738,2020.
[24] KaimingHe, XinleiChen, SainingXie, YanghaoLi, PiotrDollár, andRossGirshick.
Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF
conferenceoncomputervisionandpatternrecognition,pages16000–16009,2022.
[25] WenbinHe,SuphanutJamonnak,LiangGou,andLiuRen. Clip-s4: Language-guided
self-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference
onComputerVisionandPatternRecognition,pages11207–11216,2023.
[26] WillKay,JoaoCarreira,KarenSimonyan,BrianZhang,ChloeHillier,SudheendraVi-
jayanarasimhan,FabioViola,TimGreen,TrevorBack,PaulNatsev,etal. Thekinetics
humanactionvideodataset. arXivpreprintarXiv:1705.06950,2017.
[27] Evangelos Kazakos, Jaesung Huh, Arsha Nagrani, Andrew Zisserman, and Dima
Damen. With a little help from my temporal context: Multimodal egocentric action
recognition. arXivpreprintarXiv:2111.01024,2021.
[28] JieLei,LinjieLi,LuoweiZhou,ZheGan,TamaraLBerg,MohitBansal,andJingjing
Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In
ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,
pages7331–7341,2021.
[29] YanghaoLi,TusharNagarajan,BoXiong,andKristenGrauman. Ego-exo: Transfer-
ringvisualrepresentationsfromthird-persontofirst-personvideos. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages6943–
6953,2021.
[30] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He.
Scaling language-image pre-training via masking. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages23390–23400,2023.
[31] YinLi,MiaoLiu,andJamesMRehg.Intheeyeofbeholder:Jointlearningofgazeand
actionsinfirstpersonvideo. InProceedingsoftheEuropeanconferenceoncomputer
vision(ECCV),pages619–635,2018.
[32] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video
understanding. InProceedingsoftheIEEE/CVFinternationalconferenceoncomputer
vision,pages7083–7093,2019.
[33] Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric Z
XU,DifeiGao,Rong-ChengTu,WenzheZhao,WeijieKong,etal. Egocentricvideo-
languagepretraining. AdvancesinNeuralInformationProcessingSystems, 35:7575–
7586,2022.
[34] Wei Lin, Leonid Karlinsky, Nina Shvetsova, Horst Possegger, Mateusz Kozinski,
Rameswar Panda, Rogerio Feris, Hilde Kuehne, and Horst Bischof. Match, expand
andimprove: Unsupervisedfinetuningforzero-shotactionrecognitionwithlanguage
knowledge. arXivpreprintarXiv:2303.08914,2023.FILS:SELF-SUPERVISEDVIDEOFEATUREPREDICTIONINSEMANTICLANGUAGESPACE13
[35] ZeLiu,JiaNing,YueCao,YixuanWei,ZhengZhang,StephenLin,andHanHu.Video
swintransformer. InProceedingsoftheIEEE/CVFconferenceoncomputervisionand
patternrecognition,pages3202–3211,2022.
[36] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv
preprintarXiv:1711.05101,2017.
[37] JiasenLu,DhruvBatra,DeviParikh,andStefanLee.Vilbert:Pretrainingtask-agnostic
visiolinguisticrepresentationsforvision-and-languagetasks.Advancesinneuralinfor-
mationprocessingsystems,32,2019.
[38] MandelaPatrick,DylanCampbell,YukiAsano,IshanMisra,FlorianMetze,Christoph
Feichtenhofer, AndreaVedaldi, andJoaoFHenriques. Keepingyoureyeontheball:
Trajectoryattentioninvideotransformers. Advancesinneuralinformationprocessing
systems,34:12493–12506,2021.
[39] AJPiergiovanni,KairoMorton,WeichengKuo,MichaelSRyoo,andAneliaAngelova.
Videoquestionansweringwithiterativevideo-textco-tokenization. InEuropeanCon-
ferenceonComputerVision,pages76–94.Springer,2022.
[40] Shraman Pramanick, Yale Song, Sayan Nag, Kevin Qinghong Lin, Hardik Shah,
Mike Zheng Shou, Rama Chellappa, and Pengchuan Zhang. Egovlpv2: Egocen-
tric video-language pre-training with fusion in the backbone. In Proceedings of the
IEEE/CVFInternationalConferenceonComputerVision,pages5285–5297,2023.
[41] Rui Qian, Yeqing Li, Zheng Xu, Ming-Hsuan Yang, Serge Belongie, and Yin Cui.
Multimodal open-vocabulary video classification via pre-trained vision and language
models. arXivpreprintarXiv:2207.07646,2022.
[42] AriadnaQuattoni,MichaelCollins,andTrevorDarrell.Learningvisualrepresentations
usingimageswithcaptions.In2007IEEEConferenceonComputerVisionandPattern
Recognition,pages1–8.IEEE,2007.
[43] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,Sandhini
Agarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learning
transferablevisualmodelsfromnaturallanguagesupervision. InInternationalconfer-
enceonmachinelearning,pages8748–8763.PMLR,2021.
[44] HanoonaRasheed, MuhammadUzairKhattak, MuhammadMaaz, SalmanKhan, and
FahadShahbazKhan. Fine-tunedclipmodelsareefficientvideolearners. InProceed-
ingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
6545–6554,2023.
[45] RamprasaathRSelvaraju,MichaelCogswell,AbhishekDas,RamakrishnaVedantam,
DeviParikh,andDhruvBatra. Grad-cam: Visualexplanationsfromdeepnetworksvia
gradient-based localization. In Proceedings of the IEEE international conference on
computervision,pages618–626,2017.
[46] Gunnar A Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi, and Karteek
Alahari. Actorandobserver: Jointmodelingoffirstandthird-personvideos. Inpro-
ceedings of the IEEE conference on computer vision and pattern recognition, pages
7396–7404,2018.
[47] Gunnar A Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi, and Karteek
Alahari. Charades-ego: A large-scale dataset of paired third and first person videos.
arXivpreprintarXiv:1804.09626,2018.14FILS:SELF-SUPERVISEDVIDEOFEATUREPREDICTIONINSEMANTICLANGUAGESPACE
[48] SwathikiranSudhakaran,SergioEscalera,andOswaldLanz. Lsta:Longshort-termat-
tentionforegocentricactionrecognition. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages9954–9963,2019.
[49] Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia Schmid. Learning
video representations using contrastive bidirectional transformer. arXiv preprint
arXiv:1906.05743,2019.
[50] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid.
Videobert: Ajointmodelforvideoandlanguagerepresentationlearning. InProceed-
ingsoftheIEEE/CVFinternationalconferenceoncomputervision,pages7464–7473,
2019.
[51] Hao Tan, Jie Lei, Thomas Wolf, and Mohit Bansal. Vimpac: Video pre-training via
masked token prediction and contrastive learning. arXiv preprint arXiv:2106.11250,
2021.
[52] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoen-
coders are data-efficient learners for self-supervised video pre-training. Advances in
neuralinformationprocessingsystems,35:10078–10093,2022.
[53] LiminWang,BingkunHuang,ZhiyuZhao,ZhanTong,YinanHe,YiWang,YaliWang,
and Yu Qiao. Videomae v2: Scaling video masked autoencoders with dual masking.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecog-
nition,pages14549–14560,2023.
[54] LiweiWang,YinLi,JingHuang,andSvetlanaLazebnik. Learningtwo-branchneural
networks for image-text matching tasks. IEEE Transactions on Pattern Analysis and
MachineIntelligence,41(2):394–407,2018.
[55] Mengmeng Wang, Jiazheng Xing, and Yong Liu. Actionclip: A new paradigm for
videoactionrecognition. arXivpreprintarXiv:2109.08472,2021.
[56] RuiWang,DongdongChen,ZuxuanWu,YinpengChen,XiyangDai,MengchenLiu,
Yu-Gang Jiang, Luowei Zhou, and Lu Yuan. Bevt: Bert pretraining of video trans-
formers. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pages14733–14743,2022.
[57] Xiaohan Wang, Linchao Zhu, Heng Wang, and Yi Yang. Interactive prototype learn-
ing for egocentric action recognition. In Proceedings of the IEEE/CVF International
ConferenceonComputerVision,pages8168–8177,2021.
[58] ChenWei,HaoqiFan,SainingXie,Chao-YuanWu,AlanYuille,andChristophFeicht-
enhofer. Maskedfeaturepredictionforself-supervisedvisualpre-training. InProceed-
ingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
14668–14678,2022.
[59] Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra
Malik, and Christoph Feichtenhofer. Memvit: Memory-augmented multiscale vision
transformerforefficientlong-termvideorecognition. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages13587–13597,2022.
[60] ZhendaXie,ZhengZhang,YueCao,YutongLin,JianminBao,ZhuliangYao,QiDai,
andHanHu. Simmim: Asimpleframeworkformaskedimagemodeling. InProceed-
ingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
9653–9663,2022.FILS:SELF-SUPERVISEDVIDEOFEATUREPREDICTIONINSEMANTICLANGUAGESPACE15
[61] Shen Yan, Xuehan Xiong, Anurag Arnab, Zhichao Lu, Mi Zhang, Chen Sun, and
CordeliaSchmid. Multiviewtransformersforvideorecognition. InProceedingsofthe
IEEE/CVFconferenceoncomputervisionandpatternrecognition,pages3333–3343,
2022.
[62] Shusheng Yang, Yixiao Ge, Kun Yi, Dian Li, Ying Shan, Xiaohu Qie, and Xinggang
Wang. Rils: Masked visual reconstruction in language semantic space. In Proceed-
ingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
23304–23314,2023.
[63] Zhao Yang, Yansong Tang, Luca Bertinetto, Hengshuang Zhao, and Philip HS Torr.
Hierarchicalinteractionnetworkforvideoobjectsegmentationfromreferringexpres-
sions. InBMVC,2021.
[64] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang Zhao, and Philip HS
Torr. Lavt: Language-aware vision transformer for referring image segmentation. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion,pages18155–18165,2022.
[65] Keunwoo Peter Yu. VideoBLIP. URL https://github.com/yukw777/
VideoBLIP.
[66] Keunwoo Peter Yu, Zheyuan Zhang, Fengyuan Hu, and Joyce Chai. Efficient in-
context learning in vision-language models for egocentric videos. arXiv preprint
arXiv:2311.17041,2023.
[67] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang,
Yejin Choi, and Jianfeng Gao. Vinvl: Revisiting visual representations in vision-
language models. In Proceedings of the IEEE/CVF conference on computer vision
andpatternrecognition,pages5579–5588,2021.
[68] YueZhaoandPhilippKrähenbühl. Trainingalargevideomodelonasinglemachine
inaday. arXivpreprintarXiv:2309.16669,2023.
[69] YueZhao,IshanMisra,PhilippKrähenbühl,andRohitGirdhar. Learningvideorepre-
sentationsfromlargelanguagemodels. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages6586–6597,2023.
[70] ZhedongZheng,LiangZheng,MichaelGarrett,YiYang,MingliangXu,andYi-Dong
Shen. Dual-pathconvolutionalimage-textembeddingswithinstanceloss. ACMTrans-
actions on Multimedia Computing, Communications, and Applications (TOMM), 16
(2):1–23,2020.16FILS:SELF-SUPERVISEDVIDEOFEATUREPREDICTIONINSEMANTICLANGUAGESPACE
Supplementary Material
Inthissupplementarymaterial,weshareimplementationdetailsandpreprocessingstep
inSec.6anddetailsofthedatasetsandevaluationmetricsusedinthisworkinSec.7. We
provideadditionalquantitativeandqualitativeexperimentstoenhancecomprehensionofthe
FILSinsections 8to11.
6 Implementation Details
Forself-supervision, wesample16RGBframesfromeachvideoasaclipwithadynamic
stride (depending on the number of raw video frames). The resolution of the frames is
224×224 and uses random resized cropping as augmentation. Our spatial patch size is
16×16 with a temporal patch size of 2. Therefore, each clip is split into non-overlapping
8×14×14 tubes, yielding 1568 tokens. The encoder in each of our experiments is the ViT-
B/16 architecture [16], trained on Epic-Kitchens (EK100) and Something-Something V2
(SSV2)datasets. WeusetheAdamWoptimizer[36]with(β ,β )=(0.9,0.95)andaweight
1 2
decayof0.05. Thelearningratestartsat1e-6,growslinearlytoapeakof1.5e-4inthefirst
epoch,andthenusesahalf-wavecosinescheduletodeclineto1e-5progressively. Thepre-
dictorhassixadditionaltransformerlayers.Asapreprocessingstep,wecreatesyntheticcap-
tionsforthewholevideosinthetrainingsetusingavideo-to-textmodel(VideoBLIP)[65].
OurtextencoderemploysaCLIP-basedencoderwithfrozenweightsfromViFi-CLIP[44].
The pretraining is conducted for 800 epochs, and following [68], we employ flash atten-
tion[13]tolessenthememorybottleneckoftheattentionoperations;itismoreefficientthan
standard attention techniques in terms of IO complexity. We also leverage gradient check
pointing[9,10]forthetrainingtransformertoreducethememorycostandusePytorchgra-
dient checkpointing. For ViT-B, we employ a batch size of 64 per GPU over 4 GPUs, a
totalbatchsizeof256,significantlysmallerthanthatusedinpreviousstudiesintheorderof
thousands.
Wefixtheparametersforallourfinetuningexperimentsusingthesamehyperparameters
asthetrainedbaselines. WeuseAdamWwithamomentumof0.9andweightdecayof0.05
to finetune the pretrained model on EK100 and SSV2 for a specific number of epochs on
EK100, Charades-Ego, EGTEA,andSSV2. Weemploycosineannealingwithawarm-up,
in which the base learning rate begins at 1e-6, increases linearly to a peak of 1.5e-3 in the
first epoch, and then decreases gradually to 1e-6 using a half-wave cosine schedule. We
replacethelinearprojectionheadforactionclassificationwithadataset-specificdimension
head. We marginalize the action-level probability to obtain verb- and noun-level accuracy.
Wealsoemploy0.8mixupand0.1labelsmoothing. Weinput16sampledframesforeach
video clip during training and testing and resized the shorter side to 256 pixels. Next, we
takea224x224cropandapplydataaugmentationusingconventionalRandomResizedCrop
(0.08,1.0) and Horizontal Flip (0.5), fused at the video-decoding side. We take the centre
224×224cropatinferenceandscaletheshortersideto224pixels. ForViT-B,weemploya
batchsizeof64perGPUover4GPUs.
7 Datasets and Metrics
Toillustratetheperformanceofourapproach,weapplytheproposedmethodtofourdatasets:
Something-Something V2(SSV2) [21], which is hard to discriminate the action classes for
itsvideosbecausethesameobjects(andhumanhands)andverysimilarmotionsappearin
manydifferentactions, Epic-Kitchensthatfacesmanychallengestopredictitsfirst-personFILS:SELF-SUPERVISEDVIDEOFEATUREPREDICTIONINSEMANTICLANGUAGESPACE17
activities due to limited field of view, occlusions and camera movements [12], Charades-
Ego[47]andEGTEA[31]whicharerelativelysmalldatasetstovalidatethetransferability
ofourpretrainedmodelonthem.
Epic-Kitchens(EK100)[12]isoneofthelargestegocentric(first-person)visiondatasets,
consisting of 700 variable-length videos over 100 hours, 20 million frames; each video is
dividedintoshortactionsegments,withameandurationof3.12seconds,accompaniedby
annotations consisting of a verb and a noun describing the depicted action (for example,
’opencupboard’).With67,217intraining,9,668intestswith97verbs,300nouns,and3806
actionclasses.
Something-SomethingV2(SSV2)[21],thisdatasetdepictsindividualsineverydayhuman-
objectinteractiontasks. Thedatasetisdividedinan8:1:1ratiointotrain,validation,andtest
sets. With168,913videosinthetrainingset,24,777inthevalidationset,and27,157inthe
testset,ithas220,847videosand174labels.
Charades-Ego [47], the Charades-Ego dataset comprises 7,860 first- and third-person
recordingsofeverydayindooractivitiesfor157actionclasses. Thereare34.4hoursoffirst-
personand34.4hoursofthird-personrecordings,withanaverageof8.72activities(68,536
activityinstances)ineachvideo.Thevideosarefrom112differentroomsallovertheworld.
We use the subset containing first-person data, consisting of 3,085 videos for training and
846videosfortestingpurposes.
Extended GTEA Gaze+(EGTEA) [31], this dataset contains 29 hours of first-person
videosfrom86sessions,from32participantscompletingsevenmealpreparationtasksina
realistickitchenenvironment.Thedatasetincludesactionannotationsfor10321casesacross
106classeswithanaveragedurationof3.2seconds.
8 Comparsion FILS with Pixel Reconstruction
Wedemonstratethatourproposedmethod(FILS)exhibitsnotableenhancements,surpassing
the performance of the pixel space reconstruction technique [52, 68], which attained its
resultsrelyingonextensiveepochsofpertaining. Wepresenttheactionrecognitionresults
ofFILSandthepixel-reconstructionbaselineontheEpic-KitchensdatasetusingaViT-B/16
pretrained on the self-supervised objective for 100, 400, and 800 epochs. This experiment
aimstodemonstrateourresults’consistencyandsuperiorFILSperformanceoverthepixel
spacereconstructionmethod,evenwithfewertrainingepochs.
9 Charades-Ego and EGTEA Action Recognition using
FILS is pretrained on SSV2
To further assess the transferability of our proposed FILS, we trained our FILS on Epic-
Kitchens and then finetuned it on Charades-Ego and EGTEA datasets; reported results in
Sec.4.1inthemainsubmissiondemonstrateoursuperiorperformancecomparedtothecur-
rentstate-of-the-artonthesedatasets. Additionally,wehaveextendedourevaluationbyin-
corporatingactionrecognitionresultsforCharades-EgoandEGTEAusingFILSpretrained
ontheSSV2dataset. AsshowninTables4and5,theFILSmodelpretrainedonSSV2also
achieved superior performance, outperforming the previous state-of-the-art results. These
findingsunderscorethetransferabilityofFILSacrossdifferentdatasets,highlightingitspo-
tentialforbroaderapplicationinactionrecognitiontasks.18FILS:SELF-SUPERVISEDVIDEOFEATUREPREDICTIONINSEMANTICLANGUAGESPACE
featurepredictioninlanguagespace
55 pixelspacereconstruction
53
51
51
49
47 48.5
45
43
41
41
39 40.5
37 36.2
35
33
33.7
31
0 200 400 600 800
epochs
Figure5:TheimpactofvaryingpretrainingepochsontheEpic-Kitchens-100dataset. There
isaconsistentupwardtrendinactionrecognitionaccuracywithanincreaseinthenumber
ofpretrainingepochs.
10 Qualitative Results
We use Grad-CAM [45] to visualize the last stage feature maps. In Fig. 6, we display
the Grad-CAM of the first, middle, and last frames for the challenging examples from the
datasetsweworkedon: Epic-Kitchens,Something-SomethingV2,andEGTEA.Theatten-
tion maps demonstrate how well our proposed self-supervised technique (FILS) represents
thepotentialsemanticregioninthevideoandacquiresanunderstandingofspatiotemporal
relationshipsbylinkingpertinentareas,revealingdistinctlythesemanticsofactions.
11 Synthetic Captions
Inourself-supervisionstep,asapreliminarystep,weuseavideo-to-textmodel(VideoBLIP)[65]
togeneratesyntheticcaptionsforallofthevideosinthetrainingset. Thisprocesswascon-
ducted for both the Epic-Kitchens and Something-Something v2 datasets. In Fig. 7, we
presentexamplesofgeneratedcaptionsalongsidetheirrespectivelabelsforthetrainingsets
of these two datasets. These synthetic captions for videos have demonstrated remarkable
effectiveness, providing detailed scene descriptions and capturing concepts closely related
tovideolabels,whichcouldenhancetheinterpretabilityofvideocontent. Usingthesegen-
erated captions in our training process could enhance the comprehension and performance
ofourmodel,astheseenrichedinputfeaturesaresemanticallymeaningful,whichiscrucial
fortrainingmodels.
)%(noitingocernoitcanoycaruccA1-poTFILS:SELF-SUPERVISEDVIDEOFEATUREPREDICTIONINSEMANTICLANGUAGESPACE19
Table 4: Charades-Ego Action Recognition. FILS achieves substantial improvements in
this task, surpassing prior works in both scenarios: FILS undergoes pretraining on either
EK100 or SSV2. In the table, ’p-data’ signifies the pretraining data employed, while ’L’
denotestheincorporationoflanguagethroughthetrainingprocess.
Method Backbone p-data L mAP
ActorObserverNet[46] ResNet-152 Charades × 20
SSDA[11] I3D Charades-Ego × 25.8
Ego-Exo[29] SlowFast-R101 Kinetics-400 × 30.1
EgoVLP[33] TSF-B Ego4D ✓ 32.1
HierVL-Avg[3] ViT-Base Ego4D ✓ 32.6
HierVL-SA[3] ViT-Base Ego4D ✓ 33.8
EgoVLPv2[40] TSF-B EgoClip ✓ 34.1
LaViLA[69] TSF-B WIT+Ego4D ✓ 33.7
FILS(ours) ViT-Base EK100 ✓ 34.4
FILS(ours) ViT-Base SSV2 ✓ 34.2
Table 5: EGTEA Action Recognition. FILS significantly enhances performance in this
task,outperformingpreviousmethodsinbothcaseswhereitispretrainedoneitherEK100
orSSV2. Inthetable,’p-data’representsthepretrainingdatasetused,and’L’indicatesthe
inclusionoflanguageduringthetrainingprocess.
Method Backbone p-data L Top-1Acc. MeanAcc.
Lietal.[31] I3D K400 × - 53.30
LSTA[48] ConvLSTM IN-1k × 61.86 53.00
IPL[57] I3D K400 ✓ - 60.15
MTCN[27] SlowFast K400+VGG-S ✓ 73.59 65.87
LaViLA[69] TSF-B WIT+Ego4D ✓ 77.45 70.12
FILS(ours) ViT-Base EK100 ✓ 78.48 71.20
FILS(ours) ViT-Base SSV2 ✓ 78.57 71.3120FILS:SELF-SUPERVISEDVIDEOFEATUREPREDICTIONINSEMANTICLANGUAGESPACE
Epic-Kitchens Something-Something V2 EGTEA
Input FILS Input FILS Input FILS
First frame
Middle frame
Last frame
wash spoon covering something with something put condiment container
First frame
Middle frame
Last frame
fold filter hitting something with something open drawer
First frame
Middle frame
Last frame
crack egg turning something upside down cut cucumber
Figure 6: Visualization by Grad-CAM on Epic-Kitchens, Something-Something V2 and
EGTEA.FILS:SELF-SUPERVISEDVIDEOFEATUREPREDICTIONINSEMANTICLANGUAGESPACE21
Epic-Kitchens
Sample
frames
G ce an pe tr ia ot ned a person is opening the door of a refrigerator a person is cooking vegetables on a stove top a person cutting vegetables on a cutting board
Action label open fridge stir vegetables cut carrots
Something-Something V2
Sample
frames
Generated
caption a person holding a glass bowl with a hole in it a person holding a blue ball in their hand a person is spreading butter on a piece of bread
Action label showing that something is empty holding something spreading something onto something
Figure7: SyntheticcaptionsforsomeinstancesfromthetrainingsetofEpic-Kitchensand
Something-Somethingv2. VideoBLIPoftencapturesgoodspatialandtemporaldetails.