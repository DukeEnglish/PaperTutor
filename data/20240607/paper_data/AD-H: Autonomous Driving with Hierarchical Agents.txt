AD-H: Autonomous Driving with Hierarchical Agents
ZaibinZhang1,2∗,ShiyuTang1∗,YuanhangZhang1∗,TalasFu1∗,
YifanWang1,YangLiu1,DongWang1,JingShao2,LijunWang1†,HuchuanLu1
1DalianUniversityofTechnology,2ShanghaiArtificialIntelligenceLaboratory
dlutzzb@gmail.com, {tangshiyu, zhangyuanhang, oyontalas}@mail.dlut.edu.cn,
ljwang@dlut.edu.cn
Abstract
Duetotheimpressivecapabilitiesofmultimodallargelanguagemodels(MLLMs),
recentworkshavefocusedonemployingMLLM-basedagentsforautonomous
drivinginlarge-scaleanddynamicenvironments. However,prevalentapproaches
oftendirectlytranslatehigh-levelinstructionsintolow-levelvehiclecontrolsignals,
whichdeviatesfromtheinherentlanguagegenerationparadigmofMLLMsand
failstofullyharnesstheiremergentpowers.Asaresult,thegeneralizabilityofthese
methodsishighlyrestrictedbyautonomousdrivingdatasetsusedduringfine-tuning.
Totacklethischallenge,weproposetoconnecthigh-levelinstructionsandlow-level
controlsignalswithmid-levellanguage-drivencommands,whicharemorefine-
grainedthanhigh-levelinstructionsbutmoreuniversalandexplainablethancontrol
signals,andthuscaneffectivelybridgethegapinbetween. Weimplementthisidea
throughahierarchicalmulti-agentdrivingsystemnamedAD-H,includingaMLLM
plannerforhigh-levelreasoningandalightweightcontrollerforlow-levelexecution.
ThehierarchicaldesignliberatestheMLLMfromlow-levelcontrolsignaldecoding
and therefore fully releases their emergent capability in high-level perception,
reasoning,andplanning. ToeffectivelytrainAD-H,webuildanewautonomous
driving dataset with action hierarchy annotations encompassing multiple levels
ofinstructionsanddrivingcommands. Comprehensiveclosed-loopevaluations
demonstrateseveralkeyadvantagesofourproposedAD-Hsystem. First,AD-H
cannotablyoutperformstate-of-the-artmethodsinachievingexceptionaldriving
performance,evenexhibitingself-correctioncapabilitiesduringvehicleoperation,
ascenarionotencounteredinthetrainingdataset. Second,AD-Hdemonstrates
superiorgeneralizationunderlong-horizoninstructionsandnovelenvironmental
conditions,significantlysurpassingcurrentstate-of-the-artmethods. Wewillmake
ourdataandcodepubliclyaccessibleathttps://github.com/zhangzaibin/
AD-H
1 Introduction
Autonomousdrivingsystemsrepresentamajoradvancementincontemporarytransportation,which
requiresvehiclestoautomaticallyoperateinlarge-scaleanddynamicenvironments. Withtherapid
advancementofMultimodalLargeLanguageModels(MLLMs)Liuetal.[2024],Daietal.[2024],Li
etal.[2023a],Yinetal.[2024],Zhangetal.[2023c],Zhuetal.[2023]andMLLM-basedagentsDriess
etal.[2023],Brohanetal.[2022,2023],Belkhaleetal.[2024],Wangetal.[2023a,f],Lifshitzetal.
[2024],Qinetal.[2023b],Zhouetal.[2024a],recentattemptsSimaetal.[2023],Wangetal.[2023b],
Shaoetal.[2023],Chenetal.[2023b],Liuetal.[2023a],Shaetal.[2023],Wenetal.[2023a],Tian
etal.[2024]havebeenmadetoexploreMLLMsasthecentralagentofautonomousdrivingsystems
⋆Equalcontribution
†Correspondingauthor
Preprint.
4202
nuJ
5
]VC.sc[
1v47430.6042:viXraHigh-level Contextual Instruction
Mid-level Command Low-level Control Signal
High-level Planner
Proceed ahead and
make a right turn.
Turn right. Turn left. Go straight.
MLLM-based Agent Low-level Controller
(a) Previous Work (b) Proposed AD-H
Figure 1: Comparison between the previous method and AD-H in an oversteering scenario not
encounteredduringtraining. Thepreviousmethodkeepsmovingstraight,deviatingfromtheintended
route. Conversely,theplannerofAD-Hcanprovidecorrectivemid-levelcommandstothecontroller,
facilitatingthevehicle’sre-alignment.
for better perception, reasoning, and interactions, which have achieved remarkable progress. A
predominantparadigmadoptedbythesemethodsistotranslatehigh-levelcontextualinstructionsinto
low-levelcontrolsignalsusingMLLMs. AsMLLMsarepre-trainedtogeneratenaturallanguages,
theirabilitytodecodelow-levelcontrolsignalsishighlyreliantontheautonomousdrivingdatasets
usedduringfine-tuning,causingsignificantoverfittingtospecificscenariosandinstructions. Asan
example,Figure1(a)depictsanoversteeringscenariothatisabsentinthetrainingdataset. Most
existingmethodsstruggletoadapttothiscaseandoftenmaintainstraightmotionevenafterexcessive
turning,leadingtodangeroussituations. Theselimitationsmotivateustodelveintoanintriguingand
pivotalquestion: Isitpossibletodevelopanautonomousdrivingsystemthatcanfullyunleashthe
emergentcapabilitiesofpre-trainedMLLMformoreintelligentreasoningandstrongerscalability
towardsunseenscenariosandinstructions?
Toanswertheabovequestion,weexploretheideaofhierarchicalpolicyBelkhaleetal.[2024],Chen
et al. [2024]. Rather than directly predicting the final control signals, we propose to fill the gap
betweenhigh-levelinstructionsandlow-levelcontrolsignalswithintermediatemid-levelcommands.
Ontheonehand,comparedtohigh-levelcontextualinstructions,mid-levelcommandsofferafiner
granularityandlieclosertolow-levelcontrolsignals,permittingmoreprecisereflectiononreal-time
environmental feedback. On the other hand, different from low-level control signals, mid-level
commandsarenaturallanguage-drivenandarethereforebetteralignedwiththepre-trainingtarget
ofMLLMstoleveragetheirworldknowledge. Inaddition,breakingdownhigh-levelinstructions
intomid-levelcommandsfurtherenablesmoreflexiblehumaninteractionandeffectivesharedpolicy
structurelearningacrosssimilartasksBelkhaleetal.[2024],givingrisetostrongergeneralization
abilitiestonovelinstructionandscenarios.
In light of the above motivation, we design a Hierarchical Multi-Agent System for Autonomous
Driving(AD-H),whichcomprisestwoagents: aMLLM-basedplannerandalightweightcontroller.
AsshowninFigure1(b),theplanneraimstoperformplanninganddecision-makingbasedonthe
inputcontextualhigh-levelinstructionandpredictsamid-levelcommandateachdecisionframe. The
mid-levelcommandisthendecodedintothelow-levelcontrolsignalsbythecontrollergiventhe
currentvisualinputandthecontextualinstruction. Thehigh-levelplannerandlow-levelcontroller
together form a hierarchical policy system, which effectively frees the MLLM from low-level
decodingandunlocksitspotentialforhigh-levelperception,reasoning,andplanning. Thelastissue
remainingisthelackofannotateddatafortrainingthehierarchicalsystems,asexistingautonomous
datasetsdonotcontainmid-levelcommands. Tothisend,derivedfromLMDrivedatasetShaoetal.
[2023],wefurtherbuildanewtrainingdatasetincluding1,753Kframeswithhierarchicalannotations
encompassingmulti-levelinstructionsandcommands.
2Throughintensiveevaluationsundertheclosed-loopenvironment,weshowthatourAD-Henjoysthe
followingtwoadvantages. First,AD-Hcanbettergeneralizetonovelscenarios. Sincethehigh-level
reasoningandlow-levelexecutionaredecoupledinourhierarchicalmulti-agentsystem,theplanner
solelyfocusingonhigh-levelreasoningcanmoreeffectivelyleveragetheemergentcapabilityofpre-
trainedMLLMs,yieldingstrongergeneralizationpowerandreasoningabilityunderunseendriving
scenarios and even challenging corner cases. For example, in cases of oversteering, the planner
issuescorrectiveinstructionstoguidethevehiclebackontherighttrack(Figure1(b)). Incontrast,
previousmethodstendtoseverelyoverfittocontrolsignalpatternswithinthetrainingset,resulting
in a tendency to persistently move straight (Figure 1 (a)). As a result, AD-H achieves a notable
improvementindrivingperformancecomparedtostate-of-the-artmethods. Second,AD-Hcanbetter
generalizetonovellong-horizoninstructions. Ourlong-horizonexperimentsrevealthatAD-Hcan
comprehensivelyunderstandnovellong-horizoninstructions,performeffectiveplanning,andgenerate
precisedrivingcommandsatappropriatedecisionframes. Thishasledtoasignificantimprovement
inperformanceforlong-horizontasks. Incontrast,existingmethodsshowpoorgeneralizationto
long-horizoninstructions,oftenresultinginerroneousroutes.
Thecontributionofthispapercanbesummarizedasfollows:
• WeproposeAD-H,ahierarchicalmulti-agentsystemforautonomousdriving,whichcan
significantlyunleashthepowerofMLLMstoachievehighercontrolprecisionandgeneral-
ization.
• We construct an autonomous driving dataset with 1,753k multi-level driving command
annotations,whichcaneffectivelyfacilitatehierarchicalpolicylearning.
• We perform intensive experiments and demonstrate that our approach can considerably
outperformstate-of-the-artmethodsandexhibitsstrongergeneralizationtonovelscenarios
andlong-horizoninstructions.
2 RelatedWorks
2.1 End-EndmethodsinAutonomousdriving
Inautonomousdriving,preciseperceptionLietal.[2022d],Yangetal.[2023],Liuetal.[2023b],
PhilionandFidler[2020],Liangetal.[2022],Qinetal.[2023a],Lietal.[2022b],Jiaoetal.[2023],
Yoo et al. [2020], Li et al. [2022c], Bai et al. [2022], Chen et al. [2022], Huang et al. [2021], Li
et al. [2022a], Park et al. [2022], Li et al. [2023d], Zhou et al. [2023a], Wang et al. [2023g,d,
2024a],Zhangetal.[2023b],Geetal.[2023],Lietal.[2023c]andplanningarecritical. Totackle
the prevalent issue of long-tail distribution in autonomous driving scenarios, several generative
network-basedWorldModelshavebeendevelopedWangetal.[2023c],Jiaetal.[2023],Zhaoetal.
[2024],Wenetal.[2023b]. Thesenetworkscangenerateavastarrayofrealisticurbanstreetscenes.
However,inordertocontrolthevehicle,aseparateplanningmodelneedstobedesignedtoutilizethe
perceptionresults. Tosolvethisproblem,manyend-to-endautonomousdrivingmodelshavebeen
proposed,includingreinforcementlearningbasedPrakashetal.[2021],Wuetal.[2022],Chittaetal.
[2022],Codevillaetal.[2019],Cuietal.[2022]andimitationlearningbasedmethodsXiaoetal.
[2023],Hanselmannetal.[2022]. Besidesthese,UniADHuetal.[2023b]addressestheproblemof
end-to-endautonomousdrivingbyutilizingmultiplemodulesinBEVspace.
Sincetheemergenceofmultimodallargemodels,thefieldofautonomousdrivinghasbeencontin-
uouslyexploringthepossibilityofusingsuchlargemodelsinanend-to-endmannertosolvethis
problem. LLM-DriverChenetal.[2023a]usesVector-formertocharacterizetheperceptionofthe
environmentbyautonomousdrivinginvectorspace. Drivegpt4Xuetal.[2023]proposesanoveltwo-
stagetrainingmultimodalautonomousdrivingparadigm,whichdirectlyregressescontrolsignalsand
textresponsesthroughmulti-frameimageinputandtextinstructions. DOLPHINSMaetal.[2023]
innovativelyintroducesin-contextlearningintotheautonomousdrivingframework,whichcanbetter
mimichumanhigher-ordercontrolabilities. Unlikethemethodsmentionedabovethataretrainedand
testedonstaticdatasets,LMDriveShaoetal.[2023]firstconductsclosed-loopautonomousdriving
trainingandtestingontheCarlasimulator,demonstratingstrongclosed-loopcontrolcapabilities
andscenegeneralization. AswellasseveralothernotablecontributionsinthisareaLietal.[2024],
Zhouetal.[2023b],Dingetal.[2024],Wangetal.[2023e],Yeetal.[2024],Pengetal.[2024],Paul
etal.[2024],Wangetal.[2024b]. Besides,therehavebeensomeexploratoryendeavorstoleverage
3agent-basedapproachesinthedomainofautonomousvehicularnavigationYangetal.[2024],Mao
etal.[2023].
2.2 MultimodalLargeLanguageModels
MultimodalLargeLanguageModels(MLLMs)havegarneredconsiderableattentionfortheirremark-
ableabilitiesinmultimodalperception. SeveralstudiesLiuetal.[2024],Daietal.[2024],Zhang
et al. [2023c], Zhu et al. [2023], Lai et al. [2023], Peng et al. [2023] focus on integrating visual
contentintolanguagemodels,specificallydesignedtocomprehendandreasonaboutimages. Among
these,LLaVALiuetal.[2024]employsatwo-stageinstruction-tuningpipelineforcomprehensive
visualandlanguageunderstanding. InstructBLIPDaietal.[2024]combinesthelanguagemodelwith
aninstruction-awareQ-Formertoextractvisualcontenthighlypertinenttotheprovidedinstruction.
Additionally, researchDeshmukhetal.[2023],Lietal.[2023b],Zhangetal.[2023a],Guoetal.
[2023],Hongetal.[2023]isexpandingMLLMstoincludeaudio,video,andpointclouds,enhancing
theirabilitytohandlecomplexmultimodaltasks. ThisintegrationallowsMLLMstoprocessspatial,
auditory,andvisualdatasimultaneously,significantlyimprovingperformanceinapplicationslike
autonomousnavigationandmultimediaanalysis.
2.3 LLMsinTaskPlanning
In various fields, LLMs have demonstrated their potential in task decomposition for advanced
planning. LLMscanincorporateadditionalvisualmodules,suchascaptiondescriptions,toperceive
environmentsandinfluenceplanningoutcomes. SayCanAhnetal.[2022b]integratesLLMswith
robotic capabilities, allowing robots to follow complex, long-term natural language instructions.
Here,theLLMprovidesahigh-levelunderstandingoftheinstructionsandidentifiesskillsthatcan
offercorrespondinglow-levelcontrols. Toavoiderroraccumulationduetomodelstacking,recent
research has explored using MLLMs for planning. ViLa Hu et al. [2023a] leverages the world
knowledgeinherentinMLLMs,includingspatiallayoutsandobjectattributes,tomakemorerational
taskplanningformanipulativetasks. RT-HBelkhaleetal.[2024]improvestaskexecutionaccuracy
andlearningefficiencybydecomposingcomplextasksintosimplelanguageinstructionsthatare
thenconvertedintoroboticactions. Nevertheless,ithasmainlybeeninvestigatedundersmall-scale
andstaticscenarios. Itisunknownwhetherthisphilosophycanalsogeneralizetolarge-scaleand
dynamicautonomousdrivingenvironments. Moreimportantly,itlackssuitabletrainingdatasetsfor
learningsuchsystems. Ourworkhasfilledtheabovegaps.
3 Method
Inthissection,wewillfirstdelineatethetechnicaldetailsofourproposedAD-Hautonomousdriving
system,andthenpresentthenewdatasetfortraininghierarchicalmulti-agentsystems.
3.1 MethodOverview
The AD-H system consists of two MLLM-based agents, namely a planner and a controller, as
illustratedinFigure2(a). Ateachdecisionframe,theplannerconsumesthecurrentvisualinputand
ahigh-levelcontextualinstruction(e.g.,“turnleftatthenextintersection“),performsreasoning&
planning,andmakesadecisionforthecurrentframebypredictingamid-leveldrivingcommand(e.g.,
“slowdowntoensuresafety“).Thecontrollerthenreceivesthepredictedcommandandconvertsitinto
futurewaypointstocontrolthevehicle. Theplannerandcontroller,togetherwiththeinputhigh-level
instruction,thepredictedmid-levelcommands,andlow-levelwaypointsformahierarchicalstructure
ofactionpolicyforautonomousdriving. Theoverallpipelinecanbemathematicallyexpressedas
y =g(f(x ,i),x ,i), (1)
t t t
whereidenotesthecontextualdrivinginstruction,x andy denotesthevisualinputandthepredicted
t t
controlsignals(i.e.,waypoints)forthet-thframe,respectively,andf andgrepresentthehigh-level
plannerandlow-levelcontroller,respectively.
4(a) Pipeline of AD-H
High-level Planner Low-level Controller Driving Control
Q: What motion should the car Q: What action should the car do to
currently take to accomplish the <High-level instruction> with the
instruction <High-level perception and motion <Mid-level
instruction>? driving command>?
Vision Encoder Vision Encoder
Projector Tokenizer Projector Tokenizer
<Control Signal>
MLLM-based Planner Low-level Controller PID
<Mid-level Driving Command> <Hidden State> MLP <Waypoints>
(b) Instruction Examples
Navigation Instruction: Driving Command:
4 surround-view image Embeddings
“Turn left at next intersection.” “Approaching a junction,
prepare to follow traffic rules. High-level instruction Embeddings
Waypoints: Slow down to ensure safety.
Mid-level driving command Embeddings
“(1000, 2314) ...(1121, 2113) Make a slight left turn.”
Figure2: (a)PipelineofAD-H.Theplannerbreaksdownahigh-levelinstructionintomid-level
drivingcommandsandthecontrollerdecodeslow-levelwaypointsfromthemid-levelcommands. (b)
Examplesofahigh-levelinstruction,amid-levelcommand,andlow-levelwaypoints.
3.2 High-levelPlanner
In the AD-H system, the planner focuses solely on high-level decision-making without getting
involved in the generation of low-level control signals and therefore becomes more specialized.
Todoso, the plannerneeds toperform notonlyvisual perceptionto understandthe surrounding
environmentaswellasitsegostatusbutalsoeffectivereasoningandplanningtobreakdownthe
contextual instruction into mid-level driving commands. To this end, we adopt a MLLM as the
high-levelplannertoleveragetheirstrongemergentcapabilities(WemainlyexploreLLaVA-7BLiu
etal.[2024]andMipha-3BZhuetal.[2024]inourexperiments). Figure2(a)illustratesanoverview
oftheMLLM-basedplanner. Ateachdecisionframe,4surround-viewimagesareconcatenatedand
fedintoapre-trainedvisionencoderRadfordetal.[2021]. Theencodedvisualfeaturesarefurther
transformedintothetextualtokenspacethroughaprojector. Finally,thevisualfeaturetogetherwith
the tokenized high-level instruction are sequentially fed into the MLLM to predict the mid-level
commandinanauto-regressivemanner.
Throughinternet-scalepre-trainingandmassiveinstructiontuning,MLLMshaveacquiredpowerful
reasoning ability, along with a wealth of world knowledge, which allows MLLMs to generalize
betteracrossvarioustasksandapplicationscenarios. Wethenproceedwithdownstreamfine-tuning
onourcollectedautonomousdrivingdataset(Section4)toteachMLLMshowtogenerateprecise
mid-levelcommandsthroughthenexttokenpredictiongiventhecontextualinformation. Sincethe
drivingcommandsarealsonaturallanguages,thisdownstreamtaskisessentiallyconsistentwiththe
pre-trainingobjectivesofMLLMs. Assuch,theemergentcapabilitiesofthepre-trainedMLLMscan
befullyunleashed. OurexperimentsshowthattheMLLM-basedplannercanbettergeneralizeto
noveldrivingscenarios,long-horizoninstructions,aswellasunseenenvironments,andevenexhibits
self-correctionabilities.
3.3 LightweightController
Theroleofthecontrolleristotranslatetheintermediatedrivingcommandsgeneratedbytheplanner
intoexecutablecontrolsignals,whichismucheasierthandirectlypredictingthecontrolsignalsfrom
thehigh-levelinstructions. Therefore,insteadofusingthe7BLLaMAmodelLiuetal.[2024]as
inLMDriveShaoetal.[2023],weadoptthemorelightweightOPT-350MZhangetal.[2022]for
this purpose. Since OPT-350M is apure languagemodel, weempowerit with visualperception
abilitybyaddinganadditionalvisionencoderHeetal.[2016]andaQ-FormerLietal.[2023a]. As
5showninFigure2,thepipelineofthecontrollerissimilartothatoftheplanner. Theinputimages
arealsoencodedbythevisionencoderandthenprojectedintothetextualfeaturespacethroughthe
pre-trainedQ-Former. OPT-350Mthentakesasinputthevisualembeddingsaswellasthetextual
tokensofthehigh-levelinstructionandmid-levelcommands. Thehiddenstateofitsoutputlayer
servesastheactionembeddingandisfinallydecodedinto5futurewaypointsthrough2-layerMLP.
Thesewaypointscanbeinputintodownstreamcontrolalgorithms(e.g.,PID)toproducenumerical
informationforvehiclecontrol,suchasspeed,throttle,andsteeringangle. Theabovepipelineforthe
controllercanbemathematicallyexpressedas
h =g (x ,i,c ), (2)
t l t t
y =g (h ), (3)
t w t
wherec representsthemid-levelcommandgeneratedbytheplanner,g andg denotetheOPT-350M
t l w
modelandtheMLPforwaypointregression,respectively,andh indicatesthehiddenstateoutputof
t
OPT-350M.Duringtraining,wefeedtheground-truthmid-levelcommandintothecontrollerand
minimizetheL lossbetweenthepredictedandground-truthwaypoints.
1
4 TrainingDatasetConstruction
Themid-leveldrivingcommandsplayapivotalroleintrainingaproficientplannerandcontroller.
RecentworksSimaetal.[2023],Shaoetal.[2023],Zhouetal.[2024b]havecollectedasignifi-
cantamountofimageandinstructiondatainreal-worldscenariosandclosed-loopsimulatorslike
CARLADosovitskiyetal.[2017]. However,thesedatasetslackconsiderationformid-leveldriving
commands,renderingtrainingimpractical. Toaddressthisissue,wecreateanovelactionhierarchy
datasetLMDrive-HderivedfromLMDrivedatasetShaoetal.[2023]. Ourdatasetcomprisesannota-
tionsacrossthreedistincthierarchicallevels: high-levelinstructions,mid-leveldrivingcommands,
andlow-levelvehiclecontrolsignals. Initially,weextractabout160kvideo-instructionpairsfrom
LMDrivedatasetShaoetal.[2023],alongsidelow-levelvehiclecontrolsignalsforeachframe. Sub-
sequently,leveragingthedetailedmeasurementrecordprovidedbyCARLADosovitskiyetal.[2017]
foreachframe,includingthrottle,speed,steeringangle,etc.,weemployarule-basedmethodology
(SeeSupplementaryMaterials)toretrospectivelydeducethemid-leveldrivingcommandsforeach
frame.
Specifically, we first develop a comprehensive set of driving commands. Autonomous driving,
unlikeroboticgraspingscenes,takesplaceinadynamicandcomplexenvironment,necessitatinga
morefine-grainedcommandconstructionthanmerelyselectingactionslikeacceleration,braking,
orturningleft. Ourfine-graineddrivingcommandencompassesbothperceptualinformationand
motiondetails, includingcrucialdataaboutpedestrians, vehicles, androadsigns. Thisapproach
alignswiththestructureofLLMsandreflectsthechainofthoughtideologyWeietal.[2022],Sima
etal.[2023]. Afterresampling,weobtain1,753Khierarchalannotations. Moredetailsaboutour
datasetarepresentedinSupplementaryMaterialsA.2.
5 Experiments
5.1 ExperimentalSettings
5.1.1 ImplementationDetails.
Ourmainexperimentsareachievedbyusingthepre-trainedLLaVA-7B-V1.5Liuetal.[2024]with
ViTDosovitskiyetal.[2020]visionencoderandtheOPT-350MZhangetal.[2022]withaResNet50
vision encoder as the high-level planner and low-level controller, respectively. We also explore
otherMLLMarchitecturesLiuetal.[2024],Zhuetal.[2024]inSection5.2.4. Unlessotherwise
stated,theAD-Hisfine-tunedonourLMDrive-Hdatasetwithonlyvisionencodersfixed. Forthe
high-levelplanner,theinitiallearningrateissetto2e-5,andafewstepsofwarm-upareincorporated
intothetrainingprocess. Forthelow-levelcontroller,thelearningrateissetto1e-5. Trainingis
conductedusingtheAdamoptimizerwithabatchsizeof32on4NVIDIAA800GPUs. Pleasesee
SupplementaryMaterialsA.1formoreimplementationdetails.
65.1.2 EvaluationBenchmarksandMetrics
Weconductstandardclosed-loopevaluationsusingCARLAsimulatorDosovitskiyetal.[2017]on
theLangAutoBenchmarkShaoetal.[2023]. OntopofLangAuto,wefurtherbuildtwoadditional
benchmarks termed LangAuto-Long-Horizon and LangAuto-Novel-Environment, which contain
long-horizoninstructionsandnovelenvironments,respectively. Wepresenttheirdetailsasfollows.
LangAutoBenchmark. TheLangAutobenchmarkencompassesavarietyoftestroutesspanning
eighttowns,diverseweatherconditions,andmisleadinginterference. Throughoutthetestingpro-
cedure,algorithmsnavigatevehicleswithintheenvironment,utilizingsolelylanguagecommands
and visual input. The LangAuto benchmark is further divided into three sub-tracks: LangAuto,
withrouteslongerthan500meters;LangAuto-Short,withroutesbetween150and500meters;and
LangAuto-Tiny,withroutesshorterthan150meters. WefollowthepriormethodShaoetal.[2023]
andperformevaluationsseparatelyonthesethreesub-tracks.
LangAuto-Long-HorizonBenchmark. Planninganddecision-makingoverlong-timehorizonsis
acentralconcerninembodiedAIPirketal.[2020],Huangetal.[2022a],Zengetal.[2022],Ahn
etal.[2022a],Huangetal.[2022b],whichtypicallynecessitateaseriesofsub-instructionstofulfilla
primarygoal. ToascertaintheeffectivenessofAD-Hinsuchscenarios,wehavebuiltLangAuto-
Long-HorizonbasedontheLangAuto-TinyBenchmarkbycombiningmultipleinstructionstoform
long-horizoninstructions. Forinstance,theinstructionseries"Alright,youcanstartdriving","Keep
on rolling straight till you get to the next junction," and "Continue in a straight line along your
current path" are condensed into a streamlined directive: "Go straight ahead, turn left at the end
oftheroad,thencontinuestraight."Additionally,giventhatneitherourapproachnorthebaseline
modelincorporateshistoricalframeinformation, weincludeenvironmentalcuesinlong-horizon
instructionstoavoidambiguity(suchasuncertaintyaboutwhethertoturnataparticularintersection).
Forinstance,"Gostraightuntilyouseeaturningpointwithpalmtreesahead,thenturnrightand
followtheroad."Detailsoflong-horizoninstructionsarepresentedinSupplementaryMaterialsA.4.2.
Sinceallthelong-horizoninstructionsareabsentfromtheLMDrive-Htrainingset,theLangAuto-
Long-Horizonbenchmarkcanalsoverifythegeneralizationabilityofautonomousdrivingsystemsto
novelinstructions.
LangAuto-Novel-EnvironmentBenchmark. Toevaluatethegeneralizationabilityofautonomous
drivingsystemsundernewenvironments,wehavebuiltLangAuto-Novel-Environmentbasedonthe
LangAuto-TinyBenchmarkbyonlyretainingdrivingroutesfrom7outof8Towns(Town02-07,10).
Toensurenon-overlapbetweentrainingandtesting,wehavefurtherremovedtrainingdatabelonging
totheabove7TownsfromtheLMDrive-Htrainingset.
EvaluationMetrics. WeemploythreewidelyusedevaluationmetricsfromtheCARLALeader-
BoardDosovitskiyetal.[2017],includingroutecompletion(RC),infractionscore(IS),anddriving
score (DS). Among them, RC measures the percentage of the planned route that is successfully
completed,withaspecificfocusonthedistancecoveredalongdesignatedsegments. Anysignificant
deviation from the intended route leads to the episode being marked as a failure. The IS metric
keepstrackofviolationssuchascollisionsortrafficinfractions,whichdecreasethescorewitheach
offense. TheDSmetriccombinesboththeRCandISscorestoprovideacomprehensiveassessment
ofprogressandsafety,servingastheprimaryevaluationmetric.
5.2 ResultsandAnalysis
Inthissection,wemainlyinvestigatetheperformanceoftheautonomousdrivingmodelsfromfour
perspectives: (1) standard evaluation in a closed-loop manner, (2) generalization to novel long-
horizoninstructions,(3)generalizationtonovelenvironments,and(4)performanceachievedbyusing
differentMLLMsasplanners.AstheLangAutoisanewbenchmark,onlytheresultofLMDriveShao
etal.[2023]isavailable. Therefore,weadoptLMDriveasourmaincompetitor. Itshouldbenoted
thatLMDriveisoneofthepioneeringworksinlanguage-guidedclosed-loopdrivingandcanserveas
astrongbaselineofourmethodwithoutusinghierarchicalagents.
75.2.1 Closed-loopDrivingPerformance
Table1reportsthequantitativecomparisonsontheLangAutobenchmark. ItshowsthatourAD-H
significantly outperforms LMDrive for the three different sub-tracks, especially in terms of the
mainscoreDS,indicatingthatthemid-leveldrivingcommandsgeneratedbyourplannerenablethe
controllertoactmoreaccuratelywithinlarge-scaleandcomplexenvironments. Throughextensive
analysis, wefurtherobservethatAD-Hexhibitsfrequentself-correctionbehaviors. Asshownin
Figure 3, LMDrive fails to recognize the road conditions after a left turn, causing the vehicle to
continuemaintainingthesteeringwheelstraightasthepreviouslyreceivedhigh-levelnavigation
instruction. Consequently,thevehiclecrossesthelaneboundaryanddeviatesfromthecorrectpath.
Incontrast,ourAD-Hutilizesitsplannertodynamicallygeneratemid-levelcommands,enabling
thecontrollertoadjustitspostureaccordingly,whicheffectivelyreducestheriskoftrafficjamsand
accidents. MorevisualizationsareprovidedintheSupplementaryMaterialsA.4.2.
Table1: ComparisononLangAutobenchmark.
Method LangAuto LangAuto-Short LangAuto-Tiny
DS(↑) RC(↑) IS(↑) DS(↑) RC(↑) IS(↑) DS(↑) RC(↑) IS(↑)
LMDrive(LLaVA-7B)Shaoetal.[2023] 36.2 46.5 0.81 50.6 60.0 0.84 66.5 77.9 0.85
AD-H(Mipha-3B+OPT350m) 41.1 48.5 0.86 54.3 61.8 0.86 68.0 74.4 0.87
AD-H(LLaVA-7B+OPT350m) 44.0 53.2 0.83 56.1 68.0 0.78 77.5 85.1 0.91
(a) High-level Instruction: Continue in a straight line along your current path.
T0 T10 T20
(b)
Deviate Route
(c)
Keep Route
Keep Route
(d) Keep the steering wheel straight. Make a slight right turn. Keep the steering wheel straight.
Figure3: Resultsofself-correctionscenario. (a)High-levelinstruction;(b)Visualizationresultsof
LMDrive;(c)VisualizationresultsofAD-H;(d)Mid-leveldrivingcommandspredictedbytheplanner
ofAD-H.ThevisualresultsshowthatLMDrivemaintainsastraighttrajectoryafteroversteering,
deviatingfromtheintendedpath. However,AD-Hisabletoissueprecisecommandstoguidethe
vehiclebackontrack.
5.2.2 GeneralizationtoLong-HorizonInstruction
Table2presentstheresultsontheLangAuto-Long-Horizonbenchmark,wherethehigh-levelnav-
igation instructions are long-horizon and are provided only at the beginning of the driving task.
ConsideringthatbothLMDriveandAD-Haretrainedundershort-horizoninstructionsduringthe
drivingprocess,theseunseenlong-horizoninstructionsettingsposeasignificantchallengeinterms
8Table 2: Comparison on LangAuto-Long- Table 3: Comparison on LangAuto-Novel-
Horizonbenchmark. Environmentbenchmark.
Method DS(↑) IS(↑) RC(↑) Method DS(↑) IS(↑) RC(↑)
LMdriveShaoetal.[2023] 49.1 0.871 56.4 LMdriveShaoetal.[2023] 53.4 0.827 64.3
AD-H 62.1 0.875 68.3 AD-H 59.9 0.875 67.8
Long-horizon Instruction : Go straight until you see a turning point with palm trees ahead,
(a) then turn right and follow the road.
T0 T10 T20
(b)
Continue straight.
Missing turning point!
(c)
Right command！
Right control signal！
(d) Keep the steering wheel straight. Make a slight right turn. Make a slight right turn.
Figure4: Resultswithlong-horizoninstructions(a). (b)LMDrivepersistsinfollowingtheinitial
instructions,continuingforward;(c)AD-Hcanadeptlyassessenvironmentalcuestodeterminethe
appropriatetimingforturning;(d)Mid-levelcommandsproducedbyAD-H.
oftheirgeneralizationability. Nevertheless,ourAD-Hstilldeliversstrongperformance,surpassing
theLMDrivemethodbyaconsiderablemargin. AsillustratedinFigure4,LMDrive,whichdirectly
predictscontrolsignals,strugglestoproperlyunderstandtheglobalinstructionsandroadconditions
providedinthelong-horizoninstruction. Consequently,itcontinuesstraightinsteadofmakingaright
turnwhennecessary. Incomparison,theplannerofourAD-Hcontinuouslyanalyzestheinstructions
and the visual environment during the driving process, providing accurate and fine-grained com-
mandstothecontrollerbasedonthecurrentdrivingconditions. Theseresultsindicatethepromising
generalizationcapabilityofAD-Hforunseennavigationinstructions.
5.2.3 GeneralizationtoNovelEnvironments
Table3comparesAD-HandLMDriveontheLangAuto-Novel-Environmentbenchmarktoassess
theirzero-shotadaptationcapabilitiestotheunseenenvironment.OurAD-Hconsistentlyoutperforms
LMDriveacrossallthemetrics,whichverifiesitsstronggeneralizationabilitytonovelenvironments.
5.2.4 ExperimentonDifferentPlannerArchitectures.
Table4reportstheresultsusingdifferentMLLMfortheplanner. FollowingShaoetal.[2023],We
alsotestadditionalmetricsincludingVehicleCollisions(VC),PedestrianCollisions(PC),Layout
Collision(LC),RedLightViolations(RV),andOffroadInfractions(OI).Theresultssuggestthat
usingasmaller3BMLLMfortheplannerleadstoaslightperformancedecrease.However,itremains
competitivecomparedtoLMDrive,whichisbasedonthelargerLLaVA-7Bmodel.
9Table4: ExperimentsofDifferentPlanners.
Method DS(↑) RC(↑) IS(↑) VC(↓) PC(↓) LC(↓) RV(↓) OI(↓)
LMDrive (LLaVA-7B) 66.5 77.9 0.81 - - - - -
AD-H (LLaVA-7B) 77.5 85.1 0.908 0.000 0.000 0.000 0.082 1.620
AD-H (Mipha-3B) 66.4 79.4 0.793 1.828 0.000 2.370 0.082 1.765
6 ConclusionandLimitation
Conclusion Inconclusion,ourproposedhierarchicalmulti-agentdrivingsystem,AD-H,bridges
high-levelinstructionsandlow-levelcontrolsignalswithmid-levellanguage-drivencommands. By
liberating the multimodal large language models from the burden of decoding low-level control
signals,AD-Hfullyleveragestheiremergentcapabilitiesinhigh-levelperception,reasoning,and
planning. Thishierarchicaldesignnotonlyenhancestheefficiencyandreliabilityofautonomous
drivingsystemsbutalsoenablesthemtoachieveremarkabledrivingperformanceeveninscenarios
notencounteredduringtraining. Throughcomprehensiveevaluation,AD-Houtperformsthestate-of-
the-artmethod,demonstratingremarkabledrivingperformanceandadaptabilitytonovelscenarios
andinstructions. TheproposedAD-Hharnessestheemergentpowersofmultimodallargelanguage
models,enhancingtheefficiencyandreliabilityofautonomousdrivingsystems.
Limitation GiventhatAD-Hoperatesasahierarchicalagentsystem,bothitssizeandcomputational
needs are significant. Achieving a lighter version for deployment on actual vehicles will require
notableadvancements. Moreover,sinceourexperimentaldatamainlycomesfromsimulations,it’s
crucialtogathermorereal-worlddatatoimprovedomaintransfereffectively. Additionally,because
virtualscenariosofferlimiteddatadiversity,it’surgenttohavericherdatasets. Thesedatasetsare
essentialforrefininginstructiontuningandenhancingthecapabilitiesofMLLMs.
10References
M.Ahn,A.Brohan,N.Brown,Y.Chebotar,O.Cortes,B.David,C.Finn,C.Fu,K.Gopalakrishnan,
K.Hausman,etal. Doasican,notasisay: Groundinglanguageinroboticaffordances. arXiv
preprintarXiv:2204.01691,2022a.
M.Ahn,A.Brohan,N.Brown,Y.Chebotar,O.Cortes,B.David,C.Finn,C.Fu,K.Gopalakrishnan,
K.Hausman,etal. Doasican,notasisay: Groundinglanguageinroboticaffordances. arXiv
preprintarXiv:2204.01691,2022b.
X.Bai,Z.Hu,X.Zhu,Q.Huang,Y.Chen,H.Fu,andC.-L.Tai. Transfusion: Robustlidar-camera
fusionfor3dobjectdetectionwithtransformers. InProceedingsoftheIEEE/CVFconferenceon
computervisionandpatternrecognition,pages1090–1099,2022.
S.Belkhale,T.Ding,T.Xiao,P.Sermanet,Q.Vuong,J.Tompson,Y.Chebotar,D.Dwibedi,and
D.Sadigh. Rt-h: Actionhierarchiesusinglanguage. arXivpreprintarXiv:2403.01823,2024.
A.Brohan,N.Brown,J.Carbajal,Y.Chebotar,J.Dabis,C.Finn,K.Gopalakrishnan,K.Hausman,
A.Herzog,J.Hsu,etal. Rt-1: Roboticstransformerforreal-worldcontrolatscale. arXivpreprint
arXiv:2212.06817,2022.
A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess,
A.Dubey,C.Finn,etal. Rt-2: Vision-language-actionmodelstransferwebknowledgetorobotic
control. arXivpreprintarXiv:2307.15818,2023.
L.Chen,O.Sinavski,J.Hünermann,A.Karnsund,A.J.Willmott,D.Birch,D.Maund,andJ.Shotton.
Drivingwithllms: Fusingobject-levelvectormodalityforexplainableautonomousdriving. arXiv
preprintarXiv:2310.01957,2023a.
L.Chen,O.Sinavski,J.Hünermann,A.Karnsund,A.J.Willmott,D.Birch,D.Maund,andJ.Shotton.
Drivingwithllms: Fusingobject-levelvectormodalityforexplainableautonomousdriving. arXiv
preprintarXiv:2310.01957,2023b.
Z.Chen,Z.Li,S.Zhang,L.Fang,Q.Jiang,F.Zhao,B.Zhou,andH.Zhao. Autoalign:pixel-instance
featureaggregationformulti-modal3dobjectdetection. arXivpreprintarXiv:2201.06493,2022.
Z. Chen, Z. Shi, X. Lu, L. He, S. Qian, H. S. Fang, Z. Yin, W. Ouyang, J. Shao, Y. Qiao, et al.
Rh20t-p: A primitive-level robotic dataset towards composable generalization agents. arXiv
preprintarXiv:2403.19622,2024.
K. Chitta, A. Prakash, B. Jaeger, Z. Yu, K. Renz, and A. Geiger. Transfuser: Imitation with
transformer-basedsensorfusionforautonomousdriving. IEEETransactionsonPatternAnalysis
andMachineIntelligence,2022.
F.Codevilla,E.Santana,A.M.López,andA.Gaidon. Exploringthelimitationsofbehaviorcloning
forautonomousdriving. InProceedingsoftheIEEE/CVFinternationalconferenceoncomputer
vision,pages9329–9338,2019.
J.Cui,H.Qiu,D.Chen,P.Stone,andY.Zhu. Coopernaut: End-to-enddrivingwithcooperative
perceptionfornetworkedvehicles. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages17252–17262,2022.
W.Dai,J.Li,D.Li,A.M.H.Tiong,J.Zhao,W.Wang,B.Li,P.N.Fung,andS.Hoi. Instructblip:
Towardsgeneral-purposevision-languagemodelswithinstructiontuning. AdvancesinNeural
InformationProcessingSystems,36,2024.
S.Deshmukh,B.Elizalde,R.Singh,andH.Wang. Pengi: Anaudiolanguagemodelforaudiotasks.
AdvancesinNeuralInformationProcessingSystems,36:18090–18108,2023.
X.Ding,J.Han,H.Xu,X.Liang,W.Zhang,andX.Li. Holisticautonomousdrivingunderstanding
bybird’s-eye-viewinjectedmulti-modallargemodels. arXivpreprintarXiv:2401.00988,2024.
A.Dosovitskiy,G.Ros,F.Codevilla,A.Lopez,andV.Koltun. CARLA:Anopenurbandriving
simulator. InProceedingsofthe1stAnnualConferenceonRobotLearning,pages1–16,2017.
11A.Dosovitskiy,L.Beyer,A.Kolesnikov,D.Weissenborn,X.Zhai,T.Unterthiner,M.Dehghani,
M.Minderer,G.Heigold,S.Gelly,etal. Animageisworth16x16words: Transformersforimage
recognitionatscale. arXivpreprintarXiv:2010.11929,2020.
D.Driess,F.Xia,M.S.Sajjadi,C.Lynch,A.Chowdhery,B.Ichter,A.Wahid,J.Tompson,Q.Vuong,
T.Yu,etal. Palm-e: Anembodiedmultimodallanguagemodel. arXivpreprintarXiv:2303.03378,
2023.
C.Ge,J.Chen,E.Xie,Z.Wang,L.Hong,H.Lu,Z.Li,andP.Luo. Metabev: Solvingsensorfailures
for3ddetectionandmapsegmentation. InProceedingsoftheIEEE/CVFInternationalConference
onComputerVision,pages8721–8731,2023.
Z.Guo,R.Zhang,X.Zhu,Y.Tang,X.Ma,J.Han,K.Chen,P.Gao,X.Li,H.Li,etal. Point-bind
& point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and
instructionfollowing. arXivpreprintarXiv:2309.00615,2023.
N.Hanselmann, K.Renz, K.Chitta, A.Bhattacharyya, andA.Geiger. King: Generatingsafety-
criticaldrivingscenariosforrobustimitationviakinematicsgradients. InEuropeanConferenceon
ComputerVision,pages335–352.Springer,2022.
K.He,X.Zhang,S.Ren,andJ.Sun. Deepresiduallearningforimagerecognition. InProceedings
oftheIEEEconferenceoncomputervisionandpatternrecognition,pages770–778,2016.
Y.Hong,H.Zhen,P.Chen,S.Zheng,Y.Du,Z.Chen,andC.Gan. 3d-llm:Injectingthe3dworldinto
largelanguagemodels. AdvancesinNeuralInformationProcessingSystems,36:20482–20494,
2023.
Y.Hu,F.Lin,T.Zhang,L.Yi,andY.Gao. Lookbeforeyouleap: Unveilingthepowerofgpt-4vin
roboticvision-languageplanning. arXivpreprintarXiv:2311.17842,2023a.
Y.Hu,J.Yang,L.Chen,K.Li,C.Sima,X.Zhu,S.Chai,S.Du,T.Lin,W.Wang,L.Lu,X.Jia,
Q.Liu,J.Dai,Y.Qiao,andH.Li. Planning-orientedautonomousdriving. InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),pages17853–17862,
June2023b.
J.Huang,G.Huang,Z.Zhu,andD.Du. Bevdet: High-performancemulti-camera3dobjectdetection
inbird-eye-view. arXivpreprintarXiv:2112.11790,2021.
W.Huang,P.Abbeel,D.Pathak,andI.Mordatch. Languagemodelsaszero-shotplanners:Extracting
actionableknowledgeforembodiedagents. InInternationalConferenceonMachineLearning,
pages9118–9147.PMLR,2022a.
W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch,
Y.Chebotar,etal. Innermonologue: Embodiedreasoningthroughplanningwithlanguagemodels.
arXivpreprintarXiv:2207.05608,2022b.
F.Jia,W.Mao,Y.Liu,Y.Zhao,Y.Wen,C.Zhang,X.Zhang,andT.Wang. Adriver-i: Ageneral
worldmodelforautonomousdriving. arXivpreprintarXiv:2311.13549,2023.
Y.Jiao,Z.Jie,S.Chen,J.Chen,L.Ma,andY.-G.Jiang. Msmdfusion: Fusinglidarandcameraat
multiplescaleswithmulti-depthseedsfor3dobjectdetection. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages21643–21652,2023.
X.Lai,Z.Tian,Y.Chen,Y.Li,Y.Yuan,S.Liu,andJ.Jia. Lisa: Reasoningsegmentationvialarge
languagemodel. arXivpreprintarXiv:2308.00692,2023.
B.Li,Y.Wang,J.Mao,B.Ivanovic,S.Veer,K.Leung,andM.Pavone. Drivingeverywherewith
largelanguagemodelpolicyadaptation. arXivpreprintarXiv:2402.05932,2024.
J.Li,D.Li,S.Savarese,andS.Hoi. Blip-2: Bootstrappinglanguage-imagepre-trainingwithfrozen
imageencodersandlargelanguagemodels. In Internationalconferenceonmachine learning,
pages19730–19742.PMLR,2023a.
K. Li, Y. He, Y. Wang, Y. Li, W. Wang, P. Luo, Y. Wang, L. Wang, and Y. Qiao. Videochat:
Chat-centricvideounderstanding. arXivpreprintarXiv:2305.06355,2023b.
12Y.Li,H.Bao,Z.Ge,J.Yang,J.Sun,andZ.Li. Bevstereo: Enhancingdepthestimationinmulti-view
3dobjectdetectionwithdynamictemporalstereo. arXivpreprintarXiv:2209.10248,2022a.
Y.Li,X.Qi,Y.Chen,L.Wang,Z.Li,J.Sun,andJ.Jia. Voxelfieldfusionfor3dobjectdetection. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
1120–1129,2022b.
Y.Li, A.W.Yu, T.Meng, B.Caine, J.Ngiam, D.Peng, J.Shen, Y.Lu, D.Zhou, Q.V.Le, etal.
Deepfusion: Lidar-cameradeepfusionformulti-modal3dobjectdetection. InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pages17182–17191,2022c.
Z.Li, W.Wang, H.Li, E.Xie, C.Sima, T.Lu, Y.Qiao, andJ.Dai. Bevformer: Learningbird’s-
eye-viewrepresentationfrommulti-cameraimagesviaspatiotemporaltransformers. InEuropean
conferenceoncomputervision,pages1–18.Springer,2022d.
Z.Li,S.Lan,J.M.Alvarez,andZ.Wu. Bevnext: Revivingdensebevframeworksfor3dobject
detection. arXivpreprintarXiv:2312.01696,2023c.
Z. Li, Z. Yu, W. Wang, A. Anandkumar, T. Lu, and J. M. Alvarez. Fb-bev: Bev representation
fromforward-backwardviewtransformations. InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,pages6919–6928,2023d.
T.Liang,H.Xie,K.Yu,Z.Xia,Z.Lin,Y.Wang,T.Tang,B.Wang,andZ.Tang. Bevfusion: A
simpleandrobustlidar-camerafusionframework. AdvancesinNeuralInformationProcessing
Systems,35:10421–10434,2022.
S.Lifshitz, K.Paster, H.Chan, J.Ba, andS.McIlraith. Steve-1: Agenerativemodelfortext-to-
behaviorinminecraft. AdvancesinNeuralInformationProcessingSystems,36,2024.
H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. Advances in neural information
processingsystems,36,2024.
J.Liu,P.Hang,X.Qi,J.Wang,andJ.Sun. Mtd-gpt: Amulti-taskdecision-makinggptmodelfor
autonomousdrivingatunsignalizedintersections. In2023IEEE26thInternationalConferenceon
IntelligentTransportationSystems(ITSC),pages5154–5161.IEEE,2023a.
Z.Liu,H.Tang,A.Amini,X.Yang,H.Mao,D.L.Rus,andS.Han. Bevfusion: Multi-taskmulti-
sensorfusionwithunifiedbird’s-eyeviewrepresentation. In2023IEEEinternationalconference
onroboticsandautomation(ICRA),pages2774–2781.IEEE,2023b.
Y.Ma,Y.Cao,J.Sun,M.Pavone,andC.Xiao. Dolphins: Multimodallanguagemodelfordriving.
arXivpreprintarXiv:2312.00438,2023.
J.Mao,J.Ye,Y.Qian,M.Pavone,andY.Wang. Alanguageagentforautonomousdriving. arXiv
preprintarXiv:2311.10813,2023.
J.Park,C.Xu,S.Yang,K.Keutzer,K.Kitani,M.Tomizuka,andW.Zhan. Timewilltell: Newout-
looksandabaselinefortemporalmulti-view3dobjectdetection. arXivpreprintarXiv:2210.02443,
2022.
P.Paul,A.Garg,T.Choudhary,A.K.Singh,andK.M.Krishna. Lego-drive: Language-enhanced
goal-orientedclosed-loopend-to-endautonomousdriving. arXivpreprintarXiv:2403.20116,2024.
M. Peng, X. Guo, X. Chen, M. Zhu, K. Chen, X. Wang, Y. Wang, et al. Lc-llm: Explainable
lane-change intention and trajectory predictions with large language models. arXiv preprint
arXiv:2403.18344,2024.
Z.Peng,W.Wang,L.Dong,Y.Hao,S.Huang,S.Ma,andF.Wei. Kosmos-2: Groundingmultimodal
largelanguagemodelstotheworld. arXivpreprintarXiv:2306.14824,2023.
J.PhilionandS.Fidler. Lift,splat,shoot: Encodingimagesfromarbitrarycamerarigsbyimplicitly
unprojectingto3d. InComputerVision–ECCV2020: 16thEuropeanConference,Glasgow,UK,
August23–28,2020,Proceedings,PartXIV16,pages194–210.Springer,2020.
13S. Pirk, K. Hausman, A. Toshev, and M. Khansari. Modeling long-horizon tasks as sequential
interactionlandscapes. arXivpreprintarXiv:2006.04843,2020.
A.Prakash,K.Chitta,andA.Geiger. Multi-modalfusiontransformerforend-to-endautonomous
driving. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,
pages7077–7087,2021.
Y.Qin,C.Wang,Z.Kang,N.Ma,Z.Li,andR.Zhang. Supfusion: Supervisedlidar-camerafusion
for3dobjectdetection. InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision(ICCV),pages22014–22024,October2023a.
Y.Qin,E.Zhou,Q.Liu,Z.Yin,L.Sheng,R.Zhang,Y.Qiao,andJ.Shao. Mp5: Amulti-modal
open-endedembodiedsysteminminecraftviaactiveperception. arXivpreprintarXiv:2312.07472,
2023b.
A.Radford,J.W.Kim,C.Hallacy,A.Ramesh,G.Goh,S.Agarwal,G.Sastry,A.Askell,P.Mishkin,
J. Clark, et al. Learning transferable visual models from natural language supervision. In
Internationalconferenceonmachinelearning,pages8748–8763.PMLR,2021.
H.Sha, Y.Mu, Y.Jiang, L.Chen, C.Xu, P.Luo, S.E.Li, M.Tomizuka, W.Zhan, andM.Ding.
Languagempc: Largelanguagemodelsasdecisionmakersforautonomousdriving. arXivpreprint
arXiv:2310.03026,2023.
H.Shao, Y.Hu, L.Wang, S.L.Waslander, Y.Liu, andH.Li. Lmdrive: Closed-loopend-to-end
drivingwithlargelanguagemodels. arXivpreprintarXiv:2312.07488,2023.
C.Sima, K.Renz, K.Chitta, L.Chen, H.Zhang, C.Xie, P.Luo, A.Geiger, andH.Li. Drivelm:
Drivingwithgraphvisualquestionanswering. arXivpreprintarXiv:2312.14150,2023.
X.Tian,J.Gu,B.Li,Y.Liu,C.Hu,Y.Wang,K.Zhan,P.Jia,X.Lang,andH.Zhao. Drivevlm:
The convergence of autonomous driving and large vision-language models. arXiv preprint
arXiv:2402.12289,2024.
C.Wang,Y.Qin,Z.Kang,N.Ma,andR.Zhang. Towardaccuratecamera-based3dobjectdetection
viacascadedepthestimationandcalibration. arXivpreprintarXiv:2402.04883,2024a.
G.Wang,Y.Xie,Y.Jiang,A.Mandlekar,C.Xiao,Y.Zhu,L.Fan,andA.Anandkumar. Voyager: An
open-endedembodiedagentwithlargelanguagemodels. arXivpreprintarXiv:2305.16291,2023a.
T.Wang,E.Xie,R.Chu,Z.Li,andP.Luo. Drivecot: Integratingchain-of-thoughtreasoningwith
end-to-enddriving. arXivpreprintarXiv:2403.16996,2024b.
W.Wang,J.Xie,C.Hu,H.Zou,J.Fan,W.Tong,Y.Wen,S.Wu,H.Deng,Z.Li,etal. Drivemlm:
Aligning multi-modal large language models with behavioral planning states for autonomous
driving. arXivpreprintarXiv:2312.09245,2023b.
X.Wang,Z.Zhu,G.Huang,X.Chen,andJ.Lu. Drivedreamer: Towardsreal-world-drivenworld
modelsforautonomousdriving. arXivpreprintarXiv:2309.09777,2023c.
Y.Wang,Y.Chen,andZ.Zhang. Frustumformer:Adaptiveinstance-awareresamplingformulti-view
3ddetection. arXivpreprintarXiv:2301.04467,2023d.
Y. Wang, R. Jiao, C. Lang, S. S. Zhan, C. Huang, Z. Wang, Z. Yang, and Q. Zhu. Empow-
ering autonomous driving with large language models: A safety perspective. arXiv preprint
arXiv:2312.00812,2023e.
Z.Wang,S.Cai,A.Liu,Y.Jin,J.Hou,B.Zhang,H.Lin,Z.He,Z.Zheng,Y.Yang,etal. Jarvis-1:
Open-world multi-task agents with memory-augmented multimodal language models. arXiv
preprintarXiv:2311.05997,2023f.
Z.Wang,D.Li,C.Luo,C.Xie,andX.Yang. Distillbev: Boostingmulti-camera3dobjectdetection
withcross-modalknowledgedistillation.InProceedingsoftheIEEE/CVFInternationalConference
onComputerVision,pages8637–8646,2023g.
14J.Wei,X.Wang,D.Schuurmans,M.Bosma,F.Xia,E.Chi,Q.V.Le,D.Zhou,etal.Chain-of-thought
promptingelicitsreasoninginlargelanguagemodels. Advancesinneuralinformationprocessing
systems,35:24824–24837,2022.
L. Wen, D. Fu, X. Li, X. Cai, T. Ma, P. Cai, M. Dou, B. Shi, L. He, and Y. Qiao. Dilu: A
knowledge-drivenapproachtoautonomousdrivingwithlargelanguagemodels. arXivpreprint
arXiv:2309.16292,2023a.
Y. Wen, Y. Zhao, Y. Liu, F. Jia, Y. Wang, C. Luo, C. Zhang, T. Wang, X. Sun, and X. Zhang.
Panacea: Panoramicandcontrollablevideogenerationforautonomousdriving. arXivpreprint
arXiv:2311.16813,2023b.
P.Wu,X.Jia,L.Chen,J.Yan,H.Li,andY.Qiao. Trajectory-guidedcontrolpredictionforend-to-end
autonomousdriving: Asimpleyetstrongbaseline. AdvancesinNeuralInformationProcessing
Systems,35:6119–6132,2022.
Y.Xiao,F.Codevilla,D.P.Bustamante,andA.M.Lopez. Scalingself-supervisedend-to-enddriving
withmulti-viewattentionlearning. arXivpreprintarXiv:2302.03198,2,2023.
Z.Xu,Y.Zhang,E.Xie,Z.Zhao,Y.Guo,K.K.Wong,Z.Li,andH.Zhao. Drivegpt4: Interpretable
end-to-endautonomousdrivingvialargelanguagemodel. arXivpreprintarXiv:2310.01412,2023.
C.Yang,Y.Chen,H.Tian,C.Tao,X.Zhu,Z.Zhang,G.Huang,H.Li,Y.Qiao,L.Lu,etal.Bevformer
v2: Adaptingmodernimagebackbonestobird’s-eye-viewrecognitionviaperspectivesupervision.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
17830–17839,2023.
R.Yang,X.Zhang,A.Fernandez-Laaksonen,X.Ding,andJ.Gong. Drivingstylealignmentfor
llm-powereddriveragent. arXivpreprintarXiv:2403.11368,2024.
X.Ye,F.Tao,A.Mallik,B.Yaman,andL.Ren. Lord: Largemodelsbasedoppositerewarddesign
forautonomousdriving. arXivpreprintarXiv:2403.18965,2024.
Z.Yin,J.Wang,J.Cao,Z.Shi,D.Liu,M.Li,X.Huang,Z.Wang,L.Sheng,L.Bai,etal. Lamm:
Language-assistedmulti-modalinstruction-tuningdataset,framework,andbenchmark. Advances
inNeuralInformationProcessingSystems,36,2024.
J.H.Yoo,Y.Kim,J.Kim,andJ.W.Choi. 3d-cvf: Generatingjointcameraandlidarfeaturesusing
cross-viewspatialfeaturefusionfor3dobjectdetection. InComputerVision–ECCV2020: 16th
EuropeanConference, Glasgow, UK,August23–28, 2020, Proceedings, PartXXVII16, pages
720–736.Springer,2020.
A. Zeng, M. Attarian, B. Ichter, K. Choromanski, A. Wong, S. Welker, F. Tombari, A. Purohit,
M.Ryoo,V.Sindhwani,etal. Socraticmodels: Composingzero-shotmultimodalreasoningwith
language. arXivpreprintarXiv:2204.00598,2022.
H.Zhang,X.Li,andL.Bing. Video-llama: Aninstruction-tunedaudio-visuallanguagemodelfor
videounderstanding. arXivpreprintarXiv:2306.02858,2023a.
J.Zhang,Y.Zhang,Q.Liu,andY.Wang. Sa-bev: Generatingsemantic-awarebird’s-eye-viewfeature
formulti-view3dobjectdetection. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages3348–3357,2023b.
R.Zhang,J.Han,C.Liu,P.Gao,A.Zhou,X.Hu,S.Yan,P.Lu,H.Li,andY.Qiao. Llama-adapter:
Efficientfine-tuningoflanguagemodelswithzero-initattention. arXivpreprintarXiv:2303.16199,
2023c.
S.Zhang,S.Roller,N.Goyal,M.Artetxe,M.Chen,S.Chen,C.Dewan,M.Diab,X.Li,X.V.Lin,
etal. Opt: Openpre-trainedtransformerlanguagemodels. arXivpreprintarXiv:2205.01068,2022.
G.Zhao,X.Wang,Z.Zhu,X.Chen,G.Huang,X.Bao,andX.Wang.Drivedreamer-2:Llm-enhanced
worldmodelsfordiversedrivingvideogeneration. arXivpreprintarXiv:2403.06845,2024.
15E. Zhou, Y. Qin, Z. Yin, Y. Huang, R. Zhang, L. Sheng, Y. Qiao, and J. Shao. Minedreamer:
Learning to follow instructions via chain-of-imagination for simulated-world control. arXiv
preprintarXiv:2403.12037,2024a.
H.Zhou,Z.Ge,Z.Li,andX.Zhang. Matrixvt: Efficientmulti-cameratobevtransformationfor3d
perception. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages
8548–8557,2023a.
X.Zhou,M.Liu,B.L.Zagar,E.Yurtsever,andA.C.Knoll. Visionlanguagemodelsinautonomous
drivingandintelligenttransportationsystems. arXivpreprintarXiv:2310.14414,2023b.
Y.Zhou,L.Huang,Q.Bu,J.Zeng,T.Li,H.Qiu,H.Zhu,M.Guo,Y.Qiao,andH.Li. Embodied
understandingofdrivingscenarios. arXivpreprintarXiv:2403.04593,2024b.
D.Zhu,J.Chen,X.Shen,X.Li,andM.Elhoseiny. Minigpt-4: Enhancingvision-languageunder-
standingwithadvancedlargelanguagemodels. arXivpreprintarXiv:2304.10592,2023.
M.Zhu,Y.Zhu,X.Liu,N.Liu,Z.Xu,C.Shen,Y.Peng,Z.Ou,F.Feng,andJ.Tang.Acomprehensive
overhaulofmultimodalassistantwithsmalllanguagemodels. arXivpreprintarXiv:2403.06199,
2024.
16A SupplementalMaterial
A.1 ImplementationDetails
A.1.1 High-levelPlanner
Visual Input. The motion planner receives visual input from four directional cameras, each
capturinganRGBimage. Tomaintainconsistencywiththepre-trainedVLM,weconcatenatethese
four images vertically and feed them into the visual encoder together. This approach offers two
advantages: firstly,italignswiththeinputformatofthepre-trainedVLM,preventingconfusionthat
mightarisefromseparateinputs;secondly,itreducescomputationalcomplexitybyminimizingthe
tokencount. Preliminaryexperimentsindicatethatcombiningthefourimagesadequatelymeetsthe
requirementsforinput.
TextualInput. TheplannerinAD-Hispivotalasitbreaksdownhigh-levelnavigationinstructions
intomid-leveldrivingcommands. Indetail,thetextualinputoftheplanneris“Whatmotionshould
thecarcurrentlytaketoaccomplishtheinstruction<High-levelInstruction>?”.
Training. Inourexperiments,weemploytwoscalesofMLLMs: LLaVA-7B-V1.5Liuetal.[2024]
andMipha-3BZhuetal.[2024]. Wefine-tunetheirpre-trainedversionsontheLMDrive-Hdataset
foroneepochusing4×A800s,withthevisualencoderkeptfrozen. Duringtheindependenttraining
oftheplanner,weassessitsperformancebymeasuringaccuracyonthevalidationset,astheAD-H
systemonlysupportscombinedtesting. Thebatchsizeissetto32,and3%ofthetotalstepsare
allocatedforwarm-up. WeutilizetheAdamoptimizerwithaninitiallearningrateof2e-5.
A.1.2 Low-levelController
Model. Similartotheplanner,thecontrollerusestheResNet50Heetal.[2016]modeltoextract
featuresfromimagescapturedfromfourdifferentangles. textualinputoftheplanneris“Whataction
shouldthecardoto<High-levelInstruction>withtheperceptionandmotion<Mid-levelDriving
Command>?”. Thesefeaturesarethenprojectedintothecontroller’sinputspacefortheLLMby
anadaptermadeupofMLP,Whichareconcatenatwithmotionembeddings,whichareprocessed
fromdrivingcommandsprovidedbythehigh-levelplannerthroughatokenizer.Weultimatelychose
OPT-350mforitsoptimalbalanceofperformanceandspeed. Thefinallayer’shiddenfeaturesfrom
thismodelarefedintoanMLP-basedwaypointspredictor,whichgeneratesthevehicle’spositionfor
theupcomingfive-timesteps. Thesepositiondetailsarethentranslatedintodirectcontrolsignalslike
steeringandthrottlethroughaPIDalgorithmtointeractwiththevehicle.
Training. Specifically,ourexperimentsareconductedonfourA800GPUswithabatchsizeof32.
Thevisualencoder,ResNet50,underwentthesamepre-trainingasusedintheLMDriveShaoetal.
[2023]. Aswiththecontroller,wesetthelearningrateat1e-5,withaweightdecayof0.06. Since
thecontrollerdirectlygenerateswaypoints. WetraincontrollerwithL1lossanduseitasevaluation
metrics.
A.2 DatasetDetails
A.2.1 Overview
TheAD-HDatasetisaninnovativeactionhierarchydatasetspecificallydesignedforautonomous
driving. Itfocusesonmid-leveldrivingcommands,makingtrainingmorepractical. Specifically,
theAD-HDatasetincludes1.7millionentries,eachcontainingRGBimagesfromfourdirections,
high-level instructions, mid-level driving commands, and low-level vehicle control signals. The
processofdatasetgenerationisillustratedinFigure5.
A.2.2 DrivingCommand
Inourstudy,weanalyze26distincttypesofdrivingsub-commandswithintheAD-Hdataset. These
sub-commandscovernearlyallthekeyperceptualobjectsinvariousdrivingscenariosandencompass
allnecessarydrivingactions. Bycombiningthesesub-commands,wegenerateover160different
variationsofdrivingcommands. Thecompletelistofdrivingsub-commandsisprovidedinTable10.
17Freq
Driving Command = Perception + Motion
Watch out red light, Break down.
Motion
Driving at a constant speed forward. Driving annotation
...... Perception annotation Freq Resample
Pay attention to pedestrians, reduce
the speed. Driving Command annotation
Motion
(a) the Set of Driving Command (b) Per-frame Annotation (c) Motion-based Resample
Figure5: DatasetGenerationPipeline.
Forinstance,whenencounteringaredlight,theappropriatedrivingcommandwouldbe,“Thereisa
redlightahead. Applybrakessafely.”
A.2.3 Annotation
Themid-leveldrivingcommandisdeterminedbythedetaileddrivingdataprovidedbyCARLAfor
eachframe,whichincludesinformationsuchasthrottle,speed,andsteeringangle. Weemployeda
rule-basedmethodologytoretrospectivelydeducethemid-leveldrivingcommandsforeachframe.
Forexample,whenencounteringaredlight,theappropriatedrivingcommandwouldbe,”Thereisa
redlightahead. Applybrakessafely”. ThedatafromCARLAprovidesinformationaboutthered
lightinthesceneandwhetherthevehicleisbraking. DetailsarepresentedinTable10.
A.2.4 Resampling
Initialannotationrevealsasignificantlong-taildistributionissuewithinthedataset: somemotion
instructionsoccurhundredsoftimesmorefrequentlythanothers. Commondrivingscenarios,such
asmaintainingasteadyspeedorstopping,predominate,whileactionsliketurninganddecelerating
arerelativelyrare. Thisimbalancecanseverelyimpactthemodel’sperformance. Toaddressthis,we
resamplethedatasetbasedonthefrequencyofmotioninstructions,reducingitssizefrom3million
to1.7millionentries,therebyenhancingthedataset’squality.
A.3 LangAuto-Long-horizonBenchmark
Wepresentthelong-horizoninstructionsinTable9.
A.4 MoreResults
A.4.1 VisualizationofanExample
WepresentacompleteexampleofAD-H,fromhigh-levelinstructionsandsensorinputtomid-level
drivingcommandsandfinallytowaypoints,asshowninTable5.
A.4.2 MoreVisualization
WepresentmorevisualizationresultsinFigure5,Figure6,Figure7andFigure8.
A.5 SocietalImpacts
Theproposedapproachofutilizingmid-levellanguage-drivencommandsinautonomousdriving
systemspresentsseveralpotentialpositivesocietalimpacts. Bybridgingthegapbetweenhigh-level
instructionsandlow-levelcontrolsignals,AD-Hcouldleadtosaferandmoreefficientautonomous
drivingindiverseanddynamicenvironments. Thiscouldultimatelyreducetrafficaccidentsand
fatalities,alleviatecongestion,andimproveaccessibilityforindividualswithmobilitylimitations.
Moreover,theenhancedgeneralizabilityofAD-Hmayfosterwideradoptionofautonomousvehicles,
potentiallyleadingtoreducedgreenhousegasemissionsandenhancedurbanplanning.
However,therearealsopotentialnegativesocietalimpactstoconsider. Dependenceonadvanced
autonomousdrivingsystemslikeAD-Hmayexacerbateexistingsocietalissuessuchasjobdisplace-
mentintransportationsectorsandexacerbateprivacyconcernsrelatedtothecollectionandutilization
18Table5: AnexampleofhowourAD-Hpredictsfuturewaypoints. Ourplannerprovidedaccurate
motioninstructions,andthecontrolleraccuratelyexecutenavigationandmotioninstructions.
Challengingexamplesofnovelandcomplexenvironments.
SensorInput Front Left
Back
Right
High-levelInstruction Whatmotionshouldthecarcurrentlytaketoaccomplishtheinstruction
”Continueinastraightlinealongyourcurrentpathuntilyoureachthe
upcomingintersection.”?
High-levelPlanner Slightlybelowtargetspeed,gentlyincreaseacceleration. Makeaslight
leftturn.
Low-levelController Waypoint: [-0.1512451171875, -2.8828125], [-0.439697265625,
-5.984375], [-0.71630859375, -9.1796875], [-1.0048828125, -
12.4296875],[-1.201171875,-15.828125]
Visualization:
ofvastamountsofpersonaldata. Additionally,thedeploymentofsuchsophisticatedsystemscould
widenthedigitaldivide,asaccesstoandunderstandingofthesetechnologiesmaynotbeequitable
acrossallsocioeconomicgroups. It’scrucialtoaddressthesechallengesthroughthoughtfulregu-
lation,education,andinclusivedesignpracticestoensurethatthebenefitsofautonomousdriving
technologiesareequitablydistributedacrosssociety.
19Table6: AD-Hperformswellincomplexnighttimeturningenvironments,whereasLMDrivemay
result in the vehicle stopping in the middle of the road. The green dots in the figure represent
waypoints. Whenawaypointcoincideswiththevehicle’sposition,itindicatesthatthevehiclehas
cometoastop. NavigationInstruction: Uponcovering[x]meters,arightturnatthetrafficsignalis
mandatory.
AD-H LMDrive
Time
Drivingcommand VeritcalView VerticalView
T Watchoutforthecar
0
ahead,there’savehi-
cle in front. Apply
brakessafely.
T Slowdowntoensure
1
safety. Makeaslight
rightturn.
T Slightlybelowtarget
2
speed,gentlyincrease
acceleration. Keep
the steering wheel
straight.
20Table7: AD-Hperformswellincomplexturningenvironments,whereasLMDrivemayresultin
the vehicle stopping in the middle of the road. The green dots in the figure represent waypoints.
Whenawaypointcoincideswiththevehicle’sposition,itindicatesthatthevehiclehascometoa
stop. High-levelinstruction: Uponcovering[x]meters,arightturnatthetrafficsignalismandatory.
AD-H LMDrive
Time
Drivingcommand VerticalView VerticalView
T Approaching a junc-
0
tion, prepare to fol-
lowtrafficrules.Slow
downtoensuresafety.
Make a slight right
turn.
T Approaching a junc-
1
tion, prepare to fol-
lowtrafficrules.Slow
downtoensuresafety.
Applybrakessafely.
T Approaching a junc-
2
tion, prepare to fol-
lowtrafficrules.Slow
downtoensuresafety.
Make a slight right
turn.
Table8: Ourmethodhasstrongerinstructionfollowingperformance.
High-levelInstruction Uponcompleting10meters,aleftturnattheintersectioniscompulsory.
Method LMDrive AD-H
VerticalView
Mid-level Driving None Slowdowntoensuresafety. Makea
Command slightleftturn.
21Table9: Fulllistoflong-horizoninstructionsinLangAuto-Long-horizonbenchmark.
ID Drivingcommand
0 Gostraightahead,turnleftattheendoftheroad,thencontinuestraight.
10 Gostraightuntiltheintersectionahead,thenturnright,andcontinuealongtheroad.
12 Gostraighttothefirstintersectionaheadandturnleft,thencontinuestraight.
20 Turnrightaheadandthengostraight.
26 Turnrightahead,gostraight,thenturnrightagain.
34 GostraighttotheT-junctionahead,thenturnleftandfollowtheroute.
44 Gostraighttoacrossroads,thenturnleft,thencontinuestraight.
46 GostraighttotheT-junction,turnright,andcontinuestraight.
48 Followtheroute,andcontinuestraightwhenyoureachthecrossroads.
57 Gostraighttotheintersectionwhere,ontheleftfrontside,thereisanopenspacewithsome
parkedvehicles,andturnleft.
68 Keepgoingalongthisroad.
70 TurnleftattheT-junctionahead,thenfollowtheroad.
74 Turnleftaheadwhenyoureachthecornfield,thenturnleftagainwhenyouencounteran
openarea.
81 Slightlyturnleftalongtheroadahead,thenturnright,turnleftattheT-junction,andthen
gostraight.
84 Gostraightuntilyouseeaturningpointwithpalmtreesahead,thenturnrightandfollow
theroad.
88 TurnrightattheT-junction,gostraight,thenturnrightattheT-junctionwheretherearegrid
linesontheground. Thencontinuestraight.
Table10: Fulllistofthe26differenttypesofdrivingsub-commandsinAD-Hdataset. Combining
sub-commandscanresultinover170variationsofdrivingcommands.
Type Drivingcommand
Approachingajunction,preparetofollowtrafficrules.
Avehicleispresentatthejunction. Becautious.
Multiplevehiclesarepresentatthejunction. Becautious.
Watchoutforthecarahead,there’savehicleinfront.
Watchoutforthecarsahead,therearemultiplevehiclesinfront.
Avehicleispresentinthelane. Becautious.
Perception Multiplevehiclesarepresentinthelane. Becautious.
Thereisabikeahead. Becautious.
Multiplebikesareahead. Becautious.
Thereisapedestrianahead. Becautious.
Multiplepedestriansareahead. Becautious.
Thereisaredlightahead.
Thereisastopsignahead.
Slowdowntoensuresafety.
Startacceleratinggraduallytowardsthetargetspeed.
Remainstoppedduetobrakeapplication.
Speed Significantlybelowtargetspeed,accelerateifsafe.
Slightlybelowtargetspeed,gentlyincreaseacceleration.
Abovetargetspeed,decelerate.
Maintaincurrentspeedtomatchthetargetspeed.
Steerrightsharply.
Makeaslightrightturn.
Steer Steerleftsharply.
Makeaslightleftturn.
Keepthesteeringwheelstraight.
Break Applybrakessafely.
22