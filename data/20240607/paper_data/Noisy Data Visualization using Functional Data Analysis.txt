Noisy Data Visualization using Functional Data
Analysis
HaozheChen AndresFelipeDuqueCorrea
DepartmentofMathematics&Statistics DepartmentofMathematics&Statistics
UtahStateUniversity UtahStateUniversity
Logan,UT,USA Logan,UT,USA
a02314155@usu.edu andres.duque@usu.edu
GuyWolf KevinR.Moon
DepartmentofMathematicsandStatistics DepartmentofMathematics&Statistics
UniversityofMontreal UtahStateUniversity
Montreal,QC,Canada Logan,UT,USA
wolfguy@mila.quebec kevin.moon@usu.edu
Abstract
Datavisualizationviadimensionalityreductionisanimportanttoolinexploratory
dataanalysis. However,whenthedataarenoisy,manyexistingmethodsfailto
capturetheunderlyingstructureofthedata. ThemethodcalledEmpiricalIntrinsic
Geometry(EIG)waspreviouslyproposedforperformingdimensionalityreduc-
tiononhighdimensionaldynamicalprocesseswhiletheoreticallyeliminatingall
noise. However,implementingEIGinpracticerequirestheconstructionofhigh-
dimensionalhistograms,whichsufferfromthecurseofdimensionality. Herewe
proposeanewdatavisualizationmethodcalledFunctionalInformationGeometry
(FIG) for dynamical processes that adapts the EIG framework while using ap-
proachesfromfunctionaldataanalysistomitigatethecurseofdimensionality. We
experimentallydemonstratethattheresultingmethodoutperformsavariantofEIG
designedforvisualizationintermsofcapturingthetruestructure,hyperparameter
robustness,andcomputationalspeed. WethenuseourmethodtovisualizeEEG
brainmeasurementsofsleepactivity.
1 Introduction
High-dimensionaldatasetsoftencontainredundantinformation,resultinginanartificiallyelevated
extrinsicdimension. However,theirunderlyingstructurecanoftenbeaccuratelymodeledasalow-
dimensionalmanifoldwithsomeaddednoise. Inmachinelearning,theconceptofamanifoldloosely
referstoaconnectedsetofpointsinhigh-dimensionalspacethatcanbeeffectivelyapproximated
usingalowernumberofdimensions. Buildinguponthemanifoldassumption,manifoldlearning
methodsaimtolearnalower-dimensionalrepresentationofdatawhileretainingasmuchofitsinherent
informationaspossible. Manifoldlearninghasbeenusefulinmanyfieldssuchasimageclassification
andobjectdetection[1,2,3],imagesynthesisandenhancement[4,5],videoanalysis[6,7],3Ddata
processing[8],analyzingsingle-cellRNA-sequencingdata[9],andmore[10]
Inparticular, manifoldlearninghasbeenusedinnonlineardimensionalityreductionanddatavi-
sualization. ClassicalapproachesfornonlineardimensionreductionareIsomap [11],MDS[12],
LocalLinearEmbedding(LLE)[13]andLaplacianEigenmaps[14]. Inrecentyears,morepowerful
approachesliket-SNE[15],UMAP[16]haveemerged. However,inmanyapplications,thenoise
Preprint.Underreview.
4202
nuJ
5
]GL.sc[
1v69330.6042:viXralevelmaymakeitdifficulttolearnthetrue,low-dimensionalstructureofthedata. Manyofthese
popularmanifoldlearninganddatavisualizationmethodsfailtotakenoisedirectlyintoaccount.
A more advanced method called diffusion maps (DM) [17] has been proposed that denoises and
learnsthemanifoldstructurebyusingaMarkovdiffusionprocesstoemphasizedataconnectivityand
preservetheintrinsicgeometry. However,DMisnotoptimalforvisualizationbecauseittendsto
encodeinformationinmorethan2or3dimensions[18,19]. Morerecently,PHATE[19]hasbeen
usedtovisualizenoisydata. LikeDM,PHATEalsousesadiffusionframeworktolearntheglobal
structureanddenoise,butisbetterdesignedfordatavisualization.
DespitethedenoisingcapabilitiesofDMandPHATE,thenoiselevelsinsometypesofdatamay
stillbedifficulttoovercomecompletely. However,ifthedatacanbeassumedtobemeasuredfroma
dynamicalsystem,additionalstructurecanbeleveraged. Dynamicalsystemsandtimeseriesdatacan
oftenbeviewedasprocessesgovernedbyasmallsetofunderlyingparameters,effectivelyconfined
withinalow-dimensionalmanifold. Undercertainassumptionsonthegeneratingprocess,adistance
metriccanbelearnedbetweenpointsusingtheEmpiricalIntrinsicGeometry(EIG)frameworkthat
istheoreticallynoise-freeundervariousformsofnoise[20,21]. Todothis,EIGleveragesthefact
thattheMahalanobisdistanceisinvarianttolineartransformations. Theauthorsshowedthatvarious
formsofnoise(e.g. additiveandmultiplicative)resultinlineartransformationsintheprobability
space. ComputingtheMahalanobisdistancebetweenempiricallyconstructedlocalhistogramsthus
resultsinadistancethatisclosetothetruedistancebetweentheunderlyingparametersthatdrivethe
system[20,21]. DynamicalInformationGeometry(DIG)[22]wasthenlaterintroducedtovisualize
thedatafromthesedistancesusingadiffusionandinformationdistanceframework[19,17].
Despitetheirnicetheoreticalproperties,forfinitesamplesEIGandDIGsufferlimitationsduetotheir
useoflocalhistogramstotransformtotheprobabilityspace. Inparticular,histogramsaredifficultto
constructondatawithmorethantwodimensionsasthenumberofbinsinthehistogramincreases
exponentiallywithdimension. Thisresultsfrequentlyinsparselypopulatedhistograms,whichalso
makeslocalcovariancematrixestimationdifficult.
Toovercomethesedifficulties,werevisittheEIGframeworkbyconsideringthelocalprobability
densitiesdirectly. Thusourcontributionsarethefollowing: 1)Usingafunctionaldataanalysisframe-
work[23,24],weshowthatpairwiseMahalanobisdistancescanbeconstructedbetweenprobability
distributionswithoutdirectlyestimatingthedensities,thusmitigatingthecurseofdimensionalitythat
afflictsthehistogramapproach. Todothis,somenotionofneighborsmustbegiventhatindicates
whichpointsareneighborsofeachother. Wefocusprimarilyontimeseriesdatainwhichneighbors
aredeterminedbyatimewindow,althoughotherneighborhooddefinitionscouldbeused. 2)Wethen
embedthesedistancesintolowdimensionsusingadiffusionandinformationgeometryframework,
which is useful for data visualization. We call this visualization method Functional Information
Geometry(FIG).3)WedemonstrateFIGonsimulatedtimeseriesdataandrealdatatakenfromEEG
measurementsofthebrainduringdifferentsleepstages. WeshowthatwhencomparedtoDIGand
EIG,FIGbettercapturesthetruelow-dimensionalstructure,ismorerobusttohyperparameters,and
iscomputationallyfaster.
2 ProblemSettingandBackground
Weusethesamestate-spaceformalismgiveninEIG[20,21]andDIG[22]fortimeseriesdata:
x =y (θ )+ξ (1)
t t t t
dθi =ai(θi)+dwi,i=1,...,d. (2)
t t t
x represents the observed multivariate time series that is a noisy version of y . The parameters
t t
θ representthehiddenstatesthatdrivethecleanprocessy . Thenoiseξ isastationaryprocess
t t t
independentofy . Eachy isdrawnfromaconditionalpdfp(y|θ). Weassumethattheparameters
t t
areaffectedbytheunknowndriftfunctionsaithatareindependentfromθ whenj ̸=i. Thisallows
j
ustoassumelocalindependencebetweenθiandθj,∀j ̸=i. Finally,thewivariablesaregoverned
t t t
byaBrownianmotionprocess.
The goal is to derive a distance that approximates pairwise distances between the parameters θ
t
thatdrivetheprocess. Thedensityp(x|θ)isalineartransformationofthedensityp(y|θ)[20,21].
Sincethedensitiesareunknown,theauthorsin[20,21,22]usehistogramsastheirestimators. Each
histogramh =(h1,...,hNb)hasN binsandisbuiltwiththeobservationswithinatimewindowof
t t t b
2lengthL ,centeredatx . ItwasalsoshownthattheexpectedvalueofthehistogramsE(hj),isa
1 t t
lineartransformationofp(x|θ)[20,21]. SincetheMahalanobisdistanceisinvariantunderlinear
transformations,itcanbededucedthatthefollowingdistanceisnoiseresilient:
d2(x t,x s)=(cid:0)E[h t]−E[h s](cid:1)T(cid:0) C(cid:98)t+C(cid:98)s(cid:1)−1(cid:0)E[h t]−E[h s](cid:1) (3)
whereC(cid:98)t andC(cid:98)s arethelocalcovariancematricesofthehistograms. Theyareconstructedusing
a time window centered at h and h , respectively. Thus under certain assumptions, d2(x ,x )
t s t s
is a good approximation of the distance between the underlying state variables [21], eliminating
the effects of noise; i.e. ∥θ −θ ∥2 ≃ d2(x ,x ). Our primary goal is to derive an alternative
t s t s
Mahalanobis distance that still approximates the distances between the parameters θ but avoids
t
constructinghistogramsoranyotherdensityestimator.
Ournewlyderiveddistancecanbeusedastheinputtovariousmachinelearningalgorithms.Asweare
focusedondatavisualization,wechoosetoinputthedistancesintoPHATE[19].PHATEfirstconverts
pairwisedistancesintolocalaffinitiesbyemployinganα-decaykernelwithadaptivebandwidth,
allowingaffinitiestoadjustbasedonlocaldensity,whilecorrectinginaccuraciesinsparselysampled
regions. Diffusionisthenperformedonthecorrespondingaffinitygraph,whichlearnstheglobal
relationshipsbetweenpointswhiledenoising. PHATEthenextractstheinformationlearnedinthe
diffusionprocessforvisualizationbyconstructingpotentialdistancesfromdiffusedprobabilities
and directly embedding them into low dimensions using metric MDS [12], avoiding instability
and inaccuracies often encountered with direct embedding of diffusion distances. The resulting
visualizationtendstobetterrepresentthelocalandglobalstructureofnoisydatathancompeting
methods[19]. SeveraladaptationshavePHATEhavebeencreatedforvisualizingsuperviseddata
problems[25],visualizingdataatmultiplescales[26],andvisualizingtheinternalgeometryofneural
networks[27].
3 FunctionalInformationGeometry
Constructingthehistogramsandcovariancematricesinthedistancein3forhighdimensionaldata
is difficult due to the curse of dimensionality. Since the noise causes a linear transformation in
theprobabilityspace,wecanobtainanotherdistancethatisnoiseresilientthatavoidsestimating
theprobabilitydensitybyusingfunctionaldataanalysis. Forourapproach,wewillneedtodefine
the Mahalanobis distance between functions. To do this, we will use concepts from [23, 24].
However,wecannotdirectlyusethefunctionaldataanalysis(FDA)frameworkfrom[23,24]for
our problem. Instead, multiple modifications are required for the following reasons: 1) We are
consideringprobabilitydensitiescenteredatthedatapointsasthefunctionswhereasstandardFDA
attemptstolearnafunctiondirectlyfromthedata. 2)Thedensitiesmayhaveamultivariateinput. 3)
WeneedtodefinetheMahalanobisdistanceinthecasewherethetwopoints(functionsordensities)
comefromdifferentdistributions.
3.1 VectorMahalanobisDistance
WewillfirstdefinethevectorMahalanobisdistanceintermsofprincipalcomponentsasthefunctional
Mahalanobis distance will similarly use functional principal components. Suppose we have two
pointsu,v∈RdthathavethesamedistributionwiththesamecovariancematrixC andmeanvector
m. ThentheMahalanobisdistancebetweenuandvis
d (u,v)=[(u−v)TC−1(u−v)]1/2.
M
Thisisthedistanceobtainedafterstandardizingthedata. Sincethetwopointssharethesamemean,
themeandoesnotplayaroleinthisspecificdistance. ConsidertheeigendecompositionC =VΛVT.
Thevectorsofprincipalcomponentscoresaregivenbys = VT(u−m)ands = VT(v−m).
u v
Thusu=m+Vs andv=m+Vs . PluggingthisintotheMahalanobisdistancegives
u v
d (u,v)=[(s −s )TVTVΛ−1VTV(s −s )]1/2
M u v u v
=||Λ−1/2(s −s )||. (4)
u v
ThismethodforcomputingtheMahalanobisdistancewasgivenin[24]. Wewillnowextendthis
tothecasewhereuandvhavedifferentdistributions. Letthecovariancematricesandmeansbe
3C ,C ,m ,andm ,respectively. In[20,22],thejointcovariancebetweentwoobservationswas
u v u v
definedas(C +C ). ThustheMahalanobisdistanceisgivenby:
u v
d (u,v)=[(u−v)T(C +C )−1(u−v)]1/2. (5)
M u v
Wewillneedanexpressionof(C +C )−1intermsofprincipalcomponents.In[28],aTaylorexpan-
u v
sionaroundtheobservablevariablesuandvaregiven,whichyieldsthesecond-orderapproximation
oftheEuclideandistancebetweenunobservablehiddenprocessesθ andθ :
u v
1
∥θ −θ ∥2 = (u−v)T(C−1+C−1)(u−v)+O(∥u−v∥4). (6)
u v 2 u v
WecanthusinsteaddefinethevectorMahalanobisdistanceas:
d (u,v)=[(u−v)T(C−1+C−1)(u−v)]1/2. (7)
M u v
As this is similar to the sum of two standard Mahalanobis distances, the principal components
versionsimplyhastwoofthetermsinEq.4. Inthiscase,letC = V Λ VT andC = V Λ VT.
u u u u v v v v
The principal component scores are given by s = VT(u−m ), s = VT(v−m ), s =
uu u u vv v v uv
VT(v−m ),s =VT(u−m ). ThentheMahalanobisdistanceis:
u u vu v v
(cid:16) (cid:17)1/2
d (u,v)= ||Λ−1/2(s −s )||2+||Λ−1/2(s −s )||2 . (8)
M u uu uv v vu vv
3.2 FunctionalMahalanobisDistanceBetweenDensities
Wenowlooktoextendthepreviousdistancestothefunctionsettingwherethefunctionsareprobability
densities. Letf beafunctionalrandomvariablethatisalsoaprobabilitydensityandisinL2(T),
whereT ⊆Rd. Letµ (t)=E[f(t)]bethedensitymeanandacovarianceoperatorΓ be
f f
Γ (η)=E[(f −µ )⊗(f −µ )(η)], (9)
f f f
whereforanyη ∈L2(T),
(f −µ )⊗(f −µ )(η)=⟨f −µ ,η⟩(f −µ ), (10)
f f f f
whereweusetheL2innerproduct:
(cid:90)
⟨f −µ ,η⟩= (f(x)−µ (x))η(x)dx.
f f
T
In[23],whenΓ exists,thenthereexistsasequenceofnon-negativeeigenvaluesofΓ ,denotedas
f f
λ ≥λ ≥···
,with(cid:80)∞
λ <∞,andasetoforthonormaleigenfunctionsdenotedasψ ,ψ ,...
1 2 k=1 k 1 2
suchthatΓ (ψ )=λ ψ forallk. TheeigenfunctionsformanorthonormalbasisofL2(T)sowe
f k k k
canwritef intermsofthisbasisas:
∞
(cid:88)
f =µ + θ ψ , (11)
f k k
k=1
wheres =⟨f−µ ,ψ ⟩arethefunctionalprincipalcomponentscoresoff.Sincetheeigenfunctions
k f k
areorthonormal,thenthefunctionalprincipalcomponentscoresareuncorrelatedwithzeromeanand
varianceλ .
k
WewillneedtheinversecovarianceoperatorΓ−1.From[23,24],aregularizedversionofitisdefined
f
as:
K
(cid:88) 1
Γ−1(ξ)= (ψ ⊗ψ )(ξ), (12)
K λ k k
k
k=1
where K is chosen as some threshold and ξ is in the range of Γ. Then based on Eq. (4), if two
densitiesf andf comefromthesamedistribution(i.e. havethesamemeanfunctionandcovariance
i j
operator),thentheprincipalcomponentversionoftheMahalanobisdistanceis
K
(cid:88)
d2 (f ,f )= (ω −ω )2, (13)
FM i j ik jk
k=1
4whereω =s /λ1/2[24].
ik ik k
Wenowextendtothecasewhenf andf havedifferentdistributions. LetΓ andΓ bethetwo
i j i j
covarianceoperators. Theirinversesaredefinedasbeforewitheigenvaluesλ andλ . Definethe
ik jk
followingfunctionalprincipalcomponentscores:
s =⟨f −µ ,ψ ⟩
iik i i ik
s =⟨f −µ ,ψ ⟩
ijk j i ik
s =⟨f −µ ,ψ ⟩
jik i j jk
s =⟨f −µ ,ψ ⟩. (14)
jjk j j jk
ThentheMahalanobisdistancebecomes
K K
(cid:88) (cid:88)
d2 (f ,f )= (ω −ω )2+ (ω −ω )2, (15)
FM i j iik ijk jik jjk
k=1 k=1
whereω =s /λ1/2.
ijk ijk ik
3.3 LearningthePrincipalComponentScores
Allthatremainsistoderivethefunctionalprincipalcomponentscoress . StandardFDAmethods,
ijk
suchasFunctionalPrincipalComponentsAnalysis(FPCA),typicallyrequirebasisfunctions(e.g.
theFourierbasisorasplinebasis)tobefittothedata. Whilewedoemploybasisfunctionsinthe
following,weareabletoexploitpropertiesofprobabilitydensitiesbycomputingempiricalaverages
ofthebasisfunctions,avoidingtheneedfordirectlyestimatingthedensities. Thusourapproach
differsfromthatofstandardFPCA.
Supposewehavedensitiesf ,...,f andsomenotionof“neighbors"ofthesedensities. Inpractice,
1 n
wemodeleachmeasureddatapointx asbeingdrawnfromf andtheneighborsoff aredetermined
i i i
bythecorrespondingdatapointsthatarewithinatimewindowwithfixedlengthLwithx atthe
i
center. However,wenotethatneighborscouldbedeterminedinotherwayssuchastheEuclidean
distancebetweenthedatapoints. Thismeansthatourproposeddistancecanbecomputedaslongas
somenotionofneighborsbetweendatapointsisdefined.
Assumefornowthatthedensitiesf ,...,f areknown. Wewillrelaxthisassumptionlater. Then
1 n
themeanfunctionofthedensityf canbeestimatedas
i
1 (cid:88)
µ (x)= f (x), (16)
fi |N | j
i
j∈Ni
whereN denotesthesetofindicessuchthatf isaneighboroff . Inreal-worldapplications,we
i j i
havetheflexibilitytoemploydistinctwindowsizesforvariouspurposes, suchascomputingthe
averageofbasisfunctionsorcalculatingcovariancematrices,asexemplifiedindetailinAlgorithm1.
Tosimplifythepresentation,weusethesamesetofneighbors(andthereforewindowsize)forallof
thesetasks. Anestimateofthethecovariancefunctionassociatedwithf isthen
i
1 (cid:88)
v (x,z)= (f (x)f (z)−µ (x)µ (z)). (17)
i |N | j j fi fi
i
j∈Ni
Wewillneedtofindtheeigenfunctionofthiswhichsatisfies
(cid:90)
v (x,z)ψ (z)dz=λ ψ (x). (18)
i i i i
5Wewillassumeabasisexpansionfortheeigenfunctionψ (x)=ϕ(x)Tb whereϕ:Rd →RM isa
i i
setofbasisfunctionssuchastheFourierbasisorcubicsplines. Theeigenfunctionequationbecomes
(cid:90)
λ ϕ(x)Tb = v (x,z)ψ(z)dz
i i i
1 (cid:88) (cid:90)
= (f (x)f (z)−µ (x)µ (z))ϕ(z)Tb dz
|N | j j fi fi i
i
j∈Ni
 
1 (cid:88) (cid:90) 1 1 (cid:88) (cid:90)
=
|N |
f j(x) f j(z)ϕ(z)Tdz−µ fi(x)
n|N |
f j(z)ϕ(z)Tdzb
i
i i
j∈Ni j∈Ni
 
1 (cid:88) 1 (cid:88)
=
|N |
f j(x)aT
j
−µ fi(x)
|N |
aT jb i, (19)
i i
j∈Ni j∈Ni
(cid:82)
wherea = f (z)ϕ(z)dz. Thevectora istheexpectedvalueofthebasisfunctionϕwithrespect
i i i
tothedensityf . Thisvalueiseasilyapproximatedusingasamplemeanas
i
1 (cid:88)
aˆ = ϕ(x ). (20)
i |N | j
i
j∈Ni
DefineW =(cid:82) ϕ(x)ϕ(x)Tdx. Ifϕisanorthonormalbasis(suchastheFourierbasis),thenW isthe
identitymatrix. NowmultiplybothsidesofEq.19byϕ(x)andthenintegratetoobtain:
(cid:90)
λ ϕ(x)ϕ(x)Tdxb =λ Wb
i i i i
 
(cid:90) 1 (cid:88) 1 (cid:88)
= ϕ(x)
|N |
f j(x)aT
j
−µ fi(x)
|N |
aT jb idx
i i
j∈Ni j∈Ni
 
1 (cid:88) 1 (cid:88) 1 (cid:88)
=
|N |
a jaT
j
−
|N |
a
j|N |
aT jb
i
i i i
j∈Ni j∈Ni j∈Ni
 
1 (cid:88)
=
|N |
a jaT
j
−µ aiµT aib
i
i
j∈Ni
=A˜ b (21)
i i
whereµ = 1 (cid:80) a andA˜ = 1 (cid:80) a aT −µ µT. ThematrixA˜ canbeviewedas
ai |Ni| j∈Ni j i |Ni| j∈Ni j j a a i
thesamplecovariancematrixofthevectora . Bylettingu =W1/2b ,wegetthefollowingeigen
i i i
equation:
W−1/2A˜ W−1/2u =λ u . (22)
i i i i
Now let µˆ = 1 (cid:80) aˆ and approximate A˜ ≈ 1 (cid:80) aˆ aˆT −µˆ µˆT. If we find the
i |Ni| j∈Ni j i |Ni| j∈Ni j j j j
firstK eigenvaluesand eigenvectorsofEq.22, whichwedenote individuallyas λ andu for
ik ik
k =1,...,K,wethenobtaintheprojections
s =(aˆ −µˆ )TW−1/2u , (23)
ijk j i ik
Wethengetω =s /λ1/2,whichisusedinEq.15toobtainthefinaldistance. Thisdistancewill
ijk ijk ik
thenaccuratelyapproximatethedistancebetweentheparametersθ aslongastheapproximationsof
t
a andA˜ areaccurate. Toembedthefinaldistancesintolowdimensions,especiallyforvisualization,
i i
weinputthedistancesintothePHATEalgorithmtoobtainthefinalFIGembedding.
WereiteratethatanadvantageofFIGisthatwedonotneedtoestimatethedensities(incontrast
withEIGandDIG)nordoweneedtofitthedensitiestoabasis. Wesimplyneedtocomputethe
a vectorswhichareobtainedbytakinganempiricalaverageofthebasisfunctions. Thisbecomes
i
easytoextendtohigherdimensionsaswell. Onefinalconsiderationisthenumericalstabilityofthe
eigenvaluesλ . Inourexperiments,wefrequentlyfoundthatsomeeigenvectorsu arestructurally
ik ik
6informativebuthaverelativelyloweigenvalues. Dividingbythesquarerootoftheseeigenvalues
tendstoamplifynumericalerrors. Incontrast,exponentiationislesspronetosucherroramplification
whilestillprovidingasimilarfunctionshapeasλ−1/2,makingitamorestablechoice. Henceweuse
thefollowingnormalizedprincipalcomponentscores:
ω
ijk
=s ijk/eλik. (24)
SeeAlg.1forasummaryofallofthestepsinFIGwhenapplyingittotimeseriesdata.
Algorithm1:TheFIGAlgorithmfortimeseriesdata
Input: InputdataX ={x ,...,x }orderedbytime,basisfunctionϕ,timewindow
1 n
centeredattimex withlengthL ,windowsizeL tocomputethecovariance,
t 1 2
desiredembeddingdimensionr(usually2or3forvisualization)
Result: TheFIGembeddingZ
r
1: ϕ(x )←computethebasis(e.g. Fourier)foreachsamplex
i i
2: aˆ ←computethenewfeaturerepresentationsusingEq. 20usingawindowofpointswith
i
sizeL centeredatx .
1 i
3: Aˆ ←computethecovariancematrixasA˜ = 1 (cid:80) aˆ aˆT −µˆ µˆT,and
µˆ =i 1 (cid:80) aˆ ,whereW ,W arethei windL o2 wsoj∈ fW poi intj scj enterej dj atx andx
j L2 k∈Wj k i j i j
respectivelywithsizeL .
2
4: s ←applytheeigen-decompositiontoA˜ ,usingEq.22andEq.23toobtaintheobtain
ijk i
theprincipalcomponents.
5: w ←normalizetheprincipalcomponentsbyEq.24
ijk
6: D ←computethefunctionalMahalanobisdistancematrixinEq.15fromw
FM ijk
7: InputD intothePHATEalgorithmtoobtaintheembeddingZ
FM r
4 Experiments
4.1 Simulatedstudy
Forthisstudy,inspiredbytheworkof[20],weconductedsimulationstomimicthemovementofa
radiatingobjectacrossa3Dsphere. Themotionisprimarilygovernedbytwoangles: thehorizontal
(azimuth)angleθ1 andthevertical(elevation)angleθ2. Buildingontheassumptionsoutlinedin
t t
[20],wedemonstratethattheobservedmovementsin3Dspacecanbeunderstoodthroughthevector
θ =[θ1,θ2]. Figure1showcasesasegmentofoursimulation. ReferringtoEquations1and2,the
t t
leftsideofthefiguredisplaysthecleanmeasurementsy in3Dspace,whiletherightsiderepresents
t
the2Dunderlyingprocess,θ. Inourexperiments,wesimulated1000randomsteps,addingnoiseξ
t
asdescribedinEquation1. WeuseAlgorithm1with7Fourierbasisfunctionsforeachdimensionof
thedatatoderivethedistancematrixandobtaina2DFIGembedding. .
WeaddrandomGaussiannoisewithameanofzero,andthenoiselevelisdeterminedsolelybythe
standarddeviationσ. Toassessnoiserobustness,wecomparedFIGwithotherbaselinevisualization
methods: DIG,PHATE,UMAP,andt-SNE,acrossdifferentnoiselevels. Weassumethenoiseis
independentandidenticallydistributed(i.i.d.). WesystematicallyincreasedthenoiseuntiltheMantel
coefficient(describedbelow)betweenthenoisydataandθdroppedbelow0.5. Allembeddingsare
obtainedas2-dimensions. ForPHATE,UMAP,andt-SNE,weusethePythonAPIsandlibraries:
phate,umap,t-sne. ForDIGandFIG,wetakeL =10,andL =10.
1 2
Toevaluateglobaldisparitiesamongembeddings,weemployedtheMantel[29]test. TheMantel
testproducescorrelationcoefficientsrangingfrom0to1betweentwosetsofdistances,similarto
thestandardPearsoncorrelation. However,theManteltesttakesintoaccounttheinterdependence
ofdistances,recognizingthatalteringthepositionofasingleobservationimpactsN −1distances,
challengingtheadequacyofasimplecorrelationcalculationforevaluatingsimilaritybetweendistance
matrices. TocomputetheMantelcorrelationcoefficients,weinitiallycomputethepairwiseEuclidean
distancesoftheembeddingsandthepairwiseEuclideandistancesofthedynamicalprocessθ,then
calculatetheMantelcorrelationsbetweenthesetwodistancematrices. Toensurereproducibility,for
eachnoiselevel,wetestedusing5differentrandomseedstogeneratethedata. Wepresentthemean
Mantelcorrelations(assolidlines)alongwiththestandarddeviations(aserrorbars)inFigure2. We
7Figure1: Simulateddatasetup. (Left)Segmentofthe3Dmovementoftheobjectonthesphere.
(Right)Correspondingsegmentofthe2Dtrajectoryofthetwoindependentfactors: thehorizontal
andverticalangles.
Figure2: Mantelcoefficientbetweendifferentembeddingdistancesandthegroundtruthparameters
θ of the simulated random walk. FIG outperforms all methods in the high noise setting and is
competitiveinthelownoisesetting.
alsoincludeanotherline(blue)inwhichthenoisydataisusedtocomputetheMantelcorrelation
withθ.
FromFigure2,weobservethatasthenoiselevelincreases,theMantelcoefficientbetweenthedata
and θ drops significantly. Initially, in the noiseless setting, none of the methods outperform the
originaldataintermsoftheMantelcoefficient,althoughFIGproducesthebestresultoutofallthe
embeddingmethods. Asthenoiselevelrises,allmethodstendtohavelowerMantelcoefficients
withθ,butFIGoutperformstheothersathighernoiselevels. Whenthenoiselevelreaches0.15,
the Mantel coefficient of FIG is even higher than that of the original data, indicating that FIG is
morenoiseresilientcomparedtootherapproaches. Inparticular,theperformancegapbetweenFIG
andDIGisfairlylarge. ThismaybebecauseDIGestimateshistogramsinhighdimensionswhich
maynotbeaccuratewhenthedataiscomplexandnoisy,leadingtoover-smoothinginthesimulated
steps. Incontrast,FIGusesbasisfunctionsforthenewfeatures,avoidingdensityestimationinhigher
dimensionsandthuspreventingover-smoothingintherandomwalk.
4.2 VisualizingSleepPatterns–EEG
WenowapplyFIGtoEEGdatasourcedfrom[30,31]. Theinitialdataisamultivariatetimeseries
datawith18dimensions,sampledat512Hz,isclassifiedintosixsleepstagesfollowingR&Krules
8(a)Embeddings
(b)MantelofDIG (c)MantelofFIG
Figure3: ComparisonofFIGandDIGonEEGbrainmeasurementsduringdifferentsleepstages.
FIGismorerobusttodifferentwindowsizesthanDIG.(a)AvisualcomparisonofFIGandDIG.
Parts(b)and(c)showpairwiseMantelcorrelationsbetweentheembeddings.
(REM,Awake,S-1,S-2,S-3,S-4),eachcoveringa30-secondperiod. Toaddresslimitedobservations
withincertainstages,wecombineS-1withS-2andS-3withS-4. Furthermore,thedataundergoes
band filtering within the 8-40 Hz range and is subsequently down-sampled to 128Hz. Given the
substantialsizeoftheoriginaldata(exceeding3millionsamples),wepreprocessitbycomputing
Fourierbasisfunctionsandsubsequentlyaveragingthefunctionsϕ(x )overasegment.Following
t
Algorithm1,weusesevenFourierbasisfunctionsforeachdimension,adoptingidenticalsettingsas
inDIG[22],wesetL =3840andthedistancebetweenhistogramcentersisalsoL ,corresponding
1 1
tothenumberofobservationswithina30-secondinterval. Finally,weobtaintheembeddingsina
2-dimensionalspaceusingAlgorithm1.
As a baseline, we applied PHATE, Diffusion Maps, UMAP, t-SNE to the data. None of these
methods were able to capture any meaningful structure (see Figure 4 in the Appendix). Thus
we focus our comparisons on FIG and DIG. Using the same settings from [22], we selected the
numberofbinsforthehistogramestimationNb=20perdimensionforDIG,andvariedwindow
sizesL = 10,20,....200forbothFIGandDIGtocomputetheembeddings. InFigure3part(a),
2
we illustrate the 2-D embeddings of FIG alongside DIG (the baseline) for various window sizes
L . Figure3part(b)andpart(c)presentthemeanpairwiseMantelcorrelationsofDIGandFIG
2
respectivelyacross20windowsizes, rangingfrom10to200, comparingthe2-Dembeddingsof
DIGandFIG.5differentseedswereused. TherelativelysmallstandarddeviationsoftheMantel
correlationsindicatethereproducibilityofourexperiments(seeFigure5intheAppendix).
From Figure 3(a), we observe qualitatively that as the window size increases, the 2-dimensional
embeddingsofDIGtendtoloseimportantstructuralinformationsuchastheconnectionsbetween
different sleep stages. In contrast, the embeddings of FIG remain stable while providing similar
branchingandtrajectorystructuresasDIGwithsmallerwindowsizes. ThissuggeststhatFIGismore
9resilienttothechoiceofwindowsizethanDIG.Similarresultsareobservedinthe3Dembeddings
(Figure 6 in the Appendix). Figures 3(b) and (c) corroborate this numerically. Here we observe
thattheFIGembeddingswithdifferentwindowsizeshavehigherMantelcorrelationcoefficients
witheachotherthanDIG.Ontheotherhand,weobserveahighMantelcorrelationforFIGamong
differentwindowsizes,whichcorrespondstothevisualizationresultsobservedinpart(a). Wealso
demonstratedthatthecomputationaltimeofFIGisoptimalcomparedtoDIG,asshowninTable1in
theAppendix.
5 Conclusion
Ourcontributionliesinthedevelopmentofanovelvisualizationmethod,FIG,basedonthefunctional
data framework and integrating the dynamic framework of EIG-based metrics [20, 21]. Unlike
EIG,ourapproachbypassesdensityestimation,whichisparticularlybeneficialinhigh-dimensional
data settings. Extensive experiments demonstrate the effectiveness of FIG in achieving robust
visualizations,withstabilityobservedacrossdifferentwindowlengthsforcomputingMahalanobis
distances. Onelimitationoftheexperimentsinthispaperistheirapplicationsolelytotimeseries
data. Thislimitationarisesfromthenecessitytodefinea"timewindow,"whichposeschallengesin
non-timeseriescontexts. However,weestablishedinSection3thatFIGcanbeextendedtonon-time
seriesdataviaaproperdefinitionofneighborsbetweenpoints. Weleaveexperimentalvalidationof
thisforfuturework.
References
[1] M.LezcanoCasado,“Trivializationsforgradient-basedoptimizationonmanifolds,”Advances
inNeuralInformationProcessingSystems,vol.32,2019.
[2] V.Verma,A.Lamb,C.Beckham,A.Najafi,I.Mitliagkas,D.Lopez-Paz,andY.Bengio,“Mani-
foldmixup: Betterrepresentationsbyinterpolatinghiddenstates,”inInternationalconference
onmachinelearning. PMLR,2019,pp.6438–6447.
[3] P.Rodríguez,I.Laradji,A.Drouin,andA.Lacoste,“Embeddingpropagation: Smoothermani-
foldforfew-shotclassification,”inComputerVision–ECCV2020: 16thEuropeanConference,
Glasgow,UK,August23–28,2020,Proceedings,PartXXVI16. Springer,2020,pp.121–138.
[4] M.Dai,H.Hang,andX.Guo,“Adaptivefeatureinterpolationforlow-shotimagegeneration,”
inEuropeanConferenceonComputerVision. Springer,2022,pp.254–270.
[5] X.Luo,Z.Han,andL.Yang,“Progressiveattentionalmanifoldalignmentforarbitrarystyle
transfer,”inProceedingsoftheAsianConferenceonComputerVision,2022,pp.3206–3222.
[6] X.Zhen,R.Chakraborty,N.Vogt,B.B.Bendlin,andV.Singh,“Dilatedconvolutionalneural
networksforsequentialmanifold-valueddata,”inProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,2019,pp.10621–10631.
[7] R.Wang,X.-J.Wu,Z.Chen,T.Xu,andJ.Kittler,“Dreamnet: Adeepriemannianmanifold
networkforspdmatrixlearning,”inProceedingsoftheAsianConferenceonComputerVision,
2022,pp.3241–3257.
[8] F.Porikli,“Learningonmanifolds,”inStructural,Syntactic,andStatisticalPatternRecognition:
Joint IAPR International Workshop, SSPR&SPR 2010, Cesme, Izmir, Turkey, August 18-20,
2010.Proceedings. Springer,2010,pp.20–39.
[9] K. R. Moon, J. S. Stanley III, D. Burkhardt, D. van Dijk, G. Wolf, and S. Krishnaswamy,
“Manifold learning-based methods for analyzing single-cell rna-sequencing data,” Current
OpinioninSystemsBiology,vol.7,pp.36–46,2018.
[10] H.Fassold,“Asurveyofmanifoldlearninganditsapplicationsformultimedia,”arXivpreprint
arXiv:2310.12986,2023.
[11] M. Balasubramanian and E. L. Schwartz, “The isomap algorithm and topological stability,”
Science,vol.295,no.5552,pp.7–7,2002.
[12] T.F.CoxandM.A.Cox,Multidimensionalscaling. CRCpress,2000.
[13] S.T.RoweisandL.K.Saul,“Nonlineardimensionalityreductionbylocallylinearembedding,”
Science,vol.290,no.5500,pp.2323–2326,2000.
10[14] M.BelkinandP.Niyogi,“Laplacianeigenmapsfordimensionalityreductionanddatarepresen-
tation,”Neuralcomputation,vol.15,no.6,pp.1373–1396,2003.
[15] L.VanderMaatenandG.Hinton,“Visualizingdatausingt-sne.”Journalofmachinelearning
research,vol.9,no.11,2008.
[16] L.McInnes,J.Healy,andJ.Melville,“Umap: Uniformmanifoldapproximationandprojection
fordimensionreduction,”arXivpreprintarXiv:1802.03426,2018.
[17] R.R.CoifmanandS.Lafon,“Diffusionmaps,”Appl.Comput.Harmon.Anal.,vol.21,no.1,
pp.5–30,2006.
[18] L. Haghverdi, M. Büttner, F. A. Wolf, F. Buettner, and F. J. Theis, “Diffusion pseudotime
robustlyreconstructslineagebranching,”Naturemethods,vol.13,no.10,pp.845–848,2016.
[19] K. R. Moon, D. Van Dijk, Z. Wang, S. Gigante, D. B. Burkhardt, W. S. Chen, K. Yim,
A. v. d. Elzen, M. J. Hirn, R. R. Coifman et al., “Visualizing structure and transitions in
high-dimensionalbiologicaldata,”Naturebiotechnology,vol.37,no.12,pp.1482–1492,2019.
[20] R.TalmonandR.Coifman,“Empiricalintrinsicgeometryfornonlinearmodelingandtime
seriesfiltering,”ProceedingsoftheNationalAcademyofSciences,vol.110,no.31,p.12535–
12540,2013.
[21] ——,“Intrinsicmodelingofstochasticdynamicalsystemsusingempiricalgeometry,”Applied
andComputationalHarmonicAnalysis,vol.39,no.1,p.138–160,2015.
[22] A.F.Duque,G.Wolf,andK.R.Moon,“Visualizinghighdimensionaldynamicalprocesses,”
IEEEInternationalWorkshoponMachineLearningforSignalProcessing,pp.1–6,2019.
[23] P. Galeano, E. Joseph, and R. E. Lillo, “The mahalanobis distance for functional data with
applicationstoclassification,”Technometrics,vol.57,no.2,pp.281–291,2015.
[24] J.RamsayandB.Silverman,“Principalcomponentsanalysisforfunctionaldata,”Functional
dataanalysis,pp.147–172,2005.
[25] J.S.Rhodes,A.Cutler,G.Wolf,andK.R.Moon,“Randomforest-baseddiffusioninformation
geometryforsupervisedvisualizationanddataexploration,”in2021IEEEStatisticalSignal
ProcessingWorkshop(SSP). IEEE,2021,pp.331–335.
[26] M.Kuchroo,J.Huang,P.Wong,J.-C.Grenier,D.Shung,A.Tong,C.Lucas,J.Klein,D.B.
Burkhardt,S.Giganteetal.,“Multiscalephateidentifiesmultimodalsignaturesofcovid-19,”
Naturebiotechnology,vol.40,no.5,pp.681–691,2022.
[27] S.Gigante,A.S.Charles,S.Krishnaswamy,andG.Mishne,“Visualizingthephateofneural
networks,”Advancesinneuralinformationprocessingsystems,vol.32,2019.
[28] A.SingerandR.Coifman,“Nonlinearindependentcomponentanalysiswithdiffusionmaps,”
AppliedandComputationalHarmonicAnalysis,vol.25,no.2,p.226–239,2008.
[29] N.Mantel,“Thedetectionofdiseaseclusteringandageneralizedregressionapproach,”Cancer
research,vol.27,no.2Part1,pp.209–220,1967.
[30] M.Terzano, L.Parrino, A.Smerieri, R.Chervin, S.Chokroverty, C.Guilleminault, M.Hir-
shkowitz,M.Mahowald,H.Moldofsky,A.Rosa,R.Thomas,andA.Walters,“Atlas,rules,and
recordingtechniquesforthescoringofcyclicalternatingpattern(cap)inhumansleep,”Sleep
medicine,vol.3,no.2,p.187–199,2002.
[31] A.Goldberger,L.Amaral,J.H.L.Glass,P.Ivanov,R.Mark,J.Mietus,G.Moody,C.-K.Peng,
and H. Stanley, “Physiobank, physiotoolkit, and physionet: components of a new research
resourceforcomplexphysiologicsignals,”Circulation,vol.101,no.23,p.e215–e220,2000.
11Appendix
HereweprovideadditionalfiguresfromourexperimentsontheEEGdataaswellasacomputational
comparisonbetweenDIGandFIGinTable1.
Figure4: 2DembeddingsoftheEEGdatausingothermethods,coloredbythesamelabels(sleep
stages). Thesemethodsareunabletocaptureanyofthestructureinthedata,incontrastwithboth
FIGandDIG(Figure3).
Figure5: ThestandarddeviationsofManteltestresultsforDIGandFIGontheEEGdatacomputed
from5runs. ThecorrespondingaverageMantelcorrelationsaregiveninFigure3. Thestandard
deviationsaregenerallylow,especiallyforFIG,indicatingourresultsarereproducible.
Figure6: 3DembeddingsoftheEEGdatacomparingDIGversusFIGforvariouswindowsizes. The
FIGembeddingsaregenerallymorerobusttothewindowsizethanDIG.
12Table1: ComputationtimeoftheMahalanobisdistancesforDIGandFIGinsecondsover5different
seeds of EEG data for window size L = 10.We anticipate minimal variance of the time across
2
varyingwindowsizes,giventhatL isemployedincomputingthecovariancematrixafterthedensity
2
estimation. FIGismuchfasterthanDIG.
Seeds DIG FIG
1 171.2 33.2
2 174.4 35.2
3 161.3 23.2
4 157.5 25.7
5 161.7 34.2
13