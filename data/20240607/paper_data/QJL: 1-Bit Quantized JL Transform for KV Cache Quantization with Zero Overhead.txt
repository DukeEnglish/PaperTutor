QJL: 1-Bit Quantized JL Transform for KV Cache Quantization
with Zero Overhead
Amir Zandieh Majid Daliri Insu Han‚àó
Independent Researcher New York University Adobe Research
amir.zed512@gmail.com daliri.majid@nyu.edu insuh@adobe.com
June 6, 2024
Abstract
Serving LLMs requires substantial memory due to the storage requirements of Key-Value
(KV) embeddings in the KV cache, which grows with sequence length. An effective approach to
compress KV cache is quantization. However, traditional quantization methods face significant
memory overhead due to the need to store quantization constants (at least a zero point and a
scale) in full precision per data block. Depending on the block size, this overhead can add 1 or 2
bits per quantized number. We introduce QJL, a new quantization approach that consists of a
Johnson-Lindenstrauss (JL) transform followed by sign-bit quantization. In contrast to existing
methods, QJL eliminates memory overheads by removing the need for storing quantization
constants. We propose an asymmetric estimator for the inner product of two vectors and
demonstrate that applying QJL to one vector and a standard JL transform without quantization
to the other provides an unbiased estimator with minimal distortion. We have developed
an efficient implementation of the QJL sketch and its corresponding inner product estimator,
incorporating a lightweight CUDA kernel for optimized computation. When applied across
various LLMs and NLP tasks to quantize the KV cache to only 3 bits, QJL demonstrates a more
than fivefold reduction in KV cache memory usage without compromising accuracy, all while
achieving faster runtime. Codes are available at https://github.com/amirzandieh/QJL.
1 Introduction
Large language models (LLMs) have garnered significant attention and demonstrated remarkable
success in recent years. Their applications span various domains, including chatbot systems [1, 3]
to text-to-image [28, 11, 24], text-to-video synthesis [26], coding assistant [7] and even multimodal
domain across text, audio, image, and video [25]. The Transformer architecture with self-attention
mechanism [32] is at the heart of these LLMs as it enables capturing intrinsic pairwise correlations
across tokens in the input sequence. The ability of LLMs grows along with their model size [17],
which leads to computational challenges in terms of huge memory consumption.
Deploying auto-regressive transformers during the generation phase is costly because commercial
AI models must simultaneously serve millions of end users while meeting strict latency requirements.
One significant challenge is the substantial memory needed to store all previously generated key-
value (KV) embeddings in cache to avoid recomputations. This has become a major memory and
speed bottleneck, especially for long context lengths. Additionally, the GPU must load the entire
‚àóWork done while at Yale University.
1
4202
nuJ
5
]GL.sc[
1v28430.6042:viXraPrompt Encoding Decoding (Token Generation)
(ATTN-MLP-LAYERNORM)xùêø (ATTN-MLP-LAYERNORM)xùêø Attention cache
LLM encoding vector LLM
q
Prompt Answer
softmax(q K )V
‚ä§ ‚ä§
what is 20+24? K V 44
cache
Key Embed. k Rd S k Rm sign(Sk) 1 m KV Cache Quantization
‚àà ¬∑ ‚àà ‚àà{¬± }
k /m
sign() ‚à• ‚à•2
¬∑
JL S Rm √ód QJL(S,k) K V
‚àà
transform S ij ‚àºN (0,1) QQP uaeJ nr- tLt io zak( te i¬∑n o) n QP uae nr- tt io zk ae tin o n
Lemma 3.2 & 3.5
Sq,QJL(S,k) q,k
Œµ
‚ü® ‚ü©‚âà ‚ü® ‚ü©
Query Embed. q Rd S q Rm
‚àà ¬∑ ‚àà Cache
Figure 1: Overview of the KV cache quantization via Quantized JL (QJL) transform
KV cache from its main memory to shared memory for each token generated, resulting in low
arithmetic intensity and leaving most GPU threads idle. Therefore, reducing the KV cache size
while maintaining accuracy is crucial.
There are several approaches to address this challenge. One method involves reducing the number
of heads in the KV cache using multi-query attention [29] and multi-group attention [2], but these
require fine-tuning the pre-trained models or training from scratch. Another line of work tries to
reduce the KV cache size by pruning or evicting unimportant tokens [39, 21, 33, 37]. Additionally,
some recent works tackle the issue from a system perspective, such as offloading [30] or using virtual
memory and paging techniques in the attention mechanism [18].
A simple yet effective approach is to quantize the floating-point numbers (FPN) in the KV
cache using fewer bits. Several quantization methods have been proposed specifically for the
KV cache [36, 34, 10, 16, 38]. Most recently, KIVI [22] and KVQuant [13] proposed per-channel
quantization for the key cache to achieve better performance. However, all existing quantization
methods for the KV cache face significant ‚Äúmemory overhead‚Äù issues. Specifically, all these methods
group the data into blocks, either channel-wise or token-wise, and calculate and store quantization
constants (at least a zero point and a scale) for each group. Depending on the group size, this
overhead can add approximately 1 or 2 additional bits per quantized number, which results in
significant computational overhead. In this work, our goal is to develop an efficient, data-oblivious
quantization method, referred to as a sketching technique. This method, which we call QJL, does
not need to be tuned by or adapted to the input data with significantly less overhead than prior
works, without any loss in performance.
2
√ó1.1 Overview of Contributions
The decoding phase in the attention mechanism involves the following computations: (1) computing
attention scores by applying the softmax function to the inner product between the current query
embedding and all previously generated keys, and (2) multiplying the attention scores with all
previously generated values. To make the attention score calculations in step (1) more memory
efficient, we quantize the keys in the cache. We introduce a quantization scheme for key embeddings,
named QJL, leveraging randomized sketching techniques. Alongside, we develop a high-accuracy
estimator for the inner product of query/key pairs, crucial for mitigating errors amplified by the
softmax operation in attention score calculations.
Firstly, we revisit a fundamental concept in numerical linear algebra: applying a Johnson-
Lindenstrauss (JL) transform, i.e., a random Gaussian projection, to a pair of vectors and then
computing the inner product of the projected vectors provides an unbiased and low-distortion
estimator for their original inner product [8]. To address the key cache quantization problem, our
aim is to quantize the result after applying the JL transform to a key embedding, ideally to just a
single bit. Surprisingly, we prove that by applying the JL transform to a key embedding and then
quantizing the result to a single bit (the sign bit), while applying the same JL transform to the
query embedding without quantization, we still obtain an unbiased estimator of their inner product
(see Lemma 3.2). Moreover, the distortion of this estimator is small and comparable to that of
the standard JL transform (see Lemma 3.5). In Theorem 3.6, we demonstrate that the proposed
inner product estimator based on QJL achieves a relative distortion of 1 Œµ on the final attention
¬±
scores. Notably, the number of required bits for representing quantized keys is independent of the
embedding dimension and scales logarithmically with the context length, using a fixed number of
bits per token.
ThustheQJLsketchcombinesaJLtransform‚ÄîarandomGaussianprojection‚Äîwithquantization
to the sign bit. An overview of this approach is illustrated in Figure 1. Unlike previous methods, the
QJL sketch can quantize vectors with zero overhead because it does not require grouping the data
and storing quantization constants (zeros and scales) per group. Furthermore, this is a data-oblivious
algorithm that does not rely on specific input, requires no tuning, and can be easily parallelized and
applied in real-time.
The value cache quantization used to make step (2) memory efficient is known to be a straight-
forward task, and a standard token-wise quantization is very effective and efficient in practice, as
observed in prior work [22, 13]. Hence, we follow the same approach for the value therein.
Furthermore, we analyzed the distribution of outliers in large language models (LLMs). We
observed that while there are no significant outliers in the initial layers, certain fixed key embedding
channels (coordinates) in the deeper layers exhibit considerably larger magnitudes (see Figure 2).
To address this, we identify these outlier channels during the prompt phase and simply apply two
independent copies of our quantizer to the outliers and inliers separately.
The QJL transform and its accompanying inner product estimator are highly efficient and
GPU-friendly algorithms. In particular, we provide a lightweight CUDA kernel for their efficient
computation. We apply QJL and our inner product estimator to compress the KV cache in several
LLMs, including Llama-2 [31] and its fine-tuned models by long sequence [19], under various NLP
tasks. Our results show that quantizing the KV cache to only 3 bits per FPN results in no accuracy
drop compared to the exact model with 16 bits per FPN while reducing cache memory usage by over
fivefoldandincreasingthegenerationspeedsignificantlyforlongcontexts. Forexample, ourproposed
quantization shows better F1 scores on long-range question-answering tasks from LongBench [4] (a
collection of long-context datasets) compared to the recent KV cache quantization methods, while
minimizing memory overheads.
32 Preliminaries: Token Generation in Attention
Deploying auto-regressive language models for inference involves performing attention decoding in an
online setting, where key and value embeddings from each transformer layer are cached in memory
to remove redundant computations. The model sequentially uses and updates the KV cache to
generate the next token, one at a time.
More precisely, in every phase of token generation, the stream of tokens is represented by a triplet
of vectors called by the query, key, and value embeddings, respectively. Let q i,k i,v
i
Rd be the
‚àà
triplet at i-th generation phase and n be the total number of tokens in the stream so far either in
the prompt encoding (prefill) or the generation (decoding) phase. Then, the attention output in
n-th generation phase can be written as
(cid:88)
o = Score(i) v , (1)
n i
¬∑
i‚àà[n]
where Score Rn is the vector of attention scores defined as:
‚àà
Score := softmax([ q ,k , q ,k ,... q ,k ]). (2)
n 1 n 2 n n
‚ü® ‚ü© ‚ü® ‚ü© ‚ü® ‚ü©
Theoutputembeddingo willbeusedforcomputingthenexttokensinthestreamq ,k ,v
n n+1 n+1 n+1
unless the generation phase terminates. Observe that to compute output o , one needs to store all
n
previous key and value embeddings k ,v and keeping them in full precision requires significant
i i i‚àà[n]
{ }
memory for long-context inputs. The time complexity to compete Equation (2) is O(nd) due to
the computation of n inner products. Additionally, the inference speed is also impacted by the KV
cache size, as the KV cache must be loaded from GPU main memory for every token generated,
resulting in low arithmetic intensity and underutilization of GPU cores [27]. In this work, we focus
on compressing the KV cache by quantizing tokens, thereby reducing the memory required to store
each key or value embedding in the cache.
3 Quantized Johnson-Lindenstrauss (QJL) Transform
Our goal is to save memory space for storing the KV cache while the inner product between query
and key remains undistorted. To achieve this, we first transform the embedding vectors using a
random projection that preserves the inner products, acting as a preconditioning step, and then
quantize the result. Specifically, we project the input vectors onto a random subspace by applying
the Johnson-Lindenstrauss (JL) transform [15], which amounts to multiplying by a random Gaussian
matrix. The inner product of the resulting vectors after applying this projection provides an unbiased
and low-distortion estimator for the inner product of the original vectors [8]. We introduce a 1-bit
Johnson-Lindenstrausstransform,comprisingaJLtransformationfollowedbyquantizationtoasingle
sign bit, and demonstrate its ability to offer an unbiased and low-distortion inner product estimator.
We complement our binary quantizer by developing an unbiased estimator for the inner product of
the quantized vector with any arbitrary vector. This inner product estimator is asymmetric, as one
of the vectors is quantized to a single bit while the other remains unquantized, making it well-suited
for the KV cache mechanism. The Quantized Johnson-Lindenstrauss (QJL) transformation, acting
as a 1-bit quantizer, alongside our proposed estimator, is formally defined in the following definition:
Definition 3.1 (QJL and inner product estimator). For any positive integers d,m, let S Rm√ód
‚àà
be a JL transform matrix, i.e., entries of S are i.i.d. samples from the zero mean and unit variance
4Normal distribution. The QJL is a mapping function
S
: Rd 1,+1 m defined as:
H ‚Üí {‚àí }
S(k) := sign(Sk) for any k Rd. (3)
H ‚àà
Furthermore, for any pair of vectors k,q Rd the estimator for their inner product q,k based on
‚àà ‚ü® ‚ü©
the aforementioned quantizer is defined as:
(cid:112)
œÄ/2
Prod (q,k) := k Sq, (k) . (4)
QJL 2 S
m ¬∑‚à• ‚à• ¬∑‚ü® H ‚ü©
Now, we show that the inner product estimator Prod (q,k), exactly like the inner product
QJL
of JL-transformed vectors without quantization to sign bit, is an unbiased estimator. The crucial
point to note is that if we applied QJL to both vectors q and k in Equation (4), we would obtain an
unbiased estimator for the angle between these vectors, as shown in [6]. However, to estimate the
inner product one needs to apply the cosine function on top of the angle estimator, which results
in a biased estimation. Thus, to achieve an unbiased inner product estimator, it is necessary to
asymmetrically apply quantization to the JL transform of only one of the vectors q and k.
Lemma 3.2 (Inner product estimator Prod
QJL
is unbiased). For any vectors q,k Rd the expected
‚àà
value of the estimator Prod (q,k) defined in Equation (4) is:
QJL
E[Prod QJL(q,k)] = q,k ,
S ‚ü® ‚ü©
where the expectation is over the randomness of the JL matrix S in Definition 3.1.
Proof. Let s ,s ,...s denote the rows of the JL matrix S. Additionally, let us decompose q to its
1 2 m
projection onto the vector k and its orthogonal component, i.e., q‚ä•k := q ‚ü®q,k‚ü© k. We can write,
‚àí ‚à•k‚à•2 2 ¬∑
(cid:112)
œÄ/2 (cid:88)
Prod (q,k) = k s‚ä§q sign(s‚ä§k)
QJL 2 i i
m ‚à• ‚à• ¬∑ ¬∑
i‚àà[m]
(cid:112)
œÄ/2 (cid:88) q,k
= ‚ü® ‚ü© s‚ä§k sign(s‚ä§k)+ k s‚ä§q‚ä•k sign(s‚ä§k)
i i 2 i i
m k ¬∑ ¬∑ ‚à• ‚à• ¬∑ ¬∑
2
i‚àà[m] ‚à• ‚à•
(cid:112)
œÄ/2 (cid:88) q,k
= ‚ü® ‚ü© s‚ä§k + k s‚ä§q‚ä•k sign(s‚ä§k).
i 2 i i
m k ¬∑| | ‚à• ‚à• ¬∑ ¬∑
2
i‚àà[m] ‚à• ‚à•
Since s ‚Äôs have identical distributions, we have:
i
(cid:18) (cid:19)
(cid:112) q,k (cid:104) (cid:105) (cid:104) (cid:105)
E[Prod QJL(q,k)] = œÄ/2 ‚ü® ‚ü© E s‚ä§ 1k + k 2 E s‚ä§ 1q‚ä•k sign(s‚ä§ 1k) .
S k 2 ¬∑ | | ‚à• ‚à• ¬∑ ¬∑
‚à• ‚à•
To calculate the above expectation let us define variables x := s‚ä§k and y := s‚ä§q‚ä•k. Note that x
1 1
and y are both zero-mean Gaussian random variables and because q‚ä•k,k = 0. By the following
‚ü® ‚ü©
Fact 3.3, x and y are independent.
Fact 3.3. If x Rd is a vector of i.i.d. zero-mean normal entries with variance œÉ2 and A Rm√ód is
‚àà ‚àà
a matrix, then A x is a normal random variable with mean zero and covariance matrix œÉ2 AA‚ä§.
¬∑ ¬∑
This implies that the second expectation term above is zero because E(cid:2) s‚ä§ 1q‚ä•k sign(s‚ä§ 1k)(cid:3) =
¬∑
E[y sign(x)] = E[y] E[sign(x)] = 0. Furthermore, x is a Gaussian random variable with mean
¬∑ ¬∑
zero and variance k 2. Therefore, we have
2
‚à• ‚à•
(cid:112) q,k
E[Prod QJL(q,k)] = œÄ/2 ‚ü® ‚ü© E[ x ] = q,k .
S ¬∑ k 2 ¬∑ x | | ‚ü® ‚ü©
‚à• ‚à•
where the equality comes from the following Fact 3.4:
5Fact 3.4 (Moments of Normal Random Variable). If x is a normal random variable with zero mean
and variance œÉ2, then for any integer ‚Ñì, the ‚Ñì-th moment of x is E(cid:2) x ‚Ñì(cid:3) = œÉ‚Ñì 2‚Ñì/2Œì((‚Ñì+1)/2)/‚àöœÄ.
| | ¬∑
This completes the proof of Lemma 3.2.
Now we show that the inner product estimator Prod in Definition 3.1, just like the estimators
QJL
based on the standard JL transform, has a bounded distortion with high probability.
Lemma 3.5 (Distortion of inner product estimator Prod QJL). For any vectors q,k Rd if the
‚àà
estimator Prod (q,k) is defined as in Equation (4) for QJL with dimension m 4 1+Œµ log 2, then:
QJL ‚â• 3¬∑ Œµ2 Œ¥
Pr[ Prod (q,k) q,k > Œµ q k ] Œ¥,
QJL 2 2
S | ‚àí‚ü® ‚ü©| ‚à• ‚à• ‚à• ‚à• ‚â§
where the probability is over the randomness of the JL matrix S in Definition 3.1.
Proof. First note that, letting s ,s ,...s denote the rows of the JL transform matrix S, we have:
1 2 m
1 (cid:88) (cid:112)
Prod (q,k) = œÄ/2 k s‚ä§q sign(s‚ä§k).
QJL 2 i i
m ¬∑‚à• ‚à• ¬∑ ¬∑
i‚àà[m]
Since s ‚Äôs are i.i.d. the above is indeed the average of m i.i.d. estimators defined as z :=
i i
(cid:112)
œÄ/2 k s‚ä§q sign(s‚ä§k) for i [m]. Let us now calculate the ‚Ñì-th moment of z using Fact 3.4:
¬∑‚à• ‚à•2 ¬∑ i ¬∑ i ‚àà i
E(cid:104) |z
i
|‚Ñì(cid:105) = (cid:16)(cid:112) œÄ/2 ¬∑‚à•k ‚à•2(cid:17)‚Ñì ¬∑E(cid:104) |s‚ä§
i
q |‚Ñì(cid:105) = (cid:0) ‚àöœÄ ¬∑‚à•k
‚à•2
‚à•q ‚à•2(cid:1)‚Ñì
¬∑
Œì((‚Ñì ‚àö+ œÄ1)/2) , (5)
where the second equality above follows because s‚ä§q is a Gaussian random variable with mean zero
i
and variance q 2 along with Fact 3.4. Now we can prove the result by invoking the unbiasedness of
2
‚à• ‚à•
the estimator, Lemma 3.2, along with an appropriate version of Bernstein inequality and using the
moment bounds in Equation (5). More specifically, our moment calculation in Equation (5) implies:
E(cid:104)
z
i
‚Ñì(cid:105)
=
E(cid:2)
z
i
2(cid:3) (cid:0)
‚àöœÄ k
2
q
2(cid:1)‚Ñì‚àí2 Œì((‚Ñì+1)/2) E(cid:2)
z
i
2(cid:3)
(cid:18)
2
k
2
q
2(cid:19)‚Ñì‚àí2
‚Ñì!
| | | | ¬∑ ‚à• ‚à• ‚à• ‚à• ¬∑ Œì(3/2) ‚â§ | | ¬∑ 3 ¬∑‚à• ‚à• ‚à• ‚à• ¬∑ 2
Therefore, by invoking a proper version of the Bernstein inequality, for instance Corollary 2.11 from
[5], we have the following:
(cid:18) 3 mt2 (cid:19)
Pr[ Prod (q,k) q,k > t] 2exp .
S | QJL ‚àí‚ü® ‚ü©| ‚â§ 4 ¬∑ ‚à•k ‚à•2 2‚à•q ‚à•2 2+ ‚à•k ‚à•2 ‚à•q ‚à•2 ¬∑t
If we set t = Œµ q k the above simplifies to:
2 2
‚à• ‚à• ‚à• ‚à•
(cid:18) 3 mŒµ2 (cid:19)
Pr[ Prod (q,k) q,k > Œµ q k ] 2exp .
QJL 2 2
S | ‚àí‚ü® ‚ü©| ‚à• ‚à• ‚à• ‚à• ‚â§ 4 ¬∑ 1+Œµ
Therefore if m 4 1+Œµ log 2 the error bound follows. This completes the proof of Lemma 3.5.
‚â• 3 ¬∑ Œµ2 Œ¥
Note that the distortion bound in Lemma 3.5 has remarkably small constants, even smaller than
those of the original unquintized JL transform. This indicates that quantizing one of the vectors to
just a single sign bit does not result in any loss of accuracy. We use these properties of QJL and our
inner product estimator to prove the final approximation bound on our KV cache quantizer.
6Algorithm 1 QJL Key Cache Quantizer
Input: Stream of key tokens k 1,k 2,... Rd, integer m
‚àà
1: Draw a random sketch S Rm√ód with i.i.d. entries S i,j (0,1) as per Definition 3.1
‚àà ‚àº N
2: repeat
3: Compute kÀú i sign(Sk i) and ŒΩ i k i 2
4: store the qu‚Üê antized vector kÀú i and‚Üê the‚à• ke‚à• y norm ŒΩ i in the cache
5: until token stream ends
Procedure EstimateScores(q )
n
6: Compute inner pro (cid:16)duct (cid:17)estimators q(cid:103)K(j)
‚Üê
‚àö mœÄ/2 ¬∑ŒΩ i ¬∑‚ü®Sq n,kÀú j
‚ü©
for every j
‚àà
[n]
7: S(cid:94)core softmax q(cid:103)K
‚Üê
return S(cid:94)core
3.1 Key Cache Quantization via QJL
The key cache is used in the computation of attention scores as shown in Equation (2). To calculate
these scores, we need to compute the inner products of the current query embedding with all key
embeddings in the cache. We design a quantization scheme that allows for a low-distortion estimate
oftheinnerproductsbetweenanarbitraryqueryandallkeysinthecache. Inthissection, wedevelop
a practical algorithm with provable guarantees based on QJL and the inner product estimator defined
in Definition 3.1.
The quantization scheme presented in Algorithm 1 applies QJL, defined in Definition 3.1, to each
key embedding, mapping them to binary vectors and storing the results in the key cache. We show
in the following theorem that the attention scores calculated by Algorithm 1 have very small (1 Œµ)
¬±
relative distortion with high probability:
Theorem 3.6 (Distortion bound on QJL key cache quantizer). For any sequence of key tokens
k ,...k Rd and any integer m, Algorithm 1 stores binary vectors kÀú ,...kÀú 1,+1 m along
1 n 1 n
‚àà ‚àà {‚àí }
with scalar values ŒΩ ,...ŒΩ in the cache. If the key embeddings have bounded norm max k r
1 n i‚àà[n] i 2
‚à• ‚à• ‚â§
and m 2r2Œµ‚àí2logn, then for any query embedding q Rd with bounded norm q r the
n n 2
‚â• ‚àà ‚à• ‚à• ‚â§
output of the procedure EstimateScores(q ) satisfies the following with probability 1 1
n ‚àí poly(n)
sinultaneously for all i [n]:
‚àà
(cid:12) (cid:12)
(cid:12)S(cid:94)core(i) Score(i)(cid:12) 3Œµ Score(i),
(cid:12) (cid:12)
‚àí ‚â§ ¬∑
where Score is the vector of attention scores defined in Equation (2).
Proof. The proof is by invoking Lemma 3.5 and a union bound. For every j [n] the estimator
‚àà
q(cid:103)K(j) computed in line 6 of Algorithm 1 is in fact equal to the inner product estimator q(cid:103)K(j) =
Prod (q ,k )asdefinedinEquation(4). ThusbyLemma3.5wehavethefollowingwithprobability
QJL n j
at least 1 1 :
‚àí n3/(2+2Œµ) (cid:12) (cid:12) Œµ
(cid:12) (cid:12)q(cid:103)K(j) ‚àí‚ü®q n,k
j
‚ü©(cid:12)
(cid:12) ‚â§ r2
¬∑‚à•q
n ‚à•2
‚à•k
j ‚à•2 ‚â§
Œµ,
where the second inequality follows from the preconditions of the theorem regarding the boundedness
of the norms of the query and key embeddings. By union bound, the above inequality holds
simultaneously for all j [n] with high probability in n. Thus after applying the softmax function
‚àà
in line 7 of Algorithm 1 we get that with high probability in n:
S(cid:94)core(i) e¬±2Œµ Score(i) (1 3Œµ) Score(i).
‚àà ¬∑ ‚àà ¬± ¬∑
7This completes the proof of Theorem 3.6.
This theorem shows that if the query and key embeddings have constant norms, as is common
in practical scenarios, we can quantize each key embedding such that only m Œµ‚àí2logn bits are
‚âà
needed to store each key token. This is independent of the embedding dimension of the tokens and
scales only logarithmically with the sequence length.
3.2 Value Cache Quantization
We quantize the value cache using a standard quantization method, i.e., normalizing each token‚Äôs
entries and then rounding each entry to a few-bit integer representation. This approach aligns with
prior work, which has shown that standard token-wise quantization is highly effective for the value
cache and results in a minimal accuracy drop [22, 13].
4 Experiments
Inthissection,wevalidatetheempiricalperformanceofouralgorithm. Allexperimentsareconducted
under a single A100 GPU with 80GB memory. We implement two main CUDA kernels for our core
primitives: one for quantizing embedding vectors using various floating point data types such as
bfloat16, FP16, and FP32, and the other for computing the inner product of an arbitrary embedding
vector with all quantized vectors in the cache. The algorithm‚Äôs wrapper is implemented in PyTorch,
handling all the housekeeping tasks. We plan to complete implementation in the CUDA for future
work, which will further accelerate our algorithm.
4.1 Practical Consideration
Outliers. As reported in recent works e.g., KIVI [22], KVQuant [13], key embeddings typically
contain outliers exhibiting a distinct pattern. Specifically, certain coordinates of key embeddings
display relatively large magnitudes. To further investigate these observations, we analyze the
distribution of the magnitudes of key embedding coordinates across different layers. Firstly, we
observe that there are no significant outliers in the initial attention layers. However, in the deeper
layers, certain fixed coordinates of key embeddings consistently exhibit large magnitudes, and this
pattern persists within these channels across all tokens. The distribution of outliers across different
layers for the Llama-2 model is plotted in Figure 2. It is evident that in the initial layers, outliers are
rare, but as we approach the final layers, their frequency and impact increase significantly. Secondly,
the outliers show a persistent pattern in specific fixed coordinates of the key embeddings. This
observation aligns with previous findings that certain fixed embedding coordinates exhibit larger
outliers [9, 20, 22, 13].
As demonstrated in Theorem 3.6, the distortion on the attention scores is directly proportional
to the norms of the embeddings. Therefore, capturing these outlier coordinates is essential, as
their large magnitudes contribute significantly to the norms of key embeddings. By identifying and
isolating these outlier channels, we can reduce the norm of the key embeddings and, consequently,
significantly decrease the final distortion. Next, we quantize the outliers using an independent
instance of our QJL quantizer but with a lower compression rate, utilizing more bits to accurately
represent each outlier coordinate.
Orthogonalized JL transform. We observed that orthogonalizing the rows of the JL matrix S
in Definition 3.1 almost always improves the performance of our QJL quantizer. This finding aligns
814 7 14 8
01122
...
5. 0. 505 Magnitude 001 ... 680 241 61 802 Magnitude 3456 241 61 802 Magnitude 34567
0.0 0.4 0 2 0 2
1400 0.2 1400 1 1400 1
1200 1200 1200
0 Ch20 ann40 els6 0 (So8 r0 ted10 )0 120 02004006008 T0 o1 0 k0 e00 ns 0 Ch20 ann40 els6 0 (So8 r0 ted10 )0 120 02004006008 T0 o1 0 k0 e00 ns 0 Ch20 ann40 els6 0 (So8 r0 ted10 )0 120 02004006008 T0 o1 0 k0 e00 ns
(a) Layer 0, Head 0 (b) Layer 15, Head 0 (c) Layer 31, Head 0
Figure 2: The magnitude of key cache entries for different layers of the Llama-2 model, based
on an example prompt, reveals notable patterns. The coordinates of embeddings (channels) are
sorted by their average magnitude over tokens. In the initial layers, no significant outlier patterns
are observed. However, in the deeper layers, a few channels (approximately four) exhibit visibly
larger magnitudes, indicating the presence of significant outliers. This observation highlights the
importance of addressing these outliers to improve quantization accuracy and reduce distortion in
the key cache.
with previous work on various applications of the JL transform, such as random Fourier features [35]
and locality sensitive hashing [14]. Consequently, in our implementation and all experiments, we
first generate a random JL matrix S with i.i.d. Gaussian entries and then orthogonalize its rows
using QR decomposition. We then use this orthogonalized matrix in our QJL quantizer, as described
in Algorithm 1.
4.2 End-to-end text generation
Next we benchmark our method on LongBench [4], a benchmark of long-range context on various
tasks. We choose the base model as longchat-7b-v1.5-32k [19] (fine-tuned Llama-2 with 7B parameter
with 16,384 context length) and apply following quantization methods to this model; KIVI [22],
KVQuant [36] and our proposed quantization via QJL. Each floating-point number (FPN) in the
base model is represented by 16 bits, and we choose proper hyper-parameters of KIVI and QJL
so that their bits per FPN become 3. For KVQuant, we follow the default setting which holds its
bits per FPN as 4.3. To validate the quality of those quantized models, we benchmark them on 6
question-answer datasets from LongBench [4], and we set the maximum sequence length to 31,500.
We follow the same approach of prompting and evaluating to evaluate the prediction of the model
from the original repository. Table 1 summarizes the results. Our proposed QJL achieves the highest
F1 score within the quantization methods for NarrativeQA, Qasper and 2WikiMultiQA.
Although KVQuant performs better than other methods for MultiQA-en dataset. It requires
a huge overhead which leads to slow runtime. To validate this, we additionally report runtimes of
prompt encoding and decoding in a single attention layer. The input sequence lengths vary between
1k to 128k. As shown in Figure 3, KVQuant runs slower than other methods during both phases.
On the other hand, both KIVI and our QJL with 3 bits per FPN show marginal overhead during
prompting but run much faster than the baseline. For example, their decoding time is 2.32 faster
√ó
for inputs of 128k length. However, the performance of KIVI is worse than QJL and this justifies
the practical benefits of our method.
We additionally test our method on datasets Lambada-OpenAI, HellaSwag, PIQA, MathQA, and
MMLU, which have shorter sequence lengths. We benchmark our method using LM-eval [12] framework
9Datasets from LongBench [4]
Methods Bits
NarrativeQA Qasper MultiQA-en MultifQA-zh HotpotQA 2WikiMultiQA
FP16 (baseline) 16 20.79 29.42 42.83 34.33 33.05 24.14
KIVI [22] 3 20.96 29.01 40.93 34.75 32.79 23.01
KVQuant [13] 4.3 20.14 28.77 44.22 34.44 34.06 23.05
QJL (ours) 3 21.83 29.44 41.52 34.42 35.62 23.60
Table 1: Evaluation (F1 scores) of various quantization methods on long-context question-answering
datasets from LongBench [4]. We set bits per floating-point number (FPN) to 3. Bold indicates the
highest scores within quantization methods.
Datasets from LM-eval [12]
Models Methods Bits
Lambada-OpenAI HellaSwag PIQA MathQA MMLU
FP16 (baseline) 16 73.90 57.18 78.07 28.11 41.85
Llama-2-7B KIVI [22] 3 73.88 57.13 78.07 28.11 41.81
QJL (ours) 3 73.88 57.14 78.07 28.17 41.78
BF16 (baseline) 16 75.59 60.17 79.65 40.64 62.09
Llama-3-8B
QJL (ours) 3 75.61 60.13 79.87 40.60 62.12
Table 2: Evaluation (accuracy) of various quantization methods on regular length datasets from
LM-eval [12]. These comparisons are not typically based on long-context length; however, as evident,
even in these cases, our QJL with 3 bits per FPN performs comparably to the baseline with 16 bits
per FPN.
to ensure a thorough evaluation across various metrics. We evaluate quantization methods with
accuracy across Llama-2-7B [31] and Llama-3-8B [23] models. Note that KIVI only supports a
half-precision floating point, whereas our method can be used for any precision format type. This
makes it unable to run KIVI on the Llama-3 model.
As a results, QJL can significantly reduce memory usage by utilizing only 3 bits per FPN,
compared to the 16 bits per FPN in the baseline, achieving around an 81% reduction in memory.
We observe that this efficiency does not compromise performance significantly. Across all datasets,
our method‚Äôs accuracy is generally comparable to the baseline, with slight variations. In Table 2,
our QJL on the Llama-3-8B performs on average about slightly better than the baseline across all
datasets.
References
[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4
technical report. arXiv preprint arXiv:2303.08774, 2023.
[2] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and
Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head
10101
102 FP16
KIVI
KVQuant
QJL(ours)
101
FP16
100
KIVI
KVQuant
QJL(ours)
100
2k 8k 32k 128k 2k 8k 32k 128k
token length token length
Figure 3: Wall-clock time (ms) to encode prompt and quantize (left) and decode tokens (right)
using different quantization methods in a single attention layer. The input sequence length varies
from 1k to 128k. Both KIVI and QJL (ours) with 3 bits per FPN show faster decoding time than
the baseline. However, KVQuant runs with a huge overhead during both quantizing and decoding
processes.
checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language
Processing, pages 4895‚Äì4901, 2023.
[3] Antropic. claude, 2024. https://www.anthropic.com/news/claude-3-family.
[4] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du,
Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual,
multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023.
[5] St√©phane Boucheron, G√°bor Lugosi, and Olivier Bousquet. Concentration inequalities. In
Summer school on machine learning, pages 208‚Äì240. Springer, 2003.
[6] Moses S Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings
of the thiry-fourth annual ACM symposium on Theory of computing, pages 380‚Äì388, 2002.
[7] Microsoft Copilot, 2023. https://github.com/features/copilot.
[8] Sanjoy Dasgupta and Anupam Gupta. An elementary proof of a theorem of johnson and
lindenstrauss. Random Structures & Algorithms, 22(1):60‚Äì65, 2003.
[9] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix
multiplication for transformers at scale. Advances in Neural Information Processing Systems,
35:30318‚Äì30332, 2022.
[10] Shichen Dong, Wen Cheng, Jiayu Qin, and Wei Wang. Qaq: Quality adaptive quantization for
llm kv cache. arXiv preprint arXiv:2403.04643, 2024.
[11] Adobe FireFly, 2023. https://firefly.adobe.com/.
[12] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles
Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac‚Äôh, Haonan Li, Kyle McDonell, Niklas
Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron,
11
)sm(
emit
gnitpmorp
)sm(
emit
gnidocedLintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A
framework for few-shot language model evaluation, 2023. https://github.com/EleutherAI/
lm-evaluation-harness.
[13] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney, Yakun Sophia
Shao, Kurt Keutzer, and Amir Gholami. KVQuant: Towards 10 Million Context Length LLM
Inference with KV Cache Quantization. arXiv preprint arXiv:2401.18079, 2024.
[14] Jianqiu Ji, Jianmin Li, Shuicheng Yan, Bo Zhang, and Qi Tian. Super-bit locality-sensitive
hashing. Advances in neural information processing systems, 25, 2012.
[15] William B Johnson, Joram Lindenstrauss, and Gideon Schechtman. Extensions of Lipschitz
maps into Banach spaces. Israel Journal of Mathematics.
[16] Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, and
Tuo Zhao. Gear: An efficient kv cache compression recipefor near-lossless generative inference
of llm. arXiv preprint arXiv:2403.05527, 2024.
[17] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
models. arXiv preprint arXiv:2001.08361, 2020.
[18] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph
Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model
serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems
Principles, pages 611‚Äì626, 2023.
[19] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica,
Xuezhe Ma, and Hao Zhang. How long can open-source llms truly promise on context length?,
2023. https://huggingface.co/lmsys/longchat-7b-v1.5-32k.
[20] JiLin,JiamingTang,HaotianTang,ShangYang,XingyuDang,andSongHan. Awq: Activation-
awareweightquantizationforllmcompressionandacceleration. arXivpreprintarXiv:2306.00978,
2023.
[21] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios
Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance
hypothesisforllmkvcachecompressionattesttime. Advances in Neural Information Processing
Systems, 36, 2024.
[22] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi
Chen, and Xia Hu. Kivi: A tuning-free asymmetric 2bit quantization for kv cache. arXiv
preprint arXiv:2402.02750, 2024.
[23] Llama3, 2024. https://github.com/meta-llama/llama3.
[24] Midjourney, 2022. https://www.midjourney.com/home.
[25] OpenAI. Introducing gpt-4o, 2024. https://openai.com/index/hello-gpt-4o/.
[26] OpenAI. Sora: Creating video from text, 2024. https://openai.com/index/sora/.
12[27] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan
Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference.
Proceedings of Machine Learning and Systems, 5, 2023.
[28] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical
text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.
[29] Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint
arXiv:1911.02150, 2019.
[30] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy
Liang,ChristopherR√©,IonStoica,andCeZhang. Flexgen: High-throughputgenerativeinference
of large language models with a single gpu. In International Conference on Machine Learning,
pages 31094‚Äì31116. PMLR, 2023.
[31] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open
foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. 2017.
[33] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming
language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023.
[34] June Yong Yang, Byeongwook Kim, Jeongin Bae, Beomseok Kwon, Gunho Park, Eunho Yang,
Se Jung Kwon, and Dongsoo Lee. No token left behind: Reliable kv cache compression via
importance-aware mixed precision quantization. arXiv preprint arXiv:2402.18096, 2024.
[35] Felix Xinnan X Yu, Ananda Theertha Suresh, Krzysztof M Choromanski, Daniel N Holtmann-
Rice,andSanjivKumar. Orthogonalrandomfeatures. Advances in neural information processing
systems, 29, 2016.
[36] Yuxuan Yue, Zhihang Yuan, Haojie Duanmu, Sifan Zhou, Jianlong Wu, and Liqiang Nie.
Wkvquant: Quantizing weight and key/value cache for large language models gains more. arXiv
preprint arXiv:2402.12065, 2024.
[37] Amir Zandieh, Insu Han, Vahab Mirrokni, and Amin Karbasi. Subgen: Token generation in
sublinear time and memory. arXiv preprint arXiv:2402.06082, 2024.
[38] Tianyi Zhang, Jonah Yi, Zhaozhuo Xu, and Anshumali Shrivastava. Kv cache is 1 bit per
channel: Efficient large language model inference with coupled quantization. arXiv preprint
arXiv:2405.03917, 2024.
[39] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song,
Yuandong Tian, Christopher R√©, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient
generative inference of large language models. Advances in Neural Information Processing
Systems, 36, 2024.
13