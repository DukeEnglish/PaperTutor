WINGS: Learning Multimodal LLMs
without Text-only Forgetting
Yi-KaiZhang1,2,3∗ ShiyinLu3 YangLi3 YanqingMa3 Qing-GuoChen3
ZhaoXu3 WeihuaLuo3 KaifuZhang3 De-ChuanZhan1,2 Han-JiaYe1,2†
1SchoolofArtificialIntelligence,NanjingUniversity
2NationalKeyLaboratoryforNovelSoftwareTechnology,NanjingUniversity
3AIBusiness,AlibabaGroup
Abstract
Multimodallargelanguagemodels(MLLMs),initiatedwithatrainedLLM,first
alignimageswithtextandthenfine-tuneonmultimodalmixedinputs. However,
theMLLMcatastrophicallyforgetsthetext-onlyinstructions,whichdonotinclude
images and can be addressed within the initial LLM. In this paper, we present
WINGS,anovelMLLMthatexcelsinbothtext-onlydialoguesandmultimodal
comprehension.AnalyzingMLLMattentioninmultimodalinstructionsrevealsthat
text-onlyforgettingisrelatedtotheattentionshiftsfrompre-imagetopost-image
text. From that, we construct extra modules that act as the boosted learner to
compensatefortheattentionshift. Thecomplementaryvisualandtextuallearners,
like“wings”oneitherside,areconnectedinparallelwithineachlayer’sattention
block. Initially,imageandtextinputsarealignedwithvisuallearnersoperating
alongsidethemainattention,balancingfocusonvisualelements. Textuallearners
arelatercollaborativelyintegratedwithattention-basedroutingtoblendtheoutputs
ofthevisualandtextuallearners. WedesigntheLow-RankResidualAttention
(LoRRA) to guarantee high efficiency for learners. Our experimental results
demonstratethatWINGSoutperformsequally-scaledMLLMsinbothtext-onlyand
visualquestion-answeringtasks. OnanewlyconstructedInterleavedImage-Text
(IIT) benchmark, WINGS exhibits superior performance from text-only-rich to
multimodal-richquestion-answeringtasks.
1 Introduction
LargeLanguageModels(LLMs)[22,34,52,90,112,113]aremakingsignificantstridestoward
Artificial General Intelligence (AGI) systems. Multimodal Large Language Models (MLLMs),
as a visual expansion of LLMs, have demonstrated astonishing performance in vision-related
captioning [15, 17, 66, 68], understanding [8, 19, 33, 91, 100, 106, 118, 141], and reason-
ing [1, 114, 122, 126, 133]. Common MLLMs build upon powerful pre-trained LLMs that take
mixedtextualandvisualtokensasinputs. Thevisualonesareacquiredusinganimageencoderanda
projector. WedescribeinstructionsprocessedbytheLLMwithoutimagesastext-onlyinstructions.
Incomparison,multimodalinstructionsincorporatevisualfeaturetokensintotext-onlysequences.
ModalityfusingatthetokenlevelprovidesaflexibleandeffectivepipelinefortrainingMLLMsto
comprehendvisualinformation[76,79,80,125].However,trainingonmultimodalinstructionsseems
toimpairthepre-existingprofoundknowledge,especiallymakingMLLMforgethowtorespond
totext-onlyinstructionsliketheinitialLLM[85,87]. MLLMexperiencesadrasticperformance
declineontext-onlyevaluation. Wetermitasthetext-onlyforgettingofMLLM.
∗WorkdoneduringtheinternshipatAIBusiness,AlibabaGroup.
†Correspondingauthor,email:yehj@lamda.nju.edu.cn.
Preprint.Underreview.
4202
nuJ
5
]LC.sc[
1v69430.6042:viXraText-only Instruction Text-onlyQA v.s. MultimodalQA
User:Let's pick out a better watermelon.
User:What are some refreshing fruits
to beat the summer heat? MMLU HallusionBench WINGS(Ours):Both watermelons appear to be good
W bluI eN bG eS rr( iO esu , r ras s): pb(… err) ie w s,a pt ee ar cm he el so , n a n, ds t ora raw nb ge er sr .ies, RACE-C MMMLU AI2D SEED-Bench q m fru eoa srl heit n y s ep, s lb siu t det ud et ch toe ol ol te r h,f e t i n io ndn ti ae c c a tth i ga n rs g e a ep ns o m t se to n eo t mit ah .le A sr w n r di en e …d tn a en sd s and
ScienceQA
Interleaved Image-Text Context RACE-H Multimodal Instruction
User:Oh wow! I just found two MME User:You make a good point, but Ihaven't
watermelons in thegarden: ARC tasted it yet. You know what that
They're gonna 1 2 MMBench-CN thingis?ItisBeautiful.
be super refreshing. HellaSwag
MMBench-EN
WINGS(Ours):The object in the
WINGS(Ours):The WinoGrande image is a watermelon ,which has
image shows two watermelons, one labeledas MMMU-TEST been carved and decorated to resemble the heart. The
“1” and the other as “2.” They are placed next to GSM8K MBPP MMMU-VAL heart shape is formed by the watermelon‘s natural rind ,
each other, and both are ripe. The watermelons and the rosesare made from the watermelon’s flesh .
are greenand have a striped pattern… LLaVA-Next DeepSeek-VL WINGS(Ours) The initials "K & M" are inscribed on…
Figure1: Examplesoftext-onlyandmultimodalconversations. Fromlefttoright: Interacting
withMLLMthroughtext-onlyandinterleavedinstructions;PerformanceradarchartsforWINGS,
LLaVA-Next[80],andDeepSeek-VL[85]intext-onlyandmultimodalQAtasks,withdarkgreen
indicatingWINGSwiththecomprehensiveperformance;Interactingwithmultimodalinstructions.
Inpracticalapplications,MLLMsalsorequireengagingintext-onlyorinterleavedconversations.
AsdemonstratedinFigure1,usersoftenstartwithtext-onlyinquiriesandthen,ifnotfullysatisfied
withtheresponse,proceedtosupplementquestionswithimagecontent. Formultimodalinstructions,
MLLMsarestillpromptedtocapturecriticalelementsfromtextwithinamultimodalinstruction,
asimagesmayprovideredundantcues[16,18,84]. Thefirstexistingapproachesreplayextensive
text-onlyorinterleaved[59,142]trainingdatatomitigatecatastrophicforgettinginMLLMs[14,
71,85,87]. However,increasingtrainingdataincursadditionalcomputationaloverheadanddata
collectionchallenges. Secondly,switchingbetweenLLMandMLLMbasedonwhetherimagesare
included,asanintuitivesolution,inevitablydemandsmoredeploymentmemory[2,3]andisless
cache-friendlyinlongvision-and-languageinterleavedconversations[41,75,99,101]. Therefore,it
iscrucialtotrainMLLMwhilepreservingthetext-onlyperformanceefficiently.
Giventhattheimagefeaturetokenscanbeinsertedatanypositionwithinthetextsequence,webegin
byexaminingthetextbeforeandaftertheinsertedposition. ConsideringthatMLLM’sattention
weights reflect the focus on tokens and influence the decision-making process, we first analyze
the attention weights across each layer of the MLLM. Specifically, for each layer, we compute
the attention weight proportion on all text tokens before and after the inserted image, termed as
Layer-levelAttentionWeights(LAWS)ofthebeforeandafterimagetext. Fromthis,weexamine
the dynamic of attention across all layers as MLLM-Laws. Through training and sampling over
100diverseMLLMs,wefindthatawell-trainedonewithsuperiortext-onlyperformanceshowsa
positivecorrelationofMLLM-LAWSbetweenbeforeandafterimage. Giventhesimilarityoffeature
spaceinthetextsurroundingtheimage,anMLLM’sattentiontobothfrontandrearpartsshouldbe
correspondinglysimilar. Aclosersimilarityindeedsuggestsamoreminordisruptiontotheessential
attentionofMLLM.Conversely,anegativecorrelationimpliesashiftintokenattentionacrossthe
imagecontent,i.e.,anMLLMoverlyconcentratesonvisualtokensandneglectstextualones.
Basedonthisobservation,weproposeWINGS,whichintroducesanextramodulethatactsasthe
boostedlearnertocompensatefortheattentionshift. Weintegratecomplementaryvisualandtextual
learnersinparallelateachlayer’sattentionblock,withvisuallearnersenhancingfocusonvisual
tokensandtextuallearnersontext,respectively. Inthefirststage,visualfeaturesalignwithtextual
featuretokens,withallvisuallearnersoperatingparalleltothemainbranchattention. Thevisual
learnersallocatesomeattentiontovisualtokens,mitigatingtheattentionshiftinthemainbranch.
Subsequently,textuallearnersareintegratedinparallel. Weimplementsoftroutingbasedonshifted
attentionweightstoharmonizethelearningonvisualandtextualtokens. WedesigntheLow-Rank
Residual Attention (LoRRA) as the architecture for learners to ensure high efficiency. Figure 3
shows that the visual and textual learners on either side, like light feathers woven into “wings”.
Experiments show that our WINGS comprehensively achieves superior performance in text-only
underthesametrainingconditionandexceedsotherequal-levelMLLMsonmultimodalbenchmarks.
Inaddition,weconstructtheInterleavedImage-Text(IIT)benchmarkwithmulti-turnevaluations
towardsageneralmixed-modalityscenario. Thesamplesarefromtext-onlyquestionstostrongly
image-relatedconversations. WINGSachieveleadingperformanceacrossvariousvision-relevance
partitions. Overall,ourcontributionsareasfollows: (1)Weclaimandverifythetext-onlyforgetting
phenomenonofMLLMisrelatedtotheattentionshiftofcross-layerMLLM-LAWSbeforeandafter
theimage. (2)WINGSconstructthevisualandtextuallearnersandintroducearouterbasedonshifted
attentionweightsforcollaborativelearningtocompensateforattentionshifts. (3)Experimentson
2text-only,visual-question-answering,andnewlyconstructedInterleavedImage-Text(IIT)benchmarks
demonstratethecomprehensiveandversatileperformanceofWINGS.
2 ACloserLookatAttentionShiftinMultimodalLLMs
Inthissection,weintroducethedevelopmentfrominitializedLLMtoMLLM.Next,wedevisethe
MLLM-LAWSmetricforrepresentingattentionshiftanddiscusstheinsightsinbuildingWINGS.
2.1 GrantingSighttoLargeLanguageModels
Large Language Models (LLMs). Even though existing Transformer-based [117] models [21,
82,98,127]likeBERT[56]andOPT[136]havedemonstratedprofoundlanguageunderstanding
capabilities,therehasbeenarecentsurgeinpowerfulGenerativePre-trainedTransformers(GPT)[11]
undertheauto-regressivelanguagemodelingparadigm. Bothpublic[52,53,112,113]andprivate[4,
90,92,110]solutionsshowremarkableprogressinlanguagecomprehensionandgeneration[88,
121]. TheseLLMsgenerallyexceedabillionparameters, includingpre-training[23,32,48,54],
supervisedfine-tuningwithinstructions[27,103,109,120],andreinforcementlearningfromhuman
feedback[24,93,107,143]onmassivetrainingdata.
MultimodalLLMs(MLLMs). IntegratingvisualinputsintofoundationalLLMstocreateMLLMs
is becoming increasingly popular [19, 20, 61, 71, 129]. Unlike vision-centric multimodal frame-
works[67,130]suchasCLIPseries[97],MLLMsaimtoalignnewmodalityfeaturesastheinput
of LLM with an additional encoder [74, 79, 80, 123, 135, 140]. As illustrated in Figure 2 (a), it
enables the combined training of mixed multimodal tokens, facilitating rapid deployment across
variousapplications[25,26,43,81,118]. OneexampleofthispipelineistheLLaVA[79]series,
which integrates a CLIP vision encoder with a linear projection to Vicuna [22] and innovatively
introducesinstruction-followingtrainingdata. Followingthis,somemethodsconsidertherichness
ofthevision-relatedtrainingcontext[15,44,60], thescaledvisualbackbone[50,72,78], orthe
enhanced connectors [12, 119] to boost the visual effectiveness of MLLMs. With commonplace
text-onlychallengesinconversations,itisessentialtoenhancethelanguageabilitiesofMLLMs[87].
ThetrainingprocessofMLLMs,ascontinuedlearningonnewlyintroducedvisualfeatures,causes
competitivemodalityshift[38,73,96]andcatastrophictext-onlyforgetting. Recentstudiesacknowl-
edgethisissue,e.g.,DeepSeek-VL[85]suggeststhatsupplementingadditionaltext-onlytraining
datacanmitigatethisforgetting. Others[77,87]trytoincorporateinterleavedvisual-textualdata
intotrainingtoretainlanguageknowledge. However,thesemethodsarelimitedbytrainingresources
anddatacollectioncosts. Weaimtopreserveorevenboostperformancewithtext-relatedtraining
data as little as possible. Some studies [53, 64, 76, 105, 111, 128, 137] also consider expanding
thescalabilityofLLM,suchasusingMixture-of-Expert(MoE)withnumerousparallelFFNsin
the Transformer block alongside a sparse gating network for efficient selection. These methods,
however,requireamassiveincreaseintrainingparameters. InWINGS,thenewlydesignedparallel
learnersofLow-RankResidualAttention(LoRRA)aresimilartoMoEbutwithatleastthreeorders
ofmagnitudelessinresourceconsumption.
2.2 CapturingtheAttentionShiftwithMLLM-LAWS
Thesignificantdeclineintext-onlyperformanceiscloselylinkedtotheobservedrelatedshiftduring
thetrainingprocess. Researchoncross-modallearning[35,65,73]showsthattransferringtonew
modalitiesaffectsfeaturedistribution,outputvalues,andactivationlevels. Consideringattention
weightshighlightwhereMLLM’sfocusdependsonvisualortextualtokensfordecision-making[95],
weinvestigatehowattentionshiftsamongdifferentpartsofthesequences,mainlywheredividedby
thevisualfeaturetokens. Specifically,westudyover100diverseMLLMstouncoverhowattentionis
allocatedtoeachpartforatext-onlybetterMLLM.Wetakeacloserlookatthecross-layerdynamic
curveofattentionproportiononalltexttokensbeforeandaftertheinsertedimage.
ForainstructionxanditshiddenstatesinMLLMash = [h ,h ,··· ,h ]consistingofsmixed
1 2 s
visualandtextualtokens. Letal representtheattentionweightbetweentheith andjth tokensin
ij
thelth oftheL-layersMLLM.Wehave,for∀i,(cid:80)s al (cid:0) hl−1(cid:1) = 1. AsshowninFigure2(a),
j=0 ij
sincethesequenceofflattenedvisualtokensiscontinuouslyinterleavedwiththetextualsequence,
wedenotetheindexsetofthevisualtokensasV ={v ,v +1,··· ,v }. Werefertothe
itself start start end
31 3
ℓ=1 VLLMLayers • CorrelationofMLLM-LAWSBeforeImage v.s. AfterImage
InputFeatures 𝜌 1 , 3 =−0.723
What
···
are
the
types 𝜌 1 , 3 = 0.255
of Attention
watermelon? SumofTokens 0.173 1
thI in s 0.048 0.162 More positivecorrelation𝝆suggests bettertext-only performance.
0.821 0.584 0.688
Attention 2
SumofTokens
e.g.,0.821is the attentionweightproportion(sum)
ofthevisualfeaturetokens forthe firstlayer.
what
kindof Attention VLLMw/BetterText-OnlyPerf.
it SumofTokens VLLMw/WorseText-OnlyPerf. CorrelationofMLLM-LAWS
is? 3 BeforeandAfterimage
(a)Themixedvisualand (b)MultimodalLLMs’ −0.723 0.255
textualfeatureinputs. Layer-levelAttentionWeights(MLLM-LAWS). (c)MLLM-LAWS CorrelationIllustration
Figure2: Illustrationofmixedvisual-and-textualinputsandtheLayer-levelAttentionWeights
(LAWS)withitsproperties. (a)Thevisualfeaturetokensfromthevisualencoderandprojector
areinsertedintothetextualfeaturesequence. (b)Theattentionweightproportionontextualtokens
before-image,image-itself,andafter-imageacrosslayers. Theredcurveisfromthesuperiortext-only
MLLM,whilethebluecurveisfromtheinferiorone. (c)Experimentsonover100MLLMsshowa
positivecorrelationfromtheρforMLLM-LAWSbeforeandafterthevisualtokens(x-axis)tothe
text-onlyperformanceoftheMLLM(y-axis).
textualsequencebeforethevisualtokensasV ,andsimilarly,afterthevisualpartasV . Foran
before after
MLLMwithLlayers,wedefinetheLayer-levelAttentionWeights(MLLM-LAWS)as:
s
LAWSV∗ =(cid:2) a1 V∗,a2 V∗··· ,aL V∗(cid:3) , al
V∗
=(cid:88) (cid:88) al ij(cid:0) hl−1(cid:1) , (1)
i=0j∈V∗
wheretokenindexsetV canbeV ,V ,orV asmentionedabove,andforsimplicity,we
∗ itself before after
omit hl−1 in LAWSV∗. In practice, LAWSV∗ characterizes the MLLM’s attention on the current
sequence V regarding the dynamic curve over all MLLM-layers. As shown in Figure 2 (b), the
∗
attentiontothetextualpartinitiallyincreasesandthendecreases,whilethetrendforthevisualone
isoftentheopposite. WefindthatwhentheMLLMforgetsthetext-onlyinstructions,theLAWSof
thetextualsequenceafterthevisualonesshowadeviationfromtheinitialtrendofrisingandthen
declining. ThisimpliesashiftinthetextfollowingtheimageV comparedtothatprecedingthe
after
imageV . Thedynamicslabeledas⃝3 inFigure2(b)showtheredcurveforbettertext-only
before
performance towards the worse blue one. To quantify this, we compute the Pearson Correlation
Coefficient[89]betweenLAWSbeforeandafterthevisualsequence. Formally,
AttentionShift=E x[−ρ(LAWSVbefore, LAWSVafter)]+1.
Studyingtheattentionshiftofover100diverseMLLMs,wefindapositivecorrelationbetweenthe
shift and the text-only performance degradation. In Figure 2 (c), each point represents a trained
MLLM,andwedemonstratehowtheattentionshiftinfluencesthetext-onlyperformancewiththe
correlations. Next,Wefocusonhowtomitigatetheshiftedattentionweights. StartingwithLAWS
wegivetheMLLM“wings”.
3 WINGS: FlyingtoGeneralitywithLow-RankResidualAttentionLearners
Fromtheattentionshift,weseekasufficientlyreliableandconvenientmechanismtoaddresstext-only
forgetting. TheWINGSarchitectureisintuitive–weconstructvisualandtextuallearnerstomitigate
shiftedattention. Anattention-weight-basedrouterdynamicallyadjuststheoutputsofvisualand
textual learners to compensate for the main branch’s attention outputs. WINGS aims to excel in
text-onlyandvisualquestion-answeringtaskswithhighgenerality. Inthissection, westartwith
thetypicaltrainingpipelineforMLLM(subsection3.1). Followingthis,weexplorethemotivation
behindemployingparallelmodalitylearnersandexplainitsimplementation(subsection3.2). Finally,
wedescribethetrainingprocessforWINGS(subsection3.3).
4
ecnamrofrePylno-txeTThetypesofwatermeloninclude:
RedorYellow,SeededorSeedless,…
Norm&FFN
Visual Textual
Multi-Head
V Self-Attention T
Learner Learner
···
· Norm&FFN Weighted
· Element-wiseAdd
·
Multi-Head
V Self-Attention T
Learner Learner
Norm&FFN Weighted
Element-wiseAdd
Multi-Head
V Self-Attention T
Learner Learner
VisualFeatures LanguageFeatures
MixedFeatureInputs
Whatarethe typesofwatermelon?
Inthis<image>,whatkindofitis?
Figure 3: The WINGS - model architecture. We introduce extra modules parallel to the main
attention, serving as boosted learners to compensate for the attention shift. We train the visual
learnersononeside,alleviatingsomeshiftedattention. Then,wecollaborativelylearnvisualand
textuallearnersbasedonroutingshiftedattentionweights.Theyarelikelightfeatherswoven“wings”.
3.1 RevisittheTrainingPipelineoftheMLLM
FollowingthemainstreamarchitectureofMLLM,wetakemixedvisualandtextualfeaturesasinputs.
Foraone-turnconversation,thesequenceofthevisualfeaturetokensmayappearatanypositionin
theinputx. Werepresentthefeaturetokensas:
 
x=[x V,x T]=h 1,··· ,h vstart,h vstart+1,··· ,h vend,··· ,h s , (2)
(cid:124) (cid:123)(cid:122) (cid:125)
visualfeatures
where we omit the superscript of layer-index l for the 0th layer. The v and v represent the
start end
startingandendingindicesofthevisualfeaturetokens,usuallyobtainedthroughthevisionencoder
ψ and projector W , as x = W ·ψ(x ). Correspondingly, x = the remaining 0 to
proj V proj image T
v andv tolengthsdenotefeaturesofthetextualinstruction. Weconsidertheposteriorofthe
start end
ground-truthansweras:
s
(cid:89)
Pr(x |x)= 1 ·φ(h |[h ,··· ,h ]) . (3)
a [1,vstart)∪(vend,s] i 1 i−1
i=1
Here,φrepresentsthemainbranchLLM,whichconsistsofTransformerdecoderlayers[116].
3.2 VisualandTextualLearnersWeaveWINGS
Motivation: Learningtomitigatetheattentionshiftwithmodality-specificauxiliarystructures.
As mentioned in subsection 2.2, MLLM-Laws demonstrates the attention shift in the sequence
followingthevisualfeatures. Theshiftresultsfromexcessivedependencyonvisualfeatures. This
issuemaystemfromtheinsufficientalignmentwithinmixedinputs[8,16],wherenewmodalities
canobscureexistingknowledge. Itsuggestsaddingasmall,adjustablefactortotheshiftedmixed
modalityfeaturesandregulatingunnecessaryfluctuationsinMLLM-LAWS. Consequently,weaim
to adopt an efficient, learnable module as the visual “wing”. Compared to the image-text mixed
featureinputsofthemainbranch,itshouldfocusonextractingvisualinformationtosharetheburden
ofoverlyshiftedattention. Theinteractionbetweenthecurrenthiddenstateandvisualfeaturesis
5V
Attention
Add Router
Initialization: Router
:Random Gaussian
:Zero Concat
h LLM LLM
ScaledDot-ProductAttention
V weights main T
Learner Learner
Attention
Projector Projector
x Vx V x x EV ncis ou da el r EV ncis ou da el r
V V
Key x
V
Value x
V
Query hℓ hℓ Stage-1:alig( nb) prT or ja ecin toin rg anP da lr ea ad ri ngm
visuallearners
VisualFeatures HiddenStates
(a)Low-RankResidualAttention Stage-2:tuneLLMwithroutingvisualandtextuallearners
Figure 4: Illustrations of the detailed WINGS structure, and training strategies. WINGS is
constructedbytheLow-RankResidualAttention(LoRRA)modulewheretheprevioushiddenstate
actsasthequeryandthevisual/textualfeaturesserveasthekeyandvalue. Trainingstartswith
visuallearnersandprojectors,followedbythedynamicattention-basedrouting.
conductedwithinthismodule. Similarly,tobalancetheauxiliaryfunctionofthevisuallearner,we
alsoconstructasymmetricaltextuallearner. Moreover,weshouldappropriatelydistributethetwo
learnersacrossbothmodalitiestooperatecollectively.
Structure: parallellearner&routerofattentionoutputs. Tocapturekeyinformationinshifted
modalitieswhileensuringefficiency,wedesignamultiheadLow-RankResidualAttention(LoRRA)
learner at every layer. It takes input from the hidden state and interacts with the initial visual
ortext-onlyfeature. Thelearnerfacilitatescross-cascadingwiththeinitialprojectedinformation.
Specifically,forthelthlayer,thevisual/text-onlylearnerisformulatedas:
Learner∗(cid:16)
Q=hl,K,V=x
(cid:17)
=Softmax(cid:32) hl(cid:0) 1+WQ(cid:1) √·(cid:0)
x
∗(cid:0) 1+WK(cid:1)(cid:1)⊤(cid:33)
x
(cid:16) 1+WV(cid:17)
WO ,
∗ ∗
∗∈{V,T} d head
(4)
where the matrix WQ, WK, WV, and WO is low-rank and is obtained by the dot product of
W ∈ Rd×d and W ∈ Rd×d, and d is relatively small enough. 1 is represented as the identity
a b
matrix. FollowingLoRA[45],LoRRAlearnersalsoemployrandomGaussianinitializationforW
a
andsetsW tozero. GiventhatWQlacksaresidual,thelearner’soutputiszeroatthebeginningof
b
training. MultiheadLoRRApreservestheeffectivenessofthecross-attentionstructureandemploys
efficientlow-rankmappingtoreducecomputationaldemands. AsshowninFigure3,thevisualand
textualfeaturesarefedintotheirrespectivesidelearners, liketwo“wings”woventogether. The
outputsoftwolearnersfromeachlayerarethenweightedsumtotheattentionofthemainbranch.
AsillustratedintheleftpartofFigure4,arouterreceivesattentionweightstogeneratethebalance
weightsofthevisualandtextuallearners. Insummary,weformulatetheWINGSblockas:
AttWINGS =Attmain+ (cid:88) Router(a)·Learner∗(cid:0) hl,x (cid:1) , (5)
∗
∗∈{V,T}
wherea∈Rs×srepresentstheattentionweightsofthecurrentmainbranch. Therouterreceivesthe
attentionweightsaandthenprocessesthroughasingle-layerMLPwithSoftmax.
3.3 StableTrainingRecipe
ThearchitectureofWINGScomprisesfourelements: visionencoder,projector,initializedLLM,and
thelearnerswitharouter. Duringthetrainingprocess,thevisionencoderisconsistentlyfixed. Firstly,
weonlyfine-tunetheprojectorandvisuallearners. Weprimarilyemployimage-textpairsforvisual
alignment,whiletheoutputsofvisuallearnersaredirectlyaddedtothemainbranch. Subsequently,
theLLMbranchisupdatedwithsmallsteps. Concurrently,textuallearnersareparalleledwithvisual
learnersontheattentionblockofLLMs. Therouterlearnstoallocatevisualandtextuallearnersfrom
theattentionweightsofthemainbranch. Atthisstage,thetextualandvisuallearnersworkbetter
togethertodirectattentiontothekeytokens. Tosummarize, WINGSprioritizesenhancingvisual
learnersfirst. Subsequently,it“spreadsitswings”byconcurrentlylearningandroutingvisualand
textuallearnersbasedonshiftedattentionweights.
6
V renraeL T renraeL V renraeL T renraeLModel
Vicuna Vicuna LoRAVicu.Vicuna Qwen Qwen LoRAQw.Qwen WINGS Text-only Our
LLM + + + LLM + + + (Ours) Forgetting Impro.
Dataset CLIP CLIP SigLIP CLIP CLIP SigLIP ( - ) ( - )
MMLU 51.18 51.12 48.89 50.63 60.86 50.83 59.67 51.16 60.53 9.70 9.37
CMMLU 38.60 38.29 37.24 38.73 69.37 62.58 67.87 60.46 69.82 8.91 9.36
Exam
ARC-E 57.62 53.63 55.82 53.95 59.96 56.93 59.35 55.87 54.29 4.09 -1.58
ARC-C 33.75 34.60 34.68 35.17 38.90 39.14 38.64 39.50 43.39 -0.60 3.89
Winogrande 68.01 64.97 67.83 65.21 71.38 69.82 71.03 69.05 69.28 2.33 0.23
OpenbookQA 77.10 73.28 77.15 72.12 81.73 78.31 81.29 77.51 81.05 4.22 3.54
Under- Race-Middle 63.99 60.10 62.84 59.45 74.82 68.25 72.06 68.34 74.24 6.48 5.90
standing Race-High 58.74 53.24 54.91 52.69 71.05 59.20 65.67 57.72 69.62 13.33 11.90
WSC 51.30 47.21 51.06 47.72 56.17 54.18 57.30 55.23 66.35 0.94 11.12
CHID 39.05 49.66 45.26 53.49 71.94 71.82 72.92 74.29 74.06 -2.35 -0.23
HellaSwag 63.11 63.08 62.58 63.02 65.70 61.90 64.32 63.24 65.12 2.46 1.88
SIQA 42.37 44.06 43.27 44.52 45.57 50.20 46.83 51.71 49.64 -6.14 -2.07
Reasoning
PIQA 71.92 71.95 70.35 71.84 76.59 74.60 73.77 75.19 78.06 1.40 2.87
OCNLI 33.89 37.74 39.41 40.46 49.73 48.31 48.07 50.29 50.39 -0.56 0.10
Math GSM8K 25.19 23.72 22.68 23.05 56.77 50.10 54.25 51.37 52.08 5.40 0.71
Code MBPP 13.80 11.29 13.92 10.80 37.50 34.82 36.72 33.20 38.92 4.30 5.72
MMMU-VAL – 35.67 30.78 35.56 – 34.56 32.33 35.11 39.89 – 4.78
MMMU-TEST – 34.40 30.90 35.33 – 34.90 31.80 35.10 37.30 – 2.20
Multimodal
MMBench – 63.18 59.83 65.14 – 66.05 62.84 70.94 70.53 – -0.41
ScienceQA – 67.72 64.49 71.50 – 74.26 69.09 74.89 78.76 – 3.87
Table1: PerformancecomparisonsofWINGSandthebaselineMLLMsunderthesametraining
data. Weconsider8baselineMLLMs,includingLLMsasVicuna &Qwen1.5,visualencodersas
v1.5
CLIP[97]&SigLIP[134],andtrainingstrategiesasfull-parameter&LoRAfine-tuning. Thefirst
entryrepresentstheinitialLLM,uponwhicheachMLLMistrained. Ourevaluationspans6domains
with20datasets. WINGSisbasedontheQwen1.5andSigLIP,andthecolumn“OurImprovement”
highlightshowmuchWINGSsurpassesitsbaselinewiththesamebackbones.
4 Experiments
Inthissection,wefirstintroducethebenchmarksforevaluatingWINGS,includingTable1: text-only
forgettingonthesamemultimodaltrainingdata,Table2: comparisonwithgeneralMLLMs,and
Figure7:analysisontheInterleavedImage-Text(IIT)benchmarkwithvaryinglevelsofvision-related
conversation. Followingthat,weoutlinethetrainingdetailsandconfigurationsoftheWINGS,and
delveintoexperimentalanalysisacrosseachbenchmark. Moreover,weperformanablationstudyon
variouslearningrateswithdifferenttrainingparts.
EvaluationSetups. WeaimtoassessthroughMLLMhowmuchvisualinformationisrequiredfor
evaluation. Forexample,genericmultimodalinstructionsrequireMLLMstostronglycaptureimage
aspects,whereastext-onlyinstructionsfocusonthetext. Weintroducethreetypesofbenchmarks:
• Standardtext-onlybenchmarks. Weareparticularlyinterestedinthetext-onlyperformance
improvement of WINGS under the same training data and resource conditions. Different
datasets including interdisciplinary exams like MMLU [42], CMMLU [63], ARC-Easy, ARC-
Challenge [28], language understanding and knowledge such as WinoGrande [102], Open-
bookQA [9], Race-Middle, Race-High [58], WSC [124], CHID [138], reasoning such as Hel-
laSwag [132], SIQA [104], PIQA [10], OCNLI [46], and math and code-related tasks such as
GSM8K[29]andMBPP[5]arecomprehensivelyevaluated.
• Generalmultimodalbenchmarks. WeevaluateonMMMU[131],MME[37],MMBench[83]
(MMB)inEnglish(EN)andChinese(CN),ScienceQA[86]fortest(SciQA),SEED-Bench[62]
forimagepart(SEED),AI2D[55]fortest,andHallusionBench[40](HallB).
• OurInterleavedImage-Text(IIT)benchmarkwithdiversetext-only,interleaved,andimage-
ralated multi-turn conversations. It includes sampling for MMLU, CMMLU, OpenbookQA,
HellaSwag,MMMU,MMBench,SEED-Bench,andAI2Ddatasets.
ModelSummaries&ImplementationDetails. Wereleasethe WINGSbase and WINGSpro,with
Qwen1.5-7BLLM[7]andSigLIP[134]visualencoderasthefoundations. Wealsointroducethe
WINGS1.8B version, adaptedtoQwen1.5-1.8BLLMforedgedevicecompatibility. Asillustrated
inFigure4,weonlyoptimizetheprojectorandtheimagelearnersofWINGSforthefirstalignment
7Dataset Text-OnlyQAs MultimodalQAs
Method MMLU/C* RACE-M/H ARCHellaSwagWinog.GSM8KMBPPMMMU-V/TMMB-EN/CNMME SciQA SEED AI2D HallB
Equal-ScaleOpen-Source7BMultimodalLLMs
O-Flamingov2[6] 26.9/27.1 40.3/32.6 31.0 55.4 58.3 10.2 9.1 29.1/28.7 10.9/13.3 803.9 55.8 30.2 32.6 30.4
IDEFICS[49] 33.0/26.4 38.2/36.9 33.2 58.9 60.2 11.7 8.1 17.6/20.2 49.6/27.3 1239.3 62.4 44.8 43.4 24.6
InstructBLIP[30] 43.2/35.7 52.8/49.7 39.5 55.7 54.9 18.3 10.3 32.7/32.1 38.5/26.8 1425.6 61.3 45.7 41.1 33.3
ShareGPT4V[15] 47.6/36.9 55.9/51.0 41.6 54.7 60.1 18.0 8.9 35.5/35.2 67.4/63.1 1915.3 68.9 68.1 58.2 26.6
Qwen-VL[8] 49.7/58.3 65.2/64.8 34.4 58.2 61.0 49.0 34.6 36.4/35.9 60.3/57.4 1806.2 69.6 62.0 61.9 34.1
Monkey[72] 52.8/66.9 65.6/62.1 38.2 60.6 59.3 51.8 37.1 40.3/37.1 71.9/67.8 1815.4 78.3 69.1 62.5 42.1
LLaVAv1.5[79] 51.1/38.3 60.1/53.2 34.6 63.1 65.0 23.7 11.3 35.7/34.4 63.2/57.7 1518.6 67.7 63.7 56.4 29.7
LLaVANext[80] 50.2/39.7 65.1/58.3 36.0 63.7 68.9 30.3 23.0 37.6/35.8 67.8/61.8 1760.3 70.1 69.1 66.4 29.6
DeepSeek-VL[85] 53.9/64.0 70.6/63.8 39.2 65.1 67.2 55.3 43.1 37.6/35.3 72.7/72.5 1716.8 80.6 70.0 66.5 36.2
WINGS(Ours) 60.5/69.8 74.2/69.6 43.4 65.1 69.3 52.1 38.9 39.9/37.3 70.5/68.3 1753.8 78.8 69.5 62.7 45.8
WINGSpro(Ours) 61.3/68.5 82.8/76.3 46.3 69.2 70.9 56.3 39.3 38.2/36.9 73.1/69.0 1786.1 83.1 70.2 65.8 47.3
AdvancedPrivateMultimodalLLMs
GPT-4[92] 83.5/71.2 93.2/87.8 93.6 88.4 75.6 91.6 56.2† – – – – – – –
GPT-4V[91] 79.3/69.4 93.7/89.2 92.9 84.7 76.1 88.4 72.4 58.9/56.8 77.0/74.4 2153.6 68.4 73.7 75.5 46.5
Geminiprovision[100]85.9/73.7 88.9/83.2 85.0 78.8 71.5 86.4 61.5 60.6/62.2 73.6/74.3 2193.2 58.3 70.8 70.2 45.2
EfficientMultimodalLLMswithWINGS1.8B
DeepSeek-VL1.3B[85]31.7/38.2 63.6/58.4 35.8 52.9 45.7 17.6 16.3 33.8/32.3 65.1/60.7 1483.4 65.4 63.3 50.1 25.0
MiniCPM-V2.4B[47] 42.4/40.9 68.8/62.6 37.0 48.3 51.7 32.5 24.2 37.2/34.4 65.7/64.1 1584.1 64.9 64.7 54.9 31.8
WINGS1.8B(Ours) 44.9/50.9 68.5/63.2 37.1 50.5 53.0 40.6 28.5 35.7/33.9 64.2/61.2 1527.3 67.5 62.8 55.2 30.2
Table2: Performancecomparisonsoftheequal-scaleMLLMsandtheefficientmultimodal
LLMsontext-onlyandmultimodaldatasets. Weevaluatetheopen-source,efficient,andprivateAPI
MLLMs. Weselect18representativeevaluationdatasets. C*representstheCMMLUdataset.
stage. TheLLMbranchadaptationisincorporatedduringthesecondinstructiontuningstage. We
trainfor1epochwiththeAdamWoptimizerandtheCosinelearningschedule. Typically,thelearning
rates for the first and second stages are set at 1e−3 and 2e−6 (with the projector part as 1e−5),
respectively. ForWINGSbase,approximately1mtrainingdatatoalignimagelearnersandabout0.6m
supervisedfine-tuninginstructionsforthenextstage(thesameasLLaVA v1.5[79]). IntheWINGSpro,
weusethesamealigneddataandapproximately2mtrainingdataforlearningimage-textlearners.
ThesetwotypesofMLLMrequireabout1.5and6daysoftrainingon8×A100GPUs,respectively.
ThetrainingdatasetsforWINGSminiareconsistentwiththeWINGSpro. Ittakesapproximately5days
torunon4×A100GPUs.
4.1 TowardComprehensiveText-onlyandMultimodalPerformance
Text-onlyComparisoninFairDataandResourceEnvironments. AsshowninTable1,“Vicuna-
v1.5+CLIP”correspondstoLLaVA v1.5,and“Qwen1.5+SigLIP”servesasthefoundationforWINGS.
WhencomparingLLMitselfandtherestofMLLMs,weobservethatfine-tuningwithmultimodal
instructions,comparedtothe“QwenLLM”,thereistext-onlyforgettingin12outof16datasets,with
notabledecreasesofupto9.70,8.91,and13.33inMMLU,CMMLU,andRACE-High,respectively.
WINGSsignificantlyimproveperformanceondatasetssuchasMMLU,CMMLU,RACE-High,and
WSC,despitethepotentialforseveretext-onlyforgettingonbaselines. Additionally,wefindthat
theforgettingeffectsofCLIPandSigLIParesimilar. Incontrast,parameter-efficientfine-tuning
methodslikeLoRAresultinlesstext-onlyforgettingbutunderperformonmultimodalquestions.
Overall, WINGS’ visual and textual learners are credibly demonstrated to retain performance on
text-onlytaskswhilealsoperformingwellonvisual-relatedquestions. IndatasetslikeCHID,OCNLI,
andSIQA,MLLMsshowimprovedtext-onlyperformanceduetoincreasedlanguagediversity(e.g.,
Chinesecontext)orsemanticsimilarityintheirfine-tuningdata.
General Evaluation in Text-Only and Multimodal Tasks. We present the performance of 9,
roughly8Bopen-sourceMLLMs, 2roughly2B,and2privateAPIonesevaluatedinthegeneral
text-onlyandmultimodaltasks. Table2showsthatWINGSseriescanperformbetterontext-only
andmultimodalquestion-answeringdatasets. Itachievesstate-of-the-artperformanceon13outof
18datasets,significantlysurpassingLLaVA v1.5withthesamearchitecture. WefindthatWINGSis
875 75 75
70 67.7
70.3
68.4 70
6767.771. 68 97 .60.370 6.6
56 .98.4 70
67.167.769 6.9 87 .20.3
66.268.4
65 61.9 62.5 63.4 65 64.2 65 64.2 63.6
60 60 60
55 05 55 55.7 53.8 49 475 .41.8 485 .20.1 55 05 485 .40.1 48 485 .715 .41.8 485 .53 5.8 0.1 55 05 47.349 4951.8 49.150.1
45 424 .95.2 42.5 43.3 45 45 43.4 44.5 45.7
40 40 40
(T) (T,T) (T,T,T)(T,T,V) (T,V) (V) (T) (T,T) (T,T,T)(T,T,V) (T,V) (V) (T) (T,T) (T,T,T)(T,T,V) (T,V) (V)
LLaVA LLaVA (Qwen-SigLIP) Wings (Ours) Smaller Uniform LR Larger Uniform LR Partitioned LR Only Visual Wings Only Textual Wings Wings (Ours)
(a)ComparisonofLLaVA-v1.5SeriesandWINGS (b)ComparisonofVariousLearningRateonWINGS (c)ComparisonofDifferentTrainingPartsofWINGS
Figure5: PerformancecomparisononthenewlyconstructedInterleavedImageandText(IIT)
BenchmarkoftheLLaVAseries,differentlearningrateandfine-tuningparts.Thehorizontalaxis
representsdifferentmultimodalquestionsettings. Thehorizontalaxisshowsdifferentmultimodal
setups, e.g., (T, T, I) represents a visual question after two text-only QAs. The three subfigures
representdifferentablationsettings,withthevioletcolorrepresentingourWINGS.
equallyeffectiveformoreefficientfoundations,asshowninthe“EfficientMultimodalLLMs”parts.
WINGScanstillcapturekeyelementsanddemonstrategoodscalabilityastheparameterincreases.
AlthoughWINGSbasedoesnotreceiveadditionaltrainingforthetext-onlycomponent,itisstillable
toachievecomparableperformance.
4.2 InterleavedImage-Text(IIT)Benchmark
To finely evaluate MLLMs, we construct a series of text-only and multimodal mixed multi-turn
conversations. WeextractinstructionsfromMMLU,CMMLU,OpenbookQA,HellaSwag,MMMU,
MMBench,SEED-Bench,andAI2Ddatasetswithsimilarsemanticsbychroma[39]. Wethenpolish
theconnectionbetweensomeinstructionsusingGPT-3.5Turbotomakethemclosertoreal-world
conversations. Wesetup6vision-contentconfigurations,categorizedbythemulti-turncontentas:
(T),(T,T),(T,T,T),(T,T,V),(T,V),and(V).Forinstance,(T,T,V)indicatestwoconsecutivetext-only
queriesfollowedbyavisualquestionrequiringaresponse.
4.3 AblationStudies
ReferencingFigure7,weaddressthreequestionstocomprehensivelyanalyseWINGS:
• Can WINGS sustain performance with interleaved evaluation? We find that part(a) highlights
WINGS surpassing LLaVA
v1.5
and the same-backbone as LLaVA
v1.5
(Qwen-SigLIP) for each
multi-turnsetting,especiallyintext-centricdialogues.
• HowdoWINGSfarewithdifferentlearningratesettings? Part(b)demonstratesthatusingalower
learningratemaintainsproficiencyintext-onlytasksbutfallsshortinmultimodalquestions,while
ahigherrateboostsmultimodalabilitiesbutnottext-only. Applyingahigherlearningratetothe
projectorandaloweronetotheothersachievestheoptimal.
• AreallcomponentsofWINGSequallyeffective? Inpart(c),weexaminethatincorporatingvisual
learnersaloneslightlypreservestext-onlyabilities,likelybyminimizingdisruptiontotheLLM,
butdiminishesperformanceonmultimodaltasks.
InthediverseIITbench,whichrangesfromtext-richtomultimodalcontexts,theeffectivenessof
WINGSisparticularlyevident. AsshowninFigure1,withinreal-worldapplications,textualcontent
offersinsightsforfollowingvisualtasks. WINGSexcelsinhandlingtext-onlytaskswhileimproving
performanceonvisual-relatedinstructions.
5 Conclusion
WeproposeWINGS,whichincludesvisualandtextuallearners,toalleviatetext-onlyforgetting. The
learneriscomposedofefficientLow-RankResidualAttention(LoRRA).Westartbyconsidering
theshiftedattentionweightsinMLLMand,inthefirststage,focusonlearningthevisuallearner.
Then,weco-trainthevisualandtextuallearnerswithroutingbasedontheshiftedattentionweights.
WINGSdemonstratesremarkableperformanceontext-only,visual-question-answering,andnewly
constructed Interleaved Image-Text (IIT) benchmarks. WINGS allows for maintaining text-only
performancewithlimitedresourcesandfurtherenhancesperformanceinwell-resourcedsettings.
9References
[1] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,
ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo:avisuallanguagemodelfor
few-shotlearning. NeurIPS,35:23716–23736,2022.
[2] KeivanAlizadeh,ImanMirzadeh,DmitryBelenko,KarenKhatamifard,MinsikCho,CarloC.DelMundo,
MohammadRastegari,andMehrdadFarajtabar. LLMinaflash:Efficientlargelanguagemodelinference
withlimitedmemory. CoRR,abs/2312.11514,2023.
[3] RezaYazdaniAminabadi,SamyamRajbhandari,AmmarAhmadAwan,ChengLi,DuLi,EltonZheng,
OlatunjiRuwase,ShadenSmith,MinjiaZhang,JeffRasley,andYuxiongHe. Deepspeed-inference:
Enablingefficientinferenceoftransformermodelsatunprecedentedscale. InSC22,pages46:1–46:15,
2022.
[4] Anthropic. Introducing Claude, 2023. URL https://www.anthropic.com/index/
introducing-claude.
[5] JacobAustin,AugustusOdena,MaxwellNye,MaartenBosma,HenrykMichalewski,DavidDohan,
EllenJiang,CarrieCai,MichaelTerry,QuocLe,etal. Programsynthesiswithlargelanguagemodels.
arXivpreprintarXiv:2108.07732,2021.
[6] AnasAwadalla,IrenaGao,JoshGardner,JackHessel,YusufHanafy,WanrongZhu,KalyaniMarathe,
YonatanBitton,SamirYitzhakGadre,ShioriSagawa,JeniaJitsev,SimonKornblith,PangWeiKoh,
GabrielIlharco,MitchellWortsman,andLudwigSchmidt. Openflamingo:Anopen-sourceframework
fortraininglargeautoregressivevision-languagemodels. CoRR,abs/2308.01390,2023.
[7] JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,YangFan,WenbinGe,YuHan,
FeiHuang,etal. Qwentechnicalreport. arXivpreprintarXiv:2309.16609,2023.
[8] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,ChangZhou,and
JingrenZhou. Qwen-vl:Afrontierlargevision-languagemodelwithversatileabilities. arXivpreprint
arXiv:2308.12966,2023.
[9] PratyayBanerjee,KuntalKumarPal,ArindamMitra,andChittaBaral. Carefulselectionofknowledge
to solve open book question answering. In ACL, pages 6120–6129. Association for Computational
Linguistics,2019. doi:10.18653/V1/P19-1615. URLhttps://doi.org/10.18653/v1/p19-1615.
[10] YonatanBisk,RowanZellers,RonanLeBras,JianfengGao,andYejinChoi. PIQA:reasoningabout
physicalcommonsenseinnaturallanguage. InAAAI,pages7432–7439,2020.
[11] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,Arvind
Neelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsarefew-shotlearners.
NeurIPS,33:1877–1901,2020.
[12] JunbumCha,WooyoungKang,JonghwanMun,andByungseokRoh. Honeybee: Locality-enhanced
projectorformultimodalllm. arXivpreprintarXiv:2312.06742,2023.
[13] YingshanChang,GuihongCao,MriduNarang,JianfengGao,HisamiSuzuki,andYonatanBisk. Webqa:
MultihopandmultimodalQA. InCVPR,pages16474–16483.IEEE,2022.
[14] GuimingHardyChen,ShunianChen,RuifeiZhang,JunyingChen,XiangboWu,ZhiyiZhang,Zhihong
Chen,JianquanLi,XiangWan,andBenyouWang. Allava:Harnessinggpt4v-synthesizeddataforalite
vision-languagemodel. arXivpreprintarXiv:2402.11684,2024.
[15] LinChen,JisongLi,XiaoyiDong,PanZhang,ConghuiHe,JiaqiWang,FengZhao,andDahuaLin.
Sharegpt4v:Improvinglargemulti-modalmodelswithbettercaptions. arXivpreprintarXiv:2311.12793,
2023.
[16] LinChen,JinsongLi,XiaoyiDong,PanZhang,YuhangZang,ZehuiChen,HaodongDuan,JiaqiWang,
YuQiao, DahuaLin, andFengZhao. Areweontherightwayforevaluatinglargevision-language
models? arXivpreprintarXiv:2403.20330,2024.
[17] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and
C.LawrenceZitnick. Microsoftcococaptions:Datacollectionandevaluationserver,2015.
[18] YunkaiChen,QimengWang,ShiweiWu,YanGao,TongXu,andYaoHu. Tomgpt:Reliabletext-only
trainingapproachforcost-effectivemulti-modallargelanguagemodel. ACMTrans.Knowl.Discov.Data,
2024.
10[19] ZheChen,JiannanWu,WenhaiWang,WeijieSu,GuoChen,SenXing,ZhongMuyan,QinglongZhang,
XizhouZhu,LeweiLu,etal. Internvl: Scalingupvisionfoundationmodelsandaligningforgeneric
visual-linguistictasks. arXivpreprintarXiv:2312.14238,2023.
[20] ZheChen,WeiyunWang,HaoTian,ShenglongYe,ZhangweiGao,ErfeiCui,WenwenTong,Kongzhi
Hu,JiapengLuo,ZhengMa,etal. Howfararewetogpt-4v?closingthegaptocommercialmultimodal
modelswithopen-sourcesuites. arXivpreprintarXiv:2404.16821,2024.
[21] Po-HanChi,Pei-HungChung,Tsung-HanWu,Chun-ChengHsieh,Yen-HaoChen,Shang-WenLi,and
Hung-yiLee. Audioalbert:Alitebertforself-supervisedlearningofaudiorepresentation. InIEEESLT,
pages344–350,2021.
[22] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,Siyuan
Zhuang,YonghaoZhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing. Vicuna:Anopen-source
chatbotimpressinggpt-4with90%*chatgptquality,March2023. URLhttps://lmsys.org/blog/
2023-03-30-vicuna/.
[23] AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,
PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal. Palm:Scalinglanguage
modelingwithpathways. J.Mach.Learn.Res.,24:240:1–240:113,2023.
[24] PaulF.Christiano,JanLeike,TomB.Brown,MiljanMartic,ShaneLegg,andDarioAmodei. Deep
reinforcementlearningfromhumanpreferences. InNeurIPS,pages4299–4307,2017.
[25] XiangxiangChu,LimengQiao,XinyangLin,ShuangXu,YangYang,YimingHu,FeiWei,XinyuZhang,
BoZhang,XiaolinWei,etal. Mobilevlm:Afast,reproducibleandstrongvisionlanguageassistantfor
mobiledevices. arXivpreprintarXiv:2312.16886,2023.
[26] XiangxiangChu,LimengQiao,XinyuZhang,ShuangXu,FeiWei,YangYang,XiaofeiSun,YimingHu,
XinyangLin,BoZhang,etal. Mobilevlmv2:Fasterandstrongerbaselineforvisionlanguagemodel.
arXivpreprintarXiv:2402.03766,2024.
[27] HyungWonChung,LeHou,ShayneLongpre,BarretZoph,YiTay,WilliamFedus,EricLi,XuezhiWang,
MostafaDehghani,SiddharthaBrahma,etal. Scalinginstruction-finetunedlanguagemodels. CoRR,
abs/2210.11416,2022.
[28] PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,andOyvind
Tafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge. CoRR,
abs/1803.05457,2018.
[29] KarlCobbe,VineetKosaraju,MohammadBavarian,MarkChen,HeewooJun,LukaszKaiser,Matthias
Plappert,JerryTworek,JacobHilton,ReiichiroNakano,etal. Trainingverifierstosolvemathword
problems. arXivpreprintarXiv:2110.14168,2021.
[30] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuatTiong,JunqiZhao,WeishengWang,Boyang
Li,PascaleNFung,andStevenHoi. Instructblip:Towardsgeneral-purposevision-languagemodelswith
instructiontuning. NeurIPS,36,2024.
[31] NingDing,YulinChen,BokaiXu,YujiaQin,ZhiZheng,ShengdingHu,ZhiyuanLiu,MaosongSun,
andBowenZhou. Enhancingchatlanguagemodelsbyscalinghigh-qualityinstructionalconversations.
arXivpreprintarXiv:2305.14233,2023.
[32] QingxiuDong,LeiLi,DamaiDai,CeZheng,ZhiyongWu,BaobaoChang,XuSun,JingjingXu,LeiLi,
andZhifangSui. Asurveyforin-contextlearning. CoRR,abs/2301.00234,2023.
[33] XiaoyiDong,PanZhang,YuhangZang,YuhangCao,BinWang,LinkeOuyang,XilinWei,Songyang
Zhang,HaodongDuan,MaosongCao,etal. Internlm-xcomposer2: Masteringfree-formtext-image
compositionandcomprehensioninvision-languagelargemodel. arXivpreprintarXiv:2401.16420,2024.
[34] ZhengxiaoDu,YujieQian,XiaoLiu,MingDing,JiezhongQiu,ZhilinYang,andJieTang. Glm:General
languagemodelpretrainingwithautoregressiveblankinfilling. InACL,pages320–335,2022.
[35] HaoyiDuan,YanXia,MingzeZhou,LiTang,JiemingZhu,andZhouZhao. Cross-modalprompts:
Adaptinglargepre-trainedmodelsforaudio-visualdownstreamtasks. InNeurIPS,2023.
[36] WenfengFeng,ChuzhanHao,YueweiZhang,YuHan,andHaoWang. Mixture-of-loras:Anefficient
multitasktuningforlargelanguagemodels. CoRR,abs/2403.03432,2024.
11[37] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,ZhenyuQiu,WeiLin,
JinruiYang,XiawuZheng,etal. Mme:Acomprehensiveevaluationbenchmarkformultimodallarge
languagemodels. arXivpreprintarXiv:2306.13394,2023.
[38] ShashankGoel,HritikBansal,SumitBhatia,RyanA.Rossi,VishwaVinay,andAdityaGrover. Cyclip:
Cycliccontrastivelanguage-imagepretraining. InNeurIPS,2022.
[39] ChromaGroup. Chroma-theopen-sourceembeddingdatabase. https://github.com/chroma-core/chroma,
2017.
[40] TianruiGuan,FuxiaoLiu,XiyangWu,RuiqiXian,ZongxiaLi,XiaoyuLiu,XijunWang,LichangChen,
FurongHuang,YaserYacoob,etal.Hallusionbench:Anadvanceddiagnosticsuiteforentangledlanguage
hallucination&visualillusioninlargevision-languagemodels. arXivpreprintarXiv:2310.14566,2023.
[41] BoHe,HengduoLi,YoungKyunJang,MenglinJia,XuefeiCao,AshishShah,AbhinavShrivastava,and
Ser-NamLim. Ma-lmm:Memory-augmentedlargemultimodalmodelforlong-termvideounderstanding.
InCVPR,2024.
[42] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob
Steinhardt. Measuringmassivemultitasklanguageunderstanding. arXivpreprintarXiv:2009.03300,
2020.
[43] WenyiHong,WeihanWang,QingsongLv,JiazhengXu,WenmengYu,JunhuiJi,YanWang,Zihan
Wang,YuxiaoDong,MingDing,etal. Cogagent:Avisuallanguagemodelforguiagents. arXivpreprint
arXiv:2312.08914,2023.
[44] AnwenHu,YayaShi,HaiyangXu,JiaboYe,QinghaoYe,MingYan,ChenliangLi,QiQian,JiZhang,
andFeiHuang. mplug-paperowl:Scientificdiagramanalysiswiththemultimodallargelanguagemodel.
arXivpreprintarXiv:2311.18248,2023.
[45] EdwardJ.Hu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,and
WeizhuChen. Lora:Low-rankadaptationoflargelanguagemodels. InICLR,2022.
[46] HaiHu,KyleRichardson,LiangXu,LuLi,SandraKübler,andLawrenceS.Moss. OCNLI:original
chinese natural language inference. In EMNLP, volume EMNLP 2020 of Findings of ACL, pages
3512–3526,2020.
[47] ShengdingHu,YugeTu,XuHan,ChaoqunHe,GanquCui,XiangLong,ZhiZheng,YeweiFang,Yuxiang
Huang,WeilinZhao,etal. Minicpm: Unveilingthepotentialofsmalllanguagemodelswithscalable
trainingstrategies. arXivpreprintarXiv:2404.06395,2024.
[48] JieHuangandKevinChen-ChuanChang. Towardsreasoninginlargelanguagemodels:Asurvey. In
ACL,2023.
[49] IDEFICS. Introducingidefics:Anopenreproductionofstate-of-the-artvisuallanguagemodel. https:
//huggingface.co/blog/idefics,2023.
[50] JiteshJain,JianweiYang,andHumphreyShi. Vcoder:Versatilevisionencodersformultimodallarge
languagemodels. CoRR,abs/2312.14233,2023.
[51] YunjieJi,YongDeng,YanGong,YipingPeng,QiangNiu,LeiZhang,BaochangMa,andXiangang
Li. Exploringtheimpactofinstructiondatascalingonlargelanguagemodels:Anempiricalstudyon
real-worldusecases. arXivpreprintarXiv:2303.14742,2023.
[52] AlbertQ.Jiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,DevendraSinghChaplot,Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard
Lavaud,Marie-AnneLachaux,PierreStock,TevenLeScao,ThibautLavril,ThomasWang,Timothée
Lacroix,andWilliamElSayed. Mistral7b,2023.
[53] AlbertQJiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,BlancheSavary,ChrisBamford,
DevendraSinghChaplot,DiegodelasCasas,EmmaBouHanna,FlorianBressand,etal. Mixtralof
experts. arXivpreprintarXiv:2401.04088,2024.
[54] JaredKaplan,SamMcCandlish,TomHenighan,TomB.Brown,BenjaminChess,RewonChild,Scott
Gray,AlecRadford,JeffreyWu,andDarioAmodei. Scalinglawsforneurallanguagemodels. CoRR,
abs/2001.08361,2020.
[55] AniruddhaKembhavi,MikeSalvato,EricKolve,MinjoonSeo,HannanehHajishirzi,andAliFarhadi. A
diagramisworthadozenimages. InECCV,pages235–251,2016.
12[56] JacobDevlinMing-WeiChangKentonandLeeKristinaToutanova. Bert:Pre-trainingofdeepbidirec-
tionaltransformersforlanguageunderstanding. InNAACL,pages4171–4186,2019.
[57] GeewookKim,TeakgyuHong,MoonbinYim,JeongYeonNam,JinyoungPark,JinyeongYim,Won-
seokHwang,SangdooYun,DongyoonHan,andSeunghyunPark. Ocr-freedocumentunderstanding
transformer. InECCV,2022.
[58] GuokunLai,QizheXie,HanxiaoLiu,YimingYang,andEduardH.Hovy. RACE:large-scalereading
comprehensiondatasetfromexaminations. InEMNLP,pages785–794,2017.
[59] Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov,
ThomasWang,SiddharthKaramcheti,AlexanderM.Rush,DouweKiela,MatthieuCord,andVictor
Sanh. Obelics:Anopenweb-scalefiltereddatasetofinterleavedimage-textdocuments,2023.
[60] BoLi,PeiyuanZhang,JingkangYang,YuanhanZhang,FanyiPu,andZiweiLiu. Otterhd: Ahigh-
resolutionmulti-modalitymodel. arXivpreprintarXiv:2311.04219,2023.
[61] BoLi,YuanhanZhang,LiangyuChen,JinghaoWang,JingkangYang,andZiweiLiu. Otter:Amulti-
modalmodelwithin-contextinstructiontuning. arXivpreprintarXiv:2305.03726,2023.
[62] BohaoLi,RuiWang,GuangzhiWang,YuyingGe,YixiaoGe,andYingShan.Seed-bench:Benchmarking
multimodalllmswithgenerativecomprehension. arXivpreprintarXiv:2307.16125,2023.
[63] HaonanLi,YixuanZhang,FajriKoto,YifeiYang,HaiZhao,YeyunGong,NanDuan,andTimothy
Baldwin. CMMLU:MeasuringmassivemultitasklanguageunderstandinginChinese. arXivpreprint
arXiv:2306.09212,2023.
[64] JiachenLi,XinyaoWang,SijieZhu,Chia-wenKuo,LuXu,FanChen,JiteshJain,HumphreyShi,and
LongyinWen. Cumo:Scalingmultimodalllmwithco-upcycledmixture-of-experts. arXiv:,2024.
[65] JunnanLi,RamprasaathR.Selvaraju,AkhileshGotmare,ShafiqR.Joty,CaimingXiong,andStevenChu-
HongHoi. Alignbeforefuse:Visionandlanguagerepresentationlearningwithmomentumdistillation.
InNeurIPS,pages9694–9705,2021.
[66] JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip:Bootstrappinglanguage-imagepre-training
forunifiedvision-languageunderstandingandgeneration. InICML,pages12888–12900,2022.
[67] JunnanLi,SilvioSavarese,andStevenC.H.Hoi. Maskedunsupervisedself-trainingforzero-shotimage
classification. CoRR,abs/2206.02967,2022.
[68] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-2: Bootstrappinglanguage-imagepre-
trainingwithfrozenimageencodersandlargelanguagemodels. InICML,pages19730–19742.PMLR,
2023.
[69] LeiLi,YuqiWang,RunxinXu,PeiyiWang,XiachongFeng,LingpengKong,andQiLiu. Multimodal
arxiv: A dataset for improving scientific comprehension of large vision-language models. CoRR,
abs/2403.00231,2024.
[70] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In
ACL/IJCNLP,pages4582–4597,2021.
[71] YanweiLi,YuechenZhang,ChengyaoWang,ZhishengZhong,YixinChen,RuihangChu,Shaoteng
Liu,andJiayaJia. Mini-gemini:Miningthepotentialofmulti-modalityvisionlanguagemodels. arXiv
preprintarXiv:2403.18814,2024.
[72] ZhangLi,BiaoYang,QiangLiu,ZhiyinMa,ShuoZhang,JingxuYang,YaboSun,YuliangLiu,and
XiangBai. Monkey:Imageresolutionandtextlabelareimportantthingsforlargemulti-modalmodels.
arXivpreprintarXiv:2311.06607,2023.
[73] WeixinLiang,YuhuiZhang,YongchanKwon,SerenaYeung,andJamesY.Zou. Mindthegap:Under-
standingthemodalitygapinmulti-modalcontrastiverepresentationlearning. InNeurIPS,2022.
[74] BinLin,BinZhu,YangYe,MunanNing,PengJin,andLiYuan. Video-llava:Learningunitedvisual
representationbyalignmentbeforeprojection. arXivpreprintarXiv:2311.10122,2023.
[75] BinLin,TaoPeng,ChenZhang,MinminSun,LanboLi,HanyuZhao,WencongXiao,QiXu,XiafeiQiu,
ShenLi,ZhigangJi,YongLi,andWeiLin. Infinite-llm: EfficientLLMserviceforlongcontextwith
distattentionanddistributedkvcache. CoRR,abs/2401.02669,2024.
13[76] BinLin,ZhenyuTang,YangYe,JiaxiCui,BinZhu,PengJin,JunwuZhang,MunanNing,andLiYuan.
Moe-llava:Mixtureofexpertsforlargevision-languagemodels. arXivpreprintarXiv:2401.15947,2024.
[77] JiLin,HongxuYin,WeiPing,YaoLu,PavloMolchanov,AndrewTao,HuiziMao,JanKautz,Mohammad
Shoeybi,andSongHan. VILA:onpre-trainingforvisuallanguagemodels. CoRR,abs/2312.07533,
2023.
[78] ZiyiLin,ChrisLiu,RenruiZhang,PengGao,LongtianQiu,HanXiao,HanQiu,ChenLin,WenqiShao,
KeqinChen,etal. Sphinx:Thejointmixingofweights,tasks,andvisualembeddingsformulti-modal
largelanguagemodels. arXivpreprintarXiv:2311.07575,2023.
[79] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. NeurIPS,36,
2023.
[80] HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,ShengShen,andYongJaeLee.Llava-next:
Improvedreasoning,ocr,andworldknowledge,January2024. URLhttps://llava-vl.github.io/
blog/2024-01-30-llava-next/.
[81] ShilongLiu,HaoCheng,HaotianLiu,HaoZhang,FengLi,TianheRen,XueyanZou,JianweiYang,
HangSu,JunZhu,LeiZhang,JianfengGao,andChunyuanLi. Llava-plus: Learningtousetoolsfor
creatingmultimodalagents. arXiv:2311.05437,2023.
[82] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,OmerLevy,MikeLewis,
LukeZettlemoyer,andVeselinStoyanov. Roberta:Arobustlyoptimizedbertpretrainingapproach. arXiv
preprintarXiv:1907.11692,2019.
[83] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhang,WangboZhao,YikeYuan,Jiaqi
Wang,ConghuiHe,ZiweiLiu,etal. Mmbench:Isyourmulti-modalmodelanall-aroundplayer? arXiv
preprintarXiv:2307.06281,2023.
[84] YuliangLiu,BiaoYang,QiangLiu,ZhangLi,ZhiyinMa,ShuoZhang,andXiangBai. Textmonkey:An
ocr-freelargemultimodalmodelforunderstandingdocument. arXivpreprintarXiv:2403.04473,2024.
[85] HaoyuLu,WenLiu,BoZhang,BingxuanWang,KaiDong,BoLiu,JingxiangSun,TongzhengRen,
ZhuoshuLi,YaofengSun,etal. Deepseek-vl:Towardsreal-worldvision-languageunderstanding. arXiv
preprintarXiv:2403.05525,2024.
[86] PanLu,SwaroopMishra,TanglinXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,OyvindTafjord,
PeterClark,andAshwinKalyan. Learntoexplain:Multimodalreasoningviathoughtchainsforscience
questionanswering. NeurIPS,35:2507–2521,2022.
[87] BrandonMcKinzie,ZheGan,Jean-PhilippeFauconnier,SamDodge,BowenZhang,PhilippDufter,
DhrutiShah,XianzhiDu,FutangPeng,FlorisWeers,etal. Mm1: Methods,analysis&insightsfrom
multimodalllmpre-training. arXivpreprintarXiv:2403.09611,2024.
[88] ShervinMinaee,TomásMikolov,NarjesNikzad,MeysamChenaghlu,RichardSocher,XavierAmatriain,
andJianfengGao. Largelanguagemodels:Asurvey. CoRR,abs/2402.06196,2024.
[89] CuongVNguyen,TalHassner,CedricArchambeau,andMatthiasSeeger. Leep: Anewmeasureto
evaluatetransferabilityoflearnedrepresentations. InICML,2020.
[90] OpenAI. Chatgpt. https://openai.com/blog/chatgpt,2022.
[91] OpenAI. Gpt-4v(ision)systemcard. https://cdn.openai.com/papers/GPTV_System_Card.pdf,
2023.
[92] OpenAI. GPT-4technicalreport. CoRR,abs/2303.08774,2023.
[93] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollL.Wainwright,PamelaMishkin,Chong
Zhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelstofollowinstruc-
tionswithhumanfeedback. InNeurIPS,2022.
[94] BaolinPeng,ChunyuanLi,PengchengHe,MichelGalley,andJianfengGao. Instructiontuningwith
gpt-4. arXivpreprintarXiv:2304.03277,2023.
[95] EthanPerez,FlorianStrub,HarmdeVries,VincentDumoulin,andAaronC.Courville. Film: Visual
reasoningwithageneralconditioninglayer. InAAAI,pages3942–3951,2018.
[96] QiQian,YuanhongXu,andJuhuaHu. Intra-modalproxylearningforzero-shotvisualcategorization
withCLIP. InNeurIPS,2023.
14[97] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InICML,pages8748–8763,2021.
[98] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,
WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.
TheJ.Mach.Learn.Res.,21(1):5485–5551,2020.
[99] SamyamRajbhandari, OlatunjiRuwase, JeffRasley, ShadenSmith, andYuxiongHe. Zero-infinity:
breaking the GPU memory wall for extreme scale deep learning. In International Conference for
HighPerformanceComputing,Networking,StorageandAnalysis,SC2021,St.Louis,Missouri,USA,
November14-19,2021,page59.ACM,2021.
[100] MachelReid,NikolaySavinov,DenisTeplyashin,DmitryLepikhin,TimothyLillicrap,Jean-baptiste
Alayrac,RaduSoricut,AngelikiLazaridou,OrhanFirat,JulianSchrittwieser,etal.Gemini1.5:Unlocking
multimodalunderstandingacrossmillionsoftokensofcontext. arXivpreprintarXiv:2403.05530,2024.
[101] ShuhuaiRen,LinliYao,ShichengLi,XuSun,andLuHou. Timechat:Atime-sensitivemultimodallarge
languagemodelforlongvideounderstanding. ArXiv,abs/2312.02051,2023.
[102] KeisukeSakaguchi,RonanLeBras,ChandraBhagavatula,andYejinChoi. Winogrande:Anadversarial
winogradschemachallengeatscale,2019.
[103] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, An-
toineChaffin,ArnaudStiegler,ArunRaja,etal. Multitaskpromptedtrainingenableszero-shottask
generalization. InICLR,2022.
[104] MaartenSap,HannahRashkin,DerekChen,RonanLeBras,andYejinChoi. SocialIQA:Commonsense
reasoningaboutsocialinteractions. CoRR,abs/1904.09728,2019.
[105] NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton,andJeff
Dean. Outrageouslylargeneuralnetworks:Thesparsely-gatedmixture-of-expertslayer,2017.
[106] StepFun Research Team. Step-1v: A hundred billion parameter multimodal large model. https:
//platform.stepfun.com,2024.
[107] NisanStiennon,LongOuyang,JeffreyWu,DanielM.Ziegler,RyanLowe,ChelseaVoss,AlecRadford,
DarioAmodei, andPaulF.Christiano. Learningtosummarizewithhumanfeedback. InAdvances
inNeuralInformationProcessingSystems33: AnnualConferenceonNeuralInformationProcessing
Systems2020,NeurIPS2020,December6-12,2020,virtual,2020.
[108] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,CarlosGuestrin,PercyLiang,
andTatsunoriB.Hashimoto. Stanfordalpaca:Aninstruction-followingllamamodel. https://github.
com/tatsu-lab/stanford_alpaca,2023.
[109] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,CarlosGuestrin,PercyLiang,
andTatsunoriBHashimoto. Alpaca:Astrong,replicableinstruction-followingmodel. StanfordCenter
forResearchonFoundationModels.https://crfm.stanford.edu/2023/03/13/alpaca.html,3(6):7,2023.
[110] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,Radu
Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable
multimodalmodels. arXivpreprintarXiv:2312.11805,2023.
[111] TheMosaicResearchTeam. Introducingdbrx: Anewstate-of-the-artopenllm,March2024. URL
https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm.
[112] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama:Openandefficientfoundation
languagemodels. arXivpreprintarXiv:2302.13971,2023.
[113] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfoundationand
fine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
[114] MariaTsimpoukelli,JacobLMenick,SerkanCabi,SMEslami,OriolVinyals,andFelixHill.Multimodal
few-shotlearningwithfrozenlanguagemodels. AdvancesinNeuralInformationProcessingSystems,34:
200–212,2021.
15[115] Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi. Dylora: Parameter effi-
cient tuning of pre-trained models using dynamic search-free low-rank adaptation. arXiv preprint
arXiv:2210.07558,2022.
[116] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,Lukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. InNeurIPS,2017.
[117] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. NeurIPS,30,2017.
[118] WeihanWang,QingsongLv,WenmengYu,WenyiHong,JiQi,YanWang,JunhuiJi,ZhuoyiYang,
LeiZhao,XixuanSong,etal. Cogvlm: Visualexpertforpretrainedlanguagemodels. arXivpreprint
arXiv:2311.03079,2023.
[119] HaoranWei,LingyuKong,JinyueChen,LiangZhao,ZhengGe,JinrongYang,JianjianSun,Chunrui
Han,andXiangyuZhang.Vary:Scalingupthevisionvocabularyforlargevision-languagemodels.arXiv
preprintarXiv:2312.06109,2023.
[120] JasonWei,MaartenBosma,VincentZhao,KelvinGuu,AdamsWeiYu,BrianLester,NanDu,AndrewM.
Dai,andQuocVLe. Finetunedlanguagemodelsarezero-shotlearners. InICLR,2022.
[121] JasonWei,YiTay,RishiBommasani,ColinRaffel,BarretZoph,SebastianBorgeaud,DaniYogatama,
MaartenBosma,DennyZhou,DonaldMetzler,EdH.Chi,TatsunoriHashimoto,OriolVinyals,Percy
Liang,JeffDean,andWilliamFedus. Emergentabilitiesoflargelanguagemodels. Trans.Mach.Learn.
Res.,2022,2022.
[122] ChenfeiWu, ShengmingYin, WeizhenQi, XiaodongWang, ZechengTang, andNanDuan. Visual
chatgpt:Talking,drawingandeditingwithvisualfoundationmodels. arXivpreprintarXiv:2303.04671,
2023.
[123] ShengqiongWu,HaoFei,LeigangQu,WeiJi,andTat-SengChua. Next-gpt:Any-to-anymultimodalllm.
arXivpreprintarXiv:2309.05519,2023.
[124] LiangXu,HaiHu,XuanweiZhang,LuLi,ChenjieCao,YudongLi,YechenXu,KaiSun,DianYu,Cong
Yu,YinTian,QianqianDong,WeitangLiu,BoShi,YimingCui,JunyiLi,JunZeng,RongzhaoWang,
WeijianXie,YantingLi,YinaPatterson,ZuoyuTian,YiwenZhang,HeZhou,ShaoweihuaLiu,ZheZhao,
QipengZhao,CongYue,XinruiZhang,ZhengliangYang,KyleRichardson,andZhenzhongLan. CLUE:
Achineselanguageunderstandingevaluationbenchmark. InCOLING,pages4762–4772,2020.
[125] RuyiXu,YuanYao,ZonghaoGuo,JunboCui,ZanlinNi,ChunjiangGe,Tat-SengChua,ZhiyuanLiu,
MaosongSun,andGaoHuang. Llava-uhd: anlmmperceivinganyaspectratioandhigh-resolution
images. arXivpreprintarXiv:2403.11703,2024.
[126] ZhengyuanYang,LinjieLi,JianfengWang,KevinLin,EhsanAzarnasab,FaisalAhmed,ZichengLiu,
CeLiu,MichaelZeng,andLijuanWang. Mm-react:Promptingchatgptformultimodalreasoningand
action. arXivpreprintarXiv:2303.11381,2023.
[127] ZhilinYang,ZihangDai,YimingYang,JaimeCarbonell,RussRSalakhutdinov,andQuocVLe. Xlnet:
Generalizedautoregressivepretrainingforlanguageunderstanding. InNeurIPS,2019.
[128] ChaoYi,De-ChuanZhan,andHan-JiaYe. Bridgethemodalityandcapacitygapsinvision-language
modelselection. CoRR,abs/2403.13797,2024.
[129] AlexYoung,BeiChen,ChaoLi,ChengenHuang,GeZhang,GuanweiZhang,HengLi,JiangchengZhu,
JianqunChen,JingChang,etal.Yi:Openfoundationmodelsby01.ai.arXivpreprintarXiv:2403.04652,
2024.
[130] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu,
XuedongHuang,BoxinLi,ChunyuanLi,etal. Florence:Anewfoundationmodelforcomputervision.
arXivpreprintarXiv:2111.11432,2021.
[131] XiangYue,YuanshengNi,KaiZhang,TianyuZheng,RuoqiLiu,GeZhang,SamuelStevens,Dongfu
Jiang,WeimingRen,YuxuanSun,etal. Mmmu:Amassivemulti-disciplinemultimodalunderstanding
andreasoningbenchmarkforexpertagi. arXivpreprintarXiv:2311.16502,2023.
[132] RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,andYejinChoi. HellaSwag:Canamachine
reallyfinishyoursentence? InACL,pages4791–4800,2019.
16[133] AndyZeng,MariaAttarian,BrianIchter,KrzysztofChoromanski,AdrianWong,StefanWelker,Federico
Tombari,AveekPurohit,MichaelRyoo,VikasSindhwani,etal. Socraticmodels:Composingzero-shot
multimodalreasoningwithlanguage. arXivpreprintarXiv:2204.00598,2022.
[134] XiaohuaZhai,BasilMustafa,AlexanderKolesnikov,andLucasBeyer. Sigmoidlossforlanguageimage
pre-training. InICCV,pages11975–11986,2023.
[135] JunZhan,JunqiDai,JiashengYe,YunhuaZhou,DongZhang,ZhigengLiu,XinZhang,RuibinYuan,
GeZhang,LinyangLi,etal. Anygpt:Unifiedmultimodalllmwithdiscretesequencemodeling. arXiv
preprintarXiv:2402.12226,2024.
[136] SusanZhang,StephenRoller,NamanGoyal,MikelArtetxe,MoyaChen,ShuohuiChen,Christopher
Dewan,MonaDiab,XianLi,XiVictoriaLin,etal. Opt:Openpre-trainedtransformerlanguagemodels.
arXiv:2205.01068,2022.
[137] Yi-KaiZhang,Ting-JiHuang,Yao-XiangDing,De-ChuanZhan,andHan-JiaYe.Modelspider:Learning
torankpre-trainedmodelsefficiently. InNeurIPS,2023.
[138] ChujieZheng,MinlieHuang,andAixinSun. Chid:Alarge-scalechineseidiomdatasetforclozetest. In
ACL,pages778–787,2019.
[139] ChuntingZhou,PengfeiLiu,PuxinXu,SrinivasanIyer,JiaoSun,YuningMao,XuezheMa,AviaEfrat,
PingYu,LiliYu,SusanZhang,GargiGhosh,MikeLewis,LukeZettlemoyer,andOmerLevy. LIMA:
lessismoreforalignment. InNeurIPS,2023.
[140] BinZhu,BinLin,MunanNing,YangYan,JiaxiCui,HongfaWang,YatianPang,WenhaoJiang,Junwu
Zhang, Zongwei Li, Wancai Zhang, Zhifeng Li, Wei Liu, and Li Yuan. Languagebind: Extending
video-languagepretrainington-modalitybylanguage-basedsemanticalignment. CoRR,abs/2310.01852,
2023.
[141] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing
vision-languageunderstandingwithadvancedlargelanguagemodels. InICLR,2024.
[142] WanrongZhu,JackHessel,AnasAwadalla,SamirYitzhakGadre,JesseDodge,AlexFang,YoungjaeYu,
LudwigSchmidt,WilliamYangWang,andYejinChoi. MultimodalC4:anopen,billion-scalecorpusof
imagesinterleavedwithtext. InNeurIPS,2023.
[143] DanielM.Ziegler,NisanStiennon,JeffreyWu,TomB.Brown,AlecRadford,DarioAmodei,PaulF.Chris-
tiano,andGeoffreyIrving.Fine-tuninglanguagemodelsfromhumanpreferences.CoRR,abs/1909.08593,
2019.
17Supplementary Material
A ExperimentalSetupsandImplementationDetails
TrainingDatasets. ThetrainingdatasetsforthefirstandsecondstageofWINGSbaseareconsistent
withLLaVA v1.5[79]. Forthesecondstage,WINGSproextendsthetrainingdatasettoincludesome
visualQAdatasetsasALLaVA[14],SynthDog[57],andArXivQA[69],andtext-onlyQAdatasets
as Stanford Alpaca [108], Alpaca GPT-4 [94], LIMA [139], UltraChat [31], WebQA [13], and
BELLE-0.5M[51]. WINGS1.8BsharesthesametrainingsetasWINGSpro.
ModelStructures. WeemployQwen1.5[7]andSigLIP[134]asourfoundations.
TrainingHyperparameters. Weutilizeabatchsizeof32,alongwiththeAdamWoptimizeranda
cosineschedule. ForallWINGS-series,thelearningrateissetat1e−3forthefirststageandadjusts
to2e−6forthesecondstage,exceptfortheprojectoras1e−5.
TrainingEnvironment. WINGSbaseandWINGSproaretrainedoverapproximately1.5or6dayson
8×A100GPUs. WINGS1.8Brequireapproximately5daysoftrainingon4×A100GPUs.
B AdditionalExperimentalResults
75
70.5 70.3
70 67.7 70.1 68.4
66.1 65.6
65 63.8 63.1
60
55 51.8
50
45.3
49 46.8
45.2
47.5 46.550.1
45 43.7
40
(T) (T,T) (T,T,T) (T,T,V) (T,V) (V)
LoRA Prefix Wings (Ours)
(a)ComparisonofParameterEfficientModulesandWINGS
Figure6: PerformancecomparisononthenewlyconstructedInterleavedImageandText(IIT)
Benchmark of the Parameter Efficient Modules. The horizontal axis represents different mul-
timodal question settings. The horizontal axis shows different multimodal setups, e.g., (T, T, I)
representsavisualquestionaftertwotext-onlyQAs.
ShouldweonlyaddadditionalmodulesontopofanLLMbranchor,likeWINGS,createtwodistinct
learners for visual and textual modalities? We delve into the low-rank adaptation (LoRA) [45]
and Prefix-tuning [70] for minimally adapt to the LLM component. These techniques introduce
optimization parameters beyond the primary branch. These lightweight adjustments align with
extensivemodifications,effectivelyminimizingtext-onlyforgettingbutconcurrentlycurbingcross-
modalpositivetransfer.
C Discussion
WINGS is a universal plugin that can be integrated with any multimodal mixed-input MLLMs.
Notably,itintroducesanewconceptofcompetitivereuseamongmultipleexpertgroups: wemaynot
requiretheexpertstotheTransformerblock’sMLPlayeratascalethreeordersofmagnitudelarger;
instead,aminorupdateintheattentionforbetterallocationmaysuffice. Thisideaisalsofoundin
somevariantsofLoRA[36,115]. Inthefuture,wewillgraduallyexplorethefutureofMLLMs.
D Limitation&BroaderImpact
DespiteWINGS’strongadaptabilityforembeddingauxiliaryattentionlearnersinvariousMLLMs,
integratingvisuallearnersrequiresrestartingthefeaturealignmenttraining,incurringextracosts.
18Additionally,itsdeploymentonedgedevicesfaceslimitations,withWINGS1.8Bofferingasolutionat
theexpenseofperformance. Furthermore,WINGSstillrequiressometext-onlydatatoreplayand
enhanceoverallperformance,aimingforintegrationintomoregenericAIsystemsinthefuture.
Figure 7: Dynamics of Attention Weights from Shallow to Deep Layers. We calculate the
proportionofattentionweightsfortheimage-before(yellow),theimage-itself(red),andtheimage-
after(green)ineachlayer. Fromlefttoright,toptobottom,fromshallowtodeeplayers.
QA1: Text-only Instruction (from MMLU)
User:There is a single choice question about Sociology.
Answer the question by replying A, B, C or D.
Question: Which of the following did the post-war welfare
state of 1948 not aim to provide:
A. free health care and education for all
B. a minimum wage
C. full employment
D. universal welfare
QA2: Multimodal Instruction (from MMMU)
User:Sociologystudies <image> and governmental
relationships as.
Figure8: AnExampleofanInterleavedImage-TextBenchmark. Thisdialogueisrepresentedas
(T,V),consistingofatext-onlyQAfromMMLU[42]andavisualQAfromMMMU[131]. Itcanbe
observedthat,duetothesampling,bothincludequestionsfromtheSociologycategory.
19