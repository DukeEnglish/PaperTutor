[
    {
        "title": "Altogether: Image Captioning via Re-aligning Alt-text",
        "authors": "Hu XuPo-Yao HuangXiaoqing Ellen TanChing-Feng YehJacob KahnChristine JouGargi GhoshOmer LevyLuke ZettlemoyerWen-tau YihShang-Wen LiSaining XieChristoph Feichtenhofer",
        "links": "http://arxiv.org/abs/2410.17251v1",
        "entry_id": "http://arxiv.org/abs/2410.17251v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17251v1",
        "summary": "This paper focuses on creating synthetic data to improve the quality of image\ncaptions. Existing works typically have two shortcomings. First, they caption\nimages from scratch, ignoring existing alt-text metadata, and second, lack\ntransparency if the captioners' training data (e.g. GPT) is unknown. In this\npaper, we study a principled approach Altogether based on the key idea to edit\nand re-align existing alt-texts associated with the images. To generate\ntraining data, we perform human annotation where annotators start with the\nexisting alt-text and re-align it to the image content in multiple rounds,\nconsequently constructing captions with rich visual concepts. This differs from\nprior work that carries out human annotation as a one-time description task\nsolely based on images and annotator knowledge. We train a captioner on this\ndata that generalizes the process of re-aligning alt-texts at scale. Our\nresults show our Altogether approach leads to richer image captions that also\nimprove text-to-image generation and zero-shot image classification tasks.",
        "updated": "2024-10-22 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17251v1"
    },
    {
        "title": "JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation",
        "authors": "Shota OnoharaAtsuyuki MiyaiYuki ImajukuKazuki EgashiraJeonghun BaekXiang YueGraham NeubigKiyoharu Aizawa",
        "links": "http://arxiv.org/abs/2410.17250v1",
        "entry_id": "http://arxiv.org/abs/2410.17250v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17250v1",
        "summary": "Accelerating research on Large Multimodal Models (LMMs) in non-English\nlanguages is crucial for enhancing user experiences across broader populations.\nIn this paper, we introduce JMMMU (Japanese MMMU), the first large-scale\nJapanese benchmark designed to evaluate LMMs on expert-level tasks based on the\nJapanese cultural context. To facilitate comprehensive culture-aware\nevaluation, JMMMU features two complementary subsets: (i) culture-agnostic (CA)\nsubset, where the culture-independent subjects (e.g., Math) are selected and\ntranslated into Japanese, enabling one-to-one comparison with its English\ncounterpart MMMU; and (ii) culture-specific (CS) subset, comprising newly\ncrafted subjects that reflect Japanese cultural context. Using the CA subset,\nwe observe performance drop in many LMMs when evaluated in Japanese, which is\npurely attributable to language variation. Using the CS subset, we reveal their\ninadequate Japanese cultural understanding. Further, by combining both subsets,\nwe identify that some LMMs perform well on the CA subset but not on the CS\nsubset, exposing a shallow understanding of the Japanese language that lacks\ndepth in cultural understanding. We hope this work will not only help advance\nLMM performance in Japanese but also serve as a guideline to create\nhigh-standard, culturally diverse benchmarks for multilingual LMM development.\nThe project page is https://mmmu-japanese-benchmark.github.io/JMMMU/.",
        "updated": "2024-10-22 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17250v1"
    },
    {
        "title": "PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction",
        "authors": "Long XingQidong HuangXiaoyi DongJiajie LuPan ZhangYuhang ZangYuhang CaoConghui HeJiaqi WangFeng WuDahua Lin",
        "links": "http://arxiv.org/abs/2410.17247v1",
        "entry_id": "http://arxiv.org/abs/2410.17247v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17247v1",
        "summary": "In large vision-language models (LVLMs), images serve as inputs that carry a\nwealth of information. As the idiom \"A picture is worth a thousand words\"\nimplies, representing a single image in current LVLMs can require hundreds or\neven thousands of tokens. This results in significant computational costs,\nwhich grow quadratically as input image resolution increases, thereby severely\nimpacting the efficiency of both training and inference. Previous approaches\nhave attempted to reduce the number of image tokens either before or within the\nearly layers of LVLMs. However, these strategies inevitably result in the loss\nof crucial image information, ultimately diminishing model performance. To\naddress this challenge, we conduct an empirical study revealing that all visual\ntokens are necessary for LVLMs in the shallow layers, and token redundancy\nprogressively increases in the deeper layers of the model. To this end, we\npropose PyramidDrop, a visual redundancy reduction strategy for LVLMs to boost\ntheir efficiency in both training and inference with neglectable performance\nloss. Specifically, we partition the LVLM into several stages and drop part of\nthe image tokens at the end of each stage with a pre-defined ratio, creating\npyramid-like visual tokens across model layers. The dropping is based on a\nlightweight similarity calculation with a negligible time overhead. Extensive\nexperiments demonstrate that PyramidDrop can achieve a 40% training time and\n55% inference FLOPs acceleration of LLaVA-NeXT with comparable performance.\nBesides, the PyramidDrop could also serve as a plug-and-play strategy for\ninference acceleration without training, with better performance and lower\ninference cost than counterparts. We hope that the insights and approach\nintroduced by PyramidDrop will inspire future research to further investigate\nthe role of image tokens in LVLMs.",
        "updated": "2024-10-22 17:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17247v1"
    },
    {
        "title": "Towards Reliable Evaluation of Behavior Steering Interventions in LLMs",
        "authors": "Itamar PresLaura RuisEkdeep Singh LubanaDavid Krueger",
        "links": "http://arxiv.org/abs/2410.17245v1",
        "entry_id": "http://arxiv.org/abs/2410.17245v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17245v1",
        "summary": "Representation engineering methods have recently shown promise for enabling\nefficient steering of model behavior. However, evaluation pipelines for these\nmethods have primarily relied on subjective demonstrations, instead of\nquantitative, objective metrics. We aim to take a step towards addressing this\nissue by advocating for four properties missing from current evaluations: (i)\ncontexts sufficiently similar to downstream tasks should be used for assessing\nintervention quality; (ii) model likelihoods should be accounted for; (iii)\nevaluations should allow for standardized comparisons across different target\nbehaviors; and (iv) baseline comparisons should be offered. We introduce an\nevaluation pipeline grounded in these criteria, offering both a quantitative\nand visual analysis of how effectively a given method works. We use this\npipeline to evaluate two representation engineering methods on how effectively\nthey can steer behaviors such as truthfulness and corrigibility, finding that\nsome interventions are less effective than previously reported.",
        "updated": "2024-10-22 17:59:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17245v1"
    },
    {
        "title": "SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning",
        "authors": "Yizhou ChiYizhang LinSirui HongDuyi PanYaying FeiGuanghao MeiBangbang LiuTianqi PangJacky KwokCeyao ZhangBang LiuChenglin Wu",
        "links": "http://arxiv.org/abs/2410.17238v1",
        "entry_id": "http://arxiv.org/abs/2410.17238v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17238v1",
        "summary": "Automated Machine Learning (AutoML) approaches encompass traditional methods\nthat optimize fixed pipelines for model selection and ensembling, as well as\nnewer LLM-based frameworks that autonomously build pipelines. While LLM-based\nagents have shown promise in automating machine learning tasks, they often\ngenerate low-diversity and suboptimal code, even after multiple iterations. To\novercome these limitations, we introduce Tree-Search Enhanced LLM Agents\n(SELA), an innovative agent-based system that leverages Monte Carlo Tree Search\n(MCTS) to optimize the AutoML process. By representing pipeline configurations\nas trees, our framework enables agents to conduct experiments intelligently and\niteratively refine their strategies, facilitating a more effective exploration\nof the machine learning solution space. This novel approach allows SELA to\ndiscover optimal pathways based on experimental feedback, improving the overall\nquality of the solutions. In an extensive evaluation across 20 machine learning\ndatasets, we compare the performance of traditional and agent-based AutoML\nmethods, demonstrating that SELA achieves a win rate of 65% to 80% against each\nbaseline across all datasets. These results underscore the significant\npotential of agent-based strategies in AutoML, offering a fresh perspective on\ntackling complex machine learning challenges.",
        "updated": "2024-10-22 17:56:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17238v1"
    }
]