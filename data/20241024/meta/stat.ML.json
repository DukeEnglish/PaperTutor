[
    {
        "title": "Optimal Robust Estimation under Local and Global Corruptions: Stronger Adversary and Smaller Error",
        "authors": "Thanasis PittasAnkit Pensia",
        "links": "http://arxiv.org/abs/2410.17230v1",
        "entry_id": "http://arxiv.org/abs/2410.17230v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17230v1",
        "summary": "Algorithmic robust statistics has traditionally focused on the contamination\nmodel where a small fraction of the samples are arbitrarily corrupted. We\nconsider a recent contamination model that combines two kinds of corruptions:\n(i) small fraction of arbitrary outliers, as in classical robust statistics,\nand (ii) local perturbations, where samples may undergo bounded shifts on\naverage. While each noise model is well understood individually, the combined\ncontamination model poses new algorithmic challenges, with only partial results\nknown. Existing efficient algorithms are limited in two ways: (i) they work\nonly for a weak notion of local perturbations, and (ii) they obtain suboptimal\nerror for isotropic subgaussian distributions (among others). The latter\nlimitation led [NGS24, COLT'24] to hypothesize that improving the error might,\nin fact, be computationally hard. Perhaps surprisingly, we show that\ninformation theoretically optimal error can indeed be achieved in polynomial\ntime, under an even \\emph{stronger} local perturbation model (the\nsliced-Wasserstein metric as opposed to the Wasserstein metric). Notably, our\nanalysis reveals that the entire family of stability-based robust mean\nestimators continues to work optimally in a black-box manner for the combined\ncontamination model. This generalization is particularly useful in real-world\nscenarios where the specific form of data corruption is not known in advance.\nWe also present efficient algorithms for distribution learning and principal\ncomponent analysis in the combined contamination model.",
        "updated": "2024-10-22 17:51:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17230v1"
    },
    {
        "title": "Covariance estimation using Markov chain Monte Carlo",
        "authors": "Yunbum KookMatthew S. Zhang",
        "links": "http://arxiv.org/abs/2410.17147v1",
        "entry_id": "http://arxiv.org/abs/2410.17147v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17147v1",
        "summary": "We investigate the complexity of covariance matrix estimation for Gibbs\ndistributions based on dependent samples from a Markov chain. We show that when\n$\\pi$ satisfies a Poincar\\'e inequality and the chain possesses a spectral gap,\nwe can achieve similar sample complexity using MCMC as compared to an estimator\nconstructed using i.i.d. samples, with potentially much better query\ncomplexity. As an application of our methods, we show improvements for the\nquery complexity in both constrained and unconstrained settings for concrete\ninstances of MCMC. In particular, we provide guarantees regarding isotropic\nrounding procedures for sampling uniformly on convex bodies.",
        "updated": "2024-10-22 16:27:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17147v1"
    },
    {
        "title": "Understanding Transfer Learning via Mean-field Analysis",
        "authors": "Gholamali AminianŁukasz SzpruchSamuel N. Cohen",
        "links": "http://arxiv.org/abs/2410.17128v2",
        "entry_id": "http://arxiv.org/abs/2410.17128v2",
        "pdf_url": "http://arxiv.org/pdf/2410.17128v2",
        "summary": "We propose a novel framework for exploring generalization errors of transfer\nlearning through the lens of differential calculus on the space of probability\nmeasures. In particular, we consider two main transfer learning scenarios,\n$\\alpha$-ERM and fine-tuning with the KL-regularized empirical risk\nminimization and establish generic conditions under which the generalization\nerror and the population risk convergence rates for these scenarios are\nstudied. Based on our theoretical results, we show the benefits of transfer\nlearning with a one-hidden-layer neural network in the mean-field regime under\nsome suitable integrability and regularity assumptions on the loss and\nactivation functions.",
        "updated": "2024-10-23 06:51:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17128v2"
    },
    {
        "title": "Optimal Design for Reward Modeling in RLHF",
        "authors": "Antoine ScheidEtienne BoursierAlain DurmusMichael I. JordanPierre MénardEric MoulinesMichal Valko",
        "links": "http://arxiv.org/abs/2410.17055v2",
        "entry_id": "http://arxiv.org/abs/2410.17055v2",
        "pdf_url": "http://arxiv.org/pdf/2410.17055v2",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) has become a popular\napproach to align language models (LMs) with human preferences. This method\ninvolves collecting a large dataset of human pairwise preferences across\nvarious text generations and using it to infer (implicitly or explicitly) a\nreward model. Numerous methods have been proposed to learn the reward model and\nalign a LM with it. However, the costly process of collecting human preferences\nhas received little attention and could benefit from theoretical insights. This\npaper addresses this issue and aims to formalize the reward training model in\nRLHF. We frame the selection of an effective dataset as a simple regret\nminimization task, using a linear contextual dueling bandit method. Given the\npotentially large number of arms, this approach is more coherent than the\nbest-arm identification setting. We then propose an offline framework for\nsolving this problem. Under appropriate assumptions - linearity of the reward\nmodel in the embedding space, and boundedness of the reward parameter - we\nderive bounds on the simple regret. Finally, we provide a lower bound that\nmatches our upper bound up to constant and logarithmic terms. To our knowledge,\nthis is the first theoretical contribution in this area to provide an offline\napproach as well as worst-case guarantees.",
        "updated": "2024-10-23 12:55:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17055v2"
    },
    {
        "title": "Bayes without Underfitting: Fully Correlated Deep Learning Posteriors via Alternating Projections",
        "authors": "Marco MianiHrittik RoySøren Hauberg",
        "links": "http://arxiv.org/abs/2410.16901v1",
        "entry_id": "http://arxiv.org/abs/2410.16901v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16901v1",
        "summary": "Bayesian deep learning all too often underfits so that the Bayesian\nprediction is less accurate than a simple point estimate. Uncertainty\nquantification then comes at the cost of accuracy. For linearized models, the\nnull space of the generalized Gauss-Newton matrix corresponds to parameters\nthat preserve the training predictions of the point estimate. We propose to\nbuild Bayesian approximations in this null space, thereby guaranteeing that the\nBayesian predictive does not underfit. We suggest a matrix-free algorithm for\nprojecting onto this null space, which scales linearly with the number of\nparameters and quadratically with the number of output dimensions. We further\npropose an approximation that only scales linearly with parameters to make the\nmethod applicable to generative models. An extensive empirical evaluation shows\nthat the approach scales to large models, including vision transformers with 28\nmillion parameters.",
        "updated": "2024-10-22 11:15:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16901v1"
    }
]