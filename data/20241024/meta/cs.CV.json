[
    {
        "title": "Altogether: Image Captioning via Re-aligning Alt-text",
        "authors": "Hu XuPo-Yao HuangXiaoqing Ellen TanChing-Feng YehJacob KahnChristine JouGargi GhoshOmer LevyLuke ZettlemoyerWen-tau YihShang-Wen LiSaining XieChristoph Feichtenhofer",
        "links": "http://arxiv.org/abs/2410.17251v1",
        "entry_id": "http://arxiv.org/abs/2410.17251v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17251v1",
        "summary": "This paper focuses on creating synthetic data to improve the quality of image\ncaptions. Existing works typically have two shortcomings. First, they caption\nimages from scratch, ignoring existing alt-text metadata, and second, lack\ntransparency if the captioners' training data (e.g. GPT) is unknown. In this\npaper, we study a principled approach Altogether based on the key idea to edit\nand re-align existing alt-texts associated with the images. To generate\ntraining data, we perform human annotation where annotators start with the\nexisting alt-text and re-align it to the image content in multiple rounds,\nconsequently constructing captions with rich visual concepts. This differs from\nprior work that carries out human annotation as a one-time description task\nsolely based on images and annotator knowledge. We train a captioner on this\ndata that generalizes the process of re-aligning alt-texts at scale. Our\nresults show our Altogether approach leads to richer image captions that also\nimprove text-to-image generation and zero-shot image classification tasks.",
        "updated": "2024-10-22 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17251v1"
    },
    {
        "title": "SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes",
        "authors": "Cheng-De FanChen-Wei ChangYi-Ruei LiuJie-Ying LeeJiun-Long HuangYu-Chee TsengYu-Lun Liu",
        "links": "http://arxiv.org/abs/2410.17249v1",
        "entry_id": "http://arxiv.org/abs/2410.17249v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17249v1",
        "summary": "We present SpectroMotion, a novel approach that combines 3D Gaussian\nSplatting (3DGS) with physically-based rendering (PBR) and deformation fields\nto reconstruct dynamic specular scenes. Previous methods extending 3DGS to\nmodel dynamic scenes have struggled to accurately represent specular surfaces.\nOur method addresses this limitation by introducing a residual correction\ntechnique for accurate surface normal computation during deformation,\ncomplemented by a deformable environment map that adapts to time-varying\nlighting conditions. We implement a coarse-to-fine training strategy that\nsignificantly enhances both scene geometry and specular color prediction. We\ndemonstrate that our model outperforms prior methods for view synthesis of\nscenes containing dynamic specular objects and that it is the only existing\n3DGS method capable of synthesizing photorealistic real-world dynamic specular\nscenes, outperforming state-of-the-art methods in rendering complex, dynamic,\nand specular scenes.",
        "updated": "2024-10-22 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17249v1"
    },
    {
        "title": "JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation",
        "authors": "Shota OnoharaAtsuyuki MiyaiYuki ImajukuKazuki EgashiraJeonghun BaekXiang YueGraham NeubigKiyoharu Aizawa",
        "links": "http://arxiv.org/abs/2410.17250v1",
        "entry_id": "http://arxiv.org/abs/2410.17250v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17250v1",
        "summary": "Accelerating research on Large Multimodal Models (LMMs) in non-English\nlanguages is crucial for enhancing user experiences across broader populations.\nIn this paper, we introduce JMMMU (Japanese MMMU), the first large-scale\nJapanese benchmark designed to evaluate LMMs on expert-level tasks based on the\nJapanese cultural context. To facilitate comprehensive culture-aware\nevaluation, JMMMU features two complementary subsets: (i) culture-agnostic (CA)\nsubset, where the culture-independent subjects (e.g., Math) are selected and\ntranslated into Japanese, enabling one-to-one comparison with its English\ncounterpart MMMU; and (ii) culture-specific (CS) subset, comprising newly\ncrafted subjects that reflect Japanese cultural context. Using the CA subset,\nwe observe performance drop in many LMMs when evaluated in Japanese, which is\npurely attributable to language variation. Using the CS subset, we reveal their\ninadequate Japanese cultural understanding. Further, by combining both subsets,\nwe identify that some LMMs perform well on the CA subset but not on the CS\nsubset, exposing a shallow understanding of the Japanese language that lacks\ndepth in cultural understanding. We hope this work will not only help advance\nLMM performance in Japanese but also serve as a guideline to create\nhigh-standard, culturally diverse benchmarks for multilingual LMM development.\nThe project page is https://mmmu-japanese-benchmark.github.io/JMMMU/.",
        "updated": "2024-10-22 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17250v1"
    },
    {
        "title": "PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction",
        "authors": "Long XingQidong HuangXiaoyi DongJiajie LuPan ZhangYuhang ZangYuhang CaoConghui HeJiaqi WangFeng WuDahua Lin",
        "links": "http://arxiv.org/abs/2410.17247v1",
        "entry_id": "http://arxiv.org/abs/2410.17247v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17247v1",
        "summary": "In large vision-language models (LVLMs), images serve as inputs that carry a\nwealth of information. As the idiom \"A picture is worth a thousand words\"\nimplies, representing a single image in current LVLMs can require hundreds or\neven thousands of tokens. This results in significant computational costs,\nwhich grow quadratically as input image resolution increases, thereby severely\nimpacting the efficiency of both training and inference. Previous approaches\nhave attempted to reduce the number of image tokens either before or within the\nearly layers of LVLMs. However, these strategies inevitably result in the loss\nof crucial image information, ultimately diminishing model performance. To\naddress this challenge, we conduct an empirical study revealing that all visual\ntokens are necessary for LVLMs in the shallow layers, and token redundancy\nprogressively increases in the deeper layers of the model. To this end, we\npropose PyramidDrop, a visual redundancy reduction strategy for LVLMs to boost\ntheir efficiency in both training and inference with neglectable performance\nloss. Specifically, we partition the LVLM into several stages and drop part of\nthe image tokens at the end of each stage with a pre-defined ratio, creating\npyramid-like visual tokens across model layers. The dropping is based on a\nlightweight similarity calculation with a negligible time overhead. Extensive\nexperiments demonstrate that PyramidDrop can achieve a 40% training time and\n55% inference FLOPs acceleration of LLaVA-NeXT with comparable performance.\nBesides, the PyramidDrop could also serve as a plug-and-play strategy for\ninference acceleration without training, with better performance and lower\ninference cost than counterparts. We hope that the insights and approach\nintroduced by PyramidDrop will inspire future research to further investigate\nthe role of image tokens in LVLMs.",
        "updated": "2024-10-22 17:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17247v1"
    },
    {
        "title": "Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss",
        "authors": "Zesen ChengHang ZhangKehan LiSicong LengZhiqiang HuFei WuDeli ZhaoXin LiLidong Bing",
        "links": "http://arxiv.org/abs/2410.17243v1",
        "entry_id": "http://arxiv.org/abs/2410.17243v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17243v1",
        "summary": "Contrastive loss is a powerful approach for representation learning, where\nlarger batch sizes enhance performance by providing more negative samples to\nbetter distinguish between similar and dissimilar data. However, scaling batch\nsizes is constrained by the quadratic growth in GPU memory consumption,\nprimarily due to the full instantiation of the similarity matrix. To address\nthis, we propose a tile-based computation strategy that partitions the\ncontrastive loss calculation into arbitrary small blocks, avoiding full\nmaterialization of the similarity matrix. Furthermore, we introduce a\nmulti-level tiling strategy to leverage the hierarchical structure of\ndistributed systems, employing ring-based communication at the GPU level to\noptimize synchronization and fused kernels at the CUDA core level to reduce I/O\noverhead. Experimental results show that the proposed method scales batch sizes\nto unprecedented levels. For instance, it enables contrastive training of a\nCLIP-ViT-L/14 model with a batch size of 4M or 12M using 8 or 32 A800 80GB\nwithout sacrificing any accuracy. Compared to SOTA memory-efficient solutions,\nit achieves a two-order-of-magnitude reduction in memory while maintaining\ncomparable speed. The code will be made publicly available.",
        "updated": "2024-10-22 17:59:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17243v1"
    }
]