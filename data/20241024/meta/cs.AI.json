[
    {
        "title": "JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation",
        "authors": "Shota OnoharaAtsuyuki MiyaiYuki ImajukuKazuki EgashiraJeonghun BaekXiang YueGraham NeubigKiyoharu Aizawa",
        "links": "http://arxiv.org/abs/2410.17250v1",
        "entry_id": "http://arxiv.org/abs/2410.17250v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17250v1",
        "summary": "Accelerating research on Large Multimodal Models (LMMs) in non-English\nlanguages is crucial for enhancing user experiences across broader populations.\nIn this paper, we introduce JMMMU (Japanese MMMU), the first large-scale\nJapanese benchmark designed to evaluate LMMs on expert-level tasks based on the\nJapanese cultural context. To facilitate comprehensive culture-aware\nevaluation, JMMMU features two complementary subsets: (i) culture-agnostic (CA)\nsubset, where the culture-independent subjects (e.g., Math) are selected and\ntranslated into Japanese, enabling one-to-one comparison with its English\ncounterpart MMMU; and (ii) culture-specific (CS) subset, comprising newly\ncrafted subjects that reflect Japanese cultural context. Using the CA subset,\nwe observe performance drop in many LMMs when evaluated in Japanese, which is\npurely attributable to language variation. Using the CS subset, we reveal their\ninadequate Japanese cultural understanding. Further, by combining both subsets,\nwe identify that some LMMs perform well on the CA subset but not on the CS\nsubset, exposing a shallow understanding of the Japanese language that lacks\ndepth in cultural understanding. We hope this work will not only help advance\nLMM performance in Japanese but also serve as a guideline to create\nhigh-standard, culturally diverse benchmarks for multilingual LMM development.\nThe project page is https://mmmu-japanese-benchmark.github.io/JMMMU/.",
        "updated": "2024-10-22 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17250v1"
    },
    {
        "title": "HyperspectralViTs: Fast and Accurate methane detection on-board satellites",
        "authors": "Vít RůžičkaAndrew Markham",
        "links": "http://arxiv.org/abs/2410.17248v1",
        "entry_id": "http://arxiv.org/abs/2410.17248v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17248v1",
        "summary": "On-board processing of hyperspectral data with machine learning models would\nenable unprecedented amount of autonomy for a wide range of tasks, for example\nmethane detection or mineral identification. Methane is the second most\nimportant greenhouse gas contributor to climate change, and it's automated\ndetection on-board of satellites using machine learning models would allow for\nearly warning system and could enable new capabilities such as automated\nscheduling inside constellations of satellites. Classical methods for methane\ndetection suffer from high false positive rates and previous deep learning\nmodels exhibit prohibitive computational requirements. We propose fast and\naccurate machine learning architectures which support end-to-end training with\ndata of high spectral dimension. We evaluate our models on two tasks related to\nhyperspectral data processing - methane leak detection and mineral\nidentification. With our proposed general architectures, we improve the F1\nscore of the previous methane detection state-of-the-art models by more than\n27% on a newly created synthetic dataset and by almost 13% on the previously\nreleased large benchmark dataset. We also demonstrate that training models on\nthe synthetic dataset improves performance of models finetuned on the dataset\nof real events by 6.9% in F1 score in contrast with training from scratch. On a\nnewly created dataset for mineral identification, our models provide 3.5%\nimprovement in the F1 score in contrast to the default versions of the models.\nWith our proposed models we improve the inference speed by 85.19% in contrast\nto previous classical and deep learning approaches by removing the dependency\non classically computed features. Namely, one capture from the EMIT sensor can\nbe processed in only 30 seconds on a realistic proxy hardware used on the\nION-SCV 004 satellite.",
        "updated": "2024-10-22 17:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17248v1"
    },
    {
        "title": "Learning Precise, Contact-Rich Manipulation through Uncalibrated Tactile Skins",
        "authors": "Venkatesh PattabiramanYifeng CaoSiddhant HaldarLerrel PintoRaunaq Bhirangi",
        "links": "http://arxiv.org/abs/2410.17246v1",
        "entry_id": "http://arxiv.org/abs/2410.17246v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17246v1",
        "summary": "While visuomotor policy learning has advanced robotic manipulation, precisely\nexecuting contact-rich tasks remains challenging due to the limitations of\nvision in reasoning about physical interactions. To address this, recent work\nhas sought to integrate tactile sensing into policy learning. However, many\nexisting approaches rely on optical tactile sensors that are either restricted\nto recognition tasks or require complex dimensionality reduction steps for\npolicy learning. In this work, we explore learning policies with magnetic skin\nsensors, which are inherently low-dimensional, highly sensitive, and\ninexpensive to integrate with robotic platforms. To leverage these sensors\neffectively, we present the Visuo-Skin (ViSk) framework, a simple approach that\nuses a transformer-based policy and treats skin sensor data as additional\ntokens alongside visual information. Evaluated on four complex real-world tasks\ninvolving credit card swiping, plug insertion, USB insertion, and bookshelf\nretrieval, ViSk significantly outperforms both vision-only and optical tactile\nsensing based policies. Further analysis reveals that combining tactile and\nvisual modalities enhances policy performance and spatial generalization,\nachieving an average improvement of 27.5% across tasks.\nhttps://visuoskin.github.io/",
        "updated": "2024-10-22 17:59:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17246v1"
    },
    {
        "title": "Towards Reliable Evaluation of Behavior Steering Interventions in LLMs",
        "authors": "Itamar PresLaura RuisEkdeep Singh LubanaDavid Krueger",
        "links": "http://arxiv.org/abs/2410.17245v1",
        "entry_id": "http://arxiv.org/abs/2410.17245v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17245v1",
        "summary": "Representation engineering methods have recently shown promise for enabling\nefficient steering of model behavior. However, evaluation pipelines for these\nmethods have primarily relied on subjective demonstrations, instead of\nquantitative, objective metrics. We aim to take a step towards addressing this\nissue by advocating for four properties missing from current evaluations: (i)\ncontexts sufficiently similar to downstream tasks should be used for assessing\nintervention quality; (ii) model likelihoods should be accounted for; (iii)\nevaluations should allow for standardized comparisons across different target\nbehaviors; and (iv) baseline comparisons should be offered. We introduce an\nevaluation pipeline grounded in these criteria, offering both a quantitative\nand visual analysis of how effectively a given method works. We use this\npipeline to evaluate two representation engineering methods on how effectively\nthey can steer behaviors such as truthfulness and corrigibility, finding that\nsome interventions are less effective than previously reported.",
        "updated": "2024-10-22 17:59:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17245v1"
    },
    {
        "title": "SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning",
        "authors": "Yizhou ChiYizhang LinSirui HongDuyi PanYaying FeiGuanghao MeiBangbang LiuTianqi PangJacky KwokCeyao ZhangBang LiuChenglin Wu",
        "links": "http://arxiv.org/abs/2410.17238v1",
        "entry_id": "http://arxiv.org/abs/2410.17238v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17238v1",
        "summary": "Automated Machine Learning (AutoML) approaches encompass traditional methods\nthat optimize fixed pipelines for model selection and ensembling, as well as\nnewer LLM-based frameworks that autonomously build pipelines. While LLM-based\nagents have shown promise in automating machine learning tasks, they often\ngenerate low-diversity and suboptimal code, even after multiple iterations. To\novercome these limitations, we introduce Tree-Search Enhanced LLM Agents\n(SELA), an innovative agent-based system that leverages Monte Carlo Tree Search\n(MCTS) to optimize the AutoML process. By representing pipeline configurations\nas trees, our framework enables agents to conduct experiments intelligently and\niteratively refine their strategies, facilitating a more effective exploration\nof the machine learning solution space. This novel approach allows SELA to\ndiscover optimal pathways based on experimental feedback, improving the overall\nquality of the solutions. In an extensive evaluation across 20 machine learning\ndatasets, we compare the performance of traditional and agent-based AutoML\nmethods, demonstrating that SELA achieves a win rate of 65% to 80% against each\nbaseline across all datasets. These results underscore the significant\npotential of agent-based strategies in AutoML, offering a fresh perspective on\ntackling complex machine learning challenges.",
        "updated": "2024-10-22 17:56:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17238v1"
    }
]