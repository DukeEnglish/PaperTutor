[
    {
        "title": "LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias",
        "authors": "Haian JinHanwen JiangHao TanKai ZhangSai BiTianyuan ZhangFujun LuanNoah SnavelyZexiang Xu",
        "links": "http://arxiv.org/abs/2410.17242v1",
        "entry_id": "http://arxiv.org/abs/2410.17242v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17242v1",
        "summary": "We propose the Large View Synthesis Model (LVSM), a novel transformer-based\napproach for scalable and generalizable novel view synthesis from sparse-view\ninputs. We introduce two architectures: (1) an encoder-decoder LVSM, which\nencodes input image tokens into a fixed number of 1D latent tokens, functioning\nas a fully learned scene representation, and decodes novel-view images from\nthem; and (2) a decoder-only LVSM, which directly maps input images to\nnovel-view outputs, completely eliminating intermediate scene representations.\nBoth models bypass the 3D inductive biases used in previous methods -- from 3D\nrepresentations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar\nprojections, plane sweeps) -- addressing novel view synthesis with a fully\ndata-driven approach. While the encoder-decoder model offers faster inference\ndue to its independent latent representation, the decoder-only LVSM achieves\nsuperior quality, scalability, and zero-shot generalization, outperforming\nprevious state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive\nevaluations across multiple datasets demonstrate that both LVSM variants\nachieve state-of-the-art novel view synthesis quality. Notably, our models\nsurpass all previous methods even with reduced computational resources (1-2\nGPUs). Please see our website for more details:\nhttps://haian-jin.github.io/projects/LVSM/ .",
        "updated": "2024-10-22 17:58:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17242v1"
    },
    {
        "title": "SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning",
        "authors": "Yizhou ChiYizhang LinSirui HongDuyi PanYaying FeiGuanghao MeiBangbang LiuTianqi PangJacky KwokCeyao ZhangBang LiuChenglin Wu",
        "links": "http://arxiv.org/abs/2410.17238v1",
        "entry_id": "http://arxiv.org/abs/2410.17238v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17238v1",
        "summary": "Automated Machine Learning (AutoML) approaches encompass traditional methods\nthat optimize fixed pipelines for model selection and ensembling, as well as\nnewer LLM-based frameworks that autonomously build pipelines. While LLM-based\nagents have shown promise in automating machine learning tasks, they often\ngenerate low-diversity and suboptimal code, even after multiple iterations. To\novercome these limitations, we introduce Tree-Search Enhanced LLM Agents\n(SELA), an innovative agent-based system that leverages Monte Carlo Tree Search\n(MCTS) to optimize the AutoML process. By representing pipeline configurations\nas trees, our framework enables agents to conduct experiments intelligently and\niteratively refine their strategies, facilitating a more effective exploration\nof the machine learning solution space. This novel approach allows SELA to\ndiscover optimal pathways based on experimental feedback, improving the overall\nquality of the solutions. In an extensive evaluation across 20 machine learning\ndatasets, we compare the performance of traditional and agent-based AutoML\nmethods, demonstrating that SELA achieves a win rate of 65% to 80% against each\nbaseline across all datasets. These results underscore the significant\npotential of agent-based strategies in AutoML, offering a fresh perspective on\ntackling complex machine learning challenges.",
        "updated": "2024-10-22 17:56:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17238v1"
    },
    {
        "title": "Fine-Tuning Large Language Models to Appropriately Abstain with Semantic Entropy",
        "authors": "Benedict Aaron TjandraMuhammed RazzakJannik KossenKunal HandaYarin Gal",
        "links": "http://arxiv.org/abs/2410.17234v1",
        "entry_id": "http://arxiv.org/abs/2410.17234v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17234v1",
        "summary": "Large Language Models (LLMs) are known to hallucinate, whereby they generate\nplausible but inaccurate text. This phenomenon poses significant risks in\ncritical applications, such as medicine or law, necessitating robust\nhallucination mitigation strategies. While recent works have proposed\nfine-tuning methods to teach LLMs to abstain from answering questions beyond\ntheir knowledge or capabilities, these methods rely on the existence of\nground-truth labels or are limited to short-form responses. To address these\nlimitations, we propose fine-tuning using semantic entropy, an uncertainty\nmeasure derived from introspection into the model which does not require\nexternal labels. We demonstrate that our approach matches or outperforms models\nfine-tuned using prior work and achieves strong performance for both short and\nlong-form generations on a range of datasets.",
        "updated": "2024-10-22 17:54:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17234v1"
    },
    {
        "title": "Few-shot In-Context Preference Learning Using Large Language Models",
        "authors": "Chao YuHong LuJiaxuan GaoQixin TanXinting YangYu WangYi WuEugene Vinitsky",
        "links": "http://arxiv.org/abs/2410.17233v1",
        "entry_id": "http://arxiv.org/abs/2410.17233v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17233v1",
        "summary": "Designing reward functions is a core component of reinforcement learning but\ncan be challenging for truly complex behavior. Reinforcement Learning from\nHuman Feedback (RLHF) has been used to alleviate this challenge by replacing a\nhand-coded reward function with a reward function learned from preferences.\nHowever, it can be exceedingly inefficient to learn these rewards as they are\noften learned tabula rasa. We investigate whether Large Language Models (LLMs)\ncan reduce this query inefficiency by converting an iterative series of human\npreferences into code representing the rewards. We propose In-Context\nPreference Learning (ICPL), a method that uses the grounding of an LLM to\naccelerate learning reward functions from preferences. ICPL takes the\nenvironment context and task description, synthesizes a set of reward\nfunctions, and then repeatedly updates the reward functions using human\nrankings of videos of the resultant policies. Using synthetic preferences, we\ndemonstrate that ICPL is orders of magnitude more efficient than RLHF and is\neven competitive with methods that use ground-truth reward functions instead of\npreferences. Finally, we perform a series of human preference-learning trials\nand observe that ICPL extends beyond synthetic settings and can work\neffectively with humans-in-the-loop. Additional information and videos are\nprovided at https://sites.google.com/view/few-shot-icpl/home.",
        "updated": "2024-10-22 17:53:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17233v1"
    },
    {
        "title": "Optimal Robust Estimation under Local and Global Corruptions: Stronger Adversary and Smaller Error",
        "authors": "Thanasis PittasAnkit Pensia",
        "links": "http://arxiv.org/abs/2410.17230v1",
        "entry_id": "http://arxiv.org/abs/2410.17230v1",
        "pdf_url": "http://arxiv.org/pdf/2410.17230v1",
        "summary": "Algorithmic robust statistics has traditionally focused on the contamination\nmodel where a small fraction of the samples are arbitrarily corrupted. We\nconsider a recent contamination model that combines two kinds of corruptions:\n(i) small fraction of arbitrary outliers, as in classical robust statistics,\nand (ii) local perturbations, where samples may undergo bounded shifts on\naverage. While each noise model is well understood individually, the combined\ncontamination model poses new algorithmic challenges, with only partial results\nknown. Existing efficient algorithms are limited in two ways: (i) they work\nonly for a weak notion of local perturbations, and (ii) they obtain suboptimal\nerror for isotropic subgaussian distributions (among others). The latter\nlimitation led [NGS24, COLT'24] to hypothesize that improving the error might,\nin fact, be computationally hard. Perhaps surprisingly, we show that\ninformation theoretically optimal error can indeed be achieved in polynomial\ntime, under an even \\emph{stronger} local perturbation model (the\nsliced-Wasserstein metric as opposed to the Wasserstein metric). Notably, our\nanalysis reveals that the entire family of stability-based robust mean\nestimators continues to work optimally in a black-box manner for the combined\ncontamination model. This generalization is particularly useful in real-world\nscenarios where the specific form of data corruption is not known in advance.\nWe also present efficient algorithms for distribution learning and principal\ncomponent analysis in the combined contamination model.",
        "updated": "2024-10-22 17:51:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.17230v1"
    }
]