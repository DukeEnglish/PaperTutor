CONVEX MARKOV GAMES: A FRAMEWORK FOR
FAIRNESS, IMITATION, AND CREATIVITY IN MULTI-
AGENT LEARNING
IanGemp AndreasHaupt LukeMarris
GoogleDeepMind MIT GoogleDeepMind
London,UK Cambridge,MA London,UK
imgemp@google.com haupt@mit.edu marris@google.com
SiqiLiu GeorgiosPiliouras
GoogleDeepMind GoogleDeepMind
London,UK London,UK
liusiqi@google.com gpil@google.com
ABSTRACT
Expert imitation, behavioral diversity, and fairness preferences give rise to pref-
erencesinsequentialdecisionmakingdomainsthatdonotdecomposeadditively
across time. We introduce the class of convex Markov games that allow general
convex preferences over occupancy measures. Despite infinite time horizon and
strictlyhighergeneralitythanMarkovgames,purestrategyNashequilibriaexist
understrictconvexity. Furthermore,equilibriacanbeapproximatedefficientlyby
performinggradientdescentonanupperboundofexploitability.Ourexperiments
imitatehumanchoicesinultimatumgames,revealnovelsolutionstotherepeated
prisoner’sdilemma,andfindfairsolutionsinarepeatedasymmetriccoordination
game. Intheprisoner’sdilemma,ouralgorithmfindsapolicyprofilethatdeviates
from observed human play only slightly, yet achieves higher per-player utility
whilealsobeingthreeordersofmagnitudelessexploitable.
1 INTRODUCTION
Ithasbeenacharacteristicassumptioninmulti-agentreinforcementlearningthatagentrewardsare
adiscountedsumofper-periodrewards(Littman,1994). Thisarticleproposesaframeworkbeyond
suchdecomposedreward,andarguesitsalgorithmictractabilityandutility.
Alreadyearlygametheoryhasobservedthatmaximizingdecomposablerewardsmayleadtoahigh
number of equilibria, of very different desirability for a designer, (Friedman, 1971). In addition,
many search problems have other desiderata, e.g., diversity of play, that cannot be expressed as
rewardmaximization(Zahavyetal.,2023). Inaddition,suchutilitiesdonotpermitthedesignersto
findapproximateequilibriathathavedesirablefeaturessuchasbeinghuman-like(imitation),orthat
are“creative”inthattheirtrajectoriesvisitmanypartsofthespace,ortoincludefairnessconcerns.
Inthispaper,wedefineageneralyettractableframeworkfortheinclusionoffairness,imitationand
creativitycomponentsinmulti-agentlearning.
Acrucialobjectforouranalysisistheoccupancymeasureµ.Occupancymeasuresarethelong-term
distributions of states and actions in different time periods, taking into account discounting. That
is,theweightofastate-actionprofile(s,(a ,a ,...,a ))isobtainedbysummingtheprobabilities
1 2 n
thats =sanda =a ,a =a ,...,a =a discountedbyγt. Ingeneral,theclassofutilities
t 1t 1 2t 2 nt n
thatweconsiderhereareconcavefunctionsofoccupancymeasures(equivalently,players’“losses”
ornegatedutilitiesareconvex).Restrictingtotheclassoflinearutilityfunctions,werecoverMarkov
games,thatis,theclassicmodelofmulti-agentreinforcementlearning(Littman,1994).
We describe our more general framework in the context of three classes of utility functions. For
thisdiscussion,weconsideralinearrewardfunctionr ∈RS×A,whichweviewastherewardfrom
1
4202
tcO
22
]TG.sc[
1v00661.0142:viXraagame. 1)Afirstclassisthoseutilitiesthatfavourdispersionofthestate-actionprofiles, u(µ) =
r⊤µ+τH(µ),whereHisanentropymeasuresuchasShannon,Re´nyi,orTsallisentropy,andτ >0.
WhereasentropyofplayerpoliciesH(π)measuresrandomnessoftheiractionselectionineachstate
andonlyweaklycorrelateswithdiversityofstatevisitation,entropyofplayeroccupancymeasures
directlymeasuresboth,especiallyextendingtodirectmeasurementofthelatter. 2)Surprisingly,in
additiontogivingabonusforexploring,anapparentlyopposingutilitycanbeaccommodatedwhile
maintaining tractability, as we show: imitation utilities, u(µ) = r⊤µ−τd(µ,ν) for some target
occupancy measure ν and some distance measure d such as reverse Kullback-Leibler divergence,
Wasserstein,ortotalvariationmetric. Thisallowsforequilibriathatareclosetoothers. 3)Finally,
fairnesscanbeencouragedthroughpenaltyfunctions,e.g.,u(µ)=r⊤µ−τµ⊤ΣµforsomeΣ⪰0.
WeexploreeachofthethreeclassesinourexperimentsinSection5.
LinearLoss ConvexLoss(ConcaveUtility)
Single-Agent MarkovDecisionProcess(MDP) convexMDP
Multi-Agent Markov(Stochastic)Game (Ours)convexMarkovGame
Table 1: We introduce the convex Markov Game (cMG) to fill a gap at the intersection of multi-
agentandreinforcementlearningresearch. WedemonstratelaterhowtousecMGstomodelthree
classes of utilities for a linear reward function r: (a) Creativity, using an entropy bonus term, (b)
Imitation, using a distance penalty to a reference occupancy measure, and (c) Fairness, using a
positivedefinitepenalty. Thispaperestablishesexistenceofequilibriaforsuchutilities,proposesa
differentiablelossforitscomputation,anddemonstratestheperformanceofthislossinthepresence
ofimitation,creativity,andfairnessdesiderata.
WeinitiateourstudyofconvexMarkovgameswiththreecontributions:
• First, we show that under strictly concave utilities, pure-strategy Nash equilibria exist. This is
the foundation for training policies in multi-agent reinforcement learning that do not require
randomizingovermultiplemodels.
• Second,weprovidealossthatboundsthedeviationfromequilibriuminaconvexMarkovgame.
Minimizingthislossallowsforthecomputationofequilibria.
• Finally,weshowthatconvexMarkovgamesarecapableofgeneratingcreative,human-likeand
fairequilibriainrepeatedgames.
The rest of the article is structured as follows. We define the class of convex Markov Games in
section2. Weshowthatpure-strategyNashequilibriaexistinsection3. Ouranalysisofalossfor
equilibrium computation is in section 4, which we put to practice in our experiments in section 5.
Wediscussrelatedliteratureinsection6,andconcludeinsection7.
2 CONVEX MARKOV GAMES
ThemaindefinitionofthisarticleistheconvexMarkovGame(cMG).
Definition1. AconvexMarkovgameisgivenbya6-tupleG =⟨S,A= n A ,P,u,γ,µ ⟩:
i=1 i 0
(cid:34)
• Playersi=1,2,...,n,
• afinitestatespaceS,andaninitialstatedistributionµ ∈∆S,
0
• finiteactionspacesA ,A ,...,A ,
1 2 n
• astatetransitionfunctionP: S×( n A )→∆S,
i=1 i
• adiscountfactorγ ∈[0,1),and (cid:34)
• a set of continuous utilities with each u concave in the ith player’s occupancy measure,
i
u i: ( n j=1∆S×Aj)→R.
(cid:34)
The Policy View Players i = 1,...,n choose policies π : S ×A → [0,1]. A policy profile
i i
π = (π 1,π 2,...,π n) induces a sequence of states and joint actions (s t) t∈N and (a t) t∈N, and a
state-actionoccupancymeasure
∞
(cid:88)
µπ(s,a)=(1−γ) γtP(s =s,a =a|µ ,π,P).
t t 0
t=0
2We can recover state-action occupancies from a matrix equation. Let Pπ be the state transition
matrixunderπ.
Thenthestateoccupancymeasureµs(s)=(cid:80)
µ(s,a)canbewrittenasafunction
a
ofπinmatrixnotationas
(cid:16) (cid:17)
µs(π)=(1−γ) [I−γPπ]−1µ , (1)
0
andthestate-actionoccupancycanberecoveredasµ(s,a) = µs(s)·π(a|s). Themarginalonthis
probabilityforonlyagenti’sactionscanberecoveredas
µ (π ,π )=(1−γ)([I−γPπ]−1µ 1⊤ )⊙π , (2)
i i −i 0 |Ai| i
where⊙denotestheHadamardproductandπ indicatesallpoliciesexceptplayeri’s.
−i
Players may choose to randomize over stochastic policies. We say that a player’s strategy ρ is a
i
mixed-strategyifitisadistributionoverpolicies. Forexample,letπ(a) andπ(b) bothbestochastic
i i
policies. Then an example of a mixed-strategy is one in which player i plays π(a) and π(b) with
i i
equalprobability1/2. Ifallprobabilitymassofρ iisonasinglepolicyπ i,wecallρ iapure-strategy
andwriteπ directly.
i
Wesaythata(random)policyprofileρ=(ρ ,ρ ,...,ρ )isa(mixed-strategy)Nashequilibriumif
1 2 n
forallagentsi=1,2,...,n,
E [u (µ(π ,π ))]≥E [u (µ(π′,π ))],
π∼ρ i i −i π i′∼ρ′ i,π−i∼ρ−i i i −i
for any policy ρ′. If ρ is a pure strategy profile as defined above, we call it a pure-strategy Nash
i
equilibrium.
Wecallanypolicyπ˜ (oroccupancymeasureµ˜ )playeri’sbestresponsetoaprofileρifitachieves
i i
anexpectedutilitythatismaximalamongallpolicies(respectively,occupancymeasures).Notebest
responsesarethesolutionstoindividualplayer’sMDPwithallotherplayerpoliciesheldfixed,and
sotheynecessarilyexistaspurestrategies: π˜ =argmax E [u (µ(π′,π ))]
π i′ π−i∼ρ−i i i −i
TheOccupancyMeasureView Inouranalysisandthealgorithmsproposedinthisarticle,itwill
behelpfultoframeandsolveproblemsdirectlyinthespaceofoccupancymeasuresU.
Givenopponentpolicyprofilesπ wecandefinetheprobabilitykernelPπ−i as
−i i
Pπ−i(s′|s,a )= (cid:88) P(s′|s,a ,a )(cid:89) π (a |s).
i i i −i j j
a−i∈A−i j̸=i
Giventhese,wecanreformulateanindividualagent’sdecisionproblemas
max u (µ ,π )
i i −i
µi∈R|S|×|Ai|
s.t. µ (s,a)≥0 ∀ s∈S,a∈A
i i (3)
(cid:88) (cid:0) I−γPπ−i(·,a ,·)(cid:1) µ (·,a )=(1−γ)µ (s) ∀ s∈S
i i i i 0
ai∈Ai
where Pπ−i(·,a ,·) is a next-state by state transition matrix and µ (·,a ) is a vector denoting the
i i i i
probabilityoftakingactiona ineverystate. Becauseu areconcaveinµ ,andgiventhelinearity
i i i
of the constraints in (3), this problem is convex, motivating the name of convex Markov Games
(cMGs). If u (µ ,π ) = r⊤µ as mentioned in the introduction, we recover the vanilla Markov
i i −i i
gameframework,howevercMGsallowamuchwiderclassofutilities(e.g.,onesthatincludeentropy
oftheoccupancymeasure). Noticethatthefeasiblesetin(3)isdefinedbyconstraintsthatdepend
on π . For convenience, we define the so-called action correspondence, U = M (π ), which
−i i i −i
mapseveryprofileofopponentpoliciestothefeasiblesetofoccupancymeasuresforplayeri,i.e,
problem(3)canbesuccinctlywritten
max u (µ ,π ).
i i −i
µi∈Mi(π−i)
Wewillusethefeaturethatpoliciescanberecoveredfromoccupancymeasuresas
(cid:40) µi(s,a) if (cid:80) µ (s,a′)>0
π i(µ i)(a|s)= (cid:80) a′µi(s,a′) a′ i (4)
arbitrary otherwise.
33 EXISTENCE OF NASH EQUILIBRIUM
Wefirstshowthatunderfairlygeneralassumptions,mixedNashequilibriaexist. Wethenarguethat
forthepurposesoflearning,pure-strategyNashequilibriaareparticularlydesirable,andshowthat
theseexistwhenutilitiesarestrictlyconcave.
Proposition1. Mixed-strategyNashequilibriaexistinconvexMarkovGames.
Thisstatementfollowseasilybyusingthepolicyview. Eachplayer’soptimizationprobleminterms
ofpoliciesis:
max u (µ (π ,π ),π ). (5)
i i i −i −i
πi∈( (cid:34)| sS =| 1∆Ai)
Appendix B shows that all u are continuous, differentiable functions of (π ,π ), and since the
i i −i
strategy sets are compact, the existence of mixed-strategy Nash equilibria follows from classical
existenceresults(Glicksberg,1952).
While existence of mixed-strategy equilibria are a descriptively helpful tool, they have limitations
for applications in learning. In particular, learning a continuous distribution over stochastic poli-
cies would practically require function approximation or defining a mesh over the space, greatly
increasingthecomplexityofthelearningproblem; thischallengehasbeenbroachedbutnotsuffi-
ciently solved in the context of, for example, generative adversarial networks (Arora et al., 2017).
Therefore,weprovideanotherstatementthatensurestheexistenceofpure-strategyNashequilibria.
Theorem1. GivenaconvexMarkovgamewithuniquebestresponsecorrespondencesinthespace
of occupancy measures, Nash equilibria exist in stationary stochastic policies (and hence state-
actionoccupancymeasures).
The proof of this statement relies on the continuity of players’ utilities and compactness of their
strategy sets as before, but also the uniqueness of best responses.1 The classes of strictly convex
functions and invex functions (Hanson, 1981) are two that give rise to unique best response cor-
respondences. Most definitions of (neg)entropy and distance metrics satisfy strict convexity. Any
penaltyfunctiongivenbyaquadraticwithpositivedefinitematrixΣdoesaswell.
We briefly address the limitation of unique best response correspondences with a more detailed
discussionintheAppendixG.Whileconcaveutilitiesgiverisetoconvexbestresponsecorrespon-
dencesinoccupancymeasurespace,theimageofthesesetsinpolicyspacemaybenon-convex.The
foundationofmanyequilibriumexistenceresultsrelyontheassumptionofamappingfrompoints(a
strategyprofile)toconvexsets(asetofstrategyprofiles)viatheKakutanifixedpointtheorem. This
coreassumptionisbrokeninthemoregeneralcMGsetting. ProvingexistenceofNashequilibriain
thismoregeneralsettingisdesiredastheallowanceofconcave(not-strictly)utilitieswouldenable
thespecificationof“safe”occupancymeasuresets(Miryoosefietal.,2019).
4 COMPUTATION OF EQUILIBRIA
Whileequilibriaexist,theymaybehardtocomputewithgeneralsolversduetothesizeoftheequi-
libriumcomputationproblemandtheexpenseofrepeatedlysolvingconvexprograms. Wepresent
agradient-basedapproachforequilibriumcomputation. Wefirstusetheoccupancymeasureview
toderiveameasureofutility-gain(ϵ )fromplayerbestresponses,andthenreparameterizethisloss
i
to policies to circumvent challenges arising from non-convexity of the set of occupancy measures
(AppendixG.2).
Weprovideanupperboundonexploitability(ϵ)asalossforequilibriumcomputation,definedas
(cid:18) (cid:19)
ϵ= max ϵ where ϵ = max u (z,µ ) −u (µ ,µ ). (6)
i i i −i i i −i
i=1,...,n z∈Mi(µ−i)
Exploitabilitymeasuresthemostanyplayercangainbyunilaterallydeviating. Mechanistically, it
correspondstoeachplayersolvingproblem(3)—aconstrained,convexoptimizationproblem—and
1While our proof technique follows that of (Rosen, 1965) in relying on the Kakutani fixed-point theo-
rem(Kakutani,1941),thetheoryofconcavegamesdoesnotdirectlyapplytoourproposedcMGframeworkas
agentutilitiesareconcaveintheiroccupancymeasures,butnotconcaveintheirpolicies.
4(a)OccupancyMeasureSpace(U) (b)PolicySpace(Π)
Figure 1: (A *not strictly* convex Markov Game) A 2-player, 2-state, 2-action convex Markov
gamewithconcave,butnotstrictlyconcaveutilities(notshown). Colorshelpvisualizetheeffectof
thenonlineartransformationfromoccupancyspacetopolicyspace(seeequation4). Seeadditional
environmentdetailsinAppendixG.1. (a)Thefeasiblesetofoccupancymeasuresforplayer1given
afixedpolicyforplayer2withbestresponseregionisshowninblack.Player1’soccupancygradient
∇i pointsoffthe3-simplexsoisnotshown.Instead,weshow∇i projectedontothetangentspace
µi µi
ofthefeasiblesetinwhite,i.e.,Π (∇i ). Thevectorshadowingthewhiteoneis∇i projected
TMi µi µi
ontothetangentspaceofthesimplex;notethisvectorpointsoffthefeasibleset(intothepage). (b)
A2-dsliceofplayer1’sfeasiblepolicyspace. Thesetofbestresponsepoliciesisnon-convexwhen
viewedinpolicyspace.
thenreportingthevaluetheyattainedbeyondthatoftheirstrategyundertheapproximateequilibrium
profile(µ ,...,µ ). Inparticular,anoccupancymeasure(µ ,...,µ )withvanishingexploitabil-
1 n 1 n
ity is a pure-strategy Nash equilibrium; the same NE profile from the policy view can be derived
fromequation4.
The next result extends that of Gemp et al. (2024) from the normal-form game setting to show
that exploitability is bounded from above by a constant depending on the action space size, and a
projectedgradient(comparewhitevectortoonebehinditinFigure1). Thisboundisonlytightfor
solutionsintheinteriorofthesimplexofoccupancymeasures.Thisisintuitiveasboundarysolutions
may feature non-zero gradients in the tangent dimension despite having zero exploitability. The
followingresultboundstheexploitabilityofacMGwithutilitiesu usingplayers’utility-gradients
i
ofacMGwithasmallamountofentropyregularization,i.e. uτ(µ ,π )=u (µ ,π )+τH(µ ),
i i −i i i −i i
whereH denotesShannonentropy.
Theorem2(LowTemperatureApproximateEquilibriaareApproximateNashEquilibria). Assume
thatµ hasfullsupport.Let∇iτ beplayeri’sentropyregularizedgradientandπbeanapproximate
0 µi
equilibriumoftheentropy-regularizedgame. Then,
√
ϵ (π)≤τlog(|A |)+ 2∥Π (∇iτ)∥,
i i TMi µi
whereΠ isaprojectionmatrixonthetangentspaceofM ,and∇iτ isthegradientofuτ with
TMi i µi i
respecttoµ .
i
Inspiredbythisbound,wedefinethefollowingprojected-gradientloss(PGL)functionforcMGs:
(cid:88)
Lτ(π)= ||Π (∇iτ)||2. (7)
TMi µi
i
AsaconsequenceofTheorem2,weobtainthat
(cid:112)
ϵ(π)≤τlog(|S|n|A|)+ 2nLτ(π). (8)
Observethatthisanalysispurelyleveragestheoccupancymeasureview, thatis, viewtheproblem
asparameterizedbyµ . Thespaceoffeasibleoccupancymeasuresµ,unfortunately,isanonlinear
i
manifold, and minimization of Lτ may be challenging. While for any fixed π the contraints in
−i
equation3arelinear,thejointconstraintsforfeasibleoccupancymeasuresarenot(AppendixG.2).
5Instead of maximizing occupancy measures, we can consider optimizing directly over the space
of policies π . Each player’s policy is subject to simplex constraints that are independent of the
i
otherplayers. Lτ(π)=Lτ(µ(π))withµ(π)definedinSection2. Inthisway,wecombinethetwo
distinctstrengthsofthepolicyandoccupancymeasureviews.Inthepolicyview,playerstrategysets
(policy sets) are independent and convex, making for simple independent updates and projections
backtothefeasiblesets. Intheoccupancymeasureview,weenjoyconvexityofplayerlosseswhich
enablederivationofupperboundsonexploitability.
Note that our proposed loss is composed of a projection operator, the gradients of our concave
utilitiesu ,andthemappingfrompoliciestooccupancymeasures. Weproveinseverallemmasin
i
theappendixthatallofthesecomponentsaredifferentiableassumingthattheutilitiesarenotonly
concave,butalsodifferentiable2.
Wecangivemoreintuitionfortheanalysisoftheprojectionoperator. IfA(µ −i) ∈ R|S|×(|S|·|Ai|)
such that A(µ )µ = (1−γ)µ represents the linear equality constraints in equation 3, then the
−i i 0
projectionmatrixisgivenby
Π =I −A⊤(AA⊤)−1A. (9)
TMi(µ−i) s×s
Foreaseofnotation,weomitthedependenceofAonµ . Π isdifferentiableifandonly
−i TMi(µ−i)
if A(µ ) has full row rank (i.e., rank |S|). The following result shows that A(µ ) has full row
−i −i
rank.
Lemma1. Foranyγ <1,theequalityconstraintmatrixinequation3hasfullrowrank(|S|).
Thisyieldsa differentiablelossandallows foroptimizationusingautomatic differentiation. If we
represent agent policies in an unconstrained space with logits, we can then minimize Lτ directly
withrespecttoagentpoliciesusingourpreferredunconstrainedoptimizer,e.g.,Adam. Wereferto
thisapproachasPGLforprojectedgradientlossminimization.
5 EXPERIMENTS
We test three variants of utilities that do not decompose linearly in four domains. We compare
againstfourbaselinealgorithms,andcomparetheresultingexploitabilityandpolicyprofiles.
Baselines. We compare against four baselines. The first baseline, minϵ, directly minimizes ex-
ploitability with respect to the agent policies using a differentiable convex optimization package
CVXPYLAYERSinJAX(Agrawaletal.,2019;Bradburyetal.,2018). Inthesecondbaseline,Sim,
all players simultaneously run gradient descent on their losses with respect to their policies and
we report the performance of the running average of the policy trajectory. Policies at each state
are represented in R|Ai|−1 as a softmax over |A i|−1 logits with the last logit fixed as 0. In the
third, RR,agentsalternategradientdescentstepsinround-robinfashion. Wealsocompareagainst
the SGAMESOLVER (Eibelsha¨user & Poensgen, 2023) package of homotopy methods for Markov
games.
Hyperparameters. We use an exponentially decaying annealing schedule of τ = 10−t for t ∈
[0,...,7],with1000iterationsateachtemperatureτ forourentropyregularization.Adam’sinternal
stateisnotresetafterannealing.Weinitializetouniformpolicies(andwhateveroccupancymeasure
that induces) unless otherwise specified. All experiments except pathfinding were run on a single
CPUandtakeaboutaminutetosolvealthoughexactexploitabilityreportingviacvxopt(Diamond
&Boyd,2016)increasesruntimeapproximately10×. PathfindingwasrunonasingleGPU.
Domains. We consider five domains: The first is a multiagent pathfinding problem in a grid
world.The second domain is the classic two-player, iterated prisoner’s dilemma (compare Tucker
&StraffinJr(1983))whereagentsmaychoosetocooperateordefectwiththeirpartner. Thethird
domain is a three-player, public goods game where agents may choose to contribute all or none
oftheirsavingstoapublicpoolwhichisthenredistributedevenlywithagrowthmultiplierof1.3
(compareJanssen&Ahn(2003)); payoffismeasuredintermsofplayerprofits. Inthefourth, we
2Strictdifferentiabilityofutilitiesisnotrequiredforautodiff libraries,e.g.,JaX(Bradburyetal.,2018).
6Figure2: (ApproximatePGLEquilibrium)Arrowsindicateeachplayer’smostlikelyaction(4car-
dinaldirections+“Stay”)withtheircolorindicatingtheirprobability(lightestoccursat0.20prob-
ability,darkestat1.0). Movestothesamelocationareawardedrandomlytooneoftheagents. The
goalstateismarkedbyBandGwherebothgreenreachesthetoprightcornerandbluethetopleft.
Figure3: (ConvergencetoNashequilibria)RRandPGLdescentyieldthelowestexploitabilityyet
converge to different equilibria. PGL drops coincide with temperature annealing. minϵ crashes,
markedbystars.
consider a three-player El Farol bar problem where players choose whether to go to a bar or stay
home (Arthur, 1994). Agents receive maximum payoff (2) for attending an uncrowded bar (< 3
people),followedbystayinghome(1),followedbyattendingacrowdedbar(0).Lastly,weconsider
the classic Bach-Stravinsky game where agents must coordinate to attend a performance despite
differentpreferences. Wesetγ =0.99inalldomains,andusethelastjointactionselectedbyeach
playerasstate,i.e.,S =A.
5.1 CREATIVITY
Our first application considers utilities that value solutions that cover more than a small subset of
thestate-actionspace,whichleadsto“creative”equilibria,asweshow. Wegetsuchoutcomesfor
utilitiesthatgetentropybonusesforhigh(Shannon)entropyoccupancymeasures,u(µ) = r⊤µ+
τH(µ).
Tofindequilibriaoftheoriginalgame,weannealtheweightonthisentropybonustowardszero,fol-
lowingpriorworkonhomotopymethodsforequilibriainMarkovgames(Eibelsha¨user&Poensgen,
2019).
First,weconsideramultiagentpathfindingproblem. Twoagentsmustcoordinatetopassthrougha
bottleneckdoorwayontheirwaytoajointgoalstate. Therewardforreachingthegoalstateis100
forbothplayers;−0.01rewardotherwise. Uponreachingthegoalstate,theagentsareresettothe
startstate(leftmostgridinFigure2). Ouralgorithmreturnedanapproximateequilibriumwherethe
finalutilityforeachagentwas24.5andtheexploitabilitywas1.7(≈7%oftheirutility).
AsinglerolloutofthefinallearnedpolicyisshowninFigure2.Theagentsracetocrossthedoorway,
afterwhich,oneagenttakesthecenterpositionandtheotherstepsaside. Inthethirdandfourthto
lastframes,theblueagentmovesdownwardduetothesmallremainingentropybonus. Bothagents
then move upward towards the goal state. In the final frame, green executes a “no-op” action to
the right as blue moves into goal position. Despite learning a factorized Nash equilibrium policy
profile,theagentsexhibitcoordinatedactionsatcertainstep. Thiscoordinationisachievedthrough
observationsofpartnerplayersgridlocations,butrichercoordinationistheoreticallypossiblewith
richerobservationspaces.
Next, we examine three iterated normal-form games. Figure 3 shows our algorithm has vanishing
exploitability for all of them. In each game, directly minimizing exploitability in CVXPYLAYERS
7States=(at−1,at−1) argmax π (a|s) max π (a|s)
0 1 a 0 a 0
(Cooperate,Cooperate) Cooperate 0.73
(Cooperate,Defect) Defect 0.74
(Defect,Cooperate) Cooperate 0.66
(Defect,Defect) Defect 0.81
Table2: (ApproximatePGLEquilibriumonIPD)Theutilityper-playeris0.47comparedtothatof
otherclassicIPDstrategies: tit-for-tat(0.5),win-stay,lose-shift(0.67),grimtrigger(0.42),defect-
defect(0.33).
States=(at−1,at−1,at−1) argmax π (a|s) max π (a|s)
0 1 2 a 0 a 0
(None,None,None) None 0.99
(None,None,All-In) None 0.66
(None,All-In,None) None 0.66
(None,All-In,All-In) All-In 0.60
(All-In,None,None) None 0.80
(All-In,None,All-In) All-In 0.56
(All-In,All-In,None) All-In 0.56
(All-In,All-In,All-In) All-In 0.86
Table 3: (Approximate PGL Equilibrium on iterated public goods game) Per-player utility is 0.03
versus0forzerocontributionpolicies.
crashesduetonumericalinstabilities. Round-robindescentexhibitsasimilarqualitativebehaviorto
ourmethod,andfasterconvergencethansimultaneousdescent.
Forastudyofcreativity,itisvaluabletoinspecttheequilibriathemethodsproduced. Round-robin
gradientdescentconvergestoanasymmetricequilibriuminiteratedElFarolwhereoneplayergoes
tothebareverynight,whilethetwootherplayersmustalternate. Incontrast,ourapproachreveals
morenuanced,symmetricpoliciesinIPDandPGGthatwediscussbelow.
Note that measuring the entropy of a player’s occupancy measure is different from measuring the
entropyoftheirpolicy;thelatteronlymeasuresentropyofactiondistributionsineachstate,ignoring
thedistributionacrossstates. Interestingly,thisdifferencemanifestsinthestructureoftheequilib-
ria we discover. At high entropy, agents must explore the entire state space, which includes joint
cooperation in the iterated prisoner’s dilemma (IPD) and joint donation in the public goods game
(PGG). As temperature is annealed, the player’s receive less of a bonus for exploration, however,
thistransientintroductiontomutuallybeneficialplayhasalastingimpactonequilibriumselection.
In IPD, PGL finds a symmetric policy. In Table 2, we show player 0’s learned policy. If both
playerscooperatedonthelastround,thentheyarelikelytocontinuecooperating. Iftheotherplayer
defected, thenyouarelikelytodefect(evenmorelikelyifyoudefectedratherthancooperatedon
the last round). If you defected and the other player cooperated, you are actually more likely to
cooperateinthenextround,inanactofreciprocation.
Intheiteratedpublicgoodsgame,PGLfindsasymmetricpolicyaswell. InTable3,weshowplayer
0’slearnedpolicy. Incontrasttothezerocontributionpoliciesfoundbyallothermethods,wefind
acontributionpolicythat,intuitively,ismorelikelytocontributefundswhenotheragentsdo.
Weseethatentropybonusesinutilitiesallowsustoselectforcreativeequilibria.
5.2 IMITATION
Oursecondapplicationconsidersutilitiesthatvaluepoliciessimilartopoliciesobservedinhuman
experiments (Romero & Rosokha, 2023, Table 1, Current, Direct-Response). In this experiment,
webuildonthehomotopyexperimentinthecreativitysectionwhereweannealedourentropyco-
8States=(at−1,at−1) a∗ =argmax π (a|s) π (a∗|s) πh(a∗|s)
0 1 a 0 0 0
(Cooperate,Cooperate) Cooperate 0.83 0.86
(Cooperate,Defect) Defect 0.52 0.65
(Defect,Cooperate) Defect 0.53 0.55
(Defect,Defect) Defect 0.86 0.87
Table4: ApproximateNashequilibriumrecoveredbyouralgorithminIPDafterannealingKLreg-
ularizationtothehumanpolicyreportedintherightmostcolumn(Romero&Rosokha,2023,Table
1, Current, Direct-Response). The utility per-player under our learned symmetric policy profile
is 0.48 versus 0.46 for the human policy. In addition, our learned policy profile is 1.4 × 10−4-
exploitableateverystate,whereasthehumanpolicyprofileissubstantiallymoreexploitable,being
0.47-exploitableoveranyinitialstate.(Exploitabilityforallinitialstatesmaybeseenasananalogue
ofMarkovperfectioninMarkovgames(Maskin&Tirole,2001).
efficient τ, but instead of annealing entropy, we anneal a KL penalty to the human state-action
occupancy measure, u (µ) = r⊤µ−τd (µ∥µ ). Note that matching occupancy measures is
i KL ref
akintomatchinglong-runtrajectorieswhereasmatchingpoliciesdoesnotnecessarilymatchtrajec-
tories when other agents adjust their policies as will be the case in our experiments over learning.
The human policies were derived from experiments where subjects played the iterated prisoner’s
dilemma, selecting cooperate (C) or defect (D) in each period (Romero & Rosokha, 2023, Table
1,Current,Direct-Response). Subjectswererequiredtoconfirmtheiropponent’sactionaftereach
period. Thisensuredthattheywerecapableofrepresentingapolicythatconditionsontheprevious
action. Table4reportsthesymmetricpolicywelearnedwhileregularizingtothehumanoccupancy
measure. Notethatthisnewpolicyprofilehasslightlyhigherutilityforallagentsandisveryclose
toanequilibrium,independentofthestartingstate.
After two scenarios where we used a sequence of cMGs to discover creative resp. human-like
equilibria,weconsiderasettinginwhichtheutilitiesarenotannealed,butareconstantacrosstime.
5.3 FAIRNESS
Our final application considers utilities that value fair visitation of states. In the Bach-Stravinsky
game,twoplayersmustchoosewhethertoattendaperformancebyBachorStravinsky.Iftheydon’t
meet,theygetzeroreward. However,oneplayerprefersBachtoStravinsky(3vs. 2),whereasthe
otherplayerprefersStravinskytoBach(3vs.2).Weincorporateatermintobothplayer’sobjectives
thatpenalizes10timesthesquareddifferencebetweenattendingBachandattendingStravinskyto
incentivize fair, equal attendance of the two shows, u (µ) =
r⊤µ−((cid:80)
µ ((B,B),a)−
i a∈{S,B} i
(cid:80) µ ((S,S),a))2. We can write the second term as µ⊤Σµ for an appropriately chosen
a∈{S,B} i
positivesemidefinitematrixΣ.
Inordertoavoidthetrivialequilibriumofvoting50/50,weinitializelogitsforbothplayers’policies
withastandardnormal. Wesettemperatureτ tozeroandthenoptimizewithalearningrateof0.1
for1000iterations.
In 10 random trials, both players converge to the same approximate NE where they vote for their
favored event 60% of the time regardless of their actions on the previous day. The maximum ex-
ploitabilityoverthe10trialsis2.5×10−5,andthemaxdifferencebetween(cid:80)
µ ((B,B),a)and
a i
(cid:80) µ ((S,S),a)is2.14×10−5,implyingthisisa“fair”behavioralprofilebyourfairnessmetric.
a i
6 RELATED WORK
Ourworkrelatestothesingle-agentliteratureonconvexMarkovgamesandequilibriumselection
inMarkovgames. Werelyontechniquesfromthetheoryofabstracteconomies,lossminimization
forequilibriumcomputation,andusehomotopymethodsasinspirationforourexperiments.Finally,
weunifyapproachestocreativity,imitation,andfairnessfrommulti-agentlearning.
9Convex Markov Decision Processes Markov decision processes (MDPs) are the predominant
framework for modeling sequential decision making problems, especially in infinite-horizon set-
tings (Puterman, 2014). The goal of a decision maker in an MDP is typically to maximize a γ-
discountedsumofrewardsearnedthroughoutthesequentialdecisionprocess.Intheinfinite-horizon
setting,recentresearchhasexploitedanalternative,butequivalentviewofmaximizingtheexpected
rewardundertheagent’sstationarystate-actionoccupancymeasure(Zhangetal.,2020)—theprob-
ability of being in a given state and taking a given action. This viewpoint reveals an optimization
problemwithalinearobjective(maximizereturn)andlinearconstraints(validoccupancymeasure);
from this launchpad, research has generalized to convex objectives that incorporate, for example,
the(neg)entropyoftheoccupancymeasureinordertomaximizeexplorationoftheMDP(Zahavy
etal.,2021)ormaximizerobustness(Grand-Cle´ment&Petrik,2022).
MarkovGames. Whenmultipleagents’decisionmakingproblemsinteract,theMDPframework
isextendedtoaMarkovgame,alsoknownasastochasticgame(Thuijsman,1997;Littman,1994).
In the game setting, we seek equilibria, a notion of simultaneous optimality for all agents. Fink
(1964) proved the existence of stationary (time-independent) γ-discounted Nash equilibria in n-
player, general-sum stochastic games. A homotopy approach that traces the continuum of quantal
response equilibria performs well at approximating Nash equilibria in the limit of zero tempera-
ture(Eibelsha¨user&Poensgen,2019). Otherapproachesaretailoredformorerestrictedtwo-player,
zero-sumsettings(Daskalakisetal.,2020;Goktasetal.,2023b)orsettingswhereagentincentives
areapriorialignedsuchasMarkovpotentialgames(Leonardosetal.,2021). Priorworkproveda
negative result for value iteration based approaches stating it is not possible to derive a stationary
equilibriumpolicyfromQ-valuesingeneralMarkovgames(Zinkevichetal.,2005).
Techniques. In our existence proof, we rely on the theory of abstract economies of Arrow &
Debreu (1954). Results on abstract economies have only recently found use in machine learning
applications(Goktas&Greenwald,2022;Goktasetal.,2023a;Jordanetal.,2023). Ourapplication
combinespolicyandoccupancymeasureviewswithgeneralresultsonabstracteconomies.
Our proposed loss function extends that designed in recent work for normal-form games
(NFGs) (Gemp et al., 2024) to the convex Markov game setting. In their work, the focus was on
constructingalossamenabletounbiasedestimation. InNFGs,theprojectionmatrixΠ =
TMi(µ−i)
Π isaconstantmatrixindependentofplayerstrategies. Thisallowstheconstructionofanun-
TMi
biasedestimatoroftheirlossaslongastheycanobtainunbiasedgradientsofplayer’sutilities. Our
lossappliestoamoregeneralclassofgames,butsacrificesunbiasedness.
Ourapplicationsannealthetemperatureofanentropybonusand/oraKullback-Leiblerdivergence
penalty. Suchapproachesrelatetohomotopycontinuation-basedapproachestoequilibriumcompu-
tation. McKelvey&Palfrey(1995)introducedquantalresponseequilibriaalongwithahomotopy
from infinite to zero temperature defining their limiting logit equilibrium. They extended this to
theextensive-formgamesetting(McKelvey&Palfrey,1998),atwhichpointothersprovidedanin-
depthstudyofitsuniqueequilibriumselectionproperties(Harsanyietal.,1988). Morerecentwork
extended this approach to Markov games (Eibelsha¨user & Poensgen, 2019). Our approach selects
equilibriausingahomotopy-inspiredmethodinourapplicationstocreativityandimitation.
Applications. Ourgoalsofcreativity, imitation, andfairnessarenotnewtomulti-agentapplica-
tions. For example, Bakhtin et al. (2022) used KL-regularization towards human play to recover
strategicallysuperiorpoliciesinDiplomacy. Zahavyetal.(2022;2023)leveragedconvexMDPsto
discovermorecreativeplayinChess. AndHughesetal.(2018)modelsinequityaversionincom-
plexmulti-agentreinforcementlearningdomains. Incontrasttotheseapplications,ourframework
of convex Markov games allows for a unified analysis of several domains in a common language
andusingcommonalgorithmicprinciples.
7 CONCLUSION
Convex Markov Games are a versatile framework for multi-agent reinforcement learning, in par-
ticular for equilibria that imitate human behavior, pursue creative equilibria, and those where ad-
ditionalconstraintsoverstatesaredesired. Notonlydopure-strategyequilibriaexist, thereisalso
a differentiable upper bound for its optimization. Conceptually, optimization can be viewed via a
10parameterizationasofpoliciesandagentoccupancymeasures. Theframeworkfindsequilibriawith
diversestatevisitation,thatareclosetohumandata,andthatsatisfyfairnessconstraints.
ConvexMarkovgamesopenupseveraldirectionsforfuturework,inparticularinrelationtodecen-
tralizedtrainingandsettingswherethetransitiondynamicsareunknown.Ifdynamicsareestimated,
wepointedoutissuesobtainingunbiasedestimatesoftheprojectionoperatorΠ . Ourcur-
TMi(µ−i)
rentapproachcomputesanequilibriumforallagentsusingacentralizedgradient. Itseemsbeliev-
able that, given the fact that all agents solve convex optimization problems, decentralized training
towardsanotionof(coarse)-correlatedequilibriaincMGsispossible.
REFERENCES
AkshayAgrawal,BrandonAmos,ShaneBarratt,StephenBoyd,StevenDiamond,andJZicoKolter.
Differentiableconvexoptimizationlayers. AdvancesinNeuralInformationProcessingSystems,
32,2019.
SanjeevArora,RongGe,YingyuLiang,TengyuMa,andYiZhang. Generalizationandequilibrium
ingenerativeadversarialnets(gans). InInternationalConferenceonMachineLearning,pp.224–
232.PMLR,2017.
Kenneth J Arrow and Gerard Debreu. Existence of an equilibrium for a competitive economy.
Econometrica: JournaloftheEconometricSociety,pp.265–290,1954.
WBrianArthur. Complexityineconomictheory: Inductivereasoningandboundedrationality. The
AmericanEconomicReview,84(2):406–411,1994.
AntonBakhtin,DavidJWu,AdamLerer,JonathanGray,AthulPaulJacob,GabrieleFarina,Alexan-
derHMiller,andNoamBrown.Masteringthegameofno-pressdiplomacyviahuman-regularized
reinforcementlearningandplanning. arXivpreprintarXiv:2210.05492,2022.
StephenPBoydandLievenVandenberghe.Convexoptimization.Cambridgeuniversitypress,2004.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao
Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http:
//github.com/google/jax.
ConstantinosDaskalakis,DylanJFoster,andNoahGolowich.Independentpolicygradientmethods
forcompetitivereinforcementlearning. AdvancesinNeuralInformationProcessingSystems,33:
5527–5540,2020.
Steven Diamond and Stephen Boyd. Cvxpy: A python-embedded modeling language for convex
optimization. JournalofMachineLearningResearch,17(83):1–5,2016.
Steffen Eibelsha¨user and David Poensgen. Markov quantal response equilibrium and a homo-
topymethodforcomputingandselectingmarkovperfectequilibriaofdynamicstochasticgames.
AvailableatSSRN3314404,2019.
Steffen Eibelsha¨user and David Poensgen. sgamesolver: A python package to solve stochastic
games. AvailableatSSRN3316631,2023.
Francisco Facchinei and Christian Kanzow. Generalized Nash equilibrium problems. 4or, 5(3):
173–210,2007.
ArlingtonMFink. Equilibriuminastochasticn-persongame. Journalofscienceofthehiroshima
university,seriesai(mathematics),28(1):89–93,1964.
James W Friedman. A non-cooperative equilibrium for supergames. The Review of Economic
Studies,38(1):1–12,1971.
Ian Gemp, Luke Marris, and Georgios Piliouras. Approximating Nash equilibria in normal-form
games via stochastic optimization. In International Conference on Learning Representations,
2024.
11Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, and Mark Crowley. KKT conditions, first-order
andsecond-orderoptimization,anddistributedoptimization: tutorialandsurvey. arXivpreprint
arXiv:2110.01858,2021.
IrvingLGlicksberg.Afurthergeneralizationofthekakutanifixedtheorem,withapplicationtoNash
equilibriumpoints. ProceedingsoftheAmericanMathematicalSociety,3(1):170–174,1952.
Denizalp Goktas and Amy Greenwald. Exploitability minimization in games and beyond. In
S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in
Neural Information Processing Systems, volume 35, pp. 4857–4873. Curran Associates, Inc.,
2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/
file/1f3b0b15d6bb860dcfa6e5c8ba7d3d96-Paper-Conference.pdf.
DenizalpGoktas,DavidCParkes,IanGemp,LukeMarris,GeorgiosPiliouras,RomualdElie,Guy
Lever,andAndreaTacchetti. Generativeadversarialequilibriumsolvers. InTheTwelfthInterna-
tionalConferenceonLearningRepresentations,2023a.
Denizalp Goktas, Arjun Prakash, and Amy Greenwald. Convex-concave zero-sum
markov stackelberg games. In A. Oh, T. Naumann, A. Globerson, K. Saenko,
M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Sys-
tems, volume 36, pp. 66818–66832. Curran Associates, Inc., 2023b. URL
https://proceedings.neurips.cc/paper_files/paper/2023/file/
d2f6f1dfbf9cd89a78c5a58ef0dec245-Paper-Conference.pdf.
Julien Grand-Cle´ment and Marek Petrik. On the convex formulations of robust markov decision
processes. arXivpreprintarXiv:2209.10187,2022.
MorganAHanson. Onsufficiencyofthekuhn-tuckerconditions. J.Math.Anal.Appl,80(2):545–
550,1981.
JohnCHarsanyi,ReinhardSelten,etal. Ageneraltheoryofequilibriumselectioningames. MIT
PressBooks,1,1988.
Edward Hughes, Joel Z Leibo, Matthew Phillips, Karl Tuyls, Edgar Duen˜ez-Guzman, Antonio
Garc´ıaCastan˜eda, IainDunning, TinaZhu, KevinMcKee, RaphaelKoster, etal. Inequityaver-
sion improves cooperation in intertemporal social dilemmas. Advances in Neural Information
ProcessingSystems,31,2018.
MarcoJanssenandTKAhn. Adaptationvs.anticipationinpublic-goodgames. Inannualmeeting
oftheAmericanPoliticalScienceAssociation,Philadelphia,PA,2003.
MichaelIJordan,TianyiLin,andManolisZampetakis. First-orderalgorithmsfornonlineargener-
alizedNashequilibriumproblems. JournalofMachineLearningResearch,24(38):1–46,2023.
ShizuoKakutani. Ageneralizationofbrouwer’sfixedpointtheorem. DukeMathematicalJournal,
8(3):457,1941.
JongGwangKim. EquilibriumcomputationofgeneralizedNashgames: Anewlagrangian-based
approach. In Proceedings of the 22nd ACM Conference on Economics and Computation, pp.
676–676,2021.
StefanosLeonardos,WillOverman,IoannisPanageas,andGeorgiosPiliouras. Globalconvergence
ofmulti-agentpolicygradientinmarkovpotentialgames.arXivpreprintarXiv:2106.01969,2021.
Michael L. Littman. Markov games as a framework for multi-agent reinforcement learning. In
Proceedings of the Eleventh International Conference on International Conference on Machine
Learning,ICML’94,pp.157–163,SanFrancisco,CA,USA,1994.MorganKaufmannPublishers
Inc. ISBN1558603352.
David G Luenberger, Yinyu Ye, et al. Linear and nonlinear programming, volume 2. Springer,
1984.
Eric Maskin and Jean Tirole. Markov perfect equilibrium: I. observable actions. Journal of Eco-
nomicTheory,100(2):191–219,2001.
12RichardDMcKelveyandThomasRPalfrey. Quantalresponseequilibriafornormalformgames.
GamesandEconomicBehavior,10(1):6–38,1995.
RichardDMcKelveyandThomasRPalfrey. Quantalresponseequilibriaforextensiveformgames.
Experimentaleconomics,1:9–41,1998.
SobhanMiryoosefi,Kiante´Brantley,HalDaumeIII,MiroDudik,andRobertESchapire.Reinforce-
mentlearningwithconvexconstraints. AdvancesinNeuralInformationProcessingSystems,32,
2019.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley&Sons,2014.
Julian Romero and Yaroslav Rosokha. Mixed strategies in the indefinitely repeated prisoner’s
dilemma. Econometrica,91(6):2295–2331,2023.
JBRosen. Existenceanduniquenessofequilibriumpointsforconcaven-persongames. Economet-
rica,33(3):520–534,1965.
IosifSakos,Emmanouil-VasileiosVlatakis-Gkaragkounis,PanayotisMertikopoulos,andGeorgios
Piliouras.Exploitinghiddenstructuresinnon-convexgamesforconvergencetoNashequilibrium.
AdvancesinNeuralInformationProcessingSystems,36,2024.
FrankThuijsman. Asurveyonoptimalityandequilibriainstochasticgames. MaastrichtUniversity,
1997.
Albert W Tucker and Philip D Straffin Jr. The mathematics of tucker: A sampler. The Two-Year
CollegeMathematicsJournal,14(3):228–232,1983.
TomZahavy,BrendanO’Donoghue,GuillaumeDesjardins,andSatinderSingh. Rewardisenough
forconvexMDPs. AdvancesinNeuralInformationProcessingSystems,34:25746–25759,2021.
TomZahavy, YannickSchroecker, FeryalBehbahani, KateBaumli, SebastianFlennerhag, Shaobo
Hou,andSatinderSingh. Discoveringpolicieswithdomino: Diversityoptimizationmaintaining
nearoptimality. InTheEleventhInternationalConferenceonLearningRepresentations,2022.
Tom Zahavy, Vivek Veeriah, Shaobo Hou, Kevin Waugh, Matthew Lai, Edouard Leurent, Nenad
Tomasev, Lisa Schut, Demis Hassabis, and Satinder Singh. Diversifying AI: Towards creative
chesswithAlphaZero. arXivpreprintarXiv:2308.09175,2023.
Junyu Zhang, Alec Koppel, Amrit Singh Bedi, Csaba Szepesvari, and Mengdi Wang. Variational
policy gradient method for reinforcement learning with general utilities. Advances in Neural
InformationProcessingSystems,33:4572–4583,2020.
MartinZinkevich,AmyGreenwald,andMichaelLittman. Cyclicequilibriainmarkovgames. Ad-
vancesinNeuralInformationProcessingSystems,18,2005.
13A EQUILIBRIUM PROOF
We now prove that a pure-strategy policy Nash equilibrium exists for any player utility functions
that admit unique solutions in the space of occupancy measures (a convex, compact set). This
includesstrictlyconvexfunctions,butalsothemoregeneralclassofinvexfunctionswhichmaybe
non-convex.
Forconvenience,werepeattheKakutanifixedpointtheoremhere.
(cid:81)
Theorem ((Kakutani, 1941)). Let Π = Π be a non-empty, compact and convex subset of
i i
R|Sn|(cid:81) i|Ai|.
Letϕ:Π→2Πbeaset-valuedfunctiononΠwiththefollowingproperties:
• ϕhasaclosedgraph;
• ϕ(π)isnon-emptyandconvexforallπ ∈Π.
Thenϕhasafixedpoint.
WenowstateausefulLemmathatproveseveryoccupancymeasuremapstoaconvexsetofpolicies.
Lemma 2. The set of policies that give rise to a single occupancy measure µ
i
∈ R|S||Ai| for a
singleplayeriisaclosed,convexset.
Proof. ThemappingfromoccupancymeasurestopoliciesinconvexMDPsettingsistypicallyde-
finedsuchthatitisone-to-oneevenonstateswithzerooccupancymeasure,e.g.,
(cid:40) µi(s,a) if (cid:80) µ (s,a′)>0
π i(µ i)(a|s)= (cid:80) a′µi(s,a′) a′ i (10)
1/|A | else.
i
However, this definition is inadequate in a multi-player setting. In a game, we cannot assume (or
force)aplayer’spolicytakesaspecificvalueunlessitisrationalizable(i.e.,theychosenthatpolicy
becauseitistheirunqiuebestresponse). Therefore, inthecasewhereastatehaszerooccupancy,
thesetofallowablepoliciesinthatstateistheentiresimplex∆Ai;thisiswhywepresentequation4
inSection2.
Instateswheretheoccupancyhaspositivemeasure,themappingfromoccupancytopolicyisunique
and results in a singleton, a closed, convex set. Otherwise, the mapping is to the simplex, also a
closed, convex set. A player’s full policy is formed from the product sets of its state policies; the
productoftheseconvexsetsisconvexcompletingtheclaim.
Corollary1(ConvexBestResponseCorrespondence). Ifaplayer’sbestresponsecorrespondence
isunique(asingleton)inthespaceofoccupancymeasures,thenitsbestresponsecorrespondence
inpolicyspaceisaconvexset.
Inwhatfollows, althoughthetheoryofn-player, concavegamesbyRosen(1965)doesnotapply,
weareabletotraceasimilarlineofreasoningandemployLemma2toproveexistenceofpureNash
equilibria. First,werecallsomeusefulpropertiesandpresentsomepreliminarydefinitions.
For π ∈ Π, u (π) is continuous in π from Lemma 3 (µ (π) is continuous) and continuity of the
i i
objectiveu (µ ).
i i
AnequilibriumpointofthecMGisgivenbyapointπ∗ ∈Πsuchthat
u (π∗)=max{u (π′,π∗ )|(π′,π∗ )∈Π}(i=1,...,n). (11)
i i i −i i −i
π′
i
Theresultstofollowmakeuseofthefunctionρ(π,π′)definedfor(π,π′)∈Π×Πby
n
(cid:88)
ρ(π,π′)= u (π′,π ). (12)
i i −i
i=1
Byconstruction,themaximaofρinπ′ areequaltotheproductsetofthemaximaofeachu w.r.t.
i
π′ individually. Weobservethatfor(π,π′)∈Π×Πwehave((π′,π ))∈Π,i=1,...,n,sothat
i i −i
14ρ(π,π′)iscontinuousinπ andπ′ andbyCorollary1, itsmaximizersinπ′ formaclosed, convex
setforeveryfixedπ,for(π,π′)∈Π×Π. Wenowstateourgeneraltheoremhere.
Theorem1(Follows(Rosen,1965)). GivenaconvexMarkovgamewithuniquebestresponsecor-
respondences in the space of occupancy measures, Nash equilibria exist in stationary stochastic
policies(andhencestate-actionoccupancymeasures).
Proof. Considerthepoint-to-setmappingϕ:Π→Π,givenby
ϕ(π)={π′|ρ(π,π′)=maxρ(π,z)}. (13)
z∈Π
It follows from the continuity of ρ(π,z) and the convexity of argmax ρ(π,z) for fixed π that
z∈Π
ϕ is an upper semicontinuous mapping that maps each point of the convex, compact set Π into a
closed,convexsubsetofΠ. ThenbytheKakutanifixedpointtheorem,thereexistsapointπ∗ ∈ Π
suchthatπ∗ ∈ϕ(π∗),or
ρ(π∗,π∗)=maxρ(π∗,z). (14)
z∈Π
The fixed point π∗ is an equilibrium point satisfying equation 11. Suppose it was not. Then, for
i=ℓ,therewouldbeapointπ =π¯ suchthatπ¯ =(π∗,...,π¯ ,...,π∗)∈Πandu (π¯)>u (π∗).
ℓ ℓ 1 ℓ n ℓ ℓ
Butthenρ(π∗,π¯)>ρ(π∗,π∗),whichcontradictsequation14.
B OCCUPANCY FROM POLICY IS DIFFERENTIABLE
Lemma3. Playeri’sstate-actionoccupancymeasureµ isadifferentiable(andhencecontinuous)
i
functionoftheplayerpoliciesπ.
Proof. Recall
(cid:16) (cid:17)
µ (π)=(1−γ) [I−γPπ]−1µ 1⊤ ⊙π (15)
i 0 ai i
where
Pπ(s′,s)=⟨Pπ−j(s′,s,:),π (s,:)⟩ (16)
j j
and
Pπ−j(s′,s,:)=(cid:88)
P(s′|s,a ,a
)(cid:89)
π (s,a ) (17)
j i −j k k
a−j k̸=j
so
(cid:88) (cid:89)
Pπ(s′,s)= P(s′|s,a) π (s,a ). (18)
j j
a j
Then,
∂µ (x,y) (cid:104) (cid:105)
i =(1−γ) [I−γPπ]−1µ (s) 1(x=x′,y =y′,j =i)
∂π j(x′,y′) 0 x
∂ (cid:16) (cid:17)
+(1−γ) [I−γPπ]−1 (µ 1⊤)⊙π (19)
∂π (x′,y′) 0 ai i
j
where
∂ (cid:16) (cid:17) ∂Pπ
[I−γPπ]−1 =γ[I−γPπ]−1 [I−γPπ]−1 (20)
∂π (x′,y′) ∂π (x′,y′)
j j
and
∂Pπ (cid:26) 0 ifx̸=x′
= (21)
∂π (x′,y′) Pπ−j(s′,x,y) else.
j j
Clearly,thisrequiresinvertingthematrix[I−γPπ]. NotethatPπ isasquarestatetransitionmatrix
with distributions on columns. By the same argument as Lemma 1, this matrix has full-row rank,
andsinceitissquare,itisnon-singular,andhenceinvertible. Therefore,thederivative(19)always
exists.
15C CONTINUITY OF ACTION CORRESPONDENCE
ItisknownthattheBellman-flowconstraintsinProgram(3)additionallyencodeasimplexconstraint
on µ (see this by summing the constraints over s). Note that the Bellman flow constraints in (3)
i
encodeahyperplane. Thereforethefeasiblesetistheintersectionofahyperplanewiththesimplex,
resulting in a convex polytope. It is known that there always exists a stationary distribution µ
i
satisfying these constraints. Note that the hyperplane has fixed dimension (d = |S||A |−|S|)
h i
becausetheconstraintmatrixhasfullrow-rank|S|(proveninLemma1below).
Recallplayeri’stransitionmatrixgivenfixedpoliciesπ is:
−i
Pπ−i(s′|s,a )= (cid:88) P(s′|s,a ,a )(cid:89) π (a |s) (22)
i i i −i j j
a−i∈A−i j̸=i
wherea isplayerj’sactioncomponentofa . Notethisfunctionispolynomialinπ andthere-
j −i −i
forecontinuousinπ .
−i
NotethattheconstraintmatrixdependsonthetransitionoperatorPπ−i.
Therefore,theorientation
i
and translation of the hyperplane depend continuously on the entries in
Pπ−i.
As the hyperplane
i
continuouslymoves,theboundaryoftheconvexpolytopeinducedbyitsintersectionwiththesim-
plexalsomovescontinuously.
Inorderforthepoliciestodependcontinuouslyontheotherplayeroccupancymeasures,weneedto
ensurethattheoccupanciesalwayshavefullsupportonallstates. Recallthatπ (a|s) = µi(s,a)
(cid:80)
i (cid:80) aµi(s,a)
if µ (s,a)>0else1/|A |. Forstate-actionsconvergingtozerooccupancymeasure,thiswould
a i i
induce discontinuous changes in their induced policies which would induce discontinuities in the
transitionkernelforotheragents. Weensureoccupancieshavefullsupportbyassumingµ (s)has
0
fullsupport.
Giventheaboveintuition,wenowrigorouslyprovecontinuityoftheactioncorrespondence,begin-
ning with proving the Bellman flow constraints are all linearly independent (which guarantees its
associatedhyperplanehasfixeddimension).
Lemma1. TheBellmanflowconstraintmatrixhasfullrow-rank(|S|)andisfixedindependentof
otherplayerpolicies.
Proof. NotetheBellmanflowconstraintscanbewritteninmatrixformas
(cid:88)
(I −γP )µ =(1−γ)µ (23)
s′×s i,a i,a 0
a∈Ai
whereP =P(s′|s,a)denotestheS×S×A tensoroftransitionprobabilitiesandP =P (s′|s)
i i i,a i,a
selectsoutasingleaction,leavinganS×S matrix.
(cid:80)
Thisconstraintcanbewrittenwithoutthe byconstructingtherectangularblockmatrices
a∈Ai
I
s′×(sa)
=[I s′×s...I s′×s] (24)
and
P =[P ...P ]. (25)
i,s′×(sa) i,a1 i,am
Then
(I −γP )µ =(1−γ)µ (26)
s′×(sa) i,s′×(sa) i 0
Wecanexaminethefirsts′ ×sblockof(I −γP )andshowthatthismatrixisfull
s′×(sa) i,s′×(sa)
rank,i.e.,ofrank|S|.Ifthismatrixisfullrank,thenitsrowsarelinearlyindependent.Extendingour
viewtothefullmatrix,i.e.,allcolumns,cannotrenderanyoftheseoriginalrowslinearlydependent.
Note that the first block is represented by (I −γP ) for some action a. Using Gershgorin’s
s′×s i,a
circle theorem, we can bound the eigenvalues of this matrix to lie in a union of circles which all
16exclude the origin. Consider any column c, then every circle has a center in R . In addition, the
+
leftmostpointofeverycircleliesinR :
+
(cid:88)
(1−γP (c|c)− |γP (s′|c)| (27)
i,a i,a
s′̸=c
(cid:88)
=(1−γP (c|c)−γ P (s′|c) (28)
i,a i,a
s′̸=c
(cid:88)
=1−γ P (s′|c) (29)
i,a
s′
=1−γ >0. (30)
Therefore, thismatrixisnon-singular, i.e., full-rank. Thereareonly|S|rows, hencetherow-rank
cannotincrease,whichprovestheclaim.
Lemma4. Theactioncorrespondenceforeachplayerisacontinuousfunctionoftheotherplayer’s
policies.
Proof. Foranysequenceofotherplayerpolicies{π −(t i)} t∈Nconvergingtoπ −iandanyµ iinthefea-
siblesetinducedbyπ −i,defineacorrespondingsequenceofplayerioccupancymeasures{µ( it)} t∈N
asfollows.
Defineacontinuousfunctionπ (s):[1,∞)→Π thatlinearlyinterpolatesbetweeneachpairof
−i −i
subsequentelementsin{π −(t i)} t∈N,e.g.,π −i(s)=(1−(s−⌊s⌋))π −(⌊ is⌋)+(s−⌊s⌋)π −(⌈ is⌉)where⌊·⌋
and⌈·⌉representtheroundingoperationstothenearestinteger(down/uprespectively).
This continuous function π (s) induces a corresponding continuous function Pπ−i(s) (see Sec-
−i i
tion2),definingthehyperplanethatintersectsthepositiveorthantateverys.
Note,weareguaranteedtherealwaysexistsafeasibleµ bytheequivalencebetweenthepolicyand
i
occupancymeasureviewsofMDPs.
Chooseabasis{v j(s)} j∈[dh]withv j(s):[1,∞)→R|S||Ai|forthehyperplanethatmoveswiththe
hyperplane(i.e., afixedlocalreferenceframeforthehyperplane); selectitsuchthat µ liesinthe
i
directionofthelimitoneofthesebasisvectorsv∞ =lim v (s)startingfromthecentroidµcof
k s→∞ k i
thefeasiblepolytopeinducedbyπ . NotethatbyLemma1thedimensionalityofthishyperplane
−i
d isfixed,soitisclearhowtoconstructsuchabasis.
h
Let µc i(s) : [1,∞) → R|S||Ai| be the centroid of the feasible polytope for all s. Define b k(s) to
be the distance from µc(s) to the boundary in direction v∞ for all s. Note the hyperplane moves
i k
continuously,intersectingwiththepositiveorthant,aconvexset. Thisimpliesthefeasiblepolytope
remains convex for all s with a continuously deforming boundary. In addition, we are guaranteed
thefeasiblesetisneveremptybytheequivalencebetweenthepolicyandoccupancymeasureviews
ofMDPs. Therefore,bothµc(s)andb (s)arecontinuous. Letµc = lim µc(s),andsimilarly
i k i s→∞ i
b =lim b (s).
k s→∞ k
Bythisconstruction,µ canberepresentedasµ =µc+ξb v∞withξ ∈[0,1].
i i i k k
Finally,definethesequence{µ( it)} t∈Nwitheachµ( it) =µc i(t)(1/t)+(1−1/t)(µc i(t)+ξb k(t)v k(t)).
This sequence always lies in the feasible set because it represents an interpolation between the
centroidandthepolytopeboundary. Also,thesequence{µ( it)} t∈Nisbuiltfromanunderlyingsetof
continuous components; therefore, it converges to µ in the limit by construction: lim µ(t) =
i t→∞ i
lim µc(s)(1/s)+(1−1/s)(µc(s)+ξb (s)v (s))=µc+ξb v∞ =µ .
s→∞ i i k k i k k i
Lemma 5. Assume that the initial state distribution µ has full support. Then, M :
0 i
× j̸=i∆|S|×|Aj|−1 →∆|S|×|Ai|−1foreachplayeriisacontinuouscorrespondence.
Proof. Lemma 4 proves the action correspondence is continuous in the other players’ policies.
Giveneveryplayer’sstate-occupancyhasfull-support,eachplayer’spolicyisacontinuousfunction
17oftheiroccupancymeasure(seeequation(4)). Therefore,theactioncorrespondenceiscontinuous
overotherplayers’occupancymeasuresaswell.
D KKT CONDITIONS IMPLY FIXED POINT SUFFICIENCY
Considerthefollowingconstrainedoptimizationproblem:
max f(x) (31a)
x∈Rd
s.t. g (x)≤0 ∀i (31b)
i
h (x)=0 ∀j (31c)
j
where f is concave and g and h represent inequality and equality constraints respectively. If g
i j i
and h are affine functions, then any maximizer x∗ of f must satisfy the following necessary and
i
sufficientKKTconditions(Ghojoghetal.,2021;Boyd&Vandenberghe,2004):
• Stationarity: 0∈∂f(x∗)−(cid:80) λ ∂h (x∗)−(cid:80) µ ∂g (x∗)
j j j i i i
• Primalfeasibility: h (x∗)=0forallj andg (x∗)≤0foralli
j i
• Dualfeasibility: µ ≥0foralli
i
• Complementaryslackness: µ g (x∗)=0foralli.
i i
Lemma 6. Assuming player k’s utility, u (x ,x ), is concave in its own strategy x , a strictly-
k k −k k
positiveprimal-feasiblestrategyisabestresponseBR ifandonlyifithaszeroprojected-gradient
k
norm.
Proof. Consider the problem of formally computing ϵ (x) = max u (z,x ) −
k z≥0,Az=b k −k
u (x ,x ):
k k −k
max u (z,x )−u (x ,x ) (32a)
k −k k k −k
z∈Rd
s.t.−z ≤0 ∀k (32b)
k
A z−b =0 ∀j. (32c)
j j
Note that the objective is linear (concave) in z and the constraints are affine, therefore the KKT
conditionsarenecessaryandsufficientforoptimality. Recallthatweassumethatthesolutionz∗ is
positive, z∗ > 0foreachk. Also, lete beaonehotvector, i.e., azerosvectorexceptwitha1at
k k
indexk. MappingtheKKTconditionsontothisproblemyieldsthefollowing:
• Stationarity: 0∈∂u (z∗,x )−(cid:80) λ A +(cid:80) µ e
k −k j j j k k k
• Primalfeasibility: A z =b forallj
j j
• Dualfeasibility: µ ≥0forallk
i
• Complementaryslackness: −µ z∗ =0forallk
k k
where ∂u (z,·) is the subdifferential at z. Consider any primal-feasible point Az∗ = b. Given
k
our assumption that z∗ > 0, by complementary slackness and dual feasibility, each µ must be
k k
identically zero. This implies the stationarity condition can be simplified to 0 ∈ ∂u (z∗,x )−
k −k
(cid:80) λ A = ∂u (z∗,x )−A⊤λ. Rearrangingtermswefindthatforanyz∗,thereexistsλsuch
j j j k −k
that
(cid:88)
λ A ∈∂u (z∗,x ). (33)
j j k −k
j
Equivalently,elementsof∂u (z∗,x )areintherow-spanofA.
k −k
For the rest of the proof, we follow the derivation of the gradient projection method (Luenberger
etal.,1984, Sec12.4, p364). Let∇ ∈ ∂u beasubderivative(gradient), i.e., anelementofthe
k k
subdifferential.
18Notethat,ingeneral,wecanwriteanygradientasasumofelementsfromtherow-spanofAand
itsorthogonalcomplementd ,whichliesinthetangentspaceofthefeasibleset:
k
∇ =d +A⊤λ. (34)
k k
WemaysolveforλthroughtherequirementthatAd = 0, i.e., anymovementwithinthetangent
k
spaceofthefeasiblesetremainsfeasible. Thus
Ad =A∇ −(AA⊤)λ=0 (35)
k k
=⇒ λ=(AA⊤)−1A∇ (36)
k
=⇒ d =∇ −A⊤λ=[I−A⊤(AA⊤)−1A]∇ (37)
k k k
=Π (∇ ) (38)
TA k
where Π [I − A⊤(AA⊤)−1A] is the matrix that projects any gradient vector onto the tangent
TA
spaceofthefeasiblesetgivenbytheconstraintmatrixA.
Thefactthatelementsof∂u (z∗,x )areintherow-spanofAimpliesthat0 = d = Π (∇ )
k −k k TA k
necessarily,completingtheclaim.
E ENTROPY REGULARIZED LOSS BOUNDS EXPLOITABILITY
Our derived exploitability bound only requires concavity of the utility, bounded diameter of the
feasibleset,andlinearityoffeasibleconstraints(i.e.,feasiblesetissubsetofhyperplane). Inwhat
follow,letplayerk’slossbethenegativeoftheirutility,i.e.,ℓ =−u .
k k
Lemma7. Theamountaplayercangainbydeviatingisupperboundedbyaquantityproportional
tothenormoftheprojected-gradient:
√
ϵ (µ)≤ 2||Π (∇k )||. (39)
k TMk µk
Proof. Letzbeanypointinthefeasibleset. FirstnotethatΠ (z)=BzwhereB isanorthog-
TMk
onal projection matrix; this implies B2 = B = B⊤. Then by convexity of ℓ with respect to z,
k
ℓ (µ)−ℓ (z,µ )≤(∇k )⊤(z−µ ) (40a)
k k −k µk k
=(∇k )⊤Π (z−µ ) (40b)
µk TMk
(cid:124) (cid:123)(cid:122)
k
(cid:125)
∈TM
=(Π (∇k ))⊤ (z−µ ) (40c)
TMk µk k
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) √
√
Bz=B⊤z Diam(Mk)≤ 2
≤ 2||Π (∇k )|| (40d)
TMk µk
wherethefirstequalityfollowsfromthefactanytwopointszandµ lyinginthesamehyperplane,
k
by definition, form a direction lying in the tangent space of the hyperplane. The second equality
follows from symmetry of the projection operator and simply grouping its application to the left
√
handterm;wealsonotethatthefeasiblesetisasubsetofthesimplex,whichhasadiameterof 2.
Finally,thelaststepfollowsfromCauchy-Schwarz.
Theorem2(LowTemperatureApproximateEquilibriaareApproximateNashEquilibria). Let∇kτ
µk
be player k’s entropy regularized gradient and µ be an approximate equilibrium of the entropy
regularizedgame. Thenitholdsthat
√
ϵ =ℓ (µ)−ℓ (BR ,µ )≤τlog(|S||A |)+ 2||Π (∇kτ)||. (41)
k k k k −k k TMk µk
19Proof. Beginningwiththedefinitionofexploitability,wefind
(cid:0) (cid:1)
ℓ (µ)−ℓ (BR ,µ )= ℓ (x)+τS(µ )−τS(µ ) (42a)
k k k −k k k k
(cid:0) (cid:1)
− ℓ (BR ,µ )+τS(BR )−τS(BR )
k k −k k k
=ℓτ(µ)−ℓτ(BR ,µ )+τ(cid:0) S(µ )−S(BR )(cid:1) (42b)
k k k −k k k
≤ℓτ(µ)− min ℓτ(z,µ )+τ max S(z′) (42c)
k k −k
√
z∈Mk z′∈Mk
≤ 2||Π (∇kτ)||+τ max S(z′) (42d)
√
TMk µk
z′∈Mk
≤ 2||Π (∇kτ)||+τlog(|S||A |) (42e)
TMk µk k
wherethesecondequalityfollowsfromthedefinitionofplayerk’sentropyregularizedlossℓτ,the
k
first inequality from nonnegativity of entropy S, the second inequality from convexity of ℓτ with
k
respecttoitsfirstargument(Lemma7),andthelastfromthemaximumpossiblevalueofShannon
entropyoverdistributionson|S||A |elements.
k
Corollary 2 (Lτ Scores Nash Equilibria). Let Lτ(µ) be our proposed entropy regularized loss
functionandµbeanystrategyprofile. Thenitholdsthat
(cid:16) (cid:17) (cid:112)
ϵ≤τlog |S|n|A| + 2nLτ(µ). (43)
Proof. BeginningwiththedefinitionofexploitabilityandapplyingLemma2,wefind
ϵ=maxmaxℓ (µ)−ℓ (z,µ ) (recalleachϵ ≥0) (44a)
k k −k k
k z
(cid:88)
≤ maxℓ (µ)−ℓ (z,µ ) (44b)
k k −k
z
k
(cid:88)(cid:104) √ (cid:105)
≤ τlog(|S||A |)+ 2||Π (∇kτ)|| (44c)
k TM µk
k
√
(cid:88) (cid:88)
=τ log(|S||A |)+ 2||Π (∇kτ)|| (44d)
k TM µk
k k
(cid:16) (cid:17) (cid:88)√
≤τlog |S|n|A| + 2||Π (∇kτ)|| (44e)
TM µk 2
k
(cid:16) (cid:17) √ (cid:12)(cid:12) (cid:12)(cid:12)
=τlog |S|n|A| + 2(cid:12)(cid:12)||Π (∇1τ)|| ,...,||Π (∇nτ)|| (cid:12)(cid:12) (44f)
(cid:12)(cid:12) TM µ1 2 TM µn 2(cid:12)(cid:12)
1
(cid:16) (cid:17) √ (cid:12)(cid:12) (cid:12)(cid:12)
≤τlog |S|n|A| + 2n(cid:12)(cid:12)||Π (∇1τ)|| ,...,||Π (∇nτ)|| (cid:12)(cid:12) (44g)
(cid:12)(cid:12) TM µ1 2 TM µn 2(cid:12)(cid:12)
2
(cid:16) (cid:17) √ (cid:115) (cid:88)
≤τlog |S|n|A| + 2n ||Π (∇kτ)||2 (44h)
TM µk 2
k
(cid:16) (cid:17) (cid:112)
=τlog |S|n|A| + 2nLτ(µ). (44i)
F GAME MATRICES
Weprovidethepayoffsfortheprisoner’sdilemmaandBach-Stravinskygameusedinexperiments.
C D C D
C −1,−1 −3,0 =⇒ C 2/3,2/3 0,1
D 0,−3 −2,−2 D 1,0 1/3,1/3
Figure4: Prisoner’sDilemmaGame: Weshiftandnormalizethepayoffsoftheprisoner’sdilemma
game(left)tobepositiveandwithmaximumvalue1(right).
G OBSTACLES TO A GENERAL CMG NE-EXISTENCE PROOF
Table5summarizestheobstaclestoanNE-existenceproofforgeneralconcaveutilities.
20C D
C 3,2 0,0
D 0,0 2,3
Figure5: Bach-StravinskyGame.
Theory N.C.ofu (π ) N.C.ofBR N.C.ofU U ̸=( n U ) Deviations
i i i=1 i
(cid:34)
Kakutani NA × NA NA NA
ConcaveGames × NA × NA NA
HiddenGames NA NA NA × NA
AbstractEconomy NA NA NA NA ×
Table5: SummarizationofobstaclestovariousNashequilibriumexistenceprooftechniques. N.C.
standsfornon-convexityornon-concavitywhereappropriate. An×meansthetheoryfailsbecause
oftheissue. “Deviations”meansthatanotionofreasonableplayerdeviationsintheMDParenot
allowedbythetheory.
G.1 NON-CONVEXITYOFBESTRESPONSECORRESPONDENCES
We consider a simple 2-player, 2-state, 2-action, symmetric convex Markov game. If both agents
selectaction0,theytransitionfromtheircurrentstatetotheotherstate;otherwise,theyremainput.
Wesetγ =0.95andtheinitialstatemeasureµ tobeuniform.
0
Letplayer2employoneofthefollowingnearbypolicies(I-III):
(cid:26) (cid:26) (cid:26)
0.40 ifs=s 0.45 ifs=s 0.40 ifs=s
πI(a |s)= 0 , πII(a |s)= 0 , πIII(a |s)= 0
2 0 0.80 else 2 0 0.80 else 2 0 0.75 else
whereplayer2’sprobabilitiesforactiona areimplied.
1
In addition, consider the following “safe” MARL problem where we designate safe long-run vis-
itation measure of certain states (e.g., Bach-Bach or Stravinsky-Stravinsky) and usage of certain
actions (e.g., a safe range of motor torques); equivalently, we rule out certain unsafe states and
actions. Note this framework could also be used to tackle the fairness application we explored in
Section5.3.Asanexampleof“safe”MARL,definethefollowingconvexloss(negativeutility)over
occupancymeasuresforplayer1:
−u i(µ i)=ℓ i(µ i)=max(0,||µ a−t a|| ∞−1/20)+max(0,||µ s−t s|| ∞−1/4) (45)
whereµ andµ areplayer1’sactionandstatemarginalsrespectively. Thetargetmeasurest and
a s a
t
s
are used along with the radii 1/20 and 1/4 to encode the regions of “safe” occupancy measures.
Ifplayer1deviatesfromeitherregionbymorethantheradius(asmeasuredbytheinfinitynorm),
thentheyaccruealoss. Otherwise,player1’slossiszero.
Figure6revealstheshapesofplayer1’sbestresponsecorrespondencesinthissetting, showingin
particular,thatitsimageinpolicyspaceisnon-convexforeachofthepolicies. Thisfeatureofthe
problembreakstheassumptionsoftheKakutanifixedpointtheoremrenderingaNashequilibrium
existenceproofnon-trivial,yetdesireable.
G.2 NON-CONVEXITYOFJOINTSTRATEGYSET
We again consider the same convex Markov game as in Section G.1. Note that the feasible set of
joint occupancy measures is entirely defined by the parameters of the transition kernel, discount
factor,andinitialstatemeasure(rewardisirrelevanttotheBellmanflowconstraints).
Thesetofjointoccupancymeasuresisnon-convex. Thisbreakstheassumptionmadeinthetheory
of n-player concave games (Rosen, 1965). It also breaks most known results on monotone quasi-
variationalinequalities(Kim,2021;Facchinei&Kanzow,2007).
21(a)OccupancyMeasureSpace(I) (b)PolicySpace(I)
(c)OccupancyMeasureSpace(II) (d)PolicySpace(II)
(e)OccupancyMeasureSpace(III) (f)PolicySpace(III)
Figure 6: Non-convexity of best response correspondences (black regions) in policy space under
concaveutilitiesfora“safe”MARLapplication. Romannumeralsindicatethepolicyemployedby
player2ineachplot.
22Figure7:(Thesetoffeasibleoccupancymeasuresisnon-convex)Tobeabletovisualizethefeasible
set,weconsidersymmetricoccupancymeasures(occupancyisthesameforbothplayers). Webuild
aprobabilitymeshrangingfrom0.01probabilityto0.99probabilityforallstate-actionoccupancies
µ(s,a),andplotallpointswherethenormoftheconstraintviolationvectorisbelow0.01. Wethen
identify two points in this set and draw a line segment between them. The midpoint of this line
segmentisaconvexcombinationofthetwoendpointsandshouldnecessarilylieinthefeasibleset
ifitisconvex. However, asonecanclearlyseefromthefigure, themidpointlieswelloutsidethe
feasiblesetandwecanconfirmitsconstraintviolationnormisabove0.3.
G.3 HIDDENGAMES
Inhiddengames(Sakosetal.,2024),players’strategyspacesareindependentlymappedfromsome
complex space to a latent strategy space where payoffs are directly calculated. In (Sakos et al.,
2024, Definition 1), they assume there exists a map µ = µ (π ) for our setting. Note, in cMGs,
i i i
µ =µ (π ,...,π ),andsoitfailsthiscondition.Inaddition,theyassumealatentspaceU = U ,
i i 1 n i i
i.e.,thejointstrategysetisaproductspace. ThisalsofailsincMGs. (cid:34)
G.4 ABSTRACTECONOMIES
Inanabstracteconomy(Arrow&Debreu,1954),players’strategysetsmaydependontheactions
chosenbyotherplayers;generally,eachplayeristaskedwiththeproblem:
max u (µ ,µ )
i i −i
µi∈M′ i(µ−i)
whereM′(µ )mapseveryprofileofopponentoccupancymeasurestoafeasiblesetofoccupancy
i −i
measures for player i, the so-called action correspondence. Note that unlike M (µ ) defined in
i −i
Section 3, M′(µ ) must respect the occupancy measure constraints of all players. Intuitively,
i −i
playeri’sdeviationtoanoccupancymeasureµ′ cannotresultinamodificationtoµ foranyj ̸=i.
i j
Allplayerssharethesamestateoccupancymarginalssothismeansthatplayericannotchangetheir
long-run state visitation measure by deviating. This is overly restrictive as it means that player i
cannotincreasetheirutilityinmanygamesofinterest,forexampleingameswithrewardfunctions
that depend only on state. For this reason, we consider deviations in occupancy measures, and in
turn,theabstracteconomymodelinsufficientforcMGs.
23