Altogether: Image Captioning via Re-aligning Alt-text
HuXu1,Po-YaoHuang1,XiaoqingEllenTan1,
Ching-FengYeh1,JacobKahn1,ChristineJou1,
GargiGhosh1,OmerLevy1,LukeZettlemoyer1,2,Wen-tauYih1,
Shang-WenLi1,SainingXie3 and ChristophFeichtenhofer1
1MetaFAIR 2UniversityofWashington 3NewYorkUniversity
https://github.com/facebookresearch/MetaCLIP
Abstract abouttheimages,suchasthebreedofthedogor
thespecificnameorlocationofthepark.
Thispaperfocusesoncreatingsyntheticdata
Moreover,whilealternativetext(alt-text)inweb-
toimprovethequalityofimagecaptions. Ex-
crawleddataoftencontainsdetailedandconcrete
istingworkstypicallyhavetwoshortcomings.
First,theycaptionimagesfromscratch,ignor- visualdescriptions,currentcaptioningmodelsgen-
ingexistingalt-textmetadata,andsecond,lack erallyignorethisinformation. Instead,thesemod-
transparency if the captioners’ training data els tend to generate captions solely based on the
(e.g.GPT)isunknown. Inthispaper,westudy imagecontent,whichmissestheopportunitytoen-
aprincipledapproachAltogetherbasedonthe
hancetherelevanceandaccuracyofthecaptions.
keyideatoeditandre-alignexistingalt-texts
Additionally, advancements in caption quality
associatedwiththeimages. Togeneratetrain-
often lack transparency and are not easily repro-
ingdata,weperformhumanannotationwhere
ducible. Forinstance,recentdevelopmentssuchas
annotatorsstartwiththeexistingalt-textandre-
alignittotheimagecontentinmultiplerounds, LLaVA(Liuetal.,2024b)andShareGPT4V(Chen
consequently constructing captions with rich etal.,2023b)utilizehigh-qualitycaptionsderived
visualconcepts. Thisdiffersfrompriorwork fromproprietarymodelslikeGPT-4V.Whilethese
thatcarriesouthumanannotationasaone-time
modelsbenefitfromhigh-qualityannotations,they
description task solely based on images and
are built on processes that are not openly shared.
annotator knowledge. We train a captioner
This lack of disclosure presents significant chal-
onthisdatathatgeneralizestheprocessofre-
lenges in terms of scalability, intellectual prop-
aligning alt-texts at scale. Our results show
ourAltogetherapproachleadstoricherimage erty rights, data integrity and privacy. The use
captionsthatalsoimprovetext-to-imagegener- ofsuchproprietarymodelsinindustryapplications
ationandzero-shotimageclassificationtasks. isfraughtwithrisks,particularlywhentheimple-
mentationdetailsremainundisclosed.
1 Introduction
Thispaperpresentsaprincipledapproachtoen-
Humansocialinteractionsoftengravitatetowards hance caption quality and develops a parameter-
engaging with individuals who exhibit a higher efficientcaptionercapableofscalingre-captioning
levelofintelligence. Thisinherentsocialbehavior efforts. Weassumeeachimagecontainsinforma-
underscorestheaspirationtodevelopAIagentsthat tionthatthecaptionneedstoalignwithusingnat-
surpasstheaveragehumanintelligence. Thepur- urallanguage. Althoughobtainingthereal-world
suitofcreatingsuchadvancedAIagentshingessig- information from an image or generating a per-
nificantlyonthequalityofthetrainingdata,which fectground-truthcaptionmightbechallenging,we
ideallyencapsulatessuperhumanintelligence. demonstratethatcaptionqualitycanbeimproved
However, in the context of image captioning, relativelybyiterativelyrefiningcaptionstobetter
most existing training data is designed for naive describe the visual content (e.g., adding informa-
andwell-knownvisualconceptsthatprovidelittle tiononspecificobjects,colors,spatialrelationsor
valuetoanaverageuser, e.g., acaption“adogis morefine-grainednamedentities).
walkinginthepark”offerminimalutilitytomost Ourkeyinsightisthatthecreatorwhopostsan
usersunlessspecificaccessibilityneedsarepresent, imagealongwithitsassociatedalt-textislikelythe
e.g.,forindividualswithvisualimpairments. The mostknowledgeableexpertregardingtheconcrete
primaryissuewiththesecaptionsliesintheirlack visual concepts within that image (e.g., knowing
ofdetail;theyfailtoconveynuancedinformation thattheanimalisan"iguana"insteadofjustan"ob-
4202
tcO
22
]VC.sc[
1v15271.0142:viXraRound 1(alt-text): “common iguana, 06/01/2004”
Round 2
Round 2: “A photo of an iguana with grey head and green body.”
Round 1 .
(alt-text) Round N ..
…
Round n: “A photo of an iguana with grey head and green
body, climbing on a brown tree branch to the right.”
Figure1: AVenndiagramillustratingcaptionqualityimprovementviamultipleroundsofre-aligningprevious
captions(startingfromalt-text)totheimage.
ject," "animal," or "lizard"). It would be difficult obtain1.1%absoluteaccuracyimprovementover
foranaverageannotatortoprovidesimilarlevelof 26zero-shotclassificationdatasetsanda3%gain
detailwithinashortannotationtimeframe. Instead, onretrievaltasks,whenusingsyntheticcaptionsto
theseannotatorscouldofferweakyetcomplemen- supplementCLIPtraining. Aninterestingobserva-
tary supervision by either removing non-existent tionwemakeisthatgenerativeanddiscriminative
information from the alt-text or describing miss- tasksrequirewidelydifferentratios(100%vs.15%)
ingobjectsusingmoregeneralconcepts("lizard" ofsyntheticdata.
insteadof"iguana").
2 RelatedWork
Building on this insight, we introduce
Altogether,anapproachtoimproveimagecaptions
SyntheticDataandImageRe-captioning. Syn-
throughtheprocessofre-aligningexistingalt-texts
thetic data has recently regained popularity
with the image content. We instantiate this idea
(Nguyen et al., 2024; Li et al., 2023b) with
in two forms (i) through human annotation to
DALL·E 3 (Betker et al., 2023) replacing low-
create a fine-tuning dataset and (ii) through a
quality web data with synthetic data for learning
parameter-efficient captioner that can re-caption
imagegenerators. Sincethealt-textofwebimages
billionsofimageswhenfine-tunedforthistask.
serves various purposes and may not fully align
Forannotation(i),weperformmultiplerounds
withtheimagestheydescribe,DALL·Emixesalt-
ofalt-textrealignmenttopreserveconcretevisual
textswithsyntheticcaptionstopromotebettercon-
conceptswhileaddingorremovingrelevantinfor-
trol in image generation. Early work (Chandu
mation, as depicted in Fig. 1. Starting with the
et al., 2020) uses sub-selecting content words as
initial alt-text, which may partially overlap with
skeletonstohelpgeneratingimprovedanddenoised
theimage,subsequentannotationroundsiteratively
captions. Another very recent line of concurrent
refinethecaptionstoachievebetteralignmentwith
research uses LLMs to fuse or combine alt-texts
the image’s information. Using this data, we can
withcaptionsgeneratedfromanoff-the-shelfcap-
trainacaptioner(ii)thatiscapableofgeneralizing
tioner(Laietal.,2024;Yuetal.,2024). However,
thisprocessbyreading,grounding,andtransform-
thefusionisinlanguagespaceonlyandhasnoac-
ingalt-textsintodensecaptionsatscale.
cesstotheimageforalignment. Theresultingtext
Weevaluateourre-alignedcaptionsacrosscap-
mayincludeinformationnotpresentintheimage
tioning,generativeanddiscriminativetasks. With
and the fusion behavior of the LLM is unknown
alightweighttextdecoder,ourcaptionersurpasses
foralt-texts. SeeTable3forpotentialissuesofnot
alt-texts by 4% in CLIP (Radford et al., 2021)
usingvisioninformation.
score and outperforms state-of-the-art captioners
onachallengingtestset,whichweannotatebased Dense Captioning. While image captioning is
on a subset of the WIT (Wikipedia Image-Text) well-studied,generatingdensecaptionsprecisely
dataset(Srinivasanetal.,2021). Wefurthereval- alignedwiththeoriginalimageshasgainedmore
uate our approach on text-to-image (T2I) genera- attention recently. MSCOCO-style captions (Lin
tion,whereweobservesignificantimprovements et al., 2014) are brief and describe main objects,
in similarity between generated images and text limitingtheirvalueforalignedimage-textpairsdue
promptswhentraininglatentdiffusionmodelswith totheirbrevity,generalconcepts,andconstrained
synthetic captions. For discriminative tasks, we imagedistribution. TheDCIdataset(Urbaneketal.,2023)overcomesthebrevityissuebutstillsuffers where i represents an image, F(i) its encoding
from the other limitations. DOCCI (Onoe et al., (e.g.,CLIP),t theprecedingcaptiontokens,
j−k:j−1
2024)andImageInWords(IIW)(Gargetal.,2024) andΘtheparametersofthecaptioner. Theprocess
addressthesechallengesforspecificdatasetsusing involvesencodingtheimageintoalatentspaceand
clusteringoriterativerefinementwithobjectdetec- sequentiallydecodingthecaptiontokens.
tion tools. Our work proposes a general process
3.2 Re-aligningPreviousCaptions
toimprovecaptionqualityforwebimages,paving
thewayforfurtheradvancementsinthisarea. Toenhancecaptionaccuracy,weconditionthecap-
tioneronpreviouscaptions(e.g.,alt-texts),
RetrievalAugmentedGeneration. Realigning
alt-texts inherently grounds the captioner on in- (cid:88)
L(t,t′,i) =
putalt-texts,whichisanalogoustoRetrievalAug-
j
mentedGeneration(RAG)(Lewisetal.,2020;Gao
logP(t |t ,...,t ;t′ ;F(i);Θ), (2)
et al., 2023) in terms of taking additional knowl- j j−k j−1 1:m
edgeasinput. ImagecaptioningalsoadoptsRAG
where t′ are tokens from the previous caption.
forcaptiongeneration (Ramosetal.,2023;Yang 1:m
Thisre-alignmentaimstorefineandbetteralignt′
et al., 2023). Our captioner shares similar advan-
withtheimagecontenti.
tages, such as a parameter-efficient, lightweight
modelfortrainingandinferenceatscale,reduced 3.2.1 Annotation
factoidhallucination, andupdatingknowledgeat We improve caption quality through iterative hu-
inferencetimeunavailableduringtraining. man annotation, refining previous captions (alt-
texts) in multiple rounds. Starting with an initial
HumanPreferenceAlignment. Imagecaption-
alt-textascaptiont,thenextrounduses:
ing, as an alignment problem between captions
andcorrespondingimages,relatestoalignmentfor t′ ← t. (3)
human preference (Ouyang et al., 2022). How-
ever,imagecaptioningalignmentismoreobjective Thisiterativeprocessisdesignedbasedonthefol-
duetothecleartargetofaligningwithinformation lowingobservations: (i)thecreatorofalt-textsis
present in the image, whereas human preference possiblythebestexpert/annotatorwhocandescribe
alignmentissubjective,aspreferencescanbeun- the image in fine-grained visual concepts, and it
definedandvaryamongindividuals. could be very challenging later for an annotator
tounderstandandcaptiontheimageatthatdetail
3 Altogether: Re-aligningAlt-texts
(e.g.,identifyandspecify“iguana”inthecaption);
(ii)itisalsochallengingforanannotatortowritea
This section presents our method for re-aligning
detailedcaptionfromscratch,comparedtostarting
alt-texts to produce dense captions with concrete
fromexistinginformation.
visualconcepts,whichwelater(§4)instantiatein
Inexperiments,weshowthatthisiterativepro-
aparameter-efficientcaptionerscalabletobillions
cess of re-aligning improves the annotated data,
of images. We structure this section into three
captioner,anddownstreamperformanceafterdif-
mainparts: (§3.1)revisitingtheimagecaptioning
ferentroundsofannotation.
task,(§3.2)incorporatingre-alignmentintoexist-
ing captioning frameworks, as well as designing
3.2.2 Learning
annotationtasks(§3.2.1)andlearningmechanisms
We design a captioner to learn the process of re-
(§3.2.2)forre-aligningalt-texts.
aligning alt-texts. We build on a simple prefix
language model, ClipCap (Mokady et al., 2021),
3.1 ImageCaptioning
thatconnectsaCLIPencoderandatextdecodervia
Weformulateimagecaptioningbypredictingcap-
mappingnetworktoimplementeq.(1),seeFig.2.
tion tokens conditioned on the latent space of an
imageembedding. Thelossfunctionisdefinedas: Mapping Network. The mapping network is a
TransformertakingCLIPembeddingsasinputand
(cid:88)
L(t,i) = logP(t |t ,...,t ;F(i);Θ), produces visual tokens of fixed length (40 is de-
j j−k j−1
fault) that can be fed into a text decoder as the
j
(1) “imageprompt”.“A photo of an iguana with grey head and green
body, climbing on a brown tree branch to the right.”
Text Decoder
Mapping Visual Alt-text
CLIP
Network Tokens Tokens
“common iguana”
Figure 2: Re-aligning alt-texts: Our captioner takes visual and alt-text input. We extract frozen CLIP image
embeddingsandtransformitintoafixednumberofvisualtokens. Givenalt-text,thedecoderisabletogroundthis
information,e.g. carryingconcretevisualconcepts,togenerateabettercaptionthatisalignedwiththeimage.
Re-aligning Alt-Texts. To model inputs on alt- 4.2 CaptionerArchitecture
texts, we simply append m tokens from alt-texts,
Image Encoder. We choose the pretrained
after the visual tokens. The training loss is only
MetaCLIPViT-H/14(Xuetal.,2024)astheimage
computedontokensfromgeneratedcaptions,ex-
encoder, which outputs a single embedding with
cludingtokensfrombothvisualandalt-texttokens,
1024 dimensions. The image embedding is then
asshowninFig.2. Notethealt-textscanbeempty
transformedinto40visualtokensviathemapping
stringswhenthesearenotavailable.
networktoserveastheimagepromptforthetext
decoder. Wefreezetheimageencoderduringthe
4 Altogether: ImplementationDetails
trainingphaseandonlytrainthemappingnetwork.
Inthissection,wefirstdiscusstheannotationand
training data for our captioning model in §4.1. Text Decoder. We adopt a trainable OPT
Thenwedescribecaptionerarchitecturein§4.2. 1.3B (Zhang et al., 2022) as the text decoder for
efficienttrainingandinference(e.g.,comparedto
4.1 Dataset Llama-13B,thethroughputofthisarchitectureis
13×faster, see Table 9). We append m = 128 to-
Weuseapre-training+fine-tuningframeworkto
kens from alt-texts after visual tokens and allow
trainthecaptioner,wherethegoalofpre-training
amaximumof256tokensforgeneratedcaptions.
is to learn diverse visual concepts and the later
Thisextendsthetotallengthofdecodertobe424
fine-tuninglearnstore-alignalt-textsasresulting
(40visualtokens+128alt-texttokens+256gen-
captions.
erated tokens). For alt-text tokens, we randomly
sampleeitheralt-textoremptytextduringtraining.
Pre-trainingSet Forpre-training,werandomly
The empty text allows the captioner to generate
select 22M image-alt-text pairs from the Meta-
captionsfromscratch,incasethealt-textsarenot
CLIP (Xu et al., 2024) dataset. This data covers
availablefortheimage. Wepre-trainthecaptioner
long-tailedvisualconceptsinalt-textswhichtypi-
for1epochandfine-tuneonannotateddatafor4
callyanaveragehumanannotatorcannotinferfrom
epochs. Detailedhyperparametersarein§F.
theimagecontent.
Fine-tuning/Annotated Set. We build a fine- 5 Evaluation
tuning set (called altogether-ft) to learn and gen-
eralize thecapability of re-aligningalt-texts. We Ourevaluationspansthreeareas: (i)humananno-
collect23kimagesandhave3roundsofannotation tations,(ii)captionsgeneratedfromourcaptioner,
(includingalt-textsasthefirstround). Wechoose and(iii)downstreamtasksusingoursyntheticcap-
twoimagesources: 15kimagesfromWITand7k tions(i.e.,text-to-imagegenerationandzero-shot
imagesfromMetaCLIP(Xuetal.,2024). Weuse imageclassification).
thesetwosourcestoensurerichvisualconceptsin
alt-textsandgoodcoverageonwebimagesinorder
5.1 AnnotatedData
tomitigatetheriskofinferenceonout-of-domain
images. WeshowtheannotationguidelinesinAp- We analyze annotations in terms of length (num-
pendix§Aandside-by-sidecomparisonofmultiple ber of words), edit distance (between annotation
roundsofannotationinTable14andTable15. rounds),andCLIPimage-textalignmentscore.Captioner CLIPScore BLEU1 METEOR ROUGE CIDEr NPF1 NPPrecision NPRecall
alt-text(Round1) 29.3 5.1 9.5 17.8 4.7 13.5 9.3 36.5
GiT 26.3(-3.0) 0.0(-5.1) 2.1(-7.4) 7.3(-10.5) 0.0(-4.7) 1.8(-11.7) 1.0(-8.3) 11.3(-25.2)
BLIPv2 28.0(-1.3) 0.2(-4.9) 4.1(-5.4) 13.0(-3.8) 0.0(-4.7) 4.2(-9.3) 2.5(-6.8) 14.4(-22.1)
LLaVAv1.6 27.0(-2.3) 27.7(+22.6) 10.5(+1.0) 20.2(+2.4) 4.9(+0.2) 5.8(-7.7) 5.5(-3.8) 6.7(-29.8)
GPT-4V 27.4(-1.9) 26.7(+21.6) 10.0(+0.5) 17.4(-0.4) 3.7(-1.0) 4.4(-9.1) 4.4(-4.9) 4.9(-31.6)
GPT-4V-turbo 27.3(-2.0) 21.4(+16.3) 9.0(-0.5) 17.3(-0.5) 4.4(-0.3) 4.4(-9.1) 4.0(-5.3) 5.5(-31.0)
GPT-4o 28.3(-1.0) 18.8(+13.7) 8.8(-0.7) 17.7(-0.1) 4.0(-0.7) 5.0(-8.5) 4.3(-5.0) 7.0(-29.5)
Altogether(2)w/alt 33.1(+3.8) 50.0(+44.9) 21.5(+12.0) 37.9(+20.1) 48.2(+43.5) 24.0(+10.5) 24.1(+14.8) 25.4(-11.1)
Altogether(3)w/oalt 32.4(+3.1) 45.7(+40.6) 18.7(+9.2) 34.1(+16.3) 27.7(+23.0) 19.2(+5.7) 18.9(+9.6) 20.9(-15.6)
Altogether(3)w/randalt 29.4(+0.1) 44.6(+39.5) 18.0(+8.5) 33.0(+15.2) 24.5(+19.8) 18.7(+5.2) 18.7(+9.4) 20.0(+16.5)
Altogether(3)w/alt 33.3(+4.0) 49.6(+44.5) 21.9(+12.4) 39.1(+21.3) 55.6(+50.9) 25.2(+11.7) 24.9(+15.6) 27.3(-9.2)
Table 1: Evaluation of captioners on a separate test set created from the WIT dataset. We evaluate the CLIP
image-textalignmentscore,captioningmetricswhichmeasurealignmentofthemodelcaptionswithground-truth
humanannotatedcaptions: BLEU/METEOR/ROUGE/CIDErandnounphrase(NP)F1,precision,andrecall.
Altogether(2/3)indicatesourcaptionerfine-tunedonround2/3annotation;‘w/oalt’meanscaptioningfromscratch
withnoalt-text(similartootherbaselines),‘w/randomalt’meanscaptioningwithrandomlypairedalt-textsand‘w/
alt’meanscaptioningviare-aligningalt-texts.
Altogether Round 3 Altogether Round 2 GPT-4o GPT-4v LLaVA-NeXT 1.6 Tie
better alignment 40.8 25.5 10.2 9.2 7.1 7.1
/less hallucination
better specificity 38.8 35.7 3.13.13.1 16.3
/named entities
alt-text 37.8 29.6 4.1 4.1 4.1 20.4
information
Figure3: Humanevaluationongeneratedcaptionsonbetteralignment/lesshallucination(“whichcaptionhasthe
bestalignmentwiththeimageandleasthallucination”),specificity(“whichcaptioncontainsmorenamedentities”)
andusefulnessofalt-textinformation(“whichcaptioncontainmostusefulinformationfromalt-texts”).
Annotation Length EditDist. Alignment Weuse3versionsofourcaptioner,afterfinetuning
Round1(alt-text) 13.0 - 30.1 Round2/3annotations,aswellaswith(w/alt)and
Round2 81.7 403.8 33.7
without(w/oalt)feedingalt-text.
Round3 83.2 92.9 33.9
Wefirstevaluatethealignmentbetweentheim-
Weobservethatmultipleroundsofannotation(on
ages and captions via CLIP score (Hessel et al.,
topofthealt-text)increasesthecaptionlengthand
2021) (this metric ignores the ground-truth cap-
image-text alignment (CLIP score), with smaller
tionsandonlyusesCLIPsimilarityasmetric). The
changesinsubsequentrounds. Thisisalsoreflected
resultsaresummarizedinTable1,secondcolumn.
by the lower edit distance in the final round. We
OurAltogether captionerimprovesoveralt-texts
showfurtherannotationexamplesinAppendix§B.
by4%onCLIPscoreandsignificantlyoutperforms
off-the-shelfcaptionerssuchasGiT(Wangetal.,
5.2 Captioner
2022;Lietal.,2023a),BLIP2(Lietal.,2023a)and
Human-annotatedTestSet. Webelievethatex- LLaVA (Liu et al., 2024b,a). It also outperforms
istingdatasetssuchasMSCOCOcaptionsarenot proprietarycaptionerssuchasGPT-4V(OpenAI,b)
sufficientforevaluation,sincethesedonotcontain andGPT-4o(OpenAI,a). Thecaptionsgenerated
fine-grainedinformation,e.g. acaption“adogsit- byourcaptionertrainedwithRound3annotation
tinginapark”doesnotcontaininformationabout withoutalt-textsisworsethanwithalt-texts. This
the dog breed or park. Further, existing works implies that employing alt-texts is important for
(Moon et al., 2023; Onoe et al., 2024) show per- improvingimage-textalignment.
formanceonsuchbenchmarkscorrelatesinversely Next,wecomparethegeneratedcaptionsagainst
withcaptionquality. Therefore,weannotateatest the ground-truth provided by the annotators. We
set,consistingof500imagesfromtheWITdataset useBLEU/METEOR/ROUGE/CIDErmetricsand
using our 3-round annotation approach and com- noun phrase (NP) precision, recall and F1 score.
pare our captioner to state-of-the-art captioners. We use spaCy https://spacy.io to get twosets of NPs from generated and ground-truth 2/3data. Weuse3evaluatorsand100imagesfrom
captions, respectively; then we compute the in- WIT. The results are in Fig. 3. Humans highly
tersection of these two sets as true positives. prefer Altogether, and Round 3 further improves
We observe that Altogether significantly outper- overRound2,overthethreecriteria: Altogetheris
forms existing captioners. Non-dense caption- also much better in (i) producing aligned image
ers (e.g., GiT or BLIPv2) are struggling to fully captionswithouthallucination(ii)describingim-
describe the image with enough visual concepts ages more specifically, (iii) we see alt-texts con-
(e.g.,seeBLIPv2’slowscoresacrossallmetrics). tainusefulinformationandcaptioningfromscratch
Altogetheralsooutperformsdensecaptioners(GPT- (LLaVA1.6,GPT-4V/o)strugglestodescribethis.
4V/oorLLaVAv1.6),evenifourmodelisnotpro- To qualitatively understand the behavior of re-
vided with the alt-text. If we provide the model aligningalt-texts,wefurtherpromptthecaptioner
with the alt-text we see a further boost in perfor- withdifferentalt-textsonimagesfromImageNet,
mance. This can be explained by the long-tailed shown in Table 3. We try 3 different styles of
visualconceptspresentinalt-texts(Xuetal.,2024), alt-textprompting: (i)emptystring,(ii)ImageNet
whichisdifficultfordensecaptionerstodescribe classname,(iii)incorrectalt-texts. Wecanseethat
purelyusingimageinformation. Altogethercancarryoverconcretevisualconcepts
andcorrectthehallucinated/wrongvisualconcepts
Low Performance of GiT and BLIPv2. We
inredthatcaptioningfromscratch(emptystring)
further investigate 0.0 CIDEr scores of GiT and
has. It further rejects alt-texts that are incorrect
BLIPv2. One reason is from using long-tailed
(e.g.,alt-text“abird”thatisnotpresenttheimage).
densecaptions(averagingover80words)asrefer-
encetocomputeCIDErthatpenalizingshortcap-
5.3 Text-to-image(T2I)Generation
tions because CIDEr has a length penalty. Also,
Setup. Weutilizere-aligned(synthetic)captions
bothGiTandBLIPv2aretrainedontheMSCOCO
fortrainingtext-to-imagegenerativemodels. Us-
dataset, which typically features captions of less
ingsyntheticdatawasshowninDALL·E3(Betker
than 10 words focused on common objects. We
et al., 2023) to be highly effective for generat-
further fine-tune GiT on altogether-ft set for fair
ing images. We use DiT-XL as the model and
comparison,showninTable2. GiTisstillfarleft
CC-12M(Changpinyoetal.,2021)asthetraining
behind Altogether, probably because of lacking
dataset. Wetrainthemodelfromscratchundera
alt-textspre-training. Moreover,theWITdataset
controlled setup to compare the performance dif-
includes many out-of-domain images for which
ferencebetweenusingoriginalcaptionsandusing
thesemodelsarenotoptimized,leadingtopartial
re-aligned (synthetic) captions as the text inputs.
recognitionissues(e.g.,recognizing“sandonthe
We train on CC-12M for 24 epochs on 32 A100
beach”butfailingtodetailitfurther). Occasionally,
GPUs. DetailsareinTable11.
thismismatchintrainingandtestingalsoresultsin
thegenerationofunreadablecaptions.
Results. WetrainT2Imodelswithdifferentmix-
ingratiospofsyntheticcaptionsandoriginalcap-
Baseline CLIPScore BLEU1 METEOR ROUGE CIDEr
tion. During inference, following the evaluation
GiT(MSCOCO) 26.3 0.0 2.1 7.3 0.0
GiT(3)w/oalt 26.5 17.6 13.5 19.8 0.0 setup in DALL·E 3, we apply either the origi-
nalprompt(alt-text)orthedescriptive(synthetic)
Table2: Fine-tuningGiTonaltogether-ftset.
promptasthetextprompttogenerateimage. We
reportCLIPscorestoevaluatethesimilaritiesbe-
HumanStudy. Wefurtherconducthumaneval- tweenthegeneratedimagesandthecorresponding
uationbypresentingtheimages,alt-textsandthe textpromptsonaholdoutCC-12MsetinTable5.
captionsproducedbyvariousmodels,andasking We compare T2I models trained on alt-texts
evaluatorsaboutthreecriteria: Whetherthecaption (Round1),syntheticcaptionwithandwithoutalt-
(i)isalignedwiththeimage&hasfewerhalluci- texts grounding. Similar to DALL·E 3, we first
nations; (ii) is specific (named entities, detailed train T2I model with a high mixing ratio p=0.95
description);(iii)carriesusefulinformationfrom ofsyntheticdataw/oalt-texts,mixedwithoriginal
the alt-text. We evaluate 5 captioners with ran- data (alt-texts). Training with synthetic captions
domorderwhenpresented: LLaVAv1.6,GPT-4V, improve the CLIP score by 1.3% (29.3 vs 28.0).
GPT-4o, and our Altogether trained with Round ThenwetrainaT2Imodelwith100%(p=1.0)syn-Image Alt-Texts GeneratedCaptions
“” Aclose-upphotoofaNorthernSaw-whetOwl(Ae-
goliusnivalis)inanaturalhabitat...
“greatgrayowl,Strixnebulosa” Aclose-upphotoofaGreatGrayOwl,Strixnebu-
losa.Theowlisstandingonagrassy...
“abird” Aclose-upphotoofaNorthernSaw-whetOwl(Ae-
goliusnivalis)inanaturalhabitat...
“abirdandadog” Aclose-upphotoofaNorthernSaw-whetOwl(Ae-
goliusnivalis)inaforest...
“” Aphotoofaseashellonasandybeach.Theshellis
alightpinkcolorwith...
“conch” Aphotoofaconchshellonasandybeach.Theshell
islargeandhasaspiralshape...
“arock” Aphotoofaseashellonasandybeach.Theshellis
alightpinkcolorwith...
“” Aphotoofamushroom,specificallyaspeciesofthe
genusFusarium...
“gyromitra” A close-up photo of a mushroom, specifically a
speciesofthegenusGyromitra...
“acat” Aphotoofamushroom,specificallyaspeciesofthe
genusFusarium...
“” Aphotoofawhite-facedcapuchinmonkey(Cebus
capucinus)sittingonatreebranch...
“spidermonkey,Atelesgeoffroyi” Aphotoofaspidermonkey,Atelesgeoffroyi,sitting
onatreebranch.Themonkey...
“abird” Aphotoofawhite-facedcapuchinmonkeysittingon
atreebranch.Themonkeyhas...
Table3: Qualitativeevaluationforre-aligningdifferentalt-textsasprompts: Wemarkconceptscarriedinalt-texts
inblueanderroneouscaptionswithoutgroundedinalt-textsinred. Thecaptioneralsorejectshallucinated/general
visualconceptsinalt-textsinbrown.Thisisonlypossiblebyperformingalignmentwithtextandimageinformation.
theticdata,generatedbyAltogetherwithalt-texts onthe5Bimages,withcaptionerstrainedwith(i)
prompting. Thisyieldsanother0.5gainonCLIP Round 2 annotation, (ii) Round 3 annotation and
score. ThisindicatesDALL·E3’s5%mixingwith (iii)Round3withoutalt-textsprompts.
originalalt-textsissub-optimal,notnecessaryand
may at risk of increasing mis-aligned data, if the Results. We show the results of CLIP training
synthetic caption is already re-aligned from alt- byzero-shotevaluationon26classificationtasks
in Table 6. We first study the performance of us-
text. Ablating ratios of mixing existing captions
ingonlysyntheticcaptions(ratioofsyntheticcap-
(alt-text)doesmakeasignificantdifference.
tionsp=1.0). Multipleroundsofannotationhelp
InTable4,wequalitativelystudythere-aligned
to improve accuracy by 1.5% (Round 2 (p=1.0)
captions and show this approach promotes fine-
vsRound3(p=1.0)). Interestingly,thecaptioner
grained control and grounding for text-to-image
without re-aligning alt-text (w/o alt-text) strug-
generationwithreducedhallucination.
gles(44.5%averageaccuracy),indicatingthatre-
aligningalt-textinthecaptionerisimportant.
5.4 ClassificationandRetrieval
The next section of Table 6 shows that train-
Setup. Following the data curation in Meta- ing with only alt-text performs better than using
CLIP(Xuetal.,2024),wecollect5Bimage-text only synthetic captions above. We believe this is
pairsasCLIPtrainingdata. Wefollowthestandard because the captioner is likely not large enough
CLIP training setup for evaluating our approach to carry all the alt-text information into the syn-
usingaViT-B/32architectureasinOpenCLIP(Il- theticcaption. Wethenmixalt-textandsynthetic
harcoetal.,2021)andMetaCLIP(Xuetal.,2024). captions (ablation in Appendix §D) for training
ThetraininghyperparametersareinTable12. CLIP. With a ratio of p=0.15 synthetic captions,
Wecreate3setsofcaptionsbyrunninginference we see a +1.1% improvement over 26 classifica-Prompt Original Altogether
Ahummingbirdinmid-air,hoveringaboveabrightredflower.Thebirdismostlygreen
withablackheadandalong,pointedbeak.Itswingsarespreadwideandblurreddueto
thefastmovement.Theflowerisabrightredcolorwithfivepetalsandayellowcenter.
Thebackgroundisablurredgreen,withhintsofotherleavesandflowersvisible.
ABelgianMalinoisdogwearingaprostheticleg.Thedogisstandingonagrassyfield
withablurredbackground.Theprostheticlegismadeofmetalandhasarubbersole.
Thedogislookingdirectlyatthecamerawithitsmouthopen,asifit’ssmiling. The
dog’sfurisamixofbrownandblack.
Threepottedplants,eachplacedinawovenrattanbasket,isolatedonawhiteback-
ground.Theplantsareofdifferentsizesandspecies,withonebeingatall,leafyplant
withathickstem,anotherbeingashorter,bushyplantwithathinstem,andthethird
beingasmall,roundplantwithathinstem.Thebasketsaremadeofnatural-colored
wickerandhaveabraideddesign.
Abeautiful,modernresortwithalargeswimmingpoolandastunningviewofthesea.
Thepoolissurroundedbyawoodendeckwithloungechairsandumbrellas,andthere
arepalmtreesandothergreeneryaroundthepoolarea. Inthebackground,youcan
seetheblueseaandafewboatssailingonit. Theresortbuildingsarevisibleinthe
background,withamixofmodernandtraditionalarchitecture.
Ascenicviewofariverflowingthroughaforest.Thereisasmallstonebridgewitha
fewtreesgrowingoneitherside.Thebridgeismadeoflarge,rough-hewnstonesand
hasadistinctivearchedshape.Theriverwaterisclearandshallow,withafewrocks
andbranchesvisiblebeneaththesurface. Theforestinthebackgroundisdenseand
green,withtalltreesstretchinguptowardsthesky.
Twotacosonawhiteplate, withavioletbackground. Eachtacohasacrispycorn
tortillashellfilledwithshreddedmeat,toppedwithslicedavocado,shreddedlettuce,
andasprinkleofredcabbage. There’sadollopofcreamysauceontopofeachtaco.
Therearetwoglassesofdrinks,onewithapinkstrawandtheotherwithayellowstraw,
placedoneithersideoftheplate.
Acolorfulbirthdaycaketoppedwithalargenumber9madeoffondantanddecorated
withcolorfulsprinkles.Therearealsoseveralsmallfondantdecorationsontopofthe
cake,includingayellowchick,apinkpig,andabluebird. Thecakeisplacedona
whitecakestandandsurroundedbycolorfulballoons.
Table4: Text-to-ImageGeneration. Ineachgroup,left: Textprompt;middle(baseline): imagegeneratedbyLDM
trainedwithoriginalcaptions;right: imagegeneratedbyLDMtrainedwithAltogethersyntheticcaptions(Round
3). Hallucinationsanderrorsgeneratedbybaseline,Altogetherorbotharemarkedwithcolors. Asobserved,an
LDMtrainedwithAltogetherdatafollowstextinstructioncloserandimprovesimage-promptalignmentincomplex
scenesandspecializedentities(e.g. “aBelgianMalinoisdog”).
ageaccuracycomparedtothe72.4%withthesame
InferencePrompt
TrainingData Original Synthetic modelinMetaCLIP(Xuetal.,2024).
alt-texts(Round1) 27.0 28.0 Finally,weevaluateonzero-shottext-to-image
Altogether(3),w/oalt-texts,p=0.95 27.1(+0.1) 29.3(+1.3) retrievaltasksfromDataComp(Gadreetal.,2023).
Altogether(3),w/alt-texts,p=0.75 27.2(+0.2) 29.6(+1.6)
Results are in Table 7. Mixing alt-text with syn-
Altogether(3),w/alt-texts,p=0.95 27.3(+0.3) 29.8(+1.8)
Altogether(3),w/alt-texts,p=1.0 27.3(+0.3) 29.8(+1.8) theticcaptionsleadsto+3%forretrievalonViT-B
andevenlargergainsoverMetaCLIPViT-H/14.
Table5: Evaluationoftext-to-imagegenerationonCC-
12M:CLIPsimilarityscoresbetweenprompts(original
Discussion. Aninterestingobservationisthatim-
orsynthetic)andgeneratedimages.
agegenerationandclassificationrequiredifferent
amountofmixingratiosforsyntheticcaptions—the
optimalmixingratiois∼100%forT2Igeneration
tiontasks(Table6),showinghowre-aligncanpro- whereas as low as ∼15% for CLIP classification.
vide complementary information for CLIP train- Therootcausemaystemfromverydifferentdef-
ing. FinallywetrainalargeViT-H/14modelwith initions of these two problems: T2I needs fully
mixedAltogethercaptionsandobserve73.2%aver- alignedcaptionstohavetextcontrollingthegener-ViT-B/32
Altogether(2)(p=1.0) 52.3 51.5 68.7 90.2 70.4 47.5 57.8 67.0 13.2 37.7 67.2 88.4 51.6 64.0 43.0 95.4 50.0 57.0 44.8 15.2 8.6 54.2 54.1 37.8 23.9 51.5 50.0
Altogether(3)(w/oalt-text) 44.5 39.8 47.4 88.6 65.7 14.8 50.0 54.4 4.9 29.8 54.2 79.2 30.4 71.9 25.7 89.6 39.3 54.2 37.9 23.9 5.1 53.5 47.4 31.5 15.0 54.9 49.2
Altogether(3)(p=1.0) 53.8 52.8 70.0 90.4 71.4 47.7 57.4 67.5 14.7 41.5 69.1 88.4 50.6 62.9 42.1 94.7 56.1 55.1 48.8 33.0 8.9 57.2 56.8 38.7 23.0 52.0 48.9
Alt-text(Round1) 59.3 68.1 84.4 93.1 74.5 66.5 67.2 77.9 27.9 59.4 90.7 91.7 72.0 25.1 45.1 97.0 45.8 63.3 37.0 30.1 18.8 63.3 67.5 47.7 19.1 55.9 52.4
Altogether(2)(p=0.15) 60.3 67.9 84.1 92.1 75.3 66.7 67.1 78.2 25.1 58.8 89.4 92.5 70.3 37.4 40.2 95.7 55.0 67.3 38.3 31.9 18.0 59.7 67.4 48.0 33.1 56.2 52.9
Altogether(3)(p=0.15) 60.4 68.2 84.3 92.7 75.6 67.0 67.1 77.8 25.6 62.6 89.1 92.6 71.2 36.7 44.5 96.8 53.2 63.8 38.6 35.9 18.8 58.2 68.1 48.2 24.2 53.5 55.1
ViT-H/14
MetaCLIP 72.4 80.5 94.2 98.0 86.4 83.4 74.1 90.0 50.2 72.4 95.4 95.6 85.1 72.7 55.2 99.4 66.3 74.6 62.5 38.2 37.2 65.8 82.2 64.1 30.1 59.3 69.2
Altogether(3)(p=0.15) 73.2 82.1 95.0 97.8 87.1 88.6 74.6 93.1 63.2 73.0 95.9 95.9 86.8 86.1 54.6 99.5 70.3 76.0 57.9 28.1 43.3 50.1 85.4 65.4 32.5 58.3 62.5
Table6: Resultson26CLIPzero-shotclassificationtasks. Firstsection: Trainingwithpure(p=1.0)synthetic
captionsfromourcaptionersthatweretrainedafterdifferentroundsofannotations. Secondsection: Mixingin
alt-textduringtraining(ratioofp=0.15). Thirdsection: ComparisonofalargeViT-H/14modeltrainedonour
syntheticcaptionswithmixedalt-textoutperformsMetaCLIP(Xuetal.,2024)(72.4vs.73.2averageaccuracy).
Avg.retrievalFlickrCOCOINDist.ShiftVTAB (iii) Lack of external high-quality ground-
ViT-B/32
Alt-text(Round1) 52.6 72.9 46.6 52.3 55.3 truthcaptions(thatdescribebothalt-textand
Altogether(3)(p=1.0) 46.1 69.0 42.8 41.7 47.8 complementary information well). Note a
Altogether(3)(p=0.15) 55.6 76.0 48.9 52.5 55.9
ViT-H/14 higherqualitybenchmarkcanevaluatealower
MetaCLIP 60.4 85.0 57.5 66.1 64.6 qualitycaption,butnotthereverse. Forexam-
Altogether(3)(p=0.15) 65.7 87.6 60.7 67.3 66.2
ple,existingliteraturereportsthatbenchmarks
Table7: Zero-shotretrievalevaluation. such as MSCOCO or Flicker contain only
well-knownvisualconceptsandarenegatively
correlatedwithhumanevaluation(IIW(Garg
atedimagesineverydetail;whereastheproblemof
et al., 2024)) or higher quality benchmarks
CLIPonlyneedstorecognizeasingleclassname
(AnyMAL(Moonetal.,2023)).
fromalong-tailedvocabulary.
6 Conclusion 2. Duetolimitedcompute,wecannotevaluate
imagegenerationatalargerscale.
This paper presents Altogether, a principled way
ofimprovingimagecaptionsbyre-aligningexist-
3. Current synthetic captioning can improve
ingalt-texttoimages. Re-aligningalt-textallows
alignmentbutcannotgobeyondtheconcrete
concretevisualconceptstobecarriedintothere-
visual concepts described in alt-texts to im-
sulting caption. In experiments, we show that a
prove challenging benchmarks such as Ima-
lightweightcaptionertrainedtoperformthistask
geNetclassification.
cangeneratecaptionswithsignificantlybettercap-
tioningperformancethanalternatives. Wefurther
4. Workingonlargemultimodallanguagemod-
observethattheresultingcaptionscanbeusedfor
els faces various constraints, including be-
improvingbothtext-to-imagegenerationandzero-
ingcompetitivewithoutusingdatafrompro-
shotrecognitionacrossabroadsetoftasks.
prietary models (the community is actively
7 Limitations distilling information from models such as
GPT-4V),whichleadstolackoftransparency
Weobservethefollowinglimitationsinthiswork:
(black-box LLMs). In this work we aim
to show a principled way of improving im-
1. Evaluatingcaptionswithrareandspecificcon-
agecaptionswithmaximallypreservingtrans-
ceptsischallengingforthefollowingreasons.
parency. Wewillmakeourcode,modelsand
(i) Re-aligned alt-texts can contain superhu-
dataavailableforfutureuse.
man information (think e.g., a very specific
model type of a car or boat is not known to
Acknowledgments
the majority of people). It is challenging to
verifycorrectness,evenbyhumanevaluators.
We thank Xian Li, Ping Yu, Yuandong Tian,
(ii)Thereisnoperfectmetrictoquantifythe Chunting Zhou, Armen Aghajanyan and Mary
overallqualityofalt-textsandcomplementary Williamsonfortheinsightfuldiscussion.
informationaddedviare-aligning.
egarevA teNegamI 101-dooF 01RAFIC 001RAFIC BUC 793NUS sraC tfarcriA DTD steP 101-hcetlaC srewolF TSINM 3102-REF 01-LTS TASoruE 54CSISER BRSTG ITTIK 112yrtnuoC MACP 101FCU 007sciteniK RVELC
semeMlufetaH
2TSSReferences Dave,VaishaalShankar,HongseokNamkoong,John
Miller,HannanehHajishirzi,AliFarhadi,andLud-
JamesBetker,GabrielGoh,LiJing,TimBrooks,Jian-
wigSchmidt.2021. Openclip. Ifyouusethissoft-
fengWang,LinjieLi,LongOuyang,JuntangZhuang,
ware,pleaseciteitasbelow.
JoyceLee,YufeiGuo,etal.2023. Improvingimage
generationwithbettercaptions. ComputerScience. Zhengfeng Lai, Haotian Zhang, Bowen Zhang, Wen-
https://cdn.openai.com/papers/dall-e-3.pdf,2(3):8. tao Wu, Haoping Bai, Aleksei Timofeev, Xianzhi
Du,ZheGan,JiulongShan,Chen-NeeChuah,Yin-
Khyathi Raghavi Chandu, Piyush Sharma, Soravit
feiYang,andMengCao.2024. Veclip: Improving
Changpinyo, Ashish Thapliyal, and Radu Soricut. cliptrainingviavisual-enrichedcaptions. Preprint,
2020. Denoisinglarge-scaleimagecaptioningfrom
arXiv:2310.07699.
alt-textdatausingcontentselectionmodels. arXiv
preprintarXiv:2009.05175. PatrickLewis,EthanPerez,AleksandraPiktus,Fabio
Petroni,VladimirKarpukhin,NamanGoyal,Hein-
Soravit Changpinyo, Piyush Sharma, Nan Ding, and richKüttler, MikeLewis, Wen-tauYih, TimRock-
RaduSoricut.2021. Conceptual12M:Pushingweb- täschel,etal.2020. Retrieval-augmentedgeneration
scaleimage-textpre-trainingtorecognizelong-tail forknowledge-intensivenlptasks. AdvancesinNeu-
visualconcepts. InCVPR. ralInformationProcessingSystems,33:9459–9474.
Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.
Yao, Enze Xie, Yue Wu, Zhongdao Wang, James 2023a. BLIP-2: bootstrappinglanguage-imagepre-
Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. training with frozen image encoders and large lan-
2023a. Pixart-α: Fast training of diffusion trans- guagemodels. InICML.
former for photorealistic text-to-image synthesis.
Preprint,arXiv:2310.00426. Wenyan Li, Jonas F Lotz, Chen Qiu, and Desmond
Elliott. 2023b. The role of data curation in image
Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Con- captioning. arXivpreprintarXiv:2305.03610.
ghui He, Jiaqi Wang, Feng Zhao, and Dahua
Lin. 2023b. Sharegpt4v: Improving large multi- Tsung-YiLin,MichaelMaire,SergeBelongie,James
modalmodelswithbettercaptions. arXivpreprint Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
arXiv:2311.12793. and C Lawrence Zitnick. 2014. Microsoft coco:
Common objects in context. In Computer Vision–
Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, ECCV 2014: 13th European Conference, Zurich,
JonathanHayase,GeorgiosSmyrnis,ThaoNguyen, Switzerland, September 6-12, 2014, Proceedings,
Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, PartV13,pages740–755.Springer.
JieyuZhang,EyalOrgad,RahimEntezari,Giannis
HaotianLiu,ChunyuanLi,YuhengLi,BoLi,Yuanhan
Daras,SarahPratt,VivekRamanujan,YonatanBit-
Zhang,ShengShen,andYongJaeLee.2024a. Llava-
ton,KalyaniMarathe,StephenMussmann,Richard
next: Improvedreasoning,ocr,andworldknowledge.
Vencu,MehdiCherti,RanjayKrishna,PangWeiKoh,
OlgaSaukh,AlexanderRatner,ShuranSong,Han-
HaotianLiu,ChunyuanLi,QingyangWu,andYongJae
naneh Hajishirzi, Ali Farhadi, Romain Beaumont,
Lee.2024b. Visualinstructiontuning. Advancesin
SewoongOh,AlexDimakis,JeniaJitsev,YairCar-
neuralinformationprocessingsystems,36.
mon,VaishaalShankar,andLudwigSchmidt.2023.
Datacomp: Insearchofthenextgenerationofmulti- RonMokady,AmirHertz,andAmitHBermano.2021.
modaldatasets. Preprint,arXiv:2304.14108. Clipcap: Clip prefix for image captioning. arXiv
preprintarXiv:2111.09734.
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,
JinliuPan,YuxiBi,YiDai,JiaweiSun,andHaofen Seungwhan Moon, Andrea Madotto, Zhaojiang Lin,
Wang. 2023. Retrieval-augmented generation for TusharNagarajan,MattSmith,ShashankJain,Chun-
large language models: A survey. arXiv preprint FuYeh,PrakashMurugesan,PeymanHeidari,Yue
arXiv:2312.10997. Liu, et al. 2023. Anymal: An efficient and scal-
ableany-modalityaugmentedlanguagemodel. arXiv
Roopal Garg, Andrea Burns, Burcu Karagol Ayan, preprintarXiv:2309.16058.
Yonatan Bitton, Ceslee Montgomery, Yasumasa
Onoe, Andrew Bunner, Ranjay Krishna, Jason Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco,
Baldridge,andRaduSoricut.2024. Imageinwords: SewoongOh,andLudwigSchmidt.2024. Improv-
Unlockinghyper-detailedimagedescriptions. arXiv ingmultimodaldatasetswithimagecaptioning. Ad-
preprintarXiv:2405.02793. vances in Neural Information Processing Systems,
36.
JackHessel,AriHoltzman,MaxwellForbes,RonanLe
Bras,andYejinChoi.2021. Clipscore: Areference- Yasumasa Onoe, Sunayana Rane, Zachary Berger,
freeevaluationmetricforimagecaptioning. arXiv YonatanBitton,JaeminCho,RoopalGarg,Alexan-
preprintarXiv:2104.08718. der Ku, Zarana Parekh, Jordi Pont-Tuset, Garrett
Tanzer, et al. 2024. Docci: Descriptions of con-
GabrielIlharco,MitchellWortsman,RossWightman, nected and contrasting images. arXiv preprint
CadeGordon,NicholasCarlini,RohanTaori,Achal arXiv:2404.19753.OpenAI. a. Gpt-4o. https://openai.com/index/ onComputerVisionandPatternRecognition,pages
hello-gpt-4o. Accessed: 2024-05-13. 14022–14032.
OpenAI. b. Gpt-4v. https://cdn.openai.com/ Susan Zhang, Stephen Roller, Naman Goyal, Mikel
papers/GPTV_System_Card.pdf. Accessed: 2023- Artetxe,MoyaChen,ShuohuiChen,ChristopherDe-
09-25. wan,MonaDiab,XianLi,XiVictoriaLin,etal.2022.
Opt: Openpre-trainedtransformerlanguagemodels.
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida, arXivpreprintarXiv:2205.01068.
CarrollWainwright,PamelaMishkin,ChongZhang,
SandhiniAgarwal,KatarinaSlama,AlexRay,etal.
2022. Training languagemodelsto followinstruc-
tionswithhumanfeedback. Advancesinneuralin-
formationprocessingsystems,35:27730–27744.
AlecRadford,JongWookKim,ChrisHallacy,Aditya
Ramesh,GabrielGoh,SandhiniAgarwal,GirishSas-
try, Amanda Askell, Pamela Mishkin, Jack Clark,
etal.2021. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InInternationalconfer-
enceonmachinelearning,pages8748–8763.PMLR.
RitaRamos,BrunoMartins,DesmondElliott,andYova
Kementchedjhieva.2023. Smallcap: lightweightim-
age captioning prompted with retrieval augmenta-
tion. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages
2840–2849.
Krishna Srinivasan, Karthik Raman, Jiecao Chen,
Michael Bendersky, and Marc Najork. 2021. Wit:
Wikipedia-basedimagetextdatasetformultimodal
multilingual machine learning. In Proceedings of
the 44th International ACM SIGIR Conference on
ResearchandDevelopmentinInformationRetrieval,
pages2443–2449.
Jack Urbanek, Florian Bordes, Pietro Astolfi, Mary
Williamson, Vasu Sharma, and Adriana Romero-
Soriano.2023. Apictureisworthmorethan77text
tokens: Evaluatingclip-stylemodelsondensecap-
tions. arXivpreprintarXiv:2312.08578.
JianfengWang,ZhengyuanYang,XiaoweiHu,Linjie
Li, KevinLin, ZheGan, ZichengLiu, CeLiu, and
LijuanWang.2022. Git: Agenerativeimage-to-text
transformerforvisionandlanguage. arXivpreprint
arXiv:2205.14100.
HuXu,SainingXie,XiaoqingEllenTan,Po-YaoHuang,
RussellHowes,VasuSharma,Shang-WenLi,Gargi
Ghosh,LukeZettlemoyer,andChristophFeichten-
hofer.2024. DemystifyingCLIPdata. InTheTwelfth
International Conference on Learning Representa-
tions.
Zhuolin Yang, Wei Ping, Zihan Liu, Vijay Kor-
thikanti,WeiliNie,De-AnHuang,LinxiFan,Zhid-
ing Yu, Shiyi Lan, Bo Li, et al. 2023. Re-vilm:
Retrieval-augmentedvisuallanguagemodelforzero
and few-shot image captioning. arXiv preprint
arXiv:2302.04858.
Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui,
Fan Zhang, Yue Cao, Xinlong Wang, and Jingjing
Liu.2024. Capsfusion:Rethinkingimage-textdataat
scale. InProceedingsoftheIEEE/CVFConferenceA AnnotationGuidelines
Thissectiondetailsourannotationguidelines. We
ImageNet
highlighttheoverallgoalandgoodpracticeforan- 67.5
Avg. 26 Tasks
notation first, then show the detailed instructions
for annotators in Fig. 5. Our annotations aim to 65.0
enhancethealignmentbetweenimageandexisting
62.5
captions. We use the metadata of the image (i.e.,
alt-textattributes)asthestartingpoint. Thealt-text
60.0
isconsideredtocontaingroundtruthinformation
of the image but only partially describes the im- 57.5
age. Thegoalofourannotationistosignificantly
55.0
improve image-caption alignment and make the
captionjustright: e.g.,donotmentionmissingob-
52.5
jectsintheimageorinformationbeyondtheimage
content. 0.0 0.2 0.4 0.6 0.8 1.0
Ratio of Synthetic Caption for CLIP Training
GoodPractices
Figure4:Zero-shotclassificationaccuracyonImageNet
• Weuseshortpromptsasthestartingpointsof andaveraged26CLIPtaskswithdifferentratioofmix-
captions: suchas“aphotoof",“apaintingof", ingsyntheticcaptionsduringtrainingofvariousCLIP
“a sculpture of", instead of verbose prompts ViT-B/32models.
suchas“Thisisanimageshowing...”. Wepro-
videarecommendedlistofstartingprompts
B Side-by-sideComparisonofMultiple
inTable8.
RoundsofAnnotation
• Weprovideannotationstepstoguidethean-
Weshowside-by-sidecomparisonofannotations
notator’s workflow during annotation. See
inTable14forWITimagesandTable15forMeta-
“AnnotationSteps”inFig.5.
CLIPimages(imagesarenotshown).
• We further provide a checklist to help anno-
C AltogetherEvaluatedonMSCOCO
tatorsconfirmiftheyfolloweachstepofthe
guidelineswell. Fig.6providesascreenshot The Altogether-ft fine-tuning set is very differ-
ofourannotationinterface. ent in style from the popular captioning dataset
MSCOCO.Asareference, wealsoreportperfor-
• Weleveragetwovendorsforannotationand
manceonMSCOCO2017asthereferencecaption
askeachvendortorewrite/criticisetheother
inTable13.
vendor’sannotationfromthepreviousround.
Wesplitthedatatoannotatebetweenthetwo D RatioofMixingSyntheticCaptionsfor
vendors,andswapthedatainthenextround. CLIPTraining
Weablatedifferentmixingratiosofsyntheticcap-
“aphotoof”
tionsvs.ImageNetzero-shotaccuracy,andaverage
“aproductphotoof”
accuracyacrossthe26CLIPdatasetsinFig.4and
“alowresolutionphotoof”
“acroppedphotoof” notice that a high ratio of synthetic caption can
“aclose-upphotoof” reducetheperformancesignificantly. Agoodtrade-
“ablackandwhitephotoof” off ratio is around 15%, which allows synthetic
“ablurryphotoof” caption to complement alt-text, which is our de-
“arenderingof”
faultvaluethroughoutthepaper. Thisislikelydue
“asculptureof”
to two reasons: (i) human annotation optimizes
“apaintingof”
alignmentandisconservativeonalt-textswhenit
“acartoonof”
concerns ambiguous image information. For ex-
Table8: Recommendedstartingpromptsforcaptioning ample, a “$18/night room” in alt-texts could still
annotation. superviseanimagehavingaroomofpoorcondi-
tionbutisatriskofhavingmis-aligneddescription
.ccA
tohs-oreZ
ksaT
.gvA/teNegamIDecoder Seq.Len. ImgsperSecond GPUDaysfor1BImgs Dayson256GPUsfor3BImgs
Llama213BChat(w/oalt-texts) 296 2.6 4452 52.2
OPT1.3B(w/oalt-textstokens) 296 19.7 589 6.8
OPT1.3B(w/alt-textstokens) 424 15.6 740 8.6
Table9: ThroughputofdifferenttextdecodersmeasuredonNVIDIAA10080GBGPUs.
Hyperparameter E ThroughputofDifferentTextDecoders
Arch. ClipCap(Mokadyetal.,2021)
FrozenEncoder MetaCLIP(Xuetal.,2024) Toscalecaptionerinferencetobillionsofimages,
Resolution 224×224
weablatethethroughputofdifferentdecodersetups
CLIPEmbeddingSize 1024
VisualTokens 40 in Table 9. We note that using such an LLM is
TrainableDecoder OPT1.3B
13.2×slower than OPT (2.6 vs. 19.7 images per
Attention FlashAttention2
BatchSize 512 second).
LearningRate 1e-3
MinimalLearningRateRatio 0.1 F Hyperparameters
Warm-up 2k
Pre-trainingData MetaCLIP22M
Wedetailthehyperparametersofthecaptionerin
Pre-trainingSteps 44k
Fine-tuningData WIT15k+MetaCLIP7k Table10,downstreamtext-to-imagetraininginTa-
Fine-tuningSteps 96 ble11andCLIPtraininginTable12,respectively.
Temperature 0.2
Top-psampling(nucleussampling) 0.7
Table10: Hyperparametersofcaptionertraining.
Hyperparameter
Arch. DiT-XL
ActivationFunction GELU
TrainingData CC12M
ImageSize 256
BatchSize 8192
LearningRate 2.0e-5
Warm-up 1000
TrainingEpochs 24
Table11: Hyperparametersoftext-to-imagegeneration
training.
Hyperparameter ViT-B/32 ViT-H/14
ActivationFunction QuickGELU GELU
SeenPairs 12.8B 51.2B
BatchSize 32768 120k
LearningRate 5.0e-4 4.0e-4
Warm-up 2k 2k
Table12: HyperparametersofCLIPtraining.
Baseline CLIPScore BLEU1 METEOR ROUGE CIDEr
COCOannotation 30.37 - - - -
Altogether(3)w/oalt 33.69 17.5 17.3 19.0 0.0
Table13: AltogetherevaluatedonMSCOCO.
onprice,soanannotatormayremovethatfromalt-
text;and(ii)existingbenchmarkssuchasclassifi-
cation/retrievaltestspecific(object)classesinstead
ofwholeimagealignment.Goal Thegoalofthistaskistoenhancethealignmentin-betweenimageandcaptionviacaptionediting,leveragingthe
metadataoftheimage(i.e.alt-textattributes).Thecollecteddatawillbeusedtotrainarewritemodelforcaptiongeneration.
Thefactoidknowledgeandconcretevisualconceptsinalt-textisexpectedtobeaddedtoimprovethecaptionandnoextra
personalknowledgefromannotatorsareexpectedaspartoftheimprovedcaption.
TaskDescription Weprovideapairof(image,alt-text)toannotators,andaskannotatorstoleveragetheprovidedalt-text
asfactoidknowledgeandrewritetoimprovethealignmentbetweenthecaptionandtheimage.Abetteralignmentmeans:1)
removinganynonfactualpartsinthecaption;2)addingmissinginformationintothecaption(objectshownintheimagebutnot
mentionedincaption).Iftheimage-captionpairis90%aligned,makeit99%aligned.
AnnotationSteps
1. Copyandpastethe“PreviousCaption”totheboxof“Rewrittencaption”.
2. Aconcisestartingprompttodescribewhattheimageisabout,suchas“aphotoof”,“aproductphotoof”,dependsontypes
ofimages,ratherthan“Thisimageshows...”
3. Usealt-textasmuchaspossibleifappropriate(mostlyin1stsentence)toimprovethefactualityofthecaption.
• Paraphrasingisencouraged,butpleasedonotchangethemeaningofthealt-text.
• Usingconcretevisualconceptsinalt-textsasmuchaspossible:write“Bentley”(alt-texts)as“aBentley”insteadof“a
car”.
• Alt-textswithmetadatasuchasfilenames/datesor“photographedbyABC”canbeignored.
• Usingexternaltool(e.g.,Google)isencouragedtohelpunderstandthealt-text.
4. Remove/Editanyhallucinatedpartsinthecaption(anythingthat’seithernotexistsintheimageorwronglydescribed,e.g.,
wrongcolor)
5. Removesentencedescribingtheme/feelingofcaption,e.g.“overallthisimagegivesanimpressionofxxx”orimaginative
description“thisboymusthaveabrightfuture.”.
6. Totheextenttheimagecontainspeople,pleaseDONOTprovideanyinformationaboutthatperson’s
• racialorethnicorigin(includingskincolor,haircolor,apparentnationalityorcitizenship);
• Sexualorientation;
• Politicalaffiliation;
• Healthconditionordisability;
• Religion;
• MembershipinaTradeUnion;
• Facialfeatures,expressionoremotion(e.g,smiling/cryingaswellas“mood”),haircolor(e.g.,“darkhaired”,“blonde-
haired”,etc.);
• DONOTaddanyidentifyinginformationaboutpeopleorobjectssuchasnames,addressandemails.
7. Addinvisiblemissingdetailsifthere’sany.
• Whenlesscertain/incaseofblurryimage,usevagueandgeneraltermstodescribetheobjectssuchas“Thismaybe
NYC”ratherthan“ThisisNYC”;or“animal”insteadof“dog”/“cat”(whenit’shardtojudgedetailedtype).
• Transcribeanyreadablecharactersintheimage.
8. Checktheoverallstructure(deductivestructureetc)oftherewrittencaption.
• Makesureeverythinginthecaptionisfactual.
• Checkthestructureofcaption(seethenextsection).
StructureofCaption
1. Captionstructure
• Objects:Agooddensecaptionshouldfollowa“deductivestructure”whereittypicallystartswithageneralstatement,
followedbysubjects,secondaryobjects,background,andconcludingwithminordetails.
• Orderofobjects:Similartohowahumanwouldusuallyreadimagese.g.,“lefttoright”,“toptobottom”,or“neartofar”
order.Oncedonewithdescribingthemostsalientobjects,forsecondaryobjectsandbackgroundsthatarehardtosortby
saliency,wecanarrangesecondaryobjectsandbackgroundelementsinasimilarway,dependingontheimagestructure.
– Thedefaultspatialtermsisbasedonviewer’sangle(3rdperson);if1stpersonviewangleisneeded,explicitlywrite
downthatangle:“onherleftisacutedog”;
– Describespatialrelationfrombigtosmall,frommaintoaccessory:”...acake.There’re4cherriesonthecake.”.
– Countobjectsofthesametypewhenitislessthanorequalto10;formorethan10objects,annotatormayusethe
word“manyx”.
• Longparagraph:Pleasesplitalongparagraphintoshorterandcoherentparagraphs,andorganizethemwithaclearlogical
orderforeasierunderstanding.
2. Captionlength
• Conciseness,correlateswith“complexity”oftheimage.Thoughwewanttohavedetaileddescriptions,wealsowantto
havethedetailsbeingdescribedinaconciseway.Ifthereisonlyoneobjectpresentintheimage,weshouldn’thavea
longparagraph.
Figure5: Annotationguideline.Figure6: Annotationinterface.Image Alt-Text(Round1) Round2 Round3
A wooden gilded statue A photograph of a wooden A photo of a wooden gilded
of the bodhisattva Aval-gildedstatueofthebodhisattva statue of the bodhisattva Aval-
okites´vara from the Avalokites´varafromtheChinese okites´vara from the Chinese
Chinese Song Dynasty Song Dynasty. The image de-SongDynasty(960-1279),from
(960-1279), from the pictsastatueofaseatedfigure theShanghaiMuseum.Theim-
Shanghai Museum.placedonablackpedestal.The agedepictsastatueofaseated
Français: Statue en bois figureisadornedwithelaborate figureplacedonablackpedestal.
doré de bodhisattva attireandjewelry,suggestingit Thefigureisadornedwithelab-
Avalokites´vara.Dynastie might represent royalty or di-orateattireandjewelry,suggest-
Song(960-1279),Musée vinity. Theclothingandacces-ingitmightrepresentroyaltyor
deShanghai. sories feature intricate designs divinity.Theclothingandacces-
andpatterns.Unfortunately,the sories feature intricate designs
faceofthestatueisobscuredby andpatterns.Unfortunately,the
abeige-coloredrectangle. The faceofthestatueisobscuredby
backgroundhasadarkredhue,abeige-coloredrectangle. The
creatingcontrastwiththelighter backgroundhasadarkredhue,
colorofthestatue. creatingcontrastwiththelighter
colorofthestatue.
AlwalkeriaIllustrationof A 3D render of a dinosaur,A 3D render of a dinosaur,
Alwalkeriamaleriensisby specificallyanAlwalkeria.Itis specificallyanAlwalkeria.Itis
Karkemish depictedinarunningpose,with depictedinarunningpose,with
itsleftfootforwardanditsright itsleftfootforwardanditsright
footback.Thedinosaur’sbody footback.Thedinosaur’sbody
isamixoforange,yellow,and isamixoforange,yellow,and
browncolors,withblackstripes browncolors,withblackstripes
onitsbackandtail.Ithasalong,onitsbackandtail.Ithasalong,
curvedtailandsharpteeth.Itis curvedtailandsharpteeth.Itis
isolatedinawhitebackground. isolatedonawhitebackground.
Juvenile male in Tas-A close-up photo of a juve-Aphotoofajuvenilemalecres-
mania, Australia A nile male crescent honeyeater cent honeyeater, (Phylidonyris
male crescent hon-bird perched on a gray picket pyrrhopterus). The bird is
eyeater (Phylidonyris fence.Thebirdhasadarkgrey perchedonagraypicketfence
pyrrhopterus) in Lind-plumage,palegrayunderparts,inLindisfarne,Tasmania,Aus-
isfarne, Tasmania,andyellowwingpatches. The tralia. Thebirdhasadarkgrey
Australia birdalsohasablackbill,apair plumage,palegrayunderparts,
ofdarkeyes,andgraylegs. A andyellowwingpatches. The
fewleavesandbranchesofatree birdalsohasablackbill,apair
arepartlyvisibleintheblurring ofdarkeyes,andgraylegs.The
background. background has a few leaves
andbranchesofatreethatare
blurred.
Table14: ExamplesofannotatedimagesfromWITdatasetinmulti-rounds.Alt-Text(Round1) Round2 Round3
Vintage 1992 University Aphotoofagreent-shirtwithastraighthem.AphotoofaVintage1992UniversityofMiami
ofMiamiHurricanesOr-Thet-shirtismadeofathick,high-qualityfabric HurricanesOrangeBowlT-shirt. TheT-shirtis
angeBowlT-shirt thatissofttothetouch.Thecolorisadeepgreen,madeofathickfabric. ThecoloroftheT-shirt
almostaforestgreen, withaslightsheentoit.isadeepgreen,almostaforestgreencolor.The
Theimageistakenonayellowbackground. t-shirtisdepictedonayellowbackground.
Aqua Recessed swim-AproductphotoofAquaRecessedswimming AproductphotoofAquaRecessedswimming
mingPool11x11cmMR Pool11x11cmMR16IP68StainlessSteelAISI Pool11x11cmMR16IP68StainlessSteelAISI
16 IP68 Stainless Steel 316lightfixture.Theimageshowsaround,stain-316lightfixture.Theimageshowsaround,stain-
AISI316 lesssteelsubmersibleoutdoorpoollightfixture lesssteelsubmersibleoutdoorpoollightfixture
withaflat,glasslens.Thelightisrecessedinto withaflat,glasslens.Thelightisrecessedinto
thefixtureandsurroundedbyaringoffourmetal thefixtureandsurroundedbyaringoffourmetal
flanges. Theflangeshavesmallholesdrilledin flanges. Theflangeshavesmallholesdrilledin
them.Thelightfixtureissecuredtotheground them.Thelightfixtureissecuredtotheground
withalargeboltinthecenter.Thelightsourceis withalargeboltinthecenter.Thelightsourceis
notvisible,butitappearstobeanLEDorother notvisible,butitappearstobeanLEDorother
smalllightsource.Theimageisonawhiteback-smalllightsource.Theimageisonawhiteback-
ground,andthelightfixtureistheonlyobjectin ground,andthelightfixtureistheonlyobjectin
theimage. theimage.
NorthCarolinaTarHeels aproductphotoofaNorthCarolinaTarHeels AproductphotoofaNorthCarolinaTarHeels
Team Logo Gray Ad-GrayAdjustableHatGS.Thehatisagrayand GrayAdjustableHatGS.Thehatisagrayand
justableHatGS bluesnapbackhatwithabluelogoofinterlocking bluesnapbackhatwithabluelogoofinterlocking
letters“NC”onthefront.Thehathasablueflat letters“NC”onthefront.Thehathasablueflat
billandablueadjustablesnapbackclosureon billthatcontainsalabelstickerthatishardto
the back. The logo is surrounded by a white see,andablueadjustablesnapbackclosureon
outline,creatingasharpcontrastwiththegray the back. The logo is surrounded by a white
background.Theimageconsistsoftwophotosof outline,creatingasharpcontrastwiththegray
thesamehat,asmalleroneinthetopleftsection background.Theimageconsistsoftwophotosof
thatshowsthebackofthehat,andabiggerone thesamehat,asmalleroneinthetopleftsection
inthebottomrightsectionshowingthefrontof thatshowsthebackofthehat,andabiggerone
thehat.Thebackgroundoftheimageiswhite. inthebottomrightsectionshowingthefrontof
thehat.Thebackgroundoftheimageiswhite.
Data Visualization with A photo of image features a graph created us-A photo of image representing data visualiza-
PythonandMatplotlib ingMatplotlib,awidely-useddatavisualization tion using Python and Matplotlib. The image
libraryforPython. Thegraphshowcasesthree showcasesthreecirclesarrangedinaspiral-like
circlesarrangedinaspiral-likepattern. Thein-pattern. Theinnermostcirclecontainstwodis-
nermostcirclecontainstwodistinct-shapedim-tinct-shapedimagesinyellowandblue,while
agesinyellowandblue,whileaquartershapeis aquartershapeisprominentlyorangeincolor.
prominentlyorangeincolor.Acrosstheimageis Acrosstheimageisthetext“Matplotlib”.Theen-
thetext“Matplotlib”.Theentirecompositionis tirecompositionissetagainstagreybackground.
setagainstagreybackground.
Table15: Re-alignedalt-textsfromMetaCLIP(Xuetal.,2024)images.