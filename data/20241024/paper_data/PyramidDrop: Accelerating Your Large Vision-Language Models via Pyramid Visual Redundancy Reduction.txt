PYRAMIDDROP: ACCELERATING YOUR LARGE
VISION-LANGUAGE MODELS VIA PYRAMID VISUAL
REDUNDANCY REDUCTION
LongXing1,2 QidongHuang1,2 XiaoyiDong2,3 JiajieLu1,2 PanZhang2
YuhangZang2 YuhangCao2 ConghuiHe2 JiaqiWang2 FengWu1 DahuaLin2
1USTC 2ShanghaiAILaboratory 3CUHK
ABSTRACT
In large vision-language models (LVLMs), images serve as inputs that carry a
wealth of information. As the idiom “A picture is worth a thousand words” im-
plies,representingasingleimageincurrentLVLMscanrequirehundredsoreven
thousandsoftokens. Thisresultsinsignificantcomputationalcosts, whichgrow
quadraticallyasinputimageresolutionincreases,therebyseverelyimpactingthe
efficiency of both training and inference. Previous approaches have attempted
to reduce the number of image tokens either before or within the early layers of
LVLMs.However,thesestrategiesinevitablyresultinthelossofcrucialimagein-
formation,ultimatelydiminishingmodelperformance. Toaddressthischallenge,
we conduct an empirical study revealing that all visual tokens are necessary for
LVLMs in the shallow layers, and token redundancy progressively increases in
the deeper layers of the model. To this end, we propose PyramidDrop, a vi-
sual redundancy reduction strategy for LVLMs to boost their efficiency in both
training and inference with neglectable performance loss. Specifically, we par-
tition the LVLM into several stages and drop part of the image tokens at the
end of each stage with a pre-defined ratio, creating pyramid-like visual tokens
across model layers. The dropping is based on a lightweight similarity calcu-
lation with a negligible time overhead. Extensive experiments demonstrate that
PyramidDropcanachievea40%trainingtimeand55%inferenceFLOPsacceler-
ationofLLaVA-NeXTwithcomparableperformance. Besides,thePyramidDrop
could also serve as a plug-and-play strategy for inference acceleration without
training,withbetterperformanceandlowerinferencecostthancounterparts. We
hope that the insights and approach introduced by PyramidDrop will inspire fu-
ture research to further investigate the role of image tokens in LVLMs and ex-
plore additional methods to enhance their efficiency. Our code is available at
https://github.com/Cooperx521/PyramidDrop.
1 INTRODUCTION
Inrecentyears,LargeVision-LanguageModels(LVLMs)haveemergedasacentralfocusindeep
learningresearch(Liuetal.,2024c;Daietal.,2023;Baietal.,2023;Zhangetal.,2024b;Chenetal.,
2023a; Huang et al., 2024b). We have witnessed remarkable progress across various application
domains,includingimageandvideounderstanding(OpenAI,2024;GeminiTeam,2023). Therapid
developmentofMLLMsisgraduallypavingthewayforartificialintelligencetointegrateintodaily
life(Lietal.,2023c;Zhuetal.,2023a;Zhangetal.,2023;Liuetal.,2024f;Chenetal.,2023b).
However, despite the advancements in large vision-language models (LVLMs), a significant chal-
lengeliesintheescalatingcomputationalcosts. Images,ascontinuousandinformation-richsignals,
exhibitsubstantialspatialredundancybutaredifficulttocompresslosslessly. Itresultsinexcessive
imagetokensandasteepincreaseintrainingandinferencecosts,whichbecomesparticularlypro-
nounced with higher image resolutions (Zhang et al., 2024b; Wang et al., 2024; Hu et al., 2024).
Thenumberofimagetokensincreasesquadraticallywiththeresolution,drivingthesequencelength
intothetensofthousands(Lietal.,2023a). Giventhatthecomputationalcomplexityoftransform-
ersscaleswithsequencelength,theassociatedcomputationalcostsbecomeprohibitivelyhigh(Liu
1
4202
tcO
22
]VC.sc[
1v74271.0142:viXraetal.,2024a;Xuetal.,2024). Consequently, thereisapressingneedtoreducetheredundancyin
visualinformationformoreefficientLVLMs.
Previousexplorationofimagetokencompressioncouldberoughlycategorizedintotwoideas:com-
pressingthetokennumberbeforefedintotheLVLM(Shangetal.,2024;Arifetal.,2024;Lietal.,
2023d;Yaoetal.,2024a)ordroppingpartofthetokensattheveryshallowlayeroftheLVLM(Chen
etal.,2024a). However, bothideasinevitablyhurttheperformanceofLVLMs: theformersuffers
fromtheinformationlossintroducedbytheircompression,andthelatterdropspartoftheinforma-
tionbeforetheLVLMsfullyunderstandthem.
To break through the limitations of the aforementioned ideas, we explore the nature of LVLMs in
understandingimagesfromanintuitivequestion: areallimagetokensnecessaryforallLVLMlay-
ers? Weconductanempiricalstudybyremovingdifferentratiosofimagetokensatdifferentlayers
oftheLVLMatinferencetimeandobservingthebenchmarkperformancechange.AsshowninFig-
ure1,theLVLMsaresensitivetowardtokendroppingonshallowlayers,regardlessofthedropping
ratio. However, in deeper layers, image tokens gradually become less critical to the final results.
TheresultsindicatethattheLVLMsunderstandtheimagelayer-by-layerandtheredundancywithin
imagetokensincreasescorrespondingly. Wefurthervisualizetheattentionbetweentheinstructions
andtheimagetokens,andweobservedaconsistentphenomenonthatinshallowlayers,theLVLMs
payattentiontomostimagetokenstounderstandtheimageglobally. Withthelayerincreasing, it
tendstofocusonthefewtokensthatarerelatedtotheinstructionandtherestareunnecessary.
Basedontheobservation,weintroducePyramidDrop,asimpleyeteffectiveimagetokenreduction
strategy for LVLMs to accelerate both training and inference without performance loss. Pyramid-
Drop divides the LVLM into several stages, dropping a portion of the image tokens at the end of
each stage according to a predefined ratio. We employ a lightweight attention module to rank the
imagetokens,whichincursnegligibleoverhead. Withthisdesign,weretainallimagetokensinthe
shallowlayerstoavoidinformationloss,whileprogressivelyreducingthenumberoftokensasthe
layersdeepentomaximizetrainingandinferenceefficiency.
Extensive experiments verify the effectiveness and efficiency of our PyramidDrop. For example,
LLaVA-NeXT-7B(Liuetal.,2024b)trainedwithPyramidDropcouldreducetrainingtimeby40%
withoutsacrificingperformanceacross15Vision-Languagetasks. Moreover,PyramidDropenables
the LLaVA-NeXT model to be trained with doubled input resolution with only 269 GPU hours,
which is 70% of the vanilla LLaVA-NeXT, and reaches a better performance on high-resolution
benchmarkslikeDocVQA(Mathewetal.,2021)andInfoVQA(Mathewetal.,2022). Furthermore,
PyramidDropcanfunctionasaplug-and-playstrategyforinferenceacceleration,offeringenhanced
modelperformanceandfewerFLOPsthanFastV(Chenetal.,2024a).
2 RELATED WORK
Token Reduction The large language model (LLM) realm has made several efforts in applying
token reduction for inference acceleration and KV cache compression(Han et al., 2023). Stream-
LLM(Xiaoetal.,2023)onlykeepsattentionsinksandthemostrecenttokenstoreducethesizeof
the KV cache. FastGen(Ge et al., 2023) introduces an adaptive KV cache management approach
thatoptimizesmemoryusagebyadjustingretentionstrategiesaccordingtothespecificpropertiesof
attentionheads. Heavy-HitterOracle(H2O)(Zhangetal.,2024c)employsastrategythatselectively
pruneskey-valuepairs(KVs)duringgeneration,utilizingascoringmechanismdrivenbycumulative
attentiontoinformtheremovalprocess. ScissorHands(Liuetal.,2024d)concentratesonidentify-
ingandretainingimportanttokensthatshowaconsistentpatternofattentionweightacrossprevious
token windows during generation. These works attempt to address the redundancy of text tokens
during the inference process in LLMs. As for visual tokens, existing works (Liang et al., 2022;
Kong et al., 2022; Ding et al., 2023; Cao et al., 2023; Shi et al., 2024; Xiong et al., 2024) make
explorations on Vision Language Models (VLMs) before the era of large vision-language models,
focusing on token reduction for vision transformers (ViTs). A recent work, FastV (Chen et al.,
2024a), makes an early attempt at visual token reduction in LVLMs, which drops visual tokens at
the second layer of LVLMs during inference. In contrast, our work makes a more comprehensive
studyof thevisual redundancyin LVLMsand proposesa pyramidvisual tokenreduction solution
forbothtrainingandinferenceofLVLMs.
2What is the bus's license plate number?
Layer2 Layer16
Progressively
focus on concentrate！
localized
regions
Uniform Concentrated
Distribution Distribution
(a) (b)
Figure 1: Observatioins about visual redundancy acoross layers. Left: TextVQA performance of
LLaVA-1.5 with varying ratio of retained image tokens at different layer. The preserved image
tokens are those that receive the highest attention from the text tokens. Right: Visualization of
attentionmapinshallowanddeeplayers.
Large Vision Language Models Enabled by the open-sourcing of large language models like
LLaMA(Touvron et al., 2023) and Vicuna(Chiang et al., 2023), LVLMs(Tong et al., 2024; Chen
etal.,2023c;Yaoetal.,2024b;Huangetal.,2024b;Liuetal.,2024e;Dingetal.,2024)havead-
vancedtheabilitytounderstandandgeneratediversecontentbyseamlesslyintegratinginformation
acrossmultiplemodalities,suchastext,images,andaudio. ModelslikeLLaVA(Liuetal.,2024c),
InstructBLIP(Dai et al., 2023), and MiniGPT-4(Zhu et al., 2023b) have pushed the boundaries of
thisfield,enablinguserstointeractwiththeseintelligentsystemsthroughmultimodalprompts(Ma
etal.,2024;Qiaoetal.,2024),includingimagesandtext(Yeetal.,2024;Chenetal.,2024c;Huang
et al., 2024a; Zhang et al., 2024a). Recent advances (Zhang et al., 2024b; Wang et al., 2024; Hu
etal.,2024)havesignificantlyincreasedthenumberofimagetokensforhigh-resolutionimageun-
derstanding, resulting in substantial costs for training and inference in LVLMs. This underscores
thecriticalimportanceofdevelopingmoreefficienttrainingandinferencemethodsforLVLMs.
3 METHOD
3.1 STUDYOFVISUALTOKENREDUNDANCYINLVLMS
The fundamental design of PyramidDrop stems from an intuitive question: are all image tokens
necessaryforallLVLMlayers? ToexploreitandrevealthenatureofLVLMs, weconductatwo-
variableexperimentbyremovingdifferentratiosofimagetokensatdifferentlayersoftheLVLMat
inferencetimeandobservingthebenchmarkperformancechange.
In detail, we select LLaVA-v1.5-7B (Liu et al., 2024c) as the base model, and employ a popular
LVLM benchmark, TextVQA (Singh et al., 2019), as the evaluation data. TextVQA consists of
a substantial number of images that contain fine-grained information like text. The questions in
TextVQAfocusonthetextualelementswithinimages,requiringLVLMstocapturetheglobalimage
informationwhileminingthegreatdetailedvisualclues. Thischaracteristicincreasesthemodel’s
sensitivitytoimagetokencompression,enablingamorepreciseevaluationofredundancy.
Considering LLaVA-v1.5-7B consists of 32 layers, we drop varying proportions of image tokens
during inference at layer 2, 8, 16, and 24 to assess redundancy at different layers. The ranking of
tokensisbasedontheattentionvaluesoftexttokenstowardsimagetokens,withtheretainedimage
tokenscorrespondingtothosewiththehighestattentionvalues.AsillustratedinFigure1(a),atlayer
2, the LVLMs are sensitive toward token dropping on shallow layers, regardless of the dropping
ratio. Thisindicatesmostoftheimagetokensinshallowlayersplayaimportantroleinproviding
3Rank & Drop
F
attention 
Stage S Sequence Length
Text token Average Sequence Length per Stage
Transformer Layer ×N 2389
<system message>Who is standing on the 2.0k
dining table in the picture? A mouse. 1255 A lev ne gr ta hg de e s ce rq ee an sc ee s
tokenizer 1.0k 688 405 rapidly!
0.0
1 2 3 4
Stage s
Vision token
Stage 2 Rank & Drop (stage 2）
Rank and Drop attention
Transformer Layer ×N
Drop Low
Patch
Division
Stage 1 Rank & Drop (stage 1）
Rank and Drop attention
Vision Encoder
Projection (MLP) Transformer Layer ×N
Drop Low
Concat
system tokens image tokens instruction tokens
Figure2: OverviewofPyramidDrop. WedividetheforwardpassoftheLLMintomultiplestages,
and drop part of the image tokens at the end of each stage with a pre-defined ratio. The dropping
is based on a lightweight attention calculation with a negligible time overhead, and according to
thiscriterion,theLLMaccuratelyselectsimportantimagetokensrelatedtoinstruction. Duetothe
efficientredundancyreductionstrategy,theaveragesequencelengthdecreasesrapidly.
informationforansweringtheinstruction. Withthelayerincreases,theredundancyofimagetokens
increasesrapidly. Atlayer16,evenpreservingonly10%ofimagetokenswillnotcauseanobvious
performancedecline. Notably,atlayer24,themodelperformanceisnearlyirrelevanttotheimage
tokens, indicating that the model has already captured the necessary image information and the
imagetokensareredundantforthemodelnow.
Wefurthervalidateourhypothesiswithanattentionmapcomparisonbetweendifferentlayers. As
showninFigure1(b),theLVLMpaysattentiontomostoftheimagetokensatshallowlayersandthe
attentiontodifferenttokensshowsauniformpattern. Onthecontrary,atthemiddleoftheLVLMs,
theattentionshowsasparsepatternandmainlyfocusesonthequestionrelatedimagelocalparts.
3.2 PYRAMIDDROP
Previousresearchonimagetokencompressiontypicallydropsimagetokensbeforepassingthemto
the language model or uses a fixed compression ratio across all language model layers. However,
as we analyzed in Sec 3.1, redundancy is not consistent across different layers. Redundancy of
imagetokensisrelativelyminimalintheshallowlayersandbecomesprogressivelylargerindeeper
layers. Thus, uniformly compressing image tokens across layers may lead to the loss of valuable
informationintheshallowlayerswhileretainingunnecessaryredundancyinthedeeperlayers.
Inspiredbythisobservation,weproposePyramidDrop,whichfullyleverageslayer-wiseredundancy
tocompressimagetokens. ThepipelineoftheproposedPyramidDropisillustratedinFigure2. To
maximize training efficiency while preserving the essential information of the image tokens, we
choosetodividetheforwardpassoftheLLMintomultiplestages. Intheshallowlayers,weretain
a higher proportion of image tokens to preserve the entire vision information. At the end of each
stage,wepartiallydroptheimagetokens,untilnearlyalltheimagetokensbeingeliminatedinthe
deeper layers. This approach allows us to optimize training efficiency while maintaining critical
information.
4
···
N
snekoTLVLM Pre-fill Formulation. We denote the vision encoder as V, the vision-language projector
as P, the language model as L, a pretrained LVLM as M = (L,V,P), where L = (L ,F).
0
The language model consists of tokenizer L and J-layer transformer decoder F. We formulate
0
an image-text pair as (V,T), where the text is composed with an instruction and an answer T =
{T ;T }1. The input of the transformer F contains both the image tokens v = P(V(v)) and the
i a 0
texttokenst =L (T).
0 0
Duringtheforwardpassoftokens,wecanobtainthehiddenstatesv ,t ofvisiontokensandtext
j j
tokensinlayerj,formally:
v ,t =F (v ,t ) (1)
j j j j−1 j−1
Pyramid Visual Redundancy Reduction. We partition the language into S = {s }S stages,
n n=0
andremovetheimagetokensv withapre-definedratioλattheendofeachstage. Formally,with
theimagetokensv astheinputofstages ,weremove⌈(1−λ)·|v |⌉tokensfromthev and
sn n sn sn
treattherestimagetokensasthenextstageinputv .
sn+1
Following our observation in Sec 3.1, the attention value between image and text tokens could
reflecttheimagetokenimportanceproperly, sowebasedonittorealizethedropoperation. With
the concern of calculation efficiency and training-inference consistency, we calculate the attention
between all the image tokens and the last token of the instruction (we denote it as tI, the last-
j
instructiontokeninthefollowing).
Formally,wedenotethelastlayerofstages asF ,weobtainkeystatesoftheimagetokensaskv
n j j
andthequerystateoflastinstructiontokenqtI withthefollowingoperation:
j
kv =K (v ), qtI =Q (tI). (2)
j j j j j j
whereQ ,K arethequerymatrixandthekeymatrixreusedfromtheself-attentionblockofF .
j j j
Wecalculatethe similaritywithqtI ×(kv)T anddrop partoftheimagetokens basedonthedrop
j j
ratio λ. The image token number decreases exponentially stage by stage, and close to zero in the
deeperlayers. Wedenotetheimagetokennumberofv asV = |v |,andtheimagetokennumber
0 0
ateachstageV couldbecalculatedas:
s
V =V ·λs−1, s=1,2,...,S
s 0
EfficiencyAnalysisofPyramidDrop Hereweanalyzetheefficiencyfromtwoparts: thecompu-
tationoverheadintroducedbyPyramidDrop,andtheinputsequencecomputationcosteconomized
byPyramidDrop.
TheextracomputationcostintroducedbyPyramidDropmainlylayinthesimilaritycomputingfor
imagetokenranking. Benefitingfromourdesign,thecalculationisonlybetweenaquerytokeand
V imagetokens,soitscomputationcomplexityisO(n)andonlyS−1timesintheforwardprocess.
s
Further,wenoticetheimportanceofFalshAttentioninpractice,sowekeepusingitduringtraining
andextractthequeryandkeytokenfromtheoriginalforwardtocalculateourlightweightsimilarity
matrix.
When it comes to the computation cost economized by PyramidDrop. With the consideration of
FlashAttn(Daoetal.,2022),weroughlydefinetheforwardinferencecostofalayerwithN image
tokensasalinearfunctionwithaconstantfactorcthatc·L,sotheoverallcomputationcostofan
LVLMwithLlayersisc·N·L. WhenusingPyramidDropwithSstagesandtheratioλ,theoverall
computationcostis:
1−λS
·c·N ·L (3)
S·(1−λ)
For example, if λ = 0.5 and we reduce the redundancy with 4 stages, it could save nearly 53.2%
computationcosttheoretically,andwefindthissettinghasaneglectableperformanceinfluencefor
modelsinpractice.
1Hereweomitthesystempromptandchatformatforillustrativepurposes
54 EXPERIMENT
4.1 SETUP
Models WeverifytheeffectivenessandgeneralizeoftheproposedPyramidDorpbyexperimenton
LVLMswithdifferentarchitecturesandinputresolution. Indetail,westudyLLaVA-1.5-Vicuna-7B
(Liuetal.,2024c),LLaVA-NeXT-Vicuna-7B(Liuetal.,2024b).LLaVA-1.5isthemostwidelyused
open-sourceLVLMbackboneforresearch,whichisdesignedwithasimpleyeteffectivearchitecture
thatmapsthe576imagefeaturesfromtheCLIPencoderastheLLMinputwithaprojector.LLaVA-
Nextisthehigh-resolutionextensionofLLaVA-1.5,whichsupportsatmost2880imagetokensand
hasbetterhigh-resolutioncapability.
Benchmarks To thoroughly evaluate our image token compression strategy, we conduct experi-
ments across 14 benchmarks. The MME Benchmark (Fu et al., 2023) assesses the perception and
cognitive abilities of LMMs. MMBench and MMBench-CN (Liu et al., 2023) are benchmarks
that manually craft questions to evaluate vision-related reasoning and perception in both English
and Chinese, respectively. SEED (Li et al., 2023b), generated with the aid of GPT-4, comprises
a dataset of approximately 19,000 questions pertaining to images and videos. MM-Vet (Yu et al.,
2023)leveragesGPT-4forasix-dimensionalevaluationofLMMcapabilities. Intherealmoftradi-
tionalVQAbenchmarks,suchasVQA-v2(Goyaletal.,2017)andVizWiz(Gurarietal.,2018),are
alsoutilized. Additionally,severalbenchmarksfeaturinghigher-resolutionvisualcontent,including
DocVQA (Mathew et al., 2021), ChartQA (Masry et al., 2022), InfographicVQA (Mathew et al.,
2022),andTextVQA(Singhetal.,2019). Finally,MMStar(Chenetal.,2024b)presentstaskswith
strongvisualdependency,minimaldataleakage,andrequiressophisticatedmultimodalcapabilities.
EfficientnessEvaluation Weconsiderboththetrainingtimeefficiencyevaluationandinference
time throughout. For training efficiency, we report the real training GPU hours with the same de-
vices. Forinferencethroughout,wefollowtheFastV(Chenetal.,2024a)andreporttheFLOPsof
the image token part. In detail, we consider the FLOPs of the multi-head attention and the feed-
forwardnetworkmodulesas4nd2+2n2d+2ndm,wherenisthenumberoftokens,disthehidden
state size, and m is the intermediate size of the FFN. Considering there are three linear layers in
FFNofLLaMA,theFLOPsismodifiedas4nd2+2n2d+3ndm. OurPyramidDrophasdifferent
imagetokennumbersatdifferentstagesandtheFLOPScouldbecalculatedby:
S−1
(cid:88) K ×(cid:0) 4n d2+2n2d+3n dm(cid:1) s.t. n =λs×n, s=0,1,2,...,S−1 (4)
s s s s s
s=0
Implementationdetails GiventhattheLLMwithintheLVLMusedinourexperimentsconsists
of32layers,weemployastraightforwardapproachbyfixingS to4,effectivelydividingtheLLM
intofourequalparts. Thissegmentationallowstheforwardpasstobedividedintofourstages,with
thenumberofimagetokensdecreasingexponentiallyateachstage. Duringacceleratedtraining,we
canadjustthevalueofλtocontroltheproportionofimagetokensthatarepruned,andbydefault,
λ = 0.5. Weconductalltheexperimentson8NVIDIAA10080GBGPUs. Thepartialevaluation
likeMMStarisdoneontheOpenCompassVLMEvalKit(Duanetal.,2024)forconvenience.
It is important to note that, since the LLaVA-NeXT model’s data and training code are not open-
source, we conduct training based on the open-source project Open-LLaVA-NeXT (Lin & Long,
2024). Due to differences in a portion of the training data, the benchmark performance may vary
comparedtothatofLLaVA-NeXT(Liuetal.,2024b)blog.
4.2 EFFICIENTOFPYRAMIDDROPINTRAINING
PyramidDrop is effective for diverse architectures. We first study the PyramidDrop on both
LLaVA-1.5 and LLaVA-Next. As shown in Table 1, PyramidDrop reduces the training time (in-
cluding both pretraining and fine-tuning stages) of the LLaVA-Next from 366 to 218 GPU hours,
resultinginanimpressive40%reductioninoveralltime. Besidesthepromisingefficiencyimprove-
ment, the model’s performance remains comparable to the original on 14 different benchmarks.
Notably, for fine-grained benchmarks like TextVQA, DocVQA, and OCRVQA, images contain a
6Table1: LVLMwandw/oourmethodon6benchmarks. Benchmarknamesareabbreviateddueto
space limits. MMB: MMBenchmark (Liu et al., 2023); MMBCN: MMBench-Chinese (Liu et al.,
2023);SEEDI: SEED-Bench(Image)(Lietal.,2023b). WedenotePyramidDropasPDrop.
GPU Infer MM
Model Train&Infer #patches MME MMB MMBCN SEEDI POPE Avg
hours Flops(T) Star
vanilla 366 5 20.8 1534.1 68.7 60.5 71.1 41.1 86.1 67.4
LLaVA PDrop 218 5 9.46 1540.8 67.8 60.6 69.9 41.7 86.5 67.3
-NeXT-7B
vanilla 483 9 40.6 1544.7 67.4 60.0 69.5 40.0 86.3 66.7
PDrop 269 9 18.1 1542.0 68.1 61.0 70.3 40.9 86.6 67.3
LLaVA vanilla 104 1 3.82 1510.7 64.3 58.3 66.1 33.2 85.9 63.9
-1.5-7B PDrop 79 1 1.78 1467.3 66.1 58.5 65.5 34.0 86.0 63.9
Table2: LLaVA-NeXT-7Bonother8benchmarks. Wereportmorebenchmarkswhichcontainlots
offine-grainedcontenttoexaminetheperformance.
GPU Doc Info Text Chart OCR VQA Viz
Model Train&Infer #patches GQA Avg
hours VQA VQA VQA QA VQA V2 Wiz
vanilla 366 5 70.0 33.3 67.2 64.0 63.7 81.7 59.6 64.2 63.0
LLaVA PDrop 218 5 69.0 31.7 67.7 63.0 63.1 81.5 61.0 63.9 62.6
-NeXT-7B
vanilla 483 9 74.3 36.2 67.6 63.0 63.8 81.6 58.0 63.5 63.5
PDrop 269 9 75.0 37.4 68.4 64.3 63.5 81.7 60.6 64.1 64.4
largeamountoftextandevendocuments,whichrequestadenseandfine-grainedunderstandingof
the image. Even in this case, our approach still maintain performance at the original level. This
indicatesthatourmethodsuccessfullycompressesredundantinformationwhilepreservingthemost
criticalimagecontent.
InthecaseofLLaVA-1.5, whichprocessesfewerimagetokenspersample, theaccelerationisnot
as pronounced as with LLaVA-NeXT. However, it still offers a nearly 20% improvement in speed
with comparable performance. This underscores the potential of our method to enhance training
efficiencyacrossdifferentmodelconfigurations.
PyramidDropenableslargerresolutionwithconstrainedcost. ThePyramidDropisproposed
toreducetheredundancywithinimagetokens,andasweobservedabove,itenjoyshigherspeedup
withtheincreaseoftheimage/texttokenratio. Inthispart,weexploreitsperformancewithhigher
image/texttokenratio.Indetail,LLaVA-NeXTisdesignedwithaflexibleimageprocessingstrategy
inwhichanimageisdividedintoamaximumoffourlocalpatchesandaglobalpatch,leadingtoat
most2880imagetokens.WedenoteitasLLaVA-NeXT-p5andexperimentontheLLaVA-NeXT-p9
byincreasingthemaximumlocalpatchesinto8patches.
AsshowninTable2,withtheincreasedimage/textratio,PyramidDropreachesahigherspeedupthat
only269GPUhoursisusedfortraining,whichisonly55%ofthevanillaLLaVA-Next-p9. Besides
the superb speedup, the model trained with PyramidDrop achieves a slightly higher average per-
formanceacrossthe14benchmarks. Wearguetoomanyimagetokenswithredundantinformation
may confuse the LVLMs and hinder their performance, while our PyramidDrop efficiently reduce
theimagetokensnumberandhelpstheLVLMtofocusonthecriticalinformation. Furthermore,it
isworthnotingthatthetrainingtimeiseven70%oftheoriginalLLaVA-Next-p5butachievesbetter
performanceondiversetasks,showcasingthesuperbefficiencyandeffectivenessofPyramidDrop.
PyramidDrop training encourages LVLMs to understand images compactly. Then we dive
intothepropertiesofthemodeltrainedwithPyramidDropandconductexperimentstoinvestigate
thechangesinimagetokenredundancy. Twomodelsareemployedforthisexploration: thevanilla
LLaVA-1.5 and the LLaVA-1.5 trained with our approach. As illustrated in Figure 3, we plot the
TextVQAscoresagainsttheretainedimagetokensatlayers2,8,16,and24,maintainingthesame
experimentalsettingsasSec3.1. WefindthatthecurveofmodelstrainedwithPyramidDropkeeps
higherthanthevanillaone. Thephenomenonsuggeststhat,foragivenproportionofretainedimage
tokens, model trained with PtramimdDrop preserves more image information and achieves better
7Layer2 Layer8 Layer16 Layer24
60 60 60 60
55 55 58 58
56
56
50 50
54
54
45 45
Original Original 52 Original Original
PDrop PDrop PDrop 52 PDrop
40 40
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
Ratio Ratio Ratio Ratio
Figure 3: We compare the performance of the original LLaVA-1.5 and LLaVA-1.5 trained using
PDrop, where we preserve different ratios of image tokens at layer 2, 8, 16, and 24, respectively.
Thehorizontalaxisrepresentstheproportionofretainedimagetokensaccordingtoattentionscore.
Table3: Performance gainwith modelstrained withPyramidDrop. Directly applyingefficient in-
ferencestrategieslikeFastVtomodelstrainedwithPyramidDropyieldssubstantialimprovement.
Infer
Model Train Infer ChartQA DocVQA TextVQA MME SQAI POPE Average
Flops(T)
vanilla vanilla 20.8 64.0 70.0 67.2 1534.1 70.4 86.1 72.4
PDrop PDrop 9.46 63.0 69.0 67.7 1540.8 70.1 86.5 72.2
LLaVA
-NeXT-7B vanilla FastV 10.6 55.9 62.1 66.0 1482.0 69.2 85.5 68.8
PDrop FastV 10.6 59.9 63.9 65.6 1492.7 68.9 86.8 70.0
∆ +4.0 +1.8 -0.4 +0.5 -0.3 +1.3 +1.2
Table 4: Ablation studies results. We adjust λ form 0.4 to 0.6 for investigating the influence on
performanceandtrainingtime.
GPU Infer Doc Info
Model λ #patches MME MMB GQA MMBCN SEEDI Avg
hours Flops(T) VQA VQA
vanilla 366 5 20.8 1534.1 68.7 64.2 60.5 71.1 70.0 33.3 63.5
LLaVA 0.4 204 5 8.22 1558.4 68.1 63.7 60.5 69.5 66.6 31.8 62.6
-NeXT-7B 0.5 218 5 9.46 1540.8 67.8 63.9 60.6 69.9 69.0 31.7 62.8
0.6 240 5 11.0 1511.4 68.1 64.1 60.5 70.4 69.8 33.0 63.1
vanilla 104 1 3.82 1510.7 64.3 62.0 58.3 66.1 21.4 20.4 52.6
LLaVA 0.4 75 1 1.54 1478.8 66.2 61.7 58.0 64.5 21.1 19.9 52.2
-1.5-7B 0.5 79 1 1.78 1467.3 66.1 61.9 58.5 65.5 21.5 20.2 52.4
0.6 82 1 2.06 1471.8 65.9 62.0 58.9 65.1 22.5 21.0 52.7
performance.Alternatively,atequivalentperformancelevels,ourmethodallowsforahigherratioof
imagetokenstocompress. Thisimprovementcanprimarilybeattributedtothemulti-stagetraining
strategy,whichprogressivelyprunesimagetokens,encouragingthemodeltoconsolidateessential
informationintoasmallersetoftokens,resultinginmoredenselyinformativerepresentations.
WefurthervalidateourhypothesisbyreplacingtheinferencestrategywithFastV.Asdemonstrated
in Table 3, directly applying efficient inference strategies like FastV to models trained with Pyra-
midDrop yields substantial improvements. Notably, there is a 1.3% increase in POPE and a 0.5%
increase in MME, with even more pronounced gains observed on high-resolution benchmarks:
ChartQA shows an increase of 4%, while DocVQA improves by 1.8%. These results provide
compelling evidence for our hypothesis that training with PyramidDrop encourages the LVLMs
tounderstandimagescompactly,whichisageneralizedresult,ratherthananoverfittothetraining
strategy.
BalancingPyramidDropperformanceandefficiencywithλ. λbalancestheperformanceand
efficiencyofPyramidDrop,alargerλpreservesmoreimageinformationbutslowsdownthetraining,
and a smaller λ has higher speedup while may influence the model performance. In this part, we
studytheinfluenceofλonbothLLaVA-1.5andLLaVA-NeXT.
8
AQVtxeTTable5:Inferenceaccelerationperformance.WecomparePDrop,FastVandvanillamodel,andfind
PDropoutperformsFastVonalmostallbenchmarks. PDrophereisasaninference-onlystrategy.
Inference
Model TFLOPS MME SQAI MMBCN GQAPOPETextVQAChartQADocVQA Avg
Strategy
vanilla 20.8 1534.1 70.4 60.5 64.2 86.1 67.2 64.0 70.0 69.9
LLaVA FastV 10.6 1482.0 69.2 60.0 63.0 85.5 66.0 55.9 62.1 67.0
-NeXT-7B PDrop 9.5 1533.0 69.4 59.9 63.9 86.4 67.0 59.1 65.6 68.5
∆ +2.5 +0.2 +0.1 +0.9 +0.9 +1.0 +3.2 +3.5 +1.5
vanilla 3.82 1510.7 66.8 58.3 62 85.9 58.2 18.2 21.4 55.8
LLaVA FastV 2.01 1475.6 68.5 56.8 59.6 84.8 57.1 17.8 19.2 54.7
-1.5-7B PDrop 1.78 1500.8 69.2 58.5 60.1 84.8 57.5 18.6 21.1 55.6
∆ +1.3 +0.7 +1.7 +0.5 +0.0 +0.4 +0.8 +1.9 +0.9
As shown in Table 4, we vary the λ from 0.4 to 0.6 and report the model performance on both
generalandhigh-resolutionbenchmarks. Forthegeneralbenchmarks,weobservearelativerobust
performanceamongdifferentlambda,thisindicatesthatformostquestions,theinformationwithin
images is somewhat redundant. When it comes to the DocVQA, which requires a fine-grained
understandingonhigh-resolutionimages,themodelperformanceshowsacleardeclinewhentheλ
decreasesto0.4. Itisreasonableasthelossofcriticalimageinformationandwecouldanticipatea
morepronouncedperformancedeclinewiththeλkeepsdecreasing. Therefore,weoptforλ=0.5,
whichmaintainscomparableperformancetothebaselinewhilealsoyieldingasignificantreduction
inprocessingtime.
4.3 EFFICIENTOFPYRAMIDDROPININFERENCE
PyramidDropoutperformsSOTAmethodsasainference-onlystrategy. AsillustratedinTa-
ble 5, we directly apply the multi-stage compression strategy during the inference phase of the
vanillamodel,comparingitwiththeinferenceaccelerationapproach,FastV.TheresultsonLLaVA-
Next demonstrate that our method significantly outperforms FastV across various critical bench-
marks. Specifically,weachieveanimpressivescoreof1533.0onMME,surpassingFastvby2.5%,
while also exceeding it by 0.9% on both POPE and GQA. Notably, the advantages of our method
becomeevenmorepronouncedinhigh-resolutionbenchmarks. Forinstance,ontherelativelychal-
lenging DocVQA, our approach outperforms FastV by 3.5%, and on ChartQA and TextVQA, we
achieveimprovementsof3.2%and1%respectively.
Results from LLaVA-1.5 reveal similar trends across multiple benchmarks, including MME, Sci-
enceQA, and MMBenchCN, where our method not only demonstrates superior performance but
alsoachievesagreaterreductioninFLOPs. Whencomparedtothebaseline,ourapproachconsis-
tently reaches comparable performance levels across most benchmarks, while effectively mitigat-
inginformationlossinhigh-resolutionbenchmarks. ThesefindingsindicatethatFastV’spremature
compressionofimagetokensleadstoinevitablyimageinformationlossandsignificantperformance
declines in many benchmarks, whereas our multi-stage compression strategy preserves critical in-
formationfromimagetokenswhilemaximizingtheeliminationofredundancy. Theobservationis
alsoconsistentwithourfindinginSec3.1thatinshallowlayers,mostimagetokensarecriticalfor
LVLMstounderstandtheimageproperly,whileinthedeeplayers,mostofthemareredundantfor
theLVLMs.
PyramidDropenjoysabettertrade-offbetweenperformanceandinferencecost. Wefurther
compare PyramidDrop and FastV under a precise FLOPs-constrained setting with LLaVA-NeXT-
7B.Inpractice,weadjustthedroprateofFastVandtheλofourPyramidDroptocontrolthemodel
inferenceFLOPsandevaluatethemodelbenchmarkperformance.AstheFLOPs-performancecurve
shown in Figure 4, our PyramidDrop consistently outperforms FastV under different settings and
across diverse benchmarks. For example, under a constraint of 12 TFLOPs, PyramidDrop outper-
forms FastV with 3.0% on DocVQA and 2.6% on ChartQA. When we reduce the inference cost
toonly8TFLOPs, theperformancegapincreases, withPyramidDropsurpassingFastVby6%on
DocVQA,and5.9%onChartQA.Theresultsfurtherprovethatourmulti-stageredundantreduction
strategymatchesthenatureofLVLMsandenablesthemodeltounderstandtheimagebetterunder
constrainedinferencecost.
965
70
64
65 60
63
60 55
55 62
50
50 PDrop PDrop PDrop
FastV 45 FastV 61 FastV
45
10 15 20 10 15 20 10 15 20
TFLOPS TFLOPS TFLOPS
Figure 4: The performance of LLaVA-NeXT-7B with different inference acceleration strategies.
PDrop(withouttraining)outperformsFastVonDocVQA,ChartQA,andGQAwithacrossvarious
inferencecostbudgets.
What is the year above the clock in the picture?
Original Layer8 Layer16 Layer24
The year above the clock in the picture is 1856.
What color is the girl on the left wearing in the picture?
Original Layer8 Layer16 Layer24
The girl on the left is wearing a green dress.
Figure5: VisualizationoftokendroppinginLLMofLLaVA-1.5. Wecomputetheattentionscore
of image tokens received from the last instruction token as the ranking criterion, and find LLM
accuratelyretainimagetokensaccordingtoinstruction.
LVLMwithPyramidDropeffectivelypreservesimagetokensrelatedtoinstruction. Asshown
in Figure 5, we visualize the image tokens retained by LLaVA-1.5 with PyramidDrop in different
stages. Itisevidentthatwhentheuserasksaboutasmallobjectintheimage,theLLMaccurately
identifiestheregioncontainingtherelevantinformationbasedontheinstructionsandprovidesthe
correctanswer. ThisdemonstratesthatourmethodeffectivelyleveragestheLLM’snaturetounder-
standimages. ThetokendroppinginPyramidDropappliedduringinferencedoesnotresultinthe
lossofvaluableinformation.
5 CONCLUSION
WehaveintroducedPyramidDrop, asimpleyeteffectivestrategyforreducingvisualtokenredun-
dancyinlargevision-languagemodels(LVLMs)toenhanceefficiencywithnegligibleperformance
loss. Ourempiricalstudyrevealsthatwhileallvisualtokensarenecessaryintheshallowlayersof
LVLMs,tokenredundancyprogressivelyincreasesindeeperlayers. Extensiveexperimentsdemon-
stratethatPyramidDropcanachievesignificantaccelerationinbothtrainingandinference.
10
AQVcoD AQtrahC
AQGREFERENCES
Kazi Hasan Ibn Arif, JinYi Yoon, Dimitrios S Nikolopoulos, Hans Vandierendonck, Deepu John,
and Bo Ji. Hired: Attention-guided token dropping for efficient inference of high-resolution
vision-languagemodelsinresource-constrainedenvironments. arXivpreprintarXiv:2408.10945,
2024.
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang
Zhou,andJingrenZhou.Qwen-vl:Afrontierlargevision-languagemodelwithversatileabilities.
arXivpreprintarXiv:2308.12966,2023.
QingqingCao,BhargaviParanjape,andHannanehHajishirzi. Pumer: Pruningandmergingtokens
forefficientvisionlanguagemodels,2023.URLhttps://arxiv.org/abs/2305.17530.
KeqinChen,ZhaoZhang,WeiliZeng,RichongZhang,FengZhu,andRuiZhao.Shikra:Unleashing
multimodalllm’sreferentialdialoguemagic. arXivpreprintarXiv:2306.15195,2023a.
LiangChen,HaozheZhao,TianyuLiu,ShuaiBai,JunyangLin,ChangZhou,andBaobaoChang.
Animageisworth1/2tokensafterlayer2: Plug-and-playinferenceaccelerationforlargevision-
languagemodels. arXivpreprintarXiv:2403.06764,2024a.
Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua
Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint
arXiv:2311.12793,2023b.
LinChen, JinsongLi, XiaoyiDong, PanZhang, YuhangZang, ZehuiChen, HaodongDuan, Jiaqi
Wang,YuQiao,DahuaLin,etal. Areweontherightwayforevaluatinglargevision-language
models? arXivpreprintarXiv:2403.20330,2024b.
Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong
Duan,BinLin,ZhenyuTang,etal. Sharegpt4video: Improvingvideounderstandingandgenera-
tionwithbettercaptions. arXivpreprintarXiv:2406.04325,2024c.
Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Car-
los Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. Pali-x: On scaling up a
multilingualvisionandlanguagemodel. arXivpreprintarXiv:2305.18565,2023c.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot
impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April
2023),2(3):6,2023.
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Albert Li, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose
vision-language models with instruction tuning. ArXiv, abs/2305.06500, 2023. URL https:
//api.semanticscholar.org/CorpusID:258615266.
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re´. Flashattention: Fast and
memory-efficientexactattentionwithio-awareness,2022.URLhttps://arxiv.org/abs/
2205.14135.
Shuangrui Ding, Peisen Zhao, Xiaopeng Zhang, Rui Qian, Hongkai Xiong, and Qi Tian. Prune
spatio-temporal tokens by semantic-aware temporal accumulation. In Proceedings of the
IEEE/CVFInternationalConferenceonComputerVision,pp.16945–16956,2023.
Shuangrui Ding, Zihan Liu, Xiaoyi Dong, Pan Zhang, Rui Qian, Conghui He, Dahua Lin, and
JiaqiWang. Songcomposer: Alargelanguagemodelforlyricandmelodycompositioninsong
generation. arXivpreprintarXiv:2402.17645,2024.
Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong,
Yuhang Zang, Pan Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. Vlmevalkit: An open-source
toolkitforevaluatinglargemulti-modalitymodels,2024. URLhttps://arxiv.org/abs/
2407.11691.
11ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,ZhenyuQiu,Wei
Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. Mme: A comprehensive
evaluationbenchmarkformultimodallargelanguagemodels. arXivpreprintarXiv:2306.13394,
2023.
Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells
youwhattodiscard: Adaptivekvcachecompressionforllms. arXivpreprintarXiv:2310.01801,
2023.
Gemini Team. Gemini: a family of highly capable multimodal models. arXiv preprint
arXiv:2312.11805,2023.
YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBatra,andDeviParikh.Makingthevinvqa
matter: Elevatingtheroleofimageunderstandinginvisualquestionanswering. InProceedings
oftheIEEEconferenceoncomputervisionandpatternrecognition,pp.6904–6913,2017.
DannaGurari,QingLi,AbigaleJStangl,AnhongGuo,ChiLin,KristenGrauman,JieboLuo,and
Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pp.3608–3617,
2018.
ChiHan, QifanWang, WenhanXiong, YuChen, HengJi, andSinongWang. Lm-infinite: Simple
on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137,
2023.
AnwenHu,HaiyangXu,JiaboYe,MingYan,LiangZhang,BoZhang,ChenLi,JiZhang,QinJin,
FeiHuang,etal.mplug-docowl1.5:Unifiedstructurelearningforocr-freedocumentunderstand-
ing. arXivpreprintarXiv:2403.12895,2024.
QidongHuang,XiaoyiDong,PanZhang,BinWang,ConghuiHe,JiaqiWang,DahuaLin,Weiming
Zhang,andNenghaiYu. Opera: Alleviatinghallucinationinmulti-modallargelanguagemodels
viaover-trustpenaltyandretrospection-allocation. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pp.13418–13427,2024a.
Qidong Huang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Dahua Lin,
WeimingZhang, andNenghaiYu. Decipheringcross-modalalignmentinlargevision-language
modelswithmodalityintegrationrate. arXivpreprintarXiv:2410.07167,2024b.
ZhenglunKong,PeiyanDong,XiaolongMa,XinMeng,MengshuSun,WeiNiu,XuanShen,Geng
Yuan,BinRen,MinghaiQin,HaoTang,andYanzhiWang. Spvit: Enablingfastervisiontrans-
formersviasofttokenpruning,2022. URLhttps://arxiv.org/abs/2112.13890.
Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang, Fanyi Pu, and Ziwei Liu. Otterhd: A
high-resolutionmulti-modalitymodel. arXivpreprintarXiv:2311.04219,2023a.
BohaoLi,RuiWang,GuangzhiWang,YuyingGe,YixiaoGe,andYingShan. Seed-bench: Bench-
marking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125,
2023b.
JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-2:Bootstrappinglanguage-imagepre-
trainingwithfrozenimageencodersandlargelanguagemodels. ArXiv,abs/2301.12597,2023c.
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image
pre-trainingwithfrozenimageencodersandlargelanguagemodels. InInternationalconference
onmachinelearning,pp.19730–19742.PMLR,2023d.
Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie. Not all
patchesarewhatyouneed:Expeditingvisiontransformersviatokenreorganizations,2022. URL
https://arxiv.org/abs/2202.07800.
Chen Lin and Xing Long. Open-llava-next: An open-source implementation of llava-next se-
ries for facilitating the large multi-modal model community. https://github.com/
xiaoachen98/Open-LLaVA-NeXT,2024.
12HaoLiu,WilsonYan,MateiZaharia,andPieterAbbeel. Worldmodelonmillion-lengthvideoand
languagewithblockwiseringattention. arXivpreprintarXiv:2402.08268,2024a.
Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee.
Llava-next: Improved reasoning, ocr, and world knowledge, January 2024b. URL https://
llava-vl.github.io/blog/2024-01-30-llava-next/.
HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. Advances
inneuralinformationprocessingsystems,36,2024c.
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan,
JiaqiWang,ConghuiHe,ZiweiLiu,etal. Mmbench: Isyourmulti-modalmodelanall-around
player? arXivpreprintarXiv:2307.06281,2023.
Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios
Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance
hypothesisforllmkvcachecompressionattesttime.AdvancesinNeuralInformationProcessing
Systems,36,2024d.
ZiyuLiu,TaoChu,YuhangZang,XilinWei,XiaoyiDong,PanZhang,ZijianLiang,YuanjunXiong,
Yu Qiao, Dahua Lin, et al. Mmdu: A multi-turn multi-image dialog understanding benchmark
andinstruction-tuningdatasetforlvlms. arXivpreprintarXiv:2406.11833,2024e.
Ziyu Liu, Zeyi Sun, Yuhang Zang, Wei Li, Pan Zhang, Xiaoyi Dong, Yuanjun Xiong, Dahua Lin,
and Jiaqi Wang. Rar: Retrieving and ranking augmented mllms for visual recognition. arXiv
preprintarXiv:2403.13805,2024f.
Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu
Liu, Yan Ma, Xiaoyi Dong, et al. Mmlongbench-doc: Benchmarking long-context document
understandingwithvisualizations. arXivpreprintarXiv:2407.01523,2024.
AhmedMasry, DoXuanLong,JiaQingTan, ShafiqJoty, andEnamulHoque. Chartqa: Abench-
mark for question answering about charts with visual and logical reasoning. arXiv preprint
arXiv:2203.10244,2022.
MineshMathew,DimosthenisKaratzas,andCVJawahar. Docvqa: Adatasetforvqaondocument
images. InProceedingsoftheIEEE/CVFwinterconferenceonapplicationsofcomputervision,
pp.2200–2209,2021.
MineshMathew,VirajBagal,Rube`nTito,DimosthenisKaratzas,ErnestValveny,andCVJawahar.
Infographicvqa.InProceedingsoftheIEEE/CVFWinterConferenceonApplicationsofComputer
Vision,pp.1697–1706,2022.
OpenAI. Gpt-4v(ision)systemcard,2024.
YuxuanQiao,HaodongDuan,XinyuFang,JunmingYang,LinChen,SongyangZhang,JiaqiWang,
DahuaLin,andKaiChen. Prism: Aframeworkfordecouplingandassessingthecapabilitiesof
vlms. arXivpreprintarXiv:2406.14544,2024.
YuzhangShang,MuCai,BingxinXu,YongJaeLee,andYanYan. Llava-prumerge:Adaptivetoken
reductionforefficientlargemultimodalmodels. arXivpreprintarXiv:2403.15388,2024.
Dachuan Shi, Chaofan Tao, Anyi Rao, Zhendong Yang, Chun Yuan, and Jiaqi Wang. Crossget:
Cross-guided ensemble of tokens for accelerating vision-language transformers, 2024. URL
https://arxiv.org/abs/2305.17455.
AmanpreetSingh, VivekNatarajan, MeetShah, YuJiang, XinleiChen, DhruvBatra, DeviParikh,
and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF
conferenceoncomputervisionandpatternrecognition,pp.8317–8326,2019.
Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha
Akula,JihanYang,ShushengYang,AdithyaIyer,XichenPan,etal. Cambrian-1: Afullyopen,
vision-centricexplorationofmultimodalllms. arXivpreprintarXiv:2406.16860,2024.
13Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe´e
Lacroix, Baptiste Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
PengWang, ShuaiBai, SinanTan, ShijieWang, ZhihaoFan, JinzeBai, KeqinChen, XuejingLiu,
JialinWang,WenbinGe,etal. Qwen2-vl: Enhancingvision-languagemodel’sperceptionofthe
worldatanyresolution. arXivpreprintarXiv:2409.12191,2024.
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming
languagemodelswithattentionsinks. arXivpreprintarXiv:2309.17453,2023.
Yizhe Xiong, Hui Chen, Tianxiang Hao, Zijia Lin, Jungong Han, Yuesong Zhang, Guoxin Wang,
Yongjun Bao, and Guiguang Ding. Pyra: Parallel yielding re-activation for training-inference
efficienttaskadaptation,2024. URLhttps://arxiv.org/abs/2403.09192.
RuyiXu,YuanYao,ZonghaoGuo,JunboCui,ZanlinNi,ChunjiangGe,Tat-SengChua,Zhiyuan
Liu, Maosong Sun, and Gao Huang. Llava-uhd: an lmm perceiving any aspect ratio and high-
resolutionimages. arXivpreprintarXiv:2403.11703,2024.
Linli Yao, Lei Li, Shuhuai Ren, Lean Wang, Yuanxin Liu, Xu Sun, and Lu Hou. Deco: Decou-
plingtokencompressionfromsemanticabstractioninmultimodallargelanguagemodels. arXiv
preprintarXiv:2405.20985,2024a.
YuanYao, TianyuYu, AoZhang, ChongyiWang, JunboCui, HongjiZhu, TianchiCai, HaoyuLi,
WeilinZhao, ZhihuiHe, etal. Minicpm-v: Agpt-4vlevelmllmonyourphone. arXivpreprint
arXiv:2408.01800,2024b.
Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and
Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large
languagemodels. arXivpreprintarXiv:2408.04840,2024.
Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang,
andLijuanWang. Mm-vet:Evaluatinglargemultimodalmodelsforintegratedcapabilities. arXiv
preprintarXiv:2308.02490,2023.
BeichenZhang,PanZhang,XiaoyiDong,YuhangZang,andJiaqiWang. Long-clip: Unlockingthe
long-textcapabilityofclip. arXivpreprintarXiv:2403.15378,2024a.
Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual lan-
guage model for video understanding. ArXiv, abs/2306.02858, 2023. URL https://api.
semanticscholar.org/CorpusID:259075356.
PanZhang,XiaoyiDong,YuhangZang,YuhangCao,RuiQian,LinChen,QipengGuo,Haodong
Duan,BinWang,LinkeOuyang,etal.Internlm-xcomposer-2.5:Aversatilelargevisionlanguage
modelsupportinglong-contextualinputandoutput. arXivpreprintarXiv:2407.03320,2024b.
Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song,
YuandongTian,ChristopherRe´,ClarkBarrett,etal. H2o: Heavy-hitteroracleforefficientgen-
erativeinferenceoflargelanguagemodels. AdvancesinNeuralInformationProcessingSystems,
36,2024c.
DeyaoZhu,JunChen,XiaoqianShen,XiangLi,andMohamedElhoseiny. Minigpt-4: Enhancing
vision-language understanding with advanced large language models. ArXiv, abs/2304.10592,
2023a. URLhttps://api.semanticscholar.org/CorpusID:258291930.
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-
hancing vision-language understanding with advanced large language models. arXiv preprint
arXiv:2304.10592,2023b.
14