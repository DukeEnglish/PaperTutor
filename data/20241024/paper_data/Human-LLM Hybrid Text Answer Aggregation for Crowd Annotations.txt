Human-LLM Hybrid Text Answer Aggregation for Crowd Annotations
JiyiLi
UniversityofYamanashi,Kofu,Japan
garfieldpigljy@gmail.com
Abstract Table1: Examplesofaggregationtasksforcrowdanno-
tations. aj representsthecrowdanswertoaninstance
i
Thequalityisacrucialissueforcrowdannota- q byacrowdworkerw . zˆ representstheestimated
i j i
tions. Answeraggregationisanimportanttype answerbyanaggregationmethod. Toaidinunderstand-
ofsolution. Theaggregatedanswersestimated ingthecrowdtextansweraggregationtaskinthisstudy,
from multiple crowd answers to the same in- we also provide an example of the crowd categorical
stancearetheeventuallycollectedannotations, labelaggregationtaskinotherstudies.
ratherthantheindividualcrowdanswersthem-
selves. Recently,thecapabilityofLargeLan- I:CategoricalAnswer(Label)AggregationTask
guageModels(LLMs)ondataannotationtasks q1 a1 1 a2 1 a3 1 a4 1 a5 1 aj 1 zˆ1
leopardjaguarcheetahleopardjaguar ... leopard
hasattractedinterestfromresearchers. Mostof
theexistingstudiesmainlyfocusontheaver- II:TextAnswerAggregationTask(thisstudyfocusesthistask)
a s geg rv ee gep r aae tilr of r no ecr om e nna t cn w ac toe er go k of s ri isn ctu ad ldiv i le aid d bu eta lh sl e ac s nr co dew n Lad Lriw Moo s sr ok ufe sa er gs d; - q は な く 無1 い: 車 必ん (Tか椅 要私 ranに 子 全 が s-a a a a1 1 2 1 3 1
4
1: : : :A wT TF ato h
h
hwar e
e
el rm ehl w e. le ce he hieli
s
act e ih li rcs na .hi on r ao ii nt rs en in s ee o dc tt oe n ts faes olca l ryery s must a no er nyh e ta cf ov o ee sr hsma a ar vw e y eah fte o se a r ul l m cc l. hh ea .i ar wn azˆ te1 he a: ed le l.I lcd ho an ir’ at
aslabelcreators. However,thescenarioofag- lateintoEn-a5 1:Ineverneedwheelchair.
gregationontextanswersandtheroleofLLMs glish) aj:......
1
asaggregatorsarenotyetwell-studied. Inthis
paper, weinvestigatethecapabilityofLLMs
asaggregatorsinthescenarioofclose-ended i.e., whether an instance is preferred to the other
crowd text answer aggregation. We propose instance (Chen et al., 2013; Chen and Joachims,
ahuman-LLMhybridtextansweraggregation
2016a;Jinetal.,2020). Inthispaper,wemainlyfo-
methodwithaCreator-AggregatorMulti-Stage
cusonthetaskswithtextanswers,whichcanbethe
(CAMS)crowdsourcingframework. Wemake
translationsofgivensentences(LiandFukumoto,
theexperimentsbasedonpubliccrowdsourc-
ingdatasets. Theresultsshowtheeffectiveness 2019),free-textrationalesforexplainingNLPmod-
ofourapproachbasedonthecollaborationof els(HartmannandSonntag,2022;Wiegreffeand
crowdworkersandLLMs. Marasovic,2021),andsoon. Wemainlyconsider
the close-ended text answer annotations, i.e., the
1 Introduction
targetanswersaredeterminedtosomeextentand
can be considered as the golden standards, while
Becauseoftheabilityordiligenceoftheworkers,
theaggregationmethodsestimatethesegoldenstan-
thequalityofcrowdsourcedannotationsisacrucial
dards based on multiple crowd answers. Table 1
issue. One solution of quality control is collect-
shows examples of categorical label aggregation
ing redundant annotations by assigning multiple
andtextansweraggregation. Wedenotethesean-
crowdworkerstooneinstanceandaggregatingthe
sweraggregationmethodsasmodelaggregators.
multipleanswerstothisinstanceintoanestimated
answerwhichhashighreliability. Recently, the capability of Large Language
There are many existing works in the crowd- Models (LLMs) on data annotation tasks has at-
sourcing area on answer aggregation for various tracted interest from researchers. Because LLMs
types of annotations. For example, for the classi- are cheaper than crowd workers for annotating
ficationtasks,crowdworkersprovidecategorical the instances (e.g., Gilardi et al. (2023) reported
labelstotheinstances(Snowetal.,2008;Whitehill thatChatGPTisabouttwentytimescheaperthan
etal.,2009;Venanzietal.,2014). Forsomeranking MTurk in their experiments), whether LLMs can
tasks,workersprovidepairwisepreferencelabels, outperformcrowdsourcingisoneoftheissuescon-
4202
tcO
22
]LC.sc[
1v99071.0142:viXracerned. Veselovsky et al. (2023) found that the the true answers. We construct experiments with
crowdworkersonMTurkhavebeenrecentlyusing publiccrowdsourcingdatasets. Thecontributions
LLMstocompletethecrowdsourcingtasks. Some areasfollows.
works verified this issue with the average perfor-
• WeinvestigatethecapabilityofLLMsasag-
manceofindividualcrowdworkersandLLMson
gregatorsinthescenarioofcrowdtextanswer
somespecifictasksbycollectingnewdatasetsfor
aggregationforthequalitycontrolofcrowd-
their target tasks (Zhu et al., 2023; Gilardi et al.,
sourcedclose-endedtextanswerannotations
2023;Törnberg,2023;Ceginetal.,2023;Heetal.,
tasks.
2024a). These LLM studies mainly concentrate
on the average performance of individual crowd • We propose a human-LLM hybrid text an-
workers. However, the aggregated answers are swer aggregation method with a Creator-
the eventually collected annotations, rather than AggregatorMulti-Stage(CAMS)crowdsourc-
thecrowdanswersthemselves. Therefore,thesce- ing framework which is a solution based on
narios involving crowd answer aggregation need thecollaborationofcrowdworkersandLLMs.
furtherstudy.
• We conducted experiments using public
Severalrecentworks(Li,2024;Heetal.,2024b)
crowdsourcingdatasetsforcrowdcreatorsand
studiedthescenarioofansweraggregationonthe
additionaldataforcrowdaggregators. There-
crowdcategoricallabelsintheclassificationtasks
sults show that both crowd aggregators and
andLLMsareusedascreatorsofthelabels. The
LLMaggregatorscanproducehigher-quality
scenario of aggregation on other types of crowd
estimated answers compared to raw crowd
annotations such as text answers and other types
answers from crowd creators. Our CAMS
ofLLMrolessuchasaggregatorsarenotyetwell-
approachwithmodelaggregatorscanfurther
studied. We focus on investigating the capability
improvetheanswerqualitybasedonthecom-
ofLLMsasaggregatorsoncrowdtextanswerag-
binations of three resources of workers and
gregation. LLMscannaturallyutilizemultipletext
answers.
answers as the input, aggregate them, and gener-
atetheestimatedanswer. Inotherwords, wecan
2 RelatedWork
utilizeaLLMasaLLMaggregator.
In crowdsourcing area, because single-stage Therearemanyexistingworksinthecrowdsourc-
frameworksonlycollectallannotationsbycrowd ing area on answer aggregation for various types
workersatonce,whichislimitedfortherequesters ofannotations. Forthecategoricallabels,someap-
to flexibly set diverse additional mechanisms for proachesjointlyestimateworkerabilityandtruean-
improvingthedataquality,multi-stageframeworks swersusingthemaximumentropyprinciple(Zhou
are proposed, e.g., Creation-Review two stages etal.,2012),andBayesianinference(Venanzietal.,
(Baba and Kashima, 2013) for image descrip- 2014). Moresophisticatedmodelsincorporatetask
tionandlogodesign,Find-Fix-Verifythreestages difficulty(Whitehilletal.,2009)andtheirBayesian
(Bernstein et al., 2015) for shortening long texts, treatments(WauthierandJordan,2011;Bachrach
andsoon. Wuetal.(2023)utilizedLLMsinsome etal.,2012). Therearealsoothermethodsbasedon
multi-stagecrowdsourcingframeworks. diversetechniquessuchasautoencoders(Yinetal.,
In this paper, we propose a human-LLM hy- 2017),graphmining(Kawaseetal.,2019),andso
brid text answer aggregation method based on a on(LiandKashima,2017;Lietal.,2018a,2020).
Creator-Aggregator Multi-Stage (CAMS) crowd- Besidesansweraggregationapproaches,Thereis
sourcingframework. Inthefirststage,crowdwork- another type of approach that directly trains the
ers(CrowdCreators)providetherawtextanswers. classificationmodelswiththenoisycrowdlabels
In the next stage, we ask crowd workers (Crowd (Li et al., 2022). In addition, some approaches
Aggregators)andLLMs(LLMAggregators)toag- considerreducingthebudgetwhilepreservingthe
gregatetherawcrowdanswerstoeachinstance. In utilityoftheaggregatedlabels(Li,2019).
the final stage, we utilize the model aggregators Forpairwisepreferencecomparisonlabels (Cat-
onthecombinationsofthreeresourcesofanswers telan,2012),atypicalsolutionistheBradley-Terry
withtheworkers,includingrawcrowdanswers,the modelanditsvariousextensionsorgeneralizations
estimatedanswersbycrowdaggregators,andthe to diverse settings (Causeur and Husson, 2005;
estimatedanswersbyLLMaggregators,toestimate Chen and Joachims, 2016a,b; Chen et al., 2013;Raman and Joachims, 2014; Li et al., 2018b; Jin annotateallinstances.
etal.,2020;Zuoetal.,2020;Li,2022;Zhangetal., Inthesettingofthecrowdansweraggregation,
2022). Therearealsoothertypesofmethods,e.g., ground truths cannot be used by model aggrega-
matrixcompletion(Yietal.,2013;Ohetal.,2015). torsbecausethepurposeofdataannotationtasksis
There are also methods for pairwise similarity collectingandestimatingthegroundtruth. There-
labels(HintonandRoweis,2002;vanderMaaten fore,abstractiveaggregationmethods,suchasab-
and Hinton, 2008; Gomes et al., 2011; Yi et al., stractivetextsummarization,whichrequireground
2012), triplet similarity labels (van der Maaten truthfortraining,arenoteasilyadaptableforuseas
and Weinberger, 2012; Li et al., 2021; Lu et al., modelaggregators. Existingmodelaggregatorsfor
2023), numerical data (Li et al., 2014) and text crowd text answer aggregation (e.g., (Kobayashi,
data(LiandFukumoto,2019;Li,2020). Weshare 2018;LiandFukumoto,2019))aremainlyextrac-
ourcrowdsourcingdatasets,includingvariousdata tive aggregation methods and select the potential
types, which can be utilized for research on top- optimalonefrommultiplecrowdanswers. Given
icssuchasansweraggregationforcrowdannota- the instance set Q, worker set W and answer set
tionsathttps://github.com/garfieldpigljy/ A, a model aggregator estimates the true answer
ljycrowd. In this paper, we mainly consider the Zˆbyselectingoneoptimalanswerzˆ fromA for
i i
close-endedtextanswerannotations. each instanceq , i.e., theanswer aggregation can
i
In crowdsourcing area, because single-stage be defined as Zˆ = Agg(Q,W,A). Note that, in
frameworksonlycollectallannotationsbycrowd this study, only model aggregators are extractive
workersatonce,whichislimitedfortherequesters aggregators. CrowdaggregatorsandLLMaggrega-
to flexibly set diverse additional mechanisms for torsareabstractiveaggregatorsbecausetheyread
improvingthedataquality,multi-stageframeworks the raw crowd answers and generate aggregated
are proposed, e.g., Partition-Map-Reduce (Kittur answersintheirownwords.
etal.,2011)forwritingessays,Price-Divide-Solve
3.2 ModelAggregators
(Kulkarnietal.,2012)forwritingessays,Creation-
Review (Baba and Kashima, 2013) for image de- We utilize several aggregation approaches from
scriptionandlogodesign,Find-Fix-Verify(Bern- existing works as the model aggregators. These
stein et al., 2015) for shortening long texts, Pair- approaches utilize an encoder e(·) to convert the
Compare-Sort(Chengetal.,2015)forsorting,and textintoembeddings.
so on. Wu et al. (2023) utilized LLMs in some SequenceMajorityVoting(SMV): Itisanaïve
multi-stage crowdsourcing frameworks, e.g., the but typical answer aggregation approach that is
Find-Fix-Verifyframework. Furthermore,Wuetal. adapted from majority voting. For each instance,
(2022) compared the similarities and differences itestimatestheembeddingsofthetrueanswersby
betweenLLMchainsandmulti-stagecrowdsourc- eˆ i = mean(e(A i)),andselectstheworkeranswer
ing frameworks. In this paper, we propose a spe- zˆ i = argmax aj sim(e(aj i),eˆ i).
i
cializedcreator-aggregatormulti-stagecrowdsourc- Sequence Maximum Similarity (SMS): For
ingframeworkforcrowdtextansweraggregation eachinstance,SMS(Kobayashi,2018)selectsthe
tasks. workeranswerwhichhasthelargestsumofsimi-
laritywithotheranswerstothisquestion. Itcanbe
3 Preliminary regardedascreatingakerneldensityestimatorand
selectingthemaximumdensityanswer. Thekernel
3.1 ProblemSettingofAnswerAggregation
functionusesthecosinesimilarity. Theformulation
Weformulatethegeneralproblemsettingofcrowd iszˆ = argmax (cid:80) sim(e(aj1),e(aj2)).
i aj1 j1̸=j2 i i
textansweraggregationforqualitycontrolinclose- i
Reliability Aware Sequence Aggregation
ended text answer annotations. We define the in-
(RASA): RASA(LiandFukumoto,2019)models
stance set Q = {q } , worker set W = {w } ,
i i j j theworkerreliabilityθ tostrengthentheinfluences
answer set A = {aj} , where aj represents the
i i,j i ofanswersprovidedbytheworkerswithhigheres-
answer to question q by worker w , true answer
i j timatedreliability. Itfirstestimatestheembeddings
setZ = {z } andestimatedanswersetZˆ = {zˆ} .
i i i i ofthetrueanswersconsideringworkerreliability.
Theanswersetofaninstanceq isA ;theanswer
i i Ititerativelyestimatesθ andeˆ untilconvergence,
i
s ine st to anf ca esw co ar nke br ew laj rgis e,A aj w. oB re kc ea ru isse noth te nen cu em ssb ae ryro tof θ
j
= (cid:80)χ i|2 ( |α e/ (a2,
j
i|A )−j| eˆ) i||2,eˆ
i
= (cid:80) (cid:80)jθj je θ( jaj i) ,whereχ2 isFigure1: ExistingSingle-Stageframeworkforcrowdsourcedtextansweraggregation.
Figure2: OurCreator-AggregatorMulti-Stage(CAMS)frameworkforcrowdsourcedtextansweraggregation.
Table 2: Statistics of the datasets, |·| is the size of crowdtextanswerstoagiveninstance,andgener-
thesets. TheanswersA bycrowdcreatorsW is
C.C. C.C. atetheestimatedtextanswer. LLMscanbeusedas
fromCrowdWSA;theanswersA bycrowdaggre-
C.A. LLMaggregators. Similarly,crowdworkerscanbe
gatorsW arecollectedbythiswork.
C.A. askedtoprovidetheestimatedanswerbyreading
therawcrowdanswersofaninstance,andbeused
Data |Q| |W | |A | |W | |A |
C.C. C.C. C.A. C.A.
as crowd aggregators. We regard the Crowd Cre-
J1 250 70 2,490 106 1250
T1 100 42 1,000 69 500 ator (C.C.), Crowd Aggregator (C.A.), and LLM
T2 100 43 1,000 71 500
Aggregator(L.A.)asthreedifferenttypesofwork-
ers. Todistinguishthesethreeresourcesofworkers
andanswers,wedefinecrowdcreatorworkersand
the chi-squared distribution and the significance
theiranswersasW andA ,crowdaggrega-
C.C. C.C.
level α is set as 0.05 empirically. eˆ can be
i torworkersandtheiranswersasW andA ,
C.A. C.A.
initializedbyusingtheSMVmethod. Afterthat,it
andLLMaggregatorworkersandtheiranswersas
selectsaworkeranswerthatismostsimilartothe
W andA . NotethatcrowdcreatorsW
L.A. L.A. C.C.
embeddingsoftheestimatedtrueanswereˆ.
i andcrowdaggregatorsW areallowedtohave
C.A.
SMV and SMS are instance-wise aggregators
overlaps;theyarenotnecessarytobethesameor
thatutilizetheanswersofaninstanceastheinput.
completelydifferent. Thisisatractablesettingfor
RASAisadataset-wiseaggregatorthatleverages
requesters in practice, because A and A
C.C. C.A.
thepotentialrelationsamongworkersandinstances
arecollectedinseparatestages.
to estimate the answers for all instances. From
this viewpoint, the crowd aggregators and LLM Toeffectivelyleveragethesethreeresourcesof
aggregatorsareinstance-wiseaggregators. workersandanswersinonecrowdsourcingframe-
work, we propose a Creator-Aggregator Multi-
4 Human-LLMHybridAggregation Stage(CAMS)framework. Figure2showsthepro-
posedCAMSframework. Inthefirststage,crowd
Mostoftheexistingworkoncrowdsourcedanno-
creators provide the raw crowd answers. In the
tationisbasedonthesingle-stageframework,i.e.,
nextstage,weaskcrowdaggregatorsand/orLLM
all crowd answers are collected from crowd cre-
aggregators to aggregate the raw crowd answers
ators at once and the model aggregator estimates
intoestimatedanswersintheirownwords. Inthe
the true answers on these raw crowd answers di-
final stage, we utilize the model aggregators on
rectly. Figure1showsanexampleofasingle-stage
thecombinationsofthreeresourcesofworkersand
framework for crowdsourced text answer annota-
answerstogeneratetheestimatedtrueanswers.
tion. Single-stageframeworksarelimitedforthe
requesterstoflexiblysetdiverseadditionalmecha- Therearedifferentversionsofourapproach. We
nismsforimprovingthedataquality. Multi-stage cancombinetwoorthreeresourcesofworkersand
frameworksarecandidatesolutionstoimproveflex- answerstocreatethemergedsets,whicharethen
ibilityandquality. usedasinputforthemodelaggregatortoestimate
Becausethecrowdanswersinthisstudyaretext, thetrueanswers. Weprovideanexamplebasedon
it is intuitive to ask LLMs to read the multiple aversionthatutilizesallthreeresourcesofworkersandanswers. GiventheinstancesetQ,wecancon- speakerswhotranslatedthesameoneJapanese
structtheworkersetW bymergingthreeworker shortsentencefromJapanese. Readeachofthe
sets with W = W +W +W , and the an- Englishshortsentencesbelow,inferthemeaning
C.C. C.A. L.A.
swersetAbymergingthreeanswersetswithA= oftheoriginalJapanesetext,andbasedonyour
A +A +A . Weusethesymbol‘+’rather understanding, create what you consider to be
C.C. C.A. L.A.
thanthesymbol‘∪’forW andA. Becausethean- oneappropriateshortsentenceeachinJapanese
swercreationandansweraggregationaredifferent andEnglish.
tasks,ifthesamecrowdworkerprovidesanswers The original text instructions and task descrip-
asbothcrowdcreatorandcrowdaggregator,he/she tionsusedatLancersareinJapanese. Table2lists
may have different behaviors and reliabilities in thestatisticsoftheanswersA bycrowdaggre-
C.A.
these two roles. We thus treat such a worker as gatorsW collectedbythiswork.
C.A.
different workers when merging the worker and
answer resources. We can then utilize the model 5.2.2 LLMAggregators
aggregatorsontheseQ,W,andAtoestimatethe For LLM aggregators (L.A.), we utilize GPT-42
trueanswers,i.e.,Zˆ = Agg(Q,W,A)whereAgg and Gemini3. L.A.(O) uses GPT-4 and L.A.(G)
isoneofSMV,SMSorRASA.
usesGemini. BecauseGeminiresultsarealmostal-
waysworsethanGPT-4results,weonlyputGPT-4
5 Experiments
resultsinthemainmanuscriptandputGeminire-
5.1 Datasets sultsintheAppendixSection. Weutilizedifferent
temperaturevaluestoconstructmultipleLLMag-
Toinvestigateourstudy,weneeddatasetsthatcon-
gregators. ThemainresultsarebasedonfiveLLM
tainmultiplecrowdtextanswersforeachinstance
aggregatorswithtemperatures[0,0.25,0.5,0.75,1],
aswellasthegoldenanswers. Wethusutilizethe
withonetrialconductedforeachtemperature. Each
realdatasetsJ1,T1,andT2inCrowdWSA(Liand
LLMaggregatorprovidesanswersforallinstances.
Fukumoto,2019)whichinvolvethetranslatedsen-
ThefollowingpromptispresentedtotheLLMs:
tences of the target language by multiple crowd
workers for the sentences of the source language Prompt: The following ten short English sen-
aswellasthegoldentranslations. Table2liststhe tences were written by ten non-native English
statisticsofthethreedatasets. Thecrowdworkers speakerswhotranslatedthesameJapaneseshort
inthesedatasetsareregardedasthecrowdcreators textfromJapanesetoEnglish. Pleasereadeach
(C.C.)inourCAMSframework. Eachcrowdcre- English sentence and infer the meaning of the
atorprovidesanswersforasubsetofallinstances. original Japanese text, and based on your un-
derstanding, provide the appropriate Japanese
5.2 ExperimentalSettings original text and its English translation. Only
5.2.1 CrowdAggregators outputthetextwithoutshowing"Japaneseorigi-
naltext"and"Englishtranslation". \n\n<1. sen-
Forcrowdaggregators(C.A.),weutilizeacommer-
cial crowdsourcing platform Lancers1 to collect tence1>\n<2. sentence2>\n......<10. sentence
10>\n. Outputformatseparatedby"and<tab>:
theestimatedanswers. Eachinstanceisassigned
"Japanese original text. " <tab> "its English
to five crowd aggregators. Each crowd aggrega-
translation. "\n\n
torprovidesestimatedanswersforasubsetofall
instances. Thefollowinginstructionsandtaskde- Regardingthebudgetcostassociatedwithcrowd
scriptionsareshowntothecrowdaggregatorswhen aggregatorsandLLMaggregators,LLMsaregen-
deployingthemicrocrowdsourcingtasks: erallycheaperthancrowdworkersforannotating
Instructions: Thedataiscollectedforuseinaca- instances. For example, Gilardi et al. (2023) re-
demicresearch. Thecollecteddatawillbeused portedthatChatGPTisabouttwentytimescheaper
forresearchonnaturallanguageprocessingand than MTurk in their experiments. In our experi-
machinelearning. Theworkers’nicknameswill ments,LLMaggregatorscostlessthan0.01dollars
notbedisclosedtoanythirdpartyotherthanthe perinstance,whilecrowdaggregatorscostaround
personrequestingthistask. 0.36 dollars per instance, indicating a cost differ-
Descriptions: The ten English short sentences enceofmorethanthirtyfold.
below were written by ten non-native English
2Theversionisgpt-4-0125-preview.
1https://www.lancers.co.jp/ 3Theversionisgemini-1.5-pro-latestinMay9-14th2024.Table3: QualityofindividualanswersbyCrowdCreators(C.C.),CrowdAggregators(C.A.)andLLMAggregators
(L.A.,GPT-4(O))onGLEU,METEOR,andembeddingsimilarity.
(a)GLEU
MIN MEAN MAX STD TIAA
Data C.C. C.A. L.A.(O) C.C. C.A. L.A.(O) C.C. C.A. L.A.(O) C.C. C.A. L.A.(O) C.C. C.A. L.A.(O)
J1 0.07240.0000 0.2706 0.18680.2124 0.2729 0.59481.0000 0.2756 0.09150.1399 0.0018 0.17980.2746 0.7438
T1 0.06690.0217 0.2144 0.17640.1874 0.2184 0.55340.7895 0.2283 0.08180.1297 0.0051 0.25330.3473 0.7809
T2 0.05030.0000 0.2025 0.17160.1817 0.2136 0.45400.6842 0.2244 0.08380.1190 0.0083 0.23770.3371 0.7589
(b)METEOR
MIN MEAN MAX STD TIAA
Data
C.C. C.A. L.A.(O) C.C. C.A. L.A.(O) C.C. C.A. L.A.(O) C.C. C.A. L.A.(O) C.C. C.A. L.A.(O)
J1 0.20410.0490 0.4987 0.37620.4156 0.5049 0.74640.9985 0.5103 0.10370.1569 0.0039 0.37910.4910 0.8683
T1 0.17810.0394 0.4247 0.37710.3909 0.4294 0.71630.9170 0.4392 0.09870.1666 0.0051 0.47630.5670 0.8918
T2 0.15230.0476 0.4102 0.36300.3840 0.4278 0.64730.8288 0.4407 0.10100.1619 0.0103 0.44020.5492 0.8696
(c)EmbeddingSimilarity
MIN MEAN MAX STD TIAA
Data
C.C. C.A. L.A.(O) C.C. C.A. L.A.(O) C.C. C.A. L.A.(O) C.C. C.A. L.A.(O) C.C. C.A. L.A.(O)
J1 0.42330.2243 0.7216 0.66200.6857 0.7313 0.87951.0000 0.7379 0.06870.1135 0.0053 0.65830.7286 0.9322
T1 0.58270.1282 0.7758 0.72600.7336 0.7786 0.90431.0000 0.7824 0.06870.1511 0.0029 0.72520.7850 0.9532
T2 0.47260.2700 0.7602 0.70770.7215 0.7651 0.89581.0000 0.7679 0.08610.1250 0.0031 0.70600.7752 0.9343
5.2.3 ModelAggregators Second, the results in the columns of "MIN",
Forthemodelaggregators,followingCrowdWSA, "MEAN",and"MAX"showthatLLMsoutperform
we use the answer aggregation approaches SMV, badandaveragecrowdworkers,butareworsethan
SMS, and RASA, and utilize universal sentence goodcrowdworkers. Consideringthatmodelag-
encoder4(Ceretal.,2018)asthetextencoder. gregatorscanamplifytheinfluenceofgoodwork-
erswhenestimatingtrueanswers,ahuman-LLM
5.2.4 EvaluationMetrics hybrid aggregation with merged answer sets and
Wealsousethreeevaluationmetricstoverifythe modelaggregatorsinthefinalstagehasthepoten-
proposal. TwoofthemareGLEU5 andMETEOR6. tialtoachievebetterperformancethanaggregation
Wecomputetheaveragescoresbetweenestimated resultsonlybasedoncrowdworkersorLLMs.
answers and true answers. The other one is the
Furthermore,anobservationisthatthestandard
averagecosinesimilaritybetweentheencoderem-
deviations ("STD" column) of LLM aggregators
beddings of the estimated answers and the true
aremuchsmallerthanthoseofcrowdcreatorsand
answers. We utilize the original target sentences
crowd aggregators. In such close-ended text an-
in the Japanese-English parallel corpus used by
swers,multipleLLMaggregatorsconstructedusing
CrowdWSAasthegroundtruth.
thesameLLMwithvarioustemperaturevalues(in-
cludingrelativelyhighvalues)havelowerdiversity
5.3 Q1: QualityofIndividualAnswersby
thancrowdcreatorsandcrowdaggregators. Apo-
CrowdworkersandLLMs
tentialdisadvantageoflowdiversityisiftheanswer
Weinvestigatetheindividualperformanceofcrowd
toaninstanceisnotgood,theLLMmaynotgener-
creators,crowdaggregators,andLLMaggregators
ateagoodanswerbyre-generationorusingdiverse
fromtheviewpointofthequalityofanswersthey
temperaturevalues. Highdiversityintheanswers
provideorgenerate. Table3liststheresults. First,
isnotbadforansweraggregation,becauseitpro-
theresultsinthecolumnsof"Mean"showthatboth
videsabroaderexplorationofcandidateanswers
crowdaggregatorsandLLMaggregatorscangen-
andaggregationmethodsareproposedtoestimate
erate estimated answers with higher quality than
goodworkersandanswersfromrawanswers.
rawcrowdanswersfromcrowdcreators.
Wealso investigate the Inter-Annotator Agree-
4https://tfhub.dev/google/universal-sentence-encoder/4.
ment (IAA) among different types of workers.
This version is newer than the one used in the public
implementation of CrowdWSA, thus the results of model Because Cohen’s Kappa for IAA is a statistical
aggregators on the answers of only crowd creators are
measure used to quantify the level of agreement
differentfromthosereportedinCrowdWSA.
5WeuseNLTKpackage:nltk.translate.gleu_score between two raters who classify items into cate-
6WeuseNLTKpackage:nltk.translate.meteor_score gories,whileourannotationsaretextanswers,weTable 4: Performance of different aggregators, C.A., ble3andaddtheresultsofthemodelaggregators
L.A. (GPT-4 (O)), and Model Aggregator on the raw
to list them in Table 4. The dataset-wise model
crowdanswersA .
C.C. aggregator RASA that models worker reliability
performsbestinmanycases,andperformsbetter
L.A. ModelAggregator
Data C.A. than instance-wise model aggregators SMV and
(O) SMV SMS RASA
(a)GLEU SMS. LLM aggregators sometimes perform best
J1 0.2124 0.2729 0.1930 0.2489 0.2537 becausetheynotonlyselectanswersfromtheraw
T1 0.1874 0.2184 0.1740 0.2310 0.2376
answers but also infer answers through their lan-
T2 0.1817 0.2136 0.1616 0.2189 0.2340
(b)METEOR guageunderstandingandreasoningability.
J1 0.4156 0.5049 0.3861 0.4666 0.4745
T1 0.3909 0.4294 0.3786 0.4548 0.4718 5.5 Q3: ResultsonTextAnswerAggregation
T2 0.3840 0.4278 0.3604 0.4426 0.4653
(c)EmbeddingSimilarity Table 5 shows the results of text answer aggrega-
J1 0.6857 0.7313 0.6732 0.7426 0.7414 tion on the evaluation metric GLEU. A with
L.A.
T1 0.7336 0.7786 0.7245 0.7935 0.8020
(O)usesGPT-4. Tofacilitatethediscussions,the
T2 0.7215 0.7651 0.7105 0.7860 0.7935
resultsareorganizedintofourgroups. GroupIrep-
resentstheexistingworkwithonlycrowdcreators
Table 5: Results of text answer aggregation: GLEU;
and the single-stage framework; Group II is the
LLMaggregator(GPT-4(O)).
performance of our CAMS framework with only
crowdaggregatorsorLLMaggregators;GroupIII
Answers ModelAggregator
AC.C. AC.A. AL.A. SMV SMS RASA and Group IV are answer aggregations with our
J1
I ⃝ 0.1930 0.2489 0.2537 CAMSframeworkwithmorethantwoanswerre-
⃝ 0.2260 0.2740 0.2444
II ⃝(O) 0.2729 0.2770 0.2740 sources;GroupIVincludesthehuman-LLMhybrid
III ⃝ ⃝ 0.2040 0.2873 0.2708 combinations with LLM aggregators. Groups II,
⃝ ⃝(O) 0.2196 0.2846 0.2742
IV ⃝ ⃝(O) 0.2494 0.2962 0.2732 IIIandIVareourproposals. Weaddressthemajor
⃝ ⃝ ⃝(O) 0.2212 0.3003 0.2812
T1 observationsasfollows.
I ⃝ 0.1740 0.2310 0.2376 First, comparing the results among three
⃝ 0.1819 0.2074 0.2007
II
⃝(O) 0.2184 0.2238 0.2170 columns,SMVperformsworstinallthreemodel
III ⃝ ⃝ 0.1767 0.2257 0.2307
⃝ ⃝(O) 0.1888 0.2334 0.2218 aggregators. Itshowsthatutilizingastrongmodel
IV ⃝ ⃝(O) 0.2001 0.2249 0.2218
aggregatorratherthanamajority-basednaïveag-
⃝ ⃝ ⃝(O) 0.1871 0.2329 0.2307
T2 gregatorisimportantforansweraggregation. Al-
I ⃝ 0.1616 0.2189 0.2340
⃝ 0.1769 0.2087 0.1952 thoughtheL.A.-onlyversionofourapproachper-
II
⃝(O) 0.2136 0.2122 0.2095
forms best in all three datasets when using SMV,
III ⃝ ⃝ 0.1667 0.2369 0.2341
⃝ ⃝(O) 0.1789 0.2293 0.2194 theperformanceisalwayslowerthanthoseusing
IV ⃝ ⃝(O) 0.1953 0.2253 0.2252
⃝ ⃝ ⃝(O) 0.1784 0.2309 0.2372 SMS and RASA. The following discussions will
onlyfocusontheresultsbasedonSMSandRASA.
Second,comparingtheSMS(RASA)resultsbe-
defineaText-Inter-AnnotatorAgreement(TIAA)
tweentheL.A.-onlyversioninGroupIIandGroup
for text answers by computing the scores be-
IV, the versions in Group IV that utilize human-
tween two answers to the same instance, κ =
text LLMhybridaggregationoutperformtheversionin
1 (cid:80) 1 (cid:80) G(aj1,aj2). G canbeGLEU,
|Q| i Nj1,j2 j1,j2 i i GroupIIthatonlyutilizestheanswersoftheLLM
METEOR, and Embedding Similarity. Higher aggregatorsinmostofthecases. Inaddition, the
TIAAmeanslowerdiversityintheanswers. The versionsinGroupIValsoalwaysoutperformthe
resultsinthe"TIAA"columnsarebasedonGLEU resultsofLLMaggregatorswithoutmodelaggre-
and also show the lower diversity of LLM aggre- gators. Theyshowthat,althoughLLMaggregators
gatorsthancrowdcreatorsandcrowdaggregators. performbetterthantheaverageofcrowdworkers
Weaddressthisissueandcandidatesolutioninthe inTable3,theanswersprovidedbycrowdworkers
limitationsection. contain valuable information for model aggrega-
tors to generate good answers. One purpose of
5.4 Q2: PerformanceofDifferentAggregators
answer aggregation methods is to estimate good
Wealsocomparedifferenttypesofaggregatorson workers and good answers from the raw crowd
the raw crowd answers. We use the mean results answers(lookingfortheneedleinahaystack). Fol-
ofcrowdaggregatorsandLLMaggregatorsinTa- lowing the observation in Table 3 that there areTable6: Resultsoftextansweraggregation:METEOR; within the CAMS framework, (1) the results of
LLMaggregator(GPT-4(O)).
columnsC.A.orL.A.inTable4representcompo-
nentsthatincludeonlycrowdaggregatorsorLLM
Answers ModelAggregator
aggregators,withouttheuseofmodelaggregators;
AC.C. AC.A. AL.A. SMV SMS RASA
J1 (2)GroupIrepresentscomponentswithonlycrowd
I ⃝ 0.3861 0.4666 0.4745
⃝ 0.4397 0.4912 0.4524 creatorsandthesingle-stageframework. Compar-
II
⃝(O) 0.5049 0.5110 0.5070
ing these results with those of Groups II, III and
III ⃝ ⃝ 0.4040 0.5092 0.4946
⃝ ⃝(O) 0.4258 0.5166 0.5076 Group IV in Tables 5, 6 and 7 illustrates the ef-
IV ⃝ ⃝(O) 0.4723 0.5248 0.5064
⃝ ⃝ ⃝(O) 0.4293 0.5240 0.5147 fectivenessofourhuman-LLMhybridaggregation
T1
approachandCAMSframework.
I ⃝ 0.3786 0.4548 0.4718
⃝ 0.3927 0.4330 0.4272
II Fifth, in the main results, we utilize the same
⃝(O) 0.4294 0.4351 0.4275
III ⃝ ⃝ 0.3833 0.4528 0.4564 number of LLM aggregators with crowd aggre-
⃝ ⃝(O) 0.3955 0.4538 0.4378
IV ⃝ ⃝(O) 0.4110 0.4406 0.4366 gators, we thus also investigate the influences of
⃝ ⃝ ⃝(O) 0.3948 0.4609 0.4495
differentnumbersofLLMaggregatorsontheper-
T2
I ⃝ 0.3604 0.4426 0.4653 formance. Figures3,4and5showtheresultsonthe
⃝ 0.3746 0.4238 0.4083
II ⃝(O) 0.4278 0.4330 0.4287 evaluationmetricsGLEU,METEORandEmbed-
III ⃝ ⃝ 0.3651 0.4507 0.4551
dingSimilaritybyGPT-4LLMaggregatorsbased
⃝ ⃝(O) 0.3829 0.4523 0.4402
IV ⃝ ⃝(O) 0.4012 0.4423 0.4482 onmodelaggregatorsSMSandRASA.Theresults
⃝ ⃝ ⃝(O) 0.3808 0.4606 0.4626
byGeminiLLMaggregatorsareintheAppendix.
Noconsistentpatternswereobservedacrossdiffer-
Table7: Resultsoftextansweraggregation: Embed-
ent datasets; sometimes, more LLM aggregators
dingSimilarity;LLMaggregator(GPT-4(O)).
arebetter;sometimes,fewerLLMaggregatorsare
better;itdependsonthedatasets;wecannotknow
Answers ModelAggregator
AC.C. AC.A. AL.A. SMV SMS RASA whichcaseitisinpracticalcrowdannotations. In
J1
I ⃝ 0.6732 0.7426 0.7414 allcasesoftheJ1datasetandsomecasesoftheT1
⃝ 0.6983 0.7360 0.7188
II andT2datasets,theresultsofonlyusingoneLLM
⃝(O) 0.7313 0.7332 0.7317
III ⃝ ⃝ 0.6815 0.7516 0.7459 aggregatorarebad;usingmultipleLLMaggrega-
⃝ ⃝(O) 0.6926 0.7427 0.7356
IV ⃝ ⃝(O) 0.7148 0.7413 0.7336 torsisrecommendedtoavoidrisk.
⃝ ⃝ ⃝(O) 0.6940 0.7484 0.7410
T1 Finally, the approach we propose in this work
I ⃝ 0.7245 0.7935 0.8020
has several versions (six combinations of worker
⃝ 0.7430 0.7851 0.7758
II
⃝(O) 0.7786 0.7807 0.7762 and answer resources in Groups II, III, and IV).
III ⃝ ⃝ 0.7306 0.7965 0.7976
⃝ ⃝(O) 0.7425 0.7894 0.7793 The best version depends on the specific applica-
IV ⃝ ⃝(O) 0.7608 0.7854 0.7807
tion. If there is no prior knowledge of the target
⃝ ⃝ ⃝(O) 0.7426 0.7993 0.7890
T2 taskordataset, werecommendusingallworkers
I ⃝ 0.7105 0.7860 0.7935
⃝ 0.7282 0.7681 0.7468 andanswers(i.e.,crowdcreators,crowdaggrega-
II
⃝(O) 0.7651 0.7725 0.7673
tors,andLLMaggregators)togetherwithareliable
III ⃝ ⃝ 0.7164 0.7885 0.7822
⃝ ⃝(O) 0.7287 0.7856 0.7744 modelaggregator(e.g.,SMSorRASA).
IV ⃝ ⃝(O) 0.7467 0.7712 0.7728
⃝ ⃝ ⃝(O) 0.7286 0.7851 0.7824
6 Conclusion
stillgoodcrowdworkers(creatorsandaggregators) In this paper, we investigate the capability of
who perform better than LLMs (aggregators), a LLMs as aggregators in the scenario of crowd
strongmodelaggregatorcantargetsuchgoodwork- textansweraggregationforthequalitycontrolof
ers and answers. In contrast to replacing crowd crowdsourcedclose-endedtextanswerannotation
workersbyonlyusingLLMs,human-LLMhybrid tasks. We propose a human-LLM hybrid text
solutionspotentiallyhaveadvantages. answer aggregation method with a Creator-
Third,Table6and7listtheresultsontheevalu- Aggregator Multi-Stage (CAMS) crowdsourcing
ationmetricsMETEORandEmbeddingSimilarity. framework. The experimental results show both
Theyreachsimilarobservationswiththeevaluation crowd aggregators and LLM aggregators can
metricGLEU. generate estimated answers with higher quality
Fourth, regarding the performance of different than raw crowd answers provided by crowd
components (ablations) of the proposed method creators. Our CAMS approach with model0.30 0.24
0.24
0.28 0.22 0.22
0.26 L.A. (SMS) 0.20 L.A. (SMS) 0.20 L.A. (SMS)
L.A. (RASA) L.A. (RASA) L.A. (RASA)
C.C. + L.A. (SMS) C.C. + L.A. (SMS) C.C. + L.A. (SMS)
0.24 C.C. + L.A. (RASA) 0.18 C.C. + L.A. (RASA) 0.18 C.C. + L.A. (RASA)
C.A. + L.A. (SMS) C.A. + L.A. (SMS) C.A. + L.A. (SMS)
C.A. + L.A. (RASA) C.A. + L.A. (RASA) 0.16 C.A. + L.A. (RASA)
0.22 C.C. + C.A. + L.A. (SMS) 0.16 C.C. + C.A. + L.A. (SMS) C.C. + C.A. + L.A. (SMS)
C.C. + C.A. + L.A. (RASA) C.C. + C.A. + L.A. (RASA) C.C. + C.A. + L.A. (RASA)
0.14
1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9
# of LLM Aggregator # of LLM Aggregator # of LLM Aggregator
(a)J1 (b)T1 (c)T2
Figure3: GLEUResultsbydifferentnumberofL.A.(GPT-4(O))basedonSMSandRASA.
0.52 0.46 0.46
0.51 0.44 0.44
0.50 L.A. (SMS) 0.42 L.A. (SMS) 0.42 L.A. (SMS)
0.49 L C. .A C. . ( +R A LS .AA .) (SMS) 0.40 L C. .A C. . ( +R A LS .AA .) (SMS) 0.40 L C. .A C. . ( +R A LS .AA .) (SMS)
C.C. + L.A. (RASA) C.C. + L.A. (RASA) C.C. + L.A. (RASA)
0.48 C.A. + L.A. (SMS) 0.38 C.A. + L.A. (SMS) 0.38 C.A. + L.A. (SMS)
C.A. + L.A. (RASA) C.A. + L.A. (RASA) C.A. + L.A. (RASA)
0.47 C.C. + C.A. + L.A. (SMS) C.C. + C.A. + L.A. (SMS) C.C. + C.A. + L.A. (SMS)
C.C. + C.A. + L.A. (RASA) 0.36 C.C. + C.A. + L.A. (RASA) 0.36 C.C. + C.A. + L.A. (RASA)
0.46
1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9
# of LLM Aggregator # of LLM Aggregator # of LLM Aggregator
(a)J1 (b)T1 (c)T2
Figure4: METEORResultsbydifferentnumberofL.A.(GPT-4(O))basedonSMSandRASA.
0.75 0.80 0.80
0.74 0.79
0.73 0.78 0.78
0.72 0.77
L.A. (SMS) L.A. (SMS) 0.76 L.A. (SMS) 0.71 L.A. (RASA) 0.76 L.A. (RASA) L.A. (RASA)
C.C. + L.A. (SMS) C.C. + L.A. (SMS) C.C. + L.A. (SMS)
0.70 C.C. + L.A. (RASA) 0.75 C.C. + L.A. (RASA) 0.74 C.C. + L.A. (RASA)
0.69 C C. .A A. . + + L L. .A A. . ( (S RM ASS A) ) 0.74 C C. .A A. . + + L L. .A A. . ( (S RM ASS A) ) C C. .A A. . + + L L. .A A. . ( (S RM ASS A) )
0.68 C.C. + C.A. + L.A. (SMS) 0.73 C.C. + C.A. + L.A. (SMS) 0.72 C.C. + C.A. + L.A. (SMS)
C.C. + C.A. + L.A. (RASA) C.C. + C.A. + L.A. (RASA) C.C. + C.A. + L.A. (RASA)
0.67 0.72
1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9
# of LLM Aggregator # of LLM Aggregator # of LLM Aggregator
(a)J1 (b)T1 (c)T2
Figure5: EmbeddingSimilarityResultsbydifferentnumberofL.A.(GPT-4(O))basedonSMSandRASA.
aggregatorscanfurtherimprovetheanswerquality terestingissueforinvestigation. Inthefuturework,
based on the combinations of three resources wewillverifyotherLLMs.
of workers and answers. This work presents a Forthepurposeofinvestigatingtheresultswith
solution based on human-AI collaboration for a one LLM, people usually select a single LLM
specifictask. HumanscanleverageAItoenhance providertocompletethetasks. Intheexperiments,
the quality and efficiency of tasks like data weuseddifferentLLMs(GPT-4orGemini)sepa-
annotation. Wesharethedatasetsandsourcecodes rately. DifferentLLMaggregatorsareconstructed
at https://github.com/garfieldpigljy/ by using different temperature parameters. As
HumanLLMHybridAggregation. shown in the experimental results in Section 5.3,
theanswersgeneratedbymultipleLLMaggrega-
Limitations torsconstructedbythesameLLMhaverelatively
lowdiversity. Infuturework,acandidatesolution
Inthiswork,weonlyverifytwopopularcommer-
isusingdifferentLLMsanddifferenttemperatures
cial LLMs, i.e., GPT-4 and Gemini-1.5-Pro. We
toconstructmanyLLMaggregators.
thinkthatGPT-4andGemini-1.5-Procanrepresent
theperformancethatLLMscancurrentlyachieve,
Acknowledgements
andtheyareenoughfortheempiricalexperiments.
Ontheotherhand,theperformanceofotherLLMs This work was supported by JSPS KAKENHI
(e.g.,someopen-sourceLLMmodels)isalsoanin- GrantNumberJP23K28092andJP23K11227.
UELG
ROETEM
ytiralimiS
gniddebmE
UELG
ROETEM
ytiralimiS
gniddebmE
UELG
ROETEM
ytiralimiS
gniddebmEEthicStatement ACMInternationalConferenceonWebSearchand
DataMining(WSDM),pages193–202.
Nopersonalinformationisusedinthisstudy,and
thereforenodemographicorgeographiccharacter- Justin Cheng, Jaime Teevan, Shamsi T. Iqbal, and
MichaelS.Bernstein.2015. Breakitdown: Acom-
isticsoftheannotatorsareincludedorutilizedin
parisonofmacro-andmicrotasks. InProceedingsof
the data. The crowd workers have obtained suffi- the33rdAnnualACMConferenceonHumanFactors
cientwagesbylocalstandards. inComputingSystems(CHI),pages4061–4064.
Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli.
2023. Chatgpt outperforms crowd workers for
References
text-annotationtasks. ProceedingsoftheNational
Yukino Baba and Hisashi Kashima. 2013. Statistical AcademyofSciences,120(30):e2305016120.
qualityestimationforgeneralcrowdsourcingtasks.
RyanG.Gomes,PeterWelinder,AndreasKrause,and
InProceedingsofthe19thACMSIGKDDInterna-
PietroPerona.2011. Crowdclustering. InAdvances
tionalConferenceonKnowledgeDiscoveryandData
inNeuralInformationProcessingSystems24(NIPS),
Mining(KDD),pages554–562.
pages558–566.
YoramBachrach,TomMinka,JohnGuiver,andThore
MareikeHartmannandDanielSonntag.2022. Asurvey
Graepel.2012. Howtogradeatestwithoutknowing
onimprovingNLPmodelswithhumanexplanations.
theanswers:Abayesiangraphicalmodelforadaptive
In Proceedings of the First Workshop on Learning
crowdsourcingandaptitudetesting. InProceedings
withNaturalLanguageSupervision,pages40–47.
ofthe29thInternationalCoferenceonInternational
ConferenceonMachineLearning(ICML),pages819–
Xingwei He, Zhenghao Lin, Yeyun Gong, and et al.
826.
2024a. AnnoLLM:Makinglargelanguagemodelsto
bebettercrowdsourcedannotators. InProceedings
Michael S. Bernstein, Greg Little, Robert C. Miller,
ofthe2024ConferenceoftheNorthAmericanChap-
Björn Hartmann, Mark S. Ackerman, David R.
teroftheAssociationforComputationalLinguistics:
Karger,DavidCrowell,andKatrinaPanovich.2015.
HumanLanguageTechnologies(Volume6: Industry
Soylent: awordprocessorwithacrowdinside. Com-
mun.ACM,58(8):85–94.
Track)(NAACL),pages165–190.
ManuelaCattelan.2012. Modelsforpairedcomparison ZeyuHe, Chieh-YangHuang, Chien-KuangCornelia
data: A review with emphasis on dependent data. Ding, Shaurya Rohatgi, and Ting-Hao Kenneth
StatisticalScience,pages412–433. Huang.2024b. Ifinacrowdsourceddataannotation
pipeline, a gpt-4. In Proceedings of the CHI Con-
David Causeur and François Husson. 2005. A 2- ference on Human Factors in Computing Systems
dimensional extension of the bradley–terry model (CHI).
forpairedcomparisons. JournalofStatisticalPlan-
ningandInference,135(2):245–259. GeoffreyEHintonandSamT.Roweis.2002. Stochastic
neighborembedding. InAdvancesinNeuralInforma-
JanCegin,JakubSimko,andPeterBrusilovsky.2023. tionProcessingSystems15(NIPS),pages857–864.
ChatGPT to replace crowdsourcing of paraphrases
forintentclassification: Higherdiversityandcompa- Tao Jin, Pan Xu, Quanquan Gu, and Farzad Farnoud.
rablemodelrobustness. InProceedingsofthe2023 2020. Rankaggregationviaheterogeneousthurstone
Conference on Empirical Methods in Natural Lan- preferencemodels. InProceedingsoftheAAAICon-
guageProcessing(EMNLP),pages1889–1905. ferenceonArtificialIntelligence(AAAI),volume34,
pages4353–4360.
DanielCer,YinfeiYang,Sheng-yiKong,andetal.2018.
Universalsentenceencoder. CoRR,abs/1803.11175. YasushiKawase,YukoKuroki,andAtsushiMiyauchi.
2019. Graphminingmeetscrowdsourcing: Extract-
ShuoChenandThorstenJoachims.2016a. Modeling ingexpertsforansweraggregation. InProceedings
intransitivity in matchup and comparison data. In oftheTwenty-EighthInternationalJointConference
Proceedings of the Ninth ACM International Con- onArtificialIntelligence(IJCAI),pages1272–1279.
ferenceonWebSearchandDataMining(WSDM),
pages227–236. Aniket Kittur, Boris Smus, Susheel Khamkar, and
RobertE.Kraut.2011. Crowdforge: crowdsourcing
ShuoChenandThorstenJoachims.2016b. Predicting complexwork. InProceedingsofthe24thAnnual
matchupsandpreferencesincontext. InProceedings ACM Symposium on User Interface Software and
ofthe22ndACMSIGKDDInternationalConference Technology(UIST),pages43–52.
onKnowledgeDiscoveryandDataMining(KDD),
pages775–784. HayatoKobayashi.2018. Frustratinglyeasymodelen-
sembleforabstractivesummarization. InProceed-
XiChen, PaulN.Bennett, KevynCollins-Thompson, ingsofthe2018ConferenceonEmpiricalMethods
andEricHorvitz.2013. Pairwiserankingaggregation in Natural Language Processing (EMNLP), pages
inacrowdsourcedsetting. InProceedingsofthe6th 4165–4176.Anand Kulkarni, Matthew Can, and Björn Hartmann. Jiyi Li, Yasushi Kawase, Yukino Baba, and Hisashi
2012. Collaboratively crowdsourcing workflows Kashima. 2020. Performance as a constraint: An
withturkomatic. InProceedingsoftheACM2012 improvedwisdomofcrowdsusingperformanceregu-
Conference on Computer Supported Cooperative larization. InProceedingsoftheTwenty-NinthInter-
Work(CSCW),pages1003–1012. nationalJointConferenceonArtificialIntelligence
(IJCAI),pages1534–1541.
JingzhengLi,HailongSun,andJiyiLi.2022. Beyond
Qi Li, Yaliang Li, Jing Gao, Lu Su, Bo Zhao, Mu-
confusionmatrix: Learningfrommultipleannotators
withawarenessofinstancefeatures. Mach.Learn., rat Demirbas, Wei Fan, and Jiawei Han. 2014. A
confidence-aware approach for truth discovery on
112(3):1053–1075.
long-taildata. Proc.VLDBEndow.,8(4):425–436.
JiyiLi.2019. Budgetcostreductionforlabelcollection
Xiaotian Lu, Jiyi Li, Koh Takeuchi, and Hisashi
with confusability based exploration. In Proceed-
Kashima.2023. Multiviewrepresentationlearning
ingsofthe26thInternationalConferenceonNeural
from crowdsourced triplet comparisons. In Pro-
InformationProcessing(ICONIP),pages231–241.
ceedingsoftheACMWebConference2023(WWW),
pages3827–3836.
Jiyi Li. 2020. Crowdsourced text sequence aggrega-
tion based on hybrid reliability and representation. SewoongOh,KiranKThekumparampil,andJiaming
In Proceedings of the 43rd International ACM SI- Xu.2015. Collaborativelylearningpreferencesfrom
GIR Conference on Research and Development in ordinal data. In Advances in Neural Information
InformationRetrieval(SIGIR),pages1761–1764. ProcessingSystems28(NIPS),pages1909–1917.
JiyiLi.2022. Context-basedcollectivepreferenceag- KarthikRamanandThorstenJoachims.2014. Methods
gregation for prioritizing crowd opinions in social forordinalpeergrading. InProceedingsofthe20th
decision-making. InProceedingsoftheACMWeb ACMSIGKDDInternationalConferenceonKnowl-
Conference2022(WWW),pages2657–2667. edgeDiscoveryandDataMining(KDD),pages1037–
1046.
JiyiLi.2024. Acomparativestudyonannotationquality
R.Snow,B.O’Connor,D.Jurafsky,andA.Y.Ng.2008.
ofcrowdsourcingandllmvialabelaggregation. In
Cheap and fast—but is it good?: Evaluating non-
2024 IEEE International Conference on Acoustics,
expert annotations for natural language tasks. In
SpeechandSignalProcessing(ICASSP),pages6525–
ProceedingsoftheConferenceonEmpiricalMethods
6529.
in Natural Language Processing (EMNLP), pages
254–263.
JiyiLi,YukinoBaba,andHisashiKashima.2018a. In-
corporatingworkersimilarityforlabelaggregation
Petter Törnberg. 2023. Chatgpt-4 outperforms ex-
incrowdsourcing. InProceedingsofthe27thInter-
perts and crowd workers in annotating political
nationalConferenceonArtificialNeuralNetworks
twitter messages with zero-shot learning. CoRR,
(ICANN),pages596–606.
abs/2304.06588.
Jiyi Li, Yukino Baba, and Hisashi Kashima. 2018b. Laurens van der Maaten and Geoffrey Hinton. 2008.
Simultaneousclusteringandrankingfrompairwise Visualizingdatausingt-SNE. JournalofMachine
comparisons. InProceedingsoftheTwenty-Seventh LearningResearch,9(Nov):2579–2605.
International Joint Conference on Artificial Intelli-
gence(IJCAI),pages1554–1560. LaurensvanderMaatenandKilianWeinberger.2012.
Stochastic triplet embedding. In 2012 IEEE Inter-
nationalWorkshoponMachineLearningforSignal
JiyiLi,LucasRyoEndo,andHisashiKashima.2021.
Labelaggregationforcrowdsourcedtripletsimilarity
Processing(MLSP),pages1–6.
comparisons. In Proceedings of the 28th Interna-
Matteo Venanzi, John Guiver, Gabriella Kazai, Push-
tionalConferenceonNeuralInformationProcessing
meetKohli,andMiladShokouhi.2014. Community-
(ICONIP),pages176–185.
basedbayesianaggregationmodelsforcrowdsourc-
ing. InProceedingsofthe23rdInternationalConfer-
Jiyi Li and Fumiyo Fukumoto. 2019. A dataset of
enceonWorldWideWeb(WWW),pages155–164.
crowdsourcedwordsequences: Collectionsandan-
sweraggregationforgroundtruthcreation. InPro-
Veniamin Veselovsky, Manoel Horta Ribeiro, and
ceedingsoftheFirstWorkshoponAggregatingand
RobertWest.2023. Artificialartificialartificialin-
AnalysingCrowdsourcedAnnotationsforNLP(An-
telligence: Crowd workers widely use large lan-
noNLP2019),pages24–28.
guage models for text production tasks. CoRR,
abs/2306.07899.
JiyiLiandHisashiKashima.2017. Iterativereduction
workerfilteringforcrowdsourcedlabelaggregation. FabianLWauthierandMichaelJordan.2011. Bayesian
InProceedingsofthe18thInternationalConference biasmitigationforcrowdsourcing. InAdvancesin
on Web Information Systems Engineering (WISE), NeuralInformationProcessingSystems(NIPS),vol-
pages46–54. ume24.Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier Movellan. 2009. Whose vote
should count more: Optimal integration of labels
from labelers of unknown expertise. In Proceed-
ingsofthe22ndInternationalConferenceonNeural
InformationProcessingSystems(NIPS),pages2035–
2043.
SarahWiegreffeandAnaMarasovic.2021. Teachmeto
explain: Areviewofdatasetsforexplainablenatural
languageprocessing. InProceedingsoftheNeural
InformationProcessingSystemsTrackonDatasets
andBenchmarks,volume1.
Tongshuang Wu, Michael Terry, and Carrie Jun Cai.
2022. Ai chains: Transparent and controllable
human-ai interaction by chaining large language
model prompts. In Proceedings of the 2022 CHI
ConferenceonHumanFactorsinComputingSystems
(CHI).
TongshuangWu,HaiyiZhu,MayaAlbayrak,andetal.
2023. Llmsasworkersinhuman-computationalal-
gorithms? replicatingcrowdsourcingpipelineswith
llms. CoRR,abs/2307.10168.
JinfengYi,RongJin,ShailiJain,andAnilJain.2013.
Inferringusers’preferencesfromcrowdsourcedpair-
wise comparisons: A matrix completion approach.
InProceedingofthe1stAAAIConferenceonHuman
ComputationandCrowdsourcing(HCOMP).
Jinfeng Yi, Rong Jin, Shaili Jain, Tianbao Yang, and
AnilK.Jain.2012. Semi-crowdsourcedclustering:
Generalizingcrowdlabelingbyrobustdistancemet-
riclearning. InAdvancesinNeuralInformationPro-
cessingSystems25(NIPS),pages1772–1780.
Li’angYin,JianhuaHan,WeinanZhang,andYongYu.
2017. Aggregatingcrowdwisdomswithlabel-aware
autoencoders. In Proceedings of the Twenty-Sixth
International Joint Conference on Artificial Intelli-
gence(IJCAI),pages1325–1331.
GuoxiZhang,JiyiLi,andHisashiKashima.2022. Im-
provingpairwiserankaggregationviaqueryingfor
rankdifference. In2022IEEE9thInternationalCon-
ference on Data Science and Advanced Analytics
(DSAA),pages1–9.
DengyongZhou,JohnC.Platt,SumitBasu,andYiMao.
2012. Learningfromthewisdomofcrowdsbymin-
imax entropy. In Proceedings of the 25th Interna-
tionalConferenceonNeuralInformationProcessing
Systems(NIPS),pages2195–2203.
YimingZhu,PeixianZhang,EhsanulHaq,PanHui,and
GarethTyson.2023. Canchatgptreproducehuman-
generatedlabels? Astudyofsocialcomputingtasks.
CoRR,abs/2304.10145.
Xingkun Zuo, Jiyi Li, Qili Zhou, Jianjun Li, and Xi-
aoyang Mao. 2020. Affecti: A game for diverse,
reliable,andefficientaffectiveimageannotation. In
Proceedingsofthe28thACMInternationalConfer-
enceonMultimedia(MM),pages529–537.Table 8: Performance of different aggregators, C.A., Table 10: Results of text answer aggregation: ME-
L.A.(Gemini(G)),andModelAggregatorontheraw TEOR;LLMaggregator(Gemini(G)).
crowdanswersA .
C.C.
Answers ModelAggregator
L.A. ModelAggregator
AC.C. AC.A. AL.A. SMV SMS RASA
Data C.A. J1
(G) SMV SMS RASA I ⃝ 0.3861 0.4666 0.4745
(a)GLEU ⃝ 0.4397 0.4912 0.4524
II
J1 0.2124 0.2633 0.1930 0.2489 0.2537 ⃝(G) 0.4954 0.4925 0.4885
III ⃝ ⃝ 0.4040 0.5092 0.4946
T1 0.1874 0.2015 0.1740 0.2310 0.2376
⃝ ⃝(G) 0.4226 0.5142 0.4911
T2 0.1817 0.1751 0.1616 0.2189 0.2340 IV ⃝ ⃝(G) 0.4675 0.5084 0.4878
(b)METEOR ⃝ ⃝ ⃝(G) 0.4269 0.5204 0.5037
J1 0.4156 0.4926 0.3861 0.4666 0.4745 T1
I ⃝ 0.3786 0.4548 0.4718
T1 0.3909 0.4218 0.3786 0.4548 0.4718
⃝ 0.3927 0.4330 0.4272
T2 0.3840 0.3833 0.3604 0.4426 0.4653 II ⃝(G) 0.4258 0.4364 0.4392
(c)EmbeddingSimilarity III ⃝ ⃝ 0.3833 0.4528 0.4564
J1 0.6857 0.7310 0.6732 0.7426 0.7414 ⃝ ⃝(G) 0.3943 0.4630 0.4447
IV ⃝ ⃝(G) 0.4092 0.4395 0.4379
T1 0.7336 0.7635 0.7245 0.7935 0.8020
⃝ ⃝ ⃝(G) 0.3939 0.4584 0.4514
T2 0.7215 0.7540 0.7105 0.7860 0.7935 T2
I ⃝ 0.3604 0.4426 0.4653
⃝ 0.3746 0.4238 0.4083
II
Table 9: Results of text answer aggregation: GLEU; ⃝(G) 0.3956 0.3999 0.3754
III ⃝ ⃝ 0.3651 0.4507 0.4551
LLMaggregator(Gemini(G)). ⃝ ⃝(G) 0.3721 0.4426 0.4179
IV ⃝ ⃝(G) 0.3851 0.4200 0.4324
⃝ ⃝ ⃝(G) 0.3727 0.4384 0.4469
Answers ModelAggregator
AC.C. AC.A. AL.A. SMV SMS RASA
J1 Table11: Resultsoftextansweraggregation: Embed-
I ⃝ 0.1930 0.2489 0.2537
⃝ 0.2260 0.2740 0.2444 dingSimilarity;LLMaggregator(Gemini(G)).
II
⃝(G) 0.2661 0.2623 0.2626
III ⃝ ⃝ 0.2040 0.2873 0.2708
⃝ ⃝(G) 0.2174 0.2856 0.2620 Answers ModelAggregator
IV ⃝ ⃝(G) 0.2460 0.2746 0.2564 AC.C. AC.A. AL.A. SMV SMS RASA
⃝ ⃝ ⃝(G) 0.2195 0.2920 0.2723 J1
T1 I ⃝ 0.6732 0.7426 0.7414
I ⃝ 0.1740 0.2310 0.2376 ⃝ 0.6983 0.7360 0.7188
II
⃝ 0.1819 0.2074 0.2007 ⃝(G) 0.7310 0.7313 0.7311
II
⃝(G) 0.2082 0.2136 0.2126 III ⃝ ⃝ 0.6815 0.7516 0.7459
III ⃝ ⃝ 0.1767 0.2257 0.2307 ⃝ ⃝(G) 0.6925 0.7463 0.7311
⃝ ⃝(G) 0.1854 0.2337 0.2138 IV ⃝ ⃝(G) 0.7147 0.7391 0.7315
IV ⃝ ⃝(G) 0.1950 0.2204 0.2169 ⃝ ⃝ ⃝(G) 0.6939 0.7521 0.7423
⃝ ⃝ ⃝(G) 0.1845 0.2274 0.2274 T1
T2 I ⃝ 0.7245 0.7935 0.8020
I ⃝ 0.1616 0.2189 0.2340 ⃝ 0.7430 0.7851 0.7758
II
⃝ 0.1769 0.2087 0.1952 ⃝(G) 0.7635 0.7698 0.7701
II
⃝(G) 0.1837 0.1853 0.1707 III ⃝ ⃝ 0.7306 0.7965 0.7976
III ⃝ ⃝ 0.1667 0.2369 0.2341 ⃝ ⃝(G) 0.7375 0.7978 0.7762
⃝ ⃝(G) 0.1690 0.2174 0.2032 IV ⃝ ⃝(G) 0.7532 0.7803 0.7803
IV ⃝ ⃝(G) 0.1803 0.2061 0.2191 ⃝ ⃝ ⃝(G) 0.7389 0.7963 0.7953
⃝ ⃝ ⃝(G) 0.1710 0.2202 0.2277 T2
I ⃝ 0.7105 0.7860 0.7935
⃝ 0.7282 0.7681 0.7468
II
⃝(G) 0.7540 0.7579 0.7468
A Appendix III ⃝ ⃝ 0.7164 0.7885 0.7822
⃝ ⃝(G) 0.7250 0.7856 0.7744
IV ⃝ ⃝(G) 0.7411 0.7679 0.7724
B AdditionalResultsByGemini ⃝ ⃝ ⃝(G) 0.7258 0.7833 0.7831
Because the Gemini results are almost always
worsethanGPT-4results,weonlyputtheGPT-4 Table9showstheresultsontextansweraggre-
resultsinthemainmanuscriptandputtheGemini gationontheevaluationmetricGLEU.A with
L.A.
resultsintheAppendixSection. (G)usesGemini. Table10and11listtheresultson
Table12liststheresultsoftheindividualperfor- theevaluationmetricsMETEORandEmbedding
manceofcrowdcreators,crowdaggregators,and Similarity. They reach similar observations with
Gemini LLM aggregators from the viewpoint of theevaluationmetricGLEU.
thequalityofanswerstheyprovideorgenerate. Figure 6, 7 and 8 shows the results on GLEU,
We also make a comparison among different METEORandEmbeddingSimilaritybydifferent
types of aggregators on the raw crowd answers. numbers of Gemini LLM aggregators based on
Weusethemeanresultsofcrowdaggregatorsand modelaggregatorsSMSandRASA.Temperature
LLM aggregators in Table 12 and add the results of1LLMaggregator: [0];temperaturesof3LLM
ofthemodelaggregatortolisttheminTable8. aggregators: [0, 0.5, 1]; temperatures of 5 LLMTable12: QualityofindividualanswersbyCrowdCreators(C.C.),CrowdAggregators(C.A.)andLLMAggregators
(L.A.,Gemini(G))onGLEU,METEORandembeddingsimilarity.
(a)GLEU
MIN MEAN MAX STD TIAA
Data C.C. C.A. L.A.(G) C.C. C.A. L.A.(G) C.C. C.A. L.A.(G) C.C. C.A. L.A.(G) C.C. C.A. L.A.(G)
J1 0.07240.0000 0.2588 0.18680.2124 0.2633 0.59481.0000 0.2714 0.09150.1399 0.0057 0.17980.2746 0.7749
T1 0.06690.0217 0.1968 0.17640.1874 0.2015 0.55340.7895 0.2064 0.08180.1297 0.0039 0.25330.3473 0.8028
T2 0.05030.0000 0.1707 0.17160.1817 0.1751 0.45400.6842 0.1810 0.08380.1190 0.0043 0.23770.3371 0.7854
(b)METEOR
MIN MEAN MAX STD TIAA
Data
C.C. C.A. L.A.(G) C.C. C.A. L.A.(G) C.C. C.A. L.A.(G) C.C. C.A. L.A.(G) C.C. C.A. L.A.(G)
J1 0.20410.0490 0.4869 0.37620.4156 0.4926 0.74640.9985 0.4993 0.10370.1569 0.0051 0.37910.4910 0.8792
T1 0.17810.0394 0.4165 0.37710.3909 0.4218 0.71630.9170 0.4310 0.09870.1666 0.0065 0.47630.5670 0.8830
T2 0.15230.0476 0.3754 0.36300.3840 0.3833 0.64730.8288 0.3941 0.10100.1619 0.0078 0.44020.5492 0.8728
(c)EmbeddingSimilarity
MIN MEAN MAX STD TIAA
Data
C.C. C.A. L.A.(G) C.C. C.A. L.A.(G) C.C. C.A. L.A.(G) C.C. C.A. L.A.(G) C.C. C.A. L.A.(G)
J1 0.42330.2243 0.7265 0.66200.6857 0.7310 0.87951.0000 0.7373 0.06870.1135 0.0036 0.65830.7286 0.9373
T1 0.58270.1282 0.7566 0.72600.7336 0.7635 0.90431.0000 0.7719 0.06870.1511 0.0058 0.72520.7850 0.9502
T2 0.47260.2700 0.7458 0.70770.7215 0.7540 0.89581.0000 0.7684 0.08610.1250 0.0086 0.70600.7752 0.9360
0.28 0.23 0.225
0.22 0.200
0.26 0.21 0.175
0.24 L L C. . .A A C. .
.
( ( +S R M A LS .S AA) .)
(SMS)
00 .. 12 90 L L C. . .A A C. .
.
( ( +S R M A LS .S AA) .)
(SMS)
00 .. 11 25 50 L L C. . .A A C. .
.
( ( +S R M A LS .S AA) .)
(SMS)
C.C. + L.A. (RASA) 0.18 C.C. + L.A. (RASA) C.C. + L.A. (RASA)
0.22 C C. .A A. . + + L L. .A A. . ( (S RM ASS A) ) 0.17 C C. .A A. . + + L L. .A A. . ( (S RM ASS A) ) 0.100 C C. .A A. . + + L L. .A A. . ( (S RM ASS A) )
0.20 C C. .C C. . + + C C. .A A. . + + L L. .A A. . ( (S RM ASS A) ) 0.16 C C. .C C. . + + C C. .A A. . + + L L. .A A. . ( (S RM ASS A) ) 0.075 C C. .C C. . + + C C. .A A. . + + L L. .A A. . ( (S RM ASS A) )
0.15 0.050
1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9
# of LLM Aggregator # of LLM Aggregator # of LLM Aggregator
(a)J1 (b)T1 (c)T2
Figure6: GLEUResultsbydifferentnumberofL.A.(Gemini)basedonSMSandRASA.
0.52 0.46 0.45
0.50 0.44
0.40
0.48 0.42
0.46 L L. .A A. . ( (S RM ASS A) ) 0.40 L L. .A A. . ( (S RM ASS A) ) 0.35 L L. .A A. . ( (S RM ASS A) )
C.C. + L.A. (SMS) 0.38 C.C. + L.A. (SMS) C.C. + L.A. (SMS)
0.44 C C. .C A.. ++ LL .. AA .. (( SR MAS SA )) 0.36 C C. .C A.. ++ LL .. AA .. (( SR MAS SA )) 0.30 C C. .C A.. ++ LL .. AA .. (( SR MAS SA ))
C.A. + L.A. (RASA) C.A. + L.A. (RASA) C.A. + L.A. (RASA)
0.42 C.C. + C.A. + L.A. (SMS) 0.34 C.C. + C.A. + L.A. (SMS) C.C. + C.A. + L.A. (SMS)
C.C. + C.A. + L.A. (RASA) C.C. + C.A. + L.A. (RASA) 0.25 C.C. + C.A. + L.A. (RASA)
0.40 0.32
1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9
# of LLM Aggregator # of LLM Aggregator # of LLM Aggregator
(a)J1 (b)T1 (c)T2
Figure7: METEORResultsbydifferentnumberofL.A.(Gemini)basedonSMSandRASA.
0.80 0.80
0.75
0.78 0.74 0.78
0.73 0.76
0.72 L.A. (SMS) 0.76 L.A. (SMS) 0.74 L.A. (SMS) 0.71 L.A. (RASA) L.A. (RASA) 0.72 L.A. (RASA)
C.C. + L.A. (SMS) 0.74 C.C. + L.A. (SMS) C.C. + L.A. (SMS)
0.70 C.C. + L.A. (RASA) C.C. + L.A. (RASA) 0.70 C.C. + L.A. (RASA)
C.A. + L.A. (SMS) C.A. + L.A. (SMS) C.A. + L.A. (SMS)
0.69 C.A. + L.A. (RASA) 0.72 C.A. + L.A. (RASA) 0.68 C.A. + L.A. (RASA)
0.68 C.C. + C.A. + L.A. (SMS) C.C. + C.A. + L.A. (SMS) C.C. + C.A. + L.A. (SMS)
C.C. + C.A. + L.A. (RASA) C.C. + C.A. + L.A. (RASA) 0.66 C.C. + C.A. + L.A. (RASA)
0.67 0.70
1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9
# of LLM Aggregator # of LLM Aggregator # of LLM Aggregator
(a)J1 (b)T1 (c)T2
Figure8: EmbeddingSimilarityResultsbydifferentnumberofL.A.(Gemini)basedonSMSandRASA.
UELG
ROETEM
ytiralimiS
gniddebmE
UELG
ROETEM
ytiralimiS
gniddebmE
UELG
ROETEM
ytiralimiS
gniddebmEaggregators: [0, 0.25, 0.5, 0.75, 1]; temperatures
of 7 LLM aggregators: [0, 0.1, 0.25, 0.5, 0.75,
0.9, 1]; temperatures of 9 LLM aggregators: [0,
0.1,0.25,0.5,0.75,0.9,1,1.1,1.25]. Theyreach
similar observations with the evaluations GPT-4
LLMaggregatorsinthemainmanuscript.