1
Delay-Constrained Grant-Free Random Access in
MIMO Systems: Distributed Pilot Allocation and
Power Control
Jianan Bai, Zheng Chen, and Erik G. Larsson
Abstract—We study a delay-constrained grant-free random orthogonal pilots. Additionally, the transmit power needs to
access system with a multi-antenna base station. The users be properly selected to ensure that the data packets can be
randomlygeneratedatapacketswithexpirationdeadlines,which successfully delivered with minimal inter-user interference.
are then transmitted from data queues on a first-in first-out
basis. To deliver a packet, a user needs to succeed in both Inthispaper,weconsidertheproblemofpilotselectionand
random access phase (sending a pilot without collision) and power control in a multiple-input multiple-output (MIMO)-
data transmission phase (achieving a required data rate with
enabled GFRA system. The problem is complicated by the
imperfect channel information) before the packet expires. We
need for a cross-layer modeling and the uncoordinated nature
develop a distributed, cross-layer policy that allows the users to
dynamically and independently choose their pilots and transmit of GFRA. We aim to develop a distributed policy such that
powers to achieve a high effective sum throughput with fairness users can dynamically and independently select their pilots
consideration. Our policy design involves three key components: and transmit powers by using only local information to max-
1) a proxy of the instantaneous data rate that depends only on
imize the network performance and provide fairness among
macroscopic environment variables and transmission decisions,
users. We propose to solve this problem using deep learning,
considering pilot collisions and imperfect channel estimation; 2)
a quantitative, instantaneous measure of fairness within each which can learn a complicated policy without relying on a
communicationround;and3)adeeplearning-based,multi-agent usuallyrestrictivemodel[4].Differentlearningparadigmscan
control framework with centralized training and distributed be applied in different scenarios – supervised learning for
execution. The proposed framework benefits from an accurate,
approximatingknownpolicieswithlabeleddata;unsupervised
differentiable objective function for training, thereby achieving
learning for cases where an explicit objective function can
a higher sample efficiency compared with a conventional ap-
plication of model-free, multi-agent reinforcement learning algo- be obtained [5]; reinforcement learning (RL) for making
rithms.Theperformanceoftheproposedapproachisverifiedby sequential decisions when neither labeled data nor an explicit
simulations under highly dynamic and heterogeneous scenarios. objective function is available.
Index Terms—Grant-free random access, delay constraint, Among various learning paradigms, multi-agent reinforce-
MIMO, fairness, and distributed control.
mentlearning(MARL)appearstobethemostrelevant,andit
has been successfully applied to develop distributed policies
I. INTRODUCTION in wireless networks (e.g., [6]–[10]). However, conventional
Ultra-reliable low-latency communication (URLLC) is an- MARLschemesweredevelopedforgeneral-purposetasksand
ticipated to facilitate a variety of emergent applications such may not provide the most efficient solution to our particular
as remote surgery and autonomous vehicles [2]. Conventional use case. To be specific, they suffer from: i) delayed and
grant-based scheduling fails to meet the delay requirements sparse rewards (the immediate reward might not accurately
due to its excessive handshake overhead, often surpassing evaluate actions in the long run); ii) incapability of satisfying
the tolerable 1-millisecond delay. Grant-free random access instantaneous constraints; iii) the multi-agent credit assign-
(GFRA)isapromisingsolutiontoreduceuplinklatency[3].In ment problem (a global reward may not reflect an individual
GFRA,userscantransmitpayloaddatatogetherwithmetadata contribution); and iv) a high demand for samples (an accurate
(pilot and other signaling) without waiting for permission or sample-based estimation is required for the expected return,
scheduling information. Despite the advantages of GFRA, a which is difficult to obtain for large search spaces).
major challenge is the allocation of pilot sequences to users,
Model-basedlearninghasdemonstratedeffectivenessacross
and the handling of pilot collisions during the uplink access,
variousapplications[11],andonecouldexpectfurtherperfor-
which inevitably results if there are more users than available
mance improvements by integrating specific domain knowl-
edgeintothealgorithmdesign.Aswewillseeshortly,forour
ThispaperwaspresentedinpartattheAsilomarSSC2021conference[1].
The authors are with the Department of Electrical Engineering (ISY), problem,wepossessstrongdomainknowledge:i)thecollision
Linko¨ping University, 58183 Linko¨ping, Sweden (email: jianan.bai@liu.se, probability using a stochastic pilot selection policy can be
zheng.chen@liu.se,erik.g.larsson@liu.se).Thisworkwassupportedinpartby
calculated; ii) the success probability of payload transmission
Excellence Center at Linko¨ping-Lund in Information Technology (ELLIIT),
andbytheKnutandAliceWallenberg(KAW)foundation.Thecomputations for a given power allocation can be well approximated; and
wereenabledbyresourcesprovidedbytheNationalAcademicInfrastructure iii)thestochasticoptimizationproblemcanbe(approximately)
forSupercomputinginSweden(NAISS)andtheSwedishNationalInfrastruc-
solved by solving a sub-problem in each decision stage with
tureforComputing(SNIC)partiallyfundedbytheSwedishResearchCouncil
throughgrantagreementsno.2022-06725andno.2018-05973. anobjectivefunctionthatmorepreciselyevaluatestheactions.
4202
tcO
22
]TI.sc[
1v86071.0142:viXra2
A. Related Work of the rate expression on the random small-scale fading for
When using mutually orthogonal pilots, several approaches policy design, we develop a rate proxy that depends only on
to pilot allocation and collision resolution for random access the macroscopic environment variables and the transmission
have been proposed. For example, the possibility of using decisions of users. To the best of our knowledge, the rate
multiple or superimposed pilots, to effectively retain the pilot proxy for ZF under pilot collisions is new.
2) Quantification of Fairness:
orthogonality, was investigated in [12]–[14]. To improve the
We study min-max fairness of the system by minimizing
collisionresolution,anotherlineofwork(e.g.,[15])exploited
the (normalized) packet drop rate of the worst performing
channel hardening and favorable propagation properties of
user in Section III. The original formulation of the problem
massive MIMO and used successive interference cancellation
is a stochastic network optimization problem, which involves
to recover the collided signals. Strategies that assign users
the time average of the stochastic packet drop processes with
unique but mutually non-orthogonal pilots were investigated
time dependence imposed by the evolution of data queues
in, for example, [16] along with associated collision resolu-
that cannot be fully predicted. To overcome this challenge,
tion algorithms based on compressed sensing techniques. A
we develop two approximations to the problem that can be
comparative analysis of the use of orthogonal versus non-
solved immediately in each decision stage. Additionally, we
orthogonal pilots was presented in [17]. The results suggest
revealaunifiedstructurebehindthesetwoapproximations,and
that the performance of non-orthogonal pilots, which reduces
interpret it as a sum-priority maximization. Specifically, the
pilot collision at the expense of degraded channel estimation
priority level of each user takes accounts of both its previous
qualitycomparedtothecaseoforthogonalpilots,iscontingent
access results and the current queue status. The (normalized)
on the specific scenario. Specifically, non-orthogonal pilots
sum-priorityprovidesanaccurate,instantaneousquantification
may underperform when requiring high data rates. Studying
of fairness within each communication round.
non-orthogonal pilots is not the main focus of our paper, but
3) Deep Learning-Based Distributed Policy Design:
we will provide some numerical comparisons as a baseline.
To exactly maximize the sum-priority, the users still need
Non-coherent transmission schemes and unsourced communi-
to share information (e.g., priority levels) to each other or to
cation systems (e.g., [18], [19]) are beyond our scope.
a central server, which contradicts the open-loop operations
Applying MARL in GFRA systems has received increasing
of GFRA. Therefore, we propose a deep learning-based dis-
attention. A pilot selection policy was developed in [6] with
tributed control framework that requires centralized training
significant improvements in the average aggregate throughput
but enables distributed execution in Section IV. This learning
compared with various baseline schemes. However, [6] con-
framework is motivated by MARL, while significantly deviat-
sidered only a non-dynamic system without delay constraints
ing from conventional MARL by employing an unsupervised
and data rate requirements. In [7], a carrier-sense multiple
training scheme. Particularly, by exploiting our results above,
access (CSMA) system with a single channel was considered,
weobtainalearningobjective(theexpectedsum-priority)that
wherein each user selects its access probability based on the
isdirectlydifferentiablewithrespecttothepolicyparameters,
urgency of their packets and system load. A transmission
which obviates the need for a sample-based estimate of the
tax was introduced to decouple the multi-agent training for
expected reward over the joint action space. This objective
improvedscalability.Aclustering-basedsub-channelselection
function also accurately measures individual contributions so
and (discrete) power control policy was designed in [8] for a
that the credit assignment problem is naturally alleviated. The
non-orthogonal multiple access (NOMA) system to maximize
frameworklearnsahybridpolicythatcombines(discrete)pilot
the long-term throughput. In [9], the authors considered the
selection and (continuous) power control.
coexistence of ALOHA users and users that employ a learned
Remark:Partofthisworkwaspresentedintheconference
random access policy with delay-constrained traffic. A dis-
paper [1], where we considered only the pilot transmission in
tributed policy for dynamic resource selection is developed in
a simplified collision model and assumed that packet delivery
[10] for a lightly loaded system with a relatively large delay
is successful whenever the pilot transmission is successful. In
tolerance. To the best of our knowledge, there has not been a
this paper, we consider a much more realistic scenario with
researchworkthatconsidersjointpilotselectionand(continu-
data rate requirements and incorporate power control.
ous) power control for a realistically modeled MIMO-assisted
GFRAsystemwithstringentdelayrequirements.Additionally,
C. Notation
most research in this direction applies conventional model-
freeMARLalgorithmswithoutefficientlyexploitingthemodel Vectors aredenoted by boldfacelowercase letters, x, matri-
knowledge to accelerate the learning process. ces by boldface uppercase letters, X, and sets by calligraphic
letters, X, with cardinality |X|. The superscripts (·)T, (·)H,
B. Contributions and Organization of the Paper (·)∗,and(·)−1 denotetranspose,conjugatetranspose,complex
1) Cross-Layer Modeling: conjugate,andinverse,respectively.E[·]denotesthestatistical
Wepresentthephysicallayerandthenetworklayermodels expectation. 1{·} is the indicator function, which equals to 1
of the system in Section II. Particularly, in the physical layer, for true propositions and 0 otherwise. Cn denotes the space
we characterize the instantaneous data rate of users, for both of n-dimensional complex vectors. The multivariate circularly
maximum ratio (MR) and zero-forcing (ZF) receive combin- symmetriccomplexGaussiandistributionwithcovariancema-
ing, with a minimum mean-square error (MMSE) channel trixRisdenotedbyCN (0,R).D denotesadiagonalmatrix
x
estimator and pilot collisions. To eliminate the dependence withxonitsdiagonal.∥·∥denotestheEuclideanvectornorm.3
II. SYSTEMMODEL when |U lt| ≥ 1, and define {g lmt} as independent CN (0,1)
Weconsidertheuplinkofasingle-cellnarrowbandwireless random variables for the case when |U lt| = 0. Notice that
system. The base station (BS) has M receive antennas and g lmt =h imt whenU lt ={i},whichholdsforallnon-collided
serves N machine-type devices (users) located within its users. We can then re-write (1) as
coverage area. Time is divided into equal-length slots. We yp =(cid:88)(cid:112) ρ |U |g ϕ +wp . (3)
mt 0 lt lmt l mt
adopt the block-fading assumption, i.e., the channels remain
l∈L
constant during a slot (consisting of τ symbols) and vary
For activity detection (the process of identifying the active
independently across different slots.1 The uplink data of each
users by processing the received pilot signals), due to the
user are divided into equal-size packets and the transmission
orthogonality of pilots, we de-spread the received signal by
duration of each packet is one slot. We model the packet
arrivals at each user by a Bernoulli process, i.e., a new packet ϕHyp =(cid:112) ρ |U |g +ϕHwp , (4)
l mt 0 lt lmt l mt
is generated at user i with probability λ in each slot. Each
user, i ∈ N ≜ {1,··· ,N}, has a di ata queue to store where ϕH l wp mt ∼CN (0,1) since the pilots have unit energy.
ϕHyp hasdistributionCN (0,ρ |U |+1)andisindependent
the generated data packets, with the queue backlog in slot l mt 0 lt
across different antennas. Therefore, we have
t denoted by Q . We define the set of backlogged users
it
( bt ah co ks le ogw gi eth d n uo sen r- ,e im ∈pty K tq ,u ce au nes d) eb cy ideK wt ≜ het{ hi er: Q toit ac> ce0 s} s. aE ta thch e M1 (cid:88)M |ϕH l yp mt|2 −M −−→ −∞ →ρ 0|U lt|+1 (5)
beginning of the slot. m=1
There are L mutually orthogonal pilots ϕ ,··· ,ϕ ∈CL, by the law of large numbers. When the number of antennas is
1 L
each normalized to have unit energy such that ∥ϕ ∥ = 1 for sufficiently large so that the channel hardens, the multiplicity
l
all l∈L≜{1,··· ,L}. We require L<N due to the limited of the transmitted pilots, i.e., u ≜[|U |,··· ,|U |]T, can be
t 1t Lt
channel coherence so that the users cannot be pre-assigned accuratelydetermined byenergy detection[23]. Sinceactivity
unique,mutuallyorthogonalpilots.Pilotcollisionoccurswhen detection is not the main focus of our paper, and to simplify
multiple users select the same pilot. The pilot selection of the analysis, we assume perfect pilot detection.
each backlogged user i∈K is represented by a ∈{0}∪L, Assumption 1: The multiplicities of the transmitted pilots,
t it
where a = 0 denotes the decision to back off, and a = u , is known.When a pilotis transmitted by exactlyone user,
it it t
l∈L indicates that the l-th pilot is selected. For an idle user i.e., |U |=1, the identity of that user can be known.
lt
i ∈/ K , we set a = 0 by default. Additionally, we define 2) Channel Estimation: Define the set of active pilots as
t it
U
lt
≜{i:a
it
=l} as the set of users that select the l-th pilot, La tct ≜ {l : |U lt| ≥ 1}. Since we cannot identify the collided
and U ≜U ∪···∪U as the set of active users (those who users, we choose to estimate the effective channel coefficients
t 1t Lt
transmit any of the pilots). {g } for each active pilot, instead of estimating the actual
lmt
channel coefficients {h } for each active user. Notice that
imt
A. Physical Layer Model this makes no difference for non-collided users. The MMSE
estimate of g is given by
1) Pilot Detection: During pilot transmission in slot t, the lmt
received pilot signal, yp mt ∈CL, at the m-th antenna is g = (cid:112) ρ 0|U lt| ϕHyp , (6)
yp = (cid:88) (cid:113) Lβ ρph ϕ +wp (cid:98)lmt ρ 0|U lt|+1 l mt
mt i i imt ait mt
and the mean-square of the channel estimate is
i∈Ut (1)
=(cid:88) (cid:88) (cid:113) Lβ iρp ih imtϕ l+wp mt, c
lt
≜E[|g (cid:98)lmt|2]=
ρ
ρ |U0|U |l +t| 1. (7)
l∈Li∈Ult 0 lt
where β represents the large-scale fading coefficient (LSFC) By the orthogonality principle, the channel estimation error
i
of user i (similar to [22], β i is normalized such that the noise g (cid:101)lmt ≜g lmt−g (cid:98)lmtisuncorrelated(and,therefore,independent
hasunitvariance),h
imt
∼CN (0,1)representsthesmall-scale under Rayleigh fading) with g lmt. Also, the mean-square
fading coefficient that is assumed to be independent across estimation error is given by 1−c lt.
users and antennas, ρp ∈ [0,ρ ] is the transmit power of 3) Payload Data Transmission: During the data transmis-
i max
the pilot signal, and wp ∼CN (0,I ) is additive noise that sion phase, the received signal at the BS is given by
mt L
is independent across antennas. (cid:88) (cid:112)
y = β ρ q h +w ,
We consider channel inversion power control for pilot t i it it it t (8)
transmission, i.e., ρp = (β /β )ρ , where β ≜ i∈Ut
i min i max min
min i∈N{β i}. This gives Lβ iρp i = Lβ minρ max ≜ ρ 0. We where ρ it ∈ [0,ρ max] represents the transmit power (notice
further define the effective channel coefficient of pilot l as that we assumed channel inversion power control only for the
pilot transmission), h ≜ [h ,··· ,h ]T is the channel
1 (cid:88) it i1t iMt
g lmt ≜ (cid:112) h imt ∼CN (0,1) (2) vector, q it is the transmitted data symbol with unit energy
|U |
lt i∈Ult which is uncorrelated across users, and w t ∼CN (0,I) is the
noise vector.
1We choose the block-fading model for simplicity and tractability. More We denote by g ≜[g ,··· ,g ]T the effective channel
realistic channel models that consider intra-block variations or inter-block lt l1t lMt
correlation(forexample,thosein[20],[21])areleftforfuturework. of the l-th pilot over all antennas. Analogously, we define g (cid:98)lt4
and g as the estimate and estimation error of g . For a non- 4) Rate Proxy for Algorithm Training: The instantaneous
(cid:101)lt lt
collided user i, that satisfies h = g = g +g , we rate expression in (12) depends on the random small-scale
it aitt (cid:98)aitt (cid:101)aitt
performreceivecombiningbyusingacombiningvectorv to channel fluctuations, which cannot be acquired by the users
it
obtain (the collided users are not interesting since they cannot when making transmission decisions. Instead, we look for a
be identified) rate metric that depends only on the macroscopic environ-
ment variables and transmission decisions (e.g., LSFCs, pilot
(cid:112) (cid:112)
vH ity t = β iρ itq itvH itg (cid:98)aitt+ β iρ itq itvH itg (cid:101)aitt selection, and power control), and will be using E[R it] (more
(cid:124) (cid:123)(cid:122) (cid:125) precisely, its lower bounds for tractability) as a proxy for R ,
desiredsignal it
+ (cid:88) (cid:112) β ρ q vHh +vHw . (9) where the expectation is taken over all small-scale channel
j it it it jt it t fluctuations.(Noticethatwewillalwaysusetheinstantaneous
j∈Ut\i rate in (12) for simulations. The expressions developed here
are used only for algorithm design.)
The instantaneous signal-to-noise-plus-interference ratio
By noticing that log(1+1/x) is a convex function, we can
(SINR) of that user is given by
apply the Jensen’s inequality to obtain
SINR =
β iρ it|vH itg (cid:98)aitt|2
.
E[R it]≥R
it
≜log(1+ℓ·SINR it) (14)
it β ρ |vHg |2+(cid:80) β ρ |vHh |2+∥v ∥2
i it it(cid:101)aitt j∈Ut\i j jt it jt it where
(10)
(cid:18) (cid:20) (cid:21)(cid:19)−1
1
For ease of notation, we define a superscript (·)act. SINR ≜ E . (15)
it SINR
Definition1:ForamatrixX,oravectorx,withatleastone it
dimension corresponding to the pilot indices L, we define a Proposition 1:
reduced-dimensional matrix Xact, or a vector xact, by keeping 
(M −1)c β ρ
o xn .ly Coth ne vee rsn et lr yie ,s wc ho er nres Xpo an ctdi on rg xto actac istiv de efip nil eo dts fiL rsa t tc ,t i Xn X oro xr
SINR
it
= (cid:80)
j∈
(MUtβ −jρ
|Ljt
ac− t|a )c ci att itti β βii ρt ρit+1, MR
. (16)
r ee np trr ie es sen ct os rrt eh se pe ox nt de in nd ged tom that eri ix nao cr tiv ve ect po ir lob ty
s
wfil il ti hng zeth roe s.missing 
(cid:80) j∈Ut(cid:0)
1−t
|Uca ai jt tt
ta |i (cid:1)tt
β
ji
ρ
jj tt +1, ZF
We consider both MR and ZF combining, by introducing
Proof: The result for MR follows immediately from [22,
the combining matrix
Appendix D]. The result for ZF is proved in the Appendix.
 For R to be an accurate approximation to R , the in-
act it it
Va tct ≜

G G(cid:98) (cid:98)t a tct, (cid:16) (G(cid:98)a tct )HG(cid:98)a tct(cid:17)−1
,
ZM FR , (11) s at ra on ut na dneo Su INs RS iI tN .R Ui nn fo( r1 t0 u) nas th eo lyu ,ld wb ee msu if gfi hc tie nn otl ty ac lo wn ac ye sntr ha ate vd
e
enoughconcentration. Tosee this,examinethe numeratorand
the denominator in (10) separately with a normalized v, i.e.,
where G(cid:98)t ≜ [g (cid:98)1t,··· ,g (cid:98)Lt], and taking the a it-th column as
∥v∥=1.Inthenumerator,therandomvariable|vHg |2/M
the combining vector v it, i.e., v it ≜[V t] :,ait. concentratesforbothMRandZFasM →∞.Howit e(cid:98) va ei rt ,t inthe
For a targeted decoding error probability, we approximate
denominator, the terms |vHg |2 and {|vHh |2} might
the instantaneous achievable data rate of user i by it(cid:101)aitt it jt i̸=j
not concentrate. Take MR combining for example, |vHg |2
it(cid:101)aitt
and {|vHh |2} become independent exponential random
R =log (1+ℓ·SINR ), (12) it jt i̸=j
it 2 it variables. Unless |U | is sufficiently large, the denominator
t
does not necessarily concentrate. The problem can be allevi-
where ℓ∈(0,1] is a penalty factor accounting for the effects
atedforZF,whengoodchannelestimatesareobtainedsothat
of finite blocklength2 and the coding and modulation scheme.
theinterferencecanbeconsiderablysuppressed.Butageneral
Such an approximation has been used in, for example, [25],
conclusion is that, when making short packet transmissions,
and 1/ℓ is also known as the SINR gap [26].
one may not be able to benefit from a concentrated SINR
Recall that each user has fixed-size packets corresponding
even in massive MIMO. Fortunately, as we will observe in
to a fixed instantaneous rate requirement. Denoting the rate
the numerical results, approximating SINR by SINR still
thresholdofuseriasRth,wemakethefollowingassumption. it it
i results in a useful algorithm.
Assumption2:Anon-collidedusericansuccessfullydeliver
its head-of-line packet if R ≥Rth.
it i
B. Network Layer Model
Finally, we define the success indicator of user i, based on
Assumptions 1 and 2, as Recall that we consider the random packet arrivals at each
useri∈N,modeledasaBernoulliprocesswithrateλ .Once
i
µ ≜1{|U |=1}·1{R ≥Rth}. (13) generated,thepacketsarebackloggedinthequeueofthatuser.
it aitt it i
Additionally, to account for the timeliness of data packets,
2A more accurate characterization of the finite-blocklength effect can be we assume that every packet of user i is associated with
obtained using, for example, the normal approximation in [24, Th. 55]. a maximum tolerable delay (also referred to as a deadline),
Since an accurate finite-blocklength analysis is not our focus, we use the denoted as dmax, that is defined as the number of time slots
approximationin(12).However,ourapproachcanbeappliedaslongasthe i
rateexpressionisanon-increasing,convexfunctionofthe1/SINR. within which a newly generated packet has to be delivered to5
the destination before expiration. For simplicity, we assume ···×A , with A ={0}∪L. Notice that D is a stochastic
N i i
that all packets of user i have the same maximum tolerable function of all joint decisions {a } and {ρ } across time.
t t
delaysothateachqueueoperatesinafirst-in-first-outmanner. It is infeasible to directly solve (P) to obtain an optimal
We define d ∈ {1,··· ,dmax}, for all i ∈ K , to be the sequential decision solution due to the following two reasons:
it i t
number of remaining time slots (including the current one) • Theproblem(P)involvesthetimeaverageofthestochas-
of the head-of-line packet at slot t before it expires. When ticprocesses{D }withtimedependenceimposedbythe
it
the queue is empty, i.e., i∈/ K t, we set d it =0 by default. A evolutionofdataqueues,whichcannotbefullypredicted.
packetisdiscardedifitcannotbesuccessfullydeliveredbefore • The max function in (P) requires the determination of
the deadline. We therefore define the packet drop indicator as worst-performing user i∗ = argmax {D /Dth} for all
i i i
D ≜(1−µ )1{d =1}. (17) feasible decisions, which is a combinatorial problem.
it it it
In what follows, we develop two approaches to solve (P)
Let {γ it} denote the packet arrival process, where each γ it approximately by constructing a time-varying objective (that
is modeled as a Bernoulli random variable with E[γ it] = λ i combines both the previous access results and the urgency of
and is independent across users and slots. Also, define the undelivered packets) and greedily optimizing the objective in
packet departure process {b it} given by everyslottomakereal-timedecisionsthatdependonlyonthe
b ≜µ +D , (18) current state of the system.
it it it
Remark 1: By “greedy”, we mean making decisions based
which equals 1 when µ it = 1 or D it = 1, and 0 otherwise. on only local or immediate information, without considering
The evolution of the queue backlog of user i is described by theimpactonfuturetimeinstances[27,pp.64].Itcangreatly
reducethecomplexityofareal-timedecision-makingprocess.
Q =max{Q −b ,0}+γ . (19)
i,t+1 it it it
Meanwhile, a greedy approach can still perform well, even in
thelongrun,iftheimmediateobjectiveisproperlychosen.For
III. MIN-MAXFAIRNESS
example,inQ-learning,greedilyselectingactionstomaximize
Weconsiderafairnessperspectiveofthesystem,formulated
the Q-function (if accurately estimated) is optimal in the long
as a stochastic network optimization problem that minimizes
run, as the Q-function represents the long-term return.
the(normalized)packetdroprateoftheworst-performinguser.
To obviate the difficulties in directly solving this problem,
B. The First Approach: Log-Sum-Exp
we propose two approximations, one using a log-sum-exp ap-
proximation of the max function, and the other employing the Weapproximatethemaxfunctionin(P)bythelog-sum-exp
Lyapunovdrift-plus-penaltyframework.Thesetwoapproaches function as in [28], i.e.,
give a unified, quantitative measure of instantaneous fairness, (cid:32) (cid:33)
1 (cid:88)
interpreted as the “sum-priority” of the successful users. max{x }≈ log exp(αx ) , (21)
i i
i∈N α
i∈N
A. Stochastic Formulation where α ∈ (0,∞) can be interpreted as an “inverse temper-
We define the effective throughput of user i as the average ature”. As shown in [29, pp. 72], the approximation gap is
number of data packets it successfully delivers per time slot, upper-bounded by α1 logN, and the approximation becomes
λ −D , where D is the packet drop rate defined as an exact equality if α→∞.
i i i
By applying the log-sum-exp approximation and limiting
(cid:34) 1 (cid:88)T (cid:35) our focus to a finite frame T ≜{1,··· ,T} with T slots, we
D ≜limsupE D . (20)
i T it obtain the following problem
T→∞
t=1
Here, the expectation is taken over the randomness of the 1 (cid:32) (cid:88) (cid:32) α (cid:88)T (cid:33)(cid:33)
minimize log exp D
p {back }e .t Ta orr miv aa xl ip mro izc ees ts he{γ ei ft f} e, cta ivn ed tt hh re oup gac hk pe ut td oe fp aar utu sere r,p wro ec ce as ns {at,ρ t} α
i∈N
TD ith
t=1
it (22)
it
equivalently minimize the packet drop rate. subject to (a t,ρ t)∈A×[0,ρ max]N, ∀t∈T.
EachuserisassociatedwithadropratethresholdDth,which To simplify (22), we remove the logarithm (the problem
i
represents the quality of service (QoS) requirement. We then will not change due to the monotonicity of the logarithm) and
formulate the stochastic min-max fairness problem as3 define the normalized cumulative packet drop rate (NCPDR)
(cid:26) (cid:27)
D t
minimize max i 1 (cid:88)
su{ bat j, eρ ct t}
to
(i a∈N
,ρ
)D ∈ith
A×[0,ρ ]N
(P) ξ it = TD ith t′=1D it′, ∀i∈N, (23)
t t max
where TDth can be interpreted as the total “budget” of packet
∀t∈{1,2,···}, i
drops of user i in a frame, and ξ is the ratio of currently
it
where a ≜ [a ,··· ,a ]T and ρ ≜ [ρ ,··· ,ρ ]T are consumed budget till slot t. Then, (22) can be re-written as
t 1t Nt t 1t Nt
joint pilot and power allocations in slot t, and A ≜ A 1 × (cid:88)
minimize f(ξ )
iT
3InadditiontothefractionalobjectiveDi/D ith,theproposedapproachalso {at,ρ t} i∈N (24)
worksforotherobjectives,e.g.,Di−D ith. subject to (a t,ρ t)∈A×[0,ρ max]N, ∀t∈T,6
where we introduce a fairness-promoting function C. The Second Approach: Virtual-Queue
exp(αx)−1 By introducing an auxiliary variable, z > 0, Problem (P)
f(x)≜ , (25) can be expressed in epigraph form as
exp(α)−1
minimize z
such that α → ∞ leads to strict min-max fairness, α → 0
{at,ρ t},z
gives f(x) = x and the problem becomes a sum-drop-rate subject to D ≤zDth, ∀i∈N
minimization,andα∈(0,∞)givesanelasticleveloffairness i i (29)
(a ,ρ )∈A×[0,ρ ]N
among different devices. (The function f(x) is a normalized t t max
version of exp(αx). This does not change the problem but ∀t∈{1,2,···}.
permitsabetterinterpretationasα→0.)Problem(24)admits
Weintroduceaboundedstochasticprocessz ∈[0,z ],such
t max
a straightforward interpretation – the cost associated with a that limsup E[1 (cid:80)T z ]=z. Then, problem (29) can
user is determined by its final NCPDR through a mapping T→∞ T t=1 t
be transformed into
defined by the fairness-promoting function.
(cid:34) T (cid:35)
Obtaining an optimal sequence of decisions requires one 1 (cid:88)
minimize limsupE z (30a)
t
to know all the packet arrivals and channel conditions in the {at,ρ t,zt} T→∞ T
t=1
frame, which is still infeasible. We obviate this difficulty and (cid:34) T (cid:18) (cid:19)(cid:35)
make real-time decisions by greedily solving the following subject to limsupE 1 (cid:88) D it −z ≤0 (30b)
problem in each slot T→∞ T
t=1
D ith t
(cid:88)
(cid:18)
1
(cid:19) (a t,ρ t)∈A×[0,ρ max]N, ∀t∈{1,2,···}
minimize f ξ + D (a ,ρ )
at,ρ t i∈N i,t−1 TD ith it t t (26) 0≤z t ≤z max, ∀t∈{1,2,···}.
subject to (a ,ρ )∈A×[0,ρ ]N, The constraint (30b) can be transformed into a queue stability
t t max
problem.Toseethis,weassigneachuseravirtualqueue.The
where we write D it as D it(a t,ρ t) to accentuate that it is vector of virtual queue backlogs (one should distinguish this
an explicit function of the joint decision (a t,ρ t). Similar
from the data queue backlog Q it) of all users is denoted as
notations will be used henceforth. X ≜ [X ,··· ,X ]T, where the virtual queue backlog of
t 1t Nt
The formulation in (26) ignores the prior information about user i is updated by
future (potential) packet drops, which has already been in-
D
cluded in the expiration time d it of the head-of-line packet. X i,t+1 =max{X it−z t,0}+ Di tht . (31)
(One may also consider the expiration time of other packets i
in the queue and the arrival rates.) Therefore, similar to [30], The constraint in (30b) is satisfied if X is rate stable [31],
it
we replace D it(a t,ρ t) by δ(cid:101)it(1−µ it(a t,ρ t)), where i.e., lim t→∞X it/t=0 almost surely, for all i∈N.
(cid:0) (cid:1)
Denote by Γ = X ,d the network state, where d =
d −1 t t t t
δ(cid:101)it ≜1− dit
max
. (27) [d 1t,··· ,d Nt]T contains the packet deadlines. To establish
i queue stability, we consider the conditional Lyapunov drift
Here, δ(cid:101)it can be interpreted as an urgency level to deliver ∆
t
≜E(cid:2) φ(cid:0) Γ t+1(cid:1) −φ(cid:0) Γ t(cid:1) |Γ t(cid:3) , (32)
t µh ie t(ah te ,a ρd t- )o )f- ali nn de Dp ia tc (k ae tt ,ρi tn
)
t th ake eq tu he eu se a. mN eo et xic tre emth eat vaδ(cid:101)
li ut
e( s1 −
where φ(cid:0) Γ t(cid:1) ≜ 1 2(cid:80) i∈N X i2 t is a quadratic Lyapunov func-
tion. We further consider the drift-plus-penalty function
(cid:26)
0: if µ (a ,ρ )=1
it t t
∆
+VE(cid:2)
z |Γ
(cid:3)
, (33)
1: if µ (a ,ρ )=0 and d =1, t t t
it t t it
where V >0 is a factor controlling the trade-off between the
whiletheformercanbeseenasasoftenedversionofthelatter
queue stability and the optimality of the objective in (30a).
onebyassigningnon-zerovaluesinbetweentoincorporatethe
The drift-plus-penalty (33) is upper bounded by
prior information of future packet drops.
Finally, we approximate (22) by using a series of sub- ∆(t)+VE(cid:2) z |Γ (cid:3)
t t
problems that will be solved in each slot t∈T: (cid:34) (cid:35)
maximize (cid:88) η(S1) µ (a ,ρ ) = 21 E (cid:88)(cid:0) X i2 ,t+1−X i2 t(cid:1)(cid:12) (cid:12) (cid:12)Γ t +VE(cid:2) z t|Γ t(cid:3)
suba jt e,ρ ct
t to
(i a∈ tK ,t ρ(cid:101) ti )t ∈Ait ×t [0,ρt
max]N,
(S1) ≤1 2E(cid:34)i (cid:88)∈N (cid:32)
z
t2+(cid:18) D
Di
tht(cid:19)2(cid:33) (cid:12)
(cid:12) (cid:12)Γ
t(cid:35)
i∈N i
where η (cid:101)i( tS1) is defined by4 +E(cid:34)
(cid:88) X
(cid:18)
D it −z
(cid:19)(cid:12)
(cid:12)Γ
(cid:35)
+VE(cid:2) z |Γ (cid:3)
η (cid:101)i( tS1) ≜f(cid:0) ξ i,t−1+δ it(cid:1) −f(ξ i,t−1) (28)
i∈N
it D ith t (cid:12) t t t
with the normalized urgency level δ it ≜δ(cid:101)it/(TD ith). ≤N
2
z m2 ax+ N 22 (cid:88)(cid:18) D1 th(cid:19)2
i∈N i
4Noticethatf(cid:0) x+yµ(cid:1) =f(x)+(cid:0) f(x+y)−f(x)(cid:1) µforµ∈{0,1}. (cid:124) (cid:123)(cid:122) (cid:125)
constant7
(cid:34) (cid:32) (cid:33) (cid:35)
+E (cid:88) X it D + V−(cid:88) X z (cid:12) (cid:12)Γ . (34)
Dth it it t(cid:12) t
i∈N i i∈N
We approximate Problem (29) by greedily minimizing the
upper bound of the drift-plus-penalty function in (34). This
leads to a sequence of subproblems in each slot t∈T:
(cid:32) (cid:33)
minimize (cid:88) X it D (a ,ρ )+ V−(cid:88) X z
at,ρ t,zt
i∈N
D ith it t t
i∈N
it t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (35)
dependsonlyon(at,ρ t) dependsonlyonzt
subject to (a ,ρ )∈A×[0,ρ ]N Fig. 1: An illustration of the mapping defined by the fairness
t t max
promoting function in (S1).
0≤z ≤z .
t max
The first term in the objective function of (35) depends only
Log-Sum-Exp: In(S1),weintroducethefairness-promoting
on (a ,ρ ), and the second term depends only on z . Thus,
t t t function,f(·),toprovideamappingfromtheNCPDR,ξ ,
we can solve for the optimal (a ,ρ ) and for the optimal z i,t−1
t t t and the normalized urgency level, δ , to the priority level η .
separately. Minimizing the second part gives it (cid:101)it
See the illustration in Fig. 1. One can observe that as ξ
i,t−1
z =z
·1(cid:110)(cid:88)
X
>V(cid:111)
, (36)
increases,η
(cid:101)it
growmorerapidlywithδ it.Theimpactofξ
i,t−1
t max i∈N it is controlled by the inverse temperature α. As α → 0, the
impact of ξ disappears, and we obtain a sum-drop-rate
i,t−1
which will be used in (31) for updating the virtual queue
minimization problem.
backlog X .
it Virtual-Queue: The interpretation of (S2) becomes more
Similar to problem (S1), we replace D it(a t,ρ t) by δ(cid:101)it(1−
straightforwardintheextremecasewhenV →∞.Thevirtual
µ it(a t,ρ t)) in the first term, where δ(cid:101)it is defined in (27), to queue backlog is now given by X
it
= (cid:80) tt ′− =1 1D it′/D ith =
incorporate the prior information on future packet drops. This
Tξ . The priority level becomes η (t) = Tξ δ . One
i,t−1 (cid:101)i i,t−1 it
gives the following problem
can see that (S2) also defines a mapping from ξ
i,t−1
and δ(cid:101)it
maximize (cid:88) η(S2) µ (a ,ρ ), totheprioritylevelη (cid:101)it,andthesameargumentholds:alarger
at,ρ
t
i∈Kt(cid:101)it it t t
(S2)
ξ
i,t R− e1
mm ara kke 2s :η
(cid:101) Ei xt
ag cr to lyw sm olo vr ie ngra (p Si 1d )ly anw dith (Sδ(cid:101)
2i
)t.
still requires the
subject to (a t,ρ t)∈A×[0,ρ max]N, users to share information to each other or to a central server,
contradicting the open-loop nature of GFRA. We circumvent
where η (cid:101)i( tS2) is calculated by this need by developing a deep learning framework to enable
the users to learn a distributed access policy (through central-
η (cid:101)i( tS2) =X itδ it. (37) ized, offline training) that approximates the solution to (S1)
and (S2) by using only their local information in Section IV.
Notice that the virtual queue backlog X in (37) plays a
it
similar role as ξ in (28) – they both incorporate historical
it IV. DISTRIBUTEDPOLICYDESIGN
information about previous access results so that a user with
We now shift our focus to the development of a policy for
larger X or ξ will be prioritized. However, it is less
it it joint pilot selection and power control in the GFRA system
straightforward to interpret how the parameters V and z
max introduced in Section II. By a “policy”, we mean a mapping
affect the evolution of X . Roughly speaking, from (31) and
it fromsituationstodecisions,whichcanbeeitherdeterministic,
(36), we observe that V determines how frequently X is
it stochastic,or mixed. Notice that theapproximations, (S1)and
updated,andz determineshowsignificanteachupdatecan
max (S2), developed in Section III essentially define two different
be (their effects can not be separated though). By choosing a
policies. That is, by knowing the global information, denoted
smallV andalargez ,thehistorywillbediscardedrapidly.
max s, that consists of the priority levels, the queue status, and the
Conversely, a large V with a small z keeps a long history.
max LSFCsofallusers,solving(S1)or(S2)givesaglobalcontrol
decision (a,ρ). Nevertheless, a global policy that requires
knowing s cannot be implemented for GFRA, since the users
D. A Unified Perspective: Sum-Priority Maximization
only have access to their local information and, potentially,
The approximated problems (S1) and (S2) share a unified some limited feedback information. We therefore look for a
structure, where the coefficient η(s), for s∈{S1,S2}, can be distributed policy where each user i uses its local information
(cid:101)it
interpreted as the priority level of user i in slot t. A solution o to generate its own control decision (a ,ρ ).
i i i
(a ,ρ ) is mapped to the success indicators {µ } in (13). Notation. We will reuse variables defined in Sections II
t t it
An optimal solution should maximize the sum-priority of the and III, but with slight changes. First, since time-dependent
successful users. These two approximation approaches differ information of the environment is encapsulated within the
in how the priority levels are defined: global state s, we will omit the time index in the subscript8
when considering only a single slot. When multiple slots are By substituting (16) into (41) and by defining the coefficients
considered, we will write the global state in slot t as s and {σ (a)} in Table I, we obtain
t ji
the decisions as (a t,ρ t). Second, we will explicitly write out (cid:88)
1{a ̸=0}σ (a)β ρ +1≤σ (a)β ρ , (42)
the variables’ dependence on a, ρ, and s. For example, we j ji j j ii i i
write the success indicator µ as µ (a,ρ|s), and the priority j∈N\i
it i
level η (cid:101)it as η (cid:101)i(s). where we can interpret the LHS as interference-plus-noise
Assumptions. In this section, we will introduce several as- power that scales with the transmit power of the interfering
sumptionswhencharacterizingthe(approximate)transmission users, and the RHS as the interference tolerance of user i that
success probability. These assumptions will be used only for scales with its transmit power. Additionally, the coefficients
algorithm design. The simulation environment will be fully {σ (a)} control how fast the interference power grows
ji j̸=i
based on the system model presented in Section II. with {ρ } , and σ (a) determines how large ρ is needed
j j̸=i ii i
to overpower the interference. One can observe that {σ (a)}
ji
have a very complicated dependence on the pilot selection
A. Expected Sum-Priority
decision a. We avoid this dependence by making additional
We consider a stochastic pilot selection policy, where user
approximations.
i chooses a i = l with probability π il, and
(cid:80)L
l=0π il = 1. Asshownin(7)andTableI,{σ ji(a)}dependonathrough
The matrix of pilot selection probabilities is denoted by {|U (a)|} for MR, and, additionally, on |Lact(a)| for ZF.
Π ≜ [π ,··· ,π ], where π ≜ [π ,π ,··· ,π ]T. Under ai
1 N i i0 i1 iL We postulate that a good control policy should efficiently
a global state s, we aim to obtain a joint policy (Π,ρ) to utilize all available pilots, i.e., |Lact(a)| ≈ L, without any
maximize the expected (normalized) sum-priority
pilot collisions, i.e., |U (a)| ≈ 1 for all l ∈ L. This gives
l
J(Π,ρ|s)≜ (cid:88) η i(s)P isuc(Π,ρ|s), (38) c ea sti it m(a a) te≈ theρ 0 i( ms p)/ a( c1
t
o+
f
ρ th0 e(s p)) il. otN co oti lc lie sioth na st
.
w Bye mm aig kh int gun thd ee sr e-
i∈N
approximations, we replace {σ (a)} by {ς (s)} in Table I.
ji ji
where η i(s) ≜ η (cid:101)i(s)/(cid:80)
j∈N
η (cid:101)j(s) is the normalized priority Notice that by taking the stochastic pilot selection policy,
level of user i, and P isuc(Π,ρ|s) = E[µ i(a,ρ|s)] is the 1{a
j
̸= 0} is a Bernoulli random variable which is equal to
success probability with the expectation taken by randomly one with probability 1−π . The LHS in (42) is a weighted
i0
sampling a using the probabilities in Π and by averaging sum of independent Bernoulli random variables with unequal
over small-scale channel fluctuations. Based on the definition non-zero probabilities, whose closed-form cumulative density
in (13), the success probability can be calculated by function (CDF) is generally very complicated [32]. To obtain
amoretractableexpression,weusethenormalapproximation,
Psuc(Π,ρ|s)=Pr{|U (a)|=1}
i ai where the LHS in (42) can be approximated as a normal
(cid:124) (cid:123)(cid:122) (cid:125)
≜Pp(Π) random variable with mean
·
(cid:124)Pr(cid:8)
R
ii
(a,ρ|s)≥ (cid:123)R
(cid:122)ith(cid:12)
(cid:12)|U
ai(a)|=1(cid:9)
(cid:125),
(39)
E(Π,ρ|s)≜ (cid:88) ς ji(s)β jρ j(1−π i0)+1, (43)
≜Pd(Π,ρ|s) j∈N\i
i
where Pp(Π) is the probability that user i transmits a pilot and variance
i
without collision, i.e., |U ai(a)| = 1, and P id(Π,ρ|s) is the Var(Π,ρ|s)≜ (cid:88) ς j2 i(s)β j2ρ2 jπ i0(1−π i0). (44)
probability that the instantaneous data rate requirement is
j∈N\i
satisfied, i.e., R (a,ρ|s) ≥ Rth, when user i is non-collided.
i i
Theprobabilityofsuccessfuldatatransmissionisthenapprox-
The non-collision probability is given by
imated as
(cid:88) (cid:89)
Pp(Π)= π (1−π ). (40) (cid:32) (cid:33)
i il jl ς (s)β ρ −E(Π,ρ|s)
l∈L j∈N\i P isuc(Π,ρ|s)=1−S ii (cid:112)i i , (45)
Var(Π,ρ|s)
Now we proceed to characterize the probability of success-
fuldatatransmissionPd(Π,ρ|s).Recallthattheinstantaneous whereS(·)isthecomplementaryCDFofthestandardnormal
i
data rate R (a,ρ|s) of a non-collided user i is given by distribution.
i
(12). The characterization of Pd(Π,ρ|s) requires us to take
i
two sources of randomness into account: the random pilot B. Learning-Based Distributed Policy Optimization
selection decisions according to the probabilities in Π, and
We have obtained a closed-form approximation to the
the small-scale channel fluctuations. It appears infeasible to
expected sum-priority in (38). However, since the objective
obtain a tractable expression for Pd(Π,ρ|s). Therefore, we
i function does not decouple across users, the optimal decision
approximate the instantaneous achievable data rate by the rate
of each user depends on the decisions of other users. It is
proxy in (14).
still intractable to find the optimal distributed policy that
Since log (1+ℓx) is an increasing function of x for ℓ>0,
2 maximizes the expected sum priority by using only the users’
the rate condition R (a,ρ|s)≥Rth is equivalent to
i i localinformation.Wethereforeconsideradeeplearning-based
1 ℓ approach to this problem. Each user i has a deep neural
≤ω ≜ . (41)
SINR i(a,ρ|s) i 2R ith −1 network (referred to as a policy network) with parameter9
MR ZF
ρ (s)
0
σ (a), j ̸=i 1 1−
ji
ρ (s)|U (a)|+1
0 ai
(M −1)ω ρ (s)|U (a)|−1 (M −|Lact(a)|)ω ρ (s)|U (a)|−1
σ (a)
i 0 ai i 0 ai
ii
ρ (s)|U (a)|+1 ρ (s)|U (a)|+1
0 ai 0 ai
ρ (s)
0
ς (s), j ̸=i 1 1−
ji
ρ (s)+1
0
(M −1)ω ρ (s)−1 (M −L)ω ρ (s)−1
i 0 i 0
ς (s)
ii
ρ (s)+1 ρ (s)+1
0 0
TABLE I: The expressions of {σ (a)} and {ς (s)}.
ji ji
Input Processing Output in an unsupervised manner to maximize the expected sum-
priority over all possible network states. To incorporate the
x x temporal correlation, we consider training using sequences of
state transitions, and the training objective becomes
1-
x x (cid:34) (cid:35)
(cid:88)
maximize E J(Θ|s ) . (47)
tanh Θ {st}t∈T t
t∈T
To obtain an estimate of the expectation in (47), we collect
the generated state transitions in a replay buffer during each
Fig. 2: The structure of the policy network.
training epoch. We run a fixed number of training iterations
using a stochastic gradient descent (SGD)-based optimizer by
sampling a mini-batch, S, of state transitions:
θ as the policy generator. Once an observation o (the
i it
local information) is received in slot t, it is fed into the 1 (cid:88) (cid:88)
maximize J(Θ|s ). (48)
p To hl eic oy bsn ee rt vw ao tir ok nt io ng sle on ter ta it se the outputs (π θi(o it),ρ θi(o it)). Θ |S|
{st}∈St∈T
t
Centralized,offlinetrainingisperformedtoupdatetheparam-
o it =[d it,γ it,β it,ν it]T, (46) eters Θ in an unsupervised manner.
Toacceleratethetrainingprocess,weuseparametersharing,
whered istheexpirationtimeofthehead-of-linepacket,γ
it it
such that all users share the same policy network, i.e., θ =θ
isthepacketarrivalindicator,β isthe(normalized)LSFCin i
it
for all i∈N. To distinguish different agents and keep a more
dB, and ν is set to the NCPDR ξ for the log-sum-exp
it i,t−1
accuratehistoryofthedynamicsoftheenvironment,theinput
approximation in (S1) and the virtual queue backlog X for
it
to the policy network also contains the one-hot encoded agent
(S2).Afeedbackinformationm broadcastbytheBSineach
t
index and the action selected in the last time slot.
slot can also be included in the input to the policy network.
Duringexecution,adeviceonlyneedstofeeditsobservation
An example of the feedback information can be found in [6],
into the trained model in each slot to make the transmission
consistingofaternaryindicator(idle,collision,andsuccessful
decision. The execution is efficient and does not incur signifi-
transmission) for each pilot in the previous slot.
cant delays, as the neural network is lightweight with a short
The policy network consists of an input module, a pro-
inference time. The execution is also fully distributed, and no
cessing module, and an output module. The input module
interaction is needed between users.
is a feedforward layer with ReLU activation. The processing
PilotPre-Allocation: Onecriticalissueoflearninginmulti-
module is a gated recurrent unit (GRU) layer to address the
agent systems is that the global state and action spaces grow
partialobservabilityofagents[33].Theoutputmodulehastwo
exponentially with the number of agents. This “curse of di-
sub-modulesforgeneratingthepilotselectionprobabilitiesand
mensionality” can make the problem exceedingly challenging
the transmit power, respectively. Each sub-module consists
or even intractable. One remedy is to limit the interactions
of two feedforward layers with ReLU activation in the first
between different agents. In [34], for example, a networked
layer. The outputs from the pilot selection sub-module have
system was considered, where the agents are associated with
dimension L+1 and are normalized by the Softmax function
a graph and interact only with their connected agents in the
to produce π (o ). The last layer of the power allocation sub-
i i
graph. In our GFRA system, the interactions can be limited
module has a single neuron with Sigmoid activation and the
by pre-allocating a subset of pilots to a group of users and
output is scaled by ρ to generate the transmit power. The
max
letting different groups use disjoint subsets of pilots so that
pilotselectionactionisrandomlysampledusingthegenerated
users from different groups will never collide.
probabilities. The neural network is sketched in Fig. 2.
We denote the neural network parameters of all devices by
C. Relation to RL
Θ = {θ }. Since the joint policy is a function of Θ and
i
the network state s, we re-write the expected sum-priority OurproposedlearningapproachisrelatedtoRLintermsof
in (38) as J(Θ|s). The policy networks are jointly trained learning“amappingfromsituationtoactions[27]”throughthe10
interaction between agents and environment. However, there • MaximumNCPDR:Theobjectiveintheoriginalproblem
aresomekeydifferences.InRL,theagentsreceivea“reward” (P), given by max {D /Dth}. It characterizes the
i∈N i i
from the environment after taking an action. The reward is a performance of the worst-performing device (fairness).
non-differentiable scalar that does not reflect the long-term • Sum effective throughput: the sum of the effective
(cid:80)
effects of the actions. The goal of RL is to maximize the throughput of all devices, i.e., (λ −D ). It char-
i∈N i i
cumulative reward over time, which requires a sample-based acterizes the overall performance of the network.
estimation of a value function that represents the expected
sum of future rewards. The estimation of the value function A. Performance Evaluation
requires exploration by taking random actions and becomes
We first consider a system with N =12 devices and L=6
challenging when the state and action spaces are large, as in
pilots. The devices are divided into two classes based on their
ourcase.Incontrast,ourapproachhasadifferentiabletraining
heterogeneous traffic and QoS requirements:
objective, i.e., the expected sum-priority, which is an explicit
functionofthepolicyandalsoreflectsthelong-termeffectsof • Class 1: For i ∈ {1,2,3,4}, the packet arrival rate is
λ = 0.2 packets/slot. The packet drop rate threshold
the actions to some extent (through incorporating the urgency i
is Dth = 0.05 packets/slot. The data rate requirement is
levels of packets). By directly maximizing the expected sum- i
Rth =1bits/s/Hz.Eachpacketexpiresindmax =2slots.
priority for the generated state sequences in an unsupervised i i
manner, we avoid the exploration problem and the sample- • Class 2: For i ∈ {5,··· ,12}, the packet arrival rate is
λ = 0.65 packets/slot. The packet drop rate threshold
based estimation of the value function, thereby achieving a i
is Dth = 0.2 packets/slot. The data rate requirement is
higher sample efficiency. We provide a numerical comparison i
Rth =2 bits/s/Hz. Each packet expires in dmax=5 slots.
between our approach and RL in Section V-C. i i
To benchmark the performance, we consider the following
three baseline approaches:
V. SIMULATIONS
• Baseline 1: We assume that a genie knows the num-
We evaluate the proposed approach in a single-cell system, ber of backlogged users |K |. It informs each back-
t
where the (hexagonal) cell radius is 1 km, and the BS has logged user the optimal access barring parameter p =
bar
M =100receiveantennas.Thedevicesaredroppeduniformly min{L/|K t|,1}. At the beginning of a slot, each back-
atrandominthecellwithacircularexclusionzonearoundthe logged user generates a random number p uniformly in
BS of radius 0.05 km. For each device, the actual (unnormal- [0,1]. The user transmits a randomly selected pilot if
ized)LSFCisgeneratedbyβ(cid:101)i =−140.6−36.7log 10(dist i)+ p<p
bar
and transmits the head-of-line packet using full
Υ in dB, where dist is the distance from device i to the BS power (we observed better performance than using the
i i
inkm,andΥ representstherandomvariationsinLSFC,e.g., channel inversion power control). The BS performs ZF
i
shadow fading, with distribution N(0,σ2) – this is the 3GPP combining.
sf
Urban Microcell model in [35] with a carrier frequency of • Baseline 2: We assume scheduled transmissions to avoid
2 GHz, and we set σ2 =8 dB. When generating the LSFCs, collisions. Specifically, we pre-allocate the same pilot to
sf
weuseawrap-aroundtechniquebydrawing6cellsaroundthe user i and user i + 6, for i ∈ {1,··· ,6}. The users
central cell and setting the LSFC of a user to be the largest that share the same pilot will transmit in turn – users
one among the LSFCs to all the BSs. The maximum transmit i ∈ {1,··· ,6} can transmit in even slots and the other
power is ρ = 23 dBm [36]. The noise spectral density is users transmit in odd slots if they are backlogged. The
max
−169 dBm/Hz and the system bandwidth is 180 kHz [36]. activeusersusefullpowertotransmittheirpayloaddata.
The rate penalty factor is set to ℓ = 0.25 to give a close The BS performs ZF combining.
approximation to the normal approximation in [24, Th. 55]. • Baseline 3: Instead of reusing the mutually orthogonal
For the log-sum-exp approximation in (S1), we set α = 3 pilots, another scheme is to use pre-assigned, unique but
for the fairness promoting function in (25), and the frame non-orthogonal pilots. Specifically, each user i ∈ N is
length is set to T = 20. For the virtual-queue-based approx- assigned a pilot sequence ψ of unit energy. Since state-
i
imation in (S2), we set V = 1000 and z = 100 in (36). of-the-artactivitydetectionalgorithmsfornon-orthogonal
max
The size of all hidden layers in the neural network is set to pilots have shown remarkable performance [19], we
64. The training is performed using RMSprop with learning assume that all active users can be correctly detected.
√
rate 5 × 10−4, with smoothing constant 0.99, and without DenotingbyΨ≜[ψ 1,··· ,ψ N]andΨ(cid:101)t ≜ ρ 0(Ψa tct)∗,5
weight decay or momentum. To avoid exploding gradients, the MMSE estimate of the user channel matrix H ≜
t
we perform gradient clipping on the GRU layer and set the [h ,··· ,h ]∈CM×N is
1t Nt
ma Wxi em ru um n g 1r 0a 0d 0ie tn rt ain no inrm g et po o1 c0 h. s, each has 100 episodes with H(cid:98)a tct =Yp tΨ(cid:101) (cid:16) Ψ(cid:101)H Ψ(cid:101) +I(cid:17)−1 , (49)
T = 20 slots. The generated episodes are stored in a replay
where Yp = [y ,··· ,y ]T. Notice that, unlike the
buffer of size 5000. Each training epoch is followed by 100 t 1t Mt
case of orthogonal pilots in (6), the channel estimates do
trainingstepsperformedonamini-batchof|S|=32episodes
notdecoupleacrossusersandbecomelinearlydependent.
randomly sampled from the replay buffer. After training, we
run 200 testing epochs.
5Analogous to Definition 1, the superscript (·)act is used to represent the
We consider the following two performance metrics: elementscorrespondingtoactiveusersUt.11
Log-Sum-Exp Virtual-Queue Log-Sum-Exp Virtual-Queue
with Feedback with Feedback without Feedback without Feedback
3 3 3 3
2 2 2 2
1 1 1 1
0 200 400 600 800 1,000 0 200 400 600 800 1,000 0 200 400 600 800 1,000 0 200 400 600 800 1,000
5 5 5 5
4 4 4 4
3 3 3 3
2 2 2 2
0 200 400 600 800 1,000 0 200 400 600 800 1,000 0 200 400 600 800 1,000 0 200 400 600 800 1,000
MR ZF Pilot Selection Pre-Allocation
Baseline 1 Baseline 2 Baseline 3
Fig. 3: Performance comparison (averaged over 8 independent trials and over every 10 epochs).
We can perform MR and ZF combining by using the The learned policies in Fig. 4 with pilot pre-allocation are
combining matrix visualized in Fig. 5. Our purpose is to see how the user status
 act will affect the policy outputs (the access probability and the
Va tct ≜

H H(cid:98) (cid:98)t a tct, (cid:16) (H(cid:98)a tct )HH(cid:98)a tct(cid:17)−1
,
ZM FR . (50) t dr ua rn is nm git thp eow tee sr t) i. ngTo ed po oct hh sis, anw de cc ao ll cl uec lat teall thth ee ap vo el ri ac gy eo vu atp luu ets
s
for each given priority level and LSFC (which are uniformly
We apply the ZF combining by default. However, the
quantized in dB) and plot them as heat maps. As shown in
ZF combining does not work when |U | > L, since
t Fig. 5a, a user has higher access probability when its priority
act
the columns of H(cid:98) t become linearly dependent so that level is high, and becomes more conservative for low priority
act act
(H(cid:98) )HH(cid:98) becomes singular. In this case, we can only levels. The LSFC also has impact on the access probability.
t t
use MR combining. We use the same access barring In Fig. 5b, we can observe that users use larger transmit
scheme as in Baseline 1. powerwhentheLSFCissmall(consistentwithmostofpower
We consider the same feedback message as in [6], which control schemes), and extreme priority levels will also affect
containsaternaryindicator(successfultransmission,collision, the transmit power. Notice that this visualization shows only
and idle) for each pilot. We also consider the pilot pre- theimpactonaverage.Thelearnedpolicycouldbemuchmore
allocationwiththesameallocationpatternasinBaseline2but complicated due to the temporal correlation.
withoutscheduling.Thepilotselectionreducestoon-offdeci-
sionswhenusingpre-allocation.Theperformanceachievedby
B. Does our learning framework scale?
different schemes during different training epochs is summa-
rized in Fig. 3. We make the following observations. Both Scalabilityisalwaysacriticalaspectofmulti-agentlearning
the log-sum-exp and the virtual-queue approximations can frameworks. When complicated competition and cooperation
provide fairness among users, while the former works slightly existamongagents,theframeworksusuallydonotscalewell.
better. Using pilot status as feedback information accelerates The pilot collision represents a very strong interaction, and
theconvergenceandimprovesthefinalperformance.Pilotpre- it is difficult to train for a system with hundreds or thou-
allocation accelerates the training with a slight performance sands of users. Our framework, although more efficient than
loss. Compared with MR, ZF combing achieves significantly conventional RL in our particular scenario, also suffers from
betterperformancebyreducingtheinterferencepower,andthe performancelossduetothelimitedscalability.Oneremedyis
lossofspatialdegreesoffreedomisnegligibleduetothelarge to limit the interactions among agents. As a showcase, we
numberofantennas.InFig.4,weplotthepacketdropratesof consider a system with L = 6 pilots, and the number of
each user during training with or without pilot pre-allocation, users,N,variesfrom12to24.ThepacketarrivalrateisL/N
using the log-sum-exp approximation and ZF combining. packet/slot, the drop rate threshold is 1.2/N packets/slot, and
RDPCN
mumixaM
tuphguorhT
evitceffE12
Pilot Selection Pre-Allocation
2 2
1.5 1.5
1 1
0.5 0.5
0 0
0 200 400 600 800 1,000 1,200 0 200 400 600 800 1,000 1,200
Class 1 User Class 2 User Requirement
Fig.4:Normalizedpacketdroprateperuser(singletrial,averagedover10epochs).TherequirementlinerepresentsD /Dth =1.
i i
1.0
0.9
0.8
0.8
0.7
0.6
0.6
0.5 0.4
0.4
0.2
0.3
0.2
LSFC: small (left) to large (right) LSFC: small (left) to large (right)
(a) pilot access (b) power control
Fig. 5: Visualization of the learned policy.
PilotSelection Pre-Allocation the rate requirement is 1.5 bit/s/Hz, for all users. We consider
6 6 two schemes: 1) each user can select any of the pilots, and
2) the users are divided into two groups each with half of the
5 5 users,andeachgroupispre-allocated3pilots.Wefixthenum-
ber of training epochs to 1000 and evaluate the performance
4 4
byaveragingover200testingepochs.Weusethelog-sum-exp
approximation,ZFprocessing,andthefeedbackmessage.The
results are shown in Fig. 6. We observe that, by limiting the
3 3
numberoftrainingresources,thesecondschemescalesbetter.
Our learning framework is more suitable for a small number
2 2
ofhigh-priorityuserswithstringentperformancerequirements,
whileothersolutions(e.g.,cluster-basedscheduling)andmore
1 1 scalable approaches are necessary for large-scale systems.
0 0
1122 1144 1166 1188 2200 2222 2244 C. Comparison with RL
NumberofUsers
We compare the proposed learning scheme with VDN [37]
Fig. 6: Scalability results. and QMIX [38], two standard benchmarks for cooperative
MARL with team reward. Since VDN and QMIX do not
natively support hybrid policies, we ignore the data transmis-
sion part and consider a collision model – the transmission is
RDPCNmumixaM
RDPCN
)pot(
hgih
ot
)mottob(
wol
:leveL
ytiroirP
tuphguorhTevitceffE
)pot(
hgih
ot
)mottob(
wol
:leveL
ytiroirP13
N =2,L=1 N =2,L=1 N =12,L=6 N =12,L=6
Fully Loaded Random Traffic Fully Loaded Random Traffic
2 2
4 4
1.5 1.5
2
2
1 1
0
0 50 100 150 200 0 200 400 600 800 1,000 0 200 400 600 800 1,000 0 200 400 600 800 1,000
1 1 6
4
4
0.5 0.5
2
2
0 0 0 0
0 50 100 150 200 0 200 400 600 800 1,000 0 200 400 600 800 1,000 0 200 400 600 800 1,000
Proposed QMIX VDN γ =0 γ =0.99
Fig. 7: Comparison with VDN and QMIX. (Averaged over 4 independent trials and over every 10 epochs.)
successfulaslongastheselectedpilotisnotoccupiedbyother the adopted greedy scheme for this problem, we consider two
users.Weconsidertwodifferentsystemsizes,(N =2,L=1) discount factors, γ = 0 and γ = 0.99, for VDN and QMIX.
and (N =12,L=6), and two traffic models: When γ = 0, the agents only need to estimate the expected
immediate reward function and select the actions to greedily
• Fully-loaded: Each user generates a packet in each slot,
maximize it when making a decision. When γ = 0.99, the
i.e.,λ =1packet/slotforalli∈N.Thepacketdroprate
i
threshold is Dth = 0.5 packets/slot. Each packet expires agentsconsiderthelong-termreturn,andtheyneedtoestimate
i
immediately after the current slot, i.e., dmax =1. It is a the discounted sum of future rewards, which requires more
i
exploration. For exploration in VDN and QMIX, we adopt an
simplecase,wheretheusersalwayshavepacketsandthey
ϵ-greedy policy, where the users select random actions with
only need to learn to cooperate in a static environment
probability ϵ when generating episodes for training. (During
to avoid collisions and achieve fairness by giving up half
testing after each epoch, the users always select the action
of the transmission opportunities.
withthehighestestimatedvalue.)Similarto[38],weannealϵ
• Random Traffic: Each user randomly generates a packet
linearlyfrom1to0.05duringtraining.Basedonthescenario,
with probability λ = 0.5 in each slot. The packet drop
i
rate threshold is Dth = 0.1 packets/slot. Each packet we set the annealing time to 10 epochs for (N =2,L=1) in
i
expires in dmax =5 slots. Compared to the fully-loaded the fully-loaded system, 100 epochs for (N =2,L=1) with
i
random traffic, and 500 epochs when (N =12,L=6).
system, the users also need to learn to predict and adapt
totheenvironmentchanges(thequeuestatus)andsatisfy The performance comparison is shown in Fig. 7. When the
the delay constraints. system is small (first two columns in Fig. 7), all algorithms
WeimplementVDNandQMIXbasedonthecodeavailable (except QMIX with γ = 0.99) can efficiently learn a coop-
erative policy to avoid collisions and achieve good fairness
at https://github.com/oxwhirl/pymarl. The only difference in
between the two users. When the system becomes larger but
thenetworkstructureisthatwereplacetheoutputlayerinthe
remains relatively static (third column in Fig. 7), VDN and
agentnetwork,whichisasinglefeedforwardlayerwithlinear
activation in the original implementation, by two feedforward QMIX can still learn a cooperative policy with γ =0, but the
proposed approach can learn much more efficiently. However,
layers with a ReLU activation function for the first layer. The
other learning parameters are set to be the same as with the VDN and QMIX with γ = 0.99 fail to learn a useful policy.
In the most challenging scenario where the system is large
proposed approach, which also match the default settings in
and has highly dynamic traffic (last column in Fig. 7), the
the original implementation. After all active users select their
proposed approach can still learn efficiently, while VDN and
transmission actions, we use the obtained objective value in
(S1) as the team reward for VDN and QMIX. The parameter QMIX struggle. When γ = 0, the performance of VDN and
QMIX still slowly improves after 1000 training epochs, but it
of the log-sum-exp approximation is set to α = 15 in the
may take much longer to converge.
fully-loaded system and α=3 for random traffic.
Toinvestigatethetrade-offbetweenlong-termplanningand There are some interesting observations from the compari-
RDPCN
mumixaM
tuphguorhT
evitceffE14
son that we would like to highlight: of future states as in full RL, and the agents share the same
1) Long-term planning v.s. greedy scheme: In our devel- observationofthecontext.Thisisconceptuallydifferentfrom
opment of (S1) and (S2), we choose to greedily maximize our considered scenario, where the transmission decision will
the immediate objective function when making each decision. affect the next state (i.e., the queue backlogs and the urgency
Long-term planning is usually preferred in RL, as greedily levels of the remaining packets), and the users do not share
maximizing the immediate reward may prevent the agents to the same observation.
select better actions in the future. However, the design of our
objective function is quite different from the conventional RL
VI. CONCLUSION
reward function – we have already incorporated the urgency
level of packets, which is the most critical factor for future In this work, we provide a cross-layer GFRA model with
planning. For our particular problem, we do not see what MIMO and dynamic traffic. We formulate a fairness-based
other factors may have significant effects in the long run, stochasticnetworkoptimizationproblemanddeveloptworeal-
as future packet arrivals are independent of the current state time approximations to this stochastic problem. These ap-
and decisions. Sending packets that are most urgent while proximations give a unified measure of instantaneous fairness
prioritizing fairness also does not seem to prevent the users among users. We develop a distributed policy that seamlessly
from selecting better alternatives in the future. In this sense, combines discrete pilot selection decisions and continuous
our design of the objective function is more analogous to the powercontrolvariablestomaximizeuserfairnessandnetwork
valuefunctioninRLinsteadoftheimmediaterewardfunction, performance. In contrast to conventional sample/exploration-
and there is no need for additional long-term planning when basedRLapproaches,ourtrainingobjective(expectedreward)
implementing the RL algorithms. In the simulation results is differentiable with respect to the policy parameters and
in Fig. 7, we also observe that choosing the greedy scheme thus allows more efficient training. Our work suggests that
(γ = 0) works better than long-term planning (γ = 0.99) in one can achieve considerable performance improvements by
all considered scenarios. incorporating domain knowledge and model structure into the
2) Exploration v.s. guided learning: In conventional RL, learning design.
exploration is essential to find good actions to be reinforced.
Specifically, the agents need to take random actions to obtain
APPENDIX
a good estimate of the value function at the beginning of
the training. In small-scale systems, the chance to randomly Since we consider only a single slot here, we omit the time
take a good joint action is high, and the exploration can indices for brevity. When using ZF, for a non-collided user i,
be effective. However, as the system becomes larger, the we have vHg =1, and vHg =0 when j ̸=i. This gives
exploration becomes more challenging, especially when the
i (cid:98)ai i (cid:98)aj
system is dynamic. In contrast, our model-based approach is (cid:20) 1 (cid:21) (cid:34) (cid:104) (cid:12) (cid:105) 1
more efficient due to the closed-form, differentiable training E
SINR
=E E |vH ig (cid:101)ai|2(cid:12) (cid:12)G(cid:98) +
β ρ
∥v i∥2
i i i
objective, which enables us to directly optimize the policy
(cid:35) (51)
w ofit th ho eu pt roth pe osn ee ded apf po rr ot ar cy hin ag gara inn sd to cm ona vc et nio tin os n. aT lRhe Le if sfe vc et ri iv fie en des ins
+
(cid:88) β βjρ ρjE(cid:104)
|vH ih
j|2(cid:12)
(cid:12)
(cid:12)G(cid:98)(cid:105)
.
i i
the simulation results in Fig. 7. Our approach also seamlessly j∈U\i
integrates discrete pilot selection decisions and continuous Noticethatv becomesaconstantvectorwhenconditionedon
i
power control with data rate requirements, which, to the best G(cid:98). To evaluate the first conditional expectation, we note that
of Ro eu mr ak rn kow 3l :ed Wge e, h ca os nsn io stt enb te le yn od bo sn ee rvb eef to hr ae t.
the training of
g
p(cid:101) ia li
oti ss ,aa nlw da ty hs erein fod re ep ,e vnd He gnt o ∼f CG(cid:98) Nre (cid:0)g 0a ,r (d 1le −ss cof )∥th ve ∥e 2m
(cid:1)
;p hlo ey ne cd
e
QMIX with γ = 0.99 is unstable and does not converge in
i (cid:101)ai ai i
(cid:104) (cid:12) (cid:105)
o wu ir thsi fm ulu ll ya -t li oo an ds e. dEv tre an ffiin c,th ite fis ri sm tp file ns dt sc aase gof oo dr( pN oli= cy2 b,L ut= the1 n) E |vH ig (cid:101)ai|2(cid:12) (cid:12)G(cid:98) =(1−c ai)∥v i∥2. (52)
diverges as the training progresses. We have tried different By using (6), we have
learning rates (from 10−3 to 10−5) and different structures of
(cid:112)
the mixing network, but the problem persists. We suspect that
g =
ρ 0|U aj| 1 (cid:88)
h +
ρ 0|U aj|
w
this is due to the unnecessity of extra long-term planning in (cid:98)aj 1+ρ |U |(cid:112) |U | k 1+ρ |U |
our problem, as discussed above, and because the additional
0 aj aj k∈Uaj 0 aj
(53)
expressibility of the mixing network may result in a compro- = c aj (cid:88) h +(cid:113) c (1−c )w,
mised factorization of the joint value function. As the chosen
(cid:112)
|U aj|
k∈Uaj
k aj aj
rewardfunction(objectivesin(S1))isalreadyintheformofa
sum of individual contributions, it is more suitable for VDN, where w ∼ CN (0,I) and {h j} are mutually independent.
where the factorization is forced to be a sum. This tells that E(cid:104) h gH (cid:105) = √caj I. Since h and g are
Remark 4: Another approach that considers only the imme-
j(cid:98)aj
|Uaj|
j (cid:98)aj
jointly Gaussian, we know from [39, Theorem 10.2] that
diaterewardforagivensituationiscontextualbanditlearning √1 g is the MMSE estimate of h given g . By the
(CBL),whichisaspecialcaseoffullRL[27,Ch.2].InCBL,
|Uaj|(cid:98)aj j (cid:98)aj
takinganactionwillonlyaffecttheimmediatereward,instead orthogonality principle, we can write h j = √ |U1 aj|g (cid:98)aj +z j,15
where z j has distribution CN
(cid:16) 0,(cid:16)
1−
|Uca
aj
j|(cid:17) I(cid:17)
and is in- [15] pL r. oB toa ci o, lJ. foL riu r, anQ d. omYu, acJ c. eC ssho ii n, a mnd assW iv. eZ Mha In Mg, O“ ,”A Ic Eo Ell Eisi Jo .n Sr ee ls .ol Au rt eio an
s
dependent of G(cid:98). The second conditional expectation is then Commun.,vol.39,no.3,pp.686–699,Mar.2020.
evaluated as [16] L. Liu, E. G. Larsson, W. Yu, P. Popovski, C. Stefanovic, and E. de
(cid:104) (cid:12) (cid:105) (cid:18) c (cid:19) Carvalho,“Sparsesignalprocessingforgrant-freemassiveconnectivity:
E |vH ih j|2(cid:12) (cid:12)G(cid:98) = 1− |Uaj
|
∥v i∥2. (54) A Thif nu gt su ,r ”e IEp Ear Ead Sig igm naf lo Pr rora cn ed sso .m Maac gc .,es vs ol.pr 3o 5to ,c no ols
.
5i ,n pt ph .e 88In –t 9e 9r ,ne At uo gf
.
aj
2018.
By substituting (52) and (54) into (51), we obtain [17] J.Ding,D.Qu,andJ.Choi,“Analysisofnon-orthogonalsequencesfor
E(cid:20) SIN1
R
(cid:21) =E(cid:2) β∥v ρi∥2(cid:3)(cid:32)
(1−c ai)β iρ i+1 [18]
g n Ar o .a . Fn 1t e- , nf pr ge p le e. r,1R S5A 0 .– Hw 1 a6i gt 0h h, igJm haa n as t. ss 2 hiv 0 oe 1 ar9M ,. PI .M JuO n, g” ,I aE nE dE G.T Cra an irs e. ,C “Nom onm -Bun a. y, ev so ial n. 6 ac8 -,
i i i tivitydetection,large-scalefadingcoefficientestimation,andunsourced
(cid:18) (cid:19) (cid:33) (55) randomaccesswithamassiveMIMOreceiver,”IEEETrans.Inf.Theory,
+
(cid:88)
1−
c aj
β ρ .
vol.67,no.5,pp.2925–2951,May2021.
|U | j j [19] A. Fengler, O. Musa, P. Jung, and G. Caire, “Pilot-based unsourced
j∈U\i
aj
randomaccesswithamassiveMIMOreceiver,interferencecancellation,
and power control,” IEEE J. Sel. Areas Commun., vol. 40, no. 5, pp.
The final step is to evaluate
E(cid:2)
∥v
i∥2(cid:3)
, which is given by 1522–1534,May2022.
[20] K. T. Truong and R. W. Heath, “Effects of channel aging in massive
E(cid:2)
∥v
∥2(cid:3) =c−1(cid:104) E(cid:104) (QHQ)−1(cid:105)(cid:105)
=
1 MIMO systems,” J. Commun. and Netw., vol. 15, no. 4, pp. 338–351,
i ai i,i c ai(M −|Lact|)
[21]
A 3Gug P. P,2 “0 S1 t3 u.
dyonchannelmodelforfrequenciesfrom0.5to100GHz,”
where Q is a M ×|Lact| matrix with independent CN (0,1) 3rd Generation Partnership Project (3GPP), Technical Report (TR)
entries,andthesecondequalityfollowsimmediatelyfrom[22, 36.901,Jun.2020,version16.1.0.
[22] T.L.Marzetta,E.G.Larsson,H.Yang,andH.Q.Ngo,Fundamentals
Appendix B].
ofMassiveMIMO. CambridgeUniversityPress,2016.
[23] J. Ding, D. Qu, M. Feng, J. Choi, and T. Jiang, “Dynamic preamble-
REFERENCES resourcepartitioningforcriticalMTCinmassiveMIMOsystems,”IEEE
InternetThingsJ.,vol.8,no.20,pp.15361–15371,Oct.2021.
[1] J.Bai,Z.Chen,andE.G.Larsson,“Multi-agentpolicyoptimizationfor [24] Y. Polyanskiy, H. V. Poor, and S. Verdu´, “Channel coding rate in the
pilotselectionindelay-constrainedgrant-freemultipleaccess,”inProc. finiteblocklengthregime,”IEEETrans.Inf.Theory,vol.56,no.5,pp.
AsilomarConf.Signals,Syst.,Comput.,2021,pp.1477–1481. 2307–2359,2010.
[2] J.Sachs,G.Wikstrom,T.Dudda,R.Baldemair,andK.Kittichokechai, [25] X.QiuandK.Chawla,“Ontheperformanceofadaptivemodulationin
“5Gradionetworkdesignforultra-reliablelow-latencycommunication,” cellular systems,” IEEE Trans. Commun., vol. 47, no. 6, pp. 884–895,
IEEENetw.,vol.32,no.2,pp.24–31,Mar./Apr.2018. Jun.1999.
[3] 3GPP,“Evolveduniversalterrestrialradioaccess(E-UTRA)andevolved [26] A. Garcia-Armada, “SNR gap approximation for M-PSK-based bit
universalterrestrialradioaccessnetwork(E-UTRAN);Overalldescrip- loading,”IEEETrans.WirelessCommun.,vol.5,no.1,pp.57–60,Feb.
tion,” 3rd Generation Partnership Project (3GPP), Technical Specifica- 2006.
tion(TS)36.300,June2021,version16.6.0. [27] R.S.SuttonandA.G.Barto,ReinforcementLearning:AnIntroduction.
[4] C. She, C. Sun, Z. Gu, Y. Li, C. Yang, H. V. Poor, and B. Vucetic, MITpress,2018.
“A tutorial on ultrareliable and low-latency communications in 6G: [28] M.Chen,S.C.Liew,Z.Shao,andC.Kai,“Markovapproximationfor
Integratingdomainknowledgeintodeeplearning,”Proc.theIEEE,vol. combinatorialnetworkoptimization,”IEEETrans.Inf.Theory,vol.59,
109,no.3,pp.204–246,Mar.2021. no.10,pp.6301–6327,Oct.2013.
[5] C. Sun, C. She, and C. Yang, Unsupervised Deep Learning for Opti- [29] S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge
mizing Wireless Systems with Instantaneous and Statistic Constraints. universitypress,2004.
John Wiley & Sons, 2023, ch. 4, pp. 85–117. [Online]. Available: [30] E. Fountoulakis, N. Pappas, and A. Ephremides, “Dynamic power
https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119818366.ch4 controlfortime-criticalnetworkingwithheterogeneoustraffic,”ITUJ.
[6] R.Huang,V.W.Wong,andR.Schober,“Throughputoptimizationfor FutureandEvolvingTechnol.,vol.1,no.2,Dec.2021.
grant-freemultipleaccesswithmultiagentdeepreinforcementlearning,” [31] M. J. Neely, “Stochastic network optimization with application to
IEEETrans.WirelessCommun.,vol.20,no.1,pp.228–242,Jan.2021. communication and queueing systems,” Synthesis Lectures Commun.
[7] Z.Jiang,A.Marinescu,L.A.DaSilva,S.Zhou,andZ.Niu,“Scalable Netw.,vol.3,no.1,pp.1–211,2010.
multi-agent learning for situationally-aware multiple-access and grant- [32] W. Tang and F. Tang, “The Poisson Binomial Distribution — Old &
free transmissions,” in Proc. IEEE Int. Workshop Signal Process. Ad- New,”StatisticalScience,vol.38,no.1,pp.108–119,2023.[Online].
vancesWirelessCommun.(SPAWC),2019. Available:https://doi.org/10.1214/22-STS852
[8] J.Zhang,X.Tao,H.Wu,N.Zhang,andX.Zhang,“Deepreinforcement [33] M. Hausknecht and P. Stone, “Deep recurrent Q-learning for partially
learning for throughput improvement of the uplink grant-free NOMA observableMDPs,”inAAAIFallSymp.Series,2015,pp.29–37.
system,” IEEE Internet Things J., vol. 7, no. 7, pp. 6369–6379, Feb. [34] G.Qu,Y.Lin,A.Wierman,andN.Li,“Scalablemulti-agentreinforce-
2020. ment learning for networked systems with average reward,” in Proc.
[9] L. Deng, D. Wu, Z. Liu, Y. Zhang, and Y. S. Han, “Reinforcement NeuralInf.Process.Syst.(NeurIPS),vol.33,2020,pp.2074–2086.
learningforimprovedrandomaccessindelay-constrainedheterogeneous [35] 3GPP, “Further advancements for E-UTRA physical layer aspects
wirelessnetworks,”arXivpreprintarXiv:2205.02057,2022. (release 9), document,” 3rd Generation Partnership Project (3GPP),
[10] T. T. Le, Y. Ji, and J. C. Lui, “TinyQMIX: Distributed access control TechnicalSpecification(TS)36.814,Mar.2017.
formMTCviamulti-agentreinforcementlearning,”inProc.IEEEVeh. [36] ——, “Evolved universal terrestrial radio access (E-UTRA); radio
Technol.Conf.(VTC),2022,pp.1–6. resourcecontrol(RRC);protocolspecification,”3rdGenerationPartner-
[11] T.Wang,X.Bao,I.Clavera,J.Hoang,Y.Wen,E.Langlois,S.Zhang, shipProject(3GPP),TechnicalSpecification(TS)36.331,Oct.2017.
G.Zhang,P.Abbeel,andJ.Ba,“Benchmarkingmodel-basedreinforce- [37] P.Sunehagetal.,“Value-decompositionnetworksforcooperativemulti-
mentlearning,”arXivpreprintarXiv:1907.02057,2019. agentlearningbasedonteamreward,”inProc.Int.Conf.Auton.Agents
[12] H. Jiang, D. Qu, J. Ding, and T. Jiang, “Multiple preambles for high MultiAgentSyst.(AAMAS),2018,p.2085–2087.
success rate of grant-free random access with massive MIMO,” IEEE [38] T.Rashid,M.Samvelyan,C.S.DeWitt,G.Farquhar,J.Foerster,and
Trans.WirelessCommun.,vol.18,no.10,pp.4779–4789,Oct.2019. S. Whiteson, “Monotonic value function factorisation for deep multi-
[13] B. Singh, O. Tirkkonen, Z. Li, and M. A. Uusitalo, “Contention- agent reinforcement learning,” J. Mach. Learn. Res., vol. 21, no. 178,
basedaccessforultra-reliablelowlatencyuplinktransmissions,”IEEE pp.1–51,2020.
WirelessCommun.Lett.,vol.7,no.2,pp.182–185,Apr.2018. [39] S. M. Kay, Fundamentals of Statistical Signal Processing: Estimation
[14] J. Choi, “An approach to preamble collision reduction in grant-free Theory. Prentice-Hall,Inc.,1993.
random access with massive MIMO,” IEEE Trans. Wireless Commun.,
vol.20,no.3,pp.1557–1566,Mar.2020.