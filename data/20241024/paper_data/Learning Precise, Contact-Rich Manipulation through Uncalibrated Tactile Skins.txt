Learning Precise, Contact-Rich Manipulation through
Uncalibrated Tactile Skins
Venkatesh Pattabiraman1,∗ Yifeng Cao2 Siddhant Haldar1 Lerrel Pinto1 Raunaq Bhirangi1,3,∗,†
1 New York University 2 Columbia University 3 Carnegie Mellon University
* equal contribution
https://visuoskin.github.io/
Abstract—While visuomotor policy learning has advanced
robotic manipulation, precisely executing contact-rich tasks
remainschallengingduetothelimitationsofvisioninreasoning
about physical interactions. To address this, recent work has
soughttointegratetactilesensingintopolicylearning.However,
many existing approaches rely on optical tactile sensors that
are either restricted to recognition tasks or require complex
dimensionality reduction steps for policy learning. In this
work, we explore learning policies with magnetic skin sensors,
which are inherently low-dimensional, highly sensitive, and
inexpensive to integrate with robotic platforms. To leverage
these sensors effectively, we present the Visuo-Skin (VISK)
framework, a simple approach that uses a transformer-based
policyandtreatsskinsensordataasadditionaltokensalongside
visualinformation.Evaluatedonfourcomplexreal-worldtasks
involvingcreditcardswiping,pluginsertion,USBinsertion,and
bookshelfretrieval,VISKsignificantlyoutperformsbothvision-
onlyandopticaltactilesensingbasedpolicies.Furtheranalysis
reveals that combining tactile and visual modalities enhances
Card Swiping USB Insertion Book Retrieval
policy performance and spatial generalization, achieving an
average improvement of 27.5% across tasks. Fig. 1: VISK uses AnySkin with a simple transformer-based
I. INTRODUCTION architecture to solve precise, contact-rich tasks.
Humanseffortlesslyperformprecisemanipulationtasksin
their everyday lives, such as plugging in charger cords, or
severalimpressiveworksinareaslike3Dreconstructionand
swipingcreditcards–activitiesthatdemandexactalignment
localization [5, 6] and object recognition [7, 8]. However,
and involve constrained motion. These tasks are so com-
the high dimensionality of tactile data from such sensors
monplace that we often overlook the complexity involved
introduces additional complexity to the already challenging
in executing them with the necessary accuracy. In contrast,
problem of policy learning. In most cases, the use of optical
muchoftheexistingrobotlearningliteratureremainsfocused
sensors necessitates dimensionality reduction through rep-
on simple, low-precision primitives such as pick-and-place,
resentation learning [4], explicit state estimation [9, 10] or
slide, push-pull, and lift that does not require such fine-
discretization[11,12]tomakeitamenabletopolicylearning.
grained spatial accuracy. As we strive to create robots
This observation prompts an investigation into using alter-
capable of everyday tasks like handling cables and opening
native tactile sensing modalities that naturally offer lower-
jars, it is crucial to develop frameworks that enable precise,
dimensional representations while still effectively capturing
contact-rich manipulation.
the essential characteristics of physical contact.
While the role of tactile feedback for robust execution
In this work, we present Visuo-Skin (VISK), a simple
of precise skills in humans is widely acknowledged [1, 2],
framework for training precise robot policies using skin-
analogouscapabilitiesinroboticpolicieshavelaggedbehind
based tactile sensing. VISK uses a simple visuotactile
their vision-based counterparts. A variety of tactile sensors
policy architecture that incorporates tactile signals from
have been developed to bridge this gap in robotics, with
AnySkin [13], an affordable magnetic tactile sensor demon-
optical tactile sensors like Gelsight [3] and DIGIT [4]
stratedtoprovidespatiallycontinuous,low-dimensional(15-
becoming popular choices in robot learning due to their
dimensional) sensing while being replaceable, making it
high resolution. This increased resolution has facilitated
well-suitedforpolicylearningapplications.TheVISKpolicy
† Correspondenceto:raunaqbhirangi@nyu.edu builds upon the BAKU [14] architecture, which enables
4202
tcO
22
]OR.sc[
1v64271.0142:viXra
ezinekoT
noisiV
nikS
nikS-ousiVpolicy learning across multiple camera views and tasks. of desirable properties such as their ease of replaceability
Through VISK, we demonstrate that simply incorporating and compatibility with well-understood neural architectures
a tactile token obtained from a tactile encoder into state-of- like convolutional neural networks [7]. Similarly, magnetic
the-art visual policy learning architectures enables effective tactile sensors like Xela [21] and ReSkin [22] have garnered
visuotactile policy learning for precise real-world manipula- significant interest due to their scalable form factor, low
tion tasks that require visual as well as tactile inputs for lo- dimensionality and ability to sense shear force in addition
calization. Furthermore, using a low-dimensional sensor like to their consistency across sensor instances [22, 23]. In
AnySkin allows policies to be learned end-to-end without light of these characteristics, the VISK framework presented
requiringanytask-specificpreprocessing[9,10]ofthetactile in this work uses AnySkin [13] a magnetic tactile sensor
input or pretraining [4, 12]. To the best of our knowledge, that strikes the right balance between low dimensionality
this work presents the first visuotactile framework enabling and continuous contact sensing. Furthermore, its superior
robots to perform precise contact-rich manipulation skills cross-instance signal consistency makes it more amenable
with policies that generalize across spatial variations while than optical sensors to policy learning without the need for
requiring a small number of robot demonstrations (<200). complexadditionalfabricationtopreventwearandtear[12].
To demonstrate the effectiveness of VISK, we run exten-
sive experiments on four precise manipulation tasks using a B. Visuotactile learning
real-world xArm robot - plug insertion, credit card swiping,
The meteoric rise of deep learning has paralleled recent
USB insertion, and bookshelf retrieval. Our main findings
developments in rapid prototyping and additive manufac-
are summarized below:
turing. As a result, a number of recent works have inves-
1) Policies trained with VISK using skin-based tactile sens-
tigated the use of machine learning for a host of tactile
ing exhibit an overall 27.5% absolute improvement in
prediction tasks such as slip detection [24, 25], material
performance compared to vision-only models across 4
classification [26, 27], object identification [28, 29] and 3D
precise manipulation tasks (Section IV-C).
reconstruction [30, 31] across a range of tactile sensors.
2) Through an ablation analysis, we study the impact of
In this paper, we specifically focus on policy learning –
different modalities on policy learning, particularly the
incorporating tactile information into robotic policies to
difference between visual and visuotactile policies for
enhance contact-rich manipulation.
precise manipulation (Section IV-D).
Recent works have demonstrated impressive improve-
3) Policies trained with the AnySkin tactile sensor [13]
ments from incorporating tactile data into the policy learn-
outperform those using optical tactile sensors such as
ing framework for precise dexterity [32, 33] and bimanual
DIGIT[4]byatleast43%ontworeal-worldtasks,high-
manipulation [34]. However, the high dimensional nature of
lightingthebenefitsofskin-basedsensorsforvisuotactile
dexterous control limits the task complexity and extent of
policy learning (Section IV-E).
generalizability enabled by these works. While [11, 35] use
Allofourdatasets,codefortraining,androbotevaluation
sim2real learning to demonstrate significant generalizability
will be made publicly available. Robot videos are best
across objects for an in-hand rotation task, the task lacks
viewed at https://visuoskin.github.io/.
precision, and sim2real transfer necessitates significant di-
II. RELATEDWORK lution of the tactile input to only capture coarse, discrete
information. This limits the scalability of this approach to
A. Tactile sensing in Robotics
the precise, contact-rich tasks considered in this work.
Most robotic tasks involve physical interaction with the
Yet other works rely on explicit pose estimation [36] and
environment.Tactilesensingiscriticalinitsabilitytoenable
handcrafted feature extraction [9, 10] from optical tactile
robots to reason about the physics of contact directly at
data for alignment when performing insertion tasks. While
the point of contact. Over the years, a number of diverse
interesting, these techniques do not generalize to arbitrary
transduction mechanisms have been explored for tactile
tasks and require significant effort and domain knowledge
sensing.Resistivetactilesensors[15,16,17]areinexpensive
to adapt to every new task. While some existing works
and relatively easy to fabricate, and provide discrete sensing
have learned visuotactile policies for precise tasks such as
making them well-suited for a range of applications that
insertion [37, 38], all of these works evaluate performance
involvesensingthepresenceorabsenceofcontact.Capacita-
in restricted settings with little to no spatial variation in the
tivetactilesensors[18,19]tendtoprovidemorefine-grained
location of the insertion slot. In this paper, we investigate
measurements compared to resistive sensors and include
visuotactile policy learning for contact-rich, high-precision
proximity sensing in addtion to tactile sensing. Another
tasks requiring spatial generalization, and conclusively show
versatile category of sensors are MEMS-based sensors [20]
that VISK policies use tactile feedback in conjunction with
that often combine multiple sensors such as audio and IMU
vision to substantially improve task performance.
sensors and can offer multimodal feedback in addition to
higher resolution and mm-scale form factor.
III. VISUO-SKINPOLICYLEARNING(VISK)
Recently, optical tactile sensors like Gelsight [3] and
DIGIT [4] have emerged as a popular, high resolution alter- Two key considerations in designing a framework for
nativetoexistingtactilesensorsforroboticsduetoanumber visuotactile policy learning include the choice of a tactile1
Image
Encoder
2
Image
Encoder
3
3
Image
Encoder
a
1 2 t−H . .
.
a
4
t−1
a
Image t
Encoder
4
5
5 Action Head
Tactile
Encoder
Action
Token
Fig.2:(left)RobotsetupusedforexperimentsinSectionIV;(right) VISK policyarchitectureusesResNet-18[39]encoders
forcamerainputsandanMLPencoderforAnySkininput.Anactiontokenisappendedtotheencodedinputsbeforepassing
them through a transformer decoder, and the corresponding feature is used for action prediction by the action head.
sensor capable of providing reliable tactile data across di- commanded robot velocity during teleoperation. This proves
verse environments and tasks, and designing a neural archi- especially useful for increasing the diversity of contact-rich
tecture able to effectively leverage multimodal visual and signals in the dataset by rendering the tasks slightly more
tactileinformation.Ourproposedapproach,VISK,addresses challenging for the human operator. While large perturba-
these in the following ways: first, it employs AnySkin [13], tionsrisksteeringthelearnedbehaviorcloningpolicyastray,
a magnetic tactile skin shown to yield consistent tactile we find that injecting a minor directional noise yields an
measurements reliably under various conditions. Second, information-rich tactile signal while maintaining consistent
it builds upon state-of-the-art approaches to visual policy task success.
learning [14] by incorporating a tactile encoding stream,
B. Policy Architecture
allowing the network to effectively learn from multimodal
data.Below,wedescribeeachcomponentof VISK indetail.
The VISK policy builds on top of BAKU [14], a state-
of-the-art transformer-based policy learning architecture that
A. Data Collection
learns visual policies across multiple camera views. Similar
WeuseaVR-basedteleoperationframework[40]employ- to BAKU, our architecture contains three main components:
ing the Meta Quest 3 headset to collect data for our real-
world xArm robot experiments. Visual data from 4 camera a) Sensory Encoders: Visual inputs from cameras are
views, including an egocentric camera attached to the robot encoded using a modified ResNet-18 [39] visual encoder.
gripper, is recorded at 30 Hz. Tactile data for the AnySkin Low-dimensional tactile inputs from the AnySkin sensor
experiments is recorded as magnetometer signals at 100 Hz, are encoded with a two-layer multilayer perceptron (MLP).
while data from the DIGIT sensors in comparative tests are Drawing from [22], we subtract a baseline measurement
recorded at 30 Hz, identical to the cameras. Drawing from from each tactile reading to account for sensor drift. The
prior work demonstrating the benefits of adding noise to encoded representations for each modality are projected to
demonstrations for policy learning [41, 42], we add a uni- thesamedimensionalitytofacilitatecombiningmodalitiesin
formly sampled angular perturbation to the direction of the theobservationtrunk.Someoftheablationsandcomparisons
Transformer
DecoderFig. 3: Close-up views of VISK rollouts for the four tasks: Plug Insertion, Card Swiping, USB Insertion and Book Retrieval
noitresnI
gulP
noitresnI
BSU
gnipiwS
draC
laveirteR
kooBTrain region Train region 20 cm
20 cm (b
Test location Test configuration
40 cm
Train region
Test location
Fig. 4: Overhead view depicting variations in target object locations for training and evaluation for plug insertion, card
swiping and USB insertion (left to right). The enclosing light green box denotes the extent of variation in the training data.
Test locations for plug insertion and USB insertion are marked on the image. For the card swiping task, arrows denote test
locations and orientations of the card machine used for evaluation. For the book retrieval task (not depicted here), the order
ofbooksisrandomizedforeverytrainingdemonstration,andtestconfigurationsconsistoforderingsunseenintrainingdata.
presented in Section IV also use DIGIT sensors and robot RGB images at 128x128 resolution from three static third-
proprioception as inputs to the policy. In line with prior person cameras and an egocentric camera mounted on the
works using DIGIT sensors for policy learning [28, 43], gripper. The action space is the change in the end-effector
tactile images from the DIGIT sensor are encoded using the poseandgripperstate.Ourexperimentalsetupisdepictedin
same ResNet-18 encoder as the visual data. The propriocep- Figure2.Learnedpoliciesaredeployedata10Hzfrequency.
tive inputs are encoded using a two-layer MLP.
b) Observation Trunk: The encoded inputs from all B. Task Descriptions
camera views, robot proprioception, and the tactile signals
For all the analysis presented in this paper, we focus on a
aretreatedasseparateobservationtokensandpassedthrough
set of four contact-rich tasks that require high precision as
atransformerdecodernetwork[44].Alearnableactiontoken
well as spatial generalization. Each task has a target object
is appended to the list of observation tokens and is used to
that the robot must interact with, whose position is varied
obtain action features.
duringdemocollection.Allevaluationsuseafixedsetoften
c) Action Head: Finally, an action head takes as input target locations unseen in the training demonstration data.
the action features from the observation trunk and predicts
a) PlugInsertion: Thistaskrequirestherobottoinsert
the corresponding actions. We found a deterministic action
a plug into the first socket on a power strip. The arm starts
head learned using a mean squared error loss to suffice
with the plug grasped and the power strip randomly posi-
for our experiments. Considering the temporal correlation
tioned within a 20cm × 7cm grid with a fixed orientation.
in robot movements, we follow prior work [14, 45, 46]
The training dataset consists of 96 demonstrations.
and include action chunking to counteract the covariate shift
b) USB Insertion: This task has the robot plugging
often seen in the low-data imitation learning regime. During
a USB stick into a specific port on a USB hub. The arm
inference,weapplyexponentialtemporalsmoothing[45]for
starts with the USB stick grasped and the hub is positioned
producing smoother robot motions. Our full policy architec-
randomly within a 20cm × 15cm grid. The training dataset
ture is depicted in Figure 2.
consists of 98 demonstrations.
c) Card Swiping: This task involves swiping a credit
IV. EXPERIMENTS
card through a card reader. The arm starts with the credit
We study the effectiveness of the VISK framework in a cardgraspedandthecardreaderrandomlypositionedwithin
policy learning setting using behavior cloning. Our experi- a 40cm × 15cm grid, and oriented at a random angle in the
ments are designed to answer the following questions: range(−30◦,30◦)fromthedirectiontherobotisfacing.The
• How does VISK perform on precise manipulation tasks? training dataset consists of 90 demonstrations.
• How do different inputs affect performance of VISK? d) Book Retrieval: This task requires the robot to
• Does VISK’s use of AnySkin improve over DIGIT [4]? retrieve a specific book from a set of eight books placed
• Do VISK policies generalize to unseen task variations? together,withtheorderofbooksrandomizedeachtime.The
robot must first reach for the target book, pivot it about its
A. Environment Setup
edge, and then grasp and pull it out of the bookrack. The
We use a Ufactory xArm 7 robot with its standard two- training dataset consists of 172 demonstrations.
fingered gripper for all our experiments. To enable tactile For the first three tasks, where the robot starts with a
sensing, we attach AnySkin sensor tips to the left gripper grasped object, we do not enforce hard constraints on the
finger. An identically shaped, plain silicone tip is attached graspinglocationandallowsomevariabilityacrossruns.The
to the right finger. For baseline comparisons with the DIGIT extent of variation in target object configurations are shown
sensor,weuseaDIGITsensoroneitherfingertipinlinewith in Fig. 4. Evaluations are performed on a set of 10 held-out
prior work [24]. The camera inputs comprise synchronized configurations for each task.
mc
7 mc
51
mc
51TABLEI:Successrates(outof10)averagedoverthreeseedsforpoliciestrainedonfourtasks:PlugInsertion,USBInsertion,
Card Swiping and Book Retrieval. VISK policies are highlighted in grey.
InputModalities Policyperformance
TactileSensor
3rdPerson Wrist Robot
PlugInsertion USBInsertion CardSwiping BookRetrieval
Camera Cameras Proprioception
✓ ✗ ✗ 0.0±0.0 0.7±0.6 3.3±1.6 2.0±1.0
✓ ✗ ✓ 0.0±0.0 0.0±0.0 3.0±1.0 0.6±0.5
None ✓ ✓ ✗ 3.6±0.5 2.3±2.0 1.3±0.5 3.3±1.1
✓ ✓ ✓ 1.0±1.0 2.0±1.0 3.0±1.7 2.3±1.5
✓ ✗ ✗ 2.3±1.1 2.0±1.0 7.0±1.7 3.6±2.5
✓ ✗ ✓ 1.3±0.5 1.0±1.0 2.6±1.5 2.6±0.5
AnySkin(VISK) ✓ ✓ ✗ 6.6±1.5 5.6±1.5 1.0±1.0 5.3±2.0
✓ ✓ ✓ 3.6±1.5 2.0±1.0 3.0±1.7 4.6±2.0
✓ ✗ ✗ 2.3±0.5 0.0±0.0 N/A N/A
DIGIT ✓ ✓ ✗ 1.6±1.5 0.3±0.5 N/A N/A
C. Performance of VISK policies forthistaskreportedinTableI,therefore,useanewinstance
of AnySkin. The sustained performance improvement of
We evaluate the performance of VISK policies on the
VISK policies over vision-only policies even with replaced
aforementioned precise manipulation tasks in the real world.
AnySkin is consistent with prior work [13] and underscores
To account for the high variance in performance of behavior
the importance of AnySkin to the VISK framework.
cloning policies, we train policies across 3 random seeds
and conduct 10 trials per seed for a total of 30 trials per D. Effect of different input modalities on performance
evaluation. We report the aggregated success rate across From Table I, we find that while the addition of AnySkin
seeds in Table I, and find that VISK policies consistently inputs to the policy consistently improves performance, the
outperform other variations across tasks. additionofothermodalitieslikethewristcameraandpropri-
Additionally,weobservethat VISK policiesexhibitemer- oception can have significant impact on policy performance
gent seeking behavior. For instance, with the plug insertion depending on the task. A few consistent patterns emerge
and USB insertion tasks, we find that the policy first gets across tasks: (1) VISK results in a significant improvement
close to the location of the target (socket or port respec- (≥2×) in performance over the next best model, indicating
tively), makes contact, and proceeds to move around as it its effectiveness on precise, contact-rich manipulation. (2)
tries to find the target. Once it seems to have located a Adding proprioceptive input almost always results in a drop
change in contact characteristics, the policy pushes down in performance. This can be attributed to the learned policy
and inserts successfully. This behavior is strong evidence overfittingtoproprioceptiveinformationwhichisdetrimental
of VISK policies effectively leveraging tactile information to tasks requiring spatial generalizability over target object
from AnySkin. Further, it is distinctly different from the locations.(3)Withtheexceptionofthecardswipingtask,the
behavior of vision-only policies that simply attempt to push additionofawristcameraimprovespolicyperformance.The
downwards once they get close to the insertion location wrist camera gives the policy a local visual understanding
regardlessofalignmentwiththetarget.Weseeananalogous of the scene in the frame of the gripper, and in turn, the
trend with the card swiping task, where the VISK policy same frame as the robot’s action space. This is especially
slowsdownasthecardapproachesthemachine,andattempts useful for the more fine-grained adjustments required for
alignment through contact before performing the swiping high-precisiontasks.Forthecardswipingtask,visualization
motion. The vision-only policy, on the other hand, seems ofdemonstrationdata indicatedthatthewrist cameracannot
to skip the alignment phase, and directly attempts to swipe see the card reader due to occlusion from the gripper and
the card, often entirely missing the card slot as a result. therefore simply acts as a noise input to the policy.
These failure modes demonstrates that purely visual policies While the drops in performance due to proprioception as
lack the fine-grained tactile information that makes VISK wellasduetothewristcamerainthecardswipingtaskcould
extremely effective on contact-rich, precise manipulation. potentially be addressed by collecting more demonstrations,
Similarly, for the book retrieval task, prominent failure they highlight the true potential of the VISK framework.
modes for policies without AnySkin involve either applying The addition of AnySkin and the use of a transformer-based
too little force causing the book to flip back into the architecture enable the policy to incorporate reliable tactile
bookrack, or too much force causing the book to topple feedback directly from the interface between the robot and
over entirely. VISK policies apply a controlled downward theobjectbeinginteractedwith.Thelowdimensionalnature
force that enables them to pivot the book to an appropriate of AnySkin signal eliminates the need for dimensionality
tilt, followed by grasping and retrieval as shown in Fig. 3. reductionorintermediaterepresentationlearningandenables
Furthermore,forthistask,repeatedinteractionwiththesharp end-to-end learning of visuotactile policies from relatively
edgesofthebookcausedtheAnySkintotear.Allevaluations few (<200) demonstrations.E. Comparison between AnySkin and DIGIT
To further highlight the role of AnySkin in the VISK
frameworkforprecisemanipulationtasks,wecollectsimilar
demonstrationdatasetsfortwoofthetaskspresentedinSec-
In-domain Groundpin Shape Size Color
tion IV-B (Plug Insertion and USB Insertion) using DIGIT
sensors instead of AnySkin sensors. We maintain the same
policy architecture as VISK with the exception of the tactile
encoder,wherewereplacetheMLPwithamodifiedResNet-
18architectureidenticaltotheimageencodersusedforcam- In-domain Size Type
erainputs.WetraintwovariantsoftheDIGIT-basedpolicies:
one with raw DIGIT measurement as input to the policy,
and another with the DIGIT measurement at the start of the
trajectory subtracted from every subsequent measurement. In-domain Color Texture Thickness
Wereportstatisticsforthebest-performingalternative.While
the use of a different tactile sensor necessitates collection
of new demonstration data, we try to keep the DIGIT and
AnySkin datasets as close to each other as possible. Target
object locations used for training as well as evaluation are In-domain 5 books 11 books
identical between the experiments corresponding to both Fig. 5: We vary different parameters of the object used
sensors.TheresultsinTableIalsocomparetheperformance for collecting demonstrations to analyze the generalizability
of VISK usingtheskin-basedAnySkintactilesensoragainst of VISK policies for the four tasks: (top to bottom) Plug
the optical DIGIT [4] sensor. Insertion, USB Insertion, Card Swiping and Book Retrieval.
We find that across both tasks, policies trained with
AnySkin significantly outperform those trained with DIGIT.
Thisdifferencecouldbeattributedtothelowersensitivityof
similar to the ablation without wrist cameras reported in
the DIGIT sensor making it difficult to detect small tactile
TableI.Thisindicatesthatthepolicymightstruggletolocate
signals from extrinsic contact of the grasped object. Fur-
the socket when the wrist camera image is sufficiently out
thermore, the significantly higher dimensionality of DIGIT
of distribution, further emphasizing the importance of wrist
observations compared to VISK might also make it more
camerainformationinperformingprecisetaskslikeinsertion.
difficult to learn a sensory encoder without overfitting to
2) Card Swiping: We similarly evaluate the performance
the training data. These experiments highlight the suitability
of the best-performing VISK policy on three different varia-
of AnySkin over optical sensors for efficiently learning
tions of the card as shown in Fig. 5 – color alone, color and
visuotactile policies for precise tasks, due to its ability to
texture (credit card with with embossed text on the surface),
sense finer tactile details as well as its low dimensionality
andcolorandthickness(paper-thinmetrocard).Asindicated
resulting in more robust policies.
byTableII,the VISK policygeneralizessurprisinglywellto
F. Generalization to Unseen Task Variations variations in color and thickness. This is further evidence of
Tofurtherprobethestrengthsofthe VISK framework,we VISK policies effectively leveraging vision and touch even
investigate performance on unseen task variations for all of when faced with object variations distinctly different from
thetaskspresentedabove.Foreachvariation,weevaluatethe training. The relative performance drop due to variations in
best-performing VISK policy for the respective task on the texture could be attributed to out-of-distribution tactile data
same set of target object configurations shown in Fig. 4 and resulting from stress concentrations at the locations of the
present the results in Table II. Additionally, we also report embossed text.
generalization performance of a vision-only baseline, which 3) USB Insertion: Similarly, for USB insertion, we study
is essentially the VISK policy without tactile information. theeffectivenessofthebest-performing VISK policyontwo
1) Plug Insertion: We study the efficacy of the best- different variations of the USB stick as shown in Fig, 5 –
performing VISK policy on four different variations of color and type (USB cable), and color and size (different
the plug as shown in Fig. 5 – addition of a ground pin, USB key). Results presented in Table II show that the
shape, size and color. On this small sample set, the VISK performance of the VISK policy drops by a small amount
policy generalizes surprisingly well to every plug variation with the cable as well as the different USB stick. This
exceptcolordespitetheirpinsbeinginsignificantlydifferent drop can be attributed to the significant difference in both
positions relative to the plug used for training. This is appearance as well as the surface properties of the objects
further evidence of VISK policies effectively leveraging used, and could potentially be bridged by increasing the
vision and touch even when faced with object variations number of training demonstrations and/or increasing the
distinctly different from training. Change in color is one diversity of training data. variations in shape, and also to a
variant where we see a significant drop in performance. The combinationofchangeincolorandadditionofacable.This
behaviors corresponding to these failures are qualitatively is further evidence of VISK policies effectively leveragingTABLEII:PerformanceofthebestVISKpolicyondifferentvariationsofeachtask.Forthepluginsertion,cardswiping,and
USB insertion tasks, we vary different parameters of the plug, card, and USB stick respectively. For the book retrieval task,
we vary the number of books in the bookrack. We also report performance of a vision-only baseline policy for comparison.
Successfultrials
Task Policy
In-domain Variations
Groundpin Shape Size Color
PlugInsertion ViSk 8/10 6/10 6/10 6/10 1/10
Vision-only 5/10 1/10 2/10 4/10 1/10
Color/Type Color/Size
USBInsertion ViSk 7/10 5/10 4/10
Vision-only 4/10 1/10 2/10
Color Color/Texture Color/Thickness
CardSwiping ViSk 9/10 8/10 6/10 7/10
Vision-only 5/10 3/10 3/10 1/10
5books 11books
BookRetrieval ViSk 7/10 3/10 6/10
Vision-only 4/10 2/10 4/10
vision and touch even when faced with object variations tasks. Additionally, we also present a detailed analysis of
substantially different from training. the effect of different modalities on policy performance
4) Book Retrieval: Finally, we also evaluate the best- for this class of tasks and find that while the addition
performing VISK policy for variations of the book retrieval of wrist cameras can be critical to performance in tasks
task, with different numbers of books in the bookrack as involving fine alignment, proprioception can often hurt spa-
shown in Fig. 5. Our dataset is collected with 8 books, tial generalizability. We address a few limitations in this
and we test generalizability to two variants with 5 and 11 work:(a)While VISK showssignificantimprovementsover
books. For the 5 book variation, we start with the same vision-only policies, the policy’s performance remains at
initial arrangements as used in the original 10 evaluations, approximately 60% across all tasks. This suggests potential
andrandomlyremove3booksforeachtrial.Forthe11-book forfurtherperformanceenhancementthroughfine-tuningthe
variation,werandomizetheorderofthebooksforevaluation. VISK policy using reinforcement learning techniques [47].
SuccessratesarereportedinTableII.Weobservethatdespite (b) Contrary to findings in prior studies, we observe that
prominent visual differences from the additional books, the robot proprioception did not contribute to improved policy
VISK policy is able to generalize well to the scenario with learning performance in precise manipulation tasks. This
11books.Thisreinstatestheeffectivenessofthevisuotactile unexpected result warrants further investigation and presents
representation learned in VISK for generalizing to novel an interesting direction for future research. (c) All the
scenarios at inference. However, for the 5-book variation we tasks analyzed in this work involve maintained contact with
findthatperformancedropssignificantly.Successfulrollouts the object throughout the duration of the task. Tasks that
of the VISK policy perform a pivoting motion as shown in require making and breaking contact may involve specific
Fig. 3 before grasping and retrieving the book. A qualitative nuances that could benefit from a similar detailed analysis.
analysis of the behaviors during failed rollouts seems to These limitations notwithstanding, we believe that VISK
suggest that fewer books result in lower friction from the presentsasignificantstepintherightdirectionforadvancing
books neighboring the target book, precluding this pivoting visuotactile policy learning in robotics.
motion. As a result, the target book either falls back into the
bookrack or falls out onto the table. ACKNOWLEDGMENTS
Moreover, across the four tasks, we find that the vision-
Special thanks for Krishna Bodduluri, Mike Lambeta,
only baseline exhibits substantially worse generalization
and team from Meta AI Research for providing the DIGIT
performance ie. shows larger drops in performance when
sensorsforcomparison.ThankstoTessHellebrekersforpro-
different parameters of the object are varied. This indicates
vidingthesensorskinsanddiscussionsfortheexperiments.
that VISK leverages tactile feedback to improve robustness
of learned policies to object variations, and highlights the REFERENCES
value of tactile sensing to precise manipulation tasks.
[1] R. S. Johansson, “Sensory control of dexterous manip-
V. CONCLUSIONANDLIMITATION ulationinhumans,”inHandandbrain. Elsevier,1996,
In this work, we presented Visuo-Skin (VISK), a sim- pp. 381–414.
ple yet effective framework that leverages low-dimensional [2] J. Jenner and J. Stephens, “Cutaneous reflex responses
AnySkin tactile sensing for visuotactile policy learning in andtheircentralnervouspathwaysstudiedinman,”The
the real world. Our results demonstrate the efficacy of VISK Journal of physiology, vol. 333, no. 1, pp. 405–419,
across a diverse range of precise, contact-rich manipulation 1982.[3] W.Yuan,S.Dong,andE.H.Adelson,“Gelsight:High- and C. C. Kemp, “Tactile sensing over articulated
resolutionrobottactilesensorsforestimatinggeometry joints with stretchable sensors,” in 2013 World Haptics
and force,” Sensors, vol. 17, no. 12, p. 2762, 2017. Conference (WHC). IEEE, 2013, pp. 103–108.
[4] M. Lambeta, P.-W. Chou, S. Tian, B. Yang, B. Mal- [17] S.Stassi,V.Cauda,G.Canavese,andC.F.Pirri,“Flex-
oon, V. R. Most, D. Stroud, R. Santos, A. Byagowi, ible tactile sensing based on piezoresistive composites:
G. Kammerer, et al., “Digit: A novel design for a Areview,”Sensors,vol.14,no.3,pp.5296–5332,2014.
low-cost compact high-resolution tactile sensor with [18] O. Glauser, D. Panozzo, O. Hilliges, and O. Sorkine-
application to in-hand manipulation,” IEEE Robotics Hornung,“Deformationcaptureviasoftandstretchable
and Automation Letters, vol. 5, no. 3, pp. 3838–3845, sensor arrays,” ACM Transactions on Graphics (TOG),
2020. vol. 38, no. 2, pp. 1–16, 2019.
[5] S. Suresh, M. Bauza, K.-T. Yu, J. G. Mangelson, [19] T.-Y. Wu, L. Tan, Y. Zhang, T. Seyed, and X.-D. Yang,
A. Rodriguez, and M. Kaess, “Tactile slam: Real-time “Capacitivo: Contact-based object recognition on inter-
inference of shape and pose from planar pushing,” in active fabricsusing capacitive sensing,”in Proceedings
2021 IEEE International Conference on Robotics and of the 33rd annual acm symposium on user interface
Automation (ICRA). IEEE, 2021, pp. 11322–11328. software and technology, 2020, pp. 649–661.
[6] S. Suresh, Z. Si, S. Anderson, M. Kaess, and [20] N. Wettels, V. J. Santos, R. S. Johansson, and G. E.
M. Mukadam, “Midastouch: Monte-carlo inference Loeb, “Biomimetic tactile sensor array,” Advanced
over distributions across sliding touch,” in Conference robotics, vol. 22, no. 8, pp. 829–849, 2008.
on Robot Learning. PMLR, 2023, pp. 319–331. [21] T. P. Tomo, M. Regoli, A. Schmitz, L. Natale, H. Kris-
[7] S. Funabashi, G. Yan, A. Geier, A. Schmitz, T. Ogata, tanto, S. Somlor, L. Jamone, G. Metta, and S. Sugano,
and S. Sugano, “Morphology-specific convolutional “Anewsiliconestructureforuskin—asoft,distributed,
neural networks for tactile object recognition with a digital 3-axis skin sensor and its integration on the
multi-fingeredhand,”in2019InternationalConference humanoid robot icub,” IEEE Robotics and Automation
on Robotics and Automation (ICRA). IEEE, 2019, pp. Letters, vol. 3, no. 3, pp. 2584–2591, 2018.
57–63. [22] R. Bhirangi, T. Hellebrekers, C. Majidi, and A. Gupta,
[8] R. Bhirangi, A. DeFranco, J. Adkins, C. Majidi, “Reskin: versatile, replaceable, lasting tactile skins,” in
A.Gupta,T.Hellebrekers,andV.Kumar,“Allthefeels: 5th Annual Conference on Robot Learning, 2021.
Adexteroushandwithlarge-areatactilesensing,”IEEE [23] S.Suresh,H.Qi,T.Wu,T.Fan,L.Pineda,M.Lambeta,
Robotics and Automation Letters, 2023. J. Malik, M. Kalakrishnan, R. Calandra, M. Kaess,
[9] R.Li,R.Platt,W.Yuan,A.TenPas,N.Roscup,M.A. et al., “Neural feels with neural fields: Visuo-tactile
Srinivasan, and E. Adelson, “Localization and manipu- perception for in-hand manipulation,” arXiv preprint
lation of small parts using gelsight tactile sensing,” in arXiv:2312.13469, 2023.
2014IEEE/RSJInternationalConferenceonIntelligent [24] J. Li, S. Dong, and E. Adelson, “Slip detection with
Robots and Systems. IEEE, 2014, pp. 3988–3993. combinedtactileandvisualinformation,”in2018IEEE
[10] S. Kim and A. Rodriguez, “Active extrinsic contact International Conference on Robotics and Automation
sensing: Application to general peg-in-hole insertion,” (ICRA). IEEE, 2018, pp. 7772–7777.
in 2022 International Conference on Robotics and [25] J.W.JamesandN.F.Lepora,“Slipdetectionforgrasp
Automation (ICRA). IEEE, 2022, pp. 10241–10247. stabilization with a multifingered tactile robot hand,”
[11] H. Qi, B. Yi, S. Suresh, M. Lambeta, Y. Ma, R. Calan- IEEETransactionsonRobotics,vol.37,no.2,pp.506–
dra,andJ.Malik,“Generalin-handobjectrotationwith 519, 2021.
vision and touch,” in Conference on Robot Learning. [26] N. Jamali and C. Sammut, “Majority voting: Material
PMLR, 2023, pp. 2549–2564. classification by tactile sensing using surface texture,”
[12] A.George,S.Gano,P.Katragadda,andA.B.Farimani, IEEETransactionsonRobotics,vol.27,no.3,pp.508–
“Visuo-tactile pretraining for cable plugging,” arXiv 521, 2011.
preprint arXiv:2403.11898, 2024. [27] S. S. Baishya and B. Ba¨uml, “Robust material clas-
[13] R. Bhirangi, V. Pattabiraman, E. Erciyes, Y. Cao, sification with a tactile skin using deep learning,” in
T. Hellebrekers, and L. Pinto, “Anyskin: Plug-and- 2016IEEE/RSJInternationalConferenceonIntelligent
play skin sensing for robotic touch,” arXiv preprint Robots and Systems (IROS). IEEE, 2016, pp. 8–15.
arXiv:2409.08276, 2024. [28] J.Lin,R.Calandra,andS.Levine,“Learningtoidentify
[14] S. Haldar, Z. Peng, and L. Pinto, “Baku: An efficient objectinstancesbytouch:Tactilerecognitionviamulti-
transformer for multi-task policy learning,” 2024. modal matching,” in 2019 International Conference on
[Online]. Available: https://arxiv.org/abs/2406.07539 Robotics and Automation (ICRA). IEEE, 2019, pp.
[15] S. Sundaram, P. Kellnhofer, Y. Li, J.-Y. Zhu, A. Tor- 3644–3650.
ralba, and W. Matusik, “Learning the signatures of the [29] A. Schneider, J. Sturm, C. Stachniss, M. Reisert,
humangraspusingascalabletactileglove,”Nature,vol. H. Burkhardt, and W. Burgard, “Object identifica-
569, no. 7758, pp. 698–702, 2019. tion with tactile sensors using bag-of-features,” in
[16] T. Bhattacharjee, A. Jain, S. Vaish, M. D. Killpack, 2009IEEE/RSJInternationalConferenceonIntelligentRobots and Systems. IEEE, 2009, pp. 243–248. [43] Y. Li, J.-Y. Zhu, R. Tedrake, and A. Torralba, “Con-
[30] J. Ilonen, J. Bohg, and V. Kyrki, “Fusing visual and nectingtouchandvisionviacross-modalprediction,”in
tactile sensing for 3-d object reconstruction while ProceedingsoftheIEEE/CVFConferenceonComputer
grasping,” in 2013 IEEE International Conference on Vision and Pattern Recognition, 2019, pp. 10609–
Robotics and Automation, 2013, pp. 3547–3554. 10618.
[31] ——,“Three-dimensionalobjectreconstructionofsym- [44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,
metricobjectsbyfusingvisualandtactilesensing,”The L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,
International Journal of Robotics Research, vol. 33, “Attention is all you need,” Advances in neural infor-
no. 2, pp. 321–341, 2014. mation processing systems, vol. 30, 2017.
[32] I. Guzey, B. Evans, S. Chintala, and L. Pinto, “Dex- [45] T. Z. Zhao, V. Kumar, S. Levine, and C. Finn, “Learn-
terity from touch: Self-supervised pre-training of tac- ing fine-grained bimanual manipulation with low-cost
tile representations with robotic play,” arXiv preprint hardware,” arXiv preprint arXiv:2304.13705, 2023.
arXiv:2303.12076, 2023. [46] C.Chi,S.Feng,Y.Du,Z.Xu,E.Cousineau,B.Burch-
[33] I. Guzey, Y. Dai, B. Evans, S. Chintala, and L. Pinto, fiel, and S. Song, “Diffusion policy: Visuomotor pol-
“See to touch: Learning tactile dexterity through visual icy learning via action diffusion,” in Proceedings of
incentives,” in 2024 IEEE International Conference on Robotics: Science and Systems (RSS), 2023.
Robotics and Automation (ICRA). IEEE, 2024, pp. [47] S. Haldar, J. Pari, A. Rai, and L. Pinto, “Teach a robot
13825–13832. to fish: Versatile imitation from one minute of demon-
[34] T. Lin, Y. Zhang, Q. Li, H. Qi, B. Yi, S. Levine, strations,” arXiv preprint arXiv:2303.01497, 2023.
and J. Malik, “Learning visuotactile skills with two
multifingeredhands,”arXivpreprintarXiv:2404.16823,
2024.
[35] Y. Yuan, H. Che, Y. Qin, B. Huang, Z.-H. Yin, K.-W.
Lee,Y.Wu,S.-C.Lim,andX.Wang,“Robotsynesthe-
sia: In-hand manipulation with visuotactile sensing,” in
2024 IEEE International Conference on Robotics and
Automation (ICRA). IEEE, 2024, pp. 6558–6565.
[36] T. Kelestemur, R. Platt, and T. Padir, “Tactile pose
estimation and policy learning for unknown object
manipulation,” arXiv preprint arXiv:2203.10685, 2022.
[37] M. A. Lee, Y. Zhu, P. Zachares, M. Tan, K. Srinivasan,
S.Savarese,L.Fei-Fei,A.Garg,andJ.Bohg,“Making
sense of vision and touch: Learning multimodal repre-
sentationsforcontact-richtasks,”IEEETransactionson
Robotics, vol. 36, no. 3, pp. 582–596, 2020.
[38] H. Li, Y. Zhang, J. Zhu, S. Wang, M. A. Lee, H. Xu,
E. Adelson, L. Fei-Fei, R. Gao, and J. Wu, “See, hear,
and feel: Smart sensory fusion for robotic manipula-
tion,” arXiv preprint arXiv:2212.03858, 2022.
[39] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual
learning for image recognition,” in Proceedings of
the IEEE conference on computer vision and pattern
recognition, 2016, pp. 770–778.
[40] A. Iyer, Z. Peng, Y. Dai, I. Guzey, S. Haldar, S. Chin-
tala, and L. Pinto, “Open teach: A versatile teleoper-
ation system for robotic manipulation,” arXiv preprint
arXiv:2403.07870, 2024.
[41] D. Brandfonbrener, S. Tu, A. Singh, S. Welker,
C.Boodoo,N.Matni,andJ.Varley,“Visualbacktrack-
ing teleoperation: A data collection protocol for offline
image-based reinforcement learning,” in 2023 IEEE
International Conference on Robotics and Automation
(ICRA). IEEE, 2023, pp. 11336–11342.
[42] S. Dasari, J. Wang, J. Hong, S. Bahl, Y. Lin, A. Wang,
A. Thankaraj, K. Chahal, B. Calli, S. Gupta, et al.,
“Rb2: Robotic manipulation benchmarking with a
twist,” arXiv preprint arXiv:2203.08098, 2022.