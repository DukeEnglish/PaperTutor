Optimal Design for Reward Modeling in RLHF
Antoine Scheid Etienne Boursier Alain Durmus
CMAP - CNRS INRIA Saclay CMAP - CNRS
E´cole polytechnique Universit´e Paris Saclay, LMO E´cole polytechnique
Palaiseau, France Orsay, France Palaiseau, France
Michael I. Jordan Pierre M´enard Eric Moulines Michal Valko
U.C., Berkeley ENS Lyon CMAP - CNRS INRIA
INRIA, ENS Lyon, France E´cole polytechnique
Paris, France Palaiseau, France
Abstract bution in this area to provide an offline ap-
proach as well as worst-case guarantees.
Reinforcement Learning from Human Feed-
back(RLHF)hasbecomeapopularapproach
1 Introduction
to align language models (LMs) with human
preferences. This method involves collect-
In learning from human feedback (Christiano et al.,
ing a large dataset of human pairwise pref-
2017; Naveed et al., 2023; Wei et al., 2023), an agent
erences across various text generations and
learns to act based on a preference signal. This sub-
usingittoinfer(implicitlyorexplicitly)are-
ject has recently seen a surge of interest due to its
ward model. Numerous methods have been
effectiveness for aligning pre-trained Large Language
proposedtolearntherewardmodelandalign
Models (LLMs) with human preferences. Typically,
a LM with it. However, the costly process
thehumanfeedbackisgatheredbyconstructingalarge
of collecting human preferences has received
datasetofcontexts(prompts),pairsoflanguagemodel
little attention and could benefit from the-
outputs(completions),andhumanpreferencebetween
oretical insights. This paper addresses this
thepairs. Giventhispreferencedataset,severalmeth-
issue and aims to formalize the reward train-
ods have been proposed to align pre-trained large lan-
ing model in RLHF. We frame the selection
guage models (LLMs). For instance, reinforcement
ofaneffectivedatasetasasimpleregretmin-
learning from human feedback (RLHF, Ziegler et al.
imization task, using a linear contextual du-
2019)involvestrainingarewardmodelfromtheprefer-
eling bandit method. Given the potentially
encedatasetandthenfine-tuningthepre-trainedLLM
large number of arms, this approach is more
using reinforcement learning, typically with the PPO
coherentthanthebest-armidentificationset-
algorithm(Schulmanetal.,2017),tomaximizethere-
ting. We then propose an offline framework
wardmodel. Anotherapproachisthedirectpreference
for solving this problem. Under appropriate
optimization (DPO) procedure (Rafailov et al., 2024),
assumptions—linearityoftherewardmodel
where the pre-trained LLM is directly fine-tuned with
in the embedding space, and boundedness of
the preference dataset by learning an implicit reward
the reward parameter — we derive bounds
model.
on the simple regret. Finally, we provide a
lower bound that matches our upper bound Learningfromhumanfeedbackhasproventohaveex-
uptoconstantandlogarithmicterms. Toour traordinaryabilitiesinitsapplicationtovariousfields,
knowledge, this is the first theoretical contri- including robotics (Tucker et al., 2020; Bıyık et al.,
2020, 2024), language models (Ouyang et al., 2022;
Touvron et al., 2023; Bubeck et al., 2023), and rec-
ommendations (Chen et al., 2024b; Zhao et al., 2023).
However, the majority of works in this area focus on
Preprint Version
preferenceoptimization,andlittleisknownabouthow
4202
tcO
32
]GL.sc[
2v55071.0142:viXraOptimal Design for Reward Modeling in RLHF
to efficiently construct a human preferences dataset. 2 Setting
In this work, we provide a theoretically grounded in-
sight into the data collection process for learning a 2.1 Background on RLHF
reward model.
For what follows, X represents the set of contexts (or
Ingeneral,averylargedatasetofpromptsandassoci-
prompts) and Y the set of generations (or comple-
atedgenerationsD issampled. Amongthisdataset,
ini tions). Human labelers are presented with pairs of
a smaller one is selected (D ) and receives feed-
select prompt-completion tuples, denoted (x,y) and (x,y′),
back from human labelers, due to the cost of labeling
which we can express as {x,y,y′}. Formally, a lan-
the generations. In practice, the dataset selection is
guage model ϕ is a mapping from the set of contexts
achieved without too much care, hence a loss in the
X to probability distributions over the set of possi-
information that could have been retrieved from the
ble generations Y. The task of the labelers (anno-
original dataset D . Some techniques to improve the
ini tators) is to determine which completion between y
selectionrelyonheuristicsorblack-boxmethods(Shen
and y′ is more accurate or preferable in the context of
et al., 2024; Dong et al., 2024; Chang et al., 2024) but
x, denoted as y ≻y′|x, when (x,y) is preferred over
lack of provable bounds on the optimality of the pro-
(x,y′). To account for human uncertainty, we model
cedure. the binary feedback 1(y ≻y′|x) process probabilisti-
Based on these considerations, we study the offline callybyassumingapreferenceprobabilityP: theevent
selection of the optimal dataset D . Our goal {y ≻ y′|x} coded as a binary variable 1(y ≻y′|x) oc-
select
is to minimize the number of samples that need to curs with probability P(y ≻y′|x).
be rated by labelers while retaining as much valu-
The Bradley-Terry model (Bradley and Terry, 1952)
able information as possible from the initial dataset
provides a framework for modeling preferences based
D . Toachievethis,weproposeanewmethodcalled
ini on real-valued rewards. Given a reward function
ODPO: Optimal Design for Policy Optimization.
r(x,y) that assigns a score to each context-generation
This method guides the dataset selection process us-
pair (x,y), the probability of favoring one generation
ing the solution to an optimal design problem. We
over another is expressed as follows
prove that ODPO is optimal from a worst-case perspec-
tive. Interestingly, ODPO can be applied to select pairs P(y ≻y′|x)=σ(r(x,y)−r(x,y′)) (1)
in the dataset D at a low cost before running any
select =1/(1+e−(r(x,y)−r(x,y′))),
preference optimization procedure.
We summarize our contributions as follows: where σ is the sigmoid function. It is worth not-
ing that alternative preference models, such as the
• Under the Bradley-Terry model and the assump- Plackett-Luce model (Plackett, 1975), can be used in-
tion of a contextual linear bandit for the reward stead of this one. After having selected a dataset
model, we formalize a pure exploration bandit of pairs D select = {X t,Y t(1),Y t(2)} t∈[T] and the as-
framework for the collection of samples used to sociated human preferences to make it D˜ =
selected
train the reward model in RLHF. {X ,Y(1),Y(2),1(Y(1) ≻Y(2)|X )} ,theestimated
t t t t t t t∈[T]
reward function rˆis computed as the minimum of the
• Within that setting, we introduce ODPO: Optimal loss
Design for Preference Optimization, which
optimallyselectsthebestarmstolearnthereward J(r)=−E [log(P(y ≻y′|x))], (2)
(x,y,y′)∼Dselect
model and we upper bound its simple regret.
and is then used to fine-tune the model ϕ which needs
• We prove the optimality of our technique with a to maximize this reward while staying close from the
lower bound which matches our upper bound up initial model ϕ , which is achieved by minimizing the
0
to logarithmic factors. following loss
L(ϕ)=E [rˆ(x,y)]−γD (ϕ||ϕ ), (3)
Notethatfewworksstudytheoptimalwayofchoosing ϕ KL 0
the dataset of human preferences. Thus, we are very
where γ is some constant and D stands for the
enthusiasticaboutthepotentialimpactofourmethod KL
Kullback-Leibler divergence.
and theoretical results on the reward training steps.
As mentioned by Casper et al. (2023), collecting data Despite a growing literature around the optimization
that is representative of human preferences is an open procedures (2), (3) (see, e.g., Schulman et al., 2017;
problem in RLHF and deserves more attention, hence Rafailovetal.,2024;Azaretal.,2024), littlehasbeen
our attempt in this direction. done to select optimally the human-labelled datasetA. Scheid, E. Boursier, A. Durmus, M.I. Jordan, P. M´enard, E. Moulines, M. Valko
D ,althoughitcruciallyimpactstherewardtrain- We now work in the embedding space Rd for the
select
ing or the policy optimization. dataset selection. Defining N as the number of avail-
able prompts, and K the number of associated gener-
In practice, the selection of the pairs (y1,...,yK) as-
n n ations for each prompt (although our results still hold
sociatedwiththen-thpromptx isachievedwiththe
n forN,K tendingtoinfinity), wehaveaccesstoanini-
initial model ϕ sampling several generations for the
0 tial dataset that can be written {(x ,yk)} .
same context (with a change of the temperature be- n n n∈[N],k∈[K]
We model it as a union of N sets A ,n ∈ [N] and
tween the different ones) from which two generations n
write D = ∪ A . A subset A represents the
are randomly selected. The full dataset is then given ini n∈[N] n n
set of all the prompts associated with the same gen-
to labelers for rating. T and N are of order 1000 to
eration x : for any n ∈ [N],A = {a1,...,aK} and
100000 for usual datasets while K is around one or a n n n n
ak =ψ(x ,yk) with n∈[N],k ∈[K]. The goal of our
few dozens. n n n
procedure is to select a subset of T ∈N⋆ pairs of gen-
erations,eachpairbeingassociatedwiththesamecon-
2.2 RLHF as a Dueling Bandit Problem text. Formally, it boils down to choosing a sequence
{(A1,A2)} of pairs in D that receive a feedback
t t t∈[T] ini
We now introduce our offline setting, which is one of from labelers such that for any t ∈ [T],(A1,A2) =
t t
the main novelty of our approach as compared to pre- (ai,aj) for some n ∈ [N],i,j ∈ [K]. The selected
n n
vious works around this topic. Relying on optimal dataset D = {(A1,A2)} is given to labelers
select t t t∈[T]
design and statistical foundations, the objective is to who give the winner of the duel between A1 and A2
t t
choose a dataset D of prompts-generations that
select for any t ∈ [T]. The preference feedback is then re-
maximizes the information gained from human label-
ceived as
ers’ feedback. Our approach to minimize the size of
the collected dataset is notable for two reasons. First, {A1 t ≻A2 t} with probability σ(⟨θ⋆,A1 t −A2 t⟩)
it aligns with common practice, as it is impractical to or {A2 ≻A1} with probability σ(⟨θ⋆,A2−A1⟩),
t t t t
operateonlineandgetlabelers’feedbackbeforechoos-
and we encode it as a random variable Y , following
ing the next pair. Second, we establish matching up- t
per and lower bounds, up to constant and logarithmic Y =1(A1 ≻A2), (5)
t t t
factors, which ensures the efficiency of ODPO.
which therefore follows the distribution
Wemaketheassumptionofacontextuallinearreward,
P (Y =1|F )=σ(⟨θ⋆,A1−A2⟩) (6)
hence the existence of a known feature map ψ: X × θ⋆ t t−1 t t
Y →Rd,x,y (cid:55)→ψ(x,y)suchthatforanyx,y ∈X×Y =1/(1+e−⟨θ⋆,A1 t−A2 t⟩),
r(x,y)=⟨θ⋆,ψ(x,y)⟩. (4) for the unknown reward parameter θ⋆. We also define
the set of differences between all possible action pairs
B = {ai − aj} = ∪ {A − A }, as
The reward is given with respect to the embedding n n n∈[N],i,j∈[K] n∈[N] n n
wellasitscardinalL=Card(B)⩽TK2. Choosingan
of the promt-completion pair. Simple encoder models
arbitrary ordering among the elements of B, we can
such as BERT, RoBERTa or SBERT (Reimers, 2019; De-
write B ={b } .
vlin, 2018; Liu, 2019) can be used for the embedding l l∈[L]
step. Usually, the feature map can be obtained by It is important to note that we considered a finite ac-
removing the last layer of the initially trained model. tion space for the sake of clarity. However, our theory
stillholdsfor anarbitraryactionspaceofpossiblyinfi-
nite size. We would keep the same bounds depending
solely on T and d due to the leverage of optimal de-
sign theory which circumvents the burden of having a
large action space and relies on a distribution π⋆ with
a finite and bounded support.
Based on the feedback (Y ) from the preference
t t∈[T]
pairs, our procedure first estimates θ⋆. Then, for
any context x given as an argument, a completion
n
yi,i ∈ [K] or equivalently an action aˆ (A ) ∈ A
n T n n
can be chosen. We now define the simple regret of an
algorithm ALG, as
R (T,(A ) ,θ⋆)= max max⟨θ⋆,a−aˆ (A )⟩,
Figure 1: Illustration of ODPO among the whole ALG n n∈[N] n∈[N]a∈An T n
RLHF framework. (7)Optimal Design for Reward Modeling in RLHF
and defining a⋆ =argmax ⟨θ⋆,a⟩, we have that them now. We define the regularized log-likelihood L
n a∈An
for the collected samples up to time t and a reward
R ALG(T,(A n) n∈[N],θ⋆)= max⟨θ⋆,a⋆ n−aˆ T(A n)⟩, parameter θ ∈Rd as
n∈[N]
Note that in our setup, we are looking for an algo- (cid:88)t−1
L ({A1,A2,Y } ,θ)= log(P (A1,A2,Y ))
rithm that converges for any possible parameter θ⋆ t s s s s∈[t−1] θ s s s
in the unit ball. We also consider any set of actions s=1
−λ∥θ∥2/2, (8)
(A n) n∈[N] - which makes our results robust in an ad- 2
versarial, non i.i.d. setting. Minimizing the simple
which can be rewritten as
regret isacoherentobjectivetostudyoptimaldataset
selection. Since we only care about selecting informa- t−1
(cid:88)
tive armsandaboutthequalityofthepredictionafter L t({A1 s,A2 s,Y s} s∈[t−1],θ)= Y slog(σ(⟨θ,A1 s−A2 s⟩))
all the arms have been sampled, it makes more sense s=1
than looking at the cumulative reward. Imagine that t−1
(cid:88)
labelers need to rate a pair of bad completions: there + (1−Y s)log(σ(−⟨θ,A1 s−A2 s⟩))−λ∥θ∥2 2/2,
isnoharmforanyone. Secondly,itishardtomakehy- s=1
potheses on the form of the actions sets or the reward
andthemaximumlikelihoodestimatoratstept(MLE)
parameters for embeddings of LM generations, hence θˆ is computed following
t
the fact that we do not make any i.i.d. assumption.
θˆ ∈argmax L ({A1,A2,Y } ,θ). (9)
Objectives. Note that we could have thought about t θ∈Rd t s s s s∈[t−1]
different objectives for our problem, instead of mini-
Lemma 1. We can differentiate the likelihood defined
mizing the simple regret:
in (8), and obtain
• Best-arm identification: one formulation of
∇ L({A1,A2,Y } ,θ) (10)
it within our setup would be to maximize θ s s s s∈[t−1]
P(aˆ T(A n)=a⋆ n) over any n∈[N]. =(cid:88)t−1
(Y −P (Y =1))(A1−A2)−λθ ,
s θ s s s
• Arm distance minimization: for any T ∈ N,n ∈ s=1
[N],minimize∥aˆ (A )−a⋆∥forsomewell-chosen
T n n which gives by definition of the maximum likelihood
norm ∥·∥. estimator in (9), that θˆ must satisfy
t
We do not focus on arm distance minimization here, t−1
as it is difficult to quantify the difference in quality (cid:88) (Y −P (Y =1))(A1−A2)−λθˆ =0. (11)
s θˆ
t
s s s t
between two LM generations based on their distance s=1
or cosine similarity in Rd (Steck et al., 2024). Addi-
tionally, since the reward gap between two arms can Definingforanyt∈[T+1]thefunctionH : Rd →Rd,
t
be arbitrarily small and approach zero, the concept of θ (cid:55)→ λθ+(cid:80)t−1σ((cid:10) θ,A1−A2(cid:11) )(A1−A2), we obtain
s=1 s s s s
best-arm does not really apply to our setup. Instead, by definition of θˆ that
t
seekingsimpleregretminimizationeffectivelycaptures
thequalityofthedatasetselectionprocessbymeasur- t−1
inghowwellthesampledpairsaligntherewardmodel (cid:88) Y s(A1 s−A2 s)=H t(θˆ t). (12)
with human preferences - with the one-step final re- s=1
ward becoming close from optimality.
Note that we define the MLE as the maximizer of the
We make the following assumption about the reward likelihood over the whole set Rd although we know
parameter as well as the embeddings of the pairs in that under H1, θ⋆ ∈ B(0,1). This is why we define
Rd. the projected MLE estimator θˆP - a similar quantity
t
is also used by Faury et al. (2020) - as
H1 (Boundedness of action and parameter). For any
x,y ∈X×Y,ψ(x,y)∈B(0,1),whereB(0,1)standsfor
θˆP = argmin∥H (θ)−H (θˆ)∥ , (13)
the unit ball in Rd. We also suppose that θ⋆ ∈B(0,1). t t t t V−1
θ∈B(0,1) t
2.3 Parameter Estimation where V t stands for the design matrix in our problem,
defined as
Log-likelihood and design matrix. Several useful quan-
t−1
tities appear in the rest of the paper. Since they are V =λI+(cid:88) (A1−A2)(A1−A2)T , (14)
atthecoreofouralgorithmsandresults,weintroduce t s s s s
s=1A. Scheid, E. Boursier, A. Durmus, M.I. Jordan, P. M´enard, E. Moulines, M. Valko
whereλ>0isaregularizationparameter-thesameas Algorithm 1 ODPO: Optimally Designed Policy
for the likelihood. Since we work in an offline fashion, Optimization
wehaveaccesstoallthedata{ak n} n∈[N],k∈[K]andwork 1: Input: Number of samples T, set of actions B =
on it, trying to extract as much knowledge as possible ∪ {ai −aj} of size L, regulariza-
n∈[N] n n n∈[N],i,j∈[K]
from the pairwise comparisons. tion parameters λ, approximation parameter, ε.
2: Compute the history H = ∅, as well as t =
Notethatalotofappliedworkshelptocircumventthe 0
0,θˆ =∅ and V =λI, D =∅.
burden of manipulating very large set of parameters 0 0 select
3: Use FW to compute an (1+ε) approximation πˆ of
in language modeling (Hu et al., 2021; Houlsby et al.,
the optimal design π⋆ with B,U(B),λ.
2019; Lester et al., 2021), such as the computation of
4: for b∈B do
inversion of matrices.
5: Append b to D ⌈Tπˆ ⌉ times.
select b
6: end for
3 Offline and Online Algorithms 7: Compute θˆ according to (9) and θˆP accord-
T+1 T+1
ing to (13).
AlgorithmsforoursettingsampleT pairsfromtheset 8: Return D and θˆP .
select T+1
D (possibly with repetition). For any t ∈ [T], we
ini
write (A1,A2) for the pair of actions sampled by ODPO
t t
at iteration t, even thoughthere is no”time ordering”
in the procedure. This choice corresponds to taking
an action B ∈ B: B = A1 −A2. (6) shows that for
t t t t a reward parameter θ, we have Y ∼ Ber(σ(⟨θ,A1 −
t t
A2⟩))=Ber(σ(θTB )).
t t
3.1 ODPO: Optimal Design Policy
We now introduce ODPO: Optimally Designed
Policy Optimization,withthepseudocodeprovided a
3
in Algorithm 1. The core idea behind is to work
θ⋆
offline using the entire dataset D . The strength
ini
of optimal design techniques comes from the Kiefer- ⃗i =a
2 2
Wolfowitz theorem Appendix B) to select an optimal
core subset of samples. This makes it ideal for select-
Figure 2: Note that on this figure, a and a are op-
ing D without requiring any online feedback. In 2 3
select timal, alghough playing both of them for the duel will
ourapproach,theapproximateoptimaldesignpolicyπˆ
providefeedbackofverylowvaluesincetheylieinthe
is obtained using the Frank-Wolfe algorithm, given in
same region of Rd. This is why a duel between a and
AppendixB.InsteadofrequiringT stepsofcomputa- 1
another arm is of greater interest in our exploration
tionsofthelikelihoodandmostinformativepairs, our
setup: sampling good arms is not the optimal strat-
setup only requires to run the Frank-Wolfe algorithm
egy, hence the link with pure exploration.
toknowwhichsubsetD toselectfromD . Then,
select ini
after the human preferences over D are given, the
select
likelihood and the MLE are only computed once.
bility at least 1−δ, we have that
AftersamplingT informativepairs{A1,A2} from
t t t∈[T]
D , ODPO constructs the maximum likelihood esti-
ini
mator θˆ T for the regularized log-likelihood relying on ∥θˆP −θ⋆∥ ⩽
D˜ = {A1,A2,Y } and the maximum likeli- t Vt
selected t t t t∈[T] (cid:20)(cid:113) √ (cid:21)
hood estimator θˆP projected on the unit ball - see 20 2log(1/δ)+d log(cid:0) λ1−1/d+4t/dλ1/d(cid:1) + λ .
T+1
(13). Then, for any context x ,n ∈ [N] associated
n
with the embedded set A , and the estimated reward
n
parameter θˆP output by ODPO, we can estimate the
T+1 Lemma 2 allows to control the gap between the true
best-arm in this set aˆ (A ), following an optimistic
T n reward parameter θ⋆ and the estimation θˆ . We
procedure T+1
postpone the proof to Appendix A.
aˆ (A )=argmax ⟨θˆP ,a⟩. (15)
T n a∈An T+1 Decomposition of the Regret. Since R can be writ-
ten R(T,(A ) ,θ⋆) = ⟨θ⋆,a⋆ − aˆ (A )⟩
n n∈[N] n⋆ T+1 n⋆
Lemma 2. Under H1, for any δ ∈(0,1), with proba- with a⋆ = argmax ⟨θ⋆,a⟩ and n⋆ =
n a∈An
a=
⃗i
1
1Optimal Design for Reward Modeling in RLHF
argmax ⟨θ⋆,a⋆ −aˆ (A )⟩, we have that Our procedure achieves optimal selection without on-
n∈[N] n T+1 n⋆
line feedback, simplifying dataset design and reducing
R(T,(A n) n∈[N],θ⋆)⩽⟨θ⋆,a⋆ n⋆ −aˆ T(A n⋆)⟩ (16) computationalcosts,askeycomponentslikethedesign
−⟨θˆ ,a⋆ −aˆ (A )⟩ matrix or likelihood are computed only once.
T+1 n⋆ T n⋆
=⟨θ⋆−θˆ ,a⋆ −aˆ (A )⟩
T+1 n⋆ T n⋆ 3.2 Changing Action Set
⩽∥θ⋆−θˆ ∥
T+1 VT+1
Setup. The term online as opposed to offline is not
×∥a⋆ −aˆ (A )∥
n⋆ T n⋆ V T− +1 1 always clear in the RLHF literature. We can consider
⩽∥θ⋆−θˆ ∥ max∥b∥ , a version of our process with changing action sets: at
T+1 VT+1
b∈B
V T− +1
1 each step n ∈ [N], where N ∈ N, a prompt x is
n
wherethethirdlineholdsthankstoH¨olderinequality. drawn from the set of prompts X, and the initial lan-
Let π: B → [0,1] be a distribution over the set of guage model ϕ 0 generates K completions y n1,...,y nK
actions. We define the application g: ∆(B) → R and associated with x n. At each iteration, from this set
the design matrix V˜(π) for the distribution π as of K completions, two samples, Y(1) and Y(2), must
n n
be selected for evaluation by a human labeler. Given
V˜(π)=(cid:88) π(b)bbT andg(π)=max∥b∥2 V˜(π)−1. (17) that rating samples is costly, the goal is to design
b∈B
b∈B an algorithm that optimally selects T pairs, (x ,Y(1))
t t
and (x ,Y(2)) at each step, to form the most effective
A design π for our problem is a probability distribu- t t
dataset before training the reward model while keep-
tion over the set of actions B. An optimal design π⋆
ing T relatively small. Here, note that T = N based
is a solution in the Kiefer-Wolfowitz theorem while
on the previous notation from the offline setting.
the distribution πˆ over B is a (1+ε)-approximation
of π⋆ if g(πˆ)⩽(1+ε)g(π⋆). Objectives and metrics. As before, we are interested
in the precision of the reward estimation after the T
steps, which guides us towards selecting an optimal
Theorem 1. Let ε > 0 and suppose that we collect
at least T ⩾ d2 samples according to an (1+ε) ap- datasetD select. Thisiswhywekeepthesameobjective
proximation πˆ of the optimal design policy π⋆ for the as before and thus consider the simple regret of the
problem. Then, for any B and θ⋆ ∈B(0,1), with prob- procedure, defined here as
ability at least 1−δ, δ ∈(0,1), we have that
R(T,(A ) ,θ)= max max⟨θ,a−aˆ(A )⟩, (19)
n n∈[N] n
R(T,(A )
,θ⋆)⩽20(1+ε)(cid:112)
d/T × (18)
n∈[N]a∈An
n n∈[N]
(cid:20)(cid:113) √ (cid:21) where aˆ(A ) ∈ A is the best-arm prediction of the
(cid:0) (cid:1) n n
2log(1/δ)+d log λ1−1/d+4T/dλ1/d + λ .
procedure among the set A . Considering the same
n
kindofobjectivealsoallowsustocomparebothkinds
Corollary 1. Suppose that we have selected the sam- of procedures.
ples D to label under πˆ, a 3/2-approximation of
select Formally,ateachstept∈[T],anactionsetA ,t∈[T]
the optimal design policy π⋆. Choosing λ = 1/d for t
is provided and the algorithm must select a pair of
the regularization, for any δ ∈(0,1), under the condi- samples (A1,A2) ∈ A2. We can define the learner’s
tions of Theorem 1, with probability at least 1−δ, we history as Ht =t σ({Xt ,A1,A2} ), H = ∅ and
have that t s s s s=1,...,t 0
the learner uses an algorithm ALG to choose the ac-
R(T,(A ) ,θ⋆)⩽30(cid:112) d/T× tion pair (A1 t,A2 t) based on (H t−1,A t,U t), (U t) t∈N⋆
n n∈[N]
being a family of independent uniform random vari-
(cid:20)(cid:113) √ (cid:21)
2log(1/δ)+dlog((1+4T)/d1−1/d)+1/ d , ables on [0,1] allowing randomization in ALG. A com-
monly proposed strategy to explore is to pick the pair
(A1,A2)∈A2 at each iteration following
and as a consequence, choosing δ = d1−1/d/(4T +1), t t t
we can bound the expectation of the regret as A1,A2 ∈argmax ∥a−a′∥ .
(cid:115)
t t a,a′∈At V t−1
(cid:18) (cid:19)
d+2 4T +1
E[R(T,(A ) ,θ⋆)]⩽30 √ log
n n∈[N] T d1−1/d However,suchastrategy,aswellasanyprocedureALG
√ for a setup with changing arms cannot converge, since
+31/ T . we do not make i.i.d. assumptions throughout this
work and allow adversarial action sets. The choice of
ThelowerboundinSection4matchestheupperbound specific action sets that prevent convergence for any
in Corollary 1 up to constant or logarithmic factors. algorithm is explained in the proof of Theorem 2.A. Scheid, E. Boursier, A. Durmus, M.I. Jordan, P. M´enard, E. Moulines, M. Valko
Theorem 2. Consider the 2-dimensional euclidian 4 Lower Bound
space Span(e ,e ) as the whole action space. In that
1 2
case, there exists a set of actions (A ) and some As before, after N ∈ N sets of prompt-completions
t t∈[T]
θ⋆ ∈ B(0,1) such the regret defined in (19) for any have been sampled, we consider again B = {ai −
n
algorithm ALG satisfies aj} =∪ {A −A },thesetofallpos-
n n∈[N],i,j∈[K] n∈[N] n n
sible action pairs. We consider an algorithm ALG that
samples T pairs from this set and receives feedback Y
R (T,(A ) ,θ⋆)⩾e−c/2, t
ALG t t∈[T] from a labeler. For any t ∈ [T], we write (A1,A2) for
t t
the pair of actions sampled by ALG at iteration t, even
for some c>0 independent of T and d. though there is no ”time ordering” in our procedure.
This choice corresponds to taking an action
B ∈B: B =A1−A2 .
3.3 Extensions of the Offline Scenario t t t t
Formally, ALG selects an action B at step t and ob-
t
An online version of our setup could involve gathering serves a logistic feedback Y ∼ Ber(σ(θTB )) for re-
t t
the entire action set D before the learner iteratively
ini wardparameterθ. Basedon(Y ) ,ALGestimatesθ
t t∈[T]
selects samples from D , using feedback at each step
ini and then for any input A ,n∈[N], plays some action
n
to inform the next selection.
aˆ (A )∈A . Theperformanceisstillevaluatedwith
T n n
However, it may be unrealistic to assume online feed- the simple regret
back at each step. A more feasible approach would
R(T,(A ) ,θ)= max⟨θ,a⋆ −aˆ(A )⟩.
involvesamplingabatch of pairs offline, sendingthem n n∈[N] n n
n∈An
to labelers for evaluation, and then selecting the next
We write P for the probability distribution of our
batch based on the feedback from the entire batch of θ
linear contextual bandit instance with reward param-
preferences. This setup is more practical, as it allows
eter θ over the whole possible action space A and
a dynamic process where labelers provide preferences
(Pθ,...,Pθ ) the probability distribution associated
for a batch of pairs before the next batch is sampled, b1 bL
with the different pairs of arms from B with the pa-
althoughtheydonothavetoprovidefeedbackateach
rameter θ. P as well as (Pθ′,...,Pθ′) stand for the
step (too complex in practice). The efficiency of the sameobjectsθ w′ ithparameterb1
θ′.
WithbL
inthissetup,we
method depends on the batch size - a batch of size 1
can use the divergence bound for general spaces from
mirrorstheonlinesettingandabatchofsizeT,which
Lattimore and Szepesv´ari (2020, 15.8), and write
recovers our offline setup.
A lot of approaches in practice use pairs sampled of- T
fline,followedbytrainingDPO(Rafailovetal.,2024)on D KL(P θ,P θ′)=(cid:88) E θ[D KL(P Aθ 1−A2,P Aθ′ 1−A2)] (20)
t t t t
theresultingdataset. Ourofflinestrategy,allowingthe t=1
selection of pairs that are statistically the most infor- T
=(cid:88) E [D (Ber(σ(θTB )),Ber(σ(θ′TB ))].
mative,couldsignificantlyenhanceDPO’sperformance, θ KL t t
without requiring too many additional computations. t=1
Itwouldbeinterestingtoinvestigatewhethertheopti-
Lemma 3. Assume that P and Q are probability mea-
maldatasetselectiondependsontherewardmodeling
sures on a measurable space X,A such that P is abso-
or the choice of the ψ-function within the particular
lutely continuous with respect to Q. Then
setup of Azar et al. (2024).
Another potential sampling strategy of the dataset D KL(P,Q)⩽log(1+D χ2(P,Q))⩽D χ2(P,Q).
could be to select points based on a binary search
procedure in Rd (Lobel et al., 2018), combined with If P≪Q does not hold, then the result is trivial.
bandit feedback. The reward parameter can be effi-
The result of this lemma is of great help to upper
ciently identified through cuts of the unit ball in Rd,
bound the divergence of our Bernoulli random vari-
whichiswhythisapproachisofsomeinterest. Finally,
ables since the χ2 divergence is easier to use with
we believe that our work has strong connections with
Bernoulli distributions. The proof is postponed to ap-
best-arm identification in linear bandits (Soare et al.,
pendix A.
2014; Degenne et al., 2020), particularly where ideas
fromOptimalDesign areapplied. Thechallengearises We now present our main theorem from this sec-
from the absence of online feedback in our setup, and tion, which gives a lower bound for our setup whiwh
one should look for relaxations in order to circumvent matches our upper bound from Corollary 1 up to con-
this issue. stant or logarithmic factors.Optimal Design for Reward Modeling in RLHF
Theorem 3. Suppose that d ⩾ 16 and that T ⩾ d2. contextual dueling bandits but do not provide all the
For any algorithm ALG which samples T pairs from proofs and rely on the work by Vaswani et al. (2019)
B and receives a preference feedback before outputing to bound the distance between their estimate and the
an action aˆ(A ) ∈ A for an input A , there exists true reward vector, although the bounds of Vaswani
n n n
(A ) ⊆B(0,1) as well as θ⋆ ∈B(0,1) such that et al. (2019) hold for an ordinary least square estima-
n n∈[N]
torandnotamaximumlikelihoodestimator-aharder
√
R(T,(A ) ,θ⋆)⩾de−5/4 T . taskbecauseofthelackofexplicitformoftheestima-
n n∈[N]
tor. Saha (2021) propose a very interesting approach
to transform a contextual dueling bandit setting into
Theself-concordancepropertyofthesigmoidfunction
a linear contextual linear bandit setting but leverage
(Bach,2010)isofsomeimportanceinthisboundsince
the iterative structure of the problem to do so, which
its properties play a role inequalities we work this.
is not possible in our case. Finally, Gabillon et al.
Thislowerboundallowsustoclaimthatourproposed
(2012) give important ideas around pure exploration
method is close from optimality. To our knowledge,
since it is one of the first and most important works
our work is the first contribution with such an evi-
in best-arm identification for multi-armed bandits.
denceofoptimalityforpreferencesdatasetselectionin
RLHF. A lot of empirical works have been done around the
problem of active learning and optimal choice of sam-
5 Related Work ples for diverse and informative collection of data
(Metz et al., 2023). In an online setup, Chen et al.
(2024a) propose an interesting approach to improve
The extraordinary capabilities of RLHF to fine-tune
the alignment with a reweighing of the generations to
large language models (Brown, 2020; Bubeck et al.,
improve the collected information while some theoret-
2023; Casper et al., 2023) are a reason for the grow-
ical foundations have been have already been laid out
ing attention in the field. The modeling of RLHF
by Lindner (2023); Wang et al. (2023). Our work has
as Markov Decision Processes (Wang et al., 2023) or
hope to stand at the crossroad of algorithmic founda-
bandits is a common assumption and has been pro-
tions and practical considerations. There are already
posed for various goals (Zhu et al., 2024; Mehta et al.,
seminal works in new directions which involve work-
2023). Our theoretical work involves comparing the
ing with off-policy evaluation for preference learning
feedback given as one action preferred to another,
(Bhargava et al., 2024) or active learning for choosing
whichisdirectlyrelatedtotheproblemofduelingban-
teaching examples (Wang et al., 2021). The few the-
dits (Yue et al., 2012; Gabillon et al., 2012; Sui et al.,
oretical attempts in our direction (Das et al., 2024; Ji
2018). Some foundations have been laid out to study
et al., 2024) consider an active learning setting where
preference-based and dueling bandits or RL (Pacchi-
theselectionofthesamplepairsisdoneconcomitantly
ano et al., 2021; Novoseller et al., 2020) while other
with the received feedback (online setup), which is an
works consider offline RL where the learning does not
unrealistic assumption due to the practical operation
result from interactions with the environment (Zhan
of the work with human labelers. Also, they propose
et al., 2023). As mentioned in the latter, an issue in
interesting methods but without lower bounds nor a
offlineRListheinsufficientcoverageofthespacewith
deep theoretical analysis.
the collected data. Interestingly, this is an issue that
we address here through optimal design. More generally, recent works consider learning from
human preferences, such as Mukherjee et al. (2024)
A lot of works compare online to offline RLHF (Hu
where the preference ordering over a list is learnt or
et al., 2023; Tang et al., 2024; Cen et al., 2024) but
Munos et al. (2023), where a Nash equilibrium is
here, we make the online-offline distinction for the
learnt. Instead of learning the preferences based on
dataset generation before the reward modeling and
a score, they can be learnt with the data being some
policyoptimizationstart;somethingthatalwaysneeds
preference pairs, hence the link with the dueling ban-
to be done offline in practice.
dit framework as some theoretical model (Yan et al.,
The human feedback in RLHF is usually given as a 2022).
preference between a pair of generations associated
Finally, weproposealowerboundforoursetup. Such
with a same context, hence the link with with contex-
bounds already exist for dueling bandits but rely on
tual dueling bandits (Dud´ık et al., 2015). Our setting
thesequentialstructureoftheproblemandaglobalre-
considers a binary feedback, which relates it to gen-
gretobjective(Saha,2021;Yueetal.,2012;Komiyama
eralized linear bandits (Filippi et al., 2010) and more
et al., 2015), something that we cannot do with the
precisely to logistic bandits (Lee and Oh, 2024; Lee
simple regret that we are looking for. This is why we
et al., 2024; Abeille et al., 2021; Faury et al., 2020).
seeourproblemasalogisticbanditandgobacktothe
Bengs et al. (2022) provide an interesting setup forA. Scheid, E. Boursier, A. Durmus, M.I. Jordan, P. M´enard, E. Moulines, M. Valko
traditional Bretagnolle-Huber inequality (Bretagnolle
andHuber,1979)tocontrolthebad eventsandobtain
the lower bound for our .
6 Conclusion
This paper addresses the problem of selecting pairs of
language model generations to present to labelers in
ordertomaximizetheinformationgatheredfromtheir
feedback. The goal is to develop an efficient strategy
for selecting which generations - or arms - should be
rated to retrieve the most valuable information before
fine-tuning the model. To tackle this, we build on
theframeworkofpureexplorationinlinearcontextual
duelingbandits,awell-suitedapproachforthespecific
taskthatwearelookingfor. Weoperateunderseveral
key assumptions: a linear reward; the Bradley-Terry
model that governs the preferences between pairs and
theboundednessoftheactionsetaswellasthereward
parameter.
Thecoreofourapproachliesinleveragingoptimalde-
sign techniques, which allow us to strategically choose
the arms. By doing so, we maximize the informa-
tion gained from each comparison, making the rat-
ing process highly efficient. Furthermore, by applying
information-theoretic tools, we derive a lower bound
for the performance of our method. Remarkably, this
lower bound matches our upper bound up to constant
andlogarithmicfactors,therebydemonstratingtheop-
timality of our approach.
Finally, we highlight that the techniques developed in
this work are not only theoretical but also closely re-
lated to practical methods used for selecting the pairs
andapplyingRLHF.Theresultssuggestthatourpro-
cedurecanbebothpracticalandhighlyeffective,offer-
ing a significant advancement in how LM generations
are selected before receiving human feedback prefer-
ences.Optimal Design for Reward Modeling in RLHF
Acknowledgements Brown, T. B. (2020). Language models are few-shot
learners. arXiv preprint arXiv:2005.14165.
Funded by the European Union (ERC, Ocean,
Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke,
101071601). Views and opinions expressed are how-
J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li,
everthoseoftheauthor(s)onlyanddonotnecessarily
Y., Lundberg, S., et al. (2023). Sparks of artificial
reflect those of the European Union or the European
general intelligence: Early experiments with gpt-4.
Research Council Executive Agency. Neither the Eu-
arXiv preprint arXiv:2303.12712.
ropean Union nor the granting authority can be held
Casper, S., Davies, X., Shi, C., Gilbert, T. K.,
responsible for them.
Scheurer, J., Rando, J., Freedman, R., Korbak,
T., Lindner, D., Freire, P., et al. (2023). Open
References
problems and fundamental limitations of reinforce-
mentlearningfromhumanfeedback. arXiv preprint
Abbasi-Yadkori, Y., P´al, D., and Szepesv´ari, C.
arXiv:2307.15217.
(2011). Improved algorithms for linear stochastic
bandits. Advances in neural information processing Cen, S., Mei, J., Goshvadi, K., Dai, H., Yang, T.,
systems, 24. Yang, S., Schuurmans, D., Chi, Y., and Dai, B.
(2024). Value-incentivized preference optimization:
Abeille, M., Faury, L., and Calauz`enes, C. (2021).
A unified approach to online and offline rlhf. arXiv
Instance-wiseminimax-optimalalgorithmsforlogis-
preprint arXiv:2405.19320.
ticbandits.InInternationalConferenceonArtificial
IntelligenceandStatistics,pages3691–3699.PMLR. Chang, J. D., Shan, W., Oertell, O., Brantley, K.,
Misra, D., Lee, J. D., and Sun, W. (2024). Dataset
Azar, M. G., Guo, Z. D., Piot, B., Munos, R., Row-
reset policy optimization for rlhf. arXiv preprint
land, M., Valko, M., and Calandriello, D. (2024). A
arXiv:2404.08495.
generaltheoreticalparadigmtounderstandlearning
from human preferences. In International Confer- Chen, L., Chen, J., Liu, C., Kirchenbauer, J., Soselia,
ence on Artificial Intelligence and Statistics, pages D.,Zhu,C.,Goldstein,T.,Zhou,T.,andHuang,H.
4447–4455. PMLR. (2024a). Optune: Efficientonlinepreferencetuning.
arXiv preprint arXiv:2406.07657.
Bach, F. (2010). Self-concordant analysis for logistic
regression. Chen, Y., Tan, J., Zhang, A., Yang, Z., Sheng, L.,
Zhang, E., Wang, X., and Chua, T.-S. (2024b). On
Bengs, V., Saha, A., and Hu¨llermeier, E. (2022).
softmax direct preference optimization for recom-
Stochastic contextual dueling bandits under lin-
mendation. arXiv preprint arXiv:2406.09215.
ear stochastic transitivity models. In International
Conference on Machine Learning, pages 1764–1786. Christiano, P. F., Leike, J., Brown, T., Martic, M.,
PMLR. Legg, S., and Amodei, D. (2017). Deep reinforce-
mentlearningfromhumanpreferences. Advances in
Bhargava, A., Jain, L., Kveton, B., Liu, G.,
neural information processing systems, 30.
and Mukherjee, S. (2024). Off-policy evalua-
tion from logged human feedback. arXiv preprint Das, N., Chakraborty, S., Pacchiano, A., and Chowd-
arXiv:2406.10030. hury, S. R. (2024). Provably sample efficient rlhf
via active preference optimization. arXiv preprint
Bıyık,E.,Huynh,N.,Kochenderfer,M.J.,andSadigh,
arXiv:2402.10500.
D. (2020). Active preference-based gaussian pro-
cess regression for reward learning. arXiv preprint Degenne, R., M´enard, P., Shang, X., and Valko, M.
arXiv:2005.02575. (2020). Gamification of pure exploration for linear
bandits. In International Conference on Machine
Bıyık,E.,Huynh,N.,Kochenderfer,M.J.,andSadigh,
Learning, pages 2432–2442. PMLR.
D. (2024). Active preference-based gaussian pro-
cess regression for reward learning and optimiza- Devlin, J. (2018). Bert: Pre-training of deep bidi-
tion. The International Journal of Robotics Re- rectional transformers for language understanding.
search, 43(5):665–684. arXiv preprint arXiv:1810.04805.
Bradley,R.A.andTerry,M.E.(1952). Rankanalysis Di, Q., Jin, T., Wu, Y., Zhao, H., Farnoud, F.,
ofincompleteblockdesigns: I.themethodofpaired and Gu, Q. (2023). Variance-aware regret bounds
comparisons. Biometrika, 39(3/4):324–345. for stochastic contextual dueling bandits. arXiv
preprint arXiv:2310.00968.
Bretagnolle, J. and Huber, C. (1979). Estima-
tion des densit´es: risque minimax. Zeitschrift fu¨r Dong, H., Xiong, W., Pang, B., Wang, H., Zhao,
Wahrscheinlichkeitstheorie und verwandte Gebiete, H., Zhou, Y., Jiang, N., Sahoo, D., Xiong, C.,
47:119–137. and Zhang, T. (2024). Rlhf workflow: FromA. Scheid, E. Boursier, A. Durmus, M.I. Jordan, P. M´enard, E. Moulines, M. Valko
reward modeling to online rlhf. arXiv preprint Lindner, D. (2023). Algorithmic Foundations for Safe
arXiv:2405.07863. and Efficient Reinforcement Learning from Human
Feedback. PhD thesis, ETH Zurich.
Dud´ık,M.,Hofmann,K.,Schapire,R.E.,Slivkins,A.,
and Zoghi, M. (2015). Contextual dueling bandits. Liu, Y. (2019). Roberta: A robustly opti-
In Conference on Learning Theory, pages 563–587. mized bert pretraining approach. arXiv preprint
PMLR. arXiv:1907.11692.
Faury, L., Abeille, M., Calauz`enes, C., and Fercoq, O. Lobel, I., Leme, R. P., and Vladu, A. (2018). Mul-
(2020). Improved optimistic algorithms for logistic tidimensional binary search for contextual decision-
bandits. In International Conference on Machine making. Operations Research, 66(5):1346–1361.
Learning, pages 3052–3060. PMLR.
Mehta, V., Das, V., Neopane, O., Dai, Y., Bogunovic,
Filippi, S., Cappe, O., Garivier, A., and Szepesv´ari, I., Schneider, J., and Neiswanger, W. (2023). Sam-
C. (2010). Parametric bandits: The generalized lin- ple efficient reinforcement learning from human
earcase. Advances in neural information processing feedback via active exploration.
systems, 23.
Metz, Y., Lindner, D., Baur, R., Keim, D., and El-
Gabillon, V., Ghavamzadeh, M., and Lazaric, A. Assady, M. (2023). Rlhf-blender: A configurable
(2012). Best arm identification: A unified approach interactiveinterfaceforlearningfromdiversehuman
to fixed budget and fixed confidence. Advances in feedback. arXiv preprint arXiv:2308.04332.
Neural Information Processing Systems, 25.
Mukherjee, S., Lalitha, A., Kalantari, K., Deshmukh,
Houlsby,N.,Giurgiu,A.,Jastrzebski,S.,Morrone,B., A., Liu, G., Ma, Y., and Kveton, B. (2024). Op-
De Laroussilhe, Q., Gesmundo, A., Attariyan, M., timal design for human feedback. arXiv preprint
and Gelly, S. (2019). Parameter-efficient transfer arXiv:2404.13895.
learningfornlp. InInternational conference on ma-
Munos, R., Valko, M., Calandriello, D., Azar, M. G.,
chine learning, pages 2790–2799. PMLR.
Rowland,M.,Guo,Z.D.,Tang,Y.,Geist,M.,Mes-
Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, nard,T.,Michi,A.,etal.(2023).Nashlearningfrom
Y., Wang, S., Wang, L., and Chen, W. (2021). human feedback. arXiv preprint arXiv:2312.00886.
Lora: Low-rank adaptation of large language mod-
Naveed, H., Khan, A. U., Qiu, S., Saqib, M., Anwar,
els. arXiv preprint arXiv:2106.09685.
S.,Usman,M.,Akhtar,N.,Barnes,N.,andMian,A.
Hu, J., Tao, L., Yang, J., and Zhou, C. (2023). (2023). Acomprehensiveoverviewoflargelanguage
Aligning language models with offline reinforce- models. arXiv preprint arXiv:2307.06435.
mentlearningfromhumanfeedback. arXiv preprint
Novoseller, E., Wei, Y., Sui, Y., Yue, Y., andBurdick,
arXiv:2308.12050.
J.(2020). Duelingposteriorsamplingforpreference-
Ji,K.,He,J.,andGu,Q.(2024). Reinforcementlearn- basedreinforcementlearning. InConference on Un-
ingfromhumanfeedbackwithactivequeries. arXiv certainty in Artificial Intelligence,pages1029–1038.
preprint arXiv:2402.09401. PMLR.
Komiyama, J., Honda, J., Kashima, H., and Naka-
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wain-
gawa, H. (2015). Regret lower bound and optimal
wright, C., Mishkin, P., Zhang, C., Agarwal, S.,
algorithminduelingbanditproblem. InConference
Slama, K., Ray, A., etal.(2022). Traininglanguage
on learning theory, pages 1141–1154. PMLR.
models to follow instructions with human feedback.
Lattimore, T. and Szepesv´ari, C. (2020). Bandit algo- Advances in neural information processing systems,
rithms. Cambridge University Press. 35:27730–27744.
Lee, J. and Oh, M.-h. (2024). Nearly minimax op- Pacchiano,A.,Saha,A.,andLee,J.(2021).Duelingrl:
timal regret for multinomial logistic bandit. arXiv reinforcement learning with trajectory preferences.
preprint arXiv:2405.09831. arXiv preprint arXiv:2111.04850.
Lee, J., Yun, S.-Y., and Jun, K.-S. (2024). Improved Plackett, R. L. (1975). The analysis of permutations.
regret bounds of (multinomial) logistic bandits via Journal of the Royal Statistical Society Series C:
regret-to-confidence-setconversion. InInternational Applied Statistics, 24(2):193–202.
Conference on Artificial Intelligence and Statistics,
Rafailov, R., Sharma, A., Mitchell, E., Manning,
pages 4474–4482. PMLR.
C.D.,Ermon,S.,andFinn,C.(2024).Directprefer-
Lester, B., Al-Rfou, R., and Constant, N. (2021). The ence optimization: Your language model is secretly
powerofscaleforparameter-efficientprompttuning. a reward model. Advances in Neural Information
arXiv preprint arXiv:2104.08691. Processing Systems, 36.Optimal Design for Reward Modeling in RLHF
Reimers, N. (2019). Sentence-bert: Sentence embed- Wei, C., Wang, Y.-C., Wang, B., and Kuo, C.-C. J.
dings using siamese bert-networks. arXiv preprint (2023). An overview on language models: Re-
arXiv:1908.10084. cent developments and outlook. arXiv preprint
arXiv:2303.05759.
Saha, A. (2021). Optimal algorithms for stochastic
contextual preference bandits. Advances in Neural Yan, X., Luo, C., Clarke, C. L., Craswell, N.,
Information Processing Systems, 34:30050–30062. Voorhees, E. M., and Castells, P. (2022). Human
preferences as dueling bandits. In Proceedings of
Schulman, J., Wolski, F., Dhariwal, P., Radford, A.,
the 45th international ACM SIGIR conference on
and Klimov, O. (2017). Proximal policy optimiza-
research and development in information retrieval,
tion algorithms. arXiv preprint arXiv:1707.06347.
pages 567–577.
Shen, J. H., Sharma, A., and Qin, J. (2024). Yue, Y., Broder, J., Kleinberg, R., and Joachims, T.
Towards data-centric rlhf: Simple metrics for (2012).Thek-armedduelingbanditsproblem.Jour-
preference dataset comparison. arXiv preprint nal of Computer and System Sciences, 78(5):1538–
arXiv:2409.09603. 1556.
Soare, M., Lazaric, A., and Munos, R. (2014). Best- Zhan,W.,Uehara,M.,Kallus,N.,Lee,J.D.,andSun,
arm identification in linear bandits. Advances in W. (2023). Provable offline reinforcement learning
Neural Information Processing Systems, 27. withhumanfeedback. InICML2023WorkshopThe
Many Facets of Preference-Based Learning.
Steck, H., Ekanadham, C., and Kallus, N. (2024). Is
cosine-similarityofembeddingsreallyaboutsimilar- Zhao, Z., Fan, W., Li, J., Liu, Y., Mei, X., Wang, Y.,
ity? InCompanionProceedingsoftheACMonWeb Wen,Z.,Wang,F.,Zhao,X.,Tang,J.,etal.(2023).
Conference 2024, pages 887–890. Recommender systems in the era of large language
models (llms). arXiv preprint arXiv:2307.02046.
Sui, Y., Zoghi, M., Hofmann, K., and Yue, Y. (2018).
Zhu, B., Jordan, M. I., and Jiao, J. (2024). Iter-
Advancements in dueling bandits. In IJCAI, pages
ative data smoothing: Mitigating reward overfit-
5502–5510.
ting and overoptimization in rlhf. arXiv preprint
Tang,Y.,Guo,D.Z.,Zheng,Z.,Calandriello,D.,Cao, arXiv:2401.16335.
Y.,Tarassov,E.,Munos,R.,Pires,B.A´.,Valko,M.,
Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B.,
Cheng, Y., et al. (2024). Understanding the per-
Radford,A.,Amodei,D.,Christiano,P.,andIrving,
formance gap between online and offline alignment
G.(2019).Fine-tuninglanguagemodelsfromhuman
algorithms. arXiv preprint arXiv:2405.08448.
preferences. ArXiv, abs/1909.08593.
Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
Lachaux,M.-A.,Lacroix,T.,Rozi`ere,B.,Goyal,N.,
Hambro, E., Azhar, F., et al. (2023). Llama: Open
and efficient foundation language models. arXiv
preprint arXiv:2302.13971.
Tucker,M.,Novoseller,E.,Kann,C.,Sui,Y.,Yue,Y.,
Burdick,J.W.,andAmes,A.D.(2020). Preference-
based learning for exoskeleton gait optimization. In
2020 IEEE international conference on robotics and
automation (ICRA), pages 2351–2357. IEEE.
Vaswani, S., Mehrabian, A., Durand, A., and Kve-
ton, B. (2019). Old dog learns new tricks: Ran-
domized ucb for bandit problems. arXiv preprint
arXiv:1910.04928.
Wang, C., Singla, A., and Chen, Y. (2021). Teaching
an active learner with contrastive examples. Ad-
vances in Neural Information Processing Systems,
34:17968–17980.
Wang, Y., Liu, Q., and Jin, C. (2023). Is rlhf more
difficult than standard rl? a theoretical perspec-
tive. Advances in Neural Information Processing
Systems, 36:76006–76032.A. Scheid, E. Boursier, A. Durmus, M.I. Jordan, P. M´enard, E. Moulines, M. Valko
A Proofs
Lemma 4. For any x ∈ R,σ′(x) > 0 and for any interval of the form [−α,β] for α,β > 0, we have that σ′ is
increasing on [−α,0] and decreasing on [0,β].
Proof of Lemma 4. Note that for any x∈R we have
σ′(x)=e−x/(1+e−x)2 and σ′′(x)=e−x(e−x−1)/(1+e−x)3 ,
and we observe that σ′′ cancels out in 0, is positive on R⋆ and negative on R⋆, hence the result.
− +
Lemma 1. We can differentiate the likelihood defined in (8), and obtain
∇ L({A1,A2,Y } ,θ) (10)
θ s s s s∈[t−1]
t−1
(cid:88)
= (Y −P (Y =1))(A1−A2)−λθ ,
s θ s s s
s=1
which gives by definition of the maximum likelihood estimator in (9), that θˆ must satisfy
t
t−1
(cid:88) (Y −P (Y =1))(A1−A2)−λθˆ =0. (11)
s θˆ
t
s s s t
s=1
Proof of Lemma 1. As mentioned in the main text, a direct computation gives that
t−1
L ({A1,A2,Y } ,θ)=(cid:88)(cid:8) Y log(σ(⟨θ,A1−A2⟩))+(1−Y )log(σ(−⟨θ,A1−A2⟩))(cid:9) −λ∥θ∥2/2
t s s s s∈[t−1] s s s s s s 2
s=1
t−1(cid:26) (cid:18) (cid:19) (cid:18) (cid:19)(cid:27)
(cid:88) 1 1
= Y log +(1−Y )log −λ∥θ∥2/2,
s 1+e−⟨θ,A1 s−A2 s⟩ s 1+e⟨θ,A1 s−A2 s⟩ 2
s=1
and Lemma 4 offers an expression for the differential of the sigmoid, which gives that
∇ L ({A1,A2,Y }
,θ)=(cid:88)t−1(cid:40)
Y
(A1 s−A2 s)e−⟨θ,A1 s−A2 s⟩(1+e−⟨θ,A1 s−A2
s⟩)−2(cid:41)
θ t s s s s∈[t−1] s (1+e−⟨θ,A1 s−A2 s⟩)−1
s=1
+(cid:88)t−1(cid:40)
−(1−Y
)(A1 s−A2 s)e⟨θ,A1 s−A2 s⟩(1+e⟨θ,A1 s−A2
s⟩)−2(cid:41)
−λθ
s (1+e⟨θ,A1 s−A2 s⟩)−1
s=1
=(cid:88)t−1(cid:40)
Y (A1−A2)
e−⟨θ,A1 s−A2 s⟩
+(Y −1)(A1−A2)
e⟨θ,A1 s−A2 s⟩
(cid:41)
−λθ
s s s 1+e−⟨θ,A1 s−A2 s⟩ s s s 1+e⟨θ,A1 s−A2 s⟩
s=1
t−1 (cid:26) (cid:27)
(cid:88) 1 1
= (A1−A2) Y +(Y −1) −λθ
s s s 1+e⟨θ,A1 s−A2 s⟩ s 1+e−⟨θ,A1 s−A2 s⟩
s=1
t−1
(cid:88)
= (A1−A2){Y (1−P (Y =1))+(Y −1)P (Y =1)}−λθ
s s s θ s s θ s
s=1
t−1
(cid:88)
= (A1−A2)(Y −P (Y =1))−λθ ,
s s s θ s
s=1
hencetheresult. Bydefinitionofthemaximumlikelihoodestimator,θˆ satisfies∇ L ({A1,A2,Y } ,θˆ)=0,
t θ t s s s s∈[t−1] t
and therefore we obtain (11).
Lemma 2. Under H1, for any δ ∈(0,1), with probability at least 1−δ, we have that
∥θˆP −θ⋆∥ ⩽
t Vt
(cid:20)(cid:113) √ (cid:21)
(cid:0) (cid:1)
20 2log(1/δ)+d log λ1−1/d+4t/dλ1/d + λ .Optimal Design for Reward Modeling in RLHF
Proof of Lemma 2. Our proof is inspired by results from Di et al. (2023); Ji et al. (2024). For any t ∈ [T +1],
recall the definition of H ; we now define
t
t−1
(cid:88)
X =Y −P(Y =1)=1(A1 ≻A2)−σ(⟨θ⋆,A1−A2⟩) and Z = X (A1−A2).
t t t t t t t t s s s
s=1
As we saw in (12), by definition of the maximum likelihood estimator, and θ⋆ as the true reward parameter, we
have that
t−1 t−1
H
(θˆ)=(cid:88)
Y (A1−A2) and H
(θ⋆)=λθ⋆+(cid:88)
P(Y =1)(A1−A2),
t t s s s t s s s
s=1 s=1
where P stands for the true reward distribution, according to the parameter θ⋆. Therefore
t−1
H (θˆ)−H (θ⋆)=(cid:88) [Y −P(Y =1)](A1−A2)−λθ⋆ =Z −λθ⋆ . (21)
t t t s s s s t
s=1
WenowconsiderthedifferenceH (θ )−H (θ )forarbitraryθ ,θ inRd,andapplyafirstorderTaylorexpansion
t 1 t 2 1 2
with integral remainder to the function θ (cid:55)→ σ(⟨θ,A1 −A2⟩) on the space Rd in each term of the sum, which
s s
leads to
t−1
(cid:88)
H (θ )−H (θ )=λθ −λθ + (σ(θT(A1−A2))−σ(θT(A1−A2)))(A1−A2)
t 1 t 2 1 2 1 s s 2 s s s s
s=1
t−1 (cid:90) 1
(cid:88)
=λ(θ −θ )+ (A1−A2) (A1−A2)Tσ′(⟨θ +u(θ −θ ),A1−A2⟩)(θ −θ )du
1 2 s s s s 1 2 1 s s 1 2
s=1 u=0
(cid:34) t−1 (cid:90) 1 (cid:35)
(cid:88)
= λI+ (A1−A2)(A1−A2)T σ′(⟨θ +u(θ −θ ),A1−A2⟩)du (θ −θ ).
s s s s 1 2 1 s s 1 2
s=1 u=0
We define for any t∈[T],θ ,θ ∈Rd
1 2
t−1 (cid:90) 1
(cid:88)
G (θ ,θ )=λI+ (A1−A2)(A1−A2)T σ′(⟨θ +u(θ −θ ),A1−A2⟩)du≻0,
t 1 2 s s s s 2 1 2 s s
s=1
u=0(cid:124) (cid:123)(cid:122) (cid:125)
⩾0
since λ is chosen such that λ > 0. Note that for any t,θ ,θ : G (θ ,θ ) is symmetric. By definition, we have
1 2 t 1 2
that for any θ ,θ ∈Rd
1 2
H (θ )−H (θ )=G (θ ,θ )(θ −θ ),
t 1 t 2 t 1 2 1 2
which gives that
(cid:113)
∥θ −θ ∥ = (θ ,θ )G G−1G (θ ,θ )=∥H (θ )−H (θ )∥ . (22)
1 2 Gt(θ1,θ2) 1 2 t t t 1 2 t 1 t 2 G− t1(θ1,θ2)
Using Lemma 4, since σ′(−2)⩾0.1 and σ′(2)⩾0.1, we have that for any x∈[−2,2],σ′(x)⩾0.1. Note that for
any s ∈ [T],θ ∈ B(0,1),⟨θ,A1 −A2⟩ ∈ [−2,2]. Therefore, under H1, a convexity argument gives that for any
s s
θ
∈B(0,1),u∈[0,1],(cid:10) θ+u(θ⋆−θ),A1−A2(cid:11)
∈[−2,2], and we obtain
s s
(cid:90) 1
σ′(⟨θˆP +u(θ⋆−θˆP),A1−A2⟩)du⩾0.1,
t t s s
u=0
since θˆP ∈B(0,1). Note that we use here the fact that θˆP lies in the unit ball to control the boundedness of the
t t
sigmoid function. This leads to
(cid:32) t−1 (cid:33)−1
V ≺10G (θ⋆,θˆP) and G (θ⋆,θˆP)−1 ⪯10 λI+(cid:88) (A1−A2)(A1−A2)T =10V−1 , (23)
t t t t t s s s s t
s=1A. Scheid, E. Boursier, A. Durmus, M.I. Jordan, P. M´enard, E. Moulines, M. Valko
and we obtain
√
∥θ⋆−θˆP∥ ⩽ 10∥θ⋆−θˆP∥
t Vt
√
t Gt(θ⋆,θˆ tP)
= 10∥H (θ⋆)−H (θˆP)∥
t t t G−1(θ⋆,θˆP)
t t
⩽10∥H (θˆP)−H (θ⋆)∥
t t t V−1
t
⩽10(∥H (θˆ)−H (θ⋆)∥ +∥H (θˆP)−H (θˆ)∥ )
t t t V−1 t t t t V−1
t t
⩽20∥H (θˆ)−H (θ⋆)∥
t t t V−1
t
=20∥Z −λθ⋆∥
t V−1
t√
⩽20(∥Z ∥ + λ),
t V−1
t
where the second line holds by (22), the third line by Equation (23), the fourth by the triangular inequality,
the fifth by definition of θˆP (13) and the penultimate by (21). Note that for any t ∈ [T], X ∈ [−1,1] and is
t t
therefore 1-subgaussian by Hoeffding inequality. Therefore, we apply Abbasi-Yadkori et al. (2011, Theorem 1),
which gives that with probability at least 1−δ, we have
(cid:13) (cid:13)2
∥Z ∥2 =(cid:13) (cid:13)(cid:88)t−1 X (A1−A2)(cid:13) (cid:13) ⩽2log(cid:16)(cid:112) det(V )/((cid:112) det(V )δ)(cid:17) ⩽2log(1/δ)+log(detV /detV ).
t V−1 (cid:13) s s s (cid:13) t 0 t 0
t (cid:13) (cid:13)
s=1 V−1
t
By definition, we have that for any t⩾1
t−1
(cid:88)
V =V + (A1−A2)(A1−A2)T ,
t 0 s s s s
s=1
and now using Lattimore and Szepesv´ari (2020, Lemma 19.4), we can write
(cid:18) (cid:19) (cid:18) (cid:19)
Tr(V )+4(t−1) dλ+4(t−1)
log(detV /detV )⩽dlog 0 =dlog .
t 0 ddet(V )1/d dλ1/d
0
We finally obtain that with probability at least 1−δ
(cid:20)(cid:113) √ (cid:21)
∥θˆP −θ⋆∥ ⩽20 2log(1/δ)+d log(cid:0) λ1−1/d+4t/dλ1/d(cid:1) + λ ,
t Vt
hence the result.
Theorem 1. Letε>0andsupposethatwecollectatleastT ⩾d2 samplesaccordingtoan(1+ε)approximation
πˆ of the optimal design policy π⋆ for the problem. Then, for any B and θ⋆ ∈ B(0,1), with probability at least
1−δ, δ ∈(0,1), we have that
(cid:112)
R(T,(A ) ,θ⋆)⩽20(1+ε) d/T × (18)
n n∈[N]
(cid:20)(cid:113) √ (cid:21)
(cid:0) (cid:1)
2log(1/δ)+d log λ1−1/d+4T/dλ1/d + λ .
Proof of Theorem 1. The condition T ⩾ d(d+1)/2 ensures that we collect enough points so that the optimal
design policy πˆ satisfies the results from Theorem 4. Using the decomposition of the regret (16) and Lemma 2,
we obtain that with a probability at least 1−δ
(cid:20)(cid:113) √ (cid:21)
R(T,(A ) ,θ⋆)⩽20 2log(1/δ)+d log(cid:0) λ1−1/d+4T/dλ1/d(cid:1) + λ max ∥a−a′∥ . (24)
n n∈[N] V−1
(a,a′)∈D2 T+1
ini
Let πˆ be an 1+ε approximation of the optimal design policy π⋆. For any distribution π, we have V˜(π) =
(cid:80) π(b)bbT. The regularized design matrix based on the collected samples from πˆ is defined as V =
b∈B T+1Optimal Design for Reward Modeling in RLHF
λI +(cid:80)T B(t)B(t)T =λI +(cid:80) ⌈T πˆ ⌉bbT with B(t)=A1−A2, since the samples (A1,A2) are chosen
t=1 b∈B b t t t t t∈[T]
according to ODPO. Therefore, for any b∈B, we have that
 −1
∥b∥2 V−1 =bT λI+(cid:88) ⌈T πˆ ˜b⌉˜b˜bT  b
T+1
˜b∈B
 −1
⩽b(cid:88) T πˆ ˜b˜b˜bT  b
˜b∈B
 −1
= T1 bT (cid:88) πˆ ˜b˜b˜bT  b
˜b∈B
1
= ∥b∥2
T
V˜−1(πˆ)
=(1+ε)d/T ,
wherethelastlineholdsthankstoAlgorithm2andresultsonitsconvergence(see,e.g., LattimoreandSzepesv´ari,
2020, 21.2). It gives that
(cid:112)
max ∥a−a′∥ ⩽ (1+ε)d/T ,
V−1
(a,a′)∈Dini T+1
and plugging this bound in (24), we obtain the result.
Corollary 1. Suppose that we have selected the samples D to label under πˆ, a 3/2-approximation of the
select
optimal design policy π⋆. Choosing λ = 1/d for the regularization, for any δ ∈ (0,1), under the conditions of
Theorem 1, with probability at least 1−δ, we have that
(cid:112)
R(T,(A ) ,θ⋆)⩽30 d/T×
n n∈[N]
(cid:20)(cid:113) √ (cid:21)
2log(1/δ)+dlog((1+4T)/d1−1/d)+1/ d ,
and as a consequence, choosing δ =d1−1/d/(4T +1), we can bound the expectation of the regret as
(cid:115)
(cid:18) (cid:19)
d+2 4T +1
E[R(T,(A ) ,θ⋆)]⩽30 √ log
n n∈[N] T d1−1/d
√
+31/ T .
Proof of Corollary 1. By Equation (18), we have that for any δ ∈(0,1)
R(T,(A )
,θ⋆)⩽20(1+ε)(cid:112)
d/T(cid:20)(cid:113)
2log(1/δ)+d
log(cid:0) λ1−1/d+4T/dλ1/d(cid:1)
+√ λ(cid:21)
, (25)
n n∈[N]
and we now choose πˆ to be a 3/2-approximation of the optimal policy π⋆ as well as λ = 1/d. Plugging these
quantities in (25) gives
R(T,(A ) ,θ⋆)⩽20 ×3/2 ×(cid:112) d/T
×(cid:20)(cid:113)
2log(1/δ)+d log(cid:0) d1/d−1+4T d1/d/d(cid:1)
+1/√ d(cid:21)
n n∈[N]
(cid:20)(cid:113) √ (cid:21)
(cid:112)
=30 d/T × 2log(1/δ)+dlog((1+4T)/d1−1/d)+1/ d ,
hence the first part of the corollary. Observe that since R(T,(A ) ,θ⋆) =
n n∈[N]
max max ⟨θ⋆,a⋆ −aˆ (A )⟩, we have that R(T,(A ) ,θ⋆) ⩽ 2 under H 1. Therefore, for
n∈[N] a⋆ n∈An n T n n n∈[N]
any C>0, we can write
E(cid:2)
R(T,(A )
,θ⋆)(cid:3)⩽P(cid:0)
R(T,(A )
,θ⋆)⩽C(cid:1) ·C+2·(1−P(cid:0)
R(T,(A )
,θ⋆)⩽C(cid:1)
), (26)
n n∈[N] n n∈[N] n n∈[N]A. Scheid, E. Boursier, A. Durmus, M.I. Jordan, P. M´enard, E. Moulines, M. Valko
(cid:112) (cid:104)(cid:112) √ (cid:105)
and we now apply (26) with C = 30 d/T × 2log(1/δ)+dlog((1+4T)/d1−1/d)+1/ d and δ =
d1−1/d/(4T +1). The first part of the corollary that we already proved gives
(cid:20)(cid:113) √ (cid:21)
E(cid:2)
R(T,(A )
,θ⋆)(cid:3)⩽(1−d1−1/d/(4T +1))×30(cid:112)
d/T × 2log((4T +1)/d1−1/d)+dlog((1+4T)/d1−1/d)+1/ d
n n∈[N]
+2×d1−1/d/(4T +1)
(cid:112) (cid:34)(cid:115) (cid:18) 4T +1(cid:19) √ (cid:35)
⩽30 d/T (d+2)log +1/ d +d/2T
d1−1/d
(cid:115)
d+2 (cid:18) 4T +1(cid:19) √ d
⩽30 √ log +30/ T +
T d1−1/d 2T
(cid:115)
d+2 (cid:18) 4T +1(cid:19) √
⩽30 √ log +31/ T ,
T d1−1/d
where the last line holds since T ⩾d(d+1)/2. Hence the second part of the result.
Theorem 2. Consider the 2-dimensional euclidian space Span(e ,e ) as the whole action space. In that case,
1 2
there exists a set of actions (A ) and some θ⋆ ∈ B(0,1) such the regret defined in (19) for any algorithm
t t∈[T]
ALG satisfies
R (T,(A ) ,θ⋆)⩾e−c/2,
ALG t t∈[T]
for some c>0 independent of T and d.
Proof of Theorem 2. Consider the space R2 with an orthonormal basis (e ,e ). Suppose that θ⋆ ∈ Θ = {±e }
1 2 2
and that the action sets are A =...=A ={±e } and A ={±e }. After sampling data and preferences,
1 T−1 1 T 2
ALG outputs an action aˆ and is then evaluated with the simple regret
R(T,(A ) ,θ)=maxmax⟨θ,a−aˆ(A )⟩,
n n∈[N] t
t∈[T]a∈At
and we consider that ALG sampled arms {(A1,A2)} . We write A1 −A2 = B . For any t ∈ [T −1],B ∈
t t t∈[T] t t t t
Span(e ). Now consider the events {aˆ = e } and {aˆ = −e }. We write θ = e and θ′ = −e . We have
1 2 2 2 2
max ⟨θ,a⟩=max ⟨θ′,a⟩=1. We now use the Bretagnolle-Huber inequality and write
a∈B2(0,1) a∈B2(0,1)
P ({aˆ=−e })+P ({aˆ=e })=P ({aˆ=−e })+P ({aˆ=−e }c)⩾exp(−D (P ,P ))/2, (27)
θ 2 θ′ 2 θ 2 θ′ 2 KL θ θ′
as well as Lattimore and Szepesv´ari (2020, 15.8) to obtain
(cid:34) T (cid:35)
D (P ,P )=E (cid:88) D (Pθ ,Pθ′ )
KL θ θ′ KL Bt Bt
t=1
(cid:34)T−1 (cid:35)
(cid:88)
=E D (Ber(σ(θTB )),Ber(σ(θ′TB ))) +E [D (Ber(σ(θTB )),Ber(σ(θ′TB )))]
θ KL t t θ KL T T
t=1
=E [D (Ber(σ(θTB )),Ber(σ(θ′TB )))],
θ KL T T
since θ,θ′ ∈ Span(e ),B ∈ Span(e ) for any t ∈ [T − 1], which gives θ′TB = θTB = 0. Therefore
2 t 1 t t
D (Ber(σ(θTB )),Ber(σ(θ′TB ))) = D (Ber(1/2),Ber(1/2)) = 0 for any t ∈ [T −1]. There exists a con-
KL t t KL
stant c > 0 independent of T and the dimension such that D (Ber(σ(θTB )),Ber(σ(θ′TB ))) ⩽ c. Thus, at
KL T T
least one of the terms in the left-hand side of (27) is bigger than exp(−c)/4 - say P ({aˆ = e }) ⩾ exp(−c)/4.
θ′ 2
Whichonebeingbiggerthanexp(−c)/4doesnotmatterbysymmetry. Theregretincurredunderθ′ for{aˆ=e }
2
holding is 2 and we finally obtain that
R (T,(A ) ,θ′)⩾e−c/2.
ALG t t∈[T]Optimal Design for Reward Modeling in RLHF
Lemma 3. Assume that P and Q are probability measures on a measurable space X,A such that P is absolutely
continuous with respect to Q. Then
D (P,Q)⩽log(1+D (P,Q))⩽D (P,Q).
KL χ2 χ2
If P≪Q does not hold, then the result is trivial.
Proof of Lemma 3. By definition of the KL-divergence, we can write
(cid:90) (cid:18) dP(cid:19)
D (P,Q)= log dP,
KL dQ
X
and applying Jensen’s inequality with the logarithm, we obtain
(cid:18)(cid:90) dP (cid:19) (cid:32) (cid:90) (cid:18) dP(cid:19)2 (cid:33) (cid:32) (cid:90) (cid:18) dP(cid:19)2 (cid:33)
D (P,Q)⩽log dP =log dQ =log dQ−1+1 =log(D +1).
KL dQ dQ dQ χ2
X X X
Finally, using the inequality log(1+x)⩽x for any x>−1 allows us to conclude.
Theorem 3. Suppose that d ⩾ 16 and that T ⩾ d2. For any algorithm ALG which samples T pairs from
B and receives a preference feedback before outputing an action aˆ(A ) ∈ A for an input A , there exists
n n n
(A ) ⊆B(0,1) as well as θ⋆ ∈B(0,1) such that
n n∈[N]
√
R(T,(A ) ,θ⋆)⩾de−5/4 T .
n n∈[N]
(cid:112)
Proof of Theorem 3. We first restrict θ to belong to the set Θ={± d/T}d ⊆B(0,1). Let i∈[d] and θ,θ′ ∈Θ
such that for any j ∈[d],j ̸=i,θ =θ′ and θ′ =−θ . For any prediction aˆ output by ALG, we define the event
j j i i
A ={sgn(aˆ )=−sgn(θ )},
i,θ i i
as well as the corresponding probability
p(θ,i)=P (A )=P ({sgn(aˆ )=−sgn(θ )}).
θ i,θ θ i i
√
Consider the action set A = [±1/ d]d. Note that Ac = {sgn(aˆ ) = sgn(θ )} = {sgn(aˆ ) = −sgn(θ′)}. We now
i,θ i i i i
apply Bretagnolle-Huber’s inequality to obtain
P (A )+P (Ac )⩾exp(−D (P ,P ))/2. (28)
θ i,θ θ′ i,θ KL θ θ′
Now using the expression of the divergence from (20) as well as Lemma 3, we can write
T
D (P ,P )⩽(cid:88) E (cid:104) D (Ber(σ(θTB )),Ber(σ(θ′TB )))(cid:105) .
KL θ θ′ θ χ2 t t
t=1
Since D (Ber(p),Ber(q))=(p−q)2/q2, we can write
χ2
T
D (P ,P )⩽(cid:88) E (cid:104) (σ(θTB )−σ(θ′TB ))2/σ(θ′TB )(1−σ(θ′TB ))(cid:105) .
KL θ θ′ θ t t t t
t=1
For any x ∈ Rd,σ(x) = 1/(1 + e−x), which gives that 1/σ(x)(1 − σ(x)) = ex(1 + e−x)2 and we define
f: R→R,x(cid:55)→ex(1+e−x)2. A derivation shows that f′ cancels out in 0, is negative on R and positive on R .
− +
Therefore, for any x∈[−1/2,1/2],f(x)⩽max{f(−1/2),f(1/2)}⩽5.
We now define g: [0,1] → R,v (cid:55)→ σ(θ′TB + v(θ − θ′T)TB ). We have that g(1) = σ(θTB ) while
t t t
g(0)=σ(θ′TB ), which allows us to write
t
σ(θTB )−σ(θ′TB )=g(1)−g(0)
t t
(cid:90) 1
= (θ−θ′ )TB σ′(θ′TB +v(θ−θ′T)TB )dv
t t t
u=0
(cid:90) 1
= σ′(θ′TB +v(θ−θ′T)TB )dv (θ−θ′ )TB .
t t t
u=0A. Scheid, E. Boursier, A. Durmus, M.I. Jordan, P. M´enard, E. Moulines, M. Valko
As we showed in the proof of Theorem 1, we have that for any x∈[−2,2],σ′(x)⩽σ′(0)=1/4 and therefore, we
obtain
σ(θTB )−σ(θ′TB )⩽(θ−θ′ )TB /4.
t t t
Plugging the different inequalities together gives that
T
D (P ,P )⩽(cid:88) E (cid:104) 5((θ−θ′ )TB )2/16(cid:105)
KL θ θ′ θ t
t=1
T
=5/16(cid:88) E (cid:104) ((θ−θ′ )TB )2(cid:105)
θ t
t=1
T
(cid:88)
=5/16 16θ2/d
i
t=1
=5,
√
where the last penultimate line holds since ∥B ∥2 ⩽4/d because of A⊆[±1/ d]d and θ−θ′ =2θ e where e
t ∞ (cid:112) i i i
stands for the i-th basis vector. The last line holds since θ ∈ {± d/T}. Finally, plugging this inequality in
i
(28) gives that
p(θ,i)+p(θ′,i)⩾e−5/2. (29)
We now apply the ”averaging hammer” technique, which consists in summing all the p(θ,i) for θ ∈ Θ,i ∈ [d]
and group the term that differ in only one coordinate. It gives that
d d
(cid:88) (cid:88) (cid:88)(cid:88)
1/|Θ| p(θ,i)=1/|Θ| p(θ,i),
θ∈Θ i=1 i=1θ∈Θ
and we reckon that 2d−1 pairs appear as in (29), which gives that
d d
(cid:88) (cid:88) (cid:88)
1/|Θ| p(θ,i)⩾1/|Θ| 2d−1e−5/2=de−5/4,
θ∈Θ i=1 i=1
since Card(Θ) = 2d (hypercube). Therefore, there exists at least on θ⋆ ∈ Θ such that (cid:80)d p(θ,i) ⩾ de−5/4.
√ √ i=1
Still considering the action set A=[−1/ d,1/ d]d ⊆B(0,1), we can lower bound the regret
(cid:34) d √ (cid:35)
(cid:88)
R(T,(A ) ,θ⋆)⩾E (sgn(θ⋆)/ d−aˆ )θ⋆
n n∈[N] θ⋆ i i i
i=1
d √
(cid:88)
⩾ P (sgn(θ⋆)̸=sgn(aˆ ))|θ⋆|/ d
θ⋆ i i i
i=1
d
1 (cid:88)
= √ P (sgn(θ⋆)̸=sgn(aˆ ))
θ⋆ i i
T
i=1
√
⩾de−5/(4 T).
B Supplementary theorems and algorithms
Theorem 4 (Kiefer-Wolfowitz). Assume that the action set B is such that Span(B) = Rd. Since B ⊆ B(0,2)
and B is finite, B is compact. Therefore, the following are equivalent
• π⋆ is a minimizer of g,Optimal Design for Reward Modeling in RLHF
• π⋆ is a maximizer of π (cid:55)→logdetV˜(π),
• g(π⋆)=d,
where the quantities g and V˜ are defined in (17). Furthermore, there exists such a π⋆ with a support of size
smaller than d(d+1)/2.
Algorithm 2 FW: Frank-Wolfe Algorithm
1: Input: SetofactionsB ={b } ,initialdistributionπ overthissetofactions,precisionε,regularization
n l∈[L] 0
parameter λ.
2: Compute V˜(π )=λI,m=0.
0
(cid:112)
3: while g(πˆ )> (1+ε)d do
m
4: Compute b
m
=argmax b∈B∥b∥ V˜(πm)−1.
5: γ =argmax logdet(V((1−γ)π +γ1 ))
m γ∈[0,1] m bm
6: For any b∈B,π (b)=(1−γ )π (b)+γ 1 (b).
7: Update V˜(π
m ).+1 m m m bk
m+1
8: end while
9: Output the estimated policy πˆ =π .
m