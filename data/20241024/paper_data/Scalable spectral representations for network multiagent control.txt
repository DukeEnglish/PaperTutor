Scalable spectral representations for network multiagent control
Zhaolin Ren∗,1, Runyu (Cathy) Zhang∗,1, Bo Dai2 and Na Li1
October 23, 2024
Abstract
NetworkMarkovDecisionProcesses(MDPs),apopularmodelformulti-agentcontrol,poseasignificant
challenge to efficient learning due to the exponential growth of the global state-action space with the
number of agents. In this work, utilizing the exponential decay property of network dynamics, we
first derive scalable spectral local representations for network MDPs, which induces a network linear
subspaceforthelocalQ-functionofeachagent. Buildingontheselocalspectralrepresentations,wedesign
a scalable algorithmic framework for continuous state-action network MDPs, and provide end-to-end
guarantees for the convergence of our algorithm. Empirically, we validate the effectiveness of our scalable
representation-based approach on two benchmark problems, and demonstrate the advantages of our
approach over generic function approximation approaches to representing the local Q-functions.
1 Introduction
Multi-agentnetworksystemshavefoundapplicationsinvarioussocietalinfrastructures,suchaspowersystems,
traffic networks, and smart cities [McArthur et al., 2007, Burmeister et al., 1997, Roscia et al., 2013]. One
particularly important class of such problems is the cooperative multi-agent network MDP setting, where
agents are embedded in a graph, and each agent has its own local state [Qu et al., 2020b]. In network
MDPs, the local state transition probabilities and rewards only depend on the states and actions of the
agent’s direct neighbors in the graph. Such a property has been observed in a great variety of cooperative
network control problems, ranging from thermal control of multizone buildings [Zhang et al., 2016], wireless
access control [Zocca, 2019] to phase synchronization in electrical grids [Blaabjerg et al., 2006], where agents
typically only need to act and learn based on information within a local neighborhood due to constraints
on the information and communication infrastructure. However, despite many efforts (c.f. [Qu et al., 2021,
Lin et al., 2021a,Zhang et al., 2023,Abdallah and Lesser, 2007,Du et al., 2022,Ma et al., 2024]),efficiently
finding effective local policies for networks remains an open challenge.
Reinforcement Learning (RL) [Sutton, 2018] has emerged as a promising tool for addressing the complex
dynamics of these systems [Chen et al., 2024, Nezamoddini and Gholami, 2022, Yan and Xu, 2020]. There
are several pioneering works on designing scalable RL algorithms for network systems [Qu et al., 2021,
Lin et al., 2021a, Zhang et al., 2023]. To facilitate scalable control in network control, in [Qu et al., 2021],
the authors introduced a key insight, referred to as the exponential decay property of the Q-function. This
property suggests that each agent’s local Q-function can be well-approximated using only information from
its κ-hop neighborhood. We note that a similar property has also been proposed in [Gu et al., 2022] which
∗ : Equal contributions 1Z. Ren, R. Zhang N. Li are with Harvard University, Email: zhaolinren@g.harvard.edu, run-
yuzhang@fas.harvard.edu,nali@seas.harvard.edu;2B.DaiiswithGoogleBrainandGeorgiaTech,Email: bodai@{google.com,
cc.gatech.edu}.
Z.Ren,R.Zhang,andN.LiarefundedbyNIHR01LM014465,NSFAIinstitute: 2112085,andNSFASCENT:2328241
1
4202
tcO
22
]AM.sc[
1v12271.0142:viXrafocuses on reinforcement learning in the mean field multi-agent setting. Leveraging this property, the
proposed algorithm concentrates on learning truncated Q-functions and then applying either policy gradient
[Qu et al., 2021, Lin et al., 2021a] or policy iteration [Zhang et al., 2023]. However, although these methods
are scalable with respect to the network size, they are limited to the tabular setting, where each agent must
store a local Q-table that scales with the state and action spaces of its neighborhood, making it inefficient for
large state and action spaces. In fact, due to the inherent complexity in network MDPs, i.e., network size is
large and the state and action spaces of each agent are large or continuous, designing efficient and scalable
RL algorithms for such systems remains a long-standing challenge.
There have been several works aimed at addressing scalability in the context of large state and action
spaces. A common approach is to use function approximation to find an efficient representation of the
Q-function. For instance, [Stankovic and Stankovic, 2016] explores linear function approximation to solve
network RL problems. However, their setting is simpler than the network MDP considered here, as they
assume fully decoupled agent dynamics, whereas we allow an agent’s dynamics to depend on the states
of neighboring agents. In the broader context of multi-agent learning, linear function approximation has
also been widely studied [Zhang et al., 2018, Dubey and Pentland, 2021]. However, these works differ from
ours: [Zhang et al., 2018] focuses on the stochastic game setting, where agents share a common global state,
while [Dubey and Pentland, 2021] examines the parallel MDP setting. Additionally, outside the RL domain,
there are works on network representation learning [Dong et al., 2020, Li and Pi, 2020, Zhang et al., 2020].
However, it remains unclear whether these techniques can be applied to control and RL in network systems,
which presents an interesting open question for future research.
FindingasuitablerepresentationfortheQ-functionisnotauniqueprobleminnetworkRL.Itisalsoacen-
tral challenge in classical centralized or single-agent RL. Recently, there have been several papers that discuss
function approximation in the centralized RL regime [Jin et al., 2020a, Du et al., 2021, Uehara et al., 2022,
Grünewälder et al., ]. Forexample,[Jin et al., 2020a]focusesonthelinearMDPsetting,wherethetransition
kernel of the MDP can be represented as a linear combination of low-rank features. They demonstrate that
in this setting, scalable RL can be achieved, with sample complexity depending on the size of the feature
space rather than the size of the state and action spaces. Linear MDPs also have broad applications. Notably,
[Ren et al., 2022b] explore the connection between stochastic nonlinear dynamical systems and linear MDPs,
showing that under certain noise assumptions, stochastic nonlinear dynamics can be well-approximated by
linear MDPs through an approach called spectral dynamic embedding. Building on this, [Ren et al., 2023]
developed RL algorithms specifically for linear MDPs.
Given the existing literature, the following question remains open:
Can we identify an appropriate representation for network MDPs and leverage it for scalability in both the
size of the network and state-action space?
Our contribution Building on the existing literature, this paper addresses the critical gap by proposing a
spectral dynamic embedding-based representation and developing a multi-agent RL algorithm for network
systems that scales efficiently with both network size and the complexity of state and action spaces, while also
providing provable convergence guarantees.
Our approach integrates insights from both network RL and scalable centralized RL. Specifically, utilizing
the exponential decay property and local nature of the transition dynamics, we show how we can approximate
the local Q -value function linearly via network κ-local spectral features that factorize the κ-hop transition
i
dynamics. Leveraging this property, we develop a scalable sample-efficient method to learn local Q-functions
in continuous network MDPs, followed by policy optimization based on the learned Q-functions.
We provide rigorous sample complexity guarantees for our framework, and to the best of our knowledge,
this is the first work to propose a provably efficient multi-agent RL algorithm for network systems that
is scalable with respect to both network size and the size of the state and action spaces of individual
agents. Finally, we validate our approach with numerical experiments on network thermal control and
2Kuramoto oscillator synchronization. In both cases, we find that our approach provides benefits over generic
neural network function approximations, demonstrating the advantages of our spectral representation-based
framework.
Notations For any vectors v ,...,v Rd, the notation n v Rnd denotes their tensor product. The
inner product of two tensor pro1 ducts in s∈ defined as follows. C⊗ oi= n1 sidi e∈ r another set of vectors w ,...,w Rd.
1 n
Then, we denote n v , n w :=(cid:81)n v ,w . We also use the notation [n] to denote the set 1,..∈ .,n
⟨⊗i=1 i ⊗i=1 i ⟩ i=1⟨ i i ⟩ { }
for a positive integer n. In addition, when the context is clear, for notational convenience, we may drop the
time indices and denote (s(t),a(t),s(t+1)) as (s,a,s′).
2 Problem Setup and Preliminaries
Network Markov Decision Process (MDP) We consider the network MDP model, where there are n
agents associated with an underlying undirected graph =( , ), where = 1,...,n is the set of agents
and is the set of edges. Each agent i is aG ssociaN tedE with statN e s { , a } where RS
i i i i i
and E ⊆ NRA× aN re bounded compact sets. At each time t N, the global state∈ ofS the n∈ etwA ork is denS ot⊂ ed as
i
A ⊂ ∈
s(t)=(s (t),...,s (t)) := ... . Similarly, the global actuation of the network at each time t is
1 n 1 n
∈S S × S
denoted as a(t)=(a (t),...,a (t)) := ... . We also introduce the following notations related to
1 n 1 n
κ-hop neighborhoods. Let Nκ denote∈ tA he setA of× κ-hopA neighborhood of node i and define Nκ = Nκ, i.e.,
i −i N \ i
the set of agents that are outside of i’th agent’s κ-hop neighborhood. We write state s as (s ,s ), i.e.,
Nκ Nκ
i −i
the states of agents that are in the κ-hop neighborhood of i and outside of κ-hop neighborhood respectively.
Similarly, we write a as (a ,a ). When κ=1, for simplicity we denote N :=N1.
N iκ N −κ i i i
We assume that the next state of each agent i depends only on the current states and actions of its
neighbors, so that the probability transition admits the following factorization
n
(cid:89)
P(s(t+1) s(t),a(t))= P(s (t+1) s (t),a (t)),
|
i
|
Ni Ni
i=1
where N indicates the neighbors of agent i, and s (t) denotes the states of the neighbors of agent i at time
i Ni
t. Further, each agent is associated with a stage reward function r (s ,a ) that depends on the local state
and action, and the global stage reward is r(s,a)= 1 (cid:80)n r (s ,ai );N fi or si implicity, in the rest of our paper,
n i=1 i Ni i
we will assume that r depends only on (s ,a ), but we note that our analysis carries with minimal changes
i i i
when r depends on (s ,a ). The objective is to find a (localized) policy tuple π =(π ,...,π ), where each
i Ni i 1 n
π i( s) π i( s Nκπ) depends only on a κ π-hop neighborhood, such that the discounted global stage reward
·| ≡ ·| i
is maximized, starting from some initial state distribution µ ,
0
(cid:34) ∞ (cid:35)
(cid:88)
maxJ(π):=E E γtr(s(t),a(t))s(0)=s .
π
s∼µ0 a(t)∼π(·|s(t))
|
t=0
Next, we give the Kuramoto oscillator synchronization problem as an example of continuous state-action
network MDPs. We defer another example, that of thermal control of multi-zone buildings, to Appendix 7.1.
Example1(Kuramotooscillatorsynchronization). TheKuramotomodel[Acebrón et al., 2005,Dorfler and Bullo, 2012]
is a well-known model of nonlinear coupled oscillators, and has been widely applied in various fields, ranging
from synchronization of neurons in the brain [Cumin and Unsworth, 2007], to synchronization of frequency
of the alternating current (AC) generators or oscillators [Filatrella et al., 2008]. Concretely, we consider
here a Kuramoto system with n agents, with an underlying graph =( , ), where = 1,...,n is the
set of agents and is the set of edges. The state of eaGch ageNntE i is its phNase θ{ [ π,} π], and
i
E ⊆ N ×N ∈ −
3the action of each agent is a scalar a R in a bounded subset of R. The dynamics of each agent is
i i
influenced only by the states of its neighb∈orAs a⊂s well as its own action, satisfying the following form in discrete
time [Mozafari et al., 2012]:
  
(cid:88)
θ i(t+1)=θ i(t)+dtω i(t)+a i(t)+ K ijsin(θ
j
θ i)+ϵ i(t).
−
j∈Ni
(cid:124) (cid:123)(cid:122) (cid:125)
:=θ˙ i(t)
Above, ω denotes the natural frequency of agent i, dt is the discretization time-step, K denotes the coupling
i ij
strength between agents i and j, a (t) is the action of agent i at time t, and ϵ (t) N(0,σ2) is a noise term
i i
faced by agent i at time t. We note that this fits into the localized transition con∼sidered in network MDPs.
For the reward, we consider frequency synchronization to a fixed target ω . In this case, the local reward
target
(cid:12) (cid:12)
of each agent can be described as r (θ ,a )= (cid:12)θ˙ ω (cid:12).
i Ni i −(cid:12) i
−
target(cid:12)
To provide context for what follows, we review a few key concepts in RL. First, fixing a localized policy
tuple π =(π ,...,π ), the Q-function for this policy π is:
1 n
n (cid:34) ∞ (cid:35)
1 (cid:88) (cid:88)
Qπ(s,a)= E γtr (s (t),a (t))s(0)=s,a(0)=a
n a(t)∼π(·|s(t)) i i i |
i=1 t=0
n
1 (cid:88)
:= Qπ(s,a).
n i
i=1
In the last step, we defined the local Q-functions Qπ(s,a) which represent the Q functions for the individual
reward r . Correspondingly, we can also define thei local value function Vπ(s)=(cid:82) π(a s)Qπ(s,a)da. We
i i a | i
note that the global Q(s,a) function can be obtained by averaging n local Q (s,a) functions. This plays an
i
important role due to the policy gradient theorem, which states that the policy gradient can be computed
with knowledge of the Q(s,a) function.
Fact 1 ([Sutton et al., 1999]). Let dθ(s)=(1 γ)(cid:80)∞ γtPr(s =s) Then, we have
− t=0 t
(cid:104) (cid:105)
J(πθ)=E Qπθ (s,a) logπθ(as) .
θ s∼dθ,a∼πθ(·|s)
∇ ∇ |
A natural approach to learning the Q(s,a) function in the networked case is for each agent to learn
its local Q (s,a) function and share it across the network to form a global average. However, this poses a
i
significant challenge when (i) the network size n is large, and (ii) the individual state and action spaces
i
S
and arecontinuous. Evenif and arefinite, representingQ (s,a)requires( )n entries, which
i i i i i i
A S A |S |×|A |
grows exponentially with n. This challenge worsens with continuous spaces, which have infinite cardinality.
To address this, we first explore the exponential decay property from prior work, which improves scalability
with network size. We then present our main contribution: integrating the exponential decay property with
spectral representations from single-agent RL to derive scalable local Q -value function representations for
i
continuous state-action network MDPs. We begin by discussing the exponential decay property.
Exponential decay property. The exponential decay property [Qu et al., 2020b, Qu et al., 2020a,
Lin et al., 2021b] is defined as follows.
Definition 1. Given any c > 0 and 0 < ρ < 1, the (c,ρ)-exponential decay property holds for a policy π
if given any natural number κ, for any i ,s ,s ,a ,a , the local
Nκ Nκ Nκ Nκ Nκ Nκ Nκ Nκ
value function Qπ satisfies, ∈N i ∈S i −i ∈S −i i ∈A i −i ∈A −i
i
4(cid:12) (cid:12)
(cid:12)Qπ(s ,s ,a ,a ) Qπ(s ,s′ ,a ,a′ )(cid:12)⩽cρκ+1.
(cid:12) i N iκ N −κ i N iκ N −κ i − i N iκ N −κ i N iκ N −κ i (cid:12)
As an immediate corollary, it follows that
(cid:12) (cid:12)
(cid:12)Vπ(s ,s ) Vπ(s ,s′ )(cid:12)⩽cρκ+1.
(cid:12) i N iκ N −κ i − i N iκ N −κ i (cid:12)
We defer discussion about when the exponential decay property holds to Appendix 7.2. The power
of the exponential decay property is that it immediately guarantees that the dependence of Qπ on other
i
agents shrinks quickly as the distance between them grows, such that the true local Q (s,a)-functions can be
i
approximated by truncated Qˆ (s ,a )-functions up to an error that decays exponentially with κ. The
i Nκ Nκ
i i
truncated Qˆ function is significantly easier to represent in the finite state-action setting since each agent
i
only needs to keep track of ( )κ entries. However, continuous state and action space problems still
i i
|S |×|A |
pose a significant challenge. To overcome this, we will use the idea of spectral representations from linear
MDPs and show how this can be adapted to the networked setting to yield truncated functions.
3 Spectral representations for truncated approximations of local
Q -value functions
i
Torecap,thekeyquestionwefaceisthis: howcanwederivescalablelocalQ -valuefunctionrepresentationsin
i
network problems with continuous state-action spaces, and integrate them into a scalable control framework?
This forms the main contribution of our work. In this section, we tackle this question by demonstrating
that the spectral representation of local transition kernels provides an effective representation for the local
Q -value functions (see Lemma 3 below).
i
We first motivate our analysis by reviewing representation learning in centralized RL via spectral de-
compositions [Jin et al., 2020b, Ren et al., 2022a]. From such works, we know that if the global P(s′ s,a)
|
admits a linear decomposition in terms of some spectral features ϕ(s,a) and µ(s′), then the Q(s,a)-value
function can be linearly represented in terms of the spectral features ϕ(s,a). In the case of representing local
Q -functions, this property can be stated as follows.
i
Lemma 1 (Representing local Q -value functions via spectral decomposition of P (cf. [Jin et al., 2020b])).
i
Suppose the probability transition P(s′ s,a) of the next state s′ given the current (s,a) pair can be linearly
decomposed as P(s′ s,a) = ϕ(s,a)⊤µ| (s′) for some features ϕ(s,a) RD and µ(s′) RD, which we also
refer to as spectral re|presentations. Then, the local Q -value function∈admits the linear∈representation
i
Qπ(s,a)=ϕ˜(s,a)⊤wπ,
i i i
where
(cid:90)
ϕ˜(s,a):=[r (s ,a ),ϕ(s,a)], wπ =[1,γ µ(s′)Vπ(s′)ds′]⊤.
i i i i i i
s′
The benefit of the spectral decomposition property is that the Q -value functions can be represented
i
by a (D+1)-dimensional representation ϕ˜(s,a) comprising the spectral representation ϕ(s,a) RD and
i
local reward r (s ,a ) R. However, applying this result directly in the networked case poses s∈ ignificant
i i i
∈
challenges, since the required feature dimension D+1 may be high. To see why this is the case, recall that
the probability transition in our networked setting admits the following factorization:
P(s′ s,a)=(cid:81)n P(s′ s ,a ).
| i=1 i | Ni Ni
Assume each agent’s transition probability has the following d-dimensional spectral decomposition.
5Property 1. For any i [n] and any state-action-next state tuple (s,a,s′), there exist features ϕ¯(s ,a )
Rd and µ¯ (s′) Rd such∈that
i Ni Ni
∈
i i ∈
P(s′ s ,a )= ϕ¯(s ,a ),µ¯ (s′)
i | Ni Ni ⟨ i Ni Ni i i ⟩
Given the factorization of the dynamics, this implies that
P(s′ s,a)=(cid:81)n ϕ¯(s ,a ),µ¯ (s′)
| i=1⟨ i Ni Ni i i ⟩
= (cid:10)(cid:78)n ϕ¯(s ,a ),(cid:78)n µ¯ (s′)(cid:11) :=(cid:10) ϕ¯(s,a),µ¯(s′)(cid:11) .
i=1 i Ni Ni i=1 i i
Agnostically, this means that representing the global network dynamics may require using the dn-dimensional
features ϕ¯(s,a):=(cid:78)n ϕ¯(s ,a ), which even for small d is undesirable due to an exponential dependence
i=1 i Ni Ni
on the network size n.
While the exponential decay property suggests that the Qˆ -function can be approximated by considering
i
a κ-hop neighborhood of agent i, it is unclear how we can combine this with the spectral decomposition
property to derive scalable representations for the local Q -value functions.
i
To resolve this, we combine insights from both the exponential decay and spectral decomposition property,
which intuitively, suggests that what matters in determining Qπ(s,a) is the probability transition dynamics
i
within a κ-hop neighborhood of agent i. In fact, due to the local factorization property of the dynamics, the
evolution of κ-hop neighborhood only depends on the κ+1-hop neighborhood, which, when Property 1 holds,
admits the following spectral decomposition.
Property 2 (Network κ-local spectral features). For any i [n] and any state-action-next state tuple
( ss u, ca h,s th′) a, tthere exist some positive integer d
i,κ
and features ϕ i,κ(∈ s
N
iκ+1,a
N
iκ+1) ∈Rdi,κ and µ i,κ(s′
N
iκ) ∈Rdi,κ
(cid:16) (cid:17)
P s′ s ,a = ϕ (s ,a ),µ (s′ ) .
N iκ | N iκ+1 N iκ+1 ⟨ i,κ N iκ+1 N iκ+1 i,κ N iκ ⟩
As shown in Lemma 2, when Property 1 is true, Property 2 also holds, with ϕ and µ given by
i,κ i,κ
appropriate tensor products of the original ϕ¯ and µ¯ from the factorization of the local dynamics. We defer
i i
the proof to Appendix 7.3.
Lemma 2. Property 2 holds whenever Property 1 holds, by setting
ϕ (s (t),a (t)):=(cid:78) ϕ¯ (s (t),a (t)),
i,κ N iκ+1 N iκ+1 j∈N iκ j Nj Nj
(cid:78)
µ (s (t+1)):= µ¯ (s (t+1)).
i,κ N iκ j∈N iκ j j
Remark 1. While the tensor product representation in Lemma 2 can be used to give a factorization of the
κ-transition dynamics in Property 2, for specific problems, there may exist problem-specific alternative ϕ
i,κ
and µ features that may be lower-dimensional and thus more tractable to use.
i,κ
Property 2 presents us with a path towards scalable representation of the local Q via factorization of the
i
local κ-hop neighborhood dynamics and approximating Q (s,a) by network local representations. We first
i
formalize this in the case when the spectral decomposition is exact and error-free. When this holds, we have
the following lemma which shows how Qπ(s,a) can be approximated by network local representations. We
i
defer the proof to Appendix 7.4.1.
Lemma3(LocalQ approximationvianetworkκ-localspectralfeatures). Supposethe(c,ρ)-exponentialdecay
i
property holds. Suppose Property 2 also holds. Then, for any (s,a) pair, agent i, and natural number κ, there
exists an approximation Q¯π which depends linearly on network κ-local spectral features ϕ˜ (s ,a ),
i i,κ Nκ+1 Nκ+1
such that i i
6(cid:12) (cid:12)
(cid:12)Qπ(s ,a ,s ,a ) Q¯π(s ,a )(cid:12)⩽2cγρκ+1,
(cid:12) i N iκ+1 N iκ+1 N −κ+ i1 N iκ+1 − i N iκ+1 N iκ+1 (cid:12)
(cid:68) (cid:69)
where Q¯π(s ,a )= ϕ˜ (s ,a ),wπ (s′ ) ,
i Nκ+1 Nκ+1 i,κ Nκ+1 Nκ+1 i,κ Nκ
with the definiitionsi i i i
ϕ˜ (s ,a ):=[r (s ,a ),ϕ (s ,a )]⊤,
i,κ Nκ+1 Nκ+1 i i i i,κ Nκ+1 Nκ+1
i i i i
(cid:90)
wπ (s′ ):=[1,γ ds′ µ (s′ )V¯π(s′ )]⊤,
i,κ Nκ Nκ i,κ Nκ i Nκ
i s′
Niκ
i i i
where
V¯π(s′ ):=(cid:82)
ds′
N−κ i Vπ(s′ ,s′ ).
i N iκ s′ N−κ
i
Vol(S N−κ i) i N iκ N −κ i
Approximation. In general, it may be impossible to find ϕ and µ that can exactly factorize the
i,κ i,κ
transition kernel. However, in both the unknown-model and the known-model cases, there exist ways to
approximate the kernel. In the model-free case, we may leverage representation-learning techniques to approx-
imate the spectrum of the κ-hop transition kernel, such as the spectral decomposition in [Ren et al., 2022a]
which seeks to approximate the SVD of the kernel. In the model-based case, in the case when the local
transition evolves according to a known dynamics function subject to Gaussian noise, we may approximate
the kernel by random or Nystrom features [Ren et al., 2023]. We provide below a unified analysis for the
error bound of approximating Q (s,a) in terms of network κ-local representations ϕˆ (s ,a ) that
i i,κ Nκ+1 Nκ+1
i i
approximately factorize P(s s ,a ).
N iκ | N iκ+1 N iκ+1
Lemma 4. For any distribution νo over the space , suppose there exists a network κ-local
SN iκ+1 ×AN iκ+1
representation ϕˆ (s ,a ) Rm for which there exists µˆ(s′ ) Rm such that for every i [n], the
following holds
foi, rκ soN miκ e+1 appN riκ o+ x1 im∈
ation error ϵ >0:
N iκ ∈ ∈
P
 
(cid:90) (cid:12) (cid:12)
E νo
s′
Niκ(cid:12) (cid:12)P(s′ N iκ |s N iκ+1,a N iκ+1) −ϕˆ i,κ(s N iκ+1,a N iκ+1)⊤µˆ i,κ(s′ N iκ)(cid:12) (cid:12)ds′ N iκ⩽ϵ P, (1)
Then, by setting ϕ˜ (s ,a ):=[r (s ,a ),ϕˆ (s ,a )]⊤, for every i [n],
i,κ N iκ+1 N iκ+1 i i i i,κ N iκ+1 N iκ+1 ∈
(cid:104)(cid:12) (cid:12)(cid:105) ϵ γr¯
min E (cid:12)Q¯π(s ,a ) ϕ˜ (s ,a )⊤w(cid:12) ⩽ P . (2)
w∈Rm+1 νo (cid:12) i N iκ+1 N iκ+1 − i,κ N iκ+1 N iκ+1 (cid:12) 1 γ
−
Proof. Suppose (1) holds. Then, define w∗ :=[1,γ(cid:82) µˆ (s′ )V¯π(s′ )ds′ ]⊤ Rm+1. Then, by using
(cid:12) (cid:12)
S Niκ i,κ N iκ i N iκ N iκ ∈
the upper bound (cid:12)V¯π(s′ )(cid:12)⩽ r¯ , we have
(cid:12) i Nκ (cid:12) 1−γ
i
(cid:104)(cid:12) (cid:12)(cid:105) ϵ γr¯
min E (cid:12)Q¯π(s ,a ) ϕ˜ (s ,a )⊤w(cid:12) ⩽ P .
w∈Rm+1 ν (cid:12) i N iκ+1 N iκ+1 − i,κ N iκ+1 N iκ+1 (cid:12) 1 γ
−
The approximation error in the bound above relies on the condition in (1) to hold. In the case when the
local transition evolves according to a known dynamics function subject to a positive-definite kernel noise (e.g.
7Gaussian noise), we may approximate the κ-hop transition kernel with random features such that (1) holds
with high probability. For clarity of exposition, we focus on the approximation error of random features for
Gaussian kernels [Rahimi and Recht, 2007]1. In this case, our error bound is shown in the following result,
whose proof we defer to Appendix 7.4.2.
Lemma 5. Fix any i [n]. Suppose the local dynamics take the form s′ = f (s ,a ) + ϵ where
ϵ N(0,σ2I ), such tha∈t for any κ, s′ =f (s ,a )+ϵ where ϵ i N(0i ,σN 2i I Ni ) andi f is
coi n∼ catenationS
of f for j Nκ. Fix
anN yiκ 0⩽αi,κ <1N .iκ T+1 henN ,iκ f+ o1
r a
poN si iκ
tive
integN eriκ m∼
, define
th|N eiκ m|S -dimensii o,κ
nal
j ∈ i
features ϕˆ (s ,a ) Rm, where
i,κ N iκ+1 N iκ+1 ∈
ϕˆ (s ,a ):= g α(s N iκ+1,a N iκ+1)(cid:40)(cid:114) 2 cos(cid:32) ω ℓ⊤f i,κ(s N iκ+1,a N iκ+1) +b (cid:33)(cid:41)m ,
i,κ N iκ+1 N iκ+1 α |N iκ |S m √1 −α2 ℓ
ℓ=1
with ω m being i.i.d draws from N(0,σ−2I ), b m being i.i.d draws from Unif([0,2π]), and
g α(s
N{ iκ+ℓ 1} ,ℓ a= N1
iκ+1) :=
exp α2(cid:13)
(cid:13) (cid:13) (cid:13)fi,κ 2(s (1N −iκ α+ 21 ),a σN
2iκ+1)(cid:13)
(cid:13) (cid:13)
(cid:13)2
|N .iκ
|
DS efin{ eℓ g˜} αℓ= :1
= max i∈[n](cid:18) sup
x∈fi,κ(S Niκ+1,A Niκ+1)
αg |α N( iκx |) S(cid:19) .
(cid:18) (cid:20) (cid:18)(cid:16) (cid:17)(cid:19) (cid:21)(cid:19)
Suppose m=Ω max log |N iκ|S(diam(S Niκ))2 |N iκ|Sg˜ α2 for some δ >0. Then, with probability at least
i∈[n]
σ2(δ/n)(ϵP/g˜α) ϵ2
P
1 δ, the condition in (1) holds for every i [n] and any distribution νo over , with
− ∈ S×A
(cid:40)(cid:114) (cid:41)m
2 (cid:112)
µˆ (s′ ):= p (s′ )cos( 1 α2ω⊤s′ +b ) ,
i,κ N iκ m α N iκ − ℓ N iκ ℓ
ℓ=1
where p α(s′
N
iκ):= (2πα σ| 2N )iκ |N|S
iκ|S
exp( −(cid:13) (cid:13) (cid:13)αs 2′ N σi 2κ(cid:13) (cid:13) (cid:13)2 ) is a Gaussian distribution with standard deviation ασ.
The key takeaway from the above result is that under Gaussian noise and known reward and dynamics
function, there exists finite-dimensional features that can, with high probability, approximately factorize the
local κ-transition kernels, satisfying the condition in (1) with high probability. Moreover, from this result,
we note that the required number of features to achieve this is
O˜(cid:16) maxi∈[n]|N iκ|Sg˜ α2(cid:17)
, which only depends on
ϵ2
P
the dimension of states in the largest κ-hop neighborhood. We note that the tunable α in Lemma 5 allows
greater flexibility and may be tuned to improve empirical performance [Ren et al., 2023].
4 Algorithms
As suggested in Lemma 3, ϕ˜ serves as a good representation for the local Q -functions. Based upon this
i,κ i
observation, this section ocuses on how the local Q -function and subsequently a good localized policy can be
i
learned. The algorithm contains three major steps: feature generation, policy evaluation and policy
gradient.
The first step is feature generation (Lines 1 through 3), where we generate the appropriate features
ϕ˜ . This comprises the local reward function as well as the spectral features ϕˆ (s ) coming from the
i,κ i,κ N iκ,a Niκ
factorization of the local κ-hop dynamics. In the case of known dynamics and Gaussian noise, we know from
1Wenotethatourresulteasilygeneralizestoanypositive-definitetransitionkernelnoise(e.g. Laplacian,Cauchy,Matérn,
etc;seeTable1in[Daietal.,2014]formoreexamples)
8Lemma 5 that ϕˆ (s ) can be derived by random features which factorize the local κ-hop dynamics
i,κ N iκ,a Niκ
with high probability. In this case, we note that our spectral features are scalable with respect to both the
network size and the continuous state-action space, since the required number of features only depend on the
dimensions of the κ-hop neighborhoods.
The second step is policy evaluation, where we use the feature ϕ˜ and apply LSTD to find a set of
i,κ
weights w
i
to approximate the local Q i-functions by Qˆ
i
=ϕ(cid:101)⊤ i,κwˆ i. At each round k ∈[K], we first sample M
s
samples from the stationary distribution of π(k) (Line 5), and then perform a LSTD update for each agent
i [n] to learn the appropriate weights for the local Q -functions (Line 6).
i
∈
Finally, the last step is updating policy using policy gradient (Lines 6 to 8). For each agent i [n], with
the learned Qˆ , we perform a gradient step to update the local policy weights θ , and upd∈ ate to the
{ j }j∈N iκπ+κ i
new policy. We note that this update is scalable since from the perspective of each agent i, it only requires
knowledge of the local Qˆ for agents j in a (κ +κ)-hop neighborhood of agent i. In practice, the κ-hop
j π
spectral representation we introduce can be combined flexibly with any actor in a distributed cooperative
actor-critic framework that requires knowledge of the local Q -functions.
i
4.1 Policy evaluation error
We have the following result on the policy evaluation error with our network κ-local features. We defer the
details of the proof (including preliminary results required for the proof) to Appendix 7.5.
Lemma 6 (Policy Evaluation Error). Suppose condition (1) in Lemma 4 holds. Suppose the sample size
(cid:16) (cid:17)
M ⩾log 2(m+1) . Then, with probability at least 1 2δ, for every i [n] and k [K], the ground truth Q
s δ/(Kn) − ∈ ∈
function Qπ(k)(s,a) and the truncated Q function learnt in Algorithm 1 Qˆ (s ,a ) satisfies, for any
i i Nκ+1 Nκ+1
distribution ν¯ on , i i
S×A
(cid:104)(cid:12) (cid:12)(cid:105)
E (cid:12)Qπ(k)(s,a) Qˆ(k)(s ,a )(cid:12)
ν¯ (cid:12) i − i N iκ+1 N iκ+1 (cid:12)
 
⩽O 

c (cid:124)ρL2 (cid:123)D (cid:122)ρκ+ (cid:125)1+log(cid:18) δ( /m (K+1 n) )(cid:19) D √2 ML s5 +L 1ϵ Pγ γr¯ (cid:16)(cid:13) (cid:13) (cid:13)νν¯ o(cid:13) (cid:13) (cid:13) ∞+(cid:13) (cid:13) (cid:13)ν π ν( ok)(cid:13) (cid:13) (cid:13) ∞(cid:17)  ,
truncationerror (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) − (cid:123)(cid:122) (cid:125)
statisticalerror approximationerror
where
(cid:13) (cid:13)
D := max (cid:13)(M(k))−1(cid:13), L:=max φ˜
i∈[n],k∈[K](cid:13) i (cid:13) i∈[n]∥ i,κ ∥
and M(k) is defined as in equation 3
i
From the above result, we note that the policy evaluation error comprises three components, with one
being the statistical error due to using finite samples, which decays with the square root of the sample size
M , and the truncation error from considering a truncated κ-hop neighborhood (this decays exponentially in
s
κ), as well as the approximation error of the spectral features in approximating the κ-hop transition (ϵ ).
P
9Algorithm 1: Networked control with spectral embedding
Data: Q-value truncation radius κ, Policy truncation radius κ , Reward Function r(s,a), Number of
π
features m, Number of samples/round M , Learning Rate η, Number of rounds K
s
Result: π(K+1) =(π(K+1),...,π(K+1))
1 n
Spectral dynamic embedding generation
1 for i [n] do
2 Gene∈ rate features ϕ˜ i,κ(s
N
iκ+1,a
N
iκ+1):=[r i(s i,a i),ϕˆ i,κ(s
N
iκ+1,a
N
iκ+1)] ∈Rm+1,where ϕˆ i,κ(s
N
iκ+1,a
N
iκ+1)
satisfies the condition in (1).
3 end
Policy evaluation and update
4 for k =1,2, ,K do
···
Least squares policy evaluation
5 Set π i(k) :=π iθ i(k+1) . Sample i.i.d. D k = {(s(j),a(j),s′(j)),a′(j) }j∈[Ms] where (s(j),a(j)) ∼ν π(k),
s′(j) P( s(j),a(j)) where ν is the stationary distribution of π(k), and
∼ ·|
π(k)
j [M ], i [n]:a′(j) π(k)( s′ (j)) for i [n] do
∀ ∈ s ∀ ∈ i ∼ i ·| N iκπ ∈
6 Solve wˆ i(k) using least square temporal difference (LSTD) as follows:
wˆ(k) =(M(k))−1H(k)r
i i i i
M(k):= 1 (cid:80) φ˜ (φ˜ γφ˜′ )⊤ (3)
i |Dk| s,a,s′,a′∈Dk i,κ i,κ − i,κ
H(k):= 1 (cid:80) φ˜ φ˜⊤
i |Dk| s,a,s′,a′∈Dk i,κ i,κ
r :=[1,0,0,...,0]⊤ Rm+1
i
∈
where
7
φ˜ (j):=ϕ˜ (s (j),a (j)),
i,κ i,κ Nκ+1 Nκ+1
i i
φ˜′ (j):=ϕ˜ (s′ (j),a′ (j)).
i,κ i,κ Nκ+1 Nκ+1
i i
Update approximate Qˆ(k)-value function as Qˆ (s ,a ):=ϕ˜ (s ,a )⊤wˆ(k).
i i Nκ+1 Nκ+1 i,κ Nκ+1 Nκ+1 i
i i i i
8 end
Policy gradient for control
9 for i [n] do
∈
10 Calculate
gˆ(k) = 1 M (cid:80)s (cid:80) Qˆ( ℓk)(sNℓκ(j),aNℓκ(j)) ×
i Ms
j=1ℓ∈Nκ+κπ
n
i
(θ(k))
∇ θilogπ
i
i (a i(j)|s
N
iκπ(j))
11 Take gradient step θ i(k+1) =θ i(m)+ηgˆ i(k)
12 end
13 end
104.2 Policy optimization error and main convergence result
(cid:16) (cid:17)
Theorem 1. Suppose the sample size M ⩾ log 2(m+1) . Suppose with probability at least 1 δ, for all
s (δ/Kn) −
i [n], the following holds for some features ϕˆ Rm and µˆ Rm:
i,κ i,κ
∈ ∈ ∈
 
(cid:90) (cid:12) (cid:12)
E νo
s+
Niκ(cid:12) (cid:12)P(s+ N iκ |s N iκ+1,a N iκ+1) −ϕˆ i,κ(s N iκ+1,a N iκ+1)⊤µˆ i,κ(s+ N iκ)(cid:12) (cid:12)ds+ N iκ⩽ϵ P
for some ϵ >0 and distribution νo over . Then, if η =O(1/√K), with probability at least 1 4δ,
P
S×A −
1 (cid:88)K (cid:13)
(cid:13)
J(θ(k))(cid:13) (cid:13)2 ⩽O(cid:32) r¯/(1 −γ)
+
L πr¯ϵ
J +
L′ (cid:32)
ϵ2
+(cid:18) L πr¯ (cid:19)2(cid:33)(cid:33)
,
K (cid:13) ∇ (cid:13) √K 1 γ √K J 1 γ
k=1 − −
(cid:114)
(cid:16) (cid:17)
where ϵ J :=2cL πρκ+ 2 1r¯ −L γπ M1
s
log d δθ /+ K1 +ϵ QL π, and
ϵ Q := km ∈a [Kx ]O(cid:18) cρL2Dρκ+1+log(cid:18) ( δm /(K+ n1 ))(cid:19) D √2 ML s5 +L 1ϵ P −γ γr¯ (cid:18)(cid:13) (cid:13) (cid:13) (cid:13)νˆ ν(k o)(cid:13) (cid:13) (cid:13) (cid:13) ∞+(cid:13) (cid:13) (cid:13)ν π ν( ok)(cid:13) (cid:13) (cid:13) ∞(cid:19)(cid:19) ,
(cid:13) (cid:13)
where L′ is the Lipschitz continuity parameter of J(θ), L is a bound on (cid:13) logπθi( )(cid:13), and L :=
∇ i,π (cid:13) ∇θi i ·|· (cid:13) π
(cid:113)
(cid:80)n L2 .
i=1 i,π
From the above result, we see that our algorithm can achieve convergence to an approximate stationary
point of the global objective J as the number of rounds K increases, up to an error term depending on ϵ ,
J
which depends on the policy evaluation error ϵ from Lemma 6. As we observed before, the policy evaluation
Q
error comprises a statistical error, a truncation error decaying exponentially as κ increases, and a feature
approximation error term ϵ . Consequently, the convergence error to an approximation stationary point also
P
depends on these three terms.
5 Simulations
5.1 Thermal control of multi-zone buildings
We consider a stochastic linear dynamical system modeling the thermal control of a 50-zone building. We
assume that the network is connected, with each agent having 2 neighbors. The dynamics of each agent is
only affected by its neighbors, and subject to Gaussian noise. We also assume access to the model dynamics
and reward function. In this problem, to implement our algorithm, we utilize random features that factorize
the κ-hop Gaussian transition (cf. Lemma 5), and perform least squares, followed by normalized gradient
descent. The controller is parameterized to be linear. More details on our experimental setup can be found in
Appendix 7.8.
Since the dynamics are assumed to be linear, we have access to the cost of the optimal controller, making
this a good way to benchmark the performance of our algorithm. The performance of our algorithm is shown
inFigure1below. Aswecansee,asκ increases,ouralgorithmisindeedabletoapproximatetheperformance
π
of the optimal controller. Moreover, the speed at which it converges is faster than Qˆ approximations that
i
leverage a generic two-hidden layer neural network (NN) to represent the (truncated) local Qˆ value functions;
i
we note that in both cases, the algorithms utilize the same learning rate for the policy gradient step, and
have access to the rewards and dynamics function.
11N = 50 N = 50
2×101 Truncated NN, = 0 2×101 Truncated Spectral-Rep, = 0
Truncated NN, = 1 Truncated Spectral-Rep, = 1
Truncated NN, = 2 Truncated Spectral-Rep, = 2
1.8×101 Truncated NN, = 3 1.8×101 Truncated Spectral-Rep, = 3
optimal controller optimal controller
1.6×101 1.6×101
1.4×101 1.4×101
1.2×101 1.2×101
0 5 10 15 20 25 30 35 40 0 5 10 15 20 25 30 35 40
epochs epochs
(a)NNcritic (b)Spectralfeaturescritic
Figure 1: Learning trajectories of cost (lower is better) using Algorithm 1 + random features and NN critics on a
50-dimensionalstochasticlineardynamicalsystemforvaryingκ . Average and1stdconfidenceintervalsover 5seeds.
π
5.2 Kuramoto oscillator control
Due to the nonlinearity in this problem, we adopt the Soft-Actor-Critic (SAC) framework for this problem,
and compare the performance of a generic deep NN critic with our network spectral local-κ critic. In this
problem, we consider the more realistic and difficult setting where the dynamics is unknown. In this problem,
the network has 20 agents in total, and the network graph is connected, with each agent having 2 neighbors.
We set the goal for the agents to synchronize to a target frequency of 0.75.
In both the generic SAC and our spectral SAC implementation, the local critic for Q -value function
i
considersaκ-hopneighborhood,i.e. approximateQ byQˆ (s ,a )=ϕˆ(s ,a )⊤w ,whereϕˆ(s ,a )
i i Nκ Nκ i Nκ Nκ i i Nκ Nκ
i i i i i i
is a two-hidden layer neural network. However, for our approach (spectral + SAC), we add a feature step
that regularizes the feature ϕˆ(s ,a ) towards factorizing the local dynamics, i.e. minimizing the objective
i Nκ Nκ
i i
in Condition 1 in Lemma 4. We defer more details on the problem setup as well as experimental details to
Appendix 7.8.
In Figure 2, we compare the performance of our approach (Spectral + SAC) with a generic SAC with
two-hidden layer NN critic. As we can see, our approach leads to significantly higher rewards. Moreover,
we observe that our approach tends to lead to qualitatively better synchronization behavior when starting
from the same initial condition, as suggested in Figure 3. Finally, in Appendix 7.8, we note that in the
model-based setting, our algorithm (utilizing random features) achieves a performance comparable to that of
generic NN approaches.
6 Conclusion
In this work, utilizing local spectral representations, we provide the first provably efficient algorithm for
scalable network control in continuous state-action spaces. We validate our results numerically, where
we find that utilizing κ-local spectral features can achieve effective control on a thermal network control
problem as well as a Kuramoto nonlinear coupled oscillator control problem. Moreover, in both cases, we
demonstrate that our approach has benefits over generic neural network approximations for local Q -value
i
functions. Collectively, our theoretical and empirical results demonstrate the validity and importance of a
representation-based viewpoint in achieving more effective and scalable control in continuous state-action
network MDPs.
12
)K(J )K(JSAC
30 Spectral-SAC
35
40
45
50
55
60
65
0 5000 10000 15000 20000 25000 30000 35000 40000
Iterations
Figure 2: Change in reward during training for Kuramoto oscillator control, n=40, κ =1,κ=2. The performance
π
for each algorithm is averaged over 5 seeds.
References
[Abdallah and Lesser, 2007] Abdallah, S. and Lesser, V. (2007). Multiagent reinforcement learning and
self-organization in a network of agents. In Proceedings of the 6th international joint conference on
Autonomous agents and multiagent systems, pages 1–8, Honolulu Hawaii. ACM.
[Acebrón et al., 2005] Acebrón, J. A., Bonilla, L. L., Pérez Vicente, C. J., Ritort, F., and Spigler, R. (2005).
The kuramoto model: A simple paradigm for synchronization phenomena. Reviews of modern physics,
77(1):137–185.
[Blaabjerg et al., 2006] Blaabjerg, F., Teodorescu, R., Liserre, M., and Timbus, A. V. (2006). Overview of
controlandgridsynchronizationfordistributedpowergenerationsystems. IEEE Transactions on industrial
electronics, 53(5):1398–1409.
[Burmeister et al., 1997] Burmeister, B., Haddadi, A., and Matylis, G. (1997). Application of multi-agent
systems in traffic and transportation. IEE Proceedings-Software, 144(1):51–60.
[Chen et al., 2024] Chen, D., Zhang, K., Wang, Y., Yin, X., Li, Z., and Filev, D. (2024). Communication-
Efficient Decentralized Multi-Agent Reinforcement Learning for Cooperative Adaptive Cruise Control.
IEEE Transactions on Intelligent Vehicles, pages1–14. ConferenceName: IEEETransactionsonIntelligent
Vehicles.
[Cumin and Unsworth, 2007] Cumin, D. and Unsworth, C. (2007). Generalising the kuramoto model for the
study of neuronal synchronisation in the brain. Physica D: Nonlinear Phenomena, 226(2):181–196.
[Dai et al., 2014] Dai, B., Xie, B., He, N., Liang, Y., Raj, A., Balcan, M.-F. F., and Song, L. (2014). Scalable
kernel methods via doubly stochastic gradients. Advances in neural information processing systems, 27.
13
draweR1.0
1.0
0.5 0.5
0.0 0.0
0.5 0.5
− −
0 500 1000 1500 0 500 1000 1500
timestep timestep
(a)SACController (b)Spectral+SACController
Figure 3: Synchronization of frequency (θ˙) under SAC and Spectral + SAC controller, for 1600 time steps on a single
trajectory. Each curve represents a different agent.
[Dong et al., 2020] Dong, Y., Hu, Z., Wang, K., Sun, Y., and Tang, J. (2020). Heterogeneous Network
Representation Learning. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial
Intelligence, pages 4861–4867, Yokohama, Japan. International Joint Conferences on Artificial Intelligence
Organization.
[Dorfler and Bullo, 2012] Dorfler, F. and Bullo, F. (2012). Synchronization and transient stability in power
networks and nonuniform kuramoto oscillators. SIAM Journal on Control and Optimization, 50(3):1616–
1642.
[Du et al., 2021] Du, S., Kakade, S., Lee, J., Lovett, S., Mahajan, G., Sun, W., and Wang, R. (2021).
Bilinear Classes: A Structural Framework for Provable Generalization in RL. In Proceedings of the 38th
International Conference on Machine Learning, pages 2826–2836. PMLR. ISSN: 2640-3498.
[Du et al., 2022] Du, Y., Ma, C., Liu, Y., Lin, R., Dong, H., Wang, J., and Yang, Y. (2022). Scalable
Model-based Policy Optimization for Decentralized Networked Systems. arXiv:2207.06559 [cs, math, stat].
[Dubey and Pentland, 2021] Dubey, A. and Pentland, A. (2021). Provably Efficient Cooperative Multi-Agent
Reinforcement Learning with Function Approximation. arXiv:2103.04972 [cs, stat].
[Filatrella et al., 2008] Filatrella, G., Nielsen, A. H., and Pedersen, N. F. (2008). Analysis of a power grid
using a kuramoto-like model. The European Physical Journal B, 61:485–491.
[Grünewälder et al., ] Grünewälder, S., Lever, G., Baldassarre, L., Pontil, M., and Gretton, A. Modelling
transition dynamics in MDPs with RKHS embeddings.
[Gu et al., 2022] Gu, H., Guo, X., Wei, X., and Xu, R. (2022). Mean-Field Multi-Agent Reinforcement
Learning: A Decentralized Network Approach. arXiv:2108.02731 [cs, math].
[Haarnoja et al., 2018] Haarnoja,T.,Zhou,A.,Abbeel,P.,andLevine,S.(2018). Softactor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International conference on
machine learning, pages 1861–1870. PMLR.
14
θ˙ θ˙[Jin et al., 2020a] Jin, C., Yang, Z., Wang, Z., and Jordan, M. I. (2020a). Provably efficient reinforcement
learningwithlinearfunctionapproximation. InProceedings of Thirty Third Conference on Learning Theory,
pages 2137–2143. PMLR. ISSN: 2640-3498.
[Jin et al., 2020b] Jin, C., Yang, Z., Wang, Z., and Jordan, M. I. (2020b). Provably efficient reinforcement
learning with linear function approximation. In Conference on learning theory, pages 2137–2143. PMLR.
[Li and Pi, 2020] Li, B. and Pi, D. (2020). Network representation learning: a systematic literature review.
Neural Computing and Applications, 32(21):16647–16679.
[Li et al., 2021] Li, Y., Tang, Y., Zhang, R., and Li, N. (2021). Distributed reinforcement learning for
decentralized linear quadratic control: A derivative-free policy optimization approach. IEEE Transactions
on Automatic Control, 67(12):6429–6444.
[Lin et al., 2021a] Lin,Y.,Qu,G.,Huang,L.,andWierman,A.(2021a). Multi-AgentReinforcementLearning
in Stochastic Networked Systems. arXiv:2006.06555 [cs, stat].
[Lin et al., 2021b] Lin, Y., Qu, G., Huang, L., and Wierman, A. (2021b). Multi-agent reinforcement learning
in stochastic networked systems. Advances in neural information processing systems, 34:7825–7837.
[Ma et al., 2024] Ma, C., Li, A., Du, Y., Dong, H., and Yang, Y. (2024). Efficient and scalable reinforcement
learning for large-scale network control. Nature Machine Intelligence.
[McArthur et al., 2007] McArthur, S. D., Davidson, E. M., Catterson, V. M., Dimeas, A. L., Hatziargyriou,
N. D., Ponci, F., and Funabashi, T. (2007). Multi-agent systems for power engineering applications—part
i: Concepts, approaches, and technical challenges. IEEE Transactions on Power systems, 22(4):1743–1752.
[Mozafari et al., 2012] Mozafari, Y., Kiani, A., and Hirche, S. (2012). Oscillator network synchronization by
distributed control. In 2012 IEEE International Conference on Control Applications, pages 621–626. IEEE.
[Nezamoddini and Gholami, 2022] Nezamoddini, N. and Gholami, A. (2022). A Survey of Adaptive Multi-
Agent Networks and Their Applications in Smart Cities. Smart Cities, 5(1):318–347. Number: 1 Publisher:
Multidisciplinary Digital Publishing Institute.
[Qu et al., 2020a] Qu, G., Lin, Y., Wierman, A., and Li, N. (2020a). Scalable multi-agent reinforcement
learning for networked systems with average reward. Advances in Neural Information Processing Systems,
33:2074–2086.
[Qu et al., 2020b] Qu, G., Wierman, A., and Li, N. (2020b). Scalable reinforcement learning of localized
policies for multi-agent networked systems. In Bayen, A. M., Jadbabaie, A., Pappas, G., Parrilo, P. A.,
Recht, B., Tomlin, C., and Zeilinger, M., editors, Proceedings of the 2nd Conference on Learning for
Dynamics and Control, volume 120 of Proceedings of Machine Learning Research, pages 256–266. PMLR.
[Qu et al., 2021] Qu, G., Wierman, A., and Li, N. (2021). Scalable Reinforcement Learning for Multi-Agent
Networked Systems. arXiv:1912.02906 [cs, math].
[Rahimi and Recht, 2007] Rahimi, A. and Recht, B. (2007). Random features for large-scale kernel machines.
Advances in neural information processing systems, 20.
[Ren et al., 2023] Ren, T., Ren, Z., Li, N., and Dai, B. (2023). Stochastic Nonlinear Control via Finite-
dimensional Spectral Dynamic Embedding. In 2023 62nd IEEE Conference on Decision and Control
(CDC), pages 795–800. ISSN: 2576-2370.
15[Ren et al., 2022a] Ren, T., Zhang, T., Lee, L., Gonzalez, J. E., Schuurmans, D., and Dai, B. (2022a).
Spectral decomposition representation for reinforcement learning. arXiv preprint arXiv:2208.09515.
[Ren et al., 2022b] Ren, T., Zhang, T., Szepesvári, C., and Dai, B. (2022b). A Free Lunch from the Noise:
Provable and Practical Exploration for Representation Learning.
[Roscia et al., 2013] Roscia, M., Longo, M., and Lazaroiu, G. C. (2013). Smart city by multi-agent systems.
In 2013 International Conference on Renewable Energy Research and Applications (ICRERA), pages
371–376. IEEE.
[Stankovic and Stankovic, 2016] Stankovic,M.S.andStankovic,S.S.(2016). Multi-agenttemporal-difference
learning with linear function approximation: Weak convergence under time-varying network topologies. In
2016 American Control Conference (ACC), pages 167–172. ISSN: 2378-5861.
[Sutton, 2018] Sutton, R. S. (2018). Reinforcement learning: An introduction. A Bradford Book.
[Sutton et al., 1999] Sutton,R.S.,McAllester,D.,Singh,S.,andMansour,Y.(1999).Policygradientmethods
forreinforcementlearningwithfunctionapproximation. Advances in neural information processing systems,
12.
[Tropp et al., 2015] Tropp, J. A. et al. (2015). An introduction to matrix concentration inequalities. Founda-
tions and Trends® in Machine Learning, 8(1-2):1–230.
[Uehara et al., 2022] Uehara, M., Zhang, X., and Sun, W. (2022). Representation Learning for Online and
Offline RL in Low-rank MDPs. arXiv:2110.04652 [cs, stat].
[Yan and Xu, 2020] Yan, Z. and Xu, Y. (2020). A Multi-Agent Deep Reinforcement Learning Method for
CooperativeLoadFrequencyControlofaMulti-AreaPowerSystem. IEEE Transactions on Power Systems,
35(6):4599–4608. Conference Name: IEEE Transactions on Power Systems.
[Zhang et al., 2020] Zhang, D., Yin, J., Zhu, X., and Zhang, C. (2020). Network Representation Learning: A
Survey. IEEE Transactions on Big Data, 6(1):3–28. Conference Name: IEEE Transactions on Big Data.
[Zhang et al., 2018] Zhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. (2018). Fully Decentralized
Multi-Agent Reinforcement Learning with Networked Agents. In Proceedings of the 35th International
Conference on Machine Learning, pages 5872–5881. PMLR. ISSN: 2640-3498.
[Zhang et al., 2016] Zhang, X., Shi, W., Li, X., Yan, B., Malkawi, A., and Li, N. (2016). Decentralized
temperature control via hvac systems in energy efficient buildings: An approximate solution procedure. In
2016 IEEE Global Conference on Signal and Information Processing (GlobalSIP), pages 936–940. IEEE.
[Zhang et al., 2023] Zhang,Y.,Qu,G.,Xu,P.,Lin,Y.,Chen,Z.,andWierman,A.(2023).Globalconvergence
of localized policy iteration in networked multi-agent reinforcement learning. Proceedings of the ACM on
Measurement and Analysis of Computing Systems, 7(1):1–51.
[Zocca, 2019] Zocca,A.(2019). Temporalstarvationinmulti-channelcsmanetworks: ananalyticalframework.
ACM SIGMETRICS Performance Evaluation Review, 46(3):52–53.
167 Appendix
7.1 Example of thermal control in buildings as network MDP
Example 2 (Thermal control in buildings). The problem of thermal control of multiple zones in a building
can also be cast as a network MDP. Consider a multi-zone building with a Heating Ventilation and Air
Conditioning(HVAC)system. Eachzoneisequippedwithasensorthatcanmeasurethelocaltemperaturesand
can adjust the supply air flow rate of its associated HVAC system. For simplicity, we consider a discrete-time
linear thermal dynamics model based on [Zhang et al., 2016, Li et al., 2021], where for any i [n],
∈
(cid:114)
∆ (cid:88) ∆ ∆ ∆ ∆
x (t+1) x (t)= (θo(t) x (t))+ (x (t) x (t))+ a (t)+ π + w (t),
i − i v ζ − i v ζ j − i v i v i v i
i i i ij i i i
j∈Ni
where x (t) denotes the temperature of zone i at time t, a (t) denotes the control input of zone i that is related
i i
with the air flow rate of the HVAC system, θo(t) denotes the outdoor temperature, π represents a constant
i
heat from external sources to zone i, w (t) represents random disturbances, ∆ is the time resolution, v is the
i i
thermal capacitance of zone i, ζ represents the thermal resistance of the windows and walls between zone
i
i and the outside environment, and ζ represents the thermal resistance of the walls between zone i and j.
ij
Again, we note that the transition dynamics of each agent depends only on its neighbors (and itself). At
each zone i, there is a desired temperature θ∗ set by the users. The local reward function is composed of the
i
(negative) deviation from the desired temperature and the control cost, i.e.
r (t)= (cid:0) (x (t) θ∗)2+α a (t)2(cid:1) ,
i − i − i i i
where α >0 is a trade-off parameter.
i
7.2 On the exponential decay property
It may not be immediately clear when the exponential decay property holds. The following lemma (cf.
Appendix A in [Qu et al., 2020b]) highlights that for a local policy where each agent’s actions depend only
only on its and its neighbors’ states (i.e. π ( s) π ( s ) ), the exponential decay property holds
i
· | ≡
i
· |
Ni
generally, with ρ=γ. We defer the proof to our appendix.
Lemma 7. Suppose i [n], agent i adopts a localized policy, i.e. π ( s) π ( s ). Suppose also that
∀ ∈ (cid:16)
(cid:17)i
·| ≡
i
·|
Ni
the local rewards are bounded such that 0⩽r ⩽r¯. Then the r¯ ,γ -exponential decay property holds.
i 1−γ
We note that under some mixing time assumptions on the MDP [Qu et al., 2020b], the exponential decay
property may in fact hold for ρ<γ depending on the system parameters, making it applicable to problems
with large discount factors or even in the average-reward setting [Qu et al., 2020a].
We proceed now to prove Lemma 7, which shows that the exponential decay property holds for localized
policies and bounded rewards. We note that this was first shown in [Qu et al., 2020b], and we provide the
proof here for completeness.
Proof. Consider any i, and choose any natural number κ. For an arbitrary (s,a)=(s ,s ,a ,a ),
Nκ Nκ Nκ Nκ
i −i i −i
consider any state-action pair (s′,a′) that differs with (s,a) only outside the Nκ-neighborhood, i.e. (s′,a′)=
i
(s ,s′ ,a ,a′ ). For any natural number t, let p denote the distribution of s (t),a (t) conditional on
N iκ N −κ
i
N iκ N −κ
i
t,i i i
s(0)=s,a(0)=a, and let p′ denote the distribution of s (t),a (t) conditional on s(0)=s′,a(0)=a′. Then,
t,i i i
(cid:34) ∞ (cid:35) (cid:34) ∞ (cid:35)
(cid:88) (cid:88)
Qπ(s,a) Qπ(s′,a′)= E γtr (s (t),a (t)) s(0)=s,a(0)=a E γtr (s (t),a (t)) s(0)=s′,a(0)=a′
i − i i i i | − i i i |
t=0 t=0
17(i) (cid:88)∞ (cid:16) (cid:17)
= γt E [r (s (t),a (t))] E [r (s (t),a (t))]
pt,i i i i
−
p′
t,i
i i i
t=0
κ
(cid:88) (cid:16) (cid:17)
= γt E [r (s (t),a (t))] E [r (s (t),a (t))]
pt,i i i i
−
p′
t,i
i i i
t=0
∞
(cid:88) (cid:16) (cid:17)
+ γt E [r (s (t),a (t))] E [r (s (t),a (t))]
pt,i i i i
−
p′
t,i
i i i
t=κ+1
(ii) (cid:88)∞ (cid:16) (cid:17)
= γt E [r (s (t),a (t))] E [r (s (t),a (t))]
pt,i i i i
−
p′
t,i
i i i
t=κ+1
(iii) γκ+1
⩽ r¯.
1 γ
−
Above, (i) is a direct application of the definition of p and p′ . Meanwhile, (ii) utilizes the fact that for
t,i t,i
any 0⩽t⩽κ, we have p p′ . This is because of (a) localized policy, such that a (t) depends only on
t,i ≡ t,i i
s (t 1), and (b) factorized localized dynamics, that s (t) depends only only on s (t) for any natural
Ni − N ij N ij+1
number j; hence an iterative argument shows that for any t, p and p′ both only depend on s (0) and
t,i t,i Nt
a (0). Thus, since (s,a) and (s′,a′) share identical s (0) and a (0), it follows that p p′ fi or t⩽κ.
FN init ally, (iii) uses the fact that bounded reward assumN piκ tion, i.e. 0N i⩽κ r ⩽ r¯. The proof tt h,i e≡ n cot, ni cludes by
i
rerunning the argument on Qπ(s′,a′) Qπ(s,a).
i − i
Next, we state and prove the following elementary technical result, which bounds any two truncated Q(or
V)-functions with different weights.
Lemma 8. Suppose the (c,ρ)-exponential decay property holds. Then, for any two different weights
w (s ,a ;s ,a ) and w′(s ,a ;s ,a ) over the space , i.e.
i N iκ N iκ N −κ i N −κ i i N iκ N iκ N −κ i N −κ i SN −κ i ×AN −κ i
(cid:88)
w (s ,a ;s ,a )=1,
i Nκ Nκ Nκ Nκ
i i −i −i
s N−κ i,a N−κ
i
(cid:88)
w′(s ,a ;s ,a )=1,
i N iκ N iκ N −κ i N −κ i
s N−κ i,a N−κ
i
we have
(cid:12) (cid:12)
(cid:12)Qˆπ(s ,a ) (Qˆ′)π(s ,a )(cid:12)⩽2cρκ+1,
(cid:12) i N iκ N iκ − i N iκ N iκ (cid:12)
where
Qˆπ(s ,a )= (cid:88) w (s ,a ;s ,a )Qπ(s ,s ,a ,a ),
i N iκ N iκ i N iκ N iκ N −κ i N −κ i i N iκ N −κ i N iκ N −κ i
s N−κ i,a N−κ
i
(Qˆ′)π(s ,a )= (cid:88) w′(s ,a ;s ,a )Qπ(s ,s ,a ,a )
i N iκ N iκ i N iκ N iκ N −κ i N −κ i i N iκ N −κ i N iκ N −κ i
s N−κ i,a N−κ
i
Similarly, for any two different weights w (s ;s ) and w′(s ;s ) over the space , i.e.
i N iκ N −κ i i N iκ N −κ i SN −κ i
(cid:88)
w (s ;s )=1,
i Nκ Nκ
i −i
s N−κ
i
18(cid:88)
w′(s ;s )=1,
i N iκ N −κ i
s N−κ
i
we have
(cid:12) (cid:12)
(cid:12)Vˆπ(s (Vˆ′)π(s )(cid:12)⩽2cρκ+1,
(cid:12) i N iκ − i N iκ (cid:12)
where
Vˆπ(s )= (cid:88) w (s ;s )Vπ(s ,s ),
i N iκ i N iκ N −κ i i N iκ N −κ i
s N−κ
i
(Vˆ′)π(s )= (cid:88) w′(s ;s )Vπ(s ,s )
i N iκ i N iκ N −κ i i N iκ N −κ i
s N−κ
i
Proof. Compare both truncated Q-functions to a Q-function evaluated at any specific state action pair where
the states and actions of the agents in Nκ are s and a respectively. The desired result then follows by
i N iκ N iκ
Definition 1. A similar argument works for the V-function.
7.3 Helper results on factorization of network probability transition
Lemma 2. Property 2 holds whenever Property 1 holds, by setting
ϕ (s (t),a (t)):=(cid:78) ϕ¯ (s (t),a (t)),
i,κ N iκ+1 N iκ+1 j∈N iκ j Nj Nj
(cid:78)
µ (s (t+1)):= µ¯ (s (t+1)).
i,κ N iκ j∈N iκ j j
Proof. To see that, observe that
P(cid:16)
s (t+1) s (t),a
(t)(cid:17)
=
(cid:89) P(cid:0)
s (t+1) s (t),a
(t)(cid:1)
N iκ | N iκ+1 N iκ+1 j | Nj Nj
j∈Nκ
i
(cid:42) (cid:43)
( =iv) (cid:89) ϕ¯ (s (t),a (t)),µ¯ (s (t+1)) ( =v) (cid:79) ϕ¯ (s (t),a (t)), (cid:79) µ¯ (s (t+1)) .
⟨
j Nj Nj j j
⟩
j Nj Nj j j
j∈Nκ j∈Nκ j∈Nκ
i i i
Above, (iv) follows from Property 1, while (v) uses the definition of the inner product of two tensor products.
Thus, when Property 1 holds, Property 2 holds by setting
ϕ (s (t),a (t)):= (cid:79) ϕ¯ (s (t),a (t)), µ (s (t+1)):= (cid:79) µ¯ (s (t+1)).
i,κ N iκ+1 N iκ+1 j Nj Nj i,κ N iκ j j
j∈Nκ j∈Nκ
i i
7.4 Approximation error of spectral features
7.4.1 Approximation error when spectral features exactly factorize κ-hop transition
Werecallandprovethisresult, whichboundstheapproximationerrorofusingthetruncatedspectralfeatures
toapproximatethelocalQ -function,inthecasewhenthereisnoapproximationerrorinthespectralfeatures
i
in representing the κ-hop transition.
19Lemma3(LocalQ approximationvianetworkκ-localspectralfeatures). Supposethe(c,ρ)-exponentialdecay
i
property holds. Suppose Property 2 also holds. Then, for any (s,a) pair, agent i, and natural number κ, there
exists an approximation Q¯π which depends linearly on network κ-local spectral features ϕ˜ (s ,a ),
i i,κ Nκ+1 Nκ+1
such that i i
(cid:12) (cid:12)
(cid:12)Qπ(s ,a ,s ,a ) Q¯π(s ,a )(cid:12)⩽2cγρκ+1,
(cid:12) i N iκ+1 N iκ+1 N −κ+ i1 N iκ+1 − i N iκ+1 N iκ+1 (cid:12)
(cid:68) (cid:69)
where Q¯π(s ,a )= ϕ˜ (s ,a ),wπ (s′ ) ,
i Nκ+1 Nκ+1 i,κ Nκ+1 Nκ+1 i,κ Nκ
with the definiitionsi i i i
ϕ˜ (s ,a ):=[r (s ,a ),ϕ (s ,a )]⊤,
i,κ Nκ+1 Nκ+1 i i i i,κ Nκ+1 Nκ+1
i i i i
(cid:90)
wπ (s′ ):=[1,γ ds′ µ (s′ )V¯π(s′ )]⊤,
i,κ Nκ Nκ i,κ Nκ i Nκ
i s′
Niκ
i i i
where
V¯π(s′ ):=(cid:82)
ds′
N−κ i Vπ(s′ ,s′ ).
i N iκ s′ N−κ
i
Vol(S N−κ i) i N iκ N −κ i
Proof. For notational convenience, we omit the t and (t+1) in the parentheses of the state and action
notations, and instead use a superscript + to denote (t+1), e.g. s+ to denote s(t+1). Observe that
Qπ(s ,a ,s ,a )
i Nκ+1 Nκ+1 Nκ+1 Nκ+1
i i −i −i
 
(cid:90) (cid:16) (cid:17) (cid:90) (cid:16) (cid:17)
= r i(s i,a i)+γ
s+
Niκ
ds+ N iκP s+ N iκ |s N iκ+1,a N iκ+1 
s+
N−κ
i
ds+ N −κ iV iπ(s+ N iκ,s+ N −κ i)P s+ N −κ
i
|s,a 
(vi) (cid:90) (cid:16) (cid:17)
= r (s ,a )+γ ds+ P s+ s ,a Vˆπ(s+ )
i i i s+
Niκ
N iκ N iκ | N iκ+1 N iκ+1 i N iκ
(vii) (cid:90) (cid:16) (cid:17) (cid:90) (cid:16) (cid:17)
= r (s ,a )+γ ds+ P s+ s ,a V¯π(s+ )+γ ds+ P s+ s ,a (Vˆπ(s+ ) V¯π(s+ ))
i i i s+
Niκ
N iκ N iκ | N iκ+1 N iκ+1 i N iκ s+
Niκ
N iκ N iκ | N iκ+1 N iκ+1 i N iκ − i N iκ
(viii) (cid:90) (cid:68) (cid:69)
= r (s ,a )+γ ds+ ϕ (s ,a ),µ (s+ )V¯π(s+ )
i i i Nκ i,κ Nκ+1 Nκ+1 i,κ Nκ i Nκ
s+
Niκ
i i i i i
(cid:90) (cid:16) (cid:17)
+γ ds+ P s+ s ,a (Vˆπ(s+ ) V¯π(s+ ))
s+
Niκ
N iκ N iκ | N iκ+1 N iκ+1 i N iκ − i N iκ
In (vi) above, we used the notation
(cid:90) (cid:16) (cid:17)
Vˆπ(s+ ):= ds+ Vπ(s+ ,s+ )P s+ s,a ,
i N iκ s+
N−κ
i
N −κ i i N iκ N −κ i N −κ i |
and in (vii), we recall that we defined
(cid:90) ds+
V¯π(s+ ):= N −κ i Vπ(s+ ,s+ ).
i N iκ s+
N−κ
i
Vol( SN −κ i) i N iκ N −κ i
20Since the (c,ρ)-exponential decay property holds, applying Lemma 8, we have
(cid:12) (cid:12)
(cid:12)Vˆπ(s+ ) V¯π(s+ )(cid:12)⩽2cρκ+1.
(cid:12) i N iκ − i N iκ (cid:12)
Thus our desired result holds by setting
(cid:42) (cid:43)
(cid:90)
Q¯π(s ,a ):=r (s ,a )+ ϕ (s ,a ),γ ds+ µ (s+ )V¯π(s+ )
i Nκ+1 Nκ+1 i i i i,κ Nκ+1 Nκ+1 Nκ i,κ Nκ i Nκ
i i i i s+
Niκ
i i i
7.4.2 Results on approximation error of random features
We first state the following result on uniform convergence of random Fourier features, adapted from Claim 1
in [Rahimi and Recht, 2007].
Lemma 9 (Uniform convergence of Fourier features). Let be a compact subset of Rd with diameter
diam( ). Let k be a positive definite shift-invariant kernel k(M x,y)=k(x y). Define the mapping z, where
M −
(cid:114)
z = 2 (cid:2) cos(ω⊤x+b ) cos(ω⊤x+b )(cid:3) ,
D 1 1 ··· D D
where ω ,...,ω Rd are D iid samples from p, where p is the Fourier transform of k, i.e. p(ω) =
1 D
1 (cid:82) e−jω⊤δk(δ)dδ,∈ and b ,...,b are D are iid samples from Unif(0,2π). We assume that k is suitably
2π 1 D
scaled such that p is a probability distribution.
Then, for the mapping z defined above, we have
(cid:20) (cid:21) (cid:18) σ diam( )(cid:19)2 (cid:18) Dϵ2 (cid:19)
Pr sup z(x)⊤z(y) k(x,y) ⩾ϵ ⩽28 p M exp ,
| − | ϵ −4(d+2)
x,y∈M
where σ2 =E [ω⊤ω] is the second moment of the Fourier transform of k.
p p
Further,
sup z(x)⊤z(y) k(x,y) ⩽ϵ
| − |
x,y∈M
with probability at least 1 δ when
−
(cid:18) (cid:18) σ diam( )2(cid:19) d(cid:19)
D =Ω log p M .
δϵ ϵ2
Lemma 5. Fix any i [n]. Suppose the local dynamics take the form s′ = f (s ,a ) + ϵ where
ϵ N(0,σ2I ), such tha∈t for any κ, s′ =f (s ,a )+ϵ where ϵ i N(0i ,σN 2i I Ni ) andi f is
coi n∼ catenationS
of f for j Nκ. Fix
anN yiκ 0⩽αi,κ <1N .iκ T+1 henN ,iκ f+ o1
r a
poN si iκ
tive
integN eriκ m∼
, define
th|N eiκ m|S -dimensii o,κ
nal
j ∈ i
features ϕˆ (s ,a ) Rm, where
i,κ N iκ+1 N iκ+1 ∈
ϕˆ (s ,a ):= g α(s N iκ+1,a N iκ+1)(cid:40)(cid:114) 2 cos(cid:32) ω ℓ⊤f i,κ(s N iκ+1,a N iκ+1) +b (cid:33)(cid:41)m ,
i,κ N iκ+1 N iκ+1 α |N iκ |S m √1 −α2 ℓ
ℓ=1
21with ω m being i.i.d draws from N(0,σ−2I ), b m being i.i.d draws from Unif([0,2π]), and
g α(s
N{ iκ+ℓ 1} ,ℓ a= N1
iκ+1) :=
exp α2(cid:13)
(cid:13) (cid:13) (cid:13)fi,κ 2(s (1N −iκ α+ 21 ),a σN
2iκ+1)(cid:13)
(cid:13) (cid:13)
(cid:13)2
|N .iκ
|
DS efin{ eℓ g˜} αℓ= :1
= max i∈[n](cid:18) sup
x∈fi,κ(S Niκ+1,A Niκ+1)
αg |α N( iκx |) S(cid:19) .
(cid:18) (cid:20) (cid:18)(cid:16) (cid:17)(cid:19) (cid:21)(cid:19)
Suppose m=Ω max log |N iκ|S(diam(S Niκ))2 |N iκ|Sg˜ α2 for some δ >0. Then, with probability at least
i∈[n]
σ2(δ/n)(ϵP/g˜α) ϵ2
P
1 δ, the condition in (1) holds for every i [n] and any distribution νo over , with
− ∈ S×A
(cid:40)(cid:114) (cid:41)m
2 (cid:112)
µˆ (s′ ):= p (s′ )cos( 1 α2ω⊤s′ +b ) ,
i,κ N iκ m α N iκ − ℓ N iκ ℓ
ℓ=1
where p α(s′
N
iκ):= (2πα σ| 2N )iκ |N|S
iκ|S
exp( −(cid:13) (cid:13) (cid:13)αs 2′ N σi 2κ(cid:13) (cid:13) (cid:13)2 ) is a Gaussian distribution with standard deviation ασ.
(cid:18) (cid:19)
Proof. Observe that P s+ f (s ) follows the Gaussian distribution N(0,σ2I ). For
N iκ | i,κ N iκ+1,a Niκ+1 |N iκ |S
notational convenience, in this proof, we denote x:=f (s ), y :=s+ . In addition, in this proof
we denote d:= Nκ S. For any 0⩽α<1, observe
thati,κ N iκ+1,a Niκ+1 N iκ
| i |
P(y x)=
g α(x)
exp(cid:32) (cid:13)
(cid:13)(1 −α2)y
−x(cid:13) (cid:13)2(cid:33)
p (y),
| αd − 2σ2(1 α2) α
−
where g (x):=exp(α2 x 2/(2σ2(1 α2))), and p (y):= αd exp( αy 2/(2σ2)).2
α ∥ ∥ − α (2πσ2)d/2 −∥ ∥
Define g˜ α :=max i∈[n]sup x∈fi,κ(S Niκ+1,A Niκ+1) gα α( dx).3 Observe now (cid:18)that (cid:18)k α(z,z′):=exp( (cid:19)−2∥ σz 2 (cid:19)− (1z −′ ∥ α2 2)) is a
positive-definite shift-invariant kernel. Hence, by Lemma 9, if m=Ω log dσ−2diam(S Niκ)2 dg˜ α2 it follows
δ(ϵ/g˜α) ϵ2
that with probability at least 1 δ,
−
sup (cid:12) (cid:12)ϕ¯ i,κ(x)⊤µ¯ i,κ(y) −k α(x,(1 −α2)y)(cid:12) (cid:12)⩽ ϵ g˜P,
x,y∈S Niκ α
where
(cid:114) 2 (cid:26) (cid:18) w⊤x (cid:19)(cid:27)m
ϕ¯ (x)= cos ℓ +b ,
i,κ D √1 α2 ℓ
ℓ=1
−
(cid:114)
2 (cid:110) (cid:16)(cid:112) (cid:17)(cid:111)m
µ¯ (y)= cos 1 α2y+b ,
i,κ D − ℓ ℓ=1
and ω ’s are drawn iid from N(0,σ−2I ), b ’s are drawn iid from Unif(0,2π). It follows then for any
ℓ d ℓ
{ } { }
x f ( , ), we have
∈ i,κ SN iκ+1 AN iκ+1
(cid:90) (cid:12) (cid:12) (cid:12)P(y |x) −ϕˆ i,κ(x)⊤µˆ i,κ(y)(cid:12) (cid:12) (cid:12)dy = (cid:90) (cid:12) (cid:12) (cid:12) (cid:12)g α α( dx)(cid:12) (cid:12) (cid:12) (cid:12)(cid:12) (cid:12)k α(x,(1 −α2)y) −ϕ¯ i,κ(x)⊤µ¯ i,κ(y)(cid:12) (cid:12) |p α(y) |dy
y y
2Whenα:=0,thissimplifiestoP(y|x)∝exp(cid:16) −∥y−x∥2(cid:17)
. However,weallowageneral0⩽α<1becauseitgivesgreater
2σ2
flexibilityboththeoreticallyandempirically.
3Hereweoverloadnotationtousefi,κ(S
N
iκ+1,A
N
iκ+1)todenote{fi,κ(s
N
iκ+1,a
N
iκ+1)}
(s Niκ+1,a Niκ+1)∈S Niκ+1×A Niκ+1
22ϵ (cid:90)
⩽ g˜ P p (y)dy =ϵ
αg˜ α P
α y
where we recall that
g
ϕˆ (x):= αϕ¯ (x), µˆ (y):=p (y)µ¯ (y).
i,κ αd i,κ i,κ α i,κ
The proof then follows by rescaling ϵ :=ϵ /n, and taking a union bound over all i [n].
P P
∈
7.5 Algorithm analysis - policy evaluation error
For simplicity, we assume throughout the analysis that we are solving the LSTD step in the policy evaluation
exactly, i.e. we take the number of least square solves, T, to infinite. Moreover, we drop the i subscript in
the notation of ϕ˜ , and use ν to denote ν . At round k, the algorithm output of the policy parameter w
i,κ π(k) i
of agent i is given by
w(k) =(M(k))−1H(k)r
i i i i
where
M(k) = 1 (cid:88) ϕ˜ (s ,a )(cid:16) ϕ˜ (s ,a ) γϕ˜ (s′ ,a′ )(cid:17)⊤ ,
i D
k
i,κ N iκ+1 N iκ+1 i,κ N iκ+1 N iκ+1 − i,κ N iκ+1 N iκ+1
| |s,a,s′,a′∈Dk
H(k) = 1 (cid:88) ϕ˜ (s ,a )ϕ˜ (s ,a )⊤.
i D
k
i,κ N iκ+1 N iκ+1 i,κ N iκ+1 N iκ+1
| |s,a,s′,a′∈Dk
For notational convenience, when the context is clear, we drop the k-superscript indicating the current round
k, and denote w :=w(k), M :=M(k) and H :=H(k).
i i i i i i
We define an intermediate variable w˜ as follows:
i
w˜
i
=M(cid:102) i−1H(cid:101)ir
i
where
(cid:16) (cid:17)⊤
M(cid:102)i =E s,a∼νϕ˜ i,κ(s
N
iκ+1,a
N
iκ+1) ϕ˜ i,κ(s
N
iκ+1,a
N
iκ+1) −γE s′,a′∼P(·|s,a),π(·|s′)ϕ˜ i,κ(s′
N
iκ+1,a′
N
iκ+1)
H(cid:101)i =E s,a∼νϕ˜ i,κ(s Nκ+1,a Nκ+1)ϕ˜ i,κ(s Nκ+1,a Nκ+1)⊤
i i i i
and further define
Q(cid:101)i(s Nκ+1,a Nκ+1)=ϕ˜ i,κ(s Nκ+1,a Nκ+1)⊤w˜
i
i i i i
The real Q-function is Qπ(k)(s,a). From Lemma 3 and Lemma 4, we have that there exists
i
Qˆ (s ,a )=ϕ˜ (s ,a )⊤wˆ
i Nκ+1 Nκ+1 i,κ Nκ+1 Nκ+1 i
i i i i
such that with probability at least 1 δ, for every i [n],
− ∈
(cid:104) (cid:105) (cid:13) ν (cid:13) ϵ γr¯
E Qπ(k)(s,a) Qˆ (s ,a ) ⩽2cρκ+1+(cid:13) (cid:13) P (4)
ν | i − i N iκ+1 N iκ+1 | (cid:13)νo(cid:13) ∞1 γ
−
23Assumption 1.
M−1 ⩽D
∥ i ∥
Assumption 2.
ϕ˜ (s ,a ) ⩽L, s ,a
∥ i,κ N iκ+1 N iκ+1 ∥ ∀ N iκ+1 N iκ+1
Lemma 10 (Bellman Error). On the event that condition (1) in Lemma 4 holds, for every i [n], we have
∈
(cid:18) (cid:13) ν (cid:13) γr¯ϵ (cid:19)
w˜ wˆ ⩽2LD cρκ+1+(cid:13) (cid:13) P
∥ i − i ∥ (cid:13)νo(cid:13) ∞1 γ
−
Proof. From Bellman equation we have that
Qπ(k)(s,a)=r (s ,a )+γE Qπ(k)(s′,a′)
i i i i s′,a′∼P,π i
= Qˆ (s ,a )=r (s ,a )+γE Qˆ (s′ ,a′ )+∆(s,a),
⇒ i N iκ+1 N iκ+1 i i i s′,a′∼P,π i N iκ+1 N iκ+1
(cid:16) (cid:17) (cid:16) (cid:17)
where ∆(s,a) = Qπ(k)(s,a) Qˆ (s ,a ) +γE Qπ(k)(s′,a′) Qˆ (s′ ,a′ ) . Sub-
− i − i N iκ+1 N iκ+1 s′,a′∼P,π i − i N iκ+1 N iκ+1
stituting Qˆ (s ,a )=ϕ˜ (s ,a )⊤wˆ into the above equation we have
i Nκ+1 Nκ+1 i,κ Nκ+1 Nκ+1 i
i i i i
ϕ˜ (s ,a )⊤wˆ =ϕ˜ (s ,a )⊤r +γE ϕ˜ (s′ ,a′ )⊤wˆ +∆(s,a).
i,κ N iκ+1 N iκ+1 i i,κ N iκ+1 N iκ+1 i s′,a′∼P,π i,κ N iκ+1 N iκ+1 i
On both side multiply by ϕ˜ (s ,a ) and take expectation over s,a ν, we have that
i,κ N iκ+1 N iκ+1 ∼
E ϕ˜ (s ,a )ϕ˜ (s ,a )⊤wˆ
s,a∼ν i,κ Nκ+1 Nκ+1 i,κ Nκ+1 Nκ+1 i
i i i i
=E ϕ˜ (s ,a )ϕ˜ (s ,a )⊤r
s,a∼ν i,κ Nκ+1 Nκ+1 i,κ Nκ+1 Nκ+1 i
i i i i
+γE ϕ˜ (s ,a )E ϕ˜ (s′ ,a′ )⊤wˆ +E ϕ˜ (s ,a )∆(s,a)
s,a∼ν i,κ N iκ+1 N iκ+1 s′,a′∼P,π i,κ N iκ+1 N iκ+1 i s,a∼ν i,κ N iκ+1 N iκ+1
=
⇒
M(cid:102)iwˆ
i
=H(cid:101)ir i+E s,a∼νϕ˜ i,κ(s
N
iκ+1,a′
N
iκ+1)∆(s,a)
Further, given that
M(cid:102)iw˜
i
=H(cid:101)ir i,
on the event that condition (1) in Lemma 4 holds.
wˆ
i
−w˜
i
=M(cid:102) i−1E s,a∼νϕ˜ i,κ(s
N
iκ+1,a′
N
iκ+1)∆(s,a)
=
⇒
∥wˆ
i
−w˜
i
∥⩽ ∥M(cid:102) i−1 ∥∥E s,a∼νϕ˜ i,κ(s
N
iκ+1,a′
N
iκ+1)∆(s,a)
∥
(cid:18) (cid:13) ν (cid:13) ϵ γr¯(cid:19)
⩽2LD cρκ+1+(cid:13) (cid:13) P ,
(cid:13)νo(cid:13) ∞1 γ
−
where for the final inequality we used (4).
(cid:16) (cid:17)
Lemma 11 (Statistical Error). Fix an i [n] and k [K]. For sample size M ⩾log 2(m+1) , we have
∈ ∈ s δ
that with probability at least 1 2δ
−
(cid:18) (cid:18) (m+1)(cid:19)(cid:19) D2L4
w(k) w˜(k) ⩽O log
∥ i − i ∥ δ √M
s
24Proof. Again, for notational simplicity we drop the k-superscript. We first bound the differences M
i
M(cid:102)i ,
∥ − ∥
H
i
H(cid:101)i . Since
∥ − ∥
M = 1 (cid:88) ϕ˜ (s ,a )(cid:16) ϕ˜ (s ,a ) γϕ˜ (s′ ,a′ )(cid:17)⊤ ,
i D
k
i,κ N iκ+1 N iκ+1 i,κ N iκ+1 N iκ+1 − i,κ N iκ+1 N iκ+1
| |s,a,s′,a′∈Dk
(cid:16) (cid:17)⊤
M(cid:102)i =E s,a∼νϕ˜ i,κ(s
N
iκ+1,a
N
iκ+1) ϕ˜ i,κ(s
N
iκ+1,a
N
iκ+1) −γE s′,a′∼P(·|s,a),π(·|s′)ϕ˜ i,κ(s′
N
iκ+1,a′
N
iκ+1) ,
(cid:16) (cid:17)
FromtheMatrixBernsteininequality(seeLemma15inAppendix7.7)wehavethatwhenM ⩾log 2(m+1)
s δ
with probability at least 1 δ
−
(cid:115)
(cid:18) (cid:19)
2(m+1)
∥M
i
−M(cid:102)i ∥⩽8L2 M s−1log
δ
(cid:115)
(cid:18) (cid:19)
2(m+1)
∥H
i
−H(cid:101)i ∥⩽8L2 M s−1log
δ
Thus with probability 1 2δ
−
∥w
i
−w˜
i
∥= ∥M i−1H ir
i
−M(cid:102) i−1H(cid:101)ir
i
∥
⩽ ∥M i−1 −M(cid:102) i−1 ∥∥H(cid:101)ir
i
∥+ ∥M i−1 ∥∥H
i
−H(cid:101)i ∥∥r
i
∥
⩽ ∥M i−1M(cid:102) i−1 ∥∥M
i
−M(cid:102)i ∥∥H(cid:101)i ∥+ ∥M i−1 ∥∥H
i
−H(cid:101)i
∥
(cid:18) (cid:18) (m+1)(cid:19)(cid:19) D2L4+L4 (cid:18) (cid:18) (m+1)(cid:19)(cid:19) D2L4
⩽O log O log ,
δ √M ≃ δ √M
s s
which completes the proof.
Combining the above statement we can get the following Lemma for policy evaluation error, which is a
restatement of our result in Lemma 6.
Lemma 12 (Policy Evaluation Error, restatement of Lemma 6). Suppose condition (1) in Lemma 4 holds.
(cid:16) (cid:17)
Suppose the sample size M ⩾ log 2(m+1) . Then, with probability at least 1 2δ, for every i [n]
s δ/(Kn) − ∈
and k [K], the ground truth Q function Qπ(k)(s,a) and the truncated Q function learnt in Algorithm 1
∈ i
Qˆ (s ,a ) satisfies, for any distribution ν¯ on ,
i N iκ+1 N iκ+1 S×A
(cid:104) (cid:105)
E Qπ(k)(s,a) Qˆ (s ,a )
ν¯ | i − i N iκ+1 N iκ+1 |
(cid:18) (cid:18) (m+1)(cid:19) D2L5 ϵ γr¯ (cid:16)(cid:13) ν¯ (cid:13) (cid:13)ν (cid:13) (cid:17)(cid:19)
⩽ O cρL2Dρκ+1+log +L P (cid:13) (cid:13) +(cid:13) π(k)(cid:13) ,
δ/(Kn) √M s 1 γ (cid:13)νo(cid:13) ∞ (cid:13) νo (cid:13) ∞
−
where denoting
φ˜ :=ϕ˜ (s ,a ),φ˜′ :=ϕ˜ (s′ ,a′ ),
i,κ i,κ N iκ+1 N iκ+1 i,κ i,κ N iκ+1 N iκ+1
(cid:13) (cid:13)
D := max (cid:13)(M(k))−1(cid:13), L:=max φ˜ , where
i∈[n],k∈[K](cid:13) i (cid:13) i∈[n]∥ i,κ ∥
M(k) := 1 (cid:88) φ˜ (φ˜ γφ˜′ )⊤.
i D i,κ i,κ − i,κ
k
| |s,a,s′,a′∈Dk
25Proof. Suppose the condition (1) in Lemma 4 holds. Consider any i [n] and k [K]. From Lemma 10 and
∈ ∈
11 we have that with probability at least 1 2δ,
−
(cid:104) (cid:105)
E Qπ(k)(s,a) ϕ˜ (s ,a )⊤w(k)
ν¯ | i − i,κ N iκ+1 N iκ+1 i |
(cid:104) (cid:105) (cid:104) (cid:105)
⩽E Qπ(k)(s,a) Qˆ (s ,a ) +E Qˆ (s ,a ) ϕ˜ (s ,a )⊤w(k)
ν¯ | i − i N iκ+1 N iκ+1 | ν¯ | i N iκ+1 N iκ+1 − i,κ N iκ+1 N iκ+1 i |
(cid:18) (cid:13) ν¯ (cid:13) ϵ γr¯(cid:19) (cid:104) (cid:105)
⩽ 2cρκ+1+(cid:13) (cid:13) P +E ϕ˜ (s ,a )⊤(wˆ(k) w(k))
(cid:13)νo(cid:13) ∞1 γ ν¯ | i,κ N iκ+1 N iκ+1 i − i |
−
(cid:18) (cid:13) ν¯ (cid:13) ϵ γr¯(cid:19) (cid:16) (cid:17)
⩽ 2cρκ+1+(cid:13) (cid:13) P +L wˆ(k) w˜(k) + w˜(k) w(k)
(cid:13)νo(cid:13) ∞1 γ ∥ i − i ∥ ∥ i − i ∥
−
(cid:18) (cid:13) ν¯ (cid:13) ϵ γr¯(cid:19) (cid:18) (cid:13) ν (cid:13) ϵ γr¯ (cid:18) (cid:18) (m+1)(cid:19)(cid:19) D2L4(cid:19)
⩽ 2cρκ+1+(cid:13) (cid:13) P +L 2LD(cρκ+1+(cid:13) (cid:13) P )+O log
(cid:13)νo(cid:13) ∞1 γ (cid:13)νo(cid:13) ∞1 γ δ √M
s
− −
(cid:18) (cid:18) (m+1)(cid:19) D2L5 ϵ γr¯ (cid:16)(cid:13) ν¯ (cid:13) (cid:13) ν (cid:13) (cid:17)(cid:19)
=O cρL2Dρκ+1+log +L P (cid:13) (cid:13) +(cid:13) (cid:13) .
δ √M s 1 γ (cid:13)νo(cid:13) ∞ (cid:13)νo(cid:13) ∞
−
The desired result then follows by rescaling δ := δ/(Kn) and taking an union bound over all i [n] and
∈
k [K].
∈
7.6 Policy gradient analysis
We show now that our algorithm can find an approximate stationary point of the averaged discounted
cumulative reward function J(π(θ)). For notational convenience, for a given set of policy parameters
θ =(θ ,...,θ ), we define
1 n
(cid:34) ∞ (cid:35)
(cid:88)
J(θ):=J(πθ)=E E γtr(s(t),a(t)) s(0)=s ,
s∼µ0 a(t)∼π(θ)(·|s(t))
|
t=0
where we recall that r(s,a):= 1 (cid:80)n r (s ,a ). From Lemma 1, we have that
n i=1 i i i
(cid:104) (cid:105)
∇θJ(θ)= E
s∼dθ,a∼πθ(·|s)
Qθ(s,a) ∇θilogπ iθ(a
i
|s
N
iκπ)
 
n
1 (cid:88)
= E s∼dθ,a∼πθ(·|s) n Qθ j(s,a) ∇θlogπ iθi(a i |s N iκπ)
j=1
We first provide the following result, which shows that assuming Lipschitz continuity of the gradient of
the objective function J as well as the gradients of logπθ, there exists the following bound on the following
weighted sum of the squared gradient norms.
(cid:13) (cid:13)
Lemma13. Supposethat J(θ)isL′-Lipschitzcontinuous. Supposethatforeachi [n],(cid:13) logπθi( )(cid:13)⩽
∇ ∈ (cid:13) ∇θi i ·|· (cid:13)
(cid:113)
L . Denote L := (cid:80)n L2 . Suppose for each round k [K], θ(k+1) =θ(k) ηgˆ(k). Then,
i,π π i=1 i,π ∈ −
1 (cid:88)K (cid:13)
(cid:13)
J(θ(k))(cid:13) (cid:13)2
⩽
r¯/(1 −γ)
+
1 (cid:88)K L πr¯ (cid:13)
(cid:13) J(θ(k))
gˆ(k)(cid:13)
(cid:13)+
1 (cid:88)K L′η(cid:32) (cid:13)
(cid:13)gˆ(k)
J(θ(k))(cid:13) (cid:13)2 +(cid:18) L πr¯ (cid:19)2(cid:33)
.
K (cid:13) ∇ (cid:13) ηK K 1 γ(cid:13) ∇ − (cid:13) K (cid:13) −∇ (cid:13) 1 γ
k=1 k=1 − k=1 −
Proof. By the Lipschitz continuity of J(θ), we have
∇
(cid:68) (cid:69) L′(cid:13) (cid:13)2
J(θ(k+1))⩾ J(θ(k))+η J(θ(k)),gˆ(k) (cid:13)ηgˆ(k)(cid:13)
∇ − 2 (cid:13) (cid:13)
26(cid:13) (cid:13)2 (cid:68) (cid:69) L′η2(cid:13) (cid:13)2
= J(θ(k))+η(cid:13) J(θ(k))(cid:13) +η J(θ(k)),gˆ(k) J(θ(k)) (cid:13)gˆ(k)(cid:13)
(cid:13) ∇ (cid:13) ∇ −∇ − 2 (cid:13) (cid:13)
By rearranging and using a telescoping sum, we obtain
(cid:88)K (cid:13) (cid:13)2 (cid:88)K (cid:68) (cid:69) L′η2(cid:13) (cid:13)2
η (cid:13) J(θ(k))(cid:13) ⩽ (J(θ(k+1)) J(θ(k)))+η J(θ(k)), J(θ(k)) gˆ(k) + (cid:13)gˆ(k)(cid:13)
(cid:13) ∇ (cid:13) − ∇ ∇ − 2 (cid:13) (cid:13)
k=1 k=1
1 (cid:88)K (cid:13) (cid:13)2 J(θ(K+1)) J(θ(1)) 1 (cid:88)K (cid:13) (cid:13)(cid:13) (cid:13) 1 (cid:88)K L′η(cid:13) (cid:13)2
= (cid:13) J(θ(k))(cid:13) ⩽ − + (cid:13) J(θ(k))(cid:13)(cid:13) J(θ(k)) gˆ(k)(cid:13)+ (cid:13)gˆ(k)(cid:13) .
⇒ K (cid:13) ∇ (cid:13) ηK K (cid:13) ∇ (cid:13)(cid:13) ∇ − (cid:13) K 2 (cid:13) (cid:13)
k=1 k=1 k=1
Recall that J(θ(K+1)) J(θ(1))⩽r¯/(1 γ). Hence, by the given assumption on the bound on the derivative
term ∇θilogπ iθi, it fol− lows that ∥∇J( ·)− ∥⩽ 1L −π γr¯. The desired bound then follows by plugging this in as well
as using the triangle inequality to decompose
(cid:13) (cid:13)gˆ(k)(cid:13) (cid:13)2
.
As we can tell from the above result, the crux to bounding the average stationarity gap after K rounds of
optimization is the difference between the true gradient J(θ(k)) and the learned gradient gˆ(k) used in the
update. In this next result, we bound this error, assumin∇ g that the truncated local Qˆ -functions are learned
i
up to some error.
Lemma 14. For any optimization round k [K], let νˆ(k) denote the empirical distribution of the samples
used during round k, i.e. s(j),a(j) ∈where (s(j),a(j)) ν . Suppose that for each ℓ [n], the
{
}j∈[Ms]
∼
π(k)
∈
learnt Qˆ -value function satisfies the following error bound:
ℓ
(cid:104)(cid:12) (cid:12)(cid:105)
E (cid:12)Qˆ(k)(s ,a ) Q (s,a)(cid:12) ⩽ϵ (5)
νˆ(k) (cid:12) ℓ N ℓκ+1 N ℓκ+1 − ℓ (cid:12) Q
(cid:13) (cid:13) (cid:113)
Suppose that for each i [n], (cid:13) logπθi( )(cid:13) ⩽ L . Denote L := (cid:80)n L2 . Then, for any
∈ (cid:13) ∇θi i ·|· (cid:13) i,π π i=1 i,π
optimization round k [K], with probability at least 1 δ,
∈ −
(cid:115)
(cid:13) (cid:13) 2r¯L 1 (cid:18) d +1(cid:19)
(cid:13)gˆ(k) J(θ(k))(cid:13)⩽2cL ρκ+ π log θ +ϵ L , (6)
(cid:13) −∇θ (cid:13) π 1 γ M δ/K Q π
s
−
where the i-th component of the approximate gradient
gˆ i(k) := M1
s
(cid:88)Ms
n1 (cid:88) Qˆ ℓ(s
N
ℓκ+1(j),a
N
ℓκ+1(j)) ∇θilogπ i(θ i(k)) (a i(j) |s
N
iκπ(j))
j=1 ℓ∈Nκ+κπ
i
is defined in Line 10 of Algorithm 1.
Proof. For notational convenience, in the proof, we fix the optimization round k [K], and hence, denote
∈
gˆ
i
:= gˆ i(k), θ := θ(k) and Qˆ
ℓ
:= Qˆ( ℓk) unless otherwise specified. Moreover, we also denote Qθ := Qπθ for
simplicity. From Lemma 1, for any agent i [n], we have that
∈
(cid:104) (cid:105)
∇θiJ(θ)= E
s∼dθ,a∼πθ(·|s)
Qθ(s,a) ∇θilogπ iθi(a
i
|s
N
iκπ)
(cid:34) n (cid:35)
1 (cid:88)
= E
s∼dθ,a∼πθ(·|s) n
Qθ ℓ(s,a) ∇θilogπ iθi(a
i
|s
N
iκπ)
ℓ=1
27To bound the difference between gˆ and J(θ), we define the following intermediate terms.
i ∇θi
We define the terms
 
1
(cid:88)Ms
1 (cid:88)
g i := M
s
 n Qθ ℓ(s(j),a(j)) ∇θilogπ iθi(a i(j) |s N iκπ(j))
j=1 ℓ∈Nκ+κπ
i
 
1 (cid:88)
h i :=E s∼dθ,a∼πθ(·|s) n Qθ ℓ(s,a) ∇θilogπ iθi(a i |s N iκπ).
ℓ∈Nκ+κπ
i
Then, we decompose the error as
J(θ) gˆ =( J(θ) h )+(h g )+(g gˆ). (7)
∇θi
−
i ∇θi
−
i i
−
i i
−
i
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
EJ,h Eh,g Eg,gˆ
We proceed now to bound the three error terms in (7).
Error term E . We can bound the term E as follows. For any ℓ [n] and positive integer κ, we
J,h J,h
∈
define
Q˜θ(s ,a ):= (cid:88) w((s )′,(a )′)Q (s ,a ,(s )′,(a )′),
ℓ Nκ Nκ Nκ Nκ ℓ Nκ Nκ Nκ Nκ
ℓ ℓ −ℓ −ℓ ℓ ℓ −ℓ −ℓ
(s N−κ ℓ)′,(a N−κ ℓ)′
where we let w((s )′,(a )′) denote the uniform weight over the space . From Lemma 8, we
Nκ Nκ Nκ Nκ
ℓ ℓ S −ℓ ×A −ℓ
know that
(cid:12) (cid:12)
(cid:12)Qθ(s,a) Q˜θ(s ,a )(cid:12)⩽2cρκ. (8)
(cid:12) ℓ − ℓ N ℓκ N ℓκ (cid:12)
We then have
  
n
1 (cid:88) 1 (cid:88)
∇θiJ(θ) −h i = E s∼dθ,a∼πθ(·|s) n Q ℓ(s,a) − n Qθ ℓ(s,a) ∇θilogπ iθi(a i |s N iκπ)
ℓ=1 ℓ∈Nκ+κπ
i
  
1 (cid:88)
= E s∼dθ,a∼πθ(·|s) 
n
Qθ ℓ(s,a) ∇θilogπ iθi(a
i
|s
N
iκπ)

ℓ∈Nκ+κπ
−i
  
= E s∼dθ,a∼πθ(·|s)  n1 (cid:88) Q˜θ ℓ(s
N
ℓκ,a
N
ℓκ)+(cid:16) Qθ ℓ(s,a) −Q˜θ ℓ(s
N
ℓκ,a
N
ℓκ)(cid:17)  ∇θilogπ iθi(a
i
|s
N
iκπ)

ℓ∈Nκ+κπ
−i
  
= E s∼dθ,a∼πθ(·|s)  n1 (cid:88) (cid:16) Qθ ℓ(s,a) −Q˜θ ℓ(s
N
ℓκ,a
N
ℓκ)(cid:17)  ∇θilogπ iθi(a
i
|s
N
iκπ)

ℓ∈Nκ+κπ
−i
  
+ E s∼dθ,a∼πθ(·|s)  n1 (cid:88) Q˜θ ℓ(s
N
ℓκ,a
N
ℓκ) ∇θilogπ iθi(a
i
|s
N
iκπ)

ℓ∈Nκ+κπ
−i
:= E +E
J,h,1 J,h,2
28To bound E , utilizing the bound in (8) as well as the bound logπθi(a sκ )⩽L in the statement
J,h,1 ∇θi i i | Ni i,π
of the lemma, we have that
E ⩽2cL ρκ.
J,h,1 i,π
∥ ∥
Meanwhile, observe that by definition, for any ℓ ∈N −κ+ iκπ, Q˜θ ℓ(s
N
ℓκ,a
N
ℓκ) does not depend on s
N
iκπ. Hence,
  
E
J,h,2
= E s∼dθ,a∼πθ(·|s)  n1 (cid:88) Q˜θ ℓ(s
N
ℓκ,a
N
ℓκ) ∇θilogπ iθi(a
i
|s
N
iκπ)

ℓ∈Nκ+κπ
−i
  
= E s∼dθ,a−i∼πθ(·|s)  n1 ℓ∈N(cid:88) κ+κπQ˜θ ℓ(s
N
ℓκ,a
N
ℓκ) E
ai∼π iθi(·|s
Niκπ)(cid:104) ∇θilogπ iθi(a
i
|s
N
iκπ)(cid:105) 

−i
  
= E s∼dθ,a−i∼πθ(·|s)  n1 (cid:88) Q˜θ ℓ(s
N
ℓκ,a
N
ℓκ)
∇θi(cid:18)(cid:90)
π iθi(a
i
|s
N
iκπ)da
i(cid:19)


ℓ∈Nκ+κπ ai
−i
  
= E 1 (cid:88) Q˜θ(s ,a ) (1)=0.
s∼dθ,a−i∼πθ(·|s)n ℓ N ℓκ N ℓκ ∇θi 
ℓ∈Nκ+κπ
−i
Thus E =0. This implies then that
J,h,2
E = J(θ) h ⩽2cL ρκ. (9)
∥
J,h
∥
∥∇θi
−
i
∥
i,π
Error term E . To bound E , we may use standard concentration inequalities. Observe that
h,g h,g
E := h g
h,g i i
−
 
1
(cid:88)Ms
1 (cid:88)
= h i − M
s
 n Qθ ℓ(s(j),a(j)) ∇θilogπ iθi(a i(j) |s N iκπ(j))
j=1 ℓ∈Nκ+κπ
i
  
1
(cid:88)Ms
1 (cid:88)
= M
s
h i − n Qθ ℓ(s(j),a(j)) ∇θilogπ iθi(a i(j) |s N iκπ(j)).
j=1 ℓ∈Nκ+κπ
i
(cid:124) (cid:123)(cid:122) (cid:125)
Eh,g(j)
Since
 
1 (cid:88)
h i =E s∼dθ,a∼πθ(·|s) n Qθ ℓ(s,a) ∇θilogπ iθi(a i |s N iκπ),
ℓ∈Nκ+κπ
i
it follows that E(cid:2) E (cid:3) =0. Moreover, using the fact that for any ℓ [n], θ and (s,a) pair, 0⩽Qθ(s,a)⩽
h,g(j) ∈ ℓ
1−r¯ γ, and the bound ∇θilogπ iθi(a
i
|
s
N
iκ) ⩽ L
i,π
in the assumption, we have ∥E h,g(j)
∥
⩽ 2 1r¯ −L γπ. Using the
i.i.d. assumptionbetweenthesamplesj [M ],wemayapplyBernstein’sconcentrationinequalityforvectors
s
∈
(see Lemma 15) to find that for any δ >0, with probability at least 1 δ,
−
(cid:115)
(cid:18) (cid:19)
2r¯L 1 d +1
E ⩽ i,π log θ , (10)
∥ h,g ∥ 1 γ M δ
s
−
29where d is the dimension of θ .
θ i
Error term E . Observe that
g,gˆ
g
i
−gˆ
i
= M1
s
(cid:88)Ms
n1 (cid:88) (cid:16) Q ℓ(s(j),a(j)) −Qˆ ℓ(s
N
ℓκ+1(j),a
N
ℓκ+1(j))(cid:17) ∇θilogπ i(θ i(k)) (a i(j) |s
N
iκπ(j))
j=1 ℓ∈Nκ+κπ
i
= n1 (cid:88) M1
s
(cid:88)Ms
(cid:16) Q ℓ(s(j),a(j)) −Qˆ ℓ(s
N
ℓκ+1(j),a
N
ℓκ+1(j))(cid:17) ∇θilogπ i(θ i(k)) (a i(j) |s
N
iκπ(j))
ℓ∈Nκ+κπ j=1
i
⩽ n1 (cid:88) M1
s
(cid:88)Ms(cid:12) (cid:12) (cid:12)(cid:16) Q ℓ(s(j),a(j)) −Qˆ ℓ(s N ℓκ+1(j),a N ℓκ+1(j))(cid:17)(cid:12) (cid:12) (cid:12)(cid:13) (cid:13) (cid:13) (cid:13)∇θilogπ i(θ i(k)) (a i(j) |s N iκπ(j))(cid:13) (cid:13) (cid:13) (cid:13)
ℓ∈Nκ+κπ j=1
i
(ix) 1 (cid:88)
⩽ ϵ L ⩽ϵ L .
n Q · i,π Q · i,π
ℓ∈Nκ+κπ
i
(cid:13) (cid:13)
Above, (ix) follows from the bound in (5), as well as the bound (cid:13) logπθi( )(cid:13)⩽L in the assumption.
(cid:13) ∇θi i ·|· (cid:13) i,π
Combining the bounds for E ,E and E , we find that with probability at least 1 δ,
J,h h,g g,gˆ
−
(cid:118) (cid:115)
(cid:117) n (cid:18) (cid:19)
∥∇θJ(θ) −gˆ ∥⩽(cid:117) (cid:116)(cid:88) ∥∇θiJ(θ) −gˆ
i
∥2 ⩽2cL πρκ+ 12r¯L π
γ
M1 log d θ δ+1 +ϵ QL
π
s
i=1 −
The final result then follows by applying a union bound over k [K].
∈
We are now ready to state our main convergence result.
(cid:16) (cid:17)
Theorem 2 (Restatement of Theorem 1). Suppose the sample size M s ⩾ log (δ2 /d Kκ n) . Suppose with
probability at least 1 δ, for all i [n], the following holds for some features ϕˆ and µˆ :
i,κ i,κ
− ∈
 
(cid:90) (cid:12) (cid:12)
E νo
s+
Niκ(cid:12) (cid:12)P(s+ N iκ |s N iκ+1,a N iκ+1) −ϕˆ i,κ(s N iκ+1,a N iκ+1)⊤µˆ i,κ(s+ N iκ)(cid:12) (cid:12)ds+ N iκ⩽ϵ P
for some ϵ >0. Then, if η =O(1/√K), we have that with probability at least 1 4δ,
P
−
1 (cid:88)K (cid:13)
(cid:13)
J(θ(k))(cid:13) (cid:13)2 ⩽O(cid:32) r¯/(1 −γ)
+
L πr¯ϵ
J +
L′ (cid:32)
ϵ2
+(cid:18) L πr¯ (cid:19)2(cid:33)(cid:33)
,
K (cid:13) ∇ (cid:13) √K 1 γ √K J 1 γ
k=1 − −
where
(cid:115)
(cid:18) (cid:19)
2r¯L 1 d +1
ϵ :=2cL ρκ+ π log θ +ϵ L ,
J π 1 γ M δ/K Q π
s
−
and
ϵ Q := k∈{0,m 1..a .,x K−1}O(cid:18) cρL2Dρκ+1+log(cid:18) d δκ(cid:19) D √2 ML s5 +L 1ϵ P −γ γr¯ (cid:18)(cid:13) (cid:13) (cid:13) (cid:13)νˆ ν(k o)(cid:13) (cid:13) (cid:13) (cid:13) ∞+(cid:13) (cid:13) (cid:13)ν π ν( ok)(cid:13) (cid:13) (cid:13) ∞(cid:19)(cid:19) .
30Proof. Fix a δ > 0. Suppose the condition in (1) holds with probability at least 1 δ for all i [n] for a
− ∈
distribution νo over . In other words, with probability at least 1 δ, for all i [n], the following holds:
S×A − ∈
 
(cid:90) (cid:12) (cid:12)
E νo
s+
Niκ(cid:12) (cid:12)P(s+ N iκ |s N iκ+1,a N iκ+1) −ϕˆ i,κ(s N iκ+1,a N iκ+1)⊤µˆ i,κ(s+ N iκ)(cid:12) (cid:12)ds+ N iκ⩽ϵ P
for some ϵ >0. Then, by Lemma 6, it follows that with probability at least 1 3δ, for every i [n] and
P
− ∈
optimization round k [K], we have
∈
(cid:104)(cid:12) (cid:12)(cid:105)
E (cid:12)Qˆ(k)(s ,a ) Q (s,a)(cid:12) ⩽ϵ(k),
νˆ(k) (cid:12) ℓ N ℓκ+1 N ℓκ+1 − ℓ (cid:12) Q
where
ϵ( Qk) :=O(cid:18) cρL2Dρκ+1+log(cid:18) δ/(d Kκ n)(cid:19) D √2 ML s5 +L 1ϵ P −γ γr¯ (cid:18)(cid:13) (cid:13) (cid:13) (cid:13)νˆ ν(k o)(cid:13) (cid:13) (cid:13) (cid:13) ∞+(cid:13) (cid:13) (cid:13)ν π ν( ok)(cid:13) (cid:13) (cid:13) ∞(cid:19)(cid:19) .
Note that by Lemma 13, with probability at least 1 δ, for every optimization round k [K], we have
− ∈
(cid:115)
(cid:13) (cid:13) 2r¯L 1 (cid:18) d +1(cid:19)
(cid:13)gˆ(k) J(θ(k))(cid:13)⩽2cL ρκ+ π log θ +ϵ(k)L .
(cid:13) −∇θ (cid:13) π 1 γ M δ/K Q π
s
−
Thus, by picking η =O(1/√K), using union bound, with probability at least 1 4δ, we have
−
1 (cid:88)K (cid:13)
(cid:13)
J(θ(k))(cid:13) (cid:13)2 ⩽O(cid:32) r¯/(1 −γ)
+
L πr¯ϵ
J
+L′η(cid:32)
ϵ2
+(cid:18) L πr¯ (cid:19)2(cid:33)(cid:33)
,
K (cid:13) ∇ (cid:13) √K 1 γ J 1 γ
k=1 − −
where
(cid:115)
(cid:18) (cid:19)
2r¯L 1 d +1
ϵ :=2cL ρκ+ π log θ + max ϵ(k)L .
J π 1 γ M s δ/K k∈[K] Q π
−
7.7 Concentration inequalities
Lemma 15 (Matrix Bernstein). Suppose {M
k
}n
k=1
are i.i.d random matrices where M
k
∈Rd1×d2 and that
M EM ⩽C,
k k
∥ − ∥
then for a given δ (0,1) and n⩾log(cid:0)d1+d2(cid:1), we have
∈ δ
Pr(cid:32) 1 (cid:13) (cid:13) (cid:13)(cid:88)n
(M EM
)(cid:13) (cid:13) (cid:13)⩾2C(cid:115) n−1log(cid:18) d 1+d 2(cid:19)(cid:33)
⩽δ,
n(cid:13) (cid:13) k − k (cid:13) (cid:13) δ
k=1
(cid:113)
Proof. Let ϵ:=2C n−1log(cid:0)d1+d2(cid:1) , then since n⩾log(cid:0)d1+d2(cid:1) , we have ϵ⩽2C.
δ δ
Now we can apply the matrix Bernstein inequality (Theorem 6.1.1 in [Tropp et al., 2015]) and get that
(cid:32) 1 (cid:13) (cid:13)(cid:88)n (cid:13) (cid:13) (cid:33) (cid:18) n2ϵ2/2 (cid:19)
Pr (cid:13) (M EM )(cid:13)⩾ϵ ⩽(d +d )exp −
n(cid:13) (cid:13) k − k (cid:13) (cid:13) 1 2 nC2+Cnϵ/3
k=1
31(cid:18) n2ϵ2/2 (cid:19) (cid:18) nϵ2 (cid:19)
⩽(d +d )exp − =(d +d )exp
1 2 nC2+nC2 1 2 4C2
(cid:113)
Substituting ϵ=2C
n−1log(cid:0)d1+d2(cid:1)
into the right hand side of the equation we get
δ
(cid:32) 1 (cid:13) (cid:13)(cid:88)n (cid:13) (cid:13) (cid:33)
Pr (cid:13) (M EM )(cid:13)⩾ϵ ⩽δ,
n(cid:13)
(cid:13)
k
−
k (cid:13)
(cid:13)
k=1
which completes the proof.
327.8 Simulation details
7.8.1 Thermal control of multi-zone building
Problem setup details. In the simulations, we consider a discrete-time linear thermal dynamics model
adapted from [Zhang et al., 2016, Li et al., 2021], where for any i [n],
∈
(cid:114)
∆ (cid:88) ∆ ∆ ∆
x (t+1) x (t)= (θo(t) x (t))+ (x (t) x (t))+ α a (t)+ β w (t),
i − i v ζ − i v ζ j − i v i i v i i
i i i ij i i
j∈Ni
where x (t) denotes the temperature of zone i at time t, a (t) denotes the control input of zone i that is
i i
related with the air flow rate of the HVAC system, θo(t) denotes the outdoor temperature, π represents a
i
constant heat from external sources to zone i, w (t) represents random disturbances, ∆ is the time resolution,
i
v is the thermal capacitance of zone i, ζ represents the thermal resistance of the windows and walls between
i i
zone i and the outside environment, ζ represents the thermal resistance of the walls between zone i and j,
ij
and α and β denote scaling factors on the input and noise respectively. The local reward is defined as
i i
r (t)= ρ ((x (t) θ∗))2+a (t)2,
i − i i − i i
where θ∗ is the target temperature and ρ is a trade-off parameter.
i i
Theparameters inthedynamics andrewardsareset asfollows. Forsimplicity, wecenterthe temperatures
at 0, and hence set the target θ∗ to be 0. We set ρ =3. We set the following parameters for the dynamics:
i i
∆=20,v
i
=200,ζ
ij
=1,ζ
i
= 21,α
i
= 1 7,β
i
=(cid:112)v ∆i,θ0 =0.
We also assume w (t) to be drawn iid from N(0,1). We set the discount factor in the problem to be 0.75,
i
and (when collecting data) set the horizon length of each episode to be 20.
Connectivity. In this problem, there are n=50 agents, and the agents have circular connectivity and
has two neighbors each, such that agent 1 is connected to agents N and agent 2, agent 2 is connected to
agents 1 and 3, so on and so forth.
Experimental details. We assume knowledge of the dynamics and rewards. For policy truncation
parameter κ =0,1,2,3, we use κ=0,1,2,2 respectively4 as the evaluation κ parameter. We now explain
π
the simulation setup for our implementation of Algorithm 1 with random features, as well as the benchmark
algorithm using a two-hidden layer NN.
1. (Spectral embedding generation step). For Algorithm 1 with random features, for each agent i, we use
random feature dimension of m=30,50,800,800 for each of the four experiments (κ =0,1,2,3) to
π
represent the function Tπ(s Nκ+1,a Nκ+1):=
Qπ i(s Niκ+1,a
N
γiκ+1)−ri(si,ai)
. For the NN implementation, we
i i
used a two-hidden layer NN with 128 neurons to represent the function Tπ(s ,a ).
Nκ+1 Nκ+1
i i
2. (Policy evaluation step) We used M = 100,200,500,1000 episodes respectively for each of the four
s
experiments (κ =0,1,2,3) to perform the policy evaluation. For the random features implementation,
π
we used the least squares method in Algorithm 1 to compute the new weights for the local value
functions. For the NN implementation, we ran batch gradient descent, and used a target network with
update rate of 0.005.
3. (Policy update step). For both implementations, we normalize the policy gradient, and run gradient
descent with η =0.2.
4Wefoundinpracticethatusingκ=3forκπ =3performedlesswellinthisspecificexample.
337.8.2 Kuramoto synchronization
Problem setup details. We recall the setup described earlier in the paper. We consider here a Kuramoto
system with n agents, with an underlying graph =( , ), where = 1,...,n is the set of agents and
G N E N { }
isthesetofedges. Thestateofeachagent iisitsphase θ [ π,π], andtheactionofeachagent
i
E is⊆ a sN ca× larN a R in a bounded subset of R. The dynamics of each∈ ag− ent is influenced only by the states
i i
∈A ⊂
of its neighbors as well as its own action, satisfying the following form in discrete time [Mozafari et al., 2012]:
  
(cid:88)
θ i(t+1)=θ i(t)+dtω i(t)+a i(t)+ K ijsin(θ
j
θ i)+ϵ i(t).
−
j∈Ni
(cid:124) (cid:123)(cid:122) (cid:125)
:=θ˙ i(t)
Above,ω denotesthenaturalfrequencyofagenti,dtisthediscretizationtime-step,K denotesthecoupling
i ij
strength between agents i and j, a (t) is the action of agent i at time t, and ϵ (t) N(0,σ2) is a noise term
i i
∼
faced by agent i at time t. We note that this fits into the localized transition considered in network MDPs.
For the reward, we consider frequency synchronization to a fixed target ω . In this case, the local reward
target
(cid:12) (cid:12)
of each agent can be described as r (θ ,a )= (cid:12)θ˙ ω (cid:12).
i Ni i −(cid:12) i
−
target(cid:12)
The parameters in the dynamics and rewards are set as follows. We set the target ω to be 0.2. We
target
set the action space as [ 1,1]. For agents i and j that are connected, we sample K uniformly at random
ij
−
from [0.2,1.2]. For the natural frequency ω ’s, we sample them iid uniformly at random from [ 0.5,0.5]. For
i
−
the noise, we sample ϵ (t) N(0,0.00252). The time resolution is dt=0.01.
i
∼
We also assume w (t) to be drawn iid from N(0,1). We set the discount factor in the problem to be 0.99,
i
and set the horizon length to be 800 steps.
Connectivity. In this problem, there are n=40 agents, and the agents have circular connectivity and
has two neighbors each, such that agent 1 is connected to agents N and agent 2, agent 2 is connected to
agents 1 and 3, so on and so forth.
Experimental details (model-free). In this case, we do not assume access to the dynamics function.
We now explain the simulation setup for our implementation of Algorithm 1 with spectral features, as well as
the benchmark algorithm using a two-hidden layer NN.
1. (Policy evaluation step) For both the spectral feature and NN implementation, the features are the last
layer of a two-hidden layer neural network with hidden dimension 256. At each iteration, for each agent
i, we draw a batch (of 128 transitions) from the replay buffer and we run 1 step of gradient descent on
the least square bellman error, and used a target network with update rate of 0.005.
2. (Policy update step). For each agent i, the policy is parameterized to be a 3-hidden layer NN which
outputs the mean and standard deviation of the agent’s action, and the input is s , i.e. the states of
Ni
the neighborhood of agent i. We update the policy parameters θ n by taking one gradient descent
{ i }i=1
step on the following objective:
(cid:34) (cid:32) n (cid:13)exp((cid:80)n Qˆ (s , n ))(cid:33)(cid:35)
J π(θ)=E s∼D D KL (cid:89) π θi( ·i |s Ni)(cid:13) (cid:13)
(cid:13)
i=1 i Z(N s)iκ+1 {·i }i=1 ,
i=1
where is a set of data from the replay buffer, Z(s) is a normalization constant. Above, we assume the
D
temperature parameter τ to be 1. This objective is identical to the implementation in Soft-Actor-Critic
(SAC) [Haarnoja et al., 2018] but for factored policies, as well as using the learned Qˆ -value functions
i
to approximate the value function.
343. (Feature step). For the spectral features, for each agent i, we add an additional feature step, which
seeks to regularize the features such that they approximate the top left eigenfunctions of the probability
transition P(s′ s ), by taking a gradient descent step on the following objective to update
N iκ | N iκ+1,a Niκ+1
agent i’s feature ϕ(s ,a ):
Nκ+1 Nκ+1
i i
min E [ ϕ(s ,a ) 2] 2E [ω(s′ )⊤ϕ(s ,a )], (11)
ϕ={ϕ1,...,ϕL} d(s,a) ∥ N iκ+1 N iκ+1 ∥ − d(s,a),s′∼P(·|s,a) N iκ N iκ+1 N iκ+1
where in practice we pick d(s,a) to be the a set of samples from the current replay buffer. We note
that (11) can be seen as a randomized way to compute the singular value decomposition (SVD) of the
transition operator P(s′ s ), and we picked it due to its better numerical performance
N iκ | N iκ+1,a Niκ+1
compared to existing spectral decomposition methods in the literature [Ren et al., 2022a], in our
simulation example. We also note that unlike for the model-based case (with random features), we
do not have guarantees on the end-to-end performance of the model-free version of the algorithm.
However, we note that the feature step encourages the features to minimize the objective in (1). We
leave more detailed analysis of this to future work. We give more details on the derivation of (11) in
Appendix 7.8.3.
Experimental details and results (model-based). In this case, we do assume knowledge of the
dynamics function and that the noise is Gaussian, allowing us to use random features as the spectral features.
We focus on discussing the feature generation step, since this is the only difference with the previous
model-free case. For the random features, for each agent i, we select the random features according to the
procedure in Lemma 5 (with feature dimension being 1024), and in the simulations we set α=0. For the NN
implementation, we use a two-hidden layer NN with hidden dimension 256. For the NN implementation, for
a fair comparison, we also give it knowledge of the dynamics function f (s ,a ), such that it can use
i,κ Nκ+1 Nκ+1
i i
this information when computing the local value functions. We note that for the policy evaluation step, for
both random features and NN, we perform gradient descent on the Bellman least square error.
The results of the learning performance are shown below. We see that the spectral-based method displays
comparable performance to the NN-based implementation.
7.8.3 Randomized Spectral Decomposition
We give here a more detailed derivation of the objective in (11). For simplicity, we focus on the single-agent
case, when we are trying to decompose P(s′ s,a) as P(s′ s,a) ϕ(s,a)⊤µ(s′), and in particular trying to
| | ≈
find the ϕ(s,a) in this decomposition. As suggested in [Ren et al., 2022a], this is akin to finding the top left
eigenfunctions of P(s′ s,a). Motivated by randomized SVD for computing the top left singular vectors of
|
finite-dimensional matrices, in the functional space setting, we can perform an analogous randomized SVD to
learn the top left eigenfunctions of P(s′ s,a) according to the following procedure.
|
1. Fix a positive integer L.
2. Foreachi [L],samplearandomfunctionω (s′) R,e.g. ω (s′)=cos(α⊤s′+β ),whereα N(0,I )
∈ i ∈ i i i i ∼ S
and β Unif([0,2π]).
i
∼
3. For each i [L], learn a ϕ (s,a) that approximates Pω (s,a):=(cid:82) P(s′ s,a)ω (s′)ds′ as follows:
∈ i i s′ | i
(a) Pick a sampling distribution d(s,a), e.g. uniform distribution.
(b) For each i [L], solve
∈
(cid:90)
min d(s,a)(ϕ (s,a) Pω (s,a))2
i i
ϕi s,a −
35SAC
52
Spectral-SAC
54
56
58
60
62
64
66
0 2000 4000 6000 8000 10000 12000 14000
Iterations
Figure4: ChangeinrewardduringtrainingforKuramotooscillatorcontrol,n=40,κ =1,κ=2. Inthisexperiment,
π
the dynamics model is known. The performance for each algorithm is averaged over 5 seeds.
(cid:90) (cid:18) (cid:90) (cid:19)2
min d(s,a) ϕ (s,a) P(s′ s,a)ω (s′)
i i
⇐⇒ ϕi s,a − s′ |
(cid:90) (cid:90) (cid:90)
min d(s,a)ϕ (s,a)2 2 d(s,a) ds′P(s′ s,a)ω (s′)ϕ (s,a)
i i i
⇐⇒ ϕi s,a − s,a s′ |
minE [ϕ (s,a)2] 2E [ω (s′)ϕ (s,a)]
d(s,a) i d(s,a),s′∼P(·|s,a) i i
⇐⇒ ϕi −
We note that the final objective is equivalent to solving the L ϕ ’s jointly which is single-agent analogue
i
of the objective in (11):
min E [ ϕ(s,a) 2] 2E [ω(s′)⊤ϕ(s,a)].
d(s,a) d(s,a),s′∼P(·|s,a)
ϕ={ϕ1,...,ϕL} ∥ ∥ −
36
draweR