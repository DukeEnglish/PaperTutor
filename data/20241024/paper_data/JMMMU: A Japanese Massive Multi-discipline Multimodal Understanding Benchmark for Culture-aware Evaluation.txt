JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding
Benchmark for Culture-aware Evaluation
ShotaOnohara♣* AtsuyukiMiyai♣* YukiImajuku♣* KazukiEgashira♣* JeonghunBaek♣*
XiangYue♢ GrahamNeubig♢ KiyoharuAizawa♣
♣TheUniversityofTokyo ♢CarnegieMellonUniversity
Abstract demonstratedremarkableperformanceacrosstasks
rangingfromcommonsensereasoningtodomain-
Accelerating research on Large Multimodal
specific,expert-levelchallenges(Antoletal.,2015;
Models (LMMs) in non-English languages
Liuetal.,2023a,2024c;Yueetal.,2024). Astheir
is crucial for enhancing user experiences
capabilities grow, the need for robust criteria to
acrossbroaderpopulations. Inthispaper,we
introduce JMMMU (Japanese MMMU), the evaluateLMMshasbecomeincreasinglyimportant,
firstlarge-scaleJapanesebenchmarkdesigned highlightingtheroleofcomprehensivebenchmarks
toevaluateLMMsonexpert-leveltasksbased inassessingthefullscopeoftheirabilities.
ontheJapaneseculturalcontext. Tofacilitate However, current benchmarks focus primarily
comprehensive culture-aware evaluation,
onperformanceinEnglish(Liuetal.,2024c;Yue
JMMMUfeaturestwocomplementarysubsets:
et al., 2024; Li et al., 2024b; Liu et al., 2023b;
(i) culture-agnostic (CA) subset, where the
Yuetal.,2024;Fuetal.,2024),withlessempha- culture-independent subjects (e.g., Math) are
sis on evaluation in other languages. Given that
selectedandtranslatedintoJapanese,enabling
one-to-onecomparisonwithitsEnglishcoun- LMMsarewidelyusedacrossdiverselanguages,it
terpartMMMU;and(ii)culture-specific(CS) isimperativetoevaluatetheirperformancebeyond
subset,comprisingnewlycraftedsubjectsthat English. Additionally, such multilingual evalua-
reflectJapaneseculturalcontext. UsingtheCA
tions should actively involve contributions from
subset,weobserveperformancedropinmany
diversecommunities,ensuringthattheassociated
LMMswhenevaluatedinJapanese,whichis
culturalcontextsareappropriatelyconsidered.
purelyattributabletolanguagevariation. Using
Inthispaper,weintroduceJMMMU(Japanese
the CS subset, we reveal their inadequate
Japanese cultural understanding. Further, by MMMU), the first benchmark designed to evalu-
combiningbothsubsets,weidentifythatsome ateLMMsonextensive,multi-disciplinarytasksin
LMMs perform well on the CA subset but Japanesethatrequirecollege-levelsubjectknowl-
not on the CS subset, exposing a shallow edge,deliberatereasoning,andculturalunderstand-
understanding of the Japanese language that
ing. The overview of JMMMU is shown in Fig-
lacks depth in cultural understanding. We
ure 1. JMMMU draws inspiration from the well-
hope this work will not only help advance
establishedMMMU(Yueetal.,2024)andexpands
LMM performance in Japanese but also
existing culture-aware Japanese benchmarks (In-
serve as a guideline to create high-standard,
culturally diverse benchmarks for multilin- oue et al., 2024b; SakanaAI, 2024c) by over 10
gual LMM development. The project page times, with 1,320 questions using 1,118 images,
is https://mmmu-japanese-benchmark. coveringadiverserangeofsubjects.
github.io/JMMMU/.
JMMMU offers two key subsets: (i) Culture-
Agnostic (CA) Subset: We extracted and
1 Introduction translated the culture-agnostic components from
MMMU.Thissubsetallowsforadirectcompari-
In recent years, large language models (LLMs)
sonoftheperformancegapsbetweenEnglishand
haverevolutionizedthefieldoflanguageprocess-
Japanese that are purely attributable to language
ing(Chenetal.,2023a;vic,2023;Touvronetal.,
variations. (ii)Culture-Specific(CS)Subset: We
2023; Wei et al., 2023). Building on the success
carefully crafted brand-new questions that align
ofLLMs,largemultimodalmodels(LMMs)have
with the Japanese cultural context. With CS sub-
*Equalcontribution. set,developerscanassesscapabilitiesspecifically
1
4202
tcO
22
]LC.sc[
1v05271.0142:viXraCulture-Agnostic Subjects Culture-Specific Subjects
(720, 54.5%) (600, 45.5%)
Art & Psych. Science (120, 9.1%) Jpn. Art Jpn. Heritage
(90, 6.8%) Biology Tech & Eng. Jpn. Art (150, 11.3 %) (150, 11.3 %)
Design Chemistry Jpn.
Music Physics Heritage
Psychology Math
Business Medicine Tech & Eng. Medicine
(150, 11.3%) (150, 11.3 %) (210, 15.9%) Jpn.
Agriculture History Jpn. History World History
Arch & Eng. Business (150, 11.3 %) (150, 11.3 %)
Accounting Basic Med. Computer Sci.
Economics Clinical Med. Electronics
Finance Diagnostics Energy & Power Science Art World
Manage Pharmacy Materials & History
Marketing Public Health Mech Eng. Psych.
Figure1: OverviewoftheJMMMUdataset. JMMMUincludes720culture-agnostic(translation-based)questions
and600culture-specific(newlycreated)questions,totaling1,320questions,thusexpandingtheexistingculture-
awareJapanesebenchmark(Inoueetal.,2024b)byover10times. JMMMUservesasadiagnostictoolforassessing
bothJapaneseculturalunderstandingandculture-agnosticlanguageunderstandingcapability.
tailored to Japanese culture. Together, JMMMU estimationofanLMM’smultilingualcapabil-
serves as a diagnostic tool for model developers, itywithouttrulyunderstandingthecontextof
providing valuable feedback for future improve- theindividualcultures.
ments.
Evaluating15open-sourceLMMsandthreead- OurfindingsindicatethatEnglish-centeredper-
vanced proprietary LMMs on JMMMU, our key formance evaluation may lead to biased develop-
findingsaresummarizedasfollows: ment,neglectingnon-Englishlanguages. Wehope
our findings not only spark interest in Japanese
• Overallperformanceisupto58.6%,leaving performance but also motivate the community to
greatroomforimprovementintheutilityof craftavarietyofhigh-standardbenchmarksthaten-
theJapanesecontext. compassdiverseculturesandtheirassociatedlan-
guages, thereby promoting more inclusive LMM
• TheCAsubsetrevealsthatmostmodelsper-
development.
form worse when asked in Japanese than in
English(upto8.6%),evenwhenthequestion 2 RelatedWork
asks exactly the same content. This apple-
to-applecomparisonclearlyindicatesthatthe LargeMultimodalModels(LMMs) Following
utilityinnon-Englishlanguagesisfallingbe- the success of large language models (LLMs),
hindincurrentLMMs. many LMMs have been developed with im-
provedknowledgeandinstruction-followingcapa-
• The CS subset reveals that models trained bilities(Liuetal.,2023b,2024a,b;Lietal.,2024a;
onJapanesedatasetsperformthebestamong Ye et al., 2024; Zhao et al., 2023; Li et al., 2023;
open-sourcemodels,suggestingthatsuchfine- Monajatipooretal.,2023;Zhaoetal.,2024). How-
tuningeffectivelycontributestoincorporating ever,theprogressofthesemodelsistypicallyeval-
Japaneseculturalknowledgeintothemodels. uatedonEnglishbenchmarks(Yueetal.,2024;Liu
et al., 2024c). Therefore, a significant challenge
• Combining both subsets, we reveal a signif-
remains in accurately evaluating the capabilities
icantdiscrepancyamongthestate-of-the-art
ofotherlanguages,highlightingtheneedfornon-
proprietarymodels. Whiletheyperformsim-
Englishbenchmarks.
ilarly on English benchmarks and even on
culture-agnosticquestionsinJapanese,their LMM Benchmarks Among various recent
performancesaresignificantlydifferentonCS benchmarks (Li et al., 2024b; Liu et al., 2023b,
subset. Thisfindingisparticularlyalarming, 2024c; Lu et al., 2024; Yue et al., 2024; Miyai
asitindicatesthatevaluationexclusivelyona et al., 2024), MMMU (Yue et al., 2024) is the
translation-basedbenchmarkcouldriskover- mostwidelyusedtomeasuretheadvancementsof
2cutting-edge LMMs. MMMU requires advanced Table1: OverviewofJapaneseLMMbenchmarks.
JMMMUisthefirstbenchmarkthatevaluatesexpert-
university-level knowledge and reasoning across
levelskillsandisthelargestamongculture-awarebench-
abroaderrangeofsubjects,enablingamorecom-
marks.
prehensive and expert-level evaluation. Subse-
quently, CMMMU (Zhang et al., 2024) has been Benchmark Culture Level QuestionsImages
JA-VG-VQA-500(SakanaAI,2024b) ✗ Commonsense 500 500
proposed as its Chinese counterpart. While CM- LLaVA-Bench-in-the-wild(Turing,2024b) ✗ Commonsense 60 24
MMUcomprisesentirelynewculture-specificques- JA-Multi-Image-VQA(SakanaAI,2024a) ✓ Commonsense 55 39
JA-VLM-Bench-in-the-wild(SakanaAI,2024c) ✓ Commonsense 50 42
tions, our JMMMU has not only culture-specific HeronBench(Inoueetal.,2024b) ✓ Commonsense 102 21
JMMMU(Ours) ✓ Expert 1,320 1,118
subjectsbutalsotranslation-basedculture-agnostic
subjects, facilitating one-to-one comparisons be-
tweenEnglishandJapaneseusingtheexactsame 28 different subjects. This benchmark is strategi-
questions. In line with multilingual ability eval- callydividedintotwodistinctcategories: culture-
uation, several VQA benchmarks have been pro- agnosticandculture-specificsubjects.
posed(Gaoetal.,2015;Changpinyoetal.,2022;
Culture-agnosticsubsetconsistsof24subjects
Guptaetal.,2020;Liuetal.,2021;Pfeifferetal.,
with720questionsacrossfivedisciplines: (1)Art&
2021;Tangetal.,2024;Romeroetal.,2024). How-
Psychology,(2)Business,(3)Health&Medicine,
ever,unliketheMMMUseries,theirprimaryfocus
(4)Science,and(5)Tech&Engineering. Culture-
is on daily knowledge, (e.g., Pop Culture, Sports
specificsubsetconsistsof600questionsacrossfour
inCVQA(Romeroetal.,2024)),stillleavingthe
subjects: (1)JapaneseArt,(2)JapaneseHeritage,
multilingualexpert-levelreasoningskillsasanim-
(3) Japanese History, and (4) World History. We
portantdirectionforfuturework.
providesamplequestionsinAppendixE
JapaneseLMMBenchmarks Thedevelopment
3.2 DataCurationProcess
ofJapaneseLMMbenchmarksremainsbehindthat
of English benchmarks. While efforts have been JMMMUisderivedfromthewidely-usedvalida-
made to create Japanese benchmarks as shown tion set of MMMU, consisting of 900 questions
in Table 1, they still exhibit the following criti- across30subjects. ToconstructJMMMU,wefirst
callimitations: (i)Existingbenchmarks(Shimizu examinedtheculturaldependenciesintheoriginal
etal.,2018;Turing,2024c,b;Inoueetal.,2024b; MMMU subjects. For culture-agnostic subjects,
SakanaAI, 2024c,a) focus primarily on common wetranslatedthequestionsintoJapanese. Wefur-
sense knowledge but do not adequately address therreplacedculture-dependentsubjectswithnew
expert-levelknowledge,despitetheadvancement subjects that are conceptually similar, but better
inLMMsandtheimportanceofevaluatingsuchca- alignedwiththeJapanesecontext. Alltheprocess
pabilities. (ii)Manydonotaccountforculturaldif- hasbeenconductedwiththehelpof19university
ferences. Theyareoftencreatedbydirectlytrans- students, including the authors, who have expert
latingexistingEnglishbenchmarks(Shimizuetal., knowledge in the respective fields and native flu-
2018;Turing,2024c,b),resultinginquestionsthat ency in Japanese. Here, we describe the dataset
mayfeelunfamiliartoJapanesepeopleduetocul- creationprocessindetail.
turalcontext. (iii)Althoughrecentbenchmarksat-
tempttoconsiderculturaldifferences(Inoueetal., ExaminingCulturalDependenciesinMMMU
2024b;SakanaAI,2024c,a),theyarelimitedinsize Amongthe30subjectsinMMMU,weidentified
(up to 102 questions), raising concerns about the that questions in six subjects are particularly un-
reliabilityofquantitativeevaluation. Ourproposed familiar to Japanese people and thus we catego-
JMMMUaddressesallthreeoftheaforementioned rized them as culture-specific subjects; Art, Art
challenges,significantlyadvancingthebenchmark Theory, Geography, History, Literature, and So-
intherealmofJapaneseevaluation. ciology. The remaining subjects (e.g., Biology,
Chemistry, Computer Science, Electronics) exist
3 JMMMUBenchmark
inJapanwithsimilarcontents,andthuswecatego-
rizedthemasculture-agnosticsubjects. Asaresult,
3.1 OverviewofJMMMU
weexcludedthesixculture-specificsubjectswhile
AsillustratedinFigure1,JMMMUcontainsato- keepingtheremaining24culture-agnosticsubjects
talof1,320questionsand1,118images,covering inJMMMU.
3callycoveredinJapanesetextbookstobetter
reflecttheJapaneseeducationalcontextthan
HistoryinMMMU.
TheimagesareprimarilysourcedfromWikime-
Translation dia Commons1, ensuring that all selected images
areavailableunderlicensessuitableforpublicre-
lease. Incraftingquestions,weaimedtokeepthe
text as simple as possible and ensure that no op-
tionsstandout,makingithardtoguessthecorrect
choicewithoutreferringtotheimage.
Figure2: Exampleoftheimagetranslationprocess.
3.3 ComparisonwithOtherJapanese
Englishwordsintheimagearemanuallyoverwritten
MultimodalBenchmarks
withJapanese.
Here, we compare JMMMU with other Japanese
multimodal benchmarks, provided in Table 1,
TranslatingCulture-AgnosticSubjects Theex-
to demonstrate its uniqueness. First and fore-
pertswereprovidedwiththeoriginalEnglishtexts,
most, JMMMU is the only benchmark that in-
GPT-4o-translatedquestiontextsinJapanese,and
cludesexpert-levelquestions,whiletherestofthe
corresponding images. For texts, their task in-
benchmarks(Shimizuetal.,2018;Turing,2024a;
volved: (i) refining the auto-translated Japanese
SakanaAI,2024a,c;Inoueetal.,2024b)arefocused
texttoensurenaturalnessandfluency;(ii)confirm-
oncommonknowledge. Further,JMMMUiscare-
ingthattechnicaltermsandacademicexpressions
fullydesignedtotaketheJapaneseculturalcontext
adheretoconventionalJapaneseusage;and(iii)ad-
intoaccount. Whilesomeexistingbenchmarkscon-
justingthecurrencytoreflecttypicaldigitlengths
siderJapaneseculture,theyarealllimitedinsize
in Japanese yen (¥). For currency conversion, a
(onlyupto102questionsinInoueetal.(2024b)),
simplifiedconversion($1→¥100)wasemployed
raising concerns about whether reliable quantita-
toavoidmakingthecalculationunnecessarilycom-
tiveevaluationscanbeconducted. Incontrast,JM-
plicated. Forimages,weaskedtheexpertstoover-
MMUcontainsmorethan10timeslargerthanany
writetheEnglishtextwithJapanesetextbyusing
oftheexistingculture-awarebenchmarks.
an image editing tool. An example of the image
translationprocessispresentedinFigure2. 4 Experiments
Consequently, we obtained 720 questions cov-
4.1 Setup
ering24culture-agnosticsubjectsfullytranslated
andadaptedforJapaneseusage. LMMs WeevaluateadiversesetofLMMs.
CreatingCulture-SpecificSubjects Recogniz-
• ProprietaryLMMs: GPT-4o(OpenAI,2024)
ingthatmostoftheremovedsubjectsarerelatedto
Gemini1.5Pro(DeepMind,2024;Reidetal.,
artorsocialstudies,wecreatedthefollowingnew
2024) and Claude 3.5 Sonnet (Anthropic,
subjectstotestsimilarknowledgeintheJapanese
2024).
context:
• Japanese LMMs: LLaVA CALM2 (Ina-
gaki,2024)andEvoVLMJPv2(Inoueetal.,
• Japanese Art: Questions about traditional
2024a),whicharetrainedonbothEnglishand
Japaneseart,suchasUkiyo-eandNoh.
Japanesedatasets.
• Japanese Heritage: Questions about tradi- • Open-source LMMs: LLaVA-OneVision
tional, culturally significant locations and 0.5B&7B(Lietal.,2024a),LLaVA1.6-13B
buildings in Japan such as temples and & 34B (Liu et al., 2024b), Phi-3 & 3.5 Vi-
shrines. sion (Abdin et al., 2024), InternVL2-2B &
• JapaneseHistory: Questionsabouthistorical 8B (Chen et al., 2023b), xGen-MM (Xue
incidentsinJapan. et al., 2024), Idefics2-8B (Laurençon et al.,
2024b),Idefics3-8B(Laurençonetal.,2024a),
• WorldHistory: Questionsaboutglobalhistor-
ical incidents, but based on the content typi- 1https://commons.wikimedia.org/
4Table2: Overallresults. CA(EN)showstheresultoncultureagnosticsubsetinEnglish. Therestoftheresults
areaverageandindividualsubjects’scoresonJMMMU.†denotesJapaneseLMMs. Thebest-performingmodel
amongopensourceandproprietarymodelsareinbold. Overall,theperformanceisupto40.5%foropen-source,
and58.6%forproprietarymodels,leavinggreatroomforimprovement.
CA Jpn. Jpn. Jpn. World Art& Health&Tech&
Models Overall CS CA BusinessScience
(EN) Art HeritageHistoryHistoryPsych. Medicine Eng.
(1,320) (600)(720)(720)(150) (150) (150) (150) (90) (150) (120) (150) (210)
Random 24.8 25.0 24.6 24.6 25.0 25.0 25.0 25.0 25.4 25.0 22.8 25.6 24.3
OpenSource
LLaVA-OV-0.5B 26.0 23.3 28.2 29.4 22.7 22.7 24.0 24.0 26.7 27.3 24.2 30.7 30.0
InternVL2-2B 28.3 29.2 27.6 31.9 31.3 22.7 30.7 32.0 30.0 30.0 30.8 25.3 24.8
xGen-MM 28.6 28.2 28.9 35.7 30.0 20.7 22.7 39.3 32.2 21.3 22.5 36.7 31.0
Phi-3v 29.5 26.5 31.9 37.6 31.3 18.7 29.3 26.7 26.7 28.7 25.8 37.3 36.2
LLaVA-1.6-13B 31.1 33.7 29.0 29.9 32.0 24.0 32.0 46.7 25.6 28.7 30.0 34.0 26.7
Idefics2-8B 31.9 37.0 27.6 35.1 40.7 24.0 30.0 53.3 32.2 22.7 22.5 32.0 29.0
Phi-3.5v 32.4 34.3 30.8 39.2 37.3 27.3 35.3 37.3 27.8 31.3 30.0 36.7 28.1
†LLaVACALM2 34.9 41.5 29.4 29.9 42.7 36.7 40.0 46.7 27.8 26.0 26.7 34.0 31.0
Mantis8B 35.5 39.5 32.2 36.0 42.0 30.0 35.3 50.7 37.8 28.0 31.7 37.3 29.5
CogVLM2-19B 36.1 39.7 33.1 36.8 39.3 24.0 36.0 59.3 28.9 32.7 30.8 30.0 38.6
Idefics3-8B 37.3 42.8 32.8 36.9 43.3 24.7 42.0 61.3 34.4 28.0 26.7 38.0 35.2
†EvoVLMJPv2 38.1 45.2 32.2 33.9 44.0 40.0 42.0 54.7 32.2 28.7 28.3 38.7 32.4
InternVL2-8B 38.3 42.5 34.7 43.3 41.3 38.0 35.3 55.3 40.0 36.0 34.2 34.0 32.4
LLaVA-1.6-34B 39.8 43.2 37.1 45.7 42.0 36.0 40.7 54.0 42.2 41.3 25.0 36.7 39.0
LLaVA-OV-7B 40.5 43.0 38.5 45.1 36.0 30.7 37.3 68.0 41.1 36.7 31.7 38.7 42.4
Proprietary
Claude3.5Sonnet 50.8 51.0 50.6 52.1 39.3 46.7 54.7 63.3 53.3 56.7 51.7 55.3 41.0
Gemini1.5Pro 51.5 60.3 44.2 51.1 54.7 55.3 55.3 76.0 51.1 44.0 44.2 48.0 38.6
GPT-4o 58.6 66.7 51.8 52.1 60.7 70.7 58.7 76.7 53.3 55.3 45.8 61.3 45.2
TextOnly
GPT-4otext 38.1 35.5 40.3 44.9 32.7 32.0 35.3 42.0 38.9 36.0 41.7 45.3 39.5
CogVLM2-19B (Hong et al., 2024), and notrespondinaparsableformat,arandomchoice
Mantis-8B(Jiangetal.,2024). isassignedasitsanswer.
In Appendix A, we provide further details of 4.2 MainResult
thesemodels,withaparticularfocusonJapanese
Table2demonstratestheevaluationresultsonour
languagesupport.
JMMMU benchmark. We provide the average
Text-onlyLLM Asareference,wepresentthe scores across all subjects, culture-agnostic (CA)
accuracyofGPT-4owhenprovidedonlywiththe subjects,andculture-specific(CS)subjects,aswell
questiontextandchoices,withoutimages. asscoresonindividualsubjects. Forcomparison,
wealsoprovidetheperformanceonCAsusetinEn-
Evaluation The evaluation method is based on
glishCA(EN).NotethatCA(EN)isoftensmaller
the setup in MMMU (Yue et al., 2024). Prompts
thantheoverallaverageofMMMUgivenbyYue
aretranslatedasfollows: formultiple-choiceques-
etal.(2024)becausesubjectsselectedasCAarerel-
tions, 与えられた選択肢の中から最も適切な
ativelydifficultamongallsubjectsinMMMUasit
回答のアルファベットを直接記入してくだ
oftenrequiresstrongerreasoningcapabilities(e.g.,
さい。 (Answer with the option’s letter from the
Math).
givenchoicesdirectly.) ;andforopen-endedques-
Here,wesummarizeourkeyobservations.
tions, 質問に対する回答を単語や短いフレー
ズで記入してください。 (Answerthequestion ChallengingNature Inourexperiment,theper-
usingasinglewordorphrase.). formance is up to 40.5% for open-source, and
FollowingMMMU,(i)wepreparearule-based 58.6%forproprietarymodels,leavinggreatroom
parser to extract the model’s choice from typical for improvement. This also highlights a signifi-
generationstylessuchas“答えはA”(Theanswer cantgapbetweenopen-sourceandproprietarymod-
is A), making the evaluation robust to some vari- els,presentingamoredifficultchallengeforopen-
etiesofanswerstyles,and(ii)whenamodeldoes sourcemodels.
570 Table3: Theeffectoftranslation. Eachcolumnshows
LLaVA-OV 0.5B the model performance when image (I) and text (T)
InternVL2 2B
xGen-MM arein Japanese(jp) orin English(en). ∆ shows the
60 Phi-3 Vision i
LLaVA-NeXT 13B differencefromI enT en.
Idefics2 8B
Phi-3.5 Vision Model I T I T (∆ ) I T (∆ )
50 Mantis-8B en en en jp 1 jp jp 2
CogVLM2 19B
Idefics3 8B LLaVA-1.6-13B 26.4 31.9(+5.5) 29.2(+2.8)
InternVL2 8B
40 LLaVA-NeXT 34B Phi-3.5v 39.2 33.6(-5.6) 31.1(-8.1)
LLaVA-OV 7B LLaVA-CALM2 29.4 28.3(-1.1) 31.4(+2.0)
LLaVA CALM2
30 EvoVLM-JP v2 CogVLM2-19B 32.8 31.9(-0.9) 34.4(+1.6)
Claude 3.5 Sonnet
Gemini 1.5 Pro EvoVLMJPv2 30.0 30.8(+0.8) 28.6(-1.4)
GPT-4o
20 InternVL2-8B 43.9 38.3(-5.6) 37.2(-6.7)
20 30 40 50 60
LLaVA-1.6-34B 43.6 40.8(-2.8) 38.9(-4.7)
Culture Agnostic [%]
LLaVA-OV-7B 45.0 38.3(-6.7) 35.6(-9.4)
Figure3: Scorecorrelationbetweensubsets. While
proprietarymodels(■)performthebestonbothsubsets,
JapaneseLMMs(⋆)performremarkablyhighonCS whosegapfromGPT-4ois≤16.7%,indicatingthe
subset compared to models that perform similarly on particularinadequacyoftheopen-sourcemodelin
CAsubset. Heritagedomain.
GPT-4ovs. Claude3.5Sonnet Werevealasig-
TheEffectofTranslationinCASubset First, nificantperformancegapbetweenthetwoleading
as a general trend, the score on the CA subset is models; GPT-4o and Claude 3.5. They are state-
significantlylowerthanitsEnglishcounterpart(CA of-the-artmodelsandtheirperformanceisknown
(EN)inTable2)withanaveragedropof4.5%. This to be similar with only 0.8% difference on the
indicatesthat,evenforthesamequestions,many MMMUbenchmarkinEnglish(Anthropic,2024).
models perform worse when asked in Japanese. Further, their performance is similar even on CA
Second, despite such a general trend, Japanese- splitinJapanese(1.2%differenceinTable2). How-
madeLMMs(i.e.,LLaVACALM2andEvoVLM ever,ontheCSsplit,GPT-4ooutperformsClaude
JP v2) face a minimal drop (up to 1.7 %), which 3.5Sonnetbyasubstantial15.7%.
impliesthatincorporatingtheJapanesedatasetsuc- Thisstronglyindicatesthatamodel’sJapanese
cessfully mitigates the performance gap between language skill and its understanding of Japanese
EnglishandJapanese. culture should be separately discussed. Our re-
searchispioneeringinrevealingthis,adiscrepancy
ThePerformanceofJapaneseLMMs Figure3
thatwouldhaveremainedobscuredwithoutcom-
demonstrates the correlation between the scores
bining translation-based CA subjects and brand-
ontheCAandCSsubjects. TheJapaneseLMMs,
newCSsubjects. Ourfindingunderscoresthelim-
LLaVACALM2andEvoVLMJPv2,showhigher
itations of relying exclusively on auto-translated
scores on CS subjects compared to other mod-
benchmarks for a thorough evaluation of model
els that perform similarly on CA subjects. This
capabilitiesinnon-Englishlanguages,highlighting
stronglyindicatestheirproficiencyinCSsubjects.
the importance of evaluating models on culture-
Ontheotherhand,however,comparedtostronger
specificquestions.
modelssuchasInternVL2-8b,LLaVA1.6-34b,and
LLaVA-OV-7b, the Japanese LMMs show lower
5 Analysis
scores on CA subjects, suggesting room for im-
provementintheirgeneralreasoningandproblem-
5.1 AblationonImageTranslation
solvingcapabilitiesinculture-agnosticcontext.
Here, we investigate how translating text and im-
Scores on Japanese Heritage Among CS sub- ages affects the model performances. Using 360
jects, the performance of open-source models questionsfromtheculture-agnosticsubsetwhich
is particularly low in Japanese Heritage (Ta- involvedtranslationofbothtextsandimages,we
ble 2). Even the best-performing open-source comparethescoresinEnglish(I T ),whenonly
en en
model(EvoVLMJPv2)scores30.7%lowerthan textistranslated(I T ),andwhenbothtextand
en jp
GPT-4o in Japanese Heritage, while in other CS imagesaretranslated(I T ). Weprovidescores
jp jp
subjects, there is at least one open-source model forselectedmodelsinTable3andthefullsetinAp-
6
]%[
cificepS
erutluCTextual Misunderstanding
4.8%
EN: , JP:
256 258
(35.6%) (35.8%) EN: , JP: Answer
EN: , JP: Rejection
108 98 EN: , JP: 10.6%
(15.0%)(13.6%)
(a) GPT-4o’sErrordistributioninculture-
agnosticsubjects. ImageRecognition Lack of Knowledge
Errors 53.8%
30.8%
Question: In <image 1>. 𝑣#=sin2𝜋𝑡. Find an expression for 𝑖and
calculate 𝑖at the instants 𝑡=0.
(<image 1>において𝑣#=𝑠𝑖𝑛2𝜋𝑡である。𝑖の式を求め、
𝑡=0における𝑖を計算しなさい。)
Option:
A.3𝜋×10!" A
B.𝜋×10!" A
C.2𝜋×10!" A Figure5:Errordistributionoverculture-specificsub-
D.𝜋×10!$ A Ground Truth: C.2𝜋×10!" A
jects. LackofKnowledgeisthemajorityerrortypeat
GPT-4o’s Answer GPT-4o’s Answer in Japanese:
in English: コンデンサの電流𝑖は、コンデンサの電圧𝑣!の時間微分にキャパシタン over50%.
ス𝐶を掛けたものです。すなわち、(The current 𝑖of a capacitor is the
B.𝜋×10!" A c T 与 " し"a #h $p !え たaa =t c ら がii s "t ",a れ っ) $n (𝑖c sた て=e in 𝑣 、𝐶 𝐶 2! 𝜋 ("m =" T𝑡# $u )! hs l u=t ii n sp ,)22l i 𝑖𝜋𝜋ed =𝑡 c を ob 𝐶y s微 ( 3t 2h 2分 𝜋e
𝜋
𝑡t し )i cm oま se d す 2e 𝜋r 。i 𝑡v (a Gti iv ve e o nf 𝑣t !he = ca sp inac 2i 𝜋to 𝑡r ,’s w v eo dlt ia ffg ee r e𝑣 n! t. i ate:)
ここで、𝐶=10%&Fですので、(Here, 𝐶=10%&F, so)𝑖=10%&3 guagesonculture-agnosticsplit(only0.3%differ-
2𝜋cos2𝜋𝑡 𝑡=0のとき、(At 𝑡=0,) 𝑖=10%&32𝜋cos0
cos(0)=1なので、(Since cos(0)=1,) 𝑖=10%&32𝜋31=2𝜋310%&A
したがって正しい選択肢はCです。(Therefore, the correct choice is C.) ence in Table 2), we have found that there are a
(b) AnExamplequestionwhereGPT-4oanswers significantamount(28.6%)ofquestionstowhich
correctlyonlyinJapanese. itansweredcorrectlyonlyineitheroneofthelan-
Figure4: (a)Thereareaconsiderableamountofques- guages. Wenowinvestigatethisphenomenon. For
tionstowhichGPT-4oanswerscorrectlyonlyineither questionsansweredcorrectlyonlyinEnglish(or-
oneofthelanguages(yellow+orange). (b)InJapanese, ange in Figure 4(a)), we observe simple perfor-
the model relatively more often goes against the in-
mance degradation after translation. In contrast,
structionthataskstoanswerdirectlyandgeneratesits
wehavefoundsomedistinctiveexamplesintheop-
reasoningprocess,leadingtoacorrectanswer.
positecase(yellow). InanexampleofFigure4(b),
GPT-4ooutputsonlythedirectanswerinEnglish,
pendix B.2. Many models experience a drop in whereasinJapanese,themodelincludesthereason-
scores by text translation, with further degrada- ingprocessinitsresponsealthoughthemodelisin-
tionobservedwhenimagesarealsotranslated(i.e., structedtogeneratethechoicedirectlybyusingthe
0 > ∆ > ∆ ). However,somemodelsexhibitdif- promptinSection4.1. Forafaircomparisonwith
1 2
ferentperformancetrends,showingadropbytext MMMU(Yueetal.,2024),wecountaresponseto
translationbutanimprovementbytranslatingboth be correct as far as the model’s response is accu-
(i.e.,∆ < 0 < ∆ ),orviceversa. Overall,while rateandcanbeparsedbyarule-basedalgorithm,
1 2
thetrendsarecomplex,ourresultindicatesthattext- regardless of its instruction-following ability. As
only translation, as is done in many non-English aresult,thescorescansometimesbecounterintu-
benchmarks,couldresultinabiasedperformance itivelyoverestimatedduetothelackofinstruction
evaluation. Rigorousinvestigationonthispointis followingskillsinJapanese. Whiletheprimaryfo-
leftforfuturework. cusofJMMMUisonevaluatingexpertknowledge
andsupportingtheimprovementofsuchcapabili-
5.2 ErrorsinCulture-agnosticSubjects ties, our findings highlight a crucial direction for
futurework: measuringandenhancinginstruction-
JMMMU shares 720 culture-agnostic questions
followingabilityinnon-Englishlanguages.
withMMMU,whichallowsustocomparetheout-
putonebyone. Usingthesequestions,weevaluate
5.3 ErrorsinCulture-specificSubjects
howtranslationaffectsmodelperformance. Taking
GPT-4oasanexample,weclassifytheresponses This section presents an analysis of the tendency
intofourcategoriesbasedonwhethertheyarecor- ofGPT-4o’serrorsintheculture-specificsubjects.
rectorincorrectineachlanguage. Figure4presents Toinvestigatethecausesoftheseerrors,weman-
theresultsbeforeandaftertranslation. Theresults uallyreviewGPT-4o’sresponsesandclassifythe
ontheothermodelsareprovidedinAppendixB.1 errorsintofourcategories: (i)LackofKnowledge,
While GPT-4o performs similarly in both lan- wherethemodelsuccessfullyextractsthenecessary
7Lack of Knowledge Image Recognition Error AnswerRejection Textual Misunderstanding
Question: <image 1>には棟飾が取り付けられていま Question: <image 1>で⾒られることで有名な動物は Question: <image 1>の⼈物が研究していた Question: <image 1>の作者の代表作ではないものを
すが、これを制作したとされる⼈物は誰ですか？ 何でしょう？ 病気として代表的なものは何でしょう？ 選びなさい
(An ornamental ridgepole is attached to <image 1>; (What animal is famous for being seen in the image?) (What is a representative disease that the person (Select the work that is not a representative work
who is the person said to have created it?） Option: in <image 1> was researching? ) by the author of <image 1>)
Option: A. ヤンバルクイナ(Yanbaru rail) Option: Option:
A. ⼩橋川永昌(EishoKobashigawa) B. イリオモテヤマネコ(Iriomotecat) A.⻩熱病(Yellow fever) A. 乞⾷と⼥(The Beggar and the Woman)
B. 平⽥典通(NorimichiHirata) C. イヌワシ(Golden eagle) B.結核(Tuberculosis) B. ⼤島の⽔汲み⼥(Woman Fetching Water in Oshima)
C. 河井寛次(Kanji Kawai) D. トキ(Japanese crested ibis) C.⾚痢(Dysentery) C. 尿する裸僧(Nude Monk Urinating)
D. ⾦城次郎(Jiro Kinjo) GPT-4o’s Answer: この画像は⽇本の⽯垣島を D.コレラ(Cholera) D. 髪梳ける⼥(Woman Combing Her Hair)
GPT-4o’s Answer:この画像は⾸⾥城の正殿を⽰して ⽰しています。⽯垣島で⾒られることで有名な GPT-4o’s Answer:この画像の⼈物が誰かを特定すること GPT-4o’s Answer: この画像は「尿する裸僧」という
います。⾸⾥城の棟飾（⿓頭）は、⾦城次郎（きん 動物は「イリオモテヤマネコ」です。したがって、 はできませんが、選択肢の中で⻩熱病、結核、⾚痢、 作品です。したがって、選択肢の中でこの作者の
じょうじろう）によって制作されたとされていま 正解はB. イリオモテヤマネコです。 コレラのいずれかを研究していた可能性があります。 代表作ではないものは「C. 尿する裸僧」です。
す ( T s TT ah hih。 de ei r s r t eし oo fi om ohた f raa t eo vg ,が p ee th っ o bd er ee n cepて a onic m r 、 c rts ee r a c正 nt fh tt t e ae解 ( d ndm r s は ba wa g yiD en o J r n. ih ir ⾦ sa h o l e Dl K城 a o .d i f Jn次 ) i jS ro o o郎 h f . u KSで r hi i nuCす jr oa i .s。 C )tl ae s. tle is ( f CaT amh t’i o .s u Ti sm h f ea o rg r e e fb os e rh i eno ,g w t hss ee I e cs nh o i rog rn Gea ck I rtsi o h aI ui ns gl nsa a w dn k ed i T r I ri s in usl a tJ B hna d .p : Ia Di rn s i . . o t トhT meh o キ e ‘ tI ea rin Coi amm t.oa )tl e 具 特 ( h d kI a y n a v s o体 定 m ee wn 的 す r ntu e e ,n s な る r iea y ta b ,⼈ こ w rl oce orh物 と ut ceo lhd名 が d i o d o blが で e n e en re paわ き t i o . of f I syか ま f st t h itれ す h bhee le ef ば 。 p o s te pl o、 lr e os dcwoそ ein f ti i en の ci rgn mn: ⼈ t a iyh nmei物 es l e l ti o の hom w efa 研 i t rfg h ee f究 ev i, e e pb l分 r e du , r t t s野 o ut o fh b nを re e ey wr sc em eu arla e ro cy s h is .), ( T reT h ph e ri r es e s fi em o nr ta e ag , t e ia v mi es o wt nh oge r kta hr betw G yo o rp thr otk i i uo s t n ni at ds ul ,e t Thtd h ro e“ ur N to i hsu n :d e “ DCe t h .M . a 尿 髪to in す 梳sk n る けU otr 裸 るi an 僧a ⼥ti .n )g.”
Ground Truth: B.平⽥典通 E Gr Pr To -r 4 oR e mas iso tn ak: es SadoIsland Ground Truth: A. ⻩熱病 E Gr Pr To -r 4 oR ce oa rs ro en c: t ly provides
Error Reason: shown in <image 1> for Ishigaki Error Reason: the title of <image 1>, but it fails to
GPT-4o correctly answers Island which is another island The person in the image is a famous understand the question. Although
the name of the building in in Japan. historical figure in Japan, the question asked to select
<image 1>, but it (However, GPT-4o correctly Hideyo Noguchi,but GPT-4o says it a non-representative work,
hallucinates when providing answers the animal, is unable to identify the person and it answers with a representative
more detailed information ‘IriomoteCat’, which is found refused to provide an answer. work, indicating a lack of proper
about it. on Ishigaki Island.) understanding of the Japanese text.
(a) LackofKnowledge (b) ImageRecognitionErrors (c) AnswerRejection (d) TextualMisunderstanding
Figure6: Examplesfromeacherrortype: (a)LackofKnowledge,wherethemodeldoesnotknowthenecessary
information;(b)ImageRecognitionErrors,wherethemodelfailstocorrectlyinterprettheimage;(c)Answer
Rejection, where the model rejects to answer; and (d) Textual Misunderstanding, where the response is not
alignedwiththequestion.
informationfromtheimagebutlackstheculture- uresfromimages. InFigure6(c),GPT-4oresponds
specific knowledge required to produce a correct that it is unable to identify the person in the im-
answer,(ii)ImageRecognitionErrors,whereit age(HideyoNoguchi),resultinginafailuretose-
failstocorrectlyinterprettheimageduringthevi- lect the option associated with him. We hypothe-
sualunderstandingstage,(iii)AnswerRejection, size this is due to their strong privacy awareness
where it declines to provide an answer, and (iv) to avoid giving private information (Wang et al.,
Textual Misunderstanding, where the response 2024),evenwhenthequestionasksforinformation
is not aligned with the question. The overall dis- thatiswidelyknownaboutahistoricalfigure.
tributionoftheseerrortypesisshowninFigure5.
Textual Misunderstanding (4.8%) There are
LackofKnowledgeistheoverwhelmingmajority
rareinstanceswhereGPT-4oprovidesanincorrect
atover50%,indicatingthatculture-specificknowl-
responsedespitecorrectlyidentifyingthecontent
edge is the most critical requirement to achieve
oftheimage. Forexample,inFigure6(d),GPT-4o
highperformanceinJMMMU.Inthissection,we
accurately names the title of the artwork, but its
discussnotableexamplesforeacherrorcategory.
answerdoesnotcorrespondtothequestion.
LackofKnowledge(53.8%) Figure6(a)shows
6 Conclusion
anexampleofanerrorinJapaneseHeritage. Here,
GPT-4o correctly recognizes Shuri Castle in the
We propose JMMMU, a benchmark designed to
image but fails to provide the related contextual
comprehensivelyevaluatetheexpert-levelknowl-
knowledge. Similar cases have been observed in
edge, reasoning abilities, and understanding of
JapaneseArt,whereGPT-4ocorrectlyanswersthe
Japanese culture. The evaluation results suggest
nameoftheartworkbutisunabletospecifytheera
crucialdirectionsfordevelopingmodelswithhigh-
inwhichitwascreated.
level reasoning skills grounded in cultural under-
standing. We have also revealed the importance
ImageRecognitionErrors(30.8%) Figure6(b)
ofevaluatingmodelsonculture-specificquestions
showsanexampleofanimagerecognitionerrorof
by showing that some models perform well in
a question. Here, GPT-4o mistakes the image of
culture-agnosticquestionsinJapanese,butnotin
SadoIslandforIshigakiIsland,anditanswersthe
culture-specificquestions. Wehopethisworkwill
famousanimalinIshigaki(correctlyiftheimage
serve as an important step towards a comprehen-
wasindeedIshigaki).
sivemultilingualevaluation,motivatecommunities
AnswerRejection(10.6%) Thistypeoferroris inotherculturesandlanguagestocrafttheirown
particularlyevidentinJapaneseHistoryandWorld high-standard benchmarks, and lead to LMM de-
History, where GPT-4o declines to answer ques- velopmentsthataremoreinclusiveandtrulyuseful
tions requiring the identification of historical fig- indiversepopulation.
8Limitations Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed
Awadallah, et al. 2024. Phi-3 technical report: A
Throughout our experiment and extensive analy- highlycapablelanguagemodellocallyonyourphone.
sis,wehaveshownanumberofcriticaldirections arXivpreprintarXiv:2404.14219.
of improvement in multilingual benchmarks and
TakuyaAkiba,MakotoShing,YujinTang,QiSun,and
model developments. While they are outside of
DavidHa.2024. Evolutionaryoptimizationofmodel
thescopeofthispaper, theyareleftasimportant mergingrecipes. arXivpreprintarXiv:2403.13187.
directionsforfuturework,andthuswesummarize
Anthropic. 2024. Claude 3.5 sonnet. https://www.
themhere:
anthropic.com/news/claude-3-5-sonnet.
SubjectSetExpansion WhileJMMMUcanas-
StanislawAntol,AishwaryaAgrawal,JiasenLu,Mar-
sessthelatestLMMs’expert-levelskills,itcannot
garetMitchell,DhruvBatra,CLawrenceZitnick,and
evaluate model performance on subjects outside DeviParikh.2015. Vqa: Visualquestionanswering.
ofthosecurrentlycovered. Asmodelsgainmore InICCV.
knowledgeandimprovetheirreasoningabilities,it
Soravit Changpinyo, Linting Xue, Michal Yarom,
willbenecessarytoexpandtherangeofsubjects
Ashish VThapliyal, Idan Szpektor, JulienAmelot,
andincludemorechallengingquestions. Xi Chen, and Radu Soricut. 2022. Maxm: To-
wardsmultilingualvisualquestionanswering. arXiv
BenchmarksinOtherCultures SinceJMMMU preprintarXiv:2209.05401.
only covers the Japanese, evaluating model per-
formanceinotherlanguagesandculturalcontexts LichangChen,ShiyangLi,JunYan,HaiWang,Kalpa
Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srini-
remains an important area for future work. We
vasan,TianyiZhou,HengHuang,etal.2023a. Al-
hopetheseeffortswillhelpmitigatetheunderrep-
pagasus: Training a better alpaca with fewer data.
resentationofdiverseculturesandlanguages. arXivpreprintarXiv:2307.08701.
Instruction Following Ability in Japanese LinChen,JisongLi,XiaoyiDong,PanZhang,Conghui
InSection5.2,wehaveshownagapininstruction- He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2024.
Sharegpt4v: Improving large multi-modal models
followingabilitybetweenlanguagesandthatmod-
withbettercaptions. InECCV.
elsgoagainsttheinstructionandgeneratetheirrea-
soningmoreofteninJapanese. Whiletheprimary ZheChen,JiannanWu,WenhaiWang,WeijieSu,Guo
focus of our benchmark is on evaluating expert Chen, Sen Xing, Muyan Zhong, Qinglong Zhang,
Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong
knowledgeandtherebyhelpingimprovesuchskills,
Lu, Yu Qiao, and Jifeng Dai. 2023b. Internvl:
itisleftasanimportantfutureworktoimprovethe
Scaling up vision foundation models and aligning
instruction-followingabilityinJapanese. Further, for generic visual-linguistic tasks. arXiv preprint
itisalsoimportanttodesignanevaluationprotocol arXiv:2312.14238.
tomeasureinstruction-followingabilitytoenhance
GoogleDeepMind.2024. Gemini1.5pro. https://
the development of such skills. While there are
deepmind.google/technologies/gemini/pro/.
somemethodstoevaluatethemodel’sinstruction-
following ability (Zhou et al., 2023; Qian et al., XingyuFu,YushiHu,BangzhengLi,YuFeng,Haoyu
2024),theseshouldbeappropriatelyincorporated Wang,XudongLin,DanRoth,NoahASmith,Wei-
ChiuMa,andRanjayKrishna.2024. Blink: Multi-
inthecontextofmultilingualperformanceevalua-
modallargelanguagemodelscanseebutnotperceive.
tion. InECCV.
Acknowledgement HaoyuanGao,JunhuaMao,JieZhou,ZhihengHuang,
LeiWang,andWeiXu.2015. Areyoutalkingtoa
ThisworkwaspartiallysupportedbyJSTBOOST, machine?datasetandmethodsformultilingualimage
Japan Grant Number JPMJBS2418, JSPS KAK- question. Advancesinneuralinformationprocessing
systems,28.
ENHI Grant Number 24K23882, and JST JP-
MJCR22U4.
Deepak Gupta, Pabitra Lenka, Asif Ekbal, and Push-
pakBhattacharyya.2020. Aunifiedframeworkfor
multilingualandcode-mixedvisualquestionanswer-
References ing. InProceedingsofthe1stconferenceoftheAsia-
Pacificchapteroftheassociationforcomputational
2023. Vicuna: Anopen-sourcechatbotimpressinggpt- linguisticsandthe10thinternationaljointconference
4with90%*chatgptquality. onnaturallanguageprocessing,pages900–913.
9WenyiHong,WeihanWang,MingDing,WenmengYu, Fangyu Liu, Emanuele Bugliarello, Edoardo Maria
QingsongLv,YanWang,YeanCheng,ShiyuHuang, Ponti, Siva Reddy, Nigel Collier, and Desmond
JunhuiJi,ZhaoXue,etal.2024. Cogvlm2: Visual Elliott. 2021. Visually grounded reasoning
languagemodelsforimageandvideounderstanding. across languages and cultures. arXiv preprint
arXivpreprintarXiv:2408.16500. arXiv:2109.13238.
Jinyi Hu, Yuan Yao, Chongyi Wang, SHAN WANG, Fuxiao Liu, Hao Tan, and Chris Tensmeyer. 2023a.
YinxuPan,QianyuChen,TianyuYu,HanghaoWu, Documentclip: Linking figures and main body
YueZhao,HaoyeZhang,XuHan,YankaiLin,Jiao text in reflowed documents. arXiv preprint
Xue,dahaili,ZhiyuanLiu,andMaosongSun.2024. arXiv:2306.06306.
Large multilingual models pivot zero-shot multi-
modallearningacrosslanguages. InICLR. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae
Lee.2024a. Improvedbaselineswithvisualinstruc-
Aozora Inagaki. 2024. llava-calm2-siglip. tiontuning. InCVPR.
https://huggingface.co/cyberagent/
llava-calm2-siglip. HaotianLiu,ChunyuanLi,YuhengLi,BoLi,Yuanhan
Zhang,ShengShen,andYongJaeLee.2024b. Llava-
YuichiInoue,TakuyaAkiba,andShingMakoto.2024a. next: Improvedreasoning,ocr,andworldknowledge.
Llama-3-evovlm-jp-v2.
HaotianLiu,ChunyuanLi,QingyangWu,andYongJae
YuichiInoue,KentoSasaki,YumaOchi,KazukiFujii, Lee.2023b. Visualinstructiontuning. InNeurIPS.
KotaroTanahashi,andYuYamaguchi.2024b. Heron-
bench: Abenchmarkforevaluatingvisionlanguage Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,
modelsinjapanese. InCVPRworkshop. Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi
Wang, Conghui He, Ziwei Liu, et al. 2024c. Mm-
DongfuJiang,XuanHe,HuayeZeng,CongWei,Max bench: Is your multi-modal model an all-around
Ku,QianLiu,andWenhuChen.2024. Mantis: Inter- player? InECCV.
leavedmulti-imageinstructiontuning. arXivpreprint
arXiv:2405.01483. PanLu,HritikBansal,TonyXia,JiachengLiu,Chun-
yuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-
HugoLaurençon,AndrésMarafioti,VictorSanh,and WeiChang,MichelGalley,andJianfengGao.2024.
Léo Tronchon. 2024a. Building and better under- Mathvista: Evaluating mathematical reasoning of
standingvision-languagemodels: insightsandfuture foundationmodelsinvisualcontexts. InICLR.
directions. arXivpreprintarXiv:2408.12637.
AtsuyukiMiyai,JingkangYang,JingyangZhang,Yifei
HugoLaurençon,LucileSaulnier,LeoTronchon,Stas Ming, Qing Yu, Go Irie, Yixuan Li, Hai Li, Ziwei
Bekman,AmanpreetSingh,AntonLozhkov,Thomas Liu,andKiyoharuAizawa.2024. Unsolvableprob-
Wang, Siddharth Karamcheti, Alexander M Rush, lemdetection: Evaluatingtrustworthinessofvision
DouweKiela,MatthieuCord,andVictorSanh.2023. languagemodels. arXivpreprintarXiv:2403.20331.
OBELICS:Anopenweb-scalefiltereddatasetofin-
terleavedimage-textdocuments. InThirty-seventh Masoud Monajatipoor, Liunian Harold Li, Mozhdeh
ConferenceonNeuralInformationProcessingSys- Rouhsedaghat, Lin F Yang, and Kai-Wei Chang.
temsDatasetsandBenchmarksTrack. 2023. Metavl: Transferringin-contextlearningabil-
ityfromlanguagemodelstovision-languagemodels.
HugoLaurençon, LéoTronchon, MatthieuCord, and arXivpreprintarXiv:2306.01311.
Victor Sanh. 2024b. What matters when build-
ing vision-language models? arXiv preprint OpenAI.2024. Gpt-4o.
arXiv:2405.02246.
JonasPfeiffer,GregorGeigle,AishwaryaKamath,Jan-
BoLi,YuanhanZhang,LiangyuChen,JinghaoWang, MartinOSteitz,StefanRoth,IvanVulic´,andIryna
Jingkang Yang, and Ziwei Liu. 2023. Otter: A Gurevych.2021. xgqa: Cross-lingualvisualquestion
multi-modalmodelwithin-contextinstructiontuning. answering. arXivpreprintarXiv:2109.06082.
arXivpreprintarXiv:2305.03726.
Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier,
Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Peter Grasch, Yinfei Yang, and Zhe Gan. 2024.
Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Mia-bench: Towards better instruction following
Li, Ziwei Liu, and Chunyuan Li. 2024a. Llava- evaluation of multimodal llms. arXiv preprint
onevision: Easyvisualtasktransfer. arXivpreprint arXiv:2407.01509.
arXiv:2408.03326.
Machel Reid, Nikolay Savinov, Denis Teplyashin,
BohaoLi,RuiWang,GuangzhiWang,YuyingGe,Yix- Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste
iaoGe,andYingShan.2024b. Seed-bench: Bench- Alayrac, et al. 2024. Gemini 1.5: Unlocking mul-
marking multimodal llms with generative compre- timodalunderstandingacrossmillionsoftokensof
hension. InCVPR. context. arXivpreprintarXiv:2403.05530.
10DavidRomero,ChenyangLyu,HaryoAkbariantoWi- LeXue,ManliShu,AnasAwadalla,JunWang,AnYan,
bowo, Teresa Lynn, Injy Hamed, Aditya Nanda SenthilPurushwalkam,HongluZhou,VirajPrabhu,
Kishore, Aishik Mandal, Alina Dragonetti, Artem et al. 2024. xgen-mm (blip-3): A family of
Abzaliev,AtnafuLambeboTonja,etal.2024. Cvqa: open large multimodal models. arXiv preprint
Culturally-diverse multilingual visual question an- arXiv:2408.08872.
sweringbenchmark. InNeurIPSDatasetsandBench-
marksTrack. QinghaoYe,HaiyangXu,JiaboYe,MingYan,Anwen
Hu,HaoweiLiu,QiQian,JiZhang,andFeiHuang.
2024. mplug-owl2: Revolutionizing multi-modal
SakanaAI. 2024a. Ja-multi-image-vqa.
https://huggingface.co/datasets/SakanaAI/ large language model with modality collaboration.
JA-Multi-Image-VQA. InCVPR.
WeihaoYu,ZhengyuanYang,LinjieLi,JianfengWang,
SakanaAI. 2024b. Ja-vg-vqa-500. https:
KevinLin,ZichengLiu,XinchaoWang,andLijuan
//huggingface.co/datasets/SakanaAI/
Wang.2024. Mm-vet: Evaluatinglargemultimodal
JA-VG-VQA-500.
modelsforintegratedcapabilities. InICML.
SakanaAI. 2024c. Ja-vlm-bench-in-the-wild. XiangYue,YuanshengNi,KaiZhang,TianyuZheng,
https://huggingface.co/datasets/SakanaAI/ RuoqiLiu,GeZhang,SamuelStevens,DongfuJiang,
JA-VLM-Bench-In-the-Wild. WeimingRen,YuxuanSun,etal.2024. Mmmu: A
massivemulti-disciplinemultimodalunderstanding
Nobuyuki Shimizu, Na Rong, and Takashi Miyazaki. andreasoningbenchmarkforexpertagi. InCVPR.
2018. Visual question answering dataset for bilin-
gualimageunderstanding: Astudyofcross-lingual Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang,
transferusingattentionmaps. InCOLING. Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang
Cheng,ChunpuXu,ShuyueGuo,etal.2024. Cm-
JingqunTang,QiLiu,YongjieYe,JinghuiLu,ShuWei, mmu: A chinese massive multi-discipline multi-
ChunhuiLin,WanqingLi,MohamadFitriFaizBin modal understanding benchmark. arXiv preprint
Mahmood,HaoFeng,ZhenZhao,etal.2024. Mtvqa: arXiv:2401.11944.
Benchmarkingmultilingualtext-centricvisualques-
Bo Zhao, Boya Wu, and Tiejun Huang. 2023. Svit:
tionanswering. arXivpreprintarXiv:2405.11985.
Scalingupvisualinstructiontuning. arXivpreprint
arXiv:2307.04087.
HugoTouvron,ThibautLavril,GautierIzacard,Xavier
Martinet,Marie-AnneLachaux,TimothéeLacroix,
Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang,
Faisal Azhar, et al. 2023. Llama: Open and effi-
WenjuanHan,andBaobaoChang.2024. Mmicl:Em-
cient foundation language models. arXiv preprint
poweringvision-languagemodelwithmulti-modal
arXiv:2302.13971.
in-contextlearning. InICLR.
Turing. 2024a. Llava-bench-in-the-wild. https: Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Sid-
//huggingface.co/datasets/liuhaotian/ dharthaBrahma,SujoyBasu,YiLuan,DennyZhou,
llava-bench-in-the-wild/tree/main. and Le Hou. 2023. Instruction-following evalu-
ation for large language models. arXiv preprint
Turing. 2024b. Llava-bench-in-the-wild arXiv:2311.07911.
(japanese). https://github.com/turingmotors/
heron/tree/main/playground/data/
llava-bench-in-the-wild.
Turing. 2024c. Llava-bench-ja. https:
//github.com/turingmotors/heron/tree/
main/playground/data/llava-bench-ja.
YuxiaWang,HaonanLi,XudongHan,PreslavNakov,
andTimothyBaldwin.2024. Do-not-answer: Evalu-
atingsafeguardsinLLMs. InFindingsoftheAsso-
ciationforComputationalLinguistics: EACL2024,
pages896–911,St.Julian’s,Malta.Associationfor
ComputationalLinguistics.
Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert
Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu,
DaHuang,DennyZhou,etal.2023. Largerlanguage
models do in-context learning differently. arXiv
preprintarXiv:2303.03846.
11Appendix TableA:LMM’sJapanesesupport.
JMMMU Japanesesupport
A LMMs’JapaneseSupport
Model Overall BaseLLM LLM LMM
TodiscussthemultilingualcapabilitiesofLMMs, OpenSource
xGen-MM 28.6 Phi-3 ✗ ✗
wesummarizewhethereachmodelofficiallysup- Mantis8B 35.5 Llama3 ✗ ✗
portsJapanese. TableApresentstheJapaneselan- Idefics2-8B 31.9 Mistralv0.1 ? ✗
guagesupportstatusforeachmodel. “✓”indicates Idefics3-8B 37.3 Llama3 ✗ ✗
CogVLM2-19B 36.1 Llama3 ✗ ✗
officialsupportforJapanese, while“✗”indicates InternVL2-2B 28.3 InternLM2 ✗ ✗
theabsenceofsuchsupport. Also,wedenote“?” InternVL2-8B 38.3 InternLM2 ✗ ✗
formodelsofwhichwecouldnotfindtheinforma- LLaVA-1.613B 31.1 Vicunav1.5 ✗ ✗
LLaVA-1.634B 39.8 NousHermes2Yi ✗ ✗
tion. LLaVA-OneVision0.5B 26.0 Qwen2 ✓ ✗
Even if a model is marked as “✗”, it may still LLaVA-OneVision7B 40.5 Qwen2 ✓ ✗
Phi-3Vision 29.5 Phi-3 ✗ ✗
demonstrate some Japanese language capability
Phi-3.5Vision 32.4 Phi-3.5 ✓ ✗
due to the presence of Japanese data in publicly †LLaVACALM2 34.9 CALM2 ✓ ✓
availabledatasetslikeShareGPT-4V(Chenetal., †EvoVLMJPv2 38.1 (mergedmodel) ✓
2024)andShareGPT-4o2,ordatacrawledfromthe ClosedSource
Claude3.5Sonnet 50.8 ? ? ✓
web. Gemini1.5Pro 51.5 ? ? ✓
Proprietary commercial models, such as GPT- GPT-4o 58.6 ? ? ✓
4o,Gemini1.5Pro,andClaude3.5Sonnet,donot
publiclydisclosedetailedinformationabouttheir
Chinese,withnoexplicitmentionofJapanesesup-
trainingdata. However,basedontheirreleaseblog
port.
posts,itcanbeinferredthatthesemodelssupport
Idefics2, Idefics3, xGen-MM, and Mantis use
manylanguages,includingJapanese.
large-scaledatasetsformultimodaltraining. How-
LLaVACALM2isbasedontheJapaneseLLM
ever, there is no clear evidence of Japanese
CALM23,andithasbeentrainedusingJapanese
data inclusion, and in some datasets, such as
multimodaldatasets,officiallysupportingJapanese.
OBELICS (Laurençon et al., 2023), non-English
EvoVLM JP v2, a merged model (Akiba et al.,
dataisexplicitlyfilteredout. WhileLlama3,the
2024), also incorporates Japanese data for opti-
base model for some of these LMMs, mentions
mization and is officially released as a Japanese
multilingualtraining,itdoesnotexplicitlyconfirm
LMM.
support for Japanese. Mistral v0.1 also does not
Phi-3.5 Vision does not officially support
discloseitstrainingdata.
Japanese, despite its base model, Phi-3.5, having
Theperformanceofthesemodelsdependsona
officialsupportformultiplelanguages,including
complex interplay of factors, including the quan-
Japanese. Phi-3Vision,likewise,doesnotsupport
tity and quality of the training data and the size
non-Englishlanguages.
andcapabilitiesofthebaselanguagemodel. Offi-
IntheLLaVAseries,LLaVA-OneVisionexplic-
cial support for Japanese is not the only consid-
itlymentionssupportforChineseinitstrainingbut
eration; there are reports of models trained on
doesnotextendthistoothernon-Englishlanguages.
English-onlymultimodaldatageneralizingtoother
However, Qwen2, the base LLM for the LLaVA-
languages (Hu et al., 2024), including Japanese.
OneVision models, officially supports Japanese.
Moreover, since many models are designed with
LLaVA-1.6modelsaretrainedfromdifferentbase
Chinesesupport,theculturalandlinguisticproxim-
LLMs,suchasVicunav1.5andNousHermes2Yi,
itybetweenJapaneseandChinese-speakingregions
neitherofwhichofficiallysupportJapanese. Thus,
mayresultinahighperformanceinJapanese.
Japaneselanguagecapabilitiesarenotguaranteed
intheirvisualinstructiontraining.
B MoreResult
InternVL and its base model, InternLM2, offi-
ciallysupportonlyEnglishandChinese. Similarly,
B.1 ErrorAnalysisinCulture-Agnostic
CogVLM2claimsproficiencyinbothEnglishand
subjects
2https://huggingface.co/datasets/OpenGVLab/
In Section 5.2, we present the error analysis for
ShareGPT-4o
GPT-4o on the CA subjects. In this section, we
3https://huggingface.co/cyberagent/
calm2-7b-chat provide the error analysis for all models. While
12EN: , JP: EN: , JP: EN: , JP: EN: , JP:
(181 .3 32 %) (211 .5 76 %) (171 .2 67 %) (241 .7 46 %) (107 .77 %) 122
78 70 341 (16.9%)
(594 .3 91 %) (1( 11 7 .0 09. %8% )) (564 .0 57 %) (118( .1 530 %7 .34 )%) (554 .0 60 %) (171( .29 13. %7% )) (47.4%) 132(9.7 91 %) (543 .8 09 %) (181 .3 32
%)
(18.3%)
LLaVA CALM2 EvoVLM JP v2 idefics2 8B InternVL2 8B LLaVA-OV 0.5B
148 158 114
(20.6%) 249 241 237 (21.9%) (15.8%)
(513 .7 83 %) (171 .2 8( 89 %.7 9 )1 %) (34.6 (% 171) .2 24 %)(141( .0 73 6 %3.5 )%) (392 .8 76 %) (171 .2 24 %)(1( 03 7 .2 13. %9% )) (533 .8 66 %) (141 .0( 21 2 %07 . )34 %) (574 .1 86 %) (1( 31 9 .52 79 %.93 % ))
Phi-3.5v Claude 3.5 Sonnet Gemini 1.5 Pro idefics3 8B LLaVA1.6 13B
(292 .1 95 %) (191 .4 72 %) (201 .5 80 %) 256 258 (118 .74 %) 112
(473 .4 41 %) (513 .7 72 %) (118 .74 %) (513 .7 72 %) (118 .96 %) (35.6%) (35.8%) (533 .8 23 %) (15.6%)
59
(141 .0 65 %(8 ).2%) (161 .2 92 %) (151 .1 62 %) (151 .0 08 %)(139 .68 %) (191 .4 61 %)
LLaVA-OV 7B Phi-3v CogVLM2 GPT-4o InternVL2 2B
(251 .8 86 %) (211 .5 12 %) (161 .2 92 %)
321 84
(44.6%) 384 78 383 (11.7%)
76 (53.3%) (10.8%) (53.2%)
(10.6%) 106 131
137 (14.7%) (18.2%)
(19.0%)
LLaVA1.6 34B Mantis 8B xGen-MM
FigureA:Errorinculture-agnosticsubjects. Thisfigurecategorizesthecorrectnessofanswersinculture-agnostic
subjectsbasedontheoriginalMMMUEnglishresponses(correctorincorrect)andthecorrespondingJMMMU
translatedresponses(correctorincorrect).
we have shown in Table 2 that most models per- tion 5.2, which allows it to infer answers more
form worse in Japanese, there are some amount easily. Notethatourexperimentherehasbeencon-
of questions where the model answers correctly ductedbyusingquestionsthatinvolvedtranslation
onlyinJapaneseforeverymodel. Thenumberof ofbothtextsandimages. Manyofthemconsistof
suchquestionsisparticularlyhighforLLaVA-OV tabledata,whichrequiresstrongerreasoningbased
0.5B and InternVL2 2B. This occurrence, how- on data processing, so the result may vary when
ever,appearstobearandomphenomenon,likely investigatingdifferentdatatypesthatdonotexist
attributabletotheoverallweakerperformanceof intheCAsubsetofJMMMU.
thesemodels.
B.2 AblationonImageTranslation
B.3 ScoreCorrelationbetweenlanguages
ThefullsetofTable3ispresentedinTableB.As
discussedinSection5.1,eachmodelreactsdiffer-
entlyasthetranslationproceeds,andthetendency Usingtheculture-agnosticsubset,wehavedemon-
is difficult to summarize. Notably, here, GPT-4o stratedinSection4.2that(i)modelsperformworse
showsa7.2%improvementinscoreaftertexttrans- inJapaneseand(ii)JapaneseLMMsshowrobust-
lation. Thispartlystemsfromitsweakinstruction- ness to translation. To illustrate these points, we
following skills in Japanese, as discussed in Sec- provideFigureB.
13TableB:Thefullsetofthetranslationeffect. Each 55
columnshowsthemodelperformancewhenimage(I) LLaVA-OV 0.5B
InternVL2 2B
and text (T) are in Japanese (jp) or in English (en). 50 xGen-MM
Phi-3 Vision
∆ i shows the difference from I enT en. † represents LLaVA-NeXT 13B
JapaneseLMMs. 45 I Pd he if -i 3c .s 52 V 8 iB sion
Mantis-8B
I enT en I enT jp(∆ 1) I jpT jp(∆ 2) CogVLM2 19B
40 Idefics3 8B
Opensource InternVL2 8B
LLaVA-NeXT 34B
LLaVA-OV-0.5B 28.9 28.9(±0.0) 29.7(+0.8) 35 LLaVA-OV 7B
InternVL2-2B 32.5 29.7(-2.8) 28.6(-3.9) LLaVA CALM2
EvoVLM-JP v2
xGen-MM 36.7 28.3(-8.4) 28.3(-8.4) 30 Claude 3.5 Sonnet
Phi-3v 35.0 31.7(-3.3) 29.7(-5.3) Gemini 1.5 Pro
GPT-4o
LLaVA-1.6-13B 26.4 31.9(+5.5) 29.2(+2.8)
25
Idefics2-8b 28.9 28.1(-0.8) 28.1(-0.8) 25 30 35 40 45 50 55
Phi-3.5v 39.2 33.6(-5.6) 31.1(-8.1) Culture Agnostic (EN) [%]
†LLaVA-CALM2 29.4 28.3(-1.1) 31.4(+2.0)
Mantis8B 32.5 31.1(-1.4) 31.4(-1.1)
FigureB:Scorecorrelationbetweenlanguages.■rep-
CogVLM2-19B 32.8 31.9(-0.9) 34.4(+1.6)
resentsproprietarymodelsand⋆representsJapanese
Idefics3-8b 33.1 31.7(-1.4) 29.7(-3.4)
†EvoVLMJPv2 30.0 30.8(+0.8) 28.6(-1.4) LMMs. WhileallmodelsperformworseinJapanese,
InternVL2-8B 43.9 38.3(-5.6) 37.2(-6.7) Japanese LMMs perform similarly in both languages
LLaVA-1.6-34B 43.6 40.8(-2.8) 38.9(-4.7) (i.e.,closetothegraydashedline)
LLaVA-OV-7B 45.0 38.3(-6.7) 35.6(-9.4)
Proprietary
Claude3.5Sonnet 53.6 56.4(+2.8) 54.2(+0.6) While this allows an evaluation robust against
Gemini1.5Pro 50.6 42.2(-8.4) 42.2(-8.4)
somevarietyofanswergenerationstyles,wehave
GPT-4o 48.1 55.3(+7.2) 53.1(+5.0)
showninSection5.2thatthiscansometimesover-
estimatetheperformanceinJapanesebecausemod-
C FurtherExperimentalDetails els’ instruction-following abilities are relatively
lowinJapanese.
C.1 ExperimentalSetup
ComputingInfrastructures Weconductallour D AnnotationInstruction
evaluations of open-source models on a single
Recruitment and Payment Annotators were
NVIDIAA100(80GB)GPU.
paid at least the minimum wage set in Japan, ac-
Parameters for LMM Inference A maximum cordingtothetimespentonthetask.
outputlengthis setto1,024andatemperature is
Data Consent They were informed that trans-
setto0forallmodelsduringinference.
lateddatawouldbeusedforevaluationpurposes.
C.2 EvaluationProtocol
Instructions Given to Participants The docu-
AnswerExtractioninMultipleChoiceQuestion ment containing the instructions presented to the
While the models are instructed to answer their annotatorsisshowninFigureC.
choice directly, they often generate some contex-
E Examples
tualinformationorunnecessarysymbols. Totackle
thispoint,followingMMMU(Yueetal.,2024),we
Weprovidesamplequestionsfromculture-agnostic
extract an answer from the model response with
subset in Figure D, and questions from culture-
a rule-based method. For multiple-choice ques-
specificsubsetinFigureE
tions, this parser can extract the model’s choice
evenwhenthechoiceissurroundedbysomesym-
bol(e.g.,’(A)’,’A.’,’A’)orbytext.
Forexample,theseanswers,whichareallsome
variantsof“TheanswerisA.”inJapanese,canbe
parsedas“A”:
• 回答はA
• 答えは、Aであると考えられる
• 画像は首里城のため、答えは(A)。
• 答え:A.15.3
14
]%[
)PJ(
citsongA
erutluCFigureC:AnnotationInstruction. AnnotatorswereprovidedwiththeJapaneseversionofthisinstruction.
15Art&Psychology
Question: <image 1> 拍⼦記号に基づくと、次の楽譜の⼩節数は__です。 Question: <image 1> 上記のデータに対して最適な説明を⽰しているのは
(<image 1> Based on the time signature, the number of measures/bars in the 次のうちどれですか？
following music score is _____.) (<image 1> Which of the following provides an effective explanation for the
data above?)
Options:
A. 6 Options:
B. 7 A. オペラント条件付け(Operant conditioning)
C. 5 B. 古典的条件付け(Classical conditioning)
D. 4 C. 準備された条件付け(Prepared conditioning)
D. ⾃⼰実現(Self-actualization)
E. 観察学習(Observational learning)
Business
Question: <image 1> 提供された画像は次のどれを描いていますか？ Question: こちらは1929年から1933年までのインフレ率と⽶国株式市場お
(<image 1> The provided image depicts a/an) よび⽶国財務省短期証券のリターンです: <image 1> 。
1932年の株式市場の実質リターンは何でしたか？
(Here are inflation rates and U.S. stock market and Treasury bill returns
Options: between 1929 and 1933: <image 1> . What was the real return
A. 需要曲線(demandcurve) on the stock market in 1932?)
B. フィリップス曲線(Phillips curve) Options:
C. ⽣産可能性フロンティア A. -14.33%
(production possibilities frontier) B. -23.72%
D. 総供給曲線(aggregate supply curve) C. 0.45%
E. ローレンツ曲線(Lorenz curve) D. 56.52%
Science
Question: Question:
関数f(x) = x^2 -6x + 4 は凸ですか、それとも凹ですか？<image 1> <image 1>核反応において? は何を表していますか
(Is the function f(x) = x^2 -6x + 4convex or concave? <image 1>) (<image 1>What does the ? represent in the nuclear reaction)
Options: Options:
A. 凸(Convex) A. アルファ粒⼦(an alpha particle)
B. 凹(Concave) B. 電⼦(an electron)
C. どちらでもない(Neither) C. 中性⼦(a neutron)
D. 両⽅(Both) D. 陽⼦(a proton)
Medicine
Question: 25歳の移⺠は発熱と数ヶ⽉にわたる脊髄の問題をかかえていま Question:
す。この最も可能性の⾼い病因は何ですか？: <image 1> このX線写真で異常が⾒られる臓器は何ですか？<image 1>
(25 year oldimmigrant with fever and several month history of cord problems. The (What organ appears abnormal in this radiograph? <image 1>)
most likely etiologyof this process is: <image 1>)
Options: Options:
A. トリパノソーマ症(Trypanosomiasis) A. 胃(Stomach)
B. アメーバ性脳炎(Amebicencephalitis) B. 肝臓(Liver)
C. 脳マラリア(Cerebral malaria) C. 胆嚢(Gallbladder)
D. 結核性髄膜炎(Tuberculous meningitis) D. ⼗⼆指腸(Duodenum)
E. アスペルギルス症(Aspergillosis)
Tech & Engineering
Question: <image 1> に⽰されているシステムについて、 Question:
⼊⼒が50u(t) のときの定常誤差を求めなさい。 <image 1> の⾏列の⾛査⽅法はどれに当てはまりますか？
(For the system shown in <image 1>, (What kind of matrix traversal is <image 1>?)
Find the steady-state error for an input of 50u(t).)
Options:
A. ⾏列の通常の⾛査
Options: (Normal traversal of the matrix.)
A. 17.59 B. ⾏列の⾏ごとの⾛査
B. 27.59 (Row-wise traversal of the matrix.)
C. 37.59 C. ⾏列の列ごとの⾛査
(Column-wise traversal of the matrix.)
D. ⾏列のスパイラル⾛査
(spiral traversal of the matrix.)
FigureD:Examplesinculture-agnosticsubjects. SomeimagesthatcontainEnglisharetranslated.
16Japanese Art
Question: <image 1>は何と⾔う作品でしょう？ Question:
(What is the name of the work in <image 1>?) <image 1>に描かれている⼈物が持っているものはなんでしょう？
(What is the person depicted in <image 1> holding?)
Options:
A. 紫式部⽇記絵巻
(The Diary of Murasaki Shikibu Emaki) Options:
B. 更級⽇記絵巻 A. ⽩⽠(White gourd)
(The SarashinaDiary Emaki) B. 瓢箪(Gourd)
C. 蜻蛉⽇記絵巻(The KageroDiary Emaki) C. 琵琶(Biwa)
D. 清少納⾔⽇記絵巻 D. 籠(Basket)
(The Diary of Sei ShonagonEmaki)
Japanese Heritage
Question: Question: <image 1>がある地域で栄えた⼀族は誰でしょう？
<image 1>の城の名前は何でしょう？ (Which clan prospered in the region with <image 1>?)
(What is the name of the castle in <image 1>?)
Options:
Options: A. 鎌倉源⽒(Kamakura Genji)
A. 名古屋城(Nagoya Castle) B. 藤原北家(Fujiwara Hokke)
B. 弘前城(HirosakiCastle) C. 奥州藤原⽒(OshuFujiwara)
C. 彦根城(HikoneCastle) D. 信濃武⽥⽒(ShinanoTakeda)
D. 松本城(Matsumoto Castle)
Japanese History
Question: Question: <image 1>を⽤いて幕府がおこなった⾏為を
<image 1>が起きた時の⽼中は誰でしょう？ 何というでしょう？
(Who was the senior councilor (What is the act performed by the shogunate
when <image 1> occurred?) using <image 1> called?)
Options: Options:
A. ⽔野忠邦(Mizuno Tadakuni) A. 絵踏(Fumi-e)
B. 松平定信(MatsudairaSadanobu) B. 検地(Land survey)
C. 遠⼭⾦四郎(TōyamaKinjirō) C. 勘合(Kango)
D. ⽥沼意次(TanumaOkitsugu) D. 鎖国(Sakoku)
WorldHistory
Question: Question:
<image 1>の統治を何というでしょう？ <image 1>が表す出来事は何でしょう？
(What is the reign of <image 1> called?) (What event is represented by <image 1>?)
Options: Options:
A. 貞観の治(Reign of Jōgan) A. カノッサの屈辱(Humiliation of Canossa)
B. 開元の治(Reign of Kaiyuan) B. アヴィニョン捕囚(Avignon Captivity)
C. 永楽の治(Reign of Yongle) C. ギュイエンヌの屈辱('Humiliation of Guyenne)
D. 康熙の治(Reign of Kangxi) D. ウォルムスの屈辱(Humiliation of Worms)
Figure E: Examples in culture-specific subjects. The questions are created by Japanese native speakers and
requiresknowledgeofJapaneseculture.
17