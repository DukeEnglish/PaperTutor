Coniferest: a complete active anomaly detection
framework
M. V. Kornilov1,2⋆, V. S. Korolev3, K. L. Malanchev4,5, A. D. Lavrukhina1,6,
E. Russeil7, T. A. Semenikhin1,6, E. Gangler8, E. E. O. Ishida8,
M. V. Pruzhinskaya8, A. A. Volnova7, S. Sreejith9
(The SNAD team)
1 Lomonosov Moscow State University, Sternberg Astronomical Institute,
Universitetsky pr. 13, Moscow, 119234, Russia
2 National Research University Higher School of Economics, 21/4 Staraya
Basmannaya Ulitsa, Moscow, 105066, Russia
3 Independent researcher
4 McWilliams Center for Cosmology and Astrophysics, Department of Physics,
Carnegie Mellon University, Pittsburgh, PA 15213, USA
5 Department of Astronomy, University of Illinois at Urbana-Champaign, 1002 West
Green Street, Urbana, IL 61801, USA
6 Lomonosov Moscow State University, Faculty of Space Research, Leninsky Gori 1
bld. 52, Moscow 119234, Russia
7 Space Research Institute of the Russian Academy of Sciences (IKI), 84/32
Profsoyuznaya Street, Moscow, 117997, Russia
8 Université Clermont Auvergne, CNRS/IN2P3, LPCA, 63000 Clermont-Ferrand,
France
9 Physics Department, University of Surrey, Stag Hill Campus, Guildford GU2 7XH,
UK.
Abstract. Wepresentconiferest,anopensourcegenericpurposeac-
tiveanomalydetectionframeworkwritteninPython.Thepackagedesign
and implemented algorithms are described. Currently, static outlier de-
tectionanalysisissupportedviatheIsolationforestalgorithm.Moreover,
Active Anomaly Discovery (AAD) and Pineforest algorithms are avail-
able to tackle active anomaly detection problems. The algorithms and
packageperformanceareevaluatedonaseriesofsyntheticdatasets.We
alsodescribeafewsuccesscaseswhichresultedfromapplyingthepack-
age to real astronomical data in active anomaly detection tasks within
the SNAD project.
Keywords: Machine learning · Active learning · Anomaly detection.
1 Introduction
The ultimate goal of every machine learning (ML) algorithm is to extract in-
formation present from large data sets, and in the process, optimise the allo-
cation of human resources devote to it. Ultimately, improving human life. In
⋆ kornilov@physics.msu.ru
4202
tcO
22
]MI.hp-ortsa[
1v24171.0142:viXra2 M. V. Kornilov et al.
the context of anomaly detection (AD) strategies this translates into enabling
discoveries which would never happen otherwise. Depending on the considered
field, it means that we can discover hidden or upcoming hardware failures, rare
software bugs, malicious or fraud activity in software systems, or even new and
completely unforeseen astrophysical objects. All these cases have implications
either on our understanding of the Universe as a whole, or on crucial aspects of
daily life.
In AD tasks, it is usually assumed that the dataset includes rare objects
(or groups of objects) which are very unlikely to exist when working with the
hypothesisthatallelementsinthedatasetweregeneratebythesameunderlying
mechanism. For example, it is often assumed that anomalies are a minority
consisting of a few instances, and that they have attribute values that are very
differentfromthoseofnominalinstancesin[6].Inthiscontext,suchobjectscan
becaughtbyoneofthemanyoutlierdetectionalgorithmsavailable.However,in
the context of scientific research, both assumptions are not fully correct. First,
being rare is not enough attribute to consider an astrophysical source valuable
or scientifically interesting. When outliers are inspected by a human expert,
it often happens that most of them are statistically rare but not useful data
samples – since the final expert request is not to find samples in low density
partsofthefeaturespacebuttosolvethereal-worldproblems(e.g.68percentof
outliers in [10] were found to be bogus light curves, including issues with survey
automatic photometry and external effects such as satelite or plane tracks).
Second, sometimes having attribute values slightly above a certain threshold is
enough to be considered a scientifically interesting anomaly. For instance, if we
imagine a white dwarf with a mass greater than the Chandrasekhar limit, it
becomes clear that the mass only needs to be significantly greater within the
measurement error, rather than an order of magnitude greater.
However, identifying such cases is far from a trivial task. Given the volume
and complexity of modern data sets, the expert is usually able to inspect a very
small subset of the objects identified as anomalous by traditional algorithms.
Indeed, inspecting the hardware or checking software behaviour are extremely
time consuming tasks, not to mention the specific case of astronomy, where
additional measurements may be required in order to make a final decision.
Thus,algorithmsabletofully,andoptimally,exploitpastexpertdecisionswhen
dealing with new and unlabeled data are required.
Active learning (AL) algorithms constitute a set of learning strategies that
employ expert feedback to fine tune the learning model [14]. In the context of
AD, one possible approach is to sequentially show to the expert the object with
highestanomalyscoreandusingthefeedbacktoupdatethehyperparametersof
thelearningmodel[4].Theill-posednatureoftheADproblemcoupledwiththe
AL strategy results in personalized models which are trained to identify, within
a large data set, the specific type of anomaly that is interesting to the expert.
While performing research in this field, the SNAD10 team noticed still an-
other practical issue preventing the wide spread use of AL strategies in as-
10 https://snad.spaceConiferest: a complete active anomaly detection framework 3
tronomy: the lack of AL algorithms in popular scientific software, like scikit-
learn11. Recent efforts in this direction are very sparse and do not completely
address these issues12,13. This motivated us to develop our own software frame-
work, as well as completely new algorithms which were specifically designed
to the application of active AD strategies in astronomy catalog data. This re-
sulted in a fully automatized environment and optimize algorithms which have
fulfilled our expectations throughout the last few years. We believe this tool
is now ready to benefit others, so we decided to provide the community with
coniferest,aMLframeworkcreatedusingourteamdeepexpertiseinthefield
of active anomaly detection. The package accepts as input any rectangular data
matrix holding one line per object and one column per feature14. Despite our
maininterestbeingonastronomicalapplication,theentireframeworkissuitable
for a large range of scientific applications.
This paper is organized as follows. In Section 2 we present all currently
implemented algorithms employed in the package. In Section 3 we highlight
packagedesignconsiderationsandoverallusagepattern.InSection4weprovide
package evaluation and conclusions are given in Section 5.
2 Algorithms
2.1 Isolation forest
Isolation forest (IF) is an outlier detection algorithm described in [6] and later
in [7]. It is not an active anomaly detection method, however it is the base for
other tree-based algorithms implemented within the package. Each member of
thetreeensembleisaparticularvariantofextremelyrandomizedtreescalledan
iTree (isolation trees) in the original paper [7].
Consideringabinarytree,eachnoderepresentsasplitpointandafeature(co-
ordinateindex),resultingintothefeaturespacepartitioningintonon-overlapping
multidimensional rectangles. An arbitrary value can be assigned to every rect-
angle, for instance a class, for a classification problem, or a real value, for a
regression problem. Note that it is computationally inexpensive to find the cor-
responding rectangle for any input point in the feature space. Its assigned value
is then used as the tree prediction for that particular object. Depending on the
problem, the split value and the feature are evaluated for every node in some
optimal way to create the optimal feature space partitioning, providing good
predictions.
An ensemble of trees — called as forest — consists of a number of weak
predictors (trees) each solving the same problem, being it a classification, a
regression problem, or an outlier isolation problem. Then, the predictions of
11 https://scikit-learn.org/
12 ActiveAD(https://github.com/clarenceluo78/ActiveAD)hostsmanyalgorithms
but documentation is sparse and it is not adapted to astronomical data.
13 Astronomaly [8], is posterior to our efforts and follows its own interface design.
14 At this moment it does not handle missing data.4 M. V. Kornilov et al.
every tree are averaged, summed, or taken into account in another manner over
the whole forest, depending on the considered method. Exactly as Leo Tolstoy
writes, Happy families are all alike; every unhappy family is unhappy in its
own way [15], all trees in the forest provide a slightly wrong prediction in its
own way, but using the ensemble averaging allows us to improve prediction
qualitydramatically.Usually,everytreeneedstobeadditionallyrandomizedby
subsampling the dataset or choosing a subset of considered features, otherwise
all trees would be wrong in a similar way and the ensemble averaging would not
help. Decision trees for classification via the random forest method make splits
in some local-optimal manner, trying to separate samples belonging to different
classes as much as possible. In case of AD, isolation trees choose completely
random order for both, feature and the split point. At every node a random
feature and a random split point, from the domain covered by the data, are
chosen. Empty (or degenerated) splits are prohibited, i.e. every single feature
space rectangle encloses at least one sample from the initial dataset. The single-
pointrectanglecannotbesplitfurtherandcorrespondstoaleafofthetree.Then
it is fairly obvious that splitting single point into the separate region are more
likely when the point is distant from the bulk of the data in the feature space.
Therefore,theregionsoffeaturespacepopulatedbyoutliersappearearlier,near
therootofthetree.Itmeansthataverageleafdepthsaresmallerfortheoutliers
than those for the nominal data.
For every particular point in the feature space it is possible to evaluate the
leafdepthineverytreeoftheforest.Then,theleafdepthsareaveragedandused
as a measure of abnormality. Namely, the following anomality score s(x) is used
within coniferest package, to be consistent with those used in scikit-learn:
s(x)=−2− cd¯ (( Nx) ). (1)
In the above equation, c(N) is the normalized constant which depends on the
dataset size N following:
(cid:18) (cid:19)
1 1
c(N)≡2(H(N)−1)≈2 γ−1+ln(N)+ − , (2)
2N 12N2
where H(N) is so-called Harmonic number, γ ≈ 0.5772 is Euler’s constant and
theconstantc(N)isessentiallyanaverageleafdepthforanisolationtreebuiltfor
uniformly distributed data. Moreover, d¯(x) is an ensemble-averaged leaf depth
for a given data point, x:
K
d¯(x)≡ 1 (cid:88) (d(l (x))+c(N(l (x)))), (3)
K i i
i=1
withK representingthenumberoftreesintheforest;l (x)denotingtheleafini-
i
thtreecoveringpointx,d(l)beingtheleafldepth,andN(l)correspondingtothe
numberofsamplesfromthedatasetinleafl.Apopularoptimizationisemployed
in our isolation forest implementation: anomaly scores for the nominal data areConiferest: a complete active anomaly detection framework 5
evaluatedonlyapproximately,sincetheyarenotasimportantasanomalyscores
for outliers. The tree depth is limited to log (N), which allows to speedup the
2
process of building trees. The term c(N(l(x))) in Equation 3 a consequence of
such optimization.
We notice that s(x) → −1 as d¯(x) → 0, and s(x) → +1 as d¯(x) → +∞,
which results in outliers having negative scores.
2.2 Active Anomaly Discovery
ActiveAnomalyDiscovery(AAD)isagenericactivelearningtechniqueproposed
by [2,3,4]. Basically, AAD is not limited by forest algorithms such as Isolation
forest. It is also possible to apply AAD on top of any other outlier detection
algorithm. However, we briefly describe here only the forest flavour of AAD
approach, since it is the one currently implemented in the coniferest package.
To understand some motivation of this technique, let us note that our final
goal is to mimic the decisions provided by a human expert. If the expert could
label the entire dataset, we would have reformulated the problem as a binary
classification one. Then, the Random forest classifier or even Extremely ran-
domizedtreescouldhavebeenappliedtosolvetheproblem.Theclass(anomaly
versus nominal) would have been assigned to every leaf of the classifier.
The above algorithm seems very similar to our previous description of IF,
exceptthataleafdepth,insteadofaclass,isassignedtoeveryleafoftheforest.
Indeed, both IF and Extremely randomized trees, perform a partitioning of the
feature space. The only difference is in the values assigned to the leafs of each
tree. The key idea of AAD is to take IF as a starting point and then iteratively
adapt the leaf values to make the forest similar to a hypothetical Extremely
randomizedtreeclassifier,trainedtorecognise2classes(nominalandanomaly),
giventhattheanomalyclassiscomposedbyobjectsthatfulfilltheexpectations
oftheexpert.ConsideringthatIFisagoodenoughstartingpoint,wecanexpect
that the classifier state is potentially reachable using partially labeled data.
There are two possible sources of partially labeled data. First, we can op-
tionally provide the algorithm with the labels known in advance, e.g. given well
establishedcatalogsorliteraturereports.Second,thehumanexpertcanbeasked
to input decisions while the active learning loop is running.
ForAAD,wehavethefollowingequationforthescore,whichreplacesEqua-
tion (1):
K
(cid:88)
s(x)= w φ(d(l (x))+c(N(l (x)))), (4)
i,li(x) i i
i=1
where K is the number of trees in the forest; φ(d) is some nonlinear trans-
formation function15 and w are weights which can be adjusted, allowing
i,li(x)
Equation (4) to better predict decisions reported by the human expert. There
15 Our default choice is φ(d) = −d−1, however other variants such as φ(d) = −2d,
φ(d)=1, or φ(d)=d are also considered [2,3,4].6 M. V. Kornilov et al.
areasmanydifferentweightparametersasthereareleafsintheforest.Forsim-
plicity,wealsousenotationw furthereachj correspondstoaleafintheforest.
j
Additionally, all weights were normalized, so ∥w∥2 =1.
Considering A as a known anomaly subset of the data and N as a known
nominal subset, the optimization problem for estimating weights w is defined
as:
(cid:32)
w=argmin C a (cid:88) ReLU(s(x |w)−q )+
w |A| i τ
i∈A
(cid:33)
1 (cid:88) 1
+ ReLU(q −s(x |w))+ ∥w−w ∥2 . (5)
|N| τ i 2 0
i∈N
ThefirstterminEquation(5)isalossduetoknownanomaliespredictedasnom-
inals by the model. The second term is a loss due to known nominals wrongly
predicted as anomalies and the third term accounts for regularization. The con-
stant C allows us to assign a relative importance of to both kind of errors, our
a
current default choice being C =1. The threshold score q is the limiting score
a τ
necessary to identify the 1−τ percentile of the dataset. Our current default
choice is τ =0.97. This means that all known anomalies should occupy the top
3% anomaly scores, which is achieved by adjusting the weights w. The w in
0
regularization term is either an uniform vector or the weights from the previous
iteration, while ReLU denotes the rectified linear function:
(cid:40)
x x≥0,
ReLU(x)≡ (6)
0 otherwise.
Solving the optimization problem described by Equation (5) is challenging due
to high number of unknown parameters to be determined. Currently, we use
trust-krylovmethodfromscipyoptimizationframework,whichusestheNew-
ton GLTR trust-region algorithm [5].
Ateachiteration,Equation (5)issolvedandthescoresfortheentiredataset
are reevaluated using Equation (4) and the new weights w. Thus, resulting in
a new unknown object associated to the highest anomaly score and a new τ
score quantile. The most anomalous unknown object is shown to the human
expert. Depending on the expert decision, this object is added to either A or
N,andthenextiterationoccurs.Weemphasizethatitiscrucialtoperformthe
model inference as fast as possible, since estimations for the entire dataset are
reevaluated at every iteration.
The total number of iterations, called budget, is an input chosen by the
expert, and should be determined taking into account the available time, as
well as other resources, necessary to properly judge the candidates presented by
the algorithm. Once the budget is exhausted, we can calculate one of the most
important metrics used to evaluate method effectiveness: tje number of expert-
approved anomalies within the budget. In case of simulated data, we can also
useconvenientmetrics,e.g.confusionmatrices.However,thisisnotpossibleinaConiferest: a complete active anomaly detection framework 7
realdatascenariosincetheuserwouldnotknowhowmanyinterestinganomalies
are included in the entire data set.
2.3 Pineforest
Pineforest16 presents another approach of refining IF by incorporating feedback
from experts. Unlike the previous method, Pineforest does not directly modify
existing trees. Instead, it selectively discards trees that inadequately represent
the underlying data distribution.
In order to determine the suitability of each tree, a scoring mechanism is
devisedbasedonthelabeleddata.Weassignscorestothedatapointsasfollows:

−1 if x is labeled as an anomaly,
 i
y = 0 if x is unlabeled,
i i
1
if x is labeled as a regular point.
i
For any given tree t , we calculate its score as:
i
N
(cid:88)
s(t )= y ·d(l (x )),
i j i j
j=1
where d(l (...)) denotes the depth of the sample x in tree t .
i j i
Usingthesescores,weadapttheforesttothelabeleddatabydiscardingtrees
with lower scores and retaining those with higher ones. This iterative process
involvesbuildinganinitialsetoftrees,filteringoutacertainpercentageofthem
based on their scores (e.g., discarding 90%), and subsequently rebuilding the
forest with the remaining trees. This procedure can be repeated multiple times
for further refinement.
Thescoringmechanismisdesignedtoprioritizetreesthataccuratelycapture
regular points deep within the forest, while potentially isolating anomalies at
shallowerdepths.Followingtheacquisitionofnewdata,thelearningprocesscan
be reiterated to incorporate the updated information, thus continually adapting
to evolving data distributions.
3 coniferest package
Theconiferestpackageisanopensourcesoftware,andwefollowallbestprac-
ticescurrentlyadoptedfordevelopingsuchtools.Thesourcecodesareavailable
atGitHub:https://github.com/snad-space/coniferestaccordingtheterms
of MIT license.
The package is mainly written in Python, since this language is de-facto
standard within contemporary academic machine learning communities. How-
ever,theperformance-criticalparts,suchastreesearchalgorithms,requiredthe
16 Korolev et al., in prep.8 M. V. Kornilov et al.
use of native binary code. We choose to use Cython to write the performance-
criticalcode,whichislattercompiledtonativebinarycode.Cythonisafamous
solutionforsuchcases,forinstancescikit-learnisheavilyrelatedonCython.
It is also popular in the machine learning community, which simplifies possible
contributions from the community. Using Cython allows us to support parallel
andfastIFinference,whichisrequiredbytheAADalgorithm.Inalternativeto
Cython, performance-critical code could be written in C++ or Rust.
UsingnativecompiledbinarycodeproducedbyCython,C++,orRustcom-
plicates the package installation, particularly for non-experienced users. In or-
der to circumvent this issue, we provide pre-built python packages in the Wheel
format, distributed via the PYthon Package index, for all major platforms, in-
cludingWindows(both32-bitand64-bitarchitectures),MacOS(bothInteland
ARM processors), and Linux (x86_64 and arm64 are supported). It means that
pip install coniferesteasilydoesthejobformostusers.Thepre-builtpack-
agesarecompiledintheGitHubActionsenvironmentautomatically,whichsim-
plifies package preparation and makes it more reproducible, reliable and secure
for the end user. GitHub Actions are also used to build the code and run unit
testsondifferentPythonversionsandoperationsystems.Employingunittesting
helps finding regressions and dramatically improves overall code quality.
Thereareonlyafewdependenciesforconiferest.Obviously,numpyisonthe
list, and we also currently re-use scikit-learn for building trees. The latter is
atemporarilysolution,becausecurrentlywehavetouseinternalscikit-learn
interface which is subject to change without notice. Using internal interfaces
of other libraries dramatically complicates development and distribution of the
package, since it requires carefully handling dependencies versions. Addition-
ally, we would like to support parallel (yet reproducible) tree building in future
versions of our package, which may come in hand for Pineforest algorithm.
Weemphasizethat,whileIFalgorithmisimplementedwithinscikit-learn
package,wehadtore-implementittoenhanceitsperformance.Whiletrainingis
typically the most time-consuming aspect of most machine learning algorithms,
forIFandactivelearningstrategiesbaseduponit,scoringisthemoredemanding
process.Thisisbecausescoring,whichisessentialforanomalydetection,requires
the use of the entire dataset, whereas training often only uses a small subset of
the original data.
Inordertoprovideusefuldocumentationforthepackage,weusePythondoc-
string, to describe functions and classes, and Markdown, for long read tutorials.
Both are rendered automatically by ReadTheDocs service and available online
at https://coniferest.snad.space.
Even though active anomaly detection usually assumes some active session
and the final product are filtered subsets of the input data, we also provide
support for serializing the trained models to portable ONNX format. This is
usefulforre-usingthetrainedmodelsforautomaticanomalydetectionpipelines.
Under such circumstances, the model cannot be update further, but it can be
alreadysatisfactorypre-trainedbytheexperttoproduceareasonablestreamof
data.Coniferest: a complete active anomaly detection framework 9
3.1 Models and the Session
There are three basic kinds of entities in coniferest packages: datasets, mod-
els, and sessions. The datasets are used solely for testing and demonstrating
purposes.Theyarenotrequiredinproduction,sinceeachusercananalyzetheir
own data.
The models are scikit-learn-style classes, implementing every supported
algorithm: Isolation forest (IF), Active Anomaly Discovery (AAD) and Pinefor-
est. These classes mimic the well-known scikit-learn interface with a few ex-
ceptions,becausescikit-learnhaspoorsupportforpartially-labeleddata,and
consequentlytherewasnogoodinterfacetofollow.However,forthescikit-learn
user it will not be difficult to get acquainted with the coniferest interface.
The sessions are objects to support iterative training of the models, as it
is done in active anomaly detection. The idea behind the session is to create
glue layer between the human expert and the retraining of the model. Unlike
scikit-learn model class, the Session class constructor takes data and meta-
data at the object construction time. The data are ordinary two-dimensional
arrayofthefeatures.Themetadataareone-dimensionalarrayofauxiliarydata,
onlyrequiredtohelpthehumanexpertindistinguishingtheobjectsinsidemain
dataset. In the simplest case the sample metadata are their row index, however,
a more description choice is also possible which would help identifying each
sample.
Thesessionconstructoralsorequirestheexistingmodelobjecttobecreated,
which can either be pre-trained or not. Currently, both AAD and Pineforest
models are supported.
The end user can modify the session behaviour by providing callbacks to
the object constructor. The callbacks are used to input the expert decision, to
visualize the learning process, to snapshot current model and to save results,
among other tasks. For instance, in our workflow, we supply the expert with a
SNAD viewer [11] URL, pointing to a portal containing extensive information
abouttheobjectunderconsideration.Thecodethenexpectsadecision(anomaly
or not anomaly) to be input interactively.
An alternative to callbacks could have been to use an abstract class and
inheritance to tune desired session behaviour. However, we found that it could
be difficult for a normal data scientist to grasp the object-oriented concept. So,
we decided to use the functional approach.
3.2 Supplied datasets
In order to make the package self-consistent some toy datasets are also supplied
withinthecode.Itisamoderngoodcustominthemachinelearningcommunity
to provide the dataset within the code package in order to allow the user to
start exploring the framework immediately. Currently, we provide the following
datasets.
In documentation and tutorial one can find references to an astronomical
dataset ztf_m31. It is a sky field around the M31 galaxy from Zwicky Tran-10 M. V. Kornilov et al.
sient Facility17 (ZTF) [1] survey with features extracted as described in [10].
For those who don’t like astronomy, we adopted datasets collected by [12] for
theirworkinneuralnetworkanomalydetection(dev_net_dataset).Thefollow-
ing datasets are available: donors (school projects excitement), census (high-
income person), fraud (credit card transactions), celeba (annotated celebrity
images), backdoor (backdoor attack data), campaign (phone calls marketing),
thyroid (disease detection data set). For further detailed description, please
see [12].
There is also a debugging dataset called non_anomalous_outliers, this
dataset is generated by request and consists of nominal data and a required
fraction of outliers.
4 Evaluation
4.1 Isolation forest prediction
Isolation forest prediction for 106 dataset
Intel Xeon Gold 6148 (80 threads)
sklearn 1.4.2
Apple M3 Pro
sklearn 1.3.2
sklearn 1.2.2
sklearn 1.1.3
coniferest 0.0.13
(single thread)
coniferest 0.0.13
sklearn 1.4.2
coniferest 0.0.13
(single thread)
0 50 100 150 200 250 300 350
CPU wall time, seconds
Fig.1. Performance comparison between scikit-learn (listed as "sklearn") and
coniferest. A dataset containing ≈ 106 samples, each having 2 features is consid-
ered. The dataset anomaly score evaluation has been measured for different versions
ofscikit-learn.Additionally,coniferestinsingle-threadmodeisconsidered.Note,
that scikit-learn is always single-threaded.
As we noted before, it is important to be able to quickly evaluate the model
forAADandPineforest.Unfortunately,atthetimeofthecreationofconiferest,
IFpredictionwithinscikit-learnwassingle-threadedandnotquiteoptimized,
even in single-threaded mode. While scikit-learn performance has been suffi-
cientlyimprovedlater,itisstillimpossibletomakemulti-threadingpredictions.
There is two ways to make multi-threading prediction for IF: process each tree
17 https://www.ztf.caltech.edu/Coniferest: a complete active anomaly detection framework 11
in a separate thread or process each data sample in a separate thread. We use
the latter approach. Performance comparison is shown in Figure 1.
4.2 Success cases
In[16],coniferestpackagehasbeenemployedwithinZWADpipeline[9]toper-
form anomaly detection for ZTF DR17 dataset. The human expert was happy
to find anything astrophysically exciting. For instance, a unknown binary mi-
crolensing event AT 2021uey has been found, an optical counterpart for radio
source NVSS J080730+755017, and lots of variable stars and fast transients,
such as red dwarf flares.
The same ZWAD pipeline has been used in [13]. Instead of general anomaly
detectionproblem,theproblemwasstatedassemi-supervisedclassificationone.
Thisallowedustodiscover104supernovae(including57previouslyunreported)
in ZTF DR3 dataset. The human expert was serching only for supernovae, and
answered to the active anomaly algorithm accordingly. About 2000 candidates
wereinspected,andlabeled,leadingtothediscoveryof100supernovae.Similarly,
red dwarf flares were of interest in [17]. ZTF DR8 dataset was used to find red
dwarf flare events and about 130 flares were found within 1200 candidates.
Note that instead of inspecting millions of light curves in ZTF dataset only
tiny subsets were scrutinized while looking for supernovae and red dwarf flares.
From astrophysical intuition we understand that it is unlikely to one would find
100supernovaewithinrandom2000samplessubsetofZTFdataset,otherwiseit
wouldmeanthattheentireGalaxyisconsistedofsupernovaewhichisobviously
not true.
5 Conclusion
Weintroducedtheconiferestpackage,amulti-algorithmandopen-sourcesoft-
ware designed for anomaly detection, developed by the SNAD team. The soft-
ware, written in Python, supports a range of algorithms, including Isolation
Forest (IF) for static anomaly detection, Active Anomaly Discovery (AAD) and
Pineforest for active anomaly detection. The framework’s integration of Cython
forperformance-criticaltasksensuresefficientparallelprocessing,enhancingthe
speed and scalability of anomaly detection tasks. Coniferest’s design aligns with
modern machine learning practices, offering a user-friendly interface similar to
scikit-learn,andsupportstheserializationoftrainedmodelsinONNXformat
for seamless integration into automated pipelines.
The successful application of coniferest algorithms for the Zwicky Tran-
sientFacilitydatahighlightsitseffectivenessinidentifyingastrophysicallysignif-
icant events such as binary microlensing, optical counterparts for radio sources,
rare variable stars, and supernovae. This framework not only simplifies the im-
plementationofanomalydetectionalgorithms,butalsobridgesthegapbetween
data scientists and domain experts, fostering more effective and efficient iden-
tification of anomalies in large datasets. As we continue to develop and refine12 M. V. Kornilov et al.
coniferest, we anticipate its broader adoption across different fields, facilitat-
ing more accurate and timely detection of anomalies in diverse datasets, which
will be particularly beneficial for the V. Rubin Observatory Legacy Survey of
Space and Time18.
Acknowledgments. M. Kornilov, A. Lavrukhina, A. Volnova and T. Semenikhin
acknowledges support from a Russian Science Foundation grant 24-22-00233, https:
//rscf.ru/en/project/24-22-00233/. Support was provided by Schmidt Sciences,
LLC. for K. Malanchev.
Disclosure of Interests. The authors declare no conflicts of interest.
References
1. Bellm, E.C., Kulkarni, S.R., Graham, M.J., Dekany, R., Smith, R.M., Riddle, R.,
Masci, F.J., Helou, G., Prince, T.A., Adams, S.M., Barbarino, C., Barlow, T.,
Bauer,J.,Beck,R.,Belicki,J.,Biswas,R.,Blagorodnova,N.,Bodewits,D.,Bolin,
B., Brinnel, V., Brooke, T., Bue, B., Bulla, M., Burruss, R., Cenko, S.B., Chang,
C.K.,Connolly,A.,Coughlin,M.,Cromer,J.,Cunningham,V.,De,K.,Delacroix,
A.,Desai,V.,Duev,D.A.,Eadie,G.,Farnham,T.L.,Feeney,M.,Feindt,U.,Flynn,
D., Franckowiak, A., Frederick, S., Fremling, C., Gal-Yam, A., Gezari, S., Giomi,
M., Goldstein, D.A., Golkhou, V.Z., Goobar, A., Groom, S., Hacopians, E., Hale,
D., Henning, J., Ho, A.Y.Q., Hover, D., Howell, J., Hung, T., Huppenkothen, D.,
Imel, D., Ip, W.H., Ivezić, Ž., Jackson, E., Jones, L., Juric, M., Kasliwal, M.M.,
Kaspi,S.,Kaye,S.,Kelley,M.S.P.,Kowalski,M.,Kramer,E.,Kupfer,T.,Landry,
W., Laher, R.R., Lee, C.D., Lin, H.W., Lin, Z.Y., Lunnan, R., Giomi, M., Maha-
bal,A.,Mao,P.,Miller,A.A.,Monkewitz,S.,Murphy,P.,Ngeow,C.C.,Nordin,J.,
Nugent, P., Ofek, E., Patterson, M.T., Penprase, B., Porter, M., Rauch, L., Reb-
bapragada, U., Reiley, D., Rigault, M., Rodriguez, H., van Roestel, J., Rusholme,
B.,vanSanten,J.,Schulze,S.,Shupe,D.L.,Singer,L.P.,Soumagnac,M.T.,Stein,
R., Surace, J., Sollerman, J., Szkody, P., Taddia, F., Terek, S., Van Sistine, A.,
vanVelzen,S.,Vestrand,W.T.,Walters,R.,Ward,C.,Ye,Q.Z.,Yu,P.C.,Yan,L.,
Zolkower, J.: The Zwicky Transient Facility: System Overview, Performance, and
First Results. Publications of the Astronomical Society of the Pacific 131(995),
018002 (Jan 2019). https://doi.org/10.1088/1538-3873/aaecbe
2. Das, S., Islam, M.R., Jayakodi, N.K., Doppa, J.R.: Effectiveness of tree-based en-
semblesforanomalydiscovery:Insights,batchandstreamingactivelearning(2024)
3. Das, S., Rakibul Islam, M., Kannappan Jayakodi, N., Rao Doppa, J.: Active
Anomaly Detection via Ensembles. arXiv e-prints arXiv:1809.06477 (Sep 2018)
4. Das, S., Wong, W.K., Fern, A., Dietterich, T.G., Amran Siddiqui, M.: Incorpo-
rating Feedback into Tree-based Anomaly Detection. In: Workshop on Interactive
Data Exploration and Analytics (IDEA’17). p. arXiv:1708.09441. KDD workshop
(Aug 2017)
5. Gould, N.I.M., Lucidi, S., Roma, M., Toint, P.L.: Solving the trust-region sub-
problem using the lanczos method. SIAM Journal on Optimization 9(2), 504–525
(1999). https://doi.org/10.1137/S1052623497322735
18 https://www.lsst.org/Coniferest: a complete active anomaly detection framework 13
6. Liu, F.T., Ting, K.M., Zhou, Z.H.: Isolation forest. In: 2008 Eighth IEEE Inter-
national Conference on Data Mining. pp. 413–422 (2008). https://doi.org/10.
1109/ICDM.2008.17
7. Liu,F.T.,Ting,K.M.,Zhou,Z.H.:Isolation-basedanomalydetection.ACMTrans.
Knowl. Discov. Data 6(1) (Mar 2012). https://doi.org/10.1145/2133360.
2133363, https://doi.org/10.1145/2133360.2133363
8. Lochner, M., Bassett, B.A.: ASTRONOMALY: Personalised active anomaly de-
tection in astronomical data. Astronomy and Computing 36, 100481 (Jul 2021).
https://doi.org/10.1016/j.ascom.2021.100481
9. Malanchev, K.L., Pruzhinskaya, M.V., Korolev, V.S., Aleo, P.D., Kornilov, M.V.,
Ishida, E.E.O., Krushinsky, V.V., Mondon, F., Sreejith, S., Volnova, A.A., Belin-
ski, A.A., Dodin, A.V., Tatarnikov, A.M., Zheltoukhov, S.G.: ZWAD: Anomaly
detection pipeline. Astrophysics Source Code Library, record ascl:2106.033 (Jun
2021)
10. Malanchev, K.L., Pruzhinskaya, M.V., Korolev, V.S., Aleo, P.D., Kornilov, M.V.,
Ishida, E.E.O., Krushinsky, V.V., Mondon, F., Sreejith, S., Volnova, A.A., Be-
linski, A.A., Dodin, A.V., Tatarnikov, A.M., Zheltoukhov, S.G., (The SNAD
Team): Anomaly detection in the Zwicky Transient Facility DR3. Monthly No-
tices of the Royal Astronomical Society 502(4), 5147–5175 (Apr 2021). https:
//doi.org/10.1093/mnras/stab316
11. Malanchev, K., Kornilov, M.V., Pruzhinskaya, M.V., Ishida, E.E.O., Aleo, P.D.,
Korolev, V.S., Lavrukhina, A., Russeil, E., Sreejith, S., Volnova, A.A., Voloshina,
A., Krone-Martins, A.: The snad viewer: Everything you want to know about
your favorite ztf object. Publications of the Astronomical Society of the Pacific
135(1044), 024503 (Feb 2023). https://doi.org/10.1088/1538-3873/acb292,
https://dx.doi.org/10.1088/1538-3873/acb292
12. Pang, G., Shen, C., van den Hengel, A.: Deep anomaly detection with deviation
networks. In: Proceedings of the 25th ACM SIGKDD international conference on
knowledge discovery & data mining. pp. 353–362 (2019)
13. Pruzhinskaya, M. V., Ishida, E. E. O., Novinskaya, A. K., Russeil, E., Volnova,
A. A., Malanchev, K. L., Kornilov, M. V., Aleo, P. D., Korolev, V. S., Krushin-
sky, V. V., Sreejith, S., Gangler, E.: Supernova search with active learning in ztf
dr3.AstronomyandAstrophysics672, A111(2023).https://doi.org/10.1051/
0004-6361/202245172, https://doi.org/10.1051/0004-6361/202245172
14. Settles, B.: Active Learning. Morgan & Claypool Publishers (2012)
15. Tolstoy, L., Dole, N., Smith, E.: Anna Karenina. No. v. 1-3 in Anna Karen-
ina, T. Crowell & Company (1899), https://books.google.com.br/books?id=
4tMpAAAAYAAJ
16. Volnova, A., Aleo, P.D., Gangler, E., Ishida, E.E.O., Kornilov, M., Korolev, V.,
Krushinsky, V., Lavrukhina, A., Malanchev, K., Pruzhinskaya, M., Russeil, E.,
Semenikhin, T., Sreejith, S., Team), T.S.: The most interesting anomalies discov-
eredinztfdr17fromthesnad-viworkshop.ResearchNotesoftheAAS7(7), 155
(jul 2023). https://doi.org/10.3847/2515-5172/ace9dd, https://dx.doi.org/
10.3847/2515-5172/ace9dd
17. Voloshina, A.S., Lavrukhina, A.D., Pruzhinskaya, M.V., Malanchev, K.L., Ishida,
E.E.O.,Krushinsky,V.V.,Aleo,P.D.,Gangler,E.,Kornilov,M.V.,Korolev,V.S.,
Russeil, E., Semenikhin, T.A., Sreejith, S., Volnova, A.A.: SNAD catalogue of M-
dwarf flares from the Zwicky Transient Facility. Monthly Notices of the Royal
Astronomical Society (Aug 2024). https://doi.org/10.1093/mnras/stae2031