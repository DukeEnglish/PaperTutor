Understanding Transfer Learning via Mean-field Analysis
Gholamali Aminian1 Łukasz Szpruch13 Samuel N. Cohen12
October 24, 2024
Abstract
We propose a novel framework for exploring generalization errors of transfer learning through
the lens of differential calculus on the space of probability measures. In particular, we consider
two main transfer learning scenarios, α-ERM and fine-tuning with the KL-regularized empirical
risk minimization and establish generic conditions under which the generalization error and the
population risk convergence rates for these scenarios are studied. Based on our theoretical results,
we show the benefits of transfer learning with a one-hidden-layer neural network in the mean-field
regime under some suitable integrability and regularity assumptions on the loss and activation
functions.
1
The Alan Turing Institute.
2
Mathematical Institute, University of Oxford.
3
School of Mathematics, University of Edinburgh.
1
4202
tcO
32
]LM.tats[
2v82171.0142:viXraContents
Contents 2
1 Introduction 3
2 Preliminaries 3
2.1 Problem formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.1.1 Loss function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.1.2 Risks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.2 Related works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
3 Generalization Errors Via Functional Derivatives 7
4 KL-regularized Risk Minimization 8
4.1 α-ERM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
4.2 Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
4.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
5 Application 13
6 Conclusions and Future Works 14
References 14
A Other related works 17
B Technical Preliminaries 18
C Proofs and details from Section 3 21
D Proofs and details from Section 4 24
D.1 Technical Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
D.2 α-ERM details and proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
D.3 Fine-tuning details and proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
E Proofs and details from Section 5 40
F More Discussion 45
F.1 Complexity Terms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
F.2 Non-similar Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
F.3 Other Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
21 Introduction
In supervised learning, a common assumption posits that both training and test datasets originate from
the same data-generating distribution. This assumption often fails in real-world scenarios; for example,
while a large amount of data is often available from the source task, we may aim to deploy a model –
trainedonthis sourcedataanda littledatafromthe target task–on adisparatetargettask. To address
this discrepancy, methodologies such as transfer learning and domain adaptation3 have been formulated.
Recent advances in transfer learning, particularly algorithms leveraging pre-trained models followed by
fine-tuning, or mixing the source and target tasks datasets, have achieved noteworthy progress across
many domains including computer vision, natural language processing and large language models (Ding
et al., 2023; Li et al., 2012; Long et al., 2015; Yosinski et al., 2014). However, our understanding of
transfer learning within neural networks remains limited.
A crucial problem in transfer learning theory is understanding the performance of a learning algorithm
– trained on both source and the target tasks datasets – on the target task unseen data. This is
described by the transfer generalization error, which quantifies the deviation between the algorithm’s
performance on the training data from the target task and its performance on unseen data sampled
from the target task distribution. In the under-parameterized regime, where the number of model
parameters is significantly less than the number of training data points, different approaches have been
applied to study the theory of the transfer generalization error (Hanneke and Kpotufe, 2019; Kalan and
Fabian, 2020; Tripuraneni et al., 2020). However, in the overparameterized regime, where the number of
parameters may greatly exceed the number of training data points in both source and target tasks, these
approaches become inadequate. Mathematical models, such as the Neural Tangent Kernel (Jacot et al.,
2018), Mean-Field (Mei et al., 2018), and Random Feature models (Rahimi and Recht, 2008), have
been proposed to understand the behavior of overparameterized neural networks (NNs) in supervised
learning. However, our current understanding of generalization in these contexts for transfer learning
still needs to be completed.
Our approach is motivated by (Aminian et al., 2023) where a novel framework via differential calculus
over measure space is proposed to study the supervised learning algorithms in a mean-field regime.
The picture that emerges from the mean-field regime is that the aim of a learning algorithm is to
find an optimal distribution over the parameter space (rather than optimal values of the parameters).
Our work explores how this mean-field view can illuminate the transfer generalization performance of
overparameterized neural networks in transfer learning contexts, specifically addressing α-ERM and
fine-tuning techniques.
The contributions of this work are as follows:
• We utilize calculus on the space of measures to derive the generalization error of α-ERM and
fine-tuning transfer learning scenarios.
• TheKullback–Leibler(KL)regularizedempiricalriskminimization,inbothtransferlearningscenarios,
is studied and convergence rates on generalization error and population risk are provided.
• For one-hidden-layer neural network in the mean-field regime, our analysis reveals precise conditions
that guarantee the desired convergence rate.
2 Preliminaries
Notation: We adopt the following convention for random variables and their distributions. A random
variable is denoted by an upper-case letter (e.g., Z), its space of possible values is denoted with the
corresponding calligraphic letter (e.g. Z), and an arbitrary value of this variable is denoted with the
lower-case letter (e.g., z). We will write E [·] for the expectation taken over Z, all other random
Z
3
In domain adaptation, we do not have access to the target task dataset and the model is trained solely on the source
task dataset.
3variables being left constant3. We will further write E [·] for the expectation over a random variable
Z∼m
Z independent of all others, which is distributed according to m, and similarly V [·] for the variance.
Z∼m
We write δ for a Dirac measure supported at z.
z
If A is a normed space, then P(A) denotes the space of probability measures on A and P (A) the
p
probability measures with finite p-th moment. We equip spaces of probability measures with the
weak topology and its associated Borel σ-algebra. For the spaces under consideration (including finite
products thereof), we assume a priori defined metrics. Furthermore, we posit that the metric on a
product space is equivalent to the product (Euclidean) metric.
We now introduce the functional linear derivative (Cardaliaguet et al., 2019) for functionals on measure
spaces. For simplicity3, we will restrict our attention to the finite variance (P (Rn) case).
2
Definition 1. Extending (Carmona and Delarue, 2018, Definition 5.43) Consider U : P (Rn)×Rk → R.
2
We say m (cid:55)→ U(m,x) is of class C1 if there exists a map δU : P (Rn)×Rk ×Rn → R, such that
δm 2
(i) δU is measurable with respect to x,a, and continuous with respect to m;
δm
(ii) for every bounded set B ⊂ P (Rn)×Rk, there exists a constant C > 0 such that |δU(m,x,a)| ≤
2 δm
C(1+|a|2) for all (m,x) ∈ B;
(iii) for all m,m′ ∈ P (Rn),
2
U(m′,x)−U(m,x)
(cid:90) 1(cid:90)
δU
= (m+λ(m′−m),x,a)(m′−m)(da)dλ.
δm
0 Rn
Since δU is only defined up to a constant we demand (cid:82) δU(m,x,a)m(da) = 0. By extension, we say U
δm δm
is of class C2 if both U and δU are of class C1.
δm
Similarly, derivativesformeasure-valuedfunctionalscanbedefined(Aminianetal.,2023, Definition2.2).
2.1 Problem formulation
Consider a transfer learning scenario, with the same input space X and target space Y for both source
and the target tasks. Define Z := X ×Y. We are often interested in learning a function f : X → Y
parameterized using a (large) number of parameters from Θ ⊆ Rk d, for some k
d
> 0.
Let νt ∈ P (Z) be the (unknown) distribution, describing the joint values of (x,y) in the population
pop 2
of the target task. Similarly, we define νs ∈ P (Z) for the source task. Intuitively, we suppose
pop 2
that the mass of νt is near the graph of f that is, yt ≈ f(xt) when zt = (xt,yt) is sampled from
pop
νt . Since νt is unknown, we approximate f based on a finite dataset from both target and the
pop pop
source tasks, Zt = {Zt}nt and Zs = {Zs}ns . We make the following basic assumption on our
nt i i=1 ns i i=1
data throughout the paper, that our observations from the target tasks are given by Zt = {Zt}nt ,
where Zt = (Xt,Yt) ∼ νt are i.i.d. We write νt := 1 (cid:80)nt δ for the correspon nt ding ti ari g= e1 t
empiricai
l
measui re.i Similap rlo yp
, we define Zs =
{Zs}n nt
s ,
whntere i Z= s1 =Z it
(Xs,Ys) ∼ νs are i.i.d. and
ns i i=1 i i i pop
ν ns
s
:= n1
s
(cid:80)n i=s 1δ
Z
is.
We are interested in quantifying the performance of our model on unseen data from the target task. To
this end, let Z(cid:101)t = {Z(cid:101)t }nt be a second i.i.d sample set with law νt , independent of Zt . We define
nt i i=1 pop nt
the perturbations of ν obtained by ‘resampling’ one or two data points, by
nt
1
νt = νt + (δ −δ ). (1)
nt,(1) nt n
t
Z(cid:101)t
1
Z 1t
3
Formally, this corresponds to the conditional expectation over all variables in our setting excluding Z. As we will
only have countably many variables in our problem, this does not cause any technical difficulties.
3
This can be relaxed, along with most integrability assumptions in this paper, at a cost of complexity.
4Transfer Learning algorithms: In a mean-field approach, we represent transfer learning as a map
from the observed target and source empirical measures to a measure over parameters, (νt ,νs ) (cid:55)→
nt ns
m(νt ,νs ) ∈ P(Θ). We motivate this representation below. In this work, we study two well-known
nt ns
transfer learning algorithms, α-ERM and fine-tuning in a mean-field regime. We will specify how this
map is chosen in the next section.
α-ERM: Inspired by (Ben-David et al., 2010), we train the model based on a convex combination of
empirical risks of source and the target tasks,
αR(m,νt )+(1−α)R(m,νs ), α ∈ [0,1].
nt ns
The α-ERM transfer learning algorithm is represented as a (measurable) map from the convex com-
bination ν
α
:= αν nt
t
+(1−α)ν ns
s
of empirical measures to a measure over parameters θ ∈ Θ ⊂ Rk d,
ν (cid:55)→ m (ν ). Therefore, for α-ERM we have
α α α
m(νt ,νs ) = m (ν ).
nt ns α α
Fine-tuning: Inspired by (Tripuraneni et al., 2020) in fine-tuning algorithm, we consider two sets of
parameters for training, including common parameters θ
c
∈ Θ
c
⊂ Rk dc, and specific parameters for
source and the target tasks, θ ss
p
∈ Θ
sp
⊂ Rk dsp and θ st
p
∈ Θ
sp
⊂ Rk dsp, respectively. Conceptually, we
first train (a measure over) θs and θ based on the source task’s dataset. Then, we fine-tune the specific
sp c
parameters, θt , based on the target task, while leaving the common parameters fixed. Formally, for
sp
the target task, this gives us a parameter measure of the form,
m(νt ,νs ) = m (νs )mt (m (νs ),νt ), (2)
nt ns c ns sp c ns nt
where m : P(Z) (cid:55)→ P(Θ ) and mt : P(Θ )×P(Z) (cid:55)→ P(Θ ) represent the measures over the common
c c sp c sp
parameters induced by training on the source dataset and target-specific parameters induced by training
on target dataset and fixing the common parameter measure, respectively.
Note that the model parameter in fine-tuning is the union of common and target specific parameters in
fine-tuning, i.e. θ = θ ∪θt .
c sp
To investigate resampling the training data, we extend (1) and make use of the convex perturbations,
for λ ∈ [0,1],
m (λ) = m(νt ,νs )
(1) nt,(1) ns
+λ(m(νt ,νs )−m(νt ,νs )), (3)
nt ns nt,(1) ns
νt (λ) = νt +λ(νt −νt ).
(1) nt,(1) nt nt,(1)
2.1.1 Loss function
We focus on training methods where m is the minimizer of a loss function. We generically represent the
single-observation loss function as
(m,z) (cid:55)→ ℓ(m,z) ∈ R+. (4)
This represents the loss, for the parameter distribution m and a data point z. For the fine-tuning
scenario, given the structure shown in (2), we consider the map (m m ,z) (cid:55)→ ℓ(m m ,z) ∈ R+ where
c sp c sp
m and m are the common and specific parameter distributions.
c sp
In order to motivate our generic formulation of this problem, we present the concrete example of
overparameterized one-hidden-layer neural in mean-field regime.
Overparameterized one-hidden-layer neural network (NN): We consider a one-hidden-layer NN.
Let x ∈ Rq be a feature vector. For each of r hidden neurons, we denote the parameters of the hidden
layer by w ∈ Rq and the outer layer by a ∈ R. The parameter space is Θ = R(q+1) = {a ∈ R,w ∈ Rq}.
5To ease notation, we write ϕ(θ ,x) = a φ(w ·x), where φ(·) is the activation function. The output of
i i i
the NN is
r r
1 (cid:88) 1 (cid:88)
fˆ(x) = a φ(w ·x) = ϕ(θ ,x)
i i i
r r
i=1 i=1 (5)
(cid:90)
= ϕ(θ,x)m(dθ) = E [ϕ(θ,x)],
r θ∼mr
where m := 1 (cid:80)r δ . Observe that any hidden nodes can be exchanged (along with their
r r i=1 (ai,wi)
parameters) without changing the output of the network; this symmetry implies that it is the (joint)
distributionm whichisimportant. Asrincreases,m canconverge(weakly)toacontinuousdistribution
r r
overtheparameterspace; theweightsforindividualneuronscanbeviewedassamplesfromthismeasure.
This is the mean-field model studied in (Hu et al., 2020; Mei et al., 2018, 2019).
Training neural networks chooses parameters θ to minimize a loss ℓ between fˆ(x) and the observed
o
value y. With yˆ:= fˆ(x) = E [ϕ(θ,x)], we can write our single-observation loss function as
θ∼mr
ℓ(m,z) := ℓ (cid:0)E [ϕ(θ,x)],y(cid:1) . (6)
o θ∼m
For example, consider ℓ (cid:0) yˆ,y(cid:1) = c(yyˆ) for binary classification, where c(·) is a margin-based loss
o
function (Bartlett et al., 2006), or absolute loss ℓ (yˆ,y) = |y−yˆ| or quadratic loss ℓ (yˆ,y) = (y−yˆ)2
o o
for classification and regression problems. As we can observe, for overparameterized one-hidden-layer
NN, as in (6) the loss function is non-linear in m.
2.1.2 Risks
The risk function3 takes distributions m of parameters and ν of data points and evaluates the loss:
(cid:90)
R(m,ν) := ℓ(cid:0) m,z(cid:1) ν(dz). (7)
Z
The population risk for the target task is R(m,νt ), and the empirical risk for the source and the
pop
target tasks are R(m,νt ) and R(m,νs ), respectively, as
nt ns
R(m,νt ) =
(cid:90)
ℓ(cid:0) m,z(cid:1) νt (dz) = 1
(cid:88)nt
ℓ(cid:0) m,zt(cid:1) ,
nt nt n i
Z t
i=1
and similarly for νs .
ns
Weak Transfer Generalization error: We aim to study the performance of the model trained
with the empirical data measures (νt ,νs ), and evaluated against the population measure νt , that is,
nt ns pop
R(m(νt ,νs ),νt ). The risk can be decomposed:
nt ns pop
R(m(νt ,νs ),νt )
nt ns pop
(cid:16) (cid:17)
= R(m(νt ,νs ),νt )−R(m(νt ,νs ),νt )
nt ns pop nt ns nt
(cid:124) (cid:123)(cid:122) (cid:125) (8)
transfergeneralizationerror
+ R(m(νt ,νs ),νt ) .
nt ns nt
(cid:124) (cid:123)(cid:122) (cid:125)
trainingerrorofthetargettask
Assumption 1. The training map m, loss function ℓ, and population measure ν are such that the
training error satisfies E [R(m(νt ),νt )] = E [(cid:82) ℓ(m(νt ),zt)νt (dzt)] < ∞. The same assumption
Zt n nt nt Zt n Z nt nt
also holds for source task.
3
The functional R is (weakly) continuous in ν, and is measurable in m (as a consequence of the measurability of ℓ).
As ℓ(m,z) is nonnegative, the integral (7) can always be defined, without integrability assumptions on ℓ, but may take
the value +∞.
6For a training map m, we will consider the Weak Transfer Generalization Error (WTGE),
gen(m,νt ) ≜ E (cid:2) R(m,νt )−R(m,νt )(cid:3) . (9)
pop Zt nt,Zs
ns
pop nt
Weak Transfer Excess Risk: In addition, we also define the Weak Transfer Excess Risk (WTER)
for a training map as,
E(m,νt )
pop
(10)
≜ E (cid:2) R(m,νt )]− inf R(m,νt ).
Zt ,Zs pop pop
nt ns m∈P(Θ)
Remark 1. We will use both the WTGE and WTER in what follows. In the context of transfer learning,
these two quantities have slightly different interpretations: WTGE measures the average overfitting of
our model relative to our training data, while WTER measures the average overfitting relative to a
perfect model for the target problem. Ultimately, our goal is to control the excess risk (WTER); we will
do this by proving bounds on the WTGE as an intermediate step.
2.2 Related works
Generalization error and transfer learning: Various works have tried to understand the empirical
performance of transfer learning and domain adaptation. The first theoretical study addressing domain
adaptation was presented by (Ben-David et al., 2007), focusing on binary classification. This work
establishes an excess risk bound for the zero-one loss using the VC-dimension, grounding it in the
d -distance—a metric quantifying the disparity between source and the target tasks. Further deepening
A
the exploration, (Hanneke and Kpotufe, 2019) introduced a novel metric for assessing discrepancy in
transfer learning termed the transfer-exponent, operating under the covariate-shift and a Bernstein
class condition. (Kalan and Fabian, 2020) formulated a minimax lower bound on the generalization
error associated with transfer learning in the neural network. A more recent development is the
introduction of an Empirical Risk Minimization (ERM) strategy motivated by representation learning
by (Tripuraneni et al., 2020), which offers an upper boundary on the excess risk for the new task via
Gaussian complexity analysis. Advancing the discourse, (Wang et al., 2019) delineated an upper bound
on excess risk leveraging importance weighting. Lastly, (Wu et al., 2020) proposed an information-
theoretic generalization error upper bound for transfer learning, employing the KL divergence as a
metric for the similarity between source and target data distributions. Our work differs from these, as
our focus is on over-parameterized models for transfer learning in a mean-field regime
3 Generalization Errors Via Functional Derivatives
In this section, we apply functional calculus to study transfer generalization error (WTGE); these
calculations will also enable us to prove bounds on the excess risk (WTER). Our goal is to give abstract
conditions under which bounds can be established, which can be applied in a range of contexts, as
will be explored in subsequent sections. We begin this section by providing a general representation of
generalization error, inspired by the approach of (Bousquet and Elisseeff, 2002, Lemma 7) for transfer
learning. All proofs are deferred to Appendix C.
Lemma 1. Consider a generic loss function (m,z) (cid:55)→ ℓ(m,z), and (νt ,νs ) as defined in (1). The
nt,(1) ns
WTGE (9) is given by,
gen(m(νt ,νs ),νt ) = (11)
nt ns pop
E
Zt nt,Zs
ns,Z(cid:101)t
1(cid:104) ℓ(cid:0) m(ν nt t,ν ns s),Z(cid:101)t 1(cid:1) −ℓ(cid:0) m(ν nt t,(1),ν ns s),Z(cid:101)t 1(cid:1)(cid:105) .
The right hand side of (11) measures the expected change in the loss function when resampling one
training data point from the target task, which connects weak transfer generalization error to stability
of m with respect to both target training dataset. We will next quantify this stability precisely in
transfer learning, in terms of functional derivatives.
7Assumption 2. In addition to Assumption 1,
(i) The loss function ℓ is C1 (cf. Definition 1), nonnegative, and convex with respect to m;
(ii) When restricted to νt,νs ∈ P (Z), the training map (νt,νs) (cid:55)→ m(νt,νs) is C1, in the sense of
2
(Aminian et al., 2023, Definition 2.2).
Theorem 1. Given Assumption 2, the weak transfer generalization error has the representation
gen(m,νt )
pop
= E
Zt nt,Zs
ns,Z(cid:101)t
1(cid:104)(cid:90) 01(cid:90) Θ(cid:16) δmδℓ sp(cid:0) m (1)(λ),Z(cid:101)t 1,θ(cid:1)(cid:17)
(cid:105)
(cid:0) m(νt ,νs )−m(νt ,νs )(cid:1) (dθ)dλ .
nt ns nt,(1) ns
By applying the functional linear derivative to the term (cid:0) m(νt ,νs )−m(νt ,νs )(cid:1) in Theorem 1,
we can provide yet another representation of the WTGE for
α-n Et RMns
and
finn et-, t( u1) ninn gs
scenarios.
Theorem 2 (α-ERM). The WTGE of α-ERM can be written
gen(m,ν pt op) = nα tE
Zt nt,Zs
ns,Z(cid:101)t
1(cid:2) h(Zt nt,Zs ns,Z(cid:101)t 1)(cid:3) , (12)
where
h(Zt nt,Zs ns,Z(cid:101)t 1)=(cid:82) 01(cid:82) 01(cid:82) ZA(λ,λ˜)(cid:0) δ
Z 1t
−δ
Z˜
1t(cid:1) (dz)dλ˜dλ, A(λ,λ˜)=(cid:82)
Θ
δmδℓ sp(cid:0) m (1)(λ),Z(cid:101)t 1,θ(cid:1)δ δm ν(cid:0) ν α,(1)(λ˜),z(cid:1) (dθ),
and ν (λ˜) = (ανt +(1−α)νs )+λ˜α(νt −νt ).
α,(1) nt,(1) ns nt nt,(1)
Theorem 3 (Fine-tuning). The WTGE of fine-tuning can be written,
gen(m,ν pt op) = n1 tE
Zt nt,Zs
ns,Z(cid:101)t
1(cid:2) K(Zt nt,Zs ns,Z(cid:101)t 1)(cid:3) , (13)
where
K(Zt ,Zs ,Z(cid:101)t )
nt ns 1
= (cid:90) 1(cid:90) (cid:16) δℓ (cid:0) mf (λ),Z(cid:101)t ,θt (cid:1)(cid:17)
δm (1) 1 sp
0 Θsp sp
(cid:18)(cid:90) 1(cid:90) δm
× sp (m (νs ),νt (λ ),z)(dθt )
δν c ns nt,(1) 1 sp
0 Z
(cid:19)
(δ −δ )(dz)dλ dλ,
Z 1t Z(cid:101)t
1
1
and mf (λ) = m (νs )(cid:2) (1−λ)(mt (m (νs ),νt )+λmt (m (νs ),νt )(cid:3) .
(1) c ns sp c ns nt sp c ns nt,(1)
Remark 2 (Convergence rate). The representation in (12) reveals that the convergence rates of the
WTGE based on α-ERM is at worst O( nt+1 ns) provided that E
Zt nt,Zs
ns,Z(cid:101)t
1(cid:2) h(Zt nt,Zs ns,Z(cid:101)t 1)(cid:3) ≤ O(1) with
respect to n and α = nt . Furthermore, for fine-tuning scenario, the representation in (13) reveals
t nt+ns
that the convergence rate is at worst O( n1 t) provided that E
Zt nt,Zs
ns,Z(cid:101)t
1(cid:2) K(Zt nt,Zs ns,Z(cid:101)t 1)(cid:3) ≤ O(1) with
respect to n .
t
4 KL-regularized Risk Minimization
In the last section, we gave general conditions under which we can establish convergence rates for weak
transfer generalization error, in terms of the functional derivatives of the loss function and learning
algorithms in different scenarios, including, α-ERM and fine-tuning. Practically, these conditions are
8only useful if one can verify that they hold in specific examples for these scenarios. In this section, we
will provide an intermediate step in this direction, where we assume the learning algorithm m is chosen
to minimize a regularized version of the empirical risk. This will provide us with criteria which can be
easily verified in practical examples; we will see this fully in Section 5.
We will frequently use m to represent the density of a measure or measure-valued functional, that is,
θ
m(ν)(dθ) = m (ν;θ)dθ. For probability distributions m and m′ with positive densities m (θ) and m′(θ)
θ θ θ
we define the Kullback–Leibler divergence KL(m′∥m) = (cid:82) log(cid:0)m′ θ(θ)(cid:1) m (θ)dθ, and KL(m′∥m) = ∞
m (θ) θ
θ
otherwise3. We will also write the Kullback–Leibler divergence in terms of the densities, with the abuse
of notation KL(m′∥m) = KL(m′∥m ) = KL(m′∥m ) as convenient. For our analysis, we define the
θ θ θ
following integral probability metric.
Definition 2. We define a general integral probability metric3 (IPM) d (ρ,η), between two measures
F
ρ,η ∈ P(X) by
(cid:12)(cid:90) (cid:90) (cid:12)
d (ρ,η) := sup(cid:12) f(x)ρ(dx)− f(x)η(dx)(cid:12),
F (cid:12) (cid:12)
f∈F X X
where F ⊂
(cid:8)
f : X (cid:55)→
R+(cid:9)
are measurable functions.
Remark 3. Defining FC1 := (cid:8) f : X (cid:55)→ R+ s.t. f(x) ≤ (1+∥x∥p),f ∈ C1(cid:9) , we have an L growth IPM
p p
(cid:12)(cid:90) (cid:90) (cid:12)
d (ρ,η) := sup (cid:12) f(x)ρ(dx)− f(x)η(dx)(cid:12).
Fp (cid:12) (cid:12)
f∈FC1 X X
p
Furthermore, considering the set of 1-Lipschitz functions, FLip := (cid:8) f : X (cid:55)→ R+ s.t. f ∈ Lip(1)(cid:9) , we
recover the Wasserstein W distance; taking indicator functions of intervals, or bounded continuous
1
functions, we obtain the total variation distance, TV.
In learning, we sometimes consider learning algorithms which minimize a regularized objective Vβ given
by the sum of a risk function R and the KL-divergence:
σ2
Vβ(m,ν) = R(m,ν)+ KL(m∥γσ), (14)
2β2
where γσ(θ) = 1 exp(cid:8) − 1 U(θ)(cid:9) is a Gibbs measure which serves as prior to the parameter measure;
here Fσ is simF pσ ly a normσ a2 lizing constant to ensure (cid:82) γσ(θ)dθ = 1, and we call U : Θ → R the
‘regularizing potential’. In (Aminian et al., 2023), it is shown that under Assumption 2, the minimizer
of problem (14) exists and is unique,
1 (cid:26) 2β2(cid:104)δR
mβ(ν) = exp − (mβ(ν),ν,θ) (15)
F σ2 δm
β
(cid:27)
1 (cid:105)
+ U(θ) ,
2β2
forF isanormalizationconstant. Furthermore, themapν (cid:55)→ mβ(ν)isC1 (Definition1). Fornotational
β
convenience, we similarly define γ˜σ = 1 exp(cid:8) − 1 U(θ)+∥θ∥8(cid:9). We observe that, unless δR does not
8 F˜σ σ2 δm
depend on m (i.e. unless R is linear in m), (15) does not provide an explicit representation of mβ(ν),
but instead describes it implicitly in terms of a fixed point.
4.1 α-ERM
In α-ERM, we aim to minimize the KL-regularized risk, Vβ(m,ν ), and denote the solution mβ(ν ).
α α
Applying Theorem 2 under the following assumption (see Appendix D.2 for the detailed formulation of
this assumption), we can derive upper bound on the WTGE of the α-ERM.
3 Extending to the case where m and m′ are equivalent but do not have (Lebesgue) densities will not be needed here.
3
This is generally a pseudometric, and is a true metric if F separates points in X.
9Assumption 3. The loss function and the functional derivative of loss function with respect to m, are
such that,
(i) For all z ∈ Z and m ∈ P (Θ), there exists L > 0 such that the loss satisfies
4 m
0 ≤ ℓ(m,z) ≤ L (1+E [∥θ∥4])(cid:0) 1+∥z∥2(cid:1) ,
m θ∼m
(ii) For all z ∈ Z and m ∈ P (Θ), there exists L > 0 such that the functional derivative of loss satisfies
4 e
(cid:12) (cid:12) δℓ (m,z,θ)(cid:12) (cid:12) ≤ L (cid:0) 1+E [∥θ∥4]+∥θ∥4(cid:1)(cid:0) 1+∥z∥2(cid:1) .
(cid:12)δm (cid:12) e θ∼m
Theorem 4 (WTGE of the α-ERM). Given Assumption 3, E (cid:2) ∥Z∥8(cid:3) < ∞ and E (cid:2) ∥Z∥4(cid:3) <
Z∼νt Z∼νs
pop pop
∞, the WTGE for the α-ERM satisfies
|gen(mβ(ν ),νt )|
α pop
(cid:34)
c α2β2 (cid:104) (cid:105)
≤ t Comp(θ) (2+α)2E (1+∥Zt∥2)4
n
t
σ2 Z 1t 1
(cid:35)
(cid:104) (cid:105) (cid:104) (cid:105)
+(1−α)2E Zt (1+∥Z 1t∥2)2 E Zs (1+∥Z 1s∥2)2 ,
1 1
√
where c = 2L2(1+αL )2(1+ 2 )2 and
t e m nt
(cid:90)
Comp(θ) = (cid:2) 1+2 ∥θ∥8γ˜σ(dθ)+2E [∥θ∥4](cid:3)2 .
8 θ∼γ˜σ
8
Θ
Remark 4. From Theorem 4, we have bounded WTGE whenever the 8th moment of the target task
and 4th moment of the source task population measures are bounded.
From Theorem 4, we observe that when α = 1, which corresponds to not considering the source training
dataset, we recover results for supervised learning on the target task.
Using Theorem 4 , we provide upper bounds on the WTER of the α-ERM transfer learning scenario.
Theorem 5 (WTER of α-ERM). Under the same assumptions as in Theorem 4, there exist constants
C , C , C and C such that the WTER under α-ERM, satisfies
s t d m
E(cid:0) mβ(ν ),νt (cid:1) ≤ C tα8β2 + C s(1−α)8β2
θ α pop n σ2 n σ2
t s
+(1−α)C d (νs ,νt )
d F2 pop pop
σ2
+C d (m¯ ,m¯t)+ KL(m¯ ∥γσ).
m Fp α 2β2 α
wherem¯ = argmin R(m,ν )andm¯t = argmin R(m,νt ),providedthatKL(m¯ ∥γσ) <
α m∈P8(Θ) α m∈P8(Θ) pop α
∞ .
Remark 5. In order to simplify the upper bound in Theorem 5, we can assume that there exists3 a true
measure m¯ such that R(m¯ ,ν ) = 0 and KL(m¯ ∥γσ) < ∞. Then
α α α α
E(cid:0) mβt(ν ),νt (cid:1) ≤ C tα8β2 + C s(1−α)8β2
θ α pop n σ2 n σ2
t s
+(1−α)C d (νs ,νt )
d F2 pop pop
σ2
+ KL(m¯ ∥γσ).
2β2 α
3
The universal approximation theorem of neural networks (Hornik et al., 1989) states that neural networks with an
arbitrary number of hidden units can approximate continuous functions on a compact set with any desired precision.
Therefore, the existence of true measure, m¯ , is reasonable in the mean-field regime. A similar assumption is made in
α
Chen et al. (2020, Theorem 4.5).
10Remark 6. Assuming a bounded loss function, the similarity metric d (νs ,νt ) can be simplified
F2 pop pop
to total variation distance TV(νs ,νt ).
pop pop
4.2 Fine-tuning
There are two different steps in fine-tuning approach. In the first step, we solve the KL-regularized risk
minimization problem on the source task,
σ2
Vβs(m,νs ) := R(m,νt )+ KL(m∥γσ), (16)
ns nt 2β2
t
where the solution is mβs(νs ) over both common and specific parameters for the source task, (θ ,θs ).
s ns c sp
In the second step, we fix the measure over the common parameters, mβs(νs ) := (cid:82) mβs(νs )(dθs ),
c ns Θsp s ns sp
and solve the following KL-regularized with respect to mt ,
sp
Vβt(mβs(νs )mt ,νt )
c ns sp nt
σ2 (17)
:= R(mβs(νs )mt ,νt )+ KL(mt ∥γσ ),
c ns sp nt 2β2 sp θsp
t
with solution mt,βt(m (νs ),νt ). For notational convenience, we define γ˜σ = 1 exp(cid:8) − 1 U(θt )+
sp c ns nt sp F˜σ,sp σ2 sp
∥θt ∥8(cid:9) and γˆσ = 1 exp(cid:8) − 1 U(θ)+∥θ ∥8(cid:9). Applying Theorem 3 under the following assumption
sp 8 F˜σ σ2 c
(see Appendix D.3 for the detailed formulation of this assumption), we can drive upper bounds on the
WTGE and WTER of fine-tuning scenario.
Assumption 4. The loss function and the functional derivative of loss function with respect to m ,
sp
are such that,
(i) For all z ∈ Z, m ∈ P (Θ ) and m ∈ P (θ ), there exists L such that the loss satisfies
c 8 c sp 8 sp m
ℓ(m m ,z) ≤ g(m m
)(cid:0) 1+∥z∥2(cid:1)
,
c sp c sp
where g(m m ) = L (1+E [∥θ ∥4]+E [∥θ ∥4]).
c sp m θc∼mc c θsp∼msp sp
(ii) For all z ∈ Z, m ∈ P (Θ ) and m ∈ P (θ ), there exists L such that the functional derivative
c 8 c sp 8 sp e
of loss function with respect to m satisfies
sp
(cid:12) (cid:12) δℓ (m m ,z,θ )(cid:12) (cid:12) ≤ g (m m ,θ )(cid:0) 1+∥z∥2(cid:1) ;
(cid:12)δm c sp sp (cid:12) sp c sp sp
sp
where g (m m ,θ ) = L (1+E [∥θ ∥4]+E [∥θ ∥4]+∥θ ∥4).
sp c sp sp e θc∼mc c θsp∼msp sp sp
Theorem 6 (WTGE of fine-tuning). Given Assumption 4, E (cid:2) ∥Z∥8(cid:3) < ∞ and E (cid:2) ∥Z∥4(cid:3) <
Z∼νt Z∼νs
pop pop
∞, the WTGE under fine-tuning satisfies
|gen(m (νs )mt,βt(m (νs ),νt ),νt )|
c ns sp c ns nt pop
2 2 16β2
≤ (1+ )2 L2(1+L )2Comp(θ ,θt ,θs )
n n σ2 e m c sp sp
t t
(cid:104) (cid:105)(cid:104) (cid:104) (cid:105)(cid:105)
×E Zs (1+∥Z 1s∥2)2 E Zt (1+∥Z 1t∥2)4 .
1 1
where
(cid:104) (cid:90)
Comp(θ ,θt ,θs ) = 1+ ∥θ ∥4(2+∥θ ∥4)γˆσ (dθ )
c sp sp c c 8,c c
Θc
(cid:90)
+ ∥θt ∥4(1+2∥θt ∥4)γ˜σ (dθt )
sp sp sp sp
Θsp
(cid:90) (cid:105)2
+ ∥θs ∥4γˆσ (dθs ) ,
sp 8,sp sp
Θsp
with γˆσ = (cid:82) γˆσ(dθs ) and γˆσ = (cid:82) γˆσ(dθ ).
8,c Θsp 8 sp 8,sp Θc 8 c
11Using Theorem 6, we can provide upper bound on the excess risk of fine-tune transfer learning scenario.
Theorem 7 (WTER of fine-tuning). Under the same assumptions in Theorem 6 and Assumption 3,
there exist constants C ,C ,C ,C and C where the following upper bound holds on WTER under
t s d2 dp k
dsp
fine-tuning scenario,
E(mβs(νs )mβt(mβs(νs ),νt ),νt ) (18)
c ns sp c ns nt pop
C β2 σ2 C β2
≤ t t + KL(m˜t ∥γ˜σ )+ s s
n σ2 2β2 sp sp n σ2
t t s
σ2
+ KL(m¯s⊗m¯s ∥γσ ⊗γσ )
2β2 c sp c sp
s
+C d (νs ,νt )+C d (m˜s ,m˜t )
d2 F2 pop pop dsp F4 sp sp
+C d (m¯s⊗m¯s ,m¯t ⊗m¯t ),
dp F4 c sp c sp
where m˜t = argmin R(mβs(νs )m ,νt ) provided KL(m˜t ∥γ˜σ ) < ∞, with minimizers
sp msp∈P8(Θsp) c ns sp pop sp sp
m˜s = argmin R(mβs(νs )m ,νs ), m¯s ⊗ m¯s = argmin R(m¯s ⊗
sp msp∈P8(Θsp) c ns sp pop c sp m¯s c⊗m¯s sp∈P8(Θc)×P8(Θsp) c
m¯s ,νs ) and m¯t ⊗m¯t = argmin R(m¯t ⊗m¯t ,νt ).
sp pop c sp m¯t c⊗m¯t sp∈P8(Θc)×P8(Θsp) c sp pop
Remark 7. To simplify the upper bound in Theorem 7, we can assume that there exist m¯s ⊗m¯s ∈
c sp
P (Θ )×P (Θ ) and m¯t ⊗m¯t ∈ P (Θ )×P (Θ ) such that R(m¯s ⊗m¯s ,νs ) = 0 and R(m¯t ⊗
8 c 8 sp c sp 8 c 8 sp c sp pop c
m¯t ,νt ) = 0, respectively. In addition, assume that there exist m˜t ∈ P (Θ ) and m˜s ∈ P (Θ )
sp pop sp 8 sp sp 8 sp
such that E [R(mβs(νs )m˜s ,νs )] = 0 and E [R(mβs(νs )m˜t ,νt )] = 0. Then, we have,
Zs ns c ns sp pop Zs ns c ns sp pop
E(mβs(νs )mβt(mβs(νs ),νt ),νt )
c ns sp c ns nt pop
C β2 σ2 C β2
≤ t t + KL(m˜t ∥γ˜σ )+ s s
n σ2 2β2 sp sp n σ2
t t s (19)
σ2
+ KL(m¯s⊗m¯s ∥γσ ⊗γσ )
2β2 c sp c sp
s
+C d (νs ,νt ).
d2 F2 pop pop
4.3 Discussion
α-ERM: As shown in Theorem 4, the WTER depends on the similarity metric d (νs ,νt ). If source
F2 pop pop
and the target tasks are similar, i.e. d (νs ,νt ) ≈ 0, then, choosing α = nt and β = (n +n )1/4
results in a convergence rate of O(1/F√2 np +op n p )o fp or the WTER of the α-ERn Ms+n stcenario. Howt evers , for
t s
non-similar source and the target tasks, i.e. d (νs ,νt ) > 0, choosing α = nt results in a rate of
F2 pop pop ns+nt
max(√ nt1 +ns, nsn +s nt), which can be worse for large n s. Therefore, for non-similar source and the target
tasks, i.e., d (νs ,νt ) > 0, by choosing α = 1−n−1 and β2 = (cid:0)α + (1−α)(cid:1)−1/2, we can get an
F2 pop pop t nt ns
overall convergence rate of O(√1 ) which is similar to supervised learning scenario.
nt
Fine-tuning: If source and the target tasks are similar, i.e. d (νs ,νt ) ≈ 0, in fine-tuning scenario,
√F2 pop pop √
we have a convergence rate O(√1
nt
+ √1 ns) by choosing β s2 = n
s
and β t2 = n t.
In Table 1, more details are provided for comparison of the supervised learning scenario where the
source task training dataset is not available, and transfer learning scenarios, including α-ERM and
fine-tuning.
Further discussion is provided in Appendix F.
Comparison to previous works: In Bu et al. (2022), the authors provide a maximum likelihood
estimation analysis of the WTER for α-ERM and fine-tuning under asymptotic assumptions, i.e.,
n → ∞ and n → ∞. However, our result holds for finite number of samples. In addition, their
s t
results focus on expected loss, where the loss function is linear in parameter measure. Furthermore,
Bu et al. (2022) lacks a clear definition of similarity between target and source tasks, making it
12Table 1: Comparison of different algorithms setting under similar tasks (d (νs ,νt ) ≈ 0).
F2 pop pop
Supervised Learning α-ERM Fine-tuning
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
GeneralizationError O 1 O 1 O 1
(cid:16)
nt
(cid:17) (cid:16)
ns+nt
(cid:17) (cid:16)
nt
(cid:17)
Excessrisk O √1 O √ 1 O √1 + √1
nt ns+nt nt ns
difficult to determine when transfer learning is beneficial. A Gaussian complexity analysis of fine-
tuning is proposed by Tripuraneni et al. (2020), which cannot be extended to the mean-field regime.
Furthermore, the convergence rate under Gaussian complexity analysis is worse than our approach. In
comparison to Hanneke and Kpotufe (2019), we can apply our method to unbounded loss functions in
the over-parameterized regime.
5 Application
We highlight the application of our model to an overparameterized one-hidden-layer neural network in
the mean-field regime (Hu et al., 2020; Mei et al., 2018) for transfer learning scenarios, α-ERM and
fine-tuning. The loss is represented in (6). For simplicity, we also write (yˆ,y) (cid:55)→ ℓ (yˆ,y) for the loss
o
function, where yˆ = E [ϕ(θ,x)] is the output of the NN, where ϕ(θ,x) = aφ(w.x) for a ∈ R and
θ∼m
w ∈ Rq.
For α-ERM scenario, we assume that the risk function R is given by
R(m (ν ),νt ) = E [ℓ (E [ϕ(θ,X)],Y)],
α α pop (X,Y)∼ν pt
op
o θ∼mβ(να)
as is done in mean-field models of one-hidden-layer neural networks (Hu et al., 2020; Mei et al., 2019;
Tzen and Raginsky, 2020), and is KL-regularized as seen previously. Similarly, for fine-tuning scenario,
we assume that the risk function in second step is given by
R(m (νs )mt (m (νs ),νt ),νt ) = E [ℓ (Y′,Y)],
c ns sp c ns nt pop (X,Y)∼ν pt
op
o
where Y′ = E [φ(wX)]E [a].
w∼mc(ν ns s) a∼mt sp(mc(ν ns s),ν nt t)
We make the following assumptions to investigate weak transfer generalization performance.
U(θ)
Assumption 5. Suppose that the regularizing potential U satisfies lim = ∞, the loss ℓ
∥θ∥→∞ ∥θ∥4 o
is convex and nonnegative, and there exist finite constants L ,L ,L , and L such that, for all
ℓ ℓ,1 ℓ,2 ϕ
(x,y) ∈ X ×Y and θ ∈ Θ,
(cid:12) (cid:12)ℓ o(yˆ,y)(cid:12) (cid:12) ≤ L ℓ(1+∥yˆ∥2+∥y∥2),
(cid:12)
|∂ yˆℓ o(yˆ,y)(cid:12) ≤ L ℓ,1(1+∥yˆ∥+∥y∥),
(20)
(cid:12) (cid:12)
(cid:12)∂ yˆyˆℓ o(yˆ,y)(cid:12) ≤ L ℓ,2,
(cid:12) (cid:12)
(cid:12)φ(w.x)(cid:12) ≤ L φ(1+∥x∥)(1+∥w∥).
As shown in Appendix E, Assumption 3 and Assumption 4 are verified for overparameterized one-hidden
layer neural network under Assumption 5. Therefore, Theorems 4, 5, 6 and 7 yield upper bounds on
WTGE and WTER under α-ERM and fine-tuning algorithms.
Activation and Loss Functions: If the activation function is of linear growth, then Assumption
5 is satisfied. Note that the ReLU, Heaviside unit-step, tanh and sigmoid activation functions are
of this type, and no smoothness assumption is needed. For loss functions, twice differentiable loss
functions which are of quadratic growth with Lipschitz derivatives satisfy Assumption 5. For example,
the quadratic, product-type (as in (Bartlett et al., 2006)), and logcosh loss functions (Wang et al., 2022)
are of this type.
136 Conclusions and Future Works
Our study introduces a novel framework for analyzing the generalization error of risk functions and
excess risk in transfer learning. The framework utilizes calculus on the space of probability measures,
which allows us to gain a deeper understanding of the factors that influence the generalization of
machine learning models in transfer learning scenarios, α-ERM and fine-tuning. We demonstrate the
efficacy of our framework by applying it to over-parameterized one-hidden layer neural network in
mean-field regime.
Given that mean-field analysis is applicable to single-hidden layer neural networks, we aim to extend our
current work to multiple-layer neural networks in future research based on (Sirignano and Spiliopoulos,
2022), Chizat et al. (2022), Jabir et al. (2019) and Geshkovski et al. (2023).
Acknowledgements
Authors acknowledge the support of the UKRI Prosperity Partnership Scheme (FAIR) under EPSRC
Grant EP/V056883/1 and the Alan Turing Institute. Samuel N. Cohen and Łukasz Szpruch also
acknowledge the support of the Oxford–Man Institute for Quantitative Finance.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. Advances in neural information processing systems, 32,
2019.
Gholamali Aminian, Samuel N Cohen, and Łukasz Szpruch. Mean-field analysis of generalization errors.
arXiv preprint arXiv:2306.11623, 2023.
Gholamali Aminian, Yixuan He, Gesine Reinert, Łukasz Szpruch, and Samuel N Cohen. Generalization
error of graph neural networks in the mean-field regime. ICML, 2024.
SanjeevArora,SimonDu,WeiHu,ZhiyuanLi,andRuosongWang. Fine-grainedanalysisofoptimization
and generalization for overparameterized two-layer neural networks. In International Conference on
Machine Learning, pages 322–332. PMLR, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. In Advances in Neural Information Processing
Systems, 2019b.
Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classification, and risk bounds.
Journal of the American Statistical Association, 101(473):138–156, 2006.
Shai Ben-David, John Blitzer, Koby Crammer, Fernando Pereira, et al. Analysis of representations for
domain adaptation. Advances in neural information processing systems, 19:137, 2007.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. A theory of learning from different domains. Machine learning, 79(1):151–175, 2010.
Olivier Bousquet and André Elisseeff. Stability and generalization. J. Mach. Learn. Res., 2:499–526,
March 2002. ISSN 1532-4435. doi: 10.1162/153244302760200704. URL https://doi.org/10.1162/
153244302760200704.
YuhengBu,GholamaliAminian,LauraToni,GregoryWWornell,andMiguelRodrigues. Characterizing
and understanding the generalization error of transfer learning with gibbs algorithm. In International
Conference on Artificial Intelligence and Statistics, pages 8673–8699. PMLR, 2022.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep
neural networks. Advances in neural information processing systems, 32, 2019.
14Pierre Cardaliaguet, François Delarue, Jean-Michel Lasry, and Pierre-Louis Lions. The master equation
and the convergence problem in mean field games. Princeton University Press, 2019.
René Carmona and François Delarue. Probabilistic Theory of Mean Field Games with Applications I.
Springer, 2018.
Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu. How much over-parameterization is sufficient
to learn deep relu networks? In International Conference on Learning Representations, 2019.
Zixiang Chen, Yuan Cao, Quanquan Gu, and Tong Zhang. A generalized neural tangent kernel analysis
for two-layer neural networks. Advances in Neural Information Processing Systems, 33:13363–13373,
2020.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized
models using optimal transport. Advances in neural information processing systems, 31, 2018.
Lénaïc Chizat, Maria Colombo, Xavier Fernández-Real, and Alessio Figalli. Infinite-width limit of deep
linear neural networks. arXiv preprint arXiv:2211.16980, 2022.
Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin
Chen, Chi-Min Chan, Weize Chen, et al. Parameter-efficient fine-tuning of large-scale pre-trained
language models. Nature Machine Intelligence, 5(3):220–235, 2023.
Cong Fang, Hanze Dong, and Tong Zhang. Over parameterized two-level neural networks can learn
near optimal feature representations. arXiv preprint arXiv:1910.11508, 2019.
Cong Fang, Hanze Dong, and Tong Zhang. Mathematical models of overparameterized neural networks.
Proceedings of the IEEE, 109(5):683–703, 2021.
Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, and Philippe Rigollet. The emergence of clusters
in self-attention dynamics. arXiv preprint arXiv:2305.05465, 2023.
Steve Hanneke and Samory Kpotufe. On the value of target data in transfer learning. Advances in
Neural Information Processing Systems, 32:9871–9881, 2019.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal
approximators. Neural networks, 2(5):359–366, 1989.
Kaitong Hu, Zhenjie Ren, David Šiška, and Łukasz Szpruch. Mean-field langevin dynamics and energy
landscape of neural networks, 2020.
Jean-François Jabir, David Šiška, and Łukasz Szpruch. Mean-field neural odes via relaxed optimal
control. arXiv preprint arXiv:1912.05475, 2019.
Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and general-
ization in neural networks. Advances in neural information processing systems, 31, 2018.
Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve arbitrarily
small test error with shallow relu networks. In International Conference on Learning Representations,
2020.
MM Kalan and Z Fabian. Minimax lower bounds for transfer learning with linear and one-hidden layer
neural networks. Neural Information Processing Systems (NeuRIPS 2020), 2020.
Wei Li, Rui Zhao, and Xiaogang Wang. Human reidentification with transferred metric learning. In
Asian conference on computer vision, pages 31–44. Springer, 2012.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. Advances in neural information processing systems, 31, 2018.
15Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with
deep adaptation networks. In International conference on machine learning, pages 97–105. PMLR,
2015.
Chao Ma, Lei Wu, et al. The generalization error of the minimum-norm solutions for over-parameterized
neural networks. arXiv preprint arXiv:1912.06987, 2019.
Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and the double descent curve. Communications on Pure and Applied Mathematics, 75
(4):667–766, 2022.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer
neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665–E7671, 2018.
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural
networks: dimension-free bounds and kernel limit, 2019.
Naoki Nishikawa, Taiji Suzuki, Atsushi Nitanda, and Denny Wu. Two-layer neural network on infinite
dimensional data: global optimization guarantee in the mean-field regime. In Advances in Neural
Information Processing Systems, 2022.
Atsushi Nitanda, Denny Wu, and Taiji Suzuki. Particle dual averaging: Optimization of mean field
neural network with global convergence rate analysis. Advances in Neural Information Processing
Systems, 34:19608–19621, 2021.
Ali Rahimi and Benjamin Recht. Uniform approximation of functions with random bases. In 2008
46th Annual Allerton Conference on Communication, Control, and Computing, pages 555–561. IEEE,
2008.
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of deep neural networks. Mathe-
matics of Operations Research, 47(1):120–152, 2022.
Nilesh Tripuraneni, Michael Jordan, and Chi Jin. On the theory of transfer learning: The importance
of task diversity. Advances in Neural Information Processing Systems, 33, 2020.
Belinda Tzen and Maxim Raginsky. A mean-field theory of lazy training in two-layer neural nets:
entropic regularization and controlled McKean–Vlasov dynamics. arXiv preprint arXiv:2002.01987,
2020.
Boyu Wang, Jorge Mendez, Mingbo Cai, and Eric Eaton. Transfer learning via minimizing the
performance gap between domains. Advances in Neural Information Processing Systems, 32:10645–
10655, 2019.
Qi Wang, Yue Ma, Kun Zhao, and Yingjie Tian. A comprehensive survey of loss functions in machine
learning. Annals of Data Science, 9(2):187–212, 2022.
Xuetong Wu, Jonathan H Manton, Uwe Aickelin, and Jingge Zhu. Information-theoretic analysis for
transfer learning. In 2020 IEEE International Symposium on Information Theory (ISIT), pages
2819–2824. IEEE, 2020.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? arXiv preprint arXiv:1411.1792, 2014.
16A Other related works
Generalization error and overparameterization: There are three main approaches to analyzing
learning problems in an overparameterized regime: the neural tangent kernel (NTK), random feature
and mean-field approaches. Under some assumptions, the NTK approach (a.k.a. lazy training) shows
that an overparameterized one-hidden-layer NN converges to an (infinite dimensional) linear model.
The neural tangent random feature approach is similar to NTK, where the model is defined based on
the network gradients at the initialization. The random feature model is similar to NTK, with an
extra assumption of constant weights in the single hidden layer of the NN. The mean-field approach
uses the exchangeability of neurons to work with distributions of parameters. The study of these
methods’ generalization performance allows us to extend our understanding of the overparameterized
regime. See, for example, for NTK (Allen-Zhu et al., 2019; Arora et al., 2019a,b; Cao and Gu, 2019; Ji
and Telgarsky, 2020; Li and Liang, 2018), neural tangent random feature (Cao and Gu, 2019; Chen
et al., 2019), random feature (Ma et al., 2019; Mei and Montanari, 2022) and mean-field (Nishikawa
et al., 2022; Nitanda et al., 2021) settings. The neural tangent kernel and random feature results do
not precisely reflect practice due to their constraints, such as the solution’s inability to deviate too
far from the weights’ initialization (Fang et al., 2021). A generalized NTK approach is considered in
(Chen et al., 2020), inspired by the mean-field approach, they derive a high-probability bound on the
√
generalization error of one-hidden-layer NNs with convergence rate O(1/ n) where n is the number of
√
training samples. A high probability generalization error bound with convergence rate O(1/ n) in
one-hidden-layer NN and with 0–1 and quadratic loss is studied in (Nishikawa et al., 2022; Nitanda
et al., 2021) assuming a mean-field regime. In this work, we study the transfer learning in mean-field
regime. Recently, (Aminian et al., 2023) proposed an approach based on differential calculus on the
space of probability measures to study the weak and strong generalization error in supervised learning
scenario. A similar approach is utilized to study the generalization error of graph neural networks in
mean-field regime by Aminian et al. (2024).
Mean-field: Our study employs the mean-field framework utilized in a recent line of research (Chizat
andBach,2018;Fangetal.,2019,2021;Huetal.,2020;Meietal.,2018,2019;SirignanoandSpiliopoulos,
2022). Chizat and Bach (2018) establishes the convergence of gradient descent for training one-hidden-
layer NNs with infinite width under certain structural assumptions. The study of Mei et al. (2018)
proves the global convergence of noisy stochastic gradient descent and establishes approximation bounds
between finite and infinite neural networks. Furthermore, Mei et al. (2019) demonstrates that this
approximation error can be independent of the input dimension in certain cases, and establishes that
the residual dynamics of noiseless gradient descent are close to the dynamics of NTK-based kernel
regression under some conditions. The mean-field approach is mostly restricted to one-hidden-layer
NNs and the extension to Deep NNs is not trivial (Chizat et al., 2022; Sirignano and Spiliopoulos, 2022).
Fang et al. (2019) proposes a new concept known as neural feature repopulation inspired by the mean
field view. Lastly, the mean-field approach’s performance with respect to other methods in feature
learning (Fang et al., 2021) suggests it is a viable option for analyzing one-hidden-layer NNs. The focus
of our study is based on the work conducted by Hu et al. (2020) in the area of non-linear functional
minimization with KL regularization. Their investigation demonstrated the linear convergence (in
continuous time) of the resulting mean-field Langevin dynamics under the condition of sufficiently
robust regularization. In this work, we focus on the study of weak transfer generalization error and
weak transfer excess risk in mean-field regime.
17B Technical Preliminaries
An overview of our main results is provided in Fig. 1. All notations are summarized in Table 2.
Transfer
Learning
in
Mean-Field
Regime
α-ERM Fine-tuning
WeakTransfer WeakTransfer
WeakTransfer WeakTransfer
Generaliza- Generaliza-
ExcessRisk ExcessRisk
tionError tionError
UpperBound UpperBound UpperBound UpperBound
(Theorem4) (Theorem5) (Theorem6) (Theorem7)
Figure 1: Overview of Transfer Learning Results
Table 2: Summary of notations in the paper
Notation Definition Notation Definition
Zs Source training dataset Zt Target Training dataset
ns nt
n Number of target training samples n Number of source training samples
t s
νs Source empirical measure νs Source population measure
ns pop
νt Target empirical measure νt Target population measure
nt pop
σ2 KL–Regularization parameter m(.) Measure over parameters
2β2
θ Parameters of model Θ Model’s parameter space
θ Specific parameters of model Θ Model’s specific parameter space
sp sp
θ Comman parameters of model Θ Model’s common parameter space
c c
gen(m,νt ) Generalization Error E(m,νt ) Excess Risk
pop pop
ℓ(m,z) Loss function φ(.) Activation function
mβ(ν) Gibbs measure KL(m∥γσ) KL divergence between m and γσ
θ θ
18We provide the full versions of Assumption 3 and Assumption 4.
Assumption 3 (Full version). The loss function and the functional derivative of loss function with
respect to m, are such that,
(i) For all z ∈ Z and m ∈ P (Θ), there exists L > 0 such that the loss satisfies
4 m
0 ≤ ℓ(m,z) ≤ g(m)(cid:0) 1+∥z∥2(cid:1) , (21)
where g(m) = L (1+E [∥θ∥4])
m θ∼m
(ii) For all z ∈ Z and m ∈ P (Θ), there exists L > 0 such that the functional derivative of loss
4 e
satisfies
(cid:12) (cid:12) δℓ (m,z,θ)(cid:12) (cid:12) ≤ g (m,θ)(cid:0) 1+∥z∥2(cid:1) , (22)
(cid:12)δm (cid:12) e
where g (m,θ) = L (cid:0) 1+E [∥θ∥4]+∥θ∥4(cid:1) .
e e θ∼m
U(θ)
(iii) For all m ∈ P (Θ), the regularizing potential U satisfies lim = ∞;
8 ∥θ∥→∞ ∥θ∥4+ge(m,θ)
(iv) We have the pointwise integrability conditions g(γ˜σ) < ∞ and, for all ν ∈ P (Z) and m,m′ ∈
8 2
P (Θ),
8
(cid:104)(cid:16) δ2ℓ (cid:17)2(cid:105)1/2
E E (m,z,θ,θ′) < ∞. (23)
θ,θ′ z∼ν δm2
Assumption 4 (Full version). The loss function and the functional derivative of loss function with
respect to m , are such that,
sp
(i) For all z ∈ Z, m ∈ P (Θ ) and m ∈ P (θ ), there exists L such that the loss satisfies
c 8 c sp 8 sp m
ℓ(m m ,z) ≤ g(m m
)(cid:0) 1+∥z∥2(cid:1)
,
c sp c sp
where g(m m ) = L (1+E [∥θ ∥4]+E [∥θ ∥4]).
c sp m θc∼mc c θsp∼msp sp
(ii) For all z ∈ Z, m ∈ P (Θ ) and m ∈ P (θ ), there exists L such that the functional derivative
c 8 c sp 8 sp e
of loss function with respect to m satisfies
sp
(cid:12) (cid:12) δℓ (m m ,z,θt )(cid:12) (cid:12) ≤ g (m m ,θt )(cid:0) 1+∥z∥2(cid:1) ; (24)
(cid:12)δm c sp sp (cid:12) sp c sp sp
sp
where g (m m ,θt ) = L (1+E [∥θ ∥4]+E [∥θt ∥4]+∥θt ∥4).
sp c sp sp e θc∼mc c θsp∼msp sp sp
(iii) For all m ∈ P (Θ ) and m ∈ P (Θ ), the regularizing potential U satisfies
c 8 c sp 8 sp
U(θt )
sp
lim = ∞;
∥θsp∥→∞ ∥θ st p∥8+g sp(m cm sp,θ st p)
(iv) We assume that g(γˆσ) < ∞, g(γ˜σ ) < ∞ and, for all ν ∈ P (Z) and m ,m′ ∈ P (Θ ),
p sp 2 sp sp 8 sp
(cid:104)(cid:16) δ2ℓ (cid:17)2(cid:105)1/2
E E (m m ,z,θt ,θt,′) < ∞. (25)
θsp,θ s′ p z∼ν δm2 c sp sp sp
sp
Lemma 2. Suppose that ν˜ ∈ P (Z), m ,m ∈ P (Θ). Then, under Assumption 3, the following upper
2 1 2 4
bound holds,
R(m ,ν˜)−R(m ,ν˜) ≤ L E [(1+∥Z∥2)2]d (m ,m ).
1 2 e Z∼ν˜ F4 1 2
19Proof. Define m(λ) = m +λ(m −m )
2 1 2
R(m ,ν˜)−R(m ,ν˜)
1 2
(cid:90)
= ℓ(m ,z)−ℓ(m ,z)ν˜(dz)
1 2
Z
(cid:90) 1(cid:90) (cid:90)
δℓ
= (m(λ),z,θ)(m −m )(dθ)ν˜(dz)dλ
1 2
δm
0 Z Θ
(cid:32) (cid:32) (cid:33)2 (cid:33)1/2
(a) (cid:90) 1(cid:90) (cid:16)(cid:90) (cid:17)1/2 (cid:90) δℓ (m(λ),z,θ)
≤ (1+∥Z∥2)2ν˜(dz) δm ν˜(dz) (m −m )(dθ)dλ
(1+∥z∥2) 1 2
0 Θ Z Z
(cid:32) (cid:33)2
(cid:16)(cid:90) (cid:17)1/2(cid:90) 1(cid:90) (cid:16)(cid:90) δℓ (m(λ),z,θ) (cid:17)1/2
= (1+∥z∥2)2ν˜(dz) δm ν˜(dz) (m −m )(dθ)dλ
(1+∥Z∥2) 1 2
Z 0 Θ Z
(cid:16)(cid:90) (cid:17)1/2
= (1+∥z∥2)2ν˜(dz)
Z
(cid:32) (cid:32) (cid:33)2 (cid:33)1/2
(cid:90) 1(cid:90) (cid:90) δℓ (m(λ),z,θ)
× g (m(λ),θ) δm ν˜(dz) (m −m )(dθ)dλ
e (1+∥Z∥2)g (m(λ),θ) 1 2
0 Θ Z e
(cid:16)(cid:90) (cid:17)1/2
= (1+∥z∥2)2ν˜(dz)
Z
(cid:32) (cid:32) (cid:33)2 (cid:33)1/2
(cid:90) 1(cid:90) (cid:90) δℓ (m(λ),z,θ)
× g (m(λ),θ) δm ν˜(dz) (m −m )(dθ)dλ
e (1+∥z∥2)g (m(λ),θ) 1 2
0 Θ Z e
(cid:90) 1(cid:90)
≤ L E [(1+∥Z∥2)2] (cid:0) 1+E [∥θ∥4]+∥θ∥4(cid:1) (m −m )(dθ)dλ,
e Z∼ν˜ θ∼m(λ) 1 2
0 Θ
(b)
≤ L E [(1+∥Z∥2)2]d (m ,m ),
e Z∼ν˜ F4 1 2
where g (m(λ),θ) = L (cid:0) 1+E [∥θ∥4]+∥θ∥4(cid:1) and (a) and (b) follow from Cauchy–Schwarz inequality
e e θ∼m
and Assumption 3(ii).
Similar result also holds under Assumption 4.
Lemma 3. Suppose that ν ,ν ∈ P (Z), m ∈ P (Θ). Then, under Assumption 3, the following upper
1 2 2 8
bound holds,
R(m,ν )−R(m,ν ) ≤ g(m)d (ν ,ν ),
1 2 F2 1 2
where g(m) = L (1+E [∥θ∥4]).
m θ∼m
Proof.
R(m,ν )−R(m,ν )
1 2
(cid:90)
= ℓ(m,z)(ν −ν )(dz)
1 2
Z
(cid:90)
ℓ(m,z)
= g(m) (ν −ν )(dz) (26)
1 2
g(m)
Z
(cid:12)(cid:90) ℓ(m,z) (cid:12)
≤ g(m)(cid:12) (ν −ν )(dz)(cid:12)
(cid:12) g(m) 1 2 (cid:12)
Z
≤ g(m)d (ν ,ν ).
F2 1 2
Remark 8. We can see that the integral probability metric (IPM) in the above calculation is a relatively
simple way to give a bound on the impact of changing ν to ν , for a given parameter distribution m. In
1 2
20many contexts, a tighter bound could also be used, for example if ℓ(m,·)/g(m) has more restrictive growth
assumptions (e.g. ℓ is bounded), or if ℓ satisfies an anisotropic bound (i.e. where certain directions of z
are less significant than others when computing the loss). In these settings, we can simply replace the
IPM wherever it appears by a corresponding bound. This may be particularly valuable when assessing
the significance of the difference between source and target problem distributions, where the quantity
d (νs,νt) is an otherwise unavoidable term in our final error bounds.
F2
Lemma 4. Consider m ∈ P (Θ) and m ∈ P (Θ). Then, the distance d (m ,m ) is finite.
1 4 2 4 F4 1 2
Proof.
(cid:12)(cid:90) (cid:12)
d (m ,m ) = sup (cid:12) f(θ)(m −m )(dθ)(cid:12)
Fp 1 2 (cid:12) 1 2 (cid:12)
f∈F4 Θ (27)
(cid:90) (cid:90)
≤
(cid:0) 1+∥θ∥4(cid:1)
m (dθ)+
(cid:0) 1+∥θ∥4(cid:1)
m (dθ),
1 2
Θ Θ
where (cid:82) ∥θ∥pm (dθ) is bounded due to assumption m ∈ P (Θ) for i = 1,2.
Θ i i 4
In the our proof, we will regularly make use of the basic inequality (based on Cauchy–Schwarz), for any
square integrable random variables X,Y,
(cid:112) (cid:112)
|Cov(X,Y)| ≤ V(X)V(Y) ≤ E(X2)E(Y2).
A slightly more involved, but fundamentally similar, inequality is given in the following lemma.
Lemma 5. (Aminian et al., 2023, Lemma D.7). Let π : (R+)2n → R be a polynomial with positive
coefficients. Then
(cid:104) (cid:16) (cid:17)(cid:105) (cid:104) (cid:16) (cid:17)(cid:105)
E
Zn,Z(cid:101)n
π ∥Z 1∥,∥Z 2∥,...,∥Z n∥,∥Z(cid:101)1∥,∥Z(cid:101)2∥,...,∥Z(cid:101)n∥ ≤ E
Z1
π ∥Z 1∥,∥Z 1∥,...,∥Z 1∥ .
The same result also holds when the polynomial involves expectation of E [∥Z ∥ζ] terms.
Zi i
C Proofs and details from Section 3
Lemma 1. Consider a generic loss function (m,z) (cid:55)→ ℓ(m,z), and (νt ,νs ) as defined in
nt,(1) ns
(1). The WTGE (9) is given by,
gen(m(ν nt t,ν ns s),ν pt op) = E
Zt nt,Zs
ns,Z(cid:101)t
1(cid:104) ℓ(cid:0) m(ν nt t,ν ns s),Z(cid:101)t 1(cid:1) −ℓ(cid:0) m(ν nt t,(1),ν ns s),Z(cid:101)t 1(cid:1)(cid:105) . (28)
Proof of Lemma 1. Recall that Z(cid:101)t
1
∼ ν
pop
is independent of {Z it}n i=1. Since elements of {Z it}n
i=1
are
i.i.d., the WTGE (9) can be written
gen(m,νt ) = E (cid:2) R(m(νt ,νs ),νt )−R(m(νt ,νs ),νt )(cid:3)
pop Zt nt,Zs
ns
nt ns pop nt ns nt
(cid:104)(cid:90) (cid:90) (cid:105)
= E ℓ(m(νt ,νs ),z)νt (dz)− ℓ(m(νt ,νs ),z)νt (dz)
Zt nt,Zs
ns
Z
nt ns pop
Z
nt ns nt
n (29)
= E
Zt nt,Zs
ns(cid:104) E
Z(cid:101)t
1[ℓ(m(ν nt t,ν ns s),Z(cid:101)t 1)]− n1
t
(cid:88) ℓ(m(ν nt t,ν ns s),Z it)(cid:105)
i=1
= E
Zt nt,Zs
ns(cid:104) E
Z(cid:101)t
1[ℓ(m(ν nt t,ν ns s),Z(cid:101)t 1)]−ℓ(m(ν nt t,ν ns s),Z 1t)(cid:105) .
Recall that in (1) we defined the perturbation νt := νt + 1 (cid:0) δ −δ (cid:1) , which corresponds to a
nt,(1) nt nt Z(cid:101)t
1
Z 1t
target data measure with one different data point, and that, as Zt and Z(cid:101)t are i.i.d., we have
1 1
E
Zt nt,Zs
ns(cid:2) ℓ(m(ν nt t,ν ns s),Z 1t)(cid:3) = E
Zt nt,Zs
nsE
Z(cid:101)t
1(cid:2) ℓ(m(ν nt t,(1),ν ns s),Z(cid:101)t 1)(cid:3) . (30)
Combining this observation with (29) yields the representation (11).
21Theorem 1 (Restated). Given Assumption 2, the weak transfer generalization error has the
representation
(cid:34) (cid:35)
gen(m,ν pt op) = E
Zt nt,Zs
ns,Z(cid:101)t
1
(cid:90) 01(cid:90) Θ(cid:16) δmδℓ sp(cid:0) m (1)(λ),Z(cid:101)t 1,θ(cid:1)(cid:17) (cid:0) m(ν nt t,ν ns s)−m(ν nt t,(1),ν ns s)(cid:1) (dθ)dλ .
Proof of Theorem 1. Recall that from (3), νt = νt + 1 (δ −δ ) and m (λ) = m(νt ,νs )+
λ(cid:0) m(νt ,νs )−m(νt ,νs )(cid:1) . Using the den fit n,( i1 t) ion on ft then lt ineZ a(cid:101)t 1 r funZ c1 tt ional der(1 iv) ative (Defin nt ition ns 1)
and
Len mt,( m1)
a
1ns
we
havent ns
gen(m,ν pt op) = E
Zt nt,Zs
ns,Z(cid:101)t
1(cid:104) ℓ(cid:0) m(ν nt t,ν ns s),Z(cid:101)t 1(cid:1) −ℓ(cid:0) m(ν nt t,(1),ν ns s),Z(cid:101)t 1(cid:1)(cid:105)
= E
Zt nt,Zs
ns,Z(cid:101)t
1(cid:104)(cid:90) 01(cid:90) Θ(cid:16) δmδℓ sp(cid:0) m (1)(λ),Z(cid:101)t 1,θ(cid:1)(cid:17) (cid:0) m(ν nt t,ν ns s)−m(ν nt t,(1),ν ns s)(cid:1) (dθ)dλ(cid:105) .
Theorem 2 (Restated). The WTGE of α-ERM can be written
(cid:34) (cid:35)
gen(m,νt ) = α E
(cid:90) 1(cid:90) 1(cid:90)
A(λ,λ˜)(cid:0) δ −δ (cid:1) (dz)dλ˜dλ , (31)
pop n
t
Zt nt,Zs ns,Z(cid:101)t
1 0 0 Z
Z 1t Z˜ 1t
where
A(λ,λ˜) = (cid:82)
Θ
δmδℓ sp(cid:0) m (1)(λ),Z(cid:101)t 1,θ(cid:1)δ δm ν(cid:0) ν α,(1)(λ˜),z(cid:1) (dθ), and ν α,(1)(λ˜) = (αν nt
t,(1)
+ (1 − α)ν ns s) +
λ˜α(νt −νt ).
nt nt,(1)
Proof of Theorem 2. We know from (1) that
νt −νt = 1 (cid:0) δ −δ (cid:1) .
nt nt,(1) n
t
Z 1t Z(cid:101)t
1
From Theorem 1, for α-ERM we have
gen(m,νt )
pop
= E
Zt nt,Zs
ns,Z(cid:101)t
1(cid:104)(cid:90) 01(cid:90) Θ(cid:16) δmδℓ sp(cid:0) m (1)(λ),Z(cid:101)t 1,θ(cid:1)(cid:17) (32)
(cid:105)
(cid:0) m(ανt +(1−α)νs )−m(ανt +(1−α)νs )(cid:1) (dθ)dλ .
nt ns nt,(1) ns
Applying the functional derivative to m and recalling the definition of ν (λ˜) in (3), for any Borel set
(1)
B ⊂ Θ we have
(cid:0) m (ν )−m(ανt +(1−α)νs )(cid:1) (B)
α α nt,(1) ns
(cid:90) 1(cid:90) (cid:16)δm (cid:17)
= α (ν (λ˜),z)(B) (νt −νt )(dz)dλ˜
δν α,(1) nt nt,(1)
0 Z
= nα (cid:90) 1(cid:16)δ δm
ν
(ν α,(1)(λ˜),Z 1t)(B)− δ δm
ν
(ν α,(1)(λ˜),Z(cid:101)t 1)(B)(cid:17) dλ˜,
t 0
where ν (λ˜) = (ανt +(1−α)νs )+λ˜α(νt −νt ). Then, we substitute this expression for
α,(1) nt,(1) ns nt nt,(1)
and m(ανt +(1−α)νs )−m(ανt +(1−α)νs ) in (32). The result follows immediately.
nt ns nt,(1) ns
22Theorem 3 (Restated). The WTGE of fine-tuning can be written,
gen(m,ν pt op) = n1 tE
Zt nt,Zs
ns,Z(cid:101)t
1(cid:2) K(Zt nt,Zs ns,Z(cid:101)t 1)(cid:3) , (33)
where
K(Zt ,Zs ,Z(cid:101)t )
nt ns 1
= (cid:90) 1(cid:90) (cid:16) δℓ (cid:0) mf (λ),Z(cid:101)t ,θt (cid:1)(cid:17)
δm (1) 1 sp
0 Θsp sp
(cid:18)(cid:90) 1(cid:90) δm (cid:19)
× sp (m (νs ),νt (λ ),z)(dθt )(δ −δ )(dz)dλ dλ,
0 Z
δν c ns nt,(1) 1 sp Z 1t Z(cid:101)t
1
1
and mf (λ) = m (νs )(cid:2) (1−λ)(mt (m (νs ),νt )+λmt (m (νs ),νt )(cid:3) .
(1) c ns sp c ns nt sp c ns nt,(1)
Proof of Theorem 3. From Theorem 1 and considering θ = θ ∪θt for target task, we have,
c sp
gen(m,νt )
pop
= E
Zt nt,Zs
ns,Z(cid:101)t
1(cid:104)(cid:90) 01(cid:90) Θsp(cid:16) δmδℓ sp(cid:0) mf (1)(λ),Z(cid:101)t 1,θ st p(cid:1)(cid:17)
(cid:105)
(cid:0) m (νs )m (m (νs ),νt )−m (νs )m (m (νs ),νt )(cid:1) (dθ dθt )dλ
c ns sp c ns nt c ns sp c ns nt,(1) c sp
= E
Zt nt,Zs
ns,Z(cid:101)t
1(cid:104)(cid:90) 01(cid:90) Θsp(cid:16) δmδℓ sp(cid:0) mf (1)(λ),Z(cid:101)t 1,θ st p(cid:1)(cid:17)
(cid:105)
(cid:0) m (νs )m (m (νs ),νt )−m (νs )m (m (νs ),νt )(dθ dθt )dλ
c ns sp c ns nt c ns sp c ns nt,(1) c sp
(cid:34)
= E
Zt nt,Zs
ns,Z(cid:101)t
1
(cid:90) 01(cid:90) Θsp(cid:16) δmδℓ sp(cid:0) mf (1)(λ),Z(cid:101)t 1,θ st p(cid:1)(cid:17)
(cid:35)
(cid:16)(cid:90) 1(cid:90) δm (cid:17)
sp (m (νs ),νt (λ ),z)(dθt )(νt −νt )(dz)dλ dλ
δν c ns nt,(1) 1 sp nt nt,(1) 1
0 Z
(cid:34)
= E
Zt nt,Zs
ns,Z(cid:101)t
1
(cid:90) 01(cid:90) Θsp(cid:16) δmδℓ sp(cid:0) mf (1)(λ),Z(cid:101)t 1,θ st p(cid:1)(cid:17)
(cid:35)
(cid:16)(cid:90) 1(cid:90) δm (cid:17)
sp (m (νs ),νt (λ ),z)(dθt )(νt −νt )(dz)dλ dλ
δν c ns nt,(1) 1 sp nt nt,(1) 1
0 Z
(cid:34)
= n1 tE
Zt nt,Zs
ns,Z(cid:101)t
1
(cid:90) 01(cid:90) Θsp(cid:16) δmδℓ sp(cid:0) mf (1)(λ),Z(cid:101)t 1,θ st p(cid:1)(cid:17)
(cid:35)
(cid:16)(cid:90) 1(cid:90) δm (cid:17)
sp (m (νs ),νt (λ ),z)(dθt )(δ −δ )(dz)dλ dλ .
0 Z
δν c ns nt,(1) 1 sp Z 1t Z(cid:101)t
1
1
where
mf (λ) = m (νs )(cid:2) (mt (m (νs ),νt )+λ(mt (m (νs ),νt )−mt (m (νs ),νt ))(cid:3) .
(1) c ns sp c ns nt sp c ns nt,(1) sp c ns nt
23D Proofs and details from Section 4
D.1 Technical Tools
We borrowed some main technical tools which are used in our proofs from (Aminian et al., 2023).
The results in (Aminian et al., 2023) are applicable for supervised learning scenario. The following
assumption is needed for the results in (Aminian et al., 2023).
Assumption 6. For a fixed p ≥ 2, there exists g : P (Θ) → (0,∞) and g : P (Θ)×Θ → (0,∞) such
2 e 2
that,
(i) The loss function is ℓ is C2 (Definition 1), nonnegative, and convex with respect to m;
(ii) For all z ∈ Z and m ∈ P (Θ), the loss satisfies
8
(cid:12) (cid:12)ℓ(m,z)(cid:12)
(cid:12) ≤
g(m)(cid:0) 1+∥z∥2(cid:1)
and the derivative of the loss satisfies
(cid:12) (cid:12) δℓ (m,z,θ)(cid:12) (cid:12) ≤ g (m,θ)(cid:0) 1+∥z∥2(cid:1) ;
(cid:12)δm (cid:12) 1
U(θ)
(iii) For all m ∈ P (Θ), the regularizing potential U satisfies lim = ∞;
8 ∥θ∥→∞ ∥θ∥p+ge(m,θ)
(iv) There exists L > 0 such that, for all m,m′ ∈ P (Θ),
e 8
E
(cid:104)
(cid:0) g
(m,θ)(cid:1)2(cid:105)1/2
≤ L
(cid:16)
1+E (cid:2) ∥θ∥p(cid:3) +E (cid:2)
∥θ∥p(cid:3)(cid:17)
, (34)
θ∼m′ 1 e θ∼m′ θ∼m
(v) We have the pointwise integrability conditions g(γ˜σ) < ∞ and, for all ν ∈ P (Z) and m,m′ ∈
8 2
P (Θ);
8
(cid:104)(cid:16) δ2ℓ (cid:17)2(cid:105)1/2
E E (m,z,θ,θ′) < ∞. (35)
θ,θ′∼m z∼ν δm2
Lemma 6. Defining the set valued map
(cid:110) σ2
B(ν) := m ∈ P (Θ) : KL(m∥γσ) ≤ R(γσ,ν)
8 2β2
(cid:90) (cid:90) (cid:111)
and ∥θ∥8m(dθ) ≤ R(γ˜σ,ν)+ ∥θ∥8γ˜σ(dθ) ⊂ P (Θ),
8 8 8
Θ Θ
for any ν ∈ P (Z), the KL-regularized risk minimizer satisfies mβ(ν) ∈ B(ν).
2
Lemma 7. (Aminian et al., 2023, Lemma E.2). For any choice of m ∈ B(ν), the linear map
C : L2(m,Θ) → L2(m,Θ) defined by
m
(cid:104)δ2R (cid:105)
C f(θ) := E (m,ν,θ,θ′)f(θ′)
m θ′∼m δm2
is positive, in the sense that
(cid:90)
⟨f,C f⟩ = f(θ)(C f)(θ)m(dθ) ≥ 0.
m L2(m,Θ) m
Θ
In particular, C is a Hilbert–Schmidt operator with discrete spectrum
m
σ(C ) = {λC} ⊂ [0,∞).
m i i≥0
24Lemma 8. (Aminian et al., 2023, Lemma E.3). Define
(cid:90)
δR δℓ
S(ν,θ) := (mβ(ν),ν,θ) = (mβ(ν),z,θ)ν(dz). (36)
δm δm
Z
Under Assumption 6, we know S is differentiable in ν, and its derivative satisfies the bound
(cid:90) (cid:16)δS (cid:17)2
(ν,θ,z) mβ(ν;dθ)
δν
Θ
(cid:90) (cid:16) δℓ (cid:90) δℓ (cid:17)2
≤ (mβ(ν),θ,z)− (mβ(ν),θ,z′)ν(dz′) mβ(ν;dθ).
δm δm
Θ Z
In particular, we have the representation:
(cid:90)
δS (ν,θ,z) = δℓ (cid:0) mβ(ν),z,θ(cid:1) − δℓ (cid:0) mβ(ν),z′,θ(cid:1) ν(dz′)
δν δm δm
Z
2β2 (cid:104)(cid:90) δ2ℓ δS (cid:105)
− Cov (mβ(ν),z′,θ,θ′)ν(dz′), (ν,θ′,z) .
σ2 θ′∼mβ(ν) δm2 δν
Z
Lemma 9. (Aminian et al., 2023, Lemma E.4). Under Assumption 6, the density of the Gibbs measure,
that is, the map ν (cid:55)→ mβ(ν), has derivative
θ
δmβ 2β2 (cid:16)δS (cid:104)(cid:90) δS (cid:105)(cid:17)
θ(ν,θ,z) = − mβ(ν;θ) (ν,θ,z)− mβ(ν;θ′) (ν,θ′,z)dθ .
δν σ2 θ δν θ δν
Θ
In particular, for any f ∈ L2(dθ),
(cid:90) δmβ 2β2 (cid:104) δS (cid:105)
f(θ) θ(ν,θ,z)dθ = − Cov f(θ), (ν,θ,z) .
δν σ2 θ∼mβ(ν) δν
Θ
D.2 α-ERM details and proofs
Note that under Assumption 3, Assumption 6 also holds. Therefore, the results in Lemma 9, Lemma 6,
Lemma 7 and Lemma 8 also hold.
Proposition 1. Suppose Assumption 3 holds. Then
(cid:90) (cid:104)δS (ν,θ,z)− δS (ν,θ,z′)(cid:105)2 mβ(ν;dθ) ≤ 6L2(cid:16) 1+2E (cid:2) ∥θ∥8(cid:3)(cid:17)(cid:16) 2+∥z∥2+∥z′∥2(cid:17)2 ,
δν δν e θ∼mβ(ν)
Θ
where S(ν,δ) is defined in (36).
Proof of Proposition 1. We know from Lemma 7 that δS(ν,θ,z)− δS(ν,θ,z′) is bounded in L2(mβ(ν)),
δν δν
in particular
(cid:90) (cid:104)δS δS (cid:105)2
(ν,θ,z)− (ν,θ,z′) mβ(ν;dθ)
δν δν
Θ
(cid:90) (cid:104) δℓ δℓ (cid:105)2
≤ (mβ(ν),θ,z)− (mβ(ν),θ,z′) mβ(ν;dθ)
δm δm
Θ
(cid:90) (cid:104) δℓ (cid:105)2 (cid:104) δℓ (cid:105)2
≤ 2 (mβ(ν),θ,z) + (mβ(ν),θ,z′) mβ(ν;dθ).
δm δm
Θ
Assumption 3(ii) yields
(cid:90) (cid:104)δS δS (cid:105)2
(ν,θ,z)− (ν,θ,z′) mβ(ν;dθ)
δν δν
Θ
≤
2(cid:16) 2+∥z∥2+∥z′∥2(cid:17)2(cid:90)
(cid:2) 1+E [∥θ∥4]+∥θ∥4(cid:3)2 mβ(ν;dθ)
θ∼mβ(ν;dθ)
Θ
≤
6(cid:16) 2+∥z∥2+∥z′∥2(cid:17)2(cid:90)
(cid:2) 1+E [∥θ∥4]2+∥θ∥8(cid:3) mβ(ν;dθ)
θ∼mβ(ν;dθ)
Θ
≤
6(cid:16) 2+∥z∥2+∥z′∥2(cid:17)2(cid:2)
1+2E [∥θ∥8](cid:3) .
θ∼mβ(ν;dθ)
25Theorem 4 (Full Version). Given Assumption 3, E (cid:2) ∥Z∥8(cid:3) < ∞ and E (cid:2) ∥Z∥4(cid:3) <
Z∼νt Z∼νs
pop pop
∞, the weak transfer generalization error for the α-ERM satisfies
√ 2
|gen(mβ(ν ),νt )| ≤ 2L2(1+αL )2(1+ )2Comp(θ)
α pop e m n
t
(cid:34)
(cid:104) (cid:105)
× 2(2+α)2E (1+∥Zt∥2)4
Zt 1
1
(cid:35)
(cid:104) (cid:105) (cid:104) (cid:105)
+2(1−α)2E Zt (1+∥Z 1t∥2)2 E Zs (1+∥Z 1s∥2)2 ,
1 1
where Comp(θ) = (cid:2) 1+2(cid:82) ∥θ∥8γ˜σ(dθ)+2E [∥θ∥4](cid:3)2 .
Θ 8 θ∼γ˜ 8σ
Proof of Theorem 4. Recall from (3) that ν (λ) = ν +λ(ν −ν ). From Theorem 2,
(1) n,(1) n α,(1)
gen(m α(ν α),ν pt op) = n1 tE
Zt nt,Zs
ns,Z(cid:101)t
1(cid:2) h(Zt nt,Zs ns,Z(cid:101)t 1)(cid:3) , (37)
where
h(Zt ,Zs ,Z(cid:101)t )
nt ns 1
= (cid:90) 01(cid:90) 01(cid:90) Z(cid:16)(cid:90)
Θ
δδ mℓ (cid:0) m (1)(λ),Z(cid:101)t 1,θ(cid:1)δ δm
ν
(cid:0) ν α,(1)(λ˜),z(cid:1) (dθ)(cid:17) (cid:0) δ
Z 1t
−δ
Z˜
1t(cid:1) (dz)dλ˜dλ, (38)
AsE (cid:2)δS(cid:0) ν (λ˜),z,θ(cid:1)(cid:3) ≡ 0(bydefinitionofS = δR/δmandthenormalizationcondition
θ∼mβ,σ(ν (λ˜)) δν α,(1)
α,(1)
and chain rule), from Lemma 9 we know
(cid:90) (cid:16) δδ mℓ (cid:0) m (1)(λ),Z 1t,θ(cid:1)(cid:17)(cid:16)δ δm
ν
(cid:0) ν α,(1)(λ˜),Z 1t(cid:1) − δ δm
ν
(cid:0) ν α,(1)(λ˜),Z(cid:101)t 1(cid:1)(cid:17) (dθ)
Θ
=
−2β2
Cov
(cid:104) δℓ (cid:0)
m
(λ),Zt,θ(cid:1)
,
δS(cid:0)
ν
(λ˜),Zt,θ(cid:1)
−
δS(cid:0)
ν
(λ˜),Z˜t,θ(cid:1)(cid:105)
σ2 θ∼mβ,σ(ν α,(1)(λ˜)) δm (1) 1 δν α,(1) 1 δν α,(1) 1 (39)
≤ 2β2 E (cid:104)(cid:16) δℓ (cid:0) m (λ),Zt,θ(cid:1)(cid:17)2(cid:105)1/2
σ2 θ∼mβ,σ(ν α,(1)(λ˜)) δm (1) 1
×E (cid:104)(cid:16)δS(cid:0) ν (λ˜),Zt,θ(cid:1) − δS(cid:0) ν (λ˜),Z˜t,θ(cid:1)(cid:17)2(cid:105)1/2 .
θ∼mβ,σ(ν α,(1)(λ˜)) δν α,(1) 1 δν α,(1) 1
Using Proposition 1 and (22),
(cid:104) E (cid:16)(cid:12) (cid:12)δS(cid:0) ν (λ˜),Zt,θ(cid:1) − δS(cid:0) ν (λ˜),Z˜t,θ(cid:1)(cid:12) (cid:12)2(cid:17)(cid:105)1/2
θ∼mβ,σ(ν α,(1)(λ˜)) (cid:12)δν α,(1) 1 δν α,(1) 1 (cid:12)
≤ L e(cid:16)(cid:90) (cid:2) 1+E
θ∼mβ,σ(ν
(λ˜))(cid:2) ∥θ∥4(cid:3) +∥θ∥4(cid:3)2 mβ,σ(ν α,(1)(λ˜);dθ)(cid:17)1/2(cid:16) 2+∥Z 1t∥2+∥Z(cid:101)t 1∥2(cid:17) (40)
Θ α,(1)
≤ √ 2L e(cid:104) 1+2E
θ∼mβ,σ(ν
(λ˜))(cid:2) ∥θ∥8(cid:3)(cid:105)(cid:16) 2+∥Z 1t∥2+∥Z(cid:101)t 1∥2(cid:17) .
α,(1)
From Lemma 6, we know
(cid:90)
E (cid:2) ∥θ∥8(cid:3) ≤ R(γ˜σ,ν (λ˜))+ ∥θ∥8γ˜σ(dθ).
θ∼mβ,σ(ν (λ˜)) 8 α,(1) 8
α,(1) Θ
By construction,
ν (λ˜) = ν
+α1−λ˜
(cid:0) δ −δ (cid:1) ,
α,(1) α n
t
Z(cid:101)t
1
Z 1t
26so
R(γ˜ 8σ,ν α,(1)(λ˜)) = R(γ˜ 8σ,ν α)+α1 n−λ˜(cid:16) ℓ(γ˜ 8σ,Z(cid:101)t 1)−ℓ(γ˜ 8σ,Z 1t)(cid:17)
t (41)
≤ R(γ˜ 8σ,ν α)+g(γ˜ 8σ)α1 n−λ˜(cid:16) 2+∥Z(cid:101)t 1∥2+∥Z 1t∥2)(cid:17) ,
t
where g(γ˜σ) = L (1+E [∥θ∥4]), and hence
8 m θ∼γ˜σ
8
E (cid:104)(cid:12) (cid:12)δS(cid:0) ν (λ˜),Zt,θ(cid:1) − δS(cid:0) ν (λ˜),Z˜t,θ(cid:1)(cid:12) (cid:12)2(cid:105)1/2
θ∼mβ,σ(ν α,(1)(λ˜)) (cid:12)δν α,(1) 1 δν α,(1) 1 (cid:12)
≤ √ 2L e(cid:16) 1+2R(γ˜ 8σ,ν α)+2(cid:90) ∥θ∥8γ˜ 8σ(dθ)+2g(γ˜ 8σ)α1 n−λ˜(cid:16) 2+∥Z 1t∥2+∥Z(cid:101)t 1∥2(cid:17)(cid:17) (42)
Θ t
×(cid:16) 2+∥Zt∥2+∥Z(cid:101)t ∥2(cid:17)
.
1 1
We also know
E (cid:104)(cid:16) δℓ (cid:0) m (λ),Zt,θ(cid:1)(cid:17)2(cid:105)1/2
θ∼mβ,σ(ν α,(1)(λ˜)) δm (1) 1
≤ E (cid:104) g (m (λ),θ)2(cid:105)1/2(cid:0) 1+∥Zt∥2(cid:1) (43)
θ∼mβ,σ(ν (λ˜)) e (1) 1
α,(1)
√ (cid:16) (cid:17)
≤ 2L 1+E (cid:2) ∥θ∥8(cid:3) +E (cid:2) ∥θ∥8(cid:3) (cid:0) 1+∥Zt∥2(cid:1) .
e θ∼mβ,σ(ν α,(1)(λ˜)) θ∼m (1)(λ) 1
By definition, m (λ) = m(ν )+λ(m (ν )−m(ν )), and hence
(1) α,(1) α α α,(1)
E (cid:2) ∥θ∥8(cid:3) = (1−λ)E (cid:2) ∥θ∥8(cid:3) +λE (cid:2) ∥θ∥8(cid:3) .
θ∼m (1)(λ) θ∼m(ν α,(1)) θ∼mα(να)
In a similar approach to (41), we have,
R(γ˜ 8σ,ν α,(1)) = R(γ˜ 8σ,ν α)+ nα(cid:16) ℓ(γ˜ 8σ,Z(cid:101)t 1)−ℓ(γ˜ 8σ,Z 1t)(cid:17)
t (44)
≤ R(γ˜ 8σ,ν α)+g(γ˜ 8σ) nα(cid:16) 2+∥Z(cid:101)t 1∥2+∥Z 1t∥2)(cid:17) .
t
As before, it follows from Lemma 6 that
E (cid:2) ∥θ∥8(cid:3) +E (cid:2) ∥θ∥8(cid:3)
θ∼mβ,σ(ν α,(1)(λ˜)) θ∼m (1)(λ)
= E (cid:2) ∥θ∥8(cid:3) +(1−λ)E (cid:2) ∥θ∥8(cid:3) +λE (cid:2) ∥θ∥8(cid:3)
θ∼mβ,σ(ν α,(1)(λ˜)) θ∼m(ν α,(1)) θ∼mα(να)
(cid:104) (cid:90) (cid:105) (cid:104) (cid:90) (cid:105)
≤ R(γ˜σ,ν (λ˜))+ ∥θ∥8γ˜σ(dθ) +(1−λ) R(γ˜σ,ν )+ ∥θ∥8γ˜σ(dθ)
8 α,(1) 8 8 α,(1) 8 (45)
Θ Θ
(cid:104) (cid:90) (cid:105)
+λ R(γ˜σ,ν )+ ∥θ∥8γ˜σ(dθ)
8 α 8
Θ
≤ 2R(γ˜ 8σ,ν α)+2(cid:90) ∥θ∥8γ˜ 8σ(dθ)+g(γ˜ 8σ)α(2− nλ˜−λ)(cid:16) 2+∥Z(cid:101)t 1∥2+∥Z 1t∥2(cid:17) .
Θ t
Therefore, substituting (39), (42), (43) and (45) into (38) and simplifying, we have
h(Zt ,Zs ,Z(cid:101)t )
nt ns 1
≤ 2 σβ 22 L e(cid:104) 1+2R(γ˜ 8σ,ν α)+2(cid:90) ∥θ∥8γ˜ 8σ(dθ)+2αg n(γ˜ 8σ)(cid:16) 2+∥Z(cid:101)t 1∥2+∥Z 1t∥2(cid:17)(cid:105) (cid:0) 1+∥Z 1t∥2(cid:1)
Θ t
×√ 2L e(cid:104) 1+2R(γ˜ 8σ,ν α)+2(cid:90) ∥θ∥8γ˜ 8σ(dθ)+2αg n(γ˜ 8σ)(cid:16) 2+∥Z 1t∥2+∥Z(cid:101)t 1∥2(cid:17)(cid:105)(cid:16) 2+∥Z 1t∥2+∥Z(cid:101)t 1∥2(cid:17)
Θ t
≤ √ 22 σβ 22 L2 e(cid:104) 1+2R(γ˜ 8σ,ν α)+2(cid:90) ∥θ∥8γ˜ 8σ(dθ)+2αg n(γ˜ 8σ)(cid:16) 2+∥Z 1t∥2+∥Z(cid:101)t 1∥2(cid:17)(cid:105)2
Θ t
×(cid:0) 1+∥Zt∥2(cid:1)(cid:16) 2+∥Zt∥2+∥Z(cid:101)t ∥2(cid:17)
.
1 1 1
(46)
27We have the bound
(cid:90) (cid:16)α (cid:88)nt (1−α) (cid:88)ns (cid:17)
R(γ˜σ,ν ) ≤ g(γ˜σ)(1+∥z∥2)ν (dz) = g(γ˜σ) (1+∥Zt∥2)+ (1+∥Zs∥2) . (47)
8 α 8 α 8 n j n j
Z t s
j=1 j=1
Therefore, using the inequality (a+b(c+d)) ≤ (a+cb)(c+d)/c for a,b,c,d ≥ 0,
h(Zt ,Zs ,Z(cid:101)t )
nt ns 1
≤ (cid:104) 1+2(cid:90) ∥θ∥8γ˜σ(dθ)+2g(γ˜ 8σ)(cid:16) 2+∥Zt∥2+∥Z(cid:101)t ∥2+ α (cid:88)nt (1+∥Zt∥2)+ (1−α) (cid:88)ns (1+∥Zs∥2)(cid:17)(cid:105)2
8 n 1 1 n j n j
Θ t t s
j=1 j=1
×√ 22β2 L2(cid:0) 1+∥Zt∥2(cid:1)(cid:16) 2+∥Zt∥2+∥Z(cid:101)t ∥2(cid:17)
σ2 e 1 1 1
√
2β2 (cid:104) (cid:90) αg(γ˜σ)(cid:105)2
≤ L2 1+2 ∥θ∥8γ˜σ(dθ)+4 8
2 σ2 e 8 n
Θ t
×(cid:16) 2+∥Zt∥2+∥Z(cid:101)t ∥2+ α
(cid:88)nt
(1+∥Zt∥2)+ (1−α)
(cid:88)ns
(1+∥Zs∥2)(cid:17)2(cid:0) 1+∥Zt∥2(cid:1)(cid:16) 2+∥Zt∥2+∥Z(cid:101)t ∥2(cid:17) .
1 1 n j n j 1 1 1
t s
j=1 j=1
(48)
Applying Lemma 5 we have
E (cid:104)(cid:16) 2+∥Zt∥2+∥Z(cid:101)t ∥2+ α
(cid:88)nt
(1+∥Zt∥2)+ (1−α)
(cid:88)ns
(1+∥Zs∥2)(cid:17)2
Zt nt,Zs ns,Z(cid:101)n 1 1 n
t
j n
s
j
j=1 j=1
×(cid:0) 1+∥Zt∥2(cid:1)(cid:16) 2+∥Zt∥2+∥Z(cid:101)t ∥2(cid:17)(cid:105)
1 1 1
≤ E
(cid:104)(cid:16) 2+∥Zt∥2+∥Zt∥2+α(1+∥Zt∥2)+(1−α)(1+∥Zs∥2)(cid:17)2(cid:0) 1+∥Zt∥2(cid:1)(cid:16) 2+∥Zt∥2+∥Zt∥2(cid:17)(cid:105)
Zt,Zs 1 1 1 1 1 1 1
1 1
(cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
≤ 2(2+α)2E Zt (1+∥Z 1t∥2)4 +2(1−α)2E Zt (1+∥Z 1t∥2)2 E Zs (1+∥Z 1s∥2)2 .
1 1 1
(49)
And therefore, substituting (49) and (48) in (37),
√
α 2β2 (cid:104) (cid:90) αg(γ˜σ)(cid:105)2
|gen(mβ(ν ),νt )| ≤ L2 1+2 ∥θ∥8γ˜σ(dθ)+4 8
α pop n 2 σ2 e 8 n
t Θ t
(cid:34) (cid:35)
(cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
× 2(2+α)2E Zt (1+∥Z 1t∥2)4 +2(1−α)2E Zt (1+∥Z 1t∥2)2 E Zs (1+∥Z 1s∥2)2
1 1 1
√
=
α 2β2 L2(cid:104) 1+2(cid:90) ∥θ∥8γ˜σ(dθ)+4αL m(1+E θ∼γ˜ 8σ[∥θ∥4])(cid:105)2
n 2 σ2 e 8 n
t Θ t
(cid:34) (cid:35)
(cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
× 2(2+α)2E Zt (1+∥Z 1t∥2)4 +2(1−α)2E Zt (1+∥Z 1t∥2)2 E Zs (1+∥Z 1s∥2)2
1 1 1
α √ β2 2 (cid:104) (cid:90) (cid:105)2
≤ 2 2 L2(1+αL )2(1+ )2 1+2 ∥θ∥8γ˜σ(dθ)+2E [∥θ∥4]
n σ2 e m n 8 θ∼γ˜ 8σ
t t Θ
(cid:34) (cid:35)
(cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
× 2(2+α)2E Zt (1+∥Z 1t∥2)4 +2(1−α)2E Zt (1+∥Z 1t∥2)2 E Zs (1+∥Z 1s∥2)2 .
1 1 1
(50)
28Theorem 5 (Full version). Under the same assumptions in Theorem 2, the following upper
bound holds on expected target population risk,
E(cid:0) mβ(ν ),νt (cid:1)
θ α pop
(cid:34)
c α2β2 (cid:104) (cid:105)
≤ t Comp(θ) (2+α)2E (1+∥Zt∥2)4
n
t
σ2 Z 1t 1
(cid:35)
(cid:104) (cid:105) (cid:104) (cid:105)
+(1−α)2E Zt (1+∥Z 1t∥2)2 E Zs (1+∥Z 1s∥2)2
1 1
(cid:34)
c (1−α)2β2 (cid:104) (cid:105)
+ s n σ2 Comp(θ) (3−α)2E Z 1s (1+∥Z 1s∥2)4
s
(cid:35)
(cid:104) (cid:105) (cid:104) (cid:105)
+α2E Zt (1+∥Z 1t∥2)2 E Zs (1+∥Z 1s∥2)2
1 1
+6L (1+L )(1−α)d (νs ,νt )
m m F2 pop pop
(cid:16) (cid:90) (cid:17)(cid:16) (cid:90) (cid:17)
× 1+ ∥θ∥4m¯ (dθ) 1+ ∥θ∥4γ˜σ(dθ)
α 8
Θ Θ
(cid:16) (cid:17)
× 1+αE Zt[(1+∥Z 1t∥2)]+(1−α)E Zs[(1+∥Z 1s∥2)]
1 1
σ2
+ KL(m¯ ∥γσ)
2β2 α
+L E[(1+∥Zt∥2)2]d (m¯ ,m¯t),
e 1 F4 α
where
√ 1
c = 2L2(1+αL )2(1+ )2 > 0,
t e m n
t
√ 1
c = 2L2(1+(1−α)L )2(1+ )2 > 0,
s e m n
s
(cid:104) (cid:90) (cid:90) (cid:105)2
Comp(θ) = 1+2 ∥θ∥8γ˜σ(dθ)+2 ∥θ∥4γ˜σ(dθ) ,
8 8
Θ Θ
and m¯ = inf R(m,να ) provided that KL(m¯ ∥γσ) < ∞ and m¯t =
α m∈P2(Θ) pop α
inf R(m,νt ).
m∈P2(Θ) pop
Proof of Theorem 5. From the optimization defining mβ, we know that
θ
σ2
R(mβ(ν ),ν ) ≤ Vβ(mβ(ν ),ν ) ≤ Vβ(m¯ ,ν ) = R(m¯ ,ν )+ KL(m¯ ∥γσ). (51)
θ α α θ α α α α α α 2β2 α
We also know that
R(mβ(ν ),νt ) = R(mβ(ν ),ν )+(1−α)[R(mβ(ν ),νt )−R(mβ(ν ),νs )].
θ α nt θ α α θ α nt θ α ns
Hence,
E(cid:0) mβ(ν ),νt (cid:1)
θ α pop
(cid:104) (cid:105)
= E R(mβ(ν ),νt )−R(m¯t,νt )
Zs ns,Zt nt θ α pop pop
(cid:104) (cid:105)
= E R(mβ(ν ),νt )−R(mβ(ν ),ν )+R(mβ(ν ),ν )−R(m¯t,νt )
Zs ns,Zt nt θ α pop θ α α θ α α pop
(cid:104) (cid:105)
≤ E R(mβ(ν ),νt )−R(mβ(ν ),νt )+(1−α)[R(mβ(ν ),νt )−R(mβ(ν ),νs )]
Zs ns,Zt nt θ α pop θ α nt θ α nt θ α ns
σ2
+ KL(m¯ ∥γσ)+R(m¯ ,να )−R(m¯t,νt )
2β2 α α pop pop
29(cid:104) (cid:105)
= E R(mβ(ν ),νt )−R(mβ(ν ),νt )
Zs ns,Zt nt θ α pop θ α nt
(cid:104)
+(1−α)E R(mβ(ν ),νt )−R(mβ(ν ),νt )
Zs ns,Zt nt θ α nt θ α pop
+R(mβ(ν ),νt )−R(mβ(ν ),νs )
θ α pop θ α pop
(cid:105)
+R(mβ(ν ),νs )−R(mβ(ν ),νs )
θ α pop θ α ns
σ2
+ KL(m¯ ∥γσ)+R(m¯ ,να )−R(m¯t,νt )
2β2 α α pop pop
= αgen(m (ν ),νt )+(1−α)gen(m (ν ),νs )
α α pop α α pop
+(1−α)E (cid:2) R(mβ(ν ),νt )−R(mβ(ν ),νs )(cid:3)
Zs ns,Zt nt θ α pop θ α pop
σ2
+ KL(m¯ ∥γσ)+R(m¯ ,να )−R(m¯t,νt )
2β2 α α pop pop
= αgen(m (ν ),νt )+(1−α)gen(m (ν ),νs )
α α pop α α pop
+(1−α)E (cid:2) R(mβ(ν ),νt )−R(mβ(ν ),νs )(cid:3)
Zs ns,Zt nt θ α pop θ α pop
+
σ2
KL(m¯ ∥γσ)+(cid:0) R(m¯ ,νt )−R(m¯t,νt )(cid:1)
2β2 α α pop pop
+(1−α)(cid:0) R(m¯ ,νs )−R(m¯ ,νt )(cid:1) .
α pop α pop
Note that we have,
(cid:104)(cid:90) (cid:105)
E ∥θ∥4mβ(ν ,dθ)
Zs ns,Zt nt
Θ
θ α
(cid:104) (cid:105) (cid:90)
≤ E R(γ˜σ,ν ) + ∥θ∥4γ˜σ(dθ)
Zs ,Zt 8 α 8
ns nt
Θ
(cid:16) (cid:17) (cid:90)
≤ g(γ˜ 8σ) αE Zt[(1+∥Z 1t∥2)]+(1−α)E Zs[(1+∥Z 1s∥2)] + ∥θ∥4γ˜ 8σ(dθ)
1 1
Θ
(cid:16) (cid:90) (cid:17)(cid:16) (cid:17)
≤ (1+L m) 1+ ∥θ∥4γ˜ 8σ(dθ) 1+αE Zt[(1+∥Z 1t∥2)]+(1−α)E Zs[(1+∥Z 1s∥2)] ,
1 1
Θ
where g(γ˜σ) = L (1+E [∥θ∥4]). Using Theorem 4, Lemma 3 and Lemma 2 implies that
8 m θ∼γ˜σ
8
E(cid:0) mβ(ν ),νt (cid:1)
θ α pop
≤ αgen(m (ν ),νt )+(1−α)gen(m (ν ),νs )
α α pop α α pop
+(1−α)(cid:2) R(mβ(ν ),νt )−R(mβ(ν ),νs )(cid:3)
θ α pop θ α pop
+
σ2
KL(m¯ ∥γσ)+(cid:0) R(m¯ ,νt )−R(m¯t,νt )(cid:1)
2β2 α α pop pop
+(1−α)(cid:0) R(m¯ ,νs )−R(m¯ ,νt )(cid:1)
α pop α pop
(cid:34) (cid:35)
c α2β2 (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
≤ nt
t
σ2 Comp(θ) (2+α)2E Z 1t (1+∥Z 1t∥2)4 +(1−α)2E Z 1t (1+∥Z 1t∥2)2 E Z 1s (1+∥Z 1s∥2)2
(cid:34) (cid:35)
c (1−α)2β2 (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
+ s n
s
σ2 Comp(θ) (3−α)2E Z 1s (1+∥Z 1s∥2)4 +(α)2E Z 1t (1+∥Z 1t∥2)2 E Z 1s (1+∥Z 1s∥2)2
+
σ2
KL(m¯ ∥γσ)+(cid:0) R(m¯ ,νt )−R(m¯t,νt )(cid:1)
2β2 α α pop pop
+(1−α)(cid:0) R(m¯ ,νs )−R(m¯ ,νt )(cid:1)
α pop α pop
+(1−α)E (cid:2) R(mβ(ν ),νt )−R(mβ(ν ),νs )(cid:3)
Zs nsZt nt θ α pop θ α pop
(cid:34) (cid:35)
c α2β2 (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
≤ nt
t
σ2 Comp(θ) (2+α)2E Z 1t (1+∥Z 1t∥2)4 +(1−α)2E Z 1t (1+∥Z 1t∥2)2 E Z 1s (1+∥Z 1s∥2)2
30(cid:34) (cid:35)
c (1−α)2β2 (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
+ s n
s
σ2 Comp(θ) (3−α)2E Z 1s (1+∥Z 1s∥2)4 +(α)2E Z 1t (1+∥Z 1t∥2)2 E Z 1s (1+∥Z 1s∥2)2
+L (1−α)d (νs ,νt )(cid:0) 2+E [∥θ∥4](cid:1)(cid:0) 2+E (cid:2)E [∥θ∥4](cid:3)(cid:1)
m F2 pop pop θ∼m¯α Zs nsZt nt θ∼mβ θ(να)
σ2
+ KL(m¯ ∥γσ)
2β2 α
+L E[(1+∥Zt∥2)2]d (m¯ ,m¯t)
e 1 F4 α
(cid:34) (cid:35)
c α2β2 (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
≤ nt
t
σ2 Comp(θ) (2+α)2E Z 1t (1+∥Z 1t∥2)4 +(1−α)2E Z 1t (1+∥Z 1t∥2)2 E Z 1s (1+∥Z 1s∥2)2
(cid:34) (cid:35)
c (1−α)2β2 (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
+ s n
s
σ2 Comp(θ) (3−α)2E Z 1s (1+∥Z 1s∥2)4 +(α)2E Z 1t (1+∥Z 1t∥2)2 E Z 1s (1+∥Z 1s∥2)2
+L (1−α)d (νs ,νt )
m F2 pop pop
(cid:16) (cid:17)(cid:16) (cid:90) (cid:16) (cid:17)(cid:17)
× 2+E θ∼m¯α[∥θ∥4] 2+ ∥θ∥4γ˜ 8σ(dθ)+g(γ˜ 8σ) αE
Z
1t[(1+∥Z 1t∥2)]+(1−α)E
Z
1s[(1+∥Z 1s∥2)]
Θ
σ2
+ KL(m¯ ∥γσ)
2β2 α
+L E[(1+∥Zt∥2)2]d (m¯ ,m¯t)
e 1 F4 α
(cid:34) (cid:35)
c α2β2 (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
≤ nt
t
σ2 Comp(θ) (2+α)2E Z 1t (1+∥Z 1t∥2)4 +(1−α)2E Z 1t (1+∥Z 1t∥2)2 E Z 1s (1+∥Z 1s∥2)2
(cid:34) (cid:35)
c (1−α)2β2 (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
+ s n
s
σ2 Comp(θ) (3−α)2E Z 1s (1+∥Z 1s∥2)4 +(α)2E Z 1t (1+∥Z 1t∥2)2 E Z 1s (1+∥Z 1s∥2)2
(cid:16) (cid:17)
+L (1−α)d (νs ,νt ) 2+E [∥θ∥4]
m F2 pop pop θ∼m¯α
(cid:16) (cid:16) (cid:90) (cid:17)(cid:16) (cid:17)(cid:17)
× 2+(1+L m) 1+ ∥θ∥4γ˜ 8σ(dθ) 1+αE Zt[(1+∥Z 1t∥2)]+(1−α)E Zs[(1+∥Z 1s∥2)]
1 1
Θ
σ2
+ KL(m¯ ∥γσ)
2β2 α
+L E[(1+∥Zt∥2)2]d (m¯ ,m¯t)
e 1 F4 α
(cid:34) (cid:35)
c α2β2 (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
≤ nt
t
σ2 Comp(θ) (2+α)2E Z 1t (1+∥Z 1t∥2)4 +(1−α)2E Z 1t (1+∥Z 1t∥2)2 E Z 1s (1+∥Z 1s∥2)2
(cid:34) (cid:35)
c (1−α)2β2 (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
+ s n
s
σ2 Comp(θ) (3−α)2E Z 1s (1+∥Z 1s∥2)4 +(α)2E Z 1t (1+∥Z 1t∥2)2 E Z 1s (1+∥Z 1s∥2)2
+6L (1+L )(1−α)d (νs ,νt )
m m F2 pop pop
(cid:16) (cid:17)(cid:16) (cid:90) (cid:17)(cid:16) (cid:17)
× 1+E θ∼m¯α[∥θ∥4] 1+ ∥θ∥4γ˜ 8σ(dθ) 1+αE
Z
1t[(1+∥Z 1t∥2)]+(1−α)E
Z
1s[(1+∥Z 1s∥2)]
Θ
σ2
+ KL(m¯ ∥γσ)
2β2 α
+L E[(1+∥Zt∥2)2]d (m¯ ,m¯t).
e 1 F4 α
It completes the proof.
D.3 Fine-tuning details and proofs
We first provide the similar results to Lemma 6, Lemma 6 and Proposition 1 for fine-tuning scenario.
31Lemma 10. Defining the set valued map
(cid:110) σ2
B (m ,ν) := m ∈ P (Θ ) : KL(m ∥γσ ) ≤ R(m γσ ,ν)
f c sp 8 sp 2β2 sp sp c sp
t
(cid:90) (cid:90) (cid:111)
and ∥θt ∥8m (dθt ) ≤ R(m γ˜σ ,ν)+ ∥θt ∥8γ˜σ (dθt ) ⊂ P (Θ ),
sp sp sp c sp sp sp sp 8 sp
Θsp Θsp
for any ν ∈ P 2(Z), the KL-regularized risk minimizer satisfies mβt(m c,ν) ∈ B(m c,ν).
Lemma 11. Defining the set valued map
(cid:110) σ2
B (ν) := m ∈ P (Θ) : KL(m∥γσ) ≤ R(γσ,ν),
f q 2β2 p p
s
(cid:90) (cid:90)
∥θ ∥qm (dθ ) ≤ R(γˆσ,ν)+ ∥θ ∥qγˆσ (dθ ),
c c c p c p,c c
Θc Θc
(cid:90) (cid:90) (cid:111)
γˆσ := γˆσ(dθs ) and m := m(dθt ) ⊂ P (Θ),
p,c p sp c sp q
Θsp Θsp
for any ν ∈ P (Z), q = 4,8, the KL-regularized risk minimizer satisfies mβ(ν) ∈ B (ν).
2 f
Proposition 2. Suppose Assumption 4 holds. Then
(cid:90) (cid:104)δS δS (cid:105)2
f (νt,m (νs),θ ,z)− f (νt,m (νs),θ ,z′) mβt(m (νs),νt;dθ )
δν c sp δν c sp sp c sp
Θ
(cid:16) (cid:17)
≤ 8L2 1+E [∥θ ∥8]+2E [∥θ ∥8] .
e θc∼mc(νs) c θsp∼mβ spt(mc(νs),νt) sp
where S := δR .
f δmsp
Proof of Proposition 2. Using the similar approach to approach to Proposition 1 and linearity,
δS δS
f (νt,m (νs),θ ,zt)− f (νt,m (νs),θ ,z˜t)
c sp c sp
δν δν
(cid:16) 2β2 (cid:17)−1(cid:104) δℓ δℓ (cid:105)
= id+ C (m (νs)mβt(m (νs),νt),·,zt)− (m (νs)mβt(m (νs),νt),·,z˜t) .
σ2 mβ spt(mc(νs),νt) δm
sp
c sp c δm
sp
c sp c
where C : L2(m ,Θ ) → L2(m ,Θ ) is defined as,
mβ spt(mc(νs),νt) sp sp sp sp
(cid:104) δ2R (cid:105)
C f(θt ) := E (m (νs)mβt(m (νs),νt),νt,θt ,θt,′) .
mβ spt(mc(νs),νt) sp θ s′ p∼mβ spt(mc(νs),νt) δm2
sp
c sp c sp sp
Note that this is bounded in L2(mβ (m (νs),νt)), in particular
sp c
(cid:90) (cid:104)δS δS (cid:105)2
f (νt,m (νs),θ ,zt)− f (νt,m (νs),θt ,z˜t) mβt(m (νs),νt;dθt )
δν c sp δν c sp sp c sp
Θsp
(cid:90) (cid:104) δℓ δℓ (cid:105)2
≤ (m (νs)mβ (m (νs),νt),·,zt)− (m (νs)mβt(m (νs),νt),·,z˜t) mβt(m (νs),νt;dθt )
δm c sp c δm c c sp c sp
Θsp sp sp
(cid:90) (cid:104) δℓ (cid:105)2
≤ 2 (m (νs)mβ (m (νs),νt),·,zt)
δm c sp c
Θsp sp
(cid:104) δℓ (cid:105)2
+ (m (νs)mβ (m (νs),νt),·,z˜t) mβt(m (νs),νt;dθt ).
δm c sp c sp c sp
sp
Assumption 4(i) yields
(cid:90) (cid:104)δS δS (cid:105)2
f (νt,m (νs),θt ,zt)− f (νt,m (νs),θt ,z˜t) mβt(m (νs),νt;dθt )
δν c sp δν c sp sp c sp
Θsp
≤
2(cid:16) 2+∥zt∥2+∥z˜t∥2(cid:17)2(cid:90)
(cid:2) g (m (νs)mβt(m (νs),νt),θt )(cid:3)2 mβt(m (νs),νt;dθt )
sp c sp c sp sp c sp
Θsp
(cid:16) (cid:17)2(cid:16) (cid:17)
≤ 8L 2+∥zt∥2+∥z˜t∥2 1+E [∥θ ∥8]+2E [∥θt ∥8] .
e θc∼mc(νs) c θ st p∼mβ spt(mc(νs),νt) sp
32where g (m (νs)mβt(m (νs),νt),θt ) = L (1+E [∥θ ∥4]+E [∥θt ∥4]+∥θt ∥4).
sp c sp c sp e θc∼mc(νs) c θ st p∼mβ spt(mc(νs),νt) sp sp
The final result follows from Assumption 4(ii).
Theorem 6 (Full Version). Given Assumption 4, E (cid:2) ∥Z∥8(cid:3) < ∞ and E (cid:2) ∥Z∥4(cid:3) <
Z∼νt Z∼νs
pop pop
∞, the weak transfer generalization error under fine-tuning satisfies
2 2 16β2
|gen(m (νs )mt,βt(m (νs ),νt ),νt ),νt )| ≤ (1+ )2 L2(1+L )2Comp(θ ,θt ,θs )
c ns sp c ns nt nt pop n n σ2 e m c sp sp
t t
(cid:104) (cid:105) (cid:104) (cid:105)
×E Zs (1+∥Z 1s∥2)2 E Zt (1+∥Z 1t∥2)4 ,
1 1
where
(cid:104) (cid:90)
Comp(θ ,θt ,θs ) = 1+ ∥θ ∥4(2+∥θ ∥4)γˆσ (dθ )
c sp sp c c 8,c c
Θc
(cid:90) (cid:90) (cid:105)2
+ ∥θt ∥4(1+2∥θt ∥4)γ˜σ (dθt )+ ∥θs ∥4γˆσ (dθs ) .
sp sp sp sp sp 8,sp sp
Θsp Θsp
Proof of Theorem 6. Recall from (3) that νt (λ) = νt +λ(νt −νt ). From Theorem 3,
(1) nt,(1) nt nt,(1)
gen(m cm sp,ν pt op) = n1 tE
Zt nt,Zs
ns,Z(cid:101)t
1(cid:2) K(Zt nt,Zs ns,Z(cid:101)t 1)(cid:3) , (52)
where
K(Zt nt,Zs ns,Z(cid:101)t 1) = (cid:90) 1(cid:90) (cid:16) δmδℓ (cid:0) mf (1)(λ),Z(cid:101)t 1,θ sp(cid:1)(cid:17)
0 Θsp sp
(53)
(cid:32) (cid:33)
(cid:90) 1(cid:90) δm
× sp (m (νs ),νt (λ ),z)(dθ )(δ −δ )(dz)dλ dλ,
0 Z
δν c ns (1) 1 sp Z 1t Z(cid:101)t
1
1
As E (cid:2)δS f(cid:0) m (νs ),νt (λ˜),z,θ (cid:1)(cid:3) ≡ 0 (by definition of S = δR/δm and the
θsp∼mβ spt(mc(ν ns s),ν (t 1)(λ˜)) δν c ns (1) sp f sp
normalization condition and chain rule), inspired by Lemma 9 we know
(cid:90) (cid:16) δℓ (cid:0) mf (λ),Zt,θ (cid:1)(cid:17)
δm (1) 1 sp
Θsp sp
×(cid:16)δm δνsp(cid:0) m c(ν ns s),ν (t 1)(λ˜),Z 1t(cid:1) − δm δνsp(cid:0) m c(ν ns s),ν (t 1)(λ˜),Z(cid:101)t 1(cid:1)(cid:17) (dθ sp)
= −2β t2 Cov (cid:104) δℓ (cid:0) mf (λ),Zt,θ (cid:1)
σ2 θsp∼mβ spt(mc(ν ns s),ν (t 1)(λ˜)) δm
sp
(1) 1 sp
δS δS (cid:105)
, f(cid:0) m (νs ),νt (λ˜),Zt,θ (cid:1) − f(cid:0) m (νs ),νt (λ˜),Z˜t,θ (cid:1)
δν c ns (1) 1 sp δν c ns (1) 1 sp
≤ 2β t2 E (cid:104)(cid:16) δℓ (cid:0) mf (λ),Zt,θ (cid:1)(cid:17)2(cid:105)1/2
σ2 θsp∼msβ pt(mc(ν ns s),ν (t 1)(λ˜)) δm
sp
(1) 1 sp
×E
(cid:104)(cid:16)δS
f(cid:0) m (νs ),νt (λ˜),Zt,θ (cid:1) −
δS
f(cid:0) m (νs ),νt (λ˜),Z˜t,θ
(cid:1)(cid:17)2(cid:105)1/2
.
θsp∼mβ spt(mc(ν ns s),ν (t 1)(λ˜)) δν c ns (1) 1 sp δν c ns (1) 1 sp
(54)
Using Proposition 2 and (24),
E
(cid:104)(cid:16)δS
f(cid:0) m (νs ),νt (λ˜),Zt,θ (cid:1) −
δS
f(cid:0) m (νs ),νt (λ˜),Z˜t,θ
(cid:1)(cid:17)2(cid:105)1/2
θsp∼mβ spt(mc(ν ns s),ν (t 1)(λ˜)) δν c ns (1) 1 sp δν c ns (1) 1 sp
(55)
≤ 2√ 2L e(cid:16) 1+2E
θsp∼mβ spt(mβ cs(ν ns s),ν (t
1)(λ˜))(cid:2) ∥θ sp∥8(cid:3) +E
θc∼mβ cs(ν ns
s)(cid:2) ∥θ c∥8(cid:3)(cid:17)(cid:16) 2+∥Z 1t∥2+∥Z(cid:101)t 1∥2(cid:17) ,
33From Lemma 10, we know
(cid:90)
E (cid:2) ∥θ ∥8(cid:3) ≤ R(m (νs )γ˜σ ,νt (λ˜))+ ∥θ ∥8γ˜σ (dθt ),
θsp∼mβ spt(mc(ν ns s),ν (t 1)(λ˜)) sp c ns sp (1)
Θsp
sp sp sp
and from Lemma 11, we have,
(cid:90)
E (cid:2) ∥θ ∥8(cid:3) ≤ R(γˆσ,νs )+ ∥θ ∥8γˆσ (dθ ),
θc∼mβ cs(ν ns s) c p ns
Θc
c 8,c c
where γσ (dθ ) = (cid:82) γˆσ(dθ ). By construction,
p,c c Θsp p sp
νt (λ˜) = νt +
1−λ˜
(cid:0) δ −δ (cid:1) ,
(1) nt n
t
Z(cid:101)t
1
Z 1t
so
R(m c(ν ns s)γ˜ sσ p,ν (t 1)(λ˜)) = R(m c(ν ns s)γ˜ sσ p,ν nt t)+ 1 n−λ˜(cid:16) ℓ(m c(ν ns s)γ˜ sσ p,Z(cid:101)t 1)−ℓ(m c(ν ns s)γ˜ sσ p,Z 1t)(cid:17)
t (56)
≤ R(m c(ν ns s)γ˜ sσ p,ν nt t)+g(m c(ν ns s)γ˜ sσ p)1 n−λ˜(cid:16) 2+∥Z(cid:101)t 1∥2+∥Z 1t∥2)(cid:17) .
t
We also have,
R(γˆσ,νs ) ≤ g(γˆσ γˆσ ) 1
(cid:88)ns
(1+(cid:13) (cid:13)Zs(cid:13) (cid:13)2 ), (57)
p ns 8,c 8,sp n j
s
j=1
and hence
E
(cid:104)(cid:16)δS
f(cid:0) m (νs ),νt (λ˜),Zt,θ (cid:1) −
δS
f(cid:0) m (νs ),νt (λ˜),Z˜t,θ
(cid:1)(cid:17)2(cid:105)1/2
θsp∼mβ spt(mc(ν ns s),ν (t 1)(λ˜)) δν c ns (1) 1 sp δν c ns (1) 1 sp
√ (cid:16) (cid:90)
≤ 2 2L 1+2R(m (νs )γ˜σ ,νt )+2 ∥θ ∥8γ˜σ (dθt )
e c ns sp nt sp sp sp
Θsp
+2g(m c(ν ns s)γ˜ sσ p)1 n−λ˜(cid:16) 2+∥Z 1t∥2+∥Z(cid:101)t 1∥2(cid:17) +g(γˆ 8σ ,cγˆ 8σ ,sp) n1 (cid:88)ns (1+(cid:13) (cid:13)Z js(cid:13) (cid:13)2 )+(cid:90) ∥θ c∥8γˆ 8σ ,c(dθ c)(cid:17)
t s
j=1
Θc
×(cid:16) 2+∥Zt∥2+∥Z(cid:101)t ∥2(cid:17)
.
1 1
(58)
Bydefinition,mf (λ) = m (νs )m (m (νs ),νt )(λ),wherem (m (νs ),νt )(λ) := m (m (νs ),νt )+
(1) c ns sp c ns (1) sp c ns (1) sp c ns nt,(1)
λ(m (m (νs ),νt )−m (m (νs ),νt )), we also have
sp c ns nt sp c ns nt,(1)
E (cid:104)(cid:16) δℓ (cid:0) mf (λ),Zt,θ (cid:1)(cid:17)2(cid:105)1/2
θsp∼mβ spt(mc(ν ns s),ν (t 1)(λ˜)) δm
sp
(1) 1 sp
≤ E
(cid:104)
g (mf (λ),θ
)2(cid:105)1/2(cid:0)
1+∥Zt∥2(cid:1)
θsp∼mβ spt(mc(ν ns s),ν (t 1)(λ˜)) sp (1) sp 1 (59)
√ (cid:16)
≤ 2L 1+E (cid:2) ∥θ ∥8(cid:3)
e θsp∼mβ spt(mc(ν ns s),ν (t 1)(λ˜)) sp
(cid:17)
+E (cid:2) ∥θ ∥8(cid:3) +E (cid:2) ∥θ ∥8(cid:3) (cid:0) 1+∥Zt∥2(cid:1) .
θsp∼mβ spt(mc(ν ns s),ν (t 1))(λ) sp θc∼mc(ν ns s) c 1
and hence
E (cid:2) ∥θ ∥8(cid:3) = (1−λ)E (cid:2) ∥θ ∥8(cid:3) +λE (cid:2) ∥θ ∥8(cid:3) .
mβ spt(mc(ν ns s),ν (t 1))(λ) sp θsp∼msp(mc(ν ns s),ν nt t,(1)) sp θsp∼msp(mc(ν ns s),ν nt t) sp
34As before, it follows from Lemma 10 that
E (cid:2) ∥θ ∥8(cid:3) +E (cid:2) ∥θ ∥8(cid:3)
mβ spt(mc(ν ns s),ν (t 1))(λ) sp mβ spt(mc(ν ns s),ν (t 1)(λ˜)) sp
= E (cid:2) ∥θ ∥8(cid:3) +(1−λ)E (cid:2) ∥θ ∥8(cid:3) +λE (cid:2) ∥θ ∥8(cid:3)
θsp∼mβ spt(mc(ν ns s),ν (t 1)(λ˜)) sp θsp∼msp(mc(ν ns s),ν nt t,(1)) sp θsp∼msp(mc(ν ns s),ν nt t) sp
(cid:104) (cid:90) (cid:105) (cid:104) (cid:90) (cid:105)
≤ R(m (νs )γ˜σ ,νt (λ˜))+ ∥θ ∥8γ˜σ (dθt ) +(1−λ) R(m (νs )γ˜σ ,νt )+ ∥θ ∥8γ˜σ (dθt )
c ns sp (1) sp sp sp c ns sp nt,(1) sp sp sp
Θsp Θsp
(cid:104) (cid:90) (cid:105)
+λ R(m (νs )γ˜σ ,νt )+ ∥θ ∥8γ˜σ (dθt )
c ns sp nt sp sp sp
Θsp
≤ 2R(m c(ν ns s)γ˜ sσ p,ν nt t)+2(cid:90) ∥θ sp∥8γ˜ sσ p(dθ)+g(m c(ν ns s)γ˜ sσ p)(2− nλ˜−λ)(cid:16) 2+∥Z(cid:101)t 1∥2+∥Z 1t∥2(cid:17)
Θsp t
(cid:90)
≤ 2R(m (νs )γ˜σ ,νt )+2 ∥θ ∥8γ˜σ (dθ)
c ns sp nt sp sp
Θsp
+(cid:16) 1+g(γˆσ γˆσ ) 1 (cid:88)ns (cid:0) 1+∥Zs∥2(cid:1) +(cid:90) ∥θ ∥4γˆσ (θ )dθ +E [∥θ ∥4](cid:17)
8,c 8,sp n j c 8,c c c θsp∼γ˜ sσ p sp
s
j=1
Θc
×
(2−λ˜−λ)(cid:16) 2+∥Z(cid:101)t ∥2+∥Zt∥2(cid:17)
.
n 1 1
t
(60)
It also follows from Lemma 11,
E (cid:2) ∥θ ∥4(cid:3)
θc∼mc(ν ns s) c
(cid:90)
≤ R(γˆσ,νs )+ ∥θ ∥4γˆσ (θ )dθ
p ns c 8,c c c
(61)
Θc
1
(cid:88)ns (cid:90)
≤ g(γˆσ γˆσ ) (1+∥Zs∥2)+ ∥θ ∥4γˆσ (θ )dθ .
8,c 8,sp n j c 8,c c c
s
j=1
Θc
We also have the following bound on R(m (νs )γ˜σ ,νt ) from Lemma 11,
c ns sp nt
R(m (νs )γ˜σ ,νt )
c ns sp nt
(cid:90)
≤ g(m (νs )γ˜σ )(1+∥z∥2)νt (dz)
c ns sp nt
Z
(cid:104) 1
(cid:88)nt
(cid:105)
= g(m (νs )γ˜σ )E (1+∥Zt∥2)
c ns sp n j
t
j=1
≤ L (cid:0) 1+E [∥θ ∥4]+E [∥θ ∥4](cid:1)(cid:16) 1
(cid:88)nt
(1+∥Zt∥2)(cid:17)
m θc∼mc(ν ns s) c θsp∼γ˜ sσ p sp n
t
j
j=1
(cid:16) (cid:90) (cid:17)(cid:16) 1 (cid:88)nt (cid:17)
≤ L 1+R(γˆσ,νs )+ ∥θ ∥4γˆσ (dθ )+E [∥θ ∥4] (1+∥Zt∥2)
m p ns c 8,c c θsp∼γ˜ sσ p sp n j
Θc t
j=1
≤ L (cid:16) 1+g(γˆσ γˆσ ) 1 (cid:88)ns (cid:0) 1+∥Zs∥2(cid:1) +(cid:90) ∥θ ∥4γˆσ (θ )dθ +E [∥θ ∥4](cid:17)(cid:16) 1 (cid:88)nt (1+∥Zt∥2)(cid:17) .
m 8,c 8,sp n j c 8,c c c θsp∼γ˜ sσ p sp n j
s
j=1
Θc t
j=1
(62)
Therefore, substituting (54), (58), (59) and (60) into (53) and simplifying and denoting A :=
c
(cid:82) ∥θ ∥8γˆσ (dθ ),B := (cid:82) ∥θ ∥8γ˜σ (dθt ),A = (cid:82) ∥θ ∥4γˆσ (θ )dθ andB = (cid:82) ∥θ ∥4γ˜σ (dθt ),
wΘ echavc
e,
8,c c p Θsp sp sp sp c,4 Θc c 8,c c c p,4 Θsp sp sp sp
k(Zt ,Zs ,Z(cid:101)t ,λ,λ˜)
nt ns 1
352β2 √ (cid:16) (cid:90)
≤ 2 2L 1+2R(m (νs )γ˜σ ,νt )+2 ∥θ ∥8γ˜σ (dθt )
σ2 e c ns sp nt sp sp sp
Θsp
+2g(m c(ν ns s)γ˜ sσ p)1 n−λ˜(cid:16) 2+∥Z 1t∥2+∥Z(cid:101)t 1∥2(cid:17) +2g(γˆ 8σ ,cγˆ 8σ ,sp) n1 (cid:88)ns (1+(cid:13) (cid:13)Z js(cid:13) (cid:13)2 )+(cid:90) ∥θ c∥8γˆ 8σ ,c(dθ c)(cid:17)
t s
j=1
Θc
×(cid:16) 2+∥Zt∥2+∥Z(cid:101)t ∥2(cid:17)
1 1
(cid:16)
×L 1+E (cid:2) ∥θ ∥8(cid:3)
e θsp∼mβ spt(mc(ν ns s),ν (t 1)(λ˜)) sp
(cid:17)
+E (cid:2) ∥θ ∥8(cid:3) +E (cid:2) ∥θ ∥8(cid:3) (cid:0) 1+∥Zt∥2(cid:1)
θsp∼mβ spt(mc(ν ns s),ν (t 1))(λ) sp θc∼mc(ν ns s) c 1
≤
2β2 8L2(cid:0) 1+∥Zt∥2(cid:1)(cid:16) 2+∥Zt∥2+∥Z(cid:101)t ∥2(cid:17)
σ2 e 1 1 1
×(cid:32)
1+2g(cid:0) γˆσ γˆσ (cid:1) 1
(cid:88)ns
(1+∥Zs∥2)+A +2B +2G (cid:16) 1
(cid:88)nt
(1+∥Zt∥2)(cid:17)
8,c 8,sp n j c p 1 n j
s t
j=1 j=1
(cid:33)
+2G
11 n−λ˜(cid:16)
2+∥Z
1t∥2+∥Z(cid:101)t 1∥2(cid:17)
t
×(cid:32)
1+2g(cid:0) γˆ 8σ ,cγˆ 8σ ,sp(cid:1) n1
(cid:88)ns
(1+(cid:13) (cid:13)Z js(cid:13) (cid:13)2 )+A c+2B p+2G 1(cid:16) n1
(cid:88)nt
(1+∥Z jt∥2)(cid:17)
s t
j=1 j=1
(cid:33)
+G 1(2− nλ˜−λ)(cid:16) 2+∥Z(cid:101)t 1∥2+∥Z 1t∥2(cid:17) ,
t
(cid:16) (cid:17)
where G = L 1+g(γˆσ γˆσ ) 1 (cid:80)ns (1+∥Zs∥2)+A +B .
1 m 8,c 8,sp ns j=1 j c,4 p,4
More simplifying with respect to λ and λ˜,
K(Zt ,Zs ,Z(cid:101)t )
nt ns 1
≤
2β2 8L2(cid:0) 1+∥Zt∥2(cid:1)(cid:16) 2+∥Zt∥2+∥Z(cid:101)t ∥2(cid:17)
σ2 e 1 1 1
×(cid:32)
1+2g(cid:0) γˆσ γˆσ (cid:1) 1
(cid:88)ns
(1+∥Zs∥2)+A +2B +2G (cid:16) 1
(cid:88)nt
(1+∥Zt∥2)(cid:17) (63)
8,c 8,sp n j c p 1 n j
s t
j=1 j=1
(cid:33)2
+G
1
n2 (cid:16)
2+∥Z
1t∥2+∥Z(cid:101)t 1∥2(cid:17)
t
And therefore, applying Lemma 5 and substituting (63) in (52),
2 1 16β2
|gen(m (νs )m (m (νs ),νt ),νt )| ≤ (1+ )2 L2(1+L )2
c ns sp c ns nt pop n n σ2 e m
t t
(cid:104) (cid:90) (cid:90) (cid:90) (cid:90) (cid:105)2
× 1+g(γˆσ γˆσ )+ ∥θ ∥8γˆσ (dθ )+2 ∥θ ∥8γ˜σ (dθt )+ ∥θ ∥4γˆσ (dθ )+ ∥θ ∥4γ˜σ (dθt )
8,c 8,sp c 8,c c sp sp sp c 8,c c sp sp sp
Θc Θsp Θc Θsp
(cid:104) (cid:105) (cid:104) (cid:105)
×E Zs (1+∥Z 1s∥2)2 E Zt (1+∥Z 1t∥2)4
1 1
2 2 16β2
≤ (1+ )2 L2(1+L )2
n n σ2 e m
t t
(cid:104) (cid:90) (cid:90) (cid:90)
× 1+ ∥θ ∥8γˆσ (dθ )+2 ∥θ ∥8γ˜σ (dθt )+2 ∥θ ∥4γˆσ (dθ )
c 8,c c sp sp sp c 8,c c
Θc Θsp Θc
(cid:90) (cid:90) (cid:105)2
+ ∥θt ∥4γ˜σ (dθt )+ ∥θs ∥4γˆσ (dθs )
sp sp sp sp 8,sp sp
Θsp Θsp
(cid:104) (cid:105) (cid:104) (cid:105)
×E Zs (1+∥Z 1s∥2)2 E Zt (1+∥Z 1t∥2)4 .
1 1
(64)
36Theorem 7 (Full version). Under Assumption 4 and Assumption 3, the following upper bound
holds on the WTER under fine-tuning scenario,
E(mβs(νs )mβt(mβs(νs ),νt ),νt )
c ns sp c ns nt pop
2β216 2 (cid:104) (cid:105) (cid:104) (cid:105)
≤ σ2t n t(1+ n t)2L2 e(1+L m)2Comp(θ c,θ st p,θ ss p)E Z 1s (1+∥Z 1s∥2)2 E Z 1t (1+∥Z 1t∥2)4
σ2
+ KL(m˜t ∥γ˜σ )
2β2 sp sp
t √
2β29 2 2 (cid:104) (cid:105)
+ s (1+ )2L2(1+L )2Comp(θ)E (1+∥Zs∥2)4
σ2 n
s
n
s
e m Z 1t 1
σ2
+ KL(m¯s⊗m¯s ∥γσ ⊗γσ )
2β2 c sp c sp
s
(cid:16) (cid:90) (cid:90) (cid:17)
+4 1+ ∥θ ∥4γˆσ (dθ )+ ∥θs ∥4γˆσ (dθs )
c 8,c c sp 8,sp sp
Θc Θsp
(cid:16) (cid:90) (cid:90) (cid:90) (cid:17)
× 1+ ∥θ c∥4m¯s c(dθ c)+ ∥θ ss p∥4m¯s sp(dθ ss p)+ ∥θ st p∥4m˜t sp(dθ st p) E Zs(cid:2) (1+∥Z 1s∥2)(cid:3)
1
Θc Θsp Θsp
×d (νs ,νt )
F2 pop pop
+L eE
Z
1s[(1+∥Z 1s∥2)2]d F4(m˜s sp,m˜t sp)
+L E [(1+∥Zt∥2)2]d (m¯s⊗m¯s ,m¯t ⊗m¯t ),
e Z 1t 1 F4 c sp c sp
(65)
where
(cid:104) (cid:90) (cid:105)2
Comp(θ) = 1+2 ∥θ∥8γ˜σ(dθ)+2E [∥θ∥4] ,
8 θ∼γ˜σ
8
Θ
(cid:104) (cid:90)
Comp(θ ,θt ,θs ) = 1+ ∥θ ∥4(2+∥θ ∥4)γˆσ (dθ )
c sp sp c c 8,c c
Θc
(cid:90) (cid:90) (cid:105)2
+ ∥θt ∥4(1+2∥θt ∥4)γ˜σ (dθt )+ ∥θs ∥4γˆσ (dθs ) ,
sp sp sp sp sp 8,sp sp
Θsp Θsp
m˜t = argmin R(mβs(νs )m ,νt ),
sp c ns sp pop
msp∈P4(Θsp)
provided that KL(m˜t ∥γ˜σ ) < ∞,
sp sp
m˜s = argmin R(mβs(νs )m ,νs ),
sp c ns sp pop
msp∈P4(Θsp)
m¯s⊗m¯s = argmin R(m¯s⊗m¯s ,νs ),
c sp c sp pop
m¯s c⊗m¯s sp∈P4(Θc)×P4(Θsp)
and
m¯t ⊗m¯t = argmin R(m¯t ⊗m¯t ,νt ).
c sp c sp pop
m¯t c⊗m¯t sp∈P4(Θc)×P4(Θsp)
37Proof of Theorem 7.
E(mβs(νs )mβt(mβs(νs ),νt ),νt )
c ns sp c ns nt pop
= E [R(mβs(νs )mβt(mβs(νs ),νt ),νt )]−R(m¯t ⊗m¯t ,νt )
Zt nt,Zs
ns
c ns sp c ns nt pop c sp pop
= E [R(mβs(νs )mβt(mβs(νs ),νt ),νt )]−E [R(mβs(νs )mβt(mβs(νs ),νt ),νt )]
Zt nt,Zs
ns
c ns sp c ns nt pop Zt nt,Zs
ns
c ns sp c ns nt nt
(cid:124) (cid:123)(cid:122) (cid:125)
I1
+E [R(mβs(νs )mβt(mβs(νs ),νt ),νt )]−E [R(mβs(νs )m˜t ,νt )]
Zt nt,Zs
ns
c ns sp c ns nt nt Zt nt,Zs
ns
c ns sp pop
(cid:124) (cid:123)(cid:122) (cid:125)
I2
+E [R(mβs(νs )m˜t ,νt )]−E [R(mβs(νs )m˜t ,νs )]
Zt nt,Zs
ns
c ns sp pop Zt nt,Zs
ns
c ns sp pop
(cid:124) (cid:123)(cid:122) (cid:125)
I3
+E [R(mβs(νs )m˜t ,νs )]−E [R(mβs(νs )m˜s ,νs )]
Zt nt,Zs
ns
c ns sp pop Zt nt,Zs
ns
c ns sp pop
(cid:124) (cid:123)(cid:122) (cid:125)
I4
+E [R(mβs(νs )m˜s ,νs )]−E [R(mβs(νs )mβs(mβs(νs ),νs ),νs ),νs )]
Zt nt,Zs
ns
c ns sp pop Zt nt,Zs
ns
c ns sp c ns ns ns pop
(cid:124) (cid:123)(cid:122) (cid:125)
I5
+E [R(mβs(νs )mβs(mβs(νs ),νs ),νs ),νs )]−E [R(mβs(νs )mβs(mβs(νs ),νs ),νs ),νs )]
Zt nt,Zs
ns
c ns sp c ns ns ns pop Zt nt,Zs
ns
c ns sp c ns ns ns ns
(cid:124) (cid:123)(cid:122) (cid:125)
I6
+E [R(mβs(νs )mβs(mβs(νs ),νt ),νs ),νs )]−R(m¯s⊗m¯s ,νs )
Zt nt,Zs
ns
c ns sp c ns nt ns ns c sp pop
(cid:124) (cid:123)(cid:122) (cid:125)
I7
+R(m¯s⊗m¯s ,νs )−R(m¯s⊗m¯s ,νt )
c sp pop c sp pop
(cid:124) (cid:123)(cid:122) (cid:125)
I8
+R(m¯s⊗m¯s ,νt )−R(m¯t ⊗m¯t ,νt ),
c sp pop c sp pop
(cid:124) (cid:123)(cid:122) (cid:125)
I9
(66)
where m¯T = (cid:82) m¯Tdθ , m¯T = (cid:82) m¯Tdθ , m¯T = arginf R(m,νT ) for T ∈ {s,t} and
c Θsp sp sp Θc c m∈P(Θ) pop
m˜t = argmin R(m (νs )m ,νt ). Using the Theorem 6 result, we have the following
sp msp∈P(Θsp) c ns sp pop
bound on term I in (66),
1
E [R(mβs(νs )mβt(mβs(νs ),νt ),νt )]
Zt nt,Zs
ns
c ns sp c ns nt pop
−E
Zt nt,Zs
ns[R(mβ cs(ν ns s)mβ spt(mβ cs(ν ns s),ν nt t),ν nt t)]
(67)
2β216 2 (cid:104) (cid:105) (cid:104) (cid:105)
≤ σ2t n t(1+ n t)2L2 e(1+L m)2Comp(θ c,θ st p,θ ss p)E Z 1s (1+∥Z 1s∥2)2 E Z 1t (1+∥Z 1t∥2)4 .
Using the definition of m¯ and the KL-regularized, we have the following bound on term I in (66),
sp 2
E [R(mβs(νs )mβt(mβs(νs ),νt ),νt )]−E [R(mβs(νs )m˜t ,νt )]
Zt nt,Zs
ns
c ns sp c ns nt nt Zt nt,Zs
ns
c ns sp pop
(68)
σ2
≤ KL(m¯t ∥γ˜σ ).
2β2 sp sp
t
Using Lemma 3, we have the following bound on term I in (66),
3
E [R(mβs(νs )m˜t ,νt )]−E [R(mβs(νs )m˜t ,νs )]
Zt nt,Zs
ns
c ns sp pop Zt nt,Zs
ns
c ns sp pop
≤ L (cid:0) 1+E [∥θ ∥4]+E [∥θt ∥4](cid:1) d (νs ,νt )
≤
2Lm
mE Zs(cid:2)
(θ 1c∼ +m ∥β cs Z(ν 1sns
∥s
2)
)(cid:3)
c θsp∼m˜t sp sp F2 pop pop
(69)
1
(cid:16) (cid:90) (cid:90) (cid:90) (cid:17)
× 1+ ∥θ ∥4γˆσ (dθ )+ ∥θs ∥4γˆσ (dθs )+ ∥θ ∥4m˜t (dθt ) d (νs ,νt ).
c 8,c c sp 8,sp sp sp sp sp F2 pop pop
Θc Θsp Θsp
38where the last inequality follows from applying Lemma 11,
E (cid:2) ∥θ ∥4(cid:3)
θc∼mc(ν ns s) c
(cid:90)
≤ R(γˆσ,νs )+ ∥θ ∥4γˆσ (θ )dθ
p ns c 8,c c c
(70)
Θc
1
(cid:88)ns (cid:90)
≤ g(γˆσ γˆσ ) (1+∥Zs∥2)+ ∥θ ∥4γˆσ (θ )dθ ,
8,c 8,sp n j c 8,c c c
s
j=1
Θc
Using a similar approach to Lemma 2, we have the following bound on term I in (66),
4
E [R(mβs(νs )m˜t ,νs )]−E [R(mβs(νs )m˜s ,νs )]
Zt nt,Zs
ns
c ns sp pop Zt nt,Zs
ns
c ns sp pop
(cid:16) (cid:17)
≤ L eE
Z
1s[(1+∥Z 1s∥2)2] 1+E
Zs
ns(cid:2)E
θc∼mβ cs(ν ns
s)[∥θ c∥4](cid:3) +E
θsp∼m˜s sp+ 2m˜t
sp[∥θ sp∥4] (71)
×d (m˜s ,m˜t ).
F4 sp sp
Using the definition of m˜s , we have the following bound on term I in (66),
sp 5
E [R(mβs(νs )m˜s ,νs )]−E [R(mβs(νs )mβs(mβs(νs ),νs ),νs ),νs )] ≤ 0. (72)
Zt nt,Zs
ns
c ns sp pop Zt nt,Zs
ns
c ns sp c ns ns ns pop
Using the same approach in Theorem 6 in the first step under Assumption 3, we have the following
bound on term I in (66),
6
E [R(mβs(νs )mβt(mβs(νs ),νs ),νs ),νs )]
Zt nt,Zs
ns
c ns sp c ns ns ns pop
−E [R(mβs(νs )mβs(mβs(νs ),νs ),νs ),νs )]
√
Zt nt,Zs
ns
c ns sp c ns ns ns ns (73)
9 2 1 2β2 (cid:104) (cid:105)
≤ L2(1+L )2(1+ )2 sComp(θ)E (1+∥Zs∥2)4 .
n
s
e m n
s
σ2 Z 1t 1
Furthermore, we have the following bound on term I in (66),
7
E [R(mβs(νs )mβt(mβs(νs ),νt ),νs ),νs )]−R(m¯sm¯s ,νs )
Zt nt,Zs
ns
c ns sp c ns nt ns ns c sp pop
(74)
σ2
≤ KL(m¯sm¯s ∥γσ ⊗γσ ).
2β2 c sp c sp
s
Using Lemma 3, we have the following bound on term I in (66),
8
R(m¯sm¯s ,νs )−R(m¯sm¯s ,νt )
c sp pop c sp pop (75)
≤ (1+E [∥θ ∥4]+E [∥θ ∥4])d (νs ,νt ).
θc∼m¯s c c θsp∼m¯s sp sp F2 pop pop
Using Lemma 2, we have the following bound on term I in (66),
9
R(m¯s⊗m¯s ,νt )−R(m¯t ⊗m¯t ,νt )
c sp pop c sp pop (76)
≤ E [(1+∥Zt∥2)2]d (m¯s⊗m¯s ,m¯t ⊗m¯t ).
Z 1t 1 F4 c sp c sp
39The final result holds via substituting (67)-(72) in (66),
E(mβs(νs )mβt(mβs(νs ),νt ),νt )
c ns sp c ns nt pop
2β216 2 (cid:104) (cid:105) (cid:104) (cid:105)
≤ σ2t n t(1+ n t)2L2 e(1+L m)2Comp(θ c,θ st p,θ ss p)E Z 1s (1+∥Z 1s∥2)2 E Z 1t (1+∥Z 1t∥2)4
σ2
+ KL(m˜t ∥γ˜σ )
2β2 sp sp
t
(cid:16) (cid:90) (cid:90) (cid:90) (cid:17)
+2E Zs(cid:2) (1+∥Z 1s∥2)(cid:3) 1+ ∥θ c∥4γˆ 8σ ,c(dθ c)+ ∥θ sp∥4γˆ 8σ ,sp(dθ ss p)+ ∥θ sp∥4m˜t sp(dθ sp)
1
Θc Θsp Θsp
×d (νs ,νt )
F2 pop pop
+E
Z
1s[(1 √+∥Z 1s∥2)2]d F4(m˜s sp,m˜t sp)
2β29 2 2 (cid:104) (cid:105)
+ s L2(1+L )2(1+ )2Comp(θ)E (1+∥Zs∥2)4
σ2 n
s
e m n
s
Z 1t 1
σ2
+ KL(m¯s⊗m¯s ∥γσ ⊗γσ )
2β2 c sp c sp
s
+L (1+E [∥θ ∥4]+E [∥θ ∥4])d (νs ,νt )
e θc∼m¯s c c θsp∼m¯s sp sp F2 pop pop
+L E [(1+∥Zt∥2)2]d (m¯s⊗m¯s ,m¯t ⊗m¯t )
e Z 1t 1 F4 c sp c sp (77)
2β216 2 (cid:104) (cid:105) (cid:104) (cid:105)
≤ σ2t n t(1+ n t)2L2 e(1+L m)2Comp(θ c,θ st p,θ ss p)E Z 1s (1+∥Z 1s∥2)2 E Z 1t (1+∥Z 1t∥2)4
σ2
+ KL(m˜t ∥γ˜σ )
2β2 sp sp
t √
2β29 2 2 (cid:104) (cid:105)
+ s L2(1+L )2(1+ )2Comp(θ)E (1+∥Zs∥2)4
σ2 n
s
e m n
s
Z 1t 1
σ2
+ KL(m¯s⊗m¯s ∥γσ ⊗γσ )
2β2 c sp c sp
s
(cid:16) (cid:90)
+4E
Z
1s(cid:2) (1+∥Z 1s∥2)(cid:3) 1+E
θc∼m¯s
c[∥θ c∥4]+E
θsp∼m¯s
sp[∥θ sp∥4]+ ∥θ c∥4γˆ 8σ ,c(dθ c)
Θc
(cid:90) (cid:90) (cid:17)
+ ∥θ ∥4γˆσ (dθs )+ ∥θ ∥4m˜t (dθ )
sp 8,sp sp sp sp sp
Θsp Θsp
×d (νs ,νt )
F2 pop pop
+L eE
Z
1s[(1+∥Z 1s∥2)2]d F4(m˜s sp,m˜t sp)
+L E [(1+∥Zt∥2)2]d (m¯s⊗m¯s ,m¯t ⊗m¯t ).
e Z 1t 1 F4 c sp c sp
E Proofs and details from Section 5
The Fine-tuning scenario in one-hidden layer neural network is shown in Fig. 2. Furthermore, the block
diagram of α-ERM is shown in Fig. 3 where we have αR(m,νt )+(1−α)R(m,νs ) as risk function.
nt ns
40h1 h1=φ(cid:0)(cid:80)q i=1wi,1xs i(cid:1)
w1,1 a 1,s
xs
1
w 2,1
h2
a2,s
Os Os= r1(cid:80)r j=1aj,sφ(cid:0)(cid:80)q i=1wi,jxs i(cid:1)
xs
2
...
wq,1
a3,s
h3 Ot
xs
q
...
ar,s
hr
(a) First step of Fine-tuning
h1 h1=φ(cid:0)(cid:80)q i=1wi,1xt i(cid:1)
w1,1
xt
1 w 2,1 a 1,t
h2 Os
xt
2
a
...
wq,1 2,t
h3
a3,t
Ot Ot= r1(cid:80)r j=1aj,tφ(cid:0)(cid:80)q i=1wi,jxt i(cid:1)
xt
q
...
ar,t
hr
(b) Second step of Fine-tuning
Figure 2: Fine-tuning Scenario
h1 h1=φ(cid:0)(cid:80)q i=1wi,1xα i(cid:1)
w1,1
xα 1 a 1
w 2,1
h2
xα 2 a2
...
wq,1 Ot Ot= r1(cid:80)r j=1ajφ(cid:0)(cid:80)q i=1wi,jxα i(cid:1)
h3
a3
xα
q
...
ar
hr
Figure 3: α-ERM Scenario
41Lemma 12. Under Assumption 5, there exists a constant c > 0 independent of n such that, for
all z ∈ Z, θ ∈ Θ , θ ∈ Θ , m ∈ P (θ ), m ∈ P (θ ) and ν ∈ P (Z),
c c sp sp sp 4 sp c 4 c 2
(cid:12) (cid:12)ℓ o(E a∼mc,w∼msp[aφ(w.x)],y)(cid:12) (cid:12) ≤ 12L ℓL2 φ(cid:0) 1+E a∼msp[a4]+E w∼mc[∥w∥4](cid:1)(cid:0) 1+∥y∥2+∥x∥2),
(cid:12) δℓ (cid:12)
(cid:12) o (E [aφ(w.x)],y,a)(cid:12)
(cid:12)δm a∼mc,w∼msp (cid:12)
sp
(cid:16) (cid:17)
≤ 64L L (1+L )(cid:0) 1+E [∥w∥4]+E [a4]+|a|4(cid:1) 1+∥x∥2+∥y∥2 ,
ℓ,1 φ φ w∼mc a∼msp
(cid:104)(cid:16) δ2ℓ (cid:17)2(cid:105)1/2 (cid:104) (cid:105)
E E (m,z,θ,θ′) ≤ c E [1+∥θ∥4] E [1+∥Z∥2].
θ,θ′∼m z∼ν δm2 θ∼m Z∼ν
In particular, if p ≥ 8 in Assumption 5, then Assumption 4 is satisfied for some choice of
L < ∞.
e
Proof of Lemma 12. We recall that our loss is of the form ℓ(m,z) = ℓ (E [aφ(w.x)],y), for
o a∼mc,w∼msp
ℓ a convex function. It follows that ℓ is convex and nonnegative with respect to m.
o
By the chain rule, we can differentiate with respect to m , to obtain, with Φ(m ⊗ m ,x) :=
sp c sp
E [aφ(w.x)], θ = a and θ = w,
a∼msp,w∼mc sp c
δΦ
(m ⊗m ,x,a) = (a−E [a])E [φ(w.x)],
δm
c sp a∼msp w∼mc
sp (78)
δℓ
(m ⊗m ,z,a) = ∂ ℓ (Φ(m,x),y)(a−E [a])E [φ(w.x)].
δm
c sp yˆ o a∼msp w∼mc
sp
Note that,
δ2ℓ δ (cid:18) δℓ (cid:19)
(m ⊗m ,z,a,a′) = (·,·,a) (m ⊗m ,z,a′),
δm2 c sp δm δm c sp
sp sp sp
and due to the normalization convention, see (Cardaliaguet et al., 2019, Remark 2.5),
δ2ℓ
(m ⊗m ,z,a,a′) = ∂ ℓ (Φ(m ⊗m ,x),y)(a′−E [a])(a−E [a])E [φ(w.x)]2.
δm2 c sp yˆyˆ o c sp a∼msp a∼msp w∼mc
sp
(79)
It follows that ℓ is C2 with respect to m (the required bounds on the derivatives will be obtained below).
Withoutlossofgenerality,fornotationalconvenience,weassumethatL ,L ,L andL inAssumption
ℓ ℓ,1 ℓ,2 ϕ
5 are all at least 1. Throughout, c is a constant which can vary from line to line, but does not depend
on n.
We first consider obtaining bounds on ℓ. We know that
ℓ(m (νs )m (m (νs ),νt ),z) ≤ L (1+∥y∥2+∥yˆ∥2) = L (1+∥y∥2+∥E [aφ(w.x)]∥2)).
c ns sp c ns nt ℓ ℓ a,w
As
∥E [aφ(w.x)]∥2 ≤ 2L2(1+∥x∥)2(cid:0) |a|(1+∥w∥)(cid:1)2
a,w φ
≤ 8L2(1+∥x∥)2(cid:0) 1+E [a2]+E [∥w∥2](cid:1)2
φ a∼msp w∼mc
we know
ℓ(m (νs )m (m (νs ),νt ),z) ≤ L (cid:0) 1+∥y∥2+2L2(1+∥x∥)2(cid:0) 1+E [a2]+E [∥w∥2](cid:1)2(cid:1)
c ns sp c ns nt ℓ φ a∼msp w∼mc
≤ 8L L2(cid:0) 1+E [a2]+E [∥w∥2](cid:1)2(cid:0) 1+∥y∥2+∥x∥2).
ℓ φ a∼msp w∼mc
As we have
8L L2(cid:0) 1+E [a2]+E [∥w∥2](cid:1)2 ≤ 24L L2(cid:0) 1+E [a4]+E [∥w∥4](cid:1) , (80)
ℓ ϕ a∼msp w∼mc ℓ ϕ a∼msp w∼mc
42then, we take g(m ⊗m ) = 24L L2(cid:0) 1+E [a4]+E [∥w∥4](cid:1) .
c sp ℓ ϕ a∼msp w∼mc
We now consider the derivative of ℓ. Writing m := E [∥w∥2] and m := E [a2], we have
2,c w∼mc 2,sp a∼msp
(cid:12) (cid:12) δℓ (cid:0) m,z,θ(cid:1)(cid:12) (cid:12) = (cid:12) (cid:12)∂ ℓ (E [aφ(w.x)],y)(cid:12) (cid:12)|(a−E [a])||E [φ(w.x)]|
(cid:12)δm (cid:12) (cid:12) yˆ o a,w (cid:12) a∼msp w∼mc
sp
≤ L (1+|E [aφ(w.x)]|+∥y∥)|(a−E [a])||E [φ(w.x)]|
ℓ,1 a,w a∼msp w∼mc
(cid:16) (cid:17) (cid:16) (cid:17)
≤ L L 1+L (1+∥x∥)(1+m +m )+∥y∥ (1+∥x∥) 1+m |(a−E [a])|
ℓ,1 φ φ 2,c 2,sp 2,c a∼msp
(cid:16) (cid:17)
≤ L L (1+L )(1+m +m ) 1+∥x∥+∥y∥ (1+∥x∥)2|(a−E [a])|
ℓ,1 φ φ 2,c 2,sp a∼msp
(cid:16) (cid:17)
≤ 2L L (1+L )(1+m +m ) 1+∥x∥+∥y∥ (1+∥x∥2)|(a−E [a])|
ℓ,1 φ φ 2,c 2,sp a∼msp
(cid:16) (cid:17)
≤ 8L L (1+L )(1+m +m )(cid:0) 1+|a|+|E [a]|(cid:1) 1+∥x∥2+∥y∥2
ℓ,1 φ φ 2,c 2,sp a∼msp
(cid:16) (cid:17)
≤ 32L L (1+L )(1+m +m )(cid:0) 1+|a|2+m (cid:1) 1+∥x∥2+∥y∥2
ℓ,1 φ φ 2,c 2,sp 2,sp
(cid:16) (cid:17)
≤ 64L L (1+L )(1+m +m +|a|2(cid:1) 1+∥x∥2+∥y∥2
ℓ,1 φ φ 2,c 2,sp
We then take
g (m m ,θ ) = 64L L (1+L )(cid:0) 1+E [∥w∥4]+E [a4]+|a|4(cid:1) ,
sp c sp sp ℓ,1 φ φ w∼mc a∼msp
and we have, with c a constant varying from line to line,
E (cid:2) g (m m ,θ )2(cid:3)1/2 = cE
(cid:104)(cid:16)
1+E [∥w∥4]+E
[a4]+|a|4(cid:17)2(cid:105)1/2
θ∼m′
sp
sp c sp sp θsp∼m′
sp
w∼mc a∼msp
≤ c(cid:16) 1+E [∥w∥8]+E [a8]+E [a8](cid:17)1/2 (81)
w∼mc a∼msp a∼m′
sp
(cid:16) (cid:17)
≤ c 1+E [∥w∥8]+E [a8]+E [a8] .
w∼mc a∼msp a∼m′
sp
Together with Jensen’s inequality, this yields (24).
Finally, we consider the second derivative of ℓ. We have,
(cid:104)(cid:16) δ2ℓ (cid:17)2(cid:105)
E E (m ⊗m ,z,a,a′)
a,a′∼msp,m′ sp z∼ν δm2 c sp
sp
(cid:90) (cid:104) (cid:105)2 (cid:16) (cid:17) (cid:16) (cid:17)
= ∂ ℓ (Φ(m ⊗m ,x),y) V a V a′ E [φ(w.x)]4ν(dz)
yˆyˆ o c sp θsp∼msp θ s′ p∼m′ sp w∼mc (82)
Z
(cid:90) (cid:104) (cid:105)2(cid:104) (cid:16) (cid:17)(cid:105)2
≤ ∂ ℓ (Φ(m ⊗m ,x),y) V a E [(1+∥w∥)4](1+∥x∥)4ν(dz)
yˆyˆ o c sp θsp∼msp w∼mc
Z
≤ cE [(1+∥Z∥)4]E [(1+∥w∥)4]E [a4].
Z∼ν w∼mc a∼msp
giving the stated inequalities. As ν ∈ P (Z), and assuming m ⊗m ∈ P (Θ )×P (Θ ). It follows
2 sp c 8 c 8 sp
that Assumption 4 holds whenever p ≥ 8.
Lemma 13. Under Assumption 5, there exists a constant c > 0 independent of n such that, for
all z ∈ Z, θ ∈ Θ, m ∈ P (Θ), and ν ∈ P (Z),
8 2
(cid:12) (cid:12)ℓ(m,z)(cid:12) (cid:12) ≤ c(cid:104) E θ∼m[1+∥θ∥4](cid:105) (cid:0) 1+∥z∥2(cid:1) ,
(cid:12) (cid:12) δℓ (m,z,θ)(cid:12) (cid:12) ≤ c(cid:104) 1+∥θ∥4+E [∥θ′∥4](cid:105) (cid:0) 1+∥z∥2(cid:1) ,
(cid:12)δm (cid:12) θ′∼m
(cid:104)(cid:16) δ2ℓ (cid:17)2(cid:105)1/2 (cid:104) (cid:105)
E E (m,z,θ,θ′) ≤ c E [1+∥θ∥4] E [1+∥Z∥2].
θ,θ′∼m z∼ν δm2 θ∼m Z∼ν
In particular, under Assumption 5, then Assumption 3 is satisfied for some choice of L < ∞.
e
43Proof of Lemma 13. We recall that our loss is of the form ℓ(m,z) = ℓ (E [ϕ(θ,x)],y), for ℓ a convex
o θ∼m o
function. It follows that ℓ is convex and nonnegative with respect to m.
We write ∂ ℓ for the derivative of ℓ with respect to its first argument. By the chain rule, we can
yˆ o o
differentiate with respect to m, to obtain, with Φ(m,x) := E [ϕ(θ,x)],
θ∼m
δΦ
(m,x,θ) = ϕ(θ,x)−Φ(m,x),
δm (83)
δℓ (cid:16) (cid:17)
(m,z,θ) = ∂ ℓ (Φ(m,x),y) ϕ(θ,x)−Φ(m,x) .
yˆ o
δm
Note that,
δ2ℓ δ (cid:18) δℓ (cid:19)
(m,z,θ,θ′) = (·,·,θ) (m,z,θ′),
δm2 δm δm
and due to the normalization convention, see (Cardaliaguet et al., 2019, Remark 2.5),
δ2ℓ (cid:16) (cid:17)(cid:16) (cid:17)
(m,z,θ,θ′) = ∂ ℓ (Φ(m,x),y) ϕ(θ,x)−Φ(m,x) ϕ(θ′,x)−Φ(m,x) . (84)
δm2 yˆyˆ o
It follows that ℓ is C2 with respect to m (the required bounds on the derivatives will be obtained below).
Withoutlossofgenerality,fornotationalconvenience,weassumethatL ,L ,L andL inAssumption
ℓ ℓ,1 ℓ,2 ϕ
5 are all at least 1. Throughout, c is a constant which can vary from line to line, but does not depend
on n.
We first consider obtaining bounds on ℓ. We know that
ℓ(m(ν ),z) ≤ L (1+∥y∥2+∥yˆ∥2) = L (1+∥y∥2+∥Φ(m,x)∥2)).
n ℓ ℓ
As
∥E [aφ(w.x)]∥2 ≤ 2L2(1+∥x∥)2(cid:0) |a|(1+∥w∥)(cid:1)2
a,w φ
≤ 8L2(1+∥x∥)2(cid:0) 1+E [a2]+E [∥w∥2](cid:1)2
φ a∼msp w∼mc
≤ 8L2(1+∥x∥)2(cid:0) 1+E [∥θ∥2](cid:1)2
φ θ∼m
we know
ℓ(m(ν ),z) ≤ L (cid:0) 1+∥y∥2+L2(1+∥x∥)2E [1+∥θ∥2]2(cid:1)
n ℓ ϕ θ∼m
≤ 2L L2E [1+∥θ∥2]2(cid:0) 1+∥y∥2+∥x∥2).
ℓ ϕ θ∼m
We therefore take g(m) := 4L L2E [1+∥θ∥4]. As we know p ≥ 4 and γ˜σ is a density in P , it follows
ℓ ϕ θ∼m 8 4
that g(γ˜σ) < ∞.
8
We now consider the derivative of ℓ. Writing m := E [∥θ∥2], we have
2 θ∼m
(cid:12) (cid:12) δℓ (cid:0) m,z,θ(cid:1)(cid:12) (cid:12) = (cid:12) (cid:12)∂ ℓ (Φ(m,x),y)(cid:12) (cid:12)|ϕ(θ,x)−Φ(m,x)|
(cid:12)δm (cid:12) (cid:12) yˆ o (cid:12)
(cid:12) (cid:12)
≤ L ℓ,1(1+|Φ(m,x)|+∥y∥)(cid:12)ϕ(θ,x)−Φ(m,x)(cid:12)
(cid:16) (cid:17) (cid:16) (cid:17)
≤ L L 1+L (1+∥x∥)(1+m )+∥y∥ (1+∥x∥) 2+∥θ∥2+m
ℓ,1 ϕ ϕ 2 2
(cid:16) (cid:17)(cid:16) (cid:17)
≤ L L (1+L )(1+m ) 2+∥θ∥2+m 1+∥x∥+∥y∥ (1+∥x∥)
ℓ,1 ϕ ϕ 2 2
(cid:16) (cid:17)2(cid:16) (cid:17)
≤ 2L L (1+L ) 2+∥θ∥2+m 1+∥x∥2+∥y∥2
ℓ,1 ϕ ϕ 2
(cid:16) (cid:17)(cid:16) (cid:17)
≤ 6L L (1+L ) 4+∥θ∥4+E [∥θ∥4] 1+∥x∥2+∥y∥2 .
ℓ,1 ϕ ϕ θ∼m
We then take
(cid:16) (cid:17)
g (m,θ) = 6L L (1+L ) 4+∥θ∥4+E [∥θ∥4] ,
e ℓ,1 ϕ ϕ θ∼m
44and we have, with c a constant varying from line to line,
E (cid:2) g (m,θ)2(cid:3)1/2 = cE
(cid:104)(cid:16)
4+∥θ∥4+E
[∥θ∥4](cid:17)2(cid:105)1/2
θ∼m′ e θ∼m′ θ∼m
≤ c(cid:16) 8+2E [∥θ∥8]+2E [∥θ∥8](cid:17)1/2 (85)
θ∼m′ θ∼m
(cid:16) (cid:17)
≤ c 1+E [∥θ∥8]+E [∥θ∥8] .
θ∼m′ θ∼m
Together with Jensen’s inequality, this yields (22) in the case p ≥ 8.
Finally, we consider the second derivative of ℓ. We have,
(cid:104)(cid:16) δ2ℓ (cid:17)2(cid:105)
E E (m,z,θ,θ′)
θ,θ′∼m z∼ν δm2
(cid:90) (cid:104) (cid:105)2 (cid:16) (cid:17) (cid:16) (cid:17)
= ∂ ℓ (Φ(m,x),y) V ϕ(θ,x) V ϕ(θ′,x) ν(dz)
yˆyˆ o θ∼m θ′∼m
(86)
Z
(cid:90) (cid:104) (cid:105)2(cid:104) (cid:16) (cid:17)(cid:105)2
= ∂ ℓ (Φ(m,x),y) V ϕ(θ,x) ν(dz)
yˆyˆ o θ∼m
Z
≤ cE [(1+∥Z∥)2]E (cid:2) (1+∥θ∥2)2(cid:3) .
Z∼ν θ∼m
giving the stated inequalities. As ν ∈ P (Z), and assuming m ∈ P (Θ). It follows that Assumption 3
2 8
holds.
F More Discussion
In this section, we provide more details on comparison between α-ERM and fine-tuning scenario.
F.1 Complexity Terms
In this section we compare the complexity terms in fine-tuning and α-ERM, i.e., Comp(θ ,θt ,θs ) and
c sp sp
Comp(θ), respectively.
Assumingγˆσ ∈ P (Θ ),γ˜σ ,γˆσ ∈ P (Θ ),γ˜σ ∈ P (Θ), theComp (θ ,θ )andComp (θ)aredefined
8,c 8 c sp 8,sp 8 sp 8 8 2 c sp 2
based on L -norm,
2
(cid:104) (cid:90)
Comp (θ ,θt ,θs ) = 1+ ∥θ ∥4(2+∥θ ∥4)γˆσ (dθ )
2 c sp sp c 2 c 2 8,c c
Θc (87)
(cid:90) (cid:90) (cid:105)2
+ ∥θt ∥4(1+2∥θt ∥4)γ˜σ (dθt )+ ∥θs ∥4γˆσ (dθs ) ,
sp 2 sp 2 sp sp sp 2 8,sp sp
Θsp Θsp
and
(cid:16) (cid:90) (cid:90) (cid:17)2
Comp (θ) = 1+2 ∥θ∥8γ˜σ(dθ)+2 ∥θ∥4γ˜σ(dθ)] , (88)
2 2 8 2 8
Θ Θ
which are bounded. We think of these as measures of the complexity of our model, as they determine
how extreme the parameters in the model can become, which acts as a proxy for the model’s expressive
power. We note that these quantities do not depend on the training data, and are only functions of the
chosen initialization distribution and regularization.
Lemma 14. Let θ ∈ Θ ⊂ Rk d , θ c ∈ Θ c ⊂ Rk dc, θ sp ∈ Θ sp ⊂ Rk dsp and γ˜ 8σ ∈ P 8(Θ). Assume the
following conditions:
• For each dimension i ∈ [k ], the 8th moment of θ under the marginal distribution γ˜σ[i] is bounded by
d i 8
B: E [θ8] ≤ B
θi∼γ˜ 8σ[i] i
• The 8th moments of each dimension for distributions γˆσ , γˆσ , and γ˜σ are also bounded by B.
8,c 8,sp sp
45Then, the following inequalities hold,
Comp (θ ,θ ) ≤ (cid:0) 1+(k4 +2k4 )B+2(k2 +k2 )B1/2(cid:1)2 ,
2 c sp dc dsp dc dsp (89)
Comp (θ) ≤
(cid:0) 1+k4B+k2B1/2(cid:1)2
.
2 d d
Proof. Note that we have,
(cid:90) (cid:90) (cid:16)(cid:88)k dc (cid:17)2
∥θ ∥4γˆσ (dθ ) = θ2 γˆσ (dθ )
c 2 8,c c i,c 8,c c
Θc Θc
i=1
(cid:90) (cid:16)(cid:88)k dc (cid:17)
≤ k θ4 γˆσ (dθ )
dc i,c 8,c c
Θc
i=1
k
(cid:88)dc
= k E [θ4 ]
dc γˆ 8σ ,c[i] i,c
i=1
≤ k2 B1/2.
dc
where the last inequality follows from Jensen inequality. Similarly, we have,
(cid:90)
∥θ ∥8γˆσ (dθ ) ≤ k4 B,
c 2 8,c c dc
Θc
(cid:90)
∥θ ∥8γˆσ (dθ ) ≤ k4 B, (90)
sp 2 8,sp sp dsp
Θsp
(cid:90)
∥θ∥8γ˜σ(dθ) ≤ k4B.
2 8 d
Θ
It completes the proof.
From Lemma 14 and due to the fact that k = k + k , we observe that the upper bound on
d dsp dc
Comp (θ ,θ )isdemonstrablylowerthanthatonComp (θ). Thisdifferenceisparticularlypronounced
2 c sp 2
in numerous applications where k ≪ k , for example, in one-hidden layer neural network in Section 5,
dsp dc
we have k = 1 and k = q where q is the dimension of input. Consequently, this suggests that the
dsp dc
upper bound on the WTER for fine-tuning (Theorem 7) will typically be smaller than the upper bound
on WTER for α-ERM (Theorem 5), while the two bounds will be asympotically of the same order as
k → ∞.
d
F.2 Non-similar Tasks
In this section, we study the effect of non-similar tasks where d (νs ,νt ) > 0. For this purpose,
F2 pop pop
we compare the coefficients of d (νs ,νt ) in Theorem 7 and Theorem 5. Therefore, we have the
F2 pop pop
following similarity terms,
• Coefficient of d (νs ,νt ) in α-ERM:
F2 pop pop
(cid:16) (cid:90) (cid:17)(cid:16) (cid:90) (cid:17)(cid:16) (cid:17)
C α(θ) := (1−α) 1+ ∥θ∥4m¯ α(dθ) 1+ ∥θ∥4γ˜ 8σ(dθ) 1+αE Zt[∥Z 1t∥2]+(1−α)E Zs[∥Z 1s∥2]
1 1
Θ Θ
• Coefficient of d (νs ,νt ) in Fine-tuning:
F2 pop pop
C (θ ,θs ,θt )
FT c sp sp
(cid:16) (cid:90) (cid:90) (cid:17)(cid:16) (cid:90) (cid:90)
:= 1+ ∥θ ∥4γˆσ (dθ )+ ∥θs ∥4γˆσ (dθs ) 1+ ∥θ ∥4m¯s(dθ )+ ∥θs ∥4m¯s (dθs )
c 8,c c sp 8,sp sp c c c sp sp sp
Θc Θsp Θc Θsp
(cid:90) (cid:17)
+ ∥θ st p∥4m˜t sp(dθ st p) E Zs(cid:2) (1+∥Z 1s∥2)(cid:3)
1
Θsp
46Similar to Lemma 14, we can provide the following comparison.
Lemma 15. Under the same assumptions in Lemma 14, we have,
(cid:16) (cid:17)
C α(θ) ≤ (1−α)(cid:0) 1+k d4B1/2)2 1+αE
Z
1t[∥Z 1t∥2]+(1−α)E
Z
1s[∥Z 1s∥2] ,
(91)
C FT(θ c,θ ss p,θ st p) ≤ (cid:0) 1+k d4 cB1/2+k d4 spB1/2(cid:1)(cid:0) 1+k d4 cB1/2+2k d4 spB1/2(cid:1)E
Z
1s(cid:2) (1+∥Z 1s∥2)(cid:3) .
Proof. The proof approach is similar to Lemma 14.
From Lemma 14 and due to the fact that k = k +k , the upper bound on C (θ) can be larger
d dsp dc α
than the upper bound on C (θ ,θs ,θt ) for the case k ≪ k .
FT c sp sp dsp dc
F.3 Other Comparison
Note that in α-ERM, the hyper-parameter α must be tuned for each specific application. Fine-tuning,
on the other hand, requires no such hyperparameter tuning. Moreover, in many cases, particularly
in large language models, access to the original source training dataset is often impossible, and only
the pre-trained model is shared. In these situations, fine-tuning can be directly applied to the target
dataset and we can not apply α-ERM scenario.
47