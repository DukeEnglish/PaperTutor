PreprintPaper
FEW-SHOT IN-CONTEXT PREFERENCE LEARNING
USING LARGE LANGUAGE MODELS
ChaoYu1,HongLu1JiaxuanGao1,QixinTan1,XintingYang1,YuWang1∗,
YiWu1,2∗,EugeneVinitsky3∗
1TsinghuaUniversity,2ShanghaiQiZhiInstitute3NewYorkUniversity
zoeyuchao@gmail.com
ABSTRACT
Designing reward functions is a core component of reinforcement learning but
can be challenging for truly complex behavior. Reinforcement Learning from
Human Feedback (RLHF) has been used to alleviate this challenge by replac-
ing a hand-coded reward function with a reward function learned from pref-
erences. However, it can be exceedingly inefficient to learn these rewards as
they are often learned tabula rasa. We investigate whether Large Language
Models (LLMs) can reduce this query inefficiency by converting an iterative
series of human preferences into code representing the rewards. We propose
In-Context Preference Learning (ICPL), a method that uses the grounding of
an LLM to accelerate learning reward functions from preferences. ICPL takes
the environment context and task description, synthesizes a set of reward func-
tions, and then repeatedly updates the reward functions using human rankings
of videos of the resultant policies. Using synthetic preferences, we demonstrate
that ICPL is orders of magnitude more efficient than RLHF and is even com-
petitive with methods that use ground-truth reward functions instead of pref-
erences. Finally, we perform a series of human preference-learning trials and
observe that ICPL extends beyond synthetic settings and can work effectively
with humans-in-the-loop. Additional information and videos are provided at
https://sites.google.com/view/few-shot-icpl/home.
1 INTRODUCTION
Designing state-of-the-art agents using reinforcement learning (RL) often requires the design of
rewardfunctionsthatspecifydesiredandundesirablebehaviors. However,forsufficientlycomplex
tasks,designinganeffectiverewardfunctionremainsasignificantchallenge. Thisprocessisoften
difficult,andpoorlydesignedrewardscanleadtobiased,misguided,orevenunexpectedbehaviors
inRLagents(Boothetal.,2023). Recentadvancesinlargelanguagemodels(LLMs)haveshown
potentialintacklingthischallengeastheyareabletozero-shotgeneraterewardfunctionsthatsatisfy
a task specification (Yu et al., 2023). However, the ability of LLMs to directly write a reward
functionislimitedwhentaskcomplexityincreasesortasksareout-of-distributionfrompre-training
data. As an additional challenge, humans may be unable to perfectly specify their desired agent
behaviorintext.
Human-in-the-loop learning (HL) offers a potential enhancement to the reward design process by
embeddinghumanfeedbackdirectlyintothelearningprocess. Aubiquitousapproachispreference-
basedRLwherepreferencesbetweendifferentagentbehaviorsservesastheprimarylearningsignal.
Instead of relying on predefined rewards, the agent learns a reward function aligned with human
preferences. This interactive approach has shown success in various RL tasks, including standard
benchmarks (Christiano et al., 2017; Ibarz et al., 2018), encouraging novel behaviors (Liu et al.,
2020;Wuetal.,2021),andovercomingrewardexploitation(Leeetal.,2021a). However,inmore
complex tasks involving extensive agent-environment interactions, preference-based RL often de-
mandshundredsorthousandsofhumanqueriestoprovideeffectivefeedback.Forinstance,arobotic
∗EqualAdvising
1
4202
tcO
22
]IA.sc[
1v33271.0142:viXraPreprintPaper
armbutton-pushingtaskrequiresover10kqueriestolearnreasonablebehavior(Leeetal.), which
couldbeamajorbottleneck.
In this work, we introduce a novel method, In-Context Preference Learning (ICPL), which signif-
icantly enhances the sample efficiency of preference learning through LLM guidance. Our pri-
mary insight is that we can harness the coding capabilities of LLMs to autonomously generate
reward functions, then utilize human preferences through in-context learning to refine these func-
tions. Specifically,ICPLleveragesanLLM,suchasGPT-4,togenerateexecutable,diversereward
functions based on the task description and environment source code. We acquire preferences by
evaluating the agent behaviors resulting from these reward functions, selecting the most and least
preferredbehaviors. Theselectedfunctions,alongwithhistoricaldatasuchasrewardtracesofthe
generatedrewardfunctionsfromRLtraining,arethenfedbackintotheLLMtoguidesubsequent
iterationsofrewardfunctiongeneration.Wehypothesizethatasaresultofitsgroundingintextdata,
ICPL will be able to improve the quality of the reward function through incorporating the prefer-
encesandthehistoryofthegeneratedrewardfunctions,ensuringtheyalignmoreandmoreclosely
withhumanpreferences. UnlikeevolutionarysearchmethodslikeEUREKAMaetal.(2023),there
isnoground-truthrewardfunctionthattheLLMcanusetoevaluateagentperformance, andthus,
successherewoulddemonstratethatLLMshavesomenativepreference-learningcapabilities.
To study the effectiveness of ICPL, we perform experiments on a diverse set of RL tasks. For
scalability, we first study tasks with synthetic preferences where a ground-truth reward function
is used to assign preference labels. We observe that compared to traditional preference-based RL
algorithms,ICPLachievesovera30timesreductionintherequirednumberofpreferencequeriesto
achieveequivalentorsuperiorperformance. Moreover,ICPLattainsperformancecomparableto,or
evenbetterthan,methodsthathaveaccesstoagroundtruthspecificationoftherewardfunctionMa
et al. (2023). Finally, we test ICPL on a particularly challenging task, “making a humanoid jump
likearealhuman,”wheredesigningarewardisdifficult.Byusingrealhumanfeedback,ourmethod
successfullytrainedanagentcapableofbendingbothlegsandperformingstable,human-likejumps,
showcasingthepotentialofICPLintaskswherehumanintuitionplaysacriticalrole.
Insummary,thecontributionsofthepaperarethefollowing:
• We propose ICPL, an LLM-based preference learning algorithm. Over a synthetic set of
preferences,wedemonstratethatICPLcaniterativelyoutputrewardsthatincreasinglyre-
flectpreferences.Viaasetofablations,wedemonstratethatthisimprovementisonaverage
monotonic,suggestingthatpreferencelearningisoccurringasopposedtoarandomsearch.
• We demonstrate, via human-in-the-loop trials, that ICPL is able to work effectively with
humans-in-the-loopdespitesignificantlynoisierpreferencelabels.
• WedemonstratethatICPLsharplyoutperformstabula-rasaRLHFmethodsandisalsocom-
petitivewithmethodsthatrelyonaccesstoaground-truthreward.
2 RELATED WORK
Reward Design. In reinforcement learning, reward design is a core challenge, as rewards must
bothrepresentadesiredsetofbehaviorsandprovideenoughsignalforlearning. Themostcommon
approachtorewarddesignishandcrafting,whichrequiresalargenumberoftrialsbyexperts(Sutton,
2018; Singh et al., 2009). Since hand-coded reward design requires extensive engineering effort,
severalpriorworkshavestudiedmodelingtherewardfunctionwithprecollecteddata. Forexample,
InverseReinforcementLearning(IRL)aimstorecoverarewardfunctionfromexpertdemonstration
data(Arora&Doshi,2021;Ngetal.,2000). Withadvancesinpretrainedfoundationmodels,some
recent works have also studied using large language models or vision-language models to provide
reward signals (Ma et al., 2022; Fan et al., 2022; Du et al., 2023; Karamcheti et al., 2023; Kwon
etal.,2023;Wangetal.,2024;Maetal.,2024).
Among these approaches, EUREKA (Ma et al., 2023) is the closest to our work, instructing the
LLM to generate and select novel reward functions based on environment feedback with an evo-
lutionary framework. However, as opposed to performing preference learning, EUREKA uses the
LLM to design dense rewards that help with learning a hand-designed sparse reward. In contrast,
ICPLfocusesonfindingarewardthatcorrectlyordersasetofpreferences. However,wenotethat
EUREKA also has a small, preliminary investigation combining human preferences with an LLM
2PreprintPaper
to generate human-preferred behaviors in a single scenario. We note that this experiment had the
humanprovidetheirfeedbackintext,whereasweonlyusepreferencequeries. Thispaperisasig-
nificantly scaled-up version of that investigation as well as a methodological study of how best to
incorporatepriorroundsoffeedback.
Human-in-the-loopReinforcementLearning. Feedbackfromhumanshasbeenproventobeef-
fectiveintrainingreinforcementlearningagentsthatbettermatchhumanpreferences(Retzlaffetal.,
2024; Mosqueira-Rey et al., 2023; Kwon et al., 2023). Previous works have investigated human
feedbackinvariousforms,suchastrajectorycomparisons,preferences,demonstrations,andcorrec-
tions(Wirthetal.,2017;Ngetal.,2000;Jeonetal.,2020;Pengetal.,2024). Amongthesevarious
methods,preference-basedRLhasbeensuccessfullyscaledtotrainlargefoundationmodelsforhard
taskslikedialogue,e.g. ChatGPT(Ouyangetal.,2022). InLLM-basedapplications,promptingis
a simple way to provide human feedback in order to align LLMs with human preferences (Giray,
2023; White et al., 2023; Chen et al., 2023). Iteratively refining the prompts with feedback from
theenvironmentorhumanusershasshownpromiseinimprovingtheoutputoftheLLM(Wuetal.,
2021;Nasirianyetal.,2024). ThisworkextendstheusageoftheabilitytocontrolLLMbehavior
via in-context prompts. We aim to utilize interactive rounds of preference feedback between the
LLMandhumanstoguidetheLLMtogeneraterewardfunctionsthatcanelicitbehaviorsthatalign
withhumanpreferences.
3 PROBLEM DEFINITION
Ourgoalistodesignarewardfunctionthatcanbeusedtotrainreinforcementlearningagentsthat
demonstrate human-preferred behaviors. It is usually hard to design proper reward functions in
reinforcementlearningthatinducepoliciesthatalignwellwithhumanpreferences.
MarkovDecisionProcesswithPreferences(Wirthetal.(2017))AMarkovDecisionProcesswith
Preferences(MDPP)isdefinedasatupleM = ⟨S,A,µ,σ,γ,ρ⟩whereS denotesthestatespace,
A denotes the action space, µ is the distribution of initial states, σ is the state transition model,
γ ∈ [0,1) is the discount factor. ρ is the preference relation over trajectories, i.e. ρ(τ ≻ τ )
i j
denotestheprobabilitywithwhichtrajectoryτ ispreferredoverτ . Givenasetofpreferencesζ,
i j
thegoalinanMDPPistofindapolicyπ∗thatmaximallycomplieswithζ. Apreferenceτ ≻τ is
1 2
satisfiedbyπifandonlyifPr (τ )>Pr (τ )wherePr (τ)=µ(s
)(cid:81)|τ|
π(a |s )σ(s |s ,a ).
Thiscanbeviewedasfindingπ aπ1
∗
thatπ min2 imizesaprπ eferencelos0 sL(t π=0
) =
(cid:80)t t L(π,t ζ+1
),
wt het
re
ζ i i
L(π,τ ≻τ )=−(Pr (τ )−Pr (τ )).
1 2 π 1 π 2
Reward Design Problem with Preferences. A reward design problem with preferences (RDPP)
isatupleP = ⟨M,R,A ,ζ⟩,whereM isaMarkovDecisionProcesswithPreferences,Risthe
M
space of reward functions, A (·) : R → Π is a learning algorithm that outputs a policy π that
M
optimizes a reward R ∈ R in the MDPP. ζ = {(τ ,τ )} is the set of preferences. In an RDPP,
1 2
the goal is to find a reward function R ∈ R such that the policy π = A (R) that optimizes R
M
maximally complies with the preference set ζ. In Preference-based Reinforcement Learning, the
learning algorithms usually involve multiple iterations, and the preference set ζ is constructed in
everyiterationbysamplingtrajectoriesfromthepolicyorpolicypopulation.
4 METHOD
Ourproposedmethod,In-ContextPreferenceLearning(ICPL),integratesLLMswithhumanprefer-
encestosynthesizerewardfunctions. TheLLMreceivesenvironmentalcontextandataskdescrip-
tiontogenerateaninitialsetofK executablerewardfunctions. ICPLtheniterativelyrefinesthese
functions. Ineachiteration,theLLM-generatedrewardfunctionsareusedtotrainagentswithinthe
environment, producing a set of agents; we use these agents to generate videos of their behavior.
A ranking is formed over the videos, from which we retrieve the best and worst reward functions
corresponding to the top and bottom videos in the ranking. These selections serve as examples of
positive and negative preferences. The preferences, along with additional contextual information,
suchasrewardtracesanddifferencesfrompreviousgoodrewardfunctions,areprovidedasfeedback
promptstotheLLM.TheLLMtakesinthiscontextandisaskedtogenerateanewsetofrewards.
Algo.1presentsthepseudocode,andFig.1illustratestheoverallprocessofICPL.
3PreprintPaper
Figure 1: ICPL employs the LLM to generate initial K executable reward functions based on the
taskdescriptionandenvironmentcontext. UsingRL,agentsaretrainedwiththeserewardfunctions.
Videosaregeneratedoftheresultantagentbehaviorfromwhichhumanevaluatorsselecttheirmost
and least preferred. These selections serve as examples of positive and negative preferences. The
preferences,alongwithadditionalcontextualinformation,areprovidedasfeedbackpromptstothe
LLM,whichisthenrequestedtosynthesizeanewsetofrewardfunctions. Forexperimentssimu-
latinghumanevaluators,taskscoresareusedtodeterminethebestandworstrewardfunctions.
Algorithm1:In-ContextPreferenceLearning(ICPL)
Input: NumberofiterationsN,NumberofsamplesK,EnvironmentEnv,CodingLLMLLM
RF
// Initialize the prompt with environment context and task description
Prompt←InitializePrompt(Env)
1
fori←1toN do
2
RF ,...,RF ←LLM (Prompt,K)
3 1 K RF
// Render videos for each reward function
Video ,...,Video ←Render(Env,RF ),...,Render(Env,RF )
4 1 K 1 K
// Human selects the most preferred (G) and least preferred (B) videos
G,B ←Human(Video ,...,Video )
5 1 K
// Retrieve the best and worst reward functions
GoodRF,BadRF←RF ,RF
6 G B
// Update the prompt with feedback
Prompt←GoodRF+BadRF+HistoricalDifference+RewardTrace
7
end
8
4.1 REWARDFUNCTIONINITIALIZATION
ToenabletheLLMtosynthesizeeffectiverewardfunctions, itisessentialtoprovidetask-specific
information, which consists of two key components: a description of the environment, including
theobservationandactionspace, andadescriptionofthetaskobjectives. Ateachiteration, ICPL
ensuresthatKexecutablerewardfunctionsaregeneratedbyresamplinguntilthereareKexecutable
rewardfunctions.
4.2 SEARCHREWARDFUNCTIONSBYHUMANPREFERENCES
For tasks without reward functions, the traditional preference-based approach typically involves
constructing a reward model, which often demands substantial human feedback. Our approach,
ICPL,aimstoenhanceefficiencybyleveragingLLMstodirectlysearchforoptimalrewardfunctions
withouttheneedtolearnarewardmodel. Toexpeditethissearchprocess,weuseanLLM-guided
searchtofindwell-performingrewardfunctions.Specifically,wegenerateK =6executablereward
functionsperiterationacrossN = 5iterations. Ineachiteration,humansselectthemostpreferred
4PreprintPaper
andleastpreferredvideos, resultinginagoodrewardfunctionandabadone. Theseareusedasa
contextfortheLLMtousetosynthesizeanewsetofK rewardfunctions. Theserewardfunctions
are then used in a PPO (Schulman et al., 2017) training loop, and videos are rendered of the final
trainedagents.
4.3 AUTOMATICFEEDBACK
Ineachiteration,theLLMnotonlyincorporateshumanpreferencesbutalsoreceivesautomatically
synthesized feedback. This feedback is composed of three elements: the evaluation of selected
rewardfunctions,thedifferencesbetweenhistoricalgoodrewardfunctions,andtherewardtraceof
thesehistoricalrewardfunctions.
Evaluation of reward functions: The component values that make up the good and bad reward
functions are obtained from the environment duringtraining and provided tothe LLM. This helps
theLLMassesstheusefulnessofdifferentpartsoftherewardfunctionbycomparingthetwo.
Differencesbetweenhistoricalrewardfunctions: Thebestrewardfunctionsselectedbyhumans
fromeachiterationaretakenout, andforanytwoconsecutivegoodrewardfunctions, theirdiffer-
encesareanalyzedbyanotherLLM.ThesedifferencesaresuppliedtotheprimaryLLMtoassistin
adjustingtherewardfunction.
Reward trace of historical reward functions: The reward trace, consisting of the values of the
goodrewardfunctionsduringtrainingfromallprioriterations,isprovidedtotheLLM.Thisreward
trace enables the LLM to evaluate how well the agent is actually able to optimize those reward
components.
5 EXPERIMENTS
Inthis section, we conductedtwo setsof experimentstoevaluate theeffectiveness ofour method:
oneusingproxyhumanpreferencesandtheotherusingrealhumanpreferences.
1) Proxy Human Preference: In this experiment, human-designed rewards, taken from EU-
REKA(Maetal.,2023), wereusedasproxiesofhumanpreferences. Specifically, ifgroundtruth
rewardR > R ,sample1ispreferredoversample2. Thismethodenablesrapidandquantitative
1 2
evaluationofourapproach. Itcorrespondstoanoise-freecasethatislikelyeasierthanhumantrials;
ifICPLperformedpoorlyhereitwouldbeunlikelytoworkinhumantrials. Importantly, human-
designed rewards were only used to automate the selection of samples and were not included in
thepromptssenttotheLLM;theLLMneverobservesthefunctionalformofthegroundtruth
rewardsnordoesiteverreceiveanyvaluesfromthem. Sinceproxyhumanpreferencesarefree
fromnoise, theyofferareliablecomparisontoevaluateourapproachefficiently. However, asdis-
cussedlaterinthelimitationssection,theseproxiesmaynotcorrectlymeasurechallengesinhuman
feedbacksuchasinabilitytoranksamples,intransitivepreferences,orotherbiases.
2) Human-in-the-loop Preference: To further validate our method, we conducted a second set of
experiments with human participants. These participants repeated the tasks from the Proxy Hu-
manPreferencesandengagedinanadditionaltaskthatlackedaclearrewardfunction: “Makinga
humanoidjumplikearealhuman.”
5.1 TESTBED
All experiments were conducted on tasks from the Eureka benchmark (Ma et al., 2023) based on
IsaacGym,coveringadiverserangeofenvironments: Cartpole,BallBalance,Quadcopter,Anymal,
Humanoid, Ant, FrankaCabinet, ShadowHand, and AllegroHand. We adhered strictly to the orig-
inal task configurations, including observation space, action space, and reward computation. This
ensuresthatourmethod’sperformancewasevaluatedunderconsistentandwell-establishedcondi-
tionsacrossavarietyofdomains.
5.2 BASELINES
Wecomparedourmethodagainsttwobaselines:
5PreprintPaper
Eureka (Ma et al., 2023). Eureka is an LLM-powered reward design method that uses sparse re-
wardsasfitnessscores. Itconsistsofthreemaincomponents:(1)zero-shotrewardgenerationbased
ontheenvironmentcontext,(2)anevolutionarysearchthatiterativelyrefinescandidaterewardfunc-
tions, and (3) reward reflection, which allows further fine-tuning of the reward functions. Sparse
rewards are used to select the best candidate reward function, which is then refined by the LLMs
and incorporated as the “task score” in the reward reflection. In each iteration, Eureka generates
16rewardfunctionswithoutcheckingtheirexecutability,assumingatleastonewilltypicallywork
acrossallconsideredenvironmentsinthefirstiteration. Toensureafaircomparison, wemodified
Eureka to generate a fixed number of executable reward functions, specifically K = 6 per itera-
tion,thesameasICPL.ThisadjustmentimprovesEureka’sperformanceinmorechallengingtasks,
whereitoftengeneratesfewerexecutablerewardfunctions.
PrefPPO(Leeetal.): B-Prefisabenchmarkspecificallydesignedforpreference-basedreinforce-
mentlearning. WeusePrefPPO,oneofthebenchmarkalgorithmsfromB-Pref,asourpreference-
based RL baseline. PrefPPO operates in a two-step process: first, the policy interacts with the
environment,gatheringexperienceandupdatingthroughtheon-policyRLalgorithmPPOtomax-
imize the learned rewards. Then, the reward model is optimized via supervised learning based on
feedback from a human teacher, with these steps repeated iteratively. To enhance performance,
we incorporate unsupervised pretraining using intrinsic motivation at the beginning, which helps
theagentexploretheenvironmentandcollectdiverseexperiences. PrefPPOusesdenserewardsas
the preference metric for the simulated teacher, providing a more stronger and informative signal
forlabelingpreferences. WeadopttheoriginalshapedrewardfunctionsfromtheIsaacGymtasks,
which were developed by active reinforcement learning researchers who designed the tasks. Note
thatwealsoexperimentedwithsparserewardsinPrefPPOandobservedsimilarperformance. Ad-
ditionally, while the original PrefPPO provides feedback on fixed-length trajectory segments, we
offerfeedbackonfulltrajectoriesofvaryinglengthstoensureafaircomparisonwiththepreference
acquisition method used in ICPL (i.e. having humans pick between full videos). Once the maxi-
mumnumberofhumanqueriesisreached,therewardmodelstopsupdating,andonlythepolicyis
updatedusingPPO.Formoredetails,refertoAppendixA.3.
5.3 EXPERIMENTSETUP
5.3.1 TRAININGDETAILS
WeusePPOasthereinforcementlearningalgorithmforallmethods. Foreachtask,weusethede-
faulthyperparametersofPPOprovidedbyIsaacGymasthesehavepreviouslybeentunedtoachieve
highperformanceandwerealsousedinourbaselines,makingafairercomparison. Wetrainedpoli-
ciesandrenderedvideosonasingleA100GPUmachine. Thetotaltimeforafullexperimentwas
lessthanonedayofwallclocktime. WeutilizedGPT-4,specificallyGPT-4-0613,asthebackbone
LLMforbothourmethod,ICPL,andEurekaintheProxyHumanPreferenceexperiment. Forthe
Human-in-the-loopPreferenceexperiment,weemployGPT-4o.
5.3.2 EVALUATIONMETRIC
Here,weprovideaspecificexplanationofhowsparserewards(detailedinAppendixA.4)areused
astaskmetricsintheadoptedIsaacGymtasks. Thetaskmetricistheaverageofthesparserewards
across all environment instances. To assess the generated reward function or the learned reward
model,wetakethemaximumtaskmetricvaluefrom10policycheckpointssampledatfixedinter-
vals,markedasthetaskscoreofrewardfunction/modeldenotedas(RTS).
Eureka can access the ground-truth task score and select the highest RTS from these iterations as
thetaskscore, denotedas(TS)foreachexperiment. Forafaircomparison, ICPLalsoperforms5
iterations, selecting the highest RTS from these iterations as TS for each experiment. Unlike (Ma
etal.,2023),whichadditionallyconducts5independentPPOtrainingrunsfortherewardfunction
with the highest RTS and reports the average of 5 maximum task metric values from 10 policy
checkpoints sampled at fixed intervals as the task score, we focus on selecting the best reward
functionaccordingtohumanpreferences,yieldingpoliciesbetteralignedwiththetaskdescription.
DuetotheinherentrandomnessofLLMsampling,werun5experimentsforallmethods,including
PrefPPOandEUREKAandreportthehighestTSasthefinaltaskscore,denotedas(FTS),foreach
approach. AhigherFTSindicatesbetterperformanceacrossalltasks.
6PreprintPaper
Table1:Thefinaltaskscore(FTS)ofallmethodsacrossdifferenttasksinIssacGym. Thetopresult
andthosewithinonestandarddeviationarehighlightedinbold. Standarddeviationsareprovidedin
Table5ofAppendixA.5.1duetospacelimitations.
Cart. Ball. Quad. Anymal Ant Human. Franka Shadow Allegro
PrefPPO-49 499 499 -1.066 -1.861 0.743 0.457 0.0044 0.0746 0.0125
PrefPPO-150 499 499 -0.959 -1.818 0.171 0.607 0.0179 0.0617 0.0153
PrefPPO-1.5k 499 499 -0.486 -1.417 4.458 1.329 0.3248 0.0488 0.0284
PrefPPO-15k 499 499 -0.250 -1.357 4.626 1.317 0.0399 0.0468 0.0157
Eureka 499 499 -0.023 -0.003 10.86 9.059 0.9999 11.532 25.250
ICPL(Ours) 499 499 -0.0195 -0.007 12.04 9.227 0.9999 13.231 25.030
Figure 2: Distribution of which iteration is selected as the top-scoring iteration. While it is not
perfectlymonotonic,weobservethatthefinaliterationisgenerallythebestone,suggestingthatthe
inferredrewardisgraduallyapproachingtheground-truthreward.
5.4 RESULTSOFPROXYHUMANPREFERENCE
5.4.1 MAINRESULTS
InICPL,weusehuman-designedrewardsasproxiestosimulateidealhumanpreferences. Specifi-
cally,ineachiteration,weselecttherewardfunctionwiththehighestRTSasthegoodexampleand
the reward function with the lowest RTS as the bad example for feedback. In Eureka, the reward
function with the highest RTS is selected as the candidate reward function for feedback in each
iteration. InPrefPPO,human-designeddenserewardsareusedasthepreferencemetric: ifthecu-
mulativedenserewardoftrajectory1isgreaterthanthatoftrajectory2,thentrajectory1ispreferred
overtrajectory2. Table1showsthefinaltaskscore(FTS)forallmethodsacrossIsaacGymtasks.
ForICPLandPrefPPO,wetrackthenumberofsyntheticqueriesQrequiredasaproxyformeasur-
ingthelikelyrealhumaneffortinvolved,whichiscrucialformethodsthatrelyonhuman-in-the-loop
preferencefeedback. Specifically,wedefineasinglequeryasahumancomparingtwotrajectories
andprovidingapreference. InICPL,eachiterationgeneratesK rewardfunctionsamples,resulting
in K corresponding videos. The human compares these videos, first selecting the best one, then
pickingtheworstfromtheremainingK −1videos. AfterN =5iterations,thebestvideoofeach
iterationiscomparedtoselecttheoverallbest. ThenumberofhumanqueriesQcanbecalculated
asQ=(K−1)×2N −1. ForICPL,withK =6andN =5,thisresultsinQ=49. InPrefPPO,
thesimulatedhumanteachercomparestwosampledtrajectoriesandprovidesapreferencelabelto
updatetherewardmodel. WesetthemaximumnumberofqueriestoQ = 49,matchingICPL,and
alsotestQ=150,1.5k,and15k,denotedasPrefPPO-#QinTable1,tocomparethefinaltaskscore
(FTS)acrossdifferenttasks.
AsshowninTable1,forthesimplertaskslikeCartpoleandBallBalance,allmethodsachieveequal
performance. Notably,weobservethatfortheseparticularlysimpletasks,LLM-poweredmethods
likeEurekaandICPLcangeneratecorrectrewardfunctionsinazero-shotmanner,withoutrequir-
ing feedback. As a result, ICPL only requires querying the human 5 times, while PrefPPO, after
5queries, failstotrainareasonablerewardmodelwiththepreference-labeleddata. Forrelatively
morechallengingtasks, PrefPPO-49performssignificantlyworsethanICPLwhenusingthesame
7PreprintPaper
Table2: AblationstudiesonICPLmodules. Wereportthefinaltaskscore(FTS)andthefulltable
withstd. deviationsincludedcanbefoundinAppendixA.5.1. Therunshavefairlyhighvariance
so we highlight the top two results in bold. We observe that ICPL with all of the components is
consistentlythebestperforming,suggestingthatmostofthecomponentsareuseful.
Cart. Ball. Quad. Anymal Ant Human. Franka Shadow Allegro
ICPLw/oRT 499 499 -0.0340 -0.387 10.50 8.337 0.9999 10.769 25.641
ICPLw/oRTD 499 499 -0.0216 -0.009 10.53 9.419 1.0000 11.633 23.744
ICPLw/oRTDB 499 499 -0.0136 -0.014 11.97 8.214 0.5129 13.663 25.386
OpenLoop 499 499 -0.0410 -0.016 9.350 8.306 0.9999 9.476 23.876
ICPL(Ours) 499 499 -0.0195 -0.007 12.04 9.227 0.9999 13.231 25.030
numberofhumanqueries. Infact,PrefPPO-49failsinmosttasks. Asthenumberofhumanqueries
increases,PrefPPO’sperformanceimprovesacrossmosttasks,butitstillfallsnoticeablyshortcom-
paredtoICPL.ThisdemonstratesthatICPL,withtheintegrationofLLMs,canreducehumaneffort
inpreference-basedlearningbyatleast30times. ComparedtoEureka,whichusestaskscoresasa
rewardreflectionsignal,ICPLachievescomparableperformance. ThisindicatesthatICPL’suseof
LLMsforpreferencelearningiseffective.
Fromtheanalysisconductedacross7taskswherezero-shotgenerationofoptimalrewardfunctions
was not feasible in the first iteration, we examined which iteration’s RTS was chosen as the final
FTS.ThedistributionofRTSselectionsoveriterationsisillustratedinFig.2. Theresultsindicate
thatFTSselectionsdonotalwayscomefromthelastiteration; somearealsoderivedfromearlier
iterations. However, the majority of FTS selections originate from iterations 4 and 5, suggesting
thatICPLisprogressivelyrefiningandenhancingtherewardfunctionsoversuccessiveiterationsas
opposedtorandomlygeneratingdiverserewardfunctions.
5.5 METHODANALYSIS
To validate the effectiveness of ICPL’s module design, we conducted ablation studies. We aim to
answerseveralquestionsthatcouldunderminetheresultspresentedhere:
1. Arecomponentssuchastherewardtraceortherewarddifferencehelpful?
2. IstheLLMactuallyperformingpreferencelearning? Orisitsimplyzero-shotoutputting
thecorrectrewardfunctionduetothetaskbeinginthetrainingdata?
5.5.1 ABLATIONS
TheresultsoftheablationsareshowninTable2.Inthesestudies,“ICPLw/oRT”referstoremoving
therewardtracefromthepromptssenttotheLLMs. “ICPLw/oRTD”indicatestheremovalofboth
therewardtraceandthedifferencesbetweenhistoricalrewardfunctionsfromtheprompts. “ICPL
w/o RTDB” removes the reward trace, differences between historical reward functions, and bad
rewardfunctions,leavingonlythegoodrewardfunctionsandtheirevaluationintheprompts. The
“OpenLoop”configurationsamplesK ×N rewardfunctionswithoutanyfeedback,corresponding
totheabilityoftheLLMtozero-shotaccomplishthetask.
Duetothelargevarianceoftheexperiments(seeAppendix), wemarkthetoptworesultsinbold.
Asshown,ICPLachievestop2resultsin8outof9tasksandiscomparableontheAllegrotask. The
“OpenLoop” configuration performs the worst, indicating that our method does not solely rely on
GPT-4’seitherhavingrandomlyproducedtherightrewardfunctionorhavingmemorizedthereward
functionduringitstraining.ThisimprovementisfurtherdemonstratedinSec.5.5.2,whereweshow
the step-by-step improvements of ICPL through proxy human preference feedback. Additionally,
“ICPLw/oRT”underperformsonmultipletasks,highlightingtheimportanceofincorporatingthe
rewardtraceofhistoricalrewardfunctionsintotheprompts.
5.5.2 IMPROVEMENTANALYSIS
Table1presentstheperformanceachievedbyICPL.WhileitispossiblethattheLLMscouldgen-
erate an optimal reward function in a zero-shot manner, the primary focus of our analysis is not
solelyonabsoluteperformancevalues. Rather,weemphasizewhetherICPLiscapableofenhanc-
ingperformancethroughtheiterativeincorporationofpreferences. WecalculatedtheaverageRTS
8PreprintPaper
Figure3: AverageimprovementoftheRewardTaskScore(RTS)oversuccessiveiterationsrelative
to the first iteration in ICPL for two tasks with the largest improvements compared with “Open-
Loop”,AntandShadowHand.Theresultsdemonstratethemethod’seffectivenessinrefiningreward
functionsovertime. Notethattheimprovementisnearlymonotonic.
improvementoveriterationsrelativetothefirstiterationforthetwotaskswiththelargestimprove-
ments compared with “OpenLoop”, Ant and ShadowHand. As shown in Fig. 3, the RTS exhibits
anupwardtrend,demonstratingitseffectivenessinimprovingrewardfunctionsovertime. Wenote
that this trend is roughly monotonic, indicating that on average the LLM is using the preferences
toconstructrewardfunctionsthatareclosertotheground-truthreward. Wefurtheruseanexample
intheHumanoidtasktodemonstratehowICPLprogressivelygeneratedimprovedrewardfunctions
oversuccessiveiterationsinAppendixA.5.2.
5.6 RESULTSOFHUMAN-IN-THE-LOOPPREFERENCE
Toaddressthelimitationsofproxyhumanpreferences,whichsimulateidealizedhumanpreference
andmaynotfullycapturethechallengeshumansmayfaceinprovidingpreferences,weconducted
experiments with real human participants. We recruited 6 volunteers to participate in human-in-
the-loop experiments, including 5 tasks from IsaacGym and a newly designed task. None of the
volunteers had prior experience with these tasks, ensuring an unbiased evaluation based on their
preferences.
5.6.1 HUMANEXPERIMENTSETUP
Beforetheexperiment,eachvolunteerwasprovidedwithadetailedexplanationoftheexperiment’s
purposeandprocess. Additionally,volunteerswerefullyinformedoftheirrights,andwrittencon-
sent was obtained from each participant. The experimental procedure was approved by the de-
partment’s ethics committee to ensure compliance with institutional guidelines on human subject
research.
More specifically, each volunteer was assigned an account with a pre-configured environment to
ensure smooth operation. After starting the experiment, LLMs generated the first iteration of re-
wardfunctions. Oncethereinforcementlearningtrainingwascompleted, videoscorrespondingto
thepoliciesderivedfromeachrewardfunctionwereautomaticallyrendered. Volunteerscompared
the behaviors in the videos with the task descriptions and selected both the best and the worst-
performingvideos. Theythenenteredtherespectiveidentifiersofthesevideosintotheinteractive
interfaceandpressed“Enter”toproceed. ThehumanpreferencewasprocessedasanLLMprompt
forgeneratingfeedback,leadingtothenextiterationofrewardfunctiongeneration.
This training-rendering-selection process was repeated across 4 iterations. At the end of the final
iteration,thevolunteerswereaskedtoselectthebestvideofromthosepreviouslymarkedasgood,
designating it as the final result of the experiment. For IsaacGym tasks, the corresponding RTS
wasrecordedasTS.Itisimportanttonotethat,unlikeproxyhumanpreferenceexperimentswhere
theTSisthemaximumRTSacrossiterations,inthehuman-in-the-looppreferenceexperiment,TS
9PreprintPaper
Table3: Thefinaltaskscoreofhuman-in-the-looppreferenceacross5IsaacGymtasks. Thevalues
inparenthesesrepresentthestandarddeviation.
Quadcopter Ant Humanoid Shadow Allegro
OpenLoop -0.0410(0.32) 9.350(2.35) 8.306(1.63) 9.476(2.44) 23.876(7.91)
ICPL-proxy -0.0195(0.09) 12.040(1.69) 9.227(0.93) 13.231(1.88) 25.030(3.72)
ICPL-real -0.0183(0.29) 11.142(0.37) 8.392(0.53) 10.74(0.92) 24.134(6.52)
refers to the highest RTS chosen by the human, as human selections are not always based on the
maximumRTSateachiteration. GiventhatICPLrequiredreinforcementlearningtraininginevery
iteration, each experiment lasted two to three days. Each volunteer was assigned a specific task
and conducted five experiments, one for each task, with the highest TS being recorded as FTS in
IsaacGymtasks.
5.6.2 ISAACGYMTASKS
DuetothesimplicityoftheCartpole, BallBalance, Frankatasks, whereLLMswereabletozero-
shot generate correct reward functions without any feedback, these tasks were excluded from the
humantrials. TheAnymaltask,whichinvolvedcommandingaroboticdogtofollowrandomcom-
mands, was also excluded as it was difficult for humans to evaluate whether the commands were
followed based solely on the videos. For the 5 adopted tasks, we describe in the Appendix A.6.1
howhumansinfertasksthroughvideosandthepotentialreasonsthatmayleadtopreferencerank-
ingsthatdonotaccuratelyreflectthetask.
Table3presentstheFTSforthehuman-in-the-looppreferenceexperimentsconductedacross5suit-
ableIsaacGymtasks,labeledas“ICPL-real”.Theresultsoftheproxyhumanpreferenceexperiment
arelabeledas“ICPL-proxy”.Asobserved,theperformanceof“ICPL-real”iscomparableorslightly
lowerthanthatof“ICPL-proxy”inall5tasks,yetitstilloutperformsthe“OpenLoop”resultsin3
out of 5 tasks. This indicates that while humans may have difficulty providing consistent prefer-
encesfromvideosasproxies, theirfeedbackcanstillbeeffectiveinimprovingperformancewhen
combinedwithLLMs.
5.6.3 HUMANOIDJUMPTASK
Inourstudy, weintroducedanewtask: HumanoidJump, withthetaskdescriptionbeing“tomake
humanoidjumplikearealhuman.” Definingaprecisetaskmetricforthisobjectiveischallenging,
asthecriteriaforhuman-likejumpingarenoteasilyquantifiable. Thetask-specificpromptsusedin
thisexperimentaredetailedintheAppendixA.6.2.
Themostcommonbehaviorobservedinthistask,asillustrated
inFig.4,iswhatwerefertoasthe“leg-liftjump.”Thisbehav-
iorinvolvesinitiallyliftingonelegtoraisethecenterofmass,
followedbytheoppositelegpushingoffthegroundtoachieve
lift. The previously lifted leg is then lowered to extend air-
time. Variousadjustmentsofthecenterofmasswiththelifted
leg were also noted. This behavior meets the minimal metric
ofajump: achievingacertaindistanceofftheground. Iffeed- Figure4: Acommonbehavior.
back were provided based solely on this minimal metric, the
“leg-liftjump”wouldlikelybeselectedasacandidaterewardfunction. However,suchcandidates
showlimitedimprovementinsubsequentiterations,failingtoevolveintomorehuman-likejumping
behaviors.
Conversely, when real human preferences were used to guide the task, the results were notably
different. Thevolunteerjudgedtheoverallqualityofthehumanoid’sjumpbehaviorinsteadofjust
the metric of leaving the ground. Fig. 5 illustrates an example where the volunteer successfully
guided the humanoid towards a more human-like jump by selecting behaviors that, while initially
notoptimal,displayedpromisingmovementpatterns. TherewardfunctionsareshowninAppendix
A.6.2. Inthefirstiteration, “leg-liftjump”wasnotselecteddespitethehumanoidjumpingoffthe
ground. Instead, a video where the humanoid appears to attempt a jump using both legs, without
leavingtheground,waschosen. Bythefifthandsixthiterations,thehumanoiddemonstratedmore
10PreprintPaper
Figure 5: The humanoid learns a human-like jump by bending both legs and lowering the upper
bodytoshiftthecenterofmassinatrialofhuman-in-the-loopexperiments. Notethatbothlegsare
usedtojumpandtheagentbendsatthehips.
sophisticatedbehaviors, suchasbendingbothlegsandloweringtheupperbodytoshiftthecenter
ofmass,behaviorsthataremuchmoreakintoarealhumanjump.
6 CONCLUSION
Our proposed method, In-Context Preference Learning (ICPL), demonstrates significant potential
foraddressingthechallengesofpreferencelearningtasksthroughtheintegrationoflargelanguage
models. ByleveragingthegenerativecapabilitiesofLLMstoautonomouslyproducerewardfunc-
tions,anditerativelyrefiningthemusinghumanfeedback,ICPLreducesthecomplexityandhuman
efforttypicallyassociatedwithpreference-basedRL.Ourexperimentalresults,bothinproxyhuman
andhuman-in-the-loopsettings, showthatICPLnotonlysurpassestraditionalRLHFinefficiency
but also competes effectively with methods utilizing ground-truth rewards instead of preferences.
Furthermore,thesuccessofICPLincomplex,subjectivetaskslikehumanoidjumpinghighlightsits
versatilityincapturingnuancedhumanintentions,openingnewpossibilitiesforfutureapplications
incomplexreal-worldscenarioswheretraditionalrewardfunctionsaredifficulttodefine.
Limitations. WhileICPLdemonstratessignificantpotential,itfaceslimitationsintaskswherehu-
manevaluatorsstruggletoassessperformancefromvideoalone,suchasAnymal’s"followrandom
commands."Insuchcases,subjectivehumanpreferencesmaynotprovideadequateguidance. Fu-
tureworkwillexploreintegratinghumanpreferenceswithartificiallydesignedmetricstoenhance
theeasewithwhichhumanscanassessthevideos,ensuringmorereliableperformanceincomplex
tasks. Additionally, we observe that the performance of the task is qualitatively dependent on the
diversity of the initial reward functions that seed the search. While we do not study methods to
achievethishere,relyingontheLLMtoprovidethisinitialdiversityisacurrentlimitation.
REFERENCES
SaurabhAroraandPrashantDoshi. Asurveyofinversereinforcementlearning: Challenges,meth-
odsandprogress. ArtificialIntelligence,297:103500,2021.
Serena Booth, W. Bradley Knox, Julie Shah, Scott Niekum, Peter Stone, and Alessandro Allievi.
Theperilsoftrial-and-errorrewarddesign: Misdesignthroughoverfittingandinvalidtaskspec-
ifications. In Brian Williams, Yiling Chen, and Jennifer Neville (eds.), Thirty-Seventh AAAI
Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Appli-
cationsofArtificialIntelligence,IAAI2023,ThirteenthSymposiumonEducationalAdvancesin
ArtificialIntelligence,EAAI2023,Washington,DC,USA,February7-14,2023,pp.5920–5929.
11PreprintPaper
AAAIPress,2023. doi: 10.1609/AAAI.V37I5.25733. URLhttps://doi.org/10.1609/aaai.
v37i5.25733.
Banghao Chen, Zhaofeng Zhang, Nicolas Langrené, and Shengxin Zhu. Unleashing the poten-
tial of prompt engineering in large language models: a comprehensive review. arXiv preprint
arXiv:2310.14735,2023.
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcementlearningfromhumanpreferences. Advancesinneuralinformationprocessingsys-
tems,30,2017.
Yuqing Du, Ksenia Konyushkova, Misha Denil, Akhil Raju, Jessica Landon, Felix Hill, Nando
de Freitas, and Serkan Cabi. Vision-language models as success detectors. arXiv preprint
arXiv:2303.07280,2023.
LinxiFan,GuanzhiWang,YunfanJiang,AjayMandlekar,YuncongYang,HaoyiZhu,AndrewTang,
De-AnHuang, YukeZhu, andAnimaAnandkumar. Minedojo: Buildingopen-endedembodied
agentswithinternet-scaleknowledge. AdvancesinNeuralInformationProcessingSystems, 35:
18343–18362,2022.
LouieGiray. Promptengineeringwithchatgpt: aguideforacademicwriters. Annalsofbiomedical
engineering,51(12):2629–2633,2023.
Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward
learning from human preferences and demonstrations in atari. Advances in neural information
processingsystems,31,2018.
Hong Jun Jeon, Smitha Milli, and Anca Dragan. Reward-rational (implicit) choice: A unifying
formalism for reward learning. Advances in Neural Information Processing Systems, 33:4415–
4426,2020.
Siddharth Karamcheti, Suraj Nair, Annie S Chen, Thomas Kollar, Chelsea Finn, Dorsa Sadigh,
and Percy Liang. Language-driven representation learning for robotics. arXiv preprint
arXiv:2302.12766,2023.
MinaeKwon,SangMichaelXie,KaleshaBullard,andDorsaSadigh. Rewarddesignwithlanguage
models. arXivpreprintarXiv:2303.00001,2023.
KiminLee,LauraSmith,AncaDragan,andPieterAbbeel. B-pref:Benchmarkingpreference-based
reinforcement learning. In Thirty-fifth Conference on Neural Information Processing Systems
DatasetsandBenchmarksTrack(Round1).
Kimin Lee, Laura Smith, and Pieter Abbeel. Pebble: Feedback-efficient interactive rein-
forcement learning via relabeling experience and unsupervised pre-training. arXiv preprint
arXiv:2106.05091,2021a.
KiminLee,LauraSmith,AncaDragan,andPieterAbbeel. B-pref:Benchmarkingpreference-based
reinforcement learning. In Thirty-fifth Conference on Neural Information Processing Systems
Datasets and Benchmarks Track (Round 1), 2021b. URL https://openreview.net/forum?
id=ps95-mkHF_.
Fei Liu et al. Learning to summarize from human feedback. In Proceedings of the 58th Annual
MeetingoftheAssociationforComputationalLinguistics,2020.
YechengJasonMa,ShagunSodhani,DineshJayaraman,OsbertBastani,VikashKumar,andAmy
Zhang. Vip: Towardsuniversalvisualrewardandrepresentationviavalue-implicitpre-training.
arXivpreprintarXiv:2210.00030,2022.
YechengJasonMa,WilliamLiang,GuanzhiWang,De-AnHuang,OsbertBastani,DineshJayara-
man, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via
codinglargelanguagemodels. arXivpreprintarXiv:2310.12931,2023.
12PreprintPaper
YechengJasonMa,WilliamLiang,Hung-JuWang,SamWang,YukeZhu,LinxiFan,OsbertBas-
tani, and Dinesh Jayaraman. Dreureka: Language model guided sim-to-real transfer. arXiv
preprintarXiv:2406.01967,2024.
EduardoMosqueira-Rey,ElenaHernández-Pereira,DavidAlonso-Ríos,JoséBobes-Bascarán,and
ÁngelFernández-Leal. Human-in-the-loopmachinelearning: astateoftheart. ArtificialIntelli-
genceReview,56(4):3005–3054,2023.
SoroushNasiriany,FeiXia,WenhaoYu,TedXiao,JackyLiang,IshitaDasgupta,AnnieXie,Danny
Driess,AyzaanWahid,ZhuoXu,etal. Pivot: Iterativevisualpromptingelicitsactionableknowl-
edgeforvlms. arXivpreprintarXiv:2402.07872,2024.
Andrew Y Ng, Stuart Russell, et al. Algorithms for inverse reinforcement learning. In Icml, vol-
ume1,pp. 2,2000.
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to fol-
lowinstructionswithhumanfeedback. Advancesinneuralinformationprocessingsystems, 35:
27730–27744,2022.
ZhenghaoMarkPeng,WenjieMo,ChendaDuan,QuanyiLi,andBoleiZhou. Learningfromactive
humaninvolvementthroughproxyvaluepropagation.Advancesinneuralinformationprocessing
systems,36,2024.
CarlOrgeRetzlaff,SrijitaDas,ChristabelWayllace,PayamMousavi,MohammadAfshari,Tianpei
Yang,AnnaSaranti,AlessaAngerschmid,MatthewETaylor,andAndreasHolzinger. Human-in-
the-loopreinforcementlearning: Asurveyandpositiononrequirements,challenges,andoppor-
tunities. JournalofArtificialIntelligenceResearch,79:359–415,2024.
JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximalpolicy
optimizationalgorithms,2017. URLhttps://arxiv.org/abs/1707.06347.
SatinderSingh,RichardLLewis,andAndrewGBarto. Wheredorewardscomefrom. InProceed-
ingsoftheannualconferenceofthecognitivesciencesociety,pp.2601–2606.CognitiveScience
Society,2009.
RichardSSutton. Reinforcementlearning: Anintroduction. ABradfordBook,2018.
YufeiWang, ZhanyiSun, JesseZhang, ZhouXian, ErdemBiyik, DavidHeld, andZackoryErick-
son. Rl-vlm-f: Reinforcementlearningfromvisionlanguagefoundationmodelfeedback. arXiv
preprintarXiv:2402.03681,2024.
Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf El-
nashar, Jesse Spencer-Smith, and Douglas C Schmidt. A prompt pattern catalog to enhance
promptengineeringwithchatgpt. arXivpreprintarXiv:2302.11382,2023.
ChristianWirth,RiadAkrour,GerhardNeumann,andJohannesFürnkranz. Asurveyofpreference-
based reinforcement learning methods. Journal of Machine Learning Research, 18(136):1–46,
2017.
JeffWu,LongOuyang,DanielMZiegler,NisanStiennon,RyanLowe,JanLeike,andPaulChris-
tiano. Recursivelysummarizingbookswithhumanfeedback. arXivpreprintarXiv:2109.10862,
2021.
Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montserrat Gonzalez
Arenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, Brian Ichter,
Ted Xiao, Peng Xu, Andy Zeng, Tingnan Zhang, Nicolas Heess, Dorsa Sadigh, Jie Tan, Yuval
Tassa,andFeiXia. Languagetorewardsforroboticskillsynthesis. InJieTan,MarcToussaint,
and Kourosh Darvish (eds.), Conference on Robot Learning, CoRL 2023, 6-9 November 2023,
Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research, pp. 374–404.
PMLR,2023. URLhttps://proceedings.mlr.press/v229/yu23a.html.
13PreprintPaper
A APPENDIX
Wewouldsuggestvisitinghttps://sites.google.com/view/few-shot-icpl/homeformorein-
formationandvideos.
A.1 FULLPROMPTS
ThepromptsusedinICPLforsynthesizingrewardfunctionsarepresentedinPrompts1,2,and3.
ThepromptforgeneratingthedifferencesbetweenvariousrewardfunctionsisshowninPrompt4.
Prompt1: InitialSystemPromptsofSynthesizingRewardFunctions
You are a reward engineer trying to write reward functions to solve reinforcement learning
tasks as effective as possible.
Your goal is to write a reward function for the environment that will help the agent learn the
task described in text.
Your reward function should use useful variables from the environment as inputs. As an example
, the reward function signature can be:
@torch.jit.script
def compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor,
Dict[str, torch.Tensor]]:
...
return reward, {}
Since the reward function will be decorated with @torch.jit.script, please make sure that the
code is compatible with TorchScript (e.g., use torch tensor instead of numpy array).
Make sure any new tensor or variable you introduce is on the same device as the input tensors.
Prompt2: FeedbackPrompts
The reward function has been iterated {current_iteration} rounds.
In each iteration, a good reward function and a bad reward function are generated.
The good reward function generated in the x-th iteration is denoted as "iterx-good", and the
bad reward function generated is denoted as "iterx-bad".
The following outlines the differences between these reward functions.
We trained an RL policy using iter1-good reward function code and tracked the values of the
individual components in the reward function after every {epoch_freq} epochs and the
maximum, mean, minimum values encountered:
<REWARD FEEDBACK>
The difference between iter2-good and iter1-good is: <DIFFERENCE>
<REPEAT UNTIL THE CURRENT ITERATION>
Next, the two reward functions generated in the {current_iteration_ordinal} iteration are
provided.
The 1st generated reward function is as follows:
<REWARD FUNCTION>
We trained an RL policy using the 1st reward function code and tracked the values of the
individual components in the reward function after every {epoch_freq} epochs and the
maximum, mean, minimum values encountered:
<REWARD FEEDBACK>
The 2nd generated reward function is as follows:
<REWARD FUNCTION>
We trained an RL policy using the 2nd reward function code and tracked the values of the
individual components in the reward function after every {epoch_freq} epochs and the
maximum, mean, minimum values encountered:
<REWARD FEEDBACK>
The following content is the most important information.
Good example: 1st reward function. Bad example: 2nd reward function.
You need to modify based on the good example. DO NOT based on the code of the bad example.
Please carefully analyze the policy feedback and provide a new, improved reward function that
can better solve the task. Some helpful tips for analyzing the policy feedback:
(1) If the values for a certain reward component are near identical throughout, then this
means RL is not able to optimize this component as it is written. You may consider
(a) Changing its scale or the value of its temperature parameter
(b) Re-writing the reward component
(c) Discarding the reward component
(2) If some reward components’ magnitude is significantly larger, then you must re-scale
its value to a proper range
Please analyze each existing reward component in the suggested manner above first, and then
write the reward function code.
14PreprintPaper
Prompt3: PromptsofTipsforWritingRewardFunctions
The output of the reward function should consist of two items:
(1) the total reward,
(2) a dictionary of each individual reward component.
The code output should be formatted as a python code string: "‘‘‘python ... ‘‘‘".
Some helpful tips for writing the reward function code:
(1) You may find it helpful to normalize the reward to a fixed range by applying
transformations like torch.exp to the overall reward or its components
(2) If you choose to transform a reward component, then you must also introduce a
temperature parameter inside the transformation function; this parameter must be a named
variable in the reward function and it must not be an input variable. Each transformed
reward component should have its own temperature variable
(3) Make sure the type of each input variable is correctly specified; a float input
variable should not be specified as torch.Tensor
(4) Most importantly, the reward code’s input variables must contain only attributes of
the provided environment class definition (namely, variables that have prefix self.).
Under no circumstance can you introduce new input variables.
Prompt4: PromptsofDescribingDifferences
You are an engineer skilled at comparing the differences between two reward function code
snippets used in reinforcement learning.
Your goal is to describe the differences between two reward function code snippets.
The following are two reward functions written in Python code used for the task:
<TASK_DESCRIPTION>
The first reward function is as follows:
<REWARD_FUNCTION>
The second reward function is as follows:
<REWARD_FUNCTION>
Please directly describe the differences between these two codes. No additional descriptions
other than the differences are required.
A.2 ICPLDETAILS
ThefullpseudocodeofICPLislistedinAlgo. 2.
A.3 BASELINEDETAILS
ThebaselinePrefPPOadoptedinourexperimentscomprisestwoprimarycomponents: agentlearn-
ing and reward learning, as outlined in (Lee et al., 2021b). Throughout this process, the method
maintainsapolicydenotedasπ andarewardmodelrepresentedbyrˆ .
φ ψ
Agent Learning. In the agent learning phase, the agent interacts with the environment and col-
lects experiences. The policy is subsequently trained using reinforcement learning, to maximize
the cumulative rewards provided by the reward model rˆ . We utilize the on-policy reinforcement
ψ
learningalgorithmPPO(Schulmanetal.,2017)asthebackbonealgorithmfortrainingthepolicy.
Additionally, we apply unsupervised pre-training to match the performance of the original bench-
mark. Specifically, during earlier iterations, when the reward model has not collected sufficient
trajectories and exhibits limited progress, we utilize the state entropy of the observations, defined
as H(s) = −E [logp(s)], as the goal for agent training. During this process, trajectories of
s∼p(s)
varyinglengthsarecollected. Formally,atrajectoryσ isdefinedasasequenceofobservationsand
actions(s ,a ),...,(s ,a )thatrepresentsthecompleteinteractionoftheagentwiththeenviron-
1 1 t t
ment,concludingattimestept.
Reward Learning. A preference predictor is developed using the current reward model to align
withhumanpreferences,formulatedasfollows:
exp(cid:0)(cid:80)
rˆ
(s1,a1)(cid:1)
P [σ1 ≻σ0]= t ψ t t ,
ψ (cid:80) exp(cid:0)(cid:80)
rˆ
(si,ai)(cid:1)
i∈{0,1} t ψ t t
whereσ =(s0,a0),...,(s0,a0)andσ =(s1,a1),...,(s1,a1)representtwocompletetrajec-
0 1 1 l0 l0 1 1 1 l1 l1
torieswithdifferenttrajectorylengthl andl . P [σ1 ≻ σ0]denotestheprobabilitythattrajectory
0 1 ψ
σ1ispreferredoverσ0asindicatedbythepreferencepredictor. IntheoriginalPrefPPOframework,
testtasktrajectoriesareoffixedlength,allowingfortheextractionoffixed-lengthsegmentstotrain
therewardmodel. However, thetasksinthispaperhavevaryingtrajectorylengths, soweusefull
15PreprintPaper
Algorithm2:ICPL
Input: #iterationsN,#samplesineachiterationsK,environmentEnv,codingLLMLLM ,
RF
differenceLLMLLM
Diff
FunctionFeedback(Env,RF):
1
returnThevaluesofeachcomponentthatmakeupRFduringthetrainingprocessinEnv
2
FunctionHistory(RFlist,Env,LLM ):
3 Diff
HistoryFeedback←“”
4
fori←1tolen(RFlist)−1do
5
// The reward trace of historical reward functions
HistoryFeedback←HistoryFeedback+Feedback(Env,RFlist[i−1])
6
// The differences between historical reward functions
HistoryFeedback←
7
HistoryFeedback+LLM (DifferencePrompt+RFlist[i]+RFlist[i−1])
Diff
end
8
returnHistoryFeedback
9
// Initialize the prompt containing the environment context and task description
Prompt←InitializePrompt
10
RFlist←[]
11
fori←1toN do
12
RF ,...,RF ←LLM (Prompt,K)
13 1 K RF
whileanyofRF ,...,RF isnotexecutabledo
14 1 K
j ,...,j ←Indexofnon-executablerewardfunctions
15 1 K′
// Regenerate non-executable reward functions
RF ,...,RF ←LLM (Prompt,K′)
16 j1 j K′ RF
end
17
// Render videos for sampled reward functions
Video ,...,Video ←Render(Env,RF ),...,Render(Env,RF )
18 1 K 1 K
// Human selects the most preferred and least preferred videos
G,B ←Human(Video ,...,Video )
19 1 K
GoodRF,BadRF←RF ,RF
20 G B
RFlist.append(GoodRF)
21
// Update prompt for feedback
Prompt←
22
GoodRF+Feedback(Env,GoodRF)+BadRF+Feedback(Env,BadRF)+PreferencePrompt
Prompt←Prompt+History(RFlist,Env,LLM )
23 Diff
end
24
16PreprintPaper
trajectorypairsastrainingdatainsteadofsegments. Wealsotriedzero-paddingtrajectoriestothe
maximumepisodelengthandthensegmentingthem,butthisapproachwasineffectiveinpractice.
To provide more effective labels, the original PrefPPO utilizes dense rewards r to simulate oracle
humanpreferences,asfollows:
(cid:26) 1 If (cid:80) r(s1,a1)>(cid:80) r(s1,a1)
P[σ1 ≻σ0]= t t t t t t ,
0 Otherwise
TheprobabilityP[σ1 ≻ σ0]reflectsthepreferenceoftheidealteacher,whichisdeterministicand
alwaysrankstrajectoriescorrectly,withoutincorporatingnoise.Weutilizethedefaultdenserewards
in the adopted IsaacGym tasks, which differ from ICPL and EUREKA, both of which use sparse
rewards(taskmetrics)astheproxypreference. Whilewealsoexperimentedwithsparserewardsin
PrefPPOandfoundsimilarperformance(refertoTable7),weoptedtoretaintheoriginalPrefPPO
approach in all experiments. The reward model is trained by minimizing the cross-entropy loss
between the predictor and labels, utilizing trajectories sampled from the agent learning process.
Notethatsincetheagentlearningprocessrequiressignificantlymoreexperiencesfortrainingthan
rewardtraining,weonlyusetrajectoriesfromasubsetoftheenvironmentsforrewardtraining.
Tosampletrajectoriesforrewardlearning,weemploythedisagreementsamplingschemefrom(Lee
etal.,2021b)toenhancethetrainingprocess.Thisschemefirstgeneratesalargerbatchoftrajectory
pairsuniformlyatrandomandthenselectsasmallerbatchwithhighvarianceacrossanensembleof
preferencepredictors. Theselectedpairsareusedtoupdatetherewardmodel.
Forafaircomparison,werecordedthenumberoftimesPrefPPOqueriedtheoraclehumansimulator
to compare two trajectories and obtain labels during the reward learning process, using this as a
measureofthehumaneffortinvolved.Intheproxyhumanexperiment,wesetthemaximumnumber
ofhumanqueriesQto49,150,1.5k,and15k.Oncethislimitisreached,therewardmodelceasesto
update,andonlythepolicymodelisupdatedviaPPO.Algo. 3illustratesthepseudocodeforreward
learning.
Algorithm3:RewardLearningofPrefPPO
Input: rewardmodelrˆ ,#samplesforhumanqueriespertimeMbSize,#maximaliterations
ψ
forrewardlearningMaxUpdate,maximalnumberofhumanqueriesQ,environments
Env
LabeledQueries←[]
1
HumanQueryCount←0
2
FunctionTrainReward(rˆ ,Trajectories):
3 ψ
// Use disagreement sampling to sample trajectories
σ ,σ ←DisagreementSampling(Trajectories,MbSize)
4 0 1
for(x ,x )in(σ ,σ )do
5 0 1 0 1
// Give oracle human preferences between two trajectories according to the sum
of dense reward.
LabeledQueries←LabeledQueries+(x ,x ,HumanQuery(x ,x ))
6 0 1 0 1
// In experiments, we do not add HumanQueryCount if the pair has already been
queried before
HumanQueryCount←HumanQueryCount+1
7
ifHumanQueryCount>Qthen
8
BREAK
9
end
10
end
11
fori←1toMaxUpdatedo
12
// Update reward model by minimizing the cross entropy loss and record the
accuracy on all pairs.
rˆ ,Accuracy←RewardLearning(rˆ ,LabeledQueries)
13 ψ ψ
ifAccuracy≥97%then
14
BREAK
15
end
16
end
17
returnrˆ
18 ψ
17PreprintPaper
Algo. 4illustratesthepseudocodeforPrefPPO.
Algorithm4:PrefPPO
Input: #iterationsB,#unsupervisedlearningiterationsM,#rolloutstepsS,rewardmodel
rˆ ,#environmentsforrewardlearningE,#iterationsforcollectingtrajectories
ψ
RewardTrainingInterval,maximalnumberofhumanqueriesQ,environmentsEnv
HumanQueryCount←0
1
Trajectories←[]
2
FunctionTrainReward(rˆ ,Trajectories):
3 ψ
FunctionCollectRollout(RewardType,S,Policy,rˆ ,Env):
4 ψ
RolloutBuffer←[]
5
forj ←1toS do
6
Action←Policy(Observation)
7
// Here EnvDones is a binary sequence replied from the envrionment,
representing whether the environments are done.
NewObservation, EnvReward, EnvDones←Env(Actions)
8
ifRewardType==Unsuperthen
9
PredReward←ComputeStateEntropy(Observation)
10
end
11
else
12
PredReward←rˆ (Observation,Action)
13 ψ
end
14
// Collect trajectories for reward learning
Trajectories←Trajectories+(Observation,Action,EnvDones,EnvReward)
15
// Add complete trajectory to reward model
fork ←1toE do
16
ifEnvDones[Env[k]]then
17
AddTrajectory(rˆ ,Trajectories[k])
18 ψ
Trajectories[k]←[]
19
end
20
end
21
// Reward Learning
ifj is divisible by RewardTrainingIntervalandHumanQueryCount<Qthen
22
rˆ ←TrainReward(rˆ ,Trajectories)
23 ψ ψ
end
24
// Collect rollouts for agent learning
RolloutBuffer←RolloutBuffer+(Observation,Action,PredReward)
25
Observation←NewObservation
26
end
27
returnRolloutBuffer
28
Policy←Initialize
29
fori←1toBdo
30
// Collect rollouts and trajectories
if i<M then
31
RolloutBuffer←CollectRollout(Unsuper,S,Policy,rˆ ,Env)
32 ψ
end
33
else
34
RolloutBuffer←CollectRollout(RewardModel,S,Policy,rˆ ,Env)
35 ψ
end
36
// Agent Learning: Train agent with the collect RolloutBuffer via PPO, omitted
here
AgentLearning(Policy,RolloutBuffer)
37
end
38
18PreprintPaper
A.4 ENVIRONMENTDETAILS
In Table 4, we present the observation and action dimensions, along with the task description and
taskmetricsfor9tasksinIsaacGym.
Environment(obsdim,actiondim)
TaskDescription
TaskMetric
Cartpole(4,1)
Tobalanceapoleonacartsothatthepolestaysupright
duration
Quadcopter(21,12)
Tomakethequadcopterreachandhovernearafixedposition
-cur_dist
FrankaCabinet(23,9)
Toopenthecabinetdoor
1ifcabinet_pos>0.39
Anymal(48,12)
Tomakethequadrupedfollowrandomlychosenx,y,andyawtargetvelocities
-(linvel_error+angvel_error)
BallBalance(48,12)
Tokeeptheballonthetabletopwithoutfalling
duration
Ant(60,8)
Tomaketheantrunforwardasfastaspossible
cur_dist-prev_dist
AllegroHand(88,16)
Tomakethehandspintheobjecttoatargetorientation
numberofconsecutivesuccesseswherecurrentsuccessis1ifrot_dist<0.1
Humanoid(108,21)
Tomakethehumanoidrunasfastaspossible
cur_dist-prev_dist
ShadowHand(211,20)
Tomaketheshadowhandspintheobjecttoatargetorientation
numberofconsecutivesuccesseswherecurrentsuccessis1ifrot_dist<0.1
Table4: DetailsofIsaacGymTasks.
A.5 PROXYHUMANPREFERENCE
A.5.1 ADDITIONALRESULTS
Due to the high variance in LLMs performance, we report the standard deviation across 5 experi-
mentsasasupplement,whichispresentedinTable5andTable6. Wealsoreportthefinaltaskscore
ofPrefPPOusingsparserewardsasthepreferencemetricforthesimulatedteacherinTable7.
Cart. Ball. Quad. Anymal Ant Human. Franka Shadow Allegro
PrefPPO-49 499(0) 499(0) -1.066(0.16) -1.861(0.03) 0.743(0.20) 0.457(0.09) 0.0044(0.00) 0.0746(0.02) 0.0125(0.003)
PrefPPO-150 499(0) 499(0) -0.959(0.15) -1.818(0.07) 0.171(0.05) 0.607(0.02) 0.0179(0.01) 0.0617(0.01) 0.0153(0.004)
PrefPPO-1.5k 499(0) 499(0) -0.486(0.11) -1.417(0.21) 4.458(1.30) 1.329(0.33) 0.3248(0.12) 0.0488(0.01) 0.0284(0.005)
PrefPPO-15k 499(0) 499(0) -0.250(0.06) -1.357(0.02) 4.626(0.57) 1.317(0.34) 0.0399(0.02) 0.0468(0.00) 0.0157(0.003)
Eureka 499(0) 499(0) -0.023(0.07) -0.003(0.38) 10.86(0.85) 9.059(0.83) 0.9999(0.23) 11.532(1.38) 25.250(9.583)
ICPL(Ours) 499(0) 499(0) -0.0195(0.09) -0.007(0.35) 12.04(1.69) 9.227(0.93) 0.9999(0.24) 13.231(1.88) 25.030(3.721)
Table 5: The final task score of all methods across different tasks in IssacGym. The values in
parenthesesrepresentthestandarddeviation.
19PreprintPaper
Cart. Ball. Quad. Anymal Ant Human. Franka Shadow Allegro
ICPLw/oRT 499(0) 499(0) -0.0340(0.05) -0.387(0.26) 10.50(0.45) 8.337(0.60) 0.9999(0.25) 10.769(2.30) 25.641(9.46)
ICPLw/oRTD 499(0) 499(0) -0.0216(0.14) -0.009(0.38) 10.53(0.39) 9.419(2.10) 1.0000(0.18) 11.633(1.25) 23.744(8.80)
ICPLw/oRTDB 499(0) 499(0) -0.0136(0.03) -0.014(0.42) 11.97(0.71) 8.214(2.88) 0.5129(0.06) 13.663(1.83) 25.386(3.42)
OpenLoop 499(0) 499(0) -0.0410(0.32) -0.016(0.50) 9.350(2.34) 8.306(1.63) 0.9999(0.22) 9.476(2.44) 23.876(7.91)
ICPL(Ours) 499(0) 499(0) -0.0195(0.09) -0.007(0.35) 12.04(1.69) 9.227(0.93) 0.9999(0.24) 13.231(1.88) 25.030(3.721)
Table6: AblationstudiesonICPLmodules. Thevaluesinparenthesesrepresentthestandarddevi-
ation.
Cart. Ball. Quad. Anymal Ant Human. Franka Shadow Allegro
PrefPPO-49 499(0) 499(0) -1.288(0.04) -1.833(0.05) 0.281(0.06) 0.855(0.24) 0.0009(0.00) 0.1178(0.03) 0.1000(0.024)
PrefPPO-150 499(0) 499(0) -1.288(0.02) -1.814(0.07) 0.545(0.16) 0.546(0.09) 0.0012(0.00) 0.0517(0.01) 0.0544(0.010)
PrefPPO-1.5k 499(0) 499(0) -1.292(0.05) -1.583(0.13) 2.235(0.63) 2.480(0.59) 0.0077(0.00) 0.0495(0.01) 0.0667(0.017)
PrefPPO-15k 499(0) 499(0) -1.322(0.04) -1.611(0.12) 3.694(0.86) 1.867(0.19) 0.0066(0.00) 0.0543(0.01) 0.1002(0.030)
Eureka 499(0) 499(0) -0.023(0.07) -0.003(0.38) 10.86(0.85) 9.059(0.83) 0.9999(0.23) 11.532(1.38) 25.250(9.583)
(Ours) 499(0) 499(0) -0.0195(0.09) -0.007(0.35) 12.04(1.69) 9.227(0.93) 0.9999(0.24) 13.231(1.88) 25.030(3.721)
Table 7: The final task score of all methods across different tasks in IssacGym, where PrefPPO
uses sparse rewards as the preference metric for the simulated teacher. The values in parentheses
representthestandarddeviation.
A.5.2 IMPROVEMENTANALYSIS
WeuseatrialoftheHumanoidtasktoillustratehowICPLprogressivelygeneratedimprovedreward
functions over successive iterations. The task description is “to make the humanoid run as fast
as possible”. Throughout five iterations, adjustments were made to the penalty terms and reward
weightings. In the first iteration, the total reward was calculated as 0.5×speed_reward+0.25×
deviation_reward+0.25×action_reward,yieldinganRTSof5.803.Thespeedrewardanddeviation
rewardmotivatethehumanoidtorunfast,whiletheactionrewardpromotessmoothermotion.Inthe
seconditeration,theweightofthespeedrewardwasincreasedto0.6,whiletheweightsfordeviation
and action rewards were adjusted to 0.2 each, improving the RTS to 6.113. In the third iteration,
theactionpenaltywasraisedandtherewardweightswerefurthermodifiedto0.7×speed_reward,
0.15×deviation_reward,and0.15×action_reward,resultinginanRTSof7.915. Duringthefourth
iteration, the deviation penalty was reduced to 0.35 and the action penalty was lowered, with the
rewardweightssetto0.8, 0.1, and0.1forspeed, deviation, andactionrewards, respectively. This
changeledtoanRTSof8.125. Finally,inthefifthiteration,anadditionaluprightrewardtermwas
incorporated, withthetotalrewardcalculatedas0.7×speed_reward+0.1×deviation_reward+
0.1×action_reward+0.1×upright_reward. ThisadjustmentproducedthehighestRTSof8.232.
Belowarethespecificrewardfunctionsproducedateachiterationduringoneexperiment.
HumanoidTask: RewardFunctionwithhighestRTS(5.803)ofIteration1
def compute_reward(root_states: torch.Tensor, actions: torch.Tensor
) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
velocity = root_states[: , 7:10]
forward_velocity = velocity[:, 0]
target_velocity = 5.0
deviation_penalty = 0.5
action_penalty = 0.1
# Measure how fast the humanoid is going
speed_reward = torch.exp((forward_velocity - target_velocity))
# Penalize deviation from the target velocity
deviation = torch.abs(forward_velocity - target_velocity)
deviation_reward = torch.exp(-deviation_penalty * deviation)
# Penalize actions, to induce smoother motion
sum_actions = torch.sum(actions ** 2, dim=1)
action_reward = torch.exp(-action_penalty * sum_actions)
20PreprintPaper
rewards = {’speed_reward’: speed_reward, ’deviation_reward’:
deviation_reward, ’action_reward’: action_reward }
total_reward = 0.5 * speed_reward + 0.25 * deviation_reward +
0.25 * action_reward
total_reward = total_reward.to(deviation.device)
return total_reward, rewards
HumanoidTask: RewardFunctionwithhighestRTS(6.113)ofIteration2
def compute_reward(root_states: torch.Tensor, actions: torch.Tensor
) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
velocity = root_states[: , 7:10]
forward_velocity = velocity[:, 0]
target_velocity = 5.0
deviation_penalty = 0.5
action_penalty = 0.1
# Measure how fast the humanoid is going
speed_reward = torch.exp((forward_velocity - target_velocity))
# Penalize deviation from the target velocity
deviation = torch.abs(forward_velocity - target_velocity)
deviation_reward = torch.exp(-deviation_penalty * deviation)
# Penalize actions, to induce smoother motion
sum_actions = torch.sum(actions ** 2, dim=1)
action_reward = torch.exp(-action_penalty * sum_actions)
rewards = {’speed_reward’: speed_reward, ’deviation_reward’:
deviation_reward, ’action_reward’: action_reward }
# Adjusted weights in total reward calculation
total_reward = 0.6 * speed_reward + 0.2 * deviation_reward +
0.2 * action_reward # Changed weights
total_reward = total_reward.to(deviation.device)
return total_reward, rewards
HumanoidTask: RewardFunctionwithhighestRTS(7.915)ofIteration3
def compute_reward(root_states: torch.Tensor, actions: torch.Tensor
) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
velocity = root_states[: , 7:10]
forward_velocity = velocity[:, 0]
target_velocity = 5.0
deviation_penalty = 0.5
action_penalty = 0.15 # Increased action penalty
# Measure how fast the humanoid is going
speed_reward = torch.exp((forward_velocity - target_velocity))
# Penalize deviation from the target velocity
deviation = torch.abs(forward_velocity - target_velocity)
deviation_reward = torch.exp(-deviation_penalty * deviation)
# Penalize actions, to induce smoother motion
sum_actions = torch.sum(actions ** 2, dim=1)
action_reward = torch.exp(-action_penalty * sum_actions)
21PreprintPaper
rewards = {’speed_reward’: speed_reward, ’deviation_reward’:
deviation_reward, ’action_reward’: action_reward }
# Adjusted weights in total reward calculation
total_reward = 0.7 * speed_reward + 0.15 * deviation_reward +
0.15 * action_reward # Changed weights
total_reward = total_reward.to(deviation.device)
return total_reward, rewards
HumanoidTask: RewardFunctionwithhighestRTS(8.125)ofIteration4
def compute_reward(root_states: torch.Tensor, actions: torch.Tensor
) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
velocity = root_states[:, 7:10]
forward_velocity = velocity[:, 0]
target_velocity = 5.0
deviation_penalty = 0.35 # decrease penalty for deviation
action_penalty = 0.1 # decrease penalty for action
# Measure how fast the humanoid is going
speed_reward = torch.exp((forward_velocity - target_velocity))
# Penalize deviation from the target velocity
deviation = torch.abs(forward_velocity - target_velocity)
deviation_reward = torch.exp(-deviation_penalty * deviation)
# Penalize actions, to induce smoother motion
sum_actions = torch.sum(actions ** 2, dim=1)
action_reward = torch.exp(-action_penalty * sum_actions)
rewards = {’speed_reward’: speed_reward, ’deviation_reward’:
deviation_reward, ’action_reward’: action_reward }
# Adjust the weights in total reward calculation
total_reward = 0.8 * speed_reward + 0.1 * deviation_reward +
0.1 * action_reward # Changed weights
total_reward = total_reward.to(deviation.device)
return total_reward, rewards
HumanoidTask: RewardFunctionwithhighestRTS(8.233)ofIteration5
def compute_reward(root_states: torch.Tensor, actions: torch.Tensor
, up_vec: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.
Tensor]]:
velocity = root_states[: , 7:10]
forward_velocity = velocity[:, 0]
target_velocity = 5.0
deviation_penalty = 0.35 # Reduced deviation penalty
action_penalty = 0.1 # Decreased action penalty
# Measure how fast the humanoid is going
speed_reward = torch.exp((forward_velocity - target_velocity))
# Penalize deviation from the target velocity
deviation = torch.abs(forward_velocity - target_velocity)
deviation_reward = torch.exp(-deviation_penalty * deviation)
# Penalize actions, to induce smoother motion
sum_actions = torch.sum(actions ** 2, dim=1)
action_reward = torch.exp(-action_penalty * sum_actions)
22PreprintPaper
# Reward for maintaining an upright position
upright_penalty = 1.0 # New upright penalty for the humanoid
upright_reward = torch.exp(-upright_penalty * (1 - up_vec[:,
2])) # Added upright reward
rewards = {’speed_reward’: speed_reward, ’deviation_reward’:
deviation_reward, ’action_reward’: action_reward, ’
upright_reward’: upright_reward }
# Adjusted weights in total reward calculation
total_reward = 0.7 * speed_reward + 0.1 * deviation_reward +
0.1 * action_reward + 0.1 * upright_reward # Added upright
reward to total
total_reward = total_reward.to(deviation.device)
return total_reward, rewards
A.6 HUMAN-IN-THE-LOOPPREFERENCE
A.6.1 ISAACGYMTASKS
We evaluate human-in-the-loop preference experiments on tasks in IsaacGym, including Quad-
copter, Humanoid, Ant, ShadowHand, and AllegroHand. In these experiments, volunteers only
provided feedback by comparing videos showcasing the final policies derived from each reward
function.
IntheQuadcoptertask,humansevaluateperformancebyobservingwhetherthequadcoptermoves
quickly and efficiently, and whether it stabilizes in the final position. For the Humanoid and Ant
tasks, where the task description is "make the ant/humanoid run as fast as possible," humans esti-
mate speed by comparing the time taken to cover the same distance and assessing the movement
posture. However, due to the variability in movement postures and directions, speed is often esti-
matedinaccurately.IntheShadowHandandAllegroHandtasks,wherethegoalis“tomakethehand
spintheobjecttoatargetorientation,”Thetargetorientationisdisplayednearbytherobothandso
that human can estimate difference between the current orientation and the target orientation. Be-
sides,sincethetargetorientationregeneratesuponbeingreached,thefrequencyoftargetorientation
changescanalsohelptoevaluateperformance.
Duetothelackofpreciseenvironmentaldata,volunteerscannotmakeabsolutelyaccuratejudgments
duringtheexperiments. Forinstance,intheHumanoidtask,robotsmaymoveinvaryingdirections,
whichcanintroducebiasesinvolunteers’assessmentsofspeed. However,volunteersarestillable
to filter out extremely poor results and select videos with relatively better performance. In most
cases,theselectedresultscloselyalignwiththosederivedfromproxyhumanpreferences,enabling
effectiveimprovementsintaskperformance.
Below is a specific case from the Humanoid task that illustrates the potential errors humans
may make during evaluation and the learning process of the reward function under this as-
sumption. The reward task scores (RTS) chosen by the volunteer across five iterations are
4.521,6.069,6.814,6.363,6.983.
In the first iteration, the ground-truth task scores of each policy were
0.593,2.744,4.520,0.192,2.517,5.937, note that the volunteer was unaware of these scores.
Initially,Initially,thevolunteerreviewedallthevideosandselectedtheonewiththeworstbehavior.
Thehumanoidinvideo0andvideo3exhibitedsimilarspinningbehaviorandthevolunteerchose
video3astheworstvideo. Subsequently,thevolunteerevaluatedtheremainingvideosbasedonthe
humanoids’runningspeed. Thehumanoidsinvideo1andvideo4appearedtorunslightlyslower,
while those in video 2 and video 5 ran faster. Ultimately, the volunteer chose video 2 as the most
preferred,demonstratingthathumandecision-makingmayleadtosuboptimalchoices.
Thus,therewardfunctionselectediniteration1consistsofseveralkeycomponents:velocityreward,
uprightreward,forcepenalty,unnaturalposepenalty,andactionpenalty.Thesecomponentsnotonly
23PreprintPaper
promote faster training, which is the primary objective, but also encourage the maintenance of an
uprightpose. Additionally,thefunctionpenalizesexcessiveforceusage,extremejointangles,and
largeactionvaluestofostersmootherandmorecontrolledmovements.
Insubsequentiterations, thevolunteercorrectlyidentifiedthebestandworstvideos. Adjustments
were made to the weights of each component, and specific temperature values were introduced
for each. These modifications resulted in a more balanced reward structure, ensuring that critical
aspectsexertastrongerinfluence, therebyallowingforgreatercontrolover thelearningdynamics
andimprovingtheagent’sperformanceinachievingthetask. EveninIteration4,thevolunteerdid
notselecttherewardfunctionwiththehighestRTS(6.813)butinsteadoptedforthesecond-highest
reward function (RTS = 6.363). Nevertheless, the generated reward function exhibited consistent
improvementduringtheseiterations.
Hereweshowthefullrewardfunctionduringtheprocess.
HumanoidTask: RewardFunctionchosenbyvolunteerwithRTS(4.521)ofIteration1
def compute_reward(
velocity: torch.Tensor,
dof_pos: torch.Tensor,
dof_force_tensor: torch.Tensor,
actions: torch.Tensor,
up_vec: torch.Tensor,
heading_vec: torch.Tensor
) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
# Reward for forward velocity (run as fast as possible)
velocity_reward = velocity[:, 0]
# Encouragement for upright pose (penalize for deviation from
vertical)
upright_reward = up_vec[:, 2]
# Penalize high force usage (energy efficiency)
force_penalty = torch.sum(torch.abs(dof_force_tensor), dim=1)
# Penalize unnatural joint positions (for instance, avoid
extreme angles)
unnatural_pose_penalty = torch.sum(torch.abs(dof_pos), dim=1)
# Penalize for large actions (to promote smoother movement)
action_penalty = torch.sum(torch.abs(actions), dim=1)
# Normalize the rewards and penalties
velocity_reward = torch.exp(velocity_reward) - 1
upright_reward = torch.exp(upright_reward) - 1
temperature = 1.0
force_penalty = torch.exp(-force_penalty / temperature)
unnatural_pose_penalty = torch.exp(-unnatural_pose_penalty /
temperature)
action_penalty = torch.exp(-action_penalty / temperature)
# Combine the rewards and penalties into a single reward
total_reward = (
velocity_reward +
0.5 * upright_reward -
0.01 * force_penalty -
0.01 * unnatural_pose_penalty -
0.01 * action_penalty
)
# Return the total reward and each component for analysis
reward_components = {
"velocity_reward": velocity_reward,
24PreprintPaper
"upright_reward": upright_reward,
"force_penalty": force_penalty,
"unnatural_pose_penalty": unnatural_pose_penalty,
"action_penalty": action_penalty
}
return total_reward, reward_components
HumanoidTask: RewardFunctionchosenbyvolunteerwithRTS(6.069)ofIteration2
def compute_reward(
velocity: torch.Tensor,
dof_pos: torch.Tensor,
dof_force_tensor: torch.Tensor,
actions: torch.Tensor,
up_vec: torch.Tensor,
heading_vec: torch.Tensor
) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
# Reward for forward velocity (run as fast as possible)
velocity_reward = velocity[:, 0]
velocity_temperature = 1.2 # increased slightly
velocity_reward = torch.exp(velocity_reward /
velocity_temperature) - 1
# Encouragement for upright pose (penalize for deviation from
vertical)
upright_reward = up_vec[:, 2]
upright_temperature = 0.5 # introduce a specific temperature
upright_reward = torch.exp(upright_reward / upright_temperature
) - 1
# Penalize high force usage (energy efficiency)
force_penalty = torch.sum(torch.abs(dof_force_tensor), dim=1)
force_temperature = 0.1 # decreased to make it more
significant
force_penalty = torch.exp(-force_penalty / force_temperature)
# Penalize unnatural joint positions (for instance, avoid
extreme angles)
unnatural_pose_penalty = torch.sum(torch.abs(dof_pos), dim=1)
pose_temperature = 0.1 # decreased to make it more significant
unnatural_pose_penalty = torch.exp(-unnatural_pose_penalty /
pose_temperature)
# Penalize for large actions (to promote smoother movement)
action_penalty = torch.sum(torch.abs(actions), dim=1)
action_temperature = 0.1 # decreased to make it more
significant
action_penalty = torch.exp(-action_penalty / action_temperature
)
# Combine the rewards and penalties into a single reward
total_reward = (
velocity_reward +
0.5 * upright_reward -
0.01 * force_penalty -
0.01 * unnatural_pose_penalty -
0.01 * action_penalty
)
25PreprintPaper
# Return the total reward and each component for analysis
reward_components = {
"velocity_reward": velocity_reward,
"upright_reward": upright_reward,
"force_penalty": force_penalty,
"unnatural_pose_penalty": unnatural_pose_penalty,
"action_penalty": action_penalty
}
return total_reward, reward_components
HumanoidTask: RewardFunctionchosenbyvolunteerwithRTS(6.814)ofIteration3
def compute_reward(
velocity: torch.Tensor,
dof_pos: torch.Tensor,
dof_force_tensor: torch.Tensor,
actions: torch.Tensor,
up_vec: torch.Tensor,
heading_vec: torch.Tensor
) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
# Reward for forward velocity (run as fast as possible)
velocity_reward = velocity[:, 0]
velocity_temperature = 1.1 # minor adjustment
velocity_reward = torch.exp(velocity_reward /
velocity_temperature) - 1
# Encouragement for upright pose (penalize for deviation from
vertical)
upright_reward = up_vec[:, 2]
upright_temperature = 0.6 # slight adjustment
upright_reward = torch.exp(upright_reward / upright_temperature
) - 1
# Penalize high force usage (energy efficiency)
force_penalty = torch.sum(torch.abs(dof_force_tensor), dim=1)
force_temperature = 0.15 # increased to try to make it
effective
force_penalty = torch.exp(-force_penalty / force_temperature)
# Penalize unnatural joint positions (for instance, avoid
extreme angles)
unnatural_pose_penalty = torch.sum(torch.abs(dof_pos), dim=1)
pose_temperature = 0.2 # increased to try to make it effective
unnatural_pose_penalty = torch.exp(-unnatural_pose_penalty /
pose_temperature)
# Penalize for large actions (to promote smoother movement)
action_penalty = torch.sum(torch.abs(actions), dim=1)
action_temperature = 0.2 # increased to try to make it
effective
action_penalty = torch.exp(-action_penalty / action_temperature
)
# Combine the rewards and penalties into a single reward
total_reward = (
velocity_reward +
0.5 * upright_reward -
0.02 * force_penalty - # increased slightly for more
impact
26PreprintPaper
0.02 * unnatural_pose_penalty - # increased slightly for
more impact
0.02 * action_penalty # increased slightly for more impact
)
# Return the total reward and each component for analysis
reward_components = {
"velocity_reward": velocity_reward,
"upright_reward": upright_reward,
"force_penalty": force_penalty,
"unnatural_pose_penalty": unnatural_pose_penalty,
"action_penalty": action_penalty
}
return total_reward, reward_components
HumanoidTask: RewardFunctionchosenbyvolunteerwithRTS(6.363)ofIteration4
def compute_reward(
velocity: torch.Tensor,
dof_pos: torch.Tensor,
dof_force_tensor: torch.Tensor,
actions: torch.Tensor,
up_vec: torch.Tensor,
heading_vec: torch.Tensor
) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
# Reward for forward velocity (run as fast as possible)
velocity_reward = velocity[:, 0]
velocity_temperature = 1.05 # slight adjustment to refine the
impact
velocity_reward = torch.exp(velocity_reward /
velocity_temperature) - 1
# Encouragement for upright pose (penalize for deviation from
vertical)
upright_reward = up_vec[:, 2]
upright_temperature = 0.65 # slight loosening for more upright
reward
upright_reward = torch.exp(upright_reward / upright_temperature
) - 1
# Penalize high force usage (energy efficiency)
force_penalty = torch.sum(torch.abs(dof_force_tensor), dim=1)
force_temperature = 0.2 # increased to make it more
significant
force_penalty = torch.exp(-force_penalty / force_temperature)
# Penalize unnatural joint positions (for instance, avoid
extreme angles)
unnatural_pose_penalty = torch.sum(torch.abs(dof_pos), dim=1)
pose_temperature = 0.25 # slight increase to make this
component effective
unnatural_pose_penalty = torch.exp(-unnatural_pose_penalty /
pose_temperature)
# Penalize for large actions (to promote smoother movement)
action_penalty = torch.sum(torch.abs(actions), dim=1)
action_temperature = 0.25 # slightly adjusted for more
prominent constraint
27PreprintPaper
action_penalty = torch.exp(-action_penalty / action_temperature
)
# Combine the rewards and penalties into a single reward
total_reward = (
velocity_reward +
0.5 * upright_reward -
0.015 * force_penalty - # slight increase for more impact
0.015 * unnatural_pose_penalty - # slight increase for
more impact
0.015 * action_penalty # slight increase for more impact
)
# Return the total reward and each component for analysis
reward_components = {
"velocity_reward": velocity_reward,
"upright_reward": upright_reward,
"force_penalty": force_penalty,
"unnatural_pose_penalty": unnatural_pose_penalty,
"action_penalty": action_penalty
}
return total_reward, reward_components
HumanoidTask: RewardFunctionwithbestRTS(6.813)ofIteration4(notchosenbyvol-
unteer)
def compute_reward(
velocity: torch.Tensor,
dof_pos: torch.Tensor,
dof_force_tensor: torch.Tensor,
actions: torch.Tensor,
up_vec: torch.Tensor,
heading_vec: torch.Tensor
) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
# Reward for forward velocity (run as fast as possible)
velocity_reward = velocity[:, 0]
velocity_temperature = 1.15
velocity_reward = torch.exp(velocity_reward /
velocity_temperature) - 1
# Encouragement for upright pose (penalize for deviation from
vertical)
upright_reward = up_vec[:, 2]
upright_temperature = 0.55
upright_reward = torch.exp(upright_reward / upright_temperature
) - 1
# Penalize high force usage (energy efficiency)
force_penalty = torch.sum(torch.abs(dof_force_tensor), dim=1)
force_temperature = 0.12
force_penalty = torch.exp(-force_penalty / force_temperature)
# Penalize unnatural joint positions (for instance, avoid
extreme angles)
unnatural_pose_penalty = torch.sum(torch.abs(dof_pos), dim=1)
pose_temperature = 0.18
unnatural_pose_penalty = torch.exp(-unnatural_pose_penalty /
pose_temperature)
28PreprintPaper
# Penalize for large actions (to promote smoother movement)
action_penalty = torch.sum(torch.abs(actions), dim=1)
action_temperature = 0.18
action_penalty = torch.exp(-action_penalty / action_temperature
)
# Combine the rewards and penalties into a single reward
total_reward = (
velocity_reward +
0.5 * upright_reward -
0.02 * force_penalty -
0.02 * unnatural_pose_penalty -
0.02 * action_penalty
)
# Return the total reward and each component for analysis
reward_components = {
"velocity_reward": velocity_reward,
"upright_reward": upright_reward,
"force_penalty": force_penalty,
"unnatural_pose_penalty": unnatural_pose_penalty,
"action_penalty": action_penalty
}
return total_reward, reward_components
HumanoidTask: RewardFunctionchosenbyvolunteerwithRTS(6.983)ofIteration5
def compute_reward(
velocity: torch.Tensor,
dof_pos: torch.Tensor,
dof_force_tensor: torch.Tensor,
actions: torch.Tensor,
up_vec: torch.Tensor,
heading_vec: torch.Tensor
) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
# Adjusted parameters based on analysis
velocity_temperature = 1.1
upright_temperature = 0.65
force_temperature = 0.25
pose_temperature = 0.3
action_temperature = 0.3
# Reward for forward velocity (run as fast as possible)
velocity_reward = velocity[:, 0]
velocity_reward = torch.exp(velocity_reward /
velocity_temperature) - 1
# Encouragement for upright pose (penalize for deviation from
vertical)
upright_reward = up_vec[:, 2]
upright_reward = torch.exp(upright_reward / upright_temperature
) - 1
# Penalize high force usage (energy efficiency)
force_penalty = torch.sum(torch.abs(dof_force_tensor), dim=1)
force_penalty = torch.exp(-force_penalty / force_temperature)
# Penalize unnatural joint positions (for instance, avoid
extreme angles)
29PreprintPaper
unnatural_pose_penalty = torch.sum(torch.abs(dof_pos), dim=1)
unnatural_pose_penalty = torch.exp(-unnatural_pose_penalty /
pose_temperature)
# Penalize for large actions (to promote smoother movement)
action_penalty = torch.sum(torch.abs(actions), dim=1)
action_penalty = torch.exp(-action_penalty / action_temperature
)
# Combine the rewards and penalties into a single reward
total_reward = (
velocity_reward +
0.5 * upright_reward -
0.02 * force_penalty -
0.02 * unnatural_pose_penalty -
0.02 * action_penalty
)
# Return the total reward and each component for analysis
reward_components = {
"velocity_reward": velocity_reward,
"upright_reward": upright_reward,
"force_penalty": force_penalty,
"unnatural_pose_penalty": unnatural_pose_penalty,
"action_penalty": action_penalty
}
return total_reward, reward_components
A.6.2 HUMANOIDJUMPTASK
Inourstudy,weintroducedanoveltask: HumanoidJump,withthetaskdescriptionbeing“tomake
humanoid jump like a real human.” The prompt of environment context in this task is shown in
Prompt5.
Prompt5: PromptsofEnvironmentContextinHumanoidJumpTask
class HumanoidJump(VecTask):
"""Rest of the environment definition omitted."""
def compute_observations(self):
self.gym.refresh_dof_state_tensor(self.sim)
self.gym.refresh_actor_root_state_tensor(self.sim)
self.gym.refresh_force_sensor_tensor(self.sim)
self.gym.refresh_dof_force_tensor(self.sim)
self.obs_buf[:], self.torso_position[:],
self.prev_torso_position[:], self.velocity_world[:],
self.angular_velocity_world[:], self.velocity_local[:],
self.angular_velocity_local[:], self.up_vec[:],
self.heading_vec[:], self.right_leg_contact_force[:],
self.left_leg_contact_force[:] = \
compute_humanoid_jump_observations(
self.obs_buf, self.root_states, self.torso_position,
self.inv_start_rot, self.dof_pos, self.dof_vel,
self.dof_force_tensor, self.dof_limits_lower,
self.dof_limits_upper, self.dof_vel_scale,
self.vec_sensor_tensor, self.actions,
self.dt, self.contact_force_scale,
self.angular_velocity_scale,
self.basis_vec0, self.basis_vec1)
def compute_humanoid_jump_observations(obs_buf, root_states, torso_position, inv_start_rot
, dof_pos, dof_vel, dof_force, dof_limits_lower, dof_limits_upper, dof_vel_scale,
sensor_force_torques, actions, dt, contact_force_scale, angular_velocity_scale,
basis_vec0, basis_vec1):
# type: (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, float
, Tensor, Tensor, float, float, float, Tensor, Tensor) -> Tuple[Tensor, Tensor, Tensor,
Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]
30PreprintPaper
prev_torso_position_new = torso_position.clone()
torso_position = root_states[:, 0:3]
torso_rotation = root_states[:, 3:7]
velocity_world = root_states[:, 7:10]
angular_velocity_world = root_states[:, 10:13]
torso_quat, up_proj, up_vec, heading_vec = compute_heading_and_up_vec(
torso_rotation, inv_start_rot, basis_vec0, basis_vec1, 2)
velocity_local, angular_velocity_local, roll, pitch, yaw = compute_rot_new(
torso_quat, velocity_world, angular_velocity_world)
roll = normalize_angle(roll).unsqueeze(-1)
yaw = normalize_angle(yaw).unsqueeze(-1)
dof_pos_scaled = unscale(dof_pos, dof_limits_lower, dof_limits_upper)
scale_angular_velocity_local = angular_velocity_local * angular_velocity_scale
obs = torch.cat((root_states[:, 0:3].view(-1, 3), velocity_local,
scale_angular_velocity_local,
yaw, roll, up_proj.unsqueeze(-1),
dof_pos_scaled, dof_vel * dof_vel_scale,
dof_force * contact_force_scale,
sensor_force_torques.view(-1, 12) * contact_force_scale,
actions), dim=-1)
right_leg_contact_force = sensor_force_torques[:, 0:3]
left_leg_contact_force = sensor_force_torques[:, 6:9]
abdomen_y_pos = dof_pos[:, 0]
abdomen_z_pos = dof_pos[:, 1]
abdomen_x_pos = dof_pos[:, 2]
right_hip_x_pos = dof_pos[:, 3]
right_hip_z_pos = dof_pos[:, 4]
right_hip_y_pos = dof_pos[:, 5]
right_knee_pos = dof_pos[:, 6]
right_ankle_x_pos = dof_pos[:, 7]
right_ankle_y_pos = dof_pos[:, 8]
left_hip_x_pos = dof_pos[:, 9]
left_hip_z_pos = dof_pos[:, 10]
left_hip_y_pos = dof_pos[:, 11]
left_knee_pos = dof_pos[:, 12]
left_ankle_x_pos = dof_pos[:, 13]
left_ankle_y_pos = dof_pos[:, 14]
right_shoulder1_pos = dof_pos[:, 15]
right_shoulder2_pos = dof_pos[:, 16]
right_elbow_pos = dof_pos[:, 17]
left_shoulder1_pos = dof_pos[:, 18]
left_shoulder2_pos = dof_pos[:, 19]
left_elbow_pos = dof_pos[:, 20]
right_shoulder1_action = actions[:, 15]
right_shoulder2_action = actions[:, 16]
right_elbow_action = actions[:, 17]
left_shoulder1_action = actions[:, 18]
left_shoulder2_action = actions[:, 19]
left_elbow_action = actions[:, 20]
return obs, torso_position, prev_torso_position_new, velocity_world,
angular_velocity_world, velocity_local, scale_angular_velocity_local,
up_vec, heading_vec, right_leg_contact_force, left_leg_contact_force
Rewardfunctions. Weshowtherewardfunctionsinatrialthatsuccessfullyevolvedahuman-like
jump: bending both legs to jump. Initially, the reward function focused on encouraging vertical
movement while penalizing horizontal displacement, high contact force usage, and improper joint
movements. Over time, the scaling factors for the rewards and penalties were gradually adjusted
bychangingthetemperatureparametersintheexponentialscaling. Theseadjustmentsaimedtoen-
hancethemodel’ssensitivitytodifferentmovementbehaviors. Forexample,theverticalmovement
reward’stemperaturewasreduced,leadingtomorepreciserewardsforpositiveverticalmovements.
Similarly, the horizontal displacement penalty was fine-tuned by modifying its temperature across
iterations, either decreasing or increasing the penalty’s impact on lateral movements. The contact
forcepenaltyevolvedbydecreasingitstemperaturetopenalizeexcessiveforceusagemorestrongly,
especially in the later iterations, making the task more sensitive to leg contact forces. Finally, the
jointusagerewardwasrefinedbyadjustingthetemperaturetoeitherencourageordiscouragecer-
tainjointbehaviors,withmorefocusonlegextensionandcontractionpatterns. Overall,thechanges
31PreprintPaper
primarily revolved around adjusting the sensitivity of different components, refining the balance
between rewards and penalties to better align the humanoid’s behavior with the desired jumping
performance.
HumanoidJumpTask: RewardFunctionofIteration1
def compute_reward(torso_position: torch.Tensor,
prev_torso_position: torch.Tensor, velocity_world: torch.Tensor,
right_leg_contact_force: torch.Tensor,
left_leg_contact_force: torch.Tensor, dof_pos: torch.Tensor) ->
Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
# Ensure all tensors are on the same device
device = torso_position.device
# Compute vertical torso movement reward
vertical_movement = torso_position[:, 2] - prev_torso_position
[:, 2]
vertical_movement_reward = torch.clamp(vertical_movement, min
=0.0) # Reward positive vertical movement
vertical_movement_reward = torch.exp(vertical_movement_reward /
0.1) # Use exponential scaling with temperature
# Compute horizontal displacement penalty
horizontal_displacement = torch.sum(torch.abs(torso_position[:,
:2] - prev_torso_position[:, :2]), dim=-1)
horizontal_displacement_penalty = torch.exp(-
horizontal_displacement / 0.1) # Penalize large movements with
temperature
# Compute leg forces usage reward
contact_force_usage = torch.sum(torch.abs(
right_leg_contact_force) + torch.abs(left_leg_contact_force),
dim=-1)
contact_force_usage_penalty = torch.exp(-contact_force_usage /
10.0) # Penalize high contact force usage with temperature
# Compute joint usage reward (encourages proper leg extension
and contraction)
leg_joints_indices = torch.tensor([6, 7, 8, 12, 13, 14], device
=device) # Indices of leg joints
leg_joint_usage = torch.mean(dof_pos[:, leg_joints_indices],
dim=-1)
leg_joint_usage_reward = torch.exp(-torch.abs(leg_joint_usage)
/ 0.1) # Encourage movements from neutral position
# Sum all rewards and penalties
total_reward = vertical_movement_reward +
horizontal_displacement_penalty + contact_force_usage_penalty +
leg_joint_usage_reward
# Create a dictionary for individual reward components
reward_components = {
’vertical_movement_reward’: vertical_movement_reward,
’horizontal_displacement_penalty’:
horizontal_displacement_penalty,
’contact_force_usage_penalty’: contact_force_usage_penalty,
’leg_joint_usage_reward’: leg_joint_usage_reward
}
return total_reward, reward_components
32PreprintPaper
HumanoidJumpTask: RewardFunctionofIteration2
def compute_reward(
torso_position: torch.Tensor,
prev_torso_position: torch.Tensor,
velocity_world: torch.Tensor,
right_leg_contact_force: torch.Tensor,
left_leg_contact_force: torch.Tensor,
dof_pos: torch.Tensor
) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
# Ensure all tensors are on the same device
device = torso_position.device
# Compute vertical torso movement reward
vertical_movement = torso_position[:, 2] - prev_torso_position
[:, 2]
vertical_movement_reward = torch.clamp(vertical_movement, min
=0.0) # Reward positive vertical movement
temperature_vertical = 0.05
vertical_movement_reward = torch.exp(vertical_movement_reward /
temperature_vertical) # Use exponential scaling
# Compute horizontal displacement penalty
horizontal_displacement = torch.sum(torch.abs(torso_position[:,
:2] - prev_torso_position[:, :2]), dim=-1)
temperature_horizontal = 0.1
horizontal_displacement_penalty = torch.exp(-
horizontal_displacement / temperature_horizontal) # Penalize
large movements with temperature
# Compute leg forces usage reward
contact_force_usage = torch.sum(torch.abs(
right_leg_contact_force) + torch.abs(left_leg_contact_force),
dim=-1)
temperature_force = 1.0
contact_force_usage_penalty = torch.exp(-contact_force_usage /
temperature_force) # Penalize high contact force usage with
temperature
# Compute joint usage reward (encourages proper leg extension
and contraction)
leg_joints_indices = torch.tensor([6, 7, 8, 12, 13, 14], device
=device) # Indices of leg joints
leg_joint_usage = torch.mean(dof_pos[:, leg_joints_indices],
dim=-1)
temperature_joints = 0.05
leg_joint_usage_reward = torch.exp(-torch.abs(leg_joint_usage)
/ temperature_joints) # Encourage movements from the neutral
position
# Sum all rewards and penalties
total_reward = vertical_movement_reward +
horizontal_displacement_penalty + contact_force_usage_penalty +
leg_joint_usage_reward
# Create a dictionary for individual reward components
reward_components = {
’vertical_movement_reward’: vertical_movement_reward,
’horizontal_displacement_penalty’:
horizontal_displacement_penalty,
’contact_force_usage_penalty’: contact_force_usage_penalty,
’leg_joint_usage_reward’: leg_joint_usage_reward
}
33PreprintPaper
return total_reward, reward_components
HumanoidJumpTask: RewardFunctionofIteration3
def compute_reward(
torso_position: torch.Tensor,
prev_torso_position: torch.Tensor,
velocity_world: torch.Tensor,
right_leg_contact_force: torch.Tensor,
left_leg_contact_force: torch.Tensor,
dof_pos: torch.Tensor
) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
# Ensure all tensors are on the same device
device = torso_position.device
# Compute vertical torso movement reward
vertical_movement = torso_position[:, 2] - prev_torso_position
[:, 2]
vertical_movement_reward = torch.clamp(vertical_movement, min
=0.0) # Reward positive vertical movement
temperature_vertical = 0.05
vertical_movement_reward = torch.exp(vertical_movement_reward /
temperature_vertical) # Use exponential scaling
# Compute horizontal displacement penalty
horizontal_displacement = torch.sum(torch.abs(torso_position[:,
:2] - prev_torso_position[:, :2]), dim=-1)
temperature_horizontal = 0.05 # Adjusted temperature for finer
tuning
horizontal_displacement_penalty = torch.exp(-
horizontal_displacement / temperature_horizontal) # Penalize
large movements
# Compute leg forces usage reward
contact_force_usage = torch.sum(torch.abs(
right_leg_contact_force) + torch.abs(left_leg_contact_force),
dim=-1)
temperature_force = 5.0 # Adjusted to make contact force usage
more noticeable
contact_force_usage_penalty = torch.exp(-contact_force_usage /
temperature_force) # Penalize high contact force usage
# Compute joint usage reward (encourages proper leg extension
and contraction)
leg_joints_indices = torch.tensor([6, 7, 8, 12, 13, 14], device
=device) # Indices of leg joints
leg_joint_usage = torch.mean(dof_pos[:, leg_joints_indices],
dim=-1)
temperature_joints = 0.05
leg_joint_usage_reward = torch.exp(-torch.abs(leg_joint_usage)
/ temperature_joints) # Encourage movements from the neutral
position
# Sum all rewards and penalties
total_reward = vertical_movement_reward +
horizontal_displacement_penalty + contact_force_usage_penalty +
leg_joint_usage_reward
# Create a dictionary for individual reward components
34PreprintPaper
reward_components = {
’vertical_movement_reward’: vertical_movement_reward,
’horizontal_displacement_penalty’:
horizontal_displacement_penalty,
’contact_force_usage_penalty’: contact_force_usage_penalty,
’leg_joint_usage_reward’: leg_joint_usage_reward
}
return total_reward, reward_components
HumanoidJumpTask: RewardFunctionofIteration4
def compute_reward(
torso_position: torch.Tensor,
prev_torso_position: torch.Tensor,
velocity_world: torch.Tensor,
right_leg_contact_force: torch.Tensor,
left_leg_contact_force: torch.Tensor,
dof_pos: torch.Tensor
) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
# Ensure all tensors are on the same device
device = torso_position.device
# Compute vertical torso movement reward
vertical_movement = torso_position[:, 2] - prev_torso_position
[:, 2]
vertical_movement_reward = torch.clamp(vertical_movement, min
=0.0) # Reward positive vertical movement
temperature_vertical = 0.04 # Adjusted temperature for better
sensitivity
vertical_movement_reward = torch.exp(vertical_movement_reward /
temperature_vertical) # Use exponential scaling
# Compute horizontal displacement penalty
horizontal_displacement = torch.sum(torch.abs(torso_position[:,
:2] - prev_torso_position[:, :2]), dim=-1)
temperature_horizontal = 0.1 # Increased temperature to
penalize horizontal movement more
horizontal_displacement_penalty = torch.exp(-
horizontal_displacement / temperature_horizontal) # Penalize
large movements
# Compute leg forces usage reward
contact_force_usage = torch.sum(torch.abs(
right_leg_contact_force) + torch.abs(left_leg_contact_force),
dim=-1)
temperature_force = 0.1 # Significantly increase sensitivity
to contact forces
contact_force_usage_penalty = torch.exp(-contact_force_usage /
temperature_force) # Penalize high contact force usage
# Compute joint usage reward (encourages proper leg extension
and contraction)
leg_joints_indices = torch.tensor([6, 7, 8, 12, 13, 14], device
=device) # Indices of leg joints
leg_joint_usage = torch.mean(dof_pos[:, leg_joints_indices],
dim=-1)
temperature_joints = 0.02 # Adjusted for joint usage
sensitivity
35PreprintPaper
leg_joint_usage_reward = torch.exp(-torch.abs(leg_joint_usage)
/ temperature_joints) # Encourage movements from the neutral
position
# Sum all rewards and penalties
total_reward = vertical_movement_reward +
horizontal_displacement_penalty + contact_force_usage_penalty +
leg_joint_usage_reward
# Create a dictionary for individual reward components
reward_components = {
’vertical_movement_reward’: vertical_movement_reward,
’horizontal_displacement_penalty’:
horizontal_displacement_penalty,
’contact_force_usage_penalty’: contact_force_usage_penalty,
’leg_joint_usage_reward’: leg_joint_usage_reward
}
return total_reward, reward_components
HumanoidTask: RewardFunctionofIteration5
def compute_reward(
torso_position: torch.Tensor,
prev_torso_position: torch.Tensor,
velocity_world: torch.Tensor,
right_leg_contact_force: torch.Tensor,
left_leg_contact_force: torch.Tensor,
dof_pos: torch.Tensor
) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
# Ensure all tensors are on the same device
device = torso_position.device
# Compute vertical torso movement reward
vertical_movement = torso_position[:, 2] - prev_torso_position
[:, 2]
vertical_movement_reward = torch.clamp(vertical_movement, min
=0.0) # Reward positive vertical movement
temperature_vertical = 0.04 # Adjusted temperature for better
sensitivity
vertical_movement_reward = torch.exp(vertical_movement_reward /
temperature_vertical) # Use exponential scaling
# Compute horizontal displacement penalty
horizontal_displacement = torch.sum(torch.abs(torso_position[:,
:2] - prev_torso_position[:, :2]), dim=-1)
temperature_horizontal = 0.05 # Decreased temperature for more
sensitivity
horizontal_displacement_penalty = torch.exp(-
horizontal_displacement / temperature_horizontal) # Penalize
large movements
# Compute leg forces usage penalty (Rewritten to reduce contact
force)
contact_force_usage = torch.sum(torch.abs(
right_leg_contact_force) + torch.abs(left_leg_contact_force),
dim=-1)
temperature_force = 0.5 # Adjusted to penalize contact force
usage
36PreprintPaper
contact_force_usage_penalty = torch.exp(-contact_force_usage /
temperature_force) # Penalize high contact force usage
# Compute joint usage reward (encourages proper leg extension
and contraction)
leg_joints_indices = torch.tensor([6, 7, 8, 12, 13, 14], device
=device) # Indices of leg joints
leg_joint_usage = torch.mean(torch.abs(dof_pos[:,
leg_joints_indices]), dim=-1)
temperature_joints = 0.02 # Adjusted for joint usage
sensitivity
leg_joint_usage_reward = torch.exp(-leg_joint_usage /
temperature_joints) # Encourage movements from the neutral
position
# Sum all rewards and penalties
total_reward = vertical_movement_reward +
horizontal_displacement_penalty + contact_force_usage_penalty +
leg_joint_usage_reward
# Create a dictionary for individual reward components
reward_components = {
’vertical_movement_reward’: vertical_movement_reward,
’horizontal_displacement_penalty’:
horizontal_displacement_penalty,
’contact_force_usage_penalty’: contact_force_usage_penalty,
’leg_joint_usage_reward’: leg_joint_usage_reward
}
return total_reward, reward_components
HumanoidTask: RewardFunctionofIteration6
def compute_reward(
torso_position: torch.Tensor,
prev_torso_position: torch.Tensor,
velocity_world: torch.Tensor,
right_leg_contact_force: torch.Tensor,
left_leg_contact_force: torch.Tensor,
dof_pos: torch.Tensor
) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
# Ensure all tensors are on the same device
device = torso_position.device
# Compute vertical torso movement reward
vertical_movement = torso_position[:, 2] - prev_torso_position
[:, 2]
vertical_movement_reward = torch.clamp(vertical_movement, min
=0.0) # Reward positive vertical movement
temperature_vertical = 0.03 # Fine-tuned temperature for
better sensitivity
vertical_movement_reward = torch.exp(vertical_movement_reward /
temperature_vertical) # Use exponential scaling
# Compute horizontal displacement penalty
horizontal_displacement = torch.sum(torch.abs(torso_position[:,
:2] - prev_torso_position[:, :2]), dim=-1)
temperature_horizontal = 0.04 # Decreased temperature for more
sensitivity
37PreprintPaper
horizontal_displacement_penalty = torch.exp(-
horizontal_displacement / temperature_horizontal) # Penalize
large movements
# Compute leg forces usage penalty (encourage minimal contact
force)
contact_force_usage = torch.sum(torch.abs(
right_leg_contact_force) + torch.abs(left_leg_contact_force),
dim=-1)
temperature_force = 0.5 # Adjusted to penalize contact force
usage
contact_force_usage_penalty = torch.exp(-contact_force_usage /
temperature_force) # Penalize high contact force usage
# Compute joint usage reward (encourages proper leg extension
and contraction)
leg_joints_indices = torch.tensor([6, 7, 8, 12, 13, 14], device
=device) # Indices of leg joints
leg_joint_usage = torch.mean(torch.abs(dof_pos[:,
leg_joints_indices]), dim=-1)
temperature_joints = 0.02 # Fine-tuned for joint usage
sensitivity
leg_joint_usage_reward = torch.exp(-torch.abs(leg_joint_usage)
/ temperature_joints) # Encourage movements from the neutral
position
# Sum all rewards and penalties
total_reward = vertical_movement_reward +
horizontal_displacement_penalty + contact_force_usage_penalty +
leg_joint_usage_reward
# Create a dictionary for individual reward components
reward_components = {
’vertical_movement_reward’: vertical_movement_reward,
’horizontal_displacement_penalty’:
horizontal_displacement_penalty,
’contact_force_usage_penalty’: contact_force_usage_penalty,
’leg_joint_usage_reward’: leg_joint_usage_reward
}
return total_reward, reward_components
38