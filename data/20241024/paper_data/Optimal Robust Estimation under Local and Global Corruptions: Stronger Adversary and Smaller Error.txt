Optimal Robust Estimation under Local and Global Corruptions:
Stronger Adversary and Smaller Error
Thanasis Pittas Ankit Pensia
University of Wisconsin-Madison Simons Institute for the Theory of Computing
pittas@wisc.edu ankitp@berkeley.edu
October 23, 2024
Abstract
Algorithmic robust statistics has traditionally focused on the contamination model where a
smallfractionofthesamplesarearbitrarilycorrupted. Weconsiderarecentcontaminationmodel
that combines two kinds of corruptions: (i) small fraction of arbitrary outliers, as in classical
robust statistics, and (ii) local perturbations, where samples may undergo bounded shifts on
average. While each noise model is well understood individually, the combined contamination
model poses new algorithmic challenges, with only partial results known. Existing efficient
algorithms are limited in two ways: (i) they work only for a weak notion of local perturbations,
and (ii) they obtain suboptimal error for isotropic subgaussian distributions (among others).
The latter limitation led [NGS24] to hypothesize that improving the error might, in fact, be
computationallyhard. Perhapssurprisingly, weshowthatinformationtheoreticallyoptimalerror
canindeedbeachievedinpolynomialtime, underanevenstronger localperturbationmodel(the
sliced-Wasserstein metric as opposed to the Wasserstein metric). Notably, our analysis reveals
that the entire family of stability-based robust mean estimators continues to work optimally in a
black-box manner for the combined contamination model. This generalization is particularly
useful in real-world scenarios where the specific form of data corruption is not known in advance.
We also present efficient algorithms for distribution learning and principal component analysis in
the combined contamination model.
1 Introduction
We study the problems of high-dimensional parameter estimation and distribution learning in the
setting where the data available to the statistician may be corrupted. We start with the former
problem, where a prototypical task is that of (multivariate) mean estimation, defined as follows:
Let P be a family of probability distributions over Rd. Given a set of samples S generated from an
(unknown) distribution P ∈ P, compute (in a computationally-efficient manner) an estimate µ that
(cid:98)
is close to the true mean µ := E [X] in the Euclidean norm (with high probability).
P X∼P
The precise stochastic process generating the data S plays a crucial role in this problem. Much
of the historical focus has been on the ideal setting, where S is a set of i.i.d. samples from P ∈ P.
However, this i.i.d. assumption is often violated in real-world scenarios, a phenomenon known as
model misspecification. Two prominent examples of such misspecification include (i) outliers, which
Authors are listed in random order.
1
4202
tcO
22
]SD.sc[
1v03271.0142:viXramight occur due to data poisoning attacks [BNJT10; BNL12; SKL17; TLM18; HKSO21], errors in
the front-end systems in geometric perception [YC23], biological settings [RPWCKZF02; PLJD10;
LATSCR+08], and (ii) local distribution shifts [CD23; YZLL24] due to varying biases of different
sensors from which data is collected. Importantly, in high dimensions, these corruptions can lead to
dramatic failures of standard off-the-shelf estimators (developed for the i.i.d. setting).
The field of robust statistics was initiated in the 1960s to develop estimators that were robust
to perturbations in the data [HR09; HRRS11]. Starting from the seminal work of Huber [Hub64],
the prototypical contamination model to capture these perturbations is when a small fraction of
arbitrary outliers is included in the data, formalized as the adversary below.
Contamination Model 1 (Global contamination). Let ϵ ∈ (0,1/2). Let S be a multiset of n points
in Rd. Consider all n-sized sets in Rd that differ in at most ϵ-fraction of points, i.e., O(S,ϵ) :=
(cid:8) S′ ⊂ Rd : |S′| = n and |S′∩S| ≥ (1−ϵ)n(cid:9) . The adversary can return any set T ∈ O(S,ϵ). We
call points in S to be the inliers and points in T \S to be the outliers.
These outliers are termed “global” because they can be completely arbitrary, without any
constraints on their magnitude. The effect of such outliers on the statistical rates of estimation was
understoodearlyinthestatisticsliterature[HR09; DL88]. However, earlyrobuststatisticsalgorithms
were either computationally infeasible in large dimensions, with runtimes scaling exponentially with
the dimension d, or achieved asymptotic errors scaling with d.1 In the last decade, the sub-field of
algorithmicrobuststatisticshasresolvedthisissuebydevelopingafamilyofcomputationally-efficient
algorithms for robust estimation under global outliers [DKKLMS16; LRV16]. We summarize these
results for mean estimation below:
Fact 1.1 (Algorithms for robust mean estimation; see, for example, the book [DK23]). Let P be one
of the following distribution families:
(bounded covariance) P : all distributions that have covariance at most identity.
cov
(isotropic bounded moments) P : all distributions that have identity covariance
k−bdd−moments
and bounded k-th moments for an even k ∈ N.2
(isotropic subgaussian) P : all subgaussian distributions that have identity covariance.
subgaussian
There exist polynomial-time algorithms for robust mean estimation under Contamination Model 1
with (near)-optimal error for the distribution families above.
We refer the reader to Appendix A for the exact rates and further discussion of their optimality.
While Contamination Model 1 has served as an extremely successful testbed for the development
of robust statistics for around sixty decades, a potential weakness is that the bulk of the inliers, S∩T,
remain exactly the same. That is, it permits global but sparse perturbations—corresponding to a
small (sparse) fraction of arbitrary (global) outliers. However, it does not account for dense but local
perturbations. Such dense, local perturbations could arise in practice, for example, when measure-
mentsmadebydifferentlymiscalibratedsensors. Tomodelsuchlocalperturbations,recentworkshave
proposed using the Wasserstein distance [ZJT22; ZJS19; LL22; CD23; NGS24], which measures per-
turbations using the Euclidean norm. In contrast, we consider a significantly strengthened adversary:
1An important distinction from classical statistics (with i.i.d. data) is that, due to biases caused by the outliers,
the optimal asymptotic error (as the number of samples goes to infinity) may not vanish. Still, for many well-behaved
distribution families, the optimal asymptotic error is a function that depends only on the contamination rate ϵ.
Importantly, the optimal asymptotic error is dimension-free for such families.
2That is, E [|⟨v,X−µ ⟩|k]1/k ≤σ for a constant σ .
X∼P P k k
2Contamination Model 2 (Strong local contamination). Let ρ ≥ 0. Let S = {x ,...,x } be an
0 1 n
n-sized set in Rd. Consider an adversary that perturbs each point x to x with the only restriction
i (cid:101)i
that in each direction, the average perturbation (over n points) is at most ρ. Formally, we define
 
Wstrong(S 0,ρ) :=  S = {x (cid:101)1,...,x (cid:101)n} ⊂ Rd : sup 1 (cid:88) (cid:12) (cid:12)v⊤(x (cid:101)i−x i)(cid:12) (cid:12) ≤ ρ . (1)
n
 v∈Rd:∥v∥2=1
i∈[n]

The adversary returns an arbitrary set S ∈ Wstrong(S ,ρ) after possibly reordering the points.
0
Contamination Model 2 allows each inlier point to be shifted in a bounded manner in any
direction. Still, the overall perturbation in the Euclidean norm could be significant: Taking the
simplest setting of Gaussian inliers x and independent Gaussian perturbations x = x +ρz for
i √ (cid:101)i i i
z ∼ N(0,I) as an example, we see that each inlier is perturbed roughly by Θ(ρ d) in the Euclidean
i
norm, which is of the same order as the distance of the inlier from the mean µ.3 Despite the
possibility of these large perturbations, one can see that Contamination Model 2, in isolation, is
rather benign: (i) the sample mean of the set S incurs at most O(ρ) error due to local perturbations,
and (ii) Ω(ρ) error is also information-theoretically unavoidable, since all points could be perturbed
by ρ in the same direction. Together, (i) and (ii) imply that simply outputting the sample mean
of the contaminated set achieves error with optimal dependence on ρ. Moreover, as we already
saw in Fact 1.1, mean estimation in Contamination Model 1 is already well-understood. However,
somewhat surprisingly, the contamination model combining the two we have seen above poses new
algorithmic challenges.
Contamination Model 3 (Global and strong local contamination). Let ϵ ∈ (0,1/2) and ρ > 0.
Let S = {x ,...,x } be a set of n points in Rd. The adversary can return an arbitrary set T such
0 1 n
that T ∈ O(S,ϵ) for some S ∈ Wstrong(S ,ρ).
0
The combined contamination model is more reasonable because data might be perturbed in
numerous unforeseen ways [ZJT22; NGS24]. To highlight the algorithmic challenges posed by the
combined contamination model, observe that the two algorithms mentioned above run into problems:
(i) the sample mean is highly sensitive to arbitrary outliers, and (ii) the algorithms from Fact 1.1
are no longer guaranteed to work because the local contamination destroys the moment structure.
To elaborate the latter, the prescribed recipe of algorithmic robust statistics relies crucially on the
covariance of the inliers being bounded (more generally, bounded higher moments). In contrast, even
though the local corruptions have bounded first moment by definition, the higher moments might
be arbitrarily large (cf. Example 1.9). The aim of our work is to develop algorithms for fundamental
estimation tasks that are robust to Contamination Model 3. In particular, we focus on the tasks of
mean estimation and distribution learning; we also show that our techniques extend to principal
component analysis at the end.
1.1 Motivating Questions
In this section, we describe the questions that motivated our technical results in Section 1.2. For
brevity, we focus only on the task of mean estimation in this section.
Starting with the information-theoretic error rates, for any distribution family P in Fact 1.1,
it is easy to see that the optimal error for mean estimation under Contamination Model 3 is
Θ(cid:0) frobust(ϵ)+ρ(cid:1), where frobust(ϵ) denotes the optimal asymptotic error for Contamination Model 1.
P P
3This follows by the fact that the mass of Gaussian distribution is extremely concentrated on a thin spherical shell
√
of radius d around the mean.
3This error is achieved by the multivariate trimmed mean, which is also optimal in each model individ-
ually [LM21]. Unfortunately, the trimmed mean is not computationally efficient in high dimensions,
with runtime scaling exponentially with the dimension, leading to the central focus of our work:
Question 1. Can we perform robust estimation (mean estimation and distribution learning) under
Contamination Model 3 in a computationally-efficient manner?
The simultaneous success of the multivariate trimmed mean across both global and local
contaminationmodelssuggestsanoptimisticidea: perhapstheexistingoutlier-robustalgorithmsfrom
Fact 1.1 continue to work for the more general Contamination Model 3—despite this being far from
obvious, as noted earlier. The main contribution of our work is to show that this is indeed the case.
As we stress below, existing results for Question 1 achieve suboptimal error rates, both in terms
of the global contamination parameter ϵ and the local contamination parameter ρ.
Dependence on Local Perturbation ρ. Starting with the dependence on ρ, the most relevant
work who studied efficient algorithms for Question 1 is the recent paper of Nietert, Goldfeld, and
Shafiee [NGS24]. However, they used a weaker definition for the local corruptions, defiend next.
Contamination Model 4 (Weak local contamination [NGS24]). Let ρ > 0. Let S = {x ,...,x }
0 1 n
be a multiset of n points in Rd. Define Wweak(S ,ρ) as follows:
0
(cid:110) (cid:111)
Wweak(S ,ρ) := S = {x ,...,x } ⊂ Rd : 1(cid:80) ∥x −x ∥ ≤ ρ .
0 (cid:101)1 (cid:101)n n i∈[n] (cid:101)i i 2
The adversary can return any S ∈ Wweak(S ,ρ) after possibly reordering the samples.
0
Withtheabovedefinitionoflocalperturbations(asopposedtoContaminationModel2), [NGS24]
focused on the following combined contamination model (as opposed to Contamination Model 3).
Contamination Model 5 (Global and weak local contamination [NGS24]). Let ϵ ∈ (0,1/2) and
ρ > 0. Let S = {x ,...,x } be a set of n points in Rd. The adversary can return any set T such
0 1 n
that T ∈ O(S,ϵ) for some S ∈ Wweak(S ,ρ).
0
In the contamination model above, local perturbations have bounded average Euclidean norm,
rather than being bounded in each direction individually as in Contamination Model 3. The
observation that the analysis of trimmed mean relies only on the boundedness in each direction
individually (as opposed to bounded Euclidean norm) served as an inspiration for us to consider
Contamination Model 3 and Question 1.
Comparing the weak local contamination (Contamination Model 4) with the strong local
contamination (Contamination Model 2) quantitatively, we see that the corresponding contamination
radii ρ could greatly differ in high dimensions for a given perturbation. Formally, for any set S ⊂ Rd:
0
√
Wweak(S ,ρ) ⊂ Wstrong(S ,ρ) ⊆ Wweak(S ,ρ d), (2)
0 0 0
√
with the d factor being necessary for the last inclusion. Hence, applying the results of [NGS24] to
√
our contamination model (Contamination Model 3) leads to an error that has an extraneous d in
front of ρ, which is undesirable in large dimensions. More importantly, the technical arguments in
[NGS24] critically rely on the local perturbations having small Euclidean norms, raising the question:
Question 2. Do weak and strong local contamination lead to different computational landscapes, when
combined with global contamination? In particular, does the dependence on ρ in the computationally-
efficiently achievable error differ between Contamination Models 3 and 5?
4Dependence on Global Perturbation ϵ. Turning our attention to the dependence on ϵ (the
fraction of global outliers), even for the weaker Contamination Model 5, [NGS24] achieves only
partial results. While the error of their algorithm has optimal ϵ-dependence for the family P of
cov
bounded covariance distributions, defined in Fact 1.1, the dependence is suboptimal for the other
two important distribution families P and P from Fact 1.1.4 In fact, [NGS24]
k−bdd−moments subgaussian
conjectured that no polynomial-time algorithm achieves optimal error for P , noting that
subgaussian
“We suspect that there may be similar obstacles [(computational hardness)] as those known for robust
mean estimation with stable but non-isotropic distributions [HL19]”. The following question thus
generalizes an open problem in [NGS24]:
Question 3. Do local corruptions (either weak or strong) induce new information-computation gaps
for robust estimation? In particular, does the dependence on ϵ in the computationally-efficiently
achievable error change in the presence of local contamination?
In our work, we completely resolve Questions 1 to 3. Importantly, our techniques generalize in a
uniform manner for mean estimation, distribution learning, and principal component analysis.
1.2 Our Results
We present efficient algorithms for mean estimation in Section 1.2.1 and distribution learning in
Section 1.2.2. We defer the results for robust principal component analysis to Section 7.
1.2.1 Mean Estimation
We start with our results for the problem of mean estimation. Our conceptual contribution is
to show that a family of existing outlier-robust algorithms continue to work under the combined
contamination model. In particular, we consider the stability-based algorithms that are guaranteed
to work for global contamination as long as the inliers satisfy the following deterministic condition
called stability. For a set S, we use µ and Σ to denote the empirical mean and empirical covariance
S S
of S, respectively.
Definition 1.2 (Stability, see, e.g., [DK23]). Let ϵ ∈ (0,1/2) and δ ∈ [ϵ,∞). A finite multiset
S ⊂ Rd is called (ϵ,δ)-stable with respect to µ ∈ Rd if for every S′ ⊆ S with |S′| ≥ (1−ϵ)|S|, the
following hold:
(Sample mean) ∥µ −µ∥ ≤ δ,
S′ 2
(Sample covariance) ∥Σ −I ∥ ≤ δ2/ϵ.
S′ d op
The stability condition posits that for all large subsets, the sample mean is close to the true
mean µ, and the sample covariance is comparable to identity in the operator norm. It is known
(Fact A.1) that stability holds with high probability for nice distribution families such as the ones
from Fact 1.1, and a growing body of work has developed algorithms for mean estimation with
global outliers under the sole assumption that the inliers are stable.
Definition1.3(Stability-basedAlgorithms). LetS bean(ϵ,δ)-stablesetwithrespecttoan(unknown)
µ ∈ Rd. Let T be any set such that T ∈ O(S,ϵ) (cf. Contamination Model 1). We call an algorithm
A(T,ϵ,δ) stability-based algorithm if it takes as an input T, ϵ, and δ, and outputs an estimate µ in
(cid:98)
polynomial time such that, with high probability, ∥µ−µ∥ ≲ δ.
(cid:98) 2
4In fact, it can be shown that their algorithm necessarily has suboptimal dependence on ϵ even in one dimension.
5There exist such algorithms based on convex programming [DKKLMS16; SCV18; CDG19],
iterative filtering [DKKLMS16], gradient descent [CDGS20; ZJS22]. Over the years, these stability-
based algorithms have been optimized to be near-optimal in other important aspects: runtime
[CDG19; DHL19; DL22], sample complexity [DKP20], and memory [DKPP22]. Stability-based
algorithms give a unified way to achieve the rates in Fact 1.1. In fact, they have found black-
box consequences on other problems such as principal component analysis (Section 7) and linear
regression [PJL20]. Our main result allows us to seamlessly apply this huge repertoire of algorithms
to the combination of global and local contamination.
Theorem 1.4 (Main Result for Mean Estimation). Let c be a sufficiently small positive constant and
C a sufficiently large constant. Let outlier rate ϵ ∈ (0,c) and contamination radius ρ > 0. Let S be a
0
set that is (ϵ,δ)-stable with respect to an (unknown) µ ∈ Rd, where δ > ϵ. Let T be a corrupted dataset
after ϵ-fraction of global outliers and ρ-strong local corruptions (as per Contamination Model 3). Then,
any stability-based algorithm A(T,ϵ,δ(cid:101)) executed with input T,ϵ,δ(cid:101)= C ·(δ+ρ), outputs an estimate
µ such that with high probability (over the internal randomness of the algorithm): ∥µ−µ∥ ≲ δ+ρ.
(cid:98) (cid:98) 2
Observe that the dependence on ρ is optimal. An astute reader might note that the success of
stability-based algorithms is somewhat surprising in the light of the following contradictory facts:
(i) stability requires the covariance to be bounded and (ii) the covariance of a (stable) set S can
0
increase drastically after local contamination, say, S ∈ Wstrong(S ,ρ). One of our main technical
0
results shows that even though S is not stable (due to large covariance), it does contain a large
stable set, which suffices for stability-based algorithms to work.
Combining this with the fact that stability holds with high probability for the distribution
families of interest (Fact A.1), we obtain the following corollary.
Corollary 1.5. Let P be a family of distributions. Fix a P ∈ P with the (unknown) mean µ, Let
S be a set of n i.i.d. samples from P. Let T be a corrupted version of S with local contamination
0 0
parameter ρ and global contamination rate ϵ (Contamination Model 3). Let τ be the failure prob-
ability such that log(1/τ)/n is less than an absolute constant. There exist computationally-efficient
algorithms that (i) take as input T, ϵ, P, τ, and ρ and (ii) output µ that satisfies the following
(cid:98)
guarantees with probability 1−τ:
If P is the family of isotropic subgaussian distributions, then ∥µ−µ∥ ≲ ϵ(cid:112) log(1/ϵ)+ρ+
(cid:98) 2
(cid:112) (cid:112)
d/n+ log(1/τ)/n.
If P is the family of distributions with isotropic covariance and bounded k-th moments, then
∥µ (cid:98)−µ∥
2
≲ ϵ1− k1 +ρ+(cid:112) (dlogd)/n+(cid:112) log(1/τ)/n.
If P is the family of distributions with covariance Σ ⪯ I then ∥µ−µ∥ ≲ √ ϵ+ρ+(cid:112) (dlogd)/n+
(cid:98) 2
(cid:112)
log(1/τ)/n.
We highlight that the error rates above are known to be information-theoretically optimal in all
√
the parameters ϵ,ρ,d,n,τ (up to the logd factor in the term (cid:112) (dlogd)/n); See Appendix A for
further discussion on optimality. Therefore, Corollary 1.5 simultaneously answers Questions 1 to 3 for
robust mean estimation. In particular, (i) both weak and strong local contamination yield the same
computationally-efficient rates, answering Question 2 and (ii) local contamination (whether weak or
strong) does not induce new information-computation gap, refuting the hypothesis in [NGS24] and
answering Question 3.
A particularly appealing property of our results is that a well-understood, practical, and highly-
optimized family of algorithms is shown to work against local corruptions as well. Furthermore,
the fact that the same algorithms work for all contamination models (local, global, and combined),
6without any modification in the algorithm (other than simple modification in a single parameter), is
highly advantageous, since in practice we often do not know in advance what kinds of corruptions
are present in the dataset.5
Beyond the distribution families in Fact 1.1, another important distribution family for which effi-
cientalgorithmsareknownistheclassofdistributionswithcertifiablyboundedmoments,whichisthe
classofdistributionsforwhichtheboundedmomentconditionshasalow-degreesumofsquaresproof.
These algorithms have the benefit that they do not need to know the covariance of the underlying
inlier distribution. We refer the reader to Section 8 for the related definitions and background.
Theorem 1.6 (Optimal Asymptotic Error for Certifiably Bounded Distributions; informal). Let
ϵ ∈ (0,c) for a sufficiently small absolute constant c. Let P be a distribution over Rd with mean µ
and t-th moment certifiably bounded by M. Then there is an algorithm that takes n = poly(dt,1/ϵ)
samples, runs in time poly(nt,dt2), and outputs an estimate µ ∈ Rd such that with high constant
(cid:98)
probability ∥µ (cid:98)−µ∥
2
≲ Mϵ1−1 t.
The asymptotic error ϵ1−1/t is again optimal for this class of distributions, and the sample com-
plexity is qualitatively optimal for a broad family of algorithms such as statistical query algorithms
and low-degree polynomials [DKKPP22].
1.2.2 Distribution Learning
We now move beyond mean estimation to the problem of distribution learning with respect to the
(sliced)-Wasserstein metric, defined below.
Definition 1.7 (Sliced Wasserstein Distance). Let P,Q be two distributions. The k-sliced p-
Wasserstein Distance is defined as follows:
W (P,Q) := max inf E
(cid:2) ∥V(x−x′)∥p(cid:3)1/p
,
p,k 2
V: rank-k π∈Π(P,Q)(x,x′)∼π
projection matrix
where Π(P,Q) is the set of all couplings of P and Q. By slightly overloading our notation, when S
and Sˆ are sets of points, we denote by W (S,Sˆ) the k-sliced p-Wasserstein distance between the
p,k
uniform distributions over S and Sˆ, respectively.
Distribution learning in the sliced Wasserstein metric has applications in distributionally robust
optimization [NGS24]. To present our result in generality for this problem, we consider the following
contamination model, that interpolates between Contamination Models 2 and 4 for k = 1 and k = d,
respectively.
Contamination Model 6 (Strong Wasserstein Contamination: Contamination in W ). Let ρ > 0.
1,k
Let S = {x ,...,x } be a multiset of n points in Rd. Define the following set of local perturbations
0 1 n
which are small in all rank-k subspaces:
Wstrong(S ,ρ) := (cid:110) S = {x ,...,x } ⊂ Rd : max 1 (cid:88) ∥V(x −x )∥ ≤ ρ.(cid:111) (3)
1,k 0 (cid:101)1 (cid:101)n n (cid:101)i i 2
V: rank-k
projection matrix i∈[n]
The adversary returns any set S ∈ Wstrong(S ,ρ) after possibly reordering the points. We call the set
1,k 0
S to be a ρ-contaminated version of the set S under the k-sliced Wasserstein adversary.
0
5AlthoughTheorem1.4indicatesthatρisincludedintheinputaspartoftheparameterδ(cid:101)=ρ+δforstability-based
algorithms, there exist stability-based algorithms that do not require δ(cid:101)as an input parameter [DKP20, Appendix A].
7Observe that Wstrong is a stronger adversary than Wstrong for k′ ≤ k. The distribution problem
1,k′ 1,k
we consider is the following: Let S be a stable set corresponding to the inliers, which is then
0
corrupted by a combination of local corruptions (Contamination Model 6) and global corruptions
(Contamination Model 1), and the statistician’s goal is to output a distribution Pˆ which is close to
the uniform distribution over S with respect to the W metric (this is a natural metric to use for
0 1,k
measuring the error since the local corruptions in Contamination Model 6 are also measured using
the same metric). Formally, our result is the following:
Theorem 1.8 (Main Result for Distribution Learning). Let ϵ ∈ (0,c) be a parameter for the outlier
rate, where c is a sufficiently small absolute constant, ρ > 0 be a parameter for the local contamination
radius, and δ > ϵ be a parameter for stability. Let S be a set that is (ϵ,δ)-stable with respect to an
0
(unknown) µ ∈ Rd. For a slicing parameter k ∈ [d], let T be the corrupted dataset after local and
global corruptions from Contamination Models 1 and 6 with parameters ρ and ϵ, respectively; formally,
T ∈ O(S,ϵ) for some S ∈ Wstrong(S ,ρ). Then, there exists a polynomial-time algorithm that on
1,k 0
input T,ϵ,ρ,δ, and k′ ∈ [k] √, outputs an estimate S(cid:98)⊂ T such that, with high constant probability, it
holds that W 1,k′(S(cid:98),S 0) ≲ δ k′+ρ.
We note that [NGS24] also provides rates for distribution estimation in the W metric. However,
1,k
their adversary for the local contamination is much weaker (Wstrong). In contrast, our rates for both
1,d
the local perturbation and accuracy are measured in W .
1,k
Our result for mean estimation, Theorem 1.4, is a special case of the above for k′ = 1.6 For
general k ≥ 1, our proposed algorithm is a filtering-based algorithm that uses a certificate lemma
from Nietert, Goldfeld, and Shafiee [NGS24]. We directly optimize this certificate in an efficient
√
manner to obtain the optimal error δ k+ρ; In contrast, [NGS24] optimizes an approximation of
√ √
their certificate, which leads to the larger error max(δ, ϵ) k+ρ.7
We now discuss the error guarantee of Theorem 1.8 in more detail. First, the error of Ω(ρ) is
trivially needed because each point could be shifted by distance ρ along the same direction. Next,
√
the error term δ k is also optimal; see [NGS24, Corollary 5].
We now show how to instantiate the error guarantee of Theorem 1.8 for learning a distribution
P over Rd as opposed to the uniform distribution over a set S . Applying the triangle inequality, we
0 √
get a simple approximation W (Sˆ,P) ≤ W (Sˆ,S )+W (S ,P) = O(δ k+ρ+W (S ,P)).
1,k 1,k 0 1,k 0 1,k 0
While the first two terms are optimal (as shown above), the third term can be upper bounded by
√
O(cid:101)( dkn− max1 (k,2)) using [Boe24]. On the other hand, it has been shown in [NR22] that even for clean
i.i.d. data, any estimator Pˆ necessarily incurs error W (Pˆ,P) = Ω(c n−1/max(k,2)+(cid:112) d/n) for a
1,k d
dimension-dependent term c . Thus, the resulting error guarantee is tight up to the suboptimality
d
of W (S ,P), which we leave for future work.
1,k 0
Finally, the results for robust principal component analysis are deferred to Section 7.
1.3 Related Work
Our work lies broadly in the field of algorithmic robust statistics. We refer the reader to Diakonikolas
and Kane [DK23] for a recent book on the topic. Within this line of our work, our work is most
closely related to [SCV18; DL22; DKP20], which we discuss in Section 1.4. As mentioned earlier,
robust statistics has primarily focused on Contamination Model 1. Some notable exceptions include
[ZJT22; ZJS19; LL22; CD23; NGS24], discussed below in detail.
6It follows by a standard property of sliced-Wasserstein distance that ∥µ −µ ∥ ≲W (Sˆ,S ).
Sˆ S0 2 1,1 0√
7Recallthatfornicedistributionfamilies,δ isthefunctionofϵfromFactA.1;importantlyδ=o( ϵ)forGaussians
and distributions with k>2 bounded moments
8Zhu, Jiao, and Steinhardt [ZJS19] and Liu and Loh [LL22] studied the problems of covariance
estimation and linear regression under the Wasserstein-1 perturbations. Similarly, Chao and
Dobriban [CD23] investigated the Wasserstein-2 perturbations and developed minmax-optimal
estimators under those models. To the best of our knowledge, the combined contamination model
(Contamination Model 5) was first proposed and studied in Zhu, Jiao, and Tse [ZJT22], inspired by
chained perturbations in the computer vision literature [BPD18]. Liu and Loh [LL22] also studied
Contamination Model 5 by focusing on the problems of covariance estimation and linear regression.
However, all of these works focused on the statistical aspects (and weak local contamination), and
did not provide computationally-efficient algorithms.
Our work is most closely related to Nietert, Goldfeld, and Shafiee [NGS24] who developed
computationally-efficient algorithms for the combined contamination model (with weak local pertur-
bations) in Contamination Model 5. In contrast, we study the stronger Contamination Model 3. We
also obtain the improved dependence on ϵ for certain distribution families, which was phrased as an
open question in their work.
Finally, we mention related works from the theory of optimal transport. In fact, the combined
contamination model (Contamination Model 5) is closely related to the notion of outlier-robust
optimal transport cost [BCF20; NGC22], but their focus is rather different. In fact, our results can
be seen as learning when the samples are perturbed in outlier-robust sliced optimal transport cost.
The sliced-Wasserstein distance has been studied in several recent works because it avoids the curse
of dimensionality fundamental to the usual Wasserstein distance [RPDB12; NDCKSS20; MBW22;
Boe24; CNR24].
1.4 Overview of Techniques
In this section, we give an overview of the challenges posed by the (strong) local contamination and
highlight a key technical result towards establishing Theorem 1.4.
We begin by highlighting the issue that local perturbations (whether strong or weak, as defined
in Contamination Models 2 and 4, respectively) can significantly increase the covariance—or more
generally, the higher moments—of the data.
Example 1.9 (Local Corruptions Can Destroy Higher Moment Structure). Suppose the inliers are
S = {x ,...,x } and they have identity covariance. For a unit vector v, consider the following
0 1 n
locally corrupted set S = {x +0.5ρnv,x −0.5ρnv,x ,x ,...,x }. These local corruptions increase
1 2 3 4 n
the covariance of S by at least Θ(ρ2n) in the operator norm. In particular, the set S is not stable
since its stability parameter diverges with n.
While S above does not have bounded covariance (and hence not stable), we see that only a
tiny fraction of points contributes disproportionately to the covariance, and hence, we might as well
consider them outliers since the stability-based algorithms are robust to outliers. Thus, the goal
shifts towards establishing the stability of a large subset S′ ⊂ S of the locally perturbed data, which
would directly imply our result for mean estimation. More generally, for our distribution learning
result, we need an analogous claim for a generalized notion of stability, defined below (where the
notation µ denotes the empirical mean 1 (cid:80) x and Σ the empirical second moment centered
S |S| x∈S S
around µ, i.e., 1 (cid:80) (x−µ)(x−µ)⊤):
|S| x∈S
Definition 1.10 (Generalized Stability). Let ϵ ∈ (0,1/2) and δ ∈ [ϵ,∞). Let S be a set of points in
Rd and µ be a vector in Rd. We say that S satisfies the (ϵ,δ,k)-generalized-stability with respect to µ
if for all S′ ⊆ S with |S′| ≥ (1−ϵ)S, the following hold:
∥µ −µ∥ ≤ δ.
S′ 2
9For every V ∈ V k, (cid:12) (cid:12)(cid:10) V,Σ S′ −I(cid:11)(cid:12) (cid:12) ≤ δ2/ϵ.
where V denotes the set of all rank-k projection matrices.
k
As highlighted above, the key difficulty in establishing the generalized stability property of
S concerns the second property in Definition 1.10, which posits that for all large subsets S′, the
covariance is small in the sense that its inner product with any rank-k projection is at most δ2/ϵ.
To simplify the discussion, let us demonstrate our ideas for the special case when S′ = S, i.e.,
the complete set. The variance-like quantity ⟨V,Σ ⟩ is mainly composed of two terms (ignoring
S
the cross terms): (i) the covariance of the unperturbed data S , ⟨V,Σ ⟩ and (ii) the second
0 S0
moment of the local perturbations: 1 (cid:80) ∥V∆ ∥2. The first term can be handled by stability of
n i i 2
the original data. Thus, the goal is to find a large subset of local perturbations that have bounded
second moment (Theorem 4.2). To be more precise, we need to identify a (1−ϵ)-fraction of local
perturbations {∆ } with second moment matrix bounded by ρ2/ϵ in every rank-k projection.
i i∈[n]
However, establishing the existence of a large stable subset is significantly different for the weak and
strong local contamination, as explained next.
Differences between weak and strong local contamination. To highlight the challenges
between the strong and weak local contamination, we define (∆ ) and (∆′) to be 2n vectors
i i∈[n] i i∈[n]
in Rd corresponding to strong and weak local contamination, respectively. That is, these vectors
satisfy
1 (cid:88) 1 (cid:88)
sup ∥V∆ ∥ ≤ ρ and ∥∆′∥ ≤ ρ, (4)
n i 2 n i 2
V∈V
k i∈[n] i∈[n]
respectively, where V denotes the set of all rank-k projection matrices. For the weak local
k
contamination, finding a large stable subset of the {∆′} with bounded second moment is rather
i i∈[n]
easy: Let I ⊂ [n] be the set of indices corresponding to the (1−ϵ)n many vectors from {∆′}
i i∈[n]
with the smallest Euclidean norms. It can be then checked that the {∆′} have appropriately
i i∈I
bounded second moment as follows: For any V ∈ V , it holds
k
1 (cid:88) 1 (cid:88) 1 (cid:88) ρ ρ2
∆⊤V∆ ≤ ∥∆ ∥2 ≤ max∥∆′∥ · ∥∆′∥ ≲ ·ρ ≲ , (5)
|I| i i |I| i 2 i∈I i 2 |I| i 2 ϵ ϵ
i∈I i∈I i∈I
where we used the Markov inequality to get that all ∆′’s in I have Euclidean norm at most ρ/ϵ, and
i
we also used that 1 (cid:80) ∥∆′∥ ≤ ρ by definition of the weak local perturbations. Implicitly, this
|I| i∈I i 2
is the strategy used in Nietert, Goldfeld, and Shafiee [NGS24].8
However, this norm-based truncation can not work for the strong local contamination. This is
simply because ∥∆′∥ might be Θ(ρ(cid:112) d/k) for all i ∈ [n], and hence the resulting inequality in (5)
i 2
is too loose.
Towards tackling strong local contamination. Let {∆ } now be perturbations according
i i∈[n]
to the strong local contamination (i.e., satisfying the first inequality in (4)). A natural strategy
is to adopt the proof strategy in a direction-dependent manner. For a “direction” V ∈ V , we can
k
define the set I ⊂ [n] to be the set of (1−ϵ)n many indices with the smallest ∥V∆ ∥ ’s. A similar
V i 2
8While [NGS24] does not obtain the optimal dependence on the stability parameter after this truncation, a careful
calculation leads to optimal stability parameter for the locally perturbed data; see Theorem 4.2.
10application of Markov’s inequality implies that max ∥V∆ ∥2 ≤ ρ/ϵ. Following the arguments
i∈IV i 2
similar to (5), we find that for any V ∈ V :
k
1 (cid:88) ρ2
∥V∆ ∥2 ≲ . (6)
|I | i 2 ϵ
V
i∈IV
That is, for any V ∈ V , there is a large subset whose second moment in the “direction“ V ∈ V is
k k
at most ρ2/ϵ. However, the order of quantifiers of V and the I is reversed compared to what we
V
want; we would like to find a single subset that works for every V. In what follows, we show that
the order of quantifiers can actually be fixed by establishing the following statement in this section:
Proposition 1.11. Let points ∆ ∈ Rd as in (4). Then for every ϵ ∈ (0,1) there exists a subset
i
I ⊆ [n] such that (i) |I| ≥ (1−ϵ)n and (ii) for all V ∈ V , 1 (cid:80) ∥V∆ ∥2 ≲ ρ2/ϵ.
k |I| i∈I i 2
We note that the proposition above is deterministic. Our proof strategy builds on Steinhardt,
Charikar, and Valiant [SCV18] and Diakonikolas, Kane, and Pensia [DKP20], with crucial differences,
as explained next. [SCV18] includes a similar result for the k = 1 case, but their formulation and
proof do not seem to capture the rank-k sliced distance, and hence their result does not yield the
more general version of k ≫ 1, which is crucially needed for our result on distribution learning.
On the other hand, [DKP20] focuses on establishing good sample complexity (again for k = 1) for
stability (Fact A.1) as opposed to the deterministic statement above.
We now sketch the proof of Proposition 1.11. Instead of solving the discrete problem above
(optimizing over all large subsets I), following [SCV18; DKP20], we begin by performing a convex
relaxation and define
(cid:40) n (cid:41)
(cid:88) 1
∆ := w ∈ Rn : w = 1;0 ≤ w ≤ .
n,ϵ + i i (1−ϵ)n
i=1
Aroundingargumentshowsthatfindingaw ∈ ∆ sufficestoproveProposition1.11,i.e.,itsufficesto
n,ϵ
show that min max (cid:80) w ∥V∆ ∥2 ≲ ρ2/ϵ (Lemma A.3 from [DKP20]). As alluded to ear-
w∈∆n,ϵ V∈V k i i i 2
lier, if the order of quantifiers for w and V was reversed, the desired conclusion would follow from (6).
In order to reverse these quantifiers, we shall use the min-max duality for bilinear programs
over convex compact sets. Thus, we perform a convexification of the max variable and define
M := {M ∈ Rd×d : 0 ⪯ M ⪯ I;tr(M) = k}, which is the convex hull of {V ∈ Rd×d : V ∈ V }. We
k k
thus arrive at the key reformulation:
n n n
(cid:88) (cid:88) (cid:88)
min max w ∥V∆ ∥2 = min max w ∆⊤M∆ = max min w ∆⊤M∆ , (7)
i i 2 i i i i i i
w∈∆n,ϵV∈V
k i=1
w∈∆n,ϵM∈M
k i=1
M∈M kw∈∆n,ϵ
i=1
where the last equality is due to the min-max duality.
While the order of quantifiers allows us to perform direction-dependent truncation, we are faced
with the new challenge that we do not have guarantees on the behavior of {∆⊤M∆ }n for a general
i i i=1
M ∈ M . To be precise, while (6) implies that max min (cid:80) w ∆⊤V∆ ≲ ρ2/ϵ, the same
k V∈V k w∈∆n,ϵ i i i i
argument does not apply when the max is taken over M . This is because of the non-linearity
k
induced by the min operator (if it was linear in M, then the maximum over V and the analogous
w k
maximum over M ∈ M would have been equal by convexity).
k
A simple observation here is to note that if the ∆ ’s were well-behaved with respect to M in
i
(cid:113)
the sense that sup (cid:80)n 1 ∆⊤M∆ ≲ ρ, then we can control the right hand side in (7) by
M∈M k i=1 n i i
11ρ2/ϵ using the same truncation strategy as in (6). Using a Gaussian rounding scheme, inspired by a
similar rounding scheme from Depersin and Lecué [DL22], we prove in Proposition 3.1 that
(cid:88)n 1(cid:113) 1 (cid:88)
sup ∆⊤M∆ ≲ sup ∥V∆ ∥ ,
n i i n i 2
M∈M V∈V
k i=1 k i∈[n]
which completes the proof of Proposition 1.11.
Completing the proof of Theorem 1.4. In Theorem 4.2, we show that if the local perturba-
tions have the second moment matrix bounded by ρ2/ϵ in each “direction” M ∈ M , then these
k
perturbations can degrade the stability parameter δ by at most additive ρ (up to additional constant
prefactors). Importantly, Theorem 4.2 preserves the dependence on δ as opposed to the analysis
√
in [NGS24], which obtains a bound in terms of max(δ, ϵ). Proposition 1.11 implies that a large
subset of local perturbations has bounded second moment matrix in each “direction” V ∈ V (and
k
by convexity the same is true for every direction M ∈ M ). Combining these two claims, we get the
k
existence of a large stable subset after strong local contamination, finishing the proof of Theorem 1.4.
1.5 Organization
The rest of the paper is organized as follows: Section 2 contains basic definitions and the key
properties of stable sets that will be useful later on. Section 3 states the relationship between the
low-rank projections and their convex counterparts. In Section 4, we show that local perturbations
with bounded covariance suffice for stability. Sections 5, 6 and 8 include the proofs of Theorems 1.4,
1.6 and 1.8, respectively. We include the results for principal component analysis in Section 7.
2 Preliminaries
Basic notation. We use Z for the set of positive integers and [n] to denote {1,...,n}. For a
+
vector x we denote by ∥x∥ its Euclidean norm. Let I denote the d×d identity matrix (omitting
2 d
the subscript when it is clear from the context). We use Sd−1 to denote the set of points v ∈ Rd
with ∥v∥ = 1. We use ⊤ for the transpose of matrices and vectors. For a subspace V of Rd of
2
dimension m, we denote by P ∈ Rd×d the orthogonal projection matrix of V. That is, if the
V
subspace H is spanned by the columns of the matrix A, then P := A(A⊤A)−1A⊤. By slightly
H
overloading notation, if A is a matrix, we will also use P to denote the orthogonal projection matrix
A
for the subspace spanned by the columns of A. We say that a symmetric d×d matrix A is PSD
(positive semidefinite) and write A ≽ 0 if for all x ∈ Rd it holds x⊤Ax ≥ 0. We use ∥A∥ for the
op
operator (or spectral) norm of the matrix A. We use tr(A) to denote the trace of the matrix A and
⟨A,B⟩ = tr(AB⊤) to denote the Frobenius inner product between matrices A and B. For a PSD
√
matrix M and a vector x, ∥x∥ := x⊤Mx denotes the Mahalanobis norm of x with respect to M.
M
We write x ∼ D for a random variable x following the distribution D and use E[x] for its
expectation. We use N(µ,Σ) to denote the Gaussian distribution with mean µ and covariance
matrix Σ. We write Pr(E) for the probability of an event E. We write 1 for the indicator function
E
of the event E.
We use a ≲ b to denote that there exists an absolute universal constant C > 0 (independent of
the variables or parameters on which a and b depend) such that a ≤ Cb. Sometime, we shall abuse
the notation and use a = O(b) to denote the same to save space.
12Projection matrices and convex relaxations. We use V to denote the set of all rank-k
k
projection matrices in Rd×d. Recall that for any V ∈ V , V is symmetric, PSD, and idempotent.
k
We use M to denote the set of convex relaxation of V , i.e.,
k k
M := {M ∈ Rd×d : M ⪰ 0, M ⪯ I, tr(M) = k}. (8)
k
Empirical mean and second moment matrices. For a S ⊂ Rd, we use the following notation
for the sample mean, sample covariance, and the centered second moment matrix with respect to µ
(which shall be clear from context), respectively:
1 (cid:88) 1 (cid:88) 1 (cid:88)
µ := x, Σ := (x−µ )(x−µ )⊤, Σ := (x−µ)(x−µ)⊤ . (9)
S S S S S
|S| |S| |S|
x∈S x∈S x∈S
2.1 Generalized Rank-k Stability
As outlined in Section 1, our algorithm for mean estimation relies on the exact same stability
condition developed in prior work. However, for our distribution learning result, our algorithm
is a multi-dimensional generalization of the standard filtering, which requires us to consider an
appropriate generalization of the stability condition, presented in Definition 1.10. For k = 1 this
definition reduces to the standard stability condition.
Definition 1.10 (Generalized Stability). Let ϵ ∈ (0,1/2) and δ ∈ [ϵ,∞). Let S be a set of points in
Rd and µ be a vector in Rd. We say that S satisfies the (ϵ,δ,k)-generalized-stability with respect to µ
if for all S′ ⊆ S with |S′| ≥ (1−ϵ)S, the following hold:
∥µ −µ∥ ≤ δ.
S′ 2
For every V ∈ V k, (cid:12) (cid:12)(cid:10) V,Σ S′ −I(cid:11)(cid:12) (cid:12) ≤ δ2/ϵ.
where V denotes the set of all rank-k projection matrices.
k
Remark 2.1. Using convexity arguments, it can be seen that we can replace the condition “for
every V ∈ V ” with “for every M ∈ M ” (cf. (8)) in the second condition of Definition 1.10.
k k
2.1.1 Equivalent Definitions of Generalized Stability
We will often need to use basic properties of the stability condition that follow directly from its
definition. These properties are presented in Lemma 2.2 as equivalent ways to define the stability
condition. These equivalences have been shown in the literature for the special case of k = 1 (see,
e.g., Claim 4.1 in [DKP20] and Lemma 3.1 in [DK23]), but the proof readily extends to general k.
Lemma 2.2. Definitions 1.10, 2.3 and 2.4 are all equivalent to each other, up to an absolute constant
factor in front of the parameter δ.
Definition 2.3 (Generalized Stability; Alternative Definition I). Let ϵ ∈ (0,1/2) and δ ∈ [ϵ,∞). Let
S be a set of points in Rd and µ be a vector. We say that S satisfies the (ϵ,δ,k)-generalized-stability
condition with respect to µ ∈ Rd if the following holds for every M ∈ M : (i) ∥µ −µ∥ ≤ δ, (ii)
k S 2
(cid:10) M,Σ −I(cid:11) ≤ δ2/ϵ, and (iii) For all S′ ⊂ S with |S′| ≥ (1−ϵ)|S| it holds (cid:10) M,Σ −I(cid:11) ≥ −δ2/ϵ.
S S′
Definition 2.4 (GeneralizedStability; AlternativeDefinitionII). Let ϵ ∈ (0,1/2) and δ ∈ [ϵ,∞). Let
S be a set of points in Rd and µ be a vector. We say that S satisfies the (ϵ,δ,k)-generalized-stability
condition with respect to µ ∈ Rd if for every M ∈ M , the set S satisfies the first two conditions of
k
Definition 2.3 and it also satisfies the following condition: For all T ⊂ S with |T| ≤ ϵ|S| it holds
that 1 (cid:80) ∥x−µ∥2 = |T| ⟨M,Σ ⟩ ≤ δ2/ϵ.
|S| x∈T M |S| T
132.2 Consequences of (Generalized) Stability
The next result gives a bound on the average of ∥x−µ∥ over a small subset of a stable set.
M
Lemma 2.5. Let S be a finite multiset of n points satisfying the (ϵ,δ,k)-generalized-stability condition
with respect to µ ∈ Rd. Then max max 1 (cid:80) ∥x−µ∥ ≲ δ.
M∈M k T⊂S:|T|≤ϵ|S| |S| x∈T M
Proof. Using Cauchy-Schwarz inequality and the last condition in Definition 2.4, we obtain
(cid:115)
1 (cid:88) |T| 1 (cid:88)
max max ∥x∥ ≤ max max ∥x∥2
M∈M T⊂S |S| M M∈M T⊂S |S| |T| M
k |T|≤ϵn x∈T k |T|≤ϵn x∈T
(cid:115) (cid:115) (cid:114)
|T| 1 (cid:88) √ δ2
= max max ∥x∥2 ≲ ϵ ≤ δ.
M∈M T⊂S |S| |S| M ϵ
k |T|≤ϵn x∈T
Lemma 2.6. Let S be an (ϵ,δ)-stable set with respect to µ. Then S is also (ϵ,δ′,k)-generalized
√
stable with respect to µ with δ′ ≲ δ k.
Proof. Since the mean condition in the definition of generalized stability is the same as the one in
the plain stability, this condition holds trivially. For the covariance condition, let M = (cid:80)d λ v v⊤
i=1 i i i
be the spectral decomposition of M. Then, for a subset S′ ⊆ S with |S′| ≥ (1−ϵ)|S| we have that
(cid:12)(cid:42) (cid:43)(cid:12)
d
(cid:12)(cid:42) (cid:43)(cid:12)
d
(cid:12) 1 (cid:88) (cid:12) (cid:88) (cid:12) 1 (cid:88) (cid:12) (cid:88)
(cid:12) M, xx⊤−I (cid:12) ≤ λ (cid:12) v v⊤, xx⊤−I (cid:12) ≤ λ δ2/ϵ = tr(M)δ2/ϵ = kδ2/ϵ ,
(cid:12) |S′| (cid:12) i(cid:12) i i |S′| (cid:12) i
(cid:12) (cid:12) (cid:12) (cid:12)
x∈S′ i=1 x∈S′ i=1
where the second inequality uses the (ϵ,δ)-stability of S.
The next result shows that all large subsets of a stable set are stable, and the contamination
parameter ϵ is “robust” to constant prefactors.
Lemma 2.7. Let S be (ϵ,δ,k)-generalized stable with respect to µ. Let r ≥ 1 be such that rϵ ≤ 1/2.
√
1. Then S is also (rϵ,δ′,k)-generalized stable with respect to µ for δ′ ≲ δ r.
2. Any subset S′ ⊂ S such that |S′| ≥ (1−rϵ)|S|, is also (ϵ,δ′,k)-generalized-stable with respect
√
to µ with δ′ ≲ δ r.
Proof. We start with the first claim, which we show using Definition 2.4 for the definition of
generalized stability (and Lemma 2.2, stating that all definitions are equivalent to each other up
to an absolute constant in front of the δ). The first two conditions in Definition 2.4 (about the
mean and second moment over all the points in S) hold trivially by the (ϵ,δ,k)-generalized stability
of S. It remains to show the last condition, that for every set T ⊆ S with |T| ≤ rϵ|S| it holds
1 (cid:80) ∥x−µ∥2 ≤ δ′2/ϵ. This can be seen by splitting T into at most r disjoint sets of size at
|S| x∈T M
most ϵ|S| each, and apply the corresponding condition from the (ϵ,δ,k)-generalized stability of S.
That is, write T = T ∪···∪T where T are disjoint, |T | ≤ ϵ|S| and r′ ≤ r. Then
1 r′ j j
r′
1 (cid:88) (cid:88) 1 (cid:88)
∥x−µ∥2 = ∥x−µ∥2 ≤ r′δ2/ϵ .
|S| M |S| M
x∈T j=1 x∈Tj
14We move to the second claim. Using the first claim, we have that S is ((r+1)ϵ,δ′,k)-generalized
√
stable with δ′ ≲ rδ (since r ≥ 1). It remains to check that the two conditions from Definition 1.10
holdforeverysubsetS′′ofsize|S′′| ≥ (1−ϵ)|S′|. Since(1−ϵ)|S′| ≥ (1−ϵ)(1−rϵ)|S| ≥ (1−(r+1)ϵ))|S|,
the desired conditions follow by the ((r+1)ϵ,δ′,k)-generalized stability of S.
Finally,thenextresultshowsthatalllargesubsetsofastablesetarecloseinthesliced-Wasserstein
metrics (Definition 1.7).
Lemma 2.8. Let S be a set satisfying (ϵ,δ,k)-generalized stability, and S′ be a subset of S with
0 √ 0 √ √0
|S′| ≥ (1−ϵ)|S | for ϵ ≤ 1/2. Then, W (S ,S′) ≲ ϵ k+δ and W (S ,S′) ≲ ϵk+δ/ ϵ.
0 0 1,k 0 0 2,k 0 0
Proof. Let us use the notation S = {x ,...,x }, for our set satisfying (ϵ,δ,k)-generalized stability
0 1 n
with respect to µ. Let us use µ = 0 thought the proof without loss of generality. Define J ⊂ [n] for
the set of indices corresponding to the points in S′ (and J∁ = [n]\J for the rest of the points),
0
and denote m := |J| = |S′|. Recall the definition of sliced-Wasserstein distance from Definition 1.7
0
for p ∈ {1,2}:
W (S ,S′) = sup inf E (cid:2) ∥V(x−x′)∥p(cid:3)1/p .
p,k 0 0 2
V∈V kπ∈Π(S0,S 0′)(x,x′)∼π
We will use the following coupling π on (x,x′):
• First, x′ = x for an index i chosen uniformly at random from J.
i
• Then, conditioned on x′ = x , with probability m/n, x is set to be x and with probability
i i
1−m/n, x is chosen to be x for an index chosen uniformly at random from J∁.
i
It can be checked that this is a valid coupling: The marginal of x′ is the uniform distribution
on S′ (by definition), and the marginal of x is uniform on S since for every i ∈ J we have
0 0
P[x = x ] = 1 m = 1/n and for every i ∈ J∁ we have P[x = x ] = (cid:80) 1(1−m/n) 1 = 1/n.
i m n i i∈[m] m n−m
We can thus bound W (S ,S′) as follows:
p,k 0 0
W (S ,S′)p ≤ sup E (cid:2) ∥V(x−x′)∥p(cid:3)
p,k′ 0 0 2
V∈V (x,x′)∼π
k
≲ sup 1 (cid:88) E (cid:2) ∥V(x−x′)∥p|x′ = x (cid:3)
V∈V m (x,x′)∼π 2 i
k i∈J
 
= sup m1 (cid:88) m n∥V(x i−x i)∥p 2+ n− nm n−1
m
(cid:88) ∥V(x j −x i)∥p 2
V∈V
k i∈J j∈J∁
= sup 1 (cid:88) ∥V(x −x )∥p . (10)
mn j i 2
V∈V
k i∈J,j∈J∁
Controlling W . We first consider the easy case of p = 1, for which we can use the triangle
1,k
inequality to obtain the following:
1 (cid:88) 1 (cid:88)
∥V(x −x )∥ ≤ (∥Vx ∥ +∥Vx ∥ )
j i 2 j 2 i 2
mn mn
i∈J,j∈J∁ i∈J,j∈J∁
1 (cid:88) 1 (cid:88)
≲ ∥Vx ∥ +ϵ· ∥Vx ∥
j 2 i 2
n m
j∈J∁ i∈J
151 (cid:88)
≲ δ+ϵ· ∥Vx ∥ , (11)
i 2
n
i∈[n]
where the bound on the first term follows by Lemma 2.5, and the bound on the second term uses
that n ≲ m. We now use the (ϵ,δ,k)-generalized-stability of S and Cauchy-Schwarz inequality to
0
obtain the following:
(cid:118)
1 (cid:88) (cid:117)1 (cid:88) (cid:112) (cid:112) √ √
∥Vx ∥ ≤ (cid:117) ∥Vx ∥2 = ⟨V,Σ ⟩ ≲ k+δ2/ϵ ≲ k+δ/ ϵ .
n i 2 (cid:116)n i 2 S0
i∈[n] i∈[n]
√ √
This concludes an upper bound on W (S ,S′) of the order δ+ϵ k+δ ϵ.
1,k 0 0
Controlling W . We now turn our attention to W (S ,S′) using p = 2 in (10). Using the
2,k 2,k 0 0
inequality (a+b)2 ≤ 2a2+2b2 to analyze the cross term, we obtain:
1 (cid:88) (cid:88) ϵ (cid:88) 1 (cid:88)
∥V(x −x )∥2 ≲ ∥Vx ∥2+ ∥Vx ∥2
nm j i 2 m i 2 n j 2
i∈J j∈J∁ i∈J j∈J∁
δ2 (cid:18) δ2(cid:19) δ2 δ2
≲ ϵ⟨V,Σ ⟩+ ≲ ϵ k+ + ≲ ϵk+ .
S0
ϵ ϵ ϵ ϵ
where the bounds in the last line follow by the generalized stability of the original points S . Thus,
0
we conclude that W (S ,S′)2 ≲ ϵk+δ2/ϵ.
2,k 0
3 Averages of Low-rank Projections and Their Convex Relaxations
Inthissection, wederivethecrucialstructuralpropertyofthelocalperturbationandits(generalized)
projections. To elaborate, let {∆ }n be the local perturbations satisfying Contamination Model 6
i i=1
(the sliced Wasserstein distance). Towards our ultimate goal of establishing stability of the locally
perturbed data, we need to argue that the {∥∆ ∥ } behaves nicely for any M ∈ M . While for
i M i∈[n] k
any projection V ∈ V , the desired niceness of {∥∆ ∥ } follows directly from Contamination
k i V i∈[n]
Model 6, our proof arguments necessitate understanding the behavior of the generalized projections
for M ∈ M . The reason for considering these general matrices stems from a convex relaxation
k
needed to employ the min-max duality theorem in the proof of Proposition 4.1.
Proposition 3.1 (Bound on Average Projections). Let y ,y ,...,y be vectors in Rd. Then the
1 2 n
following holds: sup 1 (cid:80)n ∥y ∥ ≲ sup 1 (cid:80)n ∥y ∥ .
M∈M k n i=1 i M V∈V k n i=1 i V
We prove the above result using a Gaussian rounding scheme, inspired by [DL22].
Proof. Let ρ := sup 1 (cid:80)n ∥Vy ∥ . Suppose that there exists a M ∈ M with the property
(cid:113) V∈V k n i=1 i 2 k
1 (cid:80)n y⊤My ≥ C′ ·ρ for a sufficiently large constant C′. We will show that this leads to a
n i=1 i i
contradiction.
For a r > 0, let B := {B ∈ Rd×d ⪰ 0 : ∥B∥ ≤ r, B is rank-k} be the set of rank-k PSD
r op
matrices with bounded operator norm. Let g ,...,g be i.i.d. samples from N(0,M) and define
1 k
B := 1 (cid:80)k g g⊤ to be the empirical second moment matrix, which is an unbiased estimate of M.
k i=1 i i
We define the random variable
n
1 (cid:88)
Z = ∥y ∥ 1 .
n
i B B∈Br
i=1
16On the one hand, we have that
1 (cid:88)n √ 1 (cid:88)n √
Z ≤
n
∥B1/2∥ op∥y i∥P B1
B∈Br
≤ r sup
n
∥y i∥
V
≤ rρ, (12)
V∈V
i=1 k i=1
√
where we use that B is a rank-k matrix if B ∈ B . The above implies that E[Z] ≤ rρ. We shall
r
now show contradiction by deriving a lower bound on E[Z].
Towards that goal, we use the following decomposition to handle the indicator event B ∈ B :
r
n n
1 (cid:88) 1 (cid:88)
E[Z] = E[∥y ∥ ]− E[∥y ∥ 1 ] (13)
n
i B
n
i B B̸∈Br
i=1 i=1
where the expectation is taken with respect to the random matrix B.
We first obtain a lower bound on the first term above. Consider the random variables
R := ∥y ∥2 = 1 (cid:80)k (g⊤y )2, which is a degree-two polynomial of the Gaussian samples. Then
i i B k j=1 j i √
E[R ] = ∥y ∥2 . To obtain a lower bound on E[ R ], we shall prove an upper bound on E[R2].
i i M i i
Using E[G4] ≤ 3(E[G2])2 for a Gaussian variable G, we obtain E[R2] = 1 (cid:80)k E[(g⊤y )4] +
i k2 j=1 j i
k(k−1) (E[R ])2 < k+3(E[R ])2 ≤ 4(E[R ])2. We shall use this upper bound in the following inequal-
k2 i k i i
ity: E[|X|]2/3E[|X|4]1/3 ≥ E[|X|2] which holds for any real-valued random variable X with finite
√ √
fourth moments. Applying it to R , we obtain E[ R ] ≥ E[Ri]3/2 ≥ E[Ri]3/2 = 1(cid:112) E[R ], where
i i E[R i2]1/2 2E[Ri] 2 i
the middle step uses the aforementioned upper bound on E[R2]. Combining everything, we have
i
shown the following lower bound on the first term:
1 (cid:88)n 11 (cid:88)n (cid:113) (cid:2) (cid:3) 11 (cid:88)n (cid:113)
E[∥y ∥ ] ≥ E ∥y ∥2 = ∥y ∥2 .
n i B 2n i B 2n i M
i=1 i=1 i=1
where the second step uses that E[B] = M. We now show that the second term in (13) can be
ignored as follows:
1 (cid:88)n E[∥y ∥ 1 ] ≤ 1 (cid:88)n (cid:113) E(cid:2) ∥y ∥2 (cid:3)(cid:112) Pr[B ̸∈ B ] (Cauchy-Schwarz)
n i B B̸∈Br n i B r
i=1 i=1
n (cid:114)
1 (cid:88) 4C
≤ ∥y ∥ ,
i M
n r
i=1
where the last inequality follows by concentration of covariance of Gaussians, which we establish
next. First, we observe that M must have rank at least tr(M)/∥M∥ ≥ k. Since g ,...,g are
op 1 k
sampled i.i.d. from N(0,M) with rank(M) ≥ k, the matrix B := (cid:80) g g⊤ has rank exactly k has
i i i
rank exactly k with probability 1; This is because the Lebesgue measure of a rank-deficient subspace
is zero. Thus, it remains to show that B has operator norm at most r with probability at least
1−1/32. Observe that B is the second moment matrix of k independent Gaussians whose covariance
matrix has trace equal to k. Applying the Gaussian covariance concentration results to this setting
(see, e.g., [KL17, Theorem 4] or [Van17, Theorem 5.1]), we obtain that for an absolute constant C:
(cid:32)(cid:115) (cid:33)
1 1 (cid:113)
E[∥B−M∥ ] ≤ C∥M∥ + ≤ C(1+ ∥M∥ ) ≤ 2C.
op op op
∥M∥ ∥M∥
op op
Since for any r ≥ 2, B ̸∈ B implies that ∥B−M∥ ≥ r−1 ≥ r/2, applying the Markov inequality,
r op
we obtain the desired inequality Pr(B ̸∈ B ) ≤ 4C.
r/2 r
17Putting everything together and taking r = 64C, we obtain the following:
(cid:32) (cid:114) (cid:33) n n
1 4C 1 (cid:88) 11 (cid:88)
E[Z] ≥ − ∥y ∥ = ∥y ∥ ≥ C′ρ/4, (14)
i M i M
2 r n 4n
i=1 i=1
√ √
where we used the assumption (cid:80)n ∥y ∥ > C′ρ. If C′/4 > r = 128C, this contradicts the
√ i=1 i M
upper bound E[Z] ≤ rρ established earlier.
4 Stability Is Preserved Under Local Perturbations
In this section, we present the main technical results behind Theorems 1.4 and 1.8. First, we restate
Proposition 1.11, which establishes the existence of a large subset of local perturbations whose
second moment is bounded appropriately.
Proposition 4.1. Let z ,...,z be vectors in Rd satisfying max 1 (cid:80) ∥z ∥ ≤ ρ. Then for
1 n V∈V k n i∈[n] i V
every ϵ ∈ (0,1) there exists a subset I ⊆ [n] such that (i) |I| ≥ (1−ϵ)n and (ii)
1 (cid:88) 1 (cid:88)
max ∥z ∥2 = max ∥z ∥2 ≲ ρ2/ϵ. (15)
V∈V |I| i V M∈M |I| i M
k i∈I k i∈I
The proof of this result was provided in Section 1.4. The first equality is simply because M is
k
the convex hull of V and VV⊤ = V for all V ∈ V .
k k
We shall use the above result and Proposition 3.1 in combination with the following result,
stating that as long as local perturbations have bounded second moment, the stability parameter
degrades in a dimension-independent manner.
Theorem 4.2. Let S′ = {x ,...,x } be an (ϵ,δ,k)-generalized stable set with respect to µ ∈ Rd.
0 1 n
Let ∆ ,...,∆ ∈ Rd be vectors satisfying
1 n
1 (cid:88) 1 (cid:88) ρ2
max ∥∆ ∥ ≲ ρ and max ∥∆ ∥2 ≲ . (16)
M∈M n i M M∈M n i M ϵ
k k
i∈[n] i∈[n]
Define x := x +∆ for all i ∈ [n] and define the set S′ to be {x } . Then the following hold:
(cid:101)i i i (cid:101)i i∈[n]
S′ satisfies (ϵ,δ(cid:101),k)-generalized stability with respect to µ (Definition 1.10) for δ(cid:101)≲ δ+ρ.
√
For all large subsets S′′ ⊂ S′ with |S′′| ≥ (1−ϵ)|S′|: W (S′,S′′) ≲ ρ+ϵ k+δ.
1,k 0
√
For all large subsets S′′ ⊂ S′ with |S′′| ≥ (1−ϵ)|S′|: W (S′,S′′) ≲ ϵk+ √δ + √ρ .
2,k 0 ϵ ϵ
We break down the proof into two separate statements, showing the stability and closeness in
Wasserstein distance individually:
Lemma 4.3. Consider the setting in Theorem 4.2. Then S′ satisfies (ϵ,δ(cid:101),k)-generalized-stability
with respect to µ for δ(cid:101)≲ δ+ρ.
Lemma 4.4. Consider the setting in Theorem 4.2. Then for all |S′′| ≥ (1 − ϵ)|S′|, we have
√ √ √ √
W (S′,S′′) ≲ ρ+ϵ k+δ and W (S′,S′′) ≲ ϵk+δ/ ϵ+ρ/ ϵ.
1,k 0 2,k 0
Theorem 4.2 follows trivially from the above two results. We provide the proofs of Lemmata 4.3
and 4.4 in the next two sections.
184.1 Proof of Lemma 4.3
Proof of Lemma 4.3. We use µ = 0 throughout this proof without loss of generality. To establish
the desired stability result, Lemma 2.2 implies that it is equivalent (up to a constant factor in δ(cid:101)) to
establishing the following conditions (simultaneously):
1. (Mean) ∥µ S′∥
2
≤ δ(cid:101).
2. (Upper bound on covariance) For all M ∈ M k: (cid:10) M,Σ
S′
−I(cid:11) ≤ δ(cid:101)2/ϵ.
3. (Lower bound on all large subsets) For all S′′ ⊂ S′ with |S′′| ≥ (1−ϵ)|S′| and all M ∈ M :
k
(cid:10) M,Σ
S′
−I(cid:11) ≥ −δ(cid:101)2/ϵ.
We will establish these three conditions separately.
Mean condition. We start with the mean condition, which follows directly by triangle inequality,
the stability of the original points {x } , and the assumption (16):
i i∈[n]
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) n
(cid:13) (cid:13) (cid:13)n1 (cid:88) x (cid:101)i(cid:13) (cid:13)
(cid:13)
≤ (cid:13) (cid:13) (cid:13)n1 (cid:88) x i(cid:13) (cid:13)
(cid:13)
+(cid:13) (cid:13) (cid:13)n1 (cid:88) ∆ i(cid:13) (cid:13)
(cid:13)
≲ δ+ sup (cid:88) |v i⊤∆ i| ≲ δ+ρ .
(cid:13) i∈[n] (cid:13) (cid:13) i∈[n] (cid:13) (cid:13) i∈[n] (cid:13) v∈Sd−1 i=1
2 2 2
Upper bound on covariance. Using the decomposition x = x +∆ yields the following:
(cid:101)i i i
max (cid:10) M,Σ −I(cid:11) = 1 (cid:88) ∥x ∥2 −tr(M)
M∈M S n (cid:101)i M
k
i∈[n]
= 1 (cid:88) (cid:16) (cid:0) ∥x ∥2 −tr(M)(cid:1) +∥∆ ∥2 +2x⊤M∆ (cid:17) . (17)
n i M i M i i
i∈[n]
We now bound each of the terms above separately.
• The first expression (17) can be handled by the generalized stability of the original points in
S′. Since M ∈ M , and S is (ϵ,δ,k)-stable, we obtain that 1 (cid:80) ∥x ∥2 −tr(M) ≤ δ2/ϵ.
0 k 0 n i∈[n] i M
• The next term in (17) is at most ρ2/ϵ by (16).
• We finally bound the cross term in (17). For any fixed M ∈ M, define I to be the set
M
of indices in [n] such that ∥x ∥ is bigger than Cδ/ϵ. For a large enough constant C, the
i M
last condition in Definition 2.4 implies that |I | ≤ ϵ. Combining this upper bound on the
M
cardinality of I with the last condition in Definition 2.4 we have that 1 (cid:80) ∥x ∥2 ≲ δ2.
Using the
CaucM
hy-Schwarz inequality, we obtain that for any M ∈ M
:n i∈IM i M ϵ
k
1 (cid:88) 1 (cid:88)
x⊤M∆ ≤ ∥∆ ∥ ∥x ∥
n i i n i M i M
i∈[n] i∈[n]
1 (cid:88) 1 (cid:88)
≤ ∥∆ ∥ ∥x ∥ 1 + ∥∆ ∥ ∥x ∥ 1
n
i M i M i̸∈IM
n
i M i M i∈IM
i∈[n] i∈[n]
(cid:118) (cid:115)
1 (cid:88) δ (cid:117)1 (cid:88) 1 (cid:88)
≲ ∥∆ ∥ +(cid:117) ∥∆ ∥2 ∥x ∥2
n ϵ i M (cid:116)n i M n i M
i∈[n] i∈[n] i∈IM
19(cid:114) (cid:114)
δρ ρ2 δ2 δρ
≲ + ≲ . (18)
ϵ ϵ ϵ ϵ
where the last step uses (16) and 1 (cid:80) ∥x ∥2 ≲ δ2 which we have shown earlier.
n i∈IM i M ϵ
Combining everything, we have shown that for all matrices M ∈ M , the following bound holds:
k
⟨M,Σ
S′
−I⟩ ≲ δ ϵ2 + ρ ϵ2 + δ ϵρ ≤ δ(cid:101) ϵ2. Therefore ⟨M,Σ
S′
−I⟩ ≲ δ(cid:101)2/ϵ for some δ(cid:101)≲ δ+ρ.
Lower bound on covariance. ForanyMandanysubsetS′′ ⊂ S′,thedecompositionin(17)yields
1 (cid:88) ∥x ∥2 −tr(M) = 1 (cid:88) (cid:16) (cid:0) ∥x ∥2 −tr(M)(cid:1) +∥∆ ∥2 +2x⊤M∆ (cid:17) .
|S′′| (cid:101)i M |S′′| i M i M i i
i:x (cid:101)i∈S′′ i:x (cid:101)i∈S′′
Since |S′′| ≥ (1−ϵ)|S′| ≥ (1−2ϵ)|S′|, the first term is at least −O(δ2/ϵ) by the stability of S′
0 0
(the last part in Definition 2.3) and Lemma 2.7. The second term is non-negative. The third term
has absolute value at most δρ/ϵ because (18) shows that the average value of the absolute value
of the cross terms is small. Thus, we have shown the following lower bound on the covariance: for
all subsets S′′ ⊂ S with |S′′| ≥ (1−ϵ)|S′|, it holds that (cid:10) M,Σ
S′′
−I(cid:11) ≥ −δ(cid:101)2/ϵ for some δ(cid:101)≲ δ+ρ.
4.2 Proof of Lemma 4.4
Lemma 4.4. Consider the setting in Theorem 4.2. Then for all |S′′| ≥ (1 − ϵ)|S′|, we have
√ √ √ √
W (S′,S′′) ≲ ρ+ϵ k+δ and W (S′,S′′) ≲ ϵk+δ/ ϵ+ρ/ ϵ.
1,k 0 2,k 0
Proof of Lemma 4.4. Using the triangle inequality:
W (S′,S′′) ≤ W (S′,S′)+W (S′,S′′) .
1,k 0 1,k 0 1,k
For the first term, note that S′ and S′ have the same cardinality, and for every point x ∈ S′ we have
0 i 0
exactly one point x ∈ S′ with x −x =: ∆ . Thus we can use this simple coupling in Definition 1.7
(cid:101)i i (cid:101)i i
to get this upper bound:
1 (cid:88)
W (S′,S′) ≤ max ∥∆ ∥ ≲ ρ
1,k 0
V∈V
|S′| i 2
k 0 i:i∈S′
0
√
w √here the last inequality follows by (16). For the second term we have W 1,k(S′,S′′) ≲ ϵ k+δ(cid:101)≤
ϵ k+ρ+δ by Lemma 2.8, which is applicable because S′′ is an (1−ϵ)-subset of S′ and we have
already shown in Lemma 4.3 that S′ is (ϵ,δ(cid:101),k)-generalized stable with δ(cid:101)≲ δ+ρ.
The bound for W (S′,S′′) is similar. First, W (S′,S′′) ≤ W (S′,S′)+W (S′,S′′). The
√ 2,k 0 2,k 0 √ 2,k √ 0 √ 2,k √ √
firsttermisO(ρ/ ϵ)by(16)andthesecondtermisatmostO( ϵk+δ(cid:101)/ ϵ) = O( ϵk+δ/ ϵ+ρ/ ϵ)
by Lemma 2.8.
5 Mean Estimation: Proof of Theorem 1.4
In this section, we prove the first algorithmic result of our paper (Theorem 1.4), whose proof follows
easily from Proposition 3.1, Proposition 4.1 and Theorem 4.2.
20Theorem 1.4 (Main Result for Mean Estimation). Let c be a sufficiently small positive constant and
C a sufficiently large constant. Let outlier rate ϵ ∈ (0,c) and contamination radius ρ > 0. Let S be a
0
set that is (ϵ,δ)-stable with respect to an (unknown) µ ∈ Rd, where δ > ϵ. Let T be a corrupted dataset
after ϵ-fraction of global outliers and ρ-strong local corruptions (as per Contamination Model 3). Then,
any stability-based algorithm A(T,ϵ,δ(cid:101)) executed with input T,ϵ,δ(cid:101)= C ·(δ+ρ), outputs an estimate
µ such that with high probability (over the internal randomness of the algorithm): ∥µ−µ∥ ≲ δ+ρ.
(cid:98) (cid:98) 2
Proof of Theorem 1.4. Let S = {x } be an (ϵ,δ)-stable set with respect to µ ∈ Rd. The final
0 i i∈[n]
set T is constructed by first picking S ∈ Wstrong(S ,ρ) (cf. Contamination Model 2) and then
0
T ∈ O(S,ϵ) (cf. Contamination Model 1) by the corresponding adversaries. Let ∆ denote the
i
perturbabtions of Wstrong adversary, i.e., S = {x } where x = x +∆ . By Proposition 4.1
(cid:101)i i∈[n] (cid:101)i i i
applied with z = ∆ and k = 1, there exists a subset S′ ⊂ S of size at most (1−ϵ)n for which
i i 0 0
max 1 (cid:80) ∥∆ ∥2 ≲ ρ2/ϵ. Applying Proposition 3.1 with k = 1 and y = ∆ for the set
S′,
wM e∈ hM a1 ve|S t0′ h| ati s:x ui p∈S 0′ i M
1 (cid:80) ∥∆ ∥ ≲ sup 1 (cid:80) ∥∆ ∥ ≲
ρi (whi
ere the last
0 M∈M1 |S 0′| i:xi∈S 0′ i M V∈V k |S 0′| i:xi∈S 0′ i V
inequality follows by the definition of the local perturbations model).
So far, we have established the necessary conditions in (16) for applying Theorem 4.2 with k = 1.
The condition in Theorem 4.2 that S′ is (ϵ,O(δ),1)-generalized stable is satisfied because (i) S is
0 0
(ϵ,δ) and (ii) S′ is a large subset of S (Lemma 2.7).
0 0
Theorem 4.2 guarantees that if S′ denotes the set {x +∆ : x ∈ S′} (i.e., the points in S′ after
i i i 0 0
the local perturbations), then S′ is (ϵ,δ(cid:101))-stable with respect to µ (cf. Definition 1.2), for δ(cid:101)≲ ρ+δ.
After T is chosen by the second adversary (the one associated with the global outliers), we have that
|T ∩S| ≥ (1−ϵ)|T| which implies that |T ∩S′| ≥ |T ∩S|−|S \S′| ≥ (1−2ϵ)|T|. This means that
T ∈ O(S′,2ϵ) for an (2ϵ,O(δ(cid:101)))-stable set S′ (which follows from (ϵ,δ(cid:101))-stability of S′ and Lemma 2.7),
and thus any stability-based algorithm outputs a µˆ such that ∥µˆ−µ∥
2
≲ δ(cid:101)≲ ρ+δ.
6 Distribution Learning Under Global and Local Corruptions
In this section, we prove Theorem 1.8, which yields guarantees for distribution learning in the
presence of the combined contamination model.
Theorem 1.8 (Main Result for Distribution Learning). Let ϵ ∈ (0,c) be a parameter for the outlier
rate, where c is a sufficiently small absolute constant, ρ > 0 be a parameter for the local contamination
radius, and δ > ϵ be a parameter for stability. Let S be a set that is (ϵ,δ)-stable with respect to an
0
(unknown) µ ∈ Rd. For a slicing parameter k ∈ [d], let T be the corrupted dataset after local and
global corruptions from Contamination Models 1 and 6 with parameters ρ and ϵ, respectively; formally,
T ∈ O(S,ϵ) for some S ∈ Wstrong(S ,ρ). Then, there exists a polynomial-time algorithm that on
1,k 0
input T,ϵ,ρ,δ, and k′ ∈ [k] √, outputs an estimate S(cid:98)⊂ T such that, with high constant probability, it
holds that W 1,k′(S(cid:98),S 0) ≲ δ k′+ρ.
This result also relies on the structural result of Proposition 4.1 and Theorem 4.2, provided
in Section 4. For the distribution learning result, we provide an algorithm which uses a multi-
dimensional variant of the standard iterative filtering procedure, given in Algorithm 1. Then,
leveraging Theorem 4.2, a certification lemma from [NGS24] in Section 6.1, and a now-standard
analysis of the iterative filtering procedure, we prove Theorem 1.8 in Section 6.2.
6.1 Certification of Solutions
In this section, we state the certificate lemma (from [NGS24]) that provides a way to bound the W
1,k′
distance between the current filtered version of the dataset T and the uniform distribution over the
21Algorithm 1 Distribution learning under global and strong local corruptions
Input: (Multi)-Set of samples T ⊂ Rd, and parameters k′ ∈ N,ϵ ∈ (0,c),δ ≥ ϵ,ρ ≥ 0.
√
Output: S(cid:98)⊂ Rd and µ (cid:98)∈ Rd such that ∥µ (cid:98)−µ∥
2
≲ δ+ρ and W 1,k(S(cid:98),S) ≲ δ k′+ρ.
1: Let C be a s √ufficiently large absolute constant.
2: Define δ(cid:101)= δ k′+ρ.
3: while true do
4: Compute M ∈ Rd×d that maximizes ⟨M,Σ T⟩ under the constraints 0 ⪯ M ⪯ I, tr(M) = k′.
5: if ⟨M,Σ T⟩ ≤ k′+Cδ(cid:101)2/ϵ then
Go to Line 13.
6:
7: else
8: Let L ⊂ T consisting of the ϵ|T| points with the largest score g(x) = (x−µ T)⊤M(x−µ T).
9: Define the thresholded scores τ(x) := g(x)1 x∈L for x ∈ T.
10: for each x ∈ T do
11: Delete x from T with probability τ(x)/max x∈T τ(x).
12: Let S(cid:98)← T.
13: return S(cid:98)
original inliers S . This bound is expressed as a function of a variance-like quantity of the dataset.
0
This insight informs the design of our algorithm’s stopping condition (cf. Line 5). Consequently, we
can guarantee that upon termination, if the variance is sufficiently small, the solution output by the
algorithm will be close to the uniform distribution over the inliers S in the W metric.
0 1,k′
Lemma 6.1 (Lemma 20 in [NGS24]). Let S be an (ϵ,δ)-stable set. Let S′ be any set satisfying
0
W (S′,S ) ≤ r and T be a set with |T ∩S′| ≥ (1−ϵ)|T|. Then, the following holds:9
2,k′ 0
√ √ √
W (T,S′) ≲ δ k′+r ϵ+ϵ r′ ,
1,k′
where r′ = sup ⟨M,Σ ⟩+(µ −µ )⊤M(µ −µ ).
M∈M k′ T\S′ T\S′ S′ T\S′ S′
The next result provides an upper bound for the quantity r′ appearing in Lemma 6.1 in terms
of the simpler quantity ⟨M,Σ ⟩−k. This simpler quantity acts as the stopping condition of our
T
algorithm (Line 5). In addition, the lemma below also bounds ∥µ −µ∥ , which is the error of the
T M
empirical mean over the current dataset T.
Lemma 6.2. Let k be a positive integer, M ∈ M k, and δ(cid:101) ≥ ϵ. Let S′ be a set satisfying the
(ϵ,δ(cid:101),k)-generalized-stability with respect to the vector µ ∈ Rd (cf. Definition 1.10). Let T be a set
such that |T ∩S′| ≥ (1−ϵ)|T| and denote λ := ⟨M,Σ ⟩−k. Then, the following hold:10
T
√ √
1. ∥µ
T
−µ∥
M
≲ δ(cid:101)+ϵ k+ λϵ.
2. max(cid:0) ⟨M,Σ T\S′⟩,∥µ
S′∩T
−µ T\S′∥2 M(cid:1) ≲ λ/ϵ+δ(cid:101)2/ϵ2+k.
Proof. The covariance matrix can be decomposed as follows:
Σ = (1−ϵ)Σ +ϵΣ +ϵ(1−ϵ)(µ −µ )(µ −µ )⊤ .
T S′∩T T\S′ S′∩T T\S′ S′∩T T\S′
9The following result is implied by [NGS24, Lemma 20] after using P′ = P both being equal to the uniform
distribution over S′, and Q being the uniform distribution over T.
√
10Recall that ∥x∥ = x⊤Mx denotes the Mahalanobis norm of x with respect to M.
M
22Using the decomposition above with our assumptions, we obtain
k+λ ≥ ⟨M,Σ ⟩
T
= (1−ϵ)⟨M,Σ ⟩+ϵ⟨M,Σ ⟩+ϵ(1−ϵ)(µ −µ )⊤M(µ −µ )
S′∩T T\S′ S′∩T T\S′ S′∩T T\S′
(cid:32) (cid:32) (cid:33)(cid:33)
δ(cid:101)2
≥ (1−ϵ) k−O +ϵ⟨M,Σ ⟩+ϵ(1−ϵ)(µ −µ )⊤M(µ −µ ) ,
ϵ
T\S′ S′∩T T\S′ S′∩T T\S′
(19)
where the second line is derived by the assumption that S′ is a set satisfying the (ϵ,δ(cid:101),k)-generalized-
stability as follows:
(cid:42) (cid:43)
1 (cid:88)
⟨M,Σ ⟩ = M, (x−µ )(x−µ )⊤
S′∩T
|S′∩T|
S′∩T S′∩T
x∈S′∩T
(cid:42) (cid:43)
1 (cid:88) (cid:68) (cid:69)
= M, (x−µ)(x−µ)⊤ + M,(µ−µ )(µ−µ )⊤
|S′∩T|
S′∩T S′∩T
x∈S′∩T
(cid:42) (cid:43) (cid:42) (cid:43)
1 (cid:88) 1 (cid:88)
+ M, (x−µ)(µ−µ )⊤ + M, (µ−µ )(x−µ)⊤
|S′∩T|
S′∩T
|S′∩T|
S′∩T
x∈S′∩T x∈S′∩T
(cid:42) (cid:43)
1 (cid:88)
≥ M, (x−µ)(x−µ)⊤ −(µ−µ )⊤M(µ−µ )
|S′∩T|
S′∩T S′∩T
x∈S′∩T
δ(cid:101)2 δ(cid:101)2 3δ(cid:101)2
≥ k− −2∥M∥ op∥µ−µ S′∩T∥
2
≥ k− −2δ(cid:101)≥ k− ,
ϵ ϵ ϵ
where we used that M ⪯ I, ∥µ − µ S′∩T∥
2
≲ δ(cid:101) by the stability assumption for S′ and δ(cid:101) ≥ ϵ.
Rearranging (19) yields
⟨M,Σ T\S′⟩+(1−ϵ)(µ
S′∩T
−µ T\S′)⊤M(µ
S′∩T
−µ T\S′) ≲ λ/ϵ+δ(cid:101)2/ϵ2+k . (20)
This implies that both ⟨M,Σ ⟩ and (µ −µ )⊤M(µ −µ ) are at most O(λ/ϵ+
T\S′ S′∩T T\S′ S′∩T T\S′
δ(cid:101)2/ϵ2+k), which shows the second part of our lemma. The first part of Lemma 6.2 follows simply
by the decomposition below:
∥µ −µ∥ = ∥(1−ϵ)µ +ϵµ −µ∥
T M S′∩T T\S′ M
≤ ∥µ −µ∥ +ϵ∥µ −µ ∥
S′∩T M S′∩T T\S′ M
√ √
≲ δ(cid:101)+ϵ k+ λϵ .
whereweusedthegeneralized-stabilityassumptiontoboundthefirstterm,and(µ −µ )⊤M(µ −
S′∩T T\S′ S′∩T
µ T\S′) = O(λ/ϵ+δ(cid:101)2/ϵ2+k) from earlier to bound the second term.
6.2 Filtering Scores
In this subsection, we show that the scores τ(x) used in Algorithm 1 remove more outliers than
inliers in expectation in each round.
23Lemma 6.3 (Analysis of One Round of Filtering: Scores of Outliers > Scores of Inliers). Let S′ be a
multi-set of Rd satisfying (ϵ,δ(cid:101),k)-generalized-stability with respec √t to µ ∈ Rd. Assume the following:
ϵ ∈ (0,c) for a sufficiently small absolute constant c, and δ(cid:101)≥ ϵ k. Let T be a multiset such that
|T ∩ S′| ≥ (1 − 20ϵ). Let C be a sufficiently large absolute constant. Let τ(x) be the scores as
defined in Line 9 of Algorithm 1, i.e., g(x) := ∥x−µ ∥2 , L is the set of points in T with the ϵ·|T|
T M
largest scores g(x), and τ(x) := g(x)1 . If M ∈ M is a matrix with ⟨M,Σ ⟩ > k+Cδ(cid:101)2/ϵ, then
x∈L k T
(cid:80) (cid:80)
τ(x) ≤ 0.1 τ(x).
x∈S′∩T x∈T
Proof. Denote λ := ⟨M,Σ ⟩−k. For the inlier points, we have the following (explanations are
T
provided after the inequalities):
(cid:88) (cid:88) (cid:88)
τ(x) = g(x) = ∥x−µ ∥2 (21)
T M
x∈S′∩T x∈S′∩L x∈S′∩L
(cid:88)
≤ 2 (x−µ)⊤M(x−µ)+2|S′∩L|·∥µ−µ ∥2 (22)
T M
x∈S′∩L
≲ |S′|δ(cid:101)2/ϵ+ϵ|S′|(δ(cid:101)2+ϵ2k+ϵλ) ≤ 0.01λ|S′| , (23)
where the steps used were the following: (22) used the triangle inequality combined with the
inequality (a+b)2 ≤ 2a2 +2b2, the first term in (23) was bounded using the third condition in
Definition 2.4 (and our assumption that S′ satisfies the generalized stability condition), the second
term in (23) used that ∥µ−µ T∥2
M
≲ δ(cid:101)2+ϵ2k+ϵλ by the certificate lemma (Lemma 6.2), and we
also used that |S′∩L| ≤ |L| = ϵ|T| ≤ ϵ |S′|. The last inequality in (23) uses our assumptions
√ 1−Ω(ϵ)
λ ≥ Cδ(cid:101)2/ϵ, δ(cid:101)≥ ϵ, δ(cid:101)≥ ϵ k, C ≫ 1, ϵ ≪ 1.
We now show the lower bound for the sum of the scores over all points:
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
τ(x) = g(x) ≥ g(x) ≥ g(x)− g(x) . (24)
x∈T x∈T∩L x∈T\S′ x∈T x∈S′∩T
where the second step above is based on the fact that |T \S′| ≤ ϵ|T| and that L is defined to be the
points with the largest ϵ|T| scores. For the first term, we have that, by definition:
(cid:88)
g(x) = ⟨M,Σ ⟩ = (k+λ)|T| ≥ (k+λ)(1−ϵ)|S′| . (25)
T
x∈T
For the second term in the RHS of (24), we have the following:
(cid:88) (cid:88) (cid:88)
g(x) ≤ g(x) = (x−µ )⊤M(x−µ )
T T
x∈S′∩T x∈S′ x∈S′
(cid:88) (cid:88)
= (x−µ)⊤M(x−µ)+|S′|(µ−µ )⊤M(µ−µ )+2 (x−µ)⊤M(µ−µ ) .
T T T
x∈S′ x∈S′
(26)
The first term is at most (k+δ(cid:101)2/ϵ)|S′| by our generalized-stability assumption. The second term is
at most |S′|(δ(cid:101)2+ϵ2k+ϵλ) by Lemma 6.2. For the third term, we have that
1 (cid:88)
(x−µ )⊤M(µ−µ ) = (µ −µ )⊤M(µ−µ ) ≤ ∥µ −µ ∥ ∥M∥ ∥µ−µ ∥
|S′| T T S′ T T S′ T 2 op T 2
x∈S′
≲ (∥µ−µ S′∥ 2+∥µ−µ T∥ 2)∥µ−µ T∥
2
≲ δ(cid:101)2+ϵ2k+λϵ , (27)
24where we used that ∥M∥ ≤ 1, and then we applied the triangle inequality and Lemma 6.2. Putting
op
(24)-(27) together, we have that
(cid:88)
τ(x) ≥ (k+λ)(1−ϵ)|S′|−(k+δ(cid:101)2+δ(cid:101)2/ϵ+ϵ2k+λϵ)|S′|
x∈T
√
≥ (k+λ)(1−ϵ)|S′|−(k+0.001λ)|S′| (using λ ≥ Cδ(cid:101)2/ϵ, δ(cid:101)≥ ϵ, δ(cid:101)≥ ϵ k, C ≫ 1, ϵ ≪ 1)
≥ (λ−ϵk−λϵ−0.001λ)|S′|
≥ 0.9λ|S′| . (using ϵ ≪ 1, λ ≥ Cδ(cid:101)2/ϵ ≥ Cϵk)
Combining with (23) concludes the proof of this lemma.
6.3 Proof of Theorem 1.8
We are now ready to combine the previous components to conclude the analysis of our algorithm
and complete the proof of Theorem 1.8.
Proof of Theorem 1.8. We use k′ = k without loss of generality, as the same arguments go through
for any other k′ ≤ k by noting that the local corruptions adversary is stronger for k′ ≤ k.
We briefly recall the notation. As in the theorem statement, S = {x ,...,x } is the original
0 1 n
set of inliers (before any kind of corruptions), which is assumed to satisfy the stability conditions,
S = {x +∆ } is the set after the strong Wasserstein corruptions of Contamination Model 6,
i i i∈[n]
(∆ ’s denote the shift that each point undergoes), and T is the final dataset after globally corrupting
i
S (Contamination Model 1).
ThesetS = {x ,...,x }is(ϵ,δ)-stablebyassumption. UsingLemma2.6italsosatisfies(ϵ,δ′,k)-
0 1 n √
generalized stability, with δ′ ≲ kδ. By Proposition 4.1 we have the existence of a set S′ ⊂ S with
0 0
|S′| ≥ (1−ϵ)|S | such that max 1 (cid:80) ∥∆ ∥2 ≲ ρ2/ϵ. By Proposition 3.1 we have that
th0
e same set
S0
′ also satisfies
mM a∈ xM k |S 0′|
1
i: (cid:80)xi∈S 0′ i ∥∆M
∥ ≲ max 1 (cid:80) ∥∆ ∥ ≲ ρ
0 M∈M k |S 0′| i:xi∈S 0′ i M V∈V k |S 0′| i:xi∈S 0′ i V
(where the last inequality is by definition of our local perturbation model).
By the above discussion Theorem 4.2 is applicable, stating that if S′ is defined to be S′ d =ef
√
{x i+∆
i
: x
i
∈ S 0′},thenS′ satisfies(ϵ,δ(cid:101),k)-generalized-stabilitywithrespecttoµ,forδ(cid:101)= O(δ k+ρ).
Moreover, for any (1−ϵ)|S′| sized subset S′′ of S′ it holds that
√ √
W 1,k(S 0′,S′′) ≲ ρ+ϵ k+δ(cid:101)≲ ρ+δ k
√ √ √ √ √
and W 2,k(S 0′,S′′) ≲ ϵk+δ(cid:101)/ ϵ+ρ/ ϵ ≲ ( kδ+ρ)/ ϵ, (28)
where we used that δ ≥ ϵ.
We now argue that filtering does not remove too many “stable inliers” (T ∩S′) throughout its
execution. Lemma 6.3 states that, as long as the main while loop of the algorithm has not been
terminated, the scores τ(x) that the algorithm assigns to inlier points in T is substantially bigger (at
leastbyaconstantfactor)thantheonesforoutlierpoints. Followingthestandardanalysisoffiltering
algorithms in [DK23], we obtain that with probability at least 9/10, we have that |T△S′| ≤ 20ϵ
throughout the execution.
Let S(cid:98) denote the set T at the end of the filtering algorithm and condition on the high probability
event that satisfies |S(cid:98)∩S′| ≤ 20ϵ and ⟨M,Σ S(cid:98)⟩ ≤ k+Cδ(cid:101)2/ϵ for all M ∈ M k. To apply Lemma 6.1,
we need a bound on W (S ,S′), which we obtain below:
2,k′ 0
√
(cid:112) (cid:112)
W (S ,S′) ≤ W (S ,S′)+W (S′,S′) ≤ kϵ+δ k/ϵ+ ρ2/ϵ,
2,k 0 2,k 0 0 2,k 0
25where use Lemma 2.8 for the first term and (28) for the second term (with S′′ = S′). With this
bound on W (S′,S′), applying Lemma 6.1 yields
2,k 0
√ √
W 1,k(S(cid:98),S′) ≲ δ k+ρ+ϵ r′ ,
where r′ is defined in Lemma 6.1. To upper bound r′, we apply Lemma 6.2 with T = S(cid:98) and ϵ′ = 20ϵ
in place of the parameter ϵ appearing in the statement of that lemma. This gives that
r′ = sup ⟨M,Σ ⟩+∥µ −µ ∥2
S(cid:98)\S′ S(cid:98)\S′ S′ M
M∈M
k
≲ sup ⟨M,Σ ⟩+∥µ −µ ∥2 +∥µ −µ ∥2
S(cid:98)\S′ S(cid:98)\S′ S′∩S(cid:98) M S′∩S(cid:98) S′ M
M∈M
k
≲ δ(cid:101)2/ϵ2+k+∥µ S′∩S(cid:98)−µ S′∥2
M
≲ δ(cid:101)2/ϵ2 .
where the last line used ∥µ S′∩S(cid:98)−µ S′∥2
M
≲ ∥µ S′∩S(cid:98)− √µ∥2 M+∥µ−µ S′∥2
M
≲ δ(cid:101)2/ϵ2 by the generalized
stability of S′ (we √also used that δ(cid:101) ≥ ϵ and δ(cid:101) ≥ ϵ k). Plugging this back, we obtain a bound
of W 1,k(S(cid:98),S′) ≲ δ k +ρ. We can translate this into a bound for W 1,k(S 0,S(cid:98)) using the triangle
inequality as follows:
W 1,k(S 0,S(cid:98)) ≤ W 1,k(S 0,S 0′)+W 1,k(S 0′,S′)+W 1,k(S′,S(cid:98)).
√
The first term above is upper bounded by ϵ k+δ by Lemma 2.8, the second term is upper bounded
√
by ρ by the definition of local contamination, and the last term was shown to be at most δ k+ρ.
Combining these three terms yields the desired result.
Runtime Note that the algorithm removes at least one point per iteration and the set S′ ∩T
satisfies the stopping condition of Line 5. This is because of stability of S′ and therefore stability of
any large subset of S′ (cf. Lemma 2.7). This means that the algorithm will terminate after O(n)
iterations. In each iteration, the algorithm requires solving an SDP (Line 4), which can be done in
polynomial time by ellipsoid method or interior point method [Nes04].
7 Principal Component Analysis
In this section, we present our result for robust principal component analsysis (PCA) in Theorem 7.4
below. The goal for PCA is to output a high-variance direction v of the unknown covariance Σ in the
following sense: v⊤Σv ≥ (1−γ)∥Σ∥ for γ as small as possible. Under the global contamination
op
model, robust PCA algorithms have been developed in [KSKO20; JLT20; DKPP23; JKLPPT24].
We will work with zero-mean distributions (for inliers) in this section, which is without loss of
generality because one can always reduce to this setting by taking differences of pairs of samples.
We state an appropriate version of the stability condition which is more relevant to PCA.
Definition 7.1 (PCA Stability). Let 0 < ϵ ≤ γ. A finite multiset S ⊂ Rd is called (ϵ,γ)-PCA-stable
with respect to a PSD matrix Σ ∈ Rd×d if for every S′ ⊆ S with |S′| ≥ (1−ϵ)|S|, the following holds:
(1−γ)Σ ⪯ 1 (cid:80) xx⊤ ⪯ (1+γ)Σ.
|S′| x∈S′
ThedefinitionaboveisverycloselyrelatedtoDefinition1.10asshownbythefollowingobservation.
26Fact 7.2. Let Σ be a positive definite matrix. If a set of samples {Σ−1/2x } is (ϵ,δ)-stable
i i∈[n]
(Definition 1.2) with respect to µ = 0 (Definition 1.10), then {x } is (ϵ,γ)-PCA-stable with
i i∈[n]
respect to Σ (Definition 7.1) for γ ≲ δ2/ϵ.
Using the connection above, it can be seen that the stability definition above is satisfied by many
distribution families of interest. Similarly, a set of i.i.d. samples from such distributions continue to
satisfy this definition with high probability [JLT20; JKLPPT24]. Consequently, the stability-based
algorithms obtain the state of the art results for robust PCA for many distribution families [JLT20;
DKPP23; JKLPPT24].
Definition 7.3 (Stability-based Algorithms for PCA). Let S be an (ϵ,γ)-PCA-stable set with
respect to an (unknown) PSD matrix Σ ∈ Rd×d (Definition 7.1). Let T be any set in O(S,ϵ) (cf.
Contamination Model 1). We call an algorithm stability-based PCA-algorithm if it takes as an input
T, ϵ, and γ, and outputs a unit vector v ∈ Rd in polynomial time such that v⊤Σv ≥ (1−O(γ))∥Σ∥ .
op
For this section, we consider a version of the contamination model where local corruptions are
introduced to the data after whitening.
Contamination Model 7 (Strong Local Contamination After Whitening). Let ρ ≥ 0. Let
S = {x ,...,x } be an n-sized set in Rd and Σ ∈ Rd×d be a PSD matrix. Consider an adversary
0 1 n
thatperturbseachpointx tox withtheonlyrestrictionthatineachdirection, theaverageperturbation
i (cid:101)i
is at most ρ. Formally, we define
 
Wstrong(S 0,ρ,Σ) :=  S = {x (cid:101)1,...,x (cid:101)n} ⊂ Rd : sup 1 (cid:88) (cid:12) (cid:12)v⊤Σ−1/2(x (cid:101)i−x i)(cid:12) (cid:12) ≤ ρ .
n
 v∈Rd:∥v∥2=1
i∈[n]

The adversary returns an arbitrary set S ∈ Wstrong(S ,ρ,Σ) after possibly reordering the points.
0
As a remark, we note that Contamination Model 7 and Contamination Model 2 are equivalent to
√ √
each other, as long as the matrix Σ is well-conditioned. In particular, ρ ≤ ρ/ λ and ρ ≤ ρ λ
min max
where λ ,λ denote the largest and smallest eigenvalues of Σ respectively. The appealing
max min
property of the whitened local perturbations is that (i) it allows the amount of local perturbations
to increase in high-variance directions, and (ii) it decouples the local contamination parameter ρ
from the scale of the covariance matrix Σ.11
Theorem 7.4. Let c be a sufficiently small positive constant and C a sufficiently large constant. Let
√
outlier rate ϵ ∈ (0,c) and contamination radius ρ ∈ (0, ϵ). Let S be a set of samples satisfying (ϵ,γ)-
0
PCA-stability with respect to a PSD matrix Σ ∈ Rd×d (Definition 7.1). Let T be a corrupted dataset
after ϵ-fraction of outliers and ρ-strong local corruptions after whitening (as per Contamination
Model 7). Then, any stability-based PCA algorithm (Definition 7.3) on input T,ϵ,γ = C ·(γ +
ρ2
),
(cid:101) ϵ
outputs a unit vector v such that with high probability (over the internal randomness of the algorithm):
(cid:16) (cid:16) (cid:17)(cid:17)
v⊤Σv ≥ 1−O γ + ρ2 ∥Σ∥ .
ϵ op
Proof. For a matrix A ∈ Rd×d and set S ⊂ Rd, we use A[S] to denote the set {Ax : x ∈ S}. Let
S be the set after the local perturbations of S (as per Contamination Model 7). It suffices to
0
11Indeed, it can be seen that the range of parameter ρ where robust PCA is non-trivial depends on the scale of
Σ. For example, consider the case when the inlier distribution is N(0,σ2(I+vv⊤)) for a unit vector v and a scalar
σ2. Then the 2-Wasserstein distance between N(0,σ2(I+vv⊤)) and N(0,σ2I) is Θ(σ). Hence, for ρ≳σ, the local
adversary can simulate samples from an isotropic distribution, removing any signal from the direction of interest v.
On the other hand, as shown in Theorem 7.4, the range of ρ does not depend on Σ.
27show that S contains a subset S′ ⊂ S such that |S′| ≥ (1−ϵ) and S′ is (ϵ,γ)-PCA stable with
(cid:101)
respect to Σ for γ ≲ γ +ρ2/ϵ. By Fact 7.2, it suffices to show that the whitened data Σ−1/2[S]
(cid:101)
contains a large subset S′ that is (ϵ,γ)-PCA-stable with respect to I. Leveraging the connections
(cid:101)
between PCA-stability and the usual stability (Definition 1.10), it suffices to show that S′ satisfies
√
the conditions pertaining to the second moment of (ϵ, ϵγ+ρ)-stability (with respect to µ = 0). The
existence of a large S′ with the desired stability can be shown by following the proof in Theorem 4.2
√
mutatis mutandis for k = 1 and δ = ϵγ.12
Finally, we briefly mention how to generalize Theorem 7.4 to k-robust PCA for k > 1. Observe
that in the proof above, we have shown that S contains a large subset that is (ϵ,γ)-PCA-stable for
(cid:101)
γ ≲ γ +ρ2/ϵ. Generalization to k > 1 then follows directly from [JKLPPT24, Corollary 3].
(cid:101)
8 Sum-of-squares Based Algorithm: Proof of Theorem 1.6
In this section, we prove the result on mean estimation under the combined contamination model for
distributions with certifiably bounded moments in the sum-of-squares (SoS) proof system. We show
that the approach of [KSS18; HL18] extends to the contamination model of this paper.
We refer the reader to [BS16; FKP19] for the necessary definitions of the terms such as degree-d
SoS proofs and pseudoexpectations. We list only a few basic facts that we use and refer the reader
to the aforementioned references for the full background.
Fact 8.1 (Cauchy-Schwarz for Pseudoexpectations). Let f,g be polynomials of degree at most t.
(cid:113) (cid:113)
Then, for any degree-2t pseudoexpectation E(cid:101), E(cid:101)[fg] ≤ E(cid:101)[f2] E(cid:101)[g2]. Consequently, for every
squared polynomial p of degree t, and k a power of two, E(cid:101)[pk] ≥ (E(cid:101)[p])k for every E(cid:101) of degree-2tk.
Fact 8.2 (SoS Triangle Inequality). If k is a power of two, a1,a2,...,an (cid:110) ((cid:80) a )k ≤ nk(cid:0)(cid:80) ak(cid:1)(cid:111) .
k i i i i
Fact 8.3 (SoS Cauchy-Schwartz and Hölder). Let f ,g ,...,f ,g be indeterminates over R. Then,
1 1 n n
 
(cid:32) n (cid:33)2 (cid:32) n (cid:33)(cid:32) n (cid:33)
f1,...,fn,g1,...,gn  1 (cid:88) f g ≤ 1 (cid:88) f2 1 (cid:88) g2  .
2 n i i n i n i
 
i=1 i=1 i=1
Moveover, if p ,...,p are indeterminates, for any t ∈ Z that is a power of 2, we have that
1 n +
(cid:32) (cid:33)t
 t−1
{w i2 = w i | i ∈ [n]} p1 O,.. (. t, )pn (cid:88) w ip i ≤ (cid:88) w i · (cid:88) pt i and
i i∈[n] i∈[n]
(cid:32) (cid:33)t
 t−1
{w i2 = w i | i ∈ [n]} p1 O,.. (. t, )pn (cid:88) w ip i ≤ (cid:88) w i · (cid:88) w ipt i .
i i∈[n] i∈[n]
Definition 8.4 (Certifiably Bounded Moments). For an even t ∈ N, we say a distribution P with
mean µ over Rd has (t,M)-certifiably bounded moments if the polynomial inequality p(v) ≥ 0 for
P
p(v) := Mt−E [⟨v,X −µ ⟩t] has an SoS proof of degree O(t) under the assumption ∥v∥2 = 1.
X∼P P 2
If S is a set of points in Rd, we say that S has (t,M)-certifiably bounded moments if the uniform
distribution over S satisfies the previous definition.
12Infact,ifwemakethestrongerassumptioninthetheoremthatΣ−1/2[S ]is(ϵ,δ,1)-generalizedstable(asopposed
0
to PCA stable), then the desired conclusion follows as a direct corollary from Theorem 4.2 and Fact 7.2.
28Many distribution families, such as rotationally invariant distributions, t-wise product distribu-
tions with bounded moments, and Poincare distributions are known to be certifiably bounded; see,
for example, [KSS18].
Theorem 8.5. Let ϵ ∈ (0,c) for a sufficiently small absolute constant c. Let S be a set of n points in
Rd with (unknown) mean µ. Further assume that the uniform distribution on S has (t,M)-certifiably
bounded moments for t being a power of 2. Let T be the version of the dataset S after introducing
ϵ-fraction of global outliers and ρ-strong local corruptions (as per Contamination Model 3). Then,
there exists an algorithm that takes as input T,ρ,ϵ,M, and t, runs in time poly(nt,dt2), and returns
µ such that with probability at least 0.9, it holds ∥µ−µ∥ ≲ Mϵ1−1/t+ρ.
(cid:98) (cid:98) 2
Let S = {x ,...,x } be the original dataset of inliers. Let S′ = {x′,...,x′ } with x′ = x +z
1 n 1 n i i i
such that ∀v ∈ Sd−1: E [|⟨v,z ⟩|] ≤ ρ be the dataset after the local corruptions (we use the
i∼[n] i
notation E to denote taking the average over i ∈ [n], for example, E [x ] = 1 (cid:80) x ).
i∼[n] i∼[n] i n i∈[n] i
Finally, let T = {y ,...,y } be the final dataset after the global corruptions, i.e., T is such that for
1 n
all but ϵn of the points we have x′ = y . Let I ⊂ [n] denote the set of indices such that x′ = y .
i i i i
The algorithm is the following: First, it finds a pseudoexpectation E(cid:101) over (i) d-dimensional
variables (y )n ,(x )n ,(z )n , µ, (ii) scalar variables (w )n , and (iii) appropriate auxiliary
(cid:101)i i=1 (cid:101)i i=1 (cid:101)i i=1 (cid:101) (cid:101)i i=1
variables,13 under the constraints that E(cid:101) satisfies the following set of polynomial (in)equalities for a
large enough absolute constant C:
(I.i) For all i ∈ [n]: w2 = w .
(cid:101)i (cid:101)i
(I.ii) For all i ∈ [n]: w y = w y .
(cid:101)i(cid:101)i (cid:101)i i
(I.iii) (cid:80)n w ≥ (1−2ϵ)n.
i=1 (cid:101)i
(I.iv) For all i ∈ [n]: y = x +z .
(cid:101)i (cid:101)i (cid:101)i
(I.v) µ = 1 (cid:80)n y .
(cid:101) n i=1(cid:101)i
(I.vi) There exists an SoS proof in the variable v of the inequality E [⟨v,x −µ⟩t] ≤ Ct(cid:0) Mt+ρt(cid:1)
i∼[n] (cid:101)i (cid:101)
under the constraints ∥v∥2 = 1.
2
(I.vii) There exists an SoS proof in the variable v of the inequality E [⟨v,z ⟩2] ≤ Cρ2/ϵ under the
i∼[n] (cid:101)i
constraints ∥v∥2 = 1.
2
Finally, the algorithm outputs µ (cid:98)= E(cid:101)[µ (cid:101)].
8.1 Proof of Theorem 8.5
If z := x′ − x denote the local perturbations, then by Proposition 1.11, we know that there
i i i
exists a subset of indices I′ ⊂ [n] with |I′| ≥ (1−ϵ)n such that E [⟨v,z ⟩2] ≲ ρ2/ϵ. Without
i∼I′ i
loss of generality, we can treat the remaining points i ∈ [n]\I′ as outliers. This is why we use
1−2ϵ in the right hand side of Constraint (I.iii). Thus, throughout this proof, we assume that
E [⟨v,z ⟩2] ≲ ρ2/ϵ and that we have 2ϵ of outliers (i.e., the set I of indices i ∈ [n] with x′ = y
i∼[n] i i i
has size |I| ≥ (1−2ϵ)n).
13These are needed for encoding the constraints Constraints (I.vi) and (I.vii). We refer the reader to [HL18; KS17]
for further details on how to encode these constraints using auxiliary variables.
29Satisfiability. We first argue that the system of polynomial inequalities above is satisfiable. Recall
the notation for S,x ,x′,z ,y ,T provided after the statement of Theorem 8.5. Let µ = 1 (cid:80) x .
i i i i n i∈[n] i
To show satisfiability, we make the following choice of the variables: x = x for i ∈ I and x = µ
(cid:101)i i (cid:101)i
otherwise, y = y for i ∈ I and y = µ otherwise, w = 1 , and we choose z = y −x (recall that
(cid:101)i i i (cid:101)i i∈I (cid:101)i (cid:101)i (cid:101)i
I is the set of indices i such that x′ = y ).
i i
Under these choices, Constraints (I.ii) and (I.iv) are satisfied trivially. Moreover, the w ’s satisfy
(cid:101)i
the Constraints (I.i) and (I.iii) because |I| ≥ (1−2ϵ)n.
We now argue that the Constraint (I.vi) is also satisfiable. First, define µ = 1 (cid:80) y and
(cid:101) n i∈[n](cid:101)i
µ′ = 1 (cid:80) x , and observe that under the above choices of y and x , we see that ∥µ−µ′∥ ≲ ρ;
(cid:101) n i∈[n](cid:101)i (cid:101)i (cid:101)i (cid:101) (cid:101) 2
this is because ∥µ−µ′∥ ≤ ∥(cid:80) z /n∥ ≤ ρ. Moreover, by SoS Cauchy-Schwarz inequality, there
(cid:101) (cid:101) 2 i∈[n] i
exists an O(t)-degree SoS proof of the following inequality in the variable v under the constraint
∥v∥2 = 1: ⟨v,µ′−µ⟩t ≲ ∥µ′−µ∥t ≤ ρt. Applying the SoS triangle inequality Fact 8.2, we obtain
2 (cid:101) (cid:101) (cid:101) (cid:101) 2
that the following inequality has an O(t)-degree sum of squares proof in the variable v:
E [⟨v,x −µ⟩t] ≤ 2t E [⟨v,x −µ′⟩t]+2t⟨v,µ′−µ⟩t ≲ 2t(Mt+ρt) .
(cid:101)i (cid:101) (cid:101)i (cid:101) (cid:101) (cid:101)
i∼[n] i∼[n]
Thus, Constraint (I.vi) is satisfiable.
By Proposition 1.11, z have bounded covariance, which is equivalent to a degree two polynomial
(cid:101)i
inequality in the variable v, and since it is a degree-two polynomial, it also has a sum of squares
proof in the variable v, satisfying Constraint (I.vii).14 Therefore, all the constraints in our program
are satisfied by this construction.
Correctness. Fixadirectionv ∈ Sd−1. Letµ = n1 (cid:80) i∈[n]x i. Wewillshowthat⟨µ (cid:98)−µ,v⟩ = E(cid:101)[⟨µ (cid:101)−
µ,v⟩] ≤ τ for τ = O(Mϵ1−1/t) for any pseudoexpectation E(cid:101) satisfying the program Constraints (I.i)
to (I.vii). By duality between pseudoexpectations and SoS proofs, it suffices to show that there is an
SoS proof of ⟨µ−µ,v⟩ ≤ τ under the polynomial constraints above.
(cid:101)
Let r = 1 be the locally corrupted inliers and W = w r be the variables corresponding to
i i∈I i (cid:101)i i
the surviving inliers “selected” by the program. Let W′ = (1−W ). Then there is an SoS proof of
i i
(W′)2 = W′ and (cid:80) W′ ≤ 3ϵn (with proof similar to Claim 4.3 in [DKKPP22]). Therefore, under
i i i i
the constraints above we have the following (recall that we use the notation E [x ] to denote the
i∼[n] i
average 1 (cid:80) x ):
n i∈[n] i
(cid:18) (cid:19)2t (cid:18) (cid:19)2t
⟨v,µ−µ⟩2t = E [⟨v,y −x ⟩] = E (cid:2) ⟨v,y −x′⟩(cid:3) + E [⟨v,z ⟩]
(cid:101) (cid:101)i i (cid:101)i i i
i∼[n] i∼[n] i∼[n]
(cid:18) (cid:19)2t (cid:18) (cid:19)2t
≤ 22t E (cid:2) ⟨v,y −x′⟩(cid:3) +22t E [⟨v,z ⟩]
(cid:101)i i i
i∼[n] i∼[n]
(cid:18) (cid:19)2t (cid:18) (cid:19)2t
= 22t E (cid:2) W′⟨v,y −x′⟩(cid:3) +22t E [⟨v,z ⟩] .
i (cid:101)i i i
i∼[n] i∼[n]
where we used the SoS triangle inequality (Fact 8.2) and that y W = x′W . The last term, which
(cid:101)i i i i
does not depend on the program variables, is at most (2ρ)2t by assumption. We now focus on the
first term. By using Constraint (I.iv) and another application of the SoS triangle inequality:
(cid:18) (cid:19)2t (cid:18) (cid:19)2t
E (cid:2) W′⟨v,y −x′⟩(cid:3) = E (cid:2) W′⟨v,x +z −x′⟩(cid:3)
i (cid:101)i i i (cid:101)i (cid:101)i i
i∼[n] i∼[n]
14Formally, we can replace this constraint by an equivalent constraint, E z z⊤ = (ρ2/ϵ)I −BB⊤, for some
i∼[n](cid:101)i(cid:101)i
auxiliary matrix variable B ∈Rd×d.
30(cid:18) (cid:19)2t (cid:18) (cid:19)2t
≤ 22t E (cid:2) W′⟨v,x −x ⟩(cid:3) +22t E (cid:2) W′⟨v,z −z ⟩(cid:3) . (29)
i (cid:101)i i i (cid:101)i i
i∼[n] i∼[n]
We shall use different assumptions on x and z to handle these terms differently. For the first term,
(cid:101)i (cid:101)i
we use the SoS Hölder inequality (Fact 8.3) and the constraints that W′2 = W with E W′ ≤ 3ϵ
i i i∼[n] i
(within the SoS proof system) to get the following:
(cid:18) (cid:19)2t (cid:18) (cid:19)2t−2(cid:18) (cid:19)2
E (cid:2) W′⟨v,x −x ⟩(cid:3) ≤ E [W′] E (cid:2) ⟨v,x −x ⟩t(cid:3)
i (cid:101)i i i (cid:101)i i
i∼[n] i∈[n] i∼[n]
(cid:18) (cid:19)2
≤ (3ϵ)2t−2 E (cid:2) ⟨v,x −x ⟩t(cid:3) (30)
(cid:101)i i
i∼[n]
To control the right hand side above, we further have the following inequalities (in the SoS proof
system): Starting with the SoS triangle inequality, we obtain
(cid:18) (cid:19)2 (cid:18) (cid:19)
E (cid:2) ⟨v,x −x ⟩t(cid:3) ≲ 2t E (cid:2) ⟨v,x −µ⟩t(cid:3)2 +⟨v,µ−µ⟩2t+ E (cid:2) ⟨v,x −µ⟩t(cid:3)2
(cid:101)i i (cid:101)i (cid:101) (cid:101) i
i∼[n] i∼[n] i∼[n]
≲ 2t(cid:0) M2t+ρ2t+⟨v,µ−µ⟩2t+M2t(cid:1) , (31)
(cid:101)
where we used Constraint (I.vi) and the moment assumption on the inliers. Combining (30) and
(31) we get that E (cid:2) (W′⟨v,x −x ⟩)2t(cid:3)2 ≲ (Cϵ)2t−2⟨v,µ−µ⟩2t+(Cϵ)2tM2t+(Cρ)2t.
i∼[n] i (cid:101)i i (cid:101)
We now move to the second term in (29). We apply the SoS Hölder inequality to get
(cid:18) (cid:19)2 (cid:18) (cid:19)
E (cid:2) W′⟨v,z −z ⟩(cid:3) ≤ E [W′] E (cid:2) ⟨v,z −z ⟩2(cid:3)
i (cid:101)i i i (cid:101)i i
i∼[n] i∼[n] i∼[n]
(cid:18) (cid:19)
≲ ϵ E (cid:2) ⟨v,z ⟩2(cid:3) + E (cid:2) ⟨v,z ⟩2(cid:3)
i (cid:101)i
i∼[n] i∼[n]
≲ ϵ(ρ2/ϵ+ρ2/ϵ) = O(ρ2) .
where the last line uses Constraint (I.vii) and our assumption for the inliers. Combining everything,
we have shown an SoS proof of
⟨v,µ−µ⟩2t ≲ (Cϵ)2t−2⟨v,µ−µ⟩2t+(Cϵ)2t−2M2t+(Cρ)2t .
(cid:101) (cid:101)
for some constant absolute C. By solving for ⟨v,µ−µ⟩2t, and using that ϵ < c for a sufficiently
(cid:101)
small constant, we get ⟨v,µ−µ⟩2t ≲ (Cϵ)2t−2M2t+(Cρ)2t. Finally, by SoS Hölder inequality, we
(cid:101)
have that ⟨v,µ−µ⟩ ≲ (Cϵ)1−1/tM +ρ.
(cid:101)
Acknowledgments
We thank Po-Ling Loh for helpful comments and discussion on prior work.
References
[BCF20] Y.Balaji,R.Chellappa,andS.Feizi.“RobustOptimalTransportwithApplications
in Generative Modeling and Domain Adaptation”. Advances in Neural Information
Processing Systems 33 (NeurIPS). 2020. [9]
31[BNJT10] M. Barreno, B. Nelson, A. D. Joseph, and J. D. Tygar. “The security of machine
learning”. Machine Learning (2010). [2]
[BNL12] B. Biggio, B. Nelson, and P. Laskov. “Poisoning Attacks against Support Vector
Machines”. Proc. 29th International Conference on Machine Learning (ICML).
2012. [2]
[Boe24] M. T. Boedihardjo. “Sharp bounds for the max-sliced Wasserstein distance”. arXiv
preprint arXiv:2403.00666 (2024). [8, 9]
[BPD18] A.Bora,E.Price,andA.G.Dimakis.“AmbientGAN:Generativemodelsfromlossy
measurements”. Proc. 6th International Conference on Learning Representations
(ICLR). 2018. [9]
[BS16] B. Barak and D. Steurer. “Proofs, beliefs, and algorithms through the lens of
sum-of-squares”. 1 (2016). [28]
[Cat12] OlivierCatoni.“Challengingtheempiricalmeanandempiricalvariance:adeviation
study”. Annales de l’IHP Probabilités et statistiques. Vol. 48. 4. 2012. [36]
[CD23] P.ChaoandE.Dobriban.“StatisticalEstimationUnderDistributionShift:Wasser-
steinPerturbationsandMinimaxTheory”.arXiv preprint arXiv:2308.01853 (2023).
[2, 8, 9]
[CDG19] Y. Cheng, I. Diakonikolas, and R. Ge. “High-Dimensional Robust Mean Estimation
in Nearly-Linear Time”. Proc. 30th Annual Symposium on Discrete Algorithms
(SODA). SIAM, 2019. [6]
[CDGS20] Y.Cheng,I.Diakonikolas,R.Ge,andM.Soltanolkotabi.“High-DimensionalRobust
Mean Estimation via Gradient Descent”. Proc. 37th International Conference on
Machine Learning (ICML). 2020. [6]
[CNR24] S. Chewi, J. Niles-Weed, and P. Rigollet. “Statistical optimal transport”. arXiv
preprint arXiv:2407.18163 (2024). [9]
[DHL19] Y. Dong, S. B. Hopkins, and J. Li. “Quantum Entropy Scoring for Fast Robust
MeanEstimationandImprovedOutlierDetection”.AdvancesinNeuralInformation
Processing Systems 32 (NeurIPS). 2019. [6]
[DK23] I. Diakonikolas and D. M. Kane. Algorithmic High-Dimensional Robust Statistics.
Cambridge University Press, 2023. [2, 5, 8, 13, 25, 36]
[DKKLMS16] I.Diakonikolas,G.Kamath,D.M.Kane,J.Li,A.Moitra,andA.Stewart.“Robust
Estimators in High Dimensions without the Computational Intractability”. Proc.
57th IEEE Symposium on Foundations of Computer Science (FOCS). 2016. [2, 6]
[DKKPP22] I. Diakonikolas, D. M. Kane, S. Karmalkar, A. Pensia, and T. Pittas. “Robust
Sparse Mean Estimation via Sum of Squares”. Proc. 35th Annual Conference on
Learning Theory (COLT). 2022. [7, 30]
[DKP20] I. Diakonikolas, D. M. Kane, and A. Pensia. “Outlier Robust Mean Estimation
with Subgaussian Rates via Stability”. Advances in Neural Information Processing
Systems 33 (NeurIPS). 2020. [6–8, 11, 13, 36]
[DKPP22] I. Diakonikolas, D. M. Kane, A. Pensia, and T. Pittas. “Streaming Algorithms
for High-Dimensional Robust Statistics”. Proc. 39th International Conference on
Machine Learning (ICML). 2022. [6]
32[DKPP23] I. Diakonikolas, D. M. Kane, A. Pensia, and T. Pittas. “Nearly-Linear Time
and Streaming Algorithms for Outlier-Robust PCA”. Proc. 40th International
Conference on Machine Learning (ICML). 2023. [26, 27]
[DL22] J. Depersin and G. Lecué. “Robust Sub-Gaussian Estimation of a Mean Vector in
Nearly Linear Time”. The Annals of Statistics (2022). [6, 8, 12, 16]
[DL88] D. L. Donoho and R. C. Liu. “The "Automatic" Robustness of Minimum Distance
Functionals”. The Annals of Statistics 16.2 (1988). [2]
[FKP19] N. Fleming, P. Kothari, and T. Pitassi. “Semialgebraic Proofs and Efficient Algo-
rithm Design”. Found. Trends Theor. Comput. Sci. (2019). [28]
[HKSO21] J.Hayase,W.Kong,R.Somani,andS.Oh.“SPECTRE:defendingagainstbackdoor
attacks using robust statistics”. Proc. 38th International Conference on Machine
Learning (ICML). 2021. [2]
[HL18] S. B. Hopkins and J. Li. “Mixture Models, Robustness, and Sum of Squares Proofs”.
Proc. 50th Annual ACM Symposium on Theory of Computing (STOC). 2018. [28,
29]
[HL19] S. B. Hopkins and J. Li. “How Hard Is Robust Mean Estimation?” Proc. 32nd
Annual Conference on Learning Theory (COLT). 2019. [5]
[HR09] P. J. Huber and E. M. Ronchetti. Robust Statistics. John Wiley & Sons, 2009. [2]
[HRRS11] F. R. Hampel, E. M. Ronchetti, P. J. Rousseeuw, and W. A. Stahel. Robust
Statistics: The Approach Based on Influence Functions. Vol. 196. John Wiley &
Sons, 2011. [2]
[Hub64] P. J. Huber. “Robust Estimation of a Location Parameter”. The Annals of Mathe-
matical Statistics 35.1 (Mar. 1964). [2]
[JKLPPT24] A. Jambulapati, S. Kumar, J. Li, S. Pandey, A. Pensia, and K. Tian. “Black-Box
k-to-1-PCA Reductions: Theory and Applications”. Proc. 37th Annual Conference
on Learning Theory (COLT). 2024. [26–28]
[JLT20] A. Jambulapati, J. Li, and K. Tian. “Robust sub-gaussian principal component
analysis and width-independent schatten packing”. Advances in Neural Information
Processing Systems 33 (NeurIPS) (2020). [26, 27]
[KL17] V. Koltchinskii and K. Lounici. “Concentration inequalities and moment bounds
for sample covariance operators”. Bernoulli (2017). [17]
[KS17] P. K Kothari and D. Steurer. “Outlier-robust moment-estimation via sum-of-
squares”. arXiv preprint arXiv:1711.11581 (2017). [29]
[KSKO20] W. Kong, R. Somani, S. Kakade, and S. Oh. “Robust Meta-learning for Mixed
Linear Regression with Small Batches”. Advances in Neural Information Processing
Systems 33 (NeurIPS). 2020. [26]
[KSS18] P. K. Kothari, J. Steinhardt, and D. Steurer. “Robust Moment Estimation and
Improved Clustering via Sum of Squares”. Proc. 50th Annual ACM Symposium on
Theory of Computing (STOC). 2018. [28, 29]
[LATSCR+08] J.Z. Li, D.M. Absher, H. Tang, A.M. Southwick, A.M. Casto, S. Ramachandran,
H.M. Cann, G.S. Barsh, M. Feldman, L.L. Cavalli-Sforza, and R.M. Myers. “World-
wide human relationships inferred from genome-wide patterns of variation”. Science
(2008). [2]
33[LL22] Z. Liu and P. Loh Loh. “Robust W-GAN-based estimation under Wasserstein
contamination”. Information and Inference: A Journal of the IMA (2022). [2, 8, 9]
[LM21] G. Lugosi and S. Mendelson. “Robust Multivariate Mean Estimation: The Opti-
mality of Trimmed Mean”. The Annals of Statistics 49.1 (2021). [4]
[LRV16] K. A. Lai, A. B. Rao, and S. Vempala. “Agnostic Estimation of Mean and Covari-
ance”. Proc. 57th IEEE Symposium on Foundations of Computer Science (FOCS).
2016. [2]
[MBW22] T. Manole, S. Balakrishnan, and L. Wasserman. “Minimax confidence intervals for
the Sliced Wasserstein distance”. Electronic Journal of Statistics (2022). [9]
[NDCKSS20] K. Nadjahi, A. Durmus, L. Chizat, S. Kolouri, S. Shahrampour, and U. Simsekli.
“Statistical and topological properties of sliced probability divergences”. Advances
in Neural Information Processing Systems 33 (NeurIPS). 2020. [9]
[Nes04] Y. Nesterov. Introductory Lectures on Convex Optimization. 2004. [26]
[NGC22] S. Nietert, Z. Goldfeld, and R. Cummings. “Outlier-robust optimal transport:
Duality, structure, and statistical analysis”. 2022. [9]
[NGS24] S. Nietert, Z. Goldfeld, and S. Shafiee. “Robust Distribution Learning with Local
and Global Adversarial Corruptions (extended abstract)”. Proc. 37th Annual
Conference on Learning Theory (COLT). 2024. [1–10, 12, 21, 22]
[NR22] J. Niles-Weed and P. Rigollet. “Estimation of wasserstein distances in the spiked
transport model”. Bernoulli (2022). [8]
[PJL20] A. Pensia, V. Jog, and P. Loh. “Robust Regression with Covariate Filtering: Heavy
Tails and Adversarial Contamination”. CoRR abs/2009.12976 (Sept. 2020). [6]
[PLJD10] P. Paschou, J. Lewis, A. Javed, and P. Drineas. “Ancestry Informative Markers for
Fine-Scale Individual Assignment to Worldwide Populations”. Journal of Medical
Genetics (2010). [2]
[RPDB12] J. Rabin, G. Peyré, J. Delon, and M. Bernot. “Wasserstein Barycenter and Its
Application to Texture Mixing”. Scale Space and Variational Methods in Computer
Vision. 2012. [9]
[RPWCKZF02] N. Rosenberg, J. Pritchard, J. Weber, H. Cann, K. Kidd, L.A. Zhivotovsky, and
M.W. Feldman. “Genetic structure of human populations”. Science (2002). [2]
[SCV18] J. Steinhardt, M. Charikar, and G. Valiant. “Resilience: A Criterion for Learning in
the Presence of Arbitrary Outliers”. Proc. 9th Innovations in Theoretical Computer
Science Conference (ITCS). 2018. [6, 8, 11]
[SKL17] J. Steinhardt, P. W. Koh, and P. Liang. “Certified Defenses for Data Poisoning
Attacks”. Advances in Neural Information Processing Systems 30 (NeurIPS). 2017.
[2]
[TLM18] B. Tran, J. Li, and A. Madry. “Spectral Signatures in Backdoor Attacks”. Advances
in Neural Information Processing Systems 31 (NeurIPS). 2018. [2]
[Van17] R. Van Handel. “Structured random matrices”. Convexity and concentration. 2017.
[17]
[YC23] H.YangandL.Carlone.“CertifiablyOptimalOutlier-RobustGeometricPerception:
Semidefinite Relaxations and Scalable Global Optimization”. IEEE Trans. Pattern
Anal. Mach. Intell. (2023). [2]
34[YZLL24] Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. “Generalized out-of-
distribution detection: A survey”. International Journal of Computer Vision (2024).
[2]
[ZJS19] B. Zhu, J. Jiao, and J. Steinhardt. “Generalized Resilience and Robust Statistics”.
The Annals of Statistics (2019). [2, 8, 9]
[ZJS22] B. Zhu, J. Jiao, and J. Steinhardt. “Robust Estimation via Generalized Quasi-
Gradients”. Information and Inference: A Journal of the IMA (2022). [6]
[ZJT22] B. Zhu, J. Jiao, and D. Tse. “Deconstructing Generative Adversarial Networks”.
IEEE Transactions on Information Theory (2022). [2, 3, 8, 9]
35A Additional Preliminaries
The statistical rates for stability (Definition 1.2) are well understood, up to logarithmic factors from
the optimal [DKP20].
Fact A.1 (Stability rates for nice distribution families [DKP20]). Let D be a family of distributions.
Let c be a small enough absolute constant. Fix a D ∈ D and let S be a set of n samples drawn
0
log(1/τ)
i.i.d. from D. For each of the cases below and ϵ+ ∈ (0,c), there exists an S ⊂ S with
n 1 0
|S | ≥ (1−ϵ)|S | which is (ϵ,δ)-stable with probability at least 1−τ, for the following parameter δ:
1 0
• If D is the family of isotropic subgaussian distributions, then δ ≲ ϵ(cid:112) log(1/ϵ) + (cid:112) d/n +
(cid:112)
log(1/τ)/n.
• If D is the family of distributions with isotropic covariance and bounded k-th moments, then
δ ≲ ϵ1− k1 +ρ+(cid:112) (dlogd)/n+(cid:112) log(1/τ)/n.
• If D is the family of distributions with covariance Σ ⪯ I, then δ ≲ √ ϵ + (cid:112) (dlogd)/n +
(cid:112)
log(1/τ)/n.
Recall the family of stability-based algorithms from Definition 1.3. Hence, we immediately obtain
the following, which is a more detailed version of Fact 1.1.
Fact A.2. Let P be a family of distributions. Fix a P ∈ P and let S be a set of n i.i.d. samples from
0
P. Let T be a corrupted version of S with ϵ-fraction of global outliers (Contamination Model 1).
0
Let µ be the (unknown) mean of P. There exist computationally-efficient algorithms that take as
input T, ϵ, and ρ and output µ with the following guarantees:
(cid:98)
If D is the family of isotropic subgaussian distributions, then ∥µ−µ∥ ≲ ϵ(cid:112) log(1/ϵ)+(cid:112) d/n+
(cid:98) 2
(cid:112)
log(1/τ)/n.
If D is the family of distributions with isotropic covariance and bounded k-th moments, then
∥µ (cid:98)−µ∥
2
≲ ϵ1− k1 +(cid:112) (dlogd)/n+(cid:112) log(1/τ)/n.
If D is the family of distributions with covariance Σ ⪯ I then ∥µ−µ∥ ≲ √ ϵ+(cid:112) (dlogd)/n+
(cid:98) 2
(cid:112)
log(1/τ)/n.
Theaboveboundsforrobustmeanestimationareoptimalwithrespecttotheparametersϵ,d,n,τ
√
(up to the logd term). Specifically for ϵ, the matching lower bound can be derived using a simple
identifiability argument (see Section 1.3 in [DK23] for formal details). The (near-)optimality with
respect to the remaining parameters are also folklore facts: Ω((cid:112) d/n) error is necessary even for
estimating the mean of N(µ,I) without outliers (as a standard application of Fano’s method), and
Ω((cid:112) log(1/τ)/n) is necessary even for one-dimensional N(µ,1) without outliers (see e.g., Proposition
6.1 in [Cat12]).
Lemma A.3. Let y ,...,y be n vectors in Rd and a w ∈ ∆ . Let µ ∈ Rd be fixed and define
1 n n,ϵ
µ := (cid:80) w y and Σ := (cid:80) w (y −µ)(y −µ)⊤. Then there exists a set I ⊂ [n] satisfying (i)
w i i i w i i i i
|I| ≥ (1−2ϵ)n and (ii) the set S := {y } satisfying max ⟨M,Σ ⟩ ≲ max ⟨M,Σ ⟩
i i∈I M∈M S M∈M w
Proof. Without loss of generality, let w ≥ w ≥ ··· ≥ w . Define the set I to be the set [(1−2ϵ)n].
1 2 n
Then for each i ∈ I, we have w ≳ 1. Let S = (y ) . Since |S| ≥ (1−2ϵ)n, it follows that 1 ≲ w
i n i i∈I |S| i
for all i ∈ I; see [DKP20, Lemma D.2]. Defining z := y −µ, for any M ∈ M , we have
i i k
1 (cid:88) (cid:88) (cid:88) ρ2
⟨Σ ,M⟩ = ∥z ∥2 ≲ w ∥z ∥2 ≲ w ∥z ∥2 = ⟨Σ ,M⟩ ≲ .
S |S| i M i i M i i M w ϵ
i∈I i∈I i∈[n]
36