Satori:TowardsProactiveARAssistantwithBelief-Desire-IntentionUser
Modeling
CHENYILI∗,
TandonSchoolofEngineering,NewYorkUniversity,UnitedStates
GUANDEWU∗,
TandonSchoolofEngineering,NewYorkUniversity,UnitedStates
GROMITYEUK-YINCHAN,
AdobeResearch,UnitedStates
DISHITAGTURAKHIA,
TandonSchoolofEngineering,NewYorkUniversity,UnitedStates
SONIACASTELOQUISPE,
VisualizationandDataAnalyticsLab,NewYorkUniversity,UnitedStates
DONGLI,
TandonSchoolofEngineering,NewYorkUniversity,UnitedStates
LESLIEWELCH,
BrownUniversity,UnitedStates
CLAUDIOSILVA,
NewYorkUniversity,UnitedStates
JINGQIAN,
TandonSchoolofEngineering,NewYorkUniversity,UnitedStates
ManuscriptsubmittedtoACM 1
4202
tcO
22
]CH.sc[
1v86661.0142:viXraFig.1. Satoriisamind-readingmonkey-shapedmonsterinJapanesefolklore.WenameoursystembySatoritohighlightthe
importanceofunderstandingtheuserstate(humanmind)inbuildingproactiveARassistants.TheSatorisystemcombinesthe
users’self-knowledgeandtheirgoalofthetasktotheimmediateuseractionwithanLLMmodeltoprovidevisualassistancethatis
relevanttotheuser’simmediateneeds.WecallthisproactiveARassistance.ThisisachievedbyimplementingtheBelief-Desire-
and-Intentionmodelbasedontwoformativestudieswith12experts.Herethebeliefreflectswhethertheusersknowwherethe
taskobjectis,andhowtodocertaintasks(e.g.,knowledgelevel);thedesirecomponentistheactionablegoal;andtheintentthe
immediatenextstepneededtocompletetheactionablegoal.Thecodewillbeopen-sourceduponacceptance.
AugmentedRealityassistanceareincreasinglypopularforsupportinguserswithtaskslikeassemblyandcooking.However,current
practicetypicallyprovidereactiveresponsesinitializedfromuserrequests,lackingconsiderationofrichcontextualanduser-specific
information.Toaddressthislimitation,weproposeanovelARassistancesystem,Satori,thatmodelsbothuserstatesandenvironmental
contextstodeliverproactiveguidance.OursystemcombinestheBelief-Desire-Intention(BDI)modelwithastate-of-the-artmulti-
modallargelanguagemodel(LLM)toinfercontextuallyappropriateguidance.Thedesignisinformedbytwoformativestudies
involvingtwelveexperts.Asixteenwithin-subjectstudyfindthatSatoriachievesperformancecomparabletoandesigner-created
Wizard-of-Oz(WoZ)systemwithoutrelyingonmanualconfigurationsorheuristics,therebyenhancinggeneralizability,reusability
andopeningupnewpossibilitiesforARassistance.
CCSConcepts:•Human-centeredcomputing→Mixed/augmentedreality.
AdditionalKeyWordsandPhrases:Augmentedrealityassistant,proactivevirtualassistant,usermodeling
ACMReferenceFormat:
ChenyiLi,GuandeWu,GromitYeuk-YinChan,DishitaGTurakhia,SoniaCasteloQuispe,DongLi,LeslieWelch,ClaudioSilva,
andJingQian.2018.Satori:TowardsProactiveARAssistantwithBelief-Desire-IntentionUserModeling. 1,1(October2018),42pages.
https://doi.org/XXXXXXX.XXXXXXX
1 Introduction
Satori 悟り,aJapaneseghost-likedeitylongknowntoreadhumanminds,respondstoone’sthoughtsbeforeactions
arrive.Whilesuchsupernaturalbeingsbelongtofolklore,modernAItechnologiesarebeginningtoemulatethis
capability,strivingtopredicthumanactionsandprovideproactiveassistanceduringtaskinteractions[38].Proactive
virtual/digitalassistance,whichdeterminetheoptimalcontentandtimingwithoutexplicitusercommands,aregaining
tractionfortheirabilitytoenhanceproductivityandstreamlineworkflowsbyanticipatinguserneedsfromcontextand
pastinteractions[62].However,thereisascarcityofresearchtounderstandhowtobestdesignandimplementsuch
system.
∗Bothauthorscontributedequallytothisresearch.
Authors’ContactInformation:ChenyiLi,chenyili@nyu.edu,TandonSchoolofEngineering,NewYorkUniversity,NewYork,NewYork,UnitedStates;
GuandeWu,guandewu@nyu.edu,TandonSchoolofEngineering,NewYorkUniversity,NewYorkCity,NewYork,UnitedStates;GromitYeuk-YinChan,
ychan@adobe.com,AdobeResearch,SanJose,California,UnitedStates;DishitaGTurakhia,d.turakhia@nyu.edu,TandonSchoolofEngineering,New
YorkUniversity,NewYorkCity,NewYork,UnitedStates;SoniaCasteloQuispe,VisualizationandDataAnalyticsLab,NewYorkUniversity,NewYork,
NewYork,UnitedStates;DongLi,dl5214@nyu.edu,TandonSchoolofEngineering,NewYorkUniversity,Brooklyn,NewYork,UnitedStates;Leslie
Welch,BrownUniversity,Providence,RhodeIsland,UnitedStates;ClaudioSilva,NewYorkUniversity,NewYorkCity,NewYork,UnitedStates;Jing
Qian,jqian1590@gmail.com,TandonSchoolofEngineering,NewYorkUniversity,Brooklyn,NewYork,UnitedStates.
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenot
madeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitationonthefirstpage.Copyrightsforcomponents
ofthisworkownedbyothersthantheauthor(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,toposton
serversortoredistributetolists,requirespriorspecificpermissionand/orafee.Requestpermissionsfrompermissions@acm.org.
©2018Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
ManuscriptsubmittedtoACM
2 ManuscriptsubmittedtoACMSatori:TowardsProactiveARAssistantwithBelief-Desire-IntentionUserModeling 3
Mostcurrentassistanceinaugmentedreality(AR)remainreactive,respondingtousercommandsorenvironmental
triggerswithoutthecapabilityformoreactiveengagement.Suchreactivesystemsrequireuserstoinitiateinteractions,
whichcanbeinefficientinARenvironmentswhereusersoftenhavelimitedattentionforinterfaceinteractionsor
busywithphysicaltasks.SomeARassistantsincorporateproactiveelements;forinstance,inmaintenancetasks,they
provideguidancebasedonrecognizedobjectsorcomponents[44,61].Yet,thesesystemsaremadewithfixedrulesand
oftenlackadaptabilityandreusability,strugglingtogeneralizeacrossdiversetasksorsupporteffectivemultitasking.
DesigningproactiveassistanceforARisparticularlychallengingduetotheneedtounderstandboththeuser’s
stateandthe3Dphysicalenvironment.Usersoftenperformreal-worldtaskssuchasmaintenanceandassemblywhile
wearingARheadsets.Theassistanceshouldthereforeberelevanttotheuser’scurrentimmediatetasksandlong-term
goals.Duetolimitedattentionandassitivenatureintasks,timelyassistanceiscrucial.Providingassistancetooearly,
toolate,orsimplytoofrequentcanincreasecognitiveloadandnegativelyimpacttheuser’sexperience[2].
Toidentifythein-depthbenefits,challengesforcreatingaproactiveARassistanceandexplorehowtodesignone,
weconductedtwoformativestudiestoexplorethebenefits,challenges,andpotentialsolutionsfordevelopingsucha
system.ThefirststudyinvolvedsixprofessionalARdesignersandrevealedseveralchallenges:1)limitedgeneralizability
andreusabilityofcurrentnon-proactiveARassistance,and2)difficultiesinaccuratelydetectinguserintentions;andthe
3)theneedtobalancegeneraladvicewithtask-specificsolutions.TheprofessionalsrecognizedthatusingproactiveAR
assistancecouldpotentiallyimprovingthescalabilityandefficiency,butalsohighlightedtechnicalchallengesrelatedto
accuratelytrackingandunderstandingusers’actions.
Buildingonthefindingsfromthefirststudy,thesecondformativestudyengagedsixexperts(threeHCIresearchers
andthreepsychologyresearchers)inaparticipatorydesigntoexplorestrategiesformakingARassistancemore
proactive.Thedesignsessionshighlightedseveralkeyfactors:understandinghumanactions,recognizingsurrounding
objectsandtools,assessingthecurrenttask,andanticipatingimmediatenextsteps.Theseinsightswereintegrated
withthewell-establishedBelief-Desire-Intention(BDI)theory,resultinginanAR-specificadaptationthatguidedthe
developmentofoursystem,Satori.
ToadapttheBDItheoryforARassistance,SatorineededtoaccountforthelimitationsofARheadsethardware,
whichprimarilyreliesonegocentricvision.WebuilttheBDImodel’spredictionframeworkusinganensembleof
egocentricvisionmodelscombinedwithamulti-modalLargeLanguageModel(LLM)toleverageitsrobusttask-learning
capabilities.TheBDImodeliscrucialforregulatingboththetimingandcontentoftheassistanceprovided.Wepropose
amulti-modalassistanceframeworkwheretheintention(i.e.,immediatenextstep)determinesthecontentofthe
assistance.Meanwhile,Sceneobjectsanduseractionhistoryregulatesthetimingoftheassistance.Thisapproach
ensuresthattheARassistancedeliversrelevantinformationatappropriatemoments,enhancingtheuser’sexperience
withoutoverwhelmingthem.
WeevaluatedSatoriagainstfoureverydayARtasksdesignedbysixprofessionalARdesignersandfoundthatSatori’s
proactiveguidancewasaseffective,useful,andcomprehensibleasARguidanceapplicationscreatedbythedesigners.
Additionally,Satori’sguidanceallowedparticipantstoswitchbetweentaskswithouttheneedforpre-trainingor
scanning.OurfindingsindicatethattheapplicationoftheBDImodelnotonlysuccessfullyunderstoodusers’intentions
butalsocapturedthesemanticcontextofagiventask,reducingtheneedtocraftARguidanceforeveryspecificscenario
andimprovingthegeneralizabilityandreusabilityofARguidance.
Tosummarize,ourcontributionsinclude:
ManuscriptsubmittedtoACM4 LiandWu,etal.
(1) ThedesignrequirementsforcreatingaproactiveARassistantandadaptingtheBDImodelforARenvironments,
basedontwoconsecutiveformativestudiesinvolvingtwelveexperts;
(2) AnARproactiveassistantsystem,Satori,thatappliesconceptsfromtheBDImodelcombinedwithadeep
model-LLMfusionarchitecturetoinferusers’currenttasksandintentions,providingguidanceforimmediate
nextstepsthroughmultimodalfeedback;
(3) Asixteen-userempiricalstudydemonstratingthatourproactiveARassistantdeliversperformancecomparable
todesigner-createdARguidanceintermsoftiming,comprehensibility,usefulness,andefficacy.
2 RelatedWork
2.1 VirtualAssistantinAR/VR
VirtualassistantsinAR/MRcanwellsupportthetaskinassembly[7,35],surgery[20,69],maintenance[6,22,36]and
cooking[16].Theseassistantsystemsareoftentask-orientedandtheirprinciplesdonoteasilygeneralizetoother
domains.Onewaytoimprovegeneralizabilityisviaacommand-basedARassistant,whichcanenhanceuserconfidence
intheagent’sreal-worldinfluenceandawareness[32].Yet,theyrequiretheuser’sexplicitcommandsandlimitusability.
OurworkbuildsonthepreviousresearchasavirtualassistantinAR/MR,whileaddressingtheusers’needswithout
explicitcommandsordomainlimitations.
Pre-made assistance in AR and VR applications typically involves prepared actions or reminders triggered by
specificuserinputsorsituations.Thiskindofassistanceissimpleandintuitive,providinguserswithreadilyavailable
supportthatcanbeaccessedon-demandorintimesequences[45],whichisstraightforwardtoimplementanduse[78].
Assistantsrequireextensiveusermanualinteractionstodescribeandconfirmtheuser’sneeds.Forexample,Saraetal.
demonstratedanARmaintenanceassistantwhilethetechnicianstillneedstomanuallyconfirmthecompletionofeach
stepandproceedtothenextstepusingtouchpadcontrolsorvoicecommands[78].
Proactiveassistance,ontheotherhand,isdesignedtoactivelyrecognizecontextinformationandinferuserintentions
eveniftheyarenotexplicitlyprovided[18,70,80].Suchassistancenormallydoesnotrequirehumanintervention[41,
79,94]andcaneasilybescalableforeverydayARtasks,suchashealthcare[71,81],navigation[66]andlaboratory
education[83].Theyenhanceusability[82],fostertrust[39],andimprovetaskefficiency[93].DuringARinteractions,
proactiveassistanceoftentakesintoaccounttheuser’ssurroundingenvironment,predictstheuser’sgoalsandoffers
context-awarerecommendations,oftenforthesakeofimprovingattention[29,59,60,85].However,existingproactive
assistantreliesonpre-determinedcontextualsignalssuchasthelocation,time,andeventstotriggertheassistantsto
intervene[60].Forinstance,Renetal.[76]proposeaproactiveinteractiondesignmethodforsmartproduct-service
systems,usingfunctionalsensorstogatherexplicitcontextualdata(e.g.,physicallocation,lightintensity,environment
temperature)topredictimplicituserstates(e.g.,userattentionlevel).Althoughthesemethodsadvancetheprogress
ofproactiveassistance,suchsignalsmaynotalignwiththeactualusers’needs,leadingtoineffectiveandobtrusive
assistance[42,91].Toaddressthis,weproposetomodeltheuser’sintentdirectlytodetermineabettertimingandtype
ofguidance.
Furthermore,eventhoughproactiveassistantshavebeenwidelyused,mostARassistantstodayremainpassiveas
defininguserintentsisdifficultineverydaysettings.Onemainchallengeisthatunderstandingusers’intentrelies
notonlyontheexplicitcues(e.g.,verbalorsignals)butalsosignificantlyontheimplicitnon-verbalcuesandvisual
embodiment[32].Successfullydecomposingandreasoningwiththeimplicitcuesimprovesthechancesofintent
referring.Recentadvancementsinvision-languagemodelsoffernewopportunitiestobeusedtounderstandvisual
ManuscriptsubmittedtoACMSatori:TowardsProactiveARAssistantwithBelief-Desire-IntentionUserModeling 5
embodimentifintegratedintovirtualassistants.Therefore,weproposeamultimodalinputmechanismthattakesboth
voiceandvisualcuestosupportabetterunderstandingofusers’interactionintentions.
2.2 UnderstandingUserIntention
Understandinguserintentioninelectronicdevices,rangingfromsmartmobiledevicestoaugmentedreality(AR)
systems,isessentialforimprovinguserinteractionandexperience.Researchinthefieldofinformationneedshas
highlightedtheimportanceofintentionclassificationandsystematictaxonomytoachievethisgoal.Borderproposed
thetaxonomyofwebsearches,classifyingintentionsintonavigational,informational,andtransactional[11].This
foundationalworklaidthegroundworkformoredetailedclassifications.Forinstance,Dearmanetal.categorized
informationneedsandsharingentriesintoninedistinctcategories,extendingtheconceptofinformationneedsto
acollaborativelevel[17].Thisclassificationallowsdeveloperstodesignproductsthatbetterfacilitatecollaborative
sharingofinformation.Churchetal.foundthatcontextssuchaslocation,time,socialinteractions,andusergoals
influenceusers’informationneeds.Forexample,itwasfoundthatusersgeneratedmoreneedswithlocationalor
temporaldependencieswhentheywereonthego.Usersalsorequiremoregeographicalinformationwhentheyare
commuting.ThisstudyenabledresearcherstodesignaninformationsearchplatformSocialSearchBrowsertofitdifferent
users’informationneedsinacontext-sensitiveway[14].Additionally,Lietal.furtheredthisbranchofresearchby
developingadesignspaceofdigitalfollow-upactionsformultimodalinformation[52].Theyclassifiedactionsinto17
typesandidentifiedsevencategoriesoffollow-upactionsbyqualitativeanalysisofusers’diaries.Theyalsodeployed
thesystemonmobileARandconductedauserstudytotestthecapacityoffollow-upactiondesigns.Thestudyshowed
thatthesystemcouldaccuratelypredictuser’sgeneralactions,provideproactiveassistance,andreducefrictions[52].
Generally,priorstudiesoninformationneeds,particularlyonmobiledevices,demonstratedthatintentiontaxonomy
couldinspirethedesignofinformationsearchsystemswithmoreproactiveandcontextualassistance.
2.3 Belief-Desire-IntentionFramework
TheBelief-Desire-Intention(BDI)model[8,15,30,49]isaframeworktosimulatehumandecision-makingbehaviorsin
bothindividual[74]andmulti-agentsettings[33,48,65].Themodeloriginatesfromfolkpsychologyandisextensively
appliedincognitivemodeling,agent-orientedprogramming,andsoftwaredevelopment.Thismodelcomprisesthree
primarycomponents:beliefs,desires,andintentions[8].Beliefsrepresenttheinformationthathumansperceiveabout
asituation(e.g.,itisraining),limitedbytheirperceptions.Desiresarethegoalsthatindividualsaimtoachievegiven
thecurrentsituation(e.g.,Apersonprefersnottogetwetduringarainyday).Intentionsare“conduct-controlling
pro-attitudes,oneswhichwearedisposedtoretainwithoutreconsideration,andwhichplayasignificantroleasinputs
to[means-end]reasoning”[8].Inotherwords,user’sbehaviortowardsachievingthedesire(i.e.,goals)byselecting
andcommittingtospecificplansofaction(e.g.Thepersonplanstogetanumbrella).
PreviousstudieshavedemonstratedtheeffectivenessoftheBDIframeworkinmodelinghumanbehavior[33,65].
Therefore,theBDImodelcanhelpinthebuildingofintelligentagentsinvariousapplications.Forexample,inagent-
oriented programming, the BDI model is pervasively used to model an agent executing programming functions.
Agent-orientedsoftwareengineeringutilizesbeliefs,actions,plans,andintentionstodevelopprograms.TheBDI
modelenablesmorerationalandautonomousexecutionsinunpredictableenvironments,suchasAgentSpeak(L)[73],
3APL[28],JACK[12],JADEX[9],andGOAL[27].OnebenefitofusingtheBDIframeworkisthatitmakesagent
behaviorintelligibletoendusersandstakeholders.Bycommittingtospecificcoursesofactionorintentions,BDIagents
enhanceuserunderstandingandpredictabilityofactions[1,5,19,21,25,31,34,67,75,86].
ManuscriptsubmittedtoACM6 LiandWu,etal.
ThoughBDI-inspiredagentshaveenabledautomaticdecisions,makingdecisionsinARrequiresadifferenttypeof
intelligentandrealisticbehavior.TheenvironmentforARapplicationsinvolvescomplexreal-worlddynamics,suchas
egocentricvideo,audio,andgesturalinputs[4].Theusers’interactiongoals,physicalactions,andsurroundingcontext
(e.g.,objects,tools,interactionagents)furtherincreasethedifficultyofprovidingin-timeassistance[53].
AlthoughtheBDIframeworkhasnotyetbeenappliedtoAR,ourworkdrawsinspirationfromthephilosophyand
designofpriorBDI-basedsystemstoenhanceARassistance.Withrecentadvancementsinlargelanguagemodels
(LLMs),BDI-drivenagentspresentapromisingdirection[5],asLLMscannaturallyserveasinterpretersandreasoning
machines,bridginglanguageandtextwithintheBDIframework.
2.4 UserModellinginHuman-AICollaboration
Modellingtheuserstateisalong-standingprobleminHCI[3,58].Previousresearchfocusesontheusergoalandintent
[92],Expertisemodelingtosupportadaptivecomputingsystems[87],andthestudyofthememoryoftheuserfor
AR/MR-specificresearch[26,84].TheBDImodel,acommonlyacceptedpsychologicalframework[24,49],becomes
crucialintheemergenthuman-AIcollaboration,necessitatingabettermodeloftheuserstate[43].Existingresearch,
however,focusesontheuser’sintentionandgoalandseldomaddressestheuser’sknowledgeorbelief[23,47,89,90].
Furthermore,there’salackofdistinctionbetweenhigh-levelgoals(desires)andimmediategoals(intents)[37].Hence,
weproposeageneralmodelfortheuserstate,amalgamatingbelief,desire,andintent.
3 FormativeStudy1:DesignwithProfessionalARDesigners
WefirstconductaformativestudytoexploretheproblemspaceandpotentialbenefitsofproactiveARassistance.
Thestudybeginswithasemi-structuredinterviewonparticipants’backgroundknowledge,followedbyfourdifferent
interactionscenariosthatarecommoneverydayARtasksshowntoparticipantsfordesignfeedback.Afinalapparatus
combiningparticipants’designfeedbackiscreatedforlaterstudy.
3.1 Participants
Usingemailandsnowballsampling,werecruitedsixprofessionalARdesigners(threefemaleandthreemale,age:
𝑥¯=30).Sincewewanttocollectinsightsfromexperiencedindividuals,allparticipantsareprofessionalARdesigners
currentlyworkingincompanieswithatleastthreeyearsofexperiencedevelopingARapplicationsandwhohave
experiencecreatingARapplicationsforreal-worldguidingtasks.Participantsarepaid$30perhour.
3.2 TaskDesign
Thestudycontainstwosessions:asemi-structuredinterviewandatasktodesignARassistanceforfoureveryday
scenarios.Eachparticipantwasaskedtodesigntwooutofthefourscenarios,ensuringabalanceddistributionacross
scenarios.Asaresult,eachscenariowasdesignedbythreedifferentparticipants.
Inthefirstsession,weaskedparticipantsabouttheirpriorworkingexperiencewithARassistants,thechallengesthey
facedincreatingthem,potentialbenefits,andapplicationsineverydaysettings.Wefurthercollectedtheirresponseson
insights,potentialbenefits,andusescenariosofproactiveassistants.
Inthesecondsession,designerswereaskedtodesignARassistantsfortwoeverydayscenarios.Thescenarioswere
assignedinapre-determinedordertobalancethetotalnumberofdesigns.Sincethesescenariosareineverydaysettings,
ManuscriptsubmittedtoACMSatori:TowardsProactiveARAssistantwithBelief-Desire-IntentionUserModeling 7
weuseWikiHow1toobtaindetailed,step-by-stepinstructionsasthetask’sbackgroundinformationforparticipants.
Thetaskshaveanaverageof7steps,butparticipantscanaddadditionalsubstepsifneeded.Wetakescreenshotsand
videosusingHololense2followingtheobtainedinstructions.Weusethemasvisualguidancetodemonstratethe
task’sinteractioncontext.Thetasksarepresentedtoparticipantsasdigitalformscontainingtheaboveinformation,
andparticipantsareaskedtodesign:1)ifguidanceisneededforthecurrentstep;2)guidanceappearingtimingand
duration;3)modalityofguidance;4)contentofguidance.Theabovequestionsfocusonthequestionsof“if”,“when”,
“how”,and“what”inARguidance,whicharecommonwaystoguideusersintheliteratureandcurrentpractice[todo].
3.3 Procedure
Sinceparticipantsresideglobally,theexperimentwasconductedremotelyviaZoomafterobtainingtheirinformed
consent.Participantswerefirstaskedtointroducetheirbackground,describetheirdailywork,anddiscussprojects
relatedtoARguidance.WefurtherinquiredabouttheirinsightsintotheadvantagesanddisadvantagesofARguidance,
includingchallengesfacedduringdevelopmentandbyendusers.Finally,wepresentedtheconceptofproactiveAR
guidanceandsolicitedtheiropinionsonpotentialchallenges,applications,andfeasibility.
Afterthesemi-structuredinterviews,participantsreceiveddigitalformscontainingmaterialstodesignARguidance
fortheirassignedtasks,includingtextualdescriptions,correspondingimages,andvideos.Duringthisphase,participants
were introduced to the process outlined in the previous paragraph. The experimenters addressed any questions
participantsraisedviaZoom.
Onaverage,thestudy’sfirstsessionlastedapproximately28minutes(𝑥¯=28),whilethesecondsessiontookaround
60minutes(𝑥¯=60).Theentireexperimentlastedabout1.5hours.Allparticipantssuccessfullycompletedthedesign
task,resultinginthecreationoffourARtaskdesigns.
3.4 Results
3.4.1 Interviewresult.
BenefitsofARAssistance. TheexperimentersfoundARguidanceparticularlybeneficialinprovidingreal-time,
contextualinformationthatenhancesboththeuser’sawarenessanddecision-makinginphysicalenvironments.A
keyadvantageofARisitsabilitytorevealforgottenoroverlookedinformation.Forinstance,E1emphasized
that“IfindthatARassistancemostusefulwhenithelpstheuserrealizesomethingtheymightnotknow...theymight
forgetaboutanobject,orarenotawarethatthisobjectcouldbeusedinthissituation...then(withARguidance)theyhave
thisEurekamoment.”The“Eurekemoment”referstothemomentswhereuserssuddenlyrealizetheutilityofobjects
oractionstheyhadn’tconsidered.Thisfunctionisespeciallyusefulinspatialtasks,asmentionedbyE2andE3.E2
highlightedthatbyoverlayingvisualcuessuchasarrowsoranimationsdirectlyontotheenvironment,ARcanhelpthe
userbetterunderstandcomplexelectricalcircuits.E3statedthat“intaskswithspatiallysensitivemovements...ARisa
propermediumbecauseuserscanintuitivelyknowwhattheyneedtodo.”E3furtherintroducedthattheusersreceived
spatiallypositionedguidanceonturningknobsorpressingbuttonsinamachineoperationtask,whichwasmore
intuitivethantraditional2Dinstructions.Additionally,E4statedthatARreducedinteractioncosts,particularlyfor
tasksthatrequirehigh-frequencyoperations,andenabledhands-freeoperations,makingithighlyvaluableinscenarios
likecooking.Thefirst-personperspectiveofferedbyARalsoaidsinbettercomprehensionofinstructions,asmentioned
byE7,especiallyforindividualswithlimitedexperience,suchasstudentsinlaboratorysettings.
1https://www.wikihow.com/
ManuscriptsubmittedtoACM8 LiandWu,etal.
ChallengesofARAssistance. TheinterviewrevealedseveralkeychallengesindesigningARassistance.Onemajor
challengeisthedifficultyofgeneralizingARcontenttofitdiversecontexts,asARdesignersoftencreatedesigns
basedontheirassumptionsabouttheuser’senvironment.However,usersmayinteractwithobjectsthatfalloutside
theseinitialassumptions.AsE1noted,“It’shardtocoveralltheedgecasesofwhatapersonmighthave...Iassumethey’re
inanindoorspace,butthatmightnotbethecase,”highlightingthecomplexityofaccommodatingvariedenvironments.
Anotherchallenge,asE5noted,isthelackofastandardizedapproachintheexpansiveinteractiondesignspace,
especiallycomparedtotraditional2Dinteraction.E3pointedoutthedifficultyincreating3Dvisualassetsfromscratch,
furthercomplicatingtheprocess.Additionally,designingeffectiveassistancethataccuratelyalignswithuser
actionsandintentionsremainsproblematic.BothE3andE4notedthedifficultyindefininganaccuratemapping
betweenuseractionsandARresponses.E4emphasizedthatmisinterpretinguserbehaviorcanresultinirrelevantor
unhelpfulguidance.Forexample,recommendingataxiwhentheusermerelyintendstowalk.E3alsoemphasizedthe
difficultyfacedbytaskexpertswithoutengineeringexpertise,stating,“SupposeIamadesignerandIknownothing
aboutcoding,butIstillwanttomakeARassistanceforusers,howshouldIdothat?”Finally,E6highlightedthatcurrent
designsoftenfailtoaccountforusers’priorexperienceswithAR,whichcouldhindernovicesfromgainingan
immersiveexperienceinspatialinteractions.
BenefitsofProactiveARAssistance. TheexpertshighlightedseveralbenefitsofProactiveARassistancefromboththe
ARdevelopersanduserspointofview.
ForARDevelopers:Firstofall,E1,E2,andE6agreedthatproactiveassistancecouldtremendouslyreduces
developmenttimeandincreasesefficiency.Forinstance,E2remarked,“Wewilldefinitelyseeahugeimprovementin
theefficiencyofthecontentcreationthroughthisauto-generationprocess.”Similarly,E1notedthatautomaticallyassist
usercansimplifytaskssuchasaddinglabels,recognizingobjects,andgeneratingguidance.Shecontinuedtoofferan
exampleofacookingappwheresuchautomationwouldbeparticularlyusefulinidentifyingingredientsorsuggesting
cookingsteps.
BothE1andE3highlightedhowautomaticARdesigncouldgeneralizeacrossdifferentdomains.According
toE1,“Ifwehaveapipeline...usingcomputervision,itwouldsavealotoftime...couldhaveauniversalpipelineto
createguidance.”Moreover,E3pointedoutthatproactiveARassistancemaybeadaptasauthoringtoolslikespatial
programmingandprogram-by-demonstration,increasingtheaccessibilityfornon-developerexperts.E4,E5,andE6
envisionthepotentialoftheautomaticdesignprocessasprovideproactiveguidance.E4pointedoutthatsuch
assistanceanticipatestheuser’sintentionsandtheenvironmenttoprovideaccurateguidance.E6addedthattheycould
beusedtodetecterrorsingesturesandautomaticallyprovidecorrectiveguidance.
Forusers:,designerspointedthatproactiveassistancemayhelpavoidinformationoverload.E5emphasizedthat
automaticdetectionofuserintentcouldhelpavoidinformationoverloadbypresentingonlyrelevantinformation.It
mayalsogaintrustfromuserssincetheproactiveassistancesmaketheuserthinkthattheyunderstand.
ChallengesofProactiveARAssistanceDesign. KeychallengesofautomaticARdesigncenteronscalabilityanduser
understanding.E1highlightedtheneedforauniversalsystemthatcanoperateacrossdifferentdevicesanddomains.
However,asE3elaborated,scalabilityremainsasignificanthurdlebecauseARsystemsrequiredomain-specific
knowledgetoprovideeffectiveguidance.AsE3noted,“Scalabilityisthemainissue...ARsystemsmustlieinaspecific
domain,andit’shardtodothisforeverydomain.”E6addedthattheautomaticsystemmustbeadeptatmanaging
unforeseensituations,whichrequireadeepunderstandingofthetaskathand.Evenwiththehelpoflargelanguage
models,furthertrainingandcustomizationofthetaskswerenecessary.ExpertsE2,E4,E5,andE6alsoemphasizedthe
ManuscriptsubmittedtoACMSatori:TowardsProactiveARAssistantwithBelief-Desire-IntentionUserModeling 9
Modality DetailedAssistanceType Content
text text overview;instructioninformation;reminder
visuals animations instruction
image instruction
arrows location;interactionpoint
progressbar checkprogress
checkpointcue stepcompletion;warning
audio soundcue stepcompletion;warning
voice instruction
gadget timer counttime
Table1. TypesofassistanceprovidedacrossdifferentmodalitiessuggestedbyARdesignexperts.
difficultyofaccuratelydetectinguserintentions.E5highlightedthelimitedfieldofviewinARheadsetsandthe
lowaccuracyofdetectionalgorithmsinreal-worldenvironments,asshenoted“Sometimes,thesystemmighttrigger
guidancewhentheuserdoesn’tneedit,whichcouldleadtoconfusion.”Similarly,E4discussedhowARsoftwareinthe
industrystruggledtofullyunderstandcomplexuserenvironmentsandactionsinreal-time.Furthermore,E2mentioned
thatsuchautomationcanalsoconfuseusersduetoitslackofself-explanatoryfeatures.E2statedthat“if(thesystemis)
fullyautomatic,youneedthesystemtohavesometypeoffeedback.Automationwithoutfeedbackmayconfusetheuser.”
Finally,E6stressedtheneedtobalancegeneraladvicewithtask-specificsolutions.ARsystemsmustremain
relevanttotheuser’scurrenttask,offeringguidancethatisbothpracticalandactionable.E5alsostatedthechallenge
tobalancetheinitiativeoftheuserandthesystem.Assheremarked,“Findingthebalancebetweensystemflexibilityand
thecontrolitgivesusersisachallenge.”
3.4.2 Designresult. Designersoptedtodisplayappropriatecontentattherighttime.Foreachstep,ARprofessionals
designedtherelevantassistance.Thedesignersuseduser-centeredandobject-centeredstrategiestodeterminewhen
toassist.Theuser-centeredstrategyreliesontheuser’sactions.Forexample,onedesignercreatedtheinstructions
toshowupwhentheusergetsstuckinthesteporwhentheusershowsintention.Theobject-centeredreliesonan
object’sstatus.Forexample,oneexpertdesignedaremindertochangethemoppadwhentheoldpadisdirtyinthe
roomcleaningtask.Somedesignerscreatedinstructionstoshowupwhentheuserfinishesthelaststepandwhen
somethingunexpectedhappens.Theyalsodesignedasuccesscuewhentheusercompletethestep.
Theexpert-designedassistanceexpandsmultiplemodalities,includingtext,visuals,audio,andanothertool.Notably,
theexpertstendedtochooseacertaintypeofmodality("how")fordifferentcontents("what").Table1showsan
overviewoftheassistancemodalityandcontent.Textassistanceisusuallyusedtoshowanoverviewofthestep,
detailedsub-stepsinthestep,informationabouttheobject,oragentlereminder.Thevisualsdesignedbytheexpert
canbeclassifiedintovisualoverlays(e.g.,arrows,progressbar,checkpointcue),images,andanimations.Thearrows
andothervisualhighlightsareusedtoindicatethelocationsorinteractionpointsoftheobjects.Theprogressbaris
designedfortheprogresscheckfortheuser.Theimageandanimationcanillustratethedetailedinstructions.The
checkpointcueisdesignedtoshowtheuserstepcompletionorwarning.Theaudioassistancecancreateatimely
warning,step-by-stepguidance,orasuccesscue.Thetimerisdesignedtocounttimefortime-sensitivesteps,for
instance,makingpour-overcoffee.
ManuscriptsubmittedtoACM10 LiandWu,etal.
3.4.3 WizardofOzsystem. WeaskedtheARprofessionalstodesignanapparatusthatcanbeusedforlaterempirical
study.EachdesignermadetwoARassistancedesignsfortwotasks.TheWoZcontainsimage,voice,andtext-based
assistance.TheimagesweresourcedfromtaskinstructionsonWikiHow,andthetextandvoiceguidanceweredeveloped
basedonexpertdesignsandWikiHowinstructions.Intotal,ARdesignerscreatedeachtaskthreetimes.Wecombined
similartiming,modality,andcontenttoformonefinalARassistancepertask.WethenimplementtheseARassistances
inUnityandemployawizard-of-oztotriggertheassistancetimelyandaccurateviawirelesskeyboardcontrol.To
visualizehighlights,weoverdrawvisualsaboutinteractionpoints,tasklocations,andthequantitiesofmaterialsdirectly
onstaticimages.Theanimationsweresimplifiedbyconcatenatingmultipleimagestoofferstep-by-stepguidance.
Theresultingsystemisvideo-recordedoverMicrosoftHololensandsentbacktoARprofessionalsforrecognition.All
designersagreewithhoweachstepisimplementedafteranydiscrepanciesareresolvedthrougheitherclarificationor
modificationtotheapparatus.
4 FormativeStudy2:Co-DesignWiththePsychologicalandHCIExperts
TobuildanautomatedARassistancethatproactivelyprovidesuserassistiveinformation(instructionsortips)and
understandsthecurrenttask,werecruitedsixexpertsfromcomputerscienceandpsychology(E1-6)tospurdiscussions
forpotentialsolutionsovertwosessionsofdyadicinterviews.Thestudyisfocusedonhowtodesignthesystemby
askingtheexpertstodiscussfactorsthatconstructaproactiveassistancesystem.Wepairedexpertswithcomplementary
backgroundstoformthreegroups(GroupsA,B,andC)astable2shows.Theirideasanddesigndecisionsarethen
formedasdesignfindingsthatmotivateoursystemimplementations.
4.1 DyadicInterviews
Weconducttwosessionsofdyadicinterviewswheretwoexpertsofcomplementarybackgroundsweregroupedto
discussthesolutionstothepresentedchallenges.Dyadicinterviewsallowtwoparticipantstoco-worktowardsopen-
endedquestions[64].ThissetupletusunderstandhowtodesigntheproactingARassistancefrominteractionanduser
modeling.
4.2 Challenges
BasedonthefindingsfromprofessionalARdesignersinthepreviousformativestudy,wefurtherperformedaroundof
literaturesurveybysearchingARassistance,embodiedassistant,andimmersiveassistantonGoogleScholarandACM
DL.Wefilterouttheunrelatedpapersandderivethedesignchallengesfromthefilteredpapercollection.Twoauthors
separatelyreviewedthesepapersandcodedthekeychallengesfromthem.Intotal,wefound25commonchallenges
andgroupedthembythemes,followingtheconceptsfromthematicanalysis[10].
4.2.1 C1:Triggeringassistanceatrighttimeischallenging. TheARassistanceneedstobetriggeredattheproper
timeduringARinteraction.Impropertimingstrategymaydisappointtheusersandreducetheuser’strust[40].For
example,whentheuserisoccupiedorunderstress,frequentdisplayingofARassistancemayfurtherincreasetheuser’s
stress.Currentproactiveassistantsregulatethetimingusingtheuser’sintentandactions[79],orusingfixedintervals
todisplayassistanceperiodically.However,thesemethodsdonotconsidertheuser’sgoalandleadtosub-optimal
performance.
4.2.2 C2:ReusuabilityandscalabilityinARassistanceisaproblem. MostoftheexistingARassistancesystemsare
designedasad-hocsolutions,wheretheformsofassistance(image,text,andvoice)aredevelopedindividually[60,66,71]
ManuscriptsubmittedtoACMSatori:TowardsProactiveARAssistantwithBelief-Desire-IntentionUserModeling 11
Expert Background Gender Group
E1 HCI M A
E2 Psychology F A
E3 ComputerVision&Psychology F B
E4 Psychology M B
E5 HCI M C
E6 Psychology M C
Table2. Expertbackgroundintheco-design.Wepairedonecomputerscienceexpertwithonepsychologyexpertpereachgroup.In
totalthreegroupsparticipatedtheco-design.
(a)Initiallypresenteddiagramandtheavailablemodules. (b)SampleresultfromGroupB.
Fig.2. Participatorydesigninthefirstsession.Theexpertsneedtocollaborateoncreatingadesiredassistantframeworkbasedon
theprovidedbaselinediagram.Atthebottomoffigure(a),theexpertscanfindthesystemcomponentsforperception.
andre-adapttolateruse.Thisisbecauseeachinteractionscenarioisdifferent;thus,contextualinformationisrequired.
Designersoftenneedtomodifytheirdesignsduringdevelopment,resultinginunnecessaryslowdowns.
4.2.3 C3:TaskinterruptionandMulti-tasktrackingisdifficult. Usershandlingmultipletasksatonceiscommonin
everydaylifebutchallengingforARassistance[?].Iftheassistancecannotrespondcorrectlyfollowingtheuser’s
immediatetaskswitchingorpausing,theinteractionefficacywillbeaffected,resultinginusersnottrustingtheassistive
system[57,95].However,existingtechnologiesthatcanprovideproactiveARassistancearelimitedinreasoningthe
currenttaskinamulti-tasksetting.
4.3 Firstsession:participatorydesign
Toformalizehowdowedesignaproactivesystemcapableofdeterminingwhattoshowusersfortaskcompletion,
wefirstpresentthebackgroundknowledgeofARassistance,modalities,applications,andchallengesdescribedin
Sec4.2.Duringthepresentation,weclarifiedanyconcernsexpertsraised.Attheendofthepresentation,eachdyadic
groupisaskedtodiscuss1)whatthesystemneedstoknowtoactproactively,2)whatkindoffeaturesthesystemneeds
tohave,and3)whetherusermodelingwouldbehelpful,and4)howdowemitigatetheknownchallenges?
Aftera50-minuteopen-endeddiscussion,weprovidedthemwithalistofcommonlyusedtracking,computer
perception,contextualunderstanding,anddisplaytechnologiesandintroducedtheirfunctions(Fig2).Basedonthe
ManuscriptsubmittedtoACM12 LiandWu,etal.
discussion,thedyadicgroupcanaddmorecategoriesorfunctionstothislistiftheyfindittheoreticallyusefulfor
proactiveassistance.TheirmodifiedlistsareillustratedinMiro2.
4.4 Secondsession:designforimplementation
Thesecondsessioninvolvedreconveningthesamegroupsofexpertsfordyadicinterviews.Initially,wepresentedthe
outcomesofthefirstsessionalongsideoursynthesizedframework,seekingconfirmationthatitaccuratelyreflected
theirinitialideas.Thiswasfollowedbyanopendiscussionwheretheexpertsdelvedintotheframework’sdetailsand
madeadjustmentstorefineitfurther.Thissession,whichlastedapproximatelyonehourforeachofthethreegroupsof
experts,wasessentialforfinalizingthedesignframeworkfortheARassistant.
4.5 DataCollection
Wescreen-recordedboththediscussionsandtheparticipatorydesignsessions.Theaudiofromtheserecordingswas
thentranscribedintotextusingZoom’sauto-transcriptionfeature.Twoco-authorsindependentlyanalyzedthevideo
recordingsandtranscribedtext,codingthefindingsintosimilarinsights.Theinsightsarethencombinedintothe
followingfindings,anddiscrepanciesareresolvedthroughdiscussion.
4.6 KeyFindings
[KI1] BDImaybebeneficialforbuildingARassistants.Duringthediscussions,allthreepsychologyexperts(E2,E4,
andE6)broughtuptheimportanceofconsideringWhattheuserseesandunderstandsinthesurroundings
whendiscussingC1.Forinstance,E4emphasized,“...itisimportanttomodelthehuman’smentalspace,sowe
canadjusttheAR(assistance’s)timing.”E4furtherintroducedthebelief-desire-intentionmodel,describingitas
awell-establishedcognitivemodelforunderstandinghumanbehaviorandcouldbeusedtowardsproactive
assistance.E2emphasizedtheimportanceofprovidingthisfeedbacklooptoensuretheassistantcorrectly
understandstheuser’sgoals,thusenhancingtheeffectivenessofthemulti-taskARassistant.
[KI2] Userintentioncanbiastheguidancetypeandcontentanditrevealstheuser’simmediateandconcretegoal.
GroupAandGroupCrecognizedtheimportanceofunderstandinguserintention(i.e.,immediatestepinatask
performance).Intheparticipatorydesign,bothgroupsincludednext-steppredictionasanessentialsystem
designcomponent.Additionally,E1stated,“Wecanunderstandtheattention(andtheintendedaction)ofusers;for
example,duringthecooking,hisattentionisondoingsomethingnotimportant(andwecandraghisattentionback
tothecorrectstep).”E6mentionedaseriesofpossiblefunctionsforinferringshort-termmemorybasedonthe
egocentricview,whichE5opposed,highlightingthatcurrentcomputervisionmethodscannotdothisreliably.
Asaresult,newmethodsarerequiredtoinferuser’sintention.
[KI3] High-levelgoals,ordesire,improvesthetransparencyinthetaskswitching,orMulti-tasking.E1andE2agreed
thattosupportmulti-taskingeffectively,itisnecessaryforuserstoviewtheassistant’sinferredtaskordesire.
ThisideaalignswiththeBDImodel,asmentionedbyE2.WefurthervalidatedthisconceptwithE1-3and
confirmedtheirproposedideaswereconsistent.WealsosharedtheconceptwithE5andE6,andtheyagreeditis
crucialformulti-taskassistancedesign.
[KI4] ModernAImodelsenablenewopportunitiesinunderstandingthecontext,environment,objects,andactions
inanimage.E5hasextensiveexperienceintraditionalcomputervisionmodelsandexpressedconcernsthat
2https://miro.com
ManuscriptsubmittedtoACMSatori:TowardsProactiveARAssistantwithBelief-Desire-IntentionUserModeling 13
currentcomputervisionmodelsmaynotbesufficientduetotheinaccuracyofactionandintentprediction.
Evenifusers’intention(i.e.,immediategoals)canbedetected,predictedintentcannotbefullyusedbecause
thesemodelsoftenlacktheabilitytohandlecomplexscenariosormakeaccuratedecisionsbasedonintent
predictions.E1,whohassignificantexperienceinlargelanguagemodel(LLM)development,suggestedthat
multi-modalLLMslikeGPT-4Vcouldofferasolutionbecauseoftheiradvancedreasoningcapabilities.Giventhe
strongoutcomesofcurrentmultimodalLLMs,exploringonefficientlypromptingthemmayhelptobetterdetect
context,environment,objects,andactionswithitsstrongreasonings.
5 DesignRequirements
Summarizingthekeyfindingstheaforementionedformativestudy,weproposethedesignrequirementsforproactive
ARassistance.
[D1] BDImodelcanbeused(K1)indecidingthetimingandcontentoftheassistant.Ideasfromthebeliefcompo-
nenthelptheassistantfiltertheduplicatedandunnecessaryassistantswhileconsiderationofdesireaddsthe
importanceofthecurrenttask.
[D2] Proactively,ARassistanceshouldprovidemulti-modal,multi-taskassistancebasedontheuser’scurrentintention.
Theassistanceshouldguidetheintendedtaskstepwithauxiliarysupporttoenhancetheuser’scapabilityfor
completingthetask;andtheusershouldbeabletoexitandentryanyothertasksduringthetaskperformance
[KI2].
[D3] ARassistantsshouldcommunicatethesystemstatetotheusertoensuretransparencyandinterpretability[KI3].
Duetotheintricatecomplexityofthephysicalspace,predictingassistanceforphysicaltasksisnon-trivial,and
usersmaylosetrustintheARassistantifunexpectedconditionsoccur.Providingtheimmediatereasoningpaths
leadingtothefinalassistanceisbeneficialforuserstounderstandthesystemstatusandmaintaintrust.
[D4] LLMsareusefultoactivelyperceivetheenvironment,modeltheuser’sactionstatustrackthetaskusingthe
implementedBDImodel,andselectappropriateassistance[KI4].
6 SatoriSystem
Inthissection,wepresentthedesignofourproposedSatorisystem,guidedbythedesignrequirementssummarized
above.SatoriisaproactiveARassistantthatimplementstheBDImodeltoprovidemultimodalassistanceatthe
appropriatetime,usingthemulti-modalLLMandanensembleofvisionmodels.WefirstadapttheBDImodelfortheAR
domainanddescribeourimplementationforpredictingtheBDImodelinSatori.Next,wedetailtheimplementationsfor
timingpredictionandassistanceprediction.Amongthemodalitiesofassistance,imagegenerationisthemostimportant
andnuanced.Therefore,weproposeanovelimagegenerationpipelinebasedonDALLE-3andacustomizableprompting
method.Finally,wedescribeourinterfaceandinteractiondesign,whichenhancestheaffordanceandinterpretabilityof
theassistant.
6.1 ImplementingBDImodelforARAssistance
Tofulfill[D1],weneedtoaligntheBDImodelwiththeconstraintsandaffordancesofAR-mediatedtasks.Todoso,we
mustaccountfortheuniquecharacteristicsofARtechnology,includinglimitedfieldofview,real-timeenvironmental
mapping,andtheblendofphysicalanddigitalinformation.
ManuscriptsubmittedtoACM14 LiandWu,etal.
6.1.1 Belief. Humanbelief isacomplexpsycho-neuralfunctionintegrallyconnectedwithmemoryandcognition[56,
68].PrecisemodelingofhumanbeliefwithintheconstraintsofARtechnologyisnotfeasiblewithoutaccesstohuman
neuralsignals.ForARassistance,primaryinformationsourcesarevisualperceptionandusercommunication.We
proposeatwo-foldmethodfromcapturingscenesandobjectsfromvisualinput,andpastactionhistoryfromuser
communication,toapproximatetheuser’sbeliefstatewithinARconstraints.
Scenesprovideinformationontheuser’scurrenttaskcontextandpotentialshiftsintheirdesiresandintentions.
Forexample,whentheuserlooksattask-unrelatedareas,theassistantcanreduceassistancetoavoiddistraction.We
representthescenebythelabelpredictedbytheimageclassificationmodel.
Objectinformationhelpsuserstolocateandinteractwithrelevantobjects.Weusetwodifferentmodelsforobject
detection.ADetrmodel[13]todetectobjectsinthesceneinzero-shot[13];andLLaVAmodeltodetectobjectsthat
arebeingheld/touched/movedbyhumanhandswhilepromptingitto“detecttheobjectsinteractingwiththehuman
hands”[54].Wedidnotusetheobjectdetectionmodelsinthiscasebecausethesemodelsaretrainedtopredictafixed
setoflabels,limitingthegeneralizability.
Actionandassistanthistoryenablesnon-repeatingguidance;forrepeatedactions,thesystemprogressesfrom
detailedvisualguidestoconcisetextinstructions,reducingcognitiveloadastheusergainsfamiliarity.Thishistory
containsalogofuserinteractionsandtheARassistantresponses,suchasboththetypeofassistanceprovidedand
descriptionsofeachinstance.Thislogservesasareferencetodeterminewhethertheuserhaspreviouslyencountered
specificassistance.Utilizingthishistoricaldataaidsinaccuratelyinferringtheuser’sexpectationsregardingthetype
ofsupporttheyreceive,allowingformoretailoredandeffectiveassistanceinfutureinteractions.
6.1.2 Desire. Desirerepresentstheuser’shigh-levelgoals,suchascleaningaroomororganizingashelf.Weinferthe
user’sdesireusingastandaloneLLMagent,whichanalyzesthecurrentcameraframeandsupportsvoiceinteraction
withtheuser.Voiceinteractionallowstheusertoprovidefeedbackonthepredicteddesire,andthetranscribedtextcan
thenbeusedbytheLLMagenttoadjustthedesireprediction.
6.1.3 Intention. Intentionreferstotheuser’simmediate,low-levelactions,suchasgrabbingabroomtosweep.It
focusesonspecificstepscontributingtobroadergoals.Recognizingintentiscrucialforprovidingaction-oriented
ARassistance.Weinferintentionprimarilythroughperceptualinformation([D1]),includingvisualcuesanduser
interactionswithobjects.Voicecommandsserveasasecondarysource.Thesystemcomparestheintentionwitha
desiretodetectpotentialusererrors.
6.1.4 Intentionforecasting. Topredictuserintentions,weproposeusingamulti-modalLLMtoforecastupcominguser
intentions.Intentionforecastingisinherentlychallengingduetothevastrangeofpotentialfutureactionsandthe
ambiguousnatureofusergoals.Predictingintentionsbetweenhumansisdifficult,andsimilarly,priorexperimentsshow
thatcurrentactionforecastingmodelsstruggleinourscenarioduetomisalignmentwiththelabelset.Weconstrainthe
forecastingprocessbyincorporatinguserdesire,whichhelpsnarrowdowntherangeofpossiblefutureintentions.By
settingaspecificdesireinSec.6.1.2,thesearchspaceforthenextintentionissignificantlyreduced.Wethenpromptthe
LLMtopredicttheintentionwithinthisconstrainedcontext[D4].Thispredictionprocessfollowsasearch-and-reflect
frameworkconsistingofthreestages:
(1) AnalysisStage:TheLLMfirstanalyzesthecurrentdesiredtaskanditscorrespondingtaskplan.Thisinvolves
understandingtheuser’sgoalandbreakingitdownintoactionablesteps,therebyestablishingafoundationfor
anticipatingthenextactions.
ManuscriptsubmittedtoACMSatori:TowardsProactiveARAssistantwithBelief-Desire-IntentionUserModeling 15
(2) PredictionStage:Basedonthecurrentactionandtheestablishedtaskplan,theLLMdeterminesthenext
plausiblesteps.Thisinvolvesleveragingcontextualcuesandunderstandingtypicalsequencesofactionsrelated
tothecurrenttask.
(3) ReflectionStage:TheLLMevaluatesthefeasibilityofthepredictednextstepsbyconsideringtheavailable
objectsandtoolsinthescene.Actionsthatrequiremissingorunavailableobjectsareeliminated,ensuringthat
onlyviableactionsaresuggested.Thisfilteringhelpsrefinethepredictionfurtherbyaligningitwiththeactual
scenecontext,reducingirrelevantorimpossibleoptions.
6.2 BDI-DrivenTimingPrediction
Theassistancetimingisbasicallydeterminedbytheoccurrenceofuserintention.Whentheintentionisdetected,
thecorrespondingassistanceshouldbegeneratedandpresented.However,theusermaybedistractedinthereal
scenario.Therefore,adelayedassistancemechanismisimplementedbasedontheuser’sperceivedscenecapturedby
theego-centriccamera.Toimplement,weuseacombinationofintentionforecastingandEarlyforecastingmechanisms.
Inastandardpipeline,intentionforecastingbeginsonlyafterthepreviousactioniscompleted.Thisapproachcan
negativelyimpactuserexperienceduetothelatency,requiringuserstowaitforthepipelinetofinishprocessing.To
addressthisissue,weproposeanearlyforecastingmechanism.Whiletheintentionforecastingpipelinerunsandcaches
continuously,aparallelprocessearlyforecastingmechanismdetectswhenanactioniscompleted.Oncetheactionis
detectedasfinished,thecachedintentionandthecorrespondingassistanceareimmediatelydisplayed.Thiswaythe
usernolongerhastowaitfortheintentionforecastingpipelinetocomplete,whichisnormallyhighinlatency.
6.2.1 Earlyforecastingmechanismwithactionfinishdetection. Thispipelineaimstominimizewaitingtimes,providing
amoreseamlessandresponsiveinteraction.ItallowstheARassistancetoproactivelyanticipateuserneedsanddeliver
timelyguidanceassoonasanactioniscompleted,ratherthanwaitingfortheforecastingprocesstobeginafterward.
Unlikeconventionalactionrecognitionortemporalgroundingtasks,actioncompletiondetectionlackspre-trained
modelsorlarge-scaledatasets.Toaddressthis,weusethezero-shotlearningcapabilitiesofvision-languagemodelsand
proposeanensemble-basedapproachtobalancelatencyandeffectivenessinpredictingthenextaction.
Implementation:Weuseanin-stepcheckpointtopredicttheactionfinishdetection.Weusecheckpointstomark
whereausersuccesfullyfinishesstepsprogresstowardtaskcompletion.Thesecheckpointsarederivedfromvarious
cues,suchasactiondetectionresults,objectstates,andhand-objectinteractionstates.Ataskplanner(formulatedas
booleanstatements)isusedtogeneratethesecheckpoints.
Toseeifacheckpointisreached,weensemblethelocalimagecaptioningmodelBLIP-2[51]withtheonlineGPT-
4Vmodel.SincetheBLIP-2modelhasloweraccuracy,itspredictionsrequiredoubleconfirmationtobeconsidered
reliable,whiletheGPT-4Vmodelisinherentlytrusted.Thisensemblestrategyoptimizestheperformanceofaction
completiondetectionbyleveragingthestrengthsofbothmodels,ensuringmoreaccurateandtimelyassistanceinthe
ARenvironment.
6.3 Multi-ModalAssistancePrediction
6.3.1 Assistancedesign. ARassistancecancomeindifferentmodalitiessuchassound,text,image,etc[D2].Eachhas
differentfunctionsandshouldbeuseddependingonthescenecontextandusers’actions.Forthat,weimplement:
(1) Textualassistance:isessentialforguidinguserstowardtheirdesiredoutcomes.Weusewhitetextonablack,
transparentcontainertoensurereadability.
ManuscriptsubmittedtoACM16 LiandWu,etal.
(2) Toolandobjectutilityvisualization:Fortasksinvolvingtoolsvisualcuescanaidproperhandlingandsupport
operationtailoredtothetaskcontext.WefocusonautomaticallygeneratingasingleimageusingDALLE-3
modeltorepresentactionandobjectswithillustrationstyles.Formorecomplexactions,wealsoemploymultiple
imagesasanalternative.WeelaboratedthedesignandimplementationinSec6.4.
(3) Timer:Fortasksrequiringspecifictiming(e.g.,boilingwater,monitoringreactions),timeroffersawaytocount.
WeimplementatimerinARtoproactivelycountingtimeforusersduringtime-relatedtasks.
6.3.2 Inferringmodality. Giventhepredictedintention,theARassistanceshouldinferassistancemodalities.For
example,acookchoppingcarrotswhilewaitingforwatertoboilinvolvesmultiplecomponents:timemanagement
(monitoringboilingtime),andtoolutilization(usingaknife,waterpitcher,etc.).TheARassistantcanthenprovide
targetedassistanceforeachofthesecomponents.Thiscomponent-basedapproachenhancesthescalabilityandadapt-
abilityoftheARassistancesystem.Byreusingassistancestrategiesfortasksthatsharesimilarintentioncomponents,
thesystemimprovesefficiencyandconsistencyindeliveringguidance.
6.3.3 Regulatingassistancecontent. Toavoidincreasingtaskloadandreadingtime,theassistantomitsimage-based
assistanceifsimilarcontenthasalreadybeendisplayedinrecentinteractions.Whenanobjectispresentintheuser’s
beliefstate,anindicatorshowingtheobject’slocationinthesceneisprovided.Byaligninggeneratedimageassistance
withtheuser’sscenecontext,thesystemenhancesuserengagement.Theassistanceimagesshouldincorporatescene
informationandbeconsistentwiththecurrentbeliefstate.
6.4 ImageAssistanceGeneration
Imageassistanceaimstoprovideclearandstraightforwardinstructionsfortoolusage.Imageassistanceisgenerated
usingtheDALLE-3modeltoflexiblyadapttocurrentuserstatesandcontexts[55].
Clarityandconcisenessareprioritizedinthedesignofeachimage.Avoidinganyunnecessarydetailsthatmight
confuseoroverwhelmtheuserisimportant.Second,consistentformattingneedstobemaintainedthroughoutall
images.Thesimilardesignstylereducescognitiveloadandhelpsusersquicklygrasptheinstructionswithoutneeding
toadapttoanewformat.
Additionally,someimagesareaction-oriented,withdirectionsexplicitlydemonstratedthroughthearrowstoguide
theuserstepbystep.Thisvisualcueenhancescomprehensionbyfocusingtheuser’sattentionontherequiredactions.
Lastly,animmersiveexperienceisemphasizedbyensuringthattheobjectsdepictedintheimagesareconsistent
withthoseintheuser’sreal-worldenvironment.Thisconsistencyalignswiththeuser’sbeliefstate[D1],allowingfora
moreseamlessandintuitiveinteractionbetweenthesystemandthephysicaltaskathand.
6.4.1 Prompttemplatewithmodifier. Wehavedevelopedaprompttemplateandanassociatedmodifiertaxonomy
usingtheabovedesignconsideration.Theprompttemplateisstructuredtoencapsulateallnecessaryelementsofan
instructionalimageinastandardizedformat:
[Object][Attributes][Action].[Indicator][Attributes][Direction].[Background].[Style Modifier].
[Quality Booster]
Eachelementintheprompttemplate,detailedinTable3,isassociatedwithaspecificmodifiertorefineitsdescription.
ThebasicpromptincludesObject andActionmodifiers,whichdepicttheactionandthetargetedobjectwithinthe
assistanceimage.TheAttributemodifierincorporatesreal-worldattributessuchascolor,shape,andmaterialsofthe
objects,enhancingtaskimmersionandoperationalaccuracy.Additionally,Indicatormodifiers,suchasarrows,and
ManuscriptsubmittedtoACMSatori:TowardsProactiveARAssistantwithBelief-Desire-IntentionUserModeling 17
Modifier Description Example
Object Denotesthemainobjectorthetool button
Indicator Graphicelementthatpointsoutspecificpartsordirections arrow
Attribute Attributesofobjectsorindicators,suchasshapeandcolor red
Action Specifiestheactionbeingperformedbytheobject press
Direction Clarifiesthedirectioninwhichtheactionorattentionisguided pointingtoright
Background Specifiestheabsenceorpresenceofabackground nobackground
StyleModifier Dictatestheartisticstyleoftheillustration instructionalillustration
QualityBooster Enhancementsthatimprovetheoverallclarityandeffectiveness accuracy
Table3. Taxonomyofimageassistancegenerationpromptmodifiers.
Directionmodifiersexplicitlyhighlighttheactiondirectionorfocalpointsforuserattention.TheBackgroundmodifier
removesunnecessarydetails,clarifyingtheimage.TheStyleModifiergovernstheaestheticoftheimage;wechooseto
applythe"flat,instructionalillustration"stylebasedonfeedbackfromexperiments,ensuringthatthevisualsmaintain
aconsistentappearance.Lastly,theQualityBoosterenhancesimagequalitywithdescriptorssuchas"accurate"and
"concise,"furtherensuringclarityandeffectivenessinthevisualassistanceprovided.
6.4.2 Examples. Wepresenttwoexamplesofimageassistancegenerationtodemonstratetheeffectivenessofour
template,asshownFig.3.Pleaserefertothesupplementarymaterialsfordetails.
(a)Ours (b)Raw (c)Ours (d)Raw
Fig.3. ComparisonofnaivelygeneratedimagefromGPTmodel(i.e.,Raw)withourproposedprompt(i.e.,Ours).(a)“Onehand
pressesawhitebuttononawhiteespressomachine.Alargeredarrowpointstothebutton.Nobackground,inthestyleofflat,instructional
illustrations.Accurate,concise,comfortablecolorstyle.”(b)“Onehandpressesawhitebuttononawhiteespressomachine.”(c)“Cutstem
ofaredflowerupfrombottom,withwhitescissorsat45degrees.Onebigredarrowpointingtobottomoftheflowerstem.Inthestyleof
flat,instructionalillustrations.Nobackground.Accurate,concise,comfortablecolorstyle.”(d)“Cutstemofaredflowerupfrombottom
withwhitescissorsat45degrees.”
6.4.3 Implementingimageassistancegeneration. WeproposeusingDalle-3togeneratevisualillustrations.Theimage
generationpipelinereceivesaninitialpromptinput,whichincludestheuser’snext-stepintentionandalistofkey
objects,fromtheguidancegenerationpipeline.Thepipelinefinalizesthepromptbyintegratingtheinformationwith
modifier,asoutlinedintheprompttemplate.Forgeneratingmultipleimagestosupportcomplexillustrations,wefirst
decomposetheuser’snext-stepintention,ifapplicable,andthenformthefinalprompts,whicharesenttotheDalle-3
modelinparallel.
ManuscriptsubmittedtoACM18 LiandWu,etal.
(a)Interfacewithbeliefanddesirestatedisplay. (b)Interfacewithtaskassistanceconfirmationdisplayedontop.
Fig.4. Theinterfacedisplaystheuser’sbeliefanddesirestates,actioncompletionfeedback,andassistancecontent.Inthisexample,
theuserisconnectingagameconsoletothemonitor.Theinterfaceaccuratelyrepresentsthebeliefstateas"Switchdock"and
thedesiredstateas"ConnectSwitch."Theactioncheckpointindicatesthatthecurrentstepiscompleted.(b)Ataskassistance
confirmationappearswhenthesystemdetectsstepcompletion.Theconfirmationpromptstheuser,askingiftheyareabouttousea
coffeefilterandwhethertheyneedimageassistance.
6.5 InterfaceandInteractionDesign
6.5.1 Interfacedesign. AsshowninFigure4,theSatoriinterfacedisplaysthecurrentassistanceandtherelatedBDI
states.Atthetopoftheinterface,Satoripresentsthedesiredtask(e.g.,ConnectSwitchintheexample)andthein-belief
objectindicator(e.g.,Switchdeck),asshowninFigure4(a).TheseBDIstateshelpusersunderstandandtrackthe
assistant’sstate,aligningwithdesignrequirement[D3].Thein-beliefobjectindicatorshowstherelativepositionof
theobjectthattheuserneedstointeractwith.Thearrowindicatestherelativepositionoftheobjectrelativetotheuser.
Theobjectisdetectedinadvanceandstoredinthemodeledbeliefstate,whichisanin-memoryprogramvariable.The
objectwillchangebasedontheuser’sintention,anditslocationwillbeupdatedbasedonobjectdetectionresults.Inthe
centeroftheinterface,asshowninFigure4(a),arethein-stepcheckpointsdescribedinSec.6.2.1.Thesecheckpoints
helpuserstracktheprogressofthecurrentstepofthetask,therebyimprovingtheinterpretabilityofthemodel’s
predictions.Userscancontrolthecheckpointsmanually,andwhenallcheckpointsarereached(representedbygreen
checksymbolsintheexample),theARassistantwilldisplaythenextguidancestep.
6.5.2 Interactiondesign. IntheSatorisystem,wesupportmultipleinteractionsfortheusertofeedbackonthemodel
predictionandadjustthesystemstates.Thevoiceinteractionisusedtoconveytheuser’sintentionanddesiredirectly.
TheARassistantcanadjusttheBDIstatesbasedonthetranscribedtext.Satoripresentsthenextassistancewhenthe
currentactionisdetectedtobefinished.Intheearlyversionofthesystem,wefoundthatuserscouldbeoverwhelmed
whentheactionstatechangedabruptly.Therefore,itisnecessarytodisplayaconfirmationpanelaskingwhetherthe
system’sintentionpredictionmatchestheuser’sintention,asshowninFig.4.Moreover,whenauser’sdesiredtask
involvesmultiplesteps(especiallywhenthenumberofstepsexceedsfive),errorsintheearlierstepsmaypropagateto
laterstepswithouthumancorrection.Forexample,inmakingcoffee,iftheassistantfailstodetectthatthecoffeebeans
havebeenground,itmaycontinuepromptingtheusertogrindthebeans.Toaddressthis,weallowuserstomanually
controltheassistantandprovidefeedbackonthecurrentprediction.Whennewassistanceisdisplayed,theusermust
confirmwhetheritmatchestheirneeds,asshowninFig.4(b)(Looksyouaregoingtoopenacoffeefilterandplaceitin
thebrewer...).Foraccessibility,weprovideanoptionforaudiooutputthroughbuttoncontrol.
ManuscriptsubmittedtoACMSatori:TowardsProactiveARAssistantwithBelief-Desire-IntentionUserModeling 19
Camera View Belief-Desire-Intention Assistance Timing Prediction
Intent Assistance (GPT4)
Use scissors to cut flower stems Type = image

Title: use scissors to cut the stems

Text Content: Cut the flower stems
Dialogue Desire d ui sa ig no gn a sl hl ay r, p a sb co iu st s oa rn s
 inch from the ends,
Dalle3 Prompt: One hand cuts the stem
user: I want to arrange flowers. Arrange Flowers of a red flower up from the bottom with
I am a novice. Can you give me white scissors at 45 degrees. A large
some assistance?

 Belief r be ad c ka gr rr oo uw n dp ,o i in nt s t ht eo st th ye l ec u ot f, fn lo a t,
Satori: Sure! instructional illustrations. Accurate,
Object:
 concise, comfortable color style

Logger [flower, red, center],[vase, glass, Image Assistance 
center][scissors, white, right]...
Time = 48
 Time = 54
 Time = 60

D Ce os fi fr ee e
 = Make D Fe ls oi wr ee r
 = Arrange D Fe ls oi wr ee r
 = Arrange Scene: arrange flowers
Flag = False
 Flag = False
 Flag = True

Type = text
 Type = text
 Type = text

T wi at tl ee r = o vp eo ru r ti hn eg T ti ot l ve a s= e
 Add water T ti ot l ve a s= e
 Add water Assistance History: 
... coffee
Fig.5. OverviewofBDIusermodel.Theinputisthecamera’sview,thedialogue(thevoicecommunicationbetweentheuserandthe
GPTmodel),andthehistoricallogger(priorAssistancealreadyhappened).Theflowchartdisplayshoweachinputistransferred
tothecorrespondingcomponentsintheBDImodel.ThelastlayerofthemodelcontainshowtheparametersintheAssistanceis
affectedbytheBDImodel,forexample,theBelief componentwouldaffectthetimingandfrequencyoftheassistance.
6.6 OtherImplementationDetails
WeimplementedthetimingpredictionandnextassistancepredictionmodulesonGPT-4omodel.Weconcatthelast
fourframesandfeedOpenAI’sAPIwiththepreparedprompttext.Theframeissampledon1FPS.Weimplementedthe
spatialobjectbeliefbyusingtheOWL-ViTmodel[63],whichisthezero-shotobjectdetectionmodel.Thescenebelief
isimplementedwiththezero-shotimagerecognitionmodelCLIPmodel[72].Theactionhistorybeliefispreservedby
anarrayrecordingthedata.WeimplementedourinterfaceonUnitywithMRTKframework.WestreamtheHoloLens
videostreamtoastreamingserver,wherethedifferentdownstreamMLmodulesaretriggeredbythestreamsofdata
andthenrunparallel.
6.6.1 Prompttechniques. WeenhancethereasoningcapabilitiesoftheLLMcomponentsbyapplyingtheChain-of-
Thought(CoT)techniquealongwithin-contextexamples[88].CoTreasoningallowsthemodeltodeliverassistancein
astructuredmannerbysequentiallyfollowinglogicalsteps.ByconceptualizingtheBDImodelasaseriesofthoughts,
themodelcansystematicallyproducetheappropriateassistance.TofacilitateCoT,weencodetheconceptsintheBDI
modelandformalizetheoutputusingXMLformat.Eachthoughtintheprocessismarkedwithahashtag,enabling
themodeltodecomposecomplextasksintomanageablesteps,therebyimprovingtheaccuracyandrelevanceofthe
providedassistance.
7 Evaluation
WeevaluateSatoriprototypethroughanopen-endedexploratorystudy,focusingonthefollowingresearchquestions:
(1) CanSatoriprovidethecorrectassistantcontentattherighttiming?
(2) CanSatoriprovidecomprehensibleandeffectiveguidance?
(3) HowdoesourproposedsystemguideuserscomparedtoguidancemadebyprofessionalARexperts?
ManuscriptsubmittedtoACM20 LiandWu,etal.
(b) Satori: connect Nintendo
(a)Satori:cleanroom Switch (c)WoZ:makecoffee (d)WoZ:arrangeflowers
Fig.6. Studyprocedures.(a)Theparticipantisassemblingamopduringthetaskroomcleaning,followingSatorisystem.(b)
TheparticipantisconnectingaHDMIcabletoaNintendoSwitchdockduringthetaskconnectingNintendoSwitch,following
Satorisystem.(c)Theparticipantispreparingafilterduringthetaskofmakingcoffee,followingWoZsystem.(d)Theparticipantis
trimmingflowerstemsduringthetaskofarrangingflowers,followingWoZsystem.
7.1 Conditions
Participantswerepresentedwithtwoconditions,Wizard-of-OZ(WoZ)andSatori,respectively.Theycompleted
bothconditionsinacounterbalancedordertocontrolforsequencingeffects.Thetasks(indexedas1,2,3,and4)and
conditionswerepresentedinacounterbalancedordertomitigatethelearningeffects.
7.2 StudyProcedure
7.2.1 Participants. Atotalofsixteenparticipants(P01-P16,11male,5female)wererecruitedviaauniversityemail
groupandflyer.Theaverageagewas23.8withthemaximumageat27andaminimumageat21.10/16participants
hadtheARexperiencebeforethestudy.Eachparticipantwascompensatedwitha$30giftcardfortheirparticipation.
Sicknessinformationwascollectedfromparticipantsbothbeforeandafterthestudy,andnosicknesswasobserved
followingthestudy.
7.2.2 Apparatus. WeusedaMicrosoftHoloLens2headsetastheARdeviceforthestudy.TheyusetheSatorisystem
describedearlierwhileperformingthetasks.TheheadsetconnectstoaserverwithaNvidia3090graphicscardtofetch
real-timeresults.
7.2.3 Tasks. ThestudybeganwithabrieftutorialintroducingparticipantstotheARinterfaceoftwosystems.Afterward,
participantswereassignedfourdailytasksdesignedtoevaluatetheguidanceprovidedbybothsystems.Thefour
taskswereinitiallysampledfromWikiHow3andsubsequentlyrewrittentoensureaconsistenttaskloadacrossthem.
FollowingRQ2,weselectedtheseparticulartasksbecausetheyrepresenttypicaldailyactivitiesthatareneithertoo
familiarnortoocomplexfortheaverageuser.
(1) ArrangingFlowers:Participantsarrangedavarietyofflowersintoavase,testingthesystem’sabilitytoprovide
accurateandaestheticguidance.
(2) ConnectingNintendoSwitch:ThistaskinvolvedsettingupaNintendoSwitchwithamonitor,evaluatingthe
system’stechnicalguidanceandtroubleshootingsupport.
(3) RoomCleaning:Participantsassembledamopandaduster,andcleanedthedeskandfloor,wheretheARsystem
suggestedassemblinginstructionandcleaningstrategy.
(4) MakingCoffee:Thistaskrequiredmakingcoffeeusingthepour-overmethod,withtheARassistantproviding
instructionsontoolusageandpouringtechniques.
3https://www.wikihow.com/
ManuscriptsubmittedtoACMSatori:TowardsProactiveARAssistantwithBelief-Desire-IntentionUserModeling 21
TheparticipantscompletedthetaskwiththehelpofWoZsystemorSatorisystem,asshowninFig.6.Aftereachtask
completion,participantsevaluatedtheirexperienceusingausabilityscaleandassessedtheircognitiveloadusing
theNASATaskLoadIndex.Wealsoconductedabrief,recordedinterview,askingparticipantsabouttheadvantages,
disadvantages,usefulness,andtimelinessofthetwosystems.TheexperimentsweresupervisedbytheInstitutional
ReviewBoard(IRB),andalltasksessionswerevideo-recorded.Theserecordingsweresecurelystoredonaninternal
serverinaccessiblefromoutsidetheuniversity.Participantprovidedconsent,andtheirpersonalidentitywasstrictly
protected.Wecollecteddataonparticipants’well-beingbothbeforeandaftertheexperimentandobservednosignificant
adverseeffects.Thedurationoftheentirestudyis2hoursonaverage.Alltheparticipantscompletedthefourtasks
usingbothsystems.
7.3 EvaluationMetrics
ToassesstheperformanceofSatoriandtheuserexperience,weutilizedthefollowingevaluationmetrics.
7.3.1 Usabilityscale. ToanswerRQ1andRQ2,weusedthesystemusabilityscaleconsistingofaseriesofquestions
designedtomeasurethetimeliness,easeofuse,effectivenessandsatisfactionoftheARinterface.Wedesignedour
questionnairebasedon[50]byselectingtheusability-relatedquestionsandaddingsometask-specificquestions.Final
questionsincludeditemssuchasIthinktheguidanceappearedattherightmoment,Icaneasilycomprehendthecontentvia
thetext/audio/imageguidance,Theguidance’scontentishelpfulincompletionthetask,Ifoundthattheguidanceaccurately
reflectsmytaskintentions,andOverall,Ithinkthesystemcanbebeneficialinmyeverydaylife.Intotal,thereare11
questions.Responseswerecollectedonaseven-pointLikertscalefromstronglydisagreetostronglyagree,allowingus
toquantitativelygaugeusersatisfactionandsystemusability.Wecomputedthemeanandconfidenceintervalsfor
eachquestionusingthebootstrappingmethod.Specifically,1,000bootstrapsamplesweregeneratedfromtheoriginal
datasettocomputethemeanofeachsample.The95%confidenceintervalswerederivedfromthepercentilesofthe
bootstrapdistributiontoprovidearobustestimateoftheuncertaintyaroundthemean.
7.3.2 NASAtaskloadindex. TofurtheranswerRQ2,wealsousedtheNASATLXformconsistingofaseriesofscalesto
evaluatethecognitiveloads.Theevaluationincludesmentaldemand,physicaldemand,temporaldemand,performance,
effort,andfrustration.Theyareratedwithina100-pointrange.Wecalculatedthemeanandconfidenceintervalsfor
eachquestionusingthebootstrappingtechnique.Inthisprocess,1,000bootstrapsamplesweredrawnfromtheoriginal
datasettoestimatethemeanforeachsample.The95%confidenceintervalswerethenobtainedfromthepercentilesof
thebootstrapdistribution,offeringareliablemeasureoftheuncertaintysurroundingthemean.
7.4 DataAnalysis
ForNASA-TLXandusabilityscale,wefirstaveragethemeasuredtaskloadsacrossthefourtasksforeachparticipant
undereachcondition.ThenweconducttheWilcoxonSigned-Ranktestontheaverageddatatotestwhetherthereisa
significanceinthetaskloadsbetweentheWoZconditionandSatoricondition.
7.4.1 Wilcoxonsigned-rankone-sidedtest. SimplyverifyingthatthereisnosignificantdifferencebetweenSatoriand
WoZconditiondoesnotensurethatthetwoconditionsaresimilar.Instead,weaimtotestwhetherSatoriisnoworse
thantheWoZbyapredefinedmarginΔ.Toachievethis,weuseaone-sidedWilcoxonsigned-ranktest,
Thetestdefines𝐷
𝑖
=𝑋 𝐴𝑖−𝑋 𝐵𝑖asthedifferencebetweenthescoresforeachparticipant𝑖underConditions𝑆(Satori)
andW(WoZ),respectively.Theadjusteddifferenceaccountingforthemarginisgivenby𝐷 𝑖′ =𝐷 𝑖−Δ=𝑋 𝐴𝑖−𝑋 𝐵𝑖−Δ.
ManuscriptsubmittedtoACM22 LiandWu,etal.
Vanilla Non-Inferiority
Question Condition Mean 95%CI
W 𝑝-value W 𝑝-value
[Q1]Icaneasilycomprehendcontent Satori 6.25 [6.00,6.75]
26.500 0.099 89.500 0.001
viatext/audio/imageguidance. WoZ 5.94 [5.25,6.50]
[Q2]Theguidance’scontentishelpful Satori 6.22 [5.75,6.75]
26.000 0.094 131.000 0.000
incompletionthetask. WoZ 5.80 [5.50,6.50]
[Q3]Ithinktheguidanceappearatthe Satori 5.97 [6.00,6.50]
17.500 0.090 125.000 0.001
rightmoment. WoZ 5.53 [5.00,6.25]
[Q4]Ifoundthattheguidance Satori 6.48 [6.00,7.00]
11.500 0.016 134.500 0.000
accuratelyreflectsmytaskintentions. WoZ 5.95 [5.62,6.50]
[Q5]Theguidanceappearsata Satori 6.23 [5.88,6.75]
15.000 0.032 131.500 0.000
adequatelocation. WoZ 5.66 [5.25,6.25]
[Q6]Iamabletocompletemywork Satori 6.08 [5.50,6.62]
30.000 0.273 108.500 0.003
quicklyusingthissystem. WoZ 5.75 [5.25,6.50]
[Q7]Itwaseasytolearntousethis Satori 6.48 [6.00,7.00]
22.000 0.179 103.500 0.001
system. WoZ 6.06 [5.75,7.00]
[Q8]HowengagedIamusingthe Satori 6.16 [5.88,6.50]
20.500 0.145 109.500 0.002
system? WoZ 5.75 [5.38,6.50]
[Q9]Thesystem’sguidancematches Satori 6.27 [6.00,6.75]
32.500 0.357 91.000 0.001
thecontext. WoZ 6.05 [5.62,7.00]
[Q10]Overall,thesystem’sguidance Satori 6.30 [5.75,6.75]
30.000 0.156 97.500 0.002
frequencyissuitable. WoZ 5.92 [5.75,6.50]
[Q11]Overall,Ithinkthesystemcan Satori 5.94 [5.50,6.50]
30.000 0.277 105.500 0.005
bebeneficialinmyeverydaylife. WoZ 5.58 [5.25,6.25]
Table4. ResultsforNASATLXQuestionsandNon-InferiorityTests.Thetablesummarizesthemeanscoresand95%confidence
intervals(CI)foreachsystem(ourSatorisystemandWoZdesignedbyARdesigner)acrossvarioususerexperiencequestions
relatedtosystemusabilityandcognitiveload.The“Vanilla”columnsprovidetheWilcoxonsigned-ranktestresults(Wstatisticand
p-values)forsignificantdifferencesbetweensystems.The“Non-Inferiority”columnsshowWstatisticsandp-valuestestingifSatori’s
performanceisnon-inferiortoWoZwithinasetmargin.Green-highlightedcellsindicateestablishednon-inferiority,suggestingthat
SatoriperformsaswellasorbetterthanWoZinuserguidanceandcognitivedemand.
Thehypothesesforthisnon-inferioritytestare:
𝐻 0:median(𝐷′) >0 (AisworsethanBbymorethanΔ),
𝐻 1:median(𝐷′) ≤0 (AisnoworsethanBbyatmostΔ).
SimilartothevanillaWilcoxonsigned-ranktest,theprocedureinvolvesrankingtheabsoluteadjusteddifferences
|𝐷 𝑖′|,calculatingthesumofranksforpositive(𝑊+)andnegative(𝑊−)differences,andusingtheteststatistic𝑊 =
min(𝑊+,𝑊−)tocomputeaone-sidedp-value.Thisp-valueindicateswhetherwecanreject𝐻 0infavorof𝐻 1.We
chosethemarginvalueΔ 𝑇𝐿𝑋 =2.5forNASATLXandΔ 𝑢𝑠 fortheusabilityscaleastheyrepresenthalfoftherating
interval.
8 UserStudyResult
8.1 UsabilityScaleResult
Wepresenttheparticipants’rawscaledataacrossthedifferenttasksinFigure8andprocessedstatisticsinTable4.The
usabilityofSatoriwasassessedusingaseriesofquestions(Q1-Q11)relatedtotheeaseofcomprehension,helpfulness,
ManuscriptsubmittedtoACMSatori:TowardsProactiveARAssistantwithBelief-Desire-IntentionUserModeling 23
Fig.7. NASATLXboxplotforWoZandSatori.Theplotillustratesthedistributionofcognitiveloadratingsacrosssixdimensions:
MentalDemand,PhysicalDemand,TemporalDemand,Performance,Effort,andFrustration.Eachboxrepresentstheinterquartile
range(IQR)withthemedianmarkedbyahorizontalline,showingthevariabilityandcentraltendencyofparticipants’workload
ratingsforbothsystems.ThecomparisonhighlightsdifferencesinperceivedworkloadbetweentheWoZandSatoriconditions,
providinginsightsintotheeffectivenessandusabilityofeachapproach.
timeliness,andoveralleffectivenessofthesystem’sguidance.Formostquestions,therewasnosignificantdifference
betweenSatoriandtheWoZcondition,asindicatedbyp-valuesabovetheconventionalsignificancethreshold(e.g.,Q1:
𝑝 =0.099,Q2:𝑝 =0.094,Q3:𝑝 =0.090,Q6:𝑝 =0.273).However,non-inferioritytestsdemonstratedthatSatoriwasnot
worsethantheWoZconditionforthesequestions,withp-valueswellbelow0.05(e.g.,Q1:𝑝 =0.001,Q2:𝑝 =0.000,Q6:
𝑝 =0.001)andamargin𝛿 =0.5.Belowwewillanalyzethefine-grainedresults.
8.1.1 Satoricanprovidethetimelyassistance. Satoridemonstratesitscapabilitytoprovidetimelyguidancetousersby
dynamicallyaligningitsassistancewiththeuser’scurrenttaskcontext.ThisresultisreflectedbyQ3(𝑝 =0.0.90and
𝑝
𝑛𝑜𝑛_𝑖𝑛𝑓𝑒𝑟𝑖𝑜𝑟𝑖𝑡𝑦
=<0.05)andQ11(𝑝 =0.357and𝑝
𝑛𝑜𝑛_𝑖𝑛𝑓𝑒𝑟𝑖𝑜𝑟𝑖𝑡𝑦
=<0.05).Specifically,Satoriachivedthehigheruser
scalecomparedtotheWoZcondition.Forinstance,P6notedthat“thereisnocircumstance,wheremyintentiondoesnot
alignwiththeprovidedguidance,”underscoringtheperceivedreliabilityofSatoriinunderstandingandanticipatinguser
needs.Furthermore,theBDImodelhelpsregulatetheassistancetimingandeliminateduplicatedguidance,contributing
toabetteruserrating.
8.1.2 Satoricanprovidecomprehensibleandeffectiveguidancethroughthemulti-modalcontentandintention-relatedtask
instructions. ReflectedbyQ2(𝑝 =0.094and𝑝 𝑛𝑜𝑛_𝑖𝑛𝑓𝑒𝑟𝑖𝑜𝑟𝑖𝑡𝑦 =<0.05),Satoriprovideshelpfulcontentforguidingthe
usertoachievetheirdesiredtask.Wefindthattheparticipantshighlyappreciatethemulti-modalcontent’seffectiveness
inthisguidance.Forexample,P3stated,“Ilikedthatitcombinesthevariousmodalitiesoftext,audio,andimagetogenerate
guidance,IbelievethatwashelpfulonmultipleoccasionswhereImighthavebeenuncertainwithonlyasinglemodality.”
Furthermore,theassistancematchingtheparticipant’sbeliefstatecontributestotheeffectivenessofthesystem.P14
commented,“Theguidancehelpsmealot,especiallyincoffeemaking.Itprovidesmewithverydetailedinstructions
includingtime,andamountofcoffeebeansIneed.IwouldhavetogoogleitifIdon’thavetheguidance.”Similarly,P8
appreciatedtheassistanceforthedetailedguidance,noting,“Fortasklikearrangingtheflowervase,theintricatedetails
ManuscriptsubmittedtoACM24 LiandWu,etal.
Fig.8. Userratingsonthesystemusabilityusingaseven-pointLikertscale.Therearetwelveparticipantsinthestudyandtheratings
areencodedbythecolorfromredtoblue.ThefigurecomparestheresponsesforSatoriandWoZsystemsacrossfourtasks:Arranging
Flowers,MakingCoffee,CleaningtheRoom,andConnectingaConsole.Eachbarrepresentsthedistributionofresponsesfora
specificusabilityquestion,highlightingdifferencesinusersatisfaction,comprehensibility,andtasksupportprovidedbybothsystems.
liketrimtheleaves,cuttingthestemat45degreeetc.areverynecessarydetailsthatImightnothaveperformedonmy
own.”ThecomprehensibilityoftheassistancecontentisprovedbyQ1(𝑝 =0.099and𝑝
𝑛𝑜𝑛_𝑖𝑛𝑓𝑒𝑟𝑖𝑜𝑟𝑖𝑡𝑦
=<0.05).The
imagecontentiscomprehensiblewithourcarefullydesignedimageprompts.Forexample,P1noticedthat“thepicture
ofthesecondoneisveryniceanditlooksgood.”WealsonoticethattheWoZalsobearsimpressiveaccessibility,asP8
remarked,“Guidanceasawhole(text,images,andanimations)wasveryhelpful.Whereas,textaloneasshowninthe
imagelacksinformation.”
8.1.3 SatoridemonstratescompetitivenesswiththeWoZcondition,achievingsimilareffectivenessintaskguidancewhile
fosteringamoreengagingandtrustworthyuserexperience. Weobservethatforallquestions,Satoridemonstratesa
non-inferiorresultcomparedtotheWoZcondition(𝑝
𝑛𝑜𝑛_𝑖𝑛𝑓𝑒𝑟𝑖𝑜𝑟𝑖𝑡𝑦
< 0.05).GiventhattheWoZwasdesignedby
expertswhocarefullyconsideredthetimingandcontentoftheassistanceforthespecificscenario,thisresultindicates
thatSatoricanachievepromisingoutcomeswithoutrequiringextensivemanualeffort,thusdemonstratingbetter
generalizability.ParticipantfeedbackunderscoresthisincreasedengagementfacilitatedbySatori’sdesign.Forexample,
P3remarked,“Notasingularcomponentbyitself,butallcomponentstogetherdomakememoreengaged,”indicating
thatthecombinationoftransparency,interpretability,andcontextualguidancecreatesacohesiveandcompellinguser
experience.Similarly,P10expressedasenseofactiveinvolvementinthetask,stating,“Yes.Itmayautomaticallydetect
ManuscriptsubmittedtoACMSatori:TowardsProactiveARAssistantwithBelief-Desire-IntentionUserModeling 25
myprogresstomakememoreengagedinthetask.”Thissentimentreflectshowthesystem’sautomaticactionfinish
displayalignswithuserexpectationsandpromotesasenseofpartnershipbetweentheuserandSatorisystem.While
SatorieffectivelymatchestheguidanceperformanceoftheWoZandexcelsinfosteringengagement,itdoeshave
limitationsinthemodalitiesofguidanceprovided.UnliketheWoZ,whichutilizesmorevariedanddynamicmodalities,
suchasanimationsandwarnings,Satori’sguidanceiscomparativelystraightforward.P1pointedout,“Theanimationis
reallyclear,”highlightingthebenefitofanimatedguidancethattheWoZoffers,whichisonlyavailableinWoZsystem.
8.1.4 Perceivedlatencyandsystemerrors. EventhoughSatorimanagestoreducelatencythroughtheearlyforecasting
mechanism,thelatencyisstillobservabletoparticipants.P8mentioned,“mostofthetimethesystemknowswhatIhave
doneinthepaststepeventually,butIwishitcouldbemoreresponsivesoIdon’tneedtowaitforthesystemtorecognize
whatIhavedone.”Thislatencyismainlycausedbytheactionfinishdetectionmodule,whichsharesGPUresourceswith
thebeliefpredictionmodule.WenoticethatthedatatransferbetweentheGPUandCPUcansometimesleadtolatency
intheactionfinishdetectionmodule.Wecouldeliminatethislatencybyaddingmorecomputingresourcestoavoid
memorysharingbetweendifferentmodules.Anothertechnicalchallengecontributingtothesedelaysisthelimited
fieldofview(FoV)oftheHoloLens2,whichaffectsthesystem’sabilitytocapturecompletecontextualinformation
whentheuser’sheadpositionchanges,suchaslookingup.Insuchcases,thecameramayfailtodetectrelevantscene
information,resultinginatemporaryguidancemismatch.However,wealsonoticethatSatori’sinteractionmechanisms,
whichallowuserstomanuallyadjustorcorrectthesystem’sunderstanding,canhelpparticipantsaddressthesystem
latencymanually.Forexample,P12mentioned,“Ilikethecheckmarkandtheprevious/nextstep.”
8.1.5 SatorishowspromisingresultsforbuildingaproactiveARassistantineverydaylife. Q11(𝑝 =0.277and𝑝 𝑛𝑜𝑛_𝑖𝑛𝑓𝑒𝑟𝑖𝑜𝑟𝑖𝑡𝑦 <
0.05)indicatesthatparticipantsbelieveSatorihasthepotentialtobegeneralizedtoeverydayscenarios.Forexample,
P9mentioned,“maybewhenweneedtoassemblefurniture,insteadofgoingthroughthemanualbackandforthallthe
time,wecanjusthavethissystemtoguideus.”SinceSatoridoesnotrelyonaspecificlabelsetormanualconfiguration,
itcanbedirectlyappliedtonewtasks.Furthermore,Q7(𝑝 =0.179and𝑝
𝑛𝑜𝑛_𝑖𝑛𝑓𝑒𝑟𝑖𝑜𝑟𝑖𝑡𝑦
<0.05)suggeststhatSatorihas
aflatlearningcurve,whichwassupportedbyparticipantfeedback,asmostparticipantsacknowledgedtheywouldnot
needadditionaltraining.SucheaseoflearningopensopportunitiestoturnSatoriintoasystemforlearningnewtasks,
asP10mentioned,“(Thesystemcanbeusedfor)learningtocompleteadifficulttask.”
8.2 NASATLXResult
We present the test results in Table 5 and the mean values in Figure 7. When there is no significant difference
betweenSatoriandWoZonallTLXmeasures,Satoriachivessignificantlyno-worsethantheWoZonperformanceand
frustration.
8.2.1 MentalDemand,physicaldemand,andtemporaldemand. FortheMentalDemanddimension,thereisnosignificant
differencebetweenSatoriandtheWoZcondition(𝑝 =0.744).The95%confidenceintervalforSatoriis[17.50,43.81],
whilefortheWoZcondition,itis[16.25,50.00].Theseoverlappingconfidenceintervalssuggestsimilarlevelsofmental
demandperceivedbyparticipantsinbothconditions.Asimilarpatternisreflectedinthephysicaldemand(𝑝 =0.776
and𝑝 𝑛𝑜𝑛_𝑖𝑛𝑓𝑒𝑟𝑖𝑜𝑟𝑖𝑡𝑦 = 0.281).Weobservethatthephysicaldemandmaybeattributedtotheincreasedinteraction
requiredbySatori.AsshowninSec.6.5,Satoriexploitsinteractionforintentionconfirmationandassistancecontrol.
SuchinteractionswiththebuttonscanbechallenginginHoloLens2,requiringparticipantstoperformtheclicking
actionmultipletimes.Forexample,P9mentioned,“thebiggestdifficultyformewasusingtheARdevice.Itwasfrustrating
ManuscriptsubmittedtoACM26 LiandWu,etal.
Vanilla Non-Inferiority
Question Condition Mean 95%CI
W 𝑝-value W 𝑝-value
Satori 34.06 [17.50,43.81]
MentalDemand 60.500 0.744 78.000 0.316
WoZ 33.12 [16.25,50.00]
Satori 32.34 [11.25,50.00]
PhysicalDemand 55.000 0.776 80.000 0.281
WoZ 30.47 [10.00,43.75]
Satori 28.52 [21.25,37.50]
TemporalDemand 46.000 0.274 65.000 0.388
WoZ 26.41 [15.00,32.50]
Satori 16.17 [7.50,21.25]
Performance 44.000 0.593 95.500 0.022
WoZ 17.27 [7.50,20.00]
Satori 28.20 [17.50,37.50]
Effort 52.500 0.464 58.500 0.353
WoZ 26.02 [15.00,36.25]
Satori 19.84 [10.00,28.75]
Frustration 41.500 0.175 126.000 0.001
WoZ 26.95 [11.25,34.38]
Table5. ResultsforNASATLXQuestionsandNon-InferiorityTests.Thistableshowsthemeanscoresand95%confidenceintervals
(CI)forSatoriandWoZsystemsacrosssixdimensions:MentalDemand,PhysicalDemand,TemporalDemand,Performance,Effort,
andFrustration.TheVanillaWilcoxonsigned-ranktestresultsandnon-inferioritytests(highlightedingreen)indicatewhetherthe
SatorisystemperformscomparablyorbetterthantheWoZsystemintermsofcognitiveload.
whenIfailedtoclickonthebuttonmultipletimes.”TheTemporaldemandshowsasimilarpattern(𝑝 = 0.274and
𝑝
𝑛𝑜𝑛_𝑖𝑛𝑓𝑒𝑟𝑖𝑜𝑟𝑖𝑡𝑦
=0.388),indicatingthatSatoridoesnotincreasethetimepressureforcompletingthetask.
8.2.2 Performance. TheperformancedimensionshowsnosignificantdifferencebetweenSatoriandtheWizard-of-Oz
condition(𝑝 =0.593).Furthermore,thenon-inferioritytestindicatesthatSatoriisnoworsethantheWizard-of-Oz
condition(𝑝 =0.593and𝑝
𝑛𝑜𝑛_𝑖𝑛𝑓𝑒𝑟𝑖𝑜𝑟𝑖𝑡𝑦
<0.05),strictlyverifyingthatSatoriprovidesaperformanceexperiencethat
isequivalenttothatoftheWoZcondition.
8.2.3 Effort. RegardingEffort,thereisnosignificantdifferencebetweenSatoriandtheWoZcondition(𝑝 =0.464and
𝑝
𝑛𝑜𝑛_𝑖𝑛𝑓𝑒𝑟𝑖𝑜𝑟𝑖𝑡𝑦
=0.353).Webelievethisisreasonable,asmostoftheeffortliesintaskcompletionratherthanusing
theARassistantinterface.
8.2.4 Frustration. Satoriachievedstrictlyno-worse-thanperformancecomparedtotheWoZcondition(𝑝 =0.175and
𝑝
𝑛𝑜𝑛_𝑖𝑛𝑓𝑒𝑟𝑖𝑜𝑟𝑖𝑡𝑦
<0.05).Furthermore,themeanvaluessuggestthatSatorihaslowerfrustration(𝑚𝑒𝑎𝑛
𝑆𝑎𝑡𝑜𝑟𝑖
=19.84
and𝑚𝑒𝑎𝑛 𝑊𝑜𝑍 = 26.94).Webelievethiscanbeexplainedbythemoreengaginguserexperienceandthesystem’s
transparency.AsP5mentioned,“It(Satori)givesmetheimpressionthatthemachineunderstandswhatI’mdoing,making
itsinstructionsfeeltrustworthy.”ThisalignswithourfindingsinSec.8.1.3.
9 Discussion
9.1 TowardstheProactiveARAssistant
OurSatorisystemrepresentsanearlyattempttoprovideappropriateassitanceattherighttime.Thestudyfindingsin
timing,comprehensibility,andcognitiveloadalldemonstratedthatSatoriperformssimilartoARassistancecreatedby
humandesigners.However,thereisstillsubstantialroomforimprovementtofullyrealizethevisionofatrulyproactive
ARassistant.Forexample,thecurrentsystemstillhasalatencyabout2-3seconds,limitingapplicationtonon-rapid
performance.Wedescribeinsightsbelow:
ManuscriptsubmittedtoACMSatori:TowardsProactiveARAssistantwithBelief-Desire-IntentionUserModeling 27
9.1.1 Abroaderrangeofassistancemodalitiesisneededtoaccommodatethediversityofhumantasks. Humantasks
areinherentlydiverse,rangingfromhighlycognitiveactivitiestomorephysicaltasks[?].Therefore,expandingthe
modalitiesofassistancecouldsignificantlyenhancethesystem’sadaptabilityandusability.Forexample,incorporating
additionalmodalitiessuchasauditorywarnings,dynamicanimations,andspatialobjectindicatorscouldprovidemore
intuitiveandimmediatefeedback,especiallyintasksrequiringquickreflexesorspatialawareness.Thesemodalities
canhelpbridgethegapbetweenvirtualguidanceandreal-worldtaskexecutionbyaligningtheassistancemoreclosely
withthespecificdemandsofthetask.
9.1.2 Greateraccesstoenvironmentalinformationiscrucialforpredictingthetimingandcontentoftheassistance. Cur-
rently,theHololens2setuponlyprovidesanegocentricviewoftheuser’senvironment,whichrestrictstheinformation
availabletothemodel.Thislimitationconstrainstheperformanceofoursystem,particularlyinunderstandingthe
broadercontextoftheuser’ssurroundingsandinteractions.Enhancingthesystemwithadditionalenvironmental
sensingcapabilities,suchasbettercamerasorthird-personviews,couldprovideamoreholisticunderstandingof
theenvironment.ThiswouldallowtheARassistanttooffermorepreciseandcontextuallyappropriateguidance.For
instance,athird-personviewcouldhelpinscenarioswheretheuser’simmediateperspectiveislimitedorobstructed,
enablingthesystemtoinfercriticalinformationthatwouldotherwisebemissed[46].
9.1.3 Considerationofcollaborativeinteractionsandsocialdynamicsiscrucialforfuturedevelopment. Humansoften
performtasksingroupsettings,whereindividualintentionsareinfluencedbysocialinteractions,suchascollaboration,
negotiation,andsharedgoals.Inourformativestudies,theARdesignersmentionedthatthetrulyproactiveARassistant
shouldbecapableofrecognizingthesesocialcontextsandadaptingitsguidancetosupportnotonlyindividualusersbut
alsothegroupasawhole.Othernon-ARstudiesalsoconfirmtheimportanceofcollaborativeinteractionindesigning
anAIassistant[?].
9.2 LimitationandFutureDirections
Theprimarylimitationliesintheincompletepredictionofuserbeliefs(i.e.,surroundingobjects,history,andactions).
Inpsychology,humanbeliefsarehighlycomplexandnuanced,andourcurrentimplementationonlypartiallycaptures
thiscomplexity.Researchincognitivepsychologysuggeststhathumanbeliefsareinfluencedbyavarietyoffactors
includingpersonalexperiences,socialinfluences,andcognitivebiases[30,77].Thus,oursystemmaybenefitfrom
incorporatingmoresophisticatedmodelsofbeliefformationandupdating,drawingfrominterdisciplinaryresearchon
cognitivescienceanddecision-making.
Additionally,ourimplementationreliesontheGPTmodel,whichsuffersfromlatencyissues.Althoughweexperi-
mentedwithLLaVAinourpreliminarystudies,theresultswerenotsatisfactory.Toaddressthis,weplantoexplore
alternativemodelsandtechniquesforimprovingtheefficiencyandresponsivenessofoursystem.Thismayinvolve
fine-tuningexistingmodelsorinvestigatingnovelarchitecturesthatbettersuitourapplicationdomain.
Furthermore,thelimitedfieldofviewprovidedbydevicessuchastheHololensposesachallengeforobjectdetection
andpredictionaccuracy.Thislimitationbecomesparticularlyevidentwhenusersraisetheirheadstoviewvisualcontent,
asthecameramaynotbecenteredontherelevanttaskobjects.Futureiterationsofoursystemcouldexplorestrategies
tomitigatethisissue,suchasincorporatingmulti-viewobjectdetectionalgorithmsoroptimizingtheplacementof
virtualelementswithintheuser’sfieldofview.
ManuscriptsubmittedtoACM28 LiandWu,etal.
9.3 DesignLessons
IndesigningSatori,wewentthroughmultipleiterativestages.OurinitialgoalwastobuildanARguidancesystemthat
providedstep-by-stepinstructionsbasedonpredefinedrecipes.However,thisapproachprovedtohavelimitedusability
duetothelackofgeneralizabilityofpredefinedrecipesandthedifficultyincreatingthem.Toaddressthis,weshifted
ourfocustosupportingamoreflexibletaskmodelusingtask-agnosticmachinelearningmodels.Weexperimentedwith
variousimplementationsbasedonobjectdetection,actiondetection,andourcurrentbackboneGPTmodels.Amajor
challengeweencounteredwascascadingerrors,whereanerrorinonestepcouldleadtofurthererrorsinsubsequent
steps.Forinstance,ifanearlierstepisnotdetectedascompletedandtheuserproceedstothenextstep,thesystem
mightfailtodetectthistransitionandremainstuck,astheuser’sactionsnolongercorrespondtotheexpectedstep.To
addressthisissue,weintroduceduserinteractionintothesystem.Althoughweobservedthatthisinteractioncould
increasetaskloadandleadtosomenegativefeedback,italloweduserstoprovidefeedbackonmodelpredictions,
therebyimprovingpredictionaccuracyintheremainingsteps.
10 Conclusion
WepresentedSatori,aproactiveARassistantsystemthatintegratestheBDImodelwithadeepmodel-LLMfusion
architecturetoprovidecontext-aware,multimodalguidanceinARenvironments.Ourresearchaddressesthecurrent
limitationbyfocusingonproactiveassistancethatcanunderstanduserintentions,anticipateneeds,andprovidetimely
andrelevantguidanceduringcomplextasks.Throughtwoformativestudiesinvolvingtwelveexperts,weidentified
keydesignrequirementsforadaptingtheBDImodeltotheARdomain,emphasizingtheimportanceofunderstanding
humanactions,surroundingobjects,andtaskcontext.Buildingonthesefindings,Satoriwasdevelopedtoleveragethe
BDImodelforinferringuserintentionsandguidingthemthroughimmediatenextsteps.Ourempiricalstudywith
sixteenusersdemonstratedthatSatoriperformscomparablytodesigner-createdARguidancesystemsintermsof
timing,comprehensibility,usefulness,andefficacy,whilealsoofferinggreatergeneralizabilityandreusability.The
resultsofourresearchindicatethatcombiningtheBDImodelwithmulti-modaldeeplearningarchitecturesprovidesa
robustframeworkfordevelopingproactiveARassistance.Bycapturingbothuserintentionsandsemanticcontext,
SatorireducestheneedforcustomizedARguidanceforeachspecificscenario,addressingthescalabilitychallenges
facedbycurrentARsystems.OurworkopensnewpossibilitiesforenhancingARexperiencesbyprovidingadaptive,
proactiveassistancethatcansupportuserseffectivelyacrossawiderangeoftasksandenvironments.
References
[1] DejaniraAraiza-Illan,TonyPipe,andKerstinEder.2016.Model-basedtesting,usingbelief-desire-intentionsagents,ofcontrolcodeforrobotsin
collaborativehuman-robotinteractions.arXivpreprintarXiv:1603.00656(2016).
[2] JamesBaumeister,SeungYoubSsin,NevenAMElSayed,JillianDorrian,DavidPWebb,JamesAWalsh,TimothyMSimon,AndrewIrlitti,RossT
Smith,MarkKohler,etal.2017.Cognitivecostofusingaugmentedrealitydisplays.IEEEtransactionsonvisualizationandcomputergraphics23,11
(2017),2378–2388.
[3] DavidBenyonandDianneMurray.1993.Adaptivesystems:fromintelligenttutoringtoautonomousagents.Knowl.BasedSyst.6,4(1993),179–219.
https://doi.org/10.1016/0950-7051(93)90012-I
[4] DanBohus,SeanAndrist,NickSaw,AnnParadiso,IshaniChakraborty,andMahdiRad.2024.SIGMA:AnOpen-SourceInteractiveSystemfor
Mixed-RealityTaskAssistanceResearch–ExtendedAbstract.In2024IEEEConferenceonVirtualRealityand3DUserInterfacesAbstractsandWorkshops
(VRW).IEEE,889–890.
[5] RafaelHBordini,AmalElFallahSeghrouchni,KoenHindriks,BrianLogan,andAlessandroRicci.2020.Agentprogramminginthecognitiveera.
AutonomousAgentsandMulti-AgentSystems34(2020),1–31.
[6] DiegoBorro,ÁngelSuescun,AlfonsoBrazález,JoséManuelGonzález,EloyOrtega,andEduardoGonzález.2021. WARM:WearableARand
tablet-basedassistantsystemsforbusmaintenance.AppliedSciences11,4(2021),1443.
ManuscriptsubmittedtoACMSatori:TowardsProactiveARAssistantwithBelief-Desire-IntentionUserModeling 29
[7] CarolaBotto,AlbertoCannavò,DanieleCappuccio,GiadaMorat,AmirNematollahiSarvestani,PaoloRicci,ValentinaDemarchi,andAlessandra
Saturnino.2020. AugmentedRealityfortheManufacturingIndustry:TheCaseofanAssemblyAssistant.In2020IEEEConferenceonVirtual
Realityand3DUserInterfacesAbstractsandWorkshops,VRWorkshops,Atlanta,GA,USA,March22-26,2020.IEEE,299–304. https://doi.org/10.1109/
VRW50115.2020.00068
[8] MichaelBratman.1987.Intention,plans,andpracticalreason.(1987).
[9] LarsBraubach,AlexanderPokahr,andWinfriedLamersdorf.2005.Jadex:ABDI-agentsystemcombiningmiddlewareandreasoning.InSoftware
agent-basedapplications,platformsanddevelopmentkits.Springer,143–168.
[10] VirginiaBraunandVictoriaClarke.2012.Thematicanalysis.AmericanPsychologicalAssociation.
[11] AndreiBroder.2002.Ataxonomyofwebsearch.InACMSigirforum,Vol.36.ACMNewYork,NY,USA,3–10.
[12] PaoloBusetta,RalphRönnquist,AndrewHodgson,andAndrewLucas.1999. Jackintelligentagents-componentsforintelligentagentsinjava.
AgentLinkNewsLetter2,1(1999),2–5.
[13] NicolasCarion,FranciscoMassa,GabrielSynnaeve,NicolasUsunier,AlexanderKirillov,andSergeyZagoruyko.2020.End-to-endobjectdetection
withtransformers.InEuropeanconferenceoncomputervision.Springer,213–229.
[14] KarenChurchandBarrySmyth.2009.Understandingtheintentbehindmobileinformationneeds.InProceedingsofthe14thinternationalconference
onIntelligentuserinterfaces.247–256.
[15] PhilipRCohenandHectorJLevesque.1990.Intentionischoicewithcommitment.Artificialintelligence42,2-3(1990),213–261.
[16] J.D’Agostini,L.Bonetti,A.Salee,L.Passerini,G.Fiacco,P.Lavanda,E.Motti,MicheleStocco,K.T.Gashay,E.G.Abebe,S.M.Alemu,R.Haghani,A.
Voltolini,ChristopheStrobbe,NicolaCovre,G.Santolini,M.Armellini,T.Sacchi,D.Ronchese,C.Furlan,F.Facchinato,LucaMaule,PaoloTomasin,
AlbertoFornaser,andMariolinoDeCecco.2018.AnAugmentedRealityVirtualAssistanttoHelpMildCognitiveImpairedUsersinCookinga
SystemAbletoRecognizetheUserStatusandPersonalizetheSupport.In2018WorkshoponMetrologyforIndustry4.0andIoT,Brescia,Italy,April
16-18,2018.IEEE,12–17. https://doi.org/10.1109/METROI4.2018.8428314
[17] DavidDearman,MelanieKellar,andKhaiNTruong.2008.Anexaminationofdailyinformationneedsandsharingopportunities.InProceedingsof
the2008ACMconferenceonComputersupportedcooperativework.679–688.
[18] YangDeng,WenqiangLei,MinlieHuang,andTat-SengChua.2023. RethinkingConversationalAgentsintheEraofLLMs:Proactivity,Non-
collaborativity,andBeyond.InProceedingsoftheAnnualInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval
intheAsiaPacificRegion(Beijing,China)(SIGIR-AP’23).AssociationforComputingMachinery,NewYork,NY,USA,298–301. https://doi.org/10.
1145/3624918.3629548
[19] BrianRDuffy,MauroDragone,andGregoryMPO’Hare.2005.Socialrobotarchitecture:Aframeworkforexplicitsocialinteraction.InAndroid
Science:TowardsSocialMechanisms,CogSci2005Workshop,Stresa,Italy.3–4.
[20] DavidEscobar-Castillejos,JulietaNoguez,FernandoBello,LuisNeri,AlejandraJMagana,andBedrichBenes.2020. Areviewoftrainingand
guidancesystemsinmedicalsurgery.AppliedSciences10,17(2020),5752.
[21] LorisFichera,DanieleMarletta,VincenzoNicosia,andCorradoSantoro.2011.Flexiblerobotstrategydesignusingbelief-desire-intentionmodel.In
ResearchandEducationinRobotics-EUROBOT2010:InternationalConference,Rapperswil-Jona,Switzerland,May27-30,2010,RevisedSelectedPapers.
Springer,57–71.
[22] JamesFrandsen,JoeTenny,WalterFrandsenJr,andYuriHovanski.2023. Anaugmentedrealitymaintenanceassistantwithreal-timequality
inspectiononhandheldmobiledevices.TheInternationalJournalofAdvancedManufacturingTechnology125,9(2023),4253–4270.
[23] QiGao,WeiXu,MoweiShen,andZaifengGao.2023.AgentTeamingSituationAwareness(ATSA):ASituationAwarenessFrameworkforHuman-AI
Teaming.CoRRabs/2308.16785(2023). https://doi.org/10.48550/ARXIV.2308.16785arXiv:2308.16785
[24] MichaelGeorgeff,BarneyPell,MarthaPollack,MilindTambe,andMichaelWooldridge.1999. Thebelief-desire-intentionmodelofagency.In
IntelligentAgentsV:AgentsTheories,Architectures,andLanguages:5thInternationalWorkshop,ATAL’98Paris,France,July4–7,1998Proceedings5.
Springer,1–10.
[25] SebastianGottifredi,MarianoTucat,DanielCorbatta,AlejandroJavierGarcía,andGuillermoRicardoSimari.2008.ABDIarchitectureforhighlevel
robotdeliberation.InXIVCongresoArgentinodeCienciasdelaComputación.
[26] MorganHarvey,MarcLangheinrich,andGeoffWard.2016.Rememberingthroughlifelogging:Asurveyofhumanmemoryaugmentation.Pervasive
Mob.Comput.27(2016),14–26. https://doi.org/10.1016/J.PMCJ.2015.12.002
[27] KoenVHindriks.2009.ProgrammingrationalagentsinGOAL.InMulti-agentprogramming:Languages,toolsandapplications.Springer,119–157.
[28] KoenVHindriks,FrankSdeBoer,WiebevanderHoek,andJohn-JulesChMeyer.1998.Formalsemanticsforanabstractagentprogramming
language.InIntelligentAgentsIVAgentTheories,Architectures,andLanguages:4thInternationalWorkshop,ATAL’97Providence,RhodeIsland,USA,
July24–26,1997Proceedings4.Springer,215–229.
[29] PranutJain,RostaFarzan,andAdamJLee.2023. Co-DesigningwithUserstheExplanationsforaProactiveAuto-ResponseMessagingAgent.
ProceedingsoftheACMonHuman-ComputerInteraction7,MHCI(2023),1–23.
[30] DanielKahnemanandAmosTversky.2013. Prospecttheory:Ananalysisofdecisionunderrisk. InHandbookofthefundamentalsoffinancial
decisionmaking:PartI.WorldScientific,99–127.
[31] BurakKaraduman,BarisTekinTezel,andMoharramChallenger.2023.RationalsoftwareagentswiththeBDIreasoningmodelforCyber–Physical
Systems.EngineeringApplicationsofArtificialIntelligence123(2023),106478.
ManuscriptsubmittedtoACM30 LiandWu,etal.
[32] KangsooKim,LukeBoelling,SteffenHaesler,JeremyBailenson,GerdBruder,andGregFWelch.2018.Doesadigitalassistantneedabody?The
influenceofvisualembodimentandsocialbehaviorontheperceptionofintelligentvirtualagentsinAR.In2018IEEEInternationalSymposiumon
MixedandAugmentedReality(ISMAR).IEEE,105–114.
[33] SojungKim,HuiXi,SantoshMungle,andYoung-JunSon.2012.Modelinghumaninteractionswithlearningundertheextendedbelief-desire-intention
framework.InIIEAnnualConference.Proceedings.InstituteofIndustrialandSystemsEngineers(IISE),1.
[34] DavidKinny,MichaelGeorgeff,andAnandRao.1996.AmethodologyandmodellingtechniqueforsystemsofBDIagents.InEuropeanworkshopon
modellingautonomousagentsinamulti-agentworld.Springer,56–71.
[35] MaximilianKönig,MartinStadlmaier,TobiasRusch,RobinSochor,LukasMerkel,StefanBraunreuther,andJohannesSchilp.2019.MA2RA-manual
assemblyaugmentedrealityassistant.In2019IEEEInternationalConferenceonIndustrialEngineeringandEngineeringManagement(IEEM).IEEE,
501–505.
[36] FotiosKKonstantinidis,IoannisKansizoglou,NicholasSantavas,SpyridonGMouroutsos,andAntoniosGasteratos.2020. Marma:Amobile
augmentedrealitymaintenanceassistantforfast-trackrepairproceduresinthecontextofindustry4.0.Machines8,4(2020),88.
[37] ChulmoKoo,YouheeJoun,HeejeongHan,andNamhoChung.2016. Astructuralmodelfordestinationtravelintentionasamediaexposure:
Belief-desire-intentionmodelperspective.InternationalJournalofContemporaryHospitalityManagement28,7(2016),1338–1360.
[38] MatthiasKraus,MarvinR.G.Schiller,GregorBehnke,PascalBercher,MichaelDorna,MichaelDambier,BirteGlimm,SusanneBiundo,andWolfgang
Minker.2020."Wasthatsuccessful?"OnIntegratingProactiveMeta-DialogueinaDIY-AssistantusingMultimodalCues.InICMI’20:International
ConferenceonMultimodalInteraction,VirtualEvent,TheNetherlands,October25-29,2020,KhietP.Truong,DirkHeylen,MaryCzerwinski,Nadia
Berthouze,MohamedChetouani,andMikioNakano(Eds.).ACM,585–594. https://doi.org/10.1145/3382507.3418818
[39] MatthiasKraus,NicolasWagner,ZoraidaCallejas,andWolfgangMinker.2021.TheRoleofTrustinProactiveConversationalAssistants.IEEE
Access9(2021),112821–112836. https://doi.org/10.1109/ACCESS.2021.3103893
[40] MatthiasKraus,NicolasWagner,andWolfgangMinker.2020.EffectsofProactiveDialogueStrategiesonHuman-ComputerTrust.InProceedingsof
the28thACMConferenceonUserModeling,AdaptationandPersonalization,UMAP2020,Genoa,Italy,July12-18,2020,TsviKuflik,IlariaTorre,Robin
Burke,andCristinaGena(Eds.).ACM,107–116. https://doi.org/10.1145/3340631.3394840
[41] MatthiasKraus,NicolasWagner,andWolfgangMinker.2021.ModellingandPredictingTrustforDevelopingProactiveDialogueStrategiesinMixed-
InitiativeInteraction.InICMI’21:InternationalConferenceonMultimodalInteraction,Montréal,QC,Canada,October18-22,2021,ZakiaHammal,Carlos
Busso,CatherinePelachaud,SharonL.Oviatt,AlbertAliSalah,andGuoyingZhao(Eds.).ACM,131–140. https://doi.org/10.1145/3462244.3479906
[42] MatthiasKraus,NicolasWagner,andWolfgangMinker.2022.ProDial-AnAnnotatedProactiveDialogueActCorpusforConversationalAssistants
usingCrowdsourcing.InProceedingsoftheThirteenthLanguageResourcesandEvaluationConference,LREC2022,Marseille,France,20-25June
2022,NicolettaCalzolari,FrédéricBéchet,PhilippeBlache,KhalidChoukri,ChristopherCieri,ThierryDeclerck,SaraGoggi,HitoshiIsahara,
BenteMaegaard,JosephMariani,HélèneMazo,JanOdijk,andSteliosPiperidis(Eds.).EuropeanLanguageResourcesAssociation,3164–3173.
https://aclanthology.org/2022.lrec-1.339
[43] YiLai,AtreyiKankanhalli,andDesmondC.Ong.2021.Human-AICollaborationinHealthcare:AReviewandResearchAgenda.In54thHawaii
InternationalConferenceonSystemSciences,HICSS2021,Kauai,Hawaii,USA,January5,2021.ScholarSpace,1–10. https://hdl.handle.net/10125/70657
[44] Ze-HaoLai,WenjinTao,MingCLeu,andZhaozhengYin.2020.Smartaugmentedrealityinstructionalsystemformechanicalassemblytowards
worker-centeredintelligentmanufacturing.JournalofManufacturingSystems55(2020),69–81.
[45] Jean-FrançoisLapointe,MohandSaïdAllili,LucBelliveau,LoucifHebbache,DariushAmirkhani,andHichamSekkati.2022.AI-ARfor&nbsp;Bridge
Inspectionby&nbsp;Drone.InVirtual,AugmentedandMixedReality:ApplicationsinEducation,AviationandIndustry:14thInternationalConference,
VAMR2022,HeldasPartofthe24thHCIInternationalConference,HCII2022,VirtualEvent,June26–July1,2022,Proceedings,PartII.Springer-Verlag,
Berlin,Heidelberg,302–313. https://doi.org/10.1007/978-3-031-06015-1_21
[46] GunALee,HyeSunPark,andMarkBillinghurst.2019. Optical-reflectiontype3daugmentedrealitymirrors.InProceedingsofthe25thACM
symposiumonvirtualrealitysoftwareandtechnology.1–2.
[47] MinaLee,PercyLiang,andQianYang.2022. CoAuthor:DesigningaHuman-AICollaborativeWritingDatasetforExploringLanguageModel
Capabilities.InCHI’22:CHIConferenceonHumanFactorsinComputingSystems,NewOrleans,LA,USA,29April2022-5May2022,SimoneD.J.
Barbosa,CliffLampe,CarolineAppert,DavidA.Shamma,StevenMarkDrucker,JulieR.Williamson,andKojiYatani(Eds.).ACM,388:1–388:19.
https://doi.org/10.1145/3491102.3502030
[48] SeungHoLee.2009.Integratedhumandecisionbehaviormodelingunderanextendedbelief-desire-intentionframework.TheUniversityofArizona.
[49] AlanMLeslie,TimPGerman,andPamelaPolizzi.2005.Belief-desirereasoningasaprocessofselection.Cognitivepsychology50,1(2005),45–85.
[50] JamesRLewis.1995.IBMcomputerusabilitysatisfactionquestionnaires:psychometricevaluationandinstructionsforuse.InternationalJournalof
Human-ComputerInteraction7,1(1995),57–78.
[51] JunnanLi,DongxuLi,SilvioSavarese,andStevenC.H.Hoi.2023.BLIP-2:BootstrappingLanguage-ImagePre-trainingwithFrozenImageEncoders
andLargeLanguageModels.InInternationalConferenceonMachineLearning,ICML2023,23-29July2023,Honolulu,Hawaii,USA(Proceedingsof
MachineLearningResearch,Vol.202),AndreasKrause,EmmaBrunskill,KyunghyunCho,BarbaraEngelhardt,SivanSabato,andJonathanScarlett
(Eds.).PMLR,19730–19742. https://proceedings.mlr.press/v202/li23q.html
[52] JiahaoNickLi,YanXu,ToviGrossman,StephanieSantosa,andMichelleLi.2024.OmniActions:PredictingDigitalActionsinResponsetoReal-World
MultimodalSensoryInputswithLLMs.InProceedingsoftheCHIConferenceonHumanFactorsinComputingSystems.1–22.
ManuscriptsubmittedtoACMSatori:TowardsProactiveARAssistantwithBelief-Desire-IntentionUserModeling 31
[53] DavidLindlbauer,AnnaMariaFeit,andOtmarHilliges.2019.Context-awareonlineadaptationofmixedrealityinterfaces.InProceedingsofthe32nd
annualACMsymposiumonuserinterfacesoftwareandtechnology.147–160.
[54] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.2024.Visualinstructiontuning.Advancesinneuralinformationprocessingsystems36
(2024).
[55] VivianLiuandLydiaB.Chilton.2022.DesignGuidelinesforPromptEngineeringText-to-ImageGenerativeModels.InCHI’22:CHIConference
onHumanFactorsinComputingSystems,NewOrleans,LA,USA,29April2022-5May2022,SimoneD.J.Barbosa,CliffLampe,CarolineAppert,
DavidA.Shamma,StevenMarkDrucker,JulieR.Williamson,andKojiYatani(Eds.).ACM,384:1–384:23. https://doi.org/10.1145/3491102.3501825
[56] FrederickHansenLund.1925.Thepsychologyofbelief.TheJournalofAbnormalandSocialPsychology20,1(1925),63.
[57] AmamaMahmood,JeanieW.Fung,IsabelWon,andChien-MingHuang.2022.OwningMistakesSincerely:StrategiesforMitigatingAIErrors.
InCHI’22:CHIConferenceonHumanFactorsinComputingSystems,NewOrleans,LA,USA,29April2022-5May2022,SimoneD.J.Barbosa,
CliffLampe,CarolineAppert,DavidA.Shamma,StevenMarkDrucker,JulieR.Williamson,andKojiYatani(Eds.).ACM,578:1–578:11. https:
//doi.org/10.1145/3491102.3517565
[58] MichaelF.McTear.1993.Usermodellingforadaptivecomputersystems:asurveyofrecentdevelopments.Artif.Intell.Rev.7,3-4(1993),157–184.
https://doi.org/10.1007/BF00849553
[59] Anna-MariaMeck,ChristophDraxler,andThuridVogt.2023.HowmayIinterrupt?Linguistic-drivendesignguidelinesforproactivein-carvoice
assistants.InternationalJournalofHuman–ComputerInteraction(2023),1–15.
[60] ChristianMeurisch,Maria-DorinaIonescu,BenediktSchmidt,andMaxMühlhäuser.2017.Referencemodelofnext-generationdigitalpersonal
assistant:integratingproactivebehavior.InAdjunctProceedingsofthe2017ACMInternationalJointConferenceonPervasiveandUbiquitousComputing
andProceedingsofthe2017ACMInternationalSymposiumonWearableComputers,UbiComp/ISWC2017,Maui,HI,USA,September11-15,2017,
SeungyonClaireLee,LeilaTakayama,andKhaiN.Truong(Eds.).ACM,149–152. https://doi.org/10.1145/3123024.3123145
[61] ChristianMeurisch,CristinaA.Mihale-Wilson,AdrianHawlitschek,FlorianGiger,FlorianMüller,OliverHinz,andMaxMühlhäuser.2020.
ExploringUserExpectationsofProactiveAISystems. Proc.ACMInteract.Mob.WearableUbiquitousTechnol.4,4(2020),146:1–146:22. https:
//doi.org/10.1145/3432193
[62] OndrejMiksik,I.Munasinghe,J.Asensio-Cubero,S.ReddyBethi,S.-T.Huang,S.Zylfo,X.Liu,T.Nica,A.Mitrocsak,S.Mezza,RoryBeard,Ruibo
Shi,RaymondW.M.Ng,PedroA.M.Mediano,ZafeiriosFountas,S.-H.Lee,J.Medvesek,H.Zhuang,YvonneRogers,andPawelSwietojanski.2020.
BuildingProactiveVoiceAssistants:WhenandHow(not)toInteract.CoRRabs/2005.01322(2020).arXiv:2005.01322 https://arxiv.org/abs/2005.01322
[63] MatthiasMinderer,AlexeyGritsenko,AustinStone,MaximNeumann,DirkWeissenborn,AlexeyDosovitskiy,AravindhMahendran,AnuragArnab,
MostafaDehghani,ZhuoranShen,etal.2022. Simpleopen-vocabularyobjectdetection.InEuropeanConferenceonComputerVision.Springer,
728–755.
[64] DavidLMorgan,JuttaAtaie,PaulaCarder,andKimHoffman.2013. Introducingdyadicinterviewsasamethodforcollectingqualitativedata.
Qualitativehealthresearch23,9(2013),1276–1284.
[65] AlexandrePauchet,NathalieChaignaud,andAmalElFallahSeghrouchni.2007.Acomputationalmodelofhumaninteractionandplanningfor
heterogeneousmulti-agentsystems.InProceedingsofthe6thinternationaljointconferenceonAutonomousagentsandmultiagentsystems.1–3.
[66] VeljkoPejovicandMircoMusolesi.2015.Anticipatorymobilecomputing:Asurveyofthestateoftheartandresearchchallenges.ACMComputing
Surveys(CSUR)47,3(2015),1–29.
[67] DavidPereira,EugnioOliveira,NelmaMoreira,andLusSarmento.2005.TowardsanarchitectureforemotionalBDIagents.In2005portuguese
conferenceonartificialintelligence.IEEE,40–46.
[68] NicolasPorotandEricMandelbaum.2021.Thescienceofbelief:Aprogressreport.WileyInterdisciplinaryReviews:CognitiveScience12,2(2021),
e1539.
[69] LongQian,AntonDeguet,andPeterKazanzides.2018.ARssist:augmentedrealityonahead-mounteddisplayforthefirstassistantinrobotic
surgery.Healthcaretechnologyletters5,5(2018),194–200.
[70] RodrigoChacónQuesadaandYiannisDemiris.2022. ProactiveRobotAssistance:Affordance-AwareAugmentedRealityUserInterfaces. IEEE
Robotics&AutomationMagazine29,1(2022),22–34. https://doi.org/10.1109/MRA.2021.3136789
[71] MashfiquiRabbi,MinHaneAung,MiZhang,andTanzeemChoudhury.2015. MyBehavior:automaticpersonalizedhealthfeedbackfromuser
behaviorsandpreferencesusingsmartphones.InProceedingsofthe2015ACMInternationalJointConferenceonPervasiveandUbiquitousComputing,
UbiComp2015,Osaka,Japan,September7-11,2015,KenjiMase,MarcLangheinrich,DanielGatica-Perez,HansGellersen,TanzeemChoudhury,and
KojiYatani(Eds.).ACM,707–718. https://doi.org/10.1145/2750858.2805840
[72] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,GirishSastry,AmandaAskell,PamelaMishkin,Jack
Clark,etal.2021.Learningtransferablevisualmodelsfromnaturallanguagesupervision.InInternationalconferenceonmachinelearning.PMLR,
8748–8763.
[73] AnandSRao.1996.AgentSpeak(L):BDIagentsspeakoutinalogicalcomputablelanguage.InEuropeanworkshoponmodellingautonomousagents
inamulti-agentworld.Springer,42–55.
[74] AnandSRaoandMichaelPGeorgeff.1997.ModelingrationalagentswithinaBDI-architecture.Readingsinagents(1997),317–328.
[75] AnandSRaoandMichaelPGeorgeff.1998.DecisionproceduresforBDIlogics.(1998).
[76] MengyangRen,LiangDong,ZiqingXia,JingchenCong,andPaiZheng.2023.AProactiveInteractionDesignMethodforPersonalizedUserContext
PredictioninSmart-ProductServiceSystem.ProcediaCIRP119(2023),963–968. https://doi.org/10.1016/j.procir.2023.01.021The33rdCIRPDesign
ManuscriptsubmittedtoACM32 LiandWu,etal.
Conference.
[77] LeeRossandRichardENisbett.2011.Thepersonandthesituation:Perspectivesofsocialpsychology.Pinter&MartinPublishers.
[78] GabrieleSara,GiuseppeTodde,andMariaCaria.2022.Assessmentofvideosee-throughsmartglassesforaugmentedrealitytosupporttechnicians
duringmilkingmachinemaintenance.ScientificReports12,1(2022),15729.
[79] RuhiSarikaya.2017.TheTechnologyBehindPersonalDigitalAssistants:Anoverviewofthesystemarchitectureandkeycomponents.IEEESignal
Process.Mag.34,1(2017),67–81. https://doi.org/10.1109/MSP.2016.2617341
[80] AndreasJ.Schmid,OliverWeede,andHeinzWorn.2007.ProactiveRobotTaskSelectionGivenaHumanIntentionEstimate.InRO-MAN2007-The
16thIEEEInternationalSymposiumonRobotandHumanInteractiveCommunication.726–731. https://doi.org/10.1109/ROMAN.2007.4415181
[81] BenediktSchmidt,SebastianBenchea,RüdigerEichin,andChristianMeurisch.2015.Fitnesstrackerordigitalpersonalcoach:howtopersonalize
training.InProceedingsofthe2015ACMInternationalJointConferenceonPervasiveandUbiquitousComputingandProceedingsofthe2015ACM
InternationalSymposiumonWearableComputers,UbiComp/ISWCAdjunct2015,Osaka,Japan,September7-11,2015,KenjiMase,MarcLangheinrich,
DanielGatica-Perez,HansGellersen,TanzeemChoudhury,andKojiYatani(Eds.).ACM,1063–1067. https://doi.org/10.1145/2800835.2800961
[82] MariaSchmidt,WolfgangMinker,andSteffenWerner.2020.HowUsersReacttoProactiveVoiceAssistantBehaviorWhileDriving.InProceedingsof
The12thLanguageResourcesandEvaluationConference,LREC2020,Marseille,France,May11-16,2020,NicolettaCalzolari,FrédéricBéchet,Philippe
Blache,KhalidChoukri,ChristopherCieri,ThierryDeclerck,SaraGoggi,HitoshiIsahara,BenteMaegaard,JosephMariani,HélèneMazo,Asunción
Moreno,JanOdijk,andSteliosPiperidis(Eds.).EuropeanLanguageResourcesAssociation,485–490. https://aclanthology.org/2020.lrec-1.61/
[83] PhilippM.Scholl,MatthiasWille,andKristofVanLaerhoven.2015. Wearablesinthewetlab:alaboratorysystemforcapturingandguiding
experiments.InProceedingsofthe2015ACMInternationalJointConferenceonPervasiveandUbiquitousComputing,UbiComp2015,Osaka,Japan,
September7-11,2015,KenjiMase,MarcLangheinrich,DanielGatica-Perez,HansGellersen,TanzeemChoudhury,andKojiYatani(Eds.).ACM,
589–599. https://doi.org/10.1145/2750858.2807547
[84] JunxiaoShen,JohnJ.Dudley,andPerOlaKristensson.2023.Encode-Store-Retrieve:EnhancingMemoryAugmentationthroughLanguage-Encoded
EgocentricPerception.CoRRabs/2308.05822(2023). https://doi.org/10.48550/ARXIV.2308.05822arXiv:2308.05822
[85] Naai-JungShih,Hui-XuChen,Tzu-YuChen,andYi-TingQiu.2020.Digitalpreservationandreconstructionofoldculturalelementsinaugmented
reality(AR).Sustainability12,21(2020),9262.
[86] KUjjwalandJChodorowski.[n.d.].Acasestudyofaddingproactivityinindoorsocialrobotsusingbelief-desire-intention(bdi)model,vol.4
(4)(2019).
[87] KentP.VaubelandCharlesF.Gettys.1990.InferringUserExpertiseforAdaptiveInterfaces.Hum.Comput.Interact.5,1(1990),95–117. https:
//doi.org/10.1207/S15327051HCI0501_3
[88] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,DennyZhou,etal.2022.Chain-of-thoughtprompting
elicitsreasoninginlargelanguagemodels.Advancesinneuralinformationprocessingsystems35(2022),24824–24837.
[89] GuandeWu,JianzheLin,andCláudioT.Silva.2022.IntentVizor:TowardsGenericQueryGuidedInteractiveVideoSummarization.InIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,CVPR2022,NewOrleans,LA,USA,June18-24,2022.IEEE,10493–10502. https://doi.org/10.
1109/CVPR52688.2022.01025
[90] GuandeWu,ChenZhao,ClaudioSilva,andHeHe.2024.YourCo-WorkersMatter:EvaluatingCollaborativeCapabilitiesofLanguageModelsin
BlocksWorld.arXivpreprintarXiv:2404.00246(2024).
[91] JunXiao,RichardCatrambone,andJohnT.Stasko.2003.BeQuiet?EvaluatingProactiveandReactiveUserInterfaceAssistants.InHuman-Computer
InteractionINTERACT’03:IFIPTC13InternationalConferenceonHuman-ComputerInteraction,1st-5thSeptember2003,Zurich,Switzerland,Matthias
Rauterberg,MarinoMenozzi,andJanetWesson(Eds.).IOSPress.
[92] SuryaB.Yadav.2010.Aconceptualmodelforuser-centeredqualityinformationretrievalontheWorldWideWeb.J.Intell.Inf.Syst.35,1(2010),
91–121. https://doi.org/10.1007/S10844-009-0090-Y
[93] NeilYorke-Smith,ShahinSaadati,KarenL.Myers,andDavidN.Morley.2012.TheDesignofaProactivePersonalAgentforTaskManagement.Int.
J.Artif.Intell.Tools21,1(2012). https://doi.org/10.1142/S0218213012500042
[94] NimaZargham,LeonReicherts,MichaelBonfert,SarahTheresVoelkel,JohannesSchöning,RainerMalaka,andYvonneRogers.2022.Understanding
CircumstancesforDesirableProactiveBehaviourofVoiceAssistants:TheProactivityDilemma.InCUI2022:4thConferenceonConversationalUser
Interfaces,Glasgow,UnitedKingdom,July26-28,2022,MartinHalvey,MaryEllenFoster,JeffDalton,CosminMunteanu,andJohanneTrippas(Eds.).
ACM,3:1–3:14. https://doi.org/10.1145/3543829.3543834
[95] NaimZierau,ChristianEngel,MatthiasSöllner,andJanMarcoLeimeister.2020.TrustinSmartPersonalAssistants:ASystematicLiteratureReview
andDevelopmentofaResearchAgenda.InEntwicklungen,ChancenundHerausforderungenderDigitalisierung:Proceedingsder15.Internationalen
TagungWirtschaftsinformatik,WI2020,Potsdam,Germany,March9-11,2020.ZentraleTracks,NorbertGronau,MoreenHeine,HannaKrasnova,and
K.Poustcchi(Eds.).GITOVerlag,99–114. https://doi.org/10.30844/WI_2020_A7-ZIERAU
ManuscriptsubmittedtoACMSatori:TowardsProactiveARAssistantwithBelief-Desire-IntentionUserModeling 33
A ImplementationDetails
A.1 BackendStreamingServer
ThebackendserverreceivesthestreamingdatafromHoloLens2headset,processesthedata,andsendstheprocessed
resultbacktoHoloLens2.Toimplementthispipeline,weimplementastreamingserverwithRedis-Streamandcreate
anextensionmodulewiththeLambdaprogram.
A.1.1 Hardware. Duetohardwarelimitations,weareunabletohostourentirebackendprogramonasingleserver.As
aresult,wedistributethebackendserveracrosstwodevices.OneserverisequippedwithanIntelCore™i7-8700K
CPU@3.70GHzandanNVIDIAGeForceGTX1080GPU.TheotherserverfeaturesanIntel®Corei9-10980XECPU@
3.00GHzandtwoNVIDIARTX3090GPUs.
Stream Description DataFormat
main Thestreamwhichcontainstheframessentby ImageBytes
HoloLens2headset
processed_main The stream which contains the frames pro- ImageBytes
cessedbyanimagerecognitionmoduletofilter
outtherelatedframes
guidance Thestreamusedtostorethegeneratedguidance JSON
result
assistant:images Thestreamusedtostorethegeneratedimage ProtobufBytes
assistance.
intent:belief The stream used to store the inferred belief JSON
state.
intent:desire The stream used to store the inferred desire JSON
state
intent:task_plan Thestreamusedtostorethetaskplanconsisting JSON
thedesiredtaskplan andactioncheckpoints
generatedbythetaskplanLLM
intent:task:checkpoints Thestreamusedtostoretheinferredstatesof JSON
theactioncheckpoints
intent:task:step:nex Thestreamusedtostoretheinferrednextin- JSON
tendedaction.
intent:task:step:current Thestreamusedtostoretheinferrednextin- JSON
tendedaction.
feedback Thestreamusedtostoretheuser’feedback. JSON
Table6. Thestreamlistusedinthesystem.Welistthestreamnames,correspondingdescriptions,anddataformatsinthetable.The
namesintheopen-sourcedcodemaybeslightlydifferentfromthenamesinthedevelopmentbuild.
A.1.2 Redis-StreamsModule. Toaccommodatethissetupandenabledatacommunicationacrossmultipledevices,we
introducedtheRedis-Streamsmodule.RedisStreamsisafeatureindexdesignedtohandlehigh-throughputstreaming
data.Itallowsformanagingtime-orderedeventsandisparticularlyusefulforbuildingmessagequeuesandreal-timedata
processingsystems.WeuseitforhandlingthestreamingdatageneratedbyHoloLens2,enablingefficientprocessing
andcommunicationofthedataacrosstheHoloLensclient,backendserver,anddistributedmodelservers.Thebasicunit
inthismoduleistheStreamclass,whichisdefinedbyastreamkeyandapre-definedformat,supportingvariousdata
typessuchastheProtobufbytes,JSON,plainstrings,andimagebytes.Eachstreamconsistsofaseriesofentries,where
ManuscriptsubmittedtoACM34 LiandWu,etal.
eachentryrepresentsapieceofdataindexedbythetimestamp.Thestreamdatacanbeadded,read,andprocessed
bythedifferentclientsandservers.WelistthestreamsinTable6.OurserversupportsbothWebSocketandHTTP
requests,enablingclient-sideapplicationstosubscribetostreamsusingWebSocketconnectionsandsendnewdatainto
thestreamsviaWebSocketaswell.Thissetupprovidesreal-time,bidirectionalcommunicationbetweentheHoloLens
deviceandthebackendserver.ForscenarioswhereWebSocketisnotavailable,wealsosupportdatasubmissionand
retrievalthroughstandardHTTPrequests,offeringflexibilityinhowdataistransmitted.
A.1.3 LambdaExtensiontoRedis-StreamsModule. Lambdafunctionsarestateless,event-drivenfunctionsthatexecute
inresponsetospecificeventsordatatriggers.Inthecontextofourstreamingprogram,thestatelessnatureofLambda
functionsmakesthemasuitablechoiceforhandlingreal-timedataprocessingtaskswithouttheneedforpersistent
statemanagement.OurimplementationisbasedonaclassnamedPipeline(theactualnameusedinthecodebase),
whichservesasastatelessstreamprocessor.EachPipelineinstancecansubscribetomultiplestreams,processthedata
asneeded,andpublishtheprocessedresultstooutputstreams.Forexample,theobjectdetectionmodulesubscribesto
imagestreams,processestheimagestodetectobjects,andthenoutputsthedetectedobjectsinJSONformattoanother
stream.Tosimplifythedevelopmentofsuchmodules,weimplementedasetofbaseclassesfortaskssuchasrunning
GPTmodelsandperformingimageanalysis.Thesebaseclassesprovidecommonfunctionalities,allowingotherservices
toextendandimplementspecificprocessinglogicwithminimaleffort.Giventhehighframerate(fps)oftheimage
streamandthefactthatourmachine-learningmodulesmaynotneedorbecapableofprocessingdataatsuchhigh
frequencies,weintroducedafrequencyadjustmentmechanism.ThismechanismallowstheLambdafunctiontocache
incomingframesandprocessthemataconfigurableinterval,reducingthecomputationalloadandensuringefficient
processing.
A.2 ClientImplementation
WeimplementedourHoloLens2interfaceusingUnityandtheMixedRealityToolkit(MRTK).Fordatacommunication,
weutilizedGoogle’sProtocolBuffers(Protobuf)moduleandusedtheNativeWebSocketlibrarytoestablishWebSocket
connections.WeacquiredthemaincameraframesfromtheHoloLens2usingtheResearchModeandthecorresponding
C++API.
B BDIInferenceandAssistanceGenerationPrompt
You are an AR assistant helping users with tasks. Given an image, a task guidance and
1
the next step, generate guidance in the required format. Please make sure your
guidance is not too simple and can actually help the user.
2
<TASK_DESCRIPTION>: [Task description]
3
<NEXT_STEP>: [Description of the next step]
4
<image>: [first-person perspective image of the user's environment]
5
6
<INSTRUCTIONS>:
7
Based on the <NEXT_STEP>, provide the following:
8
0.<DESORE> [Based on the given <NEXT_STEP>, generate the user's high-level goal. Refer
9
to the <TASK_DESCRIPTION> for possible tasks' goal. Output this high-level desire
prefixed with <DESIRE>.]
ManuscriptsubmittedtoACMSatori:TowardsProactiveARAssistantwithBelief-Desire-IntentionUserModeling 35
1. <INTENT> [Describe a basic, concrete action in the step. keep concise and clear]
10
2. <META_INTENT> [Generate meta-intent from given meta-intent list [make a tool,
11
interact with time-dependent tools, interact with time-independent tools, interact
with materials] based on the user_intent <INTENT>. Output this single meta-intent
prefixed with <META_INTENT>. \
The meta-intent refers the user's most fundamental intent without the contextual
12
information.
make a tool example intent: assemble Swiffer mob, make coffee filter, arrange flower
13
creatively;
interact with time-dependent tool example intent: use grinder to grind coffee, heat
14
food using microwave;
interact with time-independent example intent: connect to VR headset, use a mop, use
15
a stainer;
interact with materials example intent: add ingredients to a bowl, pour water into a
16
cup, cut flower stems]
3. <GUIDANCE_TYPE> [Select between 'image' and 'timer'. Based on the identified
17
<META_INTENT>, select the corresponding guidance type from the following mappings: \
{"make a tool": "image",
18
"interact with time dependent-tools": "timer",
19
"interact with time independent-tools": "image",
20
"interact with materials": "image"}]
21
4. <TEXT_GUIDANCE_TITLE> [Short title]
22
5. <TEXT_GUIDANCE_CONTENT> [Generate text guidance content best fit for the user in this
23
step. This guidance should consider the contextual information, e.g. the properties
object in real environment, the tips that user should pay attention to. Output based
on the <STEP_DESCRIPTION> and user's <INTENT>, the object interaction list
<OBJECT_LIST> and level of detail <LOD>,starting with <TEXT_GUIDANCE_TITLE> and
<TEXT_GUIDANCE_CONTENT>. NOT generate text guidance for <DESIRE>, only generate
guidance for the fundamental action <INTENT>\
In <TEXT_GUIDANCE_CONTENT>, incorporate concrete numbers as required by the
24
<TASK_DESCRIPTON> if possible.]
6. <DALLE_PROMPT> [Based on <INTENT>, <OBJECT_LIST> and <EXPERTISE>, generate a DALLE
25
prompt with the following template in appropriate detail. The prompt should not
consider <DESIRE>. The prompt should integrate the intent <INTENT>, assistance
<TEXT_GUIDANCE_CONTENT> and object interactions <OBJECT_LIST> to depict action
clearly and include a red arrow (<INDICATOR>) showing action direction.\
if <EXPERTISE> is novice, prompt DALLE to show actions and interacting objects using
26
the template: "<INTENT>or<TEXT_GUIDANCE_CONTENT> <OBJECT_LIST>. <INDICATOR>".
if <EXPERTISE> is expert, prompt DALLE to show final result of the <INTENT> using
27
the template: "<INTENT>or<TEXT_GUIDANCE_CONTENT> <OBJECT_LIST>.<INDICATOR>".]
7. <OBJECT_LIST> [Key objects with properties in the image: identify key objects which
28
the user is interacting with following the <STEP_DESCRIPTION> and the properties of
the objects, e.g. color, shape, texture, size. Output an object interaction list
with descriptions of properties.]
8. <HIGHLIGHT_OBJECT_FLAG> [True if key objects to highlight]
29
ManuscriptsubmittedtoACM36 LiandWu,etal.
9. <HIGHLIGHT_OBJECT_LOC> [Location of key object if applicable]
30
10. <HIGHLIGHT_OBJECT_LABEL> [Name of key object if applicable]
31
11. <CONFIRMATION_CONTENT> [Select confirmation content based on <META_INTENT> from the
32
following options, and insert <INTENT> into sentence, starting with
<CONFIRMATION_CONTENT>: "Looks like you are going to <INTENT>, do you need
<GUIDANCE_TYPE>?"]
---
33
Example:
34
Input:
35
<TASK_DESCRIPTION> Making pour-over coffee
36
<NEXT_STEP> Pour water into the coffee brewer
37
<image> [image of using a black coffee brewer and metal kettle]
38
Output:
39
<INTENT> Pour water into coffee brewer
40
<DESIRE> make coffee
41
<META_INTENT> interact with time-dependent tools
42
<GUIDANCE_TYPE> timer
43
<TEXT_GUIDANCE_TITLE> pour water into coffee brewer
44
<TEXT_GUIDANCE_CONTENT> Pour water into coffee brewer.
45
<DALLE_PROMPT> Hand pouring water from gooseneck kettle into pour-over coffee maker. Red
46
arrow shows pour direction. Timer displays 30 seconds.
<OBJECT_LIST> Coffee brewer (black), Kettle (metal, gooseneck), Coffee grounds (dark
47
brown)
<HIGHLIGHT_OBJECT_FLAG> True
48
<HIGHLIGHT_OBJECT_LOC> center
49
<HIGHLIGHT_OBJECT_LABEL> coffee brewer
50
<CONFIRMATION_CONTENT> Looks like you are going to pour water into a black coffee
51
brewer, do you need a timer assistance for it?
C ImageGenerationPrompt
Wegeneratedimagesforeachstepasabackup,althoughimagesmaynotalwaysbethemostsuitablemodalityfor
everystep.Weprovidedthepromptsandthecorrespondinggeneratedimagesasreferences.Tomaintainconsistent
styles,weappended“inthestyleofflat,instructionalillustrations.Nobackground.Accurate,concise,comfortablecolor
style”totheendofeachprompt.Wealsoprefixedthepromptwith“INEEDtotesthowthetoolworkswithextremely
simpleprompts.DONOTaddanydetail,justuseitAS-IS:”topreventanymodificationtotheprompts.
C.1 Task1:ArrangeFlowers
Prompt1: pouringhalfofflowerfoodpacketintoglassvase.Redarrowindicatingthepouringaction.
Prompt2: pour16ozofwaterfromglassmeasuringcupintoaglassvase.Redarrowshowspouringdirection.
Highlight"16oz"onthemeasuringcup.
ManuscriptsubmittedtoACMSatori:TowardsProactiveARAssistantwithBelief-Desire-IntentionUserModeling 37
Prompt3: Trimyellowandpurpleflowerleavesbelowthewaterlineinaglassvaseusingwhitescissors.Redlines
indicatethewaterline.
Prompt4: trimming2-3inchesoffyellowandpurpleflowerstemsata45-degreeanglewithwhitescissors.Redline
highlightsthecuttingangle.
Prompt5: Arrange yellow and purple flowers neatly in a glass vase filled with water. Red arrows indicate the
positioningstepsforaneatarrangement.
(a)Pouringflowerfood (b)Pouringwater (c)Trimmingleaves (d)Trimmingstems (e)Arrangingflowers
Fig.C.1. Stepsforarrangingflowers.
C.2 Task2:CleanRoom
Prompt1: connectgreenmoppolestowhitesquaremoppadofswiffermop.Redarrowsindicatetheconnection
points.
Prompt2: wrappingwhitesquaremoppadaroundgreenmoppolesofswiffersweeper.Redarrowsindicatewrapping
directionaroundmophead.
Prompt3: insertingwhitesquaremoppadintofoursocketsongreenmophead.Redarrowshighlightinsertion
points.
Prompt4: connectyellowswifferdusterhandles.Redarrowshowsthealignmentandconnectiondirection.
Prompt5: connectyellowhandlestobluefeatherdustersofswifferduster.Redarrowsshowtheconnectionpoints.
Prompt6: mopthefloorwithgreenswiffersweepermopusingwhitesquaremoppad.Redarrowillustratesmopping
motionacrossthefloor.
Prompt7: dustingawhitedeskusingabluefeatherdusterwithayellowhandle.Redarrowshighlightcarefuldusting
aroundfragileitems.
C.3 Task3:MakePour-OverCoffee
Prompt1: Measure11gcoffeebeansusingasilverkitchenscale.Redarrowpointstothedigitaldisplayshowing11g.
Coffeebeansaredarkbrown.
Prompt2: Grindingcoffeebeansintopowderusingablackgrinder.Redarrowshighlightthegrindingaction.
Prompt3: placingabrowncoffeefilteronawhitecoffeebrewer.Redarrowsshowmovementtowardthebrewer.
ManuscriptsubmittedtoACM38 LiandWu,etal.
(a) Connect mop(b)Wrapmoppad(c)Insertmoppad(d)Connectduster(e) Connect(f)Mopthefloors(g) Duster the ta-
poles tomop tosockets handles dusters thoroughly blecarefully
Fig.C.2. Stepsforcleaningtheroom.
Prompt4: Placethewhitecoffeebreweronacupandwetthecoffeefilterusingablackgooseneckkettle.Redarrow
pointsfrombrewertothecup.Redarrowindicatesthewaterpouringdirection.
Prompt5: Addingdarkbrowncoffeegroundstoabrowncoffeefilterinawhitecoffeebrewer.Redarrowemphasizes
thepouringmotionofthecoffeegrounds.
Prompt6: settingthesilverkitchenscaletozero.Redarrowhighlightingthezeromarkonthedisplay.
Prompt7: Pouringwaterfromblackgooseneckkettleintowhitecoffeebrewerwithdarkbrowncoffeegroundsin30
seconds,usingcircularmotion.Redarrowshowscircularpourdirection.Highlighttimerdisplays30secondsand50g
onsilverkitchenscale.
(a)Measurecoffee-(b) Grind coffee(c) Place filter on(d) Set brewer on (f) Add coffee
beans beans brewer cup (e)Wetcoffeefiltergrounds (g)Setscaletozero
Fig.C.3. Stepsformakingpour-overcoffee.
C.4 Task4:ConnectSwitchtoMonitor
Prompt1: ConnecttheblackHDMIcabletotheblackHDMIportontheNintendoSwitchdock.Redarrowindicates
theconnectiondirection.
Prompt2: connectingblacktypeCpowercabletotheblackdockoftheNintendoswitch.Redarrowshowsthe
connectiondirection.
Prompt3: connectingblackpowercabletoanACoutlet.Redarrowshowsthedirectionofconnection.
Prompt4: InsertingNintendoSwitchintoblackNintendoSwitchdock.Redarrowshowsdirectionofinsertion.
Prompt5: PressthepowerbuttonontheNintendoSwitchconsoletoturniton.Redarrowindicatesthepowerbutton
location.
ManuscriptsubmittedtoACMSatori:TowardsProactiveARAssistantwithBelief-Desire-IntentionUserModeling 39
Prompt6: blackmonitorwithavisiblepowerbuttonintheleftcorner.pressingpowerbuttononmonitortoturnit
on.Redarrowindicatesthepowerbuttonlocation.
(a)ConnectHDMIca-(b) Connect Type C (c)Connectpowerca-(d)InsertSwitchinto (e) Press Switch (f)Turnoncomputer
ble cable ble dock powerbutton monitor
Fig.C.4. StepsforconnectingtheNintendoSwitchtoamonitor.
C.5 Comparisonbetweenoriginalpromptandmodifiedprompt
AsstatedinSec.6.4.2,wepresenttwoexamplesofimageassistancegenerationtodemonstratetheeffectivenessofour
template,asshownFig.3.
Thefirstexampleinvolvesthetaskofmakingcoffee.Thebasicprompt,withoutanymodifier,is“pressesabutton
onanespressomachine.”Ourenhancedpromptincorporatesspecificmodifiersforclarity:“pressesawhitebuttonona
whiteespressomachine.Aredarrowpointstothebutton.Nobackground,styledinflat,instructionalillustrations.Accurate,
concise,comfortablecolorstyle.”
Inthesecondscenario,theuserneedstocutthestemofaflowerataspecificangle.Therawpromptreads:“cutsstem
ofaflowerupfromthebottomwithscissorsat45degrees.”Ourproposedprompt,enrichedwithdetailedmodifiers,is
“Onehandcutsthestemofaredflowerupfromthebottomwithwhitescissorsat45degrees.Alargeredarrowpointstothe
cut,setagainstawhitebackgroundinthestyleofflat,instructionalillustrations.Accurate,concise,comfortablecolorstyle.”
AsshowninFig3,Fig3aandFig3c,derivedfromourtemplate,eliminateunnecessarydetailsandemphasizethe
coreactionofpressingabutton.Clearvisualelements,suchasboldoutlinesanddirectionalarrows,highlightthe
instructedaction,makingitimmediatelyapparentwhatactionisbeinginstructed.Thistypeofimageryisespecially
effectiveininstructionalmaterialswhererapidcomprehensionisessential.Incontrast,Fig3bandFig3dmayintroduce
ambiguityinaninstructionalcontextduetotheirrealisticdepictionthatincludesreflectivesurfacesandshadows.
Whileaestheticallypleasing,thislevelofdetailcandistractfromthecoreinstructionalmessage.Therefore,ourtemplate
enhancesimageclarityanddirectlyalignswiththeuser’sneedforclear,actionableinstructionsintheirspecificcontext.
D UserStudyInterviewQuote
Weclusteredtheopinionssharedbyparticipantsandpresentedthecorrespondingquotes.Theopinionswerecollected
throughtranscriptionsofin-personinterviewsconductedaftertheexperimentsandfromfollow-upquestionnaires.
D.1 SatoriSystem
D.1.1 Satorisystemisbetterfornoviceusers. P8:“ifsomeoneisnewto,let’ssay,doingacertaintask,therewerevisual
cuesthatwerethere,whichwehadtoseeonthescreenandreplicatethat.”
P6:“ifsomeonewhodoesn’tknowwhatisagrinder,ordoesn’tknowwhatisthebrewer,orstufflikethat,it(animation)
actuallyshowedme.”
ManuscriptsubmittedtoACM40 LiandWu,etal.
D.1.2 Satorisystemcanprovideclearandusefulinstructions. P8:“thoughthetasksweresimple,theinstructionswere
veryclearinboththethings.”
P14:“Theguidancehelpsmealot,especiallyincoffeemaking.Itprovidesmewithverydetailedinstructionsincluding
time,andamountofcoffeebeansIneed.IwouldhavetogoogleitifIdon’thavetheguidance.”
D.1.3 Satorisystemcandetectmyintentions. P9:“theprovidedguidanceprettymuchalignedwithmyintentionallthe
time.Theconfirmationmessagehelpsbutalsoannoyedmeabitsinceitshowedupaftereverystep.”
P12:“Inmyexperience,mostofthetimethesystemknowswhatIhavedoneinthepaststepeventually,butIwishit
couldbemoreresponsivesoIdon’tneedtowaitforthesystemtorecognizewhatIhavedone”
D.1.4 DesigninSatorimakestheusersmoreengagedandfeeltrustworthy. P9:“Iliketohaveavoicetalkingtomemorefor
emotionalsupportsuchascomplimentsaftercompletingonestepsuccessfully.”P5:“Theautomaticstep-by-stepexperience
ishighlyengagingandmakesmeexcitedaboutitsfuture.Itgivesmetheimpressionthatthemachineunderstandswhat
I’mdoing,makingitsinstructionsfeeltrustworthy.”P10:“Itautomaticallydetectsmyprogress.”P12:“Ilikehowthesystem
automaticplaythetextthatyoufinishedthetaskwhichmadememoreengagedatthebeginning,butawhenIrealizethatit
oftentakestimeforthesystemtoknowyouhavefinishedandIhavetowaitforit,thatengagementandenthusiasmfaded.”
D.1.5 Satorisystemcannotalwaysdetectthecompletionofastep. P8:“mostofthetime...ifIhavetoassociateanumber
withit,60%ofthetime... itwasn’tabletopickifIdidfinishthetaskornot.”
D.1.6 SatoriisbitlagbehindWozintermsoftimelyassistance. P4:“Itwasabitdelayedcomparedtothesecondone.”
P6:“(Satori)Ithinkmaybeittookalittlebitextra.Ithinkfive,10secondsextraIneededtoshowtheimageproperlyfor
ittounderstandthatI’vedonethestuff.Thesecondone,(WOZ)Isawthatitcouldcomprehendmuchmoreeasily.”
D.1.7 Satorimainfrustrationpointisdetectionoffinish. P3:“ButalsoIthinkthatitwouldbehelpfulifitactuallydid
recognizeallofthethingsthatIwasdoing.”
P2:“thefirstsystem,whatIfeelistheguidancewasgood.Theimageswerenice,theanimationwasonpoint,justthe
detectionwasnotgood.”
P9:“Ijusthopethedetection,whateveralgorithm,canbemoreaccurate,soIdon’tneedtoclickitbymyself.”
D.1.8 Thesubstepcheckpointandstepsareuseful. P9:“Ilikethemissionkindofpointofview,anditshowseachstep,
likethesub-steps,withtheprogresscheckings,likethecirclething.It’seasierformetounderstandifI’mworkingonthe
correctstep,orifIalreadymessedsomethingupbeforeIevennoticed.”
P13:“whatyouaredoingandtheoverallobjectivesandindividualsteps.Toshow,like,it’seasytounderstandwhatstepI
didandgoingbackandforth.”
D.1.9 Combinationofvisual,textandaudiomodalitymakestaskcompletioneasier. P3:“SoIlikethatthesecondsystem
(SATORI)hasalsolikeatextfeedback.SoincaseI’mlost,Icanjustfigureitout.Ithinkthisone(WOZ)didnothaveatext
feedback.SoonceImissthevoice,ifIhappentomissthevoiceorifIforget,ifit’salongtask,thenImightforgetexactly
whereIamorwhatthesubtasksinvolvedinthistaskare.”
P3:“Animationsarehelpfulineithercase.Butthesecondoneseemskindofmoredetailed.ButIjustfeelthatmightbe
justbecauseitcombinesdifferentmodalities.ThatIfeellikebetter,abettersenseofwhereIaminthetask.”
P5:“like,theimagesandthetextandaudio..thewholethingiscomposedmoreneatly.”
D.1.10 Imageassistanceisuseful. P1:“thepictureofthesecondoneisveryniceanditlooksgood.”
ManuscriptsubmittedtoACMSatori:TowardsProactiveARAssistantwithBelief-Desire-IntentionUserModeling 41
P2:“Theimageswerenice,theanimationwasonpoint.”
P6:“whatIlikedbetterwastherewasanimationforeverything.Soyeah,Imean,ifsomeonewhodoesn’tknowwhatisa
grinder,ordoesn’tknowwhatisthebrewer,orstufflikethat,itactuallyshowedme.Andwhodoesn’tknowhowtocut
belowthewaterlineandstufflikethat,itactuallyshowedmetheanimationonwhethertocut.”
D.1.11 Imageassistanceisconfusing,orcomplex. P1:“butIthinkitcanbemoresimplebecausemaybemanycontentin
thepictureisnotnecessaryforme.iconcanbejustshowthemostimportantthingduringthetaskandnotmanyextralines
orsomething.”
P5:“Butinthesecondsystem,youjustuse,like,theAI-generatedimages.Andthatsometimesisjustnot,like,matching
therealsetting.So,sometimesifyouusetherealliveobjects,itmaymaketheimageinstructionmoreclear.”
P14:“sometimesthefigureisconfusing,Ithinkbecauseit’sinflashstyleandit’ssometimesit’salittledifferentfrom
whattheactualitemis.”
D.1.12 Timerisuseful. P13:“...forexamplethesystemgivesvideoinstructionsonhowtofoldthecoffeefilterandplaceit
inthecone;that’sveryhelpful.IalsolikethetimerwhenImakethecoffee”
D.1.13 Satorisystemgivesuserhighersatisfactionontheassistance. P2:“Ifeltthefirstonewasmuchbetter,butthe
secondwas,IfeltIwasabletodothetaskmuchfaster.Reason:thefirstsystem,whatIfeelistheguidancewasgood.The
imageswerenice,theanimationwasonpoint,justthedetectionwasnotgood.”
D.1.14 Tasktransitionsarenaturalforboth. P2:“inthetwosystemsisit(natural).Ifelttherewasalaginfirst,insystem
one,butinsystemtwo,itwaseverythingwasrobust.”
D.1.15 Satoricanbeappliedindailylife. P12:“Ithinkthisspecificscenarioisactuallyprettyhelpfulbecausesometimes
weneedtoconnectdifferentdevicesthatarenewtous.IhadasimilarissuewithmyWi-Firouterandmodematmy
apartmentlastyearandIhavetocallaspecialisttocomeandfixit,butanARtutorialwillbehelpful.”
P8:“ThingslikeIKEAassemblyetc.,wouldbeagreatusecase.”
P13:“cooking,assemblingequipment(ex.PC)orfurnitures(ex.shelf),operatingonmachines(ex.coffeemaker),exercise
(ex.differentyogamoves)”
P9:“...maybewhenweneedtoassemblyafurniture,insteadofgoingthroughthemanualbackandforthallthetime,we
canjusthavethissystemtoguideus.”
D.2 WoZ
D.2.1 WoZ system detect user’s intentions. P9: “There is no misalignment between my intentions and the provided
guidance”
P13:“sometimestherearetimelag,butmostlyitworksfine”
D.2.2 Themodalitydesignisnotasmuchashelpful. P1:“ThetimingofthefirstoneisreallynotusefulformebecauseI
cannotfigureoutwhetherwhichwayitwillpresenttome.Maybethevoice,maybetheimage.TherewasatimethatIthink
itwillbeavoicetoleadmebutactuallythereisanimagebutIdidn’tknow.IjustwaitforthevoiceandIdon’tknowhow
todothenext.”
P2:“...However,thelackoftextguidancemadesometasksmoredifficult.”
P4:“Itsguidancewasnotuniform.Itshowedtext,audio,imagesrandomly.WhenIneedaanimationtohelp,itonly
showedmeatext.”
ManuscriptsubmittedtoACM42 LiandWu,etal.
D.2.3 Wozhelpsusercompletethetaskfasterandsmoother. P2:“Ifeltthefirstonewasmuchbetter,butthesecondwas,I
feltIwasabletodothetaskmuchfaster.Iwoulddefinitelypreferlikethesystemtwo,overlikesystemone,becauseit’s
muchfaster,eventhoughtherearelessinstructions(WOZ).”
P8:“Thissystemhadthrough-and-throughaudioguidance.Thishelpedmefeeltheprocesswasmuchsmoother”
P2:“TheguidanceinSystemBwashelpfulduetothefastdetectionoftaskcompletion.Forexample,whenconnectingthe
mopandduster,thesystemquicklyrecognizedthetaskascomplete.”
D.2.4 Animationandimageassistancearebetter. P9:“animationfromthelastone,fromthefirstsystem,waseasierto
followcomparedtotheimagesthat’sintoday’ssystem.”
“Thesystemgivesmerealisticimagesandvoiceguidance.”
ReceivedSeptember2024;revised1May2024
ManuscriptsubmittedtoACM