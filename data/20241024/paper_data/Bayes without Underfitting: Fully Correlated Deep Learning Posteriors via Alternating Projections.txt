Bayes without Underfitting: Fully Correlated Deep Learning
Posteriors via Alternating Projections
Marco Miani† Hrittik Roy† Søren Hauberg
Technical University of Denmark Technical University of Denmark Technical University of Denmark
mmia@dtu.dk hroy@dtu.dk sohau@dtu.dk
Abstract
Bayesian deep learning all too often underfits
so that the Bayesian prediction is less
accurate than a simple point estimate. Uncer-
tainty quantification then comes at the cost
of accuracy. For linearized models, the null
spaceofthegeneralizedGauss-Newtonmatrix
corresponds to parameters that preserve the Figure 1: Key idea: In overparametrized linear mod-
trainingpredictionsofthepointestimate. We els, the kernel (null space) contains all models that
propose to build Bayesian approximations in haveidenticalpredictionsonthetrainingdata.Wepro-
this null space, thereby guaranteeing that the pose restricting approximate posteriors of deep neural
Bayesian predictive does not underfit. We networks to this kernel to avoid underfitting.
suggest a matrix-free algorithm for projecting
ontothisnullspace,whichscaleslinearlywith
despite having attractive theoretical properties [Ger-
the number of parameters and quadratically
main et al., 2016]. To counteract this trend, we first
with the number of output dimensions.
We further propose an approximation that explicate why underfitting happens with Gaussian ap-
proximateposteriors,and thereafter proposea solution
only scales linearly with parameters to
for low-noise data (e.g. images).
make the method applicable to generative
models. An extensive empirical evaluation What is underfitting? Deep learning performs
shows that the approach scales to large very well when the training data is subject to limited
models, including vision transformers with observation noise. In contrast, Bayesian deep learning
28 million parameters. Code is available at: oftenunderfitsinthesensethattheBayesianprediction
https://github.com/h-roy/projected-bayes deviates significantly from a point estimate prediction
on the training data D, i.e.
1 Underfitting in Bayesian deep
E θ∼q[f(θ,x)]̸=f(θmap,x) for x∈D, (1)
learning
whereq isanapproximateposteriorandθmap isapoint
estimate of the neural network f. This notion of under-
Bayesian deep learning tends to underfit. Numerous
fitting only applies to low-noise data, where the poste-
studies demonstrate that marginalizing approximate
rior should have little uncertainty on the training data,
weight posteriors yields less accurate predictions than
butthisisthemostprominentscenarioindeeplearning.
applyingamaximum a posteriori (map)pointestimate
[Wenzel et al., 2020, Daxberger et al., 2021a, Zhang WhydoBayesianneuralnetworksunderfit?Deep
et al., 2024, Kristiadi et al., 2022]. This significantly neural networks are commonly overparametrized, i.e.
reduces the practical value of Bayesian deep learning, the model has more parameters than observations, as
this tends to improve both optimization and gener-
alization [Allen-Zhu et al., 2019]. Fig. 1 sketches the
situation which we later formalize: Consider fitting
a quadratic function with three parameters to only
† Equal contribution.
two observations. This leaves one degree of freedom
4202
tcO
22
]GL.sc[
1v10961.0142:viXraBayes without Underfitting: Fully Correlated Deep Learning Posteriors via Alternating Projections
undetermined and a linear subspace of the model pa- most correlations in Σ, e.g. with diagonal or block-
rameters exists wherein all parameters yield identical diagonal approximations.
predictions on the training data. For low-noise data,
Unfortunately, there is significant theoretical evidence
the true posterior will concentrate around this sub-
against disregarding correlations. Foong et al. [2019]
space. Unfortunately, common Gaussian approxima-
shows that diagonal approximations underestimate
tions to the posterior are axis-aligned (i.e. mean field
‘in-between’ uncertainty, while Roy et al. [2024]
approximations [Bishop, 2006]) and not aligned with
shows that uncorrelated models cannot generally be
thementionedsubspace.Thismismatchleadstoapoor
infinitesimally invariant to reparametrizations even if
posteriorapproximationthattendstounderfit.Wesug-
this property is held by the true posterior.
gest, quite simply, to project the approximate posterior
to the subspace in which underfitting cannot happen. The linearized Laplace approximation (lla; Im-
mer et al. [2021b], Khan et al. [2019]) use a first-
Why is this beneficial? Our proposed posterior
order Taylor expansion of f(θ,x) ≈ fθmap(θ,x) =
reflectsthedegreesoffreedominthemodelthatfunda- lin
mentally cannot be determined by even noise-free data.
f(θmap,x)+J θmap(x)(θ−θmap),whereθmapistheexpan-
sion point (usually a map estimate of the parameter).
Predicting according to this distribution gives reliable
Secondly, lla perform a standard Laplace approxima-
out-of-distribution detection and general uncertainty
tion [MacKay, 1992] of the linearized model, yielding
quantification without underfitting. Our approach cap-
t au ccre us rac co yr pre rl ea dt ii co tn ios nb se wtw hie leen scl aa ly ine grs lina en ad rlyre wta itin hs mh oi dgh el- qlla(θ|D)=N (cid:16) θ (cid:12) (cid:12) θmap, (αI+ggn θmap)−1(cid:17) (2)
size. Table 1 contrasts our method with existing ones. ggn =J⊤ H J ∈RP×P. (3)
θmap θmap θmap θmap
Why is this difficult? Wewillsoonseethattherele-
Here ggn is the so-called generalized Gauss-
vantsubspaceonwhichtoprojectisgivenbythekernel θmap
Newton matrix, α is the prior precision, H (x) =
(i.e. null space) of a matrix that is quadratic in the θ
−∂2 logp(y|f(θ,x))∈RO×O and H ∈RNO×NO
number of model parameters. Even for models of mod- f(θ,x) θ
is its stacked counterpart. This Hessian takes a sim-
estsize,thismatrixistoolargetobestoredinmemory
ple closed-form for common likelihoods, e.g. it is the
and direct projection methods cannot be applied. We
identity for Gaussian likelihoods [Immer et al., 2021b].
propose a linear-time sampling algorithm that can be
For notational simplicity, we treat this Hessian as an
implemented entirely using automatic differentiation.
identity for the remainder of this paper.
Paper outline. Sec.2givesthebackgroundtoderive
The lla has a strong empirical performance [Immer
ourapproach.Awiderdiscussionofrelatedworkispost-
et al., 2021b], but it is computationally taxing as the
poned to Sec. 5. We develop our approach in two steps.
ggn has size P ×P. This is infeasible to instantiate
First, Sec. 3 describes our proposed posterior approx-
even for models of modest size, and the O(P3) cost
imation, while Sec. 4 derives an efficient sampling algo-
associated with sampling presents a significant com-
rithm. Empirical investigations are conducted in Sec. 6.
putational challenge. Iterative matrix-free solvers can
potentially alleviate these concerns [Roy et al., 2024],
2 Background and notation but the ggn is highly ill-conditioned [Papyan, 2020,
Miani et al., 2024] and this approach requires overcom-
ing issues of numerical stability. Practical lla imple-
Notation. Let f : RP ×RI → RO denote a neural
mentations therefore resort to sparse, e.g. diagonal or
network with parameter θ ∈ RP that maps inputs
block-diagonal, approximations of the ggn.
x ∈ RI to outputs y ∈ RO. We define the per-datum
JacobianasJ (x)=∂ f(θ,x)∈RO×P anditsstacked
θ θ
counterpart J =[J (x );...;J (x )]∈RNO×P for a 3 The proposed approximate posterior
θ θ 1 θ N
fixed training dataset D ={x ,...,x }.
1 N
We next propose a fully correlated Gaussian posterior
Gaussian approximate posteriors, i.e. p(θ|D) ≈
that is guaranteed to not underfit. Unless otherwise
q(θ|D) = N(θ|µ,Σ), are ever-present in Bayesian
stated,thepresentedresultsarenovelcontributionsand
deep learning [Blundell et al., 2015, Botev et al., 2017,
proofs of all theorems can be found in the appendix.
Sharma et al., 2021, Khan et al., 2018, Maddox et al.,
2019, Stephan et al., 2017, Osawa et al., 2019, Antorán As alluded to, we propose restricting the posterior
et al., 2022]. For modern neural networks, the param- covariance to a particular subspace of the parameter
eter dimension P can easily be several billions, such space.LettingU∈RP×R denoteanorthogonalbasisof
that even storing the covariance Σ in memory is in- this subspace, we consider an isotropic model therein,
feasible. To allow for linear scaling, common Gaussian
approximate posteriors are realized by disregarding qproj(θ|θmap,D)=N (cid:0) θmap,α−1UU⊤(cid:1) . (4)Marco Miani†, Hrittik Roy†, Søren Hauberg
Table 1: Comparisons between Laplace approximations. N: number of data points, O: output dimensions, P:
number of parameters, P , P : input and output dimensions of layer l, P : last layer parameters.
l,in l,out ll
Method Correlation Space per-sampleTime preprocessingTime Error Model Tractable
structure complexity complexity complexity bound agnostic optimallml
Diagonal P P PNO ✗ ✓ ✗
Kronecker-Factored PL l=1P l2 ,in+P l2
,out
PL l=1P l2 ,in+P l2
,out
PL l=1NPl,in+NPl,out+P l3 ,in+P l3
,out
✗ ✗ ✗
Last-Layer P2 P2 O2NP2+P3 ✗ ✓ ✗
ll ll ll ll
Thispaper P+NO2orP+N tmaxPN+NO3ortmaxPN 0 ✓(Lemmas3.5&4.3) ✓ ✓(Sec.3)
Specifically,wechoosetheabovesubspaceasthekernel These results can be contrasted with lla, where we
(i.e. null space) of the ggn matrix and call the result can show the following result.
the projected posterior. Formally, Theorem 3.3. For α>0, the predictive variance of
UU⊤ =I −P(ggn)=I −V [D>0] V⊤, (5) lla on any training datapoint is positive and bounded,
P P
Oγ2 Oλ2
where VDV⊤ is an eigen decomposition of the ggn ≤Var fθmap(θ,x)≤ for x∈D.
matrix and [D>0]∈RP×P is diagonal with elements
γ2+α θ∼qlla lin λ2+α
1 where D holds positive values. We use the shorthand
Here, λ and γ are the largest and smallest singular
notation P(ggn) to denote the projection onto the im-
values of the dataset Jacobian J , respectively.
age of the ggn. We next justify the projected posterior θmap
through the lens of underfitting. Here, γ is strictly positive with high probability under
reasonable assumption [Bombari et al., 2022, Nguyen
The projected posterior never underfits. When
using the linearized neural network, fθmap(θ,x) = et al., 2021, Karhadkar et al., 2024]. lla then under-
wf( hθ em nap J, θx m) ap+ (xJ )θ (m θap −(x) θ( mθ ap− )θ =map 0), ow ne ta hv eoi td rli aun in nd ine grfit dt ain tag . ( (fi ict n.s f t. rw aLi ct e th m abh mi lg eah )3p g.r g1o ) nb .a -N bbi aoli stt ey e d, tw h coh a vi tc ah t rh iac io s nn cit s er .a os Wnt ls y eo t eu r xr u pea ep cfp o tr ro ta hthc ah e
t
The linearized model then avoids underfitting when
sparse approximations to this covariance, e.g. diagonal
θ−θmapisrestrictedtothekernel(nullspace)ofJ
θmap
=
and kfac, has even higher training set variance.
[J (x );...;J (x )]. For the proposed projected
θmap 1 θmap N
posterior (4), we, thus, choose U as an orthonormal Underfitting in existing models. The above anal-
basis of the Jacobian kernel and note that this kernel ysis justifies the projected posterior and also sheds
coincides with the zero eigenvectors of the ggn. light on why current Bayesian approximations often
underfit. For efficiency, mean field approximations of
These considerations can be formalized as follows.
the posterior covariance are quite common, e.g. diag-
Lemma 3.1. The projected posterior (4) is supported onal or Kronecker factored covariances [Ritter et al.,
on equal functions on the training data, i.e. ∀x∈D 2018,MartensandGrosse,2015].Theseapproximations
will generally sample outside the ggn kernel, implying
f lθ inmap(θ,x)=f(θmap,x) ∀θ ∼qproj, (6)
strictly positive predictive variances on the training
which implies that Var fθmap(θ,x)=0. data even for noise-free data. This reasoning also mo-
θ∼qproj lin tivates recent works that have attempted to improve
We emphasize that the statement only holds on the these approximations by making their spectrum more
training data. Elsewhere, the approximate posterior aligned with the spectrum of the ggn [George et al.,
yields strictly positive variances with high probability 2018, Dhahri et al., 2024].
under strong but reasonable assumptions.
Computationalbenefits. Beyondtheabovetheoret-
Lemma 3.2. Let J θ=[J θ(x 1)⊤...J θ(x N)⊤]⊤ and ical motivations, our projected covariance also brings
x test ∈RI, then computational benefits. Since U is an orthonormal
basis, the covariance UU⊤ is a projection matrix, im-
Var fθmap(θ,x )>0 (7)
θ∼qproj lin test plying that its eigenvalues are all 0 or 1. This, in turn,
if Rank(cid:18)(cid:16) Jθmap (cid:17)(cid:16) Jθmap (cid:17)⊤(cid:19) >Rank(cid:0)J J⊤ (cid:1) . implies that UU⊤ =(UU⊤)2 =(UU⊤)⊤ =(UU⊤)†,
Jθmap(xtest) Jθmap(xtest) θmap θmap where † denotes pseudo-inversion. Drawing samples
fromq,thus,onlyrequiresmatrix-vectorproductswith
Jointly the two lemmas show that out-of-distribution
UU⊤ and the matrix need not be instantiated. The
data is guaranteed to have higher predictive variance
only open question is then how to efficiently perform
thanthetrainingdata.The(technical)rankassumption
such matrix-vector products. We answer this in Sec. 4.
in Lemma 3.2 is known to be satisfied with high prob-
ability under reasonable assumptions [Bombari et al., SincealleigenvaluesofUU⊤ are0or1,wecanapriori
2022, Nguyen et al., 2021, Karhadkar et al., 2024]. expect the matrix to yield numerically stable compu-Bayes without Underfitting: Fully Correlated Deep Learning Posteriors via Alternating Projections
tations even at moderate numerical precision. This
Figure 2: To project a point
is in contrast to the ggn, which easily has condition
onto the intersection of two
numbers that exceed 106 [Miani et al., 2024].
subspaces, we alternate be-
Tractable model selection. A benefit of Bayesian tween projecting onto the in-
neuralnetworksisthatwecanchooseαbymaximizing dividual subspaces.
the marginal likelihood, p(D|α), on the training data,
i.e. without relying on validation data [MacKay, 1995].
However, since the marginal likelihood of the true pos- Intersections of kernels. To arrive at a feasible
terior is intractable, it is common to consider that of algorithm, we note that the kernel of M⊤M is the
the Laplace approximation [Immer et al., 2021a] kernel of a sum of positive semi-definite matrices,
log qlla(D|α)=log p(D|θmap)+log p(θmap|α) ker(cid:0)P bM⊤
b
M b(cid:1)=T bker(M⊤
b
M b), (11)
1 (cid:18) 1 (cid:19) (8) where we have decomposed M into batches,
− logdet (ggn+αI) .
2 2π
M 
1 B
T α.hi Is nc ca on ntt rh ae sn t,b te heop pt ri om jeiz ce td ivenu pm ose tr ei rc ia ol rly (4to
)
des ot ei sm na ote
t
M=

. . . 

M⊤M=X M⊤
b
M b. (12)
M b=1
require optimization as α is available in closed form. B
Lemma 3.4. The marginal likelihood for the projected Hence,wecanprojectontotheggnkernelbyprojecting
posterior (4) has a globally optimal α given by onto the intersection of per batch-ggn kernels.
von Neumann’s [1949] alternating projections
α∗ =
∥θmap∥2
. (9) algorithm projects onto intersections of subspaces.
P −Tr(I −P(ggn ))
P θmap Fig. 2 illustrates this idea, formalized in Lemma 4.1.
In practice, the trace can be approximated using Lemma 4.1. For any row-partition of a matrix M=
Hutchinson’s estimator [1989].
(cid:0)M⊤
···
M⊤(cid:1)⊤
it holds that
1 B
Links to LLA. Theprojectedposteriorcanbeviewed I−P(cid:16) M⊤M(cid:17) = lim (cid:16) Q (cid:0)I−P(M⊤M )(cid:1)(cid:17)t . (13)
as a fully correlated tractable approximation to the t→∞ b b b
lla. The following statement shows that the difference
Moreover, the limit converges linearly with a rate
between the two approximate posteriors is bounded. c=QB cos2(θ ) where θ = min ∠(M ,M ) and
b=1 b b b′̸=b b b′
Lemma 3.5. Let τ denote the smallest non-zero eigen- ∠(M ,M ) is the minimum angle between linear com-
b b′
value of the ggn[Nguyen et al., 2021], then binations of rows of M and linear combinations of
b
rows of M .
(cid:13) (cid:13) 1 b′
(cid:13)(αI +ggn)−1−α−1(I −P(ggn )(cid:13)≤
(cid:13) P P θmap (cid:13) τ +α In practice, we stop the algorithm after a number of
| {z } | {z }
llacov Projcov(scaled) iterations, so that the limit (13) stops at t=t . The
max
Additionally, let k denote the ggn rank and d the W
induced error is then upper bound by ctmax.
2
Wasserstein distance, then d2(qlla,qproj)≤(τ+α)−1k. Projection for the approximate posterior. The
algorithm projects to the ggn kernel, by iteratively
4 Sampling via alternating projections projecting to the kernels of per-batch ggns. Using
Eq.10,asinglestepofthealgorithmrequiresinverting
We next develop a novel scalable algorithm to sample an SO×SO matrix, which is computationally cheap
the projected posterior. Recall that we can simulate for small batch-sizes S. Fig. 3 illustrates this pipeline.
this posterior as UU⊤ϵ+θmap, where ϵ∼N(0,α−1I)
Implementation details. For sufficiently small
and UU⊤ projects to the ggn kernel. In general, the
batches, we can precompute and store the inverses of
orthogonal projection onto a subspace spanned by the
M M⊤ ∈RSO×SO. Thereafter, matrix-vector products
columns of any matrix M∈RR×P, where R<P, is witb
h
Mb
and M⊤ can be efficiently computed using
Jacobian-vector products and vector-Jacobian prod-
P(cid:0)M⊤M(cid:1)=M⊤(cid:0)MM⊤(cid:1)−1M ∈RP×P. (10)
ucts, respectively. We, thus, do not need to instantiate
the per-batch ggn matrices, which gives rise to an
TheprojectionintothekernelofM⊤Misthengivenby
efficient low-memory sampling algorithm.
the matrix I−P(cid:0)M⊤M(cid:1). In our case M=J H1/2 ∈
θ θ
RNO×P and Eq. 10 requires inverting a NO×NO Complexity. The space complexity of the algorithm
matrix, which is infeasible even for moderate datasets. is O(P +NSO2) and time complexity is O(t PN +
maxMarco Miani†, Hrittik Roy†, Søren Hauberg
Figure 3: Visualization of Jacobian projections Direct calculation of the projection(left) involves inverting a
large NO×NO matrix. This is replaced by an infinite (in practice, truncated) series of cheap projections (right)
which only require precomputing and storing inverses of several small SO×SO matrices.
NS2O3).ForoverparametrizedmodelswhereP ≫NO which implies that
thesecomplexitiesboildowntoO(P)andO(t PN),
respectively. This matches the cost of
trainingm fa ox
r t
max
Var θ∼qlossl(f(θ,x n),y n)≤O(α2). (16)
epochs.Consequently,theprojectionisfeasibleonhard-
ware that can train a model. In particular, these results hold regardless of whether
weusethelinearizedmodelfθmap orthestandardonef.
lin
4.1 Scaling to high-dimensional outputs Complexity. The space complexity of the algorithm
is O(P+NS) and the time complexity is O(t PN+
The computational complexity of the projection algo- max
NS2).
rithm scales superlinearly with the model’s output size.
Formanytasksthisisunproblematic,butitrendersthe
5 Related work
method inapplicable to e.g. image-generative models.
To handle high-dimensional outputs, we propose re- Deep neural networks are known to suffer from poor
moving the requirement of preserving the predictions calibration, where predictive uncertainties are often
of θmap at the training data. Instead, we propose to indistinguishable between in-distribution and out-of-
(locally) preserve the loss at each training point. This distribution data [Guo et al., 2017]. Deep ensembles
is achieved by considering the loss-Jacobian, i.e. the [Lakshminarayanan et al., 2017] remains one of the
stacking of the per-datum loss-gradients. most widely used solutions as it has strong empiri-
cal performance. Unfortunately, this approach requires
 ∇ θl(f(θ,x 1),y 1)  retrainingmultiplemodelsfromscratch,makingitcom-
.
JL = . . ∈RN×P (14) putationally prohibitive for large models.
θ  
∇ θl(f(θ,x N),y N) Gaussian approximate posteriors are commonly
used to avoid retraining from scratch and instead only
explore the mode of a single model. Established ap-
Lemma 4.2. The kernel of the stacked loss gradients
JL ∈ RN×P contains the kernel of the full Jacobian proaches in this area include Bayes by backprop [Blun-
Jθ ∈ RNO×P, i.e. ker(J ) ⊆ ker(JL). Further, note dell et al., 2015]), stochastic weight averaging (swag)
thθ at these subspaces are iθ dentical forθ O =1. [Maddox et al., 2019], and Laplace approximations
[Daxberger et al., 2021a]. Our work can be viewed
Intuitively, for each datapoint, the gradient is a as a variant of the Laplace approximation that avoids
linear combination of its Jacobian rows, namely the underfitting commonly seen in Gaussian posteriors.
∇ θl(f(θ,x),y) = ∇ f(θ,x)l(f(θ,x),y)J θ(x). This ap- The computational costs of working with high-
proach can, thus, be seen as aggregating each per-
dimensional Gaussians can be daunting. Common
datum Jacobian into a single meaningful row, lowering
workarounds include using low-rank or sparse covari-
the row count by a factor O.
ances [Maddox et al., 2019, Ritter et al., 2018, Deng
We propose the loss-projected approximate posterior et al., 2022], subspace inference approaches [Izmailov
etal.,2020],andsubnetworkinference[Daxbergeretal.,
qloss(θ|θmap,D)=N (cid:0) θmap,α−1U LU⊤ L(cid:1) , (15) 2021b]. These methods reduce computational costs
while preserving strong predictive performance. In con-
where U ∈RP×R denotes an orthogonal basis of the trast, our method additionally provides a rigorous the-
L
kernel of JL. Samples from this approximate posterior oretical justification for the choice of subspace. We can
θ
are guaranteed to have the same per-datum loss as the infer properties of distributions within this subspace
mode parameter, up to first order, and derive formal error bounds, a level of theoretical
grounding often lacking in other approaches.
Lemma 4.3. For any θ ∼qloss and x
n
∈D it holds
Antoran et al. [2023] use a sample-then-optimize pro-
|l(f(θ,x n),y n)−l(f(θmap,x n),y n)|=O(∥θ−θmap∥2)
cedure to simulate the lla posterior, which has someBayes without Underfitting: Fully Correlated Deep Learning Posteriors via Alternating Projections
WefirstassessdifferentLaplaceapproximationsinatoy
regressiontask.Foongetal.[2019]showsthatBayesian
neural networks with mean-field variational inference
fail to capture in-between uncertainties. We observe a
similar issue with sparsely correlated Laplace approxi-
mations (Fig. 4). In contrast, the projected posterior
correlates all parameters, improving predictive uncer-
taintyestimatesforunseenregions.Thisillustratesthat
Figure 4: The sparse approximations of the posterior
maintaining full parameter correlations is essential for
fail to capture in-between uncertainty whereas fully
accurate in-between uncertainties.
correlated posteriors can capture them.
6.2 Predictive uncertainties on standard
algorithmic similarities to our work. We both simulate image classification
a simple distribution followed by a gradient-based iter-
Hypothesis: The projected posterior provide
ative refinement. While Antoran et al. [2023] focus on
competitive or superior predictive uncertainties
lla,weaimtoavoidunderfitting,whicharepotentially
across tasks, including in-distribution calibra-
complementary objectives. Here we emphasize that our
tion, robustness to distribution shifts, and out-
approachisgeneralandcanbeappliedtoanyBayesian
of-distribution detection.
approximation in a post-hoc manner.
AfrequentistperspectivewasgivenbyMadrasetal. In-distribution performance (Table 2). We next
[2020]. Their local ensembles perturb a map estimate consider standard image classification tasks. We train
in the kernel of the network’s Hessian. Algorithmically, a LeNet [LeCun et al., 1989] on MNIST and Fashion
theyapproximatethelargestHessianeigenvectorsusing MNIST, and a ResNet [He et al., 2016] on CIFAR-10
Lanczos’s decomposition [Meurant, 2006] and assume [Krizhevskyetal.,2009]andSVHN[Netzeretal.,2011].
that the orthogonal complement is the Hessian kernel. Keeping the map fixed we assess the predictive poste-
We improve this in several ways. First, we show that rior of all methods using metrics such as confidence,
the ggn kernel is more appropriate than the Hessian accuracy, negative log-likelihood (NLL), Brier score
kernel. Secondly, we propose a significantly more effi- [Brier, 1950], expected calibration error (ECE), and
cient algorithm that projects onto the ggn kernel by maximumcalibrationerror(MCE)[Naeinietal.,2015].
leveraging its partitioned structure. Our method guar-
As shown in Table 2, the projected posterior matches
antees exact convergence to the kernel, and it is not
the map in-distribution, as expected from Lemma 3.1,
memory-limited as it avoids storing Lanczos vectors.
with zero variance in-distribution and higher variance
Alternating projections first appeared in von Neu- out-of-distribution. The loss-projected posterior im-
mann’s notes on operator theory [1949], while Kayalar proves calibration, which is discussed in Appendix B.
andWeinert[1988]provedtheconvergencerate.Toour
Distributionshift(Fig.6). Toassessrobustnessun-
knowledge,theonlyrecentmachinelearninguse-caseis
derdistributionshifts,weevaluatetherobustnessofpre-
invertingGrammatricesinGaussianprocessregression
dictive uncertainties using rotated-mnist, rotated-
[Wuetal.,2024].Thisisratherdifferentfromourwork.
fmnist,andcorruptedcifar(withfogandGaussian
blur).Theloss-projectedposteriormaintainslow,stable
6 Experiments calibration error with increasing shift intensity, while
the projected posterior retains high accuracy (Fig. 6).
We benchmark our projected and loss-projected pos- Out-of-distribution detection (Table 3). Forout-
teriors against popular post-hoc Bayesian methods,
of-distribution (ood) detection, we use the maximum
including swag, and diagonal and last-layer Laplace.
variance of logits across output dimensions as an ood
We also include the prediction map. Baseline prior
scoreforallBayesianposteriors,whilethemapbaseline
precisions are tuned via grid search, while projection
employs maximum softmax, which is a strong baseline.
posteriors use the optimal prior precision (9). Details
Lemma 3.1 dictates that the in-distribution variance of
on models and hyperparameters are in Appendix C.
the projected posterior is zero, providing a strong ood
detection signal. Table 3 supports the theory, showing
6.1 Toy regression that the projected posterior achieves superior auroc
scores across various ood tasks over baselines.
Hypothesis: The projected posterior captures
in-between uncertainties better than sparsely cor-
6.3 ImageNet and CelebA vision transformer
related Laplace.Marco Miani†, Hrittik Roy†, Søren Hauberg
Table 2: In-distribution performance across methods trained on mnist, fmnist SVHN and
CIFAR-10.
Conf.(↑) NLL(↓) Acc.(↑) Brier(↓) ECE(↓) MCE(↓)
MAP 0.981±0.002 0.080±0.005 0.977±0.002 1.775±0.004 0.787±0.009 0.895±0.014
Projectedposterior(ours) 0.981±0.002 0.080±0.005 0.977±0.002 1.774±0.004 0.787±0.009 0.895±0.014
Loss-projectedposterior(ours) 0.813±0.018 1.225±0.099 0.949±0.000 1.532±0.028 0.666±0.007 0.894±0.011
SWAG 0.982±0.001 0.064±0.006 0.982±0.000 1.776±0.002 0.788±0.005 0.906±0.013
Last-Layer 0.977±0.002 0.090±0.005 0.975±0.002 1.768±0.003 0.784±0.007 0.887±0.008
Diagonal 0.951±0.008 0.129±0.016 0.975±0.001 1.727±0.012 0.784±0.008 0.894±0.015
MAP 0.938±0.005 0.325±0.011 0.897±0.002 1.713±0.008 0.732±0.006 0.901±0.006
Projectedposterior(ours) 0.937±0.005 0.325±0.011 0.897±0.002 1.713±0.008 0.732±0.006 0.902±0.006
Loss-projectedposterior(ours) 0.744±0.031 1.529±0.371 0.871±0.006 1.441±0.041 0.617±0.025 0.901±0.013
SWAG 0.931±0.006 0.327±0.001 0.898±0.001 1.703±0.008 0.725±0.003 0.907±0.003
LastLayer 0.931±0.005 0.339±0.011 0.896±0.002 1.703±0.008 0.727±0.004 0.902±0.004
Diagonal 0.735±0.024 0.922±0.051 0.855±0.000 1.431±0.033 0.621±0.021 0.895±0.021
MAP 0.949±0.004 0.188±0.004 0.949±0.002 1.691±0.004 0.740±0.012 0.889±0.004
Projectedposterior(ours) 0.949±0.004 0.188±0.004 0.949±0.002 1.691±0.004 0.740±0.012 0.889±0.007
Loss-projectedposterior(ours) 0.948±0.005 0.191±0.007 0.949±0.003 1.685±0.013 0.734±0.017 0.880±0.012
SWAG 0.897±0.007 0.217±0.014 0.947±0.004 1.606±0.010 0.745±0.007 0.874±0.003
Last-Layer 0.943±0.005 0.197±0.009 0.946±0.001 1.681±0.006 0.740±0.007 0.899±0.009
Diagonal 0.948±0.003 0.187±0.005 0.949±0.002 1.689±0.003 0.742±0.011 0.899±0.000
MAP 0.952±0.002 0.422±0.011 0.894±0.002 1.698±0.038 0.703±0.013 0.878±0.009
Projectedposterior(ours) 0.952±0.001 0.422±0.011 0.894±0.002 1.698±0.038 0.703±0.013 0.878±0.002
Loss-projectedposterior(ours) 0.701±0.013 2.643±0.205 0.855±0.002 1.387±0.018 0.559±0.006 0.802±0.005
SWAG 0.914±0.035 0.445±0.063 0.865±0.029 1.670±0.049 0.694±0.018 0.881±0.005
Last-Layer 0.944±0.001 0.406±0.005 0.894±0.001 1.712±0.001 0.704±0.000 0.880±0.007
Diagonal 0.934±0.028 0.465±0.069 0.872±0.032 1.698±0.038 0.703±0.013 0.873±0.005
Table 3: Out-of-distribution auroc (↑) performance for mnist, fmnist, SVHN and
CIFAR-10.
Trainedon mnist fmnist CIFAR-10 SVHN
Testedon fmnist emnist kmnist mnist emnist kmnist svhn CIFAR-100 CIFAR-10 CIFAR-100
MAP 0.924±0.015 0.888±0.010 0.846±0.019 0.714±0.001 0.788±0.016 0.664±0.012 0.874±0.002 0.816±0.026 0.960±0.007 0.954±0.008
Proj.(ours) 0.926±0.013 0.904±0.004 0.862±0.011 0.861±0.029 0.844±0.032 0.867±0.012 0.881±0.002 0.836±0.004 0.960±0.002 0.957±0.001
Loss-proj.(ours) 0.899±0.011 0.893±0.006 0.856±0.002 0.914±0.035 0.928±0.007 0.907±0.026 0.863±0.008 0.800±0.013 0.966±0.009 0.960±0.006
SWAG 0.917±0.024 0.916±0.010 0.861±0.032 0.685±0.014 0.751±0.032 0.655±0.015 0.798±0.040 0.782±0.042 0.777±0.029 0.787±0.027
Last-Layer 0.793±0.215 0.782±0.182 0.759±0.166 0.768±0.001 0.824±0.016 0.699±0.020 0.811±0.024 0.801±0.026 0.914±0.005 0.908±0.005
Diagonal 0.772±0.218 0.768±0.191 0.745±0.174 0.726±0.028 0.725±0.034 0.683±0.013 0.836±0.022 0.806±0.027 0.917±0.005 0.916±0.004
Table 4: In- and out-of-
Conf.(↑) NLL(↓) Acc.(↑) Brier(↓) ECE(↓) MCE(↓) AUROC (↑)
map 0.626 1.358 0.726 0.407 0.136 0.583 0.751 distribution metrics for
Loss-projectedposterior 0.618 1.427 0.716 0.394 0.107 0.259 0.756 SWIN transformer trained
on ImageNet and tested on
places365.
Figure 5: Variational autoencoder reconstructions and corresponding uncertainty estimates. Left:
mean reconstructions of MNIST and Fashion MNIST images sampled from the latent space. Right: pixel-wise
uncertainty estimates generated by sampling decoder parameters from the Loss Kernel Posterior, highlighting
key semantic features such as edges and contours. This demonstrates Projected Laplace’s ability to capture
uncertainty in high-dimensional generative models.
TSINM
TSINMF
NHVS
01-RAFICBayes without Underfitting: Fully Correlated Deep Learning Posteriors via Alternating Projections
Table 5: auroc (3 seeds) for a VAN trained on
CelebA. Details are in the appendix.
Food101 Baldonly Eyeglassesonly Mustacheonly
Maxlogit 0.959±0.008 0.32±0.00 0.57±0.01 0.40±0.03
Deepensemble 0.957 0.71 0.74 0.61
Proj.(ours) 0.958±0.004 0.81±0.03 0.75±0.01 0.66±0.01
Loss-proj.(ours) 0.947±0.007 0.76±0.03 0.72±0.01 0.62±0.01
Proj-Lanczos 0.954±0.004 0.80±0.03 0.74±0.01 0.64±0.01
Localensemble 0.951±0.003 0.79±0.04 0.73±0.01 0.63±0.01
LLA 0.954±0.00 0.80±0.03 0.74±0.01 0.64±0.01
LLA-diag 0.797±0.021 0.54±0.03 0.57±0.02 0.45±0.03
SCOD 0.964±0.000 0.79±0.03 0.73±0.01 0.64±0.01
SWAG 0.740±0.021 0.70±0.08 0.52±0.04 0.49±0.04
SLU 0.953±0.003 0.80±0.03 0.74±0.01 0.64±0.01
ing settings and is only second to scod in the last.
The loss-projected posterior shows a minor drop in
performance.
6.4 Generative models
Hypothesis: The loss-projected posterior scales
togenerativemodelswithlargeoutputdimensions.
We evaluate the loss-projected posterior on variational
autoencoders [Kingma, 2013] trained on MNIST and
Figure 6: Model calibration and fit on in-distribution
Fashion MNIST, showcasing its flexibility with high-
test data (left) and under distribution shift (middle,
dimensional outputs and diverse tasks. After training,
right) where we plot shift intensities against accuracy
wesampledecoderparameterstogenerateimagesfrom
and expected calibration error (ece), respectively.
latentspacesamples.Fig.5showsthatthesampledde-
coders produce diverse reconstructions with pixel-wise
uncertaintyestimatesthatcapturemeaningfulfeatures.
Hypothesis: The projected posterior scales effec-
tivelytolargemodelsanddatasets,suchasSWIN This experiment highlights that our approach scales
transformers trained on ImageNet and CelebA. to generative models while preserving essential predic-
tive uncertainties.Itsadaptabilitybeyondclassification
makes it a robust tool for diverse tasks.
We assess the scalability of our approach by sampling
from the loss-projected posterior of a SWIN trans-
former [Liu et al., 2021] pre-trained on the ImageNet- 7 Summary and limitations
1K dataset [Deng et al., 2009, Russakovsky et al.,
2015], which includes about 1 million images of size Theoretically we show that our proposed projected
224×224×3 with 1000 output classes. Table 4 shows posterior isoptimalw.r.t.underfittingfornoiselessdata.
marginal improvement in-distribution calibration over We further show that existing Laplace approximations
map alongside improved out-of-distribution detection do not meet this fundamental requirement. However,
on places365 [Zhou et al., 2017]. The main point is the applicability to high-noise data remains unresolved.
that our method scales gracefully to large models and
Algorithmically we introduce a memory-efficient al-
datasets due to linear complexity.
gorithm for computing projection-vector-products for
We further consider a 4M parameter Visual Attention a Jacobian kernel. This approach is based on the novel
network(VAN)[Guoetal.,2023].WetrainonCelebA idea of representing the kernel as an intersection of ker-
[Liu et al., 2015] holding out three classes. Table 5 re- nels, which enables the use of alternating projections.
ports auroc scores to measure the ability to detect
Empirically our methods outperform existing base-
thethreeheld-outclassesandFood101[Bossardetal.,
linesinout-of-distributiondetectionandin-distribution
2014]asout-of-distribution.Bothourmethodandbase-
calibration in diverse settings, ranging from toy regres-
lines have a limited memory budget of 300P numbers.
sion problems to vision transformers on ImageNet.
lla and local ensemble are implemented using Lanc-
zos and we also include our projected score computed Extending the method can be done in several ways.
with Lanczos as a reference. The projected posterior We merely project an isotropic Gaussian to isolate
outperforms the baselines on the three most challeng- the strength of the projection itself, but we stressMarco Miani†, Hrittik Roy†, Søren Hauberg
that the algorithm lets us project samples from any A. Botev, H. Ritter, and D. Barber. Practical gauss-
distribution. For example, it is straightforward to newton optimisation for deep learning. In Inter-
project samples from last-layer Laplace into the kernel. national Conference on Machine Learning, pages
We emphasize that our method is very general as it 557–565. PMLR, 2017.
works for any differentiable model, unlike e.g. kfac
L. M. Bregman. Finding the common point of con-
[Dangel et al., 2020] which requires network-specific
vex sets by the method of successive projection. In
implementations. Our open-source implementation is
Doklady Akademii Nauk, volume 162, pages 487–490.
applicable whenever we can access Jacobian-vector
Russian Academy of Sciences, 1965.
and vector-Jacobian products.
G. W. Brier. Verification of forecasts expressed
in terms of probability. Monthly Weather
Acknowledgements
Review, 78(1):1 – 3, 1950. doi: 10.1175/
1520-0493(1950)078<0001:VOFEIT>2.0.CO;2.
This work was supported by a research grant (42062)
fromVILLUMFONDEN.Thisworkwaspartlyfunded URL https://journals.ametsoc.org/view/
by the Novo Nordisk Foundation through the Center journals/mwre/78/1/1520-0493_1950_078_
for Basic Research in Life Science (NNF20OC0062606).
0001_vofeit_2_0_co_2.xml.
This project received funding from the European Re- F. Dangel, F. Kunstner, and P. Hennig. BackPACK:
search Council (ERC) under the European Union’s Packing more into backprop. In International Con-
Horizon Programme (grant agreement 101125003). ference on Learning Representations, 2020. URL
https://openreview.net/forum?id=BJlrF24twB.
References DarshanDeshpande. Jax models. https://github.
com/DarshanDeshpande/jax-models, 2021.
Z. Allen-Zhu, Y. Li, and Y. Liang. Learning and gener-
E.Daxberger,A.Kristiadi,A.Immer,R.Eschenhagen,
alizationinoverparameterizedneuralnetworks,going
M. Bauer, and P. Hennig. Laplace redux-effortless
beyond two layers. Advances in neural information
bayesian deep learning. Advances in Neural Infor-
processing systems, 32, 2019.
mation Processing Systems, 34:20089–20103, 2021a.
J.Antorán,S.Padhy,R.Barbano,E.Nalisnick,D.Janz,
E. Daxberger, E. Nalisnick, J. U. Allingham, J. An-
and J. M. Hernández-Lobato. Sampling-based in-
torán, and J. M. Hernández-Lobato. Bayesian deep
ference for large linear models, with application to
learning via subnetwork inference. In International
linearised laplace. arXiv preprint arXiv:2210.04994, Conference on Machine Learning, pages 2510–2521.
2022.
PMLR, 2021b.
J. Antoran, S. Padhy, R. Barbano, E. Nalisnick,
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and
D. Janz, and J. M. Hernández-Lobato. Sampling-
L.Fei-Fei. Imagenet:Alarge-scalehierarchicalimage
based inference for large linear models, with appli-
database. In 2009 IEEE conference on computer
cation to linearised laplace. In The Eleventh Inter- vision and pattern recognition, pages 248–255. Ieee,
national Conference on Learning Representations, 2009.
2023. URL https://openreview.net/forum?id=
Z. Deng, F. Zhou, and J. Zhu. Accelerated linearized
aoDyX6vSqsd.
laplace approximation for bayesian deep learning. In
C. M. Bishop. Pattern recognition and machine learn- A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho,
ing, volume 4. Springer, 2006. editors, Advances in Neural Information Process-
C. Blundell, J. Cornebise, K. Kavukcuoglu, and ing Systems,2022. URLhttps://openreview.net/
D. Wierstra. Weight uncertainty in neural network. forum?id=jftNpltMgz.
In International conference on machine learning, R. Dhahri, A. Immer, B. Charpentier, S. Günne-
pages 1613–1622. PMLR, 2015. mann, and V. Fortuin. Shaving weights with oc-
S.Bombari,M.H.Amani,andM.Mondelli. Memoriza- cam’s razor: Bayesian sparsification for neural net-
tion and optimization in deep neural networks with works using the marginal likelihood. arXiv preprint
minimumover-parameterization. AdvancesinNeural arXiv:2402.15978, 2024.
Information Processing Systems, 35:7628–7640, 2022. A. Y. Foong, Y. Li, J. M. Hernández-Lobato, and R. E.
Turner. ’in-between’uncertainty in bayesian neural
L. Bossard, M. Guillaumin, and L. Van Gool. Food-
101–mining discriminative components with random networks. arXiv preprint arXiv:1906.11537, 2019.
forests. In Computer vision–ECCV 2014: 13th Eu- T. George, C. Laurent, X. Bouthillier, N. Ballas, and
ropean conference, zurich, Switzerland, September P. Vincent. Fast approximate natural gradient de-
6-12, 2014, proceedings, part VI 13, pages 446–461. scent in a kronecker factored eigenbasis. Advances
Springer, 2014. in Neural Information Processing Systems, 31, 2018.Bayes without Underfitting: Fully Correlated Deep Learning Posteriors via Alternating Projections
P.Germain,F.Bach,A.Lacoste,andS.Lacoste-Julien. sian processes. Advances in Neural Information Pro-
Pac-bayesian theory meets bayesian inference. Ad- cessing Systems (NeurIPS), 32, 2019.
vances in Neural Information Processing Systems,29, D. P. Kingma. Auto-encoding variational bayes. arXiv
2016.
preprint arXiv:1312.6114, 2013.
C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger.
A. Kristiadi, M. Hein, and P. Hennig. Being a bit
On calibration of modern neural networks. In In- frequentist improves bayesian neural networks. In
ternational conference on machine learning, pages
International Conference on Artificial Intelligence
1321–1330. PMLR, 2017.
and Statistics, pages 529–545. PMLR, 2022.
M.-H. Guo, C.-Z. Lu, Z.-N. Liu, M.-M. Cheng, and A. Krizhevsky, G. Hinton, et al. Learning multiple
S.-M. Hu. Visual attention network. Computational layers of features from tiny images. 2009.
Visual Media, 9(4):733–752, 2023.
B.Lakshminarayanan,A.Pritzel,andC.Blundell. Sim-
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual ple and scalable predictive uncertainty estimation
learning for image recognition. In Proceedings of using deep ensembles. Advances in Neural Informa-
the IEEE conference on computer vision and pattern tion Processing Systems, 30, 2017.
recognition, pages 770–778, 2016.
Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.
D. Hendrycks and K. Gimpel. A baseline for detect- Howard, W. Hubbard, and L. D. Jackel. Backpropa-
ing misclassified and out-of-distribution examples in gation applied to handwritten zip code recognition.
neural networks. arXiv preprint arXiv:1610.02136, Neural computation, 1(4):541–551, 1989.
2016.
Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning
M.F.Hutchinson.Astochasticestimatorofthetraceof face attributes in the wild. In Proceedings of Inter-
the influence matrix for laplacian smoothing splines. national Conference on Computer Vision (ICCV),
Communications in Statistics-Simulation and Com- December 2015.
putation, 18(3):1059–1076, 1989.
Z.Liu,Y.Lin,Y.Cao,H.Hu,Y.Wei,Z.Zhang,S.Lin,
A. Immer, M. Bauer, V. Fortuin, G. Rätsch, and K. M. and B. Guo. Swin transformer: Hierarchical vision
Emtiyaz. Scalable marginal likelihood estimation for transformerusingshiftedwindows. InProceedings of
model selection in deep learning. In International theIEEE/CVFinternationalconferenceoncomputer
Conference on Machine Learning, pages 4563–4573. vision, pages 10012–10022, 2021.
PMLR, 2021a.
D. J. MacKay. Probable networks and plausible
A. Immer, M. Korzepa, and M. Bauer. Improving pre- predictions-a review of practical bayesian methods
dictions of Bayesian neural nets via local lineariza- for supervised neural networks. Network: computa-
tion. In International Conference on Artificial In- tion in neural systems, 6(3):469, 1995.
telligence and Statistics (AISTATS), pages 703–711, D. J. C. MacKay. A practical Bayesian framework for
2021b.
backpropagation networks. Neural Computation, 4
P. Izmailov, W. J. Maddox, P. Kirichenko, T. Garipov, (3):448–472, 1992.
D. Vetrov, and A. G. Wilson. Subspace inference for W. J. Maddox, P. Izmailov, T. Garipov, D. P. Vetrov,
bayesian deep learning. In Uncertainty in Artificial and A. G. Wilson. A simple baseline for bayesian
Intelligence, pages 1169–1179. PMLR, 2020. uncertainty in deep learning. Advances in neural
K. Karhadkar, M. Murray, and G. Montúfar. Bounds information processing systems, 32, 2019.
for the smallest eigenvalue of the ntk for arbitrary D. Madras, J. Atwood, and A. D’Amour. Detecting
sphericaldataofarbitrarydimension. arXiv preprint extrapolation with local ensembles. In International
arXiv:2405.14630, 2024. Conference on Learning Representations, 2020. URL
S. Kayalar and H. L. Weinert. Error bounds for the https://openreview.net/forum?id=BJl6bANtwH.
method of alternating projections. Mathematics of J. Martens and R. Grosse. Optimizing neural networks
Control, Signals and Systems, 1(1):43–59, 1988. with kronecker-factored approximate curvature. In
M.E.Khan,D.Nielsen,V.Tangkaratt,W.Lin,Y.Gal, International conference on machine learning, pages
2408–2417. PMLR, 2015.
and A. Srivastava. Fast and scalable bayesian deep
learning by weight-perturbation in adam. In In- G. Meurant. The Lanczos and conjugate gradient algo-
ternational conference on machine learning, pages rithms: from theory to finite precision computations.
2611–2620. PMLR, 2018. SIAM, 2006.
M.E.Khan,A.Immer,E.Abedi,andM.Korzepa. Ap- M.Miani,L.Beretta,andS.Hauberg. Sketchedlanczos
proximate inference turns deep networks into Gaus- uncertainty score: a low-memory summary of theMarco Miani†, Hrittik Roy†, Søren Hauberg
fisher information. In Neural Information Processing F. Wenzel, K. Roth, B. S. Veeling, J. Świątkowski,
Systems (NeurIPS), 2024. L. Tran, S. Mandt, J. Snoek, T. Salimans, R. Jenat-
ton, and S. Nowozin. How good is the bayes poste-
M.P.Naeini,G.Cooper,andM.Hauskrecht.Obtaining
well calibrated probabilities using bayesian binning. rior in deep neural networks really? arXiv preprint
In Proceedings of the AAAI conference on artificial
arXiv:2002.02405, 2020.
intelligence, volume 29, 2015. K. Wu, J. Wenger, H. T. Jones, G. Pleiss, and J. Gard-
ner. Large-scale gaussian processes via alternating
Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu,
A.Y.Ng,etal. Readingdigitsinnaturalimageswith projection. In International Conference on Artificial
unsupervised feature learning. In NIPS workshop Intelligence and Statistics, pages 2620–2628. PMLR,
2024.
on deep learning and unsupervised feature learning,
volume 2011, page 4. Granada, 2011. Y.Zhang,Y.-S.Wu,L.A.Ortega,andA.R.Masegosa.
The cold posterior effect indicates underfitting, and
Q. Nguyen, M. Mondelli, and G. F. Montufar. Tight
cold posteriors represent a fully bayesian method
bounds on the smallest eigenvalue of the neural tan-
gent kernel for deep relu networks. In International to mitigate it. Transactions on Machine Learning
Conference on Machine Learning, pages 8119–8129. Research, 2024. ISSN 2835-8856. URL https://
PMLR, 2021.
openreview.net/forum?id=GZORXGxHHT.
B.Zhou,A.Lapedriza,A.Khosla,A.Oliva,andA.Tor-
K. Osawa, S. Swaroop, M. E. Khan, A. Jain, R. Es-
ralba. Places: A 10 million image database for scene
chenhagen, R. E. Turner, and R. Yokota. Practical
deep learning with bayesian principles. Advances in recognition. IEEE Transactions on Pattern Analysis
neural information processing systems, 32, 2019. and Machine Intelligence, 2017.
V.Papyan.Tracesofclass/cross-classstructurepervade
deep learning spectra. Journal of Machine Learning
Research, 21(252):1–64, 2020.
H. Ritter, A. Botev, and D. Barber. A scalable laplace
approximation for neural networks. In 6th interna-
tional conference on learning representations, ICLR
2018-conference track proceedings, volume 6. Interna-
tional Conference on Representation Learning, 2018.
H. Roy, M. Miani, C. H. Ek, P. Hennig, M. Pförtner,
L. Tatzel, and S. Hauberg. Reparameterization in-
variance in approximate bayesian inference, 2024.
URL https://arxiv.org/abs/2406.03334.
O.Russakovsky,J.Deng,H.Su,J.Krause,S.Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bern-
stein, et al. Imagenet large scale visual recognition
challenge. International journal of computer vision,
115:211–252, 2015.
A. Sharma, N. Azizan, and M. Pavone. Sketching
curvature for efficient out-of-distribution detection
fordeepneuralnetworks. InUncertainty in artificial
intelligence, pages 1958–1967. PMLR, 2021.
K. T. Smith, D. C. Solmon, and S. L. Wagner. Prac-
tical and mathematical aspects of the problem of
reconstructing objects from radiographs. 1977.
M.Stephan,M.D.Hoffman,D.M.Blei,etal. Stochas-
tic gradient descent as approximate bayesian infer-
ence. Journal of Machine Learning Research, 18
(134):1–35, 2017.
J. von Neumann. On rings of operators. reduc-
tion theory. Annals of Mathematics, 50:401,
1949. URL https://api.semanticscholar.org/
CorpusID:124439084.Bayes without Underfitting: Fully Correlated Deep Learning Posteriors via Alternating Projections
A Proofs
A.1 Proof of positive variance lemma
In this section, we prove Lemma 3.2. We restate it for convenience.
Let J =[J (x )⊤...J (x )⊤]⊤ and x ∈RI, then
θ θ 1 θ N test
Var fθmap(θ,x )>0
θ∼qproj lin test
Rank(cid:18)(cid:16) Jθmap (cid:17)(cid:16) Jθmap (cid:17)⊤(cid:19) >Rank(cid:0)J J⊤ (cid:1) .
Jθmap(xtest) Jθmap(xtest) θmap θmap
Proof. It is easier to show the contrapositive i.e. to show that if the variance is 0, then the square matrix has
rank at most NO.
Let’s first define a more compact notation J = J and J = J (x ) for the two Jacobians. Let
D θmap T θmap test
D =Rank(J ) and T =Rank(J ). It holds that D ≤NO and T ≤O since the rank is always upper bound by
T T
the number of rows. Consider their singular values decompositions
D T
X X
J = σ v V⊤ J = γ w W⊤, (17)
D i i i T j j j
i=1 j=1
where σ ,γ > 0, v ∈ RD, V ∈ RP, w ∈ RT, W ∈ RP, v⊤v = V⊤V = δ and w⊤W = W⊤W = δ
i j i i j j i i′ i i′ ii′ j j′ j j′ jj′
for any i,i′ = 1,...,D and j,j′ = 1,...,T. Moreover let V ,...,V ∈ RP be a orthonormal completion of
D+1 P
V ,...,V as a basis.
1 D
 I−P(ggn) 
z }| {
 T P ! 
Var θ∼qf lθ inmap(θ,x test)=Tr 

X γ jw jW j⊤ X V iV i⊤ γ j′W j′w j⊤ ′ 

(18)
j,j′=1 i=D+1 
P T
X X
= γ2W⊤V V⊤W (19)
j j i i j
i=D+1j=1
P T
X X
= γ2(W⊤V )2 (20)
j j i
i=D+1j,=1
and a sum of positive elements being 0 implies that every element is 0, thus
Var fθmap(θ,x )=0 =⇒ W ⊥V ∀j =1,...T ∀i=D+1,...,P (21)
θ∼q lin test j i
and since V ,...,V is a basis, this implies that there exists some coefficients β(j) such that W =PD β(j)V .
1 P k j k=1 k k
Consequently
D D
W W⊤ = X β(j)V β(j)V⊤ =X (β(j))2V V⊤ ∀j =1,...O (22)
j j k k k′ k′ k k k
k,k′=1 k=1Marco Miani†, Hrittik Roy†, Søren Hauberg
Now consider the matrix
(cid:18)J (cid:19)⊤(cid:18)J (cid:19)
D D =J⊤J +J⊤J (23)
J J D D T T
T T
D T
X X
= σ V V⊤+ γ W W⊤ (Eq. 22) (24)
i i i j j j
i=1 j=1
D T D
=X σ V V⊤+X γ X (β(j))2V V⊤ (25)
i i i j k k k
i=1 j=1 k=1
 
D D T
=X σ iV iV i⊤+X X γ j(β k(j))2 V kV k⊤ (26)
i=1 k=1 j=1
 
D T
=X σ i+X γ j(β i(j))2 V iV i⊤ (27)
i=1 j=1
which has rank equal to D. Then finally
(cid:18)J (cid:19)(cid:18)J (cid:19)⊤! (cid:18)J (cid:19)⊤(cid:18)J (cid:19)!
Rank D D =Rank D D =D ≤NO (28)
J J J J
T T T T
which concludes the proof.
A.2 Proof of the upper bound on Linearized Laplace predictive variance
In this section, we prove Theorem 3.3. We restate it below for convenience
Theorem 3.3. For α>0, the predictive variance of lla on any training datapoint is positive and bounded,
Oγ2 Oλ2
≤Var fθmap(θ,x)≤ for x∈D.
γ2+α θ∼qlla lin λ2+α
Here, λ and γ are the largest and smallest singular values of the dataset Jacobian J , respectively.
θmap
Proof. Let J = PNOσ w⊤v be a SVD decomposition of the full dataset Jacobian. Namely, w ∈ RNO,
θmap i=1 i i i i
v ∈RP for any i, and w⊤w =δ , v⊤v =δ for any i,j. Then it holds
i i i ij i i ij
NO(cid:18) 1 1(cid:19)
X
(ggn+αI)−1 =α−1I+ − v⊤v (29)
σ2+α α i i
i=1 i
and consequently
NO NO(cid:18) 1 1(cid:19) !
X X
J (ggn+αI)−1J⊤ = σ w⊤v α−1I+ − v⊤v σ v⊤w (30)
θmap θmap j j j σ2+α α i i k k k
j,k=1 i=1 i
XNO σ2 XNO (cid:18) 1 1(cid:19)
= i w⊤w + σ2 − w⊤w (31)
α i i i σ2+α α i i
i=1 i=1 i
XNO σ2
= i w⊤w (32)
σ2+α i i
i=1 i
Now note that each datapoint Jacobian can be written in terms of the full dataset Jacobian as J (x ) =
θmap n
PO f⊤e J where {f } and {e } are the canonical bases of RO and RNO, respectively.
i=1 i (n−1)O+i θmap i i=1...O i i=1...NOBayes without Underfitting: Fully Correlated Deep Learning Posteriors via Alternating Projections
We can finally express the Linearized Laplace predictive variance, for any train datapoint x , as
n
Var θ∼N(θmap,(ggn+αI)−1)f lθ inmap(θ,x n)=Tr(cid:0)J θmap(x n)(ggn+αI)−1J θmap(x n)⊤(cid:1) (33)
 
O
X
=Tr  f i⊤e (n−1)O+iJ θmap(x n)(ggn+αI)−1J⊤ θmape⊤ (n−1)O+jf j
i,j=1
O
X
= e J (ggn+αI)−1J⊤ e⊤
(n−1)O+i θmap θmap (n−1)O+i
i=1
=XO XNO σ j2
e w⊤w e⊤
σ2+α (n−1)O+i j j (n−1)O+i
i=1j=1 j
Now the upper bound follows by noting that for all j
σ2 σ2
j ≤max k (34)
σ2+α
k
σ2+α
j k
and thus from Eq. 33, noting that PNOw⊤w =I we have for any train datapoint x
j=1 j j NO n
Var θ∼N(θmap,(ggn+αI)−1)f lθ inmap(θ,x
n)=XO XNO σ2σ +j2
αe (n−1)O+iw j⊤w je⊤
(n−1)O+i
(35)
i=1j=1 j
σ2 XO XNO
≤max k e w⊤w e⊤
k
σ2+α (n−1)O+i j j (n−1)O+i
k i=1j=1
σ2 XO σ2
=max k e e⊤ =Omax k
k
σ2+α (n−1)O+i (n−1)O+i
k
σ2+α
k i=1 k
While the lower bound, similarly, follows by noting that for all j
σ2 σ2
j ≥min k (36)
σ2+α
k
σ2+α
j k
and thus from Eq. 33 we have for any train datapoint x
n
Var θ∼N(θmap,(ggn+αI)−1)f lθ inmap(θ,x
n)=XO XNO σ2σ +j2
αe (n−1)O+iw j⊤w je⊤
(n−1)O+i
(37)
i=1j=1 j
σ2 XO XNO σ2
≥min k e w⊤w e⊤ =Omin k
k
σ2+α (n−1)O+i j j (n−1)O+i
k
σ2+α
k i=1j=1 k
which concludes the proof by noting that the function σ 7→ σ is monotonic for any α > 0, and that the
σ+α
eigenvalues of the ggn are a simple function of the singular values of the Jacobian: λ (ggn)=max σ2 and
max k k
λ̸=0 (ggn)=min σ2. We emphasize that the singular values of the Jacobian affect the non-zero eigenvalues of
min k k
the ggn, thus the minimum singular value corresponds to the minimum-non-zero eigenvalue.
A.3 Proof of Lemma 3.4
Proof. It follows from equation 8 the approximate log marginal likelihood is given by:
1 (cid:18) 1 (cid:19)
log qlla(D|α)=log p(D|θmap)+log p(θmap|α)+ 2logdet
2π
(ggn+αI)−1 (38)Marco Miani†, Hrittik Roy†, Søren Hauberg
We can approximate (ggn+αI)−1 with α−1(I −P(ggn ))(cf. lemma 3.5). Hence log-determinant of (ggn+
P θmap
αI)−1 can also be approximated by sum of log-eigenvalues α−1(I −P(ggn )).
P θmap
1 (cid:18) 1 (cid:19)
log qlla(D|α)=log p(D|θmap)+log p(θmap|α)+ 2logdet
2π
(ggn+αI)−1 (39)
1 P −Tr(I −P(ggn ))
≈− 2α∥θmap∥2+ P
2
θmap log(α)+C (40)
where C denotes all the terms that don’t depend on α. Taking the derivative wrt α of the above equation and
setting it to zero gives us the stationary points.
dlog q dll αa(D|α) ≈−1 2∥θmap∥2+ P −Tr(I P − 2P(ggn θmap)) α1 =0 (41)
=⇒ α∗ =
∥θmap∥2
(42)
P −Tr(I −P(ggn ))
P θmap
To conclude the proof one only needs to notice that the second derivative of log qlla(D|α) is given by
−P−Tr(I P−P(ggn θmap)) 1 which is always negative. Hence the stationary point is the maximum value of the
2 α2
approximate log marginal likelihood.
A.4 Proof of Error Bounds in Lemma 3.5
Proof. ToprovetheboundonthematrixnormofthedifferencebetweenthecovariancesofLaplace’sapproximation
and projection posterior, notice that
(αI +ggn)−1 =α−1(I −P(ggn )+V(Λ+α)−1VT
P P θmap
Where Λ and V correspond to non-zero eigenvalues and eigenvectors of ggn respectively. Therefore from the
properties of the spectral norm, we have that:
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)(αI +ggn)−1−α−1(I −P(ggn )(cid:13)=(cid:13)V(Λ+α)−1VT(cid:13)
(cid:13) P P θmap (cid:13) (cid:13) (cid:13)
1
≤
τ +α
This proves the first bound. To prove the bound on Wasserstein distance note that the Wasserstein distance
between two Gaussian, N(µ ,Σ ) and N(µ ,Σ ), is given by:
1 1 2 2
d2 =(cid:13) (cid:13) (cid:13)µ 1−µ 2(cid:13) (cid:13) (cid:13)2 +Tr(Σ 1+Σ 2−2(Σ 21 2Σ 1Σ 21 2)1 2)
We plug in µ
1
= µ
1
= θmap, Σ
1
= (αI
P
+ggn)−1 and Σ
2
= α−1(I
P
−P(ggn θmap)). Also notice that by the
properties of projection matrices, it follows that Σ1 2Σ Σ1 2 = 1 (I −P(ggn )). Hence we have that
2 1 2 α2 P θmap
d2 =Tr(cid:0)(αI +ggn)−1−α−1(I −P(ggn )(cid:1)
P P θmap
=Tr(V(Λ+α)−1VT)
k
≤
τ +αBayes without Underfitting: Fully Correlated Deep Learning Posteriors via Alternating Projections
A.5 Proof of equation 11
Proof. To prove that ker(cid:0)P M⊤M (cid:1)=T ker(M⊤M ), assume that v ∈ker(cid:0)P M⊤M (cid:1) then it follows that:
b b b b b b b b b
!
X
M⊤M v =0
b b
b
!
X
=⇒ vT M⊤M v =0
b b
b
X
=⇒ vTM⊤M v =0
b b
| {z }
b
≥0
=⇒ vTM⊤M v =0 ∀b
b b
Hence, we can conclude that v ∈T ker(M⊤M ). On the other hand, it is obvious that if v ∈T ker(M⊤M ) we
b b b b b b
have that
M⊤M v =0 ∀b
b b
X
=⇒ M⊤M v =0
b b
b
!
X
=⇒ v ∈ker M⊤M
b b
b
This proves that ker(cid:0)P M⊤M (cid:1)=T ker(M⊤M ).
b b b b b b
A.6 Discussion of Lemma 4.1
The Convergence of Alternating Projections for two subspaces first appeared in John Von Neuman’s Lecture
Notes on Operator Theory[von Neumann, 1949]. It was extended to the case of infinite convex sets in Bregman
[1965]. Rate of convergence was analyzed in several works such as Kayalar and Weinert [1988], Smith et al. [1977].
This gives us some understanding of the approximation error incurred by truncating the interactive algorithm
after t steps.
A.7 Proof of Lemma 4.2
Proof. Suppose v ∈ ker(J ). Then we have that J v = 0. Note that JL = ∇ l(f(θ,x),y)J (x). Hence,
θ θ θ f(θ,x) θ
we have that JLv = ∇ l(f(θ,x),y)(J (x)v) = ∇ l(f(θ,x),y)0 = 0. Thus we have that v ∈ ker(JL).
θ f(θ,x) θ f(θ,x) θ
Therefore, ker(J )⊆ker(JL).
θ θ
A.8 Proof of Lemma 4.3
In this section, we prove Lemma 4.3. We restate it for convenience.
Lemma 4.3. For any θ ∼qloss and x
n
∈D it holds
|l(f(θ,x n),y n)−l(f(θmap,x n),y n)|=O(∥θ−θmap∥2)
which implies that
Var l(f(θ,x ),y )≤O(α2). (16)
θ∼qloss n n
Proof. Consider the first order Taylor expansion of f around θmap
f(θ,x)=f(θmap,x)+J θmap(θ−θmap)+O(∥θ−θmap∥2) (43)
and the first order Taylor expansion of l around f(θmap,x), which we shorten as fmap
l(f(θ,x),y)=l(fmap,y)+∇ fmapl(fmap,y)(l(f(θ,x),y)−l(fmap,y))+O(∥l(f(θ,x),y)−l(fmap,y)∥2) (44)Marco Miani†, Hrittik Roy†, Søren Hauberg
And we can use these to write
l(f(θ,x),y)−l(fmap,y)=∇ fmapl(fmap,y)J θmap(θ−θmap)+O(∥θ−θmap∥2) (45)
=∇ θmapl(f(θmap,x),y)(θ−θmap)+O(∥θ−θmap∥2) (46)
where we collected all the second order terms in θ in the O term.
Then, for any (x n,y n)∈D and for any θ ∼qloss, by definition of the loss kernel in Eq. 15, it holds that
∇ θmapl(f(θmap,x n),y n)(θ−θmap)=0 (47)
which we plug back into Eq. 45 and we get
l(f(θ,x n),y n)−l(f(θmap,x n),y n)=O(∥θ−θmap∥2) for any θ ∼qloss (48)
and the first part of the Lemma is proved. The variance bound directly follows since the norm ∥θ−θmap∥ is
controlled by the precision scale α of qloss.
B Calibration of loss kernel projection
Todemonstratehowtheposs-projectedposteriorimprovescalibration,considerasimpleexampleofaclassification
taskwithC outputclasses.Supposethemaximumaposteriori(map)estimateisoverlyconfidentinitspredictions,
consistently assigning a probability of 1.0 to the predicted class and 0.0 to the remaining C −1 classes. For
simplicity, assume this map estimate correctly classifies 80% of the examples while misclassifying the remaining
20%.
In this scenario, the map estimate’s confidence—represented by the maximum output probability is uniformly 1.0
for both correctly classified and misclassified examples. This results in overconfident predictions for the 20% of
cases where the model is wrong.
Effect of Loss Kernel on Misclassified Examples
1.0
0.9
0.8
0.7
0.6
0.5
0.4
Initial Predictions (Map Predictions)
0.3
Perturbed Predictions (After Projection into Loss Kernel)
Correctly Classified (80%)
0.2 Misclassified (20%)
0 200 400 600 800 1000
Example Index (Sorted by Accuracy)
Figure 7: The loss-projected posterior perturb the predicted probabilities of all the classes except the true label.
This leads to a lower confidence in misclassified examples, leading to better calibration.
ecnedifnoC
detciderPBayes without Underfitting: Fully Correlated Deep Learning Posteriors via Alternating Projections
Now, consider using the loss-projected posterior for predictions. This posterior is designed to preserve the model’s
accuracy by maintaining the probability assigned to the true label, especially for correctly classified examples.
Thus, for the 80% of cases where the map is correct, both accuracy and confidence remain largely unchanged.
However, for the 20% of misclassified examples, the loss-projected posterior adjusts the predicted probabilities.
Specifically, it reduces overconfidence by perturbing the probabilities of all classes except the true label. By
redistributing some of the confidence away from the incorrect class, the model achieves better calibration.
Misclassified examples no longer exhibit extreme certainty, thus reducing the calibration error and yielding
predictions that are more aligned with the model’s actual performance.
This behavior is illustrated in Figure 7, which shows how confidence is moderated on misclassified examples,
leading to an overall improvement in the calibration of the model.
C Implementation details and experimental setup
In this section, we outline the specifics of the experimental setup and provide key hyperparameter details. Any
additional information about the experimental setup can be found in the submitted source code.
C.1 Motivation for the choice of baselines
We benchmark our method against several baselines, including map, Diagonal Laplace, Last-Layer Laplace, and
SWAG. The motivation behind these choices is to compare our approach with other post-hoc methods that
approximate a Gaussian posterior centered at the same mode.
We chose map as a baseline because the maximum softmax probability is a strong baseline for out-of-distribution
(OOD) detection.
The last-layer Laplace approximation was selected as a baseline following recommendations from [Daxberger
et al., 2021a]. This method is considered a strong representative of Laplace approximations and is expected to
provide near-optimal performance across various configurations of Laplace approximations.
WeincludeSWAGasanotherbaseline,whichprovidesasimpleyeteffectivemethodforuncertaintyquantification.
SWAGbuildsaGaussianapproximationclosetothemapbutwithalow-rankcovariancematrixwhichisdifferent
from Laplace approximations.
Finally, diagonal Laplace is included to assess the impact of posterior correlations. This method simplifies the full
Laplace by using only diagonal covariance, providing insight into the difference between sparsely correlated and
fully correlated posteriors.
We exclude KFAC-Laplace from the benchmarks because it requires specialized layer-specific implementations,
which are not readily available for modern architectures like SWIN transformers.
C.2 Toy regression
We train a two-layer MLP with 10 hidden units per layer on a simple regression task. For uncertainty estimation,
we sample from approximate posteriors of various methods, including projected posterior, loss-projected posterior,
linearized Laplace, sampled Laplace, and diagonal Laplace.
While linearized Laplace and sampled Laplace refer to the same Gaussian distribution in parameter space, they
differ in how predictions are generated. Linearized Laplace uses the linearized neural network for predictions
and sampled Laplace uses neural network for predictions. The diagonal Laplace method, on the other hand,
approximates the covariance of this Gaussian by only considering its diagonal, which leads to sparsely correlated
posteriors. For a consistent and fair comparison, all methods utilize a prior precision of 1.0.
C.3 Image classification on MNIST and FMNIST
We train a standard LeNet for the MNIST and FashionMNIST experiments. We train LeNet with Adam
optimizer and a 10−3 learning rate. We choose the prior precision for each baseline by doing a grid search
over {0.1,1.0,5.0,10.0,50.0,100.0}. For Projected Posterior and Loss Projected Posteriors, we use the analyticalMarco Miani†, Hrittik Roy†, Søren Hauberg
expression for the optimal prior precision and we do 1000 iterations of alternating projections for both and a
projection batch size, S =16. For all Bayesian methods, we use 30 Monte Carlo samples for predictions. Whereas
for SWAG we use a learning rate of 10−2 with momentum of 0.9 and weight decay of 3×10−4 and the low-rank
covariancestructureinallexperiments.Wecollect20modelstosamplefromtheposterior.ForProjectedPosterior
and Loss Projected Posterior, we use the linearized predictive and for Last-Layer and Diagonal baselines we use
the neural network predictive.
C.4 Image classification on CIFAR-10 and SVHN
We train a ResNet architecture consisting of three groups of three ResNet blocks. The model is trained using
Stochastic Gradient Descent (SGD) with a learning rate of 0.1, momentum, and weight decay. The prior precision
is chosen in the same way as the experiment above, and the same predictive functions are applied.
For the projected posteriors, we perform 1000 iterations of alternating projections with a projection batch size of
S =16. All Bayesian methods utilize 30 Monte Carlo samples to compute predictions.
For SWAG on CIFAR-10, we use a learning rate of 10−2 with momentum of 0.9 and weight decay of 3×10−4
and the low-rank covariance structure in all experiments. We collect 20 models to sample from the posterior.
C.5 Image binary multi-classification on CelebA
We first remove from the training dataset all the images belonging to the classes ’Bald’, ’Eyeglasses’, and
’Mustache’. Then we train a VisualAttentionNetwork [Guo et al., 2023] with blocks of depths (3,3,5,2) and
embeddeddimensionsof (32,64,160,256) withrelu activationfunctions,wetrainedwith Adam for50epochswith
batch size 128 and a learning rate decreasing from 10−3 to 10−5; parameter size is P =3858309. All experiments
are run on a single H100 GPU.
We compare the scores of a series of baselines including Max Logit [Hendrycks and Gimpel, 2016] and a Deep
Ensemble (DE) [Lakshminarayanan et al., 2017] of 10 independently trained models, the high training cost is
the reason why DE is missing standard deviations, since that would require training 30 models. We included
several low-rank-approximation methods: Linearized Laplace Approximation (LLA) [Immer et al., 2021b], Local
Ensemble (LE) [Madras et al., 2020], Stochastic Weight Averaging Gaussian (SWAG) [Maddox et al., 2019],
Sketching Curvature for OoD Detection (SCOD) [Sharma et al., 2021] and Sketched Lanczos Uncertainty (SLU)
[Miani et al., 2024]. All of these but SLU use a rank 300 approximation for memory limit, while SLU uses a rank
1000 and a sketch size of 1M. All of these but SCOD and SWAG are based on Lanczos algorithm, thus we also
computed our Projected score using Lanczos algorithm to compute the top eigenvectors, this is referred to as
’Proj-Lanczos’ in Table 5. Lastly, we also include the diagonal version of Laplace (LLA-d) which is a common
method used for large models thanks to its low memory requirement.
C.6 Image classification on ImageNet
Weobtainapre-trainedSWINtransformer[Liuetal.,2021],with28millionparameters,from[DarshanDeshpande,
2021]. We sample from the Loss Projected Posterior. We use 5 Monte Carlo samples for predictions and do 15
iterations of alternating projections. We use a projection batch size of S =16.
C.7 Generative model
We train a VAE with approximately 100,000 parameters on MNIST and FMNIST. While keeping the encoder
fixed we sample various decoders from the loss-projected posterior. We do 5 iterations of alternating projections
and use 20 Monte Carlo Samples for predictions. Prior precision is chosen in the usual way.