Fine-Tuning Large Language Models to Appropriately
Abstain with Semantic Entropy
BenedictAaronTjandra1∗ MuhammedRazzak1∗ JannikKossen1∗
KunalHanda1 YarinGal1
1OATML,DepartmentofComputerScience,UniversityofOxford
Abstract
LargeLanguageModels(LLMs)areknowntohallucinate,wherebytheygenerate
plausiblebutinaccuratetext. Thisphenomenonposessignificantrisksincritical
applications,suchasmedicineorlaw,necessitatingrobusthallucinationmitigation
strategies. Whilerecentworkshaveproposedfine-tuningmethodstoteachLLMs
toabstainfromansweringquestionsbeyondtheirknowledgeorcapabilities,these
methodsrelyontheexistenceofground-truthlabelsorarelimitedtoshort-form
responses. Toaddresstheselimitations,weproposefine-tuningusingsemantic
entropy,anuncertaintymeasurederivedfromintrospectionintothemodelwhich
doesnotrequireexternallabels. Wedemonstratethatourapproachmatchesor
outperformsmodelsfine-tunedusingpriorworkandachievesstrongperformance
forbothshortandlong-formgenerationsonarangeofdatasets.
1 Introduction
Largelanguagemodels(LLMs)havemadesignificantprogressinnaturallanguageprocessing,achiev-
ingremarkableperformanceacrossawiderangeoftasks(OpenAI,2024;Meta,2024;Google,2024).
Models,likeGPT-4,Llama3,andGemini,havedemonstratedcapabilitiesthatrivalorsurpasshuman
performanceinarangeofdomainsandareincreasinglydeployedintherealworldforvariousappli-
cations(Yangetal.,2023a). However,despitetheseadvancementsinperformance,LLMsremainfar
fromflawless,particularlywhenitcomestohandlingtasksthatfalloutsidetheirscopeofknowledge
orreasoningabilities(Jietal.,2023a;Huangetal.,2023),wheretheytendtoexhibithallucinations.
Hallucinations,inthecontextofLLMs,refertoinstanceswheremodelsgeneratecontentthat,while
appearingplausible,isfactuallyinaccurate,contradictspreviouslyestablishedworldknowledge,or
doesnotmakelogicalsense(Zhangetal.,2023). Thesehallucinations,whileharmlessinlow-stakes
applications,canhavesevereconsequenceswhenLLMsaredeployedinsafety-criticaldomainssuch
ashealthcareorlegalservices,wherethegenerationoferroneousinformationcouldbecostlyorlead
toreal-worldharm(Hanetal.,2024;Weiser,2023). Asaresult,mitigatinghallucinationsiscrucial
toensuringthesafety,trustworthiness,andoverallreliabilityofLLMs(Rawteetal.,2023).
Variousstrategieshavebeenproposedtomitigatehallucinations(Jietal.,2023a;Tonmoyetal.,2024).
RetrievalAugmentedGeneration(RAG)approachesattempttogroundLLMoutputsbyincorporating
external knowledge sources (Lewis et al., 2021; Shuster et al., 2021; Ji et al., 2023b), anchoring
responsesinverifiedfacts. Othertechniquesinvolvemodifyingtheinferenceprocesstoencourage
more cautious generation (Shi et al., 2023; Chuang et al., 2024) or focus on post-hoc detection,
wherehallucinationsareflaggedaftergenerationthroughmethodssuchasconfidenceoruncertainty
estimation(Kadavathetal.,2022;AzariaandMitchell,2023;Kuhnetal.,2023;Farquharetal.,2024).
Anapproachthathasgainedattentionduetoitssimplicityandeffectivenessinvolvesfine-tuning
LLMs to abstain from answering questions that are outside the scope of their knowledge or
∗EqualContribution.Correspondencetoaaron_tjandra@yahoo.com.
Preprint.Underreview.
4202
tcO
22
]LC.sc[
1v43271.0142:viXracapabilities(Wenetal.,2024). Todothis,anLLMisfine-tunedonadatasetthatconsistsofexamples
ofquestionsthattheLLMshouldabstainfromansweringand,conversely,examplesofquestions
thattheLLMshouldwillinglyanswer. Severalrecentworks(Yangetal.,2023b;Chengetal.,2024;
Wolfeetal.,2024)teachtheLLMtoabstainfromansweringquestionsitansweredincorrectly. These
approaches,however,requireaccesstoground-truthlabels,whichinmanycasescanbedifficultor
costlytoobtain. Additionally,sinceground-truthlabelscomefromexternalsources,thereisevidence
thattheyarepotentiallynoisyandhencenotthemostappropriateteachingsignalKossenetal.(2024).
Toavoidbeingdependentonground-truthlabels,Zhangetal.(2024)exploredanuncertainty-based
fine-tuningapproach,R-Tuning-U,wheretheyfirstapproximateanLLM’suncertainty(asmeasured
byentropy)inansweringaquestionandsubsequentlytrainingtheLLMtoabstainfromanswering
questionsitisuncertain. R-Tuning-U,however,issensitivetothelexicalandsyntacticalvariations
ofgenerations,limitingitsapplicationtosettingswheretheLLMisinstructedtogenerateshort-form
responsesconsistingofnotmorethanafewwords,hinderingitsusefulness.
Inthiswork,weproposetoleveragesemanticentropy(Kuhnetal.,2023;Farquharetal.,2024)to
overcometheselimitations. SemanticentropyimprovesoverR-Tuning-Ubycomputingentropyover
thesemanticspacethatmodelgenerationsoccupy,ratherthanoverrawtokensequences. Assemantic
entropyevaluatestheentropyoverthesemanticmeaningofthegenerations,itisrobusttolexical
andsyntacticalvariationsofgenerations. Thisresultsinabetterindicatorofhallucinationsinboth
short-formandlong-formgenerationsettings(Kuhnetal.,2023;Farquharetal.,2024).Byfine-tuning
modelstoabstainfromansweringquestionsitisuncertainaboutusingsemanticentropy,weprovide
anapproachtoreducethemodel’shallucinations,withoutrelyingonground-truthlabelsorbeing
restrictedtotheshort-formresponsesetting. Thisallowsforamoreflexibleandreliableabstention
acrossawiderangeoftasksanddomains,providingapowerfultoolforreducinghallucinations.
We evaluate our method across several benchmarks and introduce a new metric called accuracy-
engagementdistance(AED),whichquantifiesmodelhallucinationsmorecomprehensivelybytaking
intoaccountitsaccuracyandengagement,thenumberofquestionsitchoosestoanswerwillingly.
Usingthismetric,weshowthatmodelsfine-tunedwithsemanticentropysignificantlyoutperform
R-TuningandR-Tuning-U,existingapproacheswheretheformerislabel-dependentandthelatter
is label-independent. Compared to R-Tuning and R-Tuning-U (Zhang et al., 2024), our method
achievesupto30.1%reductioninhallucinationratesforlong-formgenerationsandupto8.7%for
short-formgenerations. Ourmethodopensupnewavenuestofine-tunemodelsonbothshort-form
andlong-formgenerationswithoutrelyingonground-truthlabels,makingiteasilyscalable.
Contributions. Insummary,thekeycontributionsofthispaperare:
• Wedemonstratethatmodelsfine-tunedusingsemanticentropy(Section5)matchoroutper-
formmodelsfine-tunedusingpriorwork,underbothlong-form(Long-QA)andshort-form
(Short-QA)answeringsettings(Section6).
• Weintroducetheaccuracy-engagementdistance(AED),anovelevaluationmetric,thatmore
holisticallyquantifiestheextentofamodel’shallucinationbytakingintoaccountboththe
accuracyandengagementofamodel(Section4).
2 RelatedWork
Thereisasignificantbodyofworkintheliteraturethatstudieswhyhallucinationoccursandhow
to prevent them. We redirect the reader to recent surveys (Ji et al., 2023a; Huang et al., 2023;
Rawteetal.,2023;Chakrabortyetal.,2024)foracomprehensiveoverviewofhallucinations. For
conciseness,wefocusonpreviousworkthataremostpertinenttoours.
UncertaintyEstimationinLLMs. Avarietyofworkshaveproposeduncertaintymeasurestodetect
hallucinationsinLLMgenerations.Grey-boxmethodsrelyontokenlikelihoodsandmultiplesamples
toaprompttomeasureLLMuncertainty(Kadavathetal.,2022;Kuhnetal.,2023;Farquharetal.,
2024;Zhangetal.,2024). White-boxmethodsassumeaccesstotheinternalsofanLLM(weights
andactivations)andtrainmodelsontheactivationsduringgenerationtoprobeintotheuncertaintyof
LLMs(Ahdritzetal.,2024;Kossenetal.,2024;Liuetal.,2024).
AbstentionFine-Tuning. Thereisarangeofproposedmethodstofine-tuneLLMstoabstainfrom
answeringquestionsbeyondtheircapabilities(Wenetal.,2024). Wolfeetal.(2024)lookedinto
usingQAdatasetswithunanswerablequestionsandfine-tunedLLMstoabstainfromanswering
2thosequestions. Similarly,Brahmanetal.(2024)developedataxonomyfordifferentabstention
scenariosand,usingpromptingtechniques,constructedasyntheticdatasettocaptureeachabstention
scenario. Theythenusedthisdatasettofine-tunemodelsandmeasuredtheirabstentionrateineach
abstentionscenario. Yangetal.(2023b)andChengetal.(2024)sampledmultipleresponsestoeach
questionandleveragedtheaccuracyratetofine-tuneLLMstoabstainfromquestionswhoseaccuracy
rateisbelowacertainthreshold.Zhangetal.(2024)proposedR-TuningandR-Tuning-Utofine-tune
LLMsundertheShort-QAansweringsetting. InR-Tuning,modelsareinstructedtogeneratesingle
responsestoasetofquestions,andarefine-tunedtoabstainfromquestionsitansweredincorrectly.
InR-Tuning-U,multipleresponsesaresampledperquestionandtheentropyofthoseresponsesis
usedtomeasuretheuncertaintyofanLLMtoeachquestion. Themodelisthenfine-tunedtoabstain
onthetop-50%mostuncertainquestionswhilekeepingtheiroriginalresponsesfortheotherhalf.
3 Background: Label-FreeUncertaintyEstimationinLLMs
Inthissection,wefamiliarisethereaderwithR-Tuning-Uandsemanticentropy,twouncertaintyquan-
tifyingmethodsforLLMsthatweusetodeterminethesetofquestionsanLLMshouldabstainfrom.
R-Tuning-U.R-Tuning-U(Zhangetal.,2024)usesanapproximationoftheclassicalconditional
entropygivenaparticularquestionqtomeasureanLLM’suncertainty. LetGbethesetofpossible
generations,wheres ∈ Gdenotesasequenceoftokenswiths representingthei-thtokeninthe
i
sequence, and let q be the sequence of tokens obtained by tokenising our prompt q. Using the
conditional probability distribution p that we can obtain from an LLM, the probability of some
θ
tokensequencesoccurringgivenqis
|s|
(cid:89)
p (s|q)= p (s |s ,q), (1)
θ θ i <i
i=1
wheres =(s ,...,s ). Theclassicalconditionalentropyovergenerationsisformallydefinedas
<i 1 i−1
(cid:88)
E(q)=− p (s|q)logp (s|q). (2)
θ θ
s∈G
However,sincethesizeofGmaybeextremelylarge,theaboveequationisintractabletocompute
exactly. R-Tuning-UarrivesatadiscreteapproximationofEquation(2)bysamplingM generations
andsubsequentlymeasuringtheempiricalprobabilityoftheoccurrenceofeachgeneration. Ifwe
letU tobethesetofuniquegenerationsobtainedfromS,ourlistofsampledgenerations,andc(u)
tobethenumberoftimesu∈U occursinS,thenR-Tuning-Uapproximatestheclassicalentropyby
(cid:18) (cid:19) (cid:18) (cid:19)
(cid:88) c(u) c(u)
E(q)≈− log . (3)
M M
u∈U
SemanticEntropy. ThekeydrawbackofR-Tuning-Uisthatitdisregardsthesemanticmeaningof
thegenerationsandissensitivetolexicalandsyntacticalvariations. Amodelthatgenerates“The
capitalofFranceisParis”and“ParisisFrance’scapital”whenpromptedtwiceinresponsetothe
question“WhatisFrance’scapital?” isnotuncertaininanymeaningfulsense(Kuhnetal.,2023).
R-Tuning-U,however,willassignanon-zerouncertaintyasthegenerationsaredifferentstrings.
Semantic entropy improves over R-Tuning-U by taking into account the semantic meaning of
generations. Insteadofcomputingtheentropyofthegenerations,itcomputestheentropyofsemantic
equivalenceclassesthatthegenerationsoccupy,whereasemanticequivalenceclassC isdefined
tobethesetofgenerationsthatsharethesameparticularmeaning. Moreconcretely,lettingC to
bethesetofallsemanticequivalenceclasses,foranysemanticequivalenceclassC ∈C,wehave
∀s,s′ ∈ C: R(s,s′),whereR(·,·)isasemanticequivalencerelationthatholdsifandonlyiftwo
generationshavethesamesemanticmeaning. Formally,thesemanticentropyisdefinedas
(cid:88)
SE(q)=− p (C |q)logp (C |q), (4)
θ θ
C∈C
3wherep (C |q)istheprobabilityofasemanticequivalenceclassC occurringgivenq:
θ
|s|
(cid:88) (cid:88)(cid:89)
p (C |q)= p (s|q)= p (s |s ,q). (5)
θ θ θ i <i
s∈C s∈Ci=1
However,justascalculatingclassicalentropyisintractable,calculatingsemanticentropyisequally
intractable. SimilartoR-Tuning-U,wecanapproximatethe(discrete)semanticentropybysampling
M generationsinresponsetoqandusingthenumberofgenerationsineachsemanticequivalence
class(Farquharetal.,2024)toapproximatep (C |q). LettingC ,...,C tobethesemanticequiv-
θ 1 m
alenceclassesthatwecanextractfromoursampledlistofgenerationsS,thediscreteapproximation
ofsemanticentropyisgivenby
m m (cid:18) (cid:19) (cid:18) (cid:19)
SE(q)≈−(cid:88)
p (C |q)logp (C
|q)≈−(cid:88) |C i|
log
|C i|
. (6)
θ i θ i M M
i=1 i=1
Tocomputetheaboveequationinpractice,weneedtofirstoperationalisethesemanticequivalence
relationR(·,·). Inthiswork,wefollowKuhnetal.(2023)’sapproachtoimplementR(·,·)using
theconceptofquestion-dependentbi-directionalentailment. Morespecifically,wedeemthattwo
generationssands′aresemanticallyequivalentifanentailmentmodelsaystheylogicallyentailone
anotherwithinthecontextofthequestion. WethenuseR(·,·)toclusterS intosemanticequivalence
classes,whereeachclassconsistsofresponsesthatsharethesamesemanticmeaning. Giventhese
equivalenceclasses,weestimatetheprobabilityp (C )ofeachclassC bydividingthenumberof
θ i i
responsesinclassC bythetotalnumberofresponsesM. Thesemanticentropyforaquestionq
i
thenfollowsviaEquation(6). Moredetailsconcerningtheentailmentmodel,semanticclustering,
andsemanticentropycalculationcanbefoundinAppendixA.3.
4 Accuracy-EngagementDistance
Several works (Zhang et al., 2024; Feng et al., 2024) suggest using the accuracy over the set of
willinglyansweredquestionstomeasuretheextentofhallucinationofamodel. Thoughanatural
idea,wearguethatthismetricisnotaholisticmeasureofmodelperformanceasitdoesnotpenalise
themodelfromwronglyabstainingfromaquestion. Toillustrate,consideramodelA thatwillingly
1
answersallquestionsonadatasetof2500questionsandattainsanaccuracyof70%. Supposethat
fine-tuningA yieldsamodelA thatwillinglyanswers10questionsandattainsanaccuracyof70%.
1 2
IfweuseaccuracytocompareA andA ,thenwewoulddeemthemasequivalentastheyhavethe
1 2
sameaccuracies. However,fromahelpfulnesspointofview,thisismisleadingasA isclearlyworse
2
thanA asitavoidsansweringasubstantialnumberofquestionsthatA previouslygotcorrect.
1 1
WecanseethatA hasalowengagementasitabstainsfromansweringalargenumberofquestions.
2
Ideally,ourmetricshouldpenaliselowengagementandlowaccuracyandrewardhighengagement
andhighaccuracy. AdaptingTianetal.(2023)’smethodtocomparethetruthfulnessofbiography
generations,weproposetoevaluatefine-tunedmodelsusinganovelevaluationmetric,theAccuracy-
EngagementDistance,thattakesintoaccountboththeaccuracyandengagementofamodel.
Considerafine-tunedmodelthatanswersQquestionswillingly.AmongtheseQquestions,themodel
answers I questions incorrectly and C questions correctly. We can conceptualise our fine-tuned
modelasoccupyingasinglepointinR2 whosecoordinatesare(I,C). Anidealmodel, thathas
the highest accuracy and engagement, answers all questions correctly and is represented by the
point(0,|D|)inthisspace,where|D|isthetotalnumberofquestionsinaparticulardataset. The
Accuracy-Engagement Distance (AED) is the normalised Euclidean distance between the point
representingthefine-tunedmodelandtheidealmodel:
(cid:115)
I2+(|D|−C)2
AED= .
2·|D|2
TheAEDrangesfrom0to1andismaximisedwhenthemodelanswerseveryquestioncorrectly(max
accuracy,maxengagement)andisminimisedwhenthemodelanswerseveryquestionincorrectly
(minaccuracy, maxengagement). IfwenowcompareA andA usingAED,wecanseethat
1 2
A achievesanAEDof0.30whileA achievesanAEDof0.71,penalisingA ’slowengagement.
1 2 2
45 AbstentionFine-TuningusingSemanticEntropy
Inthissection,weintroduceafine-tuningstrategythatleveragessemanticentropytoenablemodel
abstentioninuncertainscenarios.
Overview.Thekeyideaistodeterminewhichquestionstoabstainfromandtowillinglyanswerbased
onthesemanticentropyofamodel’sresponses. Questionswithhighsemanticentropy,indicatinga
highlikelihoodofahallucinationbeinggenerated,shouldbeabstainedfromanswering,whilethose
withlowsemanticentropyshouldbeansweredwiththemodel’sstandardresponse.
DatasetConstruction. Foreachquestioninthetrainingdataset,wegenerateastandardresponse
usingalow-temperaturesetting(T = 0.1)toencourageadeterministicoutput. Additionally,we
generateM = 10responsesbysamplingatahightemperature(T = 1.0)tocapturethemodel’s
variabilityundermorestochasticconditions.
Thehigh-temperatureresponsesareusedtocomputethesemanticentropyasdescribedinSection3.
Computingthesemanticentropyofeachquestionqresultsinasetof(q,SE(q))pairswhereSE(q)
represents the semantic entropy of q’s responses. With the computed semantic entropy for each
question,wepartitionthedatasetintotwosubsetsbasedonauser-defineduncertaintythresholdτ:
• High-entropysetH: ThissetcontainsquestionswhereSE(q)>τ,indicatingahighlevel
ofuncertaintyinthemodel’sresponses. Forthesequestions,wemodifytheground-truth
labeltoanabstentionphrase: "Idon’tknowtheanswer."
• Low-entropy set L: This set includes questions with SE(q) ≤ τ, where the model is
relativelyconfident. Here,theground-truthlabelissettobethemodel’sstandardresponse.
Fine-TuningProcedure. Oncethedatasetispartitioned,wefine-tunethemodelusingH andL.
Weemploysupervisedfine-tuningwithcross-entropyloss,wherethemodelistrainedtopredictthe
nexttokenintheconcatenatedinputsequence(prompt+question+adjustedlabel). Themodelis
encouragedtogeneratethestandardresponseforquestionsinL,whileabstainingforthoseinH.
Formally,givenanansweringsettingprompt,i.e. Long-QAorShort-QA,tokenisedquestionsq,and
theircorrespondingtokenisedground-truthlabelsy(q) (eitherthestandardresponseorabstention
phrase),themodellearnstominimisethefollowingfine-tuningobjectiveduringtraining:
|y(q)|
L (p )=−(cid:88) (cid:88) logp (y(q) |prompt,q,y(q) ), (7)
CE θ θ t t−1
q∈Q t=1
whereQdenotesthesetofquestionsinthetrainingsetandp (· | prompt,q,y(q) )isthemodel’s
θ t−1
predictednext-tokenprobabilitydistributiongiventheansweringsettingprompt,question,andthe
firstt−1tokensofthemodifiedground-truthlabel.
6 Experiments
Weevaluateourabstentionfine-tuningapproachLLAMA-3-8B-INSTRUCT(Meta,2024)acrossfour
datasetsandtwoansweringsettings: 1)Long-QA,whereweinstructtheLLMtogeneratefree-form
sentence-lengthgenerationsand2)Short-QA,whereweinstructtheLLMtogenerateshort-form
answers. OurpromptsforeachansweringsettingcanbefoundinAppendixA.1.
Datasets. Weevaluateacrossfourdatasets: TriviaQAJoshietal.(2017),BioASQTsatsaronisetal.
(2015),NQKwiatkowskietal.(2019),andSQuADRajpurkaretal.(2016). Werandomlyselect
2500QApairsfromthevalidationsplitofeachdatasetanddesignate2000datapointsfortraining
and500datapointsforin-distributionvalidation. Weadoptaclosed-booksettingforourexperiments
andremoveadditionalcontextthatispresentinTriviaQAandSQuAD.Foreachquestion,weuse
themodel’sstandardresponseandthequestion’sground-truthlabeltoassignanaccuracyscoreto
thatquestion(AppendixA.2). AsdescribedinSection5,thehigh-temperatureresponsesareusedto
calculatethesemanticentropyofeachquestion. Wecomputesemanticentropywithtwoentailment
modelsresultingintwovariantsofsemanticentropy: SemanticEntropywithDeBERTaentailment
(SE(DeBERTa))andSemanticEntropywithLlama-3-70B-Instructentailment(SE(Llama)). Further
implementationdetailscanbefoundinAppendixA.3.Inadditiontosemanticentropy,weusethehigh-
temperatureresponsestocomputetheentropyforR-Tuning-UviaadirectapplicationofEquation(2).
5Fine-Tuning. Ourexperimentsinvolvefine-tuningamodelonatrainingdatasetandevaluatingthe
fine-tunedmodelonthein-distributionvalidationsplitandout-of-distributiondatasetsthatexclude
thetrainingset. Forexperimentsconcerninganuncertaintymetric,i.e. R-Tuning-U,SE(DeBERTa),
andSE(Llama),eachfine-tuningrunisassociatedwithathresholdτ,wherewepartitionthetraining
setintothehigh-entropysetandthelow-entropyset. Wethenfine-tunethemodelviathemethod
describedinSection5. ForR-Tuning,weassignthesetofincorrectquestionstoH andthesetof
correct questions to L and equivalently replace the ground-truth labels of H with the abstention
phraseandLwiththestandardresponseofthemodel. Duetoresourceconstraints,weuseLoRA(Hu
etal.,2021)toperformsupervisedfine-tuning. Inadditiontosingledatasetexperiments,i.e. training
ononedatasetandevaluatingonanotherdataset,wealsoconductexperimentswherewetrainon
multipledatasets–specificallybycombiningthetrainingsplitsofTriviaQA,BioASQ,andNQ.We
denotethissettingas“Mult”insubsequentsections. In“Mult”,thevalidationsetisthecombined
validationsetsofTrivaQA,BioASQ,andNQ,andtheout-of-distributionsetconsistsofSQuAD.
HyperparameterdetailscanbefoundinAppendixA.4.
ModelSelectionandEvaluation. Weconducttwoformsofevaluation: 1)Best-ThresholdEval-
uationand2)All-ThresholdEvaluation. InBest-ThresholdEvaluation,weconductexperiments
with R-Tuning with 3 random seeds and report the mean and standard deviation of the AED on
the in-distribution validation set and out-of-distribution datasets. For each of R-Tuning-U, SE
(DeBERTa),andSE(Llama),wefirsttrainadifferentmodelon9equally-spacedthresholds,ranging
from0.25and2.25,andselectthethresholdτ thatachievesthelowestAEDonthein-distribution
validationset. Weconductexperimentswiththatthresholdusing3randomseedsandreportthemean
and standard deviation AED on the in-distribution validation set and out-of-distribution datasets.
Thiscapturesarealisticscenariowhereanin-distributionvalidationsetisusedtopickthethresholdτ
thatleadstothelowestAEDandsubsequentlyevaluatinghowwellthisfine-tunedmodelgeneralises
onout-of-distributionsettings.
InAll-ThresholdEvaluation,weevaluateeachfine-tunedmodeltrainedateachofthe9thresholds
in our single-dataset experiments, recording the number of incorrect and correct questions each
fine-tunedmodelgetsonout-of-distributiondatasets. Toinspecttheformationofanyoveralltrends,
we aggregate the number of incorrect and correct questions to build an “adaptation” plot, where
each point represents the average incorrect and correct responses a fine-tuned model gets on an
out-of-distribution dataset when trained at a specific threshold. For both types of evaluation, we
performgreedyinference(T =0).
7 ResultsandDiscussion
ThissectionpresentsanddiscussesourresultsforBest-ThresholdandAll-ThresholdEvaluation.
ModelsFine-TunedonSemanticEntropyHaveLowerAEDs. Figure1andFigure2presents
results for Best-Threshold Evaluation. Figure 1 shows in-distribution experiments where we
averageresultsacrossdifferentseeds,whileFigure2showsout-of-distributionexperimentswhere
we average results across different seeds and further averaging experiments that share the same
out-of-distribution dataset. A granular breakdown of our out-of-distribution experiments can be
found in Appendix A.6. We further aggregate the means of all datasets and present the overall
averageforeachsettinginTable1. FromFigure1andFigure2,fine-tuningonsemanticentropy,
underbothLong-QAandShort-QAansweringsettingsandonin-distributionandout-of-distribution
evaluations, yields models with AEDs that are typically equal or lower than models fine-tuned
withR-TuningandR-Tuning-U.Wealsoseethatfine-tuningonsemanticentropycomputedwith
astrongerentailmentmodel(SE(Llama))largelyledtomodelswithlowerAEDs. Moreover,from
Table1: SE(Llama)andSE(DeBERTa)achievesthelowestoverallaverageAccuracy-Engagement
DistancesforbothLong-QAandShort-QA.Boldindicateslowest.
Setting SE(Llama)(ours) SE(DeBERTa)(ours) R-Tuning R-Tuning-U OriginalModel
Long-QA: In-Distribution 0.364 0.411 0.399 0.521 0.380
Short-QA: In-Distribution 0.428 0.427 0.469 0.438 0.467
Long-QA: Out-of-Distribution 0.406 0.432 0.438 0.541 0.425
Short-QA: Out-of-Distribution 0.473 0.482 0.508 0.485 0.525
6Long-QA
0.7 0.654
0.629
0.6 0.556 0.5430.5550.584 0.588
0.5 0.481 0.506
0.438 0.4180.44 0.426 0.421
0.4 0.353 0.377 0.354 0.357
0.319 0.328
0.3 0.261
0.2
0.1870.2010.196 0.202
TriviaQA BioASQ NQ SQuAD Mult
Short-QA
0.708
0.7
0.646
0.6 0.551 0.562
0.5930.595 0.613
0.5180.509 0.522
0.42
0.5
0.4 0.315 0.3970.3950.414 0.3990.412 0.3810.385 0.3940.405
0.3 0.2510.253 0.2610.25
0.2
TriviaQA BioASQ NQ SQuAD Mult
SE (Llama) (ours) SE (DeBERTa) (ours) R-Tuning R-Tuning-U Original
Figure1: Ourmethod,SE(Llama),matchesoroutperformsR-TuningandR-Tuning-UforLong-QA
andShort-QAinin-distributionexperiments. MeanAccuracy-EngagementDistances(AEDs)are
shownontopofeachbar. Standarddeviationsareshownaserrorbars. ThelowertheAED,thebetter.
Long-QA
0.7 0.656
0.612 0.631 0.588
0.6 0.519 0.5380.5520.543 0.583 0.532 0.556 0.583
0.5
0.4280.440.443 0.448
0.389
0.4 0.372
0.286 0.318 0.318
0.3 0.258
0.2
0.2160.207
0.192
TriviaQA BioASQ NQ SQuAD Mult
Short-QA
0.702 0.702
0.7
0.6180.6310.6170.629 0.6120.6260.6170.609
0.6 0.574
0.5140.520.5480.529
0.5 0.448
0.4 0.3710.382
0.4020.392
0.309
0.3 0.2480.249 0.2580.253
0.2
TriviaQA BioASQ NQ SQuAD Mult
SE (Llama) (ours) SE (DeBERTa) (ours) R-Tuning R-Tuning-U Original
Figure2: Ourmethod,SE(Llama),matchesoroutperformsR-TuningandR-Tuning-UforLong-QA
andShort-QAinout-of-distributionexperiments. MeanAccuracy-EngagementDistances(AEDs)are
shownontopofeachbar. Standarddeviationsareshownaserrorbars. ThelowertheAED,thebetter.
Table1,weobservethat,inaggregate,fine-tuningonR-TuningandR-Tuning-Uleadstomodels
thatoftenareworsethantheoriginalmodel. Incontrast,fine-tuningonSE(Llama)yieldsmodels
thatsignificantlyoutperformexistingmethodsandtheoriginalmodel. Notably, ifwecompare
SE(Llama)withR-TuningandR-Tuning-U,weobtainupto30.1%and8.7%reductioninAEDs
for in-distribution experiments for Long-QA and Short-QA respectively. For out-of-distribution
experiments,weobtainreductionsupto25.0%and6.9%forLong-QAandShort-QA.
7
DEA
DEA
DEA
DEATriviaQA BioASQ NQ SQuAD
800 350
700 600 500 300
600 500 400 250
500 400
300 200
400
300 150
300 200
200 100
200
100
100 100 50
0 0 0 0
0 50 100 150 200 0 50 100 150 200 250 300 350 0 100 200 300 400 0 100 200 300 400
No. Incorrect No. Incorrect No. Incorrect No. Incorrect
SE (Llama) SE (DeBERTa) R-Tuning R-Tuning-U
Figure3: SE(Llama)formsafrontieroverothermethodsintheLong-QAAdaptationPlot. Each
pointrepresentsafine-tunedmodeltrainedataspecificthreshold.
Fine-TuningwithSemanticEntropyIsaParetoImprovementForLong-QA.Wepresentthe
Long-QA adaptation plot in Figure 3 as a result of All-Threshold Evaluation. Here, we observe
that models fine-tuned with SE (Llama) form a frontier over models fine-tuned using other
methods. SincetheAEDistheEuclideandistancefromanidealmodelthathasmaximumaccuracy
andengagement(thetopleftcorneroftheadaptationplot),thenitfollowsthatmodelsfine-tuned
withSE(Llama)attainslowerAEDsnomattertheuncertaintythresholdonout-of-distribution
settings for Long-QA. This further underscores the effectiveness of semantic entropy as an
abstentionfine-tuningmethod. Itisdifficult,however,todiscernanoveralltrendintheShort-QA
adaptationplot,whichweshowatAppendixA.5,whereitseemsthatnomethodformsafrontier
aboveanother. ThismaybebecausetherearelesserlexicalandsyntacticalvariancesinShort-QA
thaninLong-QA,whichleadstoweakermethodssuchasSE(DeBERTa)andR-TuningEntropy
performingequivalentlyasSE(Llama)atnumerousthresholds.
Takeaways. Wecanseemodelsfine-tunedwithsemanticentropy,inmostcases,outperformmodels
fine-tunedwithR-TuningandR-Tuning-U,whichindicatesthatsemanticentropyisaclearersignal
to reduce hallucinations than a model’s correctness on a question and classical entropy. This is
becausesemanticentropyisamoredelineativestatisticofamodel’suncertaintythanbothR-Tuning
and R-Tuning-U, which facilitates model learning and generalisation. Our findings represent an
advancement in fine-tuning methodologies for both Long-QA and Short-QA answering settings,
opening new avenues for reducing hallucinations for both short-form and long-form generations
withouttherelianceonexhaustivelylabeleddatasets.
Limitations. Wenotethatinsomeinstancesofoursingledatasetexperiments,theoriginalmodel
stillattainslowerAEDsthanthefine-tuningapproacheswehaveexploredinourexperiments. Dueto
resourceconstraints,wesuspectthatthisisduetotherelativelylowLoRArankr = 8employed
duringtrainingandtherelativelysmallnumberofdatapoints(2000)thatwehaveusedtotrainour
models.Indeed,despiteconvergenceonthetrainingset,weobservethatourfine-tunedmodelscannot
exactlyfitthetrainingset. Futureworkcouldincludefullfine-tuningandtoscaleupourexperiments
toincludemoretrainingpoints. Wewouldalsoliketoimprovethereliabilityofouradaptationplots
byrepeatingeachthresholdexperimentmultipletimes.
8 Conclusion
In this work, we proposed using semantic entropy to fine-tune LLMs to abstain from answering
questionsbeyondtheircapabilities. Underourproposedevaluationmetric,theAccuracy-Engagement
Distancewhichaccountsforboththeaccuracyandengagementofamodel,wedemonstratedthat
models fine-tuned on semantic entropy matched or outperformed models fine-tuned via existing
methodsthatreliedonground-truthlabelsorclassicalentropy.
Otherthanscalingupourexperimentsasdiscussedpreviously,futureworkmayentailexperimenting
with other open-source LLMs to see if our conclusions have a generalising effect. Furthermore,
future work can adapt our work to apply to longer generations, i.e. paragraphs or biographies.
Finally,giventhesuccessofusingsemanticentropytoreducehallucinationsandrecentevidence
8
tcerroC
.oNthat the model may be computing semantic entropy internally (Kossen et al., 2024), future work
canalsoexploreusingsemanticentropyasafine-tuningmethodtocalibratemodelsandcomparing
itsviabilitywithpreviousworks(Zhangetal.,2024;Kadavathetal.,2022).
Acknowledgments. TheauthorsthankNeilBandandmembersoftheOATMLgroupforinsightful
discussionsthroughoutthedevelopmentofthispaper.
References
G.Ahdritz,T.Qin,N.Vyas,B.Barak,andB.L.Edelman. Distinguishingtheknowablefromthe
unknowablewithlanguagemodels,2024. URLhttps://arxiv.org/abs/2402.03563.
A.AzariaandT.Mitchell. Theinternalstateofanllmknowswhenit’slying,2023. URLhttps:
//arxiv.org/abs/2304.13734.
F.Brahman,S.Kumar,V.Balachandran,P.Dasigi,V.Pyatkin,A.Ravichander,S.Wiegreffe,N.Dziri,
K.Chandu,J.Hessel,Y.Tsvetkov,N.A.Smith,Y.Choi,andH.Hajishirzi. Theartofsayingno:
Contextualnoncomplianceinlanguagemodels,2024. URLhttps://arxiv.org/abs/2407.
12043.
N.Chakraborty,M.Ornik,andK.Driggs-Campbell. Hallucinationdetectioninfoundationmodels
fordecision-making: Aflexibledefinitionandreviewofthestateoftheart,2024. URLhttps:
//arxiv.org/abs/2403.16527.
Q. Cheng, T. Sun, X. Liu, W. Zhang, Z. Yin, S. Li, L. Li, Z. He, K. Chen, and X. Qiu. Can ai
assistantsknowwhattheydon’tknow?,2024. URLhttps://arxiv.org/abs/2401.13275.
Y.-S.Chuang,Y.Xie,H.Luo,Y.Kim,J.Glass,andP.He. Dola: Decodingbycontrastinglayers
improves factuality in large language models, 2024. URL https://arxiv.org/abs/2309.
03883.
S. Farquhar, J. Kossen, L. Kuhn, and Y. Gal. Detecting hallucinations in large language models
usingsemanticentropy. Nature,630(8017):625–630,Jun2024. ISSN1476-4687. doi: 10.1038/
s41586-024-07421-0. URLhttps://doi.org/10.1038/s41586-024-07421-0.
S.Feng,W.Shi,Y.Wang,W.Ding,V.Balachandran,andY.Tsvetkov. Don’thallucinate,abstain:
Identifyingllmknowledgegapsviamulti-llmcollaboration,2024. URLhttps://arxiv.org/
abs/2402.00367.
Google. Gemini: A family of highly capable multimodal models, 2024. URL https://arxiv.
org/abs/2312.11805.
T.Han,A.Kumar,C.Agarwal,andH.Lakkaraju. Medsafetybench: Evaluatingandimprovingthe
medicalsafetyoflargelanguagemodels,2024. URLhttps://arxiv.org/abs/2403.03744.
P.He,X.Liu,J.Gao,andW.Chen. Deberta: Decoding-enhancedbertwithdisentangledattention,
2021. URLhttps://arxiv.org/abs/2006.03654.
E.J.Hu,Y.Shen,P.Wallis,Z.Allen-Zhu,Y.Li,S.Wang,L.Wang,andW.Chen. Lora: Low-rank
adaptationoflargelanguagemodels,2021. URLhttps://arxiv.org/abs/2106.09685.
L.Huang,W.Yu,W.Ma,W.Zhong,Z.Feng,H.Wang,Q.Chen,W.Peng,X.Feng,B.Qin,and
T.Liu. Asurveyonhallucinationinlargelanguagemodels: Principles,taxonomy,challenges,and
openquestions,2023. URLhttps://arxiv.org/abs/2311.05232.
Z.Ji,N.Lee,R.Frieske,T.Yu,D.Su,Y.Xu,E.Ishii,Y.J.Bang,A.Madotto,andP.Fung. Surveyof
hallucinationinnaturallanguagegeneration. ACMComputingSurveys,55(12):1–38,Mar.2023a.
ISSN1557-7341. doi: 10.1145/3571730. URLhttp://dx.doi.org/10.1145/3571730.
Z.Ji,Z.Liu,N.Lee,T.Yu,B.Wilie,M.Zeng,andP.Fung. Rho(ρ): Reducinghallucinationin
open-domain dialogues with knowledge grounding, 2023b. URL https://arxiv.org/abs/
2212.01588.
M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer. Triviaqa: A large scale distantly supervised
challenge dataset for reading comprehension, 2017. URL https://arxiv.org/abs/1705.
03551.
S.Kadavath,T.Conerly,A.Askell,T.Henighan,D.Drain,E.Perez,N.Schiefer,Z.Hatfield-Dodds,
N.DasSarma,E.Tran-Johnson,S.Johnston,S.El-Showk,A.Jones,N.Elhage,T.Hume,A.Chen,
9Y. Bai, S. Bowman, S. Fort, D. Ganguli, D. Hernandez, J. Jacobson, J. Kernion, S. Kravec,
L.Lovitt,K.Ndousse,C.Olsson,S.Ringer,D.Amodei,T.Brown,J.Clark,N.Joseph,B.Mann,
S.McCandlish,C.Olah,andJ.Kaplan. Languagemodels(mostly)knowwhattheyknow,2022.
URLhttps://arxiv.org/abs/2207.05221.
J.Kossen,J.Han,M.Razzak,L.Schut,S.Malik,andY.Gal. Semanticentropyprobes: Robustand
cheaphallucinationdetectioninllms,2024. URLhttps://arxiv.org/abs/2406.15927.
L. Kuhn, Y. Gal, and S. Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty
estimationinnaturallanguagegeneration,2023. URLhttps://arxiv.org/abs/2302.09664.
T.Kwiatkowski,J.Palomaki,O.Redfield,M.Collins,A.Parikh,C.Alberti,D.Epstein,I.Polosukhin,
J.Devlin,K.Lee,K.Toutanova,L.Jones,M.Kelcey,M.-W.Chang,A.M.Dai,J.Uszkoreit,Q.Le,
andS.Petrov. Naturalquestions: Abenchmarkforquestionansweringresearch. Transactions
oftheAssociationforComputationalLinguistics,7:452–466,2019. doi: 10.1162/tacl_a_00276.
URLhttps://aclanthology.org/Q19-1026.
P.Lewis,E.Perez,A.Piktus,F.Petroni,V.Karpukhin,N.Goyal,H.Küttler,M.Lewis,W.tauYih,
T.Rocktäschel,S.Riedel,andD.Kiela. Retrieval-augmentedgenerationforknowledge-intensive
nlptasks,2021. URLhttps://arxiv.org/abs/2005.11401.
L.Liu,Y.Pan,X.Li,andG.Chen. Uncertaintyestimationandquantificationforllms: Asimple
supervisedapproach,2024. URLhttps://arxiv.org/abs/2404.15993.
I.LoshchilovandF.Hutter. Decoupledweightdecayregularization,2019. URLhttps://arxiv.
org/abs/1711.05101.
Meta. Thellama3herdofmodels,2024. URLhttps://arxiv.org/abs/2407.21783.
OpenAI. Gpt-4technicalreport,2024. URLhttps://arxiv.org/abs/2303.08774.
P.Rajpurkar,J.Zhang,K.Lopyrev,andP.Liang. Squad: 100,000+questionsformachinecompre-
hensionoftext,2016. URLhttps://arxiv.org/abs/1606.05250.
V. Rawte, S. Chakraborty, A. Pathak, A. Sarkar, S. M. T. I. Tonmoy, A. Chadha, A. P. Sheth,
andA.Das. Thetroublingemergenceofhallucinationinlargelanguagemodels–anextensive
definition,quantification,andprescriptiveremediations,2023. URLhttps://arxiv.org/abs/
2310.04988.
W.Shi,X.Han,M.Lewis,Y.Tsvetkov,L.Zettlemoyer,andS.W.tauYih. Trustingyourevidence:
Hallucinate less with context-aware decoding, 2023. URL https://arxiv.org/abs/2305.
14739.
K.Shuster,S.Poff,M.Chen,D.Kiela,andJ.Weston. Retrievalaugmentationreduceshallucination
inconversation,2021. URLhttps://arxiv.org/abs/2104.07567.
K.Tian,E.Mitchell,H.Yao,C.D.Manning,andC.Finn. Fine-tuninglanguagemodelsforfactuality,
2023. URLhttps://arxiv.org/abs/2311.08401.
S. M. T. I. Tonmoy, S. M. M. Zaman, V. Jain, A. Rani, V. Rawte, A. Chadha, and A. Das. A
comprehensivesurveyofhallucinationmitigationtechniquesinlargelanguagemodels,2024. URL
https://arxiv.org/abs/2401.01313.
G.Tsatsaronis,G.Balikas,P.Malakasiotis,I.Partalas,M.Zschunke,M.R.Alvers,D.Weissenborn,
A.Krithara,S.Petridis,D.Polychronopoulos,Y.Almirantis,J.Pavlopoulos,N.Baskiotis,P.Gal-
linari, T. Artiéres, A.-C. N. Ngomo, N. Heino, E. Gaussier, L. Barrio-Alvers, M. Schroeder,
I. Androutsopoulos, and G. Paliouras. An overview of the BIOASQ large-scale biomedical
semanticindexingandquestionansweringcompetition. BMCBioinformatics, 16(1):138, Apr.
2015.
B.Weiser. LawyerwhousedChatGPTfacespenaltyformadeupcitations. TheNewYorkTimes,
June2023.
B.Wen,J.Yao,S.Feng,C.Xu,Y.Tsvetkov,B.Howe,andL.L.Wang. Knowyourlimits: Asurvey
ofabstentioninlargelanguagemodels,2024. URLhttps://arxiv.org/abs/2407.18418.
R. Wolfe, I. Slaughter, B. Han, B. Wen, Y. Yang, L. Rosenblatt, B. Herman, E. Brown, Z. Qu,
N.Weber,andB.Howe. Laboratory-scaleai: Open-weightmodelsarecompetitivewithchatgpt
eveninlow-resourcesettings. InThe2024ACMConferenceonFairness, Accountability, and
Transparency,volume35,page1199–1210.ACM,June2024. doi: 10.1145/3630106.3658966.
URLhttp://dx.doi.org/10.1145/3630106.3658966.
10J.Yang,H.Jin,R.Tang,X.Han,Q.Feng,H.Jiang,B.Yin,andX.Hu.Harnessingthepowerofllmsin
practice:Asurveyonchatgptandbeyond,2023a. URLhttps://arxiv.org/abs/2304.13712.
Y. Yang, E. Chern, X. Qiu, G. Neubig, and P. Liu. Alignment for honesty, 2023b. URL https:
//arxiv.org/abs/2312.07000.
H.Zhang,S.Diao,Y.Lin,Y.R.Fung,Q.Lian,X.Wang,Y.Chen,H.Ji,andT.Zhang. R-tuning:
Instructinglargelanguagemodelstosay‘idon’tknow’,2024. URLhttps://arxiv.org/abs/
2311.09677.
Y.Zhang,Y.Li,L.Cui,D.Cai,L.Liu,T.Fu,X.Huang,E.Zhao,Y.Zhang,Y.Chen,L.Wang,A.T.
Luu,W.Bi,F.Shi,andS.Shi. Siren’ssongintheaiocean: Asurveyonhallucinationinlarge
languagemodels,2023. URLhttps://arxiv.org/abs/2309.01219.
A Appendix/supplementalmaterial
A.1 AnsweringSettingPrompts
A.1.1 Long-QAAnsweringSetting
IntheLong-QAsetting,weinstructmodelstogeneratefree-form,sentence-lengthresponsesusing
thefollowingprompt:
Answerthefollowingquestioninasinglecompletesentence. Short-formanswerswithouta
propersubjectandverbarenotallowed. Thereshouldbeasubject,verb,andanobjectin
yourcompletesentenceandyouranswersshouldaddressthequestiondirectly.
Question: {{InsertQuestion}}
Answer:
Figure4: Long-QAFree-formPrompt.
A.1.2 Short-QAAnsweringSetting
IntheShort-QAsetting,weusethefollowingprompttoinstructthemodeltogenerateshort-form
responsesnotmorethanafewwords:
11Answerthefollowingquestionasbrieflyaspossible.
Question: WhichchemicalelementhasthechemicalsymbolCa?
Answer: Calcium
Question: HowmanyTAp73isoformshavebeenidentifiedinhumans?
Answer: Seven
Question: Whatisthepowerhouseofthecell?
Answer: Mitochondria
Question: WhoauthoredtheHarryPotterbookseries?
Answer: J.K.Rowling
Question: WhatcountriesistheG7madeupof?
Answer: Canada,France,Germany,Italy,Japan,theUnitedKingdomandtheUnitedStates.
Question: {{InsertQuestion}}
Answer:
Figure5: PromptforShort-QA.
A.2 AccuracyEvaluation
Givenastandardresponseofourmodeltoaquestionq,andtheground-truthlabelforthatquestion,
weuseLLAMA-3-8B-INSTRUCTandthefollowingprompttoassesstheaccuracyofthestandard
response:
Weareassessingthequalityofanswerstothefollowingquestion: {{InsertQuestion}}
Thefollowingareexpectedanswerstothisquestion: {{InsertGround-TruthLabels}}
Theproposedansweris: {{InsertGeneration}}
Withinthecontextofthequestion,doestheproposedanswermeanthesameasanyofthe
expectedanswers?
Respondonlywithyesorno.
Response:
Figure6: Prompttemplateforaccuracyevaluation.
A.3 SemanticEntropyImplementation
Thenon-trivialpartofcomputingthesemanticentropyliesincomputingthesemanticequivalence
relationR(·,·)thatistrueiftwogenerationssharethesamesemanticmeaning(Section3). While
therearepotentiallymanychoicestoimplementR(·,·),inthiswork,wefollowKuhnetal.(2023)in
usingtheideaofbi-directionalentailmenttodeterminesemanticequivalence. Here,wetreattwo
generationssands′tobesemanticallyequivalentifandonlyifslogicallyentailss′andviceversa.
For example, ‘The capital of France is Paris’ and ‘Paris is the capital of France’ share the same
meaningastheylogicallyentaileachother. However,aswearesuppliedwithaquestion,wemust
constrainourentailmenttoholdwithinthecontextofthequestion. Forexample,thegenerations
‘Paris’and‘ThecapitalofFranceisParis’ontheirowndonotentailoneanotherastheformeronly
declares‘Paris’, withoutstatingthatitisthecapitalofFrance. However, ifthegenerationswere
producedwithrespecttothequestion‘WhatisthecapitalofFrance?’,thenwithinthecontextofthe
question,bothgenerationsentailoneanother.
12AsdescribedinSection6,weassigntwovariantsofsemanticentropytoeachquestion,SE(DeBERTa)
andSE(Llama),eachusingdifferentlanguagemodelstoperformentailment. Thefirstoftheseuses
DeBERTa-NLI,whichfollowsKuhnetal.(2023)’sproposal,andthesecondoftheseusesLlama-
3-70B-Instruct, which is inspired from Farquhar et al. (2024)’s findings that LLMs can perform
entailmentwell.
A.3.1 EntailmentusingDeBERTa-NLI
DeBERTa-NLI (He et al., 2021) is a language model based on the transformer encoder-decoder
architecturethatisfine-tunedonthetaskofnaturallanguageinference(NLI).InNLI,wearegiven
a‘premise’anda‘hypothesis’,andthetaskistoclassifywhetherthehypothesislogicallyfollows
from the premise (entailment), logically contradicts the premise (contradiction), or is logically
undeterminedgiventhepremise(neutral). TouseDeBERTa-NLItoseewhethertwogenerationss
ands′entailoneanothergiventhequestion,wefirstconcatenatethequestiontoeachsands′and
thenconcatenatebothconcatenationstogetherusingaspecialtoken. DeBERTa-NLIthenclassifies
thisconcatenationas‘entailment’,‘contradiction’,or‘neutral’. Wenextdothisfortheotherdirection
andonlydeemthatsands′aresemanticallyequivalentifDeBERTa-NLIsays‘entailment’forboth
directions.
A.3.2 EntailmentusingLlama-3-70B-Instruct
Llama-3-70B-Instructisthe70Billion-parametervariantofLlama-3-8B-Instruct. LeveragingLlama-
3-70B-Instruct’sabilitytofollowinstructionsandtoperformNLPtasksthroughin-contextlearning,
we used a 5-shot ICL prompt to do question-dependent (uni-directional) NLI (Appendix A.3.2).
Equivalently to the above, we only deem two generations s and s′ as semantically equivalent if
Llama-3-70B-Instructproduces‘entailment’forbothdirections.
Wearegiventwopossibleanswerstoaquestion,"PossibleAnswer1"and"PossibleAnswer
2". Inthistask,wearetryingtoevaluatewhether"PossibleAnswer1"semanticallyentail
"PossibleAnswer2".
Question: "Bywhatnameissinger’AnthonyDominicBenevetto’betterknown?"
PossibleAnswer1: ThesingerAnthonyDominicBenevettoisbetterknownasToniBasil.
Possible Answer 2: The singer Anthony Dominic Benevetto is better known as Antonio
CarlosJobim.
Does Possible Answer 1 semantically entail Possible Answer 2? Respond with only one
word: entailment,contradiction,orneutral.
Answer: contradiction
Question: "WhichwifeofHenryVIIIhadalreadymarriedtwicebeforeshebecamequeen,
andmarriedforafourthtimeafterHenry’sdeath?"
PossibleAnswer1: AnneBoleynisthewifeofHenryVIII.
PossibleAnswer2: AnneBoleynistheanswer.
Does Possible Answer 1 semantically entail Possible Answer 2? Respond with only one
word: entailment,contradiction,orneutral.
Answer: entailment
Question: "WhodidSimpleSimonmeetonhiswaytothefair?"
PossibleAnswer1: Hemetapie-man.
PossibleAnswer2: Hemetthefollowing: apie-man,ahorse,acow,andafox.
Does Possible Answer 1 semantically entail Possible Answer 2? Respond with only one
word: entailment,contradiction,orneutral.
Answer: neutral
13Question: "ThemostnortherlypartofmainlandAustraliaisinwhichstate?"
PossibleAnswer1: QueenslandisthemostnortherlypartofmainlandAustralia.
PossibleAnswer2: ThemostnortherlypartofmainlandAustraliaisWesternAustralia.
Does Possible Answer 1 semantically entail Possible Answer 2? Respond with only one
word: entailment,contradiction,orneutral.
Answer: contradiction
Question: "ThemostnortherlypartofmainlandAustraliaisinwhichstate?"
PossibleAnswer1: ItisinQueensland,inWesternAustralia.
PossibleAnswer2: Queensland.
Does Possible Answer 1 semantically entail Possible Answer 2? Respond with only one
word: entailment,contradiction,orneutral.
Answer: entailment
Question: "DavidJasonstarredasInspectorFrost,butwhoplayedhisbossSuperintendent
NormanMullet?"
PossibleAnswer1: StephenMcGannplayedhisboss.
PossibleAnswer2: NormanMulletplayedhisbossinSuperintendentNormanMullet.
Does Possible Answer 1 semantically entail Possible Answer 2? Respond with only one
word: entailment,contradiction,orneutral.
Answer: contradiction
Question: {{InsertQuestion}}
PossibleAnswer1: {{Inserts}}
PossibleAnswer2: {{Inserts′}}
Does Possible Answer 1 semantically entail Possible Answer 2? Respond with only one
word: entailment,contradiction,orneutral.
Answer:
Figure7: Uni-directionalentailmentICLprompt.
Having shown how to implement the semantic equivalence relation R via one of the two meth-
odsabove, wecannowclusterthehigh-temperatureresponsesintosemanticequivalenceclasses
C ,...,C . ThediscretesemanticentropythenfollowsviadirectcalculationofEquation(6). We
1 m
showaconcreteimplementationofsemanticclusteringandthesubsequentcalculationofdiscrete
semanticentropyinCodeBlock1.
1 from collections import Counter
2 def semantic_entropy(question, high_temp_generations):
3 next_id = 0
4 assignment = [-1] * len(high_temp_generations)
5 for i, s1 in enumerate(high_temp_generations):
6 if assignment != -1:
7 continue
8 # If s1 has not been assigned an id, assign it next_id.
9 assignment[i] = next_id
10 for j, s2 in enumerate(high_temp_generations[i + 1:]):
11 if is_semantically_equivalent(question, s1, s2):
12 assignment[j] = assignment[i]
13 next_id += 1
1414 freq_list = list(Counter(assignment.values()))
15 semantic_entropy = scipy.stats.entropy(freq_list)
16 return semantic_entropy
CodeBlock1: Pythoncodetocomputediscretesemanticentropy.
A.4 HyperparameterDetails
Allofourexperimentsunderbothansweringsettingsuseaglobalhyperparameterconfiguration.
We found that a learning rate of 3×10−5, a batch size of 48, and training for 7 epochs under a
cosineannealingschedulewithacycleof0.2yieldsdecentconvergenceonthetrainingsetandthe
in-distributionvalidationset. WeemployedtheAdamWoptimiser(LoshchilovandHutter,2019)for
allexperimentsandusedLoRAwithr =8onthequeryandvalueprojectionmatrices.
A.5 Short-QAOut-of-DistributionAdaptationPlot
WepresenttheShort-QAadaptationplotinFigure8. Aswecansee,theeffectislesspronounced
thanthatoftheLong-QAadaptationplotanditishardertodiscernanoveralltrend.
TriviaQA BioASQ NQ SQuAD
600 250
700 400
500 350
600 200
300
400
500
250 150
400 300
200
100
300 200 150
200 100 100 50
50
100
0
0 0
0 50 100 150 200 250 0 100 200 300 400 0 100 200 300 400 500 0 100 200 300 400 500 600
No. Incorrect No. Incorrect No. Incorrect No. Incorrect
SE (Llama) SE (DeBERTa) R-Tuning R-Tuning-U
Figure8: Short-QAOut-of-DistributionAdaptationPlot. Eachpointrepresentsafine-tunedmodel
trainedataspecificthreshold.
A.6 GranularBreakdownofExperiments
Table2showsagranularbreakdownofourexperimentsforBest-ThresholdEvaluation. Here,the
Settingcolumndenoteshowthemodelistrainedandevaluated. Forexample,“Long-QA:TriviaQA
◦TriviaQA”meansthatthemodelistrainedunderLong-QA,onthetrainingsplitofTriviaQA,and
evaluatedonthein-distributionvalidationsplitofTriviaQA.“Short-QA:BioASQ◦SQuAD”means
thatthemodelistrainedonthetrainingsplitofBioASQandevaluatedonallSQuADdatapoints
undertheShort-QAansweringsetting.
15
tcerroC
.oNTable2: GranularBreakdownofExperiments: Long-QAandShort-QAmean±standarddeviation
Accuracy-EngagementDistancesforeachfine-tuningmethod,thelowerthebetter. Greenindicates
lowest,blueindicatessecond-lowest.
Setting R-Tuning R-Tuning-U SE(DeBERTa)(ours) SE(Llama)(ours) OriginalModel
Long-QA: TriviaQA ◦ TriviaQA 0.196±0.001 0.261±0.007 0.201±0.002 0.187±0.002 0.202
Long-QA: TriviaQA ◦ BioASQ 0.313±0.002 0.562±0.018 0.322±0.002 0.310±0.002 0.318
Long-QA: TriviaQA ◦ NQ 0.424±0.001 0.491±0.008 0.443±0.001 0.432±0.003 0.448
Long-QA: TriviaQA ◦ SQuAD 0.532±0.004 0.615±0.009 0.557±0.008 0.541±0.001 0.583
Long-QA: BioASQ ◦ TriviaQA 0.213±0.001 0.268±0.032 0.203±0.003 0.210±0.002 0.192
Long-QA: BioASQ ◦ BioASQ 0.377±0.011 0.654±0.020 0.438±0.011 0.353±0.004 0.354
Long-QA: BioASQ ◦ NQ 0.427±0.002 0.541±0.037 0.432±0.001 0.424±0.002 0.448
Long-QA: BioASQ ◦ SQuAD 0.538±0.000 0.628±0.025 0.544±0.001 0.531±0.001 0.583
Long-QA: NQ ◦ TriviaQA 0.267±0.014 0.339±0.020 0.212±0.002 0.216±0.003 0.192
Long-QA: NQ ◦ BioASQ 0.403±0.020 0.658±0.008 0.404±0.012 0.318±0.002 0.318
Long-QA: NQ ◦ NQ 0.481±0.011 0.556±0.005 0.440±0.005 0.418±0.003 0.426
Long-QA: NQ ◦ SQuAD 0.560±0.006 0.651±0.006 0.554±0.002 0.541±0.001 0.583
Long-QA: SQuAD ◦ TriviaQA 0.293±0.034 0.252±0.026 0.206±0.004 0.223±0.012 0.192
Long-QA: SQuAD ◦ BioASQ 0.451±0.041 0.617±0.033 0.389±0.012 0.327±0.007 0.318
Long-QA: SQuAD ◦ NQ 0.478±0.019 0.526±0.030 0.444±0.004 0.427±0.003 0.448
Long-QA: SQuAD ◦ SQuAD 0.584±0.017 0.629±0.018 0.555±0.005 0.543±0.002 0.588
Long-QA: Triv,Bio,Nq◦Triv,Bio,Nq 0.357±0.019 0.506±0.006 0.421±0.018 0.319±0.002 0.328
Long-QA: Triv,Bio,Nq◦ SQuAD 0.556±0.010 0.656±0.010 0.588±0.021 0.532±0.002 0.583
Short-QA: TriviaQA ◦ TriviaQA 0.315±0.059 0.261±0.000 0.253±0.003 0.251±0.002 0.250
Short-QA: TriviaQA ◦ BioASQ 0.398±0.007 0.382±0.005 0.382±0.006 0.369±0.002 0.392
Short-QA: TriviaQA ◦ NQ 0.545±0.010 0.520±0.003 0.524±0.003 0.515±0.000 0.574
Short-QA: TriviaQA ◦ SQuAD 0.612±0.003 0.612±0.002 0.604±0.003 0.608±0.002 0.702
Short-QA: BioASQ ◦ TriviaQA 0.273±0.001 0.252±0.002 0.254±0.002 0.246±0.001 0.253
Short-QA: BioASQ ◦ BioASQ 0.414±0.012 0.399±0.005 0.395±0.006 0.397±0.011 0.412
Short-QA: BioASQ ◦ NQ 0.528±0.003 0.521±0.003 0.521±0.002 0.511±0.004 0.574
Short-QA: BioASQ ◦ SQuAD 0.611±0.003 0.657±0.006 0.654±0.005 0.618±0.007 0.702
Short-QA: NQ ◦ TriviaQA 0.327±0.026 0.250±0.002 0.243±0.001 0.246±0.001 0.253
Short-QA: NQ ◦ BioASQ 0.479±0.042 0.402±0.009 0.379±0.002 0.373±0.003 0.392
Short-QA: NQ ◦ NQ 0.551±0.027 0.522±0.005 0.509±0.002 0.518±0.004 0.562
Short-QA: NQ ◦ SQuAD 0.629±0.013 0.619±0.001 0.634±0.003 0.628±0.003 0.702
Short-QA: SQuAD ◦ TriviaQA 0.326±0.006 0.271±0.009 0.249±0.001 0.252±0.002 0.253
Short-QA: SQuAD ◦ BioASQ 0.468±0.010 0.422±0.008 0.384±0.005 0.372±0.001 0.392
Short-QA: SQuAD ◦ NQ 0.570±0.003 0.546±0.011 0.514±0.004 0.516±0.002 0.574
Short-QA: SQuAD ◦ SQuAD 0.646±0.005 0.613±0.007 0.595±0.005 0.593±0.003 0.708
Short-QA: Triv,Bio,Nq◦Triv,Bio,Nq 0.420±0.080 0.394±0.004 0.385±0.002 0.381±0.001 0.405
Short-QA: Triv,Bio,Nq◦ SQuAD 0.617±0.005 0.609±0.005 0.626±0.007 0.612±0.008 0.702
16