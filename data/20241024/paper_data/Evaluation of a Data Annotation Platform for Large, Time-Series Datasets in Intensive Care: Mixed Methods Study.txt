Evaluation of a Data Annotation Platform for Large, Time-Series
Datasets in Intensive Care: Mixed Methods Study
Marceli Wac1,2, Raul Santos-Rodriguez1, Chris McWilliams1,2, and Christopher Bourdeaux2
1Faculty of Engineering, University of Bristol
2University Hospitals Bristol and Weston NHS Foundation Trust
September 2023
1 Introduction
Intensive Care Units (ICUs) are complex and data-rich environments where critically ill patients
are admitted due to the seriousness of their condition [1], frequently necessitating multiple organ
support, continuous observations and sophisticated treatment such as mechanical ventilation. ICUs
arecharacterisedbyalargenumberofmedicaldevicescollectingdatasurroundingpatient’streatment,
medication intake and vital signs. This routinely collected data is collected by and aggregated in a
clinicalinformationsystemwhichallowsclinicianstoaccessittoinformtheprovisionofcare. Including
vital signs, medication, laboratory test results and treatment-specific measurements - each patient can
generate over 1000 different types of data points over the course of their stay in the ICU [2], resulting
in a large volume of data which is difficult to manage and utilise effectively.
The application of Machine Learning (ML) and computational techniques within healthcare is a
continuously growing area of interest within both research and commercial settings, and allows for
moreeffectiveutilisationofthedatatotargetspecificchallengesandproblemswithinhealthcare[3,4].
In a typical ML workflow within healthcare, a subset of the collected data is used together with an
ML algorithm to compute, or train a ML model capable of making predictions or deriving insights
from the data. Such model can then be deployed within the clinical setting where it continuously
ingests the patient data and integrates with the clinical information systems to deliver insights to the
clinical team and help them stratify their patients, inform their decisions or predict adverse events
before they become apparent to the staff.
While the data gathered in the ICUs can often be used directly, for example, to present the
correlation between different vital signs and the patient prognosis, complex tasks that rely heavily on
clinical experience and expertise may require human involvement to provide further guidance [5]. This
guidance, mostfrequentlyreferredtoaslabelsorannotations[6], candeliveradditionalinformationor
context to the existing data. An example of such a label could be a “yes” or “no” indicator of whether
a patient is ready for discharge or the type and class of tumour present on an x-ray image. With
this further knowledge, ML models can take advantage of the human experience to tackle complex
problems and deliver solutions that could not be inferred from the raw data alone [7]. While the
large volumes of data available in intensive care offer significant opportunities, working with this
1
4202
tcO
22
]CH.sc[
1v95961.0142:viXradata also presents a unique set of challenges, as the effort required to annotate the dataset increases
proportionallytoitssize[8]. Furthermore, thecomplexnatureofthehealthcaredataandthefactthat
applying a single label can require clinicians to look at multiple parameters, patient history and lab
resultsmaketheannotationtaskhighlylabour-intensive. Withtheclinician’stimebeingaremarkably
valuable resource, ensuring the effectiveness of the data annotation workflow is paramount. The lack
of an annotation system tailored to the unique nature of the healthcare data (e.g. accessing a subset
of relevant variables from a list of potentially hundreds of parameters [9]) further complicates this
problem.
1.1 Semi-supervised and Weak Supervision Methods
Annotation of clinical data is a particularly challenging problem which relies on the domain expertise
andfrequentlylargetimecommitmentfromitsannotators. Thismakestraditionalapproaches,suchas
“crowdsourcing”,impracticalduetothelargevolumeofdatathatneedstobeannotatedandrelatively
narrow pool of annotators who have the time and expertise needed to annotate it. Furthermore,
because these approaches assume a regime in which a large number of annotators label small subsets
of the dataset, their results frequently rely on the robustness of individual labels and high degree of
agreement between the annotators. The results produced by crowdsourcing may therefore generate
data insufficient for training ML models using purely supervised approaches (e.g. those relying solely
on the labelled data), particularly if the number and diversity of annotations do not adequately cover
the variance of data present within the dataset.
Problems addressable by supervised ML methods can however utilise additional techniques when
the underlying dataset contains only a partially annotated data [10]. These approaches could be
categorised as either semi-supervised learning, in which unlabelled data is used to improve the gen-
eralisability of the model trained on the limited number of labels, or weak supervision methods,
where less accurate or noisy data sources are used to generate labels for the unlabelled data. Both
semi-supervised and weak supervision approaches are particularly well suited to the problems where
annotation of the entire dataset is prohibitively difficult or expensive, but obtaining large volumes of
unlabelled data is relatively easy [11].
One prominent example of weak supervision techniques which could be facilitated using our tool is
known as “data programming” [12]. In data programming, instead of applying labels directly to the
data, annotators describe the process through which the labels could be applied, by defining heuristic
rules known as labelling functions [12]. Depending on the task, these functions can utilise several
approaches that rely on existing knowledge bases and libraries, specific characteristics and patterns in
the data or the combination of both. Because each of the labelling functions can prioritise different
heuristic and aspect of data, annotations created by individual functions are typically very noisy and
biased towards their specific approach or a subset of targeted cases. Additionally, combining multiple
labelling functions is a complex task that needs to address conflicts or overlaps of created labels and
thedependenciesbetweenthedifferentfunctions. Thissuggeststhatsetsoflabellingfunctionsneedto
undergo additional processing before they can be used to annotate the data. Several approaches could
be assumed for this task, including simple techniques such as majority voting where the annotation is
applied only when majority of the labelling functions agree on the label. These techniques, however,
operateunderanassumptionthatallannotatorssharethesamedegreeofexpertise,whichisfrequently
not the case within the clinical setting. More complex methods of aggregating labels from multiple
2sources such as multi-task weak supervision (which estimates the accuracy of each labelling function,
aggregates their output and produces a model that can be used to annotate the data [11, 13]) or the
probabilistic approach proposed by Reykar et al. (which “jointly learns the classifier/regressor, the
annotator accuracy, and the actual true label” [14]) have been shown to outperform majority voting
and provide more accurate results [11, 14].
1.2 Research Scope
In our previous study, we conducted an experimental activity with the clinical staff from the ICUs,
which focused on capturing the approach to the data annotation task [15]. Through the analysis of
the observations made during this activity, we established 11 key requirements for a data annotation
platformtailoredtothelarge,time-seriesdatasetsintheclinicalsetting[15]. Wethenanalysedexisting
data annotation tools and evaluated their features for use in the clinical context, and proposed a novel
solution to the data annotation in a form of a bespoke tool designed to meet these requirements [16].
In this study, we present a novel tool for data annotation within the clinical setting, focusing on
two distinct approaches it facilitates. The first approach allows for direct annotation of individual
admissions, while the second allows the annotators to define rulesets that can be used to create
labels across the entire dataset (see subsection 2.3). Both approaches are capable of collecting the
annotations from multiple users, and in the case of annotation of individual admissions, our tool also
facilitates dynamic assignment of subsets of data to the annotators. This makes our platform suitable
for capturing annotations within multi-user frameworks such as crowdsourcing, but it also enables the
useofweaksupervisiontechniquessuchasdataprogrammingbytreatingcollectedrulesetsaslabelling
functions. Furthermore, our tool allows for simultaneous and flexible annotation using either method,
making it an adequate solution for the deployment of semi-supervised techniques that supplement
partially annotated datasets with additional information from the rulesets.
Designing machine learning pipelines is however a complex process that encompasses various areas
beyond the data annotation, such as data pre-processing, model selection, feature engineering and
hyper-parameter tuning. Similarly, the process of data annotation is itself concerned with many
challenges such as aggregating the annotations from multiple sources, establishing their biases and
accuracies and consolidating multiple labelling functions into a reliable annotation system. Because
each of these areas requires a careful consideration and expertise, they all merit their own dedicated
research, whichfallsoutsideofthescopeofthisstudy. Instead, inourworkwefocussolelyonthedata
collection and usability aspects of our tool and acknowledge that further processing of the annotations
collected as part of this study may be required to establish a robust annotation model and highlight
the limited immediate usability of the annotations generated using the individual rulesets.
2 Methods
To evaluate our tool, we set out to capture several metrics which could inform both the usability of
our tool and its performance in the data collection as part of the annotation workflow. We assumed
a sequential, mixed-method study design involving practical data annotation with the clinical staff
followedbyanassessmentofitsusability. Thestudywassplitintotwoconsecutivestagesandinvolved
annotation of weaning from mechanical ventilation. The Stage 1 (S1) focused on the annotation of
individual admissions, in which participants created labels directly on top of the admission data,
3while the Stage 2 (S2) was centred around the semi-automated approach to annotation and involved
participants creating rulesets that would be used to annotate the entire dataset simultaneously. Both
stages were piloted with an ICU Consultant who had extensive experience in critical care medicine, as
well as researchers from a ML and data scientists background prior to the recruitment of participants.
Piloting the activity allowed us to refine the user interface of the software and expand the creation
form with additional fields which enabled us to capture the annotator’s confidence in the created label
and provide a set of parameters that suggested the label creation in the first place. Both stages were
facilitated using the same instance of our custom tool developed for this study [16].
2.1 Participant Recruitment
TherecruitmentofparticipantsforbothS1andS2followedamixtureofconvenienceandsnowballsam-
pling and took place at the University Hospitals Bristol and Weston NHS Foundation Trust (UHBW)
in the United Kingdom (UK). Participants were invited to each stage of the study separately via
e-mail sent to the staff working in the ICU who had prior experience with mechanical ventilation
treatment. The e-mails contained the participant information sheet which detailed the purpose of the
study, instructions for the enrolment using our tool as well as an illustrated guide of the annotation
process facilitated using our tool. The enrolment process was facilitated entirely through our tool and
involved participants visiting a provided link, which presented them with an account creation form
and a link to an electronic signature-enabled consent form. Upon completion of the sign-up process,
participantshadtoverifyboththeirelectronicsignatureandanaccountcreatedonourplatformusing
links sent to their e-mail address. Crucially, as S1 and S2 were recruited for separately, participants
who took part in S1 were not required to, or excluded from taking part in S2.
We encountered significant challenges in the recruitment and engagement of participants through-
out the course of this study. This necessitated the development of interventions which would increase
the number of participants and promote involvement in the research activities. Section 3 outlines the
problems encountered during the participant recruitment and engagement throughout the study, as
well as the mitigation strategies implemented to assimilate them. Our recruitment process yielded a
sample size of 28 participants, of whom 22 consented to take part in S1 and 13 consented to take part
in S2. Of those, 12 participants engaged with S1 by creating at least 1 annotation and 9 engaged with
S2 by creating at least 1 ruleset; 1 of the participants engaged in annotation in both S1 and S2 and 8
never engaged with annotation despite consenting to take part in the study (see Table 1).
The research site (UHBW) had a pre-established research partnership with University of Bristol
and some of the participants might have known the researchers involved through their professional
relationship and the research previously undertaken at this site.
2.2 Dataset for Annotation
To provide data adequate for the annotation process, Medical Information Mart for Intensive Care
IV version 2.0 (MIMIC) [2] was used as the underlying dataset. The data was processed to produce
a format which could be used in annotation by aggregating time-series parameters relevant to the
mechanical ventilation on an hourly basis for each of the eligible admissions. The included parameters
were selected by two independent clinicians working in the ICU and consisted of 50 parameters,
including vital signs, haemodynamics, and mechanical ventilation-specific parameters (see Table 5).
4Figure 1: Enrolment screen embedded within our tool to guide participants on how to take part in
our study.
5Table 1: Participants and their job roles aggregated across the two stages of the study. Columns
enrolled and annotated denote whether participant consented to take part in the study and created
at least one annotation in the given stage of the study respectively.
Enrolled Annotated
ID Job role Stage 1 Stage 2 Stage 1 Stage 2
p01 ICU Consultant
p02 ICU Consultant (cid:52) (cid:52) (cid:52) (cid:52)
p03 ICU Registrar (cid:50) (cid:52) (cid:50) (cid:52)
p04 ICU Consultant (cid:50) (cid:52) (cid:50) (cid:52)
p05 ICU Consultant (cid:52) (cid:52) (cid:50) (cid:52)
p06 ICU Registrar (cid:52) (cid:50) (cid:52) (cid:50)
p07 ICU Registrar (cid:52) (cid:50) (cid:52) (cid:50)
p08 ICU Consultant (cid:52) (cid:50) (cid:52) (cid:50)
p09 Paediatric ICU Consultant (cid:52) (cid:50) (cid:52) (cid:50)
p10 ICU Registrar (cid:50) (cid:52) (cid:50) (cid:52)
p11 ICU Fellow (cid:50) (cid:52) (cid:50) (cid:52)
p12 ICU Fellow (cid:52) (cid:52) (cid:50) (cid:50)
p13 ICU Fellow (cid:52) (cid:50) (cid:52) (cid:50)
p14 ICU Consultant (cid:52) (cid:50) (cid:52) (cid:50)
p15 ICU Registrar (cid:52) (cid:50) (cid:52) (cid:50)
p16 ICU Consultant (cid:52) (cid:52) (cid:50) (cid:50)
p17 Advanced Critical Care Practitioner (cid:52) (cid:50) (cid:52) (cid:50)
p18 ICU Physiotherapist (cid:52) (cid:52) (cid:50) (cid:50)
p19 ICU Consultant (cid:52) (cid:52) (cid:50) (cid:50)
p20 ICU Consultant (cid:52) (cid:50) (cid:52) (cid:50)
p21 ICU Registrar (cid:52) (cid:52) (cid:50) (cid:52)
p22 Paediatric ICU Consultant (cid:52) (cid:50) (cid:52) (cid:50)
p23 ICU Physiotherapist (cid:50) (cid:52) (cid:50) (cid:52)
p24 Paediatric ICU Consultant (cid:52) (cid:50) (cid:52) (cid:50)
p25 ICU Registrar (cid:50) (cid:52) (cid:50) (cid:52)
p26 ICU Consultant (cid:52) (cid:50) (cid:50) (cid:50)
p27 ICU Fellow (cid:52) (cid:50) (cid:50) (cid:50)
p28 ICU Registrar (cid:52) (cid:50) (cid:50) (cid:50)
(cid:52) (cid:50) (cid:50) (cid:50)
Total count 22 13 12 9
6The eligibility criteria used to select the admissions required that subjects were at least 18 years old
at the time of admission, had undergone an invasive mechanical ventilation treatment that lasted for
a minimum of 24 hours during their admission, their stay in the ICU lasted for a minimum of 4 days,
andthatdidnotendwithdeath, includingupto48hoursfollowingdischarge. Thedatawasextracted
using an Structured Query Language (SQL) script ran on the PostgreSQL installation of MIMIC with
the concept tables computed [17].
2.3 Data Collection
2.3.1 Stage 1 - Annotation of Individual Admissions
The first stage focused on the annotation of individual admissions and used a configuration that
assigned 40 admissions to each of the participants. Of those, 20 were shared among all participants,
and 20 were unique to each of the participants. In order to ensure that the annotated data could be
used to compare the annotations created by different participants while maximising the total number
of annotated admissions, the assigned admissions were ordered in an alternating fashion (e.g. every
other admission annotated by a participant belonged to a “shared” set, while the rest were unique
to that participant). S1 annotation activity was designed to run over the course of 28 days in an
entirely asynchronous way, allowing each participant to undertake the task at the time that suited
their schedule and over any number of sessions. Due to the initially poor engagement with the study
during this stage, this approach had to have been revised. While the original time-frame for the
data collection remained the same, we conducted several interventions to further promote the study
with prospective participants and improve the engagement with the annotation activity (see Section
section 3). This also resulted in a change from our initial approach to conducting the annotation
activity exclusively asynchronously and remotely to a mixture of in-person, synchronous workshops
followed by further asynchronous participation.
Participants of S1 were instructed to annotate as many admissions as they could over the course
of this stage. The annotation process allowed participants to view the list of admissions assigned to
them, view any admission in the list independently and navigate to their next unannotated admission.
Upon annotating an admission, our tool automatically directed the participants to their next assigned
admission to preserve the continuity of the workflow.
2.3.2 Stage 2 - Semi-Automated Annotation
InS2, participantswereaskedtocreatearulesetthatbestdescribestheweaningfrommechanicalven-
tilation when applied to the entire dataset. Due to the more technical nature of the task, participants
were introduced to the tool during a remote video call which lasted approximately 75 minutes. Over
the course of the call, the facilitator demonstrated the functionality of the software by sharing his
screen, creating a ruleset and evaluating it using the calculated statistics which lasted approximately
45 minutes. The participants were then asked to perform the task on their own and given a chance
to ask questions and seek any further clarification on how to use the software over the remaining 30
minutes. Followingthisonboardingprocess, participantsweregivenaccesstothesoftwareandallowed
to use it independently over the course of 7 consecutive days at the time that suited their schedules.
Over that time, participants were asked to use the tool to create a ruleset that captures the weaning
from mechanical ventilation most accurately to the best of their ability. Each participant was able to
7Table 2: Questions featured in the Stage 2 evaluation questionnaire were used to assess the usability
and feasibility of the software and identify potential areas for its improvement.
No. Question
1. How did you find the data annotation tool overall?
2. Did you find the tool easy or difficult to use?
3. Were there any aspects of activity you have struggled with?
4. Can you think of any improvements that would make this tool or process better?
5. Was there anything you found particularly interesting or challenging in the process of data
annotation?
6. Doyouthinktheannotationofdatausingtheruleset-basedapproach(comparedtomanually
annotating each admission) is a feasible way of annotating large clinical datasets?
7. Was there anything you discovered or learnt about the weaning from mechanical ventilation
by taking part in this activity?
8. Having taken part in this activity, would you change anything about how you approach the
problem of weaning from mechanical ventilation in practice?
freely experiment with the software and create any number of rulesets. Following the completion of
the 7-day period, participants were asked to provide the identifier of the “final” ruleset that marked
their best attempt at solving the task, and to fill out a questionnaire capturing their feedback on the
usability and feasibility of the data annotation tool (see Table 2). The choice to utilise questionnaires
was made on the basis of the highly focused metrics we aimed to capture and the straightforward
nature of collecting data they offered. The questionnaire was designed to elicit feedback on several
key metrics surrounding the usability of the software and the views on the annotation process as a
whole. These included the overall sentiment to the data annotation process, the ease of use of our
tool, specific points of friction and potential improvements to the software, as well as participants’
attitudes towards the semi-automated approach to annotation.
3 Interventions to Promote Study Engagement
The initial approach to recruitment for S1 involved sending out an e-mail with a call for participation.
In addition to the study information, the e-mail explained the enrolment process, in which partic-
ipants were prompted to visit the attached link to the platform, sign the consent form and create
an account, before proceeding to the data annotation task. This proved ineffective and resulted in 6
consent signatures, 1 account registration and no annotations. These unsatisfactory results necessi-
tated intervention which would spur the recruitment of additional participants. Initially, we re-sent
the e-mail inviting participants to take part in the study, generating a further 1 consent signature,
3 account registrations and 8 annotations. While this increased the number of participants for our
study, it had a minimal effect on their engagement in the annotation task, indicating the need for
further interventions targetting participant engagement specifically. The limited response to the call
for participation suggested that a more personal approach, similar to the in-person workshop held in
our previous study [15] could prove more feasible. To further incentivise participation in our study, we
decided to host a series of workshops focused on discussing the use of data science within healthcare
8Figure 2: Participant engagement throughout the study has improved following our interventions.
Workshops were most effective at recruiting new participants, while the approach by a colleague
resulted in the highest increase in the number of created annotations.
framed as “structured learning events” which are required as part of the trainees’ portfolio to progress
their careers. Following the end of the second week of the study, we held three 2-hour workshops
which further increased the number of our participants, resulting in an additional 13 consent signa-
tures, 10 registrations and 31 annotations. While this also improved the engagement of the existing
participants demonstrated by the annotations they created following the workshops, participants who
enrolled during the workshops did not subsequently engage with the study. Our final approach to
remedy this and encourage more continuous participation focused on utilising the existing community
at the hospital. To that extent, we expanded our research team by onboarding one of the members
of clinical staff working at the hospital, to signpost and promote the study among her peers. This
approach proved particularly effective at improving the engagement with the activity, generating fur-
ther 2 consent signatures, 2 registrations and 17 annotations. The timeline demonstrating participant
engagement and interventions undertaken during S1 is depicted in the Figure 2.
ToensurebetterengagementforS2,weappliedtheinsightslearntfromourpreviousstudy[15]and
the interventions deployed during S1. Our approach to recruitment included an invitational e-mail to
the staff working in the ICUs in UHBW, as well as an in-person approach facilitated by the member
of clinical staff incorporated in our team; the participants were further incentivised to enrol in the
study with an offer of possibility to co-author the paper upon study completion. We restructured
the activity from an entirely asynchronous and remote process to one that began with an onboarding
video call and asynchronous annotation over the following 7 days. Over the course of three weeks, 12
participants consented to take part in the study, of whom 9 were on-boarded onto the platform and
completed the annotation task. One of the participants suggested an additional person who in their
description fitted the candidate profile for the study and was subsequently recruited.
9(a) Participants’ engagement differed following each (b)CumulativeannotationcountthroughoutS1. The
intervention. Workshops generated the highest num- initial lack of annotations has improved following the
ber of consent signatures, registrations and annotat- e-mail intervention and workshops, but the approach
ing participants; while the approach by the colleague by the colleagues resulted in the largest count of an-
was primarily effective at engaging participants with notations created using our tool.
the annotation activity.
Figure 3: Participant engagement throughout S1 following the implemented interventions.
3.1 Impact of Interventions
We observed varying effects on the participants’ involvement in the study following different types of
interventions we carried out. Overall, the most effective method of engaging with participants (gen-
erating consent form signatures, registrations and participation in the annotation task) was hosting
the workshops in the ICU (see Figure 3a). While the majority of the participants signed the consent
forms and registered for an account following e-mail and workshop interventions, the primary motiva-
tors for engagement with the task were both workshops and the approach of the colleague. In-person
workshops which focused on both improving recruitment and promoting engagement with the anno-
tation task proved to be effective but had short-lived effects in engaging participants with the task.
Addressing the prospective participants during workshops allowed for the recruitment of participants
who did not respond to the previous e-mails but was limited by the availability of staff present when
the workshop was taking place. Utilising the approach of the colleague as a strategy appeared to
be primarily effective in engaging existing participants rather than recruiting new ones, and resulted
in the largest number of annotations being created, suggesting that utilising social connections to
promote interest in the study can be particularly effective at improving engagement.
Approach by colleague led to the largest number of created annotations following the intervention
(see Figure 3b); despite being ineffective as a recruitment strategy, it had the highest conversion rate
of participants who consented to the study, registered on the tool and annotated data. Recruitment
via e-mail resulted in participants registering on the system but not engaging with the annotation
task (see Figure 3a).
3.2 Ethics Statement
This work was approved by the Faculty of Engineering Research Ethics Committee at the University
of Bristol (case 2022-150).
104 Results
4.1 Feedback Questionnaire
Participants found the interface of the data annotation tool to be intuitive and easy to use. They
expressed a preference for the graphical interface for creating rulesets and suggested that the node
environment allowed them to define the logic in an intuitive way. Participants also reflected on the
onboardingexperienceandfoundithelpfulasanintroductiontothetool. Intheirexperience,theyfelt
competent at using the tool after an initial 20-30 minutes of experimentation. One of the participants
suggested that an instructional video or an integrated tutorial could be helpful if the tool were to be
scaledtoalargerpopulation. Furthermore, participantsfeltthattheprocessofiteratingontheruleset
definition was intuitive and using the statistics page displayed following the ruleset creation helped
theminrefiningtheruleset. Someoftheparticipantsalsofoundtheabilitytopreviewtheannotations
in the context of individual admissions to be valuable and helpful in identifying edge cases, but felt
that it could be further improved to benefit the iterative process of creating rulesets.
The responses provided many suggestions for improvements to the tool, including the ability to
compare the results of their annotations to those of other annotators, limiting the set of parameters
to choose from when creating rules, facilitating a way to visually describe the impact of changes made
between different rulesets and ability to specify the temporal change in categorical variables (e.g.
change from a certain ventilator mode to a different one). They also found themselves questioning the
specificity of the annotation task and indicated that a concrete and robust definition of the research
question and desired annotation is critical for the effectiveness and accuracy of the process.
Some of the challenges experienced during the task included having to account for a large number
ofproprietarymodesofventilationandnotbeingabletoaccessadditionalcontextualinformationsuch
asthereasonsforadmission,clinicalnotesandvariablesrelatedtothemedicationintake. Additionally,
participants reported inconsistencies in the dataset which manifested as conflicting values for different
parameters and a large variability in clinical practice present throughout the data, making it difficult
to account for the edge cases. They reflected on the time-consuming nature of the manual annotation
process and the limitations associated with annotating large datasets using this approach. Conversely,
participantsalsofoundthesemi-automatedannotationtobetime-efficientanddescribeditasafeasible
approach to time-series data annotation in the clinical setting. One of the participants highlighted
the importance of a close working relationship between the researchers and annotators and the role it
can play in the effectiveness of the tool.
4.2 Annotation
4.2.1 Individual annotation
Over the course of 28 days, 12 participants annotated 31 unique admissions of which 4 were annotated
by more than one participant, resulting in a total of 47 annotated admissions and an average of
3.92 admissions annotated per participant. The admissions annotated by more than one participant
were annotated by 9, 5, 4 and 2 distinct participants respectively. 118 individual annotations were
created in total across all admissions and participants, resulting in an average of 2.51 annotations per
admission and 9.83 annotations per participant.
During the annotation process, participants could create any number of annotations for each
11Figure 4: Characteristics of annotations created by different participants; the boxes encapsulate the
Interquartile Range (IQR) as defined by Q3-Q1, the whiskers range up until Q1 - 1.5 IQR and Q3
+ 1.5 IQR, outliers are plotted as individual circles and the median is plotted in red. The majority
of participants stated their confidence as higher than 50%, and the median number of annotations
created for each admission was equal to or below 3 for the majority of the participants.
admission and specify the confidence in the accuracy of each of the created annotations using a slider
rangingbetween(0,1),withadefaultpositionof0.5. Themajorityofparticipants(8outof12)defined
the confidence in the created annotations as either 0.5 or above, suggesting high overall confidence
in the accuracy of the label. The median number of annotations created for each of the admissions
was equal to or lower than 3 for the majority of participants (11 out of 12). The characteristics of
annotations created by the participants during S1 are depicted on the Figure 4.
To gain a better insight into the collective approach to annotation of multiple annotators, we
trainedadecisiontreeclassifierbasedonthelabelscreatedbyallS1participants. Themodelassumed
two classes (weaning and not weaning) which were computed using the majority threshold for each
of the annotated admissions. To establish optimal parameters for the classifier we ran a 10-fold
grid search hyper-parameter tuning across a maximum number of features and maximum tree depth
ranging between (1,100) and (1,100) respectively. We reached an accuracy of 0.975 under a 70% train,
30% test split with the optimal parameters of 36 maximum features and a maximum depth of 52. A
comparison of the parameters specified by the participants as part of the annotations created during
S1 and the features of the decision tree classifier yielded several parameters that placed high on both
lists, including ventilator mode, oxygen delivery devices and the fraction of inspired oxygen. The
feature importance of the decision tree and the prevalence of parameters used by the participants in
S1 are depicted on the Figure 5.
4.2.2 Semi-automated annotation
The goal of S2 was for each participant to design a ruleset that, to the best of their ability, annotates
weaning from mechanical ventilation when applied to the data. During S2, 9 participants created a
12(a) (b)
Figure 5: Figure 5a depicts the feature importance for the decision tree classifier trained from S1
annotations;featureswithimportance< 1%arediscarded. Figure5bdepictstheparameterprevalence
asdescribedbythecountofparticipantswhousedthatparameterintheindividualannotationsovera
sum of all unique participant-parameter pairs used. The error bars describe the distance between the
importance of the feature in the decision tree and the prevalence of a given parameter in individual
annotations.
total of 65 unique rulesets composed of 489 rules and 255 relations. This resulted in an average of 7.22
rulesets, 54.33 rules and 28.33 relations per participant and 7.52 rules and 3.92 relations per ruleset.
Each ruleset was evaluated against all 1,967,145 individual hours of admissions in the dataset with
an average execution time of 1 minute and 20 seconds. This produced a total number of 2,421,873
annotations across all rulesets and 763,581 annotations across the final rulesets for each participant.
The distribution of rule counts across rulesets for each of the participants is depicted in Figure 6.
We observed a large variance across the parameters used in the rulesets to annotate the data. The
parameters were aggregated based on their type into three categories including set parameters explic-
itly defined by the clinician (e.g. mode of ventilation), observed parameters measured for the patient
Figure 6: The median count of rules within a ruleset was between 5 and 10 for the majority of
participants. The boxes encapsulate the IQR as defined by Q3-Q1; the whiskers range up until Q1 -
1.5 IQR and Q3 + 1.5 IQR; the median is plotted in red.
13Figure 7: Radial charts depict the preference for different types of parameters across the participants’
final rulesets; the average is depicted in a dashed line. We observed different preferences among par-
ticipants where preference was defined as a parameter type with the highest prevalence. 7 of the
participants preferred set parameters over other types, and 2 participants preferred observed parame-
ters.
(e.g. heart rate) and set-or-observed parameters which could change depending on the characteristics
of treatment (e.g. pressure delivered by the ventilator, which is set in pressure-controlled modes of
ventilation, but observed in the volume-controlled modes of ventilation). Participants exhibited differ-
ent preferences in parameter types they used in their final rulesets (see Figure 7). On average 64% of
rules used in the final rulesets were set parameters, 24% were observed parameters and the remaining
12% were set-or-observed parameters. 3 of the participants used only set and set-or-observed param-
eters and 3 other participants used only set and observed parameters; the 3 remaining participants
used all three types of parameters. We found no correlation between the roles of participants and
their preference towards different types of parameters.
4.2.3 Approach comparison
We compared the annotations in the context of the single admission annotated by the highest number
of annotators during S1 (9 annotators) to establish the similarity between different approaches to
annotation. This comparison included annotations created individually during S1, the annotations
created by participants using rulesets during S2 as well as those created using a ruleset derived
from the decision tree classifier trained on the individual annotations from S1 (see Figure 8). We
observed the smallest average error between annotations created during S1 (12.28%) and the highest
averageerrorbetweenannotationscreatedduringS2(30.65%)(seeTable3). Theaverageerroramong
annotations created during both stages was 28.47% and aggregating the annotations by type and
applying a majority threshold of > 50% produced an average error of 5.19% for annotations created
by both stages (see Table 3). For this particular admission, we also observed differences between
the continuity of annotations across different approaches. The decision tree-based ruleset provided
a single annotation, S1 annotations provided an average of 2.11 annotations per participant and S2
annotations provided an average of 4.33 annotations per participant.
Aggregating the annotations created using each approach across all shared admissions allowed us
toestablishthesimilaritybetweendifferentparticipantsandacrossdifferentapproaches(seeFigure9).
14Table 3: Average error between annotations of different types for admission annotated by the high-
est number of annotators during S1. The average error was computed as the average number of
mismatched annotations between all admission hours across all participants.
Average error [%]
Annotation source
Unaggregated Threshold > 50 %
Stage 1 12.28 % -
Stage 2 30.65 % -
Stage 1 and Stage 2 21.90 % 5.19 %
Stage 1 and Decision Tree 12.29 % 12.28 %
Stage 2 and Decision Tree 28.47 % 7.56 %
Stage 1, Stage 2 and Decision Tree 21.32 % 4.17 %
Figure 8: Annotations created using different techniques presented a large overlap on the admission
annotated by the highest number of annotators during S1. Figure 8a depicts the timeline for the
admission, together with the annotations created by different participants, aggregated by annotation
type. Figure 8b compares the mean number of annotations present across the admission timeline,
aggregated by annotation type. Figure 8c compares the different annotation types using the majority
of annotators as a threshold for annotation presence (mean > 0.5).
15The mean error across annotations created during S1 was 15.94%, those created during S2 - 33.78%
and across both S1 and S2 annotations - 31.62% (see Table 4). We observed a particularly low error
(2.98%) between S1 annotations created by participants p19 and p05 and a minimum error (2.63%)
between S1 annotations of p08 and S2 annotations of p02. The annotations created by the participant
who took part in both stages (p01) achieved an error of 10.02% between S1 and S2 annotations. In
particular, annotations created by the participants p08 and p01 in S1 exhibited the lowest average
error when compared to S2 annotations and the decision-tree-based annotations.
Table 4: Average error between annotations of different types for across the entire dataset. The
average error was computed as the average number of mismatched annotations between all admission
hours across all participants.
Annotation source Average error [%]
Stage 1 15.94 %
Stage 2 33.78 %
Stage 1 and Stage 2 31.62 %
Stage 1 and Decision Tree 25.77 %
Stage 2 and Decision Tree 30.46 %
Stage 1, Stage 2 and Decision Tree 28.80 %
5 Discussion
5.1 Effectiveness of Interventions
Recruitment of research participants can be a challenging process that evolves over the course of the
research to adapt to the changing circumstances and conditions [18, 19]. Despite the efforts made to
accommodate the busy schedules of the clinical staff working in the ICUs, such as asynchronous and
remote enrolment and annotation, we encountered challenges in recruiting and effectively engaging
participants during S1 of our study. The recruitment e-mails we sent out failed to generate a response
that satisfied our criteria, which could be attributed to several factors. The task brief asked partic-
ipants to annotate as many responses as they could in the time frame provided, suggesting a higher
time commitment than that of S1 and therefore lack of engagement with the study. Furthermore,
the asynchronous and remote nature of the task meant that participants were unable to seek addi-
tional support, particularly at the early stage of the activity. Finally, the lack of engagement with
the study beyond granting consent or registration could be attributed to the inconsequential nature
of this remote participation and the lack of incentive that would motivate further commitment to the
task.
We observed improved conversion of enrolled to active participants, as well as rise in engagement
with the annotation activity following the implementation of several interventions. Resending the
invitational e-mail prompted the existing participants to create an account in the tool and undertake
the annotation task, but failed at recruitment of new participants. This suggests a follow-up commu-
nication to be a viable strategy for re-engaging existing participants who have already enrolled in a
study, but also highlights the limitations of repeated attempts at recruitment via e-mail. A technique
that proved effective in recruiting participants involved directly incentivising prospective participants
16Figure 9: Confusion matrix for annotations created individually during S1 and semi-automated anno-
tations created using rulesets during S2 as well as those created by the ruleset built to resemble the
decision tree trained on the individual annotations from S1. The difference between annotations from
different sources is expressed in percentages, where 0% denotes perfect match and 100% denotes lack
of overlap. Missing data indicates that specific annotation sources did not share any admissions.
17by hosting workshops in the form of structured learning events. These events were required by the
clinical staff in the trainee roles to progress their careers and their deployment resulted in the most
effectiveacquisitionofnewparticipantsinourstudy. Conversely, thistechniquehadalimitedeffecton
generating prolonged engagement following the workshops, which could be attributed to the high fre-
quency at which research is being conducted at this particular hospital and therefore research fatigue.
We also anticipate that this particular incentive was of limited appeal to participants who were not
trainees and that offering a more ubiquitous incentives could prove more effective at recruiting more
diverse population. Finally, incorporating one of the members of clinical staff from the research site
within our research team proved to be the most impactful technique for promoting engagement with
the study activity. We anticipate that this form of utilising the existing community at the hospital
helped build trust and credibility of our research, and allowed us to approach the participants on a
more personal level, which in turn encouraged their involvement in the study. While we observed
substantial improvements in participation following this type of intervention, we acknowledge that it
maynotbesuitableincertainsettings,particularlywhenthereisalackofexistingconnectionbetween
the facilitators and the research site.
5.2 Usability
Data annotation is a complex and time-consuming process that relies on the domain expertise of
annotators for bringing in additional context to the data. Within intensive care settings, where the
number of available annotators is very limited, their time is in high demand and the volumes of data
are frequently too large for being labelled directly, annotating data poses a substantial challenge.
This calls for a solution that is both effective and time-efficient, offering a workflow that minimises the
frictionwiththetoolandmaximisestheproductivityoftheannotator. Thequalitativefindingsonthe
usability of our semi-automated annotation feature suggested that participants were able to use the
software effectively and found the interface intuitive and easy to use. In their feedback, participants
indicated a preference towards the semi-automated approach over the direct annotation of individual
admissions and reflected on the time-consuming nature of the manual annotation of data, speaking to
the appeal of using the graphical interface for creating rulesets. These results are supported by our
quantitative analysis where participants were able to define the rulesets that describe how annotations
shouldbeappliedandobservetheresultsoftheannotationinanaverageof1minuteand20seconds(in
addition to the time spent on describing the ruleset using our graphical interface). Furthermore, when
questionedaboutthepracticalityofsemi-automatedannotationforlargeclinicaldatasets,participants
identified this approach as a more feasible technique under the limiting circumstances of intensive care
settings. User acceptance plays a critical role in the successful adoption of software [20, 21], which
is particularly important in the context of clinical users annotating medical data. Ensuring that the
provided functionality meets the standards of the end users can help prevent errors when using the
software and ultimately lead to better outcomes [22, 23, 24, 25, 26, 27].
5.3 Requirements for Semi-automated Annotation
In our previous research, we established several requirements for the digital data annotation tool
that could be used in the intensive care setting [15]. In that study we observed multiple annotators
labellingindividualadmissionsandanalysedtheiractionstogaininsightintotheprocessofinteraction
18with the annotation task. As part of that approach, we observed participants reflecting on the data
through the lens of the previously created annotations, allowing them to better understand the data
and evaluate their annotations, frequently resulting in adjustments that improved the quality of their
annotations [15]. We identified this as a key characteristic of the annotation process and hypothesised
that, while this observation was made based on the annotation of individual admissions, the reflexive
process itself would also apply in a wider annotation context, including the semi-automated approach
toannotation. Consequently, we reflectedit inour requirementsas aneed forthe abilityto investigate
andanalysethecreatedannotations. Inourcurrentstudy,participantsdescribedtheabilitytoanalyse
the annotations created using rulesets as valuable and helpful in refining their rulesets and improving
the accuracy of their annotations. These findings support our initial hypothesis and highlight the
reflexive nature of annotation and the importance of its capture in the digital tool, irrespective of the
mechanisms used to annotate the data.
5.4 Approach to Annotation
The analysis of the data collected during both stages of this study allowed us to quantitatively assess
the feasibility and limitations of the semi-automated annotation in the clinical setting. Annotations
created by participants during S1 were characterised by high confidence in the annotations overall,
however, we observed this metric to be particularly high for ICU Consultants in comparison to other
roles. This could be attributed to the seniority of the position (in comparison to the ICU Registrars),
as well as the responsibilities associated with this job role. Collecting the self-described confidence
metric supplementing the annotation is an important aspect of crowdsourced data annotation that
can support the later use of annotations in ML contexts [28].
To enhance the understanding of the annotation process, we trained a decision tree classifier based
on the annotations created during S1. The analysis of the feature importance of that model and the
parameters marked as relevant during the individual annotation in S1 allowed us to cross-validate
the importance of different parameters on the specific annotation task of our study – weaning from
mechanical ventilation. In our analysis, several parameters scored high across both lists, including
ventilator mode, oxygen delivery devices and the fraction of inspired oxygen, suggesting the key role
of these parameters in the weaning process and our desired label. We observed an average error of
15.94% within annotations created during S1, suggesting some degree of disagreement between the
annotators and likely an inherent uncertainty within the task. Together with the high accuracy of
our model and the relatively small difference to a low average error between S1 annotations and those
created using our model (25.77%), our findings suggest the feasibility of a decision tree model for
several use-cases in the annotation process. Firstly, the model could be used as a preliminary measure
to establish the conformity of individual annotators to the overall annotators group by capturing
the distance between their individual annotations and those created by the decision tree classifier.
Calculating the divergence of annotators in specific cases could be used for anomaly detection and in
consequence “active learning”, in which annotations of particular disagreement would be highlighted
and prioritised for additional input by annotators. Secondly, the explainable nature of the decision
tree classifier could also be used to aid the interpretability of the resulting model by providing a set
of traceable steps used to create the labels. In the context of machine learning, and in particular the
critical settings such as intensive care, explainability plays a critical role in decision support systems
as it allows the clinical staff to understand the reasoning behind suggested decisions, which in turn
19builds trust and provides safeguarding mechanisms necessary within healthcare.
By analysing the final rulesets created by each of the participants and aggregating the parameters
theyusedintheirrulesets, wewereabletoidentifydifferentpreferenceswithinourparticipantpopula-
tion. The majority of the participants (7 out of 9) chose to use primarily the set parameters, while the
2 remaining participants focused primarily on the observed parameters. Despite the fact that we did
not observe the correlation between the choice of different parameter types and participants’ job roles,
this analysis generated substantial insights into the process of annotation within the clinical setting.
Primarily, the use of different types of parameters could be attributed to several factors including the
desired subject of the annotation, the preferences of annotators in how they assess the patients and
the combination of both. Treatment or delivery of specific procedures and interventions is informed
by the condition the patient is in and their individual needs. While these needs could be assessed by
focusing on the observed parameters, gaining a broader understanding of what led to this condition
in the first place may also require analysis of the set parameters (e.g. high heart rate (observed pa-
rameter could be attributed to a specific mode of ventilation (set parameter) that a patient is on).
Furthermore, creating labels that focus on a treatment-oriented task (e.g. presence of weaning from
mechanicalventilation)willlikelyresultinfocusonthetypeofparameterswhichdescribethedelivery
of that treatment – the set parameters. Conversely, labels capturing the inherent patient condition
(e.g. dependence on the ventilator) could instead focus on the observed parameters which describe
the overall state of the patient and their response to treatment. This insight is significant for the
process of data annotation because it allows for prioritisation of the data cleaning efforts and selection
of relevant parameters depending on the annotation task, but it also informs the inherent biases that
may be present in the created labels. Set parameters specific to a given the treatment are a result of
the clinical decisions made at the time when patient care was delivered. This suggests that both the
way in which the care was delivered and the resulting patient condition can be biased towards specific
treatment approaches or characteristics of the data collection site, which should be taken into account
when collecting the data annotations. Lastly, the choice to use different parameter types can provide
insight into the thought process and approach to data annotation of specific annotations which could
further enhance the collected annotations. Annotators who focus on any specific subset of parame-
ters may inherently omit other data sources and fail label cases which don’t align with their thought
process. Alternatively, their bias towards certain parameters could also translate to the created labels
and influence the performance of the model trained on those annotations.
5.5 Strengths and Limitations
Ourstudyfocusedonevaluatingthedigitaltime-seriesdataannotationtoolpurpose-builtfortheclin-
ical settings. We explored the feasibility of a novel, semi-automated approach to creating annotations,
which facilitated time-efficient annotation of large volumes of data by a limited number of annotators
characteristicoftheintensivecaresettings. Inthisstudy,wedemonstratedtheusabilityandfeasibility
of collecting annotations using our tool in a real-world scenario using real patient data. To the best of
our knowledge, this is the first study of its kind focused on evaluating the semi-automated approach
to collecting annotation within the clinical setting. One of the strengths of this study was the use of
mixed methods for the evaluation, which allowed us to capture both objective quantitative data on
the use of the software and more detailed and personal insights from the participants. Utilising mixed
methods and different sources of data is a strategy that provides a holistic picture of the results and
20improves the trustworthiness of research. In our findings, the collected results supported our previous
hypothesis on the importance of re-evaluating data in the context of created annotations, which was
reflected in both the number of created rulesets and annotations and the data collected via feedback
questionnaires.
While the low overlap of participants between S1 and S2 did not prevent us from evaluating the
tool, it limited our ability to measure the differences between annotations created directly on data
and those created via rulesets within the context of individual annotators. We anticipate that this
type of analysis could benefit our study and further improve the understanding of the limitations
of our tool and the characteristics of the annotation process as a whole. Furthermore, this study
focusedoncollectingdataannotationsexplicitlyinthecontextofweaningfrommechanicalventilation,
which resembled the task definition from our previous study. This allowed us to directly evaluate the
relevance of the captured requirements without introducing the bias of a different task, but limited
the generalisability of our results as it likely introduced a bias towards that specific task.
5.6 Future Work
The primary goal of the our evaluation was aimed at identifying the strengths and limitations of the
proposed design, assessing its usability and identifying potential improvements to the tool. To that
extent, we conducted a data annotation activity and collected annotators’ input using two distinct
techniques – by directly annotating individual admissions and by creating rulesets which could be
used to annotate the data. Future work should therefore focus on evaluating the performance of the
annotations collected via our tool within a broader machine learning contexts. This could include
addressing the problem of modelling the accuracy of annotations created by multiple experts, particu-
larly when the ground truth is not unavailable [12], as well as exploring the techniques for aggregating
annotationstoestablishmorerobusttrainingdatasets. Inparticular, whiletherulesetscollectedusing
our tool were used to directly annotate the data in order to allow for their analysis and refinement,
research should focus on aggregating multiple rulesets to develop a more durable annotation model.
To that extent, we suggest that the development of such model follows the approaches previously
demonstrated in weak supervision techniques such as data programming [12, 13], which are partic-
ularly suitable given the similarities between the rulesets and labelling functions. Furthermore, we
anticipate that incorporating additional functionality that facilitates active learning within our tool
would result in further improvements to the efficiency of the annotation process and provide unique
opportunities for effective annotation within the clinical setting.
Finally, the evaluation of our tool at a larger scale and among a wider population of annotators
could further improve the understanding of the feasibility and limitations of our proposed tool. In
particular, the effectiveness of the tool could be measured in a study that defines an annotation task
surrounding a concrete problem with a pre-established ground truth and with a higher overlap of
participants between different annotation methods. Such study could provide further insights into
the inter-annotator differences, their needs and preferences, and strengthen the understanding of the
annotation process within the clinical setting. While our tool was designed to support asynchronous
annotation by a large number of geographically distributed users, the evaluation took place in a single
research site over a limited span of time. Conducting a multi-centre study would therefore benefit the
evaluation of our tool and could also serve as a basis for identifying the differences and variation in
practice between different sites.
215.7 Conclusions
In this study, we presented a digital time-series data annotation tool for clinical settings. We demon-
strated its use in practice using real patient data from an anonymised dataset with expert annotators
from ICUs and evaluated its usability. By conducting two stages focused on individual and semi-
automated annotation, we explored how clinicians approach the task of data annotation and investi-
gated the feasibility of the semi-automated approach facilitated by our tool. The results of our study
demonstrated satisfaction with the tool and suggested the feasibility of its use for the annotation
of large clinical datasets. We observed similarities in annotations created using different techniques,
discussed the challenges of opportunities of the annotation in the healthcare setting and provided
suggestions for future research directions.
22Acknowledgements
This work was partly supported by the Engineering and Physical Sciences Research Council Digital
HealthandCareCentreforDoctoralTrainingattheUniversityofBristol(UKRIgrantEP/S023704/1
to MW). CMW was funded by the South West Better Care Partnership, supported by Health Data
Research UK. The project received an Amazon Web Services (AWS) Cloud Credit for Research grant.
Author Contributions
MW was involved in the study design, development of the software, data collection, data analysis, and
writing. CMW and RSR were involved in study design and critical revision.
Conflicts of Interest
None declared.
Acronyms
AWS Amazon Web Services
ICU Intensive Care Unit
IQR Interquartile Range
MIMIC Medical Information Mart for Intensive Care IV version 2.0
ML Machine Learning
S1 Stage 1
S2 Stage 2
SQL Structured Query Language
UHBW University Hospitals Bristol and Weston NHS Foundation Trust
UK United Kingdom
23Appendix
Table 5: Parameters used for the annotation activity.
Unit of
Parameter Group Parameter
measurement
Heart Rate bpm
Arterial Blood Pressure Systolic mmHg
Arterial Blood Pressure Diastolic mmHg
Arterial Blood Pressure Mean mmHg
Non-invasive Blood Pressure Systolic mmHg
Non-invasive Blood Pressure Diastolic mmHg
Vitals
Non-invasive Blood Pressure Mean mmHg
Cardiac Output (thermodilution) L/min
Respiratory Rate insp/min
Arterial O Pressure mmHg
2
Arterial O Saturation %
2
O Saturation (pulse-oximetry) %
2
PEEP Set cm H O
2
Temperature C
PCWP mmHg
F O %
i 2
Ventilator Mode
Cuff Pressure cm H O
2
Tidal Volume (set) mL
Tidal Volume (observed) mL
Tidal Volume (spontaneous) mL
Minute Volume L/min
Minute Volume Target (for IBW) %
Respiratory Rate (set) insp/min
Respiratory Rate (spontaneous) insp/min
Respiratory Rate (total) insp/min
Peak Inspiratory Pressure cm H O
2
Plateau Pressure cm H O
2
Mean Airway Pressure cm H O
Respiratory 2
Total PEEP Level cm H O
2
SBT Started
SBT Stopped
SBT Succeeded
SBT Deferred
Expiratory Ratio
Inspiratory Ratio
Inspiratory Pressure (Draeger) cm H O
2
BiPAP Mode
BiPAP EPAP cm H O
2
BiPAP IPAP cm H O
2
BIPAP BPM (S/T - back up) bmp
E CO mmHg
t 2
Ventilator Mode (Hamilton)
Inspiratory Pressure (Hamilton) cm H O
2
Resistance (expiratory) cm H O/L/sec
2
Resistance (inspiratory) cm H O/L/sec
2
Ventilation Type
Oxygen Delivery Devices
Equipment
Oxygen Flow L/min
Oxygen Flow (additional) L/min
24References
[1] D.BennettandJ.Bion,“Organisationofintensivecare,”BMJ : British Medical Journal,vol.318,
pp. 1468–1470, May 1999. tex.pmcid: PMC1115845.
[2] A. Johnson, L. Bulgarelli, T. Pollard, et al., “Mimic-iv,” June 2022.
[3] N. Noorbakhsh-Sabet, R. Zand, Y. Zhang, et al., “Artificial Intelligence Transforms the Future
of Health Care,” The American Journal of Medicine, vol. 132, pp. 795–801, July 2019.
[4] T. Davenport and R. Kalakota, “The potential for artificial intelligence in healthcare,” Future
Healthcare Journal, vol. 6, pp. 94–98, June 2019.
[5] B. Hunter, S. Hindocha, and R. W. Lee, “The Role of Artificial Intelligence in Early Cancer
Diagnosis,” Cancers, vol. 14, p. 1524, Mar. 2022.
[6] M.-S. Heo, J.-E. Kim, J.-J. Hwang, et al., “Artificial intelligence in oral and maxillofacial ra-
diology: what is currently possible?,” Dentomaxillofacial Radiology, vol. 50, p. 20200375, Mar.
2021.
[7] S. Azizi, B. Mustafa, F. Ryan, et al., “Big Self-Supervised Models Advance Medical Image Clas-
sification,” in2021 IEEE/CVF International Conference on Computer Vision (ICCV),pp.3458–
3468, Oct. 2021.
[8] N. Martinez-Martin, Z. Luo, A. Kaushal, et al., “Ethical issues in using ambient intelligence in
health-care settings,” The Lancet Digital Health, vol. 3, pp. e115–e123, Feb. 2021.
[9] T. K. Burki, “Artificial intelligence hold promise in the ICU,” The Lancet Respiratory Medicine,
vol. 9, pp. 826–828, Aug. 2021.
[10] R. Poyiadzi, D. Bacaicoa-Barber, J. Cid-Sueiro, et al., “The Weak Supervision Landscape,” in
2022 IEEE International Conference on Pervasive Computing and Communications Workshops
and other Affiliated Events (PerCom Workshops), pp. 218–223, Mar. 2022.
[11] A. Ratner, B. Hancock, J. Dunnmon, et al., “Training complex models with multi-task weak
supervision,” in Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and
Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Sym-
posium on Educational Advances in Artificial Intelligence, Aaai’19/iaai’19/eaai’19, (Honolulu,
Hawaii, USA), pp. 4763–4771, AAAI Press, Jan. 2019.
[12] A. Ratner, C. D. Sa, S. Wu, et al., “Data programming: creating large training sets, quickly,”
in Proceedings of the 30th International Conference on Neural Information Processing Systems,
Nips’16, (Red Hook, NY, USA), pp. 3574–3582, Curran Associates Inc., Dec. 2016.
[13] Snorkel Team, “LabelModel,” Sept. 2020.
[14] V. C. Raykar, S. Yu, L. H. Zhao, et al., “Learning from crowds,” Journal of Machine Learning
Research, vol. 11, no. 43, pp. 1297–1322, 2010.
[15] M. Wac, R. Santos-Rodriguez, C. McWilliams, et al., “Capturing Requirements for a Data An-
notation Tool for Intensive Care: Experimental User-Centered Design Study,” Sept. 2023.
25[16] M. Wac, R. Santos-Rodriguez, C. McWilliams, et al., “CATS: Cloud-native time-series data
annotation tool for intensive care,” SoftwareX, vol. 24, p. 101593, Dec. 2023.
[17] A. E. W. Johnson, D. J. Stone, L. A. Celi, et al., “The MIMIC Code Repository: enabling re-
producibility in critical care research,” Journal of the American Medical Informatics Association,
vol. 25, pp. 32–39, Jan. 2018.
[18] I. Bonisteel, R. Shulman, L. A. Newhook, et al., “Reconceptualizing Recruitment in Qualitative
Research,” International Journal of Qualitative Methods, vol. 20, p. 16094069211042493, Jan.
2021.
[19] K. A. Negrin, S. E. Slaughter, S. Dahlke, et al., “Successful Recruitment to Qualitative
Research: A Critical Reflection,” International Journal of Qualitative Methods, vol. 21,
p. 16094069221119576, Apr. 2022.
[20] H. Taherdoost, “A review of technology acceptance and adoption models and theories,” Procedia
Manufacturing, vol. 22, pp. 960–967, 2018.
[21] A. M. Momani, M. M. Jamous, and S. M. S. Hilles, “Technology Acceptance Theories: Review
and Classification,” International Journal of Cyber Behavior, Psychology and Learning, vol. 7,
pp. 1–14, Apr. 2017.
[22] L. Damodaran, “User involvement in the systems design process-a practical guide for users,”
Behaviour & Information Technology, vol. 15, pp. 363–377, Jan. 1996.
[23] P. Slattery, A. K. Saeri, and P. Bragge, “Research co-design in health: a rapid overview of
reviews,” Health Research Policy and Systems, vol. 18, p. 17, Feb. 2020.
[24] L. Teixeira, C. Ferreira, and B. S. Santos, “Using task analysis to improve the requirements
elicitation in health information system,” in 2007 Annual International Conference Of The Ieee
Engineering In Medicine And Biology Society, Vols 1-16, Proceedings Of Annual International
Conference Of The Ieee Engineering In Medicine And Biology Society, (345 E 47th St, New York,
Ny 10017 Usa), pp. 3669–3672, Ieee, 2007.
[25] J.L.Martin,D.J.Clark,S.P.Morgan,etal.,“Auser-centredapproachtorequirementselicitation
in medical device development: A case study from an industry perspective,” Applied Ergonomics,
vol. 43, pp. 184–190, Jan. 2012.
[26] B. Davey and K. R. Parker, “Requirements Elicitation Problems: A Literature Review,” Issues
in Informing Science and Information Technology, vol. 12, pp. 71–82, 2015.
[27] A. O’Cathain, L. Croot, E. Duncan, et al., “Guidance on how to develop complex interventions
to improve health and healthcare,” BMJ Open, vol. 9, p. e029954, Aug. 2019. Publisher: British
Medical Journal Publishing Group Section: Research methods.
[28] Y.Ni,M.McVicar,R.Santos-Rodr´ıguez,etal.,“UnderstandingEffectsofSubjectivityinMeasur-
ingChordEstimationAccuracy,”IEEE Transactions on Audio, Speech, and Language Processing,
vol. 21, pp. 2607–2615, Dec. 2013.
26