[
    {
        "title": "Supporting Annotators with Affordances for Efficiently Labeling Conversational Data",
        "authors": "Austin Z. HenleyDavid Piorkowski",
        "links": "http://arxiv.org/abs/2403.07762v1",
        "entry_id": "http://arxiv.org/abs/2403.07762v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07762v1",
        "summary": "Without well-labeled ground truth data, machine learning-based systems would\nnot be as ubiquitous as they are today, but these systems rely on substantial\namounts of correctly labeled data. Unfortunately, crowdsourced labeling is time\nconsuming and expensive. To address the concerns of effort and tedium, we\ndesigned CAL, a novel interface to aid in data labeling. We made several key\ndesign decisions for CAL, which include preventing inapt labels from being\nselected, guiding users in selecting an appropriate label when they need\nassistance, incorporating labeling documentation into the interface, and\nproviding an efficient means to view previous labels. We implemented a\nproduction-quality implementation of CAL and report a user-study evaluation\nthat compares CAL to a standard spreadsheet. Key findings of our study include\nusers using CAL reported lower cognitive load, did not increase task time,\nusers rated CAL to be easier to use, and users preferred CAL over the\nspreadsheet.",
        "updated": "2024-03-12 15:51:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07762v1"
    },
    {
        "title": "Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion",
        "authors": "Dongyang LiChen WeiShiying LiJiachen ZouQuanying Liu",
        "links": "http://arxiv.org/abs/2403.07721v1",
        "entry_id": "http://arxiv.org/abs/2403.07721v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07721v1",
        "summary": "How to decode human vision through neural signals has attracted a\nlong-standing interest in neuroscience and machine learning. Modern contrastive\nlearning and generative models improved the performance of fMRI-based visual\ndecoding and reconstruction. However, the high cost and low temporal resolution\nof fMRI limit their applications in brain-computer interfaces (BCIs), prompting\na high need for EEG-based visual reconstruction. In this study, we present an\nEEG-based visual reconstruction framework. It consists of a plug-and-play EEG\nencoder called the Adaptive Thinking Mapper (ATM), which is aligned with image\nembeddings, and a two-stage EEG guidance image generator that first transforms\nEEG features into image priors and then reconstructs the visual stimuli with a\npre-trained image generator. Our approach allows EEG embeddings to achieve\nsuperior performance in image classification and retrieval tasks. Our two-stage\nimage generation strategy vividly reconstructs images seen by humans.\nFurthermore, we analyzed the impact of signals from different time windows and\nbrain regions on decoding and reconstruction. The versatility of our framework\nis demonstrated in the magnetoencephalogram (MEG) data modality. We report that\nEEG-based visual decoding achieves SOTA performance, highlighting the\nportability, low cost, and high temporal resolution of EEG, enabling a wide\nrange of BCI applications. The code of ATM is available at\nhttps://anonymous.4open.science/status/EEG_Image_decode-DEEF.",
        "updated": "2024-03-12 14:58:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07721v1"
    },
    {
        "title": "generAItor: Tree-in-the-Loop Text Generation for Language Model Explainability and Adaptation",
        "authors": "Thilo SpinnerRebecca KehlbeckRita SevastjanovaTobias StähleDaniel A. KeimOliver DeussenMennatallah El-Assady",
        "links": "http://dx.doi.org/10.1145/3652028",
        "entry_id": "http://arxiv.org/abs/2403.07627v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07627v1",
        "summary": "Large language models (LLMs) are widely deployed in various downstream tasks,\ne.g., auto-completion, aided writing, or chat-based text generation. However,\nthe considered output candidates of the underlying search algorithm are\nunder-explored and under-explained. We tackle this shortcoming by proposing a\ntree-in-the-loop approach, where a visual representation of the beam search\ntree is the central component for analyzing, explaining, and adapting the\ngenerated outputs. To support these tasks, we present generAItor, a visual\nanalytics technique, augmenting the central beam search tree with various\ntask-specific widgets, providing targeted visualizations and interaction\npossibilities. Our approach allows interactions on multiple levels and offers\nan iterative pipeline that encompasses generating, exploring, and comparing\noutput candidates, as well as fine-tuning the model based on adapted data. Our\ncase study shows that our tool generates new insights in gender bias analysis\nbeyond state-of-the-art template-based methods. Additionally, we demonstrate\nthe applicability of our approach in a qualitative user study. Finally, we\nquantitatively evaluate the adaptability of the model to few samples, as\noccurring in text-generation use cases.",
        "updated": "2024-03-12 13:09:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07627v1"
    },
    {
        "title": "Imagine a dragon made of seaweed: How images enhance learning in Wikipedia",
        "authors": "Anita SilvaMaria TracyKatharina ReineckeEytan AdarMiriam Redi",
        "links": "http://arxiv.org/abs/2403.07613v1",
        "entry_id": "http://arxiv.org/abs/2403.07613v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07613v1",
        "summary": "Though images are ubiquitous across Wikipedia, it is not obvious that the\nimage choices optimally support learning. When well selected, images can\nenhance learning by dual coding, complementing, or supporting articles. When\nchosen poorly, images can mislead, distract, and confuse. We developed a large\ndataset containing 470 questions & answers to 94 Wikipedia articles with images\non a wide range of topics. Through an online experiment (n=704), we determined\nwhether the images displayed alongside the text of the article are effective in\nhelping readers understand and learn. For certain tasks, such as learning to\nidentify targets visually (e.g., \"which of these pictures is a gujia?\"),\narticle images significantly improve accuracy. Images did not significantly\nimprove general knowledge questions (e.g., \"where are gujia from?\"). Most\ninterestingly, only some images helped with visual knowledge questions (e.g.,\n\"what shape is a gujia?\"). Using our findings, we reflect on the implications\nfor editors and tools to support image selection.",
        "updated": "2024-03-12 12:50:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07613v1"
    },
    {
        "title": "Customizable Avatars with Dynamic Facial Action Coded Expressions (CADyFACE) for Improved User Engagement",
        "authors": "Megan A. WitherowCrystal ButlerWinston J. ShieldsFurkan IlginNorou DiawaraJanice KeenerJohn W. HarringtonKhan M. Iftekharuddin",
        "links": "http://arxiv.org/abs/2403.07314v1",
        "entry_id": "http://arxiv.org/abs/2403.07314v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07314v1",
        "summary": "Customizable 3D avatar-based facial expression stimuli may improve user\nengagement in behavioral biomarker discovery and therapeutic intervention for\nautism, Alzheimer's disease, facial palsy, and more. However, there is a lack\nof customizable avatar-based stimuli with Facial Action Coding System (FACS)\naction unit (AU) labels. Therefore, this study focuses on (1) FACS-labeled,\ncustomizable avatar-based expression stimuli for maintaining subjects'\nengagement, (2) learning-based measurements that quantify subjects' facial\nresponses to such stimuli, and (3) validation of constructs represented by\nstimulus-measurement pairs. We propose Customizable Avatars with Dynamic Facial\nAction Coded Expressions (CADyFACE) labeled with AUs by a certified FACS\nexpert. To measure subjects' AUs in response to CADyFACE, we propose a novel\nBeta-guided Correlation and Multi-task Expression learning neural network\n(BeCoME-Net) for multi-label AU detection. The beta-guided correlation loss\nencourages feature correlation with AUs while discouraging correlation with\nsubject identities for improved generalization. We train BeCoME-Net for\nunilateral and bilateral AU detection and compare with state-of-the-art\napproaches. To assess construct validity of CADyFACE and BeCoME-Net, twenty\nhealthy adult volunteers complete expression recognition and mimicry tasks in\nan online feasibility study while webcam-based eye-tracking and video are\ncollected. We test validity of multiple constructs, including face preference\nduring recognition and AUs during mimicry.",
        "updated": "2024-03-12 05:00:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07314v1"
    }
]