[
    {
        "title": "TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation",
        "authors": "Shivin DassWensi AiYuqian JiangSamik SinghJiaheng HuRuohan ZhangPeter StoneBen AbbatematteoRoberto Martin-Martin",
        "links": "http://arxiv.org/abs/2403.07869v1",
        "entry_id": "http://arxiv.org/abs/2403.07869v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07869v1",
        "summary": "A critical bottleneck limiting imitation learning in robotics is the lack of\ndata. This problem is more severe in mobile manipulation, where collecting\ndemonstrations is harder than in stationary manipulation due to the lack of\navailable and easy-to-use teleoperation interfaces. In this work, we\ndemonstrate TeleMoMa, a general and modular interface for whole-body\nteleoperation of mobile manipulators. TeleMoMa unifies multiple human\ninterfaces including RGB and depth cameras, virtual reality controllers,\nkeyboard, joysticks, etc., and any combination thereof. In its more accessible\nversion, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering\nthe entry bar for humans to provide mobile manipulation demonstrations. We\ndemonstrate the versatility of TeleMoMa by teleoperating several existing\nmobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation and\nthe real world. We demonstrate the quality of the demonstrations collected with\nTeleMoMa by training imitation learning policies for mobile manipulation tasks\ninvolving synchronized whole-body motion. Finally, we also show that TeleMoMa's\nteleoperation channel enables teleoperation on site, looking at the robot, or\nremote, sending commands and observations through a computer network, and\nperform user studies to evaluate how easy it is for novice users to learn to\ncollect demonstrations with different combinations of human interfaces enabled\nby our system. We hope TeleMoMa becomes a helpful tool for the community\nenabling researchers to collect whole-body mobile manipulation demonstrations.\nFor more information and video results,\nhttps://robin-lab.cs.utexas.edu/telemoma-web.",
        "updated": "2024-03-12 17:58:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07869v1"
    },
    {
        "title": "Exploring Safety Generalization Challenges of Large Language Models via Code",
        "authors": "Qibing RenChang GaoJing ShaoJunchi YanXin TanWai LamLizhuang Ma",
        "links": "http://arxiv.org/abs/2403.07865v1",
        "entry_id": "http://arxiv.org/abs/2403.07865v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07865v1",
        "summary": "The rapid advancement of Large Language Models (LLMs) has brought about\nremarkable capabilities in natural language processing but also raised concerns\nabout their potential misuse. While strategies like supervised fine-tuning and\nreinforcement learning from human feedback have enhanced their safety, these\nmethods primarily focus on natural languages, which may not generalize to other\ndomains. This paper introduces CodeAttack, a framework that transforms natural\nlanguage inputs into code inputs, presenting a novel environment for testing\nthe safety generalization of LLMs. Our comprehensive studies on\nstate-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a\ncommon safety vulnerability of these models against code input: CodeAttack\nconsistently bypasses the safety guardrails of all models more than 80\\% of the\ntime. Furthermore, we find that a larger distribution gap between CodeAttack\nand natural language leads to weaker safety generalization, such as encoding\nnatural language input with data structures or using less popular programming\nlanguages. These findings highlight new safety risks in the code domain and the\nneed for more robust safety alignment algorithms to match the code capabilities\nof LLMs.",
        "updated": "2024-03-12 17:55:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07865v1"
    },
    {
        "title": "MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric",
        "authors": "Haokun LinHaoli BaiZhili LiuLu HouMuyi SunLinqi SongYing WeiZhenan Sun",
        "links": "http://arxiv.org/abs/2403.07839v1",
        "entry_id": "http://arxiv.org/abs/2403.07839v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07839v1",
        "summary": "Vision-language pre-trained models have achieved impressive performance on\nvarious downstream tasks. However, their large model sizes hinder their\nutilization on platforms with limited computational resources. We find that\ndirectly using smaller pre-trained models and applying magnitude-based pruning\non CLIP models leads to inflexibility and inferior performance. Recent efforts\nfor VLP compression either adopt uni-modal compression metrics resulting in\nlimited performance or involve costly mask-search processes with learnable\nmasks. In this paper, we first propose the Module-wise Pruning Error (MoPE)\nmetric, accurately assessing CLIP module importance by performance decline on\ncross-modal tasks. Using the MoPE metric, we introduce a unified pruning\nframework applicable to both pre-training and task-specific fine-tuning\ncompression stages. For pre-training, MoPE-CLIP effectively leverages knowledge\nfrom the teacher model, significantly reducing pre-training costs while\nmaintaining strong zero-shot capabilities. For fine-tuning, consecutive pruning\nfrom width to depth yields highly competitive task-specific models. Extensive\nexperiments in two stages demonstrate the effectiveness of the MoPE metric, and\nMoPE-CLIP outperforms previous state-of-the-art VLP compression methods.",
        "updated": "2024-03-12 17:24:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07839v1"
    },
    {
        "title": "Label Dropout: Improved Deep Learning Echocardiography Segmentation Using Multiple Datasets With Domain Shift and Partial Labelling",
        "authors": "Iman IslamEsther Puyol-AntónBram RuijsinkAndrew J. ReaderAndrew P. King",
        "links": "http://arxiv.org/abs/2403.07818v1",
        "entry_id": "http://arxiv.org/abs/2403.07818v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07818v1",
        "summary": "Echocardiography (echo) is the first imaging modality used when assessing\ncardiac function. The measurement of functional biomarkers from echo relies\nupon the segmentation of cardiac structures and deep learning models have been\nproposed to automate the segmentation process. However, in order to translate\nthese tools to widespread clinical use it is important that the segmentation\nmodels are robust to a wide variety of images (e.g. acquired from different\nscanners, by operators with different levels of expertise etc.). To achieve\nthis level of robustness it is necessary that the models are trained with\nmultiple diverse datasets. A significant challenge faced when training with\nmultiple diverse datasets is the variation in label presence, i.e. the combined\ndata are often partially-labelled. Adaptations of the cross entropy loss\nfunction have been proposed to deal with partially labelled data. In this paper\nwe show that training naively with such a loss function and multiple diverse\ndatasets can lead to a form of shortcut learning, where the model associates\nlabel presence with domain characteristics, leading to a drop in performance.\nTo address this problem, we propose a novel label dropout scheme to break the\nlink between domain characteristics and the presence or absence of labels. We\ndemonstrate that label dropout improves echo segmentation Dice score by 62% and\n25% on two cardiac structures when training using multiple diverse partially\nlabelled datasets.",
        "updated": "2024-03-12 16:57:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07818v1"
    },
    {
        "title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM",
        "authors": "Sainbayar SukhbaatarOlga GolovnevaVasu SharmaHu XuXi Victoria LinBaptiste RozièreJacob KahnDaniel LiWen-tau YihJason WestonXian Li",
        "links": "http://arxiv.org/abs/2403.07816v1",
        "entry_id": "http://arxiv.org/abs/2403.07816v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07816v1",
        "summary": "We investigate efficient methods for training Large Language Models (LLMs) to\npossess capabilities in multiple specialized domains, such as coding, math\nreasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts\nfrom a seed model, which is branched to train experts in embarrassingly\nparallel fashion with high throughput and reduced communication cost. After\nindividual experts are asynchronously trained, BTX brings together their\nfeedforward parameters as experts in Mixture-of-Expert (MoE) layers and\naverages the remaining parameters, followed by an MoE-finetuning stage to learn\ntoken-level routing. BTX generalizes two special cases, the Branch-Train-Merge\nmethod, which does not have the MoE finetuning stage to learn routing, and\nsparse upcycling, which omits the stage of training experts asynchronously.\nCompared to alternative approaches, BTX achieves the best accuracy-efficiency\ntradeoff.",
        "updated": "2024-03-12 16:54:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07816v1"
    }
]