[
    {
        "title": "Beyond Text: Frozen Large Language Models in Visual Signal Comprehension",
        "authors": "Lei ZhuFangyun WeiYanye Lu",
        "links": "http://arxiv.org/abs/2403.07874v1",
        "entry_id": "http://arxiv.org/abs/2403.07874v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07874v1",
        "summary": "In this work, we investigate the potential of a large language model (LLM) to\ndirectly comprehend visual signals without the necessity of fine-tuning on\nmulti-modal datasets. The foundational concept of our method views an image as\na linguistic entity, and translates it to a set of discrete words derived from\nthe LLM's vocabulary. To achieve this, we present the Vision-to-Language\nTokenizer, abbreviated as V2T Tokenizer, which transforms an image into a\n``foreign language'' with the combined aid of an encoder-decoder, the LLM\nvocabulary, and a CLIP model. With this innovative image encoding, the LLM\ngains the ability not only for visual comprehension but also for image\ndenoising and restoration in an auto-regressive fashion-crucially, without any\nfine-tuning. We undertake rigorous experiments to validate our method,\nencompassing understanding tasks like image recognition, image captioning, and\nvisual question answering, as well as image denoising tasks like inpainting,\noutpainting, deblurring, and shift restoration. Code and models are available\nat https://github.com/zh460045050/V2L-Tokenizer.",
        "updated": "2024-03-12 17:59:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07874v1"
    },
    {
        "title": "Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation",
        "authors": "Shihao ZhaoShaozhe HaoBojia ZiHuaizhe XuKwan-Yee K. Wong",
        "links": "http://arxiv.org/abs/2403.07860v1",
        "entry_id": "http://arxiv.org/abs/2403.07860v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07860v1",
        "summary": "Text-to-image generation has made significant advancements with the\nintroduction of text-to-image diffusion models. These models typically consist\nof a language model that interprets user prompts and a vision model that\ngenerates corresponding images. As language and vision models continue to\nprogress in their respective domains, there is a great potential in exploring\nthe replacement of components in text-to-image diffusion models with more\nadvanced counterparts. A broader research objective would therefore be to\ninvestigate the integration of any two unrelated language and generative vision\nmodels for text-to-image generation. In this paper, we explore this objective\nand propose LaVi-Bridge, a pipeline that enables the integration of diverse\npre-trained language models and generative vision models for text-to-image\ngeneration. By leveraging LoRA and adapters, LaVi-Bridge offers a flexible and\nplug-and-play approach without requiring modifications to the original weights\nof the language and vision models. Our pipeline is compatible with various\nlanguage models and generative vision models, accommodating different\nstructures. Within this framework, we demonstrate that incorporating superior\nmodules, such as more advanced language models or generative vision models,\nresults in notable improvements in capabilities like text alignment or image\nquality. Extensive evaluations have been conducted to verify the effectiveness\nof LaVi-Bridge. Code is available at\nhttps://github.com/ShihaoZhaoZSH/LaVi-Bridge.",
        "updated": "2024-03-12 17:50:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07860v1"
    },
    {
        "title": "Distilling the Knowledge in Data Pruning",
        "authors": "Emanuel Ben-BaruchAdam BotachIgor KviatkovskyManoj AggarwalGérard Medioni",
        "links": "http://arxiv.org/abs/2403.07854v1",
        "entry_id": "http://arxiv.org/abs/2403.07854v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07854v1",
        "summary": "With the increasing size of datasets used for training neural networks, data\npruning becomes an attractive field of research. However, most current data\npruning algorithms are limited in their ability to preserve accuracy compared\nto models trained on the full data, especially in high pruning regimes. In this\npaper we explore the application of data pruning while incorporating knowledge\ndistillation (KD) when training on a pruned subset. That is, rather than\nrelying solely on ground-truth labels, we also use the soft predictions from a\nteacher network pre-trained on the complete data. By integrating KD into\ntraining, we demonstrate significant improvement across datasets, pruning\nmethods, and on all pruning fractions. We first establish a theoretical\nmotivation for employing self-distillation to improve training on pruned data.\nThen, we empirically make a compelling and highly practical observation: using\nKD, simple random pruning is comparable or superior to sophisticated pruning\nmethods across all pruning regimes. On ImageNet for example, we achieve\nsuperior accuracy despite training on a random subset of only 50% of the data.\nAdditionally, we demonstrate a crucial connection between the pruning factor\nand the optimal knowledge distillation weight. This helps mitigate the impact\nof samples with noisy labels and low-quality images retained by typical pruning\nalgorithms. Finally, we make an intriguing observation: when using lower\npruning fractions, larger teachers lead to accuracy degradation, while\nsurprisingly, employing teachers with a smaller capacity than the student's may\nimprove results. Our code will be made available.",
        "updated": "2024-03-12 17:44:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07854v1"
    },
    {
        "title": "12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning",
        "authors": "Yoga Esa WibowoCristian CioflanThorir Mar IngolfssonMichael HerscheLeo ZhaoAbbas RahimiLuca Benini",
        "links": "http://arxiv.org/abs/2403.07851v1",
        "entry_id": "http://arxiv.org/abs/2403.07851v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07851v1",
        "summary": "Few-Shot Class-Incremental Learning (FSCIL) enables machine learning systems\nto expand their inference capabilities to new classes using only a few labeled\nexamples, without forgetting the previously learned classes. Classical\nbackpropagation-based learning and its variants are often unsuitable for\nbattery-powered, memory-constrained systems at the extreme edge. In this work,\nwe introduce Online Few-Shot Class-Incremental Learning (O-FSCIL), based on a\nlightweight model consisting of a pretrained and metalearned feature extractor\nand an expandable explicit memory storing the class prototypes. The\narchitecture is pretrained with a novel feature orthogonality regularization\nand metalearned with a multi-margin loss. For learning a new class, our\napproach extends the explicit memory with novel class prototypes, while the\nremaining architecture is kept frozen. This allows learning previously unseen\nclasses based on only a few examples with one single pass (hence online).\nO-FSCIL obtains an average accuracy of 68.62% on the FSCIL CIFAR100 benchmark,\nachieving state-of-the-art results. Tailored for ultra-low-power platforms, we\nimplement O-FSCIL on the 60 mW GAP9 microcontroller, demonstrating online\nlearning capabilities within just 12 mJ per new class.",
        "updated": "2024-03-12 17:43:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07851v1"
    },
    {
        "title": "MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric",
        "authors": "Haokun LinHaoli BaiZhili LiuLu HouMuyi SunLinqi SongYing WeiZhenan Sun",
        "links": "http://arxiv.org/abs/2403.07839v1",
        "entry_id": "http://arxiv.org/abs/2403.07839v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07839v1",
        "summary": "Vision-language pre-trained models have achieved impressive performance on\nvarious downstream tasks. However, their large model sizes hinder their\nutilization on platforms with limited computational resources. We find that\ndirectly using smaller pre-trained models and applying magnitude-based pruning\non CLIP models leads to inflexibility and inferior performance. Recent efforts\nfor VLP compression either adopt uni-modal compression metrics resulting in\nlimited performance or involve costly mask-search processes with learnable\nmasks. In this paper, we first propose the Module-wise Pruning Error (MoPE)\nmetric, accurately assessing CLIP module importance by performance decline on\ncross-modal tasks. Using the MoPE metric, we introduce a unified pruning\nframework applicable to both pre-training and task-specific fine-tuning\ncompression stages. For pre-training, MoPE-CLIP effectively leverages knowledge\nfrom the teacher model, significantly reducing pre-training costs while\nmaintaining strong zero-shot capabilities. For fine-tuning, consecutive pruning\nfrom width to depth yields highly competitive task-specific models. Extensive\nexperiments in two stages demonstrate the effectiveness of the MoPE metric, and\nMoPE-CLIP outperforms previous state-of-the-art VLP compression methods.",
        "updated": "2024-03-12 17:24:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07839v1"
    }
]