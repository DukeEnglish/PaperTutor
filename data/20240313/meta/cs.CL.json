[
    {
        "title": "Rethinking Generative Large Language Model Evaluation for Semantic Comprehension",
        "authors": "Fangyun WeiXi ChenLin Luo",
        "links": "http://arxiv.org/abs/2403.07872v1",
        "entry_id": "http://arxiv.org/abs/2403.07872v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07872v1",
        "summary": "Despite their sophisticated capabilities, large language models (LLMs)\nencounter a major hurdle in effective assessment. This paper first revisits the\nprevalent evaluation method-multiple choice question answering (MCQA), which\nallows for straightforward accuracy measurement. Through a comprehensive\nevaluation of 24 models across 11 benchmarks, we highlight several potential\ndrawbacks of MCQA, for instance, the inconsistency between the MCQA evaluation\nand the generation of open-ended responses in practical scenarios. In response,\nwe introduce an RWQ-Elo rating system, engaging 24 LLMs such as GPT-4, GPT-3.5,\nGoogle-Gemini-Pro and LLaMA-1/-2, in a two-player competitive format, with\nGPT-4 serving as the judge. Each LLM receives an Elo rating thereafter. This\nsystem is designed to mirror real-world usage, and for this purpose, we have\ncompiled a new benchmark called ``Real-world questions'' (RWQ), comprising\n20,772 authentic user inquiries. Additionally, we thoroughly analyze the\ncharacteristics of our system and compare it with prior leaderboards like\nAlpacaEval and MT-Bench. Our analysis reveals the stability of our RWQ-Elo\nsystem, the feasibility of registering new models, and its potential to reshape\nLLM leaderboards.",
        "updated": "2024-03-12 17:59:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07872v1"
    },
    {
        "title": "Exploring Safety Generalization Challenges of Large Language Models via Code",
        "authors": "Qibing RenChang GaoJing ShaoJunchi YanXin TanWai LamLizhuang Ma",
        "links": "http://arxiv.org/abs/2403.07865v1",
        "entry_id": "http://arxiv.org/abs/2403.07865v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07865v1",
        "summary": "The rapid advancement of Large Language Models (LLMs) has brought about\nremarkable capabilities in natural language processing but also raised concerns\nabout their potential misuse. While strategies like supervised fine-tuning and\nreinforcement learning from human feedback have enhanced their safety, these\nmethods primarily focus on natural languages, which may not generalize to other\ndomains. This paper introduces CodeAttack, a framework that transforms natural\nlanguage inputs into code inputs, presenting a novel environment for testing\nthe safety generalization of LLMs. Our comprehensive studies on\nstate-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a\ncommon safety vulnerability of these models against code input: CodeAttack\nconsistently bypasses the safety guardrails of all models more than 80\\% of the\ntime. Furthermore, we find that a larger distribution gap between CodeAttack\nand natural language leads to weaker safety generalization, such as encoding\nnatural language input with data structures or using less popular programming\nlanguages. These findings highlight new safety risks in the code domain and the\nneed for more robust safety alignment algorithms to match the code capabilities\nof LLMs.",
        "updated": "2024-03-12 17:55:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07865v1"
    },
    {
        "title": "The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage Brought By Model Editing",
        "authors": "Jianchen WangZhouhong GuZhuozhi XiongHongwei FengYanghua Xiao",
        "links": "http://arxiv.org/abs/2403.07825v1",
        "entry_id": "http://arxiv.org/abs/2403.07825v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07825v1",
        "summary": "Large Language Models have revolutionized numerous tasks with their\nremarkable efficacy.However, the editing of these models, crucial for\nrectifying outdated or erroneous information, often leads to a complex issue\nknown as the ripple effect in the hidden space. This effect, while difficult to\ndetect, can significantly impede the efficacy of model editing tasks and\ndeteriorate model performance.This paper addresses this scientific challenge by\nproposing a novel evaluation methodology, Graphical Outlier Relation based\nAssessment(GORA), which quantitatively evaluates the adaptations of the model\nand the subsequent impact of editing. Furthermore, we introduce the Selective\nOutlier Re-Editing Approach(SORA), a model editing method designed to mitigate\nthis ripple effect. Our comprehensive evaluations reveal that the ripple effect\nin the hidden space is a significant issue in all current model editing\nmethods. However, our proposed methods, GORA and SORA, effectively identify and\nalleviate this issue, respectively, contributing to the advancement of LLM\nediting techniques.",
        "updated": "2024-03-12 17:04:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07825v1"
    },
    {
        "title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM",
        "authors": "Sainbayar SukhbaatarOlga GolovnevaVasu SharmaHu XuXi Victoria LinBaptiste RozièreJacob KahnDaniel LiWen-tau YihJason WestonXian Li",
        "links": "http://arxiv.org/abs/2403.07816v1",
        "entry_id": "http://arxiv.org/abs/2403.07816v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07816v1",
        "summary": "We investigate efficient methods for training Large Language Models (LLMs) to\npossess capabilities in multiple specialized domains, such as coding, math\nreasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts\nfrom a seed model, which is branched to train experts in embarrassingly\nparallel fashion with high throughput and reduced communication cost. After\nindividual experts are asynchronously trained, BTX brings together their\nfeedforward parameters as experts in Mixture-of-Expert (MoE) layers and\naverages the remaining parameters, followed by an MoE-finetuning stage to learn\ntoken-level routing. BTX generalizes two special cases, the Branch-Train-Merge\nmethod, which does not have the MoE finetuning stage to learn routing, and\nsparse upcycling, which omits the stage of training experts asynchronously.\nCompared to alternative approaches, BTX achieves the best accuracy-efficiency\ntradeoff.",
        "updated": "2024-03-12 16:54:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07816v1"
    },
    {
        "title": "pyvene: A Library for Understanding and Improving PyTorch Models via Interventions",
        "authors": "Zhengxuan WuAtticus GeigerAryaman AroraJing HuangZheng WangNoah D. GoodmanChristopher D. ManningChristopher Potts",
        "links": "http://arxiv.org/abs/2403.07809v1",
        "entry_id": "http://arxiv.org/abs/2403.07809v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07809v1",
        "summary": "Interventions on model-internal states are fundamental operations in many\nareas of AI, including model editing, steering, robustness, and\ninterpretability. To facilitate such research, we introduce $\\textbf{pyvene}$,\nan open-source Python library that supports customizable interventions on a\nrange of different PyTorch modules. $\\textbf{pyvene}$ supports complex\nintervention schemes with an intuitive configuration format, and its\ninterventions can be static or include trainable parameters. We show how\n$\\textbf{pyvene}$ provides a unified and extensible framework for performing\ninterventions on neural models and sharing the intervened upon models with\nothers. We illustrate the power of the library via interpretability analyses\nusing causal abstraction and knowledge localization. We publish our library\nthrough Python Package Index (PyPI) and provide code, documentation, and\ntutorials at https://github.com/stanfordnlp/pyvene.",
        "updated": "2024-03-12 16:46:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07809v1"
    }
]