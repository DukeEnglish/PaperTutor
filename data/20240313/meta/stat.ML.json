[
    {
        "title": "Low coordinate degree algorithms I: Universality of computational thresholds for hypothesis testing",
        "authors": "Dmitriy Kunisky",
        "links": "http://arxiv.org/abs/2403.07862v1",
        "entry_id": "http://arxiv.org/abs/2403.07862v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07862v1",
        "summary": "We study when low coordinate degree functions (LCDF) -- linear combinations\nof functions depending on small subsets of entries of a vector -- can\nhypothesis test between high-dimensional probability measures. These functions\nare a generalization, proposed in Hopkins' 2018 thesis but seldom studied\nsince, of low degree polynomials (LDP), a class widely used in recent\nliterature as a proxy for all efficient algorithms for tasks in statistics and\noptimization. Instead of the orthogonal polynomial decompositions used in LDP\ncalculations, our analysis of LCDF is based on the Efron-Stein or ANOVA\ndecomposition, making it much more broadly applicable. By way of illustration,\nwe prove channel universality for the success of LCDF in testing for the\npresence of sufficiently \"dilute\" random signals through noisy channels: the\nefficacy of LCDF depends on the channel only through the scalar Fisher\ninformation for a class of channels including nearly arbitrary additive i.i.d.\nnoise and nearly arbitrary exponential families. As applications, we extend\nlower bounds against LDP for spiked matrix and tensor models under additive\nGaussian noise to lower bounds against LCDF under general noisy channels. We\nalso give a simple and unified treatment of the effect of censoring models by\nerasing observations at random and of quantizing models by taking the sign of\nthe observations. These results are the first computational lower bounds\nagainst any large class of algorithms for all of these models when the channel\nis not one of a few special cases, and thereby give the first substantial\nevidence for the universality of several statistical-to-computational gaps.",
        "updated": "2024-03-12 17:52:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07862v1"
    },
    {
        "title": "FairRR: Pre-Processing for Group Fairness through Randomized Response",
        "authors": "Xianli ZengJoshua WardGuang Cheng",
        "links": "http://arxiv.org/abs/2403.07780v1",
        "entry_id": "http://arxiv.org/abs/2403.07780v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07780v1",
        "summary": "The increasing usage of machine learning models in consequential\ndecision-making processes has spurred research into the fairness of these\nsystems. While significant work has been done to study group fairness in the\nin-processing and post-processing setting, there has been little that\ntheoretically connects these results to the pre-processing domain. This paper\nproposes that achieving group fairness in downstream models can be formulated\nas finding the optimal design matrix in which to modify a response variable in\na Randomized Response framework. We show that measures of group fairness can be\ndirectly controlled for with optimal model utility, proposing a pre-processing\nalgorithm called FairRR that yields excellent downstream model utility and\nfairness.",
        "updated": "2024-03-12 16:08:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07780v1"
    },
    {
        "title": "Probabilistic Easy Variational Causal Effect",
        "authors": "Usef FaghihiAmir Saki",
        "links": "http://arxiv.org/abs/2403.07745v1",
        "entry_id": "http://arxiv.org/abs/2403.07745v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07745v1",
        "summary": "Let $X$ and $Z$ be random vectors, and $Y=g(X,Z)$. In this paper, on the one\nhand, for the case that $X$ and $Z$ are continuous, by using the ideas from the\ntotal variation and the flux of $g$, we develop a point of view in causal\ninference capable of dealing with a broad domain of causal problems. Indeed, we\nfocus on a function, called Probabilistic Easy Variational Causal Effect\n(PEACE), which can measure the direct causal effect of $X$ on $Y$ with respect\nto continuously and interventionally changing the values of $X$ while keeping\nthe value of $Z$ constant. PEACE is a function of $d\\ge 0$, which is a degree\nmanaging the strengths of probability density values $f(x|z)$. On the other\nhand, we generalize the above idea for the discrete case and show its\ncompatibility with the continuous case. Further, we investigate some properties\nof PEACE using measure theoretical concepts. Furthermore, we provide some\nidentifiability criteria and several examples showing the generic capability of\nPEACE. We note that PEACE can deal with the causal problems for which\nmicro-level or just macro-level changes in the value of the input variables are\nimportant. Finally, PEACE is stable under small changes in $\\partial\ng_{in}/\\partial x$ and the joint distribution of $X$ and $Z$, where $g_{in}$ is\nobtained from $g$ by removing all functional relationships defining $X$ and\n$Z$.",
        "updated": "2024-03-12 15:28:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07745v1"
    },
    {
        "title": "The Minimax Rate of HSIC Estimation for Translation-Invariant Kernels",
        "authors": "Florian KalinkeZoltan Szabo",
        "links": "http://arxiv.org/abs/2403.07735v1",
        "entry_id": "http://arxiv.org/abs/2403.07735v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07735v1",
        "summary": "Kernel techniques are among the most influential approaches in data science\nand statistics. Under mild conditions, the reproducing kernel Hilbert space\nassociated to a kernel is capable of encoding the independence of $M\\ge 2$\nrandom variables. Probably the most widespread independence measure relying on\nkernels is the so-called Hilbert-Schmidt independence criterion (HSIC; also\nreferred to as distance covariance in the statistics literature). Despite\nvarious existing HSIC estimators designed since its introduction close to two\ndecades ago, the fundamental question of the rate at which HSIC can be\nestimated is still open. In this work, we prove that the minimax optimal rate\nof HSIC estimation on $\\mathbb R^d$ for Borel measures containing the Gaussians\nwith continuous bounded translation-invariant characteristic kernels is\n$\\mathcal O\\!\\left(n^{-1/2}\\right)$. Specifically, our result implies the\noptimality in the minimax sense of many of the most-frequently used estimators\n(including the U-statistic, the V-statistic, and the Nystr\\\"om-based one) on\n$\\mathbb R^d$.",
        "updated": "2024-03-12 15:13:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07735v1"
    },
    {
        "title": "CAS: A General Algorithm for Online Selective Conformal Prediction with FCR Control",
        "authors": "Yajie BaoYuyang HuoHaojie RenChangliang Zou",
        "links": "http://arxiv.org/abs/2403.07728v1",
        "entry_id": "http://arxiv.org/abs/2403.07728v1",
        "pdf_url": "http://arxiv.org/pdf/2403.07728v1",
        "summary": "We study the problem of post-selection predictive inference in an online\nfashion. To avoid devoting resources to unimportant units, a preliminary\nselection of the current individual before reporting its prediction interval is\ncommon and meaningful in online predictive tasks. Since the online selection\ncauses a temporal multiplicity in the selected prediction intervals, it is\nimportant to control the real-time false coverage-statement rate (FCR) to\nmeasure the averaged miscoverage error. We develop a general framework named\nCAS (Calibration after Adaptive Selection) that can wrap around any prediction\nmodel and online selection rule to output post-selection prediction intervals.\nIf the current individual is selected, we first perform an adaptive selection\non historical data to construct a calibration set, then output a conformal\nprediction interval for the unobserved label. We provide tractable\nconstructions for the calibration set for popular online selection rules. We\nproved that CAS can achieve an exact selection-conditional coverage guarantee\nin the finite-sample and distribution-free regimes. For the decision-driven\nselection rule, including most online multiple-testing procedures, CAS can\nexactly control the real-time FCR below the target level without any\ndistributional assumptions. For the online selection with symmetric thresholds,\nwe establish the error bound for the control gap of FCR under mild\ndistributional assumptions. To account for the distribution shift in online\ndata, we also embed CAS into some recent dynamic conformal prediction methods\nand examine the long-run FCR control. Numerical results on both synthetic and\nreal data corroborate that CAS can effectively control FCR around the target\nlevel and yield more narrowed prediction intervals over existing baselines\nacross various settings.",
        "updated": "2024-03-12 15:07:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.07728v1"
    }
]