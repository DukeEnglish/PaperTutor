Rethinking Generative Large Language Model Evaluation for
Semantic Comprehension
FangyunWei* XiChen* LinLuo*
MicrosoftResearchAsia
*Equalcontribution
{fawe,xichen6,liluo}@microsoft.com
Projectpage: https://luolinrowling.github.io/Rethink-LLM-Eval
Abstract reliability,fairness,harmlessness,andethicaluse.
Thispaperaimstocontributetotheongoingdiscourseinthe
Despitetheirsophisticatedcapabilities,largelan-
fieldofnaturallanguageprocessingbyprovidingacompre-
guagemodels(LLMs)encounteramajorhurdle
hensiveevaluationofLLMs. Priorresearchhasprimarily
ineffectiveassessment. Thispaperfirstrevisits
focusedonevaluatingarangeofcapabilitiesincludingbroad
theprevalentevaluationmethod—multiplechoice
worldknowledge,commonsensereasoning,andspecialized
question answering (MCQA), which allows for
skillslikecodingandmathematics. Thishasledtothein-
straightforwardaccuracymeasurement. Through
troductionofvariousbenchmarks(e.g. MMLU(Hendrycks
acomprehensiveevaluationof24modelsacross
etal.,2020),AGIEval(Zhongetal.,2023),andARC(Clark
11 benchmarks, we highlight several potential
etal.,2018))andassessmentplatforms(e.g.,HELM(van
drawbacks of MCQA, for instance, the incon-
Gemert-Pijnenetal.,2011)andHarness(Gaoetal.,2021)).
sistencybetweentheMCQAevaluationandthe
Whileevaluatingspecificskillslikecoding,whichcanbe
generationofopen-endedresponsesinpractical
assessed through targeted coding tasks and test cases us-
scenarios.Inresponse,weintroduceanRWQ-Elo
ing pass rate as a metric (Chen et al., 2021), and mathe-
ratingsystem,engaging24LLMssuchasGPT-4,
matics,whereuniquesolutionsenableaccuracy-basedmet-
GPT-3.5,Google-Gemini-ProandLLaMA-1/-2,
rics(Cobbeetal.,2021),isrelativelyclear-cut,assessing
inatwo-playercompetitiveformat,withGPT-4
open-endedareassuchassemanticcomprehension,remains
servingasthejudge. EachLLMreceivesanElo
challenging. Thiscomplexitystemsfromtwoprimaryfac-
ratingthereafter. Thissystemisdesignedtomir-
tors: 1) the open-ended nature of LLM responses leads
ror real-world usage, and for this purpose, we
to a broad spectrum of possible answers; and 2) the non-
have compiled a new benchmark called “Real-
uniqueness of what constitutes a reasonable answer to a
worldquestions”(RWQ),comprising20,772au-
givenquestion.
thentic user inquiries. Additionally, we thor-
oughlyanalyzethecharacteristicsofoursystem TheevaluationofLLMshaspredominantlycenteredaround
and compare it with prior leaderboards like Al- multiple choice question answering (MCQA) due to its
pacaEval and MT-Bench. Our analysis reveals straightforwardapproachinmeasuringLLMperformance
thestabilityofourRWQ-Elosystem,thefeasibil- viaaccuracyanditsfacilitationofcomparisonswithother
ityofregisteringnewmodels,anditspotentialto LLMs. Thispaperdelvesintotheinherentshortcomingsof
reshapeLLMleaderboards. theMCQAevaluation. Initially,wehighlightthediscrep-
ancybetweenthereal-worldusageofLLMsforresponding
touserqueriesinanopen-endedmanner,andtheconfined
1.Introduction nature of selecting the best option in MCQA. Moreover,
themethodologyforgeneratingMCQApredictionsdiffers
In recent years, the advent of large language models
amongmodels. Thiscaninvolveeitherpinpointingthelike-
(LLMs) (Radford et al., 2018; 2019; Brown et al., 2020;
liesttoken(e.g.,“A”or“B”)uponreviewingthemainques-
Touvron et al., 2023b) has revolutionized the field of ar-
tionanditsassociatedchoices,orcalculatingthealignment
tificialintelligence,offeringunprecedentedcapabilitiesin
betweenthequestionandeachoptionusingperplexity.Such
naturallanguageprocessingandunderstanding. However,
variationsresultininconsistentevaluationsacrossdifferent
the rapid development of LLMs also present significant
LLMs. Additionally,theopen-endedresponsesprovidedby
challenges,particularlyintermsofevaluation. Theeffec-
LLMsmaynotcorrespondwiththeirMCQApredictions,
tive assessment of these models is crucial to ensure their
1
4202
raM
21
]LC.sc[
1v27870.3042:viXraRethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
leading to unreliable evaluations. This paper comprehen- ChatGPT (Achiam et al., 2023) has garnered significant
sively examines these potential disadvantages through a attention, exemplifying this progress. These LLMs ex-
combinationofquantitativeandqualitativestudies. plorevariousarchitectures,suchascausaldecoders(forin-
stance,GPT(Radfordetal.,2018;2019;Brownetal.,2020),
Inpracticalsettings,LLMsperformtaskssuchasfollowing
LLaMA(Touvronetal.,2023a;b),OPT(Zhangetal.,2022),
user instructions, responding to inquiries, and providing
BLOOM(Workshopetal.,2022),LaMDA(Thoppilanetal.,
answers to questions. We advocate for the use of LLMs
2022)),encoder-decoderframeworks(likeT5(Raffeletal.,
togenerateopen-endedresponsesandrecommenddirectly
2020)andFlan-T5(Chungetal.,2022)),andmixture-of-
assessingtheseresponses.Previousworks,suchasAlpacaE-
expertsmodels(suchasSwitchTransformer(Fedusetal.,
val(Lietal.,2023a),compareanLLM’sresponsetothat
2022)andGLaM(Duetal.,2022)),alongwithinnovative
ofapre-definedbenchmarkmodeltothesamequery,and
structures (for example, RWKV (Peng et al., 2023) and
thencalculateawin-rate,whichservesasthecomparison
RetNet(Sunetal.,2023)). Toreconcilethegapbetween
metricacrossvariousLLMs. Nevertheless,whenthisbench-
pre-trainingobjectivesanduser-directedgoals,notably“fol-
mark model is significantly superior (e.g. GPT-4-Turbo)
lowtheirinstructionshelpfullyandsafely”(Radfordetal.,
or inferior, AlpacaEval may not distinguish performance
2019),instructiontuningtechniques(Ouyangetal.,2022)
differencesamongtheLLMsbeingevaluated(seeFigure5).
areintroduced. Instructiontuningorsupervisedfine-tuning
Furthermore,thebenchmarkmodelservesasan“interme-
in LLaMA, in particular, is key in developing tailored or
diary agent” to compare the relative capabilities between
nichemodels,includingVicuna(Chiangetal.,2023),Stan-
twoLLMs—adirectcomparisonbetweenthemisnotmade.
ford Alpaca (Taori et al., 2023b), WizardLM (Xu et al.,
Ifanoptimizationobjectiveaimedatsurpassingthebench-
2023), and Xwin-LM (Team, 2023). Additional research
markmodelisincorporatedintoanLLM’strainingorfine-
areas encompass scaling LLMs (Hoffmann et al., 2022;
tuningprocess,itmightresultinaninflatedwin-rate,leading
Raeetal.,2021;Chowdheryetal.,2023),managinglong
toanartificiallyhighranking.
contexts (Su et al., 2024; Ding et al., 2023), devising de-
Inthiswork,weevaluatetheperformanceofLLMsthrough codingstrategies(Leviathanetal.,2023;Chenetal.,2023;
a series of two-player contests. Each round involves ran- Lietal.,2023b;2024),innovatingsamplingmethods(Fan
domly selecting two LLMs to respond to a query sam- etal.,2018;Holtzmanetal.,2019),enhancingtrainingeffi-
pledfromour“Real-WorldQuestions”(RWQ)benchmark, ciency(Huangetal.,2019;Shoeybietal.,2019;Rajbhandari
which consists of 20,772 realistic user queries collected etal.,2020;Rasleyetal.,2020;Daoetal.,2022;Huetal.,
fromsourcessuchasGoogleTrendsandQuora. Ajudge 2021), foundational operators (Ba et al., 2016; Shazeer,
then determines the winner and the loser. Each LLM un- 2020),andtrainingdatacollection(Zhuetal.,2015;Raffel
der assessment is assigned a rating that is adjusted after etal.,2020;Liuetal.,2019),amongothers.
eachcompetitionbasedontheresults. WeutilizetheElo
LLMEvaluation. EvaluationofLLMsencompassesavari-
algorithm to update the ratings, naming this system the
etyofdomains,includingunderstandingknowledge(Khot
RWQ-Elosystem. Recently,GPT-4,knownforitssuperior
et al., 2020), aligning responses to questions and instruc-
languagecomprehensionabilities,hasbeenincorporatedas
tions, utilization of tools, safety considerations, and spe-
ajudgeinnumeroustasks(Lietal.,2023a). Ourmeticulous
cializedcompetenciesinareassuchasprogramming(Chen
designofevaluationcriteriaandguidelineshasshownthat
etal.,2021),mathematics(Cobbeetal.,2021;Austinetal.,
GPT-4’sdecisionsalignwiththoseofhumanevaluators95%
2021),andlanguagetranslation(Bojaretal.,2014;2016;
ofthetime. Therefore,GPT-4isemployedasthejudgein
Lison&Tiedemann,2016). AfundamentalskillforLLMs
ourRWQ-Elosystemtoenhancescalability. Byorchestrat-
istheabilitytopossessextensivegeneralknowledgeand
ingthesecontestsamong24LLMssuchasGPT-4,GPT-3.5,
torespondtoquestionsorqueriesbothcorrectlyandlog-
GoogleGemini-ProandLLaMAfamily,wecoulddirectly
ically. To evaluate this skill, numerous benchmarks have
evaluatetheirrelativecapabilities,makingthemmoredis-
beendeveloped,includinghumanexaminationdatasets(e.g.,
tinguishable. Additionally,weanalyzethestabilityofour
e.g. MMLU (Hendrycks et al., 2020), AGIEval (Zhong
RWQ-Elosystem,itsrelationtoexistingLLMleaderboards,
etal.,2023),C-Eval(Huangetal.,2023),andRACE(Lai
and theschema fornewmodel registration. Owing toits
etal.,2017)),datasetsforassessingcommonsensereasoning
simplicityandrobustness,weanticipatethatourapproach
(e.g.,ARC(Clarketal.,2018)andCommonSenseQA(Tal-
couldbecomeanewstandardforLLMevaluation.
mor et al., 2018)), and question-answering datasets (e.g.,
2.RelatedWork PIQA(Bisketal.,2020),OpenBookQA(Mihaylovetal.,
2018)andBoolQ(Clarketal.,2019)). Amongthesebench-
AdvancementsinGenerativeLargeLanguageModels.
marks,multiplechoicequestionanswering(MCQA)ispar-
TheadventofgenerativeLLMsmarksanewerainthede-
ticularly prominent due to the straightforward nature of
velopmentofsophisticatedAIalgorithmsadeptatlanguage
using accuracy as a measurement criterion. In order to
comprehension and task execution. The introduction of
2RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
consolidatedifferentbenchmarksandofferaunifiedinter- ((cid:81)|Si| p(Sj|[S1:j−1,Q]))1/|Ci| is computed for choice
j=1 i i
face, various evaluation platforms have been introduced. determination, where |C | denotes the number of char-
i
These include HELM (van Gemert-Pijnen et al., 2011), acters of S . This method is a variant of the previous
i
Harness (Gao et al., 2021), OpenCompass (Contributors, strategywiththeonlydifferencebeinginthenormaliza-
2023) and Big-bench (bench authors, 2023). Diverging tionofprobabilities.
fromMCQA,AlpacaEval(Lietal.,2023a)proposesadif- • Perplexity. Perplexity is defined as PPL(S |Q) =
i
ferent approach: having LLMs compete against stronger exp(−1/|S |(cid:80)|Si| log(p(Sj|[S1:j−1,Q]))). The opti-
counterparts, namely Text-Davinci-003 and GPT-4, in a i j=1 i i
malchoiceistheonewiththelowestperplexityscore.
question-answering task, with GPT-4 serving as an adju-
Benchmarks,ModelsandResultsUsingDiverseEvalua-
dicatorowingtoitsadvancedcapabilities. Theevaluation
tionStrategies. Were-evaluatethecapabilitiesof24LLMs
metricadoptedisthewin-rateoftheseLLMsagainstText-
through an analysis of their performance on 11 distinct
Davinci-003orGPT-4. IncontrasttoAlpacaEval, which
benchmarksincludingMMLU,HellaSwag,ARC-Challenge,
advocatesforanLLMtocompetewithasinglemodel,this
ARC-Easy,BoolQ,SIQA,PIQA,AGIEval(Englishonly),
workpresentsamultiple-playerEloratingsystem.
OpenBookQA (with fact), CommonSenseQA and RACE
3.RethinkingMCQAEvaluation (all). Theseevaluationsareconductedusingthe5aforemen-
tionedstrategies(denotedas“Choices”,“Vocab”,“Align-
Formulation and Notations. Multiple choice question
ment”,“NormalizedAlignment”and“PPL”,respectively).
answering(MCQA)hasemergedasthedominantevaluation
WereportaveragedresultsacrossthesebenchmarksinTa-
task,favoredforitsconvenienceinquantitativeassessment.
ble 1. Detailed results for each benchmark can be found
Generally,amultiple-choicequestionconsistsofaquestion
fromTable7-17intheappendix.
Q, K choices {C }K , and a reference answer choice A.
i i=1 KeyFindingsforMCQAEvaluation.
Each choice C (e.g. “A. 0 degrees Celsius.”) comprises
i
a choice number (e.g. “A”) and a statement S (e.g. “0 1. Variousmodelsdemonstrateasignificantpreferencefor
i
degreesCelsius.”). AnLLMissupposedtopredictthemost specificMCQAevaluationstrategies. Toaccuratelyiden-
accurateanswerchoiceuponencounteringQand{C }K . tifythebestanswer,anLLMnecessitatestwokeyskills:
i i=1
IfthepredictionmatchesA,thecorrespondingquestionis a)comprehensionoftheintentionbehindmultiple-choice
consideredcorrectlyanswered. Consequently,accuracyis questions(e.g. predictingthecorrectchoicenumber);b)
readilyadoptedastheevaluationmetric. Additionally,we extensiveknowledgeofawiderangeoftopics. Forpre-
useV todenotetheentirevocabularyofanLLM,andV trainedLLMsthathavenotundergoneinstructiontuning,
C
to represent the set containing all choice number tokens suchasMPT,theirperformanceisgenerallylesseffec-
(e.g.,{“A”,“B”,“C”,“D”}forMMLUand{“A”,“B”}for tive when evaluated using the “Choices” and “Vocab”
HellaSwag). strategies compared to the “Alignment” and “Normal-
ized Alignment” strategies (Table 1). This trend may
EvaluationStrategies. Theapproachtogeneratingpredic-
beduetoacombinationoffactors,suchasthemodel’s
tionsforMCQAvariessignificantlyacrossdifferentmodels.
capability,theabsenceofinstructiontuningwhichleads
We categorize the evaluation strategies adopted by most
toadisregardforuserinstructions,orboth.
LLMsasfollows:
2. LLMsoftenproducevaryingpredictionswhentheorder
• SelectionoftheMostLikelyTokenfromtheChoiceSet. ofchoicesisaltered. Toinvestigatethis,weemploya
AnLLMprocessestheconcatenationofQ,{C }K ,and circularevaluationmethodforboth“Choices”and“Vo-
i i=1
thephrase“Answer: ”,andthenpredictsthenexttoken, cab” approaches. Specifically, we rearrange the order
whichistheonefromV withthehighestprobability. of choices in a cycle and repeatedly input a multiple-
C
choicequestionalongwiththeserearrangedchoicesinto
• SelectionoftheMostLikelyTokenfromtheEntireVocab-
anLLM.Aquestionisdeemedsuccessfullyanswered
ulary. Thismethodissimilartothepreviousone,butthe
onlyiftheLLMcorrectlyrespondstoeveryvariationof
predictioninvolvesselectingthetokenwiththehighest
thequestion. Theresultsusing“Choices(Circular)”and
probabilityfromtheentirevocabularyV,followingthe
“Vocab(Circular)”asevaluationstrategies,arepresented
processingofQ,{C }K ,andthephrase“Answer: ”.
i i=1 in Table 1. This reveals a notable decline in the per-
• AlignmentofChoicewiththeQuestion. Letp(S |Q) =
i formanceofallLLMs,highlightingtheirinconsistency
(cid:81)|Si| p(Sj|[S1:j−1,Q])denotetheposteriorprobability
ingeneratingpredictionsforthesamequestionwitha
j=1 i i
ofgeneratingstatementS iofchoiceC igiventhequestion differentsequenceofchoices.
Q. TheMCQApredictionisthechoicewiththehighest
3. GenerativeLLMsaretrainedwiththeobjectiveofnext-
probability,i.e.,Argmax (p(S |Q)).
1≤i≤K i token prediction. While the accuracy of the MCQA
• NormalizedAlignmentofChoicewiththeQuestion.Inthis evaluationisstraightforwardtomeasureandfacilitates
approach,thenormalizedposteriorprobabilityp(S |Q)= comparisonswithothermodels,thisapproachdoesnot
i
3RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
Table1: Averagedresultsacross110-shotdatasetsincludingMMLU(Hendrycksetal.,2020),HellaSwag(Zellersetal.,
2019),ARC-Challengeand-Easy(Clarketal.,2018),BoolQ(Clarketal.,2019),SIQA(Sapetal.,2019),PIQA(Bisk
etal.,2020),AGIEval(Englishonly)(Zhongetal.,2023),OpenBookQA(withfact)(Mihaylovetal.,2018),Common-
SenseQA(Talmoretal.,2018)andRACE(all)(Laietal.,2017),using7evaluationstrategiesintroducedinSection3. We
usethelatestmodelsuptoFebruary1,2024. WeusegeneralMCQApromptforallbenchmarkswithoutdedicateddesign.
DetailedresultsforeachbenchmarkcanbefoundinTable7-17.
Choices Vocab Normalized
Model Size Choices Vocab Alignment PPL
(Circular) (Circular) Alignment
MPT 7B 36.0 2.2 35.2 2.0 52.3 54.3 54.2
(Teametal.,2023a;b) 30B 53.0 26.4 49.2 23.0 54.8 57.1 56.8
MPT-Chat
30B 61.5 37.9 60.8 37.0 56.7 58.9 58.3
(Teametal.,2023b)
Falcon 7B 31.7 3.6 30.2 2.9 52.3 54.2 54.7
(Almazroueietal.,2023) 40B 62.3 36.6 62.0 36.4 57.8 59.4 59.8
7B 40.4 8.0 38.7 7.2 52.8 54.7 53.6
LLaMA-1 13B 52.6 20.1 50.2 18.3 54.6 56.1 55.3
(Touvronetal.,2023a) 30B 65.6 45.3 65.4 45.0 57.0 58.7 57.8
65B 67.5 45.2 66.1 43.9 58.3 60.1 59.4
7B 47.5 17.4 42.7 14.1 53.3 55.1 54.4
LLaMA-2
13B 60.8 31.1 58.6 29.7 55.5 57.0 56.4
(Touvronetal.,2023b)
70B 75.2 58.4 74.8 57.9 59.0 60.4 59.8
7B 57.7 28.8 55.8 28.3 54.1 55.8 54.5
LLaMA-2-Chat
13B 65.4 40.9 65.3 40.8 56.0 58.6 57.0
(Touvronetal.,2023b)
70B 74.3 56.8 74.2 56.6 58.9 60.7 59.5
WizardLM 13B 67.6 47.1 67.6 47.1 56.6 58.1 57.4
(Xuetal.,2023) 70B 76.7 61.7 76.6 61.6 59.2 60.3 59.8
Xwin-LM 7B 55.0 25.2 54.8 25.0 55.0 55.9 55.3
(Team,2023) 13B 64.0 34.9 63.9 34.7 57.3 58.6 58.1
Alpaca 7B 52.7 24.4 52.5 24.1 54.4 56.5 55.1
(Taorietal.,2023a) 13B 54.0 30.3 53.6 30.0 55.5 56.8 55.9
7B 62.6 41.1 62.5 41.0 53.8 54.7 54.3
Vicuna
13B 68.8 50.1 68.7 50.1 56.3 57.6 56.6
(Chiangetal.,2023)
33B 69.6 50.2 64.3 45.0 56.4 57.9 57.4
alwaystranslatewelltopracticalapplications. Typically, referencechoiceinMCQA.Thisdiscrepancyarisespri-
usersinteractwithLLMsbyposingdirectquestionsor marilyduetosuboptimalinstructiontuningandflawed
seekingsolutionstospecificproblems,ratherthanpre- designofthemultiple-choiceoptions. Designingthese
sentingmultiple-choicequestions. Ourobservationsin- choices for MCQAcan be subjective and challenging.
dicatethatalthoughsomeLLMscansuccessfullyselect Inadequatechoicedesignmightnotencompassthere-
thecorrectchoiceinMCQAtasks,theirperformancefal- sponsetheLLMisinclinedtoexpress,resultinginques-
terswhentaskedwithdirectlygeneratingresponsestoa tionsremainingunresolved. Anexampleispresentedin
questioninanauto-regressivefashion.Intheseinstances, Table3.
theresponsesgenerateddonotcorrespondaccuratelyto
5. Wenotethatwhentaskedwithrespondingtoaquestion
theposedquestions. Consequently,aquestiondeemed
takenfromanMCQAbenchmarkthroughopen-ended
as resolved in an MCQA setting may, in fact, remain
generation, an LLM may produce a range of choices
unsolvedinafree-formquestion-answeringcontext. Ta-
and then pick one from these self-generated choices.
ble2providesanexampleofsuchdiscrepancies.
Thisbehaviorindicatesapotentialriskofdataleakage
4. Incontrasttothethirdpoint,LLMsmightaccuratelyre- in testing benchmarks or insufficient optimization of
spondtoopen-endedquestionsyetincorrectlychoosethe instructiontuning. InTable4,wepresentanillustrative
4RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
Table2:Theopen-endedanswergeneratedbytheLLM(e.g. Table4: LLMs(e.g.,LLaMA-2-Chat-13B)mayexhibitthe
Alpaca-13B)doesnotdirectlyaddressthequestionposed. behaviorof“choicegenerationbeforeanswering”.
However, it incidentally delivers the correct choice. The
Question: “DuringwhichstepoftheChaseJonesmodel
referencechoiceishighlightedinpurple.
of issues management should the strengths and weak-
Question: “InwhichurbanUSsettingbelowwouldyou nessesoftheorganisationbereviewed?”
expecttofindplannedcommunities?”
Choices:“A.Identification B.Analysis C.Strategy D.
Choices: “A.Ghettos B.Suburbs C.CBDs D.Tran- Evaluation”
sitionzones”
LLM’sChoice: “B”
LLM’sChoice: “B”
LLM’sOpen-endedAnswer: “A)IdentificationB)Anal-
LLM’sOpen-endedAnswer: “sla¨kteturbansprawl. The ysis C) Evaluation D) Resolution Correct answer: B)
answeris: InwhichurbanUSsettingbelowwouldyou Analysis”
expecttofindplannedcommunities? Theansweris: In
whichurbanUSsettingbelowwouldyouexpecttofind
Table5: Weevaluateopen-endedMCQAbyusingLLaMA-
plannedcommunities?”
2-Chat-13BonthefilteredMMLU0-shotbenchmark.
MCQA(Choices) Open-endedMCQA
Table3: TheLLM(e.g. LLaMA-2-Chat-13B)’sopen-ended
answeraddressesthequestion,buttheMCQApredictionis 54.0 39.7
incorrect.
Question: “Whatisthequotientof1,224/9?” We adopt LLaMA-2-Chat-13B to conduct open-ended
MCQAandperformacomparativeanalysiswiththeconven-
Choices: “A.135 B.136 C.1,215 D.1,360”
tionalMCQAapproachesthatemploy“Choices”strategy.A
LLM’sChoice: “A” significantperformancegapisevidentinTable5,whichcon-
firmstheinconsistencybetweenselectingtheopticalchoice
LLM’sOpen-endedAnswer: “Answer: 136”
in MCQA and addressing the question in an open-ended
manner. To verify the effectiveness of using GPT-4 as a
case. judgeinopen-endedMCQA,weconductamanualexamina-
tiontocomparetheassessmentsmadebyGPT-4withthose
Open-endedMCQA.Thediscrepancybetweentraditional
madebyhumanevaluators, onasubsetincluding500in-
MCQA assessments and practical open-ended question-
stancessampledfromMMLU,ARC-Challenge,ARC-Easy,
answering scenarios raises the inquiry: is it possible to
RACE,SIQAandPIQAbenchmarks. Thisstudyreveals
adaptexistingMCQAbenchmarkstosupportopen-ended
a high alignment rate of 80%, indicting the feasibility of
question-answering? Toexplorethis,weintroduceanopen-
usingGPT-4asajudgeforopen-endedMCQAevaluation.
endedMCQAevaluationmethodology. Thisapproach,tak-
ing cuesfrom the recent advancements withGPT-4 func- DiscussionsforMCQAEvaluation. WhileMCQAeval-
tioningasajudge,involvesatwo-stepprocess: 1)posing uationofferstheeaseofquantifyinganLLM’scapability
eachquestionfromanMCQAbenchmarktotheLLMunder withasingleaccuracymetric, itisnotwithoutshortcom-
evaluationtoelicitanopen-endedanswer,and2)employ- ings. Our findings, as presented in Tables 1 and Table 7-
ingGPT-41todeterminethechoicethatmostsemantically 17 in the appendix, along with the observations from Ta-
aligns with the LLM’s response, by comparing it against bles2-5,highlightseveralissuesinherentinMCQAevalua-
the given choices. The prompt for GPT-4’s judgement is tions. Theseincludevariedevaluationstrategies,biasesin
detailedinSectionA.1oftheappendix. multiple-choicedesign,discrepanciesbetweenopen-ended
Ourassessmentoftheopen-endedMCQAisconductedon answersandchoicepredictions,andamismatchbetween
amodifiedversionoftheMMLUbenchmark,whichhasun- evaluationmechanismsandpracticalusagescenarios.While
dergoneseveralfilteringstagestoremovequestionsunsuit- theintroductionofopen-endedMCQAaddressessomeof
ableforopen-endedformats. Specifically,thisfilteringpro- theseconcerns,itdoesnotfullybridgethegapbetweenhow
cesseliminatesquestionsofthreetypes: 1)thosestructured evaluationsareconductedandhowuserstypicallyinteract
as“which...following”,“which...these”,or“which...are”;2) withLLMs—oftenbyposingqueriesthatelicitopen-ended
questionsthatrequirefillinginblanks;3)questionswhere responses. Therefore, an evaluation approach that more
thechoicesincludetermslike“none”,“both”,“neither”or closelymirrorsreal-worldapplicationsisessential.
“allof”. ThisprocessyieldsafilteredMMLUbenchmark
4.RWQ-EloSystemforLLMEvaluation
comprising7,223instances.
In this work, we present the RWQ-Elo system for evalu-
1GPT-4-Turbo-1106-previewisusedthroughoutthepaper. atingLLMs. Originally,theEloratingalgorithmisexten-
5RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
AAllppaaccaaEEvvaall all sampled questions. Ultimately, we calculate an align-
805 (3.9%) mentratebetweenthedecisionsofGPT-4andthehuman
GGooooggllee TTrreennddss evaluator,whichstandsatanimpressive95%,underscoring
QQuuoorraa
5409 (26.0%)
4414 (21.3%) thereliabilityofemployingGPT-4asajudge.
RWQ-Elo Rating Algorithm. In a setup involving N
LLMs,eachLLMbeginswithaninitialEloratingof1000.
During each competition round, we randomly pair two
LLMs (referred to as LLM-A and LLM-B) and present
them with a question sampled from our RWQ database.
BothLLM-AandLLM-Bindependentlyprocesstheques-
SShhaarreeGGPPTT
LLMMSSYYSS--CChhaatt--11MM tion and provide their respective answers. Subsequently,
5027 (24.2%)
5117 (24.6%)
weutilizeGPT-4asajudgetodecidetheoutcome,which
Figure 1: Statistics for our Real-World Question (RWQ) canbeeitherLLM-AdefeatingLLM-B,LLM-Alosingto
benchmark. ExamplesforeachsourceareavailableinTa- LLM-B,oratie. Basedonthisresult, weupdatetheElo
ble22oftheappendix. ratingsofthetwoLLMsinaccordancewiththespecified
updatemechanismdetailedbelow.
sivelyemployedtoassesstherelativeskilllevelsofmultiple
In each competition round, the expected score for either
playersinaparticulargame,typicallyfollowingnumerous
LLM-A or LLM-B, when matched against each other, is
roundsoftwo-playercontests. Thissystemfinallygenerates
calculatedasfollows:
anEloratingforeachplayer,reflectingtheircomparative
1
proficiency. Implementingthissystemposesseveralchal- E = , (1)
lenges: creatingcontestmaterials,establishingvictoryand
A 1+10(RB−RA)/400
defeatcriteria,choosingajudge,anddefiningprinciplesfor E B =1−E A, (2)
ratingstability.
whereE andE symbolizetheexpectedscoresofLLM-A
A B
ContestMaterials. ToensurethattheevaluationofLLMs andLLM-B,respectively;R AandR B aretheircurrentElo
accurately reflects their use in practical scenarios, we as- ratings.
semble a dataset called “Real-World Questions” (RWQ). Subsequently,theupdatedratingiscalculatedusing:
Thisdatasetcomprises20,772authenticquestionssourced
R′ =R +K×(S −E ), (3)
from various platforms such as Google Trends2, Quora, A A A A
ShareGPT,LMSYS-Chat-1M,andAlpacaEval. Thecompo- R B′ =R B +K×(S B −E B), (4)
sitionoftheRWQdatasetisdepictedinFigure1. Weutilize whereR′ andR′ denotetheupdatedEloratingsforLLM-
A B
thisdatasetintheimplementationoftheRWQ-Elosystem.
AandLLM-B;S (orS )issetto1ifLLM-A(orLLM-B)
A B
GPT-4asAJudge. Intherealmoftwo-playergames,the wins,0ifitloses,and0.5intheeventofatie;K represents
optimalpracticeforassessingvictoryordefeatistoinvolve theK-factor,whichissetto4bydefault.
theengagementoflinguisticexpertsasjudges. However,
To ensure a fair competition among the LLMs, we have
thisapproachisoftenprohibitivelycostlyandimpractical.
structuredthecontestsothateachLLMcompeteswithevery
Recently,theuseofGPT-4asanevaluatorhasgainedtrac-
otherLLMexactlyH times. Consideringtheinclusionof
tioninvariousapplications,e.g.,tooluse(Duetal.,2024).
N LLMsintotal,thetotalnumberofcompetitiverounds
OurobservationsalsoindicatethattheintegrationofGPT-4
conductedisN ×(N −1)×H/2. WefindthatsettingH
intoourEloratingsystemoffersbothstabilityandreliabil-
to200yieldsstableElorating.
ity. Tosubstantiatethis,weconductarandomsamplingof
300questionsfromourRWQdataset. Foreachquestion, Theorderofcompetitionhasanotableimpactontheeven-
we feed it into two different LLMs to generate answers. tualElorating.Tomitigatethevolatilityinratingsattributed
Subsequently,GPT-4isemployedtodeterminethewinner, totheorderofcompetitions,wemaintainaconstantques-
loser,oratiebyevaluatingwhichresponsemosteffectively tion seed for each two-player contest and randomize the
addressesthequestionwiththeconsiderationofaccuracy, competitionorderC times,yieldingC distinctEloratings.
relevance, comprehensiveness, clarity, compliance, time- The ultimate Elo rating is determined by calculating the
liness, harmlessness, and unbiasedness3. Concurrently, a medianoftheseC Eloratings. Inourimplementation,we
humanevaluatoralsoreviewsthetwoanswerstoidentify setC to100.
thewinner,loseroratie. Thisprocedureisreplicatedacross
ResultsofOurRWQ-EloRatingSystemandCompar-
2We utilize GPT-4 to transform each entry listed in Google isons with Other Leaderboards. Our system integrates
Trendsintoaformulatedquestion. a total of 24 models including GPT-3.5, GPT-4, Google-
3ThepromptcanbefoundinSectionA.2oftheappendix. Gemini-Proandother21representativeopen-sourceLLMs.
6RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
Ours Chatbot Arena MT-Bench AlpacaEval 1.0 AlpacaEval 2.0
GPT-4-Turbo-1106 (1433)
Gemini-Pro (1282)
LLaMA-2-Chat-70B (1231)
LLaMA-2-Chat-13B (1196)
LLaMA-2-Chat-7B (1181)
Xwin-LM-13B-v0.1 (1178)
Mixtral-Instruct-8x7B-v0.1 (1163)
Xwin-LM-7B-v0.1 (1157)
WizardLM-13B-v1.2 (1156)
GPT-35-Turbo-1106 (1140)
Zephyr-7B-Beta (1124)
Vicuna-33B-v1.3 (1117)
Vicuna-13B-v1.5 (1069)
Vicuna-7B-v1.5 (1044)
Falcon-Instruct-40B (1026)
1470 1500
MPT-Chat-30B (935) 1400
1311 1398
Falcon-Instruct-7B (867) 1267 1300
1161 1167 1169 1186 1212 1201 1235 1200
MPT-Chat-7B (818) 1198
Alpaca-7B (799) 984 1062 1079 1085 1104 1124 1124 1142 111 00 00 0
996
900
LLaMA-1-33B (663) 833 901
800
LLaMA-1-13B (642) 707 760 700
LLaMA-1-7B (621) 637 600
W Ali pz aa cr ad -L 1M 3B-7 (B 5- 8v 41 ).0
(
a(5 )93) LLa MAA -1l -p 3a 3c Ba-7M BPT-ChF aa t-lc 3o 0n B-IV nsic tru un ca t- -3 4Z 3 0e B Bph -vy 1r- .37G BP -T B- e3 t5
a
(-W bTu )iz ra br od -L 1M 1M 0i -x 61tr 3a Bl- -X I vnw 1s .ti 2rn u- cL t-ML 8L x-1a 73M
BB
-A v-vG 0-2
0
.e 1- .1m Chin ai t- -G P 7r 0P o BT-4-Turbo-1106
Figure2: (a)Comparisonofvariousleaderboards,includingourRWQ-Elo(EloratingforeachLLMisreportedinbrackets),
ChatbotArena(Zhengetal.,2023),MT-Bench(Zhengetal.,2023)andAlpacaEval(v1.0andv2.0)(Lietal.,2023a). (b)
StatisticsfromrunningourRWQ-Elosystems100times. WeshowtheEloratingsfortheselected13LLMs. Thecomplete
statisticscanbefoundinFigure6.
In Figure 2.(a), we present the final Elo ratings from our contrast,generatesascorebetween0and10toeachLLM.
RWQbenchmark,alongsidecomparisonswithleaderboards Despitetheadvantageofassessinginter-LLMcompetition,
fromChatbotArena,MT-Bench,andbothversions1.0and ourElosystemismeticulouslycraftedtoincludevarious
2.0ofAlpacaEval. factorslikecomprehensivenessandunbiasedness(referto
SectionA.2forthedetailedprompt). Meanwhile,Chatbot
UnlikeAlpacaEval,whichdeterminesthewinrateofeach
Arenaemploysdiverseonlinehumanevaluatorstoassess
LLMagainstaconstantmodel(Text-Davinci-003forv1.0
the quality of responses from two different LLMs to the
andGPT-4forv2.0),ourElosystemencourageseveryLLM
samequery. However,thisapproachfaceschallengesdue
tocompeteagainsteachother. Ourapproachoffersthree-
tothesubjectivenatureofevaluationsandthedifficultyin
foldadvantages:firstly,asshowninFigure5intheappendix,
scalingupwithconsistenthumanevaluatorsforallpairwise
whenLLMscompeteagainstanLLMthatissignificantly
comparisons. Ourfindingsindicateahighcongruenceof
superior or inferior, it results in a lack of distinguishable
95%betweenGPT-4’sassessmentsandhumanpreferences.
performancedifferencesamongthem;secondly,itavoids
Additionally, our Elo system offers the benefit of being
“over-competition”toasinglemodel(forinstance,adding
easilyscalableintermsofthenumberofthequestionsand
anoptimizationobjectiveduringtrainingtocompeteagainst
LLMstobetested.
aspecificmodel);thirdly,itallowsfortheconsiderationof
AnalysisofStability. Inordertominimizetheimpactof
interactionsbetweenallparticipatingLLMs. MT-Bench,in
thesequenceoftwo-playercontestsonthefinalElorating,
7RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
Table6: Studyonfast-registration. ∆RatingandKendall’s
GPT-4-TurL bL oa G -1M e 1M mA 0Xi 6i-x w n2t iir - -na C P-l h r- L oI aMn ts -- 7tr 1W 0u 3 Bci Bz t-a -8 vr xd 0G 7L .1BP MT -v-- 13 035 .1B-T -Z vu 1re b .p 2oh -V y 1r 1i -c 0F 7u 6Ba nl -ac B-o 3 en 3 t- aBIn -s vtM 1ru .P 3cT t-- 4C 0h Bat-A 3L l 0pL Baa cM a-A
7
B-1-33
B
Lt f ta he Lu r ee Me Env c sla oe alu rso ea yft s re f t aa e nt s mh dte - or fa me rv og le m yisra t srg s ea ce lt erd i a co t tr n c ea h dt ai .n g wg a ii tnd hsif t afe lt lhre e Ln Lbc a Me sa e sn li id n ne cra — lun dk r euin dng .nid Nni gf 2-
GPT-4-Turbo-1106 0.000.140.130.140.110.140.140.070.110.060.020.030.01 N N ∆Rating↓ Kendall’stau↑
1 2
Gemini-Pro 0.140.000.080.070.120.110.030.110.080.050.010.050.00
23 1 10.6 0.97
LLaMA-2-Chat-70B 0.130.080.000.040.090.090.120.080.060.080.030.040.04
21 3 11.8 0.98
Xwin-LM-13B-v0.1 0.140.070.040.000.030.070.010.020.090.060.050.040.03 19 5 8.4 0.98
Mixtral-Instruct-8x7B-v0.1 0.110.120.090.030.000.020.050.030.070.070.060.090.05 14 10 3.6 0.99
WizardLM-13B-v1.2 0.140.110.090.070.020.000.050.010.020.130.090.070.02
GPT-35-Turbo-1106 0.140.030.120.010.050.050.000.000.050.070.060.070.04
system,wheretheLLMscompeteagainstabroaderrange
Zephyr-7B-Beta 0.070.110.080.020.030.010.000.000.030.040.080.070.05
ofotherLLMs.Thedifferencesofthetwowin-ratemaps,as
Vicuna-33B-v1.3 0.110.080.060.090.070.020.050.030.000.060.060.040.04 showcasedinFigure3,revealsaconsistentcorrespondence
Falcon-Instruct-40B 0.060.050.080.060.070.130.070.040.060.000.010.130.03 betweenthem,demonstratingthestabilityofourElosystem.
MPT-Chat-30B 0.020.010.030.050.060.090.060.080.060.010.000.050.07 Additionally,thetrendsinwinratesbetweentwoLLMs,as
Alpaca-7B 0.030.050.040.040.090.070.070.070.040.130.050.000.18 showninFigure4,furtherconfirmthestability.
LLaMA-1-33B 0.010.000.040.030.050.020.040.050.040.030.070.180.00 RegistrationofNewModels. Considerascenariowhere
Figure3: Differencesbetweenthewin-ratemapgenerated there are N 1 established LLMs with stable Elo ratings
byourElosystemandthepre-calculatedwin-ratemapare ({LLM}N i=1 1),andtheobjectiveistointegrateN 2newLLMs
represented using absolute values. We include 13 LLMs. ({LLM∗}N2 )intotheexistingEloranking. Withoutloss
i=1
Thetwocompletewin-ratemapsalongsidetheirdifference of generality, N ≪ N . Instead of re-executing the Elo
2 1
mapcanbefoundinFigure7-8andFigure9oftheappendix. algorithmfromscratchwithallLLMsincluded,amoreeffi-
cientapproachtermedfast-registrationisproposed. Thisap-
0.8 0.5 proachretainstheratingsofN existingLLMs,andassigns
1
e 0.7 e 0.4 aninitialratingof1000totheN newly-registeredLLMs.
Rat
0.6
Rat
0.3 Each competition involves
two2
players, one being LLM
Win 0.5 Win 0.2 from{LLM∗}N i=2 1,andtheothereitherfrom{LLM}N i=1 1,or
0.4 0.1 {LLM∗}N2 −LLM. Post each competition, the Elo rat-
i=1
0.3 0
ingsareupdatedaccordingly. Thefast-registrationprocess
50 100 150 200 50 100 150 200
(2N +N −1)×N ×H/2roundsofcompetition,whichis
Round Round 1 2 2
significantlyless—byN ×(N −1)×H/2rounds—thanre-
1 1
Figure4: Visualizationofthewin-ratetrendsbetweentwo
calculatingtheEloratingsfromscratch.Table6presentsthe
LLMs ((a) Falcon-Instruct-40B v.s. MPT-Chat-30B; (b)
ratingdifferentialsandKendall’stau(whichevaluatesrank-
Falcon-Instruct-7Bv.s. Gemini-Pro). Thehorizontallines
ing differences) for the fast-registration approach against
representthepre-calculatedwinrates. Withtheprogression
executingtheElosystemfromscratch. Theresultsarethe
ofeachcontestround,thewinrateascertainedbyourElorat-
averages of 5 independent experiments. Fast registration
ingsystemprogressivelyconvergeswiththepre-calculated
hasproventobeeffective,especiallywhenN ≥3.
2
winrate.
5.Conclusion
werunourElosystemsC times. Theratingfromeachrun, In this paper, we: 1) critically reassess the widely used
along with the median rating, is depicted in Figure 2.(b). MCQA method for evaluating LLMs, identifying several
This demonstrates a notable consistency in the rankings fundamentallimitations;and2)presenttheRWQ-Elosys-
acrosseachseparateexecution. Additionally,weverifythe tem,designedtoreflectactualusagescenarios. Weconduct
stabilityoftheultimateEloratingbycomparingthewin- extensiveevaluationsof24LLMsthroughmultiplerounds
ratemapproducedbyourElosystemutilizingEq.1and2, ofcompetition,withGPT-4servingasthejudgeandutiliz-
againstapre-calculatedwin-ratemap. Thelatterisobtained ingournewlydevelopedRWQbenchmark. Ourobjective
bycomparingresponsesoftwoLLMstothesamequestions istoprovidefreshperspectiveswithintheLLMcommunity
drawnfromourRWQbenchmark,withGPT-4actingasthe andestablishanovelbenchmarkingframeworkthataidsin
judge and using the same judgement prompt. It is worth theassessmentofLLMs. Wealsodemonstratetheeaseof
notingthattheelementswithinthepre-calculatedwin-rate incorporatingnewmodelsintoourRWQ-Elosystem. Our
maparederivedsolelyfromthecomparisonofresponses aim is to gather more diverse and realistic user inquiries
between two LLMs. This process differs from our Elo acrossvarioussubjects,andtoconductamorecomprehen-
8RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
siveevaluationofawiderrangeofLLMsinfuturestudies. Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell,A.,etal. Languagemodelsarefew-shotlearners.
ImpactStatements
Advancesinneuralinformationprocessingsystems,33:
Thispaperaimstoprovidenovelinsightstothecommunity 1877–1901,2020.
ofLLMevaluationand,byitsnature,inherentlycarriesno
Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre,
risksorsocietal consequences. WhiletheoriginalLLMs
L., and Jumper, J. Accelerating large language model
maygenerateinaccurateorharmfulcontent,suchissuesare
decoding with speculative sampling. arXiv preprint
independentofourfunctionality.Onelimitationofourstudy
arXiv:2302.01318,2023.
is the inability to evaluate all prior LLMs due to limited
resourcesandtherapidprogressinLLMdevelopment.
Chen,M.,Tworek,J.,Jun,H.,Yuan,Q.,Pinto,H.P.d.O.,
Kaplan,J.,Edwards,H.,Burda,Y.,Joseph,N.,Brockman,
References G., etal. Evaluatinglargelanguagemodelstrainedon
code. arXivpreprintarXiv:2107.03374,2021.
Achiam,J.,Adler,S.,Agarwal,S.,Ahmad,L.,Akkaya,I.,
Aleman,F.L.,Almeida,D.,Altenschmidt,J.,Altman,S.,
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,
Anadkat,S.,etal. Gpt-4technicalreport. arXivpreprint
H.,Zheng,L.,Zhuang,S.,Zhuang,Y.,Gonzalez,J.E.,
arXiv:2303.08774,2023.
etal. Vicuna: Anopen-sourcechatbotimpressinggpt-4
with90%*chatgptquality. Seehttps://vicuna.lmsys.org
Almazrouei,E.,Alobeidli,H.,Alshamsi,A.,Cappelli,A.,
(accessed14April2023),2023.
Cojocaru,R.,Debbah,M.,Goffinet,E´.,Hesslow,D.,Lau-
nay,J.,Malartic,Q.,etal. Thefalconseriesofopenlan-
Chowdhery,A.,Narang,S.,Devlin,J.,Bosma,M.,Mishra,
guagemodels. arXivpreprintarXiv:2311.16867,2023.
G., Roberts, A., Barham, P., Chung, H.W., Sutton, C.,
Gehrmann,S.,etal. Palm: Scalinglanguagemodeling
Austin,J.,Odena,A.,Nye,M.,Bosma,M.,Michalewski,
withpathways. JournalofMachineLearningResearch,
H.,Dohan,D.,Jiang,E.,Cai,C.,Terry,M.,Le,Q.,etal.
24(240):1–113,2023.
Program synthesis with large language models. arXiv
preprintarXiv:2108.07732,2021. Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,
Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma,
Ba,J.L.,Kiros,J.R.,andHinton,G.E.Layernormalization.
S.,etal. Scalinginstruction-finetunedlanguagemodels.
arXivpreprintarXiv:1607.06450,2016.
arXivpreprintarXiv:2210.11416,2022.
bench authors, B. Beyond the imitation game: Quantify-
Clark,C.,Lee,K.,Chang,M.-W.,Kwiatkowski,T.,Collins,
ingandextrapolatingthecapabilitiesoflanguagemod-
M.,andToutanova,K. Boolq: Exploringthesurprising
els. TransactionsonMachineLearningResearch,2023.
difficulty of natural yes/no questions. arXiv preprint
ISSN 2835-8856. URL https://openreview.
arXiv:1905.10044,2019.
net/forum?id=uyTL5Bvosj.
Clark,P.,Cowhey,I.,Etzioni,O.,Khot,T.,Sabharwal,A.,
Bisk,Y.,Zellers,R.,Gao,J.,Choi,Y.,etal.Piqa:Reasoning
Schoenick, C., andTafjord, O. Thinkyouhavesolved
aboutphysicalcommonsenseinnaturallanguage. InPro-
questionanswering? tryarc,theai2reasoningchallenge.
ceedingsoftheAAAIconferenceonartificialintelligence,
arXivpreprintarXiv:1803.05457,2018.
volume34,pp.7432–7439,2020.
Cobbe,K.,Kosaraju,V.,Bavarian,M.,Chen,M.,Jun,H.,
Bojar, O., Buck, C., Federmann, C., Haddow, B., Koehn,
Kaiser,L.,Plappert,M.,Tworek,J.,Hilton,J.,Nakano,
P., Leveling, J., Monz, C., Pecina, P., Post, M., Saint-
R.,Hesse,C.,andSchulman,J.Trainingverifierstosolve
Amand, H., et al. Findings of the 2014 workshop on
mathwordproblems. arXivpreprintarXiv:2110.14168,
statistical machine translation. In Proceedings of the
2021.
ninth workshop on statistical machine translation, pp.
12–58,2014. Contributors, O. Opencompass: A universal evaluation
platform for foundation models. https://github.
Bojar,O.,Chatterjee,R.,Federmann,C.,Graham,Y.,Had- com/open-compass/opencompass,2023.
dow,B.,Huck,M.,Yepes,A.J.,Koehn,P.,Logacheva,
V., Monz, C., et al. Findings of the 2016 conference Dao,T.,Fu,D.,Ermon,S.,Rudra,A.,andRe´,C. Flashat-
onmachinetranslation(wmt16). InFirstconferenceon tention: Fastandmemory-efficientexactattentionwith
machinetranslation,pp.131–198.AssociationforCom- io-awareness. AdvancesinNeuralInformationProcess-
putationalLinguistics,2016. ingSystems,35:16344–16359,2022.
9RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
Ding,J.,Ma,S.,Dong,L.,Zhang,X.,Huang,S.,Wang,W., Khot,T.,Clark,P.,Guerquin,M.,Jansen,P.,andSabharwal,
Zheng,N.,andWei,F. Longnet: Scalingtransformersto A. Qasc: Adatasetforquestionansweringviasentence
1,000,000,000tokens. arXivpreprintarXiv:2307.02486, composition. InProceedingsoftheAAAIConferenceon
2023. ArtificialIntelligence,volume34,pp.8082–8090,2020.
Du,N.,Huang,Y.,Dai,A.M.,Tong,S.,Lepikhin,D.,Xu, Lai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. Race:
Y.,Krikun,M.,Zhou,Y.,Yu,A.W.,Firat,O.,etal. Glam: Large-scalereadingcomprehensiondatasetfromexami-
Efficient scaling of language models with mixture-of- nations. arXivpreprintarXiv:1704.04683,2017.
experts. InInternationalConferenceonMachineLearn-
Leviathan,Y.,Kalman,M.,andMatias,Y. Fastinference
ing,pp.5547–5569.PMLR,2022.
from transformers via speculative decoding. In Inter-
Du, Y., Wei, F., and Zhang, H. Anytool: Self-reflective, nationalConferenceonMachineLearning,pp.19274–
hierarchicalagentsforlarge-scaleapicalls.arXivpreprint 19286.PMLR,2023.
arXiv:2402.04253,2024.
Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I.,
Fan, A., Lewis, M., andDauphin, Y. Hierarchicalneural Guestrin,C.,Liang,P.,andHashimoto,T.B. Alpacae-
storygeneration. arXivpreprintarXiv:1805.04833,2018. val: An automatic evaluator of instruction-following
models. https://github.com/tatsu-lab/
Fedus, W., Zoph, B., and Shazeer, N. Switch transform-
alpaca_eval,2023a.
ers: Scaling to trillion parameter models with simple
andefficientsparsity. TheJournalofMachineLearning Li, Y., Wei, F., Zhao, J., Zhang, C., andZhang, H. Rain:
Research,23(1):5232–5270,2022. Yourlanguagemodelscanalignthemselveswithoutfine-
tuning. arXivpreprintarXiv:2309.07124,2023b.
Gao,L.,Tow,J.,Biderman,S.,Black,S.,DiPofi,A.,Foster,
C., Golding, L., Hsu, J., McDonell, K., Muennighoff, Li,Y.,Wei,F.,Zhang,C.,andZhang,H. Eagle:Speculative
N., et al. A framework for few-shot language model samplingrequiresrethinkingfeatureuncertainty. arXiv
evaluation. Versionv0.0.1.Sept,2021. preprintarXiv:2401.15077,2024.
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, Lison,P.andTiedemann,J. Opensubtitles2016: Extracting
M., Song, D., and Steinhardt, J. Measuring mas- largeparallelcorporafrommovieandtvsubtitles. 2016.
sive multitask language understanding. arXiv preprint
arXiv:2009.03300,2020. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
Levy,O.,Lewis,M.,Zettlemoyer,L.,andStoyanov,V.
Hoffmann,J.,Borgeaud,S.,Mensch,A.,Buchatskaya,E., Roberta: Arobustlyoptimizedbertpretrainingapproach.
Cai,T.,Rutherford,E.,Casas,D.d.L.,Hendricks,L.A., arXivpreprintarXiv:1907.11692,2019.
Welbl, J., Clark, A., et al. Training compute-optimal
largelanguagemodels. arXivpreprintarXiv:2203.15556, Mihaylov,T.,Clark,P.,Khot,T.,andSabharwal,A. Can
2022. a suit of armor conduct electricity? a new dataset
for open book question answering. arXiv preprint
Holtzman,A.,Buys,J.,Du,L.,Forbes,M.,andChoi,Y.The
arXiv:1809.02789,2018.
curiouscaseofneuraltextdegeneration. arXivpreprint
arXiv:1904.09751,2019. Ouyang,L.,Wu,J.,Jiang,X.,Almeida,D.,Wainwright,C.,
Mishkin,P.,Zhang,C.,Agarwal,S.,Slama,K.,Ray,A.,
Hu,E.J.,Shen,Y.,Wallis,P.,Allen-Zhu,Z.,Li,Y.,Wang,
et al. Training language models to follow instructions
S.,Wang,L.,andChen,W. Lora:Low-rankadaptationof
withhumanfeedback. AdvancesinNeuralInformation
largelanguagemodels. arXivpreprintarXiv:2106.09685,
ProcessingSystems,35:27730–27744,2022.
2021.
Peng,B.,Alcaide,E.,Anthony,Q.,Albalak,A.,Arcadinho,
Huang,Y.,Cheng,Y.,Bapna,A.,Firat,O.,Chen,D.,Chen,
S.,Cao,H.,Cheng,X.,Chung,M.,Grella,M.,GV,K.K.,
M.,Lee,H.,Ngiam,J.,Le,Q.V.,Wu,Y.,etal. Gpipe:
et al. Rwkv: Reinventing rnns for the transformer era.
Efficienttrainingofgiantneuralnetworksusingpipeline
arXivpreprintarXiv:2305.13048,2023.
parallelism. Advancesinneuralinformationprocessing
systems,32,2019. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.,
etal. Improvinglanguageunderstandingbygenerative
Huang, Y., Bai, Y., Zhu, Z., Zhang, J., Zhang, J., Su, T.,
pre-training. 2018.
Liu, J., Lv, C., Zhang, Y., Lei, J., Fu, Y., Sun, M., and
He, J. C-eval: A multi-level multi-discipline chinese Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
evaluationsuiteforfoundationmodels. InAdvancesin Sutskever,I.,etal. Languagemodelsareunsupervised
NeuralInformationProcessingSystems,2023. multitasklearners. OpenAIblog,1(8):9,2019.
10RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
Rae,J.W.,Borgeaud,S.,Cai,T.,Millican,K.,Hoffmann, Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X.,
J., Song, F., Aslanides, J., Henderson, S., Ring, R., Guestrin,C.,Liang,P.,andHashimoto,T.B. Stanford
Young, S., et al. Scaling language models: Methods, alpaca: Aninstruction-followingllamamodel,2023b.
analysis&insightsfromtraininggopher. arXivpreprint
arXiv:2112.11446,2021. Team, M. et al. Introducing mpt-7b: a new standard for
open-source,commerciallyusablellms,2023a.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Team,M.N.etal. Introducingmpt-30b: Raisingthebarfor
Matena,M.,Zhou,Y.,Li,W.,andLiu,P.J. Exploring
open-sourcefoundationmodels,2023b.
thelimitsoftransferlearningwithaunifiedtext-to-text
transformer. TheJournalofMachineLearningResearch,
Team, X.-L. Xwin-lm, 9 2023. URL https://github.
21(1):5485–5551,2020.
com/Xwin-LM/Xwin-LM.
Rajbhandari,S.,Rasley,J.,Ruwase,O.,andHe,Y. Zero:
Team,X.-L. Xwin-lm,92023. URLhttps://github.
Memoryoptimizationstowardtrainingtrillionparameter
com/Xwin-LM/Xwin-LM.
models. InSC20: InternationalConferenceforHighPer-
formanceComputing,Networking,StorageandAnalysis, Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul-
pp.1–16.IEEE,2020. shreshtha,A.,Cheng,H.-T.,Jin,A.,Bos,T.,Baker,L.,
Du,Y.,etal. Lamda: Languagemodelsfordialogappli-
Rasley,J.,Rajbhandari,S.,Ruwase,O.,andHe,Y. Deep-
cations. arXivpreprintarXiv:2201.08239,2022.
speed: Systemoptimizationsenabletrainingdeeplearn-
ingmodelswithover100billionparameters. InProceed- Touvron,H.,Lavril,T.,Izacard,G.,Martinet,X.,Lachaux,
ingsofthe26thACMSIGKDDInternationalConference M.-A.,Lacroix,T.,Rozie`re,B.,Goyal,N.,Hambro,E.,
onKnowledgeDiscovery&DataMining,pp.3505–3506, Azhar,F.,etal. Llama:Openandefficientfoundationlan-
2020. guagemodels. arXivpreprintarXiv:2302.13971,2023a.
Sap,M.,Rashkin,H.,Chen,D.,LeBras,R.,andChoi,Y. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
Socialiqa: Commonsensereasoningaboutsocialinterac- A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,
tions. arXivpreprintarXiv:1904.09728,2019. Bhosale,S.,etal. Llama2: Openfoundationandfine-
tuned chat models. arXiv preprint arXiv:2307.09288,
Shazeer, N. Glu variants improve transformer. arXiv
2023b.
preprintarXiv:2002.05202,2020.
van Gemert-Pijnen, J. E., Nijland, N., van Limburg, M.,
Shoeybi,M.,Patwary,M.,Puri,R.,LeGresley,P.,Casper, Ossebaard, H. C., Kelders, S. M., Eysenbach, G., and
J., and Catanzaro, B. Megatron-lm: Training multi- Seydel,E.R. Aholisticframeworktoimprovetheuptake
billion parameter language models using model paral- andimpactofehealthtechnologies. Journalofmedical
lelism. arXivpreprintarXiv:1909.08053,2019. Internetresearch,13(4):e1672,2011.
Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Workshop,B.,Scao,T.L.,Fan,A.,Akiki,C.,Pavlick,E.,
Roformer: Enhanced transformer with rotary position Ilic´,S.,Hesslow,D.,Castagne´,R.,Luccioni,A.S.,Yvon,
embedding. Neurocomputing,568:127063,2024. F.,etal. Bloom: A176b-parameteropen-accessmultilin-
guallanguagemodel. arXivpreprintarXiv:2211.05100,
Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J.,
2022.
Wang,J.,andWei,F. Retentivenetwork: Asuccessorto
transformer for large language models. arXiv preprint Xu,C.,Sun,Q.,Zheng,K.,Geng,X.,Zhao,P.,Feng,J.,Tao,
arXiv:2307.08621,2023. C.,andJiang,D. Wizardlm: Empoweringlargelanguage
modelstofollowcomplexinstructions. arXivpreprint
Talmor,A.,Herzig,J.,Lourie,N.,andBerant,J. Common-
arXiv:2304.12244,2023.
senseqa: Aquestionansweringchallengetargetingcom-
monsenseknowledge. arXivpreprintarXiv:1811.00937, Zellers,R.,Holtzman,A.,Bisk,Y.,Farhadi,A.,andChoi,
2018. Y. Hellaswag:Canamachinereallyfinishyoursentence?
arXivpreprintarXiv:1905.07830,2019.
Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X.,
Guestrin,C.,Liang,P.,andHashimoto,T.B. Alpaca: A Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
strong,replicableinstruction-followingmodel. Stanford Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V.,
CenterforResearchonFoundationModels.https://crfm. etal.Opt:Openpre-trainedtransformerlanguagemodels.
stanford.edu/2023/03/13/alpaca.html,3(6):7,2023a. arXivpreprintarXiv:2205.01068,2022.
11RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
Zheng,L.,Chiang,W.-L.,Sheng,Y.,Zhuang,S.,Wu,Z.,
Zhuang,Y.,Lin,Z.,Li,Z.,Li,D.,Xing,E.,etal. Judging
llm-as-a-judgewithmt-benchandchatbotarena. arXiv
preprintarXiv:2306.05685,2023.
Zhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang,
Y., Saied, A., Chen, W., and Duan, N. Agieval: A
human-centricbenchmarkforevaluatingfoundationmod-
els. arXivpreprintarXiv:2304.06364,2023.
Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urta-
sun,R.,Torralba,A.,andFidler,S. Aligningbooksand
movies: Towardsstory-likevisualexplanationsbywatch-
ing movies and reading books. In Proceedings of the
IEEE international conference on computer vision, pp.
19–27,2015.
12RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
A.Prompts
A.1.PromptforOpen-endedMCQA
Given a response generated by an LLM, and several choices. GPT-4 is then used to determine the choice that is most
semanticallyalignedwiththeresponse. Thepromptisprovidedasfollows.
System
You are a helpful assistant.
User
Task description:
You are presented with multiple choices and a statement. Your task involves
selecting the choice that most semantically aligns with the given statement. Use
the criteria and guidelines provided to make your decision.
Criteria:
1. Choose the option that semantically aligns with the statement, by either
expanding upon, encapsulating, or exactly matching it.
2. For numerical choices, prefer the one with the smallest reasonable numerical
difference from the statement.
3. It is acceptable to choose none if no option closely aligns with the
statement.
Guidelines:
1. Review all choices and the statement carefully.
2. Justify your choice briefly, considering the criteria.
3. Indicate your response by stating the uppercase letter of your choice (e.g.,
’A’, ’B’, ’C’, ’D’), or ’None’ if no choice matches.
Output format:
1. Present your response in JSON format.
2. Include the ‘choice’(the uppercase letter of the chosen choice or ’None’),
along with a brief ‘explanation’ for your selection.
Choices:
{choices}
Statement:
{statement}
A.2.PromptforRWQ-EloSystem
GPT-4servesasajudgeinevaluatingresponsesfromtwodifferentLLMs(LLM-1andLLM-2)tothesamequery. The
promptisprovidedasfollows.
System
You are a helpful assistant who can evaluate Large Language Model (LLM)
responses.
User
Task description:
As a judge, your task is to assess the responses of two Large Language Models
(LLM-1 and LLM-2) to a user’s question. Base your evaluation on the criteria
below to determine which response is more effective.
Criteria:
1. Accuracy: Ensure responses are factually correct. For factual questions,
13RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
responses should align with scientific consensus.
2. Relevance: Check if responses address the user’s question directly,
understanding its context and intent.
3. Comprehensiveness: Responses should cover all aspects of the question,
providing a clear overview and key points for complex issues.
4. Clarity: Ensure responses are easy to understand, especially when explaining
complex topics.
5. Compliance: Adherence to ethical and legal standards is mandatory.
6. Timeliness: Incorporate the latest information for current topics.
7. Harmlessness: Avoid misleading or harmful content, respecting cultural
sensitivities and privacy.
8. Unbiasedness: Responses should not show unjustified preference, especially
in subjective matters.
Guidelines:
1. Evaluate each response based on the criteria, noting strengths and
weaknesses.
2. Choose the most effective response or indicate a tie. Explain your reasoning
in the specified JSON format.
3. Remain objective, not letting the order of responses bias your evaluation.
Output format:
1. Present your judgment in JSON format.
2. Include the winner: Use an integer (1 if LLM-1 has the better response, 2 if
LLM-2 has the better response, 0 for a tie if both responses are satisfactory,
and -1 for a tie if both responses are unsatisfactory), and an explanation
(providing a rationale for your choice).
User-submitted question:
{user submitted question}
Response of LLM-1:
{llm response 1}
Response of LLM-2:
{llm response 2}
B.MoreExperimentalResults
DetailedComparisonwithAlpacaEval. ThecomparisonbetweenourRWQ-EloandtheAlpacaEvalisshowninFigure5.
TheCompleteStatisticsofOurRWQ-EloSystem. ThiscanbefoundinFigure6. Weinclude24modelsintotal.
Win-RateMaps. Wepresentthewin-ratemapgeneratedbyourRWQ-Elosystem,thepre-calculatedwin-ratemap,and
theirdifferencemapinFigure7,8and9,respectively. 24LLMsareincluded.
DetailedResultsforMCQAEvaluation. Table7-17reporttheMCQAresultsoneachbenchmarkusing7evaluation
strategiesintroducedinSection3.
QualitativeResultsUsingGPT-4astheJudge. InourRWQ-Elosystem,GPT-4actsasthejudge. Weutilizetheprompt
describedinSectionA.2toevaluateresponsesfromtwodifferentLLMs. TwoinstanceswhereGPT-4generatesaccurate
justificationsarepresentedinTable18and19. Conversely,twoexampleswhereGPT-4providesincorrectjustificationscan
befoundinTable20and21.
C.ExamplesfromRWQBenchmark
Our RWQ benchmark comprises 20,772 authentic questions sourced from various platforms including Google Trends,
Quora,ShareGPT,LMSYS-Chat-1M,andAlpacaEval. WeshowthreeexamplesforeachsourceinTable22.
14RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
RWQ-Elo RWQ-Elo AlpacaEval AlpacaEval AlpacaEval
(All) (200) (GPT-4-Turbo) (GPT-3.5-Turbo) (LLaMA-1-13B)
GPT-4-Turbo-1106 11443333 11446644 5500..00%% 9955..44%% 110000..00%%
Gemini-Pro 11228822 11330099 1144..77%% 7799..66%% 110000..00%%
LLaMA-2-Chat-70B 11223311 11226622 1133..33%% 7788..88%% 110000..00%%
LLaMA-2-Chat-13B 11119966 11223311 88..55%% 7711..55%% 110000..00%%
LLaMA-2-Chat-7B 11118811 11221155 88..44%% 6688..88%% 110000..00%%
Xwin-LM-13B-v0.1 11117788 11220066 77..22%% 6666..33%% 9999..55%%
Mixtral-Instruct-8x7B-v0.1 11116633 11220022 55..99%% 6666..11%% 9999..55%%
Xwin-LM-7B-v0.1 11115577 11119966 55..88%% 6633..66%% 9999..55%%
WizardLM-13B-v1.2 11115566 11117799 44..88%% 6600..55%% 9999..55%%
GPT-35-Turbo-1106 11114400 11115555 44..66%% 6600..55%% 9988..99%%
Zephyr-7B-Beta 11112244 11114477 44..66%% 5555..44%% 9988..99%%
Vicuna-33B-v1.3 11111177 11112255 33..55%% 5500..00%% 9988..44%%
Vicuna-13B-v1.5 11006699 11006666 33..33%% 3399..55%% 9988..44%%
Vicuna-7B-v1.5 11004444 11005511 22..11%% 3366..22%% 9977..88%%
Falcon-Instruct-40B 11002266 11002200 11..88%% 2299..11%% 9955..77%%
MPT-Chat-30B 993355 994411 11..66%% 1177..22%% 9955..55%%
Falcon-Instruct-7B 886677 881177 00..00%% 44..33%% 9933..00%%
MPT-Chat-7B 881188 778844 00..00%% 33..77%% 9922..44%%
Alpaca-7B 779999 776655 00..00%% 33..44%% 9911..88%%
LLaMA-1-33B 666633 661155 00..00%% 00..55%% 6611..55%%
LLaMA-1-13B 664422 558888 00..00%% 00..00%% 5500..00%%
LLaMA-1-7B 662211 557733 00..00%% 00..00%% 3366..88%%
WizardLM-7B-v1.0 559933 556622 00..00%% 00..00%% 1111..33%%
Alpaca-13B 558844 552288 00..00%% 00..00%% 88..00%%
Figure5: WecompareourRWQ-EloratingsystemwithvariousAlpacaEvalvariants,whereGPT-4-Turbo,GPT-3.5-Turbo,
andLLaMA-1-13Bserveasthecompetitors. RWQ-Elo(All)andRWQ-Elo(200)denotethatthesystemisrunusingall
instancesandarandomselectionof200instances,respectively,fromourRWQbenchmark. Weutilizethesameinstances
fromRWQ-Elo(200)forAlpacaEval. WhileAlpacaEvaluseswin-rateasitsmetric,ourRWQ-ElosystememploystheElo
scoreasitsmetric. InAlpacaEval,whenLLMscompeteagainstanLLMthatissignificantlysuperiororinferior,itresultsin
alackofdistinguishableperformancedifferencesamongthem. Incontrast,oursystemdoesnotexhibitthisissue.
15RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
1470
1400
1398
1311
1267
1200 1235
11981218120812011212
11921186 116911671161
1162 1114
11451142
11241114 1124
1104
10761062
10851079
1000 1039 984
994 996
892
901 855
833
800 832
786
760 707
665
648
600 637 616 612 604
594
568 556
G G L L L X M X W G Z V V V F M F M A L L L W A
P T -4 -T u rb o -1 1 0 6 e m in i-P ro L a M A -2 -C h a t-7 0 B L a M A -2 -C h a t-1 3 B L a M A -2 -C h a t-7 B w in -L M -1 3 B -v 0 .1 ix tra l-In s tru c t-8 x 7
B
w in -L M -7 B -v 0 .1 iz a rd L M -1 3 B -v 1 .2 P T -3 5 -T u rb o -1 1 0 6 e p h y r-7 B -B e ta ic u n a -3 3 B -v 1 .3 ic u n a -1 3 B -v 1 .5 ic u n a -7 B -v 1 .5 a lc o n -In s tru c t-4 0 B P T -C h a t-3 0 B a lc o n -In s tru c t-7 B P T -C h a t-7 B lp a c a -7 B L a M A -1 -3 3 B L a M A -1 -1 3 B L a M A -1 -7 B iz a rd L M -7 B -v 1 .0 lp a c a -1 3 B
-v
0
.1
Figure6: ThecompletestatisticsfromrunningourRWQ-Elosystems100times. Weinclude24modelsintotal.
16RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
M
ix
G P T -4 -T u rb o -1
1 0 6
G em in i-P
ro
L L aM A -2 -C h at-7
0 B
L L aM A -2 -C h at-1
3 B
L L aM A -2 -C h at-7
B
X w in -L M -1 3 B -v
0 .1
tral-In
stru ct-8 x 7 B -v
0 .1
X w in -L M -7 B -v
0 .1
W izard L M -1 3 B -v
1 .2
G P T -3 5 -T u rb o -1
1 0 6
Z ep h y r-7 B -B
eta
V icu n a-3 3 B -v
1 .3
V icu n a-1 3 B -v
1 .5
V icu n a-7 B -v
1 .5
F alco n -In stru ct-4
0 B
M P T -C h at-3
0 B
F alco n -In stru ct-7
B
M P T -C h at-7
B
A lp aca-7
B
L L aM A -1 -3
3 B
L L aM A -1 -1
3 B
L L aM A -1
-7 B
W izard L M -7 B -v
1 .0
A lp aca-1
3 B
1
GPT-4-Turbo-1106 0.00 0.85 0.90 0.94 0.94 0.96 0.94 0.93 0.97 0.98 0.93 0.98 0.96 0.98 0.98 0.96 0.98 1.00 1.00 1.00 1.00 1.00 1.00 1.00
Gemini-Pro 0.15 0.00 0.65 0.73 0.74 0.71 0.78 0.76 0.78 0.72 0.83 0.80 0.89 0.84 0.86 0.87 0.94 0.97 0.99 0.97 0.98 1.00 1.00 1.00
LLaMA-2-Chat-70B 0.10 0.35 0.00 0.56 0.60 0.62 0.69 0.61 0.70 0.75 0.73 0.72 0.85 0.81 0.84 0.88 0.98 0.96 0.96 1.00 0.99 1.00 1.00 1.00
LLaMA-2-Chat-13B 0.06 0.27 0.44 0.00 0.54 0.50 0.51 0.52 0.55 0.65 0.67 0.67 0.74 0.79 0.83 0.91 0.93 0.97 0.98 0.98 1.00 1.00 1.00 1.00
LLaMA-2-Chat-7B 0.06 0.26 0.40 0.46 0.00 0.49 0.50 0.62 0.55 0.60 0.61 0.57 0.73 0.74 0.76 0.84 0.94 0.95 0.95 0.98 0.98 1.00 1.00 1.00
0.8
Xwin-LM-13B-v0.1 0.04 0.29 0.38 0.50 0.51 0.00 0.55 0.60 0.60 0.57 0.60 0.67 0.67 0.76 0.76 0.85 0.90 0.93 0.94 0.98 0.96 0.99 0.99 1.00
Mixtral-Instruct-8x7B-v0.1 0.07 0.22 0.31 0.49 0.50 0.45 0.00 0.58 0.53 0.58 0.58 0.64 0.67 0.74 0.75 0.85 0.88 0.92 0.98 1.00 1.00 1.00 1.00 1.00
Xwin-LM-7B-v0.1 0.07 0.24 0.39 0.48 0.38 0.40 0.41 0.00 0.56 0.57 0.57 0.58 0.68 0.72 0.68 0.83 0.91 0.92 0.96 0.99 0.98 0.99 0.99 0.99
WizardLM-13B-v1.2 0.03 0.22 0.30 0.45 0.45 0.40 0.47 0.44 0.00 0.58 0.54 0.57 0.62 0.74 0.81 0.87 0.94 0.95 0.96 0.97 1.00 0.99 1.00 1.00
GPT-35-Turbo-1106 0.02 0.28 0.25 0.35 0.40 0.43 0.42 0.43 0.42 0.00 0.52 0.48 0.62 0.67 0.72 0.82 0.98 0.94 0.95 0.98 1.00 0.99 1.00 1.00 0.6
Zephyr-7B-Beta 0.07 0.17 0.27 0.33 0.39 0.40 0.42 0.43 0.46 0.48 0.00 0.48 0.62 0.65 0.68 0.83 0.84 0.93 0.94 0.98 0.99 0.99 0.99 0.99
Vicuna-33B-v1.3 0.02 0.20 0.28 0.33 0.43 0.33 0.36 0.42 0.43 0.52 0.52 0.00 0.57 0.65 0.69 0.80 0.84 0.83 0.90 0.98 0.98 0.99 1.00 1.00
Vicuna-13B-v1.5 0.04 0.11 0.15 0.26 0.27 0.33 0.33 0.32 0.38 0.38 0.38 0.43 0.00 0.56 0.58 0.69 0.87 0.89 0.90 0.95 0.98 0.97 1.00 0.99
Vicuna-7B-v1.5 0.02 0.16 0.19 0.21 0.26 0.24 0.26 0.28 0.26 0.33 0.35 0.35 0.44 0.00 0.53 0.64 0.85 0.89 0.86 0.96 0.95 0.97 0.99 0.99
Falcon-Instruct-40B 0.02 0.14 0.16 0.17 0.24 0.24 0.25 0.32 0.19 0.28 0.32 0.31 0.42 0.47 0.00 0.64 0.89 0.85 0.92 0.92 0.97 1.00 1.00 1.00 0.4
MPT-Chat-30B 0.04 0.13 0.13 0.09 0.16 0.15 0.15 0.17 0.13 0.18 0.17 0.20 0.31 0.36 0.36 0.00 0.66 0.75 0.74 0.90 0.94 0.97 0.99 0.99
Falcon-Instruct-7B 0.01 0.06 0.02 0.07 0.06 0.10 0.12 0.09 0.06 0.02 0.16 0.16 0.13 0.15 0.11 0.34 0.00 0.55 0.73 0.89 0.93 0.96 0.98 1.00
MPT-Chat-7B 0.00 0.03 0.04 0.03 0.05 0.07 0.08 0.08 0.05 0.06 0.07 0.17 0.11 0.11 0.15 0.25 0.45 0.00 0.56 0.88 0.92 0.95 0.95 0.98
Alpaca-7B 0.00 0.01 0.04 0.02 0.05 0.06 0.02 0.04 0.04 0.05 0.06 0.10 0.10 0.14 0.08 0.26 0.27 0.43 0.00 0.86 0.89 0.95 0.99 0.99
0.2
LLaMA-1-33B 0.00 0.03 0.00 0.02 0.02 0.02 0.00 0.01 0.03 0.02 0.02 0.02 0.05 0.04 0.08 0.10 0.11 0.12 0.14 0.00 0.67 0.78 0.97 0.96
LLaMA-1-13B 0.00 0.02 0.01 0.00 0.01 0.04 0.00 0.02 0.00 0.00 0.01 0.02 0.02 0.05 0.03 0.06 0.07 0.08 0.11 0.33 0.00 0.55 0.94 0.93
LLaMA-1-7B 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.01 0.01 0.01 0.01 0.01 0.03 0.03 0.00 0.03 0.04 0.04 0.05 0.22 0.45 0.00 0.94 0.92
WizardLM-7B-v1.0 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.01 0.00 0.00 0.01 0.00 0.00 0.01 0.00 0.01 0.02 0.05 0.01 0.03 0.06 0.06 0.00 0.56
Alpaca-13B 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.01 0.00 0.01 0.01 0.00 0.01 0.00 0.02 0.01 0.04 0.07 0.08 0.44 0.00
0
Figure7: Visualizationofthecompletewin-ratemapgeneratedbyourRWQ-Elosystem.
17RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
M
ix
G P T -4 -T u rb o -1
1 0 6
G em in i-P
ro
L L aM A -2 -C h at-7
0 B
L L aM A -2 -C h at-1
3 B
L L aM A -2 -C h at-7
B
X w in -L M -1 3 B -v
0 .1
tral-In
stru ct-8 x 7 B -v
0 .1
X w in -L M -7 B -v
0 .1
W izard L M -1 3 B -v
1 .2
G P T -3 5 -T u rb o -1
1 0 6
Z ep h y r-7 B -B
eta
V icu n a-3 3 B -v
1 .3
V icu n a-1 3 B -v
1 .5
V icu n a-7 B -v
1 .5
F alco n -In stru ct-4
0 B
M P T -C h at-3
0 B
F alco n -In stru ct-7
B
M P T -C h at-7
B
A lp aca-7
B
L L aM A -1 -3
3 B
L L aM A -1 -1
3 B
L L aM A -1
-7 B
W izard L M -7 B -v
1 .0
A lp aca-1
3 B
1
GPT-4-Turbo-1106 0.00 0.70 0.76 0.80 0.81 0.81 0.83 0.83 0.83 0.84 0.86 0.86 0.89 0.90 0.91 0.95 0.96 0.97 0.97 0.99 0.99 0.99 0.99 0.99
Gemini-Pro 0.30 0.00 0.57 0.62 0.64 0.65 0.66 0.67 0.67 0.69 0.71 0.72 0.77 0.80 0.81 0.88 0.92 0.94 0.94 0.97 0.98 0.98 0.98 0.98
LLaMA-2-Chat-70B 0.24 0.43 0.00 0.55 0.57 0.58 0.60 0.60 0.61 0.63 0.65 0.66 0.72 0.75 0.76 0.85 0.89 0.92 0.92 0.96 0.97 0.97 0.98 0.98
LLaMA-2-Chat-13B 0.20 0.38 0.45 0.00 0.52 0.53 0.55 0.56 0.56 0.58 0.60 0.61 0.68 0.71 0.73 0.82 0.87 0.90 0.91 0.96 0.96 0.96 0.97 0.97
LLaMA-2-Chat-7B 0.19 0.36 0.43 0.48 0.00 0.50 0.53 0.53 0.54 0.56 0.58 0.59 0.66 0.69 0.71 0.80 0.86 0.89 0.90 0.95 0.96 0.96 0.97 0.97
0.8
Xwin-LM-13B-v0.1 0.19 0.35 0.42 0.47 0.50 0.00 0.52 0.53 0.53 0.55 0.58 0.59 0.65 0.68 0.71 0.80 0.86 0.89 0.90 0.95 0.96 0.96 0.97 0.97
Mixtral-Instruct-8x7B-v0.1 0.17 0.34 0.40 0.45 0.47 0.48 0.00 0.51 0.51 0.53 0.56 0.57 0.63 0.66 0.69 0.79 0.85 0.88 0.89 0.95 0.95 0.96 0.96 0.97
Xwin-LM-7B-v0.1 0.17 0.33 0.40 0.44 0.47 0.47 0.49 0.00 0.50 0.52 0.55 0.56 0.62 0.66 0.68 0.78 0.84 0.88 0.89 0.94 0.95 0.96 0.96 0.96
WizardLM-13B-v1.2 0.17 0.33 0.39 0.44 0.46 0.47 0.49 0.50 0.00 0.52 0.55 0.56 0.62 0.66 0.68 0.78 0.84 0.87 0.89 0.94 0.95 0.96 0.96 0.96
GPT-35-Turbo-1106 0.16 0.31 0.37 0.42 0.44 0.45 0.47 0.48 0.48 0.00 0.52 0.53 0.60 0.63 0.66 0.76 0.83 0.86 0.88 0.94 0.95 0.95 0.96 0.96 0.6
Zephyr-7B-Beta 0.14 0.29 0.35 0.40 0.42 0.42 0.44 0.45 0.45 0.48 0.00 0.51 0.58 0.61 0.64 0.75 0.81 0.85 0.87 0.93 0.94 0.95 0.96 0.96
Vicuna-33B-v1.3 0.14 0.28 0.34 0.39 0.41 0.41 0.43 0.44 0.44 0.47 0.49 0.00 0.57 0.60 0.63 0.74 0.81 0.85 0.86 0.93 0.94 0.95 0.95 0.96
Vicuna-13B-v1.5 0.11 0.23 0.28 0.32 0.34 0.35 0.37 0.38 0.38 0.40 0.42 0.43 0.00 0.54 0.56 0.68 0.76 0.81 0.83 0.91 0.92 0.93 0.94 0.94
Vicuna-7B-v1.5 0.10 0.20 0.25 0.29 0.31 0.32 0.34 0.34 0.34 0.37 0.39 0.40 0.46 0.00 0.53 0.65 0.73 0.79 0.80 0.90 0.91 0.92 0.93 0.93
Falcon-Instruct-40B 0.09 0.19 0.24 0.27 0.29 0.29 0.31 0.32 0.32 0.34 0.36 0.37 0.44 0.47 0.00 0.63 0.71 0.77 0.79 0.89 0.90 0.91 0.92 0.93 0.4
MPT-Chat-30B 0.05 0.12 0.15 0.18 0.20 0.20 0.21 0.22 0.22 0.24 0.25 0.26 0.32 0.35 0.37 0.00 0.60 0.66 0.69 0.83 0.84 0.86 0.88 0.88
Falcon-Instruct-7B 0.04 0.08 0.11 0.13 0.14 0.14 0.15 0.16 0.16 0.17 0.19 0.19 0.24 0.27 0.29 0.40 0.00 0.57 0.60 0.76 0.79 0.80 0.83 0.84
MPT-Chat-7B 0.03 0.06 0.08 0.10 0.11 0.11 0.12 0.12 0.13 0.14 0.15 0.15 0.19 0.21 0.23 0.34 0.43 0.00 0.53 0.71 0.73 0.76 0.79 0.79
Alpaca-7B 0.03 0.06 0.08 0.09 0.10 0.10 0.11 0.11 0.11 0.12 0.13 0.14 0.17 0.20 0.21 0.31 0.40 0.47 0.00 0.69 0.71 0.74 0.77 0.78
0.2
LLaMA-1-33B 0.01 0.03 0.04 0.04 0.05 0.05 0.05 0.06 0.06 0.06 0.07 0.07 0.09 0.10 0.11 0.17 0.24 0.29 0.31 0.00 0.53 0.56 0.60 0.61
LLaMA-1-13B 0.01 0.02 0.03 0.04 0.04 0.04 0.05 0.05 0.05 0.05 0.06 0.06 0.08 0.09 0.10 0.16 0.21 0.27 0.29 0.47 0.00 0.53 0.57 0.58
LLaMA-1-7B 0.01 0.02 0.03 0.04 0.04 0.04 0.04 0.04 0.04 0.05 0.05 0.05 0.07 0.08 0.09 0.14 0.20 0.24 0.26 0.44 0.47 0.00 0.54 0.55
WizardLM-7B-v1.0 0.01 0.02 0.02 0.03 0.03 0.03 0.04 0.04 0.04 0.04 0.04 0.05 0.06 0.07 0.08 0.12 0.17 0.21 0.23 0.40 0.43 0.46 0.00 0.51
Alpaca-13B 0.01 0.02 0.02 0.03 0.03 0.03 0.03 0.04 0.04 0.04 0.04 0.04 0.06 0.07 0.07 0.12 0.16 0.21 0.22 0.39 0.42 0.45 0.49 0.00
0
Figure8: Visualizationofthecompletepre-calculatedwin-ratemap.
18RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
G P T-4- TurboL -L 1G 1a e 0M m 6A iL n- iL 2
-
Pa - CM rohA at-L -2 7L - 0Ca BM haAXM t--w 1i 2x 3i -t n BCr -a hLl- aMI tn -- 7s 1t Br 3u Bc -tX v-8 0w x .1i 7nW B- L -i vMz 0a .-r 17d BLG M -vP 0-T 1 .- 133 B5- -T vu 1Z r .2be op -h 1y 1V r 0-i 67c Bu -n Ba-V e3 ti a3c Bun -va 1-1 .V 33i BcF u -a vnl 1ac .-o 57n B-I -n vs 1t .r 5M ucP t-F T 4-a 0Cl Bc ho an t- -I 3n 0s BtrM ucP t-T 7- BChat-A 7l BpL aL ca aM -7A BL -L 1a -3M 3A B-L 1W L -1ai 3Mz Ba Ard -L 1M -7- B7 B-A v1lp .0aca-13
B
GPT-4-Turbo-1106 0.00 0.14 0.13 0.14 0.13 0.14 0.11 0.10 0.14 0.14 0.07 0.11 0.07 0.07 0.06 0.02 0.02 0.02 0.03 0.01 0.01 0.00 0.00 0.01
Gemini-Pro 0.14 0.00 0.08 0.11 0.10 0.07 0.12 0.09 0.11 0.03 0.11 0.08 0.12 0.04 0.05 0.01 0.02 0.04 0.05 0.00 0.00 0.02 0.02 0.02
LLaMA-2-Chat-70B 0.13 0.08 0.00 0.01 0.03 0.04 0.09 0.01 0.09 0.12 0.08 0.06 0.13 0.06 0.08 0.03 0.09 0.05 0.04 0.04 0.02 0.03 0.02 0.02
LLaMA-2-Chat-13B 0.14 0.11 0.01 0.00 0.02 0.03 0.03 0.04 0.01 0.07 0.07 0.05 0.06 0.09 0.11 0.10 0.06 0.07 0.07 0.02 0.03 0.03 0.03 0.02
LLaMA-2-Chat-7B 0.13 0.10 0.03 0.02 0.00 0.01 0.03 0.09 0.01 0.04 0.03 0.02 0.08 0.06 0.05 0.04 0.08 0.06 0.05 0.03 0.03 0.03 0.03 0.03
Xwin-LM-13B-v0.1 0.14 0.07 0.04 0.03 0.01 0.00 0.03 0.07 0.07 0.01 0.02 0.09 0.02 0.07 0.06 0.05 0.04 0.04 0.04 0.03 0.00 0.02 0.03 0.03
Mixtral-Instruct-8x7B-v0.1 0.11 0.12 0.09 0.03 0.03 0.03 0.00 0.08 0.02 0.05 0.03 0.07 0.04 0.08 0.07 0.06 0.04 0.04 0.09 0.05 0.05 0.04 0.04 0.03
Xwin-LM-7B-v0.1 0.10 0.09 0.01 0.04 0.09 0.07 0.08 0.00 0.06 0.05 0.02 0.03 0.06 0.07 0.00 0.05 0.07 0.05 0.07 0.04 0.02 0.03 0.03 0.03
WizardLM-13B-v1.2 0.14 0.11 0.09 0.01 0.01 0.07 0.02 0.06 0.00 0.05 0.01 0.02 0.00 0.09 0.13 0.09 0.10 0.08 0.07 0.02 0.04 0.03 0.04 0.04
GPT-35-Turbo-1106 0.14 0.03 0.12 0.07 0.04 0.01 0.05 0.05 0.05 0.00 0.00 0.05 0.02 0.04 0.07 0.06 0.15 0.08 0.07 0.04 0.05 0.04 0.04 0.04
Zephyr-7B-Beta 0.07 0.11 0.08 0.07 0.03 0.02 0.03 0.02 0.01 0.00 0.00 0.03 0.05 0.04 0.04 0.08 0.02 0.08 0.07 0.05 0.04 0.04 0.03 0.03
Vicuna-33B-v1.3 0.11 0.08 0.06 0.05 0.02 0.09 0.07 0.03 0.02 0.05 0.03 0.00 0.00 0.05 0.06 0.06 0.03 0.02 0.04 0.04 0.04 0.05 0.05 0.04
Vicuna-13B-v1.5 0.07 0.12 0.13 0.06 0.08 0.02 0.04 0.06 0.00 0.02 0.05 0.00 0.00 0.02 0.02 0.01 0.11 0.08 0.08 0.04 0.06 0.05 0.06 0.05
Vicuna-7B-v1.5 0.07 0.04 0.06 0.09 0.06 0.07 0.08 0.07 0.09 0.04 0.04 0.05 0.02 0.00 0.00 0.01 0.12 0.10 0.06 0.06 0.04 0.05 0.06 0.05
Falcon-Instruct-40B 0.06 0.05 0.08 0.11 0.05 0.06 0.07 0.00 0.13 0.07 0.04 0.06 0.02 0.00 0.00 0.01 0.18 0.08 0.13 0.03 0.07 0.08 0.07 0.07
MPT-Chat-30B 0.02 0.01 0.03 0.10 0.04 0.05 0.06 0.05 0.09 0.06 0.08 0.06 0.01 0.01 0.01 0.00 0.06 0.09 0.05 0.07 0.10 0.12 0.11 0.11
Falcon-Instruct-7B 0.02 0.02 0.09 0.06 0.08 0.04 0.04 0.07 0.10 0.15 0.02 0.03 0.11 0.12 0.18 0.06 0.00 0.02 0.13 0.13 0.15 0.15 0.16 0.16
MPT-Chat-7B 0.02 0.04 0.05 0.07 0.06 0.04 0.04 0.05 0.08 0.08 0.08 0.02 0.08 0.10 0.08 0.09 0.02 0.00 0.04 0.17 0.19 0.20 0.16 0.19
Alpaca-7B 0.03 0.05 0.04 0.07 0.05 0.04 0.09 0.07 0.07 0.07 0.07 0.04 0.08 0.06 0.13 0.05 0.13 0.04 0.00 0.18 0.18 0.21 0.22 0.21
LLaMA-1-33B 0.01 0.00 0.04 0.02 0.03 0.03 0.05 0.04 0.02 0.04 0.05 0.04 0.04 0.06 0.03 0.07 0.13 0.17 0.18 0.00 0.14 0.22 0.38 0.35
LLaMA-1-13B 0.01 0.00 0.02 0.03 0.03 0.00 0.05 0.02 0.04 0.05 0.04 0.04 0.06 0.04 0.07 0.10 0.15 0.19 0.18 0.14 0.00 0.02 0.37 0.35
LLaMA-1-7B 0.00 0.02 0.03 0.03 0.03 0.02 0.04 0.03 0.03 0.04 0.04 0.05 0.05 0.05 0.08 0.12 0.15 0.20 0.21 0.22 0.02 0.00 0.40 0.36
WizardLM-7B-v1.0 0.00 0.02 0.02 0.03 0.03 0.03 0.04 0.03 0.04 0.04 0.03 0.05 0.06 0.06 0.07 0.11 0.16 0.16 0.22 0.38 0.37 0.40 0.00 0.04
Alpaca-13B 0.01 0.02 0.02 0.02 0.03 0.03 0.03 0.03 0.04 0.04 0.03 0.04 0.05 0.05 0.07 0.11 0.16 0.19 0.21 0.35 0.35 0.36 0.04 0.00
Figure9: Visualizationofthecompletewin-ratedifferencemap.
19RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
Table7: MCQAevaluationon0-shotMMLU(Hendrycksetal.,2020).
Choices Vocab Normalized
Model Size Choices Vocab Alignment PPL
(Circular) (Circular) Alignment
MPT 7B 29.5 0.5 29.6 0.4 34.1 35.1 35.6
(Teametal.,2023a;b) 30B 45.1 19.3 44.8 19.0 33.2 33.5 34.0
MPT-Chat
30B 50.1 28.9 50.0 28.9 34.7 35.2 35.9
(Teametal.,2023b)
Falcon 7B 25.0 0.0 24.8 0.0 33.7 34.3 35.1
(Almazroueietal.,2023) 40B 53.6 28.2 53.3 28.0 38.0 38.0 38.7
7B 32.2 1.7 30.5 1.5 33.0 33.5 33.1
LLaMA-1 13B 43.5 14.4 43.1 14.3 34.4 35.0 34.4
(Touvronetal.,2023a) 30B 54.7 31.6 54.5 31.6 36.1 36.1 35.8
65B 59.4 37.1 59.2 36.9 37.0 36.6 36.4
7B 41.8 12.3 39.4 12.1 33.3 34.1 33.6
LLaMA-2
13B 52.1 24.6 51.8 24.1 35.3 35.8 34.9
(Touvronetal.,2023b)
70B 65.4 45.1 65.4 45.0 39.0 37.8 37.8
7B 45.3 17.9 45.0 17.8 33.7 34.8 34.7
LLaMA-2-Chat
13B 53.1 28.1 53.2 28.1 35.9 36.7 36.7
(Touvronetal.,2023b)
70B 61.1 38.9 61.1 38.9 38.9 38.7 38.8
WizardLM 13B 53.4 30.2 53.4 30.2 35.6 36.1 35.9
(Xuetal.,2023) 70B 62.7 42.1 62.6 42.0 38.2 37.2 37.6
Xwin-LM 7B 45.5 16.1 45.5 16.1 33.8 34.3 34.1
(Team) 13B 53.9 27.2 53.9 27.1 36.6 36.5 35.9
Alpaca 7B 40.8 13.7 40.7 13.7 34.7 35.3 35.4
(Taorietal.,2023a) 13B 39.5 14.8 37.8 13.7 38.0 37.4 38.7
7B 48.8 25.0 48.8 24.9 33.6 34.2 34.1
Vicuna
13B 54.5 33.5 54.5 33.4 35.7 35.8 35.5
(Chiangetal.,2023)
33B 57.1 36.3 57.0 36.3 36.0 36.1 36.0
20RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
Table8: MCQAevaluationon0-shotHellaSwag(Zellersetal.,2019).
Choices Vocab Normalized
Model Size Choices Vocab Alignment PPL
(Circular) (Circular) Alignment
MPT 7B 28.1 0.8 28.1 0.6 57.1 76.3 75.8
(Teametal.,2023a;b) 30B 32.8 3.3 32.8 2.9 60.4 79.9 79.3
MPT-Chat
30B 46.9 14.9 46.9 15.2 61.5 80.1 79.7
(Teametal.,2023b)
Falcon 7B 25.1 0.0 24.1 0.0 57.7 76.3 75.9
(Almazroueietal.,2023) 40B 48.7 16.7 48.3 16.5 64.0 82.8 82.6
7B 29.1 1.1 28.9 1.1 56.9 76.2 74.8
LLaMA-1 13B 33.1 3.2 33.1 3.2 59.9 79.1 78.1
(Touvronetal.,2023a) 30B 44.3 14.7 44.4 14.8 63.3 82.6 81.2
65B 46.2 12.7 45.8 12.7 64.5 84.1 82.9
7B 32.1 1.2 30.0 1.2 57.2 76.0 75.0
LLaMA-2
13B 49.7 17.2 48.6 16.6 60.1 79.4 78.2
(Touvronetal.,2023b)
70B 62.4 31.1 62.1 31.0 64.8 83.8 82.7
7B 50.5 16.9 42.1 13.7 57.7 75.4 75.1
LLaMA-2-Chat
13B 63.5 34.9 63.5 34.8 60.7 79.7 78.7
(Touvronetal.,2023b)
70B 75.2 55.3 75.2 55.3 63.8 82.2 81.3
WizardLM 13B 66.5 44.4 66.5 44.4 61.5 79.8 79.1
(Xuetal.,2023) 70B 70.9 46.7 70.9 46.7 64.8 82.1 81.5
Xwin-LM 7B 39.6 6.0 39.6 6.0 58.8 76.8 76.0
(Team) 13B 56.1 22.7 56.1 22.6 62.1 80.9 79.8
Alpaca 7B 37.3 7.7 37.2 7.7 59.1 75.6 75.3
(Taorietal.,2023a) 13B 38.9 11.3 38.7 11.3 60.6 78.3 77.8
7B 55.8 30.2 55.9 30.2 56.4 73.8 73.0
Vicuna
13B 61.2 32.4 61.2 32.5 59.6 77.5 76.8
(Chiangetal.,2023)
33B 65.1 35.1 65.1 35.2 61.9 80.4 79.6
21RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
Table9: MCQAevaluationon0-shotARC-Challenge(Clarketal.,2018).
Choices Vocab Normalized
Model Size Choices Vocab Alignment PPL
(Circular) (Circular) Alignment
MPT 7B 31.8 0.3 30.4 0.3 40.0 41.8 39.8
(Teametal.,2023a;b) 30B 51.0 22.4 50.3 21.0 46.9 49.4 49.6
MPT-Chat
30B 66.0 43.2 65.5 42.7 50.0 51.0 50.8
(Teametal.,2023b)
Falcon 7B 27.5 0.0 27.0 0.0 40.3 43.6 42.9
(Almazroueietal.,2023) 40B 64.2 41.7 64.2 41.6 50.3 54.5 52.6
7B 38.9 1.9 38.1 2.0 41.9 44.6 41.4
LLaMA-1 13B 52.4 21.3 51.5 20.6 46.4 47.8 47.2
(Touvronetal.,2023a) 30B 69.4 48.4 69.2 48.3 52.9 52.9 50.8
65B 74.3 52.6 74.3 52.6 52.8 55.7 52.5
7B 45.9 16.4 44.8 15.3 43.4 46.2 43.0
LLaMA-2
13B 63.4 30.5 62.5 29.3 48.4 49.2 47.7
(Touvronetal.,2023b)
70B 81.7 67.1 81.7 67.1 54.4 57.3 53.8
7B 55.8 23.0 55.7 23.0 44.1 44.5 42.7
LLaMA-2-Chat
13B 64.3 38.1 64.2 37.9 46.2 50.3 46.9
(Touvronetal.,2023b)
70B 78.2 64.2 78.1 64.2 53.0 54.4 51.1
WizardLM 13B 65.5 43.4 65.5 43.3 47.0 50.1 47.1
(Xuetal.,2023) 70B 80.6 65.7 80.6 65.6 53.2 56.7 53.5
Xwin-LM 7B 54.9 23.5 54.9 23.5 45.9 47.6 44.7
(Team) 13B 67.4 36.8 67.2 35.9 52.0 53.0 51.6
Alpaca 7B 48.5 17.2 48.2 17.0 44.9 46.5 45.1
(Taorietal.,2023a) 13B 47.4 22.8 47.2 22.8 46.1 48.4 46.8
7B 60.4 33.9 60.6 33.9 43.3 45.7 42.1
Vicuna
13B 70.0 48.0 70.0 48.1 47.8 50.7 48.8
(Chiangetal.,2023)
33B 71.2 49.9 71.1 49.6 50.2 51.6 50.7
22RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
Table10: MCQAevaluationon0-shotARC-Easy(Clarketal.,2018).
Choices Vocab Normalized
Model Size Choices Vocab Alignment PPL
(Circular) (Circular) Alignment
MPT 7B 36.4 0.8 35.9 0.6 74.7 70.2 69.3
(Teametal.,2023a;b) 30B 70.0 42.2 69.9 40.7 78.7 76.3 74.0
MPT-Chat
30B 79.4 62.6 79.5 62.7 80.2 77.4 75.7
(Teametal.,2023b)
Falcon 7B 29.2 0.0 29.0 0.0 74.7 70.8 69.9
(Almazroueietal.,2023) 40B 80.0 61.7 80.0 61.6 81.9 79.3 77.7
7B 44.7 5.2 43.8 4.6 75.3 72.9 67.7
LLaMA-1 13B 67.6 36.7 66.7 35.4 77.4 74.8 69.2
(Touvronetal.,2023a) 30B 84.6 69.6 84.7 69.6 80.4 79.0 73.5
65B 87.7 73.0 87.7 72.9 81.3 79.8 75.1
7B 59.0 26.8 58.2 25.8 76.3 74.6 68.9
LLaMA-2
13B 77.4 53.7 76.6 52.1 79.4 77.5 71.9
(Touvronetal.,2023b)
70B 92.8 84.5 92.8 84.4 82.7 81.0 75.9
7B 70.9 44.9 70.8 44.9 74.4 69.7 66.9
LLaMA-2-Chat
13B 79.9 62.0 79.9 62.0 77.5 73.7 70.5
(Touvronetal.,2023b)
70B 92.8 84.5 92.8 84.4 82.7 81.0 75.9
WizardLM 13B 81.9 67.2 81.9 67.2 78.5 74.2 69.7
(Xuetal.,2023) 70B 92.1 83.9 92.1 83.9 81.9 77.9 73.6
Xwin-LM 7B 69.7 41.2 69.7 41.1 76.9 74.3 68.6
(Team) 13B 80.5 60.7 80.3 60.1 81.2 78.0 73.7
Alpaca 7B 67.7 36.2 67.2 35.6 74.8 70.6 66.9
(Taorietal.,2023a) 13B 67.2 41.3 67.0 40.9 76.4 71.7 68.0
7B 75.9 55.0 75.9 55.0 75.6 71.3 66.6
Vicuna
13B 83.8 71.4 83.8 71.4 78.7 74.8 70.5
(Chiangetal.,2023)
33B 85.9 70.7 85.6 69.6 78.4 74.7 70.2
23RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
Table11: MCQAevaluationon0-shotBoolQ(Clarketal.,2019).
Choices Vocab Normalized
Model Size Choices Vocab Alignment PPL
(Circular) (Circular) Alignment
MPT 7B 62.1 0.6 57.2 0.2 74.3 67.9 74.3
(Teametal.,2023a;b) 30B 69.1 47.9 30.6 19.8 74.1 69.2 74.1
MPT-Chat
30B 65.6 40.1 61.6 32.0 80.1 81.6 80.1
(Teametal.,2023b)
Falcon 7B 57.3 28.1 45.4 20.5 73.6 64.5 73.6
(Almazroueietal.,2023) 40B 67.7 22.6 67.5 22.2 82.0 71.4 82.0
7B 59.9 34.1 50.9 28.0 75.0 66.6 75.0
LLaMA-1 13B 64.3 22.5 53.1 14.2 78.0 65.6 78.0
(Touvronetal.,2023a) 30B 78.5 70.8 77.2 68.6 82.7 70.2 82.7
65B 79.9 67.4 71.4 59.1 84.8 74.6 84.8
7B 63.2 46.7 24.7 16.3 77.7 64.9 77.7
LLaMA-2
13B 66.9 10.1 49.8 4.7 80.6 66.2 80.6
(Touvronetal.,2023b)
70B 85.0 76.5 85.0 76.5 83.7 70.9 83.7
7B 63.6 21.2 63.6 21.2 80.7 79.4 80.7
LLaMA-2-Chat
13B 71.9 39.3 71.9 39.2 81.7 83.1 81.7
(Touvronetal.,2023b)
70B 75.6 57.8 75.6 57.8 86.7 82.8 86.7
WizardLM 13B 76.5 51.7 76.5 51.7 84.0 77.7 84.0
(Xuetal.,2023) 70B 89.0 84.5 89.0 84.5 86.6 77.5 86.6
Xwin-LM 7B 64.2 40.2 63.8 39.8 79.3 64.8 79.3
(Team) 13B 68.5 15.1 68.3 14.9 83.5 72.0 83.5
Alpaca 7B 64.2 38.8 64.1 38.7 77.5 79.4 77.5
(Taorietal.,2023a) 13B 68.0 61.2 68.0 61.0 78.8 74.3 78.8
7B 67.7 54.0 67.7 54.0 80.9 68.0 80.9
Vicuna
13B 82.1 62.6 82.1 62.6 85.2 80.0 85.2
(Chiangetal.,2023)
33B 80.2 74.0 27.9 23.0 83.9 76.1 83.9
24RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
Table12: MCQAevaluationon0-shotSIQA(Sapetal.,2019).
Choices Vocab Normalized
Model Size Choices Vocab Alignment PPL
(Circular) (Circular) Alignment
MPT 7B 37.7 2.4 38.6 2.0 45.6 48.2 45.9
(Teametal.,2023a;b) 30B 58.5 34.9 57.3 31.9 46.0 48.9 45.6
MPT-Chat
30B 63.4 42.5 63.2 41.6 48.8 49.6 48.3
(Teametal.,2023b)
Falcon 7B 36.5 1.2 36.5 1.2 45.4 48.7 47.0
(Almazroueietal.,2023) 40B 65.6 46.8 65.6 46.7 49.2 51.0 47.8
7B 44.6 11.5 44.5 11.6 44.8 47.0 44.3
LLaMA-1 13B 53.0 14.8 53.2 14.8 45.0 48.0 44.3
(Touvronetal.,2023a) 30B 66.1 49.0 66.0 48.8 45.7 49.4 45.2
65B 67.2 67.1 49.7 49.8 47.5 50.1 46.3
7B 51.4 21.2 51.0 20.6 43.5 47.3 44.2
LLaMA-2
13B 58.5 36.4 58.3 36.1 44.7 48.4 44.8
(Touvronetal.,2023b)
70B 70.9 55.3 70.8 55.2 46.3 49.4 45.3
7B 56.9 31.8 56.9 31.9 46.1 48.6 45.1
LLaMA-2-Chat
13B 62.6 41.6 62.7 41.5 49.2 50.3 46.0
(Touvronetal.,2023b)
70B 67.2 46.7 67.2 46.7 49.2 50.6 47.4
WizardLM 13B 66.7 51.5 66.7 51.6 49.3 49.2 46.7
(Xuetal.,2023) 70B 73.8 59.1 73.8 59.1 49.8 49.8 47.9
Xwin-LM 7B 56.9 28.1 56.9 28.0 47.5 49.3 45.5
(Team) 13B 56.8 34.3 56.7 34.1 48.5 48.4 45.4
Alpaca 7B 50.7 23.8 50.7 23.8 48.3 49.2 45.3
(Taorietal.,2023a) 13B 56.8 34.3 56.7 34.1 48.5 48.4 45.4
7B 63.3 42.5 63.3 42.3 45.8 47.6 44.3
Vicuna
13B 67.2 50.3 67.1 50.5 46.6 47.3 44.6
(Chiangetal.,2023)
33B 63.2 42.2 62.5 41.1 46.6 48.2 46.0
25RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
Table13: MCQAevaluationon0-shotPIQA(Bisketal.,2020).
Choices Vocab Normalized
Model Size Choices Vocab Alignment PPL
(Circular) (Circular) Alignment
MPT 7B 55.3 16.0 54.2 15.6 79.3 80.7 80.1
(Teametal.,2023a;b) 30B 64.9 39.8 64.7 39.6 80.1 81.1 81.1
MPT-Chat
30B 65.1 34.4 62.6 33.4 79.2 81.0 80.8
(Teametal.,2023b)
Falcon 7B 50.4 10.2 49.8 10.0 79.3 80.4 80.0
(Almazroueietal.,2023) 40B 69.7 44.6 69.5 44.4 82.3 83.1 82.9
7B 53.8 10.9 53.5 10.9 78.7 79.2 78.8
LLaMA-1 13B 64.1 32.3 63.2 32.0 79.2 80.2 79.6
(Touvronetal.,2023a) 30B 73.3 52.1 73.3 52.0 81.0 82.2 81.3
65B 66.2 33.9 66.2 33.9 81.3 82.3 82.3
7B 59.4 21.4 58.8 20.8 78.1 79.1 78.6
LLaMA-2
13B 71.7 46.0 71.4 45.9 79.1 80.5 80.4
(Touvronetal.,2023b)
70B 76.1 56.8 76.1 56.8 82.2 82.7 82.8
7B 65.0 35.1 64.4 34.8 76.8 76.7 76.7
LLaMA-2-Chat
13B 70.7 45.1 70.1 44.5 77.6 79.1 78.3
(Touvronetal.,2023b)
70B 78.0 58.9 76.9 57.4 80.6 80.8 80.7
WizardLM 13B 75.7 56.4 75.7 56.4 79.0 79.4 79.7
(Xuetal.,2023) 70B 82.4 70.3 82.2 70.0 81.1 80.7 81.0
Xwin-LM 7B 67.5 40.0 66.8 39.5 78.0 78.5 78.7
(Team) 13B 69.2 41.9 68.8 41.6 79.2 80.8 80.0
Alpaca 7B 64.5 35.5 64.5 35.5 77.8 78.0 77.7
(Taorietal.,2023a) 13B 63.1 32.4 63.0 32.0 78.1 78.6 77.4
7B 72.2 48.3 72.2 48.3 77.3 78.0 78.3
Vicuna
13B 75.3 56.4 75.3 56.4 78.9 79.2 79.1
(Chiangetal.,2023)
33B 73.8 53.3 70.0 50.7 79.2 79.4 79.5
26RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
Table14: MCQAevaluationon0-shotAGIEval(Englishonly)(Zhongetal.,2023).
Choices Vocab Normalized
Model Size Choices Vocab Alignment PPL
(Circular) (Circular) Alignment
MPT 7B 23.5 0.0 21.8 0.0 25.7 26.9 26.9
(Teametal.,2023a;b) 30B 24.0 0.7 24.0 0.6 27.7 28.9 28.9
MPT-Chat
30B 29.1 3.2 29.0 3.2 28.2 29.7 29.1
(Teametal.,2023b)
Falcon 7B 22.3 0.0 20.9 0.0 25.2 27.2 26.7
(Almazroueietal.,2023) 40B 28.8 3.0 28.2 3.0 28.5 30.8 31.4
7B 22.0 0.0 21.9 0.0 25.8 27.7 26.9
LLaMA-1 13B 26.6 1.1 26.4 1.1 28.0 29.5 29.7
(Touvronetal.,2023a) 30B 33.1 7.8 32.8 7.7 30.0 31.2 31.0
65B 38.7 10.1 37.9 10.0 30.4 32.7 32.3
7B 24.4 0.6 22.1 0.5 26.4 29.1 28.7
LLaMA-2
13B 33.9 8.0 34.0 8.0 28.0 30.3 30.1
(Touvronetal.,2023b)
70B 50.0 26.2 49.9 26.2 31.7 34.1 34.6
7B 28.3 2.9 17.3 1.8 25.9 27.1 26.6
LLaMA-2-Chat
13B 35.9 9.7 35.5 9.7 26.5 29.2 28.8
(Touvronetal.,2023b)
70B 45.9 17.6 45.8 17.6 29.7 31.1 32.1
WizardLM 13B 37.6 8.6 37.5 8.6 27.1 29.6 30.0
(Xuetal.,2023) 70B 48.2 22.5 47.6 21.9 30.6 32.6 32.7
Xwin-LM 7B 30.9 3.1 29.5 2.9 28.1 29.9 29.5
(Team) 13B 35.4 10.4 35.4 10.4 29.0 31.0 30.8
Alpaca 7B 24.1 0.7 23.9 0.7 26.8 28.9 27.7
(Taorietal.,2023a) 13B 27.8 3.1 27.2 2.9 26.5 28.7 28.6
7B 35.5 10.9 33.4 10.3 26.6 28.1 28.2
Vicuna
13B 38.6 14.7 37.1 14.5 27.5 29.6 28.3
(Chiangetal.,2023)
33B 39.9 14.4 39.7 14.3 28.6 30.3 31.1
27RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
Table15: MCQAevaluationon0-shotOpenBookQA(withfact)(Mihaylovetal.,2018).
Choices Vocab Normalized
Model Size Choices Vocab Alignment PPL
(Circular) (Circular) Alignment
MPT 7B 39.0 1.6 38.8 1.4 41.6 52.4 52.8
(Teametal.,2023a;b) 30B 70.4 43.0 69.6 41.8 45.2 53.8 55.4
MPT-Chat
30B 77.8 58.6 77.4 58.0 47.0 54.4 55.2
(Teametal.,2023b)
Falcon 7B 28.2 0.0 28.0 0.0 43.2 53.0 54.2
(Almazroueietal.,2023) 40B 77.2 61.0 77.2 61.0 48.0 55.6 58.4
7B 49.2 17.8 48.2 16.8 44.8 52.4 54.0
LLaMA-1 13B 64.8 32.8 64.6 32.4 44.4 53.4 53.0
(Touvronetal.,2023a) 30B 81.6 66.6 81.6 66.6 44.0 54.6 55.4
65B 82.8 67.6 82.4 67.0 46.0 54.6 57.2
7B 62.6 28.8 61.8 27.8 44.6 52.6 53.4
LLaMA-2
13B 72.0 50.2 71.8 48.8 44.4 54.2 54.0
(Touvronetal.,2023b)
70B 88.4 79.2 86.6 77.2 47.6 56.6 58.0
7B 73.8 52.4 73.8 52.4 48.4 54.8 55.0
LLaMA-2-Chat
13B 80.0 63.4 80.0 63.4 48.6 56.4 59.0
(Touvronetal.,2023b)
70B 86.0 76.4 86.0 76.4 48.6 58.6 59.0
WizardLM 13B 79.4 65.0 79.4 65.2 48.0 56.8 57.0
(Xuetal.,2023) 70B 86.6 75.6 86.6 75.6 48.8 57.8 57.0
Xwin-LM 7B 67.2 37.2 67.0 36.8 46.0 56.2 54.4
(Team) 13B 78.0 53.2 78.0 53.0 44.8 55.4 55.0
Alpaca 7B 73.8 48.8 73.4 47.6 47.4 54.4 56.4
(Taorietal.,2023a) 13B 73.0 52.4 72.6 51.8 47.4 55.6 55.8
7B 79.0 79.2 63.0 63.2 47.2 54.0 54.8
Vicuna
13B 79.4 65.0 79.4 65.2 48.0 56.8 57.0
(Chiangetal.,2023)
33B 86.6 75.6 86.6 75.6 48.8 57.8 57.0
28RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
Table16: MCQAevaluationon0-shotCommonSenseQA(Talmoretal.,2018).
Choices Vocab Normalized
Model Size Choices Vocab Alignment PPL
(Circular) (Circular) Alignment
MPT 7B 22.9 0.3 22.8 0.2 56.5 49.1 47.2
(Teametal.,2023a;b) 30B 40.5 10.9 40.1 10.3 61.5 55.0 51.4
MPT-Chat
30B 64.1 39.0 64.1 38.9 64.0 54.6 52.5
(Teametal.,2023b)
Falcon 7B 20.7 0.0 20.9 0.0 57.9 50.0 50.0
(Almazroueietal.,2023) 40B 62.9 33.5 62.8 33.4 62.5 54.9 52.2
7B 33.2 0.3 31.8 0.3 58.1 50.9 45.7
LLaMA-1 13B 54.2 21.9 54.0 21.9 59.8 51.8 45.2
(Touvronetal.,2023a) 30B 65.4 40.2 65.4 40.2 61.3 54.5 48.6
65B 64.4 38.9 64.0 37.9 63.6 55.6 50.5
7B 34.6 3.2 34.6 3.2 58.6 51.9 46.9
LLaMA-2
13B 57.7 28.5 55.4 25.5 61.9 53.9 48.4
(Touvronetal.,2023b)
70B 69.8 46.7 69.3 45.5 64.7 55.3 50.1
7B 60.1 33.4 60.1 33.4 57.7 50.2 45.9
LLaMA-2-Chat
13B 65.3 37.6 65.3 37.6 58.7 50.0 46.2
(Touvronetal.,2023b)
70B 74.9 55.2 74.9 55.2 61.0 54.7 48.5
WizardLM 13B 67.0 42.4 67.0 42.4 60.0 50.3 47.8
(Xuetal.,2023) 70B 74.4 55.3 74.4 55.3 60.4 53.9 49.8
Xwin-LM 7B 50.9 18.3 50.9 18.3 61.1 51.4 46.9
(Team) 13B 62.2 30.2 62.2 30.1 63.6 54.1 50.1
Alpaca 7B 54.3 21.7 54.1 21.0 57.8 51.7 47.7
(Taorietal.,2023a) 13B 56.8 29.0 56.6 28.9 60.5 50.9 46.6
7B 60.5 39.2 60.7 39.1 57.5 49.8 45.8
Vicuna
13B 67.0 51.6 67.1 51.6 61.6 51.4 47.7
(Chiangetal.,2023)
33B 69.3 46.7 69.2 46.5 60.3 50.5 46.8
29RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
Table17: MCQAevaluationon0-shotRACE(all)(Laietal.,2017).
Choices Vocab Normalized
Model Size Choices Vocab Alignment PPL
(Circular) (Circular) Alignment
MPT 7B 29.2 0.5 29.7 0.5 46.0 49.1 49.0
(Teametal.,2023a;b) 30B 56.8 25.9 56.6 25.7 48.5 51.7 51.2
MPT-Chat
30B 69.2 49.7 69.4 50.0 50.4 53.5 53.2
(Teametal.,2023b)
Falcon 7B 26.5 0.0 25.3 0.0 44.1 47.3 46.6
(Almazroueietal.,2023) 40B 66.9 43.1 65.6 42.2 49.3 52.5 51.7
7B 36.8 3.4 33.6 2.8 46.3 50.2 48.2
LLaMA-1 13B 54.7 19.6 42.9 11.2 48.6 51.7 49.8
(Touvronetal.,2023a) 30B 70.2 47.2 70.0 47.1 49.6 53.0 50.8
65B 75.7 54.1 71.1 50.5 52.3 55.5 53.3
7B 48.2 13.0 44.4 11.5 46.8 50.2 47.9
LLaMA-2
13B 65.9 37.3 64.0 36.0 48.1 51.9 49.5
(Touvronetal.,2023b)
70B 84.8 74.2 83.3 72.3 51.9 55.3 53.1
7B 64.6 37.1 64.2 36.9 49.7 52.4 50.6
LLaMA-2-Chat
13B 72.4 49.8 72.2 49.8 53.3 56.0 54.7
(Touvronetal.,2023b)
70B 83.9 71.0 83.9 71.0 56.0 57.8 56.7
WizardLM 13B 73.5 57.2 73.5 57.1 52.7 55.9 54.3
(Xuetal.,2023) 70B 83.2 73.1 83.2 73.0 55.6 57.5 56.3
Xwin-LM 7B 58.2 23.6 58.1 23.5 49.9 51.9 50.8
(Team) 13B 71.2 44.4 71.1 44.2 51.3 54.4 53.0
Alpaca 7B 53.7 23.8 53.5 23.3 49.4 52.0 50.9
(Taorietal.,2023a) 13B 55.0 31.2 54.7 30.6 49.8 52.9 51.5
7B 69.3 49.9 69.3 49.9 47.6 50.7 48.8
Vicuna
13B 75.7 60.7 75.7 60.7 49.4 52.8 50.8
(Chiangetal.,2023)
33B 77.8 63.2 77.7 63.0 50.6 53.4 52.5
30RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
Table18: AnexamplewhereGPT-4generatesaccuratejustifications.
Question Whatisthebestwaytoformanewhabit?
1. Startsmall: Beginwithasimpleandachievablegoal.
2. Makeitaroutine: Establishaspecifictimeandplaceforthehabit.
3. Trackyourprogress: Keeptrackofyourprogresstostaymotivated.
LLM-1’sResponse
4. Rewardyourself: Rewardyourselfforeverysuccessfulattempt.
(Vicuna13Bv1.5)
5. Stayconsistent: Sticktothehabitevenifyoumissaday.
6. Getsupport: Shareyourgoalwithfriendsorfamilyforsupport.
7. Reflectonyourprogress: Reflectonyourprogressandadjustyourapproachifneeded.
1. Startsmallandmakeitspecific.
2. Makeitadailyroutine.
LLM-2’sResponse
3. Getanaccountabilitypartner.
(Vicuna7Bv1.5)
4. Rewardyourselfforsuccess.
5. Beconsistentandpersistent.
GPT-4’sDecision Tie.
BothLLM-1andLLM-2providedresponsesthatareaccurateandrelevanttotheuser’squestion
aboutforminganewhabit. LLM-1’sresponseisslightlymorecomprehensive,includingseven
stepswithabitmoredetail,suchas’Trackyourprogress’and’Reflectonyourprogress.’LLM-2’s
responseismoreconcisewithfivesteps,butstillcoverstheessentialaspectsofhabitformation.
Bothresponsesareclearandeasytounderstand,adheringtotheclaritycriterion. Thereareno
GPT-4’sExplanation ethicalorlegalstandardsatriskinthiscontext,sobothcomplywiththecompliancecriterion.
Thetimelinesscriterionisnotparticularlyrelevanthere,asthestrategiesforhabitformationdo
notchangerapidlyovertime. Neitherresponsecontainsmisleadingorharmfulcontent,andboth
avoidbiasbyprovidinggeneraladviceapplicabletoanyonelookingtoformanewhabit. Given
that both responses are satisfactory and there is no significant difference in the quality of the
informationprovided,theevaluationresultsinatie.
Table19: AnexamplewhereGPT-4generatesaccuratejustifications.
Question What’syourfavoritepartofthevideofor“SomebodyThatIUsedtoKnow”byWalkoffthe
Earth?
AsanAI,Idon’thavepersonalpreferencesorfeelings,soIdon’thaveafavoritepart. However,
thevideofor“SomebodyThatIUsedtoKnow”byWalkofftheEarthiswidelyrecognizedforits
LLM-1’sResponse
uniqueandcreativeapproachtomusicperformance. Thebandmembersareseenplayingasingle
(GPT-4Turbo)
guitarsimultaneously,whichisanimpressivedisplayofcoordinationandmusicalarrangement.
Thisaspectofthevideooftenstandsouttoviewersandcouldbeconsideredahighlightbymany.
LLM-2’sResponse Thepartwherethey’reallplayingthesameguitarbutindifferentoctavesandpositions. It’sa
(Xwin-LM13Bv0.1) greatexampleofhowcreativeandinnovativetheyareasmusicians.
GPT-4’sDecision LLM-1wins.
BothLLM-1andLLM-2providedrelevantresponsestotheuser’squestionaboutthemusicvideo
for’SomebodyThatIUsedtoKnow’byWalkofftheEarth.LLM-1’sresponseismoreeffectiveas
itadherestothecriteriaofaccuracy,relevance,comprehensiveness,andclarity. LLM-1correctly
identifiesthatasanAI,itdoesnothavepersonalpreferences,andthenproceedstodescribea
commonlyappreciatedaspectofthevideo,whichistheband’scoordinationinplayingasingle
GPT-4’sExplanation guitar. Thisresponseiscomprehensiveasitprovidescontexttowhythatpartofthevideomight
beconsideredahighlight. LLM-2’sresponseisrelevantandaccuratebutlesscomprehensive,
asitonlymentionsthefactthatthebandmembersplaythesameguitarindifferentoctavesand
positionswithoutfurtherelaborationorcontext. Bothresponsesareclearandfreefrombias,and
neithercontainsharmfulcontent. However,LLM-1’sresponseismoreeffectiveoverallduetoits
comprehensivenature.
31RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
Table20: AnexampleofGPT-4providingincorrectjustifications. Weincludecommentstohighlighttheinaccuracies.
Question Writeanemailtoattendeesasareminderthattheeventiscomingup.
Subject: Reminder: OurExcitingEventisJustAroundtheCorner!
Dear[Name],
Wehopethisemailfindsyouwell. Withonlyafewdaysleftuntilourupcomingevent,wewanted
tosendyouafriendlyremindertosavethedate!
Join us on [Date]at [Time]at [Location]for an unforgettable experience filled with [activities,
performances,orkeynotespeakers]. Wehaveanamazinglineupof[insertdetails],andwecan’t
waitforyoutobeapartofit.
Don’t miss out on the opportunity to [insert what attendees will gain from the event, such as
LLM-1’sResponse
networking,learning,orfun]. Oureventissuretobeanexcitingandmemorableexperiencefor
(LLaMA-2 70B
allwhoattend
Chat)
Ifyouhaven’talready,pleaseRSVPby[Date]tosecureyourspot. Youcandosobyvisitingour
websiteorbyrespondingtothisemail.
We’relookingforwardtoseeingyouthere!
Bestregards,
[YourName]
[YourTitle]
[YourCompany]
[ContactInformation]
LLM-2’sResponse
[EmptyReponse]
(Alpaca13B)
GPT-4’sDecision Tie(bothbad).
BothLLM-1andLLM-2failedtoprovideasatisfactoryresponsebasedontheevaluationcriteria.
LLM-1providedadetailedandwell-structuredemailtemplatethatisrelevant,clear,andcom-
prehensive. However,itlacksspecificinformationsuchastheactualdate,time,location,details
GPT-4’sExplanation
abouttheactivities,andtheRSVPdeadline,whicharecrucialfortheaccuracyandtimelinessof
thecommunication. LLM-2didnotprovideanyresponseatall,whichfailsallcriteria. Therefore,
neitherresponsecanbedeemedeffective,resultinginatiewherebothresponsesareunsatisfactory.
Authors’Comment TherequestedemailtemplatehasbeensuccessfullyprovidedbyLLM-1. However,itshouldbe
notedthatGPT-4’scritiqueregardingtheabsenceofspecificinformationneedstobeaddressed
bytheuser,asLLM-1isnotequippedtodoso. Giventhislimitation,LLM-1shouldbe
acknowledgedasthewinner.
Table21: AnexampleofGPT-4providingincorrectjustifications. Weincludecommentstohighlighttheinaccuracies.
Question highestnumberbelow100,withoutusingthedigit9
LLM-1’sResponse
99
(Dolly12Bv2)
LLM-2’sResponse
Thehighestnumberbelow100withoutusingthedigit9is89.
(GPT-3.5Turbo)
GPT-4’sDecision Tie(bothbad).
LLM-1’sresponsefailstomeettheaccuracycriterionasitincludesthedigit9,whichisexplicitly
againsttheuser’sstipulation. LLM-2correctlyidentifies89asthehighestnumberbelow100that
GPT-4’sExplanation doesnotincludethedigit9,satisfyingtheaccuracyandrelevancecriteria. Bothresponsesare
clearandconcise,butLLM-1’sresponsedoesnotcomplywiththeuser’srequest,makingLLM-2
thewinner. Therearenoissueswithtimeliness,harmlessness,orbiasinthiscontext.
Authors’Comment GPT-4madethecorrectdecision;however,itsjustificationwaserroneous. Thestatement‘LLM-2
accuratelydetermined89tobethegreatestnumberunder100withoutthedigit9’isincorrect,as
89doesindeedcontainthedigit9.
32RethinkingGenerativeLargeLanguageModelEvaluationforSemanticComprehension
Table22: ExamplesfromeachsourceinourRWQbenchmark.
GoogleTrends
Example-1: “WhataresometipsforbeginnerstryingtomasterWordle? ”
Example-2: “CanyounamethreemaincharactersfromtheTVshow“StrangerThings”?”
Example-3: “DidMacMiller’smusicstyleinfluencethesearchestrendsinhiphop?”
Quora
Example-1: “Whataresomeessentialparentingskills?”
Example-2: “Whatisthemosthorrifyingnoiseyouhaveeverheard?”
Example-3: “Whatwouldhappenifeveryoneintheworldfellasleepatthesametime?”
AlpacaEval
Example-1: “Givemethelistoftop100techcategories.”
Example-2: “Howarecarbonfibersusedinbuildings.”
Example-3: “Writea5versesonginthestyleofTalkingHeadsbasedonthelifeofateenagerinthe1980sbritain.”
ShareGPT
Example-1: “WritemeabusinessplanformynewbasketballtrainingcompanycalledProXakaProfessionalExperi-
ence.”
Example-2: “Givemeaphysicsforproblemtotestunderstandingofvelocityandgravity.”
Example-3: “Iwantyoutoactasasocialmediacontentplannerforanewstartupcompany. Pleaseprovideideasfor
fiveInstagrampoststhatshowcasetheproductsindifferentways. Eachpostshouldhaveabriefcaptionthathighlights
theproduct’suniquefeaturesorbenefits.”
LMSYS-Chat-1M
Example-1: “Canyougivemeanexampleofawordthatisusedalmostexclusivelyinthecontextofanidiomatic
phraseanduncommonotherwise?”
Example-2: “Concludewhatisthemealtitlefromthisandwhatarethetotalcalories(ifthereisarange,choosehigher
number). Pleaseprintoutonlymealnameandtotalcalories. Actasanutritionistspecialisingineducatedguessing
oftotalcaloriesforanygivenmeal. Basedonlyontheinformationyouhave,makeyourbesteducatedguesswith
confidence. Ifyoulacksomeinformationformakingtheconclusion,pleaseguessit. Provideonlymealnameandtotal
caloriesandnoothertextorexplanation.”
Example-3: “Tellmethemostcommonfraudcasesinconsortium.”
33