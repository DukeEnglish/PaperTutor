Bridging Different Language Models and
Generative Vision Models for
Text-to-Image Generation
Shihao Zhao1, Shaozhe Hao1, Bojia Zi2, Huaizhe Xu3, and Kwan-Yee K. Wong1
1 The University of Hong Kong
{shzhao,szhao,kykwong}@cs.hku.hk
2 The Chinese University of Hong Kong
bjzi@se.cuhk.edu.hk
3 The Hong Kong University of Science and Technology
hxubr@connect.ust.hk
Abstract. Text-to-imagegenerationhasmadesignificantadvancements
with the introduction of text-to-image diffusion models. These models
typically consist of a language model that interprets user prompts and
a vision model that generates corresponding images. As language and
vision models continue to progress in their respective domains, there is
a great potential in exploring the replacement of components in text-
to-image diffusion models with more advanced counterparts. A broader
research objective would therefore be to investigate the integration of
any two unrelated language and generative vision models for text-to-
image generation. In this paper, we explore this objective and propose
LaVi-Bridge,apipelinethatenablestheintegrationofdiversepre-trained
language models and generative vision models for text-to-image genera-
tion.ByleveragingLoRAandadapters,LaVi-Bridgeoffersaflexibleand
plug-and-play approach without requiring modifications to the original
weights of the language and vision models. Our pipeline is compatible
with various language models and generative vision models, accommo-
datingdifferentstructures.Withinthisframework,wedemonstratethat
incorporatingsuperiormodules,suchasmoreadvancedlanguagemodels
orgenerativevisionmodels,resultsinnotableimprovementsincapabili-
tiesliketextalignmentorimagequality.Extensiveevaluationshavebeen
conductedtoverifytheeffectivenessofLaVi-Bridge.Codeisavailableat
https://github.com/ShihaoZhaoZSH/LaVi-Bridge.
Keywords: Diffusion model · Text-to-image generation
1 Introduction
Inrecentyears,therehavebeenremarkableadvancementsinthefieldoftext-to-
image generation, specifically through the use of diffusion models [9,18,43,45].
These models have made significant contributions and have gained considerable
attention for their exceptional performance. By leveraging large-scale training
4202
raM
21
]VC.sc[
1v06870.3042:viXra2 S. Zhao et al.
Encoder-Only
LanguageModels
U-Net-Based
GenerativeVisionModels
LaVi-Bridge
Encoder-Decoder
LanguageModels
Transformer-Based
GenerativeVisionModels
Decoder-Only
LanguageModels
Fig.1: Overview of LaVi-Bridge. LaVi-Bridge is capable of integrating various lan-
guagemodelsandgenerativevisionmodels.Ontheleftside,wekeepthevisionmodel
fixedandexperimentwithdifferentlanguagemodelsinourpipeline.Ontherightside,
we keep the language model fixed and try out different vision models. We display the
visualization results alongside each combination.
datasetsalongsidelargedeepmodels,text-to-imagediffusionmodelsarecapable
of producing high-quality images that faithfully align with the textual descrip-
tions provided by users. This has rendered them highly applicable in real-world
scenarios such as content creation and architectural design.
Text-to-image diffusion models [2,6,29,35,38,41,51] typically consist of two
key components, namely a language model and a generative vision model. The
language model is responsible for comprehending the input prompts, whereas
the vision model is tasked with generating images that align with the extracted
context.Existingtext-to-imagediffusionmodelsemployvariouslanguagemodels
and generative vision models and have gained widespread usage. For instance,
StableDiffusion(SD)[38]isahighlypopulartext-to-imagediffusionmodelthat
employs the CLIP text encoder [31] as its language model and a U-Net [39] as
its generative vision model. Another example is PixArt [6], a recently proposed
text-to-imagediffusionmodelthatadoptstheT5[34]asitslanguagemodeland
aVisionTransformer(ViT)[11]asitsgenerativevisionmodel.Thesemodelsare
trained on a vast amount of text-image pairs, enabling seamless collaboration
between their language modules and vision modules.
The advancements in deep language models and deep vision models have
witnessed rapid progress in recent years, with both fields experiencing continu-
ous developments and the introduction of more powerful models. However, this
rapid development poses a challenge for the research in text-to-image gener-
ation when it comes to integrating more advanced language or vision models
into existing text-to-image diffusion models. The problem of how to integrate
any two unrelated language and vision models is unexplored, and the impact of
newly developed models on text-to-image generation capabilities also remains
uncertain. The current situation highlights the presence of a gap between the
language or vision modules within text-to-image diffusion models and the state-
of-the-art models in their respective domains. Therefore, it has become crucial
to address this gap and explore ways to incorporate more advanced language
or vision models into existing text-to-image diffusion models. Furthermore, theLaVi-Bridge 3
broader challenge of integrating any pre-trained language model with any gen-
erative vision model deserves a thorough investigation.
In this paper, our objective is to delve into the aforementioned problem. We
propose LaVi-Bridge, a flexible framework that facilitates the integration of di-
versewell-trainedlanguagemodelsandgenerativevisionmodelstoachievetext-
to-image generation. Our framework enables the integration of two unrelated
language and vision models that have not been previously trained together, as
shown in Fig. 1. Importantly, LaVi-Bridge does not require modifying the orig-
inal weights of the language and vision models. Instead, it injects LoRA [20]
intothelanguageandvisionmodelsseparatelyandutilizesanadaptertobridge
these two modules. Moreover, LaVi-Bridge only necessitates a relatively small
dataset to integrate different language models and generative vision models for
text-to-image generation.
We summarize the advantages and features of LaVi-Bridge as follows:
1. LaVi-Bridge is designed for text-to-image diffusion models and serves as
a bridge, capable of connecting various pre-trained language models and
generative vision models. Our framework can accommodate different model
structures, including encoder-only, encoder-decoder, and decoder-only lan-
guage models, as well as U-Net-based and Transformer-based generative vi-
sion models.
2. LaVi-Bridge utilizes LoRA and adapters, eliminating the need to modify
the original weights of the models. It is more flexible and requires relatively
small computing resources compared to training the entire diffusion model.
3. We evaluated various text-image alignment and image quality metrics on
shortprompts,longprompts,andcompositionalprompts.Wealsoconducted
extensivevisualization.Wethendrewseveralconclusions.Forinstance,inte-
gratingsuperiormodelsleadstoimprovedperformanceinthecorresponding
modality,suchasenhancedsemanticunderstandingwithadvancedlanguage
modelsorimprovedimagequalitywithmorepowerfulgenerativevisionmod-
els. Additionally, the diffusion model utilizing Llama-2 demonstrates excep-
tionalsemanticunderstanding,whilethediffusionmodelutilizingthetrans-
former in PixArt yields images with enhanced aesthetics.
2 Related Work
2.1 Language Models and Generative Vision Models
The mainstream Large Language Models (LLMs) [8,32–34] are built based on
the transformer structure [49], with three main types of architectures, namely
encoder-only, encoder-decoder, and decoder-only. All these three belong to Se-
quencetoSequence(Seq2Seq)[47].Theencoder-onlyarchitectureisexemplified
by BERT [8]. CLIP text encoder [31] is based on BERT and further trained to
align with the image domain. Models of this type excel at understanding the
content of the input and generating outputs tailored to specific tasks. On the4 S. Zhao et al.
other hand, the encoder-decoder framework is adept at handling tasks that in-
volvecomplexmappingsbetweeninputandoutputsequences.Examplesinclude
T5 [34] and BART [24]. Recently, due to the tremendous success of ChatGPT,
attention has been drawn to models that consist solely of a decoder, like GPT-
3 [4] and Llama-2 [48]. The decoder-only architecture demonstrates exceptional
performance in semantic understanding. For text-to-image generation, all three
types of LLMs can provide effective semantic information to serve as conditions
for image generation in diffusion models. In this paper, we explore and compare
all these three types of language models.
Agenerativevisionmodelreferstoavisionmodelwiththeabilitytogenerate
images or visual contents. There are two common types of structures, namely
U-Net-based [39] and Transformer-based [11]. Generative Adversarial Networks
(GANs) [14,37,52] employ a framework consisting of a discriminator and a
generator, with the generator’s structure based on U-Net. On the other hand,
motivated by the success of GPT models, recent works have attempted to use
the Transformer architecture for image generation in an autoregressive manner,
with notable examples being DALLE [36] and CogView [10]. Another popular
class of generative models is diffusion models [7,18,43,45], which are based on
the diffusion process and gradually denoise to produce natural images. Early
diffusion models often employed U-Net as their generative vision model, such
as Stable Diffusion, which scaled up the Latent Diffusion Model (LDM) [38]
with larger data scales. Some recent works have started to replace the U-Net in
diffusion models with Vision Transformer and have made significant progress,
such as DiT [30], U-ViT [1] and PixArt [6]. In this paper, we focus on diffusion
models and explore both U-Net-based and Transformer-based vision models.
2.2 Text-to-Image Diffusion Models
Text-to-imagediffusionmodels[2,9,29,38,41,44]arecapableofgeneratingimages
based on user prompts. These models consist of two main components, namely
a language model and a vision model. The language module is responsible for
understanding the text input provided by the user, extracting contextual infor-
mation, and injecting it into the vision module to generate the desired image.
Text-to-image diffusion models have paved the way for various exciting research
areas,includingimageediting[3,22,27],controllableimagegeneration[28,53,54],
personalized object generation [12,16,40], as well as other interesting applica-
tions[5,13,15].Intheextensiveexplorationofdiffusionmodels,researchershave
utilized different language models and vision models. For instance, Stable Diffu-
sion[38]employsCLIPtextencoder[31]asitslanguagemodelandU-Netasits
vision model. Imagen [41] utilizes T5 [34] as its language model, which claims
to enhance both sample fidelity and image-text alignment. ParaDiffusion [50]
focuses on paragraph-to-image generation and leverages the powerful seman-
tic understanding capability of Llama-2 [48] to comprehend lengthy sentences.
PixArt[6],ontheotherhand,utilizesaViT[11]asitsvisionmodelandachieves
high image fidelity while being trained at a lower cost.LaVi-Bridge 5
After training on a large dataset of text-image pairs [42], the language and
vision models in the text-to-image diffusion model become closely intertwined.
This tight coupling ensures a strong alignment between the provided text de-
scriptionandthegeneratedimage,butatthesametimealsolimitstheflexibility
ofthediffusionmodel.Forinstance,ifamoreadvancedlanguageorvisionmodel
becomes available, it may have the potential to enhance the text-to-image task.
However, decoupling the language and vision modules in existing text-to-image
diffusionmodelsandreplacingamodulewithanewoneisnontrivial.Therefore,
thispaperexploresthedilemmafacedbytext-to-imagegenerationandproposes
a framework that enables efficient integration of various language models and
generative vision models.
3 Method
3.1 Preliminary
A diffusion model is based on the diffusion process for image generation. This
process consists of two stages, namely the forward process and the reverse pro-
cess. During the forward process, Gaussian noise is progressively added to a
natural image until the image becomes completely noisy. After that, during the
reverse process, the noise is gradually eliminated over a series of time steps,
resulting in a natural image. In the reverse process, a trainable vision model is
usedtopredictandremovethenoise.Byemployingthisdenoisingmodel,weare
abletoobtainanaturalimagefromGaussiannoisebydenoising.Withinatext-
to-imagediffusionmodel,therearetwocomponentsateachtimestep,namelya
languagemodelf andavisionmodelg.Thelanguagemodelconvertsuserinput
text y into embeddings, which capture the semantic meaning of the text. On
the other hand, the vision model, which is the denoising model aforementioned,
encodes image features z, extracting relevant visual information from the input
images.Theinteractionbetweentextembeddingsandimagefeaturesisachieved
through cross-attention layers, which can be formulated as
c=f(y), (1)
Q=W (z),K =W (c),V =W (c), (2)
q k v
CrossAttention(Q,K,V)=softmax(Q·KT)·V, (3)
where W ,W and W are projection matrices.
q k v
3.2 Language and Vision Alignment
LaVi-Bridge enables the integration of any two pre-trained language and gen-
erative vision models, even though these models are not related and have been
trainedseparately.Here,wedenotethelanguagemodelasf andthevisionmodel
asg,asmentionedpreviously.Ifwedirectlyinteractthetextualinformationand6 S. Zhao et al.
LoRA LoRA
LanguageModel Adapter GenerativeVisionModel
Encoder-Only U-Net-Based
Decoder-Only
Encoder-Decoder Transformer-Based
LanguageModelPool GenerativeVisionModelPool
Fig.2: Pipeline of LaVi-Bridge. We select one model each from the language and
vision model pools. We then freeze the pre-trained language and vision models and
incorporate LoRA into both models. The connection between the language and vision
models is established through an adapter. The only weights we need to train are the
ones introduced by LoRA and the adapter.
image information using Eq. (1), considering that f and g are trained indepen-
dently, the parameters in the cross-attention layers of g cannot comprehend the
text embedding output by f, resulting in meaningless model outputs.
To establish a connection between them, LaVi-Bridge keeps the pre-trained
language and vision models fixed and utilizes LoRA to introduce trainable pa-
rameters∆θintoboththelanguagemodelandthevisionmodel.Inthiscontext,
we denote the language and vision models with LoRA as fθ1+∆θ1 and gθ2+∆θ2,
where θ and θ are the original parameters of f and g, respectively. Further-
1 2
more,weintroduceanadapterasabridgebetweenthelanguagemodelandvision
modeltofacilitatebetteralignment.Theadapterconsistsofstackedfeedforward
layers,denotedash.Consequently,thecross-attentionlayercanbeexpressedas
c=fθ1+∆θ1(y), (4)
Q=Wθ2+∆θ2(z),K =W θ2+∆θ2(h(c)),V =Wθ2+∆θ2(h(c)), (5)
q k v
CrossAttention(Q,K,V)=softmax(Q·KT)·V. (6)
Now, we only need to train ∆θ , ∆θ , and h on a relatively small amount of
1 2
text-image pairs. After training, the language and generative vision models can
effectivelycollaboratetogeneratemeaningfulimages.Wepresenttheframework
of LaVi-Bridge in Fig. 2. LaVi-Bridge is very straightforward, with both LoRA
and the adapter being its crucial and indispensable components.
3.3 Design Details
LaVi-Bridge is designed to accommodate a wide range of language model struc-
tures, including encoder-only, encoder-decoder, decoder-only, as well as genera-
tive vision model structures such as U-Net and ViT. In the language model,
we inject LoRA into all linear layers of the attention layers. Likewise, in a
drawrofdeeF drawrofdeeFLaVi-Bridge 7
transformer-based vision model, LoRA is injected into all linear layers of the
attention layers. In a U-Net-based vision model, LoRA is injected into all linear
layers and convolutional layers of the ResBlocks, attention, and cross-attention
layers.Toaddressthedimensiondisparitybetweentheoutputembeddingofthe
language model and the dimensions handled by the cross-attention of the vision
model, we employ two feedforward layers for the adapter. The input dimension
of the adapter matches the output text embedding dimension of the language
model, while the output dimension aligns with the dimensions received by the
cross-attention of the vision model.
For training, we first select the language and generative vision models that
we choose to integrate. We keep their original weights fixed and train LoRA
and the adapter on text-image pairs following the design mentioned above. The
trainedLoRAandadapterhavefewerparameterscomparedtotheoriginalmodel
weights, which makes LaVi-Bridge highly flexible. For evaluation, we used var-
ious metrics to assess text alignment and image quality across short prompts,
long prompts, and compositional prompts.
4 Experiments
4.1 Experimental Settings
In this section, we explored the performance of different language models and
generativevisionmodelsunderLaVi-Bridge.WealsotestedtheimpactofLoRA
andadapters.Wetrainedonadatasetconsistingofatotalof1milliontext-image
pairs,includingaround600ktext-imagepairsfromtheCOCO2017[25]trainset
and 400k text-image pairs from an internal dataset with high-quality images
and captions. For each setting, we set the LoRA rank to 32, image resolution to
512×512 and the batch size to 256. We used the AdamW optimizer [26] with a
learning rate of 1×10−4 and trained for a total of 50k steps. During inference,
weemployedtheDDIMsampler[43]forsamplingwiththenumberoftimesteps
set to 50 and the classifier free guidance scale [19] set to 7.5.
As mentioned above, we conducted our quantitative evaluation on short
prompts, long prompts, and compositional prompts. Specifically,
1. Forshortprompts,weevaluatedusingtheCOCO2014[25]validationset.We
randomly sampled 30k images and tested image quality and text alignment
within this subset. We used FID [17] and aesthetic score [23] as evaluation
metrics for image quality and CLIP score for text alignment.
2. For long prompts, we employed the same 30k-subset of COCO2014 and uti-
lizedLlama-2togenerateexpandedcaptionsrangingfrom20to70wordsto
constructadatasetof30klongprompts.Sincethecaptionexpansionprocess
does not refer to the content of the reference image, we solely used aesthetic
score to evaluate image quality and CLIP score for text alignment.
3. For compositional prompts, we utilized the benchmark proposed by Comp-
bench [21]. Compositional prompts were mainly used to test the model’s8 S. Zhao et al.
Acute cat. mfrA paa o otcm ttr fe uci md er bdel olal ude vine e-aa smng,g ld ge i es rd nsteep tw e nl ai nno s ns tm dw,e oari ste rtn haid l o nls, an pf gi le n melA sat ss sort,i .it sac ae Htan nr et m dpde r yael rsy seis k n tec diit nem cs p an ,t b lact o ,e qtan thd p ueeed t a,ru r len ai r tdds i yln a i i gwn tgr hok iaht t hhh ehh eai rn ua i rdr r e a fmas o lts niro kre c me ena ea i . took ihnuef e sdb a p rbswo ouleki rrt n ce rh eedn s la aopi ll i v fo nj ue gr , xrcl a, ote daa sl da pp siy sopn sae a. innaT tir dh os e n gAl oan fs s si,l u lsu nts la it gnra hdt triin eo pg vn ie e oo ran cf lei naa t ghp h aue e dm tcie nla os ynt u a udhl n sea ,iam vir lelti urd m sms eta i wnad ae is tt t ho io nifr ng mt . r tay hn esse hlau e.c aRe rna t,yt s tdheepD sici hgtoeit rda e lw .i l Aliut shys wa tbr raaenyti gsi go ue en nny oa tslrf ey na t, ,s a b wone na dav c tle ihh ts te s la e c hr e yeon a rme rizn oac sdr na ee, f a cto sae hfsd ebt if llnlur so ge d m aa o nwty da tah r rwn em.h s Tighth le ooe ryw esa.a lr iYnnn ae dcr.yrn ab spehaailnc mhg toi rsne teos
London luxurious interior living-room. cA o p na tepB neo trt e hc d rs la hyitf a t wa n am hdr icti l d erd as e tft htp p eei dco gt t pi itn re alg d ps a m ep rg il la fi er ln ol s t .g ws Ti,v e hwi rn esig t sah ch n ete dh nr le eec a ac isat v t aea dsp g .oue rrn nrit enle dg h wu itg h. ch iMa nr aa drc sisv ut tee prrl ee ,m srf s uo d,l v el m i tbe aeo isc ld s,h y y8a ,kpr la oh pc so ot tte s o htr ,, op iA pr hom eon ,te fomr o i mc oaa dn an ,, n wad ippr inueas elr ast ,t m hu iodpe n,n t ,8 to r, ke ml ,y a ri la en pt agc hl h ,d o rom teow ao .lnv it,i y el ,i f e M pa artA ii nnn ti ièa nrn grec ,o, i De bdn oeat ntr ik is cs t y eo Se xn al pl reo ar ewC zhso sa il nion o,sd nds iul asigs mr khw , t eh iet mih gm e hee r y adrea led,l t d S aa,t i nlec .dop lh so ia lr vn z e o rn , e
Fig.3: Visualization results of LaVi-Bridge with different language models. The first
row to the fifth row present the results with CLIP text encoder, T5-Small, T5-Base,
T5-Large, and Llama-2, respectively. The prompts are displayed at the top or bottom
of each column.
understanding of textual attributes, such as generating correct object prop-
ertieslikecolorandshape,aswellasaccuraterelationshipsbetweenobjects,
such as spatial positioning.
We conducted a user study on different combinations of language and vision
models. For each combination, we evaluated two metrics, namely image quality
and text alignment. Users were asked to rank the generated images based on
theseevaluationcriteria.Theimagerankedlastreceivedascoreof1,thesecond-
to-last received a score of 2, and so on. We then calculated the percentage of
scoresforeachmodel.Weselected20promptsandincluded30usersparticipated
inthetesting.Inadditiontoquantitativeevaluationanduserstudy,weprovided
amplevisualizationresultsineachsectiontoofferamoreintuitiveunderstanding
of the performance of each model.
4.2 Evaluation on Different Language Models
This section evaluates the performance of LaVi-Bridge with different language
models. We fixed the vision model to the U-Net of Stable Diffusion V1.4 and
integrated it with different language models under LaVi-Bridge. We considered
CLIP text encoder, based on the encoder-only framework, T5 series (T5-Small,
T5-Base,T5-Large),basedontheencoder-decoderframework,andLlama-2-7B,
PILC
llamS-5T
esaB-5T
egraL-5T
2-amalLLaVi-Bridge 9
Table 1: Quantitative evaluation of LaVi-Bridge with different language models.
“Short”, “Long” and “Comp” denote short prompts, long prompts, and compositional
prompts respectively. The best results are in bold.
CLIP T5-Small T5-Base T5-Large Llama-2
Short - FID 23.57 22.98 22.62 23.11 21.80
Short - Aesthetics 5.609 5.813 5.888 5.881 5.883
Short - CLIP Score 0.3102 0.3122 0.3149 0.3156 0.3172
Long - Aesthetics 6.003 6.206 6.284 6.305 6.355
Long - CLIP Score 0.3120 0.3111 0.3179 0.3193 0.3231
Comp - Color 0.3578 0.3368 0.3856 0.3889 0.4859
Comp - Shape 0.3752 0.2962 0.3266 0.3552 0.4285
Comp - Texture 0.4506 0.3728 0.4132 0.4524 0.5055
Comp - Spatial 0.1296 0.1456 0.1569 0.1582 0.1914
Comp - Non-Spatial 0.3009 0.2984 0.3054 0.3068 0.3106
Comp - Complex 0.2985 0.2728 0.3055 0.3072 0.3094
based on the decoder-only framework. We present the visualization results in
Fig. 3, quantitative evaluation in Tab. 1, and user study in Fig. 5.
Visualization From Fig. 3, we can observe that with LaVi-Bridge, all these
language models can effectively integrate with U-Net of Stable Diffusion V1.4
and generate meaningful results, such as cases of the cat and living room in
Fig. 3. This demonstrates the great generalization ability of LaVi-Bridge for
various language models. Additionally, we notice that the performance of dif-
ferent model structures varies when the provided prompts contain more com-
plexsemantics.Wefindthatthetext-to-imagediffusionmodelcorrespondingto
Llama-2 can perfectly describe semantic information. For example, in the third
column, Llama-2’s generated result effectively integrates a woman into the sea
offragmentedporcelain.Inthefourthcolumn,itcorrectlyunderstandsandgen-
erates both the girl and the cat in a paper craft art. In the seventh column,
it even portrays an entire beach scene using yarn. These examples surpass the
capabilities of those models with CLIP and T5. Furthermore, we observe that
T5-Large and Llama-2 accurately generate food and wine in the case of Iron
Man, and in the last column, they successfully generate “an ancient stone with
eyes in dark yellow and emerald”. Models with CLIP text encoder, T5-Small,
and T5-Base are not able to capture these cases accurately.
Quantitative Evaluation From Tab. 1, we can observe that Llama-2 achieves
thebestresultsforallthemetricsusedtoevaluatetextalignmentability,under
the setting of all the short prompts, long prompts, and compositional prompts.
Besides,Llama-2alsoperformsthebestonmostofthemetricsevaluatingimage
quality. On the other hand, as the model capacity increases, in general circum-
stances T5-Large usually outperforms T5-Base, and T5-Base outperforms T5-
Small in the area of Natural Language Processing. This conclusion also holds
true for LaVi-Bridge. For all the metrics used to evaluate text alignment ability
inTab.1,T5-LargeissuperiortoT5-Base,andT5-BaseissuperiortoT5-Small.
This tells us that incorporating a better language model into the text-to-image10 S. Zhao et al.
Forest. Pirba let igeah cs thh s ii ,w lp llui hgt sir htra rltpp a po tp iooe olld nl ue i atnn irog ta ni n a c ,re o ct, wis nvm ooeri mlcu k m am btyeia c te S ral eis tc nt m rl siog eomh isJpt a in hyn ee eg ,b r, eius n,pl ta ae r, ric ctrt ae na tncoeduu delva er ere t,aad aui lmi .n st bc yio leesn,m t ic streAebtas,n cdaorns e e madg p ec tdi yt yb s twy r etiti emh t ,re u v, iit bnr ree ade n sb t, u cfl oiold lw oin re sgr ,ss l,, i nslo ecna atg rt t ed .reesde rlteeadv es, sA u f ri re ortucine ny dg esawdrd oberynd c gaonnloodrm o fs nuhe li h ew fill soda wfr. arHei coerer s ,,s artc enala andd dd t ysi on twv oaae rl dim raienno fgt erl nypc dr loaa n hnf t ita se s d .gr oAa fc rr ddko eematnme l kreimd ia nsv gitne d aes o dab m nleo .d xo pbmra eirnsk sg, i obgnarar idn s ed pni as,h i ne ts e da
Smooth me ea lt e t ga ab nle t,, lr ige hst ta s.urant, Paris, enAc Tgo s uhfwflefes i ser l au,i n rwcrg ooi,tf uhm f cen rwu eedl a a it cntivuc ige no p s dgl , o iow agrf nie ht t ad i ahcl l ep h lau o rrs rtir iec nt lrh aa gvnl le sei dqs cm s u eaci ne sadr epa gg e oe ge fs ran e etftefrl ny lo wed mr clai etpy sstp sh ttlo hie pn e a odg s ce fo saopu inlbt otth iw rla is stsa io toerifdcfs a .a. t lhnT de ho i empc e opea rontnr ast io lao, fln . Asok nn hy .a ot tThuuhesre ea mr l me ola auan drnde ets a sc oiena fv p s gse.e rt r eTo ap ehnla neemi rn paeot ln i auin ds nng a tt w swa s oi neni ot eash dx bb w toee tnh ll ooo i tt ww the h ec tw e hl soi hehtu oh omd u rss seoo e.f um l .To ne ha t eatpi irn nl eag s n a .i t rn Ts e h t g mehrreo ae w nb iysliun aeg mPa ook fre t mrua ip dit, bs wp roih mgeo ha mtr toi an pg rg ar s baa tp y em h Al y ca r, ois ak Alo sww rtseoi ,t rmh p, rmat ia mn as s ei dne el l ea s on ,g fi s ln fea l m o.th woe er os rstuy ,s le
Fig.4: Visualization results of LaVi-Bridge under different generative vision models.
ThefirstrowtothethirdrowpresenttheresultswithU-NetinLatentDiffusionModel,
U-Net in Stable Diffusion V1.4 and transformer in PixArt, respectively. The prompts
are displayed at the top or bottom of each column.
diffusion model under LaVi-Bridge can lead to improved text alignment. This
makesoneofthemotivationsofLaVi-Bridgemeaningful,whichisthatreplacing
the model in the existing text-to-image diffusion model with a better model can
lead to performance improvements.
User Study We follow the settings described in Sec. 4.1, and the results are
presented in the two disk diagrams on the left side of Fig. 5. The model using
Llama-2 demonstrates the best performance in terms of both image quality and
textalignment,withaparticularlypronouncedadvantageintextalignment.On
the other hand, CLIP and T5-Small exhibit noticeably poorer performance on
both image quality and text alignment compared to other models.
4.3 Evaluation on Different Vision Models
ThissectionevaluatestheperformanceofLaVi-Bridgewithdifferentvisionmod-
els. We fixed the language model to T5-Large and integrated it with different
generative vision models under LaVi-Bridge. We considered the well-trained U-
Nets in the Latent Diffusion Model and Stable Diffusion V1.4, as well as the
Vision Transformer in PixArt, totally three models. We present the visualiza-
tionresultsinFig.4,quantitativeevaluationinTab.2,anduserstudyinFig.5.
Visualization From Fig. 4, we can see that all these three vision models inte-
grate well with T5-Large and generate relatively accurate images based on the
giventextprompts.Fromthesecases,wecanobservethattheimagesgenerated
by the transformer model based on PixArt exhibit richer details compared to
the images generated by the other two models based on U-Net. For example,
the forest in the first column, the hull of the pirate ship in the third column,
and the bushes at the foot of the mountain in the sixth column are very intri-
cateandrealistic.Additionally,wecanobservefromthesecasesthattheimages
generated by the model with U-Net of Stable Diffusion V1.4 have more detailed
teN-U
teN-U
remrofsnarT
)MDL(
)DS(
)trAxiP(LaVi-Bridge 11
Table 2: Quantitative evaluation of LaVi-Bridge under different generative vision
models.“Short”,“Long” and“Comp” denoteshortprompts,longprompts,andcompo-
sitional prompts respectively. The best results are in bold.
U-Net(LDM) U-Net(SD) Transformer(PixArt)
Short - FID 25.94 23.11 23.02
Short - Aesthetics 5.703 5.881 6.145
Short - CLIP Score 0.3126 0.3156 0.3172
Long - Aesthetics 6.122 6.305 6.406
Long - CLIP Score 0.3189 0.3193 0.3210
Comp - Color 0.4099 0.3889 0.3689
Comp - Shape 0.3724 0.3552 0.3316
Comp - Texture 0.5046 0.4524 0.4553
Comp - Spatial 0.1550 0.1582 0.1725
Comp - Non-Spatial 0.3004 0.3068 0.3098
Comp - Complex 0.3060 0.3072 0.3014
features compared to the images generated by the model with U-Net of Latent
DiffusionModel.Furthermore,wecanalsofindthat,forthePixArt-basedmodel,
text alignment is better in some cases. For instance, in the image of the fifth
column, only the model that is based on the transformer of PixArt generates
the “aged car” mentioned in the prompt. Similarly, in the seventh column, the
garden warrior holding a sword and shield is highly consistent with the prompt
description.
Quantitative EvaluationFromTab.2,itcanbeobservedthatforallthemet-
ricsmeasuringimagequality,LaVi-BridgewiththePixArtvisionmodelachieves
the best results. Additionally, PixArt also achieves the best text alignment for
both short and long prompts. This reflects the use of PixArt’s transformer as a
vision model can also improves the model’s understanding of semantics to some
extent. Additionally, it is noteworthy that the U-Net in Stable Diffusion, an
enhanced version of the U-Net utilized in the Latent Diffusion Model, still out-
performs Latent Diffusion Model’s U-Net under LaVi-Bridge on all the metrics
measuringimagequality.ThisalignswithourpreviousdiscussioninSec.4.2and
further validates the underlying motivation behind our proposed LaVi-Bridge.
User Study We follow the settings described in Sec. 4.1, and the results are
presented in the two disk diagrams on the right side of Fig. 5. The model using
thetransformerfromPixArtdemonstratesthebestperformanceintermsofboth
imagequalityandtextalignment,withanotablysignificantadvantageinimage
quality. Additionally, the U-Net in Stable Diffusion outperforms the U-Net in
the Latent Diffusion Model overall.
4.4 Ablation Study
Inthissection,weinvestigatetwosetsofablationexperiments.Thefirstsetaims
to explore the impact of training LaVi-Bridge on the original pre-trained text-
to-image diffusion model. The second set of experiments is to study the effects12 S. Zhao et al.
ImageQuality TextAlignment ImageQuality TextAlignment
15% 16%
24% 27% 24% 28%
18% 15% 42% 38%
21% 22% 22% 20% 34% 34%
CLIP T5-Small T5-Base T5-Large Llama-2 U-Net(LDM) U-Net(SD) Transformer(PixArt)
Fig.5: Userstudy.Thetwodiskdiagramsontheleftdisplaytheuser’sscoringresults
on different language models, while the two disk diagrams on the right display the
user’s scoring results on different generative vision models. The percentage represents
the proportion of the score obtained by a model out of the total score of all models.
of LoRA and adapters in LaVi-Bridge. For both sets of experiments, we present
visualization results in Fig. 6 and provide quantitative evaluations in Tab. 3.
Training with LaVi-Bridge We investigate the impact of our LaVi-Bridge
training framework on the original pre-trained text-to-image diffusion model.
Specifically, we consider Stable Diffusion V1.4 which adopts CLIP text encoder
as its language model and U-Net as its vision model. We incorporate LoRA
and an adapter and apply LaVi-Bridge to the same language and vision models
withidenticalstructures andweightsto thosein StableDiffusion V1.4.Wethen
compare the performance of the model under LaVi-Bridge with the original
Stable Diffusion V1.4.
The visualization results are shown in the first two rows of Fig. 6. For these
two models, there is no significant difference in image quality and text align-
ment, varying on a case-by-case basis. In some cases, Stable Diffusion performs
better,suchasinthethirdcolumn,whereStableDiffusionsuccessfullygenerates
the case of a “Fox bracelet made of buckskin with fox features”, while the model
trained under LaVi-Bridge only generates the fox and fails to understand the
braceletmadeofbuckskin.Similarly,inthecaseofMarvel’sHulkplayingbasket-
ball,StableDiffusiongeneratesaslamdunkactionfollowingtheprompt,whereas
the model trained under LaVi-Bridge does not. However, in the second column,
the model trained under LaVi-Bridge correctly understands the quantity and
successfully generates two elephants, while Stable Diffusion only generates one.
Moreover, in the last column, the model trained under LaVi-Bridge accurately
describes a frog in a spacesuit, while Stable Diffusion fails.
ThelefttwocolumnsofTab.3presentthequantitativeevaluationresults.It
can be observed that Stable Diffusion achieves the best image quality and text
alignmentforbothshortpromptsandlongprompts.However,forcompositional
prompts, the model trained under LaVi-Bridge outperforms Stable Diffusion in
four out of six settings.
Based on the visualization results and quantitative evaluations, we can con-
clude that overall there is no significant improvement or decline in text align-
ment. Regarding image quality, it should be noted that training with LaVi-
Bridgemayresultinadecreasecomparedtotheoriginaltext-to-imagediffusion
model, if the same models and weights are used. However, it is important toLaVi-Bridge 13
Sea. Fox bracelet made of buckskin with fox features. M pa er hv rsoe pol‘ es p c ,H t a iu vnl ek d , p ghl rea a y pisin h dg icr seb ,sa sss uek rde r t eib n aa lt il h sl, te ih c Le ra ei ks ae j lu r issm m up ,n i ein f mog r ot mo ti , vd r eu e n ra ek li a so lt iin sc m th .e W wdh eoi iit gne hg N t alie fltl oo etr r e o p fb o su swtil rtl h, ei oia n tnn eg,t t bh dh ar o, o ceinkp x ggo p r m rWoe uo sensr igp i dohh ,n ti fc luoi fla ftl d ibsnu t ogrl d,et wync ,a ge st taut hrl r,e i rnl, ei gfa at ai lW n trghee laai eg l itvh see tt 'il rsci yf .ct he loer t, a hv vee ysr b,y a as crt btr io eon lnl,g , ,
Two elephants in the forest. suA r rm ouis nc dh eie dv o bu y s c ofe lorr re fut lw ci at nh d a y p . Tla hy efu jal rg r si in ts s oq nu e ae wze os o i dts ee nl f t ain bt lo e a in l a ar g coe z g yl a ks its c j ha er, n , A rabbit, forest in spring. A cute f tr ho rg o uw ge ha sr pin ag c ea bsp ua t c se le s eu pit y .floating
and warm sunlight filters through a nearby window.
Fig.6: Visualization results of the ablation study. The top two rows show the impact
ofLaVi-Bridgeontheoriginalpre-trainedtext-to-imagediffusionmodels.Thebottom
threerowsillustratetheinfluenceoftheadapterandLoRA.Thepromptsaredisplayed
at the top or bottom of each column.
understand that the main purpose of LaVi-Bridge is to establish connections
between different language and vision models, enabling the utilization of more
advancedmodelsforperformanceenhancement.Itisnotintendedtobedirectly
appliedtotheoriginaltext-to-imagediffusionmodelsusingthesamemodelsand
weights.
LoRA and Adapter Here, we investigate the role of LoRA and adapters in
LaVi-Bridge.WeuseT5-LargeasthelanguagemodelandStableDiffusionV1.4’s
U-Netasthevisionmodel.FortheLoRAexperiments,wekeptthelanguageand
visionmodelsfixedwithoutintroducingLoRA,andonlytrainedtheadapter.For
the adapter experiments, considering the mismatch in the dimensions of text
embeddings from the language model and the input embeddings acceptable by
the vision model, we aligned the dimensions between the language and vision
models using a single linear layer instead of stacked feedforward layers which
include non-linear activation layers. Under this setting, we trained both LoRA
and this linear layer.
The visualization results are shown in the bottom three rows of Fig. 6. We
canobservethatbothimagequalityandtextalignmentaresignificantlyaffected
when LoRA and adapters are not used. For example, in the case of “Bull Fit
Athlete” intheseventhcolumn,withoutLoRAortheadapter,themodelcannot
understand and integrate these two less related elements, and the image quality
is much lower compared to results generated by the original setting. We also
DS
teN-U+PILC
retpadAo/w
ARoLo/w
teN-U+5T14 S. Zhao et al.
Table 3: Quantitative evaluation of the ablation study. The left two columns present
the impact of LaVi-Bridge on the original pre-trained text-to-image diffusion models.
The right three columns demonstrate the influence of the adapter and LoRA. “Short”,
“Long” and “Comp” denote short prompts, long prompts, and compositional prompts
respectively. The best results are in bold.
SD CLIP+U-Net w/o Adapter w/o LoRA T5+U-Net
Short - FID 20.32 23.57 23.81 22.35 23.11
Short - Aesthetics 5.899 5.609 5.807 5.829 5.881
Short - CLIP Score 0.3132 0.3102 0.3147 0.3107 0.3156
Long - Aesthetics 6.120 6.003 6.131 6.273 6.305
Long - CLIP Score 0.3171 0.3120 0.3106 0.3097 0.3193
Comp - Color 0.3570 0.3578 0.3550 0.2485 0.3889
Comp - Shape 0.3563 0.3752 0.3044 0.2944 0.3552
Comp - Texture 0.4028 0.4506 0.4001 0.3190 0.4524
Comp - Spatial 0.1225 0.1296 0.1651 0.0956 0.1582
Comp - Non-Spatial 0.3104 0.3009 0.3065 0.2998 0.3068
Comp - Complex 0.3042 0.2985 0.2878 0.2687 0.3072
found that the results without LoRA are worse than those without the adapter.
Forinstanceinthefourthcolumn,thereisnotevenaferretpresentintheimage
in the absence of LoRA.
TherightthreecolumnsofTab.3presentthequantitativeevaluationresults.
We find that our default setting, which utilizes both LoRA and the adapter,
achieves the best performance in most cases. Additionally, overall, the absence
of LoRA has a significant impact on text alignment, with many text alignment
evaluation metrics being much lower compared to the absence of the adapter.
5 Conclusion
In this paper, we propose LaVi-Bridge, which works on text-to-image diffusion
models. LaVi-Bridge is capable of connecting various language models and gen-
erative vision models for text-to-image generation. It is highly versatile and can
adapt to different structures. LaVi-Bridge is also flexible, as it achieves inte-
gration without modifying the original weights of language and vision models.
Instead, it utilizes LoRA and an adapter for fine-tuning. Additionally, under
LaVi-Bridge,usingsuperiorlanguageorvisionmodelscanenhancethetextcom-
prehension capability or image quality. These advantages enable LaVi-Bridge to
helptext-to-imagediffusionmodelsleveragethelatestadvancementsintheareas
ofNaturalLanguageProcessingandComputerVision,toenhancetext-to-image
generation.Webelievethatthistaskholdssignificantresearchvalueandrequires
further exploration. LaVi-Bridge allows designers, artists, and others to flexibly
utilize existing language and vision models to achieve their creative goals. It
is of utmost importance to avoid misuse and mitigate potential negative social
impacts. In practical deployment, it is crucial to standardize its usage, improve
model transparency.LaVi-Bridge 15
References
1. Bao,F.,Li,C.,Cao,Y.,Zhu,J.:Allareworthwords:avitbackboneforscore-based
diffusion models. CVPR (2023) 4
2. Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang,
J., Lee, J., Guo, Y., Manassra, W., Dhariwal, P., Chu, C., Jiao, Y., Ramesh,
A.: Improving image generation with better captions. https://cdn.openai.com/
papers/dall-e-3.pdf (2023) 2, 4
3. Brooks, T., Holynski, A., Efros, A.A.: Instructpix2pix: Learning to follow image
editing instructions. CVPR (2023) 4
4. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-
lakantan,A.,Shyam,P.,Sastry,G.,Askell,A.,etal.:Languagemodelsarefew-shot
learners. NeurIPS 2020 4
5. Chen, J., Huang, Y., Lv, T., Cui, L., Chen, Q., Wei, F.: Textdiffuser: Diffusion
models as text painters. NeurIPS (2024) 4
6. Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok, J., Luo, P.,
Lu, H., et al.: Pixart-α: Fast training of diffusion transformer for photorealistic
text-to-image synthesis. ICLR (2024) 2, 4
7. Croitoru, F.A., Hondru, V., Ionescu, R.T., Shah, M.: Diffusion models in vision:
A survey. TPAMI (2023) 4
8. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidi-
rectional transformers for language understanding. NAACL-HLT (2018) 3
9. Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. NeurIPS
(2021) 1, 4
10. Ding,M.,Yang,Z.,Hong,W.,Zheng,W.,Zhou,C.,Yin,D.,Lin,J.,Zou,X.,Shao,
Z.,Yang,H.,etal.:Cogview:Masteringtext-to-imagegenerationviatransformers.
NeurIPS (2021) 4
11. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,
T.,Dehghani,M.,Minderer,M.,Heigold,G.,Gelly,S.,Uszkoreit,J.,Houlsby,N.:
Animageisworth16x16words:Transformersforimagerecognitionatscale.ICLR
(2021) 2, 4
12. Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G.,
Cohen-Or, D.: An image is worth one word: Personalizing text-to-image gener-
ation using textual inversion. arXiv preprint arXiv:2208.01618 (2022) 4
13. Ge,S.,Park,T.,Zhu,J.Y.,Huang,J.B.:Expressivetext-to-imagegenerationwith
rich text. ICCV (2023) 4
14. Goodfellow,I.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-Farley,D.,Ozair,S.,
Courville, A., Bengio, Y.: Generative adversarial nets. NeurIPS (2014) 4
15. Guo,Y.,Yang,C.,Rao,A.,Liang,Z.,Wang,Y.,Qiao,Y.,Agrawala,M.,Lin,D.,
Dai, B.: Animatediff: Animate your personalized text-to-image diffusion models
without specific tuning. ICLR (2024) 4
16. Hao,S.,Han,K.,Zhao,S.,Wong,K.Y.K.:Vico:Detail-preservingvisualcondition
forpersonalizedtext-to-imagegeneration.arXivpreprintarXiv:2306.00971(2023)
4
17. Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,Hochreiter,S.:Ganstrained
by a two time-scale update rule converge to a local nash equilibrium. NeurIPS
(2017) 7
18. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. NeurIPS
(2020) 1, 416 S. Zhao et al.
19. Ho,J.,Salimans,T.:Classifier-freediffusionguidance.NeurIPSWorkshoponDeep
Generative Models and Downstream Applications (2021) 7
20. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen,
W.: Lora: Low-rank adaptation of large language models. ICLR (2022) 3
21. Huang, K., Sun, K., Xie, E., Li, Z., Liu, X.: T2i-compbench: A comprehensive
benchmarkforopen-worldcompositionaltext-to-imagegeneration.arXivpreprint
arXiv: 2307.06350 (2023) 7
22. Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., Irani,
M.: Imagic: Text-based real image editing with diffusion models. CVPR (2023) 4
23. LAION-AI: aesthetic-predictor. https://github.com/LAION-AI/aesthetic-
predictor (2022) 7
24. Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O.,
Stoyanov, V., Zettlemoyer, L.: Bart: Denoising sequence-to-sequence pre-training
for natural language generation, translation, and comprehension. arXiv preprint
arXiv:1910.13461 (2019) 4
25. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. ECCV (2014) 7, 20
26. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. ICLR (2019) 7
27. Meng,C.,He,Y.,Song,Y.,Song,J.,Wu,J.,Zhu,J.Y.,Ermon,S.:Sdedit:Guided
image synthesis and editing with stochastic differential equations. ICLR (2022) 4
28. Mou, C., Wang, X., Xie, L., Zhang, J., Qi, Z., Shan, Y., Qie, X.: T2i-adapter:
Learning adapters to dig out more controllable ability for text-to-image diffusion
models. arXiv preprint arXiv:2302.08453 (2023) 4
29. Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B.,
Sutskever,I.,Chen,M.:Glide:Towardsphotorealisticimagegenerationandediting
with text-guided diffusion models. ICML (2021) 2, 4
30. Peebles, W., Xie, S.: Scalable diffusion models with transformers. ICCV (2023) 4
31. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models
from natural language supervision. ICML (2021) 2, 3, 4
32. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.: Improving lan-
guage understanding by generative pre-training. OpenAI blog (2018) 3
33. Radford,A.,Wu,J.,Child,R.,Luan,D.,Amodei,D.,Sutskever,I.,etal.:Language
models are unsupervised multitask learners. OpenAI blog (2019) 3
34. Raffel,C.,Shazeer,N.,Roberts,A.,Lee,K.,Narang,S.,Matena,M.,Zhou,Y.,Li,
W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text
transformer. JMLR (2020) 2, 3, 4, 21
35. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-
conditional image generation with clip latents. arXiv preprint arXiv:2204.06125
(2022) 2
36. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M.,
Sutskever, I.: Zero-shot text-to-image generation. ICML (2021) 4
37. Reed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B., Lee, H.: Generative
adversarial text to image synthesis. ICML (2016) 4
38. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. CVPR (2022) 2, 4, 21
39. Ronneberger,O.,Fischer,P.,Brox,T.:U-net:Convolutionalnetworksforbiomed-
ical image segmentation. MICCAI (2015) 2, 4
40. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dream-
booth: Fine tuning text-to-image diffusion models for subject-driven generation.
CVPR (2023) 4LaVi-Bridge 17
41. Saharia,C.,Chan,W.,Saxena,S.,Li,L.,Whang,J.,Denton,E.L.,Ghasemipour,
K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-
to-image diffusion models with deep language understanding. NeurIPS (2022) 2,
4
42. Schuhmann,C.,Vencu,R.,Beaumont,R.,Kaczmarczyk,R.,Mullis,C.,Katta,A.,
Coombes,T.,Jitsev,J.,Komatsuzaki,A.:Laion-400m:Opendatasetofclip-filtered
400 million image-text pairs. arXiv preprint arXiv:2111.02114 (2021) 5
43. Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. ICLR (2021)
1, 4, 7
44. Song,Y.,Dhariwal,P.,Chen,M.,Sutskever,I.:Consistencymodels.ICML(2023)
4
45. Song,Y.,Sohl-Dickstein,J.,Kingma,D.P.,Kumar,A.,Ermon,S.,Poole,B.:Score-
based generative modeling through stochastic differential equations. ICLR (2021)
1, 4
46. Sun, K., Pan, J., Ge, Y., Li, H., Duan, H., Wu, X., Zhang, R., Zhou, A., Qin, Z.,
Wang, Y., et al.: Journeydb: A benchmark for generative image understanding.
NeurIPS (2024) 21
47. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning with neural
networks. NeurIPS (2014) 3
48. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-
lykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023) 4
49. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,
Ł., Polosukhin, I.: Attention is all you need. NeurIPS (2017) 3
50. Wu, W., Li, Z., He, Y., Shou, M.Z., Shen, C., Cheng, L., Li, Y., Gao, T., Zhang,
D., Wang, Z.: Paragraph-to-image generation with information-enriched diffusion
model. arXiv preprint arXiv:2311.14284 (2023) 4
51. Xue, Z., Song, G., Guo, Q., Liu, B., Zong, Z., Liu, Y., Luo, P.: Raphael: Text-to-
image generation via large mixture of diffusion paths. NeurIPS (2024) 2
52. Zhang,H.,Xu,T.,Li,H.,Zhang,S.,Wang,X.,Huang,X.,Metaxas,D.N.:Stack-
gan: Text to photo-realistic image synthesis with stacked generative adversarial
networks. ICCV (2017) 4
53. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image
diffusion models. ICCV (2023) 4
54. Zhao, S., Chen, D., Chen, Y.C., Bao, J., Hao, S., Yuan, L., Wong, K.Y.K.: Uni-
controlnet: All-in-one control to text-to-image diffusion models. NeurIPS (2024)
418 S. Zhao et al.
A Long Prompts
In this section, we provided a comprehensive explanation of the evaluation for
long prompts mentioned in the main paper. To conduct this evaluation, we uti-
lized the subset of 30k text-image pairs from COCO2014, which was originally
used for evaluating short prompts, and employed the Llama-2 to extend the
captions within this subset to a length of 20-70 words.
Fig.7: Statisticsregardingthelongpromptsutilizedinevaluationsinthemainpaper.
The figure on the left visualizes word frequency. The histogram on the right presents
thedistributionofsentencelength,wherehorizontalaxisrepresentstherangeofword
counts in the prompts, and the vertical axis represents the number of prompts falling
within each sentence length range.
Specifically, we used the Llama-2-7b-chat model and provided the following
prompt:“Please expand the caption to 20-70 words to enrich its semantic mean-
ing: ‘placeholder’. Just give me one answer.” Here, the term “placeholder”
represents the short prompts that we aimed to expand. In this way, we success-
fully generated 30k long prompts for evaluation. Here are a few examples of the
generated long prompts:
1. Short Prompt: “a brown, white and yellow bird standing in the grass.” −→
Long Prompt: “A striking, multi-hued bird with warm brown plumage, crisp
white patches on its wings, and vibrant yellow feathers stands gracefully in
lush green grass, creating a picturesque scene.”
2. Short Prompt: “two sheep standing in the snow with one looking for food”
−→LongPrompt:“Twofluffywhitesheepstandstatelyinthepristinesnow,
their wool glistening under the crisp sunlight. One of them eagerly scans the
ground,sniffingoutpotentialnourishmentamidstthefrozenlandscape,while
the other stands watchful and still, seemingly lost in thought.”
3. ···
WepresentedstatisticalanalysisofthegeneratedlongpromptsintheFig.7.
The figure on the left visualizes word frequency, while histogram on the right
illustratesthedistributionofsentencelengths.Weobservedthatthevocabulary
diversityinlongpromptsisquiterich,andthemajorityoflongpromptstypically
have sentence lengths ranging from 20 to 40 words, and there is also a certain
portion that falls within the 40-60 words range.LaVi-Bridge 19
B Training Cost
As mentioned in the main paper, LaVi-Bridge does not require modifying the
original weights of the language and vision models. Instead, it introduces and
trains LoRA and adapters. This approach significantly reduces the training cost
comparedtotrainingtheentiretext-to-imagediffusionmodel.Forthetrainingof
LaVi-Bridge, we utilized 8 A100 GPUs with a batch size of 256 and completed
the training in less than 2 days. Furthermore, we provided a comparison of
the number of parameters for different language and vision model combinations
in Tab. 4. The leftmost column shows the language and vision models used.
It can be observed that training only the LoRA and the adapter leads to a
significantreductioninthenumberoftrainableparameterscomparedtotraining
theoriginallanguageandvisionmodel.Additionally,bothLoRAandtheadapter
are plug-and-play components, making LaVi-Bridge highly flexible.
Table 4: Comparison of number of parameters.
Language Vision Language Vision
Sum Adapter Sum
Model Model LoRA LoRA
CLIP
123M 860M 983M 14M 2M 28M 44M
U-Net(SD)
T5-Small
35M 860M 895M 9M 0.8M 28M 38M
U-Net(SD)
T5-Base
110M 860M 970M 14M 2M 28M 44M
U-Net(SD)
T5-Large
335M 860M 1195M 21M 6M 28M 55M
U-Net(SD)
Llama-2
6738M 860M 7598M 229M 34M 28M 291M
U-Net(SD)
T5-Large
335M 872M 1207M 30M 6M 29M 65M
U-Net(LDM)
T5-Large
Transformer 335M 611M 946M 113M 6M 17M 136M
(PixArt)
C Training Steps
As mentioned in the main paper, we trained the LaVi-Bridge for 50k steps. In
this section, we present the generated images as training progresses. We utilized
the T5-Large as the language model and the U-Net from Stable Diffusion V1.4
as the vision model.
Fig. 8 illustrates the evolution of the model’s performance. Initially, during
the first 1k steps, the image quality was poor, and the model struggled to com-
prehend the given prompt. However, as the training progressed to 10k steps,20 S. Zhao et al.
therewasasignificantimprovementinimagequality.Bythetimeitreached20k
steps, the model exhibited enhanced semantic understanding. Finally, at 50k
steps, the model demonstrated further optimization compared to the model at
20k step, showcasing the best performance.
A mischievous ferret with a playful grin squeezes itself into a large glass jar, surrounded by colorful candy. The
jar sits on a wooden table in a cozy kitchen, and warm sunlight filters through a nearby window.
Training
1k 5k 10k 20k 50k Steps
Fig.8: Theresultsfordifferenttrainingsteps.Promptsaredisplayedabove,whilethe
number of training steps is shown below.
T thh ee r se tr i es e a t f wir he i lh ey ad r va an nt io sn in A co c mat p usl te ee rp ms oo un s t eo .p of a A ba b ca ks se wb ia nl gl p ol fa hy ie tr t io nn g ath pe i tch.
the background.
a big jumbo jet sits some food is in a glass three men around a table
parked at an airport jar in a blender looking at an object.
s Ft ro iec dk eim gga sg e tr ep no dp .ular. n h coo o lr u od s ri e sc ,, l hea v yk e pe n e w i rn i rt g eh , a pa lio ss l tm a icra ,l il lgl o hw tt so s o,o fbd r ight o c nei ol n p bta e oi r rn dot ei fn r w sg h ,o wif t iec th l bo fau ucd lk ls g co r oof l ouco rn sl d ,o nr ts o ic uo ecn h ing
details, resolution 8k transition,pink color, soft details
A s h f w f w a ar ue n ton a a i mrr n dr m nlve i l d oz ,e i sx e s oo s stsp h yu w pn ia a is lr ht n in s s sa rs eg . os hl e t s r Pi h u pv t e ala e ne o u ,a cn d, r s wg cb dt hu e eaa r i , s tl dia sct n hsnli r k ebt s o a ed tt eyf rm hu r r ia ko e ea c to ro f sp m m fad l n o rclfe e i attn o ro or e mem rirn m nm at t e oo e tl h ai p ec dv c ri l o e s ek ii asn s r iu me nt l eag i p nm c ar air . y ngo t i o p nT i vn no t h iic ny g tm e r i g ne gte a p c o t g f rlh ehr eh r ot nn ea oo oi das m g rt at t eo ,rpi eb ac n rg o ,e pr ic a c g wra haw as hp t i a l c tha ih y rn r e tp p s pod id s h mi hf d erw o oc a ee , t i o tl , n on o otn oa ,n hf g g ct ri aa rin ur to ate ag bg pn ns s brw has u ea li l yero ni d v , n d oie dva n ael rd k o p s o s mt ti r fe ol e i anp p r d mca p f ee ai i vin n l ni ilt n ag i gni l n , ,tg g g h d ur p, e r r l o taa ae ru m ln ab ag cl aa rho e etr n i,m at c d h d l, io e e a sfr a t ww r in cu na tb , ar br si l yo e ,r
the focal point, free of any distractions or cinematic color grade
obstructions.
Fig.9: Examples from COCO2017 train set and the internal dataset.
D Training Set
As mentioned in our main paper, we conducted training using the COCO2017
[25] train set, which consists of around 600k text-image pairs, along with an ad-
ditional 400k internal data. The COCO2017 dataset primarily comprises highly
realisticimageswithshortandstraightforwardcaptions.Inordertoenhancethe
diversity and quality of the training data, we collected an additional 400k text-
image pairs that exhibit a wide range of artistic styles and high-quality visuals,
accompanied by accurate and detailed captions. We provide some examples of
the COCO2017 dataset and the internal dataset in Fig. 9.
7102OCOC
tesataDlanretnILaVi-Bridge 21
Th toe ts hle ee fk lu b fl fa yc wk hca itt e s pa it l ln oe wxt A ob fr o aw n n o lb de wn hch it esi t bs u i in ld f ir no gnt fAlobn swrh iisgic m y ho tmlv,a ein etrd rs a i s nf c lc gaal mip gff le e. o , sI wnw s eth ohee nemr t ec hi nea eng m sltyuea rrfg rr o oonf uzi f et nihc ndee iin nnst gc pfe ir lcnao eecz e,e a ,an n c dfw ai rssaetnt i noebgwruf ra a.nl ls Aj marci,os sc zuh yr i kreoiv tuo cnhudes nefde , ar br neydt c wwolaiot rhrmf au slp uclaa nny lif gdu hyl. tg Tfr ihi ln tee s rjqas ru t e hsie rtozse uos gn hi ta s a ew nlfoe ioan rdt boey na w tlaainrbg dlee o wignla as s
Two elephants in the forest. Tamheb siaonftc,e w, aa rgmen gtlloew fl iocfk ethr eo fc alingdhlte i nc rtehaet edda rak ncoezsys rm esoA onglns urtc teei ie eornsnn t sid npp rkra aa g wtwe eis ll nl sa gf ic stllra e dold eess s tw a oa ii lfnt h em o aslacdk ghe,i t cw cc areh el ae aats dth u vae ren erned ’std uw i njrroetiut rsi i,rn c nwaga ts hl e .o i lT cef hh p taehla r ef ana chtd ts ie eg ad rhn i -sdd ta i cr sk . pA doo isfrm w t ocaeo fi l r nf telf hi snenieoeg g , p, nu mw o.lf Trsiu tt ha ahlt e l i c ,w c so co ua rfl evfro ere ar oese tui d nco nu gfp d p to aih,nnr ewgt a a rhdl li lic ie ucghm rhiti nl aseiq gler u a g r svrie cd tes e ls g naf er aneonsd om tasl fcy gt ea rh a npie tp dee p ld w erle ei snap fsyg l t ep h to coos tu s so at s w t f if bh aaa ien lnr i d ttc o ia. oe c sT lstoehia rce sn a l
Fig.10: ComparisonoftrainingonlyonCOCO2017andtrainingonbothCOCO2017
and the internal dataset.
In Fig. 10, we present a comparison of the results obtained from training on
COCO2017aloneandtrainingonbothCOCO2017andourinternaldataset.We
usedT5-Large[34]asthelanguagemodelandStableDiffusionV1.4’sU-Net[38]
asthevisionmodelforLaVi-Bridge.ItcanbeobservedinFig.10thatinthefirst
three columns, the model trained solely on COCO2017 performs well in terms
of both image quality and text alignment. The model accurately understands
quantities,asseeninthecaseoftwoelephants,andattributes,suchastheblack
cat and white pillow or the brown bench in front of the white building, and
so on. Furthermore, the model is capable of generating images using complex
prompts, as demonstrated in columns 4-6. However, when the model is tasked
with generating images depicting fanciful scenarios, as shown in the seventh
column, or images with a non-realistic style, as shown in the last column, the
model trained only on COCO2017 struggles to produce such images.
Asmentionedearlier,thetext-imagepairsinCOCO2017,fromacertainper-
spective, lack diversity. This is because the images in COCO2017 have a highly
realistic style, resulting in a lack of variety, and their quality is relatively low.
Additionally, all the captions in COCO2017 are short and very direct, as illus-
tratedinFig.9.Consequently,itisexpectedthatmodelstrainedonCOCO2017
will face challenges in generating images of whimsical scenarios or non-realistic
styles. Fortunately, there are now many high-quality text-image datasets avail-
able, such as [46], and we highly recommend incorporating these datasets into
training in order to achieve better image generation.
Here, we want to emphasize the contribution of LaVi-Bridge again. LaVi-
Bridge is a framework designed to bridge various language and vision models.
Even though it was trained only on COCO2017, as can be seen from the first
six columns in Fig. 10, LaVi-Bridge is still effective. The reason for the poor
performanceinthelasttwocolumnsinthefirstrowofFig.10isduetothelimited
diversityoftheCOCO2017trainingsetaspreviouslymentioned.IfLaVi-Bridge
is trained on a more general text-image dataset, it will become more versatile,
enabling better generation on a wide variety of images.
7102OCOC
7102OCOC
+
tesataDlanretnI22 S. Zhao et al.
E More Visualization Results
Inthissection,wepresentadditionalvisualizationresultsinFig.11.Incolumns
one through five, we kept the vision model fixed as U-Net from Stable Diffusion
V1.4andemployeddifferentlanguagemodels.Incolumnsfour,six,andseven,we
kept the language model fixed as T5-Large and utilized different vision models.LaVi-Bridge 23
T5-Large+
CLIP+U-Net-SD T5-Small+U-Net-SD T5-Base+U-Net-SD T5-Large+U-Net-SD Llama-2+U-Net-SD T5-Large+U-Net-LDM Transformer-PixArt
Fig.11: More visualization results. The first column to the seventh column present
theresultsofdifferentcombinationsusingLaVi-Bridge,wherethelanguageandvision
models used are indicated at the top of each column. The prompts for each row are
displayed either on the right or left.
.sniatnuoM
.m am g0n2ih1 c,tdauwo l,cri amhoco ar hnsou
hmc aeegbu
he hat hntoiw
s tniso
riteimsoipexhen
reapeplcOun eer .et vc ail ce d lae ch itc sy ys mp a a f no i r te sud dn e gr n it ts aa nrt imno
uc ll ih eg fih il fk oraD os,ta Tnf e c s e. isle te ee sv re ihr gte r rt tha so s e drT eme n r.g as fudi ht s dl ntar eioo w tewc ar rsse eg eehn ghmw gi st aai s rn xla ufo er osti t han lc f to ei tip wc ste a eo d dnrw o eid mt fte cdnot ipeedn teee saw di enr o gt ,eren- vebt augi a rje ioetrn t redri l aho n g p Ta,n t hd .si eddge gieizv arrii ild e y ly oh ter fs teh A itf .s d lae tt cc aip rfe fd o e tufi ol n ya lm eru
itH ne
s’erutan .ghntrizibiloebr mdnyas s,doeahrrce ttseddimnua d tniraip lsla gtn sidrnuadtnse ,serutaef
r Ado bi uo os 3rm l Drt ines i grfas el en ad ncd i sdm t tet eoul d ry rr mo bil ni fut y a, tl eh d a cned aot yd fc w .fi o n eTfag ehfv e et meeo s s, u t tsw gh oe e ri pet m h ld n a r cm oia n eum is dn ti sd a i oa iet dnti ec ut a h ra t ee hwt m elm ii ng o wdhus iotgp nn. wh d iTne o h sgr wiee l l .
g3lo2w
–it nbh gie t p s pot iy xwl ee el aro tsf e tm dre i fen ute t uw cr raa erf t eH ,t i o pnm hoore ipyr o h psa rhon idu ouk ha cr. eari ,i nin
Hocytpaeni rem r erm eane l dr gs lei av rs,e s u d fnle ort wea aei ll r , e se ,n n bgh liuna een ,c o be rd lge anq ndu icea l rti wt ry ei. sntdinegr,. wb be Ol rau oure wi tn dj ne g oa ohan rals iisf grtu t,hr y l hol t ly e au b l ps fi lt u ae p sr eb ys h a l, aio c oi nnk n ndg g t r p sh so hme lu e o ibn eltivad onec, ,g k dn f,g u a hbr lt leo l u o – u ir usna b ssdl oe ta d at wfh ynt i ee dt sr h hie nn ox cgo tlt oo e oosnr nfei o l adhirg i mbsho ut hf ai tn ia ntpng osw. ntA ies tl r d ha bi n ss ihdct yoo dcrr alte e r, k
ct rhp oTeo winit r nya v s t p,o a oss s it ttu ap tb tio noje t g kac oitt nos ng a k tsn i h n wd rg oe p d nao o reit m sna ,g t ofo i mvl lc ee aa rd js se t ew sl ee ti iis t cn. h g